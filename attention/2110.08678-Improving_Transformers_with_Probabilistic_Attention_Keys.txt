# 2110.08678.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2110.08678.pdf
# File size: 4141437 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Transformers with Probabilistic Attention Keys
Tam Nguyen* 1Tan M. Nguyen* 2Dung D. Le3Duy Khuong Nguyen1Viet-Anh Tran4Richard G. Baraniuk5
Nhat Ho** 6Stanley J. Osher** 2
Abstract
Multi-head attention is a driving force behind
state-of-the-art transformers, which achieve re-
markable performance across a variety of natu-
ral language processing (NLP) and computer vi-
sion tasks. It has been observed that for many
applications, those attention heads learn redun-
dant embedding, and most of them can be re-
moved without degrading the performance of the
model. Inspired by this observation, we propose
Transformer with a Mixture of Gaussian Keys
(Transformer-MGK), a novel transformer archi-
tecture that replaces redundant heads in transform-
ers with a mixture of keys at each head. These
mixtures of keys follow a Gaussian mixture model
and allow each attention head to focus on differ-
ent parts of the input sequence efÔ¨Åciently. Com-
pared to its conventional transformer counterpart,
Transformer-MGK accelerates training and infer-
ence, has fewer parameters, and requires fewer
FLOPs to compute while achieving comparable or
better accuracy across tasks. Transformer-MGK
can also be easily extended to use with linear
attention. We empirically demonstrate the advan-
tage of Transformer-MGK in a range of practical
applications, including language modeling and
tasks that involve very long sequences. On the
Wikitext-103 and Long Range Arena benchmark,
Transformer-MGKs with 4 heads attain compa-
rable or better performance to the baseline trans-
formers with 8 heads.
*Equal contribution. Co-Ô¨Årst authors are listed in alphabeti-
cal order**Co-last authorship1FPT Software AI Center, Ha Noi,
Vietnam2Department of Mathematics, University of California,
Los Angeles, USA3College of Engineering and Computer Sci-
ence, VinUniversity, Ha Noi, Vietnam4Deezer Research, France
5Department of Electrical and Computer Engineering, Rice Univer-
sity, Houston, USA6Department of Statistics and Data Sciences,
The University of Texas at Austin, USA. Correspondence to: Tan
Nguyen<tanmnguyen89@ucla.edu >.
Proceedings of the 39thInternational Conference on Machine
Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).1. Introduction
Transformers (Vaswani et al., 2017) have become the state-
of-the-art model for sequence processing tasks, solving
many challenging problems in natural language processing
and computer vision (Al-Rfou et al., 2019; Dai et al., 2019;
Williams et al., 2018; Devlin et al., 2018; Brown & et al.,
2020; Howard & Ruder, 2018; Rajpurkar et al., 2016; De-
hghani et al., 2018; So et al., 2019; Dosovitskiy et al., 2020;
Touvron et al., 2020; Nguyen et al., 2021a). These models
can also transfer the learned knowledge from a pre-trained
model to tasks that involve different data modalities and
have limited supervision (Radford et al., 2018; 2019; Devlin
et al., 2018; Yang et al., 2019; Liu et al., 2019). The success
of transformers is rooted in the self-attention mechanism as
their fundamental building blocks for modeling (Cho et al.,
2014; Parikh et al., 2016; Lin et al., 2017). For each token,
self-attention computes a weighted average of the feature
representations of other tokens where the weight is propor-
tional to a similarity score between each pair of tokens. This
mechanism allows a token to pay attention to other tokens
in the sequence and attain a contextual representation (Bah-
danau et al., 2014; Vaswani et al., 2017; Kim et al., 2017).
It has been shown that the representation capacity of the
attention mechanism (Tenney et al., 2019) and its capability
of capturing diverse syntactic and semantic relationships
(Tenney et al., 2019; Vig & Belinkov, 2019; Clark et al.,
2019; V oita et al., 2019a; Hewitt & Liang, 2019) is key to
the impressive performance of transformers in practice.
1.1. Self-Attention
For a given input sequence X:= [x1;;xN]>2
RNDxofNfeature vectors, self-attention transforms X
into the output sequence Hin the following two steps:
Step 1. The input sequence Xis projected into the query
matrix Q, the key matrix K, and the value matrix Vvia
three linear transformations
Q=XW>
Q;K=XW>
K;V=XW>
V;
whereWQ;WK2RDDx, andWV2RDvDxare the
weight matrices. We denote Q:= [q1;;qN]>;K:=
[k1;;kN]>, andV:= [v1;;vN]>, where the vec-
torsqi;ki;vifori= 1;;Nare the query, key, and valuearXiv:2110.08678v2  [cs.LG]  13 Jun 2022

--- PAGE 2 ---
Improving Transformers with Probabilistic Attention Keys
vectors, respectively.
Step 2. The output sequence H:= [h1;;hN]>is then
computed as follows
H= softmax
QK>=p
D
V:=AV; (1)
where the softmax function is applied to each row of the
matrix (QK>)=p
D. For each query vector qifori=
1;;N, an equivalent form of Eqn. (1) to compute the
output vector hiis given by
hi=NX
j=1softmax
q>
ikj=p
D
vj:=NX
j=1aijvj:(2)
The matrix A2RNNand its component aijfori; j=
1;;Nare the attention matrix and attention scores, re-
spectively. The self-attention computed by Eqn. (1) and (2)
is called the scaled dot-product attention or softmax atten-
tion. In our paper, we call a transformer that uses this
attention the softmax transformer. The structure that the at-
tention matrix Alearns from training determines the ability
of the self-attention to capture contextual representation for
each token.
Multi-head Attention Each output sequence Hforms an
attention head. In multi-head attention, multiple heads are
concatenated to compute the Ô¨Ånal output. Let Hbe the
number of heads and WO2RHDvHDvbe the projection
matrix for the output. The multi-head attention is deÔ¨Åned as
MultiHead (fQ;K;VgH
i=1) =Concat (H1;:::;HH)WO:
Even though multi-head attention extends single-head atten-
tion to capture diverse attention patterns and improve the
performance of transformers, it has been shown that trans-
formers for practical tasks including sequence classiÔ¨Åcation
and language modeling learn redundant heads (Michel et al.,
2019). These redundant heads compute similar attention
mappings. Having many of them in the model limits the
representation capacity of the transformer while wasting
parameters, memory and computation, impeding the appli-
cation of transformers to many important large-scale tasks.
1.2. Contribution
We establish the correspondence between self-attention in
transformer and a Gaussian mixture model (GMM) and
propose Transformer with a Mixture of Gaussian Keys
(Transformer-MGK), a novel class of transformers that can
avoid the head redundancy. At the core of Transformer-
MGK is replacing the attention key kjin each head by a
GMM to allow the query qi, as well as its associated token,
to attend to more diverse positions in the input sequence,
thereby increasing the representation of each attention head
and reducing the chance of learning redundant heads. Our
contribution is four-fold:1.We construct a GMM and show that attention scores
in self-attention match posterior distribution in our
model, providing a probabilistic framework to study
self-attention in transformers.
2.Under our probabilistic framework for self-attention, we
introduce an additional mixture of Gaussian to model
each attention key. We empirically show that this mix-
ture of Gaussian keys (MGK) can capture a diversity of
attention patterns, thus alleviating head redundancy.
3.We extend our MGK to use with linear attentions and
propose the mixture of linear keys (MLK) for efÔ¨Åcient
computation and better memory footprint.
4.We empirically show that Transformer-MGK and
Transformer-MLK are comparable or better than the
corresponding baseline transformers with softmax and
linear attentions while only using half the number of
attention heads and reducing both model complexity
measured by the number of parameters and computa-
tional cost in terms of FLOPs.
Organization: We structure this paper as follows: In Sec-
tion 2, we establish the connection between GMM and
self-attention and then present our Transformer-MGK and
its extensions including Transformer-MLK. In Section 3,
we validate and empirically analyze the efÔ¨Åciency and accu-
racy of Transformer-MGK/MLK. We discuss related works
in Section 4. The paper ends up with concluding remarks.
More experimental details are provided in the Appendix.
2. Transformer with a Mixture of Gaussian
Keys
2.1. Attention Score as a Posterior Distribution
We Ô¨Årst consider a query qi2Qand a key kj2K. Lett
be aN-dimensional binary random variable having a 1-of-
Nrepresentation in which a particular element tjis equal to
1 and all other elements are equal to 0. We use tjto indicate
the positionjof the key kj. LetIbe the identity matrix, we
model the distribution p(qi)by the following GMM:
p(qi) =NX
j=1jp(qijtj= 1) =NX
j=1jN(qijkj;2
jI);(3)
wherejis the priorp(tj= 1) . Given the query qi, how
likely qimatches the key kjis given by posterior p(tj=
1jqi). This posterior is computed as follows
p(tj= 1jqi) =jN(qijkj;2
j)P
j0j0N(qijkj0;2
j0)
=jexp
  
kqik2+kkjk2
=22
j
exp 
qik>
j=2
j
P
j0j0exph
 (kqik2+kkj0k2)=22
j0i
exp
qik>
j0=2
j0:

--- PAGE 3 ---
Improving Transformers with Probabilistic Attention Keys
We further assume that the query qiand the key kjare
normalized, and the prior jis uniform. We will justify
these assumptions in our Remarks at the end of this section.
We also let2
j=2,j= 1;2;:::;K . Then the posterior
p(tj= 1jqi)can be written as
p(tj= 1jqi) = exp 
qik>
j=2
=X
j0exp 
qik>
j0=2
:(4)
The right-hand side of Eqn. (4) matches the attention score
given in Eqn. (2) when 2=p
D. Thus, we show that under
right assumptions, the attention score between the query qi
and the key kjin an attention unit of a transformer is the
posteriorp(tj= 1jqi), which indicates the responsibility
that the key kjtakes for ‚Äòexplaining‚Äô the query qi, which
in turn decide, for example, how much a token at position i
pays attention to a token at position jin the input sequence.
Remark 1. The assumption that the query qiand the key
kjare normalized is realistic and not artiÔ¨Åcial. In many
applications, those two vectors are normalized. (Schlag
et al., 2021) points out that such normalization is to avoid
instability occurring during the training.
Remark 2. In practice, the prior is chosen to be uniform
when there is no prior knowledge available.
2.2. Transformer with a Mixture of Gaussian Keys:
Each Key is Again a Gaussian Mixture Model
As we have seen from Eqn. (4), the key kjis used to explain
the queryqivia the posterior P(tj= 1jqi). Via this simple
connection, each query qiis treated to be as a sample from
the mixture of NkeysPN
j=1jN(qijkj;2
jId). However,
the assumption that the distribution P(qijtj= 1) at each
subpopulation is Gaussian in Eqn. (3) can be quite strong
as there is no guarantee that this assumption is valid in
practice. In particular, it may happen that the distribution
of each subpopulation is asymmetric or skewed or even
multimodal. Therefore, using the Gaussian distribution for
each subpopulation can potentially limit the explanation
power and diversity of each subpopulation/ key. It indicates
that we should use more expressive distributions to represent
P(qijtj= 1) .
Mixture of Gaussian keys: To improve the explanation
power of each key kj, potentially increase the representation
of each attention head, and reduce the chance of learning re-
dundant heads, we would like to model it as mixture of Gaus-
sian distributions. We refer to this model as Transformer
with a Mixture of Gaussian Keys (Transformer-MGK). In
particular, in Transformer-MGK we model each key kj
at positionjas a mixture of MGaussiansN(kjr;2
jrI),
r= 1;2;:::;M . Here we are overloading the notation a
little bit and use kjrand2
jrIto denote the mean and co-
variance matrix of the rthGaussian at position j. Letzbe
aM-dimensional binary random variable having a 1-of- Mrepresentation. We use zrto indicate the rthGaussian in
the mixture. Let jrP(zr= 1jtj= 1) , our MGK can
be written as
P(qijtj= 1) =X
rP(zr= 1jtj= 1)P(qijzr= 1;tj= 1)
=X
rjrN(qijkjr;2
jrI): (5)
Our motivation of using mixture of Gaussian distributions to
represent the distribution of each subpopulation in Eqn. (5)
stems from the following important approximation result:
Theorem 1. Assume that Pis probability distribution on
[ a;a]dfor somea > 0and admits density function p
such thatpis differentiable and bounded. Then, for any
given variance  > 0and for any  > 0, there exists a
mixture ofKcomponentsPK
i=1iN(i;2I)whereK
(Clog(1=))dfor some universal constant Csuch that
sup
x2Rdjp(x) KX
i=1i(xji;2I)j;
where(xj;2I)is the density function of multivariate
Gaussian distribution with mean and covariance matrix
2I.
The proof of Theorem 1 is in Appendix C. The result of
Theorem 1 suggests that regardless of the real form of
P(qijtj= 1) , we can use Ô¨Ånite mixture of Gaussian distri-
butions to approximate P(qijtj= 1) . It allows us to have
richer approximation of P(qijtj= 1) than by using a simple
Gaussian distribution in Eqn. (3). Similar to the derivation
above, the posterior p(tj= 1jqi)in Transformer-MGK can
be written as
P(tj= 1jqi) =P
rjrexp 
qik>
jr=2
jr
P
j0P
rj0rexp
qik>
j0r=2
j0r:(6)
Furthermore, in Transformer-MGK, we relax the assump-
tion that the queries and keys are normalized. Thus, when
computing P(tj= 1jqi), we compute the Gaussian kernels
between the queries and keys instead of their dot products.
The posterior P(tj= 1jqi)in Transformer-MGK is then
given by
P(tj= 1jqi) =P
rjrexp 
 kqi kjrk2=22
jr
P
j0P
rj0rexp
 kqi kj0rk2=22
j0r:
(7)
As proven in Section 2.1, this posterior corresponds to the
attention score. Thus, Eqn. (7) is the formula for computing
the attention score in Transformer-MGK. We compute the
output vector hiof the self-attention in Transformer-MGK

--- PAGE 4 ---
Improving Transformers with Probabilistic Attention Keys
as follows
hi=X
j0
@P
rjrexp 
 kqi kjrk2=22
jr
P
j0P
rj0rexp
 kqi kj0rk2=22
j0r1
Avj:
2.3. Inference and Learning via the Expectation
Maximization Algorithm
LetirP(zr= 1jqi;tj= 1) , in MGK, we apply the
E-step inference in the Expectation-Maximization (EM) al-
gorithm to estimate this posterior given the query qi. The
posterioriris also known as the responsibility that the
componentN(kjr;2
jrI)takes to account for the observa-
tion, which in MGK is the query qi. Below we propose two
approaches to estimate this responsibility.
Soft E-step Using soft E-step inference, the EM algorithm
makes a soft assignment, in which each query is associated
with all clusters. The responsibilities are then given by
ir=jrexp 
 kqi kjrk2=22
jr
P
r0jr0exp
 kqi kjr0k2=22
jr0:(8)
At learning time, the responsibilities estimated by Eqn. (8)
are used to update the prior jr, i.e.jr=Njr=N, where
Nis the number of queries and Njr=PN
i=1ir. These
updated priors jrare then used in Eqn. (7) to compute
attention scores.
Hard E-step Hard E-step performs a hard assignment of
queries to key clusters, in which each query is associated
uniquely with one cluster. This is similar to the K-means
algorithm (Lloyd, 1982) and corresponds to the MGK at the
limit when the variance parameter 2
jrgoes to 0. Following
the derivation of K-means from a GMM in (Bishop, 2006),
Eqn. (7) becomes
P(tj= 1jqi) =maxrexp 
 kqi kjrk2=22
jr
P
j0maxrexp
 kqi kj0rk2=22
j0r:
Remark 3. The hard E-step inference allows the attention
score to be computed more efÔ¨Åciently because the priors jr
no longer play an active role in the algorithm and can be
completely ignored.
Learning via Stochastic Gradient Descent (SGD) In or-
der to increase the efÔ¨Åciency of the model, in MGK, we Ô¨Åx
the variance parameter 2
jrto bep
Das in the standard soft-
max attention and make the cluster means, i.e. the keys, kjr
learnable parameters. We also make the prior jrlearnable
parameters as one of the design options. In that case, both
kjrandjrare learned via SGD. This update via SGD can
be considered as a generalized M-step (Bishop, 2006).
Design Options for Keys (Option A) We follow the stan-
dard setting in the softmax transformer and make the keyskjra linear projection of the input xj, i.e.kjr=xjW>
Kr,
where xj2R1Dx,WKr2RDDxandr= 1;2;:::;M .
(Option B) Alternatively, we also make the keys kjrshifted
version of each other to save computation, i.e. kjr=
xjW>
K+br, where WK2RDDx.
2.4. Transformer with a Mixture of Linear Keys
The MGK can be easily extended to use with linear atten-
tions. We call that model Transformer with a Mixture of
Linear Keys (Transformer-MLK). In this section, we adopt
the formulation of linear attentions from (Katharopoulos
et al., 2020) to derive Transformer-MLK. Similar approach
can be taken to derive Transformer-MLK when using with
other linear attentions such as those in performers (Choro-
manski et al., 2021) and fast-weight transformers (Schlag
et al., 2021). In Transformer-MLK, the Gaussian kernel
in softmax attention is linearized as the product of feature
maps()on the vectors qiandkj. The associative prop-
erty of matrix multiplication is then utilized to derive the
following efÔ¨Åcient computation of the attention map
hi=(qi)>P
jP
rjr(kjr)v>
j
(qi)>P
jP
rjr(kjr):
ReplacingP
jP
rjr(qi)>(kjr)vj with
(qi)>P
jP
rjr(kjr)v>
j, as in linear transform-
ers, reduces the memory and computational cost of
computing the attention map in Transformer-MLK from
O(N2)toO(N), making Transformer-MLK scalable to
very long sequences.
2.5. Reduction in Model Complexity and
Computational Cost from Transformer-MGK
We provide a detailed analysis of reduction in model com-
plexity and computational cost of Transformer-MGK over
the basline softmax attention in Appendix B.
3. Experimental Results
In this section, we numerically justify the efÔ¨Åciency of
Transformer-MGK/MLK and empirically study the advan-
tage of using mixture of keys on various benchmarks, in-
cluding different tasks in the Long Range Arena (LRA) (Tay
et al., 2021) (Section 3.1) and language modeling on
Wikitext-103 (Merity et al., 2017) (Section 3.2). We aim
to show that: (i) Transformer-MGK/MLK with half the
number of heads is comparable or better than the baseline
softmax and linear transformers with the full number of
heads while being more efÔ¨Åcient in both computational cost
and memory footprints; (ii) Mixture of keys helps reduce the
redundancy in multi-head transformers and beneÔ¨Åts learning
of the long-term dependency in long input sequences; (iii)
Using the same number of heads, Transformer-MGK/MLK

--- PAGE 5 ---
Improving Transformers with Probabilistic Attention Keys
Table 1. Test Accuracy (%) of Transformer-MGK compared with
the baseline softmax transformer on the LRA benchmark. Our
Transform-MGKs outperform softmax transformers while using
half the number of heads, having less parameters, and requiring
less FLOPs (see Figure 4). Results are averaged over 5 runs.
Model ListOps Text Retrieval Average
Softmax 12 heads 36.64 65.62 82.18 61.48
Softmax 8 heads 37.03 65.71 81.74 61.49
sMGK 4 heads 37.25 65.51 82.79 61.85
MGK 4 heads 36.98 65.69 82.23 61.63
Softmax 4 heads 36.89 65.26 81.54 61.23
sMGK 2 heads 37.35 65.17 82.20 61.57
MGK 2 heads 36.88 65.37 81.83 61.36
Softmax 2 heads 36.76 64.90 79.1 60.25
sMGK 1 head 37.31 65.04 81.23 61.19
MGK 1 head 37.13 65.40 80.63 61.05
Softmax 1 head 36.81 64.48 77.9 59.73
signiÔ¨Åcantly outperforms the baseline softmax and linear
transformers. Especially in the case of Transformer-MLK,
it helps reduce the performance gap between softmax and
linear transformers.
Throughout this section, we compare Transformer-
MGK/MLK with the softmax and linear transformers that
have the same or double the number of attention heads. In
all experiments, for our Transformer-MGK/MLK models,
we set M=2 where M is the number of Gaussians, i.e. keys,
at each timestep. Among the design options for Transformer-
MGK mentioned in Section 2.3, we use the one with Soft-E
step but make the parameter jrandkjrlearnable and Ô¨Åx
the variance 2
jrto be constants. We study both implemen-
tations for keys: (A) kjris a linear projection of the input
xj, i.e.,kjr=xjW>
Krand (B) kjrare shifted version of
each other, i.e., kjr=xjW>
K+br.
In this section, we refer to the Transformer-MGK/MLK
whose keys are implemented by (A) as Transformer-
MGK/MLK, and whose keys are implemented by (B)
as Transformer-sMGK/sMLK. We empirically compare
these models with other design options for Transformer-
MGK in Section 3.4. Details on datasets, models,
and training are provided in Appendix A.1. Our
PyTorch (Paszke et al., 2019) code are available
at https://github.com/minhtannguyen/transformer-mgk.
3.1. Long Range Arena (LRA) Benchmark
Models and baselines We compare our 1-head, 2-head,
4-head Transformer-MGK and MLK with the baseline
softmax (Vaswani et al., 2017) and linear transform-
ers (Katharopoulos et al., 2020) that have 1 head, 2 heads, 4
heads, and 8 heads. Each model consists of two layers, and
we adopt the model and training setting from (Xiong et al.,
2021) in our experiments.
Results We summarize our results in Table 1. Transformer-Table 2. Test Accuracy (%) of Transformer-MLK compared with
the linear transformer on the LRA. Our Transform-MLKs achieve
comparable/better accuracy than the baselines while using half
the number of heads, having less parameters, and requiring less
FLOPs (see Figure 4 for details). Results are averaged over 5 runs.
Model ListOps Text Retrieval Average
Linear 12 heads 20.26 65.87 81.97 56.03
Linear 8 heads 19.17 65.85 81.18 55.40
sMLK 4 heads 20.11 65.74 81.53 55.79
MLK 4 heads 20.06 65.7 81.34 55.7
Linear 4 heads 19.37 65.81 81.65 55.61
sMLK 2 heads 19.88 65.61 81.66 55.71
MLK 2 heads 20.12 65.72 80.80 55.54
Linear 2 heads 18.35 65.94 80.94 55.07
sMLK 1 head 18.87 65.57 80.37 54.93
MLK 1 head 18.34 65.70 81.09 55.04
Linear 1 head 18.60 65.70 80.6 54.96
MGKs with half the number of heads consistently achieve
better test accuracy than the baseline softmax attention
across tasks. Since fewer heads are needed, transformer-
MGKs use less parameters and need less FLOPs to compute
than the baselines. We provide a detailed efÔ¨Åciency analy-
sis for Transformer-MGKs in Figure 4. More interestingly,
these efÔ¨Åciency advantages of Transformer-MGK over the
baseline become more signiÔ¨Åcant as the number of heads
in the baseline model grows. When using the same num-
ber of heads as the baseline models, Transformer-MGKs
further improve over those baselines. Among the models,
Transformer-sMGK performs the best across LRA tasks.
We also compare the performance of Transformer-MLK
with the baseline linear transformers in Table 2. Like
Transformer-MGK, Transformer-MLK yields comparable
or better results than the baseline using only half the number
of heads with less parameters and FLOPs. When using the
same number of heads, Transformer-MLK helps improve
the linear transformer further.
We provide results of the 12-head baselines in Table 1
and 2 for reference. It is interesting to notice from Table 1
and 2 that even our 2-head Transformer-MGK/MLK mod-
els achieve better or equivalent results to the 12-head and
8-head baselines. A comparison between the 12-head base-
lines with our 6-head Transformer-MGK/MLK models on
the retrieval task is provided in Table 9 in Appendix A.10.
In Figure 1, we compare the training loss and test accuracy
curves of our 1-head and 2-head Transformer-MGK/MLK
with the 2-head softmax and 2-head linear transformers
on the document retrieval task. This retrieval task has the
longest average sequence-length and attention span among
the LRA tasks (Tay et al., 2021). On this task, as shown
in Figure 1, our Transformer-MGKs/MLKs are always bet-
ter than the baseline models throughout the training. This
observation corroborates our models‚Äôs capability of captur-

--- PAGE 6 ---
Improving Transformers with Probabilistic Attention Keys
Figure 1. Training loss and test accuracy of Transformer-MGK/MLK vs. softmax/linear transformer on the retrieval task, which has the
longest average sequence-length and attention span among the LRA tasks (Tay et al., 2021). The impressive performance of Transformer-
MGK/MLK on this challenging task validates the capability of our models to capture long-range dependencies via learning a diversity of
attention patterns.
ing long-range dependencies in very long input sequences.
3.2. Language Modeling on WikiText-103
Next we conÔ¨Årm the advantage of our models on a large-
scale application. We consider the word-level language
modeling task on WikiText-103 (Merity et al., 2017) for our
experiments in this section.
Models and baselines We compare 4 and 8-head
Transformer-MGKs/MLKs with 8-head softmax (Vaswani
et al., 2017) and linear transformers (Katharopoulos et al.,
2020). Each model consists of 16 layers. Our experiments
follow the setting for small/medium models from (Schlag
et al., 2021).
Results As shown in Table 3, our Transformer-MGKs out-
perform the baseline softmax transformers. Even when
using half the number of attention heads (i.e., 4 vs. 8 heads
as in the baselines), the Transformer-MGK still achieves bet-
ter test perplexities (PPL) than the baseline. Adding more
heads into Transformer-MGKs improves their performance.
Similarly, Transformer-MLKs attain comparable test/valida-
tion PPL to the baseline linear transformers when using half
the number of attention heads. When using the same num-
ber of attention heads as in the baseline, Transformer-MLKs
consistently achieve better performance.Note that reducing
the number of heads from 8 to 4 in the baseline models sig-
niÔ¨Åcantly decreases their performance with more than 1.5
reduction in test/validation PPL for the softmax transformer
and more than 1.0 reduction in test/validation PPL for the
linear transformer. Our proposed Transformer-MGK and
Transformer-MLK helps close this gap.
To further examine the scalability of our models, we ap-
ply the MGK on a stronger baseline, which is the 8-head
medium softmax transformer in (Schlag et al., 2021). This
model has 90M parameters, 16 layers, 8 attention heads
per layer, and hidden size of 256. The size of our baseline
model is close to BERT Base(Devlin et al., 2019), which has
110M parameters, 12 layers, 12 attention heads per layer,
and hidden size of 768. Applying our MGK on top of this
baseline and using only 4 heads instead of 8, we signiÔ¨Åcantly
improve the test PPL from 29.60 to 28.86 while reducingTable 3. Perplexity (PPL) on WikiText-103 of Transformer-MGK
and MLK compared to the baselines. Both Transformer-MGK and
MLK achieve comparable or better PPL than the baselines while
using only half the number of heads. When using the same number
of heads, our models signiÔ¨Åcantly improve the baselines.
Method Valid PPL Test PPL
Softmax 8 heads (small) 33.15 34.29
MGK 4 heads (small) 33.28 34.21
sMGK 8 heads (small) 32.92 33.99
MGK 8 heads (small) 32.74 33.93
Softmax 4 heads (small) 34.80 35.85
Linear 8 heads (small) 38.07 39.08
MLK 4 heads (small) 38.49 39.46
MLK 8 heads (small) 37.78 38.99
Linear 4 heads (small) 39.32 40.17
Softmax 8 heads (medium) 27.90 29.60
MGK 4 heads (medium) 27.58 28.86
the model size and computational cost, demonstrating the
advantages of our scaled models.
3.3. Neural Machine Translation on IWSLT‚Äô14
German to English and WMT‚Äô14
We further examine the advantages of our methods on the
IWSLT‚Äô14 German-English machine translation task (Cet-
tolo et al., 2014). Table 4 shows that the 2-head Transformer-
MGK and sMGK models achieve the BLEU score of 34.34
and 34.69, respectively, which are comparable to and bet-
ter than the BLUE score of 34.42 of the 4-head softmax
transformer baseline.
We also compare the 8-head Transformer-MGK/sMGK with
the 16-head softmax baseline on the WMT‚Äô14 task in Ta-
ble 4. This baseline consists 12 layers with 201M trainable
parameters. Transformer-MGK/sMGK obtain the BLEU
score of 29.03 and 29.07, respectively, which are compara-
ble with the baseline BLEU score of 29.38.
3.4. Empirical Analysis
We conduct empirical analysis based on the Transformer-
MGK trained for the document retrieval task the Language
modeling task on WikiText-103, and the IWSLT14 De-En

--- PAGE 7 ---
Improving Transformers with Probabilistic Attention Keys
Table 4. Machine translation BLEU scores of 2-head Transformer-
MGKs on the IWSLT14 De-En task are better than or equivalent to
that of the 4-head baseline. Similarly, the BLEU scores of 8-head
Transformer-MGKs on the WMT‚Äô14 task are comparable to that
of the 16-head baseline.
Method Task BLEU score
Softmax 4 heads IWSLT‚Äô14 34.42
Transformer sMGK 2 head IWSLT‚Äô14 34.69
Transformer MGK 2 head IWSLT‚Äô14 34.34
Softmax 16 heads WMT‚Äô14 29.38
Transformer sMGK 8 head WMT‚Äô14 29.07
Transformer MGK 8 head WMT‚Äô14 29.03
machine translation task. Results for Transformer-MLKs
and other other results for WikiText-103 task are provided
in the Appendix.
The ability of MGK to approximate the probability
distribution of queries We compute the negative log-
likelihood (NLL) score of MGK in comparison with the
baseline that uses only 1 Gaussian distribution centered
around the key kj,j= 1;:::;N . Both models are trained
for the document retrieval task. The NLLs of MGK are
0.180 and 0.182, which are smaller than the baseline‚Äôs NLLs
of 0.187 and 0.292, in layer 1 and 2, respectively, indicating
that MGK Ô¨Åts the distribution of the attention queries better.
Ablation study on the variances of the MGK In our ex-
periments, we set M=2 and 2
j1=p
Das in the baseline
softmax transformer and 2
j2=3p
DwhereDis the feature
dimension deÔ¨Åned in Section 1.1. We conduct an ablation
study on the IWSLT14 De-En machine translation task in
which we vary 2
j2. We observe that the BLEU score of the
trained models do not differ much between different values
of2
j2. In particular, BLEU scores are 34:31,34:36,34:61,
34:34, and 34:27for2
j2= 0:5p
D;p
D;2p
D;3p
Dand
4p
D, respectively. We also make 2
j1and2
j2learnable
and observe that the resultant BLEU score is worse ( 32:86).
Transformer-MGK helps avoid learning redundant
heads We visually compare attention matrices learned by
Transformer-MGKs and the baseline softmax transformer
on the document retrieval task in Figure 2. In particular, we
randomly select an attention matrix at each head in each
layer and visualize that attention matrix for each model
in comparison. Figure 2(Left) shows that the queries in
Transformer-MGKs can attend to a variety of keys and
equivalently to other tokens at different positions in the
input sequence. This diversity in attention pattern helps re-
duce the chance that the model learns similar and redundant
attention matrices at different heads signiÔ¨Åcantly.
Another metric to measure the representation capacity of
an attention matrix is its rank. Attention matrices with high
ranks can capture more diverse attention patterns compared
to those with low ranks (Nguyen et al., 2021b). We studyTable 5. Performance of Transformer-MGK using different infer-
ence/learning techniques on LRA benchmark.
Model ListOps Text Retrieval Average
sMGK + Hard-E 1 head 37.25 64.7 81.29 61.08
sMGK + Soft-E 1 head 37.05 64.68 81.44 61.05
sMGK 1 head 37.31 65.04 81.23 61.19
MGK + Hard-E 1 head 19.40 65.40 80.72 55.17
MGK + Soft-E 1 head 33.85 65.25 80.73 59.94
MGK 1 head 37.13 65.40 80.63 61.05
the rank of the attention matrix from the Transformer-MGK
and the softmax transformer trained for the retrieval task.
In particular, we randomly select 1000 different attention
matrices at each layer from each model. Then, we perform
singular value decomposition (SVD) to compute the rank of
each matrix and threshold the singular values smaller than
10 6. Figure 2(Right) presents the distribution of the rank
of attention matrices at each layer of the Transformer-MGK
and the softmax transformer. We observe that attention
matrices in Transformer-MGK has higher rank than those
in the softmax transformer. Thus, our attention with MGK
is capable of capturing more diverse and complex attention
patterns than the baseline softmax attention.
Comparing different inference and learning techniques
Table 5 compares the performance of Transformer-MGKs
using different design options mentioned in Section 2.3 on
the LRA benchmark. In particular, we consider the fol-
lowing three design options: A) Soft-E step, parameters
jrandkjrare learnable via SGD, and variance 2
jrare
constants, B) Soft-E step, parameter jris updated accord-
ing to the M-step update, kjrare learnable via SGD, and
variance2
jrare constants, and C) Hard-E step, jrand
kjrare learnable via SGD, and variance 2
jrare constants.
Note that Transformer-MGKs with setting A are the de-
fault models we use in all experiments above. In Table 5,
Transformer-MGK + Hard-E is the Transformer-MGK with
setting C, Transformer-MGK + Soft-E is the Transformer-
MGK with setting B, and Transformer-MGK only is the
Transformer-MGK with setting A. It is worth noting that
Transformer-sMGK + Hard-E obtains comparable results
to the models with the best performance in each task even
though it is the most efÔ¨Åcient model in our study.
Transformer-MGK reduces model complexity and com-
putational cost Figure. 3A and 3B present the reduction
ratio of train and test FLOPS, respectively, of our 4-head
Transformer-MGK vs. the 8-head Softmax baseline as
functions of model dimension Dand sequence length N.
In Fig. 3C, we show the reduction ratio of model size of
Transformer-MGK vs. the baseline as Dvaries. We observe
that the efÔ¨Åciency advantage of Transformer-MGK over the
baseline grows with DandN, making it more suitable and
superior for large-scale applications. Note that the model
size in term of the number of parameters does not depend

--- PAGE 8 ---
Improving Transformers with Probabilistic Attention Keys
Count
Layer 1Layer 2RankSoftmax 4 HeadsMGK 4 HeadsMGK 2 Heads
Figure 2. (Left) Visualization of attention matrices in the baseline 4-head softmax transformer (left), 4-head Transformer-MGK (middle),
and 2-head Transformer-MGK (right) trained on the document retrieval task. Attention matrices from our Transformer-MGKs have more
diverse pattern than those from the baseline softmax transformer, reducing the risk of learning redundant heads. (Right) Rank distribution
of attention matrices shows that attention matrices in Transformer-MGK have higher rank than those in the softmax transformer and thus
can capture more diverse attention patterns.
Sequence Length Model Dimension Model Dimension 2565121024 2048 4096 
Training Testing FLOPS Ratio 
Total parameters 
Non-embedding 
parameters 
Parameters Ratio 
Ôøº 
Ôøº 
Ôøº 
A B C
Figure 3. Training (A), Inference (B) FLOP ratios and number-of-
parameter ratio (C) between 4-head Transformer-MGK with the
8-head softmax baseline across different Dmodel dimensions and
Nsequence lengths, for the language modeling task on WikiText-
103. 4-head Transformer-MGK is signiÔ¨Åcantly efÔ¨Åcient, both in
computation and model complexity, than the baseline as DandN
increases, indicating the beneÔ¨Åts of our method for long-range and
large-scale tasks.
Figure 4. Computational cost (FLOPs) and the number of param-
eters of Transformer-MGK vs the baseline softmax transformer
(Left) and Transformer-MLK vs. the baseline linear transformer
(Right) trained on the Retrieval task. The efÔ¨Åciency advantage of
Transformer-MGK/MLK over the baselines in both metrics grows
with the number of head.
on the sequence length N. The efÔ¨Åciency analysis results on
this language modeling task for Transformer-MLK and the
analysis metrics are provided in Fig. 10 in Appendix A.5.
Also, Figure 4 compares the computational cost, measuredin FLOPS, and model complexity, measured in the number
of parameters, between our Transformer-MGK/MLK that
has half the number of heads and the full-head softmax/lin-
ear transformer as functions of the number of heads. The
models are trained for the Retrieval task in the LRA bench-
mark. The more heads being used, the more advantage
Transformer-MGK/MLK has over the softmax/linear trans-
former. For much larger transformer models, this saving is
signiÔ¨Åcant.
4. Related Work
EfÔ¨Åcient Transformers EfÔ¨Åcient transformers can be clas-
siÔ¨Åed into several categories, as summarized in (Roy et al.,
2021). Among these categories are models which design the
attention matrix to have sparse structure (Parmar et al., 2018;
Liu et al., 2018; Qiu et al., 2019; Child et al., 2019; Beltagy
et al., 2020; Du et al., 2021). Another category includes
models that combine two or more different access patterns
to improve the coverage (Child et al., 2019; Ho et al., 2019).
Access patterns can also be made learnable in a data-driven
fashion (Kitaev et al., 2020; Roy et al., 2021; Tay et al.,
2020). Other efÔ¨Åcient transformers take advantage of a
side memory module to access multiple tokens at once (Lee
et al., 2019; Sukhbaatar et al., 2019; Asai & Choi, 2020;
Beltagy et al., 2020). Inspired by the use of momentum
methods in speeding up neural network training (Sutskever
et al., 2013; Bengio et al., 2013; Kingma & Ba, 2015; Wang
et al., 2022) and designing neural network architectures (Li
et al., 2018; He et al., 2020; Nguyen et al., 2020; Xia et al.,
2021; Sander et al., 2021), (Wang et al., 2021a) incorporates
adaptive momentum into the linear transformers to improve
its accuracy while maintaining the linear computational and
memory complexity. Furthermore, dynamic inference pre-
viously proposed for deep neural networks (Wang et al.,
2018a;b; 2020b; Han et al., 2021) can also be used to ac-
celerate the inference in transformers (Bakhtiarnia et al.,

--- PAGE 9 ---
Improving Transformers with Probabilistic Attention Keys
2021; Wang et al., 2021b). Low-rank and kernelization ap-
proximation are also utilized to enhance the memory and
computational efÔ¨Åciency of computing self-attention (Tsai
et al., 2019; Wang et al., 2020a; Katharopoulos et al., 2020;
Choromanski et al., 2021; Shen et al., 2021; Nguyen et al.,
2021b; Peng et al., 2021). In addition to the aforementioned
efÔ¨Åcient transformers, multi-query attention that shares keys
and values between different attention heads (Shazeer, 2019)
has also been studied to reduce the memory-bandwidth cost
and increase the speed for incremental transformer infer-
ence (see Appendix A.11). It is worth noting that efÔ¨Åcient
retrieval methods have been employed to increase the ef-
Ô¨Åciency of machine learning models including transform-
ers (Chen et al., 2020; Le & Lauw, 2020; 2021; Mittal
et al., 2022). Last but not least, methods such as using
auxiliary losses (Al-Rfou et al., 2019) and adaptive input
embedding (Baevski & Auli, 2019) have been explored to
speed up the convergence of training transformers. Our
MGK/MLK can be easily incorporated into these methods
above to further improve their accuracy and efÔ¨Åciency.
Redundancy in Transformers Latest works suggest that
most of the neurons and heads in the pre-trained trans-
former are redundant and can be removed when optimzing
towards a downstream task (Dalvi et al., 2020; Michel et al.,
2019; Durrani et al., 2020). Other works also study the
contextualized embeddings in pretrained networks under
this redundancy due to overparameterization and show that
the representations learned within these models are highly
anisotropic (Mu & Viswanath, 2018; Ethayarajh, 2019). An
emerging body of work is proposed to distill and prune the
model, including (Sanh et al., 2019; Sun et al., 2019; V oita
et al., 2019b; Sajjad et al., 2020). Our MGK/MLK approch
can be combined with these distilling and pruning methods.
Mixture Models for Transformers Several works have
used mixture models to study and enhance transformers.
Switch transformers (Fedus et al., 2021) employ the rout-
ing algorithm in Mixture of Experts (MoE) to reduce the
communication and computational costs in transformers.
(Nguyen et al., 2018; Patel et al., 2016; 2015) derive a
probablistic framework based on GMMs for deep neural
networks that can be extended to study transformers and
attention-based architectures. (Zhang & Feng, 2021) devel-
ops a Gaussian mixture attention that models each attention
score as a GMM. In contrast, the MGK models each key
kjat timestepjas a GMM. In other words, our MGK is
a generative model approach in which we build a gener-
ative model for the key kjand derive the attention score
from the posterior distribution while the Gaussian mixture
attention in (Zhang & Feng, 2021) is a discriminative ap-
proach (Bishop, 2006). Other works that use mixture models
with deep neural networks and transformers include (Cho
et al., 2020; Guo et al., 2019; Huang et al., 2020).5. Concluding Remarks
In this paper, we proposed Transformer-MGK, a class of
transformers that use Gaussian mixture model to represent
the key vectors in self-attention. Transformer-MGK re-
duces the redundancy among heads in transformer. Fur-
thermore, attention heads in the Transformer-MGK have
better representation capability than those in the baseline,
allowing the Transformer-MGK to achieve comparable or
better performance than the baseline softmax transformer
while using only half of the number of heads. Comparing
to the baseline, the Transformer-MGK uses fewer param-
eters and requires less FLOPs to compute. We extend the
Transformer-MGK into the Transformer-MLK to use linear
attentions for better efÔ¨Åciency. We empirically validate the
advantage of the Transformer-MGK/MLK over the base-
line softmax and linear transformers on various benchmarks
including tasks in the LRA benchmark, WikiText-103 lan-
guage modeling, and IWSLT‚Äô14/WMT‚Äô14 machine transla-
tion. Both Transformer-MGK/MLK and the softmax/linear
transformers assume that features in the attention queries
and keys are independent. (Nguyen et al., 2022) suggests
that the Fourier integral estimators can be used to efÔ¨Åciently
capture the dependence structure between those embedding
features. The Fourier integral estimators can be incorporated
into Transformer-MGK/MLK to increase the representation
power of the models. In our work, we make the means and
the variance of the cluster learnable variables and constants,
respectively. It is interesting to explore how to leverage
the M-step update in the EM algorithm to update those
parameters. Since the EM algorithm can have sub-linear
convergence rate due to the weak separation of the means
and variance (Dwivedi et al., 2020b;a), we may need to
develop more efÔ¨Åcient optimization algorithms than the EM
for estimating the means and variance. The current works
of (Ho et al., 2022; Ren et al., 2022) demonstrate that an ex-
ponential learning rate schedule for the gradient descent can
be utilized for the means and variance to obtain the linear
convergence rate of the parameters. It is of practical impor-
tance to investigate the performance of that algorithm in the
Transformer-MGK/MLK. Finally, we leave the application
of MGK/MLK for improving the vision transformer (Doso-
vitskiy et al., 2020; Touvron et al., 2020) as future work.
Acknowledgements
This material is based on research sponsored by the AFOSR
MURI FA9550-18-1-0502, the ONR grant N00014-20-1-
2093, the MURI N00014-20-1-2787, and the NSF under
Grant# 2030859 to the Computing Research Association
for the CIFellows Project (CIF2020-UCLA-38). NH grate-
fully acknowledges support from the NSF IFML 2019844
award and research gifts by UT Austin ML grant. RGB
was supported by NSF grants CCF-1911094, IIS-1838177,
IIS-1730574, and a Vannevar Bush Faculty Fellowship.

--- PAGE 10 ---
Improving Transformers with Probabilistic Attention Keys
References
Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,
L. Character-level language modeling with deeper self-
attention. In Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence , volume 33, pp. 3159‚Äì3166, 2019.
Asai, A. and Choi, E. Challenges in information seeking qa:
Unanswerable questions and paragraph retrieval. arXiv
preprint arXiv:2010.11915 , 2020.
Bacharoglou, A. G. Approximation of probability distri-
butions by convex mixtures of Gaussian measures. Pro-
ceedings of the American Mathematical Society , 138:
2619‚Äì2628, 2010.
Baevski, A. and Auli, M. Adaptive input representa-
tions for neural language modeling. In International
Conference on Learning Representations , 2019. URL
https://openreview.net/forum?id=ByxZX20qFQ.
Bahdanau, D., Cho, K., and Bengio, Y . Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473 , 2014.
Bakhtiarnia, A., Zhang, Q., and IosiÔ¨Ådis, A. Multi-exit
vision transformer for dynamic inference. arXiv preprint
arXiv:2106.15183 , 2021.
Beltagy, I., Peters, M. E., and Cohan, A. Long-
former: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
Bengio, Y ., Boulanger-Lewandowski, N., and Pascanu, R.
Advances in optimizing recurrent networks. In 2013 IEEE
international conference on acoustics, speech and signal
processing , pp. 8624‚Äì8628. IEEE, 2013.
Bishop, C. M. Pattern recognition. Machine learning , 128
(9), 2006.
Brown, T. and et al. Language models are few-shot learn-
ers. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
M. F., and Lin, H. (eds.), Advances in Neural Information
Processing Systems , volume 33, pp. 1877‚Äì1901, 2020.
Cettolo, M., Niehues, J., St ¬®uker, S., Bentivogli, L., and Fed-
erico, M. Report on the 11th iwslt evaluation campaign,
iwslt 2014. In Proceedings of the International Work-
shop on Spoken Language Translation, Hanoi, Vietnam ,
volume 57, 2014.
Chen, B., Liu, Z., Peng, B., Xu, Z., Li, J. L., Dao, T., Song,
Z., Shrivastava, A., and Re, C. Mongoose: A learnable
lsh framework for efÔ¨Åcient neural network training. In
International Conference on Learning Representations ,
2020.Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.
Cho, K., van Merri ¬®enboer, B., Gulcehre, C., Bahdanau,
D., Bougares, F., Schwenk, H., and Bengio, Y . Learn-
ing phrase representations using RNN encoder‚Äìdecoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pp. 1724‚Äì1734,
Doha, Qatar, October 2014. Association for Computa-
tional Linguistics. doi: 10.3115/v1/D14-1179. URL
https://www.aclweb.org/anthology/D14-1179.
Cho, S. M., Park, E., and Yoo, S. Meantime: Mixture of
attention mechanisms with multi-temporal embeddings
for sequential recommendation. In Fourteenth ACM Con-
ference on Recommender Systems , pp. 515‚Äì520, 2020.
Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song,
X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohi-
uddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and
Weller, A. Rethinking attention with performers. In Inter-
national Conference on Learning Representations , 2021.
URL https://openreview.net/forum?id=Ua6zuk0WRH.
Clark, K., Khandelwal, U., Levy, O., and Manning, C. D.
What does BERT look at? an analysis of BERT‚Äôs at-
tention. In Proceedings of the 2019 ACL Workshop
BlackboxNLP: Analyzing and Interpreting Neural Net-
works for NLP , pp. 276‚Äì286, Florence, Italy, August
2019. Association for Computational Linguistics. doi:
10.18653/v1/W19-4828. URL https://www.aclweb.org/
anthology/W19-4828.
Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and
Salakhutdinov, R. Transformer-xl: Attentive language
models beyond a Ô¨Åxed-length context. arXiv preprint
arXiv:1901.02860 , 2019.
Dalvi, F., Sajjad, H., Durrani, N., and Belinkov, Y . Analyz-
ing redundancy in pretrained transformer models. arXiv
preprint arXiv:2004.04010 , 2020.
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and
Kaiser, L. Universal transformers. arXiv preprint
arXiv:1807.03819 , 2018.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 ,
2018.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for

--- PAGE 11 ---
Improving Transformers with Probabilistic Attention Keys
Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , pp. 4171‚Äì4186,
Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.
Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D.,
Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al.
Glam: EfÔ¨Åcient scaling of language models with mixture-
of-experts. arXiv preprint arXiv:2112.06905 , 2021.
Durrani, N., Sajjad, H., Dalvi, F., and Belinkov, Y . Ana-
lyzing individual neurons in pre-trained language models.
arXiv preprint arXiv:2010.02695 , 2020.
Dwivedi, R., Ho, N., Khamaru, K., Wainwright, M. J., Jor-
dan, M. I., and Yu, B. Sharp analysis of expectation-
maximization for weakly identiÔ¨Åable models. AISTATS ,
2020a.
Dwivedi, R., Ho, N., Khamaru, K., Wainwright, M. J., Jor-
dan, M. I., and Yu, B. Singularity, misspeciÔ¨Åcation, and
the convergence rate of EM. Annals of Statistics , 44:
2726‚Äì2755, 2020b.
Ethayarajh, K. How contextual are contextualized word
representations? comparing the geometry of bert, elmo,
and gpt-2 embeddings. arXiv preprint arXiv:1909.00512 ,
2019.
Fedus, W., Zoph, B., and Shazeer, N. Switch transform-
ers: Scaling to trillion parameter models with simple and
efÔ¨Åcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.
Ghosal, S. and van der Vaart, A. Entropies and rates of
convergence for maximum likelihood and bayes estima-
tion for mixtures of normal densities. Ann. Statist. , 29:
1233‚Äì1263, 2001.
Guo, M., Zhang, Y ., and Liu, T. Gaussian transformer: a
lightweight approach for natural language inference. In
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelli-
gence , volume 33, pp. 6489‚Äì6496, 2019.
Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang,
Y . Dynamic neural networks: A survey. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 2021.
Hanson, S. J. A stochastic version of the delta rule. Physica
D: Nonlinear Phenomena , 42(1-3):265‚Äì272, 1990.He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-
mentum contrast for unsupervised visual representation
learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pp. 9729‚Äì9738,
2020.
Hewitt, J. and Liang, P. Designing and interpreting probes
with control tasks. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) , pp. 2733‚Äì2743,
Hong Kong, China, November 2019. Association for
Computational Linguistics. doi: 10.18653/v1/D19-1275.
URL https://www.aclweb.org/anthology/D19-1275.
Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.
Axial attention in multidimensional transformers. arXiv
preprint arXiv:1912.12180 , 2019.
Ho, N., Ren, T., Sanghavi, S., Sarkar, P., and Ward, R. An ex-
ponentially increasing step-size for parameter estimation
in statistical models. arXiv preprint arXiv: 2205.07999 ,
2022.
Howard, J. and Ruder, S. Universal language model Ô¨Åne-
tuning for text classiÔ¨Åcation. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pp. 328‚Äì339, Mel-
bourne, Australia, July 2018. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P18-1031. URL
https://www.aclweb.org/anthology/P18-1031.
Huang, Y ., Gornet, J., Dai, S., Yu, Z., Nguyen, T., Tsao,
D., and Anandkumar, A. Neural networks with recurrent
generative feedback. Advances in Neural Information
Processing Systems , 33:535‚Äì545, 2020.
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
with linear attention. In International Conference on
Machine Learning , pp. 5156‚Äì5165. PMLR, 2020.
Kim, Y ., Denton, C., Hoang, L., and Rush, A. M. Structured
attention networks. arXiv preprint arXiv:1702.00887 ,
2017.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In International Conference on Learning
Representations , 2015.
Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
efÔ¨Åcient transformer. arXiv preprint arXiv:2001.04451 ,
2020.
Le, D. D. and Lauw, H. EfÔ¨Åcient retrieval of matrix
factorization-based top-k recommendations: A survey
of recent approaches. Journal of ArtiÔ¨Åcial Intelligence
Research , 70:1441‚Äì1479, 2021.

--- PAGE 12 ---
Improving Transformers with Probabilistic Attention Keys
Le, D. D. and Lauw, H. W. Stochastically robust person-
alized ranking for lsh recommendation retrieval. In Pro-
ceedings of the AAAI Conference on ArtiÔ¨Åcial Intelli-
gence , volume 34, pp. 4594‚Äì4601, 2020.
Lee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and
Teh, Y . W. Set transformer: A framework for attention-
based permutation-invariant neural networks. In Interna-
tional Conference on Machine Learning , pp. 3744‚Äì3753.
PMLR, 2019.
Li, H., Yang, Y ., Chen, D., and Lin, Z. Optimization al-
gorithm inspired deep neural network structure design.
InAsian Conference on Machine Learning , pp. 614‚Äì629.
PMLR, 2018.
Lin, Z., Feng, M., dos Santos, C. N., Yu, M., Xiang, B.,
Zhou, B., and Bengio, Y . A structured self-attentive
sentence embedding. CoRR , abs/1703.03130, 2017.
Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-
ssi, R., Kaiser, L., and Shazeer, N. Generating
wikipedia by summarizing long sequences. arXiv preprint
arXiv:1801.10198 , 2018.
Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692 , 2019.
Lloyd, S. Least squares quantization in pcm. IEEE transac-
tions on information theory , 28(2):129‚Äì137, 1982.
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,
A. Y ., and Potts, C. Learning word vectors for sentiment
analysis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Human
Language Technologies , pp. 142‚Äì150, Portland, Oregon,
USA, June 2011. Association for Computational Linguis-
tics. URL https://www.aclweb.org/anthology/P11-1015.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
sentinel mixture models. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings . Open-
Review.net, 2017. URL https://openreview.net/forum?
id=Byj72udxe.
Michel, P., Levy, O., and Neubig, G. Are sixteen heads
really better than one? In Wallach, H., Larochelle, H.,
Beygelzimer, A., d 'Alch ¬¥e-Buc, F., Fox, E., and Garnett,
R. (eds.), Advances in Neural Information Processing
Systems , volume 32. Curran Associates, Inc., 2019.
Mittal, S., Raparthy, S. C., Rish, I., Bengio, Y ., and Lajoie,
G. Compositional attention: Disentangling search and
retrieval. In International Conference on Learning Rep-
resentations , 2022. URL https://openreview.net/forum?
id=IwJPj2MBcIa.Mu, J. and Viswanath, P. All-but-the-top: Simple and ef-
fective postprocessing for word representations. In Inter-
national Conference on Learning Representations , 2018.
URL https://openreview.net/forum?id=HkuGJ3kCb.
Nangia, N. and Bowman, S. ListOps: A diagnostic dataset
for latent tree learning. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association
for Computational Linguistics: Student Research Work-
shop , pp. 92‚Äì99, New Orleans, Louisiana, USA, June
2018. Association for Computational Linguistics. doi:
10.18653/v1/N18-4013. URL https://www.aclweb.org/
anthology/N18-4013.
Nguyen, T., Ho, N., Patel, A., Anandkumar, A., Jordan,
M. I., and Baraniuk, R. G. A Bayesian perspective of con-
volutional neural networks through a deconvolutional gen-
erative model. arXiv preprint arXiv:1811.02657 , 2018.
Nguyen, T., Baraniuk, R., Bertozzi, A., Osher, S., and Wang,
B. Momentumrnn: Integrating momentum into recurrent
neural networks. Advances in Neural Information Pro-
cessing Systems , 33:1924‚Äì1936, 2020.
Nguyen, T., Nguyen, P., Pham, H., Bui, T., Nguyen, T.,
and Luong, D. Sp-gpt2: Semantics improvement in viet-
namese poetry generation. In 2021 20th IEEE Interna-
tional Conference on Machine Learning and Applications
(ICMLA) , pp. 1576‚Äì1581. IEEE, 2021a.
Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher, S. J.,
and Ho, N. Transformer with Fourier integral attentions.
arXiv preprint arXiv:2206.00206 , 2022.
Nguyen, T. M., Suliafu, V ., Osher, S. J., Chen, L., and
Wang, B. Fmmformer: EfÔ¨Åcient and Ô¨Çexible transformer
via decomposed near-Ô¨Åeld and far-Ô¨Åeld attention. arXiv
preprint arXiv:2108.02347 , 2021b.
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
N., Grangier, D., and Auli, M. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of NAACL-
HLT 2019: Demonstrations , 2019.
Parikh, A., T ¬®ackstr ¬®om, O., Das, D., and Uszkoreit, J. A
decomposable attention model for natural language infer-
ence. In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing , pp. 2249‚Äì
2255, Austin, Texas, November 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1244.
URL https://www.aclweb.org/anthology/D16-1244.
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
N., Ku, A., and Tran, D. Image transformer. In Dy, J.
and Krause, A. (eds.), Proceedings of the 35th Interna-
tional Conference on Machine Learning , volume 80 of

--- PAGE 13 ---
Improving Transformers with Probabilistic Attention Keys
Proceedings of Machine Learning Research , pp. 4055‚Äì
4064. PMLR, 10‚Äì15 Jul 2018. URL http://proceedings.
mlr.press/v80/parmar18a.html.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information
Processing Systems , pp. 8024‚Äì8035, 2019.
Patel, A. B., Nguyen, T., and Baraniuk, R. G. A probabilistic
theory of deep learning. arXiv preprint arXiv:1504.00641 ,
2015.
Patel, A. B., Nguyen, M. T., and Baraniuk, R. A proba-
bilistic framework for deep learning. Advances in neural
information processing systems , 29:2558‚Äì2566, 2016.
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.,
and Kong, L. Random feature attention. In International
Conference on Learning Representations , 2021. URL
https://openreview.net/forum?id=QtTKTdVrFBB.
Qiu, J., Ma, H., Levy, O., Yih, S. W.-t., Wang, S., and Tang,
J. Blockwise self-attention for long document understand-
ing. arXiv preprint arXiv:1911.02972 , 2019.
Radev, D. R., Muthukrishnan, P., Qazvinian, V ., and Abu-
Jbara, A. The acl anthology network corpus. Language
Resources and Evaluation , 47(4):919‚Äì944, 2013.
Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding by generative pre-
training. OpenAI report , 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. OpenAI blog , 1(8):9, 2019.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:
100,000+ questions for machine comprehension of text.
InProceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing , pp. 2383‚Äì
2392, Austin, Texas, November 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1264.
URL https://www.aclweb.org/anthology/D16-1264.
Ren, T., Cui, F., Sanghavi, S., and Ho, N. Beyond EM al-
gorithm on over-speciÔ¨Åed two-component location-scale
Gaussian mixtures. arXiv preprint arXiv: 2205.11078 ,
2022.
Roy, A., Saffar, M., Vaswani, A., and Grangier, D. EfÔ¨Åcient
content-based sparse attention with routing transformers.
Transactions of the Association for Computational Lin-
guistics , 9:53‚Äì68, 2021. doi: 10.1162/tacl a00353. URL
https://www.aclweb.org/anthology/2021.tacl-1.4.Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. Poor man‚Äôs
bert: Smaller and faster transformer models. arXiv e-
prints , pp. arXiv‚Äì2004, 2020.
Sander, M. E., Ablin, P., Blondel, M., and Peyr ¬¥e, G. Mo-
mentum residual neural networks. In International Con-
ference on Machine Learning , pp. 9276‚Äì9287. PMLR,
2021.
Sanh, V ., Debut, L., Chaumond, J., and Wolf, T. Distilbert,
a distilled version of bert: smaller, faster, cheaper and
lighter. arXiv preprint arXiv:1910.01108 , 2019.
Schlag, I., Irie, K., and Schmidhuber, J. Linear transform-
ers are secretly fast weight programmers. In Interna-
tional Conference on Machine Learning , pp. 9355‚Äì9366.
PMLR, 2021.
Shazeer, N. Fast transformer decoding: One write-head is
all you need. arXiv preprint arXiv:1911.02150 , 2019.
Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. EfÔ¨Åcient
attention: Attention with linear complexities. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications
of Computer Vision , pp. 3531‚Äì3539, 2021.
So, D. R., Liang, C., and Le, Q. V . The evolved transformer.
arXiv preprint arXiv:1901.11117 , 2019.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: a simple way to prevent
neural networks from overÔ¨Åtting. The journal of machine
learning research , 15(1):1929‚Äì1958, 2014.
Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., and Joulin,
A. Augmenting self-attention with persistent memory.
arXiv preprint arXiv:1907.01470 , 2019.
Sun, S., Cheng, Y ., Gan, Z., and Liu, J. Patient knowledge
distillation for bert model compression. arXiv preprint
arXiv:1908.09355 , 2019.
Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the
importance of initialization and momentum in deep learn-
ing. In International conference on machine learning , pp.
1139‚Äì1147. PMLR, 2013.
Tay, Y ., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
Sparse Sinkhorn attention. In III, H. D. and Singh, A.
(eds.), Proceedings of the 37th International Conference
on Machine Learning , volume 119 of Proceedings of Ma-
chine Learning Research , pp. 9438‚Äì9447. PMLR, 13‚Äì18
Jul 2020. URL http://proceedings.mlr.press/v119/tay20a.
html.
Tay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D.,
Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.

--- PAGE 14 ---
Improving Transformers with Probabilistic Attention Keys
Long range arena : A benchmark for efÔ¨Åcient transform-
ers. In International Conference on Learning Represen-
tations , 2021. URL https://openreview.net/forum?id=
qVyeW-grC2k.
Tenney, I., Das, D., and Pavlick, E. BERT rediscov-
ers the classical NLP pipeline. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pp. 4593‚Äì4601, Florence, Italy, July
2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1452. URL https://www.aclweb.org/
anthology/P19-1452.
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and J ¬¥egou, H. Training data-efÔ¨Åcient image trans-
formers & distillation through attention. arXiv preprint
arXiv:2012.12877 , 2020.
Tsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and
Salakhutdinov, R. Transformer dissection: An uniÔ¨Åed
understanding for transformer‚Äôs attention via the lens of
kernel. arXiv preprint arXiv:1908.11775 , 2019.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems , pp. 5998‚Äì6008, 2017.
Vig, J. and Belinkov, Y . Analyzing the structure of at-
tention in a transformer language model. In Proceed-
ings of the 2019 ACL Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pp. 63‚Äì76,
Florence, Italy, August 2019. Association for Computa-
tional Linguistics. doi: 10.18653/v1/W19-4808. URL
https://www.aclweb.org/anthology/W19-4808.
V oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov,
I. Analyzing multi-head self-attention: Specialized
heads do the heavy lifting, the rest can be pruned. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pp. 5797‚Äì5808,
Florence, Italy, July 2019a. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P19-1580. URL
https://www.aclweb.org/anthology/P19-1580.
V oita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.
Analyzing multi-head self-attention: Specialized heads
do the heavy lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418 , 2019b.
Wang, B., Xia, H., Nguyen, T., and Osher, S. How does mo-
mentum beneÔ¨Åt deep neural networks architecture design?
a few case studies. arXiv preprint arXiv:2110.07034 ,
2021a.
Wang, B., Nguyen, T., Sun, T., Bertozzi, A. L., Baraniuk,
R. G., and Osher, S. J. Scheduled restart momentum foraccelerated stochastic gradient descent. SIAM Journal on
Imaging Sciences , 15(2):738‚Äì761, 2022.
Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-
former: Self-attention with linear complexity. arXiv
preprint arXiv:2006.04768 , 2020a.
Wang, X., Yu, F., Dou, Z.-Y ., Darrell, T., and Gonzalez,
J. E. Skipnet: Learning dynamic routing in convolutional
networks. In Proceedings of the European Conference on
Computer Vision (ECCV) , pp. 409‚Äì424, 2018a.
Wang, Y ., Nguyen, T., Zhao, Y ., Wang, Z., Lin, Y ., and Bara-
niuk, R. Energynet: Energy-efÔ¨Åcient dynamic inference.
2018b.
Wang, Y ., Shen, J., Hu, T.-K., Xu, P., Nguyen, T., Bara-
niuk, R., Wang, Z., and Lin, Y . Dual dynamic inference:
Enabling more efÔ¨Åcient, adaptive, and controllable deep
inference. IEEE Journal of Selected Topics in Signal
Processing , 14(4):623‚Äì633, 2020b.
Wang, Y ., Huang, R., Song, S., Huang, Z., and Huang,
G. Not all images are worth 16x16 words: Dynamic
transformers for efÔ¨Åcient image recognition. Advances in
Neural Information Processing Systems , 34, 2021b.
Williams, A., Nangia, N., and Bowman, S. A broad-
coverage challenge corpus for sentence understanding
through inference. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers) , pp. 1112‚Äì1122,
June 2018. doi: 10.18653/v1/N18-1101. URL https:
//www.aclweb.org/anthology/N18-1101.
Xia, H., Suliafu, V ., Ji, H., Nguyen, T., Bertozzi, A., Osher,
S., and Wang, B. Heavy ball neural ordinary differential
equations. Advances in Neural Information Processing
Systems , 34, 2021.
Xiong, Y ., Zeng, Z., Chakraborty, R., Tan, M., Fung, G.,
Li, Y ., and Singh, V . Nystr ¬®omformer: A Nystr ¬®om-based
Algorithm for Approximating Self-Attention. 2021.
Yang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,
R., and Le, Q. V . Xlnet: Generalized autoregressive
pretraining for language understanding. arXiv preprint
arXiv:1906.08237 , 2019.
Zhang, S. and Feng, Y . Modeling concentrated cross-
attention for neural machine translation with Gaussian
mixture model. In Findings of the Association for Com-
putational Linguistics: EMNLP 2021 , pp. 1401‚Äì1411,
Punta Cana, Dominican Republic, November 2021. As-
sociation for Computational Linguistics. doi: 10.18653/
v1/2021.Ô¨Åndings-emnlp.121. URL https://aclanthology.
org/2021.Ô¨Åndings-emnlp.121.

--- PAGE 15 ---
Improving Transformers with Probabilistic Attention Keys
Supplement to ‚ÄúTransformer with a Mixture of Gaussian Keys‚Äù
In this supplementary material, we provide experimental details and additional experiments of Transformer-MGK and
Transformer-MLK.
A. Additional Experiments
A.1. Experiment details
In this section, we provide model and training details for experiments in Section 3. All our experiments are conducted on a
server with 4 NVIDIA A100 GPUs.
A.1.1. L ONG RANGE ARENA BENCHMARK
Datasets and metrics We consider the following tasks in the LRA benchmark: Listops (Nangia & Bowman, 2018), byte-
level IMDb reviews text classiÔ¨Åcation (Maas et al., 2011), and byte-level document retrieval (Radev et al., 2013). These
tasks involve long sequences of length 2K,4K, and 4K, respectively. We follow the setup/evaluation protocol in (Tay et al.,
2021) and report the test accuracy for individual task and the average result across all tasks.
Models and baselines We use the softmax transformer (Vaswani et al., 2017) and linear transformer (Katharopoulos et al.,
2020) as our baselines. All models have 2 layers, 64 embedding dimension, and 128 hidden dimension. The number of
heads in each layer are set to 1, 2, 4, and 8. For Transformer-MGK/MLKs and their shifted versions, we share jrfor all
positionjand learn it for each head. The initial value for each jris set to 0.5. For Transformer-sMGK, we learn brand
initialize its elements from a standard normal distribution. Each jris a constant with valuep
DwhereDis the dimension
of each head, which is the same as in the baselines models
Details about the Long Range Arena (LRA) benchmarks can be found in the original paper (Tay et al., 2021). Our
implementation is based on the public code by (Xiong et al., 2021), and we follow their training procedures. The training
setting and additional baseline model details are provided in the conÔ¨Åguration Ô¨Åle used in (Xiong et al., 2021) and available
at https://github.com/mlpen/Nystromformer/blob/main/LRA/code.
A.1.2. L ANGUAGE MODELING ON WIKITEXT-103
Datasets and metrics WikiText-103 consists of articles from Wikipedia and is a dataset with long contextual dependencies.
The training set is made up of about 28Karticles containing 103Mrunning words; this corresponds to text blocks of about
3600 words. The validation and test sets are composed of 218Kand246Krunning words, respectively. Each of them
contains 60articles and about 268Kwords. Our experiment follows the standard setting (Merity et al., 2017; Schlag et al.,
2021) and splits the training data into L-word independent long segments. For evaluation, we use a batch size of 1, and go
through the text sequence with a sliding window of size L. We consider only the last position for computing perplexity
(PPL) except in the Ô¨Årst segment, where all positions are evaluated as in (Al-Rfou et al., 2019; Schlag et al., 2021).
Models and baselines Our language modeling implementation is based on the public code https://github.com/IDSIA/lmtool-
fwp by (Schlag et al., 2021). We use their small and medium model conÔ¨Ågurations for models in our experiments. In
particular, for small models, we set the key, value, and query dimension to 128, and the training and evaluation context
length to 256. For medium models, we set the key, value, and query dimension to 256, and the training and evaluation
context length to 384. In both conÔ¨Ågurations, we set the number of heads to 8, the feed-forward layer dimension to 2048,
and the number of layers to 16. For Transformer-MGK/MLK, we use our 4 and 8-head versions to compare with the 8-head
baselines.ir,brandjrfor this task follow our setings for LRA experiments. Other than those, our language modeling
share the same conÔ¨Ågurations as the baselines.
We train our models for language modeling on 2 A100, 40GB each with a batch size of 96, and each model is trained for
120 epochs. We apply 10% dropout (Hanson, 1990; Srivastava et al., 2014) and use the Adam optimizer (Kingma & Ba,
2015) with an initial learning rate of 0.00025 and 2000 steps for learning rate warm-up.
A.1.3. N EURAL MACHINE TRANSLATION ON IWSLT‚Äô14 G ERMAN TO ENGLISH
Datasets and metrics The IWSLT14 German-English dataset (Cettolo et al., 2014) contains 153Ktraining, 7Kvalidation,
and7Ktest TED-talks scripts German-English translated sentences. We follow the same preprocessing steps as in (Ott

--- PAGE 16 ---
Improving Transformers with Probabilistic Attention Keys
et al., 2019). We use the BiLingual Evaluation Understudy (BLEU) score as our evaluation metric for this task. All trained
models are evaluated using the evaluation protocol in (Ott et al., 2019).
Models and baselines The baseline model we use in this machine translation task is an encoder-decoder transformer with
six encoder/decoder layers, four attention heads per layer. The embedding dimension and the hidden size are 512 and 1024,
respectively, for both encoder and decoder. These architecture conÔ¨Ågurations are the same for Transformer-MGK/sMGK
except for the number of attention heads per layer, which is reduced by half. We share jracross all position jand learn it
for each head. The initial value for each jris set to 0.5. For Transformer-sMGK, we learn brand initialize its elements
from a standard normal distribution. Each jris a constant with the valuep
DwhereDis the dimension of each head, which
is the same as in the baselines models. Our experiments follow the settings from (Ott et al., 2019), and our implementation
is based on the public code https://github.com/pytorch/fairseq/tree/main/examples/translation.
A.2. More Empirical Analysis of Transformer-MGKs/MLKs trained for Language Modeling
In Table 3 in the main text, we show the improvements in valid and test perplexity of our Transformer-MGKs/MLKs
compared with the baseline softmax and linear transformers. In particular, Transformer-MGK/MLKs with the same number
of heads as the baselines, e.g. 8 heads, signiÔ¨Åcantly improve the baselines during training while Transformer-MGK/MLKs
with 4 head achieve comparable or better performance than the 8-head baselines. In this section, we provide more empirical
analysis to shed light on those results. Figure 5 shows the validation perplexity curve of our models versus the softmax and
linear transformers.
Figure 6 visualizes the attention matrices from a randomly selected sample for the trained softmax transformer with 8 heads
and Transformer-MGKs with 8 heads and 4 heads. These visualizations show that Transformer-MGKs attend to more
diverse positions in all heads and layers than the softmax attention baseline. We also compare the rank distributions of these
attention matrices computed from 1000 samples at each layer in the model. Figure 7 presents the rank histograms of the
8-head softmax attention and 4-head and 8-head MGK attentions for the 1stand5thlayer. It is clear that attention matrices
from the Transformer-MGKs have higher ranks than those in the softmax transformer, which implies that Transformer-MGK
can attend to more diverse regions than the softmax transformer without the need of using more attention heads.
Figure 5. Validation perplexity of Transformer-MGK vs. the softmax transformer (Left) and Transformer-MLK vs. the linear transformer
(Right) for language modeling on WikiText-103.
.
A.3. Additional training results for LRA
In this section, we provide additional experimental results on the LRA benchmark. Figure 8 compares the computational cost
measured by FLOPs and model complexity in term of the number of parameters of different inference and learning methods
for Transformer-MGK. The computational costs of Transformer-MGK/sMGK and Transformer-MGK Hard-E/Soft-E are on
par with each other, while Transformer-sMGK uses fewer parameters than the other without trade-off in performance 5 for
all tasks. The naming is as explained in Section 3.4 in the main text. In addition, Figure 9 visualizes the attention matrices in
the 4-head linear transformer baseline, 4-head Transformer-MLK, and 2-head Transformer-MLK trained on the document
retrieval task.

--- PAGE 17 ---
Improving Transformers with Probabilistic Attention Keys
Softmax 8 HeadsMGK 8 HeadsMGK 4 HeadsLayer 1Layer 2Layer 3Layer 1Layer 2Layer 3Layer 1Layer 2Layer 3Head 1Head 2Head 3Head 4Head 5Head 6Head 7Head 8
Figure 6. Visualization of attention matrices from 8-head softmax transformer (Left), 8-head Transformer-MGK (Middle), and 4-head
Transformer-MGK (Right) trained on WikiText-103 language modeling. Here, we plot the attention matrices for all heads and layers in
the models. The sequence length is 256 and the size of each matrix is 256 256.
.
CountLayer 1Layer 5RankSoftmax 8 HeadsMGK 8 HeadsMGK 4 Heads
Figure 7. Rank distributions of attention matrices from 8-head softmax transformer (Left), 8-head Transformer-MGK (Middle), and 4-head
Transformer-MGK (Right) trained on WikiText-103 language modeling. The rank histograms are computed from 1000 attention matrices
at each layer. The attention matrices from Transformer-MGKs have higher ranks than those from softmax transformers. This implies that
Transformer-MGKs have more diverse attention patterns, which allows us to reduce the number of heads in Transformer-MGKs.
A.4. Ablation Study on the Impact of the Mixture of Keys, the Gaussian Distance, and the Key Shifting
In this section, we conduct an ablation study of the Transformer-MGK on the LRA retrieval task to investigate where the
performance improvement is from. In particular, we would like to understand the impact of the following factors on the
performance of Transformer-MGK: 1) the mixture of keys, 2) the Gaussian distance, and 3) the key shifting. We summarize
our empirical results in Table 6 and discuss the impact of 1, 2 and 3 below.

--- PAGE 18 ---
Improving Transformers with Probabilistic Attention Keys
         Number of HeadsFLOPsParameters (1e5)RetrievalListopsText
Transformer-sMGKTransformer-MGK Hard-ETransformer-MGK
Transformer-MGK Soft-E
Figure 8. Model complexity (Top) and computational cost (Bottom) of different inference and learning methods for Transformer-MGK
trained on the document retrieval task. While computational costs are almost the same, Transformer-sMGK has more advantage in model
size, comparing to Transformer-MGK, Transformer-MGK Hard-E, and Soft-E. The naming is as explained in Section 3.4 in the main text.
1K 2.5Kk 4K1K
2.5
4Klayer 1
1K 2.5Kk 4K1K
2.5
4Klayer 2
1K 2.5Kk 4K1K
2.5
4K
1K 2.5Kk 4K1K
2.5
4K
1K 2.5K 4K1K
2.5k
4Klayer 1
1K 2.5K 4K1K
2.5k
4Klayer 2
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4Khead 1layer 1
1K 2.5K 4K1K
2.5k
4Klayer 2
1K 2.5K 4K1K
2.5k
4Khead 2
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4Khead 3
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4Khead 4
1K 2.5K 4K1K
2.5k
4K0.60.70.80.91.0
Linear 4 Heads MLK 4 Heads MLK 2 Heads
Figure 9. Visualization of attention matrices in the 4-head linear transformer baseline (Left), 4-head Transformer-MLK (Middle), and
2-head Transformer-MLK (Right) trained on the document retrieval task. Here, the sequence length is 4000, and the size of each matrix is
40004000.
Impact of the Mixture of Keys We apply our mixture of keys (MGK) approach to the softmax transformer using the dot
product between queries and keys instead of the Gaussian distance as in our paper. We name this model Softmax MGK. We
compare the Softmax MGK that has 1 head (Sofmax MGK 1 head in Table 6) with the baseline softmax transformers that
use 1 and 2 heads (Softmax 1 head and Softmax 2 heads in Table 6). Results in Table 6 show that the Softmax MGK 1 head
outperforms both the baseline softmax transformers of 1 and 2 heads. Note that our Softmax MGK 1 head is more efÔ¨Åcient
than the baseline of 2 heads in terms of the number of parameters and the number of FLOPs. These results conÔ¨Årm the
beneÔ¨Åt of using MGK.
Impact of using the Gaussian distance Next, we compare the softmax MGK with the Gaussian MGK. Here the Gaussian
MGK is the Transformer-MGK proposed and discussed in our paper, which computes the attention scores using the MGK
approach and the Gaussian distance between the queries and keys. Results in Table 6 suggest that the Gaussian MGK 1

--- PAGE 19 ---
Improving Transformers with Probabilistic Attention Keys
Table 6. Ablation study on the impact of the mixture of keys, the Gaussian distance, and the key shifting on the LRA retrieval task. We
denote the softmax Transformer by Softmax. All Softmax models (i.e., Softmax 2 heads, Softmax 1 head, Softmax MGK 1 head, and
Softmax sMGK 1 head) use dot product to compute the attention scores. All Gaussian models (i.e., Gaussian MGK 1 head, Gaussian
sMGK 1 head, and Gaussian 1 head) use Gaussian distance to compute the attention scores. We denote MGK with key shifting by sMGK.
Here MGK is used to denote our approach of using a mixture of keys at each timestep.
Method Accuracy (%)
Softmax 2 heads 79.10
Softmax sMGK 1 head 79.81
Softmax MGK 1 head 79.23
Softmax 1 head 77.90
Gaussian sMGK 1 head 81.23
Gaussian MGK 1 head 80.63
Gaussian 1 head 80.38
head improves over the Softmax MGK 1 head (80.63% vs. 79.23%). This result justiÔ¨Åes the advantage of using Gaussian
distance over dot product to compute the attention scores.
Impact of key shifting Finally, we apply key shifting to both Softmax MGK and Gaussian MGK (Softmax sMGK and
Gaussian sMGK in Table 6). From Table 6, we observe that the Softmax sMGK 1 head and Gaussian sMGK 1 head
outperform the Softmax MGK 1 head and Gaussian MGK 1 head, respectively. These results, again, corroborate the beneÔ¨Åt
of using key shifting.
We also include the result for the Gaussian 1 head model in Table 6. This Gaussian 1 head model is similar to the Softmax 1
head model but uses the Gaussian distance to compute the attention scores. Comparing the results of the Gaussian 1 head
model, the Softmax 1 head model, and the Gaussian MGK 1 head model reported in Table 6 further conÔ¨Årms the advantage
of using MGK and Gaussian distance.
A.5. EfÔ¨Åciency analysis on WikiText-103
EfÔ¨Åciency analysis on WikiText-103 In this section, we provide greater details on the metrics and results of our model
efÔ¨Åciency analysis. Extending our method to Linear Transformer (Transformer-MLK) signiÔ¨Åcantly beneÔ¨Åts the efÔ¨Åciency of
the linear baseline, making it especially applicable for long-range and large-scale tasks.
Analysis metrics Our analysis reports the number of FLOPS as model computational efÔ¨Åciency for both training and
inference. The number of total and non-embedding parameters is used to measure model complexity. Since the aim of our
method is to reduce the number of parameters in the main model, which does not include the number of input-embedding
parameters. Hence, it is also essential to provide non-embedding parameter reduction as a criterion for model size efÔ¨Åciency.
Here, we investigate the computation and memory beneÔ¨Åts of our model in large-scale tasks through increasing D2f256,
512, 1024, 2048, 4096 gandN2f128, 256, 512, 1024, 2048, 4096, 8192 g. For the FLOP calculation, we use fvcore. All
measurements are calculated when running the model through data of batch size 1.
Transformer-MLK reduces model complexity and computational cost
Fig. 10A and Fig. 10B shows the reduction in FLOPS ratio between 4-head Transformer-MLK vs. the 8-head Linear baseline
as the function of model dimension Dand the sequence length N. Fig. 10C presents the model size beneÔ¨Åts (number of
total/non-embedding parameter ratios) between our model and the baseline as Dincreases. These Ô¨Ågures indicate that as we
scale up the tasks and the model, our Transformer-MLK is signiÔ¨Åcantly more advantageous than the Linear baseline.
A.6. Time Overhead and Memory Footprint Analysis
Table 7 compares the GPU memory footprint and computational time overhead (seconds/iteration) of our 4-head Transformer-
MGKs/MLKs with those of the 8-head softmax/linear transformer baselines at test time. All models are trained on the LRA
retrieval task, and we use a batch size of 32 for each iteration. Our MGK/MLK models save memory and reduce wall-clock
time signiÔ¨Åcantly compared to the baselines. Using key shifting in our models, i.e. Transformer-sMGKs/sMLKs, helps
improve the efÔ¨Åciency further.

--- PAGE 20 ---
Improving Transformers with Probabilistic Attention Keys
Sequence Length Model Dimension Model Dimension 2565121024 2048 4096 
Training Testing FLOPS Ratio 
Total parameters 
Non-embedding 
parameters 
Parameters Ratio Ôøº 
Ôøº 
Ôøº 
A B C
Figure 10. Training (A), Inference (B) FLOP ratios and number-of-parameter ratio (C) between 4-head Transformer-MLK with the 8-head
Linear baseline across different Dmodel dimensions and Nsequence lengths, for the language modeling task on WikiText-103. 4-head
Transformer-MLK signiÔ¨Åcantly reduces computation (both training and inference FLOPS) and model complexity (both the number of
total and non-embedding parameters), than the baseline as we scale up model dimension and the sequence length, showing the advantage
of Transformer-MLK for long-range and large-scale tasks.
Table 7. Comparing the GPU memory footprint and computational time overhead (seconds/iteration) between our 4-head Transformer-
MGKs/MLKs and the 8-head softmax/linear transformer baselines trained on the LRA retrieval task at test time. Our Transformer-
MGKs/MLKs save much more memory and have signiÔ¨Åcantly smaller wall-clock time compared to the baselines. Here we use a batch
size of 32 for each iteration.
Method Memory (Gb) Time overhead (seconds/iteration)
Softmax 8 heads 58.00 0.357
Transformer-MGK 4 heads 53.58 0.278
Transformer-sMGK 4 heads 45.50 0.227
Linear 8 heads 3.38 0.055
Transformer-MLK 4 heads 2.90 0.043
Transformer-sMLK 4 heads 2.83 0.042
Table 8. The learned mixing coefÔ¨Åcient jrof all heads and layers in the 1-head Transformer-MGKs trained on the LRA retrieval task.
Here we use the same j1;j2for all time step j= 1;:::;N .
Method Layer 1 Layer 2
j1j2j1j2
Transformer-sMGK 1 heads 0.488 0.512 0.500 0.500
Transformer-MGK 1 heads 0.502 0.498 0.497 0.503
A.7. Learning Curves Showing Convergence
In this section, we replot Figure 1 and 5 to show the convergence of our trainings. In Figure 1, the results do not seem to
converge since we plot the loss and the accuracy in log-scale. In Figure 4, the results do not seem to converge since we
zoom into the speciÔ¨Åc range on the y and x-axes. Figure 11 and 12 are the replotted versions of Figure 1 and 5, respectively.
In Figure 11, the training loss/accuracy curves stop early because we use early stopping to avoid overÔ¨Åtting. The test
loss/accuracy curves in this Ô¨Ågure already converge.
A.8. Weight Matrices of the Keys, Keys and Mixing CoefÔ¨Åcient
In this section, we analyze the learned jr,kjr, andWKr,j= 1;:::;N andr= 1;:::;M in the Transformer-MGK
trained on the LRA retrieval task. In all of our experiments, we set M= 2. In Figure 13 and 14, we visualize the weight

--- PAGE 21 ---
Improving Transformers with Probabilistic Attention Keys
Figure 11. Training and test loss/accuracy of Transformer-MGK vs. softmax transformer (Left) and Transformer-MLK vs. linear Trans-
former (Right) on the retrieval task, which has the longest average sequence-length and attention span among the LRA tasks (Tay et al.,
2021). In training, we apply early stopping to avoid overÔ¨Åtting. That explains why the training loss and accuracy curves stop early. The
test loss/accuracy curves already converge. The impressive performance of Transformer-MGK/MLK on this challenging task validates the
capability of our models to capture long-range dependencies via learning a diversity of attention patterns.
Figure 12. Validation perplexity of the Transformer-MGK vs. the softmax transformer (Left) and the Transformer-MLK vs. the linear
transformer (Right) for language modeling on WikiText-103. Training converges on this task after 500000 iterations, equivalent to 115
epochs .
.
matrices WKthat computes the keys and the keys K, respectively, for all heads and layers in the 2-head softmax transformer
baseline, the 1-head Transformer-MGK with 2 keys, and the 1-head Transformer-sMGK with 2 keys trained on the LRA
retrieval task. Note that for the keys, we only plot the Ô¨Årst 100 tokens. Also, Table 8 summarizes the learned mixing
coefÔ¨Åcientjrof all heads and layers in the 1-head Transformer-MGK trained on the same retrieval task. Here we use the
samej1;j2for all time step j= 1;:::;N .
A.9. Additional Computational Complexity (FLOPs) Analysis at Training Time
Figure 15 demonstrates the computational cost (FLOPs) for each training iteration of the Transformer-MGK vs. the baseline
softmax transformer (Left) and the Transformer-MLK vs. the baseline linear transformer (Right) on the LRA retrieval task.
The efÔ¨Åciency advantage of Transformer-MGK/MLK over the baselines grows with the number of heads.

--- PAGE 22 ---
Improving Transformers with Probabilistic Attention Keys
Layer 1Layer 2Layer 1Softmax 2 HeadsHead 2Head 1MGK 1 HeadsMGK 1 HeadWeight Key 1Weight Key 2
Weight Key1Layer 2Layer 1Layer 2
Figure 13. Weight matrices WKfor computing the keys, for all heads and layers, in the 2-head softmax transformer baseline (Left), the
1-head Transformer-MGK with 2 keys (Middle), and the 1-head Transformer-sMGK with 2 keys (Right) trained on the LRA retrieval task.
Here, the dimension of each head D= 32 and that of input xiisDx= 64 . Hence, each weight matrix has the shape of (64;32).
Layer 1Layer 2Layer 1Softmax 2 headsHead 2Head 1MGK 1 HeadsMGK 1 HeadKey 1Key 2
Key 1Layer 2Layer 1Layer 2
Key 2
Figure 14. Key embeddings Kfor all heads and layers of the 2-head softmax transformer baseline (Left), the 1-head Transformer-MGK
with 2 keys (Middle), and the 1-head Transformer-sMGK with 2 keys (Right) trained on the LRA retrieval task. Here the dimension Dof
each head is 32, and we plot the key embeddings of the Ô¨Årst 100 tokens in a randomly chosen sequence. Hence, each key matrix has the
shape of (100;32).
Figure 16 shows the computational cost per training iteration (measured in FLOPs) of different inference and learning
methods for Transformer-MGK trained on the document retrieval task. Transformer-sMGK, Transformer-MGK, Transformer-
MGK Hard-E, and Soft-E have similar computational costs.
A.10. Scaling to 12-Head Baseline Models for the Retrieval task.
To further study the scalability of our model, in this section, we investigate the performance of our 6-head Transformer-
MGKs/MLKs in comparison with the 12-head baseline softmax/linear transformers on the retrieval task. Table 9 indicates

--- PAGE 23 ---
Improving Transformers with Probabilistic Attention Keys
Figure 15. Computational cost (FLOPs) for each training iteration of Transformer-MGK vs. the baseline softmax transformer (Left)
and Transformer-MLK vs. the baseline linear transformer (Right) on the LRA retrieval task. The efÔ¨Åciency advantage of Transformer-
MGK/MLK over the baselines grows with the number of heads. Here, the batch size of each training iteration is 32.
         Number of HeadsFLOPsRetrievalListopsText
Transformer-sMGKTransformer-MGK Hard-ETransformer-MGK
Transformer-MGK Soft-E
Figure 16. Computational cost (measured in FLOPs) per training iteration of different inference and learning methods for Transformer-
MGK trained on the LRA retrieval task. Computational costs are almost the same for Transformer-sMGK, Transformer-MGK, Transformer-
MGK Hard-E, and Soft-E. The naming is as explained in Section 3.4 in the main text.
Table 9. Test Accuracy (%) of 6-head Transformer-MGKs/MLKs compared with the baseline 12-head softmax and linear transformers on
the retrieval task. Our 6-head Transformer-MGKs/MLKs signiÔ¨Åcantly outperform softmax and linear transformers, respectively, while
being more efÔ¨Åcient in terms of computational cost, model size, and memory usage.
Method Accuracy (%)
Softmax 12 heads 82.18
Transformer sMGK 6 head 83.31
Transformer MGK 6 head 83.05
Linear sMGK 12 head 81.97
Transformer sMLK 6 head 82.80
Transformer MLK 6 head 82.11
that our 6-head Transform-MGKs/MLKs signiÔ¨Åcantly outperform 12-head softmax/linear transformers, respectively. More-
over, comparing these results to those in Table 1 and Table 2, although the 12-head softmax/linear transformers improve
over the 8-head ones, their accuracies are still worse than or only equivalent to those of our 4-head and even 2-head
Transformer-MGK/MLK models.
A.11. Comparison to Multi-query Attention
In this section, we compare our MGK approach to the multi-query attention (Shazeer, 2019). The multi-query attention
shares the same set of keys and values at different heads to reduce the memory-bandwidth cost during incremental inference,
which allows faster inference since the size of the reloaded ‚Äùkeys‚Äù and ‚Äùvalues‚Äù tensors are signiÔ¨Åcantly reduced. On

--- PAGE 24 ---
Improving Transformers with Probabilistic Attention Keys
Table 10. Perplexity (PPL) on Wikitext-103 of 4-head Multi-query Transformer-MGK and the 8-head Multi-query Transformer baseline.
Combining multi-query attention with transformer-MGK results in a better test performance than the baseline while being more efÔ¨Åcient
since Transformer-MGK enables the model to reduce the number of heads by half.
Method Valid PPL Test PPL
Multi-query attention 8 heads (small) 35.08 36.03
Multi-query Transformer MGK 4 head (small) 35.16 35.79
Softmax 8 heads (small) 33.15 34.29
Transformer MGK 4 head (small) 33.28 34.21
another hand, our Transformer-MGK models the key at each head as a Gaussian mixture model, which leads to the use of
multiple keys at each head and allows us to decrease the number of attention heads. This helps reduce the computations
and parameters needed to calculate additional queries and values. If using key shifting (see option (B) in paragraph Design
Options for Keys in Section 2.3), the MGK approach also helps reduce the computations and parameters needed to calculate
additional keys. The advantages of Transformer-MGK hold in both training and inference, including incremental inference.
We have provided a detailed analysis on the computational complexity and the number of parameters of the Transformer-
MGK in comparison with the corresponding softmax transformer in Appendix B. Combining the multi-query attention
and our MGK approach is interesting since each method has its own advantage and they are complementary to each other.
In particular, we can let the transformer model share the same set of values and mixtures of keys at different heads. This
approach can potentially have the advantages of both multi-query attention and MGK. Table 10 shows the Perplexity (PPL)
on Wikitext-103 of 4-head Multi-query Transformer-MGK and the 8-head Multi-query Transformer baseline. Combining
multi-query attention with transformer-MGK beneÔ¨Åts both the model‚Äôs performance (Test PPL) compared to the baselines
and the its efÔ¨Åciency since Transformer-MGK allows the model to reduce the number of heads by half.
B. An Analysis on the Computational Complexity and the Number of Parameters in
Transformer-MGK and the Softmax Transformer
In this section, we compare the computational complexity and the number of parameters in transformer-MGK with Mkeys
at each timestep and H=M heads to the baseline softmax transformer that has 1 key at each timestep and Hheads. Here we
chooseMsuch thatHis a multiple of Mand use keys design option (A) that make the key kjra linear projection of the
inputxj, i.e.kjr=xjW>
Kr, where xj2R1Dx,WKr2RDDxandr= 1;2;:::;M (see Design Options for Keys
in Section 2.3 for more details). To simplify notation and without loss of generality, we let M= 2as in our experiments
and assume that Dv=D, i.e., the values have the same feature dimension as the queries and the keys. To simplify the
computation, we also do not take the softmax operator into account since this operator yields similar costs when applied in
Transformer-MGK and softmax transformer.
B.1. Computational Complexity
(i) SoftmaxH-head attention: The number of computations in a softmax H-head attention is N2H(4D 1)+NHD (6Dx+
2HD 5).
Explanation: To calculate the query matrix Q, the key matrix K, and the value matrix Vin Step 1 in Section 1.1 at each
head, we need 3NDDxmultiplications and 3ND(Dx 1)additions. In total, these need 3ND(2Dx 1)computations.
Next, to compute the product QK>in Eqn. (1), we need N2Dmultiplications and N2(D 1)additions. Similarly, the
product AVrequiresN2Dmultiplications and N(N 1)Dadditions. In total, computing the output sequence Hin Eqn. (1)
at each head requires 3ND(2Dx 1) +N2D+N2(D 1) +N2D+N(N 1)D=N2(4D 1) +ND(6Dx 4)
computations. The total computation for all Hheads is then
H(N2(4D 1) +ND(6Dx 4)) +NHD (2HD 1)
=N2H(4D 1) +NHD (6Dx+ 2HD 5);
where the extra NHD (2HD 1)is from the linear projection by WO.
(ii) Mixture of 2 Gaussian keys attention with H=2-head: The number of computations in a mixture of 2 Gaussian keys
attention with H=2-head isN2H(3D 0:5) +NHD (4Dx+HD 4).

--- PAGE 25 ---
Improving Transformers with Probabilistic Attention Keys
Explanation: Similar to the above derivation, in a Mixture of M Gaussian keys attention, to compute the output sequence H
we needN2((2M+ 2)D 1) +ND((M+ 2)(2Dx 1) 1)computations. Note that, to compute the Gaussian distances
between the queries qiand the keys kjas in the mixture of M Gaussian keys attention and to compute their dot product as in
the softmax attention, we need the similar number of computations. Therefore, the total computation for all H=M heads is
then
(H=M )(N2((2M+ 2)D 1) +ND((M+ 2)(2Dx 1) 1)) +NHD (2(H=M )D 1)
=N2H2(M+ 1)D 1
M
+NHD2(M+ 2)
MDx+2
MHD 3M+ 2
M
:
LetM= 2, then the total computation of the mixture of 2 Gaussian keys attention is given by N2H(3D 0:5) +
NHD (4Dx+HD 4).
Soft-max H-head attention versus mixture of 2 Gaussian keys attention with H/2-head: Given the results in (i) and
(ii), when compared to the baseline softmax H-head attention, our mixture of 2 Gaussian keys attention with H=2-head
saves
N2H(D 0:5) +NHD (2Dx+HD 1)
computations. When Nis large, this difference is signiÔ¨Åcant. In conclusion, the mixture of 2 Gaussian keys attention with
H/2-heads has cheaper computational complexity than that of soft-max H-head attention.
B.2. The Number of Parameters
(iii) Softmax H-head attention: The number of parameters in a softmax H-head attention is 3HDDx+ (HD)2.
Explanation: 3HDDxis from the linear projects to calculate the query matrix Q, the key matrix K, and the value matrix V
in Step 1 in Section 1.1. (HD)2is from the linear project to compute the Ô¨Ånal output as in Eqn. ( ??).
(iv) Mixture of 2 Gaussian Keys attention with H=2-head: The number of parameters in a Mixture of 2 Gaussian Keys
attention with H=2-head is 2HDDx+ 0:5(HD)2+H.
Explanation: The linear projections to calculate QandVcontributeHDDx=2parameters each. The linear projection
to calculate KcontributesHDDx. The linear project to compute the Ô¨Ånal output has dimension (H=2)DHD, so it
contributes 0:5(HD)2parameters. Hmore parameters is from the prior jr. These priors contribute 2 parameters at each
head since we make fj1;j2gfor allj= 1;:::;N the same.
Soft-max H-head attention versus mixture of 2 Gaussian keys attention with H/2-head: Given the results in (iii) and
(iv), when compared to the baseline softmax H-head attention, our mixture of 2 Gaussian keys attention with H=2-head
savesHDDx+ 0:5(HD)2 Hparameters. When HandDare large, this saving is signiÔ¨Åcant.
C. Proofs of main results
In this appendix, we provide proofs for the main results in the paper.
C.1. Proof of Theorem 1
To ease the presentation of the proof, for any probability distribution G, we denote
pG(x) :=Z
f(x )dG() =Z
(xj;2I)dG();
for allx2Rdwheref(x) =1
(p
2)dexp
 kxk2
22
for given > 0. It means that pGis the convolution of fand
the probability distribution G. Since the space of Gaussian mixtures is dense in the space of continuous probability
measures (Bacharoglou, 2010), it indicates that there exists probability distribution G1such that
sup
x2Rdjp(x) pG1(x)j
2: (9)

--- PAGE 26 ---
Improving Transformers with Probabilistic Attention Keys
Our next step is to prove that there exists a probability measure G2with at most Ksupports where K(Clog(1=))dfor
some universal constant Csuch that
sup
x2RdjpG1(x) pG2(x)j
2: (10)
Indeed, from Lemma A.1 in (Ghosal & van der Vaart, 2001), for any k1there exists a probability distribution G2with at
most (2k 2)dsupports such that
Z
d(G1 G2)() = 0; (11)
for any= (1;2;:::;d)2Ndsuch that 0jj=Pd
j=1j2k 2, Here,=Qd
j=1j
j.
Now, for any M2ap
d, we havekx kkxk kk>M ap
d>M= 2as long askxk>M and2[ a;a]d. It
indicates that
sup
kxk>MjpG1(x) pG2(x)j= sup
kxk>MZ
f(x )d(G1 G2)()
sup
kxk>MZ1
(p
2)dexp
 kx k2
22
d(G1+G2)()
2
(p
2)dexp
 M2
82
: (12)
On the other hand, for any k1we also have that
sup
kxkMjpG1(x) pG2(x)j= sup
kxkMZ
f(x )d(G1 G2)()
sup
kxkMZ0
@f(x ) k 1X
j=0( 1)jkx k2j
(p
2)dd+2jj!1
Ad(G1 G2)()
+ sup
kxkMZk 1X
j=0( 1)jkx k2j
(p
2)dd+2jj!d(G1 G2)()
= sup
kxkMZ0
@f(x ) k 1X
j=0( 1)jkx k2j
(p
2)dd+2jj!1
Ad(G1 G2)(); (13)
where the Ô¨Ånal equality is stems from
Zk 1X
j=0( 1)jkx k2j
(p
2)dd+2jj!d(G1 G2)() = 0;
which is due to Eqn. (11).
To further bound the right-hand-side (RHS) of Eqn. (13), we use the following inequality:
exp(y) k 1X
j=0(y)j=j!jyjk=k!
for anyy2R. Sincek!(k=e)kfor anyk1, the above bound can be rewritten as
exp(y) k 1X
j=0(y)j=j!jyejk
kk: (14)

--- PAGE 27 ---
Improving Transformers with Probabilistic Attention Keys
Further simpliÔ¨Åcation of Eqn. (13) leads to
sup
kxkMjpG1(x) pG2(x)j sup
kxkMZf(x ) k 1X
j=0( 1)jkx k2j
(p
2)dd+2jj!d(G1+G2)()
2 sup
kxkM;2[ a;a]df(x ) k 1X
j=0( 1)jkx k2j
(p
2)dd+2jj!
= sup
kxkM;2[ a;a]d2
(p
2)dexp
 kx k2
22
 k 1X
j=0( 1)jkx k2j
2jj!
 sup
kxkM;2[ a;a]dekkx k2k
2k(2k)k;
where the Ô¨Ånal inequality is based on an application of inequality (14) with y= kx k2=(22). ForkxkMand
2[ a;a]d, we havekx kkxk+kkM+ap
d. Therefore, we further have
sup
kxkMjpG1(x) pG2(x)j sup
kxkM;2[ a;a]dekkx k2k
2k(2k)kek(M+ap
d)2k
2k(2k)k:
WhenM2ap
d, we haveM+ap
d3M
2and the above bound leads to
sup
kxkMjpG1(x) pG2(x)j(9e)kM2k
(82k)k: (15)
By choosing M2= 82log(1=0)for some0>0, the bounds in Eqns. (12) and (15) become
sup
kxkMjpG1(x) pG2(x)j2
(p
2)d0;
sup
kxk>MjpG1(x) pG2(x)j(9e)k(log(1=0))k
kk: (16)
As long as we choose k= 9e2log(1=0)and01, we have
sup
kxk>MjpG1(x) pG2(x)je k=e 9e2log(1=0)= (0)9e20: (17)
By choosing 0=
2 maxf2
(p
2)d;1g, the results from Eqns. (16) and (17) indicate that
sup
kxkMjpG1(x) pG2(x)j
2;and sup
kxk>MjpG1(x) pG2(x)j
2:
Therefore, if we choose M= 82log
2 maxf2
(p
2)d;1g

andk= 9e2log
2 maxf2
(p
2)d;1g

, we have
sup
x2RdjpG1(x) pG2(x)j
2:
It indicates that we obtain the conclusion of claim (10) by choosing K= (2k 2)d
18e2log
2 maxf2
(p
2)d;1g
d
.
Combining the results from Eqns. (9) and (10), we have
sup
x2Rdjp(x) pG2(x)jsup
x2Rdjp(x) pG1(x)j+ sup
x2RdjpG1(x) pG2(x)j:
As a consequence, we obtain the conclusion of the theorem.
