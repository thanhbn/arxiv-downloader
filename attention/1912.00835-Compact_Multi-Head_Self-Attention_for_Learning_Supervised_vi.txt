# Cơ chế Self-Attention Đa Đầu Gọn Nhẹ để Học Biểu Diễn Văn Bản Có Giám Sát

Sneha Mehta
Virginia Tech, USA
snehamehta@cs.vt.edu

Huzefa Rangwala
George Mason University, USA
rangwala@cs.gmu.edu

Naren Ramakrishnan
Virginia Tech, USA
naren@cs.vt.edu

## TÓM TẮT

Học biểu diễn hiệu quả từ văn bản đã là một lĩnh vực nghiên cứu tích cực trong các lĩnh vực NLP và khai thác văn bản. Các cơ chế attention đã đi đầu để học biểu diễn câu theo ngữ cảnh. Các phương pháp tiên tiến hiện tại cho nhiều tác vụ NLP sử dụng các mô hình ngôn ngữ được huấn luyện trước lớn như BERT, XLNet và tương tự để học biểu diễn. Các mô hình này dựa trên kiến trúc Transformer bao gồm các khối tính toán lặp lại gồm multi-head self-attention và mạng feedforward.

Một trong những nút thắt chính đóng góp lớn vào độ phức tạp tính toán của các mô hình Transformer là lớp self-attention, vừa tốn kém về mặt tính toán vừa sử dụng nhiều tham số. Trong công trình này, chúng tôi giới thiệu một cơ chế multi-head self-attention mới hoạt động trên GRUs được chứng minh là rẻ hơn về mặt tính toán và hiệu quả hơn về tham số so với cơ chế self-attention được đề xuất trong Transformers cho các tác vụ phân loại văn bản.

Hiệu quả của phương pháp chúng tôi chủ yếu xuất phát từ hai tối ưu hóa: 1) chúng tôi sử dụng phân tích ma trận thấp hạng của ma trận ái lực để hiệu quả có được nhiều phân phối attention thay vì có các tham số riêng biệt cho mỗi đầu 2) điểm attention được thu được bằng cách truy vấn một vector ngữ cảnh toàn cục thay vì truy vấn dày đặc tất cả các từ trong câu.

Chúng tôi đánh giá hiệu suất của mô hình được đề xuất trên các tác vụ như phân tích cảm xúc từ đánh giá phim, dự đoán xếp hạng doanh nghiệp từ đánh giá và phân loại bài báo tin tức theo chủ đề. Chúng tôi thấy rằng phương pháp được đề xuất bằng hoặc vượt trội so với một loạt các baseline mạnh và hiệu quả hơn về tham số so với các phương pháp multi-head tương đương.

Chúng tôi cũng thực hiện phân tích định tính để xác minh rằng phương pháp được đề xuất có thể diễn giải được và nắm bắt tầm quan trọng của từ phụ thuộc ngữ cảnh.

**TỪ KHÓA:** mạng neural, phân loại văn bản, attention

## 1. GIỚI THIỆU

Học biểu diễn ngôn ngữ hiệu quả là quan trọng cho nhiều tác vụ phân tích văn bản bao gồm phân tích cảm xúc, phân loại tin tức, suy luận ngôn ngữ tự nhiên và trả lời câu hỏi. Học có giám sát sử dụng mạng neural thường bao gồm học biểu diễn câu trung gian sau đó là một lớp cụ thể cho tác vụ. Đối với các tác vụ phân loại văn bản; điều này thường là một lớp kết nối đầy đủ theo sau bởi softmax N-chiều trong đó N là số lớp.

Học biểu diễn ngôn ngữ tự giám sát đã có tiến bộ đáng kể trong những năm gần đây với việc giới thiệu các kỹ thuật mới cho mô hình hóa ngôn ngữ kết hợp với các mô hình sâu như ELMo [32], ULMFit [15] và gần đây hơn là BERT [8] và GPT-2 [33]. Đã có một làn sóng các mô hình ngôn ngữ dựa trên BERT sử dụng dữ liệu lớn hơn để huấn luyện trước và các phương pháp huấn luyện trước khác nhau như XL-Net [41], RoBERTa [24] trong khi những mô hình khác kết hợp các phương thức khác nhau [37].

Các phương pháp này đã cho phép chuyển giao biểu diễn đã học thông qua huấn luyện trước sang các tác vụ hạ nguồn. Mặc dù những mô hình này hoạt động tốt trên nhiều tác vụ khác nhau, có hai hạn chế chính: 1) chúng tốn kém về mặt tính toán để huấn luyện 2) chúng thường có một số lượng lớn tham số làm tăng đáng kể kích thước mô hình và yêu cầu bộ nhớ.

Ví dụ, mô hình BERT-base đa ngôn ngữ cased có 110M tham số, mô hình GPT-2 nhỏ có 117M tham số [33] và mô hình RoBERTa được huấn luyện trên 160GB dữ liệu [24]. Gần đây, các nhà nghiên cứu đã đề xuất các mô hình BERT 'nhẹ' hơn tận dụng chưng cất kiến thức trong giai đoạn huấn luyện trước và giảm kích thước mô hình BERT [40] hoặc cố gắng cải thiện hiệu quả tham số của mô hình BERT bằng các tối ưu hóa ở lớp embedding và bằng chia sẻ tham số [21].

Tuy nhiên, tất cả các mô hình trên đều dựa trên kiến trúc Transformer [39] mà thành phần chính của nó là cơ chế scaled dot-product self-attention. Lớp này có độ phức tạp tính toán O(n²d) tăng theo bậc hai với độ dài đầu vào (n) và tuyến tính với độ dài kích thước ẩn của mô hình (d).

Tự nhiên có thể thấy làm thế nào huấn luyện cụ thể tác vụ hoặc fine-tuning có thể bị hạn chế khi dữ liệu huấn luyện và tài nguyên tính toán khan hiếm và chuỗi dài. Hơn nữa, chạy suy luận trên và lưu trữ các mô hình như vậy cũng có thể khó khăn trong các tình huống tài nguyên thấp như thiết bị IoT hoặc các trường hợp sử dụng độ trễ thấp.

Do đó, học có giám sát cho các kiến trúc cụ thể tác vụ được huấn luyện từ đầu, đặc biệt là khi có dữ liệu huấn luyện cụ thể miền là hữu ích. Chúng nhẹ và dễ triển khai. Trong công trình này, chúng tôi đề xuất cơ chế multi-head attention dựa trên phân tích ma trận thấp hạng (LAMA), một cơ chế attention gọn nhẹ rẻ hơn về mặt tính toán và hiệu quả hơn về tham số so với các phương pháp trước đây và vượt trội hoặc bằng hiệu suất của các baseline tiên tiến bao gồm các mô hình được huấn luyện trước lớn, với ít tham số hơn.

Trái ngược với các phương pháp trước đây [12,23] dựa trên cơ chế additive attention [2], LAMA dựa trên multiplicative attention [25] thay thế additive attention bằng tích vô hướng để tính toán nhanh hơn. Chúng tôi tiếp tục giới thiệu một phép chiếu bilinear trong khi tính tích vô hướng để nắm bắt sự tương tự giữa một vector ngữ cảnh toàn cục và mỗi từ trong câu [25].

Chức năng của phép chiếu bilinear là nắm bắt tầm quan trọng của từ phụ thuộc ngữ cảnh tinh tế như được chứng thực bởi các công trình trước đây [3]. Tiếp theo, khác với các phương pháp trước đây, chúng tôi sử dụng công thức thấp hạng của ma trận chiếu bilinear dựa trên tích hadamard [17,42] để tạo ra nhiều attention bằng cách truy vấn vector ngữ cảnh toàn cục cho mỗi từ thay vì có một vector đã học khác nhau [23] hoặc ma trận [43].

Trực tiếp, chúng tôi đặt tính toán điểm giữa biểu diễn từ và vector ngữ cảnh như fusion đặc trưng đơn phương thức đến không gian chiều thấp tương tự như các đối tác đa phương thức của nó [22,43]. Mỗi chiều của không gian đặc trưng chiều thấp này có thể được coi là đóng góp của từ đó cho một attention head khác nhau.

Bằng cách kiểm soát chiều của không gian đặc trưng này, các head mới có thể được thêm hoặc loại bỏ. Cuối cùng, chúng tôi thiết kế một cơ chế để thu được embedding câu có giám sát nhận biết ngữ cảnh từ các phân phối attention thu được cho các tác vụ phân loại hạ nguồn.

Chúng tôi đánh giá mô hình của mình trên các tác vụ như phân tích cảm xúc, dự đoán xếp hạng doanh nghiệp và phân loại tin tức. Chúng tôi chỉ ra rằng mô hình được đề xuất học tầm quan trọng từ phụ thuộc ngữ cảnh như các mô hình attention khác. Hơn nữa, mô hình được đề xuất hiệu quả hơn 3 lần về tham số so với các mô hình attention tương đương khác, đặc biệt là bộ mã hóa của mô hình Transformer [39].

Về hiệu suất, mô hình được đề xuất có tính cạnh tranh và bằng hoặc vượt hiệu suất của các baseline mạnh, dựa trên attention và không dựa trên attention. Hơn nữa, chúng tôi trình bày một số phân tích phụ về hiệu quả mô hình và nhu cầu cho nhiều attention head.

Tóm lại, kết quả của chúng tôi chỉ ra rằng mô hình được đề xuất có thể được sử dụng đáng tin cậy như một lựa chọn thay thế multi-head attention gọn nhẹ hơn cho các tác vụ phân loại văn bản có giám sát.

Tổ chức của phần còn lại của bài báo như sau: phần tiếp theo (§2) thảo luận các kết nối với công trình liên quan, tiếp theo là mô tả mô hình được đề xuất (§3), tiếp theo là mô tả tác vụ và đánh giá thực nghiệm (§4). Cuối cùng, trong các phần tiếp theo, kết quả được trình bày cùng với thảo luận của chúng (§5), trước khi kết luận (§6).

## 2. CÔNG TRÌNH LIÊN QUAN

Được dẫn dắt bởi thành công của chúng trong dịch máy neural [2,25], các cơ chế attention hiện được sử dụng phổ biến trong các vấn đề như trả lời câu hỏi [10,13,34], tóm tắt văn bản [3,31], trích xuất sự kiện [27], và huấn luyện các mô hình ngôn ngữ lớn [8,33]. Trong mô hình hóa chuỗi, các cơ chế attention cho phép bộ giải mã học phần nào của chuỗi nó nên "chú ý" đến dựa trên chuỗi đầu vào và đầu ra nó đã tạo ra cho đến nay [2].

### 2.1 Self-Attention

Self-attention, đôi khi được gọi là intra-attention là một cơ chế attention liên quan đến các vị trí khác nhau của một chuỗi duy nhất để tính toán biểu diễn của chuỗi [5]. Self-attention đã được sử dụng thành công trong nhiều tác vụ khác nhau bao gồm đọc hiểu, tóm tắt abstractive, entailment văn bản và học biểu diễn câu độc lập tác vụ [5,23,30,31].

Truyền thống, các phương pháp trên đã phụ thuộc vào Mạng Neural Hồi quy (RNNs) [7,14] để mô hình hóa các phụ thuộc tuần tự và các cơ chế attention được đề xuất để giảm nhẹ vấn đề gradient biến mất bằng cách thiết lập các kết nối ngắn hơn giữa các vị trí nguồn và đích.

Bản chất tuần tự vốn có của RNNs ngăn cản việc song song hóa trong các ví dụ huấn luyện điều này trở thành nút thắt bộ nhớ cho việc batching qua các ví dụ. Gần đây mô hình Transformer được đề xuất [39] thay thế sự phụ thuộc vào RNNs và dựa hoàn toàn vào cơ chế self-attention.

Trong phương pháp này, mọi vị trí chú ý đến mọi vị trí khác và điều chỉnh embedding của nó tương ứng. Tuy nhiên, tính toán dày đặc này là bậc hai theo độ dài đầu vào và tốn nhiều tài nguyên. Phương pháp được đề xuất ngược lại tận dụng RNNs để mô hình hóa các phụ thuộc tuần tự và sử dụng một vector truy vấn toàn cục duy nhất cho mỗi từ đầu vào.

Chúng tôi chỉ ra rằng phương pháp của chúng tôi hiệu quả hơn về mặt tính toán so với bộ mã hóa của mô hình Transformer trên các tác vụ phân loại văn bản và vượt trội về hiệu suất.

### 2.2 Multi-Head Attention

Các mô hình đã được đề xuất tính toán nhiều phân phối attention trên một chuỗi từ duy nhất. Mạng đa view [12] sử dụng một tập hợp tham số khác nhau cho mỗi view dẫn đến tăng số lượng tham số. Lin et al. [23] sử dụng cơ chế additive attention, đây là một phương pháp tổng quát hơn để tính attention giữa các vector truy vấn và khóa và sửa đổi nó để tạo ra nhiều attention để thu được embedding câu ma trận.

Scaled dot product attention được đề xuất bởi Vaswani et al. [39] là một phương pháp trực tiếp và dựa trên tích vô hướng giữa các vector truy vấn và khóa. Phương pháp này đã được chứng minh là rất hiệu quả trong dịch máy [39] và huấn luyện trước mô hình ngôn ngữ [8].

Tuy nhiên để tính toán nhiều attention head, các tham số biến đổi khác nhau được học cho các head khác nhau dẫn đến tăng tham số. Trong công trình này, mặt khác, điểm giữa khóa (ngữ cảnh) và truy vấn (biểu diễn từ) được tính toán bằng ma trận chiếu bilinear theo sau bởi một phương pháp lấy cảm hứng từ pooling bilinear thấp hạng đa phương thức [17] để phân tích ma trận thành hai ma trận thấp hạng để tính toán nhiều phân phối attention trên các từ.

Chúng tôi thấy rằng đây là một cách hiệu quả hơn về tham số để tính toán nhiều attention. Trái ngược với Guo et al. [12] và Vaswani et al. [39] chúng tôi sử dụng phân tích ma trận để giảm nhẹ vấn đề tăng tham số với việc tăng head và mô hình được đề xuất hoạt động vượt trội so với phương pháp của họ.

### 2.3 Phân tích Thấp Hạng

Phân tích thấp hạng đã là một phương pháp phổ biến để giảm kích thước của các lớp ẩn [4,38]. Công trình gần đây đã đạt được cải thiện đáng kể về hiệu quả tính toán thông qua các thủ thuật phân tích [35] và tính toán có điều kiện [20]. Gần đây, phân tích cũng được sử dụng ở lớp embedding của các mô hình ngôn ngữ được huấn luyện trước lớn như BERT [8] để giảm kích thước mô hình [21].

Trong công trình này, chúng tôi sử dụng phân tích cho fusion đặc trưng đơn phương thức để tính điểm attention cho nhiều attention head. Công thức tích Hadamard cho phân tích ma trận được sử dụng để nén lớp multi-head attention. Công thức này có thể được xem như pooling bilinear thấp hạng cho các đặc trưng đơn phương thức dựa trên ý tưởng tương ứng của fusion đặc trưng đa phương thức [22, 43].

## 3. MÔ HÌNH ĐỀ XUẤT

Trong phần này, chúng tôi đưa ra tổng quan về mô hình được đề xuất tiếp theo là mô tả chi tiết của từng thành phần mô hình.

Một tài liệu (một đánh giá sản phẩm hoặc một bài báo tin tức) đầu tiên được tokenize và chuyển đổi thành embedding từ thông qua tra cứu trong ma trận embedding được huấn luyện trước. Embedding của mỗi token được mã hóa thông qua một bộ mã hóa câu Gated Recurrent Unit hai chiều [6] (bi-GRU) để có được chú thích ngữ cảnh của mỗi từ trong tài liệu đó.

Cơ chế attention LAMA sau đó thu được nhiều phân phối attention trên những từ đó bằng cách tính điểm alignment của biểu diễn ẩn của chúng với một vector ngữ cảnh mức từ. Tổng của các biểu diễn từ được weighted bởi các điểm từ nhiều phân phối attention sau đó tạo thành embedding tài liệu ma trận.

Embedding ma trận sau đó được làm phẳng và chuyển đến các lớp hạ nguồn (hoặc một bộ phân loại hoặc một bộ mã hóa khác tùy thuộc vào tác vụ).

Trong phần còn lại của bài báo, các chữ cái đậm viết hoa biểu thị ma trận, các chữ cái đậm nhỏ biểu thị vector và các chữ cái nhỏ biểu thị scalar.

### 3.1 Bộ Mã Hóa Chuỗi

Chúng tôi sử dụng GRU [2] RNN làm bộ mã hóa chuỗi. GRU sử dụng cơ chế gating để theo dõi trạng thái của các chuỗi. Có hai loại gate: reset gate r_t và update gate z_t. Update gate quyết định bao nhiều thông tin quá khứ được giữ lại và bao nhiều thông tin mới được thêm vào.

Tại thời điểm t, GRU tính toán trạng thái mới của nó như:
h_t = (1-z_t)⊙h_{t-1} + z_t⊙h̃_t (1)

và update gate z_t được cập nhật như:
z_t = σ(W_z x_t + U_z h_{t-1} + b_z) (2)

Trạng thái ứng viên RNN h̃_t được tính như:
h̃_t = tanh(W_h x_t + r_t⊙(U_h h_{t-1}) + b_h) (3)

Ở đây r_t là reset gate điều khiển bao nhiều trạng thái quá khứ đóng góp vào trạng thái ứng viên. Nếu r_t bằng không, thì nó quên trạng thái trước đó. Reset gate được cập nhật như sau:
r_t = σ(W_r x_t + U_r h_{t-1} + b_r) (4)

Xem xét một tài liệu D_i chứa T từ. D_i = {w_1, ..., w_t, ..., w_T}. Gọi mỗi từ được ký hiệu bởi w_t, t ∈ [0,T] trong đó mọi từ được chuyển đổi thành vector từ có giá trị thực x_t sử dụng ma trận embedding được huấn luyện trước W_e ∈ R^{d×|V|}, x_t = W_e w_t, t ∈ [1,T] trong đó d là chiều embedding và V là từ vựng.

Ma trận embedding W_e được fine-tune trong quá trình huấn luyện. Lưu ý rằng chúng tôi đã bỏ chỉ số dưới i vì tất cả các phép dẫn xuất đều cho tài liệu thứ i và nó được giả định ngầm trong các phần sau trừ khi có quy định khác.

Chúng tôi mã hóa tài liệu bằng bi-GRU tóm tắt thông tin theo cả hai hướng dọc theo văn bản để có được chú thích ngữ cảnh của một từ. Trong bi-GRU, trạng thái ẩn tại bước thời gian t được biểu diễn như một concatenation của các trạng thái ẩn theo hướng tiến và lùi.

GRU tiến ký hiệu bởi →GRU xử lý tài liệu từ w_1 đến w_T trong khi GRU lùi ký hiệu bởi ←GRU xử lý nó từ w_T đến w_1.

x_t = W_e w_t (5)
→h_t = →GRU(x_t, h_{t-1}, θ) (6a)
←h_t = ←GRU(x_t, h_{t+1}, θ) (6b)

Ở đây chú thích từ h_t được thu được bằng cách concatenating trạng thái ẩn tiến →h_t và trạng thái ẩn lùi ←h_t.

### 3.2 Single-Head Attention

Để giảm nhẹ gánh nặng nhớ các phụ thuộc dài hạn từ GRUs, chúng tôi sử dụng cơ chế global attention [25] trong đó biểu diễn tài liệu được tính toán bằng cách attending đến tất cả các từ trong tài liệu. Gọi h_t là chú thích tương ứng với từ x_t.

Đầu tiên chúng tôi biến đổi h_t sử dụng Multi-Layer Perceptron một lớp (MLP) để thu được biểu diễn ẩn u_t của nó. Chúng tôi giả định priors Gaussian với mean 0 và standard deviation 0.1 trên W_w và b_w.

u_t = tanh(W_w h_t + b_w) (7)

Tiếp theo, để tính toán tầm quan trọng của từ trong ngữ cảnh hiện tại, chúng tôi tính toán mức liên quan của nó đến một vector ngữ cảnh toàn cục c bằng một phép chiếu bilinear.

f_t = c^T W_i u_t (8)

Ở đây, W_i ∈ R^{2h×2h}, là một ma trận chiếu bilinear được khởi tạo ngẫu nhiên và được học chung với các tham số khác trong quá trình huấn luyện. h là chiều của trạng thái ẩn GRU và u_t & c đều có chiều 2h×1 vì chúng tôi đang sử dụng bi-GRU.

Mean của các embedding từ cung cấp một xấp xỉ ban đầu tốt cho ngữ cảnh toàn cục của tài liệu. Chúng tôi khởi tạo c = 1/T ∑_{t=1}^T w_t sau đó được cập nhật trong quá trình huấn luyện. Chúng tôi sử dụng phép chiếu bilinear vì chúng hiệu quả hơn trong việc học các tương tác theo cặp như được chỉ ra trong các công trình trước đây [3].

Trọng số attention cho từ x_t sau đó được tính toán bằng hàm softmax trong đó phép tổng được thực hiện trên tất cả các từ trong tài liệu.

α_t = exp(f_t) / ∑_{t'} exp(f_{t'}) (9)

### 3.3 Cơ chế Multi-Head Attention Dựa trên Phân tích Thấp Hạng

Trong phần này, chúng tôi mô tả cơ chế multi-head attention dựa trên phân tích thấp hạng mới (LAMA). Phân phối attention trong Eq. 9 trên thường tập trung vào một thành phần cụ thể của tài liệu, như một tập hợp các từ kích hoạt đặc biệt.

Vì vậy nó được mong đợi phản ánh một khía cạnh, hoặc thành phần của ngữ nghĩa trong một tài liệu. Loại attention này hữu ích cho các đoạn văn bản nhỏ hơn như tweet hoặc đánh giá ngắn. Đối với các đánh giá lớn hơn có thể có nhiều khía cạnh mô tả đánh giá đó.

Cho điều này chúng tôi giới thiệu một cách mới để tính toán nhiều head của attention nắm bắt các khía cạnh khác nhau.

Giả sử m head của attention cần được tính toán, chúng ta cần m điểm alignment giữa mỗi biểu diễn từ ẩn u_t và vector ngữ cảnh c. Để thu được đầu ra m chiều f_t, chúng ta cần học m ma trận trọng số được cho bởi W = [W_1, ..., W_m] ∈ R^{m×2h×2h} như được chứng minh trong các công trình trước đây.

Mặc dù chiến lược này có thể hiệu quả trong việc nắm bắt các tương tác theo cặp cho mỗi khía cạnh, nó cũng giới thiệu một số lượng lớn tham số có thể dẫn đến overfitting và cũng phát sinh chi phí tính toán cao đặc biệt cho m lớn hoặc h lớn.

Để giải quyết điều này, rank của ma trận W có thể được giảm bằng cách sử dụng phương pháp bilinear thấp hạng để có ít tham số hơn [17,43]. Xem xét một head; ma trận chiếu bilinear W_i trong Eq. 8 có thể được phân tích thành hai ma trận thấp hạng P & Q.

f_t = c^T PQ^T u_t = ∑_{d=1}^k c^T p_d q_d^T u_t = 1^T (P^T c ⊙ Q^T u_t) (10)

trong đó P = [p_1, ..., p_k] ∈ R^{2h×k} và Q = [q_1, ..., q_k] ∈ R^{2h×k} là hai ma trận thấp hạng, ⊙ là tích Hadamard hoặc phép nhân theo từng phần tử của hai vector, 1 ∈ R^k là vector toàn số một và k là chiều tiềm ẩn của các ma trận được phân tích.

Để thu được m điểm, bởi Eq.10, các trọng số cần học là hai tensor bậc ba P = [P_1, ..., P_m] ∈ R^{2h×k×m} và Q = [Q_1, ..., Q_m] ∈ R^{2h×k×m} tương ứng. Không mất tính tổng quát P và Q có thể được tái công thức hóa như các ma trận 2-D P̃ ∈ R^{2h×km} và Q̃ ∈ R^{2h×km} tương ứng với các phép toán reshape đơn giản.

Đặt k=1, tương ứng với phân tích rank-1. Eq.10 có thể được viết như:
f_t = P̃^T c ⊙ Q̃^T u_t (11)

Điều này đưa hai vector đặc trưng u_t ∈ R^{2h}, biểu diễn từ ẩn và c ∈ R^{2h}, vector ngữ cảnh toàn cục vào một không gian con chung và được cho bởi ũ_t và c̃ tương ứng.

f_t ∈ R^m có thể được xem như một vector alignment multi-head cho từ x_t trong đó mỗi chiều của vector có thể được xem như điểm của từ w.r.t. một attention head khác nhau. Để tính toán attention cho một head, điều này tương đương với việc thay thế ma trận chiếu W_i trong Eq. 8 bằng tích ngoài của các vector P̃_i và Q̃_i; các hàng của các ma trận P̃ và Q̃ tương ứng và viết lại nó như tích Hadamard.

Kết quả là mỗi hàng của các ma trận P̃_i và Q̃_i đại diện cho các vector để tính toán điểm cho một head khác nhau.

Vector multi-head attention α_t ∈ R^m được thu được bằng cách tính toán hàm softmax dọc theo độ dài câu:

α_t = exp(f_t) / ∑_{t'} exp(f_{t'}) (12)

Trước khi tính toán softmax, tương tự như các công trình trước đây [17,43], để tăng thêm khả năng của mô hình, chúng tôi áp dụng phi tuyến tính tanh cho f_t. Vì phép nhân theo từng phần tử được giới thiệu, phương sai của mô hình có thể tăng, vì vậy chúng tôi áp dụng một lớp chuẩn hóa l2 qua chiều m.

Mặc dù l2 không hoàn toàn cần thiết vì cả c và u_t đều trong cùng một phương thức, theo kinh nghiệm chúng tôi thấy cải thiện sau khi áp dụng l2. Mỗi thành phần k của α_t là đóng góp của từ x_t cho head thứ k.

Tiếp theo, chúng tôi mô tả cách tính toán này có thể được vector hóa cho mỗi từ trong tài liệu. Gọi H = (h_1, h_2, ..., h_T) là ma trận của tất cả chú thích từ trong câu; H ∈ R^{T×2h}. Ma trận attention cho câu có thể được tính toán như:

A = softmax(l2(tanh(P̃^T C̃ ⊙ Q̃^T H^T))) (13)

trong đó, C̃ ∈ R^{2h×T} là c được lặp lại T lần, một lần cho mỗi từ, l2(x) = x/||x|| và softmax được áp dụng theo hàng. A ∈ R^{m×T} là ma trận attention giữa câu và ngữ cảnh toàn cục với mỗi hàng đại diện cho attention cho một khía cạnh.

Cho A = [α_1, α_2, ..., α_T], ma trận multi-head attention cho câu; A ∈ R^{m×T}. Biểu diễn tài liệu cho head j được cho bởi α_j = {α_{j1}, α_{j2}, ..., α_{jT}} có thể được tính toán bằng cách lấy tổng có trọng số của tất cả chú thích từ.

s_j = ∑_{k=1}^T h_k α_{jk} (14)

Tương tự, biểu diễn tài liệu có thể được tính toán cho tất cả head và được cho trong dạng compact bởi:

S = AH (15)

Ở đây S ∈ R^{m×2h} là embedding câu ma trận và chứa nhiều hàng bằng số lượng head. Mỗi hàng chứa một phân phối attention cho một head mới. Nó được làm phẳng bằng cách concatenating tất cả các hàng để thu được biểu diễn tài liệu d.

Từ biểu diễn tài liệu, xác suất lớp được thu được như sau:
ŷ = softmax(W_c d + b_c) (16)

Loss được tính toán bằng cross entropy:
L(y, ŷ) = ∑_{c=1}^C y_c log(ŷ_c) (17)

trong đó C là số lớp và ŷ_c là xác suất của lớp c.

### 3.4 Điều Chỉnh Disagreement

Để giảm phương sai của mô hình được giới thiệu do phép nhân theo điểm và để khuyến khích đa dạng giữa nhiều attention head, chúng tôi giới thiệu một hạng mục điều chỉnh phụ trợ.

J(θ) = argmin_θ {L(y, ŷ) + λD(A|x, y, θ)} (18)

trong đó A là ma trận attention, θ đại diện cho các tham số mô hình, λ là một siêu tham số và được đặt kinh nghiệm là 0.2 trong bài báo này. L(ŷ, y) là loss cross-entropy và D(A|x, y, θ) là hạng mục điều chỉnh phụ trợ đại diện cho disagreement giữa các attention khác nhau.

Nó hướng dẫn các thành phần attention liên quan để nắm bắt các đặc trưng khác nhau từ các không gian con được chiếu tương ứng. Chúng tôi thử hai điều chỉnh khác nhau i) điều chỉnh trên các vị trí được attend ii) điều chỉnh trên embedding tài liệu.

Đối với loại đầu tiên chúng tôi điều chỉnh hạng mục penalization trong [23] để đại diện cho disagreement giữa các phân phối attention (Eq. 19a). Tiếp theo, chúng tôi điều chỉnh trực tiếp các embedding câu kết quả từ các phân phối attention khác nhau được đại diện bằng độ tương tự cosine của chúng (Eq. 19b). Các embedding càng tương tự, disagreement càng ít.

D_penal = ||AA^T - I||_F^2 (19a)
D_emb = (1/m^2) ∑_{i=1}^m ∑_{j=1}^m (s_i s_j)/(||s_i|| ||s_j||) (19b)

Loss huấn luyện cuối cùng được cho bởi Eq. 18 tổng trên tất cả tài liệu trong một minibatch. Chúng tôi sử dụng thuật toán minibatch stochastic gradient descent [16] với momentum và weight decay để tối ưu hóa hàm loss và thuật toán backpropagation được sử dụng để tính toán gradients.

Hình 1 cho thấy một tài liệu duy nhất và luồng của nó qua các thành phần mô hình khác nhau. Khối giữa minh họa cơ chế attention được đề xuất cho một từ w_t của tài liệu. Nó đầu tiên được biến đổi qua Eq. 7 thành u_t. Song song, một vector ngữ cảnh c được khởi tạo.

Eq. 11 sau đó được sử dụng để tính toán multi-head attention cho từ này. Tính toán attention được vector hóa có thể được thực hiện cho tất cả từ sử dụng Eq. 13 để thu được ma trận attention A sau đó được nhân với ma trận trạng thái ẩn H để thu được embedding cho tài liệu sau đó được chuyển đến bộ phân loại MLP.

### 3.5 Siêu tham số

Chúng tôi sử dụng kích thước embedding từ là 100. Ma trận embedding W_e được huấn luyện trước trên corpus bằng word2vec [29]. Tất cả từ xuất hiện ít hơn 5 lần được loại bỏ. Trạng thái ẩn GRU được đặt h=50, trạng thái ẩn MLP là 512 và chúng tôi áp dụng dropout 0.4 cho lớp ẩn.

Kích thước batch được đặt 32 cho huấn luyện và tốc độ học ban đầu 0.05 được sử dụng. Để early stopping chúng tôi sử dụng patience = 5. Momentum được đặt 0.9 và weight decay 0.0001. Chúng tôi sẽ mở mã nguồn khi được chấp nhận.

### 3.6 Độ Phức Tạp Tính Toán

Các biến khác như bộ mã hóa tài liệu và chiều của trạng thái ẩn được giữ không đổi, độ phức tạp tính toán của mô hình phụ thuộc vào lớp attention. Có thể thấy rằng độ phức tạp tính toán của LAMA tuyến tính theo đầu vào và tuyến tính theo số lượng attention head.

Trong khi đó, cơ chế attention trong bộ mã hóa của mô hình Transformer [39] là bậc hai theo độ dài đầu vào (Bảng 2). Đối với các trường hợp nm < n^2, đây là tình huống phổ biến, cơ chế attention LAMA hiệu quả hơn về mặt tính toán so với self-attention trong bộ mã hóa của mô hình Transformer cho các tác vụ phân loại chuỗi.

## 4. ĐÁNH GIÁ

### 4.1 Bộ Dữ Liệu

Chúng tôi đánh giá hiệu suất của mô hình được đề xuất trên các tác vụ dự đoán xếp hạng doanh nghiệp từ Yelp, dự đoán cảm xúc từ đánh giá phim và phân loại bài báo tin tức theo chủ đề. Bảng 3 đưa ra tổng quan về các bộ dữ liệu và thống kê của chúng.

#### 4.1.1 Yelp
Bộ dữ liệu Yelp gồm 2.7M đánh giá Yelp và xếp hạng người dùng từ 1 đến 5. Cho một đánh giá, mục tiêu là dự đoán xếp hạng được gán bởi người dùng cho cửa hàng kinh doanh tương ứng. Chúng tôi coi tác vụ như phân loại văn bản 5 chiều trong đó mỗi lớp chỉ ra xếp hạng người dùng.

Chúng tôi lấy mẫu ngẫu nhiên 500K cặp đánh giá-sao làm tập huấn luyện và 4,000 cho tập kiểm tra. Các đánh giá được tokenize bằng Spacy tokenizer. Embedding từ 100 chiều được huấn luyện từ đầu trên tập dữ liệu huấn luyện bằng gói phần mềm gensim.

#### 4.1.2 Yelp-Long
Multi-head attention nắm bắt nhiều khía cạnh hữu ích hơn cho việc phân loại xếp hạng chủ quan hơn tức là các đánh giá dài hơn trong đó mọi người thể hiện trải nghiệm của họ một cách chi tiết. Chúng tôi tạo một tập con của bộ dữ liệu YELP chứa tất cả đánh giá dài hơn tức là các đánh giá chứa nhiều hơn 118 token mà chúng tôi thấy là độ dài trung bình của các đánh giá trong bộ dữ liệu.

Tập huấn luyện gồm 175,844 đánh giá, và tập kiểm tra gồm 1,378 đánh giá. Mục tiêu là dự đoán xếp hạng từ tập con trên của bộ dữ liệu Yelp. Chúng tôi gọi bộ dữ liệu này là Yelp-L (Yelp-Long) trong phần còn lại của bài báo vì nó gồm tất cả đánh giá dài hơn.

Chúng tôi giả thuyết rằng có multi-head attention sẽ có lợi trong bối cảnh này trong đó việc tìm kiếm thông tin phức tạp hơn từ các phần khác nhau của văn bản được yêu cầu để đưa ra dự đoán. Các siêu tham số mô hình và cài đặt huấn luyện vẫn giống như trên.

#### 4.1.3 Yelp-Polarity
Bộ dữ liệu polarity đánh giá Yelp [44] được xây dựng bằng cách coi sao 1 và 2 là tiêu cực, và 3 và 4 là tích cực từ bộ dữ liệu Yelp. Đối với mỗi polarity 280,000 mẫu huấn luyện và 19,000 mẫu kiểm tra được lấy ngẫu nhiên. Tổng cộng có 560,000 mẫu huấn luyện và 38,000 mẫu kiểm tra. Bộ dữ liệu này được gọi là Yelp-P trong bài báo.

#### 4.1.4 Đánh Giá Phim
Bộ dữ liệu Đánh giá Phim lớn [26] chứa đánh giá phim cùng với nhãn polarity cảm xúc nhị phân liên kết của chúng. Nó chứa 50,000 đánh giá có tính phân cực cao (điểm ≤4 trên 10 cho đánh giá tiêu cực và điểm ≥7 trên 10 cho đánh giá tích cực) được chia đều thành 25K tập huấn luyện và 25K tập kiểm tra.

Phân phối tổng thể của nhãn được cân bằng (25K pos và 25K neg). Trong toàn bộ bộ sưu tập, không quá 30 đánh giá được cho phép cho bất kỳ bộ phim nào vì đánh giá cho cùng một bộ phim có xu hướng có xếp hạng tương quan.

Hơn nữa, tập huấn luyện và kiểm tra chứa một tập hợp các bộ phim rời nhau, vì vậy không có hiệu suất đáng kể nào được thu được bằng cách ghi nhớ các thuật ngữ độc nhất phim và liên kết của chúng với các nhãn quan sát được. Chúng tôi gọi bộ dữ liệu này là IMDB trong phần còn lại của bài báo.

#### 4.1.5 News Aggregator
Bộ dữ liệu này [9] chứa tiêu đề, URL và danh mục cho các câu chuyện tin tức được thu thập bởi một bộ tổng hợp web từ ngày 10 tháng 3 năm 2014 đến ngày 10 tháng 8 năm 2014. Các danh mục tin tức được bao gồm trong bộ dữ liệu này bao gồm kinh doanh; khoa học và công nghệ; giải trí; và sức khỏe.

Các bài báo tin tức khác nhau tham chiếu đến cùng một mục tin tức cũng được phân loại cùng nhau. Cho một bài báo tin tức, tác vụ là phân loại nó vào một trong bốn danh mục. Bộ dữ liệu huấn luyện gồm 151,328 bài báo và bộ dữ liệu kiểm tra gồm 32,428. Độ dài token trung bình là 352.

#### 4.1.6 Reuters
Bộ dữ liệu này được lấy từ Reuters-21578 Text Categorization Collection. Đây là một bộ sưu tập các tài liệu xuất hiện trên newswire Reuters vào năm 1987. Các tài liệu được tập hợp và lập chỉ mục với các danh mục.

Chúng tôi đánh giá trên bộ dữ liệu Reuters8 gồm các bài báo tin tức về 8 chủ đề bao gồm acq, crude, earn, grain, interest, money-fx, ship, trade.

### 4.2 Phương Pháp So Sánh

Để đánh giá mô hình được đề xuất so với các phương pháp hiện có, chúng tôi sử dụng nhiều kiến trúc mô hình khác nhau làm baseline so sánh. Chúng tôi sử dụng BERT [8] như một trong các baseline. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng mô hình bert-base uncased được huấn luyện trước có 12 lớp, kích thước trạng thái ẩn 768, 12 head, 110M tham số và được huấn luyện trên văn bản tiếng Anh viết thường.

Chúng tôi fine-tune nó trên các bộ dữ liệu của chúng tôi trong 2 epoch sử dụng optimizer ADAM [19] với tốc độ học 5e-6.

Đã được chỉ ra rằng trung bình của embedding từ có thể tạo nên một baseline rất mạnh [36]. Chúng tôi sử dụng điều này như một baseline khác và gọi nó là AVG trong bài báo.

Chúng tôi sử dụng nhiều mô hình có và không có attention làm baseline khác. Các baseline đại diện mạnh cho các kiến trúc mô hình khác nhau được chọn; như CNN với max-over-time pooling [18], mô hình bidirectional GRU [7] với maxpooling được gọi là BiGRU.

Chúng tôi thử nghiệm với n=1 và n=2 lớp GRU và thấy rằng n=1 hội tụ nhanh hơn và dẫn đến hiệu suất tốt hơn. Đối với baseline CNN chúng tôi sử dụng 3 kernel kích thước 3, 4 và 5 với 100 filter mỗi cái.

Trong số các mô hình multi-head dựa trên attention có giám sát, chúng tôi sử dụng Self Attention Network được đề xuất bởi Lin et al. [23]. Chúng tôi gọi baseline này là SAN. Đối với mỗi tác vụ, chúng tôi tìm kinh nghiệm số lượng head cho hiệu suất tốt nhất.

Chúng tôi sử dụng Encoder 1 lớp của mô hình Transformer (TE) [39] với 8 attention head như một baseline khác. Chúng tôi sử dụng d_model = 512 sao cho d_model/heads = 64.

Chúng tôi sử dụng hai biến thể của mô hình được đề xuất để so sánh hiệu suất với các baseline trên. Đối với biến thể đầu tiên chúng tôi khởi tạo c với mean của embedding từ trong câu để cung cấp ngữ cảnh toàn cục của câu. Chúng tôi gọi baseline này là LAMA+ctx.

Trong biến thể khác, chúng tôi khởi tạo ngẫu nhiên c và học chung nó với các tham số mô hình khác (LAMA). Mô hình của chúng tôi với điều chỉnh embedding được gọi là LAMA+ctx+de và mô hình của chúng tôi với điều chỉnh trên vị trí được gọi là LAMA+ctx+dp.

Đối với các mô hình LAMA và LAMA+ctx chúng tôi xác định kinh nghiệm số lượng attention head tối ưu để có được hiệu suất tốt nhất.

## 5. KẾT QUẢ

Bảng 4 báo cáo độ chính xác của mô hình tốt nhất trên tập kiểm tra sau khi thực hiện cross-validation 3-fold. Mô hình được đề xuất với khởi tạo ngữ cảnh toàn cục LAMA+ctx vượt trội so với mô hình SAN [23] trên tất cả tác vụ từ 3.3% (Reuters) đến 8.2% (IMDB).

Điều này là do thực tế rằng trong tính toán attention, kiến trúc mô hình được đề xuất có điều kiện để truy cập ngữ cảnh toàn cục của câu trong khi đối với SAN không có điều kiện như vậy.

Ngoại suy attention trên các đoạn văn bản lớn hơn chúng ta có attention đồng nhất trên tất cả từ, tương đương với không có attention hoặc ưu tiên bằng nhau cho tất cả từ. Đây là những gì một BiGRU đơn giản thực hiện (trong bối cảnh ngữ cảnh và trung bình của embedding từ trong bối cảnh không ngữ cảnh).

Chúng tôi lưu ý rằng LAMA+ctx vượt trội so với BiGRU 2.0% (News), 12.2% (Reuters) 7.9% (Yelp) và 9.4% (Yelp-L) và 2.7% (IMDB).

Các mô hình của chúng tôi vượt trội so với Transformer Encoder (TE) trên tất cả tác vụ. Cần lưu ý rằng cải thiện hiệu suất này cũng đi kèm với ít tham số hơn TE (Bảng 5). Khi so sánh với các mô hình ngôn ngữ được huấn luyện trước lớn được fine-tune như BERT, chúng tôi thấy rằng LAMA vượt trội so với BERT trên các bộ dữ liệu News, Reuters, Yelp và IMDB.

Trên các bộ dữ liệu YELP-L và YELP-P, BERT vượt trội so với LAMA. Tuy nhiên, cần lưu ý rằng bên cạnh việc được huấn luyện trên các corpus văn bản quy mô lớn và có dung lượng bộ nhớ lớn so với LAMA, các mô hình BERT cũng mất thời gian dài hơn để huấn luyện trước.

Ví dụ, đối với Yelp-P phải mất 12.5 giờ để huấn luyện mô hình chỉ cho 1 epoch so với 20 phút cho LAMA và đối với Yelp phải mất 8.5 giờ so với 25 phút cho LAMA trên một GPU Nvidia Tesla P100.

Đối với baseline không ngữ cảnh của trung bình embedding từ có cải thiện 1.4% (News), 22.4% (Reuters) 9.8% (Yelp), 11.4% (Yelp-L) và 3.0% (IMDB) cho thấy rằng thông tin ngữ cảnh được nắm bắt bởi các mô hình BiGRU hoặc CNN thực sự quan trọng cho các tác vụ.

So với các mô hình CNN có thể quan sát được cải thiện 1% (News), 1.4% (Reuters) và 3.3% (Yelp), và 3.0% (IMDB). Trên bộ dữ liệu Yelp-L mô hình của chúng tôi và CNN hoạt động tương tự. Tuy nhiên, mô hình được đề xuất có thể diễn giải hơn và cho tùy chọn kiểm tra các từ khóa được attend.

Cuối cùng, kết quả của chúng tôi cho thấy rằng sử dụng điều chỉnh disagreement trên LAMA làm xấu đi hiệu suất nói chung.

Từ các kết quả trên có thể lưu ý rằng LAMA là một mô hình có giám sát cạnh tranh, có thể diễn giải và gọn nhẹ cho các tác vụ phân loại văn bản.

### 5.1 Tham Số vs Head

Trong các mô hình dựa trên Transformer, lớp attention có thể chứng minh là nút thắt bộ nhớ khi tài nguyên bị hạn chế. Trong phần này, chúng tôi so sánh số lượng tham số có thể huấn luyện của mô hình được đề xuất (LAMA) và Transformer Encoder (TE).

Bảng 5 cho thấy sự tăng số lượng tham số (trục y) khi số lượng attention head được thay đổi là 2, 4, 8, 16, 32, 64. Lũy thừa của 2 được chọn vì TE yêu cầu số lượng attention head chia hết cho d_model [39].

Lưu ý rằng các tham số được báo cáo cũng chứa tham số GRU & embedding cho LAMA và tham số lớp feed forward & position embedding cho TE, mặc dù những tham số này không phụ thuộc vào số lượng attention head.

Để đảm bảo so sánh công bằng, chúng tôi chọn chiều ẩn BiGRU và d_model đều là 512. Chiều lớp ẩn của lớp phân loại câu cuối cùng là 1024, giống nhau cho cả hai mô hình.

Có thể quan sát được rằng số lượng tham số trong TE không đổi cho tất cả head điều này phù hợp với định nghĩa của nó, vì với việc tăng head; các tham số chiếu Key, Value và Query được thu nhỏ. Đối với LAMA, số lượng tham số chỉ tăng nhẹ với việc tăng số lượng head vì P & Q là các ma trận tham số duy nhất phụ thuộc vào số lượng attention head m và kích thước tăng (tuyến tính).

Quan trọng hơn, cần lưu ý rằng đối với số lượng head cho hầu hết các trường hợp sử dụng thực tế, mô hình được đề xuất LAMA gọn gàng hơn gần 3 lần so với Transformer Encoder.

### 5.2 Thời Gian Chạy

Cần lưu ý rằng vì LAMA được áp dụng trên RNN, tính toán là tuần tự và attention không thể được tính toán trừ khi tất cả trạng thái ẩn RNN có sẵn. Do đó, thời gian tính toán tăng tuyến tính với độ dài đầu vào và bậc hai với chiều trạng thái ẩn (O(nh²)).

Khi các trạng thái ẩn RNN có sẵn, độ phức tạp của lớp attention LAMA là O(nmh). So với đó, độ phức tạp của cơ chế self-attention trong Transformer là O(n²d), trong đó d là chiều trạng thái ẩn. Nó tăng bậc hai với việc tăng độ dài đầu vào và tuyến tính với chiều trạng thái ẩn.

Trong phần này, chúng tôi so sánh thời gian chạy của LAMA và Transformer Encoder 1 lớp. Để so sánh công bằng, chúng tôi so sánh thời gian chạy của LAMA tự do khỏi RNN. Chúng tôi đề xuất, LAMA Encoder - một mô hình không sử dụng RNN để mô hình hóa các phụ thuộc tuần tự và hoạt động trực tiếp trên embedding từ của đầu vào.

Nghĩa là, ma trận trạng thái ẩn H trong Eq. 13 được điền bởi embedding từ được huấn luyện trước. Hình 2 cho thấy thời gian chạy trung bình mỗi epoch (trung bình trên 10 epoch) của LAMA Encoder (LE) và Transformer Encoder (TE) khi độ dài chuỗi được tăng từ 50 đến 250 cho các bộ dữ liệu IMDB và Reuters.

Có thể thấy từ hình rằng TE tốn kém hơn về mặt tính toán mỗi epoch so với LE. LE cũng là một mô hình cạnh tranh với độ chính xác kiểm tra 83.0% trên IMDB và 93.9% trên Reuters so với 81.7% (IMDB) và 90.1% (REUTERS) của TE.

### 5.3 Tham Số vs Độ Chính Xác

Trên bộ dữ liệu YELP-P mô hình của chúng tôi bị vượt trội bởi BERT 1.8%. Tuy nhiên đáng lưu ý là chi phí của cải thiện hiệu suất này. Hình 3 cho thấy độ chính xác của các mô hình khác nhau và các tham số tương ứng của chúng trên bộ dữ liệu YELP-P.

Có thể thấy rằng LAMA+ctx vượt trội so với SAN & TE với ít tham số hơn (sự khác biệt là tuyến tính). BERT vượt trội nhẹ so với LAMA+ctx tuy nhiên với sự tăng tham số hơn một bậc độ lớn.

### 5.4 Trọng Số Attention Theo Ngữ Cảnh

Để xác minh rằng mô hình của chúng tôi nắm bắt tầm quan trọng từ phụ thuộc ngữ cảnh và có thể diễn giải, chúng tôi thực hiện phân tích định tính. Chúng tôi vẽ phân phối của trọng số attention của các từ tích cực 'amazing', 'happy' và 'recommended' và các từ tiêu cực 'poor', 'terrible' và 'worst' từ tập kiểm tra của bộ dữ liệu Yelp như được hiển thị trong Hình 4.

Chúng tôi vẽ phân phối khi được điều kiện hóa trên xếp hạng của các đánh giá từ 1 đến 5. Có thể thấy từ Hình 4 rằng trọng số của các từ tích cực tập trung ở đầu thấp trong các đánh giá với xếp hạng 1 (đường cong xanh).

Khi xếp hạng tăng, phân phối trọng số chuyển sang phải. Điều này chỉ ra rằng các từ tích cực đóng vai trò quan trọng hơn đối với các đánh giá có xếp hạng cao hơn. Xu hướng ngược lại đối với các từ tiêu cực trong đó các từ có ý nghĩa tiêu cực có trọng số attention thấp hơn đối với các đánh giá có xếp hạng 5 (đường cong tím).

Tuy nhiên, có một vài ngoại lệ. Ví dụ, trực quan rằng 'amazing' nhận trọng số cao cho các đánh giá có xếp hạng cao nhưng nó cũng nhận trọng số cao cho các đánh giá có xếp hạng 2 (đường cong cam).

Điều này là do, kiểm tra bộ dữ liệu Yelp chúng tôi thấy rằng 'amazing' xuất hiện khá thường xuyên với từ 'not' trong các đánh giá có xếp hạng 2; 'above average but not amazing', 'was ok but not amazing'. Mô hình của chúng tôi nắm bắt ngữ cảnh mức cụm từ này và gán trọng số tương tự cho 'not' và 'amazing'.

'not' là một từ tiêu cực nhận trọng số cao cho xếp hạng thấp hơn và do đó 'amazing' cũng vậy. Tương tự, các ngoại lệ khác như 'terrible' cho xếp hạng 4 có thể được giải thích do thực tế rằng khách hàng có thể không thích một khía cạnh của doanh nghiệp như dịch vụ của họ nhưng thích khía cạnh khác như thức ăn.

Để minh họa thêm tầm quan trọng từ phụ thuộc ngữ cảnh, Bảng 6 liệt kê các từ khóa được attend hàng đầu cho các bộ dữ liệu Yelp và Reuters. Lưu ý rằng các so sánh như '100 stars' xuất hiện trong danh sách là các chỉ báo mạnh mẽ của cảm xúc của một đánh giá.

### 5.5 Tại Sao Nhiều Head

Các công trình trước đây đã chỉ ra rằng nhiều head hơn không nhất thiết dẫn đến hiệu suất tốt hơn cho các tác vụ dịch máy [28]. Trong phân tích này, chúng tôi tìm cách trả lời một câu hỏi tương tự cho phân loại văn bản.

Chúng tôi đánh giá hiệu suất mô hình khi chúng tôi thay đổi số lượng attention head m từ 1 đến 25. Cụ thể, chúng tôi vẽ độ chính xác validation vs. epoch cho các giá trị khác nhau của m, cho các bộ dữ liệu Yelp-L và IMDB.

Chúng tôi thay đổi m từ 1 đến 20 để có được 5 mô hình với m=1, m=5, m=10, m=15 và m=20. Các biểu đồ được hiển thị trong Hình 5. Từ hình chúng ta có thể thấy rằng đối với bộ dữ liệu Yelp-L hiệu suất mô hình đạt đỉnh cho m=15 và sau đó bắt đầu giảm cho m=20.

Chúng ta có thể thấy rõ sự khác biệt đáng kể giữa m=1 và m=20, cho thấy rằng có cơ chế attention đa khía cạnh giúp ích. Đối với bộ dữ liệu IMDB mô hình với m=15 hoạt động tốt nhất trong khi mô hình với m=1 hoạt động tệ nhất mặc dù hiệu suất cho m=5;15;20 tương tự.

## 6. KẾT LUẬN

Trong bài báo này chúng tôi đã trình bày một cơ chế multi-head attention gọn nhẹ mới và minh họa hiệu quả của nó trên các benchmark phân loại văn bản. Phương pháp được đề xuất tính toán nhiều phân phối attention trên các từ dẫn đến biểu diễn câu theo ngữ cảnh.

Kết quả cho thấy rằng cơ chế này hoạt động tốt hơn so với một số phương pháp khác với ít tham số hơn. Chúng tôi cũng xác minh rằng mô hình nắm bắt tầm quan trọng từ phụ thuộc ngữ cảnh.

Chúng tôi hình dung hai hướng cho công việc tương lai - 1) Hiện tại, mô hình dựa vào RNNs làm cho nó chậm hơn do tính toán tuần tự của chúng. Chúng tôi tìm cách điều tra các cách thích ứng cơ chế được đề xuất trong các mô hình ngôn ngữ tự giám sát kiểu Transformer như BERT, XLNet v.v. mà không phụ thuộc vào RNNs bằng cách kết hợp embedding vị trí [11] để học biểu diễn ngôn ngữ nhanh hơn và hiệu quả hơn;

2) hiện tại, mô hình sử dụng một vector ngữ cảnh toàn cục duy nhất làm vector truy vấn gộp chung toàn bộ câu thành một vector có thể dẫn đến mất thông tin. Các mô hình Transformer mặt khác sử dụng ngữ cảnh chi tiết bằng cách truy vấn mọi từ trong chuỗi cho mỗi từ ứng viên.

Mặc dù điều này có thể giúp phát triển các kết nối trực tiếp giữa các từ liên quan, nó có thể trở nên dư thừa. Trong công việc tương lai, chúng tôi tìm cách điều tra các cách kết hợp truy vấn mức cụm từ như một trung tâm giữa một vector ngữ cảnh toàn cục duy nhất như phương pháp được đề xuất và truy vấn chi tiết như Transformer cung cấp sự cân bằng giữa độ phức tạp và ngữ cảnh.
