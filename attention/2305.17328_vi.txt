# 2305.17328.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2305.17328.pdf
# Kích thước tệp: 12108957 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Zero-TPrune: Cắt tỉa Token Không cần Huấn luyện thông qua Khai thác Đồ thị Attention trong Transformers Đã được Huấn luyện sẵn
Hongjie Wang, Bhishma Dedhia, Niraj K. Jha
Đại học Princeton
Princeton, NJ 08540, USA
{hongjiewang, bdedhia, jha }@princeton.edu
Tóm tắt
Việc triển khai các mô hình Transformer trên các thiết bị biên ngày càng trở nên thách thức do chi phí suy luận tăng theo cấp số nhân, tỷ lệ thuận với bình phương số lượng token trong chuỗi đầu vào. Cắt tỉa token là một giải pháp mới nổi để giải quyết thách thức này do tính dễ triển khai trên các backbone Transformer khác nhau. Tuy nhiên, hầu hết các phương pháp cắt tỉa token đều yêu cầu tinh chỉnh tốn kém về mặt tính toán, điều này không mong muốn trong nhiều trường hợp triển khai biên. Trong công trình này, chúng tôi đề xuất Zero-TPrune, phương pháp zero-shot đầu tiên xem xét cả tầm quan trọng và sự tương đồng của token trong việc thực hiện cắt tỉa token. Nó tận dụng đồ thị attention của các mô hình Transformer đã được huấn luyện sẵn để tạo ra phân phối tầm quan trọng cho các token thông qua thuật toán Weighted Page Rank (WPR) đề xuất của chúng tôi. Phân phối này tiếp tục hướng dẫn việc phân vùng token để cắt tỉa hiệu quả dựa trên sự tương đồng. Do loại bỏ overhead tinh chỉnh, Zero-TPrune có thể cắt tỉa các mô hình lớn với chi phí tính toán không đáng kể, chuyển đổi giữa các cấu hình cắt tỉa khác nhau mà không tốn chi phí tính toán, và thực hiện điều chỉnh siêu tham số một cách hiệu quả. Chúng tôi đánh giá hiệu suất của Zero-TPrune trên các tác vụ thị giác bằng cách áp dụng nó vào các backbone vision Transformer khác nhau và kiểm tra chúng trên ImageNet. Không cần tinh chỉnh, Zero-TPrune giảm chi phí FLOPs của DeiT-S 34.7% và cải thiện throughput 45.3% chỉ với 0.4% mất độ chính xác. So với các phương pháp cắt tỉa tiên tiến yêu cầu tinh chỉnh, Zero-TPrune không chỉ loại bỏ nhu cầu tinh chỉnh sau cắt tỉa mà còn làm vậy chỉ với 0.1% mất độ chính xác. So với các phương pháp cắt tỉa không cần tinh chỉnh tiên tiến, Zero-TPrune giảm mất độ chính xác tới 49% với ngân sách FLOPs tương tự.
Trang web dự án: https://jha-lab.github.io/zerotprune.
1. Giới thiệu
Kiến trúc Transformer [37] đã nổi lên như một công cụ chính thực tế của các mô hình học máy đương đại, cho thấy khả năng tổng quát hóa ấn tượng trên nhiều tác vụ bao gồm Thị giác Máy tính (CV) [34], xử lý ngôn ngữ tự nhiên (NLP) [10], robot [31], và trò chơi [26]. Trung tâm của kiến trúc là cơ chế multi-headed self-attention tự động tổng hợp các token được xử lý song song, tạo ra một khung tính toán đa năng rất hiệu quả. Ý nghĩa đặc biệt rõ ràng trong trường hợp CV nơi khả năng của Transformer trong việc tiếp thu các trừu tượng phong phú từ dữ liệu quy mô lớn tạo điều kiện chuyển giao mạnh mẽ cho các tác vụ downstream, vượt trội hơn các Mạng Neural Tích chập (CNNs) tiên tiến [12].
Các nghiên cứu về quy luật thang đo thực nghiệm cho Vision Transformers (ViTs) [41] chỉ ra khả năng cải thiện hiệu suất mô hình với khả năng mô hình; các mô hình gần đây thực sự đã được mở rộng tới hàng tỷ tham số. Trong khi việc mở rộng mô hình mang lại lời hứa về khả năng tổng quát hóa đáng chú ý, nó đặt ra những trở ngại nghiêm trọng cho việc triển khai các kiến trúc như vậy trên các thiết bị có hạn chế tính toán như edge và thực hiện khối lượng công việc suy luận thời gian thực dưới năng lượng và bộ nhớ hạn chế. Làm thế nào để giảm độ phức tạp tính toán của forward pass trong khi vẫn duy trì sự phong phú của các biểu diễn đã học? Để đạt được mục tiêu này, Token Pruning mở ra một hướng đi đầy hứa hẹn. Rút ra một sự tương tự đơn giản với hệ thống thị giác con người, khi cố gắng nhận dạng một con chim kỳ lạ đậu trên cửa sổ vào một buổi chiều bình yên, chúng ta có xu hướng cắt tỉa những chi tiết thị giác không quan trọng như tách trà nóng hổi nằm gần đó, những người đi bộ nhàn nhã trên lối đi hoặc tán lá được ánh nắng mặt trời chiếu sáng ở phía sau. Các attention heads tạo ra độ phức tạp tính toán bậc hai liên quan đến độ dài chuỗi đầu vào. Do đó, việc cắt tỉa các token không quan trọng có thể dẫn đến tăng tốc đáng kể, đặc biệt trong trường hợp các chuỗi dài hơn. Vì việc cắt tỉa token chỉ cắt tỉa thông tin đi qua các lớp tuần tự của Transformer và không đòi hỏi các sửa đổi kiến trúc cho backbone, nó có thể được triển khai rộng rãi trên hầu hết các backbone Transformer và bất kỳ phần cứng tính toán nào đều có thể khai thác đầy đủ tính thưa thớt kết quả. Tuy nhiên, hầu hết các phương pháp cắt tỉa token hiện có dựa vào các mô-đun đánh giá token phải được huấn luyện cùng với backbone, đòi hỏi việc huấn luyện lại hoặc tinh chỉnh tốn kém về mặt tính toán để triển khai. Điều này không thực tế cho các ứng dụng và người dùng edge, do sự khan hiếm tài nguyên tính toán. Ví dụ, phương pháp cắt tỉa token tiên tiến DynamicViT [30] yêu cầu 150 giờ tinh chỉnh trên GPU NVIDIA A100 để cắt tỉa mô hình DeiT-S [36]. Hơn nữa, bộ nhớ và tài nguyên tính toán có sẵn có thể khác nhau rất nhiều giữa các thiết bị edge; chúng cũng có thể có sự biến đổi lớn về yêu cầu throughput. Các phương pháp cắt tỉa yêu cầu tinh chỉnh cần huấn luyện mô hình lặp đi lặp lại dưới các cấu hình cắt tỉa khác nhau do ràng buộc phần cứng áp đặt, như được hiển thị trong Hình 1, làm cho quá trình cắt tỉa thậm chí còn đắt đỏ hơn. Ngoài ra, các phương pháp này không thực tế để cắt tỉa các mô hình rất lớn do overhead tính toán cao của việc huấn luyện sau cắt tỉa. Ví dụ, hàng nghìn giờ GPU A100 cần thiết để áp dụng DynamicViT [30] cho các mô hình DeiT-B và DeiT-L [36].
Trong công trình này, chúng tôi đề xuất một phương pháp cắt tỉa token zero-shot không cần huấn luyện gọi là Zero-TPrune. Làm thế nào để cắt tỉa token mà không cần tinh chỉnh? Soft attention giữa các token tạo ra một đồ thị có hướng với các token làm nút và attention làm cạnh. Độ mạnh của cạnh chỉ ra giá trị attention. Chúng tôi đặt giả thiết và sau đó chỉ ra thông qua một tập hợp thí nghiệm nghiêm ngặt và toàn diện rằng đồ thị attention là một nguồn thông tin phong phú để suy luận các token quan trọng và, ngược lại, các token có thể dễ dàng bị cắt tỉa. Làm thế nào để xác định các token quan trọng từ đồ thị attention? Các trọng số của các cạnh có hướng trên đồ thị attention có thể được diễn giải như khối lượng định tuyến thông tin giữa các nút. Sử dụng giả định cơ bản rằng các token quan trọng khác chú ý đến các token quan trọng, chúng tôi lặp đi lặp lại gán tầm quan trọng tương đối cho các token. Các phương pháp xếp hạng như vậy [4] đã được sử dụng rộng rãi bởi các công cụ tìm kiếm để tổ chức các trang web trên Internet. Có thể khai thác thêm sự dư thừa giữa các token không? Các thí nghiệm của chúng tôi cho thấy các token thường học các trừu tượng tương tự và, do đó, các bản sao của cùng một đặc trưng có thể bị cắt tỉa mà không mất thông tin. Chúng tôi bổ sung xếp hạng tầm quan trọng với cắt tỉa dựa trên sự tương đồng để tính đến các token tương tự. Mặc dù Zero-TPrune có khả năng được áp dụng cho bất kỳ tác vụ dựa trên Transformer nào, chúng tôi tập trung vào các tác vụ thị giác để đánh giá hiệu suất của nó trong bài báo này.
Các đóng góp chính của công trình này có thể được tóm tắt như sau. (1) Chúng tôi trình bày Zero-TPrune, một phương pháp cắt tỉa token zero-shot hiệu quả tận dụng khả năng nhận dạng đặc trưng (xem xét ma trận attention như ma trận kề của đồ thị có hướng) của các Transformers đã được huấn luyện sẵn. Nó khai thác cả tầm quan trọng và sự tương đồng của token để thực hiện cắt tỉa. (2) Chúng tôi sử dụng tín hiệu đồ thị để biểu diễn phân phối điểm tầm quan trọng trên các token và đề xuất thuật toán Weighted Page Rank (WPR) để suy luận các token không quan trọng trong quá trình gán tầm quan trọng lặp. Sơ đồ lặp này giảm nhiễu từ các token không quan trọng trong quá trình gán. (3) Được hướng dẫn bởi phân phối tầm quan trọng, chúng tôi phân vùng token thành hai nhóm và thực hiện cắt tỉa dựa trên sự tương đồng. Việc phân vùng phụ thuộc đầu vào kiểm soát phân phối tầm quan trọng của các token được cắt tỉa bởi thước đo tương đồng. (4) Chúng tôi áp dụng Zero-TPrune và các phương pháp baseline vào các backbone Transformer khác nhau và đánh giá hiệu suất của chúng trên ImageNet [9]. Các backbone được sử dụng bao gồm DeiT [36], LV-ViT [18], AugReg [33], v.v. So với các phương pháp cắt tỉa Transformer tiên tiến yêu cầu tinh chỉnh, Zero-TPrune loại bỏ nhu cầu tinh chỉnh sau khi cắt tỉa DeiT-S chỉ với khoảng 0.1% giảm độ chính xác trong khi đạt được cùng mức tiết kiệm FLOPs. Hơn nữa, Zero-TPrune vượt trội hơn các phương pháp không cần tinh chỉnh tiên tiến về cả độ chính xác và throughput. Zero-TPrune giảm mất độ chính xác 33% trên DeiT-S khi so sánh với các phương pháp không cần tinh chỉnh tiên tiến. Về throughput, Zero-TPrune cung cấp 45.3% tăng tốc off-the-shelf chỉ với chi phí 0.4% độ chính xác.
2. Các Công trình Liên quan
Trong vài năm đầu sau khi mô hình Transformer được đề xuất vào năm 2017, nó chủ yếu được sử dụng trong lĩnh vực NLP [10]. ViT [12] là công trình đầu tiên áp dụng trực tiếp kiến trúc Transformer chỉ encoder cho các patch hình ảnh không chồng lấp trong tác vụ phân loại hình ảnh mà không sử dụng bất kỳ phép toán tích chập nào. So với các CNN tiên tiến, ViT đã có thể đạt được hiệu suất tốt hơn thông qua huấn luyện sẵn quy mô lớn. DeiT [36] là một Transformer khác không có tích chập được huấn luyện chỉ trên ImageNet [9] và đạt được hiệu suất tốt hơn ViT bằng cách dựa vào một số kỹ thuật huấn luyện. Cả ViT và các kiến trúc tiếp theo đều chia hình ảnh đầu vào thành nhiều patch hình ảnh không chồng lấp và chuyển đổi chúng thành token để xử lý tiếp. Điều này cung cấp một chiều mới cho việc khai thác tính thưa thớt khá khác với các kỹ thuật tăng cường tính thưa thớt được sử dụng trong CNNs.
Hầu hết các công trình cắt tỉa token trước đây tập trung vào các tác vụ NLP, bao gồm PoWER-BERT [15], Length-Adaptive Transformer [19], SpAtten [39], TR-BERT [42], và Learned Token Pruning [20]. Đối với các tác vụ CV, một công trình cắt tỉa token điển hình là DynamicViT [30]. Nó chèn các mô-đun dự đoán giữa các khối transformer để dự đoán và loại bỏ các token ít thông tin hơn. Các mô-đun dự đoán là các mạng neural có thể được huấn luyện cùng với backbone vision Transformer. Thay vì sử dụng chiến lược xác định để cắt tỉa token, A-ViT [43] giới thiệu một quá trình cắt tỉa ngẫu nhiên. Nó sử dụng các mô-đun dừng thích ứng để tính xác suất dừng cho mỗi token. Một token bị cắt tỉa (tức là bị loại bỏ) khi đạt điều kiện dừng. Kết quả là, số lượng token giảm dần, dẫn đến suy luận nhanh hơn. Các công trình gần đây khác về cắt tỉa token cho ViT bao gồm SPViT [21], TPS [40], Adaptive Sparse ViT [24], DToP [35], và HeatViT [11]. Mặc dù các phương pháp cắt tỉa được đề cập ở trên yêu cầu ít hoặc thậm chí không có tham số bổ sung để cắt tỉa, chúng đòi hỏi tinh chỉnh tốn kém về mặt tính toán sau cắt tỉa. Ngược lại, Zero-TPrune đề xuất của chúng tôi có thể loại bỏ quá trình huấn luyện sau cắt tỉa chỉ với 0.1% giảm độ chính xác.
Có một vài công trình trước đây đã khám phá việc cắt tỉa token mà không yêu cầu tinh chỉnh. ATS [14] sử dụng phép biến đổi nghịch đảo để thực hiện lấy mẫu token thích ứng dựa trên phân phối điểm tầm quan trọng. Khi các điểm tầm quan trọng tập trung vào một số token, số lượng token được lấy mẫu tự động giảm. Tuy nhiên, ATS chỉ sử dụng xác suất attention của token phân loại (CLS) trong ma trận attention và bỏ qua ảnh hưởng của sự tương đồng giữa các token. ToMe [3], mặt khác, tập trung vào việc hợp nhất token thay vì cắt tỉa chúng, do đó giảm overhead suy luận của các Transformers đã được huấn luyện sẵn mà không cần tinh chỉnh. Các token được hợp nhất dần dần dựa trên sự tương đồng của chúng khi các lớp trở nên sâu hơn. Tuy nhiên, ToMe chỉ dựa vào các vector embedding từ các mô hình đã được huấn luyện sẵn và quá trình khớp của nó thiếu hướng dẫn phù hợp (chi tiết thêm trong Phần 3.3). Ngược lại, Zero-TPrune hiệu quả sử dụng cả ma trận attention hoàn chỉnh và các vector embedding từ các Transformers đã được huấn luyện sẵn, đồng thời xem xét tầm quan trọng và sự tương đồng của token.
3. Phương pháp luận
Trong phần này, chúng tôi đầu tiên cung cấp tổng quan về Zero-TPrune trong Phần 3.1, sau đó mô tả các thành phần của nó, I-stage (Phần 3.2) và S-stage (Phần 3.3). Lưu ý rằng Zero-TPrune có khả năng khả vi, điều này cho phép mô hình đã cắt tỉa có thể được tinh chỉnh thêm để có hiệu suất tốt hơn. Mô hình huấn luyện sau cắt tỉa tùy chọn này được mô tả trong Tài liệu Bổ sung Phần C.
3.1. Tổng quan: Zero-TPrune
Khung tổng thể Zero-TPrune được hiển thị trong Hình 2. Mỗi lớp cắt tỉa được cấu thành từ nhiều stage và có thể được chèn ở bất cứ đâu giữa các khối Transformer. I-stage và S-stage cho phép Zero-TPrune xem xét cả tầm quan trọng và sự tương đồng. Mục tiêu của I-stage là thu được phân phối điểm tầm quan trọng trên các token và giữ lại k token quan trọng nhất. Để đạt được mục tiêu này, chúng tôi đề xuất thuật toán WPR và sử dụng ma trận attention từ khối Transformer đã được huấn luyện sẵn. Trong S-stage, chúng tôi đo sự tương đồng giữa các token dựa trên các vector embedding của chúng và chỉ giữ lại một token trong r cặp tương đồng nhất. Để giảm overhead tính toán từ tất cả các kết hợp theo cặp, chúng tôi phân vùng token thành các nhóm lưỡng phân. Các token trong cùng một nhóm không bao giờ được ghép cặp để đo sự tương đồng. Để có kiểm soát tốt hơn về phân phối tầm quan trọng của các token đã cắt tỉa, chúng tôi hướng dẫn việc phân vùng theo thứ hạng tầm quan trọng của chúng.
Một cách đơn giản để kết hợp hai stage là kết nối liên tiếp I-stage và S-stage: một số token được cắt tỉa dựa trên phân phối điểm tầm quan trọng thu được trong I-stage; phân phối này sau đó được sử dụng để hướng dẫn việc phân vùng trong S-stage và một số token khác được cắt tỉa dựa trên sự tương đồng. Tuy nhiên, chúng tôi quan sát thực nghiệm rằng sự kết hợp tầm thường như vậy có thể khiến các token không quan trọng về mặt ngữ nghĩa cuối cùng đẩy ra các token có ý nghĩa ngữ nghĩa trong I-stage. Ví dụ, đôi khi các token nền nhận được điểm tầm quan trọng cao so với các token đối tượng chính. Chi tiết về hiện tượng này có thể tìm thấy trong Tài liệu Bổ sung Phần A.1. Chúng tôi giải quyết vấn đề này bằng cách hoán đổi I-stage và S-stage. Phương pháp này cho phép loại bỏ sớm các token tương tự trong S-stage, do đó giảm đáng kể tác động bất lợi của sự tương đồng trong I-stage. Chúng tôi trình bày so sánh hai mẫu trong Tài liệu Bổ sung Phần A.2. Để tạo điều kiện phân vùng trong S-stage, chúng tôi giới thiệu I′-stage tiền xếp hạng để gán điểm tầm quan trọng cho các token với một vòng bỏ phiếu duy nhất. Đáng chú ý, không có token nào bị cắt tỉa trong I′-stage. Do đó, lớp cắt tỉa bao gồm ứng dụng tuần tự của I′-stage, S-stage, và I-stage.
3.2. I-stage: Cắt tỉa dựa trên Tầm quan trọng
Để giữ lại k token quan trọng nhất, chúng tôi giới thiệu một thước đo xếp hạng gọi là điểm tầm quan trọng. Để thu được điểm tầm quan trọng, chúng tôi coi ma trận attention A(h,l) như ma trận kề của một đồ thị đầy đủ, có hướng, gọi là đồ thị attention, như được hiển thị trong Hình 3(a). Việc xếp hạng các nút trong đồ thị là thách thức vì một số lý do. (i) Đồ thị attention dày đặc, thường bao gồm hàng trăm nút và nhiều cạnh hơn khi đầu vào là hình ảnh. (ii) Chúng tôi có ngân sách nghiêm ngặt cho overhead tính toán phát sinh bởi thuật toán trên mỗi hình ảnh trong quá trình suy luận.
Lấy cảm hứng từ thuật toán Page Rank [4], chúng tôi đề xuất thuật toán WPR để thu được các điểm tầm quan trọng. Page Rank đã được sử dụng trong Google Search để xếp hạng các trang web. Trong thuật toán Page Rank gốc, các liên kết giữa các trang web không có trọng số. Để áp dụng nó vào đồ thị attention có trọng số và có hướng, chúng tôi xem xét tín hiệu của mỗi nút trong đồ thị này như tầm quan trọng của mỗi token. Chúng tôi khởi tạo tín hiệu đồ thị đồng đều và sử dụng ma trận kề như Toán tử Dịch chuyển Đồ thị (GSO). Khi GSO được áp dụng cho tín hiệu đồ thị, mỗi nút bỏ phiếu cho nút nào quan trọng hơn thông qua trọng số được gán cho các cạnh đầu ra, tức là attention mà một token dành cho các token khác. Nếu bản thân nút đó quan trọng hơn, phiếu bầu của nút này quan trọng hơn. Điều này được hiển thị trong Thuật toán 1. Quá trình chuyển đổi từ khởi tạo đến hội tụ được hiển thị trong Hình 3(b).
Chúng tôi thu được biểu thức cho điểm tầm quan trọng của mỗi nút (tức là token) trong lớp l-th, head h-th như sau:
s(h,l)(xi) =1/N∑j=1N A(h,l)(xi, xj)·s(h,l)(xj) (1)
trong đó s(h,l)(xj) là điểm tầm quan trọng của nút xi trong head h-th của lớp l-th, và N là số lượng token trong lớp l-th. s(h,l)(xi) được tính từ tổng có trọng số của attention nhận được. WPR do đó đệ quy gán tầm quan trọng cao cho các token có ý nghĩa ngữ nghĩa và giảm nhiễu từ các token không quan trọng về mặt ngữ nghĩa yếu. Chúng tôi giữ lại k token quan trọng nhất (k được xác định bởi tỷ lệ giữ lại và tổng số token).

--- TRANG 2 ---
Hình 1. So sánh các phương pháp tăng cường hiệu quả hiện có và Zero-TPrune. ρ đại diện cho tỷ lệ giữ lại được đo bằng chi phí FLOPS. Hầu hết các phương pháp hiện có yêu cầu huấn luyện lại mô hình sau khi triển khai; mỗi cấu hình cắt tỉa khác nhau yêu cầu huấn luyện lại mô hình riêng biệt, điều này cực kỳ đắt đỏ. Ngược lại, Zero-TPrune không cần huấn luyện và có thể chuyển đổi giữa các cấu hình cắt tỉa khác nhau mà không tốn chi phí tính toán. Điều này có lợi từ thuật toán dựa trên đồ thị của chúng tôi khai thác các mối tương quan giữa các token hình ảnh.

--- TRANG 3 ---
Thuật toán 1 Thuật toán Weighted Page Rank (WPR) dựa trên đồ thị
Yêu cầu: N > 0 là số lượng nút trong đồ thị; A∈RN×N là ma trận kề của đồ thị này; s∈RN đại diện cho tín hiệu đồ thị
Đảm bảo: s∈RN đại diện cho điểm tầm quan trọng của các nút trong đồ thị
s0←1/N×eN ▷ Khởi tạo tín hiệu đồ thị đồng đều
t←0
while (|st−st−1|> ϵ) or (t= 0) do ▷ Tiếp tục lặp nếu chưa hội tụ
t←t+ 1
st←AT×st−1 ▷ Sử dụng ma trận kề như toán tử dịch chuyển đồ thị
end while
s←st

Việc đơn giản là tính trung bình các điểm tầm quan trọng trên các head khác nhau không phải là lựa chọn tối ưu. Các head khác nhau trong một lớp encoder thường chú ý đến các phần khác nhau của hình ảnh đầu vào (một ví dụ trực quan được đưa ra trong Phần E.1 của Tài liệu Bổ sung). Do đó, có một số token rất quan trọng trong một hoặc hai head, nhưng không quan trọng trong các head khác. Mặt khác, một số token có tầm quan trọng thấp đến trung bình trong tất cả các head. Các token trước thường nhiều thông tin hơn các token sau. Tuy nhiên, nếu có nhiều head và điểm tầm quan trọng được tính trung bình trực tiếp trên tất cả các head, các token sau có thể nhận được điểm tương tự hoặc thậm chí cao hơn các token trước, dẫn đến việc xếp hạng và cắt tỉa token không chính xác. Để giải quyết vấn đề này, chúng tôi tổng hợp các điểm tầm quan trọng trên các head thông qua căn bậc hai của tổng bình phương. Chúng tôi gọi đây là tổng hợp Emphasizing Informative Region (EIR). Chúng tôi quan sát thấy EIR hiệu quả phân biệt các vùng thông tin từ các vùng không thông tin. Một ví dụ cụ thể so sánh EIR với các phương pháp khác (như argmax và average) được đưa ra trong Phần E.1 của Tài liệu Bổ sung.
Bên cạnh vấn đề được đề cập ở trên, đôi khi các điểm tầm quan trọng được đưa ra bởi thuật toán WPR có thể hội tụ về một phân phối không mong muốn trong một số head: (1) các token ở rìa của hình ảnh đầu vào có thể nhận được điểm tầm quan trọng rất cao; (2) phân phối điểm tầm quan trọng có thể trở nên gần như đồng đều. Chúng tôi cung cấp các ví dụ trực quan của những trường hợp này trong Phần E.2 của Tài liệu Bổ sung. Cả hai head trong những trường hợp này không cung cấp thông tin hữu ích và thậm chí còn gây hiểu nhầm. Để giảm thiểu tác động tiêu cực của những head này, chúng tôi giới thiệu Variance-based Head Filter (VHF). Chúng tôi tính phương sai của phân phối trong mỗi head và đặt cả ngưỡng tối thiểu và tối đa cho phương sai. Các head có phương sai phân phối vượt quá ngưỡng tối đa hoặc thấp hơn ngưỡng tối thiểu bị loại khỏi tính toán. Sau đó phương trình điểm tầm quan trọng cuối cùng trở thành:
s(l)(xi) = √(∑h=1Nh s(h,l)(xi)2·η(vmin≤Varh≤vmax)) / (∑h=1Nh η(vmin≤Varh≤vmax)) (2)
trong đó η(vmin≤Varh≤vmax) bằng 1 nếu vmin≤Varh≤vmax, ngược lại bằng 0; vmin và vmax đại diện cho ngưỡng tối thiểu và tối đa, tương ứng; Varh là phương sai điểm tầm quan trọng của các token trong head h-th; Nh là số lượng head trong lớp l-th. Độ phức tạp của I-stage, bao gồm WPR, EIR, và VHF, là O(N2), trong đó N là số lượng token.
3.3. S-stage: Cắt tỉa dựa trên Sự tương đồng
Như đã thảo luận trước đây, việc đo sự tương đồng ngay cả giữa các token quan trọng và thực hiện cắt tỉa thêm là có giá trị. Công trình trước đây [3] sử dụng phân vùng token không phụ thuộc hình ảnh để đo sự tương đồng theo cặp. Thay vào đó, chúng tôi đề xuất phân vùng theo tầm quan trọng cho mỗi hình ảnh để cắt tỉa sự tương đồng, như được hiển thị trong Hình 4.

Hình 4. Quá trình cắt tỉa dựa trên tầm quan trọng trong S-stage. Như một ví dụ, phân vùng tuần tự (cắt tỉa phần không quan trọng) được sử dụng trong hình này.

Hình 4 (1&2): Dựa trên điểm tầm quan trọng của các token, chúng tôi tuần tự phân vùng chúng thành các nhóm có kích thước tương đương, A và B, và cắt tỉa nhóm ít quan trọng hơn. Chúng tôi khám phá các sơ đồ phân vùng hướng dẫn tầm quan trọng khác, bao gồm phân vùng thay thế và phân vùng ngẫu nhiên, và cung cấp kết quả ablation trong Phần G.3 của Tài liệu Bổ sung. Hình 4 (3): Sau đó chúng tôi xác định token tương đồng nhất trong Nhóm B cho mỗi token trong Nhóm A và ghi lại sự tương đồng tương ứng của mỗi cặp. Để thực hiện điều này, chúng tôi biểu diễn mỗi token bằng một vector đặc trưng, có thể được tính từ một số lựa chọn có sẵn, chẳng hạn như các vector tương ứng trong ma trận Key, Query, hoặc Value. Các thí nghiệm ablation của chúng tôi cho thấy sử dụng vector từ ma trận Key là lựa chọn tối ưu. Chúng tôi tính sự tương đồng trên những vector này bằng một thước đo được chỉ định, chẳng hạn như cosine similarity, Manhattan distance, hoặc Euclidean distance. Theo kết quả của các thí nghiệm ablation, chúng tôi sử dụng cosine similarity. Chúng tôi cung cấp báo cáo chi tiết về kết quả thí nghiệm ablation trong Phần G.3 của Tài liệu Bổ sung.

--- TRANG 4 ---
Hình 2. Khung tổng thể Zero-TPrune. Các lớp cắt tỉa có thể được chèn giữa các khối Transformer để giảm số lượng token. Các lớp cắt tỉa bao gồm I-stage và S-stage: I-stage nhằm cắt tỉa các token không quan trọng của hình ảnh, chẳng hạn như token nền (xem (b)); S-stage nhằm cắt tỉa các token quá tương đồng với nhau, chẳng hạn như token kết cấu lặp lại (xem (c)). Sự kết hợp của các stage sau đó khai thác tối đa sự dư thừa token (xem (d)).

--- TRANG 5 ---
Hình 4 (4&5): Trong bước tiếp theo, chúng tôi chọn r cặp tương đồng nhất và cắt tỉa các token tương ứng trong Nhóm A. Chúng tôi cắt tỉa một token trong mỗi cặp được chọn thay vì hợp nhất chúng, do các lý do sau: (i) vì các token trong các cặp được chọn tương đồng, việc cắt tỉa một trong số chúng dẫn đến mất thông tin tối thiểu; (ii) các token được hợp nhất nên có trọng số cao hơn trong tính toán tiếp theo [3], điều này làm cho nó không tương thích với một số backbone nhất định, chẳng hạn như Sparse Transformer [6]. Cuối cùng, chúng tôi chuyển các token còn lại đến stage tiếp theo. Độ phức tạp của S-stage là O(N2×d), trong đó N là số lượng token và d là chiều của token embedding.
Việc phân vùng hướng dẫn tầm quan trọng trong S-stage tạo điều kiện kiểm soát ổn định tầm quan trọng của các token đã cắt tỉa cho các hình ảnh đầu vào khác nhau. Bằng cách cắt tỉa các token tương đồng thay vì hợp nhất chúng, phương pháp của chúng tôi duy trì khả năng tương thích với một số backbone chuyên biệt [6] trong khi chỉ phải chịu mất thông tin tối thiểu.
4. Kết quả Thí nghiệm
Trong phần này, chúng tôi đầu tiên mô tả quá trình cắt tỉa token được trực quan hóa của một số hình ảnh trong tập dữ liệu validation ImageNet, như được hiển thị trong Hình 5. Chúng tôi cũng trình bày các thí nghiệm ablation để xác thực các lựa chọn thiết kế và hiệu quả của các phương pháp được đề xuất. Sau đó, chúng tôi so sánh Zero-TPrune với các phương pháp cắt tỉa token tiên tiến.
Thiết lập Thí nghiệm: Để so sánh các phương pháp cắt tỉa khác nhau, chúng tôi áp dụng chúng vào các backbone vision Transformer khác nhau và đánh giá hiệu suất của các mô hình đã cắt tỉa trên ImageNet [9]. Chúng tôi đánh giá các mô hình trên hình ảnh 224px trừ khi có ghi chú khác. Chúng tôi ước tính GFLOPS suy luận trên GPU V100 bằng thư viện fvcore¹. Chúng tôi đo throughput suy luận của các mô hình đã cắt tỉa trên một GPU A100 và thực hiện tinh chỉnh sau cắt tỉa trên GPU A100.
Mặc dù các thí nghiệm của chúng tôi tập trung vào tác vụ phân loại, Zero-TPrune có khả năng được áp dụng cho các tác vụ khác, chẳng hạn như generation và segmentation. Chúng tôi gọi thiết kế có thể được chuyển giao cho các tác vụ khác là "Zero-TPrune-uni", có nghĩa là "Zero-TPrune cho mục đích phổ quát". Đối với tác vụ phân loại, token CLS được biết là quan trọng hơn nhiều so với các token khác, và trọng số attention của nó là tín hiệu mạnh hơn nhiều để chọn token [14]. Do đó, thay vì khởi tạo điểm tầm quan trọng của các token đồng đều, chúng tôi gán cho token CLS một điểm tầm quan trọng lớn hơn √N lần so với các token khác trong quá trình khởi tạo trong I-stage, trong đó N là số lượng token. Chúng tôi gọi thiết kế này là "Zero-TPrune" trong các thí nghiệm sau.
Đối với các thí nghiệm ablation, chúng tôi triển khai Zero-TPrune trên mô hình DeiT-S [36] với các cấu hình và lựa chọn thiết kế khác nhau. Để so sánh với các công trình cắt tỉa token tiên tiến, chúng tôi chia chúng thành hai loại: (1) các phương pháp yêu cầu tinh chỉnh mô hình đã cắt tỉa, bao gồm DynamicViT [30] và A-ViT [43]; (2) các phương pháp không cần tinh chỉnh, bao gồm ATS [14] và ToMe [3] (đây là phương pháp hợp nhất token thay vì cắt tỉa token, nhưng cũng không cần tinh chỉnh). Đối với loại đầu tiên, chúng tôi so sánh các triển khai trên các mô hình DeiT. Chúng tôi sử dụng triển khai chính thức của DynamicViT để tái tạo kết quả của nó và cũng tạo ra một số kết quả mới để so sánh. Đối với A-ViT, chúng tôi trực tiếp sử dụng kết quả được trình bày trong bài báo đó. Đối với loại thứ hai, chúng tôi sử dụng mã nguồn mở chính thức của ATS và ToMe để triển khai chúng trên các backbone Transformer đã được huấn luyện sẵn khác nhau, bao gồm DeiT [36], LV-ViT [18], MAE [17], AugReg [33], và SWAG [32]. Chúng tôi so sánh hiệu suất off-the-shelf của Zero-TPrune với của chúng. Ngoài ra, chúng tôi so sánh hiệu suất của các mô hình đã cắt tỉa trên các tác vụ downstream để kiểm tra khả năng transfer learning của chúng, theo lựa chọn tập dữ liệu trong [5]. Chúng tôi cung cấp chi tiết về các tập dữ liệu được chọn trong Phần F của Tài liệu Bổ sung. Lưu ý rằng ToMe có một thiết kế tùy chọn, Proportional Attention (PA), được dành riêng cho phân loại [2] và không tương thích với thiết kế attention thưa thớt [6]. Chúng tôi gọi ToMe với PA bị vô hiệu hóa là "ToMe-uni" và ToMe với PA được kích hoạt là "ToMe". ATS là một phương pháp chỉ dựa trên token CLS; do đó, nó không có phiên bản universal.
Để xác thực thêm hiệu quả của Zero-TPrune, chúng tôi bổ sung so sánh với các phương pháp depth-adaptive và các phương pháp xếp hạng token dựa trên attention đơn giản khác (ví dụ, tính trung bình attention nhận được) trong Phần H.1 và H.2 của Tài liệu Bổ sung.
4.1. Thí nghiệm Ablation
Chúng tôi sử dụng các thí nghiệm ablation để xác định các siêu tham số tối ưu cho Zero-TPrune. Đầu tiên, việc kiểm tra xem thuật toán WPR có hội tụ sau mỗi lần lặp là tốn kém về mặt tính toán. Do đó, sẽ là mong muốn nếu chúng ta có thể xác định trước số lần lặp của nó. Bằng cách kiểm tra các phân phối tầm quan trọng sau các số lần lặp khác nhau và tính phân kỳ Kullback-Liebler (KL) giữa chúng, chúng tôi thấy 30-50, 5-10, và 1 lần lặp là đủ để đảm bảo hội tụ trong ba lớp đầu tiên, các lớp trung bình, và ba lớp cuối cùng, tương ứng. Chúng tôi cung cấp so sánh trực quan và định lượng trong Phần G.1 của Tài liệu Bổ sung. Thứ hai, chúng tôi xác định các ngưỡng tối thiểu và tối đa đủ tốt cho VHF thông qua khởi tạo ngẫu nhiên và tìm kiếm tham lam. Chúng tôi cung cấp các cấu hình tìm kiếm chi tiết và kết quả trong Phần G.2 của Tài liệu Bổ sung. Phạm vi tìm thấy là [0.01,0.7], đây là cài đặt mặc định trong các thí nghiệm của chúng tôi. Thứ ba, chúng tôi khám phá các lựa chọn thiết kế tối ưu trong S-stage với các thí nghiệm ablation được trình bày trong Phần G.3 của Tài liệu Bổ sung.
Để minh họa hiệu quả của các kỹ thuật khác nhau được sử dụng trong Zero-TPrune, chúng tôi phân tích đóng góp của chúng. Chúng tôi áp dụng các kết hợp khác nhau của các kỹ thuật được sử dụng trong Zero-TPrune vào mô hình DeiT-S và đánh giá hiệu suất của các mô hình đã cắt tỉa. Chúng tôi chèn các lớp cắt tỉa sau lớp [1,3,6,9,11] với tỷ lệ giữ lại [1,0.9,0.8,0.7,1] và số lần lặp [30,5,5,1,1] trong I-stage, và cắt tỉa 10 token trong mỗi S-stage. Trước khi thêm S-stage, chúng tôi chèn các lớp cắt tỉa sau lớp [3,6,9,11] với tỷ lệ giữ lại [0.8,0.7,0.7,0.6] và số lần lặp [5,5,1,1]. Chúng tôi hiển thị kết quả trong Bảng 1 (chúng tôi cung cấp kết quả tương ứng của Zero-TPrune-uni trong Phần G.4 của Tài liệu Bổ sung). Thuật toán WPR cải thiện hiệu suất đáng kể. Các kỹ thuật EIR/VHF và S-stage cải thiện hiệu suất thêm nữa.

Bảng 1. Phân tích đóng góp của các kỹ thuật khác nhau được sử dụng trong Zero-TPrune. Kích thước batch được sử dụng là 512.
Acc@1 Params FLOPS/img Throughput Phương pháp
79.8% (base) 22M 4.55G 1505.9 img/s Mô hình chưa cắt tỉa
76.8% (-3.0%) 22M 3.08G 2164.4 img/s loại bỏ ngẫu nhiên
78.6% (+1.8%) 22M 3.08G 2136.5 img/s WPR
78.8% (+0.2%) 22M 3.08G 2132.6 img/s WPR+EIR
78.9% (+0.1%) 22M 3.08G 2103.1 img/s WPR+EIR+VHF (I-stage)
79.4% (+0.5%) 22M 3.08G 2063.9 img/s I-stage + S-stage

Để cải thiện thêm hiệu suất của các mô hình đã cắt tỉa, chúng tôi có thể sử dụng Mô phỏng Monte Carlo (MCS) để khám phá ngẫu nhiên không gian siêu tham số, bao gồm số lượng và vị trí của các lớp cắt tỉa, tỷ lệ giữ lại tương ứng, số lần lặp trong mỗi lớp, và số token được cắt tỉa trong mỗi S-stage. Sau khi tiến hành hàng nghìn thử nghiệm, chúng tôi chọn cài đặt tối ưu thể hiện hiệu suất tốt nhất đạt được bởi Zero-TPrune trong khi duy trì ngân sách GFLOPS cố định. Trong trường hợp được hiển thị trong Bảng 1, MCS giúp đạt được độ chính xác 79.5% với 3.08 GFLOPS. Để đảm bảo so sánh công bằng, chúng tôi không sử dụng MCS trong Zero-TPrune trong các so sánh tiếp theo với các phương pháp tiên tiến. Zero-TPrune không nhạy cảm với lựa chọn siêu tham số, như được minh họa với kết quả thí nghiệm trong Phần G.5 của Tài liệu Bổ sung.
4.2. So sánh với Các Phương pháp Tiên tiến
Trong phần này, chúng tôi chọn số lượng và vị trí của các lớp cắt tỉa với tỷ lệ giữ lại không đổi hoặc giảm đều để phù hợp với ngân sách GFLOPS đã cho. Chúng tôi giữ số lượng token đã cắt tỉa trong mỗi S-stage không đổi. Chúng tôi cố định số lần lặp ở 30, 5, và 1 cho ba lớp đầu tiên, trung gian, và ba lớp cuối cùng, tương ứng.
So sánh với Các Phương pháp Yêu cầu Tinh chỉnh: Để minh họa ưu điểm của Zero-TPrune, chúng tôi so sánh hiệu suất của nó với DynamicViT và A-ViT có/không có tinh chỉnh sau cắt tỉa. Do khởi tạo ngẫu nhiên và thực tế rằng các mô-đun cắt tỉa trong DynamicViT và A-ViT cần được huấn luyện, hiệu suất của DynamicViT và A-ViT không có tinh chỉnh sau cắt tỉa dựa trên các token được cắt tỉa ngẫu nhiên. Hình 6 rõ ràng thể hiện ưu điểm của Zero-TPrune so với các phương pháp cắt tỉa yêu cầu tinh chỉnh tiên tiến, tức là DynamicViT và A-ViT. Không có tinh chỉnh sau cắt tỉa, Zero-TPrune vượt trội hơn DynamicViT và A-ViT (sử dụng loại bỏ ngẫu nhiên trong trường hợp này) khoảng 1%. Điều này có nghĩa Zero-TPrune giảm sự sụt giảm độ chính xác hơn 60%. Hiệu suất của Zero-TPrune không có tinh chỉnh sau cắt tỉa có thể so sánh với DynamicViT và A-ViT có tinh chỉnh sau cắt tỉa (ví dụ, 0.1% mất độ chính xác so với tốt nhất, với ngân sách 3.5 GFLOPS trên DeiT-S). Với tinh chỉnh sau cắt tỉa, Zero-TPrune vượt trội hơn cả DynamicViT và A-ViT. Zero-TPrune cũng có thể dễ dàng được áp dụng cho các mô hình lớn hơn (ví dụ, với ngân sách 13.6 GFLOPS trên DeiT-B) để có độ chính xác cao hơn. Ngược lại, việc áp dụng DynamicViT và A-ViT cho các mô hình lớn rất tốn kém về mặt tính toán do tinh chỉnh đắt đỏ sau cắt tỉa.

--- TRANG 6 ---
Hình 3. Tổng quan về I-stage: (a) từ ma trận attention 4×4 đến đồ thị attention và (b) biến đổi tín hiệu đồ thị từ khởi tạo đến hội tụ.

--- TRANG 7 ---
Hình 5. Các ví dụ trực quan hóa về quá trình cắt tỉa được thực hiện bởi Zero-TPrune. Hình ảnh được chọn ngẫu nhiên từ tập dữ liệu validation ImageNet. Khi tỷ lệ cắt tỉa mạnh và đối tượng chính chiếm phần lớn diện tích hình ảnh, việc chỉ cắt tỉa các token nền là không đủ. Zero-TPrune khai thác sự tương đồng giữa các token đối tượng chính và cắt tỉa những token dư thừa.

--- TRANG 8 ---
So sánh với Các Phương pháp Không cần Tinh chỉnh: ATS và ToMe cung cấp tùy chọn off-the-shelf để cắt tỉa các mô hình Transformer mà không yêu cầu tinh chỉnh sau cắt tỉa. Chúng tôi đầu tiên áp dụng chúng và Zero-TPrune vào mô hình DeiT-S để so sánh hiệu suất off-the-shelf sau cắt tỉa không có tinh chỉnh. Kết quả được hiển thị trong Hình 7 và Bảng 2. Chúng tôi cung cấp thêm kết quả liên quan đến throughput trong Phần H.3 của Tài liệu Bổ sung. Như được hiển thị trong Hình 7, so với các phương pháp không cần tinh chỉnh tiên tiến, Zero-TPrune giảm mất độ chính xác 33% trên mô hình DeiT-S với ngân sách 3 GFLOPS. Nếu chúng ta thay đổi cấu hình cắt tỉa và đưa ra ngân sách thấp hơn (ví dụ, giảm GFLOPS 45%), mất độ chính xác do Zero-TPrune gây ra vẫn chỉ là 0.7%. Zero-TPrune có thể giảm GFLOPS 13% với chi phí gần như bằng không. Lưu ý rằng những kết quả này được thu được mà không cần tinh chỉnh.

Hình 6. So sánh hiệu suất giữa Zero-TPrune và các phương pháp tiên tiến yêu cầu tinh chỉnh.

Hình 7. So sánh hiệu suất giữa Zero-TPrune và các phương pháp tiên tiến không cần tinh chỉnh. Backbone Transformer được áp dụng là DeiT-S.

Bảng 2. Hiệu suất của các mô hình DeiT-S đã cắt tỉa không có tinh chỉnh. Throughput được đo trên một GPU NVIDIA A100.
Phương pháp Acc@top1 GFLOPS Throughput(img/s)
DeiT-S 79.8% 4.55 1505.9
+ ATS 79.2% (-0.6%) 3.00 (-33.4%) 2062.3 (+36.9%)
+ ToMe 78.9% (-0.9%) 2.95 (-35.2%) 2263.9 (+50.3%)
+ Zero-TP-a 79.4% (-0.4%) 2.97 (-34.7%) 2188.4 (+45.3%)
+ Zero-TP-b 79.1% (-0.7%) 2.50 (-45.1%) 2458.4 (+63.2%)
+ Zero-TP-c 79.8% (-0.0%) 3.97 (-12.7%) 1673.2 (+11.1%)

Chúng tôi tiếp tục đánh giá Zero-TPrune và baseline trên các backbone khác nhau với kích thước khác nhau. Kết quả được hiển thị trong Bảng 3. Chúng tôi thấy rằng khi mô hình gốc có kích thước trung bình, ví dụ, AugReg và LV-ViT-S, Zero-TPrune vượt trội hơn các phương pháp baseline với biên độ lớn (nó giảm mất độ chính xác lên tới 49%). Đối với các mô hình lớn, nếu việc cắt tỉa là vừa phải (tức là giảm GFLOPS 20%), Zero-TPrune vẫn vượt trội hơn các phương pháp baseline. Tuy nhiên, chúng tôi thấy khi các mô hình lớn bị cắt tỉa mạnh (tức là giảm GFLOPS 50%), Zero-TPrune không vượt trội hơn baseline. Lưu ý rằng việc cắt tỉa mạnh các mô hình lớn thường không phải là ý tưởng hay, điều này được chỉ ra bằng cách so sánh mô hình LV-ViT-M đã cắt tỉa tối ưu (ToMe, 81.6% với 6.3 GFLOPS) và mô hình LV-ViT-S đã cắt tỉa tối ưu (Zero-TPrune, 81.5% với 3.5 GFLOPS). Mô hình sau chỉ yêu cầu 60% GFLOPS với chi phí 0.1% mất độ chính xác. So với các mô hình lớn được cắt tỉa mạnh, việc sử dụng mô hình đã được huấn luyện sẵn nhỏ hơn thay thế thường là lựa chọn tốt hơn. Chúng tôi cung cấp thảo luận chi tiết trong Phần I của Tài liệu Bổ sung.
Chúng tôi cũng đánh giá hiệu suất của các mô hình đã cắt tỉa trên các tác vụ downstream để đo khả năng transfer learning của chúng. Chúng tôi chọn một số tập dữ liệu hình ảnh nhỏ cho mục đích này. Zero-TPrune vượt trội hơn baseline trên hầu hết các tập dữ liệu, cho thấy khả năng transfer learning mạnh sau cắt tỉa. Chúng tôi giới thiệu các tập dữ liệu được chọn và trình bày kết quả thí nghiệm chi tiết trong Phần F của Tài liệu Bổ sung.

Bảng 3. Hiệu suất của các mô hình AugReg, LV-ViT, và SWAG đã cắt tỉa không có tinh chỉnh. Các mô hình SWAG thực hiện suy luận trên hình ảnh 384px.
Phương pháp Acc@top1 GFLOPS Phương pháp Acc@top1 GFLOPS
AugReg 81.41% 4.55 MAE 83.62% 55.4
+ ATS 79.21% 2.80 +ATS 82.07% 42.3
+ ToMe 79.30% 2.78 +ToMe 82.69% 42.2
+ Zero-TP 80.22% 2.79 +Zero-TP 82.93% 42.3
LV-ViT-S 83.3% 6.6 SWAG 85.30% 55.6
+ ATS 80.4% 3.5 +ATS 84.21% 43.8
+ ToMe 79.8% 3.6 +ToMe 85.09% 43.8
+ Zero-TP 81.5% 3.5 +Zero-TP 85.17% 43.8

5. Kết luận
Trong bài báo này, chúng tôi đề xuất Zero-TPrune, một phương pháp cắt tỉa token zero-shot khai thác cả tầm quan trọng và sự tương đồng của token để loại bỏ quá trình tinh chỉnh cho việc cắt tỉa. Trong I-stage, nó xem xét ma trận attention như ma trận kề của đồ thị attention, điều này giảm nhiễu từ các token không quan trọng. Trong S-stage, nó sử dụng phân phối tầm quan trọng để hướng dẫn phân vùng token và cắt tỉa dựa trên sự tương đồng, làm cho chúng ổn định và chính xác hơn. Thông qua việc triển khai Zero-TPrune và các phương pháp baseline trên các backbone Transformer khác nhau và đánh giá trên ImageNet, chúng tôi cho thấy nó có thể loại bỏ quá trình tinh chỉnh cho việc cắt tỉa với sự giảm độ chính xác rất nhỏ. Hơn nữa, khi so sánh với các phương pháp cắt tỉa off-the-shelf tiên tiến, Zero-TPrune không chỉ vượt trội hơn chúng bằng cách giảm mất độ chính xác lên tới 49% mà còn tăng cường khả năng transfer learning của các mô hình đã cắt tỉa. Những phát hiện này nhấn mạnh hiệu quả của Zero-TPrune trong việc cân bằng nén mô hình và bảo tồn hiệu suất, làm cho nó trở thành một cách tiếp cận đầy hứa hẹn cho việc cắt tỉa hiệu quả và chính xác các mô hình Transformer. Công việc tương lai có thể tăng cường khả năng của Zero-TPrune hơn nữa. Một chủ đề nghiên cứu tương lai hấp dẫn là kiểm tra khả năng áp dụng của Zero-TPrune trên các tác vụ như tái tạo hình ảnh, phân đoạn, và generation. Việc điều tra lợi ích tiềm năng và lợi ích hiệu quả của việc sử dụng Zero-TPrune trong những lĩnh vực này hứa hẹn sẽ thúc đẩy lĩnh vực này tiến xa hơn.

Lời cảm ơn. Công trình này được hỗ trợ bởi NSF theo Grant No. CCF-2203399.

--- TRANG 9 ---
Tài liệu tham khảo
[1] Andrea Banino, Jan Balaguer, và Charles Blundell. PonderNet: Learning to Ponder. arXiv preprint arXiv:2107.05407, 2021.
[2] Daniel Bolya và Judy Hoffman. Token Merging for Fast Stable Diffusion. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 4598–4602, 2023.
[3] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, và Judy Hoffman. Token Merging: Your ViT But Faster. arXiv preprint arXiv:2210.09461, 2022.
[4] Sergey Brin. The PageRank Citation Ranking: Bringing Order to the Web. Proceedings of ASIS, 1998, 98:161–172, 1998.
[5] Yun-Hao Cao, Hao Yu, và Jianxin Wu. Training Vision Transformers with Only 2040 Images. Trong Proceedings of the European Conference on Computer Vision, trang 220–237. Springer, 2022.
[6] Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating Long Sequences with Sparse Transformers. arXiv preprint arXiv:1904.10509, 2019.
[7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, và Andrea Vedaldi. Describing Textures in the Wild. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 3606–3613, 2014.
[8] Marco Cuturi, Olivier Teboul, và Jean-Philippe Vert. Differentiable Ranking and Sorting Using Optimal Transport. Advances in Neural Information Processing Systems, 32, 2019.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255, 2009.
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805, 2018.
[11] Peiyan Dong, Mengshu Sun, Alec Lu, Yanyue Xie, Kenneth Liu, Zhenglun Kong, Xin Meng, Zhengang Li, Xue Lin, Zhenman Fang, et al. HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers. Trong Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, trang 442–455, 2023.
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929, 2020.
[13] Maha Elbayad, Jiatao Gu, Edouard Grave, và Michael Auli. Depth-Adaptive Transformer. arXiv preprint, 2019.
[14] Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, và Jürgen Gall. Adaptive Token Sampling for Efficient Vision Transformers. Trong Proceedings of the European Conference on Computer Vision, trang 396–414. Springer, 2022.
[15] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, và Ashish Verma. PoWER-BERT: Accelerating BERT Inference via Progressive Word-Vector Elimination. Trong Proceedings of the International Conference on Machine Learning, trang 3690–3699. PMLR, 2020.
[16] Alex Graves. Adaptive Computation Time for Recurrent Neural Networks. arXiv preprint arXiv:1603.08983, 2016.
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross Girshick. Masked Autoencoders are Scalable Vision Learners. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 16000–16009, 2022.
[18] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, và Jiashi Feng. All Tokens Matter: Token Labeling for Training Better Vision Transformers. Advances in Neural Information Processing Systems, 34:18590–18602, 2021.
[19] Gyuwan Kim và Kyunghyun Cho. Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search. arXiv preprint arXiv:2010.07003, 2020.
[20] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, và Kurt Keutzer. Learned Token Pruning for Transformers. Trong Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, trang 784–794, 2022.
[21] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. SPViT: Enabling Faster Vision Transformers Latency-Aware Soft Token Pruning. Trong Proceedings of the European Conference on Computer Vision, trang 620–640. Springer, 2022.
[22] Jonathan Krause, Michael Stark, Jia Deng, và Li Fei-Fei. 3D Object Representations for Fine-Grained Categorization. Trong Proceedings of the IEEE International Conference on Computer Vision Workshops, trang 554–561, 2013.
[23] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, và Qi Ju. FastBERT: a Self-Distilling BERT with Adaptive Inference Time. arXiv preprint arXiv:2004.02178, 2020.
[24] Xiangcheng Liu, Tianyi Wu, và Guodong Guo. Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention. arXiv preprint arXiv:2209.13802, 2022.
[25] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, và Andrea Vedaldi. Fine-Grained Visual Classification of Aircraft. arXiv preprint arXiv:1306.5151, 2013.
[26] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-Level Control through Deep Reinforcement Learning. Nature, 518(7540):529–533, 2015.
[27] Maria-Elena Nilsback và Andrew Zisserman. A Visual Vocabulary for Flower Classification. Trong Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, trang 1447–1454, 2006.

--- TRANG 10 ---
[28] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, và C.V. Jawahar. Cats and Dogs. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 3498–3505, 2012.
[29] Ariadna Quattoni và Antonio Torralba. Recognizing Indoor Scenes. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 413–420, 2009.
[30] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, và Cho-Jui Hsieh. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. Advances in Neural Information Processing Systems, 34:13937–13949, 2021.
[31] Stefan Schaal và Christopher G. Atkeson. Learning Control in Robotics. IEEE Robotics & Automation Magazine, 17(2): 20–29, 2010.
[32] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, và Laurens Van Der Maaten. Revisiting Weakly Supervised Pre-Training of Visual Perception Models. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 804–814, 2022.
[33] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, và Lucas Beyer. How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers. arXiv preprint arXiv:2106.10270, 2021.
[34] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, và Rob Fergus. Intriguing Properties of Neural Networks. arXiv preprint arXiv:1312.6199, 2013.
[35] Quan Tang, Bowen Zhang, Jiajun Liu, Fagui Liu, và Yifan Liu. Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 777– 786, 2023.
[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training Data-Efficient Image Transformers & Distillation through Attention. Trong Proceedings of the International Conference on Machine Learning, trang 10347–10357. PMLR, 2021.
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is All You Need. Advances in Neural Information Processing Systems, 30, 2017.
[38] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, và Serge Belongie. The Caltech-UCSD Birds-200-2011 Dataset. California Institute of Technology, https://www.vision.caltech.edu/datasets/cub_200_2011/, 2011.
[39] Hanrui Wang, Zhekai Zhang, và Song Han. SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning. Trong Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, trang 97–110, 2021.
[40] Siyuan Wei, Tianzhu Ye, Shen Zhang, Yao Tang, và Jiajun Liang. Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 2092–2101, 2023.
[41] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, và Lucas Beyer. Scaling Vision Transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 12104–12113, 2022.
[42] Deming Ye, Yankai Lin, Yufei Huang, và Maosong Sun. Tr-BERT: Dynamic Token Reduction for Accelerating BERT Inference. arXiv preprint arXiv:2105.11618, 2021.
[43] Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya, Jan Kautz, và Pavlo Molchanov. A-ViT: Adaptive Tokens for Efficient Vision Transformer. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 10809–10818, 2022.

--- TRANG 11 ---
Zero-TPrune: Cắt tỉa Token Không cần Huấn luyện thông qua Khai thác Đồ thị Attention trong Transformers Đã được Huấn luyện sẵn
Tài liệu Bổ sung
A. Mẫu I-S và Mẫu I′-S-I
Trong phần này, chúng tôi đầu tiên thể hiện vấn đề áp đảo của nhóm đa số do mẫu I-S gây ra và sau đó so sánh nó với mẫu I′-S-I một cách trực quan.
A.1. Áp đảo của Nhóm Đa số với Mẫu I-S
Đôi khi, các phần không quan trọng của hình ảnh có thể được xác định là quan trọng và các phần quan trọng được xác định là không quan trọng bởi thuật toán WPR dựa trên đồ thị của chúng tôi. Trong I-stage, mỗi token bỏ phiếu cho "các token quan trọng hơn" và trọng số phiếu bầu của chúng được xác định bởi tầm quan trọng của chúng trong vòng bỏ phiếu cuối cùng. Bên cạnh các token có ý nghĩa ngữ nghĩa, các token cũng có xu hướng bỏ phiếu cho các token tương tự với chúng. Khi các token có ý nghĩa ngữ nghĩa (ví dụ, token đối tượng chính) chỉ là một phần nhỏ của hình ảnh và các token nền không quan trọng chiếm ưu thế, đôi khi các token nền bỏ phiếu cho nhau và dần dần thu được điểm tầm quan trọng cao sau nhiều vòng bỏ phiếu. Một ví dụ được hiển thị trong Hình 8. Nó cho thấy rằng nền của hình ảnh được coi là quan trọng trong các head Transformer. Bản thân con cá, đáng ngạc nhiên, được coi là không quan trọng.

Hình 8. Một ví dụ minh họa rằng một nhóm không quan trọng lớn có thể áp đảo một nhóm quan trọng nhỏ: (a) hình ảnh đầu vào và (b) ba ví dụ cho thấy các token nền không quan trọng áp đảo các token cá quan trọng.

Trong hình ảnh này, các token nền và cá tạo thành hai tập hợp: A và B. Ban đầu, vì các token trong tập hợp B có ý nghĩa ngữ nghĩa hơn những token trong tập hợp A, chúng có điểm tầm quan trọng tương đối cao. Tuy nhiên, cả token trong tập hợp A và tập hợp B chủ yếu có xu hướng bỏ phiếu cho các token trong tập hợp của riêng chúng. Do đó, tập hợp A dễ dàng hình thành các token có điểm tầm quan trọng cao hơn vì tập hợp A bao gồm nhiều token hơn. Những token "rất quan trọng" này có trọng số bỏ phiếu lớn hơn trong lần lặp tiếp theo. Điều này làm cho các token khác trong tập hợp A thậm chí còn dễ dàng hơn để có được "tầm quan trọng cao". Đây là một vòng phản hồi tích cực, với kết quả là các token "quan trọng" nhất cuối cùng nằm trong tập hợp A.
A.2. So sánh
Như được hiển thị trong Hình 9, bằng cách cắt tỉa các token nền tương đồng trước, vấn đề áp đảo của nhóm đa số được giảm thiểu đáng kể.
B. Ma trận Xác suất Attention
ViT [12] và các biến thể của nó chứa nhiều lớp encoder Transformer được xếp chồng lên nhau. Lớp encoder Transformer cơ bản bao gồm một khối multi-head attention (MHA) theo sau bởi một khối feed-forward network (FFN), với các kết nối residual và layer normalization xung quanh mỗi khối. Chúng tôi đưa ra giả định rằng một khối MHA bao gồm H head được tham số hóa độc lập. Một attention head h trong lớp l có thể được tham số hóa bởi các ma trận trọng số Key, Query, và Value: W(h,l)k, W(h,l)q, W(h,l)v ∈ Rdh×d, và ma trận trọng số đầu ra W(h,l)o ∈ Rd×dh, trong đó dh thường được đặt thành d/H và d là chiều đặc trưng được nhúng. Giả sử x ∈ Rd×n là chuỗi đầu vào và n là độ dài chuỗi đầu vào. Đối với mỗi head, xác suất attention giữa token xi và xj được đưa ra như một phần tử của ma trận A(h,l):
A(h,l)(xi, xj) = softmax(xTWTqWkx/√d)(i,j) ∈ R (3)
Ma trận này đo lường mức độ token xj chú ý đến token xi. Đầu ra của một khối MHA có thể được công thức hóa như sau:
xMHA = LN(Wo∑i=1NWvxiA(h,l)(xi, xj) + x) (4)
Đầu ra của một lớp encoder Transformer có thể được công thức hóa như sau:
xout = LN(σ(W2(W1xMHA + b1)) + b2 + xMHA) (5)
trong đó W1, W2, b1, và b2 là các tham số FFN, và σ và LN biểu thị hàm kích hoạt và layer normalization, tương ứng. Chúng ta có thể thấy rằng overhead tính toán của một lớp encoder Transformer trải qua việc giảm bậc hai khi các token bị cắt tỉa.

--- TRANG 12 ---
Hình 9. So sánh trực quan giữa mẫu I-S và mẫu I′-S-I.
C. Mô hình Huấn luyện Tùy chọn sau Cắt tỉa
Zero-TPrune có thể loại bỏ quá trình tinh chỉnh sau cắt tỉa với sự giảm độ chính xác rất nhỏ. Tuy nhiên, trong một số tình huống, chúng ta có thể có đủ mẫu và tài nguyên tính toán. Trong những trường hợp như vậy, hiệu suất của Zero-TPrune có thể được cải thiện thêm bằng cách huấn luyện (tinh chỉnh) sau cắt tỉa. Trong phần này, chúng tôi giới thiệu các kỹ thuật được sử dụng để thực hiện điều này.
Với việc làm cho xếp hạng dựa trên tầm quan trọng có thể khả vi là rất đắt đỏ [8], chúng tôi loại bỏ S-stage và chỉ giữ lại I-stage khi chúng tôi nhằm mục đích huấn luyện thêm (tinh chỉnh) mô hình đã cắt tỉa. Bên cạnh điều này, để làm cho Zero-TPrune khả vi, cần phải thay thế việc cắt tỉa token bình thường bằng "cắt tỉa token mềm". Thay vì hoàn toàn loại bỏ các token đã cắt tỉa, cắt tỉa token mềm gán cho chúng trọng số nhỏ để giảm ảnh hưởng của chúng đối với tính toán sau này và bảo tồn khả năng tương thích với back-propagation trong quá trình huấn luyện. Theo cách này, mặt nạ token không khả vi M được thay thế bằng mặt nạ mềm khả vi M̃ sử dụng phép toán sigmoid:
M̃(l)(xi) = σ((s(l)(xi) - θ(l))/T) (6)
trong đó s(l)(xi) là điểm tầm quan trọng của token xi và θ(l) là ngưỡng tầm quan trọng cho lớp l-th. θ(l) được xác định dựa trên tỷ lệ cắt tỉa được chọn và ngân sách GFLOPS. Chi tiết về cắt tỉa token mềm có thể tìm thấy trong [20].
Để đơn giản, chúng tôi sử dụng hàm mất mát tương tự DynamicViT [30], bao gồm ba số hạng:
L = Lcls + λdistillLdistill + λKLLKL (7)
Số hạng đầu tiên là mất mát cross-entropy phân loại tiêu chuẩn:
Lcls = CrossEntropy(y, ŷ) (8)
Trong quá trình tinh chỉnh, chúng tôi sử dụng mạng backbone gốc làm mô hình giáo viên và thúc đẩy hành vi của mô hình Zero-TPrune càng gần với mô hình giáo viên càng tốt. Đầu tiên, chúng tôi thúc đẩy các token cuối cùng được giữ lại của Zero-TPrune gần với những token của mô hình giáo viên. Điều này đóng góp vào số hạng chưng cất thứ hai ở trên. Chúng tôi cũng giảm thiểu sự khác biệt trong dự đoán giữa Zero-TPrune và giáo viên của nó thông qua phân kỳ Kullback-Liebler (KL). Điều này đóng góp vào số hạng thứ ba. Chi tiết về hàm mất mát có thể tìm thấy trong [30].
D. Trực quan hóa
Trong phần này, chúng tôi sử dụng một số ví dụ trực quan hóa để cung cấp những hiểu biết cấp cao.
D.1. Một Ví dụ Hình ảnh Đầu vào
Hình 10 cho thấy một mẫu thử nghiệm đơn giản của một con cá từ tập dữ liệu ImageNet và các phân phối điểm tầm quan trọng tương ứng trong các lớp và head khác nhau. Chúng ta có thể thấy rằng hầu hết các head có thể thành công nắm bắt phần quan trọng của hình ảnh này với sự trợ giúp của thuật toán WPR dựa trên đồ thị.
D.2. Phân phối Tầm quan trọng Trung bình trên Hàng nghìn Hình ảnh
Một ví dụ trực quan hóa thú vị khác liên quan đến chức năng chung của các lớp khác nhau trong Transformers.

--- TRANG 13 ---
Hình 10. Phần quan trọng của hình ảnh đầu vào có thể được nắm bắt thành công bởi thuật toán WPR dựa trên đồ thị: (a) một mẫu thử nghiệm của cá trong tập dữ liệu ImageNet và (b) các phân phối điểm tầm quan trọng tương ứng được đưa ra bởi thuật toán WPR trong các lớp khác nhau. Backbone được sử dụng là DeiT-S.

Hình 11 cho thấy các phân phối điểm tầm quan trọng được tính trung bình trên hàng nghìn hình ảnh. Nó chỉ ra rằng các lớp khác nhau của Transformer hoạt động khác nhau. Các lớp nông tập trung nhiều hơn vào rìa của hình ảnh đầu vào và các lớp sâu tập trung nhiều hơn vào trung tâm.
E. Kết hợp Kết quả của Các Head Khác nhau
Trong phần này, chúng tôi giới thiệu các kỹ thuật chúng tôi đề xuất để kết hợp không tầm thường phân phối điểm tầm quan trọng của các head khác nhau từ thuật toán WPR.
E.1. Nhấn mạnh Vùng Thông tin
Các head khác nhau trong một lớp encoder thường chú ý đến các phần khác nhau của hình ảnh đầu vào, như được hiển thị trong Hình 12. Đối với hình ảnh đầu vào của một cậu bé cầm một con cá, một số head chú ý nhiều hơn đến cơ thể của cậu bé này, một số đến đầu của cậu bé này, và một số đến con cá trong tay.
Chúng tôi đề xuất EIR để giải quyết vấn đề này. Giả sử có ba head tổng cộng và điểm tầm quan trọng của các token A, B, và C là [9,9,9], [9,0,0], [3,3,3], tương ứng. Thứ tự tầm quan trọng lý tưởng là A > B > C. Bảng 4 hiển thị kết quả của việc áp dụng các phương pháp tính điểm tầm quan trọng khác nhau. Phương pháp tính trung bình truyền thống gán cùng tầm quan trọng cho token A và B. Nếu chúng ta chỉ chọn điểm cao nhất trên tất cả các head, token A và B sẽ được gán cùng tầm quan trọng, điều này cũng không mong muốn. Kỹ thuật EIP được đề xuất cân bằng hai tình huống và dẫn đến thứ tự tầm quan trọng lý tưởng.

Bảng 4. Áp dụng các phương pháp tính điểm tầm quan trọng khác nhau cho ví dụ.
Điểm Tầm quan trọng Trung bình {Si} max{Si} EIP
Token A 9 9 5.2
Token B 3 9 3
Token C 3 3 1.7
Xếp hạng A > B = C A = B > C A > B > C

E.2. Bộ lọc Head dựa trên Phương sai
Các điểm tầm quan trọng được đưa ra bởi thuật toán WPR có thể hội tụ về một phân phối không mong muốn. Hai ví dụ điển hình được hiển thị trong Hình 13. Các token ở rìa của hình ảnh đầu vào nhận được điểm tầm quan trọng rất cao trong Hình 13(b) và phân phối điểm tầm quan trọng trong Hình 13(c) gần như đồng đều. Chúng tôi giới thiệu VHF để giảm thiểu tác động tiêu cực của những head này.
F. Các Tác vụ Downstream
Bảng 5 hiển thị số lượng danh mục và instance thử nghiệm trong các tập dữ liệu được chọn. DTD là tập dữ liệu kết cấu có thể mô tả; Indoor67 là tập dữ liệu nhận dạng cảnh trong nhà; CUB200 là tập dữ liệu thách thức gồm 200 loài chim. Các tập dữ liệu khác có tên tự giải thích.

--- TRANG 14 ---
Hình 11. Các phân phối điểm tầm quan trọng được tính trung bình trên hàng nghìn hình ảnh. Hàng đầu tiên được tính từ lớp đầu tiên và hàng thứ hai (thứ ba) từ lớp thứ 10 (thứ 11) của mô hình DeiT-S.

Hình 12. Phân phối điểm tầm quan trọng từ các head khác nhau cho một hình ảnh đầu vào: (a) một hình ảnh của một cậu bé cầm một con cá và (b) các phân phối điểm tầm quan trọng. Kết quả được thu được bởi thuật toán WPR với 30 lần lặp trong lớp thứ mười của mô hình DeiT-S.

Hình 13. Các ví dụ về phân phối điểm tầm quan trọng không mong muốn trong một số head được thu được bởi thuật toán WPR: (a) hình ảnh đầu vào, (b) head thứ hai trong lớp thứ hai của mô hình DeiT-S, và (c) head thứ tư trong lớp thứ ba của mô hình DeiT-S.

Kết quả thí nghiệm được hiển thị trong Bảng 6. Zero-TPrune vượt trội hơn baseline trên hầu hết các tập dữ liệu, cho thấy khả năng transfer learning mạnh sau cắt tỉa. ToMe có hiệu suất kém hơn trên các mô hình kích thước nhỏ do thiếu đủ lớp để hợp nhất token dần dần.

Bảng 5. Các tập dữ liệu cho phân loại hình ảnh downstream.
Tập dữ liệu #Danh mục #Instance Thử nghiệm
Flowers [27] 102 6149
Pets [28] 37 3669
DTD [7] 47 1880
Indoor67 [29] 67 1340
CUB200 [38] 200 5794
Aircrafts [25] 100 3333
Cars [22] 196 8041

G. Thí nghiệm Ablation
Trong phần này, chúng tôi hiển thị kết quả cho các thí nghiệm ablation tiếp theo mà chúng tôi đã thực hiện. Chúng tôi khám phá tốc độ hội tụ của WPR và xác định số lần lặp phù hợp cho mỗi lớp trong Phần G.1. Sau đó chúng tôi xác định ngưỡng phương sai đủ tốt cho VHF trong Phần G.2. Hơn nữa, chúng tôi mô tả các lựa chọn thiết kế tối ưu trong S-stage trong Phần

--- TRANG 15 ---
Bảng 6. Hiệu suất của các mô hình đã cắt tỉa trên các tác vụ downstream.
Mô hình GFLOPS Flowers Pets DTD Indoor67 CUB200 Aircrafts Cars
Deit-T 1.26 97.3 88.6 73.2 75.6 76.8 78.7 90.3
+ ATS 0.90 94.6 86.1 71.0 72.9 73.8 76.0 88.4
+ ToMe 0.90 93.2 84.7 69.9 71.6 72.9 75.2 87.1
+ Zero-TP 0.91 95.1 86.9 70.9 73.7 74.4 76.7 88.2

Hình 14. Các phân phối điểm tầm quan trọng của token trong 2,560 hình ảnh. Phân phối thay đổi theo cả số lần lặp và vị trí lớp: (a) lớp 2, (b) lớp 5, và (c) lớp 12 trong DeiT-S.

tion G.3, thể hiện hiệu suất của Zero-TPrune-uni trong Phần G.4, và thảo luận về tìm kiếm siêu tham số trong Phần G.5.
G.1. Tốc độ Hội tụ của Thuật toán WPR
Việc kiểm tra xem thuật toán WPR có hội tụ sau mỗi lần lặp là tốn kém về mặt tính toán. Do đó, sẽ là mong muốn nếu chúng ta có thể xác định trước số lần lặp của nó. Để làm như vậy, chúng ta cần tính toán hành vi hội tụ chung của thuật toán WPR. Hình 14 hiển thị các phân phối điểm tầm quan trọng của token trong 2,560 hình ảnh. Trong các lớp nông, chẳng hạn như lớp đầu tiên, các phân phối tương ứng với 30 lần lặp và năm lần lặp rõ ràng khác nhau. Điều này chỉ ra rằng năm lần lặp không đủ để làm cho thuật toán WPR hội tụ trong các lớp nông. Mặt khác, trong các lớp sâu, chẳng hạn như lớp thứ 12, phân phối tương ứng với 30 lần lặp khá tương tự với phân phối tương ứng chỉ với một lần lặp. Điều này có nghĩa là một lần lặp là đủ để làm cho thuật toán WPR hội tụ trong các lớp sâu. Ngoài ra, trong lớp thứ năm, năm lần lặp là đủ để làm cho nó hội tụ.
Để xác thực định lượng các khẳng định chúng tôi đưa ra ở trên, chúng tôi tính phân kỳ KL giữa phân phối tầm quan trọng được đưa ra bởi 30, 5, 1 lần lặp và phân phối được đưa ra bởi 50 lần lặp trong các lớp khác nhau. Kết quả được hiển thị trong Hình 15. Do đó, để đảm bảo hội tụ, chúng tôi đặt số lần lặp thành 30-50, 5-10, và 1 trong ba lớp đầu tiên, các lớp trung bình, và ba lớp cuối cùng, tương ứng. Một điều thú vị khác cần lưu ý là mô hình Transformer và thuật toán WPR gán điểm tầm quan trọng thấp cho hầu hết các token trong các lớp sâu.

Hình 15. Phân kỳ KL giữa phân phối điểm tầm quan trọng được đưa ra bởi số lần lặp khác nhau và phân phối được đưa ra bởi 50 lần lặp trong các lớp khác nhau. Backbone được sử dụng là DeiT-S.

--- TRANG 16 ---
G.2. Ngưỡng Phương sai cho VHF
Để loại trừ nhiễu từ các head hội tụ về phân phối điểm tầm quan trọng không mong muốn (như được hiển thị trong Hình 13), chúng tôi đề xuất VHF và đặt ngưỡng tối thiểu và tối đa cho phương sai của phân phối head. Chúng tôi thực hiện một thí nghiệm ablation để xác định phạm vi phương sai tối ưu. Cấu hình cắt tỉa được hiển thị trong Bảng 7. Sau đó chúng tôi sử dụng khởi tạo ngẫu nhiên và tìm kiếm chùm (k= 2) để tìm cài đặt phạm vi phương sai đủ tốt. Kết quả được hiển thị trong Hình 16, chỉ ra phạm vi [0.01,0.7].

Bảng 7. Cấu hình cắt tỉa được sử dụng để tìm kiếm ngưỡng phương sai tối ưu.
Lớp Cắt tỉa 0 2 4 6 8 10
Tỷ lệ Giữ lại 0.9 0.9 0.85 0.8 0.7 0.65
# Lần lặp 50 50 5 5 1 1

Hình 16. Kết quả thu được trong quá trình tìm kiếm ngưỡng tối ưu. Một bong bóng xanh lớn hơn đại diện cho độ chính xác cao hơn với cài đặt đó.

G.3. Lựa chọn Thiết kế Tối ưu trong S-stage
Như đã thảo luận trong Phần 3.3, không gian thiết kế của S-stage được cấu thành từ ba chiều: (1) nguồn của vector đặc trưng, (2) phương pháp phân vùng, và (3) thước đo tương đồng. Chúng tôi thấy rằng lựa chọn tối ưu là (1) ma trận key, (2) tuần tự (cắt tỉa phần không quan trọng), và (3) cosine similarity, tương ứng. Đây là cài đặt mặc định trong các thí nghiệm sau trừ khi có ghi chú khác. Đối với kết quả trong phần này, các lớp cắt tỉa được chèn sau lớp [1,3,6,9,11] với tỷ lệ giữ lại [1,0.9,0.8,0.7,1] và số lần lặp [30,5,5,1,1] trong I-stage, và 10 token được cắt tỉa trong mỗi S-stage. Lưu ý rằng tất cả kết quả trong phần phụ này được tăng cường bởi token CLS bằng cách gán cho nó một điểm tầm quan trọng lớn hơn √N lần so với các token khác trong quá trình khởi tạo trong I-stage, trong đó N là số lượng token.

Hình 17. Các vector đặc trưng tiềm năng có thể được sử dụng để biểu diễn token.

Bảng 8. Kết quả thí nghiệm ablation cho nguồn của vector đặc trưng.
Đặc trưng Acc@top1 GFLOPS
Xpre 79.113% 3.08
X 79.082% 3.08
K 79.351% 3.08
Q 79.205% 3.08
V 79.097% 3.08

Vector đặc trưng: Như được hiển thị trong Hình 17, các vector đặc trưng đại diện cho token có thể là các vector tương ứng trong ma trận Key, ma trận Query, ma trận Value, vector embedding trung gian trong ma trận Xpre, hoặc vector embedding đầu ra trong ma trận X. Chúng tôi duy trì các cài đặt khác và thay đổi vector đặc trưng được sử dụng. Hiệu suất của các mô hình đã cắt tỉa được hiển thị trong Bảng 8. Nó chỉ ra rằng ma trận Key là nguồn tối ưu của vector đặc trưng.
Phương pháp phân vùng: Sau khi xếp hạng token theo tầm quan trọng của chúng (ví dụ, token{1,2,3,4,5,6}; token 1 có điểm cao nhất và token 6 có điểm thấp nhất), chúng tôi chọn từ các tùy chọn sau: (i) Thay thế: luân phiên gán chúng cho Nhóm A và B, sau đó tầm quan trọng token trung bình trong hai nhóm gần như bằng nhau (ví dụ, A:{2,4,6}, B:{1,3,5}); (ii) Tuần tự-U: gán nửa ít quan trọng hơn của token cho Nhóm A và nửa còn lại cho Nhóm B, có nghĩa là chúng tôi phân vùng token tuần tự và cắt tỉa phần không quan trọng (ví dụ, A:{4,5,6}, B:{1,2,3}); (iii) Tuần tự-I: gán nửa quan trọng hơn của token cho Nhóm A và nửa còn lại cho Nhóm B, có nghĩa là chúng tôi phân vùng token tuần tự và cắt tỉa phần quan trọng (ví dụ, A:{1,2,3}, B:{4,5,6}), (iv) Ngẫu nhiên: gán ngẫu nhiên chúng cho Nhóm A hoặc B; và (v) Không phân vùng: gán tất cả token cho cả hai nhóm mà không phân vùng. Để đánh giá hiệu quả của những tùy chọn này, chúng tôi tiến hành thí nghiệm trong khi giữ tất cả cài đặt khác ở giá trị mặc định. Kết quả được hiển thị trong Bảng 9, trong đó Tuần tự-U đại diện cho lựa chọn (ii) và Tuần tự-I đại diện cho lựa chọn (iii). Nó rõ ràng chỉ ra rằng Tuần tự-U tốt hơn tất cả các phương pháp phân vùng khác.

--- TRANG 17 ---
Thước đo tương đồng: Chúng tôi thí nghiệm với một số thước đo để đo sự tương đồng giữa hai vector, bao gồm cosine similarity, dot product, và Minkowski distance với các giá trị p khác nhau. Khi sử dụng Minkowski distance để đo sự tương đồng giữa các vector, chúng tôi phủ định khoảng cách để tính đến thực tế rằng khoảng cách dài hơn chỉ ra sự tương đồng thấp hơn. Kết quả của những thí nghiệm này, được hiển thị trong Bảng 10, chỉ ra rằng cosine similarity là lựa chọn tốt nhất.

Bảng 9. Kết quả thí nghiệm ablation để chọn phương pháp phân vùng.
Phương pháp Acc@top1 GFLOPS
Ngẫu nhiên 79.055% 3.08
Thay thế 79.179% 3.08
Tuần tự-U 79.351% 3.08
Tuần tự-I 78.898% 3.08
Không phân vùng 78.422% 3.08

Bảng 10. Kết quả thí nghiệm ablation để chọn thước đo tương đồng.
Tương đồng Acc@top1 GFLOPS
dot product 79.257% 3.08
cosine 79.351% 3.08
Manhattan (p= 1) 79.208% 3.07
Euclidean (p= 2) 79.224% 3.07
Minkowski (p= 3) 79.246% 3.07
Minkowski (p= 4) 79.273% 3.07
Minkowski (p= 5) 79.189% 3.07
Minkowski (p=∞) 79.092% 3.07

G.4. Hiệu suất của Zero-TPrune-uni
Kết quả thí nghiệm ablation của Zero-TPrune-uni được hiển thị trong Bảng 11. Backbone để triển khai là DeiT-S, và mô hình được đánh giá trên tập validation ImageNet.
G.5. Tìm kiếm Siêu tham số
Hiệu suất của Zero-TPrune, về mặt độ chính xác, không nhạy cảm với cài đặt siêu tham số miễn là số lượng lớp cắt tỉa nhiều hơn hai và phương sai của tỷ lệ cắt tỉa của chúng bị hạn chế (tức là quá trình cắt tỉa không tập trung vào một hoặc hai lớp). Chúng tôi ngẫu nhiên chọn các cài đặt siêu tham số khác nhau và hiển thị hiệu suất của chúng trong Hình 18. Hình này chỉ ra rằng việc chọn ngẫu nhiên một cài đặt siêu tham số không làm tổn hại nhiều đến hiệu suất của chúng tôi.

Hình 18. Một trăm cài đặt siêu tham số được chọn ngẫu nhiên và hiệu suất tương ứng của chúng sau khi được áp dụng cho DeiT-S không có tinh chỉnh

Để so sánh công bằng với baseline, chúng tôi không sử dụng hiệu suất tốt nhất mà chúng tôi có thể tìm thấy thông qua tìm kiếm siêu tham số. Thay vào đó, chúng tôi sử dụng cài đặt siêu tham số với hiệu suất xấp xỉ trung bình trong các kết quả tìm kiếm. Nó cũng gần với việc đặt tỷ lệ cắt tỉa không đổi trên các lớp khác nhau.
Ngay cả quá trình tìm kiếm siêu tham số đầy đủ cũng nhanh hơn nhiều so với tinh chỉnh. Đối với tìm kiếm siêu tham số, chúng tôi chỉ cần thực hiện suy luận. Cụ thể, đối với MCS, chúng tôi ngẫu nhiên chọn 1024 hình ảnh trong tập dữ liệu validation cho mỗi cài đặt siêu tham số và thu được độ chính xác tương ứng. Chúng tôi thử 2000 cài đặt trên một GPU A100, chỉ yêu cầu 3.8 giờ. Ngược lại, tinh chỉnh DeiT-S trên tập dữ liệu ImageNet yêu cầu 144 giờ GPU A100.
H. So sánh với Các Phương pháp Tiên tiến
Trong phần này, chúng tôi đầu tiên bổ sung so sánh với các phương pháp depth-adaptive khác trong Phần H.1 và sau đó so sánh Zero-TPrune với các phương pháp xếp hạng token dựa trên attention đơn giản hơn trong Phần H.2. Cuối cùng, chúng tôi cung cấp so sánh hiệu suất với các phương pháp cắt tỉa token không cần tinh chỉnh tiên tiến về throughput trong Phần H.3.
H.1. Các Phương pháp Depth-Adaptive
Cắt tỉa token có thể được xem như một biến thể tinh vi của transformer thích ứng độ sâu, chẳng hạn như layer dropping. Một trong những baseline của chúng tôi, A-ViT [43], là một phương pháp thích ứng độ sâu theo token. Thay vì chèn các lớp cắt tỉa và đặt tỷ lệ cắt tỉa cho chúng, nó tính xác suất dừng cho mỗi token tại mỗi lớp và dừng token ở độ sâu thích ứng. Zero-TPrune không có tinh chỉnh cạnh tranh và thậm chí vượt trội hơn A-ViT có tinh chỉnh, như được hiển thị trong Hình 6. A-ViT

--- TRANG 18 ---
Bảng 11. Phân tích đóng góp của các kỹ thuật khác nhau được sử dụng trong Zero-TPrune-uni. Hậu tố "-uni" đại diện cho khởi tạo đồng đều trong I-stage.
Acc@1 Params GFLOPS Throughput (img/s) Phương pháp
79.8% (base) 22M 4.55G 1505.9 Mô hình chưa cắt tỉa
76.8% (-3.0%) 22M 3.08G 2164.4 loại bỏ ngẫu nhiên
78.0% (+1.2%) 22M 3.08G 2142.3 WPR
78.2% (+0.2%) 22M 3.08G 2139.6 WPR + EIR
78.4% (+0.2%) 22M 3.08G 2107.2 WPR + EIR + VHF (I-stage)
78.9% (+0.5%) 22M 3.08G 2066.4 I-stage + S-stage
79.1% (+0.2%) 22M 3.08G 2062.9 I-stage + S-stage + MC Simulation

Bảng 12. So sánh với các phương pháp depth-adaptive trên mô hình DeiT-T. Hiệu suất của Zero-TPrune được thu được không có tinh chỉnh, trong khi các kết quả khác được thu được có tinh chỉnh.
Phương pháp Acc@top1 GFLOPS
DeiT-T [36] 71.3% 1.3
ACT [16] 71.0% 1.0
Confidence threshold [23] 65.8% 1.1
Similarity gauging [13] 69.4% 1.1
PonderNet [1] 66.2% 1.0
DynamicViT [30] 70.9% 0.9
A-ViT [43] 71.0% 0.8
Zero-TPrune w/o FT 70.4% 0.9

vượt trội hơn nghệ thuật trước đây về các phương pháp depth-adaptive. Chúng tôi áp dụng các kết quả tương ứng và so sánh chúng với Zero-TPrune trong Bảng 12. Lưu ý rằng kết quả của Zero-TPrune được thu được off the shelf không có tinh chỉnh, trong khi các kết quả khác được thu được sau khi tinh chỉnh các mô hình thích ứng.
H.2. Các Phương pháp Xếp hạng Token dựa trên Attention
Một trong những baseline của chúng tôi, ATS [14], là một phương pháp xếp hạng tầm quan trọng dựa trên attention. Nó sử dụng attention được đưa ra bởi token CLS để xác định tầm quan trọng của token. Việc đơn giản tính trung bình điểm attention trong ma trận attention là baseline của ATS (Hình 3 trong [14]) và hoạt động kém hơn ATS. Đối với nghiên cứu ablation, chúng tôi thay thế I-stage của chúng tôi bằng lựa chọn tầm quan trọng top-k dựa trên (1) attention token CLS, (2) attention trung bình, và (3) attention trung bình tích lũy để cải thiện hiệu quả của phương pháp chúng tôi. Kết quả được hiển thị trong Bảng 13. Kích thước batch là 512 và S-stage của chúng tôi được kích hoạt trong tất cả cài đặt. Chúng tôi điều chỉnh tỷ lệ cắt tỉa một chút để phù hợp với chi phí FLOPs của các cài đặt khác nhau. I-stage được đề xuất của chúng tôi sử dụng thông tin từ tất cả token trong khi giảm nhiễu từ các token không quan trọng, dẫn đến hiệu suất tốt hơn.

Bảng 13. Hiệu suất của các mô hình DeiT-S đã cắt tỉa không có tinh chỉnh. Throughput được đo trên một GPU NVIDIA A100.
Phương pháp Acc@top1 GFLOPS Throughput(img/s)
DeiT-S 79.8% 4.55 1505.9
CLS Attn. 78.9% 3.00 2179.3
Ave. Attn. 78.4% 2.99 2185.2
Accu. Ave. Attn. 78.5% 2.97 2189.2
I-stage 79.4% 2.97 2188.4

Hình 19. So sánh hiệu suất giữa Zero-TPrune và các phương pháp tiên tiến không cần tinh chỉnh. Backbone Transformer được áp dụng là DeiT-S.

H.3. Các Phương pháp Cắt tỉa Token Không cần Tinh chỉnh
Chúng tôi tiến hành thí nghiệm trên DeiT-S để hiển thị sự vượt trội của Zero-TPrune so với các phương pháp cắt tỉa/hợp nhất token không cần tinh chỉnh tiên tiến. Kết quả thí nghiệm hiển thị sự đánh đổi giữa độ chính xác và throughput được hiển thị trong Hình 19.
I. So sánh giữa Scaling và Pruning
Như được hiển thị trong Bảng 14, Zero-TPrune không thể vượt trội hơn tất cả các phương pháp baseline khi tỷ lệ cắt tỉa tương đối cao (ví dụ,

--- TRANG 19 ---
Hình 20. Hiệu suất off-the-shelf của các mô hình ViT dưới ToMe [3]. Hình này được chấp nhận từ [3].

Bảng 14. Hiệu suất của các mô hình AugReg, LV-ViT, và SWAG đã cắt tỉa không có tinh chỉnh. Các mô hình SWAG thực hiện suy luận trên hình ảnh 384px.
Phương pháp Acc@top1 GFLOPS
LV-ViT-M 84.0% 12.7
+ ATS 80.9% 6.4
+ ToMe 81.6% 6.3
+ Zero-TP 81.4% 6.3
MAE 83.62% 55.4
+ATS 78.39% 29.1
+ToMe 78.95% 28.8
+Zero-TP 78.94% 28.6
SWAG 85.30% 55.6
+ATS 81.03% 27.8
+ToMe 84.59% 28.4
+Zero-TP 84.04% 28.3

giảm GFLOPS 50%) được áp dụng cho các mô hình lớn (ví dụ, DeiT-L). Tuy nhiên, trong trường hợp này, việc mở rộng sang mô hình nhỏ hơn thường là lựa chọn tốt hơn. ToMe vượt trội hơn Zero-TPrune khi các mô hình lớn bị cắt tỉa mạnh. Do đó, chúng tôi sử dụng kết quả từ ToMe để minh họa điểm này.
Trong Hình 20, ToMe được áp dụng cho các backbone ViT khác nhau với các cấu hình khác nhau. Các điểm khác nhau trên cùng một đường cong đại diện cho các cấu hình khác nhau được áp dụng cho cùng một backbone. Điểm đầu tiên từ bên trái trên mỗi đường cong đại diện cho mô hình chưa cắt tỉa. Việc cắt tỉa mạnh một mô hình ngụ ý chuyển từ điểm đầu tiên từ bên trái trên một đường cong đã cho đến điểm cuối cùng trên đường cong này, điều này tăng throughput nhưng chịu độ chính xác thấp hơn. Việc chuyển từ điểm đầu tiên từ bên trái trên một đường cong đã cho đến điểm đầu tiên trên đường cong khác trực tiếp mở rộng kích thước của mô hình mà không cắt tỉa. Việc cắt tỉa mạnh các mô hình lớn (ViT-L và ViT-B) kém hiệu suất hơn việc mở rộng chúng về cả độ chính xác và throughput. Ngược lại, đối với mô hình ViT-S, mặc dù việc mở rộng vượt trội hơn việc cắt tỉa mạnh về throughput, nó đạt được độ chính xác thấp hơn so với việc cắt tỉa mạnh.
