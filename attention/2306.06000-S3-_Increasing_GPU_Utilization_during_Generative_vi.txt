# 2306.06000.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2306.06000.pdf
# Kích thước tệp: 1119416 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
S3: Tăng Mức Sử Dụng GPU trong Suy Luận Sinh để Đạt Thông Lượng Cao Hơn
Yunho Jin
Đại học HarvardChun-Feng Wu
Đại học Quốc gia Yang Ming
Chiao TungDavid Brooks
Đại học HarvardGu-Yeon Wei
Đại học Harvard

Tóm tắt
Sinh văn bản với một mô hình ngôn ngữ lớn (LLM) tiêu thụ lượng bộ nhớ khổng lồ. Ngoài các tham số mô hình đã lớn, bộ nhớ đệm key/value (KV) lưu trữ thông tin về các token trước đó trong một chuỗi có thể phát triển thậm chí còn lớn hơn chính mô hình. Vấn đề này trở nên trầm trọng hơn trong một trong những framework phục vụ LLM hiện tại khi dành riêng độ dài chuỗi tối đa của bộ nhớ cho bộ nhớ đệm KV để đảm bảo sinh ra một chuỗi hoàn chỉnh vì chúng không biết độ dài chuỗi đầu ra. Điều này hạn chế chúng ta sử dụng kích thước batch nhỏ hơn dẫn đến mức sử dụng GPU thấp hơn và trên hết, thông lượng thấp hơn. Chúng tôi lập luận rằng việc thiết kế một hệ thống với kiến thức tiên nghiệm về chuỗi đầu ra có thể giảm thiểu vấn đề này. Để làm điều này, chúng tôi đề xuất S3, dự đoán độ dài chuỗi đầu ra, lập lịch các truy vấn sinh dựa trên dự đoán để tăng mức sử dụng tài nguyên thiết bị và thông lượng, và xử lý các dự đoán sai. Phương pháp đề xuất của chúng tôi đạt được thông lượng 6.49 × so với những hệ thống giả định trường hợp xấu nhất cho độ dài chuỗi đầu ra.

1 Giới thiệu
Sinh văn bản đã trở nên ngày càng phổ biến trong các dịch vụ khác nhau, dẫn đến sự gia tăng trong việc sử dụng các mô hình ngôn ngữ lớn (LLMs) được biết đến với độ chính xác cao. LLMs có các đặc điểm riêng biệt khác với các mô hình không dựa trên Transformer, bao gồm yêu cầu lượng lớn tài nguyên tính toán và bộ nhớ. Cụ thể, chúng tôi quan sát thấy rằng các LLM dựa trên Transformer thường bị giới hạn bởi dung lượng và băng thông bộ nhớ, dẫn đến việc sử dụng tài nguyên tính toán chưa hiệu quả đáng kể. Khi phục vụ GPT-J trên GPU NVIDIA A100, mức sử dụng tài nguyên tính toán GPU có thể thấp tới 0.4%. Điều này làm nổi bật bản chất bị giới hạn bởi bộ nhớ của các LLM và nhu cầu sử dụng bộ nhớ hiệu quả để tăng mức sử dụng tài nguyên tính toán GPU.

Một cách tiếp cận phổ biến để tăng mức sử dụng GPU và nâng cao thông lượng là tăng kích thước batch. Điều này là do các đầu vào trong một batch chia sẻ cùng trọng số mô hình, do đó GPU chỉ cần tải trọng số mô hình từ bộ nhớ băng thông cao (HBM) đến SRAM trên chip một lần và tái sử dụng cho tất cả đầu vào trong batch. GPU sử dụng nhiều tài nguyên tính toán hơn khi xử lý cùng trọng số mô hình. Tăng kích thước batch là một kỹ thuật tối ưu hóa đơn giản và do đó, thường được sử dụng trong việc phục vụ các mô hình mạng neural tích chập và fully-connected [1, 2].

Tuy nhiên, lớp self-attention trong các LLM sinh văn bản dựa trên Transformer đặt ra thách thức cho việc tối ưu hóa đơn giản này do bản chất tự hồi quy của nó. Cụ thể, khi sinh một token mới trong một chuỗi, mô hình cần chú ý đến tất cả các token trước đó trong chuỗi, yêu cầu mô hình giữ lại tất cả thông tin từ các token trước đó và lưu trữ chúng trong HBM. Chúng tôi gọi vùng này trong HBM lưu trữ thông tin là bộ nhớ đệm key/value (KV cache). Kích thước của bộ nhớ đệm KV tăng với kích thước batch lớn hơn và chuỗi dài hơn, điều này giới hạn kích thước batch tối đa, do đó làm giảm mức sử dụng GPU và cuối cùng giảm thông lượng. Để hỗ trợ kích thước bộ nhớ đệm KV ngày càng tăng, thư viện Transformers của Huggingface [3] liên tục cấp phát bộ nhớ mới tại mỗi lần sinh token

Bản thảo. Đang xem xét.arXiv:2306.06000v1  [cs.AR]  9 Jun 2023

--- TRANG 2 ---
Hình 1: Sự đánh đổi độ trễ so với thông lượng giữa các mô hình khác nhau (bên trái) và số lượng GPU (bên phải, phân phối GPT-3 cho 6, 8, và 10 GPU) khi sinh 60 token, lấy cảm hứng từ FlexGen [6]. Các điểm đánh dấu trong các đường thể hiện kích thước batch, từ 1 đến kích thước batch tối đa có thể tải trên GPU A100, tăng theo lũy thừa của hai. Cấp phát lượng bộ nhớ chính xác cho mỗi chuỗi mở rộng đường cong đến thông lượng cao hơn với chi phí độ trễ cao hơn. Các đường liền mô tả sự đánh đổi mà các hệ thống vanilla phải đối mặt và các đường chấm mô tả S3 có thể mở rộng sự đánh đổi này như thế nào. Các con số thể hiện kích thước batch tối đa cho S3 và hệ thống vanilla. Đường thẳng đứng trong hình bên trái biểu thị SLO độ trễ để đọc một chuỗi dài 60-token.

và phát sinh chi phí độ trễ liên quan đến việc cấp phát bộ nhớ. Điều này cải thiện khả năng sử dụng vì người dùng không cần biết độ dài chuỗi đầu ra nhưng phải chịu độ trễ suy luận dài. Thay vào đó, thư viện FasterTransformer của NVIDIA [4] cấp phát trước bộ nhớ cho độ dài chuỗi tối đa, điều này cuối cùng lãng phí bộ nhớ bằng cách cấp phát nhiều hơn mức cần thiết. Cách tiếp cận này giới hạn kích thước batch và ưu tiên độ trễ hơn thông lượng. Xu hướng hướng tới độ dài chuỗi tối đa dài trong các mô hình gần đây (ví dụ, 8K hoặc 32K) [5] khuếch đại chi phí bộ nhớ đệm KV, đòi hỏi việc sử dụng bộ nhớ hiệu quả hơn trong việc phục vụ các mô hình sinh văn bản dựa trên Transformer.

Trước những thách thức này, chúng tôi đề xuất S3, scheduling sequences with speculation, một framework tối đa hóa thông lượng thông qua dự đoán độ dài chuỗi đầu ra và giảm lãng phí bộ nhớ. Việc cấp phát bộ nhớ thường xuyên trong Transformers của Huggingface và kích thước batch bị giới hạn trong FasterTransformer bắt nguồn từ việc chúng ta thiếu kiến thức tiên nghiệm về độ dài chuỗi đầu ra. S3 giải quyết những vấn đề này bằng cách dự đoán độ dài chuỗi đầu ra mong đợi và cấp phát lượng bộ nhớ tương ứng cho mỗi truy vấn. Nó cũng lập lịch các chuỗi để tăng mức sử dụng GPU. Cuối cùng, S3 chạy một supervisor ở nền để phát hiện các dự đoán sai và điều chỉnh kích thước bộ nhớ được cấp phát để chính xác hơn. Bằng cách tích hợp các thành phần này với nhau, S3 tối ưu hóa việc sử dụng bộ nhớ và lập lịch để tối đa hóa thông lượng trong quá trình triển khai các LLM dựa trên Transformer để sinh văn bản trên GPU.

Có hai loại kịch bản triển khai LLM: trực tuyến và ngoại tuyến. Các kịch bản trực tuyến như chatbot [7, 8] yêu cầu nhà cung cấp dịch vụ sinh ra một chuỗi trong ràng buộc mục tiêu mức dịch vụ (SLO) độ trễ chặt chẽ. Các kịch bản ngoại tuyến bao gồm các ứng dụng như chấm điểm [9] hoặc xử lý dữ liệu [10], và có SLO độ trễ lỏng lẻo, nhấn mạnh thông lượng hơn độ trễ đầu cuối. Ngược lại, FasterTransformers [4] và xFormers [11] ưu tiên giảm độ trễ. Chúng tôi lập luận rằng việc đảm bảo độ trễ vẫn dưới ràng buộc SLO khiến việc ưu tiên giảm độ trễ thêm nữa trở nên không cần thiết. Để làm điều này, chúng tôi thiết kế S3 để đạt thông lượng cao hơn dưới những SLO độ trễ đó. Hình 1 làm nổi bật S3 có thể cải thiện thông lượng bao nhiêu khi đánh đổi độ trễ. Đối với các kịch bản trực tuyến, chúng tôi giả định SLO độ trễ được đặt theo tốc độ đọc trung bình của người đọc tiếng Anh, 4 từ mỗi giây [12] và 0.1875 giây mỗi token [13]. Các mô hình trong hình bên trái nhỏ hơn 100 tỷ tham số thỏa mãn SLO này cho tất cả kích thước batch có thể. SLO này cung cấp cho nhà cung cấp dịch vụ cơ hội cải thiện thông lượng trên tất cả các mô hình. Thực tế, đối với LLAMA-33B [14], có cơ hội để có được lợi ích thông lượng mà không có hình phạt độ trễ. Hình bên phải, quét qua số lượng GPU khác nhau, cho thấy cơ hội tối đa hóa thông lượng trong các kịch bản ngoại tuyến có SLO độ trễ lỏng lẻo.

Chúng tôi đánh giá S3, đánh giá cả thông lượng và hiệu quả chi phí của nó. Phân tích của chúng tôi bao gồm cả kịch bản ngoại tuyến và trực tuyến. Trong các kịch bản trực tuyến dưới ràng buộc SLO độ trễ tốc độ đọc trung bình, chúng tôi thấy rằng S3 có thể sinh ra đến 6.49× chuỗi nhiều hơn trong khi tuân thủ cùng ràng buộc SLO.

2

--- TRANG 3 ---
Trong các kịch bản ngoại tuyến, chúng tôi quan sát thấy rằng S3 đạt được tăng tốc lên đến 6.49× cho các mô hình khác nhau. S3 không ảnh hưởng đến perplexity của các mô hình vì nó không thay đổi kiến trúc của các mô hình. Hơn nữa, chúng tôi đánh giá hiệu quả chi phí của S3 và thấy rằng sử dụng 6 GPU, S3 cung cấp thông lượng gần như giống hệt so với một hệ thống vanilla với 10 GPU.

Tóm lại, chúng tôi đóng góp những điều sau:
• Chúng tôi tăng kích thước batch có thể đạt được dưới SLO độ trễ dài hơn và cho phép nhà cung cấp dịch vụ phục vụ với thông lượng cao hơn trong cả kịch bản trực tuyến và ngoại tuyến bằng cách sử dụng kích thước batch lớn hơn.
• Chúng tôi fine-tune một mô hình Distillbert để dự đoán độ dài chuỗi đầu ra cho một lời nhắc đầu vào.
• Chúng tôi cung cấp một cơ chế để phục hồi từ các dự đoán sai. S3 preempt các chuỗi vượt quá bộ nhớ được cấp phát của chúng và retrain predictor để học từ những sai lầm của nó.

2 Bối cảnh và Động lực

2.1 Mô hình AI Sinh
Một mô hình sinh dựa trên Transformer là tự hồi quy. Nó dự đoán token có khả năng cao nhất dựa trên các token trong quá khứ. Vì mô hình sinh một token tại một thời điểm, nó phải lặp lại chính nó n lần để sinh ra một chuỗi dài n-token. Một lần lặp bao gồm một token đầu vào đi qua mô hình là một stack các lớp transformer chứa một attention, hai layer norm, và hai lớp feed-forward. Đặc biệt, lớp self-attention sử dụng thông tin về các token trong quá khứ để sinh token tiếp theo.

Ví dụ, mô hình tại lần lặp thứ i chú ý token hiện tại (ti) với mọi token nó đã sinh ra (t0, ...ti−1) trong lớp self-attention. Chúng ta có thể biểu diễn lớp self-attention như:
hout=softmax (qi·KT/√dh)·V
trong đó dh là chiều ẩn của mô hình, hout, qi∈Rdh là vector ẩn đầu ra, vector truy vấn hiện tại, tương ứng, và K, V∈Ri×dh là các ma trận key và value. Hàng thứ j trong các ma trận K và V thể hiện các vector key và value của tj, tương ứng. Hai tích vô hướng chú ý ti đến tất cả các vector key và value trong chuỗi hiện tại.

Mô hình lưu trữ các ma trận K và V như bộ nhớ đệm key/value (KV cache) để tránh phải sinh các vector key và value tại mỗi lần lặp. Nếu không, nó phải lưu trữ các trạng thái ẩn của mọi token trước đó và nhân nó với ma trận trọng số WQKV∈Rdh×2dh tại mọi lớp transformer. Điều này sẽ yêu cầu 2(i−1)dh FLOP mỗi lớp gần như giống hệt 2idh FLOP cho lớp self-attention tại ti. Kích thước của bộ nhớ đệm KV là 4ldh byte mỗi token khi sử dụng số half-precision. Cache sử dụng 2 byte cho mỗi số cho cả bộ nhớ đệm key và value trong đó l là số lớp transformer trong một mô hình. Ví dụ, GPT-NEOX [15], một mô hình 20 tỷ tham số, có 44 lớp và 6144 chiều ẩn và do đó sử dụng 1MB cho mỗi KV cache mỗi token.

2.2 Quản lý KV Cache trên GPU
Bộ nhớ đệm KV tương đối nhỏ (ví dụ, vài MB) và có thể dễ dàng được lưu trữ trong GPU HBM (bộ nhớ băng thông cao) khi chuỗi ngắn vì cache lưu trữ thông tin về các token trước đó trong chuỗi. Nó tăng khi mô hình sinh nhiều token hơn. Sử dụng bản chất động này, Transformers của Huggingface [3] (HF-Transformers) liên tục cấp phát thêm bộ nhớ cho bộ nhớ đệm KV và dừng tính toán cho đến khi hoàn thành. Cách tiếp cận này cho phép thư viện cấp phát lượng bộ nhớ chính xác cho mỗi cache với chi phí truy cập bộ nhớ thường xuyên.

Để giảm thiểu điều này, thư viện FasterTransformer của NVIDIA dành riêng bộ nhớ cho độ dài chuỗi tối đa cho mỗi chuỗi [4, 16]. Nó loại bỏ các truy cập bộ nhớ dư thừa bằng cách đơn giản là điền vào bộ nhớ được dành riêng theo cách chỉ thêm vào. Tuy nhiên, cách tiếp cận này đi kèm với nhược điểm riêng vì nó dành riêng nhiều bộ nhớ hơn mức cần thiết chặt chẽ cho các chuỗi. Đối với GPT-NEOX với độ dài chuỗi tối đa 2048 token, FasterTransformer dành riêng 2048 token ngay cả cho các chuỗi cuối cùng dài 50 token. Kích thước batch tối đa mà FasterTransformer có thể sử dụng cho

3

--- TRANG 4 ---
0%
100%
0%Độ dài chuỗi(a)(b)Kích thước batchHình 2: Mức sử dụng tính toán NVIDIA A100 của (a) GPT-J và (b) GPT-NEOX.

mô hình này với GPU A100 80GB là ít hơn 20 với kích thước mô hình 40GB và 2.2GB mỗi chuỗi. Kích thước batch nhỏ không sử dụng hết các tài nguyên tính toán khổng lồ trong GPU. Lý do để dành riêng lượng bộ nhớ bằng độ dài chuỗi tối đa ngay cả với vấn đề sử dụng chưa hiệu quả là để đảm bảo rằng nó sinh ra các chuỗi đầy đủ và nâng cao trải nghiệm người dùng.

2.3 Quan sát
Các mô hình ngôn ngữ bị giới hạn bởi bộ nhớ Chúng tôi chứng minh mức độ sử dụng tài nguyên GPU chưa hiệu quả khi chúng tôi chạy GPT-J với 6B tham số trên GPU A100. Mô hình tương đối nhỏ hơn cho thấy phổ rộng hơn của các kích thước batch và độ dài chuỗi khác nhau vì chúng ta có thể fit các batch lớn hơn với chuỗi dài hơn trong GPU. Hình 2 (a) cho thấy mức sử dụng GPU quét qua các kích thước batch và độ dài chuỗi khác nhau. Như hình cho thấy, tăng kích thước batch đạt được mức sử dụng cao hơn nhưng cuối cùng phải đối mặt với vách bộ nhớ, nơi lỗi hết bộ nhớ (OOM) giết chết tiến trình. Lấy các batch với độ dài chuỗi 1024 làm ví dụ. 8 chuỗi là kích thước batch tối đa và do đó 12.19% là mức sử dụng tối đa mà chúng ta có thể đạt được với độ dài chuỗi này. Hình 2 (b) cho thấy sự sử dụng chưa hiệu quả tương tự trong mô hình GPT-NEOX lớn hơn do vấn đề vách bộ nhớ. Mô hình này đối mặt với vách bộ nhớ với kích thước batch nhỏ hơn và chuỗi ngắn hơn vì mô hình tiêu thụ nhiều bộ nhớ GPU hơn. Xin lưu ý rằng HF-Transformers vẫn cần biết độ dài chuỗi đầu ra trước khi tạo batch đầu vào để tránh vách bộ nhớ.

Như các hình minh họa, tăng kích thước batch có thể nâng cao thông lượng trong mạng neural. Cách tiếp cận này không yêu cầu tối ưu hóa phức tạp và cho phép GPU tải trọng số mô hình từ HBM đến SRAM trên chip chỉ một lần và chia sẻ nó cho số lượng lớn hơn các đầu vào. Bằng cách này, GPU có thể kích hoạt các tài nguyên tính toán nhàn rỗi và xử lý đồng thời nhiều đầu vào. Tuy nhiên, vách bộ nhớ đặt ra thách thức, giới hạn việc sử dụng tài nguyên bổ sung. Tuy nhiên, như chúng tôi trình bày chi tiết, vấn đề này có thể được giải quyết.

Lý do đằng sau sự không hiệu quả Cả việc cấp phát bộ nhớ thường xuyên trong HF-Transformers và kích thước batch bị giới hạn trong FasterTransformer đều xuất phát từ việc chúng ta không biết độ dài chuỗi được sinh ra. Nếu chúng ta biết độ dài chính xác của chuỗi được sinh ra, chúng ta có thể cấp phát bộ nhớ chính xác cho mỗi chuỗi và giải quyết các vấn đề về việc dành riêng bộ nhớ lặp đi lặp lại và cấp phát bộ nhớ không cần thiết trong HF-Transformers và FasterTransformer, tương ứng.

3 Thiết kế S3
S3 là một framework co-design hệ thống-thuật toán tối đa hóa mức sử dụng GPU với dự đoán độ dài chuỗi để đạt thông lượng cao hơn. S3 có ba thành phần như được hiển thị trong Hình 3: 1) predictor, 2) scheduler, và 3) supervisor. Một truy vấn sinh văn bản đến trong một pool yêu cầu trong DRAM host. Predictor sau đó dự đoán độ dài chuỗi đầu ra của nó mà scheduler sử dụng để tạo batch yêu cầu. Scheduler gửi batch đến GPU và mô hình text generator sinh văn bản trong

4

--- TRANG 5 ---
GPUHostSchedulerCấp phátSupervisorĐộ dài dự đoánMức sử dụng hiện tạiPreemptPhản hồiPredictorTextGeneratorHình 3: Tổng quan về S3. Các hộp màu vàng biểu thị các thành phần mới được đề xuất bởi S3.

batch. Supervisor giám sát mức sử dụng GPU và xử lý các dự đoán sai. Chúng tôi mô tả chi tiết từng thành phần và cách chúng tương tác với nhau trong phần này.

Predictor độ dài chuỗi đầu ra Chúng tôi sử dụng một predictor để dự đoán độ dài chuỗi đầu ra và giải quyết các vấn đề cấp phát bộ nhớ thường xuyên và dư thừa. Cụ thể, chúng tôi fine-tune một mô hình Distilbert [17] đã được huấn luyện cho phân loại chuỗi để phân loại bucket độ dài nào mà độ dài chuỗi đầu ra rơi vào. Chúng tôi phân bucket độ dài chuỗi vì đã biết rằng các mô hình machine learning không có khả năng tìm kiếm "dặm cuối" tốt như việc thu hẹp các ứng viên xuống dặm cuối. Mỗi bucket được cấp phát phạm vi max sequence length/number of buckets và chúng tôi sử dụng 10 bucket.

Để làm điều này, chúng tôi fine-tune mô hình trên tập dữ liệu Alpaca [18], một trong những tập dữ liệu hỏi-đáp tiêu biểu, và sử dụng các câu hỏi làm đầu vào và độ dài của các câu trả lời làm nhãn. Chúng tôi quan sát thấy rằng predictor này dự đoán đúng bucket với độ chính xác 98.61%. Khoảng cách trung bình giữa các dự đoán sai và bucket đúng là 1.03 có nghĩa là lỗi hội tụ về 0 khi chúng ta tăng gấp đôi kích thước bucket. Chúng tôi cũng đánh giá predictor trên một mô hình được fine-tune với tập dữ liệu Google Natural-Question [19] và quan sát độ chính xác 77.13%. Nó mắc lỗi nhỏ hơn thường xuyên hơn lỗi lớn hơn. Để hoàn thiện, chúng tôi fine-tune một mô hình trên tập dữ liệu Pile [20], một tập dữ liệu không phải hỏi-đáp, và thấy độ chính xác 65.6%. Predictor cho thấy độ chính xác cao đáng ngạc nhiên so với việc đoán ngẫu nhiên các bin vì cách sau chỉ đúng 10% thời gian.

Chúng tôi chọn Distilbert, một mô hình 66 triệu tham số vì kích thước nhỏ và suy luận nhanh. Kích thước mô hình không đáng kể vì nó nhỏ hơn ngay cả một lớp transformer duy nhất trong các mô hình quy mô tỷ (ví dụ, 214 triệu cho mô hình GPT-J 6 tỷ [21]). Độ trễ cũng không đáng kể vì mô hình predictor chỉ chạy một lần khi một yêu cầu đến máy chủ trong khi mô hình sinh văn bản chạy n lần để sinh ra một chuỗi đầu ra dài n-token. Chúng tôi đo rằng Distilbert mất 3.7ms để chạy so với vài giây cho các mô hình sinh văn bản trên GPU NVIDIA A100.

Scheduler chuỗi nhận biết độ dài Scheduler tạo batch và lập lịch các chuỗi dựa trên kết quả dự đoán để tối đa hóa mức sử dụng GPU mà không vượt quá dung lượng HBM của GPU. Chúng ta có thể xây dựng vấn đề này như một biến thể của bài toán bin packing với một bin duy nhất. Ràng buộc dung lượng của bin là dung lượng HBM và trọng số item là kích thước của bộ nhớ đệm KV cho mỗi chuỗi.

Chúng tôi sử dụng thuật toán decreasing first fit [22] như giải pháp cho bài toán bin packing vì tính đơn giản của nó. Scheduler đầu tiên sắp xếp các chuỗi trong pool yêu cầu chứa các chuỗi cần được lập lịch theo thứ tự giảm dần. Nó lặp qua pool và kiểm tra xem bộ nhớ đệm KV của chuỗi hiện tại có không vượt quá HBM có sẵn hay không. Nếu vậy, nó bao gồm chuỗi trong batch hiện tại và giảm HBM có sẵn bằng kích thước của bộ nhớ đệm KV. Scheduler tiếp tục quá trình này cho đến khi không có HBM có sẵn hoặc nó đã lặp qua toàn bộ pool yêu cầu.

Batch kết quả có hình dạng không đều trong đó một số chuỗi dài hơn những chuỗi khác. Thật không may, các framework hiện tại hoặc không hỗ trợ batch có hình dạng không đều [3, 4] hoặc hỗ trợ nó với hiệu suất hạn chế [23]. Những framework không hỗ trợ chức năng này đệm các chuỗi ngắn với padding token để khớp với độ dài của các chuỗi trong cùng batch. Các padding token lãng phí cả tính toán và bộ nhớ vì chúng không chứa thông tin hữu ích. ORCA [16] giới thiệu một giải pháp thú vị cho vấn đề này. Nó đầu tiên xác định rằng các đầu vào cho các lớp nhất định (ví dụ, feed-forward) trong Transformer có cùng hình dạng bất kể độ dài chuỗi và tạo batch cho chúng mà không đệm các chuỗi ngắn. Nó tuần tự hóa các đầu vào cho các lớp (tức là, self-attention) yêu cầu

5

--- TRANG 6 ---
Bảng 1: Kiến trúc mô hình được sử dụng trong các đánh giá
Mô hình Số Tham số Số lớp Chiều mô hình Số đầu
GPT-J [21] 6B 28 5120 16
LLAMA 13B [14] 13B 40 4096 40
GPT-NEOX [15] 20B 44 6144 64
LLAMA 33B [14] 30B 60 6656 52
GPT3 175B [26] 175B 96 12288 96

các đầu vào có hình dạng giống hệt thay vì batch-process chúng. ORCA cho thấy rằng điều này có tác động không đáng kể đến độ trễ vì các đầu vào cho các lớp self-attention không chia sẻ trọng số và không hưởng lợi từ batching. Như vậy, chúng tôi theo ORCA và sử dụng kỹ thuật selective batching của nó.

Cũng mượn từ ORCA kỹ thuật lập lịch cấp độ lặp, S3 không chờ cho đến khi tất cả các chuỗi trong batch hiện tại hoàn thành sinh. Thay vào đó, nó kiểm tra xem có chuỗi nào trong batch đã hoàn thành sinh tại mỗi lần lặp hay không. Điều này cấp cho S3 tính linh hoạt lập lịch cao hơn và loại bỏ mọi thời gian chờ dư thừa. Cuối cùng, nếu một mô hình không thể fit trong một GPU, S3 sử dụng pipeline parallelism và phân mảnh các mô hình theo độ chi tiết lớp Transformer.

Supervisor Supervisor chịu trách nhiệm giám sát mức sử dụng GPU và xử lý các dự đoán sai. Supervisor chạy ở nền để kiểm tra không gian có sẵn trong HBM và chuyển thông tin đến scheduler. Scheduler sau đó thêm một chuỗi vào batch đang chạy nếu bộ nhớ có sẵn đủ lớn cho chuỗi đó.

Supervisor cũng chịu trách nhiệm xử lý các dự đoán sai. Trong trường hợp dự đoán ngắn, supervisor preempt những chuỗi vượt quá bộ nhớ dành riêng của chúng. Nó giám sát độ dài của các chuỗi đầu ra hiện tại và đuổi chúng nếu chúng chưa hoàn thành nhưng đã sử dụng hết bộ nhớ dành riêng. Nó di chuyển không đồng bộ trạng thái hiện tại của những chuỗi đó bao gồm bộ nhớ đệm KV và các token được sinh ra đến pool yêu cầu và giải phóng bộ nhớ GPU. Bây giờ các ma trận K và V bị phân mảnh với các hàng trống nơi bộ nhớ đệm KV bị đuổi ban đầu được lưu trữ. Supervisor dịch chuyển các hàng bên dưới hàng trống để tất cả các hàng được lưu trữ liền kề. Định dạng bộ nhớ này được yêu cầu bởi các thư viện hiện tại [24,25] và cũng giải quyết vấn đề phân mảnh. Cuối cùng, supervisor tăng gấp đôi bộ nhớ được gán cho các chuỗi bị đuổi để sửa dự đoán ngắn.

Cuối cùng, supervisor liên tục huấn luyện predictor ở nền. Nó sử dụng các chuỗi mà predictor đã nhầm để huấn luyện predictor để nó có thể học từ những sai lầm của mình. Thời gian huấn luyện này tương đối ngắn và phép đo của chúng tôi cho thấy rằng mỗi lần lặp huấn luyện mất 11ms trung bình trong khi sinh chuỗi mất vài giây hoặc hơn. Điều này ngụ ý rằng chi phí retrain ít hơn 10% ngay cả khi chúng ta huấn luyện predictor trong 10 epoch.

Tổng hợp tất cả Chúng tôi tóm tắt phần này với lời giải thích về cách S3 sử dụng từng thành phần để phục vụ một yêu cầu sinh chuỗi. Đầu tiên, các yêu cầu sinh văn bản đến pool yêu cầu. Predictor dự đoán độ dài chuỗi đầu ra của các chuỗi trong pool. Supervisor chạy ở nền và kiểm tra việc sử dụng HBM hiện tại. Tiếp theo, scheduler sử dụng cả dự đoán và HBM có sẵn để tạo batch yêu cầu để sử dụng GPU tối đa. Nó hoàn thành công việc bằng cách lập lịch batch đó cho GPU để sinh các chuỗi được lập lịch.

4 Đánh giá
Chúng tôi cho thấy rằng S3 đạt thông lượng cao hơn bằng cách dự đoán độ dài chuỗi đầu ra. Chúng tôi cũng cho thấy rằng S3 có thể giảm chi phí phục vụ mô hình bằng cách sử dụng ít GPU hơn.

Môi trường Chúng tôi chạy đánh giá trên GPU NVIDIA 80GB A100 được kết nối với DRAM host qua PCIe 4.0 ×8 trong Máy chủ Lenovo ThinkSystem SD650-N V2 [27].

Baseline Chúng tôi so sánh với ORCA [16], một hệ thống phục vụ Transformer tăng thông lượng bằng lập lịch cấp độ lặp và selective batching. ORCA phải cấp phát bộ nhớ cho độ dài chuỗi tối đa khi nó không biết độ dài chuỗi đầu ra để đảm bảo sinh chuỗi đầy đủ. S3 dự đoán độ dài chuỗi đầu ra và cấp phát bộ nhớ dựa trên dự đoán.

6

--- TRANG 7 ---
01020304050
GPT-JLLAMA 13BGPT NEOXLLAMA 33BChuỗi / giâyORCAS³Oracle(a) Thông lượng tối đa
0100200300400500600
GPT-JLLAMA 13BGPT NEOXLLAMA 33BChuỗiORCAS³Oracle (b) Chuỗi được sinh dưới SLO độ trễ

Hình 4: Độ trễ và thông lượng của các mô hình khác nhau.

Bảng 2: Kích thước batch trung bình và số lần lặp cho các mô hình khác nhau
MôhìnhORCA S3Oracle
Kích thước batch Số lần lặp Kích thước batch Số lần lặp Kích thước batch Số lần lặp
GPT-J 69.94 7988 530 1054 564.88 989
LLAMA 13B 31.61 17675 274.66 2034 527.04 1060
GPT-NEOX 16.89 33069 157.59 3545 292.19 1912
LLAMA 33B 4 139790 42.47 13155 77.35 7223

Chúng tôi triển khai các hệ thống trên FasterTransformer [4] vì thư viện này nhanh hơn HF-Transformers [3] do nhiều tối ưu hóa hơn. Chúng tôi cũng so sánh S3 với một hệ thống lý tưởng với predictor hoàn hảo mà chúng tôi gọi là Oracle.

Mô hình Chúng tôi sử dụng các mô hình từ 6 tỷ tham số đến 175 tỷ tham số cho đánh giá. Chi tiết của các mô hình này được giải thích trong bảng 1.

4.1 Phân tích Thông lượng
Chúng tôi đánh giá thông lượng của S3 sử dụng tập dữ liệu Alpaca [18], một tập dữ liệu hỏi-đáp nguồn mở phổ biến. Cụ thể, chúng tôi truy vấn S3 với các câu hỏi và yêu cầu nó sinh ra các câu trả lời.

Kịch bản ngoại tuyến: Thông lượng tối đa Hình 4 (a) báo cáo thông lượng tối đa tính bằng chuỗi trên giây cho các mô hình khác nhau. Chúng tôi đo thông lượng sử dụng kích thước batch tối đa của mỗi cấu hình. Nó cho thấy rằng S3 vượt trội ORCA với 1.13× và lên đến 6.49× và khớp gần với Oracle, khác biệt từ 9.34% và lên đến 40.52%. Sự khác biệt được khuếch đại với các mô hình lớn hơn vì kích thước batch bị giới hạn ngay cả đối với S32 vì hầu hết HBM được sử dụng để giữ trọng số mô hình. Kích thước batch trong S3 bị cắt trước khi bão hòa thông lượng như được hiển thị trong Hình 1.

Chúng ta có thể nhận thấy rằng thông lượng tối đa tăng 6.49× trong khi kích thước batch tăng gần 10× cho mọi mô hình. Điều này xuất phát từ hành vi độc đáo của Transformers. Các lớp Feed-forward trong các mô hình hưởng lợi từ việc tạo batch các lớp trong khi các lớp self-attention thì không. Điều này là do các đầu vào trong feed-forwards chia sẻ cùng trọng số trong khi các đầu vào trong self-attentions chú ý đến các chuỗi riêng của chúng. Lợi ích hiệu suất của S3 được tăng lên khi phần song song lớn hơn phần tuần tự. Tuy nhiên, tăng kích thước batch đòi hỏi thêm nhiều đầu vào tuần tự hơn do đó làm tăng phần tuần tự. GPT-J là một ví dụ tốt về đặc tính này cho thấy mức tăng thông lượng tương tự 1.13× từ ORCA đến S3 và 1.09× từ S3 và Oracle trong khi kích thước batch khác biệt 8.69× và 1.97×, tương ứng.

Kịch bản trực tuyến: Thông lượng nhận biết SLO Bây giờ chúng tôi xem xét một kịch bản với ràng buộc SLO độ trễ. Chúng tôi đặt SLO là 0.1875 giây để sinh một token cho rằng người đọc tiếng Anh trung bình có thể đọc với tốc độ 4 từ mỗi giây [12] hoặc 0.25 giây mỗi từ, và 0.75 từ mỗi token [13]. Chúng tôi tiếp theo tính toán SLO độ trễ cho mỗi chuỗi bằng cách nhân 0.1875 với độ dài chuỗi của nó (ví dụ, 11.25s cho một chuỗi với 60 token).

7

--- TRANG 8 ---
01020304050607080
6810Chuỗi / giâySố GPU ORCAS³Oracle33.44Hình 5: Thông lượng tối đa của GPT3 chạy trên số lượng GPU khác nhau.
0%20%40%60%80%100%
GPT-JLLAMA 13BGPT NEOXLLAMA 33BGPT3SinhHình phạtChi phíHình 6: Phân tích độ trễ của S3.

Bảng 3: Kích thước batch trung bình và số lần lặp cho các cấu hình hệ thống khác nhau
Số GPUORCA S3Oracle
Kích thước batch Số lần lặp Kích thước batch Số lần lặp Kích thước batch Số lần lặp
6 11.95 46734 114.22 4891 209.47 2667
8 28.68 19482 247.85 2254 470.65 1187
10 48.99 11403 399.62 1398 564.88 989

Hình 4b báo cáo số lượng chuỗi mà mỗi mô hình sinh ra sử dụng ORCA, S3, và Oracle. Oracle vượt quá SLO cho GPT-J, LLAMA 13B, và GPT-NEOX khi nó chọn kích thước batch tối đa. Vì vậy chúng tôi giới hạn kích thước batch cho các mô hình này trong đánh giá này. S3 sinh ra số lượng chuỗi tương tự với trường hợp lý tưởng và 1.13× đến 6.49× chuỗi nhiều hơn so với ORCA. Mức tăng thông lượng tương tự với kịch bản ngoại tuyến vì S3 đáp ứng SLO trong hầu hết các trường hợp với kích thước batch tối đa của nó. Tuy nhiên, sự khác biệt giữa S3 và Oracle giảm 10% so với các kịch bản ngoại tuyến vì chúng tôi giới hạn kích thước batch do đó thông lượng của Oracle.

4.2 Phân tích Chi phí
Chúng tôi đánh giá S3 với số lượng GPU khác nhau. Chúng tôi phân vùng GPT-3 thành 6 và 8 GPU theo cách pipeline-parallel, cấp phát 16 và 12 lớp transformer cho mỗi GPU cho mỗi cấu hình, tương ứng. Chúng tôi cũng đánh giá trên cài đặt 10 GPU nơi chúng tôi cấp phát 10 lớp cho 9 GPU và 6 cho GPU còn lại. S3 pipeline mỗi batch và lập lịch một batch bất cứ khi nào GPU xử lý phân vùng đầu tiên chuyển kết quả của nó đến batch tiếp theo. Điều này giảm thời gian nhàn rỗi GPU bằng cách có mọi GPU xử lý các batch đồng thời.

Hình 5 báo cáo thông lượng tối đa sử dụng số lượng GPU khác nhau. Đầu tiên, chúng ta có thể thấy rằng S3 đạt thông lượng tương tự sử dụng 6 GPU so với ORCA với 10 GPU. ORCA cho thấy thông lượng cao hơn 0.92% so với S3 để cụ thể. Nhiều GPU hơn phân mảnh mô hình thành các mảnh nhỏ hơn và để lại nhiều không gian hơn để lưu trữ bộ nhớ đệm KV, cho phép chúng ta tăng kích thước batch. Bảng 3 hỗ trợ tuyên bố này bằng cách báo cáo kích thước batch lớn hơn với nhiều GPU hơn. S3 có thể đạt hiệu ứng tương tự với ít GPU hơn bằng cách tối ưu hóa chiến lược cấp phát bộ nhớ.

Tự nhiên, nó dẫn đến một câu hỏi tương tự với câu hỏi trong đánh giá thông lượng về tại sao S3 với 6 GPU cho thấy thông lượng tương tự với ORCA với 10 GPU ngay cả với kích thước batch gấp 2.33 lần. Câu trả lời nằm trong việc thực thi pipeline và bản chất tuần tự của các lớp self-attention trong các mô hình dựa trên Transformer như được giải thích trong 4.1. Các hệ thống hoàn thành một batch tại mỗi ⌈l/số GPU⌉ thay vì tại mỗi l lớp. Ví dụ, S3 với 6 GPU hoàn thành một batch tại mỗi 16 lớp thay vì 96 cho GPT3. Tương tự, ORCA với 10 GPU hoàn thành tại mỗi 10 lớp và do đó xử lý nhanh hơn. Việc tăng độ trễ phủ định lợi ích thông lượng từ S3 sao cho hai hệ thống cho thấy thông lượng gần như giống hệt ngay cả với kích thước batch lớn hơn 2.33× khi sử dụng S3.

4.3 Chi phí: Phân tích Độ trễ
Chúng tôi đánh giá độ trễ runtime của mỗi thành phần trong S3. Chúng tôi phân loại độ trễ runtime thành ba loại: sinh, hình phạt, và chi phí. Sinh là thời gian S3 dành để xử lý mô hình để sinh token. Hình phạt biểu thị thời gian cần để S3 preempt bộ nhớ đệm KV và các trạng thái ẩn từ GPU và tải lại chúng lên GPU. Chi phí bao gồm thời gian cần cho predictor, scheduler, và supervisor, kết hợp. Hình 6 cho thấy rằng hình phạt và chi phí kết hợp là không đáng kể (tức là, 11% trung bình) so với sinh. Tất nhiên, hình phạt sẽ tăng nếu predictor kém chính xác hơn và kích hoạt nhiều lưu lượng dữ liệu hơn giữa GPU và host. Ngược lại, chi phí sẽ tăng nếu chúng ta sử dụng predictor chính xác hơn nhưng nặng hơn. Với một predictor tốt, chi phí này sẽ được khấu hao khi mô hình sinh văn bản sinh ra các chuỗi dài hơn.

5 Công trình Liên quan
Hệ thống phục vụ machine learning Sự quan tâm cao đối với machine learning đã thúc đẩy nhiều nghiên cứu trong các nền tảng dịch vụ của nó [1, 4, 6, 16, 28–38]. Đặc biệt, hiệu suất đáng ngạc nhiên của các mô hình ngôn ngữ dựa trên Transformer đã hướng nhiều nhà nghiên cứu phát triển các hệ thống phục vụ chuyên biệt cho Transformer [4, 6, 34, 37, 38]. Hầu hết các hệ thống tập trung vào giảm độ trễ mà không quan tâm nhiều đến thông lượng với ngoại lệ của [6,31,39]. Các hệ thống hướng thông lượng sử dụng hệ thống phân cấp bộ nhớ để lưu trữ các phần của mô hình trong bộ nhớ chậm hơn và tăng kích thước batch. Tuy nhiên, tất cả chúng đều cấp phát cùng lượng bộ nhớ cho mọi chuỗi và không xem xét preempt một chuỗi dựa trên dự đoán. S3 có thể cải thiện các hệ thống này bằng cách giảm kích thước bộ nhớ yêu cầu. Điều này sẽ loại bỏ nhu cầu bộ nhớ lớn hơn nhưng chậm hơn như SSD và cũng giảm chi phí tài chính của bộ nhớ tổng thể.

Giảm chi phí bộ nhớ đệm KV Vấn đề của các lớp attention trong Transformers yêu cầu tính toán và bộ nhớ bậc hai đối với độ dài chuỗi đã được nghiên cứu rộng rãi. Các cách tiếp cận khác nhau đã được đề xuất để giải quyết vấn đề này. Xấp xỉ low-rank [40, 41] và khai thác sparsity [42–44] là một trong những phương pháp nhằm giảm thiểu vấn đề. Một cách tiếp cận khác, được biết đến như multi-query attention [37, 45], đề xuất giảm kích thước cache bằng cách sử dụng một attention head cho mỗi bộ nhớ đệm key-value (KV) thay vì sử dụng nhiều attention head, như được minh họa trong Bảng 1. Ngoài ra, các công trình tập trung vào giảm kích thước mô hình thông qua nén [46] và lượng tử hóa [6, 47, 48] cũng có thể đóng góp vào việc giảm kích thước của bộ nhớ đệm KV. Những cách tiếp cận này bổ sung cho công trình của chúng tôi và có thể được sử dụng để giảm hình phạt gây ra bởi các dự đoán sai, cho phép sử dụng predictor kém chính xác hơn nhưng nhẹ hơn.

Dự đoán độ dài chuỗi Mặc dù có ít công trình dự đoán độ dài chuỗi đầu ra dựa trên một chuỗi đầu vào, Yan et al. [49] đề xuất một mạng nhỏ dựa trên convolution với các lớp embedding để dự báo độ dài chuỗi đầu ra trong dịch máy. Trong cách tiếp cận của chúng tôi, chúng tôi sử dụng chiến lược tương tự nhưng sử dụng predictor dựa trên Transformer nhỏ để ước tính độ dài chuỗi đầu ra trong các tác vụ sinh văn bản. Predictor này cho phép chúng tôi dự đoán chính xác độ dài chuỗi đầu ra cho khối lượng công việc sinh văn bản.

6 Hạn chế và Kết luận
Hạn chế Chúng tôi đưa ra các giả định sau trong công trình này. Chúng tôi giả định rằng các trace yêu cầu sinh văn bản bắt chước các tập dữ liệu hỏi-đáp có sẵn công khai vì không có trace có sẵn công khai. Phân tích trace thực tế sẽ tạo điều kiện triển khai S3 trong khối lượng công việc thương mại. Tương tự, tác vụ sinh văn bản không có ràng buộc SLO độ trễ tiêu chuẩn hóa như trong các khối lượng công việc machine learning khác [50] vì nó là một dịch vụ tương đối mới. Vì vậy chúng tôi giả định tốc độ đọc trung bình của một người đọc tiếng Anh làm SLO của chúng tôi. Chúng tôi sẽ có thể đánh giá S3 trong các kịch bản rộng hơn nếu các tổ chức công bố công khai các SLO khác nhau cho các ứng dụng sinh văn bản khác nhau.

Kết luận Tóm lại, chúng tôi giới thiệu S3, một framework được thiết kế để đạt thông lượng cao trong việc phục vụ các mô hình sinh dựa trên Transformer. S3 tận dụng một predictor để ước tính độ dài đầu ra của các chuỗi được sinh ra và lập lịch chúng tương ứng để tối đa hóa thông lượng. Ngoài ra, S3 xử lý các lỗi dự đoán tiềm tàng để đảm bảo độ tin cậy. Bằng cách cấp phát kích thước bộ nhớ khác nhau cho các đầu vào khác nhau, S3 thừa nhận rằng không phải tất cả các chuỗi đều nên được đối xử như nhau. Cách tiếp cận này mở rộng ranh giới đánh đổi truyền thống giữa độ trễ và thông lượng, mở đường cho những khả năng mới trong việc tối ưu hóa sự đánh đổi độ trễ-thông lượng.

9

--- TRANG 10 ---
Tài liệu tham khảo
[1] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, và I. Stoica, "Clipper: A
Low-Latency online prediction serving system," trong 14th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 17) . Boston, MA: USENIX Association,
Mar. 2017, pp. 613–627. [Online]. Available: https://www.usenix.org/conference/nsdi17/
technical-sessions/presentation/crankshaw
[2] R. Chard, Z. Li, K. Chard, L. Ward, Y . Babuji, A. Woodard, S. Tuecke, B. Blaiszik, M. J.
Franklin, và I. Foster, "Dlhub: Model and data serving for science," trong 2019 IEEE Interna-
tional Parallel and Distributed Processing Symposium (IPDPS) , 2019, pp. 283–292.
[3] H. Face, "Transformers," https://github.com/huggingface/transformers.
[4] NVIDIA, "FasterTransformer," https://github.com/NVIDIA/FasterTransformer.
[5] OpenAI, "OpenAI research GPT4," https://openai.com/research/gpt-4.
[6] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, D. Y . Fu, Z. Xie, B. Chen, C. Barrett, J. E.
Gonzalez, P. Liang, C. R ´e, I. Stoica, và C. Zhang, "High-throughput Generative Inference of
Large Language Models with a Single GPU," arXiv e-prints , p. arXiv:2303.06865, Mar. 2023.
[7] Google, "Bard," https://bard.google.com/.
[8] OpenAI, "ChatGPT," https://chat.openai.com/.
[9] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan,
Y . Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. R ´e,
D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao,
J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha,
N. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli,
T. Hashimoto, T. Icard, T. Zhang, V . Chaudhary, W. Wang, X. Li, Y . Mai, Y . Zhang, và
Y . Koreeda, "Holistic Evaluation of Language Models," arXiv e-prints , p. arXiv:2211.09110,
Nov. 2022.
[10] A. Narayan, I. Chami, L. Orr, S. Arora, và C. R ´e, "Can Foundation Models Wrangle Your
Data?" arXiv e-prints , p. arXiv:2205.09911, May 2022.
[11] B. Lefaudeux, F. Massa, D. Liskovich, W. Xiong, V . Caggiano, S. Naren, M. Xu, J. Hu, M. Tin-
tore, S. Zhang, P. Labatut, và D. Haziza, "xformers: A modular and hackable transformer
modelling library," https://github.com/facebookresearch/xformers, 2022.
[12] M. Brysbaert, "How many words do we read per minute? a review and meta-analysis
of reading rate," Journal of Memory and Language , vol. 109, p. 104047, 2019. [Online].
Available: https://www.sciencedirect.com/science/article/pii/S0749596X19300786
[13] OpenAI, "OpenAI API Documentation," https://help.openai.com/en/articles/
4936856-what-are-tokens-and-how-to-count-them.
[14] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi `ere,
N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, và G. Lample, "Llama:
Open and efficient foundation language models," arXiv preprint arXiv:2302.13971 , 2023.
[15] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy,
K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang,
và S. Weinbach, "GPT-NeoX-20B: An open-source autoregressive language model," trong
Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating
Large Language Models . virtual+Dublin: Association for Computational Linguistics, May
2022, pp. 95–136. [Online]. Available: https://aclanthology.org/2022.bigscience-1.9
[16] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, và B.-G. Chun, "Orca: A distributed serving system
for Transformer-Based generative models," trong 16th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 22) . Carlsbad, CA: USENIX Association, Jul. 2022, pp.
521–538. [Online]. Available: https://www.usenix.org/conference/osdi22/presentation/yu
[17] V . Sanh, L. Debut, J. Chaumond, và T. Wolf, "DistilBERT, a distilled version of BERT:
smaller, faster, cheaper and lighter," arXiv e-prints , p. arXiv:1910.01108, Oct. 2019.
[18] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, và T. B.
Hashimoto, "Stanford alpaca: An instruction-following llama model," https://github.com/
tatsu-lab/stanford alpaca, 2023.
10

--- TRANG 11 ---
[19] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,
I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai,
J. Uszkoreit, Q. Le, và S. Petrov, "Natural questions: a benchmark for question answering
research," 2019.
[20] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
N. Nabeshima, S. Presser, và C. Leahy, "The Pile: An 800GB Dataset of Diverse Text for
Language Modeling," p. arXiv:2101.00027, Dec. 2020.
[21] B. Wang và A. Komatsuzaki, "GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model," https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[22] D. S. Johnson, "Near-optimal bin packing algorithms," Ph.D. dissertation, Massachusetts In-
stitute of Technology, 1973.
[23] PyTorch, "Pytorch Nested Tensor," https://pytorch.org/docs/stable/nested.html.
[24] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. De-
Vito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai,
và S. Chintala, "Pytorch: An imperative style, high-performance deep learning
library," trong Advances in Neural Information Processing Systems 32 . Curran Asso-
ciates, Inc., 2019, pp. 8024–8035. [Online]. Available: http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
[25] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard et al. , "Tensorflow: A system for large-scale machine learning," trong 12th{USENIX }
Symposium on Operating Systems Design and Implementation ( {OSDI}16), 2016, pp. 265–
283.
[26] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan,
R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và
D. Amodei, "Language Models are Few-Shot Learners," arXiv e-prints , p. arXiv:2005.14165,
May 2020.
[27] Lenovo, "Lenovo ThinkSystem SD650-N V2 Server," https://lenovopress.lenovo.com/
lp1396-thinksystem-sd650-n-v2-server.
[28] F. Romero, Q. Li, N. J. Yadwadkar, và C. Kozyrakis, "INFaaS: Automated model-
less inference serving," trong 2021 USENIX Annual Technical Conference (USENIX ATC
21). USENIX Association, Jul. 2021, pp. 397–411. [Online]. Available: https:
//www.usenix.org/conference/atc21/presentation/romero
[29] A. Gujarati, R. Karimi, S. Alzayat, W. Hao, A. Kaufmann, Y . Vigfusson, và
J. Mace, "Serving DNNs like clockwork: Performance predictability from the bottom
up," trong 14th USENIX Symposium on Operating Systems Design and Implementation
(OSDI 20) . USENIX Association, Nov. 2020, pp. 443–462. [Online]. Available:
https://www.usenix.org/conference/osdi20/presentation/gujarati
[30] S. Choi, S. Lee, Y . Kim, J. Park, Y . Kwon, và J. Huh, "Serving heterogeneous machine
learning models on Multi-GPU servers with Spatio-Temporal sharing," trong 2022 USENIX
Annual Technical Conference (USENIX ATC 22) . Carlsbad, CA: USENIX Association,
Jul. 2022, pp. 199–216. [Online]. Available: https://www.usenix.org/conference/atc22/
presentation/choi-seungbeom
[31] R. Yazdani Aminabadi, S. Rajbhandari, M. Zhang, A. A. Awan, C. Li, D. Li, E. Zheng,
J. Rasley, S. Smith, O. Ruwase, và Y . He, "DeepSpeed Inference: Enabling Efficient In-
ference of Transformer Models at Unprecedented Scale," arXiv e-prints , p. arXiv:2207.00032,
Jun. 2022.
[32] U. Gupta, S. Hsia, V . Saraph, X. Wang, B. Reagen, G.-Y . Wei, H.-H. S. Lee, D. Brooks, và C.-
J. Wu, "DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation
Inference," arXiv e-prints , p. arXiv:2001.02772, Jan. 2020.
[33] S. Hsia, U. Gupta, B. Acun, N. Ardalani, P. Zhong, G.-Y . Wei, D. Brooks, và C.-J.
Wu, "Mp-rec: Hardware-software co-design to enable multi-path recommendation," trong
11

--- TRANG 12 ---
Proceedings of the 28th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems, Volume 3 , ser. ASPLOS 2023. New York,
NY , USA: Association for Computing Machinery, 2023, p. 449–465. [Online]. Available:
https://doi.org/10.1145/3582016.3582068
[34] J. Fang, Y . Yu, C. Zhao, và J. Zhou, "TurboTransformers: An Efficient GPU Serving System
For Transformer Models," arXiv e-prints , p. arXiv:2010.05680, Oct. 2020.
[35] C.-F. Wu, C.-J. Wu, G.-Y . Wei, và D. Brooks, "A joint management middleware to improve
training performance of deep recommendation systems with ssds," trong Proceedings of the 59th
ACM/IEEE Design Automation Conference (DAC 22) , 2022, pp. 157–162.
[36] C. Olston, N. Fiedel, K. Gorovoy, J. Harmsen, L. Lao, F. Li, V . Rajashekhar, S. Ramesh, và
J. Soyke, "TensorFlow-Serving: Flexible, High-Performance ML Serving," arXiv e-prints , p.
arXiv:1712.06139, Dec. 2017.
[37] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek,
K. Xiao, S. Agrawal, và J. Dean, "Efficiently scaling transformer inference," arXiv preprint
arXiv:2211.05102 , 2022.
[38] X. Wang, Y . Xiong, Y . Wei, M. Wang, và L. Li, "LightSeq: A High Performance Inference
Library for Transformers," arXiv e-prints , p. arXiv:2010.13887, Oct. 2020.
[39] Huggingface, "Hugginface Accelerate," https://huggingface.co/docs/accelerate/index.
[40] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins,
J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. Colwell, và A. Weller, "Rethinking At-
tention with Performers," arXiv e-prints , p. arXiv:2009.14794, Sep. 2020.
[41] V . Likhosherstov, K. Choromanski, J. Davis, X. Song, và A. Weller, "Sub-Linear Memory:
How to Make Performers SLiM," arXiv e-prints , p. arXiv:2012.11346, Dec. 2020.
[42] I. Beltagy, M. E. Peters, và A. Cohan, "Longformer: The Long-Document Transformer,"
arXiv e-prints , p. arXiv:2004.05150, Apr. 2020.
[43] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula,
Q. Wang, L. Yang, và A. Ahmed, "Big Bird: Transformers for Longer Sequences," arXiv
e-prints , p. arXiv:2007.14062, Jul. 2020.
[44] H. Wang, Z. Zhang, và S. Han, "SpAtten: Efficient Sparse Attention Architecture with Cas-
cade Token and Head Pruning," arXiv e-prints , p. arXiv:2012.09852, Dec. 2020.
[45] N. Shazeer, "Fast Transformer Decoding: One Write-Head is All You Need," arXiv e-prints ,
p. arXiv:1911.02150, Nov. 2019.
[46] J. W. Rae, A. Potapenko, S. M. Jayakumar, và T. P. Lillicrap, "Compressive Transformers for
Long-Range Sequence Modelling," arXiv e-prints , p. arXiv:1911.05507, Nov. 2019.
[47] S. Kim, A. Gholami, Z. Yao, M. W. Mahoney, và K. Keutzer, "I-BERT: Integer-only BERT
Quantization," arXiv e-prints , p. arXiv:2101.01321, Jan. 2021.
[48] Z. Liu, Y . Wang, K. Han, W. Zhang, S. Ma, và W. Gao, "Post-training quantization for
vision transformer," trong Advances in Neural Information Processing Systems , M. Ranzato,
A. Beygelzimer, Y . Dauphin, P. Liang, và J. W. Vaughan, Eds., vol. 34. Curran Associates,
Inc., 2021, pp. 28 092–28 103. [Online]. Available: https://proceedings.neurips.cc/paper files/
paper/2021/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf
[49] Z. Yang, Y . Gao, W. Wang, và H. Ney, "Predicting and using target length in neural
machine translation," trong Proceedings of the 1st Conference of the Asia-Pacific Chapter of the
Association for Computational Linguistics and the 10th International Joint Conference on
Natural Language Processing . Suzhou, China: Association for Computational Linguistics,
Dec. 2020, pp. 389–395. [Online]. Available: https://aclanthology.org/2020.aacl-main.41
[50] V . Janapa Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J. Wu, B. Anderson,
M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos,
J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. St. John, P. Kan-
war, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhi-
menko, A. Tejusve Raghunath Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson,
F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, và Y . Zhou, "MLPerf
Inference Benchmark," arXiv e-prints , p. arXiv:1911.02549, Nov. 2019.
12
