I'll translate this research paper from English to Vietnamese, maintaining the exact same structure including all sections, tables, and figures.

# 1905.10650.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1905.10650.pdf
# Kích thước tệp: 576411 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Liệu Mười Sáu Đầu Có Thực Sự Tốt Hơn Một Đầu?
Paul Michel
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
pmichel1@cs.cmu.edu

Omer Levy
Nghiên cứu Trí tuệ Nhân tạo Facebook
Seattle, WA
omerlevy@fb.com

Graham Neubig
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
gneubig@cs.cmu.edu

Tóm tắt
Attention là một cơ chế mạnh mẽ và phổ biến cho phép các mô hình thần kinh tập trung vào các phần thông tin quan trọng cụ thể bằng cách lấy trung bình có trọng số của chúng khi đưa ra dự đoán. Cụ thể, multi-headed attention là động lực thúc đẩy nhiều mô hình xử lý ngôn ngữ tự nhiên (NLP) hiện đại như các mô hình dịch máy dựa trên Transformer và BERT. Các mô hình này áp dụng nhiều cơ chế attention song song, với mỗi "đầu" attention có thể tập trung vào các phần khác nhau của đầu vào, điều này giúp có thể biểu diễn các hàm phức tạp vượt ra ngoài trung bình có trọng số đơn giản. Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng ngay cả khi các mô hình đã được huấn luyện sử dụng nhiều đầu, trong thực tế, một tỷ lệ lớn các đầu attention có thể bị loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất. Thực tế, một số lớp thậm chí có thể được giảm xuống chỉ còn một đầu. Chúng tôi tiếp tục kiểm tra các thuật toán tham lam để cắt tỉa mô hình, và các cải thiện tiềm năng về tốc độ, hiệu quả bộ nhớ và độ chính xác có thể đạt được từ đó. Cuối cùng, chúng tôi phân tích kết quả liên quan đến việc những phần nào của mô hình phụ thuộc nhiều hơn vào việc có nhiều đầu, và cung cấp bằng chứng sơ bộ rằng động lực học huấn luyện đóng vai trò trong lợi ích do multi-head attention mang lại¹.

1 Giới thiệu
Transformer (Vaswani et al., 2017) đã cho thấy hiệu suất tối ưu trên nhiều tác vụ NLP khác nhau, bao gồm nhưng không giới hạn ở dịch máy (Vaswani et al., 2017; Ott et al., 2018), trả lời câu hỏi (Devlin et al., 2018), phân loại văn bản (Radford et al., 2018), và gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018). Trung tâm của các cải tiến kiến trúc, Transformer mở rộng cơ chế attention tiêu chuẩn (Bahdanau et al., 2015; Cho et al., 2014) thông qua multi-headed attention (MHA), nơi attention được tính toán độc lập bởi Nₕ cơ chế attention song song (đầu). Đã được chứng minh rằng ngoài việc cải thiện hiệu suất, MHA có thể giúp với sự thống nhất chủ-vị (Tang et al., 2018) và một số đầu có thể dự đoán được cấu trúc phụ thuộc (Raganato và Tiedemann, 2018). Kể từ đó, một số phần mở rộng của phương pháp tổng quát đã được đề xuất (Ahmed et al., 2017; Shen et al., 2018).

¹Code để nhân rộng các thí nghiệm của chúng tôi được cung cấp tại https://github.com/pmichel31415/are-16-heads-really-better-than-1

Hội nghị thứ 33 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2019), Vancouver, Canada.
arXiv:1905.10650v3 [cs.CL] 4 Nov 2019

--- TRANG 2 ---
Tuy nhiên, vẫn chưa hoàn toàn rõ ràng: nhiều đầu trong các mô hình này mang lại lợi ích gì cho chúng ta? Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng - trong cả các mô hình dựa trên Transformer cho dịch máy và BERT (Devlin et al., 2018) cho suy luận ngôn ngữ tự nhiên - hầu hết các đầu attention có thể được loại bỏ riêng lẻ sau khi huấn luyện mà không có bất kỳ tác động tiêu cực đáng kể nào về hiệu suất kiểm tra (§3.2). Đáng chú ý, nhiều lớp attention thậm chí có thể được giảm xuống riêng lẻ thành một đầu attention duy nhất mà không ảnh hưởng đến hiệu suất kiểm tra (§3.3).

Dựa trên quan sát này, chúng tôi tiếp tục đề xuất một thuật toán đơn giản cắt tỉa một cách tham lam và lặp đi lặp lại các đầu attention dường như đang đóng góp ít hơn cho mô hình. Bằng cách loại bỏ đồng thời các đầu attention từ toàn bộ mạng, mà không hạn chế việc cắt tỉa ở một lớp duy nhất, chúng tôi thấy rằng các phần lớn của mạng có thể được loại bỏ với ít hoặc không có hậu quả, nhưng phần lớn các đầu phải được giữ lại để tránh sụt giảm hiệu suất thảm khốc (§4). Chúng tôi tiếp tục thấy rằng điều này có lợi ích đáng kể cho hiệu quả thời gian suy luận, dẫn đến tăng tốc độ suy luận lên đến 17.5% cho mô hình dựa trên BERT.

Sau đó chúng tôi đi sâu vào phân tích thêm. Một cái nhìn gần hơn về trường hợp dịch máy cho thấy các lớp attention encoder-decoder đặc biệt nhạy cảm với việc cắt tỉa, nhiều hơn so với các lớp self-attention, cho thấy rằng tính đa đầu đóng vai trò quan trọng trong thành phần này (§5). Cuối cùng, chúng tôi cung cấp bằng chứng rằng sự phân biệt giữa các đầu quan trọng và không quan trọng tăng lên khi huấn luyện tiến triển, cho thấy sự tương tác giữa tính đa đầu và động lực học huấn luyện (§6).

2 Nền tảng: Attention, Multi-headed Attention, và Masking
Trong phần này, chúng tôi đặt nền tảng ký hiệu về attention, và cũng mô tả phương pháp của chúng tôi để che đi các đầu attention.

2.1 Single-headed Attention
Chúng tôi nhắc lại ngắn gọn cách attention vani hoạt động. Chúng tôi tập trung vào scaled bilinear attention (Luong et al., 2015), biến thể được sử dụng phổ biến nhất trong các lớp MHA. Cho một chuỗi n vector d chiều x = x₁, ..., xₙ ∈ ℝᵈ, và một vector truy vấn q ∈ ℝᵈ, lớp attention được tham số hóa bởi Wₖ, Wq, Wᵥ, Wₒ ∈ ℝᵈˣᵈ tính tổng có trọng số:

Att_{Wₖ,Wq,Wᵥ,Wₒ}(x,q) = Wₒ ∑ᵢ₌₁ⁿ αᵢWᵥxᵢ

trong đó αᵢ = softmax(q^T W_q^T W_k x_i / √d)

Trong self-attention, mỗi xᵢ được sử dụng làm truy vấn q để tính toán một chuỗi biểu diễn mới, trong khi đó trong các mô hình sequence-to-sequence, q thường là một trạng thái decoder trong khi x tương ứng với đầu ra encoder.

2.2 Multi-headed Attention
Trong multi-headed attention (MHA), Nₕ lớp attention được tham số hóa độc lập được áp dụng song song để có được kết quả cuối cùng:

MHAtt(x,q) = ∑ₕ₌₁^{Nₕ} Att_{Wₖʰ,Wqʰ,Wᵥʰ,Wₒʰ}(x,q)    (1)

trong đó Wₖʰ, Wqʰ, Wᵥʰ ∈ ℝᵈʰˣᵈ và Wₒʰ ∈ ℝᵈˣᵈʰ. Khi dₕ = d, MHA có tính biểu diễn nghiêm ngặt hơn attention vani. Tuy nhiên, để giữ số lượng tham số không đổi, dₕ thường được đặt thành d/Nₕ, trong trường hợp này MHA có thể được xem như một ensemble của các lớp attention vani low-rank². Trong phần tiếp theo, chúng tôi sử dụng Attʰ(x) như một ký hiệu viết tắt cho đầu ra của đầu h trên đầu vào x.

Để cho phép các đầu attention khác nhau tương tác với nhau, transformer áp dụng một mạng feed-forward phi tuyến trên đầu ra của MHA, tại mỗi lớp transformer (Vaswani et al., 2017).

²Ký hiệu này, tương đương với công thức "nối" từ Vaswani et al. (2017), được sử dụng để dễ dàng trình bày trong các phần tiếp theo.

--- TRANG 3 ---
[Biểu đồ 1: Phân bố đầu theo điểm số mô hình sau khi masking]

2.3 Masking Attention Heads
Để thực hiện các thí nghiệm ablation trên các đầu, chúng tôi sửa đổi công thức cho MHAtt:

MHAtt(x,q) = ∑ₕ₌₁^{Nₕ} ζₕ Att_{Wₖʰ,Wqʰ,Wᵥʰ,Wₒʰ}(x,q)

trong đó ζₕ là các biến mask với giá trị trong {0,1}. Khi tất cả ζₕ bằng 1, điều này tương đương với công thức trong Phương trình 1. Để mask đầu h, chúng tôi đơn giản đặt ζₕ = 0.

3 Tất Cả Các Đầu Attention Có Quan Trọng Không?
Chúng tôi thực hiện một loạt thí nghiệm trong đó chúng tôi loại bỏ một hoặc nhiều đầu attention từ một kiến trúc nhất định tại thời điểm kiểm tra, và đo lường sự khác biệt hiệu suất. Chúng tôi đầu tiên loại bỏ một đầu attention tại một thời điểm (§3.2) và sau đó loại bỏ mọi đầu trong toàn bộ lớp trừ một đầu (§3.3).

3.1 Thiết lập Thí nghiệm
Trong tất cả các thí nghiệm sau, chúng tôi xem xét hai mô hình đã được huấn luyện:

WMT Đây là kiến trúc transformer "large" gốc từ Vaswani et al. 2017 với 6 lớp và 16 đầu mỗi lớp, được huấn luyện trên corpus WMT2014 English to French. Chúng tôi sử dụng mô hình được huấn luyện trước của Ott et al. 2018³ và báo cáo điểm BLEU trên tập kiểm tra newstest2013. Theo Ott et al. 2018, chúng tôi tính điểm BLEU trên đầu ra đã được tokenize của mô hình sử dụng Moses (Koehn et al., 2007). Ý nghĩa thống kê được kiểm tra với paired bootstrap resampling (Koehn, 2004) sử dụng compare-mt⁴ (Neubig et al., 2019) với 1000 mẫu lại. Một đặc điểm của mô hình này là nó có 3 cơ chế attention riêng biệt: encoder self-attention (Enc-Enc), encoder-decoder attention (Enc-Dec) và decoder self-attention (Dec-Dec), tất cả đều sử dụng MHA.

BERT BERT (Devlin et al., 2018) là một transformer duy nhất được huấn luyện trước trên tác vụ "masked language modeling" không giám sát kiểu cloze và sau đó được fine-tune trên các tác vụ cụ thể. Tại thời điểm ra đời, nó đạt được hiệu suất tối ưu trên nhiều tác vụ NLP. Chúng tôi sử dụng mô hình base-uncased được huấn luyện trước của Devlin et al. 2018 với 12 lớp và 12 đầu attention mà chúng tôi fine-tune và đánh giá trên MultiNLI (Williams et al., 2018). Chúng tôi báo cáo độ chính xác trên tập validation "matched". Chúng tôi kiểm tra ý nghĩa thống kê sử dụng t-test. Trái ngược với mô hình WMT, BERT chỉ có một cơ chế attention (self-attention trong mỗi lớp).

3.2 Ablating Một Đầu
Để hiểu đóng góp của một đầu attention cụ thể h, chúng tôi đánh giá hiệu suất của mô hình khi mask đầu đó (tức là thay thế Attʰ(x) bằng số không). Nếu hiệu suất không có h tệ hơn đáng kể so với mô hình đầy đủ, h rõ ràng là quan trọng; nếu hiệu suất tương đương, h là dư thừa với phần còn lại của mô hình.

Hình 1a và 1b cho thấy phân bố các đầu về điểm số của mô hình sau khi mask nó, đối với WMT và BERT tương ứng. Chúng tôi quan sát thấy rằng phần lớn các đầu attention có thể được loại bỏ mà không lệch quá nhiều khỏi điểm số gốc. Đáng ngạc nhiên, trong một số trường hợp việc loại bỏ một đầu attention dẫn đến tăng BLEU/accuracy.

Để có cái nhìn chi tiết hơn về những kết quả này, chúng tôi phóng to các lớp encoder self-attention của mô hình WMT trong Bảng 1. Đáng chú ý, chúng tôi thấy rằng chỉ 8 (trong 96) đầu gây ra thay đổi có ý nghĩa thống kê trong hiệu suất khi chúng được loại bỏ khỏi mô hình, một nửa trong số đó thực sự dẫn đến điểm BLEU cao hơn. Điều này dẫn chúng tôi đến quan sát đầu tiên: tại thời điểm kiểm tra, hầu hết các đầu là dư thừa với phần còn lại của mô hình.

3.3 Ablating Tất Cả Các Đầu Trừ Một
Quan sát này đưa ra câu hỏi: liệu có cần nhiều hơn một đầu không? Do đó, chúng tôi tính toán sự khác biệt hiệu suất khi tất cả các đầu trừ một được loại bỏ, trong một lớp duy nhất. Trong Bảng 2 và Bảng 3, chúng tôi báo cáo điểm số tốt nhất cho mỗi lớp trong mô hình, tức là điểm số khi giảm toàn bộ lớp xuống còn đầu quan trọng nhất.

[Bảng 1: Sự khác biệt điểm BLEU cho mỗi đầu của cơ chế encoder self attention]

[Bảng 2: Delta BLEU tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình WMT]

[Bảng 3: Delta accuracy tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình BERT]

³https://github.com/pytorch/fairseq/tree/master/examples/translation
⁴https://github.com/neulab/compare-mt

--- TRANG 4 ---
Chúng tôi thấy rằng, đối với hầu hết các lớp, một đầu thực sự đủ tại thời điểm kiểm tra, mặc dù mạng được huấn luyện với 12 hoặc 16 đầu attention. Điều này đáng chú ý bởi vì các lớp này có thể được giảm xuống single-headed attention chỉ với 1/16 (tương ứng 1/12) số lượng tham số của một lớp attention vani. Tuy nhiên, một số lớp đòi hỏi nhiều đầu attention; ví dụ, thay thế lớp cuối cùng trong encoder-decoder attention của WMT bằng một đầu duy nhất làm giảm hiệu suất ít nhất 13.5 điểm BLEU. Chúng tôi phân tích thêm khi nào các thành phần mô hình khác nhau phụ thuộc vào nhiều đầu hơn trong §5.

Ngoài ra, chúng tôi xác minh rằng kết quả này vẫn đúng ngay cả khi chúng tôi không có quyền truy cập vào tập đánh giá khi chọn đầu "tốt nhất một mình". Với mục đích này, chúng tôi chọn đầu tốt nhất cho mỗi lớp trên tập validation (newstest2013 cho WMT và một tập con 5,000 mẫu được chọn ngẫu nhiên từ tập huấn luyện MNLI cho BERT) và đánh giá hiệu suất của mô hình trên tập kiểm tra (newstest2014 cho WMT và tập validation MNLI-matched cho BERT). Chúng tôi quan sát thấy rằng các phát hiện tương tự vẫn đúng: chỉ giữ một đầu không dẫn đến thay đổi có ý nghĩa thống kê trong hiệu suất cho 50% (tương ứng 100%) các lớp của WMT (tương ứng BERT). Kết quả chi tiết có thể được tìm thấy trong Phụ lục A.

3.4 Các Đầu Quan Trọng Có Giống Nhau Trên Các Tập Dữ Liệu Không?
Có một điểm cần lưu ý trong hai thí nghiệm trước của chúng tôi: những kết quả này chỉ có giá trị trên các tập kiểm tra cụ thể (và khá nhỏ), đặt ra nghi ngờ về khả năng tổng quát hóa của chúng lên các tập dữ liệu khác. Như bước đầu tiên để hiểu liệu một số đầu có quan trọng phổ quát không, chúng tôi thực hiện cùng một nghiên cứu ablation trên một tập kiểm tra thứ hai, ngoài miền. Cụ thể, chúng tôi xem xét tập validation MNLI "mismatched" cho BERT và tập kiểm tra MTNT English to French (Michel và Neubig, 2018) cho mô hình WMT, cả hai đều được tập hợp với mục đích cung cấp các bộ kiểm tra đối lập, ngoài miền cho các tác vụ tương ứng.

Chúng tôi thực hiện cùng một nghiên cứu ablation như §3.2 trên mỗi tập dữ liệu này và báo cáo kết quả trong Hình 2a và 2b. Chúng tôi nhận thấy rằng có một tương quan dương, > 0.5 (p < 0.01) giữa hiệu ứng của việc loại bỏ một đầu trên cả hai tập dữ liệu. Hơn nữa, các đầu có hiệu ứng cao nhất đối với hiệu suất trên một miền có xu hướng có cùng hiệu ứng trên miền khác, điều này cho thấy rằng các đầu quan trọng nhất từ §3.2 thực sự "phổ quát" quan trọng.

4 Cắt Tỉa Lặp Đi Lặp Lại Các Đầu Attention
Trong các thí nghiệm ablation của chúng tôi (§3.2 và §3.3), chúng tôi quan sát hiệu ứng của việc loại bỏ một hoặc nhiều đầu trong một lớp duy nhất, mà không xem xét điều gì sẽ xảy ra nếu chúng tôi thay đổi hai hoặc nhiều lớp khác nhau cùng một lúc. Để kiểm tra hiệu ứng tổng hợp của việc cắt tỉa nhiều đầu từ khắp toàn bộ mô hình, chúng tôi sắp xếp tất cả các đầu attention trong mô hình theo điểm số tầm quan trọng proxy (được mô tả dưới đây), và sau đó loại bỏ các đầu từng cái một. Chúng tôi sử dụng phương pháp lặp, heuristic này để tránh tìm kiếm tổ hợp, điều này không thực tế do số lượng đầu và thời gian cần thiết để đánh giá mỗi mô hình.

--- TRANG 5 ---
[Hình 2: Phân tích cross-task về hiệu ứng của việc cắt tỉa đối với độ chính xác]

[Hình 3: Sự tiến triển của độ chính xác theo số lượng đầu được cắt tỉa]

4.1 Điểm Số Tầm Quan Trọng Đầu để Cắt Tỉa
Như một điểm số proxy cho tầm quan trọng đầu, chúng tôi xem xét độ nhạy cảm kỳ vọng của mô hình đối với các biến mask ζₕ được định nghĩa trong §2.3:

Iₕ = |E_{x~X}[∂L(x)/∂ζₕ]|    (2)

trong đó X là phân bố dữ liệu và L(x) là loss trên mẫu x. Trực quan, nếu Iₕ có giá trị cao thì việc thay đổi ζₕ có thể có hiệu ứng lớn đối với mô hình. Cụ thể, chúng tôi thấy giá trị tuyệt đối là quan trọng để tránh các điểm dữ liệu với đóng góp rất âm hoặc dương triệt tiêu lẫn nhau trong tổng. Thay Phương trình 1 vào Phương trình 2 và áp dụng quy tắc chuỗi cho biểu thức cuối cùng sau cho Iₕ:

Iₕ = |E_{x~X}[Att^h(x)^T ∂L(x)/∂Att^h(x)]|

Công thức này gợi nhớ đến kho tàng văn học phong phú về cắt tỉa mạng thần kinh (LeCun et al., 1990; Hassibi và Stork, 1993; Molchanov et al., 2017, v.v.). Cụ thể, nó tương đương với phương pháp khai triển Taylor từ Molchanov et al. (2017).

Về hiệu suất, việc ước tính Iₕ chỉ yêu cầu thực hiện forward và backward pass, và do đó không chậm hơn huấn luyện. Trong thực tế, chúng tôi tính kỳ vọng trên dữ liệu huấn luyện hoặc một tập con của nó.⁵ Theo khuyến nghị của Molchanov et al. (2017), chúng tôi chuẩn hóa điểm số tầm quan trọng theo lớp (sử dụng chuẩn ℓ₂).

4.2 Hiệu Ứng của Cắt Tỉa đối với BLEU/Accuracy
Hình 3a (cho WMT) và 3b (cho BERT) mô tả hiệu ứng của việc cắt tỉa đầu attention đối với hiệu suất mô hình trong khi loại bỏ dần dần 10% tổng số đầu theo thứ tự tăng dần của Iₕ tại mỗi bước. Chúng tôi cũng báo cáo kết quả khi thứ tự cắt tỉa được xác định bởi sự khác biệt điểm số từ §3.2 (trong các đường nét đứt), nhưng thấy rằng việc sử dụng Iₕ nhanh hơn và cho kết quả tốt hơn.

Chúng tôi quan sát thấy rằng phương pháp này cho phép chúng tôi cắt tỉa lên đến 20% và 40% đầu từ WMT và BERT (tương ứng), mà không gây ra bất kỳ tác động tiêu cực đáng chú ý nào. Hiệu suất giảm mạnh khi cắt tỉa thêm, có nghĩa là không mô hình nào có thể được giảm xuống mô hình attention hoàn toàn single-head mà không cần huấn luyện lại hoặc chịu tổn thất đáng kể về hiệu suất. Chúng tôi tham khảo Phụ lục B cho các thí nghiệm trên bốn tập dữ liệu bổ sung.

4.3 Hiệu Ứng của Cắt Tỉa đối với Hiệu Quả
Ngoài hiệu suất tác vụ downstream, còn có những lợi thế nội tại khi cắt tỉa đầu. Đầu tiên, mỗi đầu đại diện cho một tỷ lệ không đáng kể của tổng tham số trong mỗi lớp attention (6.25% cho WMT, 8.34% cho BERT), và do đó của tổng mô hình (nói chung, trong cả hai mô hình của chúng tôi, khoảng một phần ba tổng số tham số được dành cho MHA trên tất cả các lớp).⁶ Điều này hấp dẫn trong bối cảnh triển khai mô hình trong các cài đặt hạn chế bộ nhớ.

[Bảng 4: Tốc độ suy luận trung bình của BERT trên tập validation MNLI-matched]

Hơn nữa, chúng tôi thấy rằng việc thực sự cắt tỉa các đầu (và không chỉ masking) dẫn đến tăng tốc độ suy luận đáng kể. Bảng 4 báo cáo số lượng ví dụ mỗi giây được xử lý bởi BERT, trước và sau khi cắt tỉa 50% tất cả các đầu attention. Các thí nghiệm được thực hiện trên hai máy khác nhau, cả hai đều được trang bị GPU GeForce GTX 1080Ti. Mỗi thí nghiệm được lặp lại 3 lần trên mỗi máy (tổng cộng 6 điểm dữ liệu cho mỗi cài đặt). Chúng tôi thấy rằng việc cắt tỉa một nửa đầu của mô hình tăng tốc độ suy luận lên đến 17.5% cho kích thước batch cao hơn (sự khác biệt này biến mất đối với kích thước batch nhỏ hơn).

5 Khi Nào Nhiều Đầu Quan Trọng? Trường Hợp Dịch Máy
Như được thể hiện trong Bảng 2, không phải tất cả các lớp MHA đều có thể được giảm xuống một đầu attention duy nhất mà không ảnh hưởng đáng kể đến hiệu suất. Để có ý tưởng tốt hơn về mức độ mỗi phần của mô hình dịch dựa trên transformer phụ thuộc vào tính đa đầu, chúng tôi lặp lại thí nghiệm cắt tỉa heuristic từ §4 cho mỗi loại attention riêng biệt (Enc-Enc, Enc-Dec, và Dec-Dec).

Hình 4 cho thấy rằng hiệu suất giảm nhanh hơn nhiều khi các đầu được cắt tỉa từ các lớp attention Enc-Dec. Cụ thể, việc cắt tỉa hơn 60% đầu attention Enc-Dec sẽ dẫn đến suy giảm hiệu suất thảm khốc, trong khi các lớp self-attention encoder và decoder vẫn có thể tạo ra bản dịch hợp lý (với điểm BLEU khoảng 30) chỉ với 20% đầu attention gốc. Nói cách khác, encoder-decoder attention phụ thuộc vào tính đa đầu nhiều hơn so với self-attention.

[Hình 4: BLEU khi cắt tỉa dần các đầu từ mỗi loại attention trong mô hình WMT]

⁵Đối với mô hình WMT, chúng tôi sử dụng tất cả các tập newstest20[09-12] để ước tính I.
⁶Hơi nhiều hơn trong WMT vì attention Enc-Dec.

--- TRANG 6 ---
6 Động Lực Học Tầm Quan Trọng Đầu Trong Quá Trình Huấn Luyện
Các phần trước cho chúng tôi biết rằng một số đầu quan trọng hơn những đầu khác trong các mô hình đã được huấn luyện. Để có thêm hiểu biết về động lực học tầm quan trọng đầu trong quá trình huấn luyện, chúng tôi thực hiện cùng một thí nghiệm cắt tỉa dần của §4.2 tại mỗi epoch. Chúng tôi thực hiện thí nghiệm này trên một phiên bản nhỏ hơn của mô hình WMT (6 lớp và 8 đầu mỗi lớp), được huấn luyện cho dịch Đức sang Tiếng Anh trên tập dữ liệu IWSLT 2014 nhỏ hơn Cettolo et al. (2015). Chúng tôi gọi mô hình này là IWSLT.

[Hình 5: Mối quan hệ giữa phần trăm đầu được cắt tỉa và sự giảm điểm số tương đối trong quá trình huấn luyện]

Hình 5 báo cáo, cho mỗi mức cắt tỉa (theo bước 10% — 0% tương ứng với mô hình gốc), sự tiến triển của điểm số mô hình (trên newstest2013) cho mỗi epoch. Để dễ đọc hơn, chúng tôi hiển thị các epoch trên thang logarit, và chỉ báo cáo điểm số cứ 5 epoch sau epoch thứ 10). Để làm cho điểm số có thể so sánh được qua các epoch, trục Y báo cáo sự suy giảm tương đối của điểm BLEU so với mô hình không cắt tỉa tại mỗi epoch. Đáng chú ý, chúng tôi thấy rằng có hai chế độ khác biệt: trong các epoch rất sớm (đặc biệt là 1 và 2), hiệu suất giảm tuyến tính với phần trăm cắt tỉa, tức là sự giảm tương đối trong hiệu suất độc lập với Iₕ, cho thấy rằng hầu hết các đầu ít nhiều đều quan trọng như nhau. Từ epoch 10 trở đi, có sự tập trung của các đầu không quan trọng có thể được cắt tỉa trong khi vẫn duy trì trong 85-90% điểm BLEU gốc (lên đến 40% tổng số đầu).

Điều này cho thấy rằng các đầu quan trọng được xác định sớm (nhưng không ngay lập tức) trong quá trình huấn luyện. Hai giai đoạn huấn luyện gợi nhớ đến phân tích của Shwartz-Ziv và Tishby (2017), theo đó việc huấn luyện mạng thần kinh phân tách thành giai đoạn "tối thiểu hóa rủi ro thực nghiệm", nơi mô hình tối đa hóa thông tin tương hỗ của các biểu diễn trung gian với nhãn, và giai đoạn "nén" nơi thông tin tương hỗ với đầu vào được tối thiểu hóa. Một cuộc điều tra có nguyên tắc hơn về hiện tượng này được để lại cho công việc tương lai.

7 Công Trình Liên Quan
Việc sử dụng cơ chế attention trong NLP và đặc biệt là dịch máy thần kinh (NMT) có thể được truy tìm về Bahdanau et al. (2015) và Cho et al. (2014), và hầu hết các triển khai đương thời đều dựa trên công thức từ Luong et al. (2015). Attention sớm được điều chỉnh (thành công) cho các tác vụ NLP khác, thường đạt được hiệu suất tối ưu thời bấy giờ trong đọc hiểu (Cheng et al., 2016), suy luận ngôn ngữ tự nhiên (Parikh et al., 2016) hoặc tóm tắt trừu tượng (Paulus et al., 2017) để trích dẫn một vài. Multi-headed attention được giới thiệu lần đầu bởi Vaswani et al. (2017) cho NMT và phân tích cú pháp thành phần Tiếng Anh, và sau đó được áp dụng cho transfer learning (Radford et al., 2018; Devlin et al., 2018), mô hình hóa ngôn ngữ (Dai et al., 2019; Radford et al., 2019), hoặc gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018), trong số những cái khác.

Có một văn học phong phú về cắt tỉa mạng thần kinh đã được huấn luyện, truy về LeCun et al. (1990) và Hassibi và Stork (1993) trong đầu những năm 90 và được hồi sinh sau sự ra đời của deep learning, với hai phương pháp trực giao: cắt tỉa "từng trọng số" tinh vi (Han et al., 2015) và cắt tỉa có cấu trúc (Anwar et al., 2017; Li et al., 2016; Molchanov et al., 2017), trong đó toàn bộ các phần của mô hình được cắt tỉa. Trong NLP, cắt tỉa có cấu trúc cho auto-sizing các mô hình ngôn ngữ feed-forward được điều tra lần đầu bởi Murray và Chiang (2015). Gần đây hơn, các phương pháp cắt tỉa tinh vi đã được phổ biến bởi See et al. (2016) và Kim và Rush (2016) (chủ yếu trên NMT).

Đồng thời với công trình của chúng tôi, Voita et al. (2019) đã có một quan sát tương tự về multi-head attention. Phương pháp của họ liên quan đến việc sử dụng LRP (Binder et al., 2016) để xác định các đầu quan trọng và xem xét các tính chất cụ thể như chú ý đến các vị trí lân cận, từ hiếm hoặc từ liên quan cú pháp. Họ đề xuất một cơ chế cắt tỉa thay thế dựa trên gradient descent trên các biến mask ζₕ. Trong khi phương pháp và kết quả của họ bổ sung cho bài báo này, nghiên cứu của chúng tôi cung cấp bằng chứng bổ sung về hiện tượng này ngoài NMT, cũng như phân tích động lực học huấn luyện của việc cắt tỉa các đầu attention.

8 Kết Luận
Chúng tôi đã quan sát thấy rằng MHA không phải lúc nào cũng tận dụng tính biểu diễn vượt trội về mặt lý thuyết so với attention vani một cách đầy đủ nhất. Cụ thể, chúng tôi đã chứng minh rằng trong nhiều cài đặt khác nhau, một số đầu có thể được loại bỏ khỏi các mô hình transformer đã được huấn luyện mà không có sự suy giảm có ý nghĩa thống kê trong hiệu suất kiểm tra, và một số lớp có thể được giảm xuống chỉ một đầu. Ngoài ra, chúng tôi đã chỉ ra rằng trong các mô hình dịch máy, các lớp attention encoder-decoder phụ thuộc vào tính đa đầu nhiều hơn so với các lớp self-attention, và cung cấp bằng chứng rằng tầm quan trọng tương đối của mỗi đầu được xác định trong giai đoạn đầu của huấn luyện. Chúng tôi hy vọng rằng những quan sát này sẽ thúc đẩy sự hiểu biết của chúng tôi về MHA và truyền cảm hứng cho các mô hình đầu tư tham số và attention hiệu quả hơn.

Lời Cảm Ơn
Các tác giả muốn gửi lời cảm ơn đến các reviewer ẩn danh vì phản hồi sâu sắc của họ. Chúng tôi cũng đặc biệt biết ơn Thomas Wolf từ Hugging Face, người mà nỗ lực tái tạo độc lập đã cho phép chúng tôi tìm ra và sửa lỗi trong các thí nghiệm so sánh tốc độ. Nghiên cứu này được hỗ trợ một phần bởi quà tặng từ Facebook.

Tài Liệu Tham Khảo
[Danh sách tài liệu tham khảo tiếp tục với định dạng giống như bản gốc...]

--- TRANG 7 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 8 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 11 ---
A Ablating Tất Cả Các Đầu Trừ Một: Thí Nghiệm Bổ Sung
Bảng 5 và 6 báo cáo sự khác biệt hiệu suất khi chỉ giữ một đầu cho bất kỳ lớp nào. Đầu được chọn là đầu tốt nhất một mình trên một tập dữ liệu riêng biệt.

[Bảng 5 và 6: Kết quả thí nghiệm bổ sung]

B Thí Nghiệm Cắt Tỉa Bổ Sung
Chúng tôi báo cáo kết quả bổ sung cho phương pháp cắt tỉa theo tầm quan trọng từ Phần 4 trên 4 tập dữ liệu bổ sung:

SST-2: Phiên bản GLUE của Stanford Sentiment Treebank (Socher et al., 2013). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

CoLA: Phiên bản GLUE của Corpus of Linguistic Acceptability (Warstadt et al., 2018). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

MRPC: Phiên bản GLUE của Microsoft Research Paraphrase Corpus (Dolan và Brockett, 2005). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

IWSLT: Tập dữ liệu dịch Đức sang Tiếng Anh từ IWSLT 2014 (Cettolo et al., 2015). Chúng tôi sử dụng mô hình nhỏ hơn giống như được mô tả trong Phần 6.

Hình 6 cho thấy rằng trong một số trường hợp lên đến 60% (SST-2) hoặc 50% (CoLA, MRPC) đầu có thể được cắt tỉa mà không có tác động đáng chú ý đến hiệu suất.

--- TRANG 12 ---
--- TRANG 13 ---
[Hình 6: Sự tiến triển điểm số theo phần trăm đầu được cắt tỉa - 4 biểu đồ con cho SST-2, CoLA, MRPC, và IWSLT]I'll translate this research paper from English to Vietnamese, maintaining the exact same structure including all sections, tables, and figures.

# 1905.10650.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1905.10650.pdf
# Kích thước tệp: 576411 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Liệu Mười Sáu Đầu Có Thực Sự Tốt Hơn Một Đầu?
Paul Michel
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
pmichel1@cs.cmu.edu

Omer Levy
Nghiên cứu Trí tuệ Nhân tạo Facebook
Seattle, WA
omerlevy@fb.com

Graham Neubig
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
gneubig@cs.cmu.edu

Tóm tắt
Attention là một cơ chế mạnh mẽ và phổ biến cho phép các mô hình thần kinh tập trung vào các phần thông tin quan trọng cụ thể bằng cách lấy trung bình có trọng số của chúng khi đưa ra dự đoán. Cụ thể, multi-headed attention là động lực thúc đẩy nhiều mô hình xử lý ngôn ngữ tự nhiên (NLP) hiện đại như các mô hình dịch máy dựa trên Transformer và BERT. Các mô hình này áp dụng nhiều cơ chế attention song song, với mỗi "đầu" attention có thể tập trung vào các phần khác nhau của đầu vào, điều này giúp có thể biểu diễn các hàm phức tạp vượt ra ngoài trung bình có trọng số đơn giản. Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng ngay cả khi các mô hình đã được huấn luyện sử dụng nhiều đầu, trong thực tế, một tỷ lệ lớn các đầu attention có thể bị loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất. Thực tế, một số lớp thậm chí có thể được giảm xuống chỉ còn một đầu. Chúng tôi tiếp tục kiểm tra các thuật toán tham lam để cắt tỉa mô hình, và các cải thiện tiềm năng về tốc độ, hiệu quả bộ nhớ và độ chính xác có thể đạt được từ đó. Cuối cùng, chúng tôi phân tích kết quả liên quan đến việc những phần nào của mô hình phụ thuộc nhiều hơn vào việc có nhiều đầu, và cung cấp bằng chứng sơ bộ rằng động lực học huấn luyện đóng vai trò trong lợi ích do multi-head attention mang lại¹.

1 Giới thiệu
Transformer (Vaswani et al., 2017) đã cho thấy hiệu suất tối ưu trên nhiều tác vụ NLP khác nhau, bao gồm nhưng không giới hạn ở dịch máy (Vaswani et al., 2017; Ott et al., 2018), trả lời câu hỏi (Devlin et al., 2018), phân loại văn bản (Radford et al., 2018), và gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018). Trung tâm của các cải tiến kiến trúc, Transformer mở rộng cơ chế attention tiêu chuẩn (Bahdanau et al., 2015; Cho et al., 2014) thông qua multi-headed attention (MHA), nơi attention được tính toán độc lập bởi Nₕ cơ chế attention song song (đầu). Đã được chứng minh rằng ngoài việc cải thiện hiệu suất, MHA có thể giúp với sự thống nhất chủ-vị (Tang et al., 2018) và một số đầu có thể dự đoán được cấu trúc phụ thuộc (Raganato và Tiedemann, 2018). Kể từ đó, một số phần mở rộng của phương pháp tổng quát đã được đề xuất (Ahmed et al., 2017; Shen et al., 2018).

¹Code để nhân rộng các thí nghiệm của chúng tôi được cung cấp tại https://github.com/pmichel31415/are-16-heads-really-better-than-1

Hội nghị thứ 33 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2019), Vancouver, Canada.
arXiv:1905.10650v3 [cs.CL] 4 Nov 2019

--- TRANG 2 ---
Tuy nhiên, vẫn chưa hoàn toàn rõ ràng: nhiều đầu trong các mô hình này mang lại lợi ích gì cho chúng ta? Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng - trong cả các mô hình dựa trên Transformer cho dịch máy và BERT (Devlin et al., 2018) cho suy luận ngôn ngữ tự nhiên - hầu hết các đầu attention có thể được loại bỏ riêng lẻ sau khi huấn luyện mà không có bất kỳ tác động tiêu cực đáng kể nào về hiệu suất kiểm tra (§3.2). Đáng chú ý, nhiều lớp attention thậm chí có thể được giảm xuống riêng lẻ thành một đầu attention duy nhất mà không ảnh hưởng đến hiệu suất kiểm tra (§3.3).

Dựa trên quan sát này, chúng tôi tiếp tục đề xuất một thuật toán đơn giản cắt tỉa một cách tham lam và lặp đi lặp lại các đầu attention dường như đang đóng góp ít hơn cho mô hình. Bằng cách loại bỏ đồng thời các đầu attention từ toàn bộ mạng, mà không hạn chế việc cắt tỉa ở một lớp duy nhất, chúng tôi thấy rằng các phần lớn của mạng có thể được loại bỏ với ít hoặc không có hậu quả, nhưng phần lớn các đầu phải được giữ lại để tránh sụt giảm hiệu suất thảm khốc (§4). Chúng tôi tiếp tục thấy rằng điều này có lợi ích đáng kể cho hiệu quả thời gian suy luận, dẫn đến tăng tốc độ suy luận lên đến 17.5% cho mô hình dựa trên BERT.

Sau đó chúng tôi đi sâu vào phân tích thêm. Một cái nhìn gần hơn về trường hợp dịch máy cho thấy các lớp attention encoder-decoder đặc biệt nhạy cảm với việc cắt tỉa, nhiều hơn so với các lớp self-attention, cho thấy rằng tính đa đầu đóng vai trò quan trọng trong thành phần này (§5). Cuối cùng, chúng tôi cung cấp bằng chứng rằng sự phân biệt giữa các đầu quan trọng và không quan trọng tăng lên khi huấn luyện tiến triển, cho thấy sự tương tác giữa tính đa đầu và động lực học huấn luyện (§6).

2 Nền tảng: Attention, Multi-headed Attention, và Masking
Trong phần này, chúng tôi đặt nền tảng ký hiệu về attention, và cũng mô tả phương pháp của chúng tôi để che đi các đầu attention.

2.1 Single-headed Attention
Chúng tôi nhắc lại ngắn gọn cách attention vani hoạt động. Chúng tôi tập trung vào scaled bilinear attention (Luong et al., 2015), biến thể được sử dụng phổ biến nhất trong các lớp MHA. Cho một chuỗi n vector d chiều x = x₁, ..., xₙ ∈ ℝᵈ, và một vector truy vấn q ∈ ℝᵈ, lớp attention được tham số hóa bởi Wₖ, Wq, Wᵥ, Wₒ ∈ ℝᵈˣᵈ tính tổng có trọng số:

Att_{Wₖ,Wq,Wᵥ,Wₒ}(x,q) = Wₒ ∑ᵢ₌₁ⁿ αᵢWᵥxᵢ

trong đó αᵢ = softmax(q^T W_q^T W_k x_i / √d)

Trong self-attention, mỗi xᵢ được sử dụng làm truy vấn q để tính toán một chuỗi biểu diễn mới, trong khi đó trong các mô hình sequence-to-sequence, q thường là một trạng thái decoder trong khi x tương ứng với đầu ra encoder.

2.2 Multi-headed Attention
Trong multi-headed attention (MHA), Nₕ lớp attention được tham số hóa độc lập được áp dụng song song để có được kết quả cuối cùng:

MHAtt(x,q) = ∑ₕ₌₁^{Nₕ} Att_{Wₖʰ,Wqʰ,Wᵥʰ,Wₒʰ}(x,q)    (1)

trong đó Wₖʰ, Wqʰ, Wᵥʰ ∈ ℝᵈʰˣᵈ và Wₒʰ ∈ ℝᵈˣᵈʰ. Khi dₕ = d, MHA có tính biểu diễn nghiêm ngặt hơn attention vani. Tuy nhiên, để giữ số lượng tham số không đổi, dₕ thường được đặt thành d/Nₕ, trong trường hợp này MHA có thể được xem như một ensemble của các lớp attention vani low-rank². Trong phần tiếp theo, chúng tôi sử dụng Attʰ(x) như một ký hiệu viết tắt cho đầu ra của đầu h trên đầu vào x.

Để cho phép các đầu attention khác nhau tương tác với nhau, transformer áp dụng một mạng feed-forward phi tuyến trên đầu ra của MHA, tại mỗi lớp transformer (Vaswani et al., 2017).

²Ký hiệu này, tương đương với công thức "nối" từ Vaswani et al. (2017), được sử dụng để dễ dàng trình bày trong các phần tiếp theo.

--- TRANG 3 ---
[Biểu đồ 1: Phân bố đầu theo điểm số mô hình sau khi masking]

2.3 Masking Attention Heads
Để thực hiện các thí nghiệm ablation trên các đầu, chúng tôi sửa đổi công thức cho MHAtt:

MHAtt(x,q) = ∑ₕ₌₁^{Nₕ} ζₕ Att_{Wₖʰ,Wqʰ,Wᵥʰ,Wₒʰ}(x,q)

trong đó ζₕ là các biến mask với giá trị trong {0,1}. Khi tất cả ζₕ bằng 1, điều này tương đương với công thức trong Phương trình 1. Để mask đầu h, chúng tôi đơn giản đặt ζₕ = 0.

3 Tất Cả Các Đầu Attention Có Quan Trọng Không?
Chúng tôi thực hiện một loạt thí nghiệm trong đó chúng tôi loại bỏ một hoặc nhiều đầu attention từ một kiến trúc nhất định tại thời điểm kiểm tra, và đo lường sự khác biệt hiệu suất. Chúng tôi đầu tiên loại bỏ một đầu attention tại một thời điểm (§3.2) và sau đó loại bỏ mọi đầu trong toàn bộ lớp trừ một đầu (§3.3).

3.1 Thiết lập Thí nghiệm
Trong tất cả các thí nghiệm sau, chúng tôi xem xét hai mô hình đã được huấn luyện:

WMT Đây là kiến trúc transformer "large" gốc từ Vaswani et al. 2017 với 6 lớp và 16 đầu mỗi lớp, được huấn luyện trên corpus WMT2014 English to French. Chúng tôi sử dụng mô hình được huấn luyện trước của Ott et al. 2018³ và báo cáo điểm BLEU trên tập kiểm tra newstest2013. Theo Ott et al. 2018, chúng tôi tính điểm BLEU trên đầu ra đã được tokenize của mô hình sử dụng Moses (Koehn et al., 2007). Ý nghĩa thống kê được kiểm tra với paired bootstrap resampling (Koehn, 2004) sử dụng compare-mt⁴ (Neubig et al., 2019) với 1000 mẫu lại. Một đặc điểm của mô hình này là nó có 3 cơ chế attention riêng biệt: encoder self-attention (Enc-Enc), encoder-decoder attention (Enc-Dec) và decoder self-attention (Dec-Dec), tất cả đều sử dụng MHA.

BERT BERT (Devlin et al., 2018) là một transformer duy nhất được huấn luyện trước trên tác vụ "masked language modeling" không giám sát kiểu cloze và sau đó được fine-tune trên các tác vụ cụ thể. Tại thời điểm ra đời, nó đạt được hiệu suất tối ưu trên nhiều tác vụ NLP. Chúng tôi sử dụng mô hình base-uncased được huấn luyện trước của Devlin et al. 2018 với 12 lớp và 12 đầu attention mà chúng tôi fine-tune và đánh giá trên MultiNLI (Williams et al., 2018). Chúng tôi báo cáo độ chính xác trên tập validation "matched". Chúng tôi kiểm tra ý nghĩa thống kê sử dụng t-test. Trái ngược với mô hình WMT, BERT chỉ có một cơ chế attention (self-attention trong mỗi lớp).

3.2 Ablating Một Đầu
Để hiểu đóng góp của một đầu attention cụ thể h, chúng tôi đánh giá hiệu suất của mô hình khi mask đầu đó (tức là thay thế Attʰ(x) bằng số không). Nếu hiệu suất không có h tệ hơn đáng kể so với mô hình đầy đủ, h rõ ràng là quan trọng; nếu hiệu suất tương đương, h là dư thừa với phần còn lại của mô hình.

Hình 1a và 1b cho thấy phân bố các đầu về điểm số của mô hình sau khi mask nó, đối với WMT và BERT tương ứng. Chúng tôi quan sát thấy rằng phần lớn các đầu attention có thể được loại bỏ mà không lệch quá nhiều khỏi điểm số gốc. Đáng ngạc nhiên, trong một số trường hợp việc loại bỏ một đầu attention dẫn đến tăng BLEU/accuracy.

Để có cái nhìn chi tiết hơn về những kết quả này, chúng tôi phóng to các lớp encoder self-attention của mô hình WMT trong Bảng 1. Đáng chú ý, chúng tôi thấy rằng chỉ 8 (trong 96) đầu gây ra thay đổi có ý nghĩa thống kê trong hiệu suất khi chúng được loại bỏ khỏi mô hình, một nửa trong số đó thực sự dẫn đến điểm BLEU cao hơn. Điều này dẫn chúng tôi đến quan sát đầu tiên: tại thời điểm kiểm tra, hầu hết các đầu là dư thừa với phần còn lại của mô hình.

3.3 Ablating Tất Cả Các Đầu Trừ Một
Quan sát này đưa ra câu hỏi: liệu có cần nhiều hơn một đầu không? Do đó, chúng tôi tính toán sự khác biệt hiệu suất khi tất cả các đầu trừ một được loại bỏ, trong một lớp duy nhất. Trong Bảng 2 và Bảng 3, chúng tôi báo cáo điểm số tốt nhất cho mỗi lớp trong mô hình, tức là điểm số khi giảm toàn bộ lớp xuống còn đầu quan trọng nhất.

[Bảng 1: Sự khác biệt điểm BLEU cho mỗi đầu của cơ chế encoder self attention]

[Bảng 2: Delta BLEU tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình WMT]

[Bảng 3: Delta accuracy tốt nhất theo lớp khi chỉ giữ một đầu trong mô hình BERT]

³https://github.com/pytorch/fairseq/tree/master/examples/translation
⁴https://github.com/neulab/compare-mt

--- TRANG 4 ---
Chúng tôi thấy rằng, đối với hầu hết các lớp, một đầu thực sự đủ tại thời điểm kiểm tra, mặc dù mạng được huấn luyện với 12 hoặc 16 đầu attention. Điều này đáng chú ý bởi vì các lớp này có thể được giảm xuống single-headed attention chỉ với 1/16 (tương ứng 1/12) số lượng tham số của một lớp attention vani. Tuy nhiên, một số lớp đòi hỏi nhiều đầu attention; ví dụ, thay thế lớp cuối cùng trong encoder-decoder attention của WMT bằng một đầu duy nhất làm giảm hiệu suất ít nhất 13.5 điểm BLEU. Chúng tôi phân tích thêm khi nào các thành phần mô hình khác nhau phụ thuộc vào nhiều đầu hơn trong §5.

Ngoài ra, chúng tôi xác minh rằng kết quả này vẫn đúng ngay cả khi chúng tôi không có quyền truy cập vào tập đánh giá khi chọn đầu "tốt nhất một mình". Với mục đích này, chúng tôi chọn đầu tốt nhất cho mỗi lớp trên tập validation (newstest2013 cho WMT và một tập con 5,000 mẫu được chọn ngẫu nhiên từ tập huấn luyện MNLI cho BERT) và đánh giá hiệu suất của mô hình trên tập kiểm tra (newstest2014 cho WMT và tập validation MNLI-matched cho BERT). Chúng tôi quan sát thấy rằng các phát hiện tương tự vẫn đúng: chỉ giữ một đầu không dẫn đến thay đổi có ý nghĩa thống kê trong hiệu suất cho 50% (tương ứng 100%) các lớp của WMT (tương ứng BERT). Kết quả chi tiết có thể được tìm thấy trong Phụ lục A.

3.4 Các Đầu Quan Trọng Có Giống Nhau Trên Các Tập Dữ Liệu Không?
Có một điểm cần lưu ý trong hai thí nghiệm trước của chúng tôi: những kết quả này chỉ có giá trị trên các tập kiểm tra cụ thể (và khá nhỏ), đặt ra nghi ngờ về khả năng tổng quát hóa của chúng lên các tập dữ liệu khác. Như bước đầu tiên để hiểu liệu một số đầu có quan trọng phổ quát không, chúng tôi thực hiện cùng một nghiên cứu ablation trên một tập kiểm tra thứ hai, ngoài miền. Cụ thể, chúng tôi xem xét tập validation MNLI "mismatched" cho BERT và tập kiểm tra MTNT English to French (Michel và Neubig, 2018) cho mô hình WMT, cả hai đều được tập hợp với mục đích cung cấp các bộ kiểm tra đối lập, ngoài miền cho các tác vụ tương ứng.

Chúng tôi thực hiện cùng một nghiên cứu ablation như §3.2 trên mỗi tập dữ liệu này và báo cáo kết quả trong Hình 2a và 2b. Chúng tôi nhận thấy rằng có một tương quan dương, > 0.5 (p < 0.01) giữa hiệu ứng của việc loại bỏ một đầu trên cả hai tập dữ liệu. Hơn nữa, các đầu có hiệu ứng cao nhất đối với hiệu suất trên một miền có xu hướng có cùng hiệu ứng trên miền khác, điều này cho thấy rằng các đầu quan trọng nhất từ §3.2 thực sự "phổ quát" quan trọng.

4 Cắt Tỉa Lặp Đi Lặp Lại Các Đầu Attention
Trong các thí nghiệm ablation của chúng tôi (§3.2 và §3.3), chúng tôi quan sát hiệu ứng của việc loại bỏ một hoặc nhiều đầu trong một lớp duy nhất, mà không xem xét điều gì sẽ xảy ra nếu chúng tôi thay đổi hai hoặc nhiều lớp khác nhau cùng một lúc. Để kiểm tra hiệu ứng tổng hợp của việc cắt tỉa nhiều đầu từ khắp toàn bộ mô hình, chúng tôi sắp xếp tất cả các đầu attention trong mô hình theo điểm số tầm quan trọng proxy (được mô tả dưới đây), và sau đó loại bỏ các đầu từng cái một. Chúng tôi sử dụng phương pháp lặp, heuristic này để tránh tìm kiếm tổ hợp, điều này không thực tế do số lượng đầu và thời gian cần thiết để đánh giá mỗi mô hình.

--- TRANG 5 ---
[Hình 2: Phân tích cross-task về hiệu ứng của việc cắt tỉa đối với độ chính xác]

[Hình 3: Sự tiến triển của độ chính xác theo số lượng đầu được cắt tỉa]

4.1 Điểm Số Tầm Quan Trọng Đầu để Cắt Tỉa
Như một điểm số proxy cho tầm quan trọng đầu, chúng tôi xem xét độ nhạy cảm kỳ vọng của mô hình đối với các biến mask ζₕ được định nghĩa trong §2.3:

Iₕ = |E_{x~X}[∂L(x)/∂ζₕ]|    (2)

trong đó X là phân bố dữ liệu và L(x) là loss trên mẫu x. Trực quan, nếu Iₕ có giá trị cao thì việc thay đổi ζₕ có thể có hiệu ứng lớn đối với mô hình. Cụ thể, chúng tôi thấy giá trị tuyệt đối là quan trọng để tránh các điểm dữ liệu với đóng góp rất âm hoặc dương triệt tiêu lẫn nhau trong tổng. Thay Phương trình 1 vào Phương trình 2 và áp dụng quy tắc chuỗi cho biểu thức cuối cùng sau cho Iₕ:

Iₕ = |E_{x~X}[Att^h(x)^T ∂L(x)/∂Att^h(x)]|

Công thức này gợi nhớ đến kho tàng văn học phong phú về cắt tỉa mạng thần kinh (LeCun et al., 1990; Hassibi và Stork, 1993; Molchanov et al., 2017, v.v.). Cụ thể, nó tương đương với phương pháp khai triển Taylor từ Molchanov et al. (2017).

Về hiệu suất, việc ước tính Iₕ chỉ yêu cầu thực hiện forward và backward pass, và do đó không chậm hơn huấn luyện. Trong thực tế, chúng tôi tính kỳ vọng trên dữ liệu huấn luyện hoặc một tập con của nó.⁵ Theo khuyến nghị của Molchanov et al. (2017), chúng tôi chuẩn hóa điểm số tầm quan trọng theo lớp (sử dụng chuẩn ℓ₂).

4.2 Hiệu Ứng của Cắt Tỉa đối với BLEU/Accuracy
Hình 3a (cho WMT) và 3b (cho BERT) mô tả hiệu ứng của việc cắt tỉa đầu attention đối với hiệu suất mô hình trong khi loại bỏ dần dần 10% tổng số đầu theo thứ tự tăng dần của Iₕ tại mỗi bước. Chúng tôi cũng báo cáo kết quả khi thứ tự cắt tỉa được xác định bởi sự khác biệt điểm số từ §3.2 (trong các đường nét đứt), nhưng thấy rằng việc sử dụng Iₕ nhanh hơn và cho kết quả tốt hơn.

Chúng tôi quan sát thấy rằng phương pháp này cho phép chúng tôi cắt tỉa lên đến 20% và 40% đầu từ WMT và BERT (tương ứng), mà không gây ra bất kỳ tác động tiêu cực đáng chú ý nào. Hiệu suất giảm mạnh khi cắt tỉa thêm, có nghĩa là không mô hình nào có thể được giảm xuống mô hình attention hoàn toàn single-head mà không cần huấn luyện lại hoặc chịu tổn thất đáng kể về hiệu suất. Chúng tôi tham khảo Phụ lục B cho các thí nghiệm trên bốn tập dữ liệu bổ sung.

4.3 Hiệu Ứng của Cắt Tỉa đối với Hiệu Quả
Ngoài hiệu suất tác vụ downstream, còn có những lợi thế nội tại khi cắt tỉa đầu. Đầu tiên, mỗi đầu đại diện cho một tỷ lệ không đáng kể của tổng tham số trong mỗi lớp attention (6.25% cho WMT, 8.34% cho BERT), và do đó của tổng mô hình (nói chung, trong cả hai mô hình của chúng tôi, khoảng một phần ba tổng số tham số được dành cho MHA trên tất cả các lớp).⁶ Điều này hấp dẫn trong bối cảnh triển khai mô hình trong các cài đặt hạn chế bộ nhớ.

[Bảng 4: Tốc độ suy luận trung bình của BERT trên tập validation MNLI-matched]

Hơn nữa, chúng tôi thấy rằng việc thực sự cắt tỉa các đầu (và không chỉ masking) dẫn đến tăng tốc độ suy luận đáng kể. Bảng 4 báo cáo số lượng ví dụ mỗi giây được xử lý bởi BERT, trước và sau khi cắt tỉa 50% tất cả các đầu attention. Các thí nghiệm được thực hiện trên hai máy khác nhau, cả hai đều được trang bị GPU GeForce GTX 1080Ti. Mỗi thí nghiệm được lặp lại 3 lần trên mỗi máy (tổng cộng 6 điểm dữ liệu cho mỗi cài đặt). Chúng tôi thấy rằng việc cắt tỉa một nửa đầu của mô hình tăng tốc độ suy luận lên đến 17.5% cho kích thước batch cao hơn (sự khác biệt này biến mất đối với kích thước batch nhỏ hơn).

5 Khi Nào Nhiều Đầu Quan Trọng? Trường Hợp Dịch Máy
Như được thể hiện trong Bảng 2, không phải tất cả các lớp MHA đều có thể được giảm xuống một đầu attention duy nhất mà không ảnh hưởng đáng kể đến hiệu suất. Để có ý tưởng tốt hơn về mức độ mỗi phần của mô hình dịch dựa trên transformer phụ thuộc vào tính đa đầu, chúng tôi lặp lại thí nghiệm cắt tỉa heuristic từ §4 cho mỗi loại attention riêng biệt (Enc-Enc, Enc-Dec, và Dec-Dec).

Hình 4 cho thấy rằng hiệu suất giảm nhanh hơn nhiều khi các đầu được cắt tỉa từ các lớp attention Enc-Dec. Cụ thể, việc cắt tỉa hơn 60% đầu attention Enc-Dec sẽ dẫn đến suy giảm hiệu suất thảm khốc, trong khi các lớp self-attention encoder và decoder vẫn có thể tạo ra bản dịch hợp lý (với điểm BLEU khoảng 30) chỉ với 20% đầu attention gốc. Nói cách khác, encoder-decoder attention phụ thuộc vào tính đa đầu nhiều hơn so với self-attention.

[Hình 4: BLEU khi cắt tỉa dần các đầu từ mỗi loại attention trong mô hình WMT]

⁵Đối với mô hình WMT, chúng tôi sử dụng tất cả các tập newstest20[09-12] để ước tính I.
⁶Hơi nhiều hơn trong WMT vì attention Enc-Dec.

--- TRANG 6 ---
6 Động Lực Học Tầm Quan Trọng Đầu Trong Quá Trình Huấn Luyện
Các phần trước cho chúng tôi biết rằng một số đầu quan trọng hơn những đầu khác trong các mô hình đã được huấn luyện. Để có thêm hiểu biết về động lực học tầm quan trọng đầu trong quá trình huấn luyện, chúng tôi thực hiện cùng một thí nghiệm cắt tỉa dần của §4.2 tại mỗi epoch. Chúng tôi thực hiện thí nghiệm này trên một phiên bản nhỏ hơn của mô hình WMT (6 lớp và 8 đầu mỗi lớp), được huấn luyện cho dịch Đức sang Tiếng Anh trên tập dữ liệu IWSLT 2014 nhỏ hơn Cettolo et al. (2015). Chúng tôi gọi mô hình này là IWSLT.

[Hình 5: Mối quan hệ giữa phần trăm đầu được cắt tỉa và sự giảm điểm số tương đối trong quá trình huấn luyện]

Hình 5 báo cáo, cho mỗi mức cắt tỉa (theo bước 10% — 0% tương ứng với mô hình gốc), sự tiến triển của điểm số mô hình (trên newstest2013) cho mỗi epoch. Để dễ đọc hơn, chúng tôi hiển thị các epoch trên thang logarit, và chỉ báo cáo điểm số cứ 5 epoch sau epoch thứ 10). Để làm cho điểm số có thể so sánh được qua các epoch, trục Y báo cáo sự suy giảm tương đối của điểm BLEU so với mô hình không cắt tỉa tại mỗi epoch. Đáng chú ý, chúng tôi thấy rằng có hai chế độ khác biệt: trong các epoch rất sớm (đặc biệt là 1 và 2), hiệu suất giảm tuyến tính với phần trăm cắt tỉa, tức là sự giảm tương đối trong hiệu suất độc lập với Iₕ, cho thấy rằng hầu hết các đầu ít nhiều đều quan trọng như nhau. Từ epoch 10 trở đi, có sự tập trung của các đầu không quan trọng có thể được cắt tỉa trong khi vẫn duy trì trong 85-90% điểm BLEU gốc (lên đến 40% tổng số đầu).

Điều này cho thấy rằng các đầu quan trọng được xác định sớm (nhưng không ngay lập tức) trong quá trình huấn luyện. Hai giai đoạn huấn luyện gợi nhớ đến phân tích của Shwartz-Ziv và Tishby (2017), theo đó việc huấn luyện mạng thần kinh phân tách thành giai đoạn "tối thiểu hóa rủi ro thực nghiệm", nơi mô hình tối đa hóa thông tin tương hỗ của các biểu diễn trung gian với nhãn, và giai đoạn "nén" nơi thông tin tương hỗ với đầu vào được tối thiểu hóa. Một cuộc điều tra có nguyên tắc hơn về hiện tượng này được để lại cho công việc tương lai.

7 Công Trình Liên Quan
Việc sử dụng cơ chế attention trong NLP và đặc biệt là dịch máy thần kinh (NMT) có thể được truy tìm về Bahdanau et al. (2015) và Cho et al. (2014), và hầu hết các triển khai đương thời đều dựa trên công thức từ Luong et al. (2015). Attention sớm được điều chỉnh (thành công) cho các tác vụ NLP khác, thường đạt được hiệu suất tối ưu thời bấy giờ trong đọc hiểu (Cheng et al., 2016), suy luận ngôn ngữ tự nhiên (Parikh et al., 2016) hoặc tóm tắt trừu tượng (Paulus et al., 2017) để trích dẫn một vài. Multi-headed attention được giới thiệu lần đầu bởi Vaswani et al. (2017) cho NMT và phân tích cú pháp thành phần Tiếng Anh, và sau đó được áp dụng cho transfer learning (Radford et al., 2018; Devlin et al., 2018), mô hình hóa ngôn ngữ (Dai et al., 2019; Radford et al., 2019), hoặc gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018), trong số những cái khác.

Có một văn học phong phú về cắt tỉa mạng thần kinh đã được huấn luyện, truy về LeCun et al. (1990) và Hassibi và Stork (1993) trong đầu những năm 90 và được hồi sinh sau sự ra đời của deep learning, với hai phương pháp trực giao: cắt tỉa "từng trọng số" tinh vi (Han et al., 2015) và cắt tỉa có cấu trúc (Anwar et al., 2017; Li et al., 2016; Molchanov et al., 2017), trong đó toàn bộ các phần của mô hình được cắt tỉa. Trong NLP, cắt tỉa có cấu trúc cho auto-sizing các mô hình ngôn ngữ feed-forward được điều tra lần đầu bởi Murray và Chiang (2015). Gần đây hơn, các phương pháp cắt tỉa tinh vi đã được phổ biến bởi See et al. (2016) và Kim và Rush (2016) (chủ yếu trên NMT).

Đồng thời với công trình của chúng tôi, Voita et al. (2019) đã có một quan sát tương tự về multi-head attention. Phương pháp của họ liên quan đến việc sử dụng LRP (Binder et al., 2016) để xác định các đầu quan trọng và xem xét các tính chất cụ thể như chú ý đến các vị trí lân cận, từ hiếm hoặc từ liên quan cú pháp. Họ đề xuất một cơ chế cắt tỉa thay thế dựa trên gradient descent trên các biến mask ζₕ. Trong khi phương pháp và kết quả của họ bổ sung cho bài báo này, nghiên cứu của chúng tôi cung cấp bằng chứng bổ sung về hiện tượng này ngoài NMT, cũng như phân tích động lực học huấn luyện của việc cắt tỉa các đầu attention.

8 Kết Luận
Chúng tôi đã quan sát thấy rằng MHA không phải lúc nào cũng tận dụng tính biểu diễn vượt trội về mặt lý thuyết so với attention vani một cách đầy đủ nhất. Cụ thể, chúng tôi đã chứng minh rằng trong nhiều cài đặt khác nhau, một số đầu có thể được loại bỏ khỏi các mô hình transformer đã được huấn luyện mà không có sự suy giảm có ý nghĩa thống kê trong hiệu suất kiểm tra, và một số lớp có thể được giảm xuống chỉ một đầu. Ngoài ra, chúng tôi đã chỉ ra rằng trong các mô hình dịch máy, các lớp attention encoder-decoder phụ thuộc vào tính đa đầu nhiều hơn so với các lớp self-attention, và cung cấp bằng chứng rằng tầm quan trọng tương đối của mỗi đầu được xác định trong giai đoạn đầu của huấn luyện. Chúng tôi hy vọng rằng những quan sát này sẽ thúc đẩy sự hiểu biết của chúng tôi về MHA và truyền cảm hứng cho các mô hình đầu tư tham số và attention hiệu quả hơn.

Lời Cảm Ơn
Các tác giả muốn gửi lời cảm ơn đến các reviewer ẩn danh vì phản hồi sâu sắc của họ. Chúng tôi cũng đặc biệt biết ơn Thomas Wolf từ Hugging Face, người mà nỗ lực tái tạo độc lập đã cho phép chúng tôi tìm ra và sửa lỗi trong các thí nghiệm so sánh tốc độ. Nghiên cứu này được hỗ trợ một phần bởi quà tặng từ Facebook.

Tài Liệu Tham Khảo
[Danh sách tài liệu tham khảo tiếp tục với định dạng giống như bản gốc...]

--- TRANG 7 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 8 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 11 ---
A Ablating Tất Cả Các Đầu Trừ Một: Thí Nghiệm Bổ Sung
Bảng 5 và 6 báo cáo sự khác biệt hiệu suất khi chỉ giữ một đầu cho bất kỳ lớp nào. Đầu được chọn là đầu tốt nhất một mình trên một tập dữ liệu riêng biệt.

[Bảng 5 và 6: Kết quả thí nghiệm bổ sung]

B Thí Nghiệm Cắt Tỉa Bổ Sung
Chúng tôi báo cáo kết quả bổ sung cho phương pháp cắt tỉa theo tầm quan trọng từ Phần 4 trên 4 tập dữ liệu bổ sung:

SST-2: Phiên bản GLUE của Stanford Sentiment Treebank (Socher et al., 2013). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

CoLA: Phiên bản GLUE của Corpus of Linguistic Acceptability (Warstadt et al., 2018). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

MRPC: Phiên bản GLUE của Microsoft Research Paraphrase Corpus (Dolan và Brockett, 2005). Chúng tôi sử dụng BERT đã được fine-tune làm mô hình.

IWSLT: Tập dữ liệu dịch Đức sang Tiếng Anh từ IWSLT 2014 (Cettolo et al., 2015). Chúng tôi sử dụng mô hình nhỏ hơn giống như được mô tả trong Phần 6.

Hình 6 cho thấy rằng trong một số trường hợp lên đến 60% (SST-2) hoặc 50% (CoLA, MRPC) đầu có thể được cắt tỉa mà không có tác động đáng chú ý đến hiệu suất.

--- TRANG 12 ---
--- TRANG 13 ---
[Hình 6: Sự tiến triển điểm số theo phần trăm đầu được cắt tỉa - 4 biểu đồ con cho SST-2, CoLA, MRPC, và IWSLT]