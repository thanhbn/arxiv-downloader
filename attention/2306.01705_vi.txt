[29] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, và Ilya Sutskever. 2020. Distribution augmentation for generative modeling. Trong International Conference on Machine Learning. PMLR, 5006–5019.
[30] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. Trong International Conference on Machine Learning. PMLR, 5156–5165.
[31] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, và Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172 (2019).
[32] Diederik P Kingma và Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[33] Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451 (2020).
[34] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).
[35] Yann LeCun, John Denker, và Sara Solla. 1989. Optimal brain damage. Advances in neural information processing systems 2 (1989).
[36] Conglong Li, Minjia Zhang, và Yuxiong He. 2021. Curriculum learning: A regularization method for efficient and stable billion-scale gpt model pre-training. arXiv preprint arXiv:2108.06084 (2021).
[37] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. 2016. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 (2016).
[38] Liu Liu, Zheng Qu, Zhaodong Chen, Yufei Ding, và Yuan Xie. 2021. Transformer Acceleration with Dynamic Sparse Attention. arXiv preprint arXiv:2110.11299 (2021).
[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision. 10012–10022.
[40] Matt Mahoney. 2011. Large text compression benchmark.
[41] Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 (2016).
[42] Paul Michel, Omer Levy, và Graham Neubig. 2019. Are sixteen heads really better than one? Advances in neural information processing systems 32 (2019).
[43] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, và Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 (2019).
[44] Wonpyo Park, Woong-Gi Chang, Donggeon Lee, Juntae Kim, et al. 2022. GRPE: Relative Positional Encoding for Graph Transformer. Trong ICLR2022 Machine Learning for Drug Discovery.
[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019).
[46] Sai Prasanna, Anna Rogers, và Anna Rumshisky. 2020. When bert plays the lottery, all tickets are winning. arXiv preprint arXiv:2005.00561 (2020).
[47] Ofir Press, Noah A Smith, và Mike Lewis. 2020. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832 (2020).
[48] Ofir Press, Noah A Smith, và Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409 (2021).
[49] Zheng Qu, Liu Liu, Fengbin Tu, Zhaodong Chen, Yufei Ding, và Yuan Xie. 2022. DOTA: detect and omit weak attentions for scalable transformer acceleration. Trong Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. 14–26.
[50] Alec Radford, Karthik Narasimhan, Tim Salimans, và Ilya Sutskever. 2018. Improving language understanding by generative pre-training. (2018).
[51] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, và Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507 (2019).
[52] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, và Bo Dai. 2021. Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information Processing Systems 34 (2021), 22470–22482.
[53] Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J Liu, Sharan Narang, Wei Li, và Yanqi Zhou. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. (2019).
[54] Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics 9 (2021), 53–68.
[55] Imanol Schlag, Kazuki Irie, và Jürgen Schmidhuber. 2021. Linear transformers are secretly fast weight memory systems. arXiv preprint arXiv:2102.11174 (2021).
[56] Peter Shaw, Jakob Uszkoreit, và Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155 (2018).
[57] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, và Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.

--- TRANG 11 ---
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in neural information processing systems. 5998–6008.
[59] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, và Rob Fergus. 2013. Regularization of neural networks using dropconnect. Trong International conference on machine learning. PMLR, 1058–1066.
[60] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, và Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020).
[61] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, và Hongyang Chao. 2021. Rethinking and improving relative position encoding for vision transformer. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision. 10033–10041.
[62] Wenhan Xiong, Barlas Oğuz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Wen-tau Yih, và Yashar Mehdad. 2021. Simple Local Attentions Remain Competitive for Long-Context Tasks. arXiv preprint arXiv:2112.07210 (2021).
[63] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, và Vikas Singh. 2021. Nyströmformer: A nyström-based algorithm for approximating self-attention. Trong Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 14138–14148.
[64] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, và Tie-Yan Liu. 2021. Do Transformers Really Perform Bad for Graph Representation? arXiv preprint arXiv:2106.05234 (2021).
[65] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems 33 (2020), 17283–17297.
[66] Lin Zehui, Pengfei Liu, Luyao Huang, Junkun Chen, Xipeng Qiu, và Xuanjing Huang. 2019. DropAttention: a regularization method for fully-connected self-attention networks. arXiv preprint arXiv:1907.11065 (2019).
[67] Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, và Ming Zhou. 2020. Scheduled drophead: A regularization method for transformer models. arXiv preprint arXiv:2004.13342 (2020).

A TÍNH KHẢ DỤNG CỦA DỮ LIỆU VÀ MÃ
Dữ liệu: Tất cả các tập dữ liệu được sử dụng trong công trình này đều có sẵn công khai. Các nguồn tập dữ liệu được liệt kê trong Bảng 6.
Mã: Mã có sẵn tại https://github.com/shamim-hussain/ssa.

Bảng 6: Nguồn tập dữ liệu.
Tập dữ liệu | Nguồn
WikiText-103 | https://huggingface.co/datasets/wikitext
Enwik8 | http://mattmahoney.net/dc/textdata.html
CIFAR-10 | https://www.cs.toronto.edu/~kriz/cifar.html
ImageNet-1k | https://image-net.org/
PCQM4Mv2 | https://ogb.stanford.edu

B SIÊU THAM SỐ VÀ CHI TIẾT HUẤN LUYỆN

B.1 Mô hình Ngôn ngữ Sinh
Đối với mô hình ngôn ngữ trên cả hai tập dữ liệu WikiText-103 và Enwik8, chúng tôi sử dụng transformer decoder 16 tầng của Press et al. [48] sử dụng mã hóa vị trí tương đối ALiBi. Chúng tôi sử dụng bộ công cụ fairseq [43] để thực hiện các thí nghiệm này. Chúng tôi sử dụng độ dài đầu vào 3072 token cho WikiText-103. Adaptive input embeddings [3] và adaptive softmax [28] output được sử dụng để xử lý từ vựng lớn có kích thước khoảng 260K. Đối với Enwik8, chúng tôi sử dụng vector embedding đơn giản và tầng output softmax thông thường. Chúng tôi sử dụng cùng kiến trúc và siêu tham số như Press et al. [48], ngoại trừ chúng tôi thay đổi hàm kích hoạt từ ReLU sang GELU [22]. Đối với Enwik8, chúng tôi cũng thêm tầng Layer Normalization [2] cuối cùng trước tầng softmax. Trên WikiText-103 chúng tôi huấn luyện trong 64,000 bước (16,000 bước warmup tỷ lệ học tuyến tính, theo sau bởi 48,000 bước cosine decay) với bộ tối ưu Adam [32], tỷ lệ học tối đa 0.001 và kích thước batch tăng lên 64. Đối với Enwik8, chúng tôi huấn luyện trong 10,000 bước (4,000 bước warmup theo sau bởi linear decay) với tỷ lệ học tối đa 0.001 và tỷ lệ học tối thiểu 0.0005. Một lần nữa chúng tôi sử dụng kích thước batch 64 và bộ tối ưu Adam.

Chúng tôi điều chỉnh tham số SSA σ (trong Phương trình 3) trong các tầng khác nhau cho kết quả tập validation tốt nhất. Chúng tôi biểu diễn giá trị của nó như một phần của độ dài đầu vào để nó độc lập với độ dài đầu vào. Đối với WikiText-103 chúng tôi sử dụng giá trị σ=0.2 trong tầng đầu tiên và tăng tuyến tính đến σ=0.35 trong tầng sâu nhất. Đối với Enwik8 chúng tôi bắt đầu với σ=0.1 và tăng tuyến tính đến σ=0.225 trong tầng sâu nhất.

B.2 Tạo Hình ảnh và Phân loại
Đối với tạo hình ảnh trên CIFAR-10, chúng tôi sử dụng transformer 16 tầng với cài đặt kiến trúc và siêu tham số tương tự như trong phần trước, nhưng chúng tôi sử dụng thiên lệch vị trí tương đối 2D cho mã hóa vị trí, tương tự như [39]. Thay vì sử dụng chính quy hóa dropout, chúng tôi sử dụng các kỹ thuật augmentation được mô tả trong [29] để đạt được khả năng tổng quát hóa tốt hơn. Chúng tôi sử dụng bộ tối ưu Adam với tỷ lệ học tối đa 0.002 và kích thước batch 128. Chúng tôi huấn luyện trong tổng cộng 100,000 bước, với warmup tỷ lệ học ban đầu trong 4,000 bước, theo sau bởi cosine decay. Để thực hiện locally biased SSA trên dữ liệu 2D này, chúng tôi chia hình ảnh theo chiều dọc thành 4 cửa sổ gồm 8 hàng. Locally biased source shuffling được thực hiện theo cả hướng ngang và dọc trong khi bảo tồn tính nhân quả ở cấp độ cửa sổ. Chúng tôi đặt tham số SSA σ thành 0.25 lần các chiều (chiều dài/chiều rộng), theo cả hướng ngang và dọc, và tất cả các tầng.

Đối với phân loại ImageNet-1K, chúng tôi sử dụng mô hình Swin-Tiny với 12 tầng và 28 triệu tham số và độ phân giải đầu vào 224x224. Chúng tôi sử dụng cùng kiến trúc, siêu tham số, augmentation và sơ đồ huấn luyện như trong [39]. Để áp dụng locally biased SSA trong các cửa sổ 14x14, chúng tôi tiếp tục chia nhỏ các cửa sổ thành 4 cửa sổ con 7x7. Locally biased source shuffling được thực hiện theo cả hướng ngang và dọc, với giá trị σ là 0.75 lần kích thước cửa sổ (tức là 14), nhưng chúng tôi tiếp tục đảm bảo rằng các nguồn bị ràng buộc trong các cửa sổ 14x14 (dịch chuyển) riêng của chúng sau khi xáo trộn (tuy nhiên chúng có thể di chuyển vượt qua các cửa sổ con 7x7 nhỏ hơn).

B.3 Hồi quy Đồ thị Phân tử
Đối với hồi quy đồ thị trên tập dữ liệu PCQM4Mv2, chúng tôi sử dụng Edge-augmented Graph Transformer (EGT) được mô tả trong [26]. EGT sử dụng các kênh bổ sung để biểu diễn và cập nhật edge embeddings, điều này làm cho nó hơi khác so với transformer tiêu chuẩn. Tuy nhiên, chúng tôi thí nghiệm với một biến thể rút gọn của EGT gọi là EGT-Simple giảm các biểu diễn cạnh thành mã hóa vị trí tương đối. Tuy nhiên, để ngắn gọn, chúng tôi gọi mô hình này là EGT. Chúng tôi thí nghiệm trên mô hình EGT nhỏ với 11 triệu tham số và 6 tầng. Chúng tôi sử dụng cùng siêu tham số và sơ đồ huấn luyện như trong [26], ngoại trừ, chúng tôi không sử dụng attention dropout trong các mô hình này vì SSA hoạt động như một phương pháp chính quy hóa.

--- TRANG 12 ---
Bảng 7: Chi phí huấn luyện baseline (S0) cho các tập dữ liệu khác nhau.

Tập dữ liệu | Mô hình | Compute (Exa FLOP) | Memory/GPU (GB) | Time/step (ms)
WikiText-103 | Transfo. Decoder | 7.6 | 21.3 | 492
Enwik8 | Transfo. Decoder | 1.7 | 30.5 | 628
CIFAR-10 | Transfo. Decoder | 23.8 | 29.1 | 1059
ImageNet-1k | Swin-Tiny | 3.2 | 4.0 | 146
PCQM4Mv2 | EGT small | 0.5 | – | –

C CHI PHÍ HUẤN LUYỆN BASELINE
Trong các thí nghiệm của chúng tôi, chúng tôi chuẩn hóa chi phí huấn luyện so với mô hình baseline S0, mô hình attention dày đặc không có SSA. Tuy nhiên, chúng tôi cũng báo cáo chi phí huấn luyện baseline về giá trị tuyệt đối trong Bảng 7 để hoàn thiện. Lưu ý rằng đối với tập dữ liệu PCQM4Mv2, chúng tôi không thể tính toán một cách chính xác mức tiêu thụ bộ nhớ và thời gian huấn luyện do nút thắt tải dữ liệu. Tuy nhiên, chúng ta vẫn có thể so sánh chi phí của các mô hình baseline với các mô hình SSA về compute.

D LOCALLY BIASED VS UNBIASED SSA
Trong kết quả được trình bày trong các thí nghiệm của chúng tôi, chúng tôi tuyên bố rằng thiên lệch địa phương là một thành phần quan trọng trong việc cải thiện hiệu suất của SSA. Ở đây, chúng tôi trực tiếp so sánh kết quả cho locally biased SSA và unbiased SSA cho cùng mức độ thưa thớt và khi chúng được áp dụng cho cùng tập con các tầng.

Bảng 8: Kết quả locally biased vs unbiased SSA cho các tác vụ mô hình ngôn ngữ trên WikiText-103 và Enwik8. Kết quả Locally biased SSA từ Bảng 1.

Wikitext-103 | Enwik8
% Attention được lấy mẫu | dev/test Ppl.↓ | dev/test BPB↓
| Locally Biased | Unbiased | Locally biased | Unbiased
50% attention | 17.12 / 17.84 | 17.45 / 18.18 | 1.052 / 1.028 | 1.114 / 1.087
+FT | 16.95 / 17.68 | 16.96 / 17.69 | 1.050 / 1.026 | 1.063 / 1.042
25% attention | 17.39 / 18.13 | 19.33 / 20.15 | 1.081 / 1.058 | 1.329 / 1.287
+FT | 16.91 / 17.60 | 17.89 / 18.69 | 1.052 / 1.029 | 1.100 / 1.075

Kết quả cho mô hình ngôn ngữ được trình bày trong Bảng 8 trong đó cùng mức độ thưa thớt được áp dụng cho tất cả các tầng cho cả hai loại SSA (sử dụng attention 2 hoặc 4 cửa sổ cho lấy mẫu 50% hoặc 25%, tương ứng). Chúng ta thấy rằng unbiased SSA hoạt động hơi tệ hơn so với locally biased SSA cho lấy mẫu 50% attention. Khoảng cách này có thể được giảm với fine-tuning. Tuy nhiên, khi chúng ta chỉ lấy mẫu 25% attention, việc huấn luyện bị cản trở đáng kể đối với unbiased SSA, và kết quả không thể được so sánh với locally biased SSA, ngay cả với fine-tuning. Điều này là vì unbiased SSA không thể lấy mẫu các đường dẫn quan trọng với xác suất đủ cao để việc huấn luyện tiến triển một cách duyên dáng. Điều này cho thấy sự cần thiết của thiên lệch địa phương cho việc lấy mẫu ở mức độ thưa thớt cao.

Kết quả cho phân loại hình ảnh được trình bày trong Bảng 9 trong đó cùng mức độ thưa thớt (25% được lấy mẫu, 75% được loại bỏ) được áp dụng cho 10 tầng (sử dụng 4 cửa sổ con) hoặc 4 tầng đầu tiên được loại trừ. Trong tất cả các trường hợp, chúng ta thấy rằng locally biased SSA hoạt động tốt hơn so với unbiased SSA, nhưng chúng ta có được kết quả tốt với unbiased SSA khi chúng ta loại trừ 4 tầng đầu tiên. Điều này cho thấy rằng thiên lệch địa phương quan trọng đối với SSA để hoạt động tốt, nhưng nó ít quan trọng hơn trong các tầng sâu hơn so với các tầng nông hơn. Trong các tầng sâu hơn, mô hình có xu hướng tạo thành các phụ thuộc tầm xa, dễ dự đoán hơn bởi unbiased SSA. Đây là lý do tại sao chúng ta thấy rằng unbiased SSA hoạt động tốt hơn khi chúng ta loại trừ 4 tầng đầu tiên.

Bảng 9: Kết quả locally biased vs unbiased SSA trên tác vụ phân loại hình ảnh ImageNet-1K. Kết quả Locally biased SSA được trình bày từ Bảng 2.

# Tầng được lấy mẫu | dev Acc.↑
| Locally Biased | Unbiased
10 tầng | 80.56% | 80.21%
+FT | 81.15% | 80.80%
6 tầng | 81.23% | 81.21%
+FT | 81.60% | 81.36%

Bảng 10: Kết quả self-ensembling bổ sung cho các tác vụ mô hình ngôn ngữ trên WikiText-103, được tạo ra với 50 mẫu mỗi đoạn đầu vào. Kết quả Renormalized từ Bảng 1.

dev/test Ppl.↓
Model | Renorm. | Ensemble
S16-L6 | 17.49 / 18.30 | 17.01 / 17.80
+FT | 17.09 / 17.86 | 16.83 / 17.53
S12-L8 | 17.94 / 18.69 | 17.41 / 18.10
+FT | 17.20 / 17.92 | 17.04 / 17.69

E KẾT QUẢ SELF-ENSEMBLING BỔ SUNG
Chúng tôi trình bày kết quả self-ensembling bổ sung cho mô hình ngôn ngữ trên WikiText-103 trong Bảng 10, cho các mức độ thưa thớt cao hơn – với 6 và 8 cửa sổ chúng ta lấy mẫu ít nhất 16.7% và 12.5% attention, tương ứng. Chúng ta thấy kết quả tương tự như được trình bày trong phần chính với self-ensembling cải thiện đáng kể so với các đối tác renormalization của chúng. Điều này cho thấy rằng self-ensembling có thể cải thiện hiệu suất ngay cả với mức độ thưa thớt cao.
