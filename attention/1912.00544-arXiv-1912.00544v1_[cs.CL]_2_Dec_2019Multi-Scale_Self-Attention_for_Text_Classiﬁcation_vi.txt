# 1912.00544.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1912.00544.pdf
# Kích thước tệp: 156426 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:1912.00544v1  [cs.CL]  2 Dec 2019

Tự Chú ý Đa Thang Đo cho Phân Loại Văn Bản

Qipeng Guo‡∗, Xipeng Qiu‡ †, Pengfei Liu‡, Xiangyang Xue‡, Zheng Zhang§
‡Phòng thí nghiệm Chính Xử lý Thông tin Thông minh Shanghai, Đại học Fudan
‡Trường Khoa học Máy tính, Đại học Fudan
§Phòng thí nghiệm AWS Shanghai AI
§Đại học New York Shanghai
{qpguo16, xpqiu, pﬂiu14, xyxue }@fudan.edu.cn, zz@nyu.edu

Tóm tắt

Trong bài báo này, chúng tôi giới thiệu kiến thức tiền đề, cấu trúc đa thang đo, vào các mô-đun tự chú ý. Chúng tôi đề xuất một Transformer Đa Thang Đo sử dụng tự chú ý đa đầu đa thang đo để thu thập các đặc trưng từ các thang đo khác nhau. Dựa trên quan điểm ngôn ngữ học và phân tích của Transformer được huấn luyện trước (BERT) trên một tập dữ liệu khổng lồ, chúng tôi tiếp tục thiết kế một chiến lược để kiểm soát phân bố thang đo cho mỗi lớp. Kết quả của ba loại nhiệm vụ khác nhau (21 bộ dữ liệu) cho thấy Transformer Đa Thang Đo của chúng tôi vượt trội hơn Transformer tiêu chuẩn một cách nhất quán và đáng kể trên các bộ dữ liệu có kích thước nhỏ và vừa phải.

Giới thiệu

Cơ chế Tự Chú ý được sử dụng rộng rãi trong các nhiệm vụ phân loại văn bản, và các mô hình dựa trên cơ chế tự chú ý như Transformer (Vaswani et al. 2017), BERT (Devlin et al. 2018) đạt được nhiều kết quả thú vị trên các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP), chẳng hạn như dịch máy, mô hình hóa ngôn ngữ (Dai et al. 2019), và phân loại văn bản.

Gần đây, Radford et al. (2018) chỉ ra điểm yếu của các mô-đun tự chú ý, đặc biệt là hiệu suất kém trên các bộ dữ liệu có kích thước nhỏ và vừa phải. Mặc dù việc huấn luyện trước trên tập dữ liệu khổng lồ có thể giúp điều này, chúng tôi tin rằng lý do cơ bản là mô-đun tự chú ý thiếu thiên vị quy nạp phù hợp, vì vậy quá trình học phụ thuộc nhiều vào dữ liệu huấn luyện. Rất khó để học một mô hình với khả năng tổng quát hóa tốt từ đầu trên một tập huấn luyện hạn chế mà không có thiên vị quy nạp tốt.

Cấu trúc Đa Thang Đo được sử dụng rộng rãi trong thị giác máy tính (CV), NLP, và các lĩnh vực xử lý tín hiệu. Nó có thể giúp mô hình thu thập các mẫu ở các thang đo khác nhau và trích xuất các đặc trưng mạnh mẽ. Cụ thể với lĩnh vực NLP, một cách phổ biến để triển khai đa thang đo là cấu trúc phân cấp, chẳng hạn như mạng nơ-ron tích chập (CNN) (Kalchbrenner, Grefenstette, and Blunsom 2014), mạng nơ-ron hồi tiếp đa thang đo (RNN) (Chung, Ahn, and Bengio 2016) mạng nơ-ron có cấu trúc cây (Socher et al. 2013; Tai, Socher, and Manning 2015) và chú ý phân cấp (Yang et al. 2016). Nguyên lý đằng sau các mô hình này là đặc tính của ngôn ngữ: đặc trưng cấp cao là sự kết hợp của các thuật ngữ cấp thấp. Với cấu trúc phân cấp, các mô hình này thu thập các tổ hợp cục bộ ở các lớp thấp hơn và tổ hợp phi cục bộ ở các lớp cao. Sự phân chia công việc này làm cho mô hình ít đói dữ liệu hơn.

Tuy nhiên, đối với các mô-đun tự chú ý, không có hạn chế về thiên vị tổ hợp. Các phụ thuộc giữa các từ hoàn toàn được điều khiển bởi dữ liệu mà không có bất kỳ tiền đề nào, dẫn đến việc dễ dàng quá khớp trên các bộ dữ liệu có kích thước nhỏ hoặc vừa phải.

Trong bài báo này, chúng tôi đề xuất một tự chú ý đa đầu đa thang đo (MSMSA), trong đó mỗi đầu chú ý có một thang đo biến đổi. Thang đo của một đầu hạn chế khu vực làm việc của tự chú ý. Một cách trực quan, một thang đo lớn làm cho đặc trưng liên quan đến nhiều thông tin ngữ cảnh hơn và trở nên mượt mà hơn. Một thang đo nhỏ khăng khăng với thiên vị cục bộ và khuyến khích các đặc trưng trở nên nổi bật và sắc nét. Dựa trên MSMSA, chúng tôi tiếp tục đề xuất transformer đa thang đo, bao gồm nhiều lớp MSMSA. Khác với đa thang đo trong cấu trúc phân cấp, mỗi lớp của Transformer đa thang đo bao gồm nhiều đầu chú ý với nhiều thang đo, điều này mang lại khả năng thu thập các đặc trưng đa thang đo trong một lớp duy nhất.

Đóng góp của bài báo này là:
• Chúng tôi giới thiệu cấu trúc đa thang đo vào khung tự chú ý, mô hình được đề xuất Transformer Đa Thang Đo có thể trích xuất đặc trưng từ các thang đo khác nhau.
• Được truyền cảm hứng bởi cấu trúc phân cấp của ngôn ngữ, chúng tôi tiếp tục phát triển một chiến lược đơn giản để kiểm soát phân bố thang đo cho các lớp khác nhau. Dựa trên kết quả thực nghiệm trên các nhiệm vụ thực tế và phân tích từ BERT, chúng tôi đề xuất sử dụng nhiều đầu chú ý thang đo nhỏ hơn trong các lớp nông và một lựa chọn cân bằng cho các thang đo khác nhau trong các lớp sâu.
• Khối xây dựng của Transformer Đa Thang Đo, tự chú ý đa đầu đa thang đo cung cấp một cách linh hoạt để giới thiệu thiên vị thang đo (cục bộ hoặc toàn cục), và nó là một sự thay thế cho tự chú ý đa đầu và mạng truyền xuôi theo vị trí.
• Kết quả trên ba nhiệm vụ (21 bộ dữ liệu) cho thấy Transformer Đa Thang Đo của chúng tôi vượt trội hơn Transformer tiêu chuẩn

--- TRANG 2 ---
một cách nhất quán và đáng kể trên các bộ dữ liệu có kích thước nhỏ và vừa phải.

Nền tảng

Tự chú ý và kiến trúc mở rộng của nó, Transformer, đạt được nhiều kết quả tốt trên các nhiệm vụ NLP. Thay vì sử dụng đơn vị CNN hoặc RNN để mô hình hóa tương tác giữa các từ khác nhau, Transformer đạt được tương tác cặp đôi thông qua cơ chế chú ý.

Tự chú ý Đa Đầu Thành phần chính của Transformer là chú ý tích vô hướng đa đầu, có thể được chính thức hóa như sau. Cho một chuỗi các vectơ H∈RN×D, trong đó N là độ dài của chuỗi và D là chiều của vectơ. Khi thực hiện tự chú ý đa đầu, mô-đun chiếu H vào ba ma trận: truy vấn Q, khóa K và giá trị V. Ba ma trận này sẽ được phân tách thêm thành N′ không gian con tương ứng với N′ đầu và mỗi đầu có D′ đơn vị.

MSA(H) = [ head1,···,headN′]WO, (1)
headi=softmax(QiKT
i√
D′)Vi, (2)
Q=HWQ,K=HWK,V=HWV, (3)

trong đó MSA(·) đại diện cho Tự Chú ý Đa Đầu, và WQ,WK,WV,WO là các tham số có thể học được.

Transformer Mỗi lớp trong Transformer bao gồm một tự chú ý đa đầu và một lớp FFN (còn được gọi là Mạng Truyền Xuôi Theo Vị trí trong Vaswani et al. (2017)). Trạng thái ẩn của lớp l+ 1, Hl+1 có thể được tính như sau.

Zl=norm(Hl+MSA(Hl)), (4)
Hl+1=norm(Zl+FFN(Zl)), (5)

trong đó norm(·) có nghĩa là chuẩn hóa lớp (Ba, Kiros, and Hinton 2016). Ngoài ra, Transformer tăng cường các đặc trưng đầu vào bằng cách thêm một nhúng vị trí vì tự chú ý không thể thu thập thông tin vị trí một mình.

Mặc dù hiệu quả trên dịch máy và mô hình hóa ngôn ngữ, Transformer thường thất bại trên nhiệm vụ với các bộ dữ liệu có kích thước vừa phải do thiếu thiên vị quy nạp.

Mô hình

Trong tự chú ý đa đầu, mỗi đầu thu thập các tương tác cặp đôi giữa các từ trong không gian đặc trưng khác nhau. Mỗi đầu có cùng thang đo của độ dài câu.

Tự Chú ý Nhận biết Thang đo Để giới thiệu khái niệm đa thang đo vào cơ chế tự chú ý, chúng tôi sử dụng một cách đơn giản để tăng cường tự chú ý thông thường, được đặt tên là Tự Chú ý Nhận biết Thang đo (SASA) tương đương với tự chú ý bị hạn chế được đề xuất trong Vaswani et al. (2017) nhưng sử dụng kích thước cửa sổ động.

[THIS IS FIGURE: Sơ đồ của Tự Chú ý Đa Đầu Đa Thang Đo, chúng ta có thể thấy ba đầu tương ứng với ba thang đo khác nhau trong hình. Hộp màu xanh dương, xanh lá, đỏ minh họa thang đo ω= 1,ω= 3,ω= 5, tương ứng.]

Cho một chuỗi các vectơ H∈RN×D với độ dài N. SASA có một tham số ω là một số không đổi hoặc một tỷ lệ theo độ dài chuỗi để kiểm soát phạm vi làm việc của nó. Một đầu chú ý có thể được tính như

head(H,ω)i,j=SM(QijCij(K,ω)T
√
D)Cij(V,ω),(6)
Cij(x,ω) = [xi,j−ω,...,xi,j+ω], (7)
Q=HWQ,K=HWK,V=HWV, (8)

trong đó i chỉ đầu thứ i và j có nghĩa là vị trí thứ j. SM đại diện cho hàm "Softmax", C là một hàm để trích xuất ngữ cảnh cho một vị trí cho trước. WQ,WK,WV là các tham số có thể học được. Thang đo của một đầu có thể là một biến như N/2 hoặc một số cố định như 3.

Tự Chú ý Đa Đầu Đa Thang Đo Với SASA, chúng ta có thể triển khai một Tự Chú ý Đa Đầu Đa Thang Đo (MSMSA) với nhiều đầu nhận biết thang đo. Mỗi đầu làm việc trên các thang đo khác nhau. Đối với tự chú ý N′ đầu, MSMSA với thang đo Ω = [ω1,···,ωN′] là

MSMSA(H,Ω) = [ head1(H,ω1);···;
headN′(H,ωN′)]WO, (9)

trong đó WO là một ma trận tham số.

So với tự chú ý đa đầu vanilla, biến Ω kiểm soát khu vực được chú ý và làm cho các đầu khác nhau có các nhiệm vụ khác nhau.

Transformer Đa Thang Đo Với MSMSA, chúng ta có thể xây dựng transformer đa thang đo (MS-Transformer. Tóm tắt, MS-Trans). Ngoài việc sử dụng MSMSA để thay thế tự chú ý đa đầu vanilla, chúng tôi cũng loại bỏ FFN (xem Eq. (5)) vì nó có thể được xem như một tự chú ý với thang đo ω= 1 cộng với một hàm kích hoạt phi tuyến. Vì MSMSA giới thiệu tính cục bộ vào mô hình, MSMSA với thang đo nhỏ có thể là một thay thế cho nhúng vị trí. Do đó, transformer đa thang đo có thể được chính thức hóa như sau.

Hl+1= norm( Hl+ReLU(MSMSA( Hl),Ωl)) (10)

--- TRANG 3 ---
trong đó l là chỉ số lớp.

Trong công việc này, chúng tôi giới hạn lựa chọn kích thước thang đo trong các số không đổi {1,3,···} hoặc các biến phụ thuộc vào độ dài chuỗi {N/16,N/8,···}. Và chúng tôi buộc kích thước thang đo phải là một số lẻ.

So với các mô hình đa thang đo phân cấp, transformer đa thang đo cho phép các đầu chú ý trong một lớp có các thang đo tầm nhìn khác nhau, nó là một phiên bản "mềm" của việc xem các lớp khác nhau như các thang đo khác nhau.

Nút phân loại Chúng tôi cũng tìm thấy việc thêm một nút đặc biệt ở đầu mỗi chuỗi và kết nối trực tiếp nó với biểu diễn câu cuối cùng có thể cải thiện hiệu suất. Kỹ thuật này được giới thiệu trong BERT (Devlin et al. 2018), được gọi là "[CLS]". Và nó cũng tương tự như "nút chuyển tiếp" trong Guo et al. (2019). Khác với chúng, chúng tôi kết hợp nút "[CLS]" và đặc trưng từ việc áp dụng max-pooling trên tất cả các nút trong lớp cuối cùng để đại diện cho câu. Không có sự khác biệt giữa nút "[CLS]" và các token khác trong chuỗi đầu vào ngoại trừ nó có thể đóng góp trực tiếp vào biểu diễn cuối cùng.

Tìm kiếm Thang đo Chú ý Hiệu quả

Transformer đa thang đo là một mô-đun linh hoạt trong đó mỗi lớp có thể có các đầu chú ý đa thang đo. Do đó, một yếu tố quan trọng là cách thiết kế phân bố thang đo cho mỗi lớp.

Đa Thang đo Phân cấp hay Đa Thang đo Linh hoạt

Một cách đơn giản để triển khai trích xuất đặc trưng đa thang đo là theo cách phân cấp, xếp chồng nhiều lớp với các đầu thang đo nhỏ. Các lớp thấp hơn thu thập các đặc trưng cục bộ và các lớp cao hơn thu thập các đặc trưng phi cục bộ.

Để xác minh khả năng của đa thang đo phân cấp, chúng tôi thiết kế một nhiệm vụ mô phỏng rất đơn giản được gọi là tổng gương. Cho một chuỗi A={a1,...,aN}, a∈Rd và được lấy từ U(0,1). Mục tiêu là ∑K i=1ai⊙aN−i+1, trong đó K là một số nguyên cố định nhỏ hơn độ dài chuỗi N và ⊙ có nghĩa là tích Hadamard. Độ dài đường dẫn phụ thuộc tối thiểu là N−K, chúng tôi sử dụng nhiệm vụ này để kiểm tra khả năng của các mô hình trong việc thu thập các phụ thuộc tầm xa. Cả tập huấn luyện và tập kiểm tra đều được tạo ngẫu nhiên và chúng có mỗi tập 200k mẫu.

Chúng ta có thể giả định kích thước của tập huấn luyện đủ để huấn luyện các mô hình này một cách kỹ lưỡng.

Chúng tôi sử dụng ba cài đặt khác nhau của MS-Trans.
1. MS-Trans-Hier-S: MS-Transformer với hai lớp, và mỗi lớp có 10 đầu với thang đo nhỏ ω= 3.
2. MS-Trans-deepHier-S: MS-Transformer với sáu lớp, và mỗi lớp có 10 đầu với thang đo nhỏ ω= 3.
3. MS-Trans-Flex: MS-Transformer với hai lớp, và mỗi lớp có 10 đầu với đa thang đo linh hoạt ω= 3,N/16,N/8,N/4,N/2. Mỗi thang đo có hai đầu.

Như được hiển thị trong Hình-2, Transformer đạt được kết quả tốt nhất, và MS-Trans-Flex theo sau. Mặc dù Transformer có tiềm năng cao nhất để thu thập các phụ thuộc tầm xa, nó yêu cầu mẫu huấn luyện lớn. Trong khi đó,

[THIS IS FIGURE: Nhiệm vụ Tổng Gương. Đường cong MSE theo số hợp lệ K với độ dài chuỗi n= 100 và chiều của vectơ đầu vào d= 50.]

mô hình của chúng tôi cân bằng vấn đề đói dữ liệu và khả năng thu thập các phụ thuộc tầm xa. Dựa trên so sánh của MS-Trans-Hier-S và MS-Trans-deepHier-S, chúng ta có thể thấy sự cải thiện của các lớp bổ sung tương đối nhỏ. Theo thí nghiệm tổng hợp và hiệu suất trên các nhiệm vụ thực tế (xem. Phần-), chúng tôi nghĩ các đầu thang đo lớn là cần thiết cho các lớp thấp hơn và các đầu thang đo nhỏ xếp chồng khó thu thập các phụ thuộc tầm xa. Trong trường hợp này, một tiền đề tốt nên chứa cả thang đo nhỏ và thang đo lớn.

Phân bố Thang đo của Các Lớp Khác nhau

Vì đa thang đo là cần thiết cho mỗi lớp, câu hỏi thứ hai là làm thế nào để thiết kế tỷ lệ của các thang đo khác nhau cho mỗi lớp? Chúng tôi nghĩ mỗi lớp có thể có sở thích riêng cho phân bố thang đo. Từ quan điểm ngôn ngữ học, một giả định trực quan có thể là: lớp cao hơn có xác suất cao hơn cho các đầu thang đo lớn và lớp nông có xác suất cao hơn cho các đầu thang đo nhỏ.

Để tìm kiếm bằng chứng thực nghiệm, chúng tôi khảo sát một số trường hợp điển hình và phân tích hành vi tương ứng trong một mô hình điều khiển dữ liệu, BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al. 2018).

Phân tích Tương tự từ BERT BERT là một Transformer được huấn luyện trước trên dữ liệu quy mô lớn, đã chứng minh sức mạnh của nó trên nhiều nhiệm vụ NLP và nó có khả năng tổng quát hóa tốt. Vì BERT dựa trên Transformer, nó không được hướng dẫn bởi kiến thức tiền đề, kiến thức của chúng được học từ dữ liệu. Chúng tôi khảo sát hành vi của BERT để xem liệu nó có phù hợp với giả định ngôn ngữ học không. Có hai khía cạnh chúng tôi muốn nghiên cứu, thứ nhất là thang đo làm việc của các đầu chú ý của mỗi lớp trong BERT. Thứ hai là sự khác biệt giữa phân bố thang đo của các lớp khác nhau, đặc biệt là sở thích của các mối quan hệ cục bộ và toàn cục. Để khảo sát các hành vi này, chúng tôi đầu tiên chạy BERT trên nhiều câu tự nhiên và chọn kích hoạt cao nhất

--- TRANG 4 ---
[THIS IS FIGURE: Two graphs showing attention distance distributions. Graph (a) shows three heads in first layer, Graph (b) shows different layers.]

Hình 3: Trực quan hóa của BERT. (a) Phân bố khoảng cách chú ý của ba đầu trong lớp đầu tiên. Đầu màu đỏ chỉ quan tâm đến mẫu cục bộ và đầu màu xanh nhìn đều vào các khoảng cách khác nhau. (b) Phân bố khoảng cách chú ý của các lớp khác nhau. Lớp nông thích kích thước thang đo nhỏ và có xu hướng kích thước thang đo lớn từ từ khi lớp trở nên sâu hơn. Ngay cả trong lớp cuối cùng, các mẫu cục bộ vẫn chiếm một phần trăm lớn. Chúng tôi cắt khoảng cách ở N/2 để trực quan hóa tốt hơn. Hình đầy đủ có thể được tìm thấy trong Phụ lục.

của bản đồ chú ý như cạnh chú ý. Sau đó chúng tôi ghi lại khoảng cách tương đối của các cạnh chú ý này.

Trong công việc này, chúng tôi thu được dữ liệu từ việc chạy BERT trên bộ dữ liệu CoNLL03 (xem Tab-1).

Chúng tôi đầu tiên vẽ Hình-3a để quan sát hành vi của các đầu trong cùng một lớp. Chúng tôi chọn ba đầu, và sự khác biệt của chúng là đáng kể. Như được hiển thị trong hình, "head-2" tập trung vào một khoảng cách nhất định với thang đo nhỏ, và "head-1" bao phủ tất cả các khoảng cách. Có một sự phân chia công việc rõ ràng của các đầu này, và một lớp có thể có cả tầm nhìn cục bộ và toàn cục thông qua việc kết hợp các đặc trưng từ các đầu khác nhau.

Hình thứ hai Hình-3b cho thấy xu hướng của sở thích khoảng cách khi độ sâu của lớp tăng. Chúng ta có thể thấy mô hình di chuyển từ tầm nhìn cục bộ đến tầm nhìn toàn cục từ từ và các lớp nông có sự quan tâm mạnh mẽ đến các mẫu cục bộ.

Trực quan hóa của BERT phù hợp với thiết kế của Transformer Đa Thang Đo, quan sát đầu tiên tương ứng với thiết kế của tự chú ý đa đầu đa thang đo yêu cầu các đầu khác nhau tập trung vào các thang đo khác nhau, và quan sát thứ hai cung cấp một tham chiếu tốt về xu hướng phân bố thang đo qua các lớp. Sử dụng kiến thức như vậy có thể giảm đáng kể yêu cầu dữ liệu huấn luyện cho các mô hình giống Transformer.

Yếu tố Kiểm soát Phân bố Thang đo cho Các Lớp Khác nhau

Từ quan điểm ngôn ngữ học trực quan và bằng chứng thực nghiệm, chúng tôi thiết kế một yếu tố kiểm soát phân bố thang đo cho các lớp khác nhau của transformer đa thang đo.

Gọi L biểu thị số lớp trong transformer đa thang đo, |Ω| biểu thị số kích thước thang đo ứng viên, và n^l_k biểu thị số đầu cho lớp thứ l và kích thước thang đo thứ k. Số đầu n^l_k được tính bởi

z^l_k = {0 if l=L or k=|Ω|
         z^l_{k+1}+α if l≠L and k∈{0,···,|Ω|−1}  (11)

n^l=softmax(z^l)·N′  (12)

Trong các phương trình trên, chúng tôi giới thiệu một siêu tham số α để kiểm soát sự thay đổi của sở thích của kích thước thang đo cho mỗi lớp. Ví dụ, α= 0 có nghĩa là tất cả các lớp sử dụng cùng một chiến lược của kích thước thang đo, α >0 có nghĩa là sở thích của thang đo nhỏ hơn tăng với sự giảm của độ sâu lớp, và α <0 chỉ ra sở thích của thang đo lớn hơn tăng với sự giảm của độ sâu lớp. Như kết luận của việc phân tích BERT, chúng tôi tin rằng lớp sâu có một tầm nhìn cân bằng trên cả các mẫu cục bộ và toàn cục, vì vậy lớp trên cùng nên được đặt để nhìn tất cả các kích thước thang đo một cách đồng đều. Cụ thể hơn, khi α= 0.5 và N′= 10, ba lớp có n^l=1= {5,2,2,1,0}, n^l=2={4,2,2,1,1}, n^l=3={2,2,2,2,2}, nó đại diện cho lớp đầu tiên có 5 đầu với kích thước thang đo 1, 2 đầu với kích thước thang đo 3, 2 đầu với kích thước thang đo N/16 và 1 đầu với kích thước thang đo N/8.

Thí nghiệm

Chúng tôi đánh giá mô hình của chúng tôi trên 17 bộ dữ liệu phân loại văn bản, 3 bộ dữ liệu gán nhãn chuỗi và 1 bộ dữ liệu suy luận ngôn ngữ tự nhiên. Tất cả các thống kê có thể được tìm thấy trong Tab-1. Ngoài ra, chúng tôi sử dụng GloVe (Pennington, Socher, and Manning 2014) để khởi tạo nhúng từ và JMT (Hashimoto et al. 2017) cho các đặc trưng cấp ký tự. Trình tối ưu là Adam (Kingma and Ba 2014) và tốc độ học và tỷ lệ dropout được liệt kê trong Phụ lục.

Để tập trung vào so sánh giữa các thiết kế mô hình khác nhau, chúng tôi không liệt kê kết quả của các mô hình giống BERT vì tăng cường dữ liệu và huấn luyện trước là một hướng trực giao.

--- TRANG 5 ---
Bảng 1: Tổng quan về các bộ dữ liệu và siêu tham số của nó, "H DIM,α, head DIM" chỉ chiều của trạng thái ẩn, siêu tham số để kiểm soát phân bố thang đo, chiều của mỗi đầu, tương ứng. Các thang đo ứng viên là 1,3,N/16,N/8,N/4 cho các bộ dữ liệu SST,MTL-16,SNLI. Và chúng tôi sử dụng 1,3,5,7,9 cho các nhiệm vụ gán nhãn chuỗi. MTL-16† bao gồm 16 bộ dữ liệu, mỗi bộ có 1400/200/400 mẫu trong train/dev/test.

[THIS IS TABLE: Shows dataset information including Train, Dev, Test sizes, vocabulary size, hidden dimensions, alpha values, and head dimensions for various datasets like SST, MTL-16, PTB POS, CoNLL03, CoNLL2012 NER, and SNLI]

Bảng 2: Độ chính xác kiểm tra trên bộ dữ liệu SST.

[THIS IS TABLE: Shows test accuracy results for various models on SST dataset, with Multi-Scale Transformer achieving 51.9% accuracy]

Phân loại Văn bản

Các thí nghiệm Phân loại Văn bản được tiến hành trên bộ dữ liệu Stanford Sentiment Treebank(SST) (Socher et al. 2013) và MTL-16 (Liu, Qiu, and Huang 2017) bao gồm 16 bộ dữ liệu nhỏ trong các lĩnh vực khác nhau. Ngoài mô hình cơ sở chúng tôi giới thiệu trước đó, chúng tôi sử dụng một MLP(Multi-Layer Perceptron) hai lớp với hàm softmax làm bộ phân loại. Nó nhận đặc trưng từ việc áp dụng max-pooling trên lớp trên cùng cộng với nút phân loại.

Tab-2 và 3 đưa ra kết quả trên SST và MTL-16. Transformer Đa Thang Đo đạt được 1.5 và 3.56 điểm so với Transformer trên hai bộ dữ liệu này, tương ứng. Trong khi đó, Transformer Đa Thang Đo cũng đánh bại nhiều mô hình hiện có bao gồm CNN và RNN.

Vì độ dài câu trung bình của bộ dữ liệu MTL-16 tương đối lớn, chúng tôi cũng báo cáo kết quả hiệu suất trong Hình-4. Chúng tôi triển khai MS-Trans với Pytorch¹ và DGL(Wang et al. 2019). Transformer Đa Thang Đo đạt được gia tốc 6.5 lần so với Transformer trên bộ dữ liệu MTL-16 trung bình (độ dài câu trung bình bằng 109 token). Mức gia tốc tối đa đạt 10 lần (trung bình 201 token)

¹https://pytorch.org

Bảng 3: Độ chính xác kiểm tra trên các bộ dữ liệu MTL-16. "SLSTM" đề cập đến sentence-state LSTM (Zhang, Liu, and Song 2018).

[THIS IS TABLE: Shows test accuracy results across 16 different datasets comparing MS-Trans, Transformer, BiLSTM, and SLSTM models, with MS-Trans showing consistently better performance]

và Transformer Đa Thang Đo có thể đạt được gia tốc 1.8 lần trên các câu rất ngắn (trung bình 22 token).

Gán nhãn Chuỗi

Ngoài các nhiệm vụ sử dụng mô hình như một bộ mã hóa câu, chúng tôi cũng quan tâm đến hiệu quả của mô hình trên các nhiệm vụ gán nhãn chuỗi. Chúng tôi chọn nhiệm vụ gán nhãn Part-of-Speech (POS) và nhiệm vụ Nhận dạng Thực thể Có tên (NER) để xác minh mô hình của chúng tôi. Chúng tôi sử dụng ba bộ dữ liệu làm chuẩn: bộ dữ liệu gán nhãn POS Penn Treebank (PTB) (Marcus, Santorini, and Marcinkiewicz 1993), bộ dữ liệu NER CoNLL2003 (Sang and Meulder 2003), bộ dữ liệu NER CoNLL2012 (Pradhan et al. 2012).

Kết quả trong Tab-4 cho thấy Transformer Đa Thang Đo đánh bại

--- TRANG 6 ---
Bảng 4: Kết quả trên các nhiệm vụ gán nhãn chuỗi. Chúng tôi liệt kê "Kỹ thuật Nâng cao" ngoại trừ nhúng được huấn luyện trước (GloVe, Word2Vec, JMT) trong các cột. "Char" chỉ các đặc trưng cấp ký tự, nó cũng bao gồm Đặc trưng Viết hoa, Đặc trưng Từ điển, v.v. "CRF" có nghĩa là một lớp Conditional Random Field bổ sung.

[THIS IS TABLE: Shows results on sequence labeling tasks with columns for Model, Advanced Techniques (char/CRF), and performance metrics (POS PTB Acc, NER CoNLL2003 F1, CoNLL2012 F1)]

[THIS IS FIGURE: Bar chart showing test time per batch (batch size 128) comparing MS-Trans, Trans, and BiLSTM across IMDB, MR, and average datasets]

Hình 4: Thời gian kiểm tra mỗi lô (kích thước lô là 128) trên bộ dữ liệu có độ dài dài nhất (IMDB), bộ dữ liệu có độ dài ngắn nhất (MR), và trung bình trên 16 bộ dữ liệu trong MTL-16.

Transformer vanilla trên ba bộ dữ liệu gán nhãn chuỗi này, điều này bao gồm các kết quả khác được báo cáo ở trên. Nó cho thấy Transformer Đa Thang Đo có thể trích xuất các đặc trưng hữu ích cho mỗi vị trí cũng như vậy.

Suy luận Ngôn ngữ Tự nhiên

Suy luận Ngôn ngữ Tự nhiên (NLI) là một phân loại yêu cầu mô hình đánh giá mối quan hệ của hai câu từ ba ứng viên, "entailment", "contradiction", và "neutral". Chúng tôi sử dụng một chuẩn được sử dụng rộng rãi Stanford Natural Language Inference (SNLI) (Bowman et al. 2015) để khảo sát khả năng của mô hình trong việc mã hóa câu, chúng tôi so sánh mô hình của chúng tôi với các mô hình dựa trên vectơ câu. Khác với bộ phân loại trong nhiệm vụ phân loại văn bản, chúng tôi theo công việc trước đó (Bowman et al. 2016) để sử dụng một bộ phân loại MLP hai lớp nhận concat (r1,r2,|r1−r2|,r1−r2) làm đầu vào, trong đó r1,r2 là biểu diễn của hai câu và bằng đặc trưng được sử dụng trong nhiệm vụ phân loại văn bản.

Bảng 5: Độ chính xác kiểm tra trên bộ dữ liệu SNLI cho các mô hình dựa trên vectơ câu.

[THIS IS TABLE: Shows test accuracy results on SNLI dataset for various sentence vector-based models, with Multi-Scale Transformer achieving 85.9% accuracy]

Như được hiển thị trong Tab-5, Transformer Đa Thang Đo vượt trội hơn Transformer và hầu hết các mô hình cổ điển, và kết quả có thể so sánh với state-of-the-art. Số được báo cáo của Transformer được thu được với việc lựa chọn siêu tham số heuristic, chúng tôi sử dụng một Transformer ba lớp với dropout nặng và weight decay. Và vẫn còn một khoảng cách lớn so với Transformer Đa Thang Đo. So sánh này cũng chỉ ra dữ liệu huấn luyện có kích thước vừa phải (SNLI có 550k mẫu huấn luyện) không thể thay thế tính hữu ích của kiến thức tiền đề.

Phân tích

Ảnh hưởng của Phân bố Thang đo Như chúng tôi giới thiệu trong Eq. (11), chúng tôi kiểm soát phân bố thang đo qua các lớp bằng một siêu tham số α. Trong phần này, chúng tôi đưa ra so sánh về việc sử dụng α khác nhau, trong đó giá trị dương có nghĩa là thiên vị cục bộ tăng với sự giảm của độ sâu lớp và giá trị âm có nghĩa là thiên vị toàn cục tăng với sự giảm của độ sâu lớp.

Như được hiển thị trong phần trên của Tab-6, thiên vị cục bộ trong các lớp nông là một yếu tố quan trọng để đạt được hiệu suất tốt, và một α dương phù hợp đạt được kết quả tốt nhất. Ngược lại,

--- TRANG 7 ---
Bảng 6: Phân tích các phân bố thang đo khác nhau trên tập kiểm tra SNLI. Phần trên cho thấy ảnh hưởng của siêu tham số α thay đổi phân bố thang đo qua các lớp. Năm ứng viên của kích thước thang đo là 1,3,N/16,N/8,N/4, tương ứng. Phần dưới liệt kê hiệu suất của các mô hình thang đo đơn sử dụng thang đo cố định cho toàn bộ mô hình.

[THIS IS TABLE: Two tables showing multi-scale and single-scale results with various parameters and accuracy scores]

tất cả các giá trị âm làm hại hiệu suất, điều đó có nghĩa là quá nhiều thiên vị toàn cục trong các lớp nông có thể dẫn mô hình đến hướng sai. Quan sát của thí nghiệm này phù hợp với trực giác của chúng tôi, đặc trưng cấp cao là sự kết hợp của các thuật ngữ cấp thấp.

Đa Thang đo so với Thang đo Đơn Như chúng tôi đã khẳng định trước đó, Transformer Đa Thang Đo có thể thu thập kiến thức ở các thang đo khác nhau ở mỗi lớp. Do đó, một câu hỏi đơn giản cần được đánh giá là liệu mô hình đa thang đo có vượt trội hơn mô hình thang đo đơn hay không. Để trả lời câu hỏi này, chúng tôi so sánh Transformer Đa Thang Đo với một số mô hình thang đo đơn. Mô hình F,G,H,I có cùng số lớp và đầu chú ý với Transformer Đa thang đo, nhưng thang đo của chúng được cố định.

Kết quả trong phần dưới của Tab-6 tiết lộ giá trị của Transformer Đa Thang Đo, nó đạt được cải thiện 1.6 điểm so với mô hình thang đo đơn tốt nhất. Và kết quả này cũng hỗ trợ rằng thiên vị cục bộ là một thiên vị quy nạp quan trọng cho nhiệm vụ NLP.

Công việc Liên quan

Các mô hình đa thang đo điển hình Cấu trúc đa thang đo đã được sử dụng trong nhiều mô hình NLP, nó có thể được triển khai theo nhiều cách khác nhau. Chẳng hạn như các lớp xếp chồng (Kalchbrenner, Grefenstette, and Blunsom 2014; Kim 2014), cấu trúc cây (Socher et al. 2013; Tai, Socher, and Manning 2015; Zhu, Sobhani, and Guo 2015), thang thời gian phân cấp (El Hihi and Bengio 1995; Chung, Ahn, and Bengio 2016), cổng theo lớp (Chung et al. 2015). Vì những mô hình này được xây dựng trên các mô-đun như RNN và CNN, thể hiện thiên vị cục bộ nội tại theo thiết kế, tinh thần chung của việc giới thiệu đa thang đo là để cho phép giao tiếp tầm xa. Ngược lại, Transformer cho phép giao tiếp tầm xa, vì vậy chúng tôi muốn đa thang đo mang lại thiên vị cục bộ.

Transformer với thiên vị quy nạp bổ sung Công việc này không phải là nỗ lực đầu tiên giới thiệu thiên vị quy nạp vào Transformer.

Shaw, Uszkoreit, and Vaswani (2018) đề xuất Transformer nên quan tâm đến khoảng cách tương đối giữa các token thay vì vị trí tuyệt đối trong chuỗi. Thông tin về khoảng cách tương đối có thể được thu được bằng cách nhìn đa thang đo của cùng một vị trí, vì vậy mô hình của chúng tôi có thể nhận biết khoảng cách tương đối nếu sử dụng đủ thang đo.

Li et al. (2018) đề xuất một quy tắc hóa về việc tăng cường sự đa dạng của các đầu chú ý. Tự chú ý đa đầu đa thang đo của chúng tôi có thể tạo ra sự phân chia công việc tốt của các đầu thông qua việc hạn chế chúng trong các thang đo khác nhau.

Yang et al. (2018a) và Yang et al. (2018b) cũng giới thiệu thiên vị cục bộ vào Transformer.

Khác với các mô hình trên, chúng tôi tập trung vào việc nhập khái niệm đa thang đo vào tự chú ý. Trong khi đó, các mô hình của họ sử dụng cấu trúc thang đo đơn. Kết quả thực nghiệm của chúng tôi đã chứng minh hiệu quả của cơ chế đa thang đo.

Kết luận

Trong công việc này, chúng tôi trình bày Tự Chú ý Đa Thang Đo và Transformer Đa Thang Đo kết hợp kiến thức tiền đề của đa thang đo và cơ chế tự chú ý. Kết quả là, nó có khả năng trích xuất các đặc trưng phong phú và mạnh mẽ từ các thang đo khác nhau. Chúng tôi so sánh mô hình của chúng tôi với Transformer vanilla trên ba nhiệm vụ thực tế (21 bộ dữ liệu). Kết quả cho thấy đề xuất của chúng tôi vượt trội hơn Transformer vanilla một cách nhất quán và đạt được kết quả có thể so sánh với các mô hình state-of-the-art.

Lời cảm ơn

Công việc này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung Quốc (Số 2018YFC0831103), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 61672162), Dự án Trọng điểm Khoa học và Công nghệ Thành phố Thượng Hải (Số 2018SHZDZX01) và ZJLab.

Tài liệu tham khảo

[Content continues with references in the same format as shown in the original image...]

--- TRANG 8 ---
[References section continues with full bibliographic entries as shown in the original]