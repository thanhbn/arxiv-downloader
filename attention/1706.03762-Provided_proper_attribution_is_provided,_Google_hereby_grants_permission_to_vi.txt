# 1706.03762.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1706.03762.pdf
# Kích thước tệp: 2215244 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Với điều kiện có ghi nhận đúng đắn, Google cấp phép tái tạo các bảng và hình ảnh trong bài báo này chỉ để sử dụng trong các công trình báo chí hoặc học thuật.
Attention Is All You Need
Ashish Vaswani∗
Google Brain
avaswani@google.comNoam Shazeer∗
Google Brain
noam@google.comNiki Parmar∗
Google Research
nikip@google.comJakob Uszkoreit∗
Google Research
usz@google.com
Llion Jones∗
Google Research
llion@google.comAidan N. Gomez∗ †
University of Toronto
aidan@cs.toronto.eduŁukasz Kaiser∗
Google Brain
lukaszkaiser@google.com
Illia Polosukhin∗ ‡
illia.polosukhin@gmail.com
Tóm tắt
Các mô hình chuyển đổi chuỗi chủ đạo dựa trên các mạng nơ-ron hồi quy hoặc tích chập phức tạp bao gồm một bộ mã hóa và một bộ giải mã. Các mô hình hoạt động tốt nhất cũng kết nối bộ mã hóa và bộ giải mã thông qua một cơ chế attention. Chúng tôi đề xuất một kiến trúc mạng mới đơn giản, Transformer, chỉ dựa trên cơ chế attention, loại bỏ hoàn toàn tính hồi quy và tích chập. Các thí nghiệm trên hai nhiệm vụ dịch máy cho thấy các mô hình này vượt trội về chất lượng đồng thời có thể song song hóa nhiều hơn và yêu cầu ít thời gian huấn luyện hơn đáng kể. Mô hình của chúng tôi đạt 28.4 BLEU trên nhiệm vụ dịch WMT 2014 tiếng Anh-Đức, cải thiện so với kết quả tốt nhất hiện tại, bao gồm cả các tập hợp, hơn 2 BLEU. Trên nhiệm vụ dịch WMT 2014 tiếng Anh-Pháp, mô hình của chúng tôi thiết lập điểm số BLEU tiên tiến nhất cho mô hình đơn mới là 41.8 sau khi huấn luyện trong 3.5 ngày trên tám GPU, một phần nhỏ của chi phí huấn luyện so với các mô hình tốt nhất từ tài liệu. Chúng tôi chỉ ra rằng Transformer tổng quát hóa tốt cho các nhiệm vụ khác bằng cách áp dụng thành công cho việc phân tích cú pháp thành phần tiếng Anh với cả dữ liệu huấn luyện lớn và hạn chế.
∗Đóng góp ngang nhau. Thứ tự liệt kê là ngẫu nhiên. Jakob đề xuất thay thế RNN bằng self-attention và bắt đầu nỗ lực đánh giá ý tưởng này. Ashish, cùng với Illia, thiết kế và triển khai các mô hình Transformer đầu tiên và đã tham gia quan trọng trong mọi khía cạnh của công việc này. Noam đề xuất scaled dot-product attention, multi-head attention và biểu diễn vị trí không tham số và trở thành người khác tham gia vào gần như mọi chi tiết. Niki thiết kế, triển khai, điều chỉnh và đánh giá vô số biến thể mô hình trong codebase gốc của chúng tôi và tensor2tensor. Llion cũng thử nghiệm với các biến thể mô hình mới lạ, chịu trách nhiệm cho codebase ban đầu của chúng tôi, và suy luận hiệu quả cùng trực quan hóa. Lukasz và Aidan đã dành vô số ngày dài thiết kế các phần khác nhau và triển khai tensor2tensor, thay thế codebase trước đó của chúng tôi, cải thiện kết quả rất nhiều và tăng tốc nghiên cứu của chúng tôi đáng kể.
†Công việc được thực hiện tại Google Brain.
‡Công việc được thực hiện tại Google Research.
Hội nghị thứ 31 về Hệ thống Xử lý Thông tin Nơ-ron (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023

--- TRANG 2 ---
1 Giới thiệu
Các mạng nơ-ron hồi quy, bộ nhớ ngắn hạn dài [13] và mạng nơ-ron hồi quy có cổng [7] nói riêng, đã được xác lập vững chắc như các phương pháp tiên tiến trong mô hình hóa chuỗi và các bài toán chuyển đổi như mô hình hóa ngôn ngữ và dịch máy [35,2,5]. Nhiều nỗ lực đã tiếp tục đẩy ranh giới của các mô hình ngôn ngữ hồi quy và kiến trúc bộ mã hóa-giải mã [38, 24, 15].
Các mô hình hồi quy thường phân tích tính toán dọc theo các vị trí ký hiệu của chuỗi đầu vào và đầu ra. Căn chỉnh các vị trí với các bước trong thời gian tính toán, chúng tạo ra một chuỗi các trạng thái ẩn ht, như một hàm của trạng thái ẩn trước đó ht−1 và đầu vào cho vị trí t. Bản chất tuần tự vốn có này ngăn cản song song hóa trong các ví dụ huấn luyện, điều này trở nên quan trọng ở độ dài chuỗi lớn hơn, vì các ràng buộc bộ nhớ hạn chế việc gộp batch qua các ví dụ. Công việc gần đây đã đạt được cải thiện đáng kể trong hiệu quả tính toán thông qua các thủ thuật phân tích [21] và tính toán có điều kiện [32], đồng thời cũng cải thiện hiệu suất mô hình trong trường hợp sau. Tuy nhiên, ràng buộc cơ bản của tính toán tuần tự vẫn còn.
Các cơ chế attention đã trở thành một phần không thể thiếu của các mô hình mô hình hóa chuỗi và chuyển đổi hấp dẫn trong các nhiệm vụ khác nhau, cho phép mô hình hóa các phụ thuộc mà không quan tâm đến khoảng cách của chúng trong chuỗi đầu vào hoặc đầu ra [2,19]. Tuy nhiên, trong tất cả trừ một vài trường hợp [27], các cơ chế attention như vậy được sử dụng cùng với một mạng hồi quy.
Trong công việc này, chúng tôi đề xuất Transformer, một kiến trúc mô hình tránh tính hồi quy và thay vào đó hoàn toàn dựa vào cơ chế attention để rút ra các phụ thuộc toàn cục giữa đầu vào và đầu ra. Transformer cho phép song song hóa đáng kể hơn và có thể đạt được một trạng thái tiên tiến mới về chất lượng dịch sau khi được huấn luyện chỉ mười hai giờ trên tám GPU P100.
2 Bối cảnh
Mục tiêu giảm tính toán tuần tự cũng tạo nên nền tảng của Extended Neural GPU [16], ByteNet [18] và ConvS2S [9], tất cả đều sử dụng các mạng nơ-ron tích chập như khối xây dựng cơ bản, tính toán các biểu diễn ẩn song song cho tất cả các vị trí đầu vào và đầu ra. Trong các mô hình này, số lượng phép toán cần thiết để liên kết tín hiệu từ hai vị trí đầu vào hoặc đầu ra tùy ý tăng theo khoảng cách giữa các vị trí, tuyến tính cho ConvS2S và logarit cho ByteNet. Điều này làm cho việc học các phụ thuộc giữa các vị trí xa khó khăn hơn [12]. Trong Transformer, điều này được giảm xuống một số phép toán hằng số, mặc dù với chi phí là độ phân giải hiệu quả giảm do trung bình các vị trí trọng số attention, một hiệu ứng mà chúng tôi chống lại bằng Multi-Head Attention như được mô tả trong phần 3.2.
Self-attention, đôi khi được gọi là intra-attention là một cơ chế attention liên kết các vị trí khác nhau của một chuỗi đơn để tính toán một biểu diễn của chuỗi. Self-attention đã được sử dụng thành công trong nhiều nhiệm vụ khác nhau bao gồm đọc hiểu, tóm tắt trừu tượng, suy luận văn bản và học biểu diễn câu độc lập với nhiệm vụ [4, 27, 28, 22].
Các mạng bộ nhớ đầu cuối dựa trên cơ chế attention hồi quy thay vì tính hồi quy căn chỉnh chuỗi và đã được chứng minh hoạt động tốt trên các nhiệm vụ trả lời câu hỏi ngôn ngữ đơn giản và mô hình hóa ngôn ngữ [34].
Theo hiểu biết tốt nhất của chúng tôi, tuy nhiên, Transformer là mô hình chuyển đổi đầu tiên hoàn toàn dựa vào self-attention để tính toán các biểu diễn đầu vào và đầu ra của nó mà không sử dụng RNN hoặc tích chập căn chỉnh chuỗi. Trong các phần tiếp theo, chúng tôi sẽ mô tả Transformer, tạo động lực cho self-attention và thảo luận về những ưu điểm của nó so với các mô hình như [17, 18] và [9].
3 Kiến trúc Mô hình
Hầu hết các mô hình chuyển đổi chuỗi nơ-ron cạnh tranh đều có cấu trúc bộ mã hóa-giải mã [5,2,35]. Ở đây, bộ mã hóa ánh xạ một chuỗi đầu vào các biểu diễn ký hiệu (x1, ..., xn) thành một chuỗi các biểu diễn liên tục z= (z1, ..., zn). Với z, bộ giải mã sau đó tạo ra một chuỗi đầu ra (y1, ..., ym) các ký hiệu từng phần tử một. Ở mỗi bước, mô hình là tự hồi quy [10], sử dụng các ký hiệu được tạo trước đó như đầu vào bổ sung khi tạo ra ký hiệu tiếp theo.
2

--- TRANG 3 ---
Hình 1: Transformer - kiến trúc mô hình.
Transformer tuân theo kiến trúc tổng thể này bằng cách sử dụng các lớp self-attention xếp chồng và theo điểm, được kết nối đầy đủ cho cả bộ mã hóa và bộ giải mã, được hiển thị trong nửa trái và nửa phải của Hình 1, tương ứng.
3.1 Ngăn xếp Bộ mã hóa và Bộ giải mã
Bộ mã hóa: Bộ mã hóa bao gồm một ngăn xếp N= 6 lớp giống hệt nhau. Mỗi lớp có hai lớp con. Lớp đầu tiên là cơ chế multi-head self-attention, và lớp thứ hai là một mạng feed-forward kết nối đầy đủ đơn giản, theo vị trí. Chúng tôi sử dụng kết nối dư [11] xung quanh mỗi lớp con trong hai lớp con, theo sau bởi chuẩn hóa lớp [1]. Tức là, đầu ra của mỗi lớp con là LayerNorm(x + Sublayer(x)), trong đó Sublayer(x) là hàm được triển khai bởi chính lớp con đó. Để tạo thuận lợi cho các kết nối dư này, tất cả các lớp con trong mô hình, cũng như các lớp embedding, tạo ra đầu ra có chiều dmodel = 512.
Bộ giải mã: Bộ giải mã cũng bao gồm một ngăn xếp N= 6 lớp giống hệt nhau. Ngoài hai lớp con trong mỗi lớp bộ mã hóa, bộ giải mã chèn một lớp con thứ ba, thực hiện multi-head attention trên đầu ra của ngăn xếp bộ mã hóa. Tương tự như bộ mã hóa, chúng tôi sử dụng kết nối dư xung quanh mỗi lớp con, theo sau bởi chuẩn hóa lớp. Chúng tôi cũng sửa đổi lớp con self-attention trong ngăn xếp bộ giải mã để ngăn các vị trí attend đến các vị trí tiếp theo. Việc che khuất này, kết hợp với thực tế rằng các embedding đầu ra được dịch chuyển một vị trí, đảm bảo rằng các dự đoán cho vị trí i chỉ có thể phụ thuộc vào các đầu ra đã biết tại các vị trí nhỏ hơn i.
3.2 Attention
Một hàm attention có thể được mô tả như việc ánh xạ một query và một tập hợp các cặp key-value thành một đầu ra, trong đó query, key, value và đầu ra đều là các vector. Đầu ra được tính toán như một tổng có trọng số
3

--- TRANG 4 ---
Scaled Dot-Product Attention
Multi-Head Attention
Hình 2: (trái) Scaled Dot-Product Attention. (phải) Multi-Head Attention bao gồm nhiều lớp attention chạy song song.
của các value, trong đó trọng số gán cho mỗi value được tính toán bằng một hàm tương thích của query với key tương ứng.
3.2.1 Scaled Dot-Product Attention
Chúng tôi gọi attention cụ thể của chúng tôi là "Scaled Dot-Product Attention" (Hình 2). Đầu vào bao gồm các query và key có chiều dk, và các value có chiều dv. Chúng tôi tính toán tích vô hướng của query với tất cả các key, chia mỗi cái cho √dk, và áp dụng hàm softmax để có được trọng số trên các value.
Trong thực tế, chúng tôi tính toán hàm attention trên một tập hợp các query đồng thời, được đóng gói cùng nhau thành một ma trận Q. Các key và value cũng được đóng gói cùng nhau thành các ma trận K và V. Chúng tôi tính toán ma trận đầu ra như:
Attention(Q, K, V) = softmax(QKT/√dk)V (1)
Hai hàm attention được sử dụng phổ biến nhất là additive attention [2], và dot-product (multiplicative) attention. Dot-product attention giống hệt với thuật toán của chúng tôi, ngoại trừ hệ số tỷ lệ 1/√dk. Additive attention tính toán hàm tương thích bằng cách sử dụng mạng feed-forward với một lớp ẩn duy nhất. Trong khi hai cái tương tự nhau về độ phức tạp lý thuyết, dot-product attention nhanh hơn và tiết kiệm không gian hơn nhiều trong thực tế, vì nó có thể được triển khai bằng cách sử dụng mã nhân ma trận được tối ưu hóa cao.
Trong khi với các giá trị nhỏ của dk hai cơ chế hoạt động tương tự nhau, additive attention vượt trội hơn dot product attention không có tỷ lệ cho các giá trị lớn hơn của dk [3]. Chúng tôi nghi ngờ rằng với các giá trị lớn của dk, các tích vô hướng lớn về độ lớn, đẩy hàm softmax vào các vùng mà nó có gradient cực kỳ nhỏ4. Để chống lại hiệu ứng này, chúng tôi tỷ lệ các tích vô hướng bằng 1/√dk.
3.2.2 Multi-Head Attention
Thay vì thực hiện một hàm attention đơn với key, value và query có chiều dmodel, chúng tôi thấy có lợi khi chiếu tuyến tính các query, key và value h lần với các phép chiếu tuyến tính khác nhau, được học lên các chiều dk, dk và dv, tương ứng. Trên mỗi phiên bản được chiếu này của query, key và value, chúng tôi sau đó thực hiện hàm attention song song, tạo ra các giá trị đầu ra có chiều dv. Những cái này được nối lại và một lần nữa được chiếu, tạo ra các giá trị cuối cùng, như được mô tả trong Hình 2.
Multi-head attention cho phép mô hình cùng lúc attend đến thông tin từ các không gian biểu diễn con khác nhau tại các vị trí khác nhau. Với một attention head đơn, việc lấy trung bình ngăn cản điều này.
MultiHead(Q, K, V) = Concat(head1, ..., headh)WO
trong đó headi = Attention(QWQi, KWKi, VWVi)
Trong đó các phép chiếu là các ma trận tham số WQi ∈ Rdmodel×dk, WKi ∈ Rdmodel×dk, WVi ∈ Rdmodel×dv và WO ∈ Rhdv×dmodel.
Trong công việc này, chúng tôi sử dụng h= 8 lớp attention song song, hoặc head. Đối với mỗi cái trong số này, chúng tôi sử dụng dk = dv = dmodel/h = 64. Do chiều giảm của mỗi head, tổng chi phí tính toán tương tự như của single-head attention với chiều đầy đủ.
3.2.3 Ứng dụng của Attention trong Mô hình của chúng tôi
Transformer sử dụng multi-head attention theo ba cách khác nhau:
• Trong các lớp "encoder-decoder attention", các query đến từ lớp bộ giải mã trước đó, và các key và value bộ nhớ đến từ đầu ra của bộ mã hóa. Điều này cho phép mọi vị trí trong bộ giải mã attend trên tất cả các vị trí trong chuỗi đầu vào. Điều này bắt chước các cơ chế encoder-decoder attention điển hình trong các mô hình sequence-to-sequence như [38, 2, 9].
• Bộ mã hóa chứa các lớp self-attention. Trong một lớp self-attention, tất cả các key, value và query đều đến từ cùng một nơi, trong trường hợp này, là đầu ra của lớp trước đó trong bộ mã hóa. Mỗi vị trí trong bộ mã hóa có thể attend đến tất cả các vị trí trong lớp trước đó của bộ mã hóa.
• Tương tự, các lớp self-attention trong bộ giải mã cho phép mỗi vị trí trong bộ giải mã attend đến tất cả các vị trí trong bộ giải mã lên đến và bao gồm vị trí đó. Chúng tôi cần ngăn luồng thông tin về phía trái trong bộ giải mã để bảo toàn tính chất tự hồi quy. Chúng tôi triển khai điều này bên trong scaled dot-product attention bằng cách che khuất (thiết lập thành −∞) tất cả các giá trị trong đầu vào của softmax tương ứng với các kết nối bất hợp pháp. Xem Hình 2.
3.3 Mạng Feed-Forward theo Vị trí
Ngoài các lớp con attention, mỗi lớp trong bộ mã hóa và bộ giải mã của chúng tôi chứa một mạng feed-forward được kết nối đầy đủ, được áp dụng cho mỗi vị trí một cách riêng biệt và giống hệt nhau. Điều này bao gồm hai phép biến đổi tuyến tính với một kích hoạt ReLU ở giữa.
FFN(x) = max(0, xW1 + b1)W2 + b2 (2)
Trong khi các phép biến đổi tuyến tính giống nhau qua các vị trí khác nhau, chúng sử dụng các tham số khác nhau từ lớp này sang lớp khác. Một cách khác để mô tả điều này là như hai phép tích chập với kernel size 1. Chiều của đầu vào và đầu ra là dmodel = 512, và lớp bên trong có chiều dff = 2048.
3.4 Embedding và Softmax
Tương tự như các mô hình chuyển đổi chuỗi khác, chúng tôi sử dụng embedding được học để chuyển đổi các token đầu vào và token đầu ra thành vector có chiều dmodel. Chúng tôi cũng sử dụng phép biến đổi tuyến tính được học thông thường và hàm softmax để chuyển đổi đầu ra bộ giải mã thành xác suất token tiếp theo được dự đoán. Trong mô hình của chúng tôi, chúng tôi chia sẻ cùng một ma trận trọng số giữa hai lớp embedding và phép biến đổi tuyến tính pre-softmax, tương tự như [30]. Trong các lớp embedding, chúng tôi nhân các trọng số đó với √dmodel.
4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q·k = ∑dki=1 qiki, has mean 0 and variance dk.
4

--- TRANG 5 ---
Bảng 1: Độ dài đường dẫn tối đa, độ phức tạp mỗi lớp và số lượng phép toán tuần tự tối thiểu cho các loại lớp khác nhau. n là độ dài chuỗi, d là chiều biểu diễn, k là kernel size của các tích chập và r là kích thước vùng lân cận trong restricted self-attention.
Loại Lớp | Độ phức tạp mỗi Lớp | Phép toán Tuần tự | Độ dài Đường dẫn Tối đa
Self-Attention | O(n²·d) | O(1) | O(1)
Recurrent | O(n·d²) | O(n) | O(n)
Convolutional | O(k·n·d²) | O(1) | O(logk(n))
Self-Attention (restricted) | O(r·n·d) | O(1) | O(n/r)
3.5 Mã hóa Vị trí
Vì mô hình của chúng tôi không chứa tính hồi quy và không có tích chập, để mô hình có thể sử dụng thứ tự của chuỗi, chúng tôi phải tiêm một số thông tin về vị trí tương đối hoặc tuyệt đối của các token trong chuỗi. Để làm điều này, chúng tôi thêm "positional encoding" vào input embedding ở đáy của các ngăn xếp bộ mã hóa và bộ giải mã. Các positional encoding có cùng chiều dmodel như các embedding, để hai cái có thể được cộng lại. Có nhiều lựa chọn về positional encoding, được học và cố định [9].
Trong công việc này, chúng tôi sử dụng các hàm sin và cosin của các tần số khác nhau:
PE(pos,2i) = sin(pos/10000^(2i/dmodel))
PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))
trong đó pos là vị trí và i là chiều. Tức là, mỗi chiều của positional encoding tương ứng với một sinusoid. Các bước sóng tạo thành một cấp số nhân từ 2π đến 10000·2π. Chúng tôi chọn hàm này vì chúng tôi giả thuyết rằng nó sẽ cho phép mô hình dễ dàng học attend theo vị trí tương đối, vì đối với bất kỳ offset cố định k nào, PEpos+k có thể được biểu diễn như một hàm tuyến tính của PEpos.
Chúng tôi cũng thử nghiệm với việc sử dụng learned positional embedding [9] thay thế, và thấy rằng hai phiên bản tạo ra kết quả gần như giống hệt nhau (xem Bảng 3 hàng (E)). Chúng tôi chọn phiên bản sinusoidal vì nó có thể cho phép mô hình ngoại suy đến độ dài chuỗi dài hơn những cái gặp phải trong quá trình huấn luyện.
4 Tại sao Self-Attention
Trong phần này, chúng tôi so sánh các khía cạnh khác nhau của các lớp self-attention với các lớp hồi quy và tích chập thường được sử dụng để ánh xạ một chuỗi có độ dài biến đổi của các biểu diễn ký hiệu (x1, ..., xn) thành một chuỗi khác có độ dài bằng nhau (z1, ..., zn), với xi, zi ∈ Rd, như một lớp ẩn trong một bộ mã hóa hoặc bộ giải mã chuyển đổi chuỗi điển hình. Tạo động lực cho việc sử dụng self-attention, chúng tôi xem xét ba mong muốn.
Một là tổng độ phức tạp tính toán mỗi lớp. Một cái khác là lượng tính toán có thể được song song hóa, được đo bằng số lượng phép toán tuần tự tối thiểu cần thiết.
Thứ ba là độ dài đường dẫn giữa các phụ thuộc tầm xa trong mạng. Học các phụ thuộc tầm xa là một thách thức chính trong nhiều nhiệm vụ chuyển đổi chuỗi. Một yếu tố chính ảnh hưởng đến khả năng học các phụ thuộc như vậy là độ dài của các đường dẫn tín hiệu tiến và lùi phải đi qua trong mạng. Các đường dẫn ngắn hơn giữa bất kỳ tổ hợp vị trí nào trong chuỗi đầu vào và đầu ra, việc học các phụ thuộc tầm xa càng dễ dàng hơn [12]. Do đó, chúng tôi cũng so sánh độ dài đường dẫn tối đa giữa bất kỳ hai vị trí đầu vào và đầu ra nào trong các mạng bao gồm các loại lớp khác nhau.
Như được ghi chú trong Bảng 1, một lớp self-attention kết nối tất cả các vị trí với một số lượng phép toán được thực hiện tuần tự hằng số, trong khi một lớp hồi quy yêu cầu O(n) phép toán tuần tự. Về mặt độ phức tạp tính toán, các lớp self-attention nhanh hơn các lớp hồi quy khi độ dài chuỗi n nhỏ hơn chiều biểu diễn d, điều này thường xảy ra với các biểu diễn câu được sử dụng bởi các mô hình tiên tiến trong dịch máy, như word-piece [38] và byte-pair [31]. Để cải thiện hiệu suất tính toán cho các nhiệm vụ liên quan đến chuỗi rất dài, self-attention có thể được hạn chế chỉ xem xét một vùng lân cận có kích thước r trong chuỗi đầu vào tập trung xung quanh vị trí đầu ra tương ứng. Điều này sẽ tăng độ dài đường dẫn tối đa lên O(n/r). Chúng tôi dự định điều tra phương pháp này thêm trong công việc tương lai.
6

--- TRANG 6 ---
Một lớp tích chập đơn với kernel width k < n không kết nối tất cả các cặp vị trí đầu vào và đầu ra. Để làm điều này cần một ngăn xếp O(n/k) lớp tích chập trong trường hợp kernel liền kề, hoặc O(logk(n)) trong trường hợp dilated convolution [18], tăng độ dài của các đường dẫn dài nhất giữa hai vị trí bất kỳ trong mạng. Các lớp tích chập thường đắt hơn các lớp hồi quy, bởi một hệ số k. Tuy nhiên, separable convolution [6] giảm độ phức tạp đáng kể, xuống O(k·n·d + n·d²). Thậm chí với k = n, tuy nhiên, độ phức tạp của separable convolution bằng với sự kết hợp của một lớp self-attention và một lớp feed-forward theo điểm, phương pháp chúng tôi thực hiện trong mô hình của chúng tôi.
Như lợi ích phụ, self-attention có thể mang lại các mô hình có thể diễn giải hơn. Chúng tôi kiểm tra các phân phối attention từ các mô hình của chúng tôi và trình bày cũng như thảo luận các ví dụ trong phụ lục. Không chỉ các attention head riêng lẻ học rõ ràng để thực hiện các nhiệm vụ khác nhau, nhiều cái dường như thể hiện hành vi liên quan đến cấu trúc cú pháp và ngữ nghĩa của các câu.
5 Huấn luyện
Phần này mô tả chế độ huấn luyện cho các mô hình của chúng tôi.
5.1 Dữ liệu Huấn luyện và Gộp Batch
Chúng tôi huấn luyện trên bộ dữ liệu chuẩn WMT 2014 tiếng Anh-Đức bao gồm khoảng 4.5 triệu cặp câu. Các câu được mã hóa bằng byte-pair encoding [3], có từ vựng nguồn-đích được chia sẻ khoảng 37000 token. Đối với tiếng Anh-Pháp, chúng tôi sử dụng bộ dữ liệu WMT 2014 tiếng Anh-Pháp lớn hơn đáng kể bao gồm 36M câu và chia token thành từ vựng word-piece 32000 [38]. Các cặp câu được gộp batch cùng nhau theo độ dài chuỗi xấp xỉ. Mỗi batch huấn luyện chứa một tập hợp các cặp câu chứa khoảng 25000 token nguồn và 25000 token đích.
5.2 Phần cứng và Lịch trình
Chúng tôi huấn luyện các mô hình của chúng tôi trên một máy với 8 GPU NVIDIA P100. Đối với các mô hình cơ bản của chúng tôi sử dụng các siêu tham số được mô tả trong toàn bộ bài báo, mỗi bước huấn luyện mất khoảng 0.4 giây. Chúng tôi huấn luyện các mô hình cơ bản tổng cộng 100,000 bước hoặc 12 giờ. Đối với các mô hình lớn của chúng tôi (được mô tả ở dòng dưới cùng của bảng 3), thời gian bước là 1.0 giây. Các mô hình lớn được huấn luyện trong 300,000 bước (3.5 ngày).
5.3 Bộ tối ưu hóa
Chúng tôi sử dụng bộ tối ưu hóa Adam [20] với β1 = 0.9, β2 = 0.98 và ε = 10⁻⁹. Chúng tôi thay đổi learning rate trong quá trình huấn luyện, theo công thức:
lrate = d⁻⁰·⁵model · min(step_num⁻⁰·⁵, step_num · warmup_steps⁻¹·⁵) (3)
Điều này tương ứng với việc tăng learning rate tuyến tính cho warmup_steps bước huấn luyện đầu tiên, và giảm nó sau đó tỷ lệ thuận với căn bậc hai nghịch đảo của số bước. Chúng tôi sử dụng warmup_steps = 4000.
5.4 Regularization
Chúng tôi sử dụng ba loại regularization trong quá trình huấn luyện:
7

--- TRANG 7 ---
Bảng 2: Transformer đạt điểm số BLEU tốt hơn các mô hình tiên tiến trước đó trên các bài kiểm tra newstest2014 tiếng Anh-Đức và tiếng Anh-Pháp với một phần nhỏ chi phí huấn luyện.
Mô hình | BLEU | Chi phí Huấn luyện (FLOPs)
| EN-DE | EN-FR | EN-DE | EN-FR
ByteNet [18] | 23.75 | | |
Deep-Att + PosUnk [39] | | 39.2 | | 1.0·10²⁰
GNMT + RL [38] | 24.6 | 39.92 | 2.3·10¹⁹ | 1.4·10²⁰
ConvS2S [9] | 25.16 | 40.46 | 9.6·10¹⁸ | 1.5·10²⁰
MoE [32] | 26.03 | 40.56 | 2.0·10¹⁹ | 1.2·10²⁰
Deep-Att + PosUnk Ensemble [39] | | 40.4 | | 8.0·10²⁰
GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8·10²⁰ | 1.1·10²¹
ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7·10¹⁹ | 1.2·10²¹
Transformer (base model) | 27.3 | 38.1 | 3.3·10¹⁸ |
Transformer (big) | 28.4 | 41.8 | 2.3·10¹⁹ |
Residual Dropout Chúng tôi áp dụng dropout [33] cho đầu ra của mỗi lớp con, trước khi nó được thêm vào đầu vào lớp con và được chuẩn hóa. Ngoài ra, chúng tôi áp dụng dropout cho tổng các embedding và positional encoding trong cả ngăn xếp bộ mã hóa và bộ giải mã. Đối với mô hình cơ bản, chúng tôi sử dụng tỷ lệ Pdrop = 0.1.
Label Smoothing Trong quá trình huấn luyện, chúng tôi sử dụng label smoothing với giá trị εls = 0.1 [36]. Điều này làm tổn hại perplexity, vì mô hình học để không chắc chắn hơn, nhưng cải thiện accuracy và điểm số BLEU.
6 Kết quả
6.1 Dịch máy
Trên nhiệm vụ dịch WMT 2014 tiếng Anh-Đức, mô hình transformer lớn (Transformer (big) trong Bảng 2) vượt trội hơn các mô hình được báo cáo tốt nhất trước đó (bao gồm cả ensemble) hơn 2.0 BLEU, thiết lập điểm số BLEU tiên tiến mới là 28.4. Cấu hình của mô hình này được liệt kê trong dòng dưới cùng của Bảng 3. Huấn luyện mất 3.5 ngày trên 8 GPU P100. Thậm chí mô hình cơ bản của chúng tôi cũng vượt trội hơn tất cả các mô hình và ensemble đã được xuất bản trước đó, với một phần nhỏ chi phí huấn luyện so với bất kỳ mô hình cạnh tranh nào.
Trên nhiệm vụ dịch WMT 2014 tiếng Anh-Pháp, mô hình lớn của chúng tôi đạt điểm số BLEU là 41.0, vượt trội hơn tất cả các mô hình đơn đã được xuất bản trước đó, với chi phí ít hơn 1/4 so với mô hình tiên tiến trước đó. Mô hình Transformer (big) được huấn luyện cho tiếng Anh-Pháp sử dụng tỷ lệ dropout Pdrop = 0.1, thay vì 0.3.
Đối với các mô hình cơ bản, chúng tôi sử dụng một mô hình đơn thu được bằng cách lấy trung bình 5 checkpoint cuối cùng, được viết với khoảng thời gian 10 phút. Đối với các mô hình lớn, chúng tôi lấy trung bình 20 checkpoint cuối cùng. Chúng tôi sử dụng beam search với beam size 4 và length penalty α = 0.6 [38]. Các siêu tham số này được chọn sau khi thử nghiệm trên tập phát triển. Chúng tôi thiết lập độ dài đầu ra tối đa trong quá trình suy luận là độ dài đầu vào + 50, nhưng kết thúc sớm khi có thể [38].
Bảng 2 tóm tắt kết quả của chúng tôi và so sánh chất lượng dịch và chi phí huấn luyện của chúng tôi với các kiến trúc mô hình khác từ tài liệu. Chúng tôi ước tính số lượng phép toán dấu phẩy động được sử dụng để huấn luyện một mô hình bằng cách nhân thời gian huấn luyện, số lượng GPU được sử dụng, và một ước tính về khả năng dấu phẩy động độ chính xác đơn duy trì của mỗi GPU⁵.
6.2 Biến thể Mô hình
Để đánh giá tầm quan trọng của các thành phần khác nhau của Transformer, chúng tôi thay đổi mô hình cơ bản của chúng tôi theo các cách khác nhau, đo lường sự thay đổi hiệu suất trên dịch tiếng Anh-Đức trên tập phát triển, newstest2013. Chúng tôi sử dụng beam search như được mô tả trong phần trước, nhưng không có checkpoint averaging. Chúng tôi trình bày các kết quả này trong Bảng 3.
⁵Chúng tôi sử dụng các giá trị 2.8, 3.7, 6.0 và 9.5 TFLOPS cho K80, K40, M40 và P100, tương ứng.
8

--- TRANG 8 ---
Bảng 3: Biến thể trên kiến trúc Transformer. Các giá trị không được liệt kê giống hệt với những giá trị của mô hình cơ bản. Tất cả các số liệu đều trên tập phát triển dịch tiếng Anh-Đức, newstest2013. Các perplexity được liệt kê là theo word-piece, theo byte-pair encoding của chúng tôi, và không nên so sánh với perplexity theo từ.
| N | d_model | d_ff | h | d_k | d_v | P_drop | ε_ls | train steps | PPL (dev) | BLEU (dev) | params ×10⁶
base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65
(A) | | | | 1 | 512 | 512 | | | | 5.29 | 24.9 |
| | | | 4 | 128 | 128 | | | | 5.00 | 25.5 |
| | | | 16 | 32 | 32 | | | | 4.91 | 25.8 |
| | | | 32 | 16 | 16 | | | | 5.01 | 25.4 |
(B) | | | | | 16 | | | | | 5.16 | 25.1 | 58
| | | | | 32 | | | | | 5.01 | 25.4 | 60
(C) | 2 | | | | | | | | | 6.11 | 23.7 | 36
| 4 | | | | | | | | | 5.19 | 25.3 | 50
| 8 | | | | | | | | | 4.88 | 25.5 | 80
| | 256 | | | 32 | 32 | | | | 5.75 | 24.5 | 28
| | 1024 | | | 128 | 128 | | | | 4.66 | 26.0 | 168
| | | 1024 | | | | | | | 5.12 | 25.4 | 53
| | | 4096 | | | | | | | 4.75 | 26.2 | 90
(D) | | | | | | | 0.0 | | | 5.77 | 24.6 |
| | | | | | | 0.2 | | | 4.95 | 25.5 |
| | | | | | | | 0.0 | | 4.67 | 25.3 |
| | | | | | | | 0.2 | | 5.47 | 25.7 |
(E) | | | | | | | | | | 4.92 | 25.7 |
positional embedding thay vì sinusoid
big | 6 | 1024 | 4096 | 16 | | | 0.3 | | 300K | 4.33 | 26.4 | 213
Trong Bảng 3 các hàng (A), chúng tôi thay đổi số lượng attention head và các chiều key và value attention, giữ lượng tính toán không đổi, như được mô tả trong Phần 3.2.2. Trong khi single-head attention kém hơn 0.9 BLEU so với cài đặt tốt nhất, chất lượng cũng giảm khi có quá nhiều head.
Trong Bảng 3 các hàng (B), chúng tôi quan sát thấy việc giảm kích thước attention key dk làm tổn hại chất lượng mô hình. Điều này cho thấy rằng xác định tương thích không dễ dàng và một hàm tương thích phức tạp hơn dot product có thể có lợi. Chúng tôi quan sát thêm trong các hàng (C) và (D) rằng, như mong đợi, các mô hình lớn hơn tốt hơn, và dropout rất hữu ích trong việc tránh over-fitting. Trong hàng (E), chúng tôi thay thế positional encoding sinusoidal của chúng tôi bằng learned positional embedding [9], và quan sát kết quả gần như giống hệt với mô hình cơ bản.
6.3 English Constituency Parsing
Để đánh giá liệu Transformer có thể tổng quát hóa cho các nhiệm vụ khác không, chúng tôi thực hiện các thí nghiệm về English constituency parsing. Nhiệm vụ này đặt ra những thách thức cụ thể: đầu ra chịu các ràng buộc cấu trúc mạnh và dài hơn đáng kể so với đầu vào. Hơn nữa, các mô hình RNN sequence-to-sequence không thể đạt được kết quả tiên tiến trong các chế độ dữ liệu nhỏ [37].
Chúng tôi huấn luyện một transformer 4 lớp với dmodel = 1024 trên phần Wall Street Journal (WSJ) của Penn Treebank [25], khoảng 40K câu huấn luyện. Chúng tôi cũng huấn luyện nó trong một thiết lập bán giám sát, sử dụng các corpus high-confidence và BerkleyParser lớn hơn với khoảng 17M câu [37]. Chúng tôi sử dụng từ vựng 16K token cho thiết lập chỉ WSJ và từ vựng 32K token cho thiết lập bán giám sát.
Chúng tôi chỉ thực hiện một số lượng nhỏ các thí nghiệm để chọn dropout, cả attention và residual (phần 5.4), learning rate và beam size trên tập phát triển Section 22, tất cả các tham số khác giữ nguyên so với mô hình dịch cơ bản tiếng Anh-Đức. Trong quá trình suy luận, chúng tôi
9

--- TRANG 9 ---
Bảng 4: Transformer tổng quát hóa tốt cho English constituency parsing (Kết quả trên Section 23 của WSJ)
Parser | Training | WSJ 23 F1
Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3
Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4
Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4
Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7
Transformer (4 layers) | WSJ only, discriminative | 91.3
Zhu et al. (2013) [40] | semi-supervised | 91.3
Huang & Harper (2009) [14] | semi-supervised | 91.3
McClosky et al. (2006) [26] | semi-supervised | 92.1
Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1
Transformer (4 layers) | semi-supervised | 92.7
Luong et al. (2015) [23] | multi-task | 93.0
Dyer et al. (2016) [8] | generative | 93.3
tăng độ dài đầu ra tối đa lên độ dài đầu vào + 300. Chúng tôi sử dụng beam size 21 và α = 0.3 cho cả thiết lập chỉ WSJ và bán giám sát.
Kết quả của chúng tôi trong Bảng 4 cho thấy rằng mặc dù thiếu điều chỉnh cụ thể theo nhiệm vụ, mô hình của chúng tôi hoạt động tốt một cách đáng ngạc nhiên, mang lại kết quả tốt hơn tất cả các mô hình được báo cáo trước đó ngoại trừ Recurrent Neural Network Grammar [8].
Trái ngược với các mô hình RNN sequence-to-sequence [37], Transformer vượt trội hơn BerkeleyParser [29] thậm chí khi chỉ huấn luyện trên tập huấn luyện WSJ gồm 40K câu.
7 Kết luận
Trong công việc này, chúng tôi đã trình bày Transformer, mô hình chuyển đổi chuỗi đầu tiên hoàn toàn dựa trên attention, thay thế các lớp hồi quy thường được sử dụng trong các kiến trúc encoder-decoder bằng multi-headed self-attention.
Đối với các nhiệm vụ dịch, Transformer có thể được huấn luyện nhanh hơn đáng kể so với các kiến trúc dựa trên các lớp hồi quy hoặc tích chập. Trên cả hai nhiệm vụ dịch WMT 2014 tiếng Anh-Đức và WMT 2014 tiếng Anh-Pháp, chúng tôi đạt được một trạng thái tiên tiến mới. Trong nhiệm vụ trước, mô hình tốt nhất của chúng tôi vượt trội hơn thậm chí tất cả các ensemble được báo cáo trước đó.
Chúng tôi hào hứng về tương lai của các mô hình dựa trên attention và dự định áp dụng chúng cho các nhiệm vụ khác. Chúng tôi dự định mở rộng Transformer cho các bài toán liên quan đến các phương thức đầu vào và đầu ra khác ngoài văn bản và điều tra các cơ chế attention local, restricted để xử lý hiệu quả các đầu vào và đầu ra lớn như hình ảnh, âm thanh và video. Làm cho việc sinh ít tuần tự hơn là một mục tiêu nghiên cứu khác của chúng tôi.
Mã chúng tôi sử dụng để huấn luyện và đánh giá các mô hình của chúng tôi có sẵn tại https://github.com/tensorflow/tensor2tensor.
Lời cảm ơn Chúng tôi biết ơn Nal Kalchbrenner và Stephan Gouws vì những nhận xét, sửa chữa và cảm hứng hữu ích của họ.
Tài liệu tham khảo
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.
[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.
[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.
10

--- TRANG 10 ---
[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.
[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.
[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.
[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.
[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.
[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.
[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.
[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.
[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
11

--- TRANG 11 ---
[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.
[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.
[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.
[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.
[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.
12

--- TRANG 12 ---
Trực quan hóa Attention
Input-Input Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Hình 3: Một ví dụ về cơ chế attention theo dõi các phụ thuộc tầm xa trong encoder self-attention ở lớp 5 trong tổng số 6. Nhiều attention head attend đến một phụ thuộc xa của động từ 'making', hoàn thành cụm từ 'making...more difficult'. Attention ở đây chỉ hiển thị cho từ 'making'. Các màu sắc khác nhau đại diện cho các head khác nhau. Tốt nhất khi xem bằng màu.
13

--- TRANG 13 ---
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Hình 4: Hai attention head, cũng ở lớp 5 trong tổng số 6, dường như có liên quan đến việc giải quyết anaphora. Trên: Attention đầy đủ cho head 5. Dưới: Attention được tách biệt chỉ từ từ 'its' cho attention head 5 và 6. Lưu ý rằng các attention rất sắc nét cho từ này.
14

--- TRANG 14 ---
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Hình 5: Nhiều attention head thể hiện hành vi dường như liên quan đến cấu trúc của câu. Chúng tôi đưa ra hai ví dụ như vậy ở trên, từ hai head khác nhau từ encoder self-attention ở lớp 5 trong tổng số 6. Các head rõ ràng đã học để thực hiện các nhiệm vụ khác nhau.
15
