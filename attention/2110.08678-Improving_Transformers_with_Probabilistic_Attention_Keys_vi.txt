# 2110.08678.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2110.08678.pdf
# Kích thước tệp: 4141437 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cải thiện Transformer với Khóa Chú ý Xác suất
Tam Nguyen* 1Tan M. Nguyen* 2Dung D. Le3Duy Khuong Nguyen1Viet-Anh Tran4Richard G. Baraniuk5
Nhat Ho** 6Stanley J. Osher** 2

Tóm tắt
Chú ý đa đầu là động lực đằng sau các transformer tiên tiến nhất, đạt được hiệu suất ấn tượng trên nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính. Người ta đã quan sát thấy rằng đối với nhiều ứng dụng, những đầu chú ý này học được embedding dư thừa, và hầu hết chúng có thể được loại bỏ mà không làm giảm hiệu suất của mô hình. Được truyền cảm hứng từ quan sát này, chúng tôi đề xuất Transformer với Hỗn hợp Khóa Gaussian (Transformer-MGK), một kiến trúc transformer mới thay thế các đầu dư thừa trong transformer bằng một hỗn hợp khóa tại mỗi đầu. Những hỗn hợp khóa này tuân theo mô hình hỗn hợp Gaussian và cho phép mỗi đầu chú ý tập trung vào các phần khác nhau của chuỗi đầu vào một cách hiệu quả. So với transformer thông thường tương ứng, Transformer-MGK tăng tốc quá trình huấn luyện và suy luận, có ít tham số hơn, và yêu cầu ít FLOP hơn để tính toán trong khi đạt được độ chính xác tương đương hoặc tốt hơn trên các tác vụ. Transformer-MGK cũng có thể dễ dàng được mở rộng để sử dụng với chú ý tuyến tính. Chúng tôi chứng minh thực nghiệm lợi thế của Transformer-MGK trong một loạt các ứng dụng thực tế, bao gồm mô hình hóa ngôn ngữ và các tác vụ liên quan đến chuỗi rất dài. Trên benchmark Wikitext-103 và Long Range Arena, Transformer-MGK với 4 đầu đạt được hiệu suất tương đương hoặc tốt hơn so với transformer cơ sở với 8 đầu.

*Đóng góp ngang nhau. Các tác giả đồng đầu tiên được liệt kê theo thứ tự bảng chữ cái**Tác giả đồng cuối 1FPT Software AI Center, Hà Nội, Việt Nam2Khoa Toán học, Đại học California, Los Angeles, Mỹ3Cao đẳng Kỹ thuật và Khoa học Máy tính, Đại học VinUni, Hà Nội, Việt Nam4Deezer Research, Pháp 5Khoa Kỹ thuật Điện và Máy tính, Đại học Rice, Houston, Mỹ6Khoa Thống kê và Khoa học Dữ liệu, Đại học Texas tại Austin, Mỹ. Liên hệ: Tan Nguyen<tanmnguyen89@ucla.edu >.

Kỷ yếu Hội nghị Quốc tế lần thứ 39 về Học máy , Baltimore, Maryland, Mỹ, PMLR 162, 2022. Bản quyền 2022 thuộc về (các) tác giả.

1. Giới thiệu
Transformer (Vaswani et al., 2017) đã trở thành mô hình tiên tiến nhất cho các tác vụ xử lý chuỗi, giải quyết nhiều vấn đề thách thức trong xử lý ngôn ngữ tự nhiên và thị giác máy tính (Al-Rfou et al., 2019; Dai et al., 2019; Williams et al., 2018; Devlin et al., 2018; Brown & et al., 2020; Howard & Ruder, 2018; Rajpurkar et al., 2016; Dehghani et al., 2018; So et al., 2019; Dosovitskiy et al., 2020; Touvron et al., 2020; Nguyen et al., 2021a). Những mô hình này cũng có thể chuyển giao kiến thức đã học từ một mô hình được huấn luyện trước đến các tác vụ liên quan đến các phương thức dữ liệu khác nhau và có giám sát hạn chế (Radford et al., 2018; 2019; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019). Sự thành công của transformer có gốc rễ từ cơ chế tự chú ý như khối xây dựng cơ bản cho việc mô hình hóa (Cho et al., 2014; Parikh et al., 2016; Lin et al., 2017). Đối với mỗi token, tự chú ý tính toán trung bình có trọng số của các biểu diễn đặc trưng của các token khác trong đó trọng số tỷ lệ với điểm tương đồng giữa mỗi cặp token. Cơ chế này cho phép một token chú ý đến các token khác trong chuỗi và đạt được một biểu diễn ngữ cảnh (Bahdanau et al., 2014; Vaswani et al., 2017; Kim et al., 2017). Đã được chứng minh rằng khả năng biểu diễn của cơ chế chú ý (Tenney et al., 2019) và khả năng nắm bắt các mối quan hệ cú pháp và ngữ nghĩa đa dạng (Tenney et al., 2019; Vig & Belinkov, 2019; Clark et al., 2019; Voita et al., 2019a; Hewitt & Liang, 2019) là chìa khóa cho hiệu suất ấn tượng của transformer trong thực tế.

1.1. Tự chú ý
Đối với một chuỗi đầu vào cho trước X:= [x1;...;xN]> ∈ R^(N×Dx) của N vector đặc trưng, tự chú ý biến đổi X thành chuỗi đầu ra H trong hai bước sau:

Bước 1. Chuỗi đầu vào X được chiếu thành ma trận truy vấn Q, ma trận khóa K, và ma trận giá trị V thông qua ba phép biến đổi tuyến tính
Q=XW_Q^T; K=XW_K^T; V=XW_V^T;
trong đó W_Q, W_K ∈ R^(D×Dx), và W_V ∈ R^(Dv×Dx) là các ma trận trọng số. Chúng ta ký hiệu Q:= [q1;...;qN]>, K:= [k1;...;kN]>, và V:= [v1;...;vN]>, trong đó các vector qi, ki, vi cho i = 1,...,N là các vector truy vấn, khóa, và giá trị tương ứng.

--- TRANG 2 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bước 2. Chuỗi đầu ra H:= [h1;...;hN]> sau đó được tính như sau
H = softmax(QK^T/√D)V := AV; (1)
trong đó hàm softmax được áp dụng cho mỗi hàng của ma trận (QK^T)/√D. Đối với mỗi vector truy vấn qi cho i = 1,...,N, một dạng tương đương của Công thức (1) để tính vector đầu ra hi được cho bởi
hi = ∑(j=1 to N) softmax(qi^T kj/√D) vj := ∑(j=1 to N) aij vj. (2)

Ma trận A ∈ R^(N×N) và thành phần aij của nó cho i,j = 1,...,N là ma trận chú ý và điểm chú ý tương ứng. Tự chú ý được tính bởi Công thức (1) và (2) được gọi là chú ý tích vô hướng có tỷ lệ hoặc chú ý softmax. Trong bài báo của chúng tôi, chúng tôi gọi transformer sử dụng chú ý này là transformer softmax. Cấu trúc mà ma trận chú ý A học được từ huấn luyện quyết định khả năng của tự chú ý trong việc nắm bắt biểu diễn ngữ cảnh cho mỗi token.

Chú ý Đa đầu Mỗi chuỗi đầu ra H tạo thành một đầu chú ý. Trong chú ý đa đầu, nhiều đầu được nối với nhau để tính toán đầu ra cuối cùng. Đặt H là số đầu và W_O ∈ R^(HDv×HDv) là ma trận chiếu cho đầu ra. Chú ý đa đầu được định nghĩa là
MultiHead({Q,K,V}_i^H) = Concat(H1,...,HH)W_O.

Mặc dù chú ý đa đầu mở rộng chú ý đơn đầu để nắm bắt các mẫu chú ý đa dạng và cải thiện hiệu suất của transformer, đã được chứng minh rằng transformer cho các tác vụ thực tế bao gồm phân loại chuỗi và mô hình hóa ngôn ngữ học được các đầu dư thừa (Michel et al., 2019). Những đầu dư thừa này tính toán các ánh xạ chú ý tương tự. Việc có nhiều đầu như vậy trong mô hình hạn chế khả năng biểu diễn của transformer trong khi lãng phí tham số, bộ nhớ và tính toán, cản trở việc áp dụng transformer vào nhiều tác vụ quy mô lớn quan trọng.

1.2. Đóng góp
Chúng tôi thiết lập sự tương ứng giữa tự chú ý trong transformer và mô hình hỗn hợp Gaussian (GMM) và đề xuất Transformer với Hỗn hợp Khóa Gaussian (Transformer-MGK), một lớp transformer mới có thể tránh được tính dư thừa của đầu. Cốt lõi của Transformer-MGK là thay thế khóa chú ý kj trong mỗi đầu bằng một GMM để cho phép truy vấn qi, cũng như token liên quan của nó, chú ý đến các vị trí đa dạng hơn trong chuỗi đầu vào, từ đó tăng khả năng biểu diễn của mỗi đầu chú ý và giảm khả năng học các đầu dư thừa. Đóng góp của chúng tôi gồm bốn phần:

1. Chúng tôi xây dựng một GMM và chỉ ra rằng điểm chú ý trong tự chú ý khớp với phân phối hậu nghiệm trong mô hình của chúng tôi, cung cấp một khung xác suất để nghiên cứu tự chú ý trong transformer.

2. Dưới khung xác suất của chúng tôi cho tự chú ý, chúng tôi giới thiệu một hỗn hợp Gaussian bổ sung để mô hình hóa mỗi khóa chú ý. Chúng tôi chứng minh thực nghiệm rằng hỗn hợp khóa Gaussian này (MGK) có thể nắm bắt đa dạng các mẫu chú ý, do đó giảm bớt tính dư thừa của đầu.

3. Chúng tôi mở rộng MGK của chúng tôi để sử dụng với chú ý tuyến tính và đề xuất hỗn hợp khóa tuyến tính (MLK) cho tính toán hiệu quả và dấu chân bộ nhớ tốt hơn.

4. Chúng tôi chứng minh thực nghiệm rằng Transformer-MGK và Transformer-MLK có thể so sánh hoặc tốt hơn so với các transformer cơ sở tương ứng với chú ý softmax và tuyến tính trong khi chỉ sử dụng một nửa số đầu chú ý và giảm cả độ phức tạp mô hình được đo bằng số tham số và chi phí tính toán theo FLOP.

Tổ chức: Chúng tôi cấu trúc bài báo này như sau: Trong Phần 2, chúng tôi thiết lập kết nối giữa GMM và tự chú ý và sau đó trình bày Transformer-MGK của chúng tôi và các mở rộng của nó bao gồm Transformer-MLK. Trong Phần 3, chúng tôi xác thực và phân tích thực nghiệm hiệu quả và độ chính xác của Transformer-MGK/MLK. Chúng tôi thảo luận các công trình liên quan trong Phần 4. Bài báo kết thúc với những nhận xét kết luận. Thêm chi tiết thực nghiệm được cung cấp trong Phụ lục.

2. Transformer với Hỗn hợp Khóa Gaussian

2.1. Điểm Chú ý như một Phân phối Hậu nghiệm
Trước tiên chúng tôi xem xét một truy vấn qi ∈ Q và một khóa kj ∈ K. Đặt t là một biến ngẫu nhiên nhị phân N chiều có biểu diễn 1-của-N trong đó một phần tử cụ thể tj bằng 1 và tất cả các phần tử khác bằng 0. Chúng ta sử dụng tj để chỉ vị trí j của khóa kj. Đặt I là ma trận đơn vị, chúng ta mô hình hóa phân phối p(qi) bằng GMM sau:

p(qi) = ∑(j=1 to N) πj p(qi|tj = 1) = ∑(j=1 to N) πj N(qi|kj, σj²I), (3)

trong đó πj là tiên nghiệm p(tj = 1). Cho truy vấn qi, khả năng qi khớp với khóa kj được cho bởi hậu nghiệm p(tj = 1|qi). Hậu nghiệm này được tính như sau

p(tj = 1|qi) = πj N(qi|kj, σj²) / ∑j' πj' N(qi|kj', σj'²)
= πj exp(-(||qi||² + ||kj||²)/(2σj²)) exp(qi^T kj/σj²) / ∑j' πj' exp(-(||qi||² + ||kj'||²)/(2σj'²)) exp(qi^T kj'/σj'²).

--- TRANG 3 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Chúng ta tiếp tục giả định rằng truy vấn qi và khóa kj được chuẩn hóa, và tiên nghiệm πj là đồng nhất. Chúng tôi sẽ biện minh cho những giả định này trong Nhận xét của chúng tôi ở cuối phần này. Chúng ta cũng đặt σj² = σ²/2, j = 1,2,...,K. Khi đó hậu nghiệm p(tj = 1|qi) có thể được viết như

p(tj = 1|qi) = exp(qi^T kj/σ²) / ∑j' exp(qi^T kj'/σ²). (4)

Vế phải của Công thức (4) khớp với điểm chú ý được cho trong Công thức (2) khi σ² = √D. Do đó, chúng ta chỉ ra rằng dưới các giả định đúng, điểm chú ý giữa truy vấn qi và khóa kj trong một đơn vị chú ý của transformer là hậu nghiệm p(tj = 1|qi), điều này chỉ ra trách nhiệm mà khóa kj đảm nhận để 'giải thích' truy vấn qi, điều này quyết định, ví dụ, một token ở vị trí i chú ý bao nhiêu đến một token ở vị trí j trong chuỗi đầu vào.

Nhận xét 1. Giả định rằng truy vấn qi và khóa kj được chuẩn hóa là thực tế và không nhân tạo. Trong nhiều ứng dụng, hai vector này được chuẩn hóa. (Schlag et al., 2021) chỉ ra rằng việc chuẩn hóa như vậy là để tránh bất ổn xảy ra trong quá trình huấn luyện.

Nhận xét 2. Trong thực tế, tiên nghiệm được chọn là đồng nhất khi không có kiến thức tiên nghiệm nào có sẵn.

2.2. Transformer với Hỗn hợp Khóa Gaussian: Mỗi Khóa lại là một Mô hình Hỗn hợp Gaussian

Như chúng ta đã thấy từ Công thức (4), khóa kj được sử dụng để giải thích truy vấn qi thông qua hậu nghiệm P(tj = 1|qi). Thông qua kết nối đơn giản này, mỗi truy vấn qi được coi như một mẫu từ hỗn hợp N khóa ∑(j=1 to N) πj N(qi|kj, σj²Id). Tuy nhiên, giả định rằng phân phối P(qi|tj = 1) tại mỗi tiểu quần thể là Gaussian trong Công thức (3) có thể khá mạnh vì không có đảm bảo rằng giả định này hợp lệ trong thực tế. Đặc biệt, có thể xảy ra rằng phân phối của mỗi tiểu quần thể là bất đối xứng hoặc lệch hoặc thậm chí đa phương thức. Do đó, việc sử dụng phân phối Gaussian cho mỗi tiểu quần thể có thể hạn chế khả năng giải thích và tính đa dạng của mỗi tiểu quần thể/khóa. Nó chỉ ra rằng chúng ta nên sử dụng các phân phối biểu cảm hơn để biểu diễn P(qi|tj = 1).

Hỗn hợp khóa Gaussian: Để cải thiện khả năng giải thích của mỗi khóa kj, có khả năng tăng khả năng biểu diễn của mỗi đầu chú ý, và giảm khả năng học các đầu dư thừa, chúng tôi muốn mô hình hóa nó như hỗn hợp các phân phối Gaussian. Chúng tôi gọi mô hình này là Transformer với Hỗn hợp Khóa Gaussian (Transformer-MGK). Cụ thể, trong Transformer-MGK chúng tôi mô hình hóa mỗi khóa kj tại vị trí j như một hỗn hợp M Gaussian N(kjr, σjr²I), r = 1,2,...,M. Ở đây chúng ta đang nạp chồng ký hiệu một chút và sử dụng kjr và σjr²I để biểu thị trung bình và ma trận hiệp phương sai của Gaussian thứ r tại vị trí j. Đặt z là một biến ngẫu nhiên nhị phân M chiều có biểu diễn 1-của-M. Chúng ta sử dụng zr để chỉ Gaussian thứ r trong hỗn hợp. Đặt πjr ≡ P(zr = 1|tj = 1), MGK của chúng ta có thể được viết như

P(qi|tj = 1) = ∑r P(zr = 1|tj = 1)P(qi|zr = 1, tj = 1)
= ∑r πjr N(qi|kjr, σjr²I). (5)

Động lực của chúng tôi trong việc sử dụng hỗn hợp các phân phối Gaussian để biểu diễn phân phối của mỗi tiểu quần thể trong Công thức (5) xuất phát từ kết quả xấp xỉ quan trọng sau:

Định lý 1. Giả sử P là phân phối xác suất trên [-a,a]^d cho một số a > 0 và có hàm mật độ p sao cho p khả vi và bị chặn. Khi đó, đối với bất kỳ phương sai cho trước σ > 0 và đối với bất kỳ ε > 0, tồn tại một hỗn hợp K thành phần ∑(i=1 to K) πi N(μi, σ²I) trong đó K ≤ (C log(1/ε))^d cho một số hằng số phổ quát C sao cho

sup(x∈R^d) |p(x) - ∑(i=1 to K) πi φ(x|μi, σ²I)| ≤ ε,

trong đó φ(x|μ, σ²I) là hàm mật độ của phân phối Gaussian đa biến với trung bình μ và ma trận hiệp phương sai σ²I.

Chứng minh của Định lý 1 có trong Phụ lục C. Kết quả của Định lý 1 gợi ý rằng bất kể dạng thực của P(qi|tj = 1), chúng ta có thể sử dụng hỗn hợp hữu hạn các phân phối Gaussian để xấp xỉ P(qi|tj = 1). Nó cho phép chúng ta có xấp xỉ phong phú hơn của P(qi|tj = 1) so với việc sử dụng một phân phối Gaussian đơn giản trong Công thức (3). Tương tự như phép dẫn xuất ở trên, hậu nghiệm p(tj = 1|qi) trong Transformer-MGK có thể được viết như

P(tj = 1|qi) = (∑r πjr exp(-||qi - kjr||²/(2σjr²))) / (∑j' ∑r πj'r exp(-||qi - kj'r||²/(2σj'r²))). (6)

Hơn nữa, trong Transformer-MGK, chúng tôi nới lỏng giả định rằng các truy vấn và khóa được chuẩn hóa. Do đó, khi tính P(tj = 1|qi), chúng tôi tính toán các hạt nhân Gaussian giữa các truy vấn và khóa thay vì tích vô hướng của chúng. Hậu nghiệm P(tj = 1|qi) trong Transformer-MGK khi đó được cho bởi

P(tj = 1|qi) = (∑r πjr exp(-||qi - kjr||²/(2σjr²))) / (∑j' ∑r πj'r exp(-||qi - kj'r||²/(2σj'r²))). (7)

Như đã chứng minh trong Phần 2.1, hậu nghiệm này tương ứng với điểm chú ý. Do đó, Công thức (7) là công thức để tính điểm chú ý trong Transformer-MGK. Chúng tôi tính vector đầu ra hi của tự chú ý trong Transformer-MGK

--- TRANG 4 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

như sau
hi = ∑j' [(∑r πjr exp(-||qi - kjr||²/(2σjr²))) / (∑j' ∑r πj'r exp(-||qi - kj'r||²/(2σj'r²)))] vj.

2.3. Suy luận và Học thông qua Thuật toán Expectation Maximization

Đặt ρir ≡ P(zr = 1|qi, tj = 1), trong MGK, chúng tôi áp dụng suy luận bước E trong thuật toán Expectation-Maximization (EM) để ước tính hậu nghiệm này cho truy vấn qi. Hậu nghiệm ρir cũng được biết đến như trách nhiệm mà thành phần N(kjr, σjr²I) đảm nhận để giải thích cho quan sát, trong MGK là truy vấn qi. Dưới đây chúng tôi đề xuất hai cách tiếp cận để ước tính trách nhiệm này.

Bước E Mềm Sử dụng suy luận bước E mềm, thuật toán EM thực hiện một phép gán mềm, trong đó mỗi truy vấn được liên kết với tất cả các cụm. Các trách nhiệm khi đó được cho bởi

ρir = (πjr exp(-||qi - kjr||²/(2σjr²))) / (∑r' πjr' exp(-||qi - kjr'||²/(2σjr'²))). (8)

Tại thời điểm học, các trách nhiệm được ước tính bởi Công thức (8) được sử dụng để cập nhật tiên nghiệm πjr, tức là πjr = Njr/N, trong đó N là số truy vấn và Njr = ∑(i=1 to N) ρir. Những tiên nghiệm đã cập nhật πjr này sau đó được sử dụng trong Công thức (7) để tính điểm chú ý.

Bước E Cứng Bước E cứng thực hiện một phép gán cứng của các truy vấn vào các cụm khóa, trong đó mỗi truy vấn được liên kết duy nhất với một cụm. Điều này tương tự như thuật toán K-means (Lloyd, 1982) và tương ứng với MGK ở giới hạn khi tham số phương sai σjr² tiến về 0. Theo phép dẫn xuất của K-means từ một GMM trong (Bishop, 2006), Công thức (7) trở thành

P(tj = 1|qi) = (max_r exp(-||qi - kjr||²/(2σjr²))) / (∑j' max_r exp(-||qi - kj'r||²/(2σj'r²))).

Nhận xét 3. Suy luận bước E cứng cho phép điểm chú ý được tính toán hiệu quả hơn vì các tiên nghiệm πjr không còn đóng vai trò tích cực trong thuật toán và có thể bị bỏ qua hoàn toàn.

Học thông qua Stochastic Gradient Descent (SGD) Để tăng hiệu quả của mô hình, trong MGK, chúng tôi cố định tham số phương sai σjr² bằng √D như trong chú ý softmax tiêu chuẩn và làm cho trung bình cụm, tức là các khóa, kjr trở thành tham số có thể học được. Chúng tôi cũng làm cho tiên nghiệm πjr trở thành tham số có thể học được như một trong các tùy chọn thiết kế. Trong trường hợp đó, cả kjr và πjr đều được học thông qua SGD. Việc cập nhật này thông qua SGD có thể được coi như một bước M tổng quát (Bishop, 2006).

Tùy chọn Thiết kế cho Khóa (Tùy chọn A) Chúng tôi tuân theo thiết lập tiêu chuẩn trong transformer softmax và làm cho các khóa kjr là một phép chiếu tuyến tính của đầu vào xj, tức là kjr = xjW_Kr^T, trong đó xj ∈ R^(1×Dx), W_Kr ∈ R^(D×Dx) và r = 1,2,...,M.

(Tùy chọn B) Thay thế, chúng tôi cũng làm cho các khóa kjr là phiên bản dịch chuyển của nhau để tiết kiệm tính toán, tức là kjr = xjW_K^T + br, trong đó W_K ∈ R^(D×Dx).

2.4. Transformer với Hỗn hợp Khóa Tuyến tính

MGK có thể dễ dàng được mở rộng để sử dụng với chú ý tuyến tính. Chúng tôi gọi mô hình đó là Transformer với Hỗn hợp Khóa Tuyến tính (Transformer-MLK). Trong phần này, chúng tôi áp dụng công thức của chú ý tuyến tính từ (Katharopoulos et al., 2020) để dẫn xuất Transformer-MLK. Cách tiếp cận tương tự có thể được thực hiện để dẫn xuất Transformer-MLK khi sử dụng với các chú ý tuyến tính khác như những cái trong performers (Choromanski et al., 2021) và transformer trọng số nhanh (Schlag et al., 2021). Trong Transformer-MLK, hạt nhân Gaussian trong chú ý softmax được tuyến tính hóa thành tích của các ánh xạ đặc trưng φ(·) trên các vector qi và kj. Tính chất kết hợp của phép nhân ma trận sau đó được sử dụng để dẫn xuất tính toán hiệu quả sau đây của bản đồ chú ý

hi = (φ(qi)^T ∑j ∑r πjr φ(kjr)vj^T) / (φ(qi)^T ∑j ∑r πjr φ(kjr)).

Thay thế ∑j ∑r πjr φ(qi)^T φ(kjr)vj bằng φ(qi)^T ∑j ∑r πjr φ(kjr)vj^T, như trong transformer tuyến tính, giảm chi phí bộ nhớ và tính toán của việc tính toán bản đồ chú ý trong Transformer-MLK từ O(N²) xuống O(N), làm cho Transformer-MLK có thể mở rộng với các chuỗi rất dài.

2.5. Giảm Độ phức tạp Mô hình và Chi phí Tính toán từ Transformer-MGK

Chúng tôi cung cấp phân tích chi tiết về việc giảm độ phức tạp mô hình và chi phí tính toán của Transformer-MGK so với chú ý softmax cơ sở trong Phụ lục B.

3. Kết quả Thực nghiệm

Trong phần này, chúng tôi chứng minh bằng số hiệu quả của Transformer-MGK/MLK và nghiên cứu thực nghiệm lợi thế của việc sử dụng hỗn hợp khóa trên các benchmark khác nhau, bao gồm các tác vụ khác nhau trong Long Range Arena (LRA) (Tay et al., 2021) (Phần 3.1) và mô hình hóa ngôn ngữ trên Wikitext-103 (Merity et al., 2017) (Phần 3.2). Chúng tôi nhằm chỉ ra rằng: (i) Transformer-MGK/MLK với một nửa số đầu có thể so sánh hoặc tốt hơn so với transformer softmax và tuyến tính cơ sở với số đầu đầy đủ trong khi hiệu quả hơn trong cả chi phí tính toán và dấu chân bộ nhớ; (ii) Hỗn hợp khóa giúp giảm tính dư thừa trong transformer đa đầu và có lợi cho việc học phụ thuộc dài hạn trong các chuỗi đầu vào dài; (iii) Sử dụng cùng số đầu, Transformer-MGK/MLK

--- TRANG 5 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 1. Độ chính xác Kiểm tra (%) của Transformer-MGK so với transformer softmax cơ sở trên benchmark LRA. Transformer-MGK của chúng tôi vượt trội transformer softmax trong khi sử dụng một nửa số đầu, có ít tham số hơn, và yêu cầu ít FLOP hơn (xem Hình 4). Kết quả được trung bình hóa trên 5 lần chạy.

| Mô hình | ListOps | Text | Retrieval | Trung bình |
|---------|---------|------|-----------|-----------|
| Softmax 12 heads | 36.64 | 65.62 | 82.18 | 61.48 |
| Softmax 8 heads | 37.03 | 65.71 | 81.74 | 61.49 |
| sMGK 4 heads | 37.25 | 65.51 | 82.79 | 61.85 |
| MGK 4 heads | 36.98 | 65.69 | 82.23 | 61.63 |
| Softmax 4 heads | 36.89 | 65.26 | 81.54 | 61.23 |
| sMGK 2 heads | 37.35 | 65.17 | 82.20 | 61.57 |
| MGK 2 heads | 36.88 | 65.37 | 81.83 | 61.36 |
| Softmax 2 heads | 36.76 | 64.90 | 79.1 | 60.25 |
| sMGK 1 head | 37.31 | 65.04 | 81.23 | 61.19 |
| MGK 1 head | 37.13 | 65.40 | 80.63 | 61.05 |
| Softmax 1 head | 36.81 | 64.48 | 77.9 | 59.73 |

vượt trội đáng kể so với transformer softmax và tuyến tính cơ sở. Đặc biệt trong trường hợp Transformer-MLK, nó giúp giảm khoảng cách hiệu suất giữa transformer softmax và tuyến tính.

Trong suốt phần này, chúng tôi so sánh Transformer-MGK/MLK với transformer softmax và tuyến tính có cùng hoặc gấp đôi số đầu chú ý. Trong tất cả các thí nghiệm, đối với các mô hình Transformer-MGK/MLK của chúng tôi, chúng tôi đặt M=2 trong đó M là số Gaussian, tức là khóa, tại mỗi timestep. Trong số các tùy chọn thiết kế cho Transformer-MGK được đề cập trong Phần 2.3, chúng tôi sử dụng cái có bước Soft-E nhưng làm cho tham số πjr và kjr có thể học được và cố định phương sai σjr² thành hằng số. Chúng tôi nghiên cứu cả hai triển khai cho khóa: (A) kjr là một phép chiếu tuyến tính của đầu vào xj, tức là kjr = xjW_Kr^T và (B) kjr là phiên bản dịch chuyển của nhau, tức là kjr = xjW_K^T + br.

Trong phần này, chúng tôi gọi Transformer-MGK/MLK có khóa được triển khai bởi (A) là Transformer-MGK/MLK, và có khóa được triển khai bởi (B) là Transformer-sMGK/sMLK. Chúng tôi so sánh thực nghiệm những mô hình này với các tùy chọn thiết kế khác cho Transformer-MGK trong Phần 3.4. Chi tiết về datasets, mô hình, và huấn luyện được cung cấp trong Phụ lục A.1. Mã PyTorch (Paszke et al., 2019) của chúng tôi có sẵn tại https://github.com/minhtannguyen/transformer-mgk.

3.1. Benchmark Long Range Arena (LRA)

Mô hình và cơ sở Chúng tôi so sánh Transformer-MGK và MLK 1-head, 2-head, 4-head của chúng tôi với transformer softmax (Vaswani et al., 2017) và tuyến tính (Katharopoulos et al., 2020) cơ sở có 1 head, 2 head, 4 head, và 8 head. Mỗi mô hình bao gồm hai lớp, và chúng tôi áp dụng thiết lập mô hình và huấn luyện từ (Xiong et al., 2021) trong các thí nghiệm của chúng tôi.

Kết quả Chúng tôi tóm tắt kết quả của chúng tôi trong Bảng 1. Transformer-

Bảng 2. Độ chính xác Kiểm tra (%) của Transformer-MLK so với transformer tuyến tính trên LRA. Transformer-MLK của chúng tôi đạt được độ chính xác tương đương/tốt hơn so với cơ sở trong khi sử dụng một nửa số đầu, có ít tham số hơn, và yêu cầu ít FLOP hơn (xem Hình 4 cho chi tiết). Kết quả được trung bình hóa trên 5 lần chạy.

| Mô hình | ListOps | Text | Retrieval | Trung bình |
|---------|---------|------|-----------|-----------|
| Linear 12 heads | 20.26 | 65.87 | 81.97 | 56.03 |
| Linear 8 heads | 19.17 | 65.85 | 81.18 | 55.40 |
| sMLK 4 heads | 20.11 | 65.74 | 81.53 | 55.79 |
| MLK 4 heads | 20.06 | 65.7 | 81.34 | 55.7 |
| Linear 4 heads | 19.37 | 65.81 | 81.65 | 55.61 |
| sMLK 2 heads | 19.88 | 65.61 | 81.66 | 55.71 |
| MLK 2 heads | 20.12 | 65.72 | 80.80 | 55.54 |
| Linear 2 heads | 18.35 | 65.94 | 80.94 | 55.07 |
| sMLK 1 head | 18.87 | 65.57 | 80.37 | 54.93 |
| MLK 1 head | 18.34 | 65.70 | 81.09 | 55.04 |
| Linear 1 head | 18.60 | 65.70 | 80.6 | 54.96 |

MGK với một nửa số đầu liên tục đạt được độ chính xác kiểm tra tốt hơn so với chú ý softmax cơ sở trên các tác vụ. Vì cần ít đầu hơn, transformer-MGK sử dụng ít tham số hơn và cần ít FLOP hơn để tính toán so với cơ sở. Chúng tôi cung cấp phân tích hiệu quả chi tiết cho Transformer-MGK trong Hình 4. Thú vị hơn, những lợi thế hiệu quả này của Transformer-MGK so với cơ sở trở nên đáng kể hơn khi số đầu trong mô hình cơ sở tăng lên. Khi sử dụng cùng số đầu như các mô hình cơ sở, Transformer-MGK cải thiện thêm so với những cơ sở đó. Trong số các mô hình, Transformer-sMGK thực hiện tốt nhất trên các tác vụ LRA.

Chúng tôi cũng so sánh hiệu suất của Transformer-MLK với transformer tuyến tính cơ sở trong Bảng 2. Giống như Transformer-MGK, Transformer-MLK cho kết quả tương đương hoặc tốt hơn so với cơ sở chỉ sử dụng một nửa số đầu với ít tham số và FLOP hơn. Khi sử dụng cùng số đầu, Transformer-MLK giúp cải thiện transformer tuyến tính thêm nữa.

Chúng tôi cung cấp kết quả của cơ sở 12-head trong Bảng 1 và 2 để tham khảo. Thú vị khi nhận thấy từ Bảng 1 và 2 rằng ngay cả các mô hình Transformer-MGK/MLK 2-head của chúng tôi cũng đạt được kết quả tốt hơn hoặc tương đương với cơ sở 12-head và 8-head. Một so sánh giữa cơ sở 12-head với các mô hình Transformer-MGK/MLK 6-head của chúng tôi trên tác vụ retrieval được cung cấp trong Bảng 9 trong Phụ lục A.10.

Trong Hình 1, chúng tôi so sánh đường cong mất mát huấn luyện và độ chính xác kiểm tra của Transformer-MGK/MLK 1-head và 2-head của chúng tôi với transformer softmax 2-head và transformer tuyến tính 2-head trên tác vụ truy xuất tài liệu. Tác vụ truy xuất này có độ dài chuỗi trung bình dài nhất và khoảng chú ý trong các tác vụ LRA (Tay et al., 2021). Trên tác vụ này, như được thể hiện trong Hình 1, Transformer-MGK/MLK của chúng tôi luôn tốt hơn so với các mô hình cơ sở trong suốt quá trình huấn luyện. Quan sát này khẳng định khả năng của các mô hình của chúng tôi trong việc nắm bắt

--- TRANG 6 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Hình 1. Mất mát huấn luyện và độ chính xác kiểm tra của Transformer-MGK/MLK so với transformer softmax/tuyến tính trên tác vụ retrieval, có độ dài chuỗi trung bình dài nhất và khoảng chú ý trong các tác vụ LRA (Tay et al., 2021). Hiệu suất ấn tượng của Transformer-MGK/MLK trên tác vụ thách thức này xác thực khả năng của các mô hình của chúng tôi trong việc nắm bắt các phụ thuộc tầm xa thông qua việc học đa dạng các mẫu chú ý.

các phụ thuộc tầm xa trong các chuỗi đầu vào rất dài.

3.2. Mô hình hóa Ngôn ngữ trên WikiText-103

Tiếp theo chúng tôi xác nhận lợi thế của các mô hình của chúng tôi trên một ứng dụng quy mô lớn. Chúng tôi xem xét tác vụ mô hình hóa ngôn ngữ cấp từ trên WikiText-103 (Merity et al., 2017) cho các thí nghiệm của chúng tôi trong phần này.

Mô hình và cơ sở Chúng tôi so sánh Transformer-MGK/MLK 4 và 8-head với transformer softmax (Vaswani et al., 2017) và tuyến tính (Katharopoulos et al., 2020) 8-head. Mỗi mô hình bao gồm 16 lớp. Các thí nghiệm của chúng tôi tuân theo thiết lập cho các mô hình nhỏ/trung bình từ (Schlag et al., 2021).

Kết quả Như được thể hiện trong Bảng 3, Transformer-MGK của chúng tôi vượt trội so với transformer softmax cơ sở. Ngay cả khi sử dụng một nửa số đầu chú ý (tức là 4 so với 8 đầu như trong cơ sở), Transformer-MGK vẫn đạt được perplexity kiểm tra (PPL) tốt hơn so với cơ sở. Thêm đầu vào Transformer-MGK cải thiện hiệu suất của chúng. Tương tự, Transformer-MLK đạt được PPL kiểm tra/xác thực tương đương với transformer tuyến tính cơ sở khi sử dụng một nửa số đầu chú ý. Khi sử dụng cùng số đầu chú ý như trong cơ sở, Transformer-MLK liên tục đạt được hiệu suất tốt hơn. Lưu ý rằng việc giảm số đầu từ 8 xuống 4 trong các mô hình cơ sở làm giảm đáng kể hiệu suất của chúng với hơn 1.5 giảm trong PPL kiểm tra/xác thực cho transformer softmax và hơn 1.0 giảm trong PPL kiểm tra/xác thực cho transformer tuyến tính. Transformer-MGK và Transformer-MLK đề xuất của chúng tôi giúp thu hẹp khoảng cách này.

Để kiểm tra thêm khả năng mở rộng của các mô hình của chúng tôi, chúng tôi áp dụng MGK trên một cơ sở mạnh hơn, đó là transformer softmax trung bình 8-head trong (Schlag et al., 2021). Mô hình này có 90M tham số, 16 lớp, 8 đầu chú ý mỗi lớp, và kích thước ẩn 256. Kích thước của mô hình cơ sở của chúng tôi gần với BERT Base(Devlin et al., 2019), có 110M tham số, 12 lớp, 12 đầu chú ý mỗi lớp, và kích thước ẩn 768. Áp dụng MGK của chúng tôi lên cơ sở này và chỉ sử dụng 4 đầu thay vì 8, chúng tôi cải thiện đáng kể PPL kiểm tra từ 29.60 xuống 28.86 trong khi giảm

Bảng 3. Perplexity (PPL) trên WikiText-103 của Transformer-MGK và MLK so với cơ sở. Cả Transformer-MGK và MLK đều đạt được PPL tương đương hoặc tốt hơn so với cơ sở trong khi chỉ sử dụng một nửa số đầu. Khi sử dụng cùng số đầu, các mô hình của chúng tôi cải thiện đáng kể so với cơ sở.

| Phương pháp | Valid PPL | Test PPL |
|-------------|-----------|----------|
| Softmax 8 heads (small) | 33.15 | 34.29 |
| MGK 4 heads (small) | 33.28 | 34.21 |
| sMGK 8 heads (small) | 32.92 | 33.99 |
| MGK 8 heads (small) | 32.74 | 33.93 |
| Softmax 4 heads (small) | 34.80 | 35.85 |
| Linear 8 heads (small) | 38.07 | 39.08 |
| MLK 4 heads (small) | 38.49 | 39.46 |
| MLK 8 heads (small) | 37.78 | 38.99 |
| Linear 4 heads (small) | 39.32 | 40.17 |
| Softmax 8 heads (medium) | 27.90 | 29.60 |
| MGK 4 heads (medium) | 27.58 | 28.86 |

kích thước mô hình và chi phí tính toán, chứng minh lợi thế của các mô hình được mở rộng của chúng tôi.

3.3. Dịch máy Thần kinh trên IWSLT'14 Tiếng Đức sang Tiếng Anh và WMT'14

Chúng tôi tiếp tục kiểm tra lợi thế của các phương pháp của chúng tôi trên tác vụ dịch máy IWSLT'14 Đức-Anh (Cettolo et al., 2014). Bảng 4 cho thấy rằng các mô hình Transformer-MGK và sMGK 2-head đạt được điểm BLEU 34.34 và 34.69 tương ứng, có thể so sánh và tốt hơn so với điểm BLUE 34.42 của transformer softmax cơ sở 4-head.

Chúng tôi cũng so sánh Transformer-MGK/sMGK 8-head với cơ sở softmax 16-head trên tác vụ WMT'14 trong Bảng 4. Cơ sở này bao gồm 12 lớp với 201M tham số có thể huấn luyện. Transformer-MGK/sMGK đạt được điểm BLEU 29.03 và 29.07 tương ứng, có thể so sánh với điểm BLEU cơ sở 29.38.

3.4. Phân tích Thực nghiệm

Chúng tôi tiến hành phân tích thực nghiệm dựa trên Transformer-MGK được huấn luyện cho tác vụ truy xuất tài liệu, tác vụ mô hình hóa ngôn ngữ trên WikiText-103, và tác vụ dịch máy IWSLT14 De-En

--- TRANG 7 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 4. Điểm BLEU dịch máy của Transformer-MGK 2-head trên tác vụ IWSLT14 De-En tốt hơn hoặc tương đương với cơ sở 4-head. Tương tự, điểm BLEU của Transformer-MGK 8-head trên tác vụ WMT'14 có thể so sánh với cơ sở 16-head.

| Phương pháp | Tác vụ | Điểm BLEU |
|-------------|--------|-----------|
| Softmax 4 heads | IWSLT'14 | 34.42 |
| Transformer sMGK 2 head | IWSLT'14 | 34.69 |
| Transformer MGK 2 head | IWSLT'14 | 34.34 |
| Softmax 16 heads | WMT'14 | 29.38 |
| Transformer sMGK 8 head | WMT'14 | 29.07 |
| Transformer MGK 8 head | WMT'14 | 29.03 |

machine translation task. Kết quả cho Transformer-MLK và các kết quả khác cho tác vụ WikiText-103 được cung cấp trong Phụ lục.

Khả năng của MGK trong việc xấp xỉ phân phối xác suất của các truy vấn Chúng tôi tính toán điểm negative log-likelihood (NLL) của MGK so với cơ sở chỉ sử dụng 1 phân phối Gaussian tập trung quanh khóa kj, j = 1,...,N. Cả hai mô hình đều được huấn luyện cho tác vụ truy xuất tài liệu. NLL của MGK là 0.180 và 0.182, nhỏ hơn NLL của cơ sở là 0.187 và 0.292, ở lớp 1 và 2 tương ứng, chỉ ra rằng MGK khớp với phân phối của các truy vấn chú ý tốt hơn.

Nghiên cứu loại bỏ về phương sai của MGK Trong các thí nghiệm của chúng tôi, chúng tôi đặt M=2 và σ²j1 = √D như trong transformer softmax cơ sở và σ²j2 = 3√D trong đó D là chiều đặc trưng được định nghĩa trong Phần 1.1. Chúng tôi tiến hành nghiên cứu loại bỏ trên tác vụ dịch máy IWSLT14 De-En trong đó chúng tôi thay đổi σ²j2. Chúng tôi quan sát thấy rằng điểm BLEU của các mô hình được huấn luyện không khác biệt nhiều giữa các giá trị khác nhau của σ²j2. Cụ thể, điểm BLEU là 34.31, 34.36, 34.61, 34.34, và 34.27 cho σ²j2 = 0.5√D, √D, 2√D, 3√D và 4√D tương ứng. Chúng tôi cũng làm cho σ²j1 và σ²j2 có thể học được và quan sát thấy rằng điểm BLEU thu được tệ hơn (32.86).

Transformer-MGK giúp tránh học các đầu dư thừa Chúng tôi so sánh trực quan các ma trận chú ý được học bởi Transformer-MGK và transformer softmax cơ sở trên tác vụ truy xuất tài liệu trong Hình 2. Cụ thể, chúng tôi chọn ngẫu nhiên một ma trận chú ý tại mỗi đầu trong mỗi lớp và trực quan hóa ma trận chú ý đó cho mỗi mô hình để so sánh. Hình 2(Trái) cho thấy rằng các truy vấn trong Transformer-MGK có thể chú ý đến nhiều khóa khác nhau và tương đương với các token khác tại các vị trí khác nhau trong chuỗi đầu vào. Sự đa dạng trong mẫu chú ý này giúp giảm khả năng mô hình học các ma trận chú ý tương tự và dư thừa tại các đầu khác nhau một cách đáng kể.

Một thước đo khác để đo khả năng biểu diễn của ma trận chú ý là rank của nó. Các ma trận chú ý có rank cao có thể nắm bắt các mẫu chú ý đa dạng hơn so với những cái có rank thấp (Nguyen et al., 2021b). Chúng tôi nghiên cứu

Bảng 5. Hiệu suất của Transformer-MGK sử dụng các kỹ thuật suy luận/học khác nhau trên benchmark LRA.

| Mô hình | ListOps | Text | Retrieval | Trung bình |
|---------|---------|------|-----------|-----------|
| sMGK + Hard-E 1 head | 37.25 | 64.7 | 81.29 | 61.08 |
| sMGK + Soft-E 1 head | 37.05 | 64.68 | 81.44 | 61.05 |
| sMGK 1 head | 37.31 | 65.04 | 81.23 | 61.19 |
| MGK + Hard-E 1 head | 19.40 | 65.40 | 80.72 | 55.17 |
| MGK + Soft-E 1 head | 33.85 | 65.25 | 80.73 | 59.94 |
| MGK 1 head | 37.13 | 65.40 | 80.63 | 61.05 |

rank của ma trận chú ý từ Transformer-MGK và transformer softmax được huấn luyện cho tác vụ retrieval. Cụ thể, chúng tôi chọn ngẫu nhiên 1000 ma trận chú ý khác nhau tại mỗi lớp từ mỗi mô hình. Sau đó, chúng tôi thực hiện phân rã giá trị đơn (SVD) để tính rank của mỗi ma trận và ngưỡng các giá trị đơn nhỏ hơn 10⁻⁶. Hình 2(Phải) trình bày phân phối rank của các ma trận chú ý tại mỗi lớp của Transformer-MGK và transformer softmax. Chúng tôi quan sát thấy rằng các ma trận chú ý trong Transformer-MGK có rank cao hơn so với những cái trong transformer softmax. Do đó, chú ý của chúng tôi với MGK có khả năng nắm bắt các mẫu chú ý đa dạng và phức tạp hơn so với chú ý softmax cơ sở.

So sánh các kỹ thuật suy luận và học khác nhau Bảng 5 so sánh hiệu suất của Transformer-MGK sử dụng các tùy chọn thiết kế khác nhau được đề cập trong Phần 2.3 trên benchmark LRA. Cụ thể, chúng tôi xem xét ba tùy chọn thiết kế sau: A) bước Soft-E, các tham số πjr và kjr có thể học được thông qua SGD, và phương sai σ²jr là hằng số, B) bước Soft-E, tham số πjr được cập nhật theo cập nhật bước M, kjr có thể học được thông qua SGD, và phương sai σ²jr là hằng số, và C) bước Hard-E, πjr và kjr có thể học được thông qua SGD, và phương sai σ²jr là hằng số. Lưu ý rằng Transformer-MGK với thiết lập A là các mô hình mặc định chúng tôi sử dụng trong tất cả các thí nghiệm ở trên. Trong Bảng 5, Transformer-MGK + Hard-E là Transformer-MGK với thiết lập C, Transformer-MGK + Soft-E là Transformer-MGK với thiết lập B, và Transformer-MGK chỉ là Transformer-MGK với thiết lập A. Đáng chú ý rằng Transformer-sMGK + Hard-E đạt được kết quả có thể so sánh với các mô hình có hiệu suất tốt nhất trong mỗi tác vụ mặc dù nó là mô hình hiệu quả nhất trong nghiên cứu của chúng tôi.

Transformer-MGK giảm độ phức tạp mô hình và chi phí tính toán Hình 3A và 3B trình bày tỷ lệ giảm FLOP huấn luyện và kiểm tra tương ứng của Transformer-MGK 4-head của chúng tôi so với cơ sở Softmax 8-head như các hàm của chiều mô hình D và độ dài chuỗi N. Trong Hình 3C, chúng tôi hiển thị tỷ lệ giảm kích thước mô hình của Transformer-MGK so với cơ sở khi D thay đổi. Chúng tôi quan sát thấy rằng lợi thế hiệu quả của Transformer-MGK so với cơ sở tăng theo D và N, làm cho nó phù hợp hơn và vượt trội cho các ứng dụng quy mô lớn. Lưu ý rằng kích thước mô hình theo số tham số không phụ thuộc

--- TRANG 8 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

[THIS IS FIGURE: Complex visualization showing attention matrices and rank distributions across different models and layers]

Hình 2. (Trái) Trực quan hóa các ma trận chú ý trong transformer softmax cơ sở 4-head (trái), Transformer-MGK 4-head (giữa), và Transformer-MGK 2-head (phải) được huấn luyện trên tác vụ truy xuất tài liệu. Các ma trận chú ý từ Transformer-MGK của chúng tôi có mẫu đa dạng hơn so với transformer softmax cơ sở, giảm nguy cơ học các đầu dư thừa. (Phải) Phân phối rank của các ma trận chú ý cho thấy rằng các ma trận chú ý trong Transformer-MGK có rank cao hơn so với những cái trong transformer softmax và do đó có thể nắm bắt các mẫu chú ý đa dạng hơn.

[THIS IS FIGURE: Three graphs showing training/testing FLOP ratios and parameter ratios across different sequence lengths and model dimensions]

Hình 3. Tỷ lệ FLOP Huấn luyện (A), Suy luận (B) và tỷ lệ số tham số (C) giữa Transformer-MGK 4-head với cơ sở softmax 8-head trên các chiều mô hình D khác nhau và độ dài chuỗi N, cho tác vụ mô hình hóa ngôn ngữ trên WikiText-103. Transformer-MGK 4-head hiệu quả đáng kể, cả về tính toán và độ phức tạp mô hình, so với cơ sở khi D và N tăng, chỉ ra lợi ích của phương pháp của chúng tôi cho các tác vụ tầm xa và quy mô lớn.

[THIS IS FIGURE: Two comparative charts showing computational cost and parameters for different numbers of heads]

Hình 4. Chi phí tính toán (FLOP) và số tham số của Transformer-MGK so với transformer softmax cơ sở (Trái) và Transformer-MLK so với transformer tuyến tính cơ sở (Phải) được huấn luyện trên tác vụ Retrieval. Lợi thế hiệu quả của Transformer-MGK/MLK so với cơ sở trong cả hai thước đo tăng theo số đầu.

vào độ dài chuỗi N. Phân tích hiệu quả kết quả về tác vụ mô hình hóa ngôn ngữ này cho Transformer-MLK và các thước đo phân tích được cung cấp trong Hình 10 trong Phụ lục A.5. Ngoài ra, Hình 4 so sánh chi phí tính toán, được đo bằng FLOP, và độ phức tạp mô hình, được đo bằng số tham số, giữa Transformer-MGK/MLK của chúng tôi có một nửa số đầu và transformer softmax/tuyến tính đầy đủ đầu như các hàm của số đầu. Các mô hình được huấn luyện cho tác vụ Retrieval trong benchmark LRA. Càng nhiều đầu được sử dụng, Transformer-MGK/MLK càng có lợi thế hơn so với transformer softmax/tuyến tính. Đối với các mô hình transformer lớn hơn nhiều, việc tiết kiệm này là đáng kể.

4. Công trình Liên quan

Transformer Hiệu quả Transformer hiệu quả có thể được phân loại thành nhiều danh mục, như được tóm tắt trong (Roy et al., 2021). Trong số các danh mục này là các mô hình thiết kế ma trận chú ý có cấu trúc thưa (Parmar et al., 2018; Liu et al., 2018; Qiu et al., 2019; Child et al., 2019; Beltagy et al., 2020; Du et al., 2021). Danh mục khác bao gồm các mô hình kết hợp hai hoặc nhiều mẫu truy cập khác nhau để cải thiện độ bao phủ (Child et al., 2019; Ho et al., 2019). Các mẫu truy cập cũng có thể được làm cho có thể học được theo cách dựa trên dữ liệu (Kitaev et al., 2020; Roy et al., 2021; Tay et al., 2020). Các transformer hiệu quả khác tận dụng mô-đun bộ nhớ phụ để truy cập nhiều token cùng một lúc (Lee et al., 2019; Sukhbaatar et al., 2019; Asai & Choi, 2020; Beltagy et al., 2020). Được truyền cảm hứng từ việc sử dụng các phương pháp momentum trong việc tăng tốc huấn luyện mạng neural (Sutskever et al., 2013; Bengio et al., 2013; Kingma & Ba, 2015; Wang et al., 2022) và thiết kế kiến trúc mạng neural (Li et al., 2018; He et al., 2020; Nguyen et al., 2020; Xia et al., 2021; Sander et al., 2021), (Wang et al., 2021a) kết hợp momentum thích ứng vào transformer tuyến tính để cải thiện độ chính xác của nó trong khi duy trì độ phức tạp tính toán và bộ nhớ tuyến tính. Hơn nữa, suy luận động được đề xuất trước đó cho mạng neural sâu (Wang et al., 2018a;b; 2020b; Han et al., 2021) cũng có thể được sử dụng để tăng tốc suy luận trong transformer (Bakhtiarnia et al.,

--- TRANG 9 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

2021; Wang et al., 2021b). Xấp xỉ rank thấp và kernelization cũng được sử dụng để tăng cường hiệu quả bộ nhớ và tính toán của việc tính toán tự chú ý (Tsai et al., 2019; Wang et al., 2020a; Katharopoulos et al., 2020; Choromanski et al., 2021; Shen et al., 2021; Nguyen et al., 2021b; Peng et al., 2021). Ngoài các transformer hiệu quả được đề cập ở trên, chú ý đa truy vấn chia sẻ khóa và giá trị giữa các đầu chú ý khác nhau (Shazeer, 2019) cũng đã được nghiên cứu để giảm chi phí băng thông bộ nhớ và tăng tốc độ cho suy luận transformer gia tăng (xem Phụ lục A.11). Đáng chú ý rằng các phương pháp truy xuất hiệu quả đã được sử dụng để tăng hiệu quả của các mô hình học máy bao gồm transformer (Chen et al., 2020; Le & Lauw, 2020; 2021; Mittal et al., 2022). Cuối cùng nhưng không kém phần quan trọng, các phương pháp như sử dụng mất mát phụ trợ (Al-Rfou et al., 2019) và embedding đầu vào thích ứng (Baevski & Auli, 2019) đã được khám phá để tăng tốc sự hội tụ của việc huấn luyện transformer. MGK/MLK của chúng tôi có thể dễ dàng được kết hợp vào các phương pháp trên để cải thiện thêm độ chính xác và hiệu quả của chúng.

Tính dư thừa trong Transformer Các công trình gần đây gợi ý rằng hầu hết các neuron và đầu trong transformer được huấn luyện trước là dư thừa và có thể được loại bỏ khi tối ưu hóa hướng tới một tác vụ downstream (Dalvi et al., 2020; Michel et al., 2019; Durrani et al., 2020). Các công trình khác cũng nghiên cứu các embedding được ngữ cảnh hóa trong các mạng được huấn luyện trước dưới tính dư thừa này do tham số hóa quá mức và chỉ ra rằng các biểu diễn được học trong các mô hình này có tính anisotropic cao (Mu & Viswanath, 2018; Ethayarajh, 2019). Một nhóm công trình mới nổi được đề xuất để chưng cất và cắt tỉa mô hình, bao gồm (Sanh et al., 2019; Sun et al., 2019; Voita et al., 2019b; Sajjad et al., 2020). Cách tiếp cận MGK/MLK của chúng tôi có thể được kết hợp với các phương pháp chưng cất và cắt tỉa này.

Mô hình Hỗn hợp cho Transformer Một số công trình đã sử dụng mô hình hỗn hợp để nghiên cứu và cải thiện transformer. Switch transformer (Fedus et al., 2021) sử dụng thuật toán định tuyến trong Mixture of Experts (MoE) để giảm chi phí truyền thông và tính toán trong transformer. (Nguyen et al., 2018; Patel et al., 2016; 2015) dẫn xuất một khung xác suất dựa trên GMM cho mạng neural sâu có thể được mở rộng để nghiên cứu transformer và kiến trúc dựa trên chú ý. (Zhang & Feng, 2021) phát triển một chú ý hỗn hợp Gaussian mô hình hóa mỗi điểm chú ý như một GMM. Ngược lại, MGK mô hình hóa mỗi khóa kj tại timestep j như một GMM. Nói cách khác, MGK của chúng tôi là một cách tiếp cận mô hình sinh trong đó chúng tôi xây dựng một mô hình sinh cho khóa kj và dẫn xuất điểm chú ý từ phân phối hậu nghiệm trong khi chú ý hỗn hợp Gaussian trong (Zhang & Feng, 2021) là một cách tiếp cận phân biệt (Bishop, 2006). Các công trình khác sử dụng mô hình hỗn hợp với mạng neural sâu và transformer bao gồm (Cho et al., 2020; Guo et al., 2019; Huang et al., 2020).

5. Nhận xét Kết luận

Trong bài báo này, chúng tôi đề xuất Transformer-MGK, một lớp transformer sử dụng mô hình hỗn hợp Gaussian để biểu diễn các vector khóa trong tự chú ý. Transformer-MGK giảm tính dư thừa giữa các đầu trong transformer. Hơn nữa, các đầu chú ý trong Transformer-MGK có khả năng biểu diễn tốt hơn so với những cái trong cơ sở, cho phép Transformer-MGK đạt được hiệu suất có thể so sánh hoặc tốt hơn so với transformer softmax cơ sở trong khi chỉ sử dụng một nửa số đầu. So với cơ sở, Transformer-MGK sử dụng ít tham số hơn và yêu cầu ít FLOP hơn để tính toán. Chúng tôi mở rộng Transformer-MGK thành Transformer-MLK để sử dụng chú ý tuyến tính cho hiệu quả tốt hơn. Chúng tôi xác thực thực nghiệm lợi thế của Transformer-MGK/MLK so với transformer softmax và tuyến tính cơ sở trên các benchmark khác nhau bao gồm các tác vụ trong benchmark LRA, mô hình hóa ngôn ngữ WikiText-103, và dịch máy IWSLT'14/WMT'14. Cả Transformer-MGK/MLK và transformer softmax/tuyến tính đều giả định rằng các đặc trưng trong các truy vấn và khóa chú ý là độc lập. (Nguyen et al., 2022) gợi ý rằng các ước tính tích phân Fourier có thể được sử dụng để nắm bắt hiệu quả cấu trúc phụ thuộc giữa những đặc trưng embedding đó. Các ước tính tích phân Fourier có thể được kết hợp vào Transformer-MGK/MLK để tăng khả năng biểu diễn của các mô hình. Trong công trình của chúng tôi, chúng tôi làm cho trung bình và phương sai của cụm trở thành các biến có thể học được và hằng số tương ứng. Thật thú vị khi khám phá cách tận dụng cập nhật bước M trong thuật toán EM để cập nhật những tham số đó. Vì thuật toán EM có thể có tốc độ hội tụ dưới tuyến tính do sự phân tách yếu của trung bình và phương sai (Dwivedi et al., 2020b;a), chúng ta có thể cần phát triển các thuật toán tối ưu hóa hiệu quả hơn thuật toán EM để ước tính trung bình và phương sai. Các công trình hiện tại của (Ho et al., 2022; Ren et al., 2022) chứng minh rằng một lịch tốc độ học tăng theo cấp số nhân cho gradient descent có thể được sử dụng cho trung bình và phương sai để đạt được tốc độ hội tụ tuyến tính của các tham số. Việc điều tra hiệu suất của thuật toán đó trong Transformer-MGK/MLK có tầm quan trọng thực tế. Cuối cùng, chúng tôi để lại việc áp dụng MGK/MLK để cải thiện vision transformer (Dosovitskiy et al., 2020; Touvron et al., 2020) như công việc tương lai.

Lời cảm ơn
Tài liệu này dựa trên nghiên cứu được tài trợ bởi AFOSR MURI FA9550-18-1-0502, tài trợ ONR N00014-20-1-2093, MURI N00014-20-1-2787, và NSF dưới Grant# 2030859 cho Computing Research Association cho Dự án CIFellows (CIF2020-UCLA-38). NH biết ơn sự hỗ trợ từ giải thưởng NSF IFML 2019844 và quà tặng nghiên cứu bởi tài trợ UT Austin ML. RGB được hỗ trợ bởi các tài trợ NSF CCF-1911094, IIS-1838177, IIS-1730574, và Vannevar Bush Faculty Fellowship.

--- TRANG 10 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Tài liệu tham khảo

Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3159–3166, 2019.

Asai, A. and Choi, E. Challenges in information seeking qa: Unanswerable questions and paragraph retrieval. arXiv preprint arXiv:2010.11915, 2020.

Bacharoglou, A. G. Approximation of probability distributions by convex mixtures of Gaussian measures. Proceedings of the American Mathematical Society, 138: 2619–2628, 2010.

Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ByxZX20qFQ.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Bakhtiarnia, A., Zhang, Q., and Iosifidis, A. Multi-exit vision transformer for dynamic inference. arXiv preprint arXiv:2106.15183, 2021.

Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. Advances in optimizing recurrent networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 8624–8628. IEEE, 2013.

Bishop, C. M. Pattern recognition. Machine learning, 128(9), 2006.

Brown, T. and et al. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901, 2020.

Cettolo, M., Niehues, J., Stüker, S., Bentivogli, L., and Federico, M. Report on the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam, volume 57, 2014.

Chen, B., Liu, Z., Peng, B., Xu, Z., Li, J. L., Dao, T., Song, Z., Shrivastava, A., and Re, C. Mongoose: A learnable lsh framework for efficient neural network training. In International Conference on Learning Representations, 2020.

Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724–1734, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.org/anthology/D14-1179.

Cho, S. M., Park, E., and Yoo, S. Meantime: Mixture of attention mechanisms with multi-temporal embeddings for sequential recommendation. In Fourteenth ACM Conference on Recommender Systems, pp. 515–520, 2020.

Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH.

Clark, K., Khandelwal, U., Levy, O., and Manning, C. D. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://www.aclweb.org/ anthology/W19-4828.

Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

Dalvi, F., Sajjad, H., Durrani, N., and Belinkov, Y. Analyzing redundancy in pretrained transformer models. arXiv preprint arXiv:2004.04010, 2020.

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for

--- TRANG 11 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021.

Durrani, N., Sajjad, H., Dalvi, F., and Belinkov, Y. Analyzing individual neurons in pre-trained language models. arXiv preprint arXiv:2010.02695, 2020.

Dwivedi, R., Ho, N., Khamaru, K., Wainwright, M. J., Jordan, M. I., and Yu, B. Sharp analysis of expectation-maximization for weakly identifiable models. AISTATS, 2020a.

Dwivedi, R., Ho, N., Khamaru, K., Wainwright, M. J., Jordan, M. I., and Yu, B. Singularity, misspecification, and the convergence rate of EM. Annals of Statistics, 44: 2726–2755, 2020b.

Ethayarajh, K. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019.

Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.

Ghosal, S. and van der Vaart, A. Entropies and rates of convergence for maximum likelihood and bayes estimation for mixtures of normal densities. Ann. Statist., 29: 1233–1263, 2001.

Guo, M., Zhang, Y., and Liu, T. Gaussian transformer: a lightweight approach for natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6489–6496, 2019.

Han, Y., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y. Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

Hanson, S. J. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena, 42(1-3):265–272, 1990.

He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729–9738, 2020.

Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733–2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1275. URL https://www.aclweb.org/anthology/D19-1275.

Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.

Ho, N., Ren, T., Sanghavi, S., Sarkar, P., and Ward, R. An exponentially increasing step-size for parameter estimation in statistical models. arXiv preprint arXiv: 2205.07999, 2022.

Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://www.aclweb.org/anthology/P18-1031.

Huang, Y., Gornet, J., Dai, S., Yu, Z., Nguyen, T., Tsao, D., and Anandkumar, A. Neural networks with recurrent generative feedback. Advances in Neural Information Processing Systems, 33:535–545, 2020.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020.

Kim, Y., Denton, C., Hoang, L., and Rush, A. M. Structured attention networks. arXiv preprint arXiv:1702.00887, 2017.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.

Le, D. D. and Lauw, H. Efficient retrieval of matrix factorization-based top-k recommendations: A survey of recent approaches. Journal of Artificial Intelligence Research, 70:1441–1479, 2021.

--- TRANG 12 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Le, D. D. and Lauw, H. W. Stochastically robust personalized ranking for lsh recommendation retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 4594–4601, 2020.

Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pp. 3744–3753. PMLR, 2019.

Li, H., Yang, Y., Chen, D., and Lin, Z. Optimization algorithm inspired deep neural network structure design. In Asian Conference on Machine Learning, pp. 614–629. PMLR, 2018.

Lin, Z., Feng, M., dos Santos, C. N., Yu, M., Xiang, B., Zhou, B., and Bengio, Y. A structured self-attentive sentence embedding. CoRR, abs/1703.03130, 2017.

Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Lloyd, S. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137, 1982.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/P11-1015.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=Byj72udxe.

Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

Mittal, S., Raparthy, S. C., Rish, I., Bengio, Y., and Lajoie, G. Compositional attention: Disentangling search and retrieval. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=IwJPj2MBcIa.

Mu, J. and Viswanath, P. All-but-the-top: Simple and effective postprocessing for word representations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HkuGJ3kCb.

Nangia, N. and Bowman, S. ListOps: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 92–99, New Orleans, Louisiana, USA, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-4013. URL https://www.aclweb.org/ anthology/N18-4013.

Nguyen, T., Ho, N., Patel, A., Anandkumar, A., Jordan, M. I., and Baraniuk, R. G. A Bayesian perspective of convolutional neural networks through a deconvolutional generative model. arXiv preprint arXiv:1811.02657, 2018.

Nguyen, T., Baraniuk, R., Bertozzi, A., Osher, S., and Wang, B. Momentumrnn: Integrating momentum into recurrent neural networks. Advances in Neural Information Processing Systems, 33:1924–1936, 2020.

Nguyen, T., Nguyen, P., Pham, H., Bui, T., Nguyen, T., and Luong, D. Sp-gpt2: Semantics improvement in vietnamese poetry generation. In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 1576–1581. IEEE, 2021a.

Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher, S. J., and Ho, N. Transformer with Fourier integral attentions. arXiv preprint arXiv:2206.00206, 2022.

Nguyen, T. M., Suliafu, V., Osher, S. J., Chen, L., and Wang, B. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention. arXiv preprint arXiv:2108.02347, 2021b.

Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.

Parikh, A., Täckström, O., Das, D., and Uszkoreit, J. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2249–2255, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1244. URL https://www.aclweb.org/anthology/D16-1244.

Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of

--- TRANG 13 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Proceedings of Machine Learning Research, pp. 4055–4064. PMLR, 10–15 Jul 2018. URL http://proceedings. mlr.press/v80/parmar18a.html.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019.

Patel, A. B., Nguyen, T., and Baraniuk, R. G. A probabilistic theory of deep learning. arXiv preprint arXiv:1504.00641, 2015.

Patel, A. B., Nguyen, M. T., and Baraniuk, R. A probabilistic framework for deep learning. Advances in neural information processing systems, 29:2558–2566, 2016.

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N., and Kong, L. Random feature attention. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB.

Qiu, J., Ma, H., Levy, O., Yih, S. W.-t., Wang, S., and Tang, J. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.

Radev, D. R., Muthukrishnan, P., Qazvinian, V., and Abu-Jbara, A. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training. OpenAI report, 2018.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/anthology/D16-1264.

Ren, T., Cui, F., Sanghavi, S., and Ho, N. Beyond EM algorithm on over-specified two-component location-scale Gaussian mixtures. arXiv preprint arXiv: 2205.11078, 2022.

Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. doi: 10.1162/tacl a00353. URL https://www.aclweb.org/anthology/2021.tacl-1.4.

Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. Poor man's bert: Smaller and faster transformer models. arXiv e-prints, pp. arXiv–2004, 2020.

Sander, M. E., Ablin, P., Blondel, M., and Peyré, G. Momentum residual neural networks. In International Conference on Machine Learning, pp. 9276–9287. PMLR, 2021.

Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355–9366. PMLR, 2021.

Shazeer, N. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.

Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3531–3539, 2021.

So, D. R., Liang, C., and Le, Q. V. The evolved transformer. arXiv preprint arXiv:1901.11117, 2019.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.

Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., and Joulin, A. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019.

Sun, S., Cheng, Y., Gan, Z., and Liu, J. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.

Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139–1147. PMLR, 2013.

Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse Sinkhorn attention. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9438–9447. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/tay20a. html.

Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.

--- TRANG 14 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= qVyeW-grC2k.

Tenney, I., Das, D., and Pavlick, E. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593–4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https://www.aclweb.org/ anthology/P19-1452.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.

Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. arXiv preprint arXiv:1908.11775, 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017.

Vig, J. and Belinkov, Y. Analyzing the structure of attention in a transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 63–76, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4808. URL https://www.aclweb.org/anthology/W19-4808.

Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5797–5808, Florence, Italy, July 2019a. Association for Computational Linguistics. doi: 10.18653/v1/P19-1580. URL https://www.aclweb.org/anthology/P19-1580.

Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019b.

Wang, B., Xia, H., Nguyen, T., and Osher, S. How does momentum benefit deep neural networks architecture design? a few case studies. arXiv preprint arXiv:2110.07034, 2021a.

Wang, B., Nguyen, T., Sun, T., Bertozzi, A. L., Baraniuk, R. G., and Osher, S. J. Scheduled restart momentum for accelerated stochastic gradient descent. SIAM Journal on Imaging Sciences, 15(2):738–761, 2022.

Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020a.

Wang, X., Yu, F., Dou, Z.-Y., Darrell, T., and Gonzalez, J. E. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 409–424, 2018a.

Wang, Y., Nguyen, T., Zhao, Y., Wang, Z., Lin, Y., and Baraniuk, R. Energynet: Energy-efficient dynamic inference. 2018b.

Wang, Y., Shen, J., Hu, T.-K., Xu, P., Nguyen, T., Baraniuk, R., Wang, Z., and Lin, Y. Dual dynamic inference: Enabling more efficient, adaptive, and controllable deep inference. IEEE Journal of Selected Topics in Signal Processing, 14(4):623–633, 2020b.

Wang, Y., Huang, R., Song, S., Huang, Z., and Huang, G. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. Advances in Neural Information Processing Systems, 34, 2021b.

Williams, A., Nangia, N., and Bowman, S. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, June 2018. doi: 10.18653/v1/N18-1101. URL https:// www.aclweb.org/anthology/N18-1101.

Xia, H., Suliafu, V., Ji, H., Nguyen, T., Bertozzi, A., Osher, S., and Wang, B. Heavy ball neural ordinary differential equations. Advances in Neural Information Processing Systems, 34, 2021.

Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention. 2021.

Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q. V. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.

Zhang, S. and Feng, Y. Modeling concentrated cross-attention for neural machine translation with Gaussian mixture model. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 1401–1411, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.findings-emnlp.121. URL https://aclanthology. org/2021.findings-emnlp.121.

--- TRANG 15 ---
Phụ lục cho "Transformer với Hỗn hợp Khóa Gaussian"

Trong tài liệu bổ sung này, chúng tôi cung cấp chi tiết thực nghiệm và các thí nghiệm bổ sung của Transformer-MGK và Transformer-MLK.

A. Thí nghiệm Bổ sung

A.1. Chi tiết thí nghiệm

Trong phần này, chúng tôi cung cấp chi tiết mô hình và huấn luyện cho các thí nghiệm trong Phần 3. Tất cả các thí nghiệm của chúng tôi được tiến hành trên một máy chủ với 4 GPU NVIDIA A100.

A.1.1. BENCHMARK LONG RANGE ARENA

Dataset và thước đo Chúng tôi xem xét các tác vụ sau trong benchmark LRA: Listops (Nangia & Bowman, 2018), phân loại văn bản đánh giá IMDb cấp byte (Maas et al., 2011), và truy xuất tài liệu cấp byte (Radev et al., 2013). Những tác vụ này liên quan đến các chuỗi dài có độ dài 2K, 4K, và 4K tương ứng. Chúng tôi tuân theo thiết lập/giao thức đánh giá trong (Tay et al., 2021) và báo cáo độ chính xác kiểm tra cho từng tác vụ riêng lẻ và kết quả trung bình trên tất cả các tác vụ.

Mô hình và cơ sở Chúng tôi sử dụng transformer softmax (Vaswani et al., 2017) và transformer tuyến tính (Katharopoulos et al., 2020) làm cơ sở của chúng tôi. Tất cả các mô hình có 2 lớp, 64 chiều embedding, và 128 chiều ẩn. Số đầu trong mỗi lớp được đặt là 1, 2, 4, và 8. Đối với Transformer-MGK/MLK và các phiên bản dịch chuyển của chúng, chúng tôi chia sẻ πjr cho tất cả vị trí j và học nó cho mỗi đầu. Giá trị khởi tạo cho mỗi πjr được đặt là 0.5. Đối với Transformer-sMGK, chúng tôi học br và khởi tạo các phần tử của nó từ phân phối chuẩn tắc. Mỗi σjr² là một hằng số với giá trị √D trong đó D là chiều của mỗi đầu, giống như trong các mô hình cơ sở

Chi tiết về các benchmark Long Range Arena (LRA) có thể được tìm thấy trong bài báo gốc (Tay et al., 2021). Triển khai của chúng tôi dựa trên mã công khai của (Xiong et al., 2021), và chúng tôi tuân theo các thủ tục huấn luyện của họ. Thiết lập huấn luyện và chi tiết mô hình cơ sở bổ sung được cung cấp trong tệp cấu hình được sử dụng trong (Xiong et al., 2021) và có sẵn tại https://github.com/mlpen/Nystromformer/blob/main/LRA/code.

A.1.2. MÔ HÌNH HÓA NGÔN NGỮ TRÊN WIKITEXT-103

Dataset và thước đo WikiText-103 bao gồm các bài báo từ Wikipedia và là một dataset với các phụ thuộc ngữ cảnh dài. Tập huấn luyện được tạo thành từ khoảng 28K bài báo chứa 103M từ chạy; điều này tương ứng với các khối văn bản khoảng 3600 từ. Các tập xác thực và kiểm tra được tạo thành từ 218K và 246K từ chạy tương ứng. Mỗi tập chứa 60 bài báo và khoảng 268K từ. Thí nghiệm của chúng tôi tuân theo thiết lập tiêu chuẩn (Merity et al., 2017; Schlag et al., 2021) và chia dữ liệu huấn luyện thành các đoạn dài độc lập L-từ. Để đánh giá, chúng tôi sử dụng kích thước batch là 1, và đi qua chuỗi văn bản với cửa sổ trượt có kích thước L. Chúng tôi chỉ xem xét vị trí cuối cùng để tính perplexity (PPL) ngoại trừ trong đoạn đầu tiên, nơi tất cả các vị trí được đánh giá như trong (Al-Rfou et al., 2019; Schlag et al., 2021).

Mô hình và cơ sở Triển khai mô hình hóa ngôn ngữ của chúng tôi dựa trên mã công khai https://github.com/IDSIA/lmtool-fwp của (Schlag et al., 2021). Chúng tôi sử dụng các cấu hình mô hình nhỏ và trung bình của họ cho các mô hình trong thí nghiệm của chúng tôi. Cụ thể, đối với các mô hình nhỏ, chúng tôi đặt chiều khóa, giá trị, và truy vấn là 128, và độ dài ngữ cảnh huấn luyện và đánh giá là 256. Đối với các mô hình trung bình, chúng tôi đặt chiều khóa, giá trị, và truy vấn là 256, và độ dài ngữ cảnh huấn luyện và đánh giá là 384. Trong cả hai cấu hình, chúng tôi đặt số đầu là 8, chiều lớp feed-forward là 2048, và số lớp là 16. Đối với Transformer-MGK/MLK, chúng tôi sử dụng các phiên bản 4 và 8-head của chúng tôi để so sánh với cơ sở 8-head. ρir, br và πjr cho tác vụ này tuân theo thiết lập của chúng tôi cho các thí nghiệm LRA. Ngoài những điều đó, mô hình hóa ngôn ngữ của chúng tôi chia sẻ cùng cấu hình với các cơ sở.

Chúng tôi huấn luyện các mô hình của chúng tôi cho mô hình hóa ngôn ngữ trên 2 A100, mỗi cái 40GB với kích thước batch 96, và mỗi mô hình được huấn luyện trong 120 epoch. Chúng tôi áp dụng 10% dropout (Hanson, 1990; Srivastava et al., 2014) và sử dụng trình tối ưu Adam (Kingma & Ba, 2015) với tốc độ học ban đầu 0.00025 và 2000 bước để khởi động tốc độ học.

A.1.3. DỊCH MÁY THẦN KINH TRÊN IWSLT'14 TIẾNG ĐỨC SANG TIẾNG ANH

Dataset và thước đo Dataset IWSLT14 Đức-Anh (Cettolo et al., 2014) chứa 153K huấn luyện, 7K xác thực, và 7K kiểm tra các câu dịch Đức-Anh từ kịch bản TED-talks. Chúng tôi tuân theo cùng các bước tiền xử lý như trong (Ott

--- TRANG 16 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

et al., 2019). Chúng tôi sử dụng điểm BiLingual Evaluation Understudy (BLEU) làm thước đo đánh giá của chúng tôi cho tác vụ này. Tất cả các mô hình được huấn luyện được đánh giá bằng giao thức đánh giá trong (Ott et al., 2019).

Mô hình và cơ sở Mô hình cơ sở mà chúng tôi sử dụng trong tác vụ dịch máy này là một transformer encoder-decoder với sáu lớp encoder/decoder, bốn đầu chú ý mỗi lớp. Chiều embedding và kích thước ẩn là 512 và 1024 tương ứng, cho cả encoder và decoder. Những cấu hình kiến trúc này giống nhau cho Transformer-MGK/sMGK ngoại trừ số đầu chú ý mỗi lớp, được giảm một nửa. Chúng tôi chia sẻ πjr trên tất cả vị trí j và học nó cho mỗi đầu. Giá trị khởi tạo cho mỗi πjr được đặt là 0.5. Đối với Transformer-sMGK, chúng tôi học br và khởi tạo các phần tử của nó từ phân phối chuẩn tắc. Mỗi σjr² là một hằng số với giá trị √D trong đó D là chiều của mỗi đầu, giống như trong các mô hình cơ sở. Các thí nghiệm của chúng tôi tuân theo thiết lập từ (Ott et al., 2019), và triển khai của chúng tôi dựa trên mã công khai https://github.com/pytorch/fairseq/tree/main/examples/translation.

A.2. Thêm Phân tích Thực nghiệm của Transformer-MGK/MLK được huấn luyện cho Mô hình hóa Ngôn ngữ

Trong Bảng 3 trong văn bản chính, chúng tôi cho thấy những cải thiện trong perplexity hợp lệ và kiểm tra của Transformer-MGK/MLK của chúng tôi so với transformer softmax và tuyến tính cơ sở. Cụ thể, Transformer-MGK/MLK với cùng số đầu như các cơ sở, ví dụ 8 đầu, cải thiện đáng kể các cơ sở trong quá trình huấn luyện trong khi Transformer-MGK/MLK với 4 đầu đạt được hiệu suất có thể so sánh hoặc tốt hơn so với cơ sở 8-head. Trong phần này, chúng tôi cung cấp thêm phân tích thực nghiệm để làm sáng tỏ những kết quả đó. Hình 5 cho thấy đường cong perplexity xác thực của các mô hình của chúng tôi so với transformer softmax và tuyến tính.

Hình 6 trực quan hóa các ma trận chú ý từ một mẫu được chọn ngẫu nhiên cho transformer softmax được huấn luyện với 8 đầu và Transformer-MGK với 8 đầu và 4 đầu. Những trực quan hóa này cho thấy rằng Transformer-MGK chú ý đến các vị trí đa dạng hơn trong tất cả các đầu và lớp so với cơ sở chú ý softmax. Chúng tôi cũng so sánh phân phối rank của các ma trận chú ý này được tính từ 1000 mẫu tại mỗi lớp trong mô hình. Hình 7 trình bày các biểu đồ rank của chú ý softmax 8-head và chú ý MGK 4-head và 8-head cho lớp thứ 1 và thứ 5. Rõ ràng rằng các ma trận chú ý từ Transformer-MGK có rank cao hơn so với những cái trong transformer softmax, điều này ngụ ý rằng Transformer-MGK có thể chú ý đến các vùng đa dạng hơn so với transformer softmax mà không cần sử dụng nhiều đầu chú ý hơn.

Hình 5. Perplexity xác thực của Transformer-MGK so với transformer softmax (Trái) và Transformer-MLK so với transformer tuyến tính (Phải) cho mô hình hóa ngôn ngữ trên WikiText-103.

A.3. Kết quả huấn luyện bổ sung cho LRA

Trong phần này, chúng tôi cung cấp kết quả thực nghiệm bổ sung trên benchmark LRA. Hình 8 so sánh chi phí tính toán được đo bằng FLOP và độ phức tạp mô hình theo số tham số của các phương pháp suy luận và học khác nhau cho Transformer-MGK. Chi phí tính toán của Transformer-MGK/sMGK và Transformer-MGK Hard-E/Soft-E ngang bằng nhau, trong khi Transformer-sMGK sử dụng ít tham số hơn so với các cái khác mà không có đánh đổi trong hiệu suất 5 cho tất cả các tác vụ. Việc đặt tên được giải thích như trong Phần 3.4 trong văn bản chính. Ngoài ra, Hình 9 trực quan hóa các ma trận chú ý trong transformer tuyến tính cơ sở 4-head, Transformer-MLK 4-head, và Transformer-MLK 2-head được huấn luyện trên tác vụ truy xuất tài liệu.

--- TRANG 17 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

[Hình 6 - Các ma trận chú ý phức tạp]

Hình 6. Trực quan hóa các ma trận chú ý từ transformer softmax 8-head (Trái), Transformer-MGK 8-head (Giữa), và Transformer-MGK 4-head (Phải) được huấn luyện trên mô hình hóa ngôn ngữ WikiText-103. Ở đây, chúng tôi vẽ các ma trận chú ý cho tất cả các đầu và lớp trong các mô hình. Độ dài chuỗi là 256 và kích thước của mỗi ma trận là 256 × 256.

[Hình 7 - Biểu đồ phân phối rank]

Hình 7. Phân phối rank của các ma trận chú ý từ transformer softmax 8-head (Trái), Transformer-MGK 8-head (Giữa), và Transformer-MGK 4-head (Phải) được huấn luyện trên mô hình hóa ngôn ngữ WikiText-103. Các biểu đồ rank được tính từ 1000 ma trận chú ý tại mỗi lớp. Các ma trận chú ý từ Transformer-MGK có rank cao hơn so với những cái từ transformer softmax. Điều này ngụ ý rằng Transformer-MGK có các mẫu chú ý đa dạng hơn, điều này cho phép chúng ta giảm số đầu trong Transformer-MGK.

A.4. Nghiên cứu Loại bỏ về Tác động của Hỗn hợp Khóa, Khoảng cách Gaussian, và Dịch chuyển Khóa

Trong phần này, chúng tôi tiến hành nghiên cứu loại bỏ của Transformer-MGK trên tác vụ truy xuất LRA để điều tra từ đâu cải thiện hiệu suất đến. Cụ thể, chúng tôi muốn hiểu tác động của các yếu tố sau đối với hiệu suất của Transformer-MGK: 1) hỗn hợp khóa, 2) khoảng cách Gaussian, và 3) dịch chuyển khóa. Chúng tôi tóm tắt kết quả thực nghiệm của chúng tôi trong Bảng 6 và thảo luận về tác động của 1, 2 và 3 dưới đây.

--- TRANG 18 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

[Hình 8 - Biểu đồ so sánh tính toán và tham số]

Hình 8. Độ phức tạp mô hình (Trên) và chi phí tính toán (Dưới) của các phương pháp suy luận và học khác nhau cho Transformer-MGK được huấn luyện trên tác vụ truy xuất tài liệu. Trong khi chi phí tính toán gần như giống nhau, Transformer-sMGK có lợi thế hơn về kích thước mô hình, so với Transformer-MGK, Transformer-MGK Hard-E, và Soft-E. Việc đặt tên được giải thích như trong Phần 3.4 trong văn bản chính.

[Hình 9 - Trực quan hóa ma trận chú ý cho các mô hình khác nhau]

Hình 9. Trực quan hóa các ma trận chú ý trong transformer tuyến tính cơ sở 4-head (Trái), Transformer-MLK 4-head (Giữa), và Transformer-MLK 2-head (Phải) được huấn luyện trên tác vụ truy xuất tài liệu. Ở đây, độ dài chuỗi là 4000, và kích thước của mỗi ma trận là 4000×4000.

Tác động của Hỗn hợp Khóa Chúng tôi áp dụng cách tiếp cận hỗn hợp khóa (MGK) của chúng tôi vào transformer softmax sử dụng tích vô hướng giữa các truy vấn và khóa thay vì khoảng cách Gaussian như trong bài báo của chúng tôi. Chúng tôi đặt tên mô hình này là Softmax MGK. Chúng tôi so sánh Softmax MGK có 1 đầu (Sofmax MGK 1 head trong Bảng 6) với transformer softmax cơ sở sử dụng 1 và 2 đầu (Softmax 1 head và Softmax 2 heads trong Bảng 6). Kết quả trong Bảng 6 cho thấy rằng Softmax MGK 1 head vượt trội cả transformer softmax cơ sở 1 và 2 đầu. Lưu ý rằng Softmax MGK 1 head của chúng tôi hiệu quả hơn so với cơ sở 2 đầu về số tham số và số FLOP. Những kết quả này xác nhận lợi ích của việc sử dụng MGK.

Tác động của việc sử dụng khoảng cách Gaussian Tiếp theo, chúng tôi so sánh softmax MGK với Gaussian MGK. Ở đây Gaussian MGK là Transformer-MGK được đề xuất và thảo luận trong bài báo của chúng tôi, tính toán điểm chú ý sử dụng cách tiếp cận MGK và khoảng cách Gaussian giữa các truy vấn và khóa. Kết quả trong Bảng 6 gợi ý rằng Gaussian MGK 1

--- TRANG 19 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 6. Nghiên cứu loại bỏ về tác động của hỗn hợp khóa, khoảng cách Gaussian, và dịch chuyển khóa trên tác vụ truy xuất LRA. Chúng tôi ký hiệu transformer softmax bằng Softmax. Tất cả các mô hình Softmax (tức là Softmax 2 heads, Softmax 1 head, Softmax MGK 1 head, và Softmax sMGK 1 head) sử dụng tích vô hướng để tính điểm chú ý. Tất cả các mô hình Gaussian (tức là Gaussian MGK 1 head, Gaussian sMGK 1 head, và Gaussian 1 head) sử dụng khoảng cách Gaussian để tính điểm chú ý. Chúng tôi ký hiệu MGK với dịch chuyển khóa bằng sMGK. Ở đây MGK được sử dụng để ký hiệu cách tiếp cận của chúng tôi về việc sử dụng hỗn hợp khóa tại mỗi timestep.

| Phương pháp | Độ chính xác (%) |
|-------------|-------------------|
| Softmax 2 heads | 79.10 |
| Softmax sMGK 1 head | 79.81 |
| Softmax MGK 1 head | 79.23 |
| Softmax 1 head | 77.90 |
| Gaussian sMGK 1 head | 81.23 |
| Gaussian MGK 1 head | 80.63 |
| Gaussian 1 head | 80.38 |

head cải thiện so với Softmax MGK 1 head (80.63% so với 79.23%). Kết quả này chứng minh lợi thế của việc sử dụng khoảng cách Gaussian so với tích vô hướng để tính điểm chú ý.

Tác động của dịch chuyển khóa Cuối cùng, chúng tôi áp dụng dịch chuyển khóa cho cả Softmax MGK và Gaussian MGK (Softmax sMGK và Gaussian sMGK trong Bảng 6). Từ Bảng 6, chúng tôi quan sát thấy rằng Softmax sMGK 1 head và Gaussian sMGK 1 head vượt trội Softmax MGK 1 head và Gaussian MGK 1 head tương ứng. Những kết quả này, một lần nữa, khẳng định lợi ích của việc sử dụng dịch chuyển khóa.

Chúng tôi cũng bao gồm kết quả cho mô hình Gaussian 1 head trong Bảng 6. Mô hình Gaussian 1 head này tương tự như mô hình Softmax 1 head nhưng sử dụng khoảng cách Gaussian để tính điểm chú ý. So sánh kết quả của mô hình Gaussian 1 head, mô hình Softmax 1 head, và mô hình Gaussian MGK 1 head được báo cáo trong Bảng 6 tiếp tục xác nhận lợi thế của việc sử dụng MGK và khoảng cách Gaussian.

A.5. Phân tích hiệu quả trên WikiText-103

Phân tích hiệu quả trên WikiText-103 Trong phần này, chúng tôi cung cấp chi tiết lớn hơn về các thước đo và kết quả của phân tích hiệu quả mô hình của chúng tôi. Mở rộng phương pháp của chúng tôi cho Linear Transformer (Transformer-MLK) mang lại lợi ích đáng kể cho hiệu quả của cơ sở tuyến tính, làm cho nó đặc biệt áp dụng được cho các tác vụ tầm xa và quy mô lớn.

Thước đo phân tích Phân tích của chúng tôi báo cáo số FLOPS như hiệu quả tính toán mô hình cho cả huấn luyện và suy luận. Số tham số tổng và không embedding được sử dụng để đo độ phức tạp mô hình. Vì mục đích của phương pháp của chúng tôi là giảm số tham số trong mô hình chính, không bao gồm số tham số input-embedding. Do đó, việc cung cấp giảm tham số không embedding như một tiêu chí cho hiệu quả kích thước mô hình cũng quan trọng. Ở đây, chúng tôi điều tra lợi ích tính toán và bộ nhớ của mô hình của chúng tôi trong các tác vụ quy mô lớn thông qua việc tăng D ∈ {256, 512, 1024, 2048, 4096} và N ∈ {128, 256, 512, 1024, 2048, 4096, 8192}. Để tính toán FLOP, chúng tôi sử dụng fvcore. Tất cả các phép đo được tính khi chạy mô hình qua dữ liệu có kích thước batch 1.

Transformer-MLK giảm độ phức tạp mô hình và chi phí tính toán

Hình 10A và Hình 10B cho thấy việc giảm tỷ lệ FLOPS giữa Transformer-MLK 4-head so với cơ sở Linear 8-head như hàm của chiều mô hình D và độ dài chuỗi N. Hình 10C trình bày lợi ích kích thước mô hình (tỷ lệ tham số tổng/không embedding) giữa mô hình của chúng tôi và cơ sở khi D tăng. Những hình này chỉ ra rằng khi chúng ta mở rộng các tác vụ và mô hình, Transformer-MLK của chúng tôi có lợi thế đáng kể hơn so với cơ sở Linear.

A.6. Phân tích Chi phí Thời gian và Dấu chân Bộ nhớ

Bảng 7 so sánh dấu chân bộ nhớ GPU và chi phí thời gian tính toán (giây/iteration) của Transformer-MGK/MLK 4-head của chúng tôi với những cái của cơ sở transformer softmax/tuyến tính 8-head tại thời điểm kiểm tra. Tất cả các mô hình được huấn luyện trên tác vụ truy xuất LRA, và chúng tôi sử dụng kích thước batch 32 cho mỗi iteration. Các mô hình MGK/MLK của chúng tôi tiết kiệm bộ nhớ và giảm thời gian wall-clock đáng kể so với cơ sở. Sử dụng dịch chuyển khóa trong các mô hình của chúng tôi, tức là Transformer-sMGK/sMLK, giúp cải thiện hiệu quả thêm nữa.

--- TRANG 20 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

[Hình 10 - Ba biểu đồ A, B, C hiển thị tỷ lệ FLOP và tham số]

Hình 10. Tỷ lệ FLOP Huấn luyện (A), Suy luận (B) và tỷ lệ số tham số (C) giữa Transformer-MLK 4-head với cơ sở Linear 8-head trên các chiều mô hình D khác nhau và độ dài chuỗi N, cho tác vụ mô hình hóa ngôn ngữ trên WikiText-103. Transformer-MLK 4-head giảm đáng kể tính toán (cả FLOP huấn luyện và suy luận) và độ phức tạp mô hình (cả số tham số tổng và không embedding), so với cơ sở khi chúng ta mở rộng chiều mô hình và độ dài chuỗi, cho thấy lợi thế của Transformer-MLK cho các tác vụ tầm xa và quy mô lớn.

Bảng 7. So sánh dấu chân bộ nhớ GPU và chi phí thời gian tính toán (giây/iteration) giữa Transformer-MGK/MLK 4-head của chúng tôi và cơ sở transformer softmax/tuyến tính 8-head được huấn luyện trên tác vụ truy xuất LRA tại thời điểm kiểm tra. Transformer-MGK/MLK của chúng tôi tiết kiệm nhiều bộ nhớ hơn và có thời gian wall-clock nhỏ hơn đáng kể so với cơ sở. Ở đây chúng tôi sử dụng kích thước batch 32 cho mỗi iteration.

| Phương pháp | Bộ nhớ (Gb) | Chi phí thời gian (giây/iteration) |
|-------------|-------------|-------------------------------------|
| Softmax 8 heads | 58.00 | 0.357 |
| Transformer-MGK 4 heads | 53.58 | 0.278 |
| Transformer-sMGK 4 heads | 45.50 | 0.227 |
| Linear 8 heads | 3.38 | 0.055 |
| Transformer-MLK 4 heads | 2.90 | 0.043 |
| Transformer-sMLK 4 heads | 2.83 | 0.042 |

Bảng 8. Hệ số trộn đã học πjr của tất cả các đầu và lớp trong Transformer-MGK 1-head được huấn luyện trên tác vụ truy xuất LRA. Ở đây chúng tôi sử dụng cùng πj1, πj2 cho tất cả bước thời gian j = 1,...,N.

| Phương pháp | Lớp 1 | Lớp 2 |
|-------------|--------|--------|
|             | πj1 | πj2 | πj1 | πj2 |
| Transformer-sMGK 1 heads | 0.488 | 0.512 | 0.500 | 0.500 |
| Transformer-MGK 1 heads | 0.502 | 0.498 | 0.497 | 0.503 |

A.7. Đường cong Học cho thấy Sự hội tụ

Trong phần này, chúng tôi vẽ lại Hình 1 và 5 để cho thấy sự hội tụ của các quá trình huấn luyện của chúng tôi. Trong Hình 1, các kết quả dường như không hội tụ vì chúng tôi vẽ mất mát và độ chính xác ở thang log. Trong Hình 4, các kết quả dường như không hội tụ vì chúng tôi zoom vào phạm vi cụ thể trên trục y và x. Hình 11 và 12 là các phiên bản được vẽ lại của Hình 1 và 5 tương ứng. Trong Hình 11, các đường cong mất mát/độ chính xác huấn luyện dừng sớm vì chúng tôi sử dụng dừng sớm để tránh overfitting. Các đường cong mất mát/độ chính xác kiểm tra trong hình này đã hội tụ.

A.8. Ma trận Trọng số của Khóa, Khóa và Hệ số Trộn

Trong phần này, chúng tôi phân tích các πjr, kjr, và WKr đã học, j = 1,...,N và r = 1,...,M trong Transformer-MGK được huấn luyện trên tác vụ truy xuất LRA. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi đặt M = 2. Trong Hình 13 và 14, chúng tôi trực quan hóa các ma trận trọng số

--- TRANG 21 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Hình 11. Mất mát và độ chính xác huấn luyện và kiểm tra của Transformer-MGK so với transformer softmax (Trái) và Transformer-MLK so với Transformer tuyến tính (Phải) trên tác vụ retrieval, có độ dài chuỗi trung bình dài nhất và khoảng chú ý trong các tác vụ LRA (Tay et al., 2021). Trong huấn luyện, chúng tôi áp dụng dừng sớm để tránh overfitting. Điều đó giải thích tại sao các đường cong mất mát và độ chính xác huấn luyện dừng sớm. Các đường cong mất mát/độ chính xác kiểm tra đã hội tụ. Hiệu suất ấn tượng của Transformer-MGK/MLK trên tác vụ thách thức này xác thực khả năng của các mô hình của chúng tôi trong việc nắm bắt các phụ thuộc tầm xa thông qua việc học đa dạng các mẫu chú ý.

Hình 12. Perplexity xác thực của Transformer-MGK so với transformer softmax (Trái) và Transformer-MLK so với transformer tuyến tính (Phải) cho mô hình hóa ngôn ngữ trên WikiText-103. Huấn luyện hội tụ trên tác vụ này sau 500000 iterations, tương đương với 115 epochs.

WK tính toán các khóa và các khóa K tương ứng, cho tất cả các đầu và lớp trong transformer softmax cơ sở 2-head, Transformer-MGK 1-head với 2 khóa, và Transformer-sMGK 1-head với 2 khóa được huấn luyện trên tác vụ truy xuất LRA. Lưu ý rằng đối với các khóa, chúng tôi chỉ vẽ 100 token đầu tiên. Ngoài ra, Bảng 8 tóm tắt hệ số trộn đã học πjr của tất cả các đầu và lớp trong Transformer-MGK 1-head được huấn luyện trên cùng tác vụ retrieval. Ở đây chúng tôi sử dụng cùng πj1, πj2 cho tất cả bước thời gian j = 1,...,N.

A.9. Phân tích Độ phức tạp Tính toán Bổ sung (FLOP) tại Thời điểm Huấn luyện

Hình 15 chứng minh chi phí tính toán (FLOP) cho mỗi iteration huấn luyện của Transformer-MGK so với transformer softmax cơ sở (Trái) và Transformer-MLK so với transformer tuyến tính cơ sở (Phải) trên tác vụ truy xuất LRA. Lợi thế hiệu quả của Transformer-MGK/MLK so với cơ sở tăng theo số đầu.

--- TRANG 22 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

[Hình 13 - Trực quan hóa ma trận trọng số]

Hình 13. Ma trận trọng số WK để tính toán các khóa, cho tất cả các đầu và lớp, trong transformer softmax cơ sở 2-head (Trái), Transformer-MGK 1-head với 2 khóa (Giữa), và Transformer-sMGK 1-head với 2 khóa (Phải) được huấn luyện trên tác vụ truy xuất LRA. Ở đây, chiều của mỗi đầu D = 32 và của đầu vào xi là Dx = 64. Do đó, mỗi ma trận trọng số có hình dạng (64, 32).

[Hình 14 - Trực quan hóa embedding khóa]

Hình 14. Embedding khóa K cho tất cả các đầu và lớp của transformer softmax cơ sở 2-head (Trái), Transformer-MGK 1-head với 2 khóa (Giữa), và Transformer-sMGK 1-head với 2 khóa (Phải) được huấn luyện trên tác vụ truy xuất LRA. Ở đây chiều D của mỗi đầu là 32, và chúng tôi vẽ các embedding khóa của 100 token đầu tiên trong một chuỗi được chọn ngẫu nhiên. Do đó, mỗi ma trận khóa có hình dạng (100, 32).

Hình 16 cho thấy chi phí tính toán mỗi iteration huấn luyện (được đo bằng FLOP) của các phương pháp suy luận và học khác nhau cho Transformer-MGK được huấn luyện trên tác vụ truy xuất tài liệu. Transformer-sMGK, Transformer-MGK, Transformer-MGK Hard-E, và Soft-E có chi phí tính toán tương tự.

A.10. Mở rộng đến Mô hình Cơ sở 12-Head cho tác vụ Retrieval.

Để nghiên cứu thêm khả năng mở rộng của mô hình của chúng tôi, trong phần này, chúng tôi điều tra hiệu suất của Transformer-MGK/MLK 6-head của chúng tôi so với transformer softmax/tuyến tính cơ sở 12-head trên tác vụ retrieval. Bảng 9 chỉ ra

--- TRANG 23 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Hình 15. Chi phí tính toán (FLOP) cho mỗi iteration huấn luyện của Transformer-MGK so với transformer softmax cơ sở (Trái) và Transformer-MLK so với transformer tuyến tính cơ sở (Phải) trên tác vụ truy xuất LRA. Lợi thế hiệu quả của Transformer-MGK/MLK so với cơ sở tăng theo số đầu. Ở đây, kích thước batch của mỗi iteration huấn luyện là 32.

Hình 16. Chi phí tính toán (được đo bằng FLOP) mỗi iteration huấn luyện của các phương pháp suy luận và học khác nhau cho Transformer-MGK được huấn luyện trên tác vụ truy xuất LRA. Chi phí tính toán gần như giống nhau cho Transformer-sMGK, Transformer-MGK, Transformer-MGK Hard-E, và Soft-E. Việc đặt tên được giải thích như trong Phần 3.4 trong văn bản chính.

Bảng 9. Độ chính xác Kiểm tra (%) của Transformer-MGK/MLK 6-head so với transformer softmax và tuyến tính cơ sở 12-head trên tác vụ retrieval. Transformer-MGK/MLK 6-head của chúng tôi vượt trội đáng kể transformer softmax và tuyến tính tương ứng, trong khi hiệu quả hơn về chi phí tính toán, kích thước mô hình, và sử dụng bộ nhớ.

| Phương pháp | Độ chính xác (%) |
|-------------|-------------------|
| Softmax 12 heads | 82.18 |
| Transformer sMGK 6 head | 83.31 |
| Transformer MGK 6 head | 83.05 |
| Linear sMGK 12 head | 81.97 |
| Transformer sMLK 6 head | 82.80 |
| Transformer MLK 6 head | 82.11 |

rằng Transformer-MGK/MLK 6-head của chúng tôi vượt trội đáng kể transformer softmax/tuyến tính 12-head tương ứng. Hơn nữa, so sánh những kết quả này với những cái trong Bảng 1 và Bảng 2, mặc dù transformer softmax/tuyến tính 12-head cải thiện so với những cái 8-head, độ chính xác của chúng vẫn tệ hơn hoặc chỉ tương đương với những cái của các mô hình Transformer-MGK/MLK 4-head và thậm chí 2-head của chúng tôi.

A.11. So sánh với Chú ý Multi-query

Trong phần này, chúng tôi so sánh cách tiếp cận MGK của chúng tôi với chú ý multi-query (Shazeer, 2019). Chú ý multi-query chia sẻ cùng một tập khóa và giá trị tại các đầu khác nhau để giảm chi phí băng thông bộ nhớ trong quá trình suy luận gia tăng, điều này cho phép suy luận nhanh hơn vì kích thước của các tensor "khóa" và "giá trị" được tải lại được giảm đáng kể. Mặt khác, Transformer-MGK của chúng tôi mô hình hóa khóa tại mỗi đầu như một mô hình hỗn hợp Gaussian, dẫn đến việc sử dụng nhiều khóa tại mỗi đầu và cho phép chúng ta giảm số đầu chú ý. Điều này giúp giảm các tính toán và tham số cần thiết để tính toán các truy vấn và giá trị bổ sung. Nếu sử dụng dịch chuyển khóa (xem tùy chọn (B) trong đoạn Tùy chọn Thiết kế cho Khóa trong Phần 2.3), cách tiếp cận MGK cũng giúp giảm các tính toán và tham số cần thiết để tính toán các khóa bổ sung. Lợi thế của Transformer-MGK tồn tại trong cả huấn luyện và suy luận, bao gồm suy luận gia tăng. Chúng tôi đã cung cấp phân tích chi tiết về độ phức tạp tính toán và số tham số của Transformer-MGK so với transformer softmax tương ứng trong Phụ lục B. Kết hợp chú ý multi-query và cách tiếp cận MGK của chúng tôi thật thú vị vì mỗi phương pháp có lợi thế riêng và chúng bổ sung cho nhau. Cụ thể, chúng ta có thể để mô hình transformer chia sẻ cùng một tập giá trị và hỗn hợp khóa tại các đầu khác nhau. Cách tiếp cận này có thể có lợi thế của cả chú ý multi-query và MGK. Bảng 10 cho thấy Perplexity (PPL) trên Wikitext-103 của Multi-query Transformer-MGK 4-head và cơ sở Multi-query Transformer 8-head. Kết hợp chú ý multi-query với transformer-MGK có lợi cho cả hiệu suất của mô hình (Test PPL) so với cơ sở và hiệu quả của nó vì Transformer-MGK cho phép mô hình giảm số đầu một nửa.

--- TRANG 24 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 10. Perplexity (PPL) trên Wikitext-103 của Multi-query Transformer-MGK 4-head và cơ sở Multi-query Transformer 8-head. Kết hợp chú ý multi-query với transformer-MGK dẫn đến hiệu suất kiểm tra tốt hơn so với cơ sở trong khi hiệu quả hơn vì Transformer-MGK cho phép mô hình giảm số đầu một nửa.

| Phương pháp | Valid PPL | Test PPL |
|-------------|-----------|----------|
| Multi-query attention 8 heads (small) | 35.08 | 36.03 |
| Multi-query Transformer MGK 4 head (small) | 35.16 | 35.79 |
| Softmax 8 heads (small) | 33.15 | 34.29 |
| Transformer MGK 4 head (small) | 33.28 | 34.21 |

B. Phân tích về Độ phức tạp Tính toán và Số Tham số trong Transformer-MGK và Transformer Softmax

Trong phần này, chúng tôi so sánh độ phức tạp tính toán và số tham số trong transformer-MGK với M khóa tại mỗi timestep và H/M đầu với transformer softmax cơ sở có 1 khóa tại mỗi timestep và H đầu. Ở đây chúng tôi chọn M sao cho H là bội số của M và sử dụng tùy chọn thiết kế khóa (A) làm cho khóa kjr là một phép chiếu tuyến tính của đầu vào xj, tức là kjr = xjW^T_Kr, trong đó xj ∈ R^(1×Dx), W_Kr ∈ R^(D×Dx) và r = 1,2,...,M (xem Tùy chọn Thiết kế cho Khóa trong Phần 2.3 để biết thêm chi tiết). Để đơn giản hóa ký hiệu và không mất tính tổng quát, chúng tôi đặt M = 2 như trong các thí nghiệm của chúng tôi và giả định rằng Dv = D, tức là các giá trị có cùng chiều đặc trưng với các truy vấn và khóa. Để đơn giản hóa tính toán, chúng tôi cũng không tính toán toán tử softmax vì toán tử này cho chi phí tương tự khi áp dụng trong Transformer-MGK và transformer softmax.

B.1. Độ phức tạp Tính toán

(i) Chú ý softmax H-head: Số phép tính trong chú ý softmax H-head là N²H(4D-1) + NHD(6Dx + 2HD-5).

Giải thích: Để tính toán ma trận truy vấn Q, ma trận khóa K, và ma trận giá trị V trong Bước 1 trong Phần 1.1 tại mỗi đầu, chúng ta cần 3NDDx phép nhân và 3ND(Dx-1) phép cộng. Tổng cộng, những cái này cần 3ND(2Dx-1) phép tính. Tiếp theo, để tính toán tích QK^T trong Công thức (1), chúng ta cần N²D phép nhân và N²(D-1) phép cộng. Tương tự, tích AV yêu cầu N²D phép nhân và N(N-1)D phép cộng. Tổng cộng, việc tính toán chuỗi đầu ra H trong Công thức (1) tại mỗi đầu yêu cầu 3ND(2Dx-1) + N²D + N²(D-1) + N²D + N(N-1)D = N²(4D-1) + ND(6Dx-4) phép tính. Tổng tính toán cho tất cả H đầu khi đó là

H(N²(4D-1) + ND(6Dx-4)) + NHD(2HD-1)
= N²H(4D-1) + NHD(6Dx + 2HD-5);

trong đó NHD(2HD-1) bổ sung là từ phép chiếu tuyến tính bởi W_O.

(ii) Chú ý hỗn hợp 2 khóa Gaussian với H/2-head: Số phép tính trong chú ý hỗn hợp 2 khóa Gaussian với H/2-head là N²H(3D-0.5) + NHD(4Dx + HD-4).

Giải thích: Tương tự như phép dẫn xuất ở trên, trong chú ý Hỗn hợp M khóa Gaussian, để tính toán chuỗi đầu ra H chúng ta cần N²((2M + 2)D-1) + ND((M + 2)(2Dx-1)-1) phép tính. Lưu ý rằng, để tính toán khoảng cách Gaussian giữa các truy vấn qi và khóa kj như trong chú ý hỗn hợp M khóa Gaussian và để tính toán tích vô hướng của chúng như trong chú ý softmax, chúng ta cần số phép tính tương tự. Do đó, tổng tính toán cho tất cả H/M đầu khi đó là

(H/M)(N²((2M + 2)D-1) + ND((M + 2)(2Dx-1)-1)) + NHD(2(H/M)D-1)
= N²H(2(M + 1)D-1)/M + NHD(2(M + 2)/M Dx + 2/M HD - (3M + 2)/M).

Đặt M = 2, khi đó tổng tính toán của chú ý hỗn hợp 2 khóa Gaussian được cho bởi N²H(3D-0.5) + NHD(4Dx + HD-4).

Chú ý soft-max H-head so với chú ý hỗn hợp 2 khóa Gaussian với H/2-head: Cho các kết quả trong (i) và (ii), khi so sánh với chú ý softmax H-head cơ sở, chú ý hỗn hợp 2 khóa Gaussian với H/2-head của chúng tôi tiết kiệm

N²H(D-0.5) + NHD(2Dx + HD-1)

phép tính. Khi N lớn, sự khác biệt này là đáng kể. Kết luận, chú ý hỗn hợp 2 khóa Gaussian với H/2-heads có độ phức tạp tính toán rẻ hơn so với chú ý soft-max H-head.

B.2. Số Tham số

(iii) Chú ý softmax H-head: Số tham số trong chú ý softmax H-head là 3HDDx + (HD)².

Giải thích: 3HDDx là từ các phép chiếu tuyến tính để tính toán ma trận truy vấn Q, ma trận khóa K, và ma trận giá trị V trong Bước 1 trong Phần 1.1. (HD)² là từ phép chiếu tuyến tính để tính toán đầu ra cuối cùng như trong Công thức (??).

(iv) Chú ý Hỗn hợp 2 Khóa Gaussian với H/2-head: Số tham số trong Chú ý Hỗn hợp 2 Khóa Gaussian với H/2-head là 2HDDx + 0.5(HD)² + H.

Giải thích: Các phép chiếu tuyến tính để tính toán Q và V đóng góp HDDx/2 tham số mỗi cái. Phép chiếu tuyến tính để tính toán K đóng góp HDDx. Phép chiếu tuyến tính để tính toán đầu ra cuối cùng có chiều (H/2)D × HD, vì vậy nó đóng góp 0.5(HD)² tham số. H tham số thêm là từ tiên nghiệm πjr. Những tiên nghiệm này đóng góp 2 tham số tại mỗi đầu vì chúng ta làm cho {πj1, πj2} cho tất cả j = 1,...,N giống nhau.

Chú ý soft-max H-head so với chú ý hỗn hợp 2 khóa Gaussian với H/2-head: Cho các kết quả trong (iii) và (iv), khi so sánh với chú ý softmax H-head cơ sở, chú ý hỗn hợp 2 khóa Gaussian với H/2-head của chúng tôi tiết kiệm HDDx + 0.5(HD)² - H tham số. Khi H và D lớn, việc tiết kiệm này là đáng kể.

C. Chứng minh các kết quả chính

Trong phụ lục này, chúng tôi cung cấp chứng minh cho các kết quả chính trong bài báo.

C.1. Chứng minh Định lý 1

Để dễ dàng trình bày chứng minh, đối với bất kỳ phân phối xác suất G nào, chúng ta ký hiệu

p_G(x) := ∫ f(x-μ)dG(μ) = ∫ φ(x|μ, σ²I)dG(μ);

cho tất cả x ∈ R^d trong đó f(x) = 1/(√2π)^d exp(-||x||²/(2σ²)) cho σ > 0 cho trước. Có nghĩa là p_G là convolution của f và phân phối xác suất G. Vì không gian của các hỗn hợp Gaussian dày đặc trong không gian của các phép đo xác suất liên tục (Bacharoglou, 2010), nó chỉ ra rằng tồn tại phân phối xác suất G₁ sao cho

sup_{x∈R^d} |p(x) - p_{G₁}(x)| ≤ ε/2. (9)

--- TRANG 25 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bước tiếp theo của chúng tôi là chứng minh rằng tồn tại một phép đo xác suất G₂ với tối đa K hỗ trợ trong đó K ≤ (C log(1/ε))^d cho một số hằng số phổ quát C sao cho

sup_{x∈R^d} |p_{G₁}(x) - p_{G₂}(x)| ≤ ε/2. (10)

Thật vậy, từ Lemma A.1 trong (Ghosal & van der Vaart, 2001), đối với bất kỳ k ≥ 1 tồn tại một phân phối xác suất G₂ với tối đa (2⌈k²⌉)^d hỗ trợ sao cho

∫ ∂^α(G₁ - G₂)(μ) = 0; (11)

cho bất kỳ α = (α₁, α₂, ..., α_d) ∈ N^d sao cho 0 ≤ |α| = ∑_{j=1}^d α_j ≤ 2k², Ở đây, ∂^α = ∏_{j=1}^d ∂^{α_j}/∂μ_j^{α_j}.

Bây giờ, đối với bất kỳ M ≥ 2a√d, chúng ta có ||x|| - ||x - μ|| ≥ ||μ|| > M = 2a√d > M/2 miễn là ||x|| > M và μ ∈ [-a,a]^d. Nó chỉ ra rằng

sup_{||x||>M} |p_{G₁}(x) - p_{G₂}(x)| = sup_{||x||>M} |∫ f(x-μ)d(G₁ - G₂)(μ)|
≤ sup_{||x||>M} ∫ 1/(√2π)^d exp(-||x-μ||²/(2σ²)) d(G₁ + G₂)(μ)
≤ 2/(√2π)^d exp(-M²/(8σ²)). (12)

Mặt khác, đối với bất kỳ k ≥ 1 chúng ta cũng có rằng

sup_{||x||≤M} |p_{G₁}(x) - p_{G₂}(x)| = sup_{||x||≤M} |∫ f(x-μ)d(G₁ - G₂)(μ)|
≤ sup_{||x||≤M} |∫[f(x-μ) - ∑_{j=0}^{k-1} (-1)^j ||x-μ||^{2j}/(√2π)^d d+2j σ^{2j} j!] d(G₁ - G₂)(μ)|
+ sup_{||x||≤M} |∫ ∑_{j=0}^{k-1} (-1)^j ||x-μ||^{2j}/(√2π)^d d+2j σ^{2j} j! d(G₁ - G₂)(μ)|
= sup_{||x||≤M} |∫[f(x-μ) - ∑_{j=0}^{k-1} (-1)^j ||x-μ||^{2j}/(√2π)^d d+2j σ^{2j} j!] d(G₁ - G₂)(μ)|, (13)

trong đó đẳng thức cuối cùng xuất phát từ

∫ ∑_{j=0}^{k-1} (-1)^j ||x-μ||^{2j}/(√2π)^d d+2j σ^{2j} j! d(G₁ - G₂)(μ) = 0;

điều này do Công thức (11).

Để ràng buộc thêm vế phải (RHS) của Công thức (13), chúng ta sử dụng bất đẳng thức sau:

|exp(-y) - ∑_{j=0}^{k-1} (-y)^j/j!| ≤ |y|^k e^{|y|}/k!

cho bất kỳ y ∈ R. Vì k! ≥ (k/e)^k cho bất kỳ k ≥ 1, ràng buộc trên có thể được viết lại như

|exp(-y) - ∑_{j=0}^{k-1} (-y)^j/j!| ≤ |y|^k e^{|y|}/k^k. (14)

--- TRANG 26 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Đơn giản hóa thêm Công thức (13) dẫn đến

sup_{||x||≤M} |p_{G₁}(x) - p_{G₂}(x)| ≤ sup_{||x||≤M} ∫ |f(x-μ) - ∑_{j=0}^{k-1} (-1)^j ||x-μ||^{2j}/(√2π)^d 2^{d+2j} σ^{2j} j!| d(G₁ + G₂)(μ)

≤ sup_{||x||≤M,μ∈[-a,a]^d} 2/(√2π)^d |exp(-||x-μ||²/(2σ²)) - ∑_{j=0}^{k-1} (-1)^j ||x-μ||^{2j}/(2σ²)^j j!|

≤ sup_{||x||≤M,μ∈[-a,a]^d} e^{k||x-μ||²k}/(2σ²)^k (2k)^k,

trong đó bất đẳng thức cuối cùng dựa trên việc áp dụng bất đẳng thức (14) với y = ||x-μ||²/(2σ²). Đối với ||x|| ≤ M và μ ∈ [-a,a]^d, chúng ta có ||x-μ|| ≤ ||x|| + ||μ|| ≤ M + a√d. Do đó, chúng ta tiếp tục có

sup_{||x||≤M} |p_{G₁}(x) - p_{G₂}(x)| ≤ sup_{||x||≤M,μ∈[-a,a]^d} e^{k||x-μ||²k}/(2σ²)^k (2k)^k ≤ e^{k(M+a√d)²k}/(8²k)^k.

Khi M ≥ 2a√d, chúng ta có M + a√d ≤ 3M/2 và ràng buộc trên dẫn đến

sup_{||x||≤M} |p_{G₁}(x) - p_{G₂}(x)| ≤ (9e)^k M^{2k}/(8²k)^k. (15)

Bằng cách chọn M² = 8σ² log(1/ε₀) cho một số ε₀ > 0, các ràng buộc trong Công thức (12) và (15) trở thành

sup_{||x||≤M} |p_{G₁}(x) - p_{G₂}(x)| ≤ 2/(√2π)^d ε₀;

sup_{||x||>M} |p_{G₁}(x) - p_{G₂}(x)| ≤ (9e)^k (log(1/ε₀))^k/k^k. (16)

Miễn là chúng ta chọn k = ⌈9e² log(1/ε₀)⌉ và ε₀ ≤ 1, chúng ta có

sup_{||x||>M} |p_{G₁}(x) - p_{G₂}(x)| ≤ e^{-k/e} ≤ e^{-9e² log(1/ε₀)} = (ε₀)^{9e²} ≤ ε₀. (17)

Bằng cách chọn ε₀ = ε/(2 max{2/(√2π)^d, 1}), các kết quả từ Công thức (16) và (17) chỉ ra rằng

sup_{||x||≤M} |p_{G₁}(x) - p_{G₂}(x)| ≤ ε/2; và sup_{||x||>M} |p_{G₁}(x) - p_{G₂}(x)| ≤ ε/2.

Do đó, nếu chúng ta chọn M = 8σ² log(2 max{2/(√2π)^d, 1}/ε) và k = ⌈9e² log(2 max{2/(√2π)^d, 1}/ε)⌉, chúng ta có

sup_{x∈R^d} |p_{G₁}(x) - p_{G₂}(x)| ≤ ε/2.

Nó chỉ ra rằng chúng ta đạt được kết luận của tuyên bố (10) bằng cách chọn K = (2⌈k²⌉)^d ≤ ⌈18e² log(2 max{2/(√2π)^d, 1}/ε)⌉^d.

Kết hợp các kết quả từ Công thức (9) và (10), chúng ta có

sup_{x∈R^d} |p(x) - p_{G₂}(x)| ≤ sup_{x∈R^d} |p(x) - p_{G₁}(x)| + sup_{x∈R^d} |p_{G₁}(x) - p_{G₂}(x)| ≤ ε.

Như một hệ quả, chúng ta đạt được kết luận của định lý.
