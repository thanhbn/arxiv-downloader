# 2405.17976.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2405.17976.pdf
# Kích thước tệp: 589527 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---

*wushaohua@ieisystem.com
1https://github.com/IEIT-Yuan/Yuan2.0-M32 Yuan 2.0-M32: Mixture of Experts với Attention Router

Shaohua Wu *, Jiangang Luo, Xi Chen, Lingjun Li, Xudong Zhao, Tong Yu, Chao Wang,
Yue Wang, Fei Wang, Weixu Qiao, Houbo He, Zeru Zhang, Zeyu Sun, Junxiong Mao, Chong Shen
IEIT Systems

TÓM TẮT
Yuan 2.0-M32, với kiến trúc cơ sở tương tự như Yuan-2.0 2B, sử dụng kiến trúc mixture-of-experts với 32 chuyên gia trong đó 2 chuyên gia được kích hoạt. Một mạng định tuyến mới, Attention Router, được đề xuất và áp dụng để lựa chọn chuyên gia hiệu quả hơn, giúp cải thiện độ chính xác so với mô hình với mạng định tuyến cổ điển. Yuan 2.0-M32 được huấn luyện với 2000B token từ đầu, và chi phí tính toán huấn luyện chỉ là 9.25% của một mô hình dày đặc có cùng quy mô tham số. Yuan 2.0-M32 thể hiện khả năng cạnh tranh trong lập trình, toán học, và các lĩnh vực chuyên môn khác nhau, chỉ với 3.7B tham số hoạt động trên tổng số 40B, và 7.4 GFlops tính toán tiến về phía trước mỗi token, cả hai đều chỉ là 1/19 so với Llama3-70B. Yuan 2.0-M32 vượt trội hơn Llama3-70B trên benchmark MATH và ARC-Challenge, với độ chính xác lần lượt là 55.89 và 95.8. Các mô hình và mã nguồn của Yuan 2.0-M32 được phát hành tại Github1.

1. Giới thiệu
Với một lượng tính toán cố định cho mỗi token, một mô hình với cấu trúc Mixture of Experts (MoE) có thể dễ dàng được xây dựng ở quy mô lớn hơn nhiều so với mô hình dày đặc bằng cách tăng số lượng chuyên gia, và do đó đạt được hiệu suất độ chính xác cao hơn. Trong thực tế, việc huấn luyện mô hình với tài nguyên tính toán hạn chế là phổ biến, và MoE được coi là một ứng cử viên tốt để giảm chi phí đáng kể liên quan đến quy mô cực lớn của mô hình, tập dữ liệu và sức mạnh tính toán hạn chế.

Ý tưởng về MoE có từ năm 1991 (Jacobs et al., 1991). Tổng tổn thất là sự kết hợp của tổn thất có trọng số của mỗi chuyên gia với khả năng đưa ra đánh giá độc lập. Khái niệm về MoE thưa thớt đã được Shazeer et al. (2017) đưa vào tâm điểm trong một mô hình dịch thuật. Với chiến lược định tuyến này, một số lượng rất nhỏ chuyên gia sẽ hoạt động cho lý luận thay vì gọi tất cả chuyên gia đồng thời. Tính thưa thớt này cũng cho phép mô hình mở rộng lên 1000 lần giữa các lớp LSTM xếp chồng với chi phí hiệu quả tính toán rất ít. Mạng định tuyến Noisy Top-K Gating giới thiệu một số nhiễu có thể điều chỉnh vào hàm softmax và giữ lại giá trị top-K, để cân bằng việc sử dụng chuyên gia. Trong những năm gần đây, với kích thước mô hình ngày càng tăng, vai trò của chiến lược định tuyến đã thu hút nhiều sự chú ý hơn cho việc phân bổ hiệu quả tài nguyên tính toán.

Mạng định tuyến chuyên gia là cốt lõi trong cấu trúc MoE. Cấu trúc này lựa chọn các chuyên gia ứng cử viên để tham gia vào tính toán bằng cách tính toán xác suất phân bổ token cho mỗi chuyên gia. Hiện tại, trong hầu hết các cấu trúc MoE phổ biến, việc áp dụng thuật toán định tuyến cổ điển thực hiện tích vô hướng giữa token và vector đặc trưng đại diện cho mỗi chuyên gia, sau đó lựa chọn các chuyên gia có giá trị tích vô hướng lớn nhất là phổ biến (Shazeer et al. 2017; Fedus, Zoph and Shazeer, 2022; Zhou et al., 2022). Các vector đặc trưng của các chuyên gia trong phép biến đổi này là độc lập, bỏ qua mối tương quan giữa các chuyên gia. Tuy nhiên, cấu trúc MoE thường lựa chọn nhiều hơn một chuyên gia mỗi lần, và nhiều chuyên gia thường tham gia vào tính toán một cách hợp tác, có nghĩa là should có mối tương quan vốn có giữa các chuyên gia.

--- TRANG 2 ---

2
Sẽ không nghi ngờ gì nữa là sẽ cải thiện độ chính xác của mô hình, nếu mối quan hệ giữa các chuyên gia được xem xét trong quá trình lựa chọn chuyên gia.

Các đóng góp chính của công trình này được tóm tắt như sau:
1) Đề xuất Attention Router xem xét mối tương quan giữa các chuyên gia, dẫn đến độ chính xác cao hơn so với cấu trúc router cổ điển.
2) Phát hành mô hình Yuan 2.0-M32 với 40B tham số tổng cộng và 3.7B tham số hoạt động. Có tổng cộng 32 chuyên gia và 2 chuyên gia được kích hoạt cho mỗi token. Chi phí tính toán cho huấn luyện chỉ là 1/16 so với mô hình dày đặc có quy mô tham số tương tự, và chi phí suy luận tương tự như mô hình dày đặc với 3.7B tham số.

2. Các công trình liên quan
Gshard (Lepikhin et al., 2020), một mô hình khổng lồ với hơn 600 tỷ tham số, giới thiệu phương pháp MoE vào Transformer Encoder lần đầu tiên, và cung cấp một kiến trúc tính toán song song phân tán hiệu quả với định tuyến qua các bộ tăng tốc. Switch Transformer (Fedus, Zoph and Shazeer, 2022) đơn giản hóa thuật toán định tuyến MoE với định tuyến thưa thớt. Zhou et al. (2022) đã đề xuất một thuật toán định tuyến MoE mới được gọi là thuật toán định tuyến Expert Choice (EC) để đạt được cân bằng tải tối ưu trong hệ thống MoE. Mô hình Mistral 8x7B vượt trội hơn mô hình có tham số lớn hơn 10 lần trong một số benchmark của con người với mạng định tuyến cổ điển (Jiang et al., 2024). DBRX sử dụng kiến trúc MoE tinh tế và chọn 4 chuyên gia trong số 16 (Mosaic AI research, 2024). DeepSeekMoE cải thiện sự chuyên môn hóa của chuyên gia với phân đoạn chuyên gia tinh tế cũng như cách ly chuyên gia chia sẻ (Dai et al., 2024). Các chuyên gia chia sẻ kích hoạt token cho tất cả đầu vào và không bị ảnh hưởng bởi mô-đun định tuyến, điều này có thể giúp các chuyên gia khác tập trung hơn vào các lĩnh vực kiến thức độc đáo của họ.

Các công trình được đề cập ở trên nỗ lực tối ưu hóa chiến lược định tuyến của các chuyên gia, trong khi mạng router vẫn là mạng cổ điển bỏ qua mối tương quan giữa các chuyên gia. Công trình của chúng tôi tập trung vào việc thiết kế mạng router để kết hợp mối tương quan vốn có giữa các chuyên gia. Mạng định tuyến được đề xuất trong bài báo này là bổ sung cho các công trình trước đây.

3. Kiến trúc mô hình
Yuan 2.0-M32 dựa trên cấu trúc mô hình của Yuan 2.0-2B (Wu et al., 2023). Yuan 2.0 giới thiệu sự phụ thuộc cục bộ của các token đầu vào với Localized Filtering-based Attention (LFA), để cải thiện độ chính xác của mô hình. Trong Yuan 2.0-M32, mạng feed-forward dày đặc (FFN) của mỗi lớp được thay thế bằng một thành phần MoE.

Hình 1 hiển thị kiến trúc của lớp MoE được áp dụng trong mô hình của chúng tôi. Lấy bốn FFN làm ví dụ (thực tế là 32 chuyên gia), mỗi lớp MoE được cấu thành từ một nhóm các FFN riêng lẻ làm chuyên gia. Mạng Router phía trước các chuyên gia điều phối token đầu vào đến (các) chuyên gia liên quan. Mạng Router cổ điển về cơ bản thiết lập một vector đặc trưng cho mỗi chuyên gia, và tính toán tích vô hướng giữa token đầu vào và vector đặc trưng của mỗi chuyên gia để có được khả năng cụ thể giữa token và chuyên gia. Các chuyên gia có khả năng mạnh nhất được lựa chọn để kích hoạt và tham gia vào các tính toán tiếp theo.

--- TRANG 3 ---

3

Hình 1: Minh họa của Yuan 2.0-M32. Hình bên trái trình bày việc mở rộng quy mô của kiến trúc Yuan 2.0 với các lớp MoE. Lớp MoE thay thế cho lớp feed forward trong Yuan 2.0. Hình bên phải trình bày cấu trúc lớp MoE. Trong mô hình của chúng tôi, mỗi token đầu vào sẽ được gán cho 2 chuyên gia trong tổng số 32, trong khi trong hình chúng tôi hiển thị 4 chuyên gia làm ví dụ. Đầu ra của MoE là tổng có trọng số của các chuyên gia được lựa chọn. N là số lượng lớp.

(a) Router cổ điển
(b) Attention router

Hình 2: Tổng quan về cấu trúc attention router.

--- TRANG 4 ---

4
Hình 2(a) trình bày cấu trúc của mạng router cổ điển. Các vector đặc trưng của mỗi chuyên gia độc lập với nhau, và mối tương quan giữa các chuyên gia bị bỏ qua khi tính toán xác suất. Thực tế, trong hầu hết các mô hình MoE (Lepikhin et al., 2020; Fedus, Zoph and Shazeer, 2022; Zhou et al., 2022), hai hoặc nhiều chuyên gia thường được lựa chọn để tham gia vào các tính toán tiếp theo, điều này tự nhiên mang lại mối tương quan mạnh mẽ giữa các chuyên gia. Việc xem xét mối tương quan giữa các chuyên gia chắc chắn sẽ góp phần cải thiện độ chính xác.

Hình 2(b) trình bày kiến trúc của Attention Router, một mạng router mới được đề xuất trong công trình này, kết hợp mối tương quan giữa các chuyên gia bằng cách áp dụng cơ chế Attention. Một ma trận hệ số đại diện cho mối tương quan giữa các chuyên gia được xây dựng, sau đó được áp dụng vào tính toán cho giá trị xác suất cuối cùng. Cụ thể, với N chuyên gia cho một vector token (𝐼∈𝑅𝑑), quá trình định tuyến chuyên gia như sau:

𝑄=𝑊𝐼,      𝑊∈𝑅𝑁×𝑑
𝐾=𝑊′𝐼,      𝑊′∈𝑅𝑁×𝑑
𝑉=𝑊′′𝐼,      𝑊′′∈𝑅𝑁×𝑑
𝑃=Softmax(𝑄𝐾𝑇)V,     𝑃∈𝑅𝑁

Sau đó, 𝑀 chuyên gia được chọn bằng cách lựa chọn top 𝑀 giá trị của P. Trong bài báo này, chúng tôi đặt 𝑀=2, N=32, d=2048.

Mô hình  Tham số (M)  Test loss
Attention router  826.0  2.109
Classical router  825.8  2.117
Shared Expert router  825.8  2.117

Bảng 1: So sánh các cấu trúc router khác nhau

Bảng 1 liệt kê kết quả độ chính xác của các router khác nhau. Mô hình của chúng tôi được kiểm tra trên 8 chuyên gia có thể huấn luyện với Attention Router. Mô hình router cổ điển có 8 chuyên gia có thể huấn luyện để đảm bảo quy mô tham số tương tự, và cấu trúc router giống với cấu trúc được áp dụng trong Mixtral 8*7B (Jiang et al., 2024), đó là Softmax trên một lớp tuyến tính. Shared Expert router áp dụng chiến lược Shared Expert Isolation với kiến trúc router cổ điển (Dai et al., 2014). Có 2 chuyên gia cố định để nắm bắt kiến thức chung và top-2 của 14 chuyên gia tùy chọn làm chuyên gia chuyên môn. Đầu ra của MoE là sự kết hợp của chuyên gia cố định và những chuyên gia được lựa chọn bởi router. Tất cả ba mô hình đều được huấn luyện với 30B token và kiểm tra với 10B token khác. Xem xét kết quả giữa classical router và Shared Expert router, chúng tôi thấy rằng router sau có cùng test loss nhưng với thời gian huấn luyện nhiều hơn 7.35%. Hiệu quả tính toán của Shared Expert tương đối thấp, và nó không mang lại độ chính xác huấn luyện tốt hơn so với chiến lược MOE cổ điển. Do đó trong mô hình của chúng tôi, chúng tôi áp dụng chiến lược định tuyến cổ điển mà không có chuyên gia chia sẻ nào.

Chúng tôi kiểm tra khả năng mở rộng của mô hình bằng cách tăng số lượng chuyên gia và cố định kích thước tham số mỗi chuyên gia. Việc tăng số lượng chuyên gia có thể huấn luyện chỉ thay đổi khả năng mô hình, nhưng không thay đổi tham số mô hình được kích hoạt thực tế. Tất cả các mô hình đều được huấn luyện với 50B token và kiểm tra với 10B token khác. Chúng tôi đặt số chuyên gia được kích hoạt là 2, và các siêu tham số cho huấn luyện là giống nhau cho ba mô hình. Hiệu ứng mở rộng chuyên gia được đo bằng test loss sau khi huấn luyện với 50B token (Bảng 2). So với mô hình có 8 chuyên gia có thể huấn luyện, mô hình có 16 chuyên gia có loss thấp hơn 2%, và mô hình có 32 chuyên gia có loss thấp hơn 3.6%. Chúng tôi chọn 32 chuyên gia cho Yuan 2.0-M32 xem xét độ chính xác của nó.

Mô hình  Test loss
8 chuyên gia  1.820
16 chuyên gia  1.787
32 chuyên gia  1.754

Bảng 2: Kết quả của các thí nghiệm mở rộng

4. Huấn luyện
4.1 Huấn luyện mô hình
Tương tự như chiến lược huấn luyện của Yuan 2.0, Yuan 2.0-M32 được huấn luyện với sự kết hợp của song song dữ liệu và song song pipeline, tuy nhiên không sử dụng song song tensor hoặc song song optimizer. Các siêu tham số huấn luyện được liệt kê trong Phụ lục A. Hình 3 trình bày đường cong loss, và loss huấn luyện cuối cùng là 1.22.

Hình 3: Loss pre-training của Yuan2.0-M32 trên 2000B token

4.2 Fine-tuning
Trong quá trình fine-tuning, chúng tôi mở rộng độ dài chuỗi lên 16384. Theo công trình của CodeLLama (Rozière et al., 2023), chúng tôi đặt lại giá trị cơ sở của tần số Rotary Position Embedding (RoPE) để tránh sự suy giảm trong điểm attention với các chuỗi dài hơn. Thay vì chỉ đơn giản tăng giá trị cơ sở từ 1000 lên một giá trị lớn hơn nhiều (ví dụ 1000000), chúng tôi tính toán cơ sở mới với NTK-aware (bloc97, 2023), tức là

𝑏′=𝑏∙𝑠|𝐷|/|𝐷|−2.

Trong đó 𝑏 là giá trị cơ sở ban đầu (b=10000). 𝑠 là số lần mở rộng từ độ dài ngữ cảnh ban đầu đến độ dài ngữ cảnh mở rộng. Vì chúng tôi mở rộng độ dài ngữ cảnh từ 4096 lên 16384, s bằng 4. |𝐷| là 128 trong thiết lập của chúng tôi. Do đó, cơ sở mới 𝑏′ được tính toán là 40890.

Chúng tôi cũng so sánh hiệu suất của mô hình Yuan 2.0-M32 pre-trained với cơ sở mới theo kiểu NTK-aware, và với các giá trị cơ sở khác (40000, 80000, 160000, 320000, 640000, 1280000, 2560000, 5120000, và 10240000) trong nhiệm vụ needle-retrieval với độ dài chuỗi lên đến 16K (gkamradt, 2023).

--- TRANG 5 ---

5
Chúng tôi thấy rằng cơ sở mới theo kiểu NTK-aware, 40890, hoạt động tốt hơn. Do đó 40890 được áp dụng trong quá trình fine-tuning.

4.3 Tập dữ liệu pre-training
Yuan 2.0-M32 được pre-trained với một tập dữ liệu song ngữ 2000B token từ đầu. Dữ liệu gốc cho pre-training chứa hơn 3400B token, và trọng số cho mỗi danh mục được điều chỉnh theo chất lượng và số lượng dữ liệu.

Corpus pre-training toàn diện bao gồm:
- 44 tập dữ liệu con bao gồm dữ liệu web crawl, wiki, luận văn học thuật, sách, mã, toán học và công thức, và chuyên môn theo lĩnh vực cụ thể. Một số trong số đó là tập dữ liệu nguồn mở và những tập khác được tạo bởi Yuan 2.0.
- Các phần của dữ liệu common crawl, sách tiếng Trung, đối thoại và dữ liệu tin tức tiếng Trung được kế thừa từ Yuan 1.0 (Wu et al., 2021). Hầu hết dữ liệu pre-training trong Yuan 2.0 cũng được tái sử dụng.

Thông tin chi tiết về việc xây dựng và nguồn gốc của mỗi tập dữ liệu có sẵn dưới đây.

Web (25.2%). Dữ liệu web crawling là một bộ sưu tập từ các tập dữ liệu nguồn mở và dữ liệu common crawl được xử lý trong các công trình trước đây của chúng tôi (Yuan 1.0). Vui lòng tham khảo Yuan 1.0 để biết thêm chi tiết về Massive Data Filtering System (MDFS) trích xuất nội dung chất lượng cao hơn từ các ngữ cảnh web.

Dữ liệu Bách khoa toàn thư (1.2%), luận văn (0.84%), sách (6.4%) và dịch thuật (1.1%) được kế thừa từ tập dữ liệu Yuan 1.0 và Yuan 2.0.

Mã (47.5%). Tập dữ liệu mã được mở rộng đáng kể so với Yuan 2.0. Chúng tôi áp dụng mã từ Stack v2 (Lozhkov et al., 2024). Các comment trong Stack v2 được dịch sang tiếng Trung. Dữ liệu mã tổng hợp được tạo ra bằng phương pháp tương tự như trong Yuan 2.0.

Toán học (6.36%). Tất cả dữ liệu toán học từ Yuan 2.0 được tái sử dụng. Dữ liệu chủ yếu từ các tập dữ liệu nguồn mở, bao gồm proof-pile v1 (Azerbayev, 2022) và v2 (Paster et al., 2023), AMPS (Hendrycks et al. 2021), MathPile (Wang, Xia and Liu, 2023) và StackMathQA (Zhang, 2024). Một tập dữ liệu tổng hợp cho tính toán số học được tạo ra bằng Python để có lợi cho bốn phép toán số học.

Lĩnh vực cụ thể (1.93%) là một tập dữ liệu với kiến thức từ các nền tảng khác nhau.

4.4 Tập dữ liệu fine-tuning
Tập dữ liệu fine-tuning được mở rộng dựa trên tập dữ liệu được áp dụng trong Yuan 2.0.

Tập dữ liệu Code Instruction. Tất cả dữ liệu mã hóa với hướng dẫn tiếng Trung và các phần với comment tiếng Anh được tạo ra bằng LLM. Khoảng 30% dữ liệu hướng dẫn mã là tiếng Anh, và phần còn lại là tiếng Trung. Dữ liệu tổng hợp được chế tạo theo cách bắt chước mã Python với comment tiếng Trung về tạo prompt và chiến lược làm sạch dữ liệu.

- Mã Python với comment tiếng Anh được thu thập từ Magicoder-Evol-Instruct-110K (Wei et al., 2023) và CodeFeedback-Filtered-Instruction (Zheng et al., 2024). Dữ liệu hướng dẫn có tag ngôn ngữ như "python" được trích xuất từ tập dữ liệu, và được tổ chức theo định dạng như được hiển thị trong Phụ lục B. Tập dữ liệu cũng được mở rộng với phương pháp Evol-instruct (Xu et al., 2023) và Self-instruct (Wang et al., 2022) được áp dụng trong việc xây dựng mã Python tiếng Trung.

- Các mã khác như C/C++/Go/Java/SQL/Shell v.v., với comment tiếng Anh từ tập dữ liệu nguồn mở (Wei et al., 2023; b-mc2, 2023; Clinton, 2013; gayathrimanoj, 2023a, b; byroneverson, 2024; Zheng et al., 2024) được xử lý theo cách tương tự với mã Python. Các chiến lược làm sạch tương tự như phương pháp trong Yuan 2.0. Một sandbox được thiết kế để trích xuất các dòng có thể biên dịch và thực thi trong mã được tạo ra, và giữ lại các dòng vượt qua ít nhất một unit test.

Tập dữ liệu Math Instruction. Tập dữ liệu hướng dẫn toán học đều được kế thừa từ tập dữ liệu fine-tuning trong Yuan 2.0. Để cải thiện khả năng của mô hình giải quyết các bài toán toán học bằng phương pháp lập trình, chúng tôi xây dựng dữ liệu toán học prompting Program of Thoughts (PoT) (Chen et al., 2022). PoT chuyển đổi bài toán toán học thành nhiệm vụ tạo mã để thực hiện tính toán bằng Python.

Tập dữ liệu Safety Instruction. Ngoài tập dữ liệu chat của Yuan 2.0, chúng tôi xây dựng một tập dữ liệu alignment an toàn song ngữ dựa trên một tập dữ liệu alignment an toàn nguồn mở (Ji et al., 2024). Chúng tôi chỉ lấy các câu hỏi từ tập dữ liệu công cộng, và tăng sự đa dạng của các câu hỏi và tái tạo câu trả lời tiếng Trung và tiếng Anh bằng các mô hình ngôn ngữ lớn.

4.5 Tokenizer
Đối với Yuan 2.0-M32, các tokenizer tiếng Anh và tiếng Trung được kế thừa từ những tokenizer được áp dụng trong Yuan 2.0.

5. Kết quả
Chúng tôi đánh giá Yuan 2.0-M32 trên Humaneval (Chen et al., 2021) cho tạo mã, GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021) cho giải quyết bài toán toán học, ARC (Clark et al., 2018) cho kiến thức khoa học và suy luận, và MMLU (Hendrycks et al., 2020) như một benchmark tích hợp.

5.1 Tạo mã
Khả năng tạo mã được đánh giá với HumanEval Benchmark. Phương pháp đánh giá và prompt tương tự như những gì được đề cập trong Yuan 2.0, và prompt tiếng Anh được xây dựng như Phụ lục B.

Mô hình Tham số (B) Tham số hoạt động (B) Human Eval (zero-shot)
Llama 3-70B 70 70 81.7
Llama 3-8B 8 8 62.2
Phi-3-medium 14 14 62.2
Phi-3-small 7 7 61
Phi-3-mini 3.8 3.8 58.5
Qwen1.5-72B 72 72 68.9
Deepseek V2 236 21 81.1
Mixtral-8×22B 141 39 45.1
Mixtral-8×7B 47 12.9 40.2
Yuan 2.0-M32 40 3.7 74.4
Yuan 2.0-M32 40 3.7 78.1 (14 shots)

Bảng 3: So sánh Yuan 2.0-M32 và các mô hình khác trên Human Eval pass@1.

--- TRANG 6 ---

6

Mô hình được kỳ vọng hoàn thành hàm sau <sep>. Và hàm được tạo ra sẽ được đánh giá bằng unit test. Kết quả từ zero-shot của Yuan 2.0-M32 và so sánh với các mô hình khác được hiển thị trong Bảng 3. Kết quả của Yuan 2.0-M32 chỉ đứng sau DeepseekV2 (DeepSeek-AI, 2024) và Llama3-70B (AI Meta, 2024), và vượt xa các mô hình khác, thậm chí khi các tham số hoạt động và chi phí tính toán của nó thấp hơn nhiều so với những mô hình khác. So với Deepseek V2, mô hình của chúng tôi sử dụng ít hơn một phần tư tham số hoạt động và ít hơn một phần năm nỗ lực tính toán mỗi token, trong khi đạt được hơn 90% mức độ chính xác của nó. Và so với llama3-70B, khoảng cách giữa tham số mô hình và tính toán thậm chí còn lớn hơn, và chúng tôi vẫn đạt 91% mức độ của nó. Yuan 2.0-M32 thể hiện khả năng lập trình đáng tin cậy với ba phần tư số câu hỏi được vượt qua. Yuan 2.0-M32 giỏi trong few shot learning. Độ chính xác của Humaneval được cải thiện lên 78.0 bằng cách sử dụng 14 shots.

5.2 Toán học
Khả năng toán học của Yuan 2.0-M32 được đánh giá với benchmark GSM8K và MATH. Các prompt và chiến lược kiểm tra cho GSM8K tương tự như được áp dụng cho Yuan 2.0, và sự khác biệt duy nhất là chúng tôi chạy nó với 8 shots (Bảng 4).

Mô hình Tham số (B) Tham số hoạt động (B) GSM8K MATH
Llama 3-70B 70 70 93.0 50.4
Llama 3-8B 8 8 79.6 30
Phi-3-medium 14 14 91.0 -
Phi-3-small 7 7 89.6 -
Phi-3-mini 3.8 3.8 82.5 -
Qwen1.5-72B 72 72 81.9 40.6
Deepseek V2 236 21 92.2 53.9
Mixtral-8×22B 141 39 78.6 41.8
Mixtral-8×7B 47 12.9 58.4 28.4
Yuan 2.0-M32 40 3.7 92.7 55.9

Bảng 4: So sánh Yuan 2.0-M32 và các mô hình khác trên GSM8K và MATH

MATH là một tập dữ liệu với 12.500 bài toán Mathematical Competition QA thách thức. Mỗi câu hỏi trong tập dữ liệu này có một giải pháp từng bước hoàn chỉnh dẫn dắt mô hình tạo ra sự dẫn xuất và giải thích câu trả lời. Câu trả lời cho các câu hỏi có thể là các giá trị số (0.5, 1/2, v.v.), hoặc các biểu thức toán học (y=2x+5, x2+2x-1, 2a+b, v.v.). Yuan 2.0-M32 tạo ra câu trả lời cuối cùng bằng phương pháp chain of thought (CoT) với 4 shots. Các câu trả lời sẽ được trích xuất từ phân tích và chuyển đổi thành định dạng thống nhất. Đối với kết quả số, đầu ra tương đương về mặt toán học ở tất cả các định dạng đều được chấp nhận. Câu trả lời của \frac{1}{2}, 1/2, 0.5, 0.50 đều được chuyển đổi thành 0.5 và được chấp nhận là cùng một kết quả. Đối với các biểu thức toán học, chúng tôi loại bỏ ký hiệu tab và space, và thống nhất biểu thức chính quy của phép toán số học. Ví dụ, y=(2𝑥+1)5⁄, y=2𝑥+15, y=2𝑥5+15, y=0.4x+0.2, ..., v.v., đều được chấp nhận là cùng một câu trả lời. Kết quả cuối cùng được xử lý được so sánh với câu trả lời ground truth, và đánh giá bằng điểm EM (exact match).

Từ kết quả được hiển thị trong Bảng 4, chúng ta có thể thấy Yuan 2.0-M32 đạt điểm cao nhất trên benchmark MATH. So với Mixtral-8×7B, có tham số hoạt động lớn hơn 3.48 lần so với Yuan 2.0-M32, điểm của Yuan thậm chí gần gấp đôi. Trên GSM8K, Yuan 2.0-M32 cũng đạt được điểm rất gần với Llama 3-70B, và vượt trội hơn các mô hình khác.

5.3 MMLU
Massive Multitask Language Understanding (MMLU) bao gồm 57 môn học trong STEM, nhân văn, khoa học xã hội, v.v., từ các nhiệm vụ ngôn ngữ cơ bản đến các nhiệm vụ suy luận logic cao cấp. Tất cả các câu hỏi trong MMLU đều là câu hỏi QA trắc nghiệm bằng tiếng Anh. Mô hình được kỳ vọng tạo ra lựa chọn đúng hoặc phân tích tương ứng.

Dữ liệu đầu vào cho Yuan 2.0-M32 được tổ chức như Phụ lục B. Văn bản trước <sep> được gửi đến mô hình, và tất cả câu trả lời liên quan đến câu trả lời đúng hoặc nhãn lựa chọn được áp dụng là đúng.

Độ chính xác cuối cùng được đo bằng MC1 (Bảng 5). Kết quả trên MMLU thể hiện khả năng của mô hình chúng tôi trong các lĩnh vực khác nhau. Yuan 2.0-M32 vượt trội hơn Mixtral-8×7B, Phi-3-mini, và Llama 3-8B về hiệu suất.

Mô hình Tham số (B) Tham số hoạt động (B) MMLU
Llama 3-70B 70 70 80.3
Llama 3-8B 8 8 68.4
Phi-3-medium 14 14 78.0
Phi-3-small 7 7 75.7
Phi-3-mini 3.8 3.8 68.8
Qwen1.5-72B 72 72 76.2
Deepseek V2 236 21 77.8
Mixtral-8×22B 141 39 77.8
Mixtral-8×7B 47 12.9 70.6
Yuan 2.0-M32 40 3.7 72.2

Bảng 5: So sánh Yuan 2.0-M32 và các mô hình khác trên MMLU

5.4 ARC
AI2 Reasoning Challenge (ARC) benchmark là một tập dữ liệu QA trắc nghiệm chứa các câu hỏi từ các kỳ thi khoa học từ lớp 3 đến lớp 9. Nó được chia thành các phần Easy và Challenge, với phần sau chứa các phần phức tạp hơn cần suy luận thêm. Chúng tôi kiểm tra mô hình của mình trên phần Challenge.

Mô hình Tham số (B) Tham số hoạt động (B) ARC-C
Llama 3-70B 70 70 93.3
Llama 3-8B 8 8 78.6
Phi-3-medium 14 14 91.6
Phi-3-small 7 7 90.7
Phi-3-mini 3.8 3.8 84.9
Qwen1.5-72B 72 72 91.7
Deepseek V2 236 21 92.3
Mixtral-8×22B 141 39 91.3
Mixtral-8×7B 47 12.9 85.9
Yuan 2.0-M32 40 3.7 95.8

Bảng 6: So sánh Yuan 2.0-M32 và các mô hình khác trên ARC-Challenge

--- TRANG 7 ---

7

Câu hỏi và các lựa chọn được nối trực tiếp và tách biệt bằng <n>, được prompted như trong Phụ lục B (tương tự như mẫu của MMLU). Văn bản trước <sep> được gửi đến mô hình, và mô hình được kỳ vọng tạo ra một nhãn hoặc câu trả lời tương ứng. Câu trả lời được tạo ra được so sánh với ground truth, và kết quả được tính toán bằng target MC1.

Kết quả ARC-C được hiển thị trong Bảng 6, và nó cho thấy Yuan 2.0-M32 xuất sắc trong việc giải quyết các bài toán khoa học phức tạp—nó vượt trội hơn Llama3-70B trong benchmark này.

Mô hình Tham số (B) Tham số hoạt động (B) GFlops mỗi token (Suy luận) GFlops mỗi token (Fine-tune) Độ chính xác trung bình Độ chính xác trung bình/GFlops mỗi token (Suy luận)
Llama 3-70B 70 70 140 420 79.25 0.57
Llama 3-8B 8 8 16 48 64.15 4.00
Qwen1.5-72B 72 72 144 432 72.6 0.50
Deepseek V2 236 21 42 126 79.05 1.88
Mixtral-8×22B 141 39 78 234 72.38 0.93
Mixtral-8×7B 47 12.9 25.8 77.4 60.83 2.36
Yuan 2.0-M32 40 3.7 7.4 22.2 79.15 10.69

Bảng 7: So sánh Yuan 2.0-M32 và các mô hình khác về chất lượng so với kích thước. Độ chính xác trung bình được tính trung bình trên điểm của GSM-8K, Math, Humaneval, MMLU, và ARC-C.

Từ 5.1 đến 5.4, chúng tôi so sánh hiệu suất của mình với ba mô hình MoE (họ Mixtral, Deepseek) và sáu mô hình dày đặc (Qwen (Bai et al., 2023), họ Llama và họ Phi-3 (Abdin et al., 2024)), để đánh giá hiệu suất của Yuan 2.0-M32 trên các lĩnh vực khác nhau. Bảng 7 trình bày so sánh Yuan 2.0-M32 với các mô hình khác về độ chính xác so với tính toán. Yuan 2.0-M32 chỉ sử dụng 3.7B tham số hoạt động và 22.2 GFlops mỗi token cho fine-tuning, điều này là tiết kiệm nhất, để có được kết quả tương đương hoặc thậm chí vượt trội hơn các mô hình khác được liệt kê trong các bảng. Bảng 7 ngụ ý hiệu quả tính toán và hiệu suất xuất sắc trong quá trình suy luận của mô hình chúng tôi. Độ chính xác trung bình của Yuan 2.0-M32 là 79.15, có tính cạnh tranh với Llama3-70B. Và giá trị độ chính xác trung bình/Glops mỗi token là 10.69, lớn hơn 18.9 lần so với Llama3-70B.

6. Kết luận
Trong công trình này, chúng tôi giới thiệu Yuan 2.0-M32, một mô hình ngôn ngữ MoE song ngữ dựa trên Yuan 2.0. Attention Router được áp dụng trong mô hình này đạt được độ chính xác cao hơn so với mạng router cổ điển. Yuan 2.0-M32 chỉ sử dụng 3.7B tham số hoạt động và 7.4 GFlops suy luận mỗi token, cả hai đều khoảng 1/19 của Llama3-70B. Trong benchmark ARC-C, mô hình của chúng tôi vượt trội hơn Llama 3-70B 2.5 điểm chỉ với 5% tham số hoạt động. Đối với benchmark MATH, Yuan 2.0-M32 cũng đạt được điểm cao nhất (55.9), vượt trội hơn Llama 3-70B khoảng 10% với khoảng 5% chi phí tính toán. Kết quả ngụ ý rằng mô hình của chúng tôi có hiệu quả tính toán và hiệu suất xuất sắc trong quá trình suy luận. Chúng tôi phát hành các mô hình Yuan 2.0-M32 của mình tại Github để truy cập công cộng, như những gì chúng tôi đã làm cho Yuan 2.0, và hy vọng mô hình nguồn mở có thể mang lại lợi ích cho sự phát triển của LLM và hệ sinh thái ngành AI.

--- TRANG 8 ---

8

Tài liệu tham khảo:

Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A., Awadalla, H., ... & Zhou, X. (2024). Phi-3 technical report: A highly capable language model locally on your phone. arxiv preprint arxiv:2404.14219.

Azerbayev, Z., Ayers, E., & Piotrowski, B. (2022). proof-pile. https://github.com/zhangir-azerbayev/proof-pile

Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., ... & Zhu, T. (2023). Qwen technical report. arxiv preprint arxiv:2309.16609.

bloc97 (2023). NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware

b-mc2 (2023). sql-create-context Dataset. https://huggingface.co/datasets/b-mc2/sql-create-context.

Byroneverson (2024). shell-cmd-instruct. https://huggingface.co/datasets/byroneverson/shell-cmd-instruct

Chen, W., Ma, X., Wang, X., & Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., ... & Zaremba, W. (2021). Evaluating large language models trained on code. arxiv preprint arxiv:2107.03374.

Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., & Tafjo rd, O. (2018). Think you have solved question answering? try arc, the ai2 reasoning challenge. arxiv preprint arxiv:1803.05457.

Clinton (2023). Text-to-sql-v1. https://huggingface.co/datasets/Clinton/Text-to-sql-v1

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., ... & Schulman, J. (2021). Training verifiers to solve math word problems. arxiv preprint arxiv:2110.14168.

Dai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen, D., ... & Liang, W. (2024). Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066. https://github.com/deepseek-ai/DeepSeek-MoE

DeepSeek-AI et al., (2024). DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. arXiv preprint arXiv: 2405.04434.

--- TRANG 9 ---

9

Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 1-39.

Gayathrimanoj (2023a). dataset_shell. https://huggingface.co/datasets/gayathrimanoj/dataset_shell.

Gkamradt (2023). Needle in a haystack - pressure testing llms. https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main. [Online; accessed 7Feb-2024].

Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., ... & Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2020). Measuring massive multitask language understanding. arxiv preprint arxiv:2009.03300.

Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1), 79-87.

Ji, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., ... & Yang, Y. (2024). Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36.

Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., ... & Sayed, W. E. (2024). Mixtral of experts. arXiv preprint arXiv:2401.04088.

Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2020). Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.

Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., ... & de Vries, H. (2024). StarCoder 2 and The Stack v2: The Next Generation. arXiv preprint arXiv:2402.19173.

Meta, A. I. (2024). Introducing Meta Llama 3: The most capable openly available LLM to date. Meta AI Blog (accessed 2024–04–20). There is no corresponding record for this reference.

Mosaic Research Team (2024). Introducing DBRX: A New State-of-the-Art Open LLM

Paster, K., Santos, M. D., Azerbayev, Z., & Ba, J. (2023). Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786.

Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., ... & Synnaeve, G. (2023). Code llama: Open foundation models for code. arxiv preprint arxiv:2308.12950.

Shazeer N, Mirhoseini A, Maziarz K, et al. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538.

--- TRANG 10 ---

10

Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2022). Self-instruct: Aligning language models with self-generated instructions. arxiv preprint arxiv:2212.10560.

Wang, Z., Xia, R., & Liu, P. (2023). Generative AI for Math: Part I --MathPile: A Billion-Token-Scale Pretraining Corpus for Math. arXiv preprint arXiv:2312.17120.

Wei, Y., Wang, Z., Liu, J., Ding, Y., & Zhang, L. (2023). Magicoder: Source code is all you need. arxiv preprint arxiv:2312.02120.

Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., ... & Zhang, X. (2021). Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv preprint arXiv:2110.04725.

Wu, S., Zhao, X., Wang, S., Luo, J., Li, L., Chen, X., ... & Wang, C. (2023). YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. arxiv preprint arxiv:2311.15786.

Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., ... & Jiang, D. (2023). Wizardlm: Empowering large language models to follow complex instructions. arxiv preprint arxiv:2304.12244.

Zhang, Y. (2024) StackMathQA: A Curated Collection of 2 Million Mathematical Questions and Answers Sourced from Stack Exchange. https://github.com/yifanzhang-pro/StackMathQA

Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., ... & Yue, X. (2024). OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. arxiv preprint arxiv:2402.14658.

Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., ... & Laudon, J. (2022). Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35, 7103-7114.

--- TRANG 11 ---

11

Phụ lục A: Siêu tham số cho Pre-training và fine-tuning

Tham số Pre-train Fine-tune
Tốc độ học (LR) 1.0e-5 ~ 1.0e-4 8.0e-5
Kiểu giảm LR cosine constant
Độ dài chuỗi 4096 16384
Kích thước Batch toàn cục 1536 1152

Phụ lục B: Ví dụ prompt cho các nhiệm vụ downstream

Tạo mã
Instruction: Given two positive integers a and b, return the even digits between a and b, in ascending order.

For example:
generate_integers(2, 8) => [2, 4, 6, 8]
generate_integers(8, 2) => [2, 4, 6, 8]
generate_integers(10, 14) => []
Response:
<sep>
```python
def generate_integers(a, b):

MMLU
Glucose is transported into the muscle cell:<n> A. via protein transporters called GLUT4. <n> B. only in the presence of insulin. <n> C. via hexokinase. <n>D. via monocarbylic acid transporters. <sep>
A.

ARC-C
few-shot examples<n>question<n>optionA<n> optionB<n> optionC<n> optionD<sep> answer
