# 2402.10644.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2402.10644.pdf
# File size: 1023852 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Linear Transformers with Learnable Kernel Functions are Better
In-Context Models
Yaroslav Aksenovâ™£q, Nikita Balaganskyâ™£r, Sofia Maria Lo Cicero Vainaâ™ âˆ—,
Boris Shaposhnikovâ™£, Alexey Gorbatovskiâ™£, Daniil Gavrilovâ™£
â™£TinkoffqHigher School of Economics
rMoscow Institute of Physics and Technologyâ™ Innopolis University
n.n.balaganskiy@tinkoff.ru
Abstract
Advancing the frontier of subquadratic archi-
tectures for Language Models (LMs) is crucial
intherapidlyevolvingfieldofnaturallanguage
processing. Current innovations, including
State Space Models, were initially celebrated
forsurpassingTransformerperformanceonlan-
guage modeling tasks. However, these mod-
els have revealed deficiencies in essential In-
ContextLearningcapabilitiesâ€“adomainwhere
theTransformertraditionallyshines. TheBased
modelemergedasahybridsolution,blending
a Linear Transformer with a kernel inspired
by the Taylor expansion of exponential func-
tions, augmented by convolutional networks.
MirroringtheTransformerâ€™sin-contextadept-
ness,itbecameastrongcontenderinthefield.
In our work, we present a singular, elegant al-
teration to the Based kernel that amplifies its
In-ContextLearningabilitiesevaluatedwiththe
Multi-QueryAssociativeRecalltaskandover-
alllanguagemodelingprocess,asdemonstrated
on the Pile dataset.
1 Introduction
LargeLanguageModels(LLMs)arerevolutioniz-
ing the field of natural language processing and
establishingnewbenchmarksacrossvarioustasks
(Touvron et al., 2023; Jiang et al., 2023). Neverthe-
less,despitetheirtriumphs,mostofthesemodels
arebuiltonTransformerframeworksthatemploy
attention mechanisms. These mechanisms scale
poorly with long text sequences, leading to im-
practicalcomputationalcomplexityforextending
contextual processing (Vaswani et al., 2017; Tay
et al., 2021).
To address this constraint, several alternatives
to Transformers were proposed. Katharopoulos
et al. (2020) suggested replacing the exponential
functionintheattentionmechanismwiththekernel
functiontochangetheorderofcomputationsand
âˆ—Work done while at Tinkoff.thusmoveawayfromquadraticcomplexityofthe
sequence length. However, when compared to
vanilla Transformers, this approach leads to a drop
inperformance. Furthermore,thekernelfunction
selection is a topic still in need of consideration.
An alternative way to define a linear model is to
utilizeStateSpaceModels(SSMs)(Guetal.,2022;
Smithetal.,2023;GuandDao,2023),whichare
capable ofproducing quality that iscomparable to
Transformers when measured with perplexity on
language modeling.
Notably,bothLinearTransformersKatharopou-
los et al. (2020) and SSMs can be described as
RecurrentNeural Networks(RNNs)(Chunget al.,
2014;HochreiterandSchmidhuber,1997),which
havetheirlimitationswhenitcomestomanaging
lengthy dependencies within texts since memory
capacitycanbeoverrunasthevolumeofinforma-
tion increases. Additionally, while the hidden state
of RNNs is larger for Linear Transformers than for
SSMs, the latter showed higher text modeling qual-
ity. The introduction of the Based model (Arora
et al., 2024) attempted to address the abovemen-
tioned challenges by utilizing a hybrid architecture
(Fu et al., 2023a) based on a Linear Transformer
with a novel kernel function derived from a Taylor
expansionofanexponentialfunction. Aroraetal.
(2024)demonstratedthattheBasedmodelwasless
prone to performance issues when working with
longercontentthanothermodelswhenassessedon
the Multi-Query Associative Recall (MQAR) task.
Nonetheless, even the Based model experiences
adropinperformancewhenfacedwithextensive
contexts relative to the conventional transformer
architecture.
Aprofoundcomprehensionoftheprocesses oc-
curringwithintheBasedarchitecturesisessential
for their advancement. Upon examining how at-
tention scores are distributed, we argue that the
kernel function previously adopted in Based can-
notbeconsideredoptimal,resultinginlimitationsarXiv:2402.10644v2  [cs.LG]  5 Jun 2024

--- PAGE 2 ---
64 128 256 5120.00.20.40.60.81.0Accuracy
Sequence Length: 128
64 128 256 512
Sequence Length: 256
64 128 256 512
Sequence Length: 512
64 128 256 512
Sequence Length: 1024
64 128 256 512
Sequence Length: 2048
Model Dim
ReBased Based RWKV Mamba onvAttention
Figure1: ResultsontheMQARdataset,designedtomeasureIn-ContextLearningcapabilitiesofanarchitecture
Aroraetal.(2023). ReBasedoutperformsallbaselinesexceptAttentionacrossdifferentsequencelengthsandmodel
sizes. See Section 5.2 for more details.
whendealingwithlengthycontextandsmallmodel
capacity. To address this issue, we introduce Re-
Based(Revisited Based), a novel variation of the
Linear Transformer model that improves the use of
attentionkernels. Thecruxofourdevelopmentlies
in addressing the inability of Based to disregard
specific tokens with zero probability during the
attentionprocess. Byrefiningthekernelfunction
and incorporating new architectural modifications,
wehavecreatedamodelthatimprovesaccuracyon
tasksinvolvingretrievinginformationfromlongse-
quencesoftokenswhilesimplifyingthecalculation
of the attention mechanism.
When testing our enhanced architecture on the
MQAR task, we found that ReBased surpasses the
originalBasedmodelacrossavarietyofcontexts
andmodelsizes. Additionally,aftertrainingwith
thePiledataset(Gaoetal.,2020),weobservedthat
ReBased performs better than its predecessor at
In-ContextLearningandexcelsatmodelingasso-
ciativedependenciesmeasuredthroughimproved
perplexity metrics.
2 Recent Work
The Vanilla Transformer architecture (Vaswani
etal.,2017),althoughwidelyusedinNLP(Radford
etal.,2019;Touvronetal.,2023;Devlinetal.,2019;
Jiangetal.,2023),suffersfromgrowingcomputa-
tionalandmemorydemands( O(ğ‘‘âˆ—ğ‘2)assequence
lengths (ğ‘) and head size ( ğ‘‘) increase). While this
is not much of a problem when it comes to shorter
sequences,itbecomesasignificantbottleneckwhen
working with longer ones.
Severalalternativearchitectureswereproposed
to address this issue. Katharopoulos et al. (2020)
suggested substituting the attention mechanismâ€™s
exponentialfunction,whichismeanttomeasurethe
similaritybetweenqueriesandkeys,withaproductofkernelfunctionsthatcanbeseparatelyevaluated
forqueriesandkeys. Thiskernel-basedapproach
reshapesthecomputationwithintheattentionmech-
anism, cutting the time and memory complexity to
O(ğ‘‘2âˆ—ğ‘). Additionally,duringinference,itsup-
ports sampling sequences with linear complexity
regarding length, similar to RNNs (Hochreiter and
Schmidhuber, 1997; Chung et al., 2014).
In a different approach, State Space Models
(SSMs)borrowfromcontroltheorytoofferasimpli-
fied structure akin to RNNs, but without activation
functions across time (Gu et al., 2022; Smith et al.,
2023; Gu et al., 2023). The Mamba model, also
knownasS6(GuandDao,2023),standsoutinthis
category, displaying enhanced learning of short-
term dependencies in texts compared to existing
pre-trainedLLMs(Jiangetal.,2023;Touvronetal.,
2023).
Despitetheseadvancements,thereisnostandard
way to fully evaluate these innovative architectures
to assess their performance limits. One standard
evaluation method is to pre-train a language model
and assess its perplexity with a given dataset, but
this may not truly reflect the modelâ€™s ability to
managelongcontextdependencies. Anotheroption
is to use the Long Range Arena (LRA) benchmark,
whichinvolvesclassification taskswithlong input
sequences. Though some new models have outper-
formedTransformersintheLRA,itisbelievedthat
thebenchmarkiscapableofintroducingbiasinthe
comparison (Amos et al., 2023).
One promising evaluation approach is to test an
architectureâ€™sIn-ContextLearningabilities. Olsson
et al. (2022) introduced the concept of Associative
Recall (AR), a task where the model learns to copy
atokenfromasequenceafteracertainpoint. How-
ever,whileinFuetal.(2023a)theassociativerecall
taskwasimplementedwithagoaltoretrieveonly
one token, Arora et al. (2023) noted that this task

--- PAGE 3 ---
couldbeconsideredoverlysimplistic. Thisledto
the creation of the Multi-Query Associative Recall
(MQAR) task, which requires retrieving multiple
tokens from the context. Findings on MQAR in-
dicate thatwhile newer modelsmaycompete with
Transformer in terms of perplexity, they can still
struggle with long contexts at small model sizes
because oftheirlimited In-ContextLearning capa-
bilities. Meanwhile, Transformers remain robust
against such factors. Lastly, Arora et al. (2024)
introducedLinearTransformerwithanewkernel
function(namelyBased),showcasingenhancedper-
formance on the MQAR task when compared to
Mamba.
Despite this improvement, compared to tradi-
tional Transformers, the problem of decline in
performance when handling long sequences with
smaller models still remains. Addressing this chal-
lenge is the primary goal of our paper.
3 Background
3.1 Linear Transformers
To fully grasp the Based architecture, it is vital
to first discuss the original Transformer model.
The attention mechanism, which is central to the
Transformerâ€™s functionality, evaluates the output ğ‘¦ğ‘–
for each position ğ‘–as follows
ğ‘¦ğ‘–=Ãğ‘–
ğ‘—=0sim(ğ‘„ğ‘–,ğ¾ğ‘—)ğ‘‰ğ‘—
Ãğ‘–
ğ‘›=0sim(ğ‘„ğ‘–,ğ¾ğ‘›),
where the term sim(ğ‘„ğ‘–,ğ¾ğ‘—)=expğ‘„ğ‘‡
ğ‘–ğ¾ğ‘—âˆš
ğ‘‘
rep-
resentsthe similarity betweenthe query ğ‘„ğ‘–andthe
keyğ¾ğ‘—usinganexponentialfunction. Despiteits
effectiveness, the original Transformerâ€™s reliance
onthisattentionmechanismincursaquadraticin-
crease in both computational time and memory
useasthesequencelengthgrows,whichbecomes
impractical for processing long sequences.
To address this scalability problem, Katharopou-
los et al. (2020) suggested replacing the direct
computation of similarity between ğ‘„andğ¾with a
transformationthroughanon-linearkernelfunction
ğœ™(Â·). Thisallowsforthefollowingapproximation:
sim(ğ‘„ğ‘–,ğ¾ğ‘—)â‰ˆğœ™ğ‘‡(ğ‘„ğ‘–)ğœ™(ğ¾ğ‘—). By implementing
this kernel, the Linear Transformer computes ğ‘¦ğ‘–as
ğ‘¦ğ‘–=Ãğ‘–
ğ‘—=0ğœ™ğ‘‡(ğ‘„ğ‘–)ğœ™(ğ¾ğ‘—)ğ‘‰ğ‘—
Ãğ‘–
ğ‘›=0ğœ™(ğ‘„ğ‘–)ğœ™ğ‘‡(ğ¾ğ‘›).
By rearranging the operations, we can express
the computation asğ‘¦ğ‘–=ğœ™ğ‘‡(ğ‘„ğ‘–)Ãğ‘–
ğ‘—=0ğœ™(ğ¾ğ‘—)ğ‘‰ğ‘‡
ğ‘—
ğœ™ğ‘‡(ğ‘„ğ‘–)Ãğ‘–
ğ‘›=0ğœ™(ğ¾ğ‘›).
By calculating ğœ™(ğ¾ğ‘—)ğ‘‰ğ‘‡
ğ‘—âˆˆRğ‘‘Ã—ğ‘‘upfront, the
complexityoftheattentionmechanismtransitions
tolinearwiththesequencelength,addressingthe
inefficiencies of the original model.
3.2 Based
Selecting an appropriate kernel function ğœ™(Â·)is
criticaltoaLinearTransformerâ€™sperformance. Var-
ious kernel functions have been proposed (Peng
et al., 2021; Schlag et al., 2021; Qin et al., 2022),
but on language modeling tasks, none have sur-
passed the original attention mechanism. However,
abreakthroughwasachievedbyAroraetal.(2024),
who introduced a novel kernel function inspired
by the Taylor series expansion of the exponential
function, defined as
sim(ğ‘,ğ‘˜)=1+ğ‘ğ‘‡ğ‘˜+(ğ‘ğ‘‡ğ‘˜)2
2.
The choice of this kernel is motivated by its
ability to approximate the exponential function
over a specific range of ğ‘ğ‘‡ğ‘˜values. In addition,
Arora et al. (2024) utilized a hybrid architecture
by combining linear attention with convolutional
layers since doing so was shown to help models
handle short non-associative dependencies in the
sequences (Fu et al., 2023a; Poli et al., 2023; Fu
et al., 2023b)
In doing so, when evaluated on the MQAR task,
the Based model demonstrated that it was capa-
ble of outperforming the Mamba model (Gu and
Dao, 2023) under circumstances of substantial con-
text length and constrained model capacity due
to smaller sizes. Nevertheless, compared to the
original Transformer, a discernible drop-off in per-
formanceremains,indicatingroomforfurtherim-
provement.
4 Revisiting Based
In our study, we explore the fundamental require-
mentsforkernelfunctions. Weexaminetheexpo-
nentialfunctionanditsapproximaterepresentation,
as depicted in Figure 2. We observe a limitation
in the approximation since its minimal value is
fixedat 0.5. Thisisproblematicforhandlinglong
sequences, as it is difficult to assign a near-zero
attentionscoretospecifictokenpairs. Ideally,we
want to be able to diminish the attention scores

--- PAGE 4 ---
2
 1
 0 1 2
Scalar Product qTk02468Similarity Value sim(q,k)Attention
Based
ReBased (Learnable)Figure 2: Similarity between ğ‘andğ‘˜with respect to
scalarproduct. NotethattheBasedmodelhasaminimal
sim(ğ‘,ğ‘˜)value of 0.5, which can lead to suboptimal
performance. We propose to learn the scale and shift
of the parabola jointly with the model and make it
possibleto zerooutthe similarityvalue. SeeSection4
for more details and Section 5.1 for experimental setup
description.
to zero, which would require significantly larger
values elsewhere in the normalization process with
the Based model.
To rectify this issue, a straightforward approach
would be to adjust the lowest point of the kernel
functiontozero. However,thissolutionpromptsus
toaskwhytheminimumvalueofthekernelfunc-
tionshouldoccurpreciselyat ğ‘ğ‘‡ğ‘˜=âˆ’1. Asusedin
theoriginalTransformer,thetraditionalexponential
similarityfunctionincreasesmonotonically,butthe
quadratic kernel has an optimal value to which it
decreases and then ascends from. Therefore, to de-
crease attention in the Transformer, one would aim
to minimizeğ‘ğ‘‡ğ‘˜. In contrast, the ideal ğ‘ğ‘‡ğ‘˜should
be exactlyâˆ’1for the Based method. Otherwise,
the attentionscore would increase. This condition
mayinduceless-than-idealtrainingoutcomesand
degrade the modelâ€™s accuracy.
These challengeslead usto conjecture thatif the
quadratickernelisusedtocalculatethesimilarity
betweenğ‘andğ‘˜,wemustconsidertherangeofpo-
tentialğ‘ğ‘‡ğ‘˜valuesandcreateadjustableparameters
fortheparabolicfunctiontoalignwiththesevalues
during training. Simplifying for clarity, let us look
at a one-dimensional scenario. We can express
the trainable parameters of the kernel function in
relation to the affine transformation of ğ‘andğ‘˜as
such
ğ‘â€²=ğ›¾ğ‘„Â·ğ‘+ğ›½ğ‘„, ğ‘˜â€²=ğ›¾ğ¾Â·ğ‘˜+ğ›½ğ¾;
sim(ğ‘â€²,ğ‘˜â€²)=ğœ™ğ‘‡(ğ‘â€²)ğœ™(ğ‘˜â€²).Here,ğœ™(Â·)representsaquadraticfunction. The
modelcanlearnanyquadraticfunctionwithadeter-
mined minimum value by adjusting its parameters.
We can, therefore, simplify the kernel function to
ğœ™(ğ‘¥)=ğ‘¥2.
Incorporatingtheaffinetransformationintothe
kernel function, we obtain
ğœ™(ğ‘â€²)=(ğ›¾ğ‘„Â·ğ‘+ğ›½ğ‘„)2=ğ›¾2
ğ‘„ğ‘2+2ğ›¾ğ‘„ğ›½ğ‘„ğ‘+ğ›½2
ğ‘„,
ğœ™(ğ‘˜â€²)=(ğ›¾ğ¾Â·ğ‘˜+ğ›½ğ¾)2=ğ›¾2
ğ¾ğ‘˜2+2ğ›¾ğ¾ğ›½ğ¾ğ‘˜+ğ›½2
ğ¾.
whereğ‘andğ‘˜have their unique parameters ğ›¾ğ‘„,
ğ›¾ğ¾,ğ›½ğ‘„, andğ›½ğ¾, enabling the model to learn any
quadratic function that is non-negative and has a
single real root.
Interestingly, our transformation resembles the
applicationofLayerNormalization(Baetal.,2016),
minus the normalization itself. We hypothesize
whether normalizing ğ‘andğ‘˜before the kernel
function could improve the modelâ€™s performance.
Our suspicion is confirmed when normalization
enhancesresults,asdemonstratedinalaterAblation
study. Consequently, our refined ReBased model
incorporates Layer Normalization.
Inthefollowingsections,weprovideanin-depth
analysisandconductcomprehensiveexperiments
to validate the effectiveness of these modifications.
5 Experiments
5.1 Experimental Setup
We applied the first evaluation of our ReBased
model on the MQAR task , for which we trained
a model to perform associative recall with varying
numbers of retrieved tokens. Arora et al. (2023)
suggested that for a comprehensive assessment,
models need to be tested across different sequence
lengths, model sizes, and number of query-key
pairstoberetrieved. However,thoseexperiments
were limited, only exploring sequence lengths up
to512. These constraints resulted in the Based
model displaying performance comparable to the
traditional attention mechanism.
Longersequencelengthscanbeexploredtogain
a deeper understanding of how improvements in
associative recall are affected by changes in model
configurations. Thisiswhyweextendedourtrain-
ingtoincludemodelscapableofhandlingsequence
lengths of[128,256,512,1024,2048]. We tested
a range of hidden sizes from 64to512. For our

--- PAGE 5 ---
16 24 32 48 64
Model Dimension0.00.20.40.60.81.0Accuracy
Sequence Length 256
16 24 32 48 64
Model Dimension
Sequence Length 2048
ReBased (norm(x)+)2
(x+)2
(x)2
norm(x)2x2Vanilla BasedFigure 3: Ablation study for the proposed modifications. For sequence length 256, the difference is not very
significant. Nevertheless,theReBasedmodelperformsbestonallmodeldimensions. Withasequencelengthof
2048, the difference becomes more evident. Unlike Based, the ReBased model retains performance across long and
shortsequences. SeeSection5.3fortheexperimentsetupandextendeddescriptionofourresultsandSection5.1for
experimental setup description.
ablation study to yield more precise insights, we
also employed smaller models with hidden sizes as
modest as[16,24,32,48].
In order to tailor our approach to varied se-
quences, we used different query-key (qk) pairs
for each length. The specifics of these configura-
tions are detailed in Appendix A.
We alsoput other sub-quadraticarchitectures to
thetest,includingMamba(SSMfamily)(Guand
Dao,2023),Hyena(thelongconvolutionsfamily)
(Poli et al., 2023), the vanilla attention method,
and RWKV (Peng et al., 2023). By comparing a
diverse range of models, our goal was to present a
well-roundedevaluationofhowourReBasedmodel
standsoutinthefield. ForBased,weutilizedTriton
kernelspublishedbyYangandZhang(2024),and
for ReBased, we modified it so that ğœ™(ğ‘¥)=ğ‘¥2.
Weusedahybridarchitecturewithshortconvolu-
tionandkernelsize3inthefirstlayer,andspecified
amixerinthesecond. Wefoundthatthissetupwas
more stable on longer sequence lengths, especially
when using an attention mixer. However, we did
not modify the Mamba model since convolutions
were already present inside the Mamba block. We
put the full results and model architecture details
in Appendix A.
Inlanguage modeling , our second experimen-
talsetupleveragedtheextensivePiledataset(Gao
et al., 2020) to train a language model (LM). Weopted for a sequence length of 4096, a slight in-
crease from the standard value while still ensuring
the replication of the architectural framework as
presentedbyAroraetal.(2024) 1. Notethatsome
hyperparameters such as model dimension and the
number of layers were set in order to match the
number of model parameters in the initial experi-
ment. Detailedmodelconfigurationcanbefound
in Appendix C.
The MQAR task provided insights into In-
Context Learning proficiencies across various ar-
chitectures, while the language modeling assess-
mentallowedustoappraiseshort-termdependency
modeling capacities. Beyond traditional perplexity
metrics on validation data, we also scrutinized the
Associative(AR)andNon-Associative(Non-AR)
variants of perplexity. Here, AR corresponds to to-
kenpositionsnecessitatingassociativerecall,while
Non-AR refers to other tokens. When tokens recur
within a text,the subsequent appearances are cate-
gorized as AR, highlighting the modelâ€™s capability
to recall from context.
5.2 MQAR experiment
In Figure 1, we present the capability of various
modelstohandletheMQARtask asthesequence
lengthincreases. Onekeyobservationisthat,ata
1The experiment details can be found in a blog post and
WandB report associated with the main paper.

--- PAGE 6 ---
Sequence Length 256 Sequence Length 2048
Architecture 16 24 32 48 Mean 16 24 32 48 Mean
Based 0.06Â±0.02 0.45Â±0.15 0.82Â±0.06 0.99Â±0.000.58 0.02Â±0.02 0.40Â±0.18 0.66Â±0.08 0.99Â±0.010.51
ğ‘¥20.05Â±0.05 0.47Â±0.08 0.64Â±0.42 0.99Â±0.000.54 0.03Â±0.03 0.33Â±0.21 0.46Â±0.42 0.93Â±0.090.44
ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘¥)20.09Â±0.04 0.39Â±0.24 0.84Â±0.09 0.97Â±0.020.57 0.05Â±0.05 0.56Â±0.10 0.72Â±0.17 0.99Â±0.000.58
(ğ›¾Â·ğ‘¥)20.02Â±0.02 0.35Â±0.22 0.71Â±0.09 0.98Â±0.030.51 0.02Â±0.03 0.26Â±0.45 0.65Â±0.37 0.99Â±0.010.48
(ğ›¾Â·ğ‘¥+ğ›½)20.06Â±0.01 0.51Â±0.08 0.89Â±0.03 0.99Â±0.000.61 0.06Â±0.03 0.50Â±0.08 0.85Â±0.04 0.99Â±0.010.60
ReBased
(ğ›¾Â·ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘¥)+ğ›½)20.09Â±0.05 0.59Â±0.06 0.86Â±0.08 0.99Â±0.000.63 0.04Â±0.03 0.58Â±0.01 0.83Â±0.04 0.99Â±0.000.61
Table 1: Ablation study for proposed modifications with standard deviation across 5seeds. See Figure 3 for a visual
presentation of the results, Section 5.3 for experiment setup and extended result description and Section 5.1 for the
description of our experimental setup.
sequence length of 2048, all models, except for the
Attention model, struggled to perform effectively
when limited to a model dimension of 64. As we
expandedthemodeldimensions,theperformanceof
theReBasedmodelmatchedorsurpassedtheBased
model. TheRWKVandMambaarchitecturesfailed
on the MQAR task across all tested model sizes.
This experiment highlights the significance of
utilizing more sophisticated setups, as the perfor-
mance discrepancy between the Attention model
andtheothermodels(BasedandReBased)becomes
pronouncedonlywhenthesequencelengthexceeds
512. These results suggest that the efficacy of
attention alternatives like ReBased becomes partic-
ularlyimportantwhenprocessinglongsequences.
Therefore, more consideration should be devoted
to configurations involving lengthy sequences to
leverage the full potential of such models.
5.3 Ablation Study
We comprehensively examined the individual el-
ements of our ReBased model to understand how
eachofthemcontributestoitsoveralleffectiveness,
andensurethetransparencyofourfindings. Ourex-
perimentsweremeticulouslydesignedtoevaluate
the model by assessing the influence of its separate
components on performance. The experimental
configurations were as follows:
â€¢ğ‘¥2â€“substitutingtheoriginalkernelfunction
withasimpleelement-wisesquaringoperation,
ğœ™(ğ‘¥)=ğ‘¥2.
â€¢ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘¥)2â€“ integrating a normalization step
without an affine transformation before apply-
ing the squaring operation.
â€¢(ğ›¾Â·ğ‘¥)2â€“introducinganaffinetransformation
solely in terms of scaling (without bias) for
the queries and keys.â€¢(ğ›¾Â·ğ‘¥+ğ›½)2â€“ incorporating affine transforma-
tion with both scaling and bias for the queries
and keys.
â€¢ReBased(ğ›¾Â·ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘¥)+ğ›½)2â€“ our compre-
hensivemodel,whichinvolvesnormalization
andaffinetransformation,includingbias,for
queries and keys.
Note that for ğ‘andğ‘˜, there are different scaling
parametersğ›¾ğ‘„,ğ›½ğ‘„,ğ›¾ğ¾,andğ›½ğ¾foreachexperiment
involving affine transformation.
Our goal is to highlight the effect of sequence
length variability in the MQAR task on model per-
formance. For this evaluation, we standardized
thenumberofretrievalpairsto32. Theoretically,
noimpactonperformanceshouldbeobserved,as
the amount of information required to be stored in
thehiddenstatesissequence-lengthagnostic. We
investigatedtheeffectsonsequencesoflengths256
and 2048 and illustrated our findings in Figure 3
(also available in Table 1 with a standard deviation
of accuracy across 5seeds). We must emphasize
the significance of long context setups evaluated
inourexperiments. Itscharacteristicsarevital,as
successful performance on long sequences high-
lightsthecapabilityofthemodeltomakefulluseof
itsarchitectural innovations. Italso translatesinto
notablepracticaladvantagesinreal-worldapplica-
tionswherehandlingextensivecontextefficiently
can be crucial.
The proposed ReBased model performs better
than every other modification. Performance on
theshort 256lengthislessnoticeablethanonthe
long 2048sequence length. We see a performance
dropfromsimplyreplacingtheoriginalkernelfunc-
tion withğ‘¥2. We presume that this is caused by
suboptimalscaleoffeatures,sincebyplacingnor-
malizationbeforethekernelfunction,wecannotice
aperformanceincreaseevenincomparisontothe
Basedmodel. Affinetransformations (ğ›¾Â·ğ‘¥)2and

--- PAGE 7 ---
(ğ›¾Â·ğ‘¥+ğ›½)2also show favorable performance com-
pared to theğ‘¥2model, which does not significantly
decrease with sequence length.
5.4 Language Modeling
Perplexity
Architecture All AR Non-AR
Attention 11.98 3.07 33.95
Based12.99 3.27 37.02
ReBased 12.90 3.25 36.73
Table 2: Perplexity results on Pile (Gao et al., 2020)
dataset. ReBased improves the result on AR tokens.
However,thereisstillasmallgapbetweenAttentionand
ReBased. See Section 5.4 for more details andSection
5.1 for experimental setup description.
Weconductedexperimentswithlanguagemod-
elingfollowingthesetupdescribedinSection5.1.
See Table 2 for the results.
We note that ReBased model performs better
thanBasedonbothARandnon-ARtokensleading
to lower overall perplexity. This can be considered
asasignofbetterIn-ContextLearningperformance.
Inthenextsectionweconsiderfew-shotsetup on
several tasks to validate
When considering AR perplexity, we observe
that there is still a gap between the vanilla Trans-
formerarchitectureandalternativemodels,which
is aligned with the results on the MQAR dataset.
However, we note that ReBased still performed
betterthanBased. RegardingNon-ARperplexity,
ReBased outperformed both Based architectures,
leadingtobetteroverallperplexityvalue. Notethat
attention has slightly more trainable parameters,
see Appendix C for more details.
Theseresultssuggestthat,despitelanguagemod-
eling perplexity being lower for an alternative to
Transformer architectures (Arora et al., 2024; Gu
and Dao, 2023), this may be achieved due to better
short-termdependencymodeling,whichdoesnot
requirelearningassociativeoperationsnecessaryto
perform In-Context Learning (Olsson et al., 2022).
ThevanillaTransformerstillperformsbestinterms
of its ability to attend to some token in-context.
5.5 Few-Shot Performance
To further explore the ability of the model to im-
prove results on real-world scenarios we validate
trainedBasedandReBasedmodelsonacommonfew-shot benchmarks from the LM Evaluation Har-
ness(Gaoetal.,2023)benchmarkandSuperGLUE
(Wang et al., 2019). Results are presented in Table
3 and Table 4. ReBased outperforms Based on the
most of the tasks.
5.6 Analysis
In this section, we delve into the internal dynam-
ics of the ReBased model by examining attention
matrices, which are commonly used to elucidate
thedecision-makingofmodelsandtheflowofin-
formation between tokens. Notably, we can use
the parallel mode with both Based and ReBased
models to construct these matrices.
For our analysis, we employ the MQAR dataset
(Arora et al., 2023) and train a compact model
configured with a sequence length of 128 and 32
retrieval pairs. To ensure clear interpretation of the
attention maps, we used fixed weights in the first
layer,whichconsistsofashortconvolutionwitha
kernelthatattendstotheprevioustoken. Following
the training phase, we compute the Intersection
over the Union (IoU)metric between the attention
matrixandtheactualpositionsofthetokensthatare
toberetrieved. Thecorrectpositionsarecrucial,as
theyrepresentthelocationsfromwhichthemodel
must copy the hidden states in order to success-
fullyresolvethetask. Thiscopyingmechanismis
particularly vital and is implemented via focused
attentioninthesecondlayerofthenetwork(Olsson
et al., 2022). Consequently, the IoU provides a
quantitative measure of how well our model has
learned to replicate this crucial pattern of token
retrieval. A visualization of this phenomenon us-
ingIoUonarandomlyselectedexamplefromthe
dataset is shown in Figure 4. Note that we cropped
attention matrix to examine only a region where
qk-pairs stored.
Our results are presented in Table 5. In our
experiment, the Attention model yielded a supe-
rior IoU score compared to both the Based and
ReBased models. However, the ReBased model
showspromiseinnarrowingtheperformancedivide
that exists between sub-quadratic methods and the
attention-basedmodel. Thissuggests that,despite
the relative simplicity of the method, it could serve
as an informative metric for the MQAR dataset,
particularlywhentheaccuracyscoreisclosetoone,
makingitchallengingtodiscerntheperformance
differencesbetweenmodelsinmoreintricatetesting
scenarios.

--- PAGE 8 ---
Architecture Winogrande Piqa Hellaswag Arc-E Arc-C Macro
Based 50.4 62.1 30.8 40.422.941.3
ReBased 54.6 62.8 30.7 41.021.542.1
Table3: 1-shotperformanceontasksfromLMevaluationharnessbenchmark(Gaoetal.,2023). SeeSection5.5for
more details.
Architecture WSC WIC RTE Record (F1/EM) MultiRC Copa BoolQ
Based55.8 46.5 47.6 62.7/62.1 51.5 66.0 48.3
ReBased 56.7 46.9 53.1 62.8/62.2 51.9 67.0 52.0
Table 4: 1-shot performance on SuperGLUE benchmark (Wang et al., 2019). See Section 5.5 for more details.
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Attention
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Based
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124ReBased
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Ground Truth
Figure 4: Attention matrix for the different models, and ground truth positions for the query. We measure IoU
between modelâ€™s attention and ground truth matrix for 10000examples. Illustration of the experiment is described
in Section 5.6 Results are presented in Table 5.
Architecture IoUAccuracy
Attention 0.999 1
Based 0.1570.956
ReBased 0.173 0.957
Table 5: IoU with attention matrix and ground truth po-
sitiontoretrieveontheMQARtaskfor 10000examples.
Detailed experiment setup can be found in 5.6.
6 Conclusion and Future Work
Inthis paper, wepresent ReBased, anovelarchitec-
ture for sub-quadratic attention computation. For
ourmodel,weanalyzedtheBasearchitectureand
proposedtodevelopitevenfurtherbyusingpolyno-
mial kernels with learnable parameters and adding
normalization before the kernel evaluation. While
incorporating layer normalization into model train-
ing was attempted previously (Henry et al., 2020),
our method integrates this normalization directly
into the kernel function. With this simple architec-
turalchange,weachievedresultsthatoutperformed
Based on MQAR and language modeling with the
Pile dataset tasks.
WeanalyzedtheinternalrepresentationsofRe-
Based, Based, and vanilla attentionmodules, and
concluded that ReBased resembles attention morethan Based. Notably, while Based uses a Taylor
expansion of an exponential function, a ReBased
kernelfunctionisdifferentfromtheexponentbut
shows better performance. Our research suggests
thatusingasecond-orderpolynomialmightbein-
sufficient for the best performance, and indicates
that more sophisticated learnable kernels could be
utilizedtoimprovetheperformanceoftrainedmod-
els. Normalizationcouldfurtherimprovevarious
kernel functions. This highlights a need for re-
searcherstorevisitkernel-basedmethodswiththe
goal of enhancing their adaptability and efficiency.
Our findings reveal a disparity in handling the
MQAR task between attention-based models and
others such as Based, specifically as sequence
lengths increase. Attention models excel on longer
sequences, significantly outperforming their non-
attentioncounterparts. Theseresultshighlightthe
necessity of further research into strategies that
could bridge this gap in order to reach the perfor-
mance of attention-based methods. Perhaps the
superior aspects of attention mechanisms could be
matchedorexceededbyothermodels,especiallyon
tasksthatrequireassociativerecall,suchasmachine
translation (Vardasbi et al., 2023). Future research
could give insight into this, leading to improved
models for processing long sequences.

--- PAGE 9 ---
7 Limitations
While our proposed method demonstrates applica-
bilitytoawiderangeoftaskstypicallyaddressed
by Transformers, its effectiveness in handling tasks
involving intensive copying or recalling previous
context remains unclear (see Table 2 and Jelassi
etal.(2024)). Successfullyaddressingthesetasks
is crucial for fully mitigating inference problems
associated with attention mechanisms.
Itisalsoworthnotingthatourexperimentsare
limitedtoacademic-scalemodels. Thisdoespose
certain limitations, particularly in extrapolating
the findings to larger models. However, given
the resource constraints, our results still provide
valuableinsightsintothepotentialefficacyofour
method.
References
Ido Amos, Jonathan Berant, and Ankit Gupta. 2023.
Never train from scratch: Fair comparison of long-
sequence models requires data-driven priors. CoRR,
abs/2310.02980.
SimranArora,SabriEyuboglu,AmanTimalsina,Isys
Johnson, Michael Poli, James Zou, Atri Rudra,
and Christopher RÃ©. 2023. Zoology: Measuring
and improving recall in efficient language models.
arXiv:2312.04927 .
SimranArora,SabriEyuboglu,MichaelZhang,Aman
Timalsina,SilasAlberti,DylanZinsley,JamesZou,
Atri Rudra, and Christopher RÃ©. 2024. Simple lin-
ear attention language models balance the recall-
throughput tradeoff. arXiv:2402.18668 .
JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hinton.
2016. Layer normalization. Cite arxiv:1607.06450.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence mod-
eling. Cite arxiv:1412.3555Comment: Presented
in NIPS 2014 Deep Learning and Representation
Learning Workshop.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedingsofthe2019Conferenceof
the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongandShortPapers) ,pages
4171â€“4186,Minneapolis,Minnesota.Associationfor
Computational Linguistics.
Jesse Dodge, Suchin Gururangan, Dallas Card, Roy
Schwartz, and Noah A. Smith. 2021. Expected val-
idation performance and estimation of a random
variableâ€™s maximum. CoRR, abs/2110.00613.Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W.
Thomas, Atri Rudra, and Christopher RÃ©. 2023a.
HungryHungryHippos: Towardslanguagemodeling
with state space models. In International Conference
on Learning Representations .
Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W.
Thomas, Michael Zhang, Tri Dao, Atri Rudra, and
Christopher RÃ©. 2023b. Simple hardware-efficient
long convolutions for sequence modeling. Interna-
tional Conference on Machine Learning .
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,
TravisHoppe,CharlesFoster,JasonPhang,Horace
He,AnishThite,NoaNabeshima,ShawnPresser,and
Connor Leahy.2020. ThePile: An 800gb dataset of
diverse text for language modeling. arXiv preprint
arXiv:2101.00027 .
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,
Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noacâ€™h, Haonan Li,
KyleMcDonell,NiklasMuennighoff,ChrisOciepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023.
Aframeworkforfew-shotlanguagemodelevaluation.
Albert Gu and Tri Dao. 2023. Mamba: Linear-time
sequence modeling with selective state spaces.
Albert Gu, Karan Goel, and Christopher Re. 2022. Effi-
cientlymodelinglongsequenceswithstructuredstate
spaces. In International Conference on Learning
Representations .
AlbertGu,IsysJohnson,AmanTimalsina,AtriRudra,
and Christopher Re. 2023. How to train your HIPPO:
Statespacemodelswithgeneralizedorthogonalbasis
projections. In InternationalConferenceonLearning
Representations .
Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and
Yuxuan Chen. 2020. Query-key normalization for
transformers. FINDINGS .
Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long
short-termmemory. NeuralComputation ,9(8):1735â€“
1780.
Samy Jelassi, David Brandfonbrener, Sham M. Kakade,
and Eran Malach. 2024. Repeat after me: Trans-
formers are better than state space models at copying.
arXiv preprint arXiv: 2402.01032 .
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
delasCasas,FlorianBressand,GiannaLengyel,Guil-
laumeLample,LucileSaulnier,LÃ©lioRenardLavaud,
Marie-AnneLachaux,PierreStock,TevenLeScao,
Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix,
and William El Sayed. 2023. Mistral 7b.
AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,
and FranÃ§ois Fleuret. 2020. Transformers are RNNs:
Fastautoregressivetransformerswithlinearattention.

--- PAGE 10 ---
InProceedings of the 37th International Conference
on Machine Learning , volume 119 of Proceedings
of Machine Learning Research , pages 5156â€“5165.
PMLR.
CatherineOlsson,NelsonElhage,NeelNanda,Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
AmandaAskell,YuntaoBai,AnnaChen,TomCon-
erly,DawnDrain,DeepGanguli,ZacHatfield-Dodds,
Danny Hernandez, Scott Johnston, Andy Jones, Jack-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
SamMcCandlish,andChrisOlah.2022. In-context
learningandinductionheads. TransformerCircuits
Thread. Https://transformer-circuits.pub/2022/in-
context-learning-and-induction-heads/index.html.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,
SamuelArcadinho,StellaBiderman,HuanqiCao,Xin
Cheng, Michael Chung, LeonDerczynski, Xingjian
Du,MatteoGrella,KranthiGv,XuzhengHe,Haowen
Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming
Kong, BartÅ‚omiej Koptyra, Hayden Lau, Jiaju Lin,
Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi
Saito, Guangyu Song, Xiangru Tang, Johan Wind,
StanisÅ‚awWoÅºniak,ZhenyuanZhang,QinghuaZhou,
JianZhu,andRui-JieZhu.2023. RWKV:Reinventing
RNNs for the transformer era. In Findings of the
Association for Computational Linguistics: EMNLP
2023, pages 14048â€“14077, Singapore. Association
for Computational Linguistics.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz,NoahA.Smith,andLingpengKong.2021.
Random feature attention. ArXiv, abs/2103.02143.
Michael Poli, Stefano Massaroli, Eric Q. Nguyen,
Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano
Ermon, and Christopher RÃ©. 2023. Hyena hierar-
chy: Towardslargerconvolutionallanguagemodels.
International Conference on Machine Learning .
ZhenQin,WeixuanSun,HuicaiDeng,DongxuLi,Yun-
shenWei,BaohongLv,JunjieYan,LingpengKong,
and Yiran Zhong. 2022. cosformer: Rethinking soft-
max in attention. ArXiv, abs/2202.08791.
AlecRadford,JeffWu,RewonChild,DavidLuan,Dario
Amodei, and Ilya Sutskever. 2019. Language models
are unsupervised multitask learners.
Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber.
2021. Lineartransformersaresecretlyfastweightpro-
grammers. In International Conference on Machine
Learning.
Jimmy T.H. Smith, Andrew Warrington, and Scott Lin-
derman. 2023. Simplified state space layers for se-
quence modeling. In The Eleventh International
Conference on Learning Representations .
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hancedtransformerwithrotarypositionembedding.
Neurocomputing , 568:127063.Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. 2021. Long
range arena : A benchmark for efficient transformers.
InInternational Conference on Learning Representa-
tions.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, D. Bikel, Lukas Blecher, Cristian CantÃ³n
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal,
A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelM.Kloumann,A.Korenev,PunitSinghKoura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor
Molybog,YixinNie,AndrewPoulton,JeremyReizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, R. Subramanian,
Xia Tan, Binh Tang, Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kam-
badur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic,SergeyEdunov, andThomasScialom. 2023.
Llama2: Openfoundationandfine-tunedchatmodels.
ArXiv, abs/2307.09288.
Ali Vardasbi, Telmo Pessoa Pires, Robin M. Schmidt,
and Stephan Peitz. 2023. State spaces arenâ€™t enough:
Machine translation needs attention. In Proceed-
ings of the 24thAnnual Conference of the European
Association for Machine Translation, EAMT 2023,
Tampere,Finland,12-15June2023 ,pages205â€“216.
European Association for Machine Translation.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisallyou
need. InAdvances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preetSingh,JulianMichael,FelixHill,OmerLevy,
andSamuelR.Bowman.2019. Superglue: Astickier
benchmark for general-purpose language understand-
ing systems. In Advances in Neural Information
ProcessingSystems32: AnnualConferenceonNeu-
ral Information Processing Systems 2019, NeurIPS
2019,December8-14,2019,Vancouver,BC,Canada ,
pages 3261â€“3275.
Songlin Yang and Yu Zhang. 2024. Fla: A triton-
based library for hardware-efficient implementations
of linear attention mechanism.

--- PAGE 11 ---
(a) Based architecture
 (b) ReBased architecture.
Figure 5: Architectures visualization.
Model Dimension Attention ConvAttention RWKV ConvRWKV Mamba Based (Rebased)
64 623744 578752 623872 677120 655360 577984 (+768)
128 1313024 1149312 1313280 1395200 1413120 1179520 (+768)
256 2888192 2462464 2888704 2561024 3235840 2459392 (+768)
512 6824960 5580288 6825984 5777408 7847936 5307904 (+768)
Table 6: Number of model parameters in MQAR dataset. See Appendix A.
A Details for the MQAR dataset
experiments
Inourexperiments,weusethecodefromtheofficial
MQARrepository(Aroraetal.,2024) 2. However,
wemodifytheattentionmodelfromtheonereported
in Arora et al. (2024), as we found it more stable
(see Figure 6). We can see that replacing the
first attention layer is beneficial for performance.
RWKVperformsbetterwhenwedonotreplacethe
first layer, which is why we use we use two RWKV
layersinourmainexperiment(seeFigure1). We
report the number of trainable parameters in Table
6.
64 128 256 5120.00.20.40.60.81.0Accuracy
Sequence Length: 128
64 128 256 512
Sequence Length: 512
Model Dim
RWKV ConvRWKV Attention onvAttention
Figure 6: Performance of the hybrid architecture with
convolutionsonthefirstlayerandthevanillaarchitecture.
We also modify the data configs to be more
2https://github.com/HazyResearch/zoologychallengingforthemodel. Youcanseetheadjusted
parameters in Table 7.
seq_length qk_pairs
128 16
256 64
512 128
1024 256
2048 512
Table7: SequencelengthsandnumberofQKpairsin
dataset.
We use a batch size of 512 for all experiments.
IncaseswherethereisnotenoughGPUmemory,
we use the gradient accumulation technique. For
the learning rate, we use hyperparameter search
with the following grid: 5e-4, 1e-3, 3e-3, 1e-2. We
use five different seeds for all reported results.
B Stability
In our experiments, we found ReBased to be more
stable during training with various hyperparame-
ters. To demonstrate this, we utilize an Expected
Validation Performance (EVP) plot (Dodge et al.,
2021). We treat the average across five seeds as
the final accuracy. Our results are presented in
AppendixFigure7. Wenoticedthatevenincases
wherethemodeldimensionissufficientlylargeto

--- PAGE 12 ---
storeallnecessaryinformation,ourmodifications
leadto 100%accuracyforeveryhyperparameterset
andeveryseedweuse,incontrastwiththeBased
model, where we observe degradation for certain
learning rates.
C Pile Dataset Experiment Details
Model# Parameters
Attention 151 880 448
Based147 542 016
ReBased 147 548 928
Table 8: Parameters count for the pile experiment. See
Appendix C.
WetrainourmodelonthetokenizedPiledataset
published on huggingface hub 3. Note that this
tokenization differs from the one used in Based 4.
We also use our pipeline, which we plan to release
to the public soon. We do not use rotary positional
embeddings (Su et al., 2024) or other tricks, as
we copy our models from the Based repository.
Hyperparameters can be found in Table 9.
Hyper-Parameter Value
warmup steps 200
max grad norm 1
num steps 20000
seq len 4096
lr 1e-3
weight decay 0.1
num heads 12
d model 768
effective batch size 1024
Table 9: Hyperparameters used for training.
As in Arora et al. (2023), we use more hyper-
parameters in the Based/ReBased models then in
the Attention baseline. The number of layers and
head dim reported in Tables 10 and 11. We use a
hybrid architecture for the Based/ReBased models
whereweuseshortconvolutionasamixerforevery
odd-numbered layer.
D Additional Analysis
In this section, we provide additional results and
experimentstofurtherunderstandhowtheReBased
3https://huggingface.co/datasets/EleutherAI
4See reportHyperparameters Value
layers 12
head dim 64
Table 10: Attention hyperparameters.
Hyper-Parameter Value
layers 18
head dim 16
Table 11: Based/ReBased hyperparameters.
model learns dependencies. First, we offer more
examples for our experiments with attention matri-
ces,asdetailedinSection5.6. Attentionmatrices
forrandomexamplesfromthetestsetarepresented
in Figure 8. Generally, we can observe that at-
tention"fires"moreintenselyatretrievingtokens
comparedtoBased/ReBased. Thisresultsuggests
that there may still be a flaw in our kernel function
that distributes attention to irrelevant tokens. We
furtherinvestigatethisphenomenonbyanalyzing
the distribution of attention on the last token, as
shown in Figure 9. The number of noisy tokens for
theReBasedmodelissmallercomparedtoBased,
but Attention exhibits superior results.
Layer normalization is the main difference be-
tweentheBasedandReBasedmodels. Therefore,
itisimportanttoanalyzetheparametersobtained
during the training process. We logged the mean
andstandarddeviationoftheparametersacrossdif-
ferentsequencelengths. Ourresultscanbefound
inFigure10. Notably,thefinalparametervalueis
independent of the training sequence length, which
canindicatethatwemaynotneedtotrainthemodel
for all possible lengths. Both ğ›¾andğ›½parameters
have high standard deviation values compared to
the mean absolute value. Consequently, we can
assume that it is important to provide features with
different scales.

--- PAGE 13 ---
1 2 3 4
Hyperparameter assignments0.00.20.40.60.81.0Expected Accuracy
Model Dimension 16
1 2 3 4
Hyperparameter assignments
Model Dimension 24
1 2 3 4
Hyperparameter assignments
Model Dimension 32
1 2 3 4
Hyperparameter assignments
Model Dimension 48
1 2 3 4
Hyperparameter assignments
Model Dimension 64Seqeunce Length 256
ReBased Vanilla BasedFigure 7: Expected validation accuracy across different hyperparameters. The ReBased model works best across all
hyperparameters, budgets, and model dimensions. See Section 5.3 for more details.
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Attention
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Based
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124ReBased
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Ground Truth
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Attention
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Based
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124ReBased
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Ground Truth
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Attention
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Based
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124ReBased
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Ground Truth
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Attention
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Based
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124ReBased
0481216202428323640444852566064
68
72
76
80
84
88
92
96
100
104
108
112
116
120
124Ground Truth
Figure 8: Attention matrix for the different models, and ground truth positions for the query. We measure IoU
betweenthemodelâ€™sattentionandgroundtruthmatrixfor 10000examples. llustrationoftheexperimentisdescribed
in Section 5.6 Results are presented in Table 5.

--- PAGE 14 ---
0 10 20 30 40 50 600.00.20.40.60.81.0Attention
Based
ReBased
Ground Truth
0 10 20 30 40 50 600.00.20.40.60.81.0Attention
Based
ReBased
Ground Truth
0 10 20 30 40 50 600.00.20.40.60.81.0Attention
Based
ReBased
Ground Truth
0 10 20 30 40 50 600.00.20.40.60.81.0Attention
Based
ReBased
Ground TruthFigure 9: Attention scores for a random example. Based and Rebased scores are noisy, while attention has one peak
at the ground truth position. See Appendix A.
Steps0.00.20.40.60.81.0Gamma Q
64
128
256
StepsGamma K
64
128
256
Steps0.3
0.2
0.1
0.00.10.20.3Beta Q
64
128
256
StepsBeta K
64
128
256
Figure 10: Analysis of the layer normalization parameters. Mean value of the scale parameter (gamma) tends to the
same value of about 0.13, and the bias parameter (beta) tends to 0. See Section D.
