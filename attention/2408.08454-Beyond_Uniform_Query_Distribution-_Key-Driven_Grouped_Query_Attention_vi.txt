# 2408.08454.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2408.08454.pdf
# Kích thước tệp: 1063809 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Vượt Ra Ngoài Phân Phối Truy Vấn Đồng Nhất: Cơ Chế Attention Truy Vấn Nhóm Theo Khóa
Zohaib Khan*, Muhammad Khaquan*, Omer Tafveez, Burhanuddin Samiwala, Agha Ali Raza
Đại học Khoa học Quản lý Lahore
{24100074, 25100095, 25020254, 25100255, agha.ali.raza }@lums.edu.pk
Tóm tắt
Kiến trúc Transformer đã cách mạng hóa học sâu thông qua cơ chế Self-Attention, có khả năng nắm bắt thông tin ngữ cảnh một cách hiệu quả. Tuy nhiên, dấu chân bộ nhớ của Self-Attention đặt ra những thách thức đáng kể cho các tác vụ chuỗi dài. Grouped Query Attention (GQA) giải quyết vấn đề này bằng cách nhóm các truy vấn và tính trung bình gộp các key-value head tương ứng—giảm số lượng tham số tổng thể và yêu cầu bộ nhớ một cách linh hoạt mà không ảnh hưởng xấu đến độ chính xác của mô hình. Trong công trình này, chúng tôi giới thiệu các cải tiến cho GQA, tập trung vào hai phương pháp mới lạ khác biệt với tính chất tĩnh của việc nhóm: Key-Distributed GQA (KDGQA) và Dynamic Key-Distributed GQA (DGQA), tận dụng thông tin từ chuẩn của các key head để thông báo phân bổ truy vấn. Cụ thể, KDGQA xem xét tỷ lệ của chuẩn các key head trong mỗi lần truyền tiến, trong khi DGQA kiểm tra tỷ lệ của các chuẩn khi chúng tiến hóa qua quá trình huấn luyện. Ngoài ra, chúng tôi trình bày Perturbed GQA (PGQA) như một nghiên cứu tình huống, giới thiệu tính biến đổi trong việc hình thành nhóm (tĩnh) thông qua việc trừ nhiễu từ bản đồ attention. Các thí nghiệm của chúng tôi với Vision Transformers được huấn luyện bổ sung, cho Phân loại Hình ảnh trên các bộ dữ liệu như CIFAR-10, CIFAR-100, Food101, và Tiny ImageNet, chứng minh tiềm năng của các biến thể này trong việc cải thiện GQA gốc thông qua các cơ chế nhóm có thông tin và thích ứng hơn: cụ thể ViT-L trải qua tăng độ chính xác lên đến 8% khi sử dụng DGQA so với GQA và các biến thể khác. Chúng tôi phân tích thêm tác động của số lượng Key-Value Head đến hiệu suất, nhấn mạnh tầm quan trọng của việc sử dụng ái lực query-key. Mã nguồn có sẵn trên GitHub.¹

1 Giới thiệu
Kể từ khi ra đời vào năm 2017, Transformers (Vaswani et al. 2023) đã trở thành phương pháp tiên tiến trong các tác vụ mô hình hóa ngôn ngữ (Brown et al. 2020; Touvron et al. 2023). Gần đây hơn, chúng đã được điều chỉnh cho các tác vụ thị giác máy tính như phân loại và phân đoạn hình ảnh, vượt qua sự phụ thuộc lớn vào tích chập (Dosovitskiy et al. 2021; Liu et al. 2021; Dong et al. 2022). Khả năng mở rộng của Transformers cũng đã được thiết lập thông qua thực nghiệm bởi các quy luật tỷ lệ (Hoffmann et al. 2022) gợi ý rằng lớn hơn thực sự là tốt hơn trong mô hình này. Sự mở rộng kích thước mô hình này thường xảy ra bằng cách tăng số lượng lớp, hoặc chiều của các phép chiếu.

*Những tác giả này đóng góp ngang nhau.
¹https://github.com/zohaib-khan5040/key-driven-gqa

Hình 1: Tổng quan về các cơ chế liên quan. MHA liên kết một truy vấn với một key-value head. GQA liên kết một key-value head đơn với các nhóm con của truy vấn sao cho kích thước nhóm giữ nguyên và việc nhóm được thực hiện theo cách tĩnh/đồng nhất. Các phương pháp của chúng tôi, KDGQA/DGQA, thực hiện việc nhóm theo chuẩn của các key head theo cách không đồng nhất - lưu ý cách các key tối hơn (biểu thị chuẩn cao hơn) có nhiều truy vấn được gán hơn so với những key sáng hơn.

Tuy nhiên, việc tăng kích thước mô hình đi kèm với chi phí tăng yêu cầu bộ nhớ và tính toán. Các mô hình lớn hơn cũng gây ra tắc nghẽn suy luận vì độ phức tạp tính toán của Self-Attention tăng theo bậc hai với số lượng token. Những hạn chế này, cùng với thực tế rằng hầu hết các mô hình transformer lớn đôi khi được tham số hóa quá mức (Hoffmann et al. 2022) đã tạo tiền đề cho các kỹ thuật nén mô hình khác nhau đặc biệt nhắm vào mô-đun Self-Attention.

Một trong những sơ đồ phổ biến đầu tiên là Multi-Query Attention (Shazeer 2019) gợi ý rằng trong mỗi khối self-attention, việc chỉ sử dụng một key head duy nhất giảm đáng kể các tham số mà không gây ra sự sụt giảm đáng kể về độ chính xác. Sau đó, Grouped-Query Attention (GQA) được giới thiệu (Ainslie et al. 2023) như một phép nội suy giữa Multi-Head Attention (MHA) và Multi-Query Attention (MQA). GQA đã được chứng minh đạt hiệu suất gần với MHA, trong khi gần như nhanh bằng MQA, chỉ sử dụng một arXiv:2408.08454v2 [cs.CV] 28 Aug 2024

--- TRANG 2 ---
key và value head duy nhất cho mỗi nhóm con của query head. Kể từ đó, đã có nhiều phương pháp sửa đổi kiến trúc GQA để giảm thiểu thêm sự sụt giảm độ chính xác (Javadi et al. 2023; Chinnakonduru and Mohapatra 2024; Chen et al. 2024).

Chúng tôi đề xuất các mở rộng riêng của mình cho kiến trúc GQA cố gắng làm mềm sự sụt giảm độ chính xác bằng cách làm cho cơ chế nhóm trở nên động và có thông tin hơn, được áp dụng cho phân loại hình ảnh với Vision Transformers (Dosovitskiy et al. 2021). Cụ thể, chúng tôi khám phá khả năng và tiềm năng của việc sử dụng một heuristic đơn giản như chuẩn L2 (như thước đo tầm quan trọng) của các key head để hướng dẫn quá trình phân bổ nhóm, trong quá trình huấn luyện, trong các phương pháp chúng tôi gọi là Key-Distributed GQA (KDGQA) và Dynamic Key-Distributed GQA (DGQA) - phương pháp trước giới hạn kiến thức về chuẩn trong lần truyền tiến hiện tại, trong khi phương pháp sau kiểm tra các tỷ lệ khi chúng tiến hóa qua quá trình huấn luyện (Hình 1 minh họa sự khác biệt trong việc nhóm). Chúng tôi cũng xem xét Perturbed GQA (PGQA) như một trường hợp thú vị của việc giới thiệu Nhiễu Gaussian vào bản đồ attention GQA.

2 Công trình liên quan
Tính chất toàn cục của Self-Attention, trong đó có số lượng truy vấn và key/value bằng nhau, đã làm cho việc huấn luyện và triển khai transformer ngày càng tốn kém về mặt tính toán. GQA đã giảm thiểu điều này ở một mức độ nào đó, cho phép hình thành các nhóm kích thước cố định của query head và liên kết một key-value head duy nhất với mỗi nhóm (Ainslie et al. 2023): bài báo thảo luận về việc sử dụng các checkpoint MHA hiện có, và chuyển đổi chúng sang kiến trúc GQA bằng cách tính trung bình gộp các key và value head liên kết với mỗi nhóm, tương ứng. Nhiều công trình gần đây đã nổi lên tìm cách điều chỉnh kết nối và động lực truyền tiến của cơ chế Attention, vắt kiệt hiệu suất từ mô hình, thường với cùng số lượng tham số.

Grouped Query Key Value Attention (Javadi et al. 2023) nổi lên như một phương pháp thống nhất bao gồm các biến thể như Multi-Key Value Attention (MKVA) và Grouped Key Value Attention (GKVA). Trong khi MKVA và GKVA giới thiệu các chiến lược mới lạ bằng cách nhóm key và value thành các nhóm riêng biệt và chia sẻ truy vấn trong các nhóm này, GQKVA phân vùng truy vấn và cặp key-value, tối ưu hóa cả số lượng tham số và hiệu quả tính toán. Nó khám phá phương pháp tổ hợp chia ma trận Q thành g nhóm và ma trận K, V thành gkv nhóm, cho phép số lượng linh hoạt của truy vấn, key và value trong mỗi phân vùng. Phương pháp này sử dụng dot-product attention cho mỗi kết hợp duy nhất của các nhóm này (Qi, KVj), đảm bảo đầu ra riêng biệt trên nhiều head.

Chen et al. (2024) đề xuất AsymGQA và điều tra việc phân bổ không đồng nhất các head bằng cách kết hợp các phép chiếu tương tự vào các cluster được xây dựng đồng nhất. Ví dụ, xem xét tình huống trong đó tập hợp key K được phân vùng thành ba nhóm, mỗi nhóm chứa hai key. Trong phương pháp này, độ tương tự của một key được chọn ngẫu nhiên được tính toán đối với tất cả key trong các nhóm khác. Key sau đó được chèn vào nhóm có độ tương tự cao nhất. Điều này cũng được thử nghiệm ở cấp độ kích hoạt, nơi đầu ra của mỗi lớp được sử dụng để tính toán độ tương tự để hình thành các cluster của head, hoạt động tốt hơn so với clustering cấp Trọng số.

Tương tự, Joshi et al. (2024) đề xuất Quality and Capacity-aware GQA (QCQA), sử dụng loss tối thiểu hóa được gọi là Weight-Sharing Error, là khoảng cách giữa các phân phối K và V head để tối thiểu hóa sự sụt giảm độ chính xác do nhóm truy vấn. Phương pháp này tuân theo khung tìm kiếm hai giai đoạn, đầu tiên hình thành các nhóm truy vấn Q của mỗi lớp một cách độc lập. Điều này được theo sau bởi việc đánh giá các nhóm của mỗi lớp sử dụng Weight Sharing Error và KV cache như các hàm fitness để loại bỏ các lớp có tác động lớn nhất đến độ chính xác. Các lớp thể hiện thiệt hại đáng kể đến độ chính xác được giữ lại dưới dạng MHA trong khi các lớp khác được nhóm, hoàn thiện việc lựa chọn các lớp sẽ bảo tồn các nhóm này với các lớp không được chọn được cấu hình thành MHA. Giai đoạn đầu tập trung vào tối ưu hóa nhóm cho từng lớp riêng lẻ, trong khi giai đoạn thứ hai sử dụng Non-dominated Sorting Genetic Algorithm II để tinh chỉnh lặp đi lặp lại các nhóm này.

Weighted GQA (Chinnakonduru and Mohapatra 2024) giới thiệu tham số scalar hoặc vector có thể học cho các K và V head cho (w1,k, w2,k..wh,k) và (w1,v, w2,v..wh,v). Nói cách khác, các key và value tổng hợp được hình thành nơi mỗi cột của K và V là tổ hợp tuyến tính của các ma trận key và value K1, V1 qua Kh, Vh nơi tổ hợp được kiểm soát bởi một tập hợp vector trọng số cụ thể. Tổng có trọng số của phép toán này được sử dụng để sửa đổi K, V, sau đó được sử dụng cho self-attention.

3 Phương pháp
Phần này mô tả các phương pháp của chúng tôi để thoát khỏi tính chất đồng nhất mặc định của phân bổ truy vấn trong Grouped-Query Attention.

3.1 Key-Distributed GQA (KDGQA)
Phương pháp này là nỗ lực đầu tiên của chúng tôi trong việc phân bổ query head cho mỗi nhóm theo cách động hơn. Trong KDGQA, thay vì gán đồng nhất truy vấn cho các nhóm, chúng tôi sử dụng độ lớn của các vector key, được đo bằng chuẩn L2 của chúng như một proxy cho tầm quan trọng của chúng, để thông báo phân phối truy vấn trên các nhóm khác nhau. Chúng tôi đặt giả thiết rằng việc phân bổ số lượng query head tỷ lệ với tầm quan trọng tương đối của chúng (trong lớp đó) là có lợi cho hiệu suất mô hình.

Cho một tập hợp các vector key K∈RG×dk, trong đó G là số lượng nhóm (hoặc key-value head) và dk là chiều của các vector key, chuẩn của mỗi vector key có thể được tính như sau:

ng=∥Kg∥2=√(∑[i=1 đến dk] K²g,i) cho g= 1, . . . , G (1)

Ở đây, ng là chuẩn của vector key thứ g Kg.

Để đảm bảo rằng các chuẩn được chia tỷ lệ trong phạm vi [0,1] để dễ diễn giải và phân bổ hơn, chúng tôi áp dụng chia tỷ lệ min-max cho vector chuẩn n= [n1, n2, . . . , nG]:

n̂g=(ng−min(n))/(max(n)−min(n)) (2)

--- TRANG 3 ---
Trong đó:
• n̂g là chuẩn được chia tỷ lệ cho nhóm thứ g.
• min(n) và max(n) đại diện cho giá trị tối thiểu và tối đa trong vector n.

Chúng tôi thấy rằng việc chia tỷ lệ min-max như được mô tả trong phương trình 2 giúp nén các chuẩn vào một phạm vi chặt chẽ hơn, khuếch đại sự khác biệt và dẫn đến kích thước nhóm không đồng nhất hơn, so với việc trực tiếp lấy tỷ lệ.

Cuối cùng, số lượng truy vấn, Qg, được phân bổ cho mỗi key head có thể được xác định bằng cách chuẩn hóa các chuẩn được chia tỷ lệ và nhân với tổng số truy vấn Nq:

Qg=⌊(n̂g·Nq)/(∑[g=1 đến G] n̂g)⌋ (3)

Trong đó:
• Nq là tổng số truy vấn.
• Qg đại diện cho số lượng truy vấn được phân bổ cho nhóm thứ g.

KDGQA thực hiện quy trình này để tìm số lượng query head cần phân bổ, sau đó hình thành các nhóm theo cách từ trái sang phải, trong mỗi lần truyền tiến, bất kể huấn luyện hay suy luận, gây ra một chi phí nhỏ mà chúng tôi thảo luận sau (Phần 5.2).

3.2 Dynamic Key-Distributed GQA (DGQA)
Dynamic Key-Distributed Grouped Query Attention (DGQA) mở rộng các nguyên tắc của KDGQA bằng cách không chỉ xem xét chuẩn của các vector key tại một thời điểm, mà bằng cách điều chỉnh động các thước đo tầm quan trọng này trong quá trình huấn luyện. Ý tưởng cốt lõi của DGQA là cho phép việc nhóm truy vấn thích ứng dựa trên các đặc tính đang phát triển của các vector key khi mô hình học. Sự thích ứng động này nhằm nắm bắt và tận dụng hiệu quả hơn cấu trúc cơ bản của dữ liệu, dẫn đến cải thiện hiệu suất trong các cơ chế attention.

Cơ chế cơ bản dựa vào việc định nghĩa kích thước cửa sổ: một số lần lặp cố định tại đầu mà chúng tôi cập nhật phân bổ truy vấn và duy trì trong suốt khoảng thời gian. Để định lượng một số thước đo về cách các key tiến hóa (dựa trên độ lớn của chúng), chúng tôi đề xuất hai phương pháp:

1. Tiến hóa dựa trên Sự khác biệt.
Chúng tôi lưu trữ chuẩn của các key head (như được tính trong phương trình 1) tại đầu mỗi cửa sổ/khoảng t, c(t)g. Khi quá trình huấn luyện tiến triển, chúng tôi định lượng tầm quan trọng của mỗi nhóm bằng cách đo sự khác biệt tuyệt đối giữa chuẩn được lưu trữ (đến từ khoảng trước) và chuẩn hiện tại:

Δn(t)g=n(t)g−c(t−1)g cho g= 1, . . . , G (4)

trong đó Δn(t)g đại diện cho sự thay đổi trong chuẩn cho key head thứ g và khoảng thứ t.

Sau đó chúng tôi sử dụng sự khác biệt này để hình thành thước đo tầm quan trọng, dg, cho mỗi key head:

dg=Δn(t)g (5)

Các giá trị được lưu trữ sau đó được cập nhật khi khoảng tiếp theo bắt đầu:

c(t)g=n(t)g cho g= 1, . . . , G (6)

2. Tiến hóa dựa trên Exponential Moving Average.
Phương pháp này rất tương tự với phương pháp trước về mặt thiết lập kích thước cửa sổ, sử dụng bộ nhớ đệm các chuẩn, và cập nhật bộ nhớ đệm. Sự khác biệt chính nằm ở thước đo tầm quan trọng, dg: thay vì lấy sự khác biệt tuyệt đối của các chuẩn, chúng tôi lấy exponential moving average (EMA) của hai:

c(t)g=α·n(t)g+(1−α)·c(t−1)g cho g= 1, . . . , G (7)

trong đó α là một siêu tham số xác định tầm quan trọng của giá trị hiện tại so với trung bình động trong việc xác định trung bình mới.

Giá trị được lưu trữ mới này sẽ được sử dụng để xác định phân bổ truy vấn (lưu ý chúng tôi không cần lấy giá trị tuyệt đối ở đây):

dg=c(t)g cho g= 1, . . . , G (8)

Cả hai phương pháp đều đơn giản tìm cách tính toán dg (g=1, . . . , G). Sử dụng điều này, số lượng truy vấn được phân bổ trong mỗi nhóm, Qg, có thể được tính theo cách tương tự như phương trình 3, thay thế vị trí của n̂g.

Các thí nghiệm của chúng tôi đã thiết lập sự vượt trội của biến thể EMA so với biến thể sự khác biệt: 84% so với 71% tương ứng, cho kích thước cửa sổ 300, trên CIFAR-100 trong 10 epoch, từ checkpoint MHA đã chuyển đổi. Điều này có thể được quy cho những điều sau (Hình 2):

• Thích ứng Mượt mà hơn với Độ lớn Thay đổi
Hiệu ứng trung bình của EMA giúp giảm thiểu ảnh hưởng của các điểm ngoại lai hoặc thay đổi đột ngột, dẫn đến phân bổ head mạnh mẽ và đáng tin cậy hơn so với phương pháp sự khác biệt, đặc biệt trong các tình huống nơi chuẩn của key tiến hóa nhanh chóng.

• Giảm Độ nhạy cảm với Nhiễu
Bằng cách kết hợp thông tin quá khứ vào cập nhật hiện tại, EMA giảm độ nhạy cảm với các mẫu nhiễu hoặc thoáng qua trong dữ liệu.

Đối với phần còn lại của bài báo, lưu ý rằng trừ khi được chỉ định khác, phương pháp DGQA đề cập đến biến thể EMA. Chúng tôi thực hiện tìm kiếm lưới để tìm kích thước cửa sổ hoạt động tốt nhất trên CIFAR-100, được phát hiện là 300 - điều này có nghĩa là sơ đồ phân bổ truy vấn của mô hình được cập nhật mỗi 300 bước huấn luyện, có thể bị ảnh hưởng bởi kích thước của bộ dữ liệu. Chúng tôi đã chọn bỏ qua tìm kiếm tinh tế hơn này do hạn chế thời gian.

3.3 Perturbed GQA (PGQA)
Thông qua các thí nghiệm, MHA có bằng chứng rõ ràng về việc mỗi head tương tự với chính nó hơn so với các head khác, trong khi GQA không duy trì được sự gắn kết này. Thường xuyên, đầu ra attention của một head trong GQA có thể tương tự với các head khác trong nhóm của nó hơn, hơn là chính nó dẫn đến một thiên vị tương tự nội nhóm. Mặc dù các quy luật tỷ lệ của các mô hình lớn

--- TRANG 4 ---
Hình 2: Độ lệch chuẩn của biến thể EMA gây ra ít đỉnh nhọn hơn, hỗ trợ ý tưởng rằng nó mạnh mẽ hơn đối với nhiễu thoáng qua.

cho thấy ít hoặc không có sự suy giảm hiệu suất, thiên vị tương tự góp phần làm cho GQA tụt lại phía sau MHA trong hiệu suất. Do đó, chúng tôi đề xuất Perturbed GQA loại bỏ thiên vị tương tự này bằng cách giới thiệu ma trận Nhiễu Gaussian trong đầu ra attention.

Ma trận nhiễu được tính toán riêng biệt cho mỗi nhóm g bằng cách tính trung bình μg và độ lệch chuẩn σg cho đầu ra attention Â của mỗi nhóm. Một ma trận ngẫu nhiên Rg được tạo ra có cùng hình dạng với Âg và được chuẩn hóa để phù hợp với thống kê của Âg như sau:

Gaussiang=σg∗(Rg−μ(Rg))/σ(Rg)+μg cho g= 1, . . . , G (9)

trong đó Gaussiang là ma trận Nhiễu Gaussian được tính toán cho mỗi nhóm. Để duy trì tính thưa thớt và tránh làm gián đoạn các mẫu attention, chúng tôi đặt các phần tử đường chéo của ma trận bằng không.

Ma trận Nhiễu Gaussian thu được được trừ khỏi đầu ra attention của mỗi nhóm. Trong khi đó, phân bổ nhóm được thực hiện theo cách tương tự từ trái-phải như GQA.

Hình 3 cho thấy PGQA giúp chúng tôi giảm thiểu thiên vị tương tự nội nhóm. Tuy nhiên, việc giảm thiểu thiên vị này cũng làm cho độ tương tự của mỗi head với chính nó giảm đi. Trong Phụ lục C, chúng tôi cũng thảo luận về các mẫu tương tự head được quan sát trong DGQA xấp xỉ trung bình có trọng số của các bản đồ nhiệt từ MHA và GQA.

4 Thí nghiệm
4.1 Mô hình và Bộ dữ liệu
Chúng tôi chọn sử dụng Vision Transformers (Dosovitskiy et al. 2021) trong các thí nghiệm của mình, theo sơ đồ thí nghiệm tương tự như Javadi et al. (2023). Lựa chọn này có lợi vì Vision Transformers được huấn luyện trước có ba kích thước mô hình, có thể dễ dàng phù hợp với phần cứng cấp người tiêu dùng. Các giá trị xấp xỉ cho kích thước mô hình, và số lượng head chúng được cấu hình để huấn luyện, được hiển thị trong Bảng 1.

Hình 3: PGQA giảm thiểu thiên vị tương tự từ GQA, tuy nhiên nó xóa bỏ các mẫu tự-tương tự được thấy trong bản đồ attention của MHA.

Tính linh hoạt này cho phép đánh giá mạnh mẽ hơn khi kết hợp với lựa chọn bộ dữ liệu của chúng tôi: CIFAR-10, CIFAR-100, Food101, và Tiny ImageNet (Krizhevsky (2009); Bossard, Guillaumin, and Van Gool (2014); mnmoustafa (2017)).

4.2 Chuyển đổi Checkpoint và Uptraining
Sơ đồ "uptraining" được đề xuất trong bài báo GQA bởi Ainslie et al. (2023) mô tả việc điều chỉnh các checkpoint MHA được huấn luyện trước thành kiến trúc GQA: điều này bao gồm việc xây dựng key và value head của mỗi nhóm bằng cách tính trung bình gộp tất cả các head gốc trong nhóm đó, theo sau bởi việc tiếp tục huấn luyện trước để thu hẹp khoảng cách giữa GQA và MHA. Chúng tôi thực hiện chuyển đổi checkpoint sử dụng các checkpoint MHA của ViT từ thư viện timm (Wightman 2019).

Với mô hình đã chuyển đổi, chúng tôi thực hiện up-training theo cách tương tự như đã được mô tả trong bài báo ViT gốc. Chúng tôi sử dụng bộ tối ưu AdamW với tốc độ học 1×10−4, với β1= 0.9,β2= 0.999,ε= 1×10−8 và weight decay 0.01. Chúng tôi huấn luyện trong 1 epoch trên bộ dữ liệu ImageNet-1k (Russakovsky et al. 2015), với hơn 1.3M hình ảnh huấn luyện. Điều này được thực hiện với batch size 32 trên một GPU NVIDIA GeForce RTX 4090 24GB duy nhất. Lưu ý rằng vì tất cả mô hình có cùng số lượng tham số, giữ kích thước (small, base, large) và bộ dữ liệu cố định, sự khác biệt duy nhất trong FLOPs uptraining xuất phát từ các tính toán bổ sung trong việc tìm số lượng truy vấn để phân bổ cho mỗi nhóm.

Đối với up-training, chúng tôi áp dụng augmentation ngẫu nhiên cho hình ảnh trước khi chúng được thay đổi kích thước thành kích thước hình ảnh (224,224,3). Điều này là để giảm thiểu overfitting là một thách thức lớn khi huấn luyện ViTs - augmentations là tốt hơn so với các chiến lược regularization như được thảo luận bởi Steiner et al. (2022).

Lưu ý rằng đối với uptraining và fine-tuning, chúng tôi chọn đặt các phiên bản tốt nhất của các biến thể của chúng tôi đối đầu với phiên bản tốt nhất của GQA, tức là với số lượng key-value head tối đa. Điều này cho phép thu hẹp tối đa khoảng cách giữa GQA (cộng với các biến thể của nó) và MHA: với H head cho một mô hình nhất định, chúng tôi đi với H/2 key-value head để công bằng, vì đó

--- TRANG 5 ---
Mô hình Số Tham số (M) Kích thước (MB) Số Head
ViT-S 30 77 6
ViT-B 75 300 12
ViT-L 300 1060 16

Bảng 1: Tham số mô hình, kích thước, và số lượng head. Lưu ý rằng (1) số lượng head đến từ cấu hình mô hình được huấn luyện trước, vì nó là một siêu tham số được đặt trước khi huấn luyện trước, và (2) Kích thước tính bằng MB đến từ các thí nghiệm uptraining của chúng tôi trên ImageNet-1k.

Hình 4: Chia tỷ lệ số lượng Key-Value head, và cách nó ảnh hưởng đến loss. Mỗi điểm dữ liệu đại diện cho một biến thể trên ViT-B, với 12 head, được finetuned trên CIFAR-100 trong 5 epoch, không có bất kỳ uptraining nào vì thời gian. Rõ ràng là các mô hình hoạt động tốt hơn khi chúng có số lượng Key-Value head tối đa.

là trần của những gì được cho phép với GQA. Chúng tôi bổ sung kiểm tra tác động của việc thay đổi số lượng key-value head đến hiệu suất trong Hình 4.

4.3 Fine-tuning
Để đánh giá các mô hình của chúng tôi, chúng tôi tải các checkpoint mô hình từ giai đoạn uptraining. Điều quan trọng cần lưu ý là tất cả mô hình trải qua cùng số bước huấn luyện, vì vậy không có lợi thế nào được trao cho các biến thể của chúng tôi bất chấp các sửa đổi được thực hiện. Không có cơ hội thêm nào được trao cho các biến thể của chúng tôi để phục hồi hiệu suất mô hình.

Mỗi mô hình được fine-tuned trong 5 epoch. Bộ tối ưu, phần cứng, chiến lược augmentation, và batch size giống như các lần chạy uptraining, ngoại trừ tốc độ học được đặt thành 1×10−5 cho fine-tuning.

5 Kết quả và Thảo luận
5.1 Đánh giá
Kết quả từ các lần chạy uptraining được hiển thị trong Bảng 2. Kết quả cốt lõi của chúng tôi được hiển thị trong Bảng 3, nơi chúng tôi uptrain mỗi mô hình và biến thể, và Bảng 4, nơi chúng tôi sử dụng checkpoint GQA đã uptrain và chỉ giới thiệu các sửa đổi của chúng tôi trong quá trình fine-tuning. Đối với các bảng này, GQA và các biến thể của nó có cấu hình sau: ViT-S sử dụng 3 key-value head, ViT-B sử dụng 6 key-value head, và ViT-L sử dụng 8 key-value head. Điểm chuẩn của chúng tôi là biến thể GQA, và hiệu suất của các biến thể MHA đã được cung cấp để tham khảo.

Mô hình Biến thể Độ chính xác
ViT-S GQA 65.95
KDGQA 51.85
DGQA 65.66
PGQA 59.55
ViT-B GQA 48.43
KDGQA 43.69
DGQA 45.53
PGQA 44.59
ViT-L GQA 52.49
KDGQA 37.21
DGQA 59.13
PGQA 49.94

Bảng 2: Độ chính xác validation sau 1 epoch uptraining trên ImageNet-1k. Các mô hình vượt trội hơn GQA trong danh mục của nó được tô đậm.

Từ Bảng 3, ghi chú quan trọng nhất là DGQA luôn vượt trội hơn điểm chuẩn GQA khi sử dụng kiến trúc ViT-L đã uptrain, và với biên độ khá lớn, lên đến 8% như trường hợp của Tiny ImageNet.

Rất thú vị khi lưu ý rằng đối với ViT-S và ViT-B, DGQA luôn hoạt động kém hơn so với GQA - chúng tôi muốn bao gồm những thí nghiệm thất bại này trong đánh giá của chúng tôi để làm nổi bật mức độ tác động của số lượng key-value head và kích thước mô hình đến sự thay đổi hiệu suất. Vì DGQA được thiết kế như một cải tiến so với KDGQA, không có gì ngạc nhiên khi phương pháp sau hoạt động kém trong đánh giá cuối cùng của chúng tôi, như đã có cùng mẫu trong các thí nghiệm sơ bộ.

Một mẫu thú vị khác là độ phức tạp của tác vụ phân loại: nếu chúng tôi tập trung vào ViT-L, chúng tôi thấy (xấp xỉ) rằng có tăng 1.5% độ chính xác cho CIFAR-10, tăng 5% cho CIFAR-100 và Food101 (lưu ý chúng có 100 và 101 lớp tương ứng), và 8% cho Tiny ImageNet (có 200 lớp). Điều này, cùng với ghi chú từ đoạn trước, dẫn chúng tôi đến giả thiết rằng lợi ích thực sự của các phương pháp mới, xuất phát từ việc phân nhánh từ một cơ sở được huấn luyện trước, chỉ xuất hiện ở quy mô lớn hơn, tức là với các mô hình lớn hơn và bộ dữ liệu lớn hơn.

Trong khi PGQA có thể hoạt động kém hơn điểm chuẩn trong các đánh giá của chúng tôi, nó tuân theo cùng mẫu trên (biên độ hẹp hơn khi mở rộng quy mô), và trình bày một nghiên cứu tình huống hấp dẫn. Bất chấp hiệu suất kém, PGQA cung cấp những hiểu biết có giá trị về (thiếu) tính kiên cường của các cơ chế attention

--- TRANG 6 ---
Mô hình Biến thể Độ chính xác
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-S MHA 98.42 90.94 87.29 86.51
GQA 97.27 84.70 80.95 80.23
KDGQA 94.05 71.32 72.04 63.66
DGQA 97.27 84.55 80.77 80.16
PGQA 95.81 78.42 76.55 73.81
ViT-B MHA 98.67 91.91 90.17 91.04
GQA 93.32 71.78 72.42 62.87
KDGQA 91.33 67.84 68.01 57.90
DGQA 91.95 69.63 70.59 59.55
PGQA 91.84 67.86 68.31 59.27
ViT-L MHA 99.10 94.05 91.94 92.60
GQA 94.81 76.41 74.57 67.73
KDGQA 88.87 64.50 62.90 52.49
DGQA 96.36 81.67 79.93 75.83
PGQA 93.21 75.03 73.33 66.78

Bảng 3: Độ chính xác test của các lần chạy finetuning, sau uptraining cho mỗi biến thể. Các mô hình vượt trội hơn GQA trong danh mục của nó được tô đậm.

Mô hình Biến thể Độ chính xác
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-S MHA 98.42 90.94 87.29 86.51
GQA 97.27 84.70 80.95 80.23
KDGQA 93.57 64.34 71.02 65.83
DGQA 97.09 84.54 80.81 80.03
PGQA 95.81 78.68 75.44 75.79
ViT-B MHA 98.67 91.91 90.17 91.04
GQA 93.32 71.78 72.42 62.87
KDGQA 90.36 65.64 65.99 56.42
DGQA 92.80 70.17 71.70 61.42
PGQA 91.85 68.19 68.73 59.75
ViT-L MHA 99.10 94.05 91.94 92.60
GQA 94.81 76.41 74.57 67.73
KDGQA 86.69 63.34 67.18 61.67
DGQA 93.77 75.83 73.79 66.04
PGQA 94.19 73.66 71.91 66.84

Bảng 4: Độ chính xác test của các lần chạy finetuning, sau uptraining chỉ sử dụng checkpoint GQA.

anisms. Nó gợi ý rằng ngay cả những nhiễu loạn nhỏ cũng có thể ảnh hưởng đến phân phối attention, đặc biệt khi mô hình đã được điều chỉnh để hoạt động theo cách cụ thể. Phát hiện này nhấn mạnh tầm quan trọng của uptraining sâu rộng khi thực hiện những sửa đổi như vậy đối với các mô hình được huấn luyện trước, vì động lực truyền tiến bị thay đổi đáng kể, đòi hỏi những điều chỉnh bổ sung để phục hồi hiệu suất mô hình.

Từ Bảng 4, có thể thấy rằng các biến thể của chúng tôi hoạt động tệ hơn trên quy mô tuyệt đối, bất kể so sánh với GQA. Điều này làm nổi bật rằng uptraining đặc biệt phải được tiến hành cho các biến thể là sửa đổi của GQA, đến từ việc chuyển đổi các checkpoint MHA.

5.2 Thời gian Suy luận
Bảng 5 trình bày so sánh thời gian suy luận cho các mô hình cơ sở với GQA làm đường cơ sở. PGQA thể hiện sự tăng cao nhất trong thời gian suy luận là 3.31%, trong khi DGQA và KDGQA cho thấy sự tăng tối thiểu là 0.45% và 0.21% tương ứng.

5.3 Độ nhạy cảm với Huấn luyện
Kết quả của chúng tôi cho thấy rõ ràng rằng các nhóm bất đối xứng và phân bổ có thông tin kích hoạt có thể giúp cải thiện hiệu suất. Tuy nhiên, có nhiều cân nhắc khi nhằm cải thiện các cơ chế cho các mô hình hiện có.

Từ phần con trước, chúng tôi lại chỉ ra rằng

--- TRANG 7 ---
Mô hình Thời gian Suy luận tính bằng Giây % Khác biệt
GQA 0.192415 -
DGQA 0.193276 0.4477%
KDGQA 0.192812 0.2064%
PGQA 0.198783 3.3098%

Bảng 5: So sánh thời gian suy luận giữa các biến thể ViT-B được tính trung bình trên batch size 288, GPU NVIDIA GeForce RTX 4090 24GB đơn.

quy mô/kích thước của bộ dữ liệu và mô hình đóng vai trò quan trọng trong việc xác định liệu các biến thể có khả năng vượt trội hơn điểm chuẩn hay không. Với các mô hình lớn hơn, có số lượng head lớn hơn, có nghĩa là có trần cao hơn về số lượng key-value head mà chúng tôi có thể điều chỉnh. Số lượng key-value head lớn hơn cho phép các cơ chế phân bổ có thông tin thực sự tỏa sáng, vì có quá ít key-value head ngụ ý có biên độ sai sót hẹp hơn nhiều trong việc hình thành nhóm. Điều này có thể sẽ ít là vấn đề khi áp dụng cho Transformers cho các tác vụ ngôn ngữ tự nhiên vì nhiều điểm chuẩn và mô hình phổ biến thường rất lớn về quy mô (Hoffmann et al. 2022).

Đây không phải là sự xuất hiện cô lập của độ nhạy cảm của một phương pháp mới đối với quy mô của mô hình và bộ dữ liệu. Dosovitskiy et al. (2021) đưa ra những ghi chú tương tự trong bài báo của họ về Vision Transformer khi thảo luận về yêu cầu dữ liệu huấn luyện trước: ViT-L hoạt động kém hơn so với ViT-B trên ImageNet-1k, hoạt động tương tự trên ImageNet-21k, và vượt trội trên JFT-300M. Chúng tôi giả thiết rằng mẫu này có thể được ngoại suy cho nghiên cứu của chúng tôi rằng cần nhiều hiệu chỉnh hơn cho các phương pháp nhóm có thông tin kích hoạt để vượt qua phương pháp nhóm tĩnh.

Cũng đáng chỉ ra lần nữa rằng KDGQA và DGQA có cùng ý tưởng cơ bản, nhưng phương pháp sau có góc nhìn hơi khác trong một bước của tính toán. Bất chấp sắc thái nhỏ này, rất rõ ràng rằng DGQA tốt hơn đáng kể so với KDGQA, và chỉ biến thể EMA.

Một thách thức khác mà chúng tôi gặp phải là về tốc độ học. Chúng tôi thấy rằng tốc độ học 1×10−3 quá cao cho các biến thể của chúng tôi cụ thể, phá vỡ hiệu suất của chúng trên tất cả bộ dữ liệu. Tốc độ học thấp hơn và phổ biến hơn là 3×10−4 hoạt động tốt hơn cho PGQA, nhưng vẫn phá vỡ DGQA. Cuối cùng chúng tôi bắt đầu thấy kết quả hợp lý với 1×10−4, và quyết định sử dụng 1×10−5 (xem Phụ lục A).

Thành phần cuối cùng trong công thức huấn luyện một biến thể của mô hình được huấn luyện trước (đã chuyển đổi) là bộ dữ liệu uptraining. Đối với các thí nghiệm sơ bộ, chúng tôi tìm hiểu việc huấn luyện GQA và các biến thể của nó

1. Từ đầu,
2. Chuyển đổi từ checkpoint MHA nhưng không uptraining,
3. Uptraining trên CINIC-10 trong 10 epoch,
4. Uptraining trên ImageNet-1k trong 1 epoch.

Chúng tôi thấy rằng kết quả sử dụng các biến thể của chúng tôi khá kém khi các mô hình không được uptrain đúng cách: cụ thể khi chúng tôi sử dụng phiên bản sửa đổi của CINIC-10 (Darlow et al. 2018), có 270000 hình ảnh huấn luyện. Thiếu đa dạng và độ phân giải thô thấp khiến đây là lựa chọn kém, trở nên rõ ràng khi chúng tôi uptrain trên ImageNet-1k chỉ một epoch. Mặc dù số bước huấn luyện trong thiết lập thứ tư ít hơn một nửa so với thiết lập thứ ba, tác động của chất lượng uptraining là đáng kể vì đó là lúc chúng tôi bắt đầu thấy các biến thể của mình vượt trội hơn điểm chuẩn GQA. Do hạn chế thời gian và tài nguyên, chúng tôi không thể thí nghiệm với uptraining trên ImageNet-21k hoặc JFT-300M, hoặc hiệu chỉnh dài hơn trên ImageNet-1k.

Chúng tôi khám phá ý tưởng này sâu hơn trong Phụ lục B.

6 Kết luận
Chúng tôi giới thiệu KDGQA, DGQA, và PGQA: các chiến lược để vượt qua chiến lược nhóm tĩnh như đã được tìm thấy trong GQA. Chúng tôi thấy rằng DGQA, là phiên bản tinh chỉnh của KDGQA, vượt trội hơn đáng kể so với GQA đã uptrain trên ViT-L, và cho thấy tăng hiệu suất lớn hơn cho các bộ dữ liệu phức tạp hơn. Điều này cho thấy tiềm năng của việc hiệu chỉnh cho các cơ chế nhóm có thông tin và thích ứng hơn.

7 Công việc Tương lai
Vì các thí nghiệm của chúng tôi đã chứng minh tính khả thi của việc cải thiện hiệu suất của Transformers sử dụng GQA thông qua cơ chế nhóm có thông tin hơn và thoát khỏi phân bổ đồng nhất, bước đầu tiên sẽ là mở rộng những điều này sang Decoders. Với sự phổ biến của các tác vụ Ngôn ngữ Tự nhiên ngày nay, sẽ là một chuỗi thí nghiệm thú vị để tìm hiểu các LLM có hàng tỷ tham số và xử lý các tác vụ có chiều nội tại cao hơn nhiều, như mô hình hóa ngôn ngữ. Dựa trên kinh nghiệm của chúng tôi, chúng tôi tin rằng sự kết hợp này có thể dẫn đến tăng hiệu suất thậm chí lớn hơn với việc sử dụng các cơ chế nhóm có thông tin.

Ngay cả trong khung model-agnostic, có những hướng thú vị mà chúng tôi dự định khám phá. Thứ nhất, chúng tôi lưu ý rằng việc sử dụng chuẩn của Keys, hoặc thao tác bản đồ attention, vẫn không thực sự cho phép các kích hoạt giao tiếp với nhau. Cụ thể, chúng tôi dự định khám phá liệu có thể tận dụng ái lực giữa các truy vấn và key, và key và value, để cải thiện cơ chế nhóm nhiều hơn nữa. Chúng tôi cũng có thể chuyển hướng từ chỉ các phương pháp có thông tin kích hoạt, và xem xét các phương pháp có thông tin trọng số, để quyết định phân bổ độc lập với đầu vào. Đây là các hướng trong cùng khu phố với Chen et al. (2024), nhưng chúng tôi muốn có phương pháp ít ngẫu nhiên hơn để tìm các nhóm tối ưu và cho cả các biến thể và điểm chuẩn cùng số bước huấn luyện để đánh giá công bằng.

Tham khảo
Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y.; Lebrón, F.; and Sanghai, S. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv:2305.13245.

--- TRANG 8 ---
Bossard, L.; Guillaumin, M.; and Van Gool, L. 2014. Food-101 – Mining Discriminative Components with Random Forests. In European Conference on Computer Vision.

Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165.

Chen, Y.; Zhang, C.; Gao, X.; Mullins, R. D.; Constantinides, G. A.; and Zhao, Y. 2024. Optimised Grouped-Query Attention Mechanism for Transformers. arXiv:2406.14963.

Chinnakonduru, S. S.; and Mohapatra, A. 2024. Weighted Grouped Query Attention in Transformers. arXiv:2407.10855.

Darlow, L. N.; Crowley, E. J.; Antoniou, A.; and Storkey, A. J. 2018. CINIC-10 is not ImageNet or CIFAR-10. arXiv:1810.03505.

Dong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.; Chen, D.; and Guo, B. 2022. CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. arXiv:2107.00652.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929.

Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.; Welbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.; van den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.; Simonyan, K.; Elsen, E.; Rae, J. W.; Vinyals, O.; and Sifre, L. 2022. Training Compute-Optimal Large Language Models. arXiv:2203.15556.

Javadi, F.; Ahmed, W.; Hajimolahoseini, H.; Ataiefard, F.; Hassanpour, M.; Asani, S.; Wen, A.; Awad, O. M.; Liu, K.; and Liu, Y. 2023. GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values. arXiv:2311.03426.

Joshi, V.; Laddha, P.; Sinha, S.; Omer, O. J.; and Subramoney, S. 2024. QCQA: Quality and Capacity-aware grouped Query Attention. arXiv:2406.10247.

Krizhevsky, A. 2009. Learning multiple layers of features from tiny images. Technical report.

Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2103.14030.

mnmoustafa, M. A. 2017. Tiny ImageNet.

Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3): 211–252.

Shazeer, N. 2019. Fast Transformer Decoding: One Write-Head is All You Need. arXiv:1911.02150.

Steiner, A.; Kolesnikov, A.; Zhai, X.; Wightman, R.; Uszkoreit, J.; and Beyer, L. 2022. How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. arXiv:2106.10270.

Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2023. Attention Is All You Need. arXiv:1706.03762.

Wightman, R. 2019. PyTorch Image Models. https://github.com/rwightman/pytorch-image-models.

--- TRANG 9 ---
A Tốc độ Học
Theo các ghi chú từ đánh giá chính của chúng tôi, chúng tôi muốn hiển thị tác động của tốc độ học đến các mô hình và biến thể. Tốc độ học 1×10−4 là nơi huấn luyện bắt đầu ổn định trên tất cả mô hình và biến thể - chúng tôi thấy rằng tốc độ học thấp hơn (lên đến 1×10−5) có lợi hơn cho fine-tuning.

Bảng 6 hiển thị kết quả từ cùng thiết lập như Bảng 3, nhưng với tốc độ học cao hơn. Chúng tôi thấy rằng một số biến thể ViT-S chỉ vượt trội hơn điểm chuẩn GQA một chút, mặc dù không với biên độ đáng kể, và DGQA vượt trội hơn GQA một cách nhất quán khi sử dụng ViT-L.

Nếu các số được so sánh trên hai bảng này, có thể thấy rằng các mô hình hoạt động tốt hơn với tốc độ học thấp hơn - tăng hiệu suất bởi DGQA cũng đáng kể hơn, chỉ ra rằng những mô hình này nhạy cảm hơn nhiều với thiết lập huấn luyện.

B Tác động của Phương pháp Uptraining
Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi khám phá tính khả thi của việc giảm chi phí tính toán của uptraining bằng cách sử dụng bộ dữ liệu CINIC-10 để hiệu chỉnh mô hình, trái ngược với ImageNet-1k tốn nhiều tài nguyên hơn. Chúng tôi uptrain trên một biến thể của CINIC-10, chứa 270000 hình ảnh, trong 10 epoch, chỉ thiếu chút ít gấp đôi lượng bước huấn luyện cần thiết cho 1 epoch trên ImageNet-1k.

Độ phân giải thấp, thiếu đa dạng trong các lớp, và kích thước nhỏ của bộ dữ liệu được chứng minh là xa từ tối ưu cho các mô hình của chúng tôi. Để ngắn gọn, chúng tôi chọn đánh giá tất cả mô hình chỉ sử dụng các checkpoint GQA tương ứng, như có thể thấy trong Bảng 7.

Vì CINIC-10 chứa cùng 10 lớp từ CIFAR-10, chúng tôi thấy rằng các mô hình rõ ràng hoạt động tốt hơn trên CIFAR-10. Tuy nhiên, những thiếu sót của bộ dữ liệu được phản ánh khi chúng tôi chuyển sang ba bộ dữ liệu còn lại, nơi các mô hình không thể khái quát hóa một cách kinh khủng, trải qua sự sụt giảm đáng kể về độ chính xác khi so sánh với Bảng 4.

Thí nghiệm này cho thấy rằng chất lượng uptraining rất quan trọng, và đó là bước cần thiết để chuyển sang ImageNet-1k. Chúng tôi tin rằng uptraining trên ImageNet-21k và JFT-300M có thể cải thiện chất lượng của các mô hình fine-tuned cuối cùng thậm chí nhiều hơn, nhưng do hạn chế tài nguyên và thời gian, chúng tôi không thể khám phá hướng này.

C Thiên vị Tương tự Head
Trong MHA, đầu ra của mỗi head không cho thấy độ tương tự chi phối với đầu ra từ bất kỳ head nào khác. Thực tế, chúng tôi thấy sự không tương tự giữa đầu ra của một head so với các head cách xa nó.

Tuy nhiên trong GQA, đầu ra của các head rất tương tự với các head khác trong nhóm gây ra thiên vị tương tự nội nhóm mà, trong số những lý do khác, có thể đóng góp vào sự sụt giảm độ chính xác trong GQA so với MHA.

Chúng tôi ban đầu giới thiệu PGQA như một cách để giảm thiểu thiên vị tương tự nội nhóm này, tuy nhiên chúng tôi quan sát trong Hình 3 rằng PGQA cũng xóa bỏ các xu hướng khác được quan sát trong bản đồ nhiệt của MHA. Mặt khác DGQA chứng tỏ là một điểm giữa thú vị giữa GQA và MHA (Hình 5).

Chúng tôi hiển thị trong Hình 6 rằng bản đồ nhiệt của DGQA gần với trung bình có trọng số của GQA và MHA. Theo cách này, DGQA có thể giảm thiểu phần lớn thiên vị tương tự nội nhóm trong khi cũng giữ lại xu hướng không tương tự giữa các head như được thấy trong bản đồ nhiệt của MHA.

D Dữ liệu Phức tạp Cần nhiều Nhóm Không đồng nhất hơn
Chúng tôi huấn luyện cùng ViT từ đầu với cùng siêu tham số (Batch Size 32, tốc độ học 1e−4, bộ tối ưu AdamW, và kích thước cửa sổ 100) trong 1000 bước huấn luyện trên bộ dữ liệu MNIST và CIFAR-10. Các thí nghiệm của chúng tôi trong Hình 7 cho thấy rằng bộ dữ liệu CIFAR-10 phức tạp hơn, nhiều đặc trưng hơn chọn tỷ lệ cao hơn của các phân bổ của nó (thay đổi mỗi 100 bước huấn luyện) là không đồng nhất.

Thí nghiệm đơn giản này cho thấy sự phụ thuộc của phân bổ nhóm vào bộ dữ liệu, tuy nhiên tính chất tĩnh của GQA không cho phép những phân bổ động này được cung cấp bởi cơ chế DGQA của chúng tôi.

E Tương quan giữa Số Tham số và Nhóm Không đồng nhất
Chúng tôi lặp lại các thí nghiệm từ Phần D trong phụ lục nhưng chúng tôi tăng dần số tham số bằng cách tăng chiều embedding nhưng giữ số lớp không đổi. Hình 8 cho thấy rằng trong khi số lớp giữ nguyên, việc đơn giản tăng tham số không nhất thiết dẫn đến tăng số nhóm không đồng nhất so với tổng phân bổ nhóm.

Tuy nhiên, khi chúng tôi lặp lại những thí nghiệm này một lần nữa nhưng lần này, chúng tôi giữ chiều embedding không đổi nhưng tăng số lớp và do đó số tham số cũng tăng. Hình 9 cho thấy xu hướng tăng mạnh hơn nhiều mô tả rằng việc tăng số lớp dẫn đến tỷ lệ phần trăm lớn hơn của phân bổ nhóm không đồng nhất.

--- TRANG 10 ---
Mô hình Biến thể Độ chính xác
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-S MHA 96.29 85.87 84.93 78.61
GQA 95.73 83.77 82.22 76.38
KDGQA 93.14 76.53 77.78 66.64
DGQA 96.10 83.57 82.30 76.16
PGQA 95.19 81.03 80.03 73.02
ViT-B MHA 95.08 81.86 81.13 73.04
GQA 92.80 73.58 73.75 62.99
KDGQA 91.61 71.35 71.34 59.60
DGQA 91.91 73.42 73.29 60.98
PGQA 90.93 72.06 72.02 60.65
ViT-L MHA 96.08 84.97 83.24 78.19
GQA 94.05 76.61 76.46 65.62
KDGQA 87.33 61.24 66.85 54.33
DGQA 95.11 80.85 79.08 71.91
PGQA 92.71 70.37 72.61 65.20

Bảng 6: Độ chính xác test của các lần chạy finetuning, sau uptraining trên ImageNet-1k với tốc độ học 1×10−4, sử dụng các checkpoint đặc trưng biến thể. Các mô hình vượt trội hơn GQA trong danh mục của nó được tô đậm.

Mô hình Biến thể Độ chính xác
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-S GQA 98.13 72.78 61.83 53.88
KDGQA 95.01 62.71 53.83 44.61
DGQA 98.15 72.91 60.88 53.49
PGQA 97.68 66.42 54.02 48.37
ViT-B GQA 96.74 67.36 56.46 49.26
KDGQA 95.23 61.28 50.14 41.74
DGQA 96.57 66.44 54.83 46.72
PGQA 96.25 63.52 50.34 44.41
ViT-L GQA 97.95 77.31 66.80 59.85
KDGQA 97.08 71.38 60.17 54.37
DGQA 97.78 76.77 65.53 59.31
PGQA 97.77 74.11 63.08 56.77

Bảng 7: Độ chính xác test của các lần chạy finetuning, sau uptraining trên CINIC-10 với tốc độ học 1×10−5, sử dụng checkpoint chỉ GQA.

Hình 5: DGQA là một điểm giữa thú vị giữa GQA và MHA. Không giống PGQA, bản đồ nhiệt của DGQA không xóa bỏ các mẫu được quan sát trong MHA.

--- TRANG 11 ---
Hình 6: DGQA xấp xỉ trung bình có trọng số giữa GQA và MHA. DGQA bảo tồn xu hướng từ MHA trong khi giảm thiểu đáng kể thiên vị tương tự nội nhóm làm khổ GQA.

Hình 7: Kiến trúc mô hình giống nhau, có vẻ như CIFAR10 phức tạp hơn ưa thích nhiều nhóm không đồng nhất hơn trong quá trình huấn luyện.

Hình 8: % nhóm không đồng nhất không cho thấy mẫu rõ ràng khi chúng tôi tăng tham số trong khi giữ các lớp không đổi.

Hình 9: % nhóm không đồng nhất tăng khi chúng tôi tăng số lượng lớp trong mô hình.
