# 2210.00640.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2210.00640.pdf
# Kích thước tệp: 383515 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
ATTENTION RỘNG LÀ CON ĐƯỜNG ĐI TIẾN VỀ PHÍA TRƯỚC CHO
TRANSFORMERS ?
Jason Ross Brown
University of Cambridge
jrb239@cam.ac.ukYiren Zhao
Imperial College London and University of Cambridge
a.zhao@imperial.ac.uk
Ilia Shumailov
University of Oxford
ilia.shumailov@chch.ox.ac.ukRobert D Mullins
University of Cambridge
robert.mullins@cl.cam.ac.uk

TÓM TẮT
Transformer là một kiến trúc deep learning cực kỳ mạnh mẽ và nổi bật. Trong công trình này,
chúng tôi thách thức niềm tin phổ biến trong deep learning rằng đi sâu hơn là tốt hơn, và chỉ ra
một cách tiếp cận thay thế là xây dựng Transformers attention rộng hơn. Chúng tôi chứng minh
rằng các mô hình Transformer một lớp rộng thường có thể bằng hoặc đôi khi vượt trội hơn những
mô hình sâu hơn trong nhiều tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) khác nhau khi cả hai được
huấn luyện từ đầu. Tác động của việc thay đổi tỷ lệ khung hình mô hình trên Transformers được
nghiên cứu một cách có hệ thống. Tỷ lệ này cân bằng số lượng lớp và số lượng attention heads
trên mỗi lớp, trong khi giữ tổng số attention heads và tất cả các siêu tham số khác không đổi.
Trung bình, trên 4 tác vụ NLP và 10 loại attention, các mô hình một lớp rộng hoạt động tốt hơn
0.3% so với các đối tác sâu của chúng. Chúng tôi chỉ ra một đánh giá sâu sắc và chứng minh
cách các mô hình rộng đòi hỏi một dung lượng bộ nhớ nhỏ hơn nhiều và có thể chạy nhanh hơn
trên phần cứng thông dụng, ngoài ra, những mô hình rộng hơn này cũng có thể giải thích được
hơn. Ví dụ, một Transformer một lớp trên phân loại văn bản cấp byte IMDb có độ trễ suy luận
nhanh hơn 3:1 trên CPU so với đối tác sâu hơn có độ chính xác tương đương, và chỉ bằng một
nửa kích thước. Do đó chúng tôi đề xuất các mô hình rộng hơn và nông hơn như một giải pháp
thay thế khả thi và mong muốn cho các mô hình nhỏ trên các tác vụ NLP, và như một lĩnh vực
nghiên cứu quan trọng cho các miền vượt ra ngoài điều này.

1 Giới thiệu
Kể từ Vaswani et al. [2017], các kiến trúc dựa trên Transformer đã trở nên phổ biến do những ưu thế của chúng so với
các kiến trúc trước đó như mạng nơ-ron hồi quy (RNNs) và đôi khi thậm chí cả mạng nơ-ron tích chập (CNNs). Nhiều
X-formers mới cũng đã được đề xuất để cải thiện Transformer gốc bằng cách khắc phục hạn chế về độ dài chuỗi của
nó bằng cách cung cấp một cơ chế attention có thể mở rộng hơn [Beltagy et al., 2020, Chorowski et al., 2020, Wang
et al., 2020b]. Tuy nhiên, ít nghiên cứu đã được thực hiện về tính liên quan của kích thước tính toán attention trong
mỗi lớp, số lượng lớp attention, và cách những tham số này liên quan đến các đặc điểm của Transformer kết quả.

Nguồn tham số chính trong mạng Transformer là Feed-forward Network (FFN) trong mỗi lớp encoder hoặc decoder,
và các lớp tuyến tính chuyển đổi từ chiều đặc trưng chuỗi (thường bằng chiều embedding ban đầu) sang chiều đặc
trưng attention, và trở lại sau khi attention được áp dụng. Mỗi attention head thường có số lượng đặc trưng attention
bằng nhau. Xét một chuỗi đầu vào X∈R^S×E, trong đó S là độ dài chuỗi và E là chiều embedding. Ở đây, một
multi-head attention với H heads được sử dụng và mỗi head hoạt động trên phép chiếu đã học với chiều A. Sau cơ
chế attention có một FFN với một chiều ẩn duy nhất có kích thước M. Những lớp này sau đó được xếp chồng L lần
như minh họa ở bên trái Hình 1. Thường trong một Transformer điển hình E = A×H. Tổng số tham số trong một
Transformer encoder được cho bởi:arXiv:2210.00640v2  [cs.LG]  8 Nov 2022

--- TRANG 2 ---
L Lớp m
Embedding Đầu vào & Vị trí
E
S
Lớp Tuyến tính QKV
 Lớp Tuyến tính QKV
A
S
Attention
Nối & Lớp Tuyến tính
Attention
Q K V
E
S
Mạng Feed Forward
Lớp Tuyến tính & Softmax
Đầu vàoĐầu ra
H Heads ,
Embedding Đầu vào & Vị trí
E
S
Lớp Tuyến tính QKV
 Lớp Tuyến tính QKV
A
S
Attention
Nối & Lớp Tuyến tính
Attention
Q K V
E
S
Mạng Feed Forward
Lớp Tuyến tính & Softmax
Đầu vàoĐầu ra
LxH Heads ,

Hình 1: So sánh một bộ phân loại Transformer sâu (trái, với L lớp và H heads cho mỗi lớp) vs một bộ tương đương
rộng (phải, với một lớp duy nhất và L×H heads). Layer norms và kết nối dư đã được bỏ qua trong sơ đồ để rõ ràng,
để biết chi tiết về kiến trúc Transformer đầy đủ xem Vaswani et al. [2017].

Tham số Encoder = L(3EAH + AHE + EM + ME)
= 2LE(2AH + M)

Trong bài báo này, chúng tôi điều tra các tác động của việc thay đổi L và H trong khi giữ tích của chúng, tổng số
heads, giống nhau. Chúng tôi bắt đầu với các giá trị điển hình cho L và H và sau đó di chuyển xuống một lớp duy
nhất. Một sơ đồ minh họa không gian thiết kế của chúng tôi và sự khác biệt giữa các mô hình rộng nhất và sâu nhất
được đưa ra trong Hình 1. Chúng tôi gọi tỷ lệ giữa lớp và heads là tỷ lệ khung hình mô hình. Điều này tự nhiên dẫn
đến một câu hỏi hấp dẫn: Tỷ lệ khung hình mô hình tốt nhất cho số lượng ngày càng tăng của các mô hình X-former
là gì? Chúng tôi xem xét tác động của tỷ lệ khung hình mô hình đối với độ chính xác, hiệu suất thời gian chạy, kích
thước mô hình và khả năng giải thích.

Dựa trên câu hỏi trên, chúng tôi điều tra ảnh hưởng của các tỷ lệ khung hình mô hình khác nhau trên 9 mô hình
X-former, mỗi mô hình với cơ chế attention riêng, ngoài Transformer gốc. Công trình trước đây về kiến trúc Transformer
chủ yếu tập trung vào thiết kế các kiểu attention hiệu quả hơn [Choromanski et al., 2020, Wang et al., 2020b] hoặc
sử dụng Network Architecture Search (NAS) để khám phá một kết hợp tối ưu của các toán tử [So et al., 2019]. Bằng
cách thay đổi tỷ lệ khung hình mô hình, chúng tôi xem xét một không gian thiết kế thô hơn. Không gian thiết kế này
không được khám phá phổ biến trong các thuật toán NAS cho Transformers, và chúng tôi đánh giá một số kiến trúc
mô hình thú vị như mô hình một lớp với nhiều heads song song.

Đối với mỗi tỷ lệ khung hình mô hình, chúng tôi chạy các thí nghiệm với mỗi X-former trên một số tác vụ phân loại
văn bản với các độ dài chuỗi đầu vào khác nhau từ 500 đến 4000. Chúng tôi quan sát thực nghiệm rằng các mô hình
rộng hơn và nông hơn thường có thể bằng hoặc đôi khi đánh bại độ chính xác của các mô hình sâu hơn. Quan sát này
thách thức mô hình thiết kế phổ biến của việc cố gắng xây dựng các Mạng Nơ-ron sâu hơn. Chúng tôi chỉ ra một số
ưu điểm chính khác của mô hình nông hơn và rộng hơn. Thứ nhất, nó thân thiện hơn với độ trễ trên phần cứng thông
dụng. Thứ hai, các mô hình rộng hơn nhỏ hơn về số lượng tham số. Và thứ ba, đầu ra của các mô hình rộng hơn có
thể giải thích được hơn.

Để tóm tắt, chúng tôi đóng góp những điều sau trong bài báo này:
• Chúng tôi chứng minh rằng các mô hình rộng hơn và nông hơn thường có thể bằng hoặc đôi khi đánh bại độ
chính xác của các mô hình sâu hơn khi không có tiền huấn luyện trọng số hoặc embeddings. Trên tất cả 4 tác
vụ, độ chính xác trung bình cho Transformer vanilla tăng 0.4% giữa các mô hình sâu bình thường và các mô
hình rộng một lớp của chúng tôi.
• Chúng tôi chỉ ra rằng kết quả của chúng tôi nhất quán trên nhiều cơ chế attention khác nhau và độ dài chuỗi
đầu vào, và do đó có một sự tương đương thiết kế chung trong việc tăng chiều sâu của mô hình Transformer
so với việc tăng chiều rộng. Trung bình trên tất cả các loại attention không phải vanilla và tác vụ, độ chính
xác tăng 0.3% từ sâu nhất đến rộng nhất.
• Chúng tôi chỉ ra rằng việc mở rộng các mô hình bằng cách cố định kích thước tính toán attention dẫn đến
ít tham số tổng thể hơn và suy luận nhanh hơn. Chúng tôi chỉ ra rằng các mô hình rộng hơn trung bình nhỏ
hơn 1:4 lần và có độ trễ suy luận nhanh hơn 3:1 lần trên CPU và 1:9 lần trên GPU, so với các mô hình sâu.
• Chúng tôi chứng minh cách các mạng một lớp có thể có dự đoán có thể giải thích được hơn bằng cách kiểm
tra trọng số attention của mỗi head trong một lớp.

2 Công trình liên quan
2.1 Mạng rộng hơn
Zagoruyko & Komodakis [2016] và Wu et al. [2019] tương ứng chỉ ra và điều tra cách mở rộng và làm nông hơn
ResNet CNNs [He et al., 2016] có thể cải thiện hiệu suất của chúng. Transformers cũng sử dụng kết nối dư, vì vậy
điều này giúp thúc đẩy điều tra của chúng tôi.

Xue et al. [2022b] thấy rằng các lớp rộng hơn trong Transformers (trong cả attention và đặc trưng chuỗi) có thể cải
thiện hiệu suất trên các tác vụ thị giác và ngôn ngữ tự nhiên. Tuy nhiên, họ sử dụng lớp mixture-of-experts thay vì
mạng feed-forward điển hình ở đầu ra của Transformer encoder. Họ làm điều này vì một chiều lớn hơn nhiều của các
đặc trưng chuỗi có nghĩa là số lượng tham số trong lớp feed-forward sẽ tăng lên rất nhiều. Vì chúng tôi chỉ tăng
chiều đặc trưng attention nên chúng tôi không gặp phải vấn đề này.

Xue et al. [2022a] điều tra huấn luyện masked autoencoder và cách nó có thể giúp giảm vấn đề làm mượt quá mức
trong việc huấn luyện các mạng Transformer sâu - embeddings của các token hội tụ để trở nên tương tự ở các lớp sâu
hơn. Họ khám phá cấu hình tối ưu của Transformer khi sử dụng masked autoencoding để tiền huấn luyện nó, và sau
đó tinh chỉnh nó trên các tác vụ cụ thể. Họ thấy rằng đối với vision Transformers, tốt hơn là tiền huấn luyện với một
autoencoder sâu hơn là một autoencoder rộng. Vì chúng tôi không sử dụng tiền huấn luyện và embeddings đã được
tiền huấn luyện, kết quả của họ không thể so sánh trực tiếp với kết quả của chúng tôi.

2.2 Tối ưu hóa kiến trúc cho Transformers
Rất nhiều nghiên cứu trong lĩnh vực Transformer tập trung vào việc tìm một cơ chế attention hiệu quả hơn [Beltagy
et al., 2020, Child et al., 2019, Choromanski et al., 2020, Katharopoulos et al., 2020, Kitaev et al., 2020, Liu et al.,
2018, Tay et al., 2020a, 2021, Wang et al., 2020b, Zaheer et al., 2020], vì attention tích vô hướng gốc có độ phức tạp
bậc hai đối với độ dài chuỗi đầu vào. Long Range Arena (LRA, Tay et al. [2020b]) so sánh và đối chiếu những phương
pháp này, lưu ý rằng trong trường hợp không có embeddings và tham số mô hình đã được tiền huấn luyện, cơ chế
attention tốt nhất phụ thuộc vào tác vụ. Do đó không có cơ chế attention tốt nhất rõ ràng. Vì điều này, chúng tôi thử
nghiệm các cơ chế attention khác nhau trên nhiều tác vụ khác nhau không có embeddings hoặc tham số mô hình đã
được tiền huấn luyện. Điều này làm rõ rằng những phát hiện của chúng tôi phần lớn độc lập với cơ chế attention và
tác vụ, và bối cảnh hóa việc đi rộng như một quyết định thiết kế chung.

Việc áp dụng Neural Architecture Search (NAS) cho các mô hình Transformer cũng đã được khám phá. Cả Neural
Architecture Transformer (NAT) Guo et al. [2019] và Hardware-aware Transformers Wang et al. [2020a] đều sử dụng
NAS để tìm kiếm các mô hình hiệu quả hơn về phần cứng. So et al. [2019] sử dụng tìm kiếm tiến hóa để tối ưu hóa
các thành phần và kết nối bên trong lớp encoder hoặc decoder. Liu et al. [2022] áp dụng RankNAS [Hu et al., 2021]
cho cosFormer [Qin et al., 2022] và Transformers tiêu chuẩn [Vaswani et al., 2017]. Họ thực hiện tìm kiếm các siêu
tham số trên mạng cosFormer và so sánh chúng với cùng phương pháp được áp dụng cho Transformer tiêu chuẩn.
Tsai et al. [2020] tìm kiếm không gian siêu tham số của kiến trúc mô hình BERT [Devlin et al., 2018] một cách không
đồng nhất để tìm một mạng hiệu quả tối ưu. Những thí nghiệm trước đây này không thay đổi hoặc tìm kiếm trên số
lượng lớp khác nhau trong Transformer encoder, đây là một yếu tố chính trong những gì chúng tôi điều tra.

3

--- TRANG 3 ---
3 Thiết lập thí nghiệm
3.1 Tỷ lệ khung hình mô hình
Đối với các mô hình Transformer của chúng tôi, chúng tôi cố định số lượng đặc trưng embedding, đặc trưng chuỗi, đặc
trưng attention và chiều lớp ẩn trong các FFN cho mỗi tác vụ. Chúng tôi thay đổi số lượng lớp, L, và số lượng heads
trên mỗi lớp, H, trong khi giữ L×H không đổi. Bắt đầu với các giá trị điển hình cho L & H, chúng tôi sau đó di
chuyển xuống một lớp duy nhất với một đến hai tỷ lệ khung hình mô hình trung gian, quan sát cách độ chính xác kiểm
tra thay đổi cho các mô hình đã được huấn luyện. Xem Hình 1 để minh họa các mô hình sâu nhất và rộng nhất của
chúng tôi.

Trong tất cả các tác vụ mà chúng tôi đã thử nghiệm, chúng tôi không sử dụng embeddings đã được tiền huấn luyện
hoặc tham số mô hình đã được tiền huấn luyện vì điều này cho phép chúng tôi thực hiện so sánh công bằng hơn.
Việc tính toán embeddings và trọng số đã được tiền huấn luyện được tối ưu hóa cho mỗi kết hợp attention và tỷ lệ
khung hình mô hình sẽ tốn kém về mặt tính toán, và việc sử dụng những cái thường được sử dụng cho các mạng sâu
sẽ tạo ra thiên vị.

3.2 Tập dữ liệu và mô hình
Bảng 1: Các tác vụ và tập dữ liệu khác nhau được sử dụng.
Tên Tác vụ Phân loại Tập dữ liệu Loại Đầu vào Độ dài Đầu vào
IMDb Token Level Binary IMDb Reviews Tokens văn bản đánh giá 500
IMDb Byte Level Binary IMDb Reviews Bytes văn bản đánh giá 1000
Listops 10-way LRA Listops Bytes listop 2000
Document Matching Binary ACL Anthology Bytes tài liệu 4000

Chủ yếu chúng tôi điều tra sử dụng 4 tác vụ phân loại văn bản khác nhau, một tác vụ dựa trên thị giác được điều tra
trong Phần 5.5. Hai tác vụ đầu tiên là phân tích cảm xúc (phân loại nhị phân) trên tập dữ liệu IMDb. Một tác vụ sử
dụng embeddings đầu vào ở cấp token với độ dài chuỗi đầu vào là 500, và tác vụ khác sử dụng embeddings đầu vào
ở cấp byte và độ dài chuỗi đầu vào là 1k. Tác vụ thứ hai này được lấy từ LRA [Tay et al., 2020b], cũng như hai tác
vụ cuối cùng. Tác vụ thứ ba là phân loại Listops 10-way với độ dài chuỗi là 2k. Tác vụ này liên quan đến việc lý luận
về các chuỗi phép toán phân cấp để xác định kết quả, và đầu vào được đưa ra ở cấp byte. Tác vụ cuối cùng được sử
dụng là khớp tài liệu cấp byte, một tác vụ phân loại nhị phân với độ dài chuỗi là 4k. Tác vụ này sử dụng mạng ACL
anthology để khớp bài viết liên quan [Radev et al., 2013]. Chúng tôi tóm tắt mỗi tác vụ trong Bảng 1, chi tiết thêm
về chúng có thể được tìm thấy trong Tay et al. [2020b].

Đối với các tác vụ phân loại văn bản và Listops, chúng tôi thử bốn tỷ lệ khung hình mô hình khác nhau. Về số lượng
lớp và heads trên mỗi lớp, đó là: 6 lớp, 8 heads; 3 lớp, 16 heads; 2 lớp, 24 heads; và cuối cùng 1 lớp, 48 heads. Vì
tác vụ khớp có độ dài chuỗi đầu vào là 4k, các mô hình được sử dụng nhỏ hơn để bù đắp kích thước tính toán liên
quan. Do đó các kết hợp chúng tôi sử dụng là: 4 lớp, 4 heads; 2 lớp, 8 heads; 1 lớp, 16 heads.

Để điều tra xem loại cơ chế attention có ảnh hưởng đến tác động của việc mở rộng lớp attention hay không, chúng
tôi thử nghiệm trên 10 loại attention Transformer khác nhau, bao gồm attention tích vô hướng gốc [Vaswani et al.,
2017]. Những loại khác là: Bigbird [Zaheer et al., 2020], Linear Transformer [Katharopoulos et al., 2020], Linformer
[Wang et al., 2020b], Local attention [Liu et al., 2018], Longformer [Beltagy et al., 2020], Performer [Choromanski
et al., 2020], Sinkhorn [Tay et al., 2020a], Sparse Transformer [Child et al., 2019], và Synthesizer [Tay et al., 2021].
Các triển khai và lựa chọn siêu tham số cho mỗi loại attention giống như được sử dụng trong LRA. Không giống LRA,
chúng tôi không thử nghiệm với Reformer [Kitaev et al., 2020] do nó yêu cầu các đặc trưng chuỗi và đặc trưng attention
phải có cùng chiều. Các siêu tham số huấn luyện và Transformer khác được sử dụng cho mỗi tác vụ được đưa ra trong
Phụ lục A.

4 Kết quả
4.1 Hiệu suất tổng thể
Mỗi kết hợp của tác vụ, tỷ lệ khung hình mô hình và attention được chạy ba lần với trung bình và độ lệch chuẩn được
ghi lại. Trước tiên chúng tôi tóm tắt sự khác biệt về độ chính xác cho các mô hình sâu nhất và rộng nhất trong Bảng
2.¹ Đối với các mô hình Sâu trên các tác vụ phân loại IMDb và Listops, chúng tôi có nghĩa là một Transformer với 6
lớp mỗi lớp có 8 attention heads.

¹Đối với độ lệch chuẩn xem các mục liên quan trong Bảng 3 và 4.

4

--- TRANG 4 ---
Bảng 2: Độ chính xác kiểm tra trên tất cả các tác vụ cho các mô hình sâu nhất và rộng nhất.
Loại Attention IMDb Token IMDb Byte Listops Doc Matching Trung bình
Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng
BigBird 86.0 85.3 62.7 62.4 36.7 37.3 63.9 64.0 62.3 62.3
Linear 86.7 88.0 64.5 64.7 37.1 37.4 64.2 63.9 63.1 63.5
Linformer 84.2 84.1 56.3 52.6 29.9 37.0 64.5 63.1 58.7 59.2
Local 74.2 73.9 55.5 57.0 36.8 37.7 58.0 58.1 56.1 56.7
Longformer 86.0 84.1 61.6 57.5 36.8 37.7 61.2 58.1 61.4 59.4
Performer 87.0 87.0 64.4 64.8 36.2 36.6 65.0 66.3 63.1 63.7
Sinkhorn 86.1 86.5 62.3 61.6 19.4 21.7 64.0 65.7 57.9 58.9
Sparse 85.7 85.7 61.2 62.9 37.0 36.9 63.2 63.6 61.8 62.3
Synthesizer 86.7 86.5 61.4 61.1 36.5 37.3 71.1 72.3 63.9 64.3
Transformer 85.8 85.5 62.6 62.4 35.7 37.2 64.0 64.4 62.0 62.4
Trung bình 84.8 84.7 61.2 60.8 34.2 35.7 63.9 64.0 61.0 61.3

Trên tác vụ khớp tài liệu, điều này được giảm xuống 4 lớp mỗi lớp có 4 attention heads do độ dài chuỗi đầu vào 4k
làm tăng đáng kể gánh nặng tính toán. Đây là những mô hình sâu nhất mà chúng tôi xem xét trong thiết lập của mình.
Những tỷ lệ khung hình mô hình này cũng là thiết lập được sử dụng trong Tay et al. [2020b]. Đối với các mô hình
Rộng, chúng tôi xem xét một mạng một lớp có số heads khớp với tổng số trong các mô hình Sâu. Ví dụ, nếu mô hình
Sâu gốc có tỷ lệ khung hình mô hình 6-8, đối tác rộng hơn của nó là 1-48. Đây là những mô hình rộng nhất trong
thiết lập của chúng tôi.

Bảng 2 cho thấy các mô hình Rộng đạt được độ chính xác +0:3% so với các mô hình Sâu, trung bình trên 10 loại
attention khác nhau và 5 tập dữ liệu khác nhau. Trên hầu hết các tác vụ, hiệu suất của Rộng và Sâu tương tự nhau,
với Listops là ngoại lệ (+1:5%). Kết quả này đúng cho cả Transformer 'vanilla' với attention tích vô hướng, và nói
chung cho nhiều loại attention khác. Sinkhorn thấy mức tăng trung bình lớn nhất khi đi rộng (+1:0%) và trong tất
cả các cơ chế attention chỉ có Longformer trở nên tệ hơn (-2:0%).

4.2 Phân tích hiệu suất cho mỗi tác vụ
Phân tích hiệu suất cho mỗi tác vụ và tỷ lệ khung hình mô hình được đưa ra trong Bảng 3 và 4 với độ lệch chuẩn và
trung bình.

Bảng 3: Độ chính xác kiểm tra trên IMDb cấp token và IMDb cấp byte cho các tỷ lệ khung hình mô hình khác nhau.
Attention IMDb cấp token (lớp-heads) IMDb cấp byte (lớp-heads)
6-8 3-16 2-24 1-48 6-8 3-16 2-24 1-48
BigBird 86.0±0.1 85.7±0.1 85.8±0.1 85.3±0.1 62.7±0.4 63.5±0.1 63.8±0.4 62.4±0.2
Linear 86.7±0.7 86.9±0.3 87.3±0.4 88.0±0.1 64.5±0.2 64.5±0.2 64.6±0.1 64.7±0.1
Linformer 84.2±0.2 82.4±0.1 81.1±1.0 84.1±1.0 56.3±3.6 52.8±0.4 53.0±0.0 52.6±0.1
Local 74.2±0.3 74.1±0.1 74.2±0.2 73.9±0.1 55.5±0.9 57.1±0.1 57.5±0.2 57.0±0.0
Longformer 86.0±0.0 85.8±0.2 85.7±0.1 84.1±0.1 61.6±0.1 61.8±0.2 60.5±0.3 57.5±0.0
Performer 87.0±0.1 87.3±0.3 87.0±0.3 87.0±0.0 64.4±0.1 64.7±0.0 64.7±0.1 64.8±0.1
Sinkhorn 86.1±0.3 86.4±0.2 86.4±0.1 86.5±0.1 62.3±0.0 62.0±0.1 61.9±0.2 61.6±0.1
Sparse 85.7±0.2 85.6±0.4 85.8±0.1 85.7±0.2 61.2±0.1 61.5±0.6 62.1±0.2 62.9±0.2
Synthesizer 86.7±0.2 87.2±0.1 87.1±0.2 86.5±0.2 61.4±0.2 61.2±0.0 61.1±0.1 61.1±0.1
Transformer 85.8±0.8 85.8±0.1 85.9±0.1 85.5±0.1 62.6±0.4 63.4±0.2 63.3±0.2 62.4±0.6
Trung bình 84.8±0.1 84.7±0.1 84.6±0.1 84.7±0.1 61.2±0.4 61.2±0.1 61.3±0.1 60.8±0.1

Đối với phân loại văn bản cấp token IMDb, kết quả (Bảng 3, trái) cho thấy hiệu suất mô hình không thay đổi theo tỷ
lệ khung hình mô hình, với tất cả 4 trung bình trên các cơ chế attention nằm trong phạm vi 0.2%. So sánh các mô hình
rộng nhất và sâu nhất cho mỗi cơ chế attention, chỉ có Longformer có sự tăng độ chính xác >1% khi sâu (+1.9%).
Ngược lại, chỉ có Linear Transformer có sự tăng độ chính xác >1% khi rộng (+1.3%).

Ở cấp byte (Bảng 3, phải) có một bức tranh tương tự, với tất cả 4 trung bình trong phạm vi 0.5%. Tuy nhiên có một
số độ lệch đáng chú ý từ mô hình bất biến tỷ lệ khung hình. Hai mô hình hoạt động tốt hơn >1% trong cấu hình sâu
nhất so với rộng nhất: Linformer (+3.7%) và Longformer (+4.1%). Tương tự hai mô hình hoạt động tốt hơn >1%
khi rộng nhất: Local (+1.5%) và Sparse (+1.7%).

5

--- TRANG 5 ---
Bảng 4: Độ chính xác kiểm tra trên Listops và khớp tài liệu cho các tỷ lệ khung hình mô hình khác nhau.
Attention Listops (lớp-heads) Khớp tài liệu (lớp-heads)
6-8 3-16 2-24 1-48 4-4 2-8 1-16
BigBird 36.7±0.2 37.0±0.7 37.1±0.2 37.3±0.3 63.9±0.5 64.7±0.7 64.0±1.0
Linear 37.1±0.4 37.5±0.4 37.3±0.3 37.4±0.5 64.2±0.5 63.8±0.1 63.9±0.5
Linformer 29.9±8.9 36.9±0.3 36.7±0.1 37.0±0.1 64.5±1.1 62.7±0.4 63.1±0.5
Local 36.8±0.1 37.1±0.1 37.2±0.3 37.7±0.2 58.0±0.2 58.4±0.2 58.1±0.5
Longformer 36.8±0.2 36.9±0.3 36.9±0.2 37.7±0.2 61.2±0.2 60.3±0.4 58.1±0.6
Performer 36.2±0.1 34.1±3.1 32.0±6.4 36.6±0.1 65.0±0.5 65.1±1.2 66.3±0.4
Sinkhorn 19.4±2.1 18.0±0.2 17.9±0.5 21.7±3.8 64.0±0.1 64.1±0.5 65.7±0.6
Sparse 37.0±0.3 37.1±0.1 37.6±0.6 36.9±0.0 63.2±0.2 64.3±0.6 63.6±0.3
Synthesizer 36.5±0.3 36.7±0.2 32.9±5.8 37.3±0.1 71.1±0.5 72.0±1.6 72.3±0.6
Transformer 35.7±1.2 36.4±0.3 36.7±0.1 37.2±0.2 64.0±0.5 64.6±0.3 64.4±0.3
Trung bình 34.2±0.9 34.8±0.3 34.2±0.9 35.7±0.4 63.9±0.2 64.0±0.2 64.0±0.2

Trong tác vụ Listops (Bảng 4, trái) chúng ta thấy một xu hướng mạnh mẽ để các mô hình rộng hơn hoạt động tốt hơn.
Đối với tác vụ này, các mô hình riêng lẻ trong huấn luyện thường sẽ đạt 17-20% độ chính xác và sau đó có thể có
một bước nhảy lên 36-38% độ chính xác khi quá trình huấn luyện tiếp tục. Được minh họa bởi các mục Linformer và
Sinkhorn, các mô hình rộng hơn có nhiều khả năng thực hiện bước nhảy này hơn. Dữ liệu cũng cho chúng ta thấy
rằng ngay cả khi tất cả các lần chạy của một kết hợp attention và tỷ lệ khung hình mô hình thực hiện bước nhảy, các
biến thể rộng hơn vẫn có độ chính xác cao hơn 0.3-1.5%. Tổng thể có sự tăng trung bình về độ chính xác là 1.5%
từ các mô hình sâu nhất đến rộng nhất với nhiều mô hình thấy sự tăng độ chính xác >1%.

Trên tác vụ khớp (Bảng 4, phải), có sự khác biệt nhỏ về hiệu suất giữa các mạng rộng và sâu với ba trung bình nằm
trong phạm vi 0.1% của nhau. Hai mô hình hoạt động tốt hơn >1% khi sâu nhất so với rộng nhất: Linformer (+1.4%)
và Longformer (+3.1%). Ba mô hình hoạt động tốt hơn >1% khi rộng nhất so với sâu nhất: Performer (+1.3%),
Sinkhorn (+1.7%), và Synthesizer (+1.2)%.

Kết quả trong Bảng 3 và 4 chứng minh một số xu hướng chung. Thứ nhất, đối với một số loại attention, tỷ lệ khung
hình mô hình 6-8 gốc không bao giờ cho thấy hiệu suất tối ưu, ví dụ mô hình Transformer gốc. Thứ hai, bản chất của
tác vụ dường như ảnh hưởng đến tác động của việc đi rộng hơn. Listops thấy sự tăng đáng kể so với những tác vụ
khác, có lẽ điều này là do lý luận phân cấp được hưởng lợi từ nhiều attention heads hơn. Tổng thể, chúng tôi muốn
nhấn mạnh những quan sát sau từ kết quả của chúng tôi:

• Các mạng Transformer rộng thường cung cấp độ chính xác bằng hoặc đôi khi lớn hơn trên một loạt các tác
vụ phân loại với độ dài chuỗi khác nhau.
• Sự tăng về độ chính xác phụ thuộc vào tác vụ. Listops có sự tăng độ chính xác trung bình là 1:5% khi đi
rộng, trong khi tất cả những tác vụ khác có thay đổi <0:5%.
• Cơ chế attention có một số ảnh hưởng đến việc liệu rộng hơn hay sâu hơn là tốt hơn với Longformer và
Sinkhorn nhạy cảm hơn với tỷ lệ khung hình mô hình. Chúng tôi cung cấp các giải thích có thể cho những
ngoại lệ này trong Phần 5.4.

5 Thảo luận
5.1 Hiệu suất & Kích thước mô hình
Như đã thấy trong Phần 4, các mạng Transformer rộng thường cung cấp độ chính xác bằng hoặc lớn hơn trên một loạt
các tác vụ phân loại với độ dài chuỗi khác nhau. Tác động của các mạng rộng hơn và nông hơn đối với độ chính xác
bị ảnh hưởng nhẹ bởi loại attention, với một số có những thay đổi đáng kể hơn về độ chính xác so với những loại khác.
Không có sự khác biệt đáng kể giữa thời gian hội tụ trong quá trình huấn luyện cho các mô hình rộng và sâu. Mỗi cơ
chế attention trên mỗi tác vụ đạt được độ chính xác validation tốt nhất sau khoảng cùng số bước huấn luyện cho tất
cả các tỷ lệ khung hình mô hình.

Trong khi tổng số tham số liên quan đến các lớp attention vẫn không đổi giữa các tỷ lệ khung hình khác nhau, tổng
số tham số giảm. Điều này là do phần feed-forward network (FFN) của lớp Transformer vẫn không thay đổi khi chúng
ta thay đổi chiều rộng của mô hình. Các mô hình với ít lớp hơn có ít khối FFN hơn, và do đó ít tham số hơn.

Các mô hình phân loại cấp byte IMDb sâu nhất và Listops thường là 230MiB, với các mô hình rộng nhất thường là
110MiB, chỉ 48% kích thước. Trên phân loại cấp token và khớp tài liệu, các kích thước gần nhau hơn.

6

--- TRANG 6 ---
Trung bình trên tất cả các tác vụ và cơ chế attention, các mô hình rộng nhất có kích thước 71% so với các mô hình
sâu nhất. Một bảng đầy đủ về kích thước mô hình cho các mô hình sâu nhất và rộng nhất trên tất cả các tác vụ và loại
attention được đưa ra trong Bảng 8, trong Phụ lục B.

5.2 Độ trễ
Khi số lượng lớp trong một mô hình giảm, số lượng phụ thuộc trong đồ thị tính toán cũng giảm. Vì điều này, một lượt
chuyển tiếp qua mô hình sẽ có độ trễ thấp hơn, mặc dù thông lượng tổng thể có thể vẫn không thay đổi. Đối với các
hệ thống yêu cầu độ trễ rất thấp, chẳng hạn như xử lý thời gian thực trong lái xe tự động [Talpes et al., 2020], tính
năng này có thể làm cho mô hình rộng một lớp trở nên mong muốn hơn so với mô hình sâu có độ chính xác tương đương.

Chúng tôi đo độ trễ suy luận cho các mô hình sâu nhất và rộng nhất cho mỗi tác vụ và loại attention trên CPU với
đầu vào đơn lẻ, và trên GPU với đầu vào theo batch. Chi tiết thí nghiệm được đưa ra trong Phụ lục C cùng với các
số liệu thô trong Bảng 9 và 10. Chúng tôi thấy rằng trung bình các mô hình rộng nhất nhanh hơn 3:1 lần trên CPU
và 1:9 lần trên GPU so với các mô hình sâu nhất. Việc tăng tốc nhất quán trên tất cả các tác vụ và loại attention.

5.3 Khả năng giải thích
(a) Dự đoán: rất tiêu cực
 (b) Dự đoán: hơi tích cực

Hình 2: Trọng số attention trên một số heads được chọn với phân loại dự đoán cho mô hình Transformer phân loại
văn bản cấp token IMDb một lớp trên hai ví dụ chưa thấy và tương tự.

Khả năng giải thích ngày càng quan trọng và là một lĩnh vực nghiên cứu rất tích cực của học máy [Gilpin et al., 2018,
Linardatos et al., 2020], đặc biệt khi nói đến công bằng. Bằng cách có các mô hình dễ kiểm tra hơn, chúng ta có thể
thấy lý do cho một phân loại nhất định. Ví dụ, một quyết định có dựa trên việc đề cập đến một đặc điểm được bảo vệ,
chẳng hạn như chủng tộc hoặc giới tính không?

Trong kiến trúc dựa trên Transformer, các attention heads trong một lớp có thể được kiểm tra trong quá trình suy
luận để xem head đó tìm thấy những kết nối nào giữa các đặc trưng đầu vào quan trọng. Đối với các mạng sâu, nhiều
lớp có nghĩa là thường có thể trở nên không rõ ràng đầu ra cuối cùng thực sự dựa trên điều gì [Rigotti et al., 2021].
Đối với mạng rộng một lớp, khả năng giải thích dễ dàng hơn nhiều vì chỉ cần kiểm tra một lớp. Do đó những gì được
coi là quan trọng cho đầu ra cuối cùng rõ ràng hơn nhiều.

Trong Hình 2, chúng ta có thể thấy trọng số attention trên một số heads của mô hình Transformer phân loại văn bản
cấp token rộng nhất, cũng như lớp dự đoán cho hai đầu vào ví dụ khác nhau đã được thiết kế để tương tự. Hình 2
bao gồm một tập con của tổng số heads, để xem tất cả 48 heads xem Hình 3 trong Phụ lục D.

Chúng ta có thể thấy đánh giá ở bên trái đã được gán một cách tự tin là đánh giá tiêu cực, và từ trọng số attention
chúng ta có thể thấy điều này là do mô hình nhận ra tính liên quan của từ "terrible". Đánh giá ở bên phải đã được gán
ít tự tin hơn là tích cực. Từ trọng số attention chúng ta có thể thấy nó đã nhận ra các từ như "terrible", nhưng trọng
số lớn nhất là trên "The". Điều này giải thích tại sao mô hình có thể không tự tin trong dự đoán tích cực của nó bởi
vì nó chưa nhận ra tầm quan trọng của từ "loved".

5.4 Giải thích lý thuyết về các ngoại lệ
Hầu hết các cơ chế attention thường có sự tăng độ chính xác lên đến 0.5% khi đi rộng với hai ngoại lệ đáng chú ý.
Longformer [Beltagy et al., 2020] thường hoạt động tốt hơn đáng kể khi sâu, và Sinkhorn [Tay et al., 2020a] thường
hoạt động tốt hơn đáng kể khi rộng.

Đối với Longformer, chúng tôi chỉ sử dụng sliding window attention, với chiều rộng 512. Điều này có nghĩa, đặc biệt
đối với các tác vụ dài hơn, mỗi đặc trưng đầu vào chỉ có thể có attention được tính toán giữa nó và các hàng xóm của
nó. Đối với các mô hình sâu hơn, các đặc trưng có thể lan truyền và do đó hạn chế này được giảm bớt. Tuy nhiên, các
mô hình một lớp chịu phạt hiệu suất.

Sinkhorn hoạt động tương tự như local attention, trong đó chuỗi đầu vào được chia thành các khối. Không giống như
local attention, tính toán attention trong các khối này, Sinkhorn sắp xếp chúng và tính toán attention giữa khối gốc
và khối đã được sắp xếp mới. Cơ chế sắp xếp này có thể học được cho mỗi head, do đó mỗi head có thể học một chiến
lược sắp xếp khác nhau. Đối với nhiều heads, điều này tối đa hóa cơ hội tổng thể của các kết nối tầm xa quan trọng
trong chuỗi đầu vào được chú ý đến.

5.5 Vision Transformer
Có sự quan tâm ngày càng tăng trong việc áp dụng Transformers cho thị giác máy tính [Dosovitskiy et al., 2021, Liu
et al., 2021, Wang et al., 2021, 2022]. Chúng tôi đã thử nghiệm mô hình Pyramid Vision Transformer (PVT-V2-B1)
[Wang et al., 2022] và các biến thể rộng hơn của nó trên tập dữ liệu CIFAR10 [Krizhevsky et al., 2014]. Mô hình
PVT-V2-B1 có bốn giai đoạn với mỗi giai đoạn chứa hai lớp attention. Sau đó chúng tôi thay thế hai lớp attention ở
mỗi giai đoạn bằng một lớp attention rộng duy nhất. Kiến trúc chi tiết của mô hình PVT-V2-B1 và các biến thể rộng
hơn được hiển thị trong Bảng 5. Biến thể rộng đầu tiên (Wide) khớp tổng số heads với mô hình PVT-V2-B1 gốc, trong
khi biến thể sau (Wide-V2) chứa kích thước embedding lớn hơn và nhiều heads hơn. Đây là sự khớp gần hơn với mô
hình gốc về kích thước mô hình.

Bảng 5: Hiệu suất của Pyramid Vision Transformer gốc (PVT) và các lựa chọn thay thế rộng hơn trên tập dữ liệu
CIFAR10. Mô hình gốc (PVTV2-B1) có 4 giai đoạn, mỗi giai đoạn chứa hai lớp attention. ((1, 1), (2, 2), (5, 5),
(8, 8)) mô tả mô hình gốc, ví dụ, khối đầu tiên chứa hai lớp với một head mỗi lớp, được biểu diễn là (1, 1).
Tên Cấu hình Độ chính xác Tham số
Baseline ((1;1);(2;2);(5;5);(8;8)) 95:59±0:99 13:5M
Wide ((2);(4);(10);(16)) 94:54±0:31 7:7M
Wide-V2 ((4);(8);(20);(32)) 94:94±0:20 12:6M

Bảng 5 minh họa rằng các biến thể rộng hơn không vượt trội hơn mô hình PVT-V2-B1 gốc. Một cách trực quan, các
đặc trưng không gian đóng vai trò quan trọng trong các tác vụ thị giác. Lớp average pooling đến trước mỗi lớp attention
là một thành phần quan trọng của mô hình PVT. Với chỉ một lớp rộng duy nhất, lớp pooling này không nắm bắt nhiều
thông tin không gian như trước. Việc sử dụng một lớp attention rộng duy nhất trong vision Transformers bị hạn chế
bởi thực tế là phần lớn các vision Transformers này vẫn sử dụng các lớp pooling hoặc convolution trước attention.

5.6 Mixed Attention
Bảng 6: Độ chính xác kiểm tra cho mỗi tác vụ cho các mô hình mixed attention rộng và sâu cùng với trung bình của
tất cả các mô hình đồng nhất, và mô hình đồng nhất hoạt động tốt nhất cho tác vụ đó.
Tác vụ Sâu nhất Rộng nhất
Hom. Avg Hom. Best Mixed Hom. Avg Hom. Best Mixed
IMDb Token Level 84.8 87.0 87.3 84.7 88.0 86.8
IMDb Byte Level 61.3 64.5 60.1 60.7 64.8 63.0
Listops 34.2 37.1 37.1 35.7 37.7 38.0
Document Matching 63.9 71.1 - 64.0 72.3 66.9

Với các lớp attention rộng hơn, các ưu điểm của việc trộn các phương pháp attention trở nên khả thi hơn. Chúng tôi
thử nghiệm một hỗn hợp đồng đều của attentions trong cả biến thể rộng và sâu để xem các mô hình này hoạt động
như thế nào. Đối với mỗi lớp, chúng tôi sử dụng một hỗn hợp bằng nhau của các cơ chế attention sau: BigBird, Linear
Transformer, Linformer, Local, Longformer, Performer, Sparse Transformer, và Synthesizer.

Chúng tôi bỏ qua Sinkhorn vì nó không sử dụng token [CLS] để phân loại như các phương pháp attention khác. Chúng
tôi cũng bỏ qua vanilla attention để có tổng cộng 8 cơ chế, và hoạt động attention tổng thể của chúng tôi hiệu quả
(độ phức tạp thời gian và không gian dưới bậc hai).

Chúng tôi khởi tạo một khối multiheaded attention riêng biệt cho mỗi cơ chế và lấy trung bình tất cả đầu ra của chúng
khi quay trở lại các đặc trưng chuỗi. Số lượng attention heads trong mỗi khối được mở rộng sao cho tổng số bằng với
mô hình đồng nhất. Ví dụ trong cấu hình 6 lớp, 8 heads, mỗi khối attention của chúng tôi có một head duy nhất.
Trong cấu hình 1 lớp 48 heads, mỗi khối có 6 heads.

8

--- TRANG 7 ---
Kết quả được đưa ra trong Bảng 6. Mixed attention sâu trên khớp không được thử nghiệm vì không có đủ heads để
bao gồm mọi cơ chế attention. Chúng tôi cũng bao gồm trong bảng này các lặp lại của trung bình và tốt nhất cho mỗi
tác vụ trên tất cả các cơ chế attention. Từ bảng chúng ta có thể thấy rằng ngay cả với mixed attention, các mô hình
một lớp rộng nhất thường hoạt động tốt hơn (IMDb byte level, Listops) hoặc chỉ kém một chút (IMDb token level)
so với các mô hình Transformer sâu. Trong khi đánh bại trung bình, tất cả các mô hình mixed tốt nhất ngoại trừ Listops
đều bị vượt trội bởi một trong các mô hình attention đồng nhất trong cấu hình rộng nhất của nó. Tuy nhiên, mô hình
mixed rộng nhất trên Listops chỉ ra rằng mixed attention có thể có lợi thế so với attention đồng nhất cho một số tác vụ
nhất định.

6 Kết luận
Khi cố định kích thước của tính toán attention, nói chung các Transformers rộng hơn đôi khi có thể vượt trội, hoặc
ngược lại khớp, với các đối tác sâu hơn của chúng trên các tác vụ dựa trên NLP. Trung bình trên tất cả các tác vụ và
cơ chế attention, các mô hình Rộng đạt được độ chính xác +0:3% so với các mô hình Sâu. Trên hầu hết các tác vụ,
hiệu suất của Rộng và Sâu tương tự nhau, với Listops là ngoại lệ (+1:5%). Trung bình việc đi rộng hoạt động tốt cho
hầu như tất cả các cơ chế attention. Sinkhorn được hưởng lợi nhiều nhất từ việc rộng, trong khi Longformer mất hiệu
suất.

Hơn nữa, các mạng rộng hơn nhỏ hơn nhiều, chỉ 71% kích thước của các mô hình sâu nhất trung bình. Chúng cũng
có các ưu điểm bổ sung về khả năng giải thích và độ trễ suy luận: nhanh hơn 3:1 lần trên CPU và 1:9 lần trên GPU
cho phân loại văn bản cấp byte IMDb. Tuy nhiên, trên các tác vụ dựa trên hình ảnh, các Transformers sâu hơn vẫn
vượt trội do các lớp pooling có thể nắm bắt nhiều thông tin không gian hơn.

Trong khi nghiên cứu của chúng tôi xem xét các tác động của chiều rộng attention trên nhiều cơ chế attention và các
tác vụ khác nhau, vẫn còn phải xem liệu những kết quả này có đúng cho các mô hình lớn hơn nhiều trên các tập dữ
liệu lớn hơn nhiều hay không, ví dụ mô hình hóa ngôn ngữ có mặt nạ. Do đó chúng tôi đề xuất các mô hình rộng hơn
và nông hơn như một giải pháp thay thế khả thi và mong muốn cho các mô hình nhỏ trên các tác vụ NLP, và như một
lĩnh vực nghiên cứu quan trọng cho các miền vượt ra ngoài điều này.

7 Tuyên bố về khả năng tái tạo
Phần 3.1 cùng với Hình 1 cho thấy cách chúng tôi thay đổi kiến trúc Transformer tiêu chuẩn. Các tác vụ cụ thể mà
chúng tôi huấn luyện Transformers được đưa ra trong Phần 3.2 với chi tiết về các siêu tham số được sử dụng để
huấn luyện trong Phụ lục A. Phần thân chính của mã của chúng tôi có thể được tìm thấy tại GIT-LINK.

Tài liệu tham khảo
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150, 2020.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.
arXiv preprint arXiv:1904.10509, 2019.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint
arXiv:2009.14794, 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. 2021. URL https://openreview.net/forum?id=YicbFdNTTy.

Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. Explaining explana-
tions: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data
science and advanced analytics (DSAA), pp. 80–89. IEEE, 2018.

Yong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Jian Chen, Peilin Zhao, and Junzhou Huang. Nat: Neural architecture
transformer for accurate and compact architectures. Advances in Neural Information Processing Systems, 32, 2019.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.

Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu, and Changliang Li. Ranknas:
Efficient neural architecture search by pairwise ranking. arXiv preprint arXiv:2109.07383, 2021.

9

--- TRANG 8 ---
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autore-
gressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165.
PMLR, 2020.

Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint
arXiv:2001.04451, 2020.

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www. cs. toronto. edu/kriz/cifar.
html, 55(5), 2014.

Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai: A review of machine learning
interpretability methods. Entropy, 23(1):18, 2020.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Gener-
ating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 10012–10022, 2021.

Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural architecture search
on efficient transformers and beyond. arXiv preprint arXiv:2207.13955, 2022.

Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran
Zhong. cosformer: Rethinking softmax in attention. arXiv preprint arXiv:2202.08791, 2022.

Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network
corpus. Language Resources and Evaluation, 47(4):919–944, 2013.

Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas Gschwind, and Paolo Scotton. Attention-based inter-
pretability with concept transformers. In International Conference on Learning Representations, 2021.

David So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference on Machine Learning,
pp. 5877–5886. PMLR, 2019.

Emil Talpes, Debjit Das Sarma, Ganesh Venkataramanan, Peter Bannon, Bill McGee, Benjamin Floering, Ankit Jalote,
Christopher Hsiong, Sahil Arora, Atchyuth Gorti, et al. Compute solution for tesla's full self-driving computer.
IEEE Micro, 40(2):25–35, 2020.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International
Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebas-
tian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint
arXiv:2011.04006, 2020b.

Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention
for transformer models. In International conference on machine learning, pp. 10183–10192. PMLR, 2021.

Henry Tsai, Jayden Ooi, Chun-Sung Ferng, Hyung Won Chung, and Jason Riesa. Finding fast transformers: One-shot
neural architecture search by component composition. arXiv preprint arXiv:2008.06808, 2020.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat: Hardware-aware
transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187, 2020a.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768, 2020b.

Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 568–578, 2021.

Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt
v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415–424, 2022.

Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel. Wider or deeper: Revisiting the resnet model for visual
recognition. Pattern Recognition, 90:119–133, 2019.

Fuzhao Xue, Jianghai Chen, Aixin Sun, Xiaozhe Ren, Zangwei Zheng, Xiaoxin He, Xin Jiang, and Yang You. Deeper
vs wider: A revisit of transformer configuration. arXiv preprint arXiv:2205.10505, 2022a.

10

--- TRANG 9 ---
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead of deeper. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8779–8787, 2022b.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip
Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in
Neural Information Processing Systems, 33:17283–17297, 2020.

11

--- TRANG 10 ---
A Chi tiết tác vụ thí nghiệm
Bảng 7: Siêu tham số cụ thể cho tác vụ, xem phần giới thiệu để biết định nghĩa.
Tác vụ Bước huấn luyện Warmup Độ dài chuỗi E A M
IMDb Token Level 30k 8k 1k 512 64 2048
IMDb Byte Level 30k 8k 1k 512 64 2048
Listops 10k 1k 2k 512 64 2048
Document Matching 10k 8k 4k 128 32 512

Trong tất cả các tác vụ mà chúng tôi thử nghiệm, chúng tôi đã sử dụng bộ tối ưu hóa Adam với linear warmup và
square root decay. Chúng tôi sử dụng tốc độ học cơ bản là 0.05, β₁ = 0:9, và β₂ = 0:98 và kích thước batch là 32
cho mọi tác vụ. Attention kernel và trọng số MLP được khởi tạo thông qua uniform xavier. Biases, input embeddings,
và positional embeddings được khởi tạo thông qua phân phối Gaussian với phương sai lần lượt là 10⁻⁶, 1, và 0:02.
Tất cả các cơ chế attention, ngoại trừ Sinkhorn, đều sử dụng token [CLS] để phân loại. Sinkhorn sử dụng mean
classifier pooling. Các siêu tham số cụ thể cho tác vụ được đưa ra trong bảng 7. Trong tất cả các mô hình Transformer,
feed-forward network (FFN) có một chiều ẩn duy nhất. Trước khi thử nghiệm, chúng tôi lấy mô hình có độ chính
xác validation tốt nhất trong quá trình huấn luyện, thời gian huấn luyện dài là để đảm bảo hội tụ.

B Kích thước mô hình
Bảng 8: Kích thước mô hình cho tất cả các tác vụ và attentions cho các mô hình sâu nhất và rộng nhất (MiB).
Loại Attention IMDb Token IMDb Byte Listops Doc Matching Trung bình
Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng
BigBird 779 659 230 110 229 109 13 8.0 313 222
Linear 779 659 230 110 229 109 13 8.0 313 222
Linformer 780 659 232 110 231 109 16 8.7 315 222
Local 779 659 230 110 229 109 13 8.0 313 222
Longformer 833 713 284 164 283 163 15 11 354 263
Performer 779 659 230 110 229 109 13 8.0 313 222
Sinkhorn 767 647 218 98 217 97 13 8.0 304 213
Sparse 779 659 230 110 229 109 13 8.0 313 222
Synthesizer 761 641 230 109 300 179 60 55 338 246
Transformer 779 659 230 110 229 109 13 8.0 313 222
Trung bình 782 661 234 114 241 120 18 13.1 319 227

Bảng 8 cho thấy kích thước của các mô hình sâu nhất và rộng nhất cho mọi kết hợp tác vụ và attention tính bằng
mebibytes.

C Độ trễ
Để thử nghiệm độ trễ, chúng tôi đưa đầu vào ngẫu nhiên có độ dài đầy đủ vào mô hình 100 lần và lấy trung bình
thời gian thực hiện. Thời gian độ trễ cho đầu vào đơn lẻ trên CPU được đưa ra trong Bảng 9, phần cứng được sử
dụng là 2 AMD EPYC 7763 64-Core Processor 1.8GHz. Thời gian GPU được đưa ra trong Bảng 10, cho việc này
chúng tôi đưa vào một batch 32 đầu vào. Chúng tôi sử dụng một GPU Nvidia A100-SXM-80GB duy nhất.

D Điểm attention đầy đủ
Hình 3 cho thấy điểm attention cho mọi head cho ví dụ của chúng tôi trong Phần 5.3.

12

--- TRANG 11 ---
Bảng 9: Độ trễ cho tất cả các tác vụ và attentions cho các mô hình sâu nhất và rộng nhất trên CPU (ms).
Loại Attention IMDb Token IMDb Byte Listops Doc Matching Trung bình
Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng
BigBird 116 46 106 24 103 30 125 33 113 33
Linear 54 33 33 11 33 11 26 10 37 16
Linformer 60 34 39 12 39 12 34 12 43 18
Local 59 34 34 11 33 12 27 11 38 17
Longformer 70 37 62 17 149 37 939 263 305 89
Performer 53 33 32 11 55 35 28 9 42 22
Sinkhorn 83 39 59 16 59 17 58 21 65 23
Sparse 56 40 38 14 47 27 88 66 57 37
Synthesizer 257 237 866 11 130 20 45 23 324 73
Transformer 60 34 40 13 41 16 56 36 49 25
Trung bình 87 57 131 14 69 22 140 48 107 35

Bảng 10: Độ trễ cho tất cả các tác vụ và attentions cho các mô hình sâu nhất và rộng nhất trên GPU (ms). Trung bình
trên attention bỏ qua Synthesizer do kết quả cực đoan của nó.
Loại Attention IMDb Token IMDb Byte Listops Doc Matching Trung bình
Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng Sâu Rộng
BigBird 154 63 154 59 210 128 232 115 188 91
Linear 52 21 63 37 76 32 52 22 61 28
Linformer 58 21 73 38 81 32 58 23 68 29
Local 58 26 68 41 86 40 63 34 69 35
Longformer 115 57 83 48 340 189 1202 540 435 209
Performer 47 17 60 35 95 51 49 19 63 31
Sinkhorn 92 33 112 50 121 49 106 52 108 46
Sparse 92 55 76 46 229 175 351 346 187 156
Synthesizer 25164 34 5927 6001 325 199 338 293 7939 1632
Transformer 88 50 75 47 208 170 289 289 165 139
Trung bình 76 34 76 40 145 87 240 144 144 76

E Nghiên cứu ablation mô hình nhỏ
Để xác minh tính hợp lệ của việc đi rộng và đi nông, chúng tôi chạy các mô hình Transformer attention vanilla 'nhỏ'
trên tất cả 4 tác vụ. Những mô hình này sử dụng cùng số lượng heads trên mỗi lớp như các mô hình sâu nhất nhưng
chỉ có 1 lớp. Do đó 8 heads trên cả hai tác vụ IMDb và Listops, và 4 heads trên tác vụ khớp tài liệu. Những kết quả
này được đưa ra trong bảng 11 cùng với các lặp lại của trung bình từ các mô hình Transformer sâu nhất và rộng nhất.
Như chúng ta có thể thấy, mô hình nhỏ luôn hoạt động tệ hơn ít nhất mô hình rộng nhất, mặc dù thật đáng ngạc nhiên
khi đánh bại mô hình sâu trên tác vụ Listops. Trên các tác vụ IMDb và khớp tài liệu, nó nhất quán hoạt động tệ hơn
1.5%-3% so với những mô hình khác, như mong đợi.

Bảng 11: Độ chính xác kiểm tra cho mỗi tác vụ cho mô hình Transformer vanilla nhỏ so với các đối tác rộng hoàn
toàn và sâu hoàn toàn của nó.
Tác vụ Sâu nhất Rộng nhất Nhỏ
IMDb Token Level 85.8 85.5 84.0
IMDb Byte Level 62.6 62.4 59.6
Listops 35.7 37.2 36.9
Document Matching 64.0 64.4 62.4

13

--- TRANG 12 ---
(a) Dự đoán: rất tiêu cực
 (b) Dự đoán: hơi tích cực

Hình 3: Trọng số attention trên tất cả 48 heads với phân loại dự đoán cho mô hình Transformer phân loại văn bản
cấp token IMDb một lớp trên hai ví dụ chưa thấy và tương tự.

14
