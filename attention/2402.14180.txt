# 2402.14180.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2402.14180.pdf
# File size: 1247762 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Linear Transformers are Versatile In-Context
Learners
Max Vladymyrov
Google Research
mxv@google.comJohannes von Oswald
Google, Paradigms of Intelligence Team
jvoswald@google.com
Mark Sandler
Google Research
sandler@google.comRong Ge
Duke University
rongge@cs.duke.edu
Abstract
Recent research has demonstrated that transformers, particularly linear attention
models, implicitly execute gradient-descent-like algorithms on data provided in-
context during their forward inference step. However, their capability in handling
more complex problems remains unexplored. In this paper, we prove that each layer
of a linear transformer maintains a weight vector for an implicit linear regression
problem and can be interpreted as performing a variant of preconditioned gradient
descent. We also investigate the use of linear transformers in a challenging scenario
where the training data is corrupted with different levels of noise. Remarkably,
we demonstrate that for this problem linear transformers discover an intricate and
highly effective optimization algorithm, surpassing or matching in performance
many reasonable baselines. We analyze this algorithm and show that it is a novel
approach incorporating momentum and adaptive rescaling based on noise levels.
Our findings show that even linear transformers possess the surprising ability to
discover sophisticated optimization strategies.
1 Introduction
The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning,
driving breakthroughs across various domains and serving as a foundation for powerful models (Anil
et al., 2023; Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023). However, despite their
widespread success, the mechanisms that drive their performance remain an active area of research.
A key component of their success is attributed to in-context learning (ICL, Brown et al., 2020) – an
emergent ability of transformers to make predictions based on information provided within the input
sequence itself, without explicit parameter updates.
Recently, several papers (Garg et al., 2022; Akyürek et al., 2022; von Oswald et al., 2023a) have
suggested that ICL might be partially explained by an implicit meta-optimization of the transformers
that happens on input context (aka mesa-optimization Hubinger et al., 2019). They have shown that
transformers with linear self-attention layers (aka linear transformers) trained on linear regression
tasks can internally implement gradient-based optimization.
Specifically, von Oswald et al. (2023a) demonstrated that linear transformers can execute iterations of
an algorithm similar to the gradient descent algorithm (which they call GD++), with each attention
layer representing one step of the algorithm. Later, Ahn et al. (2023); Zhang et al. (2023) further
characterized this behavior, showing that the learned solution is a form of preconditioned GD, and
this solution is optimal for one-layer linear transformers.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2402.14180v2  [cs.LG]  30 Oct 2024

--- PAGE 2 ---
In this paper, we continue to study linear transformers trained on linear regression problems. We
prove that each layer of every linear transformer maintains a weight vector for an underlying linear
regression problem. Under some restrictions, the algorithm it runs can be interpreted as a complex
variant of preconditioned gradient descent with momentum-like behaviors.
While maintaining a linear regression model (regardless of the data) might seem restrictive, we show
that linear transformers can discover powerful optimization algorithms. As a first example, we prove
that in case of GD++, the preconditioner results in a second order optimization algorithm.
Furthermore, we demonstrate that linear transformers can be trained to uncover even more powerful
and intricate algorithms. We modified the problem formulation to consider mixed linear regression
with varying noise levels1(inspired by Bai et al., 2023). This is a harder and non-trivial problem with
no obvious closed-form solution, since it needs to account for various levels of noise in the input.
Our experiments with two different noise variance distributions (uniform and categorical) demonstrate
the remarkable flexibility of linear transformers. Training a linear transformer in these settings leads
to an algorithm that outperforms GD++as well as various baselines derived from the exact closed-
form solution of the ridge regression. We discover that this result holds even when training a linear
transformer with diagonal weight matrices.
Through a detailed analysis, we reveal key distinctions from GD++, including momentum-like term
and adaptive rescaling based on the noise levels.
Our findings contribute to the growing body of research where novel, high-performing algorithms have
been directly discovered through the reverse-engineering of transformer weights. This work expands
our understanding of the implicit learning capabilities of attention-based models and highlights the
remarkable versatility of even simple linear transformers as in-context learners. We demonstrate that
transformers have the potential to discover effective algorithms that may advance the state-of-the-art
in optimization and machine learning in general.
2 Preliminaries
In this section we introduce notations for linear transformers, data, and type of problems we consider.
2.1 Linear transformers and in-context learning
Given input sequence e1, e2, ..., e n∈Rd+1, a single head in a linear self-attention layer is usually
parameterized by four matrices, key WK, query WQ, value WVand projection WP. The output of
the non-causal layer at position iisei+ ∆eiwhere ∆eiis computed as
∆ei=WPPn
j=1⟨WQei, WKej⟩WVej
. (1)
Equivalently, one can use parameters P=WPWVandQ=W⊤
KWQ, and the equation becomes
∆ei=Pn
j=1(e⊤
jQei)Pej. (2)
Multiple heads (P1, Q1),(P2, Q2), ...,(Ph, Qh)simply sum their effects
∆ei=PH
k=1Pn
j=1(e⊤
jQkei)Pkej. (3)
We define a linear transformer as a multi-layer neural network composed of Llinear self-attention
layers parameterized by θ={Ql
k, Pl
k}fork= 1. . . H, l = 1. . . L . To isolate the core mechanisms,
we consider a simplified decoder-only architecture, excluding MLPs and LayerNorm components.
This architecture was also used in previous work (von Oswald et al., 2023a; Ahn et al., 2023).
We consider two versions of linear transformers: FULL with the transformer parameters represented
by full matrices and D IAG, where the parameters are restricted to diagonal matrices only.
Inspired by von Oswald et al. (2023a), in this paper we consider regression data as the token sequence.
Each token ei= (xi, yi)∈Rd+1consists of a feature vector xi∈Rdand its corresponding output
1We consider a model where each sequence contains data with the same noise level, while different sequences
have different noise levels.
2

--- PAGE 3 ---
yi∈R. Additionally, we append a query token en+1= (xt,0)to the sequence, where xt∈Rd
represents test data. The goal of in-context learning is to predict ytfor the test data xt. We constrain
the attention to only focus on the first ntokens of the sequence so that it ignores the query token.
We use (xl
i, yl
i)to denote the i-th token in the transformer’s output at layer l. The initial layer is simply
the input: (x0
i, y0
i) = (xi, yi). For a model with parameters θ, we read out the prediction by taking the
negative2of the last coordinate of the final token in the last layer as ˆyθ({e1, ..., e n}, en+1) =−yL
n+1.
Let’s also define the following notation to be used throughout the paper
Σ =nX
i=1xi(xi)⊤;α=nX
i=1yixi; λ=nX
i=1(yi)2
Σl=nX
i=1xl
i(xl
i)⊤;αl=nX
i=1yl
ixl
i; λl=nX
i=1(yl
i)2
2.2 Noisy regression model
As a model problem, we consider data generated from a noisy linear regression model. For each input
sequence τ, we sample a ground-truth weight vector wτ∼N(0, I), and generate ndata points as
xi∼N(0, I)andyi=⟨wτ, xi⟩+ξi, with noise ξi∼N(0, σ2
τ).
Note that each sequence can have different ground-truth weight vectors wτ, but every data point in
the sequence shares the same wτandστ. The query is generated as xt∼N(0, I)andyt=⟨wτ, xt⟩
(since the noise is independent, whether we include noise in yqwill only be an additive constant to
the final objective).
We further define an ordinary least square (OLS) loss as
LOLS(w) =Pn
i=1(yi− ⟨w, xi⟩)2. (4)
The OLS solution is w∗:= Σ−1αwith residuals ri:=yi− ⟨w∗, xi⟩.
In the presence of noise στ,w∗in general is not equal to the ground truth wτ. For a known noise
levelστ, the best estimator for wτis provided by ridge regression:
LRR(w) =Pn
i=1(yi− ⟨w, xi⟩)2+σ2
τ∥w∥2, (5)
with solution w∗
σ2:= 
Σ +σ2
τI−1α. Of course, in reality the variance of the noise is not known
and has to be estimated from the data.
2.3 Fixed vs. mixed noise variance problems
We consider two different problems within the noisy linear regression framework.
Fixed noise variance. In this scenario, the variance στremains constant for all the training data.
Here, the in-context loss is:
L(θ) = E
wτ∼N(0,I)
xi∼N(0,I)
ξi∼N(0,σ2
τ)
(ˆyθ({e1, ..., e n}, en+1)−yt)2
, (6)
where ei= (xi, yi)andyi=⟨wτ, xi⟩+ξi. This problem was initially explored by Garg et al. (2022).
Later, von Oswald et al. (2023a) have demonstrated that a linear transformer (6)converges to a form
of a gradient descent solution, which they called GD++. We define this in details later.
Mixed noise variance. In this case, the noise variance στis drawn from some fixed distribution
p(στ)for each sequence. The in-context learning loss becomes:
L(θ) = E
wτ∼N(0,I)
xi∼N(0,I)
ξi∼N(0,σ2
τ)
στ∼p(στ)
(ˆyθ({e1, ..., e n}, en+1)−yt)2
. (7)
2We set the actual prediction to −yl
n+1, similar to von Oswald et al. (2023a), because it’s easier for linear
transformers to predict −yt.
3

--- PAGE 4 ---
In other words, each training sequence τhas a fixed noise level στ, but different training sequences
have different noise levels sampled from a specified distribution p(στ). This scenario adds complexity
because the model must predict wτfor changing noise distribution, and the optimal solution likely
would involve some sort of noise estimation. We have found that empirically, GD++fails to model
this noise variance and instead converges to a solution which can be interpreted as a single noise
variance estimate across all input data.
3 Related work
In-context Learning as Gradient Descent Our work builds on research that frames in-context
learning as (variants of) gradient descent (Akyürek et al., 2022; von Oswald et al., 2023a). For 1-layer
linear transformer, several works Zhang et al. (2023); Mahankali et al. (2023); Ahn et al. (2023)
characterized the optimal parameters and training dynamics. More recent works extended the ideas
to auto-regressive models (Li et al., 2023; von Oswald et al., 2023b) and nonlinear models (Cheng
et al., 2023). Fu et al. (2023) noticed that transformers perform similarly to second-order Newton
methods on linear data, for which we give a plausible explanation in Theorem 5.1.
In-context Learning in LLMs There are also many works that study how in-context learning works
in pre-trained LLMs (Kossen et al., 2023; Wei et al., 2023; Hendel et al., 2023; Shen et al., 2023).
Due to the complexity of such models, the exact mechanism for in-context learning is still a major
open problem. Several works (Olsson et al., 2022; Chan et al., 2022; Akyürek et al., 2024) identified
induction heads as a crucial mechanism for simple in-context learning tasks, such as copying, token
translation and pattern matching.
Other theories for training transformers Other than the setting of linear models, several other
works (Garg et al., 2022; Tarzanagh et al., 2023; Li et al., 2023; Huang et al., 2023; Tian et al.,
2023a,b) considered optimization of transformers under different data and model assumptions. Wen
et al. (2023) showed that it can be difficult to interpret the “algorithm” performed by transformers
without very strong restrictions.
Mixed Linear Models Several works observed that transformers can achieve good performance on
a mixture of linear models (Bai et al., 2023; Pathak et al., 2023; Yadlowsky et al., 2023). While these
works show that transformers can implement many variants of model-selection techniques, our result
shows that linear transformers solve such problems by discovering interesting optimization algorithm
with many hyperparameters tuned during the training process. Such a strategy is quite different from
traditional ways of doing model selection. Transformers are also known to be able to implement
strong algorithms in many different setups (Guo et al., 2023; Giannou et al., 2023).
Effectiveness of linear and kernel-like transformers A main constraint on transformer archi-
tecture is that it takes O(N2)time for a sequence of length N, while for a linear transformer this
can be improved to O(N). Mirchandani et al. (2023) showed that even linear transformers are quite
powerful for many tasks. Other works (Katharopoulos et al., 2020; Wang et al., 2020; Schlag et al.,
2021; Choromanski et al., 2020) uses ideas similar to kernel/random features to improve the running
time to almost linear while not losing much performance.
4 Linear transformers maintain linear regression model at every layer
While large, nonlinear transformers can model complex relationship, we show that linear transformers
are restricted to maintaining a linear regression model based on the input, in the sense that the l-th
layer output is always a linear function of the input with latent (and possibly nonlinear) coefficients.
4

--- PAGE 5 ---
Theorem 4.1. Suppose the output of a linear transformer at l-th layer is
(xl
1, yl
1),(xl
2, yl
2), ...,(xl
n, yl
n),(xl
t, yl
t), then there exists matrices Ml, vectors ul, wland scalars al
such that
xl+1
i=Mlxi+yiul, xl+1
t=Mlxt,
yl+1
i=alyi− ⟨wl, xi⟩, yl+1
t=−⟨wl, xt⟩.
Note that Ml,ul, wlandalare not linear in the input, but this still poses restrictions on what the
linear transformers can do. For example we show that it cannot represent a quadratic function:
Theorem 4.2. Suppose the input to a linear transformer is (x1, y1),(x2, y2), ...,(xn, yn)where
xi∼N(0, I)andyi=w⊤xi, let the l-th layer output be (xl
1, yl
1),(xl
2, yl
2), ...,(xl
n, yl
n)and let
yl= (yl
1, ..., yl
n)andy∗= (x1(1)2, x2(1)2, ..., x n(1)2)(here xi(1)is just the first coordinate of xi),
then when n≫dwith high probability the cosine similarity of y∗andylis at most 0.1.
Theorem 4.1 implies that the output of linear transformer can always be explained as linear combina-
tions of input with latent weights alandwl. The matrices Ml, vectors ul, wland numbers alare not
linear and can in fact be quite complex, which we characterize below:
Lemma 4.3. In the setup of Theorem 4.1, if we let

Albl
(cl)⊤dl
:=
hX
k=1
Pl
knX
j=1xl
j
yl
j
((xl
j)⊤, yl
j)
Ql
k
,
then one can recursively compute matrices Ml, vectors ul, wland numbers alfor every layer using
Ml+1= (I+Al)Ml+bl(wl)⊤
ul+1= (I+Al)ul+albl
al+1= (1 + dl)al+⟨cl, ul⟩
wl+1= (1 + dl)wl−(Ml)⊤cl,
with the init. condition a0= 1, w0= 0, M0=I, u0= 0.
The updates to the parameters are complicated and nonlinear, allowing linear transformers to imple-
ment powerful algorithms, as we will later see in Section 5. In fact, even with diagonal PandQ,
they remain flexible. The updates in this case can be further simplified to a more familiar form:
Lemma 4.4. In the setup of Theorem 4.1 with diagonal parameters, ul, wlare updated as
ul+1= (I−Λl)ul+ ΓlΣ 
alw∗−wl
;
wl+1= (1 + sl)wl−ΠlΣ(alw∗−wl)−Φlul.
Here Λl,Γl, sl,Πl,Φlare matrices and numbers that depend on Ml, ul, al, wlin Lemma 4.3.
Note that Σ 
alw∗−wl
is (proportional to) the gradient of a linear model f(wl) =Pn
i=1(alyi−
⟨wl, xi⟩)2. This makes the updates similar to a gradient descent with momentum:
ul+1= (1−β)ul+∇f(wl);wl+1=wl−ηul.
Of course, the formula in Lemma 4.4 is still much more complicated with matrices in places of βand
η, and also including a gradient term for the update of w.
5 Power of diagonal attention matrices
Although linear transformers are constrained, they can solve complex in-context learning problems.
Empirically, we have found that they are able to very accurately solve linear regression with mixed
noise variance (7), with final learned weights that are very diagonal heavy with some low-rank
component (see Fig. 4). Surprisingly, the final loss remains remarkably consistent even when their Q
andPmatrices (3)are diagonal. Here we will analyze this special case and explain its effectiveness.
5

--- PAGE 6 ---
Since the elements of xare permutation invariant, a diagonal parameterization reduces each attention
heads to just four parameters:
Pl
k=pl
x,kI0
0 pl
y,k
;Ql
k=ql
x,kI0
0 ql
y,k
. (8)
It would be useful to further reparametrize the linear transformer (3) using:
ωl
xx=PH
k=1pl
x,kql
x,k, ωl
xy=PH
k=1pl
x,kql
y,k,
ωl
yx=PH
k=1pl
y,kql
x,k, ωl
yy=PH
k=1pl
y,kql
y,k.(9)
This leads to the following diagonal layer updates:
xl+1
i=xl
i+ωl
xxΣlxl
i+wl
xyyl
iαl
xl+1
t=xl
t+ωl
xxΣlxl
t+wl
xyyl
tαl
yl+1
i=yl
i+ωl
yx⟨αl, xl
i⟩+ωl
yyyl
iλl,
yl+1
t=yl
t+ωl
yx⟨αl, xl
t⟩+ωl
yyyl
tλl.(10)
Four variables ωl
xx,ωl
xy,ωl
yx,ωl
yyrepresent information flow between the data and the labels across
layers. For instance, the term controlled by ωl
xxmeasures information flow from xltoxl+1,ωl
yx
measures the flow from xltoyl+1and so forth. Since the model can always be captured by these 4
variables, having many heads does not significantly increase its representation power. When there is
only one head the equation ωl
xxωl
yy=ωl
xyωl
yxis always true, while models with more than one head
do not have this limitation. However empirically even models with one head is quite powerful.
5.1 GD++and least squares solver
GD++, introduced in von Oswald et al. (2023a), represents a linear transformer that is trained on a
fixed noise variance problem (6). It is a variant of a diagonal linear transformer, with all the heads
satisfying ql
y,k= 0. Dynamics are influenced only by ωl
xxandωl
yx, leading to simpler updates:
xl+1
i= 
I+ωl
xxΣl
xl
i
yl+1
i=yl
i+ωl
yx⟨αl, xl
i⟩.(11)
The update on xacts as preconditioning and the update on yperforms gradient descent on the data.
While existing analysis by Ahn et al. (2023) has not yielded fast convergence rates for GD++, we
show here that it is actually a second-order optimization algorithm for the least squares problem (4):
Theorem 5.1. Given (x1, y1), ...,(xn, yn),(xt,0)where Σhas eigenvalues in the range [ν, µ]with
a condition number κ=ν/µ. Letw∗be the optimal solution to least squares problem (4), then there
exists hyperparameters for GD++algorithm that outputs ˆywith accuracy |ˆy−⟨xt, w∗⟩| ≤ϵ∥xt∥∥w∗∥
inl=O(logκ+log log 1 /ϵ)steps. In particular that implies there exists an l-layer linear transformer
that can solve this task.
The convergence rate of O(log log 1 /ϵ)is typically achieved only by second-order algorithms such
as Newton’s method.
5.2 Understanding ωyy: adaptive rescaling
If a layer only has ωl
yy̸= 0, it has a rescaling effect. The amount of scaling is related to the amount
of noise added in a model selection setting. The update rule for this layer is:
yl+1
i= 
1 +ωl
yyλl
yl
i.
This rescales every yby a factor that depends on λl. When ωl
yy<0, this shrinks of the output based
on the norm of yin the previous layer. This is useful for the mixed noise variance problem, as ridge
regression solution scales the least squares solution by a factor that depends on the noise level.
Specifically, assuming Σ≈E[Σ] = nI, the ridge regression solution becomes w∗
σ2≈n
n+σ2w∗,
which is exactly a scaled version of the OLS solution. Further, when noise is larger, the scaled factor
is smaller, which agrees with the behavior of a negative ωyy.
We can show that using adaptive scaling ωyyeven a 2-layer linear transformer can be enough to solve
a simple example of categorical mixed noise variance problem στ∈ {σ1, σ2}andn→ ∞ :
6

--- PAGE 7 ---
10−1010−5100Adj. eval loss
σmax = 0
10−210−1100
σmax = 1
10−210−1100
σmax = 2
10−1100
σmax = 3
1 3 5 7
Number of layers10−1100Adj. eval loss
σmax = 4
1 3 5 7
Number of layers10−1100
σmax = 5
1 3 5 7
Number of layers10−1
σmax = 6
1 3 5 7
Number of layers10−1
σmax = 7
GD++Diag Full ConstRR AdaRR TunedRRFigure 1: In-context learning performance for noisy linear regression problem across models with
different number of layers and σmax forστ∼U(0, σmax). Each marker corresponds to a separately
trained model with a given number of layers. Models with diagonal attention weights ( DIAG) match
those with full attention weights ( FULL). Models specialized on a fixed noise (GD++) perform poorly,
similar to a Ridge Regression solution with a constant noise ( CONST RR). Among the baselines, only
tuned exact Ridge Regression solution (T UNED RR) is comparable with linear transformers.
Theorem 5.2. Suppose the input to the transformer is (x1, y1),(x2, y2), ...,(xn, yn),(xq,0), where
xi∼N(0,1
nI),yi=w⊤xi+ξi. Here ξi∼N(0, σ2)is the noise whose noise level σcan take
one of two values: σ1orσ2. Then as ngoes to +∞, there exists a set of parameters for two-layer
linear transformers such that the implicit w2of the linear transformer converges to the optimal ridge
regression results (and the output of the linear transformer is −⟨w2, xq⟩). Further, the first layer only
hasωyxbeing nonzero and the second layer only has ωyybeing nonzero.
5.3 Understanding ωxy: adapting step-sizes
The final term in the diagonal model, ωxy, has a more complicated effect. Since it changes only the
x-coordinates, it does not have an immediate effect on y. To understand how it influences the ywe
consider a simplified two-step process, where the first step only has ωxy̸= 0and the second step
only has ωyx̸= 0(so the second step is just doing one step of gradient descent). In this case, the first
layer will update the xi’s as:
x1
i=xi+yiωxynX
j=1yjxj
=xi+ωxyyinX
j=1(⟨w∗, xj⟩+rj)xj
=xi+ωxyyiΣw∗
=xi+ωxy(⟨w∗, xi⟩+ri)Σw∗
= (I+ωxyΣw∗(w∗)⊤)xi+ωxyriΣw∗.
There are two effects of the ωxyterm, one is a multiplicative effect on xi, and the other is an additive
term that makes x-output related to the residual ri. The multiplicative step in xihas an unknown
preconditioning effect. For simplicity we assume the multiplicative term is small, that is:
x1
i≈xi+ωxyriΣw∗;x1
t≈xt.
The first layer does not change y, soy1
t=ytandy1
i=yi. For this set of xi, we can write down the
output on yin the second layer as
y2
t=yt+ωyxnX
i=1yi(x1
i)⊤xt
≈yt+ωyx[nX
i=1yixi+ωxynX
i=1yiriΣw∗]xt
=yt+ωyx(1 +ωxynX
i=1r2
i)(Σw∗)⊤xt.
7

--- PAGE 8 ---
0.0 0.5 1.00.0000.0050.010Adj. eval lossσmax = 0
0 1 20.000.01σmax = 1
0 1 2 30.000.05σmax = 2
0 1 2 3 40.000.050.10σmax = 3
0 1 2 3 4 5
Variance σ0.00.1Adj. eval lossσmax = 4
0123456
Variance σ0.00.1σmax = 5
01234567
Variance σ0.00.1σmax = 6
012345678
Variance σ0.00.10.2σmax = 7
0123456
Variance σ0.000.050.100.150.20Adj. eval loss2 layers
0123456
Variance σ3 layers
0123456
Variance σ4 layers
0123456
Variance σ5 layers
0123456
Variance σ6 layers
0123456
Variance σ7 layers
GD++Diag Full ConstRR AdaRR TunedRRFigure 2: Per-variance profile of models behavior for uniform noise variance στ∼U(0, σmax).Top
two rows: 7-layer models with varying σmax.Bottom row: models with varying numbers of layers,
fixed σmax= 5. In-distribution noise is shaded gray.
Here we used the properties of residual ri(in particularP
iyixi= Σw∗, andP
iyiri=P
ir2
i).
Note that (Σw∗)⊤xtis basically what a gradient descent step on the original input should do.
Therefore effectively, the two-layer network is doing gradient descent, but the step size is the product
of−ωyxand(1 +ωxyP
ir2
i). The factor (1 +ωxyP
ir2
i)depends on the level of noise, and when
ωxy, ωyx<0, the effective step size is smaller when there is more noise. This is especially helpful in
the model selection problem, because intuitively one would like to perform early-stopping (small
step sizes) when the noise is high.
6 Experiments
In this section, we investigate the training dynamics of linear transformers when trained with a mixed
noise variance problem (7). We evaluate three types of single-head linear transformer models:
• FULL. Trains full parameter matrices.
• D IAG. Trains diagonal parameter matrices (10).
• GD++. An even more restricted diagonal variant defined in (11).
For each experiment, we train each linear transformer modifications with a varying number of layers
(1to7) using using Adam optimizer for 200 000 iterations with a learning rate of 0.0001 and a batch
size of 2 048 . In some cases, especially for a large number of layers, we had to adjust the learning rate
to prevent stability issues. We report the best result out of 5runs with different training seeds. We
usedN= 20 in-context examples in D= 10 dimensions. We evaluated the algorithm using 100 000
novel sequences. All the experiments were done on a single H100 GPU with 80GB of VRAM. It took
on average 4–12 hours to train a single algorithm, however experimenting with different weight decay
parameters, better optimizer and learning rate schedule will likely reduce this number dramatically.
We use adjusted evaluation loss as our main performance metric. It is calculated by subtracting
the oracle loss from the predictor’s loss. The oracle loss is the closed-form solution of the ridge
regression loss (5), assuming the noise variance στis known. The adjusted evaluation loss allows for
direct model performance comparison across different noise variances. This is important because
higher noise significantly degrades the model prediction. Our adjustment does not affect the model’s
optimization process, since it only modifies the loss by an additive constant.
Baseline estimates. We evaluated the linear transformer against a closed-form solution to the ridge
regression problem (5). We estimated the noise variance στusing the following methods:
•Constant Ridge Regression ( CONST RR).The noise variance is estimated using a single
scalar value for all the sequences, tuned separately for each mixed variance problem.
8

--- PAGE 9 ---
στ∈ {1,3} στ∈ {1,3,5}
1 3 5 7
Number of layers10−210−1100Adj. eval loss
0123456
Noise variance0.00.10.2Adj. eval loss
1 3 5 7
Number of layers10−210−1100Adj. eval loss
0123456
Noise variance0.00.10.2Adj. eval lossστ∈ {1,3}
0.00.10.2Adj. eval loss2 layers 3 layers 4 layers 5 layers 6 layers 7 layers στ∈ {1,3,5}
0123456
Variance σ0.00.10.2Adj. eval loss
0123456
Variance σ0123456
Variance σ0123456
Variance σ0123456
Variance σ0123456
Variance σ
GD++Diag Full ConstRR AdaRR TunedRR
Figure 3: In-context learning performance for noisy linear regression across models with varying
number of layers for conditional noise variance στ∈ {1,3}andστ∈ {1,3,5}.Top: loss for models
with various number of layers and per-variance profile for models with 7 layers. Bottom: Per-variance
profile of the model across different numbers of layers. In-distribution noise is shaded gray.
•Adaptive Ridge Regression ( ADARR).Estimate the noise variance via unbiased estimator
(Cherkassky & Ma, 2003) σ2
est=1
n−dPn
j=1(yj−ˆyj)2, where ˆyjrepresents the solution to
the ordinary least squares (4), found in a closed-form.
•Tuned Adaptive Ridge Regression ( TUNED RR).Same as above, but after the noise is
estimated, we tuned two additional parameters to minimize the evaluation loss: (1) a max.
threshold value for the estimated variance, (2) a multiplicative adjustment to the noise
estimator. These values are tuned separately for each problem.
Notice that all the baselines above are based on ridge regression, which is a closed-form, non-iterative
solution. Thus, they have an algorithmic advantage over linear transformers that do not have access
to matrix inversion. These baselines help us gauge the best possible performance, establishing an
upper bound rather than a strictly equivalent comparison.
A more faithful comparison to our method would be an iterative version of the ADARRthat does not
use matrix inversion. Instead, we can use gradient descent to estimate the noise and the solution to
the ridge regression. However, in practice, this gradient descent estimator converges to ADARRonly
after≈100iterations. In contrast, linear transformers typically converge in fewer than 10layers.
We consider two choices for the distribution of στ:
•Uniform. στ∼U(0, σmax)drawn from a uniform distribution bounded by σmax. We tried
multiple scenarios with σmax ranging from 0 to 7.
•Categorical. στ∈Schosen from a discrete set S. We tested S={1,3}andS={1,3,5}.
Our approach generalizes the problem studied by Bai et al. (2023), who considered only categorical
variance selection and show experiments only with two στvalues.
Uniform noise variance. For the uniform noise variance, Fig. 1 shows that FULL andDIAGachieve
comparable performance across different numbers of layers and different σmax. On the other hand,
GD++converges to a higher value, closely approaching the performance of the CONST RRbaseline.
Asσmax grows, linear transformers show a clear advantage over the baselines. With 4 layers, they
outperform the closed-form solution ADARRforσmax= 4and larger. Models with 5or more layers
match or exceed the performance of T UNED RR.
The top of Fig. 2 offers a detailed perspective on performance of 7-layer models and the baselines.
Here, we computed per-variance profiles across noise variance range from 0 to σmax+ 1. We can
see that poor performance of GD++comes from its inability to estimate well across the full noise
variance range. Its performance closely mirrors to CONST RR, suggesting that GD++under the hood
might also be estimating a single constant variance for all the data.
ADARR perfectly estimates problems with no noise, but struggles more as noise variance increases.
TUNED RRslightly improves estimation by incorporating σmax into its tunable parameters, yet its
9

--- PAGE 10 ---
Figure 4: Weights for 4 layer linear transformer with FULL parametrization trained with categorical
noise στ∈ {1,3}.Top: weights for Qlmatrix, bottom: weights for Plmatrix.
prediction suffers in the mid-range. FULL andDIAGdemonstrate comparable performance across all
noise variances. While more research is needed to definitively confirm or deny their equivalence, we
believe that these models are actually not identical despite their similar performance.
At the bottom of Fig. 2 we set the noise variance to σmax= 5and display a per-variance profile for
models with varying layers. Two-layer models for FULL andDIAGbehave akin to GD++, modeling
only a single noise variance in the middle. However, the results quickly improve across the entire
noise spectrum for 3 or more layers. In contrast, GD++quickly converges to a suboptimal solution.
Categorical noise variance. Fig. 3 shows a notable difference between DIAGandFULL models
for categorical noise variance στ∈ {1,3}. This could stem from a bad local minima, or suggest
a fundamental difference between the models for this problem. Interestingly, from per-variance
profiling we see that DIAGextrapolates better for variances not used for training, while FULL, despite
its lower in-distribution error, performs worse on unseen variances. Fig. 4 shows learned weights
of the 4 layer linear transformer with FULL parametrization. The weights are very diagonal heavy,
potentially with some low-rank component.
Forστ∈ {1,3,5}, examining the per-variance profile at the bottom of Fig. 3 reveals differences
in their behaviors. FULL exhibits a more complex per-variance profile with more fluctuations than
the diagonal model, suggesting greater representational capacity. Surprisingly, it did not translate to
better loss results compared to D IAG.
For easy comparison, we compile the results of all methods and baselines in Table 1 in the Appendix.
7 Conclusions
Our research reveals the surprising ability of linear transformers to tackle challenging in-context
learning problems. We show that each layer maintains an implicit linear regression model, akin to a
complex variant of preconditioned gradient descent with momentum-like behavior.
Remarkably, when trained on noisy linear regression problems with unknown noise variance, linear
transformers not only outperform standard baselines but also uncover a sophisticated optimization
algorithm that incorporates noise-aware step-size adjustments and rescaling. This discovery highlights
the potential of linear transformers to automatically discover novel optimization algorithms when
presented with the right problems, opening exciting avenues for future research, including automated
algorithm discovery using transformers and generalization to other problem domains.
While our findings demonstrate the impressive capabilities of linear transformers in learning optimiza-
tion algorithms, we acknowledge limitations in our work. These include the focus on simplified linear
models, analysis of primarily diagonal attention matrices, and the need for further exploration into the
optimality of discovered algorithms, generalization to complex function classes, scalability with larger
datasets, and applicability to more complex transformer architectures. We believe these limitations
present valuable directions for future research and emphasize the need for a deeper understanding of
the implicit learning mechanisms within transformer architectures.
10

--- PAGE 11 ---
8 Acknowledgements
The authors would like to thank Nolan Miller and Andrey Zhmoginov for their valuable suggestions
and feedback throughout the development of this project. Part of this work was done while Rong Ge
was visiting Google Research. Rong Ge’s research is supported in part by NSF Award DMS-2031849
and CCF-1845171 (CAREER).
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 , 2023.
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement
preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297 , 2023.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algo-
rithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661 ,
2022.
Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-Context language learning: Architec-
tures and algorithms. arXiv preprint arXiv:2401.12973 , 2024.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 , 2023.
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637 ,
2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond,
James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning
in transformers. Advances in Neural Information Processing Systems , 35:18878–18891, 2022.
Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to
learn non-linear functions in context. arXiv preprint arXiv:2312.06528 , 2023.
Vladimir Cherkassky and Yunqian Ma. Comparison of model selection for regression. Neural
computation , 15(7):1691–1714, 2003.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794 , 2020.
Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization
methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086 ,
2023.
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn
in-context? a case study of simple function classes. Advances in Neural Information Processing
Systems , 35:30583–30598, 2022.
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Pa-
pailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196 ,
2023.
Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do
transformers learn in-context beyond simple functions? a case study on learning with representa-
tions. arXiv preprint arXiv:2310.10616 , 2023.
11

--- PAGE 12 ---
Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. arXiv preprint
arXiv:2310.15916 , 2023.
Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint
arXiv:2310.05249 , 2023.
Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from
learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820 ,
2019.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International conference on machine
learning , pp. 5156–5165. PMLR, 2020.
Jannik Kossen, Tom Rainforth, and Yarin Gal. In-context learning in large language models learns
label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375 , 2023.
Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers
as algorithms: Generalization and stability in in-context learning. In International Conference on
Machine Learning , pp. 19565–19594. PMLR, 2023.
Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is
provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint
arXiv:2307.03576 , 2023.
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas,
Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines.
arXiv preprint arXiv:2307.04721 , 2023.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.
arXiv preprint arXiv:2209.11895 , 2022.
Reese Pathak, Rajat Sen, Weihao Kong, and Abhimanyu Das. Transformers can optimally learn
regression mixture models. arXiv preprint arXiv:2311.08362 , 2023.
Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight
programmers. In International Conference on Machine Learning , pp. 9355–9366. PMLR, 2021.
Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn
in-context by gradient descent? arXiv preprint arXiv:2310.08540 , 2023.
Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token
selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing
Systems , 2023.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training
dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380 , 2023a.
Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying
multilayer transformers via joint dynamics of mlp and attention. In NeurIPS 2023 Workshop on
Mathematics of Modern Machine Learning , 2023b.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems , 30, 2017.
12

--- PAGE 13 ---
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev,
Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In
International Conference on Machine Learning , pp. 35151–35174. PMLR, 2023a.
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet,
Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering
mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858 , 2023b.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv
preprint arXiv:2303.03846 , 2023.
Kaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski. Transformers are uninterpretable with
myopic methods: a case study with bounded dyck grammars. In Thirty-seventh Conference on
Neural Information Processing Systems , 2023.
Steve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni. Pretraining data mixtures enable narrow
model selection capabilities in transformer models. arXiv preprint arXiv:2311.00871 , 2023.
Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.
arXiv preprint arXiv:2306.09927 , 2023.
13

--- PAGE 14 ---
A Proofs from Sections 4 and 5
A.1 Proof of Theorem 4.1
We first give the proof for Theorem 4.1. In the process we will also prove Lemma 4.3, as Theorem 4.1
follows immediately from an induction based on the lemma.
Proof. We do this by induction. At l= 0, it’s easy to check that we can set a(0)= 1, w(0)=
0, M(0)=I, u(0)= 0.
Suppose this is true for some layer l, if the weights of layer lare(Pl
1, Ql
1), ...,(Pl
k, Ql
k)forkheads,
at output of layer l+ 1we have:

xl+1
i
yl+1
i
=
xl
i
yl
i
+pX
k=1
Pl
knX
j=1xl
j
yl
j
((xl
j)⊤, yl
j)
Ql
k

xl
i
yl
i
. (12)
Note that the same equation is true for i=n+ 1just by letting yn+1= 0. Let the middle matrix has
the following structure:
A b
c⊤d
:=pX
k=1
Pl
knX
j=1xl
j
yl
j
((xl
j)⊤, yl
j)
Ql
k
,
Then one can choose the parameters of the next layer as in Lemma 4.3
Ml+1= (I+A)Ml+b(wl)⊤
ul+1= (I+A)ul+alb
al+1= (1 + d)al+⟨c, ul⟩
wl+1= (1 + d)wl−(Ml)⊤c.
One can check that this choice satisfies (12).
A.2 Proof of Lemma 4.4
This lemma is in fact a corollary of Lemma 4.3. We first give a more detailed version which explicitly
state the unknown matrices Λl,Γl,Πl,Φl:
Lemma A.1. In the setup of Theorem 4.1 with diagonal parameters (9), one can recursively compute
matrices ul, wlusing the following formula
ul+1= 
(1 +ωl
xy(al)2ρ)I+ωl
xxΣl
ul
+alωl
xy 
Ml+alul(w∗)⊤
Σ 
alw∗−wl
,
wl+1= (1 + ωl
yyλl)wl
−ωl
yx(Ml)⊤(Ml+alul(w∗)⊤)Σ(alw∗−wl)
−alρωl
yx(Ml)⊤ul,
where ρ=Pn
i=1r2
iand initial conditions a0= 1, w0= 0, M0=I, u0= 0
Proof. First, we compute the following matrix that appeared in Lemma 4.3 for the specific diagonal
case:

Albl
(cl)⊤dl
=pX
k=1
Pl
knX
j=1xl
j
yl
j
((xl
j)⊤, yl
j)
Ql
k
,
=ωl
xxΣlωl
xyαl
ωl
yx(αl)⊤ωl
yyλl
.
14

--- PAGE 15 ---
This implies that Al=ωl
xxΣl,bl=ωl
xyαl,cl=ωl
yxαlanddl=ωl
yyλl. Next we rewrite αl:
αl=nX
i=1ylxl
=nX
i=1(alyi− ⟨wl, xi⟩)(Mlxi+yiul)
=nX
i=1(alri+⟨alw∗−wl, xi⟩)((Ml+alul(w∗)⊤)xi+riul)
=nX
i=1⟨alw∗−wl, xi⟩(Ml+alul(w∗)⊤)xi+nX
i=1alr2
iul
= (Ml+alul(w∗)⊤)Σ(alw∗−wl) +alρul.
Here the first step is by Theorem 4.1, the second step replaces yiwith⟨w∗, xi⟩+ri, the third step
uses the fact thatPn
i=1rixi= 0to get rid of the cross terms.
The remaining proof just substitutes the formula for αlinto Lemma 4.3.
Now Lemma A.1 implies Lemma 4.4 immediately by setting Λl=−ωl
xy(al)2ρI−ωl
xxΣl,
Γl=alωl
xy 
Ml+alul(w∗)⊤
,sl=tuωyyλl,Πl=ωl
yx(Ml)⊤(Ml+alul(w∗)⊤)andΦl=
alρωl
yx(Ml)⊤.
A.3 Proof for Theorem 4.2
Proof. By Theorem 4.1, we know yl
i=⟨wl, xi⟩for some wl. When n≫d, with high probability
the norm of ylis on the order of Θ(√n)∥wl∥, and the norm of y∗isΘ(√n). Therefore we only need
to bound the correlation. The correlation is equal to
|⟨y∗, yl⟩|=|wl
1nX
i=1x3
1+nX
i=1xi(1)2dX
j=2wl
jxi(j)|.
We know with high probability |Pn
i=1x3
i|=O(√n)because E[x3
i] = 0 . The second term can be
written as ⟨wl, v⟩where vis a vector whose coordinates are v1= 0andvj=Pn
i=1xi(1)2xi(j)for
2≤j≤d, therefore with high probability ∥v∥=O(√
nd). Therefore, with high probability the
cosine similarity is at most
|⟨y∗, yl⟩|
∥y∗∥∥yl∥=O(1)|⟨y∗, yl⟩|
n∥wl
=O(1)|wl
1Pn
i=1x3
1+Pn
i=1xi(1)2Pd
j=2wl
jxi(j)|
n∥wl
≤O(1)|wl
1||Pn
i=1x3
1|+|wl|∥v∥
n∥wl
≤O(1)√n+√
nd
n.
When n≫dthis can be made smaller than any fixed constant.
A.4 Proof for Theorem 5.1
In this section we prove Theorem 5.1 by finding hyperparameters for GD++algorithm that solves
least squares problems with very high accuracy. The first steps in the construction iteratively makes
the data xi’s better conditioned, and the last step is a single step of gradient descent. The proof is
based on several lemma, first we observe that if the data is very well-conditioned, then one-step
gradient descent solves the problem accurately:
15

--- PAGE 16 ---
Lemma A.2. Given (x1, y1), ...,(xn, yn)where Σ :=Pn
i=1xix⊤
ihas eigenvalues between 1and
1 +ϵ. Let w∗:= arg min wPn
i=1(yi− ⟨w, xi⟩)2be the optimal least squares solution, then ˆw=Pn
i=1yixisatisfies ∥ˆw−w∗∥ ≤ϵ∥w∗∥.
Proof. We can write yi=⟨xi, w∗⟩+ri. By the fact that w∗is the optimal solution we know ri’s
satisfyPn
i=1rixi= 0. Therefore ˆw=Pn
i=1yixi=Pn
i=1⟨xi, w∗⟩xi= Σw∗. This implies
∥ˆw−w∗∥=∥(Σ−I)w∗∥ ≤ ∥ Σ−I∥∥w∗∥ ≤ϵ∥w∗∥.
Next we show that by applying just the preconditioning step of GD++, one can get a well-conditioned
xmatrix very quickly. Note that the Σmatrix is updated as Σ←(I−γΣ)Σ( I−γΣ), so an
eigenvalue of λin the original Σmatrix would become λ(1−γλ)2. The following lemma shows
that this transformation is effective in shrinking the condition number
Lemma A.3. Suppose ν/µ=κ≥1.1, then there exists an universal constant c <1such that
choosing γν= 1/3implies
max λ∈[ν,µ]λ(1−γλ)2
minλ∈[ν,µ]λ(1−γλ)2≤cκ.
On the other hand, if ν/µ=κ≤1 +ϵwhere ϵ≤0.1, then choosing γν= 1/3implies
max λ∈[ν,µ]λ(1−γλ)2
minλ∈[ν,µ]λ(1−γλ)2≤1 + 2 ϵ2.
The first claim shows that one can reduce the condition number by a constant factor in every step
until it’s a small constant. The second claim shows that once the condition number is small ( 1 +ϵ),
each iteration can bring it much closer to 1 (to the order of 1 +O(ϵ2)).
Now we prove the lemma.
Proof. First, notice that the function f(x) = x(1−γx)2is monotonically nondecreasing for
x∈[0, ν]ifγν= 1/3(indeed, it’s derivative f′(x) = (1 −γx)(1−3γx)is always nonnegative).
Therefore, the max is always achieved at x=νand the min is always achieved at x=µ. The new
ratio is therefore
ν(1−γν)2
µ(1−γµ)2=κ4/9
(1−1/3κ)2.
When κ≥1.1the ratio4/9
(1−1/3κ)2is always below4/9
(1−1/3.3)2which is a constant bounded away
from 1.
When κ= 1 + ϵ <1.1, we can write down the RHS in terms of ϵ
ν(1−γν)2
µ(1−γµ)2=κ4/9
(1−1/3κ)2= (1 + ϵ)(1 +1
2(1−1
1 +ϵ))−2.
Note that by the careful choice of γ, the RHS has the following Taylor expansion:
(1 +ϵ)(1 +1
2(1−1
1 +ϵ))−2= 1 +3ϵ2
4−5ϵ3
4+O(ϵ4).
One can then check the RHS is always upperbounded by 2ϵ2when ϵ <0.1.
With the two lemmas we are now ready to prove the main theorem:
Proof. By Lemma A.3 we know in O(logκ+ log log 1 /ϵ)iterations, by assigning κin the way of
Lemma A.3 one can reduce the condition number of xtoκ′≤1 +ϵ/2κ(we chose ϵ/2κhere to give
some slack for later analysis).
LetΣ′be the covariance matrix after these iterations, and ν′, µ′be the upper and lowerbound
for its eigenvalues. The data xi’s are transformed to a new data x′
i=Mxifor some matrix
16

--- PAGE 17 ---
0 2 4 6 8
Prediction after K layers10−1100Adjusted eval lossGD++
0 2 4 6 8
Prediction after K layers10−1100Diag
0 2 4 6 8
Prediction after K layers10−1100Full
1 layers
2 layers
3 layers
4 layers
5 layers
6 layers
7 layersFigure 5: Linear transformer models show a consistent decrease in error per layer when trained on
data with mixed noise variance στ∼U(0,5). The error bars measure variance over 5training seeds.
M. Let M=AΣ−1/2, then since M′=AA⊤we know Ais a matrix with singular values
between√µ′and√
ν′. The optimal solution (w∗)′=M−⊤w∗has norm at most√ν/√µ′∥w∗∥.
Therefore by Lemma A.2 we know the one-step gradient step with ˆw=Pn
i=11
µ′yixisatisfy
∥ˆw−(w∗)′∥ ≤(κ′−1)√ν/√µ′∥w∗∥. The test data xtis also transformed to x′
t=AΣ−1/2xt, and
the algorithm outputs ⟨ˆw, x′
t⟩, so the error is at most√ν∥w∗∥∗∥x′
t∥ ≤(κ′−1)√κ√
κ′∥w∗∥∗∥xt∥.
By the choice of κ′we can check that RHS is at most ϵ∥w∗∥∥xt∥.
A.5 Proof of the Theorem 5.2
Proof. The key observation here is that when n→ ∞ , under the assumptions we have
limn→∞Pn
i=1xix⊤
i=I. Therefore the ridge regression solutions converge to w∗
σ2=
1
1+σ2Pn
i=1yixiand the desired output is ⟨w∗
σ2, xq⟩.
By the calculations before, we know after the first-layer, the implicit wisw1=ωyxPn
i=1yixi. As
long as ωyxis a constant, when n→ ∞ we know1
nPn
i=1(y1
i)2=σ2(as the part of ythat depend
onxis negligible compared to noise), therefore the output of the second layer satisfies
w2= (1 + nσ2ωyy)w1= (1 + nσ2ωyy)ωyxnX
i=1yixi.
Therefore, as long as we choose ωyxandωyyto satisfy (1 +nσ2ωyy)ωyx=1
1+σ2when σ=σ1or
σ2(notice that these are two linear equations on ωyxandnωyxωyy, so they always have a solution),
then we have limn→∞w2=w∗
σ2for the two noise levels.
B More experiments
Here we provide results of additional experiments that did not make it to the main text.
Fig. 6 shows an example of unadjusted loss. Clearly, it is virtually impossible to compare the methods
across various noise levels this way.
Fig. 7 shows per-variance profile of intermediate predictions of the network of varying depth. It
appears that GD++demonstrates behavior typical of GD-based algorithms: early iterations model
higher noise (similar to early stopping), gradually converging towards lower noise predictions. DIAG
exhibits this patter initially, but then dramatically improves, particularly for lower noise ranges.
Intriguingly, F ULL displays the opposite trend, first improving low-noise predictions, followed by a
decline in higher noise prediction accuracy, especially in the last layer.
Finally, Table 1 presents comprehensive numerical results for our experiments across various mixed
noise variance models. For each model variant (represented by a column), the best-performing result
is highlighted in bold.
17

--- PAGE 18 ---
0 2 4 6
Variance σ0246Unadjusted eval lossGD++
Diag
Full
ConstRR
AdaRR
TunedRRFigure 6: Example of unadjusted loss given by directly minimizing (7). It is pretty hard to see
variation between comparable methods using this loss directly.
Method Uniform στ∼(0, σmax) Categorical στ∈S
0 1 2 3 4 5 6 7 {1,3} {1,3,5}
1 layer
GD++1.768 1.639 1.396 1.175 1.015 0.907 0.841 0.806 1.007 0.819
DIAG 1.767 1.639 1.396 1.175 1.015 0.906 0.841 0.806 1.007 0.819
FULL 1.768 1.640 1.397 1.176 1.016 0.907 0.842 0.806 1.008 0.820
2 layers
GD++0.341 0.295 0.243 0.265 0.347 0.366 0.440 0.530 0.305 0.427
DIAG 0.265 0.214 0.173 0.188 0.219 0.242 0.254 0.259 0.201 0.246
FULL 0.264 0.215 0.173 0.188 0.220 0.245 0.259 0.263 0.202 0.276
3 layers
GD++0.019 0.021 0.071 0.161 0.259 0.344 0.454 0.530 0.222 0.422
DIAG 0.013 0.015 0.048 0.087 0.109 0.118 0.121 0.123 0.098 0.119
FULL 0.012 0.015 0.049 0.075 0.101 0.117 0.124 0.127 0.076 0.113
4 layers
GD++9.91e-05 0.014 0.066 0.160 0.258 0.344 0.454 0.530 0.222 0.422
DIAG 1.19e-04 0.006 0.024 0.041 0.050 0.059 0.065 0.073 0.043 0.062
FULL 1.63e-04 0.005 0.021 0.038 0.052 0.065 0.068 0.076 0.032 0.061
5 layers
GD++1.14e-07 0.014 0.066 0.161 0.265 0.344 0.454 0.530 0.222 0.422
DIAG 1.81e-07 0.004 0.016 0.029 0.041 0.051 0.058 0.062 0.026 0.051
FULL 1.79e-07 0.002 0.015 0.026 0.038 0.048 0.059 0.065 0.016 0.048
6 layers
GD++2.37e-10 0.009 0.066 0.161 0.265 0.344 0.454 0.530 0.222 0.422
DIAG 2.57e-10 0.003 0.014 0.028 0.040 0.048 0.054 0.059 0.020 0.047
FULL 2.71e-10 0.002 0.014 0.025 0.036 0.044 0.052 0.059 0.011 0.043
7 layers
GD++2.65e-12 0.009 0.066 0.161 0.265 0.344 0.454 0.530 0.222 0.422
DIAG 2.50e-12 0.002 0.014 0.027 0.040 0.047 0.052 0.059 0.018 0.046
FULL 2.50e-12 0.002 0.010 0.025 0.035 0.047 0.050 0.057 0.010 0.035
Baselines
CONST RR 0 0.009 0.066 0.161 0.265 0.365 0.454 0.530 0.222 0.422
ADARR 0 0.003 0.016 0.034 0.053 0.068 0.081 0.092 0.051 0.084
TUNED RR 0 0.002 0.010 0.023 0.037 0.049 0.060 0.068 0.021 0.054
Table 1: Adjusted evaluation loss for models with various number of layers with uniform noise
variance στ∼U(0, σmax). We highlight in bold the best results for each problem setup (i.e. each
column).
18

--- PAGE 19 ---
0.000.250.500.751.002 layers model
Adj. eval lossGD++Diag Full
0.000.250.500.751.003 layers model
Adj. eval loss
0.000.250.500.751.004 layers model
Adj. eval loss
0.000.250.500.751.005 layers model
Adj. eval loss
0.000.250.500.751.006 layers model
Adj. eval loss
0.000.250.500.751.007 layers model
Adj. eval loss
0 2 4 6 8 10
Variance σ0.000.250.500.751.008 layers model
Adj. eval loss
0 2 4 6 8 10
Variance σ0 2 4 6 8 10
Variance σ
After 1 layer
After 2 layersAfter 3 layers
After 4 layersAfter 5 layers
After 6 layersAfter 7 layers
After 8 layersFigure 7: Layer by layer prediction quality for different models with στ∼U(0,5). The error bars
measure std over 5training seeds.
19
