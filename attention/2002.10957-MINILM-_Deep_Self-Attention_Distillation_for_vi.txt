# MINILM: Chưng cất Tự chú ý Sâu cho 
Nén Các Transformer Được Tiền huấn luyện Bất khả tri Tác vụ

Wenhui Wang Furu Wei Li Dong Hangbo Bao Nan Yang Ming Zhou
Microsoft Research
{wenwan,fuwei,lidong1,t-habao,nanya,mingzhou}@microsoft.com

## Tóm tắt

Các mô hình ngôn ngữ được tiền huấn luyện (ví dụ: BERT (Devlin
et al., 2018) và các biến thể của nó) đã đạt được thành công 
đáng kể trong các tác vụ NLP đa dạng. Tuy nhiên, các mô hình này 
thường bao gồm hàng trăm triệu tham số gây ra thách thức cho 
việc tinh chỉnh và phục vụ trực tuyến trong các ứng dụng thực tế 
do các ràng buộc về độ trễ và dung lượng. Trong công trình này, 
chúng tôi trình bày một phương pháp đơn giản và hiệu quả để nén 
các mô hình được tiền huấn luyện dựa trên Transformer lớn (Vaswani 
et al., 2017), được gọi là chưng cất tự chú ý sâu. Mô hình nhỏ 
(học sinh) được huấn luyện bằng cách bắt chước sâu sắc mô-đun 
tự chú ý, đóng vai trò quan trọng trong mạng Transformer, của mô 
hình lớn (giáo viên). Cụ thể, chúng tôi đề xuất chưng cất mô-đun 
tự chú ý của lớp Transformer cuối cùng của giáo viên, điều này 
hiệu quả và linh hoạt cho học sinh. Hơn nữa, chúng tôi giới thiệu 
tích vô hướng có tỷ lệ giữa các giá trị trong mô-đun tự chú ý như 
kiến thức tự chú ý sâu mới, bổ sung cho các phân phối chú ý (tức 
là tích vô hướng có tỷ lệ của truy vấn và khóa) đã được sử dụng 
trong các công trình hiện có. Hơn nữa, chúng tôi chỉ ra rằng việc 
giới thiệu một trợ lý giáo viên (Mirzadeh et al., 2019) cũng giúp 
chưng cất các mô hình Transformer được tiền huấn luyện lớn. Kết 
quả thực nghiệm chứng minh rằng mô hình đơn ngữ của chúng tôi 
vượt trội so với các baseline tối tân trong các kích thước tham số 
khác nhau của mô hình học sinh. Đặc biệt, nó giữ lại hơn 99% độ 
chính xác trên SQuAD 2.0 và một số tác vụ benchmark GLUE khi 
sử dụng 50% tham số và tính toán Transformer của mô hình giáo viên. 
Chúng tôi cũng thu được kết quả cạnh tranh trong việc áp dụng 
chưng cất tự chú ý sâu cho các mô hình được tiền huấn luyện đa ngữ.

Liên hệ với: Furu Wei <fuwei@microsoft.com>.
¹Mã nguồn và mô hình được công bố tại https://aka.ms/minilm.

## 1. Giới thiệu

Tiền huấn luyện mô hình ngôn ngữ (LM) đã đạt được thành công đáng kể cho các tác vụ xử lý ngôn ngữ tự nhiên đa dạng (Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019). Các mô hình ngôn ngữ được tiền huấn luyện, như BERT (Devlin et al., 2018) và các biến thể của nó, học các biểu diễn văn bản có ngữ cảnh bằng cách dự đoán từ dựa trên ngữ cảnh của chúng sử dụng kho văn bản quy mô lớn, và có thể được tinh chỉnh với các lớp bổ sung đặc thù tác vụ để thích ứng với các tác vụ downstream. Tuy nhiên, các mô hình này thường chứa hàng trăm triệu tham số gây ra thách thức cho việc tinh chỉnh và phục vụ trực tuyến trong các ứng dụng thực tế đối với các ràng buộc về độ trễ và dung lượng.

Chưng cất kiến thức (Hinton et al., 2015; Romero et al., 2015) (KD) đã được chứng minh là một cách hứa hẹn để nén một mô hình lớn (được gọi là mô hình giáo viên) thành một mô hình nhỏ (được gọi là mô hình học sinh), sử dụng ít tham số và tính toán hơn nhiều trong khi đạt được kết quả cạnh tranh trên các tác vụ downstream. Đã có một số công trình chưng cất các LM được tiền huấn luyện lớn thành các mô hình nhỏ đặc thù tác vụ (Tang et al., 2019; Turc et al., 2019b; Sun et al., 2019a; Aguilar et al., 2019). Họ đầu tiên tinh chỉnh các LM được tiền huấn luyện trên các tác vụ cụ thể và sau đó thực hiện chưng cất. Chưng cất đặc thù tác vụ là hiệu quả, nhưng việc tinh chỉnh các mô hình được tiền huấn luyện lớn vẫn tốn kém, đặc biệt đối với các tập dữ liệu lớn.

Khác với chưng cất đặc thù tác vụ, chưng cất LM bất khả tri tác vụ bắt chước hành vi của các LM được tiền huấn luyện gốc và mô hình học sinh có thể được tinh chỉnh trực tiếp trên các tác vụ downstream (Tsai et al., 2019; Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019b).

Các công trình trước đây sử dụng xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ hoặc biểu diễn trung gian của LM giáo viên để hướng dẫn việc huấn luyện học sinh bất khả tri tác vụ. DistilBERT (Sanh et al., 2019) sử dụng mất mát chưng cất nhãn mềm và mất mát nhúng cosine, và khởi tạo học sinh từ giáo viên bằng cách lấy một lớp trong hai. Nhưng mỗi lớp Transformer của học sinh được yêu cầu có cùng kiến trúc như giáo viên của nó. TinyBERT (Jiao et al., 2019) và MOBILE BERT (Sun et al., 2019b) sử dụng kiến thức chi tiết hơn, bao gồm các trạng thái ẩn và phân phối tự chú ý của mạng Transformer, và chuyển giao kiến thức này cho mô hình học sinh theo từng lớp. Để thực hiện chưng cất theo từng lớp, TinyBERT áp dụng một hàm đồng nhất để xác định ánh xạ giữa các lớp giáo viên và học sinh, và sử dụng ma trận tham số để biến đổi tuyến tính các trạng thái ẩn của học sinh. MOBILE BERT giả định giáo viên và học sinh có cùng số lượng lớp và giới thiệu mô-đun cổ chai để giữ kích thước ẩn của chúng giống nhau.

Trong công trình này, chúng tôi đề xuất khung chưng cất tự chú ý sâu cho chưng cất LM dựa trên Transformer bất khả tri tác vụ. Ý tưởng chính là bắt chước sâu sắc các mô-đun tự chú ý, là thành phần quan trọng cơ bản trong các mô hình giáo viên và học sinh dựa trên Transformer. Cụ thể, chúng tôi đề xuất chưng cất mô-đun tự chú ý của lớp Transformer cuối cùng của mô hình giáo viên. So với các phương pháp trước đây, việc sử dụng kiến thức của lớp Transformer cuối cùng thay vì thực hiện chưng cất kiến thức theo từng lớp làm giảm khó khăn trong ánh xạ lớp giữa các mô hình giáo viên và học sinh, và số lượng lớp của mô hình học sinh có thể linh hoạt hơn. Hơn nữa, chúng tôi giới thiệu tích vô hướng có tỷ lệ giữa các giá trị trong mô-đun tự chú ý như kiến thức tự chú ý sâu mới, bổ sung cho các phân phối chú ý (tức là tích vô hướng có tỷ lệ của truy vấn và khóa) đã được sử dụng trong các công trình hiện có. Việc sử dụng tích vô hướng có tỷ lệ giữa các giá trị tự chú ý cũng chuyển đổi các biểu diễn có chiều khác nhau thành các ma trận quan hệ có cùng chiều mà không cần giới thiệu thêm tham số để biến đổi biểu diễn học sinh, cho phép chiều ẩn tùy ý cho mô hình học sinh. Cuối cùng, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên (Mirzadeh et al., 2019) giúp chưng cất các mô hình dựa trên Transformer được tiền huấn luyện lớn và chưng cất tự chú ý sâu được đề xuất có thể tăng cường hiệu suất hơn nữa.

Chúng tôi thực hiện các thí nghiệm mở rộng trên các tác vụ NLP downstream. Kết quả thực nghiệm chứng minh rằng mô hình đơn ngữ của chúng tôi vượt trội so với các baseline tối tân trong các kích thước tham số khác nhau của mô hình học sinh. Cụ thể, mô hình 6 lớp với 768 chiều ẩn được chưng cất từ BERT BASE nhanh hơn 2.0 lần, trong khi giữ lại hơn 99% độ chính xác trên SQuAD 2.0 và một số tác vụ benchmark GLUE. Hơn nữa, mô hình đa ngữ của chúng tôi được chưng cất từ XLM-R Base cũng đạt được hiệu suất cạnh tranh với ít tham số Transformer hơn nhiều.

## 2. Sơ bộ

Các Transformer nhiều lớp (Vaswani et al., 2017) đã là cấu trúc mạng được sử dụng rộng rãi nhất trong các mô hình được tiền huấn luyện tối tân. Trong phần này, chúng tôi trình bày giới thiệu ngắn gọn về mạng Transformer và cơ chế tự chú ý, là thành phần cốt lõi của Transformer. Chúng tôi cũng trình bày các phương pháp hiện có về chưng cất kiến thức cho mạng Transformer, đặc biệt trong bối cảnh chưng cất một mô hình được tiền huấn luyện dựa trên Transformer lớn thành một mô hình Transformer nhỏ.

### 2.1. Biểu diễn Đầu vào

Văn bản được token hóa thành các đơn vị từ con bằng WordPiece (Wu et al., 2016) trong BERT (Devlin et al., 2018). Ví dụ, từ "forecasted" được tách thành "forecast" và "##ed", trong đó "##" chỉ ra các mảnh thuộc về một từ. Một token ranh giới đặc biệt [SEP] được sử dụng để tách các đoạn nếu văn bản đầu vào chứa nhiều hơn một đoạn. Ở đầu chuỗi, một token đặc biệt [CLS] được thêm vào để thu được biểu diễn của toàn bộ đầu vào. Các biểu diễn vector ({x_i}_{i=1}^{|x|}) của các token đầu vào được tính bằng cách cộng embedding token tương ứng, embedding vị trí tuyệt đối và embedding đoạn.

### 2.2. Mạng Cốt lõi: Transformer

Transformer (Vaswani et al., 2017) được sử dụng để mã hóa thông tin ngữ cảnh cho các token đầu vào. Các vector đầu vào {x_i}_{i=1}^{|x|} được đóng gói lại thành H^0 = [x_1; ...; x_{|x|}]. Sau đó các khối Transformer xếp chồng tính toán các vector mã hóa như:

H^l = Transformer^l(H^{l-1}); l ∈ [1, L]                    (1)

trong đó L là số lượng lớp Transformer, và đầu ra cuối cùng là H^L = [h_1^L; ...; h_{|x|}^L]. Vector ẩn h_i^L được sử dụng như biểu diễn có ngữ cảnh của x_i. Mỗi lớp Transformer bao gồm một lớp con tự chú ý và một mạng feed-forward kết nối đầy đủ. Kết nối dư (He et al., 2016) được sử dụng xung quanh mỗi lớp con trong hai lớp con, theo sau bởi chuẩn hóa lớp (Ba et al., 2016).

**Tự chú ý** Trong mỗi lớp, Transformer sử dụng nhiều đầu tự chú ý để tổng hợp các vector đầu ra của lớp trước. Đối với lớp Transformer thứ l, đầu ra của một đầu tự chú ý AO^{l,a}; a ∈ [1, A_h] được tính bằng:

Q^{l,a} = H^{l-1}W_Q^{l,a}, K^{l,a} = H^{l-1}W_K^{l,a}, V^{l,a} = H^{l-1}W_V^{l,a}     (2)

A^{l,a} = softmax(Q^{l,a}K^{l,a|}/√d_k)                     (3)

AO^{l,a} = A^{l,a}V^{l,a}                                   (4)

trong đó đầu ra H^{l-1} ∈ R^{|x|×d_h} của lớp trước được chiếu tuyến tính thành ba thành phần truy vấn, khóa và giá trị sử dụng các ma trận tham số W_Q^{l,a}, W_K^{l,a}, W_V^{l,a} ∈ R^{d_h×d_k}, tương ứng. A^{l,a} ∈ R^{|x|×|x|} chỉ ra các phân phối chú ý, được tính bằng tích vô hướng có tỷ lệ của truy vấn và khóa. A_h đại diện cho số lượng đầu tự chú ý. d_k A_h bằng chiều ẩn d_h trong BERT.

### 2.3. Chưng cất Transformer

Chưng cất kiến thức (Hinton et al., 2015; Romero et al., 2015) là huấn luyện mô hình học sinh nhỏ S trên một tập đặc trưng chuyển giao với nhãn mềm và biểu diễn trung gian được cung cấp bởi mô hình giáo viên lớn T. Chưng cất kiến thức được mô hình hóa như việc tối thiểu hóa sự khác biệt giữa các đặc trưng của giáo viên và học sinh:

L_{KD} = ∑_{e∈D} L(f_S(e), f_T(e))                         (5)

Trong đó D biểu thị dữ liệu huấn luyện, f_S() và f_T() chỉ ra các đặc trưng của mô hình học sinh và giáo viên tương ứng, L() đại diện cho hàm mất mát. Sai số bình phương trung bình (MSE) và KL-divergence thường được sử dụng làm hàm mất mát.

Đối với chưng cất LM dựa trên Transformer, xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ, đầu ra lớp embedding, phân phối tự chú ý và đầu ra (trạng thái ẩn) của mỗi lớp Transformer của mô hình giáo viên được sử dụng làm đặc trưng để giúp huấn luyện học sinh. Nhãn mềm và đầu ra lớp embedding được sử dụng trong DistillBERT. TinyBERT và MOBILE BERT tiếp tục sử dụng phân phối tự chú ý và đầu ra của mỗi lớp Transformer. Đối với MOBILE BERT, học sinh được yêu cầu có cùng số lượng lớp như giáo viên của nó để thực hiện chưng cất theo từng lớp. Bên cạnh đó, các mô-đun cổ chai và cổ chai đảo được giới thiệu để giữ kích thước ẩn của giáo viên và học sinh cũng giống nhau. Để chuyển giao kiến thức theo từng lớp, TinyBERT sử dụng hàm đồng nhất để ánh xạ các lớp giáo viên và học sinh. Vì kích thước ẩn của học sinh có thể nhỏ hơn giáo viên của nó, một ma trận tham số được giới thiệu để biến đổi các đặc trưng học sinh.

## 3. Chưng cất Tự chú ý Sâu

Hình 1 đưa ra tổng quan về chưng cất tự chú ý sâu. Ý tưởng chính có ba mặt. Thứ nhất, chúng tôi đề xuất huấn luyện học sinh bằng cách bắt chước sâu sắc mô-đun tự chú ý, là thành phần quan trọng trong Transformer, của lớp cuối cùng của giáo viên. Thứ hai, chúng tôi giới thiệu việc chuyển giao mối quan hệ giữa các giá trị (tức là tích vô hướng có tỷ lệ giữa các giá trị) để đạt được sự bắt chước sâu hơn, bổ sung cho việc thực hiện chuyển giao phân phối chú ý (tức là tích vô hướng có tỷ lệ của truy vấn và khóa) trong mô-đun tự chú ý. Hơn nữa, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên (Mirzadeh et al., 2019) cũng giúp chưng cất các mô hình Transformer được tiền huấn luyện lớn khi khoảng cách kích thước giữa mô hình giáo viên và mô hình học sinh là lớn.

### 3.1. Chuyển giao Phân phối Tự chú ý

Cơ chế chú ý (Bahdanau et al., 2015) đã là một thành phần mạng thần kinh rất thành công cho các tác vụ NLP, cũng quan trọng đối với các LM được tiền huấn luyện. Một số công trình chỉ ra rằng các phân phối tự chú ý của các LM được tiền huấn luyện nắm bắt một hệ thống phong phú các thông tin ngôn ngữ học (Jawahar et al., 2019; Clark et al., 2019). Việc chuyển giao phân phối tự chú ý đã được sử dụng trong các công trình trước đây cho chưng cất Transformer (Jiao et al., 2019; Sun et al., 2019b; Aguilar et al., 2019). Chúng tôi cũng sử dụng các phân phối tự chú ý để giúp huấn luyện học sinh. Cụ thể, chúng tôi tối thiểu hóa KL-divergence giữa các phân phối tự chú ý của giáo viên và học sinh:

L_{AT} = \frac{1}{A_h|x|} \sum_{a=1}^{A_h} \sum_{t=1}^{|x|} D_{KL}(A_T^{L,a,t} || A_S^{M,a,t})     (6)

Trong đó |x| và A_h đại diện cho độ dài chuỗi và số lượng đầu chú ý. L và M đại diện cho số lượng lớp cho giáo viên và học sinh. A_T^L và A_S^M là các phân phối chú ý của lớp Transformer cuối cùng cho giáo viên và học sinh, tương ứng. Chúng được tính bằng tích vô hướng có tỷ lệ của truy vấn và khóa.

Khác với các công trình trước đây chuyển giao kiến thức của giáo viên theo từng lớp, chúng tôi chỉ sử dụng các bản đồ chú ý của lớp Transformer cuối cùng của giáo viên. Việc chưng cất kiến thức chú ý của lớp Transformer cuối cùng cho phép linh hoạt hơn về số lượng lớp của các mô hình học sinh, tránh việc nỗ lực tìm ánh xạ lớp tốt nhất.

### 3.2. Chuyển giao Quan hệ Giá trị Tự chú ý

Bổ sung cho các phân phối chú ý, chúng tôi đề xuất sử dụng mối quan hệ giữa các giá trị trong mô-đun tự chú ý để hướng dẫn việc huấn luyện học sinh. Quan hệ giá trị được tính bằng tích vô hướng có tỷ lệ đa đầu giữa các giá trị. KL-divergence giữa quan hệ giá trị của giáo viên và học sinh được sử dụng làm mục tiêu huấn luyện:

VR_T^{L,a} = softmax(\frac{V_T^{L,a}V_T^{L,a|}}{\sqrt{d_k}})     (7)

VR_S^{M,a} = softmax(\frac{V_S^{M,a}V_S^{M,a|}}{\sqrt{d_k'}})    (8)

L_{VR} = \frac{1}{A_h|x|} \sum_{a=1}^{A_h} \sum_{t=1}^{|x|} D_{KL}(VR_T^{L,a,t} || VR_S^{M,a,t})     (9)

Trong đó V_T^{L,a} ∈ R^{|x|×d_k} và V_S^{M,a} ∈ R^{|x|×d_k'} là các giá trị của một đầu chú ý trong mô-đun tự chú ý cho lớp Transformer cuối cùng của giáo viên và học sinh. VR_T^L ∈ R^{A_h×|x|×|x|} và VR_S^M ∈ R^{A_h×|x|×|x|} là quan hệ giá trị của lớp Transformer cuối cùng cho giáo viên và học sinh, tương ứng.

Mất mát huấn luyện được tính bằng cách cộng mất mát chuyển giao phân phối chú ý và mất mát chuyển giao quan hệ giá trị:

L = L_{AT} + L_{VR}                                          (10)

Việc giới thiệu mối quan hệ giữa các giá trị cho phép học sinh bắt chước sâu sắc hành vi tự chú ý của giáo viên. Hơn nữa, việc sử dụng tích vô hướng có tỷ lệ chuyển đổi các vector có chiều ẩn khác nhau thành các ma trận quan hệ có cùng kích thước, điều này cho phép các học sinh sử dụng chiều ẩn linh hoạt hơn và tránh việc giới thiệu thêm tham số để biến đổi biểu diễn của học sinh.

### 3.3. Trợ lý Giáo viên

Theo Mirzadeh et al. (2019), chúng tôi giới thiệu một trợ lý giáo viên (tức là mô hình học sinh có kích thước trung gian) để cải thiện thêm hiệu suất mô hình của các học sinh nhỏ hơn.

Giả sử mô hình giáo viên bao gồm Transformer L lớp với kích thước ẩn d_h, mô hình học sinh có Transformer M lớp với kích thước ẩn d_h'. Đối với các học sinh nhỏ hơn (M ≤ \frac{1}{2}L, d_h' ≤ \frac{1}{2}d_h), chúng tôi đầu tiên chưng cất giáo viên thành một trợ lý giáo viên với Transformer L lớp và kích thước ẩn d_h'. Mô hình trợ lý sau đó được sử dụng làm giáo viên để hướng dẫn việc huấn luyện học sinh cuối cùng. Việc giới thiệu một trợ lý giáo viên bắc cầu khoảng cách kích thước giữa các mô hình giáo viên và học sinh nhỏ hơn, giúp chưng cất các LM được tiền huấn luyện dựa trên Transformer. Hơn nữa, việc kết hợp chưng cất tự chú ý sâu với một trợ lý giáo viên mang lại cải thiện thêm cho các mô hình học sinh nhỏ hơn.

### 3.4. So sánh với Công trình Trước đây

Bảng 1 trình bày so sánh với các phương pháp trước đây (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019b). MOBILE BERT đề xuất sử dụng một mô hình cổ chai đảo được thiết kế đặc biệt, có cùng kích thước mô hình như BERT LARGE, làm giáo viên. Các phương pháp khác sử dụng BERT BASE để thực hiện thí nghiệm. Đối với kiến thức được sử dụng để chưng cất, phương pháp của chúng tôi giới thiệu tích vô hướng có tỷ lệ giữa các giá trị trong mô-đun tự chú ý như kiến thức mới để bắt chước sâu sắc hành vi tự chú ý của giáo viên. TinyBERT và MOBILE BERT chuyển giao kiến thức của giáo viên cho học sinh theo từng lớp. MOBILE BERT giả định học sinh có cùng số lượng lớp như giáo viên của nó. TinyBERT sử dụng chiến lược đồng nhất để xác định ánh xạ lớp. DistillBERT khởi tạo học sinh với các tham số của giáo viên, do đó việc chọn lớp của mô hình giáo viên vẫn cần thiết. MINILM chưng cất kiến thức tự chú ý của lớp Transformer cuối cùng của giáo viên, điều này cho phép số lượng lớp linh hoạt cho các học sinh và giảm bớt nỗ lực tìm ánh xạ lớp tốt nhất. Kích thước ẩn học sinh của DistillBERT và MOBILE BERT được yêu cầu giống như giáo viên của nó. TinyBERT sử dụng ma trận tham số để biến đổi trạng thái ẩn học sinh. Việc sử dụng quan hệ giá trị cho phép các học sinh sử dụng kích thước ẩn tùy ý mà không cần giới thiệu thêm tham số.

## 4. Thí nghiệm

Chúng tôi thực hiện các thí nghiệm chưng cất với các kích thước tham số khác nhau của mô hình học sinh, và đánh giá các mô hình được chưng cất trên các tác vụ downstream bao gồm trả lời câu hỏi trích xuất và benchmark GLUE.

### 4.1. Thiết lập Chưng cất

Chúng tôi sử dụng phiên bản không phân biệt chữ hoa thường của BERT BASE làm giáo viên. BERT BASE (Devlin et al., 2018) là một Transformer 12 lớp với kích thước ẩn 768 và 12 đầu chú ý, chứa khoảng 109M tham số. Số lượng đầu của phân phối chú ý và quan hệ giá trị được đặt là 12 cho các mô hình học sinh. Chúng tôi sử dụng tài liệu của Wikipedia tiếng Anh² và BookCorpus (Zhu et al., 2015) cho dữ liệu tiền huấn luyện, theo xử lý và token hóa WordPiece của Devlin et al. (2018). Kích thước từ vựng là 30,522. Độ dài chuỗi tối đa là 512. Chúng tôi sử dụng Adam (Kingma & Ba, 2015) với β₁ = 0.9, β₂ = 0.999. Chúng tôi huấn luyện mô hình học sinh 6 lớp với kích thước ẩn 768 sử dụng 1024 làm kích thước batch và 5e-4 làm tốc độ học đỉnh trong 400,000 bước. Đối với các mô hình học sinh có kiến trúc khác, kích thước batch và tốc độ học đỉnh được đặt lần lượt là 256 và 3e-4. Chúng tôi sử dụng làm ấm tuyến tính trong 4,000 bước đầu tiên và suy giảm tuyến tính. Tỷ lệ dropout là 0.1. Suy giảm trọng số là 0.01.

Chúng tôi cũng sử dụng một mô hình Transformer được tiền huấn luyện nội bộ với kích thước BERT BASE làm mô hình giáo viên, và chưng cất nó thành các mô hình học sinh 12 lớp và 6 lớp với kích thước ẩn 384. Đối với mô hình 12 lớp, chúng tôi sử dụng Adam (Kingma & Ba, 2015) với β₁ = 0.9, β₂ = 0.98. Mô hình được huấn luyện sử dụng 2048 làm kích thước batch và 6e-4 làm tốc độ học đỉnh trong 400,000 bước. Kích thước batch và tốc độ học đỉnh được đặt lần lượt là 512 và 4e-4 cho mô hình 6 lớp. Các siêu tham số còn lại giống như các mô hình được chưng cất dựa trên BERT ở trên.

Đối với việc huấn luyện các mô hình MINILM đa ngữ, chúng tôi sử dụng Adam (Kingma & Ba, 2015) với β₁ = 0.9, β₂ = 0.999. Chúng tôi huấn luyện mô hình học sinh 12 lớp sử dụng 256 làm kích thước batch và 3e-4 làm tốc độ học đỉnh trong 1,000,000 bước. Mô hình học sinh 6 lớp được huấn luyện sử dụng 512 làm kích thước batch và 6e-4 làm tốc độ học đỉnh trong 400,000 bước.

Chúng tôi chưng cất các mô hình học sinh sử dụng 8 GPU V100 với huấn luyện độ chính xác hỗn hợp. Theo Sun et al. (2019a) và Jiao et al. (2019), thời gian suy luận được đánh giá trên tập huấn luyện QNLI với cùng siêu tham số. Chúng tôi báo cáo thời gian chạy trung bình của 100 batch trên một GPU P100 đơn.

### 4.2. Các Tác vụ Downstream

Theo tiền huấn luyện mô hình ngôn ngữ trước đây (Devlin et al., 2018; Liu et al., 2019) và chưng cất mô hình ngôn ngữ được tiền huấn luyện bất khả tri tác vụ (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019b), chúng tôi đánh giá các mô hình được chưng cất trên trả lời câu hỏi trích xuất và benchmark GLUE.

**Trả lời Câu hỏi Trích xuất** Cho một đoạn văn P, tác vụ là chọn một khoảng văn bản liên tục trong đoạn văn bằng cách dự đoán vị trí bắt đầu và kết thúc để trả lời câu hỏi Q. Chúng tôi đánh giá trên SQuAD 2.0 (Rajpurkar et al., 2018), đã phục vụ như một benchmark trả lời câu hỏi chính. Theo BERT (Devlin et al., 2018), chúng tôi đóng gói các token câu hỏi và đoạn văn lại với các token đặc biệt, để tạo thành đầu vào: "[CLS] Q [SEP] P [SEP]". Hai lớp đầu ra tuyến tính được giới thiệu để dự đoán xác suất của mỗi token là vị trí bắt đầu và kết thúc của khoảng trả lời. Các câu hỏi không có câu trả lời được xử lý như có khoảng trả lời với bắt đầu và kết thúc tại token [CLS].

**GLUE** Benchmark Đánh giá Hiểu biết Ngôn ngữ Tổng quát (GLUE) (Wang et al., 2019) bao gồm chín tác vụ phân loại cấp câu, bao gồm Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2018), Stanford Sentiment Treebank (SST) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005), Semantic Textual Similarity Benchmark (STS) (Cer et al., 2017), Quora Question Pairs (QQP) (Chen et al., 2018), Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018), Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) và Winograd Natural Language Inference (WNLI) (Levesque et al., 2012). Chúng tôi thêm một bộ phân loại tuyến tính trên đỉnh token [CLS] để dự đoán xác suất nhãn.

### 4.3. Kết quả Chính

Các công trình trước đây (Sanh et al., 2019; Sun et al., 2019a; Jiao et al., 2019) thường chưng cất BERT BASE thành một mô hình học sinh 6 lớp với kích thước ẩn 768. Chúng tôi đầu tiên thực hiện các thí nghiệm chưng cất sử dụng cùng kiến trúc học sinh. Kết quả trên các tập dev SQuAD 2.0 và GLUE được trình bày trong Bảng 2. Vì MOBILE BERT chưng cất một giáo viên được thiết kế đặc biệt với các mô-đun cổ chai đảo, có cùng kích thước mô hình như BERT LARGE, thành một học sinh 24 lớp sử dụng các mô-đun cổ chai, chúng tôi không so sánh các mô hình của chúng tôi với MOBILE BERT. MINILM vượt trội so với DistillBERT³ và TinyBERT⁴ trên hầu hết các tác vụ. Mô hình của chúng tôi vượt trội so với hai mô hình tối tân bằng 3.0+% F1 trên SQuAD 2.0 và 5.0+% độ chính xác trên CoLA. Chúng tôi trình bày thời gian suy luận cho các mô hình có kích thước tham số khác nhau trong Bảng 4. Mô hình học sinh 6 lớp 768 chiều của chúng tôi nhanh hơn 2.0 lần so với BERT BASE gốc, trong khi giữ lại hơn 99% hiệu suất trên nhiều tác vụ đa dạng, như SQuAD 2.0 và MNLI.

Chúng tôi cũng thực hiện thí nghiệm cho các mô hình học sinh nhỏ hơn. Chúng tôi so sánh MINILM với MLM-KD (chưng cất kiến thức sử dụng xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ) và TinyBERT được triển khai bởi chúng tôi, được huấn luyện sử dụng cùng dữ liệu và siêu tham số. Kết quả trên các tập dev SQuAD 2.0, MNLI và SST-2 được hiển thị trong Bảng 3. MINILM vượt trội so với chưng cất nhãn mềm và TinyBERT được triển khai bởi chúng tôi trên ba tác vụ. Chưng cất tự chú ý sâu cũng hiệu quả cho các mô hình nhỏ hơn. Hơn nữa, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên⁵ cũng hữu ích trong chưng cất LM được tiền huấn luyện dựa trên Transformer, đặc biệt cho các mô hình nhỏ hơn. Việc kết hợp chưng cất tự chú ý sâu với một trợ lý giáo viên đạt được cải thiện thêm cho các mô hình học sinh nhỏ hơn.

### 4.4. Nghiên cứu Khử bỏ

Chúng tôi thực hiện các thử nghiệm khử bỏ trên một số tác vụ để phân tích đóng góp của việc chuyển giao quan hệ giá trị tự chú ý. Kết quả dev của SQuAD 2.0, MNLI và SST-2 được minh họa trong Bảng 5, việc sử dụng chuyển giao quan hệ giá trị tự chú ý đóng góp tích cực vào kết quả cuối cùng cho các mô hình học sinh có kích thước tham số khác nhau. Việc chưng cất kiến thức chi tiết của quan hệ giá trị giúp mô hình học sinh bắt chước sâu sắc hành vi tự chú ý của giáo viên, điều này cải thiện thêm hiệu suất mô hình.

Chúng tôi cũng so sánh các hàm mất mát khác nhau trên các giá trị trong mô-đun tự chú ý. Chúng tôi so sánh quan hệ giá trị được đề xuất với sai số bình phương trung bình (MSE) trên các giá trị giáo viên và học sinh. Một ma trận tham số bổ sung được giới thiệu để biến đổi giá trị học sinh nếu chiều ẩn của học sinh nhỏ hơn giáo viên của nó. Kết quả dev trên ba tác vụ được trình bày trong Bảng 6. Việc sử dụng quan hệ giá trị đạt được hiệu suất tốt hơn. Cụ thể, phương pháp của chúng tôi mang lại khoảng 1.0% cải thiện F1 trên benchmark SQuAD. Hơn nữa, không cần thiết phải giới thiệu thêm tham số cho phương pháp của chúng tôi. Chúng tôi cũng đã thử chuyển giao mối quan hệ giữa các trạng thái ẩn. Nhưng chúng tôi thấy hiệu suất của các mô hình học sinh không ổn định cho các mô hình giáo viên khác nhau.

Để chỉ ra tính hiệu quả của việc chưng cất kiến thức tự chú ý của lớp Transformer cuối cùng của giáo viên, chúng tôi so sánh phương pháp của chúng tôi với chưng cất theo từng lớp. Chúng tôi chuyển giao cùng kiến thức và áp dụng chiến lược đồng nhất như trong Jiao et al. (2019) để ánh xạ các lớp giáo viên và học sinh để thực hiện chưng cất theo từng lớp. Kết quả dev trên ba tác vụ được trình bày trong Bảng 7. MINILM đạt được kết quả tốt hơn. Nó cũng làm giảm khó khăn trong ánh xạ lớp giữa giáo viên và học sinh. Bên cạnh đó, việc chưng cất lớp Transformer cuối cùng của giáo viên đòi hỏi ít tính toán hơn so với chưng cất theo từng lớp, dẫn đến tốc độ huấn luyện nhanh hơn.

## 5. Thảo luận

### 5.1. Giáo viên Tốt hơn Học sinh Tốt hơn

Chúng tôi báo cáo kết quả của MINILM được chưng cất từ một mô hình Transformer được tiền huấn luyện nội bộ theo UNILM (Dong et al., 2019; Bao et al., 2020) với kích thước BERT BASE. Mô hình giáo viên được huấn luyện sử dụng các tập dữ liệu tiền huấn luyện tương tự như trong RoBERTa BASE (Liu et al., 2019), bao gồm 160GB kho văn bản từ Wikipedia tiếng Anh, BookCorpus (Zhu et al., 2015), OpenWebText⁶, CC-News (Liu et al., 2019), và Stories (Trinh & Le, 2018). Chúng tôi chưng cất mô hình giáo viên thành các mô hình 12 lớp và 6 lớp với kích thước ẩn 384 sử dụng cùng kho văn bản. Mô hình 12x384 được sử dụng làm trợ lý giáo viên để huấn luyện mô hình 6x384. Chúng tôi trình bày kết quả dev của SQuAD 2.0 và benchmark GLUE trong Bảng 8, kết quả của MINILM được cải thiện đáng kể. MINILM 12x384 đạt được tăng tốc 2.7 lần trong khi thực hiện cạnh tranh tốt hơn so với BERT BASE trong SQuAD 2.0 và các tập dữ liệu benchmark GLUE.

### 5.2. MINILM cho Các Tác vụ NLG

Chúng tôi cũng đánh giá MINILM trên các tác vụ sinh ngôn ngữ tự nhiên, như sinh câu hỏi và tóm tắt trừu tượng. Theo Dong et al. (2019), chúng tôi tinh chỉnh MINILM như một mô hình chuỗi-sang-chuỗi bằng cách sử dụng mặt nạ tự chú ý cụ thể.

**Sinh Câu hỏi** Chúng tôi thực hiện thí nghiệm cho tác vụ sinh câu hỏi có nhận thức câu trả lời (Du & Cardie, 2018). Cho một đoạn văn đầu vào và một câu trả lời, tác vụ là sinh một câu hỏi hỏi về câu trả lời. Tập dữ liệu SQuAD 1.1 (Rajpurkar et al., 2016) được sử dụng để đánh giá. Kết quả của MINILM, UNILM LARGE và một số mô hình tối tân được trình bày trong Bảng 9, các mô hình được chưng cất 12x384 và 6x384 của chúng tôi đạt được hiệu suất cạnh tranh trên tác vụ sinh câu hỏi.

**Tóm tắt Trừu tượng** Chúng tôi đánh giá MINILM trên hai tập dữ liệu tóm tắt trừu tượng, tức là XSum (Narayan et al., 2018), và phiên bản không ẩn danh của CNN/DailyMail (See et al., 2017). Tác vụ sinh là tóm tắt một tài liệu thành một bản tóm tắt ngắn gọn và lưu loát, trong khi truyền đạt thông tin chính của nó. Chúng tôi báo cáo điểm ROUGE (Lin, 2004) trên các tập dữ liệu. Bảng 10 trình bày kết quả của MINILM, baseline, một số mô hình tối tân và các mô hình Transformer được tiền huấn luyện. Mô hình 12x384 của chúng tôi vượt trội so với phương pháp dựa trên BERT BERTSUMABS (Liu & Lapata, 2019) và mô hình chuỗi-sang-chuỗi được tiền huấn luyện MASS BASE (Song et al., 2019) với ít tham số hơn nhiều. Hơn nữa, MINILM 6x384 của chúng tôi cũng đạt được hiệu suất cạnh tranh.

### 5.3. MINILM Đa ngữ

Chúng tôi thực hiện thí nghiệm về chưng cất kiến thức bất khả tri tác vụ của các mô hình được tiền huấn luyện đa ngữ. Chúng tôi sử dụng XLM-R Base⁷ (Conneau et al., 2019) làm giáo viên và chưng cất mô hình thành các mô hình 12 lớp và 6 lớp với kích thước ẩn 384 sử dụng cùng kho văn bản. Mô hình 6x384 được huấn luyện sử dụng mô hình 12x384 làm trợ lý giáo viên. Cho kích thước từ vựng của các mô hình được tiền huấn luyện đa ngữ lớn hơn nhiều so với các mô hình đơn ngữ (30k cho BERT đơn ngữ, 250k cho XLM-R), chưng cất nhãn mềm cho các mô hình được tiền huấn luyện đa ngữ đòi hỏi nhiều tính toán hơn. MINILM chỉ sử dụng kiến thức tự chú ý sâu của lớp Transformer cuối cùng của giáo viên. Tốc độ huấn luyện của MINILM nhanh hơn nhiều so với chưng cất nhãn mềm cho các mô hình được tiền huấn luyện đa ngữ.

Chúng tôi đánh giá các mô hình học sinh trên benchmark suy luận ngôn ngữ tự nhiên đa ngữ (XNLI) (Conneau et al., 2018) và benchmark trả lời câu hỏi đa ngữ (MLQA) (Lewis et al., 2019b).

**XNLI** Bảng 11 trình bày kết quả XNLI của các học sinh được chưng cất và một số LM được tiền huấn luyện. Theo Conneau et al. (2019), chúng tôi chọn mô hình đơn tốt nhất trên tập dev chung của tất cả các ngôn ngữ. Chúng tôi trình bày số lượng tham số Transformer và embedding cho các mô hình được tiền huấn luyện đa ngữ khác nhau và các mô hình được chưng cất của chúng tôi trong Bảng 12. MINILM đạt được hiệu suất cạnh tranh trên XNLI với ít tham số Transformer hơn nhiều. Hơn nữa, MINILM 12x384 so sánh thuận lợi với mBERT (Devlin et al., 2018) và XLM (Lample & Conneau, 2019) được huấn luyện trên mục tiêu MLM.

**MLQA** Bảng 13 hiển thị kết quả trả lời câu hỏi đa ngữ. Theo Lewis et al. (2019b), chúng tôi áp dụng SQuAD 1.1 làm dữ liệu huấn luyện và sử dụng dữ liệu phát triển MLQA tiếng Anh để dừng sớm. MINILM 12x384 thực hiện cạnh tranh tốt hơn so với mBERT và XLM. MINILM 6 lớp của chúng tôi cũng đạt được hiệu suất cạnh tranh.

## 6. Công trình Liên quan

### 6.1. Các Mô hình Ngôn ngữ Được Tiền huấn luyện

Tiền huấn luyện không giám sát của các mô hình ngôn ngữ (Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Song et al., 2019; Dong et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019; Lewis et al., 2019a; Raffel et al., 2019) đã đạt được cải thiện đáng kể cho một loạt rộng các tác vụ NLP. Các phương pháp đầu tiên cho tiền huấn luyện (Peters et al., 2018; Radford et al., 2018) dựa trên các mô hình ngôn ngữ tiêu chuẩn. Gần đây, BERT (Devlin et al., 2018) đề xuất sử dụng mục tiêu mô hình ngôn ngữ có mặt nạ để huấn luyện một bộ mã hóa Transformer hai chiều sâu, học các tương tác giữa ngữ cảnh trái và phải. Liu et al. (2019) chỉ ra rằng hiệu suất rất mạnh có thể đạt được bằng cách huấn luyện mô hình lâu hơn trên nhiều dữ liệu hơn. Joshi et al. (2019) mở rộng BERT bằng cách che các khoảng ngẫu nhiên liên tục. Yang et al. (2019) dự đoán các token bị che một cách tự hồi quy theo thứ tự hoán vị.

Để mở rộng khả năng áp dụng của các Transformer được tiền huấn luyện cho các tác vụ NLG. Dong et al. (2019) mở rộng BERT bằng cách sử dụng các mặt nạ tự chú ý cụ thể để tối ưu hóa chung các mục tiêu mô hình ngôn ngữ có mặt nạ hai chiều, một chiều và chuỗi-sang-chuỗi. Raffel et al. (2019) sử dụng một Transformer mã hóa-giải mã và thực hiện tiền huấn luyện chuỗi-sang-chuỗi bằng cách dự đoán các token bị che trong bộ mã hóa và bộ giải mã. Khác với Raffel et al. (2019), Lewis et al. (2019a) dự đoán các token một cách tự hồi quy trong bộ giải mã.

### 6.2. Chưng cất Kiến thức

Chưng cất kiến thức đã chứng minh là một cách hứa hẹn để nén các mô hình lớn trong khi duy trì độ chính xác. Nó chuyển giao kiến thức của một mô hình lớn hoặc một tổ hợp các mạng thần kinh (giáo viên) cho một mô hình nhẹ đơn (học sinh). Hinton et al. (2015) đầu tiên đề xuất chuyển giao kiến thức của giáo viên cho học sinh bằng cách sử dụng các phân phối mục tiêu mềm để huấn luyện mô hình được chưng cất. Romero et al. (2015) giới thiệu các biểu diễn trung gian từ các lớp ẩn của giáo viên để hướng dẫn việc huấn luyện học sinh. Kiến thức của các bản đồ chú ý (Zagoruyko & Komodakis, 2017; Hu et al., 2018) cũng được giới thiệu để giúp huấn luyện.

Trong công trình này, chúng tôi tập trung vào chưng cất kiến thức bất khả tri tác vụ của các mô hình ngôn ngữ được tiền huấn luyện dựa trên Transformer lớn. Đã có một số công trình chưng cất các mô hình ngôn ngữ được tinh chỉnh trên các tác vụ downstream đặc thù tác vụ. Tang et al. (2019) chưng cất BERT được tinh chỉnh thành một LSTM hai chiều cực kỳ nhỏ. Turc et al. (2019a) khởi tạo học sinh với một LM được tiền huấn luyện nhỏ trong quá trình chưng cất đặc thù tác vụ. Sun et al. (2019a) giới thiệu các trạng thái ẩn từ mỗi k lớp của giáo viên để thực hiện chưng cất kiến thức theo từng lớp. Aguilar et al. (2019) tiếp tục giới thiệu kiến thức của các phân phối tự chú ý và đề xuất các phương pháp chưng cất tiến bộ và xếp chồng. Chưng cất đặc thù tác vụ đòi hỏi phải tinh chỉnh các LM được tiền huấn luyện lớn trên các tác vụ downstream trước và sau đó thực hiện chuyển giao kiến thức. Quy trình tinh chỉnh các LM được tiền huấn luyện lớn tốn kém và mất thời gian, đặc biệt đối với các tập dữ liệu lớn.

Đối với chưng cất bất khả tri tác vụ, mô hình được chưng cất bắt chước LM được tiền huấn luyện lớn gốc và có thể được tinh chỉnh trực tiếp trên các tác vụ downstream. Trong thực tế, nén bất khả tri tác vụ của các LM được tiền huấn luyện là mong muốn hơn. MiniBERT (Tsai et al., 2019) sử dụng các phân phối mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ để hướng dẫn việc huấn luyện mô hình học sinh đa ngữ và chỉ ra tính hiệu quả của nó trên các tác vụ gắn nhãn chuỗi. DistillBERT (Sanh et al., 2019) sử dụng nhãn mềm và đầu ra embedding của giáo viên để huấn luyện học sinh. TinyBERT (Jiao et al., 2019) và MOBILE-BERT (Sun et al., 2019b) tiếp tục giới thiệu các phân phối tự chú ý và trạng thái ẩn để huấn luyện học sinh. MOBILE-BERT sử dụng các mô-đun cổ chai đảo và cổ chai cho giáo viên và học sinh để làm cho chiều ẩn của chúng giống nhau. Mô hình học sinh của MOBILE BERT được yêu cầu có cùng số lượng lớp như giáo viên của nó để thực hiện chưng cất theo từng lớp. Bên cạnh đó, MOBILE BERT đề xuất một sơ đồ tiến bộ từ dưới lên trên để chuyển giao kiến thức của giáo viên. TinyBERT sử dụng chiến lược đồng nhất để ánh xạ các lớp của giáo viên và học sinh khi chúng có số lượng lớp khác nhau, và một ma trận tuyến tính được giới thiệu để biến đổi các trạng thái ẩn của học sinh để có cùng chiều như giáo viên. TinyBERT cũng giới thiệu chưng cất đặc thù tác vụ và tăng cường dữ liệu cho các tác vụ downstream, điều này mang lại cải thiện thêm.

Khác với các công trình trước đây, phương pháp của chúng tôi sử dụng các phân phối tự chú ý và quan hệ giá trị của lớp Transformer cuối cùng của giáo viên để giúp học sinh bắt chước sâu sắc hành vi tự chú ý của giáo viên. Việc sử dụng kiến thức của lớp Transformer cuối cùng thay vì chưng cất theo từng lớp tránh các hạn chế về số lượng lớp học sinh và nỗ lực tìm ánh xạ lớp tốt nhất. Việc chưng cất mối quan hệ giữa các giá trị tự chú ý cho phép kích thước ẩn của học sinh linh hoạt hơn và tránh việc giới thiệu các ma trận tuyến tính để biến đổi biểu diễn học sinh.

## 7. Kết luận

Trong công trình này, chúng tôi đề xuất một phương pháp chưng cất kiến thức đơn giản và hiệu quả để nén các mô hình ngôn ngữ được tiền huấn luyện dựa trên Transformer lớn. Học sinh được huấn luyện bằng cách bắt chước sâu sắc các mô-đun tự chú ý của giáo viên, là các thành phần quan trọng của mạng Transformer. Chúng tôi đề xuất sử dụng các phân phối tự chú ý và quan hệ giá trị của lớp Transformer cuối cùng của giáo viên để hướng dẫn việc huấn luyện học sinh, điều này hiệu quả và linh hoạt cho các mô hình học sinh. Hơn nữa, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên cũng giúp chưng cất LM được tiền huấn luyện dựa trên Transformer, và chưng cất tự chú ý sâu được đề xuất có thể tăng cường hiệu suất hơn nữa. Mô hình học sinh của chúng tôi được chưng cất từ BERT BASE giữ lại độ chính xác cao trên SQuAD 2.0 và các tác vụ benchmark GLUE, và vượt trội so với các baseline tối tân. Chưng cất tự chú ý sâu cũng có thể được áp dụng để nén các mô hình được tiền huấn luyện có kích thước lớn hơn. Chúng tôi để lại điều này như công việc tương lai.

[Phần còn lại của tài liệu bao gồm Tài liệu tham khảo và Phụ lục sẽ được dịch tiếp theo nếu cần]# MINILM: Chưng cất Tự chú ý Sâu cho 
Nén Các Transformer Được Tiền huấn luyện Bất khả tri Tác vụ

Wenhui Wang Furu Wei Li Dong Hangbo Bao Nan Yang Ming Zhou
Microsoft Research
{wenwan,fuwei,lidong1,t-habao,nanya,mingzhou}@microsoft.com

## Tóm tắt

Các mô hình ngôn ngữ được tiền huấn luyện (ví dụ: BERT (Devlin
et al., 2018) và các biến thể của nó) đã đạt được thành công 
đáng kể trong các tác vụ NLP đa dạng. Tuy nhiên, các mô hình này 
thường bao gồm hàng trăm triệu tham số gây ra thách thức cho 
việc tinh chỉnh và phục vụ trực tuyến trong các ứng dụng thực tế 
do các ràng buộc về độ trễ và dung lượng. Trong công trình này, 
chúng tôi trình bày một phương pháp đơn giản và hiệu quả để nén 
các mô hình được tiền huấn luyện dựa trên Transformer lớn (Vaswani 
et al., 2017), được gọi là chưng cất tự chú ý sâu. Mô hình nhỏ 
(học sinh) được huấn luyện bằng cách bắt chước sâu sắc mô-đun 
tự chú ý, đóng vai trò quan trọng trong mạng Transformer, của mô 
hình lớn (giáo viên). Cụ thể, chúng tôi đề xuất chưng cất mô-đun 
tự chú ý của lớp Transformer cuối cùng của giáo viên, điều này 
hiệu quả và linh hoạt cho học sinh. Hơn nữa, chúng tôi giới thiệu 
tích vô hướng có tỷ lệ giữa các giá trị trong mô-đun tự chú ý như 
kiến thức tự chú ý sâu mới, bổ sung cho các phân phối chú ý (tức 
là tích vô hướng có tỷ lệ của truy vấn và khóa) đã được sử dụng 
trong các công trình hiện có. Hơn nữa, chúng tôi chỉ ra rằng việc 
giới thiệu một trợ lý giáo viên (Mirzadeh et al., 2019) cũng giúp 
chưng cất các mô hình Transformer được tiền huấn luyện lớn. Kết 
quả thực nghiệm chứng minh rằng mô hình đơn ngữ của chúng tôi 
vượt trội so với các baseline tối tân trong các kích thước tham số 
khác nhau của mô hình học sinh. Đặc biệt, nó giữ lại hơn 99% độ 
chính xác trên SQuAD 2.0 và một số tác vụ benchmark GLUE khi 
sử dụng 50% tham số và tính toán Transformer của mô hình giáo viên. 
Chúng tôi cũng thu được kết quả cạnh tranh trong việc áp dụng 
chưng cất tự chú ý sâu cho các mô hình được tiền huấn luyện đa ngữ.

Liên hệ với: Furu Wei <fuwei@microsoft.com>.
¹Mã nguồn và mô hình được công bố tại https://aka.ms/minilm.

## 1. Giới thiệu

Tiền huấn luyện mô hình ngôn ngữ (LM) đã đạt được thành công đáng kể cho các tác vụ xử lý ngôn ngữ tự nhiên đa dạng (Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019). Các mô hình ngôn ngữ được tiền huấn luyện, như BERT (Devlin et al., 2018) và các biến thể của nó, học các biểu diễn văn bản có ngữ cảnh bằng cách dự đoán từ dựa trên ngữ cảnh của chúng sử dụng kho văn bản quy mô lớn, và có thể được tinh chỉnh với các lớp bổ sung đặc thù tác vụ để thích ứng với các tác vụ downstream. Tuy nhiên, các mô hình này thường chứa hàng trăm triệu tham số gây ra thách thức cho việc tinh chỉnh và phục vụ trực tuyến trong các ứng dụng thực tế đối với các ràng buộc về độ trễ và dung lượng.

Chưng cất kiến thức (Hinton et al., 2015; Romero et al., 2015) (KD) đã được chứng minh là một cách hứa hẹn để nén một mô hình lớn (được gọi là mô hình giáo viên) thành một mô hình nhỏ (được gọi là mô hình học sinh), sử dụng ít tham số và tính toán hơn nhiều trong khi đạt được kết quả cạnh tranh trên các tác vụ downstream. Đã có một số công trình chưng cất các LM được tiền huấn luyện lớn thành các mô hình nhỏ đặc thù tác vụ (Tang et al., 2019; Turc et al., 2019b; Sun et al., 2019a; Aguilar et al., 2019). Họ đầu tiên tinh chỉnh các LM được tiền huấn luyện trên các tác vụ cụ thể và sau đó thực hiện chưng cất. Chưng cất đặc thù tác vụ là hiệu quả, nhưng việc tinh chỉnh các mô hình được tiền huấn luyện lớn vẫn tốn kém, đặc biệt đối với các tập dữ liệu lớn.

Khác với chưng cất đặc thù tác vụ, chưng cất LM bất khả tri tác vụ bắt chước hành vi của các LM được tiền huấn luyện gốc và mô hình học sinh có thể được tinh chỉnh trực tiếp trên các tác vụ downstream (Tsai et al., 2019; Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019b).

Các công trình trước đây sử dụng xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ hoặc biểu diễn trung gian của LM giáo viên để hướng dẫn việc huấn luyện học sinh bất khả tri tác vụ. DistilBERT (Sanh et al., 2019) sử dụng mất mát chưng cất nhãn mềm và mất mát nhúng cosine, và khởi tạo học sinh từ giáo viên bằng cách lấy một lớp trong hai. Nhưng mỗi lớp Transformer của học sinh được yêu cầu có cùng kiến trúc như giáo viên của nó. TinyBERT (Jiao et al., 2019) và MOBILE BERT (Sun et al., 2019b) sử dụng kiến thức chi tiết hơn, bao gồm các trạng thái ẩn và phân phối tự chú ý của mạng Transformer, và chuyển giao kiến thức này cho mô hình học sinh theo từng lớp. Để thực hiện chưng cất theo từng lớp, TinyBERT áp dụng một hàm đồng nhất để xác định ánh xạ giữa các lớp giáo viên và học sinh, và sử dụng ma trận tham số để biến đổi tuyến tính các trạng thái ẩn của học sinh. MOBILE BERT giả định giáo viên và học sinh có cùng số lượng lớp và giới thiệu mô-đun cổ chai để giữ kích thước ẩn của chúng giống nhau.

Trong công trình này, chúng tôi đề xuất khung chưng cất tự chú ý sâu cho chưng cất LM dựa trên Transformer bất khả tri tác vụ. Ý tưởng chính là bắt chước sâu sắc các mô-đun tự chú ý, là thành phần quan trọng cơ bản trong các mô hình giáo viên và học sinh dựa trên Transformer. Cụ thể, chúng tôi đề xuất chưng cất mô-đun tự chú ý của lớp Transformer cuối cùng của mô hình giáo viên. So với các phương pháp trước đây, việc sử dụng kiến thức của lớp Transformer cuối cùng thay vì thực hiện chưng cất kiến thức theo từng lớp làm giảm khó khăn trong ánh xạ lớp giữa các mô hình giáo viên và học sinh, và số lượng lớp của mô hình học sinh có thể linh hoạt hơn. Hơn nữa, chúng tôi giới thiệu tích vô hướng có tỷ lệ giữa các giá trị trong mô-đun tự chú ý như kiến thức tự chú ý sâu mới, bổ sung cho các phân phối chú ý (tức là tích vô hướng có tỷ lệ của truy vấn và khóa) đã được sử dụng trong các công trình hiện có. Việc sử dụng tích vô hướng có tỷ lệ giữa các giá trị tự chú ý cũng chuyển đổi các biểu diễn có chiều khác nhau thành các ma trận quan hệ có cùng chiều mà không cần giới thiệu thêm tham số để biến đổi biểu diễn học sinh, cho phép chiều ẩn tùy ý cho mô hình học sinh. Cuối cùng, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên (Mirzadeh et al., 2019) giúp chưng cất các mô hình dựa trên Transformer được tiền huấn luyện lớn và chưng cất tự chú ý sâu được đề xuất có thể tăng cường hiệu suất hơn nữa.

Chúng tôi thực hiện các thí nghiệm mở rộng trên các tác vụ NLP downstream. Kết quả thực nghiệm chứng minh rằng mô hình đơn ngữ của chúng tôi vượt trội so với các baseline tối tân trong các kích thước tham số khác nhau của mô hình học sinh. Cụ thể, mô hình 6 lớp với 768 chiều ẩn được chưng cất từ BERT BASE nhanh hơn 2.0 lần, trong khi giữ lại hơn 99% độ chính xác trên SQuAD 2.0 và một số tác vụ benchmark GLUE. Hơn nữa, mô hình đa ngữ của chúng tôi được chưng cất từ XLM-R Base cũng đạt được hiệu suất cạnh tranh với ít tham số Transformer hơn nhiều.

## 2. Sơ bộ

Các Transformer nhiều lớp (Vaswani et al., 2017) đã là cấu trúc mạng được sử dụng rộng rãi nhất trong các mô hình được tiền huấn luyện tối tân. Trong phần này, chúng tôi trình bày giới thiệu ngắn gọn về mạng Transformer và cơ chế tự chú ý, là thành phần cốt lõi của Transformer. Chúng tôi cũng trình bày các phương pháp hiện có về chưng cất kiến thức cho mạng Transformer, đặc biệt trong bối cảnh chưng cất một mô hình được tiền huấn luyện dựa trên Transformer lớn thành một mô hình Transformer nhỏ.

### 2.1. Biểu diễn Đầu vào

Văn bản được token hóa thành các đơn vị từ con bằng WordPiece (Wu et al., 2016) trong BERT (Devlin et al., 2018). Ví dụ, từ "forecasted" được tách thành "forecast" và "##ed", trong đó "##" chỉ ra các mảnh thuộc về một từ. Một token ranh giới đặc biệt [SEP] được sử dụng để tách các đoạn nếu văn bản đầu vào chứa nhiều hơn một đoạn. Ở đầu chuỗi, một token đặc biệt [CLS] được thêm vào để thu được biểu diễn của toàn bộ đầu vào. Các biểu diễn vector ({x_i}_{i=1}^{|x|}) của các token đầu vào được tính bằng cách cộng embedding token tương ứng, embedding vị trí tuyệt đối và embedding đoạn.

### 2.2. Mạng Cốt lõi: Transformer

Transformer (Vaswani et al., 2017) được sử dụng để mã hóa thông tin ngữ cảnh cho các token đầu vào. Các vector đầu vào {x_i}_{i=1}^{|x|} được đóng gói lại thành H^0 = [x_1; ...; x_{|x|}]. Sau đó các khối Transformer xếp chồng tính toán các vector mã hóa như:

H^l = Transformer^l(H^{l-1}); l ∈ [1, L]                    (1)

trong đó L là số lượng lớp Transformer, và đầu ra cuối cùng là H^L = [h_1^L; ...; h_{|x|}^L]. Vector ẩn h_i^L được sử dụng như biểu diễn có ngữ cảnh của x_i. Mỗi lớp Transformer bao gồm một lớp con tự chú ý và một mạng feed-forward kết nối đầy đủ. Kết nối dư (He et al., 2016) được sử dụng xung quanh mỗi lớp con trong hai lớp con, theo sau bởi chuẩn hóa lớp (Ba et al., 2016).

**Tự chú ý** Trong mỗi lớp, Transformer sử dụng nhiều đầu tự chú ý để tổng hợp các vector đầu ra của lớp trước. Đối với lớp Transformer thứ l, đầu ra của một đầu tự chú ý AO^{l,a}; a ∈ [1, A_h] được tính bằng:

Q^{l,a} = H^{l-1}W_Q^{l,a}, K^{l,a} = H^{l-1}W_K^{l,a}, V^{l,a} = H^{l-1}W_V^{l,a}     (2)

A^{l,a} = softmax(Q^{l,a}K^{l,a|}/√d_k)                     (3)

AO^{l,a} = A^{l,a}V^{l,a}                                   (4)

trong đó đầu ra H^{l-1} ∈ R^{|x|×d_h} của lớp trước được chiếu tuyến tính thành ba thành phần truy vấn, khóa và giá trị sử dụng các ma trận tham số W_Q^{l,a}, W_K^{l,a}, W_V^{l,a} ∈ R^{d_h×d_k}, tương ứng. A^{l,a} ∈ R^{|x|×|x|} chỉ ra các phân phối chú ý, được tính bằng tích vô hướng có tỷ lệ của truy vấn và khóa. A_h đại diện cho số lượng đầu tự chú ý. d_k A_h bằng chiều ẩn d_h trong BERT.

### 2.3. Chưng cất Transformer

Chưng cất kiến thức (Hinton et al., 2015; Romero et al., 2015) là huấn luyện mô hình học sinh nhỏ S trên một tập đặc trưng chuyển giao với nhãn mềm và biểu diễn trung gian được cung cấp bởi mô hình giáo viên lớn T. Chưng cất kiến thức được mô hình hóa như việc tối thiểu hóa sự khác biệt giữa các đặc trưng của giáo viên và học sinh:

L_{KD} = ∑_{e∈D} L(f_S(e), f_T(e))                         (5)

Trong đó D biểu thị dữ liệu huấn luyện, f_S() và f_T() chỉ ra các đặc trưng của mô hình học sinh và giáo viên tương ứng, L() đại diện cho hàm mất mát. Sai số bình phương trung bình (MSE) và KL-divergence thường được sử dụng làm hàm mất mát.

Đối với chưng cất LM dựa trên Transformer, xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ, đầu ra lớp embedding, phân phối tự chú ý và đầu ra (trạng thái ẩn) của mỗi lớp Transformer của mô hình giáo viên được sử dụng làm đặc trưng để giúp huấn luyện học sinh. Nhãn mềm và đầu ra lớp embedding được sử dụng trong DistillBERT. TinyBERT và MOBILE BERT tiếp tục sử dụng phân phối tự chú ý và đầu ra của mỗi lớp Transformer. Đối với MOBILE BERT, học sinh được yêu cầu có cùng số lượng lớp như giáo viên của nó để thực hiện chưng cất theo từng lớp. Bên cạnh đó, các mô-đun cổ chai và cổ chai đảo được giới thiệu để giữ kích thước ẩn của giáo viên và học sinh cũng giống nhau. Để chuyển giao kiến thức theo từng lớp, TinyBERT sử dụng hàm đồng nhất để ánh xạ các lớp giáo viên và học sinh. Vì kích thước ẩn của học sinh có thể nhỏ hơn giáo viên của nó, một ma trận tham số được giới thiệu để biến đổi các đặc trưng học sinh.

## 3. Chưng cất Tự chú ý Sâu

Hình 1 đưa ra tổng quan về chưng cất tự chú ý sâu. Ý tưởng chính có ba mặt. Thứ nhất, chúng tôi đề xuất huấn luyện học sinh bằng cách bắt chước sâu sắc mô-đun tự chú ý, là thành phần quan trọng trong Transformer, của lớp cuối cùng của giáo viên. Thứ hai, chúng tôi giới thiệu việc chuyển giao mối quan hệ giữa các giá trị (tức là tích vô hướng có tỷ lệ giữa các giá trị) để đạt được sự bắt chước sâu hơn, bổ sung cho việc thực hiện chuyển giao phân phối chú ý (tức là tích vô hướng có tỷ lệ của truy vấn và khóa) trong mô-đun tự chú ý. Hơn nữa, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên (Mirzadeh et al., 2019) cũng giúp chưng cất các mô hình Transformer được tiền huấn luyện lớn khi khoảng cách kích thước giữa mô hình giáo viên và mô hình học sinh là lớn.

### 3.1. Chuyển giao Phân phối Tự chú ý

Cơ chế chú ý (Bahdanau et al., 2015) đã là một thành phần mạng thần kinh rất thành công cho các tác vụ NLP, cũng quan trọng đối với các LM được tiền huấn luyện. Một số công trình chỉ ra rằng các phân phối tự chú ý của các LM được tiền huấn luyện nắm bắt một hệ thống phong phú các thông tin ngôn ngữ học (Jawahar et al., 2019; Clark et al., 2019). Việc chuyển giao phân phối tự chú ý đã được sử dụng trong các công trình trước đây cho chưng cất Transformer (Jiao et al., 2019; Sun et al., 2019b; Aguilar et al., 2019). Chúng tôi cũng sử dụng các phân phối tự chú ý để giúp huấn luyện học sinh. Cụ thể, chúng tôi tối thiểu hóa KL-divergence giữa các phân phối tự chú ý của giáo viên và học sinh:

L_{AT} = \frac{1}{A_h|x|} \sum_{a=1}^{A_h} \sum_{t=1}^{|x|} D_{KL}(A_T^{L,a,t} || A_S^{M,a,t})     (6)

Trong đó |x| và A_h đại diện cho độ dài chuỗi và số lượng đầu chú ý. L và M đại diện cho số lượng lớp cho giáo viên và học sinh. A_T^L và A_S^M là các phân phối chú ý của lớp Transformer cuối cùng cho giáo viên và học sinh, tương ứng. Chúng được tính bằng tích vô hướng có tỷ lệ của truy vấn và khóa.

Khác với các công trình trước đây chuyển giao kiến thức của giáo viên theo từng lớp, chúng tôi chỉ sử dụng các bản đồ chú ý của lớp Transformer cuối cùng của giáo viên. Việc chưng cất kiến thức chú ý của lớp Transformer cuối cùng cho phép linh hoạt hơn về số lượng lớp của các mô hình học sinh, tránh việc nỗ lực tìm ánh xạ lớp tốt nhất.

### 3.2. Chuyển giao Quan hệ Giá trị Tự chú ý

Bổ sung cho các phân phối chú ý, chúng tôi đề xuất sử dụng mối quan hệ giữa các giá trị trong mô-đun tự chú ý để hướng dẫn việc huấn luyện học sinh. Quan hệ giá trị được tính bằng tích vô hướng có tỷ lệ đa đầu giữa các giá trị. KL-divergence giữa quan hệ giá trị của giáo viên và học sinh được sử dụng làm mục tiêu huấn luyện:

VR_T^{L,a} = softmax(\frac{V_T^{L,a}V_T^{L,a|}}{\sqrt{d_k}})     (7)

VR_S^{M,a} = softmax(\frac{V_S^{M,a}V_S^{M,a|}}{\sqrt{d_k'}})    (8)

L_{VR} = \frac{1}{A_h|x|} \sum_{a=1}^{A_h} \sum_{t=1}^{|x|} D_{KL}(VR_T^{L,a,t} || VR_S^{M,a,t})     (9)

Trong đó V_T^{L,a} ∈ R^{|x|×d_k} và V_S^{M,a} ∈ R^{|x|×d_k'} là các giá trị của một đầu chú ý trong mô-đun tự chú ý cho lớp Transformer cuối cùng của giáo viên và học sinh. VR_T^L ∈ R^{A_h×|x|×|x|} và VR_S^M ∈ R^{A_h×|x|×|x|} là quan hệ giá trị của lớp Transformer cuối cùng cho giáo viên và học sinh, tương ứng.

Mất mát huấn luyện được tính bằng cách cộng mất mát chuyển giao phân phối chú ý và mất mát chuyển giao quan hệ giá trị:

L = L_{AT} + L_{VR}                                          (10)

Việc giới thiệu mối quan hệ giữa các giá trị cho phép học sinh bắt chước sâu sắc hành vi tự chú ý của giáo viên. Hơn nữa, việc sử dụng tích vô hướng có tỷ lệ chuyển đổi các vector có chiều ẩn khác nhau thành các ma trận quan hệ có cùng kích thước, điều này cho phép các học sinh sử dụng chiều ẩn linh hoạt hơn và tránh việc giới thiệu thêm tham số để biến đổi biểu diễn của học sinh.

### 3.3. Trợ lý Giáo viên

Theo Mirzadeh et al. (2019), chúng tôi giới thiệu một trợ lý giáo viên (tức là mô hình học sinh có kích thước trung gian) để cải thiện thêm hiệu suất mô hình của các học sinh nhỏ hơn.

Giả sử mô hình giáo viên bao gồm Transformer L lớp với kích thước ẩn d_h, mô hình học sinh có Transformer M lớp với kích thước ẩn d_h'. Đối với các học sinh nhỏ hơn (M ≤ \frac{1}{2}L, d_h' ≤ \frac{1}{2}d_h), chúng tôi đầu tiên chưng cất giáo viên thành một trợ lý giáo viên với Transformer L lớp và kích thước ẩn d_h'. Mô hình trợ lý sau đó được sử dụng làm giáo viên để hướng dẫn việc huấn luyện học sinh cuối cùng. Việc giới thiệu một trợ lý giáo viên bắc cầu khoảng cách kích thước giữa các mô hình giáo viên và học sinh nhỏ hơn, giúp chưng cất các LM được tiền huấn luyện dựa trên Transformer. Hơn nữa, việc kết hợp chưng cất tự chú ý sâu với một trợ lý giáo viên mang lại cải thiện thêm cho các mô hình học sinh nhỏ hơn.

### 3.4. So sánh với Công trình Trước đây

Bảng 1 trình bày so sánh với các phương pháp trước đây (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019b). MOBILE BERT đề xuất sử dụng một mô hình cổ chai đảo được thiết kế đặc biệt, có cùng kích thước mô hình như BERT LARGE, làm giáo viên. Các phương pháp khác sử dụng BERT BASE để thực hiện thí nghiệm. Đối với kiến thức được sử dụng để chưng cất, phương pháp của chúng tôi giới thiệu tích vô hướng có tỷ lệ giữa các giá trị trong mô-đun tự chú ý như kiến thức mới để bắt chước sâu sắc hành vi tự chú ý của giáo viên. TinyBERT và MOBILE BERT chuyển giao kiến thức của giáo viên cho học sinh theo từng lớp. MOBILE BERT giả định học sinh có cùng số lượng lớp như giáo viên của nó. TinyBERT sử dụng chiến lược đồng nhất để xác định ánh xạ lớp. DistillBERT khởi tạo học sinh với các tham số của giáo viên, do đó việc chọn lớp của mô hình giáo viên vẫn cần thiết. MINILM chưng cất kiến thức tự chú ý của lớp Transformer cuối cùng của giáo viên, điều này cho phép số lượng lớp linh hoạt cho các học sinh và giảm bớt nỗ lực tìm ánh xạ lớp tốt nhất. Kích thước ẩn học sinh của DistillBERT và MOBILE BERT được yêu cầu giống như giáo viên của nó. TinyBERT sử dụng ma trận tham số để biến đổi trạng thái ẩn học sinh. Việc sử dụng quan hệ giá trị cho phép các học sinh sử dụng kích thước ẩn tùy ý mà không cần giới thiệu thêm tham số.

## 4. Thí nghiệm

Chúng tôi thực hiện các thí nghiệm chưng cất với các kích thước tham số khác nhau của mô hình học sinh, và đánh giá các mô hình được chưng cất trên các tác vụ downstream bao gồm trả lời câu hỏi trích xuất và benchmark GLUE.

### 4.1. Thiết lập Chưng cất

Chúng tôi sử dụng phiên bản không phân biệt chữ hoa thường của BERT BASE làm giáo viên. BERT BASE (Devlin et al., 2018) là một Transformer 12 lớp với kích thước ẩn 768 và 12 đầu chú ý, chứa khoảng 109M tham số. Số lượng đầu của phân phối chú ý và quan hệ giá trị được đặt là 12 cho các mô hình học sinh. Chúng tôi sử dụng tài liệu của Wikipedia tiếng Anh² và BookCorpus (Zhu et al., 2015) cho dữ liệu tiền huấn luyện, theo xử lý và token hóa WordPiece của Devlin et al. (2018). Kích thước từ vựng là 30,522. Độ dài chuỗi tối đa là 512. Chúng tôi sử dụng Adam (Kingma & Ba, 2015) với β₁ = 0.9, β₂ = 0.999. Chúng tôi huấn luyện mô hình học sinh 6 lớp với kích thước ẩn 768 sử dụng 1024 làm kích thước batch và 5e-4 làm tốc độ học đỉnh trong 400,000 bước. Đối với các mô hình học sinh có kiến trúc khác, kích thước batch và tốc độ học đỉnh được đặt lần lượt là 256 và 3e-4. Chúng tôi sử dụng làm ấm tuyến tính trong 4,000 bước đầu tiên và suy giảm tuyến tính. Tỷ lệ dropout là 0.1. Suy giảm trọng số là 0.01.

Chúng tôi cũng sử dụng một mô hình Transformer được tiền huấn luyện nội bộ với kích thước BERT BASE làm mô hình giáo viên, và chưng cất nó thành các mô hình học sinh 12 lớp và 6 lớp với kích thước ẩn 384. Đối với mô hình 12 lớp, chúng tôi sử dụng Adam (Kingma & Ba, 2015) với β₁ = 0.9, β₂ = 0.98. Mô hình được huấn luyện sử dụng 2048 làm kích thước batch và 6e-4 làm tốc độ học đỉnh trong 400,000 bước. Kích thước batch và tốc độ học đỉnh được đặt lần lượt là 512 và 4e-4 cho mô hình 6 lớp. Các siêu tham số còn lại giống như các mô hình được chưng cất dựa trên BERT ở trên.

Đối với việc huấn luyện các mô hình MINILM đa ngữ, chúng tôi sử dụng Adam (Kingma & Ba, 2015) với β₁ = 0.9, β₂ = 0.999. Chúng tôi huấn luyện mô hình học sinh 12 lớp sử dụng 256 làm kích thước batch và 3e-4 làm tốc độ học đỉnh trong 1,000,000 bước. Mô hình học sinh 6 lớp được huấn luyện sử dụng 512 làm kích thước batch và 6e-4 làm tốc độ học đỉnh trong 400,000 bước.

Chúng tôi chưng cất các mô hình học sinh sử dụng 8 GPU V100 với huấn luyện độ chính xác hỗn hợp. Theo Sun et al. (2019a) và Jiao et al. (2019), thời gian suy luận được đánh giá trên tập huấn luyện QNLI với cùng siêu tham số. Chúng tôi báo cáo thời gian chạy trung bình của 100 batch trên một GPU P100 đơn.

### 4.2. Các Tác vụ Downstream

Theo tiền huấn luyện mô hình ngôn ngữ trước đây (Devlin et al., 2018; Liu et al., 2019) và chưng cất mô hình ngôn ngữ được tiền huấn luyện bất khả tri tác vụ (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019b), chúng tôi đánh giá các mô hình được chưng cất trên trả lời câu hỏi trích xuất và benchmark GLUE.

**Trả lời Câu hỏi Trích xuất** Cho một đoạn văn P, tác vụ là chọn một khoảng văn bản liên tục trong đoạn văn bằng cách dự đoán vị trí bắt đầu và kết thúc để trả lời câu hỏi Q. Chúng tôi đánh giá trên SQuAD 2.0 (Rajpurkar et al., 2018), đã phục vụ như một benchmark trả lời câu hỏi chính. Theo BERT (Devlin et al., 2018), chúng tôi đóng gói các token câu hỏi và đoạn văn lại với các token đặc biệt, để tạo thành đầu vào: "[CLS] Q [SEP] P [SEP]". Hai lớp đầu ra tuyến tính được giới thiệu để dự đoán xác suất của mỗi token là vị trí bắt đầu và kết thúc của khoảng trả lời. Các câu hỏi không có câu trả lời được xử lý như có khoảng trả lời với bắt đầu và kết thúc tại token [CLS].

**GLUE** Benchmark Đánh giá Hiểu biết Ngôn ngữ Tổng quát (GLUE) (Wang et al., 2019) bao gồm chín tác vụ phân loại cấp câu, bao gồm Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2018), Stanford Sentiment Treebank (SST) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005), Semantic Textual Similarity Benchmark (STS) (Cer et al., 2017), Quora Question Pairs (QQP) (Chen et al., 2018), Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018), Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) và Winograd Natural Language Inference (WNLI) (Levesque et al., 2012). Chúng tôi thêm một bộ phân loại tuyến tính trên đỉnh token [CLS] để dự đoán xác suất nhãn.

### 4.3. Kết quả Chính

Các công trình trước đây (Sanh et al., 2019; Sun et al., 2019a; Jiao et al., 2019) thường chưng cất BERT BASE thành một mô hình học sinh 6 lớp với kích thước ẩn 768. Chúng tôi đầu tiên thực hiện các thí nghiệm chưng cất sử dụng cùng kiến trúc học sinh. Kết quả trên các tập dev SQuAD 2.0 và GLUE được trình bày trong Bảng 2. Vì MOBILE BERT chưng cất một giáo viên được thiết kế đặc biệt với các mô-đun cổ chai đảo, có cùng kích thước mô hình như BERT LARGE, thành một học sinh 24 lớp sử dụng các mô-đun cổ chai, chúng tôi không so sánh các mô hình của chúng tôi với MOBILE BERT. MINILM vượt trội so với DistillBERT³ và TinyBERT⁴ trên hầu hết các tác vụ. Mô hình của chúng tôi vượt trội so với hai mô hình tối tân bằng 3.0+% F1 trên SQuAD 2.0 và 5.0+% độ chính xác trên CoLA. Chúng tôi trình bày thời gian suy luận cho các mô hình có kích thước tham số khác nhau trong Bảng 4. Mô hình học sinh 6 lớp 768 chiều của chúng tôi nhanh hơn 2.0 lần so với BERT BASE gốc, trong khi giữ lại hơn 99% hiệu suất trên nhiều tác vụ đa dạng, như SQuAD 2.0 và MNLI.

Chúng tôi cũng thực hiện thí nghiệm cho các mô hình học sinh nhỏ hơn. Chúng tôi so sánh MINILM với MLM-KD (chưng cất kiến thức sử dụng xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ) và TinyBERT được triển khai bởi chúng tôi, được huấn luyện sử dụng cùng dữ liệu và siêu tham số. Kết quả trên các tập dev SQuAD 2.0, MNLI và SST-2 được hiển thị trong Bảng 3. MINILM vượt trội so với chưng cất nhãn mềm và TinyBERT được triển khai bởi chúng tôi trên ba tác vụ. Chưng cất tự chú ý sâu cũng hiệu quả cho các mô hình nhỏ hơn. Hơn nữa, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên⁵ cũng hữu ích trong chưng cất LM được tiền huấn luyện dựa trên Transformer, đặc biệt cho các mô hình nhỏ hơn. Việc kết hợp chưng cất tự chú ý sâu với một trợ lý giáo viên đạt được cải thiện thêm cho các mô hình học sinh nhỏ hơn.

### 4.4. Nghiên cứu Khử bỏ

Chúng tôi thực hiện các thử nghiệm khử bỏ trên một số tác vụ để phân tích đóng góp của việc chuyển giao quan hệ giá trị tự chú ý. Kết quả dev của SQuAD 2.0, MNLI và SST-2 được minh họa trong Bảng 5, việc sử dụng chuyển giao quan hệ giá trị tự chú ý đóng góp tích cực vào kết quả cuối cùng cho các mô hình học sinh có kích thước tham số khác nhau. Việc chưng cất kiến thức chi tiết của quan hệ giá trị giúp mô hình học sinh bắt chước sâu sắc hành vi tự chú ý của giáo viên, điều này cải thiện thêm hiệu suất mô hình.

Chúng tôi cũng so sánh các hàm mất mát khác nhau trên các giá trị trong mô-đun tự chú ý. Chúng tôi so sánh quan hệ giá trị được đề xuất với sai số bình phương trung bình (MSE) trên các giá trị giáo viên và học sinh. Một ma trận tham số bổ sung được giới thiệu để biến đổi giá trị học sinh nếu chiều ẩn của học sinh nhỏ hơn giáo viên của nó. Kết quả dev trên ba tác vụ được trình bày trong Bảng 6. Việc sử dụng quan hệ giá trị đạt được hiệu suất tốt hơn. Cụ thể, phương pháp của chúng tôi mang lại khoảng 1.0% cải thiện F1 trên benchmark SQuAD. Hơn nữa, không cần thiết phải giới thiệu thêm tham số cho phương pháp của chúng tôi. Chúng tôi cũng đã thử chuyển giao mối quan hệ giữa các trạng thái ẩn. Nhưng chúng tôi thấy hiệu suất của các mô hình học sinh không ổn định cho các mô hình giáo viên khác nhau.

Để chỉ ra tính hiệu quả của việc chưng cất kiến thức tự chú ý của lớp Transformer cuối cùng của giáo viên, chúng tôi so sánh phương pháp của chúng tôi với chưng cất theo từng lớp. Chúng tôi chuyển giao cùng kiến thức và áp dụng chiến lược đồng nhất như trong Jiao et al. (2019) để ánh xạ các lớp giáo viên và học sinh để thực hiện chưng cất theo từng lớp. Kết quả dev trên ba tác vụ được trình bày trong Bảng 7. MINILM đạt được kết quả tốt hơn. Nó cũng làm giảm khó khăn trong ánh xạ lớp giữa giáo viên và học sinh. Bên cạnh đó, việc chưng cất lớp Transformer cuối cùng của giáo viên đòi hỏi ít tính toán hơn so với chưng cất theo từng lớp, dẫn đến tốc độ huấn luyện nhanh hơn.

## 5. Thảo luận

### 5.1. Giáo viên Tốt hơn Học sinh Tốt hơn

Chúng tôi báo cáo kết quả của MINILM được chưng cất từ một mô hình Transformer được tiền huấn luyện nội bộ theo UNILM (Dong et al., 2019; Bao et al., 2020) với kích thước BERT BASE. Mô hình giáo viên được huấn luyện sử dụng các tập dữ liệu tiền huấn luyện tương tự như trong RoBERTa BASE (Liu et al., 2019), bao gồm 160GB kho văn bản từ Wikipedia tiếng Anh, BookCorpus (Zhu et al., 2015), OpenWebText⁶, CC-News (Liu et al., 2019), và Stories (Trinh & Le, 2018). Chúng tôi chưng cất mô hình giáo viên thành các mô hình 12 lớp và 6 lớp với kích thước ẩn 384 sử dụng cùng kho văn bản. Mô hình 12x384 được sử dụng làm trợ lý giáo viên để huấn luyện mô hình 6x384. Chúng tôi trình bày kết quả dev của SQuAD 2.0 và benchmark GLUE trong Bảng 8, kết quả của MINILM được cải thiện đáng kể. MINILM 12x384 đạt được tăng tốc 2.7 lần trong khi thực hiện cạnh tranh tốt hơn so với BERT BASE trong SQuAD 2.0 và các tập dữ liệu benchmark GLUE.

### 5.2. MINILM cho Các Tác vụ NLG

Chúng tôi cũng đánh giá MINILM trên các tác vụ sinh ngôn ngữ tự nhiên, như sinh câu hỏi và tóm tắt trừu tượng. Theo Dong et al. (2019), chúng tôi tinh chỉnh MINILM như một mô hình chuỗi-sang-chuỗi bằng cách sử dụng mặt nạ tự chú ý cụ thể.

**Sinh Câu hỏi** Chúng tôi thực hiện thí nghiệm cho tác vụ sinh câu hỏi có nhận thức câu trả lời (Du & Cardie, 2018). Cho một đoạn văn đầu vào và một câu trả lời, tác vụ là sinh một câu hỏi hỏi về câu trả lời. Tập dữ liệu SQuAD 1.1 (Rajpurkar et al., 2016) được sử dụng để đánh giá. Kết quả của MINILM, UNILM LARGE và một số mô hình tối tân được trình bày trong Bảng 9, các mô hình được chưng cất 12x384 và 6x384 của chúng tôi đạt được hiệu suất cạnh tranh trên tác vụ sinh câu hỏi.

**Tóm tắt Trừu tượng** Chúng tôi đánh giá MINILM trên hai tập dữ liệu tóm tắt trừu tượng, tức là XSum (Narayan et al., 2018), và phiên bản không ẩn danh của CNN/DailyMail (See et al., 2017). Tác vụ sinh là tóm tắt một tài liệu thành một bản tóm tắt ngắn gọn và lưu loát, trong khi truyền đạt thông tin chính của nó. Chúng tôi báo cáo điểm ROUGE (Lin, 2004) trên các tập dữ liệu. Bảng 10 trình bày kết quả của MINILM, baseline, một số mô hình tối tân và các mô hình Transformer được tiền huấn luyện. Mô hình 12x384 của chúng tôi vượt trội so với phương pháp dựa trên BERT BERTSUMABS (Liu & Lapata, 2019) và mô hình chuỗi-sang-chuỗi được tiền huấn luyện MASS BASE (Song et al., 2019) với ít tham số hơn nhiều. Hơn nữa, MINILM 6x384 của chúng tôi cũng đạt được hiệu suất cạnh tranh.

### 5.3. MINILM Đa ngữ

Chúng tôi thực hiện thí nghiệm về chưng cất kiến thức bất khả tri tác vụ của các mô hình được tiền huấn luyện đa ngữ. Chúng tôi sử dụng XLM-R Base⁷ (Conneau et al., 2019) làm giáo viên và chưng cất mô hình thành các mô hình 12 lớp và 6 lớp với kích thước ẩn 384 sử dụng cùng kho văn bản. Mô hình 6x384 được huấn luyện sử dụng mô hình 12x384 làm trợ lý giáo viên. Cho kích thước từ vựng của các mô hình được tiền huấn luyện đa ngữ lớn hơn nhiều so với các mô hình đơn ngữ (30k cho BERT đơn ngữ, 250k cho XLM-R), chưng cất nhãn mềm cho các mô hình được tiền huấn luyện đa ngữ đòi hỏi nhiều tính toán hơn. MINILM chỉ sử dụng kiến thức tự chú ý sâu của lớp Transformer cuối cùng của giáo viên. Tốc độ huấn luyện của MINILM nhanh hơn nhiều so với chưng cất nhãn mềm cho các mô hình được tiền huấn luyện đa ngữ.

Chúng tôi đánh giá các mô hình học sinh trên benchmark suy luận ngôn ngữ tự nhiên đa ngữ (XNLI) (Conneau et al., 2018) và benchmark trả lời câu hỏi đa ngữ (MLQA) (Lewis et al., 2019b).

**XNLI** Bảng 11 trình bày kết quả XNLI của các học sinh được chưng cất và một số LM được tiền huấn luyện. Theo Conneau et al. (2019), chúng tôi chọn mô hình đơn tốt nhất trên tập dev chung của tất cả các ngôn ngữ. Chúng tôi trình bày số lượng tham số Transformer và embedding cho các mô hình được tiền huấn luyện đa ngữ khác nhau và các mô hình được chưng cất của chúng tôi trong Bảng 12. MINILM đạt được hiệu suất cạnh tranh trên XNLI với ít tham số Transformer hơn nhiều. Hơn nữa, MINILM 12x384 so sánh thuận lợi với mBERT (Devlin et al., 2018) và XLM (Lample & Conneau, 2019) được huấn luyện trên mục tiêu MLM.

**MLQA** Bảng 13 hiển thị kết quả trả lời câu hỏi đa ngữ. Theo Lewis et al. (2019b), chúng tôi áp dụng SQuAD 1.1 làm dữ liệu huấn luyện và sử dụng dữ liệu phát triển MLQA tiếng Anh để dừng sớm. MINILM 12x384 thực hiện cạnh tranh tốt hơn so với mBERT và XLM. MINILM 6 lớp của chúng tôi cũng đạt được hiệu suất cạnh tranh.

## 6. Công trình Liên quan

### 6.1. Các Mô hình Ngôn ngữ Được Tiền huấn luyện

Tiền huấn luyện không giám sát của các mô hình ngôn ngữ (Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Song et al., 2019; Dong et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019; Lewis et al., 2019a; Raffel et al., 2019) đã đạt được cải thiện đáng kể cho một loạt rộng các tác vụ NLP. Các phương pháp đầu tiên cho tiền huấn luyện (Peters et al., 2018; Radford et al., 2018) dựa trên các mô hình ngôn ngữ tiêu chuẩn. Gần đây, BERT (Devlin et al., 2018) đề xuất sử dụng mục tiêu mô hình ngôn ngữ có mặt nạ để huấn luyện một bộ mã hóa Transformer hai chiều sâu, học các tương tác giữa ngữ cảnh trái và phải. Liu et al. (2019) chỉ ra rằng hiệu suất rất mạnh có thể đạt được bằng cách huấn luyện mô hình lâu hơn trên nhiều dữ liệu hơn. Joshi et al. (2019) mở rộng BERT bằng cách che các khoảng ngẫu nhiên liên tục. Yang et al. (2019) dự đoán các token bị che một cách tự hồi quy theo thứ tự hoán vị.

Để mở rộng khả năng áp dụng của các Transformer được tiền huấn luyện cho các tác vụ NLG. Dong et al. (2019) mở rộng BERT bằng cách sử dụng các mặt nạ tự chú ý cụ thể để tối ưu hóa chung các mục tiêu mô hình ngôn ngữ có mặt nạ hai chiều, một chiều và chuỗi-sang-chuỗi. Raffel et al. (2019) sử dụng một Transformer mã hóa-giải mã và thực hiện tiền huấn luyện chuỗi-sang-chuỗi bằng cách dự đoán các token bị che trong bộ mã hóa và bộ giải mã. Khác với Raffel et al. (2019), Lewis et al. (2019a) dự đoán các token một cách tự hồi quy trong bộ giải mã.

### 6.2. Chưng cất Kiến thức

Chưng cất kiến thức đã chứng minh là một cách hứa hẹn để nén các mô hình lớn trong khi duy trì độ chính xác. Nó chuyển giao kiến thức của một mô hình lớn hoặc một tổ hợp các mạng thần kinh (giáo viên) cho một mô hình nhẹ đơn (học sinh). Hinton et al. (2015) đầu tiên đề xuất chuyển giao kiến thức của giáo viên cho học sinh bằng cách sử dụng các phân phối mục tiêu mềm để huấn luyện mô hình được chưng cất. Romero et al. (2015) giới thiệu các biểu diễn trung gian từ các lớp ẩn của giáo viên để hướng dẫn việc huấn luyện học sinh. Kiến thức của các bản đồ chú ý (Zagoruyko & Komodakis, 2017; Hu et al., 2018) cũng được giới thiệu để giúp huấn luyện.

Trong công trình này, chúng tôi tập trung vào chưng cất kiến thức bất khả tri tác vụ của các mô hình ngôn ngữ được tiền huấn luyện dựa trên Transformer lớn. Đã có một số công trình chưng cất các mô hình ngôn ngữ được tinh chỉnh trên các tác vụ downstream đặc thù tác vụ. Tang et al. (2019) chưng cất BERT được tinh chỉnh thành một LSTM hai chiều cực kỳ nhỏ. Turc et al. (2019a) khởi tạo học sinh với một LM được tiền huấn luyện nhỏ trong quá trình chưng cất đặc thù tác vụ. Sun et al. (2019a) giới thiệu các trạng thái ẩn từ mỗi k lớp của giáo viên để thực hiện chưng cất kiến thức theo từng lớp. Aguilar et al. (2019) tiếp tục giới thiệu kiến thức của các phân phối tự chú ý và đề xuất các phương pháp chưng cất tiến bộ và xếp chồng. Chưng cất đặc thù tác vụ đòi hỏi phải tinh chỉnh các LM được tiền huấn luyện lớn trên các tác vụ downstream trước và sau đó thực hiện chuyển giao kiến thức. Quy trình tinh chỉnh các LM được tiền huấn luyện lớn tốn kém và mất thời gian, đặc biệt đối với các tập dữ liệu lớn.

Đối với chưng cất bất khả tri tác vụ, mô hình được chưng cất bắt chước LM được tiền huấn luyện lớn gốc và có thể được tinh chỉnh trực tiếp trên các tác vụ downstream. Trong thực tế, nén bất khả tri tác vụ của các LM được tiền huấn luyện là mong muốn hơn. MiniBERT (Tsai et al., 2019) sử dụng các phân phối mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ để hướng dẫn việc huấn luyện mô hình học sinh đa ngữ và chỉ ra tính hiệu quả của nó trên các tác vụ gắn nhãn chuỗi. DistillBERT (Sanh et al., 2019) sử dụng nhãn mềm và đầu ra embedding của giáo viên để huấn luyện học sinh. TinyBERT (Jiao et al., 2019) và MOBILE-BERT (Sun et al., 2019b) tiếp tục giới thiệu các phân phối tự chú ý và trạng thái ẩn để huấn luyện học sinh. MOBILE-BERT sử dụng các mô-đun cổ chai đảo và cổ chai cho giáo viên và học sinh để làm cho chiều ẩn của chúng giống nhau. Mô hình học sinh của MOBILE BERT được yêu cầu có cùng số lượng lớp như giáo viên của nó để thực hiện chưng cất theo từng lớp. Bên cạnh đó, MOBILE BERT đề xuất một sơ đồ tiến bộ từ dưới lên trên để chuyển giao kiến thức của giáo viên. TinyBERT sử dụng chiến lược đồng nhất để ánh xạ các lớp của giáo viên và học sinh khi chúng có số lượng lớp khác nhau, và một ma trận tuyến tính được giới thiệu để biến đổi các trạng thái ẩn của học sinh để có cùng chiều như giáo viên. TinyBERT cũng giới thiệu chưng cất đặc thù tác vụ và tăng cường dữ liệu cho các tác vụ downstream, điều này mang lại cải thiện thêm.

Khác với các công trình trước đây, phương pháp của chúng tôi sử dụng các phân phối tự chú ý và quan hệ giá trị của lớp Transformer cuối cùng của giáo viên để giúp học sinh bắt chước sâu sắc hành vi tự chú ý của giáo viên. Việc sử dụng kiến thức của lớp Transformer cuối cùng thay vì chưng cất theo từng lớp tránh các hạn chế về số lượng lớp học sinh và nỗ lực tìm ánh xạ lớp tốt nhất. Việc chưng cất mối quan hệ giữa các giá trị tự chú ý cho phép kích thước ẩn của học sinh linh hoạt hơn và tránh việc giới thiệu các ma trận tuyến tính để biến đổi biểu diễn học sinh.

## 7. Kết luận

Trong công trình này, chúng tôi đề xuất một phương pháp chưng cất kiến thức đơn giản và hiệu quả để nén các mô hình ngôn ngữ được tiền huấn luyện dựa trên Transformer lớn. Học sinh được huấn luyện bằng cách bắt chước sâu sắc các mô-đun tự chú ý của giáo viên, là các thành phần quan trọng của mạng Transformer. Chúng tôi đề xuất sử dụng các phân phối tự chú ý và quan hệ giá trị của lớp Transformer cuối cùng của giáo viên để hướng dẫn việc huấn luyện học sinh, điều này hiệu quả và linh hoạt cho các mô hình học sinh. Hơn nữa, chúng tôi chỉ ra rằng việc giới thiệu một trợ lý giáo viên cũng giúp chưng cất LM được tiền huấn luyện dựa trên Transformer, và chưng cất tự chú ý sâu được đề xuất có thể tăng cường hiệu suất hơn nữa. Mô hình học sinh của chúng tôi được chưng cất từ BERT BASE giữ lại độ chính xác cao trên SQuAD 2.0 và các tác vụ benchmark GLUE, và vượt trội so với các baseline tối tân. Chưng cất tự chú ý sâu cũng có thể được áp dụng để nén các mô hình được tiền huấn luyện có kích thước lớn hơn. Chúng tôi để lại điều này như công việc tương lai.

[Phần còn lại của tài liệu bao gồm Tài liệu tham khảo và Phụ lục sẽ được dịch tiếp theo nếu cần]