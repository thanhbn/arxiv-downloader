# 2405.11582.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2405.11582.pdf
# Kích thước file: 1110782 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
SLAB: Transformers Hiệu Quả với Cơ Chế Attention Tuyến Tính Đơn Giản và Batch Normalization Tái Tham Số Hóa Tiến Tiến
Jialong Guo1 *Xinghao Chen1 *Yehui Tang1Yunhe Wang1
Tóm tắt
Transformers đã trở thành kiến trúc nền tảng cho cả các tác vụ xử lý ngôn ngữ tự nhiên và thị giác máy tính. Tuy nhiên, chi phí tính toán cao khiến việc triển khai trên các thiết bị có hạn chế tài nguyên khá thách thức. Bài báo này nghiên cứu các module nút thắt tính toán của transformer hiệu quả, tức là các lớp chuẩn hóa và các module attention. LayerNorm thường được sử dụng trong kiến trúc transformer nhưng không thân thiện với tính toán do việc tính thống kê trong quá trình suy luận. Tuy nhiên, việc thay thế LayerNorm bằng BatchNorm hiệu quả hơn trong transformer thường dẫn đến hiệu suất kém hơn và sự sụp đổ trong quá trình huấn luyện. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp mới có tên PRepBN để dần dần thay thế LayerNorm bằng BatchNorm tái tham số hóa trong quá trình huấn luyện. Hơn nữa, chúng tôi đề xuất một module attention tuyến tính đơn giản (SLA) vừa đơn giản vừa hiệu quả để đạt được hiệu suất mạnh mẽ. Các thí nghiệm mở rộng về phân loại hình ảnh cũng như phát hiện đối tượng chứng minh tính hiệu quả của phương pháp được đề xuất. Ví dụ, SLAB-Swin của chúng tôi đạt được độ chính xác top-1 83.6% trên ImageNet-1K với độ trễ 16.2ms, ít hơn 2.4ms so với Flatten-Swin với độ chính xác cao hơn 0.1%. Chúng tôi cũng đánh giá phương pháp của mình cho tác vụ mô hình hóa ngôn ngữ và đạt được hiệu suất tương đương và độ trễ thấp hơn. Mã nguồn được công bố tại https://github.com/xinghaochen/SLAB
và https://github.com/mindspore-lab/models/.

1. Giới thiệu
Được giới thiệu ban đầu cho các tác vụ trong xử lý ngôn ngữ tự nhiên (Vaswani et al., 2017), kiến trúc transformer đã
*Đóng góp bằng nhau1Huawei Noah's Ark Lab. Liên hệ
với: Xinghao Chen <xinghao.chen@huawei.com >, Yunhe Wang
<yunhe.wang@huawei.com >.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học Máy, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.nhanh chóng nổi lên như một mô hình ưu việt trong bối cảnh của các mô hình ngôn ngữ. Ảnh hưởng của nó đã mở rộng đáng kể với việc giới thiệu Vision Transformer (ViT) (Dosovitskiy et al., 2020), minh họa hiệu quả và tính linh hoạt của các kiến trúc dựa trên transformer. Những kiến trúc này đã chứng minh khả năng đạt được các điểm chuẩn hiệu suất cạnh tranh so với mạng nơ-ron tích chập (CNNs) trên các tác vụ thị giác đa dạng (Han et al., 2022; Wang et al., 2022; Zheng et al., 2023; Tang et al., 2023a; Carion et al., 2020; Xu et al., 2023). Do hiệu suất mạnh mẽ của nó, transformer đã trở thành kiến trúc chủ đạo trong học sâu. Tuy nhiên, nhu cầu tính toán của kiến trúc transformer đặt ra một thách thức đáng kể, chủ yếu do độ phức tạp tính toán bậc hai của cơ chế attention và sự cần thiết của việc tính thống kê trực tuyến của thành phần LayerNorm.

Nhiều nỗ lực đã được hướng tới việc nâng cao hiệu quả của kiến trúc transformer (Tang et al., 2024; Wu et al., 2023; Tang et al., 2023b). Một số phương pháp đã tìm cách giảm thiểu độ phức tạp tính toán bằng cách hạn chế phạm vi tương tác token trong các cơ chế self-attention, chẳng hạn như việc downsample các ma trận key và value (Wang et al., 2021), triển khai các mẫu attention toàn cục thưa (Child et al., 2019), và tính self-attention trong các cửa sổ nhỏ hơn (Tu et al., 2022; Liu et al., 2021; Dong et al., 2022). Trong khi đó, attention tuyến tính nổi lên như một chiến lược thay thế để nâng cao hiệu quả tính toán bằng cách phân tách cơ chế attention thành chi phí tính toán tuyến tính (Katharopoulos et al., 2020; Cai et al., 2022; Han et al., 2023; You et al., 2023), tuy nhiên việc đạt được sự cân bằng tốt giữa hiệu quả và độ chính xác vẫn là một nhiệm vụ thách thức. Hơn nữa, có một số khám phá về việc thay thế LayerNorm (LN) bằng BatchNorm (BN) trong transformers, được thúc đẩy bởi chi phí tính toán bổ sung mà LayerNorm gây ra trong quá trình suy luận. Yang et al. (2022) đề xuất thêm một lớp BatchNorm ở giữa hai lớp tuyến tính trong mạng feed forward để ổn định quá trình huấn luyện. Tuy nhiên, vẫn tồn tại khoảng cách hiệu suất giữa các transformers dựa trên LayerNorm và dựa trên BatchNorm.

Trong bài báo này, chúng tôi tập trung vào việc có được các kiến trúc transformer hiệu quả bằng cách đào sâu vào các module không hiệu quả về tính toán, tức là các lớp chuẩn hóa và các module attention. Chúng tôi đầu tiên khám phá việc thay thế LayerNorm bằng BatchNorm để tăng tốc suy luận cho transformer. BatchNorm dẫn đến độ trễ suy luận thấp hơn nhưng có thể gây ra sự sụp đổ huấn luyện và hiệu suất kém hơn, trong khi LayerNorm có thể ổn định quá trình huấn luyện nhưng có chi phí tính toán bổ sung trong quá trình suy luận. Để giải quyết điều này, chúng tôi đầu tiên đề xuất một chiến lược tiến tiến để dần dần thay thế LayerNorm bằng BatchNorm bằng cách sử dụng một siêu tham số để kiểm soát tỷ lệ của cả hai lớp chuẩn hóa. Ban đầu kiến trúc transformer được chi phối bởi LayerNorm và dần dần chuyển sang BatchNorm thuần túy ở cuối quá trình huấn luyện. Chiến lược này hiệu quả giảm thiểu rủi ro sụp đổ huấn luyện và cũng loại bỏ nhu cầu tính thống kê trong quá trình suy luận. Ngoài chiến lược tiến tiến, chúng tôi cũng đề xuất một công thức tái tham số hóa mới cho BatchNorm (RepBN), để nâng cao độ ổn định huấn luyện và hiệu suất tổng thể.

Hơn nữa, chi phí tính toán của attention là quan trọng đối với transformer hiệu quả và các phương pháp trước đây gặp khó khăn trong việc đạt được sự cân bằng tốt về hiệu quả và độ chính xác. Để giải quyết điều này, chúng tôi đề xuất một module attention tuyến tính đơn giản (SLA) sử dụng ReLU làm hàm kernel và kết hợp một convolution depth-wise để thực hiện nâng cao đặc trưng cục bộ. Cơ chế attention được đề xuất hiệu quả hơn attention tuyến tính trước đây nhưng vẫn đạt được hiệu suất tương đương.

Chúng tôi đánh giá rộng rãi phương pháp được đề xuất cho các kiến trúc khác nhau trên các điểm chuẩn khác nhau. BatchNorm tái tham số hóa tiến tiến của chúng tôi cho thấy hiệu suất mạnh mẽ cho các tác vụ phân loại hình ảnh và phát hiện đối tượng, đạt được độ chính xác tương tự với độ trễ suy luận thấp hơn. Hơn nữa, kết hợp với RepBN tiến tiến và module attention tuyến tính đơn giản, SLAB transformer của chúng tôi đạt được độ chính xác cạnh tranh so với Flatten transformer với hiệu quả tính toán được cải thiện. Ví dụ, SLAB-Swin-S đạt được độ chính xác Top-1 83.6% trên ImageNet-1K với độ trễ 16.2ms, ít hơn 2.4ms so với Flatten-Swin-S với độ chính xác cao hơn 0.1%. Chúng tôi cũng đánh giá phương pháp của mình cho tác vụ mô hình hóa ngôn ngữ và đạt được hiệu suất tương đương và độ trễ suy luận thấp hơn.

2. Công trình liên quan
2.1. Kiến trúc Hiệu quả cho Transformers
Với sự ra đời của Vision Transformer (ViT) tiên phong (Dosovitskiy et al., 2020), tiềm năng của kiến trúc transformer cho các tác vụ thị giác máy tính đã được khám phá rất nhiều. Nhiều nhà nghiên cứu khác nhau đã dành công sức cho lĩnh vực này để làm cho kiến trúc dựa trên transformer hiệu quả và mạnh mẽ hơn. Touvron et al. (2021) đề xuất DeiT sử dụng chưng cất để đạt được hiệu suất mạnh mẽ chỉ với việc huấn luyện trên ImageNet1K. Liu et al. (Liu et al., 2021) đề xuất Swin Transformer, giới thiệu sơ đồ cửa sổ trượt và mang lại hiệu quả lớn hơn. Vì tính toán self-attention bị giới hạn trong một cửa sổ nhỏ, transformer này có độ phức tạp tính toán tuyến tính. Một số công trình cải thiện thiết kế của mẫu thưa để nâng cao tương tác của mỗi token, chẳng hạn như CSwin (Dong et al., 2022). Bên cạnh đó, cơ chế attention động cố gắng kiểm soát việc key/value tương tác với query thích ứng với dữ liệu, chẳng hạn như DAT++ (Xia et al., 2023) và BiFormer (Zhu et al., 2023).

Ngoài các phương pháp trên, attention tuyến tính là một hướng nghiên cứu phổ biến để giảm độ phức tạp tính toán cho transformer. Nhiều cơ chế hiệu quả đã được đề xuất để thay thế hàm softmax. Ví dụ, Performers (Choromanski et al., 2020) sử dụng phương pháp đặc trưng ngẫu nhiên trực giao dương để xấp xỉ softmax. Hydra attention (Bolya et al., 2022) chọn độ tương tự cosine làm hàm kernel. Flatten Transformer (Han et al., 2023) thiết kế một hàm tập trung để cải thiện khả năng tập trung của attention tuyến tính.

2.2. Chuẩn hóa cho Transformers
Chuẩn hóa được biết đến như một phương pháp hữu ích để làm cho huấn luyện ổn định và tăng hiệu suất. Ngày nay, nhiều phương pháp chuẩn hóa khác nhau đã được đề xuất, chẳng hạn như BatchNorm (Ioffe & Szegedy, 2015), LayerNorm (Ba et al., 2016), InstanceNorm (Ulyanov et al., 2016), GroupNorm (Wu & He, 2018), MABN (Yan et al., 2020) và UN (Yang et al., 2022). BatchNorm được sử dụng rộng rãi trong các mạng tích chập và LayerNorm thường được sử dụng cho các mạng như transformer và LSTM.

Chuẩn hóa có thể được phân loại thành các phương pháp ngoại tuyến và trực tuyến tùy theo việc trung bình và phương sai có cần được tính toán tại thời gian suy luận hay không (Yang et al., 2022). Các phương pháp trực tuyến thường không liên quan đến batch như LayerNorm, InstanceNorm và GroupNorm. Những phương pháp này tính thống kê trong cả huấn luyện và suy luận. LayerNorm thường được sử dụng trong kiến trúc transformer.

Các phương pháp ngoại tuyến liên quan đến batch như BatchNorm và UN, trong đó chiều batch được kết luận trong các tính toán của cả trung bình và phương sai (Yao et al., 2021). Vì trung bình và phương sai được tính toán trước trong suy luận, chuẩn hóa ngoại tuyến có thể được hợp nhất vào các phép toán tuyến tính liền kề. Trong quá trình suy luận, sẽ không có phép toán chuẩn hóa ngoại tuyến và thời gian suy luận sẽ được giảm. Tuy nhiên, các phương pháp ngoại tuyến thường đối mặt với vấn đề suy giảm hiệu suất và sự sụp đổ huấn luyện khi sử dụng trong transformer. Để giải quyết vấn đề này, Yao et al. (2021) đề xuất thêm một lớp BatchNorm ở giữa hai lớp tuyến tính trong khối MLP để làm cho thống kê huấn luyện ổn định. Yang et al. (2022) phát hiện ra rằng vấn đề là do các hành vi bất thường của thống kê kích hoạt, và đề xuất một chiến lược làm mượn dao động được điều chỉnh và một chiến lược lọc ngoại lai thích ứng để tăng hiệu suất và huấn luyện ổn định.

3. Kiến thức Cơ bản
Cho N token đầu vào X∈RN×C, trong đó C là chiều đặc trưng, kiến trúc tổng quát của khối transformer có thể được viết như:
X=X+ Attn(Norm( X)),
X=X+ MLP(Norm( X)),(1)
trong đó Attn(·) tính điểm attention, MLP(·) biểu thị perceptron đa lớp và Norm(·) là hàm chuẩn hóa. Trong cấu hình mặc định của khối transformer, Norm(·) thường là một phép toán LayerNorm và Attn(·) là cơ chế attention dựa trên softmax (Vaswani et al., 2017).

Attention đóng vai trò quan trọng trong Transformer. Ký hiệu ma trận query, key và value là Q, K, V ∈RN×C, softmax attention tính toán độ tương tự từng cặp giữa queries và keys trước tiên, và dẫn đến độ phức tạp tính toán bậc hai O(N2C) liên quan đến số lượng queries và keys N. Điều này khiến transformer tốn kém về mặt tính toán đặc biệt khi xử lý các tác vụ có đầu vào chuỗi dài. Attention tuyến tính nhằm tách rời hàm softmax với xấp xỉ phù hợp hoặc thay thế nó bằng hàm kernel khác để tính KTVtrước. Với sự thay đổi này trong thứ tự tính toán, độ phức tạp tính toán trở thành O(NC2), có liên quan tuyến tính với số lượng queries và keys N.

Tuy nhiên, LayerNorm chiếm một phần không thể bỏ qua của độ trễ vì nó yêu cầu tính toán thống kê trong quá trình suy luận. Do đó, trong bài báo này chúng tôi khám phá việc tận dụng BatchNorm để xây dựng transformers hiệu quả, chỉ tồn tại trong huấn luyện và có thể được hợp nhất với các lớp tuyến tính trước hoặc sau. Hơn nữa, module attention đóng vai trò quan trọng nhất đối với transformers và cơ chế attention dựa trên softmax không hiệu quả về mặt tính toán do độ phức tạp tính toán bậc hai của nó. Trong bài báo này, chúng tôi đề xuất một dạng attention đơn giản nhưng hiệu quả, giảm đáng kể độ trễ nhưng vẫn duy trì hiệu suất mạnh mẽ trên các tác vụ thị giác khác nhau.

4. Phương pháp
Trong bài báo này, chúng tôi tập trung vào việc xây dựng transformers hiệu quả và đề xuất một loạt chiến lược, bao gồm một chiến lược tiến tiến để thay thế LayerNorm (LN) bằng BatchNorm tái tham số hóa (BN) và module attention tuyến tính đơn giản (SLA). Các SLAB transformers được đề xuất đạt được hiệu suất mạnh mẽ so với các phương pháp trước đây trong khi hưởng hiệu quả tính toán lớn hơn.

4.1. Progressive Re-parameterized BatchNorm
LayerNorm yêu cầu tính toán thống kê trong cả huấn luyện và suy luận, do đó làm cản trọ đáng kể tốc độ chạy của transformers. Ngược lại, BatchNorm có thể được hợp nhất đơn giản với các lớp tuyến tính trong quá trình suy luận và phù hợp hơn cho các kiến trúc hiệu quả. Tuy nhiên, việc trực tiếp tận dụng BatchNorm cho transformers mang lại hiệu suất không thỏa mãn (Yao et al., 2021). Để giải quyết điều này, chúng tôi đề xuất dần dần thay thế LayerNorm bằng BatchNorm trong quá trình huấn luyện, và cũng đề xuất một công thức tái tham số hóa mới của BatchNorm được lấy cảm hứng từ Ding et al. (2021; 2022) để cải thiện thêm hiệu suất, như được hiển thị trong Hình 2.

Re-parameterized BatchNorm. RepBN được đề xuất được công thức hóa như:
RepBN( X) = BN( X) +ηX, (2)
trong đó η là một tham số có thể học được được huấn luyện chung theo cách end-to-end. Khi quá trình huấn luyện hoàn thành, RepBN có thể được tái tham số hóa như một dạng chuẩn của BN, như được hiển thị trong Bổ đề 4.1.

Bổ đề 4.1. Ký hiệu một lớp BN với trung bình µ, độ lệch chuẩn σ, các tham số rescale và shift α và β là BN(X;µ, σ, α, β ). Chúng ta có thể tái tham số hóa RepBN trong Phương trình 2 như:
RepBN( X;µ, σ, α, β ) = BN( X;µ, σ, α +ησ, β +ηµ).(3)

Chứng minh.
RepBN( X;µ, σ, α, β ) = BN( X;µ, σ, α, β ) +ηX
=X−µ
σα+β+ηX=X−µ
σα+β+X
σση
=X−µ
σα+β+X−µ
σση+µη
=X−µ
σ(α+ησ) + (β+ηµ)
= BN( X;µ, σ, α +ησ, β +ηµ).(4)

Dựa trên Bổ đề 4.1, phân phối đầu ra của RepBN được kiểm soát bởi α+ησ và β+ηµ, tương ứng với phương sai và trung bình. RepBN có thể khôi phục phân phối với sự giúp đỡ của σ và µ.

Trong khi đó, khi α= 0, β= 0, nó tương đương với việc BatchNorm bị bỏ qua. Khi η= 0, RepBN được chuyển đổi thành BatchNorm thuần túy.

Progressive LN →RepBN. Để tạo điều kiện cho việc huấn luyện transformers dựa trên BN thuần túy, chúng tôi đề xuất dần dần chuyển LN sang RepBN trong quá trình huấn luyện, tức là,
PRepBN( X) =γLN(X) + (1−γ)RepBN( X),(5)
trong đó γ là một siêu tham số để kiểm soát đầu ra của các lớp chuẩn hóa khác nhau. Thông thường γ= 1 ở đầu huấn luyện khi LN chi phối kiến trúc, và γ= 0 ở cuối huấn luyện để đảm bảo nó chuyển sang transformer dựa trên BN thuần túy. Chúng tôi sử dụng một chiến lược suy giảm đơn giản nhưng hiệu quả cho γ:
γ=T−Tcur
T, γ∈[0,1], (6)
trong đó T là tổng số bước huấn luyện với LayerNorm và Tcur là bước hiện tại. Chiến lược tiến tiến này làm dễ dàng khó khăn trong việc huấn luyện transformer dựa trên BN thuần túy và do đó dẫn đến hiệu suất mạnh mẽ trên các tác vụ khác nhau.

Có một số chiến lược suy giảm khác để giảm dần giá trị của γ, chẳng hạn như suy giảm cosine và suy giảm bước. Theo kinh nghiệm, chúng tôi thấy rằng chiến lược tuyến tính là một trong những chiến lược hiệu quả và đơn giản hơn.

--- TRANG 5 ---
4.2. Simplified Linear Attention
Module attention là phần quan trọng nhất trong mạng transformer, thường được công thức hóa như:
Q=XWQ, K=XWK, V=XWV,
Oi=NX
j=1Sim(Qi, Kj)P
jSim(Qi, Kj)Vj,(7)
trong đó WQ, WK, WV∈RC×C chiếu các token đầu vào thành các tensor query, key và value, tương ứng. Sim(·,·) biểu thị hàm tương tự. Đối với dạng ban đầu của attention, hàm tương tự là:
Sim softmax (Qi, Kj) = exp(QiKT
j√
C), (8)
attention dựa trên softmax này dẫn đến độ phức tạp tính toán cao. Một số phương pháp gần đây nghiên cứu việc sử dụng attention tuyến tính để loại bỏ tính toán softmax từ đó cải thiện hiệu quả của transformers (Han et al., 2023). Tuy nhiên, những phương pháp này vẫn có thiết kế khá phức tạp và không đủ hiệu quả về mặt tính toán. Trong bài báo này, chúng tôi đề xuất một attention tuyến tính đơn giản (SLA) được công thức hóa như sau:
SimSLA(Qi, Kj) = ReLU ( Qi) ReLU ( Kj)T,
˜Oi=NX
j=1SimSLA(Qi, Kj)P
jSimSLA(Qi, Kj)Vj,
OSLA=˜O +DWC( V),(9)
trong đó DWC (·) biểu thị một convolution depth-wise. Đó là một attention tuyến tính đơn giản nhưng hiệu quả vì nó cũng hưởng thứ tự tính toán tách rời bằng cách tính KTVtrước, và dẫn đến giảm độ phức tạp lớn. Hơn nữa, chỉ có hàm ReLU và convolution depth-wise được khám phá và cả hai phép toán đều thân thiện với tính toán trong hầu hết phần cứng.

Để chứng minh rằng phương pháp của chúng tôi vẫn duy trì sự đa dạng đặc trưng, chúng tôi trực quan hóa hiệu ứng của bản đồ attention trên DeiT-T đã áp dụng chiến lược Progressive re-parameterized BatchNorm và simplified linear attention (SLAB), như được hiển thị trong Hình 3. Có thể thấy rằng một rank cao vẫn được giữ cho phương pháp được đề xuất của chúng tôi, chứng minh khả năng tốt của nó trong việc nắm bắt thông tin attention.

5. Thí nghiệm
Trong phần này, chúng tôi đánh giá phương pháp của mình trên các tác vụ thị giác máy tính khác nhau bao gồm phân loại hình ảnh, phát hiện đối tượng và phân đoạn thể hiện và cũng trên tác vụ mô hình hóa ngôn ngữ. Chúng tôi tiến hành các thí nghiệm mở rộng cho các backbone khác nhau với Progressive re-parameterized BatchNorm và module attention tuyến tính đơn giản được đề xuất của chúng tôi. Chúng tôi huấn luyện mô hình của mình trên ImageNet-1K (Deng et al., 2009) cho phân loại hình ảnh, và đánh giá hiệu ứng của các tác vụ phát hiện đối tượng và phân đoạn thể hiện trên bộ dữ liệu COCO (Lin et al., 2014). Cuối cùng, chúng tôi nghiên cứu ablation các yếu tố thiết kế quan trọng của phương pháp được đề xuất trên tác vụ phân loại.

5.1. Phân loại Hình ảnh
Cài đặt. Đối với phân loại hình ảnh, chúng tôi tuân thủ cấu hình được nêu trong (Touvron et al., 2021). Chúng tôi huấn luyện tất cả các mô hình trong 300 epoch với bộ tối ưu AdamW, kết hợp bộ lập lịch tỷ lệ học cosine decay với 20 epoch làm ấm tuyến tính. Kích thước batch được đặt thành 1024, tỷ lệ học ban đầu là 0.001, và giá trị weight decay là 0.05. Trong trường hợp các mô hình sử dụng Progressive re-parameterized BatchNorm được đề xuất, một tỷ lệ droppath (Larsson et al., 2016) giảm được áp dụng. Các bước suy giảm tuyến tính T cho PRepBN hơi khác nhau giữa các backbone khác nhau. Do sự thay đổi phương sai do droppath gây ra, chúng tôi đóng băng các tham số mô hình và chỉ cập nhật thống kê của re-parameterized BatchNorm trong 10 epoch ở cuối quá trình huấn luyện. Chúng tôi cũng chứng minh tính hiệu quả của phương pháp của mình với cả Progressive re-parameterized BatchNorm và simplified linear attention. Chúng tôi tuân theo cài đặt của (Han et al., 2023) về thiết kế kiến trúc macro và huấn luyện. Tất cả kết quả báo cáo về throughput/latency được thu được trên một GPU V100 duy nhất. Đối với tác vụ phân loại, chúng tôi đo FLOPs cũng như throughput/latency cho độ phân giải hình ảnh 224 ×224.

Kết quả trên tác vụ phân loại hình ảnh. Bảng 1 trình bày kết quả của các backbone khác nhau với chuẩn hóa PRepBN của chúng tôi. PRepBN được đề xuất của chúng tôi chứng minh hiệu suất tương đương hoặc thậm chí vượt trội khi so sánh với LayerNorm. Cụ thể hơn, các mô hình sử dụng PRepBN của chúng tôi làm lớp chuẩn hóa thể hiện cải thiện hiệu suất từ 0.1% đến 1.4%. Đáng chú ý, PRepBN có thể hợp nhất với các phép toán tuyến tính khác, cho phép nó có được suy luận hiệu quả hơn. Chúng tôi tiếp tục so sánh phương pháp của mình với BN+FFNBN (Yao et al., 2021) cũng nhằm huấn luyện mô hình transformer với BatchNorm. Có thể thấy rằng PRepBN của chúng tôi đạt được những cải thiện nhất quán trên các backbone khác nhau. Ví dụ, PRepBN được đề xuất của chúng tôi đạt được độ chính xác top-1 80.2% trên mô hình DeiT-S, tốt hơn 1.4% so với phương pháp BN+FFNBN. Đối với swin transformer, PRepBN của chúng tôi mang lại cải thiện độ chính xác +0.5%, +0.4% và +0.5% so với BN+FFNBN trên các mô hình Swin-T, Swin-S và Swin-B.

Như được hiển thị trong Hình 4, chúng tôi trình bày một phân tích so sánh về phương pháp của mình trên các mô hình dựa trên DeiT, dựa trên PVT và dựa trên Swin. Rõ ràng là các transformers được trang bị PRepBN của chúng tôi đạt được throughput cao hơn trong khi duy trì mức độ chính xác tương tự.

Bảng 2 trình bày hiệu suất của SLAB transformer của chúng tôi, được hỗ trợ bởi Progressive re-parameterized BatchNorm và module attention tuyến tính đơn giản được đề xuất của chúng tôi. Chúng tôi so sánh mô hình của mình với Flatten transformer, sử dụng focused linear attention để có hiệu quả cao hơn. Các thí nghiệm trên các kiến trúc khác nhau bao gồm DeiT (Touvron et al., 2021), PVT (Wang et al., 2021), CSwin (Dong et al., 2022) và Swin (Liu et al., 2021) chứng minh rằng SLAB transformer của chúng tôi đạt được hiệu suất tốt hơn Flatten transformer. Cụ thể hơn, mô hình SLAB-Swin-T của chúng tôi đạt được độ chính xác top-1 83.6% trên ImageNet-1K với độ trễ 16.2ms, ít hơn 2.4ms so với Flatten-Swin với độ chính xác cao hơn 0.1%. Các mô hình của chúng tôi hiệu quả hơn về mặt tính toán chủ yếu do các lớp chuẩn hóa thân thiện với phần cứng hơn cũng như module attention tuyến tính đơn giản. Hình 1 cũng cho thấy sự đánh đổi giữa độ chính xác và độ trễ của SLAB transformer của chúng tôi, Flatten transformer (Han et al., 2023) và Swin transformer gốc, chứng minh hiệu suất tốt hơn của mô hình chúng tôi.

5.2. Phát hiện Đối tượng
Cài đặt. Chúng tôi sử dụng Mask R-CNN (He et al., 2017) để đánh giá tính hiệu quả của phương pháp của mình trên bộ dữ liệu COCO cho các tác vụ phát hiện đối tượng và phân đoạn thể hiện. Các backbone được sử dụng trong Mask R-CNN được pre-train trên ImageNet-1K. Tất cả các mô hình được huấn luyện cho lịch trình 1×, tức là 12 epoch. Độ trễ được đo với kích thước batch là 1 trên GPU V100 cho một giá trị trung bình của 100 vòng.

Kết quả trên tác vụ phát hiện đối tượng. Chúng tôi so sánh PRepBN được đề xuất của chúng tôi với LayerNorm chuẩn cho các backbone khác nhau bao gồm Swin và PVT cho tác vụ phát hiện đối tượng. Kết quả được hiển thị trong Bảng 3. Nó cho thấy rằng phương pháp của chúng tôi đạt được hiệu suất khá tương đương với các mô hình gốc được trang bị LayerNorm. Tận dụng ưu điểm của chuẩn hóa ngoại tuyến, các mô hình với PRepBN được đề xuất của chúng tôi đạt được độ trễ suy luận thấp hơn. Ví dụ, độ trễ của Mask R-CNN thể hiện sự giảm từ 64ms xuống 59.6ms khi backbone PVT-S được trang bị PRepBN của chúng tôi và độ chính xác cho phát hiện đối tượng và phân đoạn thể hiện là tương tự. Một trực quan hóa rõ ràng hơn cho sự đánh đổi giữa mAP và độ trễ cũng được trình bày trong Hình 5, chứng minh rằng phương pháp được đề xuất của chúng tôi đạt được hiệu suất tổng thể tốt hơn trên phát hiện đối tượng.

5.3. Mô hình hóa ngôn ngữ
Cài đặt. Chúng tôi cũng đánh giá phương pháp được đề xuất của mình trên tác vụ mô hình hóa ngôn ngữ dựa trên Adaptive Inputs (Baevski & Auli, 2018). Chúng tôi huấn luyện các mô hình của mình trên bộ dữ liệu Wikitext-103 (Merity et al., 2016), chứa hơn 100 triệu token. Chúng tôi đặt số token mỗi GPU là 4096 và huấn luyện trên 8 GPU. Số token mỗi mẫu được giới hạn ở 512. Chúng tôi cũng áp dụng phương pháp của mình trên mô hình LLaMA-350M, tuân theo kiến trúc và cài đặt huấn luyện tương tự như công trình trước đây (Yang et al., 2023; He et al., 2024).

Kết quả trên tác vụ mô hình hóa ngôn ngữ. Như được hiển thị trong Bảng 4, PRepBN của chúng tôi đạt được perplexity tương tự với mô hình được trang bị LayerNorm, trong khi độ trễ giảm từ 13.9 ms xuống 12.9 ms mỗi token. Bên cạnh đó, chúng tôi áp dụng PRepBN của mình trên các mô hình ngôn ngữ lớn hiện đại hơn như LLaMA, sử dụng biến thể của LayerNorm loại bỏ việc tính toán giá trị trung bình, tức là RMSNorm. Như được hiển thị trong Bảng 5, phương pháp của chúng tôi thành công tăng throughput từ 44.0 lên 50.4 token mỗi giây trên GPU V100, và đạt được độ chính xác trung bình thậm chí hơi tốt hơn. Những kết quả này chứng minh tính hiệu quả của PRepBN được đề xuất của chúng tôi trên tác vụ mô hình hóa ngôn ngữ.

--- TRANG 8 ---
Bảng 4. Kết quả perplexity trên bộ dữ liệu Wikitext-103. 256/480 chỉ ra kích thước cửa sổ ngữ cảnh đánh giá.
Mô hình # Param.256 480Thời gian Suy luận(ms/t)Val. Test Val. Test
Adaptive Inputs w/ LN 247M 19.5 20.2 19.3 20.0 13.9
Adaptive Inputs w/ PRepBN (Ours) 247M 19.2 20.0 19.1 19.8 12.9

Bảng 5. Kết quả thí nghiệm của phương pháp được đề xuất cho LLaMA-350M trên các điểm chuẩn khác nhau.
Mô hình Throughput ARC-C ARC-E BoolQ COPA HellaSwag PIQA WinoGrande OpenBookQA SciQ Avg.
LLaMA-350M 44.0 22.95 46.13 59.27 64 33.19 64.36 49.09 29.6 75.3 49.32
w/ PRepBN (Ours) 50.4 23.55 44.44 60.67 66 32.76 64.04 49.72 30.0 76.8 49.78

Bảng 6. Nghiên cứu ablation về tác động của simplified linear attention và progressive re-parameterized BatchNorm.
Phương pháp FLOPs Lat. (ms) Acc. (%)
Flatten-DeiT-T 1.1 G 15.2 74.1
+ SLA 1.1 G 10.2 73.0
+ SLA + PRepBN 1.1 G 9.6 74.3
Flatten-PVT-T 2.0 G 10.8 77.8
+ SLA 2.0 G 8.5 75.2
+ SLA + PRepBN 2.0 G 8.0 76.5
Flatten-Swin-T 4.5 G 10.9 82.1
+ SLA 4.5 G 9.5 81.9
+ SLA + PRepBN 4.5 G 8.7 81.8
Flatten-Swin-S 8.8 G 18.6 83.5
+ SLA 8.8 G 18.0 83.4
+ SLA + PRepBN 8.7 G 16.2 83.6

5.4. Nghiên cứu Ablation
Trong phần này, chúng tôi tiến hành các nghiên cứu ablation mở rộng để chứng minh tác động của các thiết kế chính của chúng tôi.

Tác động của SLA và PRepBN. Chúng tôi đầu tiên khám phá tác động của module simplified linear attention (SLA) và progressive re-parameterized BatchNorm (PRepBN) trên các backbone khác nhau. Như được hiển thị trong Bảng 6, việc sử dụng simplified linear attention (SLA) của chúng tôi mang lại cải thiện nhất quán về hiệu quả. Đối với DeiT và PVT, SLA của chúng tôi đạt được giảm độ trễ đáng kể và một chút sụt giảm độ chính xác. Hơn nữa, Swin transformers được trang bị SLA của chúng tôi đạt được độ chính xác khá tương đương với những cái gốc nhưng với độ trễ thấp hơn. Ngoài ra, độ trễ có thể được giảm thêm bằng cách thay thế LayerNorm bằng progressive re-parameterized BatchNorm (PRepBN) được đề xuất của chúng tôi. Chiến lược này hầu như không ảnh hưởng đến độ chính xác và thậm chí khôi phục độ chính xác của mô hình như DeiT và PVT. Kết hợp hai chiến lược này, độ trễ được giảm 5.6 ms khi độ chính xác được cải thiện 0.2% cho DeiT-T. Hơn nữa, phương pháp của chúng tôi đạt được độ chính xác tương tự và thu hoạch giảm độ trễ 2.2 ms và 2.4 ms cho các mô hình Swin-T và Swin-S.

Nghiên cứu ablation cho PRepBN. Chúng tôi nghiên cứu các thành phần chính của PRepBN được đề xuất của chúng tôi, tức là chiến lược tiến tiến và re-parameterized BatchNorm (RepBN). Việc trực tiếp huấn luyện transformer dựa trên BatchNorm dẫn đến huấn luyện khá không ổn định, hoặc đạt được hiệu suất kém hơn hoặc sụp đổ trong huấn luyện (ví dụ, DeiT-S và Flatten-Swin-T). Để tránh sự thay đổi phương sai (Li et al., 2019) do droppath gây ra, điều này sẽ ảnh hưởng đến hiệu suất của BatchNorm, chúng tôi đơn giản đặt tỷ lệ droppath thành 0 trên mô hình DeiT-T. Như được hiển thị trong Bảng 7, việc áp dụng chiến lược tiến tiến trên mô hình DeiT-T dựa trên BatchNorm mang lại cải thiện độ chính xác 1.2%. Chúng tôi tiếp tục sử dụng RepBN của mình trong mô hình và độ chính xác tăng lên 73.6%. Những kết quả này chứng minh rằng cả chiến lược tiến tiến được đề xuất và re-parameterized BatchNorm (RepBN) của chúng tôi đều có lợi cho việc huấn luyện transformer dựa trên BatchNorm thuần túy.

Bảng 7. Nghiên cứu ablation về tác động của chiến lược tiến tiến và re-parameterized BatchNorm.
Phương pháp Acc. (%)
DeiT-T-BN 71.9
+ Progressive Strategy 73.1
+ Progressive Strategy + RepBN 73.6

6. Kết luận
Trong bài báo này, chúng tôi nghiên cứu các module nút thắt tính toán của transformer và đề xuất các chiến lược mới bao gồm Progressive Re-parameterized BatchNorm và simplified linear attention để có được các kiến trúc transformer hiệu quả. Phương pháp của chúng tôi dần dần thay thế LayerNorm bằng re-parameterized BatchNorm trong quá trình huấn luyện để đạt được độ chính xác không mất mát, trong khi tận dụng các ưu điểm hiệu quả của BatchNorm trong quá trình suy luận. Ngoài ra, chúng tôi thiết kế một cơ chế attention tuyến tính đơn giản đạt được hiệu suất tương đương với các phương pháp attention tuyến tính khác nhưng với chi phí tính toán ít hơn. Thông qua các thí nghiệm mở rộng cho cả các tác vụ thị giác máy tính và mô hình hóa ngôn ngữ, chúng tôi cho thấy rằng phương pháp của mình đạt được hiệu suất mạnh mẽ hơn về độ chính xác và hiệu quả so với các phương pháp trước đây và làm sáng tỏ việc thiết kế transformer hiệu quả.

Lời cảm ơn. Chúng tôi biết ơn sự hỗ trợ của MindSpore (Huawei, 2020), CANN (Compute Architecture for Neural Networks) và Ascend AI Processor được sử dụng cho nghiên cứu này.

--- TRANG 9 ---
Tuyên bố Tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học Sâu. Có nhiều hậu quả xã hội tiềm tàng của công trình chúng tôi, không có gì chúng tôi cảm thấy phải được nêu bật cụ thể ở đây.

Tài liệu tham khảo
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.

Baevski, A. and Auli, M. Adaptive input representa-
tions for neural language modeling. arXiv preprint
arXiv:1809.10853 , 2018.

Bolya, D., Fu, C.-Y ., Dai, X., Zhang, P., and Hoffman, J.
Hydra attention: Efficient attention with many heads. In
European Conference on Computer Vision , pp. 35–49.
Springer, 2022.

Cai, H., Gan, C., and Han, S. Efficientvit: Enhanced lin-
ear attention for high-resolution low-computation visual
recognition. arXiv preprint arXiv:2205.14756 , 2022.

Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
A., and Zagoruyko, S. End-to-end object detection with
transformers. In European conference on computer vision ,
pp. 213–229. Springer, 2020.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.

Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,
Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,
A., Kaiser, L., et al. Rethinking attention with performers.
arXiv preprint arXiv:2009.14794 , 2020.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pp. 248–255. Ieee, 2009.

Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., and Sun,
J. Repvgg: Making vgg-style convnets great again. In
Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pp. 13733–13742, 2021.

Ding, X., Chen, H., Zhang, X., Huang, K., Han, J., and
Ding, G. Re-parameterizing your optimizers rather than
architectures. arXiv preprint arXiv:2205.15242 , 2022.

Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L.,
Chen, D., and Guo, B. Cswin transformer: A general
vision transformer backbone with cross-shaped windows.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pp. 12124–12134,
2022.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.

Han, D., Pan, X., Han, Y ., Song, S., and Huang, G. Flatten
transformer: Vision transformer using focused linear at-
tention. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 5961–5971, 2023.

Han, K., Wang, Y ., Chen, H., Chen, X., Guo, J., Liu, Z.,
Tang, Y ., Xiao, A., Xu, C., Xu, Y ., et al. A survey on
vision transformer. IEEE transactions on pattern analysis
and machine intelligence , 45(1):87–110, 2022.

He, K., Gkioxari, G., Doll ár, P., and Girshick, R. Mask r-
cnn. In Proceedings of the IEEE international conference
on computer vision , pp. 2961–2969, 2017.

He, W., Han, K., Tang, Y ., Wang, C., Yang, Y ., Guo, T., and
Wang, Y . Densemamba: State space models with dense
hidden connection for efficient large language models.
arXiv preprint arXiv:2403.00818 , 2024.

Huawei. Mindspore. https://www.mindspore.cn/ ,
2020.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
InInternational conference on machine learning , pp. 448–
456. pmlr, 2015.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
with linear attention. In International conference on ma-
chine learning , pp. 5156–5165. PMLR, 2020.

Larsson, G., Maire, M., and Shakhnarovich, G. Fractal-
net: Ultra-deep neural networks without residuals. arXiv
preprint arXiv:1605.07648 , 2016.

Li, X., Chen, S., Hu, X., and Yang, J. Understanding the
disharmony between dropout and batch normalization
by variance shift. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pp.
2682–2690, 2019.

Li, Z., Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale repa-
rameterization for post-training quantization of vision
transformers. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pp. 17227–17236,
2023.

Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-
manan, D., Doll ár, P., and Zitnick, C. L. Microsoft coco:
Common objects in context. In Computer Vision–ECCV
2014: 13th European Conference, Zurich, Switzerland,

--- TRANG 10 ---
September 6-12, 2014, Proceedings, Part V 13 , pp. 740–
755. Springer, 2014.

Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin,
S., and Guo, B. Swin transformer: Hierarchical vision
transformer using shifted windows. In Proceedings of the
IEEE/CVF international conference on computer vision ,
pp. 10012–10022, 2021.

Merity, S., Xiong, C., Bradbury, J., and Socher, R.
Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843 , 2016.

Tang, Q., Liu, C., Liu, F., Liu, Y ., Jiang, J., Zhang, B.,
Han, K., and Wang, Y . Category feature transformer for
semantic segmentation. arXiv preprint arXiv:2308.05581 ,
2023a.

Tang, Q., Zhang, B., Liu, J., Liu, F., and Liu, Y . Dynamic
token pruning in plain vision transformers for semantic
segmentation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pp. 777–786,
2023b.

Tang, Y ., Wang, Y ., Guo, J., Tu, Z., Han, K., Hu, H., and
Tao, D. A survey on transformer compression. arXiv
preprint arXiv:2402.05964 , 2024.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and J ́egou, H. Training data-efficient image transform-
ers & distillation through attention. In International con-
ference on machine learning , pp. 10347–10357. PMLR,
2021.

Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik,
A., and Li, Y . Maxvit: Multi-axis vision transformer. In
European conference on computer vision , pp. 459–479.
Springer, 2022.

Ulyanov, D., Vedaldi, A., and Lempitsky, V . Instance nor-
malization: The missing ingredient for fast stylization.
arXiv preprint arXiv:1607.08022 , 2016.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. Advances in neural information
processing systems , 30, 2017.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D.,
Lu, T., Luo, P., and Shao, L. Pyramid vision transformer:
A versatile backbone for dense prediction without convo-
lutions. In Proceedings of the IEEE/CVF international
conference on computer vision , pp. 568–578, 2021.

Wang, Y ., Chen, X., Cao, L., Huang, W., Sun, F., and Wang,
Y . Multimodal token fusion for vision transformers. In
Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pp. 12186–12195, 2022.

Wu, X., Zeng, F., Wang, X., and Chen, X. PPT: Token prun-
ing and pooling for efficient vision transformers. arXiv
preprint arXiv:2310.01812 , 2023.

Wu, Y . and He, K. Group normalization. In Proceedings of
the European conference on computer vision (ECCV) , pp.
3–19, 2018.

Xia, Z., Pan, X., Song, S., Li, L. E., and Huang, G. Dat++:
Spatially dynamic vision transformer with deformable
attention. arXiv preprint arXiv:2309.01430 , 2023.

Xu, Y ., Li, C., Li, D., Sheng, X., Jiang, F., Tian, L., and
Sirasao, A. Fdvit: Improve the hierarchical architecture
of vision transformer. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 5950–
5960, 2023.

Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y ., and
Sun, J. Towards stabilizing batch statistics in back-
ward propagation of batch normalization. arXiv preprint
arXiv:2001.06838 , 2020.

Yang, Q., Zhang, K., Lan, C., Yang, Z., Li, Z., Tan, W., Xiao,
J., and Pu, S. Unified normalization for accelerating and
stabilizing transformers. In Proceedings of the 30th ACM
International Conference on Multimedia , pp. 4445–4455,
2022.

Yang, S., Wang, B., Shen, Y ., Panda, R., and Kim, Y . Gated
linear attention transformers with hardware-efficient train-
ing.arXiv preprint arXiv:2312.06635 , 2023.

Yao, Z., Cao, Y ., Lin, Y ., Liu, Z., Zhang, Z., and Hu, H.
Leveraging batch normalization for vision transformers.
In2021 IEEE/CVF International Conference on Com-
puter Vision Workshops (ICCVW) , pp. 413–422. IEEE,
2021.

You, H., Xiong, Y ., Dai, X., Wu, B., Zhang, P., Fan, H.,
Vajda, P., and Lin, Y . C. Castling-vit: Compressing self-
attention via switching towards linear-angular attention
at vision transformer inference. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 14431–14442, 2023.

Zheng, D., Dong, W., Hu, H., Chen, X., and Wang, Y . Less
is more: Focus attention for efficient detr. In Proceedings
of the IEEE/CVF International Conference on Computer
Vision , pp. 6674–6683, 2023.

Zhu, L., Wang, X., Ke, Z., Zhang, W., and Lau, R. W. Bi-
former: Vision transformer with bi-level routing attention.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pp. 10323–10333,
2023.

--- TRANG 11 ---
A. Cài đặt siêu tham số chi tiết.
Các cài đặt siêu tham số chi tiết được cung cấp trong Bảng A1. Đối với phân loại hình ảnh, chúng tôi tuân theo cài đặt của DeiT (Touvron et al., 2021). Chúng tôi sử dụng AdamW làm bộ tối ưu mặc định và huấn luyện tất cả các mô hình trong 300 epoch với bộ lập lịch tỷ lệ học cosine decay và 20 epoch làm ấm tuyến tính. Kích thước batch được đặt thành 1024, tỷ lệ học ban đầu là 0.001, và giá trị weight decay là 0.05. Đối với mô hình sử dụng progressive re-parameterized batchnorm được đề xuất, chúng tôi sử dụng tỷ lệ droppath (Larsson et al., 2016) nhỏ hơn. Do sự thay đổi phương sai do droppath gây ra, chúng tôi đóng băng các tham số mô hình và chỉ cập nhật thống kê của re-parameterized BatchNorm trong 10 epoch ở cuối quá trình huấn luyện.

Bảng A1. Cài đặt siêu tham số cho các mô hình khác nhau cho tác vụ phân loại hình ảnh.
Tên DeiT-T DeiT-S PVT-T PVT-S PVT-M Swin-T Swin-S Swin-B
Epoch 300 300 300 300 300 300 300 300
Kích thước Batch 1024 1024 1024 1024 1024 1024 1024 1024
Tỷ lệ học 1e-3 1e-3 1e-3 1e-3 1e-3 1e-3 1e-3 1e-3
Bước Làm ấm 20 20 20 20 20 20 20 20
Bộ tối ưu AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW
Tỷ lệ Droppath 0.0 0.0 0.0 0.1 0.3 0.1 0.3 0.3
Bước Suy giảm Tuyến tính 0 3e5 0 3e5 3e5 0 3e5 3e5

B. Kết hợp với phương pháp lượng tử hóa sau huấn luyện.
Phương pháp của chúng tôi tập trung vào việc thay thế LayerNorm bằng BatchNorm để đạt được tăng tốc suy luận mà không suy giảm hiệu suất. Đó là một chiến lược bổ sung với các phương pháp nén mô hình phổ biến như lượng tử hóa trọng số, pruning hoặc chưng cất và những phương pháp này có thể được kết hợp để đạt được hiệu suất tốt hơn. Như một bằng chứng về khái niệm, chúng tôi tiến hành lượng tử hóa sau huấn luyện sử dụng RepQ-ViT (Li et al., 2023) trên mô hình DeiT-Tiny được huấn luyện với chiến lược PRepBN của chúng tôi. Như được hiển thị trong bảng dưới đây, việc áp dụng lượng tử hóa W8A8 trên phương pháp của chúng tôi vẫn đạt được độ chính xác 73.6%, trong khi giảm thêm chi phí tính toán chỉ còn 0.33 GFLOPs. Điều này chứng minh rằng phương pháp của chúng tôi có thể được kết hợp hiệu quả với các phương pháp nén khác như lượng tử hóa.

Bảng A2. Cài đặt siêu tham số cho các mô hình khác nhau cho tác vụ phân loại hình ảnh.
Phương pháp FLOPs (G) Throughput (hình ảnh/s) Top-1 Acc (%)
DeiT-T 1.3 3432 72.2
w/ PRepBN (Ours) 1.3 4194 73.6
w/ PRepBN (Ours) + RepQ-ViT (W8A8) 0.33 - 73.6
