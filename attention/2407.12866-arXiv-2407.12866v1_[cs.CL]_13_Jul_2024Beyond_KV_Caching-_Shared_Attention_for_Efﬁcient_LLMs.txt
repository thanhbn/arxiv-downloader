# 2407.12866.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2407.12866.pdf
# File size: 3070836 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2407.12866v1  [cs.CL]  13 Jul 2024Beyond KV Caching: Shared Attention for Efﬁcient LLMs
Liao Bingli1, Danilo Vasconcellos Vargas1
1Kyushu University, Fukuoka, Japan
{liao.bingli.734@s, vargas@inf }.kyushu-u.ac.jp
Abstract
The efﬁciency of large language models (LLMs) remains
a critical challenge, particularly in contexts where compu -
tational resources are limited. Traditional attention mec ha-
nisms in these models, while powerful, require signiﬁcant
computational and memory resources due to the necessity
of recalculating and storing attention weights across diff er-
ent layers. This paper introduces a novel Shared Attention
(SA) mechanism, designed to enhance the efﬁciency of LLMs
by directly sharing computed attention weights across mul-
tiple layers. Unlike previous methods that focus on sharing
intermediate Key-Value (KV) caches, our approach utilizes
the isotropic tendencies of attention distributions obser ved
in advanced LLMs post-pretraining to reduce both the com-
putational ﬂops and the size of the KV cache required dur-
ing inference. We empirically demonstrate that implementi ng
SA across various LLMs results in minimal accuracy loss on
standard benchmarks. Our ﬁndings suggest that SA not only
conserves computational resources but also maintains robu st
model performance, thereby facilitating the deployment of
more efﬁcient LLMs in resource-constrained environments.
Code: https://github.com/metacarbon/shareAtt
Introduction
The rapid growth of large language models (LLM) has
brought forth signiﬁcant challenges in terms of computa-
tional and memory efﬁciency during inference. Traditional
approaches, such as Multi-Query Attention (MQA) (Shazeer
2019) and Grouped-Query Attention (GQA) (Ainslie et al.
2023), have made strides in reducing the key-value (KV)
cache size by sharing keys and values across multiple
heads within a layer. More recently, Cross-Layer Attention
(CLA) has extended this concept by sharing keys and val-
ues across adjacent layers, further reducing memory requir e-
ments without substantially impacting model performance
(Brandon et al. 2024). Despite these advancements, the need
for more efﬁcient methods continues to grow, particularly a s
models scale and are deployed in resource-constrained envi -
ronments.
In this paper, we introduce a novel method termed Shared
Attention (SA), which signiﬁcantly reduces the KV cache
requirements and computational load during inference for
LLMs. Unlike previous methods that focused on sharing KV
caches either within the same layer or between adjacent lay-
ers, our approach inspired by the inherent similarity of at-tention weights distribution across layers, and sharing th ese
weights directly could further reduce the need for repeated
key and value computations. This innovative approach not
only reduces the KV cache size but also circumvents the
need for the computationally expensive softmax operation,
leading to a more efﬁcient inference process.
The key contributions of our work are summarized as fol-
lows:
1. We propose a novel Shared Attention mechanism that re-
duces computational and memory overhead by directly
sharing pre-computed attention weights across multiple
layers in LLMs.
2. We empirically validate the effectiveness of Shared At-
tention by implementing it across various benchmarks
and demonstrate that it achieves comparable accuracy.
3. Our analysis of attention isotropy across pretrained
LLMs provides insights into how attention mechanisms
stabilize and become more uniform across layers as train-
ing progresses. This understanding informs the optimal
layer ranges for applying Shared Attention.
Shared Attention
In this section we demonstrate motivation, Shared Atten-
tion (SA) method, and the comparison to existed KV-sharing
mechanisms.
Motivation
The self-attention mechanism in transformer models is typ-
ically deﬁned as softmax (QKT
√
d)V, whereQ,K, andVrep-
resent the query, key, and value matrices respectively, and d
is the dimension of the key vectors. This formulation neces-
sitates the recomputation of attention weights at each laye r, a
computationally intensive task, particularly when the mod el
is deployed in inference mode. To mitigate this, the concept
of a KV-cache is employed, reducing the need to recompute
KandVmatrices for previously encountered tokens.
While prior methodologies have focused on sharing KV
caches at different levels to minimize memory overhead,
they predominantly operate under the assumption that atten -
tion weights differ signiﬁcantly across layers, thereby ne -
cessitating individual computations to capture diverse co n-
textual dependencies effectively. This assumption prompt s a

--- PAGE 2 ---
Figure 1: Illustration of various sharing algorithms. The M QA and GQA methods share the Key and Value caches with the
Query within the same layer to reduce memory usage. The CLA me thod extends this by sharing the Key and Value caches
across different layers. Our method, Shared Attention, adv ances this concept further by sharing the attention weights across
multiple layers.
critical inquiry: Are the attention weights indeed markedl y
different across layers, or is this variation minimal enoug h
to allow for a uniﬁed approach across multiple layers?
To explore this, we conducted an empirical analysis on the
distribution of attention weights across different layers of the
model. Based on the Llama2-7B-chat model, we processed
the Massive Multitask Language Understanding (MMLU)
dataset (Hendrycks et al. 2020) to extract the attention ma-
trices, softmax (QKT
√
d), for each layer. Given the variability
in sequence lengths, we standardized these matrices to a uni -
form size by applying zero-padding to align them to a con-
sistent shape of maxlen ×maxlen.
Our analysis employed the cosine similarity metric to
compare the attention matrices of all layers, revealing a no -
table high degree of similarity across most of layers, parti cu-
larly from indices 3 to 30. Contrastingly, the initial layer s (0
and 1) and the ﬁnal output layer (31) exhibited substantiall y
lower similarity scores to middle layers. This observation is
intuitive as the early layers are closer to the input token em -
beddings, requiring frequent adjustments to their attenti on
distribution to accurately abstract semantic meanings fro m
diverse inputs. Similarly, the ﬁnal layer’s unique role in p re-
dicting the next token justiﬁes its distinct attention patt ern.
Inspired by these ﬁndings, we hypothesize that the high
similarity in attention weights across the majority of lay-
ers could allow for a shared representation of these weights ,
thus eliminating the need for separate softmax computation s
in each layer and reducing the key cache size. Such a strat-
egy could not only streamline the inference process but also
enhance computational efﬁciency signiﬁcantly.Based on the observed uniformity in attention weights,
we propose a novel algorithm as shown in Algorithm 1,
Shared Attention , which utilizes a single shared attention
matrix across multiple layers. This approach fundamentall y
redeﬁnes the operational paradigm by maintaining a con-
sistent attention mechanism across various contextual lay -
ers, thereby reducing redundancy and enhancing inference
speed.
Comparison with Existing Approaches
The original self-attention mechanism in Transformers,
characterized by the Multi-Head Attention (MHA) model,
necessitates caching the keys ( K) and values ( V) in each
head and layer to accelerate inference (Vaswani et al. 2017) .
This requirement has historically imposed a signiﬁcant
memory overhead, prompting a series of innovations aimed
at reducing this burden.
Among these, Multi-Query Attention (MQA) and its more
generalized counterpart, Grouped-Query Attention (GQA),
consolidate the KV cache by allowing multiple query heads
within the same layer to share a singular set of K and V
matrices. This approach effectively reduces the number of
unique key and value pairs that must be stored and retrieved
during the computation process. Subsequently, Cross-Laye r
Attention (CLA) extends this concept by facilitating the
sharing of K and V matrices across different layers, thereby
offering further reductions in the memory footprint requir ed
for KV storage.
Our method, however, introduces a fundamentally differ-
ent paradigm in addressing the challenges of self-attentio n.

--- PAGE 3 ---
Figure 2: Layer-wise similarity of attention weights acros s various LLMs. The x-axis and y-axis represent the layer ind ices,
while the z-axis depicts the cosine similarity values. The d istinct similarity patterns are indicative of the speciﬁc f unctional
roles each group of layers plays within the overall architec ture.
Algorithm 1: Shared Attention Algorithm
Input : Set of layers L, input tokens X
Parameters : Attention span S(e.g., layers 23 to 30)
Output : Updated attention weights across speciﬁed lay-
ers
1:Initialize attention weights A←∅
2:foreach layer l∈Sdo
3: ifﬁrst layer in Sthen
4: Compute initial attention weights Al←
softmax(QlKT
l√dk)
5: SetA←Al
6: else
7: Share attention weights Al←A
8: end if
9: Apply shared attention to compute outputs Ol←Al·
Vl
10:end for
11:Adjust subsequent layers’ inputs using outputs from S
12:Continue processing remaining layers with standard at-
tention
13:return Final output after processing all layers
While previous methods have focused on reducing the re-
dundancy in storing K and V matrices, our approach cen-
ters on the optimization of the computation of attention
weights themselves. In standard practice, the cached keys
(K) are primarily utilized to compute attention weights in
conjunction with the queries ( Q). Instead of indirectly fa-
cilitating this interaction through shared KV matrices, ou r
method proposes the direct sharing of the resultant attenti on
weights—speciﬁcally, the softmax-normalized scores.
This not only diminishes the memory requirements by ob-
viating the need to store separate sets of keys for each layer
but also signiﬁcantly reduces the computational complexit y.
By sharing the pre-computed softmax results across layers,
our approach circumvents the repeated calculation of soft-
max, which is often one of the most computationally inten-
sive operations in the attention mechanism. This efﬁciencygain is reﬂected in a substantial reduction in the number of
ﬂoating-point operations (FLOPs) required during model in -
ference, enhancing both the speed and scalability of Trans-
former deployments.
Unlike traditional methods that optimize memory use by
sharing physical keys and values across layers or heads,
our Shared Attention model innovates on the computational
process itself, exploiting the consistent patterns in atte ntion
weights to streamline operations across multiple layers of
the Transformer architecture.
Isotropic Attention Distribution
In an extensive analysis of layer-speciﬁc attention weight s
across a spectrum of LLMs, we explored the attention dy-
namics within models such as Llama2-7B-chat, Llama3-8B-
instruct, Llama3-70B-instruct, Baichuan2-7B-chat, Qwen 2-
7B-instruct, and Qwen2-72B-instruct (Touvron et al. 2023;
Yang et al. 2023; Bai et al. 2023). These models were eval-
uated using the MMLU.
Our investigations reveal a self-organization pattern in t he
attention weights across these diverse models. As depicted
in Figure 2, there exists a consistent global similarity pat -
tern in the layers’ attention weights across all tested mode ls.
This pattern suggests an inherent structural characterist ic in
the way LLMs process information, which can be broadly
segmented into four distinct groups:
•Group 1: Comprising the initial layers (indices 0 and
1), this group is situated closest to the input tokens and
primarily focuses on abstracting token-level semantic in-
formation. These layers exhibit data-dependent attention
patterns that are crucial for the initial semantic process-
ing of the inputs.
•Group 2: This group includes layers immediately fol-
lowing the ﬁrst group and extends up to layer index 5.
Layers in this segment demonstrate high internal similar-
ity in attention weights but are markedly different from
those in other groups. These layers likely serve as tran-
sitional zones where intermediate semantic features are
reﬁned.

--- PAGE 4 ---
•Group 3: Encompassing layers post-Group 2 and ex-
tending to the penultimate layer, this is the largest group
both in terms of the number of layers and their role within
the architecture. The layers within this group display a
high degree of similarity, suggesting an isotropy in the
attention mechanism where the reﬁned features are con-
sistently utilized to inform the model’s deeper contextual
understanding.
•Group 4: The ﬁnal group, consisting solely of the out-
put layer, distinctively processes the aggregated contex-
tual information to generate outputs. This layer’s atten-
tion weights diverge from those observed in other lay-
ers, underscoring its specialized role in the ﬁnal decision -
making process.
The distinct attention weight patterns identiﬁed across
these groups reinforce the concept of functional specializ a-
tion within LLMs. This segmentation not only highlights the
diverse roles of different layers in processing inputs but a lso
supports the potential for optimizing computational strat e-
gies, such as our proposed Shared Attention method, by ma-
nipulating these inherent patterns to reduce computationa l
redundancy.
Dynamics During Pretraining
To elucidate the formation and evolution of attention weigh t
patterns during the pretraining phase of LLMs, we utilized
intermediate checkpoints of the Baichuan 7B model, pro-
vided by the model developers. These checkpoints, spanning
from 0.2T to 2.6T tokens processed, offer a unique point of
view to observe the dynamic shifts in attention mechanisms
as the model gains exposure to an increasing volume of data.
We applied a consistent metric for measuring the simi-
larity of attention weights across layers at each pretraini ng
checkpoint. Additionally, the ﬁnal chat model, ﬁne-tuned
to align with human reference responses, was included to
benchmark the evolution against practical application out -
comes. The dynamics of these attention weights are visual-
ized in Figure 3, which illustrates the progressive differe ntia-
tion and stabilization of attention patterns across the mod el’s
layers.
As observed in the early pretraining stage at 0.2T tokens,
Groups 1 and 2 appear merged, indicating a less differen-
tiated processing strategy across these initial layers. Th is
combination suggests that early in training, the model does
not distinctly separate token-level semantic processing f rom
intermediate semantic reﬁnement. However, as the model
progresses to 1.0T tokens, a clear division emerges between
Groups 1 and 2. This separation aligns with the model be-
ginning to form more specialized and efﬁcient strategies fo r
handling different types of information across its archite c-
ture.
The similarity within Group 3, which encompasses the
bulk of the model’s layers, shows a marked improvement
from a similarity score of 0.8 to 0.9. This increase is in-
dicative of the model’s attention mechanism stabilizing an d
becoming more consistent in its approach to processing the
bulk of contextual information.The training advancements observed across the pretrain-
ing checkpoints not only demonstrate signiﬁcant shifts in t he
internal structure of the model’s attention mechanisms but
also correlate positively with performance improvements o n
multiple benchmarks. This includes results on the MMLU,
CMMLU (Li et al. 2023), and C-Eval (Huang et al. 2024)
5-shot accuracy tests, which have reportedly improved from
a baseline accuracy of 0.25 to 0.50 (Yang et al. 2023). This
notable enhancement underscores the intrinsic link betwee n
the reﬁnement of attention mechanisms within LLMs and
their enhanced capabilities in natural language understan d-
ing tasks.
Moreover, further examination of the model’s develop-
ment, as observed in supplementary material, reveals that
the similarity within Group 3—comprising the core contex-
tual processing layers of the model—continues to enhance
after the alignment stage. This observation suggests that t he
alignment process, typically aimed at ﬁne-tuning the model
to more closely mirror human-like understanding and re-
sponse generation, also contributes to the stabilization o f the
model’s attention mechanisms.
Experiments and Discussion
To validate the efﬁcacy of our proposed Shared Attention
(SA) method, we conducted series of experiments. These ex-
periments were designed to test the robustness of SA under
various conﬁgurations and to evaluate its performance on
widely recognized benchmarks.
Initially, we applied the SA mechanism directly to ad-
vanced LLMs without any prior training to assess its im-
pact on pre-trained models. This experiment aimed to un-
derstand the immediate effects of SA when integrated into
existing model architectures. We evaluated the performanc e
of these models on standard LLM benchmarks, including
GLUE (General), GSM8k (Arithmetic), HellaSwag (Rea-
soning), and MMLU (Knowledge) (Wang et al. 2018; Cobbe
et al. 2021; Zellers et al. 2019). As anticipated, the direct
application of SA resulted in a loss of accuracy on some
benchmarks. This outcome is consistent with our expecta-
tions given the lack of retraining to adapt the models fully
to the nuances of the Shared Attention mechanism. Due to
computational constraints, it was impractical for our team to
pretrain an LLM from scratch incorporating SA.
To further probe the capabilities of SA under a training
regimen, we ﬁne-tuned base LLMs equipped with Shared
Attention on the publicly available Instruct dataset (Taor i
et al. 2023). Post ﬁne-tuning, these models were tested
against the same benchmarks to ﬁnd out any performance
changes. This approach allowed us to measure the adapt-
ability of SA when models are trained to accommodate its
dynamics.
These experiments collectively demonstrate the potential
of Shared Attention to modify the traditional attention mec h-
anism in LLMs, showing a promising avenue for reduc-
ing computational demands while maintaining, and in some
cases enhancing, model performance. The detailed results
and further discussion on each benchmark and dataset are
provided in the subsequent sections.

--- PAGE 5 ---
Figure 3: Evolution of layer attention weights similarity t hroughout the pretraining phase of the Baichuan2 7B model, a s it
processes trained tokens from 220 billion to 2.6 trillion. T he color gradient in the visualization represents cosine si milarity,
effectively illustrating the transition in attention patt erns from the initial to the advanced stages of pretraining.
Experimental Setup
For the ﬁne-tuning experiments, we utilized the Llama2-7B
and Llama3-8B base models. These experiments were con-
ducted on a robust hardware conﬁguration consisting of two
NVIDIA A100 80GB GPUs. Optimization of the models
was carried out using the AdamW optimizer, with an initial
learning rate set at 2×10−5. We employed the bf16 datatype
for model parameters, which enhances the numeric range
and stability during backpropagation, crucial for maintai n-
ing precision in large model training.
Each GPU handled a micro-batch size of 16, leveraging
gradient accumulation techniques to effectively manage th e
computational load. Additionally, we utilized DeepSpeed
Zero Stage 3 to optimize the distribution of model and opti-
mizer parameters and enhance memory management across
the GPUs, ensuring efﬁcient use of available resources. The
ﬁne-tuning process spanned two epochs and employed the
standard Alpaca instruction format, which is designed to im -
prove the responsiveness and accuracy of the models in han-
dling instruction-based tasks.
Direct Application of Shared Attention
The application of SA was tested across discrete segments of
layers within the Llama2-7B and Llama3-8B models, each
comprising 32 layers in total. To evaluate the robustness an d
adaptability of SA as shown in Figure 4, it was implemented
in varying layer segments, ranging from narrower spans such
as four layers (e.g., SA:15 ∼18) to broader spans such as
eight layers (e.g., SA:23 ∼30).
Preliminary assessments of SA in the earlier layers of
Llama2-7B (e.g., layers 3 to 6) resulted in an explosion of
perplexity, indicating signiﬁcant disruptions in the mode l’s
ability to predict subsequent tokens accurately. This phe-
nomenon underscores the crucial role that attention score
variances play in the model’s early stages of processing,
which are essential for initial context setting and feature ex-
traction. To quantitatively assess the impact of attention vari-
ance throughout the model, we conducted a detailed vari-
ance analysis. We applied the same computational method
used to obtain attention mean scores to calculate the varian ce
of attention weights in Llama2-7B and Llama3-8B while
processing the MMLU dataset. We further explored the po-
tential inﬂuence of attention variance in downstream layer sFigure 4: The ﬁgure illustrates the implementation of Share d
Attention within speciﬁc layer segments of the model.
Shared Attention spans from layer 27 to 30 for a four-layer
segment and from layer 23 to 30 for an eight-layer segment.
by computing a weighted cumulative variance. This metric
aggregates the variances of all downstream layers starting
from each speciﬁc layer, weighted by the average of these
summed variances. As illustrated in Figure 5, the analy-
sis revealed that early layers exhibited signiﬁcantly high er
weighted variances compared to latter layers. This varianc e
tends to decrease as one progresses through the model’s
architecture, suggesting a stabilization of attention mec ha-
nisms in the latter layers. Given these results, our experi-
ments predominantly focused on the application of SA in
the latter layers, where such variances appear to stabilize .
The outcomes of these experiments, as summarized in Ta-
ble 1, reveal interesting patterns. For the Llama2-7B model ,
implementing SA in the latter layers (e.g., SA:23 ∼26 and
SA:27∼30) maintained relatively stable performance across
a variety of benchmarks, including GLUE and MMLU. Con-
versely, extending the scope of SA to encompass more lay-
ers, particularly mid-level layers such as SA:15 ∼18, led to a
noticeable degradation in tasks requiring mathematical re a-
soning (GSM8K).
In comparison, the Llama3-8B model, which inherently
showed higher layer-wise attention similarity as discusse d

--- PAGE 6 ---
Model GLUEGSM8K
5-shotHellaSwag MMLU
Llama2-7B 0.4050 ±0.0019 0.1395±0.0095 0.5713±0.0049 0.4119±0.0041
Llama2-7B SA:23∼30 0.3819±0.0019 0.0728±0.0072 0.5575±0.0050 0.3794±0.0040
Llama2-7B SA:27∼30 0.3882±0.0019 0.1243±0.0091 0.5616±0.0050 0.4056±0.0041
Llama2-7B SA:23∼26 0.4351±0.0019 0.1122±0.0087 0.5681±0.0049 0.3994±0.0040
Llama2-7B SA:19∼22 0.3996±0.0019 0.0834±0.0076 0.5553±0.0050 0.3926±0.0040
Llama2-7B SA:15∼18 0.3731±0.0019 0.0220±0.0040 0.4790±0.0050 0.3378±0.0047
Llama2-7B-Instruct-SFT 0.5372 ±0.0019 0.1440±0.0097 0.5772±0.0049 0.3722±0.0040
Llama2-7B-Instruct-SFT SA:23∼30 0.5401±0.0019 0.0758±0.0073 0.5671±0.0049 0.3717±0.0040
Llama3-8B 0.4804 ±0.0019 0.5155±0.0138 0.6009±0.0049 0.6198±0.0038
Llama3-8B SA:23∼30 0.5595±0.0019 0.3275±0.0129 0.6011±0.0049 0.6122±0.0038
Llama3-8B SA:27∼30 0.5532±0.0019 0.4526±0.0137 0.6060±0.0049 0.6163±0.0038
Llama3-8B SA:23∼26 0.5024±0.0019 0.4556±0.0137 0.5993±0.0049 0.6189±0.0038
Llama3-8B SA:19∼22 0.5115±0.0019 0.3745±0.0133 0.5829±0.0049 0.6181±0.0038
Llama3-8B SA:15∼18 0.4685±0.0019 0.0136±0.0032 0.5307±0.0050 0.3019±0.0038
Table 1: Performance metrics for different models across ta sks
Figure 5: The ﬁgure displays the weighted cumulative vari-
ance for the Llama2-7B-chat and Llama3-8B-instruct mod-
els. The two lower axes represent the model’s structure: the
left axis details the 32 layers, and the right axis shows the 3 2
heads within each layer. The z-axis represents the variance
values.
in the previous sections, exhibited less performance dete-
rioration when SA was applied. After implementing SA in
the layers closer to the model’s output (e.g., SA:27 ∼30), the
Llama3-8B even outperformed its original conﬁguration on
the GLUE benchmark, suggesting that strategic placement
of SA can potentially enhance the model’s performance in
complex natural language understanding tasks.
Fine-Tuning on Instruct Dataset
Given the computational constraints that preclude the pre-
training of LLMs with SA from scratch, we adopted to ﬁne-
tune existing LLMs to evaluate whether ﬁne-tuning could
ameliorate the performance deﬁcits observed with the di-
rect application of SA. This approach was particularly aime d
at understanding the adaptability of SA under a more con-
trolled learning regimen.
Fine-tuning was conducted on the publicly available In-
struct dataset, which is designed to evaluate models on task s
that require following complex instructions. This datasetwas chosen because it challenges the models to utilize their
learned representations effectively, making it an ideal be nch-
mark for testing the efﬁcacy of modiﬁcations like SA.
The results, as summarized in Table 1, demonstrate a nar-
rowed performance gap between the original models and
those modiﬁed with SA. For instance, while the original
Llama2-7B model outperformed the SA version in direct
application tests, the ﬁne-tuned Llama2-7B SA:23∼30showed
signiﬁcant improvements across multiple metrics. This sug -
gests that ﬁne-tuning enables the model to better integrate
and leverage the Shared Attention mechanism, effectively
regaining some of the lost performance noted in the initial
application of SA.
These ﬁndings indicate the potential of ﬁne-tuning as a
viable method for integrating new architectural changes li ke
SA into existing models. The recovery in performance indi-
cates that with adequate training, the initial disadvantag es of
directly applying SA can be mitigated, leading to enhanced
model capabilities that more closely align with or even ex-
ceed their original conﬁgurations.
Future Directions
Our experimental investigations have demonstrated that im -
plementing Shared Attention (SA) across multiple latter la y-
ers in LLMs arouses minimal accuracy loss, making it a
promising approach for enhancing model efﬁciency. Fur-
thermore, our analysis reveals a trend towards isotropic at -
tention patterns during the pretraining process, indicati ng
that the models’ attention mechanisms tend to stabilize as
they process more data.
Given these insights, integrating SA from the pretraining
appears to be a particularly beneﬁcial strategy. This early
integration could allow models to better adapt to the stream -
lined attention mechanism, potentially improving perfor-
mance and efﬁciency across various tasks. The foundational
embedding of SA might simplify later adaptations and in-
herently supports efﬁcient attention dynamics.

--- PAGE 7 ---
Another promising research direction involves exploring
combinations between SA and other attention-sharing strat e-
gies like Cross-Layer Attention (CLA). Combining SA with
methods such as CLA could exploit the strengths of both
approaches, leading to a more robust and ﬂexible attention
mechanism. This holistic approach to attention management
could provide a comprehensive solution that maximizes both
computational efﬁciency and model scalability.
By pursuing these avenues, future research can not only
reﬁne the application of Shared Attention within LLMs but
also explore its full potential in enhancing the architec-
tural and operational efﬁciency of next-generation langua ge
models. These efforts could lead to models that are better
equipped to handle the increasing complexity and diversity
of tasks in natural language processing.
Related Work
Efﬁcient memory management in transformers is a critical
area of research with diverse objectives ranging from reduc -
ing memory bandwidth and storage requirements to optimiz-
ing computational costs during both training and inference
phases. Notably, our work focuses on minimizing the size of
the inference Key-Value (KV) cache that persists between
model passes, thereby enhancing model efﬁciency without a
signiﬁcant compromise in performance.
Memory Efﬁciency in Attention Mechanisms
Signiﬁcant efforts have been made to address the efﬁciency
of the KV cache post-training. Techniques such as KV cache
compression have been explored extensively. For instance,
methods like KVQuant (Hooper et al. 2024) and KIVI (Liu
et al. 2024b) employ quantization strategies to reduce the
memory footprint of KV pairs to just a few bits. Moreover,
works such as AttentionSink (Xiao et al. 2023) and Scis-
sorhands (Liu et al. 2024a) introduce sparsity into the KV
cache by selectively storing elements based on their proxim -
ity or importance to the generation token, thus reducing the
overall storage requirements.
Architectural Innovations for Reducing KV Cache
Architectural modiﬁcations aimed at reducing the KV cache
size are pivotal in enhancing the efﬁciency of large lan-
guage models. Such strategies include limiting the effecti ve
sequence length, as seen in Sparse Attention (Child et al.
2019), which constrain attention to local windows to reduce
both computational load and memory overhead. Another
approach involves replacing traditional softmax attentio n
with scalable alternatives like linear attention (Katharo pou-
los et al. 2020), which maintains constant space complexity
and offers more graceful scaling with respect to the token
count. Additionally, methods such as Grouped-Query At-
tention (GQA) (Ainslie et al. 2023) and Multi-Query Atten-
tion (MQA) (Shazeer 2019) aggregate attention across mul-
tiple queries, signiﬁcantly decreasing the memory footpri nt
by sharing KV pairs across attention heads. These innova-
tions collectively contribute to reducing the redundancy i n
attention calculations and are directly relevant to our wor k,informing our development of the Shared Attention mecha-
nism that further optimizes memory usage by sharing atten-
tion weights across layers.
Conclusion
In this paper, we explored the attention dynamics within ad-
vanced LLMs and observed that the attention distribution
across layers tends to isotropize following extensive pre-
training. This isotropic pattern of attention, where layer s
exhibit similar attention mechanisms, inspired a novel ap-
proach to attention sharing that departs from conventional
methods.
Traditionally, methods like MQA and CLA have focused
on sharing KV caches to reduce memory overheads but
still required the computation of attention weights indepe n-
dently across each layer. Our proposed Shared Attention
(SA) method bypasses this redundancy by directly sharing
the computed attention weights across multiple layers. Thi s
approach not only signiﬁcantly reduces the size of the KV
cache but also decreases the computational FLOPs required
during model inference.
The introduction of Shared Attention represents a
paradigm shift in the design of attention mechanisms in neu-
ral networks, emphasizing efﬁciency without compromising
the model’s performance. By reducing both the computa-
tional burden and memory requirements, SA enables more
scalable and efﬁcient deployment of LLMs, particularly in
environments where resources are constrained.
This research paves the way for further explorations into
efﬁcient model architectures and opens up new possibilitie s
for the application of LLMs across a broader spectrum of
tasks and datasets. Future work will focus on expanding the
applicability of Shared Attention, exploring its integrat ion
during the initial phases of model training, and combining i t
with other optimization techniques to maximize the opera-
tional efﬁciency of LLMs.
References
Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y .;
Lebr´ on, F.; and Sanghai, S. 2023. Gqa: Training gen-
eralized multi-query transformer models from multi-head
checkpoints. arXiv preprint arXiv:2305.13245 .
Bai, J.; Bai, S.; Chu, Y .; Cui, Z.; Dang, K.; Deng, X.; Fan,
Y .; Ge, W.; Han, Y .; Huang, F.; et al. 2023. Qwen technical
report. arXiv preprint arXiv:2309.16609 .
Brandon, W.; Mishra, M.; Nrusimha, A.; Panda, R.; and
Kelly, J. R. 2024. Reducing Transformer Key-Value
Cache Size with Cross-Layer Attention. arXiv preprint
arXiv:2405.12981 .
Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.
Generating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 .
Cobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;
Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R. ;
et al. 2021. Training veriﬁers to solve math word problems.
arXiv preprint arXiv:2110.14168 .

--- PAGE 8 ---
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,
M.; Song, D.; and Steinhardt, J. 2020. Measuring mas-
sive multitask language understanding. arXiv preprint
arXiv:2009.03300 .
Hooper, C.; Kim, S.; Mohammadzadeh, H.; Mahoney,
M. W.; Shao, Y . S.; Keutzer, K.; and Gholami, A.
2024. Kvquant: Towards 10 million context length llm
inference with kv cache quantization. arXiv preprint
arXiv:2401.18079 .
Huang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,
J.; Lv, C.; Zhang, Y .; Fu, Y .; et al. 2024. C-eval: A multi-
level multi-discipline chinese evaluation suite for found ation
models. Advances in Neural Information Processing Sys-
tems, 36.
Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F.
2020. Transformers are rnns: Fast autoregressive transfor m-
ers with linear attention. In International conference on ma-
chine learning , 5156–5165. PMLR.
Li, H.; Zhang, Y .; Koto, F.; Yang, Y .; Zhao, H.; Gong, Y .;
Duan, N.; and Baldwin, T. 2023. Cmmlu: Measuring mas-
sive multitask language understanding in chinese. arXiv
preprint arXiv:2306.09212 .
Liu, Z.; Desai, A.; Liao, F.; Wang, W.; Xie, V .; Xu, Z.;
Kyrillidis, A.; and Shrivastava, A. 2024a. Scissorhands: E x-
ploiting the persistence of importance hypothesis for llm k v
cache compression at test time. Advances in Neural Infor-
mation Processing Systems , 36.
Liu, Z.; Yuan, J.; Jin, H.; Zhong, S.; Xu, Z.; Braverman,
V .; Chen, B.; and Kivi, X. H. 2024b. A tuning-free asym-
metric 2bit quantization for kv cache. arXiv preprint
arXiv:2402.02750 .
Shazeer, N. 2019. Fast transformer decoding: One write-
head is all you need. arXiv preprint arXiv:1911.02150 .
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;
Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca:
A strong, replicable instruction-following model. Stanford
Center for Research on Foundation Models. https://crfm.
stanford. edu/2023/03/13/alpaca. html , 3(6): 7.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A .;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale ,
S.; et al. 2023. Llama 2: Open foundation and ﬁne-tuned
chat models. arXiv preprint arXiv:2307.09288 .
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems , 30.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2018. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 .
Xiao, G.; Tian, Y .; Chen, B.; Han, S.; and Lewis, M. 2023.
Efﬁcient streaming language models with attention sinks.
arXiv preprint arXiv:2309.17453 .
Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.;
Lv, C.; Pan, D.; Wang, D.; Yan, D.; et al. 2023. Baichuan
2: Open large-scale language models. arXiv preprint
arXiv:2309.10305 .Zellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,
Y . 2019. Hellaswag: Can a machine really ﬁnish your sen-
tence? arXiv preprint arXiv:1905.07830 .
