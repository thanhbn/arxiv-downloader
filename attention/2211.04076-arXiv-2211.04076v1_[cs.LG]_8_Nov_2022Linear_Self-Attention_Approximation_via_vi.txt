# 2211.04076.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2211.04076.pdf
# Kích thước file: 92932 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
arXiv:2211.04076v1  [cs.LG]  8 Nov 2022Xấp xỉ Tự chú ý Tuyến tính thông qua
Kernel Feedforward Có thể Huấn luyện
Uladzislau Yorsh và Alexander Kovalenko[0000−0002−7194−1874]
Khoa Công nghệ Thông tin, Đại học Kỹ thuật Czech tại Prague
Prague, Cộng hòa Czech
{yorshula, kovalale }@fit.cvut.cz
Hạn chế restrictive của Transformers [18] do độ phức tạp bậc hai của cơ chế tự chú ý đã thúc đẩy một lĩnh vực nghiên cứu mới về Transformers hiệu quả [17], nhằm xấp xỉ kiến trúc gốc với các mô hình nhanh hơn về mặt tiệm cận.

Mặc dù Transformers là phổ biến, không thiên vị và có thể xử lý các phụ thuộc dài tùy ý, độ phức tạp bậc hai về không gian và thời gian hạn chế các ứng dụng Transformer trên chuỗi dài. Trong bối cảnh này, nhiều phát hiện về việc xấp xỉ attention với các module nhanh hơn về mặt tiệm cận đã được thực hiện để giải quyết chuỗi dài hơn. Tuy nhiên, do thiếu một benchmark thống nhất và có hệ thống, việc đánh giá tổng thể vẫn không chắc chắn cho đến khi Tay et al. [16] công bố benchmark cho các mô hình Transformer hiệu quả có tên "Long Range Arena", bao gồm các task với nhiều loại dữ liệu khác nhau.

Trong việc theo đuổi tính toán nhanh hơn, các Transformers Hiệu quả thể hiện sự đa dạng ấn tượng về các cách tiếp cận—các mô hình đạt được độ phức tạp attention dưới bậc hai có thể sử dụng khái niệm về tính thưa thớt [2,3,15] hoặc xấp xỉ low-rank của inputs [19,20] để giảm số lượng keys được chú ý; các cách khác để giảm độ phức tạp bao gồm locality-sensitive hashing [12], key pooling [21], bộ nhớ bổ sung để lưu trữ thông tin ở dạng nén [7,14] hoặc lai ghép với các kiến trúc khác, như CNNs [1,8].

Thường dựa trên nền tảng toán học vững chắc, các phương pháp kernelized cho phép xấp xỉ attention với độ phức tạp tuyến tính trong khi vẫn duy trì độ chính xác cao. Công trình của Katharopoulos et al. [11] mô tả một xấp xỉ bao gồm việc tính toán attention bằng tích vô hướng của queries và keys được chiếu. Sau đó, công trình của Choromanski et al. [4] đã chứng minh rằng xấp xỉ như vậy có thể chính xác tùy ý và vững chắc về mặt toán học, trong khi công trình của Chowdhury et al. [5] báo cáo rằng phép chiếu có thể được học. Do đó, trong bài báo này chúng tôi nhằm mở rộng ý tưởng về các phương pháp kernel có thể huấn luyện để xấp xỉ cơ chế tự chú ý của kiến trúc Transformer.

Đóng góp của chúng tôi: cho rằng mạng neural feedforward với ít nhất một lớp ẩn [6], phi tuyến tính tùy ý, và số lượng neuron tùy ý có thể xấp xỉ bất kỳ hàm có hành vi tốt nào với độ chính xác tùy ý, điều này mang lại cho mạng neural feedforward tiềm năng trở thành bộ xấp xỉ phổ quát [9]. Do đó, chúng tôi đề xuất rằng hàm kernel có thể huấn luyện φ(·) có thể xấp xỉ attention softmax truyền thống một cách hiệu quả. Vì vậy, chúng tôi nghiên cứu khả năng sử dụng mạng neural feedforward để biểu diễn φ(·). Chúng tôi thử nghiệm với kiến trúc của φ(·) và kiểm tra hiệu suất của nó trên ba task Long Range Arena—phân loại văn bản,

--- TRANG 2 ---
2 Yorsh et al.
khớp tài liệu và ListOps, tuân theo hướng dẫn về hạn chế [16] số lượng tham số có thể huấn luyện trong mô hình để cung cấp các chỉ số có thể so sánh.

Mô hình Kernelized. Các mô hình kernelized dựa trên phân tích thừa số sau của một phép toán attention:

Att(qi,K,V) =L/summationdisplay
j=1κ(qi,kj)/summationtextL
j′=1κ(qi,kj′)vj≈φ(qi)T/summationtextL
j=1φ(kj)vT
j
φ(qi)T/summationtextL
j=1φ(kj)

trong đó qi là một token query, K và V là các ma trận key và value, κ(q,k) là hàm kernel để mô hình hóa (exp(qTk) cho Transformer cơ bản) và φ(·) là hàm chiếu chúng tôi xấp xỉ. φ(·) được yêu cầu phải dương để duy trì tính ổn định số học, và có thể thay đổi từ các hàm đơn giản như ELU + 1 đến các xấp xỉ kernel softmax ngẫu nhiên. Trong công trình của chúng tôi, thay vì các chiến lược xấp xỉ với prior mạnh, chúng tôi sử dụng một hàm tổng quát như feedforward NN.

Feedforward Kernel. Chúng tôi bắt đầu với FFN một lớp, được định nghĩa là:
φ(X) =Softplus (XW)
trong đó W∈Rn×n là ma trận trọng số lớp. Đáng ngạc nhiên, mô hình này đã cho thấy sự cải thiện hiệu suất đáng kể so với Performer và gần với leader của bài báo LRA gốc. Theo [10], chúng tôi có thể tăng cường hiệu suất bằng cách ép buộc tính trực giao thông qua khởi tạo trực giao và regularization.

Chúng tôi cũng đã thử chồng nhiều lớp hơn, nhưng không quan sát thấy sự cải thiện hiệu suất – có hoặc không có các lớp chuẩn hóa ở giữa. Chúng tôi đã thử các phi tuyến tính GELU và logistic sigmoid.

GLU Kernel. Gated Linear Units được định nghĩa là:
GLU(X) =XWf⊙σ(XWg)
trong đó σ(·) là logistic sigmoid và Wf,Wg là các ma trận trọng số. Lớp này cung cấp phi tuyến tính theo từng phần tử và có thể biểu diễn các hàm phức tạp hơn, nhưng yêu cầu tham số hóa gấp đôi so với tuyến tính.

Cho mục đích của mô hình của chúng tôi, chúng tôi cần sửa đổi GLU cuối cùng để buộc đầu ra dương:
GLUoutput(X) =Softplus (XWf)⊙σ(XWg)

Chúng tôi cũng buộc tính trực giao của Wf trong các đơn vị này theo cách tương tự như trong phần trước. Chúng tôi gọi các đơn vị được regularized này là O(rthogonal)GLU.

Để giảm thiểu sự tăng trưởng tham số hóa, chúng tôi áp dụng các biến đổi theo head, và đề xuất rằng gating không yêu cầu lượng thông tin như biến đổi đầu vào. Vì vậy, chúng tôi có thể xấp xỉ Wg với, chẳng hạn, hai ma trận low-rank có kích thước n×r tương ứng r×n trong đó r <n/2 là rank xấp xỉ Wg. Chúng tôi gọi đơn vị này là A(pproximated)OGLU.

--- TRANG 3 ---
Linear Self-Attention Approximation 3
Gating. So với FFN một lớp trực giao, mô hình OGLU một lớp hội tụ thậm chí nhanh hơn và cho thấy phương sai điểm số ít hơn đáng kể giữa các lần chạy. Các đơn vị này cũng có thể được chồng tuần tự với lợi ích, đến một mức độ nhất định. Mặt khác, tham số hóa gấp đôi sẽ không cho phép chồng hơn hai đơn vị mà không vượt quá 10% tham số bổ sung.

Bằng cách xấp xỉ ma trận trọng số gating, chúng tôi có thể chồng nhiều đơn vị hơn – nhưng theo Bảng 1, điều này không mang lại lợi thế với chi phí tính toán cao hơn. Chúng tôi đã sử dụng các ma trận có rank r=n/4 để xấp xỉ gate, giảm tham số hóa lớp 25%.

Thí nghiệm. Theo khuyến nghị từ [16], chúng tôi tái tạo lịch trình học và tất cả các siêu tham số liên quan đến mô hình của chúng tôi, trong khi giữ tham số hóa bổ sung dưới 10%. Do hạn chế về sức mạnh tính toán, chúng tôi chỉ giới hạn bản thân trong ba task LRA—phân loại văn bản BPE, khớp văn bản BPE và ListOps, với độ dài đầu vào tương ứng 4K/4K/2K. Để cung cấp kết quả có thể so sánh và tái tạo, chúng tôi đã sử dụng tích lũy gradient để mô phỏng kích thước batch lớn hơn. Mỗi mô hình được huấn luyện năm lần để quan sát hành vi mô hình và tránh cái gọi là black swans — các seed ngẫu nhiên cho kết quả khác biệt hoàn toàn [13]. Kết quả trung bình và tốt nhất được báo cáo trong Bảng 1.

Mô hình Độ phức tạp Phân loại. Khớp ListOps
Transformer O(L2) 64.27 57.46 36.37
Linear kernel† O(CL) 65.77 73.51 18.54
1×GLU O(CL) 65.82 72.17 18.67
2×GLU O(CL) 65.99 73.36 18.42
3×GLU O(CL) 65.87 72.60 18.68
Orth. linear kernel O(CL) 65.86 72.63 18.19
1×OGLU O(CL) 65.95 72.50 18.45
2×OGLU O(CL) 66.02 72.96 18.32
3×AOGLU O(CL) 66.06 72.57 18.45

Bảng 1: Kết quả của các mô hình của chúng tôi trên các task LRA được chọn, kết quả trung bình cho năm lần chạy. Chúng tôi ký hiệu bằng † các mô hình cho thấy phương sai đáng kể trong kết quả.

Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi Bộ Giáo dục, Thanh niên và Thể thao Czech từ Chương trình Hoạt động Czech về Nghiên cứu, Phát triển và Giáo dục, dưới thỏa thuận tài trợ số CZ.02.1.01/0.0/0.0/15003/0000421.

--- TRANG 4 ---
4 Yorsh et al.
Tài liệu tham khảo
1. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Mạng convolutional tăng cường attention (2020)
2. Beltagy, I., Peters, M.E., Cohan, A.: Longformer: Transformer tài liệu dài (2020)
3. Child, R., Gray, S., Radford, A., Sutskever, I.: Tạo chuỗi dài với sparse transformers (2019)
4. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L., Weller, A.: Suy nghĩ lại về attention với performers (2021)
5. Chowdhury, S.P., Solomou, A., Dubey, A., Sachan, M.: Về việc học kernel transformer (2021)
6. Cybenko, G.: Xấp xỉ bằng superposition của hàm sigmoid. Toán học về điều khiển, tín hiệu và hệ thống 2(4), 303–314 (1989)
7. Dai, Z., Yang, Z., Yang, Y., Carbonell, J.G., Le, Q.V., Salakhutdinov, R.: Transformer-xl: Các mô hình ngôn ngữ chú ý vượt ra ngoài bối cảnh độ dài cố định. CoRR abs/1901.02860 (2019), http://arxiv.org/abs/1901.02860
8. Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., Pang, R.: Conformer: Transformer tăng cường convolution cho nhận dạng giọng nói (2020)
9. Hornik, K.: Khả năng xấp xỉ của mạng feedforward đa lớp. Mạng neural 4(2), 251–257 (1991)
10. Jia, K., Li, S., Wen, Y., Liu, T., Tao, D.: Mạng neural sâu trực giao (2019)
11. Katharopoulos, A., Vyas, A., Pappas, N., Fleuret, F.: Transformers là rnns: Transformers tự hồi quy nhanh với attention tuyến tính (2020)
12. Kitaev, N., Łukasz Kaiser, Levskaya, A.: Reformer: Transformer hiệu quả (2020)
13. Picard, D.: Torch.manual_seed(3407) is all you need: Về ảnh hưởng của seed ngẫu nhiên trong kiến trúc deep learning cho computer vision (2021)
14. Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T.P.: Transformers nén cho mô hình hóa chuỗi tầm xa (2019)
15. Roy, A., Saffar, M., Vaswani, A., Grangier, D.: Attention thưa thớt dựa trên nội dung hiệu quả với routing transformers (2020)
16. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., Metzler, D.: Long range arena: Một benchmark cho transformers hiệu quả (2020)
17. Tay, Y., Dehghani, M., Bahri, D., Metzler, D.: Transformers hiệu quả: Một khảo sát (2020)
18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (2017)
19. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Tự chú ý với độ phức tạp tuyến tính. CoRR abs/2006.04768 (2020), https://arxiv.org/abs/2006.04768
20. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V.: Nyström­former: Thuật toán dựa trên Nyström để xấp xỉ tự chú ý. CoRR abs/2102.03902 (2021), https://arxiv.org/abs/2102.03902
21. Zhang, H., Gong, Y., Shen, Y., Li, W., Lv, J., Duan, N., Chen, W.: Poolingformer: Mô hình hóa tài liệu dài với pooling attention. CoRR abs/2105.04371 (2021), https://arxiv.org/abs/2105.04371
