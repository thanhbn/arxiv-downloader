# 2402.18668.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2402.18668.pdf
# File size: 2859995 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Simple linear attention language models balance the recall-throughput tradeoff
Simran Arora* 1Sabri Eyuboglu* 1Michael Zhang* 1Aman Timalsina2Silas Alberti1Dylan Zinsley2
James Zou1Atri Rudra2Christopher R ´e1
Abstract
Recent work has shown that attention-based lan-
guage models excel at recall , the ability to ground
generations in tokens previously seen in context.
However, the efficiency of attention-based mod-
els is bottle-necked during inference by the KV-
cache’s aggressive memory consumption. In this
work, we explore whether we can improve lan-
guage model efficiency ( e.g. by reducing mem-
ory consumption) without compromising on re-
call. By applying experiments and theory to a
broad set of architectures, we identify a key trade-
off between a model’s state size and recall abil-
ity. We show that efficient alternatives to atten-
tion ( e.g.H3, Mamba, RWKV) maintain a fixed-
size recurrent state, but struggle at recall. We
propose BASED a simple architecture combining
linear and sliding window attention. By vary-
ingBASED window size and linear attention fea-
ture dimension, we can dial the state size and
traverse the Pareto frontier of the recall-memory
tradeoff curve, recovering the full quality of at-
tention on one end and the small state size of
attention-alternatives on the other. We train lan-
guage models up to 1.3b parameters and show that
BASED matches the strongest sub-quadratic mod-
els (e.g.Mamba) in perplexity and outperforms
them on real-world recall-intensive tasks by 10.36
accuracy points. We further develop IO-aware
algorithms that enable BASED to provide 24×
higher throughput on language generation than
FlashAttention-2, when generating 1024 tokens
using 1.3b parameter models. Overall, BASED ex-
pands the Pareto frontier of the throughput-recall
tradeoff space beyond prior architectures.
*Equal contribution1Stanford University2University of Buffalo.
Correspondence to: Simran Arora <simarora@stanford.edu >,
Sabri Eyuboglu <eyuboglu@stanford.edu >, Michael Zhang
<mzhang20@stanford.edu >.
Proceedings of the 2ndEfficient Systems for Foundation Models
Workshop at the International Conference on Machine Learning
(ICML) , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).1. Introduction
The choice of sequence mixer ( e.g. attention, convolu-
tion) in a language model affects both its quality and ef-
ficiency (Arora et al., 2023a; Vaswani et al., 2017). Prior
work shows that attention excels at recall , the ability to
ground generations in previously seen tokens (Olsson et al.,
2022; Arora et al., 2023a). On the other hand, the throughput
of attention-based models is bottle-necked during training
by quadratic compute complexity and during inference by
aggressive memory consumption. The natural question is:
can we improve the real-world speed and memory-use of
language models without comprising on quality?
Recently, a number of architectures have been proposed
that enable substantially higher throughput while competing
with attention in perplexity (Wang et al., 2022; Gu and Dao,
2023; Yang et al., 2023; Poli et al., 2023; Peng et al., 2023).
However, coarse metrics like overall perplexity can obscure
important differences in model quality. For example, recent
work shows that a specific class of architectures, gated-
convolutions , despite complexity scaling sub-quadratically
in sequence length, are asymptotically less efficient than
attention at performing recall (Arora et al., 2023a). Building
on this analysis, we evaluate a broader class of architectures
across real-world recall-intensive tasks and show attention
improves over a currently-popular attention-free alternative,
Mamba, by 32.2 accuracy points (Table 1).1
Motivated by these observations, we explore the Pareto fron-
tier of the tradeoff between high-recall and high-throughput
models. We evaluate a range of architectures on a popu-
lar synthetic associative recall task (Arora et al., 2023a;
Fu et al., 2023a; Olsson et al., 2022). Since generation
throughput is bottle-necked by memory consumption, we
vary hyperparameters (e.g. model dimension) that affect the
size of the recurrent state during generation and demonstrate
a fundamental recall-memory tradeoff that holds across ar-
chitecture classes (Figure 2). Attention performs associative
recall perfectly, but the recurrent state ( i.e.the KV-cache)
grows linearly with the sequence length. Sliding window
1Examples of recall-intensive tasks include information extrac-
tion, reading comprehension, summarization and code generation.
These require using in context information (contrasting memorized
information) during generation.
1arXiv:2402.18668v2  [cs.CL]  7 Mar 2025

--- PAGE 2 ---
Simple linear attention language models balance the recall-throughput tradeoff
Sliding window width /gid00035/gid00064/gid00082/gid00068/gid00067
 /gid00045/gid00072/gid00077/gid00068/gid00064/gid00081/gid00001/gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077
 /gid00052/gid00075/gid00072/gid00067/gid00072/gid00077/gid00070/gid00001/gid00056/gid00072/gid00077/gid00067/gid00078/gid00086/gid00051/gid00068/gid00066/gid00064/gid00075/gid00075/gid00001/gid00034/gid00066/gid00066/gid00084/gid00081/gid00064/gid00066/gid00088/gid00001 /gid00053/gid00068/gid00077/gid00082/gid00078/gid00081/gid00001/gid00036/gid00078/gid00081/gid00068/gid00001/gid00040/gid00038/gid00046/gid00046/gid00001/gid00045/gid00064/gid00083/gid00068/gid00077/gid00066/gid00088
Sliding window width /gid00045/gid00072/gid00077/gid00068/gid00064/gid00081/gid00001/gid00064/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077
/gid00052/gid00075/gid00072/gid00067/gid00072/gid00077/gid00070/gid00001/gid00056/gid00072/gid00077/gid00067/gid00078/gid00086
/gid00035/gid00064/gid00082/gid00068/gid00067 Taylor a pproximation 
provides large memory for
recall 
✔
Precise local token shifts
and comparison Limited memory for long
range recall
 
✔ ✗Precise local token shifts
and comparison Taylor a pproximation 
provides large memory for
recall 
✔
Precise local token shifts
and comparison  
✔✗
Figure 1. BASED overview. Combining linear attention with tinysliding window softmax attention (e.g., 64 or 128 tokens in width)
enables improved recall accuracy with limited efficiency overhead vs. smaller tile sizes. ( Left) Time to execute Cutlass GEMMs ( y) vs.
sliding window attention size ( x), with batch size 512on tensor cores. ( Center) Model recall accuracy ( y) vs. sliding window attention
size (x). We compare linear attention alone (dark blue), sliding window attention alone (light blue), and their combination ( BASED ,
orange). ( Right ) Schematic diagram of B ASED illustrating how the two components complement each other.
attention (SWA) can cap the size of the recurrent state at the
cost of worse long-range recall (Jiang et al., 2023). However,
Mamba, a recently proposed SSM architecture expands the
Pareto frontier beyond SWA. This begs the question: are
there other, perhaps simpler, models that can also expand
the Pareto frontier?
To reduce the memory consumption, we consider using two
simple techniques: SWA and softmax-approximating linear
attention. Our results on language modeling (Table 1) and
synthetic recall experiments (Figure 1, center) suggest nei-
ther primitive alone suffices to navigate the Pareto frontier.
1.We find that linear attention alone struggles to solve
associative recall (Figure 1, center). We hypothesize
that this is because linear attention lacks the precision
to perform local token shifts and comparisons (Fu et al.,
2023a; Arora et al., 2023a).
2.Insliding window attention , associative recall range is
limited by the width of the windows (Figure 1, center).
As we increase the window size, the recurrent state grows
linearly and has a non-linear affect on speed during par-
allel training and inference (Figure 1, left).
We combine these two techniques into a single architec-
ture, which we call BASED (Figure 1, right). We find that
SWA and linear attention complement each other, enabling
BASED to expand the pareto frontier of the recall-memory
tradeoff (Figure 2). We suspect that (1) the large recurrent
memory of linear attention could help model long-range
token interactions in the sequence and (2) SWA handles the
precise local shifts needed to perform associative recall.
To make BASED competitive with SoTA attention (Dao,
2023) and recurrent (Gu and Dao, 2023) models under
wall-clock and throughput metrics, we introduce several
IO-aware optimizations.1.Despite the theoretical efficiency benefits, linear at-
tention implementations are often slower than well-
optimized attention implementations (Dao et al., 2022).
To make our attention competitive in real-world wall-
clock time and memory usage, we provide hardware-
efficient CUDA algorithms for liner attention generation
prefill (Algorithm 1) and decoding (Algorithm 2).
InBASED , we show that the 2nd-order Taylor approxi-
mation of softmax as the linear attention feature map is
hardware-efficient. With sequence length Nand head di-
mension d, this na ¨ıvely requires O(Nd3)time and space
complexity (Zhang et al., 2024; Keles et al., 2023). Rel-
ative to the baseline, our algorithm reduces data move-
ment from HBM (slower-to-access memory) to SRAM
(faster-to-access memory) by O(Nd2)bytes and from
SRAM to register by O(Nd3)bytes (Section 5).
2.Sliding window attention exploits tensor cores, special-
ized units on modern GPUs for performing matrix multi-
plications (GEMMs). While prior architectures use large
window sizes (e.g. 4096 for Mistral-7B (Jiang et al.,
2023)), we propose to use small 64−128windows,
guided by hardware properties. Size 64−128window
sizes keep the tensor cores occupied Figure 1 (left).
In experiments, we show that BASED competes in qual-
ity with strong Transformer++ (Touvron et al., 2023) and
SoTA sub-quadratic baselines in models up to the 1.3Bn
parameters across language modeling on the Pile language,
DNA modeling, and the LM Eval Harness (Gao et al., 2023).
Beyond this, BASED outperforms a strong sub-quadratic
architecture, Mamba, on the associative recall slice of the
Pile and in downstream recall-intensive tasks by 10.36ac-
curacy points. In efficiency, BASED enables up to 24×
higher throughput than the strong FlashAttention-2 imple-
mentation on generation. Code for this work is provided at:
https://github.com/HazyResearch/based .
2

--- PAGE 3 ---
Simple linear attention language models balance the recall-throughput tradeoff
2. Preliminaries and Related Work
We discuss the key relevant work in this section and provide
an extended discussion in Appendix A.
Attention Thede facto language modeling primitive, soft-
max attention (Vaswani et al., 2017) takes inputs x∈RN×d
of length Nand head dimension d, and computes outputs
y∈RN×dvia the softmax over projections q,k,v=
xWq,xWk,xWv,i.e.,
yi=i∑︂
j=1exp(q⊤
ikj/√
d)vj∑︁i
m=1exp(q⊤
ikm/√
d)(1)
in the causal case where Wq,Wk,Wv∈Rd×dare learn-
able matrices . While effective at recall (Arora et al., 2023a)
and efficient to train (Eq 1 is parallelizable on GPUs and
O(N)in memory with recent advances (Dao et al., 2022)),
attention remains expensive for generation. For every new
output yn, we require ndoperations over a growing KV-
cache of prior {ki,vi}n−1
i=1. This results in larger memory
consumption and lower-throughput for longer sequences.
Efficient attentions Various works thus try to improve
on attention’s efficiency without sacrificing quality. Sparse
attentions reduce attention’s time and memory requirements
by only attending over specific strided patterns or local slid-
ing windows (Parmar et al., 2018; Child et al., 2019; Beltagy
et al., 2020). While further popularized in large language
models (Mistral, Jiang et al. (2023)), prior works either un-
derperform full attention with sparse patterns that fail to
capture dense interactions, or use large window sizes that
still permit large KV-caches and subsequent inefficiency.
Meanwhile, linear attentions replace the softmax in standard
attention with alternative kernel functions (Katharopoulos
et al., 2020a; Choromanski et al., 2020; 2021; Qin et al.,
2022a; Keles et al., 2023). By removing the exp(q⊤k)in
favor of feature map dot-products ϕ(q)⊤ϕ(k), these meth-
ods use matrix product associativity to compute attention
inO(Nd2)time and space (Katharopoulos et al., 2020b).
Furthermore, they permit a recurrent view for constant mem-
ory and O(1)time per-token generation (Kasai et al., 2021;
Schlag et al., 2021). However, present linear attention fea-
ture maps either fail to match standard attention on recall
or remain expensive to compute (Zhang et al., 2024). Lin-
ear attentions are slower in wall-clock time compared to
optimized attention implementations (Dao et al., 2022).
The line of work studying how to combine sparse and linear
attention into a single layer is also closely related to our
work (Zaheer et al., 2020; Beltagy et al., 2020; Chen et al.,
2021a; Zeng et al., 2022).
Attention alternatives Finally, various models use
attention-free sequence mixers such as state-space models
(SSMs) (Gu et al., 2021; Sun et al., 2023), gated convolu-
tions (Fu et al., 2023a; Poli et al., 2023) and input-dependentrecurrences (Peng et al., 2023; Gu and Dao, 2023) to rival
attention performance while improving its efficiency. How-
ever, while recent such models can match attention in overall
perplexity, further study suggests they may underperform
Transformers on tasks such as recall and in-context learn-
ing (Arora et al., 2023a; Aky ¨urek et al., 2024).
3. No Free Lunch: Memory-Recall Tradeoff
In this section, we demonstrate a fundamental tradeoff be-
tween a model’s memory consumption during inference ( i.e.,
the size of its recurrent state) and its capacity to perform
recall. We use a combination of experiments on synthetic
data and theoretical analysis.
•Empirical study of memory-recall tradeoff : In Sec-
tion 3.1, we evaluate a number of popular architecture
classes ( e.g.Mamba, Hyena) on a synthetic associative re-
call task, varying hyperparameters that affect the model’s
recurrent state size (Figure 2). Within each architecture
class, we observe a clear tradeoff: the larger the recur-
rent state size, the better recall. However, for a fixed
recurrent state size, performance is not consistent across
architectures. We observe that some sequence mixers fall
well-below the Pareto-frontier. This motivates the design
of sequence mixers that can expand the Pareto frontier.
•Lower bounds on memory required for recall : In Sec-
tion 3.2, we lower bound the recurrent state size required
to perform exact recall with anyrecurrent model Theo-
rem F.6. This analysis reinforces our empirical observa-
tions on the throughput-recall tradeoff.
3.1. Empirical study of memory-recall tradeoff
Setup. We use a synthetic AR task called Multi-Query
Associative Recall ( MQAR ) (Arora et al., 2023a) to demon-
strate the trade-off. In this task, input sequences consist
of a number of key-value pairs followed by queries. For a
given query, the model must recall the corresponding key-
value pair from earlier in the sequence in order to predict
the next token. For example, the correct output for input
below would be 4, 6, 1, 2, 3:
A 4 B 3 C 6 F 1⏞⏟⏟⏞
Key-ValueE 2→A ? C ? F ?⏞⏟⏟⏞
QueryE ? B ?
We train on sequences of length 256 tokens containing be-
tween 4 and 64 key-value pairs. During evaluation, we
measure accuracy on sequences of length 1,024 tokens con-
taining between 4 and 256 key-value pairs.
We train and evaluate six sequence mixers: atten-
tion (Vaswani et al., 2017), sliding window attention (Belt-
agy et al., 2020), Mamba (Gu and Dao, 2023), H3 (Fu et al.,
3

--- PAGE 4 ---
Simple linear attention language models balance the recall-throughput tradeoff
G4 C6 F1 B3 D2 A4 C?
/gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077 /gid00044/gid00055/gid00014/gid00066/gid00064/gid00066/gid00071/gid00068
/gid00045/gid00072/gid00077/gid00068/gid00064/gid00081/gid00001/gid00064/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077/gid00019/gid00067
/gid00052/gid00068/gid00080/gid00084/gid00068/gid00077/gid00066/gid00068/gid00001/gid00075/gid00068/gid00077/gid00070/gid00083/gid00071
/gid00036/gid00078/gid00077/gid00085/gid00078/gid00075/gid00084/gid00083/gid00072/gid00078/gid00077/gid00067/gid00067
/gid00039/gid00068/gid00064/gid00083/gid00084/gid00081/gid00068/gid00001/gid00067/gid00072/gid00076/gid000156/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00085/gid00068
/gid00051/gid00068/gid00066/gid00064/gid00075/gid00075/gid00051/gid00068/gid00066/gid00084/gid00081/gid00081/gid00068/gid00077/gid00083/gid00001/gid00082/gid00083/gid00064/gid00083/gid00068/gid00001/gid00082/gid00072/gid00089/gid00068/gid00001/gid00009/gid00065/gid00088/gid00083/gid00068/gid00082/gid00010/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00085/gid00068/gid00001/gid00051/gid00068/gid00066/gid00064/gid00075/gid00075/gid00001/gid00034/gid00066/gid00066/gid00084/gid00081/gid00064/gid00066/gid00088
/gid00039/gid00084/gid00075/gid00075
/gid00041/gid00088/gid00068/gid00077/gid00064/gid00052/gid00052/gid00046
/gid00046/gid00064/gid00076/gid00065/gid00064/gid00035/gid00064/gid00082/gid00068/gid00067
/gid00041/gid00020
/gid00067/gid00052/gid00083/gid00064/gid00083/gid00068/gid00001/gid00067/gid00072/gid00076/gid00015
/gid00039/gid00072/gid00075/gid00083/gid00068/gid00081/gid00001/gid00086/gid00072/gid00067/gid00083/gid00071/gid00051/gid00068/gid00066/gid00084/gid00081/gid00081/gid00068/gid00077/gid00083/gid00001/gid00052/gid00083/gid00064/gid00083/gid00068 /gid00046/gid00078/gid00067/gid00068/gid00075
/gid00052/gid00075/gid00072/gid00067/gid00072/gid00077/gid00070
Figure 2. Throughput (memory) - recall tradeoff. x-axis shows
state size (bytes) during generation; y-axis shows accuracy on
the MQAR MQAR recall task (Arora et al., 2023a). For each
architecture, we train several models varying hyperparameters that
affect the recurrent state size ( e.g. model dimension). The plot
shows a fundamental tradeoff between the recurrent state size and
recall capacity that applies to broad class of models (Arora et al.,
2023a; Gu and Dao, 2023; Fu et al., 2023a).
2023a), Hyena (Poli et al., 2023), and BASED . For each, we
vary hyperparameters that affect the memory consumption
during inference (e.g., in sliding window attention we vary
the window width). We measure how MQAR accuracy
varies with the size of the recurrent state and Appendix E.1
contains details on how state sizes are calculated.
Figures 2 and 3 can be reproduced or extended to new archi-
tectures using the scripts provided at https://github.
com/HazyResearch/zoology .
Results In Figure 2, we demonstrate a fundamental trade-
off between recurrent state size and accuracy on MQAR
that holds within and across architecture classes. Within
each architecture class ( e.g. H3 models), increasing the
recurrent state size almost always leads to an improvement
in accuracy. Across architecture classes, we see a tradeoff
as well. Attention achieves perfect recall accuracy, but its
recurrent state size grows with the length of the sequence.
Other architecture classes like Mamba and H3 admit models
with much smaller recurrent states, but these models have
limited recall capacity.Given a fixed recurrent state, not all architectures have the
same recall capacity. Among architectures proposed in
prior work, Mamba makes the best use of a limited memory
budget. Notably, architectures with a convolutional view
(e.g.Hyena and H3) fall well below the Pareto frontier. Our
proposed architecture, BASED (introduced in Section 4),
expands the Pareto-frontier beyond Mamba. By varying
hyper-parameters that determine its state size ( e.g.feature
dimension and model dimension), we can smoothly navigate
the tradeoff between efficient models and memory-hungry
models with high recall capacity.
3.2. Theoretical Analysis
Our theoretical analysis provides further insight into the
empirical observations described above. First, using results
from communication complexity theory, we show that the
recall capacity of anycausal model ( e.g.Mamba, Attention)
is bounded by the size of its recurrent state (Theorem F.12
in Appendix F).
Theorem 3.1. Any recurrent model2depending causally on
inputu∈ {0,1}N×drequires Ω(N)-bits3in state size to
solve MQAR .
This result suggests that the tradeoff observed in Figure 2 is
fundamental, not an artifact of architectural quirks.
Next, we focus on gated-convolutions , a broad class of ar-
chitectures built from gating and convolutions ( e.g. H3,
Hyena, RWKV v4). To make progress in theoretically an-
alyzing the broad set of gated convolution proposals, prior
work develops a canonical gated-convolution, referred to as
BaseConv which can provably simulate anyarchitecture
built from gating and convolution primitives.
Building on this work, we show that BaseConv cannot
solve MQAR in constant-many layers (Theorem F.19 and
Theorem F.29 in Appendix F).
Theorem 3.2. Given an input sequence u∈ {0,1}3N×d,
where Nandddenote the sequence length and head dimen-
sion, respectively, a data-independent BaseConv model
needs log(2d)-layers to solve MQAR ford= log2(c),
where cdenotes the vocabulary size4.
Remark 3.3.For a class of input encodings that general-
izes one-hot encodings termed as p-hot encodings (Def-
inition F.22), input-dependent BaseConv needs at least
⌊log(2p)⌋-layers to solve MQAR where d=p·p√c.
The above result is not as strong when c≪N, for which
we prove a complementary lower bound (Theorem F.14 in
Appendix F):
2For Mamba (Gu and Dao, 2023), see Corollary F.13.
3Here, we need the entries of the state to be bounded.
4That is, each token from the vocabulary has the natural binary
encoding in {0,1}log2(c)
4

--- PAGE 5 ---
Simple linear attention language models balance the recall-throughput tradeoff
Theorem 3.4. Given an input u∈ {0,1}N×dto the
MQAR with any encoding such that logc≤d≤
2(logN)1−ϵforϵ >0, and cpossible tokens from the vo-
cabulary with c≤N, a data-independent BaseConv
model with model parameters taking O(logN)bits needs
Ω(ϵlog log N)layers to solve AR.
In contrast, Arora et al. (2023a) show that attention solves
MQAR in constant-many layers. This result helps to ex-
plain why the gated-convolution architectures (H3 and
Hyena) in Figure 2 lie below the Pareto frontier established
by newer architectures.
Note that Theorem 3.2 and Theorem 3.4 imply that we need
Ω(max(log log c,log log N))many BaseConv layers to
solve MQAR. One might wonder if we can improve this
lower bound. In Theorem F.30, we show that this is the
best possible lower bound by showing that for certain set-
tings, O(max(log log c,log log N))BaseConv layers are
enough to solve MQAR.
Finally, we show that we can simulate linear atten-
tion (Katharopoulos et al., 2020a), the foundation of BASED ,
using BaseConv (Arora et al., 2023a) with a poly-log
blowup in the number of layers (Proposition F.8 in Ap-
pendix F), pointing to the relative efficiency of linear atten-
tion over gated-convolution architectures.
4. The B ASED Architecture
In this section, we introduce BASED . Our objective in de-
signing this architecture is to demonstrate how we can navi-
gate the Pareto-frontier of the memory-recall tradeoff using
well-known architectural building blocks.
Softmax attention excels at recall, but since its recurrent
state, the KV-cache, grows unconstrained with the length of
sequence, it is stuck in the upper right quadrant of Figure 2.
We study two simple approaches for constraining the size
of attention’s recurrent state: linear attention and sliding
window attention. The recurrent state size of linear attention
(i.e. attention without softmax) does not grow with the
sequence length and can be modulated by changing simple
hyperparameters (Katharopoulos et al., 2020a). With sliding
window attention, we cap the recurrent state size to be the
width of the window.
However, our experiments on real-world language modeling
(Table 6) and synthetic associative recall (Figure 1 middle)
suggest that neither primitive alone suffices to navigate the
pareto frontier. Linear attention lacks the precision to per-
form local token shifts and comparisons (Fu et al., 2023b;
Arora et al., 2023a). In sliding window attention, associative
recall range is limited by the width of the windows (Figure
2, center). As we increase the window size, the recurrent
state grows linearly and has a non-linear effect on speedduring parallel training and inference (Figure 2, left).
BASED combines (1) softmax-approximating linear atten-
tion applied globally and (2) exact softmax attention applied
locally in small sliding windows (Figure 1, right). This al-
lows us to use softmax attention in surprisingly small sliding
windows ( e.g.,64−128tokens) that recover 90.8%of full
softmax attention’s recall accuracy at 1e-5 ×its latency.
4.1. Taylor Linear Attention
By approximating softmax attention using linear feature
maps, we can constrain the size of the recurrent state while
maintaining global token interactions ( i.e.each token de-
pends on every token before it in the sequence).
Katharopoulos et al. (2020a); Tsai et al. (2019); Choroman-
ski et al. (2020) show that we can select a feature map
ϕ:Rd→Rd˜such that ϕ(qi)⊤ϕ(kj)≈exp(q⊤
ikj/√
d).
We can then rewrite the formula for softmax attention in
Equation (1) as
i∑︂
j=1ϕ(qi)⊤ϕ(kj)vj
ϕ(qi)∑︁i
j=1ϕ(kj)=ϕ(qi)∑︁i
j=1(︁
ϕ(kj)⊤vj)︁
ϕ(qi)∑︁i
j=1ϕ(kj)(2)
where every query attends to every past key in O(Nd2)time
and space complexity. Furthermore, Katharopoulos et al.
(2020b) show that linear attention has a fixed size recurrent
state during generation. Letting si=∑︁i
j=1ϕ(kj)⊤vjand
zi=∑︁i
j=1ϕ(kj)⊤be a “KV-state” and “K-state” respec-
tively, we can compute Equation (2) as
si=si−1+ϕ(ki)⊤vi,zi=zi−1+ϕ(ki)⊤,
yi=ϕ(qi)si
ϕ(qi)zi(3)
where si∈Rd×d˜andzi∈Rd˜.
Feature map. To approximate exp(q⊤
ikj/√
d), we use
the2nd-order Taylor series feature map, picking ϕ:Rd→
Rd2such that
ϕ(qi)⊤ϕ(kj) = 1 + q⊤
ikj+(q⊤
ikj)2
2(4)
While Zhang et al. (2024) note that picking a feature map
withd˜=d2results in linear attention with O(Nd3)time
and space complexity and large recurrent state of size O(d3),
we can tradeoff efficiency for recall capacity by projecting
queries and keys to smaller dimensions i.e.,Wq,Wk∈
Rd×d′withd′= 16 . By changing d′we modulate the size
of the recurrent state.
How does the choice of feature map affect the memory-recall
tradeoff? Prior work demonstrates the strong performance
5

--- PAGE 6 ---
Simple linear attention language models balance the recall-throughput tradeoff
/gid00053/gid00064/gid00088/gid00075/gid00078/gid00081
/gid00042/gid00067/gid00068/gid00077/gid00083/gid00072/gid00083/gid00088
/gid00051/gid00068/gid00066/gid00084/gid00081/gid00081/gid00068/gid00077/gid00083/gid00001/gid00082/gid00083/gid00064/gid00083/gid00068/gid00001/gid00082/gid00072/gid00089/gid00068/gid00001/gid00009/gid00065/gid00088/gid00083/gid00068/gid00082/gid00010
/gid00049/gid00064/gid00081/gid00064/gid00076/gid00068/gid00083/gid00068/gid00081/gid00001/gid00036/gid00078/gid00084/gid00077/gid00083/gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00085/gid00068/gid00001/gid00051/gid00068/gid00066/gid00064/gid00075/gid00075/gid00001/gid00034/gid00066/gid00066/gid00084/gid00081/gid00064/gid00066/gid00088 /gid00034/gid00082/gid00082/gid00078/gid00066/gid00072/gid00064/gid00083/gid00072/gid00085/gid00068/gid00001/gid00051/gid00068/gid00066/gid00064/gid00075/gid00075/gid00001/gid00034/gid00066/gid00066/gid00084/gid00081/gid00064/gid00066/gid00088/gid00049/gid00078/gid00082/gid00038/gid00045/gid00054 /gid00051/gid00068/gid00045/gid00054 /gid00052/gid00080/gid00084/gid00064/gid00081/gid00068
/gid00049/gid00068/gid00081/gid00069/gid00078/gid00081/gid00076/gid00068/gid00081 /gid00036/gid00078/gid00082/gid00039/gid00078/gid00081/gid00076/gid00068/gid00081/gid00039/gid00068/gid00064/gid00083/gid00084/gid00081/gid00068/gid00001/gid00046/gid00064/gid00079
Figure 3. Linear attention feature maps on AR. x: state size
(bytes) during generation or param. count; y:MQAR accuracy.
This setting is harder than Figure 2 (256 key-value pairs).
of the Taylor feature map on associative recall (Zhang et al.,
2024). Building on this analysis, we evaluate a broad set
of feature maps ( ϕReLU(x) = max( x,0),ϕPosELU (x) =
ELU(x) + 1 ,ϕSquare(x) =x2,ϕIdentity (x) =x,ϕCosFormer
as defined in (Qin et al., 2022a), and ϕPerformer as defined in
(Choromanski et al., 2020)) using the experimental setup
described in Section 3.1. In Figure 3 (top), we plot the
memory-recall tradeoff curves for these feature maps. The
Taylor series feature map, along with the simple ϕPosELU and
ϕReLU feature maps, sits at the Pareto frontier. One advan-
tage of the Taylor feature map over these alternatives is that
it expands the recurrent state size (improving recall capac-
ity) without changing the number of parameters. As shown
in Figure 3 (bottom), the Taylor series feature map requires
fewer parameters than alternatives to achieve high recall ca-
pacity. This analysis and the ablations in Table 6 informed
our decision to use the Taylor approximation, though other
simple feature maps may be effective as well.
4.2. Local Exact Attention with Small Sliding Windows
To efficiently model fine-grained local interactions, BASED
uses sliding window attention (SWA) with window sizes setat small multiples of 16(up to 64 tokens). Similar to past
(causal) implementations (Child et al., 2019; Beltagy et al.,
2020), for window size weach query qionly attends to past
keys{ki−w+1, . . . ,ki}. This enables O(Nw)time and
space complexity for linear scaling in sequence length N,
with a w-sized KV-cache for constant-memory generation.
However, unlike past SWA that keep wat sizes 256 (Parmar
et al., 2018) to 4096 (Jiang et al., 2023), BASED uses only
w≤128to best exploit modern GPUs. In Section 5, we dis-
cuss how this “Tensor core-aware” window ( TCWINDOW )
achieves 1e-5 ×the latency than the w= 4096 windows in
modern LLMs ( e.g., Mistral 7B (Jiang et al., 2023)).
While the small winTCWINDOW enable fast local and exact
attention, it presents a challenge for long range modeling.
With just w= 64 , for every layer of w= 4096 Mistral
sliding window attention we would require 64layers of
BASED to achieve the same receptive field. Controlling for
model depth and sequence length, Figure 2 indeed shows
smaller wlinearly decreasing in associative recall accuracy.
BASED ’s global linear attention described above overcomes
the lack of long-range modeling presented with low w.
Finally, we find that including gated convolution layers with
short convolutions (e.g., filter size 3) gives additional benefit
over only using TCWINDOW layers. Short convolutions
can help perform local, precise shifts for token comparisons
since they operate over the full sequence, while TCWINDOW
does not. These local mixers can complement one-another.
Additional architectural details for BASED are discussed
in Appendix C and the hybridization of layers used in ex-
periments are provided in Table 7. We include ablations
of architectural choices in Table 6 and evaluate the overall
quality and efficiency of B ASED in Section 6.
5. Efficient Implementation
In this section we focus on the efficiency of BASED . A
na¨ıve implementation is slower than the most efficient stan-
dard attention implementations (shown in Figure 4) as it
requires large amounts of high latency memory movement.
We first describe preliminaries of the GPU execution model
and memory hierarchy. We next present the baseline and
our hardware-aware algorithms for linear attention in Sec-
tion 5.2 and for sliding window attention in Appendix B.2.2.
5.1. Preliminaries
GPU operations, or kernels , are executed by many paral-
lel threads. GPU streaming multiprocessors launch thread
blocks at the software level. These blocks are divided into
warps (e.g.32 threads) that are assigned to cores at the hard-
ware level. Threads need to read inputs into their registers
to perform computations and write the outputs. The time
6

--- PAGE 7 ---
Simple linear attention language models balance the recall-throughput tradeoff
taken to read and write is referred to as the IO cost.
Operations could either be memory or compute bound, de-
pending on the time to load data vs. perform computations
on loaded data. In designing our IO-aware algorithms, we
would like to exploit two key properties of modern GPUs.
First, tensor core units (fast matrix multiply units) achieve
312 TFLOP/s speeds relative to 19 TFLOP/s for the non-
matrix multiply cores. Second, GPUs face a memory hi-
erarchy with large amounts of slow-to-access memory and
smaller amounts of fast-to-access memory. For instance,
the hierarchy on a modern NVIDIA 80GB A100 GPU is:
80GB of HBM with 2 TB/s bandwidth, 80MB of L2 cache,
192KB of L1 cache / shared memory (implemented via
SRAM) with 19 TB/s bandwidth per SM, and 256 KB of
register file per SM (NVIDIA, 2022). Register memory is
private to an executing thread, so threads need to write to
shared memory to communicate data to other threads in
the block. To reduce the IO cost, a key principle is to fuse
multiple operations on the same data slice while it’s in fast
memory before writing it back to slow memory.
5.2. Taylor Exponential Linear Attention
Despite the theoretical efficiency, the popular linear atten-
tion implementations are less efficient than well-optimized
softmax attention implementations when measured in real-
world wall-clock time and memory usage (Dao et al., 2022).
We next present hardware-aware algorithms to make Taylor
linear attention efficient. We focus on two operations: (1)
prefill (this section), corresponding to processing the prompt
during generation or the forward pass during training, and
(2) next token prediction during generation (Appendix B),
which also requires updating the recurrent hidden state state.
In this section, we refer to the batch size as B, number of
heads as H, head dimension as d, sequence length as Nand
feature dimension as d′, following Section 4. For ease of
notation, let D= 1 + d′+d′2in this section. Additional
details for these algorithms are in Appendix B
Baseline Implementation The na ¨ıve implementation de-
tailed in Appendix B only uses a CUDA kernel to compute
the causal dot product between q,k, andvprojections
(Vyas et al., 2020), but computes the feature maps in python
(non IO-aware). This is inefficient given the computation
required for the feature map computation.
Analysis In overall IO cost, ignoring the input and output pro-
jections in the linear attention layer, this procedure requires
2BHND bytes for writing featurized q,kto HBM. During
the causal dot product, this requires 2BHND +BHNd
bytes to read q,k,vtiles and BHNd bytes to write the
result. Throughout the computation, O(BHNDd )bytes
(note this is the shape KV state during the forward pass)
are read in and out of thread registers to SRAM to update
the running output and KV state at 19TB/s bandwidth.Algorithm Our kernel computes both the feature map
and causal dot product, detailed in Algorithm 1. Here we
describe the key insights. First, to handle causality in the
dot-product computation, for each tileof output yi∈R16×d,
we split the computation as shown, where qi,ki,viare also
now tiles of 16 tokens, handled in parallel by the kernel.
yi= Causal( qT
iki)vi+qii−1∑︂
j=0(kjvj)
where the first term uses the quadratic attention view and
requires applying causal masking. The second term uses the
linear view and its causality has already been handled.
Second the large KV-state,∑︁i−1
j=0(kjvj),∈RD×d, needs to
be stored as we iterate over the length- 16tiles. By partition-
ing across workers (warps), we can store the state in thread
registers (fastest memory). The partitioning is restricted by
(1) each warp has a limited quantity of threads and (2) warps
cannot access the thread memory of other warps.
Analysis In IO cost, again ignoring the input and output pro-
jections in the linear attention layer, our procedure requires
2BHNd′bytes for reading q, kand2BHNd bytes for read-
ingvand writing output ybetween HBM and SRAM. Over-
all, our algorithm avoids in HBM O(2BHND )bytes in
HBM to SRAM data movement. We further improve upon
the baseline by storing the KV-state in-register to avoid the
O(BHNDd )bytes in SRAM to register data movement.
End-to-end benchmarks for BASED implemented with these
IO-aware algorithms are provided in Section 6. Micro-
benchmarks for each kernel against the baseline implemen-
tations are provided in Appendix B.
6. Results
In this section, we present results for the following claims:
1.Language modeling overall. We evaluate architectures
in pretraining on the Pile (Gao et al., 2020) and on stan-
dard natural language understanding benchmarks. We
findBASED matches or outperforms prior sub-quadratic
architectures across these settings.
2.Language modeling recall. BASED closes the gap to
attention on the challenging associative recall slice of
the Pile corpus (see Table 1). We apply these pretrained
models zero-shot to a suite of recall-intensive tasks ( e.g.
information extraction, QA), showing that BASED sys-
tematically outperforms Mamba by large margins (10.36
accuracy points at 1.3b parameters and 50b tokens).
3.Generation throughput. Our IO-aware implementa-
tion of recurrent generation in Based enables 40−60%
speedups relative to FlashAttention-2 and Mamba for pre-
fill at 4ksequence length and up to 24×higher through-
7

--- PAGE 8 ---
Simple linear attention language models balance the recall-throughput tradeoff
Architecture Params/TokensEfficiency Language Modeling (Pile) Info. Extraction QA Common
Prefill Generate All AR Other SWDE FDA SQUAD LM-Evals
Tok./ms ↑ Tok./ms ↑ Ppl.↓ Ppl.↓ Ppl.↓ Acc↑ Acc↑ F1↑ Avg. Acc. ↑
Transformer++ 1.33b/10b 103.50 0.99 7.26 1.74 8.10 71.92 73.23 36.19 47.64
BASED 1.35b/10b 161.71 24.28 7.43 1.87 8.26 48.06 24.41 30.46 46.68
Mamba 1.32b/10b 112.22 25.69 7.48 1.96 8.29 34.74 12.89 28.20 46.84
Transformer++ 1.33b/50b 103.50 0.99 6.28 1.65 6.82 76.50 80.47 43.47 53.33
BASED 1.35b/50b 161.71 24.28 6.30 1.71 6.82 64.45 30.40 41.62 53.81
Mamba 1.32b/50b 112.22 25.69 6.28 1.74 6.78 52.75 18.51 35.92 53.50
Transformer++ 360m/10b 207.77 23.82 8.39 1.87 9.42 57.97 58.00 27.18 44.08
BASED 363m/10b 514.57 47.23 8.65 2.07 9.64 29.16 11.71 25.07 43.03
Mamba 358m/10b 267.09 39.95 8.64 2.21 9.59 23.67 6.53 24.06 43.51
GLA 362m/10b — — 9.12 2.36 10.68 — — — —
RWKV v5 362m/10b — — 9.79 2.40 10.90 — — — —
H3 362m/10b — — 10.60 4.88 11.23 6.75 0.64 7.87 39.35
Transformer++ 360m/30b 103.50 0.99 7.68 1.80 8.40 70.75 63.79 25.07 44.75
BASED 363m/30b 161.71 24.28 7.77 1.93 8.46 45.01 16.45 32.67 45.36
Mamba 358m/30b 112.22 25.69 7.73 2.02 8.38 27.63 8.71 26.71 45.62
Table 1. Evaluation of pre-trained language models. Models were trained on the same sets of 10b to50b tokens drawn from the Pile
corpus (Gao et al., 2020). We report inference throughput on 4,096tokens ( 16,384for 360m param.) of pre-fill and 2,048tokens of
recurrent generation for a subset of architectures. We report language model perplexity on the overall Pile test set as well as perplexity
on two slices of the test set: associative recall tokens and other tokens (see Section 6.1, (Arora et al., 2023a)). We report zero-shot
performance on three recall-intensive tasks: information retrieval on SWDE and FDA as well as question answering on SQUAD. Finally,
we report average performance on the set of LM Eval Harness (Gao et al., 2023) common sense reasoning tasks used in Gu and Dao
(2023), details in Appendix D. These tasks do not require significant recall capacity because the input text is typically very short. See
Section 6.1. Some proposed architectures that do not implement recurrent views for generation are marked with a —.
/gid00040/gid00068/gid00077/gid00068/gid00081/gid00064/gid00083/gid00072/gid00078/gid00077/gid00001/gid00049/gid00081/gid00068/gid00195/gid00075/gid00075 /gid00051/gid00068/gid00066/gid00084/gid00081/gid00081/gid00068/gid00077/gid00083/gid00001/gid00040/gid00068/gid00077/gid00068/gid00081/gid00064/gid00083/gid00072/gid00078/gid00077
/gid00020/gid00023/gid00017/gid00076/gid00001/gid00079/gid00064/gid00081/gid00064/gid00076/gid00068/gid00083/gid00068/gid00081/gid00082 /gid00018/gid00065/gid00001/gid00079/gid00064/gid00081/gid00064/gid00076/gid00068/gid00083/gid00068/gid00081/gid00082 /gid00020/gid00023/gid00017/gid00076/gid00001/gid00079/gid00064/gid00081/gid00064/gid00076/gid00068/gid00083/gid00068/gid00081/gid00082 /gid00018/gid00065/gid00001/gid00079/gid00064/gid00081/gid00064/gid00076/gid00068/gid00083/gid00068/gid00081/gid00082/gid00045/gid00064/gid00083/gid00068/gid00077/gid00066/gid00088/gid00001/gid00009/gid00076/gid00082/gid00010/gid00045/gid00064/gid00083/gid00068/gid00077/gid00066/gid00088/gid00001/gid00009/gid00076/gid00082/gid00010
/gid00047/gid00084/gid00076/gid00065/gid00068/gid00081/gid00001/gid00078/gid00069/gid00001/gid00083/gid00078/gid00074/gid00068/gid00077/gid00082 /gid00047/gid00084/gid00076/gid00065/gid00068/gid00081/gid00001/gid00078/gid00069/gid00001/gid00083/gid00078/gid00074/gid00068/gid00077/gid00082 /gid00035/gid00064/gid00083/gid00066/gid00071/gid00001/gid00082/gid00072/gid00089/gid00068 /gid00035/gid00064/gid00083/gid00066/gid00071/gid00001/gid00082/gid00072/gid00089/gid00068/gid00039/gid00075/gid00064/gid00082/gid00071/gid00034/gid00083/gid00083/gid00068/gid00077/gid00083/gid00072/gid00078/gid00077
/gid00046/gid00064/gid00076/gid00065/gid00064/gid00035/gid00064/gid00082/gid00068/gid00067/gid00001/gid00009/gid00049/gid00088/gid00053/gid00078/gid00081/gid00066/gid00071/gid00010/gid00001
/gid00035/gid00064/gid00082/gid00068/gid00067/gid00001/gid00009/gid00036/gid00054/gid00037/gid00034/gid00010/gid00001
Figure 4. (Left) Throughput numbers for the varied prefill sequence lengths at a fixed batch size of 2.Right generation throughput at
varied batch sizes at a fixed generation length of 1024 tokens. The y-axis shows the in latency (ms). Lines are cutoff when the model runs
out of memory. We show results for both 360M and 1.3Bn, and all numbers are computed on a single NVIDIA H100 GPU.
put over FlashAttention-2 in generating 1024 tokens at
batch size 128(see Figure 4).
Baselines We compare to several key baselines at the
360m and 1.3b parameter scales, up to 50b tokens of train-
ing. We first consider Transformer++, Transformers with
modern improvements such as rotary encodings (Su et al.,
2023) and gated linear units (Touvron et al., 2023). We then
consider a class of popular efficient architectures built from
gating and long-convolution primitives including Hyena
(Poli et al., 2023), RWKV (Peng et al., 2023), and H3 (Fu
et al., 2023a). We finally compare to the recently popular
Mamba (Gu and Dao, 2023) and Gated Linear Attention
(Yang et al., 2023) linear recurrent architectures with input-
dependent recurrent-state updates. We give each architec-
ture the Transformer++ improvements as relevant and usethe implementations provided by prior work during training.
BASED combines familiar local and global sequence mixers
to achieve high quality. We train BASED as a hybrid of
≈20% linear attention, ≈20% sliding window attention,
and≈60% gated convolution layers as discussed in Ap-
pendix E.1. In contrast to recent baselines, BASED requires
no input-dependent decays whatsoever.
6.1. Language Modeling Evaluations
Language Modeling Benchmarks We pretrain language
models from scratch at two parameter scales ( 360m and 1.3b
parameters) on the Pile (Gao et al., 2020). Each model sees
the same tokens of pretraining data in the same order. The
Pile data is tokenized using the GPT-2 BPE tokenizer (Rad-
ford et al., 2019). We measure perplexity on the Pile and
8

--- PAGE 9 ---
Simple linear attention language models balance the recall-throughput tradeoff
report results in Table 1 and further experimental details are
provided in Appendix E.1.
We additionally evaluate the pretrained models on key natu-
ral language understanding downstream benchmarks using
the LM Eval Harness (SuperGLUE, ARC, PIQA, Wino-
Grande, HellaSwag, LAMBADA). A detailed breakdown
of tasks and metrics can be found in Appendix D.
In both pretraining and on the downstream tasks, BASED
consistently competes with the strongest Transformer++ and
Mamba baselines. While these overall metrics are helpful,
we next turn to a fine-grained analysis of recall and in-
context learning ability on real-world data.
Recall Evaluations We evaluate our pretrained models
on a suite of in-context learning tasks selected to test the
downstream recall capacity in Table 1. These tasks fall into
three categories: (1) Real-world AR Beyond perplexity
scores, we slice the next token predictions on the Pile to un-
derstand each architecture’s AR quality ( Appendix E.1). (2)
Information extraction (IE) SWDE and FDA are popular
semi-structured and unstructured document IE benchmarks
respectively (Wu et al., 2021; Deng et al., 2022; Arora et al.,
2023b). SWDE has HTML for 8Movie and 5University
websites (e.g. IMDB, US News) and annotations for 8-
274 attributes per website (e.g., Movie runtime ), and (3)
Question answering from in-context passages.
We find BASED outperforms the baseline sub-quadratic ar-
chitectures across these evaluations, closing the gap to Trans-
former++. These trends track the MQAR synthetic results
from Section 3.1. We further observe that as we train for
longer (more tokens), the improvements from BASED over
Mamba grow (from 3.9to9.0points on average at 360m
scale and from 9.0to10.4points at the 1.3b scale).
Quality Ablations In Table 6, we ablate the feature maps,
feature dimensions, and sliding window and convolution
dimensions using the same Pile setting as prior experiments.
In feature maps, we consider replacing the Taylor approxi-
mation with CosFormer (Qin et al., 2022a) or Performers
(Choromanski et al., 2020), and varying the state size using
linear projections. We observe with larger sate size, Cos-
Former closes the gap to the Taylor map though note the
projections increase the parameter count. In feature dimen-
sion, we find 24and32provide diminishing returns. Further
discussion is in Appendix E.1.
6.2. Efficiency Benchmarks
We benchmark the throughput of BASED , with and without
our proposed IO-Aware algorithms (Section 5, Figure 4).
We consider both the forward pass / generation prefill and
next token prediction stages. Experiments were run using
an H100 NVIDIA GPU and averaged over 20repetitions.End-to-end benchmarks Using our efficient implemen-
tation (Section 5), BASED achieves 56% faster prefill than
FlashAttention-2 (Dao, 2023) and 44% faster than Mamba
at4ksequence length and 1.3b parameters (28% faster than
FlashAttention-2 and 76% faster than Mamba at 360mpa-
rameters). We find that next token generation, with no pre-
fill, provides 24×higher throughput (tokens/second) over
the highly optimized FlashAttention-2 implementation and
achieves 95% and the throughput of the recurrent Mamba
architecture at batch size 128and1.3b parameters (98%
higher throughput vs. FlashAttention-2 and 118% higher
throughput vs. Mamba at 360m parameters). All bench-
marks is on a single NVIDIA H100 GPU, using CUDA
cache graphs during next token prediction (NVIDIA, 2019).
In Figure 4, we also include results for the baseline imple-
mentation of BASED that uses the popular Fast Transform-
ers CUDA kernel to compute the causal dot product (Vyas
et al., 2020) (discussed in Section 5). The custom kernel
introduced in our work unlocks the efficiency of B ASED .
Micro benchmarks As the end-to-end BASED architec-
ture is a hybrid architecture, we provide micro benchmarks
of the individual kernels against key baseline implementa-
tions in Appendix B. Kernels are accessible at: https://
github.com/HazyResearch/ThunderKittens .
7. Conclusion
This work identifies a fundamental tradeoff between recall, a
critical skill for in-context learning, and throughput through
theory and experiments. Attention performs recall perfectly,
but requires retaining a KV cache that grows with the se-
quence length. As an alternative, we propose the BASED
architecture, which combines two simple techniques — lo-
cal fine-grained attention and long-range linear attention via
a Taylor approximation of the softmax exponential function –
that are sub-quadratic during training and permit an efficient
recurrent inference view. To enable wall clock efficiency,
we introduce IO-aware algorithms for the Taylor linear at-
tention inference that lead BASED to perform generation up
to24×faster than FlashAttention-2 at the 1.3b parameter
scale (generating 1024 tokens, batch size 128). Beyond
competing in overall perplexity, BASED outperforms prior
sub-quadratic architectures in recall quality by 10.36 ac-
curacy points on average. Overall, our results show that
BASED extends the Pareto frontier of the recall-throughput
tradeoff space beyond prior architectures.
Acknowledgments
We thank Benjamin Spector, Dylan Zinsley, Songlin Yang,
Daniel Fu, Jessica Grogan, Eric Nguyen, Michael Wornow,
Alyssa Unell, and Gautam Machiraju for their helpful
feedback and discussion during this work. We thank the
9

--- PAGE 10 ---
Simple linear attention language models balance the recall-throughput tradeoff
Hazy Research lab and Together AI for supporting this
work. We gratefully acknowledge the support of NIH
under No. U54EB020405 (Mobilize), NSF under Nos.
CCF2247015 (Hardware-Aware), CCF1763315 (Beyond
Sparsity), CCF1563078 (V olume to Velocity), and 1937301
(RTML); US DEVCOM ARL under Nos. W911NF-23-2-
0184 (Long-context) and W911NF-21-2-0251 (Interactive
Human-AI Teaming); ONR under Nos. N000142312633
(Deep Signal Processing), N000141712266 (Unifying Weak
Supervision), N000142012480 (Non-Euclidean Geometry),
and N000142012275 (NEPTUNE); Stanford HAI under No.
247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft,
NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture,
Ericsson, Qualcomm, Analog Devices, Google Cloud, Sales-
force, Total, the HAI-GCP Cloud Credits for Research pro-
gram, the Stanford Data Science Initiative (SDSI), and mem-
bers of the Stanford DAWN project: Facebook, Google, and
VMWare. The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes notwith-
standing any copyright notation thereon. Any opinions,
findings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reflect the views, policies, or endorsements, either expressed
or implied, of NIH, ONR, or the U.S. Government. AR’s
research is supported by NSF grant CCF#2247014.
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. We intend for BASED to aid in re-
ducing the costs of machine learning and in unlocking new
capabilities. There are many potential societal consequences
of our work, none which we feel must be specifically high-
lighted here. Detailed discussions of the risks of using and
developing LLMs are in Bommasani et al. (2021); Wei-
dinger et al. (2021).
References
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys John-
son, Michael Poli, James Zou, Atri Rudra, and Christo-
pher R ´e. Zoology: Measuring and improving recall in
efficient language models. International Conference on
Learning Representations , 2023a.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. volume 30,
2017.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. In-
context learning and induction heads. arXiv preprint
arXiv:2209.11895 , 2022.Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexan-
der M Rush. Pretraining without attention. arXiv preprint
arXiv:2212.10544 , 2022.
Albert Gu and Tri Dao. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
arXiv:2312.00752 , 2023.
Songlin Yang, Bailin Wang, Yikang Shen, Rameswar
Panda, and Yoon Kim. Gated linear attention trans-
formers with hardware-efficient training. arXiv preprint
arXiv:2312.06635 , 2023.
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y
Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano
Ermon, and Christopher R ´e. Hyena hierarchy: Towards
larger convolutional language models. arXiv preprint
arXiv:2302.10866 , 2023.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,
Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael
Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He,
Haowen Hou, Przemyslaw Kazienko, Jan Kocon, and
Jiaming et al. Kong. Rwkv: Reinventing rnns for the
transformer era. arXiv:2305.13048 , 2023.
Daniel Y . Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas,
Atri Rudra, and Christopher R ´e. Hungry Hungry Hippos:
Towards language modeling with state space models. In
International Conference on Learning Representations ,
2023a.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume
Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint
arXiv:2310.06825 , 2023.
Tri Dao. FlashAttention-2: Faster attention with better
parallelism and work partitioning. 2023.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and
Christopher R ´e. FlashAttention: Fast and memory-
efficient exact attention with IO-awareness. In Advances
in Neural Information Processing Systems , 2022.
Michael Zhang, Kush Bhatia, Hermann Kumbong, and
Christopher Re. The hedgehog & the porcupine: Ex-
pressive linear attentions with softmax mimicry. In The
Twelfth International Conference on Learning Represen-
tations , 2024.
Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena,
and Chinmay Hegde. On the computational complexity
of self-attention. In 34th International Conference on
Algorithmic Learning Theory , volume 201, page 1–23,
2023.
10

--- PAGE 11 ---
Simple linear attention language models balance the recall-throughput tradeoff
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale.
Llama 2: Open foundation and fine-tuned chat models.
arXiv:2307.09288 , 2023.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
Black, Anthony DiPofi, Charles Foster, Laurence Gold-
ing, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle Mc-
Donell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-
tang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin
Wang, and Andy Zou. A framework for few-shot lan-
guage model evaluation, 12 2023.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran.
Image transformer. In International conference on ma-
chine learning , pages 4055–4064. PMLR, 2018.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers.
arXiv preprint arXiv:1904.10509 , 2019.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-
former: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
and Fran c ¸ois Fleuret. Transformers are rnns: Fast autore-
gressive transformers with linear attention. In Interna-
tional conference on machine learning , pages 5156–5165.
PMLR, 2020a.
Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe-
ter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz
Kaiser, et al. Rethinking attention with performers. arXiv
preprint arXiv:2009.14794 , 2020.
Krzysztof Choromanski, Haoxian Chen, Han Lin, Yuanzhe
Ma, Arijit Sehanobish, Deepali Jain, Michael S Ryoo,
Jake Varley, Andy Zeng, Valerii Likhosherstov, et al. Hy-
brid random features. arXiv preprint arXiv:2110.04367 ,
2021.
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen
Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran
Zhong. cosformer: Rethinking softmax in attention. arXiv
preprint arXiv:2202.08791 , 2022a.
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Trans-
formers are rnns: Fast autoregressive transformers with
linear attention. In Proceedings of the International Con-
ference on Machine Learning (ICML) , 2020b.Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama,
Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen,
and Noah A. Smith. Finetuning pretrained transform-
ers into RNNs. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing ,
pages 10630–10643, Online and Punta Cana, Domini-
can Republic, November 2021. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2021.emnlp-main.
830.
Imanol Schlag, Kazuki Irie, and J ¨urgen Schmidhuber. Linear
transformers are secretly fast weight programmers. In
International Conference on Machine Learning , pages
9355–9366. PMLR, 2021.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua
Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham,
Anirudh Ravula, Qifan Wang, Li Yang, and et al. Big
bird: Transformers for longer sequences. Proceedings of
NeurIPS , 2020.
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra,
and Christopher R ´e. Scatterbrain: Unifying sparse
and low-rank attention approximation. arXiv preprint
arXiv:2110.15343 , 2021a.
Zhanpeng Zeng, Sourav Pak, Jeffrey Kline, Glenn Fung, and
Vikas Sing. Multi resolution analysis (mra) for approxi-
mate self-attention. Proceedings of the 39 th International
Conference on Machine Learning , 2022.
Albert Gu, Karan Goel, and Christopher R ´e. Efficiently
modeling long sequences with structured state spaces.
arXiv preprint arXiv:2111.00396 , 2021.
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing
Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive
network: A successor to transformer for large language
models, 2023.
Ekin Aky ¨urek, Bailin Wang, Yoon Kim, and Jacob An-
dreas. In-context language learning: Architectures and
algorithms. 2024.
Daniel Y . Fu, Elliot L. Epstein, Eric Nguyen, Armin W.
Thomas, Michael Zhang, Tri Dao, Atri Rudra, and
Christopher R ´e. Simple hardware-efficient long con-
volutions for sequence modeling. arXiv preprint
arXiv:2302.06646 , 2023b.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-
Philippe Morency, and Ruslan Salakhutdinov. Trans-
former dissection: a unified understanding of trans-
former’s attention via the lens of kernel. arXiv preprint
arXiv:1908.11775 , 2019.
NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.
11

--- PAGE 12 ---
Simple linear attention language models balance the recall-throughput tradeoff
A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers
with clustered attention. 2020.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding,
Travis Hoppe, Charles Foster, Jason Phang, Horace He,
Anish Thite, Noa Nabeshima, Shawn Presser, and Connor
Leahy. The Pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027 ,
2020.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-
former with rotary position embedding, 2023.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 1(8):9,
2019.
Eric Wu, Kevin Wu, Roxana Daneshjou, David Ouyang,
Daniel Ho, and James Zou. How medical ai devices
are evaluated: limitations and recommendations from an
analysis of fda approvals. Nature Medicine , 27:1–3, 04
2021.
Xiang Deng, Prashant Shiralkar, Colin Lockard, Binxuan
Huang, and Huan Sun. Dom-lm: Learning generalizable
representations for html documents. 2022.
Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika
Narayan, Andrew Hojel, Immanuel Trummer, and
Christopher R ´e. Language models enable simple sys-
tems for generating structured views of heterogeneous
data lakes. arXiv:2304.09433 , 2023b.
NVIDIA. Getting started with cuda graphs, 2019.
URLhttps://developer.nvidia.com/blog/
cuda-graphs/ .
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S Bern-
stein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
et al. On the opportunities and risks of foundation models.
arXiv preprint arXiv:2108.07258 , 2021.
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Grif-
fin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia
Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical
and social risks of harm from language models. arXiv
preprint arXiv:2112.04359 , 2021.
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Re-
former: The efficient transformer. arXiv preprint
arXiv:2001.04451 , 2020.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020.Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi,
Tom Goldstein, Anima Anandkumar, and Bryan Catan-
zaro. Long-short transformer: Efficient transformers for
language and vision. Advances in neural information
processing systems , 34:17723–17736, 2021.
Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Ku-
tyniok. Sumformer: Universal approximation for efficient
transformers. arXiv preprint arXiv:2307.02301 , 2023.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. Efficient transformers: A survey. ACM Computing
Surveys , 55(6):1–28, 2022.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty,
Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
Nystr ¨omformer: A nystr ¨om-based algorithm for approxi-
mating self-attention. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , volume 35, pages 14138–
14148, 2021.
Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Sky-
former: Remodel self-attention with gaussian kernel and
nystr\”om method. In A. Beygelzimer, Y . Dauphin,
P. Liang, and J. Wortman Vaughan, editors, Advances
in Neural Information Processing Systems , 2021b.
Alexandre De Brebisson and Pascal Vincent. An exploration
of softmax alternatives belonging to the spherical loss
family. arXiv preprint arXiv:1511.05042 , 2015.
Emmanuel Candes, Xiaodong Li, Yi Ma, and John Wright.
Robust principal component analysis? arXiv:0912.3599 ,
2009.
Zhenhai Zhu and Rau Soricut. H-transformer-1d: Fast
onedimensional hierarchical attention for sequences. n
Annual Meeting of the Association for Computational
Linguistics , 2021.
Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Ling-
peng Kong, Nick Barnes, and Yiran Zhong. The devil in
linear transformer. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing ,
pages 7025–7041, Abu Dhabi, United Arab Emirates,
December 2022b. Association for Computational Lin-
guistics. doi: 10.18653/v1/2022.emnlp-main.473.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016.
Biao Zhang and Rico Sennrich. Root mean square layer nor-
malization. Advances in Neural Information Processing
Systems , 32, 2019.
James W Cooley and John W Tukey. An algorithm for the
machine calculation of complex fourier series. Mathe-
matics of computation , 19(90):297–301, 1965.
12

--- PAGE 13 ---
Simple linear attention language models balance the recall-throughput tradeoff
David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M.
Tomczak, and Mark Hoogendoorn. Ckconv: Continuous
kernel convolution for sequential data. 2022.
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal
state spaces are as effective as structured state spaces,
2022.
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R ´e.
On the parameterization and initialization of diagonal
state space models, 2022.
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam
Neyshabur. Long range language modeling via gated
state spaces, 2022.
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,
Liangke Gui, Graham Neubig, Jonathan May, and Zettle-
moyer Luke. Mega: Moving average equipped gated
attention. arXiv preprint arXiv:2209.10655 , 2022.
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,
Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathe-
matical framework for transformer circuits. Transformer
Circuits Thread , 1, 2021.
Yann N Dauphin, Angela Fan, Michael Auli, and David
Grangier. Language modeling with gated convolutional
networks. In International conference on machine learn-
ing, pages 933–941. PMLR, 2017.
Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chen-
guang Zhu, and ChengXiang Zhai. Sparse modular ac-
tivation for efficient sequence modeling. arXiv preprint
arXiv:2306.11197 , 2023.
Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring atten-
tion with blockwise transformers for near-infinite context.
arXiv preprint arXiv:2310.01889 , 2023.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng,
Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao
Zhang, and Ion Stoica. Efficient memory management
for large language model serving with pagedattention. In
Proceedings of the 29th Symposium on Operating Systems
Principles , pages 611–626, 2023.
Daniel Y Fu, Hermann Kumbong, Eric Nguyen, and
Christopher R ´e. Flashfftconv: Efficient convolutions
for long sequences with tensor cores. arXiv preprint
arXiv:2311.05908 , 2023c.
Markus N Rabe and Charles Staats. Self-attention does not
need o(n2)memory. arXiv preprint arXiv:2112.05682 ,
2021.Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and
Jangwoo Kim. Mnnfast: A fast and scalable system
architecture for memory-augmented neural networks. In
2019 ACM/IEEE 46th Annual International Symposium
on Computer Architecture (ISCA) , pages 250–263, 2019.
Hao Liu and Pieter Abbeel. Blockwise parallel trans-
former for long context large models. arXiv preprint
arXiv:2305.19370 , 2023.
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Trans-
former quality in linear time. In International Conference
on Machine Learning , pages 9099–9117. PMLR, 2022.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus), 2023.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel R. Bowman. SuperGLUE: a stickier benchmark
for general-purpose language understanding systems .
Curran Associates Inc., Red Hook, NY , USA, 2019.
Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou,
Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel Fern ´andez.
The lambada dataset: Word prediction requiring a broad
discourse context, 2016.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
and Yejin Choi. Hellaswag: Can a machine really finish
your sentence?, 2019.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. Piqa: Reasoning about physical
commonsense in natural language, 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. Think you have solved question answering?
try arc, the ai2 reasoning challenge, 2018.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
and Yejin Choi. Winogrande: An adversarial winograd
schema challenge at scale, 2019.
Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas,
Callum Birch-Sykes, Michael Wornow, Aman Patel, Clay-
ton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano
Ermon, Stephen A. Baccus, and Chris R ´e. Hyenadna:
Long-range genomic sequence modeling at single nu-
cleotide resolution, 2023.
Katarina Gresova, Vlastimil Martinek, David Cechak, Petr
Simecek, and Panagiotis Alexiou. Genomic benchmarks:
A collection of datasets for genomic sequence classifica-
tion. bioRxiv , 2022. doi: 10.1101/2022.06.08.495248.
13

--- PAGE 14 ---
Simple linear attention language models balance the recall-throughput tradeoff
Colin Lockard, Prashant Shiralkar, and Xin Luna Dong.
OpenCeres: When open information extraction meets
the semi-structured web. In Jill Burstein, Christy Do-
ran, and Thamar Solorio, editors, Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Pa-
pers) , pages 3047–3056, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1309.
Peter B ¨urgisser, Michael Clausen, and Mohammad A
Shokrollahi. Algebraic complexity theory , volume 315.
Springer Science & Business Media, 2013.
Thathachar S Jayram, Ravi Kumar, and Dandapani Sivaku-
mar. The one-way communication complexity of ham-
ming distance. Theory of Computing , 4(1):129–135,
2008.
Swastik Kopparty. Topics in algorithms and complexity
theory: Spring 2020. 2020.
14

--- PAGE 15 ---
Simple linear attention language models balance the recall-throughput tradeoff
The appendix is organized as follows:
1. Appendix A includes an extended related works discussion.
2. Appendix B includes details on the IO-aware implementation and benchmarking for B ASED .
3. Appendix C includes additional discussion of B ASED architectural details.
4. Appendix D provides additional experimental results.
5. Appendix E provides experimental details.
6. Appendix F includes theoretical results and proofs.
To facilitate reproducing this work we release:
1. Code for model training and inference at https://github.com/HazyResearch/based
2. Model checkpoints at https://huggingface.co/collections/hazyresearch/
3. CUDA kernels at https://github.com/HazyResearch/ThunderKittens
4. Code for synthetic MQAR experiments at https://github.com/HazyResearch/zoology
15

--- PAGE 16 ---
Simple linear attention language models balance the recall-throughput tradeoff
A. Extended Related Work
Our work relates broadly to various developments in efficient sequence modeling. In this section, we organize these
related works into (1) model-based or algorithmic contributions (Appendix A.1) and (2) implementation or systems-based
contributions (Appendix A.2).
A.1. Efficient Language Modeling Architectures
While Transformers often achieve state-of-the-art language modeling quality, their design motivates various efficiency
improvements when both processing input sequences and generating outputs. In particular, various works try to retain their
modeling quality, while improving on their quadratic scaling ( O(N2)in input sequence length N) when processing inputs
andO(NM)time and space when decoding outputs for outputs of length M(when caching prior keys and values in the
attention mechanism).
We note that most related lines of work build on one of two primitives: attention approximations (e.g., linear attentions,
sparse attentions, sparse and low-rank attentions), or state-space models (SSMs) (which have alternative parameterizations
as either “long” convolutional models or recurrent neueral networks). Both model classes achieve subquadratic time and
space complexity when processing inputs, while linear attentions and SSMs also enable better than O(NM)decoding via
their ability to process inputs recurrently like a recurrent neural network (RNN).
We describe each of these model classes next.
A.1.1. E FFICIENT ATTENTIONS
We focus on two of the most related paradigms for efficiently computing attention here, structured sparse attentions and
linear attentions . We acknowledge a great deal of prior work to compute attention more efficiently, such as via locality-
sensitive hashing (Kitaev et al., 2020), random sparse attentions (Zaheer et al., 2020), and sequence compression (Wang
et al., 2020; Zhu et al., 2021; Alberti et al., 2023). Please see (Tay et al., 2022) for a comprehensive survey.
Structured sparse attentions Structured sparse attentions reduce attention’s time and memory requirements by only
attending over specific strided patterns or local sliding windows (Parmar et al., 2018; Child et al., 2019; Beltagy et al., 2020).
For example, (Parmar et al., 2018) propose computing attention only over a local window of the past wtokens, such that
processing sequences Ntokens long only takes ONw time and space. (Child et al., 2019) note that this window alone may
not all capture all desired dependencies (such as long-term interactions), and propose two strided patterns to compute dot
products between queries and keys further away. (Beltagy et al., 2020) further propose allowing specific tokens to attend to
all other tokens in a dense manner.
While further popularized in recent large language models (Mistral, Jiang et al. (2023)), we note that these implementations
use large window sizes that still leave room for improving efficiency. In Based, we introduce a hardware-guided design
(using small windows) and sliding window implementation that allows us to capitalize on sparse attention’s efficiency.
Linear attentions Linear attentions preserve the same “sequence-mixing” operations as standard attention, computing dot
products between queries and keys to weight corresponding values. However, their key insight is to replace the softmax in
standard attention with alternative kernel functions (Katharopoulos et al., 2020a). Mechanically, by removing the exp(q⊤k)
in favor of feature map dot-products ϕ(q)⊤ϕ(k), these methods use matrix product associativity to compute attention in
O(Nd2)time and space (Katharopoulos et al., 2020b) (Equation (2)). Furthermore, they permit a recurrent view for constant
memory and O(1)time per-token generation (Kasai et al., 2021; Schlag et al., 2021) (Equation (3)).
Prior works propose different feature maps ϕto improve linear attention modeling quality. (Katharopoulos et al., 2020a)
originally use the positive elu function 1 +elusuch that ϕ(q)⊤ϕ(k)remains positive and attention weights remain affine.
(Qin et al., 2022a) instead use the ReLU function combined with a cosine-based reweighting function to add a locality
bias. Other approaches propose feature maps that aim to approximate the Softmax, such as Random Fourier Features
(Choromanski et al., 2020; 2021) the Nystrom method (Xiong et al., 2021; Chen et al., 2021b), or deterministic low-degree
polynomial approximations (Zhang et al., 2024; De Brebisson and Vincent, 2015; Keles et al., 2023). Finally, recent works
treat the feature map as a learnable function (Kasai et al., 2021), and optionally train the feature map explicitly to recover
the softmax kernel (Zhang et al., 2024).
16

--- PAGE 17 ---
Simple linear attention language models balance the recall-throughput tradeoff
Combining sparse and linear attentions Finally, our work is closely related to a long line of work on combining
sparse and linear attention. Scatterbrain (Chen et al., 2021a), building on works such as BigBird (Zaheer et al., 2020) and
Longformer (Beltagy et al., 2020), shows how a sparse and low-rank approximations can be combined into a single unbiased
approximation. This approximation is inspired by robust PCA (Candes et al., 2009). As motivation, they show that any
low rank approximation of attention’s exp(QKT)will have a much larger approximation error than a sparse plus low rank
approximation. Note that the Scatterbrain method is largely agnostic to the details of any specific architecture or choice of
hyperparameters used in the sparse and low-rank approximations. The focus is on how to combine them so as to maintain
an unbiased estimate. In contrast, our work studies how the choice of architecture and hyperparameters affect the model’s
efficiency and quality (we’re agnostic to the specific approach for combining the attention). For example, Scatterbrain uses a
fixed low-rank approximation (i.e. d˜<< d ) in experiments. In contrast, we focus on the recall-memory tradeoff and study
what happens when we increase the size of d. A major takeaway from our study of this tradeoff is that we actually need
dquery> d model to match attention’s recall capacity. Our IO-aware implementation shows how to achieve large speedups
even when dquery> d model.
There are a number of other works which can also be viewed as combinations of sparse and linear attention. Multi-
resolution analysis attention (MRA-2) uses wavelets to approximate the attention matrix (Zeng et al., 2022). A special
form of MRA-2 can be viewed as a combination of sparse and low rank attention for a specific wavelet decomposition.
H-transformer-1D uses a hierarchy of matrices including full dense attention on the diagonal and low-rank approximations
elsewhere (Zhu and Soricut, 2021). TransNormer (Qin et al., 2022b) apply normalizations such as LayerNorm (Ba et al.,
2016) or RMSNorm (Zhang and Sennrich, 2019) to linear attention outputs in certain layers, and apply softmax attention in
local chunks in other layers.
A.1.2. A TTENTION ALTERNATIVES
We now review other attention alternatives, which focus on improving upon the quadratic scaling of attention. Initial work in
this vein uses linear time invariant state space models (SSMs) or long convolutions, which can efficiently process sequences
of length NinO(NlogN)time invoking the FFT-convolution theorem (Cooley and Tukey, 1965), as the sequence mixer
(Gu et al., 2021; Romero et al., 2022; Gupta et al., 2022; Gu et al., 2022; Mehta et al., 2022; Ma et al., 2022; Wang et al.,
2022; Fu et al., 2023b). SSMs can also be rewritten as recurrences to permit fast O(1)inference.
Subsequent work identified that the long convolution alone is not expressive enough to perform particular sub-tasks in
language modeling. Prior work shows pure linear SSMs cannot perform associative recall, a skill that is correlated with a
model’s in-context learning capability (Elhage et al., 2021; Olsson et al., 2022), and introduces multiplicative interactions
(via gating or Hadamard product (Dauphin et al., 2017)) between tokens to allow the model to compare tokens in the
sequence (Fu et al., 2023a; Poli et al., 2023; Peng et al., 2023). However, Arora et al. (2023a) show empirically and
theoretically the class of gated convolution architectures, any architectures built from the two gating and convolution
primitives, struggles to learn associative recall (on synthetic and real language data) as efficiently as attention. They show
that while attention solves AR in constant many layers / with model dimension that is independent of sequence length, any
gated convolution architecture uses dimensionality that scales with the sequence length — we build upon their upper bound
theoretical results with a lower bound argument in Section 3.2. We also study a broader set of architectures in this work
beyond gated convolutions.
Gu and Dao (2023); Arora et al. (2023a); Yang et al. (2023) identify that the use of input-dependent sequence mixers is
important for an architecture to perform AR as efficiently as attention. AR requires shifting information that appears prior in
a sequence to interact with the current (last) tokens in the sequence, in order to predict the next token (Fu et al., 2023a).
While gating is one way to introduce data-dependence (Poli et al., 2023), allowing comparing tokens in two (e.g. a shifted
and unshifted) sequences, it is difficult to select which information from the prefix of the sequence to shift forwards in
the first place, using gating alone. Intuitively, the information to shift depends on the input’s properties . Thus, several
subquadratic architectures consider alternate strategies to introduce input-dependence (Katharopoulos et al., 2020b; Gu and
Dao, 2023; Ren et al., 2023; Ma et al., 2022; Yang et al., 2023). We present another strategy for efficient input-dependent
sequence mixing in our work.
A.2. Efficient Implementations
Beyond designing new model architectures, various works introduce systems-level innovations to improve training and
inference efficiency. These include alternative implementations of architecture primitives such as attention (Dao, 2023; Liu
17

--- PAGE 18 ---
Simple linear attention language models balance the recall-throughput tradeoff
et al., 2023; Kwon et al., 2023), long convolutions (Fu et al., 2023c;b), and linear attention (Katharopoulos et al., 2020a;
Yang et al., 2023). They frequently achieve both reduced memory and increased computational speed on modern GPUs by
“fusing” operations such as matrix multiplications into a single CUDA kernel, and designing “IO-aware” ways to distribute
and compute the results of various read and write operations between different levels of GPU memory.
A.2.1. E FFICIENT ATTENTION IMPLEMENTATIONS
(Dao et al., 2022) introduce FlashAttention, an alternative yet exact implementation of softmax attention that improves
memory and speed by both fusing attention operations into a single CUDA kernel and distributing the attention operations to
better exploit High Bandwidth Memory (HBM) and Static Random Access Memory (SRAM). They first compute attention’s
query-key-value dot-products, masking, and softmax, together as a single kernel. By doing so after a single load to SRAM
before moving the output back to HRAM, they exploit SRAM’s fast compute and reduce the total number of read-write
operations. To get around SRAM’s small memory size and avoid attention’s quadratic memory size over input sequence
length, they use tiling to split up the query, key, and value inputs into smaller “blocks”, compute the attention operations for
each block, and adjust the outputs after computing all blocks to properly normalize the softmax (Rabe and Staats, 2021;
Jang et al., 2019). To perform backpropagation fast on SRAM, they get around SRAM’s limited storage by recomputing the
gradients rather than storing them. Despite the extra operations, this IO-aware implementation still significantly improves
wall-clock time during training.
Similarly making use of block-wise computation, (Liu et al., 2023) instead compute attention blocks across different devices
in RingAttention, enabling training and inference over much larger context lengths that scale with device count. They
distribute and compute the attention operations in each block across multiple hosts in parallel, likewise keeping track of
summary statistics to gather results correctly into exact attention. However, they introduce an “overlapping” mechanism to
coordinate communication of blocks to reduce overhead. They further make use of Blockwise Parallel Transformers (Liu
and Abbeel, 2023) to reduce memory, which similar to FlashAttention removes the quadratic in memory scaling of attention
by dividing the attention operation into separate blocks before gathering back the adjusted softmax output with block-wise
normalization statistics.
As a complement to attention training and inference, (Kwon et al., 2023) improve attention generation with PagedAttention.
PagedAttention similarly uses block-wise computation to address memory utilization issues during generation, where the
KV cache can grow an undetermined amount. Existing systems may na ¨ıvely handle this by pre-allocating large amounts of
contiguous memory. However, this can result in low utilization and computational bottlenecks. Accordingly, PagedAttention
divides attention’s growing KV cache into KV blocks that can be stored separately on physical memory. This enables
more flexible memory management, where smaller chunks can be allocated in different locations when needed to reduce
memory-based bottlenecks.
In Based, we use similar blocking strategies to more efficiently compute both the second-order Taylor series linear attention
and the sliding window softmax attention, and for both training and inference.
A.2.2. E FFICIENT ATTENTION -ALTERNATIVE IMPLEMENTATIONS
Beyond optimizations for attention, various works also introduce similar “IO-aware” implementations to improve memory
usage and speed for convolutional and recurrent operations. We overview the most relevant works to Based, which make use
of similar techniques such as fusing operations and blocking (tiling) to compute results in SRAM.
Long convolutions (Fu et al., 2023c) improve the efficiency of long convolutions on modern GPUs. They build on using
the Fast Fourier Transform (FFT), which enables computing convolutions with filter sizes equal to input sequence length
fromO(N2)(ifNis filter size and sequence length) to O(NlogN). However, to compute this algorithm efficiently on
GPUs, they break down the convolution into separate matrix multiply operations via a Monarch decomposition of the FFT,
which allows both (1) fusing multiple steps of the FFT together (for reduced read-write operations) and (2) scheduling these
operations for fast computation in SRAM while remaining under the smaller SRAM memory constraints.
Recurrence (Gu and Dao, 2023) improve the efficiency of recent neural state-space models (SSMs) (Gu et al., 2021) using
several similar techniques to FlashAttention, specifically with regard the recurrent view. They load the SSM parameters into
SRAM for computation before saving results back in HBM, and also use recomputation where during backpropagation the
intermediate states are not saved but rather recomputed when inputs are loaded from HBM to SRAM. They finally improve
18

--- PAGE 19 ---
Simple linear attention language models balance the recall-throughput tradeoff
wall-clock time by parallelizing the recurrent view of the SSM as a parallel scan.
Linear Attention Finally, several works propose techniques to improve the real-world wall-clock time and memory-usage
of linear attention. (Katharopoulos et al., 2020a) fuse several operations in the causal dot product of linear attention. (Yang
et al., 2023) use blocking to divide the linear attention matrices into SRAM-computable chunks in FlashLinearAttention. As
a trade-off between the slow yet memory-efficient RNN view of linear attention and faster but memory-intensive parallel
“standard attention” view, they further optimize a “chunk-wise” implementation of linear attention (Hua et al., 2022). When
processing input sequences, the input is first divided into several non-overlapping chunks, where we save memory by
computing “kv states” at the end of each chunk, and save time by computing the tokens in a given chunk in parallel.
19

--- PAGE 20 ---
Simple linear attention language models balance the recall-throughput tradeoff
B. IO Aware Implementations
In this section, we provide additional details pertaining to the benchmarking experiments and we provide micro-
benchmarking results for the individual kernels we contribute, to complement the end-to-end benchmarking results in the
Section 6. Each kernel operates over 16×16tiles of data, where dimension 16is motivated by the matrix multiplication
sizes computed by GPU tensor cores.
B.1. Forward / Generation Prefill
Baselines In Figure 4, we implement BASED using our IO-aware Taylor linear attention Algorithm 1. The baseline
approach presented in (Zhang et al., 2024), prior to our kernel, uses the popular linear attention CUDA kernel from Fast
Transformers for computing the causal dot product (Katharopoulos et al., 2020a; Vyas et al., 2020).5. The listing below
shows the baseline implementation for reference (where line 76-77 can be computed using pure PyTorch or the Fast
Transformers kernel) (Zhang et al., 2024).
Micro Benchmark To complement the end-to-end architecture benchmarks in Section 6, we provide micro benchmark
results for only the linear attention forward pass in Figure 5.
0 50 100 150 200 250
Batch size020406080Time (ms)
Pure PyT orch
Fast Transformers Kernel
Based Kernel
0 5000 10000 15000 20000 25000 30000
Sequence Length0255075100125150175Time (ms)
Pure PyT orch
Fast Transformers Kernel
Based Kernel
Figure 5. Time (ms) for different ways of computing the Taylor linear attention forward pass — using Pure PyTorch (shown in the Listing
and introduced in (Zhang et al., 2024)), Fast Transformers kernel (as indicated in the listing) (Vyas et al., 2020; Katharopoulos et al.,
2020b), or our BASED kernel (Algorithm 1). (Left) Varying the batch size at fixed sequence length 1024 .(Right) Varying the sequence
length at fixed batch size 4.(All) Benchmarking uses 16feature dimension, 16heads, 64head dimension, and focuses on the numerator
of the linear attention. Each point represents the median across 10iterations is measured on a single NVIDIA H100 GPU. Lines terminate
on out-of-memory errors.
from einops import rearrange
import torch
from torch import nn
class TaylorExp(nn.Module):
"""
Feature map to compute 2nd-order Taylor approx. of exp(qˆT k / sqrt(d))
"""
def __init__(self, input_dim, head_dim_idx, temp=None, eps=1e-12):
super().__init__()
self.input_dim = input_dim
self.head_dim_idx = head_dim_idx
self.temp = 1.0 if temp is None else temp
5https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/
causal_linear_attention.py
20

--- PAGE 21 ---
Simple linear attention language models balance the recall-throughput tradeoff
self.eps = eps
self.r2 = math.sqrt(2)
self.rd = math.sqrt(self.input_dim)
self.rrd = math.sqrt(self.rd)
def forward(self, x: torch.Tensor):
# Get 2nd-order terms (rearrange(x *x), ’... m n -> ... (m n)’)
x2 = (x.unsqueeze(-1) *x.unsqueeze(-2)).flatten(start_dim=-2) / self.r2
term1 = torch.ones(x[..., :1].shape).to(x.device)
term2 = x / self.rrd
term3 = x2 / self.rd
terms = [term1, term2, term3]
return torch.cat(t for t in terms), dim=self.head_dim_idx)
class TaylorLinAttn(nn.Module):
def __init__(self):
super().__init__()
self.d_model = d_model
self.feature_dim = 16
self.num_heads = 16
self.num_key_value_heads = 16
self.head_dim = self.d_model // self.num_key_value_heads
self.eps = 1e-12
feature_map_kwargs = {
"input_dim": self.feature_dim,
"head_dim_idx": -1,
"eps": 1e-12,
}
self.feature_map = TaylorExp( **feature_map_kwargs)
self.proj_q = nn.Linear(
self.d_model, self.feature_dim *self.num_heads, bias=False
)
self.proj_k = nn.Linear(
self.d_model, self.feature_dim *self.num_heads, bias=False
)
self.proj_v = nn.Linear(
self.d_model, self.num_key_value_heads *self.head_dim, bias=False
)
self.proj_o = nn.Linear(
self.num_heads *self.head_dim, self.d_model, bias=False
)
def forward(self, hidden_states: torch.Tensor, *args, **kwargs):
b, l, _ = hidden_states.size()
q = self.proj_q(hidden_states)
k = self.proj_k(hidden_states)
v = self.proj_v(hidden_states)
q = q.view(b, l, self.num_heads, self.feature_dim).transpose(1, 2)
k = k.view(b, l, self.num_key_value_heads, self.feature_dim).transpose(1, 2)
v = v.view(b, l, self.num_key_value_heads, self.head_dim).transpose(1, 2)
# Linear attention
q, k = self.feature_map(q), self.feature_map(k)
q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)
# Compute attention causal (alternatively use the Fast Transformers kernel)
num = (q *(k*v).cumsum(dim=2)).sum(dim=-1)
denom = (q *k.cumsum(dim=2)).sum(dim=-1) + self.eps
y = (num / denom)
y = rearrange(y, "b h l d -> b l (h d)")
y = self.proj_o(y)
21

--- PAGE 22 ---
Simple linear attention language models balance the recall-throughput tradeoff
return y
Listing 1. PyTorch implementation of Taylor linear attention.
Algorithm Here we revisit the key equations we aim to compute and then describe Algorithm 1 in detail.
Objective First recall from Section 4:
oi=i∑︂
j=1ϕ(qi)⊤ϕ(kj)vj
ϕ(qi)∑︁i
j=1ϕ(kj)=ϕ(qi)∑︁i
j=1(︁
ϕ(kj)⊤vj)︁
ϕ(qi)∑︁i
j=1ϕ(kj)(5)
where qireflects the ithofNtotal tokens in the sequence and every query attends to every past key in O(Nd2)time and
space complexity for embedding dimension d.
To approximate exp(q⊤
ikj/√
d), we use the 2nd-order Taylor series feature map, picking ϕ:Rd→Rd2such that
ϕ(qi)⊤ϕ(kj) = 1 + q⊤
ikj+(q⊤
ikj)2
2(6)
In this section, we will refer to qias atileof data (e.g. of 16tokens) instead of as a single token since the hardware operates
on chunks of data in parallel.
Algorithm description In Algorithm 1, we allow each thread block to compute the result for a particular (batch ,head)
input. Within the thread block, we use 8warps / workers to produce the result. We initialize data structures Bq, Bk, Bv
in SRAM and qa,qb,ka,kb,qfrag,kfrag,vfragin register to hold chunks or tiles of the q, k, v inputs. We initialize data
structures A0, A1, A2in SRAM and a0frag,a1accum ,qA2 accum in register to hold computation for the running KV state
for the 0th,1st,2ndorder Taylor polynomial terms.
We partition the computation along the sequence dimension into nblocks , where in each loop from 1tonblocks , the warps
load the next 8chunks into fast memory. Note that for 2048 sequence length and 8warps, 16tile size, we end up with
ntiles= 128 andnblocks = 16 . In each iteration, each warp loads in 16×16tiles of q, kand16×64tiles of v, where 16
indicates a chunk of 16tokens along the sequence dimension and 16,64are the feature and head dimensions respectively.
Once tiles are streamed in, we do not need to reuse them, which is key to the efficiency of linear attention.
Overall approach Our overall approach is to compute oiby splitting the q,k,vmultiplications as such:
yi= Causal( qT
iki)vi+qii−1∑︂
j=0(kjvj)
where the first term uses the quadratic attention view and requires applying causal masking. Imagining the square attention
matrix, we refer to the first term as computing the interactions on the diagonal . The second term uses the linear view and its
causality has already been handled. We refer to this term as off-diagonal .
Zeroeth order Taylor terms: During the computation, for the 0thterm in the Taylor polynomial, q, kare1after we apply
the feature map (Equation (6)). Therefore, computing a cumulative sum over q(kTv)reduces to maintaining a cumulative
sum of vas we iterate across the sequence.
First order Taylor terms: Next we consider the 1storder terms. On-diagonal: First consider the on-diagonal blocks, e.g.
with respect to tiles qi, ki, vi. For these, we simply multiply qTk, masking (making it causal), and then multiplying with v,
following the order of operations in standard attention (i.e., a quadratic attention view). This makes it easy to apply the
masking ( 0out non-causal elements). Now each warp contains a local result for its set of on-diagonal tiles of qi, ki, vi.
Off-diagonal: However, we need to obtain a global cumulative sum where (qT
ikj)vjdepends on all j∈[1..i](Equation (5)).
Each warp is therefore missing values for tiles j∈[1..i−1]. To incorporate this computation, we will now compute the
cumulative KV hidden state for the warp up until i−1and multiply this with the local tile of q(i.e.qfrag). To accomplish
22

--- PAGE 23 ---
Simple linear attention language models balance the recall-throughput tradeoff
Algorithm 1 Computing the 0th(T0),1st(T1),2nd(T2) Order Taylor Linear Attention Terms
Input: Input projected hidden states q, k, v ∈RN×d.
Output: Output y=T0 +T1 +T2∈RN×d
Parallelize intobatch×heads parallel computations, with nwarps = 8warps per block.
Within a block:
Define tile size T ▷ T= 16 in B ASED
Define ntiles=N
T▷Block along the sequence dimension
Define nblocks = ntiles/nwarps ▷Block along the number of warps
Define tic = 0 ,toc = 1 ▷Flags for asynchronous data loading
Create SRAM buffers Bq,Bk(Size 2×nwarps×T×T) andBv(Size 2×nwarps×T×4T)
Create SRAM buffers A0,A1,A2(Size nwarps×T×4T) for storing interim. results for T0, T1, T2as warps process the sequence
Create SRAM buffers total A0andtotal A1to hold cumulative (“KV”) state corresponding to T0, T1
Create SRAM buffers yof (Size nwarps×T×4T) for storing the final output
Create register fragments qa,qb,ka,kb,qfrag,kfrag,qkaccum of size 16×16. We create register fragments vfrag,a0frag,a1accum ,
A20,A21,qA2 accum ,oaccum of size 16×64. These fragments are for holding data during in-register computation. Initialize the
fragments to 0.
Each warp loads initial tiles Bq[tic][warpid] ←Qt, Bk[tic][warpid] ←KtandBv[tic][warpid] ←Vt ▷HBM into SRAM
forcurblock∈[0..nblocks−1];tic = 0 ⊕= 1,toc⊕1do ▷XORs ticandtocto toggle.
Warp loads Bq[toc][warpid] ←Qtforcurblock+ 1 ▷HBM to SRAM
Warp loads Bk[toc][warpid] ←Ktforcurblock+ 1
Warp loads Bv[toc][warpid] ←Vtforcurblock+ 1
Warp loads qfrag←q[tic][warpid] ▷SRAM into register
Warp loads kfrag←k[tic][warpid]
Warp loads vfrag←v[tic][warpid]
Compute the warp-local cumulative sum on vfrag→a0frag. ▷T0 computation
Add the running A0to the current a0frag
Compute qfragkfragT(attention) and make it causal and store in a qkaccum ▷T1 computation
Compute qkaccum vfrag→oaccum ▷Store causal qkTv
Warps store kT
fragvfrag→a1accum and write a1accum→A1[warpid ] ▷Register to SRAM
Compute cumulative sum over A1in SRAM, updating A1entries
Warps read A1tiles back to registers ▷Each warp now contains its preceeding A1
Warps multiply the values in register with qfragto update →oaccum ▷Add in T1 to the running result
Update a0frag→oaccum ▷Add in T0 to the running result
Square qkaccum , multiply with vfragand add →oaccum ▷Add in diagonal T2 to the running result
Sum the values of oaccum intoy[warpid]
forblock innwarps iterations do ▷Remaining T2 computation; Assumes feature dimension 16
Each of 8warps copies the same slice of q[tic][warpid] to2registers qa,qb
Each thread jin the warp computes qa[:,2j]qafor dimension 2j, and for 2j+ 1(and for qb). Together the threads compute the
256elements resulting from the second order outer product in the feature map.
Each warp stores two slices of A2:A20andA21 ▷Partitioning the large A2across warp registers
Accumulate both qaA20andqbA21→qA2 accum
Warp writes qA2 accum→A2[warpid] ▷Register to SRAM
Sum results across all in A2[warpid] and store the sum in y[block] ▷Add in T2
Each of 8warps copies the same slice of k[tic][block] to2registers ka,kb ▷KV state update
Square kaandkb
Each of the 8warps loads v[tic][block] tovfragin register
Multiply kaandvfrag,kbandvfragand accumulate the results into A20andA21, the two in-register slices of A2for the warp,
respectively
end for
end for
End. Store y. Optionally store A0,A1,A2(comprising the “KV state”) for generation. ▷SRAM to HBM
23

--- PAGE 24 ---
Simple linear attention language models balance the recall-throughput tradeoff
this, in Algorithm 1, we multiply kfragTandvfragto compute local tiles of the hidden state, local to each warp, in thread
register. To perform the global cumulative sum across the 8warps’ local results, we write from registers (thread specific) to
A1in SRAM (shared across warp threads). After computing the global cumulative sum in shared memory, each warp loads
back the KV state (in A1) into its registers such that it contains all the preceeding KV (history) for tiles [1..i−1]. We
then multiply the local qfragin register with this KV state to update the final output for the 1stup until the current nblocks .
Note that we maintain the running KV state corresponding to the 1storder term in A1shared memory for the next iteration
along nblocks .
Second order Taylor terms: We finally need to compute the 2ndorder term. Similar to the 1storder term, we’ll consider
On-diagonal: We can leverage the computation from above. We’ll square the causal (qkT)2from above and multiply with
vfragto obtain the portion of the 2ndorder term corresponding to the on-diagonal tiles qi, ki, vi.Off-diagonal: Again, we
also need to compute the result with respect to tiles [1..i−1].
•Partitioning KV hidden state for 2ndorder Because the hidden state for the second order term is large ( O(d2D)in
feature dimension dand head dimension D) and warps have a limited number of registers, we slice its storage across
the registers of the 8warps. Considering the the 162×64(d2×D) hidden state (stored in A2SRAM in Algorithm 1),
we divide this into 16slices along the sequence dimension and let each of the 8warps handle 2of the 16×64slices
(stored in A20, A21fragments in thread registers in Algorithm 1). Warp iwill maintain slices 2iand2i+ 1in two
registers per thread.
•Computing output for 2ndorder Each warp iloads in one tile of qiinto2registers. We will use the 32threads in
the warp to compute the 256outer product terms for each token computed by the Taylor 2ndorder term (for feature
dimension 16).
Next, the threads multiply these 256terms with the running A20andA21slices. The results for the two slices are
summed in register and then stored in SRAM ( A2[warpid] ). Since oiis ultimately the sum of qiterms multiplied with
allslices of A2(Equation (5)), we then sum the results from all the warps together (which hold the remaining slices of
A2) and store the result in y[block] . We can think of y[block] as holding the result up until the (8×curblock+ block)
tile of tokens (note 8is because in each increment of curblock , the8warps handle 8different tiles of the sequence).
•Updating the KV state: Forblock = i, we load in k[i], v[i]tiles of size 16×16and16×64respectively to registers
ka, kb,vfrag. We compute the 256outer product terms on k[i]using the 32threads, multiply with vfrag, and store the
result in the A20, A21running state.
The final result in yis summed into the output to complete the 2ndorder computation.
24

--- PAGE 25 ---
Simple linear attention language models balance the recall-throughput tradeoff
B.2. Next Token Prediction
During next token prediction in generation, we contribute IO-aware algorithms for the expensive KV-state update in Taylor
linear attention and for the sliding window attention computation.
B.2.1. T AYLOR LINEAR ATTENTION RECURRENT UPDATE
During next token prediction, an important consideration is how to efficiently update the recurrent state KVt∈RBHDdat
timestep t. The expensive operation during next token prediction is computing the outer product between projected hidden
states kt+1∈RBHDandvt+1∈RBHd. The outer product requires O(BHDd )computation and space, and the result
is summed with KVtto produce KVt+1. We provide an IO-aware algorithm for the state updates in Algorithm 2. This
algorithm incurs O(BHD +BHd )bytes of HBM to SRAM data movement (to load the q, k, v projections).
The KV update in PyTorch is provided in the following listing. In Figure 6 we benchmark the speed of the PyTorch
implementation against our kernel.
50 100 150 200 250
Batch Size0.00.20.40.60.8Time (ms)
Pure PyT orch
Based Kernel
Figure 6. Time (ms) for computing the Taylor linear attention recurrent update using Pure PyTorch (shown in the Listing and introduced
in (Zhang et al., 2024)) vs. our BASED kernel (Algorithm 2). Benchmarking uses 16feature dimension, 16heads, 64head dimension, and
focuses on the numerator of the linear attention. Each point represents the median across 10iterations is measured on a single NVIDIA
H100 GPU.
from einops import rearrange
import torch
from torch import nn
def step(self, kv_state: torch.Tensor, k_state: torch.Tensor, q: torch.Tensor, k: torch.
Tensor, v: torch.Tensor):
"""
Compute linear attention with recurrent view
-> Assume q.shape is (b, h, 1, D); k and v.shape are (b, h, l, d), where D is the
dimension after applying the feature map and d is the head dimension.
"""
b, h, l, d = q.shape
assert l == 1, f’q.shape is {q.shape} but should be ({b}, {h}, 1, {d})’
# Expand dims for broadcasting to compute linear attention
q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)
kv_state += k[:, :, -1:] *v[:, :, -1:]
k_state += k[:, :, -1:]
25

--- PAGE 26 ---
Simple linear attention language models balance the recall-throughput tradeoff
# Compute linear attention
num = (q *kv_state).sum(dim=-1)
y = num / ((q *k_state).sum(dim=-1) + self.eps)
y = rearrange(y, ’b h l d -> b l (h d)’).to(q.dtype)
return self.dropout(self.out_proj(y))
Listing 2. PyTorch implementation of Taylor linear attention KV update
26

--- PAGE 27 ---
Simple linear attention language models balance the recall-throughput tradeoff
Algorithm 2 Computing KV State Updates
Input: KVt−1state∈RHd′2d, at time t. Featurized q, k∈RB×H×1×DandV∈RB×H×1×d, fordas the head dimension (e.g. 64)
andDas the expanded feature map dimension (e.g. 273 = 1 + 16 + 162for feature dim 16). To be hardware-friendly, we let D= 320
(s.t.320 mod 64 = 0 ) via padding.
Output: Updated KVtstate.
Parallelize intobatch×heads parallel computations, with nwarps = 8warps per block.
Within a block:
Define nthreads = nwarps×32 ▷Assuming 32threads per warp
Define buffer size= nwarps×8×d
Define total batches =D
nwarps×8▷E.g.total batches = 5ifD= 320 ; Fork,320
5= 64 values per batch
Define tic = 0 ,toc = 1
Create SRAM buffer Bq(Size D) forq
Create SRAM buffer Bk(Size D) fork
Create SRAM buffer Bv(Size d) forV
Create SRAM buffer Bkvs(Size 2×buffer size) for storing blocks of kvstate
Create SRAM buffer o(Size d) for output.
Create SRAM buffer A(Size nwarps×d) for intermediate computation
Create register buffer vreg(Size 2) to store Vdata
Create register Areg(Size 2) for intermediate computation
Warps load Bq←q ▷ HBM to SRAM; Load all D= 320 elements of q
Warps load Bk←k
Warps load Bv←V
Warps load chunk Bkvs[tic]←kvstate ▷Load (1×64)×64of the (total batches ×64)×64elements in KVt−1
Initialize m= 0
forThreads j∈[0..31];j < d ;j+ = 32 , m+ = 1 do ▷Each thread holds 2values ( d= 64 ;32threads)
Load vreg[m]←v[j] ▷SRAM to Register; Now v[j]is stored in thread jmod 32
end for
fori∈[0..total batches ];i=i+ 1,tic⊕1,toc⊕1do
Loads Bkvs[toc]←next batch of kvstate ▷Asynchronous loads of next batch
forj= warpid ;j < d ;j+ = n warps do ▷Each of the 8warps loads 8of the 64rows ofk,qin the batch
kval←Bk[i∗d+j] ▷Grab single rows q[i]andk[i], Broadcast to all threads
qval←Bq[i∗d+j]
p=Bkvs[tic] + j∗d ▷ Point to output rows of KVt; We write d×D
total batchessub-matrix for this batch
Initialize m= 0
forThread k∈[0..31];k < d ;k+ = 32 , m+ = 1 do
p[k]+ = k val∗vreg[m] ▷Update running state by multiplying broadcasted kvalwith the full vreg
▷This updates a 1×dstrip of the d×DfullKVtouter product
Areg[m]+ = q val∗p[k] ▷Multiply qvalwith the running state, updating all values in the 1×doutput
end for
end for
Write out new KVtstate for this batch: Bkvs[tic][k] ▷SRAM to HBM
end for
Initialize m= 0
forThreads j∈[0..31];j < d ;j+ = 32 , m+ = 1 do ▷Each thread holds info for 2of the 64output values
Store A[warpid][ j]←Areg[m] ▷Register to SRAM
end for
forThread j;j < d ;j+ = n threads do ▷ d= 64 threads put values from first warp in nj
nj=A[0][j] ▷Each warp had only computed output values for a subset of (e.g. 8) rows of kandq
forw∈[0..nwarps]do
Sum the nj+ =A[w][j]across ▷Need to combine results across warps
end for
Store o[j]←nj
end for
Write output o ▷SRAM to HBM
27

--- PAGE 28 ---
Simple linear attention language models balance the recall-throughput tradeoff
50 100 150 200 250
Batch size0.020.040.060.080.100.120.140.160.18Time (ms)
Flash Attention (Sliding Window)
Flash Attention (no Sliding Window)
Pure PyT orch
Based Sliding Window Kernel
Figure 7. Time (ms) for different ways of computing sliding window attention next token prediction — using PyTorch, Flash Attention
(which supports a sliding window function), or our inference kernel. Each point represents the median across query tokens at different
token positions in the generation ∈ {100,250,500,750}.
B.2.2. S LIDING WINDOW ATTENTION
Next we motivate the choice of window size for TCWINDOW . In contrast to sliding-window style models such as the
popular Mistral models, which use large window sizes w= 4096 (Jiang et al., 2023), BASED chooses a window size
based on hardware specifications. GPU tensor cores operate on 16×16tiles. Large GEMMs are compute bound (for
e.g. in long-context attention). But, we need sufficient occupancy to hide the latency of the tensor core units. Figure 1
(Right) shows 64×64dimension matrix multiplications are approximately the same latency as 16×16.BASED setsw
to use 64×64tiles (Figure 1). To distinguish from prior sliding windows, we refer to this approach as TCWINDOW . We
use the Flash Attention sliding window implementation during training (Dao, 2023) and in Appendix B Algorithm 3, we
provide an IO-aware algorithm of TCWINDOW for next token prediction. The na ¨ıve sliding window approach reads and
writes O(BHwd )bytes between SRAM and HBM between each step of the attention computation. Our approach fuses
computation in thread registers to improve upon the baselines.
Baselines During training / prefill, we use the Flash Attention sliding window implementation (Dao, 2023).
Our IO-aware implementation focuses on next token prediction. In the listing below, we include a Torch reference. Our
IO-aware sliding window attention algorithm is provided in 3. The key insight is to fuse operations in thread registers to
minimize slower SRAM to register data movement.
Micro Benchmark We benchmark key baselines (Torch, Flash Attention-2 (Dao, 2023), and the BASED kernel on an
NVIDIA H100 GPU in Figure 7. The benchmark uses window size 64, head dimension 64, and number of heads 16. We
vary the batch size on the xaxis and repeat the median timing across iterations on the yaxis. Note that these timings include
only the attention computation and not the time for updating the KV-cache. These timings also do not include any processing
for Rotary encodings (as shown below).
import torch
from torch import nn
"""
b: batch size
h: number of heads
n: sequence length
28

--- PAGE 29 ---
Simple linear attention language models balance the recall-throughput tradeoff
d: head dimension
w: window size
qw: b x h x 1 x d
kw: b x h x w x d
vw: b x h x w x d
"""
w = torch.einsum("bhod, bhnd-> bhn",qw, kw)
a = torch.nn.functional.softmax(w, dim=-1)
result = torch.einsum("bhn,bhnd->bhd", a, vw)
Listing 3. PyTorch implementation of Sliding Window
29

--- PAGE 30 ---
Simple linear attention language models balance the recall-throughput tradeoff
Algorithm 3 Sliding window generation
Input: KVt−1state∈RHwd, at time tand projected hidden states q, k, v ∈RB×H×1×d, forHheads, head dimension d, sliding
window size w, and batch size B.
Output: Updated KVtstate.
Parallelize intobatch×heads parallel computations, with nwarps = 4warps per block.
Within a block:
Define tile size T ▷ T= 16 in B ASED
Define nthreads = nwarps×32 ▷Assuming 32threads per warp
Create SRAM buffers BkandBv(Each of size 4T×4T) to hold k, v. ▷Assumes 4T= 64 is the w,d
Create SRAM vector Bq(Size 1×4T) to hold qduring the kernel execution. ▷Single query, assume d= 64
Create SRAM vector Bw(Size 1×4T) of type float for intermediate attention computation.
Create SRAM vector Bo(Size 1×4T) to hold the output. ▷Single output, assume d= 64
Create SRAM buffers max andsum (Each of workers by float size).
Create register fragments qreg,kreg,vregto hold data during fused computation in-register.
Create register fragments wreg(size1×4T) and wvreg(size4T×1) to store intermediate computation in-register.
Create register fragment oreg(size4T×1) to store output in-register.
Loads Bk←kusing nthreads ;Bv←vusing nthreads ;Bq←qusing one warp. ▷HBM to SRAM
Loads qreg←Bq.qgets broadcasted to all warps. ▷SRAM to Register
Loads kreg←Bk[warpid] . Each warp gets T×4Tof the 4T×4TinBk(i.e.a column).
Loads vreg←Bv[warpid] . Each warp gets T×4Tof the 4T×4TinBv(i.e.a column).
Initialize wregto zero
wreg←qregkreg ▷Matrix-vector (GEMV) multiplication
Initialize float m=−∞ for the max ▷Obtain the max across tiles for Softmax
Update m←max(w reg)with the max from the local data
max[warpid] ←mfor all warps to access
Iterate over nwarps entries in max buffer to compute the global max of wreg
Put global max back into each warp’s mfloat
Initialize float s= 0for the sum ▷Obtain the sum across tiles for Softmax
Update s←sum(w reg)with the sum from the local data
sum[warpid] ←sfor all warps to access
Iterate over nwarps entries in sum buffer to compute the global sum of wreg
Put global sum back into each warp’s sfloat
wreg←wreg−m ▷ Start attention computation in register
wreg←exp(w reg)
wreg←wreg
sBw[warpid] ←wreg ▷Register to SRAM; storing for the slice of k
wvreg←Bw ▷SRAM to Register. Warp loads entirety of Bw; all slices
Initialize oregto zero.
oreg←wvregvreg ▷Matrix-vector (GEMV) multiplication
Write oregto global memory ▷Register to SRAM, SRAM to HBM
30

--- PAGE 31 ---
Simple linear attention language models balance the recall-throughput tradeoff
C. Extended Architecture Details
In this section, we describe two additional architectural details for BASED that can enable small improvements in language
model perplexity. We emphasize, however, that the combination of Taylor linear attention and TCWINDOW layers alone is
sufficient to come within 0.1perplexity points of our best models using these additional components (Table 6).
Convolution. We find that replacing some of the linear attention and TCWINDOW layers with gated convolution layers
enables small improvements in language modeling performance. A gated convolution layer uses a combination of gating
(Hadamard product, elementwise product) and convolution operations. In B ASED , we use BaseConv layers (Arora et al.,
2023a) with short convolutions and a SilU non-linearity (Hendrycks and Gimpel, 2023). By keeping the convolutions short
(e.g.width 3), we keep the recurrent state size for these layers low and improve throughput. The projections expand the
dimensionality by a factor c= 4.
y:= ((u·W1+b1)⏞⏟⏟ ⏞
Linear Projection⊙σ(h∗u·W2+b2)⏞ ⏟⏟ ⏞
Convolution)·W3+b3(7)
where u∈RN×dis a projected input, h∈RN×cdis a learned filter, ⊙is the Hadamard product, and W1,W2∈Rd×cd,
W3∈Rcd×d,b1,b2∈Rcd, andb3,∈Rddefine weights and biases of three linear projections.
Decay. Recent recurrent architectures include the use of decay terms, implemented in a variety of ways (Gu et al., 2021;
Sun et al., 2023; Gu and Dao, 2023; Yang et al., 2023). As intuition, decay terms control how much a token should attend to
“recent” tokens vs. “early” tokens in the sequence. Prior work falls in two categories: using input-independent (Gu et al.,
2021; Sun et al., 2023, inter alia.) or input-dependent (Gu and Dao, 2023; Yang et al., 2023) decay rates. The latter offers
improved quality, but requires the use of a parallel scan during sequence processing (Gu and Dao, 2023).
Instead, we explore a coarser input-dependent decay technique for the linear attention layer, avoiding the parallel scan.
We first use a unique decay rate per head , fixed across all inputs. We introduce a linear projection that takes in the inputs
∈RNxdand projects to RN×h, where Nis the sequence length, dis the model dimension, and His the number of heads.
We use the result of this projection to scale the attention combination across heads.
In our main experiments Table 1, we use no decay when training the models to 50b and 30b tokens. We observe that decay
can help small in our Table 6 ablations, but removing the decay does not affect the overall trends for BASED relative to other
architectures.
31

--- PAGE 32 ---
Simple linear attention language models balance the recall-throughput tradeoff
D. Extended Results
D.1. Extended empirical study of memory-recall tradeoff
In Figure 8, we provide additional experimental results using the setup described in Section 3.1. The results in Figure 8 in-
clude additional efficient architectures beyond those in Figure 3 and Figure 2. Specifically we include NystromFormer (Xiong
et al., 2021), BigBird (Zaheer et al., 2020), and ScatterBrain (Chen et al., 2021a).
104105106
State Size0.20.40.60.81.0Accuracy
model.name
based
sliding-window-attention
scatter-brain
big-bird
nystromformer
Figure 8. Extended Throughput (memory) - recall tradeoff. x-axis shows state size (bytes) during generation; y-axis shows accuracy
on the MQAR recall task (Arora et al., 2023a). For each architecture, we train several models varying hyperparameters that affect the
recurrent state size ( e.g.model dimension). The plot shows a fundamental tradeoff between the recurrent state size and recall capacity that
applies to broad class of models.
D.2. Downstream Language Results
To further evaluate BASED ’s performance in language modeling, we evaluate the PILE-pretrained models on several
downstream tasks that test general natural language understanding.
Architecture Params/Tokens LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande Average
Ppl.↓ Acc.↑ Acc. Norm. ↑ Acc↑ Acc↑ Acc. Norm. ↑ Acc.↑ Acc.↑
Transformer++ (LLaMa) 1.33b/10b 11.12 49.10 39.29 66.16 51.68 26.19 53.43 47.64
BASED 1.35b/10b 12.35 46.96 39.11 66.32 50.72 26.54 50.43 46.68
Mamba 1.32b/10b 13.11 46.13 39.41 66.38 52.36 25.94 50.83 46.84
Transformer++ (LLaMa) 1.33b/50b 7.38 57.50 49.62 70.46 57.58 27.99 56.83 53.33
BASED 1.35b/50b 6.96 57.85 50.79 71.65 58.84 28.75 55.80 53.81
Mamba 1.32b/50b 7.19 57.56 50.94 71.87 59.39 28.41 53.83 53.50
Transformer++ (LLaMa) 360m/10b 18.39 42.52 33.48 63.98 46.04 24.49 53.99 44.08
Transformer (Pythia) 356m/10b 25.17 37.16 31.32 63.76 44.82 23.8 51.54 42.08
BASED 363m/10b 21.80 38.66 33.43 64.42 45.79 24.66 51.22 43.03
Mamba 358m/10b 20.23 39.65 33.63 65.02 47.01 25.00 50.75 43.51
H3 362m/10b 57.59 23.58 30.62 63.11 45.20 23.29 50.28 39.35
Transformer++ (LLaMa) 360m/30b 15.79 44.44 36.90 66.05 48.27 20.56 52.25 44.75
BASED 363m/30b 14.43 45.20 37.41 67.46 49.45 21.42 51.22 45.36
Mamba 358m/30b 14.27 45.06 38.02 66.38 50.55 20.01 51.70 45.62
Table 2. Downstream evaluation of pre-trained language models. The same set of models as in Table 1, all were trained on the same
data drawn from the Pile (Gao et al., 2020), evaluated zero-shot using the default LM-Eval Harness settings from EleutherAI (Gao et al.,
2023). These averages are computed across the 6tasks, excluding LAMBADA perplexity. These averages are included in Table 1.
LM-Eval Harness Standard Tasks We use the same protocol as (Gu and Dao, 2023; Yang et al., 2023), utilizing the LM
evaluation harness by EleutherAI (Gao et al., 2023). In particular, we use the following set of metrics and tasks:
• LAMBADA (perplexity and accuracy) (Paperno et al., 2016)
32

--- PAGE 33 ---
Simple linear attention language models balance the recall-throughput tradeoff
Model Shots BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Avg
Acc.↑ Acc.↑ F1↑ Acc.↑ Acc.↑ F1↑ EM↑ Acc.↑ Acc.↑ Acc.↑
Based
(363m/10b)0 59.0 41.1 19.4 69.0 54.9 14.5 14.0 52.0 50.0 36.5 45.7
1 57.5 37.5 26.8 68.0 52.5 19.9 19.2 47.7 50.9 49.0 47.2
5 56.6 44.6 28.9 73.0 53.6 24.9 24.1 48.7 51.1 39.4 48.0
Transformer++
(360m/10b)0 57.3 41.1 21.3 67.0 57.0 16.6 16.1 53.8 50.0 37.5 46.3
1 54.2 39.3 25.3 69.0 51.5 22.2 21.6 50.9 47.0 55.8 47.8
5 50.7 58.9 49.9 64.0 46.9 24.2 23.6 47.3 52.2 51.9 48.9
Mamba
(358m/10b)0 57.5 35.7 24.4 71.0 57.2 18.8 18.3 52.4 50.0 36.5 46.6
1 51.1 39.3 27.4 71.0 52.9 21.6 21.0 46.6 46.2 52.9 46.9
5 41.1 37.5 23.6 69.0 49.2 20.4 19.9 48.4 51.7 51.9 45.2
Table 3. Few-shot downstream evaluation on SuperGLUE of pre-trained language models. The same set of models as in Table 1, all
were trained on the same 10 billion tokens drawn from the Pile (Gao et al., 2020), evaluated on the SuperGLUE benchmark (Wang et al.,
2019) using the LM eval harness by EleutherAI (Gao et al., 2023). When computing the average, we first average the metrics by task and
then average across tasks.
Model ParamsHG38 PPL ↓
N=1024 N=4096 N=8192
Transformer++ 46.2 2.52 2.50 2.51
Mamba 46.1 2.51 2.49 2.49
Based 48.8 2.51 2.50 2.49
Table 4. DNA modeling performance on the HG38 dataset. All models are pretrained from scratch for 10Bn tokens at N=1k, 4k, and
8k sequence lengths respectively. We report results after hyperparameter sweeping the learning rate for each architecture.
• HellaSwag (normalized accuracy) (Zellers et al., 2019)
• PIQA (accuracy) (Bisk et al., 2019)
• ARC-challenge (normalized accuracy) and, separately, the easy subset ARC-easy (accuracy) (Clark et al., 2018)
• WinoGrande (accuracy) (Sakaguchi et al., 2019)
Normalized accuracy refers to accuracy normalized by sequence length and is used to maintain the equivalent setting to (Gu
and Dao, 2023). We report results in Table 2. For both 360 million and 1.3 billion parameter models, BASED performs
competitively with recent and state-of-the art architectures, including Mamba and Transformer++ (LLaMa).
SuperGLUE Fewshot Results In order to understand in-context-learning performance, we next perform few-shot
evaluations on the SuperGLUE benchmark (Wang et al., 2019) for BASED , Mamba and Transformer++ in Table 3. Each
model was evaluated on all tasks using under 0 shot ( i.e., number of in-context examples), 1 shot and 5 shot prompting,
respectively. Transformer++ and BASED both see monotonic improvement from increasing the number of shots. For Mamba,
however, albeit getting a slight improvement from 0-shot to 1-shot, it performs worse on 5-shot than even on 0-shot. This
result suggests that the limited recall ability observed in Mamba could also impact few-shot abilities.
D.3. DNA Modeling
Towards understanding the capability of BASED beyond natural English language, we next evaluate each architecture on its
ability to model DNA sequences.
Pretraining In Table 4, we evaluate architectures on the HG38 (human genome) benchmark at 1k,4k, and 8ksequence
lengths used in prior architecture evaluations (Nguyen et al., 2023; Gu and Dao, 2023). The DNA tasks uses a byte-level
tokenizer wherein the vocabulary consists of characters corresponding to the nucleotide bases. We find BASED is competitive
with state-of-the-art architectures across evaluated sequence lengths.
Downstream DNA Classification We further evaluate how different architectures compare for DNA modeling. We
take the pretrained models described and evaluate them on DNA sequence classification using a popular benchmark
(GenomicBenchmarks) (Gresova et al., 2022) in Table 5. We find similar performance across tasks, indicating that prior
matching in quality during pretraining transfers to downstream classification. For reference, we also include results from
(Nguyen et al., 2023). Although not directly comparable to due differences in tokenization, the evaluations suggest BASED
33

--- PAGE 34 ---
Simple linear attention language models balance the recall-throughput tradeoff
Dataset Enhancer Cohn Enhancer Ens Human Reg. Non-TATA Promoters Human OCR Ens.
CNN 69.5 68.9 93.3 84.6 68.0
DNABERT 74.0 85.7 88.1 85.6 75.1
GPT 70.5 83.5 91.5 87.7 73.0
HyenaDNA 74.2 89.2 93.8 96.6 80.9
Transformer++ 73.4 89.5 89.9 94.4 79.5
Mamba 73.0 - - 96.6 -
Based 74.6 89.5 89.5 96.8 79.0
Table 5. Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks (Gresova et al., 2022). We report top-1
classification accuracy ( %) with pretrained models (Transformer++, Mamba, BASED ) along with prior reported results in (Nguyen et al.,
2023). We find the similar quality-matching in pretraining transfers to downstream tasks. Modern architectures are also able to achieve
state-of-the-art results on the classification tasks.
can perform strongly on different modalities, and that recent sequence modeling architectures are also able to outperform or
compete with prior state-of-the-art on evaluated DNA tasks.
D.4. B ASED Quality Ablations
Our objective with BASED is to measure the throughput and recall of the simplest possible linear attention model that
achieves strong performance. Therefore, we ablate the key design decisions — choice of feature map, feature dimension for
the Taylor map, use of sliding window and convolutions — to understand their contributions to the quality of BASED . We
ablate using the Pile dataset (Gao et al., 2020) with the same number of tokens and data ordering as the prior experiments.
Infeature map ablations , we consider the CosFormer (Qin et al., 2022a) and Performers (Choromanski et al., 2020) feature
maps, which have been demonstrated as strong choices in prior work (Zhang et al., 2024). We also include a baseline that
expands the state size using learned projections and applies CosFormer towards comparing to the larger state size of the
Taylor map. For these baselines, we keep the rest of the BASED architecture the same ( i.e.in the number of linear attention
layers and hybridization with sliding window and gated convolution layers). We observe that with the larger state size,
CosFormer quality is increasingly competitive with the Taylor map. We note that expanding the state size requires increasing
the model’s overall parameter count (due to the learned projections) for CosFormer, in contrast to the Taylor map.
Next, we ablate the feature dimension , holding the feature map fixed to the Taylor map. We find larger feature dimension
improves quality, with diminishing returns going from 24to32dimension. Note that feature dimension√
1024 = 32 , where
1024 is the attention model dimension at the 360parameter scale in our experiments.
Next, the ablations show that eliminating the convolutions and/or the sliding window attention degrades quality. We
observe that adding either convolutions or sliding window helps on the associative recall slice relative to neither (e.g.2.29
AR Ppl. on the Pile with neither vs.2.09or2.11with sliding window orconvolutions.). Increasing the window size from 0
to64vs.64to128(also an efficient design point in Figure 1, left) continues to help quality, but with marginal improvements.
Finally, we ablate the use of the input-dependent decay strategy introduced in Appendix C. In our main results Table 1, we
useno input-dependent decay whatsoever when training to 30b and 50b tokens for the 360m and 1.3b parameter models
respectively. At 10b tokens, we use the decay strategy and provide ablations without decay in Table 6. We find that the
decay can provide a small boost in quality, but removing the decay does not affect the overall trends.
34

--- PAGE 35 ---
Simple linear attention language models balance the recall-throughput tradeoff
Hyperparameters Language Modeling (Pile) Info. Extraction QA
Feat. Map Feat. Dim. Sliding Convs. DecayAll AR Other SWDE FDA SQUAD
Ppl.↓ Ppl.↓ Ppl.↓ Acc.↑ Acc.↑ Acc.↑
Taylor Exp. (2nd) 16 (153) ✓(64) ✓ ✓ 8.65 2.07 9.64 29.16 11.71 25.07
Performer 16 (16) ✓(64) ✓ ✓ 9.08 8.53 11.62 8.10 0.36 7.47
CosFormer 16 (32) ✓(64) ✓ ✓ 9.03 2.42 9.98 19.35 7.71 24.63
CosFormer 64 (128) ✓(64) ✓ ✓ 8.82 2.18 9.80 25.47 9.07 27.85
Taylor Exp. (2nd) 32 (561) ✓(64) ✓ ✓ 8.56 2.00 9.57 37.62 12.89 26.74
Taylor Exp. (2nd) 24 (325) ✓(64) ✓ ✓ 8.58 2.02 9.58 34.38 20.87 24.77
Taylor Exp. (2nd) 16 (153) ✓(64) ✓ ✓ 8.65 2.07 9.64 29.16 11.71 25.07
Taylor Exp. (2nd) 8 (45) ✓(64) ✓ ✓ 8.77 2.18 9.75 23.40 12.79 22.35
Taylor Exp. (2nd) 16 (153) ✓(64) ✓ ✓ 8.65 2.07 9.64 29.16 11.71 25.07
Taylor Exp. (2nd) 16 (153) ✓(64) ✓ ✗ 8.65 2.04 9.66 22.95 12.34 27.45
Taylor Exp. (2nd) 16 (153) ✗ ✓ ✓ 8.91 2.11 9.94 28.62 10.16 24.5
Taylor Exp. (2nd) 16 (153) ✓(64) ✗ ✓ 8.74 2.09 9.74 24.66 2.36 18.87
Taylor Exp. (2nd) 24 (325) ✗ ✗ ✓ 9.49 2.29 10.58 19.62 8.71 11.33
Taylor Exp. (2nd) 16 (153) ✓(128) ✓ ✓ 8.61 2.06 9.60 32.13 14.39 31.84
Taylor Exp. (2nd) 16 (153) ✓(64) ✓ ✓ 8.65 2.07 9.64 29.16 11.71 25.07
Table 6. Ablations. All models are 362M param variants of the BASED architecture described in Section 4, trained to 10 billion tokens on
the Pile. We ablate the hyperparameters central to the design of BASED : (1) the choice of feature map ϕ(see Section 4.1), (2) the size of
the feature dim d′(we show the effective size of the feature after applying the feature map in parantheses, see Section 4.1), (3) the use of
local sequence mixers (sliding window attention and short convolutions), and (4) the data-dependent decay defined in Section 4.
35

--- PAGE 36 ---
Simple linear attention language models balance the recall-throughput tradeoff
E. Experimental Details
E.1. Language Model Pretraining
We use A100 80GB Nvidia GPUs to run all experiments. We use training infrastructure closely adapted from the
FlashAttention code base: https://github.com/Dao-AILab/flash-attention/tree/main for all pre-
training runs (Dao, 2023). The Pile data is tokenized using the GPT2BPETokenizer and all models see the data in the same
order. Here we provide details on the hyperaparamters and configurations used for training each architecture. We also
provide details on the FLOPs computation.
•BASED We train using the specifications in Table 7. Our implementation is provided here: https://github.com/
HazyResearch/based . The initial models were trained and evaluated using the Fast Transformer CUDA kernels
discussed in Appendix B (Vyas et al., 2020; Katharopoulos et al., 2020b). We use no input-dependent decay whatsoever
when training the models to 30b and 50b tokens at 360m and 1.3b parameters respectively.
•Transformer++ (Touvron et al., 2023) We refer to the modern Llama architecture with Rotary encodings, RMSNorm
and SwiGLU as Transformer++, following prior work (Gu and Dao, 2023; Yang et al., 2023). We train using the
the specifications in Table 8 using the Flash Attention training code provided here: https://github.com/
Dao-AILab/flash-attention/tree/main (Dao, 2023).
•Mamba (Gu and Dao, 2023) We train using the specifications in Table 9, where the parameters are sourced from
the Appendix of (Gu and Dao, 2023). The implementation is sourced from the provided reference at https:
//github.com/state-spaces/mamba .
•Hyena (Poli et al., 2023) We train using the specifications in Table 10, where the parameters are sourced from the
Appendix of (Poli et al., 2023). The implementation is sourced from the provided reference at https://github.
com/HazyResearch/safari .
•H3 (Fu et al., 2023a) We train using the specifications in Table 11. The implementation is sourced from the provided
reference at https://github.com/HazyResearch/safari .
•RWKV (Peng et al., 2023) We train using the specifications in Table 12 and use the reference implementation at
https://github.com/BlinkDL/RWKV-LM . We specifically evaluate RWKV-V5.
•Gated Linear Attention (GLA) We train using the specifications in Table 13. We train following the reference
implementation at https://github.com/berlino/gated_linear_attention .
We give all models the improved Transformer++ recipe (e.g., SwiGLU) as relevant.
E.2. Computing Recurrent State Size
In this section, we provide details on how we compute the size of the recurrent hidden state for the results described in
Section 3.1. We train and evaluate six sequence mixers on a synthetic associative recall task: attention (Vaswani et al., 2017),
sliding window attention (Beltagy et al., 2020), Mamba (Gu and Dao, 2023), H3 (Fu et al., 2023a), Hyena (Poli et al., 2023),
andBASED . For each, we vary hyperparameters that affect the memory consumption during inference. We compare how
MQAR accuracy varies with the size of the recurrent hidden state.
BASED .The recurrent state size in BASED is determined by the model dimension dand the size of the hidden dimension
after applying the feature map d˜. The +1accounts for the K-state required for computing the denominator. For more details
on the recurrent view of B ASED , see 4.
sizeof (si) = (d+ 1)×d˜ (8)
In Based, we use the Taylor Exponential feature map after projecting ddown to a smaller dimension d′. With this approach,
recurrent state size is given by:
sizeof (si) = (d+ 1)×(1 +3d′
2+d′2
2) (9)
In our synthetic experiments, we run B ASED withd∈ {48,64,128}andd′∈ {8,16,24}.
36

--- PAGE 37 ---
Simple linear attention language models balance the recall-throughput tradeoff
Attention. The recurrent state size ( i.e.KV-cache size) in attention depends on two parameters: the model dimension
dand the sequence length N. The 2in the expression below accounts for the separate storage for keys and values in the
KV-cache.
sizeof (si) = 2×d×N (10)
In our synthetic experiments we run attention with d∈ {64,128}. The sequence length Nis determined by the task, not the
model architecture.
Sliding window attention. The recurrent state size in sliding window attention is given by the model dimension dand the
width of the sliding window ksliding . The 2in the expression below accounts for the separate storage for keys and values in
the KV-cache.
sizeof (si) = 2×d×min(N, k sliding) (11)
In our synthetic experiment we run sliding window attention with d∈ { 128}and ksliding ∈
{8,16,32,64,128,256,512,1024}.
Mamba. The recurrent state size in Mamba is determined by the model dimension dand the number of heads h. The 2in
the expression below accounts for the expansion in the Mamba block.
sizeof (si) = 2×d×dstate (12)
In our synthetic experiments, we run Mamba with d∈ {64,128,256}anddstate∈ {8,16,24}.
H3. The recurrent state size in H3 is determined by the model dimension dand the number of heads dstate.
sizeof (si) =d×dstate (13)
In our synthetic experiments, we run H3 with d∈ {64,128,256}anddstate=d
4.
Hyena. The recurrent state size in Hyena is determined by the model dimension dand the number of heads h. The 2in the
expression below accounts for the separate storage for keys and values in the KV-cache.
sizeof (si) =d×N (14)
In our synthetic experiments, we run Hyena with d∈ {64,128,256}.
E.3. Language Model Evaluation
In this section, we provide details on each of the evaluations (columns) reported in Tables 1 and 6.
Pile (Language Modeling). First, we report overall perplexity on the Pile test set (Gao et al., 2020). Then , to understand
how much of the perplexity gap is due to recall capacity, we also evaluate perplexity on two slices ( i.e.subsets) of the test
set:
1.Associative recall(AR) tokens. Tokens in the final position of a bigram which previously occured in context, but ≤1250
times in the training data.
2.Other tokens. All other tokens.
To construct these slices, we exactly follow the protocol in Arora et al. (2023a) and refer the reader to that work for more
details. We compute these slices on the first 16 million tokens in the test set.
37

--- PAGE 38 ---
Simple linear attention language models balance the recall-throughput tradeoff
SWDE (Information Extraction). The task in the SWDE benchmark is to extract semi-structured relations from raw
HTML websites. For example, given an IMBD page for a movie ( e.g. Harry Potter and the Sorcerer’s Stone ) and a relation
key ( e.g.release date), the model must extract the correct relation value ( e.g.2001). The SWDE benchmark was originally
curated by Lockard et al. (2019) for the task of open information extraction from the semi-structured web. Because we are
evaluating the zero-shot capabilities of relatively small language models, we adapt the task to make it slightly easier. Our
task setup is similar after to that used in Arora et al. (2023b).
FDA (Information Extraction). The task is to extract key-value pairs from a set of PDFs scraped from the FDA website.
We use the dataset and labels collected in (Arora et al., 2023b). We break apart the documents into chunks of 1,920 tokens.
For every key-value pair that appears in the chunk, we create a zero-shot prompt using the simple prompt template:
{chunk } \n{key}:
We allow the model to generate a fixed number of tokens after the prompt and check (with case insensitivity) if the value is
contained within the generation. We report accuracy , the fraction of prompts for which the generation contains the value.
Below we include one example of a zero-shot prompt for the key-value pair “ Type of Test: Quantitative, colorometric,
pyranose oxidase (PROD) ”. The actual chunk is substantially longer in the dataset (note the ellipsis).
510(k) SUBSTANTIAL EQUIV ALENCE DETERMINATION DECISION SUMMARY ASSAY ONLY TEMPLATE
A. 510(k) Number: k180209 B. Purpose for Submission: New Device C. Measurand: 1,5-Anhydroglucitol (1,5-AG) D.
Type of Test: Quantitative, colorometric, pyranose oxidase (PROD) E. Applicant: Diazyme Laboratories Inc. F.
Proprietary and Established Names: Diazyme 1,5-AG Assay G. Regulatory Information: 1. Regulation section: 21
CFR 864.7470; Glycosylated hemoglobin assay 2. Classification: Class II ... [1,920 tokens of context from the PDF] ...
Diazyme’s 1,5-AG assay uses the enzyme pyranose oxidase (PROD) to oxidize the 2nd position hydroxyl group of
1,5-AG and to detect the generated hydrogen peroxide by colorimetry using peroxidase (POD). Type of Test:
SQUAD (Question Answering). The Stanford Question Answering Dataset (SQUAD) can be used to evaluate the reading
comprehension of language models. The model is given a passage of text and a question whose answer is contained in the
passage.
Because the models trained in this work are relatively small-scale (up to 1.3 billion parameters trained on 10 billion tokens)
and not instruction fine-tuned, they struggle to answer questions when asked directly. To make the task more amenable to
these raw language models, we first use GPT-4 to reformat the questions to more closely resemble the next-token-prediction
task the models were trained on:
Can you rewrite this question and answer as a statement. Ensure that the answer
is the last part of the statement. \n\n Question: {question } \n\n Answer:
{answer } \n\n Rewrite:
For example, the question and answer “ Question: Which NFL team represented the AFC at Super Bowl 50? Answer: Denver
Broncos ” was rewritten by GPT-4 as “ The NFL team that represented the AFC at Super Bowl 50 was the Denver Broncos. ”
We verify that the rewritten sentence does indeed end with the answer, discarding any sentences where it does not (40% of
questions).
We run the reformatting on 5,000 squad questions from the validation set, yielding a final dataset of 2,984 questions
formatted as next token predictions.
Below we include one example of a zero-shot prompt. The reformatted question is in bold.
For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina
Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams
to have acquired a 15–1 record, while the Denver Broncos became one of four teams to have made eight appearances
in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl
XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being
38

--- PAGE 39 ---
Simple linear attention language models balance the recall-throughput tradeoff
Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior
to Super Bowl 50. The team in Super Bowl 50 that had a 15-1 record was the
39

--- PAGE 40 ---
Simple linear attention language models balance the recall-throughput tradeoff
F. Theoretical Results
F.1. Introduction
Our focus in this section will be on the theoretical results of the paper. Specifically, we will show the equivalence of models
Based andMamba (Gu and Dao, 2023) with BaseConv , a minimal gated-convolution operator (Arora et al., 2023a,
Definition 4.1), and prove lower bounds for the MQAR problem (Arora et al., 2023a, Section H.7.1) in various settings. We
begin by setting notation and introducing the theoretical formulations of the models.
Notation. We will be denoting the all 1row vector of size k, given by[︁1 1 . . . 1 1]︁
, and the all 0row vector of
sizek, given by[︁0 0 . . . 0 0]︁
, as1kand0k, respectively. We will also construe the standard basis vector eias a
column vector in these notes, and adhere to the following matrix indexing convention: M[i, j]is the entry in the ith row and
thejth column, M[i,:]∈F1×ndenotes the ith row, and M[:, j]∈Fm×1denotes the jth column of M∈Fm×n,where F
is a field and the reader can substitute FforRfor convenience. For a matrix M∈Rn×m, we define the pair-wise Hadamard
product of columns of MasM◦M∈Rn×m2, where
(M◦M)[:, i] :=M[:, j]⊙M[:, k]fori∈[m2],
j=⌊︃i−1
m⌋︃
+ 1, k = (i−1) mod m+ 1.(15)
Moreover, we define the element-wise exponentiation of a matrix Masexp[M]where exp[M]ij= exp( Mij). Next, we
denote the Hadamard product of vectors u,v∈Fnasu⊙v; the operation can be extended to matrices accordingly, and for
vectors u,v∈Fn, we denote their linear (or acyclic) convolution asu∗v
Arithmetic Circuit Notation. We briefly introduce the notation of arithmetic circuits (B ¨urgisser et al., 2013). An
arithmetic circuit Cwith variables X≜{x1, x2, . . . , x n}over a field Fis interpreted as a directed acyclic graph, where the
input nodes are labelled by either the variables from Xor constants from Fand the internal nodes are labelled by +or×
with the output being the polynomial computed at the output node.
We shall also refer to the sizeof the circuit as the number of nodes, the depth of the circuit as the length of the longest path
between an input node and the output node, and the width of the circuit as the number of parallel operations in the circuit, or
‘wires’ which will be intersected by a horizontal ‘cut’ through the circuit. Moreover, the degree of a circuit is defined as the
degree of the polynomial computed by the circuit. We summarize this with the following definition:
Definition F.1. An arithmetic circuit Cis an(n, s,∆, w)-circuit ifCis ann-variate arithmetic circuit of size sand of depth
at most ∆, and width w.
F.2. The Models
We now introduce the definitions of the models Based and Mamba for the reader’s convenience. Note that we have redefined
these models to ensure consistency with the notation presented above.
F.2.1. B ASED
The Based model combines two layer types: BaseConv and LinAtt defined below.
Definition F.2 (BaseConv (Arora et al., 2023a)) .Given an input sequence u∈RN×d,where Nis the sequence length and
dis the model dimension, a learned weight matrix WB∈Rd×dand biases BB,BK∈RN×dand a matrix of convolution
filtersK∈RN×d, aBaseConv layer computes the following:
zBaseConv:= (uWB+BB)⊙(K∗u+BK)∈RN×d, (16)
where the convolutions are applied across the input length N.
Definition F.3 (LinearAttention (Katharopoulos et al., 2020b)) .Given an input sequence u∈RN×d,where N
is the sequence length and dis the model dimension, a set of linear projections6Projection q,Projection k∈
Rd×d′,Projection v∈Rd×d, where d′is the feature dimension, the LinearAttention layer computes the following:
zLinearAttention:= (QK⊤)V∈RN×d, (17)
6By linear projections of a matrix u∈Rm×n,we mean uW +Bfor some weight matrix W∈Rn×nand bias B∈Rm×n.
40

--- PAGE 41 ---
Simple linear attention language models balance the recall-throughput tradeoff
where Q:=Projection q(u),K:=Projection k(u),V:=Projection v(u), and we have
Q= [1,Q,Q◦Q]∈RN×(1+d′+d′2),
K= [1,Q,K◦K]∈RN×(1+d′+d′2).
F.2.2. M AMBA
We now introduce the Mamba model from (Gu and Dao, 2023).
Definition F.4 (Mamba (Gu and Dao, 2023)) .Given an input sequence u∈RN×d,where Nis the sequence length and dis
the model dimension, the Mamba layer computes the following:
zMamba:=SSM(A,B,C)(u)∈RN×d, (18)
with the parameters, A∈Rd×d,B∈Rd, defined as
A:= exp(∆ A),
B:= (∆A)−1(exp(∆ A)−I)·∆B,
=A−1(exp(∆ A)−I)·B,(19)
where d, the state dimension, and A∈Rd×dare parameters of the model and do not depend on the input u, along with the
following input-dependent parameters B,C∈RN×d,∆∈RN×ddefined as
B:=LinearN×d(u)∈Rd,
C:=LinearN×d(u)∈Rd,
∆ := Linear N×d(u)∈R(20)
fori∈[N]. It is important to note here that the parameters B,C,∆are causal7and we denote the dependence on upto the
ith row of the input ufori∈[N]by adding a subscript iwhere the dependence for Ai∈Rd×dis inherited from ∆iin
equation 19 and we denote B[i,:] =:Bi,C[i,:] =:Ci.
Finally, the SSM in equation 18 is realized as a linear recurrence. That is, for every (i, j)∈[N]×[d], we have
h[i, j] =Aih[i−1, j] +Biu[i, j]
z[i, j] =C⊤
ih[i, j](21)
where h[i, j]∈Rd,z[i, j]∈Rdenote the latent state and the output of the SSM in Equation (18), respectively.
F.3. Equivalency to BaseConv
For a polynomial with variables Xover a field F, there exists a corresponding arithmetic circuit CoverXthat computes the
output of the polynomial at its terminating node when interpreted as a directed acyclic graph. For any such arithmetic circuit
Cof size sand depth ∆, (Arora et al., 2023a, Theorem 4.2) showed the existence of an equivalent BaseConv operator that
usesO˜(s∆)parameters and O˜(∆)layers. In the sequel, we use this result by expressing the model outputs computed in
equation 17 and equation 18 as polynomials in uandexp(u)to show the equivalency between these disparate models. We
would now like to recall (Arora et al., 2023a, Theorem 4.2). Before doing so, we first establish the following definitions
from (Arora et al., 2023a).
Definition F.5. An(︂
N, L, d, N ˜, d˜)︂
−Gated Convolution Model is a stacked sequence to sequence model with Llayers
such that:
1. input and output are N×dmatrices,
7That is, B[i,:], C[i,:]and∆[i,:]depend only on u[0···i−1].
41

--- PAGE 42 ---
Simple linear attention language models balance the recall-throughput tradeoff
2. each layer’s operations consist of element-wise gating, convolution, linear projection, and
3.all the individual gated convolution layers take in N˜×d˜matrices and output N˜×d˜matrices. We refer to the tuple
(N˜, d˜)as the inner dimension of the model.
We also assume that the input u∈RN×dis embedded into u′∈RN˜×d˜such that
u′[n, t] ={︄
u[n, t]ifn < N, t < d
0otherwise.
The output from the last layer z∈RN˜×d˜is transformed into output y∈RN×dby extracting the top left N×dentries in z.
Theorem F.6 ((Arora et al., 2023a), Theorem 4.2) .For any (nd, s, ∆, w)-arithmetic circuit C, there exists an equivalent(︂
N,∆′, d, N˜, d˜)︂
−BaseConv withN=n,∆′=O(∆ log w),N˜=O(w), d˜=dthat simulates C.
Remark F.7.For notational simplicity, we will use ui,jas the symbol for the variable in the polynomial in urepresenting
the entry u[i, j].
We now present the results showing equivalency between the models in Appendix F.2 and the BaseConv layer in equation 16
using Theorem F.6.
Proposition F.8. Given an input u ∈ RN×d, there exists an equivalent(︁
N, O(log2(Nd)), d, O (N(d+d′2), O(max( d, d′2)))︁
−BaseConv that computes the output of the LinearAttention
layer with feature dimension d′, cf. Equation (17).
Proof. For the matrices Q,K∈RN×d′,V∈RN×dwith the corresponding projection matrices WQ,Wk∈
Rd×d′,WV∈Rd×d, a single BaseConv layer that computes each of these matrices by simply taking identical pro-
jection and hs,hl,Bs≡0andBℓ≡ 1N×d, the all 1matrix. Using the remembering primitive (Arora et al., 2023a,
Proposition H.10), we can compute each of these in turn while remembering others using O(1)layers and Ndparameters.
Next, we derive an expression for each entry (i, j)∈[N]×[d′2]ofQ◦Q,K◦K∈RN×d′2. From equation 15, observe
that each entry of M◦Mcan be written as the product of entries from M. Hence we have
(Q◦Q)[i, j]≡Q[i, k]·Q[i, ℓ]
(K◦K)[i, j]≡K[i, k]·K[i, ℓ](22)
fork=⌊︁j−1
d′⌋︁
+ 1, ℓ= (j−1) mod d′+ 1.Note, however, that we can simulate the above by first increasing the inner
dimension and copying over columns of Qto getQ1,Q2∈RN×ddefined as Q1[i, j] :=Q[i, k]andQ2[i, j] :=Q[i, ℓ]for
k=⌊︁j−1
d′⌋︁
+ 1, ℓ= (j−1) mod d′+ 1so that (Q◦Q) =Q1⊙Q2, which, mutatis mutandis , also applies to (K◦K)
We can achieve the copying of the columns by simply using the projection matrix WBand another permutation matrix P.
Apart from the multiplication by P, we only need to use O(1)layers, and moreover, since the circuit that computes Pu
simply rearranges the input, there exists a single BaseConv layer that computes Pu(Arora et al., 2023a, Corollary H.20).
By the stacking lemma (Arora et al., 2023a, Lemma H.11), we can stack these layers to get a composition of the outputs so
far to get a(︁
N, O(1), d, O (N(d+d′2), O(max( d, d′2)))︁
−BaseConv model. Moreover, the concatenated matrices Q,K
∈RN×(1+d′+d′2)then take the addition of the computed components so far which again takes O(1)layers of BaseConv .
Finally, we can express each entry (i, j)∈[N]×[d]of the output of LinearAttention as a polynomial as follows:
zi,j(u)≡∑︂
m∈[1+d′+d′2],n∈[N]Q[i, m]·K[n, m]·V[n, j]. (23)
Thus, we can derive the arithmetic circuit that computes zi,j(u)by taking in the outputs of the BaseConv layers so far as
input and compute each of the terms inside the sum by multiplying the outputs from all three and compute the sum using
additional log⌈Nd⌉depth. Each term inside the sum requires two multiplication gates with depth 2, each of which serve
as inputs to the circuit with size Ndcomputing the sum. Moreover, there are N·dsuch output gates each of which is
computed in parallel resulting in a circuit of size O(N·d), depth O(log(Nd))and width O(Nd). O Overall, applying
Theorem F.6 then results in an equivalent(︁
N, O(log2(Nd)), d, O (N(d+d′2), O(max( d, d′2)))︁
−BaseConv model that
computes z.
42

--- PAGE 43 ---
Simple linear attention language models balance the recall-throughput tradeoff
F.4. The Lower Bounds
In the sequel, we consider the multiple-query associative recall problem ( MQAR ) as defined in (Arora et al., 2023a, Section
H.7.1). We briefly recall the definition here.
Suppose we are given an input sequence u[0···3N−1]≜{(k0,v0,q0), . . . , (kN−1,vN−1,qN−1)}with
eachki,vi,qi∈Cis a token drawn from a vocabulary of size c=|C|. Our goal is then to check, for each
1≤i≤N−1, whether there exists 0≤j < i such that qi≡kj, and if so, output vj.
F.4.1. T HESPACE COMPLEXITY OF AR
We will start by providing a lower bound on the space complexity of solving the standard associative recall (AR) problem.
As AR is a subclass of MQAR, this naturally provides a lower bound on the space complexity of MQAR as well. Here, we
formally recall the associative recall problem.
The AR problem takes key-value pairs {ki,vi}n−1
i=0along with a query qappended at the end as input and the
goal is to output viifq=kifor some i∈[0, N−1].
We now require a randomized communication complexity lower bound result for the index problem :
The index problem has two agents, Alice and Bob, where Alice has a string x∈ {0,1}nand Bob has an index
i∈[n], and the goal for the players is to output the i-th entry xi. Moreover, we also require the communication to
beone-way : only Alice is allowed to send a single message to Bob and Bob needs to output the answer.
We will make use of the following lower-bound result.
Theorem F.9 ((Jayram et al., 2008)) .The one-way randomized communication complexity8of the index problem for sending
ann-length bit string is Ω(n).
F.4.2. L OWER BOUND FOR RECURRENT MODELS
We now use Theorem F.9 to first provide a lower bound on the number of bits required by the following class of models to
solve AR.
Definition F.10 (Recurrent Models) .A model Mtaking an input u∈RN×d, where Nis the input length and dis the
model dimension, is termed a recurrent model if its i-th state, representing the output at location i,Zi
M∈Rd˜, with d˜
denoting the state size, is determined exclusively by the preceding elements of the input u[0. . . i−1]. The state Zi
M
represents the accumulated information of the model depending on the inputs up to the i-th element, and is distinct from
learned parameters that are static with respect to the input sequence.
Specifically, Zi
M(u) =ϕ(u[0. . . i−1]), indicating that the state is a function of the input history but not of the entire input
sequence simultaneously. Moreover, we can express this as:
Zi
M(u) =fi
M(Zi−1
M,u[i]), (24)
for a sequence of functions {fi
M}i∈[N], where each function is tailored to evolve the state based on the immediate past state
and the current input.
Remark F.11.Note that Definition F.10 excludes models that inherently require the entire input sequence for computation at
any state, such as those based on non-causal convolutional operations over the full input.
Theorem F.12. Any recurrent model M(Definition F .10) that solves AR requires max i|Zi
M|to be at least Ω(N)-bits.
Proof. Consider an instance (x, i)of the index problem with x∈ {0,1}N. We now describe the corresponding instance of
the AR problem:
{j,xj}N−1
j=0, i. (25)
8The randomized communication complexity of function fis defined as minπ∥π∥, where πranges over all randomized protocols that
can solve fwith probability of success at least 2/3.
43

--- PAGE 44 ---
Simple linear attention language models balance the recall-throughput tradeoff
Next, consider the following one-way protocol for solving the index problem using the regressive model M. Alice with
their access of x∈ {0,1}Ngenerate an input for AR (without the query) as in equation 25. Alice then runs the model M
on{i,xj}N−1
j=0and sends the memory content of running the model Mto Bob. This should include the state ZN−1
M of size
d˜as we can reasonably assume that both have access to the set of functions {fj
M}j∈[N]. Since we assume that this model
solves AR, the output Out[N,:] =xishould contain the associated value of i. Here, Bob can compute Out[N,:]by using
the memory content sent by Alice and applying the function fNas follows.
xi=Out[N,:] =fN(ZN−1,u[N]).
That is, the total number of bits that are communicated in this protocol is |ZN−1
M|. Now, if max j|Zj
M|iso(N)bits, we have
shown that a one-way communication protocol exists for solving the index problem exists that uses o(N)communication
complexity. This contradicts Theorem F.9 and hence, we conclude that the model Msolving AR also needs Ω(N)bits.
Corollary F.13. Given an input u∈RN×dto the AR problem, a causal Mamba model with all entries in its computation
taking O(1)bits needs d+d≥Ω(N)to solve AR.
Proof. We will first show that causal Mamba is a recurrent model. To see this, first observe equation 21 and note the fact
that the input-dependent parameters A,B,C,∆are causal as mentioned in Definition F.4.
Next, due to equation 21, in order to compute zN,:∈Rd, we need CN∈Rd,BN∈Rdand∆N∈Rdalong with
h[N−1,:]∈Rd. Here, we have the (N−1)-st state ZN−1
Mamba∈R3d+dgiven by
ZN−1
Mamba :={h[i−1,:],∆1
N,B1
N,C1
N},
where ∆1
N,B1
N,C1
Nare all linear functions of u[0···N−1]that we receive from the (N−1)-st state and we compute
∆2
N,B2
N,C2
Nas linear functions of u[N]so that we have ∆N= ∆1
N+ ∆1
N,BN=B1
N+B2
N,CN=C1
N+C2
N. We
can then define the function fNas follows:
ZN
Mamba[j] = exp(∆ N[j]A)h[N−1, j] +BNu[N, j]
=ANh[N−1, j] +BNu[N, j],
Out[N, j] =fN(ZN−1
Mamba)[j] =C⊤
NZN
Mamba[j].
Thus, due to Theorem F.12, we can conclude that |ZN−1
Mamba|does require Ω(N)-bits to solve AR. Finally, assuming each
entry of ZN−1
Mamba needs O(1)bits to represent, the overall state ZN−1
Mamba needs O(d+d)to represent, which completes the
proof of the claim.
F.4.3. L OWER BOUND ON THE NUMBER OF LAYERS FOR AR
Next, we will again use Theorem F.9 to provide a better bound on the number of layers required to solve AR. (Note that
since AR is a special case of MQAR , the result below immediately implies Theorem 3.4.)
Theorem F.14. Given an input u∈ {0,1}N×dto the AR problem with any encoding such that logc≤d≤2(logN)1−ϵfor
ϵ >0, andcpossible tokens from the vocabulary with c≤N, a data-independent BaseConv model with model parameters
taking O(logN)bits needs Ω(ϵlog log N)layers to solve AR.
Proof. For a BaseConv model that solves AR using Llayers, by definition, there exists a polynomial P(u)of degree at
most 2Lthat solves AR for any u∈ {0,1}N×d9. This is because for the output of the ith layer of BaseConv , given by
Zi
BaseConv , we have
Zi
BaseConv (Yi−1
M)≡Pi(Zi−1
BaseConv ),deg(Pi)≤2,
for some polynomial Piof degree 2which simply takes the inner products allowing the model to solve AR, where
Z0
BaseConv :=u. Further, for such a model with Llayers, by composition, the output of the i-th layer for i∈[L]is
also a polynomial over the input uand has degree at most 2i. At the end, we have a polynomial P(u)of degree ≤2L
9Since BaseConv is data independent, note that the polynomial P(·)is defined once we fix Nandd.
44

--- PAGE 45 ---
Simple linear attention language models balance the recall-throughput tradeoff
foru∈ {0,1}N×d.As in the proof of Theorem F.12, again take the instance instance (x, i)of the index problem with
x∈ {0,1}Nand the corresponding instance of the AR problem as before
u:={j,xj}N−1
j=0, i. (26)
Next, we build the following one-way protocol for solving the index problem using the BaseConv model from the
hypothesis that it solves AR. Alice with their access of x∈ {0,1}Nwill again generate an input ufor AR (without the
query) as in equation 26.
Alice first takes the values a:=u[0 :N−2,:]∈ {0,1}(N−1)×dand substitutes these known (N−1)dvalues to define the
following polynomial:
Q(uN−1,0, . . . ,uN−1,d−1) =P(a,uN−1,0, . . . ,uN−1,d−1). (27)
Here, note that Qis a polynomial in dvariables that correspond to the values u[N−1,:]that Bob has and trivially has
degree D≤2L. Now, Alice can run the model M, retrieve the coefficients of Q, and send it to Bob. Since we assume that
Psolves AR, Bob can take the coefficients of Qand substitute u[N−1,:]toQto compute P(u)which is the associated
value of i.
Here, the polynomial Qthat Alice sends has at most d2Lcoefficients as each term in Qcan have degree at most 2L.
If each such coefficient has Bbits, then using Theorem F.9, the total number of bits being communicated must satisfy
B·d2L≥Ω(N). This follows from the fact that if B·d2L≤o(N), then since the associated value of iin equation 26 is
the answer to the indexing problem, we have shown that a one-way communication protocol for solving the index problem
useso(N)communication complexity, which then contradicts Theorem F.9. Thus, we must have
B·d2L≥Ω(N) =⇒2Llog(d)≥log(︃N
B)︃
−O(1).
Taking logarithm of both sides then yields
L≥log(︄
log(︁N
B)︁
log (d))︄
−O(1)≥log(︃logN−logB
log (d))︃
−O(1)
≥log(︃logN−logB
(logN)1−ϵ)︃
, (28)
where we use the fact that d≤2(logN)1−ϵfor any ϵ >0in equation 28.
Moreover, as the model parameters are assumed to be O(logN)bits, any coefficient in Qshould have absolute value at
most(︁
2O(logN)·Nd)︁2L
as each coefficient can be a product of at most Ndvariables. That is, for some α >0, we have the
following bound on each coefficient:
2B≤(Nα+1d)2L≤(N(α+2))2L
where the last equality uses the fact that d≤N. We thus have
log(B)≤log(α+ 2) + L+ log log N. (29)
Substituting equation 29 to equation 28, we get
L≥log(︃logN−log(α+ 2)−L−log log N
(logN)1−ϵ)︃
(30)
Now, if L >log log N, we are done. Otherwise, if L≤log log N, then we can substitute this to equation 30 to get
L≥log(︃logN−log(α+ 2)−2 log log N
(logN)1−ϵ)︃
= log (log N−log(α+ 2)−2 log log N)−(1−ϵ) log log N (31)
We now claim that first term in equation 31 satisfies the following:
log (log N−log(α+ 2)−2 log log N)≥(1−ϵ
2) log log N. (32)
45

--- PAGE 46 ---
Simple linear attention language models balance the recall-throughput tradeoff
To see this, note that, for sufficiently large enough N, the following holds:
logN
2≥log(α+ 2) + 2 log log N,
hence, we get
log (log N−log(α+ 2)−2 log log N)≥log(︃logN
2)︃
≥log log N−1≥(1−ϵ
2) log log N.
This proves the claim in equation 32. Finally, using equation 32, equation 31 leads to the following:
L≥(1−ϵ
2) log log N−(1−ϵ) log log N=ϵ
2log log N,
which still provides the lower bound L= Ω(ϵlog log N), as desired.
Remark F.15.We remark that it is possible to extend Theorem F.14 to any model whose output from each layer is a
polynomial of some degree ∆≥2ot get a lower bound of Ω(ϵlog log N/log ∆) .
F.4.4. L OWER BOUND ON THE NUMBER OF LAYERS FOR MQAR WITH d= log2c
Setup. We take d= log2cto encode all cpossible tokens from C. That is, all the 2dpossible d-bit vectors can appear as
a token in the input for MQAR . We will show that data-independent BaseConv needs Ω(log d)=Ω(log log c)-layers to
solve this setting of MQAR , while Attention (+ReLU) can solve this in O(1)layers.
We first provide the trivial solution using Attention (+ReLU).
Proposition F.16. Attention (with linear biases and ReLU) followed by two layers of MLPs can solve MQAR for an input
sequence u∈ {0,1}3N×dsuch that d= log2(c)inO(1)layers.
Proof. Given a row u[i,:]∈ {0,1}d, we express each row as w[i,:]∈ {− 1,1}dby applying the projection uW +B,
where W:= diag(2 , . . . , 2)∈Rd×dand the bias matrix Bis the matrix of all −1’s so that w[i, j] = 2u[i, j]−1. Then,
we can specify the query and key projection matrices Q,K,V∈R3N×das follows:
K[i,:]≡{︄
w[i,:] =k⌊i/3⌋ifi≡0 mod 3
0 otherwise
Q[i,:]≡{︄
w[i,:] =q⌊i/3⌋ifi≡2 mod 3
0 otherwise
V[i,:]≡{︄
w[i+ 1,:] =v⌊i/3⌋ifi≡0 mod 3
0 otherwise,
where the values are shifted to the corresponding key index. Computing the pair-wise inner products then yields
QK⊤[i, j]≡{︄
⟨q⌊i/3⌋,k⌊j/3⌋⟩ifi≡2 mod 3 andj≡0 mod 3
0 otherwise
However, since both q⌊i/3⌋,k⌊j/3⌋∈ {− 1,1}d, we have ⟨q⌊i/3⌋,k⌊j/3⌋⟩ ≤dwith equality iff q⌊i/3⌋≡k⌊j/3⌋. We then
subtract off d−1from each of the 3N×3Nentries by taking the bias B∈R3N×3Nas the matrix with each entry −d+ 1.
LetZ:=RELU(QK⊤+B)so that we have
Z[i, j] = 1{q⌊i/3⌋≡k⌊j/3⌋}.
Next, as we may have multiple matches and we only need to return 1, we modify Zby multiplying with the matrices
W1,W2∈Rd×dand adding the bias B∈Rd×ddefined as follows:
W1[k, j] :={︄
1ifk≥j
0otherwise,W2[ℓ, k] :=⎧
⎪⎨
⎪⎩−1ifk= 0
1 ifk=ℓ, ℓ̸= 0
0 otherwise,B[i, j] = 1.
46

--- PAGE 47 ---
Simple linear attention language models balance the recall-throughput tradeoff
ForZ1:=ZW 1andZ2:=ZW 1W2, we have:
Z1[i, j] =∑︂
kZ[i, k]W1[k, j] =∑︂
k≥jZ[i, k],
Z2[i, j] =∑︂
kZ1[i, k]W2[k, j] =Z1[i, j]−Z1[i,0].
That is, each entry in Z1sums the entries in the row that are at the same or higher column index while each column in Z2
subtracts the first entry—the sum of all entries in the row—from each entry in the row. Semantically, for each row in Z1, the
entries from 0to the index of the first match must have the same value, and thus, are the only non-negative entries in Z2.
Next, we add the bias and activate under R ELU to get Z′∈R3N×d:
Z′[i, k] := RELU(Z2+B)[i, k] ={︄
1ifk≤min{j|q⌊i/3⌋≡k⌊j/3⌋}
0otherwise.
Now, we multiply by the weight matrix W3∈R3N×ddefined as
W3[k, j] :=⎧
⎪⎨
⎪⎩−1ifk=j+ 1
1 ifk=j
0 otherwise
This yields the retriever Z=Z′W3∈R3N×dgiven by
Z[i, k] :=∑︂
ℓZ′[i, ℓ]W3[ℓ, k] =Z′[i, k]−Z′[i, k+ 1] = 1{k= min {j|q⌊i/3⌋≡k⌊j/3⌋}}.
Finally, we multiply with the values Vto get
(ZV)[i,:]≡Z[i,:]V≡Z[i, j∗]·V[j∗,:]≡{︄
vj∗ifq⌊i/3⌋≡k⌊j∗/3⌋, j∗= min {j|q⌊i/3⌋≡k⌊j/3⌋}.
0 if no such j∗exists.
That is, the row corresponding to the query returns the value associated to the first matching key. Thus, the model with
Attention (computing Z) followed by two MLPs computing Z′andZ, respectively, solves the MQAR problem.
Next, we relate the output of Llayers of BaseConv to the degree of the polynomial that it computes.
Lemma F.17. For any input sequence u, there exists a multilinear polynomial equivalent (over Boolean inputs) to the
polynomial computed by Llayers of BaseConv with degree at most 2L.
Proof. LetP(u)be the polynomial computed by Llayers of BaseConv . Since the output of a single layer of BaseConv
is equivalent to a polynomial over the input variables with degree at most 2, composing Lsuch layers yields a polynomial of
degree at most 2L. However, P(u)need not be multi linear, but the polynomial defined as
Q(u) := (···((P(u) mod ( u2
1−u1)) mod ( u2
2−u2))···) mod ( u2
3Nd−u3Nd)
is equivalent to P(u)as(u2
i−ui)evaluates to 0for each input var ui∈ {0,1}. However, deg(Q(u))≤deg(P(u)), and
thus, the claim holds.
We now relate the MQAR (in the above setting) to the degree of the polynomial that it computes.
Lemma F.18. The MQAR problem with d= log2(c)is represented by a multi-linear polynomial of degree 2d+ 1.
Proof. We will start by specifying the obvious Boolean circuit that solves MQAR. First, we take the XNOR of keys and
queries bitwise as follows.
xij=qixnorkj:= (qi∧kj)∨(¬qi∧ ¬kj)fori > j, (33)
47

--- PAGE 48 ---
Simple linear attention language models balance the recall-throughput tradeoff
where, for x,y∈ {0,1}d, we have
[xxnory][k] :={︄
1ifx[k] =y[k]
0othwerise
That is, each bit from xijis set to 1iff the corresponding bits from qiandkjmatch. Next, we take the AND of the d-bits to
get
yij:=⋀︂
k∈[d]xij
k, i > j. (34)
Thus,yijis set to 1iff the query qimatches with the key kj. Finally, we AND with each bit of the values to get the output
zijwith the kth bit for k∈[d]given by
zij
k:=yij∧[vj]k. (35)
Thus, the output of the circuit can be represented as
zij={︄
viifqi≡kj, i > j
0 otherwise.
We can now directly translate the above circuit into a multi-linear polynomial. With slight abuse of notation, we have
the following correspondence for equation 34, where ui≡qi,uj≡kj, i > j and we use uijto represent the variable
corresponding to the entry u[i, j].
xij
k(u) :=uikujk+ (1−uik)(1−ujk)for each k∈[d], i > j.
Next, we translate equation 34 as follows.
yij(u) :=∏︂
k∈[d](uikujk+ (1−uik)(1−ujk)).
Finally, we can write the polynomial that computes MQAR as follows.
zij(u) :=⎛
⎝∏︂
k∈[d]uikujk+ (1−uik)(1−ujk)⎞
⎠u(i+1)kfor each k∈[d], i > j, (36)
where u[i+ 1,:]≡vj. It is then easy to observe that equation 36 is multi-linear and has degree 2d+ 1.
We are now ready to provide the lower bound.
Theorem F.19. A data-independent BaseConv model needs log(2d)-layers to solve MQAR for an input sequence
u∈ {0,1}3N×dwithd= log2(c).
Proof. Due to Lemma F.18, we know there exists a multi-linear polynomial that solves MQAR, and due to (Kopparty, 2020,
Lecture 3, Proposition 4), it is unique. Specifically we cannot solve MQAR with a multi-linear polynomial of degree ≤2d.
Now, assume that there is a BaseConv model with Llayers that exactly solves MQAR. Then, due to Lemma F.17, this
yields a multilinear polynomial P(u)of degree at most 2L. Here, if L≤log(2d), then the resulting BaseConv withL
layers results in a multilinear polynomial of degree ≤2d. This contradicts the above claim that we cannot have a multi
linear polynomial of degree <2d+ 1that exactly represents MQAR. Consequently, a data-independent BaseConv model
needs≥log(2d)-layers to solve MQAR .
F.5. Lower Bound on the Number of Layers for d≥log2cwith Specific Encodings
F.5.1. T HEEQUALITY PROBLEM
For an input pair u1,u2where each uiis a token drawn from a vocabulary of size c=|C|and embedded in {0,1}d, we
define the equality problem (EQ) as checking whether the two encodings are equal: u1≡u2.
We first note that any model that solves MQAR also solves EQ via the following proposition.
48

--- PAGE 49 ---
Simple linear attention language models balance the recall-throughput tradeoff
Proposition F.20. Any model MMQAR that solves MQAR also solves EQ using the same number of layers.
Proof. If there exists a model MMQAR that solves MQAR using Llayers, then for an arbitrary input instance for EQ given
byu1,u2∈R2×d, we can produce the following input instance for MQAR: u:={(u1, 1,u1),(u2, 1,u2)}and solve EQ
using Llayers with M MQAR returning 1iff there is a match.
Due to Proposition F.20, we obtain the following corollary.
Corollary F.21. Any lower bound Lon the number of layers LofBaseConv to solving EQ is also a lower bound on the
number of layers required for solving MQAR .
We now try to prove a lower bound for the case of d≥log2c. First, note that there are embeddings here where the lower
bound from F.19 holds: consider the embedding where the first log2chas the compact binary embedding as before but
the last d−log2cbits are the same for all the tokens. We will instead prove a lower bound for a more interesting set of
embeddings.
F.5.2. T HEp-HOTENCODING FOR p≥1
Definition F.22 ((Almost) p-Hot Encoding) .We define the p-hot encoding to be the collection of embeddings for a token xt
with0≤t < c such that we express tin basep√c: (t0, .., t p−1)∈[0,p√c)pand represent each tias one hot encoding in
{0,1}p√c. That is, we take d=p·p√c.
Moreover, we define the almost p-hot encoding to be the collection of embeddings where each tiis mapped in {0,1}p√c−1
obtained by dropping the last bit of its one-hot encoding in {0,1}p√c.
Note that both of the encodings have p-many blocks derived from each of the one-hot encodings.
Definition F.23 (Block-Exclusive) .We say that a polynomial Pwith variables in u:= (u0, . . . ,up−1)isblock-exclusive if
each non-zero monomial in Pgiven by the product
∏︂
i∈[p], j∈[p√c]ui,j
does not contain any product of the form ui,jui,j′fori∈[p], j, j′∈[p√c].
Remark F.24.The condition specified in Definition F.23 ensures that a block-exclusive polynomial is necessarily multilinear,
as it disallows the term ui,jui,j′forj=j′in any non-zero monomial.
Lemma F.25. For any Boolean function f:{0,1} → { 0,1}with inputs from the almost p-hot encoding orthep-hot
encoding setting, there exists a block-exclusive polynomial equivalent to f.
Proof. Given an input utoffrom the almost p-hot encoding or the p-hot encoding such that u:= (u0, . . . ,up−1), we first
observe that the polynomial P(u)representing f(u)cannot have a non-zero monomial with variables from the same block.
Specifically, for 0≤j < p , any non-zero monomial in Pcannot have a product of the form uj,kuj,k′fork̸=k′. To see
this, assume that there exists a non-zero monomial in Pwith at least two terms uj,kuj,k′from the same jth block in u, then
monomial always evaluates to 0as the jth block is derived from the one-hot encoding in {0,1}p√cor the almost one-hot
encoding in {0,1}p√c−1, and hence, cannot have more than one bit set to 1.
Next, if a non-zero monomial in Pdoes contain a product of the form uj,kuj,k′fork, k′∈[p√c], we can define the
polynomial
Q(u) := (···((P(u) mod ( u2
0,0−u0,0)) mod ( u2
0,1−u0,1))···) mod ( u2
p−1,p√c−1−up−1,p√c−1).
Since each entry is Boolean, Qis equivalent to Pover Boolean inputs, and thus, Qis the block-exclusive polynomial
equivalent to f.
Proposition F.26. Any Boolean function f:{0,1} → { 0,1}with inputs from the almost p-hot encoding setting has a
unique representation as a block-exclusive polynomial.
49

--- PAGE 50 ---
Simple linear attention language models balance the recall-throughput tradeoff
Proof. Due to (Kopparty, 2020, Proposition 4), we know that every Boolean function fis represented by a multilinear
polynomial. Moreover, from Lemma F.25, we know that the polynomial P(u)representing f(u)is block-exclusive for u
with the almost p-hot encoding.
To show uniqueness, we replicate the argument from (Kopparty, 2020, Lecture 3, Proposition 4): Given two block-exclusive
polynomials PandP′equivalent to fwith inputs from the almost p-hot encoding, we have (P−P′)(u)≡0. Now, assume,
for the sake of contradiction, that P−P′̸≡0. Here, note that as P−P′is not identically zero and we have a non-zero
monomial, and since the inputs are from the almost p-hot encoding, we know that this monomial cannot contain any product
of the form uj,kuj,k′. LetS⊆[p]×[p√c−1]be a minimal set of indices such that the monomial∏︁
(j,k)∈Suj,kappears in
P−P′with non-zero coefficient. Note that χSforms a valid input to fas each block in Scan be assigned at most one
non-zero entry. Then, since (P−P′)(χS)̸= 0as every other monomial will get at least one variable that is assigned to 0for
χS, we achieve a contradiction, and thus, P−P′must be identically zero on inputs from the almost p-hot encoding.
Lemma F.27. The EQ problem in the almost p-hot encoding setting is represented by a block-exclusive polynomial of
degree 2p.
Proof. Each input pair u1,u2to the EQ problem can be represented as ui:= (ui
0, . . . ,ui
p−1)fori∈ {1,2}, where for
each0< j < p such that we have
ui
j:= (ui
j,0, . . . ,ui
j,p√c−2)∈ {0,1}p√c−1.
The following polynomial takes the inner product of each of these one-hot encodings:
Pj(u) :=p√c−2∑︂
k=0u1
j,k·u2
j,k+ (1−p√c−2∑︂
k=0u1
j,k)(1−p√c−2∑︂
k=0u2
j,k)
for0< j < p . Here, note that there can be only be at most 1in both u1
jandu2
j, and thus, Pj(u) = 1 iff the jth block agree.
Next, the following polynomial is equivalent to the Boolean function that solves the EQ problem:
P(u) :=p−1∏︂
j=0Pj(u),
and we have P(u) = 1{u1≡u2}. Here, note that Pis multi-linear and has degree 2pas each Pjis a degree-2 polynomial.
Moreover, Pis block-exclusive as each Pjis block-exclusive and we only multiply monomials from different blocks in P.
Proposition F.28. LetPbe the block-exclusive polynomial that solves the EQ problem in the p-hot encoding. Then,
deg(P)≥2p.
Proof. For the sake of contradiction, assume that there exists a block-exclusive polynomial Pthat solves EQ in the p-hot
encoding setting with degree ≤2p−1. Then, given an input u:= (u0, . . . ,up−1)from the almost p-hot encoding, where
each block uicorresponds to the truncated bit string from the one-hot encoding in {0,1}p√c−1, we can convert this input to
thep-hot encoding v:= (v0, . . . ,vp−1)as follows:
vi:=⎛
⎝ui,0, . . . ,ui,p√c−2,1−p√c−2∑︂
j=0ui,j⎞
⎠
Then, the block-wise multilinear polynomial Q(u) =P(v)solves the EQ problem in the almost one-hot encoding setting
and has deg(Q)≤deg(P)≤2p−1which contradicts the combination of Proposition F.26 and Lemma F.27.
Theorem F.29. A data-independent BaseConv model needs at least ⌊log(2p)⌋-layers to solve MQAR for an input
sequence u∈ {0,1}3N×din the p-hot encoding setting, where d=p·p√c.
50

--- PAGE 51 ---
Simple linear attention language models balance the recall-throughput tradeoff
Proof. We know from Corollary F.21 that it suffices to show a lower bound for the EQ problem. Moreover, we know from
Proposition F.28 that we cannot solve the EQ problem in the p-hot encoding setting with a block-exclusive polynomial of
degree ≤2p−1. Now, assume that there is a BaseConv model with Llayers that exactly solves EQ in the p-hot encoding
setting. Then, due to Lemma F.17 and Proposition F.26, this yields a block-exclusive polynomial P(u)of degree at most
2L. Here, if L <⌊log(2p)⌋which , then the resulting BaseConv withLlayers results in a block-exclusive polynomial of
degree ≤2p−1. This contradicts the above claim that we cannot have a block-exclusive polynomial of degree <2pthat
exactly represents EQ. Consequently, a data-independent BaseConv model needs ≥ ⌊log(2p)⌋-layers to solve EQ.
F.6. Upperbound on MQAR with sub-logarithmically many BaseConv layers
Setup: For an input Q,K,V∈RN×d, the MQAR problem is computing (C⊙(QK⊤))×Vwhere C∈RN×Nis a
lower triangular matrix with 1s in all possible non-zero positions:
C[i, j]≡{︄
1ifj≤i
0otherwise .
Further, we define the following notation:
b=⌈log(N+ 1)⌉,
b=⌈log(d+ 1)⌉,
N= max( N, d).
Finally, we define bin(i)∈ {0,1}bto be the binary representation of 1≤i≤Nandbin(j)∈ {0,1}bto be the binary
representation of 1≤j≤d. All vectors are assumed to be in column form and all row and column indices will start from 1.
We assume that:
(i) Each row of QandKuse 1-hot encoding and
(ii) Each query matches with at most one key.
We show that BaseConv can compute the MQAR problem with O(log log( N))layers:
Theorem F.30. The MQAR problem with 1-hot encoded tokens, at most one key match per query, and N≥8can be solved
withBaseConv (N, O(log log( N)), d, O (Nlog(N)), O(Nlog(N))).
F.6.1. B ASECONV PRIMITIVES
In this section we show some basic primitives that will be helpful in proving Theorem F.30.
We define e(j)
ito be the the ith standard basis vector with a dimension of j(remember, both are one based indexing).
We first define the primitives and then show how to implement them using BaseConv.
Note that if a convolution matrix, denoted with hare given as a single column, all columns of this matrix are identical as as
defined. Specifically, if given h∈RNandx∈RN×dwe have h∗hto denote k∗xwhere k[:, j] =h∀j∈[d].
Definition F.31. repeat columns (y, r)
INPUT :y∈RN′×d′r, r∈ Z+.
OUTPUT :z∈RN′×d′r,where zhas each of the first d′columns of yrepeated rtimes. In other words,
y≡(︄
↑
y(1)
↓, . . . ,↑
y(d′)
↓,↑
?
↓, . . . ,↑
?
↓)︄
,z≡⎛
⎜⎝↑
y(1)
↓,↑
. . .y(1)
↓⏞⏟⏟⏞
r times, . . . ,↑
y(d′)
↓, . . . ,↑
y(d′)
↓⏞⏟⏟ ⏞
r times⎞
⎟⎠.
Definition F.32. repeat matrix (y, r)
INPUT :y∈RN′r×d′, r∈ Z+such that y[N′+ 1 :,:] =0N′(r−1)×d′.
OUTPUT :z∈RN′r×d′,where zis the first N′rows of yrepeated rtimes. In other words,
51

--- PAGE 52 ---
Simple linear attention language models balance the recall-throughput tradeoff
y≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←y[1 :N′,:]→
0N′×d′
...
0N′×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,z≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←y[1 :N′,:]→
←y[1 :N′,:]→
...
←y[1 :N′,:]→⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭rtimes
Definition F.33. cumulative sum(y)
INPUT :y∈RN′×d′.
OUTPUT :z∈RN′×d′,where each row of zis the sum of all rows of ywith a smaller or equal index. In other words,
y≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←y1→
...
←yi→
...
←yN′→⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,z≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←y1→
...
←∑︁i
j=1yj→
...
←∑︁N′
j=1yj→⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Definition F.34. sumallcolumns (y)
INPUT :y∈RN′×d′.
OUTPUT :z∈RN′×d′,
where the first column of zhas the sum of all columns of yand the rest are all zeros. In other words,
y≡(︄
↑
y(1)
↓,↑
y(2)
↓, . . . ,↑
y(d′)
↓)︄
,z≡(︄↑∑︁d′
j=1y(j)
↓,↑
0
↓, . . . ,↑
0
↓)︄
.
Definition F.35. sumcolumn blocks (y, B)
INPUT :y∈RN′×d′, B∈ Zsuch that Bdivides d′.
OUTPUT :z∈RN′×d′,where of column blocks the first column block of zis the sum of all column blocks and the rest are
zero. In other words,
z[:,1 :B]≡d′
B−1∑︂
j=0y[:, jB+ 1 : ( j+ 1)B]
z[:, j] =0N′for all j > B.
Note that sumallcolumns (y)=sumcolumn blocks (y,1).
Definition F.36. onehotencoding (y, d)
INPUT :y∈RN′×N′.
OUTPUT :z∈RN′×N′,where the first ⌈log(N′)⌉columns of each row of yrepresent a binary encoding yi∈[1, N′]which
is converted to a 1-hot encoding. In other words,
52

--- PAGE 53 ---
Simple linear attention language models balance the recall-throughput tradeoff
y≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝bin(y1)⊤,←?→
...
bin(yi)⊤,←?→
...
bin(yN′)⊤,←?→⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠z≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝e⊤
y1
...
e⊤
yi
...
e⊤
yN′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Definition F.37. remember (y, r, t, f )
INPUT :y∈RN′×d′, r∈ Z, t∈ Z, f:Rt−r→Rt−r+s,v1∈Rr,x∈Rt−r, where yis defined as below.
OUTPUT :z∈RN′×d′, which is defined as follows:
y≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←v1→
←x→
0s×d′
←v2→
0
...
0⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠z≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←v1→
←f(x)→
←v2→
0
...
0⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Recall, that shift-down (y, s)andshift-up (y, s)will shift the matrix ydown or up by s rows, respectively.
Proposition F.38 ( (Arora et al., 2023a)) .For any y∈RN×d, there exist (N,1, d, N, d )−BaseConv and(N,3, d, N, d )−
BaseConv that computes shift down (y, s)andshift up(y, s)for any s≤N.
Now we will show how each primitive is implemented in terms of B ASECONV layers.
Proposition F.39 (The Repeat Columns primitive) .For any y∈RN′×d′rand any r∈ Z+there exists a
(N′,1, d′r, N′, d′r)−BaseConv that computes repeat columns (y, r).
Proof. Define
z←BASECONV(y,W,0N′×d′,0N′×d′,1N′×d′),
where W∈Rd′×d′is defined as:
W[i, j] =⎧
⎨
⎩1if⌈︃i
r⌉︃
=j
0otherwise .
Then note that the output of this layer is:
z= (yW+0N′×d′)⊙(0N′×d′∗y+1N′×d′) =yW =⎛
⎜⎝↑
y(1)
↓,↑
. . .y(1)
↓⏞⏟⏟⏞
r times, . . . ,↑
y(d′)
↓, . . . ,↑
y(d′)
↓⏞⏟⏟ ⏞
r times⎞
⎟⎠,
where the last equality follows from the definition of W.
53

--- PAGE 54 ---
Simple linear attention language models balance the recall-throughput tradeoff
Proposition F.40 (The Repeat Matrix primitive) .For any y∈RN′r×d′and any r∈ Z+there exists a (N′r,1, d′, N′r, d′)−
BaseConv that computes repeat matrix (y, r).
Proof. Define
z←BASECONV(y,0d′×d′,1N′r×d′,h,0N′r×d′),
where h∈RN′×d′is defined as:
h(X)≡r−1∑︂
j=0xN′j.
The computation of the convolution will result in:
(h∗y) = coeff(( y(X))·(1 +XN′+. . .+XN′×(r−1))
= coeff( y(X) +y(X)·XN′+. . .+y(X)·(XN′×(r−1)))
=y+shift-down (y, N′) +. . .+shift-down (y, rN′).
In the above, the final equality follows from Proposition F.38. The output of this layer will compute:
z= (y·0d′×d′+1N′×d′)⊙(h∗y+0N′×d′) =h∗y=⎛
⎜⎜⎜⎝y[1 :N′,:]
...
y[1 :N′,:]⎞
⎟⎟⎟⎠.
In the above, the last equality follows from Proposition F.38 and the fact that y[N′+ 1 :,:] =0((N′−1)×r)×d′)
Proposition F.41 (The Cumulative Sum primitive) .For any y∈RN′×d′there exists a (N′,1, d′, N′, d′)−BaseConv
that computes cumulative sum(y).
Proof. Define
z←BASECONV(y,0d′×d′,1N′×d′,h,0N′×d′),
where h∈RN′×d′is defined as:
h(X)≡N′−1∑︂
j=0xj.
The computation of the convolution will result in:
(h∗y) = coeff(( y(X))·(1 +X+. . .+XN′−1)
= coeff( y(X) +y(X)·X+. . .+y(X)·XN′−1)
=y+shift-down (y,1) +. . .+shift-down (y, N′−1).
The output of this layer is:⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←y1→
...
←∑︁i
j=1yj→
...
←∑︁N
j=1yj→⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
54

--- PAGE 55 ---
Simple linear attention language models balance the recall-throughput tradeoff
Proposition F.42 (The Sum All Columns primitive) .For any y∈RN′×d′there exists a (N′,1, d′, N′, d′)−BaseConv
that computes sumallcolumns (y).
Proof. Define
z←BASECONV(y,W,0N′×d′,0N′×d′,1N′×d′),
where W∈Rd′×d′is defined as:
W[i, j]≡{︄
1ifj= 1
0otherwise .
The output of this layer will be:
(yW+0N′×d′)⊙(0N′×d′∗y+1N′×d′) =yW =(︄↑∑︁d′
j=1y(j)
↓,↑
0
↓, . . . ,↑
0
↓)︄
,
where the last equality follows from the definition of W.
Proposition F.43 (The Sum Block Columns primitive) .For any y∈RN′×d′andBthat divides d′there exists a
(N′,1, d′, N′, d′)−BaseConv that computes sumcolumn blocks (y, B).
Proof. Define
z←BASECONV(y,W,0N′×d′,0N′×d′,1N′×d′),
where W∈Rd′×d′is defined as:
W[i, j]≡{︄
1ifj≤Bandj−B⌊i
B⌋
0otherwise .
The output of this layer will be:
(yW+0N′×d′)⊙(0N′×d′∗y+1N′×d′) =yW
=⎛
⎜⎝↑
∑︁d′
B−1
j=0y[:, jB+ 1]
↓, . . . ,↑
∑︁d′
B−1
j=0y[:, jB+B−1]
↓,0, . . . ,0⏞⏟⏟⏞
d′−⌊d′
D⌋times⎞
⎟⎠,
where the last equality follows from definition of W.
Proposition F.44 (The 1-hot primitive) .For any y ∈ RN′×N′there exists a
(N′,⌈log log( N′)⌉+O(1), N′,2N′⌈logN′⌉, N′)−BaseConv that computes onehotencoding (y).
Proof. We first give a sketch of the proof. Each row has a binary representation of a number which we want to convert to
it’s 1-hot encoding. In order to do this, we need to know which position in the 1-hot encoded vector needs to be 1. We need
to extract information from each bit, which details which subset of positions the 1-hot encoded vector could potentially have
as 1. Concretely, if the least significant bit is 0, only the even position can be represented, if that same bit is 1, only the odd
positions can be represented. This pattern continues for all bits in the binary number. Each bit in the binary representation
gets its own row. Finally, we take the bit wise ANDs of each row of the same binary representation to get the resulting 1-hot
encoded vector. Next, we present the details.
First compute z1∈R2N′⌈logN′⌉×N′defined as:
z1←repeat matrix (y,2⌈log(N′)⌉).
55

--- PAGE 56 ---
Simple linear attention language models balance the recall-throughput tradeoff
Define
z2←BaseConv (z1,IN′×N′,02N′⌈logN′⌉×N′,02N′⌈logN′⌉×N′,b2
2),
where b2∈R2N′⌈logN′⌉×N′is defined as for 1≤i≤2N′⌈logN′⌉and1≤j≤N′:
b2
2[i, j]≡{︄
1if(︁⌊︁i
N′⌋︁
mod⌈log(N′)⌉)︁
+ 1 = j
0otherwise.
Note that z1has2 log( N′)copies of the binary representations in the first column block. When we zero them out to get z2,
the(imod log( N′))th matrix stores the value of the ith bit with all others being set to zero.
Compute z3∈R2N′⌈logN′⌉×N′by storing the sum all columns in the first column. Being that each row only has a single
non-zero entry, this is equivalent to moving every one of these non-zero entries to the first column. Define
z3←sumallcolumns (z2).
Compute z4∈R2N′⌈logN′⌉×N′by copying the first column to all other columns. Define
z4←repeat columns (z3, N′).
Then, we will take a Hadamard product of z4with a binary representation matrix. Define
z5←BaseConv( z4,0N′×N′,b5
1,1,02N′⌈logN′⌉×N′).
In the above, b5
1has the positions where a binary number with that has that bit set to a 1 could fall(and 0 if it in the bottom
half of the matrix. We will define it in blocks where 1≤i≤2⌈logN′⌉,1≤k≤N′,1≤j≤N′
b5
1[(i, k), j]≡⎧
⎪⎨
⎪⎩1ifjmod 2i≥2i−1andi≤ ⌈logN′⌉
1ifjmod 2i<2i−1andi >⌈logN′⌉
0otherwise .
Next, we will combine the two representations:
z6←BaseConv( z5,0N′×N′,b6
1,h6,02N′⌈logN′⌉×N′),
where b6
1,h6are defined as:
b6
1=⎛
⎝0(N′⌈logN′⌉)×N′
1(N′⌈logN′⌉)×N′⎞
⎠,h6=⎛
⎜⎝e(N′⌈logN′⌉)
1
e(N′⌈logN′⌉)
1⎞
⎟⎠.
We now specify the results of this kernel:
(︁
h6∗z5)︁
= coeff(︂(︂
1 +XN′⌈logN′⌉)︂
·z5(x))︂
= coeff(︂
z5(x) +z5(x)·XN′⌈logN′⌉)︂
=z5+shift-down (z5, XN′⌈logN′⌉)
By combining the possible positions based on a 0or1being present in the binary number, z6now stores the expanded
binary representation in each row block in the bottom half of the matrix. We move this to the top half as shown below:
z7←shift-up (z6, N′⌈logN′⌉).
56

--- PAGE 57 ---
Simple linear attention language models balance the recall-throughput tradeoff
Finally we do a bit wise multiplication between corresponding rows. For 0≤m <⌈log log N′⌉such that on the m’th
iteration the following function is performed:
z′
8,m←BASECONV(z8,m−1,I2N′⌈logN′⌉×N′,02N′⌈logN′⌉×N′,hm,02N′⌈logN′⌉×N′)
z8,m←shift-up (z′
8,m, N′2m)
Where z8,−1≡z7andhmis defined below:
hm=⎛
⎜⎝e(2m)
1
e(2m−2N′⌈logN′⌉)
1⎞
⎟⎠.
The computation of this convolution will result in:
(hm∗z8,m) = coeff(︂(︂
1 +X(2m))︂
·z8,m)︂
(x)
= coeff(︂
z8,m(x) +z8,m(x)·X(2m))︂
=z8,m+shift-down (z8,m, X(2m))
z7holds the possible positions of of the ” 1” in the one hot vector for each bit in the binary representation. This step takes a
bit-wise and of the rows corresponding to the same binary representation so that we are left with the 1-hot encoding of the
original binary representation. The idea is that for each bit in each binary representation of each value, there is a row in z7
that represents the possible positions, in the form of a bitmap, that this binary number can encode. When we bitwise AND
all of these rows together, we are left with the position that satisfies all the constraints and is therefore the index that these
binary numbers encoded.
Proposition F.45 (The Remembering Primitive) .For any x∈Rn×d′,v1∈Rr×d′,v2∈Rm−rwhere n=t−r
contained in some y∈RN′×d′such that v1is in the first rrows,xis in the next nrows, 0s fill up the next srows,
andv2are in the next m−rrows, for some 3n+ 3m+ 2s+ 2t≤N′so that for h∈Rn×dandW∈Rd′×d′with
x∗h∈R(n+s)×d′andv∗h∈R(m+t)×d′, where v∈Rm×dis defined as v2+shift down( v1, m−r), there
exists a (N′,8, d′, N′, d′)−BaseConv that computes remember (y, r, t, f ), where fcan be implemented in 1 layer of
BASECONV through the parameters W∈Rd′×d′,h∈RN′×d′,b1∈RN′×d′,b2∈RN′×d′as defined below:
f(u) =(︃(︃uW
0s×d′)︃
+(︃b1
1s×d′)︃)︃
⊙(︃
u∗h+(︃b2
0s×d′)︃)︃
Proof. First, we will convert the yso that it stores vin consecutive rows to get z1. Recall
y=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝←v1→
←x→
0s×d′
←v2→
0(N′−m−s−n)×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
We compute z1as:
z1←BASECONV(y,0d′×d′,b1
1,h1,0N′×d′),
57

--- PAGE 58 ---
Simple linear attention language models balance the recall-throughput tradeoff
where the kernels h1∈RN′×d′andb1
1∈RN′×d′are given by:
h1←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝e(n)
1
0m
0s
e(n)
1
0n
···
0n⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,b1
1←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0r×d′
1n×d′
0s×d′
1(m−r)×d′
1r×d′
0n×d′
0s×d′
0(m−r)×d′
. . .
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
We now specify the result of this kernel:
(︁
h1∗y)︁
= coeff(︁
(1 +Xn+s+m)·(︁
v1(X) +x(X)·Xr+v2(X)·Xn+s+r)︁)︁
= coeff(︁
v1(X) +x(X)·Xr+v2·Xn+s+r+v1(X)·Xn+s+m+x(X)·Xn+s+m+r+v2·X2n+2s+m+r)︁
=v1+shift-down (x, r) +shift-down (v2, n+s+r) +shift-down (v1, n+s+m)
+shift-down (x, n+s+m+r) +shift-down (v2,2n+ 2s+m+r).
With this we have:
z1=(︂
y·0d′×d′+b1
1)︂
⊙(︂
h1∗y+0N′×d′)︂
=b1
1⊙(︁
h1∗y)︁
=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0r×d′
1n×d′
0s×d′
1(m−r)×d′
1r×d′
0n×d′
0s×d′
0(m−r)×d′
. . .
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝v1
x
0s×d′
v2
v1
x
0s×d′
v2
. . .
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0r×d′
x
0s×d′
v2
v1
0n×d′
0s×d′
0(m−r)×d′
. . .
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0r×d′
x
0s×d′
v
0n×d′
0s×d′
0(m−r)×d′
. . .
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Next compute z2:
z2←shift-up (z1, r),
58

--- PAGE 59 ---
Simple linear attention language models balance the recall-throughput tradeoff
as seen in (Arora et al., 2023a) Proposition F.38.
At this point z2looks like:
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝x
0s×d′
v
0n×d′
...
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Next, we will apply ftoz2to get f(x)but also retain a unchanged version of v. Define
z3←BASECONV(z2,W,b3
1,h3,b3
2),
with the kernels h3,b3
1, andb3
2for this layer are given by:
h3←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝h
0s×d′
0m×d′
0t×d′
e(n)
1
0s×d′
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,b3
1←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝b1
1s×d′
0m×d′
0t×d′
0n×d′
0s×d′
1m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,b3
2←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝b2
0s×d′
0m×d′
0t×d′
0n×d′
0s×d′
0m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Remember that W,h,b1,b2come from the definition of f.
We specify the result of this kernel as:
(︁
h3∗z2)︁
= coeff(︁
(h(X) +Xn+s+m+t)·(︁
x(X) +v(X)·Xn+s)︁)︁
= coeff(︁
h·x(X) +h·v(X)·Xn+s+x(X)·Xn+s+m+t+v(X)·X2n+2s+m+t)︁
=h∗x+shift-down (h∗v, n+s)
+shift-down (x, n+s+m+t) +shift-down (v,2n+ 2s+m+t).
59

--- PAGE 60 ---
Simple linear attention language models balance the recall-throughput tradeoff
With this we have
z3=(︁
z2W+b3
1)︁
⊙(︁
h3∗z2+b3
2)︁
=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝xW
0s×d′
vW
0t×d′
0n×d′
0s×d′
0m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠+⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝b1
1s×d′
0m×d′
0t×d′
0n×d′
0s×d′
1m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝h∗x
h∗v
x
0s×d′
v⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠+⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝b2
0s×d′
0m×d′
0t×d′
0n×d′
0s×d′
0m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝(︃(︃xW
0s×d′)︃
+(︃b1
1s×d′)︃)︃
⊙(︃
h∗x+(︃b2
0s×d′)︃)︃
(︃
vW
0t)︃
⊙h∗v
0n×d′
0s×d′
v⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Note that(︃(︃xW
0s×d′)︃
+(︃b1
0s×d′)︃)︃
⊙(︃
h∗x+(︃b2
0s×d′)︃)︃
isf(x)as defined. This next step will mask out duplicate and
unnecessary xandvvalues. Define
z4←BASECONV(z3,IN′×d′,0N′×d′,0N′×d′,b4
2),
where the kernel b4
2∈RN′×d′for this layer is given by:
b4
2←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝1(n+s)×d′
0(m+t)×d′
0(n+s)×d′
1(m)×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
60

--- PAGE 61 ---
Simple linear attention language models balance the recall-throughput tradeoff
We will specify the output of this layer:
z4=z3⊙b4
2=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝f(x)
(︃vW
0t)︃
⊙h∗v
0(n+s)×d′
v⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝1(n+s)×d′
0(m+t)×d′
0(n+s)×d′
1m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝f(x)
0(m+t)×d′
0(n+s)×d′
v⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
In the next step we will reorder the information such that f(x)andvare contained in contiguous rows by copying it with a
convolution. Define
z5←BASECONV(z4,0d′×d′,b5
1,h5,0N′×d′),
wehre the kernels h5∈RN′×d′andb5
1∈RN′×d′for this layer is given by:
h5←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝e(n)
1
0s×d′
0m×d′
0t×d′
e(n)
1
0s×d′
0n×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,b5
1←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
1(n+s)×d′
1m×d′
0(m+t)×d′
0(n+s)×d′
0m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
(︁
h5∗z4)︁
= coeff(︁
(1 +Xn+s+m+t)·(︁
f(x)(X) +v(X)·X2n+2s+m+t)︁)︁
= coeff( f(x)(X) +f(x)(X)·Xn+s+m+t+v(X)·X2n+2s+m+t+v(X)·X3n+3s+2m+2t)
=f(x) +shift-down (f(x), n+s+m+t) +shift-down (v,2n+ 2s+m+t)
+shift-down (v,3n+ 3s+ 2m+ 2t).
61

--- PAGE 62 ---
Simple linear attention language models balance the recall-throughput tradeoff
z5=b5
1⊙(︁
h5∗z4)︁
=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
1(n+s)×d′
1m×d′
0(m+t)×d′
0(n+s)×d′
0m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝f(x)
0(m+t)×d′
f(x)
v
0(m+t)×d′
0(n+s)×d′
v⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
f(x)
v
0(m+t)×d′
0(n+s)×d′
0m×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
The next step we will duplicate the entries so we can position v1andv2in the same position relative to the enacted upon
portion of the matrix as in the input. Define
z6←BASECONV(z5,0d′×d′,b6
1,h6,0N′×d′),
where the kernels h6andb6
1for this layer is given by:
h6←⎛
⎜⎝e(2n+2s+2m+t)
1
e(N′−2n−2s−2m−t)
1⎞
⎟⎠,b6
1←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
0(n+s)×d′
0(m−r)×d′
1r×d′
1(n+s)×d′
1(m−r)×d′
0r×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Specifically the convolution does this:
(︁
h6∗z5)︁
= coeff(︁
(1 +X2n+2s+2m+t)·(︁
((f(x))(X))·Xn+m+s+t+v(X)·X2n+m+2s+t)︁)︁
= coeff(( f(x))(X))·Xn+m+s+t+ coeff(( f(x)(X))·X3n+3m+3s+2t)
+v(X)·X2n+1m+2s+t+v(X)·X4n+3m+4s+2t
=shift-down ((f(x)(X)), n+m+s+t)
+shift-down ((f(x)(X)),3n+ 3m+ 3s+ 2t)
+shift-down (v,2n+m+ 2s+t) +shift-down (v,4n+ 3m+ 4s+ 2t).
62

--- PAGE 63 ---
Simple linear attention language models balance the recall-throughput tradeoff
z6=b6
1⊙(︁
h6∗z5)︁
=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
0(n+s)×d′
0(m−r)×d′
1r×d′
1(n+s))×d′
1(m−r)×d′
0r×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
f(x)
v2
v1
f(x)
v2
v1⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝0(n+s)×d′
0(m+t)×d′
0(n+s)×d′
0(m−r)×d′
v1
f(x)
v2
0r×d′⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Finally we get,
z7←shift-up (z6,2n+ 2m+ 2s+t−r)
The final output of this layer is:
z7←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝v1
f(x)
v2
0
...
0⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,
which is our final output.
Corollary F.46. Letybe as in Proposition F .45 but now let fbe implemented with BaseConv (N′, L, d′, N′, d′). Then
remember (y, r, t, f )where t−r=ncan be implemented with BaseConv (N′, O(L), d′, N′, d′).
Proof. The Remember primitive can be used to implement any number of BaseConv layers. As shown, the remember
primitive can perform a BaseConv operation on a portion of a matrix while maintaining the values of the rest. This output
matrix can then be fed through another remember primitive repeatedly such that any number of BaseConv layers can be
performed through remember.
Definition F.47. Being that primitives 1-7 can be implemented using BaseConv layers and that remember can apply
BaseConv to a continuous subsection of a matrix, we can implement these primitives on subsections of any matrix ”through”
remember. This will be represented as Remember( i, j, f)where iandjare the start and end rows that will be effected,
respectively, and fis the function which will be applied to them.
F.6.2. P ROOF OF THEOREM F.30
We first mathematically state the major steps, after which we will show how to implement each step using BaseConv layers.
63

--- PAGE 64 ---
Simple linear attention language models balance the recall-throughput tradeoff
1. I NPUT :Q,K,V.
OUTPUT :Q,K′,Vwhere K′∈RN×dbis defined below.
In steps 1.1 and 1.2, for 1≤i≤N, replace each 1inK[i,:]bybin(i)⊤to getK∈RN×db. Then in step 1.3 then we
compute K′where every row is the sum of all previous rows and itself in K.
1.1 I NPUT :Q,K,V.
OUTPUT :Q,K′,Vwhere K′∈RN×dbis defined below.
K′:=repeat columns (K, b).
1.2 I NPUT :Q,K′,V.
OUTPUT :Q,K,Vwhere B∈RN×db,K′∈RN×dbare defined below.
B[i, jb+ 1 : ( j+ 1)b] := bin(i)⊤for all 1≤i≤Nand1≤j≤d. (37)
K:=B⊙K′.
1.3 I NPUT :Q,K,V.
OUTPUT :Q,K′,Vwhere K′∈RN×dbis defined below.
K′[i,:] :=i∑︂
j=1K[j,:]for all 1≤i≤N.
2. I NPUT :Q,K′,V.
OUTPUT :M,Vwhere M∈RN×dbis defined as follows.
In steps 2.1-2.3, compute M[i,:]so that M[i,1 :b] =bin(j)⊤where Q[i,:] =K[j,:]for every 1≤i≤N.Note
that by assumption (ii) only one such jexists.
2.1 I NPUT :Q,K′,V.
OUTPUT :Q′,K′,V.
Compute Q′∈RN×dbto beQwith each column repeated btimes.
Q′=repeat columns (Q, b).
2.2 I NPUT :Q′,K′,V.
OUTPUT :M′,V.
Compute M′as it is defined below:
M′=Q′⊙K′.
Some column block of M′holds bin(j)such that Q[i,:]matches K[j,:], we now move it to the first column
block in step 2.3.
2.3 I NPUT :M′,V.
OUTPUT :M,V.Compute Mas it is defined below:
M=sumcolumn blocks (M′, b).
The first bentries in the i’th row Mholds bin (j)such that Q[i,:]matches K[j,:].
3. I NPUT :M,V.
OUTPUT :L,Vwhere L∈RN×N.
Compute LfromMas defined below:
L=C⊙(QK⊤).
Compute L∈RN×NfromMsuch that the binary representation of jin the i’th block is converted into 1-hot encoding
ofjin the i′th row of M. Define
L←onehotencoding (M)
64

--- PAGE 65 ---
Simple linear attention language models balance the recall-throughput tradeoff
So we now have computed L=C⊙(QK⊤). All that’s left to do is to compute L×V. While mathematically this is
a simple operation, we will implement this in multiple steps so that it is easy to implement this with BaseConv layers
on input(︃L
V)︃
.
4. I NPUT :L,V.
OUTPUT :L,V, where V∈RNb×dis defined below.
In steps 4.1 and 4.2, compute VfromVwhere the v’th column holds the information for bin (v)∈ {0,1}b, for every
1≤v≤d:
4.1 I NPUT :L,V.
OUTPUT :L,V1, where V1∈RNb×dis defined below.
V1:=repeat matrix (V,b).
4.2 I NPUT :L,V1.
OUTPUT :L,V, where V1∈RNb×dis defined below. First, define B′∈Nb×dfor1≤i≤b,1≤k≤N,1≤
j≤das:
B′[(i, k), j]≡{︄
1ifjmod 2i≥2i−1
0otherwise.(38)
Then
V:=V1⊙B′.
5. I NPUT :L,V.
OUTPUT :L,V′, where V′∈RNb×dis defined below.
Compute V′fromVsuch that all the non-zero encodings of columns of Vare moved to 1st column and the other
columns are zeroed out, specifically for 1≤i≤Nb:
V′[:, i] :={︄∑︁d
j=1V[:, j]ifi= 1
0Nbotherwise .
The summation of each column of Vperforms this desired move action because each row of Vhas at most 1 non zero
value as each row of B′is a 1-hot encoding.
6. I NPUT :L,V′.
OUTPUT :L,V1where V1∈RN×Nbis defined below.
Compute V1fromV′in steps 6.1-6.6 such that for 1≤k≤NandW∈RNb×Nb:
W[i, j] ={︄
1ifi=(︁(︁
(j−1)N+⌊︁i−1
N⌋︁)︁
mod Nb)︁
+ 1
0otherwise.
V1[k,:] := (V′[:,1]⊤)W.
Wis a permutation matrix which reorders the values such that values relating to the same index rather than values
relating to the same copy of the matrix, made in step 4.1, are adjacent.
6.1 I NPUT :L,V′.
OUTPUT :L,V2.
Compute V2∈RNb×NbfromV′where V2has the 1st column of V′repeated Nbtimes, as defined below for
1≤j≤Nb:
V2[:, j] =V′[:,1]
65

--- PAGE 66 ---
Simple linear attention language models balance the recall-throughput tradeoff
6.2 I NPUT :L,V2.
OUTPUT :L,V3.
Compute V3∈RNb×NbfromV2by zeroing out all but the diagonal:
V3=INb×Nb⊙V2.
6.3 I NPUT :L,V3.
OUTPUT :L,V4.
Compute V4∈RNb×NbfromV3andWsuch that the values are permuted such that the values grouped by which
row they represent, rather than the matrix repeat number they were in. Where V4is defined as:
V4=V3W.
6.4 I NPUT :L,V4.
OUTPUT :L,V5.
Compute V5∈RNb×NbfromV4such that V5[1,:]is the sum of all rows by making V5[Nb,:]the sum of all rows
ofV4followed by zeroing the rest of rows. I.e. for all 1≤i, j≤Nb:
V5[i,:] :={︄∑︁Nb
k=1V4[k,:]ifi=Nb
0 otherwise .
6.5 I NPUT :L,V5.
OUTPUT :L,V6.
Compute V6∈RNb×NbfromV5such that V6[1,:] =V5[Nb,:]and the rest of the rows be zeroed out. I.e. for all
1≤i, j≤Nb:
V6[i,:] :={︄
V5[Nb,:]ifi= 1
0 otherwise .
6.6 I NPUT :L,V6.
OUTPUT :L,V1.
Compute V1fromV6by copying V6[1,:]to rest of the rows.
V1[i,:] :=V6[1,:]for all 1≤i≤N.
At this point, each row of V1now has the same values as the first column of V′permuted in the way that was
stated in 6.3.
7. I NPUT :L,V1.
OUTPUT :Lwhere L∈RN×Nbis defined below.
Compute Lsuch that the single 1inL[i,:](say at position 0≤j < N ) is replaced by bin(j)⊤. In other words, the i’th
query were to match a key in position j, if the i’th row of Lwould have a representation of the matching value at the
j’th block.
7.1 I NPUT :L,V1.
OUTPUT :L′,V1
Compute L′∈RN×Nbby repeating each column of Lbtimes:
L′[:, j] =L[︃
:,⌈︃j
b⌉︃]︃
if1≤j≤Nb.
7.2 I NPUT :L′,V1.
OUTPUT :L
Compute L←L′⊙V1.
8. I NPUT :L.
OUTPUT :L1=L×V,the final output.
66

--- PAGE 67 ---
Simple linear attention language models balance the recall-throughput tradeoff
8.1 I NPUT :L.
OUTPUT :L2∈RN×(N×b)by summing up bchunks of columns of Land store the result in 1st block column
and zero out remaining columns to get L2, specifically for 1≤j≤b:
L2[:,(i, j)]≡{︄∑︁N
k=1L[:,(k, j)]ifi= 1
0N×botherwise .
8.2 I NPUT :L2
OUTPUT :L1by replacing the binary representation in 1st block column by corresponding 1-hot encoding.
Compute L1fromL2such that for 1≤i≤N,L1[i,:] =e(d)
ℓwhere L2[i,(1,:)] = bin(ℓ)⊤.
Next, using the primitives defined in Appendix F.6.1, we show how each step above would be implemented with BASECONV
layers. Instead of 3 separate matrices as shown in the math layout, we will have a single matrix, Y∈R3N×dwhich contains
the information of all three.
Y=⎛
⎜⎜⎜⎜⎝Q
K
V⎞
⎟⎟⎟⎟⎠.
The internal dimension will be (4Nlog(N),Nlog(N)). For notational convenience, we define
z=Nlog(N).
Specifically, the input Y∈R3z×dhas the top left matrix of 3N×dholding⎛
⎝Q
K
V⎞
⎠.
Define
Y0←BASECONV(Y,04z×z,b0
1,h0,04z×z),
where b0
1∈R4z×zandh0∈R4z×zare defined as:
b0
1←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝1N×z
0(z−N)×z
1N×z
0(z−N)×z
1N×z
0(z−N)×z
0N×z
0(z−N)×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,h0←⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝e(z−N)
1
e(z−N)
1
e(z−N)
1
0z+3N⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
The vectors that make up the convolution have dimension z−Nbecause KandVare not top left justified in the original
matrix. This produces a matrix where Q,K andVall sit in the top left position in their own sub-matrix surrounded by
67

--- PAGE 68 ---
Simple linear attention language models balance the recall-throughput tradeoff
zeros. The structure is:
Y0≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝Qp
Kp
Vp
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
where
Qp=(︃Q,0N×(z)
0(z−N)×(z))︃
,Kp=(︃K,0N×(z)
0(z−N)×(z))︃
,Vp=(︃V,0N×(z)
0(z−N)×(z))︃
.
1. Compute Y1in steps 1.1-1.2.
1.1 Compute Y′
1←remember( Y0, z+ 1,2z, f1)where f1is defined as:
repeat-columns (Y0, b).
This results in
Y′
1≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝Qp
K′
Vp
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This repeats the columns of Kbtimes and doesn’t change QorVwithBaseConv (4z, O(1), z,4z, z)via
Corollary F.46.
1.2 Compute Y′′
1←remember( Y′
1, z+ 1,2z, f′
1)where f′
1is defined as:
BASECONV(Y′
1,Iz×z,0z×z,0z×z,B).
Where Bis defined as it was in equation 37. This results in
Y′′
1≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝Qp
K
Vp
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This replaces each 1inKwith the binary representation of the row with BaseConv (4z, O(1), z,4z, z)via
Proposition F.45.
1.3 Compute Y1←remember( Y′′
1, z+ 1,2z, f′′
1)where f′′
1is defined as:
cumulative sum(Y′′
1).
This results in
Y1≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝Qp
K′
Vp
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
68

--- PAGE 69 ---
Simple linear attention language models balance the recall-throughput tradeoff
2. Compute Y2in steps 2.1-2.3.
2.1 Compute Y′
2←Remember( Y1,1, z, f 2)where f2is defined as:
repeat-columns( Y1, b).
This results in
Y′
2≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝Q′
K′
Vp
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This repeats the columns of Qbtimes with BaseConv (4z, O(1), z,4z, z)via Corollary F.46.
2.2Compute Y2as the Hadamard product of the first and second position stored in the second position. This can be
done with the following substeps:
Y2←BASECONV(Y′
2,Iz×z,04z×z,h2,04z×z).
Where h2∈R4z×zis defined as:
h2≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
e(z)
1
0z×z
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This layer computes:
Y′′
2=Y′
2⊙(︁
h2∗Y′
2)︁
=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝Q′
K′
V
1z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
Q′
K′
V⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
Q′⊙K′
K′⊙V
V⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Next, we mask out the unnecessary position using
Y3
2≡BASECONV(Y′′
2,b2
1,04z×z,e4z×z
1,04z×z).
Where b2
1∈R4z×zis defined as:
b2
1≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
1z×z
0z×z
1z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
69

--- PAGE 70 ---
Simple linear attention language models balance the recall-throughput tradeoff
This layer computes:
Y3
2=b2
1⊙Y′′
2=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
1z×z
0z×z
1z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠⊙⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
Q′⊙K′
K′⊙V
V⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
Q′⊙K′
0z×z
V⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
M′
0z×z
V⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Finally, we shift up:
Y4
2←shift-up( Y3
2,z).
This results in:
Y4
2=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝M′
0z×z
V
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This was done using BaseConv (4z, O(1), z,4z, z)via the vanilla BaseConv layers and Proposition F.38.
2.3Sum up block columns in the first position to move the binary representations to the first column blocks through
remember (Y4
2,1, z, f 2).Where f2is defined as:
sumcolumn blocks (Y4
2, b).
This results in:
Y2≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝M
0z×z
V
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Through this step, we have computed MwithBaseConv (4z, O(1), z,4z, z)via Corollary F.46..
3. Compute Y3←Remember( Y2,1, z,f3)where f3is defined by:
onehotencoding( Y2)
This step was computed with BaseConv (4z, O(⌈log log N⌉, z,4z, z)via Corollary F.46. This converts Mto be
1-hot encoded in O(NlogN)BaseConv layers. This results in:
Y3≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
We move this binary representation to the first column block with BaseConv (4z, O(1), z,4z, z)via Proposition F.38.
4. Compute Y4in steps 4.1 and 4.2.
70

--- PAGE 71 ---
Simple linear attention language models balance the recall-throughput tradeoff
4.1 Compute Y′
4←remember( Y3,2z+ 1,3z, f′
4)where f′
4is defined as:
repeat-matrix( Y3,b)
This results in:⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V′
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
We repeat Vmatrix btimes with BaseConv (4z, O(1), z,4z, z)via Corollary F.46.
4.2 Compute Y4←remember( Y′
4,2z+ 1,3z, f4)where f4is defined as:
BaseConv( Y′
4,Iz×z,0z×z,0z×z,B′).
Where B′is defined as it was in equation 38.
This results in: ⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This step can be done with BaseConv (4z, O(1), z,4z, z)via Proposition F.45.
5. Compute Y5←remember( Y4,2z+ 1,3z, f5))where f5is defined as:
sum-all-columns( Y4)
This results in:
Y5≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V′
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Now each row of V′has that one moved to the first column if it existed with BaseConv (4z, O(1), z,4z, z)via
Corollary F.46.
6. Compute Y6through in steps 6.1-6.6.
6.1 Compute Y′
6←remember( Y5,2z+ 1,3z, f′
6)where f′
6is defined as:
repeat columns( Y5, Nb).
This results in:
Y′
6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V2
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here we repeat the columns of V′withBaseConv (4z, O(1), z,4z, z)via Corollary F.46.
71

--- PAGE 72 ---
Simple linear attention language models balance the recall-throughput tradeoff
6.2 Compute Y′′
6←remember( Y′
6,2z+ 1,3z, f′′
6)where f′′
6is defined as:
BaseConv( Y′
6,0,Iz×z,e(z)
1,0).
This results in:
Y′′
6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V3
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here we zeroed out everything except the main diagonal in VwithBaseConv (4z, O(1), z,4z, z)via Proposi-
tion F.45.
6.3 Compute Y3
6←remember( Y′′
6,2z+ 1,3z, f3
6)where f3
6is defined as:
BaseConv( Y′′
6,W6,04z×z,04z×z,14z×z).
Where W6is defined as it was in math step 6.
This results in:
Y3
6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V4
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here, because we can only repeat whole matrices and not each row, we are permuting the values on the main
diagonal to reorganize them to be as if they were repeated each row at a time with BaseConv (4z, O(1), z,4z, z)
via Corollary F.46.
6.4 Compute Y4
6←remember( Y3
6,2z,3z−1, f4
6)where f4
6is defined as:
cumulative sum(Y3
6).
This results in:
Y4
6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V5
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here, we use cumulative sum so that the final row in the matrix stores each value in each column with
BaseConv (4z, O(1), z,4z, z)via Corollary F.46.
6.5 Compute Y5
6←remember( Y4
6,2z,3z−1, f5
6)where f5
6is defined as:
shift up(Y4
6, Nb−1).
This results in:
Y5
6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V6
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
72

--- PAGE 73 ---
Simple linear attention language models balance the recall-throughput tradeoff
Here, we shift this final row up to be in the first row with BaseConv (4z, O(1), z,4z, z)via Proposition F.38.
6.6 Compute Y6←remember( Y5
6,2z,3z−1, f6
6)where f6
6is defined as:
cumulative sum(Y5
6).
This results in:
Y6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V1
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here we copied the first row to each row in VwithBaseConv (4z, O(1), z,4z, z)via Corollary F.46. This
results in:
Y6≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z×z
V1
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This step is computed with with BaseConv (4z, O(1), z,4z, z)via Corollary F.46.
7. Compute Y7in steps 7.1-7.3.
7.1 Compute Y′
7←remember( Y6,1, z, f 7)where f7is defined as:
repeat columns( Y6,b).
This results in:
Y′
7≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L′
0z×z
V1
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here we repeat the columns of LwithBaseConv (4z, O(1), z,4z, z)via Corollary F.46.
7.2 We want to Hadamard the first and third position which we can do with:
Y′′
7≡BASECONV(Y′
7,Iz×z,04z×z,h7,04z×z).
Where h7∈Rz×zis defined as:
h7≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
0z
ez
1
0z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here we Hadamard the LandV1together. Which results in:
73

--- PAGE 74 ---
Simple linear attention language models balance the recall-throughput tradeoff
Y′′
7=⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝0z×z
0z×z
L
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
This is done with with BaseConv (4z,1, z,4z, z)via a single BaseConv layer.
7.3 Finally, we shift-up( Y′′
7,2z). This results in:
Y7≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L
0z
0z
0z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This is done with with BaseConv (4z, O(1), z,4z, z)via Proposition F.38.
8. Convert Y7to the final output.
8.1 Here we will move everything to the first block using:
Y′
8≡sumcolumn blocks (Y7, b).
Y′
8≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L2
0z×z
0z×z
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This is done with with BaseConv (4z,1, z,4z, z)via Corollary F.46.
8.2 Compute Y8←remember( Y′
8,1, z, f 8)where f8is defined as:
onehotencoding( Y′
8).
This converts Lto be 1-hot encoded in O(NlogN)BaseConv layers. This results in:
Y8≡⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝L1
0z×z
0z×z
0z×z⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠.
This is done with with BaseConv (4z, O(⌈log log N⌉), z,4z, z)via Corollary F.46.
Overall cost: We can solve the MQAR problem with BaseConv (4z, O(⌈log log N⌉) +O(1), z,4z, z)by stacking
the layers presented above.
74

--- PAGE 75 ---
Simple linear attention language models balance the recall-throughput tradeoff
Table 7. BASED Training Settings
355M 1.4B
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 27 36
Hidden Size 1024 1792
MLP Activation SwiGLU
MLP Width 2
Num. Linear Attn Layers 5 7
Num. Linear Attn Heads 16
Taylor Feature Dimension 16
Linear Attn Positional Encodings None
Num. Sliding Window Layers 5 7
Sliding Window Size 64 16
Sliding Window Heads 16
Sliding Window Positional Encodings Rotary
Num. BaseConv Layers 17 22
BaseConv Projection Expansion Factor 4
BaseConv Filter Size 3
BaseConv Activation SiLU
75

--- PAGE 76 ---
Simple linear attention language models balance the recall-throughput tradeoff
Table 8. Attention Training Settings
355M 1.4B
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 24 36
Hidden Size 1024 1680
Num Heads 16 24
RMSNorm True
MLP Bias False
Flash Attn True
Rotary Emb. Fraction 0.5
MLP Activation SwiGLU
MLP Width 4
Table 9. Mamba Training Settings
355M 1.4B
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 46
Hidden Size 1024 2048
RMSNorm True
Norm Epsilon 1e−5
Dt State 16
Dt (Min, Max) (0.001,0.1)
Dt Init. Strategy Random
Dt Init. Floor 1e−4
Dt Scale 1.0
Dt Softplus True
Projection Expansion Factor 2
Short Conv Filter Size 4
76

--- PAGE 77 ---
Simple linear attention language models balance the recall-throughput tradeoff
Table 10. Hyena Training Settings
355M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 29
Hidden Size 1024
Num Heads 1
MLP Width 2
Short Conv. Filter Size 3
Exp. Mod. Decay (Fast, Slow) 0.3, 1.2
Filter Sine Freq. (w) 14
Filter Order 64
Filter Inner MLP 2
Table 11. Hyena Training Settings
355M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.99
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 24 (No Attention Layers)
Hidden Size 1024
Num Heads 16
MLP Width 4
77

--- PAGE 78 ---
Simple linear attention language models balance the recall-throughput tradeoff
Table 12. Hyena Training Settings
355M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.99
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 19
Hidden Size 1024
MLP Width 3.5
Table 13. Gated Linear Attention (GLA) Training Settings
355M
Optimizer Adam
Optimizer momentum β1, β2= 0.9,0.95
Optimizer eps 1e−8
Precision BFloat16
Warmup 1%
Learning rate decay Cosine
Learning rate (min, base) 8e-5, 8e-4
Global batch size 256
Weight decay 0.1
Num Layers 24
Hidden Size 1024
Num Heads 4
MLP Width 2
78
