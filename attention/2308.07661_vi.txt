# Attention IsNotAllYouNeedAnymore
Zhe Chen
School of Computer Science and Engineering
Northeastern University
Shenyang, Liaoning, China
ml_iot@163.com; chenzhe@mail.neu.edu.cn

## Tóm tắt
Trong những năm gần đây, kiến trúc Transformer phổ biến đã đạt được thành công lớn trong nhiều lĩnh vực ứng dụng, bao gồm xử lý ngôn ngữ tự nhiên và thị giác máy tính. Nhiều nghiên cứu hiện tại nhằm giảm độ phức tạp tính toán và bộ nhớ của cơ chế tự chú ý trong Transformer bằng cách đánh đổi hiệu suất. Tuy nhiên, hiệu suất là chìa khóa cho sự thành công liên tục của Transformer. Trong bài báo này, một họ các thành phần thay thế drop-in cho cơ chế tự chú ý trong Transformer, được gọi là Extractors, được đề xuất. Bốn loại Extractors, cụ thể là Extractor hiệu suất siêu cao (SHE), Extractor hiệu suất cao hơn (HE), Extractor đáng giá (WE), và Extractor tối giản (ME), được đề xuất làm ví dụ. Kết quả thực nghiệm cho thấy việc thay thế cơ chế tự chú ý bằng SHE cải thiện rõ rệt hiệu suất của Transformer, trong khi các phiên bản đơn giản hóa của SHE, tức là HE, WE, và ME, hoạt động gần bằng hoặc tốt hơn cơ chế tự chú ý với độ phức tạp tính toán và bộ nhớ thấp hơn. Hơn nữa, các Extractors được đề xuất có tiềm năng hoặc có thể chạy nhanh hơn cơ chế tự chú ý vì đường dẫn tính toán quan trọng của chúng ngắn hơn nhiều. Ngoài ra, bài toán dự đoán chuỗi trong ngữ cảnh tạo văn bản được công thức hóa bằng chuỗi Markov thời gian rời rạc có độ dài biến đổi, và Transformer được xem xét dựa trên hiểu biết của chúng tôi.

## 1 Giới thiệu
Đã hơn sáu năm kể từ khi giới thiệu kiến trúc Transformer được áp dụng rộng rãi Vaswani et al. [2017]. Mặc dù ban đầu được đề xuất cho các tác vụ chuyển đổi chuỗi, Transformer đã trở thành kiến trúc mô hình thực tế cho một loạt rộng các tác vụ xử lý ngôn ngữ tự nhiên trong những năm gần đây Kim et al. [2023] và đã được áp dụng rộng rãi trong nhiều lĩnh vực khác bao gồm thị giác máy tính Khan et al. [2022] và xử lý giọng nói Latif et al. [2023].

Tính đến tháng 7 năm 2023, Transformer tiêu chuẩn với cơ chế tự chú ý vẫn chiếm ưu thế, đặc biệt trong các mô hình ngôn ngữ lớn (LLM) Touvron et al. [2023], do khả năng song song hóa và dung lượng xuất sắc của nó Zhao et al. [2023].

Chìa khóa thành công của Transformer nằm ở cơ chế tự chú ý Dowdell and Zhang [2019], một loại cơ chế chú ý được giới thiệu lần đầu cho dịch máy trong Bahdanau, Cho, and Bengio [2015].

Tuy nhiên, một hạn chế đáng chú ý của Transformer là độ phức tạp tính toán và bộ nhớ bậc hai liên quan đến độ dài chuỗi. Hạn chế này phát sinh từ cơ chế tự chú ý Ren et al. [2021], đặt ra thách thức cho việc áp dụng Transformer trong các tình huống liên quan đến chuỗi đầu vào dài.

Do đó, nhiều biến thể, bao gồm "Transformers hiệu quả" hoặc "x-formers", đã được đề xuất để giải quyết vấn đề này trong vài năm qua Tay et al. [2022a]. Nói chung, các biến thể này tận dụng các mẫu được xác định trước hoặc mẫu có thể học, xấp xỉ hạng thấp, kernel, lấy mẫu xuống, hoặc thưa thớt để giảm độ phức tạp tính toán và bộ nhớ trong cơ chế tự chú ý. Kết quả là, chúng thường hoạt động kém hơn cơ chế tự chú ý vanilla Dong et al. [2023].

Thật sự, Transformer với cơ chế tự chú ý là một tác phẩm nghệ thuật đáng chú ý. Việc vượt qua nó về mặt hiệu suất là khá thách thức. Để cải thiện hiệu suất của nó, chúng ta cần hiểu toàn diện vấn đề chúng ta đối mặt và từng bit của Transformer và xem xét việc thay thế cơ chế tự chú ý, điều này là gót chân Achilles của nó. Xét cho cùng, tự chú ý không phải là thành phần không thể thiếu của Transformer Liu et al. [2021], Mehta et al. [2023], Tolstikhin et al. [2021]. Đã đến lúc vượt qua cơ chế tự chú ý.

Trong bài báo này, trước tiên chúng tôi công thức hóa bài toán dự đoán chuỗi (ví dụ, dự đoán token tiếp theo dựa trên các token trước đó) trong tạo văn bản bằng chuỗi Markov thời gian rời rạc có độ dài biến đổi. Sau đó, Transformer được xem xét dựa trên hiểu biết của chúng tôi. Hơn nữa, chúng tôi đề xuất một họ các thành phần thay thế drop-in cho cơ chế tự chú ý, được gọi là Extractors. Các Transformers được trang bị các Extractors được đề xuất được xác minh trong tình huống tạo văn bản. Kết quả thực nghiệm cho thấy với các Extractors thay thế cơ chế tự chú ý, các Transformers hoặc hoạt động tốt hơn nhiều với độ phức tạp tính toán và bộ nhớ cao hơn hoặc hoạt động gần bằng hoặc tốt hơn với độ phức tạp tính toán và bộ nhớ thấp hơn, xác nhận rằng cơ chế tự chú ý thực sự không phải là tất cả những gì chúng ta cần nữa.

Những đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi sử dụng chuỗi Markov thời gian rời rạc có độ dài biến đổi để công thức hóa bài toán dự đoán chuỗi trong tạo văn bản.
• Chúng tôi xem xét Transformer dựa trên hiểu biết riêng của chúng tôi.
• Chúng tôi đề xuất một họ các lớp con được gọi là Extractors để thay thế lớp con tự chú ý trong Transformer theo cách drop-in.
• Chúng tôi đánh giá hiệu suất của các Transformers được trang bị Extractors trong setting tạo văn bản sử dụng sách thiếu nhi tiếng Anh miễn phí làm bộ dữ liệu.
• Chúng tôi ước tính độ phức tạp tính toán và bộ nhớ của cả lớp con tự chú ý và các lớp con Extractor được đề xuất.

## 2 Nghiên cứu liên quan
Một số giải pháp đã được đề xuất để thay thế cơ chế tự chú ý theo cách drop-in.

Trong lĩnh vực thị giác máy tính, một perceptron đa lớp (MLP) được sử dụng để thay thế tự chú ý trong Tolstikhin et al. [2021] với độ phức tạp tính toán tuyến tính theo độ dài chuỗi, đạt hiệu suất hơi kém hơn so với các mô hình khác.

Trong Tay et al. [2022b], tự chú ý được thay thế bằng các khối tích chập. Hiệu suất không tốt bằng Transformer tiêu chuẩn trong các tác vụ yêu cầu mô hình hóa mối quan hệ giữa hai hoặc nhiều chuỗi.

Trong Lee-Thorp et al. [2022], biến đổi Fourier được sử dụng để thay thế cơ chế tự chú ý. Hiệu suất gần với Transformer tiêu chuẩn và tốc độ huấn luyện nhanh hơn.

SPADE (state space augmented transformer) sử dụng mô hình không gian trạng thái (SSM) ở lớp dưới cùng và các cơ chế chú ý cục bộ hiệu quả ở các lớp khác Zuo et al. [2022]. Hiệu suất của SPADE gần với Transformer tiêu chuẩn, và nó có độ phức tạp tính toán thời gian và không gian tuyến tính.

Một thành phần thay thế drop-in cho attention có độ phức tạp dưới bậc hai được gọi là Hyena được đề xuất trong Poli et al. [2023]. Hyena luân phiên áp dụng tích chập trong miền thời gian và trong miền tần số. Nó đạt hiệu suất tương đương với Transformer với giảm 20% tính toán huấn luyện cần thiết ở độ dài chuỗi 2k.

## 3 Công thức hóa bài toán
Trong bài báo này, chúng tôi sử dụng tạo văn bản làm ví dụ tác vụ cho Transformer. Trong tạo văn bản, các token bổ sung được tạo ra dựa trên các token đã cho. Trong tác vụ này, một token đề cập đến một phần văn bản, ví dụ, một từ. Tất cả các token hợp lệ tạo thành từ vựng. Để tạo điều kiện tính toán, mỗi token trong từ vựng được liên kết với một chỉ mục từ 0 đến u-1, trong đó u là kích thước của từ vựng.

Bài toán cơ bản trong tạo văn bản là dự đoán chuỗi, tức là dự đoán giá trị của chỉ mục token tiếp theo st+1 dựa trên các giá trị của một chuỗi các chỉ mục token đã cho (s1,s2,···,st), trong đó s1,s2,···,st+1∈{0,1,···,u−1} và t là độ dài của chuỗi đầu vào đã cho. Vì các chỉ mục token có thể đếm được và có thể nhận bất kỳ số nào trong {0,1,···,u−1}, các biến ngẫu nhiên rời rạc S1,S2,···,St+1 có thể được sử dụng để liên kết với các chỉ mục token ở thời điểm 1, thời điểm 2,..., thời điểm t+1, tương ứng.

Để dự đoán giá trị của chỉ mục token tiếp theo st+1 chính xác hơn, chúng ta có thể cố gắng sử dụng tốt các giá trị của các chỉ mục token trong chuỗi đã cho. Vì độ dài của chuỗi không cố định nói chung trong tạo văn bản, một cách hợp lý là xem chuỗi như một chuỗi Markov thời gian rời rạc có độ dài biến đổi Buhlmann and Wyner [1999], Chen and Qiu [2010], trong đó giá trị của chỉ mục token tiếp theo chỉ phụ thuộc vào các giá trị của các chỉ mục token trong chuỗi đã cho với độ dài chuỗi tối đa.

Theo cách này, chúng ta định nghĩa xác suất có điều kiện của việc nhận giá trị của chỉ mục token tiếp theo st+1, đây là xác suất chuyển trạng thái đồng nhất thời gian của chuỗi Markov, như sau:

P(St+1=st+1|St=st,St−1=st−1,···,S1=s1) (1)

trong đó P(·) biểu thị hàm xác suất. Đặt l biểu thị độ dài của cửa sổ ngữ cảnh trong tạo văn bản, đây cũng là bậc cao nhất của chuỗi Markov thời gian rời rạc có độ dài biến đổi. Khi t>l, theo tính chất Markov chúng ta có:

P(St+1=st+1|St=st,St−1=st−1,···,S1=s1)=P(St+1=st+1|St=st,St−1=st−1,···,St−l+1=st−l+1)(2)

Do đó, bài toán dự đoán chuỗi chúng ta đối mặt chuyển thành bài toán dự đoán xác suất nhận giá trị st+1 cho một chuỗi các giá trị (s1,s2,···,st).

Trong tạo văn bản, nếu chúng ta có các xác suất P(St+1=st+1|St=st,St−1=st−1,···,S1=s1) cho tất cả st+1 trong {0,1,···,u−1}, các chiến lược như lấy mẫu top-p và lấy mẫu top-k có thể được sử dụng để chọn một giá trị cho St+1.

Nhưng làm thế nào chúng ta có thể có được những xác suất này? Trong học máy, chúng ta cố gắng xây dựng một mô hình để dự đoán hoặc suy luận những xác suất này bằng cách lấy chuỗi (s1,s2,···,st) làm đầu vào cho mô hình. Đây là nơi Transformer được áp dụng trong tạo văn bản.

Câu hỏi tiếp theo là làm thế nào để huấn luyện mô hình. Vì chúng ta không biết những xác suất này nên như thế nào trước khi huấn luyện mô hình, một chiến lược chúng ta sử dụng ở đây đơn giản là tối đa hóa xác suất dự đoán ˆP(St+1=st+1|St=st,St−1=st−1,···,S1=s1) mà mô hình xuất ra cho một chuỗi quan sát được (s1,s2,···,st,st+1).

Với chiến lược này, hàm mất mát để huấn luyện mô hình cho một chuỗi huấn luyện có độ dài l có thể được suy ra như dưới đây. Điều này giống như những gì chúng ta có được nếu chúng ta thay thế coi dự đoán chuỗi như một tác vụ phân loại đa lớp.

L(W)=−1/l∑(t=1 to l)ln ˆP(St+1=st+1|St=st,···,S1=s1) (3)

trong đó L(·) biểu thị hàm mất mát và W nói chung đề cập đến tất cả các tham số mô hình có thể huấn luyện. Để giữ bài toán đơn giản, trong phần còn lại của bài báo này, chúng ta giả sử t≤l.

## 4 Hiểu về Transformer
Transformer cung cấp cho chúng ta một cách hiệu quả, nhưng tất nhiên không nhất thiết là cách duy nhất hiệu quả, để xây dựng mô hình học máy nói trên. Một đặc điểm đặc biệt của Transformer là nó giữ "chiều" nội bộ của mình không đổi trên tất cả các lớp, điều này làm cho Transformer trông giống như một loại biến đổi nói chung.

Như thể hiện trong Hình 1, đầu vào của Transformer là một chuỗi các chỉ mục token được biểu thị bằng s=(s1,s2,···,st)T, trong đó s∈Z^(t×1) và (·)T biểu thị chuyển vị. Chuỗi các chỉ mục token s được chuyển đổi ảo thành mã hóa one-hot X[in_tok] trước khi nó đi qua một biến đổi tuyến tính gọi là "embedding", như thể hiện trong Phương trình (4), trong đó X[in_tok]=(x[in_tok]_i)T_(1≤i≤t), X[in_tok]∈Z^(t×u), x[in_tok]_i=([si=j−1])_(1≤j≤u), x[in_tok]_i∈Z^(1×u), và [·] là dấu ngoặc Iverson, si=j−1=1 nếu si=j−1 ngược lại si=j−1=0.

X[emb_tok]=X[in_tok]W[emb_tok] (4)

trong đó X[emb_tok]∈R^(t×d), W[emb_tok] là ma trận trọng số, W[emb_tok]∈R^(u×d), và d là "chiều" nội bộ của Transformer.

Trong bài báo này, chúng tôi sử dụng embedding vị trí đã học, vì hiệu suất của chúng gần như giống hệt với phiên bản sinusoidal Vaswani et al. [2017]. Sau đó chúng ta có:

X[emb_pos]=X[in_pos]W[emb_pos] (5)

trong đó X[emb_pos]∈R^(t×d), X[in_pos]=(x[in_pos]_i)T_(1≤i≤t), X[in_pos]∈Z^(t×l), x[in_pos]_i=([i=j])_(1≤j≤l), x[in_pos]_i∈Z^(1×l), W[emb_pos] là ma trận trọng số, và W[emb_pos]∈R^(l×d). Lưu ý rằng theo Vaswani et al. [2017], các ma trận trọng số embedding như W[emb_tok] và W[emb_pos] được nhân với √d.

Sau đó, hai ma trận embedding được cộng lại với nhau, và các phần tử của ma trận tổng của chúng được đặt ngẫu nhiên thành zero với xác suất p trong quá trình huấn luyện, điều này được gọi là "dropout". Dropout là một kỹ thuật điều chuẩn tăng cường hiệu suất của mô hình trong quá trình suy luận, mặc dù với chi phí giảm "dung lượng" mô hình. Như được miêu tả trong Phương trình (6), dropout được áp dụng cho mỗi phần tử x[in_drop]_(i,j) của ma trận đầu vào X[in_drop], trong đó X[in_drop]=(x[in_drop]_(i,j))_(1≤i≤t,1≤j≤d) và X[in_drop]∈R^(t×d).

x[out_drop]_(i,j) = {(1/(1−p))x[in_drop]_(i,j) với xác suất 1−p; 0 với xác suất p} (6)

trong đó x[out_drop]_(i,j) là phần tử của ma trận đầu ra X[out_drop] và X[out_drop]∈R^(t×d). Để đơn giản hóa các phương trình, chúng ta ký hiệu dropout là "hàm" dropout(·). Sau đó, đầu vào của lớp đầu tiên của Transformer X[in_lay1] có thể được viết như:

X[in_lay1]=dropout(X[emb_tok]+X[emb_pos]) (7)

trong đó X[in_lay1]∈R^(t×d).

Như thể hiện trong Hình 1, có hai lớp con trong một lớp của Transformer. Trong Transformer tiêu chuẩn, lớp con đầu tiên là lớp con tự chú ý đa đầu, và lớp con thứ hai là một mạng feed-forward (FFN). Các đầu vào của các lớp con trước tiên đi qua các chuẩn hóa lớp, được gọi là chuẩn hóa lớp trước, và dropout được áp dụng cho các đầu ra của các lớp con. Chuẩn hóa lớp trước được sử dụng trong bài báo này vì các Transformers với chuẩn hóa lớp trước có thể được huấn luyện mà không cần giai đoạn khởi động Xiong et al. [2020]. Có các kết nối dư giữa các đầu vào của các chuẩn hóa lớp và các đầu ra của các dropout. Cả chuẩn hóa lớp trước và kết nối dư đều có lợi cho việc làm dễ dàng quá trình huấn luyện. Chuẩn hóa lớp được áp dụng cho mỗi vector hàng x[in_layernorm]_i của ma trận đầu vào X[in_layernorm], trong đó X[in_layernorm]=(x[in_layernorm]_i)T_(1≤i≤t) và X[in_layernorm]∈R^(t×d), như thể hiện trong Phương trình (8) Ba, Kiros, and Hinton [2016].

x[out_layernrom]_i=g[layernorm]◦(x[in_layernorm]_i−μi)/σi+b[layernorm] (8)

trong đó g[layernorm] là vector độ lợi và b[layernorm] là vector bias, g[layernorm],b[layernorm]∈R^(1×d), μi và σi là trung bình và độ lệch chuẩn của các phần tử của x[in_layernorm]_i, tương ứng, x[out_layernrom]_i là vector hàng thứ i của ma trận đầu ra X[out_layernorm], X[out_layernorm]∈R^(t×d), i=1,2,···,t, và ◦ biểu thị tích theo phần tử hoặc tích Hadamard. Để đơn giản hóa các phương trình, chúng ta ký hiệu chuẩn hóa lớp là "hàm" layernorm(·). Sau đó, đầu vào của lớp con đầu tiên X[in_sub1] có thể được thu được theo Phương trình (9), trong đó X[in_sub1]∈R^(t×d).

X[in_sub1]=layernorm(X[in_lay1]) (9)

Hình 2 miêu tả lớp con tự chú ý đa đầu. Đầu vào X[in_sub1] trước tiên được biến đổi thành n bộ ma trận query/key/value, cụ thể là Qj, Kj, và Vj, trong đó j=1,2,···,n và n là số "đầu".

Qj=X[in_sub1]W[Q]_j (10)
Kj=X[in_sub1]W[K]_j (11)
Vj=X[in_sub1]W[V]_j (12)

trong đó Qj,Kj,Vj∈R^(t×h), W[Q]_j,W[K]_j,W[V]_j là các ma trận trọng số, W[Q]_j,W[K]_j,W[V]_j∈R^(d×h), và h=d/n. Sau đó, các tích vô hướng có tỷ lệ được tính toán sử dụng cả ma trận query và ma trận key, như thể hiện trong Phương trình (13).

Z[att]_j=QjKT_j/√h (13)

trong đó Z[att]_j∈R^(t×t) và j=1,2,···,n. Sau đó, softmax được áp dụng cho các tích vô hướng có tỷ lệ để đầu ra các trọng số chuẩn hóa Aj, trong đó Aj=(aj,i,k)_(1≤i≤t,1≤k≤t), Aj∈R^(t×t), j=1,2,···,n, và

aj,i,k = {e^(zj,i,k)/∑(f=1 to i)e^(zj,i,f) k=1,2,···,i; 0 k=i+1,···,t} (14)

Lưu ý rằng masking cũng được áp dụng cùng lúc trong Phương trình (14). Sau đó, các kết quả có trọng số được tính toán cho mỗi "đầu" sử dụng những trọng số chuẩn hóa này, và các kết quả được đặt đơn giản cùng nhau để tạo ra một ma trận, điều này được gọi là concatenation.

Rj=AjVj (15)
X[out_con]=(R1,R2,···,Rn) (16)

trong đó X[out_con]∈R^(t×d), Rj∈R^(t×h), và j=1,2,···,n. Đầu ra của concatenation X[out_con] đi qua một biến đổi tuyến tính, điều này kết thúc lớp con tự chú ý đa đầu, như thể hiện trong Phương trình (17).

X[out_sub1]=X[out_con]W[out_att] (17)

trong đó X[out_sub1] là đầu ra của lớp con tự chú ý đa đầu, X[out_sub1]∈R^(t×d), W[out_att] là ma trận trọng số, W[out_att]∈R^(d×d). Theo Hình 1, đầu vào của lớp con thứ hai, hoặc lớp con FFN, X[in_sub2] có thể được thu được theo Phương trình (18), trong đó X[in_sub2]∈R^(t×d).

X[in_sub2]=layernorm(dropout(X[out_sub1])+X[in_lay1]) (18)

Và đầu ra của lớp con FFN X[out_sub2] được tạo ra theo Phương trình (19).

X[out_sub2]=ReLU(X[in_sub2]W[sub2_1]+b[sub2_1])W[sub2_2]+b[sub2_2] (19)

trong đó ReLU(·) biểu thị hàm kích hoạt rectified linear unit (ReLU), W[sub2_1] và W[sub2_2] là các ma trận trọng số, W[sub2_1]∈R^(d×c), W[sub2_2]∈R^(c×d), b[sub2_1] và b[sub2_2] là các vector bias, b[sub2_1]∈R^(1×c), b[sub2_2]∈R^(1×d), và c là số nút trong lớp ẩn của FFN. Đầu ra của lớp đầu tiên của Transformer X[out_lay1] có thể được tính toán theo Phương trình 20, trong đó X[out_lay1]∈R^(t×d).

X[out_lay1]=dropout(X[out_sub2])+dropout(X[out_sub1])+X[in_lay1] (20)

Có m lớp được xếp chồng lên nhau trong Transformer như được miêu tả trong Hình 1. Phương trình (9) đến Phương trình (20) được lặp lại cho đến khi chúng ta thu được đầu ra của lớp thứ m X[out_lay m]. Vì dự đoán chuỗi trong tạo văn bản cũng có thể được xem như một tác vụ phân loại đa lớp, thành phần cuối cùng trong Transformer thực tế là một hồi quy softmax, cùng với chuẩn hóa lớp trước. Đầu ra của hồi quy softmax hoặc đầu ra của Transformer ˆY được suy ra như thể hiện trong Phương trình (21).

ˆY=softmax(layernorm(X[out_lay m])W[soft]+b[soft]) (21)

trong đó softmax(·) biểu thị hàm kích hoạt softmax nhận các hàng của ma trận đầu vào của nó, W[soft] là ma trận trọng số, W[soft]∈R^(d×u), b[soft] là vector bias, b[soft]∈R^(1×u), ˆY=(ˆyi,j)_(1≤i≤t,1≤j≤u), và ˆY∈R^(t×u). Sau đó chúng ta có:

ˆP(Si+1=j−1|Si=si,Si−1=si−1,···,S1=s1)=ˆyi,j (22)

trong đó i=1,2,···,t và j=1,2,···,u.

## 5 Các Extractors được đề xuất
Trong phần này, chúng tôi đề xuất một họ các thành phần thay thế drop-in có tên là Extractors để thay thế cơ chế tự chú ý trong Transformer. Một đặc điểm đặc biệt của các Extractors là các trọng số được gán theo thứ tự ngược của chuỗi đầu vào của chúng. Làm ví dụ, chúng tôi xác định bốn loại Extractors trong phần này.

Theo quan điểm của chúng tôi, lớp con FFN bằng cách nào đó triển khai một "bảng tra cứu" hoặc một "bộ ánh xạ" và do đó "ghi nhớ" mối quan hệ giữa đầu vào và đầu ra của nó, điều này có lợi cho việc ghi nhớ các chuyển đổi trạng thái trong chuỗi Markov. Do đó, nếu độ dài hoặc thứ tự của chuỗi Markov là không đổi, chúng ta có thể giải quyết bài toán dự đoán chuỗi chỉ bằng cách sử dụng lớp con FFN. Tuy nhiên, độ dài của chuỗi Markov chúng ta gặp trong tạo văn bản là biến đổi, điều này thúc đẩy chúng ta giải quyết vấn đề độ dài biến đổi trong chuỗi Markov. Một cách tiếp cận hợp lý để giải quyết vấn đề này là chuyển đổi bài toán "độ dài biến đổi" thành bài toán "độ dài không đổi", dẫn chúng ta đến ý tưởng thiết kế các Extractors.

Ý tưởng chính của các Extractors là khá đơn giản. Đầu tiên, chúng ta trích xuất các đặc trưng thống nhất từ toàn bộ chuỗi đầu vào có độ dài biến đổi như một "chỉ mục" có độ dài không đổi để "tra cứu" "bảng". Sau đó, chúng ta điều chỉnh các đặc trưng thống nhất theo độ dài của chuỗi vì "chỉ mục" được mong đợi phản ánh độ dài của chuỗi. Vậy thôi. Với "chỉ mục" có độ dài không đổi (cũng như đầu vào của lớp), chúng ta "tra cứu" "bảng", được triển khai bởi lớp con FFN tiếp theo, và đầu ra "nội dung được ghi nhớ" tương ứng mà "tài liệu" trong hướng nào trạng thái của chuỗi Markov nên chuyển đổi.

### 5.1 Extractor Hiệu suất Siêu cao
Hình 3 minh họa một loại Extractor được đề xuất gọi là extractor hiệu suất siêu cao (SHE). Các biến đổi tuyến tính được đánh dấu màu xanh nhạt ở bên trái nhằm trích xuất các đặc trưng thống nhất từ các đầu vào của chúng. Tổng số các khối biến đổi tuyến tính này là l. Vì chúng ta giả sử t≤l, chỉ có t khối biến đổi tuyến tính được sử dụng tại một thời điểm nhất định. Thiết kế của phần "trích xuất" này được lấy cảm hứng từ cả mạng thần kinh hồi quy (RNNs) và bộ lọc đáp ứng xung hữu hạn (FIR).

Cho một ma trận đầu vào của lớp con Extractor X[in_sub1], trong đó X[in_sub1]∈R^(t×d), X[in_sub1]=(x[in_sub1]_j)T_(1≤j≤t), và x[in_sub1]_j∈R^(1×d), đầu ra của phần "trích xuất" X[out_ext] được tính toán theo Phương trình (23).

x[out_ext]_i=∑(j=1 to i)x[in_sub1]_j W[ext]_(i−j+1) (23)

trong đó X[out_ext]=(x[out_ext]_i)T_(1≤i≤t), X[out_ext]∈R^(t×d), x[out_ext]_i∈R^(1×d), W[ext]_1,W[ext]_2,···,W[ext]_l là các ma trận trọng số, W[ext]_1,W[ext]_2,···,W[ext]_l∈R^(d×d), và i=1,2,···,t.

Biến đổi tuyến tính được đánh dấu màu vàng nhạt ở bên phải trong Hình 3 có ý định đầu ra các điều chỉnh theo phần tử cho phần "trích xuất". Phần "điều chỉnh" này cũng lấy X[in_sub1] làm đầu vào của nó. Ý tưởng đằng sau lựa chọn này là vector đầu vào mới nhất x[in_sub1]_t trong mỗi chuỗi đầu vào (x[in_sub1]_j)T_(1≤j≤t) chứa thông tin vị trí phản ánh độ dài của chuỗi và chứa thông tin có giá trị để dự đoán trạng thái tiếp theo của chuỗi Markov. Đầu ra của phần "điều chỉnh" X[out_adj] được suy ra như sau:

X[out_adj]=(X[in_sub1]W[adj])◦X[out_ext] (24)

trong đó X[out_adj]∈R^(t×d), X[out_adj]=(x[out_adj]_i)T_(1≤i≤t), x[out_adj]_i∈R^(1×d), W[adj] là ma trận trọng số, và W[adj]∈R^(d×d).

Chúng ta có thể lấy X[out_adj] làm đầu ra của lớp con Extractor, vì biến đổi tuyến tính sau đây được thể hiện trong Phương trình (25) là tùy chọn, vì nó không phải là thành phần quan trọng đối với Extractor.

X[out_sub1]=X[out_adj]W[out] (25)

trong đó X[out_sub1] là đầu ra của lớp con Extractor, X[out_sub1]∈R^(t×d), X[out_sub1]=(x[out_sub1]_i)T_(1≤i≤t), x[out_sub1]_i∈R^(1×d), W[out] là ma trận trọng số, và W[out]∈R^(d×d).

### 5.2 Extractor Đáng giá
Phần "trích xuất" của SHE thực tế sử dụng các mạng thần kinh hoàn toàn kết nối có trọng số chia sẻ để "trích xuất" các "chỉ mục" có độ dài không đổi và có thể yêu cầu tài nguyên tính toán và bộ nhớ đáng kể. Một phiên bản đơn giản hóa của lớp con SHE được gọi là extractor đáng giá (WE) được đề xuất trong phần phụ này.

Như thể hiện trong Hình 4, các biến đổi tuyến tính trong phần "trích xuất" của lớp con SHE được thay thế bằng các tích theo phần tử. Do đó, Phương trình (23) được rút gọn thành Phương trình (26).

x[out_ext]_i=∑(j=1 to i)x[in_sub1]_j◦w[ext]_(i−j+1) (26)

trong đó w[ext]_1,w[ext]_2,···,w[ext]_l là các vector trọng số, w[ext]_1,w[ext]_2,···,w[ext]_l∈R^(1×d), và i=1,2,···,t.

### 5.3 Extractor Hiệu suất Cao hơn
Để tăng cường thêm hiệu suất của WE, chúng tôi kết hợp một biến đổi tuyến tính chia sẻ trong phần "trích xuất" để cho phép WE xấp xỉ SHE, như được miêu tả trong Hình 5. Chúng tôi gọi loại Extractor này là extractor hiệu suất cao hơn (HE). Trong trường hợp này, Phương trình (26) được thay thế bằng Phương trình (27) và Phương trình (28).

X[ext]=X[in_sub1]W[in_ext] (27)

trong đó X[ext] là đầu ra của biến đổi tuyến tính, X[ext]∈R^(t×d), X[ext]=(x[ext]_i)T_(1≤i≤t), x[ext]_i∈R^(1×d), W[in_ext] là ma trận trọng số, và W[in_ext]∈R^(d×d).

x[out_ext]_i=∑(j=1 to i)x[ext]_j◦w[ext]_(i−j+1) (28)

### 5.4 Extractor Tối giản
Độ phức tạp tính toán và bộ nhớ của các Extractors nói trên có thể được giảm thêm. Trong phần phụ này, chúng tôi đề xuất một Extractor đơn giản được gọi là extractor tối giản (ME).

Như được minh họa trong Hình 6, ME chỉ chứa phần "trích xuất", trong khi các vector trọng số trong WE hoặc HE được giảm thêm thành các vô hướng. Phương trình (29) mô tả những gì ME làm.

x[out_sub1]_i=∑(j=1 to i)x[in_sub1]_j w[ext]_(i−j+1) (29)

trong đó w[ext]_1,w[ext]_2,···,w[ext]_l là các vô hướng trọng số, w[ext]_1,w[ext]_2,···,w[ext]_l∈R, và i=1,2,···,t.

Cuối cùng nhưng không kém phần quan trọng, các Extractors không chỉ giới hạn ở bốn loại trên. Nhiều biến thể của bốn loại ví dụ của các Extractors cũng có thể được sử dụng để thay thế cơ chế tự chú ý. Chúng là sự đánh đổi giữa hiệu suất và độ phức tạp tính toán và bộ nhớ. Ví dụ, Extractor chỉ bao gồm phần "trích xuất" của HE.

## 6 Thực nghiệm
Để đánh giá hiệu suất của các Extractors được đề xuất, chúng tôi thay thế lớp con tự chú ý đa đầu trong Transformer được giới thiệu trong Phần 4 bằng các lớp con Extractor và sử dụng tạo văn bản làm tác vụ ví dụ. Cần lưu ý rằng các Transformers với các lớp con Extractor cũng có thể được áp dụng cho các tác vụ khác.

Các thực nghiệm được tiến hành trên GPU NVIDIA GeForce RTX 4050 (đơn vị xử lý đồ họa) với kích thước bộ nhớ 6GB, do ngân sách rất hạn chế. Vì lý do này, quy mô của các mô hình trong thực nghiệm của chúng tôi bị hạn chế. Do đó, kích thước từ vựng được sử dụng trong các LLMs thông thường có thể quá lớn để tiến hành thực nghiệm. Để giải quyết vấn đề này, chúng tôi xây dựng một bộ dữ liệu quy mô nhỏ để huấn luyện các mô hình trong thực nghiệm chỉ sử dụng sách thiếu nhi tiếng Anh miễn phí phổ biến, vì không may chúng tôi không tìm thấy bộ dữ liệu có sẵn công khai như vậy. Cụ thể, bộ dữ liệu bao gồm 100 cuốn sách hàng đầu về văn học thiếu nhi tiếng Anh có sẵn tại gutenberg.org, một thư viện các ebook miễn phí. Văn bản thô của các cuốn sách được tokenize thêm bằng tokenizer Hugging Face BPE (mã hóa cặp byte) với kích thước từ vựng là 5000, dẫn đến tổng cộng 8.4M tokens.

Chúng tôi sử dụng chi phí huấn luyện (tức là mất mát huấn luyện trung bình trên một batch) làm thước đo đánh giá vì chi phí huấn luyện bằng perplexity trong tác vụ này. Perplexity đo lường mức độ tốt của một mô hình xác suất dự đoán. Perplexity càng thấp, mô hình dự đoán càng tốt.

Các mô hình được triển khai và huấn luyện bằng framework PyTorch 2.0. Các bias của các mô hình được khởi tạo với zeros, trong khi các trọng số được khởi tạo ngẫu nhiên theo phân phối chuẩn với trung bình bằng không và độ lệch chuẩn là 0.01. Các siêu tham số và cài đặt cho thực nghiệm được liệt kê trong Bảng 1.

Để đánh giá công bằng hiệu suất của Transformer với các Extractors được đề xuất và Transformer với cơ chế tự chú ý, chúng tôi huấn luyện tất cả các mô hình với cùng các siêu tham số, cài đặt, và dữ liệu huấn luyện. Bằng "cùng dữ liệu huấn luyện", chúng tôi có nghĩa là không chỉ tất cả các batches trong huấn luyện là giống nhau, mà thứ tự của các batches cũng giống nhau. Điều này có thể được triển khai bằng cách đặt seed ngẫu nhiên một vài lần.

Vì số đầu trong lớp con tự chú ý ảnh hưởng nhẹ đến hiệu suất, tám mô hình Transformer với số đầu khác nhau (1, 2, 4, 8, 16, 32, 64, và 128, tương ứng) được huấn luyện để so sánh. Hình 7 cho thấy các trung vị của chi phí huấn luyện cho mỗi 2000 batches không chồng lấp. Từ hình, chúng ta có thể thấy rằng mô hình với các lớp con tự chú ý 1-đầu hoạt động tệ nhất, trong khi mô hình với các lớp con tự chú ý 32-đầu hoạt động tốt nhất, theo như các siêu tham số được liệt kê trong Bảng 1. Chúng tôi sử dụng cả mô hình với các lớp con tự chú ý 1-đầu và mô hình với các lớp con tự chú ý 32-đầu trong đánh giá hiệu suất sau đây.

Bốn mô hình Transformer với các lớp con tự chú ý được thay thế bằng các lớp con SHE/HE/WE/ME được đề xuất, tương ứng, được huấn luyện sử dụng cùng các siêu tham số và cài đặt. Hình 8 cho thấy các trung vị của chi phí huấn luyện cho mỗi 2000 batches không chồng lấp. Hình 9 cho thấy chi phí huấn luyện của các mô hình với hoặc các lớp con tự chú ý 32-đầu hoặc các lớp con SHE được đề xuất cho mỗi batch. Có thể quan sát thấy rằng SHE được đề xuất vượt trội rõ rệt so với cơ chế tự chú ý. Các phiên bản đơn giản hóa của SHE, cụ thể là HE/WE/ME, hoạt động gần như tốt như cơ chế tự chú ý. Cụ thể, về các siêu tham số và cài đặt nói trên, HE được đề xuất vượt trội hơn tự chú ý 32-đầu. Ngoài ra, hiệu suất của WE và ME gần với các tự chú ý 32-đầu và tự chú ý 1-đầu, tương ứng.

Với mục đích đánh giá của con người, chúng tôi có các mô hình với các lớp con SHE và các lớp con tự chú ý 32-đầu tạo ra một chuỗi văn bản bắt đầu với cùng một prompt đã cho "Once upon a time there was a little princess who" và sử dụng cùng seed ngẫu nhiên. Lấy mẫu Top-p với p được đặt thành 0.6 được sử dụng. Các văn bản được tạo ra được liệt kê trong Bảng 2. Chất lượng của các văn bản được tạo ra nói chung cải thiện khi chi phí huấn luyện giảm.

Mặc dù SHE được đề xuất vượt trội hơn cơ chế tự chú ý dưới các siêu tham số và cài đặt được liệt kê trong Bảng 1, chúng ta có thể tự hỏi liệu kết luận này vẫn đứng vững nếu các siêu tham số chính khác như độ dài của cửa sổ ngữ cảnh và số lớp được thay đổi. Để trả lời câu hỏi này, các mô hình với các độ dài khác nhau của cửa sổ ngữ cảnh và số lớp khác nhau được huấn luyện. Cùng các siêu tham số và cài đặt được sử dụng, ngoại trừ số lớp m được giảm xuống 2, độ dài của cửa sổ ngữ cảnh l được giảm xuống 32, và số batches được giảm xuống 30000, để giảm thời gian huấn luyện. Hình 10 và Hình 11 cho thấy các trung vị của chi phí huấn luyện cho 2000 batches cuối cùng. Chúng ta có thể quan sát thấy rằng, nói chung, độ dài của cửa sổ ngữ cảnh càng lớn, khoảng cách hiệu suất giữa các mô hình với các lớp con SHE được đề xuất và các mô hình với các lớp con tự chú ý 32-đầu càng lớn. Ngoài ra, cả các mô hình với các lớp con Extractor được đề xuất và các mô hình với các lớp con tự chú ý đều hoạt động tốt hơn khi số lớp tăng. Cả hai hình đều chỉ ra hiệu quả của các Extractors được đề xuất.

## 7 Độ phức tạp Tính toán
Cả hiệu suất và độ phức tạp tính toán đều quan trọng. Trong phần này, chúng tôi ước tính độ phức tạp tính toán của các lớp con Extractor được đề xuất và so sánh chúng với lớp con tự chú ý.

Theo các kiến trúc được minh họa trong Hình 2, Hình 3, Hình 5, Hình 4, và Hình 6, cho một chuỗi đầu vào có độ dài l trong quá trình huấn luyện, số phép nhân, phép cộng, phép chia, phép lũy thừa, và tham số có thể huấn luyện của các lớp con Extractor và lớp con tự chú ý được ước tính trong Bảng 3 và Bảng 4. Trong quá trình suy luận, cho một token đầu vào mới ở độ dài chuỗi t, số phép nhân, phép cộng, phép chia, và phép lũy thừa tăng thêm của các lớp con Extractor và lớp con tự chú ý được ước tính trong Bảng 5 và Bảng 6.

Khi thay thế d=128 và l=128, đây là trường hợp được thể hiện trong Bảng 1, vào Bảng 3 và Bảng 4, chúng ta thu được số các phép toán số học tổng và tham số có thể huấn luyện, như được liệt kê trong Bảng 7. Chúng ta có thể thấy rằng cả số phép toán số học tổng và số tham số có thể huấn luyện của lớp con SHE được đề xuất đều lớn hơn nhiều so với lớp con tự chú ý. Điều này giải thích tại sao SHE được đề xuất có khả năng vượt trội hơn cơ chế tự chú ý, như câu nói "bạn nhận được những gì bạn trả tiền". HE được đề xuất vượt trội hơn cơ chế tự chú ý 32-đầu với ít phép toán số học hơn và cùng số tham số có thể huấn luyện. Điều này cho thấy rằng các Extractors được đề xuất thực sự có khả năng vượt trội hơn cơ chế tự chú ý. Ngoài ra, WE được đề xuất vượt trội hơn cơ chế tự chú ý 1-đầu với ít phép toán số học và tham số có thể huấn luyện hơn. Hiệu suất của ME được đề xuất gần với cơ chế tự chú ý 1-đầu, với ít phép toán số học và tham số có thể huấn luyện hơn nhiều (khoảng 1/10 và 1/512, tương ứng).

Trong thực tế, với đủ sức mạnh tính toán, đường dẫn quan trọng của tính toán quan trọng. Lấy giai đoạn suy luận làm ví dụ. Theo Hình 2, đường dẫn quan trọng của lớp con tự chú ý là "phép nhân - tích lũy - phép nhân - tích lũy - phép chia - phép lũy thừa - tích lũy - phép chia - phép nhân - tích lũy - phép nhân - tích lũy", trong khi theo Hình 3 đường dẫn quan trọng của lớp con SHE là "phép nhân - tích lũy - phép nhân - phép nhân - tích lũy". Đường dẫn quan trọng của lớp con SHE ngắn hơn so với lớp con tự chú ý, có nghĩa là lớp con SHE được đề xuất có tiềm năng chạy nhanh hơn lớp con tự chú ý. Hơn nữa, theo Hình 6 đường dẫn quan trọng của lớp con ME là "phép nhân - tích lũy", ngắn hơn nhiều.

## 8 Kết luận
Trong bài báo này, một họ các thành phần thay thế drop-in cho cơ chế tự chú ý hiện có trong Transformer, được gọi là Extractors, được đề xuất và đánh giá. Cụ thể, bốn phiên bản của Extractors, cụ thể là SHE, HE, WE, và ME, được đề xuất làm ví dụ.

Kết quả thực nghiệm cho thấy SHE được đề xuất vượt trội rõ rệt so với cơ chế tự chú ý. Mặc dù SHE yêu cầu nhiều tham số có thể huấn luyện và tính toán hơn, nó có đường dẫn quan trọng tính toán ngắn hơn và do đó có tiềm năng chạy nhanh hơn, cung cấp một cách để tăng cường đáng kể hiệu suất của Transformer. HE và WE được đề xuất có khả năng vượt trội hơn tự chú ý đa đầu và tự chú ý 1-đầu, tương ứng, với ít phép toán số học và tham số có thể huấn luyện hơn. Chúng (cũng như các biến thể của chúng) là các ứng cử viên lý tưởng để thay thế cơ chế tự chú ý. ME được đề xuất phù hợp cho các tình huống hạn chế tính toán vì nó yêu cầu ít phép toán số học và tham số có thể huấn luyện hơn nhiều trong khi duy trì hiệu suất gần với tự chú ý 1-đầu.

Hơn nữa, bài toán dự đoán chuỗi trong ngữ cảnh tạo văn bản được công thức hóa bằng chuỗi Markov thời gian rời rạc có độ dài biến đổi, và Transformer được xem xét dựa trên hiểu biết của chúng tôi.

Chúng tôi hy vọng công trình này sẽ đóng góp vào việc xây dựng các mô hình lớn dựa trên Transformer mạnh mẽ và hiệu quả về chi phí. Hơn nữa, dự kiến rằng nhiều nghiên cứu hơn sẽ được thực hiện để cải thiện hiệu suất của Transformer hoặc giải quyết bài toán dự đoán chuỗi được lấy cảm hứng từ công trình này.

## Tài liệu tham khảo
[Các tài liệu tham khảo được giữ nguyên như trong bản gốc]
