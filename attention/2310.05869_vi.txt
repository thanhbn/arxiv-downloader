# 2310.05869.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2310.05869.pdf
# Kích thước tệp: 859294 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
HyperAttention: Chú ý Ngữ cảnh Dài trong Thời gian Gần-Tuyến tính
Insu Han
Yale University
insu.han@yale.eduRajesh Jayaram
Google Research
rkjayaram@google.comAmin Karbasi
Yale University, Google Research
amin.karbasi@yale.edu
Vahab Mirrokni
Google Research
mirrokni@google.comDavid P. Woodruff
CMU, Google Research
dwoodruf@cs.cmu.eduAmir Zandieh
Independent Researcher
amir.zed512@gmail.com
Tóm tắt
Chúng tôi trình bày một cơ chế chú ý xấp xỉ có tên "HyperAttention" để giải quyết các thách thức tính toán do độ phức tạp ngày càng tăng của các ngữ cảnh dài được sử dụng trong Mô hình Ngôn ngữ Lớn (LLMs). Nghiên cứu gần đây cho thấy rằng trong trường hợp xấu nhất, thời gian bậc hai là cần thiết trừ khi các phần tử của ma trận chú ý bị giới hạn hoặc ma trận có rank ổn định thấp. Chúng tôi giới thiệu hai tham số đo lường: (1) chuẩn cột lớn nhất trong ma trận chú ý chuẩn hóa, và (2) tỷ lệ chuẩn hàng trong ma trận chú ý chưa chuẩn hóa sau khi phát hiện và loại bỏ các phần tử lớn. Chúng tôi sử dụng những tham số chi tiết này để nắm bắt độ khó của bài toán. Mặc dù có các giới hạn dưới trước đó, chúng tôi có thể đạt được thuật toán lấy mẫu thời gian tuyến tính ngay cả khi ma trận có các phần tử không bị giới hạn hoặc rank ổn định lớn, miễn là các tham số trên là nhỏ. HyperAttention có thiết kế mô-đun dễ dàng tích hợp các triển khai cấp thấp nhanh khác, đặc biệt là FlashAttention. Về mặt thực nghiệm, sử dụng Locality Sensitive Hashing (LSH) để xác định các phần tử lớn, HyperAttention vượt trội hơn các phương pháp hiện có, mang lại cải thiện tốc độ đáng kể so với các giải pháp tiên tiến như FlashAttention. Chúng tôi xác thực hiệu suất thực nghiệm của HyperAttention trên nhiều bộ dữ liệu độ dài ngữ cảnh dài khác nhau. Ví dụ, HyperAttention làm cho thời gian suy luận của ChatGLM2 nhanh hơn 50% với độ dài ngữ cảnh 32k trong khi perplexity tăng từ 5.6 lên 6.3. Với độ dài ngữ cảnh lớn hơn, ví dụ 131k, với che dấu nhân quả, HyperAttention mang lại tăng tốc 5 lần trên một lớp chú ý đơn.

1 Giới thiệu
Transformers [29] đã được áp dụng thành công cho nhiều tác vụ học tập khác nhau trong các lĩnh vực như xử lý ngôn ngữ tự nhiên [13, 30, 3, 26], thị giác máy tính [4, 15], và dự báo chuỗi thời gian [34]. Mặc dù thành công, những mô hình này đối mặt với những hạn chế nghiêm trọng về khả năng mở rộng vì tính toán chính xác ngây thơ của các lớp chú ý của chúng phát sinh độ phức tạp runtime và bộ nhớ bậc hai (theo độ dài chuỗi). Điều này đặt ra một thách thức cơ bản trong việc mở rộng các mô hình transformer để xử lý độ dài ngữ cảnh dài hơn.

1Các nghiên cứu thực nghiệm được thực hiện bởi I. Han và A. Zandieh.
2Mã nguồn có sẵn tại https://github.com/insuhan/hyper-attn .
1arXiv:2310.05869v3  [cs.LG]  1 Dec 2023

--- TRANG 2 ---
Nhiều phương pháp khác nhau đã được khám phá để giải quyết lớp chú ý thời gian bậc hai, với một hướng đáng chú ý tập trung vào việc xấp xỉ các ma trận trung gian trong các lớp chú ý. Các phương pháp để làm điều này bao gồm xấp xỉ bằng ma trận thưa thớt [20, 11, 27, 28, 14, 18], ma trận rank thấp [9, 19], hoặc kết hợp cả hai [8, 32, 7, 12]. Những phương pháp này nhằm cung cấp xấp xỉ nhanh hơn cho các thành phần khác nhau của chú ý, nhưng không có phương pháp nào trong số chúng cung cấp xấp xỉ đầu cuối cho toàn bộ chú ý tích vô hướng. Hơn nữa, không có nghiên cứu nào trong số này hỗ trợ việc sử dụng che dấu nhân quả, một phần quan trọng của kiến trúc transformer hiện đại. Về mặt tiêu cực, các giới hạn lý thuyết gần đây cho thấy rằng xấp xỉ theo từng phần tử của ma trận chú ý là không thể trong thời gian dưới bậc hai nói chung [1].

Tuy nhiên, một nghiên cứu gần đây, được gọi là KDEFormer [33], đã được chứng minh cung cấp xấp xỉ có thể chứng minh trong thời gian dưới bậc hai, dưới giả định rằng các phần tử của ma trận chú ý bị giới hạn. Về mặt lý thuyết, KDEFormer chạy trong thời gian khoảng ˜O(n1.173); nó sử dụng ước lượng mật độ kernel (KDE) để xấp xỉ chuẩn cột, cho phép tính toán xác suất để lấy mẫu các cột của ma trận chú ý. Tuy nhiên, các thuật toán hiện tại cho KDE thiếu hiệu quả thực tế [6], và ngay cả trong lý thuyết, vẫn có khoảng cách giữa runtime của KDEFormer và các thuật toán thời gian O(n) có thể thực hiện được về mặt lý thuyết. Trong [1], các tác giả đã chứng minh rằng dưới cùng giả định về các phần tử bị giới hạn, một thuật toán thời gian gần tuyến tính O(n1+o(1)) là có thể. Tuy nhiên, thuật toán của họ cũng liên quan đến việc sử dụng phương pháp đa thức để xấp xỉ softmax và có khả năng không thực tế (ví dụ, nó không được đánh giá thực nghiệm bởi các tác giả). Trong nghiên cứu này, chúng tôi cung cấp một thuật toán đạt được điều tốt nhất của cả hai thế giới, vừa là (1) thuật toán hiệu quả thực tế vừa (2) đạt được đảm bảo thời gian gần tuyến tính tốt nhất có thể. Ngoài ra, phương pháp của chúng tôi hỗ trợ che dấu nhân quả, điều mà các nghiên cứu trước đây không thể thực hiện.

1.1 Phát biểu Bài toán
Chú ý tích vô hướng [29] liên quan đến việc xử lý ba ma trận đầu vào: Q (truy vấn), K (khóa), V (giá trị), tất cả có kích thước n×d, trong đó n là số token trong chuỗi đầu vào và d là chiều của biểu diễn tiềm ẩn. Quá trình này xuất ra:
Att=D−1AV
Ở đây, ma trận A:= exp(QK⊤) được định nghĩa là hàm mũ theo từng phần tử của QK⊤. Ngoài ra, D là một ma trận chéo n×n có được từ tổng các hàng của A, Di,i=∥Ai,:∥1 cho i∈[n]. Trong ngữ cảnh này, ma trận A được gọi là "ma trận chú ý", và D−1A được gọi là "ma trận softmax". Điều quan trọng cần lưu ý là việc tính toán ma trận chú ý A trực tiếp yêu cầu Θ(n2d) phép toán, và lưu trữ nó tiêu tốn Θ(n2) bộ nhớ. Do đó, việc tính toán đơn giản của Att đòi hỏi runtime Ω(n2d) và Ω(n2) bộ nhớ.

Mục tiêu của chúng tôi là xấp xỉ hiệu quả ma trận đầu ra Att trong khi vẫn giữ các tính chất phổ của nó. Chiến lược của chúng tôi liên quan đến việc thiết kế một ước lượng hiệu quả cho ma trận chia tỷ lệ chéo D trong thời gian gần tuyến tính. Ngoài ra, chúng tôi nhằm xấp xỉ nhanh chóng tích ma trận của ma trận softmax D−1A và ma trận giá trị V thông qua lấy mẫu con. Cụ thể hơn, mục tiêu của chúng tôi là tìm một ma trận lấy mẫu S∈Rm×n với số hàng hạn chế m=no(1), cùng với một ma trận chéo eD∈Rn×n, sao cho giới hạn sau trên chuẩn toán tử của sai số được thỏa mãn:

‖Att−eD−1AS⊤·SV‖op≤ε·‖D−1A‖op∥V∥op. (1)

--- TRANG 3 ---
1.2 Những Đóng góp của Chúng tôi
Chúng tôi chỉ ra rằng việc giải quyết hiệu quả thành phần nhân ma trận của bài toán xấp xỉ chú ý trong Eq. (1) có thể đạt được bằng cách định nghĩa ma trận lấy mẫu S dựa trên chuẩn hàng của V. Khía cạnh thách thức hơn nằm ở việc có được một xấp xỉ phổ đáng tin cậy cho ma trận chéo D. Trong một kết quả gần đây, Zandieh và cộng sự [33] hiệu quả tận dụng các bộ giải KDE nhanh để đạt được một xấp xỉ chất lượng cao của D. Tuy nhiên, chúng tôi đơn giản hóa quy trình KDEformer và chứng minh rằng lấy mẫu đều là đủ để đạt được đảm bảo phổ mong muốn, loại bỏ nhu cầu lấy mẫu quan trọng dựa trên mật độ kernel. Sự đơn giản hóa đáng kể này cho phép chúng tôi phát triển một thuật toán thời gian tuyến tính thực tế và có thể chứng minh.

Trái ngược với nghiên cứu trước [1, 33], phương pháp của chúng tôi không cần thiết các phần tử bị giới hạn hoặc rank ổn định bị giới hạn. Hơn nữa, các tham số chi tiết mà chúng tôi giới thiệu để phân tích độ phức tạp thời gian có thể vẫn nhỏ ngay cả khi các phần tử trong ma trận chú ý hoặc rank ổn định lớn.

Công trình của chúng tôi được lấy cảm hứng từ ví dụ khó của Alman & Song [1] để chỉ ra giới hạn dưới thời gian bậc hai. Những ví dụ như vậy có một phần tử lớn được đặt ngẫu nhiên trong mỗi hàng của ma trận chú ý. Thuật toán của chúng tôi có một giai đoạn ban đầu nơi chúng tôi tìm các phần tử lớn của ma trận chú ý theo cách hộp đen, chẳng hạn như bằng cách sử dụng Locality Sensitive Hashing [20], hoặc một CountSketch có thể đã học được áp dụng cho ma trận chú ý [5, 23], hoặc chỉ một mẫu phần tử nặng đã biết [7]. Chúng tôi giả định những quy trình này nhanh, và sau khi loại bỏ các phần tử nặng, hai tham số trong ma trận chú ý kết quả là nhỏ: (1) chuẩn ℓ2 cột lớn nhất, và (2) tỷ lệ chuẩn hàng trong ma trận chú ý chưa chuẩn hóa.

Nghiên cứu trước của Zandieh và cộng sự [33] sử dụng KDE để xác định các cột trong ma trận chú ý có chuẩn lớn và thực hiện tích ma trận xấp xỉ với ma trận giá trị bằng cách lấy mẫu những cột như vậy. Như đã đề cập, việc tìm những cột như vậy yêu cầu ít nhất thời gian O(n1.173). Thay vào đó, chúng tôi quan sát rằng bằng cách thực hiện lấy mẫu một phía từ chuẩn hàng bình phương của V, chúng tôi có thể tránh việc sử dụng KDE và đạt được cùng đảm bảo chuẩn phổ theo rank ổn định. Mặc dù thuật toán của chúng tôi đơn giản và chỉ lấy mẫu theo chuẩn hàng của ma trận giá trị (hoặc thậm chí lấy mẫu đều trong thực tế), thách thức kỹ thuật chính là chúng tôi không biết chuẩn hàng của ma trận chú ý cần thiết để chuẩn hóa nó và tạo ra một phân tích thích hợp của nó. Điều này gợi nhớ đến ví dụ khó thời gian bậc hai của [1] nơi chúng tôi có thể không tìm được một phần tử nặng trong một hàng dễ dàng, và do đó không thể chuẩn hóa theo chuẩn của nó trong ma trận chú ý. Các tham số (1) và (2) ở trên cho phép chúng tôi lập luận rằng các phần tử nặng, nếu chúng tồn tại, không được phân phối theo cách tồi tệ nhất có thể.

Về mặt thực nghiệm, HyperAttention thể hiện cải thiện tốc độ đáng kể, đạt được gia tốc hơn 50× trong lan truyền thuận và ngược cho độ dài chuỗi n= 131k. Khi xử lý che dấu nhân quả, phương pháp vẫn mang lại tăng tốc đáng kể 5×. Hơn nữa, khi phương pháp của chúng tôi được áp dụng cho các LLM đã được huấn luyện trước, ví dụ chatglm2-6b-32k[17] và đánh giá trên các bộ dữ liệu benchmark ngữ cảnh dài, gọi là LongBench [2], nó duy trì mức hiệu suất gần với các mô hình gốc, ngay cả không cần tinh chỉnh. Hơn nữa, chúng tôi điều tra đánh giá cụ thể theo tác vụ và phát hiện các tác vụ tóm tắt và hoàn thành mã mạnh mẽ hơn với các lớp chú ý xấp xỉ so với trả lời câu hỏi.

--- TRANG 4 ---
2 Kiến thức Cơ bản
Chúng tôi sử dụng Hamming sorted LSH, một biến thể của locality-sensitive hashing góc được giới thiệu trong nghiên cứu của Zandieh và cộng sự [33]. Trong biến thể này, các bucket hash được sắp xếp theo thứ tự khoảng cách Hamming của chúng. Biến thể LSH này đặc biệt phù hợp để thiết kế các thuật toán thân thiện với GPU nhằm xác định các phần tử chi phối trong ma trận chú ý A. Trong ngữ cảnh của Hamming sorted LSH, nếu chúng ta để H:Rd→[B] là một hàm hash với B bucket được rút ra từ một họ LSH, thì xác suất va chạm Pr_H[H(q) =H(k)] "khoảng" tỷ lệ với ⟨q, k⟩. Một tính chất rất hữu ích của biến thể LSH này là các bucket của nó được sắp xếp theo cách mà các bucket liền kề hình học có các bucket liên tiếp. Chúng tôi cung cấp định nghĩa sau.

Định nghĩa 1 (Hamming sorted LSH, Định nghĩa 7.3 của [33]). Với số nguyên dương r, tồn tại một hàm LSH H:Rd→[2r], sao cho với bất kỳ x, y∈Rd xác suất va chạm của nó là Pr[H(x) = H(y)] = (1−θ(x,y))/(πr) trong đó θ(x, y) := cos−1(x⊤y/(∥x∥∥y∥)). Hơn nữa, hàm LSH này hash các điểm tương tự vào các bucket liền kề. Cụ thể, xác suất hai điểm kết thúc trong các bucket liền kề được cho bởi Pr [H(x) =H(y)±1 (mod 2r)] = 2θ(x,y)/π · (1−θ(x,y))/(πr−1).

Sử dụng hàm LSH này, như được chứng minh bởi Zandieh và cộng sự [33], chúng ta có thể sắp xếp khóa và truy vấn trong một lớp chú ý sao cho các phần tử lớn được dịch chuyển về phía đường chéo của ma trận chú ý. Sau đó, những phần tử quan trọng này trong ma trận chú ý có thể được nắm bắt bằng cách tính toán các khối có kích thước bằng nhau dọc theo đường chéo. Phương pháp này phù hợp với các mẫu truy cập bộ nhớ khối của phần cứng hiện đại và có thể được song song hóa hiệu quả thông qua batching qua các khối.

3 Thuật toán
Để có được đảm bảo phổ khi xấp xỉ Att, bước đầu tiên của chúng tôi liên quan đến việc tạo ra một xấp xỉ 1 ±ε của các phần tử chéo trong ma trận D. Sau đó, chúng tôi xấp xỉ tích ma trận giữa D−1A và V thông qua lấy mẫu theo chuẩn ℓ2 hàng bình phương của V.

Ước lượng D. Quy trình của chúng tôi để xấp xỉ D bao gồm hai bước. Ban đầu, chúng tôi xác định các phần tử chi phối trong ma trận chú ý sử dụng một thuật toán có nguồn gốc từ Hamming sorted LSH, như được định nghĩa trong Định nghĩa 1. Bước thứ hai xoay quanh việc chọn ngẫu nhiên một tập con nhỏ các khóa K. Chúng tôi sẽ chứng minh rằng dưới một số giả định nhẹ về ma trận A và D, phương pháp đơn giản này cho phép chúng tôi thiết lập các giới hạn phổ trên ma trận ước lượng. Mục tiêu của chúng tôi là tìm một ma trận xấp xỉ đủ chính xác eD thỏa mãn:

‖eD−1−D−1‖A‖op≤ε/2‖D−1A‖op (2)

Giả định của chúng tôi là các chuẩn cột của ma trận softmax thể hiện phân phối tương đối đều. Cụ thể hơn, chúng tôi giả định rằng với bất kỳ i∈[n] tồn tại một số α=no(1) sao cho ‖D−1A·e(i)‖22≤α/n. Đáng chú ý là giả định của chúng tôi tổng quát hơn so với giả định các phần tử đầu vào bị giới hạn được đưa ra trong [1]. Thực tế, nếu giả định của họ đúng, nó ngụ ý rằng

--- TRANG 5 ---
Aq1
q2
...
q9k1k2 . . . k9
⇒PK
PQ
APQ,PKq2
q8
q5
q4
q3
q6
q7
q9
q1k2k8k4k7k3k6k9k1k5
⇒P−1
K
P−1
Q
MH⊙A
Hình 1: Cách sortLSH tìm các phần tử lớn của A: (Trái) Khóa và truy vấn trải qua hashing sử dụng Hamming ordered LSH H(·). (Giữa) Khóa và truy vấn được sắp xếp lại dựa trên bucket hash của chúng. Ma trận chú ý sau khi áp dụng các hoán vị hàng và cột này được ký hiệu là APQ,PK. Các phần tử lớn của APQ,PK tập trung xung quanh các khối chéo. (Phải) hoán vị hàng và cột được đảo ngược trên ma trận chú ý và MH⊙A được làm nổi bật.

Thuật toán 1: sortLSH để định vị các phần tử lớn của A
1:đầu vào : ma trận Q,K∈Rn×d, và kích thước khối b
2:Cho H(·) là một Hamming sorted LSH như trong Định nghĩa 1 và hash các hàng của Q,K
3:Cho PK,PQ∈Sym(n) là các hoán vị thỏa mãn PK(i)<PK(j) nếu H(Ki,:)≤ H(Kj,:) và
PQ(i)<PQ(j) nếu H(Qi,:)≤ H(Qj,:)
4:trả về Ma trận mặt nạ MH∈ {0,1}n×n được định nghĩa là MHi,j=1{⌊PQ(i)/b⌋=⌊PK(j)/b⌋}

‖D−1A·e(i)‖22≤no(1)/n cho tất cả i∈[n]. Trong Phần 4.3, chúng tôi tính toán thực nghiệm α là giá trị lớn nhất của chuẩn ℓ2 bình phương của các cột trong D−1A và xác minh rằng nó thực sự dưới tuyến tính theo n.

Bước đầu tiên của thuật toán thực nghiệm của chúng tôi liên quan đến việc xác định các phần tử lớn của ma trận chú ý A thông qua hashing khóa và truy vấn vào các bucket có kích thước đều sử dụng Hamming sorted LSH, mà chúng tôi gọi là sortLSH. Quá trình này được mô tả chi tiết trong Thuật toán 1 và được minh họa trực quan trong Hình 1. Lưu ý rằng chúng tôi cũng đề cập đến các cách khác để xác định các mẫu lớn, chẳng hạn như kiểm tra mẫu heavy hitter đã biết, hoặc sử dụng CountSketch mà chúng tôi mô tả thêm bên dưới.

Thuật toán 1 trả về một mặt nạ thưa thớt được thiết kế để cô lập các phần tử chi phối của ma trận chú ý. Với mặt nạ này, chúng tôi tính toán một xấp xỉ của ma trận D trong Thuật toán 2 thỏa mãn đảm bảo phổ trong Eq. (2). Thuật toán này hoàn thành điều này bằng cách kết hợp các giá trị chú ý tương ứng với mặt nạ với một tập con được chọn ngẫu nhiên của các cột từ ma trận chú ý. Các giả định của Bổ đề 1 được sử dụng để đảm bảo rằng phương sai của ước lượng nhỏ, và cùng độ phức tạp của thuật toán tăng như một hàm của các tham số α, κ. Chúng tôi nhận xét rằng thuật toán của chúng tôi linh hoạt và có thể hoạt động hiệu quả với một mặt nạ được xác định trước chỉ định vị trí của các phần tử chi phối trong ma trận chú ý, phản ánh phương pháp được thực hiện trong [7]. Đảm bảo chính được cung cấp bởi thuật toán này được đưa ra trong Bổ đề 1.

Bổ đề 1 (Xấp xỉ D). Với bất kỳ Q,K∈Rn×d, cho A= exp(QK⊤). Cũng cho D∈Rn×n là ma trận chéo với Di,i=∥Ai,:∥1. Ngoài ra, giả sử rằng α=n·maxi∈[n]‖D−1A·e(i)‖22. Với bất kỳ ma trận mặt nạ MH∈ {0,1}n×n chúng ta định nghĩa số điều kiện κ:=maxi∈[n]⟨1−MHi,:,Ai,:⟩/minj∈[n]⟨1−MHj,:,Aj,:⟩. Nếu m= Ω(κ7·α2/(ε6logn)), đầu ra eD của Thuật toán 2 thỏa mãn Eq. (2) với xác suất ít nhất 1−1/poly(n).

--- TRANG 6 ---
Thuật toán 2: ApproxD để ước lượng ma trận chéo D
1:đầu vào : ma trận Q,K∈Rn×d, mặt nạ phần tử lớn MH∈ {0,1}n×n, tham số κ >0,ε >1/κ4,
α > ε2κ, và số nguyên m
2:Chọn ngẫu nhiên một tập con T⊆[n] với cardinality |T|=m
3:τ←maxj∈T⟨1−MHj,:,exp(KQ⊤j,:)⟩ // ước lượng tổng hàng không mặt nạ lớn nhất
4:Tạo m mẫu i.i.d. ℓ1, . . . ℓm∼Unif([n])
5:cho i∈[n] làm
6:Ci←Θ(ε2m/(nlogn))⟨MHi,:,exp(KQ⊤i,:)⟩+τ/κ // tổng hàng của phần tử được mặt nạ
7:di←(n/m)∑j∈[m](1−MHi,ℓj) min(exp(⟨Qi,:,Kℓj,:⟩), Ci) // tổng hàng của phần tử không mặt nạ
8: ˜di← ⟨MHi,:,exp(KQ⊤i,:)⟩+ max(di,τ/κ) // ước lượng tổng hàng đầy đủ
9:trả về ma trận chéo eD= diag({˜di}ni=1)

Thuật toán 3: HyperAttention: cơ chế chú ý trong thời gian gần-tuyến tính
1:đầu vào : ma trận Q,K,V∈Rn×d, ma trận mặt nạ MH∈ {0,1}n×n, và tham số ε >1/no(1)
2:Chạy Thuật toán 2 và cho eD←ApproxD(Q,K,MH, no(1), ε, no(1), d·no(1))
3:Cho S∈Rm×n là ma trận lấy mẫu i.i.d. dựa trên chuẩn hàng bình phương của V như trong Bổ đề 2
4:trả về eD và S

Xấp xỉ tích của ma trận softmax D−1A và ma trận giá trị V. Với eD thỏa mãn các điều kiện xấp xỉ phổ như trong Eq. (2), chúng ta có thể đạt được ràng buộc phổ trong Eq. (1), bằng cách tìm một ma trận lấy mẫu thỏa mãn điều kiện sau,

‖eD−1AS⊤·SV−eD−1AV‖op≤ε/2·‖D−1A‖op∥V∥op (3)

Chúng ta có thể hiệu quả tìm một ma trận lấy mẫu S∈Rm×n với số hàng m nhỏ thỏa mãn Eq. (3) bằng cách tận dụng các kỹ thuật được thiết lập tốt trong Approximate Matrix Multiplication (AMM).

Bổ đề 2. Với bất kỳ ma trận eD,A∈Rn×n,V∈Rn×d xem xét một ma trận lấy mẫu S∈Rm×n được xây dựng như sau: đầu tiên tạo m mẫu i.i.d. ℓ1, . . . ℓm∈[n] theo chuẩn hàng bình phương của ma trận V, tức là, ∥Vi,:∥22/∥V∥2F, sau đó cho hàng thứ r của S là ∥V∥F/(√m·∥Vℓr,:∥2)·e(ℓr). Nếu m= Ω(ε−2d·srank(eD−1A)) với một số ε >0, điều sau đúng với xác suất ít nhất 0.99:

‖eD−1AS⊤·SV−eD−1AV‖op≤ε·‖eD−1A‖op∥V∥op.

Kết quả trên là tiêu chuẩn và để chứng minh tham khảo [16].

Định lý Chính. Bây giờ, chúng ta có thể tích hợp các quy trình con để xấp xỉ đường chéo eD và xấp xỉ tích ma trận giữa eD−1A và ma trận giá trị V. Với điều này, chúng tôi giới thiệu HyperAttention, một thuật toán hiệu quả có thể xấp xỉ cơ chế chú ý với đảm bảo phổ như trong Eq. (1) trong thời gian gần tuyến tính. Thuật toán 3 của chúng tôi nhận đầu vào là một mặt nạ

--- TRANG 7 ---
MH định nghĩa vị trí của các phần tử chi phối trong ma trận chú ý. Mặt nạ này có thể được tạo ra sử dụng thuật toán sortLSH (Thuật toán 1), hoặc nó có thể là một mặt nạ được xác định trước tương tự như phương pháp được thực hiện trong [7]. Mặt nạ phần tử lớn MH được giả định là thưa thớt theo thiết kế và số phần tử khác không của nó bị giới hạn nnz(MH) =n1+o(1). Bây giờ chúng tôi giới thiệu định lý chính sẽ được chứng minh trong Phụ lục A.

Định lý 1 (Đảm bảo HyperAttention). Với bất kỳ ma trận Q,K,V∈Rn×d, bất kỳ ma trận mặt nạ MH∈ {0,1}n×n, và tham số ε >1/no(1), cho A= exp(QK⊤) và cho D∈Rn×n là ma trận chéo với Di,i=∥Ai,:∥1. Nếu maxi∈[n]‖D−1A·e(i)‖22≤no(1)/n và maxi∈[n]⟨1−MHi,:,Ai,:⟩/minj∈[n]⟨1−MHj,:,Aj,:⟩≤no(1) thì với xác suất ít nhất 0.98 các đầu ra S,eD của Thuật toán 3 thỏa mãn điều kiện phổ như trong Eq.(1). Hơn nữa, runtime của thuật toán này là O(d·n1+o(1)+d·nnz(MH)).

Lưu ý rằng ngay cả khi MH không được cho trước, nhưng MH có thể được tìm trong thời gian d·n1+o(1), định lý vẫn đúng. Chúng tôi cũng đưa ra ví dụ khi điều này có thể bằng cách sử dụng Hamming sorted LSH, mà các thí nghiệm của chúng tôi dựa trên, hoặc sử dụng ExpanderSketch của [21] dựa trên CountSketch [5] nhưng cũng cho thời gian khôi phục nhanh. Trong phần bổ sung chúng tôi chỉ ra:

Hệ quả 1 (HyperAttention với sortLSH). Giả sử tất cả các điều kiện tiên quyết của Định lý 1 đúng, trong đó ma trận mặt nạ MH được định nghĩa như sau. Giả sử MH∈ {0,1}n×n được tạo ra như trong Thuật toán 1 với kích thước khối b=no(1) và r= log2n trong Định nghĩa 1. Chúng ta thêm giả định có nhiều nhất n1+o(1) cặp (i, j) với θ(Qi,∗,Kj,∗)≤π/2(1−o(1)), trong đó θ như trong Định nghĩa 1. Thì với xác suất 1−1/no(1), MH chúng ta tìm trong Thuật toán 1 có nhiều nhất n1+o(1) phần tử khác không và với xác suất ít nhất .98, các đầu ra S,eD của Thuật toán 3 thỏa mãn Eq. (1) và tổng runtime là O(d·n1+o(1)).

Chúng tôi lưu ý giả định về góc của các hàng của Q và K trong Hệ quả 1 được thỏa mãn nếu hầu hết các hàng được rút ngẫu nhiên từ một mặt cầu d chiều, vì trong trường hợp này chúng sẽ gần như trực giao, tức là có góc nhiều nhất π/2(1−o(1)) với xác suất cao. Tuy nhiên, hệ quả cũng cho phép n1+o(1) cặp hàng có góc tùy ý, điều này có thể thực tế hơn.

Hệ quả 2 (HyperAttention với ExpanderSketch). Giả sử tất cả các điều kiện tiên quyết của Định lý 1 đúng, trong đó ma trận mặt nạ MH được định nghĩa như sau. Giả sử MH∈ {0,1}n×n được định nghĩa sao cho tồn tại một ngưỡng τ=no(1) sao cho MHi,j= 1 khi và chỉ khi (QK⊤)2i,j≥∥QK⊤ej∥22/τ. Thì chúng ta có thể tìm MH chính xác với xác suất 1−O(1/n2), và với xác suất ít nhất .98, các đầu ra S,eD của Thuật toán 3 thỏa mãn Eq. (1). Runtime là O(d·n1+o(1)).

Ý tưởng chính đằng sau chứng minh Hệ quả 2 là đầu tiên phác thảo Q bằng một ExpanderSketch T, điều này hiệu quả vì T có số hàng nhỏ. Sau đó tính toán (T·Q)·K⊤ điều này lại hiệu quả vì (T·Q) có số hàng nhỏ. Do đó, chúng ta không bao giờ tạo ra ma trận Q·K⊤.

3.1 Che dấu Nhân quả
Các mô hình ngôn ngữ thường sử dụng che dấu nhân quả. Mặt nạ nhân quả là một ma trận vuông nhị phân tam giác dưới được ký hiệu là MC trong đó MCi,j=1{i≥j}. Cơ chế chú ý nhân quả được định nghĩa là:
AttC=D−1C(MC⊙A)V,

--- TRANG 8 ---
MC⊙A⇒ApproxD→CausalApproxD→A21MC1⊙A11MC2⊙A22(gọi đệ quy)Hình 2: Ma trận chú ý nhân quả có thể được chia thành ba phần khác không có kích thước bằng nhau: MC1⊙A11 và MC2⊙A22 đều là ma trận chú ý nhân quả, và A21 là một ma trận chú ý không mặt nạ.

Thuật toán 4: CausalApproxD, xấp xỉ đệ quy của DC cho che dấu nhân quả
1:đầu vào : ma trận Q,K∈Rn×d
2:Chia Q và K thành các ma trận con có kích thước bằng nhau: Q= [Q⊤1,Q⊤2]⊤ và K= [K⊤1,K⊤2]⊤
3:eDC11←CausalApproxD(Q1,K1) và eDC22←CausalApproxD(Q2,K2)
4:Chạy thuật toán không mặt nạ ApproxD (Thuật toán 2) trên Q2,K1 để được eD21
5:trả về eDC=["eDC11, 0"/"0,eD21+eDC22"]

trong đó A:= exp(QK⊤) được định nghĩa như trước và DC là một ma trận chéo n×n có được từ tổng các hàng của chú ý được mặt nạ MC⊙A, cụ thể [DC]i,i=⟨MCi,:,Ai,:⟩ cho i∈[n]. Để xấp xỉ chú ý nhân quả với đảm bảo phổ, chúng ta cần hai thành phần. Đầu tiên, chúng ta cần một xấp xỉ phổ cho ma trận chéo DC. Thứ hai, chúng ta cần xấp xỉ tích ma trận giữa D−1C(MC⊙A) và V, điều này có thể đạt được sử dụng cùng kỹ thuật lấy mẫu như được mô tả trong Thuật toán 3 và Bổ đề 2. Thành phần đầu tiên phức tạp hơn, và chúng tôi sử dụng một phương pháp đệ quy để giải quyết nó. Vì vậy chúng tôi tập trung vào cách xấp xỉ hiệu quả đường chéo DC.

Phương pháp của chúng tôi dựa trên một quan sát chính, như được mô tả trong Hình 2. Chú ý được mặt nạ MC⊙A có thể được phân tích thành ba ma trận khác không, mỗi ma trận có kích thước bằng một nửa ma trận chú ý gốc. Khối A21, nằm hoàn toàn dưới đường chéo là chú ý không mặt nạ. Do đó, chúng ta có thể xấp xỉ tổng hàng của nó sử dụng Thuật toán 2. Hai khối chéo MC1⊙A11 và MC2⊙A22 được hiển thị trong Hình 2 là chú ý nhân quả với kích thước bằng một nửa kích thước gốc. Để xử lý chúng, chúng ta áp dụng một phương pháp đệ quy và tiếp tục phân chia chúng thành các khối nhỏ hơn, và lặp lại quy trình này. Chúng tôi trình bày mã giả cho quy trình này trong Thuật toán 4.

4 Thí nghiệm
Trong phần này, chúng tôi đánh giá thuật toán của chúng tôi bằng cách mở rộng các mô hình ngôn ngữ lớn hiện có để xử lý các chuỗi tầm xa. Tất cả các thí nghiệm được thực hiện trên một GPU A100 với bộ nhớ 40 GB và chúng tôi sử dụng FlashAttention 2 [10] cho tính toán chú ý chính xác.

--- TRANG 9 ---
1.01.52.0
tăng tốc
0481216202428
số lớp thay thế4681012perplexity(a)chatglm2-6b-32k
1.01.21.4
tăng tốc
0 4 812 16 20 24
số lớp thay thế363738394041perplexity (b)phi-1.5

Hình 3: Perplexity và tăng tốc của chatglm2-6b-32k(trái) và phi-1.5(phải) được monkey patch với HyperAttention. Chúng tôi thay đổi số lớp chú ý được thay thế theo thứ tự cuối cùng.

Chi tiết Triển khai. Chúng tôi triển khai HyperAttention dựa trên sortLSH và lấy mẫu cột đều. Cụ thể, chúng tôi đầu tiên áp dụng sortLSH cho tất cả các hàng trong Q,V∈Rn×d. Sau đó, mỗi tập hàng được phân chia thành b nhóm trong đó b là kích thước khối như trong Hình 1. Tập hàng thứ i trong Q được nhân với tập tương ứng trong K, dẫn đến một xấp xỉ khối-chéo của APQ,PK. Tiếp theo, chúng tôi tối ưu hóa Thuật toán 2 để xấp xỉ D bằng cách chia sẻ các chỉ số ngẫu nhiên {ℓj}mj=1 với tất cả các hàng trong Q. Điều này tương ứng với việc lấy mẫu đều m hàng trong V. Để đơn giản hóa thêm, chúng tôi tái sử dụng các chỉ số {ℓj}mj=1 cho Approximate Matrix Multiplication (AMM) trong Bổ đề 2. Các phép toán cần thiết liên quan đến việc hoán vị n hàng, reshape tensor, và nhân ma trận nhỏ. Vì mọi batch, head và khối có cùng cấu hình, việc triển khai có thể được song song hóa sử dụng GPU.

4.1 Monkey Patching Self-attention
Chúng tôi đầu tiên đánh giá HyperAttention trên hai LLM đã được huấn luyện trước. Chúng tôi chọn ba mô hình với kiến trúc khác nhau được sử dụng rộng rãi trong các ứng dụng thực tế: chatglm2-6b-32k[17], và phi-1.5[24]. Chúng tôi patch ℓ lớp chú ý cuối cùng của chúng bằng cách thay thế bằng HyperAttentions trong đó ℓ có thể thay đổi từ 0 đến số tất cả lớp chú ý trong mỗi LLM. Lưu ý rằng chú ý trong cả hai mô hình yêu cầu che dấu nhân quả và chúng tôi sử dụng Thuật toán 4 bằng cách áp dụng đệ quy cho đến khi độ dài chuỗi đầu vào n nhỏ hơn 4,096. Chúng tôi đặt cả kích thước bucket b và số cột được lấy mẫu m là 256 cho tất cả độ dài chuỗi. Chúng tôi đánh giá hiệu suất của các mô hình được monkey patch như vậy về perplexity và tăng tốc.

Chúng tôi sử dụng LongBench [2], một bộ sưu tập các bộ dữ liệu benchmark ngữ cảnh dài, chứa 6 tác vụ khác nhau từ trả lời câu hỏi đơn và đa tài liệu, tóm tắt, học few-shot, tác vụ tổng hợp, và hoàn thành mã. Chúng tôi chọn một tập con của bộ dữ liệu có độ dài chuỗi mã hóa lớn hơn 32,768 và cắt chúng nếu độ dài vượt quá 32,768 để tất cả dữ liệu có độ dài chuỗi 32,768. Sau đó, chúng tôi tính toán perplexity (tức là, loss trên dự đoán token tiếp theo) của mỗi mô hình. Để làm nổi bật khả năng mở rộng trên các chuỗi dài, chúng tôi tính toán tổng tăng tốc trên tất cả các lớp chú ý dù được thực hiện bởi HyperAttention hay FlashAttention.

Kết quả được tóm tắt trong Hình 3. Quan sát rằng chatglm2-6b-32k cho thấy perplexity hợp lý ngay cả sau khi được monkey patch bởi HyperAttention, ví dụ, sau khi thay thế 20 lớp perplexity tăng

--- TRANG 10 ---
Số Lớp
Thay thếTác vụ
single-qa multi-qa summarization few-shot synthetic code
0 (chính xác) 80.63 68.14 53.12 186.58 84.00 99.57
7 72.34 65.37 52.54 182.12 82.00 102.06
14 71.32 62.97 52.13 169.02 80.00 92.12
21 44.75 48.91 50.90 150.44 31.00 82.58
28 27.07 22.65 46.28 65.74 8.33 73.68

Bảng 1: Đánh giá hiệu suất của chatglm2-6b-32k trang bị HyperAttentions trên các bộ dữ liệu LongBench [2]. Chúng chứa 6 tác vụ khác nhau và chúng tôi đánh giá từng tác vụ với metric riêng của nó trong đó giá trị cao hơn chỉ ra hiệu suất tốt hơn.

khoảng 1 và nó từ từ tăng lên cho đến 24 lớp. Nhưng nó cải thiện runtime trong các lớp chú ý khoảng 50%. Nếu tất cả các lớp được thay thế thì perplexity tăng lên 12 nhưng nó chạy khoảng 2.3× nhanh hơn. Đối với phi-1.5, điều tương tự xảy ra nhưng perplexity tăng tuyến tính khi số HyperAttention tăng.

Ngoài ra, chúng tôi đánh giá hiệu suất của chatglm2-6b-32k được monkey patch trên các bộ dữ liệu LongBench và tính toán điểm đánh giá cụ thể theo tác vụ trên mỗi tác vụ bao gồm trả lời câu hỏi đơn tài liệu, trả lời câu hỏi đa tài liệu, tóm tắt, học few-shot, tác vụ tổng hợp và hoàn thành mã. Kết quả được cung cấp trong Bảng 1. Trong khi việc thay thế HyperAttention nói chung dẫn đến suy giảm hiệu suất, chúng tôi quan sát rằng vai trò của nó có thể thay đổi tùy thuộc vào tác vụ. Ví dụ, tóm tắt và hoàn thành mã mạnh mẽ hơn so với các tác vụ khác. Đáng chú ý, khi một nửa tất cả các lớp chú ý được patch (tức là, 14 lớp), chúng tôi xác minh rằng hầu hết các tác vụ không suy giảm hơn 13%. Đặc biệt, hiệu suất của tác vụ tóm tắt gần như không thay đổi, cho thấy tác vụ này có thể mạnh mẽ hơn với các thay đổi một phần trong cơ chế chú ý. Chúng tôi nhớ lại rằng tính toán trong các lớp chú ý có thể nhanh hơn 1.5× khi n= 32k.

4.2 Lớp Self Attention Đơn
Chúng tôi tiếp tục khám phá tăng tốc của HyperAttention với độ dài chuỗi thay đổi từ 4,096 đến 131,072. Chúng tôi đo thời gian wall-clock của cả phép toán thuận và thuận+ngược khi chúng được tính toán với FlashAttention hoặc được tăng tốc bởi HyperAttention. Chúng tôi đo thời gian có và không có che dấu nhân quả. Tất cả đầu vào Q,K,V có cùng độ dài và chiều của chúng được cố định ở d= 64 và số head chú ý được đặt bởi 12. Chúng tôi chọn cùng tham số trong HyperAttention như được mô tả trong phần trước. Trong Hình 4, chúng tôi quan sát rằng HyperAttention chạy nhanh hơn tới 54× không có che dấu nhân quả và 5.4× khi áp dụng che dấu nhân quả. Mặc dù độ phức tạp thời gian của cả che dấu nhân quả và không che dấu đều giống nhau, một thuật toán thực tế cho che dấu nhân quả (Thuật toán 1) yêu cầu các phép toán bổ sung như phân chia Q,K,V, và hợp nhất đầu ra chú ý dẫn đến tăng runtime thực tế. Tuy nhiên, những tăng tốc đó sẽ tăng khi độ dài chuỗi n tăng. Chúng tôi tin rằng điều này mở ra cơ hội mở rộng self-attention không chỉ cho suy luận mà còn cho huấn luyện hoặc tinh chỉnh các LLM để phù hợp với các chuỗi dài đáng kể.

--- TRANG 11 ---
4k 8k 16k 32k 65k 131k
độ dài chuỗi n01020304050tăng tốc
Thuận
Thuận+Ngược(a) Không có che dấu nhân quả
4k 8k 16k 32k 65k 131k
độ dài chuỗi n012345tăng tốc
Thuận
Thuận+Ngược (b) Có che dấu nhân quả

Hình 4: Tăng tốc của tính toán chính xác sử dụng FlashAttention [10] và HyperAttention (nghiên cứu này) trong lớp self-attention đơn trong các phép toán thuận và ngược. Với n=131k, HyperAttention chạy nhanh hơn tới 54× không có che dấu nhân quả và 5.4× có che dấu nhân quả.

4.3 Xác minh Thực nghiệm Giả định
Ngoài ra, chúng tôi xác minh thực nghiệm giả định của chúng tôi trong Định lý 1, tức là, chuẩn ℓ2 bình phương của các cột trong D−1A bị giới hạn trên bởi no(1)/n. Chúng tôi điều tra các transformer đã được huấn luyện trước có và không có che dấu nhân quả. Đối với chú ý không che dấu nhân quả, chúng tôi sử dụng mô hình T2T-ViT [31] và lấy Q,K,V từ lớp chú ý đầu tiên của nó trên bộ dữ liệu test ImageNet vì nó là nút thắt cổ chai tính toán chính. Với mỗi hình ảnh, chúng tôi tính toán α là giá trị lớn nhất của chuẩn ℓ2 bình phương của các cột trong ‖D−1A·ei‖22 và thu thập giá trị trên 50k hình ảnh. Độ dài chuỗi của mô hình được cho bởi n= 3,136 và giá trị trung bình của α được quan sát là 8.1801. Điều này nhỏ hơn nhiều so với n và có thể dưới tuyến tính theo n.

Để điều tra thêm sự phụ thuộc vào n, chúng tôi sử dụng chatglm2-6b-32k và bộ dữ liệu narrative-qa LongBench, thay đổi độ dài chuỗi n từ 1k đến 9k. Chúng tôi cắt hoặc đệm ngữ cảnh đầu vào để độ dài của nó chính xác là n. Không giống như mô hình thị giác, chúng tôi nhận thấy rằng các cột đầu tiên trong D−1A thường chứa các phần tử nặng; do đó chúng tôi tính toán α là chuẩn bình phương lớn nhất loại trừ 32 cột đầu tiên. Chúng tôi thu thập những giá trị này cho tất cả head và lớp và tính toán trung bình của chúng. Hình 5 vẽ giá trị của α/n với độ dài chuỗi n khác nhau. Quan sát thấy rằng giá trị của α/n giảm khi n tăng, hỗ trợ khẳng định rằng giả định α=no(1) của chúng tôi đúng trong thực tế.

5 Kết luận
Trong nghiên cứu này, chúng tôi đề xuất một thuật toán xấp xỉ chú ý thời gian tuyến tính đơn giản bằng cách đơn giản hóa thuật toán hiện có dựa trên ước lượng mật độ kernel (KDE). Chúng tôi giới thiệu một tham số hóa tổng quát hơn cho đảm bảo xấp xỉ phổ dựa trên số điều kiện, không yêu cầu các giả định được sử dụng trong nghiên cứu trước. Thuật toán của chúng tôi sử dụng sortLSH để tìm các phần tử lớn và chúng tôi áp dụng nhân ma trận nhanh thông qua lấy mẫu chuẩn hàng. Chúng tôi nghiên cứu thêm cách thuật toán của chúng tôi được sử dụng cho che dấu nhân quả bằng phân chia đệ quy. Về mặt thực nghiệm, chúng tôi minh họa rằng các LLM đã được huấn luyện trước sử dụng thuật toán của chúng tôi có thể tăng cường tốc độ suy luận và huấn luyện với chỉ suy giảm hiệu suất tối thiểu.

--- TRANG 12 ---
1k2k3k4k5k6k7k8k9k
độ dài chuỗi n0.040.060.08(1/n) maxi‖D−1A·ei‖22

Hình 5: Ước lượng thực nghiệm về α(tức là, chuẩn ℓ2 bình phương lớn nhất của các cột trong D−1A) với n khác nhau. Trong Định lý 1, chúng tôi giả định rằng α=no(1) và biểu đồ hỗ trợ khẳng định rằng giả định của chúng tôi đúng trong thực tế.

Tài liệu tham khảo
[1] Josh Alman và Zhao Song. Fast attention requires bounded entries. arXiv preprint arXiv:2302.13214, 2023.

[2] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, và Juanzi Li. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding, 2023.

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS), 2020.

[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the European Conference on Computer Vision(ECCV), 2020.

[5] Moses Charikar, Kevin Chen, và Martin Farach-Colton. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming (ICAMP), 2002.

[6] Moses Charikar, Michael Kapralov, Navid Nouri, và Paris Siminelakis. Kernel density estimation through density constrained near neighbor search. In Foundations of Computer Science (FOCS), 2020.

[7] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, và Christopher Re. Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models. In International Conference on Learning Representations (ICLR), 2021.

[8] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, và Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. Neural Information Processing Systems (NeurIPS), 2021.

--- TRANG 13 ---
[9] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking Attention with Performers. In International Conference on Learning Representations (ICLR), 2021.

[10] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. 2023.

[11] Giannis Daras, Nikita Kitaev, Augustus Odena, và Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. Neural Information Processing Systems (NeurIPS), 2020.

[12] Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng Wang, và Yingyan Lin. Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention. arXiv preprint arXiv:2211.05109, 2022.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Association for Computational Linguistics (NAACL), 2018.

[14] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, và Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023.

[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR), 2021.

[16] Petros Drineas và Ravi Kannan. Fast monte-carlo algorithms for approximate matrix multiplication. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science, pp. 452–459. IEEE, 2001.

[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. 2022.

[18] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, và Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.

[19] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning (ICML), 2020.

[20] Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The Efficient Transformer. In International Conference on Learning Representations (ICLR), 2020.

[21] Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen, và Mikkel Thorup. Heavy hitters via cluster-preserving clustering. CoRR, abs/1604.01357, 2016.

--- TRANG 14 ---
[22] François Le Gall. Faster algorithms for rectangular matrix multiplication. In 2012 IEEE 53rd annual symposium on foundations of computer science, pp. 514–523. IEEE, 2012.

[23] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, và David P. Woodruff. Learning the positions in countsketch. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.

[24] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, và Yin Tat Lee. Textbooks Are All You Need II: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.

[25] Colin McDiarmid. Concentration. In Probabilistic methods for algorithmic discrete mathematics, pp. 195–248. Springer, 1998.

[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR), 2020.

[27] Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics (ACL), 2021.

[28] Zhiqing Sun, Yiming Yang, và Shinjae Yoo. Sparse Attention with Learning to Hash. In International Conference on Learning Representations (ICLR), 2021.

[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NeurIPS), 2017.

[30] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Neural Information Processing Systems (NeurIPS), 2019.

[31] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, và Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In International Conference on Computer Vision (ICCV), 2021.

[32] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. In Neural Information Processing Systems (NeurIPS), 2020.

[33] Amir Zandieh, Insu Han, Majid Daliri, và Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In International Conference on Machine Learning (ICML), 2023.

[34] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, và Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Conference on Artificial Intelligence (AAAI), 2021.

--- TRANG 15 ---
A Chứng minh bị bỏ qua
Ở đây chúng tôi bao gồm các chứng minh đã bị bỏ qua trong phần chính của bài báo. Đầu tiên, chúng tôi trình bày chứng minh của Bổ đề 1.

Chứng minh Bổ đề 1: Đầu tiên, chúng tôi chỉ ra rằng τ được tính toán trong dòng 3 của Thuật toán 2 gần với tổng hàng lớn nhất của ma trận (1n−MH)⊙A. Dễ dàng kiểm tra rằng τ/κ≤⟨1−MHi,:,exp(KQ⊤i,:)⟩≤τκ cho tất cả i∈[n] vì định nghĩa của κ trong phát biểu bổ đề. Hơn nữa, nếu chúng ta định nghĩa tập hợp:

S0:={i∈[n] :⟨1−MHi,:,exp(KQ⊤i,:)⟩> τ}, (4)

thì chúng ta có thể chỉ ra rằng |S0| nhỏ. Nhớ lại rằng τ là giá trị lớn nhất của tổng hàng của một tập con ngẫu nhiên của các hàng, ký hiệu bởi T trong đó |T|=m. Do đó, Pr[|S0| ≥t]≤(1−t/n)m. Chọn m= Ω(κ7α2ε−6logn) và t=O(κ−7α−2ε6n) cho rằng với xác suất ít nhất 1 −1/poly(n)

|S0| ≤O(κ−7·α−2·ε6·n) (5)

Tiếp theo, chúng ta định nghĩa phiên bản bị giới hạn trên của ma trận A trong đó các phần tử của hàng thứ i trên các vị trí mà giá trị mặt nạ MH bằng không được giới hạn ở giá trị Ci (dòng 6 của thuật toán) như:

eA∈Rn×n:eAi,j:={min (Ai,j, Ci) nếu MHi,j= 0, Ai,j nếu ngược lại,cho mọi i, j∈[n].

Chúng ta tiếp tục bằng cách giới hạn tổng khối lượng của các phần tử lớn của ma trận D−1A bị mất qua việc giới hạn (tức là, các phần tử của A lớn hơn ngưỡng Ci). Nếu chúng ta định nghĩa hằng số bC:=ε2m/(κ2nlogn), chúng ta có thể viết,

‖D−1(A−eA)‖1=∑i,j∈[n](1−MHi,j)·(Ai,j−min (Ai,j, Ci))/Di,i
=∑∞t=0∑i,j∈[n](2t−1)bCDi,i<Ai,j−Ci≤(2t+1−1)bCDi,i1{MHi,j=0}·(Ai,j−min (Ai,j, Ci))/Di,i
≤∑log2(κ2/bC)t=02t+1bC·|{i, j∈[n] :MHi,j= 0 &Ai,j> Ci+ (2t−1)bCDi,i}|
≤∑log2(κ2/bC)t=02t+1bC·α/(2tbC)2=O(ε4/(κ5·α·n)) (6)

Bất đẳng thức trong Eq. (6) theo sau vì, với mọi i∈[n], cardinality của tập hợp {i, j∈[n] :MHi,j= 0∧Ai,j> Ci+ (2t−1)bCDi,i} phải bị giới hạn bởi α/(2tbC)2. Chứng minh của điều này bằng phản chứng vì nếu không, phải có một l∈[n] sao cho cardinality của tập hợp Hl:={i∈[n] :MHi,l= 0∨Ai,l> Ci+ (2t−1)bCDi,i}

--- TRANG 16 ---
là ít nhất |Hl|>α/(n·(2tbC)2). Điều này ngụ ý rằng

‖D−1A·e(l)‖22≥∑i∈Hl(Ai,l/Di,i)2
=∑i∈Hl(Ci/Di,i+ (2t−1)bC)2
≥ |Hl|·(ε2m/(κ2nlogn)+ (2t−1)bC)2
>α/n,

tuy nhiên, điều này mâu thuẫn với điều kiện tiên quyết của bổ đề về α=n·maxi∈[n]‖D−1A·e(i)‖22.

Bây giờ, nếu chúng ta định nghĩa các tập hợp S1,S2⊆[n] như

S1={i∈[n] :εDi,i/3<∥Ai,:−eAi,:∥1≤Di,i/3},S2={i∈[n] :∥Ai,:−eAi,:∥1>Di,i/3} (7)

thì từ Eq. (6) suy ra cardinalities của S1 và S2 bị giới hạn bởi

|S1| ≤O(κ−4·α−1·ε3n),|S2| ≤O(κ−4·α−1·ε4n). (8)

Tiếp theo lưu ý rằng di được tính toán trong dòng 7 của thuật toán là một ước lượng cho chuẩn hàng thứ i của ma trận bị giới hạn và mặt nạ (1n−MH)⊙eA. Chúng ta định nghĩa một ước lượng cho ma trận bị giới hạn không mặt nạ eA là bdi:=di+⟨MHi,:,Ai,:⟩. Bằng cách gọi bất đẳng thức Chernoff-Hoeffding (xem ví dụ [25]) cùng với union bound, vì phát biểu bổ đề giả định rằng m= Ω(κ7·α2/ε6logn) điều sau đúng đồng thời cho tất cả i∈[n]\S2 với xác suất 1 −1/poly(n):

∥eAi,:∥1/(1 +ε/6)≤bdi≤∥eAi,:∥1/(1−ε/6). (9)

Bất đẳng thức này kết hợp với định nghĩa của S1,S2 trong Eq. (7) ngụ ý rằng với bất kỳ i∈[n]\(S1∪S2),
(1−ε/2)·D−1i,i≤bd−1i≤(1 +ε/2)·D−1i,i. Bây giờ chúng ta giới hạn chuẩn toán tử của sai số như sau:

‖(eD−1−D−1)A‖op≤‖(eD−1−D−1)S1∪S2A‖op+‖(eD−1−D−1)[n]\(S1∪S2)A‖op
≤(3/2)‖D−1S1A‖op+‖(eD−1−D−1)S2A‖op+(ε/2)‖D−1[n]\(S1∪S2)A‖op
≤(3/2)‖D−1S1A‖op+κ2‖D−1S2∩S0A‖op+κ‖D−1S2\S0A‖op+(ε/2)‖D−1[n]\(S1∪S2)A‖op
≤(3/2)‖D−1S1A‖op+κ2‖D−1S0A‖op+κ‖D−1S2A‖op+(ε/2)‖D−1[n]\(S1∪S2)A‖op, (10)

trong đó bất đẳng thức thứ hai ở trên theo sau từ bất đẳng thức trong Eq. (9) và cũng vì định nghĩa của S1 đảm bảo rằng eDi,i≥(1−1/3)·Di,i với bất kỳ i∈S1. Bất đẳng thức thứ ba ở trên theo sau vì việc giới hạn dưới trong dòng 8 của thuật toán đảm bảo rằng eDj,j≥ ⟨MHj,:,Aj,:⟩+τ/κ với bất kỳ j∈S2 trong khi Dj,j≤ ⟨MHj,:,Aj,:⟩+τ với bất kỳ j /∈S0 theo định nghĩa của tập hợp S0 và chúng ta biết rằng Dj,j≤ ⟨MHj,:,Aj,:⟩+τκ cho j∈S0.

Cuối cùng chúng ta kết luận chứng minh bằng cách giới hạn các số hạng ‖D−1SrA‖op cho r∈ {0,1,2} trong Eq. (10). Cố định một số r∈ {0,1,2}. Cho v là vector có chuẩn đơn vị thực hiện chuẩn toán tử của D−1SrA. Vì D−1SrA là một ma trận không âm, w.l.o.g., chúng ta có thể giả định rằng v là một vector không âm. Chính xác hơn v:= arg maxx∈Rn+∥x∥2=1‖D−1SrA·x‖2. Người ta có rằng ‖D−1SrA·v‖2=‖D−1SrA‖op. Chúng ta định nghĩa

--- TRANG 17 ---
chuỗi các ma trận nhị phân B0, B1, B2. . .có cùng hình dạng với D−1SrA như sau:
Bti,j:=1{2−t−1√α/n<[D−1SrA]i,j≤2−t√α/n} cho mọi số nguyên t≥0. (11)

Lưu ý rằng vì điều kiện tiên quyết của bổ đề về α=n·maxi∈[n]‖D−1A·e(i)‖22 ngụ ý [D−1A]i,j≤√α/n, chúng ta có bất đẳng thức sau trên mỗi phần tử của ma trận D−1SrA:

[D−1SrA]i,j≤√α/n·∑∞t=02−t·Bti,j.

Vì D−1SrA và v đều có các phần tử không âm, bất đẳng thức trên ngụ ý điều sau:

‖D−1SrA·v‖2≤√α/n·‖∑∞t=02−t·Bt·v‖2≤√α/n·∑∞t=02−t·‖Bt·v‖2. (12)

Bây giờ để giới hạn ‖Bt·v‖2 chúng ta đầu tiên tìm giới hạn trên số lượng số 1 trong các hàng và cột của Bt. Sử dụng định nghĩa của Bt trong Eq. (11) và thực tế rằng tổng hàng trong ma trận D−1A bằng 1, chúng ta có:

‖Bti,:‖0≤min(2t+1√n/α, n ). (13)

Ngoài ra, sử dụng điều kiện tiên quyết của bổ đề về α=n·maxi∈[n]‖D−1A·e(i)‖22, chúng ta có:

‖Bt:,j‖0≤min(22t+2,|Sr|). (14)

Bây giờ chúng ta giới hạn chuẩn ‖Bt·v‖2 cho một số nguyên tùy ý t≥0 như sau:

‖Bt·v‖22≤|Sr|∑i=1‖Bti,:‖0·‖Bti,:⊙v‖22(bất đẳng thức Cauchy–Schwarz)
≤2t+1√n/α·|Sr|∑i=1‖Bti,:⊙v‖22= 2t+1√n/α·∑j∈[n]|Sr|∑i=1Bti,j·v2j
= 2t+1√n/α·∑j∈[n]‖Bt:,j‖0·v2j
≤2t+1√n/α·min(22t+2,|Sr|)·∑j∈[n]v2j= 2t+1√n/α·min(22t+2,|Sr|),

trong đó bất đẳng thức trong dòng thứ hai ở trên theo sau từ Eq. (13) và bất đẳng thức trong dòng cuối cùng theo sau từ Eq. (14). Đẳng thức cuối cùng theo sau từ giả định rằng ∥v∥2= 1. Do đó,

‖Bt·v‖2≤{23t+3/2·(n/α)1/4 nếu 2t+1≤√|Sr|, 2t+1/2·(n/α)1/4·√|Sr| nếu ngược lại.

--- TRANG 18 ---
Bây giờ bằng cách thay các bất đẳng thức trên vào Eq. (12) chúng ta thấy rằng:

‖D−1SrA‖op≤√α/n·[∑log2√|Sr|−1t=02−t·‖Bt·v‖2+∑∞t=log2√|Sr|2−t·‖Bt·v‖2]

≤(1/2)α·|Sr|/n1/4

≤{ε3/2/(6κ7/4α1/4) nếu r= 0, ε3/4/(9κ) nếu r= 1, ε/(6κ) nếu r= 2,

trong đó dòng cuối cùng ở trên theo sau từ giới hạn trên về kích thước của các tập hợp Sr chúng ta thu được trong Eq. (5) và Eq. (8). Cuối cùng, bằng cách thay bất đẳng thức trên vào Eq. (10) và sử dụng thực tế rằng D−1A là một ma trận ngẫu nhiên theo hàng và do đó ‖D−1A‖op≥1 bổ đề theo sau.

Tiếp theo, chúng tôi chứng minh định lý chính.

Chứng minh Định lý 1: Ma trận chéo eD trong dòng 2 được tính toán bằng cách gọi Thuật toán 2. Theo Bổ đề 1 với xác suất ít nhất 1 −1/poly(n), đúng rằng

‖eD−1−D−1‖A‖op≤ε/2·‖D−1A‖op.

Hơn nữa, trong dòng 3 của thuật toán S được định nghĩa là ma trận lấy mẫu theo chuẩn hàng của V. Để gọi Bổ đề 2, chúng ta cần có một giới hạn trên rank ổn định của eD−1A.

Đầu tiên, từ Bổ đề 1 chúng ta biết rằng ‖eD−1A‖op≥(1−ε/2)·‖D−1A‖op≥1/2, trong đó bất đẳng thức thứ hai theo sau vì D−1A là một ma trận ngẫu nhiên theo hàng. Do đó chúng ta có srank(eD−1A)≤4‖eD−1A‖2F. Thứ hai, việc giới hạn dưới trong dòng 8 của Thuật toán 2 đảm bảo rằng ‖eD−1A‖2F≤κ2‖D−1A‖2F≤no(1)·‖D−1A‖2F≤no(1), do đó srank(eD−1A)≤no(1).

Với giới hạn này trên rank ổn định và vì m=d·no(1)= Ω(ε−2d·srank(eD−1A)), Bổ đề 2 ngụ ý rằng

‖eD−1AS⊤·SV−eD−1AV‖op≤ε/3·‖eD−1A‖op∥V∥op≤ε/2·‖D−1A‖op∥V∥op

với xác suất 0.99.

Bằng cách kết hợp hai bất đẳng thức này, chúng ta thu được đảm bảo xấp xỉ phổ trong Eq. (1). Runtime của thuật toán này chủ yếu được xác định bởi thời gian gọi Thuật toán 2 trong dòng 2, được chi phối bởi thời gian nhân hai ma trận có kích thước n×d và d×m và thời gian tính toán các phần tử ma trận chú ý tại các vị trí được định nghĩa bởi MH. Sử dụng kết quả nhân ma trận từ [22], tính toán đầu tiên có thể được thực hiện trong thời gian O(dn1+o(1)) và tính toán sau có thể được thực hiện trong nnz(MH).

Chứng minh Hệ quả 1: Vì r= log2n, chúng ta có Pr[H(Qi,∗) =H(Kj,∗)]≤1/n1−o(1) bất cứ khi nào θ(Qi,∗,Kj,∗)≥π/2(1−o(1)). Vì có nhiều nhất n2 cặp tổng cộng, số lượng kỳ vọng của những cặp như vậy va chạm dưới H nhiều nhất n1+o(1) và do đó bằng một giới hạn Markov nhiều nhất n1+o(1) với xác suất thất bại 1/no(1).

Vì chúng ta cũng giả định có nhiều nhất n1+o(1) cặp (i, j) với θ(Qi,∗,Kj,∗)<π/2(1−o(1)), có thể có nhiều nhất n1+o(1) cặp bổ sung va chạm.

Do đó, tổng cộng chúng ta có n1+o(1) va chạm, và do đó số phần tử khác không trong MH nhiều nhất n1+o(1) với xác suất thất bại 1/no(1). Chứng minh bây giờ theo sau bằng các giả định của phát biểu hệ quả cũng như Định lý 1.

Chứng minh Hệ quả 2: Cho T là một ma trận ExpanderSketch với O(τlogn) hàng. Bằng các đảm bảo [21] của T, chúng ta có rằng với xác suất ít nhất 1 −1/n3, với bất kỳ vector cố định x∈Rn, từ T·x, người ta có thể khôi phục một tập hợp S các chỉ số i∈[n] sao cho nếu x2i≥∥x∥22/τ, thì i∈S, trong khi nếu x2i≤∥x∥22/(2τ), thì i /∈S. Hơn nữa, S có thể được tìm từ Tx trong thời gian no(1).

Chúng ta tính toán T·Q, theo sau bởi (T·Q)·K⊤. Lưu ý rằng thời gian cho tính toán này là O(τnlogn). Tiếp theo, với mỗi cột j của QK⊤ điều này cho phép chúng ta xây dựng một tập hợp Sj với tính chất rằng nếu (Q·K⊤)i,j≥∥Q·K⊤ej∥22/τ, thì i∈Sj. Điều này đúng đồng thời cho tất cả các cột với xác suất ít nhất 1 −1/n2 bằng một union bound. Thời gian xây dựng tất cả các tập hợp Sj là n1+o(1)d

Lưu ý rằng |Sj| ≤2τ cho tất cả j, và chúng ta có thể tính toán rõ ràng giá trị chính xác của (Q·K⊤)i,j cho tất cả i∈Sj và tất cả j, trong thời gian O(nτd). Bằng các giả định của hệ quả, chúng ta có rằng Sj chứa một siêu tập của support của cột thứ j của MH, và vì chúng ta có thể tính toán các giá trị chính xác, chúng ta có thể xây dựng chính xác ma trận mặt nạ MH mà hệ quả yêu cầu, và trong thời gian n1+o(1)d.

Chứng minh bây giờ theo sau bằng các giả định của phát biểu cũng như Định lý 1.
