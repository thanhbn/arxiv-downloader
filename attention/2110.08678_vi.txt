# 2110.08678.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2110.08678.pdf
# Kích thước tệp: 4141437 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cải thiện Transformer với Khóa Chú ý Xác suất
Tam Nguyen* 1Tan M. Nguyen* 2Dung D. Le3Duy Khuong Nguyen1Viet-Anh Tran4Richard G. Baraniuk5
Nhat Ho** 6Stanley J. Osher** 2
Tóm tắt
Chú ý đa đầu là động lực đằng sau các transformer hiện đại nhất, đạt được hiệu suất đáng chú ý trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính. Người ta đã quan sát thấy rằng đối với nhiều ứng dụng, những đầu chú ý đó học được nhúng dư thừa, và hầu hết chúng có thể được loại bỏ mà không làm giảm hiệu suất của mô hình. Được truyền cảm hứng từ quan sát này, chúng tôi đề xuất Transformer với Hỗn hợp Khóa Gaussian (Transformer-MGK), một kiến trúc transformer mới thay thế các đầu dư thừa trong transformer bằng hỗn hợp khóa tại mỗi đầu. Những hỗn hợp khóa này tuân theo mô hình hỗn hợp Gaussian và cho phép mỗi đầu chú ý tập trung vào các phần khác nhau của chuỗi đầu vào một cách hiệu quả. So với bản transformer thông thường tương ứng, Transformer-MGK tăng tốc huấn luyện và suy luận, có ít tham số hơn, và cần ít FLOP hơn để tính toán trong khi đạt được độ chính xác tương đương hoặc tốt hơn trên các nhiệm vụ. Transformer-MGK cũng có thể dễ dàng được mở rộng để sử dụng với chú ý tuyến tính. Chúng tôi chứng minh thực nghiệm lợi thế của Transformer-MGK trong một loạt ứng dụng thực tế, bao gồm mô hình hóa ngôn ngữ và các nhiệm vụ liên quan đến chuỗi rất dài. Trên benchmark Wikitext-103 và Long Range Arena, Transformer-MGK với 4 đầu đạt được hiệu suất tương đương hoặc tốt hơn so với transformer cơ sở với 8 đầu.

*Đóng góp ngang nhau. Các tác giả đầu tiên được liệt kê theo thứ tự bảng chữ cái**Tác giả cuối cùng chung1Trung tâm AI FPT Software, Hà Nội, Việt Nam2Khoa Toán học, Đại học California, Los Angeles, Hoa Kỳ3Cao đẳng Kỹ thuật và Khoa học Máy tính, VinUniversity, Hà Nội, Việt Nam4Nghiên cứu Deezer, Pháp5Khoa Kỹ thuật Điện và Máy tính, Đại học Rice, Houston, Hoa Kỳ6Khoa Thống kê và Khoa học Dữ liệu, Đại học Texas tại Austin, Hoa Kỳ. Liên hệ: Tan Nguyen<tanmnguyen89@ucla.edu >.

Kỷ yếu Hội nghị Quốc tế lần thứ 39 về Học máy, Baltimore, Maryland, Hoa Kỳ, PMLR 162, 2022. Bản quyền 2022 thuộc về (các) tác giả.1. Giới thiệu
Transformer (Vaswani et al., 2017) đã trở thành mô hình hiện đại nhất cho các nhiệm vụ xử lý chuỗi, giải quyết nhiều vấn đề thách thức trong xử lý ngôn ngữ tự nhiên và thị giác máy tính (Al-Rfou et al., 2019; Dai et al., 2019; Williams et al., 2018; Devlin et al., 2018; Brown & et al., 2020; Howard & Ruder, 2018; Rajpurkar et al., 2016; Dehghani et al., 2018; So et al., 2019; Dosovitskiy et al., 2020; Touvron et al., 2020; Nguyen et al., 2021a). Những mô hình này cũng có thể chuyển giao kiến thức đã học từ mô hình được huấn luyện trước cho các nhiệm vụ liên quan đến các phương thức dữ liệu khác nhau và có giám sát hạn chế (Radford et al., 2018; 2019; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019). Thành công của transformer bắt nguồn từ cơ chế tự chú ý như khối xây dựng cơ bản cho mô hình hóa (Cho et al., 2014; Parikh et al., 2016; Lin et al., 2017). Đối với mỗi token, tự chú ý tính toán trung bình có trọng số của các biểu diễn đặc trưng của các token khác trong đó trọng số tỉ lệ với điểm tương tự giữa mỗi cặp token. Cơ chế này cho phép một token chú ý đến các token khác trong chuỗi và đạt được biểu diễn ngữ cảnh (Bahdanau et al., 2014; Vaswani et al., 2017; Kim et al., 2017). Đã được chứng minh rằng khả năng biểu diễn của cơ chế chú ý (Tenney et al., 2019) và khả năng nắm bắt các mối quan hệ cú pháp và ngữ nghĩa đa dạng (Tenney et al., 2019; Vig & Belinkov, 2019; Clark et al., 2019; Voita et al., 2019a; Hewitt & Liang, 2019) là chìa khóa cho hiệu suất ấn tượng của transformer trong thực tế.

1.1. Tự Chú ý
Đối với một chuỗi đầu vào cho trước X:= [x1;...;xN]> ∈ RN×Dx của N vector đặc trưng, tự chú ý biến đổi X thành chuỗi đầu ra H trong hai bước sau:

Bước 1. Chuỗi đầu vào X được chiếu thành ma trận truy vấn Q, ma trận khóa K, và ma trận giá trị V thông qua ba phép biến đổi tuyến tính
Q=XWQ>, K=XWK>, V=XWV>,
trong đó WQ,WK ∈ RD×Dx, và WV ∈ RDv×Dx là các ma trận trọng số. Chúng ta ký hiệu Q:= [q1;...;qN]>, K:= [k1;...;kN]>, và V:= [v1;...;vN]>, trong đó các vector qi,ki,vi cho i = 1,...,N là các vector truy vấn, khóa, và giá trị tương ứng.

--- TRANG 2 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bước 2. Chuỗi đầu ra H:= [h1;...;hN]> sau đó được tính như sau
H= softmax(QK>/√D)V:=AV, (1)
trong đó hàm softmax được áp dụng cho mỗi hàng của ma trận (QK>)/√D. Đối với mỗi vector truy vấn qi cho i = 1,...,N, một dạng tương đương của Phương trình (1) để tính vector đầu ra hi được cho bởi
hi=ΣNj=1 softmax(qi>kj/√D)vj:=ΣNj=1 aijvj. (2)

Ma trận A ∈ RN×N và thành phần aij của nó cho i, j = 1,...,N là ma trận chú ý và điểm chú ý, tương ứng. Tự chú ý được tính bởi Phương trình (1) và (2) được gọi là chú ý tích vô hướng có tỷ lệ hoặc chú ý softmax. Trong bài báo của chúng tôi, chúng tôi gọi một transformer sử dụng chú ý này là transformer softmax. Cấu trúc mà ma trận chú ý A học từ huấn luyện quyết định khả năng của tự chú ý để nắm bắt biểu diễn ngữ cảnh cho mỗi token.

Chú ý Đa đầu Mỗi chuỗi đầu ra H tạo thành một đầu chú ý. Trong chú ý đa đầu, nhiều đầu được nối lại để tính đầu ra cuối cùng. Gọi H là số đầu và WO ∈ RHDv×HDv là ma trận chiếu cho đầu ra. Chú ý đa đầu được định nghĩa là
MultiHead ({Q,K,V}Hi=1) =Concat (H1,...,HH)WO.

Mặc dù chú ý đa đầu mở rộng chú ý đơn đầu để nắm bắt các mẫu chú ý đa dạng và cải thiện hiệu suất của transformer, nhưng đã được chứng minh rằng transformer cho các nhiệm vụ thực tế bao gồm phân loại chuỗi và mô hình hóa ngôn ngữ học các đầu dư thừa (Michel et al., 2019). Những đầu dư thừa này tính toán các ánh xạ chú ý tương tự. Việc có nhiều đầu như vậy trong mô hình hạn chế khả năng biểu diễn của transformer trong khi lãng phí tham số, bộ nhớ và tính toán, cản trở việc áp dụng transformer cho nhiều nhiệm vụ quy mô lớn quan trọng.

1.2. Đóng góp
Chúng tôi thiết lập sự tương ứng giữa tự chú ý trong transformer và mô hình hỗn hợp Gaussian (GMM) và đề xuất Transformer với Hỗn hợp Khóa Gaussian (Transformer-MGK), một lớp transformer mới có thể tránh sự dư thừa đầu. Cốt lõi của Transformer-MGK là thay thế khóa chú ý kj trong mỗi đầu bằng một GMM để cho phép truy vấn qi, cũng như token liên quan của nó, chú ý đến các vị trí đa dạng hơn trong chuỗi đầu vào, từ đó tăng biểu diễn của mỗi đầu chú ý và giảm khả năng học các đầu dư thừa. Đóng góp của chúng tôi có bốn khía cạnh:

1. Chúng tôi xây dựng một GMM và chỉ ra rằng điểm chú ý trong tự chú ý khớp với phân phối hậu nghiệm trong mô hình của chúng tôi, cung cấp khung xác suất để nghiên cứu tự chú ý trong transformer.

2. Dưới khung xác suất cho tự chú ý của chúng tôi, chúng tôi giới thiệu một hỗn hợp Gaussian bổ sung để mô hình hóa mỗi khóa chú ý. Chúng tôi chứng minh thực nghiệm rằng hỗn hợp khóa Gaussian (MGK) này có thể nắm bắt sự đa dạng của các mẫu chú ý, do đó giảm bớt sự dư thừa đầu.

3. Chúng tôi mở rộng MGK của mình để sử dụng với chú ý tuyến tính và đề xuất hỗn hợp khóa tuyến tính (MLK) cho tính toán hiệu quả và dấu chân bộ nhớ tốt hơn.

4. Chúng tôi chứng minh thực nghiệm rằng Transformer-MGK và Transformer-MLK có thể so sánh hoặc tốt hơn so với các transformer cơ sở tương ứng với chú ý softmax và tuyến tính trong khi chỉ sử dụng một nửa số đầu chú ý và giảm cả độ phức tạp mô hình được đo bằng số tham số và chi phí tính toán tính bằng FLOP.

Tổ chức: Chúng tôi cấu trúc bài báo này như sau: Trong Phần 2, chúng tôi thiết lập kết nối giữa GMM và tự chú ý và sau đó trình bày Transformer-MGK của chúng tôi và các phần mở rộng của nó bao gồm Transformer-MLK. Trong Phần 3, chúng tôi xác nhận và phân tích thực nghiệm hiệu quả và độ chính xác của Transformer-MGK/MLK. Chúng tôi thảo luận về các công trình liên quan trong Phần 4. Bài báo kết thúc với các nhận xét kết luận. Thêm chi tiết thực nghiệm được cung cấp trong Phụ lục.

2. Transformer với Hỗn hợp Khóa Gaussian

2.1. Điểm Chú ý như Phân phối Hậu nghiệm
Đầu tiên chúng tôi xem xét một truy vấn qi ∈ Q và một khóa kj ∈ K. Gọi t là biến ngẫu nhiên nhị phân N chiều có biểu diễn 1-của-N trong đó một phần tử cụ thể tj bằng 1 và tất cả các phần tử khác bằng 0. Chúng tôi sử dụng tj để chỉ vị trí j của khóa kj. Gọi I là ma trận đơn vị, chúng tôi mô hình hóa phân phối p(qi) bằng GMM sau:
p(qi) =ΣNj=1 λjp(qi|tj = 1) =ΣNj=1 λjN(qi|kj,σ2jI), (3)
trong đó λj là tiên nghiệm p(tj = 1). Cho truy vấn qi, khả năng qi khớp với khóa kj được cho bởi hậu nghiệm p(tj = 1|qi). Hậu nghiệm này được tính như sau
p(tj = 1|qi) = λjN(qi|kj,σ2j)/Σj' λj'N(qi|kj',σ2j')
= λjexp{-(||qi||2+||kj||2)/(2σ2j)}exp{qi>kj/σ2j}/Σj' λj'exp{-(||qi||2+||kj'||2)/(2σ2j')}exp{qi>kj'/σ2j'}.

--- TRANG 3 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Chúng tôi giả định thêm rằng truy vấn qi và khóa kj được chuẩn hóa, và tiên nghiệm λj là đồng nhất. Chúng tôi sẽ biện minh những giả định này trong Nhận xét của chúng tôi ở cuối phần này. Chúng tôi cũng để σ2j = σ2, j = 1,2,...,K. Sau đó hậu nghiệm p(tj = 1|qi) có thể được viết như
p(tj = 1|qi) = exp{qi>kj/σ2}/Σj' exp{qi>kj'/σ2}. (4)

Vế phải của Phương trình (4) khớp với điểm chú ý được cho trong Phương trình (2) khi σ2 = √D. Như vậy, chúng tôi chỉ ra rằng dưới những giả định đúng, điểm chú ý giữa truy vấn qi và khóa kj trong một đơn vị chú ý của transformer là hậu nghiệm p(tj = 1|qi), điều này chỉ ra trách nhiệm mà khóa kj đảm nhận để 'giải thích' truy vấn qi, điều này lần lượt quyết định, ví dụ, một token ở vị trí i chú ý bao nhiêu đến một token ở vị trí j trong chuỗi đầu vào.

Nhận xét 1. Giả định rằng truy vấn qi và khóa kj được chuẩn hóa là thực tế và không giả tạo. Trong nhiều ứng dụng, hai vector này được chuẩn hóa. (Schlag et al., 2021) chỉ ra rằng việc chuẩn hóa như vậy là để tránh bất ổn xảy ra trong quá trình huấn luyện.

Nhận xét 2. Trong thực tế, tiên nghiệm được chọn là đồng nhất khi không có kiến thức tiên nghiệm nào có sẵn.

2.2. Transformer với Hỗn hợp Khóa Gaussian: Mỗi Khóa lại là một Mô hình Hỗn hợp Gaussian

Như chúng ta đã thấy từ Phương trình (4), khóa kj được sử dụng để giải thích truy vấn qi thông qua hậu nghiệm P(tj = 1|qi). Qua kết nối đơn giản này, mỗi truy vấn qi được coi như một mẫu từ hỗn hợp N khóa ΣNj=1 λjN(qi|kj,σ2jId). Tuy nhiên, giả định rằng phân phối P(qi|tj = 1) tại mỗi tiểu quần thể là Gaussian trong Phương trình (3) có thể khá mạnh vì không có gì đảm bảo rằng giả định này hợp lệ trong thực tế. Đặc biệt, có thể xảy ra rằng phân phối của mỗi tiểu quần thể là bất đối xứng hoặc lệch hoặc thậm chí đa phương thức. Do đó, việc sử dụng phân phối Gaussian cho mỗi tiểu quần thể có thể hạn chế khả năng giải thích và tính đa dạng của mỗi tiểu quần thể/khóa. Điều này chỉ ra rằng chúng ta nên sử dụng các phân phối biểu cảm hơn để biểu diễn P(qi|tj = 1).

Hỗn hợp khóa Gaussian: Để cải thiện khả năng giải thích của mỗi khóa kj, có khả năng tăng biểu diễn của mỗi đầu chú ý, và giảm khả năng học các đầu dư thừa, chúng tôi muốn mô hình hóa nó như hỗn hợp các phân phối Gaussian. Chúng tôi gọi mô hình này là Transformer với Hỗn hợp Khóa Gaussian (Transformer-MGK). Đặc biệt, trong Transformer-MGK chúng tôi mô hình hóa mỗi khóa kj tại vị trí j như hỗn hợp M Gaussian N(kjr,σ2jrI), r = 1,2,...,M. Ở đây chúng tôi đang overload ký hiệu một chút và sử dụng kjr và σ2jrI để biểu thị mean và ma trận covariance của Gaussian thứ r tại vị trí j. Gọi z là biến ngẫu nhiên nhị phân M chiều có biểu diễn 1-của-M. Chúng tôi sử dụng zr để chỉ Gaussian thứ r trong hỗn hợp. Gọi λjr ≡ P(zr = 1|tj = 1), MGK của chúng tôi có thể được viết như

P(qi|tj = 1) =Σr P(zr = 1|tj = 1)P(qi|zr = 1,tj = 1)
=Σr λjrN(qi|kjr,σ2jrI). (5)

Động lực của chúng tôi để sử dụng hỗn hợp phân phối Gaussian để biểu diễn phân phối của mỗi tiểu quần thể trong Phương trình (5) xuất phát từ kết quả xấp xỉ quan trọng sau:

Định lý 1. Giả sử P là phân phối xác suất trên [a,a]d cho một số a > 0 và có hàm mật độ p sao cho p khả vi và bị chặn. Khi đó, với bất kỳ phương sai σ > 0 và bất kỳ ε > 0, tồn tại hỗn hợp K thành phần ΣKi=1 λiN(μi,σ2I) trong đó K ≤ (C log(1/ε))d cho một hằng số toàn cục C sao cho
sup |p(x)-ΣKi=1 λiφ(x|μi,σ2I)| ≤ ε,
x∈Rd
trong đó φ(x|μ,σ2I) là hàm mật độ của phân phối Gaussian đa biến với mean μ và ma trận covariance σ2I.

Chứng minh Định lý 1 ở Phụ lục C. Kết quả của Định lý 1 gợi ý rằng bất kể dạng thực của P(qi|tj = 1), chúng ta có thể sử dụng hỗn hợp hữu hạn các phân phối Gaussian để xấp xỉ P(qi|tj = 1). Điều này cho phép chúng ta có xấp xỉ phong phú hơn của P(qi|tj = 1) so với việc sử dụng phân phối Gaussian đơn giản trong Phương trình (3). Tương tự như suy luận ở trên, hậu nghiệm p(tj = 1|qi) trong Transformer-MGK có thể được viết như

P(tj = 1|qi) = Σr λjr exp{qi>kjr/σ2jr}/Σj' Σr λj'r exp{qi>kj'r/σ2j'r}. (6)

Hơn nữa, trong Transformer-MGK, chúng tôi nới lỏng giả định rằng các truy vấn và khóa được chuẩn hóa. Do đó, khi tính P(tj = 1|qi), chúng tôi tính các kernel Gaussian giữa các truy vấn và khóa thay vì tích vô hướng của chúng. Hậu nghiệm P(tj = 1|qi) trong Transformer-MGK khi đó được cho bởi

P(tj = 1|qi) = Σr λjr exp{-||qi-kjr||2/(2σ2jr)}/Σj' Σr λj'r exp{-||qi-kj'r||2/(2σ2j'r)}. (7)

Như đã chứng minh trong Phần 2.1, hậu nghiệm này tương ứng với điểm chú ý. Do đó, Phương trình (7) là công thức để tính điểm chú ý trong Transformer-MGK. Chúng tôi tính vector đầu ra hi của tự chú ý trong Transformer-MGK

--- TRANG 4 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

như sau
hi=Σj' (Σr λjr exp{-||qi-kjr||2/(2σ2jr)}/Σj' Σr λj'r exp{-||qi-kj'r||2/(2σ2j'r)}) vj.

2.3. Suy luận và Học thông qua Thuật toán Expectation Maximization

Gọi τir ≡ P(zr = 1|qi,tj = 1), trong MGK, chúng tôi áp dụng suy luận E-step trong thuật toán Expectation-Maximization (EM) để ước tính hậu nghiệm này cho truy vấn qi. Hậu nghiệm τir cũng được biết đến như trách nhiệm mà thành phần N(kjr,σ2jrI) đảm nhận để giải thích cho quan sát, trong MGK là truy vấn qi. Dưới đây chúng tôi đề xuất hai phương pháp để ước tính trách nhiệm này.

Soft E-step Sử dụng suy luận soft E-step, thuật toán EM thực hiện gán mềm, trong đó mỗi truy vấn được liên kết với tất cả cluster. Các trách nhiệm khi đó được cho bởi

τir = λjr exp{-||qi-kjr||2/(2σ2jr)}/Σr' λjr' exp{-||qi-kjr'||2/(2σ2jr')}. (8)

Tại thời điểm học, các trách nhiệm được ước tính bởi Phương trình (8) được sử dụng để cập nhật tiên nghiệm λjr, tức là λjr = Njr/N, trong đó N là số truy vấn và Njr = ΣNi=1 τir. Những tiên nghiệm cập nhật λjr này sau đó được sử dụng trong Phương trình (7) để tính điểm chú ý.

Hard E-step Hard E-step thực hiện gán cứng của truy vấn cho cluster khóa, trong đó mỗi truy vấn được liên kết duy nhất với một cluster. Điều này tương tự như thuật toán K-means (Lloyd, 1982) và tương ứng với MGK ở giới hạn khi tham số phương sai σ2jr tiến đến 0. Theo suy luận K-means từ GMM trong (Bishop, 2006), Phương trình (7) trở thành

P(tj = 1|qi) = max_r exp{-||qi-kjr||2/(2σ2jr)}/Σj' max_r exp{-||qi-kj'r||2/(2σ2j'r)}.

Nhận xét 3. Suy luận hard E-step cho phép điểm chú ý được tính hiệu quả hơn vì các tiên nghiệm λjr không còn đóng vai trò tích cực trong thuật toán và có thể bị bỏ qua hoàn toàn.

Học thông qua Stochastic Gradient Descent (SGD) Để tăng hiệu quả của mô hình, trong MGK, chúng tôi cố định tham số phương sai σ2jr thành √D như trong chú ý softmax tiêu chuẩn và làm cho mean cluster, tức là các khóa, kjr thành tham số có thể học. Chúng tôi cũng làm cho tiên nghiệm λjr thành tham số có thể học như một trong các lựa chọn thiết kế. Trong trường hợp đó, cả kjr và λjr đều được học thông qua SGD. Cập nhật này thông qua SGD có thể được coi như M-step tổng quát (Bishop, 2006).

Lựa chọn Thiết kế cho Khóa (Lựa chọn A) Chúng tôi tuân theo thiết lập tiêu chuẩn trong transformer softmax và làm cho các khóa kjr thành chiếu tuyến tính của đầu vào xj, tức là kjr = xjWKr>, trong đó xj ∈ R1×Dx, WKr ∈ RD×Dx và r = 1,2,...,M.

(Lựa chọn B) Thay thế, chúng tôi cũng làm cho các khóa kjr thành phiên bản dịch chuyển của nhau để tiết kiệm tính toán, tức là kjr = xjWK> + br, trong đó WK ∈ RD×Dx.

2.4. Transformer với Hỗn hợp Khóa Tuyến tính

MGK có thể dễ dàng được mở rộng để sử dụng với chú ý tuyến tính. Chúng tôi gọi mô hình đó là Transformer với Hỗn hợp Khóa Tuyến tính (Transformer-MLK). Trong phần này, chúng tôi áp dụng công thức của chú ý tuyến tính từ (Katharopoulos et al., 2020) để suy ra Transformer-MLK. Phương pháp tương tự có thể được thực hiện để suy ra Transformer-MLK khi sử dụng với các chú ý tuyến tính khác như trong performers (Choromanski et al., 2021) và fast-weight transformers (Schlag et al., 2021). Trong Transformer-MLK, kernel Gaussian trong chú ý softmax được tuyến tính hóa thành tích của feature maps φ(·) trên các vector qi và kj. Tính chất kết hợp của phép nhân ma trận sau đó được sử dụng để suy ra tính toán hiệu quả sau của bản đồ chú ý

hi = φ(qi)>Σj Σr λjr φ(kjr)vj>/φ(qi)>Σj Σr λjr φ(kjr).

Thay thế Σj Σr λjr φ(qi)>φ(kjr)vj bằng φ(qi)>Σj Σr λjr φ(kjr)vj>, như trong transformer tuyến tính, giảm chi phí bộ nhớ và tính toán để tính bản đồ chú ý trong Transformer-MLK từ O(N2) xuống O(N), làm cho Transformer-MLK có thể mở rộng cho các chuỗi rất dài.

2.5. Giảm Độ phức tạp Mô hình và Chi phí Tính toán từ Transformer-MGK

Chúng tôi cung cấp phân tích chi tiết về việc giảm độ phức tạp mô hình và chi phí tính toán của Transformer-MGK so với chú ý softmax cơ sở trong Phụ lục B.

3. Kết quả Thực nghiệm

Trong phần này, chúng tôi biện minh số lượng hiệu quả của Transformer-MGK/MLK và nghiên cứu thực nghiệm lợi thế của việc sử dụng hỗn hợp khóa trên các benchmark khác nhau, bao gồm các nhiệm vụ khác nhau trong Long Range Arena (LRA) (Tay et al., 2021) (Phần 3.1) và mô hình hóa ngôn ngữ trên Wikitext-103 (Merity et al., 2017) (Phần 3.2). Chúng tôi nhằm chỉ ra rằng: (i) Transformer-MGK/MLK với một nửa số đầu có thể so sánh hoặc tốt hơn các transformer softmax và tuyến tính cơ sở với đầy đủ số đầu trong khi hiệu quả hơn cả về chi phí tính toán và dấu chân bộ nhớ; (ii) Hỗn hợp khóa giúp giảm sự dư thừa trong transformer đa đầu và có lợi cho việc học phụ thuộc dài hạn trong các chuỗi đầu vào dài; (iii) Sử dụng cùng số đầu, Transformer-MGK/MLK

--- TRANG 5 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 1. Độ Chính xác Kiểm tra (%) của Transformer-MGK so với transformer softmax cơ sở trên benchmark LRA. Transformer-MGK của chúng tôi vượt trội hơn transformer softmax trong khi sử dụng một nửa số đầu, có ít tham số hơn, và yêu cầu ít FLOP hơn (xem Hình 4). Kết quả được tính trung bình trên 5 lần chạy.

Model ListOps Text Retrieval Average
Softmax 12 heads 36.64 65.62 82.18 61.48
Softmax 8 heads 37.03 65.71 81.74 61.49
sMGK 4 heads 37.25 65.51 82.79 61.85
MGK 4 heads 36.98 65.69 82.23 61.63
Softmax 4 heads 36.89 65.26 81.54 61.23
sMGK 2 heads 37.35 65.17 82.20 61.57
MGK 2 heads 36.88 65.37 81.83 61.36
Softmax 2 heads 36.76 64.90 79.1 60.25
sMGK 1 head 37.31 65.04 81.23 61.19
MGK 1 head 37.13 65.40 80.63 61.05
Softmax 1 head 36.81 64.48 77.9 59.73

vượt trội đáng kể so với transformer softmax và tuyến tính cơ sở. Đặc biệt trong trường hợp Transformer-MLK, nó giúp giảm khoảng cách hiệu suất giữa transformer softmax và tuyến tính.

Trong suốt phần này, chúng tôi so sánh Transformer-MGK/MLK với transformer softmax và tuyến tính có cùng hoặc gấp đôi số đầu chú ý. Trong tất cả thí nghiệm, đối với các mô hình Transformer-MGK/MLK của chúng tôi, chúng tôi đặt M=2 trong đó M là số Gaussian, tức là khóa, tại mỗi timestep. Trong số các lựa chọn thiết kế cho Transformer-MGK được đề cập trong Phần 2.3, chúng tôi sử dụng cái với Soft-E step nhưng làm cho tham số λjr và kjr có thể học và cố định phương sai σ2jr thành hằng số. Chúng tôi nghiên cứu cả hai triển khai cho khóa: (A) kjr là chiếu tuyến tính của đầu vào xj, tức là, kjr = xjWKr> và (B) kjr là phiên bản dịch chuyển của nhau, tức là, kjr = xjWK> + br.

Trong phần này, chúng tôi gọi Transformer-MGK/MLK có khóa được triển khai bởi (A) là Transformer-MGK/MLK, và có khóa được triển khai bởi (B) là Transformer-sMGK/sMLK. Chúng tôi so sánh thực nghiệm các mô hình này với các lựa chọn thiết kế khác cho Transformer-MGK trong Phần 3.4. Chi tiết về dataset, mô hình, và huấn luyện được cung cấp trong Phụ lục A.1. Mã PyTorch (Paszke et al., 2019) của chúng tôi có sẵn tại https://github.com/minhtannguyen/transformer-mgk.

3.1. Benchmark Long Range Arena (LRA)

Mô hình và cơ sở Chúng tôi so sánh Transformer-MGK và MLK 1-head, 2-head, 4-head của chúng tôi với transformer softmax cơ sở (Vaswani et al., 2017) và tuyến tính (Katharopoulos et al., 2020) có 1 head, 2 head, 4 head, và 8 head. Mỗi mô hình bao gồm hai lớp, và chúng tôi áp dụng thiết lập mô hình và huấn luyện từ (Xiong et al., 2021) trong các thí nghiệm của chúng tôi.

Kết quả Chúng tôi tóm tắt kết quả của chúng tôi trong Bảng 1. Transformer-

Bảng 2. Độ Chính xác Kiểm tra (%) của Transformer-MLK so với transformer tuyến tính trên LRA. Transformer-MLK của chúng tôi đạt được độ chính xác tương đương/tốt hơn so với các cơ sở trong khi sử dụng một nửa số đầu, có ít tham số hơn, và yêu cầu ít FLOP hơn (xem Hình 4 để biết chi tiết). Kết quả được tính trung bình trên 5 lần chạy.

Model ListOps Text Retrieval Average
Linear 12 heads 20.26 65.87 81.97 56.03
Linear 8 heads 19.17 65.85 81.18 55.40
sMLK 4 heads 20.11 65.74 81.53 55.79
MLK 4 heads 20.06 65.7 81.34 55.7
Linear 4 heads 19.37 65.81 81.65 55.61
sMLK 2 heads 19.88 65.61 81.66 55.71
MLK 2 heads 20.12 65.72 80.80 55.54
Linear 2 heads 18.35 65.94 80.94 55.07
sMLK 1 head 18.87 65.57 80.37 54.93
MLK 1 head 18.34 65.70 81.09 55.04
Linear 1 head 18.60 65.70 80.6 54.96

MGK với một nửa số đầu liên tục đạt được độ chính xác kiểm tra tốt hơn so với chú ý softmax cơ sở trên các nhiệm vụ. Vì cần ít đầu hơn, transformer-MGK sử dụng ít tham số hơn và cần ít FLOP để tính toán hơn so với các cơ sở. Chúng tôi cung cấp phân tích hiệu quả chi tiết cho Transformer-MGK trong Hình 4. Thú vị hơn, những lợi thế hiệu quả này của Transformer-MGK so với cơ sở trở nên đáng kể hơn khi số đầu trong mô hình cơ sở tăng lên. Khi sử dụng cùng số đầu như các mô hình cơ sở, Transformer-MGK cải thiện thêm so với những cơ sở đó. Trong số các mô hình, Transformer-sMGK hoạt động tốt nhất trên các nhiệm vụ LRA.

Chúng tôi cũng so sánh hiệu suất của Transformer-MLK với transformer tuyến tính cơ sở trong Bảng 2. Giống như Transformer-MGK, Transformer-MLK mang lại kết quả tương đương hoặc tốt hơn so với cơ sở chỉ sử dụng một nửa số đầu với ít tham số và FLOP hơn. Khi sử dụng cùng số đầu, Transformer-MLK giúp cải thiện transformer tuyến tính thêm.

Chúng tôi cung cấp kết quả của các cơ sở 12-head trong Bảng 1 và 2 để tham khảo. Thú vị khi nhận thấy từ Bảng 1 và 2 rằng ngay cả các mô hình Transformer-MGK/MLK 2-head của chúng tôi cũng đạt được kết quả tốt hơn hoặc tương đương với các cơ sở 12-head và 8-head. Một so sánh giữa các cơ sở 12-head với các mô hình Transformer-MGK/MLK 6-head của chúng tôi trên nhiệm vụ retrieval được cung cấp trong Bảng 9 trong Phụ lục A.10.

Trong Hình 1, chúng tôi so sánh đường cong loss huấn luyện và độ chính xác kiểm tra của Transformer-MGK/MLK 1-head và 2-head của chúng tôi với transformer softmax 2-head và transformer tuyến tính 2-head trên nhiệm vụ document retrieval. Nhiệm vụ retrieval này có độ dài chuỗi trung bình dài nhất và khoảng chú ý trong các nhiệm vụ LRA (Tay et al., 2021). Trên nhiệm vụ này, như được hiển thị trong Hình 1, Transformer-MGK/MLK của chúng tôi luôn tốt hơn so với các mô hình cơ sở trong suốt quá trình huấn luyện. Quan sát này củng cố khả năng của mô hình chúng tôi trong việc nắm bắt

--- TRANG 6 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Hình 1. Loss huấn luyện và độ chính xác kiểm tra của Transformer-MGK/MLK so với transformer softmax/tuyến tính trên nhiệm vụ retrieval, có độ dài chuỗi trung bình dài nhất và khoảng chú ý trong các nhiệm vụ LRA (Tay et al., 2021). Hiệu suất ấn tượng của Transformer-MGK/MLK trên nhiệm vụ thách thức này xác nhận khả năng của các mô hình chúng tôi để nắm bắt phụ thuộc tầm xa thông qua việc học đa dạng các mẫu chú ý.

phụ thuộc tầm xa trong các chuỗi đầu vào rất dài.

3.2. Mô hình hóa Ngôn ngữ trên WikiText-103

Tiếp theo chúng tôi xác nhận lợi thế của các mô hình chúng tôi trên một ứng dụng quy mô lớn. Chúng tôi xem xét nhiệm vụ mô hình hóa ngôn ngữ cấp từ trên WikiText-103 (Merity et al., 2017) cho các thí nghiệm trong phần này.

Mô hình và cơ sở Chúng tôi so sánh Transformer-MGK/MLK 4 và 8-head với transformer softmax 8-head (Vaswani et al., 2017) và tuyến tính (Katharopoulos et al., 2020). Mỗi mô hình bao gồm 16 lớp. Các thí nghiệm của chúng tôi tuân theo thiết lập cho các mô hình small/medium từ (Schlag et al., 2021).

Kết quả Như được hiển thị trong Bảng 3, Transformer-MGK của chúng tôi vượt trội hơn transformer softmax cơ sở. Ngay cả khi sử dụng một nửa số đầu chú ý (tức là, 4 so với 8 đầu như trong các cơ sở), Transformer-MGK vẫn đạt được perplexity kiểm tra (PPL) tốt hơn so với cơ sở. Thêm nhiều đầu vào Transformer-MGK cải thiện hiệu suất của chúng. Tương tự, Transformer-MLK đạt được PPL kiểm tra/validation có thể so sánh với transformer tuyến tính cơ sở khi sử dụng một nửa số đầu chú ý. Khi sử dụng cùng số đầu chú ý như trong cơ sở, Transformer-MLK liên tục đạt được hiệu suất tốt hơn. Lưu ý rằng việc giảm số đầu từ 8 xuống 4 trong các mô hình cơ sở làm giảm đáng kể hiệu suất của chúng với hơn 1,5 giảm trong PPL kiểm tra/validation cho transformer softmax và hơn 1,0 giảm trong PPL kiểm tra/validation cho transformer tuyến tính. Transformer-MGK và Transformer-MLK đề xuất của chúng tôi giúp thu hẹp khoảng cách này.

Để tiếp tục kiểm tra khả năng mở rộng của các mô hình chúng tôi, chúng tôi áp dụng MGK trên một cơ sở mạnh hơn, đó là transformer softmax medium 8-head trong (Schlag et al., 2021). Mô hình này có 90M tham số, 16 lớp, 8 đầu chú ý mỗi lớp, và kích thước ẩn 256. Kích thước của mô hình cơ sở chúng tôi gần với BERT Base (Devlin et al., 2019), có 110M tham số, 12 lớp, 12 đầu chú ý mỗi lớp, và kích thước ẩn 768. Áp dụng MGK của chúng tôi lên cơ sở này và chỉ sử dụng 4 đầu thay vì 8, chúng tôi cải thiện đáng kể PPL kiểm tra từ 29.60 xuống 28.86 trong khi giảm

Bảng 3. Perplexity (PPL) trên WikiText-103 của Transformer-MGK và MLK so với các cơ sở. Cả Transformer-MGK và MLK đều đạt được PPL có thể so sánh hoặc tốt hơn so với các cơ sở trong khi chỉ sử dụng một nửa số đầu. Khi sử dụng cùng số đầu, các mô hình chúng tôi cải thiện đáng kể các cơ sở.

Method Valid PPL Test PPL
Softmax 8 heads (small) 33.15 34.29
MGK 4 heads (small) 33.28 34.21
sMGK 8 heads (small) 32.92 33.99
MGK 8 heads (small) 32.74 33.93
Softmax 4 heads (small) 34.80 35.85
Linear 8 heads (small) 38.07 39.08
MLK 4 heads (small) 38.49 39.46
MLK 8 heads (small) 37.78 38.99
Linear 4 heads (small) 39.32 40.17
Softmax 8 heads (medium) 27.90 29.60
MGK 4 heads (medium) 27.58 28.86

kích thước mô hình và chi phí tính toán, chứng minh lợi thế của các mô hình tỷ lệ của chúng tôi.

3.3. Dịch Máy Thần kinh trên IWSLT'14 Đức sang Tiếng Anh và WMT'14

Chúng tôi tiếp tục kiểm tra lợi thế của các phương pháp chúng tôi trên nhiệm vụ dịch máy IWSLT'14 Đức-Anh (Cettolo et al., 2014). Bảng 4 cho thấy rằng các mô hình Transformer-MGK và sMGK 2-head đạt được điểm BLEU lần lượt là 34.34 và 34.69, có thể so sánh và tốt hơn so với điểm BLUE 34.42 của transformer softmax 4-head cơ sở.

Chúng tôi cũng so sánh Transformer-MGK/sMGK 8-head với cơ sở softmax 16-head trên nhiệm vụ WMT'14 trong Bảng 4. Cơ sở này bao gồm 12 lớp với 201M tham số có thể huấn luyện. Transformer-MGK/sMGK đạt được điểm BLEU lần lượt là 29.03 và 29.07, có thể so sánh với điểm BLEU cơ sở 29.38.

3.4. Phân tích Thực nghiệm

Chúng tôi tiến hành phân tích thực nghiệm dựa trên Transformer-MGK được huấn luyện cho nhiệm vụ document retrieval, nhiệm vụ mô hình hóa ngôn ngữ trên WikiText-103, và nhiệm vụ dịch máy IWSLT14 De-En

--- TRANG 7 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 4. Điểm BLEU dịch máy của Transformer-MGK 2-head trên nhiệm vụ IWSLT'14 De-En tốt hơn hoặc tương đương với cơ sở 4-head. Tương tự, điểm BLEU của Transformer-MGK 8-head trên nhiệm vụ WMT'14 có thể so sánh với cơ sở 16-head.

Method Task BLEU score
Softmax 4 heads IWSLT'14 34.42
Transformer sMGK 2 head IWSLT'14 34.69
Transformer MGK 2 head IWSLT'14 34.34
Softmax 16 heads WMT'14 29.38
Transformer sMGK 8 head WMT'14 29.07
Transformer MGK 8 head WMT'14 29.03

machine translation task. Kết quả cho Transformer-MLK và các kết quả khác cho nhiệm vụ WikiText-103 được cung cấp trong Phụ lục.

Khả năng của MGK để xấp xỉ phân phối xác suất của truy vấn Chúng tôi tính điểm negative log-likelihood (NLL) của MGK so với cơ sở chỉ sử dụng 1 phân phối Gaussian tập trung xung quanh khóa kj, j = 1,...,N. Cả hai mô hình đều được huấn luyện cho nhiệm vụ document retrieval. NLL của MGK là 0.180 và 0.182, nhỏ hơn NLL của cơ sở là 0.187 và 0.292, ở lớp 1 và 2, tương ứng, chỉ ra rằng MGK khớp với phân phối của các truy vấn chú ý tốt hơn.

Nghiên cứu loại bỏ về phương sai của MGK Trong các thí nghiệm của chúng tôi, chúng tôi đặt M=2 và σ²j1=√D như trong transformer softmax cơ sở và σ²j2=3√D trong đó D là chiều đặc trưng được định nghĩa trong Phần 1.1. Chúng tôi tiến hành nghiên cứu loại bỏ trên nhiệm vụ dịch máy IWSLT14 De-En trong đó chúng tôi thay đổi σ²j2. Chúng tôi quan sát rằng điểm BLEU của các mô hình được huấn luyện không khác nhau nhiều giữa các giá trị khác nhau của σ²j2. Đặc biệt, điểm BLEU là 34.31, 34.36, 34.61, 34.34, và 34.27 cho σ²j2= 0.5√D, √D, 2√D, 3√D và 4√D, tương ứng. Chúng tôi cũng làm cho σ²j1 và σ²j2 có thể học và quan sát rằng điểm BLEU kết quả tệ hơn (32.86).

Transformer-MGK giúp tránh học các đầu dư thừa Chúng tôi so sánh trực quan các ma trận chú ý được học bởi Transformer-MGK và transformer softmax cơ sở trên nhiệm vụ document retrieval trong Hình 2. Đặc biệt, chúng tôi chọn ngẫu nhiên một ma trận chú ý tại mỗi đầu trong mỗi lớp và hiển thị ma trận chú ý đó cho mỗi mô hình để so sánh. Hình 2(Trái) cho thấy rằng các truy vấn trong Transformer-MGK có thể chú ý đến nhiều khóa khác nhau và tương đương với các token khác tại các vị trí khác nhau trong chuỗi đầu vào. Sự đa dạng này trong mẫu chú ý giúp giảm khả năng mô hình học các ma trận chú ý tương tự và dư thừa tại các đầu khác nhau một cách đáng kể.

Một chỉ số khác để đo khả năng biểu diễn của ma trận chú ý là rank của nó. Ma trận chú ý với rank cao có thể nắm bắt các mẫu chú ý đa dạng hơn so với những ma trận có rank thấp (Nguyen et al., 2021b). Chúng tôi nghiên cứu

Bảng 5. Hiệu suất của Transformer-MGK sử dụng các kỹ thuật suy luận/học khác nhau trên benchmark LRA.

Model ListOps Text Retrieval Average
sMGK + Hard-E 1 head 37.25 64.7 81.29 61.08
sMGK + Soft-E 1 head 37.05 64.68 81.44 61.05
sMGK 1 head 37.31 65.04 81.23 61.19
MGK + Hard-E 1 head 19.40 65.40 80.72 55.17
MGK + Soft-E 1 head 33.85 65.25 80.73 59.94
MGK 1 head 37.13 65.40 80.63 61.05

rank của ma trận chú ý từ Transformer-MGK và transformer softmax được huấn luyện cho nhiệm vụ retrieval. Đặc biệt, chúng tôi chọn ngẫu nhiên 1000 ma trận chú ý khác nhau tại mỗi lớp từ mỗi mô hình. Sau đó, chúng tôi thực hiện phân tích giá trị kỳ dị (SVD) để tính rank của mỗi ma trận và ngưỡng các giá trị kỳ dị nhỏ hơn 10⁻⁶. Hình 2(Phải) trình bày phân phối rank của ma trận chú ý tại mỗi lớp của Transformer-MGK và transformer softmax. Chúng tôi quan sát rằng ma trận chú ý trong Transformer-MGK có rank cao hơn so với những ma trận trong transformer softmax. Do đó, chú ý với MGK của chúng tôi có khả năng nắm bắt các mẫu chú ý đa dạng và phức tạp hơn so với chú ý softmax cơ sở.

So sánh các kỹ thuật suy luận và học khác nhau Bảng 5 so sánh hiệu suất của Transformer-MGK sử dụng các lựa chọn thiết kế khác nhau được đề cập trong Phần 2.3 trên benchmark LRA. Đặc biệt, chúng tôi xem xét ba lựa chọn thiết kế sau: A) Soft-E step, tham số λjr và kjr có thể học thông qua SGD, và phương sai σ²jr là hằng số, B) Soft-E step, tham số λjr được cập nhật theo cập nhật M-step, kjr có thể học thông qua SGD, và phương sai σ²jr là hằng số, và C) Hard-E step, λjr và kjr có thể học thông qua SGD, và phương sai σ²jr là hằng số. Lưu ý rằng Transformer-MGK với thiết lập A là các mô hình mặc định chúng tôi sử dụng trong tất cả thí nghiệm ở trên. Trong Bảng 5, Transformer-MGK + Hard-E là Transformer-MGK với thiết lập C, Transformer-MGK + Soft-E là Transformer-MGK với thiết lập B, và Transformer-MGK chỉ là Transformer-MGK với thiết lập A. Đáng chú ý rằng Transformer-sMGK + Hard-E đạt được kết quả có thể so sánh với các mô hình có hiệu suất tốt nhất trong mỗi nhiệm vụ mặc dù nó là mô hình hiệu quả nhất trong nghiên cứu của chúng tôi.

Transformer-MGK giảm độ phức tạp mô hình và chi phí tính toán Hình 3A và 3B trình bày tỷ lệ giảm FLOP huấn luyện và kiểm tra, tương ứng, của Transformer-MGK 4-head của chúng tôi so với cơ sở Softmax 8-head như hàm của chiều mô hình D và độ dài chuỗi N. Trong Hình 3C, chúng tôi hiển thị tỷ lệ giảm kích thước mô hình của Transformer-MGK so với cơ sở khi D thay đổi. Chúng tôi quan sát rằng lợi thế hiệu quả của Transformer-MGK so với cơ sở tăng theo D và N, làm cho nó phù hợp và vượt trội hơn cho các ứng dụng quy mô lớn. Lưu ý rằng kích thước mô hình tính bằng số tham số không phụ thuộc

--- TRANG 8 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Count
Layer 1Layer 2RankSoftmax 4 HeadsMGK 4 HeadsMGK 2 Heads

Hình 2. (Trái) Trực quan hóa ma trận chú ý trong transformer softmax 4-head cơ sở (trái), Transformer-MGK 4-head (giữa), và Transformer-MGK 2-head (phải) được huấn luyện trên nhiệm vụ document retrieval. Ma trận chú ý từ Transformer-MGK của chúng tôi có mẫu đa dạng hơn so với transformer softmax cơ sở, giảm nguy cơ học các đầu dư thừa. (Phải) Phân phối rank của ma trận chú ý cho thấy rằng ma trận chú ý trong Transformer-MGK có rank cao hơn so với những ma trận trong transformer softmax và do đó có thể nắm bắt các mẫu chú ý đa dạng hơn.

Sequence Length Model Dimension Model Dimension 2565121024 2048 4096 
Training Testing FLOPS Ratio 
Total parameters 
Non-embedding 
parameters 
Parameters Ratio 
￼ 
￼ 
￼ 
A B C

Hình 3. Tỷ lệ FLOP huấn luyện (A), Suy luận (B) và tỷ lệ số tham số (C) giữa Transformer-MGK 4-head với cơ sở softmax 8-head trên các chiều mô hình D khác nhau và độ dài chuỗi N, cho nhiệm vụ mô hình hóa ngôn ngữ trên WikiText-103. Transformer-MGK 4-head hiệu quả đáng kể, cả về tính toán và độ phức tạp mô hình, so với cơ sở khi D và N tăng, chỉ ra lợi ích của phương pháp chúng tôi cho các nhiệm vụ tầm xa và quy mô lớn.

Hình 4. Chi phí tính toán (FLOP) và số tham số của Transformer-MGK so với transformer softmax cơ sở (Trái) và Transformer-MLK so với transformer tuyến tính cơ sở (Phải) được huấn luyện trên nhiệm vụ Retrieval. Lợi thế hiệu quả của Transformer-MGK/MLK so với các cơ sở trong cả hai chỉ số tăng theo số đầu.

vào độ dài chuỗi N. Kết quả phân tích hiệu quả trên nhiệm vụ mô hình hóa ngôn ngữ này cho Transformer-MLK và các chỉ số phân tích được cung cấp trong Hình 10 trong Phụ lục A.5. Ngoài ra, Hình 4 so sánh chi phí tính toán, được đo bằng FLOP, và độ phức tạp mô hình, được đo bằng số tham số, giữa Transformer-MGK/MLK của chúng tôi có một nửa số đầu và transformer softmax/tuyến tính đầy đủ đầu như hàm của số đầu. Các mô hình được huấn luyện cho nhiệm vụ Retrieval trong benchmark LRA. Càng nhiều đầu được sử dụng, Transformer-MGK/MLK càng có lợi thế hơn so với transformer softmax/tuyến tính. Đối với các mô hình transformer lớn hơn nhiều, việc tiết kiệm này là đáng kể.

4. Công trình Liên quan

Transformer Hiệu quả Transformer hiệu quả có thể được phân loại thành nhiều danh mục, như được tóm tắt trong (Roy et al., 2021). Trong số các danh mục này là các mô hình thiết kế ma trận chú ý có cấu trúc thưa (Parmar et al., 2018; Liu et al., 2018; Qiu et al., 2019; Child et al., 2019; Beltagy et al., 2020; Du et al., 2021). Một danh mục khác bao gồm các mô hình kết hợp hai hoặc nhiều mẫu truy cập khác nhau để cải thiện phạm vi bao phủ (Child et al., 2019; Ho et al., 2019). Mẫu truy cập cũng có thể được làm cho có thể học theo cách dựa trên dữ liệu (Kitaev et al., 2020; Roy et al., 2021; Tay et al., 2020). Các transformer hiệu quả khác tận dụng mô-đun bộ nhớ phụ để truy cập nhiều token cùng một lúc (Lee et al., 2019; Sukhbaatar et al., 2019; Asai & Choi, 2020; Beltagy et al., 2020). Được truyền cảm hứng từ việc sử dụng các phương pháp momentum trong việc tăng tốc huấn luyện mạng thần kinh (Sutskever et al., 2013; Bengio et al., 2013; Kingma & Ba, 2015; Wang et al., 2022) và thiết kế kiến trúc mạng thần kinh (Li et al., 2018; He et al., 2020; Nguyen et al., 2020; Xia et al., 2021; Sander et al., 2021), (Wang et al., 2021a) kết hợp momentum thích ứng vào transformer tuyến tính để cải thiện độ chính xác của nó trong khi duy trì độ phức tạp tính toán và bộ nhớ tuyến tính. Hơn nữa, suy luận động đã được đề xuất trước đây cho mạng thần kinh sâu (Wang et al., 2018a;b; 2020b; Han et al., 2021) cũng có thể được sử dụng để tăng tốc suy luận trong transformer (Bakhtiarnia et al.,

--- TRANG 9 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

2021; Wang et al., 2021b). Xấp xỉ low-rank và kernelization cũng được sử dụng để tăng cường hiệu quả bộ nhớ và tính toán của việc tính toán tự chú ý (Tsai et al., 2019; Wang et al., 2020a; Katharopoulos et al., 2020; Choromanski et al., 2021; Shen et al., 2021; Nguyen et al., 2021b; Peng et al., 2021). Ngoài các transformer hiệu quả đã đề cập ở trên, chú ý đa truy vấn chia sẻ khóa và giá trị giữa các đầu chú ý khác nhau (Shazeer, 2019) cũng đã được nghiên cứu để giảm chi phí băng thông bộ nhớ và tăng tốc độ cho suy luận transformer tăng dần (xem Phụ lục A.11). Đáng chú ý rằng các phương pháp retrieval hiệu quả đã được sử dụng để tăng hiệu quả của các mô hình học máy bao gồm transformer (Chen et al., 2020; Le & Lauw, 2020; 2021; Mittal et al., 2022). Cuối cùng nhưng không kém phần quan trọng, các phương pháp như sử dụng loss phụ (Al-Rfou et al., 2019) và embedding đầu vào thích ứng (Baevski & Auli, 2019) đã được khám phá để tăng tốc sự hội tụ của việc huấn luyện transformer. MGK/MLK của chúng tôi có thể dễ dàng được tích hợp vào các phương pháp trên để cải thiện thêm độ chính xác và hiệu quả của chúng.

Sự Dư thừa trong Transformer Các công trình gần đây gợi ý rằng hầu hết các neuron và đầu trong transformer được huấn luyện trước là dư thừa và có thể được loại bỏ khi tối ưu hóa hướng tới một nhiệm vụ downstream (Dalvi et al., 2020; Michel et al., 2019; Durrani et al., 2020). Các công trình khác cũng nghiên cứu các embedding ngữ cảnh hóa trong mạng được huấn luyện trước dưới sự dư thừa này do overparameterization và cho thấy rằng các biểu diễn được học trong các mô hình này có tính anisotropic cao (Mu & Viswanath, 2018; Ethayarajh, 2019). Một nhóm công trình mới nổi được đề xuất để chưng cất và cắt tỉa mô hình, bao gồm (Sanh et al., 2019; Sun et al., 2019; Voita et al., 2019b; Sajjad et al., 2020). Phương pháp MGK/MLK của chúng tôi có thể được kết hợp với các phương pháp chưng cất và cắt tỉa này.

Mô hình Hỗn hợp cho Transformer Một số công trình đã sử dụng mô hình hỗn hợp để nghiên cứu và tăng cường transformer. Switch transformer (Fedus et al., 2021) sử dụng thuật toán routing trong Mixture of Experts (MoE) để giảm chi phí giao tiếp và tính toán trong transformer. (Nguyen et al., 2018; Patel et al., 2016; 2015) suy ra khung xác suất dựa trên GMM cho mạng thần kinh sâu có thể được mở rộng để nghiên cứu transformer và kiến trúc dựa trên chú ý. (Zhang & Feng, 2021) phát triển chú ý hỗn hợp Gaussian mô hình hóa mỗi điểm chú ý như một GMM. Ngược lại, MGK mô hình hóa mỗi khóa kj tại timestep j như một GMM. Nói cách khác, MGK của chúng tôi là phương pháp mô hình sinh trong đó chúng tôi xây dựng mô hình sinh cho khóa kj và suy ra điểm chú ý từ phân phối hậu nghiệm trong khi chú ý hỗn hợp Gaussian trong (Zhang & Feng, 2021) là phương pháp phân biệt (Bishop, 2006). Các công trình khác sử dụng mô hình hỗn hợp với mạng thần kinh sâu và transformer bao gồm (Cho et al., 2020; Guo et al., 2019; Huang et al., 2020).

5. Nhận xét Kết luận

Trong bài báo này, chúng tôi đề xuất Transformer-MGK, một lớp transformer sử dụng mô hình hỗn hợp Gaussian để biểu diễn các vector khóa trong tự chú ý. Transformer-MGK giảm sự dư thừa giữa các đầu trong transformer. Hơn nữa, các đầu chú ý trong Transformer-MGK có khả năng biểu diễn tốt hơn so với những đầu trong cơ sở, cho phép Transformer-MGK đạt được hiệu suất có thể so sánh hoặc tốt hơn so với transformer softmax cơ sở trong khi chỉ sử dụng một nửa số đầu. So với cơ sở, Transformer-MGK sử dụng ít tham số hơn và yêu cầu ít FLOP để tính toán. Chúng tôi mở rộng Transformer-MGK thành Transformer-MLK để sử dụng chú ý tuyến tính cho hiệu quả tốt hơn. Chúng tôi xác nhận thực nghiệm lợi thế của Transformer-MGK/MLK so với transformer softmax và tuyến tính cơ sở trên các benchmark khác nhau bao gồm các nhiệm vụ trong benchmark LRA, mô hình hóa ngôn ngữ WikiText-103, và dịch máy IWSLT'14/WMT'14. Cả Transformer-MGK/MLK và transformer softmax/tuyến tính đều giả định rằng các đặc trưng trong truy vấn và khóa chú ý là độc lập. (Nguyen et al., 2022) gợi ý rằng các ước tính tích phân Fourier có thể được sử dụng để nắm bắt hiệu quả cấu trúc phụ thuộc giữa các đặc trưng embedding đó. Các ước tính tích phân Fourier có thể được tích hợp vào Transformer-MGK/MLK để tăng khả năng biểu diễn của các mô hình. Trong công trình của chúng tôi, chúng tôi làm cho mean và phương sai của cluster thành biến có thể học và hằng số, tương ứng. Thú vị khi khám phá cách tận dụng cập nhật M-step trong thuật toán EM để cập nhật các tham số đó. Vì thuật toán EM có thể có tỷ lệ hội tụ dưới tuyến tính do sự tách biệt yếu của mean và phương sai (Dwivedi et al., 2020b;a), chúng ta có thể cần phát triển các thuật toán tối ưu hóa hiệu quả hơn so với EM để ước tính mean và phương sai. Các công trình hiện tại của (Ho et al., 2022; Ren et al., 2022) chứng minh rằng một lịch trình tỷ lệ học tăng theo cấp số nhân cho gradient descent có thể được sử dụng cho mean và phương sai để đạt được tỷ lệ hội tụ tuyến tính của các tham số. Việc điều tra hiệu suất của thuật toán đó trong Transformer-MGK/MLK có tầm quan trọng thực tế. Cuối cùng, chúng tôi để lại việc áp dụng MGK/MLK để cải thiện vision transformer (Dosovitskiy et al., 2020; Touvron et al., 2020) như công việc tương lai.

Lời cảm ơn

Tài liệu này dựa trên nghiên cứu được tài trợ bởi AFOSR MURI FA9550-18-1-0502, tài trợ ONR N00014-20-1-2093, MURI N00014-20-1-2787, và NSF dưới Grant# 2030859 cho Computing Research Association cho CIFellows Project (CIF2020-UCLA-38). NH biết ơn sự hỗ trợ từ giải thưởng NSF IFML 2019844 và quà tặng nghiên cứu bởi UT Austin ML grant. RGB được hỗ trợ bởi các tài trợ NSF CCF-1911094, IIS-1838177, IIS-1730574, và Vannevar Bush Faculty Fellowship.

--- TRANG 10 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Tài liệu tham khảo

Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pp. 3159–3166, 2019.

Asai, A. and Choi, E. Challenges in information seeking qa: Unanswerable questions and paragraph retrieval. arXiv preprint arXiv:2010.11915 , 2020.

Bacharoglou, A. G. Approximation of probability distributions by convex mixtures of Gaussian measures. Proceedings of the American Mathematical Society , 138: 2619–2628, 2010.

Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=ByxZX20qFQ.

Bahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.

Bakhtiarnia, A., Zhang, Q., and Iosifidis, A. Multi-exit vision transformer for dynamic inference. arXiv preprint arXiv:2106.15183 , 2021.

Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 , 2020.

Bengio, Y ., Boulanger-Lewandowski, N., and Pascanu, R. Advances in optimizing recurrent networks. In 2013 IEEE international conference on acoustics, speech and signal processing , pp. 8624–8628. IEEE, 2013.

Bishop, C. M. Pattern recognition. Machine learning , 128 (9), 2006.

Brown, T. and et al. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877–1901, 2020.

Cettolo, M., Niehues, J., St ¨uker, S., Bentivogli, L., and Federico, M. Report on the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam , volume 57, 2014.

Chen, B., Liu, Z., Peng, B., Xu, Z., Li, J. L., Dao, T., Song, Z., Shrivastava, A., and Re, C. Mongoose: A learnable lsh framework for efficient neural network training. In International Conference on Learning Representations , 2020.

Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.

Cho, K., van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 1724–1734, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.org/anthology/D14-1179.

Cho, S. M., Park, E., and Yoo, S. Meantime: Mixture of attention mechanisms with multi-temporal embeddings for sequential recommendation. In Fourteenth ACM Conference on Recommender Systems , pp. 515–520, 2020.

Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH.

Clark, K., Khandelwal, U., Levy, O., and Manning, C. D. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 276–286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://www.aclweb.org/ anthology/W19-4828.

Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 , 2019.

Dalvi, F., Sajjad, H., Durrani, N., and Belinkov, Y . Analyzing redundancy in pretrained transformer models. arXiv preprint arXiv:2004.04010 , 2020.

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for

--- TRANG 11 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.

Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905 , 2021.

Durrani, N., Sajjad, H., Dalvi, F., and Belinkov, Y . Analyzing individual neurons in pre-trained language models. arXiv preprint arXiv:2010.02695 , 2020.

Dwivedi, R., Ho, N., Khamaru, K., Wainwright, M. J., Jordan, M. I., and Yu, B. Sharp analysis of expectation-maximization for weakly identifiable models. AISTATS , 2020a.

Dwivedi, R., Ho, N., Khamaru, K., Wainwright, M. J., Jordan, M. I., and Yu, B. Singularity, misspecification, and the convergence rate of EM. Annals of Statistics , 44: 2726–2755, 2020b.

Ethayarajh, K. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512 , 2019.

Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 , 2021.

Ghosal, S. and van der Vaart, A. Entropies and rates of convergence for maximum likelihood and bayes estimation for mixtures of normal densities. Ann. Statist. , 29: 1233–1263, 2001.

Guo, M., Zhang, Y ., and Liu, T. Gaussian transformer: a lightweight approach for natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pp. 6489–6496, 2019.

Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y . Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2021.

Hanson, S. J. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena , 42(1-3):265–272, 1990.

He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 9729–9738, 2020.

Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 2733–2743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1275. URL https://www.aclweb.org/anthology/D19-1275.

Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180 , 2019.

Ho, N., Ren, T., Sanghavi, S., Sarkar, P., and Ward, R. An exponentially increasing step-size for parameter estimation in statistical models. arXiv preprint arXiv: 2205.07999 , 2022.

Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://www.aclweb.org/anthology/P18-1031.

Huang, Y ., Gornet, J., Dai, S., Yu, Z., Nguyen, T., Tsao, D., and Anandkumar, A. Neural networks with recurrent generative feedback. Advances in Neural Information Processing Systems , 33:535–545, 2020.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning , pp. 5156–5165. PMLR, 2020.

Kim, Y ., Denton, C., Hoang, L., and Rush, A. M. Structured attention networks. arXiv preprint arXiv:1702.00887 , 2017.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations , 2015.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451 , 2020.

Le, D. D. and Lauw, H. Efficient retrieval of matrix factorization-based top-k recommendations: A survey of recent approaches. Journal of Artificial Intelligence Research , 70:1441–1479, 2021.

--- TRANG 12 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Le, D. D. and Lauw, H. W. Stochastically robust personalized ranking for lsh recommendation retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp. 4594–4601, 2020.

Lee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and Teh, Y . W. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning , pp. 3744–3753. PMLR, 2019.

Li, H., Yang, Y ., Chen, D., and Lin, Z. Optimization algorithm inspired deep neural network structure design. In Asian Conference on Machine Learning , pp. 614–629. PMLR, 2018.

Lin, Z., Feng, M., dos Santos, C. N., Yu, M., Xiang, B., Zhou, B., and Bengio, Y . A structured self-attentive sentence embedding. CoRR , abs/1703.03130, 2017.

Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198 , 2018.

Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.

Lloyd, S. Least squares quantization in pcm. IEEE transactions on information theory , 28(2):129–137, 1982.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y ., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/P11-1015.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL https://openreview.net/forum? id=Byj72udxe.

Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In Wallach, H., Larochelle, H., Beygelzimer, A., d 'Alch ´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.

Mittal, S., Raparthy, S. C., Rish, I., Bengio, Y ., and Lajoie, G. Compositional attention: Disentangling search and retrieval. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum? id=IwJPj2MBcIa.

Mu, J. and Viswanath, P. All-but-the-top: Simple and effective postprocessing for word representations. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=HkuGJ3kCb.

Nangia, N. and Bowman, S. ListOps: A diagnostic dataset for latent tree learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop , pp. 92–99, New Orleans, Louisiana, USA, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-4013. URL https://www.aclweb.org/ anthology/N18-4013.

Nguyen, T., Ho, N., Patel, A., Anandkumar, A., Jordan, M. I., and Baraniuk, R. G. A Bayesian perspective of convolutional neural networks through a deconvolutional generative model. arXiv preprint arXiv:1811.02657 , 2018.

Nguyen, T., Baraniuk, R., Bertozzi, A., Osher, S., and Wang, B. Momentumrnn: Integrating momentum into recurrent neural networks. Advances in Neural Information Processing Systems , 33:1924–1936, 2020.

Nguyen, T., Nguyen, P., Pham, H., Bui, T., Nguyen, T., and Luong, D. Sp-gpt2: Semantics improvement in vietnamese poetry generation. In 2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA) , pp. 1576–1581. IEEE, 2021a.

Nguyen, T., Pham, M., Nguyen, T., Nguyen, K., Osher, S. J., and Ho, N. Transformer with Fourier integral attentions. arXiv preprint arXiv:2206.00206 , 2022.

Nguyen, T. M., Suliafu, V ., Osher, S. J., Chen, L., and Wang, B. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention. arXiv preprint arXiv:2108.02347 , 2021b.

Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations , 2019.

Parikh, A., T ¨ackstr ¨om, O., Das, D., and Uszkoreit, J. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2249– 2255, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1244. URL https://www.aclweb.org/anthology/D16-1244.

Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning , volume 80 of

--- TRANG 13 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Proceedings of Machine Learning Research , pp. 4055– 4064. PMLR, 10–15 Jul 2018. URL http://proceedings. mlr.press/v80/parmar18a.html.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems , pp. 8024–8035, 2019.

Patel, A. B., Nguyen, T., and Baraniuk, R. G. A probabilistic theory of deep learning. arXiv preprint arXiv:1504.00641 , 2015.

Patel, A. B., Nguyen, M. T., and Baraniuk, R. A probabilistic framework for deep learning. Advances in neural information processing systems , 29:2558–2566, 2016.

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N., and Kong, L. Random feature attention. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB.

Qiu, J., Ma, H., Levy, O., Yih, S. W.-t., Wang, S., and Tang, J. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972 , 2019.

Radev, D. R., Muthukrishnan, P., Qazvinian, V ., and AbuJbara, A. The acl anthology network corpus. Language Resources and Evaluation , 47(4):919–944, 2013.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. OpenAI report , 2018.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383– 2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/anthology/D16-1264.

Ren, T., Cui, F., Sanghavi, S., and Ho, N. Beyond EM algorithm on over-specified two-component location-scale Gaussian mixtures. arXiv preprint arXiv: 2205.11078 , 2022.

Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics , 9:53–68, 2021. doi: 10.1162/tacl a00353. URL https://www.aclweb.org/anthology/2021.tacl-1.4.

Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. Poor man's bert: Smaller and faster transformer models. arXiv eprints , pp. arXiv–2004, 2020.

Sander, M. E., Ablin, P., Blondel, M., and Peyr ´e, G. Momentum residual neural networks. In International Conference on Machine Learning , pp. 9276–9287. PMLR, 2021.

Sanh, V ., Debut, L., Chaumond, J., and Wolf, T. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.

Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning , pp. 9355–9366. PMLR, 2021.

Shazeer, N. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 , 2019.

Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pp. 3531–3539, 2021.

So, D. R., Liang, C., and Le, Q. V . The evolved transformer. arXiv preprint arXiv:1901.11117 , 2019.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research , 15(1):1929–1958, 2014.

Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., and Joulin, A. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470 , 2019.

Sun, S., Cheng, Y ., Gan, Z., and Liu, J. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355 , 2019.

Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learning. In International conference on machine learning , pp. 1139–1147. PMLR, 2013.

Tay, Y ., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse Sinkhorn attention. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp. 9438–9447. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/tay20a. html.

Tay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.

--- TRANG 14 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations , 2021. URL https://openreview.net/forum?id= qVyeW-grC2k.

Tenney, I., Das, D., and Pavlick, E. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4593–4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https://www.aclweb.org/ anthology/P19-1452.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J ´egou, H. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877 , 2020.

Tsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. arXiv preprint arXiv:1908.11775 , 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems , pp. 5998–6008, 2017.

Vig, J. and Belinkov, Y . Analyzing the structure of attention in a transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 63–76, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4808. URL https://www.aclweb.org/anthology/W19-4808.

Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 5797–5808, Florence, Italy, July 2019a. Association for Computational Linguistics. doi: 10.18653/v1/P19-1580. URL https://www.aclweb.org/anthology/P19-1580.

Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418 , 2019b.

Wang, B., Xia, H., Nguyen, T., and Osher, S. How does momentum benefit deep neural networks architecture design? a few case studies. arXiv preprint arXiv:2110.07034 , 2021a.

Wang, B., Nguyen, T., Sun, T., Bertozzi, A. L., Baraniuk, R. G., and Osher, S. J. Scheduled restart momentum for accelerated stochastic gradient descent. SIAM Journal on Imaging Sciences , 15(2):738–761, 2022.

Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020a.

Wang, X., Yu, F., Dou, Z.-Y ., Darrell, T., and Gonzalez, J. E. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European Conference on Computer Vision (ECCV) , pp. 409–424, 2018a.

Wang, Y ., Nguyen, T., Zhao, Y ., Wang, Z., Lin, Y ., and Baraniuk, R. Energynet: Energy-efficient dynamic inference. 2018b.

Wang, Y ., Shen, J., Hu, T.-K., Xu, P., Nguyen, T., Baraniuk, R., Wang, Z., and Lin, Y . Dual dynamic inference: Enabling more efficient, adaptive, and controllable deep inference. IEEE Journal of Selected Topics in Signal Processing , 14(4):623–633, 2020b.

Wang, Y ., Huang, R., Song, S., Huang, Z., and Huang, G. Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition. Advances in Neural Information Processing Systems , 34, 2021b.

Williams, A., Nangia, N., and Bowman, S. A broadcoverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 1112–1122, June 2018. doi: 10.18653/v1/N18-1101. URL https: //www.aclweb.org/anthology/N18-1101.

Xia, H., Suliafu, V ., Ji, H., Nguyen, T., Bertozzi, A., Osher, S., and Wang, B. Heavy ball neural ordinary differential equations. Advances in Neural Information Processing Systems , 34, 2021.

Xiong, Y ., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y ., and Singh, V . Nystr ¨omformer: A Nystr ¨om-based Algorithm for Approximating Self-Attention. 2021.

Yang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov, R., and Le, Q. V . Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237 , 2019.

Zhang, S. and Feng, Y . Modeling concentrated crossattention for neural machine translation with Gaussian mixture model. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pp. 1401–1411, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.findings-emnlp.121. URL https://aclanthology. org/2021.findings-emnlp.121.

--- TRANG 15 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Phụ lục cho "Transformer với Hỗn hợp Khóa Gaussian"

Trong tài liệu phụ này, chúng tôi cung cấp chi tiết thực nghiệm và các thí nghiệm bổ sung của Transformer-MGK và Transformer-MLK.

A. Thí nghiệm Bổ sung

A.1. Chi tiết thí nghiệm

Trong phần này, chúng tôi cung cấp chi tiết mô hình và huấn luyện cho các thí nghiệm trong Phần 3. Tất cả các thí nghiệm của chúng tôi được tiến hành trên máy chủ với 4 GPU NVIDIA A100.

A.1.1. BENCHMARK LONG RANGE ARENA

Dataset và chỉ số Chúng tôi xem xét các nhiệm vụ sau trong benchmark LRA: Listops (Nangia & Bowman, 2018), phân loại văn bản đánh giá IMDb cấp byte (Maas et al., 2011), và retrieval tài liệu cấp byte (Radev et al., 2013). Các nhiệm vụ này liên quan đến chuỗi dài có độ dài lần lượt là 2K, 4K, và 4K. Chúng tôi tuân theo thiết lập/giao thức đánh giá trong (Tay et al., 2021) và báo cáo độ chính xác kiểm tra cho từng nhiệm vụ và kết quả trung bình trên tất cả các nhiệm vụ.

Mô hình và cơ sở Chúng tôi sử dụng transformer softmax (Vaswani et al., 2017) và transformer tuyến tính (Katharopoulos et al., 2020) làm cơ sở. Tất cả các mô hình có 2 lớp, 64 chiều embedding, và 128 chiều ẩn. Số đầu trong mỗi lớp được đặt thành 1, 2, 4, và 8. Đối với Transformer-MGK/MLK và các phiên bản dịch chuyển của chúng, chúng tôi chia sẻ λjr cho tất cả vị trí j và học nó cho mỗi đầu. Giá trị khởi tạo cho mỗi λjr được đặt thành 0.5. Đối với Transformer-sMGK, chúng tôi học br và khởi tạo các phần tử của nó từ phân phối chuẩn. Mỗi σ²jr là hằng số với giá trị √D trong đó D là chiều của mỗi đầu, giống như trong các mô hình cơ sở.

Chi tiết về các benchmark Long Range Arena (LRA) có thể được tìm thấy trong bài báo gốc (Tay et al., 2021). Triển khai của chúng tôi dựa trên mã công khai bởi (Xiong et al., 2021), và chúng tôi tuân theo quy trình huấn luyện của họ. Thiết lập huấn luyện và chi tiết mô hình cơ sở bổ sung được cung cấp trong tệp cấu hình được sử dụng trong (Xiong et al., 2021) và có sẵn tại https://github.com/mlpen/Nystromformer/blob/main/LRA/code.

A.1.2. MÔ HÌNH HÓA NGÔN NGỮ TRÊN WIKITEXT-103

Dataset và chỉ số WikiText-103 bao gồm các bài viết từ Wikipedia và là dataset với phụ thuộc ngữ cảnh dài. Tập huấn luyện được tạo thành từ khoảng 28K bài viết chứa 103M từ chạy; điều này tương ứng với các khối văn bản khoảng 3600 từ. Tập validation và test được tạo thành từ 218K và 246K từ chạy, tương ứng. Mỗi tập chứa 60 bài viết và khoảng 268K từ. Thí nghiệm của chúng tôi tuân theo thiết lập tiêu chuẩn (Merity et al., 2017; Schlag et al., 2021) và chia dữ liệu huấn luyện thành các đoạn dài L-từ độc lập. Để đánh giá, chúng tôi sử dụng kích thước batch là 1, và đi qua chuỗi văn bản với cửa sổ trượt kích thước L. Chúng tôi chỉ xem xét vị trí cuối cùng để tính perplexity (PPL) ngoại trừ trong đoạn đầu tiên, nơi tất cả các vị trí được đánh giá như trong (Al-Rfou et al., 2019; Schlag et al., 2021).

Mô hình và cơ sở Triển khai mô hình hóa ngôn ngữ của chúng tôi dựa trên mã công khai https://github.com/IDSIA/lmtool-fwp bởi (Schlag et al., 2021). Chúng tôi sử dụng cấu hình mô hình small và medium của họ cho các mô hình trong thí nghiệm của chúng tôi. Đặc biệt, đối với các mô hình small, chúng tôi đặt chiều key, value, và query thành 128, và độ dài ngữ cảnh huấn luyện và đánh giá thành 256. Đối với các mô hình medium, chúng tôi đặt chiều key, value, và query thành 256, và độ dài ngữ cảnh huấn luyện và đánh giá thành 384. Trong cả hai cấu hình, chúng tôi đặt số đầu thành 8, chiều lớp feed-forward thành 2048, và số lớp thành 16. Đối với Transformer-MGK/MLK, chúng tôi sử dụng phiên bản 4 và 8-head để so sánh với các cơ sở 8-head. τir, br và λjr cho nhiệm vụ này tuân theo thiết lập của chúng tôi cho các thí nghiệm LRA. Ngoài những điều đó, mô hình hóa ngôn ngữ của chúng tôi chia sẻ cùng cấu hình với các cơ sở.

Chúng tôi huấn luyện các mô hình của mình cho mô hình hóa ngôn ngữ trên 2 A100, mỗi cái 40GB với kích thước batch 96, và mỗi mô hình được huấn luyện trong 120 epoch. Chúng tôi áp dụng 10% dropout (Hanson, 1990; Srivastava et al., 2014) và sử dụng bộ tối ưu Adam (Kingma & Ba, 2015) với tỷ lệ học ban đầu 0.00025 và 2000 bước cho việc khởi động tỷ lệ học.

A.1.3. DỊCH MÁY THẦN KINH TRÊN IWSLT'14 ĐỨC SANG TIẾNG ANH

Dataset và chỉ số Dataset IWSLT14 Đức-Anh (Cettolo et al., 2014) chứa 153K huấn luyện, 7K validation, và 7K test các câu Đức-Anh đã dịch của kịch bản TED-talks. Chúng tôi tuân theo các bước tiền xử lý giống như trong (Ott

--- TRANG 16 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

et al., 2019). Chúng tôi sử dụng điểm BiLingual Evaluation Understudy (BLEU) làm chỉ số đánh giá cho nhiệm vụ này. Tất cả các mô hình được huấn luyện được đánh giá sử dụng giao thức đánh giá trong (Ott et al., 2019).

Mô hình và cơ sở Mô hình cơ sở chúng tôi sử dụng trong nhiệm vụ dịch máy này là một transformer encoder-decoder với sáu lớp encoder/decoder, bốn đầu chú ý mỗi lớp. Chiều embedding và kích thước ẩn lần lượt là 512 và 1024, cho cả encoder và decoder. Các cấu hình kiến trúc này giống nhau cho Transformer-MGK/sMGK ngoại trừ số đầu chú ý mỗi lớp, được giảm đi một nửa. Chúng tôi chia sẻ λjr trên tất cả vị trí j và học nó cho mỗi đầu. Giá trị khởi tạo cho mỗi λjr được đặt thành 0.5. Đối với Transformer-sMGK, chúng tôi học br và khởi tạo các phần tử của nó từ phân phối chuẩn. Mỗi σ²jr là hằng số với giá trị √D trong đó D là chiều của mỗi đầu, giống như trong các mô hình cơ sở. Các thí nghiệm của chúng tôi tuân theo thiết lập từ (Ott et al., 2019), và triển khai của chúng tôi dựa trên mã công khai https://github.com/pytorch/fairseq/tree/main/examples/translation.

A.2. Phân tích Thực nghiệm khác của Transformer-MGKs/MLKs được huấn luyện cho Mô hình hóa Ngôn ngữ

Trong Bảng 3 trong văn bản chính, chúng tôi hiển thị các cải thiện trong perplexity valid và test của Transformer-MGKs/MLKs của chúng tôi so với transformer softmax và tuyến tính cơ sở. Đặc biệt, Transformer-MGK/MLKs với cùng số đầu như các cơ sở, ví dụ 8 đầu, cải thiện đáng kể các cơ sở trong quá trình huấn luyện trong khi Transformer-MGK/MLKs với 4 đầu đạt được hiệu suất có thể so sánh hoặc tốt hơn so với các cơ sở 8-head. Trong phần này, chúng tôi cung cấp thêm phân tích thực nghiệm để làm sáng tỏ những kết quả đó. Hình 5 hiển thị đường cong perplexity validation của các mô hình chúng tôi so với transformer softmax và tuyến tính.

Hình 6 trực quan hóa các ma trận chú ý từ một mẫu được chọn ngẫu nhiên cho transformer softmax được huấn luyện với 8 đầu và Transformer-MGKs với 8 đầu và 4 đầu. Các trực quan này cho thấy rằng Transformer-MGKs chú ý đến các vị trí đa dạng hơn trong tất cả các đầu và lớp so với cơ sở chú ý softmax. Chúng tôi cũng so sánh phân phối rank của các ma trận chú ý này được tính từ 1000 mẫu tại mỗi lớp trong mô hình. Hình 7 trình bày biểu đồ rank của chú ý softmax 8-head và chú ý MGK 4-head và 8-head cho lớp thứ 1 và thứ 5. Rõ ràng rằng ma trận chú ý từ Transformer-MGKs có rank cao hơn so với những ma trận trong transformer softmax, điều này ngụ ý rằng Transformer-MGK có thể chú ý đến các vùng đa dạng hơn so với transformer softmax mà không cần sử dụng nhiều đầu chú ý hơn.

Hình 5. Validation perplexity của Transformer-MGK so với transformer softmax (Trái) và Transformer-MLK so với transformer tuyến tính (Phải) cho mô hình hóa ngôn ngữ trên WikiText-103.

A.3. Kết quả huấn luyện bổ sung cho LRA

Trong phần này, chúng tôi cung cấp kết quả thực nghiệm bổ sung trên benchmark LRA. Hình 8 so sánh chi phí tính toán được đo bằng FLOP và độ phức tạp mô hình tính bằng số tham số của các phương pháp suy luận và học khác nhau cho Transformer-MGK. Chi phí tính toán của Transformer-MGK/sMGK và Transformer-MGK Hard-E/Soft-E ngang bằng nhau, trong khi Transformer-sMGK sử dụng ít tham số hơn so với các mô hình khác mà không có sự đánh đổi về hiệu suất 5 cho tất cả các nhiệm vụ. Việc đặt tên được giải thích như trong Phần 3.4 trong văn bản chính. Ngoài ra, Hình 9 trực quan hóa các ma trận chú ý trong transformer tuyến tính 4-head cơ sở, Transformer-MLK 4-head, và Transformer-MLK 2-head được huấn luyện trên nhiệm vụ document retrieval.

--- TRANG 17 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Softmax 8 HeadsMGK 8 HeadsMGK 4 HeadsLayer 1Layer 2Layer 3Layer 1Layer 2Layer 3Layer 1Layer 2Layer 3Head 1Head 2Head 3Head 4Head 5Head 6Head 7Head 8

Hình 6. Trực quan hóa ma trận chú ý từ transformer softmax 8-head (Trái), Transformer-MGK 8-head (Giữa), và Transformer-MGK 4-head (Phải) được huấn luyện trên mô hình hóa ngôn ngữ WikiText-103. Ở đây, chúng tôi vẽ ma trận chú ý cho tất cả các đầu và lớp trong các mô hình. Độ dài chuỗi là 256 và kích thước của mỗi ma trận là 256 × 256.

CountLayer 1Layer 5RankSoftmax 8 HeadsMGK 8 HeadsMGK 4 Heads

Hình 7. Phân phối rank của ma trận chú ý từ transformer softmax 8-head (Trái), Transformer-MGK 8-head (Giữa), và Transformer-MGK 4-head (Phải) được huấn luyện trên mô hình hóa ngôn ngữ WikiText-103. Biểu đồ rank được tính từ 1000 ma trận chú ý tại mỗi lớp. Ma trận chú ý từ Transformer-MGKs có rank cao hơn so với những ma trận từ transformer softmax. Điều này ngụ ý rằng Transformer-MGKs có các mẫu chú ý đa dạng hơn, cho phép chúng tôi giảm số đầu trong Transformer-MGKs.

A.4. Nghiên cứu Loại bỏ về Tác động của Hỗn hợp Khóa, Khoảng cách Gaussian, và Dịch chuyển Khóa

Trong phần này, chúng tôi tiến hành nghiên cứu loại bỏ của Transformer-MGK trên nhiệm vụ retrieval LRA để điều tra nguồn gốc của việc cải thiện hiệu suất. Đặc biệt, chúng tôi muốn hiểu tác động của các yếu tố sau đến hiệu suất của Transformer-MGK: 1) hỗn hợp khóa, 2) khoảng cách Gaussian, và 3) dịch chuyển khóa. Chúng tôi tóm tắt kết quả thực nghiệm của mình trong Bảng 6 và thảo luận về tác động của 1, 2 và 3 dưới đây.

--- TRANG 18 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

         Number of HeadsFLOPsParameters (1e5)RetrievalListopsText
Transformer-sMGKTransformer-MGK Hard-ETransformer-MGK
Transformer-MGK Soft-E

Hình 8. Độ phức tạp mô hình (Trên) và chi phí tính toán (Dưới) của các phương pháp suy luận và học khác nhau cho Transformer-MGK được huấn luyện trên nhiệm vụ document retrieval. Trong khi chi phí tính toán gần như giống nhau, Transformer-sMGK có nhiều lợi thế hơn về kích thước mô hình, so với Transformer-MGK, Transformer-MGK Hard-E, và Soft-E. Việc đặt tên được giải thích như trong Phần 3.4 trong văn bản chính.

1K 2.5Kk 4K1K
2.5
4Klayer 1
1K 2.5Kk 4K1K
2.5
4Klayer 2
1K 2.5Kk 4K1K
2.5
4K
1K 2.5Kk 4K1K
2.5
4K
1K 2.5K 4K1K
2.5k
4Klayer 1
1K 2.5K 4K1K
2.5k
4Klayer 2
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4Khead 1layer 1
1K 2.5K 4K1K
2.5k
4Klayer 2
1K 2.5K 4K1K
2.5k
4Khead 2
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4Khead 3
1K 2.5K 4K1K
2.5k
4K
1K 2.5K 4K1K
2.5k
4Khead 4
1K 2.5K 4K1K
2.5k
4K0.60.70.80.91.0
Linear 4 Heads MLK 4 Heads MLK 2 Heads

Hình 9. Trực quan hóa ma trận chú ý trong transformer tuyến tính 4-head cơ sở (Trái), Transformer-MLK 4-head (Giữa), và Transformer-MLK 2-head (Phải) được huấn luyện trên nhiệm vụ document retrieval. Ở đây, độ dài chuỗi là 4000, và kích thước của mỗi ma trận là 4000×4000.

Tác động của Hỗn hợp Khóa Chúng tôi áp dụng phương pháp hỗn hợp khóa (MGK) của chúng tôi cho transformer softmax sử dụng tích vô hướng giữa truy vấn và khóa thay vì khoảng cách Gaussian như trong bài báo của chúng tôi. Chúng tôi đặt tên mô hình này là Softmax MGK. Chúng tôi so sánh Softmax MGK có 1 đầu (Sofmax MGK 1 head trong Bảng 6) với transformer softmax cơ sở sử dụng 1 và 2 đầu (Softmax 1 head và Softmax 2 heads trong Bảng 6). Kết quả trong Bảng 6 cho thấy rằng Softmax MGK 1 head vượt trội hơn cả transformer softmax cơ sở 1 và 2 đầu. Lưu ý rằng Softmax MGK 1 head của chúng tôi hiệu quả hơn so với cơ sở 2 đầu về số tham số và số FLOP. Những kết quả này xác nhận lợi ích của việc sử dụng MGK.

Tác động của việc sử dụng khoảng cách Gaussian Tiếp theo, chúng tôi so sánh softmax MGK với Gaussian MGK. Ở đây Gaussian MGK là Transformer-MGK được đề xuất và thảo luận trong bài báo của chúng tôi, tính toán điểm chú ý sử dụng phương pháp MGK và khoảng cách Gaussian giữa truy vấn và khóa. Kết quả trong Bảng 6 gợi ý rằng Gaussian MGK 1

--- TRANG 19 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 6. Nghiên cứu loại bỏ về tác động của hỗn hợp khóa, khoảng cách Gaussian, và dịch chuyển khóa trên nhiệm vụ retrieval LRA. Chúng tôi ký hiệu Transformer softmax bằng Softmax. Tất cả các mô hình Softmax (tức là, Softmax 2 heads, Softmax 1 head, Softmax MGK 1 head, và Softmax sMGK 1 head) sử dụng tích vô hướng để tính điểm chú ý. Tất cả các mô hình Gaussian (tức là, Gaussian MGK 1 head, Gaussian sMGK 1 head, và Gaussian 1 head) sử dụng khoảng cách Gaussian để tính điểm chú ý. Chúng tôi ký hiệu MGK với dịch chuyển khóa bằng sMGK. Ở đây MGK được sử dụng để ký hiệu phương pháp của chúng tôi sử dụng hỗn hợp khóa tại mỗi timestep.

Method Accuracy (%)
Softmax 2 heads 79.10
Softmax sMGK 1 head 79.81
Softmax MGK 1 head 79.23
Softmax 1 head 77.90
Gaussian sMGK 1 head 81.23
Gaussian MGK 1 head 80.63
Gaussian 1 head 80.38

head cải thiện so với Softmax MGK 1 head (80.63% so với 79.23%). Kết quả này biện minh cho lợi thế của việc sử dụng khoảng cách Gaussian so với tích vô hướng để tính điểm chú ý.

Tác động của dịch chuyển khóa Cuối cùng, chúng tôi áp dụng dịch chuyển khóa cho cả Softmax MGK và Gaussian MGK (Softmax sMGK và Gaussian sMGK trong Bảng 6). Từ Bảng 6, chúng tôi quan sát rằng Softmax sMGK 1 head và Gaussian sMGK 1 head vượt trội hơn Softmax MGK 1 head và Gaussian MGK 1 head, tương ứng. Những kết quả này, một lần nữa, chứng thực lợi ích của việc sử dụng dịch chuyển khóa.

Chúng tôi cũng bao gồm kết quả cho mô hình Gaussian 1 head trong Bảng 6. Mô hình Gaussian 1 head này tương tự như mô hình Softmax 1 head nhưng sử dụng khoảng cách Gaussian để tính điểm chú ý. So sánh kết quả của mô hình Gaussian 1 head, mô hình Softmax 1 head, và mô hình Gaussian MGK 1 head được báo cáo trong Bảng 6 tiếp tục xác nhận lợi thế của việc sử dụng MGK và khoảng cách Gaussian.

A.5. Phân tích hiệu quả trên WikiText-103

Phân tích hiệu quả trên WikiText-103 Trong phần này, chúng tôi cung cấp chi tiết lớn hơn về các chỉ số và kết quả của phân tích hiệu quả mô hình của chúng tôi. Mở rộng phương pháp của chúng tôi cho Linear Transformer (Transformer-MLK) có lợi ích đáng kể cho hiệu quả của cơ sở tuyến tính, làm cho nó đặc biệt áp dụng được cho các nhiệm vụ tầm xa và quy mô lớn.

Chỉ số phân tích Phân tích của chúng tôi báo cáo số FLOP như hiệu quả tính toán mô hình cho cả huấn luyện và suy luận. Số tham số tổng và không embedding được sử dụng để đo độ phức tạp mô hình. Vì mục tiêu của phương pháp chúng tôi là giảm số tham số trong mô hình chính, không bao gồm số tham số input-embedding. Do đó, việc cung cấp việc giảm tham số không embedding như một tiêu chí cho hiệu quả kích thước mô hình cũng rất quan trọng. Ở đây, chúng tôi điều tra lợi ích tính toán và bộ nhớ của mô hình chúng tôi trong các nhiệm vụ quy mô lớn thông qua việc tăng D ∈ {256, 512, 1024, 2048, 4096} và N ∈ {128, 256, 512, 1024, 2048, 4096, 8192}. Để tính toán FLOP, chúng tôi sử dụng fvcore. Tất cả các phép đo được tính khi chạy mô hình qua dữ liệu với kích thước batch 1.

Transformer-MLK giảm độ phức tạp mô hình và chi phí tính toán

Hình 10A và Hình 10B hiển thị tỷ lệ giảm FLOP giữa Transformer-MLK 4-head so với cơ sở Linear 8-head như hàm của chiều mô hình D và độ dài chuỗi N. Hình 10C trình bày lợi ích kích thước mô hình (tỷ lệ số tham số tổng/không embedding) giữa mô hình của chúng tôi và cơ sở khi D tăng. Những hình này chỉ ra rằng khi chúng ta mở rộng các nhiệm vụ và mô hình, Transformer-MLK của chúng tôi có lợi thế đáng kể hơn so với cơ sở Linear.

A.6. Phân tích Chi phí Thời gian và Dấu chân Bộ nhớ

Bảng 7 so sánh dấu chân bộ nhớ GPU và chi phí thời gian tính toán (giây/lần lặp) của Transformer-MGKs/MLKs 4-head của chúng tôi với những mô hình của transformer softmax/tuyến tính 8-head cơ sở tại thời điểm kiểm tra. Tất cả các mô hình được huấn luyện trên nhiệm vụ retrieval LRA, và chúng tôi sử dụng kích thước batch 32 cho mỗi lần lặp. Các mô hình MGK/MLK của chúng tôi tiết kiệm bộ nhớ và giảm thời gian wall-clock đáng kể so với các cơ sở. Sử dụng dịch chuyển khóa trong các mô hình của chúng tôi, tức là Transformer-sMGKs/sMLKs, giúp cải thiện hiệu quả thêm.

--- TRANG 20 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Sequence Length Model Dimension Model Dimension 2565121024 2048 4096 
Training Testing FLOPS Ratio 
Total parameters 
Non-embedding 
parameters 
Parameters Ratio ￼ 
￼ 
￼ 
A B C

Hình 10. Tỷ lệ FLOP huấn luyện (A), Suy luận (B) và tỷ lệ số tham số (C) giữa Transformer-MLK 4-head với cơ sở Linear 8-head trên các chiều mô hình D khác nhau và độ dài chuỗi N, cho nhiệm vụ mô hình hóa ngôn ngữ trên WikiText-103. Transformer-MLK 4-head giảm đáng kể tính toán (cả FLOP huấn luyện và suy luận) và độ phức tạp mô hình (cả số tham số tổng và không embedding), so với cơ sở khi chúng ta mở rộng chiều mô hình và độ dài chuỗi, cho thấy lợi thế của Transformer-MLK cho các nhiệm vụ tầm xa và quy mô lớn.

Bảng 7. So sánh dấu chân bộ nhớ GPU và chi phí thời gian tính toán (giây/lần lặp) giữa Transformer-MGKs/MLKs 4-head của chúng tôi và transformer softmax/tuyến tính 8-head cơ sở được huấn luyện trên nhiệm vụ retrieval LRA tại thời điểm kiểm tra. Transformer-MGKs/MLKs của chúng tôi tiết kiệm nhiều bộ nhớ hơn và có thời gian wall-clock nhỏ hơn đáng kể so với các cơ sở. Ở đây chúng tôi sử dụng kích thước batch 32 cho mỗi lần lặp.

Method Memory (Gb) Time overhead (seconds/iteration)
Softmax 8 heads 58.00 0.357
Transformer-MGK 4 heads 53.58 0.278
Transformer-sMGK 4 heads 45.50 0.227
Linear 8 heads 3.38 0.055
Transformer-MLK 4 heads 2.90 0.043
Transformer-sMLK 4 heads 2.83 0.042

Bảng 8. Hệ số trộn đã học λjr của tất cả các đầu và lớp trong Transformer-MGKs 1-head được huấn luyện trên nhiệm vụ retrieval LRA. Ở đây chúng tôi sử dụng cùng λj1, λj2 cho tất cả bước thời gian j = 1,...,N.

Method Layer 1 Layer 2
λj1 λj2 λj1 λj2
Transformer-sMGK 1 heads 0.488 0.512 0.500 0.500
Transformer-MGK 1 heads 0.502 0.498 0.497 0.503

A.7. Đường cong Học hiển thị Hội tụ

Trong phần này, chúng tôi vẽ lại Hình 1 và 5 để hiển thị sự hội tụ của các huấn luyện của chúng tôi. Trong Hình 1, kết quả không có vẻ hội tụ vì chúng tôi vẽ loss và độ chính xác theo thang log. Trong Hình 4, kết quả không có vẻ hội tụ vì chúng tôi phóng to vào phạm vi cụ thể trên trục y và x. Hình 11 và 12 là các phiên bản được vẽ lại của Hình 1 và 5, tương ứng. Trong Hình 11, các đường cong loss/độ chính xác huấn luyện dừng sớm vì chúng tôi sử dụng early stopping để tránh overfitting. Các đường cong loss/độ chính xác kiểm tra trong hình này đã hội tụ.

A.8. Ma trận Trọng số của Khóa, Khóa và Hệ số Trộn

Trong phần này, chúng tôi phân tích λjr, kjr, và WKr đã học, j = 1,...,N và r = 1,...,M trong Transformer-MGK được huấn luyện trên nhiệm vụ retrieval LRA. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi đặt M = 2. Trong Hình 13 và 14, chúng tôi trực quan hóa ma trận trọng số

--- TRANG 21 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Hình 11. Loss/độ chính xác huấn luyện và kiểm tra của Transformer-MGK so với transformer softmax (Trái) và Transformer-MLK so với Transformer tuyến tính (Phải) trên nhiệm vụ retrieval, có độ dài chuỗi trung bình dài nhất và khoảng chú ý trong các nhiệm vụ LRA (Tay et al., 2021). Trong huấn luyện, chúng tôi áp dụng early stopping để tránh overfitting. Điều đó giải thích tại sao các đường cong loss và độ chính xác huấn luyện dừng sớm. Các đường cong loss/độ chính xác kiểm tra đã hội tụ. Hiệu suất ấn tượng của Transformer-MGK/MLK trên nhiệm vụ thách thức này xác nhận khả năng của các mô hình chúng tôi để nắm bắt phụ thuộc tầm xa thông qua việc học đa dạng các mẫu chú ý.

Hình 12. Validation perplexity của Transformer-MGK so với transformer softmax (Trái) và Transformer-MLK so với transformer tuyến tính (Phải) cho mô hình hóa ngôn ngữ trên WikiText-103. Huấn luyện hội tụ trên nhiệm vụ này sau 500000 lần lặp, tương đương với 115 epoch.

WK tính toán các khóa và các khóa K, tương ứng, cho tất cả các đầu và lớp trong transformer softmax 2-head cơ sở, Transformer-MGK 1-head với 2 khóa, và Transformer-sMGK 1-head với 2 khóa được huấn luyện trên nhiệm vụ retrieval LRA. Lưu ý rằng đối với các khóa, chúng tôi chỉ vẽ 100 token đầu tiên. Ngoài ra, Bảng 8 tóm tắt hệ số trộn đã học λjr của tất cả các đầu và lớp trong Transformer-MGK 1-head được huấn luyện trên cùng nhiệm vụ retrieval. Ở đây chúng tôi sử dụng cùng λj1, λj2 cho tất cả bước thời gian j = 1,...,N.

A.9. Phân tích Độ phức tạp Tính toán (FLOP) Bổ sung tại Thời điểm Huấn luyện

Hình 15 chứng minh chi phí tính toán (FLOP) cho mỗi lần lặp huấn luyện của Transformer-MGK so với transformer softmax cơ sở (Trái) và Transformer-MLK so với transformer tuyến tính cơ sở (Phải) trên nhiệm vụ retrieval LRA. Lợi thế hiệu quả của Transformer-MGK/MLK so với các cơ sở tăng theo số đầu.

--- TRANG 22 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Layer 1Layer 2Layer 1Softmax 2 HeadsHead 2Head 1MGK 1 HeadsMGK 1 HeadWeight Key 1Weight Key 2
Weight Key1Layer 2Layer 1Layer 2

Hình 13. Ma trận trọng số WK để tính toán các khóa, cho tất cả các đầu và lớp, trong transformer softmax 2-head cơ sở (Trái), Transformer-MGK 1-head với 2 khóa (Giữa), và Transformer-sMGK 1-head với 2 khóa (Phải) được huấn luyện trên nhiệm vụ retrieval LRA. Ở đây, chiều của mỗi đầu D = 32 và của đầu vào xi là Dx = 64. Do đó, mỗi ma trận trọng số có hình dạng (64, 32).

Layer 1Layer 2Layer 1Softmax 2 headsHead 2Head 1MGK 1 HeadsMGK 1 HeadKey 1Key 2
Key 1Layer 2Layer 1Layer 2
Key 2

Hình 14. Embedding khóa K cho tất cả các đầu và lớp của transformer softmax 2-head cơ sở (Trái), Transformer-MGK 1-head với 2 khóa (Giữa), và Transformer-sMGK 1-head với 2 khóa (Phải) được huấn luyện trên nhiệm vụ retrieval LRA. Ở đây chiều D của mỗi đầu là 32, và chúng tôi vẽ embedding khóa của 100 token đầu tiên trong một chuỗi được chọn ngẫu nhiên. Do đó, mỗi ma trận khóa có hình dạng (100, 32).

Hình 16 hiển thị chi phí tính toán mỗi lần lặp huấn luyện (được đo bằng FLOP) của các phương pháp suy luận và học khác nhau cho Transformer-MGK được huấn luyện trên nhiệm vụ document retrieval. Transformer-sMGK, Transformer-MGK, Transformer-MGK Hard-E, và Soft-E có chi phí tính toán tương tự.

A.10. Mở rộng đến Mô hình Cơ sở 12-Head cho nhiệm vụ Retrieval.

Để tiếp tục nghiên cứu khả năng mở rộng của mô hình chúng tôi, trong phần này, chúng tôi điều tra hiệu suất của Transformer-MGKs/MLKs 6-head của chúng tôi so với transformer softmax/tuyến tính 12-head cơ sở trên nhiệm vụ retrieval. Bảng 9 chỉ ra

--- TRANG 23 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Hình 15. Chi phí tính toán (FLOP) cho mỗi lần lặp huấn luyện của Transformer-MGK so với transformer softmax cơ sở (Trái) và Transformer-MLK so với transformer tuyến tính cơ sở (Phải) trên nhiệm vụ retrieval LRA. Lợi thế hiệu quả của Transformer-MGK/MLK so với các cơ sở tăng theo số đầu. Ở đây, kích thước batch của mỗi lần lặp huấn luyện là 32.

         Number of HeadsFLOPsRetrievalListopsText
Transformer-sMGKTransformer-MGK Hard-ETransformer-MGK
Transformer-MGK Soft-E

Hình 16. Chi phí tính toán (được đo bằng FLOP) mỗi lần lặp huấn luyện của các phương pháp suy luận và học khác nhau cho Transformer-MGK được huấn luyện trên nhiệm vụ retrieval LRA. Chi phí tính toán gần như giống nhau cho Transformer-sMGK, Transformer-MGK, Transformer-MGK Hard-E, và Soft-E. Việc đặt tên được giải thích như trong Phần 3.4 trong văn bản chính.

Bảng 9. Độ Chính xác Kiểm tra (%) của Transformer-MGKs/MLKs 6-head so với transformer softmax và tuyến tính 12-head cơ sở trên nhiệm vụ retrieval. Transformer-MGKs/MLKs 6-head của chúng tôi vượt trội đáng kể so với transformer softmax và tuyến tính, tương ứng, trong khi hiệu quả hơn về chi phí tính toán, kích thước mô hình, và sử dụng bộ nhớ.

Method Accuracy (%)
Softmax 12 heads 82.18
Transformer sMGK 6 head 83.31
Transformer MGK 6 head 83.05
Linear sMGK 12 head 81.97
Transformer sMLK 6 head 82.80
Transformer MLK 6 head 82.11

rằng Transform-MGKs/MLKs 6-head của chúng tôi vượt trội đáng kể so với transformer softmax/tuyến tính 12-head, tương ứng. Hơn nữa, so sánh những kết quả này với những kết quả trong Bảng 1 và Bảng 2, mặc dù transformer softmax/tuyến tính 12-head cải thiện so với các mô hình 8-head, độ chính xác của chúng vẫn tệ hơn hoặc chỉ tương đương với những mô hình Transformer-MGK/MLK 4-head và thậm chí 2-head của chúng tôi.

A.11. So sánh với Multi-query Attention

Trong phần này, chúng tôi so sánh phương pháp MGK của chúng tôi với multi-query attention (Shazeer, 2019). Multi-query attention chia sẻ cùng một tập khóa và giá trị tại các đầu khác nhau để giảm chi phí băng thông bộ nhớ trong quá trình suy luận tăng dần, cho phép suy luận nhanh hơn vì kích thước của các tensor "keys" và "values" được tải lại được giảm đáng kể. Mặt

--- TRANG 24 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Bảng 10. Perplexity (PPL) trên Wikitext-103 của Multi-query Transformer-MGK 4-head và cơ sở Multi-query Transformer 8-head. Kết hợp multi-query attention với transformer-MGK dẫn đến hiệu suất kiểm tra tốt hơn so với cơ sở trong khi hiệu quả hơn vì Transformer-MGK cho phép mô hình giảm số đầu đi một nửa.

Method Valid PPL Test PPL
Multi-query attention 8 heads (small) 35.08 36.03
Multi-query Transformer MGK 4 head (small) 35.16 35.79
Softmax 8 heads (small) 33.15 34.29
Transformer MGK 4 head (small) 33.28 34.21

khác, Transformer-MGK của chúng tôi mô hình hóa khóa tại mỗi đầu như một mô hình hỗn hợp Gaussian, dẫn đến việc sử dụng nhiều khóa tại mỗi đầu và cho phép chúng ta giảm số đầu chú ý. Điều này giúp giảm tính toán và tham số cần thiết để tính toán truy vấn và giá trị bổ sung. Nếu sử dụng dịch chuyển khóa (xem lựa chọn (B) trong đoạn Design Options for Keys trong Phần 2.3), phương pháp MGK cũng giúp giảm tính toán và tham số cần thiết để tính toán khóa bổ sung. Lợi thế của Transformer-MGK giữ nguyên trong cả huấn luyện và suy luận, bao gồm suy luận tăng dần. Chúng tôi đã cung cấp phân tích chi tiết về độ phức tạp tính toán và số tham số của Transformer-MGK so với transformer softmax tương ứng trong Phụ lục B. Kết hợp multi-query attention và phương pháp MGK của chúng tôi thú vị vì mỗi phương pháp có lợi thế riêng và chúng bổ sung cho nhau. Đặc biệt, chúng ta có thể để mô hình transformer chia sẻ cùng tập giá trị và hỗn hợp khóa tại các đầu khác nhau. Phương pháp này có thể có lợi thế của cả multi-query attention và MGK. Bảng 10 hiển thị Perplexity (PPL) trên Wikitext-103 của Multi-query Transformer-MGK 4-head và cơ sở Multi-query Transformer 8-head. Kết hợp multi-query attention với transformer-MGK có lợi cho cả hiệu suất mô hình (Test PPL) so với các cơ sở và hiệu quả của nó vì Transformer-MGK cho phép mô hình giảm số đầu đi một nửa.

B. Phân tích về Độ phức tạp Tính toán và Số Tham số trong Transformer-MGK và Transformer Softmax

Trong phần này, chúng tôi so sánh độ phức tạp tính toán và số tham số trong transformer-MGK với M khóa tại mỗi timestep và H/M đầu với transformer softmax cơ sở có 1 khóa tại mỗi timestep và H đầu. Ở đây chúng tôi chọn M sao cho H là bội số của M và sử dụng lựa chọn thiết kế khóa (A) làm cho khóa kjr thành chiếu tuyến tính của đầu vào xj, tức là kjr = xjWKr>, trong đó xj ∈ R1×Dx, WKr ∈ RD×Dx và r = 1,2,...,M (xem Design Options for Keys trong Phần 2.3 để biết thêm chi tiết). Để đơn giản hóa ký hiệu và không mất tính tổng quát, chúng tôi để M = 2 như trong các thí nghiệm của chúng tôi và giả định rằng Dv = D, tức là, các giá trị có cùng chiều đặc trưng như truy vấn và khóa. Để đơn giản hóa tính toán, chúng tôi cũng không tính toán toán tử softmax vì toán tử này mang lại chi phí tương tự khi được áp dụng trong Transformer-MGK và transformer softmax.

B.1. Độ phức tạp Tính toán

(i) Chú ý softmax H-head: Số phép tính trong chú ý softmax H-head là N²H(4D-1) + NHD(6Dx + 2HD-5).

Giải thích: Để tính ma trận truy vấn Q, ma trận khóa K, và ma trận giá trị V trong Bước 1 trong Phần 1.1 tại mỗi đầu, chúng ta cần 3NDDx phép nhân và 3ND(Dx-1) phép cộng. Tổng cộng, những điều này cần 3ND(2Dx-1) phép tính. Tiếp theo, để tính tích QK> trong Phương trình (1), chúng ta cần N²D phép nhân và N²(D-1) phép cộng. Tương tự, tích AV yêu cầu N²D phép nhân và N(N-1)D phép cộng. Tổng cộng, việc tính chuỗi đầu ra H trong Phương trình (1) tại mỗi đầu yêu cầu 3ND(2Dx-1) + N²D + N²(D-1) + N²D + N(N-1)D = N²(4D-1) + ND(6Dx-4) phép tính. Tổng tính toán cho tất cả H đầu sau đó là

H(N²(4D-1) + ND(6Dx-4)) + NHD(2HD-1)
= N²H(4D-1) + NHD(6Dx + 2HD-5),

trong đó NHD(2HD-1) bổ sung là từ chiếu tuyến tính bởi WO.

(ii) Chú ý hỗn hợp 2 khóa Gaussian với H/2-head: Số phép tính trong chú ý hỗn hợp 2 khóa Gaussian với H/2-head là N²H(3D-0.5) + NHD(4Dx + HD-4).

Giải thích: Tương tự như suy luận ở trên, trong chú ý Hỗn hợp M khóa Gaussian, để tính chuỗi đầu ra H chúng ta cần N²((2M + 2)D-1) + ND((M + 2)(2Dx-1)-1) phép tính. Lưu ý rằng, để tính khoảng cách Gaussian giữa truy vấn qi và khóa kj như trong chú ý hỗn hợp M khóa Gaussian và để tính tích vô hướng của chúng như trong chú ý softmax, chúng ta cần số phép tính tương tự. Do đó, tổng tính toán cho tất cả H/M đầu sau đó là

(H/M)(N²((2M + 2)D-1) + ND((M + 2)(2Dx-1)-1)) + NHD(2(H/M)D-1)
= N²H(2(M + 1)D-1)/M + NHD(2(M + 2)Dx/M + 2HD/M - (3M + 2)/M).

Gọi M = 2, thì tổng tính toán của chú ý hỗn hợp 2 khóa Gaussian được cho bởi N²H(3D-0.5) + NHD(4Dx + HD-4).

Chú ý soft-max H-head so với chú ý hỗn hợp 2 khóa Gaussian với H/2-head: Cho kết quả trong (i) và (ii), khi so sánh với chú ý softmax H-head cơ sở, chú ý hỗn hợp 2 khóa Gaussian với H/2-head của chúng tôi tiết kiệm

N²H(D-0.5) + NHD(2Dx + HD-1)

phép tính. Khi N lớn, sự khác biệt này đáng kể. Kết luận, chú ý hỗn hợp 2 khóa Gaussian với H/2-head có độ phức tạp tính toán rẻ hơn so với chú ý soft-max H-head.

B.2. Số Tham số

(iii) Chú ý softmax H-head: Số tham số trong chú ý softmax H-head là 3HDDx + (HD)².

Giải thích: 3HDDx là từ các chiếu tuyến tính để tính ma trận truy vấn Q, ma trận khóa K, và ma trận giá trị V trong Bước 1 trong Phần 1.1. (HD)² là từ chiếu tuyến tính để tính đầu ra cuối cùng như trong Phương trình (??).

(iv) Chú ý Hỗn hợp 2 Khóa Gaussian với H/2-head: Số tham số trong Chú ý Hỗn hợp 2 Khóa Gaussian với H/2-head là 2HDDx + 0.5(HD)² + H.

Giải thích: Các chiếu tuyến tính để tính Q và V đóng góp HDDx/2 tham số mỗi cái. Chiếu tuyến tính để tính K đóng góp HDDx. Chiếu tuyến tính để tính đầu ra cuối cùng có chiều (H/2)D × HD, vì vậy nó đóng góp 0.5(HD)² tham số. H tham số thêm là từ tiên nghiệm λjr. Những tiên nghiệm này đóng góp 2 tham số tại mỗi đầu vì chúng tôi làm cho {λj1, λj2} cho tất cả j = 1,...,N giống nhau.

Chú ý soft-max H-head so với chú ý hỗn hợp 2 khóa Gaussian với H/2-head: Cho kết quả trong (iii) và (iv), khi so sánh với chú ý softmax H-head cơ sở, chú ý hỗn hợp 2 khóa Gaussian với H/2-head của chúng tôi tiết kiệm HDDx + 0.5(HD)² - H tham số. Khi H và D lớn, việc tiết kiệm này đáng kể.

C. Chứng minh các kết quả chính

Trong phụ lục này, chúng tôi cung cấp chứng minh cho các kết quả chính trong bài báo.

C.1. Chứng minh Định lý 1

Để dễ dàng trình bày chứng minh, đối với bất kỳ phân phối xác suất G nào, chúng tôi ký hiệu

pG(x) := ∫ φ(x|μ)dG(μ) = ∫ φ(x|μ,σ²I)dG(μ),

cho tất cả x ∈ Rd trong đó φμ(x) = 1/((√2π)^d) exp{-||x||²/(2σ²)} cho σ > 0 cho trước. Điều này có nghĩa là pG là tích chập của φ và phân phối xác suất G. Vì không gian của hỗn hợp Gaussian dày đặc trong không gian của các đo lường xác suất liên tục (Bacharoglou, 2010), nó chỉ ra rằng tồn tại phân phối xác suất G1 sao cho

sup |p(x) - pG1(x)| ≤ ε/2. (9)
x∈Rd

Bước tiếp theo của chúng tôi là chứng minh rằng tồn tại đo lường xác suất G2 với nhiều nhất K hỗ trợ trong đó K ≤ (C log(1/ε))^d cho một hằng số toàn cục C sao cho

sup |pG1(x) - pG2(x)| ≤ ε/2. (10)
x∈Rd

Thật vậy, từ Lemma A.1 trong (Ghosal & van der Vaart, 2001), với bất kỳ k ≥ 1 nào tồn tại phân phối xác suất G2 với nhiều nhất (2k²)^d hỗ trợ sao cho

∫ μ^α d(G1 - G2)(μ) = 0, (11)

cho bất kỳ α = (α1, α2,..., αd) ∈ N^d sao cho 0 ≤ |α| = Σ^d_{j=1} αj ≤ 2k², Ở đây, μ^α = Π^d_{j=1} μj^{αj}.

Bây giờ, với bất kỳ M ≥ a√d, chúng ta có ||x|| ≥ ||x|| - ||μ|| > M - a√d > M/2 = 2 miễn là ||x|| > M và μ ∈ [a,a]^d. Điều này chỉ ra rằng

sup |pG1(x) - pG2(x)| = sup |∫ φμ(x)d(G1 - G2)(μ)|
||x||>M      ||x||>M

≤ sup ∫ 1/((√2π)^d) exp{-||x||²/(2σ²)} d(G1 + G2)(μ)
||x||>M

≤ 2/((√2π)^d) exp{-M²/(8σ²)}. (12)

Mặt khác, với bất kỳ k ≥ 1 chúng ta cũng có rằng

sup |pG1(x) - pG2(x)| = sup |∫ φμ(x)d(G1 - G2)(μ)|
||x||≤M      ||x||≤M

≤ sup |∫ [φμ(x) - Σ^{k-1}_{j=0} (-1)^j ||x||^{2j}/((√2π)^d σ^{d+2j} j!)] d(G1 - G2)(μ)|
||x||≤M

+ sup |∫ Σ^{k-1}_{j=0} (-1)^j ||x||^{2j}/((√2π)^d σ^{d+2j} j!) d(G1 - G2)(μ)|
||x||≤M

= sup |∫ [φμ(x) - Σ^{k-1}_{j=0} (-1)^j ||x||^{2j}/((√2π)^d σ^{d+2j} j!)] d(G1 - G2)(μ)|, (13)
||x||≤M

trong đó đẳng thức cuối xuất phát từ

∫ Σ^{k-1}_{j=0} (-1)^j ||x||^{2j}/((√2π)^d σ^{d+2j} j!) d(G1 - G2)(μ) = 0,

điều này là do Phương trình (11).

Để tiếp tục ràng buộc vế phải (RHS) của Phương trình (13), chúng tôi sử dụng bất đẳng thức sau:

|exp(-y) - Σ^{k-1}_{j=0} (-y)^j/j!| ≤ |y|^k/k!

cho bất kỳ y ∈ R. Vì k! ≥ (k/e)^k cho bất kỳ k ≥ 1, ràng buộc trên có thể được viết lại thành

|exp(-y) - Σ^{k-1}_{j=0} (-y)^j/j!| ≤ |ye/k|^k. (14)

--- TRANG 25 ---
Cải thiện Transformer với Khóa Chú ý Xác suất

Đơn giản hóa thêm Phương trình (13) dẫn đến

sup |pG1(x) - pG2(x)| ≤ sup |∫ [φμ(x) - Σ^{k-1}_{j=0} (-1)^j ||x||^{2j}/((√2π)^d σ^{d+2j} j!)] d(G1 + G2)(μ)|
||x||≤M      ||x||≤M;μ∈[a,a]^d

= sup 2/((√2π)^d) |exp{-||x||²/(2σ²)} - Σ^{k-1}_{j=0} (-1)^j ||x||^{2j}/(2^j σ^{2j} j!)|
||x||≤M;μ∈[a,a]^d

≤ sup (e||x||²)^k/(2^k σ^{2k} (2k)^k),
||x||≤M;μ∈[a,a]^d

trong đó bất đẳng thức cuối dựa trên việc áp dụng bất đẳng thức (14) với y = ||x||²/(2σ²). Với ||x|| ≤ M và μ ∈ [a,a]^d, chúng ta có ||x|| ≤ ||x|| + ||μ|| ≤ M + a√d. Do đó, chúng ta tiếp tục có

sup |pG1(x) - pG2(x)| ≤ sup (ek(M + a√d)²)^k/(2^k σ^{2k} (2k)^k).
||x||≤M      ||x||≤M;μ∈[a,a]^d

Khi M ≥ 2a√d, chúng ta có M + a√d ≤ 3M/2 và ràng buộc trên dẫn đến

sup |pG1(x) - pG2(x)| ≤ ((9e)^k M^{2k})/(8^k σ^{2k} k^k). (15)
||x||≤M

Bằng cách chọn M² = 8σ² log(1/ε₀) cho một số ε₀ > 0, các ràng buộc trong Phương trình (12) và (15) trở thành

sup |pG1(x) - pG2(x)| ≤ 2/((√2π)^d) ε₀,
||x||≤M

sup |pG1(x) - pG2(x)| ≤ ((9e)^k (log(1/ε₀))^k)/k^k. (16)
||x||>M

Miễn là chúng ta chọn k = ⌈9e² log(1/ε₀)⌉ và ε₀ ≤ 1, chúng ta có

sup |pG1(x) - pG2(x)| ≤ e^{-k/e} ≤ e^{-9e² log(1/ε₀)} = (ε₀)^{9e²} ≤ ε₀. (17)
||x||>M

Bằng cách chọn ε₀ = ε/(2 max{2/((√2π)^d), 1}), kết quả từ Phương trình (16) và (17) chỉ ra rằng

sup |pG1(x) - pG2(x)| ≤ ε/2, và sup |pG1(x) - pG2(x)| ≤ ε/2.
||x||≤M      ||x||>M

Do đó, nếu chúng ta chọn M = √{8σ² log(2 max{2/((√2π)^d), 1}/ε)} và k = ⌈9e² log(2 max{2/((√2π)^d), 1}/ε)⌉, chúng ta có

sup |pG1(x) - pG2(x)| ≤ ε/2.
x∈R^d

Điều này chỉ ra rằng chúng ta đạt được kết luận của mệnh đề (10) bằng cách chọn K = (2k²)^d ≤ (18e² log(2 max{2/((√2π)^d), 1}/ε))^d.

Kết hợp kết quả từ Phương trình (9) và (10), chúng ta có

sup |p(x) - pG2(x)| ≤ sup |p(x) - pG1(x)| + sup |pG1(x) - pG2(x)| ≤ ε.
x∈R^d      x∈R^d      x∈R^d

Kết quả là, chúng ta đạt được kết luận của định lý.
