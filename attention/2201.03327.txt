# 2201.03327.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2201.03327.pdf
# File size: 2452838 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Latency Adjustable Transformer Encoder for
Language Understanding
Sajjad Kachuee, Mohammad Sharifkhani
Department of Electrical Engineering
Sharif University of Technology
Abstract —Adjusting the latency, power, and accuracy of nat-
ural language understanding models is a desirable objective of
an efficient architecture. This paper proposes an efficient Trans-
former architecture that adjusts the inference computational cost
adaptively with a desired inference latency speedup. In fine-
tuning phase, the proposed method detects less important hidden
sequence elements (word-vectors) and eliminates them in each
encoder layer using a proposed Attention Context Contribution
(ACC) metric. After the fine-tuning phase, with the novel offline-
tuning property, the inference latency of the model can be
adjusted in a wide range of inference speedup selections without
any further training. Extensive experiments reveal that most
word-vectors in higher Transformer layers contribute less to
subsequent layers, allowing their removal to improve inference
latency. Experimental results on various language understanding,
text generation, and instruction tuning tasks and benchmarks
demonstrate the approach’s effectiveness across diverse datasets,
with minimal impact on the input’s global context. The technique
improves Time-to-First-Token (TTFT) of Llama3 by up to 2.9x,
with minor performance drop. The suggested approach posits
that in Large Language Models (LLMs), although the complete
network is necessary for training, it can be truncated during the
fine-tuning phase.
Index Terms —Word-vector elimination profile, attention con-
text contribution (ACC), offline-tuning, latency adjustable.
I. I NTRODUCTION
Recently, Transformer [1] based architectures achieved re-
markable success in various Natural Language Processing
(NLP) tasks. However, the training difficulty, inference latency,
and small size of available datasets are the main concerns with
using these models in real applications [2]. Fine-tuning a pre-
trained language model on downstream tasks mitigates some
of these concerns, including training effort and dataset size.
BERT [3], XLNet [4], GPT [5], Llama [6], and Gemma [7]
represent a spectrum of pre-trained language models, ranging
from earlier developments to the latest advancements. These
models vary significantly in size, ranging from hundreds of
millions to hundreds of billions of parameters. Hence, the
computational complexity bears trillions of Floating Point
Operations (FLOPs) for processing a single sample. In some
cases, the inference phase computational effort is beyond
the ability of resource-limited systems such as edge devices.
Therefore, baseline Transformer models cause unacceptable
latency and energy usage limitations in such devices.
BERT (Bidirectional Encoder Representations from Trans-
formers) and GPT (Generative Pre-trained Transformer) are
two landmark architectures in the development of large lan-
guage models (LLMs). BERT, introduced by Google, uses abidirectional training approach, allowing it to capture context
from both directions of a sentence simultaneously. This makes
it particularly effective for tasks like sentence classification,
question answering, and named entity recognition, where un-
derstanding relationships between words is crucial [3]. On the
other hand, GPT, developed by OpenAI, takes an autoregres-
sive approach, generating text word by word in a unidirectional
manner. Starting with GPT-1 and evolving through GPT-4,
GPT has proven its strength in creative and coherent text
generation, conversational AI, and text summarization, making
it a highly flexible model for both content generation and
interactive applications [5], [8].
More recent models like Meta’s LLaMA (Large Language
Model Meta AI) and Gemma are pushing the boundaries of
LLM development. LLaMA is designed to be highly efficient
in terms of computational resources, offering a smaller model
that still performs well on a wide range of natural language
processing tasks. Its streamlined design makes it accessible for
research and industrial applications where resources might be
limited [6]. Gemma, an emerging model family, is expected
to further refine the efficiency and adaptability of LLMs, with
an emphasis on real-time performance and scalability. Gemma
is particularly well-suited for industry-specific applications,
aiming to bridge the gap between general-purpose LLMs and
more specialized use cases [7]. Together, these model families
reflect the rapid progression and specialization in the field of
large language models, each offering distinct strengths based
on the needs of users and developers.
In the realm of Deep Neural Networks (DNNs) optimiza-
tion, diverse methods have been proposed to enhance speed
and reduce power consumption. This is crucial for applications
such as extending battery life and enabling powerful tasks
on small devices. The approaches can be categorized into
pruning, removing unnecessary parts; quantization, simpli-
fying numerical usage; Knowledge Distillation, simplifying
based on a complex model; parameter-sharing, using the
same information across components; tensor decomposition,
breaking down data structures; and sub-quadratic complexity
transformers [9].
This study introduces an innovative attention layer designed
to enhance the efficiency of inference stages in Transformer-
based models with an adjustable approach. The suggested
solution involves a sorting and elimination process for hidden
sequence elements (word-vectors), resulting in a reduction of
effective word-vectors in each layer and a decrease in the
number of FLOPs. The primary contributions of this researcharXiv:2201.03327v9  [cs.CL]  18 Sep 2024

--- PAGE 2 ---
2
can be summarized as follows:
•The novel structure and tuning policy are proposed to
speed up the inference times with negligible accuracy
degradation.
•A new Attention Context Contribution (ACC) metric is
proposed to observe the context contribution in each
attention layer. It is shown that the context contribution
is decreased at the last encoder layers significantly.
•An accurate analytical inference speedup estimation is
presented for the proposed approach that helps a system
designer determine the hyper-parameter selection at the
fine-tuning phase.
•Offline-tuning property that adjusts the inference latency
after the fine-tuning phase. This property controls the
trade-off between latency and accuracy in a wide range of
inference speedup selections without requiring additional
training.
•The proposed approach can be applied to a variety
of Transformer-based architectures, while preserving the
global context of the input.
II. R ELATED WORKS
Pruning-based methodologies have been extensively ex-
plored in the existing body of literature, with categorizations
based on distinct pruning approaches. In weight pruning meth-
ods, the model’s weights are systematically pruned according
to a predetermined threshold [10]–[13]. Head and channel
pruning methods extend their scope by entirely excising a
channel or head of the network [14]–[24]. Layer and block
pruning strategies [25]–[27], involve the targeted removal
of entire layers or blocks within the network. Token and
word-vector pruning methods, adopting a more fine-grained
approach, focus on the selective removal of specific tokens or
word-vectors within the model [28]–[30]. This classification
clarifies the various approaches taken in the pruning paradigm,
each playing a unique role in achieving the overall goal of
optimizing networks.
A. Weight Pruning
In [31], the magnitude weight pruning method is applied
to BERT and shows that the model can be pruned once
during pre-training rather than separately for each task without
affecting performance. Reweighted Proximal Pruning (RPP)
has introduced a pruning method specifically designed for
a large-scale language representation model that is much
better than typical iterative pruning. This approach yields a
sparse version of BERT [32]. Transformer mixed-length vector
pruning (TF-MVP) examines the sparsity patterns in pruned
Transformers and introduces the TF-MVP architecture using
the mixed-length vector pruning (MVP) technique [33]. While
TF-MVP achieves a pruning ratio of approximately 70%, there
is a notable decline in accuracy. In [34], the model weights
are encouraged to be zero with the L0 regularization term.
B. Head/Channel Pruning
A parameter reduction framework was proposed by identi-
fying the most critical heads in each encoder layer using layer-
wise relevance propagation and pruning the redundant heads[35]. It showed that the earlier encoder layer’s heads are much
more critical than the last. Moreover, in [36], it is shown that
around 17% of attention heads can be removed at the test
time without significantly impacting inference performance.
In [37], a rapid post-training pruning framework tailored for
Transformers is introduced. This framework involves three
key fine-tuning stages: mask search, mask rearrangement, and
mask tuning. Notably, the method obviates the necessity for
any retraining, presenting an efficient approach to achieve
model compression in the Transformer architecture.
C. Layer/Block Pruning
A progressive layer-dropping at the pre-training phase of
BERT architecture offers up to 2.5 times speedup at the
inference phase with more knowledge transferability and better
generalization on downstream tasks [38]. FastBERT is pro-
posed with adaptive inference time in which the model uses
a sufficient number of encoder layers based on the inference
certainty of a sample [39]. It dynamically adjusts the number
of active layers to reduce the overall computational effort.
ELBERT proposed the Confidence-Window Based Early Exit
mechanism, improving the ALBERT inference speed [40].
In [41], it is experimentally shown that using all layers
of a pre-trained model in downstream tasks is unnecessary.
Studying various layer-dropping strategies shows that many
model weights are not necessary. LayerDrop introduced a
type of structured dropout that imparts regularization benefits
during the training process and enables effective pruning at
inference time, resulting in the removal of approximately half
of the Transformer layers [42].
D. Token/Word-Vector Pruning
PoWER-BERT proposes a word-vector elimination method
that eliminates redundant word-vectors [28]. A supplement
learnable layer is added between each encoder self-attention
and feed-forward layer, and an additional loss function term
is used to train these supplement parameters. PoWER-BERT
training consists of 3 steps “fine-tuning, elimination config-
uration search, and re-training” [28]. Funnel-Transformer is
inspired by U-net from computer vision [43]. It improves the
Transformer architecture by inserting a pooling layer in each
self-attention layer [29]. Funnel-Transformer compresses the
sequence of hidden states to a shorter one and reduces the
computation cost. Learned Token Pruning (LTP) adaptively
removes unimportant tokens as an input sequence passes
through transformer layers [30]. LTP is applied to RoBERTa
[44], which results in up to 2.0 times throughput improvement
with less than 1% accuracy drop.
In STTABT [45], the impact of token pruning on later
layers’ attentions and on final predictions is studied. The
study shows that the proposed attention back-tracking allows
the model to better retain the full model’s performance even
at high sparsity rates. Conversely, since in the self-attention
architecture all input elements contribute to the formation of
each attention output, it can be stated that in the word-vector
pruning procedure, no independent concept is discarded. In the
subsequent layers, every concept retains the ability to expand

--- PAGE 3 ---
3
and influence the outputs and model predictions. Additionally,
STTABT prunes tokens based on a lightweight attention
approximation network trained with knowledge distillation,
which adds extra training and inference overhead.
In [46], a learnable mechanism is employed to determine
which uninformative tokens can be dropped from the context
at any point during the generation process. In [47], the model
leverages an asymmetric attention mechanism to iteratively
distill inputs into a tight latent bottleneck, allowing it to scale
to handle very large inputs.
E. Non-Pruning Methods
ALBERT is a lighter version of BERT model [48]. It
uses cross-layer weight sharing and a modified embedding
layer structure, making the embedding size independent of the
encoder’s hidden size. Moreover, the new Sentence Order Pre-
diction (SOP) task is utilized in the pre-training phase instead
of the Next Sentence Prediction task. ALBERT reports state-
of-the-art results with its parameter reduction and modified
pre-training policy. ALBERT is nine times smaller and 1.2
times faster than BERT-base, with less than 2.5% on average
accuracy drop. ALBERT is an example of the parameter-
sharing technique.
DistilBERT [49], TinyBERT [50] and MobileBERT [51] use
the knowledge distillation technique during the pre-training
and offer up to 5.5 times faster inference on downstream tasks
than BERT-base. In [52] a Self-Distillation (SD) mechanism
obtains high-accuracy models directly without going through
an assistive model. The main concerns with these techniques
are the pre-training effort of the model from scratch, extra
training steps, and significant accuracy degradation.
In [53], the authors propose sufficient conditions under
which they prove that a sparse attention model can univer-
sally approximate any sequence-to-sequence function. They
show that sparse Transformers with only Onconnections per
attention layer can approximate the same function class as
the dense model with n2connections. Consequently, in recent
years, many works [54]–[62] have proposed methods that aim
to reduce the complexity of the attention mechanism.
Low-Rank Adaptation (LoRA) involves the freezing of
weights in the pre-trained model, and during the fine-tuning
phase, it introduces trainable rank decomposition matrices
into each layer of the Transformer architecture [63]. LoRA
enhances the efficiency of memory usage during the training
phase by up to 3 times, but it does not confer any advantages
during the inference phase. QLoRA improves over LoRA
by quantizing the transformer model to 4-bit precision and
using paged optimizers to handle memory spikes [64]. LQ-
LoRA uses an iterative algorithm to decompose each pre-
trained matrix into a high-precision low-rank component and
a memory-efficient quantized component [65].
The previous works are trying to reduce the model size
and inference latency. These works show that significant part
computations in the models appear unnecessary, and models
with less computational complexity can be obtained from the
original ones. The main drawbacks of these methods are pre-
training from scratch, multi-phase fine-tuning, and significantinference accuracy drop. In these methods, the inference
speedup is set in the fine-tuning phase, and the achieved
speedup is fixed in the inference phase. The proposed solu-
tion tries to mitigate the previous drawbacks. It presents the
powerful offline-tuning mechanism that controls the inference
speedup of the model after the fine-tuning phase.
The impact of the speedup methods on the model stability
and the input’s global context understanding is an important
concern that must be studied in solutions evaluations. Back-
pack learns multiple non-contextual sense vectors for each
word in a vocabulary and represents a word in a sequence as
a context-dependent, non-negative linear combination of sense
vectors in this sequence [66]. The Backpack Model shows that
each word in the input sequence can interpreted as a sense
vector that encodes different aspects of a word which is very
sensitive. A good practice for assessing the performance of
a method is evaluating it in a more complex task, like text
generation tasks that inspect the model proficiency. Therefore
the suggested method is applied to the several autoregressive
models and evaluated on several text generation tasks.
Caching methods focus on optimizing the efficiency and
speed of inference in autoregressive large language models
(LLMs). KV-Runahead enhances prompt phase parallelization
by using multiple processes to manage KV-cache popula-
tion, thereby reducing the Time-to-First-Token (TTFT) [67].
CacheGen speeds up context loading with a custom tensor
encoder that compresses KV-cache into compact bitstream rep-
resentations, minimizing decoding overhead [68]. CacheBlend
introduces a method for reusing pre-computed KV-caches
and selectively updating a small subset of tokens, optimizing
cache reuse [69]. Prompt Cache accelerates inference by
precomputing and storing attention states for frequently oc-
curring text segments, enabling efficient reuse across different
prompts [70]. Lastly, CachedAttention employs a hierarchical
KV-caching system using cost-effective memory and storage
solutions to manage KV-caches for all requests [71]. Together,
these methods contribute to faster and more efficient LLM
inference by improving cache management and state reuse.
III. P ROPOSED METHOD
This section presents details of the proposed method and
the analysis and reasoning behind the approach. Section III-A
introduces the preliminary background of the Transformer
architecture, while Section III-B analyzes the latency and
TTFT across various Transformer-based models. The proposed
encoder layer is based on an insight into the amount of
the contribution of word-vectors at different layers, which
is considered in Section III-C. Section III-D explores the
proposed architecture in detail. Additionally, Section III-E
extends the proposed method to autoregressive architectures
and applies it to several newly developed models.
A. Background
Despite the current progress and advancements in main-
stream LLM architectures such as GPT, Gemma, Mistral [72],
and Llama, these models still follow the foundational Trans-
former architecture [1]. The primary differences among them

--- PAGE 4 ---
4
Fig. 1. BERT-base [3] architecture with output layers
lie in the number of parameters, layers, heads, hidden states,
and specific modifications to normalization layers, residual
paths, and attention implementations. For clarity and a deeper
understanding of the suggested method, we first focus on
the BERT [48] and GPT-2 [73] models before extending the
approach to other large language models (LLMs).
Fig. 1 shows BERT-base model architecture and output
classifier. The input sentence goes through the tokenizer layer
and embedding layer. Then the context is fed into 12 cascaded
encoder layers to be processed. The pooler and classifier layers
are added in the last layer to perform proper output for the
desired task. In the figure, TandHare the numbers of input
word-vectors and the hidden state vector size, respectively.
Each encoder consists of a Bidirectional Multi-Head Self-
Attention layer and a Feed-Forward layer with several ten-
sor multiplications. The residual branches and normalization
layers increase the training stability.
B. Transformer Architecture Latency and TTFT Analysis
Table I presents an analysis of the latency and TTFT share
across the embedding, attention, and feed-forward layers in
several Transformer-based models. As shown in the table,
regardless of model size, the latency of the embedding layer
is minimal, with the majority of latency attributed to the
attention and feed-forward layers. Attention share ranges from
around 20% to 47% of the total. In neural networks, latency
and computational effort are closely related to the number
of FLOPs. Consequently, reducing the number of FLOPs in
the encoder layers, which account for most of the latency,significantly decreases both total latency and TTFT.
C. Attention Context Contribution (ACC) metric
The context contribution of word-vectors at each self-
attention layer is studied in this Section to find more important
word-vectors at each encoder layer. For a given encoder layer,
the Attention Context Contribution (ACC) metric measures the
contribution of word-vectors in the layer’s output. This metric
is derived from a Score Vector (SV) to find more important
word-vectors in each layer. For a given layer, the SV indicates
the contribution of each input word-vector to the output of the
attention layer.
Fig. 2 illustrates the calculation of the Score Vector based
on the attention probability matrix. Fig. 2 (left) shows the
attention probability matrix of BERT-base model. The rows of
this matrix are normalized to one. By averaging the attention
probability matrix over the heads, the aggregated probability
matrix is resulted, Fig. 2 (center). The SV results from the sum
of this matrix along the unnormalized dimension as shown
in Fig. 2 (right). The SV elements with higher score values
correspond to the word-vectors that contribute more to the
layer output.
Fig. 3 illustrates BERT-base SV results on the IMDB
sentiment analysis task [77] for an input sample. As shown
in earlier encoder layers, the SV values approximately have
uniform distributions, and in the last encoder layers, the dis-
tribution of SV values changes to approximately δdistribution.
The distribution of SV values in the last layers indicates that
the concentration of scores is high on a few word-vectors and
others have less contribution to the self-attention layer output.
The SV elements are positive and with an average of one.
So, the median of SV is bounded. Suppose the median of the
SV is lower than its average. In that case, more than half of
the elements are lower than the average value, and only a few
are higher than the average value. It suggests that most of
the word-vectors do not bear enough contribution to the layer
output, and a few word-vectors influence the final result of
the layer. The median of the Score Vector is interpreted as the
proposed Attention Context Contribution metric.
The ACC metric calculation is explained in Algorithm 1.
The algorithm inputs are BERT encoder word-vectors and the
model parameters. The algorithm output is the ACC metric
vector that suggests the contribution of the word-vectors in
each encoder self-attention layer. For encoder layer l, the first
step is to prepare the pre-computed self-attention probability
matrix; Ml
Att. Then the average of this matrix over its heads is
computed; Ml
H. In the next step, the resulting matrix’s summa-
tion over its unnormalized dimension is computed; Ml
S. This
matrix is the Score Vector. Attention Context Contribution of
the layer ( El
ACC ) is the median of the Score Vector.
Fig. 4 shows the ACC metric for BERT-base encoder
layers on IMDB [77] dataset and several General Language
Understanding Evaluation (GLUE) [78] benchmark tasks. The
fitted curve to the ACC metric results shows that the ACC
metric is reduced at later layers gradually. It indicates that the
fraction of the word-vectors that contribute more in the last
layers is less in those layers; hence more word-vectors can be
eliminated. By eliminating less important word-vectors in each

--- PAGE 5 ---
5
TABLE I
LATENCY AND TIME-TO-FIRST-TOKEN (TTFT) A NALYSIS FOR BERT [3], GPT-2 [73], T5 [74], G EMMA 2 [75],
MISTRAL [72], AND LLAMA 3 [76] M ODELS
Model Architecture Type Parameters Layers Attention HeadsLatency/TTFT Sharea(%)
Embedding Attention Feed-Forward
BERT-base Encoder 110M 12 12 0.58 34.94 64.48
GPT-2 Decoder 124M 12 12 0.06 34.94 65.00
T5-base Encoder-Decoder 220M 12 12 0.35 47.00 52.65
T5-large Encoder-Decoder 770M 24 16 0.06 40.46 59.47
GPT-2-large Decoder 774M 36 20 0.01 32.25 67.74
Gemma2 Decoder 2B 26 8 0.01 20.36 79.63
Mistral Decoder 7B 32 32 0.01 20.34 79.65
Llama3 Decoder 8B 32 32 0.01 19.55 80.44
aLatency share for BERT model and Time-to-First-Token (TTFT) for other autoregressive models
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
0123T-2T-1
0 1 2 3 T-2 T-1Head 0Head 1Head 11
Normalized Average of the
 HeadsAttention Probability Matrix
0123T-2T-1
0 1 2 3 T-2 T-1Unnormalized
...
...
...
......
...
......
...
......
...
...
...
...
......
...
......
...
......
...
Sum  of Unnormalized  
Elements
...
0 1 2 3 T-2 T-1...
0 1 2 3 T-2 T-1Score Vector (SV)
Fig. 2. Obtaining Score Vector (SV) from Attention Probability Matrix in a given BERT [3] encoder layer
Algorithm 1 Attention Context Contribution (ACC) metric
Input: Encoder inputs ∈RLTH& fine-tuned BERT
{T= number of input word-vectors, L= number of
encoder layers, H= hidden state size }
Output: ACC vector: EACC∈RL
Initialisation :
1:EACC = [∅]
LOOP Process
2:forencoder l inEncoders do
3:Ml
Att←Self-Attention Probability Matrix ∈RHTT
4:Ml
H←Mean of Ml
Attover heads ∈RTT
5:Ml
S←Sum of Ml
Hover unnormalized dimension ∈
RT
6:El
ACC←Median of Ml
S(Score Vector) ∈R
7: Append El
ACC toEACC
8:end for
9:return EACC
layer, the processed word-vectors are decreased gradually, and
the inference time of the model is decreased.
As shown in Fig. 4, the behavior of the ACC metric stronglycorrelates with the intricate specifications inherent in each task.
The ACC results consistently exhibit a monotonic decrease
with the layer number, except in the case of the SST-2 [79]
dataset, where unexpected behavior is observed. Notably, the
SST-2 dataset, centered around sentiment analysis of movie
reviews, shares a strong resemblance to the IMDB dataset. A
comparative analysis of the ACC curves for SST-2 and IMDB
reveals a notable disparity, primarily associated with the length
of input sequences. Specifically, in the IMDB dataset, the
majority of samples consist of around 512 tokens; conversely,
in the SST-2 dataset, the majority of samples feature fewer
than 50 tokens. It can be inferred that the limited number of
input tokens leads to an undesired sensitivity to word-vectors,
and in such cases, some word-vectors are revived through
residual branches.
D. Proposed Architecture
According to the discussion in Section III-B, attention and
feed-forward layers are major contributes of the total latency.
In addition, according to the observation in Section III-C,
the word-vector contribution in the encoder decreases at later

--- PAGE 6 ---
6
0 100 200 300 400 500
Sorted Word Vectors012345Score ValuesLayer 1
Layer 2
Layer 3
Layer 4
Layer 5
Layer 6Layer 7
Layer 8
Layer 9
Layer 10
Layer 11
Layer 12
Fig. 3. BERT-base [3] Score Vector (SV) results on the IMDB sentiment
analysis task [77]
layers. Based on these observations, this Section proposes the
details of the proposed architecture.
In proposed architecture, the orginal BERT self-attention
layer is modified with suggested self-attention layer. Fig. 5
illustrates the proposed multi-head self-attention layer. The
proposed architecture adds the Sort and Eliminate layers in
the middle of the original BERT multi-head self-attention
layer. The Sort layer sorts the input context according to the
computed layer Score Vector. The Eliminate layer eliminates
a fraction of the sorted context with lower scores according to
the Elimination-Rate ( αER).
αERis a hyper-parameter vector that sets the fraction of
word-vectors that is eliminated in each encoder layer. For
layer l,αl
ERis the word-vector Elimination Rate obtained
from the pre-computed Elimination-Profile ( αEP) vector and
a pre-specified Speedup-Coefficient hyper-parameter ( αSC).
αSCadjusts the trade-off between accuracy and speed in fine-
tuning and offline-tuning phases.
αl
ER=αl
EP×αSC (1)
αl
EPis obtained from the fitted curve to the ACC metric
(red lines in Fig. 4) at layer l. In some tasks, the ACC
values of layers are not smooth. To increase stability and
prevent applying stress to the model in fine-tuning phase,
in the proposed formulation, instead of ACC metric values
(EACC ), the fitted curve to the ACC metric ( PACC ) is used
that presents a more smooth elimination trend. It is important
to note that when an increasing behavior of the ACC value
occurs in a layer, the elimination process is halted, and the
remaining layers proceed using a fixed number of retained
word-vectors. In (2), the αEPlower bound is set to one to
ensure a monotonic descending shape of this hyper-parameter
curve. This adaptive approach guarantees the preservation of
crucial information and mitigates the impact of unexpected
ACC behavior on subsequent layers.αl
EP= min (1 ,Pl
ACC
Pl−1
ACC) (2)
The number of remaining word-vectors in each encoder
layer is calculated according to the number of the layer
input word-vectors and the αERhyper-parameter. Equation
(3) presents the number of remaining word-vectors ( Tl) at the
output of encoder layer l. The minimum number of remaining
word-vectors at each encoder output is limited to one.
Tl= max (1 ,⌊αl
ER×Tl−1⌋) (3)
The effective number of processed word-vectors can be
estimated using the number of word-vectors before and after
the eliminate layer. Based on Table I, in BERT-base, around
65% of latency occur after the self-attention layer. Equation
(4) presents the effective number of Processed word-vectors
(PWl) in layer l.
PWl≈7Tl−1+ 13Tl
20(4)
Hence,
PW≈T
7
20+L−1X
i=1iY
j=1αj
EP+13
20LY
j=1αj
EP
 (5)
Processed word-vectors in the original BERT-base is:
PWBERT −base=T×L (6)
In these equations, Lis the number of BERT encoder layers,
andTis the number of encoder input word-vectors. Since
almost all inference latency occurs in the encoder, the overall
model computation effort is almost equal to that of the encoder
stage. Equation (7) presents the proposed method inference
time speedup to BERT-base.
Kspeedup ≈20L
7 + 20PL−1
i=1Qi
j=1αj
EP+ 13QL
j=1αj
EP(7)
The above equation assumes that the number of remaining
word-vectors in each encoder layer is more than a single word-
vector. In other words, (3) returns the second argument, which
is valid for most cases. For example, if Lis 12 and αl
EPis
constant and equal to 0.8 for all layers, the final inference
speed is around 2.99 times faster than BERT-base.
The proposed Sort and Elimination layers are included in
the fine-tuning phase. Given that the eliminated word-vectors
constitute a relatively small portion within each layer during
the fine-tuning process, we can assume that the gradients
effectively propagate through the remaining word vectors.
Additionally, the residual paths stay functional, contributing
to the updates in the initial layers of the network. This design
ensures a robust flow of information throughout the fine-tuning
process.

--- PAGE 7 ---
7
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
SST-2
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
MRPC
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
QQP
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
QNLI
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
RTE
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
MNLI
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
STS-B
2 4 6 8 10 12
Encoder Layer Number0.00.20.40.60.8ACC ValueFitted Curve
IMDB
Fig. 4. BERT-base [3] Encoder Attention Context Contribution (ACC) metric results (blue bars) and second degree fitted curves (red lines) on SST-2 [79],
MRPC [80], QQP [81], QNLI [82], RTE [83], MNLI [84], STS-B [85] and IMDB [77] tasks.
Input Word Vectors
2K
2Q
2V........
HK
HQ
HV
1
TTP
2
TTP
H
TTPProduct & Normalization Product & Normalization Product & Normalization...Weighted Sum Weighted Sum Weighted Sum...Concatenation & Output LayerOutput Word Vectors
...
2'v
3'v
1'v
...
2v
3v
Tv
1v...
2v
3v
Tv
1v.....
Sort Sort Sort
Head 1 - Linear Layer Head 2 - Linear Layer Head H - Linear Layer
''TvEliminate Eliminate EliminateSort & Eliminate Layer
SV
SV
SV
1K
1V
1Q
ER
ER
ER
Fig. 5. The proposed self-attention layer
To enhance the speed of the suggested self-attention layer,
the Sort and Eliminate layers can be positioned after the
self-attention concatenation layer. This adjustment results in
a reduction of per-head calculations, and for a given layer,
the eliminated word-vectors play a role in shaping the layer’s
output. Furthermore, in accordance with the suggested word-
vector elimination strategy, the pruned word-vectors efficiently
accelerate the inference phase, leading to an anticipated close
alignment between the reduction in Floating Point Operations
(FLOPs) and the speedup in inference.E. Extending Proposed Architecture to Autoregressive Models
The suggested solution can also be applied to decoder
architectures, including GPT, Gemma, Mistral, and Llama
Large Language Models (LLMs). This section outlines the
key considerations for implementing the method on the GPT-2
model, as a representative example of autoregressive models.
The foundational GPT-2 model comprises 12 decoder lay-
ers, with each decoder featuring a causal attention layer
equipped with 12 heads. The total number of parameters
in the GPT-2 model is approximately 124 million. A key
distinction between the architectures of GPT-2 and the BERT-
base model lies in the design of their attention layers. As

--- PAGE 8 ---
8
explained in Section III-A, the attention layers of BERT-
base encoders are bidirectional, involving all input word-
vectors in generating each output word-vector. Conversely,
the GPT-2 model, tailored for text generation tasks, employs
causal attention layers, where only the preceding word-vectors
contribute to the attention layer output at each time stamp.
In a causal attention layer, the attention probability matrix,
represented as Ml
Att, adopts the form of a triangular matrix.
To extend the proposed architecture outlined in Section III-D
to causal attention layers, an additional normalization step
needs to be integrated after the fifth step in Algorithm 1. This
supplementary step normalizes the Score Vector Ml
Sbased
on the count of non-zero elements in the probability matrix
Ml
Att. Equation (8) articulates this normalization process in
layer l, wherein the i-th element of the Score Vector is adjusted
relative to the number of non-zero elements in the i-th column
of the probability matrix. Through this normalization step, the
resulting score values become comparable.
Ml′
Si=Ml
Si÷Nl
MAtt i(8)
IV. E XPERIMENTS
In this section, we implement and evaluate the suggested
method on BERT-base, GPT-2, Flan-T5 [86], Gemma2 [75],
Mistral, and Llama3 [76], exploring its effectiveness across
various natural language understanding tasks, text generation
datasets, and instruction-tuning benchmarks. Toward the con-
clusion of this section, we examine the offline-tuning property
and conduct an ablation study on the proposed attention layer.
A. Datasets
The performance of the proposed method on natural lan-
guage understanding tasks is evaluated using the IMDB dataset
[77], which focuses on sentiment analysis, as well as the Gen-
eral Language Understanding Evaluation (GLUE) benchmark
[78]. The GLUE benchmark includes two single-sentence
classification tasks: SST-2 [79] and CoLA [87]; two multi-
sentence similarity classification tasks: MRPC [80] and QQP
[81]; one binary question answering task: QNLI [82]; three
natural language inference tasks: RTE [83], MNLI-M, and
MNLI-MM [84]; and one multi-sentence similarity regression
task: STS-B [85].
To assess the performance of the approach on text gen-
eration tasks, we conduct evaluations on several datasets.
The WikiText-103 [88] language modeling dataset, comprising
over 100 million tokens extracted from verified Good and
Featured articles on Wikipedia, serves as a primary evalua-
tion ground. Additionally, the PTB [89] dataset, featuring a
million words from 1989 Wall Street Journal material, and
the 1BW [90] dataset, established as a benchmark corpus
for measuring progress in statistical language modeling, are
included in our evaluation. Moreover, we assess the approach
on The LAMBADA [91] dataset, a collection of narrative
passages characterized by the intriguing feature that human
subjects can predict their last word when exposed to the entire
passage. Extracted from BookCorpus, The LAMBADA datasetcomprises 10,022 passages, contributing to a comprehensive
evaluation of the proposed method’s performance in decoder
models.
To further demonstrate the suggested approach’s per-
formance in language understanding, it is evaluated on
instruction-tuning tasks and assessed using the Massive Mul-
titask Language Understanding (MMLU) benchmark. This
benchmark includes exam questions from 57 diverse tasks,
covering subjects such as mathematics, history, law, and
medicine [92].
B. Models Settings
In the assessment of the method on BERT-base, the encoder
self-attention layer undergoes substitution with the proposed
encoder self-attention layer. The hyper-parameters are the
same as those in BERT-base with extra αEP, and αERhyper-
parameters are added to set the word-vector elimination rate
in each encoder layer. The hyper-parameters are: learning rate
- [1.5×10−5,4.5×10−5], batch size - {4, 8, 16, 32, 64,
128}, number of epochs - {2, 3, 4, 5 }, Dropout - [0.1, 0.15],
αER- [0.77, 0.97], and αEPvector that is obtained from (2).
The cross-entropy loss function is used for classification tasks
and the Mean Square Error (MSE) loss function for regression
tasks. The linear learning rate scheduler is used for all tasks.
Initial linear learning warm-up is used for some tasks. In all
tasks, AdamW optimizer with ϵ= 1×10−8,β1 = 0 .9and
β2 = 0 .999 is used. The model input sequence length is
delicately fixed so that less than 1% of samples are truncated.
The selected input sentence length for each dataset is specified
in Table II.
In the case of the GPT-2 model, the attention layer within
each decoder layer undergoes modification with the proposed
attention layer, which is described in Section III-E. Introducing
the hyper-parameters αEPandαERbecomes essential to
determine the word-vector elimination rate in each encoder
layer. The specified hyper-parameters for the evaluation are
as follows: a learning rate of 1×10−6, a batch size of 8,
a three-epoch training duration, a dropout rate of 0.1, αER
values ranging between 0.9 and 1.2, and an αEPvector derived
from equation (2). These parameters collectively define the
experimental setup for a comprehensive evaluation of the
method’s impact on the GPT-2 model.
For the instruction-tuning evaluation, the Flan-T51,
Gemma2, Mistral, and Llama3 models are targeted. In these
models, the self-attention layers are replaced with the pro-
posed self-attention layer, and other layers are adjusted to
accommodate a dynamic number of word vectors in each
encoder and decoder layer. The models use their default
weights without any tuning or fine-tuning. The parameter αER
is selected within the range [0.8, 1.0], and all other evaluation
hyperparameters remain consistent with those of the default
models.
C. Implementation Details
The initial models utilized for this study are the pre-trained
versions of BERT-base, GPT-2, Flan-T5, Gemma2, Mistral,
1Flan-T5 is an enhanced version of T5 [74]

--- PAGE 9 ---
9
TABLE II
EXPERIMENTAL RESULTS OF THE PROPOSED METHOD ON THE GLUE [78] B ENCHMARK AND IMDB [77]
DATASET WITH BERT- BASE AS THE TARGET MODEL
DatasetaCoLA SST-2 MRPC QQP QNLI RTE MNLI M MNLI MM STS-B IMDB Avg
BERT-base 52.7 93.5 88.7 71.2 91.0 68.9 84.7 83.8 84.3 93.7 81.25
This Work 53.4 92.6 88.0 70.1 90.2 67.9 83.7 82.7 84.2 92.7 80.55
Expected Speedupb3.7x 2.4x 2.9x 4.8x 2.3x 2.6x 2.5x 2.5x 2.2x 2.6x 2.8x
Inference Speedup 3.8x 2.4x 2.9x 4.7x 2.3x 2.6x 2.5x 2.5x 2.3x 2.7x 2.9x
Input Seq Length 64 64 128 128 128 256 128 128 64 512 160
aMatthew’s Correlation is reported for CoLA, F1-score for MRPC and QQP, Pearson Correlation for STS-B, and Accuracy for the
remaining tasks.
bUsing equation (7).
and Llama3 from the Hugging Face2implementations. The
implementation is carried out in the PyTorch3library, utilizing
an Nvidia Tesla P100 GPU for BERT-base, an Nvidia Tesla T4
GPU for GPT-2, and an Nvidia Tesla A100 GPU for Flan-T5,
Gemma2, Mistral, and Lllama. The datasets are sourced from
the Hugging Face Datasets library, and the GLUE results are
validated through the official GLUE service4. The instruction
tuning assessment is done with help of the INSTRUCTEV AL
[93] method. This framework ensures consistency and reliabil-
ity in the assessment of the proposed solution’s performance
on the specified models.
D. Experimental Results on Language Understanding Tasks
The experimental results of the proposed method applied to
BERT-base for a wide range of tasks are presented in Table II.
The proposed approach is 2.9 times faster than BERT-base,
with only a 0.7% accuracy drop on the test set. Additionally,
the model’s inference time is up to 4.8 times faster than BERT-
base, with just a 0.9% reduction in accuracy. These results
demonstrate that the suggested method significantly enhances
the performance and resource efficiency of Transformer-based
models while maintaining minimal accuracy loss.
The table shows the experimental speedup, and the expected
speedup. The inference speedup is consistent with expected
speedup predicted using (7). The slight differences are due
to the computational overheads, including the computational
effort of the tokenizer and output classifier layers that are not
considered in (7). The consistency between the experimental
speedup, the expected speedup, approves the hypothesis dis-
cussed in Section III-D.
Table III compares the proposed approach and the previ-
ously reported speedup methods. BERT-base model ( L=12,
h=12, d=768) is used as the basis for comparison. The test
results, the average accuracy drop compared to BERT-base,
the average number of FLOPs, and the average inference
speedup are reported for each model. As shown in the table,
the proposed approach achieves 2.9 times speedup with 0.73%on average accuracy drops compared to BERT-base model.
Compared to other works, DistilBERT model achieves 1.7
times speedup with around 2.74% accuracy degradation on
average. In DistilBERT, a pre-trained general-purpose lan-
guage presentation model must be trained from scratch with
significant effort. The TinyBERT model speedup is 2.0 times
with 0.4% accuracy drops on average. TinyBERT uses a two-
stage learning framework that performs Transformer distil-
lation at pre-training and fine-tuning phases with consider-
able excessive computational effort. The MobileBERT model
speedup is 5.5 times with around 1.19% accuracy drop, but
the starting point of this model is BERT large. The PoWER-
BERT model speedup is 2.9 times with around 0.92% accuracy
drop. However, the training of PoWER-BERT consists of 3
phases: ”fine-tuning, elimination configuration search, and re-
training” [28] that impose extra training effort. LayerDrop
and FPT speedups are 1.7 and 1.5 times, respectively, with
corresponding average accuracy drops of 1.47% and 0.93%.
The table shows that the results of the suggested technique
are competitive and more effective than previous baseline
methods across a wide range of experimental tasks. Further-
more, compared to recently reported methods, our approach is
applied only during the fine-tuning phase and offers the offline-
tuning property. Blank fields in the table, indicate instances
where some works did not report their assessments on certain
datasets.
To assess the impact of the proposed method on GPT-
2 model, the training process involves utilizing 1% of the
OpenWebText [94] dataset with a stride of 128 for three
epochs. Table IV exhibits the classification accuracy findings
of the proposed method in contrast to GPT-2 and DistilGPT2.
Notably, the proposed method attains an inference latency
three times quicker than baseline model, accompanied by a
marginal average accuracy reduction of 0.55%. Blank fields
in the tables, indicate instances where some works did not
report their assessments on certain datasets.
E. Experimental Results on Text Generation Tasks
2https://huggingface.co/
3https://pytorch.org/
4https://gluebenchmark.com/

--- PAGE 10 ---
10
TABLE III
COMPARISON OF RESULTS BETWEEN BERT BASE [3], D ISTIL BERT [49], T INYBERT [50], M OBILE BERT [51], P OWER-BERT [28],
LAYER DROP [42], FPT [37], AND LATENCY -ADJUSTABLE BERT BASE (THISWORK)
ModelaMethod CoLA SST-2 MRPC QQP QNLI RTE MNLI M MNLI MM Avg Drop FLOPs Speedup
BERT-base - 52.7 93.5 88.7 71.2 91.0 68.9 84.7 83.8 0.00 22.5B 1.0x
DistilBERT KD 51.3 91.3 87.5 70.1 89.2 59.9 82.2 - 2.74 11.3B 1.7x
TinyBERT KD 51.1 93.1 87.3 71.6 90.4 70 84.6 83.2 0.40 11.3B 2.0x
MobileBERTbKD 50.5 92.8 88.8 70.2 90.6 66.2 83.3 82.6 1.19 5.7B 5.5x
PoWER-BERTcPruning 52.3 92.1 88.1 70.2 90.1 67.4 83.8 83.1 0.92 - 2.9x
LayerDrop Pruning - 92.5 - - 89.4 - 82.9 - 1.47 11.3 1.7x
FPT Pruning - 92.6 - - 90.3 - 83.5 - 0.93 14.5 1.5x
This Work Pruning 53.4 92.6 88.0 70.1 90.2 67.9 83.7 82.7 0.73 7.7B 2.9x
aMatthew’s Correlation is reported for CoLA, F1-score for MRPC and QQP, Pearson Correlation for STS-B, and Accuracy for the remaining tasks.
bMobileBERT starts from the BERT-large architecture, which has approximately three times as many parameters as BERT-base.
cPoWER-BERT training involves three fine-tuning steps, and the model’s inference speedup is fixed during inference.
TABLE IV
ACCURACY COMPARISON BETWEEN GPT-2 [73] AND DISTIL GPT2 [49]
WITH LATENCY ADJUSTABLE GPT-2 (T HISWORK)
Model SST-2 IMDB Avg Drop Speedup
GPT-2 93.2 94.1 0.00 1.0x
DistilGPT2 91.5 92.1 1.85 1.9x
This Work 92.8 93.4 0.55 3.0x
Table V presents the results of the zero-shot perplexity
assessment for the suggested method in text generation tasks,
offering a basis for comparison with other contemporary
studies. The findings demonstrate that the method effectively
maintains the global context of the input sequence. The pro-
posed Attention Context Contribution (ACC) metric performs
well, ensuring that the elimination of word-vectors does not
compromise the input’s global context. It is noteworthy that
the original GPT-2 model, trained by OpenAI, benefited from
more extensive hardware and dataset resources. However, our
proposed architecture enables efficient adaptation with just a
few small epochs. Furthermore, the proposed technique ex-
hibits superior performance when compared to the DistilGPT2
[49] and ZipGPT2 [95] models in the evaluation.
F . Experimental Results on Instruction Tuning Tasks
For the instruction-tuning evaluation, the Flan-T5, Gemma2,
Mistral, and Llama3 models are selected. In these models,
the self-attention layers are replaced with the proposed en-
coder self-attention layer, and the default weights are used
without any additional tuning or fine-tuning. The method’s
performance is evaluated using various prompting types on the
MMLU benchmark. Table VI presents the experimental results
for 5-shot direct prompting on the MMLU benchmark. As
shown, the suggested method achieves up to a 3-fold reductionTABLE V
PERPLEXITY (PPL) C OMPARISON BETWEEN GPT-2 [73], D ISTIL GPT2
[49], Z IPGPT2 [95], AND LATENCY -ADJUSTABLE GPT-2
(THISWORK)
ModelWiki
Text-103a LAMBADA PTB 1BWbSpeedup
GPT-2 37.5 35.1 36.46 45.54 1.0x
DistilGPT2 43.0 70.43 63.57 75.40 1.9x
ZipGPT2 72.1 - - - 3.3x
This Work43.2 59.0 52.9 51.66 2.7x
49.3 64.6 61.7 54.53 3.5x
aLower perplexity is better.
bThe models are evaluated on 0.5% of the 1BW test set.
in TTFT with only a marginal decrease in score.
G. Offline-Tuning Property
After the fine-tuning phase, the trade-off between inference
accuracy and speed of the model can be adjusted with the
offline-tuning property. This property controls each encoder
layer’s word-vector elimination rate by adjusting the Speedup-
Coefficient hyper-parameter ( αSC) value. Fig. 6 presents the
offline-tuning property results on BERT-base. In the following
experiments, the model first fine-tuned with αSCequal to one
for a given task, and then in the offline-tuning phase, the
αSCvalue changes from 0.85 to 1.2 to observe its effect on
inference speedup and accuracy.
Fig. 6a shows the model inference speedup for a range of
αSChyper-parameter values on multiple tasks. The model
inference speedup increased more than three times compared
to BERT-base, with the αSCvalue decreasing to 0.85. Fig. 6b
presents the accuracy stability on multiple tasks. It shows
that the inference results are kept stable over a wide range
ofαSCvalues selections. Fig. 6c shows that the inference
results are almost unchanged even if the complexity of the
model reduces by 70% according to the proposed method. In
these experiments, the model’s accuracy on validation sets is

--- PAGE 11 ---
11
0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20
Speedup Coefficient (SC)
1.01.52.02.53.03.54.0Inference Speedup Compare to BERTbaseMRPC
SST-2
QNLI
IMDB
MNLI-M
MNLI-MM
(a)
0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20
Speedup Coefficient (SC)
020406080100Validation Set Result % MRPC
SST-2
QNLI
IMDB
MNLI-M
MNLI-MM (b)
1.0 1.5 2.0 2.5 3.0 3.5
Inference Speedup Compare to BERTbase020406080100Validation Set Result % MRPC
SST-2
QNLI
IMDB
MNLI-M
MNLI-MM (c)
Fig. 6. The proposed offline-tuning results, (a)Inference speedup compare to BERT-base versus αSC, (b)Validation set results versus αSC, (c)Validation set
results versus speedup compare to BERT-base for MRPC [80], SST-2 [79], QNLI [82], IMDB [77] and MNLI-M/MM [84] tasks.
TABLE VI
TIME-TO-FIRST-TOKEN (TTFT) S PEEDUP RESULTS OF THE PROPOSED
APPROACH ON FLAN-T5 [86], G EMMA 2 [75], M ISTRAL [72], AND
LLAMA 3 [76] E VALUATED WITH THE MMLU [92]
INSTRUCTION TUNING DATASET .
Architecture
NameModelModel
SizeMMLU
ScoreaTTFT
Speedupb
T5Flan-T5-base 220M34.05 1.00x
32.24 1.54x
Flan-T5-large 770M41.95 1.00x
41.51 1.95x
Gemma Gemma2 2B56.74 1.00x
56.20 1.63x
Mistral Mistral 7B41.85 1.00x
40.83 1.65x
Llama Llama3 8B65.69 1.00x
64.91 2.86x
aMMLU includes exam questions from 57 tasks, uses 5-shot direct
prompting, and measure the exact-match score.
bThe proposed attention layer is applied to the models, and the Time-
to-First-Token (TTFT) speedup is reported.
reported because of the GLUE test set verification limitations.
Two approaches can be offered to select the proper value
of the αSChyper-parameter in the fine-tuning phase. The first
approach is using equation (7) and calculating the approximate
value of the αSCbased on the desired inference speedup. The
second approach is to select the αSCvalue using suggested
Fig. 6(c) curves. After the fine-tuning phase, the speedup can
be accurately adjusted with the offline-tuning property.
The offline perplexity (PPL) stability of the proposed
method for text generation tasks is evaluated by training the
model with a specific speedup coefficient and then modifying
this value during inference, while measuring the model’s
zero-shot perplexity. Fig. 7 illustrates the method’s offline
perplexity results on GPT-2 across various datasets, includ-
ing WikiText-103, LAMBADA, PTB, and 1BW. The graph
demonstrates the model’s robust stability, showing a smooth
1.0 1.5 2.0 2.5 3.0 3.5 4.0
Inference Speedup Compare to GPT-2020406080100Offline Perplexity of The Proposed Method (PPL)WikiT ext-103
LAMBADA
PTB
1BWFig. 7. Offline Perplexity (PPL) of the proposed method on WikiText-103
[88], LAMBADA [91], PTB [89] and 1BW [90] datasets.
perplexity curve across a wide range of speedup values,
which highlights the solution’s effectiveness in maintaining
consistent performance under varying conditions.
H. Ablation Studies
Table VII presents the ablation study on the proposed self-
attention layer. The first row of the table presents the original
BERT-base results, and the subsequent rows are the results
of the self-attention layer with different types of word-vector
sorting strategies.
The first approach involves removing the concluding word-
vectors from the contextual sequence without the use of a
sorting layer. The second approach employs the Random-Sort-
Layer, which shuffles word-vectors and randomly eliminates
them. The final and better strategy is the proposed strategy
that incorporates the SV-Sort-Layer that arranges word-vectors
according to their Score values, subsequently removing the
concluding word-vectors.

--- PAGE 12 ---
12
TABLE VII
ABLATION STUDY ON THE PROPOSED SELF -ATTENTION LAYER WITH DIFFERENT SELF -ATTENTION SORT LAYER
IMPLEMENTATIONS STRATEGIES ON BERT- BASE [3].
ModelaSST-2 MRPC MNLI-M MNLI-MM STS-B IMDB Avg
BERT-base Self-Attention 93.5 88.7 84.7 83.8 84.3 93.7 88.12
+ Eliminate-Layer 89.9 84.4 81.7 80.4 83.3 90.7 85.07
+ Eliminate-Layer + Random-Sort-Layer 84.5 80.4 63.6 64.4 55.7 87.5 72.68
+ Eliminate-Layer + SV-Sort-Layer 92.6 88.0 83.7 82.7 84.2 92.7 87.32
aF1-score for MRPC, Pearson Correlation for STS-B, and Accuracy for the remaining tasks.
As shown in the table, the first strategy shows a significant
accuracy drop compared to the original BERT-base and the
proposed method. The second strategy has a destructive effect
on the model results, and the fine-tuning process becomes
completely unstable in CoLA, QQP, QNLI, and RTE datasets
not presented in the table. The third strategy is superior to
other investigated strategies and proves that the proposed self-
attention Sort layer is effective. In these experiments, the
hyper-parameters values and the fine-tuning conditions are the
same.
V. C ONCLUSION
This paper presents a latency-adjustable Transformer ar-
chitecture applicable to a wide range of models, with a
marginal decrease in accuracy. We introduce a novel Attention
Context Contribution (ACC) metric to evaluate the context
contribution of each Transformer encoder layer, demonstrat-
ing experimentally that word-vector contributions diminish in
the last encoder layers. Extensive experiments on language
understanding, text generation, and instruction-tuning tasks
validate the suggested technique’s effectiveness across diverse
datasets, with minimal impact on the input’s global context.
The paper also provides an accurate analytical estimation
of speedup, assisting system designers with hyper-parameter
selection. Additionally, the method’s powerful offline-tuning
capability allows for model speedup adjustments after the
fine-tuning phase. The proposed approach highlights that,
while the complete network is essential for training, it can
be truncated during the fine-tuning phase in Large Language
Models (LLMs).
ACKNOWLEDGMENT
Thanks to Mr. Seyed Mahdi Hosseini for his assistance and
comments that greatly improved our work.
REFERENCES
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, 2017.
[2] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy con-
siderations for modern deep learning research,” in Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 34, no. 09, 2020, pp.
13 693–13 696.
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.[4] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and
Q. V . Le, “Xlnet: Generalized autoregressive pretraining for language
understanding,” Advances in neural information processing systems ,
vol. 32, 2019.
[5] T. B. Brown, “Language models are few-shot learners,” arXiv preprint
arXiv:2005.14165 , 2020.
[6] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. ,
“Llama: Open and efficient foundation language models,” arXiv preprint
arXiv:2302.13971 , 2023.
[7] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak,
L. Sifre, M. Rivi `ere, M. S. Kale, J. Love et al. , “Gemma: Open
models based on gemini research and technology,” arXiv preprint
arXiv:2403.08295 , 2024.
[8] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. , “Improving
language understanding by generative pre-training,” 2018.
[9] M. Gupta and P. Agrawal, “Compression of deep learning models for
text: A survey,” ACM Transactions on Knowledge Discovery from Data
(TKDD) , vol. 16, no. 4, pp. 1–55, 2022.
[10] S. Wiedemann, K.-R. M ¨uller, and W. Samek, “Compact and com-
putationally efficient representation of deep neural networks,” IEEE
transactions on neural networks and learning systems , vol. 31, no. 3,
pp. 772–785, 2019.
[11] Y . Jiang, S. Wang, V . Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and
L. Tassiulas, “Model pruning enables efficient federated learning on edge
devices,” IEEE Transactions on Neural Networks and Learning Systems ,
2022.
[12] Z. Tang, L. Luo, B. Xie, Y . Zhu, R. Zhao, L. Bi, and C. Lu, “Automatic
sparse connectivity learning for neural networks,” IEEE Transactions on
Neural Networks and Learning Systems , 2022.
[13] M. R. Ganesh, D. Blanchard, J. J. Corso, and S. Y . Sekeh, “Slimming
neural networks using adaptive connectivity scores,” IEEE Transactions
on Neural Networks and Learning Systems , 2022.
[14] Y . Ma, X. Yao, R. Chen, R. Li, X. Shen, and B. Yu, “Small is beautiful:
Compressing deep neural networks for partial domain adaptation,” IEEE
Transactions on Neural Networks and Learning Systems , 2022.
[15] Y . Chen, R. Li, W. Li, J. Wang, and R. Li, “Three-stage global channel
pruning for resources-limited platform,” IEEE Transactions on Neural
Networks and Learning Systems , 2023.
[16] Y . Guan, N. Liu, P. Zhao, Z. Che, K. Bian, Y . Wang, and J. Tang,
“Dais: Automatic channel pruning via differentiable annealing indicator
search,” IEEE Transactions on Neural Networks and Learning Systems ,
2022.
[17] C. Peng, Y . Li, R. Shang, and L. Jiao, “Recnas: Resource-constrained
neural architecture search based on differentiable annealing and dynamic
pruning,” IEEE Transactions on Neural Networks and Learning Systems ,
2022.
[18] Y . Qian, Z. He, Y . Wang, B. Wang, X. Ling, Z. Gu, H. Wang,
S. Zeng, and W. Swaileh, “Hierarchical threshold pruning based on
uniform response criterion,” IEEE Transactions on Neural Networks and
Learning Systems , 2023.
[19] G. Liu, K. Zhang, and M. Lv, “Soks: Automatic searching of the optimal
kernel shapes for stripe-wise network pruning,” IEEE Transactions on
Neural Networks and Learning Systems , 2022.
[20] Y .-J. Zheng, S.-B. Chen, C. H. Ding, and B. Luo, “Model compression
based on differentiable network channel pruning,” IEEE Transactions
on Neural Networks and Learning Systems , 2022.
[21] H. Salehinejad and S. Valaee, “Edropout: Energy-based dropout and
pruning of deep neural networks,” IEEE Transactions on Neural Net-
works and Learning Systems , vol. 33, no. 10, pp. 5279–5292, 2021.

--- PAGE 13 ---
13
[22] T. Zhang, S. Ye, X. Feng, X. Ma, K. Zhang, Z. Li, J. Tang, S. Liu,
X. Lin, Y . Liu et al. , “Structadmm: Achieving ultrahigh efficiency in
structured pruning for dnns,” IEEE transactions on neural networks and
learning systems , vol. 33, no. 5, pp. 2259–2273, 2021.
[23] Z. Chen, T.-B. Xu, C. Du, C.-L. Liu, and H. He, “Dynamical channel
pruning by conditional accuracy change for deep neural networks,” IEEE
transactions on neural networks and learning systems , vol. 32, no. 2,
pp. 799–813, 2020.
[24] M. Lin, R. Ji, S. Li, Y . Wang, Y . Wu, F. Huang, and Q. Ye, “Network
pruning using adaptive exemplar filters,” IEEE Transactions on Neural
Networks and Learning Systems , vol. 33, no. 12, pp. 7357–7366, 2021.
[25] G. Li, P. Yang, C. Qian, R. Hong, and K. Tang, “Stage-wise magnitude-
based pruning for recurrent neural networks,” IEEE Transactions on
Neural Networks and Learning Systems , 2022.
[26] Y . Zhou, G. G. Yen, and Z. Yi, “Evolutionary shallowing deep neural
networks at block levels,” IEEE Transactions on Neural Networks and
Learning Systems , vol. 33, no. 9, pp. 4635–4647, 2021.
[27] Y . Bian, Q. Song, M. Du, J. Yao, H. Chen, and X. Hu, “Subarchitecture
ensemble pruning in neural architecture search,” IEEE Transactions on
Neural Networks and Learning Systems , vol. 33, no. 12, pp. 7928–7936,
2021.
[28] S. Goyal, A. R. Choudhury, S. Raje, V . Chakaravarthy, Y . Sabharwal,
and A. Verma, “Power-bert: Accelerating bert inference via progres-
sive word-vector elimination,” in International Conference on Machine
Learning . PMLR, 2020, pp. 3690–3699.
[29] Z. Dai, G. Lai, Y . Yang, and Q. Le, “Funnel-transformer: Filtering out
sequential redundancy for efficient language processing,” Advances in
neural information processing systems , vol. 33, pp. 4271–4282, 2020.
[30] S. Kim, S. Shen, D. Thorsley, A. Gholami, W. Kwon, J. Hassoun, and
K. Keutzer, “Learned token pruning for transformers,” in Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining , 2022, pp. 784–794.
[31] M. A. Gordon, K. Duh, and N. Andrews, “Compressing bert: Studying
the effects of weight pruning on transfer learning,” arXiv preprint
arXiv:2002.08307 , 2020.
[32] F.-M. Guo, S. Liu, F. S. Mungall, X. Lin, and Y . Wang, “Reweighted
proximal pruning for large-scale language representation,” arXiv preprint
arXiv:1909.12486 , 2019.
[33] E. Yoo, G. Park, J. G. Min, S. J. Kwon, B. Park, D. Lee, and Y . Lee, “Tf-
mvp: Novel sparsity-aware transformer accelerator with mixed-length
vector pruning,” in 2023 60th ACM/IEEE Design Automation Conference
(DAC) . IEEE, 2023, pp. 1–6.
[34] C. Louizos, M. Welling, and D. P. Kingma, “Learning sparse neural
networks through l0regularization,” arXiv preprint arXiv:1712.01312 ,
2017.
[35] E. V oita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Analyzing
multi-head self-attention: Specialized heads do the heavy lifting, the rest
can be pruned,” arXiv preprint arXiv:1905.09418 , 2019.
[36] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really better
than one?” Advances in neural information processing systems , vol. 32,
2019.
[37] W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer, and
A. Gholami, “A fast post-training pruning framework for transform-
ers,” Advances in Neural Information Processing Systems , vol. 35, pp.
24 101–24 116, 2022.
[38] M. Zhang and Y . He, “Accelerating training of transformer-based lan-
guage models with progressive layer dropping,” Advances in Neural
Information Processing Systems , vol. 33, pp. 14 011–14 023, 2020.
[39] W. Liu, P. Zhou, Z. Zhao, Z. Wang, H. Deng, and Q. Ju, “Fastbert:
a self-distilling bert with adaptive inference time,” arXiv preprint
arXiv:2004.02178 , 2020.
[40] K. Xie, S. Lu, M. Wang, and Z. Wang, “Elbert: Fast albert with
confidence-window based early exit,” in ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2021, pp. 7713–7717.
[41] H. Sajjad, F. Dalvi, N. Durrani, and P. Nakov, “Poor man’s bert: Smaller
and faster transformer models,” arXiv preprint arXiv:2004.03844 , 2020.
[42] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth on demand
with structured dropout,” arXiv preprint arXiv:1909.11556 , 2019.
[43] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention . Springer,
2015, pp. 234–241.
[44] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.[45] H. Lee, M. Kang, Y . Lee, and S. J. Hwang, “Sparse token transformer
with attention back tracking,” in The Eleventh International Conference
on Learning Representations , 2022.
[46] S. Anagnostidis, D. Pavllo, L. Biggio, L. Noci, A. Lucchi, and T. Hof-
mann, “Dynamic context pruning for efficient and interpretable au-
toregressive transformers,” Advances in Neural Information Processing
Systems , vol. 36, 2024.
[47] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and
J. Carreira, “Perceiver: General perception with iterative attention,” in
International conference on machine learning . PMLR, 2021, pp. 4651–
4664.
[48] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,
“Albert: A lite bert for self-supervised learning of language representa-
tions,” arXiv preprint arXiv:1909.11942 , 2019.
[49] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled
version of bert: smaller, faster, cheaper and lighter,” arXiv preprint
arXiv:1910.01108 , 2019.
[50] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and
Q. Liu, “Tinybert: Distilling bert for natural language understanding,”
arXiv preprint arXiv:1909.10351 , 2019.
[51] Z. Sun, H. Yu, X. Song, R. Liu, Y . Yang, and D. Zhou, “Mobilebert: a
compact task-agnostic bert for resource-limited devices,” arXiv preprint
arXiv:2004.02984 , 2020.
[52] T.-B. Xu and C.-L. Liu, “Deep neural network self-distillation exploiting
data representation invariance,” IEEE Transactions on Neural Networks
and Learning Systems , vol. 33, no. 1, pp. 257–269, 2020.
[53] C. Yun, Y .-W. Chang, S. Bhojanapalli, A. S. Rawat, S. Reddi, and
S. Kumar, “O (n) connections are expressive enough: Universal ap-
proximability of sparse transformers,” Advances in Neural Information
Processing Systems , vol. 33, pp. 13 783–13 794, 2020.
[54] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and
L. Kong, “Random feature attention,” arXiv preprint arXiv:2103.02143 ,
2021.
[55] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, “Transformers
are rnns: Fast autoregressive transformers with linear attention,” in
International conference on machine learning . PMLR, 2020, pp. 5156–
5165.
[56] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,
T. Sarlos, P. Hawkins, J. Davis, D. Belanger, L. Colwell et al. , “Masked
language modeling for proteins via linearly scalable long-context trans-
formers,” arXiv preprint arXiv:2006.03555 , 2020.
[57] I. Schlag, K. Irie, and J. Schmidhuber, “Linear transformers are secretly
fast weight programmers,” in International Conference on Machine
Learning . PMLR, 2021, pp. 9355–9366.
[58] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer:
Self-attention with linear complexity,” arXiv preprint arXiv:2006.04768 ,
2020.
[59] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, “Set trans-
former: A framework for attention-based permutation-invariant neural
networks,” in International conference on machine learning . PMLR,
2019, pp. 3744–3753.
[60] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al. , “Big bird:
Transformers for longer sequences,” Advances in neural information
processing systems , vol. 33, pp. 17 283–17 297, 2020.
[61] H. Jiang, Y . Li, C. Zhang, Q. Wu, X. Luo, S. Ahn, Z. Han, A. H.
Abdi, D. Li, C.-Y . Lin et al. , “Minference 1.0: Accelerating pre-filling
for long-context llms via dynamic sparse attention,” arXiv preprint
arXiv:2407.02490 , 2024.
[62] Q. Zhu, J. Duan, C. Chen, S. Liu, X. Li, G. Feng, X. Lv, H. Cao,
X. Chuanfu, X. Zhang et al. , “Near-lossless acceleration of long context
llm inference with adaptive structured sparse attention,” arXiv preprint
arXiv:2406.15486 , 2024.
[63] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language models,”
arXiv preprint arXiv:2106.09685 , 2021.
[64] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:
Efficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314 ,
2023.
[65] H. Guo, P. Greengard, E. P. Xing, and Y . Kim, “Lq-lora: Low-rank plus
quantized matrix decomposition for efficient language model finetuning,”
arXiv preprint arXiv:2311.12023 , 2023.
[66] J. Hewitt, J. Thickstun, C. D. Manning, and P. Liang, “Backpack
language models,” arXiv preprint arXiv:2305.16765 , 2023.
[67] M. Cho, M. Rastegari, and D. Naik, “Kv-runahead: Scalable causal
llm inference by parallel key-value cache generation,” arXiv preprint
arXiv:2405.05329 , 2024.

--- PAGE 14 ---
14
[68] Y . Liu, H. Li, Y . Cheng, S. Ray, Y . Huang, Q. Zhang, K. Du, J. Yao,
S. Lu, G. Ananthanarayanan et al. , “Cachegen: Kv cache compression
and streaming for fast large language model serving,” in Proceedings of
the ACM SIGCOMM 2024 Conference , 2024, pp. 38–56.
[69] J. Yao, H. Li, Y . Liu, S. Ray, Y . Cheng, Q. Zhang, K. Du, S. Lu, and
J. Jiang, “Cacheblend: Fast large language model serving with cached
knowledge fusion,” arXiv preprint arXiv:2405.16444 , 2024.
[70] I. Gim, G. Chen, S.-s. Lee, N. Sarda, A. Khandelwal, and L. Zhong,
“Prompt cache: Modular attention reuse for low-latency inference,”
Proceedings of Machine Learning and Systems , vol. 6, pp. 325–338,
2024.
[71] B. Gao, Z. He, P. Sharma, Q. Kang, D. Jevdjic, J. Deng, X. Yang, Z. Yu,
and P. Zuo, “ {Cost-Efficient }large language model serving for multi-
turn conversations with {CachedAttention },” in 2024 USENIX Annual
Technical Conference (USENIX ATC 24) , 2024, pp. 111–126.
[72] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al. ,
“Mistral 7b,” arXiv preprint arXiv:2310.06825 , 2023.
[73] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
“Language models are unsupervised multitask learners,” OpenAI blog ,
vol. 1, no. 8, p. 9, 2019.
[74] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning
with a unified text-to-text transformer,” Journal of machine learning
research , vol. 21, no. 140, pp. 1–67, 2020.
[75] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju,
L. Hussenot, T. Mesnard, B. Shahriari, A. Ram ´eet al. , “Gemma 2:
Improving open language models at a practical size,” arXiv preprint
arXiv:2408.00118 , 2024.
[76] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
A. Mathur, A. Schelten, A. Yang, A. Fan et al. , “The llama 3 herd of
models,” arXiv preprint arXiv:2407.21783 , 2024.
[77] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts,
“Learning word vectors for sentiment analysis,” in Proceedings of the
49th annual meeting of the association for computational linguistics:
Human language technologies , 2011, pp. 142–150.
[78] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,
“Glue: A multi-task benchmark and analysis platform for natural lan-
guage understanding,” arXiv preprint arXiv:1804.07461 , 2018.
[79] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng, and
C. Potts, “Recursive deep models for semantic compositionality over a
sentiment treebank,” in Proceedings of the 2013 conference on empirical
methods in natural language processing , 2013, pp. 1631–1642.
[80] B. Dolan and C. Brockett, “Automatically constructing a corpus of sen-
tential paraphrases,” in Third International Workshop on Paraphrasing
(IWP2005) , 2005.
[81] S. Iyer, N. Dandekar, K. Csernai et al. , “First quora dataset release:
Question pairs,” data. quora. com , 2017.
[82] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+
questions for machine comprehension of text,” arXiv preprint
arXiv:1606.05250 , 2016.
[83] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo, “The fifth pascal
recognizing textual entailment challenge.” in TAC, 2009.
[84] A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage
challenge corpus for sentence understanding through inference,” arXiv
preprint arXiv:1704.05426 , 2017.
[85] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia, “Semeval-
2017 task 1: Semantic textual similarity-multilingual and cross-lingual
focused evaluation,” arXiv preprint arXiv:1708.00055 , 2017.
[86] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li,
X. Wang, M. Dehghani, S. Brahma et al. , “Scaling instruction-finetuned
language models,” Journal of Machine Learning Research , vol. 25,
no. 70, pp. 1–53, 2024.
[87] A. Warstadt, A. Singh, and S. R. Bowman, “Neural network accept-
ability judgments,” Transactions of the Association for Computational
Linguistics , vol. 7, pp. 625–641, 2019.
[88] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel
mixture models,” 2016.
[89] M. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a large
annotated corpus of english: The penn treebank,” Computational lin-
guistics , vol. 19, no. 2, pp. 313–330, 1993.
[90] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and
T. Robinson, “One billion word benchmark for measuring progress in
statistical language modeling,” 2014.
[91] D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi,
S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez, “The LAMBADA
dataset: Word prediction requiring a broad discourse context,” inProceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) . Berlin, Germany:
Association for Computational Linguistics, August 2016, pp. 1525–
1534. [Online]. Available: http://www.aclweb.org/anthology/P16-1144
[92] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and
J. Steinhardt, “Measuring massive multitask language understanding,”
arXiv preprint arXiv:2009.03300 , 2020.
[93] Y . K. Chia, P. Hong, L. Bing, and S. Poria, “Instructeval: Towards
holistic evaluation of instruction-tuned large language models,” arXiv
preprint arXiv:2306.04757 , 2023.
[94] A. Gokaslan and V . Cohen, “Openwebtext corpus,” http://Skylion007.
github.io/OpenWebTextCorpus, 2019.
[95] E. Kurtic, E. Frantar, and D. Alistarh, “Ziplm: Hardware-aware struc-
tured pruning of language models,” arXiv preprint arXiv:2302.04089 ,
2023.
Sajjad Kachuee received the B.Sc. and M.Sc. de-
grees in electrical engineering from Sharif Univer-
sity of Technology, Tehran, Iran, in 2017 and 2019,
where he is currently pursuing the Ph.D. degree. His
research concerns artificial intelligence and machine
learning, focusing on acceleration and optimization
of natural language processing (NLP) neural net-
works.
Mohammad Sharifkhani received the B.Sc. and
M.A.Sc. degrees in electrical and computer engi-
neering from the University of Tehran, Tehran, Iran,
in 1998 and 2000, respectively. He received the
Ph.D. degree from the University of Waterloo, Wa-
terloo, ON, Canada, in 2006. He was a Postdoctoral
Research Fellow at the University of Waterloo in
2007. He is currently an Associate Professor at
the Department of Electrical Engineering, Sharif
University of Technology, Tehran, Iran. Since 2008
he has published several scientific articles on the
broad field of VLSI and Digital Systems. He served as technical committee
member and reviewer of several IEEE conferences and journals, respectively.
He founded several start-up companies on the broad field of video and image
processing as well as machine intelligent systems. His current research is on
low-power circuits and architectures, data converters, and application-specific
processors for video and machine learning applications.
