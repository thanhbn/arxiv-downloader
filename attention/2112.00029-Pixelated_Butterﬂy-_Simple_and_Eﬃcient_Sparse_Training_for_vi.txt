# 2112.00029.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2112.00029.pdf
# Kích thước tệp: 2521209 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Pixelated Butterfly: Huấn luyện thưa thớt đơn giản và hiệu quả cho các mô hình mạng nơ-ron
Tri Dao∗y, Beidi Chen∗y, Kaizhao Liang, Jiaming Yang, Zhao Songx, Atri Rudraz, và
Christopher Réy
yKhoa Khoa học Máy tính, Đại học Stanford
SambaNova Systems, Inc
Khoa Xác suất và Thống kê, Đại học Bắc Kinh
xAdobe Research
zKhoa Khoa học Máy tính và Kỹ thuật, Đại học Buffalo, SUNY
{trid,beidic}@stanford.edu ,kaizhao.liang@sambanovasystems.com ,edwinyjmpku@gmail.com ,
zsong@adobe.com ,atri@buffalo.edu ,chrismre@cs.stanford.edu
12 tháng 5, 2022

Tóm tắt
Các mạng nơ-ron được tham số hóa quá mức có khả năng tổng quát hóa tốt nhưng tốn kém để huấn luyện. Lý tưởng nhất, người ta muốn giảm chi phí tính toán của chúng trong khi vẫn giữ được lợi ích tổng quát hóa. Huấn luyện mô hình thưa thớt là một phương pháp đơn giản và hứa hẹn để đạt được điều này, nhưng vẫn còn những thách thức vì các phương pháp hiện tại gặp khó khăn với việc mất độ chính xác, thời gian chạy huấn luyện chậm, hoặc khó khăn trong việc làm thưa thớt tất cả các thành phần mô hình. Vấn đề cốt lõi là việc tìm kiếm một mặt nạ thưa thớt trên một tập hợp rời rạc các ma trận thưa thớt là khó khăn và tốn kém. Để giải quyết điều này, hiểu biết chính của chúng tôi là tối ưu hóa trên một siêu tập liên tục của các ma trận thưa thớt với cấu trúc cố định được gọi là tích của các ma trận butterfly. Vì các ma trận butterfly không hiệu quả về mặt phần cứng, chúng tôi đề xuất các biến thể đơn giản của butterfly (khối và phẳng) để tận dụng phần cứng hiện đại. Phương pháp của chúng tôi (Pixelated Butterfly) sử dụng một mẫu thưa thớt cố định đơn giản dựa trên flat block butterfly và ma trận thấp hạng để làm thưa thớt hầu hết các lớp mạng (ví dụ: attention, MLP). Chúng tôi xác thực thực nghiệm rằng Pixelated Butterfly nhanh hơn 3× so với butterfly và tăng tốc huấn luyện để đạt được sự cân bằng độ chính xác-hiệu quả thuận lợi. Trên các tác vụ phân loại ImageNet và mô hình hóa ngôn ngữ WikiText-103, các mô hình thưa thớt của chúng tôi huấn luyện nhanh hơn tới 2.5× so với MLP-Mixer, Vision Transformer và GPT-2 medium dày đặc mà không giảm độ chính xác.

1 Giới thiệu
Các kết quả gần đây cho thấy rằng các mạng nơ-ron được tham số hóa quá mức có khả năng tổng quát hóa tốt [Belkin et al., 2019], nhưng chúng tốn kém để huấn luyện [Kaplan et al., 2020]. Một mô hình lý tưởng nên sử dụng ít tính toán và bộ nhớ hơn trong khi vẫn giữ được lợi ích tổng quát hóa của các mô hình lớn. Hướng đơn giản và phổ biến nhất là làm thưa thớt các mô hình này. Ý tưởng này có lịch sử lâu dài trong học máy [LeCun et al., 1990] và đã thúc đẩy tiến bộ cơ bản trong các lĩnh vực khác như thống kê [Tibshirani, 1996], khoa học thần kinh [Foldiak, 2003], và xử lý tín hiệu [Candes et al., 2006]. Tuy nhiên, mặc dù có những nỗ lực đáng kể, việc tăng tốc huấn luyện thưa thớt về thời gian thực tế mà không làm giảm độ chính xác vẫn là một vấn đề chưa được giải quyết.

Mặc dù huấn luyện thưa thớt là một lĩnh vực nghiên cứu tích cực, nó vẫn chưa được áp dụng rộng rãi. Đầu tiên, việc tìm kiếm mẫu thưa thớt (các vị trí có thể có của các phần tử khác không) có thể duy trì cùng mức độ chính xác của các mô hình dày đặc là khó khăn và tốn kém. Nhiều phương pháp (pruning [Lee et al., 2018], lottery tickets [Frankle và Carbin, 2018], hashing [Kitaev et al., 2020]) duy trì các mặt nạ thưa thớt động. Tuy nhiên, chi phí lớn của việc phát triển mặt nạ thưa thớt có thể làm chậm đáng kể quá trình huấn luyện và phức tạp hóa việc triển khai. Thực tế, các phương pháp này hoặc yêu cầu các chu kỳ dài pruning và huấn luyện lại [Frankle và Carbin, 2018]¹ hoặc duy trì các bảng băm tốn kém [Chen et al., 2019]. Thứ hai, hầu hết các phương pháp hiện tại áp dụng thưa thớt không có cấu trúc, có thể hiệu quả về mặt lý thuyết, nhưng không tính đến hiệu quả của phần cứng huấn luyện như GPU (được tối ưu hóa cho tính toán dày đặc)². Cuối cùng, hầu hết các phương pháp nhắm đến một loại hoạt động duy nhất như attention [Child et al., 2019, Zaheer et al., 2020], trong khi các mô hình mạng nơ-ron (NN) thường bao gồm các module khác nhau (attention, MLP), và trong nhiều ứng dụng, các lớp MLP là nút thắt cổ chai huấn luyện chính [Wu et al., 2020].

Một phương pháp huấn luyện thưa thớt tốt hơn nên (i) đơn giản nhưng chính xác, lý tưởng với mẫu thưa thớt tĩnh, (ii) nhanh bằng cách căn chỉnh mẫu thưa thớt với phần cứng có sẵn, và (iii) có phạm vi bao phủ rộng các toán tử áp dụng cho hầu hết các lớp NN. Có ba thách thức kỹ thuật. Đầu tiên, chúng tôi chỉ ra rằng với một ngân sách cho trước (ví dụ: tổng số phần tử khác không trong ma trận), việc tìm mẫu thưa thớt tĩnh tối ưu cho một module NN để giảm thiểu sai số xấp xỉ với mô hình dày đặc là NP-hard. Thứ hai, đối với mỗi mẫu thưa thớt, chúng ta cần tính đến hiệu quả hướng khối của phần cứng (truy cập mỗi phần tử trong bộ nhớ mất cùng thời gian như truy cập khối các phần tử liền kề [Cook, 2012], được minh họa trong Hình 2). Các thước đo hiệu quả lý thuyết thông thường (ví dụ: số phần tử khác không, FLOP) không ánh xạ tốt với phần cứng hiện đại được thiết kế cho tính toán khối. Cuối cùng, mỗi module NN khác nhau có thể yêu cầu các mẫu thưa thớt khác nhau, điều này khiến vấn đề trở nên phức tạp hơn.

Trong khám phá ban đầu của chúng tôi, chúng tôi nghiên cứu thực nghiệm nhiều mẫu thưa thớt được đề xuất trong tài liệu để tìm những mẫu có thể xấp xỉ chặt chẽ mô hình dày đặc (Chi tiết trong Phụ lục K). Chúng tôi phát hiện ra rằng một mẫu thưa thớt, cụ thể là butterfly + low-rank, liên tục vượt trội hơn các mẫu khác. Mẫu thưa thớt này kết nối chặt chẽ với hai dòng nghiên cứu về cấu trúc ma trận: (i) ma trận thưa thớt + low-rank, có thể nắm bắt thông tin toàn cục và cục bộ [Candès et al., 2011, Udell và Townsend, 2019, Chen et al., 2021], và (ii) ma trận butterfly [Parker, 1995, Dao et al., 2019] mà các tích của chúng có thể biểu diễn chặt chẽ bất kỳ ma trận thưa thớt nào [De Sa et al., 2018, Dao et al., 2020]. Sử dụng mẫu thưa thớt cố định từ ma trận butterfly, với việc bổ sung một hạng thấp, sẽ giải quyết hai trong ba thách thức trên và tạo ra một cách đơn giản để làm thưa thớt hầu hết các lớp NN (dựa trên phép nhân ma trận).

Tuy nhiên, các ma trận butterfly không hiệu quả trên phần cứng hiện đại: (i) chúng khó song song hóa vì chứa các tích tuần tự của nhiều yếu tố, và (ii) chúng không thân thiện với phần cứng vì các mẫu thưa thớt không được căn chỉnh khối. Chúng tôi đề xuất hai thay đổi đơn giản để làm cho Butterfly hiệu quả trong khi vẫn giữ được các tính chất thuận lợi của chúng. Đề xuất của chúng tôi, Pixelated Butterfly (Pixelfly), kết hợp flat block butterfly và ma trận low-rank để tạo ra một phương pháp huấn luyện thưa thớt đơn giản và hiệu quả.

•Chúng tôi thiết kế một mẫu thưa thớt cực kỳ đơn giản được lấy cảm hứng từ ma trận butterfly + low-rank, tính đến hiệu quả hướng khối của phần cứng. Chúng tôi đề xuất ma trận block butterfly hiệu quả vì các mẫu thưa thớt của chúng căn chỉnh với các khối phần cứng. Sau đó chúng tôi giới thiệu flat butterfly, một xấp xỉ bậc nhất của butterfly với kết nối dư, biến tích ban đầu của các yếu tố thành một tổng. Phép nhân ma trận flat butterfly dễ song song hóa. Pixelfly sử dụng mẫu thưa thớt cố định từ flat & block butterfly, cùng với một hạng thấp, để tạo ra một mạng thưa thớt.

•Chúng tôi chứng minh rằng block butterfly giữ được tính biểu cảm của ma trận butterfly và do đó có thể nắm bắt chặt chẽ các ma trận thưa thớt. Chúng tôi chỉ ra rằng ma trận flat butterfly có thể xấp xỉ chặt chẽ các lớp ma trận lớn mà ma trận butterfly nắm bắt. Hơn nữa, chúng tôi chứng minh rằng ma trận flat block butterfly + low-rank có tính biểu cảm cao hơn nghiêm ngặt so với ma trận thưa thớt hoặc low-rank riêng lẻ. Cuối cùng, tận dụng tiến bộ gần đây trong neural tangent kernel (NTK), chúng tôi điều chỉnh các kỹ thuật hiện có để chứng minh sự hội tụ toàn cục của gradient descent khi huấn luyện các mạng ReLU thưa thớt và rộng.

•Pixelfly đề xuất của chúng tôi có thể được áp dụng cho tất cả các module mạng dựa trên phép nhân ma trận (ví dụ: lớp tuyến tính, attention, MLP). Để làm thưa thớt toàn bộ mạng, chúng ta chỉ cần phân bổ ngân sách tính toán cho mỗi lớp dựa trên kích thước ma trận và khối phần cứng.

Chúng tôi xác thực thực nghiệm rằng Pixelfly có thể tăng tốc huấn luyện các mô hình (Transformers, ViT, MLP-Mixer) mà không giảm chất lượng so với baseline trên nhiều lĩnh vực và tác vụ. Trên phân loại CIFAR10/100 & ImageNet, Pixelfly đạt được tăng tốc thời gian huấn luyện 2.3× so với các mô hình ViT, MLP-Mixer dày đặc và các baseline huấn luyện thưa thớt khác, trong khi duy trì cùng độ chính xác. Trên tác vụ mô hình hóa ngôn ngữ WikiText-103, chúng tôi tăng tốc huấn luyện GPT-2 Medium 2.5× và đạt được cùng perplexity. Trên benchmark Long Range Arena, chúng tôi duy trì cùng độ chính xác như Transformer với huấn luyện nhanh hơn 5.2× so với mô hình dày đặc, nhanh hơn 2× so với Sparse transformer, và nhanh hơn 6× so với các phương pháp thưa thớt không căn chỉnh khối (Reformer). Các nghiên cứu ablation của chúng tôi làm nổi bật tầm quan trọng của từng thành phần: thưa thớt butterfly của chúng tôi cải thiện các mẫu thủ công hiện có lên đến 2% độ chính xác trên ImageNet, block-sparsity nhận biết phần cứng của chúng tôi mang lại tăng tốc lên đến 5×, và phân bổ ngân sách tính toán cân bằng mang lại tăng tốc 2× so với baseline chỉ làm thưa thớt attention.³

2 Thiết lập vấn đề
Đầu tiên chúng tôi định nghĩa vấn đề như xấp xỉ ma trận thưa thớt với một mô hình chi phí phần cứng đơn giản. Sau đó chúng tôi giới thiệu ngắn gọn butterfly và ma trận thưa thớt + low-rank.

Truy cập bộ nhớ
Hình 2: Hình ảnh truy cập bộ nhớ cho phần cứng với kích thước khối 4: truy cập một (đỏ) vị trí có nghĩa là truy cập toàn bộ khối 4×4 (xanh).

Công thức hóa vấn đề: Chúng tôi tập trung vào huấn luyện các mô hình dựa trên GEMM, có thể được xem như một chuỗi phép nhân ma trận (Cho A, B ∈ R^{n×d}, tính C = AB^T). Tăng tốc huấn luyện trong khi duy trì chất lượng mô hình có thể được ánh xạ đến việc tìm một thủ tục xấp xỉ f giảm thời gian T tính toán C trong khi giảm thiểu sai số E[||f(A,B) - AB^T||²_F]. Vì phần cứng là thiết bị khối, việc truy cập bất kỳ phần tử riêng lẻ nào trong một khối bộ nhớ giống như truy cập toàn bộ khối [Cook, 2012] (Hình 2). Một mô hình chi phí đơn giản của T trên phần cứng với kích thước khối b sẽ phụ thuộc vào số lượng b-khối được truy cập và thời gian tính toán (định nghĩa chính thức trong Phụ lục A). Thí nghiệm của chúng tôi (Phụ lục L.5) tiết lộ rằng khi các phần tử khác không được nhóm thành khối, việc chọn kích thước khối nhỏ nhất được hỗ trợ bởi phần cứng có thể tăng tốc các hoạt động lên 10× so với các mẫu thưa thớt không được căn chỉnh khối.

Ma trận Butterfly, Thưa thớt + Low-rank: Ma trận Butterfly đã được sử dụng trong đại số tuyến tính số [Parker, 1995, Li et al., 2015] và học máy [Mathieu và LeCun, 2014, Jing et al., 2017, Munkhoeva et al., 2018, Dao et al., 2019, Choromanski et al., 2019]. Chúng mã hóa cấu trúc chia để trị đệ quy của thuật toán biến đổi Fourier nhanh (FFT) [Cooley và Tukey, 1965] và có thể nắm bắt bất kỳ ma trận thưa thớt nào với độ phức tạp không gian và thời gian gần tối ưu. Cấu trúc Thưa thớt và Low-rank đã được nghiên cứu trong Robust PCA [Candès et al., 2011], phân cụm đồ thị [Jalali et al., 2011], và ước lượng hiệp phương sai [Luo, 2011]. Gần đây nó đã được áp dụng trong xấp xỉ attention cho Transformers [Chen et al., 2021].

3 Ma trận Butterfly và Pixelated Butterfly
Ma trận Butterfly [Parker, 1995, Dao et al., 2019] có tính biểu cảm và hiệu quả về mặt lý thuyết. Vì chúng chứa tập hợp các ma trận thưa thớt, chúng tôi chọn tìm kiếm mẫu thưa thớt trong lớp lớn hơn này do cấu trúc thưa thớt cố định của chúng. Tuy nhiên, có ba thách thức kỹ thuật. Chúng tôi làm nổi bật chúng ở đây cùng với các phương pháp của chúng tôi để giải quyết chúng:

1. Tốc độ chậm: ma trận butterfly không thân thiện với phần cứng hiện đại vì các mẫu thưa thớt của chúng không được căn chỉnh khối, do đó chậm. Chúng tôi giới thiệu một biến thể của ma trận butterfly, block butterfly, hoạt động ở cấp độ khối, tạo ra một mẫu thưa thớt được căn chỉnh khối.

2. Khó khăn trong song song hóa: bản chất tuần tự của ma trận butterfly như tích của nhiều yếu tố khiến việc song song hóa phép nhân trở nên khó khăn. Chúng tôi đề xuất một lớp ma trận khác, ma trận flat butterfly, là xấp xỉ bậc nhất của butterfly với kết nối dư. Flat butterfly biến tích của các yếu tố thành một tổng, tạo điều kiện cho song song hóa.

3. Giảm tính biểu cảm của flat butterfly: mặc dù ma trận flat butterfly có thể xấp xỉ ma trận butterfly với kết nối dư, chúng nhất thiết có hạng cao và không thể biểu diễn ma trận low-rank [Udell và Townsend, 2019]. Chúng tôi đề xuất thêm một ma trận low-rank (cũng được căn chỉnh khối) vào flat butterfly để tăng tính biểu cảm của chúng.

Kết hợp ba phương pháp này (flat & block butterfly + low-rank), đề xuất của chúng tôi (Pixelated Butterfly) là một phương pháp rất đơn giản để huấn luyện các mạng thưa thớt.

3.1 Ma trận Block Butterfly
Chúng tôi đề xuất một phiên bản khối của ma trận butterfly, thân thiện với phần cứng hơn butterfly thông thường. Ma trận butterfly thông thường Dao et al. [2019, 2020] sẽ là trường hợp đặc biệt của block butterfly với kích thước khối b = 1. Chúng tôi bỏ qua b trong ký hiệu nếu b = 1.

Định nghĩa 3.1. Một yếu tố block butterfly (ký hiệu là B_{k,b}) có kích thước kb (trong đó k ≥ 2) và kích thước khối b là một ma trận có dạng B_{k,b} = [D₁ D₂; D₃ D₄] trong đó mỗi Dᵢ là ma trận đường chéo khối k/2 × k/2 với kích thước khối b có dạng diag{Dᵢ,₁, ..., Dᵢ,ₖ/₂} trong đó Dᵢ,ⱼ ∈ R^{b×b}. Chúng tôi hạn chế k là lũy thừa của 2.

Định nghĩa 3.2. Một ma trận yếu tố block butterfly (ký hiệu là B_k^{(n,b)}) có kích thước nb với stride k và kích thước khối b là một ma trận đường chéo khối của n/k yếu tố butterfly (có thể khác nhau) có kích thước kb và kích thước khối b: B_k^{(n,b)} = diag{[Bₖ,ᵦ]₁, [Bₖ,ᵦ]₂, ..., [Bₖ,ᵦ]ₙ/ₖ}

Định nghĩa 3.3. Một ma trận block butterfly có kích thước nb với kích thước khối b (ký hiệu là B^{(n,b)}) là một ma trận có thể được biểu diễn như một tích của các ma trận yếu tố butterfly: B^{(n,b)} = B_n^{(n,b)} B_{n/2}^{(n,b)} ... B_2^{(n,b)}. Định nghĩa Bᵦ là tập hợp tất cả các ma trận có thể được biểu diễn dưới dạng B^{(n,b)} (cho một số n).

3.2 Ma trận flat butterfly
Trong hầu hết các ứng dụng của ma trận butterfly cho mạng nơ-ron, người ta nhân O(log n) yếu tố butterfly. Tuy nhiên, hoạt động này khó được triển khai hiệu quả trên phần cứng song song (ví dụ: GPU) do bản chất tuần tự của hoạt động⁴. Thay vào đó, chúng tôi đề xuất sử dụng một tổng của các yếu tố butterfly có thể xấp xỉ các tích của các yếu tố. Tổng của các yếu tố này tạo ra một ma trận thưa thớt với mẫu thưa thớt cố định, mang lại tăng tốc lên đến 3× nhanh hơn phép nhân trên GPU (Phụ lục J).

Kết nối dư đã được đề xuất để kết nối các yếu tố butterfly [Vahid et al., 2020]. Chúng tôi chỉ ra rằng các tích dư của ma trận butterfly có xấp xỉ bậc nhất như một ma trận thưa thớt với thưa thớt cố định. Gọi M là một ma trận trong tập hợp ma trận butterfly B. Trong dạng dư, với một số λ ∈ R:

M = (I + λB_n^{(n)})(I + λB_{n/2}^{(n)}) ... (I + λB_2^{(n)}) (1)

Lưu ý rằng dạng này có thể biểu diễn cùng các ma trận trong lớp ma trận butterfly B, vì bất kỳ B_k^{(n)} nào cũng chứa ma trận đơn vị I.

Giả sử λ nhỏ, chúng ta có thể mở rộng dư và thu thập các hạng tử⁵:
M = I + λ(B_2^{(n)} + B_4^{(n)} + ... + B_n^{(n)}) + O(λ²)

Định nghĩa 3.4. Ma trận flat butterfly với stride tối đa k (với k là lũy thừa của 2) là những ma trận có dạng I + λ(B_2^{(n)} + B_4^{(n)} + ... + B_k^{(n)}).

Ma trận flat butterfly với stride tối đa n là xấp xỉ bậc nhất của ma trận butterfly trong dạng dư (Eq. (1)). Lưu ý rằng flat butterfly với stride tối đa k là ma trận thưa thớt với O(n log k) phần tử khác không với mẫu thưa thớt cố định, như được minh họa trong Hình 3. Chúng tôi gọi mẫu thưa thớt này là mẫu flat butterfly.

Ma trận flat block butterfly là phiên bản khối của flat butterfly trong Mục 3.2 (được hiển thị trong Hình 3). Chúng tôi xác thực thực nghiệm rằng ma trận flat block butterfly nhanh hơn tới 3× so với block butterfly hoặc butterfly thông thường (Phụ lục J).

Vì ma trận flat butterfly xấp xỉ dạng dư của ma trận butterfly, chúng có hạng cao nếu λ nhỏ (Mục 4). Đây là một trong những động lực cho việc thêm hạng thấp trong phương pháp của chúng tôi.

3.3 Pixelated Butterfly: Flat Block Butterfly + Low-rank cho Huấn luyện thưa thớt hiệu quả
Chúng tôi trình bày Pixelated Butterfly, một mô hình thưa thớt hiệu quả với mẫu thưa thớt đơn giản và cố định dựa trên ma trận butterfly và low-rank. Phương pháp của chúng tôi nhắm đến các mạng nơ-ron dựa trên GEMM, là những mạng có tính toán được chi phối bởi phép nhân ma trận tổng quát (GEMM), như Transformer và MLP-Mixer. Do đó, chúng ta có thể xem mạng như một chuỗi phép nhân ma trận.

Cho một lược đồ mô hình (loại lớp, số lớp, kích thước ma trận) và ngân sách tính toán, Pixelated Butterfly có ba bước: phân bổ ngân sách tính toán cho mỗi lớp, lựa chọn mặt nạ thưa thớt từ mẫu flat butterfly, và làm thưa thớt mô hình. Chúng tôi mô tả các bước này chi tiết hơn:

1. Phân bổ ngân sách tính toán: dựa trên mô hình chi phí của chúng tôi (Phụ lục A), cho loại lớp, số lớp và kích thước ma trận, chúng ta có thể tìm mật độ (phần trăm trọng số khác không) của mỗi loại lớp để giảm thiểu chi phí tính toán dự kiến. Tiếp tục mục tiêu của chúng tôi cho một phương pháp đơn giản, chúng tôi đề xuất sử dụng một quy tắc đơn giản: phân bổ ngân sách tính toán thưa thớt tỷ lệ với phần trăm tính toán của lớp. Ví dụ, nếu lớp MLP và lớp attention được dự kiến chiếm 60% và 40% thời gian tính toán tương ứng, thì phân bổ 60% ngân sách tính toán thưa thớt cho MLP và 40% cho attention. Chúng tôi xác minh trong Phụ lục I rằng quy tắc đơn giản này tạo ra kết quả tương tự như việc giải mật độ từ mô hình chi phí.

2. Lựa chọn mặt nạ thưa thớt: cho một lớp và ngân sách tính toán thưa thớt cho lớp đó, chúng tôi sử dụng một phần tư đến một phần ba ngân sách cho phần low-rank như một quy tắc đơn giản. Chúng tôi chọn hạng là bội số của kích thước khối nhỏ nhất được hỗ trợ của thiết bị (ví dụ: 32) để các ma trận low-rank cũng được căn chỉnh khối. Ngân sách tính toán còn lại được sử dụng để chọn mặt nạ thưa thớt từ mẫu thưa thớt flat block butterfly: chúng tôi chọn kích thước khối butterfly là kích thước khối nhỏ nhất được hỗ trợ của thiết bị (ví dụ: 32), và chọn stride tối đa của flat block butterfly (Định nghĩa 3.4) để lấp đầy ngân sách.

3. Làm thưa thớt mô hình: Mô hình thưa thớt kết quả đơn giản là một mô hình có trọng số hoặc attention tuân theo mặt nạ thưa thớt cố định được chọn trong bước 2, với các hạng thấp bổ sung (hạng cũng được chọn trong bước 2). Cụ thể, chúng tôi tham số hóa mỗi ma trận trọng số⁶ như: W = αB + (1-α)UV^T, trong đó B là ma trận flat block butterfly (thưa thớt), UV^T là thành phần low-rank, và α là tham số có thể học. Chúng tôi huấn luyện mô hình từ đầu như thường lệ.

Phương pháp của chúng tôi rất đơn giản, nhưng cạnh tranh với các thủ tục phức tạp hơn tìm kiếm mẫu thưa thớt (Phụ lục K). Chúng tôi mong đợi các kỹ thuật tinh vi hơn (thưa thớt động, xấp xỉ tốt hơn của butterfly) sẽ cải thiện độ chính xác của phương pháp.

4 Phân tích lý thuyết
Chúng tôi đặc trưng hóa tính biểu cảm của các ma trận được sử dụng trong phương pháp của chúng tôi. Cụ thể, chúng tôi chứng minh rằng block butterfly giữ được tính biểu cảm của butterfly, và flat butterfly có thể xấp xỉ chính xác dạng dư của butterfly. Hơn nữa, flat block butterfly + low-rank (một thể hiện của thưa thớt + low-rank) có tính biểu cảm cao hơn ma trận thưa thớt hoặc low-rank riêng lẻ. Cuối cùng, chúng tôi phân tích sự hội tụ huấn luyện và tổng quát hóa của các mạng với trọng số thưa thớt. Tất cả chứng minh đều có trong Phụ lục.

4.1 Tính biểu cảm của Block Butterfly
Đầu tiên chúng tôi chứng minh tính biểu cảm của ma trận block butterfly.

Định lý 4.1. Tập hợp B_{2b} các ma trận block butterfly n×n với kích thước khối 2b chứa tập hợp B_b các ma trận block butterfly n×n với kích thước khối b.

Bằng lập luận đệ quy, tập hợp ma trận block butterfly có kích thước khối là lũy thừa của 2 chứa tập hợp ma trận butterfly thông thường.

Dao et al. [2020] chỉ ra rằng ma trận butterfly có thể biểu diễn chặt chẽ tất cả các ma trận có cấu trúc, như ma trận thưa thớt và nhiều biến đổi nhanh. Do đó, ma trận block butterfly cũng có thể biểu diễn những ma trận có cấu trúc đó. Cụ thể,

Hệ quả 4.2. Đối với bất kỳ kích thước khối hằng số b nào là lũy thừa của 2, bất kỳ ma trận thưa thớt nb×nb nào với s phần tử khác không có thể được viết như tích của ma trận block butterfly với kích thước khối b và chuyển vị của chúng, với O(s log n) tham số.

4.2 Tính biểu cảm của Flat Butterfly
Bây giờ chúng tôi đặc trưng hóa cách ma trận flat butterfly xấp xỉ ma trận butterfly. Cụ thể, giả sử mỗi yếu tố butterfly có norm bị chặn, chúng tôi chỉ ra rằng ma trận flat-butterfly có thể xấp xỉ chính xác dạng dư của butterfly với sai số tỷ lệ như O(λ²).

Định lý 4.3. Gọi M là ma trận có dạng trong Định nghĩa 3.4 với k = n, với B_{max} := max_i ||B_i^{(n)}||_F và |λ| ≤ c/√(log n) B_{max} cho một số hằng số 0 < c ≤ 1/2 và một số ε > 0. Thì ||M - [I + λ(B_2^{(n)} + B_4^{(n)} + ... + B_n^{(n)})|| ≤ ε.

Chúng tôi chỉ ra rằng ma trận flat butterfly phải có hạng cao nếu λ nhỏ. Đây là động lực cho việc thêm hạng thấp trong Pixelfly (Mục 3).

Định lý 4.4. Gọi M như trong Eq. (1), với B_{max} := max_i ||B_i^{(n)}||_F và |λ| ≤ c/√(log n) B_{max} cho một số hằng số 0 < c ≤ 1/4 và một số ε > 0. Gọi B_{max}^1 = max_i ||B_i||_1. Giả sử B_{max}^1 ≥ B_{max}. Thì rank(I + λ(B_2^{(n)} + ... + B_n^{(n)})) = Ω(B_{max}^2/(B_{max}^1)² · log n / log(B_{max}/B_{max}^1)).

4.3 Tính biểu cảm của Flat Block Butterfly + Low-rank
Chen et al. [2021] chứng minh rằng có một lớp tự nhiên của chuỗi đầu vào (được tạo bởi một quá trình phân cụm) mà ma trận attention của chúng chỉ có thể được xấp xỉ tốt bởi ma trận thưa thớt + low-rank, chứ không phải ma trận thưa thớt hoặc low-rank riêng lẻ. Chúng tôi điều chỉnh kỹ thuật của họ để chỉ ra kết quả tương tự cho lớp ma trận chúng tôi sử dụng trong Pixelfly.

Chúng tôi yêu cầu một giả định bổ sung về quá trình phân cụm so với Chen et al. [2021]: các phần tử trong chuỗi đầu vào tạo thành các cụm có cùng kích thước. Thì ma trận attention của chúng sẽ có một thành phần đường chéo khối lớn được xấp xỉ tốt bởi flat butterfly, trong khi phần còn lại của ma trận attention có kích thước trung bình và được xấp xỉ tốt bởi low-rank.

Định lý 4.5 (Không chính thức). Tồn tại một lớp chuỗi đầu vào mà ma trận attention của chúng được xấp xỉ tốt bởi flat block butterfly + low-rank (trường hợp đặc biệt của thưa thớt + low-rank) nhưng không được xấp xỉ bởi thưa thớt hoặc low-rank riêng lẻ.

Phát biểu định lý chính thức và chứng minh có trong Phụ lục B.3.

4.4 Hội tụ và Tổng quát hóa của Mạng thưa thớt
Có những câu hỏi tự nhiên về huấn luyện và tổng quát hóa của các mô hình thưa thớt: chúng có huấn luyện tương tự như các mô hình dày đặc không, tổng quát hóa của chúng có gần với các mô hình dày đặc không, và có thể huấn luyện thành công chúng bằng gradient descent không? Phân tích của chúng tôi chỉ ra về mặt lý thuyết rằng câu trả lời là có.

Phân tích của chúng tôi dựa trên neural tangent kernel (NTK) [Jacot et al., 2018] của mạng. NTK của hai điểm dữ liệu x và y đo lường sự tương tự giữa gradient của mạng khi được đánh giá tại x so với gradient khi được đánh giá tại y. Kernel này chi phối động lực của hàm đầu ra mạng nơ-ron f(·,θ) trong suốt quá trình huấn luyện và tổng quát hóa của nó. Chúng tôi xây dựng trên tài liệu tuyệt vời của NTK [Li và Liang, 2018, Du et al., 2019, Allen-Zhu et al., 2019b]. Kết quả tiêu chuẩn [Song và Yang, 2019] ngụ ý điều sau đây, nếu NTK của mô hình thưa thớt gần với NTK của mô hình dày đặc, thì (i) tốc độ hội tụ huấn luyện của chúng tương tự, (ii) các ràng buộc tổng quát hóa của chúng tương tự. Để hoàn thiện, chúng tôi phát biểu kết quả chính thức trong Phụ lục F.

Mặc dù kết quả này không nắm bắt hiệu ứng chính quy hóa có thể có của thưa thớt, nó chỉ ra rằng các mô hình thưa thớt với sự khác biệt NTK nhỏ so với NTK dày đặc giữ được khả năng tổng quát hóa của các mô hình dày đặc, một chủ đề đã được nghiên cứu rộng rãi hơn, cả từ góc độ thực nghiệm và lý thuyết. Chúng tôi cũng chỉ ra rằng huấn luyện các mạng thưa thớt và rộng bằng gradient descent hội tụ toàn cục, tương tự như kết quả cho các mạng dày đặc rộng [Du et al., 2019, Allen-Zhu et al., 2019b] trong Phụ lục H.

5 Thí nghiệm
Trong phần này, mục tiêu của chúng tôi là chứng minh rằng một mẫu thưa thớt cố định cực kỳ đơn giản thực sự có thể tăng tốc huấn luyện mô hình thưa thớt về thời gian thực tế mà không làm giảm chất lượng mô hình. Cụ thể, chúng tôi xác thực thực nghiệm ba tuyên bố cho thấy Pixelfly có thể cải thiện tốc độ huấn luyện của các kiến trúc mô hình khác nhau trong khi duy trì chất lượng mô hình trên nhiều lĩnh vực và tác vụ.

1. Mục 5.1: đối với các tác vụ phân loại hình ảnh, đầu tiên chúng tôi chỉ ra NTK thực nghiệm của mẫu thưa thớt flat block butterfly + low-rank gần với NKT dày đặc hơn các baseline khác. Sau đó chúng tôi chứng minh hiệu suất end-to-end vượt trội của chúng tôi. Cụ thể, chúng tôi đạt được tăng tốc huấn luyện trên cả mô hình MLP-Mixer và ViT lên đến 2.3× thời gian thực tế mà không giảm độ chính xác so với mô hình dày đặc và lên đến 4× so với RigL, BigBird và các baseline thưa thớt khác.

2. Mục 5.2: đối với các tác vụ mô hình hóa ngôn ngữ và phân loại văn bản, chúng tôi có thể tăng tốc huấn luyện mô hình dày đặc GPT-2 small 2.1×, đạt được perplexity 22.5 trên wikitext-103. Ngoài ra, trên benchmark Long Range Arena (LRA), chúng tôi duy trì cùng độ chính xác nhưng có tăng tốc 5.2× trong huấn luyện.

3. Mục 5.3: chúng tôi chỉ ra sự cần thiết của cấu trúc block flat butterfly và low-rank, căn chỉnh phần cứng và phạm vi bao phủ rộng của hầu hết các lớp mạng với các nghiên cứu ablation về ba thành phần này của Pixelfly.

5.1 Phân loại hình ảnh
Chúng tôi đánh giá chất lượng và hiệu quả của Pixelfly thông qua ba thước đo: (1) khoảng cách đến động lực huấn luyện của mô hình dày đặc: so sánh khoảng cách giữa kernel NTK thực nghiệm⁷ của các mô hình có mẫu ứng viên, bao gồm BigBird [Zaheer et al., 2020], Butterfly [Dao et al., 2020], và của mô hình dày đặc, (2) độ chính xác upstream: so sánh độ chính xác và thời gian huấn luyện của Pixelfly, đối tác dày đặc và các baseline khác trên cùng tác vụ phân loại hình ảnh, (3) độ chính xác downstream: so sánh độ chính xác của Pixelfly đã được pretrain và mô hình dày đặc được fine-tune trên các tác vụ downstream (Phụ lục L.4). NTK thực nghiệm của mô hình với flat block butterfly + low-rank, được chọn bởi Pixelfly, gần với NTK của mô hình dày đặc hơn. Các mô hình Pixelfly MLP-mixer và ViT cũng giữ được cùng độ chính xác top-1 của các mô hình dày đặc gốc trong khi đạt được tăng tốc lên đến 2.3×.

Thiết lập: Chúng tôi sử dụng ba benchmark thị giác phổ biến, CIFAR-10/100 [Krizhevsky et al., 2009] và ImageNet [Deng et al., 2009]. Chúng tôi chọn Vision Transformer [Dosovitskiy et al., 2020], T2T-ViT [Yuan et al., 2021] và MLP-Mixer [Tolstikhin et al., 2021] gần đây phổ biến làm mô hình cơ sở đại diện. Các nút thắt cổ chai tính toán chính của chúng nằm ở các thành phần khác nhau, ví dụ chỉ MLP, attention, hoặc cả hai để chúng tôi có thể đánh giá khả năng áp dụng end-to-end của Pixelfly rõ ràng hơn.

NTK thực nghiệm: Để đặc trưng hóa động lực huấn luyện của các mạng thưa thớt, chúng tôi tính toán các kernel NTK thực nghiệm cho Vision Transformer dày đặc trên CIFAR-100. Sau đó, chúng tôi chỉ ra sự khác biệt tương đối giữa các kernel của các mô hình có các mẫu thưa thớt khác nhau và của mô hình dày đặc trong Hình 4. Cụ thể, chúng tôi chọn một sự kết hợp mẫu thưa thớt phổ biến – mẫu Bigbird [Zaheer et al., 2020] cho lớp attention và random (thưa thớt dựa trên magnitude tại khởi tạo bằng random) cho lớp MLP, như một baseline đại diện. Đồ thị chỉ ra rằng mẫu được thiết kế của chúng tôi, flat block butterfly + low-rank là gần nhất với mô hình dày đặc trong số tất cả các mẫu. Do đó, chúng tôi mong đợi chúng sẽ tận hưởng nhiều lợi ích nhất từ các đối tác dày đặc được tham số hóa quá mức của chúng trong các tác vụ thực tế. Chi tiết thêm về đo lường NTK thực nghiệm được đề cập trong Phụ lục L.3.

Huấn luyện từ đầu: Chúng tôi xác thực rằng Pixelfly huấn luyện nhanh hơn tới 2.3× và 2.0× so với các mô hình MLP-Mixer và ViT dày đặc từ đầu, với cùng độ chính xác trong cùng thiết lập (batch size, epochs). Cụ thể, chúng tôi làm thưa thớt các mô hình bằng Pixelfly và huấn luyện chúng trên ba tập dữ liệu benchmarking thị giác thường được sử dụng, CIFAR-10/100 và ImageNet. Chúng tôi đo độ chính xác Top-1 và thời gian huấn luyện thực tế. Để tóm tắt xu hướng chung, Hình 5 làm nổi bật rằng các mô hình thị giác thưa thớt của chúng tôi liên tục giữ được độ chính xác của các đối tác dày đặc về độ chính xác và đạt được tăng tốc thời gian huấn luyện.

Hơn nữa, chúng tôi đã thảo luận trong Mục 1 rằng các thuật toán huấn luyện thưa thớt hiện tại nhằm tìm kiếm động thưa thớt có thể tốt cho suy luận hiệu quả nhưng không tăng tốc huấn luyện về thời gian thực tế. Nhưng chúng tôi vẫn trình bày kết quả so sánh trong Hình 6 để hoàn thiện. Để so sánh công bằng, chúng tôi tiến hành thí nghiệm trên mô hình Mixer-S/32 trong 100 epochs vì RigL nhằm thưa thớt trên trọng số, trong khi chúng tôi nhằm cả trọng số và attention. Như mong đợi, RigL không tăng tốc huấn luyện (công trình tiên phong có thưa thớt không có cấu trúc và không đạt được tăng tốc trên GPU) nhưng đáng ngạc nhiên là Pixelfly vượt trội cả mô hình dày đặc và RigL về độ chính xác trong khi đạt được tăng tốc 2.1×.

Cuối cùng, chúng tôi so sánh Pixelfly với BigBird và mẫu Sparse Transformer. Để so sánh công bằng, chúng tôi chọn T2T-ViT làm mô hình cơ sở vì nút thắt cổ chai chính của nó nằm ở module T2T attention (các baseline của chúng tôi là các biến thể attention hiệu quả). Chúng ta có thể thấy từ Hình 7 rằng Pixelfly là thứ duy nhất có thể duy trì độ chính xác và có tăng tốc thực tế. Hơn nữa, Pixelfly tăng tốc module T2T (attention lớn) 1.4× so với dày đặc.

5.2 Mô hình hóa ngôn ngữ và Phân loại văn bản
Trong phần này, chúng tôi nhằm đánh giá hiệu quả của Pixelfly trong lĩnh vực văn bản, trên một tác vụ mô hình hóa ngôn ngữ và các benchmark Long Range Arena (LRA [Tay et al., 2020]). Trên WikiText-103 [Merity et al., 2016], Pixelfly đạt được 22.5 perplexity, gần bằng perplexity của GPT-2 small [Radford et al., 2019] nhưng huấn luyện nhanh hơn 2.1×. Trên LRA, Pixelfly có được gần như cùng độ chính xác với mô hình đầy đủ nhưng đạt được tăng tốc lên đến 5.2×.

Thiết lập: Chúng tôi sử dụng WikiText-103 cho mô hình hóa ngôn ngữ và LRA cho các tác vụ phân loại. Chúng tôi sử dụng GPT-2 small và vanilla Transformer làm mô hình dày đặc cơ sở. Nút thắt cổ chai tính toán của GPT-2 small cho độ dài chuỗi vừa phải, ví dụ 512, sẽ ở cả lớp attention và MLP, trong khi nút thắt cổ chai của transformer trên tác vụ LRA nằm ở attention vì benchmark được thiết kế để đánh giá các mô hình trong các tình huống ngữ cảnh dài.

GPT-2-Small, Medium trên WikiText-103: Chúng tôi chỉ ra việc huấn luyện GPT-2-Small, Medium và mô hình Pixelfly của nó từ đầu trên một tập dữ liệu benchmarking NLP thường được sử dụng, wikiText-103. Chúng tôi đo perplexity của chúng trên tập dữ liệu đó, và tăng tốc huấn luyện của chúng tôi. Tất cả thiết lập và siêu tham số fine-tuning tuân theo những gì trong bài báo gốc [Radford et al., 2019]. Chúng tôi trình bày kết quả trong Hình 8. Không khó để thấy rằng các mô hình Pixelfly có lợi thế lớn trong sự cân bằng độ chính xác-hiệu quả vì nó duy trì cùng perplexity như mô hình dày đặc nhưng đạt được tăng tốc lên đến 2.5× trong huấn luyện.

Vanilla Transformer trên LRA: Chúng tôi so sánh vanilla transformer và các mô hình Pixelfly của nó được huấn luyện từ đầu trên benchmark LRA. Chúng tôi đo độ chính xác, throughput và thời gian huấn luyện của cả hai mô hình. Mỗi tác vụ có độ dài chuỗi khác nhau thay đổi từ 1024 đến 4096. Chúng tôi tuân theo việc triển khai và thiết lập thí nghiệm trong [Xiong et al., 2021]. Chúng tôi so sánh hiệu suất của Pixelfly với dense transformer và báo cáo kết quả trong Hình 9. Chúng tôi cũng bao gồm các số của các baseline khác từ cùng repository trong phụ lục. Chúng ta có thể thấy Pixelfly gần như không gây ra giảm độ chính xác trong khi đạt được tăng tốc 5.2× về thời gian.

5.3 Nghiên cứu Ablation
Chúng tôi tiến hành các nghiên cứu ablation về từng thành phần của Pixelfly (Chi tiết trong Phụ lục L.5). Cụ thể, chúng tôi trình bày (i) flat block butterfly và low-rank ảnh hưởng đến chất lượng mô hình như thế nào, (ii) kích thước khối khác nhau sẽ ảnh hưởng đến tốc độ huấn luyện như thế nào, (iii) phân bổ ngân sách ảnh hưởng đến tăng tốc end-to-end như thế nào.

Sự cần thiết của Flat Block Butterfly và Low-rank: (i) Chúng tôi áp dụng phân bổ tham số khác nhau của thành phần flat block butterfly và Low-rank trong mô hình Pixelfly Mixer-S trên CIFAR-10 dưới mật độ khác nhau thay đổi trong [0.05, 0.1, 0.2]. Chúng tôi phát hiện rằng tương tự như những gì được báo cáo trong [Chen et al., 2021], việc sử dụng khoảng 1/4 ngân sách cho Low-rank và 3/4 cho flat block butterfly đạt được độ chính xác tốt nhất. (ii) Chúng tôi cũng so sánh Pixelfly với các mẫu thưa thớt baseline và chỉ ra nó nhanh hơn 2.7× so với dày đặc, nhanh hơn 3× so với Butterfly, nhanh hơn 3.2× so với BigBird dưới mật độ 10%.

Kích thước khối: Chúng tôi nghiên cứu sự cân bằng độ chính xác-hiệu quả cho flat block butterfly và mẫu thưa thớt random với các kích thước khối khác nhau từ 1-32 (Bảng 7). Chúng tôi phát hiện rằng đầu tiên, dưới cùng mật độ, cùng mẫu thưa thớt được bao phủ với các kích thước khối khác nhau có thể có sự khác biệt lớn về hiệu quả. Dưới cùng khối, mẫu có nhiều tính địa phương hơn có thể hiệu quả hơn. Cuối cùng, mật độ có thể có vẻ rất nhỏ, nhưng thực tế việc truy cập bộ nhớ có thể lên đến 100% của ma trận. Do đó, chúng ta luôn muốn tận dụng đầy đủ kích thước khối nhỏ nhất mà phần cứng (hoặc compiler) hỗ trợ.

Phân bổ ngân sách: Chúng tôi làm thưa thớt các thành phần khác nhau của ViT-small riêng biệt, bao gồm attention và MLP. Chúng tôi chỉ ra rằng tỷ lệ tính toán của chúng xấp xỉ 1:2, vì vậy nếu chỉ làm thưa thớt một trong số chúng, cái khác sẽ là nút thắt cổ chai ngăn chặn tăng tốc end-to-end. Do đó, cần thiết phải có một thuật toán có thể làm thưa thớt tất cả các lớp.

6 Công trình liên quan
Giả thuyết Lottery Ticket. Các mô hình được đề xuất trong công trình của chúng tôi có thể được xem đại khái như một lớp lottery ticket được xây dựng thủ công. Lottery tickets [Frankle và Carbin, 2018] là một tập hợp các mạng con nhỏ được dẫn xuất từ một mạng dày đặc lớn hơn, vượt trội hơn mạng cha mẹ của chúng. Nhiều nghiên cứu sâu sắc [Morcos et al., 2019, Orseau et al., 2020, Frankle et al., 2019, 2020, Malach et al., 2020, Pensia et al., 2020] được thực hiện để phân tích những ticket này, nhưng vẫn khó tổng quát hóa cho các mô hình lớn do chi phí huấn luyện. Trong một nỗ lực, các công trình tiếp theo [Wang et al., 2020, Tanaka et al., 2020] chỉ ra rằng người ta có thể tìm ticket mà không cần nhãn huấn luyện. Chúng tôi lấy cảm hứng từ một trong số chúng, Liu và Zenke [2020], sử dụng NTK để tránh sử dụng nhãn trong việc làm thưa thớt mạng. Các công trình gần đây khác sử dụng phần cứng chuyên biệt để tăng tốc huấn luyện thưa thớt [Goli và Aamodt, 2020, Raihan và Aamodt, 2020].

Neural Pruning. Công trình của chúng tôi có liên quan lỏng lẻo đến neural network pruning. Bằng cách loại bỏ lặp đi lặp lại các nơ-ron và kết nối, pruning đã thấy thành công lớn trong việc nén các mô hình phức tạp. Công trình tiên phong [Han et al., 2015a,b] chỉ ra rằng pruning có thể tạo ra các mô hình nhỏ hơn và nhanh hơn đáng kể cho suy luận. Các phương pháp tiếp theo [Li et al., 2016, Lin et al., 2017, Dong et al., 2017, Sanh et al., 2020, Lagunas et al., 2021, Zhu và Gupta, 2017] cải thiện chất lượng của các mô hình đã được pruned. Trong khi cả phương pháp của chúng tôi và các phương pháp pruning đều nhằm tạo ra các mô hình thưa thớt, chúng tôi nhắm đến hiệu quả huấn luyện, trong khi pruning chủ yếu tập trung vào hiệu quả suy luận với chi phí hy sinh tốc độ huấn luyện.

Các mô hình được tham số hóa quá mức và NTK. Phân tích của chúng tôi về sự hội tụ mô hình thưa thớt dựa rất nhiều vào tiến bộ gần đây trong neural tangent kernel (NTK) [Jacot et al., 2018]. NTK là một công cụ đã được sử dụng rộng rãi trong việc phân tích sự hội tụ của các mô hình được tham số hóa quá mức [Li và Liang, 2018, Du et al., 2019, Allen-Zhu et al., 2019b,c, Song và Yang, 2019], tổng quát hóa [Allen-Zhu et al., 2019a], kết nối với khả năng tách biệt dữ liệu [Oymak và Soltanolkotabi, 2020], và chi phí mỗi lần lặp [Brand et al., 2021]). Deep Double Descent [Nakkiran et al., 2019, d'Ascoli et al., 2020] đưa ra giả thuyết rằng sai số tổng quát hóa cải thiện khi số lượng tham số tăng. Không có gì đáng ngạc nhiên khi cộng đồng đang chạy đua để phá kỷ lục số lượng tham số lớn nhất [Radford et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2020, Tolstikhin et al., 2021, Zhang et al., 2021, Naumov et al., 2019, Jumper et al., 2021].

Chúng tôi cung cấp công trình liên quan mở rộng trong Phụ lục M.

7 Kết luận
Trong khám phá ban đầu của chúng tôi về nhiều mẫu thưa thớt với các thủ tục huấn luyện phức tạp, chúng tôi phát hiện rằng một mẫu đơn giản (butterfly + low-rank) liên tục (mặc dù không phải lúc nào cũng) có hiệu suất tốt nhất. Điều này thúc đẩy chúng tôi đề xuất Pixelated Butterfly, một phương pháp huấn luyện thưa thớt đơn giản và hiệu quả. Trong việc theo đuổi sự đơn giản và hiệu quả, chúng tôi đã chọn sử dụng thưa thớt cố định căn chỉnh với phần cứng hiện đại, đủ để mang lại tăng tốc thời gian huấn luyện thực tế mà không hy sinh độ chính xác. Chúng tôi hào hứng về một số hướng tương lai. Lấy cảm hứng từ thành công đáng kể của model pruning cho suy luận, có thể mặt nạ thưa thớt khối động có thể được làm hiệu quả nhưng vẫn chính xác. Flat butterfly của chúng tôi là một xấp xỉ bậc nhất đơn giản của lớp ma trận butterfly phong phú, và có thể có những xấp xỉ tinh vi hơn giữ được nhiều tính biểu cảm hơn. Phương pháp của chúng tôi là bước đầu tiên hướng tới mục tiêu làm cho các mô hình thưa thớt huấn luyện nhanh hơn các mô hình dày đặc và làm cho chúng dễ tiếp cận hơn với cộng đồng học máy nói chung.

Lời cảm ơn
Chúng tôi cảm ơn Laurel Orr, Xun Huang, Sarah Hooper, Sen Wu, Megan Leszczynski và Karan Goel vì những thảo luận hữu ích và phản hồi về các bản thảo đầu của bài báo.

Chúng tôi biết ơn sự hỗ trợ của NIH dưới số U54EB020405 (Mobilize), NSF dưới số CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), và 1937301 (RTML); ONR dưới số N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Okawa Foundation, American Family Insurance, Google Cloud, Salesforce, Total, chương trình HAI-AWS Cloud Credits for Research, Stanford Data Science Initiative (SDSI), và các thành viên của dự án Stanford DAWN: Facebook, Google và VMWare. Mobilize Center là một Biomedical Technology Resource Center, được tài trợ bởi NIH National Institute of Biomedical Imaging and Bioengineering thông qua Grant P41EB027060. Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối bản in lại cho các mục đích của Chính phủ bất chấp bất kỳ ghi chú bản quyền nào trên đó. Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của các tác giả và không nhất thiết phản ánh quan điểm, chính sách hoặc sự tán thành, được thể hiện rõ ràng hoặc ngụ ý, của NIH, ONR hoặc Chính phủ Hoa Kỳ. Nghiên cứu của Atri Rudra được hỗ trợ bởi NSF grant CCF-1763481.

Tài liệu tham khảo
[Tiếp tục với danh sách tài liệu tham khảo...]
