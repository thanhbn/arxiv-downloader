# Giảm bớt sự bất bình đẳng của các đầu chú ý cho dịch máy thần kinh
Zewei Sun1,, Shujian Huang2,3, Xin-Yu Dai2, Jiajun Chen2
1ByteDance AI Lab
2Phòng thí nghiệm trọng điểm quốc gia về công nghệ phần mềm mới, Đại học Nam Kinh
3Phòng thí nghiệm Bằng Thành, Trung Quốc
sunzewei.v@bytedance.com ,{huangsj,daixinyu,chenjj}@nju.edu.cn

## Tóm tắt
Các nghiên cứu gần đây cho thấy rằng các đầu chú ý trong Transformer không bình đẳng (Voita et al., 2019; Michel et al., 2019). Chúng tôi liên kết hiện tượng này với việc huấn luyện mất cân bằng của cơ chế chú ý đa đầu và sự phụ thuộc của mô hình vào những đầu cụ thể. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp che giấu đơn giản: HeadMask, theo hai cách cụ thể. Các thí nghiệm cho thấy rằng việc cải thiện dịch thuật đạt được trên nhiều cặp ngôn ngữ. Các phân tích thực nghiệm tiếp theo cũng hỗ trợ giả định của chúng tôi và xác nhận tính hiệu quả của phương pháp.

## 1 Giới thiệu
Gần đây, ngày càng có nhiều cấu trúc mạng mới của dịch máy thần kinh (NMT) được đề xuất (Bahdanau et al., 2015; Barone et al., 2017; Gehring et al., 2017; Vaswani et al., 2017), trong đó Transformer (Vaswani et al., 2017) đạt được kết quả tốt nhất. Một sự khác biệt quan trọng giữa Transformer và các mô hình dịch thuật khác là cơ chế chú ý đa đầu của nó.

Một số hiện tượng thú vị của các đầu chú ý được khám phá gần đây. Voita et al. (2019) phát hiện rằng chỉ có một tập con nhỏ các đầu dường như quan trọng đối với nhiệm vụ dịch thuật và phần lớn các đầu có thể bị loại bỏ mà không ảnh hưởng nghiêm trọng đến hiệu suất. Michel et al. (2019) cũng phát hiện rằng một số đầu có thể bị loại bỏ khỏi các mô hình transformer đã huấn luyện mà không có sự suy giảm có ý nghĩa thống kê trong hiệu suất kiểm tra. Hóa ra không phải tất cả các đầu đều quan trọng như nhau.

Chúng tôi suy đoán rằng điều này có thể được quy cho việc huấn luyện mất cân bằng của cơ chế chú ý đa đầu, vì một số đầu không được huấn luyện đầy đủ và đóng góp ít cho mô hình. Tuy nhiên, điều này có thể trở thành nút thắt cổ chai cho toàn bộ mô hình. Để làm ví dụ, nếu một cầu thủ bóng đá quen sử dụng chân phải và dành nhiều cơ hội tập luyện hơn cho nó, nó sẽ ngày càng mạnh hơn. Kết quả là, chân phải được dựa vào nhiều hơn, trong khi chân trái nhận được ít tập luyện hơn và dần dần trở thành hạn chế.

Trong bài báo này, chúng tôi trước tiên xác nhận thực nghiệm sự bất bình đẳng trong cơ chế chú ý đa đầu. Sau đó, một phương pháp huấn luyện mới với hai biến thể được đề xuất để tránh nút thắt cổ chai và cải thiện hiệu suất dịch thuật. Các phân tích sâu hơn cũng được thực hiện để xác minh giả định.

## 2 Sự bất bình đẳng của các đầu

Theo Michel et al. (2019), chúng tôi định nghĩa tầm quan trọng của một đầu chú ý h là:

Ih = Ex∑x ∂L(x)/∂h                                                    (1)

trong đó L(x) là mất mát trên mẫu x và là biến che đầu có giá trị trong {0, 1}. Một cách trực quan, nếu đầu h quan trọng, việc chuyển đổi h sẽ có tác động đáng kể đến mất mát. Áp dụng quy tắc chuỗi cho biểu thức cuối cùng cho Ih:

Ih = Ex∑x Atth(x)T ∂L(x)/∂Atth(x)                                    (2)

Điều này tương đương với phương pháp khai triển Taylor từ Molchanov et al. (2017). Trong Transformer base (Vaswani et al., 2017), có 3 loại chú ý (chú ý tự bộ mã hóa, chú ý tự bộ giải mã, chú ý bộ mã hóa-bộ giải mã) với 6 tầng mỗi loại và 8 đầu mỗi tầng. Do đó, tổng cộng có 144 đầu. Chúng tôi chia chúng thành 8 nhóm với 18 đầu (12,5%) mỗi nhóm theo tầm quan trọng Ih của chúng, trong đó, 1-18 là quan trọng nhất và vv.

Sau đó chúng tôi che các nhóm đầu khác nhau. Như được thể hiện trong Hình 1, việc che một nhóm đầu không quan trọng có ít tác động đến chất lượng dịch thuật trong khi che các đầu quan trọng dẫn đến sự sụt giảm hiệu suất đáng kể. Đáng ngạc nhiên, gần một nửa số đầu không quan trọng, vì việc che chúng có hay không gần như không tạo ra sự khác biệt.

Chúng tôi cũng dần dần che nhiều đầu hơn từng nhóm một theo thứ tự tăng dần và giảm dần, tương ứng. Như được thể hiện trong Hình 2, đường bắt đầu với các đầu không quan trọng giảm chậm hơn nhiều so với đường bắt đầu với các đầu quan trọng. Nó minh họa đầy đủ sự bất bình đẳng của các đầu khác nhau.

Hình 1 và Hình 2 tiếp tục chứng minh sự bất bình đẳng về tầm quan trọng của các đầu chú ý. Một giả định đơn giản để giải thích là một số đầu tình cờ có được nhiều cơ hội cập nhật hơn trong giai đoạn đầu, điều này khiến mô hình học cách phụ thuộc vào chúng dần dần. Kết quả là, mô hình ngày càng tạo ra mối liên kết mạnh mẽ với những đầu cụ thể này trong khi sự phụ thuộc cục bộ này ngăn cản các đầu chú ý còn lại khỏi việc huấn luyện đầy đủ và hạn chế khả năng tổng thể.

## 3 HeadMask

Vì vấn đề liên quan đến việc huấn luyện không công bằng của các đầu chú ý, việc cân bằng một cách rõ ràng các cơ hội huấn luyện là điều tự nhiên đối với chúng tôi. Chúng tôi đề xuất một phương pháp đơn giản: HeadMask, che một số đầu nhất định trong quá trình huấn luyện theo hai cách cụ thể.

### 3.1 Che ngẫu nhiên

Cách đầu tiên là chọn ngẫu nhiên các đầu và che chúng trong mỗi batch. Nó đảm bảo mỗi đầu có được cơ hội huấn luyện tương đối bình đẳng và tránh sự phụ thuộc một phần, như được thể hiện trong Thuật toán 1. Đối với ví dụ về bóng đá, nó giống như huấn luyện các chân ngẫu nhiên, khiến cả hai đều nhận được cùng một lượng luyện tập.

**Thuật toán 1 HeadMask: Che ngẫu nhiên**
```
Input: q; k; v cho chú ý, số lượng che n
Output: ngữ cảnh đã che
1: for batch in datasets do
2:     heads = random.sample(all_heads, n)
3:     for head in heads do
4:         head = 0
5:     end for
6:     context = attn( )
7: end for
```

### 3.2 Che những đầu quan trọng

Cách thứ hai là che những đầu quan trọng nhất. Bằng cách buộc mô hình bỏ qua các đầu quan trọng, chúng tôi hy vọng nhiều cơ hội huấn luyện hơn được gán cho các đầu yếu hơn. Đối với ví dụ về bóng đá, nó có nghĩa là tập luyện chân trái nhiều hơn nếu chân phải chiếm ưu thế. Và một khi đảo ngược, huấn luyện ngược lại. Ý tưởng chính của nó là về việc ngăn chặn việc huấn luyện gây nghiện. Cụ thể, mạng đầu tiên tiến hành tính toán feed-forward và lan truyền ngược mà không cập nhật tham số để tạo ra tầm quan trọng của các đầu. Và sau khi chọn các đầu quan trọng nhất bằng cách sắp xếp, che chúng. Trong quá trình huấn luyện, chúng tôi chỉ sử dụng phần còn lại của mạng để đạt được mất mát cuối cùng và cập nhật tham số, như được thể hiện trong thuật toán 2.

**Thuật toán 2 HeadMask: Che những đầu quan trọng**
```
Input: q; k; v cho chú ý, số lượng che n
Output: ngữ cảnh đã che
1: for batch in datasets do
2:     calculate L by feed-forward
3:     back propagation without updating params
4:     calculate importance of all heads I
5:     heads = argmaxn(I)
6:     for head in heads do
7:         head = 0
8:     end for
9:     context = attn( )
10:    calculate L by feed-forward
11:    back propagation and update params
12: end for
```

## 4 Thí nghiệm

### 4.1 Tập dữ liệu và hệ thống

Chúng tôi tiến hành thí nghiệm trên bốn tập dữ liệu, bao gồm ba tập có tài nguyên thấp (ít hơn 1 triệu). Chúng tôi sử dụng BPE (Sennrich et al., 2016) cho Zh-En (Zheng et al., 2018) và Ro-En, áp dụng các phiên bản đã tiền xử lý từ Luong và Manning (2015) cũng như các thiết lập của Huang et al. (2017) cho Vi-En, và tuân theo các thiết lập joint-BPE của Sennrich et al. (2017) cho Tr-EN. Thông tin chi tiết hơn trong Bảng 1.

**Bảng 1: Thông tin về các tập dữ liệu của chúng tôi**

| Tập dữ liệu | Quy mô | Dev | Test |
|-------------|---------|-----|------|
| NIST Zh-En | 1.34M | MT03 | MT04/05/06 |
| WMT16 Ro-En | 608K | newstest2015 | newstest2016 |
| IWSLT15 Vi-En | 133K | tst2012 | tst2013 |
| WMT17 Tr-En | 207K | newstest2016 | newstest2017 |

Chúng tôi tuân theo thiết lập Transformer base (Vaswani et al., 2017; Sun et al., 2022). Các tham số được tối ưu hóa bằng Adam (Kingma và Ba, 2015), với β₁ = 0.9, β₂ = 0.98, và ε = 10⁻⁹. Tốc độ học được lên lịch theo Vaswani et al. (2017), với warmup_steps = 4000. Label smoothing (Szegedy et al., 2016) có giá trị ε = 0.1 và dropout (Srivastava et al., 2014) có giá trị = 0.1 cũng được áp dụng.

**So sánh** Chúng tôi so sánh baseline với việc che ngẫu nhiên (Random-N) và che những đầu quan trọng (Impt-N), trong đó N là số lượng che. Trong bài báo này, chúng tôi chủ yếu sử dụng N = 18 (12.5%).

### 4.2 Kết quả

Như được thể hiện trong Bảng 2, 3, 4, ngoại trừ các thí nghiệm Vi-En, Impt-18 mang lại cải thiện trên tất cả các hướng ngôn ngữ và đạt kết quả tốt nhất trong thí nghiệm Ro → En. Và Random-18 có được những cải thiện ổn định trên tất cả các cặp và rõ ràng tốt hơn Impt-18. Có vẻ như chiến lược che mặt năng nổ ở các đầu quan trọng có thể quá khắc nghiệt và ngược lại hạn chế mô hình. Và phương pháp ngẫu nhiên giỏi hơn trong việc xây dựng một mô hình huấn luyện hợp lý. Tóm lại, việc giảm sự huấn luyện mất cân bằng giữa các đầu chú ý có thể cải thiện hiệu quả chất lượng dịch thuật.

**Bảng 2: Kết quả thí nghiệm Zh → En**

| Tập test | MT04 | MT05 | MT06 |
|----------|------|------|------|
| Baseline | 46.62 | 43.46 | 43.09 |
| Impt-18 | 46.94 (+0.28) | 44.19 (+0.73) | 43.16 (+0.07) |
| Random-18 | 47.04 (+0.42) | 44.33 (+0.87) | 43.88 (+0.79) |

**Bảng 3: Kết quả thí nghiệm Ro/Vi/Tr → En**

| Hướng | Ro → En | Vi → En | Tr → En |
|-------|---------|---------|---------|
| Baseline | 32.17 | 26.49 | 17.29 |
| Impt-18 | 32.95 (+0.78) | 26.36 (-0.13) | 17.48 (+0.19) |
| Random-18 | 32.85 (+0.68) | 26.85 (+0.36) | 17.56 (+0.27) |

**Bảng 4: Kết quả thí nghiệm En → Ro/Vi/Tr**

| Hướng | En → Ro | En → Vi | En → Tr |
|-------|---------|---------|---------|
| Baseline | 31.98 | 28.07 | 15.74 |
| Impt-18 | 32.47 (+0.49) | 28.06 (-0.01) | 16.10 (+0.36) |
| Random-18 | 32.64 (+0.66) | 28.46 (+0.39) | 16.16 (+0.42) |

### 4.3 Phân tích thống kê

#### 4.3.1 Phân phối phẳng hơn

Để đánh giá việc điều chỉnh huấn luyện của các đầu, chúng tôi kiểm tra phân phối tầm quan trọng của đầu. Như được thể hiện trong Hình 3, các phương pháp của chúng tôi làm cho phân phối tầm quan trọng phẳng hơn. Và phương sai và trung bình tổng thể cũng được tính toán, như được thể hiện trong Bảng 5, 6. So với Baseline, Impt-18 và Random-18 giảm đáng kể phương sai của các đầu chú ý, đạt mục tiêu huấn luyện bình đẳng hơn. Và trung bình cũng giảm, điều này chứng minh sự suy giảm của sự phụ thuộc vào từng đầu riêng lẻ. Cụ thể hơn, Impt-18 có thể giải quyết tốt hơn sự mất cân bằng, vì nó ngăn chặn tốt sự xuất hiện của các đầu "siêu".

#### 4.3.2 Sự phụ thuộc yếu hơn

Chúng tôi lặp lại các thí nghiệm che các nhóm đầu khác nhau. Như được thể hiện trong Hình 4, chất lượng dịch thuật vẫn được duy trì ngay cả khi các đầu quan trọng bị che, chứng minh sự phụ thuộc vào chúng đã giảm. Và Impt-18 hoạt động ổn định hơn vì nó quen với những tình huống như vậy.

#### 4.3.3 Các mô hình mạnh mẽ hơn

Chúng tôi cũng lặp lại các thí nghiệm che tất cả các đầu, như được thể hiện trong Hình 5. Hai đường giữa ban đầu nằm ở cùng vị trí với đường dưới. Khi số lượng đầu bị che trong huấn luyện (N) tăng lên, chúng dần dần di chuyển lên và tiếp cận đường trên nơi các đầu không quan trọng bị che trước. Nó cho thấy các phương pháp của chúng tôi làm cho mô hình dựa vào các đầu quan trọng ít hơn và trở nên mạnh mẽ hơn.

**Bảng 5: Các phương pháp của chúng tôi giảm đáng kể Phương sai của tầm quan trọng đầu, minh họa sự cải thiện bình đẳng của các đầu.**

| Hướng | Zh2En | Ro2En | Vi2En | Tr2En |
|-------|-------|-------|-------|-------|
| Baseline | 77.28 | 552.93 | 100.73 | 1767.70 |
| Random-18 | 33.21 | 255.98 | 48.28 | 900.70 |
| Impt-18 | 9.13 | 72.73 | 14.13 | 188.87 |

**Bảng 6: Các phương pháp của chúng tôi giảm Trung bình của tầm quan trọng đầu, minh họa sự giảm phụ thuộc vào từng đầu.**

| Hướng | Zh2En | Ro2En | Vi2En | Tr2En |
|-------|-------|-------|-------|-------|
| Baseline | 27.15 | 47.18 | 17.96 | 83.79 |
| Random-18 | 19.62 | 39.96 | 14.86 | 74.05 |
| Impt-18 | 18.95 | 37.30 | 18.96 | 85.12 |

## 5 Các công trình liên quan

Gần đây, nhiều công trình phân tích về cơ chế chú ý đa đầu được đưa ra (Raganato và Tiedemann, 2018; Tang et al., 2018; Voita et al., 2019; Michel et al., 2019; Sun et al., 2020; Behnke và Heafield, 2020). Và đối với sự bất bình đẳng của các mạng, một số nghiên cứu tập trung vào mức mô hình (Frankle và Carbin, 2019; Sun et al., 2021), mức tầng (Zhang et al., 2019), và mức neuron (Bau et al., 2019). Đối với thuật toán che, cũng có các công trình ở mức tầng (Fan et al., 2020), mức từ (Provilkov et al., 2019), và mức neuron (Srivastava et al., 2014). Khác với chúng, chúng tôi chủ yếu nghiên cứu mức chú ý và tiến hành phân tích thống kê.

## 6 Kết luận

Trong bài báo này, chúng tôi xác nhận thực nghiệm sự bất bình đẳng của các đầu chú ý trong Transformer và đưa ra giả định về việc huấn luyện mất cân bằng. Tương ứng, chúng tôi đề xuất một phương pháp cụ thể theo hai cách để giải quyết vấn đề. Các thí nghiệm cho thấy những cải thiện trên nhiều cặp ngôn ngữ. Và phân tích chi tiết cho thấy sự giảm bớt vấn đề và tính hiệu quả của các kỹ thuật của chúng tôi.

## 7 Lời cảm ơn

Chúng tôi xin cảm ơn các nhà đánh giá ẩn danh vì những nhận xét sâu sắc của họ. Shujian Huang là tác giả chính. Công trình này được hỗ trợ bởi Quỹ Khoa học Quốc gia Trung Quốc (Số 6217020152).

## Tài liệu tham khảo

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.

Antonio Valerio Miceli Barone, Jindrich Helcl, Rico Sennrich, Barry Haddow, and Alexandra Birch. 2017. Deep architectures for neural machine translation. In WMT.

Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2019. Identifying and controlling important neurons in neural machine translation. In ICLR.

Maximiliana Behnke and Kenneth Heafield. 2020. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP.

Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing transformer depth on demand with structured dropout. In ICLR.

Jonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In ICLR.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional sequence to sequence learning. In ICML.

Po-Sen Huang, Chong Wang, Dengyong Zhou, and Li Deng. 2017. Neural phrase-based machine translation. arXiv, abs/1706.05565.

Diederick P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.

Minh-Thang Luong and Christopher D Manning. 2015. Stanford neural machine translation systems for spoken language domains. In IWSLT.

Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In NeurIPS.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2017. Pruning convolutional neural networks for resource efficient inference. In ICLR.

Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. 2019. Bpe-dropout: Simple and effective subword regularization. arXiv, abs/1910.13267.

Alessandro Raganato and Jörg Tiedemann. 2018. An analysis of encoder representations in transformer-based machine translation. In BlackboxNLP@EMNLP.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich Germann, Barry Haddow, Kenneth Heafield, Antonio Valerio Miceli Barone, and Philip Williams. 2017. The university of edinburgh's neural mt systems for wmt17. In WMT.

Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In ACL.

Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929–1958.

Zewei Sun, Shujian Huang, Hao-Ran Wei, Xin-yu Dai, and Jiajun Chen. 2020. Generating diverse translation by manipulating multi-head attention. In AAAI.

Zewei Sun, Mingxuan Wang, and Lei Li. 2021. Multilingual translation via grafting pre-trained language models. In EMNLP.

Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Lei Li. 2022. Rethinking document-level neural machine translation. In ACL.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In CVPR.

Gongbo Tang, Rico Sennrich, and Joakim Nivre. 2018. An analysis of attention mechanisms: The case of word sense disambiguation in neural machine translation. In WMT.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.

Elena Voita, David Talbot, F. Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL.

Biao Zhang, Ivan Titov, and Rico Sennrich. 2019. Improving deep transformer with depth-scaled initialization and merged attention. In EMNLP-IJCNLP.

Zaixiang Zheng, Shujian Huang, Zewei Sun, Rongxiang Weng, Xinyu Dai, and Jiajun Chen. 2018. Learning to discriminate noises for incorporating external information in neural machine translation. arXiv, abs/1810.10317.# Giảm bớt sự bất bình đẳng của các đầu chú ý cho dịch máy thần kinh
Zewei Sun1,, Shujian Huang2,3, Xin-Yu Dai2, Jiajun Chen2
1ByteDance AI Lab
2Phòng thí nghiệm trọng điểm quốc gia về công nghệ phần mềm mới, Đại học Nam Kinh
3Phòng thí nghiệm Bằng Thành, Trung Quốc
sunzewei.v@bytedance.com ,{huangsj,daixinyu,chenjj}@nju.edu.cn

## Tóm tắt
Các nghiên cứu gần đây cho thấy rằng các đầu chú ý trong Transformer không bình đẳng (Voita et al., 2019; Michel et al., 2019). Chúng tôi liên kết hiện tượng này với việc huấn luyện mất cân bằng của cơ chế chú ý đa đầu và sự phụ thuộc của mô hình vào những đầu cụ thể. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp che giấu đơn giản: HeadMask, theo hai cách cụ thể. Các thí nghiệm cho thấy rằng việc cải thiện dịch thuật đạt được trên nhiều cặp ngôn ngữ. Các phân tích thực nghiệm tiếp theo cũng hỗ trợ giả định của chúng tôi và xác nhận tính hiệu quả của phương pháp.

## 1 Giới thiệu
Gần đây, ngày càng có nhiều cấu trúc mạng mới của dịch máy thần kinh (NMT) được đề xuất (Bahdanau et al., 2015; Barone et al., 2017; Gehring et al., 2017; Vaswani et al., 2017), trong đó Transformer (Vaswani et al., 2017) đạt được kết quả tốt nhất. Một sự khác biệt quan trọng giữa Transformer và các mô hình dịch thuật khác là cơ chế chú ý đa đầu của nó.

Một số hiện tượng thú vị của các đầu chú ý được khám phá gần đây. Voita et al. (2019) phát hiện rằng chỉ có một tập con nhỏ các đầu dường như quan trọng đối với nhiệm vụ dịch thuật và phần lớn các đầu có thể bị loại bỏ mà không ảnh hưởng nghiêm trọng đến hiệu suất. Michel et al. (2019) cũng phát hiện rằng một số đầu có thể bị loại bỏ khỏi các mô hình transformer đã huấn luyện mà không có sự suy giảm có ý nghĩa thống kê trong hiệu suất kiểm tra. Hóa ra không phải tất cả các đầu đều quan trọng như nhau.

Chúng tôi suy đoán rằng điều này có thể được quy cho việc huấn luyện mất cân bằng của cơ chế chú ý đa đầu, vì một số đầu không được huấn luyện đầy đủ và đóng góp ít cho mô hình. Tuy nhiên, điều này có thể trở thành nút thắt cổ chai cho toàn bộ mô hình. Để làm ví dụ, nếu một cầu thủ bóng đá quen sử dụng chân phải và dành nhiều cơ hội tập luyện hơn cho nó, nó sẽ ngày càng mạnh hơn. Kết quả là, chân phải được dựa vào nhiều hơn, trong khi chân trái nhận được ít tập luyện hơn và dần dần trở thành hạn chế.

Trong bài báo này, chúng tôi trước tiên xác nhận thực nghiệm sự bất bình đẳng trong cơ chế chú ý đa đầu. Sau đó, một phương pháp huấn luyện mới với hai biến thể được đề xuất để tránh nút thắt cổ chai và cải thiện hiệu suất dịch thuật. Các phân tích sâu hơn cũng được thực hiện để xác minh giả định.

## 2 Sự bất bình đẳng của các đầu

Theo Michel et al. (2019), chúng tôi định nghĩa tầm quan trọng của một đầu chú ý h là:

Ih = Ex∑x ∂L(x)/∂h                                                    (1)

trong đó L(x) là mất mát trên mẫu x và là biến che đầu có giá trị trong {0, 1}. Một cách trực quan, nếu đầu h quan trọng, việc chuyển đổi h sẽ có tác động đáng kể đến mất mát. Áp dụng quy tắc chuỗi cho biểu thức cuối cùng cho Ih:

Ih = Ex∑x Atth(x)T ∂L(x)/∂Atth(x)                                    (2)

Điều này tương đương với phương pháp khai triển Taylor từ Molchanov et al. (2017). Trong Transformer base (Vaswani et al., 2017), có 3 loại chú ý (chú ý tự bộ mã hóa, chú ý tự bộ giải mã, chú ý bộ mã hóa-bộ giải mã) với 6 tầng mỗi loại và 8 đầu mỗi tầng. Do đó, tổng cộng có 144 đầu. Chúng tôi chia chúng thành 8 nhóm với 18 đầu (12,5%) mỗi nhóm theo tầm quan trọng Ih của chúng, trong đó, 1-18 là quan trọng nhất và vv.

Sau đó chúng tôi che các nhóm đầu khác nhau. Như được thể hiện trong Hình 1, việc che một nhóm đầu không quan trọng có ít tác động đến chất lượng dịch thuật trong khi che các đầu quan trọng dẫn đến sự sụt giảm hiệu suất đáng kể. Đáng ngạc nhiên, gần một nửa số đầu không quan trọng, vì việc che chúng có hay không gần như không tạo ra sự khác biệt.

Chúng tôi cũng dần dần che nhiều đầu hơn từng nhóm một theo thứ tự tăng dần và giảm dần, tương ứng. Như được thể hiện trong Hình 2, đường bắt đầu với các đầu không quan trọng giảm chậm hơn nhiều so với đường bắt đầu với các đầu quan trọng. Nó minh họa đầy đủ sự bất bình đẳng của các đầu khác nhau.

Hình 1 và Hình 2 tiếp tục chứng minh sự bất bình đẳng về tầm quan trọng của các đầu chú ý. Một giả định đơn giản để giải thích là một số đầu tình cờ có được nhiều cơ hội cập nhật hơn trong giai đoạn đầu, điều này khiến mô hình học cách phụ thuộc vào chúng dần dần. Kết quả là, mô hình ngày càng tạo ra mối liên kết mạnh mẽ với những đầu cụ thể này trong khi sự phụ thuộc cục bộ này ngăn cản các đầu chú ý còn lại khỏi việc huấn luyện đầy đủ và hạn chế khả năng tổng thể.

## 3 HeadMask

Vì vấn đề liên quan đến việc huấn luyện không công bằng của các đầu chú ý, việc cân bằng một cách rõ ràng các cơ hội huấn luyện là điều tự nhiên đối với chúng tôi. Chúng tôi đề xuất một phương pháp đơn giản: HeadMask, che một số đầu nhất định trong quá trình huấn luyện theo hai cách cụ thể.

### 3.1 Che ngẫu nhiên

Cách đầu tiên là chọn ngẫu nhiên các đầu và che chúng trong mỗi batch. Nó đảm bảo mỗi đầu có được cơ hội huấn luyện tương đối bình đẳng và tránh sự phụ thuộc một phần, như được thể hiện trong Thuật toán 1. Đối với ví dụ về bóng đá, nó giống như huấn luyện các chân ngẫu nhiên, khiến cả hai đều nhận được cùng một lượng luyện tập.

**Thuật toán 1 HeadMask: Che ngẫu nhiên**
```
Input: q; k; v cho chú ý, số lượng che n
Output: ngữ cảnh đã che
1: for batch in datasets do
2:     heads = random.sample(all_heads, n)
3:     for head in heads do
4:         head = 0
5:     end for
6:     context = attn( )
7: end for
```

### 3.2 Che những đầu quan trọng

Cách thứ hai là che những đầu quan trọng nhất. Bằng cách buộc mô hình bỏ qua các đầu quan trọng, chúng tôi hy vọng nhiều cơ hội huấn luyện hơn được gán cho các đầu yếu hơn. Đối với ví dụ về bóng đá, nó có nghĩa là tập luyện chân trái nhiều hơn nếu chân phải chiếm ưu thế. Và một khi đảo ngược, huấn luyện ngược lại. Ý tưởng chính của nó là về việc ngăn chặn việc huấn luyện gây nghiện. Cụ thể, mạng đầu tiên tiến hành tính toán feed-forward và lan truyền ngược mà không cập nhật tham số để tạo ra tầm quan trọng của các đầu. Và sau khi chọn các đầu quan trọng nhất bằng cách sắp xếp, che chúng. Trong quá trình huấn luyện, chúng tôi chỉ sử dụng phần còn lại của mạng để đạt được mất mát cuối cùng và cập nhật tham số, như được thể hiện trong thuật toán 2.

**Thuật toán 2 HeadMask: Che những đầu quan trọng**
```
Input: q; k; v cho chú ý, số lượng che n
Output: ngữ cảnh đã che
1: for batch in datasets do
2:     calculate L by feed-forward
3:     back propagation without updating params
4:     calculate importance of all heads I
5:     heads = argmaxn(I)
6:     for head in heads do
7:         head = 0
8:     end for
9:     context = attn( )
10:    calculate L by feed-forward
11:    back propagation and update params
12: end for
```

## 4 Thí nghiệm

### 4.1 Tập dữ liệu và hệ thống

Chúng tôi tiến hành thí nghiệm trên bốn tập dữ liệu, bao gồm ba tập có tài nguyên thấp (ít hơn 1 triệu). Chúng tôi sử dụng BPE (Sennrich et al., 2016) cho Zh-En (Zheng et al., 2018) và Ro-En, áp dụng các phiên bản đã tiền xử lý từ Luong và Manning (2015) cũng như các thiết lập của Huang et al. (2017) cho Vi-En, và tuân theo các thiết lập joint-BPE của Sennrich et al. (2017) cho Tr-EN. Thông tin chi tiết hơn trong Bảng 1.

**Bảng 1: Thông tin về các tập dữ liệu của chúng tôi**

| Tập dữ liệu | Quy mô | Dev | Test |
|-------------|---------|-----|------|
| NIST Zh-En | 1.34M | MT03 | MT04/05/06 |
| WMT16 Ro-En | 608K | newstest2015 | newstest2016 |
| IWSLT15 Vi-En | 133K | tst2012 | tst2013 |
| WMT17 Tr-En | 207K | newstest2016 | newstest2017 |

Chúng tôi tuân theo thiết lập Transformer base (Vaswani et al., 2017; Sun et al., 2022). Các tham số được tối ưu hóa bằng Adam (Kingma và Ba, 2015), với β₁ = 0.9, β₂ = 0.98, và ε = 10⁻⁹. Tốc độ học được lên lịch theo Vaswani et al. (2017), với warmup_steps = 4000. Label smoothing (Szegedy et al., 2016) có giá trị ε = 0.1 và dropout (Srivastava et al., 2014) có giá trị = 0.1 cũng được áp dụng.

**So sánh** Chúng tôi so sánh baseline với việc che ngẫu nhiên (Random-N) và che những đầu quan trọng (Impt-N), trong đó N là số lượng che. Trong bài báo này, chúng tôi chủ yếu sử dụng N = 18 (12.5%).

### 4.2 Kết quả

Như được thể hiện trong Bảng 2, 3, 4, ngoại trừ các thí nghiệm Vi-En, Impt-18 mang lại cải thiện trên tất cả các hướng ngôn ngữ và đạt kết quả tốt nhất trong thí nghiệm Ro → En. Và Random-18 có được những cải thiện ổn định trên tất cả các cặp và rõ ràng tốt hơn Impt-18. Có vẻ như chiến lược che mặt năng nổ ở các đầu quan trọng có thể quá khắc nghiệt và ngược lại hạn chế mô hình. Và phương pháp ngẫu nhiên giỏi hơn trong việc xây dựng một mô hình huấn luyện hợp lý. Tóm lại, việc giảm sự huấn luyện mất cân bằng giữa các đầu chú ý có thể cải thiện hiệu quả chất lượng dịch thuật.

**Bảng 2: Kết quả thí nghiệm Zh → En**

| Tập test | MT04 | MT05 | MT06 |
|----------|------|------|------|
| Baseline | 46.62 | 43.46 | 43.09 |
| Impt-18 | 46.94 (+0.28) | 44.19 (+0.73) | 43.16 (+0.07) |
| Random-18 | 47.04 (+0.42) | 44.33 (+0.87) | 43.88 (+0.79) |

**Bảng 3: Kết quả thí nghiệm Ro/Vi/Tr → En**

| Hướng | Ro → En | Vi → En | Tr → En |
|-------|---------|---------|---------|
| Baseline | 32.17 | 26.49 | 17.29 |
| Impt-18 | 32.95 (+0.78) | 26.36 (-0.13) | 17.48 (+0.19) |
| Random-18 | 32.85 (+0.68) | 26.85 (+0.36) | 17.56 (+0.27) |

**Bảng 4: Kết quả thí nghiệm En → Ro/Vi/Tr**

| Hướng | En → Ro | En → Vi | En → Tr |
|-------|---------|---------|---------|
| Baseline | 31.98 | 28.07 | 15.74 |
| Impt-18 | 32.47 (+0.49) | 28.06 (-0.01) | 16.10 (+0.36) |
| Random-18 | 32.64 (+0.66) | 28.46 (+0.39) | 16.16 (+0.42) |

### 4.3 Phân tích thống kê

#### 4.3.1 Phân phối phẳng hơn

Để đánh giá việc điều chỉnh huấn luyện của các đầu, chúng tôi kiểm tra phân phối tầm quan trọng của đầu. Như được thể hiện trong Hình 3, các phương pháp của chúng tôi làm cho phân phối tầm quan trọng phẳng hơn. Và phương sai và trung bình tổng thể cũng được tính toán, như được thể hiện trong Bảng 5, 6. So với Baseline, Impt-18 và Random-18 giảm đáng kể phương sai của các đầu chú ý, đạt mục tiêu huấn luyện bình đẳng hơn. Và trung bình cũng giảm, điều này chứng minh sự suy giảm của sự phụ thuộc vào từng đầu riêng lẻ. Cụ thể hơn, Impt-18 có thể giải quyết tốt hơn sự mất cân bằng, vì nó ngăn chặn tốt sự xuất hiện của các đầu "siêu".

#### 4.3.2 Sự phụ thuộc yếu hơn

Chúng tôi lặp lại các thí nghiệm che các nhóm đầu khác nhau. Như được thể hiện trong Hình 4, chất lượng dịch thuật vẫn được duy trì ngay cả khi các đầu quan trọng bị che, chứng minh sự phụ thuộc vào chúng đã giảm. Và Impt-18 hoạt động ổn định hơn vì nó quen với những tình huống như vậy.

#### 4.3.3 Các mô hình mạnh mẽ hơn

Chúng tôi cũng lặp lại các thí nghiệm che tất cả các đầu, như được thể hiện trong Hình 5. Hai đường giữa ban đầu nằm ở cùng vị trí với đường dưới. Khi số lượng đầu bị che trong huấn luyện (N) tăng lên, chúng dần dần di chuyển lên và tiếp cận đường trên nơi các đầu không quan trọng bị che trước. Nó cho thấy các phương pháp của chúng tôi làm cho mô hình dựa vào các đầu quan trọng ít hơn và trở nên mạnh mẽ hơn.

**Bảng 5: Các phương pháp của chúng tôi giảm đáng kể Phương sai của tầm quan trọng đầu, minh họa sự cải thiện bình đẳng của các đầu.**

| Hướng | Zh2En | Ro2En | Vi2En | Tr2En |
|-------|-------|-------|-------|-------|
| Baseline | 77.28 | 552.93 | 100.73 | 1767.70 |
| Random-18 | 33.21 | 255.98 | 48.28 | 900.70 |
| Impt-18 | 9.13 | 72.73 | 14.13 | 188.87 |

**Bảng 6: Các phương pháp của chúng tôi giảm Trung bình của tầm quan trọng đầu, minh họa sự giảm phụ thuộc vào từng đầu.**

| Hướng | Zh2En | Ro2En | Vi2En | Tr2En |
|-------|-------|-------|-------|-------|
| Baseline | 27.15 | 47.18 | 17.96 | 83.79 |
| Random-18 | 19.62 | 39.96 | 14.86 | 74.05 |
| Impt-18 | 18.95 | 37.30 | 18.96 | 85.12 |

## 5 Các công trình liên quan

Gần đây, nhiều công trình phân tích về cơ chế chú ý đa đầu được đưa ra (Raganato và Tiedemann, 2018; Tang et al., 2018; Voita et al., 2019; Michel et al., 2019; Sun et al., 2020; Behnke và Heafield, 2020). Và đối với sự bất bình đẳng của các mạng, một số nghiên cứu tập trung vào mức mô hình (Frankle và Carbin, 2019; Sun et al., 2021), mức tầng (Zhang et al., 2019), và mức neuron (Bau et al., 2019). Đối với thuật toán che, cũng có các công trình ở mức tầng (Fan et al., 2020), mức từ (Provilkov et al., 2019), và mức neuron (Srivastava et al., 2014). Khác với chúng, chúng tôi chủ yếu nghiên cứu mức chú ý và tiến hành phân tích thống kê.

## 6 Kết luận

Trong bài báo này, chúng tôi xác nhận thực nghiệm sự bất bình đẳng của các đầu chú ý trong Transformer và đưa ra giả định về việc huấn luyện mất cân bằng. Tương ứng, chúng tôi đề xuất một phương pháp cụ thể theo hai cách để giải quyết vấn đề. Các thí nghiệm cho thấy những cải thiện trên nhiều cặp ngôn ngữ. Và phân tích chi tiết cho thấy sự giảm bớt vấn đề và tính hiệu quả của các kỹ thuật của chúng tôi.

## 7 Lời cảm ơn

Chúng tôi xin cảm ơn các nhà đánh giá ẩn danh vì những nhận xét sâu sắc của họ. Shujian Huang là tác giả chính. Công trình này được hỗ trợ bởi Quỹ Khoa học Quốc gia Trung Quốc (Số 6217020152).

## Tài liệu tham khảo

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.

Antonio Valerio Miceli Barone, Jindrich Helcl, Rico Sennrich, Barry Haddow, and Alexandra Birch. 2017. Deep architectures for neural machine translation. In WMT.

Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2019. Identifying and controlling important neurons in neural machine translation. In ICLR.

Maximiliana Behnke and Kenneth Heafield. 2020. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP.

Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing transformer depth on demand with structured dropout. In ICLR.

Jonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In ICLR.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional sequence to sequence learning. In ICML.

Po-Sen Huang, Chong Wang, Dengyong Zhou, and Li Deng. 2017. Neural phrase-based machine translation. arXiv, abs/1706.05565.

Diederick P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.

Minh-Thang Luong and Christopher D Manning. 2015. Stanford neural machine translation systems for spoken language domains. In IWSLT.

Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In NeurIPS.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2017. Pruning convolutional neural networks for resource efficient inference. In ICLR.

Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. 2019. Bpe-dropout: Simple and effective subword regularization. arXiv, abs/1910.13267.

Alessandro Raganato and Jörg Tiedemann. 2018. An analysis of encoder representations in transformer-based machine translation. In BlackboxNLP@EMNLP.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich Germann, Barry Haddow, Kenneth Heafield, Antonio Valerio Miceli Barone, and Philip Williams. 2017. The university of edinburgh's neural mt systems for wmt17. In WMT.

Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In ACL.

Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929–1958.

Zewei Sun, Shujian Huang, Hao-Ran Wei, Xin-yu Dai, and Jiajun Chen. 2020. Generating diverse translation by manipulating multi-head attention. In AAAI.

Zewei Sun, Mingxuan Wang, and Lei Li. 2021. Multilingual translation via grafting pre-trained language models. In EMNLP.

Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Lei Li. 2022. Rethinking document-level neural machine translation. In ACL.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In CVPR.

Gongbo Tang, Rico Sennrich, and Joakim Nivre. 2018. An analysis of attention mechanisms: The case of word sense disambiguation in neural machine translation. In WMT.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.

Elena Voita, David Talbot, F. Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL.

Biao Zhang, Ivan Titov, and Rico Sennrich. 2019. Improving deep transformer with depth-scaled initialization and merged attention. In EMNLP-IJCNLP.

Zaixiang Zheng, Shujian Huang, Zewei Sun, Rongxiang Weng, Xinyu Dai, and Jiajun Chen. 2018. Learning to discriminate noises for incorporating external information in neural machine translation. arXiv, abs/1810.10317.