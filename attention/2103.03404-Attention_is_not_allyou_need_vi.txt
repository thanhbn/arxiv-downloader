# 2103.03404.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2103.03404.pdf
# Kích thước tệp: 1591173 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Attention không phải là tất cả những gì bạn cần:
attention thuần túy mất hạng theo cấp số nhân kép với độ sâu
Yihe Dong
Google
yihed@google.comJean-Baptiste Cordonnier
EPFL
jean-baptiste.cordonnier@epfl.chAndreas Loukas
EPFL
andreas.loukas@epfl.ch

Tóm tắt
Các kiến trúc dựa trên attention đã trở nên phổ biến trong machine learning. Tuy nhiên, sự hiểu biết của chúng ta về lý do hiệu quả của chúng vẫn còn hạn chế. Công trình này đề xuất một cách mới để hiểu các mạng self-attention: chúng tôi chỉ ra rằng đầu ra của chúng có thể được phân rã thành tổng của các thành phần nhỏ hơn, mỗi thành phần liên quan đến hoạt động của một chuỗi các attention head qua các layer. Sử dụng sự phân rã này, chúng tôi chứng minh rằng self-attention có một inductive bias mạnh hướng tới "token uniformity". Cụ thể, không có skip connection hoặc multi-layer perceptron (MLP), đầu ra hội tụ theo cấp số nhân kép về ma trận hạng 1. Mặt khác, skip connection và MLP ngăn chặn đầu ra khỏi sự thoái hóa. Các thí nghiệm của chúng tôi xác minh các hiện tượng hội tụ đã xác định trên các biến thể khác nhau của kiến trúc transformer tiêu chuẩn.

1 Giới thiệu
Cơ chế attention [BCB15] ban đầu được phát triển để học tốt hơn kiến thức tuần tự tầm xa, và được sử dụng hiệu quả trong các mạng transformer [VSP+17]. Kể từ đó, các kiến trúc dựa trên attention đã thấm nhuần qua các ứng dụng machine learning trong nhiều lĩnh vực dữ liệu, chẳng hạn như trong xử lý ngôn ngữ tự nhiên [DCLT18, POT16], nhận dạng giọng nói [LZL+20], và thị giác máy tính [RPV+19, BZV+19]. Do đó, việc phát triển các công cụ để hiểu hoạt động bên trong của transformer và attention nói chung là rất quan trọng, vừa để làm sáng tỏ các mô hình hiện có, vừa để thiết kế các mô hình tương lai hiệu quả hơn.

Công trình này cung cấp những hiểu biết mới về hoạt động và inductive bias của các mạng được xây dựng bằng cách xếp chồng nhiều layer self-attention. Đáng ngạc nhiên, chúng tôi phát hiện rằng các mạng self-attention thuần túy (SAN), tức là các transformer với skip connection và multi-layer perceptron (MLP) bị vô hiệu hóa, mất sức biểu đạt theo cấp số nhân kép so với độ sâu mạng. Cụ thể hơn, chúng tôi chứng minh rằng đầu ra hội tụ với tốc độ bậc ba về ma trận hạng một có các hàng giống hệt nhau. Trong khi chúng tôi rút ra các ràng buộc hội tụ một phần bằng cách sử dụng các tính chất của ma trận ngẫu nhiên, kết quả của chúng tôi vượt ra ngoài những gì người ta mong đợi dựa trên lý thuyết tiêu chuẩn. Đặc biệt, bằng cách tận dụng các hiệu ứng lan truyền của việc xếp chồng cụ thể các module self-attention, chúng tôi cho thấy sự hội tụ nhanh hơn theo cấp số nhân so với những gì lý thuyết tiêu chuẩn quy định. Hơn nữa, trong khi các nghiên cứu trước đây đã xem xét hạng của các ma trận self-attention riêng lẻ [WLK+20, KVPF20, CLJ20a], kết quả của chúng tôi là đầu tiên giải quyết các điều kiện mà toàn bộ mạng hội tụ về hạng một.

Điều này đặt ra câu hỏi, tại sao transformer lại hoạt động? Phân tích của chúng tôi chỉ ra rằng skip connection đóng vai trò quan trọng trong việc giảm thiểu sự sụp đổ hạng, và MLP có thể làm chậm sự hội tụ bằng cách tăng hằng số Lipschitz của chúng. Chúng tôi đặc trưng hóa những lực đối kháng này bằng cách chứng minh các ràng buộc trên và dưới của hành vi hội tụ này trong các biến thể kiến trúc SAN giống với transformer. Kết quả của chúng tôi tiết lộ một tiện ích quan trọng chưa được biết đến trước đây của skip connection, ngoài việc tạo điều kiện tối ưu hóa và gradient flow [HZRS16a, BFL+18].

1Mã của chúng tôi có sẵn công khai tại https://github.com/twistedcubic/attention-rank-collapse

--- TRANG 2 ---
concat và project
Skip connection
Layer 1
concat và project
Skip connection
Layer 2
concat và project
Skip connection
Layer L

Hình 1: Hai đường dẫn trong một Self-Attention Network sâu (SAN) với H head và L layer. Tại mỗi layer, một đường dẫn có thể đi qua một trong các head hoặc bỏ qua layer. Thêm một khối MLP sau mỗi attention layer tạo thành kiến trúc transformer.

Trong quá trình này, chúng tôi phát triển một phân rã đường dẫn mới để nghiên cứu các mạng self-attention. Cụ thể, chúng tôi phân rã một SAN thành tổ hợp tuyến tính của các đường dẫn phụ thuộc yếu, trong đó mỗi 'đường dẫn' tương ứng với một SAN đơn head sâu. Một cách trực quan, người ta có thể xem các self-attention head trong mỗi layer của mạng gốc như các cổng khác nhau, và một đường dẫn theo một chuỗi lựa chọn cổng, một cổng cho mỗi layer (Hình 1). Kết hợp với phân tích sụp đổ hạng, kết quả của chúng tôi cho thấy rằng các SAN sâu với skip connection hoạt động như một ensemble của các mạng nông phụ thuộc yếu.

Những đóng góp chính của chúng tôi như sau: (1) Chúng tôi trình bày một nghiên cứu có hệ thống về các khối xây dựng của transformer, tiết lộ tác động đối lập giữa self-attention và các lực đối kháng: skip connection và MLP, trong việc góp phần và ngăn chặn sự sụp đổ hạng trong transformer. Như một hệ quả, điều này tiết lộ một hiệu ứng quan trọng chưa được biết đến trước đây của skip connection ngoài việc tạo điều kiện tối ưu hóa. (2) Chúng tôi đề xuất một phương pháp mới để phân tích SAN thông qua phân rã đường dẫn, tiết lộ SAN như một ensemble của các mạng nông. (3) Chúng tôi xác minh lý thuyết của mình bằng các thí nghiệm trên các kiến trúc transformer phổ biến.

Ký hiệu. Trong phần tiếp theo, các chữ cái in đậm viết thường/viết hoa biểu thị vector và ma trận tương ứng. Chúng tôi ký hiệu chuẩn tổng hợp ℓ1, ℓ∞ của ma trận X là ∥X∥1,∞=√∥X∥1∥X∥∞. Chúng tôi lưu ý rằng ℓ1,∞ không phải là một chuẩn thực sự vì nó không thỏa mãn bất đẳng thức tam giác, mặc dù nó hoàn toàn đồng nhất và xác định dương. Chúng tôi cũng sử dụng ký hiệu viết tắt [H] = (1, ···, H).

2 Attention mất hạng theo cấp số nhân kép

Chúng tôi bắt đầu bằng việc nghiên cứu các mạng self-attention (SAN) được xây dựng độc quyền từ các layer multi-head self-attention. Chúng tôi chứng minh rằng SAN hội tụ theo cấp số nhân (với độ sâu) về ma trận hạng 1 làm cho tất cả token giống hệt nhau. Phân tích của chúng tôi trong §2.1 dựa trên một cách không thông thường để biểu diễn đầu ra của SAN multi-head như tổng của các mạng đơn head. Chúng tôi gọi những mạng sau là đường dẫn, trong đó mỗi đường dẫn được ký hiệu bằng một chuỗi các attention head (xem Hình 1). Một phác thảo chứng minh về lý do sụp đổ hạng xảy ra được đưa ra trong §2.2, trong khi kết quả sụp đổ hạng chính được trình bày trong §2.3.

--- TRANG 3 ---
2.1 Luận cứ phân rã đường dẫn

Cho X là ma trận đầu vào n×din gồm n token. Một SAN được xây dựng từ L layer multi-head self-attention, mỗi layer có H head. Đầu ra của self-attention head thứ h có thể được viết là

SAh(X) = PhXWV,h + 1b⊤V,h.

Ở trên, WV,h là ma trận trọng số value din×dv và ma trận ngẫu nhiên hàng n×n Ph được cho bởi

Ph = softmax(d^(-1/2)_qk(XWQ,h + 1b⊤Q,h)(XWK,h + 1b⊤K,h)⊤)
= softmax(d^(-1/2)_qk(XWQK,hX⊤ + 1b⊤Q,hW⊤K,hX⊤)),

trong đó (1) các ma trận trọng số key và query WK,h và WQ,h có kích thước din×dqk, (2) WQK,h = WQ,hW⊤K,h, và (3) softmax hoạt động độc lập trên mỗi hàng của đầu vào. Chúng tôi thu được phương trình cuối cùng bằng cách lưu ý rằng softmax là bất biến dịch chuyển và bỏ qua các thành phần cung cấp đóng góp không đổi qua các hàng [CLJ20a].

Đầu ra của mỗi layer SAN được tạo thành bằng cách nối các đầu ra riêng lẻ của tất cả H attention head (dọc theo chiều cuối cùng) và chiếu tuyến tính chúng lên một không gian con có kích thước phù hợp:

SA(X) = 1[b⊤O,1,···,b⊤O,H] + [SA1(X),···,SAH(X)][W⊤O,1,···,W⊤O,H]⊤
= ∑h∈[H] PhXWh + 1b⊤O,

trong đó chúng tôi đặt Wh = WV,hW⊤O,h và bO = ∑h bO,h.

Cho Xl là đầu ra của layer thứ l và cố định X0 = X. Như thông lệ, chúng tôi để tất cả các layer gồm cùng số lượng head.

Loại trừ bias 1b⊤O,h, đầu ra SAN được cho bởi

XL = ∑h∈[H] PL_hXL-1WL_h
= ∑h∈[H] PL_h ∑h'∈[H] PL-1_h'XL-2WL-1_h' WL_h = ∑hL,hL-1∈[H]² PL_hLPL-1_hL-1XL-2WL-1_hL-1WL_hL,

điều này, sau khi mở rộng đệ quy ngược, cho ta:

XL = ∑h1,...,hL∈[H]^L (PL_hL···P1_h1)X(W1_h1···WL_hL).

Các phương trình trên có diễn giải rõ ràng nếu chúng ta nghĩ về SAN như một đồ thị acyclic có hướng, với các nút tương ứng với các self-attention head và cạnh có hướng nối các head của các layer liên tiếp. Chúng tôi chính thức hóa trực giác này trong phần sau:

Định lý 2.1 (Phân rã đường dẫn của SAN). Đầu ra của mạng self-attention độ sâu L với H head trên mỗi layer (bao gồm bias và skip connection) được cho bởi

SAN(X) = ∑path∈[H]^L PpathXWpath + 1b⊤, (1)

trong đó Ppath = PL_hL···P1_h1 là ma trận ngẫu nhiên phụ thuộc đầu vào, trong khi Wpath = W1_h1···WL_hL và b không phụ thuộc vào đầu vào.

--- TRANG 4 ---
Chứng minh. Chứng minh xuất phát từ việc tập hợp các ma trận ngẫu nhiên hàng đóng kín dưới phép nhân (tức là, PL_hL···Pi_hi là ngẫu nhiên hàng) và hơn nữa, đối với bất kỳ ma trận ngẫu nhiên hàng P nào, chúng ta có P1 = 1.

Mỗi thành phần trong (1) mô tả một đường dẫn có độ dài L qua các head của các layer khác nhau
path = (h1, . . . , hL), trong đó hl ∈ (0, 1, . . . , H)
và có tổng cộng H^L đường dẫn như vậy không có skip connection.

Do đó, phân rã đường dẫn mô tả hoạt động của SAN multi-head như sự kết hợp của các mạng đơn head đơn giản hơn. Để có được trực giác về sự phụ thuộc lẫn nhau của đường dẫn, việc tách các hoạt động được thực hiện thành hai loại sẽ hữu ích: những hoạt động tác động qua các token (nhân từ bên trái) và những hoạt động áp dụng độc lập trên mỗi token (nhân từ bên phải). Như đã thấy, mặc dù các đường dẫn có thể tương tác thông qua việc trộn token (vì các ma trận Ppath cùng phụ thuộc vào X), các hoạt động theo token là độc lập. Chúng ta cũng có thể nhận thấy rằng bias không đặc biệt có ý nghĩa: tổng đóng góp của chúng chỉ là thành phần đơn 1b⊤ không phụ thuộc vào số lượng layer hoặc head được sử dụng.

Trong phần sau, chúng tôi chỉ ra rằng mỗi đường dẫn hội tụ nhanh chóng (như một hàm của độ dài) về ma trận hạng 1 với các hàng giống hệt nhau. Thú vị là, sự hội tụ này mạnh mẽ đến mức việc thêm nhiều layer vào SAN không giúp ích: mặc dù số lượng đường dẫn tăng theo cấp số nhân, mỗi đường dẫn thoái hóa theo cấp số nhân kép, dẫn đến đầu ra hạng 1.

2.2 Hội tụ của SAN đơn head

Trước khi giải quyết SAN đầy đủ, việc xem xét hành vi của mỗi đường dẫn riêng biệt là hữu ích. Đặc biệt, chúng tôi kiểm tra cách dư lượng
res(X) = X - 1x⊤, trong đó x = argmin_x ∥X - 1x⊤∥
thay đổi trong quá trình forward pass.

Như kết quả sau đây cho thấy, chuẩn dư lượng hội tụ về không một cách đáng ngạc nhiên nhanh chóng (theo cấp số nhân kép với tốc độ bậc ba):

Định lý 2.2 (Đơn giản hóa). Đối với bất kỳ SAN đơn head nào gồm L layer với ∥Wl_QK∥1∥Wl_V∥1,∞ ≤ β và đối với thành phần γ phụ thuộc vào các entry attention, chúng ta có

∥res(SAN(X))∥1,∞ ≤ 4γβ/√dqk^(3^L-1)/2 ∥res(X)∥^3^L_1,∞, (2)

điều này tương đương với sự hội tụ theo cấp số nhân kép về ma trận hạng 1.

Đối với định lý đầy đủ, chúng tôi tham khảo độc giả đến Phụ lục.

Lưu ý rằng ràng buộc trong Phương trình 2 đảm bảo sự hội tụ ∥res(SAN(X))∥1,∞ cho tất cả đầu vào có dư lượng nhỏ bất cứ khi nào 4γβ < √dqk. Trong thực tế, các thí nghiệm của chúng tôi ngụ ý rằng vùng hội tụ có thể lớn hơn nhiều.

Tốc độ hội tụ bậc ba đã xác định nhanh hơn đáng kể so với những gì mong đợi khi phân tích tích của các ma trận ngẫu nhiên (tốc độ tuyến tính). Như quy tắc chung, để đạt được sự giảm ba bậc độ lớn, chẳng hạn từ 1000 xuống 1, người ta có thể mong đợi tốc độ hội tụ tuyến tính yêu cầu khoảng một chục lần lặp, trong khi tốc độ bậc ba có thể làm vậy chỉ trong hai hoặc ba lần lặp. Lý do chúng ta có tốc độ bậc ba là vì hạng của ma trận attention cũng phụ thuộc vào hạng của đầu vào. Như chúng tôi chỉ ra, các self-attention head trộn token nhanh hơn khi được tạo thành từ ma trận hạng thấp. Hiện tượng này trở nên mạnh hơn khi chúng ta xây dựng SAN sâu hơn, dẫn đến hiệu ứng lan truyền.

Chúng tôi cung cấp phác thảo chứng minh bên dưới. Chứng minh chi tiết có thể tìm thấy trong Phụ lục.

Phác thảo chứng minh. Để phân tích cách hình thành Ph bị ảnh hưởng bởi hạng của đầu vào, chúng ta bắt đầu bằng cách viết X = 1x⊤ + R cho R = res(X) và mở rộng ma trận attention tương ứng:

XW_QKX⊤ = (1x⊤ + R)W_QK(1x⊤ + R)⊤

Áp dụng một lần nữa tính chất bất biến dịch chuyển của softmax, điều trên có thể được đơn giản hóa thành

Ph = softmax((RW_QK/√dqk)R⊤ + 1r⊤),

cho một r phù hợp nào đó. Quan sát rằng nếu ma trận bên trong softmax là 1r⊤, thì Ph cũng sẽ thoái hóa thành ma trận hạng 1: softmax(1r⊤) = 1q⊤ và sự hội tụ sẽ xảy ra ngay lập tức.

Chứng minh xây dựng dựa trên quan sát này bằng cách chỉ ra rằng nếu E = RW_QK/√dqk R⊤ nhỏ thì Ph gần như hạng 1:
∥Ph - 1q⊤∥ ≤ 2∥D1q⊤∥,

trong đó D là đường chéo và Dii = max_j |δ⊤_iE(δj - δj')|. Do đó, chúng ta có
PhX = Ph(1x⊤ + R) = 1x⊤ + softmax(1r⊤ + E)R

và hơn nữa, ∥res(PhX)∥ ≤ 2∥D1q⊤R∥. Chứng minh kết thúc bằng cách ràng buộc thành phần trên và áp dụng luận cứ đệ quy qua các layer liên tiếp.

2.3 Hội tụ theo cấp số nhân cho mạng attention

Bây giờ chúng tôi chuyển sang phân tích sự hội tụ của SAN với nhiều head trên mỗi layer.

Kết quả chính của chúng tôi như sau:

Định lý 2.3 (Đơn giản hóa). Xem xét mạng self-attention độ sâu L và độ rộng H không có skip connection. Giả sử rằng ∥Wl_QK,h∥1∥Wl_h∥1,∞ ≤ β cho tất cả head h ∈ [H] và layer l ∈ [L], và cho γ là thành phần phụ thuộc vào các entry attention. Chúng ta có

∥res(SAN(X))∥1,∞ ≤ (4γβH/√dqk)^(3^L-1)/2 ∥res(X)∥^3^L_1,∞,

điều này tương đương với tốc độ hội tụ theo cấp số nhân kép.

Ràng buộc đảm bảo sự hội tụ của SAN(X) về hạng một khi 4γβH < √dqk. Các thí nghiệm của chúng tôi cho thấy đây là một ước tính khá bi quan, vì, trong thực tế, chúng tôi quan sát sự hội tụ rộng rãi của đầu ra về hạng 1.

Nhận xét 1. Ý nghĩa đối với Xformer. Đã có sự gia tăng mạnh mẽ của các biến thể kiến trúc - mà chúng tôi gọi chung là Xformer - nhằm cải thiện transformer vanilla [VSP+17] bằng cách giảm độ phức tạp self-attention bậc hai. Kết quả sụp đổ hạng của Định lý 2.3 mang ý nghĩa thú vị cho những kiến trúc này. Một biến thể như vậy dựa trên các xấp xỉ hạng thấp hoặc dựa trên kernel đối với ma trận attention đầy đủ [KVPF20, WLK+20, CLD+20], trong trường hợp này các đường dẫn có thể hội tụ thậm chí nhanh hơn về hạng một do tính hạng thấp được áp đặt. Một biến thể khác chỉ tính toán một tập con của các entry ma trận attention sử dụng các mẫu cụ thể [ZGD+20, CGRS19], chẳng hạn như các mẫu ngẫu nhiên, trong trường hợp này người ta mong đợi các đường dẫn hội tụ chậm hơn, vì sự ngẫu nhiên hóa có xu hướng tăng hạng của đầu ra.

--- TRANG 5 ---
3 Các cơ chế chống lại sụp đổ hạng

Những phát hiện của chúng tôi đặt ra một câu hỏi thích hợp - tại sao các mạng dựa trên attention hoạt động trong thực tế nếu attention thoái hóa thành ma trận hạng 1 theo cấp số nhân kép với độ sâu? Nhằm có được hiểu biết sâu sắc hơn, chúng tôi tập trung vào kiến trúc transformer [VSP+17] và mở rộng phân tích của mình bằng cách kết hợp ba thành phần quan trọng của transformer mà SAN thiếu: skip connection, multi-layer perceptron, và layer normalization.

Chúng tôi áp dụng một phương pháp có hệ thống trong đó các sửa đổi đối với kiến trúc SAN được giới thiệu từng cái một. Đối với mỗi trường hợp, chúng tôi rút ra lại các ràng buộc hội tụ và thảo luận về hiệu ứng quan sát được.

3.1 Skip connection là quan trọng

Một sửa đổi đơn giản cho luận cứ phân rã đường dẫn cho SAN đủ để tính đến skip connection. Cụ thể, chúng tôi chỉ ra sự kiện mà một đường dẫn đã bỏ qua một layer bằng cách đặt h = 0 trên ký hiệu tương ứng:

XL = ∑h∈[H]∪{0} PL_hXL-1WL_h
= ...
= ∑h1,...,hL∈([H]∪{0})^L (PL_hL···P1_h1)X(W1_h1···WL_hL),

trong đó chúng tôi đã cố định P0 = I và W0 = I.

Như quan sát, skip connection làm đa dạng hóa đáng kể phân phối đường dẫn. Ký hiệu bằng Pl tập hợp các đường dẫn có độ dài l. Với skip connection được bật, chúng ta có

|Pl| = (L choose l)H^l

đường dẫn có độ dài l (trong khi trước đây chúng ta chỉ có đường dẫn độ dài L). Chúng tôi giả thuyết rằng chính sự hiện diện của các đường dẫn ngắn ngăn SAN khỏi thoái hóa về hạng 1.

Trong khi chúng ta có thể rút ra một ràng buộc trên cho dư lượng tương tự như trên (mà chúng tôi làm trong Phụ lục để hoàn thiện) ràng buộc trên như vậy lại rỗng tuếch. Thật vậy, việc có một ràng buộc dưới cho dư lượng sẽ mang tính thông tin hơn, để phù hợp với thực tế, nơi SAN với skip connection không gặp phải sụp đổ hạng. Chúng tôi trình bày ràng buộc dưới đơn giản sau:

Tuyên bố 3.1. Xem xét mạng self-attention độ sâu L và độ rộng H với skip connection. Tồn tại vô số tham số hóa mà ∥res(XL)∥ ≥ ∥res(X)∥. Điều trên vẫn đúng ngay cả khi L → ∞ và β tùy ý nhỏ.

Chứng minh là cơ bản: bằng phân rã đường dẫn, luôn có một đường dẫn bỏ qua tất cả các layer, tức là đường dẫn có độ dài 0, bảo toàn dư lượng. Sau đó, đối với bất kỳ tham số hóa nào làm cho đóng góp của các layer SAN trực giao với đầu vào, chúng ta sẽ có ∥res(XL)∥ ≥ ∥res(X)∥. Một ví dụ đơn giản của tham số hóa như vậy có thể được khôi phục bằng cách đặt Wl_V = 0 cho mọi l ∈ [L], trong trường hợp này ∥res(XL)∥ = ∥res(X)∥.

Một ràng buộc dưới chặt chẽ cho dư lượng trong sự hiện diện của skip connection là rất khó khăn, và chúng tôi đặt nó như một thách thức mở cho cộng đồng.

Nhận xét 2. SAN như ensemble của mạng nông. Có thể suy ra từ Định lý 2.3 rằng SAN với skip connection được bật dựa rất nhiều vào các đường dẫn ngắn (vì dư lượng giảm nhanh chóng khi độ dài đường dẫn trở nên lớn hơn). Nói cách khác, SAN hoạt động như ensemble của các mạng self-attention đơn head nông. Hiện tượng này trước đây đã được xác định cho ResNet [VWB16a] (mặc dù nghiên cứu sau không nghiên cứu hiện tượng sụp đổ hạng). Ở đây, các thành phần của ensemble này phụ thuộc lẫn nhau, vì mỗi attention head tham gia vào nhiều đường dẫn có độ dài khác nhau. Kết quả thí nghiệm trong §4 hỗ trợ ý nghĩa này. Tài liệu bổ sung cũng cung cấp một nghiên cứu về phân phối đường dẫn qua một số kiến trúc phổ biến.

3.2 Multi-layer perceptron (MLP) giúp ích

Bây giờ chúng tôi nghiên cứu cách sử dụng MLP ảnh hưởng đến dư lượng. Đặc biệt, chúng tôi tập trung vào SAN với các layer được viết là
Xl+1 = fl(∑h∈[H] PhXlWh).

Lưu ý rằng, để giữ ký hiệu gọn gàng, chúng tôi sử dụng fl để biểu thị cả MLP cũng như bias đầu ra.

Trong phân tích tiếp theo, chúng tôi sử dụng λl,1,∞ để biểu thị hằng số Lipschitz của fl đối với chuẩn ℓ1,∞. Lưu ý rằng, mặc dù việc tìm hằng số chính xác có thể là NP-hard ngay cả đối với MLP nông [SV18], vì fl bao gồm các phép biến đổi tuyến tính với các phi tuyến tính Lipschitz, fl thường là Lipschitz.

Hệ quả 3.2 (Đơn giản hóa). Xem xét SAN độ sâu L và độ rộng H với MLP. Giả sử rằng ∥Wl_QK,h∥1∥Wl_h∥1,∞ ≤ β cho tất cả h ∈ [H] và l ∈ [L], cho γ là thành phần phụ thuộc vào các entry attention, và cố định λl,1,∞ ≤ λ. Chúng ta có

∥res(XL)∥1,∞ ≤ (4γβHλ/√dqk)^(3^L-1)/2 ∥res(X)∥^3^L_1,∞, (3)

điều này tương đương với tốc độ hội tụ theo cấp số nhân kép.

Như đã thấy, mặc dù hiệu ứng của MLP ít drastic hơn so với skip connection, tốc độ hội tụ trong Hệ quả 3.2 có thể được kiểm soát bởi các hằng số Lipschitz λf,1,∞ của MLP: MLP càng mạnh thì sự hội tụ càng chậm. Điều này tiết lộ một cuộc kéo co giữa các layer self-attention và MLP, mà do tính phi tuyến của chúng có thể tăng hạng. §4 cho thấy rằng thực sự MLP chống lại sự hội tụ trong các thí nghiệm.

Chúng tôi nên nhấn mạnh rằng việc sử dụng MLP để chống lại sự sụp đổ hạng không phải là không có nhược điểm: Trong khi việc tăng các hằng số Lipschitz làm chậm sự hội tụ dư lượng, nó cũng làm cho mô hình ít mạnh mẽ hơn và nhạy cảm hơn với các nhiễu loạn đầu vào [CKSN18]. Các hằng số Lipschitz lớn hơn cũng có thể đặt ra những thách thức lớn hơn cho việc tối ưu hóa, vì chúng dẫn đến phương sai gradient lớn hơn.

3.3 Layer normalization không đóng vai trò gì

Layer normalization được thực hiện bằng cách rescale và dịch chuyển đầu vào qua chiều feature:

LN(SA(X)) = LN(∑h∈[H] PhXWh + 1b⊤O)
= (∑h∈[H] PhXWh + 1b⊤O - 1b⊤LN)D^(-1)_LN,

trong đó bLN là trung bình của mỗi cột SA(X) và DLN là ma trận đường chéo với các entry tương ứng với độ lệch chuẩn (có thể được scaled hoặc shifted) của mỗi cột SA(X).

--- TRANG 6 ---
Bằng cách đặt W̃h = WhD^(-1)_LN và b̃O = bO - bLN, điều trên được viết lại là

LN(SA(X)) = ∑h∈[H] PhXW̃h + 1b̃⊤O,

điều này giống hệt với phương trình trước khi layer normalization được áp dụng, mặc dù bây giờ W̃h và b̃O phụ thuộc vào đầu vào. Vì phép nhân bên phải không thể tăng hạng của ma trận, chúng tôi kết luận rằng layer normalization không giảm thiểu sự sụp đổ hạng.

4 Thí nghiệm

Các thí nghiệm của chúng tôi đầu tiên kiểm tra hiện tượng sụp đổ hạng trong một số kiến trúc transformer nổi tiếng (§4.1). Chúng tôi cũng minh họa trực quan inductive bias của một số biến thể kiến trúc của transformer với một ví dụ đồ chơi trong §4.2 và kiểm tra hiệu quả của đường dẫn đối với độ dài trong §4.3. Kết quả bổ sung có thể tìm thấy trong Phụ lục.

4.1 Sụp đổ hạng trong các kiến trúc thực tế

Để xác minh dự đoán lý thuyết của chúng tôi, chúng tôi kiểm tra dư lượng của ba kiến trúc transformer nổi tiếng: BERT [DCLT18], Albert [LCG+19], và XLNet [YDY+19]. Hình 2 vẽ đồ thị dư lượng tương đối ∥res(SAN(Xl)∥1,∞/∥SAN(Xl)∥1,∞ của đầu ra mỗi layer trước và sau khi các mạng được huấn luyện.

--- TRANG 7 ---
[Biểu đồ và hình ảnh cho thấy kết quả thí nghiệm]

Để tính toán các tỷ lệ này, chúng tôi chạy mạng trên 32 mẫu của 128 token trích dẫn tiểu sử từ Wikipedia [LGA16] và hiển thị trung bình và độ lệch chuẩn.

Các thí nghiệm xác nhận rằng, ngay khi skip connection bị loại bỏ, tất cả các mạng đều thể hiện sự sụp đổ hạng nhanh chóng. Mặc dù MLP dường như không giúp ích trong việc giảm thiểu sự hội tụ, chúng tôi cảnh báo rằng quan sát này không phải là một miêu tả chính xác về cách các transformer được huấn luyện hoạt động: việc loại bỏ skip connection gây ra sự dịch chuyển phân phối drastic trong đầu vào MLP. Chúng tôi mong đợi rằng sự hội tụ sẽ chậm lại nếu mạng được huấn luyện lại.

4.2 Trực quan hóa bias của các kiến trúc khác nhau

Để điều tra thực nghiệm inductive bias của các thành phần khác nhau của kiến trúc transformer, chúng tôi nghiên cứu hành vi của transformer một layer khi được áp dụng đệ quy (tương tự như universal transformer [DGV+19]) để dự đoán một chuỗi tròn 2D đơn giản.

Cụ thể, chúng tôi huấn luyện transformer một layer để dự đoán tuần tự hai cung tròn trong R2 có bán kính 0.3, bắt đầu tại (-0.3, 0) và (0.3, 0) tương ứng, mỗi cung hướng ngược chiều kim đồng hồ và gồm 1000 điểm (được minh họa như các quỹ đạo màu xám). Một mẫu đầu vào gồm một chuỗi hai điểm đối diện trên đường tròn, một từ cung trên và một từ cung dưới. Chúng tôi áp dụng teacher-forcing ở mỗi bước, có nghĩa là chúng tôi đưa cho mạng tọa độ ground truth của hai điểm hiện tại, và huấn luyện nó để dự đoán hai điểm tiếp theo. Mô hình cố gắng tối thiểu hóa loss MSE giữa các điểm dự đoán và các điểm ground truth trên quỹ đạo. Tại thời điểm suy luận, chúng tôi không áp dụng teacher-forcing, và đơn giản là đưa đầu ra mô hình làm đầu vào cho bước tiếp theo.

--- TRANG 8 ---
Vì việc áp dụng đệ quy này của transformer một layer có thể được tham số hóa lại để tương đương với transformer nhiều layer không có skip connection, chúng tôi giả thuyết rằng tại thời điểm suy luận, các quỹ đạo dự đoán của hai cung sẽ hội tụ về cùng một điểm (chỉ ra sự sụp đổ hạng), thay vì theo các quỹ đạo huấn luyện. Lưu ý rằng setting cũng đã được xây dựng có chủ ý để cho phép huấn luyện ngay cả không có skip connection (bằng cách sử dụng teacher forcing) và do đó tách biệt hai lợi ích riêng biệt của skip connection: khả năng cải thiện tối ưu hóa và việc giảm thiểu sự sụp đổ hạng.

Chúng tôi huấn luyện mạng cho đến khi nó có thể ghi nhớ hoàn hảo bước tiếp theo trên các quỹ đạo tròn với loss gần không. Hình 3 minh họa các quỹ đạo được dự đoán tại thời điểm suy luận (tức là, không có teacher forcing). Như đã thấy ở hàng trên, không có MLP hoặc skip connection, mạng thể hiện sự sụp đổ hạng. Định lý 2.2 dự đoán rằng sự hội tụ chậm hơn khi β ≥ ∥Wl_QK∥1∥Wl_V∥1,∞ tăng. Thật vậy, khi hidden dimension tăng từ 32 lên 128 (dẫn đến β lớn hơn khi khởi tạo), sự hội tụ chậm lại, trở nên khó quan sát đối với dimension 128.

Chúng tôi kết luận rằng, phù hợp với phân tích của chúng tôi, việc thêm MLP hoặc skip connection hoặc dừng hoặc làm chậm đáng kể sự sụp đổ hạng. Như quan sát, skip connection có xu hướng làm chậm các điểm di chuyển. Hiện tượng sau là do trong setting này skip connection tạo ra bias hướng tới việc ở lại cùng vị trí. Mặt khác, việc thêm MLP không thể hiện bias tương tự.

4.3 Hiệu quả đường dẫn

SAN có thể được xem như ensemble của các đường dẫn có độ dài khác nhau (từ 0 đến L), mỗi đường dẫn liên quan đến một chuỗi khác nhau của các self-attention head. Phân tích của chúng tôi về SAN với skip connection chỉ ra rằng khả năng biểu đạt của đường dẫn giảm theo độ dài đường dẫn, ngay cả khi số lượng các hoạt động phi tuyến liên quan tăng. Để kiểm tra giả thuyết này, chúng tôi tách các đường dẫn có độ dài khác nhau và đánh giá sức mạnh dự đoán của chúng.

Các nhiệm vụ. Chúng tôi xem xét ba nhiệm vụ sau để kiểm tra hiệu quả đường dẫn đối với độ dài:

• Ghi nhớ chuỗi. Để giải quyết nhiệm vụ này, một mô hình cần ghi nhớ một ánh xạ được xác định trước từ các câu ngôn ngữ tự nhiên và các chuỗi nhãn ngẫu nhiên có cùng độ dài. Chúng tôi sử dụng các token ngẫu nhiên (thay vì nhãn thực tế) để làm cho đây thuần túy là một bài kiểm tra khả năng biểu đạt của mạng bằng cách ghi nhớ dữ liệu huấn luyện, thay vì các hiệu ứng confounding như khả năng tổng quát hóa. Các mô hình được kiểm tra được huấn luyện để tối thiểu hóa loss cross entropy giữa các nhãn dự đoán và ground truth. Dữ liệu huấn luyện gồm 500 câu tiếng Anh từ Wikipedia và nguồn News [DGM06, WSM+19], được tokenize bằng tokenizer SentencePiece [KR18] thành từ vựng có kích thước 30522 với 128 token mỗi chuỗi. Mỗi chuỗi được ánh xạ đến một chuỗi nhị phân ngẫu nhiên có cùng độ dài.

• Học sắp xếp. Cho một chuỗi đầu vào của các chữ cái, nhiệm vụ này học sắp xếp các chữ cái theo thứ tự bảng chữ cái (các nhiệm vụ tương tự đã được nghiên cứu trước đây [FOˇS19]). Cụ thể, đầu ra của mô hình cho mỗi chữ cái đầu vào được sử dụng để xác định vị trí của chữ cái đó trong thứ tự dự đoán. Mỗi chuỗi đầu vào, có độ dài 8, được tạo bằng cách lấy mẫu đồng đều ngẫu nhiên, có thay thế, từ bảng chữ cái có kích thước 10. Tập huấn luyện và kiểm tra gồm 1000 và 200 chuỗi tương ứng. Để đảm bảo tính mạnh mẽ đối với siêu tham số, chúng tôi thí nghiệm với nhiều setting khác nhau (điều chỉnh độ sâu mô hình, số lượng head, và độ khó của nhiệm vụ bằng cách thay đổi kích thước bảng chữ cái và độ dài chuỗi) và quan sát hành vi nhất quán.

• Dự đoán convex hull. Nhiệm vụ này được lấy cảm hứng từ công trình của [VFJ15]. Cho một chuỗi N điểm phân phối đồng đều trong [0,1]×[0,1] và dịch chuyển bởi một chuẩn bình thường hai biến ngẫu nhiên, nhiệm vụ này dự đoán convex hull của những điểm này. Cụ thể, cho mỗi điểm trong tập hợp, mô hình dự đoán liệu nó có phải là một phần của convex hull hay không. Tập huấn luyện gồm 10,000 chuỗi điểm trong [0,1]×[0,1], mỗi chuỗi có độ dài 10.

Trong cả ba nhiệm vụ, chúng tôi báo cáo độ chính xác dự đoán nhãn mỗi token trên tập kiểm tra làm metric đánh giá.

--- TRANG 9 ---
Kiểm tra hiệu quả đường dẫn. Chúng tôi đo hiệu quả của các đường dẫn riêng lẻ bằng một thủ tục 'tách biệt đường dẫn' mà chúng tôi áp dụng tại thời điểm suy luận: thủ tục này tách biệt các trọng số liên quan và đầu ra của một đường dẫn riêng lẻ (P^L_{hL}···P^1_{h1})X(W^1_{h1}···W^L_{hL}) cho bất kỳ chuỗi head h1, ···, hL ∈ [H∪0]^L nào được cho.

Sau khi transformer đã được huấn luyện thành công để giải quyết mỗi nhiệm vụ (không có sửa đổi), chúng tôi sử dụng thủ tục này để xác định đầu ra của một tập hợp được lấy mẫu ngẫu nhiên của các đường dẫn có độ dài cho trước. Sau đó chúng tôi đánh giá hiệu suất nhiệm vụ dựa trên tổng chuẩn hóa của tập con đường dẫn này (thay vì từ tất cả các đường dẫn). Lưu ý rằng việc huấn luyện vẫn không thay đổi và sử dụng tất cả các head đồng thời, do đó đảm bảo rằng mỗi đường dẫn học đến hiệu quả đầy đủ của nó.

Hình 4 minh họa hiệu suất kết quả qua cả ba nhiệm vụ. Chúng tôi kiểm tra các kích thước tập con khác nhau và báo cáo trung bình và độ lệch chuẩn của năm lần lặp lại. Để tham khảo, chúng tôi cũng vẽ độ chính xác của một bộ phân loại ngây thơ cũng như của toàn bộ mô hình được huấn luyện (tức là, trước khi phân rã đường dẫn). Như quan sát, các đường dẫn ngắn mang sức mạnh dự đoán, với các đường dẫn độ dài 1 đạt độ chính xác trên 0.8, 0.6, và 0.65 trong các nhiệm vụ ghi nhớ, sắp xếp, và convex hull tương ứng. Mặt khác, đầu ra của các đường dẫn dài không tốt hơn nhiều so với dự đoán ngẫu nhiên (các đường ngang màu đỏ). Chúng tôi lưu ý rằng, vì có sự mất cân bằng lớp trong nhiệm vụ convex hull, chúng tôi sử dụng bộ dự đoán lớp đa số để thu được baseline ngẫu nhiên. Mặc dù sự khác biệt về độ chính xác giữa các đường dẫn ngắn và dài ít rõ ràng hơn đối với nhiệm vụ convex hull, chúng tôi quan sát rằng phương sai của các đường dẫn dài lớn hơn đáng kể, khiến chúng không tốt hơn nhiều so với dự đoán ngẫu nhiên. Các đường dẫn độ dài zero đạt phương sai rất nhỏ, nhưng không chứa thông tin hữu ích về nhiệm vụ (có thể vì chúng không khai thác thông tin toàn cục).

Độ sâu (L), số lượng head (H), và hidden dimension (d) cho ba mô hình là: L:6, H:2, d:250 cho ghi nhớ, L:6, H:2, d:48 cho sắp xếp, và L:6, H:3, d:84 cho convex hull. Quan trọng là lưu ý rằng đối với cả ba nhiệm vụ, trong khi độ chính xác đỉnh cao hơn có thể đạt được với khả năng mô hình tăng và thời gian huấn luyện, trọng tâm của chúng tôi là nghiên cứu hiệu ứng của độ dài đường dẫn đối với hiệu suất. Thật vậy, xu hướng hiệu suất thoái hóa khi độ dài đường dẫn tăng vẫn nhất quán qua các kích thước mô hình trong tất cả các thí nghiệm.

Hiệu quả giảm nhanh chóng của các đường dẫn đối với độ dài chỉ ra rằng transformer dựa gần như hoàn toàn vào các đường dẫn ngắn. Nói cách khác, transformer hoạt động như một ensemble của các mạng nông. Hơn nữa, kết quả chỉ ra rằng có khả năng chưa được sử dụng trong các đường dẫn dài, và gợi ý rằng một cách để làm cho chúng, và do đó transformer, hiệu quả hơn, là ngăn chặn các đường dẫn dài khỏi mất hạng.

--- TRANG 10 ---
5 Các công trình liên quan

Skip connection đầu tiên được giới thiệu trong ResNet [HZRS16a], kể từ đó, nó đã được sử dụng để tạo điều kiện tối ưu hóa trong các mạng sâu [HZRS16b, VWB16b, BFL+18]. Đặc biệt, skip connection giải quyết vấn đề vanishing gradient, bằng cách cho phép gradient flow bỏ qua các layer được skip trong quá trình backpropagation. Động cơ ban đầu của việc sử dụng skip connection trong transformer theo cùng lý luận về việc tạo điều kiện tối ưu hóa [VSP+17]. Với việc phân rã đường dẫn cho transformer, chúng tôi khám phá một tầm quan trọng bổ sung đáng ngạc nhiên của skip connection: chúng ngăn chặn đầu ra transformer khỏi thoái hóa về hạng một một cách nhanh chóng theo cấp số nhân đối với độ sâu mạng.

Veit et al. ([VWB16b]) đã giới thiệu một diễn giải tương tự cho residual network như một tập hợp các đường dẫn có độ dài khác nhau, và phát hiện rằng độ dài của các đường dẫn hiệu quả trong residual network sâu ngắn hơn nhiều so với tổng độ sâu mạng, do gradient được sử dụng cho cập nhật tham số đến chủ yếu từ những đường dẫn ngắn này. Phát hiện của chúng tôi cho thấy rằng SAN dựa vào các đường dẫn ngắn để tránh sụp đổ hạng. Mặt khác, Daneshmand et al. [DKB+20] đã nghiên cứu sự sụp đổ hạng trong các mạng tuyến tính và ReLU được khởi tạo ngẫu nhiên và chỉ ra rằng batch normalization là một chiến lược giảm thiểu hiệu quả.

Một số công trình gần đây đã xấp xỉ ma trận attention với các phân tích hạng thấp [WLK+20, TBM+20] hoặc phương pháp kernel [KVPF20, CLD+20], để giảm độ phức tạp self-attention bậc hai. Công trình của chúng tôi trực giao với những công trình này, bằng cách nghiên cứu hạng của đầu ra mạng (thay vì của ma trận attention).

Đã có những tiến bộ gần đây khác trong việc hiểu lý thuyết đằng sau transformer: [PMB19, DGV+19] đã chứng minh tính phổ quát Turing, [CLJ20b] cung cấp các điều kiện cần và đủ để attention mô phỏng convolution. Một dạng tuyến tính hóa của self-attention cũng được phát hiện thể hiện sự chuyển pha độ sâu [LWS+20]; và hằng số Lipschitz của self-attention được phân tích bởi [KPM20].

Có lẽ sự hội tụ về hạng một của một đường dẫn không nên gây ngạc nhiên: mỗi thành phần đường dẫn chứa các ma trận ngẫu nhiên hàng như kết quả của softmax attention, và [AT77] đã chỉ ra sự hội tụ theo cấp số nhân của tích các ma trận ngẫu nhiên về hạng một. Trong khi trực giác đằng sau các ma trận ngẫu nhiên thúc đẩy sự hội tụ vẫn áp dụng, trong các mạng attention sâu những ma trận này tương tác theo những cách phức tạp hơn so với những gì các phân tích cổ điển xem xét. Như chúng tôi chỉ ra, vì những tương tác này, hạng sụp đổ nhanh hơn nhiều so với những gì mong đợi dựa trên các phân tích cổ điển (tốc độ bậc ba so với tốc độ tuyến tính).

6 Kết luận

Công trình này phơi bày các lực cạnh tranh về sự sụp đổ hạng trong các mạng self-attention, cụ thể là self-attention so với skip connection và MLP. Trong quá trình này, chúng tôi phát triển một phân rã đường dẫn cho SAN, điều này modularize việc nghiên cứu self-attention và có ý nghĩa độc lập đối với các ứng dụng bổ sung. Những kết quả này mở ra cửa cho nhiều hướng tương lai thú vị. Ví dụ, làm thế nào người ta có thể tận dụng inductive bias đồng đều token được tiết lộ để thiết kế các mạng hiệu quả hơn, có lẽ tốt hơn trong việc sử dụng các đường dẫn dài? Những ý nghĩa thực tế cho sự đánh đổi độ rộng-độ sâu là gì? Làm thế nào chúng ta chứng minh các ràng buộc dưới có ý nghĩa về sự hội tụ dư lượng cho transformer? Việc trả lời những câu hỏi này có ý nghĩa rộng lớn trong việc thúc đẩy hiện trạng trong deep learning.

Lời cảm ơn. Andreas Loukas xin cảm ơn Quỹ Khoa học Quốc gia Thụy Sĩ đã hỗ trợ anh trong bối cảnh dự án "Deep Learning for Graph-Structured Data" (số hiệu grant PZ00P2 179981). Jean-Baptiste Cordonnier được hỗ trợ bởi Trung tâm Khoa học Dữ liệu Thụy Sĩ (SDSC).

--- TRANG 11 ---
[Phần tài liệu tham khảo và phụ lục tiếp tục với các công thức toán học và chứng minh chi tiết...]

[Tiếp tục dịch phần còn lại của tài liệu bao gồm các tài liệu tham khảo và phụ lục với các chứng minh toán học chi tiết]
