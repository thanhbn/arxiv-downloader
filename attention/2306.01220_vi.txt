# 2306.01220.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/attention/2306.01220.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 978616 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ°
Láº­p trÃ¬nh viÃªn Con ngÆ°á»i khi Táº¡o mÃ£ khÃ´ng?
BONAN KOU, Äáº¡i há»c Purdue, Má»¹
SHENGMAI CHENâˆ—,Äáº¡i há»c Brown, Má»¹
ZHIJIE WANG, Äáº¡i há»c Alberta, Canada
LEI MA, Äáº¡i há»c Tokyo, Nháº­t Báº£n vÃ  Äáº¡i há»c Alberta, Canada
TIANYI ZHANG, Äáº¡i há»c Purdue, Má»¹
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) gáº§n Ä‘Ã¢y Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho viá»‡c táº¡o mÃ£. Do tÃ­nh phá»©c táº¡p
vÃ  má» Ä‘á»¥c cá»§a LLM, Ã­t Ä‘Æ°á»£c biáº¿t vá» cÃ¡ch cÃ¡c mÃ´ hÃ¬nh nÃ y táº¡o ra mÃ£. ChÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n ná»— lá»±c Ä‘áº§u tiÃªn Ä‘á»ƒ
kháº¯c phá»¥c khoáº£ng cÃ¡ch kiáº¿n thá»©c nÃ y báº±ng cÃ¡ch Ä‘iá»u tra xem liá»‡u LLM cÃ³ chÃº Ã½ Ä‘áº¿n cÃ¹ng nhá»¯ng pháº§n cá»§a mÃ´ táº£ nhiá»‡m vá»¥
nhÆ° láº­p trÃ¬nh viÃªn con ngÆ°á»i trong quÃ¡ trÃ¬nh táº¡o mÃ£. Má»™t phÃ¢n tÃ­ch vá» sÃ¡u LLM, bao gá»“m GPT-4, trÃªn hai Ä‘iá»ƒm chuáº©n
táº¡o mÃ£ phá»• biáº¿n Ä‘Ã£ tiáº¿t lá»™ sá»± khÃ´ng phÃ¹ há»£p nháº¥t quÃ¡n giá»¯a sá»± chÃº Ã½ cá»§a LLM vÃ  láº­p trÃ¬nh viÃªn.
ChÃºng tÃ´i Ä‘Ã£ phÃ¢n tÃ­ch thá»§ cÃ´ng 211 Ä‘oáº¡n mÃ£ khÃ´ng chÃ­nh xÃ¡c vÃ  tÃ¬m tháº¥y nÄƒm máº«u chÃº Ã½ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ
giáº£i thÃ­ch nhiá»u lá»—i táº¡o mÃ£. Cuá»‘i cÃ¹ng, má»™t nghiÃªn cá»©u ngÆ°á»i dÃ¹ng cho tháº¥y sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi má»™t
phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n thÆ°á»ng Ä‘Æ°á»£c Æ°a chuá»™ng bá»Ÿi láº­p trÃ¬nh viÃªn con ngÆ°á»i. Nhá»¯ng phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i lÃ m ná»•i báº­t nhu cáº§u vá»
LLM phÃ¹ há»£p vá»›i con ngÆ°á»i Ä‘á»ƒ cÃ³ kháº£ nÄƒng diá»…n giáº£i tá»‘t hÆ¡n vÃ  sá»± tin tÆ°á»Ÿng cá»§a láº­p trÃ¬nh viÃªn.
KhÃ¡i niá»‡m CCS: â€¢Pháº§n má»m vÃ  ká»¹ thuáº­t cá»§a nÃ³ ;â€¢PhÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n â†’Xá»­ lÃ½ ngÃ´n ngá»¯
tá»± nhiÃªn ;
Tá»« khÃ³a vÃ  Cá»¥m tá»« Bá»• sung: Táº¡o MÃ£, MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n, ChÃº Ã½
Äá»‹nh dáº¡ng Tham kháº£o ACM:
Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2024. CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½
TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng?. Proc. ACM Softw. Eng. 1, FSE, BÃ i viáº¿t 100
(ThÃ¡ng 7 nÄƒm 2024), 24 trang. https://doi.org/10.1145/3660807
1 GIá»šI THIá»†U
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) Ä‘Ã£ cÃ³ tiáº¿n bá»™ Ä‘Ã¡ng ká»ƒ trong viá»‡c táº¡o mÃ£ trong nhá»¯ng
nÄƒm gáº§n Ä‘Ã¢y [ 4,5,26,27,29,32,36,43,87]. Má»™t nghiÃªn cá»©u gáº§n Ä‘Ã¢y [ 14] cho tháº¥y GPT-4, LLM tá»‘i tÃ¢n
nháº¥t vá»›i 1.7 nghÃ¬n tá»· tham sá»‘, cÃ³ thá»ƒ giáº£i quyáº¿t chÃ­nh xÃ¡c 84% cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh Python
tá»« Ä‘iá»ƒm chuáº©n HuamnEval [ 27]. Báº¥t cháº¥p tiáº¿n bá»™ lá»›n nÃ y, váº«n khÃ´ng rÃµ táº¡i sao vÃ  lÃ m tháº¿ nÃ o
LLM cÃ³ thá»ƒ táº¡o ra mÃ£ chÃ­nh xÃ¡c tá»« cÃ¡c mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn.
PhÃ¢n tÃ­ch sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh lÃ  má»™t phÆ°Æ¡ng phÃ¡p luáº­n phá»• biáº¿n Ä‘á»ƒ hiá»ƒu cÃ¡ch má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng. NÃ³ Ä‘Ã£
Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i trong thá»‹ giÃ¡c mÃ¡y tÃ­nh [ 31,38,42,44,59,63,80,101] Ä‘á»ƒ Ä‘iá»u tra xem má»™t
mÃ´ hÃ¬nh cÃ³ chÃº Ã½ Ä‘áº¿n cÃ¡c pháº§n ná»•i báº­t cá»§a hÃ¬nh áº£nh khi Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh hay khÃ´ng. Äáº·c biá»‡t, gáº§n Ä‘Ã¢y
âˆ—CÃ´ng viá»‡c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n khi Shengmai Chen lÃ  sinh viÃªn Ä‘áº¡i há»c táº¡i Äáº¡i há»c Purdue.
Äá»‹a chá»‰ tÃ¡c giáº£: Bonan Kou, Äáº¡i há»c Purdue, West Lafayette, Má»¹, koub@purdue.edu; Shengmai Chen, Äáº¡i há»c Brown
, Providence, Má»¹, shengmai_chen@brown.edu; Zhijie Wang, Äáº¡i há»c Alberta, Edmonton, Canada, zhijie.
wang@ualberta.ca; Lei Ma, Äáº¡i há»c Tokyo, Tokyo, Nháº­t Báº£n vÃ  Äáº¡i há»c Alberta, Edmonton, Canada, ma.lei@
acm.org; Tianyi Zhang, Äáº¡i há»c Purdue, West Lafayette, Má»¹, tianyi@purdue.edu.
Quyá»n táº¡o báº£n sao ká»¹ thuáº­t sá»‘ hoáº·c báº£n cá»©ng toÃ n bá»™ hoáº·c má»™t pháº§n cÃ´ng trÃ¬nh nÃ y cho má»¥c Ä‘Ã­ch sá»­ dá»¥ng cÃ¡ nhÃ¢n hoáº·c lá»›p há»c Ä‘Æ°á»£c cáº¥p miá»…n phÃ­
vá»›i Ä‘iá»u kiá»‡n cÃ¡c báº£n sao khÃ´ng Ä‘Æ°á»£c táº¡o ra hoáº·c phÃ¢n phá»‘i vÃ¬ lá»£i nhuáº­n hoáº·c lá»£i tháº¿ thÆ°Æ¡ng máº¡i vÃ  cÃ¡c báº£n sao pháº£i ghi rÃµ thÃ´ng bÃ¡o nÃ y vÃ 
trÃ­ch dáº«n Ä‘áº§y Ä‘á»§ trÃªn trang Ä‘áº§u. Báº£n quyá»n Ä‘á»‘i vá»›i cÃ¡c thÃ nh pháº§n cá»§a cÃ´ng trÃ¬nh nÃ y thuá»™c sá»Ÿ há»¯u cá»§a nhá»¯ng ngÆ°á»i khÃ¡c ngoÃ i (cÃ¡c) tÃ¡c giáº£ pháº£i Ä‘Æ°á»£c tÃ´n trá»ng.
TÃ³m táº¯t cÃ³ ghi cÃ´ng Ä‘Æ°á»£c phÃ©p. Äá»ƒ sao chÃ©p theo cÃ¡ch khÃ¡c, hoáº·c tÃ¡i xuáº¥t báº£n, Ä‘á»ƒ Ä‘Äƒng trÃªn mÃ¡y chá»§ hoáº·c Ä‘á»ƒ phÃ¢n phá»‘i láº¡i cho danh sÃ¡ch, yÃªu cáº§u
quyá»n cá»¥ thá»ƒ trÆ°á»›c vÃ /hoáº·c phÃ­. YÃªu cáº§u quyá»n tá»« permissions@acm.org.
Â©2024 Báº£n quyá»n thuá»™c vá» chá»§ sá»Ÿ há»¯u/tÃ¡c giáº£. Quyá»n xuáº¥t báº£n Ä‘Æ°á»£c cáº¥p phÃ©p cho ACM.
ACM 2994-970X/2024/7-ART100
https://doi.org/10.1145/3660807
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.arXiv:2306.01220v2  [cs.SE]  23 May 2024

--- TRANG 2 ---
100:2 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
cÃ¡c nghiÃªn cá»©u tÃ¬m tháº¥y ráº±ng viá»‡c cÄƒn chá»‰nh sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i cÃ³ thá»ƒ nÃ¢ng cao hiá»‡u quáº£
hiá»‡u suáº¥t mÃ´ hÃ¬nh má»™t cÃ¡ch hiá»‡u quáº£ [ 8,34,42]. VÃ­ dá»¥, Huang et al. [ 42] cho tháº¥y hiá»‡u suáº¥t cá»§a cÃ¡c
mÃ´ hÃ¬nh phÃ¢n loáº¡i hÃ¬nh áº£nh dá»±a trÃªn Conv-4 cÃ³ thá»ƒ Ä‘Æ°á»£c tÄƒng lÃªn Ä‘áº¿n 23% khi chÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ cÄƒn chá»‰nh vá»›i
sá»± chÃº Ã½ cá»§a con ngÆ°á»i. HÆ¡n ná»¯a, cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y cÅ©ng cho tháº¥y ngÆ°á»i dÃ¹ng cÃ³ nhiá»u tin tÆ°á»Ÿng vÃ 
niá»m tin hÆ¡n vÃ o cÃ¡c mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i con ngÆ°á»i [ 13,40,75,89]. VÃ­ dá»¥, Boggust et al. [ 13] tÃ¬m tháº¥y ráº±ng ngÆ°á»i dÃ¹ng
xÃ¡c Ä‘á»‹nh Ä‘á»™ tin cáº­y cá»§a má»™t mÃ´ hÃ¬nh báº±ng cÃ¡ch kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ Ä‘Æ°a ra dá»± Ä‘oÃ¡n dá»±a
trÃªn cÃ¡c Ä‘áº·c trÆ°ng mÃ  há» coi lÃ  quan trá»ng hay khÃ´ng.
Nhá»¯ng phÃ¡t hiá»‡n nÃ y dáº«n Ä‘áº¿n má»™t cÃ¢u há»i khoa há»c quan trá»ng cho viá»‡c táº¡o mÃ£ dá»±a trÃªn LLMâ€” liá»‡u LLM cÃ³
chÃº Ã½ Ä‘áº¿n cÃ¡c pháº§n tÆ°Æ¡ng tá»± cá»§a mÃ´ táº£ nhiá»‡m vá»¥ nhÆ° láº­p trÃ¬nh viÃªn con ngÆ°á»i trong viá»‡c táº¡o mÃ£ khÃ´ng? ChÃºng tÃ´i chá»n
so sÃ¡nh sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i, vÃ¬ nÃ³ cÃ³ thá»ƒ giÃºp chÃºng tÃ´i xÃ¡c Ä‘á»‹nh xem LLM cÃ³ náº¯m báº¯t
ngá»¯ nghÄ©a sÃ¢u trong mÃ´ táº£ nhiá»‡m vá»¥ nhÆ° con ngÆ°á»i hay chÃºng chá»‰ há»c cÃ¡c máº«u bá» máº·t
tá»« dá»¯ liá»‡u huáº¥n luyá»‡n, Ä‘Ã³ lÃ  má»™t váº¥n Ä‘á» phá»• biáº¿n Ä‘Æ°á»£c biáº¿t Ä‘áº¿n trong há»c mÃ¡y. HÆ¡n ná»¯a, báº±ng cÃ¡ch
so sÃ¡nh cÃ¡c máº«u chÃº Ã½ cá»§a con ngÆ°á»i vÃ  mÃ´ hÃ¬nh, chÃºng tÃ´i tÃ¬m cÃ¡ch Ä‘iá»u tra xem sá»± khÃ¡c biá»‡t vá» chÃº Ã½
cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i thÃ­ch má»™t sá»‘ lá»—i táº¡o mÃ£ vÃ  thÃ´ng bÃ¡o cÃ¡c cÆ¡ há»™i má»›i Ä‘á»ƒ
cáº£i thiá»‡n LLM cho viá»‡c táº¡o mÃ£ hay khÃ´ng.
Äá»ƒ kháº¯c phá»¥c khoáº£ng cÃ¡ch kiáº¿n thá»©c, chÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n ná»— lá»±c Ä‘áº§u tiÃªn Ä‘á»ƒ tiáº¿t lá»™ quÃ¡ trÃ¬nh táº¡o mÃ£ cá»§a
LLM báº±ng cÃ¡ch phÃ¢n tÃ­ch nhá»¯ng pháº§n nÃ o cá»§a ngÃ´n ngá»¯ con ngÆ°á»i mÃ  LLM chÃº Ã½ Ä‘áº¿n khi táº¡o mÃ£ . ChÃºng tÃ´i
trÃ¬nh bÃ y má»™t nghiÃªn cá»©u quy mÃ´ lá»›n kiá»ƒm tra sá»± cÄƒn chá»‰nh chÃº Ã½ giá»¯a sÃ¡u LLM vÃ  láº­p trÃ¬nh viÃªn
con ngÆ°á»i trÃªn hai Ä‘iá»ƒm chuáº©n táº¡o mÃ£ phá»• biáº¿nâ€” Ä‘iá»ƒm chuáº©n HumanEval cá»§a OpenAI [ 27]
vÃ  Ä‘iá»ƒm chuáº©n MBPP cá»§a Google [ 7]. Giáº£ thuyáº¿t lÃ  LLM nÃªn táº¡o mÃ£ dá»±a
trÃªn cÃ¡c tá»« ná»•i báº­t tá»« mÃ´ táº£ NL tÆ°Æ¡ng tá»± nhÆ° láº­p trÃ¬nh viÃªn con ngÆ°á»i, thay vÃ¬ táº¡o
mÃ£ dá»±a trÃªn cÃ¡c token táº§m thÆ°á»ng nhÆ° giá»›i tá»« vÃ  dáº¥u phÃ¢n cÃ¡ch. Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘Ã£ Ä‘iá»u tra
cÃ¡c cÃ¢u há»i nghiÃªn cá»©u sau trong nghiÃªn cá»©u nÃ y:
RQ1 Má»©c Ä‘á»™ chÃº Ã½ cá»§a mÃ´ hÃ¬nh cÄƒn chá»‰nh vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i Ä‘áº¿n Ä‘Ã¢u?
RQ2 Sá»± chÃº Ã½ cÃ³ thá»ƒ giáº£i thÃ­ch lá»—i cá»§a cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ khÃ´ng?
RQ3 TÃ¡c Ä‘á»™ng cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau Ä‘áº¿n sá»± cÄƒn chá»‰nh chÃº Ã½ lÃ  gÃ¬?
RQ4 PhÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ nÃ o Ä‘Æ°á»£c láº­p trÃ¬nh viÃªn Æ°a chuá»™ng nháº¥t?
VÃ¬ khÃ´ng cÃ³ Ä‘iá»ƒm chuáº©n táº¡o mÃ£ hiá»‡n cÃ³ nÃ o chá»©a thÃ´ng tin chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn
,chÃºng tÃ´i Ä‘Ã£ táº¡o ra bá»™ dá»¯ liá»‡u chÃº Ã½ láº­p trÃ¬nh viÃªn Ä‘áº§u tiÃªn cho cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh tá»« HumanEval
vÃ  MBPP (tá»•ng cá»™ng 1,138 nhiá»‡m vá»¥). ChÃºng tÃ´i Ä‘Ã£ ghi láº¡i sá»± chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn báº±ng cÃ¡ch yÃªu cáº§u hai láº­p trÃ¬nh viÃªn
cÃ³ kinh nghiá»‡m gáº¯n nhÃ£n thá»§ cÃ´ng cÃ¡c tá»« vÃ  cá»¥m tá»« mÃ  há» coi lÃ  thiáº¿t yáº¿u Ä‘á»ƒ giáº£i quyáº¿t má»—i
nhiá»‡m vá»¥ láº­p trÃ¬nh. NhÃ£n cá»§a há» Ä‘Æ°á»£c xÃ¡c thá»±c bá»Ÿi má»™t láº­p trÃ¬nh viÃªn thá»© ba. Máº·t khÃ¡c, Ä‘á»ƒ ghi láº¡i
sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh, chÃºng tÃ´i Ä‘Ã£ triá»ƒn khai vÃ  thá»­ nghiá»‡m vá»›i mÆ°á»i hai phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau
trong ba danh má»¥câ€” dá»±a trÃªn tá»± chÃº Ã½ ,dá»±a trÃªn gradient , vÃ  dá»±a trÃªn nhiá»…u loáº¡n . Äá»ƒ Ä‘áº£m báº£o
phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i khÃ¡i quÃ¡t hÃ³a trÃªn cÃ¡c LLM khÃ¡c nhau, chÃºng tÃ´i Ä‘Ã£ phÃ¢n tÃ­ch sá»± chÃº Ã½ cá»§a sÃ¡u LLM vá»›i
kÃ­ch thÆ°á»›c khÃ¡c nhau, bao gá»“m GPT-4 [ 2], InCoder-1.3B [ 32], CoderGen-2.7B [ 62], PolyCoder-2.7B [ 91],
CodeParrot-1.5B [1], vÃ  GPT-J-6B [87].
NghiÃªn cá»©u cá»§a chÃºng tÃ´i tiáº¿t lá»™ má»™t sá»‘ hiá»ƒu biáº¿t quan trá»ng vá» quÃ¡ trÃ¬nh táº¡o mÃ£ cá»§a LLM. Äáº§u tiÃªn,
chÃºng tÃ´i tÃ¬m tháº¥y sá»± khÃ´ng cÄƒn chá»‰nh chÃº Ã½ nháº¥t quÃ¡n trong táº¥t cáº£ sÃ¡u LLM, báº¥t ká»ƒ cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½.
HÆ¡n ná»¯a, chÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n phÃ¢n tÃ­ch sÃ¢u vá» cÃ¡c máº«u chÃº Ã½ cá»§a 211 mÃ£ khÃ´ng chÃ­nh xÃ¡c
Ä‘Æ°á»£c táº¡o ra bá»Ÿi hai mÃ´ hÃ¬nh tá»‘t nháº¥t trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i, CodeGen-2.7B, vÃ  GPT-4. ChÃºng tÃ´i tÃ¬m tháº¥y ráº±ng 27%
lá»—i táº¡o mÃ£ cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi nÄƒm máº«u chÃº Ã½. Cuá»‘i cÃ¹ng, cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n
Ä‘Ã£ táº¡o ra Ä‘iá»ƒm chÃº Ã½ tá»•ng thá»ƒ phÃ¹ há»£p hÆ¡n vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i hÆ¡n
cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c. ChÃºng cÅ©ng Ä‘Æ°á»£c Æ°a chuá»™ng bá»Ÿi láº­p trÃ¬nh viÃªn con ngÆ°á»i, theo má»™t nghiÃªn cá»©u ngÆ°á»i dÃ¹ng vá»›i 22
ngÆ°á»i tham gia. Nhá»¯ng phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i lÃ m ná»•i báº­t nhu cáº§u phÃ¡t triá»ƒn LLM phÃ¹ há»£p vá»›i con ngÆ°á»i vÃ  cung cáº¥p hÆ°á»›ng dáº«n thá»±c táº¿
Ä‘á»ƒ cáº£i thiá»‡n viá»‡c táº¡o mÃ£ dá»±a trÃªn LLM vÃ  tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh.
TÃ³m láº¡i, bÃ i bÃ¡o nÃ y cÃ³ nhá»¯ng Ä‘Ã³ng gÃ³p sau:
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 3 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:3
â—ChÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh nghiÃªn cá»©u thá»±c nghiá»‡m Ä‘áº§u tiÃªn vá» sá»± cÄƒn chá»‰nh chÃº Ã½ cá»§a LLM vÃ  láº­p trÃ¬nh viÃªn con ngÆ°á»i
trÃªn cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£.
â—ChÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh phÃ¢n tÃ­ch so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau cho cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£
thÃ´ng qua cáº£ thá»­ nghiá»‡m Ä‘á»‹nh lÆ°á»£ng vÃ  nghiÃªn cá»©u ngÆ°á»i dÃ¹ng.
â—ChÃºng tÃ´i Ä‘Ã£ cÃ´ng khai bá»™ dá»¯ liá»‡u chÃº Ã½ láº­p trÃ¬nh viÃªn Ä‘áº§u tiÃªn cá»§a 1,138 nhiá»‡m vá»¥ Python, cÃ³ thá»ƒ
Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i con ngÆ°á»i má»›i vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p diá»…n giáº£i cho cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£
. MÃ£ vÃ  dá»¯ liá»‡u cá»§a chÃºng tÃ´i Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p trong kho GitHub cá»§a chÃºng tÃ´i [47].
2 Äá»˜NG Lá»°C VÃ€ KIáº¾N THá»¨C CÆ  Báº¢N
Trong pháº§n nÃ y, trÆ°á»›c tiÃªn chÃºng tÃ´i giáº£i thÃ­ch Ä‘á»™ng lá»±c Ä‘á»ƒ thá»±c hiá»‡n phÃ¢n tÃ­ch chÃº Ã½ cho cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£.
Sau Ä‘Ã³, chÃºng tÃ´i giá»›i thiá»‡u cÃ¡c Ä‘iá»ƒm chuáº©n vÃ  sá»‘ liá»‡u phá»• biáº¿n cho viá»‡c táº¡o mÃ£. Cuá»‘i cÃ¹ng, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a
sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh vÃ  mÃ´ táº£ cÃ¡c loáº¡i phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau cho LLM.
2.1 Äá»™ng lá»±c
PhÃ¢n tÃ­ch sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh lÃ  má»™t nhiá»‡m vá»¥ phá»• biáº¿n trong má»™t sá»‘ lÄ©nh vá»±c, nhÆ° thá»‹ giÃ¡c mÃ¡y tÃ­nh [ 31,44,
59,63], dá»‹ch mÃ¡y tháº§n kinh [ 18], vÃ  lÃ¡i xe tá»± Ä‘á»™ng [ 38,46,80]. Cá»¥ thá»ƒ, má»™t sá»‘
nghiÃªn cá»©u cho tháº¥y viá»‡c cÄƒn chá»‰nh sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t mÃ´ hÃ¬nh [ 8,34,42]. VÃ­ dá»¥, Huang et al. [ 42] cho tháº¥y
báº±ng cÃ¡ch cÄƒn chá»‰nh sá»± chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Conv-4 vÃ  ResNet vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i trÃªn
hÃ¬nh áº£nh, hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh nÃ y trÃªn phÃ¢n loáº¡i hÃ¬nh áº£nh cÃ³ thá»ƒ Ä‘Æ°á»£c tÄƒng lÃªn Ä‘áº¿n 23% trong
cÃ i Ä‘áº·t má»™t láº§n vÃ  10% trong cÃ i Ä‘áº·t nÄƒm láº§n. Trong lÃ¡i xe tá»± Ä‘á»™ng, nhiá»u nghiÃªn cá»©u Ä‘Ã£
chá»©ng minh ráº±ng viá»‡c thÃªm cÃ¡c rÃ ng buá»™c cÄƒn chá»‰nh chÃº Ã½ cÃ³ thá»ƒ giÃºp há»‡ thá»‘ng lÃ¡i xe tá»± Ä‘á»™ng
lÃ¡i xe an toÃ n hÆ¡n [ 38,46,80]. Äiá»u nÃ y thÃºc Ä‘áº©y chÃºng tÃ´i Ä‘iá»u tra sá»± cÄƒn chá»‰nh chÃº Ã½ giá»¯a LLM
vÃ  láº­p trÃ¬nh viÃªn con ngÆ°á»i trong lÄ©nh vá»±c táº¡o mÃ£.
Má»™t sá»‘ nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ phÃ¢n tÃ­ch sá»± chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh tháº§n kinh cho tÃ³m táº¯t mÃ£, sá»­a chá»¯a chÆ°Æ¡ng trÃ¬nh
vÃ  dá»± Ä‘oÃ¡n tÃªn phÆ°Æ¡ng thá»©c [ 8,64,68]. Paltenghi et al. [ 64] nghiÃªn cá»©u sá»± cÄƒn chá»‰nh chÃº Ã½
giá»¯a sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh tháº§n kinh vÃ  sá»± chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn vá» tÃ³m táº¯t mÃ£. Bansal et al. [ 8]
cho tháº¥y viá»‡c cÄƒn chá»‰nh sá»± chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh tÃ³m táº¯t mÃ£ tháº§n kinh vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i
cÃ³ thá»ƒ cáº£i thiá»‡n hiá»‡u quáº£ hiá»‡u suáº¥t mÃ´ hÃ¬nh. Rabin et al. [ 68] tÃ¬m tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh mÃ£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c
phá»¥ thuá»™c ráº¥t nhiá»u vÃ o chá»‰ má»™t vÃ i Ä‘áº·c trÆ°ng cÃº phÃ¡p trong lá»i nháº¯c Ä‘á»ƒ thá»±c hiá»‡n dá»± Ä‘oÃ¡n tÃªn phÆ°Æ¡ng thá»©c
vÃ  phÃ¡t hiá»‡n sá»­ dá»¥ng sai biáº¿n. Theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, khÃ´ng cÃ³ nghiÃªn cá»©u nÃ o hiá»‡n cÃ³ Ä‘Ã£
Ä‘iá»u tra cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£ hoáº·c LLM vá»›i hÃ ng tá»· tham sá»‘. NghiÃªn cá»©u cá»§a chÃºng tÃ´i láº¥p Ä‘áº§y khoáº£ng trá»‘ng nÃ y báº±ng cÃ¡ch
phÃ¢n tÃ­ch cÃ¡c máº«u chÃº Ã½ cá»§a sÃ¡u LLM trong cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£.
Cuá»‘i cÃ¹ng, má»™t Ä‘á»™ng lá»±c khÃ¡c Ä‘áº±ng sau nghiÃªn cá»©u nÃ y lÃ  thá»±c táº¿ ráº±ng váº«n chÆ°a cÃ³ sá»± Ä‘á»“ng thuáº­n vá» cÃ¡ch
tÃ­nh toÃ¡n Ä‘iá»ƒm chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh mÃ£ dá»±a trÃªn LLM. Äiá»u nÃ y pháº§n lá»›n lÃ  do tÃ­nh phá»©c táº¡p
cá»§a cÆ¡ cháº¿ chÃº Ã½ Ä‘a Ä‘áº§u, Ä‘a lá»›p Ä‘Æ°á»£c Ã¡p dá»¥ng bá»Ÿi cÃ¡c mÃ´ hÃ¬nh nÃ y. VÃ­ dá»¥, má»™t sá»‘
nghiÃªn cá»©u chá»‰ xem xÃ©t sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh tá»« lá»›p transformer Ä‘áº§u tiÃªn vÃ  nÃ³i ráº±ng lá»›p Ä‘áº§u tiÃªn
mÃ£ hÃ³a thÃ´ng tin cáº¥p tá»« vá»±ng [ 10,97], trong khi cÃ¡c nghiÃªn cá»©u khÃ¡c tá»•ng há»£p sá»± chÃº Ã½ tá»«
táº¥t cáº£ cÃ¡c lá»›p transformer Ä‘á»ƒ káº¿t há»£p cÃ¡c má»‘i quan há»‡ token táº§m xa [ 56,85]. Äá»ƒ kháº¯c phá»¥c khoáº£ng cÃ¡ch nÃ y,
chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh nghiÃªn cá»©u toÃ n diá»‡n Ä‘áº§u tiÃªn vá» 12 phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ vÃ  Ä‘Ã¡nh giÃ¡ chÃºng má»™t cÃ¡ch há»‡ thá»‘ng
vá»›i cÃ¡c thá»­ nghiá»‡m Ä‘á»‹nh lÆ°á»£ng vÃ  nghiÃªn cá»©u ngÆ°á»i dÃ¹ng vá»›i 22 ngÆ°á»i tham gia.
2.2 Äiá»ƒm chuáº©n vÃ  Sá»‘ liá»‡u Táº¡o mÃ£
Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i táº­p trung vÃ o cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£ táº¡o ra má»™t hÃ m tá»« mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn.
Ká»ƒ tá»« Ä‘á»™t phÃ¡ cá»§a mÃ´ hÃ¬nh Codex cá»§a OpenAI vÃ o nÄƒm 2021 [ 27], loáº¡i nhiá»‡m vá»¥ táº¡o mÃ£ nÃ y
Ä‘Ã£ trá»Ÿ nÃªn ngÃ y cÃ ng phá»• biáº¿n trong cá»™ng Ä‘á»“ng nghiÃªn cá»©u. Trong cÃ i Ä‘áº·t nhiá»‡m vá»¥ nÃ y,
cho trÆ°á»›c header hÃ m vÃ  mÃ´ táº£ nhiá»‡m vá»¥ báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn, má»™t LLM Ä‘Æ°á»£c ká»³ vá»ng hoÃ n thÃ nh
hÃ m theo mÃ´ táº£ nhiá»‡m vá»¥. HÃ¬nh 1 cho tháº¥y má»™t vÃ­ dá»¥.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 4 ---
100:4 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
HÃ¬nh 1. Má»™t hÃ m Python Ä‘Æ°á»£c táº¡o bá»Ÿi CodeGen-2.7B [62]. MÃ£ Ä‘Æ°á»£c táº¡o Ä‘Æ°á»£c lÃ m ná»•i báº­t báº±ng mÃ u xanh lÃ¡ cÃ¢y.
CÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ thÆ°á»ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c Ä‘iá»ƒm chuáº©n láº­p trÃ¬nh Ä‘Æ°á»£c cá»™ng Ä‘á»“ng Ä‘Ã³ng gÃ³p. OpenAI Ä‘Ã£ phÃ¡t triá»ƒn má»™t Ä‘iá»ƒm chuáº©n láº­p trÃ¬nh gá»i lÃ  HumanEval vÃ  sá»­ dá»¥ng nÃ³ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh Codex gá»‘c
vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ [ 27]. HumanEval [ 27] bao gá»“m 164 nhiá»‡m vá»¥ láº­p trÃ¬nh Python vÃ 
cÃ¡c giáº£i phÃ¡p thá»±c táº¿. NÃ³ cho Ä‘áº¿n nay lÃ  Ä‘iá»ƒm chuáº©n phá»• biáº¿n nháº¥t vÃ  Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ háº§u háº¿t
cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM. NgoÃ i ra, MBPP lÃ  má»™t Ä‘iá»ƒm chuáº©n lá»›n [ 7] vá»›i 974 nhiá»‡m vá»¥ láº­p trÃ¬nh
Ä‘Æ°á»£c cá»™ng Ä‘á»“ng Ä‘Ã³ng gÃ³p vÃ  cÃ¡c giáº£i phÃ¡p. Cáº£ HumanEval vÃ  MBPP Ä‘á»u bao gá»“m cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
tÃ­nh chÃ­nh xÃ¡c chá»©c nÄƒng cá»§a mÃ£ Ä‘Æ°á»£c táº¡o.
Hai loáº¡i sá»‘ liá»‡u thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM.
Äáº§u tiÃªn, náº¿u má»™t Ä‘iá»ƒm chuáº©n táº¡o mÃ£ bao gá»“m cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m, ngÆ°á»i ta cÃ³ thá»ƒ Ä‘Æ¡n giáº£n cháº¡y cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m
Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tÃ­nh chÃ­nh xÃ¡c cá»§a mÃ£ Ä‘Æ°á»£c táº¡o. Má»™t sá»‘ liá»‡u Ä‘iá»ƒn hÃ¬nh trong danh má»¥c nÃ y lÃ  Pass@k .
Pass@k ban Ä‘áº§u Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Kulal et al. [ 50]. NÃ³ Ä‘o lÆ°á»ng tá»· lá»‡ pháº§n trÄƒm cá»§a cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh
Ä‘Æ°á»£c giáº£i quyáº¿t chÃ­nh xÃ¡c trong Ä‘Ã³ ğ‘˜Ä‘á» cáº­p Ä‘áº¿n sá»‘ lÆ°á»£ng máº«u mÃ£ Ä‘Æ°á»£c táº¡o bá»Ÿi LLM. Náº¿u báº¥t ká»³
ğ‘˜máº«u nÃ o vÆ°á»£t qua táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m trong má»™t nhiá»‡m vá»¥, nhiá»‡m vá»¥ Ä‘Ã³ Ä‘Æ°á»£c coi lÃ  Ä‘Æ°á»£c giáº£i quyáº¿t chÃ­nh xÃ¡c. OpenAI sau Ä‘Ã³
giá»›i thiá»‡u má»™t phiÃªn báº£n khÃ´ng thiÃªn vá»‹ cá»§a Pass@k Ä‘á»ƒ giáº£m biáº¿n Ä‘á»™ng, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ ngÃ y nay [ 27]. Trong thá»±c táº¿, nhiá»u nhiá»‡m vá»¥ láº­p trÃ¬nh khÃ´ng cÃ³ cÃ¡c trÆ°á»ng há»£p thá»­ nghiá»‡m hiá»‡n cÃ³
vÃ  má»™t sá»‘ giáº£i phÃ¡p mÃ£ khÃ´ng cÃ³ giao diá»‡n hÃ m Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a rÃµ rÃ ng Ä‘á»ƒ kiá»ƒm tra, vÃ­ dá»¥: má»™t
dÃ²ng mÃ£ duy nháº¥t mÃ  khÃ´ng cÃ³ Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra rÃµ rÃ ng. Do Ä‘Ã³, cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y cÅ©ng Ä‘o lÆ°á»ng sá»± tÆ°Æ¡ng tá»±
giá»¯a mÃ£ Ä‘Æ°á»£c táº¡o vÃ  giáº£i phÃ¡p thá»±c táº¿ nhÆ° má»™t Ä‘áº¡i diá»‡n cho tÃ­nh chÃ­nh xÃ¡c. BLEU [ 65] vÃ 
CodeBLEU [ 69] lÃ  cÃ¡c sá»‘ liá»‡u tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ã¡p dá»¥ng phá»• biáº¿n. BLEU lÃ  má»™t sá»‘ liá»‡u thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ
Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng vÄƒn báº£n Ä‘Æ°á»£c táº¡o bá»Ÿi mÃ¡y. NÃ³ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng vÄƒn báº£n Ä‘Æ°á»£c táº¡o bá»Ÿi mÃ¡y
báº±ng cÃ¡ch so sÃ¡nh sá»± hiá»‡n diá»‡n cá»§a n-gram trong vÄƒn báº£n Ä‘Æ°á»£c táº¡o vá»›i nhá»¯ng n-gram trong vÄƒn báº£n tham kháº£o. BLEU
tÃ­nh toÃ¡n má»™t Ä‘iá»ƒm sá»‘ tá»« 0 Ä‘áº¿n 1, vá»›i 1 biá»ƒu thá»‹ sá»± tÆ°Æ¡ng tá»± hoÃ n háº£o. CodeBLEU Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ
Ä‘iá»u chá»‰nh BLEU cá»¥ thá»ƒ cho viá»‡c Ä‘Ã¡nh giÃ¡ mÃ£ Ä‘Æ°á»£c táº¡o. Trong khi BLEU chá»§ yáº¿u xem xÃ©t sá»± tÆ°Æ¡ng tá»± á»Ÿ cáº¥p Ä‘á»™ tá»«,
CodeBLEU xem xÃ©t cáº¥u trÃºc vÃ  tÃ­nh chÃ­nh xÃ¡c cá»§a mÃ£ Ä‘Æ°á»£c táº¡o, Ä‘Ã³ lÃ 
cÃ¡c khÃ­a cáº¡nh quan trá»ng trong cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£.
2.3 Sá»± chÃº Ã½ cá»§a MÃ´ hÃ¬nh
Trong cÃ´ng viá»‡c nÃ y, chÃºng tÃ´i sá»­ dá»¥ng sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘á» cáº­p Ä‘áº¿n má»©c Ä‘á»™ quan trá»ng cá»§a má»™t token trong mÃ´ táº£ nhiá»‡m vá»¥
ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘Æ°á»£c coi lÃ  quan trá»ng bá»Ÿi má»™t LLM trong quÃ¡ trÃ¬nh táº¡o mÃ£. NÃ³ ngá»¥ Ã½ nhá»¯ng pháº§n nÃ o cá»§a
Ä‘áº§u vÃ o mÃ  mÃ´ hÃ¬nh "chÃº Ã½" Ä‘áº¿n trong quÃ¡ trÃ¬nh táº¡o mÃ£. Ã tÆ°á»Ÿng nÃ y giá»‘ng vá»›i táº§m quan trá»ng cá»§a Ä‘áº·c trÆ°ng [41],
báº£n Ä‘á»“ ná»•i báº­t [61], vÃ  gÃ¡n Ä‘áº·c trÆ°ng [60] trong tÃ i liá»‡u XAI. ChÃºng tÃ´i mÃ´ táº£ ba loáº¡i
phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh nhÆ° sau.
2.3.1 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Tá»± chÃº Ã½. CÆ¡ cháº¿ tá»± chÃº Ã½ cho phÃ©p transformer cÃ¢n nháº¯c
táº§m quan trá»ng cá»§a cÃ¡c pháº§n khÃ¡c nhau cá»§a Ä‘áº§u vÃ o khi Ä‘Æ°a ra dá»± Ä‘oÃ¡n. Báº±ng cÃ¡ch táº­p trung vÃ o nhá»¯ng
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 5 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:5
HÃ¬nh 2. Ma tráº­n chÃº Ã½ cá»§a Ä‘áº§u chÃº Ã½ Ä‘áº§u tiÃªn cá»§a lá»›p transformer trong CodeGen-2.7B
token cÃ³ liÃªn quan nháº¥t vá»›i Ä‘iá»ƒm tá»± chÃº Ã½ cao nháº¥t, transformer cÃ³ thá»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n tá»‘t hÆ¡n
báº±ng cÃ¡ch náº¯m báº¯t má»‘i quan há»‡ vÃ  phá»¥ thuá»™c trong Ä‘áº§u vÃ o.
Má»™t LLM bao gá»“m nhiá»u lá»›p transformer , má»—i lá»›p bao gá»“m nhiá»u Ä‘áº§u chÃº Ã½ .
Nhá»¯ng Ä‘áº§u chÃº Ã½ nÃ y tÃ­nh toÃ¡n Ä‘á»™c láº­p Ä‘iá»ƒm tá»± chÃº Ã½ giá»¯a cÃ¡c token Ä‘áº§u vÃ o khÃ¡c nhau.
Do Ä‘Ã³, má»™t mÃ´ hÃ¬nh transformer cÃ³ thá»ƒ cÃ³ nhiá»u nguá»“n chÃº Ã½ mÃ´ hÃ¬nh tá»« cÃ¡c
lá»›p transformer khÃ¡c nhau vÃ  cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau trong má»—i lá»›p. VÃ­ dá»¥, HÃ¬nh 2 cho tháº¥y
Ä‘iá»ƒm tá»± chÃº Ã½ cá»§a ba Ä‘áº§u chÃº Ã½ Ä‘áº§u tiÃªn cá»§a lá»›p transformer Ä‘áº§u tiÃªn trong CodeGen-2.7B khi táº¡o token "numbers" tá»« chuá»—i Ä‘áº§u vÃ o "Return the max between two" . Äiá»ƒm tá»± chÃº Ã½
Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau khÃ¡c nhau cho cÃ¹ng má»™t token. CÃ¡c Ä‘áº§u khÃ¡c nhau
Ä‘áº¡i diá»‡n cho cÃ¡c loáº¡i "táº­p trung" khÃ¡c nhau tá»« mÃ´ hÃ¬nh.
Äá»ƒ tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh trÃªn chuá»—i Ä‘áº§u vÃ o, vector cá»§a Ä‘iá»ƒm tá»± chÃº Ã½ tá»« cÃ¡c
lá»›p chÃº Ã½ khÃ¡c nhau vÃ  cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau trong má»—i lá»›p cáº§n Ä‘Æ°á»£c tá»•ng há»£p thÃ nh má»™t
vector duy nháº¥t Ä‘á»ƒ Ä‘áº¡i diá»‡n cho táº§m quan trá»ng tá»•ng thá»ƒ cá»§a má»—i token Ä‘áº§u vÃ o Ä‘á»‘i vá»›i dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh. Tuy nhiÃªn,
máº·c dÃ¹ Ä‘iá»ƒm tá»± chÃº Ã½ Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng chung nhÆ° sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh [ 20,33,52,84], váº«n
chÆ°a cÃ³ sá»± Ä‘á»“ng thuáº­n vá» cÃ¡ch tá»•ng há»£p Ä‘iá»ƒm tá»± chÃº Ã½ tá»« cÃ¡c lá»›p vÃ  Ä‘áº§u chÃº Ã½ khÃ¡c nhau. VÃ­ dá»¥, má»™t sá»‘ nghiÃªn cá»©u tá»•ng há»£p Ä‘iá»ƒm chÃº Ã½ tá»« táº¥t cáº£ cÃ¡c lá»›p transformer
vÃ  táº¥t cáº£ cÃ¡c Ä‘áº§u chÃº Ã½ trong má»—i lá»›p [ 99] nhÆ° Ä‘iá»ƒm chÃº Ã½ cuá»‘i cÃ¹ng. Nhá»¯ng nghiÃªn cá»©u nÃ y láº­p luáº­n ráº±ng chiáº¿n lÆ°á»£c tá»•ng há»£p nÃ y cÃ³ thá»ƒ náº¯m báº¯t má»‘i quan há»‡ token táº§m xa. Má»™t sá»‘ nghiÃªn cá»©u khÃ¡c chá»‰ sá»­ dá»¥ng
Ä‘iá»ƒm chÃº Ã½ tá»« lá»›p transformer Ä‘áº§u tiÃªn vÃ  láº­p luáº­n ráº±ng lá»›p Ä‘áº§u tiÃªn náº¯m báº¯t cÃ¡c phá»¥ thuá»™c á»Ÿ cáº¥p Ä‘á»™ tá»« vá»±ng [10, 97].
Äá»ƒ tiáº¿t lá»™ cÃ¡ch cÃ¡c cÃ¡ch khÃ¡c nhau Ä‘á»ƒ tá»•ng há»£p tá»± chÃº Ã½ áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»± cÄƒn chá»‰nh giá»¯a mÃ´ hÃ¬nh vÃ 
sá»± chÃº Ã½ cá»§a con ngÆ°á»i, chÃºng tÃ´i Ä‘Ã£ thá»­ nghiá»‡m vá»›i sÃ¡u phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½ trong nghiÃªn cá»©u nÃ y (chi tiáº¿t trong
Pháº§n 4.2.1).
2.3.2 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Gradient. PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient táº­n dá»¥ng gradient cá»§a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh
liÃªn quan Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o Ä‘á»ƒ tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh. Viá»‡c tÃ­nh toÃ¡n cá»§a
phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient bao gá»“m hai bÆ°á»›c khÃ¡c nhau: (1) thá»±c hiá»‡n má»™t láº§n truyá»n xuÃ´i cá»§a Ä‘áº§u vÃ o quan tÃ¢m, vÃ  (2) tÃ­nh toÃ¡n gradient báº±ng cÃ¡ch sá»­ dá»¥ng lan truyá»n ngÆ°á»£c qua cÃ¡c lá»›p cá»§a máº¡ng tháº§n kinh. Báº±ng cÃ¡ch
phÃ¢n tÃ­ch Ä‘á»™ lá»›n cá»§a nhá»¯ng gradient nÃ y, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh token Ä‘áº§u vÃ o nÃ o
cÃ³ áº£nh hÆ°á»Ÿng nháº¥t trong viá»‡c xÃ¡c Ä‘á»‹nh Ä‘áº§u ra. VÃ­ dá»¥, Integrated Gradients lÃ  má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient
tÃ­nh toÃ¡n tÃ­ch phÃ¢n cá»§a gradient cá»§a Ä‘áº§u ra mÃ´ hÃ¬nh liÃªn quan Ä‘áº¿n má»—i Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o
[ 23,76,81]. Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i Ä‘Ã£ thá»­ nghiá»‡m vá»›i hai phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient Ä‘Æ°á»£c sá»­ dá»¥ng trong
cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y [76, 78] vá»›i chi tiáº¿t trong Pháº§n 4.2.2.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 6 ---
100:6 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
2.3.3 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Nhiá»…u loáº¡n. KhÃ¡c vá»›i hai danh má»¥c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y,
phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n [ 84,90] lÃ  báº¥t kháº£ tri mÃ´ hÃ¬nh. NÃ³i cÃ¡ch khÃ¡c, chÃºng khÃ´ng yÃªu cáº§u
truy cáº­p vÃ o thÃ´ng tin bÃªn trong cá»§a má»™t mÃ´ hÃ¬nh. PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n Ä‘áº·c biá»‡t há»¯u Ã­ch
Ä‘á»ƒ tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh thÆ°Æ¡ng máº¡i nhÆ° GPT-4, vÃ¬ nhá»¯ng mÃ´ hÃ¬nh nÃ y khÃ´ng tiáº¿t lá»™
cÃ¡c lá»›p tá»± chÃº Ã½ hoáº·c gradient cá»§a chÃºng cho ngÆ°á»i dÃ¹ng.
PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n Ä‘áº§u tiÃªn biáº¿n Ä‘á»•i Ä‘áº§u vÃ o vÃ  sau Ä‘Ã³ tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh dá»±a
trÃªn sá»± khÃ¡c biá»‡t Ä‘áº§u ra. LIME [ 70] vÃ  SHAP [ 57] lÃ  hai phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n phá»• biáº¿n.
LIME [ 70] táº¡o ra má»™t giáº£i thÃ­ch cá»¥c bá»™ báº±ng cÃ¡ch xáº¥p xá»‰ dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh cá»¥ thá»ƒ vá»›i má»™t
mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n (vÃ­ dá»¥: má»™t bá»™ phÃ¢n loáº¡i tuyáº¿n tÃ­nh). SHAP [ 57] nÃ¢ng cao LIME báº±ng cÃ¡ch lÃ m nhiá»…u Ä‘áº§u vÃ o dá»±a
trÃªn lÃ½ thuyáº¿t trÃ² chÆ¡i vÃ  sá»­ dá»¥ng giÃ¡ trá»‹ Shapely Ä‘á»ƒ Æ°á»›c tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c token khÃ¡c nhau.
Má»™t háº¡n cháº¿ cá»§a hai phÆ°Æ¡ng phÃ¡p nÃ y lÃ  chÃºng thÆ°á»ng yÃªu cáº§u má»™t sá»‘ lÆ°á»£ng lá»›n máº«u nhiá»…u loáº¡n
Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c Æ°á»›c tÃ­nh. Äiá»u nÃ y tá»‘n kÃ©m Ä‘á»ƒ tÃ­nh toÃ¡n sá»± chÃº Ã½ cho GPT-4, vÃ¬ chÃºng ta cáº§n
truy váº¥n GPT-4 nhiá»u láº§n. HÆ¡n ná»¯a, LIME vÃ  SHAP chá»‰ biáº¿n Ä‘á»•i Ä‘áº§u vÃ o báº±ng cÃ¡ch xÃ³a token,
cÃ³ thá»ƒ thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ Ã½ nghÄ©a hoáº·c cáº¥u trÃºc cá»§a Ä‘áº§u vÃ o. Äá»ƒ giáº£i quyáº¿t háº¡n cháº¿ nÃ y,
cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n gáº§n Ä‘Ã¢y hÆ¡n chá»n thay tháº¿ token báº±ng token tÆ°Æ¡ng tá»± hoáº·c cÃ³ liÃªn quan vá» máº·t ngá»¯ nghÄ©a
trong ngá»¯ cáº£nh [ 54,90]. ChÃºng thÆ°á»ng sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ máº·t náº¡ nhÆ° BERT [ 25]
Ä‘á»ƒ dá»± Ä‘oÃ¡n token tÆ°Æ¡ng tá»± hoáº·c cÃ³ liÃªn quan vá» máº·t ngá»¯ nghÄ©a Ä‘á»ƒ thay tháº¿ token hiá»‡n cÃ³ trong Ä‘áº§u vÃ o. Sau Ä‘Ã³,
chÃºng Ä‘o lÆ°á»ng áº£nh hÆ°á»Ÿng cá»§a nhá»¯ng thay tháº¿ nÃ y lÃªn Ä‘áº§u ra. Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i Ä‘Ã£ thá»­ nghiá»‡m
vá»›i SHAP [57] vÃ  phÆ°Æ¡ng phÃ¡p dá»±a trÃªn che máº·t náº¡ BERT [90] (chi tiáº¿t trong Pháº§n 4.2.3).
3 XÃ‚Y Dá»°NG Bá»˜ Dá»® LIá»†U CHÃš Ã Cá»¦A Láº¬P TRÃŒNH VIÃŠN
VÃ¬ khÃ´ng cÃ³ Ä‘iá»ƒm chuáº©n táº¡o mÃ£ hiá»‡n cÃ³ nÃ o chá»©a thÃ´ng tin chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn
(tá»©c lÃ  tá»« hoáº·c cá»¥m tá»« nÃ o mÃ  láº­p trÃ¬nh viÃªn coi lÃ  quan trá»ng khi viáº¿t mÃ£), chÃºng tÃ´i Ä‘Ã£ táº¡o
bá»™ dá»¯ liá»‡u chÃº Ã½ láº­p trÃ¬nh viÃªn Ä‘áº§u tiÃªn dá»±a trÃªn 1,138 nhiá»‡m vá»¥ láº­p trÃ¬nh, bao gá»“m táº¥t cáº£ 164
lá»i nháº¯c tá»« HumanEval [ 27] vÃ  974 lá»i nháº¯c tá»« MBPP [ 7]. ChÃºng tÃ´i Ä‘Ã£ chá»n hai bá»™ dá»¯ liá»‡u nÃ y,
vÃ¬ chÃºng Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ vÃ  chÃºng cÅ©ng cung cáº¥p trÆ°á»ng há»£p thá»­ nghiá»‡m cho
má»—i nhiá»‡m vá»¥ láº­p trÃ¬nh, Ä‘iá»u nÃ y quan trá»ng Ä‘á»ƒ tÃ­nh toÃ¡n tÃ­nh chÃ­nh xÃ¡c cá»§a mÃ£ do mÃ´ hÃ¬nh táº¡o ra.
Hai tÃ¡c giáº£ Ä‘áº§u tiÃªn, cÃ³ hÆ¡n nÄƒm nÄƒm kinh nghiá»‡m láº­p trÃ¬nh Python,
Ä‘Ã£ gáº¯n nhÃ£n thá»§ cÃ´ng cÃ¡c tá»« vÃ  cá»¥m tá»« mÃ  há» coi lÃ  quan trá»ng Ä‘á»ƒ giáº£i quyáº¿t nhiá»‡m vá»¥ láº­p trÃ¬nh
trong má»—i mÃ´ táº£ nhiá»‡m vá»¥. TrÆ°á»›c quÃ¡ trÃ¬nh gáº¯n nhÃ£n, hai ngÆ°á»i gáº¯n nhÃ£n Ä‘Ã£ xem qua cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh
trong HumanEval Ä‘á»ƒ lÃ m quen vá»›i cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh vÃ  cÃ¡c giáº£i phÃ¡p mÃ£.
Sau Ä‘Ã³, há» Ä‘áº§u tiÃªn Ä‘á»™c láº­p gáº¯n nhÃ£n 20 mÃ´ táº£ nhiá»‡m vá»¥ Ä‘áº§u tiÃªn trong HumanEval. VÃ²ng gáº¯n nhÃ£n Ä‘áº§u tiÃªn nÃ y
cÃ³ Ä‘iá»ƒm Cohen's Kappa lÃ  0.68. Hai ngÆ°á»i gáº¯n nhÃ£n Ä‘Ã£ tháº£o luáº­n vá» sá»± báº¥t Ä‘á»“ng vÃ  tÃ³m táº¯t bá»‘n loáº¡i tá»« khÃ³a
mÃ  cáº£ hai há» Ä‘á»u coi lÃ  quan trá»ng. Bá»‘n loáº¡i tá»« khÃ³a Ä‘Æ°á»£c tÃ³m táº¯t dÆ°á»›i Ä‘Ã¢y:
â—Loáº¡i dá»¯ liá»‡u : Tá»« hoáº·c cá»¥m tá»« mÃ´ táº£ cÃ¡c loáº¡i dá»¯ liá»‡u mÃ  mÃ£ nÃªn nháº­p hoáº·c xuáº¥t,
nhÆ° "string" ,"number" , hoáº·c"list" .
â—ToÃ¡n tá»­ : Tá»« hoáº·c cá»¥m tá»« mÃ´ táº£ cÃ¡c thao tÃ¡c mÃ  mÃ£ nÃªn thá»±c hiá»‡n trÃªn
dá»¯ liá»‡u, nhÆ° "compare" ,"sort" ,"filter" , hoáº·c"search" .
â—Äiá»u kiá»‡n : Tá»« hoáº·c cá»¥m tá»« chá»‰ Ä‘á»‹nh cÃ¡c Ä‘iá»u kiá»‡n mÃ  mÃ£ nÃªn thá»±c thi,
nhÆ° cÃ¡c cá»¥m tá»« sau "if"vÃ "when" trong mÃ´ táº£ nhiá»‡m vá»¥.
â—Thuá»™c tÃ­nh : Thuá»™c tÃ­nh quan trá»ng cá»§a dá»¯ liá»‡u vÃ  thao tÃ¡c Ä‘Æ°á»£c thao tÃ¡c, nhÆ° bá»™ Ä‘á»‹nh lÆ°á»£ng
(vÃ­ dá»¥: "all","one" ), tÃ­nh tá»« (vÃ­ dá»¥: "first" ,"closer" ), vÃ  tráº¡ng tá»« (vÃ­ dá»¥: "every" ,"none" ).
Máº·c dÃ¹ chá»‰ cÃ³ bá»‘n loáº¡i tá»« khÃ³a, má»—i loáº¡i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cÃ³ cáº¥p Ä‘á»™ cao vÃ 
bao quÃ¡t. VÃ­ dá»¥, loáº¡i "toÃ¡n tá»­" Ä‘á» cáº­p Ä‘áº¿n báº¥t ká»³ loáº¡i thao tÃ¡c nÃ o trÃªn dá»¯ liá»‡u, nhÆ°
"sort a list" ,"connect a database" , vÃ  "plot a graph" . Vá»›i tiÃªu chuáº©n gáº¯n nhÃ£n nÃ y, hai ngÆ°á»i gáº¯n nhÃ£n
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 7 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:7
HÃ¬nh 3. Hai vÃ­ dá»¥ vá» lá»i nháº¯c cÃ³ nhÃ£n tá»« bá»™ dá»¯ liá»‡u cá»§a chÃºng tÃ´i.
tiáº¿n hÃ nh gáº¯n nhÃ£n 144 mÃ´ táº£ nhiá»‡m vá»¥ cÃ²n láº¡i trong bá»™ dá»¯ liá»‡u HumanEval. Äiá»ƒm Cohen's Kappa
cá»§a vÃ²ng gáº¯n nhÃ£n nÃ y tÄƒng lÃªn 0.72, cho tháº¥y sá»± Ä‘á»“ng thuáº­n Ä‘Ã¡ng ká»ƒ [ 58]. Sau Ä‘Ã³, há»
tháº£o luáº­n vÃ  giáº£i quyáº¿t táº¥t cáº£ sá»± báº¥t Ä‘á»“ng.
Äá»ƒ xÃ¡c thá»±c nhá»¯ng nhÃ£n nÃ y, tÃ¡c giáº£ thá»© ba, ngÆ°á»i khÃ´ng tham gia vÃ o quÃ¡ trÃ¬nh gáº¯n nhÃ£n trÆ°á»›c Ä‘Ã¢y,
Ä‘á»™c láº­p gáº¯n nhÃ£n 164 mÃ´ táº£ nhiá»‡m vá»¥ tá»« bá»™ dá»¯ liá»‡u HumanEval. VÃ¬ Cohen's Kappa
chá»‰ cÃ³ thá»ƒ tÃ­nh toÃ¡n má»©c Ä‘á»™ Ä‘á»“ng thuáº­n giá»¯a hai ngÆ°á»i gáº¯n nhÃ£n, chÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng Fleiss' Kappa Ä‘á»ƒ Ä‘o lÆ°á»ng
sá»± Ä‘á»“ng thuáº­n giá»¯a ngÆ°á»i gáº¯n nhÃ£n thá»© ba vÃ  nhÃ£n ban Ä‘áº§u tá»« hai ngÆ°á»i gáº¯n nhÃ£n Ä‘áº§u tiÃªn. Äiá»ƒm Fleiss'
Kappa lÃ  0.64, cho tháº¥y sá»± Ä‘á»“ng thuáº­n Ä‘Ã¡ng ká»ƒ [ 30]. Káº¿t quáº£ nÃ y cho tháº¥y nhÃ£n Ä‘Æ°á»£c táº¡o bá»Ÿi
hai ngÆ°á»i gáº¯n nhÃ£n Ä‘áº§u tiÃªn lÃ  há»£p lÃ½ vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c cháº¥p nháº­n bá»Ÿi cÃ¡c láº­p trÃ¬nh viÃªn khÃ¡c. Sau Ä‘Ã³, hai ngÆ°á»i gáº¯n nhÃ£n Ä‘áº§u tiÃªn
tiáº¿p tá»¥c gáº¯n nhÃ£n 974 nhiá»‡m vá»¥ láº­p trÃ¬nh trong bá»™ dá»¯ liá»‡u MBPP má»™t cÃ¡ch Ä‘á»™c láº­p, Ä‘iá»u nÃ y
dáº«n Ä‘áº¿n Ä‘iá»ƒm Cohen's Kappa lÃ  0.73. Cuá»‘i cÃ¹ng, há» giáº£i quyáº¿t táº¥t cáº£ sá»± báº¥t Ä‘á»“ng vÃ  sá»­ dá»¥ng
táº­p há»£p nhÃ£n cuá»‘i cÃ¹ng lÃ m bá»™ dá»¯ liá»‡u chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn. ToÃ n bá»™ quÃ¡ trÃ¬nh gáº¯n nhÃ£n máº¥t 192
giá» cÃ´ng.
Trung bÃ¬nh, má»—i mÃ´ táº£ nhiá»‡m vá»¥ cÃ³ 29.6 tá»«, trong Ä‘Ã³ 7 tá»« Ä‘Æ°á»£c coi lÃ  quan trá»ng
bá»Ÿi cáº£ hai ngÆ°á»i gáº¯n nhÃ£n. Trong táº¥t cáº£ bá»‘n loáº¡i tá»« khÃ³a, tá»« khÃ³a thuá»™c tÃ­nh (45.2%) Ä‘Æ°á»£c gáº¯n nhÃ£n
thÆ°á»ng xuyÃªn nháº¥t bá»Ÿi hai ngÆ°á»i gáº¯n nhÃ£n, tiáº¿p theo lÃ  toÃ¡n tá»­ (27.2%), Ä‘iá»u kiá»‡n (25%), vÃ  loáº¡i dá»¯ liá»‡u
(2.6%). HÃ¬nh 3 cho tháº¥y hai vÃ­ dá»¥ vá» mÃ´ táº£ nhiá»‡m vá»¥ cÃ³ nhÃ£n. Bá»‘n loáº¡i tá»« khÃ³a Ä‘Æ°á»£c
gáº¯n nhÃ£n báº±ng cÃ¡c mÃ u khÃ¡c nhau.
4 PHÆ¯Æ NG PHÃP LUáº¬N
Pháº§n nÃ y mÃ´ táº£ thiáº¿t káº¿ nghiÃªn cá»©u Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i nghiÃªn cá»©u Ä‘Æ°á»£c liá»‡t kÃª trong Pháº§n 1.
4.1 MÃ´ hÃ¬nh Táº¡o MÃ£
Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i chá»n sÃ¡u LLM vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau vÃ  hiá»‡u suáº¥t mÃ´ hÃ¬nh khÃ¡c nhau trÃªn cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£. Báº£ng 1 cho tháº¥y kÃ­ch thÆ°á»›c, sá»‘ lá»›p tá»± chÃº Ã½, sá»‘ Ä‘áº§u chÃº Ã½
trong má»—i lá»›p, vÃ  hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn bá»™ dá»¯ liá»‡u káº¿t há»£p cá»§a HumanEval vÃ  MBPP
theo Pass@1. ChÃºng tÃ´i mÃ´ táº£ má»—i mÃ´ hÃ¬nh dÆ°á»›i Ä‘Ã¢y.
â—InCoder-1.3B [ 32]lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ mÃ£ nguá»“n má»Ÿ tá»« Meta AI. NÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 159
GB mÃ£ Ä‘Æ°á»£c cáº¥p phÃ©p cÃ³ Ä‘iá»u kiá»‡n tá»« GitHub, GitLab, vÃ  Stack Overflow. So vá»›i
cÃ¡c LLM khÃ¡c, nÃ³ Ã¡p dá»¥ng má»¥c tiÃªu che máº·t náº¡ nhÃ¢n quáº£ má»›i, cho phÃ©p nÃ³ Ä‘iá»n vÃ o cÃ¡c khá»‘i mÃ£
dá»±a trÃªn ngá»¯ cáº£nh trÃ¡i vÃ  pháº£i tÃ¹y Ã½. ChÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c lá»›n nháº¥t
Ä‘Æ°á»£c Meta phÃ¡t hÃ nh, bao gá»“m 1.3B tham sá»‘ vÃ  24 lá»›p transformer.
â—PolyCoder-2.7B [ 91]lÃ  má»™t mÃ´ hÃ¬nh nguá»“n má»Ÿ tá»« CMU. NÃ³ dá»±a trÃªn kiáº¿n trÃºc GPT-2
vÃ  Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ trá»Ÿ thÃ nh Ä‘á»‘i tÃ¡c nguá»“n má»Ÿ cá»§a OpenAI Codex [ 27], vÃ¬ Codex khÃ´ng
Ä‘Æ°á»£c má»Ÿ nguá»“n. NÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 249GB mÃ£ vÃ  cÃ³ 2.7B tham sá»‘.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 8 ---
100:8 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
Báº£ng 1. MÃ´ hÃ¬nh táº¡o mÃ£ Ä‘Æ°á»£c bao gá»“m trong nghiÃªn cá»©u nÃ y.
MÃ´ hÃ¬nh Lá»›p Äáº§u Pass@1
InCoder-1.3B 24 32 15.20%
PolyCoder-2.7B 32 32 5.59%
CodeGen-2.7B 32 32 23.70%
CodeParrot-1.5B 48 25 3.58%
GPT-J-6B 28 16 11.62%
GPT-4 - - 67%
â—CodeGen-Mono-2.7B [ 62]lÃ  má»™t mÃ´ hÃ¬nh nguá»“n má»Ÿ tá»« Salesforce Research. NÃ³ tuÃ¢n theo
kiáº¿n trÃºc mÃ´ hÃ¬nh tá»± há»“i quy transformer tiÃªu chuáº©n vá»›i mÃ£ hÃ³a vá»‹ trÃ­ xoay.
â—CodeParrot-1.5B [ 1]lÃ  má»™t ná»— lá»±c nguá»“n má»Ÿ khÃ¡c Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh GPT-2 cho viá»‡c táº¡o mÃ£.
NÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 180GB Python Code vÃ  cÃ³ 1.5B tham sá»‘.
â—GPT-J-6B [ 87]lÃ  má»™t mÃ´ hÃ¬nh nguá»“n má»Ÿ tá»« EleutherAI. NÃ³ Ã¡p dá»¥ng kiáº¿n trÃºc transformer
tÆ°Æ¡ng tá»± nhÆ° GPT-3. NÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 825 GB dá»¯ liá»‡u vÄƒn báº£n, bao gá»“m 95GB mÃ£ tá»« GitHub.
â—GPT-4 [ 2]lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»‘i tÃ¢n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi OpenAI. VÃ¬ cáº¥u trÃºc bÃªn trong
cá»§a GPT-4 khÃ´ng Ä‘Æ°á»£c tiáº¿t lá»™ cÃ´ng khai, chÃºng tÃ´i khÃ´ng bao gá»“m sá»‘ lÆ°á»£ng lá»›p vÃ  Ä‘áº§u cá»§a
GPT-4 trong Báº£ng 1. ÄÆ°á»£c bÃ¡o cÃ¡o ráº±ng GPT-4 cÃ³ khoáº£ng 1.76 nghÃ¬n tá»· tham sá»‘ [ 3]. ChÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng
API ( gpt-4 ) Ä‘Æ°á»£c cung cáº¥p bá»Ÿi OpenAI Ä‘á»ƒ truy váº¥n GPT-4.
4.2 TÃ­nh toÃ¡n Sá»± chÃº Ã½ cá»§a MÃ´ hÃ¬nh
ChÃºng tÃ´i Ä‘Ã£ thá»­ nghiá»‡m vá»›i mÆ°á»i hai phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ tá»« ba danh má»¥c khÃ¡c nhau: sÃ¡u
phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½, bá»‘n phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient, vÃ  hai phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n.
4.2.1 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Tá»± chÃº Ã½. Cho ráº±ng LLM cÃ³ nhiá»u lá»›p chÃº Ã½, hiá»‡n táº¡i
khÃ´ng cÃ³ sá»± Ä‘á»“ng thuáº­n vá» cÃ¡ch Ä‘Ãºng Ä‘á»ƒ tá»•ng há»£p nhá»¯ng tá»± chÃº Ã½ Ä‘Ã³ Ä‘á»ƒ giáº£i thÃ­ch
LLM. Zeng et al. [ 97] cho tháº¥y ráº±ng lá»›p chÃº Ã½ Ä‘áº§u tiÃªn chá»‰ ra token nÃ o mÃ  mÃ´ hÃ¬nh
chÃº Ã½ Ä‘áº¿n, trong khi Wan et al. [ 85] cho tháº¥y ráº±ng cÃ¡c lá»›p chÃº Ã½ sÃ¢u hÆ¡n tá»‘t hÆ¡n trong viá»‡c náº¯m báº¯t phá»¥ thuá»™c táº§m xa
vÃ  cáº¥u trÃºc chÆ°Æ¡ng trÃ¬nh. Äá»ƒ thá»±c hiá»‡n phÃ¢n tÃ­ch toÃ n diá»‡n, chÃºng tÃ´i quyáº¿t Ä‘á»‹nh
thá»­ nghiá»‡m vá»›i ba cÃ i Ä‘áº·t: (1) chá»‰ sá»­ dá»¥ng lá»›p chÃº Ã½ Ä‘áº§u tiÃªn (kÃ½ hiá»‡u lÃ  first), (2) chá»‰
sá»­ dá»¥ng lá»›p chÃº Ã½ cuá»‘i cÃ¹ng (kÃ½ hiá»‡u lÃ  last), vÃ  (3) sá»­ dá»¥ng táº¥t cáº£ cÃ¡c lá»›p chÃº Ã½ (kÃ½ hiá»‡u lÃ  all).
Äá»ƒ tá»•ng há»£p tá»± chÃº Ã½ trÃªn cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau trong má»™t lá»›p, chÃºng tÃ´i tuÃ¢n theo
cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y [ 99] báº±ng cÃ¡ch tá»•ng há»£p cÃ¡c giÃ¡ trá»‹ chÃº Ã½ tá»« cÃ¡c Ä‘áº§u khÃ¡c nhau. Cuá»‘i cÃ¹ng, vÃ¬ LLM táº¡o mÃ£
theo cÃ¡ch tá»± há»“i quy, sá»± chÃº Ã½ cá»§a chÃºng thay Ä‘á»•i trong má»—i bÆ°á»›c khi chÃºng Ä‘á»c thÃªm token tá»«
Ä‘áº§u vÃ o vÃ  khi chÃºng táº¡o thÃªm mÃ£. ChÃºng tÃ´i tÃ² mÃ² vá» token Ä‘áº§u vÃ o nÃ o mÃ  mÃ´ hÃ¬nh
chÃº Ã½ cao khi chÃºng Ä‘á»c Ä‘áº§u vÃ o vÃ  token Ä‘áº§u vÃ o nÃ o mÃ  mÃ´ hÃ¬nh chÃº Ã½ cao khi
chÃºng táº¡o mÃ£. VÃ¬ váº­y chÃºng tÃ´i xem xÃ©t hai cÃ i Ä‘áº·t thá»­ nghiá»‡m: (1) tá»•ng há»£p Ä‘iá»ƒm chÃº Ã½
Ä‘Æ°á»£c gÃ¡n cho má»—i token trong quÃ¡ trÃ¬nh Ä‘á»c Ä‘áº§u vÃ o (kÃ½ hiá»‡u lÃ  READING ), vÃ  (2) tá»•ng há»£p
Ä‘iá»ƒm chÃº Ã½ Ä‘Æ°á»£c gÃ¡n cho má»—i token trong quÃ¡ trÃ¬nh táº¡o mÃ£ (kÃ½ hiá»‡u lÃ  CODING ).
Cho ba cÃ i Ä‘áº·t trong tá»•ng há»£p chÃº Ã½ theo lá»›p vÃ  hai cÃ i Ä‘áº·t trong tá»•ng há»£p chÃº Ã½ theo bÆ°á»›c,
chÃºng tÃ´i cÃ³ tá»•ng cá»™ng sÃ¡u cÃ i Ä‘áº·t thá»­ nghiá»‡m: READING_first ,CODING_first ,
READING_last ,CODING_last ,READING_all , vÃ  CODING_all .
4.2.2 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Gradient. ChÃºng tÃ´i xem xÃ©t hai phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ tÃ­nh toÃ¡n sá»± chÃº Ã½ mÃ´ hÃ¬nh dá»±a trÃªn gradient: (1) Saliency [78], vÃ  (2) InputÃ—Gradient [76]. PhÆ°Æ¡ng phÃ¡p saliency tÃ­nh toÃ¡n
sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch tÃ­nh toÃ¡n gradient mÃ´ hÃ¬nh liÃªn quan Ä‘áº¿n Ä‘áº§u vÃ o. Cho má»™t LLM â„±,
giáº£ sá»­ Ä‘áº§u vÃ o lÃ  ğ‘‹=(ï¸€ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘›âŒ‹ï¸€, trong Ä‘Ã³ ğ‘›lÃ  Ä‘á»™ dÃ i cá»§a Ä‘áº§u vÃ o. Sá»± chÃº Ã½ ğ‘ ğ‘–trÃªn ğ‘¥ğ‘–Ä‘Æ°á»£c
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 9 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:9
tÃ­nh toÃ¡n nhÆ° ğ‘ ğ‘–=ğœ•â„±{ï¸ƒğ‘‹}ï¸ƒ
ğœ•ğ‘¥ ğ‘–. KhÃ¡c vá»›i phÆ°Æ¡ng phÃ¡p saliency, InputÃ—Gradient cÃ²n nhÃ¢n
gradient vá»›i giÃ¡ trá»‹ embedding cá»§a Ä‘áº§u vÃ o. Sá»± chÃº Ã½ ğ‘ ğ‘–trÃªn ğ‘¥ğ‘–Ä‘Æ°á»£c tÃ­nh toÃ¡n nhÆ° ğ‘ ğ‘–=ğ‘¥ğ‘–â‹…ğœ•â„±{ï¸ƒğ‘‹}ï¸ƒ
ğœ•ğ‘¥ ğ‘–.
TÆ°Æ¡ng tá»± nhÆ° tá»± chÃº Ã½, gradient cÅ©ng thay Ä‘á»•i liÃªn tá»¥c táº¡i má»—i bÆ°á»›c táº¡o. Do Ä‘Ã³, chÃºng tÃ´i cÅ©ng
thá»­ nghiá»‡m vá»›i hai cÃ i Ä‘áº·t tá»•ng há»£p chÃº Ã½ theo bÆ°á»›c nhÆ° trong cÃ¡c phÆ°Æ¡ng phÃ¡p tá»± chÃº Ã½.
Sá»± káº¿t há»£p cá»§a hai phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n gradient vá»›i hai cÃ i Ä‘áº·t tá»•ng há»£p theo bÆ°á»›c
dáº«n Ä‘áº¿n bá»‘n phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradientâ€” InputÃ—Gradient_reading ,InputÃ—Gradient_coding ,
Saliency_reading , vÃ  Saliency_coding .
4.2.3 PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn Nhiá»…u loáº¡n. ChÃºng tÃ´i xem xÃ©t hai phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n khÃ¡c nhau: (1)
SHAP [57], che máº·t náº¡ Ä‘áº§u vÃ o thÃ´ng qua viá»‡c xÃ³a má»™t sá»‘ token, vÃ  (2) BERT Masking [90], che máº·t náº¡
Ä‘áº§u vÃ o thÃ´ng qua viá»‡c thay tháº¿ má»™t token báº±ng káº¿t quáº£ dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ máº·t náº¡ cá»§a nÃ³
tá»« BERT.
â—SHAP . ChÃºng tÃ´i sá»­ dá»¥ng thÆ° viá»‡n SHAP chÃ­nh thá»©c vá»›i sá»‘ láº§n nhiá»…u loáº¡n báº±ng 50 Ä‘á»ƒ tÃ­nh toÃ¡n
sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh (Ä‘iá»ƒm SHAP) trÃªn cÃ¡c token khÃ¡c nhau.1ChÃºng tÃ´i tá»•ng há»£p Ä‘iá»ƒm SHAP vá» dá»± Ä‘oÃ¡n
cÃ¡c token khÃ¡c nhau báº±ng cÃ¡ch tá»•ng há»£p chÃºng láº¡i.
â—BERT Masking . VÃ¬ mÃ£ tá»« bÃ i bÃ¡o BERT Masking gá»‘c [ 90] khÃ´ng cÃ³ sáºµn cÃ´ng khai,
chÃºng tÃ´i Ä‘Ã£ triá»ƒn khai láº¡i phÆ°Æ¡ng phÃ¡p nÃ y dá»±a trÃªn mÃ´ táº£ trong bÃ i bÃ¡o. Cá»¥ thá»ƒ, cho má»—i
token trong lá»i nháº¯c Ä‘áº§u vÃ o, phÆ°Æ¡ng phÃ¡p nÃ y che máº·t náº¡ nÃ³ vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh BERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tá»« HuggingFace Ä‘á»ƒ dá»± Ä‘oÃ¡n token cÃ³ kháº£ nÄƒng nháº¥t á»Ÿ vá»‹ trÃ­ bá»‹ che máº·t náº¡. Sau Ä‘Ã³, nÃ³ thay tháº¿ token gá»‘c
báº±ng token Ä‘Æ°á»£c dá»± Ä‘oÃ¡n vÃ  nháº¯c LLM Ä‘á»ƒ tÃ¡i táº¡o giáº£i phÃ¡p mÃ£. PhÆ°Æ¡ng phÃ¡p nÃ y sau Ä‘Ã³ tÃ­nh toÃ¡n
sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh trÃªn token nÃ y báº±ng cÃ¡ch tÃ­nh toÃ¡n Ä‘iá»ƒm BLEU [ 65]
giá»¯a giáº£i phÃ¡p mÃ£ gá»‘c vÃ  giáº£i phÃ¡p má»›i. ChÃºng tÃ´i láº·p qua táº¥t cáº£ cÃ¡c token trong
lá»i nháº¯c Ä‘áº§u vÃ o Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh trÃªn cÃ¡c token khÃ¡c nhau.
4.3 Äo lÆ°á»ng CÄƒn chá»‰nh ChÃº Ã½
ChÃºng tÃ´i Ä‘Ã£ Ä‘o lÆ°á»ng sá»± cÄƒn chá»‰nh chÃº Ã½ giá»¯a mÃ´ hÃ¬nh vÃ  láº­p trÃ¬nh viÃªn con ngÆ°á»i báº±ng cÃ¡ch sá»­ dá»¥ng hai
sá»‘ liá»‡u máº¡nh máº½â€”sá»‘ liá»‡u chá»“ng láº¥p token cá»§a San Martino [ 22] vÃ  alpha cá»§a Krippendorff [ 49]. ChÃºng tÃ´i cÅ©ng
thá»­ nghiá»‡m vá»›i hai sá»‘ liá»‡u Ä‘Æ¡n giáº£n, kappa cá»§a Cohen vÃ  tá»· lá»‡ bao phá»§ tá»« khÃ³a, vÃ  thu Ä‘Æ°á»£c
káº¿t quáº£ tÆ°Æ¡ng tá»±. Do giá»›i háº¡n trang, chÃºng tÃ´i chá»‰ bÃ¡o cÃ¡o káº¿t quáº£ cá»§a hai sá»‘ liá»‡u Ä‘áº§u tiÃªn trong
bÃ i bÃ¡o nÃ y. Káº¿t quáº£ cá»§a cÃ¡c sá»‘ liá»‡u khÃ¡c Ä‘Æ°á»£c bao gá»“m trong kho GitHub [47].
4.3.1 Sá»‘ liá»‡u Chá»“ng láº¥p Token cá»§a San Martino [ 22]tÃ­nh toÃ¡n sá»± chá»“ng láº¥p giá»¯a hai táº­p há»£p
token theo Ä‘á»™ chÃ­nh xÃ¡c, thu há»“i, vÃ  Ä‘iá»ƒm F-1. NÃ³ ban Ä‘áº§u Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c nhiá»‡m vá»¥ gáº¯n nhÃ£n chuá»—i
trong NLP [ 22] vÃ  gáº§n Ä‘Ã¢y Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c nghiÃªn cá»©u SE nhÆ° má»™t sá»‘ liá»‡u máº¡nh máº½ cho viá»‡c phÃ¡t hiá»‡n Ä‘á»™c tÃ­nh
trong bÃ¬nh luáº­n Ä‘Ã¡nh giÃ¡ mÃ£ [ 72]. Cá»¥ thá»ƒ, nÃ³ gÃ¡n Ä‘iá»ƒm má»™t pháº§n cho sá»± chá»“ng láº¥p má»™t pháº§n
giá»¯a hai táº­p há»£p token, Ä‘iá»u nÃ y lÃ m cho nÃ³ trá»Ÿ thÃ nh lá»±a chá»n phÃ¹ há»£p cho nhiá»‡m vá»¥ cá»§a chÃºng tÃ´i. Do Ä‘Ã³, chÃºng tÃ´i tuÃ¢n theo [ 22] Ä‘á»ƒ
so sÃ¡nh cÃ¡c tá»« ná»•i báº­t Ä‘Æ°á»£c chá»n bá»Ÿi con ngÆ°á»i vÃ  mÃ´ hÃ¬nh. Äá»‘i vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i, má»™t token Ä‘Æ°á»£c coi lÃ 
Ä‘Æ°á»£c chÃº Ã½ cao náº¿u nÃ³ Ä‘Æ°á»£c gáº¯n nhÃ£n lÃ  má»™t tá»« quan trá»ng bá»Ÿi láº­p trÃ¬nh viÃªn con ngÆ°á»i, nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong
Pháº§n 3. Äá»‘i vá»›i sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh, vÃ¬ cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ trong Pháº§n 4.2 tÃ­nh toÃ¡n má»™t
Ä‘iá»ƒm liÃªn tá»¥c cho má»—i token, khÃ³ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh ngÆ°á»¡ng phá»• quÃ¡t Ä‘á»ƒ quyáº¿t Ä‘á»‹nh token nÃ o
Ä‘Æ°á»£c mÃ´ hÃ¬nh chÃº Ã½ cao. Do Ä‘Ã³, chÃºng tÃ´i xáº¿p háº¡ng cÃ¡c token trong mÃ´ táº£ nhiá»‡m vá»¥ láº­p trÃ¬nh dá»±a
trÃªn Ä‘iá»ƒm chÃº Ã½ cá»§a chÃºng vÃ  chá»n ğ¾token hÃ ng Ä‘áº§u nhÆ° cÃ¡c token Ä‘Æ°á»£c mÃ´ hÃ¬nh chÃº Ã½ cao.
VÃ¬ trung bÃ¬nh cÃ¡c tá»« quan trá»ng Ä‘Æ°á»£c con ngÆ°á»i gáº¯n nhÃ£n lÃ  7 má»—i mÃ´ táº£ nhiá»‡m vá»¥, chÃºng tÃ´i thá»­ nghiá»‡m vá»›i
ğ¾=5,10,20trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i. Cho má»™t táº­p há»£p token Ä‘Æ°á»£c láº­p trÃ¬nh viÃªn con ngÆ°á»i chÃº Ã½ cao vÃ  má»™t táº­p há»£p
token Ä‘Æ°á»£c mÃ´ hÃ¬nh chÃº Ã½ cao, chÃºng tÃ´i tuÃ¢n theo cÃ¡c phÆ°Æ¡ng trÃ¬nh trong [ 72] Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘á»™ chÃ­nh xÃ¡c, thu há»“i,
vÃ  Ä‘iá»ƒm F-1. ChÃºng tÃ´i bÃ¡o cÃ¡o cáº£ ba Ä‘iá»ƒm trong Báº£ng 2.
1https://shap.readthedocs.io/en/latest/
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 10 ---
100:10 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
HÃ¬nh 4. Ãnh xáº¡ tá»« NL sang sub-token LLM
4.3.2 Alpha cá»§a Krippendorff [ 48]lÃ  má»™t thÆ°á»›c Ä‘o thá»‘ng kÃª máº¡nh máº½ cho sá»± Ä‘á»“ng thuáº­n giá»¯a ngÆ°á»i gáº¯n nhÃ£n, trong Ä‘Ã³
ğ›¼=1biá»ƒu thá»‹ sá»± Ä‘á»“ng thuáº­n hoÃ n háº£o, ğ›¼=0biá»ƒu thá»‹ khÃ´ng cÃ³ sá»± Ä‘á»“ng thuáº­n, vÃ  ğ›¼=âˆ’1biá»ƒu thá»‹ sá»±
báº¥t Ä‘á»“ng hoÃ n toÃ n. NÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘o lÆ°á»ng má»©c Ä‘á»™ Ä‘á»“ng thuáº­n giá»¯a láº­p trÃ¬nh viÃªn con ngÆ°á»i vÃ 
LLM vá» cÃ¡c tá»« quan trá»ng trong mÃ´ táº£ nhiá»‡m vá»¥ láº­p trÃ¬nh. So vá»›i cÃ¡c sá»‘ liá»‡u Ä‘á»“ng thuáº­n giá»¯a ngÆ°á»i Ä‘Ã¡nh giÃ¡ khÃ¡c
nhÆ° kappa cá»§a Cohen [ 21], alpha cá»§a Krippendorff máº¡nh máº½ hÆ¡n Ä‘á»‘i vá»›i sá»‘ lÆ°á»£ng ngÆ°á»i mÃ£ hÃ³a, dá»¯ liá»‡u thiáº¿u, vÃ  kÃ­ch thÆ°á»›c máº«u. Äá»ƒ tÃ­nh toÃ¡n alpha cá»§a Krippendorff, nhÃ£n cá»§a con ngÆ°á»i vÃ  mÃ´ hÃ¬nh
pháº£i Ä‘Æ°á»£c lÆ°u trá»¯ trong cÃ¡c vector cÃ³ cÃ¹ng Ä‘á»™ dÃ i. Äiá»u nÃ y khÃ³ khÄƒn vÃ¬ LLM thá»±c hiá»‡n token hÃ³a tá»« phá»¥
Ä‘á»ƒ xá»­ lÃ½ cÃ¡c tá»« ngoÃ i tá»« vá»±ng vá»›i kÃ­ch thÆ°á»›c tá»« vá»±ng cÃ³ thá»ƒ quáº£n lÃ½. VÃ­ dá»¥,
trong HÃ¬nh 4, tá»« "separate" Ä‘Æ°á»£c token hÃ³a thÃ nh hai token: "separ" vÃ  "ate" thÃ´ng qua mÃ£ hÃ³a cáº·p byte (BPE). Trong quÃ¡ trÃ¬nh suy luáº­n, má»™t LLM sáº½ tÃ­nh toÃ¡n Ä‘iá»ƒm chÃº Ã½ riÃªng biá»‡t cho hai
subtoken nÃ y, máº·c dÃ¹ chÃºng tá»« cÃ¹ng má»™t tá»« tiáº¿ng Anh.
Äá»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c nÃ y, chÃºng tÃ´i Ã¡nh xáº¡ tá»« NL trá»Ÿ láº¡i token LLM. Náº¿u mÃ´ hÃ¬nh token hÃ³a má»™t
tá»« ngÃ´n ngá»¯ tá»± nhiÃªn thÃ nh nhiá»u token, táº¥t cáº£ sub-token sáº½ Ä‘Æ°á»£c coi lÃ  Ä‘Æ°á»£c chá»n bá»Ÿi ngÆ°á»i gáº¯n nhÃ£n con ngÆ°á»i.
Sá»­ dá»¥ng cÃ¹ng vÃ­ dá»¥ tá»« HÃ¬nh 4, náº¿u tá»« ngÃ´n ngá»¯ tá»± nhiÃªn "separate" Ä‘Æ°á»£c chá»n bá»Ÿi
ngÆ°á»i gáº¯n nhÃ£n con ngÆ°á»i, cáº£ hai sub-token "separ" vÃ  "ate" sáº½ Ä‘Æ°á»£c coi lÃ  Ä‘Æ°á»£c chá»n vÃ  Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi 1
trong cÃ¡c vector Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n alpha cá»§a Krippendorff.
4.4 Thiáº¿t káº¿ NghiÃªn cá»©u NgÆ°á»i dÃ¹ng
Äá»ƒ tráº£ lá»i RQ4, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh nghiÃªn cá»©u ngÆ°á»i dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau.2
ChÃºng tÃ´i Ä‘Ã£ tuyá»ƒn 22 sinh viÃªn (18 nam vÃ  4 ná»¯) thÃ´ng qua danh sÃ¡ch email cá»§a khoa trong má»™t
khoa CS. Theo nghiÃªn cá»©u ngÆ°á»i dÃ¹ng cá»§a chÃºng tÃ´i, táº¥t cáº£ ngÆ°á»i tham gia cÃ³ trung bÃ¬nh 5.62 nÄƒm
kinh nghiá»‡m láº­p trÃ¬nh. Táº¥t cáº£ ngÆ°á»i tham gia Ä‘á»u cÃ³ hiá»ƒu biáº¿t cÆ¡ báº£n vá» sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh trong
há»c mÃ¡y.
ChÃºng tÃ´i Ä‘Ã£ chá»n ngáº«u nhiÃªn 8 mÃ´ táº£ nhiá»‡m vá»¥ tá»« bá»™ dá»¯ liá»‡u cá»§a chÃºng tÃ´i. Äá»‘i vá»›i má»—i nhiá»‡m vá»¥, chÃºng tÃ´i Ä‘Ã£ táº­n dá»¥ng CodeGen-2.7B, mÃ´ hÃ¬nh nguá»“n má»Ÿ cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn HumanEval trong thá»­ nghiá»‡m cá»§a chÃºng tÃ´i, Ä‘á»ƒ tÃ­nh toÃ¡n
sá»± chÃº Ã½ mÃ´ hÃ¬nh cá»§a nÃ³. ChÃºng tÃ´i khÃ´ng xem xÃ©t GPT-4 trong nghiÃªn cá»©u ngÆ°á»i dÃ¹ng nÃ y, vÃ¬ nÃ³ lÃ  mÃ£ nguá»“n Ä‘Ã³ng vÃ  chÃºng tÃ´i khÃ´ng thá»ƒ
tÃ­nh toÃ¡n sá»± chÃº Ã½ cá»§a nÃ³ báº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½ vÃ  phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient. ChÃºng tÃ´i Ä‘Ã£ chá»n
má»™t phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ tá»« má»—i danh má»¥c ( dá»±a trÃªn tá»± chÃº Ã½ ,dá»±a trÃªn nhiá»…u loáº¡n , vÃ 
dá»±a trÃªn gradient ):ğ¶ğ‘‚ğ·ğ¼ğ‘ğº _ğ‘™ğ‘ğ‘ ğ‘¡,ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡Ã—ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ _ğ‘ğ‘œğ‘‘ğ‘–ğ‘›ğ‘” , vÃ  SHAP .
Trong má»—i nghiÃªn cá»©u ngÆ°á»i dÃ¹ng, ngÆ°á»i tham gia Ä‘áº§u tiÃªn Ä‘á»c mÃ´ táº£ nhiá»‡m vá»¥ Ä‘á»ƒ hiá»ƒu nhiá»‡m vá»¥ láº­p trÃ¬nh
vÃ  sau Ä‘Ã³ Ä‘á»c mÃ£ Ä‘Æ°á»£c táº¡o bá»Ÿi CodeGen. Äá»‘i vá»›i má»—i phÆ°Æ¡ng phÃ¡p chÃº Ã½, chÃºng tÃ´i hiá»ƒn thá»‹ má»™t
phiÃªn báº£n Ä‘Æ°á»£c lÃ m ná»•i báº­t cá»§a mÃ´ táº£ nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± nhÆ° HÃ¬nh 3, trong Ä‘Ã³ 10 token Ä‘Æ°á»£c chÃº Ã½ hÃ ng Ä‘áº§u Ä‘Æ°á»£c lÃ m ná»•i báº­t
dá»±a trÃªn Ä‘iá»ƒm chÃº Ã½ Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi phÆ°Æ¡ng phÃ¡p nÃ y. ChÃºng tÃ´i chá»n hiá»ƒn thá»‹ 10 tá»« khÃ³a quan trá»ng hÃ ng Ä‘áº§u
vÃ¬ nÃ³ gáº§n vá»›i sá»‘ lÆ°á»£ng tá»« quan trá»ng trung bÃ¬nh (7) Ä‘Æ°á»£c gáº¯n nhÃ£n trong bá»™ dá»¯ liá»‡u cá»§a chÃºng tÃ´i.
2Báº£ng cÃ¢u há»i nghiÃªn cá»©u ngÆ°á»i dÃ¹ng vÃ  pháº£n há»“i cá»§a ngÆ°á»i tham gia cÃ³ sáºµn trong kho GitHub cá»§a chÃºng tÃ´i:
https://github.com/BonanKou/Attention-Alignment-Empirical-Study.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 11 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:11
Báº£ng 2. CÄƒn chá»‰nh chÃº Ã½ giá»¯a con ngÆ°á»i vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau theo Äá»™ chÃ­nh xÃ¡c cá»§a San Martino
(P), Thu há»“i (R), Ä‘iá»ƒm F1 (F1), vÃ  Ä‘iá»ƒm alpha cá»§a Krippendorff (KA). Trong sá»‘ 12 phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau
, phÆ°Æ¡ng phÃ¡p táº¡o ra káº¿t quáº£ phÃ¹ há»£p nháº¥t cho má»—i mÃ´ hÃ¬nh dÆ°á»›i má»—i cÃ i Ä‘áº·t ğ¾trong
má»—i sá»‘ liá»‡u Ä‘Æ°á»£c lÃ m ná»•i báº­t báº±ng mÃ u vÃ ng . LÆ°u Ã½ ráº±ng chÃºng tÃ´i chá»‰ thá»­ nghiá»‡m vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n trÃªn
GPT-4, vÃ¬ GPT-4 khÃ´ng cung cáº¥p quyá»n truy cáº­p vÃ o cÃ¡c lá»›p tá»± chÃº Ã½ vÃ  gradient cá»§a nÃ³.
(a) PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n.
BERT_masking SHAP
Top 5 Top 10 Top 20 Top 5 Top 10 Top 20 PhÆ°Æ¡ng phÃ¡p
P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA
Incoder 48.8% 40.6% 44.3% 0.25 40.8% 67.4% 50.8% 0.25 36.6% 90.3% 52.1% 0.12 33.9% 28.4% 30.9% 0.11 33.4% 55.9% 41.8% 0.18 32.8% 86.7% 47.6% 0.15
CodeGen 51.8% 43.2% 47.1% 0.29 41.8% 68.9% 52.0% 0.27 36.8% 90.7% 52.4% 0.13 33.0% 27.5% 30.0% 0.10 32.9% 54.9% 41.1% 0.17 33.0% 87.1% 47.9% 0.16
CodeParrot 51.5% 43.0% 46.9% 0.29 42.1% 69.2% 52.3% 0.28 36.6% 90.2% 52.0% 0.12 33.2% 27.7% 30.2% 0.10 32.7% 54.7% 40.9% 0.17 33.3% 87.6% 48.3% 0.17
GPT-J-6B 49.9% 41.3% 45.2% 0.26 41.0% 67.4% 51.0% 0.26 36.6% 90.3% 52.1% 0.12 33.3% 27.8% 30.3% 0.10 33.3% 55.7% 41.7% 0.18 33.1% 87.1% 47.9% 0.16
PolyCoder 49.3% 41.3% 45.0% 0.26 40.9% 67.7% 51.0% 0.26 36.6% 90.4% 52.1% 0.12 34.4% 28.9% 31.4% 0.12 33.5% 56.2% 42.0% 0.19 33.2% 87.2% 48.1% 0.16
GPT-4 32.7% 27.6% 30.0% 0.04 36.7% 61.7% 46.0% 0.17 36.3% 89.4% 51.7% 0.11 34.7% 29.2% 31.7% 0.12 34.2% 57.4% 42.9% 0.20 33.1% 87.0% 47.9% 0.16
(b) PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient
InputÃ—Gradient_reading InputÃ—Gradient_coding
Top 5 Top 10 Top 20 Top 5 Top 10 Top 20 PhÆ°Æ¡ng phÃ¡p
P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA
Incoder 33.7% 25.9% 29.3% 0.13 36.9% 46.8% 41.2% 0.32 35.6% 52.7% 42.5% 0.28 34.6% 27.5% 30.7% 0.14 37.2% 47.1% 41.6% 0.33 35.6% 52.8% 42.5% 0.28
CodeGen 44.8% 34.2% 38.8% 0.22 39.4% 59.8% 47.5% 0.28 35.2% 87.3% 50.1% 0.22 46.0% 34.7% 39.5% 0.23 41.0% 61.5% 49.2% 0.31 36.2% 88.5% 51.4% 0.25
CodeParrot 55.0% 43.2% 48.4% 0.33 45.8% 71.2% 55.8% 0.40 35.1% 87.5% 50.1% 0.22 57.9% 44.9% 50.6% 0.37 46.8% 71.8% 56.7% 0.42 36.0% 88.7% 51.2% 0.24
GPT-J-6B 40.9% 30.7% 35.1% 0.17 37.4% 56.7% 45.1% 0.24 34.5% 85.8% 49.2% 0.20 42.7% 32.5% 36.9% 0.19 39.4% 59.6% 47.4% 0.28 35.5% 87.2% 50.5% 0.22
PolyCoder 43.3% 33.8% 38.0% 0.20 38.3% 59.9% 46.7% 0.26 34.4% 86.2% 49.1% 0.20 38.1% 30.1% 33.6% 0.14 36.4% 57.3% 44.5% 0.23 35.2% 87.4% 50.2% 0.22
Saliency_reading Saliency_coding
Top 5 Top 10 Top 20 Top 5 Top 10 Top 20 PhÆ°Æ¡ng phÃ¡p
P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA
Incoder 36.6% 29.3% 32.6% 0.06 41.9% 67.3% 51.6% 0.29 41.4% 88.6% 56.4% 0.30 39.4% 31.5% 35.0% 0.09 42.8% 67.3% 52.3% 0.29 41.3% 88.5% 56.4% 0.30
CodeGen 46.1% 34.9% 39.8% 0.23 39.3% 59.6% 47.4% 0.28 35.3% 87.4% 50.3% 0.23 47.3% 35.4% 40.5% 0.24 40.7% 60.8% 48.8% 0.30 36.2% 88.5% 51.4% 0.25
CodeParrot 53.8% 42.1% 47.2% 0.31 44.6% 69.3% 54.3% 0.38 34.8% 86.8% 49.7% 0.21 56.3% 43.7% 49.2% 0.35 45.3% 69.6% 54.9% 0.39 35.7% 87.9% 50.8% 0.23
GPT-J-6B 41.2% 30.6% 35.1% 0.17 35.5% 53.1% 42.6% 0.20 34.2% 85.1% 48.8% 0.19 41.2% 30.5% 35.0% 0.17 37.5% 56.3% 45.0% 0.24 35.5% 87.2% 50.5% 0.22
PolyCoder 41.6% 32.5% 36.5% 0.18 36.5% 56.9% 44.5% 0.23 34.2% 85.9% 49.0% 0.20 36.7% 29.0% 32.4% 0.13 35.3% 55.6% 43.2% 0.21 35.0% 87.0% 50.0% 0.22
(c) PhÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½.
READING_first CODING_first
Top 5 Top 10 Top 20 Top 5 Top 10 Top 20 PhÆ°Æ¡ng phÃ¡p
P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA
Incoder 44.9% 38.5% 41.5% 0.17 46.7% 75.9% 57.8% 0.39 41.7% 90.2% 57.0% 0.31 45.3% 38.0% 41.4% 0.18 45.1% 73.0% 55.7% 0.35 41.4% 89.5% 56.7% 0.30
CodeGen 9.8% 8.3% 9.0% -0.17 26.0% 41.6% 32.0% 0.05 34.0% 84.1% 48.4% 0.19 9.5% 8.5% 9.0% -0.17 25.6% 40.4% 31.3% 0.04 33.8% 83.8% 48.2% 0.19
CodeParrot 33.8% 27.5% 30.3% 0.10 37.9% 59.9% 46.4% 0.26 34.5% 86.2% 49.3% 0.20 44.0% 35.7% 39.4% 0.22 42.2% 67.2% 51.9% 0.35 34.6% 86.6% 49.4% 0.21
GPT-J-6B 6.5% 5.5% 6.0% -0.21 23.0% 33.2% 27.2% -0.03 33.0% 81.3% 46.9% 0.16 7.4% 6.2% 6.7% -0.20 22.2% 33.0% 26.5% -0.04 33.2% 81.9% 47.3% 0.17
PolyCoder 41.4% 32.5% 36.4% 0.19 41.7% 63.7% 50.4% 0.33 35.8% 87.9% 50.8% 0.23 43.8% 33.8% 38.1% 0.20 42.5% 64.8% 51.3% 0.34 35.8% 87.8% 50.9% 0.23
READING_last CODING_last
Top 5 Top 10 Top 20 Top 5 Top 10 Top 20 PhÆ°Æ¡ng phÃ¡p
P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA
Incoder 40.2% 31.9% 35.6% 0.12 41.2% 64.8% 50.4% 0.27 41.0% 88.8% 56.1% 0.29 41.9% 33.9% 37.5% 0.13 43.2% 69.6% 53.3% 0.31 41.4% 89.3% 56.6% 0.30
CodeGen 33.8% 24.8% 28.6% 0.08 39.5% 59.1% 47.4% 0.29 34.7% 85.2% 49.3% 0.21 31.0% 24.4% 27.3% 0.07 37.4% 57.1% 45.2% 0.25 35.1% 86.4% 49.9% 0.22
CodeParrot 60.1% 45.4% 51.7% 0.38 50.4% 76.4% 60.8% 0.48 37.0% 90.2% 52.4% 0.26 56.4% 43.2% 48.9% 0.35 48.5% 73.6% 58.4% 0.45 36.7% 89.7% 52.1% 0.26
GPT-J-6B 8.8% 7.0% 7.8% -0.19 26.6% 42.7% 32.8% 0.05 33.4% 82.7% 47.5% 0.17 13.8% 10.6% 12.0% -0.14 27.4% 41.8% 33.1% 0.06 33.6% 83.2% 47.9% 0.18
PolyCoder 23.1% 18.1% 20.3% -0.03 34.3% 53.8% 41.9% 0.19 34.7% 86.4% 49.5% 0.21 32.8% 25.7% 28.9% 0.09 37.9% 58.8% 46.1% 0.26 35.4% 87.0% 50.3% 0.22
READING_all CODING_all
Top 5 Top 10 Top 20 Top 5 Top 10 Top 20 PhÆ°Æ¡ng phÃ¡p
P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA P R F1 KA
Incoder 34.8% 32.5% 33.6% 0.08 39.0% 66.7% 49.2% 0.25 40.4% 88.2% 55.4% 0.28 40.6% 39.2% 39.9% 0.16 42.0% 71.8% 53.0% 0.31 41.2% 89.8% 56.5% 0.30
CodeGen 19.6% 14.7% 16.8% -0.09 24.9% 37.0% 29.7% -0.00 34.0% 83.9% 48.4% 0.19 28.4% 23.1% 25.4% 0.03 32.1% 50.7% 39.3% 0.15 34.5% 85.8% 49.2% 0.21
CodeParrot 41.8% 31.8% 36.1% 0.16 39.6% 61.3% 48.1% 0.28 35.1% 87.1% 50.1% 0.22 50.1% 39.7% 44.3% 0.28 43.4% 68.0% 53.0% 0.36 35.2% 87.3% 50.1% 0.22
GPT-J-6B 12.6% 10.2% 11.3% -0.15 22.9% 35.0% 27.7% -0.04 33.8% 83.6% 48.1% 0.18 30.6% 24.4% 27.2% 0.05 32.7% 51.2% 39.9% 0.15 34.6% 85.6% 49.2% 0.21
PolyCoder 28.7% 21.3% 24.4% 0.01 36.3% 56.3% 44.1% 0.21 34.8% 86.5% 49.6% 0.21 33.4% 25.7% 29.0% 0.08 38.4% 59.7% 46.8% 0.26 35.1% 86.9% 50.0% 0.22
NgÆ°á»i tham gia sau Ä‘Ã³ Ä‘Æ°á»£c yÃªu cáº§u Ä‘Ã¡nh giÃ¡ má»—i phÆ°Æ¡ng phÃ¡p chÃº Ã½ báº±ng cÃ¡ch chá»‰ ra sá»± Ä‘á»“ng Ã½ cá»§a há» vá»›i
ba cÃ¢u sau trÃªn thang Ä‘iá»ƒm Likert 7 Ä‘iá»ƒm (1â€”hoÃ n toÃ n khÃ´ng Ä‘á»“ng Ã½, 7â€”hoÃ n toÃ n Ä‘á»“ng Ã½).
Q1Tá»« khÃ³a Ä‘Æ°á»£c mÃ´ hÃ¬nh chÃº Ã½ phÃ¹ há»£p vá»›i sá»± chÃº Ã½ cá»§a tÃ´i khi Ä‘á»c mÃ´ táº£ nhiá»‡m vá»¥ ngÃ´n ngá»¯ tá»± nhiÃªn.
Q2Sá»± chÃº Ã½ nÃ y giáº£i thÃ­ch táº¡i sao mÃ´ hÃ¬nh thÃ nh cÃ´ng hoáº·c tháº¥t báº¡i trong viá»‡c táº¡o ra mÃ£ chÃ­nh xÃ¡c.
Q3TÃ´i muá»‘n tháº¥y sá»± chÃº Ã½ nÃ y khi lÃ m viá»‡c vá»›i cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ trong cuá»™c sá»‘ng thá»±c.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 12 ---
100:12 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
Thá»© tá»± cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau Ä‘Æ°á»£c ngáº«u nhiÃªn hÃ³a Ä‘á»ƒ giáº£m thiá»ƒu hiá»‡u á»©ng há»c.
ChÃºng tÃ´i cÅ©ng khÃ´ng tiáº¿t lá»™ tÃªn cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ Ä‘á»ƒ giáº£m thiÃªn vá»‹. á» cuá»‘i
nghiÃªn cá»©u ngÆ°á»i dÃ¹ng, ngÆ°á»i tham gia tráº£ lá»i ba cÃ¢u há»i má»Ÿ vá» cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau
vÃ  thiáº¿t káº¿ nghiÃªn cá»©u ngÆ°á»i dÃ¹ng. ChÃºng tÃ´i há»i nhá»¯ng cÃ¢u há»i má»Ÿ nÃ y Ä‘á»ƒ nghiÃªn cá»©u má»‘i tÆ°Æ¡ng quan
giá»¯a kháº£ nÄƒng giáº£i thÃ­ch cá»§a mÃ´ hÃ¬nh vÃ  niá»m tin cá»§a ngÆ°á»i dÃ¹ng. Nhá»¯ng cÃ¢u há»i nÃ y bao gá»“m:
Q4Báº¡n cÃ³ quan tÃ¢m biáº¿t LLM táº¡o mÃ£ nhÆ° tháº¿ nÃ o khÃ´ng?
Q5Báº¡n muá»‘n tÃ¬m hiá»ƒu gÃ¬ vá» quÃ¡ trÃ¬nh táº¡o mÃ£ bÃªn trong trong LLM?
Q6Báº¡n cÃ³ tin tÆ°á»Ÿng LLM nÃ y khÃ´ng? Báº¡n cáº§n biáº¿t gÃ¬ Ä‘á»ƒ cáº£i thiá»‡n niá»m tin vÃ o LLM?
5 Káº¾T QUáº¢
5.1 RQ1: Má»©c Ä‘á»™ CÄƒn chá»‰nh Sá»± chÃº Ã½ cá»§a MÃ´ hÃ¬nh vá»›i Sá»± chÃº Ã½ cá»§a Con ngÆ°á»i Ä‘áº¿n Ä‘Ã¢u?
Äá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y, chÃºng tÃ´i thu tháº­p ğ¾tá»« khÃ³a hÃ ng Ä‘áº§u mÃ  sÃ¡u LLM chÃº Ã½ Ä‘áº¿n vÃ  so sÃ¡nh
chÃºng vá»›i cÃ¡c tá»« khÃ³a Ä‘Æ°á»£c gáº¯n nhÃ£n lÃ  quan trá»ng trong bá»™ dá»¯ liá»‡u chÃº Ã½ láº­p trÃ¬nh viÃªn. NhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong
Pháº§n 4.3, chÃºng tÃ´i sá»­ dá»¥ng sá»‘ liá»‡u chá»“ng láº¥p token cá»§a San Martino [ 22] vÃ  alpha cá»§a Krippendorff [ 49] Ä‘á»ƒ
Ä‘o lÆ°á»ng sá»± cÄƒn chá»‰nh chÃº Ã½ giá»¯a LLM vÃ  láº­p trÃ¬nh viÃªn con ngÆ°á»i. Khi cháº¡y cÃ¡c mÃ´ hÃ¬nh
mÃ  Ä‘á»™ dÃ i Ä‘áº§u ra tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº·t, chÃºng tÃ´i Ä‘áº·t giá»›i háº¡n token báº±ng sá»‘ token cá»§a
sá»± tháº­t cÆ¡ báº£n cá»™ng 20 Ä‘á»ƒ chá»‹u Ä‘á»±ng sá»± dÆ° thá»«a vá»«a pháº£i.
Báº£ng 2a, Báº£ng 2b, vÃ  Báº£ng 2c trÃ¬nh bÃ y káº¿t quáº£ cÄƒn chá»‰nh chÃº Ã½ khi tÃ­nh toÃ¡n Ä‘iá»ƒm chÃº Ã½
vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n ,dá»±a trÃªn gradient , vÃ  dá»±a trÃªn tá»± chÃº Ã½ , tÆ°Æ¡ng á»©ng.
VÃ¬ GPT-4 khÃ´ng tiáº¿t lá»™ tráº¡ng thÃ¡i bÃªn trong cá»§a nÃ³ trong thá»i gian cháº¡y, chá»‰ cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n
má»›i cÃ³ thá»ƒ Ã¡p dá»¥ng. Do Ä‘Ã³, pháº§n nÃ y tháº£o luáº­n vá» káº¿t quáº£ cá»§a phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n tá»‘t nháº¥t,
BERT_masking . Pháº§n 5.3 so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau chi tiáº¿t.
Vá»›i phÆ°Æ¡ng phÃ¡p BERT_masking , tá»« ğ¾=5Ä‘áº¿n ğ¾=10, Ä‘iá»ƒm F1 cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh tÄƒng vÃ¬
khi ngÃ y cÃ ng nhiá»u token Ä‘Æ°á»£c chá»n bá»Ÿi cÃ¡c mÃ´ hÃ¬nh, chÃºng tÃ´i quan sÃ¡t tá»· lá»‡ thu há»“i ráº¥t cao (khoáº£ng 90%),
Ä‘iá»u nÃ y bÃ¹ Ä‘áº¯p cho Ä‘á»™ chÃ­nh xÃ¡c giáº£m. Tuy nhiÃªn, tá»« ğ¾=10Ä‘áº¿n ğ¾=20, Ä‘iá»ƒm F1
váº«n á»•n Ä‘á»‹nh do sá»± giáº£m nhanh chÃ³ng trong Ä‘iá»ƒm Ä‘á»™ chÃ­nh xÃ¡c. Máº·t khÃ¡c, Ä‘iá»ƒm alpha cá»§a Krippendorff
váº«n á»•n Ä‘á»‹nh tá»« ğ¾=5Ä‘áº¿n ğ¾=10(ngoáº¡i trá»« GPT-4) nhÆ°ng giáº£m nhanh chÃ³ng tá»« ğ¾=10Ä‘áº¿n
ğ¾=20.
NhÃ¬n chung, Ä‘á»‘i vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh vÃ  táº¥t cáº£ giÃ¡ trá»‹ ğ¾, alpha cá»§a Krippendorff khÃ´ng thay Ä‘á»•i nhiá»u vÃ 
váº«n dÆ°á»›i 0.3, vÃ  Ä‘iá»ƒm F1 váº«n dÆ°á»›i 0.6, cho tháº¥y Ã­t sá»± Ä‘á»“ng thuáº­n giá»¯a sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh
vÃ  sá»± chÃº Ã½ cá»§a con ngÆ°á»i [ 58]. Trong sá»‘ cÃ¡c mÃ´ hÃ¬nh nÃ y, GPT-4 cho tháº¥y sá»± chá»“ng láº¥p chÃº Ã½ tháº¥p nháº¥t
vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i vá» cáº£ hai sá»‘ liá»‡u. Má»™t giáº£i thÃ­ch cÃ³ thá»ƒ lÃ  cÃ¡c
mÃ´ hÃ¬nh siÃªu lá»›n nhÆ° GPT-4 Ä‘Ã£ phÃ¡t triá»ƒn má»™t chiáº¿n lÆ°á»£c lÃ½ luáº­n khÃ¡c vá»›i láº­p trÃ¬nh viÃªn con ngÆ°á»i. Nhá»¯ng káº¿t quáº£ nÃ y cho tháº¥y sá»± khÃ´ng cÄƒn chá»‰nh chÃº Ã½ nháº¥t quÃ¡n giá»¯a LLM vÃ  láº­p trÃ¬nh viÃªn
khi táº¡o mÃ£.
PhÃ¡t hiá»‡n 1
CÃ³ sá»± khÃ´ng cÄƒn chá»‰nh nháº¥t quÃ¡n giá»¯a sá»± chÃº Ã½ cá»§a LLM vÃ  sá»± chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn trong táº¥t cáº£
cÃ i Ä‘áº·t, cho tháº¥y LLM khÃ´ng lÃ½ luáº­n cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh nhÆ° láº­p trÃ¬nh viÃªn con ngÆ°á»i.
5.2 RQ2: Sá»± chÃº Ã½ cÃ³ thá»ƒ Giáº£i thÃ­ch Lá»—i cá»§a CÃ¡c mÃ´ hÃ¬nh Táº¡o mÃ£ khÃ´ng?
Äá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y, hai tÃ¡c giáº£ Ä‘áº§u tiÃªn Ä‘Ã£ phÃ¢n tÃ­ch thá»§ cÃ´ng cÃ¡c lá»—i táº¡o mÃ£ Ä‘Æ°á»£c táº¡o bá»Ÿi
hai mÃ´ hÃ¬nh tá»‘t nháº¥t trong nghiÃªn cá»©u cá»§a chÃºng tÃ´iâ€”GPT-4 vÃ  CodeGen-2.7B. Tá»•ng cá»™ng, hai mÃ´ hÃ¬nh nÃ y Ä‘Ã£ táº¡o ra
920 giáº£i phÃ¡p mÃ£ khÃ´ng chÃ­nh xÃ¡c trÃªn hai Ä‘iá»ƒm chuáº©n. ChÃºng tÃ´i Ä‘Ã£ láº¥y máº«u ngáº«u nhiÃªn 211 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c,
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 13 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:13
bao gá»“m 172 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c tá»« CodeGen-2.7B vÃ  39 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c tá»« GPT-4. KÃ­ch thÆ°á»›c máº«u
cÃ³ Ã½ nghÄ©a thá»‘ng kÃª vá»›i má»©c tin cáº­y 90% vÃ  sai sá»‘ 5%.
Hai tÃ¡c giáº£ Ä‘áº§u tiÃªn báº¯t Ä‘áº§u vá»›i 50 lá»—i táº¡o mÃ£ Ä‘áº§u tiÃªn vÃ  Ä‘á»™c láº­p kiá»ƒm tra
xem máº«u chÃº Ã½ cá»§a má»—i mÃ£ Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi BERT_masking , cho káº¿t quáº£ phÃ¹ há»£p nháº¥t
trong cÃ¡c thá»­ nghiá»‡m Ä‘á»‹nh lÆ°á»£ng, cÃ³ thá»ƒ giáº£i thÃ­ch lá»—i trong Ä‘Ã³ hay khÃ´ng. Äá»‘i vá»›i cÃ¡c nhiá»‡m vá»¥ Ä‘Æ¡n giáº£n,
máº¥t khoáº£ng nÄƒm phÃºt Ä‘á»ƒ kiá»ƒm tra má»—i nhiá»‡m vá»¥. Äá»‘i vá»›i cÃ¡c nhiá»‡m vá»¥ phá»©c táº¡p (vÃ­ dá»¥: cÃ¡c nhiá»‡m vá»¥ yÃªu cáº§u
hiá»ƒu biáº¿t vá» cÃ¡c khÃ¡i niá»‡m toÃ¡n há»c cá»¥ thá»ƒ), máº¥t khoáº£ng 10 Ä‘áº¿n 15 phÃºt, vÃ¬ cÃ¡c tÃ¡c giáº£ cáº§n
gá»¡ lá»—i mÃ£ thá»§ cÃ´ng vÃ  kiá»ƒm tra cÃ¡c giÃ¡ trá»‹ runtime cá»§a nÃ³ Ä‘á»ƒ hiá»ƒu lá»—i trÆ°á»›c. Sau khi
phÃ¢n tÃ­ch 50 lá»—i, há» tháº£o luáº­n nhá»¯ng lá»—i nÃ y vá»›i cÃ¡c tÃ¡c giáº£ khÃ¡c vÃ  tÃ³m táº¯t sÃ¡u máº«u chÃº Ã½ phá»• biáº¿n
cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i thÃ­ch lá»—i táº¡o mÃ£:
â—Thiáº¿u chÃº Ã½ Ä‘áº¿n cÃ¡c Ä‘iá»u kiá»‡n quan trá»ng. MÃ´ táº£ nhiá»‡m vá»¥ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c Ä‘iá»u kiá»‡n hoáº·c trÆ°á»ng há»£p gÃ³c nháº¥t Ä‘á»‹nh
Ä‘á»ƒ xá»­ lÃ½. Tuy nhiÃªn, mÃ´ hÃ¬nh bá» lá»¡ hoáº·c xá»­ lÃ½ khÃ´ng chÃ­nh xÃ¡c má»™t hoáº·c nhiá»u Ä‘iá»u kiá»‡n nhÆ° váº­y
vÃ¬ nÃ³ khÃ´ng chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« hoáº·c cá»¥m tá»« mÃ´ táº£ cÃ¡c Ä‘iá»u kiá»‡n tÆ°Æ¡ng á»©ng.
â—Thiáº¿u chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« mÃ´ táº£ quan trá»ng cá»§a má»™t thao tÃ¡c hoáº·c Ä‘á»‘i tÆ°á»£ng dá»¯ liá»‡u. MÃ´ táº£ nhiá»‡m vá»¥ Ä‘á» cáº­p Ä‘áº¿n má»™t thuá»™c tÃ­nh quan trá»ng cá»§a má»™t thao tÃ¡c hoáº·c Ä‘á»‘i tÆ°á»£ng dá»¯ liá»‡u, nhÆ° " largest element" vÃ 
"ascending order". Tuy nhiÃªn, mÃ´ hÃ¬nh khÃ´ng chÃº Ã½ Ä‘áº¿n nhá»¯ng tá»« mÃ´ táº£ nÃ y vÃ  do Ä‘Ã³
táº¡o ra giáº£i phÃ¡p mÃ£ vá»›i logic khÃ´ng chÃ­nh xÃ¡c.
â—Thiáº¿u chÃº Ã½ Ä‘áº¿n mÃ´ táº£ thao tÃ¡c. MÃ´ táº£ nhiá»‡m vá»¥ Ä‘á» cáº­p Ä‘áº¿n má»™t thao tÃ¡c hoáº·c hÃ nh Ä‘á»™ng,
nhÆ° open a file andsort a list . Tuy nhiÃªn, mÃ´ hÃ¬nh khÃ´ng chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« Ä‘á»™ng hoáº·c cá»¥m tá»«.
Do Ä‘Ã³, mÃ£ Ä‘Æ°á»£c táº¡o thá»±c hiá»‡n hÃ nh Ä‘á»™ng sai hoáº·c khÃ´ng thá»±c hiá»‡n hÃ nh Ä‘á»™ng.
â—Thiáº¿u chÃº Ã½ Ä‘áº¿n loáº¡i dá»¯ liá»‡u. CÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh thÆ°á»ng Ä‘á» cáº­p rÃµ rÃ ng Ä‘áº¿n loáº¡i dá»± kiáº¿n
cá»§a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. Má»™t sá»‘ mÃ´ táº£ nhiá»‡m vá»¥ cÅ©ng Ä‘á» cáº­p Ä‘áº¿n loáº¡i dá»¯ liá»‡u cá»§a má»™t sá»‘ káº¿t quáº£ trung gian. Tuy nhiÃªn, mÃ´ hÃ¬nh khÃ´ng chÃº Ã½ Ä‘áº¿n má»™t sá»‘ mÃ´ táº£ loáº¡i dá»¯ liá»‡u. Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n
cÃ¡c loáº¡i lá»—i khÃ¡c nhau, vÃ­ dá»¥: tráº£ vá» loáº¡i dá»¯ liá»‡u sai, gá»i phÆ°Æ¡ng thá»©c trÃªn
loáº¡i Ä‘á»‘i tÆ°á»£ng sai, v.v.
â—Ãnh xáº¡ khÃ´ng chÃ­nh xÃ¡c giá»¯a tá»« NL vÃ  cÃ¡c pháº§n tá»­ mÃ£. Trong má»™t sá»‘ trÆ°á»ng há»£p, chÃºng tÃ´i quan sÃ¡t mÃ´ hÃ¬nh
chÃº Ã½ Ä‘áº¿n má»™t tá»« hoáº·c cá»¥m tá»« quan trá»ng má»™t cÃ¡ch chÃ­nh xÃ¡c nhÆ°ng mÃ´ hÃ¬nh Ã¡nh xáº¡ nÃ³ Ä‘áº¿n lá»i gá»i phÆ°Æ¡ng thá»©c, biáº¿n, tham sá»‘, giÃ¡ trá»‹, hoáº·c logic sai, cÃ³ thá»ƒ do hiá»ƒu sai khÃ¡i niá»‡m vá»
Ã½ nghÄ©a ngá»¯ nghÄ©a cá»§a tá»« NL.
Sau Ä‘Ã³, há» Ä‘á»™c láº­p gáº¯n nhÃ£n cÃ¡c lá»—i cÃ²n láº¡i, tháº£o luáº­n viá»‡c gáº¯n nhÃ£n cá»§a há» vá»›i nhau,
vÃ  giáº£i quyáº¿t cÃ¡c xung Ä‘á»™t. Tá»•ng cá»™ng, chÃºng tÃ´i tÃ¬m tháº¥y ráº±ng lá»—i trong 57 trong sá»‘ 211 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c (27%)
cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi má»™t trong nÄƒm máº«u chÃº Ã½ Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ trÃªn. Cá»¥ thá»ƒ, 54 trong sá»‘ 172
giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c tá»« CodeGen-2.7B (31%) cÃ³ thá»ƒ giáº£i thÃ­ch vÃ  3 trong sá»‘ 39 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c tá»«
GPT-4 (8%) cÃ³ thá»ƒ giáº£i thÃ­ch. PhÃ¡t hiá»‡n nÃ y cho tháº¥y phÃ¢n tÃ­ch chÃº Ã½ tháº§n kinh cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cÃ³ tiá»m nÄƒng
Ä‘á»ƒ Ä‘á»‹nh vá»‹ vÃ  sá»­a chá»¯a má»™t pháº§n khÃ´ng táº§m thÆ°á»ng cá»§a lá»—i trong mÃ£ do LLM táº¡o ra. Cho ráº±ng chá»‰
3 lá»—i Ä‘Æ°á»£c táº¡o bá»Ÿi GPT-4 cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng cÃ¡c máº«u khÃ´ng cÄƒn chá»‰nh chÃº Ã½, Ä‘iá»u nÃ y ngá»¥ Ã½ ráº±ng
cÃ¡c mÃ´ hÃ¬nh yáº¿u hÆ¡n nhÆ° CodeGen-2.7B cÃ³ nhiá»u kháº£ nÄƒng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»± khÃ´ng cÄƒn chá»‰nh chÃº Ã½ vÃ 
do Ä‘Ã³ lá»—i táº¡o cá»§a chÃºng dá»… giáº£i thÃ­ch hÆ¡n. ÄÃ¢y lÃ  má»™t quan sÃ¡t thÃº vá»‹ vÃ¬ GPT-4
cÅ©ng gáº·p pháº£i sá»± khÃ´ng cÄƒn chá»‰nh chÃº Ã½, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 2, nhÆ°ng lá»—i táº¡o cá»§a nÃ³ khÃ´ng thá»ƒ
dá»… dÃ ng Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng phÃ¢n tÃ­ch chÃº Ã½. Äiá»u nÃ y cho tháº¥y ráº±ng khi cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ trá»Ÿ nÃªn Ä‘á»§ lá»›n
, chÃºng cÃ³ thá»ƒ Ä‘Ã£ phÃ¡t triá»ƒn má»™t cÃ¡ch khÃ¡c Ä‘á»ƒ diá»…n giáº£i lá»i nháº¯c Ä‘áº§u vÃ o vÃ  táº¡o
ná»™i dung. Äiá»u nÃ y kÃªu gá»i nghiÃªn cá»©u má»›i Ä‘á»ƒ hiá»ƒu táº¡i sao cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ siÃªu lá»›n nhÆ° GPT-4
táº¡o ra mÃ£ khÃ´ng chÃ­nh xÃ¡c. NgoÃ i ra, chÃºng tÃ´i thá»«a nháº­n ráº±ng nhiá»u lá»—i khÃ´ng thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng
sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh. Nhá»¯ng lá»—i nhÆ° váº­y bao gá»“m lá»—i cÃº phÃ¡p, tÃªn biáº¿n khÃ´ng xÃ¡c Ä‘á»‹nh, sá»­ dá»¥ng API khÃ´ng chÃ­nh xÃ¡c,
chá»‰ sá»‘ máº£ng khÃ´ng chÃ­nh xÃ¡c, vÃ²ng láº·p vÃ´ háº¡n, v.v. Káº¿t quáº£ nÃ y cho tháº¥y ráº±ng cáº§n phÃ¢n tÃ­ch thÃªm
Ä‘á»ƒ hiá»ƒu nguyÃªn nhÃ¢n gá»‘c rá»… cá»§a nhá»¯ng lá»—i nÃ y.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 14 ---
100:14 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
ChÃºng tÃ´i thá»ƒ hiá»‡n phÃ¢n phá»‘i vÃ  vÃ­ dá»¥ cho má»—i loáº¡i lá»—i dÆ°á»›i Ä‘Ã¢y. Trong nhá»¯ng vÃ­ dá»¥ nÃ y, chÃºng tÃ´i
lÃ m ná»•i báº­t cÃ¡c tá»« khÃ³a cÃ³ Ä‘iá»ƒm chÃº Ã½ cao tá»« mÃ´ hÃ¬nh báº±ng mÃ u xanh lam.
5.2.1 Thiáº¿u ChÃº Ã½ Ä‘áº¿n CÃ¡c Ä‘iá»u kiá»‡n Quan trá»ng. Lá»i nháº¯c nÃ y yÃªu cáº§u má»™t hÃ m khá»›p vá»›i
chuá»—i cÃ³ kÃ½ tá»± "a"theo sau bá»Ÿi khÃ´ng hoáº·c nhiá»u kÃ½ tá»± "b". Tuy nhiÃªn, CodeGen-2.7B
táº¡o ra má»™t hÃ m chá»‰ khá»›p vá»›i cÃ¡c tá»« cÃ³ má»™t "a"theo sau bá»Ÿi má»™t hoáº·c nhiá»u "b". PhÃ¢n tÃ­ch chÃº Ã½ cá»§a chÃºng tÃ´i cho tháº¥y cá»¥m tá»« Ä‘iá»u kiá»‡n "followed by zero or more" khÃ´ng chá»©a báº¥t ká»³ token nÃ o Ä‘Æ°á»£c chÃº Ã½ nháº¥t
, Ä‘iá»u nÃ y giáº£i thÃ­ch táº¡i sao mÃ´ hÃ¬nh bá» qua Ä‘iá»u kiá»‡n mÃ  kÃ½ tá»± "a"
cÃ³ thá»ƒ Ä‘Æ°á»£c theo sau khÃ´ng cÃ³ "b"nÃ o hoáº·c nhiá»u hÆ¡n má»™t "b". 6 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c thuá»™c danh má»¥c nÃ y.
1#Write a function that matches astring that has an ``a''followed by zero or more ``b''s .
2def function(string):
3 return string.count( 'ab') >= 1
5.2.2 Thiáº¿u ChÃº Ã½ Ä‘áº¿n CÃ¡c tá»« MÃ´ táº£ Quan trá»ng cá»§a má»™t Thao tÃ¡c hoáº·c Äá»‘i tÆ°á»£ng Dá»¯ liá»‡u. Lá»i nháº¯c
dÆ°á»›i Ä‘Ã¢y yÃªu cáº§u má»™t hÃ m tÃ­nh toÃ¡n lÅ©y thá»«a ğ‘›ğ‘¡â„cá»§a táº¥t cáº£ cÃ¡c pháº§n tá»­ trong danh sÃ¡ch. Tuy nhiÃªn,
mÃ£ Ä‘Æ°á»£c táº¡o bá»Ÿi CodeGen-2.7B tÃ­nh toÃ¡n lÅ©y thá»«a ğ‘›ğ‘¡â„cá»§a má»™t pháº§n tá»­ cá»¥ thá»ƒ mÃ  khÃ´ng sá»­ dá»¥ng
hÃ m lambda. PhÃ¢n tÃ­ch chÃº Ã½ cá»§a chÃºng tÃ´i cho tháº¥y cá»¥m tá»« thuá»™c tÃ­nh "individual items" khÃ´ng chá»©a
báº¥t ká»³ tá»« nÃ o Ä‘Æ°á»£c chÃº Ã½ nháº¥t trong lá»i nháº¯c. Äiá»u nÃ y giáº£i thÃ­ch táº¡i sao mÃ´ hÃ¬nh khÃ´ng thá»±c hiá»‡n
thao tÃ¡c Ä‘Æ°á»£c yÃªu cáº§u trÃªn táº¥t cáº£ cÃ¡c má»¥c thay vÃ¬ má»™t má»¥c cá»¥ thá»ƒ. 13 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c thuá»™c danh má»¥c nÃ y
.
1# Write a function to find the n-th power of individual elements in alist using lambda function.
2def function( list , n):
3 return list [n-1] ** n
5.2.3 Thiáº¿u ChÃº Ã½ Ä‘áº¿n MÃ´ táº£ Thao tÃ¡c. Lá»i nháº¯c dÆ°á»›i Ä‘Ã¢y yÃªu cáº§u má»™t hÃ m Ä‘áº¿m cÃ³ bao nhiá»u cáº·p
pháº§n tá»­ á»Ÿ cÃ¹ng chá»‰ sá»‘ trong hai máº£ng cho trÆ°á»›c báº±ng nhau. Tuy nhiÃªn,
CodeGen-2.7B táº¡o ra má»™t hÃ m kiá»ƒm tra xem hai chuá»—i cÃ³ báº±ng nhau hay khÃ´ng. PhÃ¢n tÃ­ch chÃº Ã½ cá»§a chÃºng tÃ´i cho tháº¥y
tá»« khÃ³a toÃ¡n tá»­ "count" khÃ´ng pháº£i lÃ  má»™t trong nhá»¯ng tá»« Ä‘Æ°á»£c chÃº Ã½ nháº¥t trong
lá»i nháº¯c, Ä‘iá»u nÃ y giáº£i thÃ­ch táº¡i sao mÃ£ Ä‘Æ°á»£c táº¡o khÃ´ng thá»±c hiá»‡n Ä‘áº¿m nhÆ° Ä‘Æ°á»£c yÃªu cáº§u. 6
giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c thuá»™c danh má»¥c nÃ y.
1# Write a function to count the same pair in two given lists using map function.
2def function(x,y):
3 return x == y
5.2.4 Thiáº¿u ChÃº Ã½ Ä‘áº¿n Loáº¡i Dá»¯ liá»‡u. Lá»i nháº¯c nÃ y yÃªu cáº§u má»™t hÃ m khá»›p vá»›i tá»« chá»©a "z"nhÆ°ng khÃ´ng cÃ³ "z"á»Ÿ Ä‘áº§u hoáº·c cuá»‘i. Tuy nhiÃªn, CodeGen-2.7B táº¡o ra má»™t hÃ m
kiá»ƒm tra xem chuá»—i Ä‘Æ°á»£c láº¥y tá»« viá»‡c loáº¡i bá» kÃ½ tá»± Ä‘áº§u tiÃªn khá»i tá»« cho trÆ°á»›c
cÃ³ báº±ng chuá»—i Ä‘Æ°á»£c láº¥y tá»« viá»‡c loáº¡i bá» kÃ½ tá»± cuá»‘i cÃ¹ng khá»i tá»« cho trÆ°á»›c hay khÃ´ng. PhÃ¢n tÃ­ch chÃº Ã½ cá»§a chÃºng tÃ´i cho tháº¥y
tá»« khÃ³a loáº¡i dá»¯ liá»‡u "z"khÃ´ng pháº£i lÃ  má»™t trong nhá»¯ng tá»« Ä‘Æ°á»£c chÃº Ã½ nháº¥t trong lá»i nháº¯c.
Äiá»u nÃ y giáº£i thÃ­ch táº¡i sao mÃ£ Ä‘Æ°á»£c táº¡o khÃ´ng kiá»ƒm tra sá»± tá»“n táº¡i cá»§a má»™t kÃ½ tá»± cá»¥ thá»ƒ trong
tá»« cho trÆ°á»›c. 7 lá»—i thuá»™c danh má»¥c nÃ y.
1#Write a function that matches a word containing 'z', not at the start or end of the word.
2def function(word):
3 return word[1:] == word[:-1]
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 15 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:15
5.2.5 Ãnh xáº¡ KhÃ´ng chÃ­nh xÃ¡c Giá»¯a Tá»« NL vÃ  CÃ¡c pháº§n tá»­ MÃ£. Lá»i nháº¯c nÃ y yÃªu cáº§u má»™t hÃ m
tÃ­nh toÃ¡n sá»‘ bell thá»© ğ‘›. Tuy nhiÃªn, thay vÃ¬ gá»i cÃ´ng thá»©c chÃ­nh xÃ¡c cho sá»‘ bell
, CodeGen-2.7B tÃ­nh toÃ¡n giai thá»«a cá»§a ğ‘›. PhÃ¢n tÃ­ch chÃº Ã½ cá»§a chÃºng tÃ´i cho tháº¥y tá»« khÃ³a
"bell" lÃ  má»™t trong nhá»¯ng tá»« Ä‘Æ°á»£c chÃº Ã½ nháº¥t trong lá»i nháº¯c. Tuy nhiÃªn, nÃ³ váº«n táº¡o ra khÃ´ng chÃ­nh xÃ¡c
cho lá»i nháº¯c nÃ y vÃ¬ nÃ³ khÃ´ng hiá»ƒu Ã½ nghÄ©a ngá»¯ nghÄ©a cá»§a "bell number" . 25 giáº£i phÃ¡p khÃ´ng chÃ­nh xÃ¡c
thuá»™c danh má»¥c nÃ y.
1#Write a python function to find nth bell number .
2def function(n):
3 ifn == 1:
4 return 1
5 else :
6 return n * function(n - 1)
PhÃ¡t hiá»‡n 2
PhÃ¢n tÃ­ch chÃº Ã½ cÃ³ thá»ƒ giáº£i thÃ­ch 27% lá»—i táº¡o trong má»™t Ä‘iá»ƒm chuáº©n cá»§a 211 lá»—i. Nhá»¯ng lá»—i nÃ y
cung cáº¥p hiá»ƒu biáº¿t vá» cÃ¡c háº¡n cháº¿ cá»§a LLM.
5.3 RQ3: TÃ¡c Ä‘á»™ng cá»§a CÃ¡c phÆ°Æ¡ng phÃ¡p TÃ­nh toÃ¡n ChÃº Ã½ Ä‘áº¿n Sá»± cÄƒn chá»‰nh lÃ  gÃ¬?
Äá»ƒ so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau, chÃºng tÃ´i Ä‘Ã£ tÃ­nh toÃ¡n Ä‘iá»ƒm F-1 cá»§a San Martino trung bÃ¬nh
vÃ  alpha cá»§a Krippendorff cá»§a táº¥t cáº£ nÄƒm mÃ´ hÃ¬nh (ngoáº¡i trá»« GPT-4) cho táº¥t cáº£ ğ¾(Báº£ng 3). Báº£ng 3
cho tháº¥y BERT_masking cho sá»± cÄƒn chá»‰nh cao nháº¥t trong cáº£ hai sá»‘ liá»‡u cho ğ¾=5,10. Tuy nhiÃªn,
phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n khÃ¡c, SHAP , khÃ´ng vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhÆ° BERT_masking
Ä‘Ã£ lÃ m. Máº·t khÃ¡c, cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient thÆ°á»ng tá»‘t hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½,
Ä‘áº·c biá»‡t lÃ  cho ğ¾=5theo cáº£ hai sá»‘ liá»‡u. ÄÃ¡ng ngáº¡c nhiÃªn, Ä‘iá»ƒm chÃº Ã½ Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi
cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½ (vÃ­ dá»¥: CODING_first ) Ã­t phÃ¹ há»£p nháº¥t vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i, Ä‘áº·c biá»‡t
cho cÃ¡c giÃ¡ trá»‹ ğ¾nhá» hÆ¡n. Má»™t lÃ½ do cÃ³ thá»ƒ lÃ  cÃ¡c Ä‘Æ¡n vá»‹ tÃ­nh toÃ¡n khÃ¡c trong kiáº¿n trÃºc transformer
, nhÆ° cÃ¡c lá»›p feed-forward, cÅ©ng Ä‘Ã³ng vai trÃ² quan trá»ng trong quÃ¡ trÃ¬nh táº¡o mÃ£
. VÃ­ dá»¥, Geva et al. tÃ¬m tháº¥y ráº±ng cÃ¡c lá»›p feed-forward áº£nh hÆ°á»Ÿng Ä‘áº¿n dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh báº±ng cÃ¡ch
thÃºc Ä‘áº©y cÃ¡c khÃ¡i niá»‡m trong khÃ´ng gian tá»« vá»±ng [ 35]. Do Ä‘Ã³, chá»‰ xem xÃ©t cÃ¡c lá»›p tá»± chÃº Ã½ khÃ´ng
hoÃ n toÃ n náº¯m báº¯t áº£nh hÆ°á»Ÿng cá»§a má»—i token Ä‘áº§u vÃ o Ä‘á»‘i vá»›i dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh.
Báº£ng 3. Äiá»ƒm F-1 trung bÃ¬nh cá»§a San Martino vÃ 
Alpha cá»§a Krippendorff trÃªn táº¥t cáº£ nÄƒm mÃ´ hÃ¬nh (loáº¡i trá»«
GPT-4) trong cÃ¡c cÃ i Ä‘áº·t ğ¾khÃ¡c nhau. Äiá»ƒm cao nháº¥t trong
má»—i cá»™t Ä‘Æ°á»£c lÃ m ná»•i báº­t báº±ng mÃ u vÃ ng .
Top 5 Top 10 Top 20PhÆ°Æ¡ngphÃ¡pF1 KA F1 KA F1 KA
BERT_masking 45.7% 0.27 51.4% 0.26 52.1% 0.12
SHAP 30.6% 0.11 41.5% 0.18 48.0% 0.16
InputÃ—Gradient_reading 37.9% 0.21 47.3% 0.30 48.2% 0.22
InputÃ—Gradient_coding 38.3% 0.22 47.9% 0.31 49.2% 0.24
Saliency_reading 37.5% 0.20 46.0% 0.28 48.0% 0.22
Saliency_coding 37.5% 0.21 46.7% 0.29 49.0% 0.24
READING_first 24.6% 0.01 42.7% 0.20 50.5% 0.22
READING_last 28.8% 0.07 46.6% 0.26 51.0% 0.23
READING_all 24.4% 0.00 39.8% 0.14 50.3% 0.22
CODING_first 26.9% 0.05 43.4% 0.21 50.5% 0.22
CODING_last 30.9% 0.10 47.2% 0.27 51.4% 0.24
CODING_all 33.2% 0.12 46.4% 0.25 51.0% 0.23HÆ¡n ná»¯a, trong cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient,
chÃºng tÃ´i quan sÃ¡t ráº±ng viá»‡c tÃ­nh toÃ¡n phÃ¢n phá»‘i chÃº Ã½
trong giai Ä‘oáº¡n mÃ£ hÃ³a (vÃ­ dá»¥: Saliency_coding )
cho sá»± cÄƒn chá»‰nh tá»‘t hÆ¡n so vá»›i phÃ¢n phá»‘i chÃº Ã½
trong giai Ä‘oáº¡n Ä‘á»c (vÃ­ dá»¥:
Saliency_reading ). Máº«u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c quan sÃ¡t
cho táº¥t cáº£ giÃ¡ trá»‹ ğ¾(tá»©c lÃ  ğ¾=5,10). Tuy nhiÃªn, sá»± khÃ¡c biá»‡t
giá»¯a InputÃ—Gradient vÃ  Saliency
lÃ  táº§m thÆ°á»ng.
Cuá»‘i cÃ¹ng, nhÆ° Ä‘Ã£ tháº£o luáº­n trong Pháº§n 2.3.1, khÃ´ng cÃ³
sá»± Ä‘á»“ng thuáº­n vá» cÃ¡ch tá»•ng há»£p Ä‘iá»ƒm tá»± chÃº Ã½
[ 10,97,99]. ChÃºng tÃ´i Ä‘Ã£ thá»­ nghiá»‡m vá»›i sÃ¡u
phÆ°Æ¡ng phÃ¡p tá»•ng há»£p tá»± chÃº Ã½ khÃ¡c nhau Ä‘Æ°á»£c sá»­ dá»¥ng
trong cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y Ä‘á»ƒ tÃ¬m phÆ°Æ¡ng phÃ¡p tá»•ng há»£p tá»± chÃº Ã½ tá»‘t nháº¥t. Äáº§u tiÃªn, trong ba
tÃ¹y chá»nâ€”(1) chá»‰ tá»•ng há»£p Ä‘iá»ƒm chÃº Ã½ trong
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 16 ---
100:16 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
HÃ¬nh 5. Lá»±a chá»n cá»§a ngÆ°á»i tham gia vá» cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ khÃ¡c nhau trong ba chiá»u.
lá»›p Ä‘áº§u tiÃªn, (2) chá»‰ tá»•ng há»£p Ä‘iá»ƒm chÃº Ã½ trong lá»›p cuá»‘i cÃ¹ng, vÃ  (3) tá»•ng há»£p Ä‘iá»ƒm chÃº Ã½
tá»« táº¥t cáº£ cÃ¡c lá»›pâ€”chá»‰ tá»•ng há»£p Ä‘iá»ƒm chÃº Ã½ trong lá»›p cuá»‘i cÃ¹ng táº¡o ra phÃ¢n phá»‘i chÃº Ã½
phÃ¹ há»£p nháº¥t vá»›i phÃ¢n phá»‘i chÃº Ã½ cá»§a con ngÆ°á»i trong cáº£ giai Ä‘oáº¡n Ä‘á»c vÃ  mÃ£ hÃ³a, ngoáº¡i trá»«
khi K Ä‘Æ°á»£c Ä‘áº·t thÃ nh 5 trong giai Ä‘oáº¡n mÃ£ hÃ³a. Äiá»u nÃ y cho tháº¥y nghiÃªn cá»©u tÆ°Æ¡ng lai nÃªn xem xÃ©t lá»›p cuá»‘i cÃ¹ng
khi táº­n dá»¥ng tá»± chÃº Ã½ Ä‘á»ƒ diá»…n giáº£i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ mÃ£. TÆ°Æ¡ng tá»± nhÆ° phÃ¡t hiá»‡n trong
cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient, Ä‘á»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½, phÃ¢n phá»‘i chÃº Ã½ á»Ÿ giai Ä‘oáº¡n mÃ£ hÃ³a
(vÃ­ dá»¥: CODING_first ) luÃ´n phÃ¹ há»£p hÆ¡n vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i so vá»›i nhá»¯ng phÃ¢n phá»‘i á»Ÿ giai Ä‘oáº¡n Ä‘á»c
(vÃ­ dá»¥: READING_first ).
PhÃ¡t hiá»‡n 3
BERT_masking táº¡o ra sá»± cÄƒn chá»‰nh chÃº Ã½ tá»‘t nháº¥t vá»›i láº­p trÃ¬nh viÃªn con ngÆ°á»i trong táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p.
CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient thÆ°á»ng tá»‘t hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½. Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient
vÃ  dá»±a trÃªn tá»± chÃº Ã½, phÃ¢n phá»‘i chÃº Ã½ trong giai Ä‘oáº¡n mÃ£ hÃ³a táº¡o ra sá»± cÄƒn chá»‰nh cao hÆ¡n
. Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½, phÃ¢n phá»‘i chÃº Ã½ trong lá»›p cuá»‘i cÃ¹ng táº¡o ra
sá»± cÄƒn chá»‰nh cao nháº¥t.
5.4 RQ4: PhÆ°Æ¡ng phÃ¡p TÃ­nh toÃ¡n ChÃº Ã½ nÃ o Ä‘Æ°á»£c Æ¯a chuá»™ng Nháº¥t?
HÃ¬nh 5 cho tháº¥y Ä‘Ã¡nh giÃ¡ cá»§a ngÆ°á»i tham gia vá» SHAP ,InputÃ—Gradient_coding , vÃ  CODING_all
vá» sá»± cÄƒn chá»‰nh vá»›i sá»± chÃº Ã½ cá»§a chÃ­nh há», kháº£ nÄƒng giáº£i thÃ­ch cá»§a sá»± chÃº Ã½ Ä‘Æ°á»£c tÃ­nh toÃ¡n,
vÃ  sá»Ÿ thÃ­ch cá»§a chÃ­nh há» (Q1-Q3 trong Pháº§n 4.4). NhÃ¬n chung, phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n, SHAP ,
Ä‘Æ°á»£c Æ°a chuá»™ng hÆ¡n hai phÆ°Æ¡ng phÃ¡p khÃ¡c trong cáº£ ba khÃ­a cáº¡nh. Äiá»ƒm Ä‘Ã¡nh giÃ¡ trung bÃ¬nh vá» sá»± cÄƒn chá»‰nh chÃº Ã½
cá»§a SHAP lÃ  5.13, trong khi Ä‘iá»ƒm trung bÃ¬nh cho InputXGradient_coding vÃ CODING_all láº§n lÆ°á»£t lÃ  4.59
vÃ  3.62.
HÆ¡n ná»¯a, ngÆ°á»i tham gia Ä‘á» xuáº¥t ráº±ng SHAP cÃ³ kháº£ nÄƒng giáº£i thÃ­ch tá»‘t hÆ¡n so vá»›i InputXGradi-
ent_coding (chÃªnh lá»‡ch trung bÃ¬nh: 0.59, kiá»ƒm Ä‘á»‹nh ğ‘¡Welch,ğ‘=0.02) vÃ  CODING_all (chÃªnh lá»‡ch trung bÃ¬nh: 1.26,
kiá»ƒm Ä‘á»‹nh ğ‘¡Welch,ğ‘=0.00001 ). Tuy nhiÃªn, theo pháº£n há»“i cá»§a ngÆ°á»i tham gia vá» niá»m tin cá»§a há» vÃ o
LLM (Q6 trong Pháº§n 4.4), 14 trong sá»‘ 22 ngÆ°á»i tham gia bÃ y tá» thiáº¿u tin tÆ°á»Ÿng vÃ  niá»m tin, ngay cáº£
sau khi tháº¥y cÃ¡c giáº£i thÃ­ch dá»±a trÃªn chÃº Ã½. VÃ­ dá»¥, P3 viáº¿t, "TÃ´i muá»‘n biáº¿t liá»‡u
mÃ´ hÃ¬nh LLM cÃ³ thá»ƒ cung cáº¥p lÃ½ do táº¡i sao há» táº¡o mÃ£ hay khÃ´ng. VÃ­ dá»¥, mÃ£ tham kháº£o. "
NgÆ°á»i tham gia cÅ©ng yÃªu cáº§u phÃ¢n tÃ­ch chÃº Ã½ chi tiáº¿t hÆ¡n. Cá»¥ thá»ƒ, há» muá»‘n tháº¥y
nhá»¯ng pháº§n nÃ o cá»§a Ä‘áº§u vÃ o chá»‹u trÃ¡ch nhiá»‡m táº¡o ra nhá»¯ng pháº§n nÃ o cá»§a Ä‘áº§u ra. VÃ­ dá»¥, P10
nÃ³i, "[TÃ´i muá»‘n biáº¿t] lÃ m tháº¿ nÃ o LLM xÃ¡c Ä‘á»‹nh Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, nhá»¯ng tá»« phÃ¢n biá»‡t nÃ o
thÃºc Ä‘áº©y cÃ¡c tháº¿ há»‡ khÃ¡c nhau. " NgÆ°á»i tham gia cÅ©ng thá»ƒ hiá»‡n sá»Ÿ thÃ­ch hÆ¡n cho SHAP so vá»›i
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 17 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:17
hai phÆ°Æ¡ng phÃ¡p khÃ¡c. ChÃªnh lá»‡ch sá»Ÿ thÃ­ch trung bÃ¬nh giá»¯a SHAP vÃ InputXGradient_coding
vÃ  giá»¯a SHAP vÃ  CODING_all lÃ  5.04 so vá»›i 4.56 (kiá»ƒm Ä‘á»‹nh ğ‘¡Welch,ğ‘=0.05) vÃ  5.04 so vá»›i 3.48
(kiá»ƒm Ä‘á»‹nh ğ‘¡Welch,ğ‘=0.00009 ).
PhÃ¡t hiá»‡n 4
NhÃ¬n chung, ngÆ°á»i tham gia Æ°a chuá»™ng phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n hÆ¡n phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient vÃ 
dá»±a trÃªn tá»± chÃº Ã½. Tuy nhiÃªn, ngÆ°á»i tham gia váº«n cáº£m tháº¥y thiáº¿u tin tÆ°á»Ÿng vÃ o LLM sau khi tháº¥y
cÃ¡c giáº£i thÃ­ch dá»±a trÃªn chÃº Ã½ vÃ  muá»‘n tháº¥y cÃ¡c giáº£i thÃ­ch phong phÃº hÆ¡n nhÆ° mÃ£ tham kháº£o
vÃ  Ã¡nh xáº¡ chÃº Ã½ chi tiáº¿t giá»¯a vÄƒn báº£n vÃ  mÃ£.
6 Ã NGHÄ¨A VÃ€ CÆ  Há»˜I
NghiÃªn cá»©u cá»§a chÃºng tÃ´i cÃ³ má»™t sá»‘ Ã½ nghÄ©a quan trá»ng cÃ³ lá»£i cho viá»‡c phÃ¡t triá»ƒn LLM Ä‘Ã¡ng tin cáº­y hÆ¡n vÃ 
chÃ­nh xÃ¡c hÆ¡n cho viá»‡c táº¡o mÃ£ trong tÆ°Æ¡ng lai.
Trong RQ1, chÃºng tÃ´i Ä‘Ã£ phÃ¡t hiá»‡n sá»± khÃ´ng cÄƒn chá»‰nh nháº¥t quÃ¡n giá»¯a sá»± chÃº Ã½ cá»§a con ngÆ°á»i vÃ  mÃ´ hÃ¬nh trong táº¥t cáº£
sÃ¡u mÃ´ hÃ¬nh. Máº·c dÃ¹ cÃ³ thá»ƒ LLM sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p hoÃ n toÃ n khÃ¡c Ä‘á»ƒ lÃ½ luáº­n vá» mÃ´ táº£ nhiá»‡m vá»¥
so vá»›i láº­p trÃ¬nh viÃªn con ngÆ°á»i, cÃ³ thá»ƒ tranh luáº­n liá»‡u phÆ°Æ¡ng phÃ¡p nhÆ° váº­y cÃ³ thá»±c sá»± tá»‘t cho láº­p trÃ¬nh viÃªn
do lo ngáº¡i vá» kháº£ nÄƒng diá»…n giáº£i, tÃ­nh máº¡nh máº½, vÃ  niá»m tin hay khÃ´ng. Nhiá»u nghiÃªn cá»©u
Ä‘Ã£ chá»‰ ra ráº±ng cÃ¡c mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i con ngÆ°á»i Ä‘Æ°á»£c coi lÃ  Ä‘Ã¡ng tin cáº­y hÆ¡n bá»Ÿi con ngÆ°á»i [ 8,34,38,
42,46,80]. HÆ¡n ná»¯a, trong thá»±c táº¿, LLM ngÃ y nay cÃ³ thá»ƒ giáº£i quyáº¿t Ä‘Ã¡ng tin cáº­y má»™t sá»‘ lÆ°á»£ng háº¡n cháº¿ cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh Ä‘Æ¡n giáº£n
vÃ  gáº·p khÃ³ khÄƒn Ä‘á»ƒ xá»­ lÃ½ cÃ¡c nhiá»‡m vá»¥ phá»©c táº¡p hÆ¡n hoáº·c tÃ¹y chá»‰nh, Ä‘iá»u nÃ y cho tháº¥y
khÃ´ng gian cáº£i thiá»‡n ráº¥t lá»›n. Do Ä‘Ã³, chÃºng tÃ´i tin ráº±ng viá»‡c Ä‘iá»u tra cÃ¡c máº«u chÃº Ã½ cá»§a LLM lÃ 
má»™t ná»— lá»±c Ä‘Ã¡ng giÃ¡ Ä‘á»ƒ giÃºp chÃºng ta hiá»ƒu cÃ¡ch LLM táº¡o mÃ£ vÃ  táº¡i sao LLM máº¯c má»™t sá»‘
lá»—i vÃ  cÅ©ng thÃ´ng bÃ¡o cÃ¡c cÆ¡ há»™i má»›i Ä‘á»ƒ cáº£i thiá»‡n LLM.
Trong RQ2, chÃºng tÃ´i Ä‘Ã£ phÃ¢n tÃ­ch thá»§ cÃ´ng sá»± chÃº Ã½ cá»§a 211 tháº¿ há»‡ khÃ´ng chÃ­nh xÃ¡c cá»§a hai mÃ´ hÃ¬nh tá»‘t nháº¥t
trong bÃ i bÃ¡o cá»§a chÃºng tÃ´i (GPT-4 vÃ  CodeGen-2.7B) vÃ  tÃ¬m tháº¥y 27% lá»—i cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng
sá»± cÄƒn chá»‰nh chÃº Ã½ khÃ´ng chÃ­nh xÃ¡c. PhÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y tiá»m nÄƒng sá»­a chá»¯a nhá»¯ng lá»—i táº¡o nÃ y
báº±ng cÃ¡ch Ä‘iá»u chá»‰nh sá»± cÄƒn chá»‰nh chÃº Ã½. TÆ°Æ¡ng tá»±, cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y trong Thá»‹ giÃ¡c MÃ¡y tÃ­nh Ä‘Ã£ chá»‰ ra
ráº±ng hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh tháº§n kinh cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n náº¿u chÃºng ta buá»™c sá»± chÃº Ã½ cá»§a chÃºng cÄƒn chá»‰nh vá»›i
con ngÆ°á»i [ 31,44,59,63]. Má»™t giáº£i phÃ¡p tiá»m nÄƒng lÃ  má»Ÿ rá»™ng hÃ m máº¥t mÃ¡t cá»§a LLM vá»›i má»™t
sá»‘ háº¡ng pháº¡t Ä‘o lÆ°á»ng KL divergence giá»¯a sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh vÃ  sá»± chÃº Ã½ cá»§a con ngÆ°á»i. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n,
máº¥t mÃ¡t sáº½ tÄƒng khi sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh lá»‡ch khá»i sá»± chÃº Ã½ cá»§a con ngÆ°á»i. Káº¿t quáº£ lÃ ,
LLM sáº½ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ phÃ¢n phá»‘i sá»± chÃº Ã½ theo cÃ¡ch tÆ°Æ¡ng tá»± nhÆ° láº­p trÃ¬nh viÃªn con ngÆ°á»i. ChÃºng tÃ´i Ä‘Ã£
má»Ÿ nguá»“n bá»™ dá»¯ liá»‡u chÃº Ã½ con ngÆ°á»i cá»§a chÃºng tÃ´i Ä‘á»ƒ táº¡o thuáº­n lá»£i cho cÃ´ng viá»‡c tÆ°Æ¡ng lai vá» cÄƒn chá»‰nh chÃº Ã½.
HÆ¡n ná»¯a, cÃ¡c máº«u chÃº Ã½ mÃ´ hÃ¬nh trong RQ3 cÃ³ thá»ƒ giÃºp cáº£i thiá»‡n tÃ­nh máº¡nh máº½ cá»§a cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£
báº±ng cÃ¡ch phÃ¡t triá»ƒn cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n Ä‘á»‘i nghá»‹ch dá»±a trÃªn chÃº Ã½ má»›i. Háº§u háº¿t cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n Ä‘á»‘i nghá»‹ch
chá»‰ Ã¡p dá»¥ng cÃ¡c nhiá»…u loáº¡n nhá» cho cÃ¡c token ngáº«u nhiÃªn trong lá»i nháº¯c, mÃ  khÃ´ng xem xÃ©t
táº§m quan trá»ng cá»§a má»™t token Ä‘á»‘i vá»›i mÃ´ hÃ¬nh [ 11,79,92,98]. Káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng Ä‘Ã¡ng giÃ¡
Ä‘á»ƒ Æ°u tiÃªn cÃ¡c token quan trá»ng trong quÃ¡ trÃ¬nh nhiá»…u loáº¡n. LÃ m nhiá»…u loáº¡n nhá»¯ng tá»« quan trá»ng nÃ y
vÃ  yÃªu cáº§u mÃ´ hÃ¬nh táº¡o mÃ£ cho nhá»¯ng lá»i nháº¯c bá»‹ nhiá»…u loáº¡n nÃ y cÃ³ thá»ƒ tiáº¿t lá»™ cÃ¡c váº¥n Ä‘á» tÃ­nh máº¡nh máº½
hiá»‡u quáº£ hÆ¡n.
Trong RQ3, chÃºng tÃ´i Ä‘Ã£ so sÃ¡nh 12 phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ vá»›i cÃ¡c thá»­ nghiá»‡m Ä‘á»‹nh lÆ°á»£ng (Báº£ng 2).
Do Ä‘Ã³, nghiÃªn cá»©u cá»§a chÃºng tÃ´i cung cáº¥p hÆ°á»›ng dáº«n thá»±c táº¿ Ä‘á»ƒ chá»n phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ cho
nghiÃªn cá»©u tÆ°Æ¡ng lai vá» cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM. Káº¿t quáº£ thá»­ nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y cÃ¡c nhÃ  phÃ¡t triá»ƒn
muá»‘n Ä‘o lÆ°á»ng sá»± chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ nÃªn Ä‘áº§u tiÃªn xem xÃ©t BERT_masking
vÃ¬ nÃ³ luÃ´n Ä‘áº¡t Ä‘Æ°á»£c sá»± cÄƒn chá»‰nh tá»‘t nháº¥t vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i trong táº¥t cáº£ ngoáº¡i trá»« hai cÃ i Ä‘áº·t
(Báº£ng 3). Giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tá»± chÃº Ã½ vÃ  dá»±a trÃªn gradient, cÃ¡c nhÃ  nghiÃªn cá»©u nÃªn Æ°u tiÃªn
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 18 ---
100:18 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient thÆ°á»ng cho sá»± cÄƒn chá»‰nh tá»‘t hÆ¡n vÃ  á»•n Ä‘á»‹nh hÆ¡n trong háº§u háº¿t cÃ¡c cÃ i Ä‘áº·t
(Báº£ng 3). Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn gradient vÃ  dá»±a trÃªn tá»± chÃº Ã½, viá»‡c tÃ­nh toÃ¡n phÃ¢n phá»‘i chÃº Ã½
cá»§a giai Ä‘oáº¡n mÃ£ hÃ³a cho sá»± cÄƒn chá»‰nh tá»‘t hÆ¡n (Báº£ng 3). Cuá»‘i cÃ¹ng, khi táº­n dá»¥ng tá»± chÃº Ã½ Ä‘á»ƒ
diá»…n giáº£i cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn transformer, cÃ¡c nhÃ  nghiÃªn cá»©u nÃªn xem xÃ©t sá»­ dá»¥ng Ä‘iá»ƒm tá»± chÃº Ã½
Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« lá»›p cuá»‘i cÃ¹ng, Ä‘Æ°á»£c chá»©ng minh lÃ  phÃ¹ há»£p hÆ¡n vá»›i
sá»± chÃº Ã½ cá»§a con ngÆ°á»i (Báº£ng 3).
Trong RQ4, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh nghiÃªn cá»©u ngÆ°á»i dÃ¹ng Ä‘á»ƒ so sÃ¡nh nháº­n thá»©c cá»§a nhÃ  phÃ¡t triá»ƒn vá» cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i thÃ­ch khÃ¡c nhau
. Háº§u háº¿t ngÆ°á»i tham gia coi SHAP lÃ  phÆ°Æ¡ng phÃ¡p XAI tá»‘t nháº¥t cho cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ LLM
. PhÃ¡t hiá»‡n nÃ y cho tháº¥y cÃ¡c nhÃ  nghiÃªn cá»©u tÆ°Æ¡ng lai cÃ³ thá»ƒ muá»‘n sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n
lÃ m phÆ°Æ¡ng phÃ¡p XAI máº·c Ä‘á»‹nh cho viá»‡c táº¡o mÃ£ dá»±a trÃªn LLM. HÆ¡n ná»¯a, trong kháº£o sÃ¡t sau nghiÃªn cá»©u
, ngÆ°á»i tham gia cÅ©ng yÃªu cáº§u phÃ¢n tÃ­ch chÃº Ã½ chi tiáº¿t hÆ¡n, nhÆ° tiáº¿t lá»™ má»‘i liÃªn káº¿t
giá»¯a cÃ¡c token Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra riÃªng láº». Äiá»u nÃ y lÃ m ná»•i báº­t nhu cáº§u vá» cÃ¡c phÆ°Æ¡ng phÃ¡p XAI má»›i
Ä‘á»ƒ diá»…n giáº£i cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM.
Trong tÆ°Æ¡ng lai, chÃºng tÃ´i cÅ©ng muá»‘n khÃ¡m phÃ¡ cÃ¡ch dáº¡y con ngÆ°á»i cÃ¡ch LLM diá»…n giáº£i mÃ£.
Hiá»ƒu cÃ¡ch LLM diá»…n giáº£i mÃ£ cÃ³ thá»ƒ truyá»n cáº£m há»©ng cho láº­p trÃ¬nh viÃªn con ngÆ°á»i táº¡o ra nhá»¯ng lá»i nháº¯c
dá»… hiá»ƒu hÆ¡n Ä‘á»‘i vá»›i LLM vÃ  do Ä‘Ã³ hÆ°á»›ng dáº«n LLM táº¡o ra mÃ£ tá»‘t hÆ¡n.
7 Má»I ÄE Dá»ŒA Äá»I Vá»šI TÃNH Há»¢P Lá»†
TÃ­nh há»£p lá»‡ ná»™i bá»™. Má»™t má»‘i Ä‘e dá»a tiá»m nÄƒng náº±m trong quÃ¡ trÃ¬nh gáº¯n nhÃ£n thá»§ cÃ´ng. Hai láº­p trÃ¬nh viÃªn
Ä‘Ã£ gáº¯n nhÃ£n thá»§ cÃ´ng cÃ¡c tá»« quan trá»ng Ä‘á»ƒ hiá»ƒu mÃ´ táº£ nhiá»‡m vá»¥ vÃ  triá»ƒn khai hÃ m chÃ­nh xÃ¡c
. VÃ¬ tiÃªu chÃ­ tá»« khÃ³a nÃ y ráº¥t chá»§ quan, cÃ¡c tá»« Ä‘Æ°á»£c chá»n bá»Ÿi hai ngÆ°á»i gáº¯n nhÃ£n
cÃ³ thá»ƒ khÃ´ng Ä‘áº¡i diá»‡n cho lá»±a chá»n cá»§a má»™t nhÃ³m lá»›n láº­p trÃ¬nh viÃªn. Äá»ƒ giáº£m thiá»ƒu má»‘i Ä‘e dá»a nÃ y, cÃ¡c tÃ¡c giáº£
Ä‘Ã£ thiáº¿t láº­p tiÃªu chuáº©n gáº¯n nhÃ£n thÃ´ng qua tháº£o luáº­n vÃ  Ä‘áº¡t Ä‘Æ°á»£c sá»± Ä‘á»“ng thuáº­n Ä‘Ã¡ng ká»ƒ vá»
viá»‡c gáº¯n nhÃ£n. HÆ¡n ná»¯a, chÃºng tÃ´i má»i má»™t ngÆ°á»i gáº¯n nhÃ£n thá»© ba Ä‘á»ƒ xÃ¡c thá»±c bá»™ dá»¯ liá»‡u chÃº thÃ­ch cá»§a chÃºng tÃ´i báº±ng cÃ¡ch Ä‘á»™c láº­p
gáº¯n nhÃ£n 164 lá»i nháº¯c tá»« bá»™ dá»¯ liá»‡u HumanEval. ChÃºng tÃ´i Ä‘Ã£ tÃ­nh toÃ¡n Ä‘iá»ƒm Fleiss' Kappa trong sá»‘
cÃ¡c nhÃ£n cá»§a ba ngÆ°á»i gáº¯n nhÃ£n vÃ  káº¿t quáº£ (0.64) cho tháº¥y sá»± Ä‘á»“ng thuáº­n Ä‘Ã¡ng ká»ƒ.
TÃ­nh há»£p lá»‡ ngoÃ i. Má»™t má»‘i Ä‘e dá»a tiá»m nÄƒng Ä‘á»‘i vá»›i tÃ­nh há»£p lá»‡ ngoÃ i lÃ  chÃºng tÃ´i chá»‰ thá»­ nghiá»‡m
vá»›i má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh, Python. ChÃºng tÃ´i khÃ´ng thá»ƒ Ä‘áº£m báº£o ráº±ng phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i khÃ¡i quÃ¡t hÃ³a cho
ngÃ´n ngá»¯ khÃ¡c.
TÃ­nh há»£p lá»‡ cáº¥u trÃºc. Má»™t má»‘i Ä‘e dá»a tiá»m nÄƒng Ä‘á»‘i vá»›i tÃ­nh há»£p lá»‡ cáº¥u trÃºc náº±m trong thiáº¿t káº¿ kháº£o sÃ¡t. NhÆ° chÃºng ta biáº¿t,
cÃ¡c váº¥n Ä‘á» láº­p trÃ¬nh Ä‘Ã²i há»i tinh tháº§n cao Ä‘á»ƒ giáº£i quyáº¿t vÃ  láº­p trÃ¬nh viÃªn cÃ³ thá»ƒ cÃ³ quan Ä‘iá»ƒm khÃ¡c nhau
vá» pháº§n nÃ o cá»§a lá»i nháº¯c mÃ  mÃ´ hÃ¬nh nÃªn chÃº Ã½ Ä‘áº¿n. Tuy nhiÃªn, trong thiáº¿t káº¿ hiá»‡n táº¡i,
chá»‰ cÃ³ 22 ngÆ°á»i tham gia Ä‘Æ°á»£c tham gia. KÃ­ch thÆ°á»›c máº«u nhá» cÃ³ thá»ƒ dáº«n Ä‘áº¿n cÃ¡c phÃ¡t hiá»‡n khÃ´ng
khÃ¡i quÃ¡t hÃ³a Ä‘Æ°á»£c. Äá»ƒ chá»‘ng láº¡i má»‘i Ä‘e dá»a nÃ y, chÃºng tÃ´i chá»‰ má»i nhá»¯ng ngÆ°á»i tham gia cÃ³ kinh nghiá»‡m trong láº­p trÃ¬nh Python
vÃ  Ä‘Ã£ sá»­ dá»¥ng LLM táº¡o mÃ£ Ã­t nháº¥t má»™t láº§n Ä‘á»ƒ kiá»ƒm soÃ¡t cháº¥t lÆ°á»£ng cá»§a
nghiÃªn cá»©u ngÆ°á»i dÃ¹ng.
8 CÃ”NG VIá»†C LIÃŠN QUAN
8.1 Táº¡o MÃ£ tá»« NgÃ´n ngá»¯ Tá»± nhiÃªn
Ká»ƒ tá»« CodeBERT [ 28], Ä‘Ã£ cÃ³ má»™t khá»‘i lÆ°á»£ng lá»›n tÃ i liá»‡u mÃ  MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM)
Ä‘Æ°á»£c sá»­ dá»¥ng trong viá»‡c táº¡o mÃ£ [ 5,27,29,32,37,62,66,74,82,86,88,91,96]. Cáº£ Guo et al. [ 37]
vÃ  Zeng et al. [ 96] Ä‘á»u sá»­ dá»¥ng mÃ´ hÃ¬nh BERT [ 25] Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘á»ƒ mÃ£ hÃ³a cÃ¢u há»i NL vÃ  lÆ°á»£c Ä‘á»“ cÆ¡ sá»Ÿ dá»¯ liá»‡u
cho viá»‡c táº¡o text-to-SQL. CodeBERT Ã¡p dá»¥ng cÃ¹ng kiáº¿n trÃºc mÃ´ hÃ¬nh nhÆ° BERT nhÆ°ng
Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i má»¥c tiÃªu lai trÃªn dá»¯ liá»‡u mÃ£ vÃ  vÄƒn báº£n tá»« kho GitHub [ 28]. Codex,
CodeGPT, vÃ  GraphCodeBERT cáº£i thiá»‡n CodeBERT báº±ng cÃ¡ch táº­n dá»¥ng luá»“ng dá»¯ liá»‡u trong giai Ä‘oáº¡n huáº¥n luyá»‡n trÆ°á»›c
[ 36]. Gáº§n Ä‘Ã¢y, cÃ¡c LLM hiá»‡n Ä‘áº¡i nhÆ° GPT-4 vÃ  Google Bard xuáº¥t sáº¯c trong cÃ¡c nhiá»‡m vá»¥ táº¡o mÃ£
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 19 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:19
trÃªn cÃ¡c Ä‘iá»ƒm chuáº©n khÃ¡c nhau [ 14,24]. CÃ¡c Ä‘oáº¡n mÃ£ chÃ­nh xÃ¡c cao vÃ  giÃ u ngá»¯ cáº£nh Ä‘Æ°á»£c táº¡o ra
bá»Ÿi nhá»¯ng mÃ´ hÃ¬nh nÃ y cho phÃ©p tá»± Ä‘á»™ng hÃ³a cÃ¡c nhiá»‡m vá»¥ láº­p trÃ¬nh khÃ¡c nhau.
NgoÃ i viá»‡c phÃ¡t triá»ƒn LLM má»›i cho mÃ£, cÃ´ng viá»‡c trÆ°á»›c Ä‘Ã¢y cÅ©ng Ä‘Ã£ trÃ¬nh bÃ y cÃ¡c phÆ°Æ¡ng phÃ¡p má»›i Ä‘á»ƒ
nÃ¢ng cao LLM cho viá»‡c táº¡o mÃ£ chÃ­nh xÃ¡c vÃ  máº¡nh máº½ hÆ¡n [ 15,16,51,66,74,94,100]. Thay vÃ¬
trá»±c tiáº¿p táº¡o mÃ£ tá»« vÄƒn báº£n, REDCODER [ 66] Ä‘áº§u tiÃªn truy xuáº¥t cÃ¡c Ä‘oáº¡n mÃ£ tÆ°Æ¡ng tá»± tá»« má»™t
kho vÃ  sau Ä‘Ã³ chuyá»ƒn chÃºng cÃ¹ng vá»›i mÃ´ táº£ vÄƒn báº£n cho má»™t LLM Ä‘á»ƒ táº¡o mÃ£. Shen
et al. [ 74] Ä‘á» xuáº¥t táº­n dá»¥ng kiáº¿n thá»©c miá»n tá»« tÃ i liá»‡u vÃ  bÃ¬nh luáº­n mÃ£ Ä‘á»ƒ
há»— trá»£ táº¡o mÃ£. Chakraborty et al. Ä‘á» xuáº¥t má»™t nhiá»‡m vá»¥ huáº¥n luyá»‡n trÆ°á»›c má»›i Ä‘Æ°á»£c gá»i lÃ  tá»± nhiÃªn hÃ³a
mÃ£ nguá»“n (tá»©c lÃ  dá»‹ch mÃ£ Ä‘Æ°á»£c táº¡o nhÃ¢n táº¡o thÃ nh dáº¡ng Ä‘Æ°á»£c viáº¿t bá»Ÿi con ngÆ°á»i) Ä‘á»ƒ giÃºp LLM
há»c cÃ¡ch táº¡o mÃ£ tá»± nhiÃªn [ 15]. Chen et al. Ä‘á» xuáº¥t sá»­ dá»¥ng LLM Ä‘á»ƒ tá»± Ä‘á»™ng
táº¡o trÆ°á»ng há»£p thá»­ nghiá»‡m Ä‘á»ƒ kiá»ƒm tra mÃ£ Ä‘Æ°á»£c táº¡o bá»Ÿi cÃ¹ng LLM [ 16]. Zan et al. Ä‘á» xuáº¥t Ä‘áº§u tiÃªn
phÃ¢n tÃ¡ch LLM thÃ nh hai LLM, má»™t Ä‘á»ƒ táº¡o phÃ¡c tháº£o chÆ°Æ¡ng trÃ¬nh vÃ  cÃ¡i khÃ¡c Ä‘á»ƒ Ä‘iá»n vÃ o
phÃ¡c tháº£o [ 94]. SkCoder [ 51] Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ báº¯t chÆ°á»›c hÃ nh vi tÃ¡i sá»­ dá»¥ng mÃ£ cá»§a nhÃ  phÃ¡t triá»ƒn báº±ng cÃ¡ch Ä‘áº§u tiÃªn truy xuáº¥t
má»™t vÃ­ dá»¥ mÃ£ liÃªn quan Ä‘áº¿n nhiá»‡m vá»¥ cho trÆ°á»›c, trÃ­ch xuáº¥t phÃ¡c tháº£o tá»« nÃ³, vÃ  chá»‰nh sá»­a phÃ¡c tháº£o dá»±a
trÃªn mÃ´ táº£ nhiá»‡m vá»¥.
8.2 NghiÃªn cá»©u Thá»±c nghiá»‡m vá» Táº¡o MÃ£
Gáº§n Ä‘Ã¢y, nhiá»u nghiÃªn cá»©u Ä‘Ã£ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM trong cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau,
bao gá»“m hiá»‡u suáº¥t [ 27,39,55,71,97], tÃ­nh máº¡nh máº½ [ 56,103], báº£o máº­t [ 6,67,93], mÃ¹i mÃ£ [ 77],
kháº£ nÄƒng sá»­ dá»¥ng [ 9,12,83,91], vÃ  cáº¥p phÃ©p [ 19]. Nhá»¯ng nghiÃªn cá»©u liÃªn quan nháº¥t vá»›i chÃºng tÃ´i lÃ  nhá»¯ng nghiÃªn cá»©u Ä‘iá»u tra
kháº£ nÄƒng giáº£i thÃ­ch cá»§a LLM cho mÃ£ [ 45,53,85,99]. Karmakar et al. nghiÃªn cá»©u nhá»¯ng gÃ¬ cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn BERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c
nhÆ° CodeBERT vÃ  GraphCodeBERT Ä‘Ã£ há»c vá» mÃ£ báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c nhiá»‡m vá»¥ thÄƒm dÃ² [ 45].
Má»™t nhiá»‡m vá»¥ thÄƒm dÃ² vá» cÆ¡ báº£n lÃ  má»™t nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n vá» má»™t thuá»™c tÃ­nh mÃ£ cá»¥ thá»ƒ, nhÆ° Ä‘á»™ dÃ i mÃ£ vÃ 
Ä‘á»™ phá»©c táº¡p cyclomatic, Ä‘á»ƒ kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ nháº¡y cáº£m vá»›i thuá»™c tÃ­nh Ä‘Ã³ hay khÃ´ng. So vá»›i cÃ´ng viá»‡c cá»§a chÃºng tÃ´i,
há» khÃ´ng phÃ¢n tÃ­ch quÃ¡ trÃ¬nh táº¡o mÃ£, vÃ­ dá»¥: táº¡i sao vÃ  lÃ m tháº¿ nÃ o mÃ£ nháº¥t Ä‘á»‹nh Ä‘Æ°á»£c táº¡o ra
dá»±a trÃªn mÃ´ táº£ vÄƒn báº£n. Liguori et al. Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh NMT
cho viá»‡c táº¡o mÃ£ [ 53]. NgoÃ i ra, Zhang et al. Ä‘iá»u tra quÃ¡ trÃ¬nh táº¡o mÃ£ báº±ng cÃ¡ch phÃ¢n tÃ­ch
cÃ¡c lá»›p tá»± chÃº Ã½ trong CodeBERT vÃ  GraphCodeBERT [ 99]. Wan et al. Ä‘Ã£ thá»±c hiá»‡n phÃ¢n tÃ­ch tá»± chÃº Ã½ tÆ°Æ¡ng tá»±
vÃ  cÅ©ng thiáº¿t káº¿ má»™t nhiá»‡m vá»¥ thÄƒm dÃ² má»›i vá» cáº¥u trÃºc mÃ£ Ä‘á»ƒ phÃ¢n tÃ­ch
xem LLM cÃ³ há»c Ä‘Æ°á»£c thÃ´ng tin vá» cáº¥u trÃºc mÃ£ hay khÃ´ng [ 85]. So vá»›i nhá»¯ng nghiÃªn cá»©u nÃ y, cÃ´ng viá»‡c cá»§a chÃºng tÃ´i khÃ¡c biá»‡t báº±ng cÃ¡ch phÃ¢n tÃ­ch
sá»± nháº¥t quÃ¡n giá»¯a sá»± chÃº Ã½ cá»§a LLM vÃ  sá»± chÃº Ã½ cá»§a láº­p trÃ¬nh viÃªn vá» mÃ´ táº£ NL cá»§a má»™t nhiá»‡m vá»¥ láº­p trÃ¬nh.
8.3 PhÃ¢n tÃ­ch Sá»± chÃº Ã½ cá»§a MÃ´ hÃ¬nh trong CÃ¡c lÄ©nh vá»±c KhÃ¡c
Nhiá»u phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½ Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh trong cÃ¡c lÄ©nh vá»±c khÃ¡c.
VÃ­ dá»¥, trong lÄ©nh vá»±c CV, Selvaraju et al. [ 73] Ä‘á» xuáº¥t sá»­ dá»¥ng gradient cá»§a máº¡ng tháº§n kinh tÃ­ch cháº­p
(CNN) Ä‘á»ƒ chá»‰ ra táº§m quan trá»ng cá»§a má»—i pixel trong hÃ¬nh áº£nh cho má»™t dá»± Ä‘oÃ¡n cá»¥ thá»ƒ.
TÆ°Æ¡ng tá»±, Zhou et al. [ 102] sá»­ dá»¥ng cÆ¡ cháº¿ pooling trung bÃ¬nh toÃ n cá»¥c Ä‘á»ƒ tÃ­nh toÃ¡n táº§m quan trá»ng
cá»§a má»—i vÃ¹ng Ä‘á»ƒ CNN phÃ¢n loáº¡i hÃ¬nh áº£nh chÃ­nh xÃ¡c. LÃ¡i xe tá»± Ä‘á»™ng lÃ  má»™t lÄ©nh vá»±c khÃ¡c
mÃ  nhu cáº§u vá» kháº£ nÄƒng giáº£i thÃ­ch ráº¥t máº¡nh. VÃ­ dá»¥, Zeiler et al. [ 95] sá»­ dá»¥ng cÃ¡c lá»›p deconvolution
Ä‘á»ƒ hiá»ƒu cÃ¡ch xe tá»± Ä‘á»™ng náº¯m báº¯t cÃ¡c phÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh thá»i gian thá»±c báº±ng CNN. Trong má»™t
cÃ´ng viá»‡c khÃ¡c, Chen et al. [ 17] Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p há»c chÃ­nh sÃ¡ch hiá»‡u quáº£ dá»¯ liá»‡u Ä‘Æ°á»£c gá»i lÃ  Semantic Predictive
Control (SPC) giáº£i thÃ­ch cÃ¡ch cÃ¡c tráº¡ng thÃ¡i mÃ´i trÆ°á»ng Ä‘Æ°á»£c nháº­n thá»©c Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh hÃ nh Ä‘á»™ng.
9 Káº¾T LUáº¬N
BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y má»™t nghiÃªn cá»©u thá»±c nghiá»‡m vá» sá»± cÄƒn chá»‰nh chÃº Ã½ giá»¯a cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM
vÃ  láº­p trÃ¬nh viÃªn con ngÆ°á»i. Káº¿t quáº£ cá»§a chÃºng tÃ´i tiáº¿t lá»™ ráº±ng cÃ³ sá»± khÃ´ng cÄƒn chá»‰nh nháº¥t quÃ¡n giá»¯a
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 20 ---
100:20 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
sá»± chÃº Ã½ cá»§a LLM vÃ  láº­p trÃ¬nh viÃªn. Trong sá»‘ 12 phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chÃº Ã½, cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn nhiá»…u loáº¡n
Ä‘Ã£ táº¡o ra Ä‘iá»ƒm chÃº Ã½ phÃ¹ há»£p hÆ¡n vá»›i sá»± chÃº Ã½ cá»§a con ngÆ°á»i vÃ  cÅ©ng
Ä‘Æ°á»£c Æ°a chuá»™ng hÆ¡n bá»Ÿi ngÆ°á»i tham gia nghiÃªn cá»©u ngÆ°á»i dÃ¹ng. Dá»±a trÃªn káº¿t quáº£ nghiÃªn cá»©u cá»§a chÃºng tÃ´i, chÃºng tÃ´i tiáº¿p tá»¥c tháº£o luáº­n
má»™t sá»‘ Ã½ nghÄ©a vÃ  cÆ¡ há»™i nghiÃªn cá»©u tÆ°Æ¡ng lai Ä‘á»ƒ diá»…n giáº£i tá»‘t hÆ¡n vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t
cá»§a cÃ¡c mÃ´ hÃ¬nh táº¡o mÃ£ dá»±a trÃªn LLM.
10 KHáº¢ NÄ‚NG CÃ“ Dá»® LIá»†U
MÃ£ vÃ  dá»¯ liá»‡u cá»§a chÃºng tÃ´i cÃ³ sáºµn trÃªn kho GitHub [47].
TÃ i liá»‡u tham kháº£o
[1]2022. CodeParrot. https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot.
[2] 2023. ChatGPT. http://chat.openai.com.
[3]2023. GPT-4 Parameters: Unlimited guide NLP's Game-Changer. https://medium.com/@mlubbad/the-ultimate-guide-
to-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a.
[4]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program
Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies . 2655â€“2668.
[5]Alex Andonian, Quentin Anthony, et al .2021. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch .
https://doi.org/10.5281/zenodo.5879544
[6]Owura Asare, Meiyappan Nagappan, and N Asokan. 2022. Is github's copilot as bad as humans at introducing
vulnerabilities in code? arXiv preprint arXiv:2204.04741 (2022).
[7]Jacob Austin, Augustus Odena, et al .2021. Program Synthesis with Large Language Models. arXiv preprint
arXiv:2108.07732 (2021).
[8]Aakash Bansal, Bonita Sharif, and Collin McMillan. 2023. Towards Modeling Human Attention from Eye Movements
for Neural Source Code Summarization. Proceedings of the ACM on Human-Computer Interaction 7, ETRA (2023),
1â€“19.
[9]Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with
code-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85â€“111.
[10] Joshua Bensemann et al .2022. Eye gaze and self-attention: How humans and transformers attend words in sentences.
InProceedings of the Workshop on Cognitive Modeling and Computational Linguistics . 75â€“87.
[11] Pavol Bielik and Martin Vechev. 2020. Adversarial robustness for code. In International Conference on Machine
Learning . PMLR, 896â€“907.
[12] Christian Bird et al .2022. Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-
programming tools. Queue 20, 6 (2022), 35â€“57.
[13] Angie Boggust, Benjamin Hoover, Arvind Satyanarayan, and Hendrik Strobelt. 2022. Shared interest: Measuring
human-ai alignment to identify recurring patterns in model behavior. In Proceedings of the 2022 CHI Conference on
Human Factors in Computing Systems . 1â€“17.
[14] SÃ©bastien Bubeck et al .2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712 (2023).
[15] Saikat Chakraborty et al. 2022. NatGen: generative pre-training by "naturalizing" source code. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering .
18â€“30.
[16] Bei Chen et al. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).
[17] Jianyu Chen, Shengbo Eben Li, and Masayoshi Tomizuka. 2021. Interpretable end-to-end urban autonomous driving
with latent deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems 23, 6 (2021), 5068â€“
5078.
[18] Wenhu Chen, Evgeny Matusov, Shahram Khadivi, and Jan-Thorsten Peter. 2016. Guided alignment training for
topic-aware neural machine translation. arXiv preprint arXiv:1607.01628 (2016).
[19] Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. 2022. To what extent do deep learning-based code recom-
menders generate predictions by cloning code from the training set?. In Proceedings of the 19th International Conference
on Mining Software Repositories . 167â€“178.
[20] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What Does BERT Look at? An
Analysis of BERT's Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP . 276â€“286.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 21 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:21
[21] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement 20, 1
(1960), 37â€“46.
[22] Giovanni Da San Martino et al .2019. Fine-grained analysis of propaganda in news article. In Proceedings of the 2019
conference on empirical methods in natural language processing and the 9th international joint conference on natural
language processing (EMNLP-IJCNLP) . Association for Computational Linguistics, 5636â€“5646.
[23] Misha Denil, Alban Demiraj, and Nando De Freitas. 2014. Extraction of salient sentences from labelled documents.
arXiv preprint arXiv:1412.6815 (2014).
[24] Giuseppe Destefanis, Silvia Bartolucci, and Marco Ortu. 2023. A Preliminary Analysis on the Code Generation
Capabilities of GPT-3.5 and Bard AI Models for Java Functions. arXiv preprint arXiv:2305.09402 (2023).
[25] Jacob Devlin et al .2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics .
4171â€“4186.
[26] Ahmed Elnaggar et al .2021. CodeTrans: Towards Cracking the Language of Silicon's Code Through Self-Supervised
Deep Learning and High Performance Computing. arXiv preprint arXiv:2104.02443 (2021).
[27] Mark Chen et al. 2021. Evaluating Large Language Models Trained on Code. (2021). arXiv:2107.03374 [cs.LG]
[28] Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of
table consolidation and transformation tasks from examples. ACM SIGPLAN Notices 52, 6 (2017), 422â€“436.
[29] Zhangyin Feng et al .2020. CodeBERT: A Pre-Trained Model for Programming and Natural Language Processing. In
Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 307â€“316.
[30] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin 76, 5 (1971), 378.
[31] Ruth C Fong, Walter J Scheirer, and David D Cox. 2018. Using human brain activity to guide machine learning.
Scientific reports 8, 1 (2018), 5397.
[32] Daniel Fried et al .2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh International
Conference on Learning Representations . https://openreview.net/forum?id=hQwb-lbM6EL
[33] Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020. Attention in natural language processing. IEEE transactions on
neural networks and learning systems 32, 10 (2020), 4291â€“4308.
[34] Yuyang Gao et al .2022. Aligning eyes between humans and deep neural network through interactive attention
alignment. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1â€“28.
[35] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions
by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680 (2022).
[36] Daya Guo et al .2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366
(2020).
[37] Jiaqi Guo et al .2019. Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 4524â€“4535.
[38] Christopher Hazard et al .2022. Importance is in your attention: agent importance prediction for autonomous driving.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2532â€“2535.
[39] Dan Hendrycks et al. 2021. Measuring Coding Challenge Competence With APPS. NeurIPS (2021).
[40] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020.
Aligning AI with Shared Human Values. arXiv preprint arXiv:2008.02275 (2020).
[41] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2018. Evaluating feature importance estimates.
(2018).
[42] Siteng Huang, Min Zhang, Yachen Kang, and Donglin Wang. 2021. Attributes-guided and pure-visual attention
alignment for few-shot recognition. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 7840â€“7847.
[43] Paras Jain and Ajay Jain. 2021. Contrastive Code Representation Learning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing .
[44] Shaohua Jia et al .2018. Biometric recognition through eye movements using a recurrent neural network. In 2018
IEEE International Conference on Big Knowledge (ICBK) . IEEE, 57â€“64.
[45] Anjan Karmakar and Romain Robbes. 2021. What do pre-trained code models know about code?. In 2021 36th
IEEE/ACM International Conference on Automated Software Engineering (ASE) . IEEE, 1332â€“1336.
[46] Iuliia Kotseruba, Amir Rasouli, and John K Tsotsos. 2016. Joint attention in autonomous driving (JAAD). arXiv
preprint arXiv:1609.04741 (2016).
[47] Bonan Kou. 2024. Attention-Alignment-Empirical-Study. https://github.com/BonanKou/Attention-Alignment-
Empirical-Study.
[48] Klaus Krippendorff. 2004. Reliability in content analysis: Some common misconceptions and recommendations.
Human communication research 30, 3 (2004), 411â€“433.
[49] Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology . Sage publications.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 22 ---
100:22 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
[50] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S Liang. 2019. Spoc:
Search-based pseudocode to code. Advances in Neural Information Processing Systems 32 (2019).
[51] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. SkCoder: A Sketch-based Approach for Automatic
Code Generation. arXiv preprint arXiv:2302.06144 (2023).
[52] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks through representation erasure. arXiv
preprint arXiv:1612.08220 (2016).
[53] Pietro Liguori et al .2022. Can NMT understand me? towards perturbation-based evaluation of NMT models for code
generation. In 2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE) .
IEEE, 59â€“66.
[54] Shusen Liu et al .2018. Nlize: A perturbation-driven visual interrogation tool for analyzing and interpreting natural
language inference models. IEEE transactions on visualization and computer graphics 25, 1 (2018), 651â€“660.
[55] Xiaodong Liu, Ying Xia, and David Lo. 2020. An Empirical Study on the Usage of Transformer Models for Code
Completion. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME) . IEEE, 408â€“418.
[56] Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, and Li Li. 2023. On the Reliability and Explainability of Automated
Code Generation Approaches. arXiv preprint arXiv:2302.09587 (2023).
[57] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. Advances in neural
information processing systems 30 (2017).
[58] Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica 22, 3 (2012), 276â€“282.
[59] Cristina MelÃ­cio et al .2018. Object detection and localization with artificial foveal visual attention. In 2018 Joint IEEE
8th international conference on development and learning and epigenetic robotics (ICDL-EpiRob) . IEEE, 101â€“106.
[60] Christoph Molnar. 2020. Interpretable machine learning . Lulu. com.
[61] Ernst Niebur. 2007. Saliency map. Scholarpedia 2, 8 (2007), 2675.
[62] Erik Nijkamp, Bo Pang, et al .2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program
Synthesis. In The Eleventh International Conference on Learning Representations .
[63] Afonso Nunes, Rui Figueiredo, and Plinio Moreno. 2020. Learning to search for objects in images from human gaze
sequences. In Image Analysis and Recognition: 17th International Conference . Springer, 280â€“292.
[64] Matteo Paltenghi and Michael Pradel. 2021. Thinking like a developer? comparing the attention of humans with
neural models of code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 867â€“879.
[65] Kishore Papineni et al .2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computational Linguistics . 311â€“318.
[66] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented
Code Generation and Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021 .
2719â€“2734.
[67] Hammond Pearce et al .2022. Asleep at the keyboard? assessing the security of github copilot's code contributions. In
2022 IEEE Symposium on Security and Privacy (SP) . IEEE, 754â€“768.
[68] Md Rafiqul Islam Rabin, Vincent J Hellendoorn, and Mohammad Amin Alipour. 2021. Understanding neural code
intelligence through program simplification. In Proceedings of the 29th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering . 441â€“452.
[69] Shuo Ren et al .2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297
(2020).
[70] Marco Tulio Ribeiro et al .2016. " Why should i trust you?" Explaining the predictions of any classifier. In Proceedings
of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining . 1135â€“1144.
[71] Rafael R Rodrigues et al .2021. Studying the usage of text-to-text transfer transformer to support code-related tasks.
In2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) .
IEEE, 327â€“336.
[72] Jaydeb Sarker, Sayma Sultana, Steven R Wilson, and Amiangshu Bosu. 2023. ToxiSpanSE: An Explainable Toxicity
Detection in Code Review Comments. In 2023 ACM/IEEE International Symposium on Empirical Software Engineering
and Measurement (ESEM) . IEEE, 1â€“12.
[73] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE
international conference on computer vision . 618â€“626.
[74] Sijie Shen, Xiang Zhu, Yihong Dong, Qizhi Guo, Yankun Zhen, and Ge Li. 2022. Incorporating domain knowledge
through task augmentation for front-end JavaScript code generation. In Proceedings of the 30th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Software Engineering . 1533â€“1543.
[75] Donghee Shin. 2021. The effects of explainability and causability on perception, trust, and acceptance: Implications
for explainable AI. International Journal of Human-Computer Studies 146 (2021), 102551.
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 23 ---
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cÃ³ ChÃº Ã½ TÆ°Æ¡ng tá»± nhÆ° Láº­p trÃ¬nh viÃªn Con ngÆ°á»i Khi Táº¡o MÃ£ khÃ´ng? 100:23
[76] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating
activation differences. In International conference on machine learning . PMLR, 3145â€“3153.
[77] Mohammed Latif Siddiq et al .2022. An Empirical Study of Code Smells in Transformer-based Code Generation
Techniques. In 2022 IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM) .
IEEE, 71â€“82.
[78] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising
image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).
[79] Shashank Srikant et al .2020. Generating Adversarial Computer Programs using Optimized Obfuscations. In Interna-
tional Conference on Learning Representations .
[80] Andrea Stocco et al .2022. Thirdeye: Attention maps for safe autonomous driving systems. In Proceedings of the 37th
IEEE/ACM International Conference on Automated Software Engineering . 1â€“12.
[81] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International
conference on machine learning . PMLR, 3319â€“3328.
[82] Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2022. Natural Language Processing with Transformers: Building
Language Applications with Hugging Face . O'Reilly Media, Incorporated.
[83] Priyan Vaithilingam et al .2022. Expectation vs. experience: Evaluating the usability of code generation tools powered
by large language models. In CHI conference on human factors in computing systems extended abstracts . 1â€“7.
[84] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention interpretability
across nlp tasks. arXiv preprint arXiv:1909.11218 (2019).
[85] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What do they capture? a structural
analysis of pre-trained language models for source code. In Proceedings of the 44th International Conference on Software
Engineering . 2377â€“2388.
[86] Bailin Wang et al .2020. RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 7567â€“7578.
[87] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:
//github.com/kingoflolz/mesh-transformer-jax.
[88] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-
Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing . 8696â€“8708.
[89] Katharina Weitz et al .2019. " Do you trust me?" Increasing user-trust by integrating virtual agents in explainable AI
interaction design. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents . 7â€“9.
[90] Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020. Perturbed Masking: Parameter-free Probing for Analyzing
and Interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics .
4166â€“4176.
[91] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large
language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming .
1â€“10.
[92] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of code. Proceedings of the ACM on
Programming Languages 4, OOPSLA (2020), 1â€“30.
[93] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot's code generation. In
Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering . 62â€“71.
[94] Daoguang Zan, Bei Chen, et al .2022. CERT: Continual Pre-training on Sketches for Library-oriented Code Generation.
InProceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria,
23-29 July 2022 , Luc De Raedt (Ed.). ijcai.org, 2369â€“2375. https://doi.org/10.24963/ijcai.2022/329
[95] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Computer
Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 . Springer,
818â€“833.
[96] Yu Zeng et al .2020. RECPARSER: A Recursive Semantic Parsing Framework for Text-to-SQL Task.. In IJCAI .
3644â€“3650.
[97] Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, and Lingming Zhang. 2022. An extensive
study on pre-trained models for program understanding and generation. In Proceedings of the 31st ACM SIGSOFT
International Symposium on Software Testing and Analysis . 39â€“51.
[98] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. 2020. Generating adversarial examples for holding
robustness of source code processing models. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34.
1169â€“1176.
[99] Kechi Zhang, Ge Li, and Zhi Jin. 2022. What does Transformer learn about source code? arXiv preprint arXiv:2207.08466
(2022).
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.

--- TRANG 24 ---
100:24 Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang
[100] Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022. Diet code is healthy: Simplifying programs
for pre-trained models of code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering . 1073â€“1084.
[101] Haiying Zhao, Wei Zhou, Xiaogang Hou, and Hui Zhu. 2020. Double attention for multi-label image classification.
IEEE Access 8 (2020), 225539â€“225550.
[102] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for
discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2921â€“2929.
[103] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023.
On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on
Codex. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics .
1090â€“1102.
Received 2023-09-28; accepted 2024-04-16
Proc. ACM Softw. Eng., Vol. 1, No. FSE, Article 100. Publication date: July 2024.
