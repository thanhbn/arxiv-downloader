# 2311.10642.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2311.10642.pdf
# Kích thước tệp: 940153 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Suy nghĩ lại về Attention: Khám phá các Mạng Neural Feed-Forward Nông như một
Lựa chọn thay thế cho các Lớp Attention trong Transformers
Vukasin Bozic, Danilo Dordevic, Daniele Coppola, Joseph Thommes, Sidak Pal Singh
ETH Zurich
{vbozic, ddordevic, dcoppola, jthommes }@student.ethz.ch, sidak.singh@inf.ethz.ch
Tóm tắt
Nghiên cứu này trình bày một phân tích về hiệu quả của việc sử
dụng các mạng feed-forward nông tiêu chuẩn để bắt chước hành vi
của cơ chế attention trong mô hình Transformer gốc, một kiến trúc
tiên tiến cho các tác vụ sequence-to-sequence. Chúng tôi thay thế
các thành phần chính của cơ chế attention trong Transformer bằng
các mạng feed-forward đơn giản, được huấn luyện bằng cách sử dụng
các thành phần gốc thông qua knowledge distillation. Các thí nghiệm
của chúng tôi, được tiến hành trên bộ dữ liệu IWSLT2017, tiết lộ
khả năng của những "Transformers không có attention" này có thể
cạnh tranh với hiệu suất của kiến trúc gốc. Thông qua các nghiên cứu
ablation nghiêm ngặt, và thử nghiệm với các loại và kích thước mạng
thay thế khác nhau, chúng tôi đưa ra những hiểu biết hỗ trợ tính khả
thi của phương pháp của chúng tôi. Điều này không chỉ làm sáng tỏ
khả năng thích ứng của các mạng feed-forward nông trong việc mô
phỏng các cơ chế attention mà còn nhấn mạnh tiềm năng của chúng
trong việc tinh giản các kiến trúc phức tạp cho các tác vụ sequence-to-sequence.

Giới thiệu
Bài báo tinh tuyến (Vaswani et al. 2017) đã giới thiệu mô hình
Transformer đã thay đổi cơ bản bối cảnh của các tác vụ modeling
sequence-to-sequence. Nó đã thiết lập các tiêu chuẩn mới cho dịch
thuật ngôn ngữ, được đo bằng điểm BLEU (Papineni et al. 2002).
Cơ chế attention của Transformer cho phép thiết lập các phụ thuộc
dài hạn trong dữ liệu tuần tự, cho phép nó chú ý đến mọi phần tử
trong một chuỗi, một thành tựu mà các kiến trúc mạng trước đó đã
gặp khó khăn để đạt được mà không có chi phí tính toán đáng kể.

Lấy cảm hứng từ các nghiên cứu trước (Ba and Caruana 2014),
(Urban et al. 2017) khám phá tính khả thi của việc huấn luyện các
mạng feed-forward nông để mô phỏng hành vi của các mạng tích
chập sâu với các mạng sâu làm giáo viên, chúng tôi tiến hành một
cuộc điều tra tương tự trên Transformer gốc được trình bày trong
(Vaswani et al. 2017). Trọng tâm của chúng tôi là dịch thuật ngôn
ngữ, sử dụng bộ dữ liệu IWSLT2017 (Cettolo et al. 2017). Chúng
tôi nhằm đánh giá mức độ mà các mạng feed-forward nông tiêu
chuẩn có thể mô hình hóa các cơ chế attention bằng cách thay thế
các thành phần attention chính bằng các mạng feed-forward được
huấn luyện để sao chép hành vi của chúng.

Nghiên cứu này cung cấp bằng chứng thực nghiệm hỗ trợ ý
tưởng rằng các mạng feed-forward nông có thể học hiệu quả các
hành vi của các module attention Transformer và thay thế chúng
mà không ảnh hưởng đáng kể đến hiệu suất tổng thể của nó. Mặc
dù nó không giới thiệu một lợi thế cạnh tranh so với các phương
pháp đã được thiết lập, nó đưa ra một phân tích khái niệm về các
kỹ thuật hiện có và các lựa chọn thay thế tiềm năng.

Hình 1: Các phương pháp thay thế encoder self-attention khác nhau
được trình bày.

Mô hình và Phương pháp
Kiến trúc Transformer được cấu thành từ các block encoder và
decoder được xếp chồng, sử dụng attention để xử lý dữ liệu đầu
vào. Lớp encoder có một block self-attention, trong khi lớp decoder
bao gồm cả các block self-attention và cross-attention, kết hợp dữ
liệu được xử lý bởi encoder và chính nó. Mô hình này được sử dụng
làm baseline, tức là mô hình giáo viên, nơi các activation trung gian
của các block của nó được sử dụng cho knowledge distillation
(Hinton, Vinyals, and Dean 2015) trong việc huấn luyện các mạng
feed-forward.

Thay thế encoder self-attention. Trong phương pháp được đề
xuất, một nghiên cứu ablation kỹ lưỡng về các phương pháp thay
thế tiềm năng đã được tiến hành. Các thí nghiệm được thực hiện
trên các lớp self-attention trong tất cả 6 block encoder.

Chúng tôi đã giới thiệu bốn mức độ trừu tượng khác nhau để thay
thế attention encoder gốc:
Attention Layer Replacement (ALR), Attention Layer with
Residual Connection Replacement (ALRR), Attention Separate
Heads Layer Replacement (ASLR) và Encoder Layer
Replacement (ELR), như được mô tả trong Hình 1. Hơn nữa,

--- TRANG 2 ---
tất cả các kiến trúc này đã được huấn luyện với 5 kích thước khác
nhau, từ "XS" đến "L".

Thay thế attention Transformer đầy đủ. Vì ALR được thấy là
phương pháp hiệu quả nhất trong trường hợp thay thế encoder
attention, có cả hiệu suất cao và số lượng tham số nhỏ, toàn bộ quy
trình đã được tái tạo cho việc thay thế decoder self-attention và
cross-attention. Điều này đòi hỏi các điều chỉnh của các kiến trúc
đã được giới thiệu trước đó, do các loại attention khác nhau trong
decoder. Thêm chi tiết về động lực và lựa chọn của các mạng thay
thế được đưa ra trong Phụ lục A, trong khi các thông số kỹ thuật
triển khai và huấn luyện của tất cả các mạng thay thế FF được cung
cấp trong Phụ lục B.

Kết quả
Metric BLEU được sử dụng cho mục đích đánh giá trong nghiên
cứu này, vì nó đại diện cho một metric tiêu chuẩn cho các tác vụ
dịch thuật ngôn ngữ. Kết quả cho cả nghiên cứu thay thế encoder
self-attention và Transformer đầy đủ trải rộng trên 4 tập con của bộ
dữ liệu IWSLT2017. Hơn nữa, điểm BLEU tương đối so với điểm
baseline (Transformer vanilla) của mỗi thí nghiệm đã được tính
toán và sau đó được lấy trung bình trên các bộ dữ liệu. Kết quả thí
nghiệm một phần được trình bày trong Hình 2 và 3, trong khi kết
quả đầy đủ có sẵn trong Phụ lục C. Chúng tôi cung cấp mã triển
khai trên Github1.

Thảo luận
Trong trường hợp thay thế encoder, tất cả các phương pháp được đề
xuất đạt được kết quả cạnh tranh so với baseline, như được thấy
trong Hình 2. Trong số bốn phương pháp, ELR hoạt động tệ nhất,
điều này được gây ra bởi tính đơn giản của mô hình thay thế, bỏ
qua tất cả các cấu trúc encoder hỗ trợ huấn luyện.

Hơn nữa, phương pháp thay thế Transformer đầy đủ, nơi chỉ
phương pháp ALR được sử dụng, cho kết quả thể hiện tiềm năng
của các mạng feed-forward để sao chép thành công hành vi decoder
self-attention, trong khi hiệu suất trên decoder cross-attention tương
đối tệ hơn, như được trình bày trong Hình 3. Lý do tiềm năng cho
hành vi này có thể là thiếu tính biểu cảm của mạng feed-forward
cần thiết để mô tả ánh xạ và tương tác phức tạp hơn giữa các chuỗi
được sử dụng trong block cross-attention, điều này cũng ảnh hưởng
đến điểm đánh giá cuối cùng cho Transformer hoàn toàn "không có
attention".

Tuy nhiên, tất cả các phương pháp thay thế đều có chi phí đáng
kể là có nhiều tham số hơn. Một nhược điểm khác của việc thay thế
attention bằng một mạng feed-forward có kích thước cố định là
thiếu linh hoạt sắp xảy ra của mô hình về độ dài của các chuỗi mà
mô hình có thể hoạt động.

1https://github.com/vulus98/Rethinking-attention.git

Hình 2: Điểm BLEU tương đối [%] (tương đối so với Transformer
baseline), phụ thuộc vào kích thước mạng FF. Encoder self-attention
được thay thế bằng các phương pháp thay thế khác nhau.

Hình 3: Điểm BLEU tương đối [%] (tương đối so với baseline), phụ
thuộc vào kích thước mạng FF. Phương pháp ALR được sử dụng để
thay thế các phần attention khác nhau của transformer.

Kết luận
Bằng chứng thực nghiệm cho thấy rằng các phương pháp được đề
xuất có khả năng đạt được hiệu suất tương đương với Transformer
gốc, chứng minh rằng Transformers không nhất thiết phải có
attention. Những kết luận này cũng chỉ ra những thiếu sót của các
phương pháp tối ưu hóa hiện tại, không thể huấn luyện những
"Transformers không có attention" này từ đầu mà cần các kỹ thuật
tiên tiến hơn, chẳng hạn như knowledge distillation để hội tụ vào
các cấu hình tham số mong muốn. Kết luận này nhấn mạnh rằng
với những tiến bộ trong các kỹ thuật tối ưu hóa, các kiến trúc ít
chuyên biệt hơn như mạng feed-forward có thể được sử dụng cho
các tác vụ tiên tiến, hiện tại được dành riêng cho các kiến trúc có
tính chuyên biệt cao.

--- TRANG 3 ---
Lời cảm ơn
Chúng tôi muốn bày tỏ lòng biết ơn chân thành đến phòng thí nghiệm
Data Analytics của ETH Zurich vì đã cung cấp các tài nguyên cần
thiết và hỗ trợ trong suốt quá trình thực hiện dự án này; môi trường
hợp tác và phong phú của phòng thí nghiệm đã đóng góp đáng kể
vào thành công của nghiên cứu này, và chúng tôi thực sự biết ơn
sự hỗ trợ vô giá của họ. Ngoài ra, chúng tôi xin gửi lời cảm ơn chân
thành đến G-research vì sự tài trợ hào phóng của họ, điều này đã
làm cho chúng tôi có thể tham dự hội nghị và trình bày bài báo này.

Tài liệu tham khảo
Ba, L. J.; and Caruana, R. 2014. Do Deep Nets Really Need
to be Deep? ArXiv:1312.6184 [cs].
Cettolo, M.; Federico, M.; Bentivogli, L.; Niehues, J.;
Stüker, S.; Sudoh, K.; Yoshino, K.; and Federmann, C. 2017.
Overview of the IWSLT 2017 Evaluation Campaign.
Hinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the
Knowledge in a Neural Network. ArXiv:1503.02531 [cs,
stat].
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
BLEU: A Method for Automatic Evaluation of Machine
Translation.
Snoek, J.; Larochelle, H.; and Adams, R. P. 2012. Practical
Bayesian Optimization of Machine Learning Algorithms.
arXiv:1206.2944.
Urban, G.; Geras, K. J.; Kahou, S. E.; Aslan, O.; Wang, S.;
Caruana, R.; Mohamed, A.; Philipose, M.; and Richardson,
M. 2017. Do Deep Convolutional Nets Really Need to be
Deep and Convolutional? ArXiv:1603.05691 [cs, stat].
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention Is All You Need. arXiv:1706.03762.

--- TRANG 4 ---
Phụ lục
Phụ lục A: Lựa chọn các mạng thay thế
Ở đây chúng tôi cung cấp mô tả chi tiết hơn về việc tích hợp các mạng feed-forward (FF) vào kiến trúc Transformer, và việc thay thế tiếp theo các module attention. Tất cả các phương pháp thay thế đều dựa trên việc thay thế các phần khác nhau của module attention:

•Attention Layer Replacement (ALR): chỉ thay thế block multi-head attention (MHA) bằng một mạng FF, giữ nguyên residual connection và layer normalization.

•Attention Layer with Residual Connection Replacement (ALRR): Module MHA, cùng với residual connection được thay thế bằng mạng FF. Phương pháp này hiệu quả loại bỏ residual connection khi FF Network được thay thế trong Transformer.

•Attention Separate heads Layer Replacement (ASLR): Như một biến thể của ALR, phương pháp này thay thế từng head đơn lẻ của module MHA bằng một mạng FF riêng biệt.

•Encoder Layer Replacement(ELR): Thay thế toàn bộ lớp Encoder.

Sau đây là mô tả, động lực và đặc điểm của từng phương pháp nêu trên. Các phương pháp ALR và ALRR được thiết kế theo cách tách biệt các hiệu ứng và lợi ích của attention layer khỏi các lợi ích mang lại bởi residual connection. Hơn nữa, việc bắt chước hành vi của từng head riêng biệt, được đặc trưng trong phương pháp ASLR, làm cho việc thay thế FF trở nên tương tự hơn với phương pháp MHA gốc. Cuối cùng, như mức độ trừu tượng cao nhất, toàn bộ encoder block được thay thế bằng một FF Network trong phương pháp ELR. Điều này về cơ bản đảo lộn kiến trúc encoder gốc, biến nó thành một chuỗi các mạng FF - một cho mỗi block của encoder. Bằng cách kiểm tra tất cả các mức độ thay thế thông qua thí nghiệm, chúng tôi đã có được một nghiên cứu ablation có cơ sở vững chắc về các thay thế cơ chế attention có thể.

Như một module thay thế trong tất cả các trường hợp này, mạng FF đơn giản, nông một lớp ẩn được sử dụng. Đối với tất cả các phương pháp nêu trên, các mạng FF có kích thước khác nhau đã được thiết kế, như được đưa ra trong Bảng 1. Số lượng tham số gốc cho attention layer (60,000 tham số trong trường hợp của chúng tôi) chủ yếu được vượt quá bởi các mạng thay thế, chủ yếu do đầu vào và đầu ra có kích thước cố định, và định dạng xử lý được yêu cầu bởi mạng FF.

XS S M L
ALR
320K 640K 10M 41M ALRR
ELR
ASLR 290K 1.5M 11.5M 46M

Bảng 1: Số lượng tham số của các kiến trúc được đề xuất. Số được đặc trưng là số tham số cho một mạng FF đơn lẻ trong một lớp. Việc mở rộng thêm các mạng này không mang lại cải thiện trong điểm BLEU. Số lượng tham số cho phương pháp ASLR được trình bày trên cơ sở mỗi attention-head.

Phụ lục B: Chi tiết triển khai
Điểm khởi đầu của quy trình của chúng tôi là việc huấn luyện mô hình Transformer vanilla, bao gồm sáu encoder và sáu decoder. Để giảm thời gian huấn luyện và làm cho việc kiểm tra nhanh hơn, chúng tôi đã giảm kích thước embedding từ 512 xuống 128. Những thay đổi này không làm giảm điểm tổng thể quá nhiều so với điểm BLEU gốc nhưng dẫn đến yêu cầu sức mạnh tính toán thấp hơn đáng kể. Mô hình Transformer này sau đó được sử dụng làm mô hình giáo viên để huấn luyện các mạng feedforward.

Trong Hình 4, bản chất của phương pháp huấn luyện và đánh giá của chúng tôi được trình bày, dựa trên ví dụ về thay thế ALRR, trong khi các phương pháp khác cũng tương tự. Như bước đầu tiên, các activation trung gian (cặp input-output) được trích xuất từ Transformer đã được huấn luyện và được sử dụng làm dữ liệu huấn luyện cho mạng thay thế Feed-Forward mong muốn. Sau đó, chúng phải được điều chỉnh thêm, như được mô tả dưới đây.

Các biến đổi dữ liệu cần thiết Mỗi attention layer biến đổi các biểu diễn từ đầu vào của một câu thành một tổ hợp tuyến tính của các giá trị được trích xuất bởi biểu diễn đầu vào. Để bắt chước hành vi này, mạng FF nhận các biểu diễn từ được nối của một câu làm đầu vào và tạo ra các biểu diễn từ được cập nhật làm đầu ra trong một lần chạy duy nhất. Để xử lý các câu đầu vào có độ dài khác nhau, chúng tôi đã quyết định pad tất cả các câu đến độ dài cố định tối đa và che các giá trị được pad bằng số không để ngăn chúng ảnh hưởng đến suy luận của mô hình. Quá trình này được minh họa trong Hình 5.

--- TRANG 5 ---
Hình 4: Minh họa về các chu kỳ huấn luyện và đánh giá của phương pháp ALRR trong encoder self-attention. Thay thế trong self-attention và cross-attention layer là tương tự. Các phương pháp thay thế khác tuân theo nguyên tắc tương tự, với sự khác biệt là dữ liệu đầu vào và nhãn giáo viên được lấy từ các block khác nhau của encoder, tùy thuộc vào cấu trúc của chúng.

Chúng tôi cũng áp dụng một giới hạn trên cố định cho độ dài câu, mà chúng tôi đặt là 50. Giới hạn này do đó đã giới hạn kích thước của các mạng FF thay thế của chúng tôi. Vì 96% các mẫu trong các bộ dữ liệu có độ dài 50 hoặc ít hơn, các bộ dữ liệu không bị thu nhỏ đáng kể.

Sau khi huấn luyện thành công các mạng, chúng được chèn vào kiến trúc Transformer, thay thế các lớp hiện tại dư thừa, và việc đánh giá được chạy, như được hiển thị trong Hình 4.

Các mạng thay thế Decoder Sự khác biệt chính giữa phương pháp thay thế được sử dụng cho self-attention trong encoder và phương pháp được sử dụng cho self-attention trong decoder là các mạng thay thế FF trong decoder xuất ra các word embedding từng cái một, theo masked self-attention gốc. Mạng xử lý từng từ riêng lẻ bằng cách đưa toàn bộ biểu diễn câu qua mạng và che biểu diễn của các từ đến sau từ được xử lý. Điều này được thực hiện để tính đến khái niệm nhân quả, nơi chỉ những từ trước đó trong câu mới có thể ảnh hưởng đến ý nghĩa của từ hiện tại.

Cross-attention trong decoder chấp nhận cả biểu diễn từ từ các lớp encoder và decoder, tích hợp chúng lại với nhau. Để sao chép quá trình này, biểu diễn từ từ cả encoder và decoder đã được nối lại với nhau và được pad, có kích thước đầu vào tăng gấp đôi so với các mạng thay thế self-attention. Kích thước đầu ra vẫn giữ nguyên, một lần nữa theo thiết kế cross-attention ban đầu.

Chi tiết huấn luyện mạng FF Mỗi phương pháp và mỗi kích thước mạng FF đòi hỏi huấn luyện 6 mạng độc lập cho mỗi block self-attention hoặc cross-attention. Các mạng được huấn luyện trong 20 epoch sử dụng Adam làm optimizer lựa chọn. Learning rate được đặt là 0.001, trong khi batch size là 1400. Các cài đặt huấn luyện được giữ giống nhau cho tất cả các mạng.

Phụ lục C: Bộ dữ liệu và Kết quả
Đối với tất cả các quy trình kiểm tra, bộ dữ liệu IWSLT2017 đã được sử dụng. Nó cung cấp nhiều tập con cho dịch thuật ngôn ngữ, trong đó chúng tôi đã sử dụng các tập con French-English (F2E), English-French (E2F), German-English (G2E), và English-German (E2G). Trung bình, các tập con này bao gồm 200000 câu huấn luyện và 1000 câu kiểm tra, thu được sau khi thu nhỏ kích thước tối đa của câu xuống 50 từ, như đã giải thích ở trên.

Metric được sử dụng cho tất cả các bài kiểm tra là điểm BLEU, vì nó cung cấp một so sánh trực quan với bản dịch do con người cung cấp, đặt chất lượng dịch thuật trên thang điểm chuẩn hóa 0-1, và đại diện cho metric kiểm tra chính cho nhiều nghiên cứu tương tự trong lĩnh vực này.

Trong phần tóm tắt, chúng tôi đã cung cấp kết quả được lấy trung bình trên tất cả 4 bộ dữ liệu, chứng minh tính linh hoạt và mạnh mẽ của các phương pháp được đề xuất. Kết quả thô được hiển thị trong Bảng 3, trong khi các điểm được trình bày trong phần tóm tắt là tương đối so với điểm baseline của Transformer gốc, trên mỗi bộ dữ liệu tương ứng. Các điểm baseline được trình bày trong Bảng 2.

Chúng tôi đã thay thế cơ chế attention theo các cách khác nhau trong encoder, decoder, và cả hai. Kết quả thu được trong quá trình thử nghiệm rộng rãi cho thấy rằng chướng ngại vật duy nhất để thay thế hoàn toàn các cơ chế attention trong Transformer là cross-attention. Như có thể thấy từ bảng 3, điểm BLEU thấp hơn đáng kể nếu cơ chế cross-attention được thay thế

--- TRANG 6 ---
Hình 5: Minh họa về việc tiền xử lý và hậu xử lý dữ liệu cần thiết trước và sau khi lan truyền qua mạng Feed-forward.

bởi các mạng FF. Chúng tôi đã kiểm tra việc thay thế theo ba cách: chỉ trong decoder cross-attention, chỉ trong encoder và decoder (không thay thế cross-attention), và thay thế đầy đủ. Trong tất cả các trường hợp mà decoder cross-attention được thay thế bằng các mạng FF, mô hình cuối cùng hoạt động tệ hơn đáng kể, bất kể các phần khác có được thay thế hay không.

Điều này cho thấy rằng các mạng nông được đề xuất không thể nắm bắt các tương tác phức tạp và tinh tế hơn giữa các chuỗi khác nhau đi vào cơ chế cross-attention. Mặt khác, self-attention đã được mô hình hóa và học thành công.

Phụ lục D: Nghiên cứu trong tương lai
Bằng cách khớp với hiệu suất của Transformer gốc, rất có thể việc tối ưu hóa thêm các siêu tham số của mạng FF bằng cách sử dụng tìm kiếm tham số tiên tiến (ví dụ: sử dụng tối ưu hóa Bayesian (Snoek, Larochelle, and Adams 2012)) có thể mang lại kết quả tốt hơn về chất lượng dịch thuật và thậm chí có thể cho phép sử dụng các mạng FF nhỏ hơn để thay thế, vì kích thước của các mạng đại diện cho một trong những nút thắt cổ chai chính cho việc triển khai các Transformers 'không có attention' này trong thực tế.

Hơn nữa, một hướng tiềm năng khác nằm trong việc huấn luyện các mạng FF phức tạp hơn nhằm mục đích mô hình hóa module cross-attention của decoder, vì mạng nông hiện tại cho thấy rằng, trái ngược với self-attention mà chúng có thể học thành công, cross-attention chứng minh khó khăn hơn do tính phức tạp của nó.

--- TRANG 7 ---
E2G G2E E2F F2E
Transformer 0.257 0.324 0.276 0.292

Bảng 2: Điểm BLEU transformer baseline trên tất cả 4 bộ dữ liệu dịch thuật ngôn ngữ.

E2G G2E E2F F2E
ALREnc
SAXS 0.180 0.235 0.226 0.218
S 0.196 0.257 0.244 0.240
M 0.245 0.320 0.275 0.284
L 0.252 0.327 0.276 0.288
Dec
SAXS 0.227 0.305 0.251 0.273
S 0.240 0.313 0.255 0.281
M 0.252 0.322 0.267 0.290
L 0.253 0.323 0.273 0.291
Dec
CAXS 0.035 0.036 0.035 0.039
S 0.054 0.058 0.042 0.053
M 0.104 0.125 0.089 0.108
L 0.115 0.130 0.109 0.115
E-D
SAXS 0.163 0.222 0.219 0.204
S 0.187 0.247 0.235 0.227
M 0.244 0.313 0.265 0.277
L 0.246 0.321 0.270 0.284
FullXS 0.026 0.027 0.026 0.032
S 0.041 0.053 0.048 0.044
M 0.102 0.122 0.083 0.107
L 0.105 0.134 0.117 0.116
ALRREnc
SAXS 0.013 0.010 0.003 0.001
S 0.018 0.013 0.008 0.012
M 0.158 0.181 0.153 0.150
L 0.243 0.315 0.263 0.276
ASLREnc
SAXS 0.245 0.319 0.257 0.272
S 0.250 0.323 0.260 0.285
M 0.251 0.326 0.269 0.289
L 0.252 0.326 0.271 0.290
ELREnc
SAXS 0.012 0.010 0.011 0.011
S 0.016 0.015 0.132 0.012
M 0.116 0.120 0.124 0.110
L 0.194 0.248 0.225 0.219

Bảng 3: Điểm BLEU của tất cả các phương pháp được đề xuất, trong tất cả các kích thước và trên tất cả các bộ dữ liệu được sử dụng. Các từ viết tắt được sử dụng cho mục đích rõ ràng, và được giải thích trong văn bản sau: "Enc" viết tắt cho encoder, "Dec" viết tắt cho decoder, "SA" viết tắt cho self-attention, "CA" viết tắt cho cross-attention, và E-D viết tắt cho encoder và decoder.
