I'll translate this research paper to Vietnamese while maintaining the exact structure. This is a very long document, so I'll provide the translation section by section.

# 2305.17328.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2305.17328.pdf
# Kích thước tệp: 12108957 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Zero-TPrune: Cắt Tỉa Token Không Cần Huấn Luyện thông qua Tận Dụng Đồ Thị Chú Ý trong Transformer Đã Được Huấn Luyện Trước

Hongjie Wang, Bhishma Dedhia, Niraj K. Jha
Đại học Princeton
Princeton, NJ 08540, USA
{hongjiewang, bdedhia, jha }@princeton.edu

Tóm tắt
Việc triển khai các mô hình Transformer trên các thiết bị biên ngày càng trở nên thách thức do chi phí suy luận tăng theo cấp số nhân và tỷ lệ bậc hai với số lượng token trong chuỗi đầu vào. Cắt tỉa token là một giải pháp mới nổi để giải quyết thách thức này do tính dễ triển khai trên các backbone Transformer khác nhau. Tuy nhiên, hầu hết các phương pháp cắt tỉa token đòi hỏi việc tinh chỉnh tốn kém về mặt tính toán, điều này không mong muốn trong nhiều trường hợp triển khai biên. Trong nghiên cứu này, chúng tôi đề xuất Zero-TPrune, phương pháp zero-shot đầu tiên xem xét cả tầm quan trọng và sự tương tự của token trong việc thực hiện cắt tỉa token. Nó tận dụng đồ thị chú ý của các mô hình Transformer đã được huấn luyện trước để tạo ra phân phối tầm quan trọng cho token thông qua thuật toán Weighted Page Rank (WPR) được đề xuất. Phân phối này tiếp tục hướng dẫn việc phân vùng token để cắt tỉa hiệu quả dựa trên sự tương tự. Do loại bỏ chi phí tinh chỉnh, Zero-TPrune có thể cắt tỉa các mô hình lớn với chi phí tính toán không đáng kể, chuyển đổi giữa các cấu hình cắt tỉa khác nhau mà không tốn chi phí tính toán, và thực hiện điều chỉnh siêu tham số một cách hiệu quả. Chúng tôi đánh giá hiệu suất của Zero-TPrune trên các tác vụ thị giác bằng cách áp dụng nó vào các backbone vision Transformer khác nhau và kiểm tra chúng trên ImageNet. Không cần bất kỳ tinh chỉnh nào, Zero-TPrune giảm chi phí FLOPS của DeiT-S xuống 34.7% và cải thiện thông lượng của nó lên 45.3% chỉ với 0.4% mất mát độ chính xác. So với các phương pháp cắt tỉa tiên tiến yêu cầu tinh chỉnh, Zero-TPrune không chỉ loại bỏ nhu cầu tinh chỉnh sau cắt tỉa mà còn làm như vậy chỉ với 0.1% mất mát độ chính xác. So với các phương pháp cắt tỉa không cần tinh chỉnh tiên tiến, Zero-TPrune giảm mất mát độ chính xác lên đến 49% với ngân sách FLOPS tương tự.
Trang web dự án: https://jha-lab.github.io/zerotprune.

1. Giới thiệu
Kiến trúc Transformer [37] đã nổi lên như một công cụ làm việc de facto của các mô hình học máy đương đại, cho thấy khả năng tổng quát hóa ấn tượng trên một loạt các tác vụ bao gồm Thị giác Máy tính (CV) [34], xử lý ngôn ngữ tự nhiên (NLP) [10], robot [31], và trò chơi [26]. Trung tâm của kiến trúc này là cơ chế tự chú ý đa đầu tự động tổng hợp các token được xử lý song song, tạo ra một khung tính toán mục đích chung rất hiệu quả. Những tác động đặc biệt rõ ràng trong trường hợp CV, nơi khả năng của Transformer trong việc đồng hóa các trừu tượng phong phú từ dữ liệu quy mô lớn tạo điều kiện chuyển giao mạnh mẽ đến các tác vụ hạ nguồn, vượt trội hơn các Mạng Nơ-ron Tích chập (CNN) tiên tiến [12].

Các nghiên cứu về quy luật mở rộng thực nghiệm cho Vision Transformer (ViT) [41] chỉ ra khả năng cải thiện hiệu suất mô hình với dung lượng mô hình; các mô hình gần đây thực sự đã được mở rộng đến hàng tỷ tham số. Trong khi việc mở rộng mô hình mang lại lời hứa về khả năng tổng quát hóa đáng chú ý, nó đặt ra những trở ngại nghiêm trọng cho việc triển khai các kiến trúc như vậy trên các thiết bị hạn chế về tính toán như biên và thực hiện khối lượng công việc suy luận thời gian thực dưới năng lượng và bộ nhớ hạn chế. Làm thế nào để giảm độ phức tạp tính toán của quá trình lan truyền thuận trong khi vẫn duy trì sự phong phú của các biểu diễn đã học? Để đạt được mục tiêu này, Cắt tỉa Token mở ra một hướng đi đầy hứa hẹn. Rút ra một sự tương tự đơn giản với hệ thống thị giác con người, khi cố gắng nhận dạng một con chim kỳ lạ đậu trên cửa sổ trong một buổi chiều bình yên, chúng ta có xu hướng cắt bỏ những chi tiết thị giác không quan trọng như tách trà nóng nằm gần đó, những người đi bộ tản mạn trên lối đi hoặc tán lá được ánh nắng chiếu sáng ở phía sau. Các đầu chú ý gây ra độ phức tạp tính toán bậc hai so với độ dài chuỗi đầu vào. Do đó, việc cắt tỉa các token không quan trọng có thể dẫn đến tăng tốc đáng kể, đặc biệt trong trường hợp các chuỗi dài hơn. Vì cắt tỉa token chỉ cắt tỉa thông tin đi qua các lớp tuần tự của Transformer và không đòi hỏi sửa đổi kiến trúc cho backbone, nó có thể được triển khai rộng rãi trên hầu hết các backbone Transformer và bất kỳ phần cứng tính toán nào có thể khai thác hoàn toàn độ thưa thớt kết quả. Tuy nhiên, hầu hết các phương pháp cắt tỉa token hiện tại dựa vào các mô-đun chấm điểm token phải được huấn luyện cùng với backbone, đòi hỏi việc huấn luyện lại hoặc tinh chỉnh tốn kém về mặt tính toán để triển khai. Điều này không thực tế cho các ứng dụng và người dùng biên, do sự khan hiếm tài nguyên tính toán. Ví dụ, phương pháp cắt tỉa token tiên tiến DynamicViT [30] yêu cầu 150 giờ tinh chỉnh trên GPU NVIDIA A100 để cắt tỉa mô hình DeiT-S [36]. Hơn nữa, tài nguyên bộ nhớ và tính toán có sẵn có thể khác nhau rộng rãi giữa các thiết bị biên; chúng cũng có thể có sự thay đổi lớn về yêu cầu thông lượng. Các phương pháp cắt tỉa yêu cầu tinh chỉnh cần huấn luyện mô hình lặp đi lặp lại dưới các cấu hình cắt tỉa khác nhau được áp đặt bởi các ràng buộc phần cứng, như được hiển thị trong Hình 1, làm cho quá trình cắt tỉa thậm chí còn đắt đỏ hơn. Ngoài ra, các phương pháp này không thực tế để cắt tỉa các mô hình rất lớn do chi phí tính toán cao của việc huấn luyện sau cắt tỉa. Ví dụ, hàng nghìn giờ GPU A100 cần thiết để áp dụng DynamicViT [30] cho các mô hình DeiT-B và DeiT-L [36].

Trong nghiên cứu này, chúng tôi đề xuất một phương pháp cắt tỉa token zero-shot không cần huấn luyện được gọi là Zero-TPrune. Làm thế nào để cắt tỉa token mà không cần tinh chỉnh? Sự chú ý mềm giữa các token tạo ra một đồ thị có hướng với token là nút và chú ý là cạnh. Cường độ cạnh chỉ ra giá trị chú ý. Chúng tôi đặt giả thuyết và sau đó chứng minh thông qua một tập hợp thí nghiệm nghiêm ngặt và toàn diện rằng đồ thị chú ý là một nguồn thông tin phong phú để suy luận các token quan trọng và ngược lại, các token có thể dễ dàng bị cắt tỉa. Làm thế nào để xác định các token quan trọng từ đồ thị chú ý? Trọng số của các cạnh có hướng trên đồ thị chú ý có thể được hiểu là khối lượng định tuyến thông tin giữa các nút. Sử dụng giả định cơ bản rằng các token quan trọng khác chú ý đến các token quan trọng, chúng tôi lặp đi lặp lại gán tầm quan trọng tương đối cho các token. Các phương pháp xếp hạng như vậy [4] đã được sử dụng phổ biến bởi các công cụ tìm kiếm để tổ chức các trang web trên Internet. Liệu có thể khai thác thêm sự dư thừa giữa các token? Các thí nghiệm của chúng tôi cho thấy rằng các token thường học các trừu tượng tương tự và do đó, các bản sao của cùng một đặc trưng có thể được cắt tỉa mà không mất thông tin. Chúng tôi bổ sung xếp hạng tầm quan trọng với cắt tỉa dựa trên sự tương tự để tính đến các token tương tự. Mặc dù Zero-TPrune có thể được áp dụng cho bất kỳ tác vụ dựa trên Transformer nào, chúng tôi tập trung vào các tác vụ thị giác để đánh giá hiệu suất của nó trong bài viết này.

Các đóng góp chính của nghiên cứu này có thể được tóm tắt như sau. (1) Chúng tôi trình bày Zero-TPrune, một phương pháp cắt tỉa token zero-shot hiệu quả tận dụng khả năng nhận dạng đặc trưng (xem xét ma trận chú ý như ma trận kề của một đồ thị có hướng) của các Transformer đã được huấn luyện trước. Nó khai thác cả tầm quan trọng và sự tương tự của token để thực hiện cắt tỉa. (2) Chúng tôi sử dụng tín hiệu đồ thị để biểu diễn phân phối điểm tầm quan trọng trên token và đề xuất thuật toán Weighted Page Rank (WPR) để suy luận các token không quan trọng trong quá trình gán tầm quan trọng lặp. Sơ đồ lặp này giảm nhiễu từ các token không quan trọng trong quá trình gán. (3) Được hướng dẫn bởi phân phối tầm quan trọng, chúng tôi phân vùng token thành hai nhóm và thực hiện cắt tỉa dựa trên sự tương tự. Việc phân vùng phụ thuộc đầu vào kiểm soát phân phối tầm quan trọng của các token được cắt tỉa bởi metric tương tự. (4) Chúng tôi áp dụng Zero-TPrune và các phương pháp cơ sở cho các backbone Transformer khác nhau và đánh giá hiệu suất của chúng trên ImageNet [9]. Các backbone được sử dụng bao gồm DeiT [36], LV-ViT [18], AugReg [33], v.v. So với các phương pháp cắt tỉa Transformer tiên tiến yêu cầu tinh chỉnh, Zero-TPrune loại bỏ nhu cầu tinh chỉnh sau khi cắt tỉa DeiT-S chỉ với khoảng 0.1% giảm độ chính xác trong khi đạt được cùng một mức tiết kiệm FLOPS. Hơn nữa, Zero-TPrune vượt trội hơn các phương pháp không cần tinh chỉnh tiên tiến về cả độ chính xác và thông lượng. Zero-TPrune giảm mất mát độ chính xác 33% trên DeiT-S khi so với các phương pháp không cần tinh chỉnh tiên tiến. Về thông lượng, Zero-TPrune cung cấp tăng tốc 45.3% off-the-shelf với chi phí chỉ 0.4% độ chính xác.

2. Các Nghiên cứu Liên quan
Trong vài năm đầu sau khi mô hình Transformer được đề xuất vào năm 2017, nó chủ yếu được sử dụng trong lĩnh vực NLP [10]. ViT [12] là nghiên cứu đầu tiên áp dụng trực tiếp kiến trúc Transformer chỉ có bộ mã hóa cho các patch hình ảnh không chồng lấp trong tác vụ phân loại hình ảnh mà không sử dụng bất kỳ phép toán tích chập nào. So với các CNN tiên tiến, ViT có thể đạt được hiệu suất tốt hơn thông qua huấn luyện trước quy mô lớn. DeiT [36] là một Transformer không có tích chập khác được huấn luyện chỉ trên ImageNet [9] và đạt được hiệu suất tốt hơn ViT bằng cách dựa vào một số kỹ thuật huấn luyện. Cả ViT và các kiến trúc tiếp theo của nó đều chia hình ảnh đầu vào thành nhiều patch hình ảnh không chồng lấp và chuyển đổi chúng thành token để xử lý tiếp. Điều này cung cấp một chiều kích mới để khai thác độ thưa thớt khá khác với các kỹ thuật tăng cường độ thưa thớt được sử dụng trong CNN.

Hầu hết các nghiên cứu cắt tỉa token trước đây tập trung vào các tác vụ NLP, bao gồm PoWER-BERT [15], Length-Adaptive Transformer [19], SpAtten [39], TR-BERT [42], và Learned Token Pruning [20]. Đối với các tác vụ CV, một nghiên cứu cắt tỉa token điển hình là DynamicViT [30]. Nó chèn các mô-đun dự đoán giữa các khối transformer để dự đoán và loại bỏ các token ít thông tin hơn. Các mô-đun dự đoán là các mạng nơ-ron có thể được huấn luyện chung với backbone vision Transformer. Thay vì sử dụng chiến lược xác định để cắt tỉa token, A-ViT [43] giới thiệu một quá trình cắt tỉa ngẫu nhiên. Nó sử dụng các mô-đun dừng thích ứng để tính xác suất dừng cho mỗi token. Một token được cắt tỉa (tức là loại bỏ) khi đạt đến điều kiện dừng. Kết quả là, số lượng token được giảm dần, dẫn đến suy luận nhanh hơn. Các nghiên cứu gần đây khác về cắt tỉa token cho ViT bao gồm SPViT [21], TPS [40], Adaptive Sparse ViT [24], DToP [35], và HeatViT [11]. Mặc dù các phương pháp cắt tỉa được đề cập ở trên yêu cầu ít hoặc thậm chí không có tham số bổ sung để cắt tỉa, chúng yêu cầu tinh chỉnh tốn kém về mặt tính toán sau cắt tỉa. Ngược lại, Zero-TPrune được đề xuất có thể loại bỏ quá trình huấn luyện sau cắt tỉa chỉ với 0.1% giảm độ chính xác.

Có một số nghiên cứu trước đây đã khám phá việc cắt tỉa token mà không yêu cầu tinh chỉnh. ATS [14] sử dụng phép biến đổi nghịch đảo để thực hiện lấy mẫu token thích ứng dựa trên phân phối điểm tầm quan trọng. Khi điểm tầm quan trọng tập trung vào một số token, số lượng token được lấy mẫu tự động giảm. Tuy nhiên, ATS chỉ sử dụng xác suất chú ý của token phân loại (CLS) trong ma trận chú ý và bỏ qua ảnh hưởng của sự tương tự giữa các token. ToMe [3], mặt khác, tập trung vào việc hợp nhất token thay vì cắt tỉa chúng, từ đó giảm chi phí suy luận của các Transformer đã được huấn luyện trước mà không cần tinh chỉnh. Các token được hợp nhất tiến bộ dựa trên sự tương tự của chúng khi các lớp trở nên sâu hơn. Tuy nhiên, ToMe chỉ dựa vào các vector nhúng từ các mô hình đã được huấn luyện trước và quá trình khớp của nó thiếu hướng dẫn phù hợp (chi tiết hơn trong Phần 3.3). Ngược lại, Zero-TPrune hiệu quả sử dụng cả ma trận chú ý hoàn chỉnh và các vector nhúng từ các Transformer đã được huấn luyện trước, đồng thời xem xét tầm quan trọng và sự tương tự của token.

3. Phương pháp luận
Trong phần này, chúng tôi đầu tiên cung cấp tổng quan về Zero-TPrune trong Phần 3.1, sau đó mô tả các thành phần của nó, I-stage (Phần 3.2) và S-stage (Phần 3.3). Lưu ý rằng Zero-TPrune có khả năng vi phân, điều này cho phép mô hình đã cắt tỉa được tinh chỉnh thêm để có hiệu suất tốt hơn. Mô hình huấn luyện-sau-cắt tỉa tùy chọn này được mô tả trong Tài liệu Bổ sung Phần C.

3.1. Tổng quan: Zero-TPrune
Khung Zero-TPrune tổng thể được hiển thị trong Hình 2. Mỗi lớp cắt tỉa được tạo thành từ nhiều giai đoạn và có thể được chèn vào bất kỳ đâu giữa các khối Transformer. I-stage và S-stage cho phép Zero-TPrune xem xét cả tầm quan trọng và sự tương tự. Mục tiêu của I-stage là có được phân phối điểm tầm quan trọng trên token và giữ lại top-k token quan trọng. Để đạt được mục tiêu này, chúng tôi đề xuất thuật toán WPR và sử dụng ma trận chú ý từ khối Transformer đã được huấn luyện trước. Trong S-stage, chúng tôi đo sự tương tự giữa các token dựa trên các vector nhúng của chúng và chỉ giữ lại một token trong top-r cặp tương tự. Để giảm chi phí tính toán từ tất cả các tổ hợp theo cặp, chúng tôi phân vùng token thành các nhóm lưỡng phân. Các token trong cùng một nhóm không bao giờ được ghép để đo sự tương tự. Để có kiểm soát tốt hơn đối với phân phối tầm quan trọng của các token đã cắt tỉa, chúng tôi hướng dẫn việc phân vùng bằng thứ hạng tầm quan trọng của chúng.

Một cách đơn giản để kết hợp hai giai đoạn là kết nối liên tiếp I-stage và S-stage: một số token được cắt tỉa dựa trên phân phối điểm tầm quan trọng đã có trong I-stage; phân phối này sau đó được sử dụng để hướng dẫn việc phân vùng trong S-stage và một số token khác được cắt tỉa dựa trên sự tương tự. Tuy nhiên, chúng tôi quan sát thực nghiệm rằng sự kết hợp tầm thường như vậy có thể khiến các token không quan trọng về mặt ngữ nghĩa cuối cùng đẩy ra các token có ý nghĩa ngữ nghĩa trong I-stage. Ví dụ, đôi khi các token nền nhận được điểm tầm quan trọng cao so với các token đối tượng chính. Chi tiết của hiện tượng này có thể được tìm thấy trong Tài liệu Bổ sung Phần A.1. Chúng tôi giải quyết vấn đề này bằng cách hoán đổi I-stage và S-stage. Phương pháp này cho phép loại bỏ sớm các token tương tự trong S-stage, do đó giảm đáng kể tác động bất lợi của sự tương tự trong I-stage. Chúng tôi trình bày so sánh của hai mô hình trong Tài liệu Bổ sung Phần A.2. Để tạo điều kiện cho việc phân vùng trong S-stage, chúng tôi giới thiệu I'-stage tiền xếp hạng để gán điểm tầm quan trọng cho token với một vòng bỏ phiếu duy nhất. Đáng chú ý, không có token nào bị cắt tỉa trong I'-stage. Do đó, lớp cắt tỉa bao gồm việc áp dụng tuần tự I'-stage, S-stage và I-stage.

3.2. I-stage: Cắt tỉa dựa trên tầm quan trọng
Để giữ lại top-k token quan trọng, chúng tôi giới thiệu một metric xếp hạng được gọi là điểm tầm quan trọng. Để có được điểm tầm quan trọng, chúng tôi coi ma trận chú ý A(h,l) như ma trận kề của một đồ thị hoàn chỉnh, có hướng, được gọi là đồ thị chú ý, như được hiển thị trong Hình 3(a). Xếp hạng các nút trong đồ thị là thách thức vì một số lý do. (i) Đồ thị chú ý dày đặc, thường bao gồm hàng trăm nút và nhiều cạnh hơn khi đầu vào là một hình ảnh. (ii) Chúng tôi có ngân sách nghiêm ngặt cho chi phí tính toán phát sinh bởi thuật toán per-image trong quá trình suy luận.

Lấy cảm hứng từ thuật toán Page Rank [4], chúng tôi đề xuất thuật toán WPR để rút ra điểm tầm quan trọng. Page Rank được sử dụng trong Google Search để xếp hạng các trang web. Trong thuật toán Page Rank gốc, các liên kết giữa các trang web không có trọng số. Để áp dụng nó vào đồ thị chú ý có trọng số và có hướng, chúng tôi xem xét tín hiệu của mỗi nút trong đồ thị này như tầm quan trọng của mỗi token. Chúng tôi khởi tạo tín hiệu đồ thị một cách đồng nhất và sử dụng ma trận kề như một Graph Shifting Operator (GSO). Khi GSO được áp dụng cho tín hiệu đồ thị, mỗi nút bỏ phiếu cho nút nào quan trọng hơn thông qua trọng số được gán cho các cạnh đầu ra, tức là sự chú ý mà một token dành cho các token khác. Nếu bản thân nút quan trọng hơn, việc bỏ phiếu của nút này quan trọng hơn. Điều này được hiển thị trong Thuật toán 1. Quá trình chuyển đổi từ khởi tạo đến hội tụ được hiển thị trong Hình 3(b).

Chúng tôi có được biểu thức cho điểm tầm quan trọng của mỗi nút (tức là token) trong lớp l-th, đầu h-th như sau:

s(h,l)(xi) = 1/N ∑(j=1 to N) A(h,l)(xi, xj)·s(h,l)(xj)     (1)

trong đó s(h,l)(xj) là điểm tầm quan trọng của nút xi trong đầu h-th của lớp l-th, và N là số lượng token trong lớp l-th. s(h,l)(xi) được rút ra từ tổng có trọng số của sự chú ý nhận được. WPR do đó đệ quy gán tầm quan trọng cao cho các token có ý nghĩa ngữ nghĩa và giảm nhiễu từ các token không quan trọng về mặt ngữ nghĩa yếu. Chúng tôi giữ lại top-k token quan trọng (k được xác định bởi tỷ lệ giữ lại và tổng số token).

[Tiếp tục với phần dịch còn lại...]
