# GQA: Huấn luyện các mô hình Transformer Multi-Query tổng quát từ các checkpoint Multi-Head

Joshua Ainslie∗, James Lee-Thorp∗, Michiel de Jong∗ † †
Yury Zemlyanskiy ,Federico Lebrón ,Sumit Sanghai
Google Research

## Tóm tắt
Cơ chế chú ý multi-query (MQA), chỉ sử dụng một key-value head duy nhất, tăng tốc đáng kể quá trình suy luận của decoder. Tuy nhiên, MQA có thể dẫn đến suy giảm chất lượng, và hơn nữa có thể không mong muốn phải huấn luyện một mô hình riêng chỉ để suy luận nhanh hơn. Chúng tôi (1) đề xuất một công thức để uptraining các checkpoint mô hình ngôn ngữ multi-head hiện có thành các mô hình có MQA sử dụng 5% tính toán pre-training ban đầu, và (2) giới thiệu cơ chế chú ý grouped-query (GQA), một sự tổng quát hóa của multi-query attention sử dụng một số lượng trung gian (nhiều hơn một, ít hơn số lượng query heads) các key-value heads. Chúng tôi chỉ ra rằng GQA được uptrained đạt chất lượng gần với multi-head attention với tốc độ tương đương MQA.

## 1 Giới thiệu
Suy luận decoder tự hồi quy là một nút cổ chai nghiêm trọng đối với các mô hình Transformer do chi phí băng thông bộ nhớ từ việc tải trọng số decoder và tất cả các attention keys và values tại mỗi bước giải mã (Shazeer, 2019; Pope et al., 2022; de Jong et al., 2022). Băng thông bộ nhớ từ việc tải keys và values có thể được giảm mạnh thông qua multi-query attention (Shazeer, 2019), sử dụng nhiều query heads nhưng chỉ có một key và value head.

Tuy nhiên, multi-query attention (MQA) có thể dẫn đến suy giảm chất lượng và bất ổn trong huấn luyện, và có thể không khả thi để huấn luyện các mô hình riêng được tối ưu hóa cho chất lượng và suy luận. Hơn nữa, trong khi một số mô hình ngôn ngữ đã sử dụng multi-query attention, chẳng hạn như PaLM (Chowdhery et al., 2022), nhiều mô hình khác thì không, bao gồm các mô hình ngôn ngữ có sẵn công khai như T5 (Raffel et al., 2020) và LLaMA (Touvron et al., 2023).

Công trình này chứa hai đóng góp cho việc suy luận nhanh hơn với các mô hình ngôn ngữ lớn. Đầu tiên, chúng tôi chỉ ra rằng các checkpoint mô hình ngôn ngữ với multi-head attention (MHA) có thể được uptrained (Komatsuzaki et al., 2022) để sử dụng MQA với một phần nhỏ tính toán huấn luyện ban đầu. Điều này đưa ra một phương pháp hiệu quả về chi phí để có được các checkpoint multi-query nhanh cũng như MHA chất lượng cao.

Thứ hai, chúng tôi đề xuất grouped-query attention (GQA), một phép nội suy giữa multi-head và multi-query attention với một key và value head cho mỗi nhóm con của query heads. Chúng tôi chỉ ra rằng GQA được uptrained đạt chất lượng gần với multi-head attention trong khi gần như nhanh bằng multi-query attention.

## 2 Phương pháp

### 2.1 Uptraining
Tạo ra một mô hình multi-query từ một mô hình multi-head diễn ra trong hai bước: đầu tiên, chuyển đổi checkpoint, và thứ hai, pre-training bổ sung để cho phép mô hình thích nghi với cấu trúc mới của nó. Hình 1 cho thấy quy trình chuyển đổi một checkpoint multi-head thành một checkpoint multi-query. Các ma trận chiếu cho key và value heads được gộp trung bình thành các ma trận chiếu đơn, mà chúng tôi thấy hoạt động tốt hơn so với việc chọn một key và value head duy nhất hoặc khởi tạo ngẫu nhiên các key và value heads mới từ đầu.

**Hình 1**: Tổng quan về việc chuyển đổi từ multi-head sang multi-query attention. Các ma trận chiếu key và value từ tất cả các heads được gộp trung bình thành một head duy nhất.

Checkpoint đã chuyển đổi sau đó được pre-trained cho một tỷ lệ nhỏ α của các bước huấn luyện ban đầu trên cùng một công thức pre-training.

### 2.2 Grouped-query attention
Grouped-query attention chia các query heads thành G nhóm, mỗi nhóm chia sẻ một key head và value head duy nhất. GQA-G đề cập đến grouped-query với G nhóm. GQA-1, với một nhóm duy nhất và do đó một key và value head duy nhất, tương đương với MQA, trong khi GQA-H, với số nhóm bằng số lượng heads, tương đương với MHA. Hình 2 cho thấy so sánh giữa grouped-query attention và multi-head/multi-query attention. Khi chuyển đổi một checkpoint multi-head thành một checkpoint GQA, chúng tôi xây dựng mỗi group key và value head bằng cách gộp trung bình tất cả các heads gốc trong nhóm đó.

**Hình 2**: Tổng quan về phương pháp grouped-query. Multi-head attention có H query, key, và value heads. Multi-query attention chia sẻ một key và value head duy nhất across tất cả query heads. Grouped-query attention thay vào đó chia sẻ một key và value head duy nhất cho mỗi nhóm query heads, nội suy giữa multi-head và multi-query attention.

Một số lượng nhóm trung gian dẫn đến một mô hình nội suy có chất lượng cao hơn MQA nhưng nhanh hơn MHA, và, như chúng tôi sẽ chỉ ra, đại diện cho một sự đánh đổi thuận lợi. Việc chuyển từ MHA sang MQA giảm H key và value heads xuống một key và value head duy nhất, giảm kích thước của key-value cache và do đó lượng dữ liệu cần được tải theo một hệ số H. Tuy nhiên, các mô hình lớn hơn thường mở rộng số lượng heads, sao cho multi-query attention đại diện cho một cắt giảm tích cực hơn trong cả băng thông bộ nhớ và dung lượng. GQA cho phép chúng tôi giữ cùng một mức giảm tỷ lệ trong băng thông và dung lượng khi kích thước mô hình tăng.

Hơn nữa, các mô hình lớn hơn chịu tổn thất tương đối ít hơn từ chi phí băng thông bộ nhớ từ attention, vì KV-cache tỷ lệ với chiều mô hình trong khi FLOPs và tham số mô hình tỷ lệ với bình phương chiều mô hình. Cuối cùng, việc sharding tiêu chuẩn cho các mô hình lớn nhân bản một key và value head duy nhất theo số lượng phân vùng mô hình (Pope et al., 2022); GQA loại bỏ sự lãng phí từ việc phân vùng như vậy. Do đó, chúng tôi kỳ vọng GQA sẽ đưa ra một sự đánh đổi đặc biệt tốt cho các mô hình lớn hơn.

Chúng tôi lưu ý rằng GQA không được áp dụng cho các lớp encoder self-attention; các biểu diễn encoder được tính toán song song, và băng thông bộ nhớ do đó thường không phải là nút cổ chai chính.

## 3 Thí nghiệm

### 3.1 Thiết lập thí nghiệm
**Cấu hình** Tất cả các mô hình đều dựa trên kiến trúc T5.1.1 (Raffel et al., 2020), được triển khai với JAX (Bradbury et al., 2018), Flax (Heek et al., 2020), và Flaxformer¹. Cho các thí nghiệm chính, chúng tôi xem xét T5 Large và XXL với multi-head attention, cũng như các phiên bản uptrained của T5 XXL với multi-query và grouped-query attention. Chúng tôi sử dụng bộ tối ưu hóa Adafactor với cùng các siêu tham số và lịch trình tỷ lệ học như T5 (Raffel et al., 2020). Chúng tôi áp dụng MQA và GQA cho decoder self-attention và cross-attention, nhưng không cho encoder self-attention.

**Uptraining** Các mô hình uptrained được khởi tạo từ các checkpoint T5.1.1 công khai. Các key và value heads được gộp trung bình thành cấu trúc MQA hoặc GQA phù hợp, và sau đó được pre-trained thêm α tỷ lệ các bước pre-training ban đầu với thiết lập pre-training ban đầu và bộ dữ liệu từ (Raffel et al., 2020). Với α = 0.05, việc huấn luyện mất khoảng 600 TPUv3 chip-days.

**Dữ liệu** Chúng tôi đánh giá trên các bộ dữ liệu tóm tắt CNN/Daily Mail (Nallapati et al., 2016), arXiv và PubMed (Cohan et al., 2018), MediaSum (Zhu et al., 2021), và Multi-News (Fabbri et al., 2019); bộ dữ liệu dịch WMT 2014 English-to-German; và bộ dữ liệu hỏi đáp TriviaQA (Joshi et al., 2017). Chúng tôi không đánh giá trên các benchmark phân loại phổ biến như GLUE (Wang et al., 2019) vì suy luận tự hồi quy ít áp dụng cho các nhiệm vụ đó.

**Fine-tuning** Cho fine-tuning, chúng tôi sử dụng tỷ lệ học không đổi 0.001, kích thước batch 128, và tỷ lệ dropout 0.1 cho tất cả các nhiệm vụ. CNN/Daily Mail và WMT sử dụng độ dài đầu vào 512 và độ dài đầu ra 256. Các bộ dữ liệu tóm tắt khác sử dụng độ dài đầu vào 2048 và độ dài đầu ra 512. Cuối cùng, TriviaQA sử dụng độ dài đầu vào 2048 và độ dài đầu ra 32. Chúng tôi huấn luyện cho đến khi hội tụ và chọn checkpoint có hiệu suất dev cao nhất. Chúng tôi sử dụng greedy decoding cho suy luận.

**Timing** Chúng tôi báo cáo thời gian trên mỗi mẫu trên mỗi chip TPUv4, được đo bằng xprof (Google, 2020). Cho các thí nghiệm thời gian, chúng tôi sử dụng 8 TPU với kích thước batch lớn nhất phù hợp lên đến 32 trên mỗi TPU, và song song hóa được tối ưu riêng cho mỗi mô hình.

### 3.2 Kết quả chính
Hình 3 cho thấy hiệu suất trung bình trên tất cả các bộ dữ liệu như một hàm của thời gian suy luận trung bình cho các mô hình MHA T5-Large và T5-XXL, và các mô hình MQA và GQA-8 XXL được uptrained với tỷ lệ uptraining α = 0.05. Chúng tôi thấy rằng một mô hình MQA lớn hơn được uptrained cung cấp một sự đánh đổi thuận lợi so với các mô hình MHA, với chất lượng cao hơn và suy luận nhanh hơn so với MHA-Large. Hơn nữa, GQA đạt được những cải thiện chất lượng đáng kể bổ sung, đạt hiệu suất gần với MHA-XXL với tốc độ gần với MQA. Bảng 1 chứa kết quả đầy đủ cho tất cả các bộ dữ liệu.

**Bảng 1**: So sánh thời gian suy luận và hiệu suất dev set trung bình của các mô hình T5 Large và XXL với multi-head attention, và các mô hình T5-XXL được uptrained 5% với multi-query và grouped-query attention trên các bộ dữ liệu tóm tắt CNN/Daily Mail, arXiv, PubMed, MediaSum, và MultiNews, bộ dữ liệu dịch WMT, và bộ dữ liệu hỏi đáp TriviaQA.

| Mô hình | Tinfer | Trung bình | CNN | arXiv | PubMed | MediaSum | MultiNews | WMT | TriviaQA |
|---------|---------|------------|-----|-------|---------|-----------|-----------|-----|----------|
| | s | R1 | R1 | R1 | R1 | R1 | BLEU | F1 |
| MHA-Large | 0.37 | 46.0 | 42.9 | 44.6 | 46.2 | 35.5 | 46.6 | 27.7 | 78.2 |
| MHA-XXL | 1.51 | 47.2 | 43.8 | 45.6 | 47.5 | 36.4 | 46.9 | 28.4 | 81.9 |
| MQA-XXL | 0.24 | 46.6 | 43.0 | 45.0 | 46.9 | 36.1 | 46.5 | 28.5 | 81.3 |
| GQA-8-XXL | 0.28 | 47.1 | 43.5 | 45.4 | 47.7 | 36.3 | 47.2 | 28.4 | 81.6 |

### 3.3 Ablations
Phần này trình bày các thí nghiệm để điều tra ảnh hưởng của các lựa chọn mô hình hóa khác nhau. Chúng tôi đánh giá hiệu suất trên một mẫu con đại diện của các nhiệm vụ: CNN/Daily Mail, (tóm tắt ngắn), MultiNews (tóm tắt dài), và TriviaQA (hỏi đáp).

**Chuyển đổi checkpoint** Hình 4 so sánh hiệu suất của các phương pháp khác nhau cho việc chuyển đổi checkpoint. Gộp trung bình dường như hoạt động tốt nhất, tiếp theo là chọn một head duy nhất và sau đó là khởi tạo ngẫu nhiên. Một cách trực quan, kết quả được sắp xếp theo mức độ thông tin được bảo tồn từ mô hình đã pre-trained.

**Hình 4**: So sánh hiệu suất của các phương pháp chuyển đổi checkpoint khác nhau cho T5-Large được uptrained thành MQA với tỷ lệ α = 0.05. 'Mean' gộp trung bình các key và value heads, 'First' chọn head đầu tiên và 'Random' khởi tạo heads từ đầu.

**Các bước uptraining** Hình 5 cho thấy hiệu suất thay đổi như thế nào với tỷ lệ uptraining cho T5 XXL với MQA và GQA. Đầu tiên, chúng tôi lưu ý rằng GQA đã đạt hiệu suất hợp lý sau khi chuyển đổi trong khi MQA cần uptraining để có ích. Cả MQA và GQA đều có lợi từ 5% uptraining với lợi ích giảm dần từ 10%.

**Hình 5**: Hiệu suất như một hàm của tỷ lệ uptraining α cho các mô hình T5 XXL với MQA và GQA-8.

**Số lượng nhóm** Hình 6 chứng minh ảnh hưởng của số lượng nhóm GQA đến tốc độ suy luận. Đối với các mô hình lớn hơn, chi phí băng thông bộ nhớ từ KV cache ít hạn chế hơn (Shazeer, 2019), trong khi việc giảm kích thước key-value mạnh hơn do số lượng heads tăng. Kết quả là, tăng số lượng nhóm từ MQA chỉ dẫn đến chậm lại khiêm tốn ban đầu, với chi phí tăng khi chúng ta di chuyển gần hơn đến MHA. Chúng tôi đã chọn 8 nhóm như một nền tảng trung gian thuận lợi.

**Hình 6**: Thời gian trên mỗi mẫu cho GQA-XXL như một hàm của số lượng nhóm GQA với độ dài đầu vào 2048 và độ dài đầu ra 512. Chuyển từ 1 (MQA) sang 8 nhóm thêm chi phí suy luận khiêm tốn, với chi phí tăng khi thêm nhiều nhóm hơn.

## 4 Công trình liên quan
Công trình này tập trung vào việc đạt được sự đánh đổi tốt hơn giữa chất lượng decoder và thời gian suy luận thông qua việc giảm chi phí băng thông bộ nhớ (Williams et al., 2009) từ việc tải keys và values. Shazeer (2019) đầu tiên đề xuất giảm chi phí này thông qua multi-query attention. Các công trình tiếp theo cho thấy rằng multi-query attention đặc biệt hữu ích cho các đầu vào dài (Pope et al., 2022; de Jong et al., 2022). Rabe (2023) độc lập phát triển GQA với triển khai công khai. Các công trình khác đã khám phá việc nhóm các attention heads cho hiệu quả tính toán (Park et al., 2020; Luo et al., 2022; Ni et al., 2023) mà không tập trung cụ thể vào key-value heads, điều này quyết định chi phí băng thông bộ nhớ.

Một số phương pháp khác đã được đề xuất để giảm chi phí băng thông bộ nhớ từ keys và values, cũng như các tham số. Flash attention (Dao et al., 2022) cấu trúc tính toán attention để tránh cụ thể hóa các điểm attention bậc hai, giảm bộ nhớ và tăng tốc huấn luyện. Quantization (Dettmers et al., 2022; Frantar et al., 2022) giảm kích thước của trọng số và activations, bao gồm keys và values, bằng cách giảm độ chính xác. Model distillation (Hinton et al., 2015; Gou et al., 2021) thay vào đó giảm kích thước mô hình ở một độ chính xác nhất định, sử dụng dữ liệu được tạo từ mô hình lớn hơn để finetune mô hình nhỏ hơn. Layer-sparse cross-attention (de Jong et al., 2022) loại bỏ hầu hết các lớp cross-attention chiếm chi phí chính cho các đầu vào dài hơn. Speculative sampling (Chen et al., 2023; Leviathan et al., 2022) cải thiện nút cổ chai băng thông bộ nhớ bằng cách đề xuất nhiều token với một mô hình nhỏ hơn sau đó được chấm điểm song song bởi một mô hình lớn hơn.

Cuối cùng, quy trình uptraining chúng tôi đề xuất được lấy cảm hứng từ Komatsuzaki et al. (2022), uptrains các checkpoint T5 tiêu chuẩn thành các mô hình Mixture-of-Experts được kích hoạt thưa thớt.

## 5 Kết luận
Các mô hình ngôn ngữ tốn kém cho suy luận chủ yếu do chi phí băng thông bộ nhớ từ việc tải keys và values. Multi-query attention giảm chi phí này với cái giá của việc giảm dung lượng và chất lượng mô hình. Chúng tôi đề xuất chuyển đổi các mô hình multi-head attention thành các mô hình multi-query với một phần nhỏ tính toán pre-training ban đầu. Hơn nữa, chúng tôi giới thiệu grouped-query attention, một phép nội suy của multi-query và multi-head attention đạt chất lượng gần với multi-head ở tốc độ tương đương với multi-query attention.

## Hạn chế
Bài báo này tập trung vào việc cải thiện chi phí băng thông bộ nhớ từ việc tải keys và values. Chi phí này quan trọng nhất khi tạo ra các chuỗi dài hơn, mà chất lượng về bản chất khó đánh giá. Cho tóm tắt, chúng tôi sử dụng điểm Rouge, mà chúng tôi biết là một đánh giá có sai sót không kể hết toàn bộ câu chuyện; vì lý do đó, khó có thể chắc chắn rằng các sự đánh đổi của chúng tôi là đúng. Do tính toán hạn chế, chúng tôi cũng không so sánh mô hình GQA XXL của chúng tôi với một mô hình tương đương được huấn luyện từ đầu, vì vậy chúng tôi không biết hiệu suất tương đối của uptraining so với huấn luyện từ đầu. Cuối cùng, chúng tôi đánh giá tác động của uptraining và GQA chỉ trên các mô hình encoder-decoder. Gần đây, các mô hình decoder-only cực kỳ phổ biến, và vì các mô hình này không có self-attention và cross-attention riêng biệt, chúng tôi kỳ vọng GQA sẽ có lợi thế mạnh hơn so với MQA.

## Lời cảm ơn
Chúng tôi cảm ơn Santiago Ontañón, Afroz Mohiuddin, William Cohen và những người khác tại Google Research vì lời khuyên và thảo luận sâu sắc.

## Tài liệu tham khảo
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, và John Jumper. 2023. Accelerating large language model decoding with speculative sampling. CoRR, abs/2302.01318.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, và Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615–621, New Orleans, Louisiana. Association for Computational Linguistics.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. CoRR, abs/2205.14135.

Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, và William Cohen. 2022. FiDO: Fusion-in-decoder optimized for stronger performance and faster inference. arXiv preprint arXiv:2212.08153.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. CoRR, abs/2208.07339.

Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, và Dragomir R. Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1074–1084. Association for Computational Linguistics.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. 2022. GPTQ: accurate post-training quantization for generative pre-trained transformers. CoRR, abs/2210.17323.

Google. 2020. Profile your model with cloud tpu tools. https://cloud.google.com/tpu/docs/cloud-tpu-tools. Accessed: 2022-11-11.

Jianping Gou, Baosheng Yu, Stephen J. Maybank, và Dacheng Tao. 2021. Knowledge distillation: A survey. Int. J. Comput. Vis., 129(6):1789–1819.

Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, và Marc van Zee. 2020. Flax: A neural network library and ecosystem for JAX.

Geoffrey E. Hinton, Oriol Vinyals, và Jeffrey Dean. 2015. Distilling the knowledge in a neural network. CoRR, abs/1503.02531.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, và Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada. Association for Computational Linguistics.

Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, và Neil Houlsby. 2022. Sparse upcycling: Training mixture-of-experts from dense checkpoints.

Yaniv Leviathan, Matan Kalman, và Yossi Matias. 2022. Fast inference from transformers via speculative decoding. CoRR, abs/2211.17192.

Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, và Rongrong Ji. 2022. Towards lightweight transformer via group-wise transformation for vision-and-language tasks. IEEE Trans. Image Process., 31:3386–3398.

Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos Santos, Çaglar Gülçehre, và Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 280–290. ACL.

Jinjie Ni, Rui Mao, Zonglin Yang, Han Lei, và Erik Cambria. 2023. Finding the pillars of strength for multi-head attention. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 14526–14540. Association for Computational Linguistics.

Sungrae Park, Geewook Kim, Junyeop Lee, Junbum Cha, Ji-Hoon Kim, và Hwalsuk Lee. 2020. Scale down transformer by grouping features for a lightweight character-level language model. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 6883–6893. International Committee on Computational Linguistics.

Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, và Jeff Dean. 2022. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102.

Markus Rabe. 2023. Memory-efficient attention. https://github.com/google/flaxformer/blob/main/flaxformer/components/attention/memory_efficient_attention.py. Accessed: 2023-05-23.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.

Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.

Samuel Williams, Andrew Waterman, và David A. Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM, 52(4):65–76.

Chenguang Zhu, Yang Liu, Jie Mei, và Michael Zeng. 2021. Mediasum: A large-scale media interview dataset for dialogue summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 5927–5934. Association for Computational Linguistics.

## A Ổn định Huấn luyện
Chúng tôi thấy rằng multi-query attention có thể dẫn đến bất ổn huấn luyện trong quá trình fine-tuning, đặc biệt là kết hợp với các nhiệm vụ đầu vào dài. Chúng tôi đã huấn luyện nhiều mô hình T5-Large với multi-query attention từ đầu. Trong mỗi trường hợp, pre-training chịu tổn thất từ các đỉnh loss thường xuyên và các mô hình cuối cùng phân kỳ ngay lập tức khi fine-tuning trên các nhiệm vụ đầu vào dài. Các mô hình multi-query attention được uptrained ổn định hơn nhưng vẫn hiển thị độ biến thiên cao, vì vậy cho các mô hình multi-query trên các nhiệm vụ không ổn định, chúng tôi báo cáo hiệu suất trung bình trên ba lần chạy fine-tuning. Tuy nhiên, các mô hình grouped-query attention được uptrained dường như ổn định, vì vậy chúng tôi không điều tra thêm về nguyên nhân gốc rễ của sự bất ổn multi-query.

¹ https://github.com/google/flaxformer
