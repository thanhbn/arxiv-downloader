# 1912.00544.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/1912.00544.pdf
# File size: 156426 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:1912.00544v1  [cs.CL]  2 Dec 2019Multi-Scale Self-Attention for Text Classiﬁcation
Qipeng Guo‡∗, Xipeng Qiu‡ †, Pengfei Liu‡, Xiangyang Xue‡, Zheng Zhang§
‡Shanghai Key Laboratory of Intelligent Information Proces sing, Fudan University
‡School of Computer Science, Fudan University
§AWS Shanghai AI Lab
§New York University Shanghai
{qpguo16, xpqiu, pﬂiu14, xyxue }@fudan.edu.cn, zz@nyu.edu
Abstract
In this paper, we introduce the prior knowledge, multi-scal e
structure, into self-attention modules. We propose a Multi -
Scale Transformer which uses multi-scale multi-head self-
attention to capture features from different scales. Based
on the linguistic perspective and the analysis of pre-train ed
Transformer (BERT) on a huge corpus, we further design
a strategy to control the scale distribution for each layer.
Results of three different kinds of tasks (21 datasets) show
our Multi-Scale Transformer outperforms the standard Tran s-
former consistently and signiﬁcantly on small and moderate
size datasets.
Introduction
Self-Attention mechanism is widely used in text classiﬁca-
tion tasks, and models based on self-attention mechanism
like Transformer (Vaswani et al. 2017), BERT (Devlin et al.
2018) achieves many exciting results on natural language
processing (NLP) tasks, such as machine translation, lan-
guage modeling (Dai et al. 2019), and text classiﬁcation.
Recently, Radford et al. (2018) points out the weakness of
self-attention modules, especially the poor performance o n
small and moderate size datasets. Although the pre-trainin g
on huge corpus could help this, we believe the fundamental
reason is that self-attention module lacks suitable induct ive
bias, so the learning process heavily depends on the train-
ing data. It is hard to learn a model with good generalization
ability from scratch on a limited training set without a good
inductive bias.
Multi-Scale structures are widely used in computer vi-
sion (CV), NLP, and signal processing domains. It can
help the model to capture patterns at different scales and
extract robust features. Speciﬁc to NLP domain, a com-
mon way to implement multi-scale is the hierarchical struc-
ture, such as convolutional neural networks (CNN) (Kalch-
brenner, Grefenstette, and Blunsom 2014), multi-scale re-
current neural networks (RNN) (Chung, Ahn, and Bengio
2016) tree-structured neural networks (Socher et al. 2013;
Tai, Socher, and Manning 2015) and hierarchical attention
∗Work done during internship at AWS Shanghai AI Lab.
†Corresponding author: Xipeng Qiu (xpqiu@fudan.edu.cn).(Yang et al. 2016). The principle behind these models is
the characteristic of language: the high-level feature is t he
composition of low-level terms. With hierarchical structu res,
these models capture the local compositions at lower layers
and non-local composition at high layers. This division of
labor makes the model less data-hungry.
However, for self-attention modules, there is no restric-
tion of composition bias. The dependencies between words
are purely data-driven without any prior, leading to easily
overﬁt on small or moderate size datasets.
In this paper, we propose a multi-scale multi-head self-
attention (MSMSA), in which each attention head has a vari-
able scale. The scale of a head restricts the working area
of self-attention. Intuitively, a large scale makes the fea -
ture involving more contextual information and being more
smooth. A small scale insists on the local bias and encour-
ages the features to be outstanding and sharp. Based on
MSMSA, we further propose multi-scale Transformer, con-
sisting of multiple MSMSA layers. Different from the multi-
scale in hierarchical structures, each layer of multi-scal e
Transformer consists of several attention heads with multi -
ple scales, which brings an ability to capture the multi-sca le
features in a single layer.
Contributions of this paper are:
•We introduce the multi-scale structure into self-attentio n
framework, the proposed model Multi-Scale Transformer
can extract features from different scales.
•Inspired by the hierarchical structure of language, we fur-
ther develop a simple strategy to control the scale distribu -
tion for different layers. Based on the empirical result on
real tasks and the analysis from BERT, we suggest using
more small-scale attention heads in shallow layers and a
balanced choice for different scales in deep layers.
•The building block of Multi-Scale Transformer, multi-
scale multi-head self-attention provides a ﬂexible way to
introduce scale bias (local or global), and it is a replace-
ment of the multi-head self-attention and position-wise
feed-forward networks.
•Results on three tasks (21 datasets) show our Multi-Scale
Transformer outperforms the standard Transformer con-

--- PAGE 2 ---
sistently and signiﬁcantly on small and moderate size
datasets.
Background
Self-attention and its extend architecture, Transformer,
achieved many good results on NLP tasks. Instead of uti-
lizing CNN or RNN unit to model the interaction between
different words, Transformer achieves pair-wised interac tion
by attention mechanism.
Mulit-Head Self-attention The main component of the
Transformer is the multi-head dot-product attention, whic h
could be formalized as follows. Given a sequence of vectors
H∈RN×D, whereNis the length of the sequence and the
Dis the dimension of the vector. When doing multi-head
self-attention, the module projects the Hinto three matri-
ces: the query Q, the key Kand the value V. These three
matrices would be further decomposed into N′sub-spaces
which corresponds to the N′heads and each head has D′
units.
MSA(H) = [ head1,···,headN′]WO, (1)
headi=softmax(QiKT
i√
D′)Vi, (2)
Q=HWQ,K=HWK,V=HWV, (3)
whereMSA(·)represents the Multi-head Self-Attention,
andWQ,WK,WV,WOare learnable parameters.
Transformer Each layer in the Transformer consists
of a multi-head self-attention and a FFN layer (also
called Position-wise Feed-Forward Networks in Vaswani et
al. (2017)). The hidden state of l+ 1layer,Hl+1could be
calculated as followings.
Zl=norm(Hl+MSA(Hl)), (4)
Hl+1=norm(Zl+FFN(Zl)), (5)
wherenorm(·)means the layer normalization (Ba, Kiros,
and Hinton 2016). In addition, the Transformer augments the
input features by adding a positional embedding since the
self-attention could not capture the positional informati on
by itself.
Despite its effectiveness on machine translation and lan-
guage modeling, Transformer usually fails on the task with
moderate size datasets due to its shortage of inductive bias .
Model
In the multi-head self-attention, each head captures the pa ir-
wise interactions between words in different feature space .
Each head has the same scale of sentence length.
Scale-Aware Self-Attention To introduce the concept of
multi-scale into the self-attention mechanism, we use a
simple way to enhance the regular self-attention, named
Scale-Aware Self-Attention (SASA) which is equal to the
restricted self-attention which is proposed in Vaswani et
al. (2017) but using a dynamic size of the window.head1 head2 head3hl+1
i
hl
i hl
i−1 hl
i−2 hl
i+1hl
i+2
Figure 1: A diagram of Multi-Scale Multi-Head Self-
Attention, we can see three heads which correspond to three
different scales in the ﬁgure. The blue, green, red box illus -
trate the scale of ω= 1,ω= 3,ω= 5, respectively.
Given a sequence of vectors H∈RN×Dwith length N.
SASA has a parameter ωwhich is either a constant num-
ber or a ratio according to the sequence length to control its
working scope. An attention head can be computed as
head(H,ω)i,j=SM(QijCij(K,ω)T
√
D)Cij(V,ω),(6)
Cij(x,ω) = [xi,j−ω,...,xi,j+ω], (7)
Q=HWQ,K=HWK,V=HWV, (8)
whereiindicates the i-th head and jmeans j-th position. The
SM represents “Softmax” function, Cis a function to extract
context for a given position. WQ,WK,WVare learnable
parameters. The scale of a head could be a variable likeN
2or a ﬁxed number like 3.
Multi-Scale Multi-Head Self-Attention With SASA, we
can implement a Multi-Scale Multi-Head Self-Attention
(MSMSA) with multiple scale-aware heads. Each head
works on different scales. For N′heads self-attention,
MSMSA with scales Ω = [ω1,···,ωN′]is
MSMSA(H,Ω) = [ head1(H,ω1);···;
headN′(H,ωN′)]WO, (9)
whereWOis a parameter matrix.
Compared to the vanilla multi-head self-attention, vari-
ableΩcontrols the attended area and makes different heads
have different duties.
Multi-Scale Transformer With the MSMSA, we could
construct the multi-scale Transformer (MS-Transformer. I n
short, MS-Trans). Besides using MSMSA to replace the
vanilla multi-head self-attention, we also remove the FFN
(see Eq. (5)) because it could be view as a self-attention
with scale ω= 1plus a non-linear activation function. Since
MSMSA introduces the locality to the model, the MSMSA
with a small scale can be an alternative to the positional
embedding. Therefore, the multi-scale Transformer could b e
formalized as followings.
Hl+1= norm( Hl+ReLU(MSMSA( Hl),Ωl)) (10)

--- PAGE 3 ---
wherelis the layer index.
In this work, we limit the choice of scale size among
constant numbers {1,3,···} or variables depend on the se-
quence length {N
16,N
8,···}. And we force the scale size to
be an odd number.
Compared to the hierarchical multi-scale models, multi-
scale Transformer allows the attention heads in one layer
have various scales of vision, it is a “soft” version of viewi ng
different layers as different scales.
Classiﬁcation node We also ﬁnd adding a special node at
the beginning of each sequence and connecting it directly
to the ﬁnal sentence representation can improve the perfor-
mance. This technique was introduced in BERT (Devlin et
al. 2018), refer as “[CLS]”. And it is also similar to the “re-
lay node” in Guo et al. (2019). Different with them, we com-
bine the “[CLS]” node and the feature from applying max-
pooling over all the nodes in the ﬁnal layer to represent the
sentence. There is no difference between the “[CLS]” node
and other tokens in the input sequence except it can directly
contribute to the ﬁnal representation.
Looking for Effective Attention Scales
Multi-scale Transformer is a ﬂexible module in which each
layer can have multi-scale attention heads. Therefore, an i m-
portant factor is how to design the scale distribution for ea ch
layer.
Hierarchical Multi-Scale or Flexible Multi-Scale
A simple way to implement multi-scale feature extraction is
following the hierarchical way, which stacks several layer s
with small-scale heads. The lower layers capture the local
features and the higher capture the non-local features.
To verify the ability of hierarchical multi-scale, we desig n
a very simple simulation task named mirrored summation.
Given a sequence A={a1,...,aN}, a∈Rdand drawn
fromU(0,1). The target is/summationtextK
i=1ai⊙aN−i+1, whereKis
a ﬁxed integer less than the sequence length Nand⊙means
the Hadamard production. The minimum dependency path
length isN−K, we use this task to test the ability of models
for capturing long-range dependencies. Both train and test
set are random generated and they have 200k samples each.
We can assume the size of training set is enough to train
these models thoroughly.
We use three different settings of MS-Trans.
1. MS-Trans-Hier-S: MS-Transformer with two layers, and
each layer has 10 heads with a small scale ω= 3.
2. MS-Trans-deepHier-S: MS-Transformer with six layers,
and each layer has 10 heads with a small scale ω= 3.
3. MS-Trans-Flex: MS-Transformer with two layers, and
each layer has 10 heads with ﬂexible multi-scales ω=
3,N
16,N
8,N
4,N
2. Each scale has two heads.
As shown in Fig-2, Transformer achieves the best re-
sult, and MS-Trans-Flex follows with it. Although Trans-
former has the highest potential to capture long-range de-
pendencies, it requires large training samples. Meanwhile ,010 20 30 40 50 60 70 80 9010000.010.020.030.040.050.060.070.080.090.1
KTest MSETransMS-Trans-FlexMS-Trans-Hier-SMS-Trans-deepHier-SBiLSTM
Figure 2: Mirrored Summation Task. The curve of MSE ver-
sus valid number Kwith the sequence length n= 100 and
the dimension of input vectors d= 50 .
our model balance the data-hungry problem and the ability
for capturing long-range dependencies. Based on the com-
parison of MS-Trans-Hier-S and MS-Trans-deepHier-S, we
can ﬁnd the improvement of additional layers is relatively
small. According to the synthetic experiment and the per-
formance on real tasks (see. Sec-), we think the large-scale
heads are necessary for the lower layers and stacked small-
scale heads are hard to capture long-range dependencies. In
this case, a good prior should contain both small-scale and
large-scale.
Scale Distributions of Different Layers
Since multi-scale is necessary for each layer, the second
question is how to design the proportion of different
scales for each layer? We think each layer may have its
preference for the scale distribution. From the linguistic per-
spective, an intuitive assumption may be: the higher layer
has a higher probability for the large-scale heads and the
shallow layer has a higher probability for the small-scale
heads.
To look for the empirical evidence, we probe several
typical cases and analysis the corresponding behavior in a
data-driven model, BERT (Bidirectional Encoder Represen-
tations from Transformers) (Devlin et al. 2018).
Analogy Analysis from BERT BERT is a pre-trained
Transformer on large scale data, which has shown its power
on many NLP tasks and it has good generalization ability.
Since BERT is based on Transformer, it is not guided by
prior knowledge, their knowledge is learned from data. We
probe the behavior of BERT to see whether it ﬁts the linguis-
tic assumption. There are two aspects we want to study, the
ﬁrst is the working scales of attention heads of each layer in
BERT. The second is the difference between the scale distri-
butions of different layers, especially the preference of l ocal
and global relations. To probe these behaviors, we ﬁrst run
the BERT over many natural sentences and pick the highest

--- PAGE 4 ---
0 5 10 1502040
Relative Distance of Attention HeadPercentagehead-1 at layer-1
head-2 at layer-1
head-3 at layer-1
(a)0 5 10 1501020
Relative Distance of Attention HeadPercentagelayer-1
layer-6
layer-12
(b)
Figure 3: The visualization of BERT. (a) The attention dista nce distributions of three heads in the ﬁrst layer. The red he ad only
cares about the local pattern and the blue head equally looks at different distances. (b) The attention distance distrib ution of
different layers. The shallow layer prefers the small scale size and tends to large scale size slowly when the layer gets d eeper.
Even in the ﬁnal layer, local patterns still occupy a large pe rcentage. We truncate the distance atN
2for better visualization. The
full ﬁgure can be found in the Appendix.
activation of the attention map as the attention edge. Then
we record the relative distances of these attention edges.
In this work, we obtain the data from running BERT on
CoNLL03 dataset (see Tab-1).
We ﬁrst draw the Fig-3a for observing the behavior of
heads in the same layer. We pick three heads, and their dif-
ference is signiﬁcant. As shown in the ﬁgure, “head-2” focus
on a certain distance with a small scale, and “head-1” cover
all the distances. There is a clear division of labor of these
heads, and one layer can have both local and global vision
via combining features from different heads.
The second ﬁgure Fig-3b shows the trend of distance pref-
erence when the depth of layer increased. We can ﬁnd the
model move from local vision to global vision slowly and
shallow layers have a strong interest in local patterns.
The visualization of BERT ﬁts the design of Multi-Scale
Transformer, the ﬁrst observation corresponds to the desig n
of multi-scale multi-head self-attention which ask differ ent
heads focus on different scales, and the second observation
provides a good reference of the trend of scale distribution
across layers. Using such knowledge can largely reduce the
requirement of training data for Transformer-like models.
Control Factor of Scale Distributions for Different
Layers
From the intuitive linguistic perspective and empirical ev i-
dence, we design a control factor of scale distributions for
different layers of multi-scale Transformer.
LetLdenote the number of layers in multi-scale Trans-
former,|Ω|denote the number of candidate scale sizes, and
nl
kdenote the number of heads for l-th layer and k-th scale
size.The head number nl
kis computed by
zl
k=/braceleftbigg0 l=Lork=|Ω|
zl
k+1+α
lk∈ {0,···,|Ω|−1}(11)
nl=softmax(zl)·N′(12)
In the above equations, we introduce a hyper-parameter α
to control the change of preference of scale sizes for each
layer. For example, α= 0 means all the layers use the
same strategy of scale size, α >0means the preference
of smaller scale increased with the decline of layer depth,
andα <0indicates the preference of larger scale increased
with the decline of layer depth. As the conclusion of analyz-
ing BERT, we believe the deep layer has a balanced vision
over both local and global patterns, so the top layer should
be set to looking all the scale size uniformly. More speciﬁc,
whenα= 0.5andN′= 10 , three layers have nl=1=
{5,2,2,1,0},nl=2={4,2,2,1,1},nl=3={2,2,2,2,2},
it represents the ﬁrst layer has 5 head with scale size of 1, 2
head with scale size of 3, 2 head with scale size ofN
16and 1
head with scale size ofN
8.
Experiments
We evaluate our model on 17 text classiﬁcation datasets, 3
sequence labeling datasets and 1 natural language inferenc e
dataset. All the statistics can be found in Tab-1. Besides,
we use GloVe (Pennington, Socher, and Manning 2014) to
initialize the word embedding and JMT (Hashimoto et al.
2017) for character-level features. The optimizer is Adam
(Kingma and Ba 2014) and the learning rate and dropout ra-
tio are listed in the Appendix.
To focus on the comparison between different model de-
signs, we don’t list results of BERT-like models because the
data augmentation and pre-training is an orthogonal direc-
tion.

--- PAGE 5 ---
Table 1: An overall of datasets and its hyper-parameters, “H DIM,α, head DIM” indicates the dimension of hidden states,
the hyper-parameter for controlling the scale distributio n, the dimension of each head, respectively. The candidate s cales are
1,3,N
16,N
8,N
4for SST,MTL-16,SNLI datasets. And we use 1,3,5,7,9for sequence labeling tasks. MTL-16†consists of 16
datasets, each of them has 1400/200/400 samples in train/de v/test.
Dataset Train Dev. Test |V| H DIM α head DIM
SST (Socher et al. 2013) 8k 1k 2k 20k 300 0.5 30
MTL-16†
(Liu, Qiu,
and Huang
2017)Apparel Baby Books Camera
DVD Electronics Health IMDB
Kitchen Magazines MR Music
Software Sports Toys Video1400 200 400 8k ∼28k 300 0.5 30
PTB POS (Marcus, Santorini, and Marcinkiewicz 1993) 38k 5k 5 k 44k 300 1.0 30
CoNLL03 (Sang and Meulder 2003) 15k 3k 3k 25k 300 1.0 30
CoNLL2012 NER (Pradhan et al. 2012) 94k 14k 8k 63k 300 1.0 30
SNLI (Bowman et al. 2015) 550k 10k 10k 34k 600 0.5 64
Table 2: Test Accuracy on SST dataset.
Model Acc
BiLSTM (Li et al. 2015) 49.8
Tree-LSTM (Tai, Socher, and Manning 2015) 51.0
CNN-Tensor (Lei, Barzilay, and Jaakkola 2015) 51.2
Emb + self-att (Shen et al. 2018a) 48.9
BiLSTM + self-att (Yoon, Lee, and Lee 2018) 50.4
CNN + self-att (Yoon, Lee, and Lee 2018) 50.6
Dynamic self-att (Yoon, Lee, and Lee 2018) 50.6
DiSAN (Shen et al. 2018a) 51.7
Transformer 50.4
Multi-Scale Transformer 51.9
Text Classiﬁcation
Text Classiﬁcation experiments are conducted on Stanford
Sentiment Treebank(SST) dataset (Socher et al. 2013) and
MTL-16 (Liu, Qiu, and Huang 2017) consists of 16 small
datasets in different domains. Besides the base model we in-
troduced before, we use a two-layer MLP(Multi-Layer Per-
ceptron) with softmax function as the classiﬁer. It receive s
the feature from applying max-pooling over the top layer
plus the classiﬁcation node.
Tab-2 and 3 give the results on SST and MTL-16. Multi-
Scale Transformer achieves 1.5 and 3.56 points against
Transformer on these two datasets, respectively. Meanwhil e,
Multi-Scale Transformer also beats many existing models
including CNNs and RNNs.
Since the sentence average length of MTL-16 dataset is
relatively large, we also report the efﬁciency result in Fig -4.
We implement the MS-Trans with Pytorch1and DGL(Wang
et al. 2019). Multi-Scale Transformer achieves 6.5 times ac -
celeration against Transformer on MTL-16 dataset on aver-
age (average sentence length equals 109 tokens). The maxi-
mum of acceleration reaches 10 times (average 201 tokens)
1https://pytorch.orgTable 3: Test Accuracy over MTL-16 datasets. “SLSTM”
refer to the sentence-state LSTM (Zhang, Liu, and Song
2018).
DatasetAcc (%)
MS-Trans Transformer BiLSTM SLSTM
Apparel 87.25 82.25 86.05 85.75
Baby 85.50 84.50 84.51 86.25
Books 85.25 81.50 82.12 83.44
Camera 89.00 86.00 87.05 90.02
DVD 86.25 77.75 83.71 85.52
Electronics 86.50 81.50 82.51 83.25
Health 87.50 83.50 85.52 86.50
IMDB 84.25 82.50 86.02 87.15
Kitchen 85.50 83.00 82.22 84.54
Magazines 91.50 89.50 92.52 93.75
MR 79.25 77.25 75.73 76.20
Music 82.00 79.00 78.74 82.04
Software 88.50 85.25 86.73 87.75
Sports 85.75 84.75 84.04 85.75
Toys 87.50 82.00 85.72 85.25
Video 90.00 84.25 84.73 86.75
Average 86.34 82.78 84.01 85.38
and Multi-Scale Transformer can achieve 1.8 times acceler-
ation on very short sentences (average 22 tokens).
Sequence Labelling
Besides tasks which use the model as a sentence encoder, we
are also interested in the effectiveness of our model on se-
quence labeling tasks. We choose the Part-of-Speech (POS)
tagging and the Named Entity Recognition (NER) task to
verify our model. We use three datasets as our benchmark:
Penn Treebank (PTB) POS tagging dataset (Marcus, San-
torini, and Marcinkiewicz 1993), CoNLL2003 NER dataset
(Sang and Meulder 2003), CoNLL2012 NER dataset (Prad-
han et al. 2012).
Results in Tab-4 shows Multi-Scale Transformer beats

--- PAGE 6 ---
Table 4: Results on sequence labeling tasks. We list “Advanc ed Techniques” except pre-trained embeddings (GloVe, Word 2Vec,
JMT) in columns. The “Char” indicates character-level feat ures, it also includes the Capitalization Features, Lexico n Features,
etc. The “CRF” means an additional Conditional Random Field layer.
ModelAdv TechPOS NER
PTB CoNLL2003 CoNLL2012
char CRF Acc F1 F1
Ling et al. (2015) /check /check 97.78 - -
Huang, Xu, and Yu (2015) /check /check 97.55 90.10 -
Chiu and Nichols (2016a) /check /check - 90.69 86.35
Ma and Hovy (2016) /check /check 97.55 91.06 -
Chiu and Nichols (2016b) /check /check - 91.62 86.28
Zhang, Liu, and Song (2018) /check /check 97.55 91.57 -
Akhundov, Trautmann, and Groh (2018) /check /check 97.43 91.11 87.84
Transformer 96.31 86.48 83.57
Transformer + Char /check 97.04 88.26 85.14
Multi-Scale Transformer 97.24 89.38 85.26
Multi-Scale Transformer + Char /check 97.54 91.33 86.77
Multi-Scale Transformer + Char + CRF /check /check 97.66 91.59 87.80
IMDB MR A VG050100150200
DatasetMillisecondMS-Trans
Trans
BiLSTM
Figure 4: Test time per batch (batch size is 128) on the
dataset which has the longest length (IMDB), the dataset
which has the shortest length (MR), and the average over
16 datasets in MTL-16.
the vanilla Transformer on these three sequence labeling
datasets, which consists of other results reported above. I t
shows Multi-Scale Transformer can extract useful features
for each position as well.
Natural Language Inference
Natural Language Inference (NLI) is a classiﬁcation which
ask the model to judge the relationship of two sentences
from three candidates, “entailment”, “contradiction”, an d
“neutral”. We use a widely-used benchmark Stanford Natu-
ral Language Inference (SNLI) (Bowman et al. 2015) dataset
to probe the ability of our model for encoding sentence, we
compare our model with sentence vector-based models. Dif-
ferent with the classiﬁer in text classiﬁcation task, we fol low
the previous work (Bowman et al. 2016) to use a two-layer
MLP classiﬁer which takes concat (r1,r2,/bardblr1−r2/bardbl,r1−r2)
as inputs, where r1,r2are representations of two sentences
and equals the feature used in text classiﬁcation task.Table 5: Test Accuracy on SNLI dataset for sentence vector-
based models.
Model Acc
BiLSTM (Liu et al. 2016) 83.3
BiLSTM + self-att (Liu et al. 2016) 84.2
4096D BiLSTM-max (Conneau et al. 2017) 84.5
300D DiSAN (Shen et al. 2018a) 85.6
Residual encoders (Nie and Bansal 2017) 86.0
Gumbel TreeLSTM (Choi, Yoo, and Lee 2018) 86.0
Reinforced self-att (Shen et al. 2018b) 86.3
2400D Multiple DSA (Yoon, Lee, and Lee 2018) 87.4
Transformer 82.2
Multi-Scale Transformer 85.9
As shown in Tab-5, Multi-Scale Transformer outperforms
Transformer and most classical models, and the result is
comparable with the state-of-the-art. The reported number
of Transformer is obtained with heuristic hyper-parameter
selection, we use a three-layer Transformer with heavy
dropout and weight decay. And there is still a large mar-
gin compared to Multi-Scale Transformer. This compari-
son also indicates the moderate size training data (SNLI has
550k training samples) cannot replace the usefulness of pri or
knowledge.
Analysis
Inﬂuence of Scale Distributions As we introduced in Eq.
(11), we control the scale distributions over layers by a
hyper-parameter α. In this section, we give a comparison of
using different α, where the positive value means local bias
increased with the decline of layer depth and the negative
value means global bias increased with the decline of layer
depth.
As shown in the upper part of Tab-6, local bias in shallow
layers is a key factor to achieve good performance, and an
appropriate positive αachieves the best result. In contrast,

--- PAGE 7 ---
Table 6: Analysis of different scale distributions on SNLI
test set. The upper part shows the inﬂuence of hyper-
parameter αwhich change the distribution of scales across
layers. The ﬁve candidates of scale size are 1,3,N
16,N
8,N
4,
respectively. The lower part lists the performance of singl e-
scale models which use a ﬁxed scale for the whole model.
multi-scale α N’ L Acc
A 1.0 5 3 85.5
B 0.5 5 3 85.9
C 0.0 5 3 84.9
D -0.5 5 3 84.7
E -1.0 5 3 84.3
single-scale ω L Acc
F 3 3 84.3
G N/16 3 83.9
H N/8 3 81.7
I N/4 3 80.7
all the negative values harm the performance, that means too
much global bias in shallow layers may lead the model to a
wrong direction. The observation of this experiment ﬁts our
intuition, the high-level feature is the composition of low -
level terms.
Multi-Scale vs. Single-Scale As we claimed before,
Multi-Scale Transformer can capture knowledge at differ-
ent scales at each layer. Therefore, a simple question needs
to be evaluated is whether the multi-scale model outper-
forms the single-scale model or not. To answer this question ,
we compare Multi-Scale Transformer with several single-
scale models. Model F,G,H,I have the same number of layers
and attention heads with Multi-scale Transformer, but thei r
scales are ﬁxed.
Result in the lower part of Tab-6 reveal the value of
Multi-Scale Transformer, it achieves 1.6 points improve-
ment against the best single-scale model. And this result
also supports that local bias is an important inductive bias
for NLP task.
Related Works
Typical multi-scale models The multi-scale structure has
been used in many NLP models, it could be implemented in
many different ways. Such as stacked layers (Kalchbrenner,
Grefenstette, and Blunsom 2014; Kim 2014), tree-structure
(Socher et al. 2013; Tai, Socher, and Manning 2015; Zhu,
Sobhani, and Guo 2015), hierarchical timescale (El Hihi and
Bengio 1995; Chung, Ahn, and Bengio 2016), layer-wise
gating (Chung et al. 2015). Since these models are built on
modules like RNNs and CNNs, which embodies the intrin-
sic local bias by design, the common spirit of introducing
multi-scale is to enable long-range communications. In con -
trast, Transformer allows long-range communications, so w e
want the multi-scale brings local bias.
Transformers with additional inductive bias This work
is not the ﬁrst attempt of introducing inductive bias intoTransformer.
Shaw, Uszkoreit, and Vaswani (2018) suggest Trans-
former should care about the relative distance between to-
kens rather than the absolute position in the sequence. The
information of relative distance could be obtained by look-
ing multi-scale of the same position, so our model could be
aware of the relative distance if using enough scales.
Li et al. (2018) propose a regularization of enhancing the
diversity of attention heads. Our multi-scale multi-head s elf-
attention can make a good division of labor of heads via re-
stricting them in different scales.
Yang et al. (2018a) and Yang et al. (2018b) also introduce
the local bias into Transformer.
Different from the above models, we focus on importing
the notion of multi-scale to self-attention. Meanwhile, th eir
models use the single-scale structure. Our experimental re -
sults have shown the effectiveness of the multi-scale mech-
anism.
Conclusion
In this work, we present Multi-Scale Self-Attention and
Multi-Scale Transformer which combines the prior knowl-
edge of multi-scale and the self-attention mechanism. As
a result, it has the ability to extract rich and robust fea-
tures from different scales. We compare our model with the
vanilla Transformer on three real tasks (21 datasets). The
result suggests our proposal outperforms the vanilla Trans -
former consistently and achieves comparable results with
state-of-the-art models.
Acknowledgments
This work was supported by the National Key Research and
Development Program of China (No. 2018YFC0831103),
National Natural Science Foundation of China (No.
61672162), Shanghai Municipal Science and Technology
Major Project (No. 2018SHZDZX01) and ZJLab.
References
Akhundov, A.; Trautmann, D.; and Groh, G. 2018. Sequence la-
beling: A practical approach. CoRR abs/1808.03926.
Ba, L. J.; Kiros, R.; and Hinton, G. E. 2016. Layer normalizat ion.
CoRR abs/1607.06450.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A
large annotated corpus for learning natural language infer ence. In
EMNLP , 632–642. The Association for Computational Linguistics.
Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Manning ,
C. D.; and Potts, C. 2016. A fast uniﬁed model for parsing and
sentence understanding. In ACL (1) . The Association for Com-
puter Linguistics.
Chiu, J., and Nichols, E. 2016a. Sequential labeling with bi direc-
tional lstm-cnns. In Proc. International Conf. of Japanese Associ-
ation for NLP , 937–940.
Chiu, J. P. C., and Nichols, E. 2016b. Named entity recogniti on
with bidirectional lstm-cnns. TACL 4:357–370.
Choi, J.; Yoo, K. M.; and Lee, S. 2018. Learning to compose
task-speciﬁc tree structures. In AAAI , 5094–5101. AAAI Press.
Chung, J.; Ahn, S.; and Bengio, Y . 2016. Hierarchical multis cale
recurrent neural networks. arXiv preprint arXiv:1609.01704 .

--- PAGE 8 ---
Chung, J.; G¨ ulc ¸ehre, C ¸ .; Cho, K.; and Bengio, Y . 2015. Gat ed
feedback recurrent neural networks. In ICML , volume 37 of JMLR
Workshop and Conference Proceedings , 2067–2075. JMLR.org.
Conneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Borde s, A.
2017. Supervised learning of universal sentence represent ations
from natural language inference data. In EMNLP , 670–680. Asso-
ciation for Computational Linguistics.
Dai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q. V .; and
Salakhutdinov, R. 2019. Transformer-xl: Attentive langua ge mod-
els beyond a ﬁxed-length context. CoRR abs/1901.02860.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. BERT:
pre-training of deep bidirectional transformers for langu age under-
standing. CoRR abs/1810.04805.
El Hihi, S., and Bengio, Y . 1995. Hierarchical recurrent neu ral
networks for long-term dependencies. In NIPS , 493–499. Citeseer.
Guo, Q.; Qiu, X.; Liu, P.; Shao, Y .; Xue, X.; and Zhang, Z. 2019 .
Star-transformer. In NAACL-HLT (1) , 1315–1325. Association for
Computational Linguistics.
Hashimoto, K.; Xiong, C.; Tsuruoka, Y .; and Socher, R. 2017.
A joint many-task model: Growing a neural network for multip le
NLP tasks. In EMNLP , 1923–1933. Association for Computational
Linguistics.
Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirectional LSTM-CRF
models for sequence tagging. CoRR abs/1508.01991.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A con-
volutional neural network for modelling sentences. In Proceedings
of ACL .
Kim, Y . 2014. Convolutional neural networks for sentence cl assiﬁ-
cation. In Proceedings of the 2014 Conference on EMNLP , 1746–
1751.
Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic
optimization. CoRR abs/1412.6980.
Lei, T.; Barzilay, R.; and Jaakkola, T. S. 2015. Molding cnns for
text: non-linear, non-consecutive convolutions. In EMNLP , 1565–
1575. The Association for Computational Linguistics.
Li, J.; Luong, T.; Jurafsky, D.; and Hovy, E. H. 2015. When are
tree structures necessary for deep learning of representat ions? In
EMNLP , 2304–2314. The Association for Computational Linguis-
tics.
Li, J.; Tu, Z.; Yang, B.; Lyu, M. R.; and Zhang, T. 2018. Multi-
head attention with disagreement regularization. In EMNLP , 2897–
2903. Association for Computational Linguistics.
Ling, W.; Dyer, C.; Black, A. W.; Trancoso, I.; Fermandez, R. ;
Amir, S.; Marujo, L.; and Lu´ ıs, T. 2015. Finding function in
form: Compositional character models for open vocabulary w ord
representation. In EMNLP , 1520–1530. The Association for Com-
putational Linguistics.
Liu, Y .; Sun, C.; Lin, L.; and Wang, X. 2016. Learning natu-
ral language inference using bidirectional LSTM model and i nner-
attention. CoRR abs/1605.09090.
Liu, P.; Qiu, X.; and Huang, X. 2017. Adversarial multi-task learn-
ing for text classiﬁcation. In ACL (1) , 1–10. Association for Com-
putational Linguistics.
Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling vi a
bi-directional lstm-cnns-crf. In ACL (1) . The Association for Com-
puter Linguistics.
Marcus, M. P.; Santorini, B.; and Marcinkiewicz, M. A. 1993.
Building a large annotated corpus of english: The penn treeb ank.
Computational Linguistics 19(2):313–330.Nie, Y ., and Bansal, M. 2017. Shortcut-stacked sentence enc oders
for multi-domain inference. In RepEval@EMNLP , 41–45. Associ-
ation for Computational Linguistics.
Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Glo bal
vectors for word representation. In Proceedings of the 2014
conference on empirical methods in natural language proces sing
(EMNLP) , 1532–1543.
Pradhan, S.; Moschitti, A.; Xue, N.; Uryupina, O.; and Zhang , Y .
2012. Conll-2012 shared task: Modeling multilingual unres tricted
coreference in ontonotes. In EMNLP-CoNLL Shared Task , 1–40.
ACL.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I . 2018.
Improving language understanding by generative pre-train ing.
Sang, E. F. T. K., and Meulder, F. D. 2003. Introduction to
the conll-2003 shared task: Language-independent named en tity
recognition. In CoNLL , 142–147. ACL.
Shaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-attenti on with
relative position representations. In NAACL-HLT (2) , 464–468. As-
sociation for Computational Linguistics.
Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang, C.
2018a. Disan: Directional self-attention network for rnn/ cnn-free
language understanding. In AAAI , 5446–5455. AAAI Press.
Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Wang, S.; and Zhang, C .
2018b. Reinforced self-attention network: a hybrid of hard and soft
attention for sequence modeling. In IJCAI , 4345–4352. ijcai.org.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D. ; Ng,
A. Y .; and Potts, C. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In EMNLP , 1631–
1642. ACL.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved se-
mantic representations from tree-structured long short-t erm mem-
ory networks. In ACL (1) , 1556–1566. The Association for Com-
puter Linguistics.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is
all you need. In NIPS , 6000–6010.
Wang, M.; Yu, L.; Zheng, D.; Gan, Q.; Gai, Y .; Ye, Z.; Li, M.;
Zhou, J.; Huang, Q.; Ma, C.; Huang, Z.; Guo, Q.; Zhang, H.; Lin ,
H.; Zhao, J.; Li, J.; Smola, A. J.; and Zhang, Z. 2019. Deep gra ph
library: Towards efﬁcient and scalable deep learning on gra phs.
CoRR abs/1909.01315.
Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy, E.
2016. Hierarchical attention networks for document classi ﬁcation.
InProceedings of the 2016 Conference of NAACL , 1480–1489.
Yang, B.; Tu, Z.; Wong, D. F.; Meng, F.; Chao, L. S.; and Zhang, T.
2018a. Modeling localness for self-attention networks. In EMNLP ,
4449–4458. Association for Computational Linguistics.
Yang, B.; Wang, L.; Wong, D. F.; Chao, L. S.; and Tu, Z. 2018b.
Convolutional self-attention network. CoRR abs/1810.13320.
Yoon, D.; Lee, D.; and Lee, S. 2018. Dynamic self-attention : Com-
puting attention over words dynamically for sentence embed ding.
CoRR abs/1808.07383.
Zhang, Y .; Liu, Q.; and Song, L. 2018. Sentence-state LSTM fo r
text representation. In ACL (1) , 317–327. Association for Compu-
tational Linguistics.
Zhu, X.-D.; Sobhani, P.; and Guo, H. 2015. Long short-term me m-
ory over recursive structures. In ICML , 1604–1612.
