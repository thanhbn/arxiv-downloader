# 2405.11582.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2405.11582.pdf
# Kích thước tệp: 1110782 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SLAB: Transformers Hiệu Quả với Sự Chú Ý Tuyến Tính Đơn Giản và Chuẩn Hóa Batch Tái Tham Số Hóa Tiệm Tiến
Jialong Guo1 *Xinghao Chen1 *Yehui Tang1Yunhe Wang1
Tóm tắt
Transformers đã trở thành kiến trúc nền tảng cho cả nhiệm vụ xử lý ngôn ngữ tự nhiên và thị giác máy tính. Tuy nhiên, chi phí tính toán cao khiến việc triển khai trên các thiết bị hạn chế tài nguyên trở nên khá thách thức. Bài báo này nghiên cứu các module cổ chai tính toán của transformer hiệu quả, tức là các lớp chuẩn hóa và các module sự chú ý. LayerNorm thường được sử dụng trong kiến trúc transformer nhưng không thân thiện về mặt tính toán do việc tính toán thống kê trong quá trình suy luận. Tuy nhiên, việc thay thế LayerNorm bằng BatchNorm hiệu quả hơn trong transformer thường dẫn đến hiệu suất kém hơn và sự sụp đổ trong quá trình huấn luyện. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp mới có tên PRepBN để thay thế LayerNorm một cách tiệm tiến bằng BatchNorm tái tham số hóa trong quá trình huấn luyện. Hơn nữa, chúng tôi đề xuất một module sự chú ý tuyến tính đơn giản (SLA) đơn giản nhưng hiệu quả để đạt được hiệu suất mạnh. Các thí nghiệm rộng rãi về phân loại hình ảnh cũng như phát hiện vật thể chứng minh tính hiệu quả của phương pháp được đề xuất. Ví dụ, SLAB-Swin của chúng tôi đạt được độ chính xác top-1 83.6% trên ImageNet-1K với độ trễ 16.2ms, thấp hơn 2.4ms so với Flatten-Swin với độ chính xác cao hơn 0.1%. Chúng tôi cũng đánh giá phương pháp của mình cho tác vụ mô hình hóa ngôn ngữ và đạt được hiệu suất tương đương và độ trễ thấp hơn. Mã nguồn có sẵn công khai tại https://github.com/xinghaochen/SLAB và https://github.com/mindspore-lab/models/.

1. Giới thiệu
Được giới thiệu ban đầu cho các tác vụ trong xử lý ngôn ngữ tự nhiên (Vaswani et al., 2017), kiến trúc transformer đã nhanh chóng nổi lên như một mô hình ưu việt trong bối cảnh các mô hình ngôn ngữ. Ảnh hưởng của nó đã mở rộng đáng kể với việc giới thiệu Vision Transformer (ViT) (Dosovitskiy et al., 2020), minh họa hiệu quả và tính linh hoạt của các kiến trúc dựa trên transformer. Những kiến trúc này đã chứng minh khả năng đạt được hiệu suất cạnh tranh so với mạng nơ-ron tích chập (CNNs) trên các tác vụ thị giác đa dạng (Han et al., 2022; Wang et al., 2022; Zheng et al., 2023; Tang et al., 2023a; Carion et al., 2020; Xu et al., 2023). Do hiệu suất mạnh mẽ của nó, transformer đã trở thành kiến trúc chủ đạo trong học sâu. Tuy nhiên, yêu cầu tính toán của kiến trúc transformer đặt ra một thách thức đáng kể, chủ yếu do độ phức tạp tính toán bậc hai của cơ chế sự chú ý và sự cần thiết tính toán thống kê trực tuyến của thành phần LayerNorm.

*Đóng góp bằng nhau1Huawei Noah's Ark Lab. Liên hệ với: Xinghao Chen <xinghao.chen@huawei.com>, Yunhe Wang <yunhe.wang@huawei.com>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học Máy, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

Nhiều nỗ lực đã được hướng tới việc nâng cao hiệu quả của kiến trúc transformer (Tang et al., 2024; Wu et al., 2023; Tang et al., 2023b). Một số phương pháp đã tìm cách giảm thiểu độ phức tạp tính toán bằng cách hạn chế phạm vi tương tác token trong các cơ chế tự chú ý, chẳng hạn như giảm mẫu ma trận key và value (Wang et al., 2021), thực hiện các mẫu sự chú ý toàn cục thưa thớt (Child et al., 2019), và tính toán tự chú ý trong các cửa sổ nhỏ hơn (Tu et al., 2022; Liu et al., 2021; Dong et al., 2022). Trong khi đó, sự chú ý tuyến tính nổi lên như một chiến lược thay thế để nâng cao hiệu quả tính toán bằng cách chia nhỏ cơ chế sự chú ý thành chi phí tính toán tuyến tính (Katharopoulos et al., 2020; Cai et al., 2022; Han et al., 2023; You et al., 2023), tuy nhiên việc đạt được sự cân bằng tốt giữa hiệu quả và độ chính xác vẫn là một nhiệm vụ thách thức. Hơn nữa, có một số khám phá về việc thay thế LayerNorm (LN) bằng BatchNorm (BN) trong transformers, được thúc đẩy bởi chi phí tính toán bổ sung mà LayerNorm gây ra trong quá trình suy luận. Yang et al. (2022) đề xuất thêm một lớp BatchNorm giữa hai lớp tuyến tính trong mạng feed forward để ổn định quá trình huấn luyện. Tuy nhiên, vẫn tồn tại khoảng cách hiệu suất giữa các transformers dựa trên LayerNorm và BatchNorm.

Trong bài báo này, chúng tôi tập trung vào việc có được các kiến trúc transformer hiệu quả bằng cách đào sâu vào các module không hiệu quả về mặt tính toán, tức là các lớp chuẩn hóa và các module sự chú ý. Chúng tôi đầu tiên khám phá việc thay thế LayerNorm bằng BatchNorm để tăng tốc suy luận cho transformer. BatchNorm dẫn đến độ trễ suy luận thấp hơn nhưng có thể gây ra sự sụp đổ huấn luyện và hiệu suất kém hơn, trong khi LayerNorm có thể ổn định quá trình huấn luyện nhưng có chi phí tính toán bổ sung trong quá trình suy luận. Để giải quyết vấn đề này, chúng tôi đầu tiên đề xuất một chiến lược tiệm tiến để thay thế LayerNorm bằng BatchNorm một cách dần dần bằng cách sử dụng một siêu tham số để kiểm soát tỷ lệ của cả hai lớp chuẩn hóa. Ban đầu kiến trúc transformer được chi phối bởi LayerNorm và dần dần chuyển sang BatchNorm thuần túy ở cuối quá trình huấn luyện. Chiến lược này hiệu quả giảm thiểu rủi ro sụp đổ huấn luyện và cũng loại bỏ nhu cầu tính toán thống kê trong quá trình suy luận. Ngoài chiến lược tiệm tiến, chúng tôi cũng đề xuất một công thức tái tham số hóa mới cho BatchNorm (RepBN), để nâng cao tính ổn định huấn luyện và hiệu suất tổng thể.

Hơn nữa, chi phí tính toán của sự chú ý là quan trọng đối với transformer hiệu quả và các phương pháp trước đó gặp khó khăn trong việc đạt được sự cân bằng tốt giữa hiệu quả và độ chính xác. Để giải quyết vấn đề này, chúng tôi đề xuất một module sự chú ý tuyến tính đơn giản (SLA) sử dụng ReLU làm hàm kernel và kết hợp tích chập theo chiều sâu để thực hiện nâng cao tính năng cục bộ. Cơ chế sự chú ý được đề xuất hiệu quả hơn so với sự chú ý tuyến tính trước đó nhưng vẫn đạt được hiệu suất tương đương.

Chúng tôi đánh giá rộng rãi phương pháp được đề xuất cho các kiến trúc khác nhau trên các bộ đánh giá khác nhau. RepBN tiệm tiến của chúng tôi cho thấy hiệu suất mạnh cho các tác vụ phân loại hình ảnh và phát hiện vật thể, đạt được độ chính xác tương tự với độ trễ suy luận thấp hơn. Hơn nữa, kết hợp với RepBN tiệm tiến và module sự chú ý tuyến tính đơn giản, transformer SLAB của chúng tôi đạt được độ chính xác cạnh tranh so với transformer Flatten với hiệu quả tính toán được cải thiện. Ví dụ, SLAB-Swin-S đạt độ chính xác Top-1 83.6% trên ImageNet-1K với độ trễ 16.2ms, thấp hơn 2.4ms so với Flatten-Swin-S với độ chính xác cao hơn 0.1%. Chúng tôi cũng đánh giá phương pháp của mình cho tác vụ mô hình hóa ngôn ngữ và đạt được hiệu suất tương đương và độ trễ suy luận thấp hơn.

2. Công trình Liên quan
2.1. Kiến trúc Hiệu quả cho Transformers
Với sự ra đời của Vision Transformer (ViT) tiên phong (Dosovitskiy et al., 2020), tiềm năng của kiến trúc transformer cho các tác vụ thị giác máy tính đã được khám phá rất nhiều. Nhiều nhà nghiên cứu khác nhau đã dành tâm huyết cho lĩnh vực này để làm cho kiến trúc dựa trên transformer hiệu quả và mạnh mẽ hơn. Touvron et al. (2021) đề xuất DeiT sử dụng chưng cất để đạt được hiệu suất mạnh chỉ với việc huấn luyện trên ImageNet1K. Liu et al. (Liu et al., 2021) đề xuất Swin Transformer, giới thiệu lược đồ cửa sổ dịch chuyển và mang lại hiệu quả lớn hơn. Vì việc tính toán tự chú ý được giới hạn trong một cửa sổ nhỏ, transformer này có độ phức tạp tính toán tuyến tính. Một số công trình cải thiện thiết kế của mẫu thưa thớt để nâng cao tương tác của mỗi token, chẳng hạn như CSwin (Dong et al., 2022). Bên cạnh đó, cơ chế sự chú ý động cố gắng kiểm soát key/value tương tác với query thích ứng với dữ liệu, chẳng hạn như DAT++ (Xia et al., 2023) và BiFormer (Zhu et al., 2023).

Ngoài các phương pháp trên, sự chú ý tuyến tính là một hướng nghiên cứu phổ biến để giảm độ phức tạp tính toán cho transformer. Nhiều cơ chế hiệu quả đã được đề xuất để thay thế hàm softmax. Ví dụ, Performers (Choromanski et al., 2020) sử dụng phương pháp tính năng ngẫu nhiên trực giao dương để xấp xỉ softmax. Hydra attention (Bolya et al., 2022) chọn độ tương tự cosine làm hàm kernel. Flatten Transformer (Han et al., 2023) thiết kế một hàm tập trung để cải thiện khả năng tập trung của sự chú ý tuyến tính.

2.2. Chuẩn hóa cho Transformers
Chuẩn hóa được biết đến như một phương pháp hữu ích để làm cho quá trình huấn luyện ổn định và tăng hiệu suất. Ngày nay, nhiều phương pháp chuẩn hóa khác nhau đã được đề xuất, chẳng hạn như BatchNorm (Ioffe & Szegedy, 2015), LayerNorm (Ba et al., 2016), InstanceNorm (Ulyanov et al., 2016), GroupNorm (Wu & He, 2018), MABN (Yan et al., 2020) và UN (Yang et al., 2022). BatchNorm được sử dụng rộng rãi trong các mạng tích chập và LayerNorm thường được sử dụng cho các mạng như transformer và LSTM.

Chuẩn hóa có thể được phân loại thành phương pháp ngoại tuyến và phương pháp trực tuyến tùy theo việc liệu giá trị trung bình và phương sai có cần được tính toán tại thời điểm suy luận hay không (Yang et al., 2022). Các phương pháp trực tuyến thường không liên quan đến batch như LayerNorm, InstanceNorm và GroupNorm. Những phương pháp này tính toán thống kê trong cả quá trình huấn luyện và suy luận. LayerNorm thường được sử dụng trong kiến trúc transformer.

Các phương pháp ngoại tuyến liên quan đến batch như BatchNorm và UN, trong đó chiều batch được bao gồm trong các tính toán của cả giá trị trung bình và phương sai (Yao et al., 2021). Vì giá trị trung bình và phương sai được tính toán trước trong suy luận, chuẩn hóa ngoại tuyến có thể được hợp nhất với các phép toán tuyến tính khác. Trong quá trình suy luận, sẽ không có các phép toán chuẩn hóa ngoại tuyến và thời gian suy luận sẽ được giảm. Tuy nhiên, các phương pháp ngoại tuyến thường đối mặt với vấn đề suy giảm hiệu suất và sụp đổ huấn luyện khi sử dụng trong transformer. Để giải quyết vấn đề này, Yao et al. (2021) đề xuất thêm một lớp BatchNorm giữa hai lớp tuyến tính trong khối MLP để làm ổn định thống kê huấn luyện. Yang et al. (2022) phát hiện rằng vấn đề được gây ra bởi hành vi bất thường của thống kê kích hoạt, và đề xuất một chiến lược làm mượt dao động phù hợp và một chiến lược lọc ngoại lai thích ứng để tăng hiệu suất và huấn luyện ổn định.

3. Kiến thức Cơ bản
Cho đầu vào N tokens X∈RN×C, trong đó C là chiều tính năng, kiến trúc chung của khối transformer có thể được viết như:
X=X+ Attn(Norm( X)),
X=X+ MLP(Norm( X)),(1)
trong đó Attn(·) tính toán điểm sự chú ý, MLP(·) biểu thị perceptron đa lớp và Norm(·) là hàm chuẩn hóa. Trong cấu hình mặc định của khối transformer, Norm(·) thường là một phép toán LayerNorm và Attn(·) là cơ chế sự chú ý dựa trên softmax (Vaswani et al., 2017).

Sự chú ý đóng vai trò quan trọng trong Transformer. Ký hiệu ma trận query, key và value là Q, K, V ∈RN×C, sự chú ý softmax tính toán độ tương tự theo cặp giữa queries và keys trước tiên, và dẫn đến độ phức tạp tính toán bậc hai O(N2C) liên quan đến số lượng queries và keys N. Điều này làm cho transformer tốn kém về mặt tính toán đặc biệt trong việc xử lý các tác vụ có đầu vào chuỗi dài. Sự chú ý tuyến tính nhằm tách rời hàm softmax với xấp xỉ phù hợp hoặc thay thế nó bằng hàm kernel khác để tính toán KTVtrước. Với sự thay đổi này trong thứ tự tính toán, độ phức tạp tính toán trở thành O(NC2), tuyến tính liên quan đến số lượng queries và keys N.

Tuy nhiên, LayerNorm chiếm tỷ lệ đáng kể của độ trễ vì nó đòi hỏi tính toán thống kê trong quá trình suy luận. Do đó, trong bài báo này chúng tôi khám phá việc tận dụng BatchNorm để xây dựng transformers hiệu quả, chỉ tồn tại trong quá trình huấn luyện và có thể được hợp nhất với các lớp tuyến tính trước hoặc sau. Hơn nữa, module sự chú ý đóng vai trò quan trọng nhất đối với transformers và cơ chế sự chú ý dựa trên softmax không hiệu quả về mặt tính toán do độ phức tạp tính toán bậc hai. Trong bài báo này, chúng tôi đề xuất một dạng sự chú ý đơn giản nhưng hiệu quả, giảm đáng kể độ trễ nhưng vẫn duy trì hiệu suất mạnh trên các tác vụ thị giác khác nhau.

4. Phương pháp
Trong bài báo này, chúng tôi tập trung vào việc xây dựng transformers hiệu quả và đề xuất một loạt chiến lược, bao gồm chiến lược tiệm tiến để thay thế LayerNorm (LN) bằng BatchNorm (BN) tái tham số hóa và module sự chú ý tuyến tính đơn giản (SLA). Transformers SLAB được đề xuất đạt được hiệu suất mạnh so với các phương pháp trước đây trong khi vẫn có hiệu quả tính toán cao hơn.

4.1. BatchNorm Tái Tham Số Hóa Tiệm Tiến
LayerNorm đòi hỏi tính toán thống kê trong cả quá trình huấn luyện và suy luận, do đó làm chậm đáng kể tốc độ chạy của transformers. Ngược lại, BatchNorm có thể được hợp nhất đơn giản với các lớp tuyến tính trong quá trình suy luận và phù hợp hơn cho các kiến trúc hiệu quả. Tuy nhiên, việc tận dụng BatchNorm trực tiếp cho transformers mang lại hiệu suất không thỏa đáng (Yao et al., 2021). Để giải quyết vấn đề này, chúng tôi đề xuất thay thế LayerNorm bằng BatchNorm một cách tiệm tiến trong quá trình huấn luyện, và cũng đề xuất một công thức tái tham số hóa mới cho BatchNorm được lấy cảm hứng từ Ding et al. (2021; 2022) để cải thiện hiệu suất hơn nữa, như được thể hiện trong Hình 2.

Re-parameterized BatchNorm. RepBN được đề xuất được công thức hóa như:
RepBN( X) = BN( X) +ηX, (2)
trong đó η là một tham số có thể học được được huấn luyện cùng nhau theo cách end-to-end. Một khi quá trình huấn luyện hoàn tất, RepBN có thể được tái tham số hóa như một dạng chuẩn của BN, như được thể hiện trong Bổ đề 4.1.

Bổ đề 4.1. Ký hiệu một lớp BN với giá trị trung bình µ, độ lệch chuẩn σ, các tham số rescale và shift α và β là BN(X;µ, σ, α, β ). Chúng ta có thể tái tham số hóa RepBN trong Eq. 2 như:
RepBN( X;µ, σ, α, β ) = BN( X;µ, σ, α +ησ, β +ηµ).(3)

Chứng minh.
RepBN( X;µ, σ, α, β ) = BN( X;µ, σ, α, β ) +ηX
=X−µ
σα+β+ηX=X−µ
σα+β+X
σση
=X−µ
σα+β+X−µ
σση+µη
=X−µ
σ(α+ησ) + (β+ηµ)
= BN( X;µ, σ, α +ησ, β +ηµ).(4)

Dựa trên Bổ đề 4.1, phân phối của đầu ra RepBN được kiểm soát bởi α+ησ và β+ηµ, tương ứng với phương sai và giá trị trung bình. RepBN có thể khôi phục phân phối với sự trợ giúp của σ và µ.

Trong khi đó, khi α= 0, β= 0, nó tương đương với việc bỏ qua BatchNorm. Khi η= 0, RepBN được chuyển đổi thành BatchNorm thuần túy.

Tiệm tiến LN →RepBN. Để tạo thuận lợi cho việc huấn luyện transformers dựa trên BN thuần túy, chúng tôi đề xuất chuyển tiệm tiến LN sang RepBN trong quá trình huấn luyện, tức là,
PRepBN( X) =γLN(X) + (1−γ)RepBN( X),(5)
trong đó γ là một siêu tham số để kiểm soát đầu ra của các lớp chuẩn hóa khác nhau. Thông thường γ= 1 ở đầu quá trình huấn luyện khi LN chi phối kiến trúc, và γ= 0 ở cuối quá trình huấn luyện để đảm bảo nó chuyển sang transformer dựa trên BN thuần túy. Chúng tôi sử dụng một chiến lược giảm đơn giản nhưng hiệu quả cho γ:
γ=T−Tcur
T, γ∈[0,1], (6)
trong đó T là tổng số bước huấn luyện với LayerNorm và Tcur là bước hiện tại. Chiến lược tiệm tiến này làm giảm độ khó khăn của việc huấn luyện transformer dựa trên BN thuần túy và do đó dẫn đến hiệu suất mạnh trên các tác vụ khác nhau.

Có một số chiến lược giảm khác để làm giảm giá trị của γ dần dần, chẳng hạn như giảm cosine và giảm bước. Theo kinh nghiệm, chúng tôi thấy rằng chiến lược tuyến tính là một trong những chiến lược hiệu quả và đơn giản hơn.

--- TRANG 5 ---
SLAB: Transformers Hiệu Quả với Sự Chú Ý Tuyến Tính Đơn Giản và Chuẩn Hóa Batch Tái Tham Số Hóa Tiệm Tiến

4.2. Sự Chú Ý Tuyến Tính Đơn Giản
Module sự chú ý là phần quan trọng nhất trong mạng transformer, thường được công thức hóa như:
Q=XWQ, K=XWK, V=XWV,
Oi=NX
j=1Sim(Qi, Kj)P
jSim(Qi, Kj)Vj,(7)
trong đó WQ, WK, WV∈RC×C chiếu các token đầu vào thành các tensor query, key và value tương ứng. Sim(·,·) biểu thị hàm tương tự. Đối với dạng sự chú ý gốc, hàm tương tự là:
Sim softmax (Qi, Kj) = exp(QiKT
j√
C), (8)
sự chú ý dựa trên softmax này dẫn đến độ phức tạp tính toán cao. Một số phương pháp gần đây nghiên cứu việc sử dụng sự chú ý tuyến tính để loại bỏ tính toán softmax từ đó cải thiện hiệu quả của transformers (Han et al., 2023). Tuy nhiên, những phương pháp này vẫn có thiết kế khá phức tạp và không đủ hiệu quả về mặt tính toán. Trong bài báo này, chúng tôi đề xuất một sự chú ý tuyến tính đơn giản (SLA) được công thức hóa như sau:
SimSLA(Qi, Kj) = ReLU ( Qi) ReLU ( Kj)T,
˜Oi=NX
j=1SimSLA(Qi, Kj)P
jSimSLA(Qi, Kj)Vj,
OSLA=˜O +DWC( V),(9)
trong đó DWC (·) biểu thị tích chập theo chiều sâu. Đây là một sự chú ý tuyến tính đơn giản nhưng hiệu quả vì nó cũng có thứ tự tính toán tách rời bằng cách tính toán KTV trước, và dẫn đến giảm độ phức tạp lớn. Hơn nữa, chỉ có hàm ReLU và tích chập theo chiều sâu được khám phá và cả hai phép toán đều thân thiện về mặt tính toán trong hầu hết phần cứng.

Để chứng minh rằng phương pháp của chúng tôi vẫn duy trì sự đa dạng tính năng, chúng tôi trực quan hóa hiệu ứng của bản đồ sự chú ý trên DeiT-T đã áp dụng chiến lược BatchNorm tái tham số hóa tiệm tiến và sự chú ý tuyến tính đơn giản (SLAB), như được thể hiện trong Hình 3. Có thể thấy rằng thứ hạng cao vẫn được duy trì cho phương pháp được đề xuất của chúng tôi, chứng minh khả năng tốt của nó trong việc nắm bắt thông tin sự chú ý.

5. Thí nghiệm
Trong phần này, chúng tôi đánh giá phương pháp của mình trên các tác vụ thị giác máy tính khác nhau bao gồm phân loại hình ảnh, phát hiện vật thể và phân đoạn thể hiện và cũng trên tác vụ mô hình hóa ngôn ngữ. Chúng tôi tiến hành các thí nghiệm rộng rãi cho các backbone khác nhau với BatchNorm tái tham số hóa tiệm tiến được đề xuất và module sự chú ý tuyến tính đơn giản. Chúng tôi huấn luyện mô hình của mình trên ImageNet-1K (Deng et al., 2009) cho phân loại hình ảnh, và đánh giá hiệu ứng của các tác vụ phát hiện vật thể và phân đoạn thể hiện trên bộ dữ liệu COCO (Lin et al., 2014). Cuối cùng, chúng tôi nghiên cứu các yếu tố thiết kế quan trọng của phương pháp được đề xuất trên tác vụ phân loại.

5.1. Phân loại Hình ảnh
Cài đặt. Đối với phân loại hình ảnh, chúng tôi tuân thủ cấu hình được nêu trong (Touvron et al., 2021). Chúng tôi huấn luyện tất cả các mô hình trong 300 epochs với bộ tối ưu AdamW, kết hợp bộ lập lịch tốc độ học tập cosine decay với 20 epochs warm-up tuyến tính. Kích thước batch được đặt thành 1024, tốc độ học tập ban đầu là 0.001, và giá trị weight decay là 0.05. Trong trường hợp các mô hình sử dụng BatchNorm tái tham số hóa tiệm tiến được đề xuất, tỷ lệ droppath giảm (Larsson et al., 2016) được áp dụng. Các bước giảm tuyến tính T cho PRepBN hơi khác nhau giữa các backbone khác nhau. Do sự thay đổi phương sai gây ra bởi droppath, chúng tôi đóng băng các tham số mô hình và chỉ cập nhật thống kê của BatchNorm tái tham số hóa trong 10 epochs cuối quá trình huấn luyện. Chúng tôi cũng chứng minh tính hiệu quả của phương pháp với cả BatchNorm tái tham số hóa tiệm tiến và sự chú ý tuyến tính đơn giản. Chúng tôi tuân theo cài đặt của (Han et al., 2023) về thiết kế kiến trúc macro và huấn luyện. Tất cả kết quả báo cáo về throughput/latency được thu thập trên một GPU V100. Đối với tác vụ phân loại, chúng tôi đo FLOPs cũng như throughput/latency cho độ phân giải hình ảnh 224 ×224.

Kết quả trên tác vụ phân loại hình ảnh. Bảng 1 trình bày kết quả của các backbone khác nhau với chuẩn hóa PRepBN của chúng tôi. PRepBN được đề xuất của chúng tôi chứng minh hiệu suất tương đương hoặc thậm chí vượt trội khi so sánh với LayerNorm. Cụ thể hơn, các mô hình sử dụng PRepBN của chúng tôi làm lớp chuẩn hóa thể hiện cải thiện hiệu suất từ 0.1% đến 1.4%. Đáng chú ý, PRepBN có thể hợp nhất với các phép toán tuyến tính khác, cho phép nó có được suy luận hiệu quả hơn. Chúng tôi tiếp tục so sánh phương pháp của mình với BN+FFNBN (Yao et al., 2021) cũng nhằm huấn luyện mô hình transformer với BatchNorm. Có thể thấy rằng PRepBN của chúng tôi đạt được cải thiện nhất quán trên các backbone khác nhau. Ví dụ, PRepBN được đề xuất của chúng tôi đạt được độ chính xác top-1 80.2% trên mô hình DeiT-S, tốt hơn 1.4% so với phương pháp BN+FFNBN. Đối với swin transformer, PRepBN của chúng tôi mang lại mức tăng độ chính xác +0.5%, +0.4% và +0.5% so với BN+FFNBN trên các mô hình Swin-T, Swin-S và Swin-B.

Như được thể hiện trong Hình 4, chúng tôi trình bày phân tích so sánh phương pháp của mình trên các mô hình dựa trên DeiT, PVT và Swin. Rõ ràng là các transformers được trang bị PRepBN của chúng tôi đạt được throughput cao hơn trong khi duy trì mức độ chính xác tương tự.

Bảng 2 trình bày hiệu suất của transformer SLAB của chúng tôi, được hỗ trợ bởi BatchNorm tái tham số hóa tiệm tiến được đề xuất và module sự chú ý tuyến tính đơn giản. Chúng tôi so sánh mô hình của mình với transformer Flatten, sử dụng sự chú ý tuyến tính tập trung để có hiệu quả cao hơn. Các thí nghiệm trên các kiến trúc khác nhau bao gồm DeiT (Touvron et al., 2021), PVT (Wang et al., 2021), CSwin (Dong et al., 2022) và Swin (Liu et al., 2021) chứng minh rằng transformer SLAB của chúng tôi đạt được hiệu suất tốt hơn transformer Flatten. Cụ thể hơn, mô hình SLAB-Swin-T của chúng tôi đạt được độ chính xác top-1 83.6% trên ImageNet-1K với độ trễ 16.2ms, thấp hơn 2.4ms so với Flatten-Swin với độ chính xác cao hơn 0.1%. Các mô hình của chúng tôi hiệu quả hơn về mặt tính toán chủ yếu do các lớp chuẩn hóa thân thiện với phần cứng hơn cũng như module sự chú ý tuyến tính đơn giản. Hình 1 cũng cho thấy sự cân bằng giữa độ chính xác và độ trễ của transformer SLAB của chúng tôi, transformer Flatten (Han et al., 2023) và transformer Swin gốc, chứng minh hiệu suất tốt hơn của mô hình chúng tôi.

5.2. Phát hiện Vật thể
Cài đặt. Chúng tôi sử dụng Mask R-CNN (He et al., 2017) để đánh giá tính hiệu quả của phương pháp của mình trên bộ dữ liệu COCO cho các tác vụ phát hiện vật thể và phân đoạn thể hiện. Các backbone được sử dụng trong Mask R-CNN được pre-train trên ImageNet-1K. Tất cả các mô hình được huấn luyện cho 1×schedule, tức là 12 epochs. Độ trễ được đo với kích thước batch là 1 trên GPU V100 cho giá trị trung bình của 100 vòng.

Kết quả trên tác vụ phát hiện vật thể. Chúng tôi so sánh PRepBN được đề xuất của chúng tôi với LayerNorm tiêu chuẩn cho các backbone khác nhau bao gồm Swin và PVT cho tác vụ phát hiện vật thể. Kết quả được thể hiện trong Bảng 3. Nó cho thấy rằng phương pháp của chúng tôi đạt được hiệu suất khá tương đương với các mô hình gốc được trang bị LayerNorm. Tận dụng ưu điểm của chuẩn hóa ngoại tuyến, các mô hình với PRepBN được đề xuất của chúng tôi đạt được độ trễ suy luận thấp hơn. Ví dụ, độ trễ của Mask R-CNN giảm từ 64ms xuống 59.6ms khi backbone PVT-S được trang bị PRepBN của chúng tôi và độ chính xác cho phát hiện vật thể và phân đoạn thể hiện là tương tự. Một trực quan hóa rõ ràng hơn cho sự cân bằng giữa mAP và độ trễ cũng được trình bày trong Hình 5, chứng minh rằng phương pháp được đề xuất của chúng tôi đạt được hiệu suất tổng thể tốt hơn trong phát hiện vật thể.

5.3. Mô hình hóa ngôn ngữ
Cài đặt. Chúng tôi cũng đánh giá phương pháp được đề xuất của mình trên tác vụ mô hình hóa ngôn ngữ dựa trên Adaptive Inputs (Baevski & Auli, 2018). Chúng tôi huấn luyện các mô hình của mình trên bộ dữ liệu Wikitext-103 (Merity et al., 2016), chứa hơn 100 triệu token. Chúng tôi đặt số token trên mỗi GPU là 4096 và huấn luyện trên 8 GPU. Số token trên mỗi mẫu được giới hạn ở 512. Chúng tôi cũng áp dụng phương pháp của mình trên mô hình LLaMA-350M, tuân theo kiến trúc và cài đặt huấn luyện tương tự như các công trình trước (Yang et al., 2023; He et al., 2024).

Kết quả trên tác vụ mô hình hóa ngôn ngữ. Như được thể hiện trong Bảng 4, PRepBN của chúng tôi đạt được perplexity tương tự với mô hình được trang bị LayerNorm, trong khi độ trễ giảm từ 13.9 ms xuống 12.9 ms mỗi token. Bên cạnh đó, chúng tôi áp dụng PRepBN của mình trên các mô hình ngôn ngữ lớn hiện đại hơn như LLaMA, sử dụng biến thể của LayerNorm loại bỏ việc tính toán giá trị trung bình, tức là RMSNorm. Như được thể hiện trong Bảng 5, phương pháp của chúng tôi thành công tăng throughput từ 44.0 lên 50.4 token mỗi giây trên GPU V100, và đạt được độ chính xác trung bình thậm chí hơi tốt hơn. Những kết quả này chứng minh tính hiệu quả của PRepBN được đề xuất của chúng tôi trên tác vụ mô hình hóa ngôn ngữ.

--- TRANG 8 ---
SLAB: Transformers Hiệu Quả với Sự Chú Ý Tuyến Tính Đơn Giản và Chuẩn Hóa Batch Tái Tham Số Hóa Tiệm Tiến

Bảng 4. Kết quả perplexity trên bộ dữ liệu Wikitext-103. 256/480 chỉ kích thước cửa sổ ngữ cảnh đánh giá.
Mô hình # Tham số 256 480 Thời gian Suy luận(ms/t) Val. Test Val. Test

Adaptive Inputs w/ LN 247M 19.5 20.2 19.3 20.0 13.9
Adaptive Inputs w/ PRepBN (Của chúng tôi) 247M 19.2 20.0 19.1 19.8 12.9

Bảng 5. Kết quả thí nghiệm của phương pháp được đề xuất cho LLaMA-350M trên các bộ đánh giá khác nhau.
Mô hình Throughput ARC-C ARC-E BoolQ COPA HellaSwag PIQA WinoGrande OpenBookQA SciQ Trung bình

LLaMA-350M 44.0 22.95 46.13 59.27 64 33.19 64.36 49.09 29.6 75.3 49.32
w/ PRepBN (Của chúng tôi) 50.4 23.55 44.44 60.67 66 32.76 64.04 49.72 30.0 76.8 49.78

Bảng 6. Nghiên cứu khử thành phần về tác động của sự chú ý tuyến tính đơn giản và BatchNorm tái tham số hóa tiệm tiến.
Phương pháp FLOPs Lat. (ms) Acc. (%)

Flatten-DeiT-T 1.1 G 15.2 74.1
+ SLA 1.1 G 10.2 73.0
+ SLA + PRepBN 1.1 G 9.6 74.3

Flatten-PVT-T 2.0 G 10.8 77.8
+ SLA 2.0 G 8.5 75.2
+ SLA + PRepBN 2.0 G 8.0 76.5

Flatten-Swin-T 4.5 G 10.9 82.1
+ SLA 4.5 G 9.5 81.9
+ SLA + PRepBN 4.5 G 8.7 81.8

Flatten-Swin-S 8.8 G 18.6 83.5
+ SLA 8.8 G 18.0 83.4
+ SLA + PRepBN 8.7 G 16.2 83.6

5.4. Nghiên cứu Khử Thành Phần
Trong phần này, chúng tôi tiến hành các nghiên cứu khử thành phần rộng rãi để chứng minh tác động của các thiết kế chính của chúng tôi.

Tác động của SLA và PRepBN. Chúng tôi đầu tiên khám phá tác động của module sự chú ý tuyến tính đơn giản (SLA) và BatchNorm tái tham số hóa tiệm tiến (PRepBN) trên các backbone khác nhau. Như được thể hiện trong Bảng 6, việc sử dụng sự chú ý tuyến tính đơn giản (SLA) của chúng tôi mang lại cải thiện nhất quán về hiệu quả. Đối với DeiT và PVT, SLA của chúng tôi đạt được giảm độ trễ đáng kể và giảm độ chính xác một chút. Hơn nữa, các transformers Swin được trang bị SLA của chúng tôi đạt được độ chính xác khá tương đương với những transformer gốc nhưng với độ trễ thấp hơn. Ngoài ra, độ trễ có thể được giảm thêm bằng cách thay thế LayerNorm bằng BatchNorm tái tham số hóa tiệm tiến (PRepBN) được đề xuất của chúng tôi. Chiến lược này hầu như không ảnh hưởng đến độ chính xác và thậm chí khôi phục độ chính xác của mô hình như DeiT và PVT. Kết hợp hai chiến lược này, độ trễ được giảm 5.6 ms khi độ chính xác được cải thiện 0.2% cho DeiT-T. Hơn nữa, phương pháp của chúng tôi đạt được độ chính xác tương tự và thu được giảm độ trễ 2.2 ms và 2.4 ms cho các mô hình Swin-T và Swin-S.

Nghiên cứu khử thành phần cho PRepBN. Chúng tôi nghiên cứu các thành phần chính của PRepBN được đề xuất của chúng tôi, tức là chiến lược tiệm tiến và BatchNorm tái tham số hóa (RepBN). Việc huấn luyện trực tiếp transformer dựa trên BatchNorm dẫn đến huấn luyện khá không ổn định, hoặc đạt được hiệu suất kém hơn hoặc sụp đổ trong quá trình huấn luyện (ví dụ, DeiT-S và Flatten-Swin-T). Để tránh sự thay đổi phương sai (Li et al., 2019) gây ra bởi droppath, điều này sẽ ảnh hưởng đến hiệu suất của BatchNorm, chúng tôi đơn giản đặt tỷ lệ droppath thành 0 trên mô hình DeiT-T. Như được thể hiện trong Bảng 7, việc áp dụng chiến lược tiệm tiến trên mô hình DeiT-T dựa trên BatchNorm mang lại mức tăng độ chính xác 1.2%. Chúng tôi tiếp tục sử dụng RepBN của mình trong mô hình và độ chính xác tăng lên 73.6%. Những kết quả này chứng minh rằng cả chiến lược tiệm tiến được đề xuất của chúng tôi và BatchNorm tái tham số hóa (RepBN) đều có lợi cho việc huấn luyện transformer dựa trên BatchNorm thuần túy.

Bảng 7. Nghiên cứu khử thành phần về tác động của chiến lược tiệm tiến và BatchNorm tái tham số hóa.
Phương pháp Acc. (%)

DeiT-T-BN 71.9
+ Chiến lược Tiệm tiến 73.1
+ Chiến lược Tiệm tiến + RepBN 73.6

6. Kết luận
Trong bài báo này, chúng tôi nghiên cứu các module cổ chai tính toán của transformer và đề xuất các chiến lược mới bao gồm BatchNorm tái tham số hóa tiệm tiến và sự chú ý tuyến tính đơn giản để có được các kiến trúc transformer hiệu quả. Phương pháp của chúng tôi thay thế LayerNorm bằng BatchNorm tái tham số hóa một cách tiệm tiến trong quá trình huấn luyện để đạt được độ chính xác không mất mát, trong khi tận dụng các ưu điểm hiệu quả của BatchNorm trong quá trình suy luận. Ngoài ra, chúng tôi thiết kế một cơ chế sự chú ý tuyến tính đơn giản đạt được hiệu suất tương đương với các phương pháp sự chú ý tuyến tính khác nhưng với chi phí tính toán ít hơn. Thông qua các thí nghiệm rộng rãi cho cả tác vụ thị giác máy tính và mô hình hóa ngôn ngữ, chúng tôi chứng minh rằng phương pháp của mình đạt được hiệu suất mạnh hơn về độ chính xác và hiệu quả so với các phương pháp trước đây và làm sáng tỏ việc thiết kế transformer hiệu quả.

Lời cảm ơn. Chúng tôi biết ơn sự hỗ trợ của MindSpore (Huawei, 2020), CANN (Compute Architecture for Neural Networks) và Bộ xử lý AI Ascend được sử dụng cho nghiên cứu này.

--- TRANG 9 ---
SLAB: Transformers Hiệu Quả với Sự Chú Ý Tuyến Tính Đơn Giản và Chuẩn Hóa Batch Tái Tham Số Hóa Tiệm Tiến

Tuyên bố Tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học Sâu. Có nhiều hậu quả xã hội tiềm tàng của công trình của chúng tôi, không có gì mà chúng tôi cảm thấy phải được nhấn mạnh cụ thể ở đây.

Tài liệu Tham khảo
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.

Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853 , 2018.

Bolya, D., Fu, C.-Y ., Dai, X., Zhang, P., and Hoffman, J. Hydra attention: Efficient attention with many heads. In European Conference on Computer Vision , pp. 35–49. Springer, 2022.

Cai, H., Gan, C., and Han, S. Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition. arXiv preprint arXiv:2205.14756 , 2022.

Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. End-to-end object detection with transformers. In European conference on computer vision , pp. 213–229. Springer, 2020.

Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.

Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 , 2020.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee, 2009.

Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., and Sun, J. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 13733–13742, 2021.

Ding, X., Chen, H., Zhang, X., Huang, K., Han, J., and Ding, G. Re-parameterizing your optimizers rather than architectures. arXiv preprint arXiv:2205.15242 , 2022.

Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., and Guo, B. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12124–12134, 2022.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.

Han, D., Pan, X., Han, Y ., Song, S., and Huang, G. Flatten transformer: Vision transformer using focused linear attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5961–5971, 2023.

Han, K., Wang, Y ., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y ., Xiao, A., Xu, C., Xu, Y ., et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence , 45(1):87–110, 2022.

He, K., Gkioxari, G., Doll ´ar, P., and Girshick, R. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pp. 2961–2969, 2017.

He, W., Han, K., Tang, Y ., Wang, C., Yang, Y ., Guo, T., and Wang, Y . Densemamba: State space models with dense hidden connection for efficient large language models. arXiv preprint arXiv:2403.00818 , 2024.

Huawei. Mindspore. https://www.mindspore.cn/ , 2020.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning , pp. 448–456. pmlr, 2015.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning , pp. 5156–5165. PMLR, 2020.

Larsson, G., Maire, M., and Shakhnarovich, G. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648 , 2016.

Li, X., Chen, S., Hu, X., and Yang, J. Understanding the disharmony between dropout and batch normalization by variance shift. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 2682–2690, 2019.

Li, Z., Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale reparameterization for post-training quantization of vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 17227–17236, 2023.

Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pp. 740–755. Springer, 2014.

Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision , pp. 10012–10022, 2021.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 , 2016.

Tang, Q., Liu, C., Liu, F., Liu, Y ., Jiang, J., Zhang, B., Han, K., and Wang, Y . Category feature transformer for semantic segmentation. arXiv preprint arXiv:2308.05581 , 2023a.

Tang, Q., Zhang, B., Liu, J., Liu, F., and Liu, Y . Dynamic token pruning in plain vision transformers for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 777–786, 2023b.

Tang, Y ., Wang, Y ., Guo, J., Tu, Z., Han, K., Hu, H., and Tao, D. A survey on transformer compression. arXiv preprint arXiv:2402.05964 , 2024.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J ´egou, H. Training data-efficient image transformers & distillation through attention. In International conference on machine learning , pp. 10347–10357. PMLR, 2021.

Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y . Maxvit: Multi-axis vision transformer. In European conference on computer vision , pp. 459–479. Springer, 2022.

Ulyanov, D., Vedaldi, A., and Lempitsky, V . Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022 , 2016.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems , 30, 2017.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision , pp. 568–578, 2021.

Wang, Y ., Chen, X., Cao, L., Huang, W., Sun, F., and Wang, Y . Multimodal token fusion for vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 12186–12195, 2022.

Wu, X., Zeng, F., Wang, X., and Chen, X. PPT: Token pruning and pooling for efficient vision transformers. arXiv preprint arXiv:2310.01812 , 2023.

Wu, Y . and He, K. Group normalization. In Proceedings of the European conference on computer vision (ECCV) , pp. 3–19, 2018.

Xia, Z., Pan, X., Song, S., Li, L. E., and Huang, G. Dat++: Spatially dynamic vision transformer with deformable attention. arXiv preprint arXiv:2309.01430 , 2023.

Xu, Y ., Li, C., Li, D., Sheng, X., Jiang, F., Tian, L., and Sirasao, A. Fdvit: Improve the hierarchical architecture of vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5950–5960, 2023.

Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y ., and Sun, J. Towards stabilizing batch statistics in backward propagation of batch normalization. arXiv preprint arXiv:2001.06838 , 2020.

Yang, Q., Zhang, K., Lan, C., Yang, Z., Li, Z., Tan, W., Xiao, J., and Pu, S. Unified normalization for accelerating and stabilizing transformers. In Proceedings of the 30th ACM International Conference on Multimedia , pp. 4445–4455, 2022.

Yang, S., Wang, B., Shen, Y ., Panda, R., and Kim, Y . Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635 , 2023.

Yao, Z., Cao, Y ., Lin, Y ., Liu, Z., Zhang, Z., and Hu, H. Leveraging batch normalization for vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW) , pp. 413–422. IEEE, 2021.

You, H., Xiong, Y ., Dai, X., Wu, B., Zhang, P., Fan, H., Vajda, P., and Lin, Y . C. Castling-vit: Compressing self-attention via switching towards linear-angular attention at vision transformer inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 14431–14442, 2023.

Zheng, D., Dong, W., Hu, H., Chen, X., and Wang, Y . Less is more: Focus attention for efficient detr. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 6674–6683, 2023.

Zhu, L., Wang, X., Ke, Z., Zhang, W., and Lau, R. W. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10323–10333, 2023.

--- TRANG 11 ---
SLAB: Transformers Hiệu Quả với Sự Chú Ý Tuyến Tính Đơn Giản và Chuẩn Hóa Batch Tái Tham Số Hóa Tiệm Tiến

A. Cài đặt siêu tham số chi tiết.
Các cài đặt siêu tham số chi tiết được cung cấp trong Bảng A1. Đối với phân loại hình ảnh, chúng tôi tuân theo cài đặt của DeiT (Touvron et al., 2021). Chúng tôi sử dụng AdamW làm bộ tối ưu mặc định và huấn luyện tất cả mô hình trong 300 epochs với bộ lập lịch tốc độ học tập cosine decay và 20 epochs warm-up tuyến tính. Kích thước batch được đặt thành 1024, tốc độ học tập ban đầu là 0.001, và giá trị weight decay là 0.05. Đối với mô hình sử dụng batchnorm tái tham số hóa tiệm tiến được đề xuất, chúng tôi sử dụng tỷ lệ droppath nhỏ hơn (Larsson et al., 2016). Do sự thay đổi phương sai gây ra bởi droppath, chúng tôi đóng băng các tham số mô hình và chỉ cập nhật thống kê của BatchNorm tái tham số hóa trong 10 epochs cuối quá trình huấn luyện.

Bảng A1. Cài đặt siêu tham số trên các mô hình khác nhau cho tác vụ phân loại hình ảnh.
Tên DeiT-T DeiT-S PVT-T PVT-S PVT-M Swin-T Swin-S Swin-B
Epoch 300 300 300 300 300 300 300 300
Kích thước Batch 1024 1024 1024 1024 1024 1024 1024 1024
Tốc độ học tập 1e-3 1e-3 1e-3 1e-3 1e-3 1e-3 1e-3 1e-3
Bước Warmup 20 20 20 20 20 20 20 20
Bộ tối ưu AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW
Tỷ lệ Droppath 0.0 0.0 0.0 0.1 0.3 0.1 0.3 0.3
Bước Giảm Tuyến tính 0 3e5 0 3e5 3e5 0 3e5 3e5

B. Kết hợp với phương pháp hậu lượng tử hóa.
Phương pháp của chúng tôi tập trung vào việc thay thế LayerNorm bằng BatchNorm để đạt được tăng tốc suy luận mà không suy giảm hiệu suất. Đây là một chiến lược bổ sung với các phương pháp nén mô hình thông thường như lượng tử hóa trọng số, cắt tỉa hoặc chưng cất và những phương pháp này có thể được kết hợp để đạt được hiệu suất tốt hơn. Như một bằng chứng về khái niệm, chúng tôi tiến hành hậu lượng tử hóa sử dụng RepQ-ViT (Li et al., 2023) trên mô hình DeiT-Tiny được huấn luyện với chiến lược PRepBN của chúng tôi. Như được thể hiện trong bảng dưới đây, việc áp dụng lượng tử hóa W8A8 trên phương pháp của chúng tôi vẫn đạt được độ chính xác 73.6%, trong khi giảm thêm chi phí tính toán xuống chỉ 0.33 GFLOPs. Điều này chứng minh rằng phương pháp của chúng tôi có thể được kết hợp hiệu quả với các phương pháp nén khác như lượng tử hóa.

Bảng A2. Cài đặt siêu tham số trên các mô hình khác nhau cho tác vụ phân loại hình ảnh.
Phương pháp FLOPs (G) Throughput (hình ảnh/s) Top-1 Acc (%)

DeiT-T 1.3 3432 72.2
w/ PRepBN (Của chúng tôi) 1.3 4194 73.6
w/ PRepBN (Của chúng tôi) + RepQ-ViT (W8A8) 0.33 - 73.6
