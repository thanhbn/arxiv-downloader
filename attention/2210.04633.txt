# 2210.04633.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2210.04633.pdf
# File size: 687751 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CAT-probing: A Metric-based Approach to Interpret How Pre-trained
Models for Programming Language Attend Code Structure
Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Liy, Xuesong Lu, and Ming Gao
School of Data Science and Engineering, East China Normal University, Shanghai, China
{nuochen,qiushisun,renyuzhu}@stu.ecnu.edu.cn ,
{xiangli,xslu,mgao}@dase.ecnu.edu.cn
Abstract
Code pre-trained models (CodePTMs) have
recently demonstrated signiﬁcant success in
code intelligence. To interpret these mod-
els, some probing methods have been ap-
plied. However, these methods fail to con-
sider the inherent characteristics of codes. In
this paper, to address the problem, we pro-
pose a novel probing method CAT-probing
to quantitatively interpret how CodePTMs at-
tend code structure. We ﬁrst denoise the in-
put code sequences based on the token types
pre-deﬁned by the compilers to ﬁlter those to-
kens whose attention scores are too small. Af-
ter that, we deﬁne a new metric CAT-score to
measure the commonality between the token-
level attention scores generated in CodePTMs
and the pair-wise distances between corre-
sponding AST nodes. The higher the CAT-
score, the stronger the ability of CodePTMs
to capture code structure. We conduct ex-
tensive experiments to integrate CAT-probing
with representative CodePTMs for different
programming languages. Experimental re-
sults show the effectiveness of CAT-probing in
CodePTM interpretation. Our codes and data
are publicly available at https://github.
com/nchen909/CodeAttention .
1 Introduction
In the era of “Big Code” (Allamanis et al., 2018),
the programming platforms, such as GitHub and
Stack Overﬂow , have generated massive open-
source code data. With the assumption of “Soft-
ware Naturalness” (Hindle et al., 2016), pre-trained
models (Vaswani et al., 2017; Devlin et al., 2019;
Liu et al., 2019) have been applied in the domain
of code intelligence.
Existing code pre-trained models (CodePTMs)
can be mainly divided into two categories:
structure-free methods (Feng et al., 2020; Svy-
Equal contribution, authors are listed alphabetically.
yCorresponding author.atkovskiy et al., 2020) and structure-based meth-
ods (Wang et al., 2021b; Niu et al., 2022b). The
former only utilizes the information from raw code
texts, while the latter employs code structures,
such as data ﬂow (Guo et al., 2021) and ﬂattened
AST1(Guo et al., 2022), to enhance the perfor-
mance of pre-trained models. For more details,
readers can refer to Niu et al. (2022a). Recently,
there exist works that use probing techniques (Clark
et al., 2019a; Vig and Belinkov, 2019; Zhang et al.,
2021) to investigate what CodePTMs learn. For
example, Karmakar and Robbes (2021) ﬁrst probe
into CodePTMs and construct four probing tasks
to explain them. Troshin and Chirkova (2022) also
deﬁne a series of novel diagnosing probing tasks
about code syntactic structure. Further, Wan et al.
(2022) conduct qualitative structural analyses to
evaluate how CodePTMs interpret code structure.
Despite the success, all these methods lack quan-
titative characterization on the degree of how well
CodePTMs learn from code structure. Therefore,
a research question arises: Can we develop a new
probing way to evaluate how CodePTMs attend
code structure quantitatively?
In this paper, we propose a metric-based probing
method, namely, CAT-probing, to quantitatively
evaluate how CodePTMs Attention scores relate to
distances between AS Tnodes. First, to denoise the
input code sequence in the original attention scores
matrix, we classify the rows/cols by token types
that are pre-deﬁned by compilers, and then retain
tokens whose types have the highest proportion
scores to derive a ﬁltered attention matrix (see Fig-
ure 1(b)). Meanwhile, inspired by the works (Wang
et al., 2020; Zhu et al., 2022), we add edges to im-
prove the connectivity of AST and calculate the dis-
tances between nodes corresponding to the selected
tokens, which generates a distance matrix as shown
in Figure 1(c). After that, we deﬁne CAT-score to
measure the matching degree between the ﬁltered
1Abstract syntax tree.arXiv:2210.04633v4  [cs.SE]  10 Dec 2022

--- PAGE 2 ---
function_
definition
parametersblock
expression_
statementcall
attribute
attributeargument_
listdef write
(self,data)
self tmpbufappend
.. (data ):def write (self, data):    
self.tmpbuf.append(data)
...
Non-leaves
Leaves
Leaf edgesAST edges
Dataflow edges(a) A Python code snippet with its U-AST
def
write
(
self
,
data
)
:
self
.
tmpbuf
.
append
(
data
)def
write
(
self
,
data
)
:
self
.
tmpbuf
.
append
(
data
) (b) The attention matrix (ﬁltered)
def
write
(
self
,
data
)
:
self
.
tmpbuf
.
append
(
data
)def
write
(
self
,
data
)
:
self
.
tmpbuf
.
append
(
data
) (c) The distance matrix (ﬁltered)
Figure 1: Visualization on the U-AST structure, the attention matrix generated in the last layer of CodeBERT (Feng
et al., 2020) and the distance matrix. (a) A Python code snippet with its corresponding U-AST. (b) Heatmaps of the
averaged attention weights after attention matrix ﬁltering. (c) Heatmaps of the pair-wise token distance in U-AST.
In the heatmaps, the darker the color, the more salient the attention score, or the closer the nodes. In this toy
example, only the token “.” between “tmpbuf” and “append” is ﬁltered. More visualization examples of ﬁltering
are given in Appendix D.
attention matrix and the distance matrix. Speciﬁ-
cally, the point-wise elements of the two matrices
arematched if both the two conditions are satisﬁed:
1) the attention score is larger than a threshold; 2)
the distance value is smaller than a threshold. If
only one condition is reached, the elements are un-
matched . We calculate the CAT-score by the ratio
of the number of matched elements to the summa-
tion of matched and unmatched elements. Finally,
the CAT-score is used to interpret how CodePTMs
attend code structure, where a higher score indi-
cates that the model has learned more structural
information.
Our main contributions can be summarized as
follows:
•We propose a novel metric-based probing
method CAT-probing to quantitatively inter-
pret how CodePTMs attend code structure.
•We apply CAT-probing to several representa-
tive CodePTMs and perform extensive experi-
ments to demonstrate the effectiveness of our
method (See Section 4.3).
•We draw two fascinating observations from
the empirical evaluation: 1) The token types
that PTMs focus on vary with programming
languages and are quite different from the gen-
eral perceptions of human programmers (See
Section 4.2). 2) The ability of CodePTMs
to capture code structure dramatically differs
with layers (See Section 4.4).2 Code Background
2.1 Code Basics
Each code can be represented in two modals: the
source code and the code structure (AST), as shown
in Figure 1(a). In this paper, we use Tree-sitter2
to generate ASTs, where each token in the raw
code is tagged with a unique type, such as “iden-
tiﬁer”, “return” and “=”. Further, following these
works (Wang et al., 2020; Zhu et al., 2022), we
connect adjacent leaf nodes by adding data ﬂow
edges, which increases the connectivity of AST.
The upgraded AST is named as U-AST.
2.2 Code Matrices
There are two types of code matrices: the atten-
tion matrix and the distance matrix. Speciﬁcally,
the attention matrix denotes the attention score
generated by the Transformer-based CodePTMs,
while the distance matrix captures the distance be-
tween nodes in U-AST. We transform the original
subtoken-level attention matrix into the token-level
attention matrix by averaging the attention scores
of subtokens in a token. For the distance matrix, we
use the shortest-path length to compute the distance
between the leaf nodes of U-AST. Our attention
matrix and distance matrix are shown in Figure 1(b)
and Figure 1(c), respectively.
2github.com/tree-sitter

--- PAGE 3 ---
3 CAT-probing
3.1 Code Matrices Filtering
As pointed out in (Zhou et al., 2021), the atten-
tion scores in the attention matrix follow a long
tail distribution, which means that the majority of
attention scores are very small. To address the prob-
lem, we propose a simple but effective algorithm
based on code token types to remove the small val-
ues in the attention matrix. For space limitation,
we summarize the pseudocodes of the algorithm in
Appendix Alg.1. We only keep the rows/cols cor-
responding to frequent token types in the original
attention matrix and distance matrix to generate the
selected attention matrix and distance matrix.
3.2 CAT-score Calculation
After the two code matrices are ﬁltered, we deﬁne a
metric called CAT-score, to measure the common-
ality between the ﬁltered attention matrix Aand
the distance matrix D. Formally, the CAT-score is
formulated as:
CAT-score =P
CPn
i=1Pn
j=1 1Aij>AandDij<DP
CPn
i=1Pn
j=1 1Aij>AorDij<D;
(1)
whereCis the number of code samples, nis the
length of AorD, 1is the indicator function, A
andDdenotes the thresholds to ﬁlter matrix A
andD, respectively. Speciﬁcally, we calculate the
CAT-score of the last layer in CodePTMs. The
larger the CAT-score, the stronger the ability of
CodePTMs to attend code structure.
4 Evaluation
4.1 Experimental Setup
Task We evaluate the efﬁcacy of CAT-probing on
code summarization, which is one of the most chal-
lenging downstream tasks for code representation.
This task aims to generate a natural language (NL)
comment for a given code snippet, using smoothed
BLEU-4 scores (Lin and Och, 2004) as the metric.
Datasets We use the code summarization dataset
from CodeXGLUE (Lu et al., 2021) to evaluate the
effectiveness of our methods on four programming
languages (short as PLs), which are JavaScript, Go,
Python and Java. For each programming language,
we randomly select C= 3;000examples from the
training set for probing.Pre-trained models We select four models, in-
cluding one PTM, namely RoBERTa (Liu et al.,
2019), and three RoBERTa-based CodePTMs,
which are CodeBERT (Feng et al., 2020), Graph-
CodeBERT (Guo et al., 2021), and UniX-
coder (Guo et al., 2022). All these PTMs are com-
posed of 12layers of Transformer with 12attention
heads. We conduct layer-wise probing on these
models, where the layer attention score is deﬁned
as the average of 12heads’ attention scores in each
layer. The comparison of these models is intro-
duced in Appendix B. And the details of the exper-
imental implementation are given in Appendix C.
In the experiments, we aim to answer the three
research questions in the following:
•RQ1(Frequent Token Types) : What kind
of language-speciﬁc frequent token types do
these CodePTMs pay attention to?
•RQ2(CAT-probing Effectiveness) : Is CAT-
probing an effective method to evaluate how
CodePTMs attend code structure?
•RQ3(Layer-wise CAT-score) : How does the
CAT-score change with layers?
p_identifier           =
t_identifier      return          if
f_identifier           "           :0.2250.2500.2750.300Confidence
(a) Go
       public    s_literal       return            ;            )            }            {            ,            =0.1800.2250.2700.315Confidence (b) Java
         )
s_fragmentidentifier         ;         {
  function         (         ,0.1650.2200.2750.330Confidence
(c) JavaScript
          for           if            )            ]          def       return   identifier            :            ,            =            (0.2500.3750.5000.625Confidence (d) Python
Figure 2: Visualization of the frequent token types on
four programming languages.
4.2 Frequent Token Types
Figure 2(a)-(d) demonstrates the language-speciﬁc
frequent token types for four PLs, respectively.
From this ﬁgure, we see that: 1) Each PL has
its language-speciﬁc frequent token types and

--- PAGE 4 ---
RoBERTa CodeBERTGraphCodeBERTUniXcoder
Models101214161820Score(a) Go
RoBERTa CodeBERTGraphCodeBERTUniXcoder
Models10121416182022Score (b) Java
RoBERTa CodeBERTGraphCodeBERTUniXcoder
Models101112131415161718ScoreCAT-score( x)
best_bleu
best_ppl
(c) JavaScript
RoBERTa CodeBERTGraphCodeBERTUniXcoder
Models10121416182022Score (d) Python
Figure 3: Comparisons between the CAT-score and the
performance on code summarization task.
these types are quite different. For example, the
Top-3 frequent token types for Java are “public”,
“s_literal” and “return”, while Python are “for”, “if”,
“)”. 2) There is a signiﬁcant gap between the fre-
quent token types that CodePTMs focus on and the
general perceptions of human programmers. For
instance, CodePTMs assigned more attention to
code tokens such as brackets. 3) Attention distribu-
tion on Python code snippets signiﬁcantly differs
from others. This is caused by Python having lesser
token types than other PLs; thus, the models are
more likely to concentrate on a few token types.
4.3 CAT-probing Effectiveness
To verify the effectiveness of CAT-probing, we
compare the CAT-scores with the models’ perfor-
mance on the test set (using both best-bleu and
best-ppl checkpoints). The comparison among
different PLs is demonstrated in Figure 3. We
found strong concordance between the CAT-score
and the performance of encoder-only models, in-
cluding RoBERTa, CodeBERT, and GraphCode-
BERT. This demonstrates the effectiveness of our
approach in bridging CodePTMs and code struc-
ture. Also, this result (GraphCodeBERT > Code-
BERT > RoBERTa) suggests that for PTMs, the
more code features are considered in the input and
pre-training tasks, the better structural information
is learned.
In addition, we observe that UniXcoder has com-
01234567891011
Layers0.250.300.350.400.450.50Score
(a) Go
01234567891011
Layers0.150.200.250.300.350.400.450.50Score
RoBERTa
CodeBERT
GraphCodeBERT
UniXcoder (b) Java
01234567891011
Layers0.150.200.250.300.350.400.450.50Score
(c) JavaScript
01234567891011
Layers0.150.200.250.300.350.400.45Score
 (d) Python
Figure 4: Layer-wise CAT-score results.
pletely different outcomes from the other three
CodePTMs. This phenomenon is caused by UniX-
coder utilizing three modes in the pre-training stage
(encoder-only, decoder-only, and encoder-decoder).
This leads to a very different distribution of learned
attention and thus different results in the CAT-
score.
4.4 Layer-wise CAT-score
We end this section with a study on layer-wise CAT-
scores. Figure 4 gives the results of the CAT-score
on all the layers of PTMs. From these results, we
observe that: 1) The CAT-score decreases in gen-
eral when the number of layers increases on all the
models and PLs. This is because attention scores
gradually focus on some special tokens, reducing
the number of matching elements. 2) The relative
magnitude relationship (GraphCodeBERT > Code-
BERT > RoBERTa) between CAT-score is almost
determined on all the layers and PLs, which indi-
cates the effectiveness of CAT-score to recognize
the ability of CodePTMs in capturing code struc-
ture. 3) In the middle layers (4-8), all the results of
CAT-score change drastically, which indicates the
middle layers of CodePTMs may play an important
role in transferring general structural knowledge
into task-related structural knowledge. 4) In the
last layers (9-11), CAT-scores gradually converge,
i.e., the models learn the task-speciﬁc structural
knowledge, which explains why we use the score
at the last layer in CAT-probing.

--- PAGE 5 ---
5 Conclusion
In this paper, we proposed a novel probing method
named CAT-probing to explain how CodePTMs
attend code structure. We ﬁrst denoised the in-
put code sequences based on the token types pre-
deﬁned by the compilers to ﬁlter those tokens
whose attention scores are too small. After that,
we deﬁned a new metric CAT-score to measure
the commonality between the token-level atten-
tion scores generated in CodePTMs and the pair-
wise distances between corresponding AST nodes.
Experiments on multiple programming languages
demonstrated the effectiveness of our method.
6 Limitations
The major limitation of our work is that the adopted
probing approaches mainly focus on encoder-only
CodePTMs, which could be just one aspect of the
inner workings of CodePTMs. In our future work,
we will explore more models with encoder-decoder
architecture, like CodeT5 (Wang et al., 2021b) and
PLBART (Ahmad et al., 2021), and decoder-only
networks like GPT-C (Svyatkovskiy et al., 2020).
Acknowledgement
This work has been supported by the National Nat-
ural Science Foundation of China under Grant No.
U1911203, the National Natural Science Founda-
tion of China under Grant No. 62277017, Alibaba
Group through the Alibaba Innovation Research
Program, and the National Natural Science Foun-
dation of China under Grant No. 61877018, The
Research Project of Shanghai Science and Technol-
ogy Commission (20dz2260300) and The Funda-
mental Research Funds for the Central Universities.
And the authors would like to thank all the anony-
mous reviewers for their constructive and insightful
comments on this paper.
References
Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2021. Uniﬁed pre-training for pro-
gram understanding and generation. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 2655–2668,
Online. Association for Computational Linguistics.
Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
and Charles Sutton. 2018. A survey of machine
learning for big code and naturalness. ACM Com-
puting Surveys (CSUR) , 51(4):81.Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D. Manning. 2019a. What does BERT
look at? an analysis of BERT’s attention. In Pro-
ceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for
NLP, pages 276–286, Florence, Italy. Association
for Computational Linguistics.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D Manning. 2019b. What does bert
look at? an analysis of bert’s attention. In Proceed-
ings of the 2019 ACL Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP ,
pages 276–286.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
BERT: A pre-trained model for programming and
natural languages. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1536–1547, Online. Association for Computational
Linguistics.
Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming
Zhou, and Jian Yin. 2022. UniXcoder: Uniﬁed
cross-modal pre-training for code representation. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 7212–7225, Dublin, Ireland.
Association for Computational Linguistics.
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie LIU, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,
Shao Kun Deng, Colin Clement, Dawn Drain, Neel
Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
2021. GraphCodeBERT: Pre-training code represen-
tations with data ﬂow. In International Conference
on Learning Representations .
Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,
and Premkumar T. Devanbu. 2016. On the natural-
ness of software. Commun. ACM , 59(5):122–131.
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. CoRR , abs/1909.09436.
Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. 2020. Learning and evaluating
contextual embedding of source code. In Proceed-
ings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Vir-

--- PAGE 6 ---
tual Event , volume 119 of Proceedings of Machine
Learning Research , pages 5110–5121. PMLR.
Anjan Karmakar and Romain Robbes. 2021. What do
pre-trained code models know about code? In 2021
36th IEEE/ACM International Conference on Au-
tomated Software Engineering (ASE) , pages 1332–
1336. IEEE.
Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang
goo Lee. 2020. Are pre-trained language models
aware of phrases? simple but strong baselines for
grammar induction. In International Conference on
Learning Representations .
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation met-
rics for machine translation. In COLING 2004: Pro-
ceedings of the 20th International Conference on
Computational Linguistics , pages 501–507, Geneva,
Switzerland. COLING.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR , abs/2102.04664.
Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng.
2022a. Deep learning meets software engineer-
ing: A survey on pre-trained models of source code.
CoRR , abs/2205.11739.
Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge,
Liguo Huang, and Bin Luo. 2022b. Spt-code:
Sequence-to-sequence pre-training for learning the
representation of source code. arXiv preprint
arXiv:2201.01549 .
Ankita Nandkishor Sontakke, Manasi Patwardhan,
Lovekesh Vig, Raveendra Kumar Medicherla,
Ravindra Naik, and Gautam Shroff. 2022. Code
summarization: Do transformers really understand
code? In Deep Learning for Code Workshop .
Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
and Neel Sundaresan. 2020. Intellicode compose:
code generation using transformer. Proceedings
of the 28th ACM Joint Meeting on European Soft-
ware Engineering Conference and Symposium on
the Foundations of Software Engineering .
Sergey Troshin and Nadezhda Chirkova. 2022. Prob-
ing pretrained models of source code. arXiv preprint
arXiv:2202.08975 .Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Jesse Vig and Yonatan Belinkov. 2019. Analyzing
the structure of attention in a transformer language
model. In Proceedings of the 2019 ACL Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP , pages 63–76, Florence, Italy. As-
sociation for Computational Linguistics.
Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guan-
dong Xu, and Hai Jin. 2022. What do they capture? -
A structural analysis of pre-trained language models
for source code. CoRR , abs/2202.06840.
Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi
Jin. 2020. Detecting code clones with graph neu-
ral network and ﬂow-augmented abstract syntax
tree. In 2020 IEEE 27th International Conference
on Software Analysis, Evolution and Reengineering
(SANER) , pages 261–271.
Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao
Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin
Jiang. 2021a. Syncobert: Syntax-guided multi-
modal contrastive pre-training for code representa-
tion. arXiv preprint arXiv:2108.04556 .
Yanlin Wang and Hui Li. 2021. Code completion by
modeling ﬂattened abstract syntax trees as graphs.
InProceedings of the AAAI Conference on Artiﬁcial
Intelligence , volume 35, pages 14015–14023.
Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H.
Hoi. 2021b. CodeT5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code under-
standing and generation. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 8696–8708, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Sheng Zhang, Xin Zhang, Weiming Zhang, and An-
ders Søgaard. 2021. Sociolectal analysis of pre-
trained language models. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 4581–4588, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai
Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
2021. Informer: Beyond efﬁcient transformer for
long sequence time-series forecasting. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelli-
gence , volume 35, pages 11106–11115.
Renyu Zhu, Lei Yuan, Xiang Li, Ming Gao, and
Wenyuan Cai. 2022. A neural network architecture
for program understanding inspired by human be-
haviors. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics

--- PAGE 7 ---
(Volume 1: Long Papers) , pages 5142–5153, Dublin,
Ireland. Association for Computational Linguistics.
Daniel Zügner, Tobias Kirschstein, Michele Catasta,
Jure Leskovec, and Stephan Günnemann. 2021.
Language-agnostic representation learning of source
code from structure and context. In 9th Inter-
national Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net.
A Frequent Token Types Filtering
Algorithm
Algorithm 1 describes the procedure to generate
frequent token types.
B Comparison of CodePTMs
Table 2 gives the comparison of the PTMs used in
our experiments from three perspectives: the inputs
of the model, the pre-training task, and the training
mode.
C Experimental Implementation
We keep the same hyperparameter setting for all
CodePTMs. The detailed hyperparameters are
given in Table 1.
Our codes are implemented based on PyTorch.
All the experiments were conducted on a Linux
server with two interconnected NVIDIA-V100
GPUs.
Hyperparameter value
Batch Size 48
Learning Rate 5e-5
Weight Decay 0.0
Epsilon 1e-8
Epochs 15
Max Source Length 256
A third quartile of values in A
D ﬁrst quartile of values in D
Table 1: Hyperparameters for CAT-probingD Case Study
In addition to the example visualized in Figure 1,
we have carried out three new examples to show the
effectiveness of the ﬁltering strategy in Section 3.1,
The visualizations are shown in Table 3.

--- PAGE 8 ---
Algorithm 1 Frequent Token Type Selection
Input: Languagelang
Output: Frequent token type list type_list
1:rank = len(token types) * [0] .Initialize rank for each token type
2:fortin token types do
3: formin CodePTM models do
4: conﬁdence[t,m] = 0
5: forcin code cases do
6: att= get_att(m,lang ,c) .Get attention matrix
7: mask _theta = is_gt_theta( att).Setattposition greater than Ato 1, otherwise 0
8: mask _type = is_type_t(att) .Setattposition is type tto 1, otherwise 0
9: part = sum_mat(mask _theta &mask _type).Sum all elements of the matrix
10: overall = sum_mat(mask _type)
11: conﬁdence[t,m] conﬁdence[t,m] +part /overall . Compute conﬁdence
12: end for
13: conﬁdence[t,m] conﬁdence[t,m] / len(c) .Average conﬁdence
14: rank[t] rank[t] + get_rank(conﬁdence, m).Rank conﬁdence for m, and sum rank for t
15: end for
16:end for
Return: token type list includes those twith rank[t]<40
Models Inputs Pre-training Tasks Training Mode
RoBERTa Natural Language (NL) Masked Language Modeling (MLM) Encoder-only
CodeBERT NL-PL Pairs MLM+Replaced Token Detection (RTD) Encoder-only
GraphCodeBERT NL-PL Pairs & AST MLM+Edge Prediction+Node Alignment Encoder-only
UniXcoder NL-PL Pairs & Flattened ASTMLM Encoder &
ULM (Unidirectional Language Modeling) Decoder &
Denoising Objective (DNS) Encoder-decoder
Table 2: The comparison of different language models mentioned in this paper.

--- PAGE 9 ---
Source Code Attention HeatmapAttention Heatmap with
Token Type Selection
func
(
c
*
Cache
)
Size
(
)
int64
{
c
.
Lock
(
)
defer
c
.
Unlock
(
)
return
c
.
size
}func
(
c
*
Cache
)
Size
(
)
int64
{
c
.
Lock
(
)
defer
c
.
Unlock
(
)
return
c
.
size
}
Cache
Size
int64
Lock
Unlock
return
sizeCache Size int64 Lock Unlock return size
public
Object
PAInitialization
(
Object
bean
,
String
Name
)
throws
BException
{
Object
(
bean
)
;
return
bean
;
}public
Object
PAInitialization
(
Object
bean
,
String
Name
)
throws
BException
{
Object
(
bean
)
;
return
bean
;
}
public
,
)
{
)
;
return
;
}public, ) { ) ; return; }
function
(
)
{
return
window
.
Height
||
document
.
Element
[
LEXICON
.
H
]
||
document
.
body
[
LEXICON
.
H
]
;
}function
(
)
{
return
window
.
Height
||
document
.
Element
[
LEXICON
.
H
]
||
document
.
body
[
LEXICON
.
H
]
;
}
function
(
)
{
window
document
LEXICON
document
LEXICON
;function
(
)
{
window
document
LEXICON
document
LEXICON
;
Table 3: Heatmaps of the averaged attention weights in the last layer before and after using token selection, includ-
ing Go, Java, and JavaScript code snippets (from top to bottom).
