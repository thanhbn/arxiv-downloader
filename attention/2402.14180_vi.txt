# Linear Transformers are Versatile In-Context Learners
# Các Transformer Tuyến Tính là Những Người Học Trong Ngữ Cảnh Linh Hoạt

Max Vladymyrov
Google Research
mxv@google.com

Johannes von Oswald
Google, Paradigms of Intelligence Team
jvoswald@google.com

Mark Sandler
Google Research
sandler@google.com

Rong Ge
Duke University
rongge@cs.duke.edu

## Tóm tắt
Nghiên cứu gần đây đã chứng minh rằng các transformer, đặc biệt là các mô hình attention tuyến tính, ngầm thực hiện các thuật toán giống như gradient descent trên dữ liệu được cung cấp trong ngữ cảnh trong bước suy luận tiến của chúng. Tuy nhiên, khả năng của chúng trong việc xử lý các vấn đề phức tạp hơn vẫn chưa được khám phá. Trong bài báo này, chúng tôi chứng minh rằng mỗi lớp của một transformer tuyến tính duy trì một vector trọng số cho một bài toán hồi quy tuyến tính ngầm và có thể được diễn giải như việc thực hiện một biến thể của gradient descent có điều kiện tiên quyết. Chúng tôi cũng điều tra việc sử dụng các transformer tuyến tính trong một kịch bản thách thức nơi dữ liệu huấn luyện bị hỏng với các mức độ nhiễu khác nhau. Đáng chú ý, chúng tôi chứng minh rằng đối với vấn đề này, các transformer tuyến tính khám phá ra một thuật toán tối ưu hóa phức tạp và hiệu quả cao, vượt trội hoặc sánh ngang về hiệu suất với nhiều baseline hợp lý. Chúng tôi phân tích thuật toán này và cho thấy đó là một phương pháp tiếp cận mới kết hợp momentum và tái chia tỷ lệ thích ứng dựa trên mức độ nhiễu. Các phát hiện của chúng tôi cho thấy rằng ngay cả các transformer tuyến tính cũng sở hữu khả năng đáng ngạc nhiên để khám phá các chiến lược tối ưu hóa tinh vi.

## 1 Giới thiệu
Kiến trúc transformer (Vaswani et al., 2017) đã cách mạng hóa lĩnh vực máy học, thúc đẩy những đột phá trong nhiều lĩnh vực khác nhau và phục vụ như một nền tảng cho các mô hình mạnh mẽ (Anil et al., 2023; Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023). Tuy nhiên, mặc dù thành công rộng rãi, các cơ chế thúc đẩy hiệu suất của chúng vẫn là một lĩnh vực nghiên cứu đang hoạt động.

Một thành phần chính của thành công của chúng được quy cho việc học trong ngữ cảnh (ICL, Brown et al., 2020) – một khả năng nổi lên của các transformer để đưa ra dự đoán dựa trên thông tin được cung cấp trong chính chuỗi đầu vào, mà không cần cập nhật tham số rõ ràng.

Gần đây, một số bài báo (Garg et al., 2022; Akyürek et al., 2022; von Oswald et al., 2023a) đã đề xuất rằng ICL có thể được giải thích một phần bởi một meta-tối ưu hóa ngầm của các transformer xảy ra trên ngữ cảnh đầu vào (còn gọi là mesa-optimization Hubinger et al., 2019). Họ đã chỉ ra rằng các transformer với các lớp self-attention tuyến tính (hay còn gọi là transformer tuyến tính) được huấn luyện trên các nhiệm vụ hồi quy tuyến tính có thể triển khai nội bộ tối ưu hóa dựa trên gradient.

Cụ thể, von Oswald et al. (2023a) đã chứng minh rằng các transformer tuyến tính có thể thực hiện các vòng lặp của một thuật toán tương tự như thuật toán gradient descent (mà họ gọi là GD++), với mỗi lớp attention đại diện cho một bước của thuật toán. Sau đó, Ahn et al. (2023); Zhang et al. (2023) tiếp tục đặc trưng hóa hành vi này, cho thấy giải pháp đã học là một dạng GD có điều kiện tiên quyết, và giải pháp này là tối ưu cho các transformer tuyến tính một lớp.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2402.14180v2  [cs.LG]  30 Oct 2024

Trong bài báo này, chúng tôi tiếp tục nghiên cứu các transformer tuyến tính được huấn luyện trên các bài toán hồi quy tuyến tính. Chúng tôi chứng minh rằng mỗi lớp của mọi transformer tuyến tính duy trì một vector trọng số cho một bài toán hồi quy tuyến tính cơ bản. Dưới một số hạn chế, thuật toán mà nó chạy có thể được diễn giải như một biến thể phức tạp của gradient descent có điều kiện tiên quyết với các hành vi giống momentum.

Mặc dù việc duy trì một mô hình hồi quy tuyến tính (bất kể dữ liệu) có thể có vẻ hạn chế, chúng tôi cho thấy các transformer tuyến tính có thể khám phá ra các thuật toán tối ưu hóa mạnh mẽ. Như một ví dụ đầu tiên, chúng tôi chứng minh rằng trong trường hợp GD++, điều kiện tiên quyết dẫn đến một thuật toán tối ưu hóa bậc hai.

Hơn nữa, chúng tôi chứng minh rằng các transformer tuyến tính có thể được huấn luyện để khám phá ra các thuật toán thậm chí mạnh mẽ và phức tạp hơn. Chúng tôi đã sửa đổi công thức vấn đề để xem xét hồi quy tuyến tính hỗn hợp với các mức độ nhiễu khác nhau¹ (lấy cảm hứng từ Bai et al., 2023). Đây là một vấn đề khó và không tầm thường không có giải pháp dạng đóng rõ ràng, vì nó cần tính đến các mức độ nhiễu khác nhau trong đầu vào.

Các thí nghiệm của chúng tôi với hai phân phối phương sai nhiễu khác nhau (đồng nhất và phân loại) chứng minh tính linh hoạt đáng chú ý của các transformer tuyến tính. Huấn luyện một transformer tuyến tính trong các thiết lập này dẫn đến một thuật toán vượt trội hơn GD++ cũng như các baseline khác nhau được rút ra từ giải pháp dạng đóng chính xác của hồi quy ridge. Chúng tôi khám phá ra rằng kết quả này vẫn đúng ngay cả khi huấn luyện một transformer tuyến tính với các ma trận trọng số đường chéo.

Thông qua phân tích chi tiết, chúng tôi tiết lộ những khác biệt chính với GD++, bao gồm số hạng giống momentum và tái chia tỷ lệ thích ứng dựa trên mức độ nhiễu.

Các phát hiện của chúng tôi đóng góp vào nội dung nghiên cứu ngày càng tăng nơi các thuật toán mới, hiệu suất cao đã được khám phá trực tiếp thông qua việc kỹ thuật ngược các trọng số transformer. Công trình này mở rộng hiểu biết của chúng ta về khả năng học ngầm của các mô hình dựa trên attention và làm nổi bật tính linh hoạt đáng chú ý của ngay cả các transformer tuyến tính đơn giản như những người học trong ngữ cảnh. Chúng tôi chứng minh rằng các transformer có tiềm năng khám phá ra các thuật toán hiệu quả có thể thúc đẩy tiến bộ của nghệ thuật tối ưu hóa và máy học nói chung.

## 2 Sơ bộ
Trong phần này, chúng tôi giới thiệu các ký hiệu cho các transformer tuyến tính, dữ liệu và loại vấn đề chúng tôi xem xét.

### 2.1 Transformer tuyến tính và học trong ngữ cảnh
Cho chuỗi đầu vào e₁, e₂, ..., eₙ ∈ Rᵈ⁺¹, một head đơn trong một lớp self-attention tuyến tính thường được tham số hóa bởi bốn ma trận, key Wₖ, query Wᵩ, value Wᵥ và projection Wₚ. Đầu ra của lớp không nhân quả tại vị trí i là eᵢ + Δeᵢ nơi Δeᵢ được tính như

Δeᵢ = Wₚ ∑ⱼ₌₁ⁿ ⟨Wᵩeᵢ, Wₖeⱼ⟩Wᵥeⱼ                 (1)

Tương đương, có thể sử dụng các tham số P = WₚWᵥ và Q = Wₖ^⊤Wᵩ, và phương trình trở thành

Δeᵢ = P ∑ⱼ₌₁ⁿ (eⱼ^⊤Qeᵢ)Peⱼ                     (2)

Nhiều head (P₁, Q₁), (P₂, Q₂), ..., (Pₕ, Qₕ) đơn giản cộng các hiệu ứng của chúng

Δeᵢ = ∑ₖ₌₁ᴴ ∑ⱼ₌₁ⁿ (eⱼ^⊤Qₖeᵢ)Pₖeⱼ              (3)

Chúng tôi định nghĩa một transformer tuyến tính như một mạng nơ-ron đa lớp được cấu thành từ L lớp self-attention tuyến tính được tham số hóa bởi θ = {Qₖˡ, Pₖˡ} với k = 1...H, l = 1...L. Để cô lập các cơ chế cốt lõi, chúng tôi xem xét một kiến trúc decoder-only đơn giản hóa, loại trừ các thành phần MLP và LayerNorm. Kiến trúc này cũng được sử dụng trong công trình trước đây (von Oswald et al., 2023a; Ahn et al., 2023).

Chúng tôi xem xét hai phiên bản của transformer tuyến tính: FULL với các tham số transformer được biểu diễn bởi các ma trận đầy đủ và DIAG, nơi các tham số bị hạn chế chỉ là các ma trận đường chéo.

Lấy cảm hứng từ von Oswald et al. (2023a), trong bài báo này chúng tôi xem xét dữ liệu hồi quy như chuỗi token. Mỗi token eᵢ = (xᵢ, yᵢ) ∈ Rᵈ⁺¹ bao gồm một vector đặc trưng xᵢ ∈ Rᵈ và đầu ra tương ứng yᵢ ∈ R. Ngoài ra, chúng tôi thêm một token truy vấn eₙ₊₁ = (xₜ, 0) vào chuỗi, nơi xₜ ∈ Rᵈ đại diện cho dữ liệu kiểm tra. Mục tiêu của việc học trong ngữ cảnh là dự đoán yₜ cho dữ liệu kiểm tra xₜ. Chúng tôi hạn chế attention chỉ tập trung vào n token đầu tiên của chuỗi để nó bỏ qua token truy vấn.

Chúng tôi sử dụng (xᵢˡ, yᵢˡ) để ký hiệu token thứ i trong đầu ra của transformer tại lớp l. Lớp ban đầu đơn giản là đầu vào: (xᵢ⁰, yᵢ⁰) = (xᵢ, yᵢ). Đối với một mô hình với tham số θ, chúng tôi đọc dự đoán bằng cách lấy âm² của tọa độ cuối cùng của token cuối trong lớp cuối như ŷθ({e₁, ..., eₙ}, eₙ₊₁) = -yₙ₊₁ᴸ.

Hãy cũng định nghĩa ký hiệu sau để được sử dụng xuyên suốt bài báo

Σ = ∑ᵢ₌₁ⁿ xᵢ(xᵢ)^⊤; α = ∑ᵢ₌₁ⁿ yᵢxᵢ; λ = ∑ᵢ₌₁ⁿ (yᵢ)²

Σˡ = ∑ᵢ₌₁ⁿ xᵢˡ(xᵢˡ)^⊤; αˡ = ∑ᵢ₌₁ⁿ yᵢˡxᵢˡ; λˡ = ∑ᵢ₌₁ⁿ (yᵢˡ)²

### 2.2 Mô hình hồi quy nhiễu
Như một mô hình bài toán, chúng tôi xem xét dữ liệu được tạo từ một mô hình hồi quy tuyến tính nhiễu. Đối với mỗi chuỗi đầu vào τ, chúng tôi lấy mẫu một vector trọng số ground-truth wτ ~ N(0, I), và tạo n điểm dữ liệu như xᵢ ~ N(0, I) và yᵢ = ⟨wτ, xᵢ⟩ + ξᵢ, với nhiễu ξᵢ ~ N(0, σ²τ).

Lưu ý rằng mỗi chuỗi có thể có các vector trọng số ground-truth khác nhau wτ, nhưng mọi điểm dữ liệu trong chuỗi chia sẻ cùng wτ và στ. Truy vấn được tạo như xₜ ~ N(0, I) và yₜ = ⟨wτ, xₜ⟩ (vì nhiễu độc lập, việc chúng ta có bao gồm nhiễu trong yq hay không sẽ chỉ là một hằng số cộng thêm vào mục tiêu cuối cùng).

Chúng tôi tiếp tục định nghĩa một mất mát bình phương tối thiểu thông thường (OLS) như

LOLS(w) = ∑ᵢ₌₁ⁿ (yᵢ - ⟨w, xᵢ⟩)²                   (4)

Giải pháp OLS là w* := Σ⁻¹α với phần dư rᵢ := yᵢ - ⟨w*, xᵢ⟩.

Trong sự hiện diện của nhiễu στ, w* nói chung không bằng ground truth wτ. Đối với một mức nhiễu στ đã biết, ước lượng tốt nhất cho wτ được cung cấp bởi hồi quy ridge:

LRR(w) = ∑ᵢ₌₁ⁿ (yᵢ - ⟨w, xᵢ⟩)² + σ²τ‖w‖²         (5)

với giải pháp w*σ² := (Σ + σ²τI)⁻¹α. Tất nhiên, trong thực tế phương sai của nhiễu không được biết và phải được ước lượng từ dữ liệu.

### 2.3 Các vấn đề phương sai nhiễu cố định so với hỗn hợp
Chúng tôi xem xét hai vấn đề khác nhau trong khung hồi quy tuyến tính nhiễu.

**Phương sai nhiễu cố định.** Trong kịch bản này, phương sai στ vẫn không đổi cho tất cả dữ liệu huấn luyện. Ở đây, mất mát trong ngữ cảnh là:

L(θ) = E[wτ~N(0,I), xᵢ~N(0,I), ξᵢ~N(0,σ²τ)] (ŷθ({e₁, ..., eₙ}, eₙ₊₁) - yₜ)²   (6)

nơi eᵢ = (xᵢ, yᵢ) và yᵢ = ⟨wτ, xᵢ⟩ + ξᵢ. Vấn đề này ban đầu được khám phá bởi Garg et al. (2022). Sau đó, von Oswald et al. (2023a) đã chứng minh rằng một transformer tuyến tính (6) hội tụ về một dạng giải pháp gradient descent, mà họ gọi là GD++. Chúng tôi định nghĩa điều này chi tiết sau.

**Phương sai nhiễu hỗn hợp.** Trong trường hợp này, phương sai nhiễu στ được rút từ một phân phối cố định p(στ) cho mỗi chuỗi. Mất mát học trong ngữ cảnh trở thành:

L(θ) = E[wτ~N(0,I), xᵢ~N(0,I), ξᵢ~N(0,σ²τ), στ~p(στ)] (ŷθ({e₁, ..., eₙ}, eₙ₊₁) - yₜ)²   (7)

Nói cách khác, mỗi chuỗi huấn luyện τ có một mức nhiễu cố định στ, nhưng các chuỗi huấn luyện khác nhau có các mức nhiễu khác nhau được lấy mẫu từ một phân phối chỉ định p(στ). Kịch bản này thêm độ phức tạp vì mô hình phải dự đoán wτ cho phân phối nhiễu thay đổi, và giải pháp tối ưu có khả năng sẽ liên quan đến một số loại ước lượng nhiễu. Chúng tôi đã phát hiện rằng theo kinh nghiệm, GD++ thất bại trong việc mô hình hóa phương sai nhiễu này và thay vào đó hội tụ về một giải pháp có thể được diễn giải như một ước lượng phương sai nhiễu đơn lẻ trên tất cả dữ liệu đầu vào.

## 3 Công trình liên quan

**Học trong ngữ cảnh như Gradient Descent** Công trình của chúng tôi xây dựng trên nghiên cứu đóng khung việc học trong ngữ cảnh như (các biến thể của) gradient descent (Akyürek et al., 2022; von Oswald et al., 2023a). Đối với transformer tuyến tính 1 lớp, một số công trình Zhang et al. (2023); Mahankali et al. (2023); Ahn et al. (2023) đã đặc trưng hóa các tham số tối ưu và động lực học huấn luyện. Các công trình gần đây hơn đã mở rộng các ý tưởng cho các mô hình auto-regressive (Li et al., 2023; von Oswald et al., 2023b) và các mô hình phi tuyến (Cheng et al., 2023). Fu et al. (2023) nhận thấy rằng các transformer hoạt động tương tự như các phương pháp Newton bậc hai trên dữ liệu tuyến tính, mà chúng tôi đưa ra một giải thích hợp lý trong Định lý 5.1.

**Học trong ngữ cảnh trong LLM** Cũng có nhiều công trình nghiên cứu cách học trong ngữ cảnh hoạt động trong các LLM được huấn luyện trước (Kossen et al., 2023; Wei et al., 2023; Hendel et al., 2023; Shen et al., 2023). Do độ phức tạp của các mô hình như vậy, cơ chế chính xác cho việc học trong ngữ cảnh vẫn là một vấn đề mở lớn. Một số công trình (Olsson et al., 2022; Chan et al., 2022; Akyürek et al., 2024) đã xác định induction head như một cơ chế quan trọng cho các nhiệm vụ học trong ngữ cảnh đơn giản, chẳng hạn như sao chép, dịch token và khớp mẫu.

**Các lý thuyết khác cho việc huấn luyện transformer** Ngoài thiết lập các mô hình tuyến tính, một số công trình khác (Garg et al., 2022; Tarzanagh et al., 2023; Li et al., 2023; Huang et al., 2023; Tian et al., 2023a,b) đã xem xét tối ưu hóa các transformer dưới các giả định dữ liệu và mô hình khác nhau. Wen et al. (2023) đã chỉ ra rằng có thể khó diễn giải "thuật toán" được thực hiện bởi các transformer mà không có những hạn chế rất mạnh.

**Các mô hình tuyến tính hỗn hợp** Một số công trình đã quan sát rằng các transformer có thể đạt được hiệu suất tốt trên hỗn hợp các mô hình tuyến tính (Bai et al., 2023; Pathak et al., 2023; Yadlowsky et al., 2023). Mặc dù các công trình này cho thấy rằng các transformer có thể triển khai nhiều biến thể của các kỹ thuật lựa chọn mô hình, kết quả của chúng tôi cho thấy rằng các transformer tuyến tính giải quyết các vấn đề như vậy bằng cách khám phá thuật toán tối ưu hóa thú vị với nhiều siêu tham số được điều chỉnh trong quá trình huấn luyện. Một chiến lược như vậy khá khác với các cách truyền thống để thực hiện lựa chọn mô hình. Các transformer cũng được biết là có thể triển khai các thuật toán mạnh trong nhiều thiết lập khác nhau (Guo et al., 2023; Giannou et al., 2023).

**Hiệu quả của các transformer tuyến tính và giống kernel** Một ràng buộc chính đối với kiến trúc transformer là nó mất thời gian O(N²) cho một chuỗi có độ dài N, trong khi đối với một transformer tuyến tính điều này có thể được cải thiện thành O(N). Mirchandani et al. (2023) đã chỉ ra rằng ngay cả các transformer tuyến tính cũng khá mạnh mẽ cho nhiều nhiệm vụ. Các công trình khác (Katharopoulos et al., 2020; Wang et al., 2020; Schlag et al., 2021; Choromanski et al., 2020) sử dụng các ý tưởng tương tự như kernel/random features để cải thiện thời gian chạy gần như tuyến tính mà không mất nhiều hiệu suất.

## 4 Các transformer tuyến tính duy trì mô hình hồi quy tuyến tính tại mỗi lớp

Mặc dù các transformer lớn, phi tuyến có thể mô hình hóa mối quan hệ phức tạp, chúng tôi cho thấy rằng các transformer tuyến tính bị hạn chế để duy trì một mô hình hồi quy tuyến tính dựa trên đầu vào, theo nghĩa là đầu ra của lớp thứ l luôn là một hàm tuyến tính của đầu vào với các hệ số tiềm ẩn (và có thể phi tuyến).

**Định lý 4.1.** Giả sử đầu ra của một transformer tuyến tính tại lớp thứ l là
(x₁ˡ, y₁ˡ), (x₂ˡ, y₂ˡ), ..., (xₙˡ, yₙˡ), (xₜˡ, yₜˡ), thì tồn tại các ma trận Mˡ, vector uˡ, wˡ và scalar aˡ sao cho

x_{i}^{l+1} = M^l x_i + y_i u^l, x_{t}^{l+1} = M^l x_t,
y_{i}^{l+1} = a^l y_i - ⟨w^l, x_i⟩, y_{t}^{l+1} = -⟨w^l, x_t⟩.

Lưu ý rằng Mˡ, uˡ, wˡ và aˡ không tuyến tính trong đầu vào, nhưng điều này vẫn đặt ra những hạn chế về những gì các transformer tuyến tính có thể làm. Ví dụ, chúng tôi cho thấy rằng nó không thể biểu diễn một hàm bậc hai:

**Định lý 4.2.** Giả sử đầu vào cho một transformer tuyến tính là (x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ) nơi xᵢ ~ N(0, I) và yᵢ = w^⊤xᵢ, cho đầu ra lớp thứ l là (x₁ˡ, y₁ˡ), (x₂ˡ, y₂ˡ), ..., (xₙˡ, yₙˡ) và cho yˡ = (y₁ˡ, ..., yₙˡ) và y* = (x₁(1)², x₂(1)², ..., xₙ(1)²) (ở đây xᵢ(1) chỉ là tọa độ đầu tiên của xᵢ), thì khi n ≫ d với xác suất cao độ tương tự cosine của y* và yˡ không quá 0.1.

Định lý 4.1 ngụ ý rằng đầu ra của transformer tuyến tính luôn có thể được giải thích như tổ hợp tuyến tính của đầu vào với các trọng số tiềm ẩn aˡ và wˡ. Các ma trận Mˡ, vector uˡ, wˡ và số aˡ không tuyến tính và thực tế có thể khá phức tạp, mà chúng tôi đặc trưng hóa bên dưới:

**Bổ đề 4.3.** Trong thiết lập của Định lý 4.1, nếu chúng ta đặt

[A^l  b^l]
[(c^l)^⊤ d^l] := ∑_{k=1}^h P_k^l ∑_{j=1}^n [x_j^l] [(x_j^l)^⊤, y_j^l] Q_k^l
                                                [y_j^l]

thì có thể tính toán đệ quy các ma trận Mˡ, vector uˡ, wˡ và số aˡ cho mỗi lớp sử dụng

M^{l+1} = (I + A^l)M^l + b^l(w^l)^⊤
u^{l+1} = (I + A^l)u^l + a^l b^l
a^{l+1} = (1 + d^l)a^l + ⟨c^l, u^l⟩
w^{l+1} = (1 + d^l)w^l - (M^l)^⊤c^l,

với điều kiện khởi tạo a⁰ = 1, w⁰ = 0, M⁰ = I, u⁰ = 0.

Các cập nhật cho các tham số là phức tạp và phi tuyến, cho phép các transformer tuyến tính triển khai các thuật toán mạnh mẽ, như chúng ta sẽ thấy sau trong Phần 5. Thực tế, ngay cả với P và Q đường chéo, chúng vẫn linh hoạt. Các cập nhật trong trường hợp này có thể được đơn giản hóa thành một dạng quen thuộc hơn:

**Bổ đề 4.4.** Trong thiết lập của Định lý 4.1 với các tham số đường chéo, uˡ, wˡ được cập nhật như

u^{l+1} = (I - Λ^l)u^l + Γ^l Σ(a^l w^* - w^l);
w^{l+1} = (1 + s^l)w^l - Π^l Σ(a^l w^* - w^l) - Φ^l u^l.

Ở đây Λˡ, Γˡ, sˡ, Πˡ, Φˡ là các ma trận và số phụ thuộc vào Mˡ, uˡ, aˡ, wˡ trong Bổ đề 4.3.

Lưu ý rằng Σ(aˡw* - wˡ) là (tỷ lệ thuận với) gradient của một mô hình tuyến tính f(wˡ) = ∑ᵢ₌₁ⁿ(aˡyᵢ - ⟨wˡ, xᵢ⟩)². Điều này làm cho các cập nhật tương tự như gradient descent với momentum:

u^{l+1} = (1-β)u^l + ∇f(w^l); w^{l+1} = w^l - ηu^l.

Tất nhiên, công thức trong Bổ đề 4.4 vẫn phức tạp hơn nhiều với các ma trận tại chỗ β và η, và cũng bao gồm một số hạng gradient cho việc cập nhật w.

## 5 Sức mạnh của các ma trận attention đường chéo

Mặc dù các transformer tuyến tính bị ràng buộc, chúng có thể giải quyết các bài toán học trong ngữ cảnh phức tạp. Theo kinh nghiệm, chúng tôi đã phát hiện rằng chúng có thể giải quyết một cách rất chính xác hồi quy tuyến tính với phương sai nhiễu hỗn hợp (7), với các trọng số học được cuối cùng rất nặng về đường chéo với một số thành phần hạng thấp (xem Hình 4). Đáng ngạc nhiên, mất mát cuối cùng vẫn đáng chú ý nhất quán ngay cả khi các ma trận Q và P của chúng (3) là đường chéo. Ở đây chúng tôi sẽ phân tích trường hợp đặc biệt này và giải thích hiệu quả của nó.

Vì các phần tử của x là bất biến hoán vị, một tham số hóa đường chéo giảm mỗi head attention xuống chỉ bốn tham số:

P_k^l = [p_{x,k}^l I    0    ]
        [0           p_{y,k}^l]; Q_k^l = [q_{x,k}^l I    0    ]
                                         [0           q_{y,k}^l]    (8)

Sẽ hữu ích để tái tham số hóa thêm transformer tuyến tính (3) sử dụng:

ω_{xx}^l = ∑_{k=1}^H p_{x,k}^l q_{x,k}^l, ω_{xy}^l = ∑_{k=1}^H p_{x,k}^l q_{y,k}^l,
ω_{yx}^l = ∑_{k=1}^H p_{y,k}^l q_{x,k}^l, ω_{yy}^l = ∑_{k=1}^H p_{y,k}^l q_{y,k}^l.    (9)

Điều này dẫn đến các cập nhật lớp đường chéo sau:

x_i^{l+1} = x_i^l + ω_{xx}^l Σ^l x_i^l + ω_{xy}^l y_i^l α^l
x_t^{l+1} = x_t^l + ω_{xx}^l Σ^l x_t^l + ω_{xy}^l y_t^l α^l
y_i^{l+1} = y_i^l + ω_{yx}^l ⟨α^l, x_i^l⟩ + ω_{yy}^l y_i^l λ^l,
y_t^{l+1} = y_t^l + ω_{yx}^l ⟨α^l, x_t^l⟩ + ω_{yy}^l y_t^l λ^l.    (10)

Bốn biến ω_{xx}^l, ω_{xy}^l, ω_{yx}^l, ω_{yy}^l đại diện cho luồng thông tin giữa dữ liệu và nhãn qua các lớp. Ví dụ, số hạng được điều khiển bởi ω_{xx}^l đo luồng thông tin từ x^l đến x^{l+1}, ω_{yx}^l đo luồng từ x^l đến y^{l+1} và cứ thế. Vì mô hình luôn có thể được nắm bắt bởi 4 biến này, việc có nhiều head không tăng đáng kể sức mạnh biểu diễn của nó. Khi chỉ có một head, phương trình ω_{xx}^l ω_{yy}^l = ω_{xy}^l ω_{yx}^l luôn đúng, trong khi các mô hình với nhiều hơn một head không có hạn chế này. Tuy nhiên, theo kinh nghiệm ngay cả các mô hình với một head cũng khá mạnh mẽ.

### 5.1 GD++ và bộ giải bình phương tối thiểu

GD++, được giới thiệu trong von Oswald et al. (2023a), đại diện cho một transformer tuyến tính được huấn luyện trên vấn đề phương sai nhiễu cố định (6). Đó là một biến thể của transformer tuyến tính đường chéo, với tất cả các head thỏa mãn q_{y,k}^l = 0. Động lực học chỉ bị ảnh hưởng bởi ω_{xx}^l và ω_{yx}^l, dẫn đến các cập nhật đơn giản hơn:

x_i^{l+1} = (I + ω_{xx}^l Σ^l)x_i^l
y_i^{l+1} = y_i^l + ω_{yx}^l ⟨α^l, x_i^l⟩.    (11)

Cập nhật trên x hoạt động như điều kiện tiên quyết và cập nhật trên y thực hiện gradient descent trên dữ liệu.

Mặc dù phân tích hiện tại của Ahn et al. (2023) chưa cho ra tỷ lệ hội tụ nhanh cho GD++, chúng tôi chỉ ra ở đây rằng nó thực sự là một thuật toán tối ưu hóa bậc hai cho bài toán bình phương tối thiểu (4):

**Định lý 5.1.** Cho (x₁, y₁), ..., (xₙ, yₙ), (xₜ, 0) nơi Σ có các eigenvalue trong khoảng [ν, μ] với số điều kiện κ = ν/μ. Cho w* là giải pháp tối ưu cho bài toán bình phương tối thiểu (4), thì tồn tại các siêu tham số cho thuật toán GD++ mà xuất ra ŷ với độ chính xác |ŷ - ⟨xₜ, w*⟩| ≤ ε‖xₜ‖‖w*‖ trong l = O(log κ + log log 1/ε) bước. Đặc biệt điều đó ngụ ý tồn tại một transformer tuyến tính l-lớp có thể giải quyết nhiệm vụ này.

Tỷ lệ hội tụ O(log log 1/ε) thường chỉ đạt được bởi các thuật toán bậc hai như phương pháp Newton.

### 5.2 Hiểu ω_{yy}: tái chia tỷ lệ thích ứng

Nếu một lớp chỉ có ω_{yy}^l ≠ 0, nó có hiệu ứng tái chia tỷ lệ. Lượng chia tỷ lệ liên quan đến lượng nhiễu được thêm vào trong thiết lập lựa chọn mô hình. Quy tắc cập nhật cho lớp này là:

y_i^{l+1} = (1 + ω_{yy}^l λ^l)y_i^l.

Điều này tái chia tỷ lệ mọi y bằng một hệ số phụ thuộc vào λˡ. Khi ω_{yy}^l < 0, điều này thu nhỏ đầu ra dựa trên chuẩn của y trong lớp trước. Điều này hữu ích cho vấn đề phương sai nhiễu hỗn hợp, vì giải pháp hồi quy ridge chia tỷ lệ giải pháp bình phương tối thiểu bằng một hệ số phụ thuộc vào mức nhiễu.

Cụ thể, giả sử Σ ≈ E[Σ] = nI, giải pháp hồi quy ridge trở thành w*_{σ²} ≈ (n/(n+σ²))w*, đó chính xác là một phiên bản được chia tỷ lệ của giải pháp OLS. Hơn nữa, khi nhiễu lớn hơn, hệ số chia tỷ lệ nhỏ hơn, điều này phù hợp với hành vi của một ω_{yy} âm.

Chúng ta có thể chỉ ra rằng sử dụng chia tỷ lệ thích ứng ω_{yy} ngay cả một transformer tuyến tính 2 lớp có thể đủ để giải quyết một ví dụ đơn giản của vấn đề phương sai nhiễu hỗn hợp phân loại σ_τ ∈ {σ₁, σ₂} và n → ∞:

**Định lý 5.2.** Giả sử đầu vào cho transformer là (x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ), (x_q, 0), nơi xᵢ ~ N(0, (1/n)I), yᵢ = w^⊤xᵢ + ξᵢ. Ở đây ξᵢ ~ N(0, σ²) là nhiễu có mức nhiễu σ có thể nhận một trong hai giá trị: σ₁ hoặc σ₂. Thì khi n tiến tới +∞, tồn tại một tập tham số cho các transformer tuyến tính hai lớp sao cho w² ngầm của transformer tuyến tính hội tụ về kết quả hồi quy ridge tối ưu (và đầu ra của transformer tuyến tính là -⟨w², x_q⟩). Hơn nữa, lớp đầu tiên chỉ có ω_{yx} khác không và lớp thứ hai chỉ có ω_{yy} khác không.

### 5.3 Hiểu ω_{xy}: thích ứng kích thước bước

Số hạng cuối cùng trong mô hình đường chéo, ω_{xy}, có hiệu ứng phức tạp hơn. Vì nó chỉ thay đổi tọa độ x, nó không có hiệu ứng tức thì trên y. Để hiểu cách nó ảnh hưởng đến y, chúng ta xem xét một quá trình hai bước đơn giản hóa, nơi bước đầu tiên chỉ có ω_{xy} ≠ 0 và bước thứ hai chỉ có ω_{yx} ≠ 0 (vì vậy bước thứ hai chỉ làm một bước gradient descent). Trong trường hợp này, lớp đầu tiên sẽ cập nhật xᵢ như:

x_i^1 = x_i + y_i ω_{xy} ∑_{j=1}^n y_j x_j
      = x_i + ω_{xy} y_i ∑_{j=1}^n (⟨w*, x_j⟩ + r_j)x_j
      = x_i + ω_{xy} y_i Σw*
      = x_i + ω_{xy}(⟨w*, x_i⟩ + r_i)Σw*
      = (I + ω_{xy}Σw*(w*)^⊤)x_i + ω_{xy}r_i Σw*.

Có hai hiệu ứng của số hạng ω_{xy}, một là hiệu ứng nhân trên xᵢ, và cái khác là một số hạng cộng làm cho x-output liên quan đến phần dư rᵢ. Bước nhân trong xᵢ có hiệu ứng điều kiện tiên quyết chưa biết. Để đơn giản, chúng ta giả sử số hạng nhân nhỏ, đó là:

x_i^1 ≈ x_i + ω_{xy}r_i Σw*; x_t^1 ≈ x_t.

Lớp đầu tiên không thay đổi y, vì vậy y_t^1 = y_t và y_i^1 = y_i. Đối với tập xᵢ này, chúng ta có thể viết đầu ra trên y trong lớp thứ hai như

y_t^2 = y_t + ω_{yx} ∑_{i=1}^n y_i (x_i^1)^⊤ x_t
      ≈ y_t + ω_{yx}[∑_{i=1}^n y_i x_i + ω_{xy} ∑_{i=1}^n y_i r_i Σw*] x_t
      = y_t + ω_{yx}(1 + ω_{xy} ∑_{i=1}^n r_i²)(Σw*)^⊤ x_t.

Ở đây chúng ta đã sử dụng tính chất của phần dư rᵢ (đặc biệt là ∑ᵢyᵢxᵢ = Σw*, và ∑ᵢyᵢrᵢ = ∑ᵢrᵢ²).

Lưu ý rằng (Σw*)^⊤xₜ về cơ bản là những gì một bước gradient descent trên đầu vào gốc nên làm. Do đó, hiệu quả, mạng hai lớp đang làm gradient descent, nhưng kích thước bước là tích của -ω_{yx} và (1 + ω_{xy}∑ᵢrᵢ²). Hệ số (1 + ω_{xy}∑ᵢrᵢ²) phụ thuộc vào mức độ nhiễu, và khi ω_{xy}, ω_{yx} < 0, kích thước bước hiệu quả nhỏ hơn khi có nhiều nhiễu hơn. Điều này đặc biệt hữu ích trong vấn đề lựa chọn mô hình, vì theo trực giác người ta muốn thực hiện early-stopping (kích thước bước nhỏ) khi nhiễu cao.

## 6 Thí nghiệm

Trong phần này, chúng tôi điều tra động lực học huấn luyện của các transformer tuyến tính khi được huấn luyện với vấn đề phương sai nhiễu hỗn hợp (7). Chúng tôi đánh giá ba loại mô hình transformer tuyến tính một head:

• **FULL.** Huấn luyện các ma trận tham số đầy đủ.
• **DIAG.** Huấn luyện các ma trận tham số đường chéo (10).
• **GD++.** Một biến thể đường chéo thậm chí hạn chế hơn được định nghĩa trong (11).

Đối với mỗi thí nghiệm, chúng tôi huấn luyện mỗi sửa đổi transformer tuyến tính với số lớp khác nhau (1 đến 7) sử dụng optimizer Adam trong 200,000 vòng lặp với tỷ lệ học 0.0001 và kích thước batch 2,048. Trong một số trường hợp, đặc biệt đối với số lớp lớn, chúng tôi phải điều chỉnh tỷ lệ học để ngăn ngừa các vấn đề ổn định. Chúng tôi báo cáo kết quả tốt nhất từ 5 lần chạy với các seed huấn luyện khác nhau. Chúng tôi sử dụng N = 20 ví dụ trong ngữ cảnh trong D = 10 chiều. Chúng tôi đánh giá thuật toán sử dụng 100,000 chuỗi mới. Tất cả các thí nghiệm được thực hiện trên một GPU H100 với 80GB VRAM. Trung bình mất 4-12 giờ để huấn luyện một thuật toán đơn, tuy nhiên thử nghiệm với các tham số weight decay khác nhau, optimizer tốt hơn và lịch trình tỷ lệ học có thể sẽ giảm con số này đáng kể.

Chúng tôi sử dụng mất mát đánh giá điều chỉnh như số liệu hiệu suất chính. Nó được tính bằng cách trừ mất mát oracle khỏi mất mát của predictor. Mất mát oracle là giải pháp dạng đóng của mất mát hồi quy ridge (5), giả sử phương sai nhiễu στ được biết. Mất mát đánh giá điều chỉnh cho phép so sánh hiệu suất mô hình trực tiếp qua các phương sai nhiễu khác nhau. Điều này quan trọng vì nhiễu cao làm giảm đáng kể dự đoán mô hình. Điều chỉnh của chúng tôi không ảnh hưởng đến quá trình tối ưu hóa của mô hình, vì nó chỉ sửa đổi mất mát bằng một hằng số cộng.

**Ước lượng baseline.** Chúng tôi đánh giá transformer tuyến tính so với giải pháp dạng đóng cho bài toán hồi quy ridge (5). Chúng tôi ước lượng phương sai nhiễu στ sử dụng các phương pháp sau:

• **Hồi quy Ridge Hằng số (CONST RR).** Phương sai nhiễu được ước lượng sử dụng một giá trị scalar đơn cho tất cả các chuỗi, được điều chỉnh riêng cho mỗi vấn đề phương sai hỗn hợp.

• **Hồi quy Ridge Thích ứng (ADARR).** Ước lượng phương sai nhiễu qua ước lượng không thiên vị (Cherkassky & Ma, 2003) σ²_{est} = (1/(n-d))∑ⱼ₌₁ⁿ(yⱼ - ŷⱼ)², nơi ŷⱼ đại diện cho giải pháp của bình phương tối thiểu thông thường (4), được tìm thấy dưới dạng đóng.

• **Hồi quy Ridge Thích ứng Điều chỉnh (TUNED RR).** Giống như trên, nhưng sau khi nhiễu được ước lượng, chúng tôi điều chỉnh hai tham số bổ sung để tối thiểu hóa mất mát đánh giá: (1) một giá trị ngưỡng tối đa cho phương sai ước lượng, (2) một điều chỉnh nhân cho ước lượng nhiễu. Các giá trị này được điều chỉnh riêng cho mỗi vấn đề.

Lưu ý rằng tất cả các baseline trên đều dựa trên hồi quy ridge, đó là một giải pháp dạng đóng, không lặp. Do đó, chúng có lợi thế thuật toán so với các transformer tuyến tính không có quyền truy cập vào nghịch đảo ma trận. Các baseline này giúp chúng ta đo lường hiệu suất tốt nhất có thể, thiết lập một giới hạn trên thay vì một so sánh hoàn toàn tương đương.

Một so sánh trung thực hơn với phương pháp của chúng tôi sẽ là một phiên bản lặp của ADARR không sử dụng nghịch đảo ma trận. Thay vào đó, chúng ta có thể sử dụng gradient descent để ước lượng nhiễu và giải pháp cho hồi quy ridge. Tuy nhiên, trong thực tế, ước lượng gradient descent này hội tụ về ADARR chỉ sau ≈100 vòng lặp. Ngược lại, các transformer tuyến tính thường hội tụ trong ít hơn 10 lớp.

Chúng tôi xem xét hai lựa chọn cho phân phối của στ:

• **Đồng nhất.** στ ~ U(0, σ_{max}) được rút từ một phân phối đồng nhất bị chặn bởi σ_{max}. Chúng tôi đã thử nhiều kịch bản với σ_{max} từ 0 đến 7.

• **Phân loại.** στ ∈ S được chọn từ một tập rời rạc S. Chúng tôi đã thử nghiệm S = {1,3} và S = {1,3,5}.

Phương pháp của chúng tôi tổng quát hóa vấn đề được nghiên cứu bởi Bai et al. (2023), những người chỉ xem xét lựa chọn phương sai phân loại và chỉ ra thí nghiệm với hai giá trị στ.

**Phương sai nhiễu đồng nhất.** Đối với phương sai nhiễu đồng nhất, Hình 1 cho thấy FULL và DIAG đạt được hiệu suất tương đương qua các số lớp khác nhau và σ_{max} khác nhau. Mặt khác, GD++ hội tụ về một giá trị cao hơn, tiếp cận gần hiệu suất của baseline CONST RR.

Khi σ_{max} tăng, các transformer tuyến tính cho thấy lợi thế rõ ràng so với các baseline. Với 4 lớp, chúng vượt trội hơn giải pháp dạng đóng ADARR cho σ_{max} = 4 và lớn hơn. Các mô hình với 5 hoặc nhiều lớp hơn sánh ngang hoặc vượt trội hiệu suất của TUNED RR.

Phần trên của Hình 2 cung cấp góc nhìn chi tiết về hiệu suất của các mô hình 7 lớp và các baseline. Ở đây, chúng tôi tính toán các hồ sơ theo phương sai qua phạm vi phương sai nhiễu từ 0 đến σ_{max} + 1. Chúng ta có thể thấy rằng hiệu suất kém của GD++ đến từ khả năng ước lượng kém qua toàn bộ phạm vi phương sai nhiễu. Hiệu suất của nó phản ánh gần với CONST RR, đề xuất rằng GD++ ở bên dưới cũng có thể đang ước lượng một phương sai hằng số đơn lẻ cho tất cả dữ liệu.

ADARR ước lượng hoàn hảo các vấn đề không có nhiễu, nhưng khó khăn hơn khi phương sai nhiễu tăng. TUNED RR cải thiện ước lượng một chút bằng cách kết hợp σ_{max} vào các tham số có thể điều chỉnh của nó, tuy nhiên dự đoán của nó gặp khó khăn trong phạm vi giữa. FULL và DIAG chứng minh hiệu suất tương đương qua tất cả phương sai nhiễu. Mặc dù cần nhiều nghiên cứu hơn để xác nhận hoặc phủ nhận sự tương đương của chúng một cách dứt khoát, chúng tôi tin rằng các mô hình này thực sự không giống hệt nhau mặc dù hiệu suất tương tự.

Ở dưới cùng của Hình 2, chúng tôi đặt phương sai nhiễu thành σ_{max} = 5 và hiển thị hồ sơ theo phương sai cho các mô hình với các lớp khác nhau. Các mô hình hai lớp cho FULL và DIAG hoạt động giống như GD++, chỉ mô hình hóa một phương sai nhiễu đơn lẻ ở giữa. Tuy nhiên, kết quả nhanh chóng cải thiện qua toàn bộ phổ nhiễu cho 3 hoặc nhiều lớp hơn. Ngược lại, GD++ nhanh chóng hội tụ về một giải pháp không tối ưu.

**Phương sai nhiễu phân loại.** Hình 3 cho thấy một sự khác biệt đáng chú ý giữa các mô hình DIAG và FULL cho phương sai nhiễu phân loại στ ∈ {1,3}. Điều này có thể xuất phát từ một cực tiểu địa phương xấu, hoặc đề xuất một sự khác biệt cơ bản giữa các mô hình cho vấn đề này. Thú vị, từ hồ sơ theo phương sai chúng ta thấy rằng DIAG ngoại suy tốt hơn cho các phương sai không được sử dụng để huấn luyện, trong khi FULL, mặc dù lỗi trong phân phối thấp hơn, hoạt động kém hơn trên các phương sai chưa thấy. Hình 4 cho thấy các trọng số đã học của transformer tuyến tính 4 lớp với tham số hóa FULL. Các trọng số rất nặng về đường chéo, có thể với một số thành phần hạng thấp.

Đối với στ ∈ {1,3,5}, kiểm tra hồ sơ theo phương sai ở dưới cùng của Hình 3 tiết lộ sự khác biệt trong hành vi của chúng. FULL thể hiện hồ sơ theo phương sai phức tạp hơn với nhiều biến động hơn so với mô hình đường chéo, đề xuất khả năng biểu diễn lớn hơn. Đáng ngạc nhiên, nó không dịch thành kết quả mất mát tốt hơn so với DIAG.

Để so sánh dễ dàng, chúng tôi biên dịch kết quả của tất cả phương pháp và baseline trong Bảng 1 trong Phụ lục.

## 7 Kết luận

Nghiên cứu của chúng tôi tiết lộ khả năng đáng ngạc nhiên của các transformer tuyến tính để giải quyết các bài toán học trong ngữ cảnh thách thức. Chúng tôi cho thấy rằng mỗi lớp duy trì một mô hình hồi quy tuyến tính ngầm, giống như một biến thể phức tạp của gradient descent có điều kiện tiên quyết với hành vi giống momentum.

Đáng chú ý, khi được huấn luyện trên các bài toán hồi quy tuyến tính nhiễu với phương sai nhiễu chưa biết, các transformer tuyến tính không chỉ vượt trội hơn các baseline tiêu chuẩn mà còn khám phá ra một thuật toán tối ưu hóa tinh vi kết hợp điều chỉnh kích thước bước nhận biết nhiễu và tái chia tỷ lệ. Khám phá này làm nổi bật tiềm năng của các transformer tuyến tính để tự động khám phá các thuật toán tối ưu hóa mới khi được trình bày với các vấn đề phù hợp, mở ra những hướng nghiên cứu thú vị cho tương lai, bao gồm khám phá thuật toán tự động sử dụng transformer và tổng quát hóa cho các miền vấn đề khác.

Mặc dù các phát hiện của chúng tôi chứng minh khả năng ấn tượng của các transformer tuyến tính trong việc học các thuật toán tối ưu hóa, chúng tôi thừa nhận những hạn chế trong công trình của mình. Những hạn chế này bao gồm tập trung vào các mô hình tuyến tính đơn giản hóa, phân tích chủ yếu các ma trận attention đường chéo, và nhu cầu khám phá thêm về tính tối ưu của các thuật toán được khám phá, tổng quát hóa cho các lớp hàm phức tạp, khả năng mở rộng với các tập dữ liệu lớn hơn, và khả năng áp dụng cho các kiến trúc transformer phức tạp hơn. Chúng tôi tin rằng những hạn chế này đưa ra những hướng có giá trị cho nghiên cứu tương lai và nhấn mạnh nhu cầu hiểu biết sâu hơn về các cơ chế học ngầm trong các kiến trúc transformer.

## 8 Lời cảm ơn

Các tác giả muốn cảm ơn Nolan Miller và Andrey Zhmoginov vì những đề xuất và phản hồi có giá trị của họ trong suốt quá trình phát triển dự án này. Một phần công trình này được thực hiện khi Rong Ge đang thăm Google Research. Nghiên cứu của Rong Ge được hỗ trợ một phần bởi NSF Award DMS-2031849 và CCF-1845171 (CAREER).

## Tài liệu tham khảo

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023.

Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.

Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-Context language learning: Architectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878–18891, 2022.

Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528, 2023.

Vladimir Cherkassky and Yunqian Ma. Comparison of model selection for regression. Neural computation, 15(7):1691–1714, 2003.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023.

Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.

Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.

Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai. How do transformers learn in-context beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616, 2023.

Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.

Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249, 2023.

Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820, 2019.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 5156–5165. PMLR, 2020.

Jannik Kossen, Tom Rainforth, and Yarin Gal. In-context learning in large language models learns label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375, 2023.

Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In International Conference on Machine Learning, pp. 19565–19594. PMLR, 2023.

Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023.

Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

Reese Pathak, Rajat Sen, Weihao Kong, and Abhimanyu Das. Transformers can optimally learn regression mixture models. arXiv preprint arXiv:2311.08362, 2023.

Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355–9366. PMLR, 2021.

Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn in-context by gradient descent? arXiv preprint arXiv:2310.08540, 2023.

Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023a.

Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023b.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151–35174. PMLR, 2023a.

Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023b.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.

Kaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski. Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Steve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni. Pretraining data mixtures enable narrow model selection capabilities in transformer models. arXiv preprint arXiv:2311.00871, 2023.

Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.

## A Các chứng minh từ Phần 4 và 5

### A.1 Chứng minh Định lý 4.1

Chúng tôi đầu tiên đưa ra chứng minh cho Định lý 4.1. Trong quá trình này, chúng tôi cũng sẽ chứng minh Bổ đề 4.3, vì Định lý 4.1 theo ngay lập tức từ một quy nạp dựa trên bổ đề.

**Chứng minh.** Chúng ta làm điều này bằng quy nạp. Tại l = 0, dễ dàng kiểm tra rằng chúng ta có thể đặt a^(0) = 1, w^(0) = 0, M^(0) = I, u^(0) = 0.

Giả sử điều này đúng cho một lớp l nào đó, nếu các trọng số của lớp l là (P₁ˡ, Q₁ˡ), ..., (Pₖˡ, Qₖˡ) cho k head, tại đầu ra của lớp l + 1 chúng ta có:

[x_i^{l+1}] = [x_i^l] + ∑_{k=1}^p P_k^l ∑_{j=1}^n [x_j^l] [(x_j^l)^⊤, y_j^l] Q_k^l [x_i^l]
[y_i^{l+1}]   [y_i^l]                       [y_j^l]                        [y_i^l]    (12)

Lưu ý rằng cùng một phương trình đúng cho i = n + 1 chỉ bằng cách đặt y_{n+1} = 0. Đặt ma trận giữa có cấu trúc sau:

[A  b ]
[c^⊤ d] := ∑_{k=1}^p P_k^l ∑_{j=1}^n [x_j^l] [(x_j^l)^⊤, y_j^l] Q_k^l,
                                      [y_j^l]

Thì có thể chọn các tham số của lớp tiếp theo như trong Bổ đề 4.3

M^{l+1} = (I + A)M^l + b(w^l)^⊤
u^{l+1} = (I + A)u^l + a^l b
a^{l+1} = (1 + d)a^l + ⟨c, u^l⟩
w^{l+1} = (1 + d)w^l - (M^l)^⊤c.

Có thể kiểm tra rằng lựa chọn này thỏa mãn (12).

### A.2 Chứng minh Bổ đề 4.4

Bổ đề này thực tế là một hệ quả của Bổ đề 4.3. Chúng tôi đầu tiên đưa ra một phiên bản chi tiết hơn mà tường minh nêu các ma trận chưa biết Λˡ, Γˡ, Πˡ, Φˡ:

**Bổ đề A.1.** Trong thiết lập của Định lý 4.1 với các tham số đường chéo (9), có thể tính toán đệ quy các ma trận uˡ, wˡ sử dụng công thức sau

u^{l+1} = [(1 + ω_{xy}^l(a^l)²ρ)I + ω_{xx}^l Σ^l]u^l + a^l ω_{xy}^l [M^l + a^l u^l(w^*)^⊤]/Σ (a^l w^* - w^l),

w^{l+1} = (1 + ω_{yy}^l λ^l)w^l - ω_{yx}^l(M^l)^⊤(M^l + a^l u^l(w^*)^⊤)Σ(a^l w^* - w^l) - a^l ρ ω_{yx}^l(M^l)^⊤u^l,

nơi ρ = ∑ᵢ₌₁ⁿ rᵢ² và các điều kiện khởi tạo a⁰ = 1, w⁰ = 0, M⁰ = I, u⁰ = 0

**Chứng minh.** Đầu tiên, chúng ta tính toán ma trận sau đã xuất hiện trong Bổ đề 4.3 cho trường hợp đường chéo cụ thể:

[A^l  b^l ] = ∑_{k=1}^p P_k^l ∑_{j=1}^n [x_j^l] [(x_j^l)^⊤, y_j^l] Q_k^l,
[(c^l)^⊤ d^l]                         [y_j^l]

= [ω_{xx}^l Σ^l  ω_{xy}^l α^l]
  [ω_{yx}^l(α^l)^⊤ ω_{yy}^l λ^l]

Điều này ngụ ý rằng A^l = ω_{xx}^l Σ^l, b^l = ω_{xy}^l α^l, c^l = ω_{yx}^l α^l và d^l = ω_{yy}^l λ^l. Tiếp theo chúng ta viết lại α^l:

α^l = ∑ᵢ₌₁ⁿ y_i^l x_i^l
    = ∑ᵢ₌₁ⁿ (a^l y_i - ⟨w^l, x_i⟩)(M^l x_i + y_i u^l)
    = ∑ᵢ₌₁ⁿ (a^l r_i + ⟨a^l w^* - w^l, x_i⟩)((M^l + a^l u^l(w^*)^⊤)x_i + r_i u^l)
    = ∑ᵢ₌₁ⁿ ⟨a^l w^* - w^l, x_i⟩(M^l + a^l u^l(w^*)^⊤)x_i + ∑ᵢ₌₁ⁿ a^l r_i² u^l
    = (M^l + a^l u^l(w^*)^⊤)Σ(a^l w^* - w^l) + a^l ρ u^l.

Ở đây bước đầu tiên là bởi Định lý 4.1, bước thứ hai thay thế y_i với ⟨w^*, x_i⟩ + r_i, bước thứ ba sử dụng thực tế rằng ∑ᵢ₌₁ⁿ r_i x_i = 0 để loại bỏ các số hạng chéo.

Chứng minh còn lại chỉ thay thế công thức cho α^l vào Bổ đề 4.3.

Bây giờ Bổ đề A.1 ngụ ý Bổ đề 4.4 ngay lập tức bằng cách đặt Λ^l = -ω_{xy}^l(a^l)²ρI - ω_{xx}^l Σ^l, Γ^l = a^l ω_{xy}^l [M^l + a^l u^l(w^*)^⊤], s^l = ω_{yy}^l λ^l, Π^l = ω_{yx}^l(M^l)^⊤(M^l + a^l u^l(w^*)^⊤) và Φ^l = a^l ρ ω_{yx}^l(M^l)^⊤.

### A.3 Chứng minh cho Định lý 4.2

**Chứng minh.** Bởi Định lý 4.1, chúng ta biết y_i^l = ⟨w^l, x_i⟩ cho một w^l nào đó. Khi n ≫ d, với xác suất cao chuẩn của y^l có bậc Θ(√n)‖w^l‖, và chuẩn của y^* là Θ(√n). Do đó chúng ta chỉ cần chặn tương quan. Tương quan bằng

|⟨y^*, y^l⟩| = |w_1^l ∑ᵢ₌₁ⁿ x_i³(1) + ∑ᵢ₌₁ⁿ x_i(1)² ∑ⱼ₌₂ᵈ w_j^l x_i(j)|.

Chúng ta biết với xác suất cao |∑ᵢ₌₁ⁿ x_i³| = O(√n) vì E[x_i³] = 0. Số hạng thứ hai có thể được viết như ⟨w^l, v⟩ nơi v là một vector có tọa độ v₁ = 0 và vⱼ = ∑ᵢ₌₁ⁿ x_i(1)² x_i(j) cho 2 ≤ j ≤ d, do đó với xác suất cao ‖v‖ = O(√nd). Do đó, với xác suất cao độ tương tự cosine không quá

|⟨y^*, y^l⟩|/(‖y^*‖‖y^l‖) = O(1)|⟨y^*, y^l⟩|/(n‖w^l‖) = O(1)|w_1^l ∑ᵢ₌₁ⁿ x_i³(1) + ∑ᵢ₌₁ⁿ x_i(1)² ∑ⱼ₌₂ᵈ w_j^l x_i(j)|/(n‖w^l‖)

≤ O(1)(|w_1^l||∑ᵢ₌₁ⁿ x_i³(1)| + |w^l|‖v‖)/(n‖w^l‖) ≤ O(1)(√n + √nd)/n.

Khi n ≫ d điều này có thể được làm nhỏ hơn bất kỳ hằng số cố định nào.

### A.4 Chứng minh cho Định lý 5.1

Trong phần này chúng tôi chứng minh Định lý 5.1 bằng cách tìm các siêu tham số cho thuật toán GD++ giải quyết các bài toán bình phương tối thiểu với độ chính xác rất cao. Các bước đầu tiên trong việc xây dựng lặp đi lặp lại làm cho dữ liệu x_i có điều kiện tốt hơn, và bước cuối cùng là một bước đơn của gradient descent. Chứng minh dựa trên một số bổ đề, đầu tiên chúng ta quan sát rằng nếu dữ liệu có điều kiện rất tốt, thì gradient descent một bước giải quyết bài toán một cách chính xác:

**Bổ đề A.2.** Cho (x₁, y₁), ..., (xₙ, yₙ) nơi Σ := ∑ᵢ₌₁ⁿ xᵢxᵢ^⊤ có các eigenvalue giữa 1 và 1 + ε. Cho w^* := arg min_w ∑ᵢ₌₁ⁿ(yᵢ - ⟨w, xᵢ⟩)² là giải pháp bình phương tối thiểu tối ưu, thì ŵ = ∑ᵢ₌₁ⁿ yᵢxᵢ thỏa mãn ‖ŵ - w^*‖ ≤ ε‖w^*‖.

**Chứng minh.** Chúng ta có thể viết yᵢ = ⟨xᵢ, w^*⟩ + rᵢ. Bởi thực tế rằng w^* là giải pháp tối ưu chúng ta biết rᵢ thỏa mãn ∑ᵢ₌₁ⁿ rᵢxᵢ = 0. Do đó ŵ = ∑ᵢ₌₁ⁿ yᵢxᵢ = ∑ᵢ₌₁ⁿ ⟨xᵢ, w^*⟩xᵢ = Σw^*. Điều này ngụ ý

‖ŵ - w^*‖ = ‖(Σ - I)w^*‖ ≤ ‖Σ - I‖‖w^*‖ ≤ ε‖w^*‖.

Tiếp theo chúng ta chỉ ra rằng bằng cách chỉ áp dụng bước điều kiện tiên quyết của GD++, có thể nhận được một ma trận x có điều kiện tốt rất nhanh. Lưu ý rằng ma trận Σ được cập nhật như Σ ← (I - γΣ)Σ(I - γΣ), vì vậy một eigenvalue λ trong ma trận Σ gốc sẽ trở thành λ(1 - γλ)². Bổ đề sau cho thấy rằng phép biến đổi này hiệu quả trong việc thu nhỏ số điều kiện

**Bổ đề A.3.** Giả sử ν/μ = κ ≥ 1.1, thì tồn tại một hằng số phổ quát c < 1 sao cho chọn γν = 1/3 ngụ ý

max_{λ∈[ν,μ]} λ(1-γλ)² / min_{λ∈[ν,μ]} λ(1-γλ)² ≤ cκ.

Mặt khác, nếu ν/μ = κ ≤ 1 + ε nơi ε ≤ 0.1, thì chọn γν = 1/3 ngụ ý

max_{λ∈[ν,μ]} λ(1-γλ)² / min_{λ∈[ν,μ]} λ(1-γλ)² ≤ 1 + 2ε².

Khẳng định đầu tiên cho thấy rằng có thể giảm số điều kiện bằng một hệ số hằng số trong mỗi bước cho đến khi nó là một hằng số nhỏ. Khẳng định thứ hai cho thấy rằng một khi số điều kiện nhỏ (1 + ε), mỗi vòng lặp có thể đưa nó gần hơn nhiều đến 1 (đến bậc 1 + O(ε²)).

Bây giờ chúng ta chứng minh bổ đề.

**Chứng minh.** Đầu tiên, lưu ý rằng hàm f(x) = x(1-γx)² đơn điệu không giảm cho x ∈ [0, ν] nếu γν = 1/3 (thực tế, đạo hàm f'(x) = (1-γx)(1-3γx) luôn không âm). Do đó, max luôn đạt được tại x = ν và min luôn đạt được tại x = μ. Tỷ lệ mới do đó là

ν(1-γν)² / μ(1-γμ)² = κ × 4/9 / (1-1/3κ)².

Khi κ ≥ 1.1 tỷ lệ 4/9 / (1-1/3κ)² luôn dưới 4/9 / (1-1/3.3)² đó là một hằng số bị chặn xa khỏi 1.

Khi κ = 1 + ε < 1.1, chúng ta có thể viết VT theo ε

ν(1-γν)² / μ(1-γμ)² = κ × 4/9 / (1-1/3κ)² = (1 + ε)(1 + 1/2(1-1/(1+ε)))^{-2}.

Lưu ý rằng bằng lựa chọn cẩn thận của γ, VT có khai triển Taylor sau:

(1 + ε)(1 + 1/2(1-1/(1+ε)))^{-2} = 1 + 3ε²/4 - 5ε³/4 + O(ε⁴).

Sau đó có thể kiểm tra VT luôn bị chặn trên bởi 2ε² khi ε < 0.1.

Với hai bổ đề chúng ta bây giờ sẵn sàng chứng minh định lý chính:

**Chứng minh.** Bởi Bổ đề A.3 chúng ta biết trong O(log κ + log log 1/ε) vòng lặp, bằng cách gán κ theo cách của Bổ đề A.3 có thể giảm số điều kiện của x xuống κ' ≤ 1 + ε/2κ (chúng ta chọn ε/2κ ở đây để có một chút dư địa cho phân tích sau).

Cho Σ' là ma trận hiệp phương sai sau các vòng lặp này, và ν', μ' là chặn trên và dưới cho các eigenvalue của nó. Dữ liệu xᵢ được biến đổi thành dữ liệu mới x'ᵢ = Mxᵢ cho một ma trận M nào đó. Cho M = AΣ^{-1/2}, thì vì M' = AA^⊤ chúng ta biết A là một ma trận với các singular value giữa √μ' và √ν'. Giải pháp tối ưu (w^*)' = M^{-⊤}w^* có chuẩn không quá √ν/√μ'‖w^*‖.

Do đó bởi Bổ đề A.2 chúng ta biết bước gradient một bước với ŵ = ∑ᵢ₌₁ⁿ (1/μ')yᵢxᵢ thỏa mãn ‖ŵ - (w^*)'‖ ≤ (κ' - 1)√ν/√μ'‖w^*‖. Dữ liệu thử nghiệm xₜ cũng được biến đổi thành x'ₜ = AΣ^{-1/2}xₜ, và thuật toán xuất ra ⟨ŵ, x'ₜ⟩, vì vậy lỗi không quá √ν‖w^*‖‖x'ₜ‖ ≤ (κ' - 1)√κ√κ'‖w^*‖‖xₜ‖. Bằng lựa chọn κ' chúng ta có thể kiểm tra rằng VT không quá ε‖w^*‖‖xₜ‖.

### A.5 Chứng minh Định lý 5.2

**Chứng minh.** Quan sát chính ở đây là khi n → ∞, dưới các giả định chúng ta có lim_{n→∞} ∑ᵢ₌₁ⁿ xᵢxᵢ^⊤ = I. Do đó các giải pháp hồi quy ridge hội tụ về w^*_{σ²} = 1/(1+σ²) ∑ᵢ₌₁ⁿ yᵢxᵢ và đầu ra mong muốn là ⟨w^*_{σ²}, x_q⟩.

Bằng các tính toán trước đây, chúng ta biết sau lớp đầu tiên, w ngầm là w¹ = ω_{yx} ∑ᵢ₌₁ⁿ yᵢxᵢ. Miễn là ω_{yx} là một hằng số, khi n → ∞ chúng ta biết 1/n ∑ᵢ₌₁ⁿ (y¹ᵢ)² = σ² (vì phần của yₜ phụ thuộc vào x có thể bỏ qua so với nhiễu), do đó đầu ra của lớp thứ hai thỏa mãn

w² = (1 + nσ²ω_{yy})w¹ = (1 + nσ²ω_{yy})ω_{yx} ∑ᵢ₌₁ⁿ yᵢxᵢ.

Do đó, miễn là chúng ta chọn ω_{yx} và ω_{yy} để thỏa mãn (1 + nσ²ω_{yy})ω_{yx} = 1/(1+σ²) khi σ = σ₁ hoặc σ₂ (lưu ý rằng đây là hai phương trình tuyến tính trên ω_{yx} và nω_{yx}ω_{yy}, vì vậy chúng luôn có giải pháp), thì chúng ta có lim_{n→∞} w² = w^*_{σ²} cho hai mức nhiễu.

## B Thêm thí nghiệm

Ở đây chúng tôi cung cấp kết quả của các thí nghiệm bổ sung không được đưa vào văn bản chính.

Hình 6 cho thấy một ví dụ về mất mát chưa điều chỉnh. Rõ ràng, hầu như không thể so sánh các phương pháp qua các mức nhiễu khác nhau theo cách này.

Hình 7 cho thấy hồ sơ theo phương sai của các dự đoán trung gian của mạng có độ sâu khác nhau. Có vẻ như GD++ thể hiện hành vi điển hình của các thuật toán dựa trên GD: các vòng lặp đầu mô hình nhiễu cao hơn (tương tự như early stopping), dần dần hội tụ về các dự đoán nhiễu thấp hơn. DIAG thể hiện mẫu này ban đầu, nhưng sau đó cải thiện đáng kể, đặc biệt đối với các phạm vi nhiễu thấp hơn.

Thú vị, FULL hiển thị xu hướng ngược lại, đầu tiên cải thiện các dự đoán nhiễu thấp, theo sau là sự suy giảm trong độ chính xác dự đoán nhiễu cao hơn, đặc biệt là trong lớp cuối cùng.

Cuối cùng, Bảng 1 trình bày kết quả số toàn diện cho các thí nghiệm của chúng tôi qua các mô hình phương sai nhiễu hỗn hợp khác nhau. Đối với mỗi biến thể mô hình (được đại diện bởi một cột), kết quả hiệu suất tốt nhất được tô đậm.

[Các bảng và hình ảnh tiếp tục với cấu trúc tương tự như trong bản gốc]
