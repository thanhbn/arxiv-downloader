# 2402.10644.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2402.10644.pdf
# Kích thước tệp: 1023852 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Linear Transformers với Hàm Kernel Có Thể Học Là Các Mô Hình In-Context Tốt Hơn
Yaroslav Aksenov♣q, Nikita Balagansky♣r, Sofia Maria Lo Cicero Vaina♠∗,
Boris Shaposhnikov♣, Alexey Gorbatovski♣, Daniil Gavrilov♣
♣TinkoffqTrường Đại học Kinh tế Cao cấp
rViện Vật lý và Công nghệ Moscow♠Đại học Innopolis
n.n.balaganskiy@tinkoff.ru
Tóm tắt
Việc thúc đẩy ranh giới của các kiến trúc subquadratic cho Mô hình Ngôn ngữ (LMs) là rất quan trọng trong lĩnh vực xử lý ngôn ngữ tự nhiên đang phát triển nhanh chóng. Các đổi mới hiện tại, bao gồm State Space Models, ban đầu được ca ngợi vì vượt qua hiệu suất Transformer trong các tác vụ mô hình hóa ngôn ngữ. Tuy nhiên, các mô hình này đã tiết lộ những thiếu sót trong khả năng In-Context Learning thiết yếu – một lĩnh vực mà Transformer truyền thống tỏa sáng. Mô hình Based xuất hiện như một giải pháp hybrid, kết hợp một Linear Transformer với một kernel được lấy cảm hứng từ khai triển Taylor của hàm số mũ, được bổ sung bởi các mạng tích chập. Phản ánh khả năng in-context thành thạo của Transformer, nó trở thành một ứng cử viên mạnh mẽ trong lĩnh vực này. Trong công trình của chúng tôi, chúng tôi trình bày một thay đổi duy nhất, thanh lịch cho kernel Based mà khuếch đại khả năng In-Context Learning của nó được đánh giá bằng tác vụ Multi-Query Associative Recall và quá trình mô hình hóa ngôn ngữ tổng thể, như được chứng minh trên tập dữ liệu Pile.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) đang cách mạng hóa lĩnh vực xử lý ngôn ngữ tự nhiên và thiết lập các tiêu chuẩn mới trên các tác vụ khác nhau (Touvron et al., 2023; Jiang et al., 2023). Tuy nhiên, bất chấp những thành công của chúng, hầu hết các mô hình này được xây dựng trên framework Transformer sử dụng cơ chế attention. Những cơ chế này mở rộng kém với các chuỗi văn bản dài, dẫn đến độ phức tạp tính toán không thực tế để mở rộng xử lý ngữ cảnh (Vaswani et al., 2017; Tay et al., 2021).

Để giải quyết hạn chế này, một số phương án thay thế cho Transformers đã được đề xuất. Katharopoulos et al. (2020) đề xuất thay thế hàm số mũ trong cơ chế attention bằng hàm kernel để thay đổi thứ tự tính toán và do đó thoát khỏi độ phức tạp bậc hai của độ dài chuỗi. Tuy nhiên, khi so sánh với Transformers vanilla, cách tiếp cận này dẫn đến giảm hiệu suất. Hơn nữa, việc lựa chọn hàm kernel vẫn là một chủ đề cần được xem xét.

Một cách khác để định nghĩa mô hình tuyến tính là sử dụng State Space Models (SSMs) (Gu et al., 2022; Smith et al., 2023; Gu and Dao, 2023), có khả năng tạo ra chất lượng có thể so sánh với Transformers khi được đo bằng perplexity trên mô hình hóa ngôn ngữ.

Đáng chú ý, cả Linear Transformers Katharopoulos et al. (2020) và SSMs đều có thể được mô tả là Recurrent Neural Networks (RNNs) (Chung et al., 2014; Hochreiter and Schmidhuber, 1997), có những hạn chế của chúng khi quản lý các phụ thuộc dài trong văn bản vì khả năng bộ nhớ có thể bị quá tải khi khối lượng thông tin tăng lên. Ngoài ra, trong khi trạng thái ẩn của RNNs lớn hơn đối với Linear Transformers so với SSMs, cái sau đã cho thấy chất lượng mô hình hóa văn bản cao hơn. Việc giới thiệu mô hình Based (Arora et al., 2024) đã cố gắng giải quyết các thách thức được đề cập ở trên bằng cách sử dụng kiến trúc hybrid (Fu et al., 2023a) dựa trên Linear Transformer với hàm kernel mới được bắt nguồn từ khai triển Taylor của hàm số mũ. Arora et al. (2024) đã chứng minh rằng mô hình Based ít dễ bị các vấn đề hiệu suất khi làm việc với nội dung dài hơn so với các mô hình khác khi được đánh giá trên tác vụ Multi-Query Associative Recall (MQAR).

Tuy nhiên, ngay cả mô hình Based cũng gặp phải sự giảm hiệu suất khi đối mặt với ngữ cảnh mở rộng so với kiến trúc transformer thông thường.

Một sự hiểu biết sâu sắc về các quá trình xảy ra trong kiến trúc Based là thiết yếu cho sự phát triển của chúng. Khi kiểm tra cách phân phối điểm attention, chúng tôi cho rằng hàm kernel được áp dụng trước đây trong Based không thể được coi là tối ưu, dẫn đến những hạn chế khi xử lý ngữ cảnh dài và khả năng mô hình nhỏ. Để giải quyết vấn đề này, chúng tôi giới thiệu ReBased (Revisited Based), một biến thể mới của mô hình Linear Transformer cải thiện việc sử dụng attention kernels. Điểm mấu chốt của sự phát triển của chúng tôi nằm ở việc giải quyết việc Based không thể bỏ qua các token cụ thể với xác suất không trong quá trình attention. Bằng cách tinh chỉnh hàm kernel và kết hợp các sửa đổi kiến trúc mới, chúng tôi đã tạo ra một mô hình cải thiện độ chính xác trên các tác vụ liên quan đến việc truy xuất thông tin từ chuỗi token dài trong khi đơn giản hóa việc tính toán cơ chế attention.

Khi thử nghiệm kiến trúc cải tiến của chúng tôi trên tác vụ MQAR, chúng tôi thấy rằng ReBased vượt qua mô hình Based gốc trên nhiều ngữ cảnh và kích thước mô hình khác nhau. Ngoài ra, sau khi đào tạo với tập dữ liệu Pile (Gao et al., 2020), chúng tôi quan sát thấy rằng ReBased hoạt động tốt hơn so với người tiền nhiệm trong In-Context Learning và xuất sắc trong việc mô hình hóa các phụ thuộc kết hợp được đo thông qua các chỉ số perplexity được cải thiện.

2 Công trình Gần đây
Kiến trúc Vanilla Transformer (Vaswani et al., 2017), mặc dù được sử dụng rộng rãi trong NLP (Radford et al., 2019; Touvron et al., 2023; Devlin et al., 2019; Jiang et al., 2023), chịu đựng từ nhu cầu tính toán và bộ nhớ ngày càng tăng (O(d*N²) khi độ dài chuỗi (N) và kích thước head (d) tăng). Trong khi điều này không phải là vấn đề lớn khi xử lý các chuỗi ngắn hơn, nó trở thành một nút thắt đáng kể khi làm việc với các chuỗi dài hơn.

Một số kiến trúc thay thế đã được đề xuất để giải quyết vấn đề này. Katharopoulos et al. (2020) đề xuất thay thế hàm số mũ của cơ chế attention, được thiết kế để đo sự tương tự giữa queries và keys, bằng tích của các hàm kernel có thể được đánh giá riêng biệt cho queries và keys. Cách tiếp cận dựa trên kernel này định hình lại tính toán trong cơ chế attention, cắt giảm thời gian và độ phức tạp bộ nhớ xuống O(d²*N). Ngoài ra, trong quá trình suy luận, nó hỗ trợ lấy mẫu chuỗi với độ phức tạp tuyến tính theo độ dài, tương tự như RNNs (Hochreiter and Schmidhuber, 1997; Chung et al., 2014).

Trong một cách tiếp cận khác, State Space Models (SSMs) mượn từ lý thuyết điều khiển để cung cấp một cấu trúc đơn giản tương tự như RNNs, nhưng không có hàm kích hoạt theo thời gian (Gu et al., 2022; Smith et al., 2023; Gu et al., 2023). Mô hình Mamba, còn được biết đến là S6 (Gu and Dao, 2023), nổi bật trong danh mục này, hiển thị việc học tăng cường các phụ thuộc ngắn hạn trong văn bản so với các LLMs được đào tạo trước hiện có (Jiang et al., 2023; Touvron et al., 2023).

Bất chấp những tiến bộ này, không có cách tiêu chuẩn để đánh giá đầy đủ những kiến trúc đổi mới này để đánh giá giới hạn hiệu suất của chúng. Một phương pháp đánh giá tiêu chuẩn là đào tạo trước một mô hình ngôn ngữ và đánh giá perplexity của nó với một tập dữ liệu nhất định, nhưng điều này có thể không thực sự phản ánh khả năng của mô hình trong việc quản lý các phụ thuộc ngữ cảnh dài. Một lựa chọn khác là sử dụng benchmark Long Range Arena (LRA), liên quan đến các tác vụ phân loại với chuỗi đầu vào dài. Mặc dù một số mô hình mới đã vượt qua Transformers trong LRA, người ta tin rằng benchmark có khả năng đưa ra bias trong việc so sánh (Amos et al., 2023).

Một cách tiếp cận đánh giá đầy hứa hẹn là kiểm tra khả năng In-Context Learning của một kiến trúc. Olsson et al. (2022) giới thiệu khái niệm Associative Recall (AR), một tác vụ mà mô hình học cách sao chép một token từ một chuỗi sau một điểm nhất định. Tuy nhiên, trong khi trong Fu et al. (2023a) tác vụ associative recall được thực hiện với mục tiêu chỉ truy xuất một token, Arora et al. (2023) lưu ý rằng tác vụ này có thể được coi là quá đơn giản. Điều này dẫn đến việc tạo ra tác vụ Multi-Query Associative Recall (MQAR), yêu cầu truy xuất nhiều token từ ngữ cảnh. Các phát hiện về MQAR cho thấy rằng trong khi các mô hình mới hơn có thể cạnh tranh với Transformer về perplexity, chúng vẫn có thể gặp khó khăn với ngữ cảnh dài ở kích thước mô hình nhỏ do khả năng In-Context Learning hạn chế của chúng. Trong khi đó, Transformers vẫn mạnh mẽ chống lại những yếu tố như vậy. Cuối cùng, Arora et al. (2024) giới thiệu Linear Transformer với hàm kernel mới (cụ thể là Based), thể hiện hiệu suất tăng cường trên tác vụ MQAR khi so sánh với Mamba.

Bất chấp sự cải thiện này, so với Transformers truyền thống, vấn đề giảm hiệu suất khi xử lý chuỗi dài với các mô hình nhỏ hơn vẫn còn tồn tại. Giải quyết thách thức này là mục tiêu chính của bài báo của chúng tôi.

3 Nền tảng
3.1 Linear Transformers
Để nắm bắt đầy đủ kiến trúc Based, điều quan trọng là phải thảo luận về mô hình Transformer gốc trước. Cơ chế attention, là trung tâm chức năng của Transformer, đánh giá đầu ra y_i cho mỗi vị trí i như sau

y_i = Σ_{j=0}^i sim(Q_i, K_j)V_j / Σ_{n=0}^i sim(Q_i, K_n),

trong đó thuật ngữ sim(Q_i, K_j) = exp(Q_i^T K_j / √d) đại diện cho sự tương tự giữa query Q_i và key K_j sử dụng hàm số mũ. Bất chấp hiệu quả của nó, sự phụ thuộc của Transformer gốc vào cơ chế attention này phát sinh sự tăng bậc hai trong cả thời gian tính toán và sử dụng bộ nhớ khi độ dài chuỗi tăng, điều này trở nên không thực tế cho việc xử lý chuỗi dài.

Để giải quyết vấn đề khả năng mở rộng này, Katharopoulos et al. (2020) đề xuất thay thế việc tính toán trực tiếp sự tương tự giữa Q và K bằng một biến đổi thông qua hàm kernel phi tuyến φ(·). Điều này cho phép xấp xỉ sau: sim(Q_i, K_j) ≈ φ^T(Q_i)φ(K_j). Bằng cách thực hiện kernel này, Linear Transformer tính toán y_i như

y_i = Σ_{j=0}^i φ^T(Q_i)φ(K_j)V_j / Σ_{n=0}^i φ(Q_i)φ^T(K_n).

Bằng cách sắp xếp lại các phép toán, chúng ta có thể biểu diễn tính toán như y_i = φ^T(Q_i)Σ_{j=0}^i φ(K_j)V_j^T / φ^T(Q_i)Σ_{n=0}^i φ(K_n).

Bằng cách tính toán φ(K_j)V_j^T ∈ R^{d×d} trước, độ phức tạp của cơ chế attention chuyển sang tuyến tính với độ dài chuỗi, giải quyết các thiếu hiệu quả của mô hình gốc.

3.2 Based
Việc lựa chọn hàm kernel φ(·) phù hợp là quan trọng đối với hiệu suất của Linear Transformer. Các hàm kernel khác nhau đã được đề xuất (Peng et al., 2021; Schlag et al., 2021; Qin et al., 2022), nhưng trên các tác vụ mô hình hóa ngôn ngữ, không hàm nào vượt qua cơ chế attention gốc. Tuy nhiên, một bước đột phá đã được đạt được bởi Arora et al. (2024), những người đã giới thiệu một hàm kernel mới được lấy cảm hứng từ khai triển chuỗi Taylor của hàm số mũ, được định nghĩa là

sim(q,k) = 1 + q^T k + (q^T k)^2 / 2.

Việc lựa chọn kernel này được thúc đẩy bởi khả năng xấp xỉ hàm số mũ trên một phạm vi cụ thể của các giá trị q^T k. Ngoài ra, Arora et al. (2024) đã sử dụng kiến trúc hybrid bằng cách kết hợp attention tuyến tính với các lớp tích chập vì việc làm như vậy đã được chứng minh là giúp các mô hình xử lý các phụ thuộc non-associative ngắn hạn trong chuỗi (Fu et al., 2023a; Poli et al., 2023; Fu et al., 2023b)

Khi thực hiện như vậy, khi được đánh giá trên tác vụ MQAR, mô hình Based đã chứng minh rằng nó có khả năng vượt qua mô hình Mamba (Gu and Dao, 2023) trong các trường hợp có độ dài ngữ cảnh đáng kể và khả năng mô hình bị hạn chế do kích thước nhỏ hơn. Tuy nhiên, so với Transformer gốc, một sự suy giảm hiệu suất có thể nhận thấy vẫn còn tồn tại, cho thấy còn chỗ để cải thiện thêm.

4 Xem xét lại Based
Trong nghiên cứu của chúng tôi, chúng tôi khám phá các yêu cầu cơ bản cho hàm kernel. Chúng tôi kiểm tra hàm số mũ và biểu diễn xấp xỉ của nó, như được mô tả trong Hình 2. Chúng tôi quan sát thấy một hạn chế trong việc xấp xỉ vì giá trị tối thiểu của nó được cố định ở 0.5. Điều này có vấn đề đối với việc xử lý chuỗi dài, vì khó gán điểm attention gần không cho các cặp token cụ thể. Lý tưởng, chúng ta muốn có thể giảm điểm attention xuống không, điều này sẽ yêu cầu các giá trị lớn hơn đáng kể ở nơi khác trong quá trình chuẩn hóa với mô hình Based.

Để khắc phục vấn đề này, một cách tiếp cận đơn giản sẽ là điều chỉnh điểm thấp nhất của hàm kernel về không. Tuy nhiên, giải pháp này khiến chúng ta đặt câu hỏi tại sao giá trị tối thiểu của hàm kernel lại nên xảy ra chính xác tại q^T k = -1. Như được sử dụng trong Transformer gốc, hàm tương tự số mũ truyền thống tăng một cách đơn điệu, nhưng kernel bậc hai có một giá trị tối ưu mà nó giảm xuống và sau đó tăng lên từ đó. Do đó, để giảm attention trong Transformer, người ta sẽ nhắm đến việc tối thiểu hóa q^T k. Ngược lại, q^T k lý tưởng nên chính xác là -1 cho phương pháp Based. Nếu không, điểm attention sẽ tăng. Điều kiện này có thể gây ra kết quả đào tạo kém lý tưởng và làm giảm độ chính xác của mô hình.

Những thách thức này khiến chúng tôi đoán rằng nếu kernel bậc hai được sử dụng để tính toán sự tương tự giữa q và k, chúng ta phải xem xét phạm vi của các giá trị q^T k tiềm năng và tạo ra các tham số có thể điều chỉnh cho hàm parabolic để căn chỉnh với những giá trị này trong quá trình đào tạo. Đơn giản hóa để rõ ràng, hãy xem xét một kịch bản một chiều. Chúng ta có thể biểu diễn các tham số có thể đào tạo của hàm kernel liên quan đến biến đổi affine của q và k như sau

q' = γ_Q · q + β_Q, k' = γ_K · k + β_K;
sim(q', k') = φ^T(q')φ(k').

Ở đây, φ(·) đại diện cho một hàm bậc hai. Mô hình có thể học bất kỳ hàm bậc hai nào với giá trị tối thiểu được xác định bằng cách điều chỉnh các tham số của nó. Do đó, chúng ta có thể đơn giản hóa hàm kernel thành φ(x) = x².

Kết hợp biến đổi affine vào hàm kernel, chúng ta có được

φ(q') = (γ_Q · q + β_Q)² = γ_Q² q² + 2γ_Q β_Q q + β_Q²,
φ(k') = (γ_K · k + β_K)² = γ_K² k² + 2γ_K β_K k + β_K².

trong đó q và k có các tham số riêng γ_Q, γ_K, β_Q, và β_K, cho phép mô hình học bất kỳ hàm bậc hai nào không âm và có một nghiệm thực duy nhất.

Thú vị, biến đổi của chúng tôi giống với việc áp dụng Layer Normalization (Ba et al., 2016), trừ việc chuẩn hóa bản thân. Chúng tôi đặt giả thuyết liệu việc chuẩn hóa q và k trước hàm kernel có thể cải thiện hiệu suất của mô hình. Nghi ngờ của chúng tôi được xác nhận khi việc chuẩn hóa tăng cường kết quả, như được chứng minh trong nghiên cứu Ablation sau này. Do đó, mô hình ReBased tinh chỉnh của chúng tôi kết hợp Layer Normalization.

Trong các phần tiếp theo, chúng tôi cung cấp phân tích sâu sắc và tiến hành các thí nghiệm toàn diện để xác thực hiệu quả của những sửa đổi này.

5 Thí nghiệm
5.1 Thiết lập Thí nghiệm
Chúng tôi áp dụng đánh giá đầu tiên của mô hình ReBased trên tác vụ MQAR, mà chúng tôi đã đào tạo một mô hình để thực hiện associative recall với số lượng token được truy xuất khác nhau. Arora et al. (2023) đề xuất rằng để đánh giá toàn diện, các mô hình cần được kiểm tra trên các độ dài chuỗi khác nhau, kích thước mô hình, và số lượng cặp query-key cần được truy xuất. Tuy nhiên, những thí nghiệm đó bị hạn chế, chỉ khám phá độ dài chuỗi lên đến 512. Những hạn chế này dẫn đến mô hình Based hiển thị hiệu suất có thể so sánh với cơ chế attention truyền thống.

Độ dài chuỗi dài hơn có thể được khám phá để hiểu sâu hơn về cách các cải tiến trong associative recall bị ảnh hưởng bởi những thay đổi trong cấu hình mô hình. Đây là lý do tại sao chúng tôi mở rộng đào tạo của mình để bao gồm các mô hình có khả năng xử lý độ dài chuỗi [128, 256, 512, 1024, 2048]. Chúng tôi đã kiểm tra một phạm vi kích thước ẩn từ 64 đến 512. Cho nghiên cứu ablation của chúng tôi để mang lại những hiểu biết chính xác hơn, chúng tôi cũng sử dụng các mô hình nhỏ hơn với kích thước ẩn khiêm tốn như [16, 24, 32, 48].

Để điều chỉnh cách tiếp cận của chúng tôi cho các chuỗi đa dạng, chúng tôi đã sử dụng các cặp query-key (qk) khác nhau cho mỗi độ dài. Các chi tiết cụ thể của những cấu hình này được nêu chi tiết trong Phụ lục A.

Chúng tôi cũng đưa các kiến trúc sub-quadratic khác vào thử nghiệm, bao gồm Mamba (họ SSM) (Gu and Dao, 2023), Hyena (họ long convolutions) (Poli et al., 2023), phương pháp attention vanilla, và RWKV (Peng et al., 2023). Bằng cách so sánh một loạt mô hình đa dạng, mục tiêu của chúng tôi là trình bày một đánh giá toàn diện về cách mô hình ReBased của chúng tôi nổi bật trong lĩnh vực này. Đối với Based, chúng tôi đã sử dụng Triton kernels được xuất bản bởi Yang and Zhang (2024), và đối với ReBased, chúng tôi đã sửa đổi nó để φ(x) = x². Chúng tôi đã sử dụng kiến trúc hybrid với short convolution và kernel size 3 trong lớp đầu tiên, và chỉ định một mixer trong lớp thứ hai. Chúng tôi thấy rằng thiết lập này ổn định hơn trên độ dài chuỗi dài hơn, đặc biệt khi sử dụng attention mixer. Tuy nhiên, chúng tôi không sửa đổi mô hình Mamba vì convolutions đã có sẵn bên trong khối Mamba. Chúng tôi đặt kết quả đầy đủ và chi tiết kiến trúc mô hình trong Phụ lục A.

Trong mô hình hóa ngôn ngữ, thiết lập thí nghiệm thứ hai của chúng tôi tận dụng tập dữ liệu Pile mở rộng (Gao et al., 2020) để đào tạo một mô hình ngôn ngữ (LM). Chúng tôi chọn độ dài chuỗi 4096, một sự tăng nhẹ từ giá trị tiêu chuẩn trong khi vẫn đảm bảo việc sao chép framework kiến trúc như được trình bày bởi Arora et al. (2024)¹. Lưu ý rằng một số siêu tham số như chiều mô hình và số lượng lớp được đặt để phù hợp với số lượng tham số mô hình trong thí nghiệm ban đầu. Cấu hình mô hình chi tiết có thể được tìm thấy trong Phụ lục C.

Tác vụ MQAR cung cấp hiểu biết về khả năng thành thạo In-Context Learning trên các kiến trúc khác nhau, trong khi đánh giá mô hình hóa ngôn ngữ cho phép chúng tôi đánh giá khả năng mô hình hóa phụ thuộc ngắn hạn. Ngoài các chỉ số perplexity truyền thống trên dữ liệu xác thực, chúng tôi cũng xem xét kỹ các biến thể Associative (AR) và Non-Associative (Non-AR) của perplexity. Ở đây, AR tương ứng với các vị trí token cần thiết associative recall, trong khi Non-AR đề cập đến các token khác. Khi các token lặp lại trong một văn bản, những lần xuất hiện tiếp theo được phân loại là AR, làm nổi bật khả năng của mô hình để nhớ lại từ ngữ cảnh.

5.2 Thí nghiệm MQAR
Trong Hình 1, chúng tôi trình bày khả năng của các mô hình khác nhau để xử lý tác vụ MQAR khi độ dài chuỗi tăng. Một quan sát chính là, ở độ dài chuỗi 2048, tất cả các mô hình, ngoại trừ mô hình Attention, đã gặp khó khăn để hoạt động hiệu quả khi bị giới hạn ở chiều mô hình 64. Khi chúng tôi mở rộng các chiều mô hình, hiệu suất của mô hình ReBased phù hợp hoặc vượt qua mô hình Based. Các kiến trúc RWKV và Mamba thất bại trên tác vụ MQAR trên tất cả các kích thước mô hình được kiểm tra.

Thí nghiệm này làm nổi bật tầm quan trọng của việc sử dụng các thiết lập tinh vi hơn, vì sự khác biệt hiệu suất giữa mô hình Attention và các mô hình khác (Based và ReBased) chỉ trở nên rõ ràng khi độ dài chuỗi vượt quá 512. Những kết quả này cho thấy rằng hiệu quả của các phương án thay thế attention như ReBased trở nên đặc biệt quan trọng khi xử lý chuỗi dài. Do đó, nhiều sự xem xét hơn nên được dành cho các cấu hình liên quan đến chuỗi dài để tận dụng toàn bộ tiềm năng của các mô hình như vậy.

5.3 Nghiên cứu Ablation
Chúng tôi đã kiểm tra toàn diện các yếu tố riêng lẻ của mô hình ReBased để hiểu cách mỗi yếu tố đóng góp vào hiệu quả tổng thể của nó, và đảm bảo tính minh bạch của những phát hiện của chúng tôi. Các thí nghiệm của chúng tôi được thiết kế tỉ mỉ để đánh giá mô hình bằng cách đánh giá ảnh hưởng của các thành phần riêng biệt của nó đối với hiệu suất. Các cấu hình thí nghiệm như sau:

• x² – thay thế hàm kernel gốc bằng phép toán bình phương element-wise đơn giản, φ(x) = x².

• norm(x)² – tích hợp bước chuẩn hóa mà không có biến đổi affine trước khi áp dụng phép toán bình phương.

• (γ·x)² – giới thiệu biến đổi affine chỉ về mặt tỷ lệ (không có bias) cho queries và keys.

• (γ·x+β)² – kết hợp biến đổi affine với cả tỷ lệ và bias cho queries và keys.

• ReBased (γ·norm(x)+β)² – mô hình toàn diện của chúng tôi, liên quan đến chuẩn hóa và biến đổi affine, bao gồm bias, cho queries và keys.

Lưu ý rằng đối với q và k, có các tham số tỷ lệ khác nhau γ_Q, β_Q, γ_K, và β_K cho mỗi thí nghiệm liên quan đến biến đổi affine.

Mục tiêu của chúng tôi là làm nổi bật ảnh hưởng của biến đổi độ dài chuỗi trong tác vụ MQAR đối với hiệu suất mô hình. Để đánh giá này, chúng tôi đã chuẩn hóa số lượng cặp truy xuất thành 32. Về mặt lý thuyết, không nên quan sát được tác động đến hiệu suất, vì lượng thông tin cần được lưu trữ trong các trạng thái ẩn không phụ thuộc vào độ dài chuỗi. Chúng tôi đã điều tra các ảnh hưởng trên các chuỗi có độ dài 256 và 2048 và minh họa những phát hiện của chúng tôi trong Hình 3 (cũng có sẵn trong Bảng 1 với độ lệch chuẩn của độ chính xác trên 5 seed). Chúng tôi phải nhấn mạnh tầm quan trọng của các thiết lập ngữ cảnh dài được đánh giá trong các thí nghiệm của chúng tôi. Đặc điểm của nó là quan trọng, vì hiệu suất thành công trên chuỗi dài làm nổi bật khả năng của mô hình để sử dụng đầy đủ các đổi mới kiến trúc của nó. Nó cũng chuyển thành những lợi thế thực tế đáng chú ý trong các ứng dụng thế giới thực nơi việc xử lý ngữ cảnh mở rộng một cách hiệu quả có thể rất quan trọng.

Mô hình ReBased được đề xuất hoạt động tốt hơn mọi sửa đổi khác. Hiệu suất trên độ dài ngắn 256 ít đáng chú ý hơn so với độ dài chuỗi dài 2048. Chúng tôi thấy một sự giảm hiệu suất từ việc đơn giản thay thế hàm kernel gốc bằng x². Chúng tôi cho rằng điều này được gây ra bởi quy mô tính năng không tối ưu, vì bằng cách đặt chuẩn hóa trước hàm kernel, chúng tôi có thể nhận thấy sự tăng hiệu suất ngay cả khi so sánh với mô hình Based. Các biến đổi affine (γ·x)² và (γ·x+β)² cũng cho thấy hiệu suất thuận lợi so với mô hình x², không giảm đáng kể theo độ dài chuỗi.

5.4 Mô hình hóa Ngôn ngữ

Bảng 2: Kết quả perplexity trên tập dữ liệu Pile (Gao et al., 2020). ReBased cải thiện kết quả trên các token AR. Tuy nhiên, vẫn còn một khoảng cách nhỏ giữa Attention và ReBased.

Chúng tôi đã tiến hành các thí nghiệm với mô hình hóa ngôn ngữ theo thiết lập được mô tả trong Phần 5.1. Xem Bảng 2 cho kết quả.

Chúng tôi lưu ý rằng mô hình ReBased hoạt động tốt hơn Based trên cả token AR và non-AR dẫn đến perplexity tổng thể thấp hơn. Điều này có thể được coi như một dấu hiệu của hiệu suất In-Context Learning tốt hơn. Trong phần tiếp theo, chúng tôi xem xét thiết lập few-shot trên một số tác vụ để xác thực.

Khi xem xét perplexity AR, chúng tôi quan sát thấy rằng vẫn còn khoảng cách giữa kiến trúc Transformer vanilla và các mô hình thay thế, điều này phù hợp với kết quả trên tập dữ liệu MQAR. Tuy nhiên, chúng tôi lưu ý rằng ReBased vẫn hoạt động tốt hơn Based. Về perplexity Non-AR, ReBased vượt qua cả hai kiến trúc Based, dẫn đến giá trị perplexity tổng thể tốt hơn. Lưu ý rằng attention có ít tham số có thể đào tạo hơn một chút, xem Phụ lục C để biết thêm chi tiết.

Những kết quả này cho thấy rằng, bất chấp perplexity mô hình hóa ngôn ngữ thấp hơn cho một phương án thay thế cho kiến trúc Transformer (Arora et al., 2024; Gu and Dao, 2023), điều này có thể đạt được do mô hình hóa phụ thuộc ngắn hạn tốt hơn, không yêu cầu học các hoạt động kết hợp cần thiết để thực hiện In-Context Learning (Olsson et al., 2022). Vanilla Transformers vẫn hoạt động tốt nhất về khả năng chú ý đến một số token in-context.

5.5 Hiệu suất Few-Shot
Để khám phá thêm khả năng của mô hình cải thiện kết quả trong các kịch bản thế giới thực, chúng tôi xác thực các mô hình Based và ReBased được đào tạo trên các benchmark few-shot phổ biến từ benchmark LM Evaluation Harness (Gao et al., 2023) và SuperGLUE (Wang et al., 2019). Kết quả được trình bày trong Bảng 3 và Bảng 4. ReBased vượt qua Based trên hầu hết các tác vụ.

5.6 Phân tích
Trong phần này, chúng tôi đi sâu vào động lực nội bộ của mô hình ReBased bằng cách kiểm tra các ma trận attention, thường được sử dụng để làm sáng tỏ việc ra quyết định của các mô hình và luồng thông tin giữa các token. Đáng chú ý, chúng ta có thể sử dụng chế độ song song với cả mô hình Based và ReBased để xây dựng những ma trận này.

Để phân tích của chúng tôi, chúng tôi sử dụng tập dữ liệu MQAR (Arora et al., 2023) và đào tạo một mô hình nhỏ gọn được cấu hình với độ dài chuỗi 128 và 32 cặp truy xuất. Để đảm bảo giải thích rõ ràng các bản đồ attention, chúng tôi đã sử dụng trọng số cố định trong lớp đầu tiên, bao gồm một short convolution với kernel chú ý đến token trước đó. Sau giai đoạn đào tạo, chúng tôi tính toán chỉ số Intersection over the Union (IoU) giữa ma trận attention và các vị trí thực tế của các token cần được truy xuất. Các vị trí chính xác là quan trọng, vì chúng đại diện cho các địa điểm mà mô hình phải sao chép các trạng thái ẩn để giải quyết thành công tác vụ. Cơ chế sao chép này đặc biệt quan trọng và được thực hiện thông qua attention tập trung trong lớp thứ hai của mạng (Olsson et al., 2022). Do đó, IoU cung cấp một thước đo định lượng về mức độ mô hình của chúng tôi đã học được để sao chép mẫu quan trọng này của việc truy xuất token. Một trực quan hóa của hiện tượng này sử dụng IoU trên một ví dụ được chọn ngẫu nhiên từ tập dữ liệu được hiển thị trong Hình 4. Lưu ý rằng chúng tôi đã cắt ma trận attention để chỉ kiểm tra một vùng nơi các cặp qk được lưu trữ.

Kết quả của chúng tôi được trình bày trong Bảng 5. Trong thí nghiệm của chúng tôi, mô hình Attention mang lại điểm IoU vượt trội so với cả mô hình Based và ReBased. Tuy nhiên, mô hình ReBased cho thấy triển vọng trong việc thu hẹp khoảng cách hiệu suất tồn tại giữa các phương pháp sub-quadratic và mô hình dựa trên attention. Điều này cho thấy rằng, bất chấp sự tương đối đơn giản của phương pháp, nó có thể phục vụ như một chỉ số thông tin cho tập dữ liệu MQAR, đặc biệt khi điểm accuracy gần bằng một, khiến việc phân biệt sự khác biệt hiệu suất giữa các mô hình trong các kịch bản thử nghiệm phức tạp hơn trở nên thách thức.

Bảng 3: Hiệu suất 1-shot trên các tác vụ từ benchmark LM evaluation harness (Gao et al., 2023).

Bảng 4: Hiệu suất 1-shot trên benchmark SuperGLUE (Wang et al., 2019).

Hình 4: Ma trận attention cho các mô hình khác nhau, và các vị trí ground truth cho query. Chúng tôi đo IoU giữa attention của mô hình và ma trận ground truth cho 10000 ví dụ.

Bảng 5: IoU với ma trận attention và vị trí ground truth để truy xuất trên tác vụ MQAR cho 10000 ví dụ.

6 Kết luận và Công việc Tương lai
Trong bài báo này, chúng tôi trình bày ReBased, một kiến trúc mới cho tính toán attention sub-quadratic. Đối với mô hình của chúng tôi, chúng tôi đã phân tích kiến trúc Based và đề xuất phát triển nó hơn nữa bằng cách sử dụng kernel đa thức với các tham số có thể học và thêm chuẩn hóa trước việc đánh giá kernel. Trong khi việc kết hợp layer normalization vào đào tạo mô hình đã được thử nghiệm trước đây (Henry et al., 2020), phương pháp của chúng tôi tích hợp việc chuẩn hóa này trực tiếp vào hàm kernel. Với thay đổi kiến trúc đơn giản này, chúng tôi đạt được kết quả vượt qua Based trên MQAR và mô hình hóa ngôn ngữ với các tác vụ tập dữ liệu Pile.

Chúng tôi đã phân tích các biểu diễn nội bộ của ReBased, Based, và các module attention vanilla, và kết luận rằng ReBased giống attention hơn so với Based. Đáng chú ý, trong khi Based sử dụng khai triển Taylor của hàm số mũ, hàm kernel ReBased khác với số mũ nhưng cho thấy hiệu suất tốt hơn. Nghiên cứu của chúng tôi cho thấy rằng việc sử dụng đa thức bậc hai có thể không đủ để có hiệu suất tốt nhất, và chỉ ra rằng các kernel có thể học tinh vi hơn có thể được sử dụng để cải thiện hiệu suất của các mô hình được đào tạo. Chuẩn hóa có thể cải thiện thêm các hàm kernel khác nhau. Điều này làm nổi bật nhu cầu cho các nhà nghiên cứu xem xét lại các phương pháp dựa trên kernel với mục tiêu tăng cường khả năng thích ứng và hiệu quả của chúng.

Những phát hiện của chúng tôi tiết lộ sự khác biệt trong việc xử lý tác vụ MQAR giữa các mô hình dựa trên attention và các mô hình khác như Based, đặc biệt khi độ dài chuỗi tăng. Các mô hình attention xuất sắc trên chuỗi dài hơn, vượt qua đáng kể các đối tác không attention của chúng. Những kết quả này làm nổi bật sự cần thiết của nghiên cứu thêm về các chiến lược có thể thu hẹp khoảng cách này để đạt được hiệu suất của các phương pháp dựa trên attention. Có lẽ các khía cạnh vượt trội của cơ chế attention có thể được khớp hoặc vượt qua bởi các mô hình khác, đặc biệt trên các tác vụ yêu cầu associative recall, như dịch máy (Vardasbi et al., 2023). Nghiên cứu tương lai có thể cung cấp hiểu biết về điều này, dẫn đến các mô hình cải tiến để xử lý chuỗi dài.

7 Hạn chế
Trong khi phương pháp được đề xuất của chúng tôi chứng minh khả năng áp dụng cho một loạt các tác vụ thường được giải quyết bởi Transformers, hiệu quả của nó trong việc xử lý các tác vụ liên quan đến việc sao chép hoặc nhớ lại ngữ cảnh trước đó một cách chuyên sâu vẫn chưa rõ ràng (xem Bảng 2 và Jelassi et al. (2024)). Việc giải quyết thành công những tác vụ này là quan trọng để giảm thiểu hoàn toàn các vấn đề suy luận liên quan đến cơ chế attention.

Cũng đáng lưu ý rằng các thí nghiệm của chúng tôi bị giới hạn ở các mô hình quy mô học thuật. Điều này thực sự đặt ra những hạn chế nhất định, đặc biệt trong việc ngoại suy các phát hiện cho các mô hình lớn hơn. Tuy nhiên, với những hạn chế về tài nguyên, kết quả của chúng tôi vẫn cung cấp những hiểu biết có giá trị về hiệu quả tiềm năng của phương pháp chúng tôi.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc]

--- TRANG 11 ---
(a) Kiến trúc Based
(b) Kiến trúc ReBased.
Hình 5: Trực quan hóa kiến trúc.

Bảng 6: Số lượng tham số mô hình trong tập dữ liệu MQAR.

A Chi tiết cho các thí nghiệm tập dữ liệu MQAR
Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng mã từ kho MQAR chính thức (Arora et al., 2024)². Tuy nhiên, chúng tôi sửa đổi mô hình attention từ cái được báo cáo trong Arora et al. (2024), vì chúng tôi thấy nó ổn định hơn (xem Hình 6). Chúng ta có thể thấy rằng việc thay thế lớp attention đầu tiên có lợi cho hiệu suất. RWKV hoạt động tốt hơn khi chúng tôi không thay thế lớp đầu tiên, đó là lý do tại sao chúng tôi sử dụng hai lớp RWKV trong thí nghiệm chính của chúng tôi (xem Hình 1). Chúng tôi báo cáo số lượng tham số có thể đào tạo trong Bảng 6.

Hình 6: Hiệu suất của kiến trúc hybrid với convolutions trên lớp đầu tiên và kiến trúc vanilla.

Chúng tôi cũng sửa đổi cấu hình dữ liệu để thách thức hơn cho mô hình. Bạn có thể thấy các tham số được điều chỉnh trong Bảng 7.

Bảng 7: Độ dài chuỗi và số lượng cặp QK trong tập dữ liệu.

Chúng tôi sử dụng batch size 512 cho tất cả các thí nghiệm. Trong trường hợp không đủ bộ nhớ GPU, chúng tôi sử dụng kỹ thuật gradient accumulation. Đối với learning rate, chúng tôi sử dụng tìm kiếm siêu tham số với lưới sau: 5e-4, 1e-3, 3e-3, 1e-2. Chúng tôi sử dụng năm seed khác nhau cho tất cả kết quả được báo cáo.

B Độ ổn định
Trong các thí nghiệm của chúng tôi, chúng tôi thấy ReBased ổn định hơn trong quá trình đào tạo với các siêu tham số khác nhau. Để chứng minh điều này, chúng tôi sử dụng biểu đồ Expected Validation Performance (EVP) (Dodge et al., 2021). Chúng tôi coi trung bình trên năm seed là độ chính xác cuối cùng. Kết quả của chúng tôi được trình bày trong Hình 7 của Phụ lục. Chúng tôi nhận thấy rằng ngay cả trong trường hợp chiều mô hình đủ lớn để lưu trữ tất cả thông tin cần thiết, các sửa đổi của chúng tôi dẫn đến độ chính xác 100% cho mọi bộ siêu tham số và mọi seed chúng tôi sử dụng, trái ngược với mô hình Based, nơi chúng tôi quan sát sự suy giảm cho một số learning rate nhất định.

C Chi tiết Thí nghiệm Tập dữ liệu Pile

Bảng 8: Số lượng tham số cho thí nghiệm pile.

Chúng tôi đào tạo mô hình của mình trên tập dữ liệu Pile được tokenized được xuất bản trên huggingface hub³. Lưu ý rằng tokenization này khác với cái được sử dụng trong Based⁴. Chúng tôi cũng sử dụng pipeline của mình, mà chúng tôi dự định sẽ phát hành công khai sớm. Chúng tôi không sử dụng rotary positional embeddings (Su et al., 2024) hoặc các thủ thuật khác, vì chúng tôi sao chép các mô hình của mình từ kho Based. Siêu tham số có thể được tìm thấy trong Bảng 9.

Bảng 9: Siêu tham số được sử dụng để đào tạo.

Như trong Arora et al. (2023), chúng tôi sử dụng nhiều siêu tham số hơn trong các mô hình Based/ReBased so với baseline Attention. Số lượng lớp và head dim được báo cáo trong Bảng 10 và 11. Chúng tôi sử dụng kiến trúc hybrid cho các mô hình Based/ReBased nơi chúng tôi sử dụng short convolution như một mixer cho mỗi lớp có số lẻ.

Bảng 10: Siêu tham số Attention.

Bảng 11: Siêu tham số Based/ReBased.

D Phân tích Bổ sung
Trong phần này, chúng tôi cung cấp kết quả và thí nghiệm bổ sung để hiểu thêm về cách mô hình ReBased học các phụ thuộc. Đầu tiên, chúng tôi cung cấp thêm ví dụ cho các thí nghiệm của chúng tôi với ma trận attention, như được nêu chi tiết trong Phần 5.6. Ma trận attention cho các ví dụ ngẫu nhiên từ tập kiểm tra được trình bày trong Hình 8. Nói chung, chúng ta có thể quan sát thấy rằng attention "kích hoạt" mạnh mẽ hơn tại các token truy xuất so với Based/ReBased. Kết quả này cho thấy rằng có thể vẫn còn một khiếm khuyết trong hàm kernel của chúng tôi mà phân phối attention cho các token không liên quan. Chúng tôi điều tra thêm hiện tượng này bằng cách phân tích phân phối attention trên token cuối cùng, như được hiển thị trong Hình 9. Số lượng token nhiễu cho mô hình ReBased nhỏ hơn so với Based, nhưng Attention thể hiện kết quả vượt trội.

Layer normalization là sự khác biệt chính giữa các mô hình Based và ReBased. Do đó, điều quan trọng là phân tích các tham số thu được trong quá trình đào tạo. Chúng tôi ghi lại mean và standard deviation của các tham số trên các độ dài chuỗi khác nhau. Kết quả của chúng tôi có thể được tìm thấy trong Hình 10. Đáng chú ý, giá trị tham số cuối cùng độc lập với độ dài chuỗi đào tạo, có thể chỉ ra rằng chúng ta có thể không cần đào tạo mô hình cho tất cả độ dài có thể. Cả tham số γ và β đều có giá trị độ lệch chuẩn cao so với giá trị tuyệt đối trung bình. Do đó, chúng ta có thể cho rằng điều quan trọng là cung cấp các tính năng với quy mô khác nhau.

Hình 7: Expected validation accuracy trên các siêu tham số khác nhau. Mô hình ReBased hoạt động tốt nhất trên tất cả siêu tham số, ngân sách, và chiều mô hình.

Hình 8: Ma trận attention cho các mô hình khác nhau, và các vị trí ground truth cho query. Chúng tôi đo IoU giữa attention của mô hình và ma trận ground truth cho 10000 ví dụ.

Hình 9: Điểm attention cho một ví dụ ngẫu nhiên. Điểm Based và Rebased nhiễu, trong khi attention có một đỉnh tại vị trí ground truth.

Hình 10: Phân tích các tham số layer normalization. Giá trị trung bình của tham số scale (gamma) có xu hướng về cùng giá trị khoảng 0.13, và tham số bias (beta) có xu hướng về 0.
