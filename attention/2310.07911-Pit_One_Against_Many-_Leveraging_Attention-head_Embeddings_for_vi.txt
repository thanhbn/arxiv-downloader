# 2310.07911.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2310.07911.pdf
# Kích thước tệp: 2058815 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Pit One Against Many: Leveraging Attention-head Embeddings for
Parameter-efficient Multi-head Attention
Huiyin Xue và Nikolaos Aletras
Khoa Khoa học Máy tính, Đại học Sheffield
Vương quốc Anh
{hxue12, n.aletras}@sheffield.ac.uk
Tóm tắt
Việc mở rộng các mô hình ngôn ngữ được huấn luyện trước đã mang lại những cải thiện hiệu suất lớn trong nhiều tác vụ xử lý ngôn ngữ tự nhiên nhưng đi kèm với chi phí lớn về yêu cầu bộ nhớ. Lấy cảm hứng từ các embedding vị trí trong transformer, chúng tôi nhằm đơn giản hóa và giảm dấu chân bộ nhớ của cơ chế multi-head attention (MHA). Chúng tôi đề xuất một module thay thế chỉ sử dụng một ma trận chiếu chia sẻ duy nhất và nhiều head embedding (MHE), tức là một cho mỗi head. Chúng tôi thực nghiệm chứng minh rằng MHE attention của chúng tôi hiệu quả hơn đáng kể về mặt bộ nhớ so với các cơ chế attention thay thế trong khi đạt được tỷ lệ duy trì hiệu suất dự đoán cao so với MHA vanilla trên nhiều tác vụ downstream. MHE attention chỉ yêu cầu một phần không đáng kể các tham số bổ sung (3nd, trong đó n là số lượng attention head và d là kích thước của head embedding) so với single-head attention, trong khi MHA yêu cầu (3n²−3n)d²−3nd tham số bổ sung.¹

1 Giới thiệu
Việc mở rộng các mô hình ngôn ngữ được huấn luyện trước (PLM) nhằm tăng cường hiệu suất bằng cách tăng kích thước và khả năng của chúng, dẫn đến các mô hình có số lượng tham số chưa từng có (Kaplan et al., 2020; Chowdhery et al., 2022; Hoffmann et al., 2022). Chỉ bằng việc tăng kích thước của PLM và dữ liệu huấn luyện trước đã mang lại hiệu suất tốt nhất trên nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2022a; Ouyang et al., 2022; Touvron et al., 2023).

Tuy nhiên, việc theo đuổi phát triển các PLM lớn hơn đi kèm với yêu cầu tính toán lớn. Điều này có những tác động môi trường trực tiếp như lượng khí thải carbon lớn (Lacoste et al., 2019; Strubell et al., 2019; Weidinger et al., 2022), xung đột

¹Mã nguồn: https://github.com/HUIYINXUE/simpleMHE

Hình 1: Số lượng tham số cho một sublayer attention và số lượng attention head khác nhau sử dụng multi-head attention MHA và multi-head embedding attention MHE của chúng tôi. Chúng tôi cố định chiều của attention là 64, chỉ đếm các tham số để chiếu query, key và value.

với các nguyên tắc phát triển trí tuệ nhân tạo Xanh (Schwartz et al., 2020). Hơn nữa, việc mở rộng có thể cản trở các nhà nghiên cứu có quyền truy cập hạn chế vào tài nguyên tính toán tham gia vào việc thúc đẩy lĩnh vực này (Schwartz et al., 2020). Điều này dẫn đến bất bình đẳng, nơi chỉ một số ít có đặc quyền có thể đóng góp tích cực, có thể cản trở sự đa dạng và tính bao hàm (Weidinger et al., 2022).

Xương sống của transformer (Vaswani et al., 2017) là module multi-head attention (MHA) mở rộng single-head attention (SHA) tiêu chuẩn được đề xuất bởi Cho et al. (2014). MHA áp dụng một cơ chế attention (tức là head) nhiều lần cho cùng một tập hợp query, key và value bằng cách sử dụng một tập hợp tham số khác nhau (tức là ma trận chiếu) cho mỗi head. Điều này dẫn đến các module MHA với dấu chân bộ nhớ lớn tăng theo số lượng layer và attention head trên mỗi layer trong PLM (Devlin et al., 2019; Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023). Hình 1 cho thấy số lượng tham số của một sublayer attention đơn tăng như thế nào với số lượng attention head của nó.

Các nghiên cứu trước đây đã cố gắng giải quyết vấn đề này bằng cách đề xuất chia sẻ ma trận chiếu hoặc

--- TRANG 2 ---
loại bỏ chúng hoàn toàn để cải thiện hiệu quả tham số của MHA. Lan et al. (2020a) đề xuất chia sẻ tham số chiếu cho key, query và value qua các layer, trong khi Kitaev et al. (2020) giới thiệu một phương pháp chia sẻ ma trận chiếu giữa key và value trong mỗi layer transformer. Ngoài ra, các phương pháp tương tự sử dụng cách tiếp cận multi-query attention sử dụng một cặp ma trận chiếu toàn cục cho key và value trong mỗi layer (Shazeer, 2019; Chowdhery et al., 2022; Ainslie et al., 2023). Hơn nữa, Yan et al. (2021) loại bỏ hoàn toàn ma trận chiếu và trực tiếp coi các hidden state đầu vào là cả key và value. Theo hướng khác, Lee-Thorp et al. (2022) đề xuất các mô hình thay thế các block attention bằng token-mixture block (tức là sử dụng biến đổi tuyến tính hoặc Fourier) chứa ít hoặc không có tham số so với MHA.

Lấy cảm hứng từ position embedding trong transformer (Vaswani et al., 2017; Devlin et al., 2019), chúng tôi nhằm đơn giản hóa và giảm dấu chân bộ nhớ của cơ chế MHA. Chúng tôi đạt được điều này bằng cách chỉ sử dụng một ma trận chiếu duy nhất cho mỗi key, query và value tương ứng được chia sẻ qua tất cả attention head, và một embedding trên mỗi head (MHE).

Các đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất MHE, một module attention mới sử dụng ma trận chiếu chia sẻ qua các head được sửa đổi bởi các embedding head tương ứng. Phương pháp của chúng tôi tạo ra nhiều attention head chỉ yêu cầu một phần nhỏ tham số bổ sung so với single-head attention.
• Chúng tôi thực nghiệm chứng minh rằng MHE attention của chúng tôi hiệu quả hơn đáng kể về tham số so với các cơ chế attention thay thế trong khi đạt được tỷ lệ duy trì hiệu suất dự đoán cao (tức là 92.9~98.7%) so với MHA trên nhiều tác vụ downstream. MHE nhỏ hơn MHA (3n²−3n)d²−3nd cho một sublayer attention đơn với n attention head và chiều ẩn d trên mỗi head.

2 Công trình liên quan
2.1 Nén mô hình
Để làm cho PLM hiệu quả về bộ nhớ, các nghiên cứu trước đây đã tập trung vào các phương pháp nén mô hình post-hoc sau đây (Ganesh et al., 2021; Tay et al., 2022).

Lượng tử hóa Hubara et al. (2017) đề xuất biểu diễn trọng số sử dụng ít bit hơn để giảm yêu cầu bộ nhớ. Zadeh et al. (2020) giới thiệu một phương pháp để xác định các outlier trong trọng số và loại trừ chúng trong quá trình lượng tử hóa. Một hướng khác bao gồm các bước huấn luyện bổ sung để điều chỉnh trọng số đã lượng tử hóa, tức là huấn luyện có nhận thức lượng tử hóa (Zafrir et al., 2019; Boo và Sung, 2020; Stock et al., 2020; Shen et al., 2020; Tambe et al., 2021; Tao et al., 2022). Bai et al. (2022) phát triển một phương pháp lượng tử hóa hậu huấn luyện hiệu quả hơn giảm thiểu lỗi tái tạo do lượng tử hóa gây ra.

Cắt tỉa Các phương pháp nén này loại bỏ hoàn toàn các phần của mạng như trọng số gần bằng không (Gordon et al., 2020; Mao et al., 2020; Chen et al., 2020) và trọng số di chuyển về không trong quá trình fine-tuning (Sanh et al., 2020; Tambe et al., 2021). Khác với việc thao tác trên các trọng số riêng lẻ, nghiên cứu trước đây đã cố gắng loại bỏ các khối trọng số có cấu trúc hoặc thậm chí các thành phần kiến trúc như attention head và encoder layer (Fan et al., 2019; Prasanna et al., 2020; Khetan và Karnin, 2020; Li et al., 2020a; Lin et al., 2020; Tay et al., 2021).

Chưng cất kiến thức Tập hợp các kỹ thuật này thường huấn luyện một mô hình học sinh nhẹ để bắt chước đầu ra của một PLM giáo viên lớn hơn (Sun et al., 2019; Li et al., 2020b; Jiao et al., 2020; Sun et al., 2020; Li et al., 2021; Tahaei et al., 2022). Theo hướng tương tự, các PLM nhỏ hơn gần đây đã được fine-tune trên văn bản được tạo bởi các PLM lớn hơn (Chiang et al., 2023; Taori et al., 2023).

Phân tách ma trận trọng số Nghiên cứu trước đây cũng đề xuất thay thế ma trận trọng số lớn bằng tích của hai ma trận nhỏ hơn để giảm kích thước mô hình và bộ nhớ thời gian chạy. Phân tách ma trận trọng số đã được áp dụng cho các layer tuyến tính (Mao et al., 2020; Ben Noach và Goldberg, 2020), ma trận embedding (Lan et al., 2020b; Tambe et al., 2021; Wang et al., 2022), và các block attention (Hu et al., 2022; Wang et al., 2022).

Nén ma trận embedding Cuối cùng, nhiều nỗ lực khác nhau đã được giới thiệu để nén ma trận embedding trong quá trình huấn luyện trước và fine-tuning (Xue et al., 2022; Clark et al., 2022b; Xue và Aletras, 2022).

--- TRANG 3 ---
2.2 Cải thiện hiệu quả Attention
Nghiên cứu trước đây về làm cho attention hiệu quả hơn bao gồm nỗ lực hướng tới (1) tăng tốc tính toán theo cặp giữa các biểu diễn token; và (2) hiệu quả tham số.

Hiệu quả tính toán Mặc dù cải thiện hiệu quả tính toán của attention nằm ngoài phạm vi bài báo của chúng tôi, chúng tôi cung cấp một tổng quan ngắn gọn về nghiên cứu trước đây vì nó bổ sung cho hiệu quả tham số. Một cách tiếp cận để tăng tốc tính toán attention là bằng cách giảm số lượng tính toán tương đồng giữa các biểu diễn ở các vị trí khác nhau sử dụng cửa sổ cục bộ được xác định trước, stride cố định hoặc động (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020; Kitaev et al., 2020). Các phương pháp khác tận dụng xấp xỉ của SoftMax để thay đổi thứ tự nhân ma trận, dẫn đến độ phức tạp tính toán thấp hơn (Katharopoulos et al., 2020; Choromanski et al., 2021; Schlag et al., 2021; Qin et al., 2022). Các phương pháp liên quan theo hướng này đề xuất các hàm kernel yêu cầu tham số bổ sung (Choromanski et al., 2021; Wang et al., 2020). Cuối cùng, Dao et al. (2022) đề xuất cải thiện trong truy cập bộ nhớ GPU để tối ưu hóa và tăng tốc tính toán MHA.

Hiệu quả bộ nhớ Lan et al. (2020a) giới thiệu một phương pháp chia sẻ tham số chiếu cho query, key và value qua các layer transformer. Hơn nữa, Kitaev et al. (2020) đề xuất chia sẻ ma trận chiếu giữa key và value trong mỗi layer. Ngoài ra, các phương pháp khác sử dụng cách tiếp cận multi-query attention chia sẻ trọng số chiếu cho key và value qua các head khác nhau (Shazeer, 2019; Chowdhery et al., 2022; Ainslie et al., 2023), trong khi Yan et al. (2021) trực tiếp coi các hidden state đầu vào là cả key và value. Theo hướng khác, Lee-Thorp et al. (2022) đề xuất thay thế các block attention bằng các token-mixture block nhanh hơn bao gồm một vài tham số hoặc không có tham số nào. Điều này bao gồm các phương pháp như biến đổi tuyến tính hoặc Fourier trong token-mixture block. Tuy nhiên, các phương pháp này có xu hướng mang lại hiệu suất dự đoán thấp hơn so với MHA.

3 Multiple Head Embeddings Attention
Lấy cảm hứng từ absolute position embedding (Vaswani et al., 2017; Devlin et al., 2019) để phân biệt biểu diễn của cùng một token trong các context khác nhau, chúng tôi đề xuất Multiple Head Embeddings (MHE) attention. MHE sử dụng ma trận chiếu 'seed' chia sẻ sau đó được kết hợp với các head embedding riêng biệt để tạo ra nhiều attention head.

3.1 Multi-head Attention (MHA)
Trước tiên chúng tôi bắt đầu bằng cách định nghĩa chính thức MHA. MHA bao gồm các ma trận chiếu khác nhau (W^Q_i, W^K_i, W^V_i ∈ R^{d_m×d_h}, i = 1, ..., n, trong đó d_m là chiều của biểu diễn đầu vào và d_h là chiều của n attention head) cho query (Q), key (K) và value (V) trên mỗi head, tổng cộng 3×n. Nó được tính như sau:

Q_i, K_i, V_i = XW^{Q,K,V}_i (1)
H_i = Att(Q_i, K_i, V_i) (2)
= SoftMax(Q_iK_i^T/√d_h)V_i (3)

Lưu ý rằng chúng tôi sử dụng scale-dot attention, nhưng phương pháp của chúng tôi có thể được sử dụng với bất kỳ cơ chế attention nào khác.

3.2 Ma trận chiếu Seed
Không giống như MHA sử dụng ma trận chiếu khác nhau trên mỗi head, MHE attention chỉ sử dụng một ma trận chiếu duy nhất cho mỗi query, key và value, W^Q, W^K, W^V ∈ R^{d_m×d_h}. Các ma trận này được chia sẻ qua tất cả attention head.

Chúng tôi thu được chiếu query, key và value của chuỗi đầu vào X như sau:
Q, K, V = XW^{Q,K,V} (4)

3.3 Attention Head Embeddings
Sử dụng ma trận chiếu seed cho Q, K, V tương đương với một module single-head attention (SHA). Do đó, chúng tôi cần một cơ chế để biến đổi ma trận chiếu seed để thu được attention head khác nhau. Với mục đích này, chúng tôi biểu diễn mỗi attention head i bằng các head embedding cụ thể e^Q_i, e^K_i, e^V_i ∈ R^{d_h}, i = 1, ..., n cho query, key và value. Các embedding này có dấu chân bộ nhớ nhỏ hơn đáng kể so với việc sử dụng ma trận chiếu khác nhau trên mỗi head. Biểu diễn có ngữ cảnh H_i của toàn bộ chuỗi đầu vào X cho head i được tính như sau:

Q̃_i, K̃_i, Ṽ_i = ψ(Q; K; V, e^{Q,K,V}_i) (5)
H_i = Att(Q̃_i, K̃_i, Ṽ_i) (6)

trong đó ψ(·) là một hàm sửa đổi ma trận query, key và value với head embedding tương ứng e_i.

--- TRANG 4 ---
[Hình 2 được giữ nguyên vì chứa sơ đồ]

Hình 2: Multi-head attention (trái) yêu cầu 3×n ma trận chiếu cho query, key và value (W^{Q,K,V}) trong đó n là số lượng attention head. Multi-head embedding attention (phải) chỉ sử dụng ba ma trận chiếu và 3×n head embedding.

3.4 Sửa đổi Query, Key và Value bằng Head Embeddings
Chúng tôi đề xuất hai biến thể MHE, một cộng và một nhân head embedding với ma trận chiếu seed.

MHE-ADD: Được thúc đẩy bởi absolute position embedding (Devlin et al., 2019), chúng tôi sử dụng phép toán cộng trong Phương trình 5, được biểu diễn như ψ(A,b) := A + b, trong đó A ∈ {Q,K,V} và b ∈ {e^Q, e^K, e^V} tương ứng.

MHE-MUL: Tương tự, được thúc đẩy bởi rotary position embedding (Su et al., 2021), MHE-MUL sử dụng phép nhân như phép toán tích hợp trong Phương trình 5 như ψ(A,b) := A ⊙ (b + 1), trong đó ⊙ biểu diễn tích Hadamard.²

Hình 2 cho thấy tổng quan về cơ chế MHE so với MHA.

4 Thiết lập thử nghiệm
4.1 Cơ chế Attention
Chúng tôi so sánh MHE attention của chúng tôi với các cơ chế attention sau:³

²Chúng tôi cộng 1 để tránh các phần tử trong query, key và value trở nên quá nhỏ trong quá trình khởi tạo.
³Chúng tôi cũng đã thử nghiệm với các mô hình token-mixture Tuyến tính và Fourier (Lee-Thorp et al., 2022) mang lại hiệu suất thấp hơn đáng kể. Để có kết quả đầy đủ của các phương pháp này, xem Phụ lục A.

• Multi-head Attention (MHA): Đây là cơ chế multi-head attention gốc (Vaswani et al., 2017; Devlin et al., 2019).
• Single-head Attention (SHA): Tương tự như MHA nhưng chỉ sử dụng một attention head.
• EL-ATT: Được giới thiệu bởi Yan et al. (2021), biến thể attention này hoàn toàn loại bỏ ma trận chiếu cho tất cả key và value.
• MQA: Được giới thiệu bởi Shazeer (2019), phương pháp này sử dụng ma trận chiếu chia sẻ cho key và value qua tất cả attention head. Lưu ý rằng các ma trận chiếu khác nhau được sử dụng cho query qua các head.
• SKV: Được giới thiệu bởi Kitaev et al. (2020), biến thể attention này buộc key và value chia sẻ cùng ma trận chiếu trong mỗi module attention.

4.2 Dữ liệu
Chúng tôi thử nghiệm với một loạt tác vụ đa dạng bao gồm: (1) hai benchmark hiểu ngôn ngữ tự nhiên tiêu chuẩn bằng tiếng Anh, GLUE (Wang et al., 2018) và SUPER GLUE (Wang et al., 2019); (2) hai benchmark hỏi đáp bằng tiếng Anh, SQUAD V1.1 (Rajpurkar et al., 2016) và SQUAD V2.0 (Rajpurkar et al., 2018); (3) dịch máy WMT-14 tiếng Anh-sang-tiếng Đức (Bojar et al., 2014); và (4) hai bộ dữ liệu mô hình hóa ngôn ngữ bằng tiếng Anh WIKITEXT-103 (Merity et al., 2017) và PENN TREEBANK (Marcus et al., 1993).

4.3 Mô hình
Chúng tôi kiểm tra tất cả các biến thể attention khác nhau trên hai kiến trúc: (1) transformer chỉ encoder (Devlin et al., 2019) và (2) transformer encoder-decoder (Vaswani et al., 2017).

Chỉ encoder Cho GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0, chúng tôi sử dụng kiến trúc BERT-base. Điều này bao gồm 12 layer transformer, kích thước embedding là 768, chiều hidden state là 768, 12 attention head và độ dài chuỗi tối đa là 512.

Chỉ decoder Chúng tôi cũng kiểm tra mô hình chỉ decoder sử dụng kiến trúc GPT2-base trên WIKITEXT-103, PENN TREEBANK và GLUE. GPT2-base bao gồm 12 layer transformer, kích thước embedding là 768, chiều hidden state là 768, 12 attention head và độ dài chuỗi tối đa là 512.

--- TRANG 5 ---
Encoder-decoder Cho WMT-14, chúng tôi huấn luyện một transformer encoder-decoder từ đầu. Nó bao gồm 12 layer (6 cho encoder và decoder tương ứng), kích thước embedding là 512, chiều hidden state là 512 và 8 attention-head và độ dài chuỗi tối đa là 100.

Chúng tôi đặt số lượng attention head là 1 cho tất cả mô hình SHA. Thử nghiệm với các mô hình lớn hơn và số lượng attention head khác nằm ngoài phạm vi bài báo của chúng tôi và để lại cho nghiên cứu tương lai do quyền truy cập hạn chế vào tài nguyên tính toán.

4.4 Chi tiết thực hiện
Huấn luyện trước Chúng tôi huấn luyện trước tất cả mô hình trên English Wikipedia và BookCorpus (Zhu et al., 2015) từ HuggingFace (Lhoest et al., 2021) lên đến 1M bước với kích thước batch là 128. Chúng tôi chọn masked language modelling làm mục tiêu huấn luyện trước. Cho tất cả mô hình, chúng tôi sử dụng từ vựng WordPiece 30K (Devlin et al., 2019).

Fine-tuning và Huấn luyện Cho GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0, chúng tôi fine-tune tất cả mô hình được huấn luyện trước lên đến 20 epoch với early stopping cố định kích thước batch là 32. Cho mỗi tác vụ, chúng tôi sử dụng năm seed khác nhau và báo cáo trung bình.

Chúng tôi huấn luyện mô hình encoder-decoder từ đầu trên tập huấn luyện của bộ dữ liệu dịch máy WMT-14 tiếng Anh-sang-tiếng Đức lên đến 100K bước với kích thước batch là 256. WMT-14 chứa 4.5M cặp câu và đánh giá trên tập kiểm tra của nó. Chúng tôi huấn luyện tokenizer sử dụng byte-pair-encoding (Sennrich et al., 2016) với 37K bước merge trên tập huấn luyện. Chúng tôi cho phép cả ngôn ngữ nguồn và ngôn ngữ đích chia sẻ từ vựng. Chúng tôi sử dụng một seed ngẫu nhiên và báo cáo trung bình trên năm epoch cuối. Chúng tôi tối ưu hóa tất cả mô hình sử dụng AdamW (Loshchilov và Hutter, 2019).

Siêu tham số Chi tiết lựa chọn siêu tham số trong Phụ lục B.

Phần cứng Cho huấn luyện trước, chúng tôi sử dụng bốn GPU NVIDIA Tesla A100 và một cho fine-tuning trên các tác vụ downstream.

4.5 Đánh giá hiệu suất dự đoán
Cho GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0, chúng tôi sử dụng metric chính thức của mỗi tác vụ (xem Phụ lục A để biết chi tiết về metric cho mỗi tác vụ). Chúng tôi báo cáo điểm F1 cho SQUAD V1.1 và SQUAD V2.0. Chúng tôi sử dụng BLEU để báo cáo hiệu suất trong tác vụ dịch máy WMT-14 tiếng Anh-sang-tiếng Đức. Chúng tôi sử dụng perplexity (PPL) để báo cáo hiệu suất tạo sinh trên WIKITEXT-103 và PENN TREEBANK bằng cách cố định độ dài stride là 256.

4.6 Đánh giá hiệu quả bộ nhớ
Hơn nữa, chúng tôi sử dụng các metric sau để đo lường và so sánh hiệu quả bộ nhớ của MHE và các baseline.

• Tỷ lệ duy trì hiệu suất: Chúng tôi tính tỷ lệ giữa hiệu suất dự đoán của mỗi cơ chế attention so với hiệu suất baseline MHA upper-bound (càng cao càng tốt).

Cho chỉ số trực tiếp (như accuracy v.v.):
PRR = score_model / score_MHA

Cho chỉ số nghịch đảo (như perplexity v.v.):
PRR = 1 - (score_model - score_MHA) / score_MHA

• Độ đàn hồi hiệu suất của tham số: Lấy cảm hứng từ khái niệm độ đàn hồi trong kinh tế học (Bittermann, 1934), đo lường khả năng phản ứng của một biến kinh tế (như nhu cầu đầu tư) với sự thay đổi của biến khác (như lãi suất), chúng tôi mở rộng nó để đo lường tỷ lệ sử dụng tham số của mô hình đích so với SHA lower-bound. Độ đàn hồi hiệu suất của tham số (PEoP) chỉ ra mức độ hiệu quả của tham số đóng góp vào hiệu suất dự đoán, so với SHA. Nó được tính như sau:

Cho chỉ số trực tiếp (như accuracy v.v.):
PEoP = ((score_model/score_SHA) - 1) / ((params_model/params_SHA) - 1)

Cho chỉ số nghịch đảo (như perplexity v.v.):
PEoP = -((score_model/score_SHA) - 1) / ((params_model/params_SHA) - 1)

PEoP định lượng mức độ mà hiệu suất của mô hình có thể được tăng cường với 1% tham số bổ sung so với mô hình baseline (càng cao càng tốt).⁴

⁴Chúng tôi trừ 1 trong cả tử số và mẫu số, theo định nghĩa gốc của độ đàn hồi.

--- TRANG 6 ---
[Bảng 1: Kết quả của kiến trúc chỉ encoder - nội dung được dịch nhưng giữ nguyên cấu trúc bảng]

Bảng 1: Kết quả của kiến trúc chỉ encoder trên tập dev GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0 với tỷ lệ duy trì hiệu suất (PRR) và độ đàn hồi hiệu suất của tham số (PEoP) trên năm lần chạy. Các giá trị in đậm biểu thị phương pháp có hiệu suất tốt nhất trong mỗi benchmark.

[Bảng 2: Kết quả của kiến trúc chỉ decoder - nội dung được dịch nhưng giữ nguyên cấu trúc bảng]

Bảng 2: Kết quả của kiến trúc chỉ decoder trên tập dev GLUE và tập test WIKITEXT-103, PENN TREEBANK với tỷ lệ duy trì hiệu suất (PRR) và độ đàn hồi hiệu suất của tham số (PEoP) trên năm lần chạy. Các giá trị in đậm biểu thị phương pháp có hiệu suất tốt nhất trong mỗi benchmark.

5 Kết quả
5.1 So sánh hiệu suất dự đoán
Bảng 1 trình bày kết quả trên GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0 cho các biến thể MHE của chúng tôi và tất cả baseline. Trước tiên chúng tôi quan sát rằng hiệu suất của cả MHE-ADD và MHE-MUL của chúng tôi đều có thể so sánh được với vanilla MHA trên hai benchmark phân loại văn bản (80.4, 80.6 so với 81.9 trên GLUE trung bình và 69.1, 69.6 so với 70.5 trên SUPER GLUE trung bình) với tỷ lệ duy trì hiệu suất cao (PRR) từ 97.9% đến 98.7%. Trên các tác vụ hỏi đáp SQUAD V1.1 và SQUAD V2.0, cả hai biến thể MHE cũng có tính cạnh tranh, với PRR cao hơn 93%.

Kết quả tương tự được quan sát trên tác vụ dịch máy WMT-14 tiếng Anh-sang-tiếng Đức cho transformer encoder-decoder. Theo Bảng 3, MHE-ADD và MHE-MUL đạt điểm BLEU lần lượt là 23.0 và 23.6. Hiệu suất của MHE-MUL thấp không đáng kể so với MHA (24.8) trong khi nhỏ hơn đáng kể.

Kết quả nhất quán cho transformer chỉ decoder được hiển thị trong Bảng 2. PRR cho MHE-ADD và MHE-MUL trên GLUE vẫn cao (tức là 97.8% và 99.0%). Trong khi sử dụng các metric nội tại để đánh giá, MHE-MUL dẫn đến perplexity 53.8 và 50.7 so với 43.0 và 44.3 cho MHA trên WIKITEXT-103 và PENN TREEBANK tương ứng, cho thấy PRR ổn định cao hơn 74.9%.

Trong tất cả tác vụ, MHE luôn vượt trội so với SHA với biên độ lớn chỉ với 0.03M tham số bổ sung, tức là 0.6~17.4. Ví dụ, 69.6 so với 67.1 trong SUPER GLUE, 72.3 so với 67.6 trong SQUAD V2.0, 23.6 so với 22.5 trong WMT-14 và 62.0 so với 53.8 trong WIKITEXT-103 cho biến thể MHE-MUL. Chúng tôi cũng lưu ý rằng các cơ chế attention MQA và SKV thường hoạt động tốt hơn MHE, tuy nhiên chúng lớn hơn 1.7 và 2.4 lần so với MHE, tức là 15.34M và 21.23M so với 8.88M tham số.

Đáng chú ý là MHE-MUL vượt trội hơn EL-ATT trên ba trong số năm benchmark, mặc dù có gần một nửa tham số trong module attention.

5.2 So sánh hiệu quả bộ nhớ
Kết quả của chúng tôi cho đến nay cho thấy hiệu suất tăng theo số lượng tham số cơ chế attention, điều này được mong đợi. Tiếp theo, chúng tôi kiểm tra mức độ hiệu quả mà các cơ chế attention khác nhau sử dụng tham số của chúng⁵. Bảng 1 và 3 cho thấy mức độ hiệu quả tham số của hai biến thể MHE attention và tất cả baseline của chúng tôi, được đo bằng PEoP. Lưu ý rằng điểm PEoP cho SHA không thể tính được vì nó được sử dụng làm mô hình tham chiếu. Chúng tôi cũng báo cáo PRR sử dụng MHA làm baseline để hoàn thiện, tuy nhiên metric này không tính đến kích thước mô hình.

Trước tiên chúng tôi quan sát trong Bảng 1 rằng cả MHE-ADD và MHE-MUL của chúng tôi đều đạt điểm PEoP cao nhất trên hai benchmark hiểu ngôn ngữ tự nhiên (4.92, 5.53 trên GLUE, và 9.44, 12.07 trên SUPER GLUE) và hai tác vụ hỏi đáp (4.65, 13.19 trên SQUAD V1.1, và 19.88, 22.25 trên SQUAD V2.0). Ngược lại, vanilla MHA cho kết quả điểm PEoP thấp nhất trong tất cả mô hình như mong đợi, từ 0.02 đến 0.06. Điều này cho thấy tính không hiệu quả về bộ nhớ của MHA.

PEoP của EL-ATT và SKV nhẹ hơn tương tự như MHA (0.02) trên GLUE trung bình, chỉ 4‰ so với MHE, cho thấy chúng kém hiệu quả hơn nhiều về bộ nhớ so với MHE.

Những phát hiện tương tự được quan sát trong WMT-14 cho các mô hình encoder-decoder được mô tả trong Bảng 3. MHE-ADD và MHE-MUL đạt điểm PEoP lần lượt là 20.0 và 27.9. Ngược lại, điểm PEoP của MHA, EL-ATT, MQA và SKV gần bằng không (chỉ 0.1). Điều này có nghĩa là đầu tư thêm tham số vào các module attention của chúng sẽ không mang lại lợi ích tương xứng trong hiệu suất dự đoán. Ngay cả đối với SKV có kích thước bằng một nửa MHA và đạt PRR cao, khi số lượng tham số tăng 1%, điểm BLEU tăng không đáng kể 0.1%, trong khi phát triển từ SHA. Tuy nhiên, với cùng số lượng tham số, MHE-MUL kém hiệu quả nhất về bộ nhớ của chúng tôi có thể cải thiện điểm BLEU 11.0%. Tỷ lệ hoàn vốn như vậy lớn hơn 110 lần so với SKV. Tận dụng head embedding bằng cách chỉ thêm một số lượng không đáng kể tham số cải thiện hiệu quả hiệu suất dự đoán.

Chúng tôi tiếp tục quan sát rằng MHE-ADD và MHE-MUL không phụ thuộc vào kiến trúc, đạt được hiệu quả bộ nhớ tương tự cho mô hình chỉ decoder trong Bảng 2. Cả MHE-ADD và MHE-MUL của chúng tôi đều đạt điểm PEoP cao nhất trên hai benchmark mô hình hóa ngôn ngữ (41.29, 42.32 trên WIKITEXT-103 và 60.15 và 81.76 trên PENN TREEBANK) và GLUE (2.18 và 5.92). Đồng thời, MHA không thể hoạt động tốt trên GLUE và PENN TREEBANK với PEoP lần lượt là 0.01 và 0.16. MHE-ADD và MHE-MUL cũng luôn vượt trội hơn các biến thể attention hiệu quả khác (tức là EL-ATT, MQA và SKV) từ 72~340 lần về PEoP qua ba benchmark.

Trong tất cả tác vụ, MHE luôn vượt trội hơn MHA theo bậc độ lớn về hiệu quả tham số. Chúng tôi cũng lưu ý rằng EL-ATT, MQA và SKV chỉ dẫn đến điểm PEoP với cùng bậc độ lớn như MHA. Điều này làm nổi bật khả năng sử dụng tham số vượt trội của các biến thể MHE attention, đạt được hiệu quả bộ nhớ tiên tiến.

5.3 Độ phức tạp bộ nhớ lý thuyết
Bảng 4 trình bày độ phức tạp bộ nhớ lý thuyết và tổng số tham số của hai MHE và các cơ chế attention baseline của chúng tôi trong một sublayer transformer đơn. Trước tiên, chúng ta thấy rằng độ phức tạp bộ nhớ lý thuyết của MHA và các tham số hiệu quả khác (EL-ATT, MQA và SKV) là bậc hai với số lượng attention head, trong khi MHE của chúng tôi là hai biến thể duy nhất có độ phức tạp tuyến tính với attention head tương tự như SHA.

Nhìn kỹ hơn vào cột ngoài cùng bên phải trong Bảng 4, chúng ta quan sát rằng số lượng tham số bổ sung của tất cả biến thể attention so với SHA có mối quan hệ bậc hai với cả số lượng n và chiều của attention head d, ngoại trừ hai biến thể MHE của chúng tôi. MHE chỉ yêu cầu một phần tương đối nhỏ tham số bổ sung so với SHA.

⁵Để có báo cáo chi tiết về việc sử dụng bộ nhớ của các cơ chế attention khác nhau, xem Phụ lục C.

--- TRANG 7 ---
[Bảng 3: Điểm BLEU trên tác vụ dịch máy - nội dung được dịch nhưng giữ nguyên cấu trúc bảng]

Bảng 3: Điểm BLEU trên tác vụ dịch máy WMT-14 tiếng Anh sang tiếng Đức với tỷ lệ duy trì hiệu suất (PRR) và độ đàn hồi hiệu suất của tham số (PEoP). Các giá trị in đậm biểu thị phương pháp có hiệu suất tốt nhất trong mỗi benchmark.

[Bảng 4: Độ phức tạp bộ nhớ - nội dung được dịch nhưng giữ nguyên cấu trúc bảng]

Bảng 4: Độ phức tạp bộ nhớ liên quan đến số lượng tham số trong mỗi sublayer attention, trong khi cố định chiều của attention head là d. n biểu thị số lượng attention head. Để đơn giản, chiều của hidden state d_m được đặt là nd. Phép chiếu cuối cùng để gộp attention head bị loại trừ.

5.4 Mở rộng số lượng tham số Attention
Đi sâu hơn vào ảnh hưởng của việc mở rộng đến dấu chân bộ nhớ, chúng tôi hiển thị trong Hình 3 tổng số tham số cần thiết cho một module attention đơn (ví dụ trong một encoder layer). Chúng tôi cố định chiều của attention head là 64 thường được sử dụng bởi BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020) và T5 (Raffel et al., 2020). Nói chung, chúng tôi lưu ý rằng số lượng tham số trong MHA có thể đạt hơn 200M nếu sử dụng 128 attention head. Đồng thời, SKV, MQA và EL-ATT sẽ yêu cầu lần lượt 2/3, 1/3 và 1/3 số đó. Ngược lại, MHE chỉ chiếm 1% tham số MHA.

Hơn nữa, chúng tôi cũng trình bày trong Hình 4 tổng số tham số yêu cầu qua các biến thể attention khi xếp chồng 12, 24 và 48 layer cùng với 32 và 64 attention head tương ứng. Chúng tôi cũng cố định chiều của attention head là 64. Chúng ta có thể quan sát, khi số lượng attention head đạt 64, MHA với 24 layer đã chiếm hơn 1B tham số, trong khi EL-ATT và MQA đạt 0.8B tham số với 48 layer. SKV cần 24 layer để đạt 0.8B tham số. Tuy nhiên, tổng số tham số trong MHE attention không vượt quá 0.1B ngay cả khi mở rộng đến 48 layer với 64 attention head. Cũng rõ ràng là việc mở rộng module attention đến 48 layer, 32 attention head và 12 layer cần số lượng tham số có thể so sánh cho MHA, EL-ATT, MQA hoặc SKV. Điều này cho thấy, các nhà phát triển LLM phải đưa ra lựa chọn có tăng gấp đôi số lượng attention head hay cắt giảm số lượng layer xuống một phần tư khi làm việc trong ngân sách bộ nhớ chặt chẽ. Tuy nhiên, MHE không gặp phải những vấn đề như vậy.

Hơn nữa, chúng tôi dự kiến các ước tính này cho mô hình GPT-3 phổ biến (Brown et al., 2020). Đây là mô hình chỉ decoder với 96 decoder layer, 96 attention head trên mỗi layer, và chiều head là 128. Module multi-head attention vanilla yêu cầu 43.48B tham số khổng lồ. Tuy nhiên, sử dụng MHE attention, con số này có thể được giảm đáng kể xuống 0.46B tham số, tức là giảm khoảng 98.9%.⁶ So sánh điều này với các biến thể attention hiệu quả tham số khác như EL-ATT (14.50B tham số), MQA attention (14.80B tham số), và SKV attention (28.99B tham số), rõ ràng là MHE của chúng tôi cung cấp hiệu quả bộ nhớ tốt hơn. Điều này làm cho nó trở thành một lựa chọn thay thế hấp dẫn cho các tình huống hạn chế bộ nhớ. Xem Phụ lục D để nghiên cứu chi tiết về tính mạnh mẽ của MHE đối với thay đổi kích thước mô hình (tức là mở rộng).

⁶Sẽ tuyệt vời nếu báo cáo kết quả bằng cách huấn luyện trước mô hình MHE GPT-3 của riêng chúng tôi, tuy nhiên điều này không khả thi với tính toán khiêm tốn mà chúng tôi có sẵn.

6 Thảo luận
MHA cho phép mô hình chú ý đến thông tin từ các không gian con biểu diễn khác nhau ở các vị trí khác nhau (Vaswani et al., 2017). Nó sử dụng ma trận chiếu riêng biệt cho mỗi attention head và tích hợp thông tin từ các không gian con biểu diễn khác nhau này. Tuy nhiên, Vaswani et al. (2017) đã không khám phá các phương pháp khác nhau để thực hiện biến đổi không gian trên mỗi head.

Nghiên cứu trước đây đã chỉ ra rằng các mô hình được tham số hóa quá mức có thể có chiều nội tại thấp. Do đó, việc biến đổi ma trận chiếu thành các ma trận low-rank nhỏ hơn thường không làm hại nghiêm trọng hiệu suất dự đoán của mô hình (Li et al., 2018; Aghajanyan et al., 2020). Trong khi đó, phương pháp MHA cổ điển cũng không áp đặt bất kỳ ràng buộc nào về tính trực giao của các không gian con này trong quá trình huấn luyện trước và fine-tuning. Các vector cột trong những ma trận chiếu đó có thể có tính thẳng hàng cao, tức là ma trận chiếu có thể bị thiếu hạng. Kết quả là, cơ chế làm việc bên trong của nó có thể được hiểu đơn giản như việc đưa ra các mức độ biến thiên cho biểu diễn được mã hóa của cùng một token ở cùng vị trí qua các head khác nhau.

Phương pháp MHE của chúng tôi có thể đạt được hiệu quả bộ nhớ (tương tự SHA) cùng với PRR cao so với MHA bằng cách bắt chước position embedding để biểu diễn các attention head khác nhau.

Một mặt, phép toán cộng trong MHE-ADD được sử dụng để biến đổi key, query và value. Điều này có thể được xem như một biến dạng nhỏ của không gian con thu được thông qua phép chiếu, theo sau bởi phép quay. Đối với một biểu diễn đầu vào, sự khác biệt giữa query, key và value được chiếu và được tiêm (tức là thông qua việc cộng head embedding) là một vector không đổi qua bất kỳ cặp head nào. Mặt khác, phương pháp MHE-MUL sử dụng phép toán nhân, biến dạng và định hình lại key, query và value subspace một cách tích cực hơn. Head embedding trong MHE-MUL đóng vai trò như các hệ số tỷ lệ, kéo dãn từng chiều của biểu diễn đầu vào tương ứng. Do đó, sự khác biệt giữa key, query và value được tạo ra bởi các head khác nhau cho cùng biểu diễn đầu vào, là một vector song song với đầu vào được chiếu. Vector này phụ thuộc vào đầu vào cụ thể, không giống như vector không đổi trong MHE-ADD.

Thú vị là, kết quả thực nghiệm của chúng tôi liên tục cho thấy rằng phép toán nhân vượt trội hơn phép cộng trong đa số benchmark. Điều này khẳng định những phát hiện của một nghiên cứu thực nghiệm trước đây của Su et al. (2021) đã so sánh rotary position embedding (có phần tương tự với MHE-MUL) với absolute position embedding (tương tự với MHE-ADD).

7 Kết luận
Chúng tôi đã đề xuất MHE attention sử dụng ma trận chiếu chia sẻ đơn cùng với nhiều head embedding, để đơn giản hóa và giảm dấu chân bộ nhớ của MHA. Kết quả thực nghiệm của chúng tôi đã chứng minh rằng MHE attention thể hiện hiệu quả bộ nhớ vượt trội so với các biến thể attention hiệu quả bộ nhớ khác, trong khi đạt được tỷ lệ hiệu suất dự đoán cao so với MHA trên các tác vụ downstream khác nhau. So với single-head attention, MHA yêu cầu (3n²−3n)d² tham số cho n attention head và chiều head d, trong khi MHE chỉ cần 3nd không đáng kể. Cho nghiên cứu tương lai, chúng tôi dự định điều tra việc mở rộng các mô hình MHE và khám phá khả năng ngôn ngữ học của chúng (Vulić et al., 2020; Koto et al., 2021).

Hạn chế
Chúng tôi chỉ thử nghiệm sử dụng các mô hình kích thước 'base' mà không thử nghiệm với các kiến trúc lớn hơn, do quyền truy cập hạn chế vào tài nguyên tính toán. Tương tự, chúng tôi đã không thử nghiệm với các kiến trúc chỉ decoder (Brown et al., 2020) mà chúng tôi để lại cho nghiên cứu tương lai. Chúng tôi đã không kết hợp phương pháp MHE của chúng tôi với các phương pháp attention hiệu quả tính toán với độ phức tạp tuyến tính, như Linformer (Wang et al., 2020). Chúng tôi mong đợi rằng nó sẽ tăng tốc tính toán của MHE, nhưng nó nằm ngoài phạm vi bài báo của chúng tôi.

Lời cảm ơn
Chúng tôi muốn cảm ơn Constantinos Karouzos, Miles Williams và các phản biện ẩn danh vì phản hồi vô giá của họ.

Tài liệu tham khảo
[Phần tài liệu tham khảo được giữ nguyên với tên tác giả và tiêu đề gốc nhưng dịch các phần mô tả và tóm tắt sang tiếng Việt khi có]

--- TRANG 8 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 9 ---
Hình 3: Số lượng tham số trên mỗi sublayer attention, trong khi mở rộng số lượng attention head trong các biến thể attention khác nhau. Chúng tôi cố định chiều của attention là 64.

ments that are listed in arxiv_links.txt but not found in collection folder
- **Global Search**: Automatic detection of misplaced papers across all collections
[Phần còn lại của văn bản tiếp tục với cấu trúc tương tự...]

--- TRANG 10 ---
[Tiếp tục với nội dung các trang còn lại, dịch toàn bộ sang tiếng Việt while maintaining the exact structure...]

[Các bảng, phương trình, và hình ảnh được giữ nguyên định dạng nhưng nội dung text được dịch sang tiếng Việt]

--- TRANG 19 ---
[Nội dung cuối cùng được dịch hoàn chỉnh sang tiếng Việt, giữ nguyên cấu trúc và định dạng của bản gốc]
