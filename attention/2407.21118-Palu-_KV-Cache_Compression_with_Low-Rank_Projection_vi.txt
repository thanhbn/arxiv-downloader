# Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2407.21118.pdf
# Kích thước tệp: 6933056 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
PALU: NÉN KV-CACHE VỚI PHÉP CHIẾU 
HẠNG THẤP
Chi-Chih Chang1,3∗†Wei-Cheng Lin1∗Chien-Yu Lin2∗
Chong-Yan Chen1Yu-Fang Hu1Pei-Shuo Wang1Ning-Chi Huang1
Luis Ceze2Mohamed S. Abdelfattah3Kai-Chiang Wu1
1Đại học Quốc gia Yang Ming Chiao Tung2Đại học Washington3Đại học Cornell
TÓM TẮT
Các phương pháp nén KV-Cache sau huấn luyện thường hoặc lấy mẫu một tập con các token hiệu quả hoặc lượng tử hóa dữ liệu thành độ rộng bit số thấp hơn. Tuy nhiên, các phương pháp này không thể khai thác sự dư thừa trong chiều ẩn của các tensor KV. Bài báo này trình bày một phương pháp nén chiều ẩn được gọi là Palu, một khung nén KV-Cache sử dụng phép chiếu hạng thấp để giảm việc sử dụng bộ nhớ LLM trong thời gian suy luận. Palu phân tích các lớp tuyến tính thành các ma trận hạng thấp, lưu trữ các trạng thái trung gian đã nén, và tái tạo các khóa và giá trị đầy đủ một cách tức thì. Để cải thiện độ chính xác, tỷ lệ nén và hiệu quả, Palu bao gồm thêm (1) một sơ đồ phân tách hạng thấp hạt trung bình, (2) một thuật toán tìm kiếm hạng hiệu quả, (3) các cải tiến tương thích lượng tử hóa nhận biết hạng thấp, và (4) các kernel GPU được tối ưu hóa với việc hợp nhất toán tử. Các thí nghiệm mở rộng với các LLM phổ biến cho thấy Palu nén KV-Cache 50%, trong khi duy trì độ chính xác mạnh và mang lại tăng tốc lên đến 1.89× trên mô-đun attention dựa trên RoPE. Khi kết hợp với lượng tử hóa, thiết kế thân thiện với lượng tử hóa vốn có của Palu mang lại sự giảm độ chính xác nhỏ đến không đáng kể, trong khi tiết kiệm bộ nhớ bổ sung hơn các phương pháp chỉ lượng tử hóa và đạt được tăng tốc lên đến 2.91× cho attention dựa trên RoPE. Hơn nữa, nó duy trì độ chính xác tương đương hoặc thậm chí tốt hơn (lên đến 1.19 perplexity thấp hơn) so với các phương pháp chỉ lượng tử hóa. Những kết quả này chứng minh khả năng vượt trội của Palu trong việc giải quyết hiệu quả các thách thức về hiệu quả và bộ nhớ của suy luận LLM do KV-Cache gây ra. Mã của chúng tôi có sẵn công khai tại: https://github.com/shadowpa0327/Palu

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã đẩy AI vào các ứng dụng và khả năng mới, cung cấp một trí thông minh cấp cao mà các mô hình học máy (ML) trước đây không thể đạt được. Để tăng tốc độ suy luận, việc lưu trữ các trạng thái Key-Value (KV-Cache) trong bộ nhớ là một kỹ thuật đơn giản nhưng hiệu quả. Tuy nhiên, kích thước của KV-Cache có thể tăng nhanh chóng, gây căng thẳng cho dung lượng và băng thông bộ nhớ đặc biệt với độ dài ngữ cảnh dài (Fu, 2024); hơn nữa, bản chất bị giới hạn bởi bộ nhớ của giai đoạn giải mã hạn chế tốc độ suy luận khi tải dữ liệu KV-Cache (Gholami et al., 2024). Do đó, nén KV-Cache đã trở thành một chủ đề nghiên cứu trung tâm để chạy LLM một cách hiệu quả.

Mặc dù các cơ chế attention mới nổi như Multi-Query Attention (MQA) (Shazeer, 2019), Group-Query Attention (GQA) (Ainslie et al., 2023) và Multi-head Latent Attention (MLA) (DeepSeek-AI et al., 2024) có thể giảm kích thước KV-Cache, nó hoặc yêu cầu tiền huấn luyện mô hình hoặc có tác động đáng kể đến độ chính xác của mô hình khi chuyển đổi từ Multi-Head Attention (MHA) truyền thống (Chen et al., 2024). Ngược lại, các kỹ thuật nén KV-Cache sau huấn luyện cung cấp một phương pháp thay thế để thúc đẩy hiệu quả cho các mô hình hiện có. Trong số các phương pháp nén KV-Cache khác nhau, lượng tử hóa (Liu et al., 2024b; Hooper et al., 2024) và loại bỏ token (Zhang et al., 2024; Xiao et al., 2024) nổi bật như các chiến lược hiệu quả để giảm dấu chân bộ nhớ của KV-Cache.

∗Đóng góp bằng nhau
†Liên hệ với: Chi-Chih Chang, cc2869@cornell.edu
1arXiv:2407.21118v2  [cs.AI]  4 Nov 2024

--- TRANG 2 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
Các phương pháp lượng tử hóa nhằm giảm độ rộng bit được sử dụng để biểu diễn từng phần dữ liệu, trong khi các kỹ thuật loại bỏ token tập trung vào việc giữ lại một tập con của KV-Cache. Tuy nhiên, cả hai phương pháp đều bỏ qua các chiều ẩn của KV-Cache, nơi thường tồn tại sự dư thừa đáng kể. Để tận dụng tiềm năng chưa được khai thác này, chúng tôi giới thiệu Palu, một khung nén KV-Cache sau huấn luyện tận dụng phép chiếu hạng thấp để giảm chiều ẩn của các tensor KV, cung cấp một chiều nén bổ sung và trực giao với các phương pháp lượng tử hóa và loại bỏ token hiện có.

Một cách ngây thơ để sử dụng phép chiếu hạng thấp cho việc nén KV-Cache là bằng cách ánh xạ trực tiếp các ma trận được lưu trữ vào không gian hạng thấp (Jolliffe & Cadima, 2016; Zhao et al., 2024). Tuy nhiên, phương pháp này áp đặt một chi phí không thể chấp nhận được trong việc tính toán các ma trận phân tách trong thời gian chạy. Để tránh điều này, Palu phân tách tĩnh các ma trận trọng số phép chiếu Key và Value và lưu trữ các biểu diễn tiềm ẩn của phép phân tách hạng thấp (xem Hình 1). Thiết kế sáng tạo này cho phép Palu giảm bộ nhớ trong khi giảm thiểu chi phí thời gian chạy của phép phân tách hạng thấp KV-Cache.

ABWXHYOriginal KVDecompose(Offline)Cache Hinstead of Y

Hình 1: Phương pháp phép chiếu hạng thấp của Palu cho việc giảm KV-Cache. Một ma trận trọng số W của phép chiếu tuyến tính được phân tách thành hai ma trận hạng thấp. Đầu vào X được chiếu xuống thành biểu diễn tiềm ẩn H, được lưu trữ. Y có thể được tái tạo từ H bằng cách sử dụng ma trận up-projection B.

Trong việc thiết kế một chiến lược phân tách hiệu quả cho các mô-đun attention với nhiều đầu attention, chúng tôi quan sát một sự đánh đổi rõ ràng giữa độ chính xác và chi phí tái tạo. Phân tách các ma trận phép chiếu trên tất cả các đầu attention cùng nhau cải thiện độ chính xác bằng cách bảo tồn thông tin toàn cục, nhưng phương pháp này làm tăng đáng kể chi phí tái tạo. Mặt khác, phân tách từng đầu riêng biệt giảm chi phí tái tạo nhưng dẫn đến mất độ chính xác cao hơn. Để giải quyết điều này, Palu giới thiệu một phép phân tách hạng thấp theo nhóm đầu hạt trung bình tạo ra sự cân bằng giữa độ chính xác và hiệu quả tái tạo.

Đối với LLM, mỗi mô-đun phép chiếu tuyến tính có độ nhạy cảm khác nhau với nén (Sharma et al., 2023; Yuan et al., 2023). Để khai thác độ nhạy cảm và cải thiện độ chính xác, chúng tôi thiết kế một thuật toán tìm kiếm hạng hiệu quả dựa trên thông tin Fisher (Ly et al., 2017; Liu et al., 2021). Thuật toán của chúng tôi tự động gán hạng cao hơn cho các ma trận quan trọng và hạng thấp hơn cho những ma trận ít quan trọng hơn, tăng cường độ chính xác ở cùng tỷ lệ nén KV-Cache tổng thể.

Ngoài việc phân tách hạng thấp, Palu tương thích với các kỹ thuật lượng tử hóa. Chúng tôi phát hiện rằng phép phân tách hạng thấp có thể gây ra các giá trị ngoại lai nghiêm trọng trong biểu diễn tiềm ẩn, điều này cản trở đáng kể việc lượng tử hóa chính xác bit thấp. Mặc dù phép biến đổi Hadamard đã được chứng minh là hiệu quả cho việc loại bỏ giá trị ngoại lai trong các nghiên cứu gần đây (Tseng et al., 2024; Ashkboos et al., 2024b; Liu et al., 2024a; Chiang et al., 2024), việc tích hợp nó thường gây ra chi phí tính toán trong thời gian chạy. Tuy nhiên, cấu trúc cặp ma trận vốn có của Palu làm cho nó có tính tương thích cao với kỹ thuật này, cho phép các ma trận biến đổi được hợp nhất một cách liền mạch vào các ma trận thuận và nghịch, hiệu quả giảm thiểu các giá trị ngoại lai mà không ảnh hưởng đến hiệu quả thời gian chạy.

Chúng tôi đánh giá Palu trên các LLM và benchmark được sử dụng rộng rãi. Các thí nghiệm của chúng tôi chứng minh rằng Palu duy trì độ chính xác zero-shot mạnh và perplexity với nén hạng thấp lên đến 50%. Hơn nữa, khi kết hợp nén hạng thấp với lượng tử hóa, Palu đạt được một tỷ lệ nén ấn tượng trên 91.25% (giảm 11.4×) và mang lại perplexity thấp hơn đáng kể là 1.19 so với KVQuant (Hooper et al., 2024), một phương pháp lượng tử hóa KV-Cache hiện đại, chỉ đạt được tỷ lệ nén 87.5%.

Đối với đánh giá độ trễ, dưới tỷ lệ nén KV-Cache 50% không có lượng tử hóa, Palu chứng minh tăng tốc lên đến 1.89× và 2.2× cho các mô-đun attention dựa trên RoPE và không RoPE. Khi tích hợp với lượng tử hóa, Palu đạt được tăng tốc lên đến 2.91× và 6.17× trên attention dựa trên RoPE và không RoPE, tương ứng. Những kết quả này nhấn mạnh khả năng của Palu trong việc giảm đáng kể dấu chân bộ nhớ KV-Cache trong khi tăng cường hiệu quả suy luận cho LLM.

2

--- TRANG 3 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
Các đóng góp chính của chúng tôi bao gồm:
• Palu, một khung nén KV-Cache sau huấn luyện mới lưu trữ các biểu diễn tiềm ẩn hạng thấp của các trạng thái Key và Value.
• Phép phân tách hạng thấp theo nhóm đầu (G-LRD), một tối ưu hóa để cân bằng độ chính xác và hiệu quả tái tạo.
• Một thuật toán tìm kiếm hạng tự động để gán hạng thích ứng cho mỗi ma trận được phân tách, với tỷ lệ nén mục tiêu.
• Một tối ưu hóa tương thích lượng tử hóa được đồng thiết kế loại bỏ các giá trị ngoại lai do hạng thấp gây ra và không áp đặt chi phí thời gian chạy.

2 NỀN TẢNG
2.1 CƠ CHẾ MULTI-HEAD ATTENTION
Cơ chế multi-head attention (MHA) (Vaswani et al., 2017) là một thành phần cốt lõi của kiến trúc transformer. Cho một token đầu vào mới x∈Rd, một MHA với n đầu chiếu đầu vào vào nhiều query, key và value sử dụng các ma trận trọng số Wq_i, Wk_i và Wv_i, tương ứng, cho mỗi đầu i, như được thể hiện bởi

qi = xWq_i, ki = xWk_i, vi = xWv_i. (1)

Ở đây, ki và vi biểu diễn key và value tại bước thời gian t cho đầu i. Chúng ta sau đó có thể tính điểm attention cho mỗi đầu i và đầu ra attention tương ứng là

pt,i = Softmax(qiKT_i/√dh), ai = piVi, (2)

trong đó Ki và Vi biểu thị sự nối các key và value hiện tại và tất cả trước đó tương ứng với đầu thứ i. Đầu ra MHA cuối cùng được thu được bằng cách nối các đầu ra của tất cả các đầu và sau đó áp dụng lớp out-projection Wo, như được thể hiện bởi

MHA(x) = Σ(i=1 to h) aiWo_i = Σ(i=1 to h) (piVi)Wo_i, (3)

trong đó Wo_i ∈ Rdh×d biểu diễn các ma trận con của ma trận out-projection cho mỗi đầu i.

2.2 PHÂN TÁCH GIÁ TRỊ ĐƠN (SVD)
SVD (Jolliffe & Cadima, 2016) là một kỹ thuật thường được sử dụng để tính toán xấp xỉ hạng thấp cho một ma trận cho trước. Chúng tôi bây giờ giới thiệu việc sử dụng cụ thể SVD cho phương pháp phân tách hạng thấp mặc định của Palu.

Cho một ma trận trọng số W∈Rm×n, SVD phân tách W thành ba ma trận: W = UΣVT. Ở đây, U và V là các ma trận trực giao chứa các vector đơn trái và phải, tương ứng. Ma trận Σ là một ma trận chéo bao gồm các giá trị đơn. Chúng tôi mô tả phép phân tách là

W ≈ AB, A = Ur√Σr, B = √ΣrVT_r,

trong đó A∈Rm×r, B∈Rr×n, Σr∈Rr×r. Σr là một ma trận chéo chứa r giá trị đơn lớn nhất, và Ur, VT_r là các vector đơn tương ứng được cắt từ U và VT. Việc cắt này và hình thành ma trận tiếp theo cho phép chúng ta xấp xỉ ma trận trọng số W với hai ma trận hạng thấp A và B, do đó giảm lưu trữ bằng mr+rn/mn.

3 KHUNG PALU
3.1 NÉN KV-CACHE QUA PHÉP CHIẾU HẠNG THẤP
Để áp dụng phép chiếu hạng thấp hiệu quả hơn việc phân tách trực tiếp KV-Cache trong thời gian chạy, Palu sử dụng SVD để phân tách các ma trận phép chiếu Key và Value. Phương pháp này dựa trên

3

--- TRANG 4 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Hình 2: Palu sử dụng phép phân tách hạng thấp (W ≈ AB) để chiếu key (hoặc value) vào một biểu diễn tiềm ẩn chiều thấp hơn (h), do đó giảm kích thước của KV-Cache. Key ban đầu (Kt) được tái tạo một cách tức thì với Bk, và Bv được hợp nhất vào Wo để tránh chi phí tái tạo. Việc hợp nhất cũng giảm gánh nặng tính toán cho phép chiếu đầu ra.

quan sát rằng phép phân tách hạng thấp viết lại lớp phép chiếu tuyến tính từ y = xW thành y = xAB. Ở đây, A∈Rd×r là ma trận phép chiếu hạng thấp, và B∈Rr×d là ma trận tái tạo được suy ra bởi SVD. Quá trình thuận đầu tiên chiếu xuống token đầu vào x∈Rd vào một không gian tiềm ẩn chiều thấp h∈Rr và sau đó chiếu ngược lên không gian ban đầu:

h = Ax, y = Bh (4)

Quá trình hai bước này cho phép Palu (1) lưu trữ biểu diễn tiềm ẩn chiều thấp hơn thay vì các trạng thái key và value ban đầu, và (2) tái tạo chúng trong quá trình giải mã.

3.1.1 TÍCH HỢP VỚI CƠ CHẾ ATTENTION VÀ HỢP NHẤT MA TRẬN NGOẠI TUYẾN

Chúng tôi bây giờ mô tả cách Palu phân tách các lớp tuyến tính key và value cho cơ chế attention. Đối với mỗi đầu attention i, Palu áp dụng SVD và ánh xạ ma trận key-projection Wk_i và ma trận value-projection Wv_i thành Ak_iBk_i và Av_iBv_i.

Dựa trên công thức đầu ra attention trong Phương trình 2, Palu hấp thụ ma trận tái tạo Bv_i vào ma trận output projection Wo_i ngoại tuyến:

aiWo_i = (piVi)Wo_i = (piHv_iBv_i)Wo_i = piHv_i(Bv_iWo_i) (5)

Việc hợp nhất như vậy cho phép Palu bỏ qua việc tái tạo rõ ràng các vector value đầy đủ, giảm số lượng phép nhân ma trận và cải thiện hiệu quả. Một phương pháp tương tự áp dụng cho việc tính toán điểm attention. Ma trận Bk_i có thể được hợp nhất vào ma trận query projection Wq_i ngoại tuyến, như được thể hiện bởi

qiKT_i = qi(Hk_iBk_i)T = xtWq_i(Bk_i)T(Hk_i)T = xt[Wq_i(Bk_i)T](Hk_i)T. (6)

Ở đây, Bk_i∈Rr×dh và Wq_i∈Rd×dh, vì vậy ma trận hợp nhất (Wq_i(Bk_i)T) có kích thước Rd×r. Việc hợp nhất này tăng cường hiệu quả tính toán bằng cách giảm chiều ma trận trong quá trình tính toán điểm attention.

3.1.2 TƯƠNG THÍCH VỚI EMBEDDING VỊ TRÍ

Các LLM gần đây, như họ Llama, áp dụng Rotary Positional Embedding (tức là RoPE (Su et al., 2021)) lên các trạng thái Query và Key trước khi nhân chúng. Bản chất phi tuyến của các embedding vị trí này ngăn cản việc hợp nhất ma trận của các điểm attention, như được nêu trong Phương trình 6. Để giải quyết điều này, Palu tái tạo động các key từ các biểu diễn tiềm ẩn trong quá trình giải mã. Chúng tôi tăng cường hiệu quả tái tạo với một kernel GPU được thiết kế tùy chỉnh, được chi tiết trong Mục 4.1 và được mô tả thêm trong Phụ lục B.

Lưu ý rằng đối với một số phương pháp embedding vị trí, như ALiBi (Press et al., 2022), hoặc các cơ chế attention mới, như MLA (DeepSeek-AI et al., 2024), embedding vị trí không được áp dụng trực tiếp lên các trạng thái Key. Do đó, việc hợp nhất được mô tả trong Phương trình 6 vẫn có hiệu lực. Đối với những

4

--- TRANG 5 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
mô-đun attention không RoPE này, Palu đạt được tăng tốc lớn hơn so với attention dựa trên RoPE, vì việc tái tạo của chúng có thể được tránh với việc hợp nhất ma trận. Một quy trình làm việc chi tiết của Palu trong bối cảnh attention dựa trên RoPE của Llama được minh họa trong Hình 2.

3.2 MỨC ĐỘ CHI TIẾT PHÂN TÁCH
3.2.1 PHÉP PHÂN TÁCH HẠNG THẤP MULTI-HEAD
Chúng tôi đặt tên cho sơ đồ phân tách theo đầu trong Mục 3.1.1 là phép phân tách hạng thấp multi-head (M-LRD). Chúng tôi phát hiện M-LRD thường gây ra sự giảm độ chính xác không thể bỏ qua (được thảo luận thêm trong Mục 4.2), có thể do SVD không nắm bắt được thông tin chung được chia sẻ giữa các đầu. Do đó, các phương pháp thay thế cần thiết để bảo tồn độ chính xác mô hình.

3.2.2 PHÉP PHÂN TÁCH HẠNG THẤP JOINT-HEAD
Một phương pháp thay thế là phân tách chung các ma trận trọng số cho tất cả các đầu. Bằng cách xem xét ma trận trọng số kết hợp Wjoint = [W1, W2, ..., Wn]∈Rd×(dh·nh), chúng ta có thể thực hiện một phép phân tách hạng thấp duy nhất Wjoint ≈ AjointBjoint, trong đó Ajoint∈Rd×rjoint và Bjoint∈Rrjoint×(dh·nh). Chúng tôi gọi sơ đồ này là phép phân tách hạng thấp joint-head (J-LRD).

J-LRD có ưu điểm bảo tồn các thành phần chính chung được chia sẻ giữa các đầu khác nhau. Điều này xảy ra vì SVD đặc biệt hiệu quả trong việc nắm bắt các thành phần chiếm ưu thế khi được áp dụng cho một ma trận lớn hơn, kết hợp, dẫn đến một xấp xỉ chính xác hơn.

Đối với J-LRD, biểu diễn tiềm ẩn chung được chia sẻ giữa tất cả các đầu có thể được tính toán với hjoint = xAjoint. Trong quá trình giải mã, các trạng thái ban đầu cho mỗi đầu có thể được tái tạo qua
[y1, ..., yn] = hjointBjoint.

Chi phí Suy luận Cao. Mặc dù bảo tồn độ chính xác mô hình tốt hơn, J-LRD gây ra chi phí tính toán và bộ nhớ đáng kể trong quá trình giải mã. Cụ thể, tổng số phép toán dấu phẩy động (FLOP) để tái tạo trạng thái Key hoặc Value của một đầu bây giờ trở thành rjoint·dh·n. Giả sử cùng kích thước như tổng các biểu diễn tiềm ẩn hạng thấp (tức là rjoint = Σri), tổng chi phí tái tạo cao hơn M-LRD n lần, mà tổng FLOP của nó là ri·dh·n. Khi xem xét việc hợp nhất ma trận trong Mục 3.1.1, ma trận hợp nhất của J-LRD có kích thước rjoint·d·n, cũng lớn hơn M-LRD n lần, dẫn đến tiêu thụ bộ nhớ cao hơn đáng kể.

Hình 3: Thực hiện phân tách ở các mức độ chi tiết khác nhau. Phân tách chung nhiều đầu có thể đạt được độ chính xác cao hơn. Giả sử cùng tổng kích thước của các biểu diễn tiềm ẩn (tức là 4·ri = 2·rg = rjoint), FLOP cho chi phí tái tạo trong các sơ đồ phân tách joint-head lớn hơn 4 lần so với những sơ đồ multi-head.

5

--- TRANG 6 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
3.2.3 PHÉP PHÂN TÁCH HẠNG THẤP GROUP-HEAD
Để cân bằng sự đánh đổi giữa độ chính xác và chi phí tái tạo, chúng tôi đề xuất phép phân tách hạng thấp group-head (G-LRD). G-LRD phân tách các ma trận cho một nhóm các đầu cùng nhau. Với các ma trận trọng số kết hợp, nó nắm bắt thông tin được chia sẻ trong mỗi nhóm trong khi hạn chế chi phí tính toán và bảo tồn độ chính xác.

Để minh họa quá trình G-LRD, hãy xem xét các ma trận trọng số cho một nhóm s đầu, Wgj = [Wj,1...Wj,s], trong đó Wgj∈Rd×(dh·s). Chúng ta phân tách hạng thấp Wgj ≈ AgjBgj, trong đó Agj∈Rd×rg và Bgj∈Rrg×(dh·s). Biểu diễn tiềm ẩn được chia sẻ giữa các đầu attention trong cùng nhóm có thể được tính toán là hgj = xUgj. Trong quá trình giải mã, key hoặc value ban đầu cho mỗi đầu có thể được tái tạo qua
[yj,1...yj,s] = hgjBgj.

FLOP để tái tạo các key và value cho mỗi đầu trong G-LRD là rg·dh·ng, trong đó ng = n/s là số nhóm. So sánh chi phí với J-LRD và giả sử cùng tổng kích thước hạng (rg·ng = rjoint), G-LRD giảm chi phí tái tạo bằng ng. Tương tự, G-LRD cũng giảm kích thước ma trận hợp nhất bằng ng. Tóm lại, G-LRD cung cấp một điểm trung gian giữa chi phí tính toán và độ chính xác xấp xỉ. Chúng tôi minh họa M-LRD, J-LRD và G-LRD trong Hình 3. Vui lòng tham khảo Phụ lục C để thảo luận thêm về chi phí của các mức độ chi tiết phân tách khác nhau.

3.3 PHÂN BỔ HẠNG TỰ ĐỘNG
Để phân bổ kích thước hạng lý tưởng cho mục tiêu phân tách, việc ước tính chính xác tầm quan trọng của ma trận mục tiêu (ví dụ, trọng số được nhóm) là rất quan trọng. Trong Palu, chúng tôi xác định thông tin Fisher (Ly et al., 2017; Liu et al., 2021) như một xấp xỉ chính xác vì nó có thể định lượng lượng thông tin cho mỗi tham số. Sau đó chúng tôi sử dụng tổng thông tin Fisher để ước tính tầm quan trọng của ma trận trọng số của mỗi lớp tuyến tính (Abdelfattah et al., 2021).

Giả sử rằng độ nhạy cảm nén tỷ lệ với thông tin Fisher, chúng tôi xác định hạng cho mỗi ma trận trọng số bằng cách tính tỷ lệ thông tin Fisher của nó với tổng thông tin Fisher trên tất cả các mục tiêu phân tách. Chúng tôi sử dụng tỷ lệ này để phân bổ tỷ lệ nén (tức là mức hạng r), đảm bảo rằng các lớp quan trọng hơn giữ lại mức hạng cao hơn. Để có một nghiên cứu loại bỏ chi tiết về Phân bổ Hạng Tự động của chúng tôi, vui lòng tham khảo Phụ lục G.3.

3.4 TƯƠNG THÍCH LƯỢNG TỬ HÓA

(a) Low-Rank Key Cache(b) Low-Rank Key Cache (với Hadamard)Khó lượng tử hóaDễ lượng tử hóa

Hình 4: Phân bố kích hoạt của các low-rank key cache tại lớp attention thứ 4 của Llama-2.

Chúng tôi tích hợp lượng tử hóa vào Palu để nén KV-Cache thêm nữa. Chúng tôi quan sát rằng các biểu diễn tiềm ẩn nén hạng thấp có các giá trị ngoại lai nghiêm trọng, điều này hạn chế khả năng áp dụng lượng tử hóa trong Palu. Không giống như các giá trị ngoại lai tự nhiên được mô tả trong tài liệu lượng tử hóa KV-Cache trước đây (Liu et al., 2024b; Hooper et al., 2024), những giá trị ngoại lai này được gây ra bởi phép phân tích hạng thấp dựa trên SVD.

Hình 4 (a) hiển thị phân bố của các trạng thái key nén hạng thấp từ một lớp của Llama-2 với G-LRD. Các mẫu giá trị ngoại lai lặp lại xuất hiện ở đầu mỗi nhóm được phân tách vì SVD sắp xếp các eigenvalue lớn hơn trong các hàng hoặc cột ban đầu, dẫn đến các giá trị giảm nhanh trong biểu diễn tiềm ẩn. Mẫu này kéo dài phân bố dữ liệu và làm tổn hại độ chính xác lượng tử hóa. Được truyền cảm hứng từ tài liệu lượng tử hóa LLM gần đây (Ashkboos et al., 2024b; Tseng et al., 2024), chúng tôi áp dụng phép biến đổi Walsh-Hadamard (WHT, Fino & Algazi) để loại bỏ các giá trị ngoại lai (Hình 4 (b)), cho phép độ chính xác lượng tử hóa cao. Tuy nhiên, phép biến đổi này gây ra một phép nhân ma trận bổ sung với chi phí thời gian chạy liên quan. Không giống như các phương pháp trước đây (Ashkboos et al., 2024b) phải áp dụng WHT trực tuyến khi lượng tử hóa KV-Cache, chúng tôi tối ưu hóa quá trình này bằng cách tích hợp ma trận Hadamard vào các trọng số phân tách hạng thấp mà không có chi phí tính toán bổ sung, như được mô tả bởi

6

--- TRANG 7 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

W ≈ AB = (AR)(RTB) = ÂB̂, (7)

trong đó R là ma trận Hadamard. Tối ưu hóa này cho phép Palu tích hợp kỹ thuật nén hạng thấp được đề xuất với lượng tử hóa bit thấp. Các thí nghiệm của chúng tôi cho thấy rằng, ngoài việc nén hạng thấp, phương pháp lượng tử hóa của chúng tôi chỉ tăng perplexity một cách không đáng kể, ngay cả ở mức cực kỳ như 3-bit hoặc 2-bit với một sơ đồ lượng tử hóa đơn giản theo token (xem Mục 4.3).

4 THÍ NGHIỆM
4.1 THIẾT LẬP THÍ NGHIỆM
Mô hình và Nhiệm vụ. Chúng tôi đánh giá Palu trên bốn họ LLM, Llama-2 (Touvron et al., 2023), Llama-3 (Dubey et al., 2024), Mistral (Jiang et al., 2023) và LongChat (Li et al., 2023). Để đánh giá độ chính xác, chúng tôi đo perplexity trên các tập dữ liệu WikiText-2 (Merity et al., 2016) và C4 (Raffel et al., 2020) và sử dụng LM-Evaluation-Harness (Gao et al., 2023) để đo độ chính xác zero-shot trên sáu nhiệm vụ common sense. Chúng tôi cũng đánh giá độ chính xác ngữ cảnh dài trên 16 nhiệm vụ trong LongBench (Bai et al., 2023). Trừ khi có chỉ định, chúng tôi đề cập đến baseline là một mô hình với KV-Cache không nén. Xem Phụ lục H để biết thêm chi tiết về tập dữ liệu và cài đặt.

Cài đặt Nén. Chúng tôi triển khai Palu dựa trên thư viện Huggingface (Wolf et al., 2020). Phép phân tách các lớp phép chiếu Key và Value được thực hiện bằng phương pháp SVD nhận biết cắt được đề xuất bởi SVD-LLM (Wang et al., 2024). Trừ khi có chỉ định khác, kết quả của Palu là G-LRD với kích thước nhóm 4 (gs-4), với kích thước hạng bằng nhau cho mỗi nhóm. Để tính toán thông tin Fisher trong tìm kiếm hạng, chúng tôi sử dụng 2048 mẫu ngẫu nhiên từ Wikitext-2, mỗi mẫu có độ dài chuỗi 1024. Để tích hợp lượng tử hóa trong Palu, chúng tôi sử dụng lượng tử hóa số nguyên bất đối xứng đơn giản theo token. Để đánh giá kết quả lượng tử hóa, chúng tôi so sánh Palu với các phương pháp lượng tử hóa KV-Cache tiên tiến, bao gồm Atom (Zhao et al., 2023), KVQuant (Hooper et al., 2024), và KIVI (Liu et al., 2024b). Tham khảo Mục 5 để có tóm tắt ngắn gọn về các phương pháp này.

Triển khai GPU Kernel. Để tính toán hiệu quả điểm attention với RoPE trong Palu, chúng tôi triển khai một kernel tùy chỉnh trong Triton (Tillet et al., 2019). Kernel của chúng tôi hợp nhất việc tái tạo key, áp dụng RoPE, và phép nhân cuối cùng với query trong một lời gọi kernel duy nhất, giảm thiểu di chuyển dữ liệu (Xem Phụ lục B). Để tích hợp lượng tử hóa, chúng tôi triển khai các kernel trong CUDA cho các trường hợp hợp nhất ma trận trên cả điểm attention và đầu ra attention (tham khảo Mục 3.1.1 và Hình 2). Kernel của chúng tôi hợp nhất quá trình giải lượng tử hóa và phép nhân theo sau với các key hoặc value nén hạng thấp, cho phép xử lý hiệu quả trên KV-Cache tiềm ẩn được lượng tử hóa. Khi đánh giá tăng tốc với lượng tử hóa, chúng tôi so sánh với baseline không nén và KIVI (Liu et al., 2024b), mà chúng tôi sử dụng mã chính thức† trong các thí nghiệm của chúng tôi.

4.2 KẾT QUẢ VỚI CÁC MỨC ĐỘ CHI TIẾT PHÂN TÁCH KHÁC NHAU
Chúng tôi đánh giá perplexity và độ chính xác zero-shot của Palu với tỷ lệ nén hạng thấp 50% sử dụng M-LRD, G-LRD, và J-LRD trên Llama-2-7B và Llama-3-8B-Instruct, và trình bày kết quả trong Bảng 1.

Bảng 1: Perplexity và độ chính xác zero-shot của Palu ở tỷ lệ nén 50%.
Model MethodPerplexity ↓ Zero-Shot Accuracy (%) ↑
Wiki2 C4 OBQA Hella PIQA ARC-e ARC-c Wino Avg.
Llama-2-7BBaseline 5.47 7.26 44.20 76.00 78.07 76.30 46.42 69.30 65.05
J-LRD 5.62 7.75 45.40 75.57 77.48 75.97 45.31 69.22 64.82
G-LRD 6.01 9.82 43.60 73.39 76.33 73.02 42.57 66.77 62.61
M-LRD 6.75 12.01 39.60 65.35 74.76 67.17 35.24 64.64 57.79
Llama-3-8B-InstBaseline 8.28 13.01 43.20 75.80 78.62 81.61 56.83 71.90 67.99
J-LRD 9.12 15.90 43.40 73.20 76.50 79.63 51.96 72.45 66.19
G-LRD 10.11 17.87 42.60 70.36 76.06 76.30 48.99 72.38 64.45
M-LRD 12.38 23.02 38.80 63.04 73.67 69.78 42.58 62.51 58.40

†https://github.com/jy-yuan/KIVI

7

--- TRANG 8 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
Đánh giá Perplexity. Như Bảng 1 cho thấy, đối với mô hình Llama-2-7B, phương pháp M-LRD của Palu không thể duy trì perplexity thấp ở tỷ lệ nén 50%. Ngược lại, mặc dù có chi phí tính toán lại cao, J-LRD vượt trội đáng kể so với M-LRD và đạt được perplexity 5.62 trên WikiText-2.

Đối với G-LRD, vẫn duy trì chi phí tính toán thấp, mang lại perplexity 6.01 trên Wikitext-2, cho thấy sự cân bằng tuyệt vời giữa độ chính xác mô hình và chi phí nén. Xu hướng tương tự cũng được quan sát trong mô hình Llama-3-8B. Kết quả thêm về Llama-2-13B có thể được tìm thấy trong Phụ lục F.

Kết quả Đánh giá Zero-shot. Tương tự như đánh giá perplexity, phương pháp J-LRD chứng minh hiệu suất tốt nhất cho độ chính xác zero-shot trên Llama-2-7B, chỉ với 0.23% giảm độ chính xác trung bình. Phương pháp M-LRD dẫn đến hiệu suất trung bình thấp nhất, với 7.26% giảm độ chính xác so với baseline. So sánh, G-LRD chỉ có 2.4% giảm độ chính xác trung bình, cung cấp một điểm ngọt ngào giữa độ chính xác mô hình và chi phí nén một lần nữa.

4.3 KẾT QUẢ TÍCH HỢP LƯỢNG TỬ HÓA

Bảng 2: Perplexity lượng tử hóa và kích thước KV-Cache cho Llama-2-7B trên WikiText-2. Đối với perplexity, độ dài chuỗi là 4096. Kích thước KV-Cache được thể hiện cho độ dài chuỗi 128K.
Method Bit PPLKV-Cache Size (GB)Comp. Rate
Baseline 16 5.12 64.0 -
Palu-30% 16 5.25 44.8 30%
Palu-50% 16 5.63 32.0 50%
Atom 3 6.15 12.6 80.32%
KVQuant 3 5.35 12.0 81.25%
Palu-30% 3 5.33 8.4 86.87%
Palu-50% 3 5.77 6.0 90.63%
Atom 2 117.88 8.6 86.56%
KVQuant 2 6.95 8.0 87.50%
Palu-30% 2 5.76 5.6 91.25%
Palu-50% 2 6.41 4.0 93.75%

Bảng 2 thể hiện tác động của lượng tử hóa lên perplexity và kích thước KV-Cache khi kết hợp với Palu. Với lượng tử hóa 3-bit, Palu chỉ gây ra tăng perplexity nhẹ 0.08 và 0.23 ở tỷ lệ nén hạng thấp 30% và 50%. Những điều này chứng minh sự đánh đổi độ chính xác tối thiểu cho lợi ích nén đáng kể so với baseline 16-bit.

Đáng chú ý, ở lượng tử hóa 2-bit, Palu vượt trội hẳn phương pháp KVQuant hiện đại, giảm perplexity 1.19 và 0.54, trong khi tiếp tục giảm việc sử dụng bộ nhớ 30% và 50%. Những kết quả này thiết lập Palu với lượng tử hóa như một phương pháp nén KV-Cache vượt trội.

4.4 ĐÁNH GIÁ TRÊN CÁC TẬP DỮ LIỆU NGỮ CẢNH DÀI
Để truy cập khả năng của Palu cho các kịch bản ngữ cảnh dài, chúng tôi đánh giá độ chính xác của baseline, KIVI và Palu trên LongBench (Bai et al., 2023). Ở đây chúng tôi đánh giá trên các mô hình Mistral-7B và LongChat-7B, có độ dài ngữ cảnh lên đến 32K. Chúng tôi báo cáo điểm trung bình cho mỗi loại nhiệm vụ riêng biệt, cũng như trung bình tổng thể trên tất cả 16 nhiệm vụ. Kết quả được hiển thị trong Bảng 3.

Như Bảng 3 chỉ ra, chúng tôi phát hiện rằng ở mức nén hạng thấp 50%, Palu tương đối khó để bảo tồn hoàn toàn độ chính xác. Tuy nhiên, ở mức nén 30%, Palu đạt được chỉ một sự giảm độ chính xác trung bình nhỏ (<1%) so với baseline cho cả hai mô hình.

Hơn nữa, Palu có thể lượng tử hóa KV-Cache tiềm ẩn hạng thấp xuống 3 bit, với ít hơn 1% giảm độ chính xác thêm. Tổng thể, Palu duy trì độ chính xác trung bình mạnh 40.77% và 34.33% cho Mistral-7B và LongChat-7B, với tỷ lệ nén ấn tượng 7.59x.

Khi so sánh với KIVI (Liu et al., 2024b), Palu đạt được độ chính xác tương tự, trong khi có thêm 30% tỷ lệ nén từ hạng thấp.

Đáng chú ý, Palu không yêu cầu các kỹ thuật lượng tử hóa nhóm phức tạp và độ chính xác hỗn hợp được sử dụng bởi KIVI, dẫn đến hiệu quả suy luận cao (xem Mục 4.5 để biết chi tiết).

4.5 ĐÁNH GIÁ ĐỘ TRỄ
Trong phần này, chúng tôi cung cấp đánh giá độ trễ và tăng tốc, sử dụng Llama-2-7B làm mô hình cơ sở. Chúng tôi đo độ trễ trên một GPU RTX 4090 duy nhất và so sánh Palu với các baseline FP16 và KIVI-4-bit.

Chúng tôi đánh giá độ trễ của Palu ở tỷ lệ nén 50%, nơi chúng tôi đặt tỷ lệ nén cho key và value lần lượt là 75% và 25%. Phân bổ này dựa trên quan sát của chúng tôi từ kết quả phân bổ hạng (xem Phụ lục G.3 để biết chi tiết).

8

--- TRANG 9 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
Đối với baseline FP16, chúng tôi sử dụng triển khai mặc định từ HuggingFace. Đối với KIVI, chúng tôi sử dụng các kernel CUDA từ kho chính thức của nó. Chúng tôi không áp dụng FlashAttention (Dao et al., 2022) để so sánh táo với táo với các baseline của chúng tôi. Do dung lượng bộ nhớ nhỏ của GPU RTX 4090, chúng tôi áp dụng lượng tử hóa 4-bit (Frantar et al., 2024) cho trọng số của tất cả các lớp tuyến tính. Kết quả của chúng tôi là trung bình của 100 lần chạy.

Bảng 3: Kết quả thí nghiệm trên Longbench. Độ rộng bit trung bình được ước tính cho mỗi phương pháp, giả sử độ dài ngữ cảnh 10K. Mỗi cột biểu diễn điểm trung bình cho các nhiệm vụ của mỗi loại.
Model MethodAvg. Comp. Multi- Single- Summa- Few-Shot Code Synthetic Avg.Bits Ratio QA QA rization
Mistral-7B-v0.2Baseline 16 1.00x 29.63 36.43 28.10 66.71 54.16 44.87 42.54
Palu-30% 16 1.43x 29.83 36.52 27.48 65.70 55.16 37.92 41.55
Palu-50% 16 2.00x 26.92 35.33 26.01 64.04 44.54 16.88 36.23
KIVI-2 3.16 5.05x 28.81 35.07 27.60 66.45 54.47 40.28 41.45
Palu-30% (3 bits) 3 7.59x 29.48 36.40 27.20 65.73 53.19 34.74 40.77
Palu-50% (3 bits) 3 10.6x 26.73 32.72 25.73 63.25 44.43 18.57 35.71
LongChat-7B-v1.5Baseline 16 1.00x 23.95 31.12 26.74 63.80 56.91 15.25 36.32
Palu-30% 16 1.43x 22.42 29.43 25.52 62.87 58.99 14.25 35.45
Palu-50% 16 2.00x 22.61 25.33 22.73 60.12 43.52 6.84 30.82
KIVI-2 3.16 5.06x 23.24 30.19 26.47 63.54 53.51 16.13 35.60
Palu-30% (3 bits) 3 7.59x 23.12 29.21 25.04 61.99 54.38 11.25 34.33
Palu-50% (3 bits) 3 10.6x 18.56 24.14 22.35 58.76 40.50 6.02 29.03

(a) Attention Module (w/ RoPE)(c) End-to-End Model (w/ RoPE)(d) End-to-End Model (w/o RoPE)(b) Attention Module (w/o RoPE)6.17x2.20x1.89x2.91x2.59x5.53x

Hình 5: Tăng tốc chuẩn hóa cho cả mô-đun attention và giải mã mô hình đầu cuối. Các đường liền nét biểu diễn các phép đo chính xác, trong khi các đường đứt nét chỉ ra rằng các baseline FP16 hết bộ nhớ, và tăng tốc được so sánh với độ trễ ước tính của baseline.

4.5.1 TĂNG TỐC CỦA MÔ-ĐUN ATTENTION VÀ GIẢI MÃ ĐẦU CUỐI.
Tăng tốc mô-đun attention. Chúng tôi so sánh độ trễ với attention tiêu chuẩn không có nén hoặc lượng tử hóa và hiển thị tăng tốc của Palu và KIVI-4-bit trong Hình 5 (a) và (b) cho attention dựa trên RoPE và không RoPE. Đối với attention dựa trên RoPE, chúng tôi áp dụng kernel tái tạo trực tuyến của chúng tôi cho key và sử dụng hợp nhất ngoại tuyến cho value như được mô tả trong Mục 3.1.2.

Như được hiển thị trong Hình 5 (a), Palu có tăng tốc tối thiểu đến không có khi độ dài chuỗi ngắn, ví dụ 4K. Tuy nhiên, khi độ dài chuỗi tăng, Palu mang lại lợi ích hiệu suất đáng kể. Ở độ dài đầu vào 64K, Palu đạt được tăng tốc 1.89× so với baseline FP16 khi chỉ sử dụng phép chiếu hạng thấp.

Bằng cách áp dụng thêm lượng tử hóa 4-bit cho các trạng thái Value, tăng tốc tăng lên 2.91× cho cùng độ dài ngữ cảnh 64K, nhờ kernel độ chính xác thấp được tối ưu hóa của chúng tôi và thời gian tải bộ nhớ giảm. Hiệu suất này vượt trội đáng kể so với KIVI-4-bit, chỉ đạt được tăng tốc 1.89× ở 64K, bị cản trở bởi chi phí của lượng tử hóa nhóm fine-grained. Đáng chú ý, đối với attention dựa trên RoPE, Palu-4-bit không lượng tử hóa key, vì kernel tái tạo trực tuyến của chúng tôi hiện chỉ hỗ trợ độ chính xác FP16.

Đối với attention không RoPE, chúng tôi áp dụng hợp nhất ma trận cho cả trạng thái Key và Value (Phương trình 6), hiệu quả loại bỏ tất cả chi phí tái tạo. Ở độ dài chuỗi 64K với tỷ lệ nén 50%, Palu đạt được tăng tốc 2.20× so với baseline FP16. Bằng cách áp dụng thêm lượng tử hóa 4-bit cho cả trạng thái Key và Value, Palu tăng cường tăng tốc lên 6.17× cho độ dài đầu vào 64K. Những

9

--- TRANG 10 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
kết quả này chứng minh rằng việc kết hợp nén hạng thấp và lượng tử hóa tăng cường đáng kể hiệu quả suy luận, đặc biệt trong các kịch bản ngữ cảnh dài.

Tăng tốc đầu cuối. Chúng tôi trình bày tăng tốc đầu cuối trong Hình 5 (c) và (d), đo độ trễ của việc tạo token tiếp theo ở các độ dài đầu vào khác nhau và so sánh kết quả với baseline FP16. Tương tự như kết quả hiệu suất attention, Palu hiển thị tăng tốc tối thiểu hoặc không có cho các chuỗi ngắn nhưng mang lại tăng tốc đáng kể cho các chuỗi dài hơn. Không có lượng tử hóa, Palu đạt được tăng tốc lên đến 1.71× và 2.05× cho các mô hình dựa trên RoPE và không RoPE, tương ứng. Với tỷ lệ nén 50%, Palu chạy lên đến độ dài đầu vào 32K trên một GPU RTX 4090. Bằng cách kết hợp lượng tử hóa 4-bit, Palu xử lý được các chuỗi thậm chí dài hơn và mang lại tăng tốc đầu cuối 2.59× và 5.53× ở độ dài chuỗi 64K. Palu tích hợp với lượng tử hóa cung cấp lợi thế tốc độ đáng kể so với KIVI-4-bit, chỉ đạt được tăng tốc 1.78× và 1.81× ở độ dài chuỗi 32K cho các kịch bản RoPE và không RoPE, tương ứng, và hết bộ nhớ cho các chuỗi dài hơn.

4.5.2 KERNEL CHO ĐIỂM ATTENTION DỰA TRÊN ROPE
Trong phần này, chúng tôi đánh giá hiệu suất của kernel tái tạo trực tuyến của chúng tôi cho điểm attention dựa trên RoPE. Chúng tôi đo độ trễ từ vector query trước RoPE đến điểm attention sau GEMV, và so sánh nó với GEMV của PyTorch, được sử dụng trong attention baseline (xem Hình 2).

Hình 6: Tăng tốc của kernel điểm attention Palu với tái tạo trực tuyến.

Chúng tôi trình bày tăng tốc cho kích thước nhóm 1, 4, và 32 ở các độ dài chuỗi khác nhau trong Hình 6. Đối với gs-32 (J-LRD), phép phân tách độ chính xác cao nhất, chi phí tái tạo cao gây ra tốc độ chậm đáng kể trên tất cả độ dài chuỗi. Đối với gs-1 (M-LRD), kernel của chúng tôi đạt được tăng tốc lên đến 3.56× ở độ dài chuỗi 16K, hiển thị hiệu suất mạnh khi mất độ chính xác vừa phải có thể chấp nhận được. Đối với gs-4 (G-LRD), kernel của chúng tôi đạt được tăng tốc lên đến 1.95×. Những kết quả này nhấn mạnh nhu cầu khám phá các mức độ chi tiết phân tách khác nhau để có sự đánh đổi độ chính xác và tốc độ tốt hơn.

Chúng tôi cũng quan sát rằng tăng tốc giảm cho độ dài chuỗi vượt quá 16K do chi phí tái tạo tăng, chuyển việc tái tạo trực tuyến từ bị giới hạn bởi bộ nhớ sang bị giới hạn bởi tính toán. Một tối ưu hóa tiềm năng là lượng tử hóa thêm các ma trận trọng số được phân tách và tận dụng phần cứng thông lượng cao, độ chính xác thấp (ví dụ, INT4 Tensor Cores) cho tái tạo trực tuyến, mà chúng tôi để lại cho công việc tương lai. Mặc dù tăng tốc giảm ở độ dài dài hơn, tăng tốc attention tổng thể của Palu tăng với đầu vào dài hơn, nhờ hợp nhất ma trận trên trạng thái Value và dấu chân bộ nhớ giảm.

5 CÔNG TRÌNH LIÊN QUAN
SVD cho Nén LLM. Một số công trình đã khám phá việc sử dụng SVD để nén LLM. Một phương pháp sớm (Noach & Goldberg, 2020) áp dụng SVD tiêu chuẩn cho các ma trận trọng số, dẫn đến lỗi nén đáng kể. FWSVD (Hsu et al., 2022) giải quyết điều này bằng cách sử dụng thông tin Fisher để ưu tiên các tham số, trong khi ASVD (Yuan et al., 2023) xem xét các giá trị ngoại lai kích hoạt. SVD-LLM (Wang et al., 2024) tiếp tục giảm thiểu mất mát nén cho mỗi giá trị đơn. Không giống như các phương pháp này, nén trọng số mô hình, Palu tập trung vào việc giảm kích thước KV-Cache. Một công trình đồng thời, (Yu et al., 2024), cũng khám phá nén KV-Cache sử dụng phép chiếu hạng thấp, suy ra các ma trận hạng thấp từ KV-Cache với dữ liệu hiệu chuẩn và nhóm các đầu attention tương tự như G-LRD của Palu. Tuy nhiên, nó yêu cầu tinh chỉnh LoRA sau phép phân tách. Ngược lại, Palu trực tiếp phân tách ma trận trọng số, bảo tồn độ chính xác mà không cần tinh chỉnh, và giới thiệu các cải tiến bổ sung, bao gồm tìm kiếm hạng, tích hợp lượng tử hóa, và các kernel GPU được tối ưu hóa.

Nén KV-Cache. Lượng tử hóa là một kỹ thuật được sử dụng rộng rãi để nén KV-Cache. Atom (Zhao et al., 2023) áp dụng lượng tử hóa đơn giản theo token, trong khi WKVQuant (Yue et al., 2024) giới thiệu một sơ đồ hai cấp để tăng cường độ chính xác. KIVI (Liu et al., 2024b) sử dụng lượng tử hóa theo kênh và theo token cho Key và Value, kết hợp với lượng tử hóa nhóm fine-grained ở kích thước nhóm 32. KVQuant (Hooper et al., 2024) sử dụng thiết lập tương tự nhưng kết hợp lượng tử hóa không đồng nhất và ma trận thưa để xử lý các giá trị ngoại lai. Ngoài các phương pháp này, GEAR thêm một

10

--- TRANG 11 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
ma trận hạng thấp để bù đắp cho lỗi lượng tử hóa. Trong Palu, chúng tôi tận dụng các kỹ thuật hạng thấp để khai thác sự dư thừa chiều ẩn và, với lượng tử hóa đơn giản theo token, đạt được kết quả nén xuất sắc.

MLA. Mô hình DeepSeek-V2 được phát hành gần đây (DeepSeek-AI et al., 2024) giới thiệu cơ chế MLA, giảm kích thước KV-Cache bằng cách chiếu xuống Key và Value vào không gian hạng thấp và tái tạo chúng thành hạng đầy đủ trong thời gian chạy. Mặc dù MLA có thể tương tự như Palu ở mức độ cao, đặc biệt với J-LRD, các quá trình thiết kế và suy ra của chúng tôi về cơ bản khác nhau. Không giống như MLA, một cơ chế attention mới yêu cầu tiền huấn luyện, Palu được thiết kế cụ thể cho tích hợp sau huấn luyện. Palu tập trung vào việc chuyển đổi các mô hình hiện có với MHA hoặc GQA để hỗ trợ KV-Cache nén hạng thấp, bảo tồn độ chính xác cao trong khi tăng cường hiệu quả suy luận.

6 KẾT LUẬN
Chúng tôi giới thiệu Palu, một khung nén KV-Cache mới phân tách các ma trận trọng số phép chiếu tuyến tính và lưu trữ các biểu diễn tiềm ẩn đã nén. Chúng tôi đề xuất các tối ưu hóa khác nhau, bao gồm phép phân tách hạng thấp group-head, thuật toán phân bổ hạng tự động, tăng cường tương thích lượng tử hóa, và các kernel tùy chỉnh với hợp nhất toán tử. Với những tối ưu hóa này, Palu có thể duy trì độ chính xác trong khi đạt được giảm bộ nhớ đáng kể và tăng tốc suy luận cao. Các thí nghiệm cho thấy rằng, với nén hạng thấp 50% và lượng tử hóa 4-bit, Palu tăng tốc mô-đun attention dựa trên RoPE lên đến 2.91× và mang lại tăng tốc đầu cuối lên đến 2.2× cho cùng mô hình dựa trên RoPE, trong khi bảo tồn độ chính xác mạnh trên các benchmark khác nhau.

TÀI LIỆU THAM KHẢO
Mohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, và Nicholas Donald Lane. Zero-cost proxies for lightweight NAS. 2021. URL https://openreview.net/forum?id=0cmMMy8J5q.

Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, và Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.

Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, và James Hensman. SliceGPT: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=vXxardq6db.

Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, và James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024b.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, và Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, và các cộng sự. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432–7439, 2020.

Yuang Chen, Cheng Zhang, Xitong Gao, Robert D. Mullins, George A. Constantinides, và Yiren Zhao. Optimised grouped-query attention mechanism for transformers, 2024. URL https://arxiv.org/abs/2406.14963.

Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, và Diana Marculescu. Quamba: A post-training quantization recipe for selective state space models, 2024. URL https://arxiv.org/abs/2410.13229.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.

11

--- TRANG 12 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, và Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, và Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4599–4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https://aclanthology.org/2021.naacl-main.365.

DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, và Ziwei Xie. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur

12

--- TRANG 13 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Tim-

13

--- TRANG 14 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

othy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, và Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.

Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, và Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Anna Korhonen, David Traum, và Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074–1084, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1102. URL https://aclanthology.org/P19-1102.

Fino và Algazi. Unified matrix treatment of the fast walsh-hadamard transform. IEEE Transactions on Computers, C-25(11):1142–1146, 1976. doi: 10.1109/TC.1976.1674569.

Elias Frantar, Roberto L. Castro, Jiale Chen, Torsten Hoefler, và Dan Alistarh. Marlin: Mixed-precision auto-regressive parallel inference on large language models, 2024. URL https://arxiv.org/abs/2408.11743.

Yao Fu. Challenges in deploying long-context transformers: A theoretical peak performance analysis, 2024. URL https://arxiv.org/abs/2405.08944.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.

Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, và Kurt Keutzer. Ai and memory wall, 2024. URL https://arxiv.org/abs/2403.14123.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, và Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL http://arxiv.org/abs/1911.12237.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, và Julian J. McAuley. Longcoder: A long-range pre-trained language model for code completion. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, và Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 12098–12107. PMLR, 2023. URL https://proceedings.mlr.press/v202/guo23j.html.

Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, và Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024.

Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, và Hongxia Jin. Language model compression with weighted low-rank factorization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5.

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, và William El Sayed. Mistral 7b, 2023.

14

--- TRANG 15 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Ian T Jolliffe và Jorge Cadima. Principal component analysis: a review and recent developments. Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.

Mandar Joshi, Eunsol Choi, Daniel Weld, và Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay và Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, và Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.

Xin Li và Dan Roth. Learning question classifiers. In 19th International Conference on Computational Linguistics, COLING 2002, Howard International House and Academia Sinica, Taipei, Taiwan, August 24 - September 1, 2002, 2002. URL https://aclanthology.org/C02-1150/.

Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, và Wayne Zhang. Group fisher pruning for practical network compression. In Marina Meila và Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7021–7032. PMLR, 2021. URL http://proceedings.mlr.press/v139/liu21ab.html.

Tianyang Liu, Canwen Xu, và Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2023.

Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, và Tijmen Blankevoort. Spinquant–llm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024a.

Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, và Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024b.

Alexander Ly, Maarten Marsman, Josine Verhagen, Raoul Grasman, và Eric-Jan Wagenmakers. A tutorial on fisher information, 2017.

Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large language models, 2023.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture models, 2016.

Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

Matan Ben Noach và Yoav Goldberg. Compressing pre-trained language models by matrix decomposition. In Kam-Fai Wong, Kevin Knight, và Hua Wu (eds.), Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2020, Suzhou, China, December 4-7, 2020, pp. 884–889. Association for Computational Linguistics, 2020. URL https://aclanthology.org/2020.aacl-main.88/.

Ofir Press, Noah Smith, và Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.

15

--- TRANG 16 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.

Pratyusha Sharma, Jordan T. Ash, và Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction, 2023.

Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019. URL https://arxiv.org/abs/1911.02150.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, và Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.

Philippe Tillet, Hsiang-Tsung Kung, và David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp. 10–19, 2019.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.

Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, và Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, và Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.

Xin Wang, Yu Zheng, Zhongwei Wan, và Mi Zhang. Svd-llm: Truncation-aware singular value decomposition for large language model compression. arXiv preprint arXiv:2403.07378, 2024.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu và David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, và Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF.

16

--- TRANG 17 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Hao Yu, Zelan Yang, Shen Li, Yong Li, và Jianxin Wu. Effectively compress kv heads for llm, 2024. URL https://arxiv.org/abs/2406.07056.

Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, và Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models, 2023.

Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, và Liqiang Nie. Wkvquant: Quantizing weight and key/value cache for large language models gains more. arXiv preprint arXiv:2402.12065, 2024.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, và các cộng sự khác. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024.

Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, và Yuandong Tian. Galore: Memory-efficient LLM training by gradient low-rank projection. CoRR, abs/2403.03507, 2024. doi: 10.48550/ARXIV.2403.03507. URL https://doi.org/10.48550/arXiv.2403.03507.

Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zhenga, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, và Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, và Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, và Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905–5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.naacl-main.472.

17

--- TRANG 18 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
PHỤ LỤC

A CƠ BẢN VỀ LƯỢNG TỬ HÓA
Các kỹ thuật lượng tử hóa sử dụng các giá trị rời rạc bit thấp để xấp xỉ các số dấu phẩy động độ chính xác cao. Hàm lượng tử hóa đồng nhất bất đối xứng tổng quát được định nghĩa là:
X̃ = clamp(⌊X/s⌉ + z, 0, 2^B - 1), (8)
trong đó X̃ biểu thị tensor xấp xỉ với các biểu diễn bit thấp (tức là số nguyên 4-bit), X là tensor dấu phẩy động, s = (X_max - X_min)/(2^B - 1) là hệ số tỷ lệ, và z = -⌊X_min/s⌉ là điểm không. Ký hiệu ⌊·⌉ là phép toán làm tròn.

B CHI TIẾT TRIỂN KHAI KERNEL

𝐇_r 𝐁_q 𝑟×𝑑_ℎ 1 Split 1 Split 2 … Split N-1 𝐁𝐇_𝐢 Matmul 𝐊_𝐢 𝐿_"#$% 𝐋 RoPE 𝐊_𝐢* Matmul 𝐚_q Thread Block 1 𝐋 𝐋_!"#$ Cos/Sin Position id 𝐋_1 …

Hình 7: Minh họa kernel GPU hợp nhất của chúng tôi để tính điểm attention với tái tạo trực tuyến. Trong hình này, q biểu diễn vector query, H biểu thị các trạng thái key nén hạng thấp, và B đại diện cho các ma trận tái tạo.

Kernel cho tính toán điểm attention với tái tạo. Ý tưởng trung tâm của Palu là tận dụng các biểu diễn tiềm ẩn hạng thấp để tăng tốc cơ chế attention bằng cách giảm chi phí chuyển dữ liệu. Thay vì làm việc trực tiếp với ma trận key kích thước đầy đủ, chúng tôi lưu trữ và chuyển một biểu diễn tiềm ẩn nén hạng thấp, được ký hiệu là H∈R^(L×r). Trong quá trình tính toán, kernel GPU tùy chỉnh của chúng tôi thực hiện tái tạo tức thì bằng ma trận tái tạo B∈R^(r×d_h), tạo ra ma trận key được khôi phục K∈R^(L×d_h), trong đó L là độ dài chuỗi, d_h là chiều ẩn, và r biểu thị hạng còn lại sau khi thực hiện phép chiếu hạng thấp. Vector query, được biểu diễn là q∈R^(1×d_h), sau đó nhân với các key được tái tạo để thu được điểm attention.

Để tận dụng hiệu quả tính song song, chúng tôi thực hiện chia tile theo chiều độ dài chuỗi L. Cụ thể, chúng tôi chia chuỗi thành các tile nhỏ hơn có kích thước L_tile, gán mỗi tile cho một thread block chuyên dụng. Mỗi thread block độc lập tái tạo một ma trận con H_i∈R^(L_tile×d_h) từ biểu diễn tiềm ẩn hạng thấp H, sau đó áp dụng embedding vị trí bằng RoPE, và cuối cùng thực hiện phép nhân ma trận-vector giữa q và H_i để tạo ra điểm attention một phần. Thiết kế này đảm bảo rằng tất cả các tính toán trung gian, từ tái tạo đến embedding và phép nhân cuối cùng, vẫn hoàn toàn trong bộ nhớ trên chip (tức là bộ nhớ chia sẻ), do đó giảm thiểu truy cập bộ nhớ độ trễ cao và tận dụng đầy đủ khả năng xử lý song song của GPU để đạt được tăng tốc đáng kể.

18

--- TRANG 19 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

C THẢO LUẬN VỀ SỬ DỤNG BỘ NHỚ
Trong công trình này, kết quả thí nghiệm tập trung vào tỷ lệ nén của KV-Cache như một chỉ số chính. Tuy nhiên, việc xem xét tiết kiệm bộ nhớ tổng thể như một yếu tố quan trọng hơn là rất quan trọng. Ví dụ, như đã được chứng minh trong Mục 2.2, một tỷ lệ nén điển hình là 30% có thể dẫn đến tăng kích thước trọng số khoảng 40%. Sự tăng này được tính toán dưới giả định rằng m = n và r = 0.7n, dẫn đến phương trình (mr + nr)/mn = 1.4. Sự tăng như vậy chỉ ra việc sử dụng bộ nhớ bổ sung đáng kể.

Vấn đề này chủ yếu phát sinh trong các sơ đồ phân tách J-LRD, nơi các phép chiếu của tất cả các đầu được phân tách chung. Ngược lại, các sơ đồ phân tách M-LRD và sơ đồ G-LRD được tối ưu hóa của chúng tôi liên quan đến các ma trận mục tiêu không vuông. Ví dụ, trong sơ đồ G-LRD với kích thước nhóm 4, ma trận mục tiêu được hình thành bằng cách nối các ma trận phép chiếu ban đầu của mỗi đầu attention trong nhóm. Trong mô hình Llama-2-7b, với chiều embedding là 4096 và chiều đầu là 128, mỗi ma trận phép chiếu là 4096 × 128, dẫn đến ma trận nối có kích thước 4096 × 512. Trong trường hợp này, các chiều nên được xem xét là m = 8n. Áp dụng phương trình tham chiếu (mr + nr)/mn với r = 0.7n, chúng ta thấy rằng (mr + nr)/mn = 0.7875, chỉ ra không có chi phí lưu trữ bổ sung và, thực tế, đạt được thêm 21.25% tiết kiệm bộ nhớ.

Hơn nữa, quan trọng là phải nhấn mạnh rằng các trọng số liên quan đến các phép chiếu K và V chỉ chiếm 2 trong số 7 lớp tuyến tính trong các khối transformer, chỉ bao gồm 16% tham số trong các mô hình Llama-2-7b. Điều này hạn chế tác động tổng thể đến việc sử dụng bộ nhớ. Vì vậy, trong khi J-LRD có thể phát sinh chi phí, các sơ đồ M-LRD và G-LRD cung cấp các giải pháp thay thế hiệu quả không dẫn đến tăng việc sử dụng bộ nhớ, làm cho chúng trở thành các lựa chọn khả thi cho các ứng dụng thực tế.

D GIẢM BỘ NHỚ TỔNG THỂ
Trong Hình 8, chúng tôi hiển thị tổng việc sử dụng bộ nhớ, bao gồm trọng số mô hình và KV-Cache dưới các độ dài chuỗi khác nhau. Chúng tôi quan sát rằng KV-Cache nhanh chóng trở thành nút thắt cổ chai bộ nhớ khi độ dài chuỗi tăng. Ở độ dài chuỗi 64k, KV-Cache chiếm 78% tổng tiêu thụ bộ nhớ. Với nén hạng thấp 50%, Palu hiệu quả giảm việc sử dụng bộ nhớ 1.7×, và khi kết hợp với lượng tử hóa 2-bit, nó tiếp tục giảm tổng việc sử dụng bộ nhớ 4.6×.

Hình 8: Tổng việc sử dụng bộ nhớ cho các độ dài chuỗi khác nhau trong Llama-2-7B. nén hạng thấp là 50% cho Palu.

E TÍCH HỢP Palu VỚI TINH CHỈNH LORA
LoRA (Hu et al., 2022) đã trở thành một trong những kỹ thuật tinh chỉnh hiệu quả được sử dụng rộng rãi nhất để thích ứng các mô hình với các nhiệm vụ hoặc lĩnh vực cụ thể với dữ liệu hạn chế. Nó cũng đã được áp dụng với các phương pháp nén LLM (Wang et al., 2024; Ma et al., 2023) như một kỹ thuật khôi phục sau nén để phục hồi mất mát thông tin sau nén. Trong Palu, LoRA cũng có thể áp dụng để tăng cường độ chính xác thêm nữa.

19

--- TRANG 20 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Bảng 4: Đánh giá tích hợp LoRA với Palu trên Llama-2-7B.
Comp. Rate MethodZero-Shot Accuracy (%) ↑
OBQA HellaSwag PIQA ARC-e ARC-c WinoGrande Avg. Diff.
Rate = 0% baseline 44.20 76.00 78.07 76.30 46.42 69.30 65.05 -
Rate = 50%
w/o LoRAJ-LRD 45.40 75.57 77.48 75.97 45.31 69.22 64.83 -0.22
G-LRD 43.60 73.39 76.33 73.02 42.57 66.77 62.61 -2.44
M-LRD 39.60 65.35 74.76 67.17 35.24 64.64 57.79 -7.26
Rate = 50%
w/ LoRAJ-LRD 44.20 74.09 78.51 77.27 48.81 71.03 65.65 +0.60
G-LRD 43.40 73.08 78.56 75.72 47.10 69.85 64.62 -0.43
M-LRD 41.80 70.78 78.02 73.86 43.86 69.22 62.92 -2.12

Bảng 5: Perplexity và độ chính xác zero-shot của Palu, với các chiến lược phân tách khác nhau ở 50%
Model MethodPerplexity ↓ Zero-Shot Accuracy (%) ↑
Wiki2 C4 OBQA Hella PIQA ARC-e ARC-c Wino Avg.
Llama-2-13BBaseline 4.88 6.70 45.20 79.39 79.11 79.42 49.06 72.38 67.43
J-LRD 4.97 6.92 46.40 79.48 78.62 79.29 49.91 70.56 67.38
G-LRD 5.31 7.76 45.60 77.29 77.42 76.05 45.99 72.45 65.80
M-LRD 5.65 8.34 43.20 74.34 77.53 75.76 45.39 68.98 64.20

Để tích hợp LoRA với Palu, chúng tôi giới thiệu các ma trận hạng thấp bổ sung A'_r∈R^(d×r') và B'_r∈R^(r'×d) để tinh chỉnh phép chiếu hạng thấp ban đầu như dưới đây:
h = Ax + A'_rB'_rx (9)

Ở đây, A sẽ là các tham số cố định được suy ra từ phép phân tách hạng thấp từ các trọng số được tiền huấn luyện của các lớp tuyến tính, trong khi A'_r và B'_r là các tham số có thể huấn luyện để nắm bắt các sắc thái cụ thể của nhiệm vụ và khôi phục thông tin bị mất trong quá trình nén.

Thiết lập. Theo Ashkboos et al. 2024a, chúng tôi lấy mẫu 8k mẫu từ tập dữ liệu huấn luyện Alpaca làm tập dữ liệu tinh chỉnh và áp dụng LoRA với hạng r' = 32 và α = 32. Tất cả các siêu tham số khác được căn chỉnh với Ashkboos et al. (2024a), ngoại trừ tỷ lệ học 2e-4, và việc sử dụng bộ lập lịch tỷ lệ học cosine.

Kết quả Thí nghiệm. Chúng tôi trình bày kết quả thí nghiệm với LoRA trong Bảng 4. Theo Ashkboos et al. 2024a Với LoRA được kết hợp, J-LRD tiếp tục cho thấy sự giảm hiệu suất tối thiểu với sự giảm trung bình 1.00%. G-LRD (gs=4) và M-LRD cho thấy kết quả cải thiện so với các đối tác không LoRA của chúng, với sự giảm trung bình lần lượt là 2.01% và 5.14%. Đáng chú ý, với tích hợp LoRA, G-LRD chỉ cho thấy 1.03% khác biệt độ chính xác so với J-LRD.

F KẾT QUẢ THÊM VỀ ĐỘ CHÍNH XÁC ZERO-SHOT
Tiếp theo Mục 4.2, chúng tôi tiếp tục báo cáo kết quả đánh giá perplexity và zero-shot của Palu trên Llama-2-13B ở tỷ lệ nén 50%. Như được hiển thị trong Bảng 5, chúng tôi quan sát rằng Palu đạt được sự giảm độ chính xác cạnh tranh khoảng 3% hoặc ít hơn trên các phương pháp khác nhau sử dụng J-LRD, G-LRD, hoặc M-LRD. Vì vậy, người dùng có thể áp dụng M-LRD trước để tối ưu hóa hiệu quả thêm nữa.

G NGHIÊN CỨU LOẠI BỎ
G.1 ẢNH HƯỞNG CỦA KÍCH THƯỚC NHÓM KHÁC NHAU
Vì phương pháp G-LRD được đề xuất của chúng tôi cho phép cân bằng hiệu suất và hiệu quả bằng cách điều chỉnh kích thước nhóm, chúng tôi đã tiến hành một nghiên cứu loại bỏ về kích thước nhóm. Như được thấy trong Bảng 6, khi kích thước nhóm tăng, lượng thông tin được chia sẻ cũng tăng, dẫn đến hiệu suất cải thiện.

20

--- TRANG 21 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

Bảng 6: Nghiên cứu loại bỏ về kích thước nhóm phân tách hạng thấp trên perplexity cho mô hình Llama-2-7B ở tỷ lệ nén 50% sử dụng Wikitext-2.
Method Group Size Perplexity
Baseline - 5.47
J-LRD 32 5.62
G-LRD16 5.74
8 5.88
4 6.01
2 6.42
M-LRD 1 6.81

G.2 ẢNH HƯỞNG CỦA PHÉP BIẾN ĐỔI WALSH-HADAMARD
Chúng tôi tiến hành nghiên cứu loại bỏ để đánh giá lợi ích của việc áp dụng Phép biến đổi Walsh-Hadamard (WHT). Kết quả thí nghiệm được báo cáo tại Bảng 7. Ở mức lượng tử hóa 3-bit, chúng tôi quan sát rằng Phép biến đổi Hadamard chỉ mang lại một lượng perplexity nhẹ. Tuy nhiên, khi chúng tôi lượng tử hóa biểu diễn hạng thấp cực kỳ (tức là 2-bit), chúng ta có thể quan sát một cải tiến perplexity đáng chú ý là 4.17. Đáng nhấn mạnh lại rằng Phép biến đổi Hadamard sẽ không mang lại chi phí bổ sung trong quá trình suy luận, vì Palu tối ưu hóa quá trình WHT qua tiền xử lý ngoại tuyến. Người đọc có thể tham khảo Mục 3.4 để biết thêm chi tiết.

G.3 PHÂN BỔ HẠNG TỰ ĐỘNG VS. PHÂN BỔ HẠNG ĐỒNG NHẤT
Bảng 8 trình bày nghiên cứu loại bỏ về tác động của các sơ đồ phân bổ hạng khác nhau đến độ chính xác của mô hình. Áp dụng tìm kiếm hạng dẫn đến cải thiện hiệu suất đáng chú ý. Ví dụ, ở tỷ lệ nén 50%, có sự giảm perplexity đáng kể là 2.18. Hình 9 hình dung phân bổ hạng trên các khối transformer khác nhau cho các lớp phép chiếu key và value. Kết quả rõ ràng chứng minh một kết quả phân bổ không đồng nhất. Cụ thể, chúng tôi quan sát rằng value thường được phân bổ hạng cao hơn key. Ngoài ra, nửa đầu của các lớp được gán hạng cao hơn, chỉ ra tầm quan trọng lớn hơn của chúng trong việc bảo tồn hiệu suất mô hình. Hình dung này nhấn mạnh hiệu quả của thuật toán tìm kiếm hạng của chúng tôi trong việc xác định và phân bổ hạng phù hợp cho các thành phần khác nhau, do đó tối ưu hóa sự cân bằng giữa nén và độ chính xác.

Bảng 7: Nghiên cứu Loại bỏ về các cài đặt lượng tử hóa khác nhau để lượng tử hóa các biểu diễn tiềm ẩn hạng thấp. Giống như Mục 4.3, chúng tôi sử dụng WikiText-2 với độ dài chuỗi được đặt thành 4096 làm benchmark đánh giá.
Method Wikitext-2 PPL ↓
Llama2-7B 5.12
Palu-30% (FP16) 5.25
+ 3-bits w/o Hadamard 5.52
+ 3-bits w Hadamard 5.33 (0.19 ↓)
+ 2-bits w/o Hadamard 9.48
+ 2-bits w Hadamard 5.77 (3.71 ↓)
Palu-50% (FP16) 5.63
+ 3-bits w/o Hadamard 5.99
+ 3-bits w Hadamard 5.77 (0.22 ↓)
+ 2-bits w/o Hadamard 10.58
+ 2-bits w Hadamard 6.41 (4.17 ↓)

21

--- TRANG 22 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp

0 5 10 15 20 25 30
Layer Index0.00.20.40.60.8Compression Rate
 key
value

Hình 9: Hình dung tỷ lệ nén hạng thấp theo lớp trên Llama-2-7B với 50% tỷ lệ nén tổng thể. Ở đây, tỷ lệ nén (tức là hạng) được phân bổ bằng thuật toán phân bổ hạng tự động dựa trên Thông tin Fisher được đề xuất.

Bảng 8: Nghiên cứu loại bỏ về có/không có tìm kiếm hạng. Chúng tôi sử dụng Llama-2-7b và Wikitext-2 với độ dài chuỗi 2048 làm benchmark.
Rate=30% Rate=50% Rate=70%
Uniform 6.34 7.36 10.77
Automatic (ours) 5.62 (0.72 ↓) 6.02 (1.36 ↓) 8.59 (2.18 ↓)

H CHI TIẾT THÍ NGHIỆM
H.1 CHI TIẾT ĐÁNH GIÁ ZERO-SHOT
Chúng tôi đã chọn sáu nhiệm vụ zero-shot từ benchmark LM-eval để đánh giá Palu:
• OpenBookQA (accuracy, Mihaylov et al.)
• HellaSwag (acc norm, Zellers et al.)
• PIQA (accuracy, Bisk et al.)
• ARC-Easy (accuracy, Clark et al.)
• ARC-Challenge (acc norm, Clark et al.)
• WinoGrande (accuracy, Sakaguchi et al.)

Chúng tôi báo cáo accuracy cho WinoGrande, PIQA, và ARC-Easy, và accuracy được chuẩn hóa theo độ dài chuỗi (acc norm) cho HellaSwag và ARC-Challenge.

H.2 CHI TIẾT ĐÁNH GIÁ LONG BENCH
Để đánh giá LongBench trong bản thảo, chúng tôi đã chọn tám nhiệm vụ từ bốn nhóm con, đảm bảo đánh giá toàn diện về Palu. Các nhiệm vụ và các chỉ số tương ứng của chúng được chi tiết dưới đây:
• Single-Document QA:
  –Qasper (F1 score, Dasigi et al.)
• Summarization:
  –QMSum (ROUGE score, Zhong et al.)
  –MultiNews (ROUGE score, Fabbri et al.)
• Few-shot Learning:
  –TREC (classification score, Li & Roth)
  –TriviaQA (F1 score, Joshi et al.)
  –SAMSum (ROUGE score, Gliwa et al.)
• Code Completion:

22

--- TRANG 23 ---
Palu: Nén KV-Cache với Phép Chiếu Hạng Thấp
  –LCC (similarity score, Guo et al.)
  –RepoBench-P (similarity score, Liu et al.)

Trong quá trình đánh giá, chúng tôi đặt độ dài chuỗi tối đa thành 31500 cho cả mô hình Mistral và LongChat.

23
