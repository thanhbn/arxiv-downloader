# 1912.00544.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1912.00544.pdf
# Kích thước tệp: 156426 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:1912.00544v1  [cs.CL]  2 Dec 2019

Cơ chế Tự chú ý Đa tỷ lệ cho Phân loại Văn bản

Qipeng Guo‡∗, Xipeng Qiu‡ †, Pengfei Liu‡, Xiangyang Xue‡, Zheng Zhang§
‡Phòng thí nghiệm Trọng điểm Thượng Hải về Xử lý Thông tin Thông minh, Đại học Fudan
‡Khoa Khoa học Máy tính, Đại học Fudan
§Phòng thí nghiệm AI AWS Thượng Hải
§Đại học New York Thượng Hải
{qpguo16, xpqiu, pﬂiu14, xyxue }@fudan.edu.cn, zz@nyu.edu

Tóm tắt
Trong bài báo này, chúng tôi giới thiệu kiến thức tiên nghiệm, cấu trúc đa tỷ lệ, vào các mô-đun tự chú ý. Chúng tôi đề xuất một Transformer Đa tỷ lệ sử dụng cơ chế tự chú ý đa đầu đa tỷ lệ để thu thập các đặc trưng từ các tỷ lệ khác nhau. Dựa trên quan điểm ngôn ngữ học và phân tích Transformer đã được huấn luyện trước (BERT) trên một tập dữ liệu lớn, chúng tôi tiếp tục thiết kế một chiến lược để kiểm soát phân phối tỷ lệ cho mỗi lớp. Kết quả của ba loại nhiệm vụ khác nhau (21 bộ dữ liệu) cho thấy Transformer Đa tỷ lệ của chúng tôi vượt trội so với Transformer tiêu chuẩn một cách nhất quán và đáng kể trên các bộ dữ liệu có kích thước nhỏ và vừa.

Giới thiệu
Cơ chế Tự chú ý được sử dụng rộng rãi trong các nhiệm vụ phân loại văn bản, và các mô hình dựa trên cơ chế tự chú ý như Transformer (Vaswani et al. 2017), BERT (Devlin et al. 2018) đạt được nhiều kết quả thú vị trên các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP), chẳng hạn như dịch máy, mô hình hóa ngôn ngữ (Dai et al. 2019), và phân loại văn bản.

Gần đây, Radford et al. (2018) chỉ ra điểm yếu của các mô-đun tự chú ý, đặc biệt là hiệu suất kém trên các bộ dữ liệu có kích thước nhỏ và vừa. Mặc dù việc huấn luyện trước trên tập dữ liệu lớn có thể giúp ích cho việc này, chúng tôi tin rằng lý do cơ bản là mô-đun tự chú ý thiếu thiên kiến quy nạp phù hợp, do đó quá trình học phụ thuộc nhiều vào dữ liệu huấn luyện. Rất khó để học một mô hình với khả năng tổng quát hóa tốt từ đầu trên một tập huấn luyện hạn chế mà không có thiên kiến quy nạp tốt.

Cấu trúc Đa tỷ lệ được sử dụng rộng rãi trong các lĩnh vực thị giác máy tính (CV), NLP và xử lý tín hiệu. Nó có thể giúp mô hình thu thập các mẫu ở các tỷ lệ khác nhau và trích xuất các đặc trưng mạnh mẽ. Cụ thể đối với lĩnh vực NLP, một cách phổ biến để thực hiện đa tỷ lệ là cấu trúc phân cấp, chẳng hạn như mạng nơ-ron tích chập (CNN) (Kalchbrenner, Grefenstette, và Blunsom 2014), mạng nơ-ron hồi tiếp đa tỷ lệ (RNN) (Chung, Ahn, và Bengio 2016), mạng nơ-ron có cấu trúc cây (Socher et al. 2013; Tai, Socher, và Manning 2015) và chú ý phân cấp (Yang et al. 2016). Nguyên lý đằng sau các mô hình này là đặc tính của ngôn ngữ: đặc trưng cấp cao là sự kết hợp của các thuật ngữ cấp thấp. Với cấu trúc phân cấp, các mô hình này thu thập các kết hợp cục bộ ở các lớp thấp hơn và kết hợp phi cục bộ ở các lớp cao. Sự phân chia lao động này làm cho mô hình ít đói dữ liệu hơn.

Tuy nhiên, đối với các mô-đun tự chú ý, không có hạn chế nào về thiên kiến kết hợp. Các phụ thuộc giữa các từ hoàn toàn được điều khiển bởi dữ liệu mà không có bất kỳ tiên nghiệm nào, dẫn đến việc dễ dàng bị overfitting trên các bộ dữ liệu có kích thước nhỏ hoặc vừa.

Trong bài báo này, chúng tôi đề xuất một cơ chế tự chú ý đa đầu đa tỷ lệ (MSMSA), trong đó mỗi đầu chú ý có một tỷ lệ biến đổi. Tỷ lệ của một đầu hạn chế khu vực hoạt động của tự chú ý. Một cách trực quan, tỷ lệ lớn làm cho đặc trưng bao gồm nhiều thông tin ngữ cảnh hơn và trở nên mượt mà hơn. Tỷ lệ nhỏ nhấn mạnh vào thiên kiến cục bộ và khuyến khích các đặc trưng trở nên nổi bật và sắc nét. Dựa trên MSMSA, chúng tôi tiếp tục đề xuất transformer đa tỷ lệ, bao gồm nhiều lớp MSMSA. Khác với đa tỷ lệ trong cấu trúc phân cấp, mỗi lớp của Transformer đa tỷ lệ bao gồm nhiều đầu chú ý với nhiều tỷ lệ, mang lại khả năng thu thập các đặc trưng đa tỷ lệ trong một lớp duy nhất.

Đóng góp của bài báo này bao gồm:
• Chúng tôi giới thiệu cấu trúc đa tỷ lệ vào khung tự chú ý, mô hình Transformer Đa tỷ lệ được đề xuất có thể trích xuất các đặc trưng từ các tỷ lệ khác nhau.
• Được truyền cảm hứng từ cấu trúc phân cấp của ngôn ngữ, chúng tôi tiếp tục phát triển một chiến lược đơn giản để kiểm soát phân phối tỷ lệ cho các lớp khác nhau. Dựa trên kết quả thực nghiệm trên các nhiệm vụ thực tế và phân tích từ BERT, chúng tôi đề xuất sử dụng nhiều đầu chú ý tỷ lệ nhỏ hơn ở các lớp nông và lựa chọn cân bằng cho các tỷ lệ khác nhau ở các lớp sâu.
• Khối xây dựng của Transformer Đa tỷ lệ, cơ chế tự chú ý đa đầu đa tỷ lệ cung cấp một cách linh hoạt để giới thiệu thiên kiến tỷ lệ (cục bộ hoặc toàn cục), và nó là sự thay thế cho cơ chế tự chú ý đa đầu và mạng feed-forward theo vị trí.
• Kết quả trên ba nhiệm vụ (21 bộ dữ liệu) cho thấy Transformer Đa tỷ lệ của chúng tôi vượt trội so với Transformer tiêu chuẩn một cách nhất quán và đáng kể trên các bộ dữ liệu có kích thước nhỏ và vừa.

--- TRANG 2 ---

Kiến thức Nền tảng

Tự chú ý và kiến trúc mở rộng của nó, Transformer, đạt được nhiều kết quả tốt trên các nhiệm vụ NLP. Thay vì sử dụng đơn vị CNN hoặc RNN để mô hình hóa tương tác giữa các từ khác nhau, Transformer đạt được tương tác theo cặp thông qua cơ chế chú ý.

Tự chú ý Đa đầu Thành phần chính của Transformer là cơ chế chú ý tích vô hướng đa đầu, có thể được hình thức hóa như sau. Cho một chuỗi các vector H∈RN×D, trong đó N là độ dài của chuỗi và D là chiều của vector. Khi thực hiện tự chú ý đa đầu, mô-đun chiếu H thành ba ma trận: truy vấn Q, khóa K và giá trị V. Ba ma trận này sẽ được phân tách thêm thành N′ không gian con tương ứng với N′ đầu và mỗi đầu có D′ đơn vị.

MSA(H) = [ head1,···,headN′]WO, (1)
headi=softmax(QiKT i√D′)Vi, (2)
Q=HWQ,K=HWK,V=HWV, (3)

trong đó MSA(·) đại diện cho Tự chú ý Đa đầu, và WQ,WK,WV,WO là các tham số có thể học được.

Transformer Mỗi lớp trong Transformer bao gồm một tự chú ý đa đầu và một lớp FFN (cũng được gọi là Mạng Feed-Forward theo Vị trí trong Vaswani et al. (2017)). Trạng thái ẩn của lớp l+ 1, Hl+1 có thể được tính như sau.

Zl=norm(Hl+MSA(Hl)), (4)
Hl+1=norm(Zl+FFN(Zl)), (5)

trong đó norm(·) có nghĩa là chuẩn hóa lớp (Ba, Kiros, và Hinton 2016). Ngoài ra, Transformer tăng cường các đặc trưng đầu vào bằng cách thêm một embedding vị trí vì tự chú ý không thể tự nó thu thập thông tin vị trí.

Mặc dù hiệu quả trên dịch máy và mô hình hóa ngôn ngữ, Transformer thường thất bại trên nhiệm vụ với các bộ dữ liệu có kích thước vừa do thiếu thiên kiến quy nạp.

Mô hình

Trong tự chú ý đa đầu, mỗi đầu thu thập các tương tác theo cặp giữa các từ trong không gian đặc trưng khác nhau. Mỗi đầu có cùng tỷ lệ với độ dài câu.

Tự chú ý Nhận biết Tỷ lệ Để giới thiệu khái niệm đa tỷ lệ vào cơ chế tự chú ý, chúng tôi sử dụng một cách đơn giản để tăng cường tự chú ý thông thường, được gọi là Tự chú ý Nhận biết Tỷ lệ (SASA) tương đương với tự chú ý bị hạn chế được đề xuất trong Vaswani et al. (2017) nhưng sử dụng kích thước cửa sổ động.

[Hình 1: Sơ đồ của Tự chú ý Đa đầu Đa tỷ lệ, chúng ta có thể thấy ba đầu tương ứng với ba tỷ lệ khác nhau trong hình. Hộp màu xanh dương, xanh lá, đỏ minh họa tỷ lệ của ω= 1,ω= 3,ω= 5, tương ứng.]

Cho một chuỗi các vector H∈RN×D với độ dài N. SASA có một tham số ω là số không đổi hoặc tỷ lệ theo độ dài chuỗi để kiểm soát phạm vi hoạt động của nó. Một đầu chú ý có thể được tính như

head(H,ω)i,j=SM(QijCij(K,ω)T√D)Cij(V,ω),(6)
Cij(x,ω) = [xi,j−ω,...,xi,j+ω], (7)
Q=HWQ,K=HWK,V=HWV, (8)

trong đó i chỉ ra đầu thứ i và j có nghĩa là vị trí thứ j. SM đại diện cho hàm "Softmax", C là hàm để trích xuất ngữ cảnh cho một vị trí cho trước. WQ,WK,WV là các tham số có thể học được. Tỷ lệ của một đầu có thể là biến như N/2 hoặc số cố định như 3.

Tự chú ý Đa đầu Đa tỷ lệ Với SASA, chúng ta có thể thực hiện Tự chú ý Đa đầu Đa tỷ lệ (MSMSA) với nhiều đầu nhận biết tỷ lệ. Mỗi đầu làm việc trên các tỷ lệ khác nhau. Đối với tự chú ý N′ đầu, MSMSA với tỷ lệ Ω = [ω1,···,ωN′] là

MSMSA(H,Ω) = [ head1(H,ω1);···; headN′(H,ωN′)]WO, (9)

trong đó WO là ma trận tham số.

So với tự chú ý đa đầu vanilla, biến Ω kiểm soát khu vực được chú ý và làm cho các đầu khác nhau có nhiệm vụ khác nhau.

Transformer Đa tỷ lệ Với MSMSA, chúng ta có thể xây dựng transformer đa tỷ lệ (MS-Transformer. Viết tắt là MS-Trans). Ngoài việc sử dụng MSMSA để thay thế tự chú ý đa đầu vanilla, chúng tôi cũng loại bỏ FFN (xem Eq. (5)) vì nó có thể được xem như một tự chú ý với tỷ lệ ω= 1 cộng với một hàm kích hoạt phi tuyến. Vì MSMSA giới thiệu tính cục bộ vào mô hình, MSMSA với tỷ lệ nhỏ có thể là một thay thế cho embedding vị trí. Do đó, transformer đa tỷ lệ có thể được hình thức hóa như sau.

Hl+1= norm( Hl+ReLU(MSMSA( Hl),Ωl)) (10)

--- TRANG 3 ---

trong đó l là chỉ số lớp.

Trong công trình này, chúng tôi giới hạn lựa chọn kích thước tỷ lệ trong số các số không đổi {1,3,···} hoặc các biến phụ thuộc vào độ dài chuỗi {N/16,N/8,···}. Và chúng tôi buộc kích thước tỷ lệ phải là số lẻ.

So với các mô hình đa tỷ lệ phân cấp, transformer đa tỷ lệ cho phép các đầu chú ý trong một lớp có các tỷ lệ tầm nhìn khác nhau, đây là phiên bản "mềm" của việc xem các lớp khác nhau như các tỷ lệ khác nhau.

Nút phân loại Chúng tôi cũng thấy rằng việc thêm một nút đặc biệt ở đầu mỗi chuỗi và kết nối trực tiếp nó với biểu diễn câu cuối cùng có thể cải thiện hiệu suất. Kỹ thuật này được giới thiệu trong BERT (Devlin et al. 2018), được gọi là "[CLS]". Và nó cũng tương tự như "nút chuyển tiếp" trong Guo et al. (2019). Khác với họ, chúng tôi kết hợp nút "[CLS]" và đặc trưng từ việc áp dụng max-pooling trên tất cả các nút trong lớp cuối cùng để biểu diễn câu. Không có sự khác biệt nào giữa nút "[CLS]" và các token khác trong chuỗi đầu vào ngoại trừ nó có thể đóng góp trực tiếp vào biểu diễn cuối cùng.

Tìm kiếm Tỷ lệ Chú ý Hiệu quả

Transformer đa tỷ lệ là một mô-đun linh hoạt trong đó mỗi lớp có thể có các đầu chú ý đa tỷ lệ. Do đó, một yếu tố quan trọng là cách thiết kế phân phối tỷ lệ cho mỗi lớp.

Đa tỷ lệ Phân cấp hay Đa tỷ lệ Linh hoạt

Một cách đơn giản để thực hiện trích xuất đặc trưng đa tỷ lệ là theo cách phân cấp, xếp chồng nhiều lớp với các đầu tỷ lệ nhỏ. Các lớp thấp hơn thu thập các đặc trưng cục bộ và các lớp cao hơn thu thập các đặc trưng phi cục bộ.

Để xác minh khả năng của đa tỷ lệ phân cấp, chúng tôi thiết kế một nhiệm vụ mô phỏng rất đơn giản được gọi là tổng gương. Cho một chuỗi A={a1,...,aN}, a∈Rd và được rút từ U(0,1). Mục tiêu là ∑i=1K ai⊙aN−i+1, trong đó K là một số nguyên cố định nhỏ hơn độ dài chuỗi N và ⊙ có nghĩa là tích Hadamard. Độ dài đường dẫn phụ thuộc tối thiểu là N−K, chúng tôi sử dụng nhiệm vụ này để kiểm tra khả năng của các mô hình trong việc thu thập các phụ thuộc tầm xa. Cả tập huấn luyện và tập kiểm tra đều được tạo ngẫu nhiên và chúng có 200k mẫu mỗi tập. Chúng ta có thể giả định rằng kích thước của tập huấn luyện là đủ để huấn luyện các mô hình này một cách kỹ lưỡng.

Chúng tôi sử dụng ba cài đặt khác nhau của MS-Trans.
1. MS-Trans-Hier-S: MS-Transformer với hai lớp, và mỗi lớp có 10 đầu với tỷ lệ nhỏ ω= 3.
2. MS-Trans-deepHier-S: MS-Transformer với sáu lớp, và mỗi lớp có 10 đầu với tỷ lệ nhỏ ω= 3.
3. MS-Trans-Flex: MS-Transformer với hai lớp, và mỗi lớp có 10 đầu với đa tỷ lệ linh hoạt ω= 3,N/16,N/8,N/4,N/2. Mỗi tỷ lệ có hai đầu.

Như được hiển thị trong Hình-2, Transformer đạt kết quả tốt nhất, và MS-Trans-Flex theo sau với nó. Mặc dù Transformer có tiềm năng cao nhất để thu thập các phụ thuộc tầm xa, nó yêu cầu các mẫu huấn luyện lớn. Trong khi đó, mô hình của chúng tôi cân bằng vấn đề đói dữ liệu và khả năng thu thập các phụ thuộc tầm xa. Dựa trên so sánh của MS-Trans-Hier-S và MS-Trans-deepHier-S, chúng ta có thể thấy rằng cải thiện của các lớp bổ sung là tương đối nhỏ. Theo thí nghiệm tổng hợp và hiệu suất trên các nhiệm vụ thực tế (xem. Sec-), chúng tôi nghĩ rằng các đầu tỷ lệ lớn là cần thiết cho các lớp thấp hơn và các đầu tỷ lệ nhỏ xếp chồng khó thu thập các phụ thuộc tầm xa. Trong trường hợp này, một tiên nghiệm tốt nên chứa cả tỷ lệ nhỏ và tỷ lệ lớn.

[Hình 2: Nhiệm vụ Tổng Gương. Đường cong MSE so với số hợp lệ K với độ dài chuỗi n= 100 và chiều của các vector đầu vào d= 50.]

Phân phối Tỷ lệ của Các Lớp Khác nhau

Vì đa tỷ lệ là cần thiết cho mỗi lớp, câu hỏi thứ hai là làm thế nào để thiết kế tỷ lệ của các tỷ lệ khác nhau cho mỗi lớp? Chúng tôi nghĩ rằng mỗi lớp có thể có sở thích riêng cho phân phối tỷ lệ. Từ quan điểm ngôn ngữ học, một giả định trực quan có thể là: lớp cao hơn có xác suất cao hơn cho các đầu tỷ lệ lớn và lớp nông có xác suất cao hơn cho các đầu tỷ lệ nhỏ.

Để tìm kiếm bằng chứng thực nghiệm, chúng tôi thăm dò một số trường hợp điển hình và phân tích hành vi tương ứng trong một mô hình được điều khiển bởi dữ liệu, BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al. 2018).

Phân tích Tương tự từ BERT BERT là một Transformer được huấn luyện trước trên dữ liệu quy mô lớn, đã cho thấy sức mạnh của nó trên nhiều nhiệm vụ NLP và có khả năng tổng quát hóa tốt. Vì BERT dựa trên Transformer, nó không được hướng dẫn bởi kiến thức tiên nghiệm, kiến thức của chúng được học từ dữ liệu. Chúng tôi thăm dò hành vi của BERT để xem liệu nó có phù hợp với giả định ngôn ngữ học không. Có hai khía cạnh chúng tôi muốn nghiên cứu, thứ nhất là tỷ lệ hoạt động của các đầu chú ý của mỗi lớp trong BERT. Thứ hai là sự khác biệt giữa phân phối tỷ lệ của các lớp khác nhau, đặc biệt là sở thích của các mối quan hệ cục bộ và toàn cục. Để thăm dò những hành vi này, trước tiên chúng tôi chạy BERT trên nhiều câu tự nhiên và chọn kích hoạt cao nhất

--- TRANG 4 ---

[Hình 3: Trực quan hóa BERT. (a) Phân phối khoảng cách chú ý của ba đầu trong lớp đầu tiên. Đầu màu đỏ chỉ quan tâm đến mẫu cục bộ và đầu màu xanh dương nhìn đều nhau ở các khoảng cách khác nhau. (b) Phân phối khoảng cách chú ý của các lớp khác nhau. Lớp nông ưa thích kích thước tỷ lệ nhỏ và có xu hướng tỷ lệ lớn từ từ khi lớp sâu hơn. Ngay cả trong lớp cuối cùng, các mẫu cục bộ vẫn chiếm tỷ lệ lớn. Chúng tôi cắt khoảng cách tại N/2 để trực quan hóa tốt hơn. Hình đầy đủ có thể được tìm thấy trong Phụ lục.]

của bản đồ chú ý như cạnh chú ý. Sau đó chúng tôi ghi lại các khoảng cách tương đối của những cạnh chú ý này. Trong công trình này, chúng tôi thu được dữ liệu từ việc chạy BERT trên bộ dữ liệu CoNLL03 (xem Tab-1).

Trước tiên chúng tôi vẽ Hình-3a để quan sát hành vi của các đầu trong cùng một lớp. Chúng tôi chọn ba đầu, và sự khác biệt của chúng là đáng kể. Như được hiển thị trong hình, "head-2" tập trung vào một khoảng cách nhất định với tỷ lệ nhỏ, và "head-1" bao phủ tất cả các khoảng cách. Có một sự phân chia lao động rõ ràng của những đầu này, và một lớp có thể có cả tầm nhìn cục bộ và toàn cục thông qua việc kết hợp các đặc trưng từ các đầu khác nhau.

Hình thứ hai Hình-3b cho thấy xu hướng của sở thích khoảng cách khi độ sâu của lớp tăng. Chúng ta có thể thấy mô hình chuyển từ tầm nhìn cục bộ sang tầm nhìn toàn cục từ từ và các lớp nông có sự quan tâm mạnh mẽ đến các mẫu cục bộ.

Trực quan hóa của BERT phù hợp với thiết kế của Transformer Đa tỷ lệ, quan sát đầu tiên tương ứng với thiết kế của tự chú ý đa đầu đa tỷ lệ yêu cầu các đầu khác nhau tập trung vào các tỷ lệ khác nhau, và quan sát thứ hai cung cấp một tham chiếu tốt về xu hướng phân phối tỷ lệ qua các lớp. Sử dụng kiến thức như vậy có thể giảm đáng kể yêu cầu dữ liệu huấn luyện cho các mô hình giống Transformer.

Yếu tố Kiểm soát Phân phối Tỷ lệ cho Các Lớp Khác nhau

Từ quan điểm ngôn ngữ học trực quan và bằng chứng thực nghiệm, chúng tôi thiết kế một yếu tố kiểm soát phân phối tỷ lệ cho các lớp khác nhau của transformer đa tỷ lệ.

Gọi L biểu thị số lớp trong transformer đa tỷ lệ, |Ω| biểu thị số kích thước tỷ lệ ứng viên, và nlk biểu thị số đầu cho lớp thứ l và kích thước tỷ lệ thứ k. Số đầu nlk được tính bởi

zlk={0 l=L hoặc k=|Ω|
zlk+1+α k∈ {0,···,|Ω|−1} (11)

nl=softmax(zl)·N′ (12)

Trong các phương trình trên, chúng tôi giới thiệu một siêu tham số α để kiểm soát sự thay đổi sở thích của kích thước tỷ lệ cho mỗi lớp. Ví dụ, α= 0 có nghĩa là tất cả các lớp sử dụng cùng một chiến lược kích thước tỷ lệ, α >0 có nghĩa là sở thích của tỷ lệ nhỏ hơn tăng với sự giảm của độ sâu lớp, và α <0 chỉ ra sở thích của tỷ lệ lớn hơn tăng với sự giảm của độ sâu lớp. Như kết luận của việc phân tích BERT, chúng tôi tin rằng lớp sâu có tầm nhìn cân bằng trên cả các mẫu cục bộ và toàn cục, vì vậy lớp trên cùng nên được đặt để nhìn tất cả kích thước tỷ lệ một cách đồng đều. Cụ thể hơn, khi α= 0.5 và N′= 10, ba lớp có nl=1= {5,2,2,1,0}, nl=2={4,2,2,1,1}, nl=3={2,2,2,2,2}, nó đại diện cho lớp đầu tiên có 5 đầu với kích thước tỷ lệ là 1, 2 đầu với kích thước tỷ lệ là 3, 2 đầu với kích thước tỷ lệ là N/16 và 1 đầu với kích thước tỷ lệ là N/8.

Thí nghiệm

Chúng tôi đánh giá mô hình của chúng tôi trên 17 bộ dữ liệu phân loại văn bản, 3 bộ dữ liệu gán nhãn chuỗi và 1 bộ dữ liệu suy luận ngôn ngữ tự nhiên. Tất cả các thống kê có thể được tìm thấy trong Tab-1. Ngoài ra, chúng tôi sử dụng GloVe (Pennington, Socher, và Manning 2014) để khởi tạo word embedding và JMT (Hashimoto et al. 2017) cho các đặc trưng cấp ký tự. Trình tối ưu hóa là Adam (Kingma và Ba 2014) và tỷ lệ học và tỷ lệ dropout được liệt kê trong Phụ lục.

Để tập trung vào việc so sánh giữa các thiết kế mô hình khác nhau, chúng tôi không liệt kê kết quả của các mô hình giống BERT vì việc tăng cường dữ liệu và huấn luyện trước là một hướng trực giao.

--- TRANG 5 ---

Bảng 1: Tổng quan về các bộ dữ liệu và siêu tham số của nó, "H DIM,α, head DIM" chỉ ra chiều của các trạng thái ẩn, siêu tham số để kiểm soát phân phối tỷ lệ, chiều của mỗi đầu, tương ứng. Các tỷ lệ ứng viên là 1,3,N/16,N/8,N/4 cho các bộ dữ liệu SST,MTL-16,SNLI. Và chúng tôi sử dụng 1,3,5,7,9 cho các nhiệm vụ gán nhãn chuỗi. MTL-16† bao gồm 16 bộ dữ liệu, mỗi bộ có 1400/200/400 mẫu trong train/dev/test.

[Bảng dữ liệu với các cột: Dataset, Train, Dev., Test, |V|, H DIM, α, head DIM]

Bảng 2: Độ chính xác Test trên bộ dữ liệu SST.

[Bảng kết quả với các mô hình và độ chính xác của chúng]

Phân loại Văn bản

Các thí nghiệm Phân loại Văn bản được tiến hành trên bộ dữ liệu Stanford Sentiment Treebank(SST) (Socher et al. 2013) và MTL-16 (Liu, Qiu, và Huang 2017) bao gồm 16 bộ dữ liệu nhỏ trong các lĩnh vực khác nhau. Ngoài mô hình cơ sở mà chúng tôi đã giới thiệu trước đó, chúng tôi sử dụng MLP(Multi-Layer Perceptron) hai lớp với hàm softmax làm bộ phân loại. Nó nhận đặc trưng từ việc áp dụng max-pooling trên lớp trên cùng cộng với nút phân loại.

Tab-2 và 3 đưa ra kết quả trên SST và MTL-16. Transformer Đa tỷ lệ đạt được 1.5 và 3.56 điểm so với Transformer trên hai bộ dữ liệu này, tương ứng. Trong khi đó, Transformer Đa tỷ lệ cũng đánh bại nhiều mô hình hiện có bao gồm CNN và RNN.

Vì độ dài câu trung bình của bộ dữ liệu MTL-16 tương đối lớn, chúng tôi cũng báo cáo kết quả hiệu quả trong Hình-4. Chúng tôi thực hiện MS-Trans với Pytorch và DGL(Wang et al. 2019). Transformer Đa tỷ lệ đạt được tăng tốc 6.5 lần so với Transformer trên bộ dữ liệu MTL-16 trung bình (độ dài câu trung bình bằng 109 token). Tăng tốc tối đa đạt 10 lần (trung bình 201 token)

Bảng 3: Độ chính xác Test trên các bộ dữ liệu MTL-16. "SLSTM" đề cập đến sentence-state LSTM (Zhang, Liu, và Song 2018).

[Bảng kết quả chi tiết cho từng dataset trong MTL-16]

và Transformer Đa tỷ lệ có thể đạt được tăng tốc 1.8 lần trên các câu rất ngắn (trung bình 22 token).

Gán nhãn Chuỗi

Ngoài các nhiệm vụ sử dụng mô hình như một bộ mã hóa câu, chúng tôi cũng quan tâm đến hiệu quả của mô hình trên các nhiệm vụ gán nhãn chuỗi. Chúng tôi chọn nhiệm vụ gán nhãn Part-of-Speech (POS) và nhiệm vụ Nhận dạng Thực thể Có tên (NER) để xác minh mô hình của chúng tôi. Chúng tôi sử dụng ba bộ dữ liệu làm benchmark: bộ dữ liệu gán nhãn POS Penn Treebank (PTB) (Marcus, Santorini, và Marcinkiewicz 1993), bộ dữ liệu NER CoNLL2003 (Sang và Meulder 2003), bộ dữ liệu NER CoNLL2012 (Pradhan et al. 2012).

Kết quả trong Tab-4 cho thấy Transformer Đa tỷ lệ đánh bại

--- TRANG 6 ---

Bảng 4: Kết quả trên các nhiệm vụ gán nhãn chuỗi. Chúng tôi liệt kê "Kỹ thuật Nâng cao" ngoại trừ embedding được huấn luyện trước (GloVe, Word2Vec, JMT) trong các cột. "Char" chỉ ra các đặc trưng cấp ký tự, nó cũng bao gồm Đặc trưng Viết hoa, Đặc trưng Từ điển, v.v. "CRF" có nghĩa là một lớp Conditional Random Field bổ sung.

[Bảng chi tiết kết quả các mô hình trên các task sequence labeling]

[Hình 4: Thời gian test trên mỗi batch (kích thước batch là 128) trên bộ dữ liệu có độ dài dài nhất (IMDB), bộ dữ liệu có độ dài ngắn nhất (MR), và trung bình trên 16 bộ dữ liệu trong MTL-16.]

Transformer vanilla trên ba bộ dữ liệu gán nhãn chuỗi này, điều này phù hợp với các kết quả khác được báo cáo ở trên. Nó cho thấy Transformer Đa tỷ lệ có thể trích xuất các đặc trưng hữu ích cho từng vị trí.

Suy luận Ngôn ngữ Tự nhiên

Suy luận Ngôn ngữ Tự nhiên (NLI) là một phân loại yêu cầu mô hình đánh giá mối quan hệ của hai câu từ ba ứng viên, "entailment", "contradiction", và "neutral". Chúng tôi sử dụng benchmark được sử dụng rộng rãi Stanford Natural Language Inference (SNLI) (Bowman et al. 2015) để thăm dó khả năng của mô hình trong việc mã hóa câu, chúng tôi so sánh mô hình của chúng tôi với các mô hình dựa trên vector câu. Khác với bộ phân loại trong nhiệm vụ phân loại văn bản, chúng tôi theo công trình trước đó (Bowman et al. 2016) để sử dụng bộ phân loại MLP hai lớp nhận concat (r1,r2,|r1−r2|,r1−r2) làm đầu vào, trong đó r1,r2 là biểu diễn của hai câu và bằng đặc trưng được sử dụng trong nhiệm vụ phân loại văn bản.

Bảng 5: Độ chính xác Test trên bộ dữ liệu SNLI cho các mô hình dựa trên vector câu.

[Bảng kết quả các mô hình trên SNLI]

Như được hiển thị trong Tab-5, Transformer Đa tỷ lệ vượt trội so với Transformer và hầu hết các mô hình cổ điển, và kết quả có thể so sánh với state-of-the-art. Số được báo cáo của Transformer được thu được với việc lựa chọn siêu tham số heuristic, chúng tôi sử dụng Transformer ba lớp với dropout và weight decay nặng. Và vẫn còn một khoảng cách lớn so với Transformer Đa tỷ lệ. So sánh này cũng chỉ ra rằng dữ liệu huấn luyện có kích thước vừa (SNLI có 550k mẫu huấn luyện) không thể thay thế tính hữu ích của kiến thức tiên nghiệm.

Phân tích

Ảnh hưởng của Phân phối Tỷ lệ Như chúng tôi đã giới thiệu trong Eq. (11), chúng tôi kiểm soát phân phối tỷ lệ qua các lớp bằng một siêu tham số α. Trong phần này, chúng tôi đưa ra so sánh về việc sử dụng α khác nhau, trong đó giá trị dương có nghĩa là thiên kiến cục bộ tăng với sự giảm của độ sâu lớp và giá trị âm có nghĩa là thiên kiến toàn cục tăng với sự giảm của độ sâu lớp.

Như được hiển thị trong phần trên của Tab-6, thiên kiến cục bộ trong các lớp nông là một yếu tố chính để đạt được hiệu suất tốt, và một α dương phù hợp đạt kết quả tốt nhất. Ngược lại,

--- TRANG 7 ---

Bảng 6: Phân tích các phân phối tỷ lệ khác nhau trên tập test SNLI. Phần trên cho thấy ảnh hưởng của siêu tham số α thay đổi phân phối của các tỷ lệ qua các lớp. Năm ứng viên kích thước tỷ lệ là 1,3,N/16,N/8,N/4, tương ứng. Phần dưới liệt kê hiệu suất của các mô hình tỷ lệ đơn sử dụng tỷ lệ cố định cho toàn bộ mô hình.

[Bảng phân tích với các giá trị α khác nhau và mô hình single-scale]

tất cả các giá trị âm đều làm hại hiệu suất, điều đó có nghĩa là quá nhiều thiên kiến toàn cục trong các lớp nông có thể dẫn mô hình đến hướng sai. Quan sát của thí nghiệm này phù hợp với trực giác của chúng tôi, đặc trưng cấp cao là sự kết hợp của các thuật ngữ cấp thấp.

Đa tỷ lệ so với Đơn tỷ lệ Như chúng tôi đã tuyên bố trước đó, Transformer Đa tỷ lệ có thể thu thập kiến thức ở các tỷ lệ khác nhau tại mỗi lớp. Do đó, một câu hỏi đơn giản cần được đánh giá là liệu mô hình đa tỷ lệ có vượt trội so với mô hình đơn tỷ lệ hay không. Để trả lời câu hỏi này, chúng tôi so sánh Transformer Đa tỷ lệ với một số mô hình đơn tỷ lệ. Mô hình F,G,H,I có cùng số lớp và đầu chú ý với Transformer đa tỷ lệ, nhưng tỷ lệ của chúng được cố định.

Kết quả trong phần dưới của Tab-6 tiết lộ giá trị của Transformer Đa tỷ lệ, nó đạt được cải thiện 1.6 điểm so với mô hình đơn tỷ lệ tốt nhất. Và kết quả này cũng hỗ trợ rằng thiên kiến cục bộ là một thiên kiến quy nạp quan trọng cho nhiệm vụ NLP.

Công trình Liên quan

Các mô hình đa tỷ lệ điển hình Cấu trúc đa tỷ lệ đã được sử dụng trong nhiều mô hình NLP, nó có thể được thực hiện theo nhiều cách khác nhau. Chẳng hạn như các lớp xếp chồng (Kalchbrenner, Grefenstette, và Blunsom 2014; Kim 2014), cấu trúc cây (Socher et al. 2013; Tai, Socher, và Manning 2015; Zhu, Sobhani, và Guo 2015), thang thời gian phân cấp (El Hihi và Bengio 1995; Chung, Ahn, và Bengio 2016), gating theo lớp (Chung et al. 2015). Vì các mô hình này được xây dựng trên các mô-đun như RNN và CNN, thể hiện thiên kiến cục bộ nội tại theo thiết kế, tinh thần chung của việc giới thiệu đa tỷ lệ là để cho phép giao tiếp tầm xa. Ngược lại, Transformer cho phép giao tiếp tầm xa, vì vậy chúng tôi muốn đa tỷ lệ mang lại thiên kiến cục bộ.

Transformer với thiên kiến quy nạp bổ sung Công trình này không phải là nỗ lực đầu tiên giới thiệu thiên kiến quy nạp vào Transformer.

Shaw, Uszkoreit, và Vaswani (2018) đề xuất Transformer nên quan tâm đến khoảng cách tương đối giữa các token thay vì vị trí tuyệt đối trong chuỗi. Thông tin về khoảng cách tương đối có thể được thu được bằng cách nhìn đa tỷ lệ của cùng một vị trí, vì vậy mô hình của chúng tôi có thể nhận biết khoảng cách tương đối nếu sử dụng đủ tỷ lệ.

Li et al. (2018) đề xuất một regularization để tăng cường tính đa dạng của các đầu chú ý. Tự chú ý đa đầu đa tỷ lệ của chúng tôi có thể tạo ra sự phân chia lao động tốt của các đầu thông qua việc hạn chế chúng trong các tỷ lệ khác nhau.

Yang et al. (2018a) và Yang et al. (2018b) cũng giới thiệu thiên kiến cục bộ vào Transformer.

Khác với các mô hình trên, chúng tôi tập trung vào việc nhập khái niệm đa tỷ lệ vào tự chú ý. Trong khi đó, các mô hình của họ sử dụng cấu trúc đơn tỷ lệ. Kết quả thực nghiệm của chúng tôi đã cho thấy hiệu quả của cơ chế đa tỷ lệ.

Kết luận

Trong công trình này, chúng tôi trình bày Tự chú ý Đa tỷ lệ và Transformer Đa tỷ lệ kết hợp kiến thức tiên nghiệm về đa tỷ lệ và cơ chế tự chú ý. Kết quả là, nó có khả năng trích xuất các đặc trưng phong phú và mạnh mẽ từ các tỷ lệ khác nhau. Chúng tôi so sánh mô hình của chúng tôi với Transformer vanilla trên ba nhiệm vụ thực tế (21 bộ dữ liệu). Kết quả cho thấy đề xuất của chúng tôi vượt trội so với Transformer vanilla một cách nhất quán và đạt kết quả có thể so sánh với các mô hình state-of-the-art.

Lời cảm ơn

Công trình này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc (Số 2018YFC0831103), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 61672162), Dự án Chính Khoa học và Công nghệ Thành phố Thượng Hải (Số 2018SHZDZX01) và ZJLab.

Tài liệu Tham khảo

[Danh sách tài liệu tham khảo được dịch theo format gốc...]

--- TRANG 8 ---

[Tiếp tục danh sách tài liệu tham khảo...]