# 2010.03688.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2010.03688.pdf
# File size: 590826 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AxFormer: Accuracy-driven Approximation of
Transformers for Faster, Smaller and more Accurate
NLP Models
Amrit Nagarajan, Sanchari Sen¬ß, Jacob R. Stevens+and Anand Raghunathan
School of ECE, Purdue University
fnagaraj9, sen9, steven69, raghunathan g@ purdue.edu
Abstract ‚ÄîTransformers have greatly advanced the state-of-
the-art in Natural Language Processing (NLP) in recent years,
but present very large computation and storage requirements.
We observe that the design process of Transformers (pre-train a
foundation model on a large dataset in a self-supervised manner,
and subsequently Ô¨Åne-tune it for different downstream tasks)
leads to task-speciÔ¨Åc models that are highly over-parameterized,
adversely impacting both accuracy and inference efÔ¨Åciency. We
propose AxFormer , a systematic framework that applies accuracy-
driven approximations to create optimized transformer models
for a given downstream task. AxFormer combines two key
optimizations ‚Äî accuracy-driven pruning and selective hard
attention. Accuracy-driven pruning identiÔ¨Åes and removes parts
of the Ô¨Åne-tuned transformer that hinder performance on the
given downstream task. Sparse hard-attention optimizes attention
blocks in selected layers by eliminating irrelevant word aggre-
gations, thereby helping the model focus only on the relevant
parts of the input. In effect, AxFormer leads to models that
are more accurate, while also being faster and smaller . Our
experiments on GLUE and SQUAD tasks show that AxFormer
models are up to 4.5% more accurate, while also being up
to 2.5 faster and up to 3.2 smaller than conventional Ô¨Åne-
tuned models. In addition, we demonstrate that AxFormer
can be combined with previous efforts such as distillation or
quantization to achieve further efÔ¨Åciency gains. Code is available
at https://github.com/amrnag/Specialized-Transformers.
I. I NTRODUCTION
Transformer models [2], [15], [1] have revolutionized the
Ô¨Åeld of Natural Language Processing (NLP), greatly advancing
the state-of-the-art in many NLP tasks. Models that achieve
good performance on these tasks are of high practical sig-
niÔ¨Åcance, Ô¨Ånding their place in commercial applications such
as social media monitoring (sentiment analysis), AI chat as-
sistants (question answering), automated summarization tools
(analyzing sentence similarity), etc.Therefore, there is a strong
interest in creating more accurate and efÔ¨Åcient models for
these tasks.
Transformers are Ô¨Årst pre-trained on very large datasets,
enabling them to capture rich linguistic knowledge and gain
a deep understanding of the structure of the target language.
Subsequent Ô¨Åne-tuning reÔ¨Ånes this knowledge for a speciÔ¨Åc
downstream task by training a task-speciÔ¨Åc Ô¨Ånal layer. How-
ever, we Ô¨Ånd that the current method of pre-training and Ô¨Åne-
tuning transformers has two major drawbacks. First, large
¬ßCurrently at IBM T.J. Watson Research Center (sanchari.sen@ibm.com).
+Currently at Meta Platforms Inc. (jrstevens@fb.com).pre-trained transformers are highly over-parameterized for
downstream tasks, especially since many of these tasks have
very limited training data. This leads to unstable models [3]
with sub-optimal generalization ability [12]. Second, these
large Ô¨Åne-tuned models present high computation and storage
requirements for inference. This problem is compounded by
the trend towards larger models, driven by the need for higher
accuracy and tackling more complex tasks. For instance,
increasing the number of parameters from 1.5B to 175B
enabled a reduction in perplexity for Language Modeling (on
the Penn Treebank dataset) from 35.8 in GPT-2 (2019) to
20.5 in GPT-3 (2020). In this work, we address both these
challenges, taking advantage of the over-parameterized nature
of pre-trained models to create individually optimized models
for the different downstream tasks that are smaller, faster and
more accurate.
We propose AxFormer, a framework that combines two
key optimizations ‚Äì accuracy-driven pruning and selective
replacement of ‚Äúsoft‚Äù self-attention with ‚Äúhard‚Äù self-attention.
Accuracy-driven pruning identiÔ¨Åes and removes elements (we
deÔ¨Åne elements as parameters grouped at different levels of
granularity, i.e., self-attention blocks, feed-forward neural net-
work blocks, attention heads and neurons) of the transformer
that hinder performance on a speciÔ¨Åc downstream task, with
the goal of maximizing accuracy. In contrast to prior pruning
methods that prune elements with little-or-no impact on the
network output, the proposed method prunes elements that
actually have a considerable impact on the output, leading
to the highest positive impact on accuracy. In order to reduce
the large pruning space, we consider the different elements
of the Ô¨Åne-tuned transformer for pruning in a hierarchical
manner, starting with entire self-attention or feed-forward
neural network blocks, followed by attention heads, and Ô¨Ånally
individual neurons.
The core of the transformer is self-attention, where each
token in the input builds its representation based on the extent
of attention it places on all the other tokens. However, we
observe that in some layers, restricting the attention span
of each token to only focus on the relevant tokens leads to
better information Ô¨Çow inside the model. AxFormer identiÔ¨Åes
the appropriate layers and replaces the ‚Äúsoft‚Äù self-attention
with ‚Äúhard‚Äù self-attention in these layers. AxFormer does
not require any additional re-training or Ô¨Åne-tuning (therebyarXiv:2010.03688v2  [cs.CL]  10 Jun 2022

--- PAGE 2 ---
enabling it to scale to large transformer models), and can be
applied in a plug-and-play manner to any Transformer model
that is Ô¨Åne-tuned for any downstream task.
We summarize our main contributions as follows:
We introduce AxFormer , a framework that optimizes
transformer models through approximate computing for
speciÔ¨Åc downstream tasks.
We propose accuracy-driven pruning to identify and elim-
inate elements that are harmful to performance on the
downstream task.
We propose the selective replacement of soft self-
attention with hard attention in certain layers, helping the
model to focus only on the relevant parts of the input to
build better representations.
Across a suite of different transformer networks and
downstream tasks, we demonstrate that AxFormer models
are consistently more accurate and stable, while also
being signiÔ¨Åcantly faster and smaller than their (conven-
tional) Ô¨Åne-tuned counterparts.
II. R ELATED WORK
We discuss relevant previous effors by categorizing them
along three directions - task-agnostic optimizations, task-
speciÔ¨Åc optimizations and pruning.
Task-agnostic optimizations. Given the popularity of trans-
former models, several techniques have been proposed to
overcome their computational and memory challenges, for
efÔ¨Åcient inference. A vast majority of these works introduce
task-agnostic optimizations by pre-training efÔ¨Åcient models
from scratch. Some notable examples include DistilBERT [18]
and MobileBERT [20], which use knowledge distillation to
train smaller and faster networks using the original network
as a teacher. LayerDrop [4] randomly drops layers during pre-
training, thereby enabling their dropping during inference. Lite
Transformer [23] uses Long-Short Range Attention to speed
up the self-attention operation. AlBERT [9] uses factorized
embeddings and cross-layer parameter sharing. Q8BERT [26]
quantizes all weights and activations in the model to 8-bit
integers. Using DistilBERT and Q8BERT as examples, we
demonstrate that our techniques are complementary to these
works, and can be applied to derive beneÔ¨Åts over and beyond
them.
Task-speciÔ¨Åc optimizations. QBERT [19] and TinyBERT [8]
perform task-speciÔ¨Åc quantization and knowledge distillation,
but the gain in efÔ¨Åciency comes at the cost of degradation
in accuracy on the downstream task. In contrast, AxFormer
models are simultaneously and more efÔ¨Åcient.
Pruning techniques. Pruning has been applied to various
classes of neural networks, including transformers ([5], [13],
[17]), and pruning strategies have been explored to identify
weights and parameters that the output is least sensitive
to ([6], [25]). In contrast, AxFormer identiÔ¨Åes and prunes
parameters that have the most detrimental effect on the output,
leading to simultaneous gains in accuracy and efÔ¨Åciency. Using
the popular Lottery Ticket Hypothesis [5] as an illustrativeexample, we demonstrate that our accuracy-driven pruning
method is complementary to previously proposed pruning
techniques, and hence, they can be combined to produce even
more efÔ¨Åcient models for a given accuracy constraint.
III. T HEAXFORMER FRAMEWORK
The AxFormer framework for producing optimized trans-
former models for speciÔ¨Åc downstream tasks is illustrated in
Algorithm 1. It performs two key optimizations: (1) Identify
and prune elements that hinder performance on the given
downstream task (lines 16-25), and (2) Selectively replace soft
self-attention with hard self-attention to help the model focus
only on the relevant parts of the input (lines 9-15). We describe
these steps in the following subsections.
Algorithm 1: The AxFormer Framework
Input : Fine-tuned (for the given downstream task)
Transformer T, Validation set D
Output: Optimized AxFormer model for the given
downstream task T
1Function analyze _element (element E ):
2 Tpruned =T E
3 New Loss =Evaluate (Tpruned ; D)
4 ifNew Loss < Min Loss and
num samples helped > 0:5cardinality (D)then
5 Min Loss =New Loss
6 T =Tpruned
7Min Loss = Evaluate (T,D)
8Q =Order elements forinspection (T; D )
9foreach layer L in T do
10 Replace soft self-attention in L with hard self-attention
11 New Loss =Evaluate (T; D )
12 ifNew Loss < Min Loss and
num samples helped > 0:5cardinality (D)then
13 Min Loss =New Loss
14 else
15 Restore soft self-attention in L
16while Q is not empty do
17 TrialElement = Q.pop()
18 analyze element (TrialElement)
19 ifTrialElement has not been pruned from T and
New Loss < 1:1Min loss then
20 ifTrialElement is an attention block then
21 foreach attention head h in TrialElement do
22 analyze element (h)
23 else if TrialElement is a feed-forward block then
24 foreach neuron w in TrialElement do
25 analyze element (w)
26return T
A. Accuracy-driven pruning
The problem of identifying an optimal set of elements to
prune in order to maximize accuracy is challenging, and this
is especially true for transformers for two reasons. First, trans-
formers have billions of parameters, leading to an enormous
pruning search space. Second, the iterative fapproximate, Ô¨Åne-
tune, approximategcycle used in previously proposed pruning

--- PAGE 3 ---
methods (to recover accuracy loss from pruning) does not
work for large transformers during Ô¨Åne-tuning, since they very
quickly overÔ¨Åt the limited training data for the downstream
tasks (usually within 2-3 epochs). We address both these
challenges through the use of a hierarchical greedy algorithm
that does not require any additional training or Ô¨Åne-tuning
(lines 16-25 in Alg. 1). To determine the signiÔ¨Åcance of each
transformer element for the given downstream task, we Ô¨Årst
Ô¨Åne-tune the original transformer model for the given down-
stream task to obtain the baseline loss. Then, for the element
under consideration in each iteration of the framework, we
compute the loss of the current transformer model with the
element removed. We prune the element under consideration if
the validation loss when it is removed is less than the minimum
loss seen thus far during the optimization process, since the
goal is to Ô¨Ånd a model with minimum loss. As a result,
contrary to previously proposed pruning methods, accuracy-
driven pruning can be seen as a form of training, since the
objective of accuracy-driven pruning ‚Äì minimizing loss on the
(validation) dataset ‚Äì is exactly the same as the objective of
training (see Appendix C). Also, in order to prevent overÔ¨Åtting
to the validation set, we introduce a generalization constraint in
addition to the aforementioned loss condition. This constraint
ensures that an element is pruned only if it decreases the
loss of more than half the total number of samples in the
validation set (computed by num samples helped function
in Alg. 1). Therefore, elements are pruned only if a majority
of the samples in the validation set beneÔ¨Åt from their removal,
resulting in improved generalization performance. If the loss
with the element removed is slightly greater than the minimum
loss seen so far, we inspect the element at a Ô¨Åner granularity,
and prune only parts of the element that hinder performance
(rather than pruning the entire element).
1)Hierarchical processing of elements :It is compu-
tationally prohibitive to analyze every single parameter in
large transformers using the method described in Alg. 1.
Since the framework iterates through the entries of the queue
sequentially, its efÔ¨Åcacy is dependent on both the total number
of elements under consideration, and the time required to
analyze each element. We take advantage of the inherently hi-
erarchical structure of transformers and consider the elements
in a hierarchical manner, ordered by increasing granularity.
SpeciÔ¨Åcally, we analyze entire feed-forward and self-attention
blocks Ô¨Årst, and inspect them at Ô¨Åner granularity (attention
heads and neurons) only when required. Through this ordering,
we are able to quickly eliminate large numbers of parameters
from further consideration. In addition, due to the over-
parameterized nature of transformers, it is likely that time-
consuming blocks are pruned from the transformer earlier
in the process, thereby speeding up future iterations of the
framework. For example, eliminating a single feed-forward
block in the BERT-Base model removes 5.6% of all parameters
under consideration, and speeds up future iterations by 1.15 .
To further reduce the number of elements under consideration,
we also dynamically remove elements if they are encompassed
by a high-importance block. For example, if a given self-attention block is determined to be of high importance (the
validation loss with the block removed is much greater than
the minimum loss seen so far), we remove all heads within
that block from further consideration.
2)Creating an ordered queue of elements for inspection :
Since our framework performs greedy pruning of highly over-
parameterized models, the order in which elements are in-
spected has a signiÔ¨Åcant impact on the quality (accuracy, speed
and size) of the Ô¨Ånal AxFormer model. Some elements may ap-
pear to be harmful for the downstream task (especially in early
iterations of the framework), but this often ends up being an
artifact of over-parameterization. As a result, when elements
are not carefully ordered for inspection, our framework lands
in bad local minima of the validation loss function, where loss
cannot be reduced further through pruning. Our solution to
this problem utilizes the unique linguistic properties captured
by the different transformer layers [7] to guide the ordering
of elements for inspection (line 8 in Alg. 1) For example,
it was found that BERT captures phrase-level information in
the lower layers, mapping related tokens together. The lower
layers also capture surface features, while the middle layers
capture syntactic features and higher layers capture semantic
features. It was also observed that BERT requires deeper layers
only when long-range dependency information is required.
Different tasks require different types of linguistic knowledge.
For example, sentiment analysis requires only local context,
and long-range information often ends up confusing the model,
since sentiments often change rapidly; it is also unlikely that
syntactic and semantic information are needed (see Appendix
A). Hence, we place the Ô¨Ånal layer at the front of the queue,
and work our way backwards towards the Ô¨Årst layer, since
blocks in the Ô¨Ånal layers are more likely to hinder performance
on sentiment analysis. This ordering ensures that elements
that are pruned early in our framework (when the model is
most over-parameterized) do not lead the system into bad
local minima, since elements that are pruned are expected to
hinder performance based on prior insights into the working
of transformers. We note that the use of prior knowledge about
different tasks is only a heuristic to minimize the overheads of
the AxFormer framework. All the exact results in this paper
can be obtained without assuming any knowledge about the
downstream task (but at the cost of additional overheads),
using the procedure described in Appendix B.
B. Selective use of hard self-attention
In traditional transformer architectures, the self-attention
operation computes the attention scores of each token in
the input sequence with all the other tokens. These attention
scores, after passing through the softmax operation, are used
to build the new representation for the token based on the
extent of attention it places on the other tokens. The use of
‚Äúsoft‚Äù attention enables end-to-end training of the transformer.
However, we observe that considering only the top-K attention
scores for each token (K=30 is optimal for all of our studied
tasks) in selected layers after training and Ô¨Åne-tuning is
completed helps the transformer focus only on the relevant

--- PAGE 4 ---
parts of the input sequence (Fig. 1). This leads to better
information Ô¨Çow inside the model, thereby improving accuracy
on downstream tasks (especially those involving sequence
classiÔ¨Åcation and question answering). This also introduces
a large amount of activation sparsity that sparse accelerators
can exploit for faster inference, and helps alleviate the critical
memory bottleneck in transformers. We note that replacing soft
attention with hard attention in all layers (especially the deeper
layers) leads to loss of accuracy, necessitating its selective use.
In order to identify the layers that beneÔ¨Åt from hard attention,
we replace the soft attention with hard attention in each layer
(one by one), and use it only when it leads to smaller validation
loss.
Fig. 1. Illustrations of (a) Traditional ‚Äúsoft‚Äù self-attention and (b) ‚ÄúHard‚Äù
self-attention for the word ‚Äúhe‚Äù. In hard self-attention, ‚Äùhe‚Äù concentrates all
of its focus on ‚ÄúJohn‚Äù, while in soft self-attention, ‚Äúhe‚Äù places a small amount
of attention on the other irrelevant words also. Therefore, hard self-attention
helps build a better representation for ‚Äúhe‚Äù.
IV. E XPERIMENTAL RESULTS
We implemented our techniques within Huggingface‚Äôs
Transformers library in PyTorch [22]. We used Intel AI‚Äôs
NLP Architect for experiments on Q8BERT. The experiments
were performed on a GeForce RTX 2080 Ti GPU with 11GB
memory. All results are reported on the test set, averaged
across 10 runs with random seeds after 3 epochs of Ô¨Åne-tuning,
unless otherwise speciÔ¨Åed. We randomly sample 15% of the
training set with class balance, and use it as the validation set
in the AxFormer framework. Thus, the test set is not used in
AxFormer.
A. AxFormer models are faster, smaller and more accurate
than conventional Ô¨Åne-tuned models
We present results on GLUE [21], a set of Language Under-
standing tasks, and SQUADv1.1 [16], a Question Answering
task, in Table I. For GLUE, we present results on the test set
using the GLUE evaluation server to obtain the scores [24],
and for SQUAD, we present results on the dev set. AxFormer
models are up to 4.5% more accurate, while also being up to
2.5faster and up to 3.2 smaller than the baseline . In ad-
dition, AxFormer models show substantial improvements over
Q8BERT-base (which is already 4 smaller than BERT-base
due to the use of 8-bit integer quantization) and DistilBERT-
base (which is already 60% faster and smaller than BERT-
base). AxFormer versions of Q8BERT-base and DistilBERT-
base models exceed the accuracy of BERT-base models, while
being up to 3.7faster and 12.1smaller than BERT-base .
Therefore, AxFormer is complementary to, and can be used inconjunction with, prior quantization and distillation methods.
The beneÔ¨Åts of AxFormer are further analyzed in Appendix
C.
B. AxFormer models are less sensitive to random seed initial-
ization
Previous research [3] has shown that the random seed used
to determine the initialization of the task-speciÔ¨Åc layer and the
order of training data for the downstream task has a signiÔ¨Åcant
impact on the quality of the Ô¨Åne-tuned models. When the
amount of training data is small (which is the case with most
downstream NLP tasks), this effect becomes more pronounced,
evidenced by the fact that WNLI, which has the least amount
of training samples (634 training samples), exhibits highest
variance across runs. Therefore, in order to Ô¨Ånd a model with
high accuracy on a downstream task, multiple models need
to be created (using different random seeds), and evaluated.
In Table II, we demonstrate that AxFormer models exhibit
signiÔ¨Åcantly less sensitivity to random seeds than their non-
AxFormer counterparts, thereby greatly increasing the odds of
Ô¨Ånding ‚Äúgood‚Äù models in fewer iterations.
C. Large transformers beneÔ¨Åt greatly from AxFormer
Current state-of-the-art transformer networks (such as GPT-
3 [1]) have hundreds of billions of parameters. Model sizes
are also expected to grow further in the future as increasing
the number of parameters has been shown to improve per-
formance. This makes it computationally challenging to train
transformers as well as perform inference using them. Recent
research [10] has shown that larger models converge in a
signiÔ¨Åcantly smaller number of training iterations than smaller
models, and hence they train faster in spite of requiring more
time per iteration. However, larger models are signiÔ¨Åcantly
slower than smaller models at inference time. In Table III,
we demonstrate that AxFormer applied to larger transformers
achieves much higher accuracy while being comparable in
speed to AxFormer models of smaller transformers. This
demonstrates the scalability of AxFormer to larger models.
D. Accuracy-driven pruning is complementary to conventional
pruning techniques
Fig. 2. The winning tickets of conventional
Ô¨Åne-tuned and AxFormer BERT-Base on
MRPC.Accuracy-driven
pruning identiÔ¨Åes
and prunes harmful
elements of the
transformer for the
downstream task at
hand. Techniques
that prune elements
that have minimal
impact on the output
are complementary
to our techniques,
and we demonstrate
this using the popular Lottery Ticket Hypothesis [5], which
Ô¨Ånds sparse sub-networks that can be trained to match

--- PAGE 5 ---
TABLE I
Accuracy, speedup and compression of AxFormer models on GLUE and SQUAD v1.1 tasks. We report Matthews correlation for CoLA, Pearson
Correlation for STS-B and accuracy for all other tasks. We report only ‚Äúmatched‚Äù accuracy for MNLI and the Exact Match score for SQUAD. Speedup and
Compression are reported over the non-AxFormer baselines for DistilBERT and Q8BERT, and not over BERT-base.
SQUAD CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI Average
BERT-Base 81.97 51.38 85.37 83.11 90.74 89.76 62.65 95.94 83.8 64.1 78.88
AxFormer 83.88 54.43 86.63 85.88 91.17 91.12 65.81 96.7 85.08 65.1 80.58
Speedup 1.38X 1.98X 1.21X 1.61X 1.16X 1.49X 2.2X 1.32X 1.71X 2.51X 1.67X
Compression 1.52X 2.88X 1.6X 2.02X 1.31X 1.48X 2.69X 1.44X 1.38X 3.18X 1.95X
Q8BERT-Base 81.55 50.5 84.73 82.05 90.19 89.65 61.26 94.32 83.09 61.21 77.86
AxFormer 82.52 54.12 85.95 84.77 91.08 91.44 65.8 96.02 84.14 63.1 79.73
Speedup 1.31X 2.28X 1.08X 1.57X 1.3X 1.26X 2.28X 1.15X 1.58X 2.44X 1.63X
Compression 1.41X 3.02X 1.24X 1.47X 1.2X 1.38X 2.74X 1.21X 1.46X 2.82X 1.8X
DistilBERT-Base 79.94 50.5 83.04 81.07 89.72 88.92 62.01 93.88 83.49 61.3 77.39
AxFormer 81.91 54.08 85.47 85.02 91.01 89.2 64.5 95.48 84.16 63.3 79.41
Speedup 1.21X 1.84X 1.09X 1.34X 1.07X 1.21X 1.94X 1.08X 1.31X 2.31X 1.44X
Compression 1.28X 1.96X 1.14X 1.41X 1.08X 1.14X 1.7X 1.17X 1.28X 1.98X 1.41X
XLNet-Base 82.28 51.98 85.93 83.68 91.01 89.83 62.01 95.88 84.12 64.8 79.15
AxFormer 83.93 54.28 86.8 85.92 91.78 91.12 65.78 96.98 85.46 65.1 80.71
Speedup 1.48X 2.06X 1.38X 1.98X 1.47X 1.78X 2.11X 1.59X 1.93X 2.46X 1.83X
Compression 1.55X 2.84X 1.72X 2.12X 1.56X 1.6X 2.48X 1.71X 1.65X 2.98X 2.02X
TABLE II
Sensitivity to random seeds. Results reported are averaged across the
GLUE tasks and SQUAD on the Base models of each transformer.
TransformerAverage
variance
across
tasksMaximum
variance
(single
task)Average of
maximum
task scoresAverage of
minimum
task scores
BERT 1.48 4.86 80.05 76.53
AxFormer 0.86 1.95 82.02 79.93
Q8BERT 1.61 4.62 79.01 75.29
AxFormer 1.1 2.02 81.14 79.58
DistilBERT 1.29 3.85 78.78 74.89
AxFormer 0.83 1.78 80.96 79.59
TABLE III
AxFormer models of BERT-base and BERT-large. Results reported are
averaged across the GLUE tasks and SQUAD.
Transformer Accuracy Samples/second
BERT-Base 78.88 0.21
AxFormer 80.58 0.12
BERT-Large 80.98 0.45
AxFormer 82.89 0.15
the performance of the large network. The Lottery Ticket
Hypothesis has been successfully applied to transformers
([11], [14]). We demonstrate that the winning tickets of
the AxFormer model are consistently more accurate than
the winning tickets of conventional Ô¨Åne-tuned models for a
comparable inference time (Figure 2).
E. AxFormer provides insights into the working of transform-
ers
We analyze which elements of the transformer are pruned
for different downstream tasks using different transformer
models (Fig. 3). We Ô¨Ånd that the differences in importance
of elements are more pronounced across different tasks than
across different models. For example, for sentiment analysis,
long-range dependency information is not required, and often
ends up confusing the model. Hence, for all models Ô¨Åne-
tuned for sentiment analysis, we observe that componentsin later layers (closer to the output) are more likely to be
pruned. This is not the case with Language Modeling tasks
(predicting masked words in text), where longer attention
spans are required. Across models, we only observe subtle
differences. For example, we Ô¨Ånd that XLNet (auto-regressive)
is able to learn important task-speciÔ¨Åc information earlier than
BERT (non auto-regressive), similar to the observation made in
[17]. Hence, we are able to drop more components (in earlier
layers) in XLNet than in BERT, leading to more efÔ¨Åcient
models for inference. In DistilBERT (a distilled model), we
Ô¨Ånd that there is a clear demarcation in linguistic knowledge
across layers due to the reduced capacity of the model. This is
evidenced by the fact that elements in the top four layers are
never pruned across all Language Understanding tasks, while
the boundaries are more soft in the original models. We also
observe that Hard Attention is most useful in the lower layers,
where phrase-level information is captured, for all the models
and tasks.
F . Accuracy-driven pruning using previously proposed impor-
tance estimation techniques leads to less accurate models
L1-norm and magnitude-based pruning techniques are de-
signed to prune elements that have minimal impact on the
output. Hence, they are not effective at pruning elements that
have detrimental impact on output. Gradient-based methods
such as Taylor expansion are not reliable for increasing
accuracy on the downstream tasks, since transformers cannot
be repeatedly Ô¨Åne-tuned to recover the accuracy losses from
approximating the model (they very quickly overÔ¨Åt the limited
training data for the downstream tasks). In addition, while
knowledge distillation is complementary to our method (as
evidenced by our results on DistilBERT), we show that the
accuracy gains from AxFormer are signiÔ¨Åcantly higher than
those from regularized knowledge distillation proposed in [6].
Super-tickets [11] demonstrate that accuracy can be improved
by Ô¨Ånding winning lottery tickets at low levels of pruning.
In Table IV, we Ô¨Ånd that alleviating the over-parameterization

--- PAGE 6 ---
Fig. 3. Elements pruned for (a) downstream tasks and (b) Transformer architectures across GLUE and SQUAD in our most accurate models for
each task. Here, ATTN-Approx means only certain attention heads inside the attention block are pruned, and FFN-Approx means only certain neurons inside
the feed-forward block are pruned.
issue by pruning elements that hinder performance (and the
selective use of hard attention) leads to more accurate models
than pruning elements that have least impact on output in the
original model.
TABLE IV
Results of pruning with previously proposed methods (MRPC with
BERT-Base). For L1-norm, magnitude and Taylor (absolute gradient of
loss), we prune 5% of the least important weights at a time and record the
test accuracy. We report the highest test accuracy seen in this process.
Pruning Method Accuracy
Baseline (No Pruning) 83.11
L1-Norm 83.16
Magnitude 83.18
Taylor Expansion (with additional Ô¨Åne-tuning) 83.19
Taylor Expansion (no additional Ô¨Åne-tuning) 83.32
Regularized Knowledge Distillation (DynaBERT) 83.46
Super-ticket 84.62
Ours 85.88
G. AxFormer runtime
The AxFormer framework does not require any additional
training or Ô¨Åne-tuning iterations. It only requires multiple
passes through a small validation set. Hierarchical processing
ensures that the number of elements to inspect is greatly
reduced. In addition, each pass is expected to become pro-
gressively faster, since the framework potentially prunes an
element in each iteration, and our ordering of elements ensures
that large, time-consuming blocks are pruned early in the pro-
cess, making future iterations faster. Therefore, the runtimes
of AxFormer are quite small compared to the time required for
re-training or additional Ô¨Åne-tuning. We Ô¨Ånd that the average
wall-clock time for AxFormer is only 7.6 minutes for BERT-
base and 18.3 minutes for BERT-large on a single NVIDIA
RTX 2080Ti GPU.
We also evaluate the heuristics used in our framework (Table
V). When elements are not carefully ordered for inspection,we Ô¨Ånd that elements that are pruned in early iterations of the
framework are pruned due to over-parameterization, and not
because the linguistic knowledge contained in these elements
is harmful/not useful for the task at hand. This leads the
system into bad local minima, where the validation loss cannot
be further reduced by pruning other elements. We observe
that starting at the layer granularity leads to worse models
than starting at the block granularity. This is because the
attention and feed-forward blocks in each layer have vastly
differently functionality, and hence, the effect of removing an
attention block of relatively high signiÔ¨Åcance is often masked
by removing the feed-forward block that greatly hinders per-
formance in the same layer (and vice-versa), causing the entire
layer to be pruned. This also leads the system into bad local
minima, thereby creating inferior models. To establish that our
greedy approach combined with a global error bound does not
lead to inferior models, we also experiment with an adaptive
loss threshold. In particular, we use a very tight constraint
when analyzing elements at coarse granularities, and relax the
constraint as we move towards Ô¨Åner granularities. We again
Ô¨Ånd that there is negligible change in the quality of the Ô¨Ånal
model produced (the accuracy is slightly lower, possibly due to
over-Ô¨Åtting to the validation set), but the AxFormer overheads
are signiÔ¨Åcantly larger. We hypothesize that a single global
error bound is sufÔ¨Åcient because we order the elements in such
a way that for the given task at hand, we intuitively expect
that the elements at the head of the queue are likely to be
removed using the linguistic knowledge in different layers.
V. C ONCLUSION
We propose AxFormer, a framework to optimize trans-
formers for different downstream tasks. The framework iden-
tiÔ¨Åes and prunes elements that hinder performance on the
downstream task at hand. We also demonstrate the advantage

--- PAGE 7 ---
TABLE V
[Right] Accuracy of the resulting models and runtime of AxFormer
with different heuristics (MRPC with BERT-Base). All heuristics use
hierarchical processing of ordered elements with Selective Hard Attention,
unless otherwise speciÔ¨Åed.
HeuristicAxFormer
accuracyRuntime
(minutes)
Randomly ordered elements 83.42 7.2
Start by inspecting layers 83.44 5.8
No Selective Hard Attention 84.94 6.3
Adaptive Threshold 85.82 29.3
Ours 85.88 6.6
of selectively using hard self-attention in selected layers to
improve information Ô¨Çow. Using this framework, we produce
models that are more accurate, while also being faster and
smaller than conventional Ô¨Åne-tuned models.
Acknowledgment: This work was supported by C-BRIC, one
of six centers in JUMP, a Semiconductor Research Corporation
(SRC) program sponsored by DARPA.
REFERENCES
[1] Tom B. Brown, Benjamin Mann, and Nick Ryder et al. Language models
are few-shot learners. CoRR , 2020.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
BERT: pre-training of deep bidirectional transformers for language
understanding. In NAACL-HLT 2019 .
[3] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh
Hajishirzi, and Noah A. Smith. Fine-tuning pretrained language models:
Weight initializations, data orders, and early stopping. CoRR , 2020.
[4] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer
depth on demand with structured dropout. In ICLR 2020 .
[5] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis:
Finding sparse, trainable neural networks. In ICLR 2019 .
[6] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun
Liu. Dynabert: Dynamic BERT with adaptive width and depth. In
NeurIPS 2020 .
[7] Ganesh Jawahar, Beno ÀÜƒ±t Sagot, and Djam ¬¥e Seddah. What does BERT
learn about the structure of language? In Anna Korhonen, David R.
Traum, and Llu ¬¥ƒ±s M `arquez, editors, ACL 2019 .
[8] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin
Li, Fang Wang, and Qun Liu. Tinybert: Distilling BERT for natural
language understanding. CoRR , 2019.
[9] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-
supervised learning of language representations. In ICLR 2020 .
[10] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan
Klein, and Joey Gonzalez. Train big, then compress: Rethinking model
size for efÔ¨Åcient training and inference of transformers. In ICML 2020 .
[11] Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu,
Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained
language models: From model compression to improving generalization.
InACL/IJCNLP 2021 .
[12] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really
better than one? In NeurIPS 2019 .
[13] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan
Kautz. Pruning convolutional neural networks for resource efÔ¨Åcient
inference. In ICLR 2017 .
[14] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT plays
the lottery, all tickets are winning. In EMNLP 2020 .
[15] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and
Ilya Sutskever. Language models are unsupervised multitask learners.
2019.
[16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
Squad: 100, 000+ questions for machine comprehension of text. In
EMNLP 2016 .
[17] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. Poor
man‚Äôs bert: Smaller and faster transformer models, 2020.[18] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
Distilbert, a distilled version of BERT: smaller, faster, cheaper and
lighter. CoRR , 2019.
[19] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir
Gholami, Michael W. Mahoney, and Kurt Keutzer. Q-BERT: hessian
based ultra low precision quantization of BERT. In AAAI 2020 .
[20] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and
Denny Zhou. Mobilebert: a compact task-agnostic BERT for resource-
limited devices. In ACL 2020 .
[21] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis
platform for natural language understanding. In ICLR 2019 .
[22] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ¬¥emi Louf, Morgan
Funtowicz, and Jamie Brew. Huggingface‚Äôs transformers: State-of-the-
art natural language processing. CoRR , 2019.
[23] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite
transformer with long-short range attention. In ICLR 2020 .
[24] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
Bert-of-theseus: Compressing BERT by progressive module replacing.
CoRR , 2020.
[25] Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans,
and Qiang Liu. Good subnetworks provably exist: Pruning via greedy
forward selection. In ICML 2020 .
[26] OÔ¨År Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
Q8BERT: quantized 8bit BERT. CoRR , 2019.
APPENDIX
A. AxFormer without using prior knowledge about tasks
The use of prior knowledge about tasks is only a heuristic
to help reduce the overheads of AxFormer. All the exact
results in this paper can be obtained without the use of
this prior knowledge. Prior research [7] has shown that the
linguistic knowledge contained in different transformer layers
can be demarcated into three main functional regions: phrase-
level information in bottom layers (the layers closest to the
input), semantic and syntactic information in the middle layers,
and long-range dependency information in the top layers.
Following these insights into the inner workings of transformer
models, we consider all possible (3! = 6) orderings of these
functional regions for pruning (Table VI), i.e., we create 6
different AxFormer models, each with elements inspected in
different orders. Then, the AxFormer model with minimum
validation loss is chosen as the Ô¨Ånal model for testing. The
quality of the AxFormer model is worst when the blocks con-
taining the most important linguistic knowledge are inspected
Ô¨Årst (bottom layers for sentiment analysis, see Appendix B),
since these blocks may get pruned simply to alleviate the over-
parameterization. In addition, we Ô¨Ånd that the ordering of ele-
ments within the same functional block (for example, elements
in layer 10 before layer 11 or vice-versa) has negligible impact
on the quality of the AxFormer model ( <0.2 absolute points in
all our studied tasks), further demonstrating the existence of
regions with distinct types of linguistic knowledge. Without
the use of prior knowledge, the average wall-clock time of
AxFormer for BERT-Base increases from 7.6 minutes to 17.2
minutes on a single RTX 2080Ti GPU. The increase is not
six-fold because in most orderings, the system enters ‚Äúbad‚Äù
local minima very quickly. As a result, most of the elements
inspected are deemed to be of high importance, and are not
inspected at Ô¨Åner granularity (in fact, most orderings require

--- PAGE 8 ---
less than 30 iterations in Alg. 1 for accuracy-driven pruning
of BERT-Base models).
TABLE VI
AxFormer accuracy from inspecting elements in different orders on
SST-2 using BERT-Base. The baseline validation and dev accuracy of the
Ô¨Åne-tuned model are 93.98 and 95.94, respectively.
Order of inspectionValidation accuracy
of AxFormer modelTest accuracy of
AxFormer model
Bottom, Middle, Top 94.06 95.98
Bottom, Top, Middle 94.38 96.01
Middle, Bottom, Top 94.44 96.12
Middle, Top, Bottom 95.22 96.28
Top, Bottom, Middle 95.18 96.46
Top, Middle, Bottom 95.24 96.7
B. Analyzing the linguistic knowledge required for different
downstream tasks
Different downstream tasks require different types of lin-
guistic knowledge. We Ô¨Ånd that the most important functional
block of the transformer model for each task (corresponding
to the most important kind of linguistic knowledge required
to effectively solve the task) agrees with our intuition about
the task. For example, we expect sentiment analysis to require
only local context, since long-range information often ends
up confusing the model (sentiments often change rapidly);
it is also unlikely that syntactic and semantic information
are needed. This is experimentally validated by the fact that
inspecting the blocks in top layers, followed by the middle
and Ô¨Ånally the bottom layers, leads to most accurate models
on SST-2 (Table VII). Similarly, we expect knowledge about
language syntax and semantics to be most important for a
task that tests the linguistic acceptability of a given sentence
(CoLA). We also Ô¨Ånd that this holds across all transformer
architectures that we study in this work (BERT, DistilBERT,
Q8BERT and XLNet). We leverage this intuition to reduce the
overheads of AxFormer without compromising on the quality
of the generated models.
C. Analyzing the beneÔ¨Åts of AxFormer
Transformers are pre-trained on a large text corpus through
a self-supervised NLP task, such as predicting the next word
in text given all the preceding words. When the pre-trained
models are Ô¨Åne-tuned for different downstream tasks, they
contain task-irrelevant information that adds noise and con-
fuses the model. In addition, since the pre-trained models are
highly overparameterized, they severely overÔ¨Åt the small Ô¨Åne-
tuning datasets. AxFormer helps the model focus on the task
at hand by eliminating the irrelevant information in the Ô¨Åne-
tuned models.
1) Improved Generalization: Accuracy-driven pruning can
be seen as a form of training, since the objective of accuracy-
driven pruning ‚Äì minimizing loss on the (validation) dataset ‚Äì
is exactly the same as the objective of training. When large,
over-parameterized models are trained on very limited data,
the constraints of accuracy-driven pruning ‚Äì (1) loss can be
reduced only be setting weights to 0 (equivalent to pruning
elements), and no other changes to weights are allowed, andTABLE VII
The order of inspected elements that provides maximum accuracy for
different downstream tasks using BERT, DistilBERT, Q8BERT and
XLNet. We Ô¨Ånd that the same ordering provides best performance on all the
studied transformer architectures.
Dataset Task Best orderingMost
important
knowledge
CoLALinguistic
acceptabilityTop, Bottom,
MiddleSemantic/
Syntactic
MNLI EntailmentTop, Middle,
BottomPhrase-level
MRPCSemantic
equivalenceTop, Middle,
BottomPhrase-level
QNLIQuestion
answeringTop, Middle,
BottomPhrase-level
QQPSemantic
equivalenceTop, Middle,
BottomPhrase-level
RTE EntailmentTop, Middle,
BottomPhrase-level
SST-2Sentiment
AnalysisTop, Middle,
BottomPhrase-level
STS-BSentence
similarityTop, Middle,
BottomPhrase-level
WNLIReading
comprehensionTop, Middle,
BottomPhrase-level
SQUADQuestion
answeringTop, Middle,
BottomPhrase-level
(2) a majority of samples in the validation set must beneÔ¨Åt
from each weight update; reduction in loss is a necessary
but insufÔ¨Åcient condition for weights to get updated ‚Äì help
it learn more effectively than SGD. As a result, we Ô¨Ånd that
training with SGD on a subset of the training data, followed
by accuracy-driven pruning on the remaining unseen data,
produces models with better generalization performance than
training with SGD on the entire dataset (Figure 4).
Fig. 4. Change in class boundaries using AxFormer on MRPC with
BERT-Base. We use T-distributed Stochastic Neighbor Embedding (TSNE) on
word embeddings before the Ô¨Ånal classiÔ¨Åer, using the 3 main components of
each embedding. Here, blue and orange points refer to sentence pairs that are
semantically non-equivalent and equivalent, respectively. AxFormer models
show better class separability, and hence, are more accurate.
2) Reduced variance: Accuracy-driven pruning and se-
lective hard attention Ô¨Ålter out noise in the model by re-
moving irrelevant information that ends up confusing the
model. Accuracy-driven pruning reduces noise by pruning
task-irrelevant elements. Selective hard attention further re-
duces noise by ensuring that the model focuses only on the
relevant parts of the input. We Ô¨Ånd that this, in turn, leads
to reduced cross-seed variance on the test set. However, the
reduction in variance is limited by the effects of random
initialization of the task-speciÔ¨Åc Ô¨Ånal layer.
