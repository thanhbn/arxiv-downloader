# Một Khảo Sát Thực Tiễn về Các Transformer Nhanh Hơn và Nhẹ Hơn

Mạng thần kinh hồi quy là các mô hình hiệu quả để xử lý chuỗi. Tuy nhiên, chúng không thể học được các phụ thuộc dài hạn do bản chất tuần tự vốn có của chúng. Để giải quyết vấn đề này, Vaswani và cộng sự đã giới thiệu Transformer, một mô hình chỉ dựa trên cơ chế attention có thể liên kết bất kỳ hai vị trí nào của chuỗi đầu vào, do đó mô hình hóa các phụ thuộc dài tùy ý. Transformer đã cải thiện hiệu suất tốt nhất hiện tại trên nhiều tác vụ mô hình hóa chuỗi. Tuy nhiên, hiệu quả của nó đi kèm với chi phí là độ phức tạp tính toán và bộ nhớ bậc hai theo chiều dài chuỗi, cản trở việc áp dụng nó. May mắn thay, cộng đồng học sâu luôn quan tâm đến việc cải thiện hiệu quả của các mô hình, dẫn đến vô số giải pháp như chia sẻ tham số, cắt tỉa, độ chính xác hỗn hợp và chưng cất kiến thức. Gần đây, các nhà nghiên cứu đã trực tiếp giải quyết hạn chế của Transformer bằng cách thiết kế các lựa chọn thay thế có độ phức tạp thấp hơn như Longformer, Reformer, Linformer và Performer. Tuy nhiên, do phạm vi rộng lớn của các giải pháp, việc xác định phương pháp nào cần áp dụng trong thực tế để đạt được sự cân bằng mong muốn giữa khả năng, tính toán và bộ nhớ đã trở nên khó khăn đối với các nhà nghiên cứu và thực hành viên. Khảo sát này giải quyết vấn đề này bằng cách điều tra các phương pháp phổ biến để làm cho Transformer nhanh hơn và nhẹ hơn và bằng cách cung cấp lời giải thích toàn diện về điểm mạnh, hạn chế và các giả định cơ bản của các phương pháp.
