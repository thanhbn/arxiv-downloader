# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2012.15828.pdf
# Kích thước tệp: 537362 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
MINILMv2: Chưng cất Mối quan hệ Tự Chú ý Đa Đầu
để Nén các Transformer Được Huấn luyện Trước
Wenhui Wang Hangbo Bao Shaohan Huang Li Dong Furu Wei
Microsoft Research
{wenwan,t-habao,shaohanh,lidong1,fuwei}@microsoft.com
Tóm tắt
Chúng tôi tổng quát hóa việc chưng cất tự chú ý sâu
trong M INILM (Wang et al., 2020) bằng cách chỉ sử
dụng chưng cất mối quan hệ tự chú ý để nén bất khả
tri nhiệm vụ các Transformer được huấn luyện trước.
Cụ thể, chúng tôi định nghĩa các mối quan hệ tự chú
ý đa đầu là tích vô hướng có tỷ lệ giữa các cặp vector
truy vấn, khóa và giá trị trong mỗi mô-đun tự chú ý.
Sau đó chúng tôi sử dụng kiến thức mối quan hệ trên
để huấn luyện mô hình học sinh. Bên cạnh tính đơn
giản và nguyên tắc thống nhất, thuận lợi hơn, không
có hạn chế về số lượng đầu chú ý của học sinh, trong
khi hầu hết công trình trước đây phải đảm bảo cùng
số đầu giữa giáo viên và học sinh. Hơn nữa, các mối
quan hệ tự chú ý tinh vi có xu hướng khai thác đầy đủ
kiến thức tương tác được học bởi Transformer. Ngoài
ra, chúng tôi kiểm tra kỹ lưỡng chiến lược lựa chọn
lớp cho các mô hình giáo viên, thay vì chỉ dựa vào
lớp cuối như trong M INILM. Chúng tôi tiến hành các
thí nghiệm mở rộng về nén cả các mô hình được huấn
luyện trước đơn ngữ và đa ngữ. Kết quả thí nghiệm
chứng minh rằng các mô hình1 của chúng tôi được
chưng cất từ các giáo viên kích thước cơ sở và lớn
(BERT, RoBERTa và XLM-R) vượt trội hơn so với
hiện đại nhất.

1 Giới thiệu
Các Transformer được huấn luyện trước (Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Yang et al.,
2019; Joshi et al., 2019; Liu et al., 2019; Bao et al.,
2020; Radford et al., 2019; Raffel et al., 2019;
Lewis et al., 2019a) đã rất thành công cho một
loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên.
Tuy nhiên, những mô hình này thường bao gồm
hàng trăm triệu tham số và đang ngày càng lớn hơn. Điều này mang lại thách thức cho việc tinh chỉnh và phục vụ trực tuyến trong các ứng dụng thực tế do các hạn chế về tài nguyên tính toán và độ trễ.

Chưng cất kiến thức (KD; Hinton et al.
2015, Romero et al. 2015) đã được sử dụng rộng rãi để nén các Transformer được huấn luyện trước, điều này chuyển giao kiến thức của mô hình lớn (giáo viên) cho mô hình nhỏ (học sinh) bằng cách giảm thiểu sự khác biệt giữa các đặc trưng của giáo viên và học sinh.
Các xác suất mục tiêu mềm (nhãn mềm) và biểu diễn trung gian thường được sử dụng để thực hiện huấn luyện KD. Trong công trình này, chúng tôi tập trung vào nén bất khả tri nhiệm vụ của các Transformer được huấn luyện trước (Sanh et al., 2019; Tsai et al., 2019; Jiao et al.,
2019; Sun et al., 2019b; Wang et al., 2020). Các mô hình học sinh được chưng cất từ các Transformer lớn được huấn luyện trước sử dụng các kho dữ liệu văn bản quy mô lớn. Mô hình bất khả tri nhiệm vụ được chưng cất có thể được tinh chỉnh trực tiếp trên các nhiệm vụ hạ nguồn, và có thể được sử dụng để khởi tạo chưng cất đặc thù nhiệm vụ.

DistilBERT (Sanh et al., 2019) sử dụng các xác suất mục tiêu mềm cho dự đoán mô hình ngôn ngữ có mặt nạ và đầu ra nhúng để huấn luyện học sinh. Mô hình học sinh được khởi tạo từ giáo viên bằng cách lấy một lớp trong hai. TinyBERT (Jiao et al., 2019) sử dụng các trạng thái ẩn và phân phối tự chú ý (tức là bản đồ chú ý và trọng số), và áp dụng hàm đồng nhất để ánh xạ các lớp học sinh và giáo viên cho chưng cất theo lớp. MobileBERT (Sun et al., 2019b) giới thiệu các mô hình giáo viên và học sinh được thiết kế đặc biệt sử dụng cấu trúc nghịch đảo-bottleneck và bottleneck để giữ số lớp và kích thước ẩn của chúng giống nhau, chuyển giao theo lớp các trạng thái ẩn và phân phối tự chú ý. MINILM(Wang et al., 2020) đề xuất chưng cất tự chú ý sâu, sử dụng phân phối tự chú ý và mối quan hệ giá trị để giúp học sinh bắt chước sâu sắc các mô-đun tự chú ý của giáo viên. MINILMcho thấy rằng chuyển giao kiến thức của lớp cuối của giáo viên đạt hiệu suất tốt hơn so với chưng cất theo lớp.

arXiv:2012.15828v2  [cs.CL]  27 Jun 2021

--- TRANG 2 ---
lation. Tóm lại, hầu hết công trình trước đây dựa vào
phân phối tự chú ý để thực hiện huấn luyện KD,
dẫn đến hạn chế là số lượng đầu chú ý của mô hình học sinh phải giống với giáo viên của nó.

Trong công trình này, chúng tôi tổng quát hóa và đơn giản hóa chưng cất tự chú ý sâu của MINILM(Wang et al.,
2020) bằng cách sử dụng chưng cất mối quan hệ tự chú ý.
Chúng tôi giới thiệu các mối quan hệ tự chú ý đa đầu
được tính toán bởi tích vô hướng có tỷ lệ của các cặp truy vấn,
khóa và giá trị, điều này hướng dẫn việc huấn luyện học sinh. Lấy vector truy vấn làm ví dụ, để có được truy vấn của nhiều đầu mối quan hệ, chúng tôi
trước tiên nối các vector truy vấn của các đầu chú ý khác nhau, và sau đó tách vector được nối theo số lượng đầu mối quan hệ mong muốn.
Sau đó, đối với các mô hình giáo viên và học sinh với số lượng đầu chú ý khác nhau, chúng tôi có thể căn chỉnh truy vấn của chúng với cùng số lượng đầu mối quan hệ cho chưng cất. Hơn nữa, sử dụng số lượng đầu mối quan hệ lớn hơn mang lại kiến thức tự chú ý tinh vi hơn, giúp học sinh đạt được sự bắt chước sâu hơn mô-đun tự chú ý của giáo viên.

Ngoài ra, đối với các giáo viên kích thước lớn (24 lớp, 1024 kích thước ẩn), các thí nghiệm mở rộng chỉ ra rằng chuyển giao một lớp giữa trên có xu hướng hoạt động tốt hơn so với sử dụng lớp cuối như trong M INILM.

Kết quả thí nghiệm cho thấy các mô hình đơn ngữ của chúng tôi được chưng cất từ BERT và RoBERTa, và các mô hình đa ngữ được chưng cất từ XLM-R vượt trội hơn các mô hình hiện đại nhất trong các kích thước tham số khác nhau. Mô hình 6768(6 lớp, 768 kích thước ẩn) được chưng cất từ BERT LARGE nhanh hơn 2:0 lần, trong khi vẫn hoạt động tốt hơn BERT BASE. Mô hình kích thước cơ sở được chưng cất từ RoBERTa LARGE vượt trội hơn RoBERTa BASE sử dụng ít ví dụ huấn luyện hơn nhiều.

Để tóm tắt, đóng góp của chúng tôi bao gồm:
•Chúng tôi tổng quát hóa và đơn giản hóa chưng cất tự chú ý sâu trong MINILMbằng cách giới thiệu chưng cất mối quan hệ tự chú ý đa đầu, điều này mang lại kiến thức tự chú ý tinh vi hơn và cho phép linh hoạt hơn cho số lượng đầu chú ý của học sinh.

•Chúng tôi tiến hành các thí nghiệm chưng cất mở rộng trên các giáo viên kích thước lớn khác nhau và thấy rằng sử dụng kiến thức của lớp giữa trên của giáo viên đạt hiệu suất tốt hơn.

•Kết quả thí nghiệm chứng minh tính hiệu quả của phương pháp chúng tôi cho các giáo viên đơn ngữ và đa ngữ khác nhau trong kích thước cơ sở và lớn.

2 Công trình liên quan

2.1 Mạng xương sống: Transformer
Transformer đa lớp (Vaswani et al., 2017) đã được áp dụng rộng rãi trong các mô hình được huấn luyện trước. Mỗi lớp Transformer bao gồm một lớp con tự chú ý và một lớp con feed-forward được kết nối đầy đủ theo vị trí.

Tự Chú ý Transformer dựa vào tự chú ý đa đầu để nắm bắt các phụ thuộc giữa các từ. Cho đầu ra của lớp Transformer trước đóHl−1∈Rˆ|x|×dh, đầu ra của một đầu tự chú ýOl,a; a∈[1,Ah]được tính toán qua:

Ql,a=Hl−1WQ l,a(1)
Kl,a=Hl−1WK l,a (2)
Vl,a=Hl−1WV l,a (3)
Ol,a= softmax(Ql,aKT l,a/√dk)Vl,a (4)

Đầu ra của lớp trước được chiếu tuyến tính thành truy vấn, khóa và giá trị sử dụng các ma trận tham sốWQ l,a,WK l,a,WV l,a∈Rdh×dk, tương ứng. Các phân phối tự chú ý được tính toán qua tích vô hướng có tỷ lệ của truy vấn và khóa. Những trọng số này được gán cho các vector giá trị tương ứng để có được đầu ra chú ý. |x| biểu thị độ dài của chuỗi đầu vào. Ah và dh chỉ ra số lượng đầu chú ý và kích thước ẩn. dk là kích thước đầu chú ý. dk×Ah thường bằng dh.

2.2 Mô hình Ngôn ngữ được Huấn luyện Trước
Huấn luyện trước đã dẫn đến những cải thiện mạnh mẽ trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên.
Các mô hình ngôn ngữ được huấn luyện trước được học trên lượng lớn dữ liệu văn bản, và sau đó được tinh chỉnh để thích ứng với các nhiệm vụ cụ thể. BERT (Devlin et al., 2018) đề xuất huấn luyện trước một Transformer hai chiều sâu sử dụng mục tiêu mô hình ngôn ngữ có mặt nạ (MLM).
UNILM (Dong et al., 2019) được huấn luyện trước chung trên ba loại mục tiêu mô hình ngôn ngữ để thích ứng với cả nhiệm vụ hiểu và sinh.
RoBERTa (Liu et al., 2019) đạt hiệu suất mạnh mẽ bằng cách huấn luyện các bước dài hơn sử dụng kích thước lô lớn và nhiều dữ liệu văn bản hơn. MASS (Song et al., 2019), T5 (Raffel et al., 2019) và BART (Lewis et al.,

--- TRANG 3 ---
ℎ1𝑙−1ℎ2𝑙−1ℎ3𝑙−1ℎ4𝑙−1ℎ5𝑙−1Truy vấn Khóa Giá trị
Tuyến tínhNốiTách…
Q-Q Att-Rel K-K Att-Rel V-V Att-Rel …
Trạng thái Ẩn
(ℝ𝑥×𝑑ℎ)Vector Chú ý
Đa Đầu
(ℝ𝐴ℎ×𝑥×𝑑𝑘)Vector Mối quan hệ
Đa Đầu
(ℝ𝐴𝑟×𝑥×𝑑𝑟)Mối quan hệ
Tự Chú ý
(ℝ𝐴𝑟×𝑥×|𝑥|)

Giáo viên (Mô hình Lớn)Học sinh (Mô hình Nhỏ)Q-Q Att-Rel K-K Att-Rel V-V Att-Rel
Q-Q Att-Rel K-K Att-Rel V-V Att-Rel
Mối quan hệ Tự Chú ý (Lớp Cuối hoặc Lớp Giữa Trên)Mối quan hệ Tự Chú ý (Lớp Cuối)
Chuyển giao Mối quan hệ
(KL-Divergence)
Nhúng Đầu vàoTự Chú ý
Đa ĐầuFeed Forward
Cộng & ChuẩnCộng & Chuẩn
𝐿xNhúng Đầu vàoTự Chú ý
Đa ĐầuFeed Forward
Cộng & ChuẩnCộng & Chuẩn
𝑀x
Tích Vô hướng

Hình 1: Tổng quan về chưng cất mối quan hệ tự chú ý đa đầu. Chúng tôi giới thiệu các mối quan hệ tự chú ý đa đầu được tính toán bởi tích vô hướng có tỷ lệ của các cặp truy vấn, khóa và giá trị để hướng dẫn việc huấn luyện học sinh. Để có được vector (truy vấn, khóa và giá trị) của nhiều đầu mối quan hệ, chúng tôi trước tiên nối các vector tự chú ý của các đầu chú ý khác nhau và sau đó tách chúng theo số lượng đầu mối quan hệ mong muốn. Chúng tôi chọn chuyển giao các mối quan hệ tự chú ý Q-Q, K-K và V-V để đạt được sự cân bằng giữa hiệu suất và tốc độ huấn luyện. Đối với giáo viên kích thước lớn, chúng tôi chuyển giao kiến thức tự chú ý của một lớp giữa trên của giáo viên. Đối với giáo viên kích thước cơ sở, sử dụng lớp cuối đạt hiệu suất tốt hơn.

2019a) sử dụng cấu trúc mã hóa-giải mã tiêu chuẩn và huấn luyện trước bộ giải mã tự hồi quy. Bên cạnh các mô hình được huấn luyện trước đơn ngữ, các mô hình được huấn luyện trước đa ngữ (Devlin et al., 2018; Lample and Conneau, 2019; Chi et al., 2019; Conneau et al., 2019; Chi et al., 2020) cũng thúc đẩy hiện đại nhất về hiểu và sinh đa ngôn ngữ.

2.3 Chưng cất Kiến thức
Chưng cất kiến thức đã được chứng minh là một cách đầy hứa hẹn để nén các mô hình lớn trong khi vẫn duy trì độ chính xác. Kiến thức của một hoặc một tập hợp các mô hình lớn được sử dụng để hướng dẫn việc huấn luyện các mô hình nhỏ. Hinton et al. (2015) đề xuất sử dụng các xác suất mục tiêu mềm để huấn luyện các mô hình học sinh. Kiến thức tinh vi hơn như trạng thái ẩn (Romero et al., 2015) và phân phối chú ý (Zagoruyko and Komodakis, 2017; Hu et al., 2018) được giới thiệu để cải thiện mô hình học sinh.

Trong công trình này, chúng tôi tập trung vào chưng cất kiến thức bất khả tri nhiệm vụ của các Transformer được huấn luyện trước. Mô hình bất khả tri nhiệm vụ được chưng cất có thể được tinh chỉnh để thích ứng với các nhiệm vụ hạ nguồn. Nó cũng có thể được sử dụng để khởi tạo chưng cất đặc thù nhiệm vụ (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), sử dụng mô hình giáo viên được tinh chỉnh để hướng dẫn việc huấn luyện học sinh trên các nhiệm vụ cụ thể. Kiến thức được sử dụng cho chưng cất và hàm ánh xạ lớp là hai điểm chính cho chưng cất bất khả tri nhiệm vụ của các Transformer được huấn luyện trước. Hầu hết công trình trước đây sử dụng các xác suất mục tiêu mềm, trạng thái ẩn, phân phối tự chú ý và mối quan hệ giá trị để huấn luyện mô hình học sinh.
Đối với hàm ánh xạ lớp, TinyBERT (Jiao et al., 2019) sử dụng chiến lược đồng nhất để ánh xạ các lớp giáo viên và học sinh. MobileBERT (Sun et al., 2019b) giả định học sinh có cùng số lượng lớp với giáo viên để thực hiện chưng cất theo lớp.
MINILM(Wang et al., 2020) chuyển giao kiến thức tự chú ý của lớp cuối của giáo viên đến lớp Transformer cuối của học sinh. Khác với công trình trước đây, phương pháp của chúng tôi sử dụng các mối quan hệ tự chú ý đa đầu để loại bỏ hạn chế về số lượng đầu chú ý của học sinh. Hơn nữa, chúng tôi cho thấy rằng chuyển giao kiến thức tự chú ý của một lớp giữa trên của mô hình giáo viên kích thước lớn hiệu quả hơn.

3 Chưng cất Mối quan hệ Tự Chú ý Đa Đầu

Theo MINILM(Wang et al., 2020), ý tưởng chính của phương pháp chúng tôi là bắt chước sâu sắc mô-đun tự chú ý của giáo viên, điều này vẽ ra các phụ thuộc giữa các từ và là thành phần quan trọng của Transformer. MINILMsử dụng phân phối tự chú ý của giáo viên để huấn luyện mô hình học sinh. Điều này mang lại

--- TRANG 4 ---
Mô hình Giáo viên #Param Tăng tốc SQuAD2 MNLI-m QNLI QQP RTE SST MRPC CoLA Trung bình
BERT BASE - 109M1.0 76.8 84.5 91.7 91.3 68.6 93.2 87.3 58.9 81.5
RoBERTa BASE - 125M1.0 83.7 87.6 92.8 91.9 78.7 94.8 90.2 63.6 85.4
BERT SMALL - 66M2.0 73.2 81.8 89.8 90.6 67.9 91.2 84.9 53.5 79.1
Truncated BERT BASE - 66M2.0 69.9 81.2 87.9 90.4 65.5 90.8 82.7 41.4 76.2
Truncated RoBERTa BASE - 81M2.0 77.9 84.9 91.1 91.3 67.9 92.9 87.5 55.2 81.1
DistilBERT BERT BASE 66M2.0 70.7 82.2 89.2 88.5 59.9 91.3 87.5 51.3 77.6
TinyBERT BERT BASE 66M2.0 73.1 83.5 90.5 90.6 72.2 91.6 88.4 42.8 79.1
MINILM BERT BASE 66M2.0 76.4 84.0 91.0 91.0 71.5 92.0 88.4 49.2 80.4
6384Ours BERT BASE 22M5.3 72.9 82.8 90.3 90.6 68.9 91.3 86.6 41.8 78.2
6384Ours BERT LARGE 22M5.3 74.3 83.0 90.4 90.7 68.5 91.1 87.8 41.6 78.4
6384Ours RoBERTa LARGE 30M5.3 76.4 84.4 90.9 90.8 69.9 92.0 88.7 42.6 79.5
6768Ours BERT BASE 66M2.0 76.3 84.2 90.8 91.1 72.1 92.4 88.9 52.5 81.0
6768Ours BERT LARGE 66M2.0 77.7 85.0 91.4 91.1 73.0 92.5 88.9 53.9 81.7
6768Ours RoBERTa LARGE 81M2.0 81.6 87.0 92.7 91.4 78.7 94.5 90.4 54.0 83.8

Bảng 1: Kết quả của các học sinh được chưng cất từ giáo viên kích thước cơ sở và lớn trên bộ phát triển của GLUE và SQuAD 2.0. Chúng tôi báo cáo F1 cho SQuAD 2.0, hệ số tương quan Matthews cho CoLA, và độ chính xác cho các bộ dữ liệu khác. Kết quả GLUE của DistilBERT được lấy từ Sanh et al. (2019). Các kết quả còn lại của DistilBERT, TinyBERT2, BERT SMALL, Truncated BERT BASE và M INILM được lấy từ Wang et al. (2020). BERT SMALL (Turc et al., 2019) được huấn luyện sử dụng mục tiêu MLM, không sử dụng KD. Chúng tôi cũng báo cáo kết quả của truncated BERT BASE và truncated RoBERTa BASE, loại bỏ 6 lớp trên của mô hình cơ sở. Việc bỏ lớp trên đã được chứng minh là một đường cơ sở mạnh (Sajjad et al., 2020). Kết quả tinh chỉnh là trung bình của 4 lần chạy.

hạn chế về số lượng đầu chú ý của học sinh, được yêu cầu phải giống với giáo viên của nó. Để giới thiệu kiến thức tự chú ý tinh vi hơn và tránh sử dụng phân phối tự chú ý của giáo viên, chúng tôi tổng quát hóa chưng cất tự chú ý sâu trong MINILM và giới thiệu các mối quan hệ tự chú ý đa đầu của các cặp truy vấn, khóa và giá trị để huấn luyện học sinh. Bên cạnh đó, chúng tôi tiến hành các thí nghiệm mở rộng và thấy rằng việc lựa chọn lớp của mô hình giáo viên là quan trọng để chưng cất các mô hình kích thước lớn. Hình 1 đưa ra tổng quan về phương pháp của chúng tôi.

3.1 Mối quan hệ Tự Chú ý Đa Đầu
Các mối quan hệ tự chú ý đa đầu được có được bởi tích vô hướng có tỷ lệ của các cặp3 truy vấn, khóa và giá trị của nhiều đầu mối quan hệ. Lấy vector truy vấn làm ví dụ, để có được truy vấn của nhiều đầu mối quan hệ, chúng tôi trước tiên nối truy vấn của các đầu chú ý khác nhau và sau đó tách vector được nối dựa trên số lượng đầu mối quan hệ mong muốn. Cùng thao tác cũng được thực hiện trên khóa và giá trị. Đối với các mô hình giáo viên và học sinh sử dụng số lượng đầu chú ý khác nhau, chúng tôi chuyển đổi truy vấn, khóa và giá trị của chúng từ số lượng đầu chú ý khác nhau thành

2 Ngoài chưng cất bất khả tri nhiệm vụ, TinyBERT sử dụng chưng cất đặc thù nhiệm vụ và tăng cường dữ liệu để cải thiện thêm mô hình. Chúng tôi báo cáo kết quả tinh chỉnh của mô hình bất khả tri nhiệm vụ công khai của họ.

3 Có chín loại mối quan hệ tự chú ý, như mối quan hệ truy vấn-truy vấn, khóa-khóa, khóa-giá trị và truy vấn-giá trị.

vector của cùng số lượng đầu mối quan hệ để thực hiện huấn luyện KD. Phương pháp của chúng tôi loại bỏ hạn chế về số lượng đầu chú ý của các mô hình học sinh. Hơn nữa, sử dụng nhiều đầu mối quan hệ hơn trong việc tính toán các mối quan hệ tự chú ý mang lại kiến thức tự chú ý tinh vi hơn và cải thiện hiệu suất của mô hình học sinh.

Chúng tôi sử dụng A1;A2;A3 để biểu thị truy vấn, khóa và giá trị của nhiều đầu mối quan hệ. KL-divergence giữa các mối quan hệ tự chú ý đa đầu của giáo viên và học sinh được sử dụng làm mục tiêu huấn luyện:

L=3∑i=13∑j=1αijLij (5)

Lij=1/Ar|x||x|∑Ar a=1∑|x| t=1DKL(RTij,l,a,t∥RSij,m,a,t )
(6)

RTij,l,a= softmax(ATi,l,aAT|j,l,a/√dr) (7)

RSij,m,a = softmax(ASi,m,aAS|j,m,a/√d'r) (8)

trong đó ATi,l,a∈R|x|×dr và ASi,m,a∈R|x|×d'r(i∈[1;3]) là truy vấn, khóa và giá trị của một đầu mối quan hệ của lớp thứ l của giáo viên và lớp thứ m của học sinh.
dr và d'r là kích thước đầu mối quan hệ của mô hình giáo viên và học sinh. RTij,l∈RAr×|x|×|x|là mối quan hệ tự chú ý của ATi,l và ATj,l của mô hình giáo viên.

--- TRANG 5 ---
RTij,l,a∈R|x|×|x|là mối quan hệ tự chú ý của một đầu mối quan hệ của giáo viên. RSij,m∈RAr×|x|×|x|là mối quan hệ tự chú ý của mô hình học sinh. Ví dụ, RT11,l biểu thị mối quan hệ chú ý Q-Q của giáo viên trong Hình 1. Ar là số lượng đầu mối quan hệ. Nếu số lượng đầu mối quan hệ và đầu chú ý giống nhau, mối quan hệ Q-K tương đương với trọng số chú ý trong mô-đun tự chú ý.

αij∈{0,1} là trọng số được gán cho mỗi mất mát mối quan hệ tự chú ý. Chúng tôi chuyển giao mối quan hệ tự chú ý truy vấn-truy vấn, khóa-khóa và giá trị-giá trị để đạt được sự cân bằng giữa hiệu suất và chi phí huấn luyện.

3.2 Lựa chọn Lớp của Mô hình Giáo viên
Bên cạnh kiến thức được sử dụng cho chưng cất, hàm ánh xạ giữa các lớp giáo viên và học sinh là một yếu tố quan trọng khác. Như trong MINILM, chúng tôi chỉ chuyển giao kiến thức tự chú ý của một trong các lớp giáo viên đến lớp cuối của học sinh. Chỉ chưng cất một lớp của mô hình giáo viên là nhanh và hiệu quả. Khác với công trình trước đây thường tiến hành thí nghiệm trên các giáo viên kích thước cơ sở, chúng tôi thí nghiệm với các giáo viên kích thước lớn khác nhau và thấy rằng chuyển giao kiến thức tự chú ý của một lớp giữa trên hoạt động tốt hơn so với sử dụng các lớp khác. Đối với BERT LARGE và BERT LARGE-WWM, chuyển giao lớp thứ 21 (bắt đầu từ một) đạt hiệu suất tốt nhất. Đối với RoBERTa LARGE và XLM-R LARGE, sử dụng kiến thức tự chú ý của lớp thứ 19 đạt hiệu suất tốt hơn. Đối với giáo viên kích thước cơ sở, chúng tôi cũng thấy rằng sử dụng lớp cuối của giáo viên hoạt động tốt hơn.

4 Thí nghiệm
Chúng tôi tiến hành các thí nghiệm chưng cất trên các mô hình giáo viên khác nhau bao gồm BERT BASE, BERT LARGE, BERT LARGE-WWM, RoBERTa BASE, RoBERTa LARGE, XLM-R BASE và XLM-R LARGE.

4.1 Thiết lập
Chúng tôi sử dụng phiên bản không phân biệt chữ hoa chữ thường cho ba mô hình giáo viên BERT. Đối với dữ liệu huấn luyện trước, chúng tôi sử dụng Wikipedia tiếng Anh và BookCorpus (Zhu et al., 2015). Chúng tôi huấn luyện các mô hình học sinh sử dụng 256 làm kích thước lô và 6e-4 làm tốc độ học đỉnh trong 400,000 bước. Chúng tôi sử dụng khởi động tuyến tính trong 4,000 bước đầu tiên và suy giảm tuyến tính. Chúng tôi sử dụng Adam (Kingma and Ba, 2015) với β1 = 0:9, β2 = 0:999. Độ dài chuỗi tối đa được đặt là 512. Tỷ lệ dropout và suy giảm trọng số là 0:1 và 0:01. Số lượng đầu chú ý là 12 cho tất cả mô hình học sinh. Số lượng đầu mối quan hệ là 48 và 64 cho mô hình giáo viên kích thước cơ sở và lớn, tương ứng. Các mô hình học sinh được khởi tạo ngẫu nhiên.

Đối với các mô hình được chưng cất từ RoBERTa, chúng tôi sử dụng các bộ dữ liệu huấn luyện trước tương tự như trong Liu et al. (2019). Đối với mô hình học sinh 12768, chúng tôi sử dụng Adam với β1 = 0:9, β2 = 0:98. Các siêu tham số còn lại giống với các mô hình được chưng cất từ BERT.

Đối với các mô hình học sinh đa ngữ được chưng cất từ XLM-R, chúng tôi thực hiện huấn luyện sử dụng cùng bộ dữ liệu như trong Conneau et al. (2019) trong 1,000,000 bước. Chúng tôi tiến hành các thí nghiệm chưng cất sử dụng 8 GPU V100 với huấn luyện độ chính xác hỗn hợp.

4.2 Nhiệm vụ Hạ nguồn
Theo công trình huấn luyện trước (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) và chưng cất bất khả tri nhiệm vụ (Sun et al., 2019b; Jiao et al., 2019) trước đây, chúng tôi đánh giá các mô hình học sinh tiếng Anh trên bộ đánh giá GLUE và trả lời câu hỏi trích xuất. Các mô hình đa ngữ được đánh giá trên suy luận ngôn ngữ tự nhiên đa ngôn ngữ và trả lời câu hỏi đa ngôn ngữ.

GLUE Bộ đánh giá Hiểu Ngôn ngữ Chung (GLUE) (Wang et al., 2019) bao gồm hai nhiệm vụ phân loại câu đơn (SST-2 (Socher et al., 2013) và CoLA (Warstadt et al., 2018)), ba nhiệm vụ tương tự và diễn giải (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) và QQP), và bốn nhiệm vụ suy luận (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli

--- TRANG 6 ---
Mô hình Giáo viên #Param SQuAD2 MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STS Trung bình
BERT BASE - 109M 1.0 76.8 84.6/83.4 90.5 71.2 66.4 93.5 88.9 52.1 85.8 79.3
BERT LARGE - 340M 0.3 81.9 86.7/85.9 92.7 72.1 70.1 94.9 89.3 60.5 86.5 82.1
6768Ours BERT BASE 66M 2.0 76.3 83.8/83.3 90.2 70.9 69.2 92.9 89.1 46.6 84.3 78.7
6768Ours BERT LARGE 66M 2.0 77.7 84.5/84.0 91.5 71.3 69.2 93.0 89.1 48.6 85.1 79.4

Bảng 2: Kết quả của các học sinh 6768 được chưng cất từ BERT trên bộ kiểm tra GLUE và bộ phát triển SQuAD 2.0. Các kết quả được báo cáo được tinh chỉnh trực tiếp trên các nhiệm vụ hạ nguồn. Chúng tôi báo cáo F1 cho SQuAD 2.0, QQP và MRPC, tương quan Spearman cho STS-B, hệ số tương quan Matthews cho CoLA và độ chính xác cho phần còn lại.

Mô hình Giáo viên #Param SQuAD2 MNLI-m QNLI QQP RTE SST MRPC CoLA STS Trung bình
BERT BASE - 109M 76.8 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 82.4
RoBERTa BASE - 125M 83.7 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2 86.1
12768Ours BERT LARGE 109M 81.8 86.5 92.6 91.6 76.4 93.3 89.2 62.3 90.5 84.9
12768Ours RoBERTa LARGE 125M 86.6 89.4 94.0 91.8 83.1 95.9 91.2 65.0 91.3 87.6

Bảng 3: Kết quả của các mô hình 12768 trên bộ phát triển của bộ đánh giá GLUE và SQuAD 2.0. Kết quả tinh chỉnh là trung bình của 4 lần chạy cho mỗi nhiệm vụ. Chúng tôi báo cáo F1 cho SQuAD 2.0, tương quan Pearson cho STS-B, hệ số tương quan Matthews cho CoLA và độ chính xác cho phần còn lại.

Mô hình SQuAD2 MNLI-m SST-2 Trung bình
MINILM (Lớp Cuối) 79.1 84.7 91.2 85.0
+ Lớp Giữa Trên 80.3 85.2 91.5 85.7
12384Ours 80.7 85.7 92.3 86.2

Bảng 4: So sánh các phương pháp khác nhau sử dụng BERT LARGE-WWM làm giáo viên. Chúng tôi báo cáo kết quả phát triển của mô hình học sinh 12384 với kích thước nhúng 128.

et al., 2009) và WNLI (Levesque et al., 2012)).

Trả lời Câu hỏi Trích xuất Nhiệm vụ nhằm dự đoán một phân đoạn con liên tục của đoạn văn để trả lời câu hỏi. Chúng tôi đánh giá trên SQuAD 2.0 (Rajpurkar et al., 2018), được sử dụng như một bộ đánh giá trả lời câu hỏi chính.

Suy luận Ngôn ngữ Tự nhiên Đa ngôn ngữ (XNLI) XNLI (Conneau et al., 2018) là một bộ đánh giá phân loại đa ngôn ngữ. Nó nhằm xác định mối quan hệ ngữ nghĩa giữa hai câu và cung cấp các thể hiện trong 15 ngôn ngữ.

Trả lời Câu hỏi Đa ngôn ngữ Chúng tôi sử dụng MLQA (Lewis et al., 2019b) để đánh giá các mô hình đa ngữ. MLQA mở rộng bộ dữ liệu SQuAD tiếng Anh (Rajpurkar et al., 2016) sang bảy ngôn ngữ.

4.3 Kết quả Chính
Bảng 1 trình bày kết quả phát triển của các mô hình 6384 và 6768 được chưng cất từ BERT BASE, BERT LARGE và RoBERTa LARGE trên GLUE và SQuAD 2.0. (1) Các phương pháp trước đây (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Wang et al., 2020) thường chưng cất BERT BASE thành mô hình 6 lớp với kích thước ẩn 768. Chúng tôi trước tiên báo cáo kết quả của cùng thiết lập. Mô hình 6768 của chúng tôi vượt trội hơn DistilBERT, TinyBERT, MINILM và hai đường cơ sở BERT trên hầu hết các nhiệm vụ. Hơn nữa, phương pháp của chúng tôi cho phép linh hoạt hơn về số lượng đầu chú ý của các mô hình học sinh. (2) Cả mô hình 6384 và 6768 được chưng cất từ BERT LARGE đều vượt trội hơn các mô hình được chưng cất từ BERT BASE. Mô hình 6768 được chưng cất từ BERT LARGE nhanh hơn 2:0 lần so với BERT BASE, trong khi đạt hiệu suất tốt hơn. (3) Các mô hình học sinh được chưng cất từ RoBERTa LARGE đạt được những cải thiện thêm. Giáo viên tốt hơn dẫn đến học sinh tốt hơn. Chưng cất mối quan hệ tự chú ý đa đầu hiệu quả cho các Transformer được huấn luyện trước kích thước lớn khác nhau.

Chúng tôi báo cáo kết quả của các học sinh 6768 được chưng cất từ BERT BASE và BERT LARGE trên bộ kiểm tra GLUE và bộ phát triển SQuAD 2.0 trong Bảng 2. Mô hình 6768 được chưng cất từ BERT BASE giữ lại hơn 99% độ chính xác của giáo viên trong khi sử dụng 50% tham số Transformer. Mô hình 6768 được chưng cất từ BERT LARGE so sánh thuận lợi với BERT BASE.

Chúng tôi nén RoBERTa LARGE và BERT LARGE thành mô hình học sinh kích thước cơ sở. Kết quả phát triển của bộ đánh giá GLUE và SQuAD 2.0 được hiển thị trong Bảng 3. Các mô hình kích thước cơ sở của chúng tôi được chưng cất từ giáo viên kích thước lớn vượt trội hơn BERT BASE và RoBERTa BASE. Phương pháp của chúng tôi có thể được áp dụng để huấn luyện học sinh trong kích thước tham số khác nhau. Hơn nữa, học sinh của chúng tôi được chưng cất từ RoBERTa LARGE sử dụng kích thước lô huấn luyện nhỏ hơn nhiều (gần 32 lần nhỏ hơn) và ít bước huấn luyện hơn so với RoBERTa BASE. Phương pháp của chúng tôi đòi hỏi ít ví dụ huấn luyện hơn nhiều.

Hầu hết công trình trước đây tiến hành thí nghiệm sử dụng các giáo viên kích thước cơ sở. Để so sánh với các phương pháp trước đây trên giáo viên kích thước lớn, chúng tôi tái triển khai MINILM và nén BERT LARGE-WWM thành mô hình học sinh 12384. Kết quả phát triển của SQuAD 2.0, MNLI-m và SST-2 được trình bày trong Bảng 4. Phương pháp của chúng tôi cũng vượt trội hơn MINILM cho các giáo viên kích thước lớn. Hơn nữa, chúng tôi báo cáo kết quả chưng cất một lớp giữa trên thay vì lớp cuối cho MINILM. Lựa chọn lớp cũng hiệu quả cho MINILM khi chưng cất các giáo viên kích thước lớn.

Bảng 5 và Bảng 6 hiển thị kết quả của các mô hình học sinh được chưng cất từ XLM-R trên XNLI và MLQA. Đối với XNLI, mô hình đơn tốt nhất được chọn trên bộ phát triển chung của tất cả các ngôn ngữ như trong Conneau et al. (2019). Theo Lewis et al. (2019b), chúng tôi áp dụng SQuAD 1.1 làm dữ liệu huấn luyện và đánh giá trên bộ phát triển MLQA tiếng Anh cho

--- TRANG 7 ---
Mô hình Giáo viên #L #H #Param en fr es de el bg ru tr ar vi th zh hi sw ur Trung bình
mBERT - 12 768 170M 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0 66.3
XLM- 100 - 16 1280 570M 83.2 76.7 77.7 74.0 72.7 74.1 72.7 68.7 68.6 72.9 68.9 72.5 65.6 58.2 62.4 70.7
XLM-R BASE - 12 768 277M 85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.3 76.2
MINILM XLM-R BASE 6 384 107M 79.2 72.3 73.1 70.3 69.1 72.0 69.1 64.5 64.9 69.0 66.0 67.8 62.9 59.0 60.6 68.0
6384Ours XLM-R BASE 6 384 107M 78.8 72.1 73.4 69.4 70.7 72.2 69.9 66.7 66.7 69.5 67.4 68.6 65.3 61.2 61.9 68.9
6384Ours XLM-R LARGE 6 384 107M 78.9 72.5 73.5 69.8 70.9 72.3 69.7 67.5 67.1 70.2 67.4 69.1 65.4 62.3 63.4 69.3
MINILM XLM-R BASE 12 384 117M 81.5 74.8 75.7 72.9 73.0 74.5 71.3 69.7 68.8 72.1 67.8 70.0 66.2 63.3 64.2 71.1
12384Ours XLM-R BASE 12 384 117M 82.2 75.3 76.5 73.3 73.8 75.1 73.1 70.7 69.5 72.8 69.4 71.5 68.6 65.6 65.5 72.2
12384Ours XLM-R LARGE 12 384 117M 82.4 76.2 76.8 73.9 74.6 75.5 73.7 70.8 70.5 73.4 71.1 72.5 68.8 66.8 67.0 72.9

Bảng 5: Kết quả phân loại đa ngôn ngữ của các mô hình đa ngữ của chúng tôi trên XNLI. Chúng tôi báo cáo độ chính xác trên mỗi trong 15 ngôn ngữ XNLI và độ chính xác trung bình. #L và #H chỉ ra số lượng lớp và kích thước ẩn.

Mô hình Giáo viên #L #H #Param en es de ar hi vi zh Trung bình
mBERT - 12 768 170M 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.3 57.7 / 41.6
XLM- 15 - 12 1024 248M 74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.6 61.6 / 43.5
XLM-R BASE - 12 768 277M 77.1 / 64.6 67.4 / 49.6 60.9 / 46.7 54.9 / 36.6 59.4 / 42.9 64.5 / 44.7 61.8 / 39.3 63.7 / 46.3
XLM-R BASEy - 12 768 277M 80.3 / 67.4 67.0 / 49.2 62.7 / 48.3 55.0 / 35.6 60.4 / 43.7 66.5 / 45.9 62.3 / 38.3 64.9 / 46.9
MINILM XLM-R BASE 6 384 107M 75.5 / 61.9 55.6 / 38.2 53.3 / 37.7 43.5 / 26.2 46.9 / 31.5 52.0 / 33.1 48.8 / 27.3 53.7 / 36.6
6384Ours XLM-R BASE 6 384 107M 75.6 / 61.9 60.4 / 42.6 57.5 / 42.4 49.9 / 31.1 53.4 / 36.3 57.2 / 36.7 54.4 / 32.4 58.3 / 40.5
6384Ours XLM-R LARGE 6 384 107M 75.8 / 62.4 60.5 / 43.4 56.7 / 41.6 49.3 / 30.9 55.1 / 38.1 58.6 / 38.5 56.9 / 34.3 59.0 /41.3
MINILM XLM-R BASE 12 384 117M 79.4 / 66.5 66.1 / 47.5 61.2 / 46.5 54.9 / 34.9 58.5 / 41.3 63.1 / 42.1 59.0 / 33.8 63.2 / 44.7
12384Ours XLM-R BASE 12 384 117M 79.0 / 65.8 66.6 / 48.9 62.1 / 47.4 54.4 / 35.5 60.6 / 43.6 64.7 / 44.1 59.4 / 36.5 63.8 / 46.0
12384Ours XLM-R LARGE 12 384 117M 78.9 / 65.9 67.3 / 49.4 61.9 / 47.0 56.0 / 36.5 63.0 / 45.9 65.1 / 44.4 61.9 / 39.2 64.9 /46.9

Bảng 6: Kết quả trả lời câu hỏi đa ngôn ngữ của các mô hình đa ngữ của chúng tôi trên MLQA. Chúng tôi báo cáo điểm F1 và EM (khớp chính xác) trên mỗi trong 7 ngôn ngữ MLQA. #L và #H chỉ ra số lượng lớp và kích thước ẩn.
y chỉ ra kết quả tinh chỉnh của chúng tôi cho XLM-R BASE.

Mô hình SQuAD2 MNLI-m SST-2 Trung bình
Ours (Q-Q + K-K + V-V) 72.8 82.2 91.5 82.2
– Q-Q Att-Rel 71.6 81.9 90.6 81.4
– K-K Att-Rel 71.9 81.9 90.5 81.4
– V-V Att-Rel 71.5 81.6 90.5 81.2
Q-K + V-V Att-Rels 72.4 82.2 91.0 81.9

Bảng 7: Nghiên cứu khử bỏ của các mối quan hệ tự chú ý khác nhau. Chúng tôi báo cáo kết quả của mô hình học sinh 6384 được chưng cất từ BERT BASE. Số lượng đầu mối quan hệ là 12.

#Đầu Mối quan hệ 6 12 24 48
mô hình 6384 được chưng cất từ RoBERTa BASE
MNLI-m 82.8 82.9 83.0 83.4
SQuAD 2.0 74.5 75.0 74.9 75.7
mô hình 6384 được chưng cất từ BERT BASE
MNLI-m 81.9 82.2 82.2 82.4
SQuAD 2.0 71.9 72.8 72.7 73.0

Bảng 8: Kết quả của mô hình 6384 sử dụng số lượng đầu mối quan hệ khác nhau.

dừng sớm. Mô hình 6384 của chúng tôi vượt trội hơn mBERT (Devlin et al., 2018) với tăng tốc 5:3 lần. Phương pháp của chúng tôi cũng hoạt động tốt hơn MINILM, điều này càng xác nhận tính hiệu quả của chưng cất mối quan hệ tự chú ý đa đầu. Chuyển giao các mối quan hệ tự chú ý đa đầu có thể mang lại kiến thức tự chú ý tinh vi hơn.

4.4 Nghiên cứu Khử bỏ
Hiệu quả của việc sử dụng các mối quan hệ tự chú ý khác nhau
Chúng tôi thực hiện nghiên cứu khử bỏ để phân tích đóng góp của các mối quan hệ tự chú ý khác nhau. Kết quả phát triển của ba nhiệm vụ được minh họa trong Bảng 7. Các mối quan hệ tự chú ý Q-Q, K-K và V-V đóng góp tích cực vào kết quả cuối cùng. Bên cạnh đó, chúng tôi cũng so sánh Q-Q + K-K + V-V với Q-K + V-V cho rằng truy vấn và khóa được sử dụng để tính toán phân phối tự chú ý trong mô-đun tự chú ý. Kết quả thí nghiệm cho thấy sử dụng Q-Q + K-K + V-V đạt hiệu suất tốt hơn.

Hiệu quả của chưng cất các lớp giáo viên khác nhau Hình 2 trình bày kết quả của mô hình 6384 được chưng cất từ các lớp khác nhau của BERT BASE, BERT LARGE và XLM-R LARGE. Đối với BERT BASE, sử dụng lớp cuối đạt hiệu suất tốt hơn so với các lớp khác. Đối với BERT LARGE và XLM-R LARGE, chúng tôi thấy rằng sử dụng một trong các lớp giữa trên đạt hiệu suất tốt nhất. Cùng xu hướng cũng được quan sát cho BERT LARGE-WWM và RoBERTa LARGE.

Hiệu quả của số lượng đầu mối quan hệ khác nhau
Bảng 8 hiển thị kết quả của mô hình 6384 được chưng cất từ BERT BASE và RoBERTa BASE sử dụng số lượng đầu mối quan hệ khác nhau. Sử dụng số lượng đầu mối quan hệ lớn hơn đạt hiệu suất tốt hơn. Kiến thức tự chú ý tinh vi hơn

--- TRANG 8 ---
Mô hình Giáo viên #Param Tăng tốc SQuAD2 MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STS Trung bình
BERT BASE - 109M 1.0 76.8 84.6/83.4 90.5 71.2 66.4 93.5 88.9 52.1 85.8 79.3
MobileBERT IB-BERT LARGE 25M 1.8 80.2 84.3/83.4 91.6 70.5 70.4 92.6 88.8 51.1 84.8 79.8
12384Ours BERT LARGE-WWM 25M 2.7 80.7 85.9/84.6 91.9 71.4 71.9 93.3 89.2 44.9 85.5 79.9
+ More Att-Rels BERT LARGE-WWM 25M 2.7 80.9 85.8/84.8 92.3 71.6 72.0 93.6 89.7 46.6 86.0 80.3

Bảng 9: So sánh giữa MobileBERT và mô hình cùng kích thước (12 lớp, 384 kích thước ẩn và 128 kích thước nhúng) được chưng cất từ BERT LARGE (Whole Word Masking) trên bộ kiểm tra GLUE và bộ phát triển SQuAD 2.0. Theo MobileBERT (Sun et al., 2019b), các kết quả được báo cáo được tinh chỉnh trực tiếp trên các nhiệm vụ hạ nguồn. Chúng tôi tính toán tăng tốc của MobileBERT theo độ trễ được báo cáo của họ.

(a) BERT BASE làm giáo viên (b) BERT LARGE làm giáo viên (c) XLM-R LARGE làm giáo viên

Hình 2: Các mô hình 6384 được huấn luyện sử dụng các lớp BERT BASE (a), BERT LARGE (b) và XLM-R LARGE (c) khác nhau.

Mô hình Giáo viên SQuAD2 MNLI-m SST-2
6384Ours BERT BASE 72.9 82.8 91.3
+ More Att-Rels BERT BASE 73.3 82.8 91.6
6384Ours BERT LARGE 74.3 83.0 91.1
+ More Att-Rels BERT LARGE 74.7 83.2 92.4
6384Ours RoBERTa LARGE 76.4 84.4 92.0
+ More Att-Rels RoBERTa LARGE 76.0 84.4 92.1
6768Ours BERT BASE 76.3 84.2 92.4
+ More Att-Rels BERT BASE 76.8 84.4 92.3
6768Ours BERT LARGE 77.7 85.0 92.5
+ More Att-Rels BERT LARGE 78.1 85.2 92.5
6768Ours RoBERTa LARGE 81.6 87.0 94.5
+ More Att-Rels RoBERTa LARGE 81.2 87.3 94.1

Bảng 10: Kết quả của việc giới thiệu thêm các mối quan hệ tự chú ý (mối quan hệ Q-K, K-Q, Q-V, V-Q, K-V và V-K).

có thể được nắm bắt bằng cách sử dụng nhiều đầu mối quan hệ hơn, điều này giúp học sinh bắt chước sâu sắc mô-đun tự chú ý của giáo viên. Bên cạnh đó, chúng tôi thấy rằng số lượng đầu mối quan hệ không cần là bội số dương của cả số lượng đầu chú ý học sinh và giáo viên. Đầu mối quan hệ có thể là một phần của một đầu chú ý đơn hoặc chứa các phần từ nhiều đầu chú ý.

5 Thảo luận

5.1 So sánh với MobileBERT
MobileBERT (Sun et al., 2019b) nén một mô hình giáo viên được thiết kế đặc biệt (trong kích thước BERT LARGE) với các mô-đun nghịch đảo bottleneck thành một học sinh 24 lớp sử dụng các mô-đun bottleneck. Vì mục tiêu của chúng tôi là nén các mô hình lớn khác nhau (ví dụ BERT và RoBERTa) thành các mô hình nhỏ sử dụng kiến trúc Transformer tiêu chuẩn, chúng tôi lưu ý rằng mô hình học sinh của chúng tôi không thể so sánh trực tiếp với MobileBERT. Chúng tôi cung cấp kết quả của một mô hình học sinh có cùng kích thước tham số để tham khảo. Một mô hình kích thước lớn công khai (BERT LARGE-WWM) được sử dụng làm giáo viên, đạt hiệu suất tương tự như giáo viên của MobileBERT. Chúng tôi chưng cất BERT LARGE-WWM thành một mô hình học sinh (≈ 25M tham số) sử dụng cùng dữ liệu huấn luyện (tức là Wikipedia tiếng Anh và BookCorpus). Kết quả kiểm tra của GLUE và kết quả phát triển của SQuAD 2.0 được minh họa trong Bảng 9. Mô hình của chúng tôi vượt trội hơn MobileBERT trên hầu hết các nhiệm vụ với tốc độ suy luận nhanh hơn. Hơn nữa, phương pháp của chúng tôi có thể được áp dụng cho các giáo viên khác nhau và có ít hạn chế hơn nhiều đối với học sinh.

Chúng tôi cũng quan sát thấy rằng mô hình của chúng tôi hoạt động tương đối kém hơn trên CoLA so với MobileBERT. Nhiệm vụ của CoLA là đánh giá tính chấp nhận ngữ pháp của một câu. Nó đòi hỏi kiến thức ngôn ngữ học tinh vi hơn có thể được học từ các mục tiêu mô hình ngôn ngữ. Tinh chỉnh mô hình sử dụng mục tiêu MLM như trong MobileBERT mang lại cải thiện cho CoLA. Tuy nhiên, các thí nghiệm sơ bộ của chúng tôi cho thấy chiến lược này sẽ dẫn đến sự sụt giảm nhẹ cho các nhiệm vụ GLUE khác.

--- TRANG 9 ---
5.2 Kết quả của Thêm Mối quan hệ Tự Chú ý
Trong Bảng 9 và 10, chúng tôi báo cáo kết quả của các học sinh được huấn luyện sử dụng thêm các mối quan hệ tự chú ý (mối quan hệ Q-K, K-Q, Q-V, V-Q, K-V và V-K). Chúng tôi quan sát thấy cải thiện trên hầu hết các nhiệm vụ, đặc biệt cho các mô hình học sinh được chưng cất từ BERT. Kiến thức tự chú ý tinh vi trong nhiều mối quan hệ chú ý hơn cải thiện các học sinh của chúng tôi. Tuy nhiên, việc giới thiệu thêm các mối quan hệ tự chú ý cũng mang lại chi phí tính toán cao hơn. Để đạt được sự cân bằng giữa hiệu suất và chi phí tính toán, chúng tôi chọn chuyển giao các mối quan hệ tự chú ý Q-Q, K-K và V-V thay vì tất cả các mối quan hệ tự chú ý trong công trình này.

6 Kết luận
Chúng tôi tổng quát hóa chưng cất tự chú ý sâu trong MINILMbằng cách sử dụng các mối quan hệ tự chú ý đa đầu để huấn luyện học sinh. Phương pháp của chúng tôi giới thiệu kiến thức tự chú ý tinh vi hơn và loại bỏ hạn chế về số lượng đầu chú ý của học sinh. Hơn nữa, chúng tôi cho thấy rằng chuyển giao kiến thức tự chú ý của một lớp giữa trên đạt hiệu suất tốt hơn cho các giáo viên kích thước lớn. Các mô hình đơn ngữ và đa ngữ của chúng tôi được chưng cất từ BERT, RoBERTa và XLM-R có được hiệu suất cạnh tranh và vượt trội hơn các phương pháp hiện đại nhất. Đối với công trình tương lai, chúng tôi đang khám phá một thuật toán lựa chọn lớp tự động. Chúng tôi cũng muốn áp dụng phương pháp của chúng tôi cho các Transformer được huấn luyện trước lớn hơn.

Tài liệu tham khảo
Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,
Xing Fan, and Edward Guo. 2019. Knowledge
distillation from internal representations. CoRR ,
abs/1910.03723.

Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan
Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-
feng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.
Unilmv2: Pseudo-masked language models for uni-
ﬁed language model pre-training. arXiv preprint
arXiv:2002.12804 .

Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and
Danilo Giampiccolo. 2006. The second PASCAL
recognising textual entailment challenge. In Pro-
ceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment .

Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
ﬁfth pascal recognizing textual entailment challenge.
InIn Proc Text Analysis Conference (TAC'09 .

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055 .

Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-
ling Mao, and Heyan Huang. 2019. Cross-lingual
natural language generation via pre-training. CoRR ,
abs/1909.10481.

Zewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-
sham Singhal, Wenhui Wang, Xia Song, Xian-
Ling Mao, Heyan Huang, and Ming Zhou. 2020.
Infoxlm: An information-theoretic framework for
cross-lingual language model pre-training. CoRR ,
abs/2007.07834.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. CoRR ,
abs/1911.02116.

Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad-
ina Williams, Samuel R Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018. Xnli: Evaluating cross-
lingual sentence representations. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP) .

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Proceedings of the First Inter-
national Conference on Machine Learning Chal-
lenges: Evaluating Predictive Uncertainty Visual
Object Classiﬁcation, and Recognizing Textual En-
tailment , MLCW'05, pages 177–190, Berlin, Hei-
delberg. Springer-Verlag.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR , abs/1810.04805.

William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing (IWP2005) .

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019. Uniﬁed language
model pre-training for natural language understand-
ing and generation. In 33rd Conference on Neural
Information Processing Systems (NeurIPS 2019) .

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing , pages 1–9, Prague. Association
for Computational Linguistics.

--- TRANG 10 ---
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
CoRR , abs/1503.02531.

Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao
Chen, and Qun Liu. 2020. Dynabert: Dynamic
BERT with adaptive width and depth. In Advances
in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual .

Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang,
Dongsheng Li, Nan Yang, and Ming Zhou. 2018.
Attention-guided answer distillation for machine
reading comprehension. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31 -
November 4, 2018 , pages 2077–2086.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2019. Tinybert: Distilling BERT for natural lan-
guage understanding. CoRR , abs/1909.10351.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,
Luke Zettlemoyer, and Omer Levy. 2019. Spanbert:
Improving pre-training by representing and predict-
ing spans. arXiv preprint arXiv:1907.10529 .

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations , San
Diego, CA.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. Advances in
Neural Information Processing Systems (NeurIPS) .

Hector Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Thirteenth International Conference on the Princi-
ples of Knowledge Representation and Reasoning .

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2019a. BART: Denoising sequence-to-sequence
pre-training for natural language generation, trans-
lation, and comprehension. arXiv preprint
arXiv:1910.13461 .

Patrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebas-
tian Riedel, and Holger Schwenk. 2019b. MLQA:
evaluating cross-lingual extractive question answer-
ing. CoRR , abs/1910.07475.

Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng
Xu, Min Yang, and Yaohong Jin. 2020. BERT-EMD:
many-to-many layer mapping for BERT compres-
sion with earth mover's distance. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 3009–3018. Associ-
ation for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692 .

Subhabrata Mukherjee and Ahmed Hassan Awadallah.
2020. Xtremedistil: Multi-stage distillation for mas-
sive multilingual models. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2020, Online, July 5-10,
2020 , pages 2221–2234. Association for Computa-
tional Linguistics.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. arXiv preprint arXiv:1910.10683 .

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don't know: Unanswerable ques-
tions for SQuAD. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 2: Short Papers , pages 784–
789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2015. Fitnets: Hints for thin deep nets. In
3rd International Conference on Learning Represen-
tations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings .

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and
Preslav Nakov. 2020. Poor man's BERT:
smaller and faster transformer models. CoRR ,
abs/2004.03844.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR ,
abs/1910.01108.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing ,
pages 1631–1642.

--- TRANG 11 ---
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence
pre-training for language generation. arXiv preprint
arXiv:1905.02450 .

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019a.
Patient knowledge distillation for BERT model com-
pression. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Nat-
ural Language Processing, EMNLP-IJCNLP 2019,
Hong Kong, China, November 3-7, 2019 , pages
4322–4331.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2019b. Mobile-
bert: Task-agnostic compression of bert by progres-
sive knowledge transfer.

Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-
vazhagan, Xin Li, and Amelia Archer. 2019. Small
and practical BERT models for sequence labeling.
InProceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , pages 3630–
3634.

Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019. Well-read students learn better:
The impact of student initialization on knowledge
distillation. CoRR , abs/1908.08962.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30 , pages 5998–6008. Curran Asso-
ciates, Inc.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Inter-
national Conference on Learning Representations .

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In Advances in Neural
Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .

Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018. Neural network acceptability judgments.
arXiv preprint arXiv:1805.12471 .

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume

Corpus #Train #Dev #Test Metrics
Nhiệm vụ Câu Đơn
CoLA 8.5k 1k 1k Matthews Corr
SST-2 67k 872 1.8k Accuracy
Nhiệm vụ Tương tự và Diễn giải
QQP 364k 40k 391k Accuracy/F1
MRPC 3.7k 408 1.7k Accuracy/F1
STS-B 7k 1.5k 1.4k Pearson/Spearman Corr
Nhiệm vụ Suy luận
MNLI 393k 20k 20k Accuracy
RTE 2.5k 276 3k Accuracy
QNLI 105k 5.5k 5.5k Accuracy
WNLI 634 71 146 Accuracy

Bảng 11: Tóm tắt bộ đánh giá GLUE.

#Train #Dev #Test Metrics
130,319 11,873 8,862 Exact Match/F1

Bảng 12: Thống kê bộ dữ liệu và thước đo của SQuAD 2.0.

1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,
and Ming Zhou. 2020. Bert-of-theseus: Compress-
ing BERT by progressive module replacing. CoRR ,
abs/2002.02925.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.
XLNet: Generalized autoregressive pretraining for
language understanding. In 33rd Conference on
Neural Information Processing Systems (NeurIPS
2019) .

Sergey Zagoruyko and Nikos Komodakis. 2017. Pay-
ing more attention to attention: Improving the per-
formance of convolutional neural networks via at-
tention transfer. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings .

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE inter-
national conference on computer vision , pages 19–
27.

A Bộ đánh giá GLUE
Tóm tắt các bộ dữ liệu được sử dụng cho bộ đánh giá Hiểu Ngôn ngữ Chung (GLUE)4(Wang et al., 2019) được trình bày trong Bảng 11.

4https://gluebenchmark.com/

--- TRANG 12 ---
B SQuAD 2.0
Chúng tôi trình bày thống kê bộ dữ liệu và thước đo của SQuAD 2.05(Rajpurkar et al., 2018) trong Bảng 12.

C Siêu tham số cho Tinh chỉnh
Trả lời Câu hỏi Trích xuất Đối với SQuAD 2.0, độ dài chuỗi tối đa là 384. Kích thước lô được đặt là 32. Chúng tôi chọn tốc độ học từ {3e-5, 6e-5, 8e-5, 9e-5} và tinh chỉnh mô hình trong 3 epoch. Tỷ lệ khởi động và suy giảm trọng số là 0.1 và 0.01.

GLUE Độ dài chuỗi tối đa là 128 cho bộ đánh giá GLUE. Chúng tôi đặt kích thước lô là 32, chọn tốc độ học từ {1e-5, 1.5e-5, 2e-5, 3e-5, 5e-5} và epoch từ {3,5,10} cho các mô hình học sinh khác nhau. Chúng tôi tinh chỉnh nhiệm vụ CoLA với các bước huấn luyện dài hơn (25 epoch). Tỷ lệ khởi động và suy giảm trọng số là 0.1 và 0.01.

Suy luận Ngôn ngữ Tự nhiên Đa ngôn ngữ (XNLI) Độ dài chuỗi tối đa là 256 cho XNLI. Chúng tôi tinh chỉnh 10 epoch sử dụng 64 làm kích thước lô. Tốc độ học được chọn từ {3e-5, 4e-5, 5e-5, 6e-5}.

Trả lời Câu hỏi Đa ngôn ngữ Đối với MLQA, độ dài chuỗi tối đa là 512. Chúng tôi tinh chỉnh 4 epoch sử dụng 32 làm kích thước lô. Tốc độ học được chọn từ {3e-5, 4e-5, 5e-5, 6e-5}.

5http://stanford-qa.com
