# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/attention/2012.15828.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 537362 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
MINILMv2: ChÆ°ng cáº¥t Má»‘i quan há»‡ Tá»± ChÃº Ã½ Äa Äáº§u
Ä‘á»ƒ NÃ©n cÃ¡c Transformer ÄÆ°á»£c Huáº¥n luyá»‡n TrÆ°á»›c
Wenhui Wang Hangbo Bao Shaohan Huang Li Dong Furu Wei
Microsoft Research
{wenwan,t-habao,shaohanh,lidong1,fuwei}@microsoft.com
TÃ³m táº¯t
ChÃºng tÃ´i tá»•ng quÃ¡t hÃ³a viá»‡c chÆ°ng cáº¥t tá»± chÃº Ã½ sÃ¢u
trong M INILM (Wang et al., 2020) báº±ng cÃ¡ch chá»‰ sá»­
dá»¥ng chÆ°ng cáº¥t má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘á»ƒ nÃ©n báº¥t kháº£
tri nhiá»‡m vá»¥ cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c.
Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a cÃ¡c má»‘i quan há»‡ tá»± chÃº
Ã½ Ä‘a Ä‘áº§u lÃ  tÃ­ch vÃ´ hÆ°á»›ng cÃ³ tá»· lá»‡ giá»¯a cÃ¡c cáº·p vector
truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ trong má»—i mÃ´-Ä‘un tá»± chÃº Ã½.
Sau Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng kiáº¿n thá»©c má»‘i quan há»‡ trÃªn
Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c sinh. BÃªn cáº¡nh tÃ­nh Ä‘Æ¡n
giáº£n vÃ  nguyÃªn táº¯c thá»‘ng nháº¥t, thuáº­n lá»£i hÆ¡n, khÃ´ng
cÃ³ háº¡n cháº¿ vá» sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a há»c sinh, trong
khi háº§u háº¿t cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y pháº£i Ä‘áº£m báº£o cÃ¹ng
sá»‘ Ä‘áº§u giá»¯a giÃ¡o viÃªn vÃ  há»c sinh. HÆ¡n ná»¯a, cÃ¡c má»‘i
quan há»‡ tá»± chÃº Ã½ tinh vi cÃ³ xu hÆ°á»›ng khai thÃ¡c Ä‘áº§y Ä‘á»§
kiáº¿n thá»©c tÆ°Æ¡ng tÃ¡c Ä‘Æ°á»£c há»c bá»Ÿi Transformer. NgoÃ i
ra, chÃºng tÃ´i kiá»ƒm tra ká»¹ lÆ°á»¡ng chiáº¿n lÆ°á»£c lá»±a chá»n
lá»›p cho cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn, thay vÃ¬ chá»‰ dá»±a vÃ o
lá»›p cuá»‘i nhÆ° trong M INILM. ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c
thÃ­ nghiá»‡m má»Ÿ rá»™ng vá» nÃ©n cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n
luyá»‡n trÆ°á»›c Ä‘Æ¡n ngá»¯ vÃ  Ä‘a ngá»¯. Káº¿t quáº£ thÃ­ nghiá»‡m
chá»©ng minh ráº±ng cÃ¡c mÃ´ hÃ¬nh1 cá»§a chÃºng tÃ´i Ä‘Æ°á»£c
chÆ°ng cáº¥t tá»« cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ vÃ  lá»›n
(BERT, RoBERTa vÃ  XLM-R) vÆ°á»£t trá»™i hÆ¡n so vá»›i
hiá»‡n Ä‘áº¡i nháº¥t.

1 Giá»›i thiá»‡u
CÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Yang et al.,
2019; Joshi et al., 2019; Liu et al., 2019; Bao et al.,
2020; Radford et al., 2019; Raffel et al., 2019;
Lewis et al., 2019a) Ä‘Ã£ ráº¥t thÃ nh cÃ´ng cho má»™t
loáº¡t rá»™ng cÃ¡c nhiá»‡m vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.
Tuy nhiÃªn, nhá»¯ng mÃ´ hÃ¬nh nÃ y thÆ°á»ng bao gá»“m
hÃ ng trÄƒm triá»‡u tham sá»‘ vÃ  Ä‘ang ngÃ y cÃ ng lá»›n hÆ¡n. Äiá»u nÃ y mang láº¡i thÃ¡ch thá»©c cho viá»‡c tinh chá»‰nh vÃ  phá»¥c vá»¥ trá»±c tuyáº¿n trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿ do cÃ¡c háº¡n cháº¿ vá» tÃ i nguyÃªn tÃ­nh toÃ¡n vÃ  Ä‘á»™ trá»….

ChÆ°ng cáº¥t kiáº¿n thá»©c (KD; Hinton et al.
2015, Romero et al. 2015) Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ nÃ©n cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, Ä‘iá»u nÃ y chuyá»ƒn giao kiáº¿n thá»©c cá»§a mÃ´ hÃ¬nh lá»›n (giÃ¡o viÃªn) cho mÃ´ hÃ¬nh nhá» (há»c sinh) báº±ng cÃ¡ch giáº£m thiá»ƒu sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c Ä‘áº·c trÆ°ng cá»§a giÃ¡o viÃªn vÃ  há»c sinh.
CÃ¡c xÃ¡c suáº¥t má»¥c tiÃªu má»m (nhÃ£n má»m) vÃ  biá»ƒu diá»…n trung gian thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thá»±c hiá»‡n huáº¥n luyá»‡n KD. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i táº­p trung vÃ o nÃ©n báº¥t kháº£ tri nhiá»‡m vá»¥ cá»§a cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (Sanh et al., 2019; Tsai et al., 2019; Jiao et al.,
2019; Sun et al., 2019b; Wang et al., 2020). CÃ¡c mÃ´ hÃ¬nh há»c sinh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« cÃ¡c Transformer lá»›n Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c sá»­ dá»¥ng cÃ¡c kho dá»¯ liá»‡u vÄƒn báº£n quy mÃ´ lá»›n. MÃ´ hÃ¬nh báº¥t kháº£ tri nhiá»‡m vá»¥ Ä‘Æ°á»£c chÆ°ng cáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh trá»±c tiáº¿p trÃªn cÃ¡c nhiá»‡m vá»¥ háº¡ nguá»“n, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khá»Ÿi táº¡o chÆ°ng cáº¥t Ä‘áº·c thÃ¹ nhiá»‡m vá»¥.

DistilBERT (Sanh et al., 2019) sá»­ dá»¥ng cÃ¡c xÃ¡c suáº¥t má»¥c tiÃªu má»m cho dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ máº·t náº¡ vÃ  Ä‘áº§u ra nhÃºng Ä‘á»ƒ huáº¥n luyá»‡n há»c sinh. MÃ´ hÃ¬nh há»c sinh Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« giÃ¡o viÃªn báº±ng cÃ¡ch láº¥y má»™t lá»›p trong hai. TinyBERT (Jiao et al., 2019) sá»­ dá»¥ng cÃ¡c tráº¡ng thÃ¡i áº©n vÃ  phÃ¢n phá»‘i tá»± chÃº Ã½ (tá»©c lÃ  báº£n Ä‘á»“ chÃº Ã½ vÃ  trá»ng sá»‘), vÃ  Ã¡p dá»¥ng hÃ m Ä‘á»“ng nháº¥t Ä‘á»ƒ Ã¡nh xáº¡ cÃ¡c lá»›p há»c sinh vÃ  giÃ¡o viÃªn cho chÆ°ng cáº¥t theo lá»›p. MobileBERT (Sun et al., 2019b) giá»›i thiá»‡u cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn vÃ  há»c sinh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t sá»­ dá»¥ng cáº¥u trÃºc nghá»‹ch Ä‘áº£o-bottleneck vÃ  bottleneck Ä‘á»ƒ giá»¯ sá»‘ lá»›p vÃ  kÃ­ch thÆ°á»›c áº©n cá»§a chÃºng giá»‘ng nhau, chuyá»ƒn giao theo lá»›p cÃ¡c tráº¡ng thÃ¡i áº©n vÃ  phÃ¢n phá»‘i tá»± chÃº Ã½. MINILM(Wang et al., 2020) Ä‘á» xuáº¥t chÆ°ng cáº¥t tá»± chÃº Ã½ sÃ¢u, sá»­ dá»¥ng phÃ¢n phá»‘i tá»± chÃº Ã½ vÃ  má»‘i quan há»‡ giÃ¡ trá»‹ Ä‘á»ƒ giÃºp há»c sinh báº¯t chÆ°á»›c sÃ¢u sáº¯c cÃ¡c mÃ´-Ä‘un tá»± chÃº Ã½ cá»§a giÃ¡o viÃªn. MINILMcho tháº¥y ráº±ng chuyá»ƒn giao kiáº¿n thá»©c cá»§a lá»›p cuá»‘i cá»§a giÃ¡o viÃªn Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n so vá»›i chÆ°ng cáº¥t theo lá»›p.

arXiv:2012.15828v2  [cs.CL]  27 Jun 2021

--- TRANG 2 ---
lation. TÃ³m láº¡i, háº§u háº¿t cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y dá»±a vÃ o
phÃ¢n phá»‘i tá»± chÃº Ã½ Ä‘á»ƒ thá»±c hiá»‡n huáº¥n luyá»‡n KD,
dáº«n Ä‘áº¿n háº¡n cháº¿ lÃ  sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a mÃ´ hÃ¬nh há»c sinh pháº£i giá»‘ng vá»›i giÃ¡o viÃªn cá»§a nÃ³.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i tá»•ng quÃ¡t hÃ³a vÃ  Ä‘Æ¡n giáº£n hÃ³a chÆ°ng cáº¥t tá»± chÃº Ã½ sÃ¢u cá»§a MINILM(Wang et al.,
2020) báº±ng cÃ¡ch sá»­ dá»¥ng chÆ°ng cáº¥t má»‘i quan há»‡ tá»± chÃº Ã½.
ChÃºng tÃ´i giá»›i thiá»‡u cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u
Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi tÃ­ch vÃ´ hÆ°á»›ng cÃ³ tá»· lá»‡ cá»§a cÃ¡c cáº·p truy váº¥n,
khÃ³a vÃ  giÃ¡ trá»‹, Ä‘iá»u nÃ y hÆ°á»›ng dáº«n viá»‡c huáº¥n luyá»‡n há»c sinh. Láº¥y vector truy váº¥n lÃ m vÃ­ dá»¥, Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c truy váº¥n cá»§a nhiá»u Ä‘áº§u má»‘i quan há»‡, chÃºng tÃ´i
trÆ°á»›c tiÃªn ná»‘i cÃ¡c vector truy váº¥n cá»§a cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau, vÃ  sau Ä‘Ã³ tÃ¡ch vector Ä‘Æ°á»£c ná»‘i theo sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ mong muá»‘n.
Sau Ä‘Ã³, Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn vÃ  há»c sinh vá»›i sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ khÃ¡c nhau, chÃºng tÃ´i cÃ³ thá»ƒ cÄƒn chá»‰nh truy váº¥n cá»§a chÃºng vá»›i cÃ¹ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ cho chÆ°ng cáº¥t. HÆ¡n ná»¯a, sá»­ dá»¥ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ lá»›n hÆ¡n mang láº¡i kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n, giÃºp há»c sinh Ä‘áº¡t Ä‘Æ°á»£c sá»± báº¯t chÆ°á»›c sÃ¢u hÆ¡n mÃ´-Ä‘un tá»± chÃº Ã½ cá»§a giÃ¡o viÃªn.

NgoÃ i ra, Ä‘á»‘i vá»›i cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n (24 lá»›p, 1024 kÃ­ch thÆ°á»›c áº©n), cÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng chá»‰ ra ráº±ng chuyá»ƒn giao má»™t lá»›p giá»¯a trÃªn cÃ³ xu hÆ°á»›ng hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n so vá»›i sá»­ dá»¥ng lá»›p cuá»‘i nhÆ° trong M INILM.

Káº¿t quáº£ thÃ­ nghiá»‡m cho tháº¥y cÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n ngá»¯ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT vÃ  RoBERTa, vÃ  cÃ¡c mÃ´ hÃ¬nh Ä‘a ngá»¯ Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« XLM-R vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i nháº¥t trong cÃ¡c kÃ­ch thÆ°á»›c tham sá»‘ khÃ¡c nhau. MÃ´ hÃ¬nh 6768(6 lá»›p, 768 kÃ­ch thÆ°á»›c áº©n) Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT LARGE nhanh hÆ¡n 2:0 láº§n, trong khi váº«n hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n BERT BASE. MÃ´ hÃ¬nh kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« RoBERTa LARGE vÆ°á»£t trá»™i hÆ¡n RoBERTa BASE sá»­ dá»¥ng Ã­t vÃ­ dá»¥ huáº¥n luyá»‡n hÆ¡n nhiá»u.

Äá»ƒ tÃ³m táº¯t, Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i bao gá»“m:
â€¢ChÃºng tÃ´i tá»•ng quÃ¡t hÃ³a vÃ  Ä‘Æ¡n giáº£n hÃ³a chÆ°ng cáº¥t tá»± chÃº Ã½ sÃ¢u trong MINILMbáº±ng cÃ¡ch giá»›i thiá»‡u chÆ°ng cáº¥t má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u, Ä‘iá»u nÃ y mang láº¡i kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n vÃ  cho phÃ©p linh hoáº¡t hÆ¡n cho sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a há»c sinh.

â€¢ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m chÆ°ng cáº¥t má»Ÿ rá»™ng trÃªn cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n khÃ¡c nhau vÃ  tháº¥y ráº±ng sá»­ dá»¥ng kiáº¿n thá»©c cá»§a lá»›p giá»¯a trÃªn cá»§a giÃ¡o viÃªn Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n.

â€¢Káº¿t quáº£ thÃ­ nghiá»‡m chá»©ng minh tÃ­nh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i cho cÃ¡c giÃ¡o viÃªn Ä‘Æ¡n ngá»¯ vÃ  Ä‘a ngá»¯ khÃ¡c nhau trong kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ vÃ  lá»›n.

2 CÃ´ng trÃ¬nh liÃªn quan

2.1 Máº¡ng xÆ°Æ¡ng sá»‘ng: Transformer
Transformer Ä‘a lá»›p (Vaswani et al., 2017) Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i trong cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. Má»—i lá»›p Transformer bao gá»“m má»™t lá»›p con tá»± chÃº Ã½ vÃ  má»™t lá»›p con feed-forward Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ theo vá»‹ trÃ­.

Tá»± ChÃº Ã½ Transformer dá»±a vÃ o tá»± chÃº Ã½ Ä‘a Ä‘áº§u Ä‘á»ƒ náº¯m báº¯t cÃ¡c phá»¥ thuá»™c giá»¯a cÃ¡c tá»«. Cho Ä‘áº§u ra cá»§a lá»›p Transformer trÆ°á»›c Ä‘Ã³Hlâˆ’1âˆˆRË†|x|Ã—dh, Ä‘áº§u ra cá»§a má»™t Ä‘áº§u tá»± chÃº Ã½Ol,a; aâˆˆ[1,Ah]Ä‘Æ°á»£c tÃ­nh toÃ¡n qua:

Ql,a=Hlâˆ’1WQ l,a(1)
Kl,a=Hlâˆ’1WK l,a (2)
Vl,a=Hlâˆ’1WV l,a (3)
Ol,a= softmax(Ql,aKT l,a/âˆšdk)Vl,a (4)

Äáº§u ra cá»§a lá»›p trÆ°á»›c Ä‘Æ°á»£c chiáº¿u tuyáº¿n tÃ­nh thÃ nh truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ sá»­ dá»¥ng cÃ¡c ma tráº­n tham sá»‘WQ l,a,WK l,a,WV l,aâˆˆRdhÃ—dk, tÆ°Æ¡ng á»©ng. CÃ¡c phÃ¢n phá»‘i tá»± chÃº Ã½ Ä‘Æ°á»£c tÃ­nh toÃ¡n qua tÃ­ch vÃ´ hÆ°á»›ng cÃ³ tá»· lá»‡ cá»§a truy váº¥n vÃ  khÃ³a. Nhá»¯ng trá»ng sá»‘ nÃ y Ä‘Æ°á»£c gÃ¡n cho cÃ¡c vector giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ä‘áº§u ra chÃº Ã½. |x| biá»ƒu thá»‹ Ä‘á»™ dÃ i cá»§a chuá»—i Ä‘áº§u vÃ o. Ah vÃ  dh chá»‰ ra sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ vÃ  kÃ­ch thÆ°á»›c áº©n. dk lÃ  kÃ­ch thÆ°á»›c Ä‘áº§u chÃº Ã½. dkÃ—Ah thÆ°á»ng báº±ng dh.

2.2 MÃ´ hÃ¬nh NgÃ´n ngá»¯ Ä‘Æ°á»£c Huáº¥n luyá»‡n TrÆ°á»›c
Huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã£ dáº«n Ä‘áº¿n nhá»¯ng cáº£i thiá»‡n máº¡nh máº½ trÃªn nhiá»u nhiá»‡m vá»¥ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ°á»£c há»c trÃªn lÆ°á»£ng lá»›n dá»¯ liá»‡u vÄƒn báº£n, vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ thÃ­ch á»©ng vá»›i cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ. BERT (Devlin et al., 2018) Ä‘á» xuáº¥t huáº¥n luyá»‡n trÆ°á»›c má»™t Transformer hai chiá»u sÃ¢u sá»­ dá»¥ng má»¥c tiÃªu mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ máº·t náº¡ (MLM).
UNILM (Dong et al., 2019) Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c chung trÃªn ba loáº¡i má»¥c tiÃªu mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘á»ƒ thÃ­ch á»©ng vá»›i cáº£ nhiá»‡m vá»¥ hiá»ƒu vÃ  sinh.
RoBERTa (Liu et al., 2019) Ä‘áº¡t hiá»‡u suáº¥t máº¡nh máº½ báº±ng cÃ¡ch huáº¥n luyá»‡n cÃ¡c bÆ°á»›c dÃ i hÆ¡n sá»­ dá»¥ng kÃ­ch thÆ°á»›c lÃ´ lá»›n vÃ  nhiá»u dá»¯ liá»‡u vÄƒn báº£n hÆ¡n. MASS (Song et al., 2019), T5 (Raffel et al., 2019) vÃ  BART (Lewis et al.,

--- TRANG 3 ---
â„1ğ‘™âˆ’1â„2ğ‘™âˆ’1â„3ğ‘™âˆ’1â„4ğ‘™âˆ’1â„5ğ‘™âˆ’1Truy váº¥n KhÃ³a GiÃ¡ trá»‹
Tuyáº¿n tÃ­nhNá»‘iTÃ¡châ€¦
Q-Q Att-Rel K-K Att-Rel V-V Att-Rel â€¦
Tráº¡ng thÃ¡i áº¨n
(â„ğ‘¥Ã—ğ‘‘â„)Vector ChÃº Ã½
Äa Äáº§u
(â„ğ´â„Ã—ğ‘¥Ã—ğ‘‘ğ‘˜)Vector Má»‘i quan há»‡
Äa Äáº§u
(â„ğ´ğ‘ŸÃ—ğ‘¥Ã—ğ‘‘ğ‘Ÿ)Má»‘i quan há»‡
Tá»± ChÃº Ã½
(â„ğ´ğ‘ŸÃ—ğ‘¥Ã—|ğ‘¥|)

GiÃ¡o viÃªn (MÃ´ hÃ¬nh Lá»›n)Há»c sinh (MÃ´ hÃ¬nh Nhá»)Q-Q Att-Rel K-K Att-Rel V-V Att-Rel
Q-Q Att-Rel K-K Att-Rel V-V Att-Rel
Má»‘i quan há»‡ Tá»± ChÃº Ã½ (Lá»›p Cuá»‘i hoáº·c Lá»›p Giá»¯a TrÃªn)Má»‘i quan há»‡ Tá»± ChÃº Ã½ (Lá»›p Cuá»‘i)
Chuyá»ƒn giao Má»‘i quan há»‡
(KL-Divergence)
NhÃºng Äáº§u vÃ oTá»± ChÃº Ã½
Äa Äáº§uFeed Forward
Cá»™ng & Chuáº©nCá»™ng & Chuáº©n
ğ¿xNhÃºng Äáº§u vÃ oTá»± ChÃº Ã½
Äa Äáº§uFeed Forward
Cá»™ng & Chuáº©nCá»™ng & Chuáº©n
ğ‘€x
TÃ­ch VÃ´ hÆ°á»›ng

HÃ¬nh 1: Tá»•ng quan vá» chÆ°ng cáº¥t má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u. ChÃºng tÃ´i giá»›i thiá»‡u cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u Ä‘Æ°á»£c tÃ­nh toÃ¡n bá»Ÿi tÃ­ch vÃ´ hÆ°á»›ng cÃ³ tá»· lá»‡ cá»§a cÃ¡c cáº·p truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ Ä‘á»ƒ hÆ°á»›ng dáº«n viá»‡c huáº¥n luyá»‡n há»c sinh. Äá»ƒ cÃ³ Ä‘Æ°á»£c vector (truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹) cá»§a nhiá»u Ä‘áº§u má»‘i quan há»‡, chÃºng tÃ´i trÆ°á»›c tiÃªn ná»‘i cÃ¡c vector tá»± chÃº Ã½ cá»§a cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau vÃ  sau Ä‘Ã³ tÃ¡ch chÃºng theo sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ mong muá»‘n. ChÃºng tÃ´i chá»n chuyá»ƒn giao cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Q-Q, K-K vÃ  V-V Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  tá»‘c Ä‘á»™ huáº¥n luyá»‡n. Äá»‘i vá»›i giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n, chÃºng tÃ´i chuyá»ƒn giao kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a má»™t lá»›p giá»¯a trÃªn cá»§a giÃ¡o viÃªn. Äá»‘i vá»›i giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ, sá»­ dá»¥ng lá»›p cuá»‘i Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n.

2019a) sá»­ dá»¥ng cáº¥u trÃºc mÃ£ hÃ³a-giáº£i mÃ£ tiÃªu chuáº©n vÃ  huáº¥n luyá»‡n trÆ°á»›c bá»™ giáº£i mÃ£ tá»± há»“i quy. BÃªn cáº¡nh cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ¡n ngá»¯, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘a ngá»¯ (Devlin et al., 2018; Lample and Conneau, 2019; Chi et al., 2019; Conneau et al., 2019; Chi et al., 2020) cÅ©ng thÃºc Ä‘áº©y hiá»‡n Ä‘áº¡i nháº¥t vá» hiá»ƒu vÃ  sinh Ä‘a ngÃ´n ngá»¯.

2.3 ChÆ°ng cáº¥t Kiáº¿n thá»©c
ChÆ°ng cáº¥t kiáº¿n thá»©c Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  má»™t cÃ¡ch Ä‘áº§y há»©a háº¹n Ä‘á»ƒ nÃ©n cÃ¡c mÃ´ hÃ¬nh lá»›n trong khi váº«n duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c. Kiáº¿n thá»©c cá»§a má»™t hoáº·c má»™t táº­p há»£p cÃ¡c mÃ´ hÃ¬nh lá»›n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hÆ°á»›ng dáº«n viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh nhá». Hinton et al. (2015) Ä‘á» xuáº¥t sá»­ dá»¥ng cÃ¡c xÃ¡c suáº¥t má»¥c tiÃªu má»m Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c sinh. Kiáº¿n thá»©c tinh vi hÆ¡n nhÆ° tráº¡ng thÃ¡i áº©n (Romero et al., 2015) vÃ  phÃ¢n phá»‘i chÃº Ã½ (Zagoruyko and Komodakis, 2017; Hu et al., 2018) Ä‘Æ°á»£c giá»›i thiá»‡u Ä‘á»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh há»c sinh.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i táº­p trung vÃ o chÆ°ng cáº¥t kiáº¿n thá»©c báº¥t kháº£ tri nhiá»‡m vá»¥ cá»§a cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. MÃ´ hÃ¬nh báº¥t kháº£ tri nhiá»‡m vá»¥ Ä‘Æ°á»£c chÆ°ng cáº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ thÃ­ch á»©ng vá»›i cÃ¡c nhiá»‡m vá»¥ háº¡ nguá»“n. NÃ³ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khá»Ÿi táº¡o chÆ°ng cáº¥t Ä‘áº·c thÃ¹ nhiá»‡m vá»¥ (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), sá»­ dá»¥ng mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ hÆ°á»›ng dáº«n viá»‡c huáº¥n luyá»‡n há»c sinh trÃªn cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ. Kiáº¿n thá»©c Ä‘Æ°á»£c sá»­ dá»¥ng cho chÆ°ng cáº¥t vÃ  hÃ m Ã¡nh xáº¡ lá»›p lÃ  hai Ä‘iá»ƒm chÃ­nh cho chÆ°ng cáº¥t báº¥t kháº£ tri nhiá»‡m vá»¥ cá»§a cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. Háº§u háº¿t cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y sá»­ dá»¥ng cÃ¡c xÃ¡c suáº¥t má»¥c tiÃªu má»m, tráº¡ng thÃ¡i áº©n, phÃ¢n phá»‘i tá»± chÃº Ã½ vÃ  má»‘i quan há»‡ giÃ¡ trá»‹ Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c sinh.
Äá»‘i vá»›i hÃ m Ã¡nh xáº¡ lá»›p, TinyBERT (Jiao et al., 2019) sá»­ dá»¥ng chiáº¿n lÆ°á»£c Ä‘á»“ng nháº¥t Ä‘á»ƒ Ã¡nh xáº¡ cÃ¡c lá»›p giÃ¡o viÃªn vÃ  há»c sinh. MobileBERT (Sun et al., 2019b) giáº£ Ä‘á»‹nh há»c sinh cÃ³ cÃ¹ng sá»‘ lÆ°á»£ng lá»›p vá»›i giÃ¡o viÃªn Ä‘á»ƒ thá»±c hiá»‡n chÆ°ng cáº¥t theo lá»›p.
MINILM(Wang et al., 2020) chuyá»ƒn giao kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a lá»›p cuá»‘i cá»§a giÃ¡o viÃªn Ä‘áº¿n lá»›p Transformer cuá»‘i cá»§a há»c sinh. KhÃ¡c vá»›i cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u Ä‘á»ƒ loáº¡i bá» háº¡n cháº¿ vá» sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a há»c sinh. HÆ¡n ná»¯a, chÃºng tÃ´i cho tháº¥y ráº±ng chuyá»ƒn giao kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a má»™t lá»›p giá»¯a trÃªn cá»§a mÃ´ hÃ¬nh giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n hiá»‡u quáº£ hÆ¡n.

3 ChÆ°ng cáº¥t Má»‘i quan há»‡ Tá»± ChÃº Ã½ Äa Äáº§u

Theo MINILM(Wang et al., 2020), Ã½ tÆ°á»Ÿng chÃ­nh cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i lÃ  báº¯t chÆ°á»›c sÃ¢u sáº¯c mÃ´-Ä‘un tá»± chÃº Ã½ cá»§a giÃ¡o viÃªn, Ä‘iá»u nÃ y váº½ ra cÃ¡c phá»¥ thuá»™c giá»¯a cÃ¡c tá»« vÃ  lÃ  thÃ nh pháº§n quan trá»ng cá»§a Transformer. MINILMsá»­ dá»¥ng phÃ¢n phá»‘i tá»± chÃº Ã½ cá»§a giÃ¡o viÃªn Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c sinh. Äiá»u nÃ y mang láº¡i

--- TRANG 4 ---
MÃ´ hÃ¬nh GiÃ¡o viÃªn #Param TÄƒng tá»‘c SQuAD2 MNLI-m QNLI QQP RTE SST MRPC CoLA Trung bÃ¬nh
BERT BASE - 109M1.0 76.8 84.5 91.7 91.3 68.6 93.2 87.3 58.9 81.5
RoBERTa BASE - 125M1.0 83.7 87.6 92.8 91.9 78.7 94.8 90.2 63.6 85.4
BERT SMALL - 66M2.0 73.2 81.8 89.8 90.6 67.9 91.2 84.9 53.5 79.1
Truncated BERT BASE - 66M2.0 69.9 81.2 87.9 90.4 65.5 90.8 82.7 41.4 76.2
Truncated RoBERTa BASE - 81M2.0 77.9 84.9 91.1 91.3 67.9 92.9 87.5 55.2 81.1
DistilBERT BERT BASE 66M2.0 70.7 82.2 89.2 88.5 59.9 91.3 87.5 51.3 77.6
TinyBERT BERT BASE 66M2.0 73.1 83.5 90.5 90.6 72.2 91.6 88.4 42.8 79.1
MINILM BERT BASE 66M2.0 76.4 84.0 91.0 91.0 71.5 92.0 88.4 49.2 80.4
6384Ours BERT BASE 22M5.3 72.9 82.8 90.3 90.6 68.9 91.3 86.6 41.8 78.2
6384Ours BERT LARGE 22M5.3 74.3 83.0 90.4 90.7 68.5 91.1 87.8 41.6 78.4
6384Ours RoBERTa LARGE 30M5.3 76.4 84.4 90.9 90.8 69.9 92.0 88.7 42.6 79.5
6768Ours BERT BASE 66M2.0 76.3 84.2 90.8 91.1 72.1 92.4 88.9 52.5 81.0
6768Ours BERT LARGE 66M2.0 77.7 85.0 91.4 91.1 73.0 92.5 88.9 53.9 81.7
6768Ours RoBERTa LARGE 81M2.0 81.6 87.0 92.7 91.4 78.7 94.5 90.4 54.0 83.8

Báº£ng 1: Káº¿t quáº£ cá»§a cÃ¡c há»c sinh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ vÃ  lá»›n trÃªn bá»™ phÃ¡t triá»ƒn cá»§a GLUE vÃ  SQuAD 2.0. ChÃºng tÃ´i bÃ¡o cÃ¡o F1 cho SQuAD 2.0, há»‡ sá»‘ tÆ°Æ¡ng quan Matthews cho CoLA, vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho cÃ¡c bá»™ dá»¯ liá»‡u khÃ¡c. Káº¿t quáº£ GLUE cá»§a DistilBERT Ä‘Æ°á»£c láº¥y tá»« Sanh et al. (2019). CÃ¡c káº¿t quáº£ cÃ²n láº¡i cá»§a DistilBERT, TinyBERT2, BERT SMALL, Truncated BERT BASE vÃ  M INILM Ä‘Æ°á»£c láº¥y tá»« Wang et al. (2020). BERT SMALL (Turc et al., 2019) Ä‘Æ°á»£c huáº¥n luyá»‡n sá»­ dá»¥ng má»¥c tiÃªu MLM, khÃ´ng sá»­ dá»¥ng KD. ChÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o káº¿t quáº£ cá»§a truncated BERT BASE vÃ  truncated RoBERTa BASE, loáº¡i bá» 6 lá»›p trÃªn cá»§a mÃ´ hÃ¬nh cÆ¡ sá»Ÿ. Viá»‡c bá» lá»›p trÃªn Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  má»™t Ä‘Æ°á»ng cÆ¡ sá»Ÿ máº¡nh (Sajjad et al., 2020). Káº¿t quáº£ tinh chá»‰nh lÃ  trung bÃ¬nh cá»§a 4 láº§n cháº¡y.

háº¡n cháº¿ vá» sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a há»c sinh, Ä‘Æ°á»£c yÃªu cáº§u pháº£i giá»‘ng vá»›i giÃ¡o viÃªn cá»§a nÃ³. Äá»ƒ giá»›i thiá»‡u kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n vÃ  trÃ¡nh sá»­ dá»¥ng phÃ¢n phá»‘i tá»± chÃº Ã½ cá»§a giÃ¡o viÃªn, chÃºng tÃ´i tá»•ng quÃ¡t hÃ³a chÆ°ng cáº¥t tá»± chÃº Ã½ sÃ¢u trong MINILM vÃ  giá»›i thiá»‡u cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u cá»§a cÃ¡c cáº·p truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ Ä‘á»ƒ huáº¥n luyá»‡n há»c sinh. BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng vÃ  tháº¥y ráº±ng viá»‡c lá»±a chá»n lá»›p cá»§a mÃ´ hÃ¬nh giÃ¡o viÃªn lÃ  quan trá»ng Ä‘á»ƒ chÆ°ng cáº¥t cÃ¡c mÃ´ hÃ¬nh kÃ­ch thÆ°á»›c lá»›n. HÃ¬nh 1 Ä‘Æ°a ra tá»•ng quan vá» phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i.

3.1 Má»‘i quan há»‡ Tá»± ChÃº Ã½ Äa Äáº§u
CÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u Ä‘Æ°á»£c cÃ³ Ä‘Æ°á»£c bá»Ÿi tÃ­ch vÃ´ hÆ°á»›ng cÃ³ tá»· lá»‡ cá»§a cÃ¡c cáº·p3 truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ cá»§a nhiá»u Ä‘áº§u má»‘i quan há»‡. Láº¥y vector truy váº¥n lÃ m vÃ­ dá»¥, Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c truy váº¥n cá»§a nhiá»u Ä‘áº§u má»‘i quan há»‡, chÃºng tÃ´i trÆ°á»›c tiÃªn ná»‘i truy váº¥n cá»§a cÃ¡c Ä‘áº§u chÃº Ã½ khÃ¡c nhau vÃ  sau Ä‘Ã³ tÃ¡ch vector Ä‘Æ°á»£c ná»‘i dá»±a trÃªn sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ mong muá»‘n. CÃ¹ng thao tÃ¡c cÅ©ng Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn khÃ³a vÃ  giÃ¡ trá»‹. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn vÃ  há»c sinh sá»­ dá»¥ng sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ khÃ¡c nhau, chÃºng tÃ´i chuyá»ƒn Ä‘á»•i truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ cá»§a chÃºng tá»« sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ khÃ¡c nhau thÃ nh

2 NgoÃ i chÆ°ng cáº¥t báº¥t kháº£ tri nhiá»‡m vá»¥, TinyBERT sá»­ dá»¥ng chÆ°ng cáº¥t Ä‘áº·c thÃ¹ nhiá»‡m vá»¥ vÃ  tÄƒng cÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ cáº£i thiá»‡n thÃªm mÃ´ hÃ¬nh. ChÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ tinh chá»‰nh cá»§a mÃ´ hÃ¬nh báº¥t kháº£ tri nhiá»‡m vá»¥ cÃ´ng khai cá»§a há».

3 CÃ³ chÃ­n loáº¡i má»‘i quan há»‡ tá»± chÃº Ã½, nhÆ° má»‘i quan há»‡ truy váº¥n-truy váº¥n, khÃ³a-khÃ³a, khÃ³a-giÃ¡ trá»‹ vÃ  truy váº¥n-giÃ¡ trá»‹.

vector cá»§a cÃ¹ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ Ä‘á»ƒ thá»±c hiá»‡n huáº¥n luyá»‡n KD. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i loáº¡i bá» háº¡n cháº¿ vá» sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh há»c sinh. HÆ¡n ná»¯a, sá»­ dá»¥ng nhiá»u Ä‘áº§u má»‘i quan há»‡ hÆ¡n trong viá»‡c tÃ­nh toÃ¡n cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ mang láº¡i kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh há»c sinh.

ChÃºng tÃ´i sá»­ dá»¥ng A1;A2;A3 Ä‘á»ƒ biá»ƒu thá»‹ truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ cá»§a nhiá»u Ä‘áº§u má»‘i quan há»‡. KL-divergence giá»¯a cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u cá»§a giÃ¡o viÃªn vÃ  há»c sinh Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m má»¥c tiÃªu huáº¥n luyá»‡n:

L=3âˆ‘i=13âˆ‘j=1Î±ijLij (5)

Lij=1/Ar|x||x|âˆ‘Ar a=1âˆ‘|x| t=1DKL(RTij,l,a,tâˆ¥RSij,m,a,t )
(6)

RTij,l,a= softmax(ATi,l,aAT|j,l,a/âˆšdr) (7)

RSij,m,a = softmax(ASi,m,aAS|j,m,a/âˆšd'r) (8)

trong Ä‘Ã³ ATi,l,aâˆˆR|x|Ã—dr vÃ  ASi,m,aâˆˆR|x|Ã—d'r(iâˆˆ[1;3]) lÃ  truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹ cá»§a má»™t Ä‘áº§u má»‘i quan há»‡ cá»§a lá»›p thá»© l cá»§a giÃ¡o viÃªn vÃ  lá»›p thá»© m cá»§a há»c sinh.
dr vÃ  d'r lÃ  kÃ­ch thÆ°á»›c Ä‘áº§u má»‘i quan há»‡ cá»§a mÃ´ hÃ¬nh giÃ¡o viÃªn vÃ  há»c sinh. RTij,lâˆˆRArÃ—|x|Ã—|x|lÃ  má»‘i quan há»‡ tá»± chÃº Ã½ cá»§a ATi,l vÃ  ATj,l cá»§a mÃ´ hÃ¬nh giÃ¡o viÃªn.

--- TRANG 5 ---
RTij,l,aâˆˆR|x|Ã—|x|lÃ  má»‘i quan há»‡ tá»± chÃº Ã½ cá»§a má»™t Ä‘áº§u má»‘i quan há»‡ cá»§a giÃ¡o viÃªn. RSij,mâˆˆRArÃ—|x|Ã—|x|lÃ  má»‘i quan há»‡ tá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh há»c sinh. VÃ­ dá»¥, RT11,l biá»ƒu thá»‹ má»‘i quan há»‡ chÃº Ã½ Q-Q cá»§a giÃ¡o viÃªn trong HÃ¬nh 1. Ar lÃ  sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡. Náº¿u sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ vÃ  Ä‘áº§u chÃº Ã½ giá»‘ng nhau, má»‘i quan há»‡ Q-K tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i trá»ng sá»‘ chÃº Ã½ trong mÃ´-Ä‘un tá»± chÃº Ã½.

Î±ijâˆˆ{0,1} lÃ  trá»ng sá»‘ Ä‘Æ°á»£c gÃ¡n cho má»—i máº¥t mÃ¡t má»‘i quan há»‡ tá»± chÃº Ã½. ChÃºng tÃ´i chuyá»ƒn giao má»‘i quan há»‡ tá»± chÃº Ã½ truy váº¥n-truy váº¥n, khÃ³a-khÃ³a vÃ  giÃ¡ trá»‹-giÃ¡ trá»‹ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  chi phÃ­ huáº¥n luyá»‡n.

3.2 Lá»±a chá»n Lá»›p cá»§a MÃ´ hÃ¬nh GiÃ¡o viÃªn
BÃªn cáº¡nh kiáº¿n thá»©c Ä‘Æ°á»£c sá»­ dá»¥ng cho chÆ°ng cáº¥t, hÃ m Ã¡nh xáº¡ giá»¯a cÃ¡c lá»›p giÃ¡o viÃªn vÃ  há»c sinh lÃ  má»™t yáº¿u tá»‘ quan trá»ng khÃ¡c. NhÆ° trong MINILM, chÃºng tÃ´i chá»‰ chuyá»ƒn giao kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a má»™t trong cÃ¡c lá»›p giÃ¡o viÃªn Ä‘áº¿n lá»›p cuá»‘i cá»§a há»c sinh. Chá»‰ chÆ°ng cáº¥t má»™t lá»›p cá»§a mÃ´ hÃ¬nh giÃ¡o viÃªn lÃ  nhanh vÃ  hiá»‡u quáº£. KhÃ¡c vá»›i cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y thÆ°á»ng tiáº¿n hÃ nh thÃ­ nghiá»‡m trÃªn cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ, chÃºng tÃ´i thÃ­ nghiá»‡m vá»›i cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n khÃ¡c nhau vÃ  tháº¥y ráº±ng chuyá»ƒn giao kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a má»™t lá»›p giá»¯a trÃªn hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n so vá»›i sá»­ dá»¥ng cÃ¡c lá»›p khÃ¡c. Äá»‘i vá»›i BERT LARGE vÃ  BERT LARGE-WWM, chuyá»ƒn giao lá»›p thá»© 21 (báº¯t Ä‘áº§u tá»« má»™t) Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t. Äá»‘i vá»›i RoBERTa LARGE vÃ  XLM-R LARGE, sá»­ dá»¥ng kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a lá»›p thá»© 19 Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n. Äá»‘i vá»›i giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ, chÃºng tÃ´i cÅ©ng tháº¥y ráº±ng sá»­ dá»¥ng lá»›p cuá»‘i cá»§a giÃ¡o viÃªn hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n.

4 ThÃ­ nghiá»‡m
ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m chÆ°ng cáº¥t trÃªn cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn khÃ¡c nhau bao gá»“m BERT BASE, BERT LARGE, BERT LARGE-WWM, RoBERTa BASE, RoBERTa LARGE, XLM-R BASE vÃ  XLM-R LARGE.

4.1 Thiáº¿t láº­p
ChÃºng tÃ´i sá»­ dá»¥ng phiÃªn báº£n khÃ´ng phÃ¢n biá»‡t chá»¯ hoa chá»¯ thÆ°á»ng cho ba mÃ´ hÃ¬nh giÃ¡o viÃªn BERT. Äá»‘i vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c, chÃºng tÃ´i sá»­ dá»¥ng Wikipedia tiáº¿ng Anh vÃ  BookCorpus (Zhu et al., 2015). ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c sinh sá»­ dá»¥ng 256 lÃ m kÃ­ch thÆ°á»›c lÃ´ vÃ  6e-4 lÃ m tá»‘c Ä‘á»™ há»c Ä‘á»‰nh trong 400,000 bÆ°á»›c. ChÃºng tÃ´i sá»­ dá»¥ng khá»Ÿi Ä‘á»™ng tuyáº¿n tÃ­nh trong 4,000 bÆ°á»›c Ä‘áº§u tiÃªn vÃ  suy giáº£m tuyáº¿n tÃ­nh. ChÃºng tÃ´i sá»­ dá»¥ng Adam (Kingma and Ba, 2015) vá»›i Î²1 = 0:9, Î²2 = 0:999. Äá»™ dÃ i chuá»—i tá»‘i Ä‘a Ä‘Æ°á»£c Ä‘áº·t lÃ  512. Tá»· lá»‡ dropout vÃ  suy giáº£m trá»ng sá»‘ lÃ  0:1 vÃ  0:01. Sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ lÃ  12 cho táº¥t cáº£ mÃ´ hÃ¬nh há»c sinh. Sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ lÃ  48 vÃ  64 cho mÃ´ hÃ¬nh giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ vÃ  lá»›n, tÆ°Æ¡ng á»©ng. CÃ¡c mÃ´ hÃ¬nh há»c sinh Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn.

Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« RoBERTa, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c tÆ°Æ¡ng tá»± nhÆ° trong Liu et al. (2019). Äá»‘i vá»›i mÃ´ hÃ¬nh há»c sinh 12768, chÃºng tÃ´i sá»­ dá»¥ng Adam vá»›i Î²1 = 0:9, Î²2 = 0:98. CÃ¡c siÃªu tham sá»‘ cÃ²n láº¡i giá»‘ng vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT.

Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh há»c sinh Ä‘a ngá»¯ Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« XLM-R, chÃºng tÃ´i thá»±c hiá»‡n huáº¥n luyá»‡n sá»­ dá»¥ng cÃ¹ng bá»™ dá»¯ liá»‡u nhÆ° trong Conneau et al. (2019) trong 1,000,000 bÆ°á»›c. ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m chÆ°ng cáº¥t sá»­ dá»¥ng 8 GPU V100 vá»›i huáº¥n luyá»‡n Ä‘á»™ chÃ­nh xÃ¡c há»—n há»£p.

4.2 Nhiá»‡m vá»¥ Háº¡ nguá»“n
Theo cÃ´ng trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) vÃ  chÆ°ng cáº¥t báº¥t kháº£ tri nhiá»‡m vá»¥ (Sun et al., 2019b; Jiao et al., 2019) trÆ°á»›c Ä‘Ã¢y, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh há»c sinh tiáº¿ng Anh trÃªn bá»™ Ä‘Ã¡nh giÃ¡ GLUE vÃ  tráº£ lá»i cÃ¢u há»i trÃ­ch xuáº¥t. CÃ¡c mÃ´ hÃ¬nh Ä‘a ngá»¯ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn suy luáº­n ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘a ngÃ´n ngá»¯ vÃ  tráº£ lá»i cÃ¢u há»i Ä‘a ngÃ´n ngá»¯.

GLUE Bá»™ Ä‘Ã¡nh giÃ¡ Hiá»ƒu NgÃ´n ngá»¯ Chung (GLUE) (Wang et al., 2019) bao gá»“m hai nhiá»‡m vá»¥ phÃ¢n loáº¡i cÃ¢u Ä‘Æ¡n (SST-2 (Socher et al., 2013) vÃ  CoLA (Warstadt et al., 2018)), ba nhiá»‡m vá»¥ tÆ°Æ¡ng tá»± vÃ  diá»…n giáº£i (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) vÃ  QQP), vÃ  bá»‘n nhiá»‡m vá»¥ suy luáº­n (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli

--- TRANG 6 ---
MÃ´ hÃ¬nh GiÃ¡o viÃªn #Param SQuAD2 MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STS Trung bÃ¬nh
BERT BASE - 109M 1.0 76.8 84.6/83.4 90.5 71.2 66.4 93.5 88.9 52.1 85.8 79.3
BERT LARGE - 340M 0.3 81.9 86.7/85.9 92.7 72.1 70.1 94.9 89.3 60.5 86.5 82.1
6768Ours BERT BASE 66M 2.0 76.3 83.8/83.3 90.2 70.9 69.2 92.9 89.1 46.6 84.3 78.7
6768Ours BERT LARGE 66M 2.0 77.7 84.5/84.0 91.5 71.3 69.2 93.0 89.1 48.6 85.1 79.4

Báº£ng 2: Káº¿t quáº£ cá»§a cÃ¡c há»c sinh 6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT trÃªn bá»™ kiá»ƒm tra GLUE vÃ  bá»™ phÃ¡t triá»ƒn SQuAD 2.0. CÃ¡c káº¿t quáº£ Ä‘Æ°á»£c bÃ¡o cÃ¡o Ä‘Æ°á»£c tinh chá»‰nh trá»±c tiáº¿p trÃªn cÃ¡c nhiá»‡m vá»¥ háº¡ nguá»“n. ChÃºng tÃ´i bÃ¡o cÃ¡o F1 cho SQuAD 2.0, QQP vÃ  MRPC, tÆ°Æ¡ng quan Spearman cho STS-B, há»‡ sá»‘ tÆ°Æ¡ng quan Matthews cho CoLA vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho pháº§n cÃ²n láº¡i.

MÃ´ hÃ¬nh GiÃ¡o viÃªn #Param SQuAD2 MNLI-m QNLI QQP RTE SST MRPC CoLA STS Trung bÃ¬nh
BERT BASE - 109M 76.8 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 82.4
RoBERTa BASE - 125M 83.7 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2 86.1
12768Ours BERT LARGE 109M 81.8 86.5 92.6 91.6 76.4 93.3 89.2 62.3 90.5 84.9
12768Ours RoBERTa LARGE 125M 86.6 89.4 94.0 91.8 83.1 95.9 91.2 65.0 91.3 87.6

Báº£ng 3: Káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh 12768 trÃªn bá»™ phÃ¡t triá»ƒn cá»§a bá»™ Ä‘Ã¡nh giÃ¡ GLUE vÃ  SQuAD 2.0. Káº¿t quáº£ tinh chá»‰nh lÃ  trung bÃ¬nh cá»§a 4 láº§n cháº¡y cho má»—i nhiá»‡m vá»¥. ChÃºng tÃ´i bÃ¡o cÃ¡o F1 cho SQuAD 2.0, tÆ°Æ¡ng quan Pearson cho STS-B, há»‡ sá»‘ tÆ°Æ¡ng quan Matthews cho CoLA vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho pháº§n cÃ²n láº¡i.

MÃ´ hÃ¬nh SQuAD2 MNLI-m SST-2 Trung bÃ¬nh
MINILM (Lá»›p Cuá»‘i) 79.1 84.7 91.2 85.0
+ Lá»›p Giá»¯a TrÃªn 80.3 85.2 91.5 85.7
12384Ours 80.7 85.7 92.3 86.2

Báº£ng 4: So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau sá»­ dá»¥ng BERT LARGE-WWM lÃ m giÃ¡o viÃªn. ChÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ phÃ¡t triá»ƒn cá»§a mÃ´ hÃ¬nh há»c sinh 12384 vá»›i kÃ­ch thÆ°á»›c nhÃºng 128.

et al., 2009) vÃ  WNLI (Levesque et al., 2012)).

Tráº£ lá»i CÃ¢u há»i TrÃ­ch xuáº¥t Nhiá»‡m vá»¥ nháº±m dá»± Ä‘oÃ¡n má»™t phÃ¢n Ä‘oáº¡n con liÃªn tá»¥c cá»§a Ä‘oáº¡n vÄƒn Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ trÃªn SQuAD 2.0 (Rajpurkar et al., 2018), Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t bá»™ Ä‘Ã¡nh giÃ¡ tráº£ lá»i cÃ¢u há»i chÃ­nh.

Suy luáº­n NgÃ´n ngá»¯ Tá»± nhiÃªn Äa ngÃ´n ngá»¯ (XNLI) XNLI (Conneau et al., 2018) lÃ  má»™t bá»™ Ä‘Ã¡nh giÃ¡ phÃ¢n loáº¡i Ä‘a ngÃ´n ngá»¯. NÃ³ nháº±m xÃ¡c Ä‘á»‹nh má»‘i quan há»‡ ngá»¯ nghÄ©a giá»¯a hai cÃ¢u vÃ  cung cáº¥p cÃ¡c thá»ƒ hiá»‡n trong 15 ngÃ´n ngá»¯.

Tráº£ lá»i CÃ¢u há»i Äa ngÃ´n ngá»¯ ChÃºng tÃ´i sá»­ dá»¥ng MLQA (Lewis et al., 2019b) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘a ngá»¯. MLQA má»Ÿ rá»™ng bá»™ dá»¯ liá»‡u SQuAD tiáº¿ng Anh (Rajpurkar et al., 2016) sang báº£y ngÃ´n ngá»¯.

4.3 Káº¿t quáº£ ChÃ­nh
Báº£ng 1 trÃ¬nh bÃ y káº¿t quáº£ phÃ¡t triá»ƒn cá»§a cÃ¡c mÃ´ hÃ¬nh 6384 vÃ  6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE, BERT LARGE vÃ  RoBERTa LARGE trÃªn GLUE vÃ  SQuAD 2.0. (1) CÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Wang et al., 2020) thÆ°á»ng chÆ°ng cáº¥t BERT BASE thÃ nh mÃ´ hÃ¬nh 6 lá»›p vá»›i kÃ­ch thÆ°á»›c áº©n 768. ChÃºng tÃ´i trÆ°á»›c tiÃªn bÃ¡o cÃ¡o káº¿t quáº£ cá»§a cÃ¹ng thiáº¿t láº­p. MÃ´ hÃ¬nh 6768 cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n DistilBERT, TinyBERT, MINILM vÃ  hai Ä‘Æ°á»ng cÆ¡ sá»Ÿ BERT trÃªn háº§u háº¿t cÃ¡c nhiá»‡m vá»¥. HÆ¡n ná»¯a, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cho phÃ©p linh hoáº¡t hÆ¡n vá» sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a cÃ¡c mÃ´ hÃ¬nh há»c sinh. (2) Cáº£ mÃ´ hÃ¬nh 6384 vÃ  6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT LARGE Ä‘á»u vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE. MÃ´ hÃ¬nh 6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT LARGE nhanh hÆ¡n 2:0 láº§n so vá»›i BERT BASE, trong khi Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n. (3) CÃ¡c mÃ´ hÃ¬nh há»c sinh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« RoBERTa LARGE Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng cáº£i thiá»‡n thÃªm. GiÃ¡o viÃªn tá»‘t hÆ¡n dáº«n Ä‘áº¿n há»c sinh tá»‘t hÆ¡n. ChÆ°ng cáº¥t má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u hiá»‡u quáº£ cho cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c kÃ­ch thÆ°á»›c lá»›n khÃ¡c nhau.

ChÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ cá»§a cÃ¡c há»c sinh 6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE vÃ  BERT LARGE trÃªn bá»™ kiá»ƒm tra GLUE vÃ  bá»™ phÃ¡t triá»ƒn SQuAD 2.0 trong Báº£ng 2. MÃ´ hÃ¬nh 6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE giá»¯ láº¡i hÆ¡n 99% Ä‘á»™ chÃ­nh xÃ¡c cá»§a giÃ¡o viÃªn trong khi sá»­ dá»¥ng 50% tham sá»‘ Transformer. MÃ´ hÃ¬nh 6768 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT LARGE so sÃ¡nh thuáº­n lá»£i vá»›i BERT BASE.

ChÃºng tÃ´i nÃ©n RoBERTa LARGE vÃ  BERT LARGE thÃ nh mÃ´ hÃ¬nh há»c sinh kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ. Káº¿t quáº£ phÃ¡t triá»ƒn cá»§a bá»™ Ä‘Ã¡nh giÃ¡ GLUE vÃ  SQuAD 2.0 Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 3. CÃ¡c mÃ´ hÃ¬nh kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n vÆ°á»£t trá»™i hÆ¡n BERT BASE vÃ  RoBERTa BASE. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n há»c sinh trong kÃ­ch thÆ°á»›c tham sá»‘ khÃ¡c nhau. HÆ¡n ná»¯a, há»c sinh cá»§a chÃºng tÃ´i Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« RoBERTa LARGE sá»­ dá»¥ng kÃ­ch thÆ°á»›c lÃ´ huáº¥n luyá»‡n nhá» hÆ¡n nhiá»u (gáº§n 32 láº§n nhá» hÆ¡n) vÃ  Ã­t bÆ°á»›c huáº¥n luyá»‡n hÆ¡n so vá»›i RoBERTa BASE. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘Ã²i há»i Ã­t vÃ­ dá»¥ huáº¥n luyá»‡n hÆ¡n nhiá»u.

Háº§u háº¿t cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y tiáº¿n hÃ nh thÃ­ nghiá»‡m sá»­ dá»¥ng cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c cÆ¡ sá»Ÿ. Äá»ƒ so sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y trÃªn giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n, chÃºng tÃ´i tÃ¡i triá»ƒn khai MINILM vÃ  nÃ©n BERT LARGE-WWM thÃ nh mÃ´ hÃ¬nh há»c sinh 12384. Káº¿t quáº£ phÃ¡t triá»ƒn cá»§a SQuAD 2.0, MNLI-m vÃ  SST-2 Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 4. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÅ©ng vÆ°á»£t trá»™i hÆ¡n MINILM cho cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n. HÆ¡n ná»¯a, chÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ chÆ°ng cáº¥t má»™t lá»›p giá»¯a trÃªn thay vÃ¬ lá»›p cuá»‘i cho MINILM. Lá»±a chá»n lá»›p cÅ©ng hiá»‡u quáº£ cho MINILM khi chÆ°ng cáº¥t cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n.

Báº£ng 5 vÃ  Báº£ng 6 hiá»ƒn thá»‹ káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh há»c sinh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« XLM-R trÃªn XNLI vÃ  MLQA. Äá»‘i vá»›i XNLI, mÃ´ hÃ¬nh Ä‘Æ¡n tá»‘t nháº¥t Ä‘Æ°á»£c chá»n trÃªn bá»™ phÃ¡t triá»ƒn chung cá»§a táº¥t cáº£ cÃ¡c ngÃ´n ngá»¯ nhÆ° trong Conneau et al. (2019). Theo Lewis et al. (2019b), chÃºng tÃ´i Ã¡p dá»¥ng SQuAD 1.1 lÃ m dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ trÃªn bá»™ phÃ¡t triá»ƒn MLQA tiáº¿ng Anh cho

--- TRANG 7 ---
MÃ´ hÃ¬nh GiÃ¡o viÃªn #L #H #Param en fr es de el bg ru tr ar vi th zh hi sw ur Trung bÃ¬nh
mBERT - 12 768 170M 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0 66.3
XLM- 100 - 16 1280 570M 83.2 76.7 77.7 74.0 72.7 74.1 72.7 68.7 68.6 72.9 68.9 72.5 65.6 58.2 62.4 70.7
XLM-R BASE - 12 768 277M 85.8 79.7 80.7 78.7 77.5 79.6 78.1 74.2 73.8 76.5 74.6 76.7 72.4 66.5 68.3 76.2
MINILM XLM-R BASE 6 384 107M 79.2 72.3 73.1 70.3 69.1 72.0 69.1 64.5 64.9 69.0 66.0 67.8 62.9 59.0 60.6 68.0
6384Ours XLM-R BASE 6 384 107M 78.8 72.1 73.4 69.4 70.7 72.2 69.9 66.7 66.7 69.5 67.4 68.6 65.3 61.2 61.9 68.9
6384Ours XLM-R LARGE 6 384 107M 78.9 72.5 73.5 69.8 70.9 72.3 69.7 67.5 67.1 70.2 67.4 69.1 65.4 62.3 63.4 69.3
MINILM XLM-R BASE 12 384 117M 81.5 74.8 75.7 72.9 73.0 74.5 71.3 69.7 68.8 72.1 67.8 70.0 66.2 63.3 64.2 71.1
12384Ours XLM-R BASE 12 384 117M 82.2 75.3 76.5 73.3 73.8 75.1 73.1 70.7 69.5 72.8 69.4 71.5 68.6 65.6 65.5 72.2
12384Ours XLM-R LARGE 12 384 117M 82.4 76.2 76.8 73.9 74.6 75.5 73.7 70.8 70.5 73.4 71.1 72.5 68.8 66.8 67.0 72.9

Báº£ng 5: Káº¿t quáº£ phÃ¢n loáº¡i Ä‘a ngÃ´n ngá»¯ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘a ngá»¯ cá»§a chÃºng tÃ´i trÃªn XNLI. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c trÃªn má»—i trong 15 ngÃ´n ngá»¯ XNLI vÃ  Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh. #L vÃ  #H chá»‰ ra sá»‘ lÆ°á»£ng lá»›p vÃ  kÃ­ch thÆ°á»›c áº©n.

MÃ´ hÃ¬nh GiÃ¡o viÃªn #L #H #Param en es de ar hi vi zh Trung bÃ¬nh
mBERT - 12 768 170M 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.3 57.7 / 41.6
XLM- 15 - 12 1024 248M 74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.6 61.6 / 43.5
XLM-R BASE - 12 768 277M 77.1 / 64.6 67.4 / 49.6 60.9 / 46.7 54.9 / 36.6 59.4 / 42.9 64.5 / 44.7 61.8 / 39.3 63.7 / 46.3
XLM-R BASEy - 12 768 277M 80.3 / 67.4 67.0 / 49.2 62.7 / 48.3 55.0 / 35.6 60.4 / 43.7 66.5 / 45.9 62.3 / 38.3 64.9 / 46.9
MINILM XLM-R BASE 6 384 107M 75.5 / 61.9 55.6 / 38.2 53.3 / 37.7 43.5 / 26.2 46.9 / 31.5 52.0 / 33.1 48.8 / 27.3 53.7 / 36.6
6384Ours XLM-R BASE 6 384 107M 75.6 / 61.9 60.4 / 42.6 57.5 / 42.4 49.9 / 31.1 53.4 / 36.3 57.2 / 36.7 54.4 / 32.4 58.3 / 40.5
6384Ours XLM-R LARGE 6 384 107M 75.8 / 62.4 60.5 / 43.4 56.7 / 41.6 49.3 / 30.9 55.1 / 38.1 58.6 / 38.5 56.9 / 34.3 59.0 /41.3
MINILM XLM-R BASE 12 384 117M 79.4 / 66.5 66.1 / 47.5 61.2 / 46.5 54.9 / 34.9 58.5 / 41.3 63.1 / 42.1 59.0 / 33.8 63.2 / 44.7
12384Ours XLM-R BASE 12 384 117M 79.0 / 65.8 66.6 / 48.9 62.1 / 47.4 54.4 / 35.5 60.6 / 43.6 64.7 / 44.1 59.4 / 36.5 63.8 / 46.0
12384Ours XLM-R LARGE 12 384 117M 78.9 / 65.9 67.3 / 49.4 61.9 / 47.0 56.0 / 36.5 63.0 / 45.9 65.1 / 44.4 61.9 / 39.2 64.9 /46.9

Báº£ng 6: Káº¿t quáº£ tráº£ lá»i cÃ¢u há»i Ä‘a ngÃ´n ngá»¯ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘a ngá»¯ cá»§a chÃºng tÃ´i trÃªn MLQA. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘iá»ƒm F1 vÃ  EM (khá»›p chÃ­nh xÃ¡c) trÃªn má»—i trong 7 ngÃ´n ngá»¯ MLQA. #L vÃ  #H chá»‰ ra sá»‘ lÆ°á»£ng lá»›p vÃ  kÃ­ch thÆ°á»›c áº©n.
y chá»‰ ra káº¿t quáº£ tinh chá»‰nh cá»§a chÃºng tÃ´i cho XLM-R BASE.

MÃ´ hÃ¬nh SQuAD2 MNLI-m SST-2 Trung bÃ¬nh
Ours (Q-Q + K-K + V-V) 72.8 82.2 91.5 82.2
â€“ Q-Q Att-Rel 71.6 81.9 90.6 81.4
â€“ K-K Att-Rel 71.9 81.9 90.5 81.4
â€“ V-V Att-Rel 71.5 81.6 90.5 81.2
Q-K + V-V Att-Rels 72.4 82.2 91.0 81.9

Báº£ng 7: NghiÃªn cá»©u khá»­ bá» cá»§a cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ khÃ¡c nhau. ChÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ cá»§a mÃ´ hÃ¬nh há»c sinh 6384 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE. Sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ lÃ  12.

#Äáº§u Má»‘i quan há»‡ 6 12 24 48
mÃ´ hÃ¬nh 6384 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« RoBERTa BASE
MNLI-m 82.8 82.9 83.0 83.4
SQuAD 2.0 74.5 75.0 74.9 75.7
mÃ´ hÃ¬nh 6384 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE
MNLI-m 81.9 82.2 82.2 82.4
SQuAD 2.0 71.9 72.8 72.7 73.0

Báº£ng 8: Káº¿t quáº£ cá»§a mÃ´ hÃ¬nh 6384 sá»­ dá»¥ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ khÃ¡c nhau.

dá»«ng sá»›m. MÃ´ hÃ¬nh 6384 cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n mBERT (Devlin et al., 2018) vá»›i tÄƒng tá»‘c 5:3 láº§n. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÅ©ng hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n MINILM, Ä‘iá»u nÃ y cÃ ng xÃ¡c nháº­n tÃ­nh hiá»‡u quáº£ cá»§a chÆ°ng cáº¥t má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u. Chuyá»ƒn giao cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u cÃ³ thá»ƒ mang láº¡i kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n.

4.4 NghiÃªn cá»©u Khá»­ bá»
Hiá»‡u quáº£ cá»§a viá»‡c sá»­ dá»¥ng cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ khÃ¡c nhau
ChÃºng tÃ´i thá»±c hiá»‡n nghiÃªn cá»©u khá»­ bá» Ä‘á»ƒ phÃ¢n tÃ­ch Ä‘Ã³ng gÃ³p cá»§a cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ khÃ¡c nhau. Káº¿t quáº£ phÃ¡t triá»ƒn cá»§a ba nhiá»‡m vá»¥ Ä‘Æ°á»£c minh há»a trong Báº£ng 7. CÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Q-Q, K-K vÃ  V-V Ä‘Ã³ng gÃ³p tÃ­ch cá»±c vÃ o káº¿t quáº£ cuá»‘i cÃ¹ng. BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i cÅ©ng so sÃ¡nh Q-Q + K-K + V-V vá»›i Q-K + V-V cho ráº±ng truy váº¥n vÃ  khÃ³a Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n phÃ¢n phá»‘i tá»± chÃº Ã½ trong mÃ´-Ä‘un tá»± chÃº Ã½. Káº¿t quáº£ thÃ­ nghiá»‡m cho tháº¥y sá»­ dá»¥ng Q-Q + K-K + V-V Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n.

Hiá»‡u quáº£ cá»§a chÆ°ng cáº¥t cÃ¡c lá»›p giÃ¡o viÃªn khÃ¡c nhau HÃ¬nh 2 trÃ¬nh bÃ y káº¿t quáº£ cá»§a mÃ´ hÃ¬nh 6384 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« cÃ¡c lá»›p khÃ¡c nhau cá»§a BERT BASE, BERT LARGE vÃ  XLM-R LARGE. Äá»‘i vá»›i BERT BASE, sá»­ dá»¥ng lá»›p cuá»‘i Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n so vá»›i cÃ¡c lá»›p khÃ¡c. Äá»‘i vá»›i BERT LARGE vÃ  XLM-R LARGE, chÃºng tÃ´i tháº¥y ráº±ng sá»­ dá»¥ng má»™t trong cÃ¡c lá»›p giá»¯a trÃªn Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t. CÃ¹ng xu hÆ°á»›ng cÅ©ng Ä‘Æ°á»£c quan sÃ¡t cho BERT LARGE-WWM vÃ  RoBERTa LARGE.

Hiá»‡u quáº£ cá»§a sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ khÃ¡c nhau
Báº£ng 8 hiá»ƒn thá»‹ káº¿t quáº£ cá»§a mÃ´ hÃ¬nh 6384 Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT BASE vÃ  RoBERTa BASE sá»­ dá»¥ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ khÃ¡c nhau. Sá»­ dá»¥ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ lá»›n hÆ¡n Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n. Kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n

--- TRANG 8 ---
MÃ´ hÃ¬nh GiÃ¡o viÃªn #Param TÄƒng tá»‘c SQuAD2 MNLI-m/mm QNLI QQP RTE SST MRPC CoLA STS Trung bÃ¬nh
BERT BASE - 109M 1.0 76.8 84.6/83.4 90.5 71.2 66.4 93.5 88.9 52.1 85.8 79.3
MobileBERT IB-BERT LARGE 25M 1.8 80.2 84.3/83.4 91.6 70.5 70.4 92.6 88.8 51.1 84.8 79.8
12384Ours BERT LARGE-WWM 25M 2.7 80.7 85.9/84.6 91.9 71.4 71.9 93.3 89.2 44.9 85.5 79.9
+ More Att-Rels BERT LARGE-WWM 25M 2.7 80.9 85.8/84.8 92.3 71.6 72.0 93.6 89.7 46.6 86.0 80.3

Báº£ng 9: So sÃ¡nh giá»¯a MobileBERT vÃ  mÃ´ hÃ¬nh cÃ¹ng kÃ­ch thÆ°á»›c (12 lá»›p, 384 kÃ­ch thÆ°á»›c áº©n vÃ  128 kÃ­ch thÆ°á»›c nhÃºng) Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT LARGE (Whole Word Masking) trÃªn bá»™ kiá»ƒm tra GLUE vÃ  bá»™ phÃ¡t triá»ƒn SQuAD 2.0. Theo MobileBERT (Sun et al., 2019b), cÃ¡c káº¿t quáº£ Ä‘Æ°á»£c bÃ¡o cÃ¡o Ä‘Æ°á»£c tinh chá»‰nh trá»±c tiáº¿p trÃªn cÃ¡c nhiá»‡m vá»¥ háº¡ nguá»“n. ChÃºng tÃ´i tÃ­nh toÃ¡n tÄƒng tá»‘c cá»§a MobileBERT theo Ä‘á»™ trá»… Ä‘Æ°á»£c bÃ¡o cÃ¡o cá»§a há».

(a) BERT BASE lÃ m giÃ¡o viÃªn (b) BERT LARGE lÃ m giÃ¡o viÃªn (c) XLM-R LARGE lÃ m giÃ¡o viÃªn

HÃ¬nh 2: CÃ¡c mÃ´ hÃ¬nh 6384 Ä‘Æ°á»£c huáº¥n luyá»‡n sá»­ dá»¥ng cÃ¡c lá»›p BERT BASE (a), BERT LARGE (b) vÃ  XLM-R LARGE (c) khÃ¡c nhau.

MÃ´ hÃ¬nh GiÃ¡o viÃªn SQuAD2 MNLI-m SST-2
6384Ours BERT BASE 72.9 82.8 91.3
+ More Att-Rels BERT BASE 73.3 82.8 91.6
6384Ours BERT LARGE 74.3 83.0 91.1
+ More Att-Rels BERT LARGE 74.7 83.2 92.4
6384Ours RoBERTa LARGE 76.4 84.4 92.0
+ More Att-Rels RoBERTa LARGE 76.0 84.4 92.1
6768Ours BERT BASE 76.3 84.2 92.4
+ More Att-Rels BERT BASE 76.8 84.4 92.3
6768Ours BERT LARGE 77.7 85.0 92.5
+ More Att-Rels BERT LARGE 78.1 85.2 92.5
6768Ours RoBERTa LARGE 81.6 87.0 94.5
+ More Att-Rels RoBERTa LARGE 81.2 87.3 94.1

Báº£ng 10: Káº¿t quáº£ cá»§a viá»‡c giá»›i thiá»‡u thÃªm cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ (má»‘i quan há»‡ Q-K, K-Q, Q-V, V-Q, K-V vÃ  V-K).

cÃ³ thá»ƒ Ä‘Æ°á»£c náº¯m báº¯t báº±ng cÃ¡ch sá»­ dá»¥ng nhiá»u Ä‘áº§u má»‘i quan há»‡ hÆ¡n, Ä‘iá»u nÃ y giÃºp há»c sinh báº¯t chÆ°á»›c sÃ¢u sáº¯c mÃ´-Ä‘un tá»± chÃº Ã½ cá»§a giÃ¡o viÃªn. BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i tháº¥y ráº±ng sá»‘ lÆ°á»£ng Ä‘áº§u má»‘i quan há»‡ khÃ´ng cáº§n lÃ  bá»™i sá»‘ dÆ°Æ¡ng cá»§a cáº£ sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ há»c sinh vÃ  giÃ¡o viÃªn. Äáº§u má»‘i quan há»‡ cÃ³ thá»ƒ lÃ  má»™t pháº§n cá»§a má»™t Ä‘áº§u chÃº Ã½ Ä‘Æ¡n hoáº·c chá»©a cÃ¡c pháº§n tá»« nhiá»u Ä‘áº§u chÃº Ã½.

5 Tháº£o luáº­n

5.1 So sÃ¡nh vá»›i MobileBERT
MobileBERT (Sun et al., 2019b) nÃ©n má»™t mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t (trong kÃ­ch thÆ°á»›c BERT LARGE) vá»›i cÃ¡c mÃ´-Ä‘un nghá»‹ch Ä‘áº£o bottleneck thÃ nh má»™t há»c sinh 24 lá»›p sá»­ dá»¥ng cÃ¡c mÃ´-Ä‘un bottleneck. VÃ¬ má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  nÃ©n cÃ¡c mÃ´ hÃ¬nh lá»›n khÃ¡c nhau (vÃ­ dá»¥ BERT vÃ  RoBERTa) thÃ nh cÃ¡c mÃ´ hÃ¬nh nhá» sá»­ dá»¥ng kiáº¿n trÃºc Transformer tiÃªu chuáº©n, chÃºng tÃ´i lÆ°u Ã½ ráº±ng mÃ´ hÃ¬nh há»c sinh cá»§a chÃºng tÃ´i khÃ´ng thá»ƒ so sÃ¡nh trá»±c tiáº¿p vá»›i MobileBERT. ChÃºng tÃ´i cung cáº¥p káº¿t quáº£ cá»§a má»™t mÃ´ hÃ¬nh há»c sinh cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c tham sá»‘ Ä‘á»ƒ tham kháº£o. Má»™t mÃ´ hÃ¬nh kÃ­ch thÆ°á»›c lá»›n cÃ´ng khai (BERT LARGE-WWM) Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m giÃ¡o viÃªn, Ä‘áº¡t hiá»‡u suáº¥t tÆ°Æ¡ng tá»± nhÆ° giÃ¡o viÃªn cá»§a MobileBERT. ChÃºng tÃ´i chÆ°ng cáº¥t BERT LARGE-WWM thÃ nh má»™t mÃ´ hÃ¬nh há»c sinh (â‰ˆ 25M tham sá»‘) sá»­ dá»¥ng cÃ¹ng dá»¯ liá»‡u huáº¥n luyá»‡n (tá»©c lÃ  Wikipedia tiáº¿ng Anh vÃ  BookCorpus). Káº¿t quáº£ kiá»ƒm tra cá»§a GLUE vÃ  káº¿t quáº£ phÃ¡t triá»ƒn cá»§a SQuAD 2.0 Ä‘Æ°á»£c minh há»a trong Báº£ng 9. MÃ´ hÃ¬nh cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n MobileBERT trÃªn háº§u háº¿t cÃ¡c nhiá»‡m vá»¥ vá»›i tá»‘c Ä‘á»™ suy luáº­n nhanh hÆ¡n. HÆ¡n ná»¯a, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c giÃ¡o viÃªn khÃ¡c nhau vÃ  cÃ³ Ã­t háº¡n cháº¿ hÆ¡n nhiá»u Ä‘á»‘i vá»›i há»c sinh.

ChÃºng tÃ´i cÅ©ng quan sÃ¡t tháº¥y ráº±ng mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng tÆ°Æ¡ng Ä‘á»‘i kÃ©m hÆ¡n trÃªn CoLA so vá»›i MobileBERT. Nhiá»‡m vá»¥ cá»§a CoLA lÃ  Ä‘Ã¡nh giÃ¡ tÃ­nh cháº¥p nháº­n ngá»¯ phÃ¡p cá»§a má»™t cÃ¢u. NÃ³ Ä‘Ã²i há»i kiáº¿n thá»©c ngÃ´n ngá»¯ há»c tinh vi hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c há»c tá»« cÃ¡c má»¥c tiÃªu mÃ´ hÃ¬nh ngÃ´n ngá»¯. Tinh chá»‰nh mÃ´ hÃ¬nh sá»­ dá»¥ng má»¥c tiÃªu MLM nhÆ° trong MobileBERT mang láº¡i cáº£i thiá»‡n cho CoLA. Tuy nhiÃªn, cÃ¡c thÃ­ nghiá»‡m sÆ¡ bá»™ cá»§a chÃºng tÃ´i cho tháº¥y chiáº¿n lÆ°á»£c nÃ y sáº½ dáº«n Ä‘áº¿n sá»± sá»¥t giáº£m nháº¹ cho cÃ¡c nhiá»‡m vá»¥ GLUE khÃ¡c.

--- TRANG 9 ---
5.2 Káº¿t quáº£ cá»§a ThÃªm Má»‘i quan há»‡ Tá»± ChÃº Ã½
Trong Báº£ng 9 vÃ  10, chÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ cá»§a cÃ¡c há»c sinh Ä‘Æ°á»£c huáº¥n luyá»‡n sá»­ dá»¥ng thÃªm cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ (má»‘i quan há»‡ Q-K, K-Q, Q-V, V-Q, K-V vÃ  V-K). ChÃºng tÃ´i quan sÃ¡t tháº¥y cáº£i thiá»‡n trÃªn háº§u háº¿t cÃ¡c nhiá»‡m vá»¥, Ä‘áº·c biá»‡t cho cÃ¡c mÃ´ hÃ¬nh há»c sinh Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT. Kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi trong nhiá»u má»‘i quan há»‡ chÃº Ã½ hÆ¡n cáº£i thiá»‡n cÃ¡c há»c sinh cá»§a chÃºng tÃ´i. Tuy nhiÃªn, viá»‡c giá»›i thiá»‡u thÃªm cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ cÅ©ng mang láº¡i chi phÃ­ tÃ­nh toÃ¡n cao hÆ¡n. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  chi phÃ­ tÃ­nh toÃ¡n, chÃºng tÃ´i chá»n chuyá»ƒn giao cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Q-Q, K-K vÃ  V-V thay vÃ¬ táº¥t cáº£ cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ trong cÃ´ng trÃ¬nh nÃ y.

6 Káº¿t luáº­n
ChÃºng tÃ´i tá»•ng quÃ¡t hÃ³a chÆ°ng cáº¥t tá»± chÃº Ã½ sÃ¢u trong MINILMbáº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c má»‘i quan há»‡ tá»± chÃº Ã½ Ä‘a Ä‘áº§u Ä‘á»ƒ huáº¥n luyá»‡n há»c sinh. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i giá»›i thiá»‡u kiáº¿n thá»©c tá»± chÃº Ã½ tinh vi hÆ¡n vÃ  loáº¡i bá» háº¡n cháº¿ vá» sá»‘ lÆ°á»£ng Ä‘áº§u chÃº Ã½ cá»§a há»c sinh. HÆ¡n ná»¯a, chÃºng tÃ´i cho tháº¥y ráº±ng chuyá»ƒn giao kiáº¿n thá»©c tá»± chÃº Ã½ cá»§a má»™t lá»›p giá»¯a trÃªn Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n cho cÃ¡c giÃ¡o viÃªn kÃ­ch thÆ°á»›c lá»›n. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n ngá»¯ vÃ  Ä‘a ngá»¯ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c chÆ°ng cáº¥t tá»« BERT, RoBERTa vÃ  XLM-R cÃ³ Ä‘Æ°á»£c hiá»‡u suáº¥t cáº¡nh tranh vÃ  vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n Ä‘áº¡i nháº¥t. Äá»‘i vá»›i cÃ´ng trÃ¬nh tÆ°Æ¡ng lai, chÃºng tÃ´i Ä‘ang khÃ¡m phÃ¡ má»™t thuáº­t toÃ¡n lá»±a chá»n lá»›p tá»± Ä‘á»™ng. ChÃºng tÃ´i cÅ©ng muá»‘n Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cho cÃ¡c Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c lá»›n hÆ¡n.

TÃ i liá»‡u tham kháº£o
Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,
Xing Fan, and Edward Guo. 2019. Knowledge
distillation from internal representations. CoRR ,
abs/1910.03723.

Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan
Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-
feng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.
Unilmv2: Pseudo-masked language models for uni-
ï¬ed language model pre-training. arXiv preprint
arXiv:2002.12804 .

Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and
Danilo Giampiccolo. 2006. The second PASCAL
recognising textual entailment challenge. In Pro-
ceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment .

Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
ï¬fth pascal recognizing textual entailment challenge.
InIn Proc Text Analysis Conference (TAC'09 .

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
cross-lingual focused evaluation. arXiv preprint
arXiv:1708.00055 .

Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-
ling Mao, and Heyan Huang. 2019. Cross-lingual
natural language generation via pre-training. CoRR ,
abs/1909.10481.

Zewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-
sham Singhal, Wenhui Wang, Xia Song, Xian-
Ling Mao, Heyan Huang, and Ming Zhou. 2020.
Infoxlm: An information-theoretic framework for
cross-lingual language model pre-training. CoRR ,
abs/2007.07834.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. CoRR ,
abs/1911.02116.

Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad-
ina Williams, Samuel R Bowman, Holger Schwenk,
and Veselin Stoyanov. 2018. Xnli: Evaluating cross-
lingual sentence representations. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP) .

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Proceedings of the First Inter-
national Conference on Machine Learning Chal-
lenges: Evaluating Predictive Uncertainty Visual
Object Classiï¬cation, and Recognizing Textual En-
tailment , MLCW'05, pages 177â€“190, Berlin, Hei-
delberg. Springer-Verlag.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR , abs/1810.04805.

William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing (IWP2005) .

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019. Uniï¬ed language
model pre-training for natural language understand-
ing and generation. In 33rd Conference on Neural
Information Processing Systems (NeurIPS 2019) .

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing , pages 1â€“9, Prague. Association
for Computational Linguistics.

--- TRANG 10 ---
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
CoRR , abs/1503.02531.

Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao
Chen, and Qun Liu. 2020. Dynabert: Dynamic
BERT with adaptive width and depth. In Advances
in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual .

Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang,
Dongsheng Li, Nan Yang, and Ming Zhou. 2018.
Attention-guided answer distillation for machine
reading comprehension. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31 -
November 4, 2018 , pages 2077â€“2086.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2019. Tinybert: Distilling BERT for natural lan-
guage understanding. CoRR , abs/1909.10351.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,
Luke Zettlemoyer, and Omer Levy. 2019. Spanbert:
Improving pre-training by representing and predict-
ing spans. arXiv preprint arXiv:1907.10529 .

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations , San
Diego, CA.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. Advances in
Neural Information Processing Systems (NeurIPS) .

Hector Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Thirteenth International Conference on the Princi-
ples of Knowledge Representation and Reasoning .

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2019a. BART: Denoising sequence-to-sequence
pre-training for natural language generation, trans-
lation, and comprehension. arXiv preprint
arXiv:1910.13461 .

Patrick S. H. Lewis, Barlas Oguz, Ruty Rinott, Sebas-
tian Riedel, and Holger Schwenk. 2019b. MLQA:
evaluating cross-lingual extractive question answer-
ing. CoRR , abs/1910.07475.

Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng
Xu, Min Yang, and Yaohong Jin. 2020. BERT-EMD:
many-to-many layer mapping for BERT compres-
sion with earth mover's distance. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 3009â€“3018. Associ-
ation for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692 .

Subhabrata Mukherjee and Ahmed Hassan Awadallah.
2020. Xtremedistil: Multi-stage distillation for mas-
sive multilingual models. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2020, Online, July 5-10,
2020 , pages 2221â€“2234. Association for Computa-
tional Linguistics.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a uniï¬ed text-to-text trans-
former. arXiv preprint arXiv:1910.10683 .

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don't know: Unanswerable ques-
tions for SQuAD. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 2: Short Papers , pages 784â€“
789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383â€“2392, Austin,
Texas. Association for Computational Linguistics.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2015. Fitnets: Hints for thin deep nets. In
3rd International Conference on Learning Represen-
tations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings .

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and
Preslav Nakov. 2020. Poor man's BERT:
smaller and faster transformer models. CoRR ,
abs/2004.03844.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR ,
abs/1910.01108.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing ,
pages 1631â€“1642.

--- TRANG 11 ---
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence
pre-training for language generation. arXiv preprint
arXiv:1905.02450 .

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019a.
Patient knowledge distillation for BERT model com-
pression. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Nat-
ural Language Processing, EMNLP-IJCNLP 2019,
Hong Kong, China, November 3-7, 2019 , pages
4322â€“4331.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2019b. Mobile-
bert: Task-agnostic compression of bert by progres-
sive knowledge transfer.

Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-
vazhagan, Xin Li, and Amelia Archer. 2019. Small
and practical BERT models for sequence labeling.
InProceedings of the 2019 Conference on Empiri-
cal Methods in Natural Language Processing and
the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , pages 3630â€“
3634.

Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019. Well-read students learn better:
The impact of student initialization on knowledge
distillation. CoRR , abs/1908.08962.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30 , pages 5998â€“6008. Curran Asso-
ciates, Inc.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Inter-
national Conference on Learning Representations .

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In Advances in Neural
Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .

Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018. Neural network acceptability judgments.
arXiv preprint arXiv:1805.12471 .

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume

Corpus #Train #Dev #Test Metrics
Nhiá»‡m vá»¥ CÃ¢u ÄÆ¡n
CoLA 8.5k 1k 1k Matthews Corr
SST-2 67k 872 1.8k Accuracy
Nhiá»‡m vá»¥ TÆ°Æ¡ng tá»± vÃ  Diá»…n giáº£i
QQP 364k 40k 391k Accuracy/F1
MRPC 3.7k 408 1.7k Accuracy/F1
STS-B 7k 1.5k 1.4k Pearson/Spearman Corr
Nhiá»‡m vá»¥ Suy luáº­n
MNLI 393k 20k 20k Accuracy
RTE 2.5k 276 3k Accuracy
QNLI 105k 5.5k 5.5k Accuracy
WNLI 634 71 146 Accuracy

Báº£ng 11: TÃ³m táº¯t bá»™ Ä‘Ã¡nh giÃ¡ GLUE.

#Train #Dev #Test Metrics
130,319 11,873 8,862 Exact Match/F1

Báº£ng 12: Thá»‘ng kÃª bá»™ dá»¯ liá»‡u vÃ  thÆ°á»›c Ä‘o cá»§a SQuAD 2.0.

1 (Long Papers) , pages 1112â€“1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,
and Ming Zhou. 2020. Bert-of-theseus: Compress-
ing BERT by progressive module replacing. CoRR ,
abs/2002.02925.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.
XLNet: Generalized autoregressive pretraining for
language understanding. In 33rd Conference on
Neural Information Processing Systems (NeurIPS
2019) .

Sergey Zagoruyko and Nikos Komodakis. 2017. Pay-
ing more attention to attention: Improving the per-
formance of convolutional neural networks via at-
tention transfer. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings .

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE inter-
national conference on computer vision , pages 19â€“
27.

A Bá»™ Ä‘Ã¡nh giÃ¡ GLUE
TÃ³m táº¯t cÃ¡c bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng cho bá»™ Ä‘Ã¡nh giÃ¡ Hiá»ƒu NgÃ´n ngá»¯ Chung (GLUE)4(Wang et al., 2019) Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 11.

4https://gluebenchmark.com/

--- TRANG 12 ---
B SQuAD 2.0
ChÃºng tÃ´i trÃ¬nh bÃ y thá»‘ng kÃª bá»™ dá»¯ liá»‡u vÃ  thÆ°á»›c Ä‘o cá»§a SQuAD 2.05(Rajpurkar et al., 2018) trong Báº£ng 12.

C SiÃªu tham sá»‘ cho Tinh chá»‰nh
Tráº£ lá»i CÃ¢u há»i TrÃ­ch xuáº¥t Äá»‘i vá»›i SQuAD 2.0, Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a lÃ  384. KÃ­ch thÆ°á»›c lÃ´ Ä‘Æ°á»£c Ä‘áº·t lÃ  32. ChÃºng tÃ´i chá»n tá»‘c Ä‘á»™ há»c tá»« {3e-5, 6e-5, 8e-5, 9e-5} vÃ  tinh chá»‰nh mÃ´ hÃ¬nh trong 3 epoch. Tá»· lá»‡ khá»Ÿi Ä‘á»™ng vÃ  suy giáº£m trá»ng sá»‘ lÃ  0.1 vÃ  0.01.

GLUE Äá»™ dÃ i chuá»—i tá»‘i Ä‘a lÃ  128 cho bá»™ Ä‘Ã¡nh giÃ¡ GLUE. ChÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c lÃ´ lÃ  32, chá»n tá»‘c Ä‘á»™ há»c tá»« {1e-5, 1.5e-5, 2e-5, 3e-5, 5e-5} vÃ  epoch tá»« {3,5,10} cho cÃ¡c mÃ´ hÃ¬nh há»c sinh khÃ¡c nhau. ChÃºng tÃ´i tinh chá»‰nh nhiá»‡m vá»¥ CoLA vá»›i cÃ¡c bÆ°á»›c huáº¥n luyá»‡n dÃ i hÆ¡n (25 epoch). Tá»· lá»‡ khá»Ÿi Ä‘á»™ng vÃ  suy giáº£m trá»ng sá»‘ lÃ  0.1 vÃ  0.01.

Suy luáº­n NgÃ´n ngá»¯ Tá»± nhiÃªn Äa ngÃ´n ngá»¯ (XNLI) Äá»™ dÃ i chuá»—i tá»‘i Ä‘a lÃ  256 cho XNLI. ChÃºng tÃ´i tinh chá»‰nh 10 epoch sá»­ dá»¥ng 64 lÃ m kÃ­ch thÆ°á»›c lÃ´. Tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c chá»n tá»« {3e-5, 4e-5, 5e-5, 6e-5}.

Tráº£ lá»i CÃ¢u há»i Äa ngÃ´n ngá»¯ Äá»‘i vá»›i MLQA, Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a lÃ  512. ChÃºng tÃ´i tinh chá»‰nh 4 epoch sá»­ dá»¥ng 32 lÃ m kÃ­ch thÆ°á»›c lÃ´. Tá»‘c Ä‘á»™ há»c Ä‘Æ°á»£c chá»n tá»« {3e-5, 4e-5, 5e-5, 6e-5}.

5http://stanford-qa.com
