# 2407.12866.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2407.12866.pdf
# Kích thước tệp: 3070836 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2407.12866v1  [cs.CL]  13 Jul 2024Vượt xa KV Caching: Shared Attention cho LLMs Hiệu quả
Liao Bingli1, Danilo Vasconcellos Vargas1
1Kyushu University, Fukuoka, Japan
{liao.bingli.734@s, vargas@inf }.kyushu-u.ac.jp
Tóm tắt
Hiệu quả của các mô hình ngôn ngữ lớn (LLMs) vẫn là một thách thức quan trọng, đặc biệt trong các bối cảnh mà tài nguyên tính toán bị hạn chế. Các cơ chế attention truyền thống trong những mô hình này, mặc dù mạnh mẽ, đòi hỏi tài nguyên tính toán và bộ nhớ đáng kể do tính cần thiết của việc tính toán lại và lưu trữ trọng số attention qua các lớp khác nhau. Bài báo này giới thiệu một cơ chế Shared Attention (SA) mới, được thiết kế để nâng cao hiệu quả của LLMs bằng cách chia sẻ trực tiếp các trọng số attention đã tính toán qua nhiều lớp. Khác với các phương pháp trước đây tập trung vào việc chia sẻ bộ đệm Key-Value (KV) trung gian, cách tiếp cận của chúng tôi sử dụng các xu hướng đẳng hướng của phân bố attention quan sát được trong các LLMs tiên tiến sau tiền huấn luyện để giảm cả các phép tính toán và kích thước của bộ đệm KV cần thiết trong quá trình suy luận. Chúng tôi chứng minh thực nghiệm rằng việc triển khai SA qua các LLMs khác nhau dẫn đến tổn thất độ chính xác tối thiểu trên các benchmark tiêu chuẩn. Các phát hiện của chúng tôi cho thấy rằng SA không chỉ bảo tồn tài nguyên tính toán mà còn duy trì hiệu suất mô hình mạnh mẽ, từ đó tạo điều kiện cho việc triển khai các LLMs hiệu quả hơn trong các môi trường hạn chế tài nguyên.
Code: https://github.com/metacarbon/shareAtt
Giới thiệu
Sự tăng trưởng nhanh chóng của các mô hình ngôn ngữ lớn (LLM) đã mang đến những thách thức đáng kể về hiệu quả tính toán và bộ nhớ trong quá trình suy luận. Các cách tiếp cận truyền thống, như Multi-Query Attention (MQA) (Shazeer 2019) và Grouped-Query Attention (GQA) (Ainslie et al. 2023), đã có những bước tiến trong việc giảm kích thước bộ đệm key-value (KV) bằng cách chia sẻ keys và values qua nhiều heads trong một lớp. Gần đây hơn, Cross-Layer Attention (CLA) đã mở rộng khái niệm này bằng cách chia sẻ keys và values qua các lớp liền kề, tiếp tục giảm yêu cầu bộ nhớ mà không ảnh hưởng đáng kể đến hiệu suất mô hình (Brandon et al. 2024). Mặc dù có những tiến bộ này, nhu cầu về các phương pháp hiệu quả hơn tiếp tục tăng, đặc biệt khi các mô hình mở rộng quy mô và được triển khai trong các môi trường hạn chế tài nguyên.

Trong bài báo này, chúng tôi giới thiệu một phương pháp mới được gọi là Shared Attention (SA), giảm đáng kể yêu cầu bộ đệm KV và tải tính toán trong quá trình suy luận cho LLMs. Khác với các phương pháp trước đây tập trung vào việc chia sẻ bộ đệm KV trong cùng một lớp hoặc giữa các lớp liền kề, cách tiếp cận của chúng tôi được truyền cảm hứng bởi sự tương tự vốn có của phân bố trọng số attention qua các lớp, và việc chia sẻ những trọng số này trực tiếp có thể tiếp tục giảm nhu cầu tính toán key và value lặp lại. Cách tiếp cận sáng tạo này không chỉ giảm kích thước bộ đệm KV mà còn bỏ qua nhu cầu cho phép toán softmax tốn kém về tính toán, dẫn đến một quá trình suy luận hiệu quả hơn.

Các đóng góp chính của công việc chúng tôi được tóm tắt như sau:
1. Chúng tôi đề xuất một cơ chế Shared Attention mới giảm chi phí tính toán và bộ nhớ bằng cách chia sẻ trực tiếp các trọng số attention đã tính toán trước qua nhiều lớp trong LLMs.
2. Chúng tôi xác thực thực nghiệm hiệu quả của Shared Attention bằng cách triển khai nó qua các benchmark khác nhau và chứng minh rằng nó đạt được độ chính xác tương đương.
3. Phân tích của chúng tôi về đẳng hướng attention qua các LLMs đã được tiền huấn luyện cung cấp những hiểu biết về cách các cơ chế attention ổn định và trở nên đồng đều hơn qua các lớp khi quá trình huấn luyện tiến triển. Sự hiểu biết này thông tin cho các phạm vi lớp tối ưu để áp dụng Shared Attention.

Shared Attention
Trong phần này chúng tôi chứng minh động lực, phương pháp Shared Attention (SA), và so sánh với các cơ chế chia sẻ KV hiện có.

Động lực
Cơ chế self-attention trong các mô hình transformer thường được định nghĩa là softmax (QKT/√d)V, trong đó Q, K, và V đại diện cho các ma trận query, key, và value tương ứng, và d là chiều của các vector key. Công thức này đòi hỏi việc tính toán lại trọng số attention ở mỗi lớp, một nhiệm vụ tốn kém về tính toán, đặc biệt khi mô hình được triển khai trong chế độ suy luận. Để giảm thiểu điều này, khái niệm KV-cache được sử dụng, giảm nhu cầu tính toán lại các ma trận K và V cho các token đã gặp trước đó.

Trong khi các phương pháp trước đây đã tập trung vào việc chia sẻ bộ đệm KV ở các cấp độ khác nhau để giảm thiểu chi phí bộ nhớ, chúng chủ yếu hoạt động dưới giả định rằng trọng số attention khác biệt đáng kể qua các lớp, do đó cần thiết các tính toán riêng lẻ để nắm bắt các phụ thuộc ngữ cảnh đa dạng một cách hiệu quả. Giả định này khơi gợi một câu hỏi quan trọng: Liệu các trọng số attention thực sự khác biệt rõ ràng qua các lớp, hay sự biến đổi này đủ tối thiểu để cho phép một cách tiếp cận thống nhất qua nhiều lớp?

--- TRANG 2 ---
Hình 1: Minh họa các thuật toán chia sẻ khác nhau. Các phương pháp MQA và GQA chia sẻ bộ đệm Key và Value với Query trong cùng một lớp để giảm sử dụng bộ nhớ. Phương pháp CLA mở rộng điều này bằng cách chia sẻ bộ đệm Key và Value qua các lớp khác nhau. Phương pháp của chúng tôi, Shared Attention, tiến bộ hơn khái niệm này bằng cách chia sẻ trọng số attention qua nhiều lớp.

Để khám phá điều này, chúng tôi đã tiến hành một phân tích thực nghiệm về phân bố trọng số attention qua các lớp khác nhau của mô hình. Dựa trên mô hình Llama2-7B-chat, chúng tôi xử lý bộ dữ liệu Massive Multitask Language Understanding (MMLU) (Hendrycks et al. 2020) để trích xuất các ma trận attention, softmax (QKT/√d), cho mỗi lớp. Do sự biến đổi trong độ dài chuỗi, chúng tôi chuẩn hóa những ma trận này về kích thước đồng nhất bằng cách áp dụng zero-padding để căn chỉnh chúng với hình dạng nhất quán là maxlen × maxlen.

Phân tích của chúng tôi sử dụng metric cosine similarity để so sánh các ma trận attention của tất cả các lớp, tiết lộ một mức độ tương tự cao đáng chú ý qua hầu hết các lớp, đặc biệt từ chỉ số 3 đến 30. Ngược lại, các lớp ban đầu (0 và 1) và lớp đầu ra cuối cùng (31) thể hiện điểm tương tự thấp hơn đáng kể so với các lớp giữa. Quan sát này trực quan vì các lớp đầu gần với embeddings token đầu vào, đòi hỏi điều chỉnh thường xuyên phân bố attention của chúng để trừu tượng hóa chính xác ý nghĩa semantic từ các đầu vào đa dạng. Tương tự, vai trò độc đáo của lớp cuối trong việc dự đoán token tiếp theo biện minh cho mẫu attention riêng biệt của nó.

Được truyền cảm hứng bởi những phát hiện này, chúng tôi giả thuyết rằng sự tương tự cao trong trọng số attention qua phần lớn các lớp có thể cho phép một biểu diễn chung của những trọng số này, do đó loại bỏ nhu cầu tính toán softmax riêng biệt trong mỗi lớp và giảm kích thước key cache. Một chiến lược như vậy không chỉ có thể hợp lý hóa quá trình suy luận mà còn nâng cao hiệu quả tính toán một cách đáng kể.

Dựa trên tính đồng nhất quan sát được trong trọng số attention, chúng tôi đề xuất một thuật toán mới như được hiển thị trong Thuật toán 1, Shared Attention, sử dụng một ma trận attention chung duy nhất qua nhiều lớp. Cách tiếp cận này về cơ bản tái định nghĩa mô hình hoạt động bằng cách duy trì một cơ chế attention nhất quán qua các lớp ngữ cảnh khác nhau, từ đó giảm sự dư thừa và nâng cao tốc độ suy luận.

So sánh với Các Cách tiếp cận Hiện có
Cơ chế self-attention gốc trong Transformers, được đặc trưng bởi mô hình Multi-Head Attention (MHA), đòi hỏi việc lưu trữ đệm các keys (K) và values (V) trong mỗi head và lớp để tăng tốc suy luận (Vaswani et al. 2017). Yêu cầu này về mặt lịch sử đã áp đặt một chi phí bộ nhớ đáng kể, thúc đẩy một loạt các đổi mới nhằm giảm gánh nặng này.

Trong số này, Multi-Query Attention (MQA) và đối tác tổng quát hơn của nó, Grouped-Query Attention (GQA), hợp nhất bộ đệm KV bằng cách cho phép nhiều query heads trong cùng một lớp chia sẻ một tập K và V matrices duy nhất. Cách tiếp cận này hiệu quả giảm số lượng cặp key và value độc đáo phải được lưu trữ và truy xuất trong quá trình tính toán. Tiếp theo, Cross-Layer Attention (CLA) mở rộng khái niệm này bằng cách tạo điều kiện cho việc chia sẻ K và V matrices qua các lớp khác nhau, từ đó cung cấp thêm các giảm thiểu trong dấu chân bộ nhớ cần thiết cho lưu trữ KV.

Tuy nhiên, phương pháp của chúng tôi giới thiệu một mô hình về cơ bản khác biệt trong việc giải quyết các thách thức của self-attention.

--- TRANG 3 ---
Hình 2: Tương tự theo lớp của trọng số attention qua các LLMs khác nhau. Trục x và trục y đại diện cho các chỉ số lớp, trong khi trục z mô tả các giá trị cosine similarity. Các mẫu tương tự riêng biệt cho thấy vai trò chức năng cụ thể mà mỗi nhóm lớp đóng trong tổng thể kiến trúc.

Thuật toán 1: Thuật toán Shared Attention
Đầu vào: Tập các lớp L, tokens đầu vào X
Tham số: Khoảng attention S (ví dụ, lớp 23 đến 30)
Đầu ra: Trọng số attention được cập nhật qua các lớp được chỉ định

1: Khởi tạo trọng số attention A ← ∅
2: foreach lớp l ∈ S do
3:    if lớp đầu tiên trong S then
4:        Tính toán trọng số attention ban đầu Al ← softmax(QlKlT/√dk)
5:        Đặt A ← Al
6:    else
7:        Chia sẻ trọng số attention Al ← A
8:    end if
9:    Áp dụng shared attention để tính toán đầu ra Ol ← Al · Vl
10: end for
11: Điều chỉnh đầu vào của các lớp tiếp theo bằng đầu ra từ S
12: Tiếp tục xử lý các lớp còn lại với attention tiêu chuẩn
13: return Đầu ra cuối cùng sau khi xử lý tất cả các lớp

Trong khi các phương pháp trước đây đã tập trung vào việc giảm sự dư thừa trong việc lưu trữ K và V matrices, cách tiếp cận của chúng tôi tập trung vào tối ưu hóa việc tính toán trọng số attention. Trong thực tế tiêu chuẩn, các keys được lưu đệm (K) chủ yếu được sử dụng để tính toán trọng số attention kết hợp với các queries (Q). Thay vì gián tiếp tạo điều kiện cho tương tác này thông qua các KV matrices chung, phương pháp của chúng tôi đề xuất việc chia sẻ trực tiếp các trọng số attention kết quả—cụ thể, các điểm số đã được chuẩn hóa softmax.

Điều này không chỉ giảm thiểu yêu cầu bộ nhớ bằng cách loại bỏ nhu cầu lưu trữ các tập keys riêng biệt cho mỗi lớp mà còn giảm đáng kể độ phức tạp tính toán. Bằng cách chia sẻ các kết quả softmax đã tính toán trước qua các lớp, cách tiếp cận của chúng tôi bỏ qua việc tính toán lặp lại softmax, thường là một trong những phép toán tốn kém về tính toán nhất trong cơ chế attention. Lợi ích hiệu quả này được phản ánh trong việc giảm đáng kể số lượng phép toán dấu phẩy động (FLOPs) cần thiết trong quá trình suy luận mô hình, nâng cao cả tốc độ và khả năng mở rộng của việc triển khai Transformer.

Khác với các phương pháp truyền thống tối ưu hóa sử dụng bộ nhớ bằng cách chia sẻ keys và values vật lý qua các lớp hoặc heads, mô hình Shared Attention của chúng tôi đổi mới trên chính quá trình tính toán, khai thác các mẫu nhất quán trong trọng số attention để hợp lý hóa các hoạt động qua nhiều lớp của kiến trúc Transformer.

Phân bố Attention Đẳng hướng
Trong một phân tích mở rộng về trọng số attention cụ thể theo lớp qua một phổ LLMs, chúng tôi khám phá động lực attention trong các mô hình như Llama2-7B-chat, Llama3-8B-instruct, Llama3-70B-instruct, Baichuan2-7B-chat, Qwen2-7B-instruct, và Qwen2-72B-instruct (Touvron et al. 2023; Yang et al. 2023; Bai et al. 2023). Những mô hình này được đánh giá sử dụng MMLU.

Các điều tra của chúng tôi tiết lộ một mẫu tự tổ chức trong trọng số attention qua những mô hình đa dạng này. Như được mô tả trong Hình 2, tồn tại một mẫu tương tự toàn cục nhất quán trong trọng số attention của các lớp qua tất cả các mô hình được thử nghiệm. Mẫu này gợi ý một đặc tính cấu trúc vốn có trong cách LLMs xử lý thông tin, có thể được phân đoạn rộng rãi thành bốn nhóm riêng biệt:

• Nhóm 1: Bao gồm các lớp ban đầu (chỉ số 0 và 1), nhóm này nằm gần nhất với các tokens đầu vào và chủ yếu tập trung vào việc trừu tượng hóa thông tin semantic cấp độ token. Những lớp này thể hiện các mẫu attention phụ thuộc vào dữ liệu quan trọng cho việc xử lý semantic ban đầu của các đầu vào.

• Nhóm 2: Nhóm này bao gồm các lớp ngay sau nhóm đầu tiên và mở rộng đến chỉ số lớp 5. Các lớp trong phân đoạn này thể hiện tương tự nội bộ cao trong trọng số attention nhưng khác biệt rõ ràng so với những lớp trong các nhóm khác. Những lớp này có thể phục vụ như các vùng chuyển tiếp nơi các đặc trưng semantic trung gian được tinh chỉnh.

--- TRANG 4 ---
• Nhóm 3: Bao gồm các lớp sau Nhóm 2 và mở rộng đến lớp áp chót, đây là nhóm lớn nhất cả về số lượng lớp và vai trò của chúng trong kiến trúc. Các lớp trong nhóm này hiển thị một mức độ tương tự cao, gợi ý một đẳng hướng trong cơ chế attention nơi các đặc trưng được tinh chỉnh được sử dụng nhất quán để thông báo cho sự hiểu biết ngữ cảnh sâu hơn của mô hình.

• Nhóm 4: Nhóm cuối cùng, chỉ bao gồm lớp đầu ra, xử lý thông tin ngữ cảnh được tổng hợp một cách riêng biệt để tạo ra đầu ra. Trọng số attention của lớp này phân kỳ khỏi những gì quan sát được trong các lớp khác, nhấn mạnh vai trò chuyên biệt của nó trong quá trình ra quyết định cuối cùng.

Các mẫu trọng số attention riêng biệt được xác định qua những nhóm này củng cố khái niệm chuyên môn hóa chức năng trong LLMs. Sự phân đoạn này không chỉ làm nổi bật các vai trò đa dạng của các lớp khác nhau trong việc xử lý đầu vào mà còn hỗ trợ tiềm năng tối ưu hóa các chiến lược tính toán, như phương pháp Shared Attention được đề xuất của chúng tôi, bằng cách thao tác những mẫu vốn có này để giảm sự dư thừa tính toán.

Động lực Trong Quá trình Tiền huấn luyện
Để làm sáng tỏ sự hình thành và tiến hóa của các mẫu trọng số attention trong giai đoạn tiền huấn luyện của LLMs, chúng tôi đã sử dụng các checkpoint trung gian của mô hình Baichuan 7B, được cung cấp bởi các nhà phát triển mô hình. Những checkpoint này, trải dài từ 0.2T đến 2.6T tokens được xử lý, cung cấp một góc nhìn độc đáo để quan sát những thay đổi động trong cơ chế attention khi mô hình tiếp xúc với một khối lượng dữ liệu tăng dần.

Chúng tôi áp dụng một metric nhất quán để đo lường sự tương tự của trọng số attention qua các lớp tại mỗi checkpoint tiền huấn luyện. Ngoài ra, mô hình chat cuối cùng, được tinh chỉnh để phù hợp với các phản hồi tham chiếu của con người, được bao gồm để đánh giá sự tiến hóa so với kết quả ứng dụng thực tế. Động lực của những trọng số attention này được visualized trong Hình 3, minh họa sự phân biệt và ổn định tiến bộ của các mẫu attention qua các lớp của mô hình.

Như quan sát được trong giai đoạn tiền huấn luyện đầu tại 0.2T tokens, Nhóm 1 và 2 xuất hiện được hợp nhất, cho thấy một chiến lược xử lý ít phân biệt hơn qua những lớp ban đầu này. Sự kết hợp này gợi ý rằng sớm trong quá trình huấn luyện, mô hình không phân biệt rõ ràng việc xử lý semantic cấp độ token khỏi việc tinh chỉnh semantic trung gian. Tuy nhiên, khi mô hình tiến triển đến 1.0T tokens, một sự phân chia rõ ràng xuất hiện giữa Nhóm 1 và 2. Sự tách biệt này phù hợp với việc mô hình bắt đầu hình thành các chiến lược chuyên biệt và hiệu quả hơn để xử lý các loại thông tin khác nhau qua kiến trúc của nó.

Sự tương tự trong Nhóm 3, bao gồm phần lớn các lớp của mô hình, hiển thị một cải thiện rõ ràng từ điểm tương tự 0.8 lên 0.9. Sự gia tăng này cho thấy cơ chế attention của mô hình ổn định và trở nên nhất quán hơn trong cách tiếp cận xử lý phần lớn thông tin ngữ cảnh.

Những tiến bộ huấn luyện quan sát được qua các checkpoint tiền huấn luyện không chỉ chứng minh những thay đổi đáng kể trong cấu trúc nội bộ của cơ chế attention của mô hình mà còn tương quan tích cực với cải thiện hiệu suất trên nhiều benchmark. Điều này bao gồm kết quả trên MMLU, CMMLU (Li et al. 2023), và các bài kiểm tra độ chính xác 5-shot C-Eval (Huang et al. 2024), đã được báo cáo cải thiện từ độ chính xác cơ sở 0.25 lên 0.50 (Yang et al. 2023). Sự nâng cao đáng chú ý này nhấn mạnh mối liên kết nội tại giữa việc tinh chỉnh cơ chế attention trong LLMs và khả năng tăng cường của chúng trong các nhiệm vụ hiểu biết ngôn ngữ tự nhiên.

Hơn nữa, việc kiểm tra sâu hơn sự phát triển của mô hình, như quan sát được trong tài liệu bổ sung, tiết lộ rằng sự tương tự trong Nhóm 3—bao gồm các lớp xử lý ngữ cảnh cốt lõi của mô hình—tiếp tục tăng cường sau giai đoạn alignment. Quan sát này gợi ý rằng quá trình alignment, thường nhằm tinh chỉnh mô hình để phản ánh gần hơn sự hiểu biết và tạo phản hồi giống con người, cũng đóng góp vào sự ổn định của cơ chế attention của mô hình.

Thí nghiệm và Thảo luận
Để xác thực hiệu quả của phương pháp Shared Attention (SA) được đề xuất của chúng tôi, chúng tôi đã tiến hành loạt thí nghiệm. Những thí nghiệm này được thiết kế để kiểm tra tính mạnh mẽ của SA dưới các cấu hình khác nhau và để đánh giá hiệu suất của nó trên các benchmark được công nhận rộng rãi.

Ban đầu, chúng tôi áp dụng cơ chế SA trực tiếp vào các LLMs tiên tiến mà không có bất kỳ huấn luyện trước nào để đánh giá tác động của nó lên các mô hình đã được tiền huấn luyện. Thí nghiệm này nhằm hiểu những tác động ngay lập tức của SA khi được tích hợp vào các kiến trúc mô hình hiện có. Chúng tôi đánh giá hiệu suất của những mô hình này trên các benchmark LLM tiêu chuẩn, bao gồm GLUE (General), GSM8k (Arithmetic), HellaSwag (Reasoning), và MMLU (Knowledge) (Wang et al. 2018; Cobbe et al. 2021; Zellers et al. 2019). Như dự đoán, việc áp dụng trực tiếp SA dẫn đến tổn thất độ chính xác trên một số benchmark. Kết quả này phù hợp với kỳ vọng của chúng tôi do thiếu huấn luyện lại để thích nghi đầy đủ các mô hình với các sắc thái của cơ chế Shared Attention. Do ràng buộc tính toán, việc tiền huấn luyện một LLM từ đầu tích hợp SA là không thực tế cho nhóm của chúng tôi.

Để tiếp tục thăm dò khả năng của SA dưới một chế độ huấn luyện, chúng tôi tinh chỉnh các LLMs cơ sở được trang bị Shared Attention trên bộ dữ liệu Instruct có sẵn công khai (Taori et al. 2023). Sau tinh chỉnh, những mô hình này được kiểm tra trên cùng các benchmark để phát hiện bất kỳ thay đổi hiệu suất nào. Cách tiếp cận này cho phép chúng tôi đo lường khả năng thích nghi của SA khi các mô hình được huấn luyện để phù hợp với động lực của nó.

Những thí nghiệm này cùng nhau chứng minh tiềm năng của Shared Attention để sửa đổi cơ chế attention truyền thống trong LLMs, hiển thị một hướng đi hứa hẹn để giảm yêu cầu tính toán trong khi duy trì, và trong một số trường hợp nâng cao, hiệu suất mô hình. Các kết quả chi tiết và thảo luận sâu hơn về mỗi benchmark và bộ dữ liệu được cung cấp trong các phần tiếp theo.

--- TRANG 5 ---
Hình 3: Tiến hóa của sự tương tự trọng số attention lớp xuyên suốt giai đoạn tiền huấn luyện của mô hình Baichuan2 7B, khi nó xử lý tokens được huấn luyện từ 220 tỷ đến 2.6 nghìn tỷ. Độ dốc màu trong visualization đại diện cho cosine similarity, hiệu quả minh họa quá trình chuyển đổi trong các mẫu attention từ giai đoạn ban đầu đến giai đoạn tiên tiến của tiền huấn luyện.

Thiết lập Thí nghiệm
Cho các thí nghiệm tinh chỉnh, chúng tôi sử dụng các mô hình cơ sở Llama2-7B và Llama3-8B. Những thí nghiệm này được tiến hành trên một cấu hình phần cứng mạnh mẽ bao gồm hai GPU NVIDIA A100 80GB. Tối ưu hóa các mô hình được thực hiện sử dụng optimizer AdamW, với tốc độ học ban đầu được đặt ở 2×10^-5. Chúng tôi sử dụng kiểu dữ liệu bf16 cho các tham số mô hình, điều này nâng cao phạm vi số và độ ổn định trong quá trình lan truyền ngược, quan trọng để duy trì độ chính xác trong huấn luyện mô hình lớn.

Mỗi GPU xử lý kích thước micro-batch là 16, tận dụng các kỹ thuật tích lũy gradient để quản lý hiệu quả tải tính toán. Ngoài ra, chúng tôi sử dụng DeepSpeed Zero Stage 3 để tối ưu hóa việc phân phối các tham số mô hình và optimizer và nâng cao quản lý bộ nhớ qua các GPU, đảm bảo sử dụng hiệu quả các tài nguyên có sẵn. Quá trình tinh chỉnh kéo dài hai epochs và sử dụng định dạng instruction Alpaca tiêu chuẩn, được thiết kế để cải thiện tính phản hồi và độ chính xác của các mô hình trong việc xử lý các nhiệm vụ dựa trên instruction.

Áp dụng Trực tiếp Shared Attention
Việc áp dụng SA được kiểm tra qua các phân đoạn lớp riêng biệt trong các mô hình Llama2-7B và Llama3-8B, mỗi mô hình bao gồm 32 lớp tổng cộng. Để đánh giá tính mạnh mẽ và khả năng thích nghi của SA như được hiển thị trong Hình 4, nó được triển khai trong các phân đoạn lớp khác nhau, từ các khoảng hẹp hơn như bốn lớp (ví dụ, SA:15∼18) đến các khoảng rộng hơn như tám lớp (ví dụ, SA:23∼30).

Các đánh giá sơ bộ của SA trong các lớp đầu của Llama2-7B (ví dụ, lớp 3 đến 6) dẫn đến bùng nổ perplexity, cho thấy những gián đoạn đáng kể trong khả năng dự đoán chính xác các tokens tiếp theo của mô hình. Hiện tượng này nhấn mạnh vai trò quan trọng mà sự biến thiên điểm attention đóng trong các giai đoạn đầu xử lý của mô hình, điều quan trọng cho việc thiết lập ngữ cảnh ban đầu và trích xuất đặc trưng. Để đánh giá định lượng tác động của biến thiên attention xuyên suốt mô hình, chúng tôi đã tiến hành một phân tích biến thiên chi tiết. Chúng tôi áp dụng cùng phương pháp tính toán được sử dụng để thu được điểm attention trung bình để tính toán biến thiên của trọng số attention trong Llama2-7B và Llama3-8B trong khi xử lý bộ dữ liệu MMLU. Chúng tôi tiếp tục khám phá ảnh hưởng tiềm năng của biến thiên attention trong các lớp downstream bằng cách tính toán một biến thiên tích lũy có trọng số. Metric này tổng hợp các biến thiên của tất cả các lớp downstream bắt đầu từ mỗi lớp cụ thể, có trọng số bởi trung bình của những biến thiên được tổng hợp này. Như minh họa trong Hình 5, phân tích tiết lộ rằng các lớp đầu thể hiện biến thiên có trọng số cao hơn đáng kể so với các lớp sau. Biến thiên này có xu hướng giảm khi tiến triển qua kiến trúc của mô hình, gợi ý sự ổn định của cơ chế attention trong các lớp sau. Dựa trên những kết quả này, các thí nghiệm của chúng tôi chủ yếu tập trung vào việc áp dụng SA trong các lớp sau, nơi những biến thiên như vậy dường như ổn định.

Hình 4: Hình này minh họa việc triển khai Shared Attention trong các phân đoạn lớp cụ thể của mô hình. Shared Attention trải dài từ lớp 27 đến 30 cho một phân đoạn bốn lớp và từ lớp 23 đến 30 cho một phân đoạn tám lớp.

Kết quả của những thí nghiệm này, như được tóm tắt trong Bảng 1, tiết lộ các mẫu thú vị. Đối với mô hình Llama2-7B, việc triển khai SA trong các lớp sau (ví dụ, SA:23∼26 và SA:27∼30) duy trì hiệu suất tương đối ổn định qua nhiều benchmark, bao gồm GLUE và MMLU. Ngược lại, việc mở rộng phạm vi của SA để bao gồm nhiều lớp hơn, đặc biệt các lớp cấp độ giữa như SA:15∼18, dẫn đến sự suy giảm đáng chú ý trong các nhiệm vụ đòi hỏi lý luận toán học (GSM8K).

So sánh, mô hình Llama3-8B, vốn hiển thị sự tương tự attention theo lớp cao hơn như đã thảo luận

--- TRANG 6 ---
Mô hình GLUE GSM8K 5-shot HellaSwag MMLU
Llama2-7B 0.4050 ±0.0019 0.1395±0.0095 0.5713±0.0049 0.4119±0.0041
Llama2-7B SA:23∼30 0.3819±0.0019 0.0728±0.0072 0.5575±0.0050 0.3794±0.0040
Llama2-7B SA:27∼30 0.3882±0.0019 0.1243±0.0091 0.5616±0.0050 0.4056±0.0041
Llama2-7B SA:23∼26 0.4351±0.0019 0.1122±0.0087 0.5681±0.0049 0.3994±0.0040
Llama2-7B SA:19∼22 0.3996±0.0019 0.0834±0.0076 0.5553±0.0050 0.3926±0.0040
Llama2-7B SA:15∼18 0.3731±0.0019 0.0220±0.0040 0.4790±0.0050 0.3378±0.0047
Llama2-7B-Instruct-SFT 0.5372 ±0.0019 0.1440±0.0097 0.5772±0.0049 0.3722±0.0040
Llama2-7B-Instruct-SFT SA:23∼30 0.5401±0.0019 0.0758±0.0073 0.5671±0.0049 0.3717±0.0040
Llama3-8B 0.4804 ±0.0019 0.5155±0.0138 0.6009±0.0049 0.6198±0.0038
Llama3-8B SA:23∼30 0.5595±0.0019 0.3275±0.0129 0.6011±0.0049 0.6122±0.0038
Llama3-8B SA:27∼30 0.5532±0.0019 0.4526±0.0137 0.6060±0.0049 0.6163±0.0038
Llama3-8B SA:23∼26 0.5024±0.0019 0.4556±0.0137 0.5993±0.0049 0.6189±0.0038
Llama3-8B SA:19∼22 0.5115±0.0019 0.3745±0.0133 0.5829±0.0049 0.6181±0.0038
Llama3-8B SA:15∼18 0.4685±0.0019 0.0136±0.0032 0.5307±0.0050 0.3019±0.0038

Bảng 1: Metrics hiệu suất cho các mô hình khác nhau qua các nhiệm vụ

Hình 5: Hình này hiển thị biến thiên tích lũy có trọng số cho các mô hình Llama2-7B-chat và Llama3-8B-instruct. Hai trục dưới đại diện cho cấu trúc của mô hình: trục trái chi tiết 32 lớp, và trục phải hiển thị 32 heads trong mỗi lớp. Trục z đại diện cho các giá trị biến thiên.

trong các phần trước, thể hiện ít suy giảm hiệu suất hơn khi SA được áp dụng. Sau khi triển khai SA trong các lớp gần hơn với đầu ra của mô hình (ví dụ, SA:27∼30), Llama3-8B thậm chí vượt trội hơn cấu hình gốc của nó trên benchmark GLUE, gợi ý rằng việc đặt chiến lược SA có thể tiềm năng nâng cao hiệu suất của mô hình trong các nhiệm vụ hiểu biết ngôn ngữ tự nhiên phức tạp.

Tinh chỉnh trên Bộ dữ liệu Instruct
Dựa trên các ràng buộc tính toán ngăn cản việc tiền huấn luyện LLMs với SA từ đầu, chúng tôi áp dụng việc tinh chỉnh các LLMs hiện có để đánh giá liệu tinh chỉnh có thể cải thiện những thiếu hụt hiệu suất quan sát được với việc áp dụng trực tiếp SA. Cách tiếp cận này đặc biệt nhằm hiểu khả năng thích nghi của SA dưới một chế độ học tập được kiểm soát hơn.

Tinh chỉnh được tiến hành trên bộ dữ liệu Instruct có sẵn công khai, được thiết kế để đánh giá các mô hình trên các nhiệm vụ đòi hỏi tuân theo các instructions phức tạp. Bộ dữ liệu này được chọn vì nó thách thức các mô hình sử dụng các biểu diễn đã học của chúng một cách hiệu quả, làm cho nó trở thành một benchmark lý tưởng để kiểm tra hiệu quả của các sửa đổi như SA.

Kết quả, như được tóm tắt trong Bảng 1, chứng minh một khoảng cách hiệu suất thu hẹp giữa các mô hình gốc và những mô hình được sửa đổi với SA. Ví dụ, trong khi mô hình Llama2-7B gốc vượt trội phiên bản SA trong các bài kiểm tra áp dụng trực tiếp, Llama2-7B SA:23∼30 được tinh chỉnh hiển thị cải thiện đáng kể qua nhiều metrics. Điều này gợi ý rằng tinh chỉnh cho phép mô hình tích hợp và tận dụng cơ chế Shared Attention tốt hơn, hiệu quả phục hồi một số hiệu suất bị mất đã ghi nhận trong việc áp dụng ban đầu SA.

Những phát hiện này cho thấy tiềm năng của tinh chỉnh như một phương pháp khả thi để tích hợp các thay đổi kiến trúc mới như SA vào các mô hình hiện có. Sự phục hồi trong hiệu suất cho thấy rằng với huấn luyện đầy đủ, những bất lợi ban đầu của việc áp dụng trực tiếp SA có thể được giảm thiểu, dẫn đến khả năng mô hình tăng cường phù hợp gần hơn với hoặc thậm chí vượt quá các cấu hình gốc của chúng.

Hướng Tương lai
Các điều tra thí nghiệm của chúng tôi đã chứng minh rằng việc triển khai Shared Attention (SA) qua nhiều lớp sau trong LLMs khơi gợi tổn thất độ chính xác tối thiểu, làm cho nó trở thành một cách tiếp cận hứa hẹn để nâng cao hiệu quả mô hình. Hơn nữa, phân tích của chúng tôi tiết lộ một xu hướng hướng tới các mẫu attention đẳng hướng trong quá trình tiền huấn luyện, cho thấy rằng các cơ chế attention của mô hình có xu hướng ổn định khi chúng xử lý nhiều dữ liệu hơn.

Dựa trên những hiểu biết này, việc tích hợp SA từ tiền huấn luyện dường như là một chiến lược đặc biệt có lợi. Sự tích hợp sớm này có thể cho phép các mô hình thích nghi tốt hơn với cơ chế attention được hợp lý hóa, tiềm năng cải thiện hiệu suất và hiệu quả qua các nhiệm vụ khác nhau. Việc nhúng nền tảng của SA có thể đơn giản hóa các thích nghi sau này và vốn hỗ trợ động lực attention hiệu quả.

--- TRANG 7 ---
Một hướng nghiên cứu hứa hẹn khác liên quan đến việc khám phá các kết hợp giữa SA và các chiến lược chia sẻ attention khác như Cross-Layer Attention (CLA). Việc kết hợp SA với các phương pháp như CLA có thể khai thác điểm mạnh của cả hai cách tiếp cận, dẫn đến một cơ chế attention mạnh mẽ và linh hoạt hơn. Cách tiếp cận toàn diện này đối với quản lý attention có thể cung cấp một giải pháp toàn diện tối đa hóa cả hiệu quả tính toán và khả năng mở rộng mô hình.

Bằng cách theo đuổi những hướng này, nghiên cứu tương lai không chỉ có thể tinh chỉnh việc áp dụng Shared Attention trong LLMs mà còn khám phá tiềm năng đầy đủ của nó trong việc nâng cao hiệu quả kiến trúc và hoạt động của các mô hình ngôn ngữ thế hệ tiếp theo. Những nỗ lực này có thể dẫn đến các mô hình được trang bị tốt hơn để xử lý độ phức tạp và tính đa dạng ngày càng tăng của các nhiệm vụ trong xử lý ngôn ngữ tự nhiên.

Công trình Liên quan
Quản lý bộ nhớ hiệu quả trong transformers là một lĩnh vực nghiên cứu quan trọng với các mục tiêu đa dạng từ việc giảm băng thông bộ nhớ và yêu cầu lưu trữ đến tối ưu hóa chi phí tính toán trong cả giai đoạn huấn luyện và suy luận. Đáng chú ý, công việc của chúng tôi tập trung vào việc giảm thiểu kích thước của bộ đệm Key-Value (KV) suy luận tồn tại giữa các lần chuyển mô hình, từ đó nâng cao hiệu quả mô hình mà không có sự thỏa hiệp đáng kể trong hiệu suất.

Hiệu quả Bộ nhớ trong Cơ chế Attention
Những nỗ lực đáng kể đã được thực hiện để giải quyết hiệu quả của bộ đệm KV sau huấn luyện. Các kỹ thuật như nén bộ đệm KV đã được khám phá rộng rãi. Ví dụ, các phương pháp như KVQuant (Hooper et al. 2024) và KIVI (Liu et al. 2024b) sử dụng các chiến lược quantization để giảm dấu chân bộ nhớ của các cặp KV xuống chỉ một vài bit. Hơn nữa, các công trình như AttentionSink (Xiao et al. 2023) và Scissorhands (Liu et al. 2024a) giới thiệu tính thưa thớt vào bộ đệm KV bằng cách lưu trữ có chọn lọc các phần tử dựa trên sự gần gũi hoặc tầm quan trọng của chúng đối với token tạo ra, do đó giảm yêu cầu lưu trữ tổng thể.

Đổi mới Kiến trúc để Giảm Bộ đệm KV
Các sửa đổi kiến trúc nhằm giảm kích thước bộ đệm KV là then chốt trong việc nâng cao hiệu quả của các mô hình ngôn ngữ lớn. Những chiến lược như vậy bao gồm việc giới hạn độ dài chuỗi hiệu quả, như được thấy trong Sparse Attention (Child et al. 2019), ràng buộc attention với các cửa sổ địa phương để giảm cả tải tính toán và chi phí bộ nhớ. Một cách tiếp cận khác liên quan đến việc thay thế attention softmax truyền thống bằng các lựa chọn thay thế có thể mở rộng như linear attention (Katharopoulos et al. 2020), duy trì độ phức tạp không gian không đổi và cung cấp khả năng mở rộng uyển chuyển hơn đối với số lượng token. Ngoài ra, các phương pháp như Grouped-Query Attention (GQA) (Ainslie et al. 2023) và Multi-Query Attention (MQA) (Shazeer 2019) tổng hợp attention qua nhiều queries, giảm đáng kể dấu chân bộ nhớ bằng cách chia sẻ các cặp KV qua các attention heads. Những đổi mới này cùng nhau đóng góp vào việc giảm sự dư thừa trong tính toán attention và liên quan trực tiếp đến công việc của chúng tôi, thông báo cho sự phát triển của chúng tôi về cơ chế Shared Attention tiếp tục tối ưu hóa sử dụng bộ nhớ bằng cách chia sẻ trọng số attention qua các lớp.

Kết luận
Trong bài báo này, chúng tôi khám phá động lực attention trong các LLMs tiên tiến và quan sát rằng phân bố attention qua các lớp có xu hướng đẳng hướng hóa sau tiền huấn luyện rộng rãi. Mẫu đẳng hướng của attention này, nơi các lớp thể hiện các cơ chế attention tương tự, truyền cảm hứng cho một cách tiếp cận mới để chia sẻ attention khác biệt khỏi các phương pháp thông thường.

Truyền thống, các phương pháp như MQA và CLA đã tập trung vào việc chia sẻ bộ đệm KV để giảm chi phí bộ nhớ nhưng vẫn yêu cầu việc tính toán trọng số attention độc lập qua mỗi lớp. Phương pháp Shared Attention (SA) được đề xuất của chúng tôi bỏ qua sự dư thừa này bằng cách chia sẻ trực tiếp các trọng số attention đã tính toán qua nhiều lớp. Cách tiếp cận này không chỉ giảm đáng kể kích thước của bộ đệm KV mà còn giảm các FLOPs tính toán cần thiết trong quá trình suy luận mô hình.

Việc giới thiệu Shared Attention đại diện cho một thay đổi mô hình trong thiết kế các cơ chế attention trong mạng neural, nhấn mạnh hiệu quả mà không làm tổn hại đến hiệu suất của mô hình. Bằng cách giảm cả gánh nặng tính toán và yêu cầu bộ nhớ, SA cho phép triển khai LLMs có thể mở rộng và hiệu quả hơn, đặc biệt trong các môi trường mà tài nguyên bị ràng buộc.

Nghiên cứu này mở đường cho các khám phá sâu hơn vào các kiến trúc mô hình hiệu quả và mở ra những khả năng mới cho việc áp dụng LLMs qua một phổ rộng hơn của các nhiệm vụ và bộ dữ liệu. Công việc tương lai sẽ tập trung vào việc mở rộng khả năng áp dụng của Shared Attention, khám phá sự tích hợp của nó trong các giai đoạn ban đầu của huấn luyện mô hình, và kết hợp nó với các kỹ thuật tối ưu hóa khác để tối đa hóa hiệu quả hoạt động của LLMs.

Tài liệu Tham khảo
Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y.; Lebrón, F.; and Sanghai, S. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245.

Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan, Y.; Ge, W.; Han, Y.; Huang, F.; et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Brandon, W.; Mishra, M.; Nrusimha, A.; Panda, R.; and Kelly, J. R. 2024. Reducing Transformer Key-Value Cache Size with Cross-Layer Attention. arXiv preprint arXiv:2405.12981.

Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.

Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

--- TRANG 8 ---
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.

Hooper, C.; Kim, S.; Mohammadzadeh, H.; Mahoney, M. W.; Shao, Y. S.; Keutzer, K.; and Gholami, A. 2024. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079.

Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Fu, Y.; et al. 2024. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36.

Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, 5156–5165. PMLR.

Li, H.; Zhang, Y.; Koto, F.; Yang, Y.; Zhao, H.; Gong, Y.; Duan, N.; and Baldwin, T. 2023. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212.

Liu, Z.; Desai, A.; Liao, F.; Wang, W.; Xie, V.; Xu, Z.; Kyrillidis, A.; and Shrivastava, A. 2024a. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36.

Liu, Z.; Yuan, J.; Jin, H.; Zhong, S.; Xu, Z.; Braverman, V.; Chen, B.; and Kivi, X. H. 2024b. A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750.

Shazeer, N. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150.

Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 3(6): 7.

Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.

Xiao, G.; Tian, Y.; Chen, B.; Han, S.; and Lewis, M. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453.

Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; et al. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305.

Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.
