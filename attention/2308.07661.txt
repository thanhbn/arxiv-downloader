# 2308.07661.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2308.07661.pdf
# File size: 884680 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Attention IsNotAllYouNeedAnymore
Zhe Chen
School of Computer Science and Engineering
Northeastern University
Shenyang, Liaoning, China
ml_iot@163.com; chenzhe@mail.neu.edu.cn
Abstract
In recent years, the popular Transformer architecture has achieved great success in many application
areas, including natural language processing and computer vision. Many existing works aim to reduce
the computational and memory complexity of the self-attention mechanism in the Transformer by
trading o ffperformance. However, performance is key for the continuing success of the Transformer.
In this paper, a family of drop-in replacements for the self-attention mechanism in the Transformer,
called the Extractors, is proposed. Four types of the Extractors, namely the super high-performance
Extractor (SHE), the higher-performance Extractor (HE), the worthwhile Extractor (WE), and the
minimalist Extractor (ME), are proposed as examples. Experimental results show that replacing
the self-attention mechanism with the SHE evidently improves the performance of the Transformer,
whereas the simplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close
to or better than the self-attention mechanism with less computational and memory complexity.
Furthermore, the proposed Extractors have the potential or are able to run faster than the self-attention
mechanism since their critical paths of computation are much shorter. Additionally, the sequence
prediction problem in the context of text generation is formulated using variable-length discrete-time
Markov chains, and the Transformer is reviewed based on our understanding.
1 Introduction
It has been over six years since the introduction of the widely adopted Transformer architecture Vaswani et al. [2017].
While initially proposed for sequence transduction tasks, the Transformer has become the de-facto model architecture
for a broad range of natural language processing tasks in recent years Kim et al. [2023] and has been widely applied in
many other areas including computer vision Khan et al. [2022] and speech processing Latif et al. [2023].
As of July 2023, the standard Transformer with the self-attention mechanism still prevails, especially in large language
models (LLM) Touvron et al. [2023], due to its excellent parallelizability and capacity Zhao et al. [2023].
The key to the success of the Transformer lies in the self-attention mechanism Dowdell and Zhang [2019], a type of the
attention mechanism first introduced for machine translation in Bahdanau, Cho, and Bengio [2015].
However, a notable limitation of the Transformer is its quadratic computation and memory complexity in relation to
sequence length. This limitation arises from the self-attention mechanism Ren et al. [2021], posing a challenge for
applying the Transformer in scenarios involving long input sequences.
Therefore, many variants, including “e fficient Transformers” or “ x-formers”, have been proposed to address this issue
in the past few years Tay et al. [2022a]. In general, these variants leverage pre-determined patterns or learnable patterns,
low-rank approximations, kernels, downsampling, or sparsification to reduce the computation and memory complexity
in the self-attention mechanism. As a result, they generally underperform the vanilla self-attention mechanism Dong
et al. [2023].
Truly, the Transformer with the self-attention mechanism is a remarkable piece of artwork. It is quite challenging to
surpass it in terms of performance. In order to improve its performance, we need to comprehensively understand the
problem we face and every bit of the Transformer and consider replacing the self-attention mechanism, which is itsarXiv:2308.07661v2  [cs.LG]  19 Sep 2023

--- PAGE 2 ---
Attention Is Not All You Need Anymore ZheChen
Achilles’ heel. After all, self-attention is not an indispensable ingredient of the Transformer Liu et al. [2021], Mehta
et al. [2023], Tolstikhin et al. [2021]. It is time to move beyond the self-attention mechanism.
In this paper, we first formulate the sequence prediction problem (e.g., predicting the next token based on the preceding
tokens) in text generation using variable-length discrete-time Markov chains. Then, the Transformer is reviewed based
on our understanding. Furthermore, we propose a family of drop-in replacements for the self-attention mechanism,
called the Extractors. The Transformers equipped with the proposed Extractors are verified in a text generation
scenario. Experimental results show that with the Extractors replacing the self-attention mechanism, the Transformers
either perform much better with more computational and memory complexity or perform close to or better with less
computational and memory complexity, confirming that the self-attention mechanism is indeed not all we need anymore.
Our contributions are summarized as follows.
•We employ variable-length discrete-time Markov chains to formulate the sequence prediction problem in text
generation.
•We review the Transformer based on our own understanding.
•We propose a family of sublayers called the Extractors to replace the self-attention sublayer in the Transformer
in a drop-in fashion.
•We evaluate the performance of the Transformers equipped with the Extractors in a text generation setting
using free English children’s books as the dataset.
•We estimate the computational and memory complexities of both the self-attention sublayer and the proposed
Extractor sublayers.
2 Related Work
Several solutions have been proposed to replace the self-attention mechanism in a drop-in fashion.
In the area of computer vision, a multi-layer perceptron (MLP) is employed to replace the self-attention in Tolstikhin
et al. [2021] with linear computational complexity in sequence length, achieving slightly inferior performance to other
models.
In Tay et al. [2022b], the self-attention is replaced with convolutional blocks. The performance is not as good as that of
the standard Transformer in tasks that require modeling the relationship between two or more sequences.
In Lee-Thorp et al. [2022], the Fourier transform is employed to replace the self-attention mechanism. The performance
is close to that of the standard Transformer and the training speed is faster.
SPADE (state space augmented transformer) uses a state space model (SSM) in the bottom layer and e fficient local
attention mechanisms in other layers Zuo et al. [2022]. The performance of SPADE is close to that of the standard
Transformer, and it has a linear time and space computational complexity.
A subquadratic drop-in replacement for attention named Hyena is proposed in Poli et al. [2023]. Hyena alternatively
applies convolutions in the time domain and in the frequency domain. It achieves comparable performance to the
Transformer with a 20% reduction in training compute required at a sequence length of 2k.
3 Problem Formulation
In this paper, we use text generation as an example task for the Transformer. In text generation, additional tokens are
generated based on the given tokens. In this task, a token refers to a piece of text, e.g., a word. All the valid tokens
make up the vocabulary. To facilitate computation, each token in the vocabulary is associated with an index ranging
from 0 to u−1, where uis the size of the vocabulary.
The fundamental problem in text generation is sequence prediction, i.e., predicting the value of the next token index st+1
based on the values of a sequence of given token indices (s1,s2,···,st), where s1,s2,···,st+1∈{0,1,···,u−1}andt
is the length of the given input sequence. Since token indices are countable and can take any numbers in {0,1,···,u−1},
discrete random variables S1,S2,···,St+1can be used to associate with the token indices at time 1, time 2,..., time
t+1, respectively.
To predict the value of the next token index st+1more accurately, we may try to make good use of the values of the
token indices in the given sequence. Since the length of the sequence is not fixed in general in text generation, a
reasonable way is to view the sequence as a variable-length discrete-time Markov chain Buhlmann and Wyner [1999],
2

--- PAGE 3 ---
Attention Is Not All You Need Anymore ZheChen
Chen and Qiu [2010], where the value of the next token index only depends on the values of the token indices in the
given sequence with a maximum sequence length.
In this way, we define the conditional probability of taking the value of the next token index st+1, which is the
time-homogeneous state-transition probability of the Markov chain, as follows.
P(St+1=st+1|St=st,St−1=st−1,···,S1=s1) (1)
where P(·)denotes probability function. Let ldenote the length of the context window in text generation, which is also
the highest order of the variable-length discrete-time Markov chain. When t>l, according to the Markov property we
have
P(St+1=st+1|St=st,St−1=st−1,···,S1=s1)=P(St+1=st+1|St=st,St−1=st−1,···,St−l+1=st−l+1)(2)
Thus, the sequence prediction problem we are facing turns into the problem of predicting the probability of taking value
st+1given a sequence of values ( s1,s2,···,st).
In text generation, if we have the probabilities P(St+1=st+1|St=st,St−1=st−1,···,S1=s1)for all st+1in
{0,1,···,u−1}, strategies such as top- psampling and top- ksampling can be employed to choose a value for St+1.
But how can we get these probabilities? In machine learning, we try building a model to predict or infer these
probabilities by taking the sequence (s1,s2,···,st)as the input to the model. This is where the Transformer is applied
in text generation.
The next question is how to train the model. Since we have no idea what these probabilities should look
like before we train the model, a strategy we employ here is simply maximizing the predicted probability
ˆP(St+1=st+1|St=st,St−1=st−1,···,S1=s1)that the model outputs given an observed sequence (s1,s2,···,st,st+1).
With this strategy, the loss function for training the model given a training sequence of length lcan be derived as below.
This is the same as what we get if we alternatively regard sequence prediction as a multi-class classification task.
L(W)=−1
llX
t=1lnˆP(St+1=st+1|St=st,···,S1=s1)
(3)
where L(·)denotes the loss function and Wgenerally refers to all the trainable model parameters. To keep the problem
simple, in the rest of this paper, we assume t≤l.
4 Understanding The Transformer
The Transformer provides us with an e ffective way, but of course not necessarily the only e ffective way, to build the
aforementioned machine learning model. A distinctive feature of the Transformer is that it keeps its internal “dimension”
constant across all layers, which makes the Transformer look like a kind of transformation in general.
As shown in Fig. 1, the input to the Transformer is a sequence of token indices denoted by s=(s1,s2,···,st)T,
where s∈Zt×1and(·)Tdenotes transpose. The sequence of token indices sis virtually converted into one-hot
encodings X[in_tok]before it goes through a linear transformation called “embedding”, as shown in Eq. (4), where
X[in_tok]=(x[in_tok]
i)T
1≤i≤t,X[in_tok]∈Zt×u,x[in_tok]
i=([si=j−1])1≤j≤u,x[in_tok]
i∈Z1×u, and [·]is the Iverson bracket,si=j−1=1 ifsi=j−1 otherwisesi=j−1=0.
X[emb_tok]=X[in_tok]W[emb_tok](4)
where X[emb_tok]∈Rt×d,W[emb_tok ]is a weight matrix, W[emb_tok ]∈Ru×d, and dis the internal “dimension” of the
Transformer.
In this paper, we use learned positional embeddings, since their performance is nearly identical to that of the sinusoidal
version Vaswani et al. [2017]. Then we have
X[emb_pos]=X[in_pos]W[emb_pos](5)
where X[emb_pos]∈Rt×d,X[in_pos]=(x[in_pos]
i)T
1≤i≤t,X[in_pos]∈Zt×l,x[in_pos]
i=([i=j])1≤j≤l,x[in_pos]
i∈Z1×l,W[emb_pos]is
a weight matrix, and W[emb_pos]∈Rl×d. Note that according to Vaswani et al. [2017], embedding weight matrices such
asW[emb_tok]andW[emb_pos]are mulitiplied by√
d.
Then, the two embedding matrices are added together, and the elements of their sum matrix are randomly set to zeros
with a probability of pduring training, which is called “dropout”. Dropout is a regularization technique that enhances
3

--- PAGE 4 ---
Attention Is Not All You Need Anymore ZheChen
Figure 1: The Transformer.
the performance of the model during inference, albeit at the expense of decreased model “capacity”. As depicted in
Eq. (6), dropout is applied to each element x[in_drop]
i,jof its input matrix X[in_drop], where X[in_drop]=(x[in_drop]
i,j)1≤i≤t,1≤j≤d
andX[in_drop]∈Rt×d.
x[out_drop]
i,j=(1
1−px[in_drop]
i,jwith probability 1 −p
0 with probability p(6)
where x[out_drop]
i,jis the element of its output matrix X[out_drop]andX[out_drop]∈Rt×d. To simplify equations, we denote
dropout as the “function” dropout( ·). Then, the input to the first layer of the Transformer X[in_lay1]can be written as
X[in_lay1]=dropout( X[emb_tok]+X[emb_pos]) (7)
where X[in_lay1]∈Rt×d.
As shown in Fig. 1, there are two sublayers in a layer of the Transformer. In the standard Transformer, the first
sublayer is the multi-head self-attention sublayer, and the second sublayer is a feed-forward network (FFN). The inputs
4

--- PAGE 5 ---
Attention Is Not All You Need Anymore ZheChen
Figure 2: The multi-head self-attention sublayer.
to the sublayers first go through layer normalizations, which are called pre-layer normalizations, and dropouts are
applied to the outputs of the sublayers. Pre-layer normalization is employed in this paper since Transformers with
pre-layer normalizations can be trained without the warm-up stage Xiong et al. [2020]. There are residual connections
between the inputs of the layer normalizations and the outputs of the dropouts. Both pre-layer normalizations and
residual connections are beneficial for easing the training process. Layer normalization is applied to each row vector
x[in_layernorm]
iof its input matrix X[in_layernorm], where X[in_layernorm]=(x[in_layernorm]
i)T
1≤i≤tandX[in_layernorm]∈Rt×d, as
shown in Eq. (8) Ba, Kiros, and Hinton [2016].
x[out_layernrom]
i=g[layernorm]◦x[in_layernorm]
i−µi
σi+b[layernorm](8)
where g[layernorm]is the gain vector and b[layernorm]is the bias vector, g[layernorm],b[layernorm]∈R1×d,µiandσiare the
mean and standard deviation of the elements of x[in_layernorm]
i, respectively, x[out_layernrom]
iis the i-th row vector of the
output matrix X[out_layernorm],X[out_layernorm]∈Rt×d,i=1,2,···,t, and◦denotes element-wise product or Hadamard
product. To simplify equations, we denote layer normalization as the “function” layernorm (·). Then, the input to the
first sublayer X[in_sub1]can be obtained following Eq. (9), where X[in_sub1]∈Rt×d.
X[in_sub1]=layernorm( X[in_lay1]) (9)
Fig. 2 depicts the multi-head self-attention sublayer. The input X[in_sub1]is first transformed into nsets of query /key/value
matrices, namely Qj,Kj, and Vj, where j=1,2,···,nandnis the number of “heads”.
Qj=X[in_sub1]W[Q]
j(10)
Kj=X[in_sub1]W[K]
j(11)
Vj=X[in_sub1]W[V]
j(12)
where Qj,Kj,Vj∈Rt×h,W[Q]
j,W[K]
j,W[V]
jare weight matrices, W[Q]
j,W[K]
j,W[V]
j∈Rd×h, and h=d
n. Then, scaled dot
products are calculated using both query matrices and key matrices, as shown in Eq. (13).
Z[att]
j=QjKT
j√
h(13)
5

--- PAGE 6 ---
Attention Is Not All You Need Anymore ZheChen
where Z[att]
j∈Rt×tandj=1,2,···,n. After that, softmax is applied to the scaled dot products to output normalized
weights Aj, where Aj=
aj,i,k
1≤i≤t,1≤k≤t,Aj∈Rt×t,j=1,2,···,n, and
aj,i,k=ezj,i,kPi
f=1ezj,i,fk=1,2,···,i
0 k=i+1,···,t(14)
Note that masking is also applied at the same time in Eq. (14). Then, weighted resultants are computed for each
“head” using these normalized weights, and the resultants are simply put together to produce a matrix, which is called
concatenation.
Rj=AjVj (15)
X[out_con]=(R1,R2,···,Rn) (16)
where X[out_con]∈Rt×d,Rj∈Rt×h, and j=1,2,···,n. The output of the concatenation X[out_con]goes through a linear
transformation, which concludes the multi-head self-attention sublayer, as shown in Eq. (17).
X[out_sub1]=X[out_con]W[out_att](17)
where X[out_sub1]is the output of the multi-head self-attention sublayer, X[out_sub1]∈Rt×d,W[out_att]is a weight matrix,
W[out_att]∈Rd×d. According to Fig. 1, the input to the second sublayer, or the FFN sublayer, X[in_sub2]can be obtained
following Eq. (18), where X[in_sub2]∈Rt×d.
X[in_sub2]=layernorm(dropout( X[out_sub1])+X[in_lay1]) (18)
And the output of the FFN sublayer X[out_sub2]is produced according to Eq. (19).
X[out_sub2]=ReLU( X[in_sub2]W[sub2_1]+b[sub2_1])W[sub2_2]+b[sub2_2](19)
where ReLU (·)denotes the rectified linear unit (ReLU) activation function, W[sub2_1 ]andW[sub2_2 ]are weight matrices,
W[sub2_1]∈Rd×c,W[sub2_2]∈Rc×d,b[sub2_1 ]andb[sub2_2 ]are bias vectors, b[sub2_1]∈R1×c,b[sub2_2]∈R1×d, and cis the
number of the nodes in the hidden layer of the FFN. The output of the first layer of the Transformer X[out_lay1]can be
computed following Eq. 20, where X[out_lay1]∈Rt×d.
X[out_lay1]=dropout( X[out_sub2])+dropout( X[out_sub1])+X[in_lay1](20)
There are mlayers stacked together in the Transformer as depicted in Fig. 1. Eq. (9) to Eq. (20) are repeated until
we obtain the output of the m-th layer X[out_lay m]. Since sequence prediction in text generation can also be viewed as
a multi-class classification task, the last component in the Transformer is actually a softmax regression, along with
pre-layer normalization. The output of the softmax regression or the output of the Transformer ˆYis derived as shown in
Eq. (21).
ˆY=softmax
layernorm( X[out_lay m])W[soft]+b[soft]
(21)
where softmax (·)denotes the softmax activation function that takes the rows of its input matrix, W[soft]is a weight
matrix, W[soft]∈Rd×u,b[soft]is a bias vector, b[soft]∈R1×u,ˆY=
ˆyi,j
1≤i≤t,1≤j≤u, and ˆY∈Rt×u. Then we have
ˆP(Si+1=j−1|Si=si,Si−1=si−1,···,S1=s1)=ˆyi,j (22)
where i=1,2,···,tandj=1,2,···,u.
5 The Proposed Extractors
In this section, we propose a family of drop-in replacements named the Extractors to replace the self-attention mechanism
in the Transformer. A distinctive feature of the Extractors is that the weights are assigned in accordance with the reverse
order of their input sequence. As examples, we specify four types of the Extractors in this section.
In our view, the FFN sublayer somehow implements a “lookup table” or a “mapper” and thus “memorizes” the
relationship between its input and its output, which is beneficial for memorizing the state transitions in the Markov chain.
Therefore, if the length or the order of the Markov chain is constant, we may solve the sequence prediction problem
just using the FFN sublayer. However, the length of the Markov chain we encounter in text generation is variable,
which motivates us to address the variable-length issue in the Markov chain. A reasonable approach to tackle this issue
is to convert the “variable-length” problem into a “constant-length” problem, leading us to the idea of designing the
Extractors.
6

--- PAGE 7 ---
Attention Is Not All You Need Anymore ZheChen
Figure 3: The proposed SHE.
The primal idea of the Extractors is somewhat straightforward. Firstly, we extract unified features from the entire
variable-length input sequence as a constant-length “index” for “looking up” the “table”. Then, we adjust the unified
features according to the length of the sequence since the “index” is expected to reflect the length of the sequence.
That’s it. With the constant-length “index” (as well as the input of the layer), we “look up” the “table”, which is
implemented by the succedent FFN sublayer, and output the corresponding “memorized content” that “documents” in
which direction the state of the Markov chain should transition.
5.1 The Super High-Performance Extractor
Fig. 3 illustrates a type of the proposed Extractor called the super high-performance Extractor (SHE). The linear
transformations marked in light blue on the left aim to extract unified features from their inputs. The total number of
these linear transformation blocks is l. Since we assume t≤l, only tlinear transformation blocks are in use at a certain
time. The design of this “extraction” part is inspired by both recurrent neural networks (RNNs) and finite impulse
response (FIR) filters.
Given an input matrix to the Extractor sublayer X[in_sub1 ], where X[in_sub1]∈Rt×d,X[in_sub1 ]=(x[in_sub1 ]
j)T
1≤j≤t, and
x[in_sub1 ]
j∈R1×d, the output of the “extraction” part X[out_ext ]is computed following Eq. (23).
x[out_ext]
i=iX
j=1x[in_sub1]
jW[ext]
i−j+1(23)
where X[out_ext ]=(x[out_ext ]
i)T
1≤i≤t,X[out_ext]∈Rt×d,x[out_ext ]
i∈R1×d,W[ext]
1,W[ext]
2,···,W[ext]
lare weight matrices,
W[ext]
1,W[ext]
2,···,W[ext]
l∈Rd×d, and i=1,2,···,t.
The linear transformation marked in light yellow on the right in Fig. 3 intends to output element-wise adjustments for
the “extraction” part. This “adjustment” part takes X[in_sub1 ]as its input as well. The idea behind this choice is that the
latest input vector x[in_sub1]
t in each input sequence (x[in_sub1 ]
j)T
1≤j≤tcontains the positional information that reflects the
length of the sequence and contains valuable information for predicting the next state of the Markov chain. The output
of the “adjustment” part X[out_adj]is derived as follows.
X[out_adj]=(X[in_sub1]W[adj])◦X[out_ext](24)
where X[out_adj]∈Rt×d,X[out_adj ]=(x[out_adj ]
i)T
1≤i≤t,x[out_adj ]
i∈R1×d,W[adj]is a weight matrix, and W[adj]∈Rd×d.
We can take X[out_adj]as the output of the Extractor sublayer, for the following linear transformation shown in Eq. (25)
is optional, as it is not a critical component to the Extractor.
X[out_sub1]=X[out_adj]W[out](25)
7

--- PAGE 8 ---
Attention Is Not All You Need Anymore ZheChen
Figure 4: The proposed WE.
where X[out_sub1]is the output of the Extractor sublayer, X[out_sub1]∈Rt×d,X[out_sub1 ]=(x[out_sub1 ]
i)T
1≤i≤t,x[out_sub1 ]
i∈R1×d,
W[out]is a weight matrix, and W[out]∈Rd×d.
5.2 The Worthwhile Extractor
The “extraction” part of the SHE virtually uses shared-weight fully-connected neural networks to “extract” constant-
length “indices” and may require significant computational and memory resources. A simplified version of the SHE
sublayer called the worthwhile Extractor (WE) is proposed in this subsection.
As shown in Fig. 4, linear transformations in the “extraction” part of the SHE sublayer are replaced by element-wise
products. Thus, Eq. (23) is reduced to Eq. (26).
x[out_ext]
i=iX
j=1x[in_sub1]
j◦w[ext]
i−j+1(26)
where w[ext]
1,w[ext]
2,···,w[ext]
lare weight vectors, w[ext]
1,w[ext]
2,···,w[ext]
l∈R1×d, and i=1,2,···,t.
5.3 The Higher-Performance Extractor
To further enhance the performance of the WE, we incorporate a shared linear transformation in the “extraction”
part to enable the WE to approximate the SHE, as depicted in Fig. 5. We refer to this type of the Extractor as the
higher-performance Extractor (HE). In this case, Eq. (26) is replaced by Eq. (27) and Eq. (28).
X[ext]=X[in_sub1]W[in_ext](27)
where X[ext]is the output of the linear transformation, X[ext]∈Rt×d,X[ext]=(x[ext]
i)T
1≤i≤t,x[ext]
i∈R1×d,W[in_ext ]is a
weight matrix, and W[in_ext]∈Rd×d.
x[out_ext]
i=iX
j=1x[ext]
j◦w[ext]
i−j+1(28)
5.4 The Minimalist Extractor
The computational and memory complexity of the aforementioned Extractors can be further reduced. In this subsection,
we propose a simple Extractor called the minimalist Extractor (ME).
8

--- PAGE 9 ---
Attention Is Not All You Need Anymore ZheChen
Figure 5: The proposed HE.
Figure 6: The proposed ME.
As illustrated in Fig. 6, the ME only contains the “extraction” part, while the weight vectors in the WE or the HE are
further reduced to scalars. Eq. (29) describes what the ME does.
x[out_sub1]
i=iX
j=1x[in_sub1]
jw[ext]
i−j+1(29)
where w[ext]
1,w[ext]
2,···,w[ext]
lare weight scalars, w[ext]
1,w[ext]
2,···,w[ext]
l∈R, and i=1,2,···,t.
Last but not least, the Extractors are not limited to the above four types. Many variations of the four example types of
the Extractors can also be used to replace the self-attention mechanism. They are trade-o ffs between performance and
computational and memory complexity. For example, the Extractor that consists of merely the “extraction” part of the
HE.
6 Experiments
In order to evaluate the performance of the proposed Extractors, we replace the multi-head self-attention sublayer in the
Transformer introduced in Section 4 with the Extractor sublayers and employ text generation as an example task. It
should be noted that Transformers with the Extractor sublayers can also be applied to other tasks.
9

--- PAGE 10 ---
Attention Is Not All You Need Anymore ZheChen
Hyperparameter or setting Value
Size of the vocabulary ( u) 5000
Length of the context window ( l) 128
“Dimension” of the model ( d) 128
Number of the nodes in the hidden layer of the FFN ( c)512
Number of layers ( m) 18
Batch size 64
Number of batches for training 60000 (0.457 epochs)
Optimizer AdamW (γ=0.001,β1=0.9,β2=0.999)
Learning rate 0.001
Dropout rate ( p) 0.1
Table 1: Hyperparameters and settings for the experiments.
Experiments are conducted on an NVIDIA GeForce RTX 4050 GPU (graphics processing unit) with a memory size of
6GB, due to a very limited budget. For this reason, the scale of the models in our experiments is constrained. Therefore,
the vocabulary size used in commonplace LLMs may be too large for conducting the experiments. To address this issue,
we build a small-scale dataset for training the models in the experiments using only popular free English children’s
books, since unfortunately we did not find such a publicly available dataset. Specifically, the dataset is composed of the
top 100 books on English children’s literature available at gutenberg.org, a library of free ebooks. The raw text of the
books is further tokenized using the Hugging Face BPE (byte-pair encoding) tokenizer with a vocabulary size of 5000,
resulting in a total of 8.4M tokens.
We use training cost (i.e., the average training loss over a batch) as the evaluation matric since training cost equals
perplexity in this task. Perplexity measures how well a probability model predicts. The lower the perplexity, the better
the model predicts.
The models are implemented and trained using the PyTorch 2.0 framework. The biases of the models are initialized with
zeros, while the weights are initialized randomly following a normal distribution with a mean of zero and a standard
deviation of 0.01. The hyperparameters and settings for the experiments are listed in Table 1.
In order to fairly evaluate the performance of the Transformer with the proposed Extractors and the Transformer with
the self-attention mechanism, we train all the models with the same hyperparameters, settings, and training data. By
“the same training data”, we mean that not only all the batches in the training are the same, but also the orders of the
batches are the same. This can be implemented by setting the random seed a couple of times.
Since the number of heads in the self-attention sublayer slightly a ffects the performance, eight Transformer models
with di fferent numbers of heads (1, 2, 4, 8,16, 32, 64, and 128, respectively) are trained for comparison purposes. Fig. 7
shows the medians of the training costs for every non-overlapping 2000 batches. From the figure, we can see that the
model with the 1-head self-attention sublayers performs the worst, whereas the model with the 32-head self-attention
sublayers performs the best, as far as the hyperparameters listed in Table 1 are concerned. We use both the model with
the 1-head self-attention sublayers and the model with the 32-head self-attention sublayers in the following performance
evaluation.
Four Transformer models with the self-attention sublayers replaced by the proposed SHE /HE/WE/ME sublayers,
respectively, are trained using the same hyperparameters and settings. Fig. 8 shows the medians of the training
costs for every non-overlapping 2000 batches. Fig. 9 shows the training costs of the models with either the 32-head
self-attention sublayers or the proposed SHE sublayers for every batch. It can be observed that the proposed SHE
evidently outperforms the self-attention mechanism. The simplified versions of the SHE, namely the HE /WE/ME,
perform roughly as well as the self-attention mechanism. Specifically, in terms of the aforementioned hyperparameters
and settings, the proposed HE outperforms the 32-head self-attention. Additionally, the performances of WE and ME
are close to those of the 32-head self-attention and the 1-head self-attention, respectively.
For the purpose of human evaluation, we have the models with the SHE sublayers and the 32-head self-attention
sublayers generate a sequence of text starting with the same given prompt “Once upon a time there was a little princess
who” and using the same random seed. Top- psampling with pset to 0.6 is employed. The generated texts are listed in
Table 2. The quality of the generated texts generally improves as training cost drops.
Although the proposed SHE outperforms the self-attention mechanism under the hyperparameters and settings listed in
Table 1, we may wonder whether this conclusion still stands if other major hyperparameters such as the length of the
context window and the number of layers are altered. In order to answer this question, models with di fferent lengths
10

--- PAGE 11 ---
Attention Is Not All You Need Anymore ZheChen
Figure 7: Medians of the training costs of the models with the self-attention sublayers.
Figure 8: Medians of the training costs of the models with either the self-attention sublayers or the proposed Extractor
sublayers.
of the context window and di fferent numbers of layers are trained. The same hyperparameters and settings are used,
except for the number of layers mbeing reduced to 2, the length of the context window lbeing reduced to 32, and the
number of batches being reduced to 30000, for the sake of reducing the training time. Fig. 10 and Fig. 11 show the
medians of the training costs for the last 2000 batches. We can observe that, in general, the larger the length of the
context window, the greater the performance gap between the models with the proposed SHE sublayers and the models
with the 32-head self-attention sublayers. Additionally, both the models with the proposed Extractor sublayers and the
11

--- PAGE 12 ---
Attention Is Not All You Need Anymore ZheChen
Figure 9: Training costs of the models with either the 32-head self-attention sublayers or the proposed SHE sublayers.
Model Generated text
Transformer Once upon a time there was a little princess who was so fond of a great feast that she always went
with the 32- to bed, and slept till morning. The next morning, when the emperor was asleep, he felt that it was
head self- his duty to go to the emperor, and his eyes was full of pleasure. When he awoke he was in the
attention habit of taking up his thoughts, and taking up his hands to listen to the idea of his own being
sublayers disposed of, and he gave up the news of the emperor’s designs. He felt very much in love with ...
Transformer Once upon a time there was a little princess who was lying in a window. She was a little lonely
with the woman, and had found a little girl lying in a tree; but the old woman had never seen a golden
proposed apple, and she was too sleepy to find that the goose was so frightened that she could not see her
SHE eyes fall to the place where she could get it. ‘I am going to be a witch,’ said she; ‘I will soon get
sublayers a princess and behave with all my other things.’ So the old woman said to her that she was ...
Table 2: Generated texts.
models with the self-attention sublayers perform better as the number of layers increases. Both figures indicate the
effectiveness of the proposed Extractors.
7 Computational Complexity
Both performance and computational complexity matter. In this section, we estimate the computational complexities of
the proposed Extractor sublayers and compare them with that of the self-attention sublayer.
According to the architectures illustrated in Fig. 2, Fig. 3, Fig. 5, Fig. 4, and Fig. 6, given an input sequence of length l
during training, the numbers of multiplications, additions, divisions, exponentiations, and trainable parameters of the
Extractor sublayers and the self-attention sublayer are estimated in Table 3 and Table 4. During inference, given a new
input token at sequence length t, the incremental numbers of multiplications, additions, divisions, and exponentiations
of the Extractor sublayers and the self-attention sublayer are estimated in Table 5 and Table 6.
When substituting d=128andl=128, which is the case shown in Table 1, into Table 3 and Table 4, we obtain the
numbers of total arithmetic operations and trainable parameters, as listed in Table 7. We can see that both the number of
total arithmetic operations and the number of trainable parameters of the proposed SHE sublayer are much larger than
those of the self-attention sublayer. This explains why the proposed SHE is capable of outperforming the self-attention
mechanism, as the saying goes, “you get what you pay for”. The proposed HE outperforms the 32-head self-attention
12

--- PAGE 13 ---
Attention Is Not All You Need Anymore ZheChen
Figure 10: Medians of the training costs of the models with di fferent lengths of the context window (when m=2).
Figure 11: Medians of the training costs of the models with di fferent numbers of layers (when l=32).
mechanism with fewer arithmetic operations and the same number of trainable parameters. This suggests that the
proposed Extractors are indeed capable of outperforming the self-attention mechanism. Additionally, the proposed
WE outperforms the 1-head self-attention mechanism with fewer arithmetic operations and trainable parameters. The
performance of the proposed ME is close to that of the 1-head self-attention mechanism, with much fewer arithmetic
operations and trainable parameters (approximately1
10and1
512, respectively).
13

--- PAGE 14 ---
Attention Is Not All You Need Anymore ZheChen
Number of The self-attention sublayer The proposed SHE sublayer
Multiplications (3d·d
n·l+Pl
t=1(t·d
n+t·d
n))·n+d·d·lPl
t=1(d·d·t+d·d+d+d·d)
=l2d+4ld2+ld =1
2l2d2+5
2ld2+ld
Additions (Pl
t=1(t·(d
n−1)+(t−1)·d
n+(t−1))Pl
t=1((d−1)·d·t+(t−1)·d
+3(d−1)·d
n·l)·n+(d−1)·d·l +(d−1)·d+(d−1)·d)
=l2d+4ld2−4ld−ln =1
2l2d2+5
2ld2−3ld
Divisions nPl
t=1(t+t)=nl2+nl 0
Exponentiations nPl
t=1t=1
2nl2+1
2nl 0
Parameters 3d·d
n·n+d·d=4d2d·d·l+d·d+d·d=ld2+2d2
Table 3: Computational complexity in training (part I).
Number of The proposed HE sublayer The proposed WE sublayer The proposed ME sublayer
MultiplicationsPl
t=1(d·t+d·d+d+d·d)Pl
t=1(d·t+d·d+d+d·d)Pl
t=1(d·t)
+d·d·l =1
2l2d+3
2ld+2ld2=1
2l2d+1
2ld
=1
2l2d+3
2ld+3ld2
AdditionsPl
t=1((t−1)·d+(d−1)·dPl
t=1((t−1)·d+(d−1)·dPl
t=1((t−1)·d)
+(d−1)·d)+(d−1)·d·l+(d−1)·d) =1
2l2d−1
2ld
=3ld2+1
2l2d−7
2ld =2ld2+1
2l2d−5
2ld
Divisions 0 0 0
Exponentiations 0 0 0
Parameters d·l+d·d+d·d+d·d d·l+d·d+d·d l
=ld+3d2=ld+2d2
Table 4: Computational complexity in training (part II).
In practice, given su fficient computing power, the critical path of computation matters. Take the inference phase as an
example. According to Fig. 2, the critical path of the self-attention sublayer is “multiplication - cumulation - multipli-
cation - cumulation - division - exponentiation - cumulation - division - multiplication - cumulation - multiplication -
cumulation”, whereas according to Fig. 3 the critical path of the SHE sublayer is “multiplication - cumulation - multi-
plication - multiplication - cumulation”. The critical path of the SHE sublayer is shorter than that of the self-attention
sublayer, meaning the proposed SHE sublayer has the potential to run faster than the self-attention sublayer. Moreover,
according to Fig. 6 the critical path of the ME sublayer is “multiplication - cumulation”, which is much shorter.
8 Conclusion
In this paper, a family of drop-in replacements for the existing self-attention mechanism in the Transformer, called the
Extractors, is proposed and evaluated. Specifically, four versions of the Extractors, namely the SHE, the HE, the WE,
and the ME, are proposed as examples.
Experimental results show that the proposed SHE evidently outperforms the self-attention mechanism. Although the
SHE requires more trainable parameters and computations, it has a shorter critical path of computation and thus has the
potential to run faster, providing a way to significantly boost the performance of the Transformer. The proposed HE
and WE are capable of outperforming the multi-head self-attention and the 1-head self-attention, respectively, with
fewer arithmetic operations and trainable parameters. They (as well as their variants) are ideal candidates for replacing
the self-attention mechanism. The proposed ME is suitable for computation-constrained scenarios since it requires
much fewer arithmetic operations and trainable parameters while maintaining a performance close to that of the 1-head
self-attention.
Furthermore, the sequence prediction problem in the context of text generation is formulated using variable-length
discrete-time Markov chains, and the Transformer is reviewed based on our understanding.
14

--- PAGE 15 ---
Attention Is Not All You Need Anymore ZheChen
Number of The self-attention sublayer The proposed SHE sublayer
Multiplications
3d·d
n+t·d
n+t·d
n
·n+d·d d·d·t+d·d+d+d·d
=2td+4d2=td2+2d2+d
Additions (3(d−1)·d
n+t·(d
n−1)+(t−1)·d
n(d−1)·d·t+(t−1)·d+(d−1)·d
+(t−1))·n+(d−1)·d +(d−1)·d
=2td−tn+t+4d2−5d−1 =td2+2d2−3d
Divisions (t+t)·n=2tn 0
Exponentiations t·n=tn 0
Table 5: Computational complexity in inference (part I).
Number of The proposed HE sublayer The proposed WE sublayer The proposed ME sublayer
Multiplications d·t+d·d+d·d+d d·t+d·d+d+d·d d·t=td
+d·d =td+2d2+d
=td+3d2+d
Additions (t−1)·d+(d−1)·d (t−1)·d+(d−1)·d (t−1)·d=td−d
+(d−1)·d+(d−1)·d +(d−1)·d
=td+3d2−4d =td+2d2−3d
Divisions 0 0 0
Exponentiations 0 0 0
Table 6: Computational complexity in inference (part II).
We hope this work will contribute to building powerful and cost-e ffective Transformer-based large models. Moreover,
it is anticipated that more research will be carried out to improve the performance of the Transformer or address the
sequence prediction problem inspired by this work.
References
Ba, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer Normalization. arXiv:1607.06450.
Bahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural Machine Translation by Jointly Learning to Align and Translate.
In Bengio, Y .; and LeCun, Y ., eds., 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings .
Buhlmann, P.; and Wyner, A. J. 1999. Variable Length Markov Chains. The Annals of Statistics , 27(2): 480–513.
Chen, Z.; and Qiu, R. C. 2010. Prediction of channel state for cognitive radio using higher-order hidden Markov model.
InProceedings of the IEEE SoutheastCon 2010 (SoutheastCon) , 276–282.
Dong, Z.; Tang, T.; Li, L.; and Zhao, W. X. 2023. A Survey on Long Text Modeling with Transformers.
arXiv:2302.14502.
Dowdell, T.; and Zhang, H. 2019. Is Attention All What You Need? – An Empirical Investigation on Convolution-Based
Active Memory and Self-Attention. arXiv:1912.11959.
Khan, S.; Naseer, M.; Hayat, M.; Zamir, S. W.; Khan, F. S.; and Shah, M. 2022. Transformers in Vision: A Survey.
ACM Comput. Surv. , 54(10s).
Kim, S.; Mangalam, K.; Moon, S.; Canny, J.; Malik, J.; Mahoney, M. W.; Gholami, A.; and Keutzer, K. 2023. Big
Little Transformer Decoder. arXiv:2302.07863.
Latif, S.; Zaidi, A.; Cuayahuitl, H.; Shamshad, F.; Shoukat, M.; and Qadir, J. 2023. Transformers in Speech Processing:
A Survey. arXiv:2303.11607.
Lee-Thorp, J.; Ainslie, J.; Eckstein, I.; and Ontañón, S. 2022. FNet: Mixing Tokens with Fourier Transforms. In
Carpuat, M.; de Marne ffe, M.; and Ruíz, I. V . M., eds., Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle,
WA, United States, July 10-15, 2022 , 4296–4313. Association for Computational Linguistics.
15

--- PAGE 16 ---
Attention Is Not All You Need Anymore ZheChen
Number of The 1-head The 32-head The SHE The HE The WE The ME
self-attention self-attention sublayer sublayer sublayer sublayer
sublayer sublayer
Multiplications 10,502,144 10,502,144 139,476,992 7,364,608 5,267,456 1,056,768
Additions 10,420,096 10,416,128 139,411,456 7,282,688 5,201,920 1,040,384
Divisions 16,512 528,384 0 0 0 0
Exponentiations 8,256 264,192 0 0 0 0
Parameters 65,536 65,536 2,129,920 65,536 49,152 128
Total arithmetic 20,947,008 21,710,848 278,888,448 14,647,296 10,469,376 2,097,152
operations
Table 7: Number of trainable parameters and total arithmetic operations in training when d=128 and l=128.
Liu, H.; Dai, Z.; So, D. R.; and Le, Q. V . 2021. Pay Attention to MLPs. In Ranzato, M.; Beygelzimer, A.; Dauphin,
Y . N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , 9204–9215.
Mehta, H.; Gupta, A.; Cutkosky, A.; and Neyshabur, B. 2023. Long Range Language Modeling via Gated State Spaces.
InThe Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .
Poli, M.; Massaroli, S.; Nguyen, E.; Fu, D. Y .; Dao, T.; Baccus, S.; Bengio, Y .; Ermon, S.; and Ré, C. 2023. Hyena
Hierarchy: Towards Larger Convolutional Language Models. arXiv:2302.10866.
Ren, H.; Dai, H.; Dai, Z.; Yang, M.; Leskovec, J.; Schuurmans, D.; and Dai, B. 2021. Combiner: Full Attention
Transformer with Sparse Computation Cost. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y . N.; Liang, P.; and Vaughan,
J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information
Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , 22470–22482.
Tay, Y .; Dehghani, M.; Bahri, D.; and Metzler, D. 2022a. E fficient Transformers: A Survey. ACM Comput. Surv. , 55(6).
Tay, Y .; Dehghani, M.; Gupta, J.; Bahri, D.; Aribandi, V .; Qin, Z.; and Metzler, D. 2022b. Are Pre-trained Convolutions
Better than Pre-trained Transformers? arXiv:2105.03322.
Tolstikhin, I. O.; Houlsby, N.; Kolesnikov, A.; Beyer, L.; Zhai, X.; Unterthiner, T.; Yung, J.; Steiner, A.; Keysers, D.;
Uszkoreit, J.; Lucic, M.; and Dosovitskiy, A. 2021. MLP-Mixer: An all-MLP Architecture for Vision. In Ranzato,
M.; Beygelzimer, A.; Dauphin, Y . N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,
2021, virtual , 24261–24272.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.;
Bhosale, S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.;
Fuller, B.; Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .;
Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao,
Y .; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.;
Schelten, A.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.;
Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and
Scialom, T. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.
Attention is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing
Systems , NIPS’17, 6000–6010. Red Hook, NY , USA: Curran Associates Inc. ISBN 9781510860964.
Xiong, R.; Yang, Y .; He, D.; Zheng, K.; Zheng, S.; Xing, C.; Zhang, H.; Lan, Y .; Wang, L.; and Liu, T. 2020.
On Layer Normalization in the Transformer Architecture. In III, H. D.; and Singh, A., eds., Proceedings of the
37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research ,
10524–10533. PMLR.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .; Min, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;
Chen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu, Z.; Liu, P.; Nie, J.-Y .; and Wen, J.-R. 2023. A Survey of
Large Language Models. arXiv:2303.18223.
Zuo, S.; Liu, X.; Jiao, J.; Charles, D.; Manavoglu, E.; Zhao, T.; and Gao, J. 2022. E fficient Long Sequence Modeling
via State Space Augmented Transformer. arXiv:2212.08136.
16
