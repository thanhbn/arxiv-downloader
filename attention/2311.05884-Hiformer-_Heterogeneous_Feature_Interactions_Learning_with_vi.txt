# Hiformer: Học Tương Tác Đặc Trưng Dị Nhất với Transformers cho Hệ Thống Gợi Ý
Huan Gui
Google DeepMind
Mountain View, California, USA
hgui@google.com

Ruoxi Wang
Google DeepMind
Mountain View, California, USA
ruoxi@google.com

Ke Yin
Google Inc
Mountain View, California, USA
keyin@google.com

Long Jin
Google Inc
Mountain View, California, USA
longjin@google.com

Maciej Kula
Google DeepMind
Mountain View, California, USA
maciejkula@google.com

Taibai Xu
Google Inc
Mountain View, California, USA
taibaixu@google.com

Lichan Hong
Google DeepMind
Mountain View, California, USA
lichan@google.com

Ed H. Chi
Google DeepMind
Mountain View, California, USA
edchi@google.com

## TÓM TẮT
Học tương tác đặc trưng là xương sống quan trọng để xây dựng hệ thống gợi ý. Trong các ứng dụng quy mô web, việc học tương tác đặc trưng cực kỳ thách thức do không gian đặc trưng đầu vào thưa thớt và lớn; đồng thời, việc chế tác thủ công các tương tác đặc trưng hiệu quả là không khả thi vì không gian giải pháp theo số mũ. Chúng tôi đề xuất tận dụng kiến trúc dựa trên Transformer với các lớp attention để tự động nắm bắt tương tác đặc trưng. Kiến trúc Transformer đã chứng kiến thành công lớn trong nhiều lĩnh vực, như xử lý ngôn ngữ tự nhiên và thị giác máy tính. Tuy nhiên, chưa có nhiều áp dụng kiến trúc Transformer cho mô hình hóa tương tác đặc trưng trong công nghiệp. Chúng tôi nhắm đến việc thu hẹp khoảng cách này. Chúng tôi xác định hai thách thức chính khi áp dụng kiến trúc Transformer thông thường cho hệ thống gợi ý quy mô web: (1) Kiến trúc Transformer không thể nắm bắt các tương tác đặc trưng dị nhất trong lớp self-attention; (2) Độ trễ phục vụ của kiến trúc Transformer có thể quá cao để triển khai trong hệ thống gợi ý quy mô web. Đầu tiên chúng tôi đề xuất lớp heterogeneous self-attention, là một sửa đổi đơn giản nhưng hiệu quả cho lớp self-attention trong Transformer, để tính đến tính dị nhất của tương tác đặc trưng. Sau đó chúng tôi giới thiệu Hiformer (Heterogeneous Interaction Transformer) để cải thiện thêm khả năng biểu đạt của mô hình. Với xấp xỉ rank thấp và cắt tỉa mô hình, Hiformer có suy luận nhanh cho triển khai trực tuyến. Kết quả thí nghiệm offline mở rộng xác nhận hiệu quả và hiệu suất của mô hình Hiformer. Chúng tôi đã triển khai thành công mô hình Hiformer vào mô hình xếp hạng App thực tế quy mô lớn tại Google Play, với cải thiện đáng kể trong các chỉ số tương tác chính (lên đến +2.66%).

**TỪ KHÓA**
Tương Tác Đặc Trưng Dị Nhất, Transformer, Hệ Thống Gợi Ý

**Định Dạng Tham Chiếu ACM:**
Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong, và Ed H. Chi. 2023. Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems. Trong Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 10 pages. https://doi.org/10.475/123_4

## 1 GIỚI THIỆU
Internet tràn ngập với vô số thông tin, khiến việc điều hướng hiệu quả và định vị nội dung phù hợp trở nên thách thức đối với người dùng. Hệ thống gợi ý lọc thông tin và trình bày nội dung phù hợp nhất cho từng người dùng [35,49]. Thông thường, hệ thống gợi ý được xây dựng như một bài toán học máy có giám sát, với mục tiêu tăng tương tác tích cực của người dùng với các gợi ý, như clicks [40,45,46], xem [50], hoặc mua hàng [36]. Do đó, việc triển khai hệ thống gợi ý với độ chính xác dự đoán cao có tầm quan trọng tối cao vì nó có thể có ảnh hưởng trực tiếp đến hiệu suất tài chính của doanh nghiệp, như doanh số và doanh thu [3–5, 49].

Tương tác đặc trưng là nơi nhiều đặc trưng có hiệu ứng cộng tác phức tạp lên việc dự đoán kết quả [43,46]. Đây là một trong những thành phần quan trọng của hệ thống gợi ý. Ví dụ, quan sát thấy người dùng thường xuyên tải xuống ứng dụng giao đồ ăn trong giờ ăn [8], cho thấy tương tác đặc trưng của app ids (ví dụ: ứng dụng giao đồ ăn) và bối cảnh thời gian (ví dụ: giờ ăn) có thể cung cấp tín hiệu quan trọng để dự đoán hành vi người dùng và đưa ra gợi ý cá nhân hóa. Tuy nhiên, mô hình hóa tương tác đặc trưng trong hệ thống gợi ý quy mô web đưa ra ba thách thức đáng kể. Trước hết, các tương tác đặc trưng đúng thường đặc thù cho từng lĩnh vực, việc chế tác thủ công các tương tác đặc trưng tốn thời gian và đòi hỏi kiến thức chuyên môn. Thứ hai, đối với các ứng dụng quy mô web, do số lượng lớn các đặc trưng thô, việc tìm kiếm các tương tác đặc trưng có thể có không gian giải pháp theo số mũ. Điều này khiến việc trích xuất thủ công tất cả các tương tác trở nên không khả thi. Cuối cùng nhưng không kém phần quan trọng, các tương tác đặc trưng được trích xuất có thể không tổng quát hóa tốt cho các nhiệm vụ khác. Do tầm quan trọng của nó, tương tác đặc trưng tiếp tục thu hút sự chú ý ngày càng tăng từ cả học thuật và công nghiệp [4, 5, 27, 33, 49].

Trong những năm gần đây, mạng nơ-ron sâu (DNNs) đã thu hút sự chú ý đáng kể trong nhiều lĩnh vực nghiên cứu, do hiệu suất mô hình xuất sắc cũng như khả năng học biểu diễn [22,49]. DNNs đã được áp dụng rộng rãi cho hệ thống gợi ý để học biểu diễn đặc trưng đầu vào thưa thớt [30] và học tương tác đặc trưng [1,3,8,26,29,32,40,45,46,49]. Nó đã trở thành công cụ mạnh mẽ để ánh xạ đầu vào lớn và thưa thớt vào không gian ngữ nghĩa chiều thấp. Một số công trình gần đây [8,24,29,32,46] trong học tương tác đặc trưng dựa trên các hàm tương tác đặc trưng tường minh, như Factorization Machine [26,34], inner product [32], kernel Factorization Machine [33], và Cross Network [45,46]. DNNs cũng được tận dụng để học các tương tác đặc trưng ẩn với nhiều lớp nơ-ron nhân tạo với các hàm kích hoạt phi tuyến [3, 29].

Một hướng nghiên cứu khác dựa trên cơ chế attention, đặc biệt là kiến trúc dựa trên Transformer [44] để học tương tác đặc trưng. Kiến trúc Transformer đã trở thành mô hình tiên tiến (SOTA) cho nhiều nhiệm vụ khác nhau, bao gồm thị giác máy tính (CV) [9,21] và xử lý ngôn ngữ tự nhiên (NLP) [19]. AutoInt [40] và InterHAt [25] đề xuất tận dụng lớp multi-head self-attention để học tương tác đặc trưng. Tuy nhiên, không giống như lĩnh vực NLP, ngữ nghĩa đặc trưng trong hệ thống gợi ý là động dưới các bối cảnh khác nhau.

Giả sử chúng ta đang gợi ý ứng dụng giao đồ ăn cho người dùng và chúng ta có các đặc trưng: app_id, hour_of_day, và user_country. Trong trường hợp này, app_id có thể có nghĩa này đối với hour_of_day và nghĩa khác đối với user_country. Để tương tác hiệu quả các đặc trưng khác nhau, chúng ta sẽ cần nhận thức ngữ nghĩa và căn chỉnh không gian ngữ nghĩa. Thật không may, các mô hình Transformer không xem xét điều này. Trong lớp attention thông thường, các đặc trưng được chiếu qua cùng các ma trận chiếu (tức là WQ và WK) được chia sẻ trên tất cả các đặc trưng. Đây là thiết kế tự nhiên cho các ứng dụng nơi ngữ nghĩa đặc trưng (token văn bản) độc lập với bối cảnh; tuy nhiên đối với hệ thống gợi ý nơi ngữ nghĩa đặc trưng thường phụ thuộc vào bối cảnh, thiết kế đồng nhất này sẽ dẫn đến khả năng biểu đạt mô hình hạn chế cho học tương tác đặc trưng.

Bên cạnh hạn chế không thể nắm bắt tương tác đặc trưng dị nhất, một hạn chế khác của mô hình Transformer là về độ trễ. Trong các ứng dụng quy mô web, thường có số lượng rất lớn người dùng và yêu cầu. Quan trọng là các mô hình có khả năng xử lý số lượng truy vấn cực cao mỗi giây, để mang lại trải nghiệm người dùng tối ưu. Tuy nhiên, chi phí suy luận của kiến trúc Transformer tỷ lệ bậc hai với độ dài đầu vào, điều này khiến suy luận thời gian thực sử dụng kiến trúc Transformer trở nên cấm kỵ trong hệ thống gợi ý quy mô web.

Mặc dù có những hạn chế, chúng tôi cho rằng việc nghiên cứu và cải thiện kiến trúc dựa trên Transformer cho tương tác đặc trưng là quan trọng. Trước hết, do thành công của kiến trúc Transformer trong nhiều lĩnh vực, số lượng đáng kể các tiến bộ và sửa đổi đã được thực hiện cho kiến trúc để cải thiện hiệu suất và khả năng. Việc cho phép các mô hình tương tác đặc trưng dựa trên Transformer cho hệ thống gợi ý có thể phục vụ như một cầu nối, cho phép các tiến bộ gần đây trong kiến trúc Transformer được áp dụng trong lĩnh vực gợi ý. Thứ hai, với việc áp dụng rộng rãi, thiết kế phần cứng (ví dụ: thiết kế chip [28]) có thể ưu tiên các kiến trúc giống Transformer. Do đó, hệ thống gợi ý với kiến trúc dựa trên Transformer cũng có thể hưởng lợi từ các tối ưu hóa được mang lại bởi phần cứng. Thứ ba, vì các mô hình Transformer để học tương tác đặc trưng dựa trên cơ chế attention, chúng cung cấp khả năng giải thích mô hình tốt [40]. Xây dựng mô hình gợi ý với cơ chế attention trong các ứng dụng quy mô web có thể mở ra cơ hội nghiên cứu mới.

Trong bài báo này, chúng tôi xem xét các hạn chế của kiến trúc mô hình Transformer thông thường và đề xuất một mô hình mới: Hiformer (Heterogeneous Feature Interaction Transformer) để học tương tác đặc trưng trong hệ thống gợi ý. Hiformer có nhận thức ngữ nghĩa đặc trưng và quan trọng hơn, hiệu quả cho triển khai trực tuyến. Để cung cấp nhận thức ngữ nghĩa đặc trưng trong học tương tác đặc trưng, chúng tôi thiết kế lớp attention dị nhất mới. Để tuân thủ các yêu cầu độ trễ nghiêm ngặt của các ứng dụng quy mô web, chúng tôi tận dụng kỹ thuật xấp xỉ rank thấp và cắt tỉa mô hình để cải thiện hiệu suất.

Chúng tôi sử dụng mô hình xếp hạng App quy mô lớn tại Google Play làm nghiên cứu tình huống. Chúng tôi thực hiện cả thí nghiệm offline và online với Hiformer để xác nhận hiệu quả trong mô hình hóa tương tác đặc trưng và hiệu suất trong phục vụ. Theo hiểu biết của chúng tôi, chúng tôi lần đầu tiên cho thấy kiến trúc dựa trên Transformer (tức là Hiformer) có thể vượt trội hơn các mô hình gợi ý SOTA để học tương tác đặc trưng. Chúng tôi đã triển khai thành công Hiformer như mô hình production.

Tóm lại, các đóng góp của bài báo như sau:
• Chúng tôi đề xuất mô hình Hiformer với lớp attention dị nhất mới để nắm bắt các hiệu ứng cộng tác phức tạp của các đặc trưng trong học tương tác đặc trưng. So với các phương pháp dựa trên Transformer thông thường hiện có, mô hình của chúng tôi có khả năng biểu đạt mô hình nhiều hơn.
• Chúng tôi tận dụng xấp xỉ rank thấp và cắt tỉa mô hình để giảm độ trễ phục vụ của mô hình Hiformer, mà không ảnh hưởng đến chất lượng mô hình.
• Chúng tôi tiến hành so sánh offline mở rộng với tập dữ liệu quy mô web để chứng minh tầm quan trọng của việc nắm bắt tương tác đặc trưng dị nhất. Chúng tôi cho thấy hiformer, như một kiến trúc dựa trên Transformer, có thể vượt trội hơn các mô hình gợi ý SOTA.
• Chúng tôi thực hiện kiểm tra A/B trực tuyến để đo lường tác động của các mô hình khác nhau lên các chỉ số tương tác chính, và mô hình Hiformer cho thấy lợi ích trực tuyến đáng kể so với mô hình baseline với sự gia tăng độ trễ hạn chế.

## 2 ĐỊNH NGHĨA BÀI TOÁN
Mục tiêu của hệ thống gợi ý là cải thiện tương tác tích cực của người dùng, như clicks, chuyển đổi, v.v. Tương tự như các nghiên cứu trước [26,27], chúng tôi xây dựng nó như một nhiệm vụ học máy có giám sát để dự đoán tương tác.

**Định nghĩa 2.1 (Hệ Thống Gợi Ý).** Gọi x∈R^(d_x) biểu thị các đặc trưng của một cặp (user, item). Có các đặc trưng phân loại x^C

và đặc trưng dày đặc x^D. Các đặc trưng phân loại được biểu diễn bằng mã hóa one-hot. Các đặc trưng dày đặc có thể được coi như các đặc trưng embedding đặc biệt với chiều embedding là 1. Số lượng đặc trưng phân loại và dày đặc lần lượt là |C| và |D|. Bài toán của hệ thống gợi ý là dự đoán liệu người dùng có tương tác với item dựa trên các đặc trưng đầu vào x.

Như đã đề cập trước đó, học tương tác đặc trưng cho phép mô hình nắm bắt mối quan hệ phức tạp hơn giữa các đặc trưng, từ đó cung cấp thông tin ảo cho các gợi ý chính xác hơn. Chúng tôi định nghĩa chính thức học tương tác đặc trưng dị nhất.

**Định nghĩa 2.2 (Tương Tác Đặc Trưng Dị Nhất bậc z).** Cho đầu vào với |F| đặc trưng và đầu vào là x={x_i}^|F|_(i=1), một tương tác đặc trưng bậc z là học một hàm ánh xạ phi cộng duy nhất ρ_Z(·) ánh xạ từ danh sách đặc trưng Z với z đặc trưng (tức là [x_(i1),...x_(iz)]) sang một biểu diễn mới hoặc một số vô hướng để nắm bắt mối quan hệ phức tạp giữa danh sách đặc trưng Z. Khi z = 2, ρ_Z(·) nắm bắt tương tác đặc trưng bậc hai.

Ví dụ, g([x1,x2])=w1x1+w2x2 không phải là tương tác đặc trưng bậc 2, vì w1x1+w2x2 là cộng. Trong khi đó, g([x1,x2])=x1^T x2 là tương tác đặc trưng bậc 2; và ρ1,2(·)=ρ2,1(·)=x1^T x2, có nghĩa là tương tác đặc trưng dị nhất là đối xứng.

Trước khi đi sâu vào chi tiết mô hình, chúng tôi định nghĩa ký hiệu. Số vô hướng được ký hiệu bằng chữ thường (a,b,...), vector bằng chữ thường đậm (a,b,...), ma trận bằng chữ hoa đậm (A,B,...).

## 3 MÔ HÌNH
Chúng tôi sẽ trình bày khung tổng quát trong Phần 3.1. Sau đó chúng tôi đề xuất lớp attention dị nhất trong Phần 3.3, dựa trên đó chúng tôi giới thiệu thêm mô hình Hiformer trong Phần 3.4.

### 3.1 Tổng Quan

#### 3.1.1 Lớp Input. 
Lớp input bao gồm các đặc trưng phân loại và đặc trưng dày đặc. Ngoài ra, chúng ta có task embedding. Task embedding có thể được coi như CLS token [6]. Khung đề xuất cũng có thể được mở rộng dễ dàng cho multi-task [2] với nhiều task embedding, và mỗi task embedding biểu diễn thông tin của mục tiêu huấn luyện tương ứng. Task embedding trong lớp input là tham số mô hình và được học thông qua huấn luyện mô hình end-to-end. Chúng tôi ký hiệu số lượng task embedding là t.

#### 3.1.2 Lớp Preprocessing. 
Chúng ta có lớp preprocessing để biến đổi lớp input thành danh sách các feature embedding, ngoài task embedding, như đầu vào cho các lớp tương tác đặc trưng. Chúng ta có một lớp preprocessing cho mỗi loại đặc trưng.

**Đặc Trưng Phân Loại.** Các đặc trưng phân loại đầu vào thường thưa thớt và có chiều cao. Việc sử dụng trực tiếp mã hóa one-hot cho huấn luyện mô hình sẽ dễ dẫn đến overfitting mô hình. Do đó, một chiến lược phổ biến là chiếu các đặc trưng phân loại vào không gian dày đặc chiều thấp. Đặc biệt, chúng ta học ma trận chiếu W^C_i ∈ R^(V_i×d) liên quan đến mỗi đặc trưng phân loại, và

e_i = x^C_i W^C_i,

trong đó e_i ∈ R^d, x^C_i ∈ {0,1}^V_i, V_i là chiều của mã hóa one-hot cho đặc trưng x^C_i, và d là chiều mô hình. Sau đó chúng ta sẽ sử dụng vector embedding dày đặc e_i như biểu diễn của đặc trưng phân loại x^C_i. {W^C_i}^|C| là các tham số mô hình và được gọi là bảng lookup embedding.

**Đặc Trưng Số Dày Đặc.** Các đặc trưng số dày đặc là các đặc trưng số. Vì các đặc trưng số dày đặc có thể đến từ các phân phối rất khác nhau, chúng ta sẽ cần biến đổi các đặc trưng về phân phối tương tự (tức là phân phối đồng đều) để đảm bảo tính ổn định số [51]. Ngoài ra, trong các ứng dụng quy mô web, có số lượng lớn đặc trưng số dày đặc, ít hơn nhiều so với các đặc trưng phân loại. Do đó, chúng ta tổng hợp tất cả các đặc trưng số dày đặc và chiếu chúng thành n_D embedding [29], trong đó n_D là siêu tham số được chọn theo kết quả thực nghiệm, n_D≪|D| để giảm tổng số lượng đặc trưng. Hàm chiếu f^D(·) có thể là multilayer perceptron (MLP) với các kích hoạt phi tuyến:

e_i = split_i(f^D(concat normalize({x^D_i}))), split_size = d,

trong đó e_i ∈ R^d, concat(·) là hàm nối, normalize(·) là hàm biến đổi đặc trưng, và split_i(·,split_size) là hàm chia tensor đầu vào thành các tensor có chiều bằng split_size và lấy tensor thứ i làm đầu ra. Trong kiến trúc dựa trên Transformer, chi phí suy luận tỷ lệ bậc hai với độ dài danh sách embedding đầu vào. Bằng cách tổng hợp |D| đặc trưng số dày đặc thành n_D embedding, độ dài của feature embedding được giảm, và do đó, chi phí suy luận cũng được giảm, mặc dù có đánh đổi việc thêm f^D(·).

Tóm lại về các lớp preprocessing, chúng ta biến đổi |C| đặc trưng phân loại, |D| đặc trưng số dày đặc, và t task embedding thành danh sách các embedding với chiều mô hình d. Số lượng embedding đầu ra từ các lớp preprocessing là L=|C|+n_D+T. Để dễ thảo luận, task embedding cũng được coi như các đặc trưng đặc biệt trong danh sách embedding. Do đó, tương tác đặc trưng không chỉ đề cập đến tương tác giữa các đặc trưng, mà còn giữa đặc trưng và nhiệm vụ.

#### 3.1.3 Lớp Feature Interaction. 
Lớp tương tác đặc trưng được sử dụng để học các hiệu ứng cộng tác phức tạp giữa các đặc trưng,

điều này quan trọng để nắm bắt sở thích của người dùng dưới các bối cảnh khác nhau. Đầu vào cho lớp tương tác đặc trưng là danh sách embedding đầu ra của các lớp preprocessing.

#### 3.1.4 Lớp Output và Mục Tiêu Huấn Luyện. 
Chúng ta chỉ sử dụng các task embedding được mã hóa từ lớp tương tác đặc trưng để dự đoán nhiệm vụ. Chúng ta huấn luyện một tháp MLP để chiếu task embedding được mã hóa thành dự đoán cuối cùng. Các nhiệm vụ huấn luyện có thể được cấu hình linh hoạt dựa trên ứng dụng gợi ý. Ví dụ, nhiệm vụ huấn luyện có thể là nhiệm vụ phân loại nếu các nhãn là click [39,40], cài đặt [15,47], hoặc mua hàng [23,31], và cũng có thể là nhiệm vụ hồi quy, như trong trường hợp thời gian xem [50]. Không mất tính tổng quát, chúng ta xem xét nhiệm vụ phân loại, với giá trị 0 chỉ tương tác tiêu cực và 1 chỉ tương tác tích cực. Do đó, mục tiêu huấn luyện có thể được xây dựng như bài toán phân loại nhị phân với Binary Cross Entropy (tức là Log Loss):

ℓ = 1/N ∑^N_(i=1) [-y_i log(p_i) - (1-y_i)log(1-p_i)],

trong đó y_i và p_i lần lượt là nhãn thực tế và xác suất dự đoán tương tác cho ví dụ i, và N là tổng số ví dụ huấn luyện.

### 3.2 Tương Tác Đặc Trưng Dị Nhất
Kiến trúc Transformer đã trở thành tiêu chuẩn de-facto cho nhiều nhiệm vụ như NLP, CV, v.v., và lớp multi-head self-attention đã đạt được hiệu suất đáng chú ý trong việc mô hình hóa các mối quan hệ phức tạp trong chuỗi [9,19,21,44]. Tuy nhiên, kiến trúc Transformer chưa được áp dụng rộng rãi cho hệ thống gợi ý quy mô web. Đã quan sát thấy rằng các kiến trúc dựa trên Transformer, như AutoInt [40], có thể không thể hiện hiệu suất tối ưu khi được áp dụng cho hệ thống gợi ý quy mô web.

Nhớ lại việc tính toán điểm attention của Transformer thông thường, việc tham số hóa học tương tác đặc trưng được chia sẻ trên tất cả các nhóm đặc trưng trong lớp multi-head self-attention. Thiết kế chia sẻ tham số này đến một cách tự nhiên trong các nhiệm vụ NLP, vì đầu vào cho Transformer trong các nhiệm vụ NLP thường là một câu được biểu diễn bởi danh sách các text embedding. Text embedding chủ yếu mã hóa thông tin độc lập với bối cảnh. Chúng ta gọi thiết lập này là tương tác đặc trưng đồng nhất. Minh họa chi tiết của kiến trúc Transformer được hiển thị trong Hình 4(a).

Ngược lại, có thông tin bối cảnh đa dạng trong hệ thống gợi ý. Các biểu diễn embedding được học động hơn và bao hàm thông tin từ nhiều góc độ. Trong ví dụ gợi ý ứng dụng giao đồ ăn, chúng ta có ba đặc trưng: app_id (ví dụ: Zomato), hour_of_day (ví dụ: 12pm), và user_country (ví dụ: India). Biểu diễn embedding được học cho ứng dụng Zomato mã hóa các thông tin ẩn khác nhau, bao gồm bản chất của ứng dụng, các quốc gia nơi nó phổ biến, thời gian nó được sử dụng thường xuyên, và ngôn ngữ của người dùng, v.v. Không phải tất cả thông tin được mã hóa sẽ hữu ích trong việc học tương tác đặc trưng nhất định. Khi học tương tác đặc trưng giữa app_id và user_country, có thể thông tin về các quốc gia nơi ứng dụng phổ biến quan trọng hơn nhiều so với những thông tin khác. Do đó, cốt lõi của học tương tác đặc trưng dị nhất là cung cấp nhận thức bối cảnh cho việc lựa chọn và biến đổi thông tin, để đạt được học tương tác đặc trưng chính xác hơn.

### 3.3 Lớp Attention Dị Nhất
Đầu tiên chúng tôi giới thiệu một sửa đổi đơn giản nhưng hiệu quả cho lớp multi-head self-attention trong mô hình Transformer: lớp heterogeneous multi-head self-attention, để nắm bắt tương tác đặc trưng bậc hai dị nhất. Để đơn giản, chúng tôi gọi lớp heterogeneous multi-head self-attention là lớp attention dị nhất, và mô hình tương ứng là mô hình attention dị nhất. Đặc biệt, chúng tôi thiết kế lại tính toán điểm multi-head self-attention cho đặc trưng i và j:

Att(i,j)^h = exp φ^h_(i,j)(e_i,e_j) / ∑^L_(m=1) exp φ^h_(i,m)(e_i,e_m), (1)

trong đó h đề cập đến head thứ h trong tổng số H head, L là độ dài danh sách embedding, φ^h_(i,j)(e_i,e_j) đo tương quan ngữ nghĩa giữa embedding e_i và e_j liên quan đến head h. Sau đó chúng tôi sẽ gọi lớp self-attention thông thường là lớp attention đồng nhất.

Với mỗi cặp đặc trưng (i,j), chúng ta có một φ^h_(i,j)(·,·) duy nhất để đo tương quan ngữ nghĩa. Nói cách khác, φ^h_(i,j)(·,·) phục vụ mục đích lựa chọn thông tin phù hợp nhất từ e_i và e_j cho bối cảnh của cặp đặc trưng (i,j). Chúng ta có thể có các hàm tùy ý cho φ^h_(i,j)(·,·):[R^d,R^d]→R, như một số hàm phi cộng tường minh, một mạng nơ-ron với biến đổi phi tuyến. Do tính đơn giản, chúng tôi chọn hàm dot-product. Theo đó,

φ^h_(i,j)(e_i,e_j) = e_i Q^h_i (e_j K^h_j)^T / √(d_k), (2)

trong đó Q_i ∈ R^(d×d_k), K_j ∈ R^(d×d_k) là các phép chiếu query và key cho đặc trưng i và j tương ứng, và √(d_k) để chuẩn hóa độ lớn của tích dot, thường được đặt là d_k=d/H.

Với các trọng số attention được tính trong Eq (1), chúng ta sau đó có thể tính đầu ra của lớp attention dị nhất như sau:

o_i = concat[∑_j Att(i,j)^h e_j V^h_j]^H_(h=1) O_j, (3)

trong đó V_j ∈ R^(d×d_v), O_j ∈ R^(Hd_v×d) là các phép chiếu value và output, và d_v thường được đặt là d_v=d/H.

Tương tự như lớp attention dị nhất, chúng tôi cũng thiết kế một Feed Forward Net (FFN) đầy đủ kết nối dị nhất cho mỗi đặc trưng. FFN được thực hiện thông qua hai hàm biến đổi tuyến tính với kích hoạt Gaussian Error Linear Unit (GELU) [12, 37]:

FFN^GELU_i(o_i) = GELU(o_i W^1_i + b^1_i)W^2_i + b^2_i, (4)

trong đó W^1_i ∈ R^(d×d_f), W^2_i ∈ R^(d_f×d), b^1_i ∈ R^d_f, b^2_i ∈ R^d, và d_f là kích thước lớp của lớp trung gian. Theo các nghiên cứu hiện có [44], d_f được đặt là d_f=4d, d là chiều của mô hình.

Như chúng ta có thể thấy thay đổi chính của lớp attention dị nhất, so với lớp self-attention là chúng ta học các ma trận chiếu query, key, và value (QKV) riêng lẻ (tức là Q,K,V) cho mỗi đặc trưng. Do đó, chúng ta tăng số lượng tham số trong lớp attention dị nhất, so với mô hình Transformer thông thường. Số lượng tham số tỷ lệ tuyến tính với

độ dài của danh sách embedding đầu vào. Tuy nhiên, đáng chú ý là tổng số FLOPs giống nhau đối với lớp attention dị nhất và lớp đồng nhất (lớp multi-head self-attention tiêu chuẩn trong Transformer). Điều này là do chúng ta có chính xác cùng các ops so với lớp Transformer đồng nhất, nhưng với các tham số hóa khác nhau.

Chúng tôi hình dung mẫu attention được học Att(·,·) trong Hình 2. Mẫu attention được học với mô hình Transformer thông thường, trong Hình 2(a), tương đối thưa thớt và thể hiện mẫu đường chéo mạnh, cho thấy lớp attention có thể không nắm bắt hiệu quả các hiệu ứng cộng tác giữa các đặc trưng khác nhau. Trong khi đó, đối với lớp attention dị nhất, có mẫu attention dày đặc. Một cách trực quan, quan sát có thể được giải thích bởi thực tế rằng lớp attention dị nhất có thể tốt hơn trong việc nắm bắt các hiệu ứng cộng tác giữa các đặc trưng nhờ ma trận biến đổi để căn chỉnh ngữ nghĩa đặc trưng.

### 3.4 Hiformer
Bây giờ chúng ta sẵn sàng giới thiệu mô hình Hiformer, nơi chúng ta tăng thêm khả năng biểu đạt mô hình, bằng cách giới thiệu học đặc trưng composite trong các phép chiếu QKV. Lấy phép chiếu key làm ví dụ. Thay vì học phép chiếu key đặc trưng cho mỗi đặc trưng, chúng ta định nghĩa lại phép chiếu key như sau:

[k̂^h_1,...k̂^h_L] = concat([e^h_1,...e^h_L])K̂^h, (5)

trong đó K̂^h ∈ R^(Ld×Ld_k). So với các ma trận chiếu key trong Phần 3.3, [K^h_i,...,K^h_L] ∈ R^(d×Ld_k), chúng ta tăng khả năng biểu đạt mô hình bằng cách thêm nhiều tham số vào phép chiếu key. Chúng ta gọi phép chiếu mới K̂^h là phép chiếu Composite. Về mặt toán học, thay vì học tương tác đặc trưng một cách tường minh với các cặp đặc trưng, chúng ta đầu tiên biến đổi danh sách feature embedding thành các đặc trưng composite làm key và value; sau đó chúng ta học tương tác đặc trưng dị nhất giữa các đặc trưng composite và task embedding. Tương tự, chúng ta áp dụng phép chiếu cross cho các phép chiếu query và value.

Tương tự như trong Eq (1), mô hình Hiformer tính điểm attention giữa query q_i và key k_i như sau:

Att^h_Composite(i,j) = exp q̂^h_i(k̂^h_i)^T/√(d_k) / ∑^L_l exp q̂^h_i(k̂^h_l)^T/√(d_k),

Với điểm attention, chúng ta sau đó có thể tính đầu ra của lớp attention trong mô hình Hiformer như:

ô_i = concat[∑_m Att^h_Composite(i,j)v̂^h_i]^H_(h=1) O_j. (6)

### 3.5 Hiệu Suất Tốt Hơn cho Hiformer
So với mô hình Transformer và lớp attention dị nhất, mô hình Hiformer đi kèm với khả năng biểu đạt mô hình nhiều hơn, nhưng cũng có chi phí tính toán trong suy luận. Do lớp Composite attention được giới thiệu, các kiến trúc Transformer hiệu quả hiện có [42] không thể được áp dụng trực tiếp.

Độ dài của danh sách feature embedding là L=|C|+n_D+t. Với chiều mô hình là d, chúng ta có phân tích chi phí suy luận¹ cho Hiformer: phép chiếu QKV: 3L²d², tính toán điểm attention: 2L²d, phép chiếu output: Ld², mạng FFN: 8Ld². Do đó, tổng tính toán là O(L²d²+L²d+Ld²). Để triển khai Hiformer cho suy luận thời gian thực, chúng ta giảm chi phí suy luận của Hiformer thông qua xấp xỉ rank thấp và cắt tỉa.

#### 3.5.1 Xấp Xỉ Rank Thấp. 
Như chúng ta có thể thấy, thành phần nổi bật cho chi phí suy luận Hiformer là các phép chiếu QKV với độ phức tạp tính toán O(L²d²).

Với phép chiếu Composite key được định nghĩa trong (5), chúng ta có thể xấp xỉ K̂^h với xấp xỉ rank thấp, tức là,

K̂^h = L^h_k(R^h_k)^T, (7)

trong đó L^h_v ∈ R^(Ld×r_v), R^h_v ∈ R^(Ld_k×r_v), và r_v là rank của xấp xỉ rank thấp cho K̂^h. Xấp xỉ rank thấp giảm chi phí tính toán cho phép chiếu value xuống O(Lr_v(d+d_k)). Tương tự, nó có thể được áp dụng cho phép chiếu composite query và value với rank r_k và r_v. Nếu r_k<Ld_k/2 và r_v<Ld_v/2, chi phí sẽ được giảm. Cấu trúc rank thấp cũng được quan sát trong phân tích dữ liệu. Lấy V̂^h làm ví dụ, chúng tôi vẽ các giá trị kỳ dị trong Hình 3, nơi chúng ta có thể thấy ma trận V̂^h thực sự có rank thấp.

![Hình 3: Các giá trị kỳ dị cho V̂^h trong phép chiếu Composite.]

Sau khi áp dụng xấp xỉ rank thấp, độ phức tạp mô hình Hiformer được tính như sau: phép chiếu query và key: O(Lr_k(d+d_k)), phép chiếu value O(Lr_v(d+d_v)), lớp attention O(L²d), phép chiếu output O(Ld²), và lớp FFN O(Ld²). Với d_k=d/H, d_v=d/H, và r_k<d_k, r_v<d_v, chúng ta có độ phức tạp tính toán là O(L²d+Ld²), tỷ lệ bậc hai với độ dài của danh sách embedding đầu vào L.

¹Tính toán này dựa trên thiết lập nơi d_k=d/H, d_v=d/H, và d_f=4d.

#### 3.5.2 Cắt Tỉa Mô Hình. 
Nhớ lại rằng trong Lớp Output (xem Hình 1), chúng ta chỉ sử dụng các task embedding được mã hóa cho huấn luyện và dự đoán mục tiêu cuối cùng. Do đó, chúng ta có thể tiết kiệm một số tính toán trong lớp cuối cùng của mô hình Hiformer bằng cách cắt tỉa tính toán mã hóa feature embedding. Đặc biệt, việc cắt tỉa dịch sang việc chỉ sử dụng task embedding làm query, và sử dụng cả task embedding và feature embedding làm key và value trong lớp attention Hiformer. Do đó, chúng ta có độ phức tạp tính toán của lớp Hiformer cuối cùng: phép chiếu query: O(tr_k(d+d_k)), phép chiếu key: O(Lr_k(d+d_k)), phép chiếu value: O(Lr_v(d+d_v)), tính toán lớp attention: O(Ltd), phép chiếu output: O(td²), lớp FFN: O(td²). Giả sử số lượng nhiệm vụ t(≪L) được coi là hằng số. Chúng ta có độ phức tạp tính toán của lớp cuối cùng của Hiformer là O(L(r_k+r_v)d+Ltd+td²).

Cắt tỉa cho phép chúng ta giảm độ phức tạp tính toán của lớp Hiformer cuối cùng từ tỷ lệ bậc hai xuống tỷ lệ tuyến tính với L. Kỹ thuật cắt tỉa này có thể được áp dụng cho tất cả các kiến trúc dựa trên Transformer. Một số công trình gần đây, như Perceiver [14], cũng đã giới thiệu ý tưởng tương tự để giảm chi phí phục vụ của các mô hình Transformer từ tỷ lệ bậc hai xuống tuyến tính.

## 4 THÍ NGHIỆM
Chúng tôi thực hiện kiểm tra thực nghiệm kỹ lưỡng các kiến trúc mô hình đề xuất với mô hình xếp hạng App quy mô web tại Google Play. Chúng tôi tiến hành cả thí nghiệm offline và online, với mục đích trả lời các câu hỏi nghiên cứu sau:

Q1. Lớp attention dị nhất có thể nắm bắt các tương tác đặc trưng dị nhất để đưa ra gợi ý tốt hơn, so với các mô hình Transformer thông thường không?
Q2. Với khả năng biểu đạt mô hình nhiều hơn trong mô hình Hiformer, liệu nó có thể cải thiện thêm chất lượng mô hình?
Q3. Hiệu suất phục vụ của mô hình Hiformer như thế nào so với các mô hình SOTA?
Q4. Các thiết lập siêu tham số ảnh hưởng như thế nào đến hiệu suất mô hình và độ trễ phục vụ và làm thế nào để đánh đổi giữa hiệu suất và hiệu quả?

### 4.1 Thiết Lập Đánh Giá Offline

#### 4.1.1 Tập Dữ Liệu. 
Để đánh giá offline, chúng tôi sử dụng dữ liệu ghi log từ mô hình xếp hạng. Các tương tác của người dùng là nhãn huấn luyện, 0 hoặc 1 chỉ liệu tương tác có xảy ra giữa người dùng và ứng dụng và hàm mất mát huấn luyện là LogLoss (xem Eq (3.1.4)). Chúng tôi huấn luyện mô hình với dữ liệu cửa sổ trượt 35 ngày, và đánh giá trên dữ liệu ngày thứ 36. Mô hình được huấn luyện lại từ đầu hàng ngày với dữ liệu mới nhất. Có 31 đặc trưng phân loại và 30 đặc trưng số dày đặc để mô tả các ứng dụng, như ID ứng dụng, tiêu đề ứng dụng, ngôn ngữ người dùng, v.v. Kích thước từ vựng của các đặc trưng phân loại dao động từ hàng trăm đến hàng triệu.

#### 4.1.2 Chỉ Số Đánh Giá. 
Chúng tôi sẽ đánh giá hiệu suất mô hình sử dụng AUC (Area Under the Receiver Operating Characteristic Curve). Chỉ số AUC có thể được hiểu như xác suất của mô hình gán một ví dụ tích cực được chọn ngẫu nhiên một xếp hạng cao hơn một ví dụ tiêu cực được chọn ngẫu nhiên. Trong tập dữ liệu ứng dụng quy mô web, do kích thước lớn của tập dữ liệu đánh giá, chúng tôi nhận thấy rằng cải thiện AUC ở mức 0.001 đạt được ý nghĩa thống kê. Các quan sát tương tự đã được thực hiện trong các ứng dụng quy mô web khác [3,8,15]. Ngoài ra, chúng tôi báo cáo LogLoss chuẩn hóa với Transformer một lớp làm baseline.

Chúng tôi huấn luyện mô hình trên TPU [16,17], và chúng tôi báo cáo Queries per second (QPS) huấn luyện để đo tốc độ huấn luyện. Chúng tôi ước tính độ trễ thông qua mô phỏng offline, nơi chúng tôi thực hiện suy luận của các mô hình trên 20 lô ví dụ với kích thước lô là 1024. Sau đó chúng tôi chuẩn hóa độ trễ của tất cả các mô hình so với mô hình baseline, là mô hình Transformer một lớp với cắt tỉa. Do số lượng lớn yêu cầu phục vụ trực tuyến, độ trễ phục vụ quan trọng hơn nhiều so với QPS huấn luyện.

#### 4.1.3 Kiến Trúc Mô Hình. 
Lưu ý rằng công trình của chúng tôi về học tương tác đặc trưng với Transformers, khác biệt hoàn toàn với mô hình hóa chuỗi lịch sử người dùng với Transformers [20,41]. Do đó, chúng tôi tập trung vào so sánh các mô hình đề xuất với các mô hình tương tác đặc trưng SOTA sau:

• AutoInt [40] được đề xuất để học tương tác đặc trưng thông qua lớp multi-head self-attention, và sau đó học các dự đoán cuối cùng thông qua các lớp Multi-layer Perceptrons (MLP).
• DLRM [29] là học tương tác đặc trưng thông qua factorization machine, và sau đó học tương tác đặc trưng ẩn thông qua một lớp MLP, cho các dự đoán cuối cùng.
• DCN [45] (tức là DCN-v2) là một trong những mô hình SOTA về học tương tác đặc trưng trong ứng dụng quy mô web, có mạng cross tạo ra các feature cross bậc bị chặn một cách tường minh nơi bậc tương tác đặc trưng tăng với độ sâu lớp.
• Transformer [44] với lớp multi-head self-attention để học tương tác đặc trưng. Sự khác biệt chính giữa Transformer và AutoInt là: 1. Transformer sử dụng task embedding được mã hóa cho dự đoán; trong khi AutoInt sử dụng tất cả feature và task embedding được mã hóa; 2. Có lớp FFN trong kiến trúc Transformer, nhưng không có trong lớp AutoInt.
• Transformer + PE là thêm Position Encoding vào kiến trúc Transformer, nơi chúng ta học một embedding bổ sung cho mỗi đặc trưng như feature encoding.
• HeteroAtt là lớp Heterogeneous Attention đề xuất của chúng tôi để nắm bắt tương tác đặc trưng dị nhất.
• Hiformer là mô hình đề xuất của chúng tôi với xấp xỉ rank thấp và cắt tỉa mô hình.

#### 4.1.4 Chi Tiết Thực Hiện. 
Huấn luyện và phục vụ mô hình được xây dựng trên Tensorflow 2 [38] và Model Garden [48]. Để đảm bảo so sánh công bằng, chúng tôi giữ tất cả các thành phần mô hình, ngoại trừ các lớp tương tác đặc trưng, giống nhau. Trong lớp embedding, chúng tôi đặt chiều embedding là 128 cho tất cả các mô hình. Lưu ý rằng đối với kiến trúc Transformer, HeteroAtt, và Hiformer, chúng tôi chỉ sử dụng CLS token được mã hóa (tức là task token được học) cho dự đoán nhiệm vụ cuối cùng.

Trong các thí nghiệm của chúng tôi trên Transformers, chúng tôi đặt chiều mô hình d=128, số head trong lớp attention H=4, và kích thước lớp của FFN là d_f=512. Chúng tôi đặt d_k=16 và d_v=64, thay vì đặt d_k=d/H, d_v=d/H như trong mô hình Transform và các mô hình dựa trên Transformer khác. Đối với các phương pháp khác, chúng tôi thực hiện điều chỉnh siêu tham số để có hiệu suất tốt nhất. Đối với thí nghiệm trực tuyến, chúng tôi điều chỉnh các siêu tham số dựa trên dữ liệu một ngày (ví dụ ngày 1 tháng 2), và cố định các thiết lập cho những ngày sau (tức là các ngày sau ngày 2 tháng 2).

### 4.2 So Sánh Tương Tác Đặc Trưng Đồng Nhất vs Dị Nhất (Q1)
Trước hết, chúng tôi quan sát rằng mô hình Transformer+PE hoạt động tương tự như mô hình Transformer thông thường. Điều này phù hợp với giả thuyết của chúng tôi. Mặc dù có nhiều tham số hơn, feature encoding chỉ học một bias embedding term cho mỗi đặc trưng, không đủ để học biến đổi đặc trưng cho học tương tác đặc trưng dị nhất. Thứ hai, chúng tôi quan sát rằng mô hình Transformer với lớp attention dị nhất (HeteroAtt) hoạt động tốt hơn đáng kể so với mô hình Transformer thông thường. Kết quả này xác nhận lập luận của chúng tôi rằng HeteroAtt có thể nắm bắt hiệu quả các tương tác đặc trưng phức tạp thông qua các ma trận biến đổi M_(i,j). Hơn nữa, chúng tôi quan sát rằng số tham số # của mô hình HeteroAtt hai lớp lớn hơn nhiều so với mô hình Transformer hai lớp, vì mô hình HeteroAtt có khả năng biểu đạt mô hình nhiều hơn. Do đó, chúng ta có thể trả lời khẳng định cho Q1, rằng tương tác đặc trưng dị nhất với nhận thức bối cảnh đặc trưng tốt hơn là quan trọng để cung cấp gợi ý cá nhân hóa.

### 4.3 So Sánh Hiệu Suất Mô Hình (Q2)
So sánh offline của các mô hình baseline và các mô hình đề xuất của chúng tôi được tóm tắt trong Bảng 1. Chúng tôi báo cáo hiệu suất tốt nhất của mỗi mô hình, với số lớp tương ứng.

Chúng tôi xem xét kỹ hơn mô hình AutoInt, DLRM, và Transformer thông thường. Trong học tương tác đặc trưng của các mô hình này, không có nhận thức đặc trưng và căn chỉnh ngữ nghĩa. Do đó, hiệu suất của chúng tương đối yếu hơn so với các mô hình khác. Trong mô hình DCN, Cross Net ẩn tạo ra tất cả các cross theo cặp và sau đó chiếu nó vào không gian chiều thấp hơn. Việc tham số hóa tất cả các cross theo cặp khác nhau, tương tự như cung cấp nhận thức đặc trưng. Do đó hiệu suất mô hình DCN có thể so sánh với các mô hình HeteroAtt của chúng tôi.

So với lớp attention dị nhất (tức là HeteroAtt), mô hình Hiformer cung cấp khả năng biểu đạt mô hình nhiều hơn trong các phép chiếu QKV. Nó đạt hiệu suất mô hình tốt nhất chỉ với một lớp, vượt trội hơn mô hình HeteroAtt và các mô hình SOTA khác.

### 4.4 Hiệu Suất Phục Vụ (Q3)
Chúng tôi so sánh hiệu suất phục vụ của tất cả các mô hình. Trước hết, chúng tôi so sánh mô hình Transformer hai lớp với mô hình một lớp. Như chúng ta có thể thấy, độ trễ của mô hình hai lớp hơn 2x so với mô hình một lớp. Điều này do thực tế là chúng tôi chỉ áp dụng cắt tỉa cho lớp thứ hai trong mô hình hai lớp, và chúng tôi đã áp dụng cắt tỉa cho mô hình một lớp. Tương tự, chúng ta có thể rút ra kết luận tương tự cho lớp HeteroAtt vì hai mô hình có cùng toán tử, do đó độ trễ. Tuy nhiên, kỹ thuật cắt tỉa như vậy không thể được áp dụng cho mô hình AutoInt. Do đó, mô hình AutoInt một lớp đắt hơn nhiều so với mô hình Transformer thông thường và mô hình HeteroAtt.

Thứ hai, chúng tôi so sánh mô hình Hiformer với mô hình HeteroAtt một lớp. Cả mô hình Hiformer một lớp và mô hình HeteroAtt một lớp đều hưởng lợi từ tối ưu hóa suy luận từ cắt tỉa. Tuy nhiên, do phép chiếu QKV biểu đạt hơn, chúng ta thấy rằng mô hình Hiformer đắt hơn 50.05%.

Bảng 1: So sánh hiệu suất mô hình offline về số tham số, AUC, và hiệu suất chuẩn hóa. Số tham số không bao gồm lớp embedding. ↑(↓) có nghĩa càng cao (thấp) càng tốt.

| Mô Hình | Lớp | Số Tham Số | AUC (↑) | LogLoss (%,↓) | Train QPS (↑) | Serving Latency (↓) |
|---------|-----|-------------|---------|---------------|---------------|-------------------|
| AutoInt | 1 | 12.39M | 0.7813 | -0.37096 | 5.45e6 | 2.28 |
| DLRM | - | 5.95M | 0.7819 | -0.47695 | 5.14e6 | 0.95 |
| DCN | 1 | 13.73M | 0.7857 | -0.79491 | 5.76e6 | 1.46 |
| Transformer | 1 | 0.74M | 0.7795 | 0 | 5.75e6 | 1.00 |
| | 2 | 0.84M | 0.7811 | -0.31797 | 4.12e6 | 3.03 |
| | 3 | 0.97M | 0.7838 | -0.45045 | 3.13e6 | 5.05 |
| Transformer+PE | 3 | 1.08M | 0.7833 | -0.39746 | 3.12e6 | 5.06 |
| HeteroAtt (ours) | 1 | 2.36M | 0.7796 | -0.05299 | 5.71e6 | 1.01 |
| | 2 | 10.50M | 0.7856 | -0.82141 | 4.10e6 | 3.11 |
| Hiformer (ours) | 1 | 16.68M | 0.7875 | -0.87440 | 5.69e6 | 1.52 |

Tuy nhiên, chúng tôi muốn chỉ ra rằng không có xấp xỉ rank thấp, mô hình Hiformer có thể đắt hơn nhiều. Trong thí nghiệm của chúng tôi, chúng tôi đặt rank của query và key là r_k=128, và value r_v=1024. Chúng tôi so sánh Hiformer có và không có xấp xỉ rank thấp trong Bảng 2. Xấp xỉ rank thấp tiết kiệm 62.7% độ trễ suy luận. Ngoài ra, không có mất mát chất lượng mô hình đáng kể, điều này được mong đợi vì cấu trúc rank thấp được quan sát của các ma trận chiếu query, key, và value của Hiformer (ví dụ V, được hiển thị trong Hình 3).

Bảng 2: Xấp xỉ rank thấp cho Hiformer.

| Mô Hình Hiformer | Số Tham Số | AUC | Độ Trễ |
|------------------|-------------|-----|--------|
| xấp xỉ rank thấp | 16.68M | 0.7875 | 1.52 |
| không xấp xỉ rank thấp | 59.95M | 0.7882 | 3.35 |

### 4.5 Độ Nhạy Tham Số (Q4)
Vì một trong những thách thức là giảm độ trễ phục vụ, chúng tôi cũng quan tâm đến độ nhạy tham số của mô hình như trong RQ4. Chính thức hơn, chúng tôi nhắm đến trả lời câu hỏi sau:

Như chúng tôi đã thảo luận trước đó trong Phần 4, thay vì sử dụng trực tiếp d_k=d/H và d_v=d/H, chúng tôi thấy rằng lựa chọn d_k và d_v thể hiện những đánh đổi nhất định giữa hiệu suất mô hình và độ trễ phục vụ cho mô hình HeteroAtt và Hiformer.

Trước hết, đối với Hd_k trong HeteroAtt trong Hình 5(a), sau khi giảm Hd_k từ d=128 xuống 64, không có mất mát chất lượng mô hình. Tuy nhiên, chúng ta có thể thu được cải thiện độ trễ miễn phí (3%). Nếu chúng ta tiếp tục giảm Hd_k, có mất mát chất lượng mô hình đáng kể. Do đó, chúng tôi chọn Hd_k=64, như được làm nổi bật bởi đường màu xám. So sánh, việc tăng Hd_k cho mô hình HeteroAtt mang lại cải thiện chất lượng mô hình, nhưng cũng tăng độ trễ đáng kể (trong Hình 5(b)). Để đánh đổi chất lượng và độ trễ, chúng tôi đặt Hd_v=256.

Tương tự, chúng tôi quan sát trong Hình 5(c), đối với mô hình Hiformer, việc giảm Hd_k xuống 64 không mất chất lượng, nhưng không giống như mô hình HeteroAtt, gần như không có lợi ích độ trễ. Điều này do xấp xỉ rank thấp trong phép chiếu QK, đặc biệt chúng tôi đặt rank r_k=256, tương đối nhỏ, so với r_v=1024. Theo đó, phép chiếu query chi phối chi phí tính toán. Với Hd_v tăng, độ trễ tăng đáng kể, được hiển thị trong Hình 5(d).

Quan sát này về đánh đổi chất lượng mô hình và độ trễ của Hd_v cung cấp tính độc đáo của mô hình HeteroAtt và Hiformer. Nó có thể phục vụ như một công cụ để điều chỉnh năng lực mô hình với chi phí độ trễ. Ví dụ, khi số lượng yêu cầu trên truy vấn tương đối nhỏ, chúng ta có thể đạt hiệu suất mô hình tốt hơn bằng cách tăng d_v. Công cụ như vậy có thể không có sẵn cho các mô hình SOTA khác, như DCN [46].

![Hình 5: Độ Nhạy Tham Số. Với d_k và d_v tăng, lợi ích chất lượng mô hình bão hòa; tuy nhiên, chi phí suy luận mô hình tăng mạnh. Các đường chấm đen đánh dấu các giá trị d_k, d_v được chọn, mang lại đánh đổi chất lượng và chi phí suy luận tốt nhất.]

### 4.6 Kiểm Tra A/B Trực Tuyến
Ngoài các thí nghiệm offline mở rộng, chúng tôi cũng đã tiến hành thí nghiệm kiểm tra A/B trực tuyến với khung mô hình đề xuất. Với mỗi nhóm control và treatment, có 1% người dùng được chọn ngẫu nhiên, nhận gợi ý item dựa trên các thuật toán xếp hạng khác nhau. Để hiểu cải thiện chúng tôi đạt được thông qua học tương tác đặc trưng dị nhất, mô hình baseline đang tận dụng mô hình Transformer một lớp để nắm bắt tương tác đặc trưng. Chúng tôi thu thập các chỉ số tương tác của người dùng trong 10 ngày, như được hiển thị trong Bảng 3. Mô hình HeteroAtt một lớp vượt trội đáng kể so với mô hình Transformer một lớp, phù hợp với phân tích offline của chúng tôi. Ngoài ra, hai

mô hình HeteroAtt lớp cải thiện thêm so với mô hình một lớp. Hiformer hoạt động tốt nhất trong tất cả các mô hình, bao gồm cả mô hình DCN. Quan sát như vậy cho thấy rằng các kiến trúc dựa trên Transformer có thể vượt trội hơn các mô hình tương tác đặc trưng SOTA. Chúng tôi đã triển khai thành công mô hình Hiformer vào production.

Bảng 3: Các chỉ số tương tác trực tuyến và độ trễ phục vụ của các mô hình khác nhau. * chỉ ý nghĩa thống kê.

| Mô Hình | Số Lớp | Chỉ Số Tương Tác |
|---------|---------|------------------|
| Transformer | 1 | +0.00% |
| HeteroAtt (ours) | 1 | +1.27%* |
| HeteroAtt (ours) | 2 | +2.33%* |
| Hiformer (ours) | 1 | +2.66%* |
| DCN | 1 | +2.20%* |

## 5 CÔNG TRÌNH LIÊN QUAN
Trước thời đại deep learning, người ta thường chế tác thủ công các cross feature và thêm chúng vào các mô hình logistic regression để cải thiện hiệu suất. Một số cũng tận dụng các mô hình dựa trên decision tree [11] để học feature cross. Sau đó, sự phát triển của kỹ thuật embedding đã dẫn đến thiết kế Factorization Machines (FMs) [34], được đề xuất để mô hình hóa tương tác đặc trưng bậc hai với inner product của hai vector tiềm ẩn.

Gần đây, DNNs đã trở thành xương sống của nhiều mô hình trong công nghiệp. Để cải thiện hiệu suất của học feature cross, nhiều công trình mô hình hóa tương tác đặc trưng một cách tường minh bằng cách thiết kế hàm g(x_i,x_j) trong khi tận dụng các feature cross ẩn được học bởi DNNs. Mô hình Wide & deep [3] kết hợp mô hình DNN với thành phần wide bao gồm các cross của raw feature. Kể từ đó, nhiều công trình đã được đề xuất để tự động hóa công việc feature cross thủ công của wide & deep bằng cách giới thiệu các phép toán giống FM (inner product) hoặc Hadamard product. DeepFM [8] và DLRM [29] áp dụng FM trong mô hình, Neural FM [10] tổng quát hóa FM bằng cách kết hợp Hadamard product, PNN [32] cũng sử dụng inner product và outer product để nắm bắt tương tác đặc trưng theo cặp. Tuy nhiên, các phương pháp này chỉ có thể mô hình hóa các feature cross lên đến bậc hai. Cũng có công trình có thể mô hình hóa tương tác đặc trưng bậc cao hơn. Deep Cross Network (DCN) [45,46] thiết kế mạng cross để học tương tác đặc trưng bậc cao một cách tường minh nơi bậc tương tác tăng với độ sâu lớp. DCN-V2 [46] làm cho DCN biểu đạt hơn và thực tế trong các thiết lập công nghiệp quy mô lớn. xDeepFM [26] cũng cải thiện khả năng biểu đạt của mạng cross trong DCN và dựa vào Hadamard product để nắm bắt feature cross bậc cao.

Một hướng nghiên cứu khác tận dụng cơ chế attention trong các mô hình Transformer để nắm bắt tương tác đặc trưng. AutoInt [40] được đề xuất để nắm bắt tương tác đặc trưng thông qua lớp multi-head self-attention. Tuy nhiên, vì Transformer được thiết kế để mô hình hóa các mối quan hệ độc lập với bối cảnh, như text token, việc áp dụng trực tiếp lớp multi-head self-attention cho tương tác đặc trưng có khả năng biểu đạt mô hình rất hạn chế. Tương tự, [25] dựa trên cơ chế attention với biến đổi cross-product để nắm bắt attention phân cấp, mà không cung cấp nhận thức đặc trưng và căn chỉnh ngữ nghĩa.

Công trình Heterogeneous Graph Transformer [13] liên quan đến chúng tôi, nơi các loại node được xem xét trong lớp heterogeneous mutual attention. Mặc dù chúng tôi chia sẻ một số điểm tương đồng trong động lực, công trình của chúng tôi khác biệt đáng kể vì mô hình Hiformer

được đề xuất để nắm bắt tương tác đặc trưng cho hệ thống gợi ý quy mô web, và thiết kế lớp attention Hiformer có khả năng biểu đạt nhiều hơn. Công trình field-aware factorization machine [18] cũng liên quan. Trong [18], nhận thức đặc trưng của học tương tác đặc trưng được thực hiện thông qua các bảng lookup embedding riêng lẻ, trong khi các phương pháp đề xuất của chúng tôi thông qua biến đổi thông tin trong lớp attention dị nhất. PNN [33] tận dụng kernel Factorization Machine để xem xét động lực đặc trưng trong học tương tác đặc trưng, chia sẻ điểm tương đồng với lớp attention dị nhất. Tuy nhiên, lớp attention dị nhất dựa trên attention trong khi PNN được dẫn xuất từ factorization machine. Ngoài ra, chúng tôi tận dụng phép chiếu Query và Key để học tương tác đặc trưng dị nhất trong khi PNN dựa trên các ma trận kernel. Hơn nữa, chúng tôi tăng thêm khả năng biểu đạt mô hình trong Hiformer với giảm chi phí.

## 6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong công trình này, chúng tôi đề xuất lớp attention dị nhất, là một sửa đổi đơn giản nhưng hiệu quả của lớp self-attention để cung cấp nhận thức đặc trưng cho học tương tác đặc trưng. Sau đó chúng tôi cải thiện thêm khả năng biểu đạt mô hình, và đề xuất Hiformer để cải thiện học tương tác đặc trưng. Chúng tôi cũng cải thiện hiệu suất phục vụ của Hiformer sao cho nó có thể đáp ứng yêu cầu độ trễ phục vụ cho các ứng dụng quy mô web. Thay đổi chính trong phương pháp của chúng tôi là xác định các mối quan hệ khác nhau giữa các đặc trưng khác nhau trong cơ chế attention, sao cho chúng ta có thể nắm bắt tính liên quan ngữ nghĩa dựa trên thông tin động trong các feature embedding được học. Cả kết quả thí nghiệm offline và online trên nền tảng dịch vụ phân phối kỹ thuật số hàng đầu thế giới chứng minh hiệu quả và hiệu suất của phương pháp của chúng tôi. Các kết quả thí nghiệm này cho thấy rằng mô hình dựa trên Transformer để học tương tác đặc trưng có thể hoạt động tốt hơn các phương pháp SOTA trong các ứng dụng quy mô web.

Công trình của chúng tôi phục vụ như một cầu nối để đưa các kiến trúc dựa trên Transformer vào các ứng dụng trong hệ thống gợi ý quy mô web. Đối với công việc tương lai, chúng tôi quan tâm đến cách các tiến bộ gần đây trong kiến trúc Transformer [9] trong các lĩnh vực khác, như NLP và CVR, có thể được chuyển sang lĩnh vực gợi ý. Với thành phần tương tác đặc trưng mới được giới thiệu trong bài báo này, nó cũng có thể được tận dụng trong neural architecture search [7] để cải thiện thêm hệ thống gợi ý.

## TÀI LIỆU THAM KHẢO
[1] Beutel, A., Covington, P., Jain, S., Xu, C., Li, J., Gatto, V., and Chi, E. H. Latent cross: Making use of context in recurrent recommender systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (2018), pp. 46–54.
[2] Caruana, R. Multitask learning. Machine learning 28, 1 (1997), 41–75.
[3] Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., Anderson, G., Corrado, G., Chai, W., Ispir, M., et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems (2016), pp. 7–10.
[4] Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems (2016), pp. 191–198.
[5] Davidson, J., Liebald, B., Liu, J., Nandy, P., Van Vleet, T., Gargi, U., Gupta, S., He, Y., Lambert, M., Livingston, B., et al. The youtube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems (2010), pp. 293–296.
[6] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[7] Elsken, T., Metzen, J. H., and Hutter, F. Neural architecture search: A survey. The Journal of Machine Learning Research 20, 1 (2019), 1997–2017.
[8] Guo, H., Tang, R., Ye, Y., Li, Z., and He, X. Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247 (2017).
[9] Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y., et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence (2022).
[10] He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web (2017), pp. 173–182.
[11] He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y., Atallah, A., Herbrich, R., Bowers, S., et al. Practical lessons from predicting clicks on ads at facebook. In Proceedings of the eighth international workshop on data mining for online advertising (2014), pp. 1–9.
[12] Hendrycks, D., and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016).
[13] Hu, Z., Dong, Y., Wang, K., and Sun, Y. Heterogeneous graph transformer. In Proceedings of the web conference 2020 (2020), pp. 2704–2710.
[14] Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In International conference on machine learning (2021), PMLR, pp. 4651–4664.
[15] Joglekar, M. R., Li, C., Chen, M., Xu, T., Wang, X., Adams, J. K., Khaitan, P., Liu, J., and Le, Q. V. Neural input search for large scale recommendation models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2020), pp. 2387–2397.
[16] Jouppi, N., Young, C., Patil, N., and Patterson, D. Motivation for and evaluation of the first tensor processing unit. ieee Micro 38, 3 (2018), 10–19.
[17] Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture (2017), pp. 1–12.
[18] Juan, Y., Zhuang, Y., Chin, W.-S., and Lin, C.-J. Field-aware factorization machines for ctr prediction. In Proceedings of the 10th ACM conference on recommender systems (2016), pp. 43–50.
[19] Kalyan, K. S., Rajasekharan, A., and Sangeetha, S. Ammus: A survey of transformer-based pretrained models in natural language processing. arXiv preprint arXiv:2108.05542 (2021).
[20] Kang, W.-C., and McAuley, J. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) (2018), IEEE, pp. 197–206.
[21] Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M. Transformers in vision: A survey. ACM computing surveys (CSUR) 54, 10s (2022), 1–41.
[22] LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature 521, 7553 (2015), 436–444.
[23] Li, D., Hu, B., Chen, Q., Wang, X., Qi, Q., Wang, L., and Liu, H. Attentive capsule network for click-through rate and conversion rate prediction in online advertising. Knowledge-Based Systems 211 (2021), 106522.
[24] Li, M., Liu, Z., Smola, A. J., and Wang, Y.-X. Difacto: Distributed factorization machines. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining (2016), pp. 377–386.
[25] Li, Z., Cheng, W., Chen, Y., Chen, H., and Wang, W. Interpretable click-through rate prediction through hierarchical attention. In Proceedings of the 13th International Conference on Web Search and Data Mining (2020), pp. 313–321.
[26] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., and Sun, G. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (2018), pp. 1754–1763.
[27] Lu, J., Wu, D., Mao, M., Wang, W., and Zhang, G. Recommender system application developments: a survey. Decision Support Systems 74 (2015), 12–32.
[28] Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Songhori, E., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Nazi, A., et al. A graph placement methodology for fast chip design. Nature 594, 7862 (2021), 207–212.
[29] Naumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sundaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.-J., Azzolini, A. G., et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091 (2019).
[30] Okura, S., Tagami, Y., Ono, S., and Tajima, A. Embedding-based news recommendation for millions of users. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (2017), pp. 1933–1942.
[31] Pan, X., Li, M., Zhang, J., Yu, K., Wen, H., Wang, L., Mao, C., and Cao, B. Metacvr: Conversion rate prediction via meta learning in small-scale recommendation scenarios. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (2022), pp. 2110–2114.
[32] Qu, Y., Cai, H., Ren, K., Zhang, W., Yu, Y., Wen, Y., and Wang, J. Product-based neural networks for user response prediction. In 2016 IEEE 16th International Conference on Data Mining (ICDM) (2016), IEEE, pp. 1149–1154.
[33] Qu, Y., Fang, B., Zhang, W., Tang, R., Niu, M., Guo, H., Yu, Y., and He, X. Product-based neural networks for user response prediction over multi-field categorical data. ACM Transactions on Information Systems (TOIS) 37, 1 (2018), 1–35.
[34] Rendle, S. Factorization machines. In 2010 IEEE International conference on data mining (2010), IEEE, pp. 995–1000.
[35] Resnick, P., and Varian, H. R. Recommender systems. Communications of the ACM 40, 3 (1997), 56–58.
[36] Schafer, J. B., Konstan, J., and Riedl, J. Recommender systems in e-commerce. In Proceedings of the 1st ACM conference on Electronic commerce (1999), pp. 158–166.
[37] Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020).
[38] Singh, P., Manure, A., Singh, P., and Manure, A. Introduction to tensorflow 2.0. Learn TensorFlow 2.0: Implement Machine Learning and Deep Learning Models with Python (2020), 1–24.
[39] Song, Q., Cheng, D., Zhou, H., Yang, J., Tian, Y., and Hu, X. Towards automated neural interaction discovery for click-through rate prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2020), pp. 945–955.
[40] Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., and Tang, J. Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (2019), pp. 1161–1170.
[41] Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management (2019), pp. 1441–1450.
[42] Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. ACM Computing Surveys 55, 6 (2022), 1–28.
[43] Tsang, M., Cheng, D., and Liu, Y. Detecting statistical interactions from neural network weights. arXiv preprint arXiv:1705.04977 (2017).
[44] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems 30 (2017).
[45] Wang, R., Fu, B., Fu, G., and Wang, M. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17. 2017, pp. 1–7.
[46] Wang, R., Shivanna, R., Cheng, D., Jain, S., Lin, D., Hong, L., and Chi, E. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems. In Proceedings of the Web Conference 2021 (2021), pp. 1785–1797.
[47] Yang, J., Yi, X., Zhiyuan Cheng, D., Hong, L., Li, Y., Xiaoming Wang, S., Xu, T., and Chi, E. H. Mixed negative sampling for learning two-tower neural networks in recommendations. In Companion Proceedings of the Web Conference 2020 (2020), pp. 441–447.
[48] Yu, H., Chen, C., Du, X., Li, Y., Rashwan, A., Hou, L., Jin, P., Yang, F., Liu, F., Kim, J., et al. Tensorflow model garden. GitHub (2020).
[49] Zhang, S., Yao, L., Sun, A., and Tay, Y. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys (CSUR) 52, 1 (2019), 1–38.
[50] Zhao, Z., Hong, L., Wei, L., Chen, J., Nath, A., Andrews, S., Kumthekar, A., Sathiamoorthy, M., Yi, X., and Chi, E. Recommending what video to watch next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender Systems (2019), pp. 43–51.
[51] Zhuang, H., Wang, X., Bendersky, M., and Najork, M. Feature transformation for neural ranking models. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval (2020), pp. 1649–1652.
