# 1706.03762.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/1706.03762.pdf
# Kích thước tệp: 2215244 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Với điều kiện ghi nhận đầy đủ tác giả, Google tại đây cấp phép tái sản xuất các bảng và hình ảnh trong bài báo này chỉ để sử dụng trong các tác phẩm báo chí hoặc học thuật.
Attention Is All You Need
Ashish Vaswani∗
Google Brain
avaswani@google.comNoam Shazeer∗
Google Brain
noam@google.comNiki Parmar∗
Google Research
nikip@google.comJakob Uszkoreit∗
Google Research
usz@google.com
Llion Jones∗
Google Research
llion@google.comAidan N. Gomez∗ †
University of Toronto
aidan@cs.toronto.eduŁukasz Kaiser∗
Google Brain
lukaszkaiser@google.com
Illia Polosukhin∗ ‡
illia.polosukhin@gmail.com
Tóm tắt
Các mô hình chuyển đổi chuỗi thống trị được dựa trên các mạng nơ-ron tích hồi hoặc tích chập phức tạp bao gồm một bộ mã hóa và một bộ giải mã. Các mô hình có hiệu suất tốt nhất cũng kết nối bộ mã hóa và bộ giải mã thông qua một cơ chế attention. Chúng tôi đề xuất một kiến trúc mạng mới đơn giản, Transformer, dựa hoàn toàn trên các cơ chế attention, loại bỏ hoàn toàn sự tái diễn và tích chập. Các thí nghiệm trên hai nhiệm vụ dịch máy cho thấy các mô hình này vượt trội về chất lượng đồng thời có khả năng song song hóa cao hơn và yêu cầu thời gian huấn luyện ít hơn đáng kể. Mô hình của chúng tôi đạt 28.4 BLEU trên nhiệm vụ dịch WMT 2014 Tiếng Anh-Tiếng Đức, cải thiện hơn 2 BLEU so với kết quả tốt nhất hiện có, bao gồm cả ensemble. Trên nhiệm vụ dịch WMT 2014 Tiếng Anh-Tiếng Pháp, mô hình của chúng tôi thiết lập một kỷ lục BLEU mới cho mô hình đơn lẻ là 41.8 sau khi huấn luyện trong 3.5 ngày trên tám GPU, một phần nhỏ chi phí huấn luyện của các mô hình tốt nhất từ tài liệu. Chúng tôi chỉ ra rằng Transformer tổng quát hóa tốt cho các nhiệm vụ khác bằng cách áp dụng thành công nó cho phân tích cú pháp thành phần tiếng Anh với cả dữ liệu huấn luyện lớn và hạn chế.
∗Đóng góp ngang nhau. Thứ tự liệt kê là ngẫu nhiên. Jakob đề xuất thay thế RNN bằng self-attention và bắt đầu nỗ lực đánh giá ý tưởng này. Ashish, cùng với Illia, đã thiết kế và triển khai các mô hình Transformer đầu tiên và tham gia quan trọng vào mọi khía cạnh của công việc này. Noam đề xuất scaled dot-product attention, multi-head attention và biểu diễn vị trí không tham số và trở thành người khác tham gia vào hầu hết mọi chi tiết. Niki đã thiết kế, triển khai, điều chỉnh và đánh giá vô số biến thể mô hình trong codebase gốc và tensor2tensor của chúng tôi. Llion cũng thử nghiệm với các biến thể mô hình mới, chịu trách nhiệm cho codebase ban đầu, và suy luận hiệu quả cùng trực quan hóa. Lukasz và Aidan đã dành vô số ngày dài thiết kế các phần khác nhau và triển khai tensor2tensor, thay thế codebase trước đó của chúng tôi, cải thiện đáng kể kết quả và tăng tốc đại quy mô nghiên cứu của chúng tôi.
†Công việc được thực hiện trong thời gian tại Google Brain.
‡Công việc được thực hiện trong thời gian tại Google Research.
Hội nghị lần thứ 31 về Hệ thống Xử lý Thông tin Nơ-ron (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023

--- TRANG 2 ---
1 Giới thiệu
Các mạng nơ-ron tái diễn, bộ nhớ ngắn hạn dài [13] và các mạng nơ-ron tái diễn có cổng [7] đặc biệt, đã được thiết lập vững chắc như các phương pháp tiên tiến trong mô hình hóa chuỗi và các vấn đề chuyển đổi như mô hình hóa ngôn ngữ và dịch máy [35,2,5]. Nhiều nỗ lực tiếp tục đẩy ranh giới của các mô hình ngôn ngữ tái diễn và kiến trúc mã hóa-giải mã [38, 24, 15].
Các mô hình tái diễn thường phân tích tính toán dọc theo các vị trí ký hiệu của chuỗi đầu vào và đầu ra. Căn chỉnh các vị trí với các bước trong thời gian tính toán, chúng tạo ra một chuỗi các trạng thái ẩn ht, như một hàm của trạng thái ẩn trước đó ht−1 và đầu vào cho vị trí t. Bản chất tuần tự này ngăn cản việc song song hóa trong các ví dụ huấn luyện, điều này trở nên quan trọng ở độ dài chuỗi lớn hơn, vì các ràng buộc bộ nhớ giới hạn việc ghép batch qua các ví dụ. Công việc gần đây đã đạt được những cải thiện đáng kể trong hiệu quả tính toán thông qua các thủ thuật phân tích nhân tử [21] và tính toán có điều kiện [32], đồng thời cũng cải thiện hiệu suất mô hình trong trường hợp sau. Tuy nhiên, ràng buộc cơ bản của tính toán tuần tự vẫn còn.
Các cơ chế attention đã trở thành một phần không thể thiếu của các mô hình mô hình hóa chuỗi và chuyển đổi hấp dẫn trong các nhiệm vụ khác nhau, cho phép mô hình hóa các phụ thuộc mà không quan tâm đến khoảng cách của chúng trong chuỗi đầu vào hoặc đầu ra [2,19]. Tuy nhiên, trong tất cả trừ một số ít trường hợp [27], các cơ chế attention như vậy được sử dụng kết hợp với mạng tái diễn.
Trong công việc này, chúng tôi đề xuất Transformer, một kiến trúc mô hình tránh tái diễn và thay vào đó dựa hoàn toàn vào cơ chế attention để vẽ ra các phụ thuộc toàn cục giữa đầu vào và đầu ra. Transformer cho phép song song hóa nhiều hơn đáng kể và có thể đạt trạng thái tiên tiến mới trong chất lượng dịch thuật sau khi được huấn luyện chỉ trong vòng mười hai giờ trên tám GPU P100.
2 Bối cảnh
Mục tiêu giảm tính toán tuần tự cũng tạo thành nền tảng của Extended Neural GPU [16], ByteNet [18] và ConvS2S [9], tất cả đều sử dụng các mạng nơ-ron tích chập như khối xây dựng cơ bản, tính toán các biểu diễn ẩn song song cho tất cả các vị trí đầu vào và đầu ra. Trong các mô hình này, số lượng phép toán cần thiết để liên kết tín hiệu từ hai vị trí đầu vào hoặc đầu ra tùy ý tăng theo khoảng cách giữa các vị trí, tuyến tính cho ConvS2S và logarithmic cho ByteNet. Điều này làm cho việc học các phụ thuộc giữa các vị trí xa trở nên khó khăn hơn [12]. Trong Transformer, điều này được giảm xuống một số phép toán không đổi, mặc dù với chi phí giảm độ phân giải hiệu quả do trung bình hóa các vị trí có trọng số attention, một hiệu ứng mà chúng tôi chống lại bằng Multi-Head Attention như được mô tả trong phần 3.2.
Self-attention, đôi khi được gọi là intra-attention, là một cơ chế attention liên kết các vị trí khác nhau của một chuỗi đơn để tính toán một biểu diễn của chuỗi đó. Self-attention đã được sử dụng thành công trong nhiều nhiệm vụ khác nhau bao gồm hiểu đọc, tóm tắt trừu tượng, suy luận văn bản và học biểu diễn câu độc lập với nhiệm vụ [4, 27, 28, 22].
Các mạng bộ nhớ end-to-end dựa trên cơ chế attention tái diễn thay vì tái diễn căn chỉnh chuỗi và đã được chứng minh là hoạt động tốt trên các nhiệm vụ trả lời câu hỏi ngôn ngữ đơn giản và mô hình hóa ngôn ngữ [34].
Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, Transformer là mô hình chuyển đổi đầu tiên dựa hoàn toàn vào self-attention để tính toán các biểu diễn của đầu vào và đầu ra mà không sử dụng RNN hoặc tích chập căn chỉnh chuỗi. Trong các phần tiếp theo, chúng tôi sẽ mô tả Transformer, động cơ thúc đẩy self-attention và thảo luận về các lợi thế của nó so với các mô hình như [17, 18] và [9].
3 Kiến trúc Mô hình
Hầu hết các mô hình chuyển đổi chuỗi nơ-ron cạnh tranh có cấu trúc mã hóa-giải mã [5,2,35]. Ở đây, bộ mã hóa ánh xạ một chuỗi đầu vào của các biểu diễn ký hiệu (x1, ..., xn) thành một chuỗi các biểu diễn liên tục z = (z1, ..., zn). Cho z, bộ giải mã sau đó tạo ra một chuỗi đầu ra (y1, ..., ym) của các ký hiệu từng phần tử một. Tại mỗi bước, mô hình là tự hồi quy [10], tiêu thụ các ký hiệu được tạo trước đó như đầu vào bổ sung khi tạo ra cái tiếp theo.
2

--- TRANG 3 ---
Hình 1: Transformer - kiến trúc mô hình.
Transformer tuân theo kiến trúc tổng thể này sử dụng các lớp self-attention xếp chồng và point-wise, fully connected cho cả bộ mã hóa và bộ giải mã, được thể hiện trong nửa trái và nửa phải của Hình 1, tương ứng.
3.1 Ngăn xếp Mã hóa và Giải mã
Bộ mã hóa: Bộ mã hóa được cấu tạo từ một ngăn xếp gồm N = 6 lớp giống hệt nhau. Mỗi lớp có hai lớp phụ. Đầu tiên là một cơ chế multi-head self-attention, và thứ hai là một mạng feed-forward fully connected đơn giản, theo vị trí. Chúng tôi sử dụng một kết nối dư [11] xung quanh mỗi lớp phụ trong hai lớp, theo sau bởi chuẩn hóa lớp [1]. Tức là, đầu ra của mỗi lớp phụ là LayerNorm(x + Sublayer(x)), trong đó Sublayer(x) là hàm được thực hiện bởi chính lớp phụ. Để tạo thuận lợi cho các kết nối dư này, tất cả các lớp phụ trong mô hình, cũng như các lớp embedding, tạo ra các đầu ra có kích thước dmodel = 512.
Bộ giải mã: Bộ giải mã cũng được cấu tạo từ một ngăn xếp gồm N = 6 lớp giống hệt nhau. Ngoài hai lớp phụ trong mỗi lớp mã hóa, bộ giải mã chèn một lớp phụ thứ ba, thực hiện multi-head attention trên đầu ra của ngăn xếp mã hóa. Tương tự như bộ mã hóa, chúng tôi sử dụng các kết nối dư xung quanh mỗi lớp phụ, theo sau bởi chuẩn hóa lớp. Chúng tôi cũng sửa đổi lớp phụ self-attention trong ngăn xếp giải mã để ngăn các vị trí attend đến các vị trí tiếp theo. Việc che này, kết hợp với việc các embedding đầu ra được offset một vị trí, đảm bảo rằng các dự đoán cho vị trí i chỉ có thể phụ thuộc vào các đầu ra đã biết tại các vị trí nhỏ hơn i.
3.2 Attention
Một hàm attention có thể được mô tả như việc ánh xạ một query và một tập hợp các cặp key-value thành một đầu ra, trong đó query, keys, values, và đầu ra đều là các vector. Đầu ra được tính toán như một tổng có trọng số
3

--- TRANG 4 ---
Scaled Dot-Product Attention
Multi-Head Attention
Hình 2: (trái) Scaled Dot-Product Attention. (phải) Multi-Head Attention bao gồm một số lớp attention chạy song song.
của các values, trong đó trọng số được gán cho mỗi value được tính toán bởi một hàm tương thích của query với key tương ứng.
3.2.1 Scaled Dot-Product Attention
Chúng tôi gọi attention đặc biệt của chúng tôi là "Scaled Dot-Product Attention" (Hình 2). Đầu vào bao gồm các queries và keys có kích thước dk, và values có kích thước dv. Chúng tôi tính toán tích vô hướng của query với tất cả keys, chia mỗi cái cho √dk, và áp dụng hàm softmax để có được trọng số trên các values.
Trong thực tế, chúng tôi tính toán hàm attention trên một tập hợp các queries đồng thời, được đóng gói cùng nhau thành một ma trận Q. Các keys và values cũng được đóng gói cùng nhau thành các ma trận K và V. Chúng tôi tính toán ma trận đầu ra như:
Attention(Q, K, V) = softmax(QKT/√dk)V (1)
Hai hàm attention được sử dụng phổ biến nhất là additive attention [2], và dot-product (multiplicative) attention. Dot-product attention giống hệt thuật toán của chúng tôi, ngoại trừ hệ số tỷ lệ 1/√dk. Additive attention tính toán hàm tương thích bằng cách sử dụng mạng feed-forward với một lớp ẩn đơn. Trong khi hai cách này tương tự về độ phức tạp lý thuyết, dot-product attention nhanh hơn và hiệu quả không gian hơn nhiều trong thực tế, vì nó có thể được thực hiện bằng cách sử dụng mã nhân ma trận được tối ưu hóa cao.
Trong khi với các giá trị nhỏ của dk, hai cơ chế hoạt động tương tự, additive attention vượt trội hơn dot product attention không có tỷ lệ cho các giá trị lớn hơn của dk [3]. Chúng tôi nghi ngờ rằng với các giá trị lớn của dk, các tích vô hướng tăng lớn về độ lớn, đẩy hàm softmax vào các vùng mà nó có gradient cực kỳ nhỏ4. Để chống lại hiệu ứng này, chúng tôi tỷ lệ các tích vô hướng bằng 1/√dk.
3.2.2 Multi-Head Attention
Thay vì thực hiện một hàm attention đơn với keys, values và queries có kích thước dmodel, chúng tôi thấy có lợi khi chiếu tuyến tính các queries, keys và values h lần với các phép chiếu tuyến tính khác nhau, đã học lên các kích thước dk, dk và dv, tương ứng. Trên mỗi phiên bản được chiếu này của queries, keys và values, chúng tôi sau đó thực hiện hàm attention song song, tạo ra các giá trị đầu ra có kích thước dv. Những cái này được nối và một lần nữa được chiếu, tạo ra các giá trị cuối cùng, như được miêu tả trong Hình 2.
Multi-head attention cho phép mô hình cùng lúc attend thông tin từ các không gian con biểu diễn khác nhau tại các vị trí khác nhau. Với một head attention đơn, việc trung bình hóa ngăn cản điều này.
MultiHead(Q, K, V) = Concat(head1, ..., headh)WO
trong đó headi = Attention(QWQi, KWKi, VWVi)
Trong đó các phép chiếu là các ma trận tham số WQi ∈ Rdmodel×dk, WKi ∈ Rdmodel×dk, WVi ∈ Rdmodel×dv
và WO ∈ Rhdv×dmodel.
Trong công việc này, chúng tôi sử dụng h = 8 lớp attention song song, hoặc heads. Cho mỗi cái trong số này, chúng tôi sử dụng dk = dv = dmodel/h = 64. Do kích thước giảm của mỗi head, tổng chi phí tính toán tương tự với single-head attention với kích thước đầy đủ.
3.2.3 Ứng dụng của Attention trong Mô hình của chúng tôi
Transformer sử dụng multi-head attention theo ba cách khác nhau:
• Trong các lớp "encoder-decoder attention", các queries đến từ lớp decoder trước đó, và các memory keys và values đến từ đầu ra của encoder. Điều này cho phép mỗi vị trí trong decoder attend trên tất cả các vị trí trong chuỗi đầu vào. Điều này bắt chước các cơ chế encoder-decoder attention điển hình trong các mô hình sequence-to-sequence như [38, 2, 9].
• Encoder chứa các lớp self-attention. Trong một lớp self-attention, tất cả các keys, values và queries đến từ cùng một nơi, trong trường hợp này là đầu ra của lớp trước trong encoder. Mỗi vị trí trong encoder có thể attend tất cả các vị trí trong lớp trước của encoder.
• Tương tự, các lớp self-attention trong decoder cho phép mỗi vị trí trong decoder attend tất cả các vị trí trong decoder lên đến và bao gồm vị trí đó. Chúng tôi cần ngăn luồng thông tin hướng trái trong decoder để bảo toàn tính chất tự hồi quy. Chúng tôi thực hiện điều này bên trong scaled dot-product attention bằng cách che (đặt thành −∞) tất cả các giá trị trong đầu vào của softmax tương ứng với các kết nối bất hợp pháp. Xem Hình 2.
3.3 Mạng Feed-Forward theo Vị trí
Ngoài các lớp phụ attention, mỗi lớp trong bộ mã hóa và giải mã của chúng tôi chứa một mạng feed-forward fully connected, được áp dụng cho mỗi vị trí một cách riêng biệt và giống hệt nhau. Điều này bao gồm hai phép biến đổi tuyến tính với kích hoạt ReLU ở giữa.
FFN(x) = max(0, xW1 + b1)W2 + b2 (2)
Trong khi các phép biến đổi tuyến tính giống nhau qua các vị trí khác nhau, chúng sử dụng các tham số khác nhau từ lớp này sang lớp khác. Một cách khác để mô tả điều này là như hai tích chập với kích thước kernel bằng 1.
Kích thước của đầu vào và đầu ra là dmodel = 512, và lớp trong có kích thước dff = 2048.
3.4 Embeddings và Softmax
Tương tự như các mô hình chuyển đổi chuỗi khác, chúng tôi sử dụng embeddings đã học để chuyển đổi các tokens đầu vào và tokens đầu ra thành các vector có kích thước dmodel. Chúng tôi cũng sử dụng phép biến đổi tuyến tính đã học thông thường và hàm softmax để chuyển đổi đầu ra decoder thành xác suất token tiếp theo được dự đoán. Trong mô hình của chúng tôi, chúng tôi chia sẻ cùng một ma trận trọng số giữa hai lớp embedding và phép biến đổi tuyến tính pre-softmax, tương tự như [30]. Trong các lớp embedding, chúng tôi nhân các trọng số đó với √dmodel.
5

--- TRANG 5 ---
Bảng 1: Độ dài đường dẫn tối đa, độ phức tạp mỗi lớp và số lượng tối thiểu các phép toán tuần tự cho các loại lớp khác nhau. n là độ dài chuỗi, d là kích thước biểu diễn, k là kích thước kernel của tích chập và r là kích thước của vùng lân cận trong self-attention hạn chế.
Loại Lớp | Độ phức tạp mỗi Lớp | Phép toán Tuần tự | Độ dài Đường dẫn Tối đa
Self-Attention | O(n²·d) | O(1) | O(1)
Recurrent | O(n·d²) | O(n) | O(n)
Convolutional | O(k·n·d²) | O(1) | O(logk(n))
Self-Attention (hạn chế) | O(r·n·d) | O(1) | O(n/r)
3.5 Mã hóa Vị trí
Vì mô hình của chúng tôi không chứa tái diễn và không có tích chập, để mô hình có thể sử dụng thứ tự của chuỗi, chúng tôi phải tiêm một số thông tin về vị trí tương đối hoặc tuyệt đối của các tokens trong chuỗi. Để làm điều này, chúng tôi thêm "mã hóa vị trí" vào embeddings đầu vào ở đáy của các ngăn xếp mã hóa và giải mã. Các mã hóa vị trí có cùng kích thước dmodel như các embeddings, để hai cái có thể được cộng lại. Có nhiều lựa chọn cho mã hóa vị trí, đã học và cố định [9].
Trong công việc này, chúng tôi sử dụng các hàm sine và cosine của các tần số khác nhau:
PE(pos,2i) = sin(pos/10000^(2i/dmodel))
PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))
trong đó pos là vị trí và i là kích thước. Tức là, mỗi kích thước của mã hóa vị trí tương ứng với một sinusoid. Các bước sóng tạo thành một cấp số nhân từ 2π đến 10000·2π. Chúng tôi chọn hàm này vì chúng tôi giả thuyết rằng nó sẽ cho phép mô hình dễ dàng học attend theo vị trí tương đối, vì với bất kỳ offset cố định k nào, PEpos+k có thể được biểu diễn như một hàm tuyến tính của PEpos.
Chúng tôi cũng thử nghiệm với việc sử dụng embeddings vị trí đã học [9] thay thế, và thấy rằng hai phiên bản tạo ra kết quả gần như giống hệt nhau (xem Bảng 3 hàng (E)). Chúng tôi chọn phiên bản sinusoidal vì nó có thể cho phép mô hình ngoại suy đến độ dài chuỗi dài hơn những cái gặp phải trong quá trình huấn luyện.
4 Tại sao Self-Attention
Trong phần này, chúng tôi so sánh các khía cạnh khác nhau của các lớp self-attention với các lớp tái diễn và tích chập được sử dụng phổ biến để ánh xạ một chuỗi có độ dài biến đổi của các biểu diễn ký hiệu (x1, ..., xn) thành một chuỗi khác có độ dài bằng nhau (z1, ..., zn), với xi, zi ∈ Rd, như một lớp ẩn trong một bộ mã hóa hoặc giải mã chuyển đổi chuỗi điển hình. Thúc đẩy việc sử dụng self-attention của chúng tôi, chúng tôi xem xét ba mong muốn.
Một là tổng độ phức tạp tính toán mỗi lớp. Cái khác là lượng tính toán có thể được song song hóa, được đo bằng số lượng tối thiểu các phép toán tuần tự cần thiết.
Thứ ba là độ dài đường dẫn giữa các phụ thuộc tầm xa trong mạng. Học các phụ thuộc tầm xa là một thách thức chính trong nhiều nhiệm vụ chuyển đổi chuỗi. Một yếu tố chính ảnh hưởng đến khả năng học các phụ thuộc như vậy là độ dài của các đường dẫn mà tín hiệu tiến và lùi phải đi qua trong mạng. Càng ngắn các đường dẫn này giữa bất kỳ sự kết hợp nào của các vị trí trong chuỗi đầu vào và đầu ra, càng dễ dàng học các phụ thuộc tầm xa [12]. Do đó, chúng tôi cũng so sánh độ dài đường dẫn tối đa giữa bất kỳ hai vị trí đầu vào và đầu ra nào trong các mạng được cấu thành từ các loại lớp khác nhau.
Như được ghi chú trong Bảng 1, một lớp self-attention kết nối tất cả các vị trí với một số lượng không đổi các phép toán được thực hiện tuần tự, trong khi một lớp tái diễn yêu cầu O(n) phép toán tuần tự. Về mặt độ phức tạp tính toán, các lớp self-attention nhanh hơn các lớp tái diễn khi độ dài chuỗi n nhỏ hơn kích thước biểu diễn d, điều này thường xảy ra với các biểu diễn câu được sử dụng bởi các mô hình tiên tiến trong dịch máy, như biểu diễn word-piece [38] và byte-pair [31]. Để cải thiện hiệu suất tính toán cho các nhiệm vụ liên quan đến chuỗi rất dài, self-attention có thể được hạn chế chỉ xem xét một vùng lân cận có kích thước r trong chuỗi đầu vào tập trung xung quanh vị trí đầu ra tương ứng. Điều này sẽ tăng độ dài đường dẫn tối đa lên O(n/r). Chúng tôi có kế hoạch điều tra phương pháp này sâu hơn trong công việc tương lai.
Một lớp tích chập đơn với chiều rộng kernel k < n không kết nối tất cả các cặp vị trí đầu vào và đầu ra. Việc làm như vậy yêu cầu một ngăn xếp gồm O(n/k) lớp tích chập trong trường hợp kernels liền kề, hoặc O(logk(n)) trong trường hợp dilated convolutions [18], tăng độ dài của các đường dẫn dài nhất giữa bất kỳ hai vị trí nào trong mạng. Các lớp tích chập thường đắt hơn các lớp tái diễn, với một hệ số k. Tuy nhiên, separable convolutions [6] giảm độ phức tạp đáng kể, xuống O(k·n·d + n·d²). Ngay cả với k = n, độ phức tạp của separable convolution bằng sự kết hợp của một lớp self-attention và một lớp feed-forward point-wise, phương pháp chúng tôi thực hiện trong mô hình.
Như lợi ích phụ, self-attention có thể tạo ra các mô hình dễ diễn giải hơn. Chúng tôi kiểm tra các phân phối attention từ các mô hình của chúng tôi và trình bày và thảo luận các ví dụ trong phụ lục. Không chỉ các head attention riêng lẻ rõ ràng học để thực hiện các nhiệm vụ khác nhau, nhiều cái dường như thể hiện hành vi liên quan đến cấu trúc cú pháp và ngữ nghĩa của các câu.
5 Huấn luyện
Phần này mô tả chế độ huấn luyện cho các mô hình của chúng tôi.
5.1 Dữ liệu Huấn luyện và Batching
Chúng tôi huấn luyện trên tập dữ liệu WMT 2014 Tiếng Anh-Tiếng Đức tiêu chuẩn gồm khoảng 4.5 triệu cặp câu. Các câu được mã hóa bằng cách sử dụng byte-pair encoding [3], có từ vựng nguồn-đích chung khoảng 37000 tokens. Cho Tiếng Anh-Tiếng Pháp, chúng tôi sử dụng tập dữ liệu WMT 2014 Tiếng Anh-Tiếng Pháp lớn hơn đáng kể gồm 36M câu và chia tokens thành từ vựng word-piece 32000 từ [38]. Các cặp câu được ghép batch theo độ dài chuỗi gần đúng. Mỗi batch huấn luyện chứa một tập hợp các cặp câu chứa khoảng 25000 tokens nguồn và 25000 tokens đích.
5.2 Phần cứng và Lịch trình
Chúng tôi huấn luyện các mô hình của chúng tôi trên một máy với 8 GPU NVIDIA P100. Cho các mô hình cơ bản của chúng tôi sử dụng các hyperparameters được mô tả trong suốt bài báo, mỗi bước huấn luyện mất khoảng 0.4 giây. Chúng tôi huấn luyện các mô hình cơ bản tổng cộng 100,000 bước hoặc 12 giờ. Cho các mô hình lớn của chúng tôi, (được mô tả ở dòng cuối của bảng 3), thời gian bước là 1.0 giây. Các mô hình lớn được huấn luyện trong 300,000 bước (3.5 ngày).
5.3 Optimizer
Chúng tôi sử dụng optimizer Adam [20] với β1 = 0.9, β2 = 0.98 và ε = 10^-9. Chúng tôi thay đổi tốc độ học trong suốt quá trình huấn luyện, theo công thức:
lrate = d_model^(-0.5) · min(step_num^(-0.5), step_num · warmup_steps^(-1.5)) (3)
Điều này tương ứng với việc tăng tốc độ học tuyến tính trong warmup_steps bước huấn luyện đầu tiên, và giảm nó sau đó tỷ lệ nghịch với căn bậc hai của số bước. Chúng tôi sử dụng warmup_steps = 4000.
5.4 Regularization
Chúng tôi sử dụng ba loại regularization trong quá trình huấn luyện:
7

--- TRANG 6 ---
Bảng 2: Transformer đạt điểm BLEU tốt hơn so với các mô hình tiên tiến trước đó trên các bài kiểm tra newstest2014 Tiếng Anh-Tiếng Đức và Tiếng Anh-Tiếng Pháp với một phần nhỏ chi phí huấn luyện.
Mô hình | BLEU | Chi phí Huấn luyện (FLOPs)
| EN-DE | EN-FR | EN-DE | EN-FR
ByteNet [18] | 23.75 | | |
Deep-Att + PosUnk [39] | | 39.2 | | 1.0·10²⁰
GNMT + RL [38] | 24.6 | 39.92 | 2.3·10¹⁹ | 1.4·10²⁰
ConvS2S [9] | 25.16 | 40.46 | 9.6·10¹⁸ | 1.5·10²⁰
MoE [32] | 26.03 | 40.56 | 2.0·10¹⁹ | 1.2·10²⁰
Deep-Att + PosUnk Ensemble [39] | | 40.4 | | 8.0·10²⁰
GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8·10²⁰ | 1.1·10²¹
ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7·10¹⁹ | 1.2·10²¹
Transformer (mô hình cơ bản) | 27.3 | 38.1 | 3.3·10¹⁸ |
Transformer (lớn) | 28.4 | 41.8 | 2.3·10¹⁹ |
Residual Dropout: Chúng tôi áp dụng dropout [33] vào đầu ra của mỗi lớp phụ, trước khi nó được cộng vào đầu vào lớp phụ và được chuẩn hóa. Ngoài ra, chúng tôi áp dụng dropout vào tổng của các embeddings và mã hóa vị trí trong cả ngăn xếp mã hóa và giải mã. Cho mô hình cơ bản, chúng tôi sử dụng tỷ lệ Pdrop = 0.1.
Label Smoothing: Trong quá trình huấn luyện, chúng tôi sử dụng label smoothing với giá trị εls = 0.1 [36]. Điều này làm tổn hại perplexity, vì mô hình học để không chắc chắn hơn, nhưng cải thiện độ chính xác và điểm BLEU.
6 Kết quả
6.1 Dịch máy
Trên nhiệm vụ dịch WMT 2014 Tiếng Anh-Tiếng Đức, mô hình transformer lớn (Transformer (lớn) trong Bảng 2) vượt trội hơn các mô hình tốt nhất được báo cáo trước đây (bao gồm cả ensembles) hơn 2.0 BLEU, thiết lập một điểm BLEU tiên tiến mới là 28.4. Cấu hình của mô hình này được liệt kê trong dòng cuối của Bảng 3. Việc huấn luyện mất 3.5 ngày trên 8 GPU P100. Ngay cả mô hình cơ bản của chúng tôi cũng vượt trội hơn tất cả các mô hình và ensembles đã công bố trước đây, với một phần nhỏ chi phí huấn luyện của bất kỳ mô hình cạnh tranh nào.
Trên nhiệm vụ dịch WMT 2014 Tiếng Anh-Tiếng Pháp, mô hình lớn của chúng tôi đạt điểm BLEU là 41.0, vượt trội hơn tất cả các mô hình đơn lẻ đã công bố trước đây, với ít hơn 1/4 chi phí huấn luyện của mô hình tiên tiến trước đó. Mô hình Transformer (lớn) được huấn luyện cho Tiếng Anh-Tiếng Pháp sử dụng tỷ lệ dropout Pdrop = 0.1, thay vì 0.3.
Cho các mô hình cơ bản, chúng tôi sử dụng một mô hình đơn được thu được bằng cách trung bình hóa 5 checkpoints cuối cùng, được ghi với khoảng thời gian 10 phút. Cho các mô hình lớn, chúng tôi trung bình hóa 20 checkpoints cuối cùng. Chúng tôi sử dụng beam search với kích thước beam là 4 và length penalty α = 0.6 [38]. Các hyperparameters này được chọn sau khi thử nghiệm trên tập phát triển. Chúng tôi đặt độ dài đầu ra tối đa trong quá trình suy luận là độ dài đầu vào + 50, nhưng kết thúc sớm khi có thể [38].
Bảng 2 tóm tắt kết quả của chúng tôi và so sánh chất lượng dịch thuật và chi phí huấn luyện với các kiến trúc mô hình khác từ tài liệu. Chúng tôi ước tính số lượng phép toán dấu phẩy động được sử dụng để huấn luyện một mô hình bằng cách nhân thời gian huấn luyện, số lượng GPU được sử dụng, và một ước tính về công suất dấu phẩy động độ chính xác đơn duy trì của mỗi GPU5.
6.2 Biến thể Mô hình
Để đánh giá tầm quan trọng của các thành phần khác nhau của Transformer, chúng tôi thay đổi mô hình cơ bản của chúng tôi theo nhiều cách khác nhau, đo lường sự thay đổi trong hiệu suất trên dịch Tiếng Anh-Tiếng Đức trên tập phát triển, newstest2013. Chúng tôi sử dụng beam search như được mô tả trong phần trước, nhưng không có trung bình hóa checkpoint. Chúng tôi trình bày những kết quả này trong Bảng 3.
Trong Bảng 3 hàng (A), chúng tôi thay đổi số lượng attention heads và các kích thước attention key và value, giữ lượng tính toán không đổi, như được mô tả trong Phần 3.2.2. Trong khi single-head attention kém hơn 0.9 BLEU so với thiết lập tốt nhất, chất lượng cũng giảm với quá nhiều heads.
Trong Bảng 3 hàng (B), chúng tôi quan sát thấy việc giảm kích thước attention key dk làm tổn hại chất lượng mô hình. Điều này gợi ý rằng việc xác định tương thích không dễ dàng và một hàm tương thích phức tạp hơn so với dot product có thể có lợi. Chúng tôi tiếp tục quan sát trong hàng (C) và (D) rằng, như mong đợi, các mô hình lớn hơn tốt hơn, và dropout rất hữu ích trong việc tránh over-fitting. Trong hàng (E), chúng tôi thay thế mã hóa vị trí sinusoidal của chúng tôi bằng embeddings vị trí đã học [9], và quan sát kết quả gần như giống hệt với mô hình cơ bản.
6.3 Phân tích Cú pháp Thành phần Tiếng Anh
Để đánh giá liệu Transformer có thể tổng quát hóa cho các nhiệm vụ khác, chúng tôi thực hiện các thí nghiệm trên phân tích cú pháp thành phần tiếng Anh. Nhiệm vụ này đặt ra những thách thức cụ thể: đầu ra phải tuân theo các ràng buộc cấu trúc mạnh và dài hơn đáng kể so với đầu vào. Hơn nữa, các mô hình RNN sequence-to-sequence đã không thể đạt được kết quả tiên tiến trong các chế độ dữ liệu nhỏ [37].
Chúng tôi huấn luyện một transformer 4 lớp với dmodel = 1024 trên phần Wall Street Journal (WSJ) của Penn Treebank [25], khoảng 40K câu huấn luyện. Chúng tôi cũng huấn luyện nó trong thiết lập bán giám sát, sử dụng các corpus high-confidence và BerkleyParser lớn hơn với khoảng 17M câu [37]. Chúng tôi sử dụng từ vựng 16K tokens cho thiết lập chỉ WSJ và từ vựng 32K tokens cho thiết lập bán giám sát.
Chúng tôi chỉ thực hiện một số ít thí nghiệm để chọn dropout, cả attention và residual (phần 5.4), tỷ lệ học và kích thước beam trên tập phát triển Phần 22, tất cả các tham số khác vẫn không thay đổi từ mô hình dịch cơ bản Tiếng Anh-Tiếng Đức. Trong quá trình suy luận, chúng tôi
9

--- TRANG 7 ---
Bảng 3: Các biến thể trên kiến trúc Transformer. Các giá trị không được liệt kê giống hệt với mô hình cơ bản. Tất cả các chỉ số đều trên tập phát triển dịch Tiếng Anh-Tiếng Đức, newstest2013. Các perplexities được liệt kê là per-wordpiece, theo byte-pair encoding của chúng tôi, và không nên so sánh với perplexities per-word.
N | dmodel | dff | h | dk | dv | Pdrop | εls | train steps | PPL (dev) | BLEU (dev) | params ×10⁶
cơ bản | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65
(A) | 1 | 512 | | | | | | | | 5.29 | 24.9 |
| 4 | | | | 128 | 128 | | | | 5.00 | 25.5 |
| 16 | | | | 32 | 32 | | | | 4.91 | 25.8 |
| 32 | | | | 16 | 16 | | | | 5.01 | 25.4 |
(B) | | | | 16 | | | | | | 5.16 | 25.1 | 58
| | | | 32 | | | | | | 5.01 | 25.4 | 60
(C) | 2 | | | | | | | | | 6.11 | 23.7 | 36
| 4 | | | | | | | | | 5.19 | 25.3 | 50
| 8 | | | | | | | | | 4.88 | 25.5 | 80
(D) | | 256 | | | 32 | 32 | | | | 5.75 | 24.5 | 28
| | 1024 | | | 128 | 128 | | | | 4.66 | 26.0 | 168
| | | 1024 | | | | | | | 5.12 | 25.4 | 53
| | | 4096 | | | | | | | 4.75 | 26.2 | 90
| | | | | | | 0.0 | | | 5.77 | 24.6 |
| | | | | | | 0.2 | | | 4.95 | 25.5 |
| | | | | | | | 0.0 | | 4.67 | 25.3 |
| | | | | | | | 0.2 | | 5.47 | 25.7 |
(E) | | | | | | | | | | | positional embedding thay vì sinusoids | 4.92 | 25.7 |
lớn | 6 | 1024 | 4096 | 16 | | | 0.3 | | 300K | 4.33 | 26.4 | 213
tăng độ dài đầu ra tối đa lên độ dài đầu vào + 300. Chúng tôi sử dụng kích thước beam là 21 và α = 0.3 cho cả thiết lập chỉ WSJ và bán giám sát.
Kết quả của chúng tôi trong Bảng 4 cho thấy rằng mặc dù thiếu điều chỉnh cụ thể cho nhiệm vụ, mô hình của chúng tôi hoạt động tốt một cách đáng ngạc nhiên, tạo ra kết quả tốt hơn tất cả các mô hình đã báo cáo trước đây ngoại trừ Recurrent Neural Network Grammar [8].
Trái ngược với các mô hình RNN sequence-to-sequence [37], Transformer vượt trội hơn BerkeleyParser [29] ngay cả khi chỉ huấn luyện trên tập huấn luyện WSJ gồm 40K câu.
7 Kết luận
Trong công việc này, chúng tôi đã trình bày Transformer, mô hình chuyển đổi chuỗi đầu tiên dựa hoàn toàn trên attention, thay thế các lớp tái diễn được sử dụng phổ biến nhất trong các kiến trúc mã hóa-giải mã bằng multi-headed self-attention.
Đối với các nhiệm vụ dịch thuật, Transformer có thể được huấn luyện nhanh hơn đáng kể so với các kiến trúc dựa trên các lớp tái diễn hoặc tích chập. Trên cả hai nhiệm vụ dịch WMT 2014 Tiếng Anh-Tiếng Đức và WMT 2014 Tiếng Anh-Tiếng Pháp, chúng tôi đạt được một trạng thái tiên tiến mới. Trong nhiệm vụ trước, mô hình tốt nhất của chúng tôi vượt trội hơn ngay cả tất cả các ensembles đã báo cáo trước đây.
Chúng tôi hào hứng về tương lai của các mô hình dựa trên attention và có kế hoạch áp dụng chúng cho các nhiệm vụ khác. Chúng tôi có kế hoạch mở rộng Transformer cho các vấn đề liên quan đến các phương thức đầu vào và đầu ra khác ngoài văn bản và điều tra các cơ chế attention cục bộ, hạn chế để xử lý hiệu quả các đầu vào và đầu ra lớn như hình ảnh, âm thanh và video. Làm cho việc tạo sinh ít tuần tự hơn là một mục tiêu nghiên cứu khác của chúng tôi.
Mã chúng tôi sử dụng để huấn luyện và đánh giá các mô hình của chúng tôi có sẵn tại https://github.com/tensorflow/tensor2tensor.
Lời cảm ơn: Chúng tôi biết ơn Nal Kalchbrenner và Stephan Gouws vì những bình luận, sửa chữa và cảm hứng hữu ích của họ.
Tài liệu tham khảo
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.
[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.
[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.
10

--- TRANG 8 ---
[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.
[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.
[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.
[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.
[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.
[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.
[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.
[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.
[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
11

--- TRANG 9 ---
[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.
[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.
[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.
[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.
[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.
12

--- TRANG 10 ---
Bảng 4: Transformer tổng quát hóa tốt cho phân tích cú pháp thành phần tiếng Anh (Kết quả trên Phần 23 của WSJ)
Parser | Huấn luyện | WSJ 23 F1
Vinyals & Kaiser el al. (2014) [37] | chỉ WSJ, discriminative | 88.3
Petrov et al. (2006) [29] | chỉ WSJ, discriminative | 90.4
Zhu et al. (2013) [40] | chỉ WSJ, discriminative | 90.4
Dyer et al. (2016) [8] | chỉ WSJ, discriminative | 91.7
Transformer (4 lớp) | chỉ WSJ, discriminative | 91.3
Zhu et al. (2013) [40] | bán giám sát | 91.3
Huang & Harper (2009) [14] | bán giám sát | 91.3
McClosky et al. (2006) [26] | bán giám sát | 92.1
Vinyals & Kaiser el al. (2014) [37] | bán giám sát | 92.1
Transformer (4 lớp) | bán giám sát | 92.7
Luong et al. (2015) [23] | đa nhiệm vụ | 93.0
Dyer et al. (2016) [8] | generative | 93.3
Trực quan hóa Attention
Input-Input Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Hình 3: Một ví dụ về cơ chế attention theo dõi các phụ thuộc tầm xa trong encoder self-attention ở lớp 5 trong tổng số 6. Nhiều attention heads attend đến một phụ thuộc xa của động từ 'making', hoàn thiện cụm từ 'making...more difficult'. Attentions ở đây chỉ được hiển thị cho từ 'making'. Các màu khác nhau đại diện cho các heads khác nhau. Tốt nhất khi xem với màu sắc.
13

--- TRANG 11 ---
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Hình 4: Hai attention heads, cũng ở lớp 5 trong tổng số 6, rõ ràng liên quan đến giải quyết anaphora. Trên: Attentions đầy đủ cho head 5. Dưới: Attentions riêng biệt chỉ từ từ 'its' cho attention heads 5 và 6. Lưu ý rằng các attentions rất sắc nét cho từ này.
14

--- TRANG 12 ---
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Hình 5: Nhiều attention heads thể hiện hành vi có vẻ liên quan đến cấu trúc của câu. Chúng tôi đưa ra hai ví dụ như vậy ở trên, từ hai heads khác nhau từ encoder self-attention ở lớp 5 trong tổng số 6. Các heads rõ ràng học để thực hiện các nhiệm vụ khác nhau.
15