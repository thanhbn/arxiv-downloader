# 2310.07911.pdf
# Đã được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2310.07911.pdf
# Kích thước tệp: 2058815 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đặt Một Chống Lại Nhiều: Tận Dụng Các Embedding Head Attention cho
Multi-head Attention Hiệu quả về Tham số
Huiyin Xue và Nikolaos Aletras
Khoa Khoa học Máy tính, Đại học Sheffield
Vương quốc Anh
{hxue12, n.aletras}@sheffield.ac.uk
Tóm tắt
Việc mở rộng các mô hình ngôn ngữ được huấn luyện trước đã
mang lại những cải thiện hiệu suất lớn trong các tác vụ xử lý
ngôn ngữ tự nhiên khác nhau nhưng đi kèm với chi phí lớn về
yêu cầu bộ nhớ. Được truyền cảm hứng từ các position embedding
trong transformer, chúng tôi nhằm đơn giản hóa và giảm dấu
chân bộ nhớ của cơ chế multi-head attention (MHA). Chúng tôi
đề xuất một module thay thế chỉ sử dụng một ma trận chiếu
được chia sẻ duy nhất và nhiều head embedding (MHE), tức là
một embedding cho mỗi head. Chúng tôi chứng minh thực nghiệm
rằng MHE attention của chúng tôi hiệu quả hơn đáng kể về mặt
bộ nhớ so với các cơ chế attention thay thế trong khi đạt được
tỷ lệ duy trì hiệu suất dự đoán cao so với MHA vanilla trên
một số tác vụ downstream. MHE attention chỉ yêu cầu một phần
nhỏ không đáng kể của các tham số bổ sung (3nd, trong đó n
là số lượng attention head và d là kích thước của head embedding)
so với single-head attention, trong khi MHA yêu cầu (3n²−3n)d²−3nd
tham số bổ sung.¹
1 Giới thiệu
Việc mở rộng các mô hình ngôn ngữ được huấn luyện trước (PLM)
nhằm nâng cao hiệu suất bằng cách tăng kích thước và khả năng
của chúng, dẫn đến các mô hình với số lượng tham số chưa từng
có (Kaplan et al., 2020; Chowdhery et al., 2022; Hoffmann et al.,
2022). Chỉ bằng cách tăng kích thước của PLM và dữ liệu huấn
luyện trước đã mang lại hiệu suất tối tân trên các tác vụ xử lý
ngôn ngữ tự nhiên (NLP) khác nhau (Devlin et al., 2019; Liu
et al., 2019; Clark et al., 2020; Raffel et al., 2020; Brown et al.,
2020; Clark et al., 2022a; Ouyang et al., 2022; Touvron et al.,
2023).
Tuy nhiên, việc theo đuổi phát triển PLM lớn hơn đi kèm với
các yêu cầu tính toán lớn. Điều này có những tác động trực tiếp
đến môi trường như lượng khí thải carbon lớn (Lacoste et al.,
2019; Strubell et al., 2019; Weidinger et al., 2022), mâu thuẫn
¹Code: https://github.com/HUIYINXUE/simpleMHE
Hình 1: Số lượng tham số cho một attention sub-layer và
số lượng attention head khác nhau sử dụng multi-head attention
MHA và multi-head embedding attention MHE của chúng tôi.
Chúng tôi cố định chiều của attention là 64, chỉ đếm các tham số
để chiếu queries, keys và values.
với các nguyên tắc phát triển trí tuệ nhân tạo xanh (Schwartz
et al., 2020). Hơn nữa, việc mở rộng có thể cản trở các nhà
nghiên cứu có quyền truy cập hạn chế vào tài nguyên tính toán
tham gia vào việc thúc đẩy lĩnh vực này (Schwartz et al., 2020).
Điều này dẫn đến bất bình đẳng, nơi chỉ một số ít đặc quyền
có thể đóng góp tích cực, có thể cản trở sự đa dạng và tính
bao gồm (Weidinger et al., 2022).
Xương sống của transformer (Vaswani et al., 2017) là module
multi-head attention (MHA) mở rộng single-head attention
(SHA) tiêu chuẩn được đề xuất bởi Cho et al. (2014). MHA áp
dụng một cơ chế attention (tức là head) nhiều lần cho cùng một
tập hợp queries, keys và values bằng cách sử dụng một tập hợp
tham số khác nhau (tức là ma trận chiếu) cho mỗi cái trong số
chúng. Điều này dẫn đến các module MHA với dấu chân bộ nhớ
lớn tăng theo số lượng layer và attention head mỗi layer trong
PLM (Devlin et al., 2019; Brown et al., 2020; Ouyang et al.,
2022; Touvron et al., 2023). Hình 1 cho thấy số lượng tham số
của một attention sublayer duy nhất tăng như thế nào với số
lượng attention head của nó.
Các nghiên cứu trước đây đã cố gắng giải quyết vấn đề này
bằng cách đề xuất chia sẻ ma trận chiếu hoặcarXiv:2310.07911v1  [cs.CL]  11 Oct 2023

--- TRANG 2 ---
loại bỏ chúng hoàn toàn để cải thiện hiệu quả tham số của MHA.
Lan et al. (2020a) đề xuất chia sẻ các tham số chiếu cho keys,
queries và values giữa các layer, trong khi Kitaev et al. (2020)
giới thiệu một phương pháp chia sẻ ma trận chiếu giữa keys và
values trong mỗi layer transformer. Ngoài ra, các phương pháp
tương tự sử dụng phương pháp multi-query attention sử dụng một
cặp ma trận chiếu toàn cục cho keys và values trong mỗi layer
(Shazeer, 2019; Chowdhery et al., 2022; Ainslie et al., 2023).
Hơn nữa, Yan et al. (2021) loại bỏ hoàn toàn các ma trận chiếu
và trực tiếp coi các trạng thái ẩn đầu vào như cả keys và values.
Theo một hướng khác, Lee-Thorp et al. (2022) đề xuất các mô
hình thay thế các khối attention bằng các khối token-mixture
(tức là sử dụng các phép biến đổi tuyến tính hoặc Fourier) chứa
ít tham số hơn hoặc không có tham số so với MHA.
Được truyền cảm hứng từ position embedding trong transformer
(Vaswani et al., 2017; Devlin et al., 2019), chúng tôi nhằm đơn
giản hóa và giảm dấu chân bộ nhớ của cơ chế MHA. Chúng tôi
đạt được điều này bằng cách chỉ sử dụng một ma trận chiếu duy
nhất cho mỗi keys, queries và values tương ứng được chia sẻ
giữa tất cả các attention head, và một embedding cho mỗi head
(MHE).
Các đóng góp của chúng tôi như sau:
•Chúng tôi đề xuất MHE, một module attention mới sử dụng
các ma trận chiếu được chia sẻ giữa các head được sửa đổi
bởi các embedding head tương ứng. Phương pháp của chúng
tôi tạo ra nhiều attention head chỉ yêu cầu một phần nhỏ
tham số bổ sung so với single-head attention.
•Chúng tôi chứng minh thực nghiệm rằng MHE attention của
chúng tôi hiệu quả hơn đáng kể về mặt tham số so với các
cơ chế attention thay thế trong khi đạt được tỷ lệ duy trì
hiệu suất dự đoán cao (tức là 92,9~98,7%) so với MHA trên
một số tác vụ downstream. MHE nhỏ hơn MHA (3n²−3n)d²−3nd
cho một attention sublayer duy nhất với n attention head
và chiều ẩn d mỗi head.
2 Nghiên cứu liên quan
2.1 Nén mô hình
Để làm cho PLM hiệu quả về bộ nhớ, các nghiên cứu trước đây
đã tập trung vào các phương pháp nén mô hình post-hoc sau đây
(Ganesh et al., 2021; Tay et al., 2022).
Lượng tử hóa Hubara et al. (2017) đề xuất biểu diễn trọng số
bằng cách sử dụng ít bit hơn để giảm yêu cầu bộ nhớ. Zadeh
et al. (2020) giới thiệu một phương pháp xác định các giá trị
ngoại lai trong trọng số và loại trừ chúng trong quá trình lượng
tử hóa. Một hướng khác liên quan đến các bước huấn luyện bổ
sung để điều chỉnh các trọng số được lượng tử hóa, tức là huấn
luyện nhận thức lượng tử hóa (Zafrir et al., 2019; Boo và Sung,
2020; Stock et al., 2020; Shen et al., 2020; Tambe et al., 2021;
Tao et al., 2022). Bai et al. (2022) phát triển một phương pháp
lượng tử hóa sau huấn luyện hiệu quả hơn để giảm thiểu lỗi
tái tạo phát sinh do lượng tử hóa.
Cắt tỉa Các phương pháp nén này loại bỏ hoàn toàn các phần
của mạng như trọng số gần bằng không (Gordon et al., 2020;
Mao et al., 2020; Chen et al., 2020) và trọng số di chuyển về
phía không trong quá trình fine-tuning (Sanh et al., 2020; Tambe
et al., 2021). Khác với việc hoạt động trên các trọng số riêng
lẻ, các nghiên cứu trước đây đã cố gắng loại bỏ các khối trọng
số có cấu trúc hoặc thậm chí các thành phần kiến trúc như
attention head và encoder layer (Fan et al., 2019; Prasanna
et al., 2020; Khetan và Karnin, 2020; Li et al., 2020a; Lin
et al., 2020; Tay et al., 2021).
Chưng cất tri thức Tập hợp các kỹ thuật này thường huấn
luyện một mô hình học sinh nhẹ để bắt chước đầu ra của một
PLM giáo viên lớn hơn (Sun et al., 2019; Li et al., 2020b; Jiao
et al., 2020; Sun et al., 2020; Li et al., 2021; Tahaei et al.,
2022). Theo một hướng tương tự, các PLM nhỏ hơn gần đây đã
được fine-tune trên văn bản được tạo bởi các PLM lớn hơn
(Chiang et al., 2023; Taori et al., 2023).
Phân tách ma trận trọng số Các nghiên cứu trước đây cũng đề
xuất thay thế các ma trận trọng số lớn bằng tích của hai ma trận
nhỏ hơn để giảm kích thước mô hình và bộ nhớ runtime. Phân
tách ma trận trọng số đã được áp dụng cho các layer tuyến tính
(Mao et al., 2020; Ben Noach và Goldberg, 2020), ma trận embedding
(Lan et al., 2020b; Tambe et al., 2021; Wang et al., 2022), và
các khối attention (Hu et al., 2022; Wang et al., 2022).
Nén ma trận embedding Cuối cùng, nhiều nỗ lực khác nhau đã
được giới thiệu để nén ma trận embedding trong quá trình huấn
luyện trước và fine-tuning (Xue et al., 2022; Clark et al., 2022b;
Xue và Aletras, 2022).

--- TRANG 3 ---
2.2 Cải thiện hiệu quả attention
Các nghiên cứu trước đây về việc làm cho attention hiệu quả
hơn bao gồm các nỗ lực hướng tới (1) tăng tốc tính toán cặp
đôi giữa các biểu diễn token; và (2) hiệu quả tham số.
Hiệu quả tính toán Mặc dù việc cải thiện hiệu quả tính toán
của attention nằm ngoài phạm vi bài báo của chúng tôi, chúng
tôi cung cấp một cái nhìn tổng quan ngắn gọn về các nghiên
cứu trước đây vì nó bổ sung cho hiệu quả tham số. Một phương
pháp để tăng tốc tính toán attention là giảm số lượng tính toán
tương tự giữa các biểu diễn ở các vị trí khác nhau bằng cách
sử dụng các cửa sổ cục bộ được xác định trước, stride cố định
hoặc động (Child et al., 2019; Zaheer et al., 2020; Beltagy
et al., 2020; Kitaev et al., 2020). Các phương pháp khác tận
dụng sự xấp xỉ của SoftMax để thay đổi thứ tự nhân ma trận,
dẫn đến độ phức tạp tính toán thấp hơn (Katharopoulos et al.,
2020; Choromanski et al., 2021; Schlag et al., 2021; Qin et al.,
2022). Các phương pháp liên quan theo hướng này đề xuất các
hàm kernel yêu cầu tham số bổ sung (Choromanski et al., 2021;
Wang et al., 2020). Cuối cùng, Dao et al. (2022) đề xuất các
cải tiến trong truy cập bộ nhớ GPU để tối ưu hóa và tăng tốc
tính toán MHA.
Hiệu quả bộ nhớ Lan et al. (2020a) giới thiệu một phương pháp
chia sẻ các tham số chiếu cho queries, keys và values giữa các
layer transformer. Hơn nữa, Kitaev et al. (2020) đề xuất chia
sẻ ma trận chiếu giữa keys và values trong mỗi layer. Ngoài
ra, các phương pháp khác sử dụng phương pháp multi-query
attention chia sẻ trọng số chiếu cho keys và values giữa các
head khác nhau (Shazeer, 2019; Chowdhery et al., 2022; Ainslie
et al., 2023), trong khi Yan et al. (2021) trực tiếp coi các
trạng thái ẩn đầu vào như cả keys và values. Theo một hướng
khác, Lee-Thorp et al. (2022) đề xuất thay thế các khối attention
bằng các khối token-mixture nhanh hơn bao gồm một vài tham số
hoặc không có tham số nào cả. Điều này bao gồm các phương
pháp như biến đổi tuyến tính hoặc Fourier trong khối token-mixture.
Tuy nhiên, các phương pháp này có xu hướng mang lại hiệu
suất dự đoán thấp hơn so với MHA.
3 Multiple Head Embeddings Attention
Được truyền cảm hứng từ các absolute position embedding
(Vaswani et al., 2017; Devlin et al., 2019) để phân biệt biểu
diễn của cùng một token trong các ngữ cảnh khác nhau, chúng
tôi đề xuất Multiple Head Embeddings (MHE) attention. MHE
sử dụng một ma trận chiếu 'seed' được chia sẻ sau đó được
kết hợp với các head embedding riêng biệt để tạo ra nhiều
attention head.
3.1 Multi-head Attention (MHA)
Trước tiên chúng tôi bắt đầu bằng cách định nghĩa chính thức
MHA. MHA bao gồm các ma trận chiếu khác nhau (WQ_i, WK_i,
WV_i ∈ R^(d_m×d_h), i = 1, ..., n, trong đó d_m là chiều của
biểu diễn đầu vào và d_h là chiều của n attention head) cho
queries (Q), keys (K) và values (V) mỗi head, tổng cộng 3×n.
Nó được tính như sau:
Q_i, K_i, V_i = XW^(Q,K,V)_i (1)
H_i = Att(Q_i, K_i, V_i) (2)
= SoftMax(Q_iK_i^T/√d_h)V_i (3)
Lưu ý rằng chúng tôi sử dụng scale-dot attention, nhưng phương
pháp của chúng tôi có thể được sử dụng với bất kỳ cơ chế attention
nào khác.
3.2 Ma trận chiếu seed
Không giống như MHA sử dụng các ma trận chiếu khác nhau
cho mỗi head, MHE attention chỉ sử dụng một ma trận chiếu
duy nhất cho mỗi queries, keys và values, W^Q, W^K, W^V ∈
R^(d_m×d_h). Các ma trận này được chia sẻ giữa tất cả các
attention head.
Chúng tôi thu được các chiếu query, key và values của chuỗi
đầu vào X như sau:
Q, K, V = XW^(Q,K,V) (4)
3.3 Attention Head Embeddings
Việc sử dụng ma trận chiếu seed cho Q, K, V tương đương với
một module single-head attention (SHA). Do đó, chúng tôi cần
một cơ chế để biến đổi các ma trận chiếu seed để thu được các
attention head khác nhau. Cho mục đích này, chúng tôi biểu
diễn mỗi attention head i bằng các head embedding cụ thể
e^Q_i, e^K_i, e^V_i ∈ R^d_h, i = 1, ..., n cho queries, key
và values. Các embedding này có dấu chân bộ nhớ nhỏ hơn đáng
kể so với việc sử dụng các ma trận chiếu khác nhau cho mỗi
head. Biểu diễn được ngữ cảnh hóa H_i của toàn bộ chuỗi
đầu vào X cho head i được tính như sau:
Q̃_i, K̃_i, Ṽ_i = ψ(Q; K; V, e^(Q,K,V)_i) (5)
H_i = Att(Q̃_i, K̃_i, Ṽ_i) (6)
trong đó ψ(·) là một hàm sửa đổi các ma trận query, key và
value với head embedding tương ứng e_i.

--- TRANG 4 ---
[Hình ảnh cho thấy hai sơ đồ: (a) MHA và (b) MHE, minh họa sự khác biệt giữa multi-head attention và multi-head embedding attention]
Hình 2: Multi-head attention (trái) yêu cầu 3×n ma trận chiếu
cho queries, keys và values (W^(Q,K,V)) trong đó n là số lượng
attention head. Multi-head embedding attention (phải) chỉ sử
dụng ba ma trận chiếu và 3×n head embedding.
3.4 Sửa đổi Queries, Keys và Values với Head Embeddings
Chúng tôi đề xuất hai biến thể MHE, một biến thể cộng và biến
thể kia nhân các head embedding với các ma trận chiếu seed.
MHE-ADD: Được động lực bởi absolute position embedding
(Devlin et al., 2019), chúng tôi sử dụng phép toán cộng trong
Phương trình 5, được biểu diễn là ψ(A,b) := A + b, trong đó
A ∈ {Q,K,V} và b ∈ {e^Q,e^K,e^V} tương ứng.
MHE-MUL: Tương tự, được động lực bởi rotary position embedding
(Su et al., 2021), MHE-MUL sử dụng phép nhân như phép toán
tích hợp trong Phương trình 5 là ψ(A,b) := A ⊙ (b + 1),
trong đó ⊙ biểu diễn tích Hadamard.²
Hình 2 cho thấy tổng quan về cơ chế MHE so với MHA.
4 Thiết lập thực nghiệm
4.1 Các cơ chế attention
Chúng tôi so sánh MHE attention của chúng tôi với các cơ chế
attention sau:³
²Chúng tôi cộng 1 để tránh các phần tử trong queries, keys và values
trở nên quá nhỏ trong quá trình khởi tạo.
³Chúng tôi cũng đã thử nghiệm với các mô hình token-mixture tuyến
tính và Fourier (Lee-Thorp et al., 2022) mang lại hiệu suất thấp hơn
đáng kể. Để xem kết quả đầy đủ của các phương pháp này, xem Phụ
lục A.
•Multi-head Attention (MHA): Đây là cơ chế multi-head
attention gốc (Vaswani et al., 2017; Devlin et al., 2019).
•Single-head Attention (SHA): Tương tự như MHA nhưng
chỉ sử dụng một attention head.
•EL-ATT: Được giới thiệu bởi Yan et al. (2021), biến thể
attention này hoàn toàn loại bỏ các ma trận chiếu cho tất
cả keys và values.
•MQA: Được giới thiệu bởi Shazeer (2019), phương pháp
này sử dụng các ma trận chiếu được chia sẻ cho keys và
values giữa tất cả các attention head. Lưu ý rằng các ma
trận chiếu khác nhau được sử dụng cho queries giữa các
head.
•SKV: Được giới thiệu bởi Kitaev et al. (2020), biến thể
attention này ép buộc keys và values chia sẻ cùng ma trận
chiếu trong mỗi module attention.
4.2 Dữ liệu
Chúng tôi thử nghiệm với một loạt tác vụ đa dạng bao gồm:
(1) hai benchmark hiểu ngôn ngữ tự nhiên tiêu chuẩn bằng tiếng
Anh, GLUE (Wang et al., 2018) và SUPER GLUE (Wang et al.,
2019); (2) hai benchmark hỏi đáp bằng tiếng Anh, SQUAD V1.1
(Rajpurkar et al., 2016) và SQUAD V2.0 (Rajpurkar et al.,
2018); (3) dịch máy WMT-14 Anh-Đức (Bojar et al., 2014);
và (4) hai tập dữ liệu mô hình hóa ngôn ngữ bằng tiếng Anh
WIKITEXT-103 (Merity et al., 2017) và PENN TREEBANK (Marcus
et al., 1993).
4.3 Mô hình
Chúng tôi kiểm tra tất cả các biến thể attention khác nhau trên
hai kiến trúc: (1) transformer chỉ-encoder (Devlin et al., 2019)
và (2) transformer encoder-decoder (Vaswani et al., 2017).
Chỉ-encoder Đối với GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD
V2.0, chúng tôi sử dụng kiến trúc BERT-base. Điều này bao
gồm 12 layer transformer, kích thước embedding 768, chiều
trạng thái ẩn 768, 12 attention head và độ dài chuỗi tối đa
512.
Chỉ-decoder Chúng tôi cũng kiểm tra mô hình chỉ-decoder sử
dụng kiến trúc GPT2-base trên WIKITEXT-103, PENN TREEBANK
và GLUE. GPT2-base bao gồm 12 layer transformer, kích thước
embedding 768, chiều trạng thái ẩn 768, 12 attention head và
độ dài chuỗi tối đa 512.

--- TRANG 5 ---
Encoder-decoder Đối với WMT-14, chúng tôi huấn luyện một
transformer encoder-decoder từ đầu. Nó bao gồm 12 layer (6
cho encoder và decoder tương ứng), kích thước embedding 512,
chiều trạng thái ẩn 512 và 8 attention-head và độ dài chuỗi
tối đa 100.
Chúng tôi đặt số lượng attention head là 1 cho tất cả các mô
hình SHA. Việc thử nghiệm với các mô hình lớn hơn và số lượng
attention head khác nhau nằm ngoài phạm vi bài báo của chúng
tôi và để lại cho nghiên cứu tương lai do quyền truy cập hạn
chế vào tài nguyên tính toán.
4.4 Chi tiết triển khai
Huấn luyện trước Chúng tôi huấn luyện trước tất cả các mô
hình trên English Wikipedia và BookCorpus (Zhu et al., 2015)
từ HuggingFace (Lhoest et al., 2021) trong tối đa 1M bước
với batch size 128. Chúng tôi chọn masked language modelling
làm mục tiêu huấn luyện trước. Đối với tất cả các mô hình,
chúng tôi sử dụng từ vựng WordPiece 30K (Devlin et al., 2019).
Fine-tuning và Huấn luyện Đối với GLUE, SUPER GLUE, SQUAD
V1.1 và SQUAD V2.0, chúng tôi fine-tune tất cả các mô hình
được huấn luyện trước tối đa 20 epoch với early stopping cố
định batch size là 32. Đối với mỗi tác vụ, chúng tôi sử dụng
năm seed khác nhau và báo cáo trung bình.
Chúng tôi huấn luyện mô hình encoder-decoder từ đầu trên tập
huấn luyện của tập dữ liệu dịch máy WMT-14 Anh-Đức tối đa
100K bước với batch size 256. WMT-14 chứa 4,5M cặp câu và
đánh giá trên tập kiểm tra của nó. Chúng tôi huấn luyện tokenizer
sử dụng byte-pair-encoding (Sennrich et al., 2016) với 37K
bước merge trên tập huấn luyện. Chúng tôi cho phép cả ngôn
ngữ nguồn và ngôn ngữ đích chia sẻ từ vựng. Chúng tôi sử dụng
một random seed và báo cáo trung bình trên năm epoch cuối.
Chúng tôi tối ưu hóa tất cả các mô hình bằng AdamW (Loshchilov
và Hutter, 2019).
Siêu tham số Chi tiết lựa chọn siêu tham số trong Phụ lục B.
Phần cứng Để huấn luyện trước, chúng tôi sử dụng bốn GPU
NVIDIA Tesla A100 và một GPU cho fine-tuning trên các tác vụ
downstream.
4.5 Đánh giá hiệu suất dự đoán
Đối với GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0, chúng
tôi sử dụng metric chính thức của mỗi tác vụ (xem Phụ lục A
để biết chi tiết về metric cho mỗi tác vụ). Chúng tôi báo cáo
điểm F1 cho SQUAD V1.1 và SQUAD V2.0. Chúng tôi sử dụng BLEU
để báo cáo hiệu suất trong tác vụ dịch máy WMT-14 Anh-Đức.
Chúng tôi sử dụng perplexity (PPL) để báo cáo hiệu suất tạo
sinh trên WIKITEXT-103 và PENN TREEBANK bằng cách cố định
độ dài stride là 256.
4.6 Đánh giá hiệu quả bộ nhớ
Hơn nữa, chúng tôi sử dụng các metric sau để đo lường và so
sánh hiệu quả bộ nhớ của MHE và các baseline.
•Tỷ lệ duy trì hiệu suất: Chúng tôi tính tỷ lệ giữa hiệu
suất dự đoán của mỗi cơ chế attention so với hiệu suất
baseline MHA upper-bound (càng cao càng tốt).
Đối với chỉ số trực tiếp (ví dụ: accuracy v.v.):
PRR = score_model / score_MHA
Đối với chỉ số nghịch đảo (ví dụ: perplexity v.v.):
PRR = 1 - (score_model - score_MHA) / score_MHA
•Độ đàn hồi hiệu suất của tham số: Được truyền cảm hứng
từ khái niệm độ đàn hồi trong kinh tế học (Bittermann, 1934),
đo lường khả năng phản ứng của một biến kinh tế (ví dụ:
nhu cầu đầu tư) với sự thay đổi của biến khác (ví dụ: lãi
suất), chúng tôi mở rộng nó để đo lường tỷ lệ sử dụng tham
số của mô hình đích so với SHA lower-bound. Độ đàn hồi
hiệu suất của tham số (PEoP) chỉ ra mức độ hiệu quả mà
tham số đóng góp vào hiệu suất dự đoán, so với SHA. Nó
được tính như sau:
Đối với chỉ số trực tiếp (ví dụ: accuracy v.v.):
PEoP = ((score_model/score_SHA) - 1) / ((params_model/params_SHA) - 1)
Đối với chỉ số nghịch đảo (ví dụ: perplexity v.v.):
PEoP = -((score_model/score_SHA) - 1) / ((params_model/params_SHA) - 1)
PEoP định lượng mức độ hiệu suất của mô hình có thể được
tăng cường với 1% tham số bổ sung so với mô hình baseline
(càng cao càng tốt).⁴
⁴Chúng tôi trừ 1 ở cả tử số và mẫu số, theo định nghĩa gốc của độ đàn hồi.

--- TRANG 6 ---
[Bảng 1: Kết quả của kiến trúc chỉ-encoder trên các tập dev GLUE, SUPER GLUE, SQUAD V1.1 và SQUAD V2.0 với tỷ lệ duy trì hiệu suất (PRR) và độ đàn hồi hiệu suất của tham số (PEoP) qua năm lần chạy. Giá trị in đậm biểu thị phương pháp hiệu suất tốt nhất trong mỗi benchmark.]

5 Kết quả
5.1 So sánh hiệu suất dự đoán
Bảng 1 trình bày kết quả trên GLUE, SUPER GLUE, SQUAD V1.1
và SQUAD V2.0 cho các biến thể MHE của chúng tôi và tất cả
các baseline. Trước tiên chúng tôi quan sát thấy rằng hiệu suất
của cả MHE-ADD và MHE-MUL đều có thể so sánh với MHA vanilla
trên hai benchmark phân loại văn bản (80,4, 80,6 so với 81,9
trên GLUE trung bình và 69,1, 69,6 so với 70,5 trên SUPER GLUE
trung bình) với tỷ lệ duy trì hiệu suất cao (PRR) từ 97,9% đến
98,7%. Trên các tác vụ hỏi đáp SQUAD V1.1 và SQUAD V2.0,
cả hai biến thể MHE cũng có tính cạnh tranh, với PRR cao hơn
93%.
Kết quả tương tự được quan sát trên tác vụ dịch máy WMT-14
Anh-Đức cho transformer encoder-decoder. Theo Bảng 3, MHE-ADD
và MHE-MUL đạt điểm BLEU lần lượt là 23,0 và 23,6. Hiệu suất
của MHE-MUL thấp không đáng kể so với MHA (24,8) trong khi
nhỏ hơn đáng kể.
Kết quả nhất quán cho transformer chỉ-decoder được hiển thị
trong Bảng 2. PRR cho MHE-ADD và MHE-MUL trên GLUE vẫn cao
(tức là 97,8% và 99,0%). Trong khi sử dụng các metric nội tại
để đánh giá, MHE-MUL dẫn đến perplexity 53,8 và 50,7 so với
43,0 và 44,3 cho MHA trên WIKITEXT-103 và PENN TREEBANK
tương ứng, cho thấy PRR ổn định cao hơn 74,9%.
Trong tất cả các tác vụ, MHE luôn vượt trội hơn SHA với biên
độ lớn chỉ với 0,03M tham số bổ sung, tức là 0,6~17,4. Ví dụ,
69,6 so với 67,1 trong SUPER GLUE, 72,3 so với 67,6 trong
SQUAD V2.0, 23,6 so với 22,5 trong WMT-14 và 62,0 so với
53,8 trong WIKITEXT-103 cho biến thể MHE-MUL. Chúng tôi cũng
lưu ý rằng các cơ chế attention MQA và SKV thường hoạt động
tốt hơn MHE, tuy nhiên chúng lớn hơn MHE 1,7 và 2,4 lần,
tức là 15,34M và 21,23M so với 8,88M tham số.
Đáng chú ý là MHE-MUL vượt trội hơn EL-ATT trên ba trong
năm benchmark, mặc dù có gần một nửa tham số trong module
attention.
5.2 So sánh hiệu quả bộ nhớ
Kết quả của chúng tôi cho đến nay cho thấy hiệu suất tăng
theo số lượng tham số cơ chế attention, điều này được mong
đợi. Tiếp theo, chúng tôi kiểm tra mức độ hiệu quả mà các
cơ chế attention khác nhau sử dụng tham số của chúng⁵. Bảng
1 và 3 cho thấy mức độ hiệu quả tham số của hai biến thể MHE
attention và tất cả các baseline, được đo bằng PEoP. Lưu ý
rằng điểm PEoP cho SHA không thể tính được vì nó được sử
dụng làm mô hình tham chiếu. Chúng tôi cũng báo cáo PRR sử
dụng MHA làm baseline để đầy đủ, tuy nhiên metric này không
tính đến kích thước mô hình.
Trước tiên chúng tôi quan sát trong Bảng 1 rằng cả MHE-ADD
và MHE-MUL đều đạt điểm PEoP cao nhất trên hai benchmark
hiểu ngôn ngữ tự nhiên (4,92, 5,53 trên GLUE, và 9,44, 12,07
trên SUPER GLUE) và hai tác vụ hỏi đáp (4,65, 13,19 trên SQUAD
V1.1, và 19,88, 22,25 trên SQUAD V2.0). Ngược lại, MHA vanilla
dẫn đến điểm PEoP thấp nhất trong tất cả các mô hình như mong
đợi, từ 0,02 đến 0,06. Điều này cho thấy tính không hiệu quả
bộ nhớ của MHA.
PEoP của EL-ATT và SKV nhẹ hơn tương tự như MHA (0,02) trên
GLUE trung bình, chỉ bằng 4‰ của MHE, cho thấy chúng kém
hiệu quả bộ nhớ hơn nhiều so với MHE.
Những phát hiện tương tự được quan sát trong WMT-14 cho các
mô hình encoder-decoder được mô tả trong Bảng 3. MHE-ADD và
MHE-MUL đạt điểm PEoP lần lượt là 20,0 và 27,9. Ngược lại,
điểm PEoP của MHA, EL-ATT, MQA và SKV gần bằng không (chỉ
bằng 0,1). Điều này có nghĩa là đầu tư thêm tham số vào các
module attention của chúng sẽ không mang lại lợi ích tương
xứng trong hiệu suất dự đoán. Ngay cả đối với SKV có kích
thước bằng một nửa MHA và đạt PRR cao, khi số lượng tham số
tăng 1%, điểm BLEU tăng không đáng kể 0,1%, trong khi phát
triển từ SHA. Tuy nhiên, với cùng số lượng tham số, MHE-MUL
kém hiệu quả bộ nhớ nhất của chúng tôi có thể cải thiện điểm
BLEU 11,0%. Tỷ lệ hoàn vốn như vậy lớn gấp 110 lần so với
SKV. Việc tận dụng các head embedding bằng cách chỉ thêm một
số lượng tham số không đáng kể sẽ cải thiện hiệu quả hiệu
suất dự đoán.
Chúng tôi tiếp tục quan sát thấy rằng MHE-ADD và MHE-MUL là
architecture-agnostic, đạt được hiệu quả bộ nhớ tương tự cho
mô hình chỉ-decoder trong Bảng 2. Cả MHE-ADD và MHE-MUL đều
đạt điểm PEoP cao nhất trên hai benchmark mô hình hóa ngôn
ngữ (41,29, 42,32 trên WIKITEXT-103 và 60,15 và 81,76 trên
PENN TREEBANK) và GLUE (2,18 và 5,92). Đồng thời, MHA không
thể hoạt động tốt trên GLUE và PENN TREEBANK với PEoP lần
lượt là 0,01 và 0,16. MHE-ADD và MHE-MUL cũng luôn vượt
trội hơn các biến thể attention hiệu quả khác (tức là EL-ATT,
MQA và SKV) 72~340 lần về PEoP trên ba benchmark.
Trong tất cả các tác vụ, MHE luôn vượt trội hơn MHA theo
bậc độ lớn về hiệu quả tham số. Chúng tôi cũng lưu ý rằng
EL-ATT, MQA và SKV chỉ dẫn đến điểm PEoP có cùng bậc độ
lớn như MHA. Điều này làm nổi bật việc sử dụng tham số vượt
trội hơn của các biến thể MHE attention, đạt được hiệu quả
bộ nhớ tối tân.
5.3 Độ phức tạp bộ nhớ lý thuyết
Bảng 4 trình bày độ phức tạp bộ nhớ lý thuyết và tổng số tham
số của hai MHE và các cơ chế attention baseline trong một
transformer sublayer duy nhất. Trước tiên, chúng ta thấy rằng
độ phức tạp bộ nhớ lý thuyết của MHA và các tham số hiệu quả
khác (EL-ATT, MQA và SKV) là bậc hai với số lượng attention
head, trong khi MHE của chúng tôi là hai biến thể duy nhất có
độ phức tạp tuyến tính với attention head tương tự như SHA.
Nhìn kỹ hơn vào cột bên phải trong Bảng 4, chúng ta quan sát
thấy số lượng tham số bổ sung của tất cả các biến thể attention
so với SHA có mối quan hệ bậc hai với cả số lượng n và chiều
attention head d, ngoại trừ hai biến thể MHE của chúng tôi.
MHE chỉ yêu cầu một phần tương đối nhỏ tham số bổ sung so
với SHA.

[Bảng 2: Kết quả của kiến trúc chỉ-decoder trên tập dev GLUE và tập test WIKITEXT-103, PENN TREEBANK với tỷ lệ duy trì hiệu suất (PRR) và độ đàn hồi hiệu suất của tham số (PEoP) qua năm lần chạy. Giá trị in đậm biểu thị phương pháp hiệu suất tốt nhất trong mỗi benchmark.]

⁵Để có báo cáo chi tiết về việc sử dụng bộ nhớ của các cơ chế attention khác nhau, xem Phụ lục C.

[Bảng 3: Điểm BLEU trên tác vụ dịch máy WMT-14 Anh sang Đức với tỷ lệ duy trì hiệu suất (PRR) và độ đàn hồi hiệu suất của tham số (PEoP). Giá trị in đậm biểu thị phương pháp hiệu suất tốt nhất trong mỗi benchmark.]

--- TRANG 7 ---
[Bảng 4: Độ phức tạp bộ nhớ về số lượng tham số trong mỗi attention sublayer, trong khi cố định chiều attention head là d. n biểu thị số lượng attention head. Để đơn giản hóa, chiều trạng thái ẩn d_m được đặt là nd. Projection cuối cùng để gộp attention head được loại trừ.]

5.4 Mở rộng số lượng tham số attention
Đi sâu hơn vào tác động của việc mở rộng đến dấu chân bộ nhớ,
chúng tôi hiển thị trong Hình 3 tổng số tham số cần thiết cho
một module attention duy nhất (ví dụ: trong một encoder layer).
Chúng tôi cố định chiều attention head là 64 thường được sử
dụng bởi BERT (Devlin et al., 2019), RoBERTa (Liu et al.,
2019), GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020)
và T5 (Raffel et al., 2020). Nhìn chung, chúng tôi lưu ý rằng
số lượng tham số trong MHA có thể đạt hơn 200M nếu sử dụng
128 attention head. Đồng thời, SKV, MQA và EL-ATT sẽ yêu cầu
2/3, 1/3 và 1/3 số lượng đó tương ứng. Ngược lại, MHE chỉ
chiếm 1% tham số MHA.
Hơn nữa, chúng tôi cũng trình bày trong Hình 4 tổng số tham
số yêu cầu giữa các biến thể attention khi xếp chồng 12, 24
và 48 layer cùng với 32 và 64 attention head tương ứng. Chúng
tôi cũng cố định chiều attention head là 64. Chúng ta có thể
quan sát, khi số lượng attention head đạt 64, MHA với 24 layer
đã chiếm hơn 1B tham số, trong khi EL-ATT và MQA đạt 0,8B
tham số với 48 layer. SKV mất 24 layer để đạt 0,8B tham số.
Tuy nhiên, tổng số tham số trong MHE attention không vượt quá
0,1B ngay cả khi mở rộng đến 48 layer với 64 attention head.
Cũng rõ ràng rằng việc mở rộng module attention đến 48 layer,
32 attention head và 12 layer cần số lượng tham số tương đối
cho MHA, EL-ATT, MQA hoặc SKV. Điều này cho thấy, các nhà
phát triển LLM phải đưa ra lựa chọn liệu có tăng gấp đôi số
lượng attention head hay cắt giảm số lượng layer xuống một
phần tư khi làm việc dưới ngân sách bộ nhớ chặt chẽ. Tuy nhiên,
MHE không gặp phải những vấn đề như vậy.
Hơn nữa, chúng tôi dự đoán những ước tính này cho mô hình GPT-3
phổ biến (Brown et al., 2020). Đây là mô hình chỉ-decoder với
96 decoder layer, 96 attention head mỗi layer, và chiều head
128. Module multi-head attention vanilla yêu cầu 43,48B tham
số khổng lồ. Tuy nhiên, sử dụng MHE attention, con số này có
thể được giảm đáng kể xuống 0,46B tham số, tức là giảm khoảng
98,9%.⁶ So sánh điều này với các biến thể attention hiệu quả
tham số khác như EL-ATT (14,50B tham số), MQA attention (14,80B
tham số), và SKV attention (28,99B tham số), rõ ràng rằng MHE
của chúng tôi cung cấp hiệu quả bộ nhớ tốt hơn. Điều này làm
cho nó trở thành một lựa chọn thay thế hấp dẫn cho các kịch
bản hạn chế bộ nhớ. Xem Phụ lục D để có nghiên cứu chi tiết
về tính mạnh mẽ của MHE đối với các thay đổi kích thước mô
hình (tức là mở rộng).
6 Thảo luận
MHA cho phép mô hình chú ý đến thông tin từ các không gian
con biểu diễn khác nhau tại các vị trí khác nhau (Vaswani et al.,
2017). Nó sử dụng các ma trận chiếu riêng biệt cho mỗi attention
head và tích hợp thông tin từ các không gian con biểu diễn khác
nhau này. Tuy nhiên, Vaswani et al. (2017) không khám phá
các phương pháp khác nhau để thực hiện biến đổi không gian
cho mỗi head.
Các nghiên cứu trước đây đã chỉ ra rằng các mô hình quá tham
số hóa có thể có chiều nội tại thấp. Do đó, việc biến đổi các
ma trận chiếu thành các ma trận hạng thấp nhỏ hơn thường không
làm hại nghiêm trọng hiệu suất dự đoán mô hình (Li et al.,
2018; Aghajanyan et al., 2020). Trong khi đó, phương pháp MHA
cổ điển cũng không áp đặt bất kỳ ràng buộc nào về tính trực
giao của các không gian con này trong quá trình huấn luyện
trước và fine-tuning. Các vector cột trong các ma trận chiếu
đó có thể có tính đồng tuyến cao, tức là các ma trận chiếu có
thể thiếu hạng. Kết quả là, cơ chế hoạt động bên trong của nó
có thể được hiểu đơn giản là việc đưa ra các mức độ biến thiên
vào biểu diễn được mã hóa của cùng một token tại cùng vị trí
giữa các head khác nhau.
Phương pháp MHE của chúng tôi có thể đạt được hiệu quả bộ
nhớ (tương tự SHA) cùng với PRR cao so với MHA bằng cách bắt
chước position embedding để biểu diễn các attention head khác
nhau.
Mặt khác, phép toán cộng trong MHE-ADD được sử dụng để biến
đổi keys, queries và values. Điều này có thể được xem như một
biến dạng nhỏ của không gian con thu được thông qua phép
chiếu, sau đó là phép quay. Đối với một biểu diễn đầu vào,
sự khác biệt giữa các queries, keys và values được chiếu và
được tiêm (tức là thông qua việc cộng head embedding) là một
vector hằng số giữa bất kỳ cặp head nào. Mặt khác, phương
pháp MHE-MUL sử dụng phép toán nhân, biến dạng và định hình
lại các không gian con keys, queries và values một cách tích
cực hơn. Head embedding trong MHE-MUL đóng vai trò như các
yếu tố tỷ lệ, tương ứng kéo dài mỗi chiều của biểu diễn đầu
vào. Do đó, sự khác biệt giữa keys, queries và values được
tạo bởi các head khác nhau cho cùng biểu diễn đầu vào, là một
vector song song với đầu vào được chiếu. Vector này phụ thuộc
vào đầu vào cụ thể, không giống như vector hằng số trong MHE-ADD.
Thú vị là, kết quả thực nghiệm của chúng tôi liên tục cho thấy
rằng phép toán nhân vượt trội hơn phép cộng trong đa số các
benchmark. Điều này chứng thực các phát hiện của một nghiên
cứu thực nghiệm trước đây bởi Su et al. (2021) so sánh rotary
position embedding (có phần tương tự với MHE-MUL) với absolute
position embedding (tương tự với MHE-ADD).
7 Kết luận
Chúng tôi đã đề xuất MHE attention sử dụng một ma trận chiếu
được chia sẻ duy nhất cùng với nhiều head embedding, để đơn
giản hóa và giảm dấu chân bộ nhớ của MHA. Kết quả thực nghiệm
của chúng tôi đã chứng minh rằng MHE attention thể hiện hiệu
quả bộ nhớ vượt trội so với các biến thể attention hiệu quả
bộ nhớ khác, trong khi đạt được tỷ lệ hiệu suất dự đoán cao
so với MHA trên các tác vụ downstream khác nhau. So với single-head
attention, MHA yêu cầu (3n²−3n)d² tham số cho n attention
head và dimensionality head d, trong khi MHE chỉ yêu cầu 3nd
không đáng kể. Đối với nghiên cứu tương lai, chúng tôi dự định
điều tra việc mở rộng các mô hình MHE và khám phá khả năng
ngôn ngữ học của chúng (Vulić et al., 2020; Koto et al., 2021).
Hạn chế
Chúng tôi chỉ thử nghiệm sử dụng các mô hình kích thước 'base'
mà không thử nghiệm với các kiến trúc lớn hơn, do quyền truy
cập hạn chế vào tài nguyên tính toán. Tương tự, chúng tôi không
thử nghiệm với các kiến trúc chỉ-decoder (Brown et al., 2020)
mà chúng tôi để lại cho nghiên cứu tương lai. Chúng tôi không
kết hợp phương pháp MHE với các phương pháp attention hiệu
quả tính toán với độ phức tạp tuyến tính, như Linformer (Wang
et al., 2020). Chúng tôi mong đợi rằng nó sẽ tăng tốc tính
toán MHE, nhưng nó nằm ngoài phạm vi bài báo của chúng tôi.
Lời cảm ơn
Chúng tôi muốn cảm ơn Constantinos Karouzos, Miles Williams
và các nhà phản biện ẩn danh vì phản hồi vô giá của họ.

[Hình 3: Số lượng tham số mỗi attention sublayer, trong khi mở rộng số lượng attention head trong các biến thể attention khác nhau. Chúng tôi cố định chiều attention là 64.]

⁶Sẽ tuyệt vời nếu báo cáo kết quả bằng cách huấn luyện trước mô hình GPT-3 MHE của riêng chúng tôi, tuy nhiên điều này là không thể với tài nguyên tính toán khiêm tốn mà chúng tôi có.

--- TRANG 8 ---
[Hình 4: Tổng số tham số trong attention sub-layers, trong khi mở rộng số lượng attention layer đến 12, 24 và 48 với 32 attention head và 64 attention head tương ứng. Chúng tôi cố định chiều attention là 64.]

Tài liệu tham khảo
Armen Aghajanyan, Luke Zettlemoyer, và Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.

Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, và Sumit Sanghai. 2023. Colt5: Faster long-range transformers with conditional computation.

Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, và Michael R Lyu. 2022. Towards efficient post-training quantization of pre-trained language models. Advances in Neural Information Processing Systems, 35:1405–1418.

Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Matan Ben Noach và Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889, Suzhou, China. Association for Computational Linguistics.

Henry J Bittermann. 1934. Elasticity of supply. The American Economic Review, pages 417–429.

Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, và Aleš Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA. Association for Computational Linguistics.

Yoonho Boo và Wonyong Sung. 2020. Fixed-point optimization of transformer neural network. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1753–1757. IEEE.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và Michael Carbin. 2020. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834–15846.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023).

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, và Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111, Doha, Qatar. Association for Computational Linguistics.

Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, và Adrian Weller. 2021. Rethinking attention with performers. In International Conference on Learning Representations.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Jonathan H. Clark, Dan Garrette, Iulia Turc, và John Wieting. 2022a. Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation. Transactions of the Association for Computational Linguistics, 10:73–91.

Jonathan H. Clark, Dan Garrette, Iulia Turc, và John Wieting. 2022b. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:73–91.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, và Christopher D. Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Angela Fan, Edouard Grave, và Armand Joulin. 2019. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations.

--- TRANG 9 ---
tích hợp với các nguyên tắc phát triển trí tuệ nhân tạo xanh
(Schwartz et al., 2020). Hơn nữa, việc mở rộng có thể cản
trở các nhà nghiên cứu có quyền truy cập hạn chế vào tài nguyên
tính toán tham gia vào việc thúc đẩy lĩnh vực này (Schwartz
et al., 2020). Điều này dẫn đến bất bình đẳng, nơi chỉ một
số ít đặc quyền có thể đóng góp tích cực, có thể cản trở sự
đa dạng và tính bao gồm (Weidinger et al., 2022).

Các nghiên cứu trước đây đã chỉ ra rằng các mô hình quá tham
số hóa có thể có chiều nội tại thấp. Do đó, việc biến đổi các
ma trận chiếu thành các ma trận hạng thấp nhỏ hơn thường không
làm hại nghiêm trọng hiệu suất dự đoán mô hình (Li et al.,
2018; Aghajanyan et al., 2020). Trong khi đó, phương pháp MHA
cổ điển cũng không áp đặt bất kỳ ràng buộc nào về tính trực
giao của các không gian con này trong quá trình huấn luyện
trước và fine-tuning. Các vector cột trong các ma trận chiếu
đó có thể có tính đồng tuyến cao, tức là các ma trận chiếu có
thể thiếu hạng. Kết quả là, cơ chế hoạt động bên trong của nó
có thể được hiểu đơn giản là việc đưa ra các mức độ biến thiên
vào biểu diễn được mã hóa của cùng một token tại cùng vị trí
giữa các head khác nhau.

Phương pháp MHE của chúng tôi có thể đạt được hiệu quả bộ
nhớ (tương tự SHA) cùng với PRR cao so với MHA bằng cách bắt
chước position embedding để biểu diễn các attention head khác
nhau.

Mặt khác, phép toán cộng trong MHE-ADD được sử dụng để biến
đổi keys, queries và values. Điều này có thể được xem như một
biến dạng nhỏ của không gian con thu được thông qua phép
chiếu, sau đó là phép quay. Đối với một biểu diễn đầu vào,
sự khác biệt giữa các queries, keys và values được chiếu và
được tiêm (tức là thông qua việc cộng head embedding) là một
vector hằng số giữa bất kỳ cặp head nào. Mặt khác, phương
pháp MHE-MUL sử dụng phép toán nhân, biến dạng và định hình
lại các không gian con keys, queries và values một cách tích
cực hơn. Head embedding trong MHE-MUL đóng vai trò như các
yếu tố tỷ lệ, tương ứng kéo dài mỗi chiều của biểu diễn đầu
vào. Do đó, sự khác biệt giữa keys, queries và values được
tạo bởi các head khác nhau cho cùng biểu diễn đầu vào, là một
vector song song với đầu vào được chiếu. Vector này phụ thuộc
vào đầu vào cụ thể, không giống như vector hằng số trong MHE-ADD.

Thú vị là, kết quả thực nghiệm của chúng tôi liên tục cho thấy
rằng phép toán nhân vượt trội hơn phép cộng trong đa số các
benchmark. Điều này chứng thực các phát hiện của một nghiên
cứu thực nghiệm trước đây bởi Su et al. (2021) so sánh rotary
position embedding (có phần tương tự với MHE-MUL) với absolute
position embedding (tương tự với MHE-ADD).

7 Kết luận
Chúng tôi đã đề xuất MHE attention sử dụng một ma trận chiếu
được chia sẻ duy nhất cùng với nhiều head embedding, để đơn
giản hóa và giảm dấu chân bộ nhớ của MHA. Kết quả thực nghiệm
của chúng tôi đã chứng minh rằng MHE attention thể hiện hiệu
quả bộ nhớ vượt trội so với các biến thể attention hiệu quả
bộ nhớ khác, trong khi đạt được tỷ lệ hiệu suất dự đoán cao
so với MHA trên các tác vụ downstream khác nhau. So với single-head
attention, MHA yêu cầu (3n²−3n)d² tham số cho n attention
head và dimensionality head d, trong khi MHE chỉ yêu cầu 3nd
không đáng kể. Đối với nghiên cứu tương lai, chúng tôi dự định
điều tra việc mở rộng các mô hình MHE và khám phá khả năng
ngôn ngữ học của chúng (Vulić et al., 2020; Koto et al., 2021).

Hạn chế
Chúng tôi chỉ thử nghiệm sử dụng các mô hình kích thước 'base'
mà không thử nghiệm với các kiến trúc lớn hơn, do quyền truy
cập hạn chế vào tài nguyên tính toán. Tương tự, chúng tôi không
thử nghiệm với các kiến trúc chỉ-decoder (Brown et al., 2020)
mà chúng tôi để lại cho nghiên cứu tương lai. Chúng tôi không
kết hợp phương pháp MHE với các phương pháp attention hiệu
quả tính toán với độ phức tạp tuyến tính, như Linformer (Wang
et al., 2020). Chúng tôi mong đợi rằng nó sẽ tăng tốc tính
toán MHE, nhưng nó nằm ngoài phạm vi bài báo của chúng tôi.

Lời cảm ơn
Chúng tôi muốn cảm ơn Constantinos Karouzos, Miles Williams
và các nhà phản biện ẩn danh vì phản hồi vô giá của họ.

Tài liệu tham khảo
Armen Aghajanyan, Luke Zettlemoyer, và Sonal Gupta. 2020. Intrinsic dimensionality explains the

--- TRANG 10 ---
effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.

Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, và Sumit Sanghai. 2023. Colt5: Faster long-range transformers with conditional computation.

Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, và Michael R Lyu. 2022. Towards efficient post-training quantization of pre-trained language models. Advances in Neural Information Processing Systems, 35:1405–1418.

Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Matan Ben Noach và Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889, Suzhou, China. Association for Computational Linguistics.

Henry J Bittermann. 1934. Elasticity of supply. The American Economic Review, pages 417–429.

Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, và Aleš Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA. Association for Computational Linguistics.

Yoonho Boo và Wonyong Sung. 2020. Fixed-point optimization of transformer neural network. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1753–1757. IEEE.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và Michael Carbin. 2020. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834–15846.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023).

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, và Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111, Doha, Qatar. Association for Computational Linguistics.

Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, và Adrian Weller. 2021. Rethinking attention with performers. In International Conference on Learning Representations.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Jonathan H. Clark, Dan Garrette, Iulia Turc, và John Wieting. 2022a. Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation. Transactions of the Association for Computational Linguistics, 10:73–91.

Jonathan H. Clark, Dan Garrette, Iulia Turc, và John Wieting. 2022b. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:73–91.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, và Christopher D. Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Angela Fan, Edouard Grave, và Armand Joulin. 2019. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations.

--- TRANG 11 ---
Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, và Marianne Winslett. 2021. Compressing large-scale transformer-based models: A case study on BERT. Transactions of the Association for Computational Linguistics, 9:1061–1080.

Mitchell Gordon, Kevin Duh, và Nicholas Andrews. 2020. Compressing BERT: Studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 143–155, Online. Association for Computational Linguistics.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, và Yoshua Bengio. 2017. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research, 18(1):6869–6898.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174, Online. Association for Computational Linguistics.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156–5165. PMLR.

Ashish Khetan và Zohar Karnin. 2020. schuBERT: Optimizing elements of BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2807–2818, Online. Association for Computational Linguistics.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations.

Fajri Koto, Jey Han Lau, và Timothy Baldwin. 2021. Discourse probing of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864, Online. Association for Computational Linguistics.

Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, và Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. 2020a. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. 2020b. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.

James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, và Santiago Ontanon. 2022. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4296–4313, Seattle, United States. Association for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, và Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175–184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, và Caiwen Ding. 2020a. Efficient transformer-based large scale language representations using hardware-friendly block structured pruning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3187–3199, Online. Association for Computational Linguistics.

--- TRANG 12 ---
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, và Jason Yosinski. 2018. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations.

Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang, và Yaohong Jin. 2020b. BERT-EMD: Many-to-many layer mapping for BERT compression with earth mover's distance. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3009–3018, Online. Association for Computational Linguistics.

Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, và Xu Sun. 2021. Dynamic knowledge distillation for pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Zi Lin, Jeremiah Liu, Zi Yang, Nan Hua, và Dan Roth. 2020. Pruning redundant mappings in transformer models via spectral-normalized identity prior. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 719–730, Online. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.

Ilya Loshchilov và Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.

Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Quanlu Zhang, Yaming Yang, Yunhai Tong, và Jing Bai. 2020. LadaBERT: Lightweight adaptation of BERT through hybrid model compression. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3225–3234, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Mitchell P. Marcus, Beatrice Santorini, và Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2017. Pointer sentinel mixture models. In International Conference on Learning Representations.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, và Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.

Sai Prasanna, Anna Rogers, và Anna Rumshisky. 2020. When BERT Plays the Lottery, All Tickets Are Winning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3208–3229, Online. Association for Computational Linguistics.

Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, và Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551.

Pranav Rajpurkar, Robin Jia, và Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789, Melbourne, Australia. Association for Computational Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.

Victor Sanh, Thomas Wolf, và Alexander Rush. 2020. Movement pruning: Adaptive sparsity by fine-tuning. Advances in Neural Information Processing Systems, 33:20378–20389.

Imanol Schlag, Kazuki Irie, và Jürgen Schmidhuber. 2021. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pages 9355–9366. PMLR.

Roy Schwartz, Jesse Dodge, Noah A Smith, và Oren Etzioni. 2020. Green AI. Communications of the ACM, 63(12):54–63.

Rico Sennrich, Barry Haddow, và Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguistics.

--- TRANG 13 ---
Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, và Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815–8821.

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.

Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, Rémi Gribonval, Herve Jegou, và Armand Joulin. 2020. Training with quantization noise for extreme model compression. In International Conference on Learning Representations.

Emma Strubell, Ananya Ganesh, và Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy. Association for Computational Linguistics.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, và Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.

Siqi Sun, Yu Cheng, Zhe Gan, và Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4323–4332, Hong Kong, China. Association for Computational Linguistics.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, và Denny Zhou. 2020. MobileBERT: a compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2158–2170, Online. Association for Computational Linguistics.

Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi, và Mehdi Rezagholizadeh. 2022. KroneckerBERT: Significant compression of pre-trained language models through kronecker decomposition and knowledge distillation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2116–2127, Seattle, United States. Association for Computational Linguistics.

Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M Rush, David Brooks, et al. 2021. Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, pages 830–844.

Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, và Ngai Wong. 2022. Compression of generative pre-trained language models via quantization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4821–4836, Dublin, Ireland. Association for Computational Linguistics.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca.

Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, và Che Zheng. 2021. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning, pages 10183–10192. PMLR.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. 2022. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1–28.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008.

Ivan Vulić, Edoardo Maria Ponti, Robert Litschko, Goran Glavaš, và Anna Korhonen. 2020. Probing pretrained language models for lexical semantics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7222–7240, Online. Association for Computational Linguistics.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the

--- TRANG 14 ---
2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium. Association for Computational Linguistics.

Benyou Wang, Yuxin Ren, Lifeng Shang, Xin Jiang, và Qun Liu. 2022. Exploring extreme parameter compression for pre-trained language models. In International Conference on Learning Representations.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, và Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.

Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 214–229.

Huiyin Xue và Nikolaos Aletras. 2022. HashFormers: Towards vocabulary-independent pre-trained transformers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7862–7874, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, và Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291–306.

Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan Duan, và Ruofei Zhang. 2021. El-attention: Memory efficient lossless attention for generation. In International Conference on Machine Learning, pages 11648–11658. PMLR.

Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, và Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 811–824. IEEE.

Ofir Zafrir, Guy Boudoukh, Peter Izsak, và Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36–39. IEEE.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, và Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.

--- TRANG 15 ---
A Metric được báo cáo cho mỗi tác vụ
Chúng tôi đánh giá tất cả các mô hình trên GLUE (Wang et al., 2018), SUPER GLUE (Wang et al., 2019), SQUAD V1.1 (Rajpurkar et al., 2016) và SQUAD V2.0 (Rajpurkar et al., 2018). Chúng tôi báo cáo matched accuracy cho MNLI, Matthews correlation cho CoLA, Spearman correlation cho STS, điểm F1 cho QQP, CB, MultiRC và SQUAD và accuracy cho tất cả các tác vụ khác. Bảng 5 và Bảng 6 trình bày kết quả trên GLUE và SUPER GLUE tương ứng cho các mô hình MHE-FORMERS của chúng tôi và tất cả các baseline với kiến trúc chỉ-encoder. Bảng 7 và 8 trình bày kết quả của điểm số và độ đàn hồi hiệu suất của tham số (PEoP) trên tất cả các mô hình cho mỗi tác vụ trong GLUE và SUPER GLUE. Bảng 10 trình bày kết quả trên GLUE cho các mô hình MHE-FORMERS của chúng tôi và tất cả các baseline với kiến trúc chỉ-decoder. Bảng 10 trình bày kết quả của điểm số và độ đàn hồi hiệu suất của tham số (PEoP) trên tất cả các mô hình cho mỗi tác vụ trong GLUE.

B Siêu tham số
Các siêu tham số được sử dụng trong huấn luyện trước được liệt kê trong Bảng 11. Các siêu tham số được sử dụng trong fine-tuning được liệt kê trong Bảng 12.

C Sử dụng bộ nhớ
Để minh họa thêm tính hiệu quả bộ nhớ của các mô hình MHE của chúng tôi so với các baseline, chúng tôi lấy kiến trúc BERT-base (12 attention head, mỗi head có chiều 64) làm ví dụ, và đo việc sử dụng bộ nhớ mỗi attention block như trong Mục 2.1.1 từ Smith et al. (2022) và báo cáo tỷ lệ tiết kiệm sử dụng bộ nhớ (%) trong quá trình tính toán attention trong Bảng 13:

Việc tính toán dựa trên đầu vào với batch size 32, chiều ẩn 768, độ dài chuỗi 512 và huấn luyện fp16 mixture precision sử dụng công thức sau:
• Memory(weights) = #params*(2+4) bytes;
• Memory(gradients) = #params*(2+4) bytes;
• Memory(Adam states) = #params*(4+4) bytes;
• Memory(activations) = batch-size*sequence-length*hidden-dimension*2 bytes.

Từ Bảng 13, chúng tôi quan sát tỷ lệ tiết kiệm sử dụng bộ nhớ của MHE được đề xuất của chúng tôi tốt hơn SKV 2,75 lần, tốt hơn MQA 1,50 lần và tốt hơn EL-ATT 1,37 lần, điều này cho thấy khả năng tiết kiệm bộ nhớ SotA so với tất cả các biến thể attention hiệu quả tham số khác.

D Tính mạnh mẽ đối với việc mở rộng
Chúng tôi cũng tiến hành thí nghiệm để quan sát tính hiệu quả và tính mạnh mẽ của MHE-MUL tốt nhất của chúng tôi trong khi mở rộng kích thước mô hình.

Bảng 14 trình bày accuracy trung bình trên hai benchmark phân loại văn bản (GLUE và SUPER GLUE), perplexity trên hai benchmark mô hình hóa ngôn ngữ (WIKITEXT-103 và PENN TREEBANK) với tỷ lệ duy trì hiệu suất tương ứng (PRR) cho MHA và MHE-MUL trong cả kiến trúc chỉ-encoder và chỉ-decoder trên các kích thước mô hình khác nhau.⁷ Đối với các mô hình chỉ-encoder, chúng tôi quan sát rằng PRR của MHE-MUL vẫn ổn định trên GLUE (từ 98,4% đến 98,7%) và SUPER GLUE (từ 98,7% đến 96,2%) trong khi mở rộng số lượng tham số trong các khối attention lên 3,5 lần lớn hơn. Đối với các mô hình chỉ-decoder, PRR trên GLUE cho MHE-MUL ổn định ở 97,9% (tức là thấp hơn 1,1%) sau khi mở rộng. Đáng ngạc nhiên, PRR của MHE-MUL tăng trên WIKITEXT-103 (từ 74,9% đến 95,2%) và PENN TREEBANK (từ 85,6% đến 88,5%) trong khi mở rộng lên kích thước MEDIUM.

Kết quả tương tự được quan sát cho kiến trúc encoder-decoder trên tác vụ dịch máy WMT14. Theo Bảng 15, trước tiên chúng tôi quan sát PRR của MHE-MUL vẫn ổn định (tức là từ 91,5% đến 96,0%) trên tất cả các kích thước khác nhau, trong đó số lượng tham số trong MHA tương ứng từ 19,87M đến 75,50M. Trong khi đó, chúng tôi cũng quan sát rằng việc làm cho mô hình sâu hơn bằng cách xếp chồng nhiều encoder và decoder layer hơn dẫn đến sự gia tăng đều đặn về PRR cho MHE-MUL (ví dụ: 93,6%, 95,0% và 96,0% tương ứng, cho 8 layer, 12 layer và 16 layer tổng cộng). Hơn nữa, đối với cùng số lượng tham số trong attention, attention head rộng hơn luôn dẫn đến PRR tốt hơn cho MHE-MUL, tức là 91,5%, 95,0% và 95,3% cho dimensionality 32, 64 và 128 của attention head tương ứng.

Những kết quả này cho thấy MHE luôn đạt được tỷ lệ duy trì hiệu suất tốt và mạnh mẽ đối với thay đổi kích thước mô hình.

⁷BASE: 12 encoder/decoder layer, mỗi layer chứa 12 attention head; LARGE/MEDIUM: 24 encoder/decoder layer, mỗi layer chứa 16 attention head.

--- TRANG 16 ---
[Bảng 5: Kết quả cho các mô hình chỉ-encoder trên tập dev GLUE với độ lệch chuẩn qua năm lần chạy trong ngoặc đơn. Giá trị in đậm biểu thị phương pháp hiệu suất tốt nhất trong mỗi tác vụ.]

[Bảng 6: Kết quả cho các mô hình chỉ-encoder trên tập dev SUPER GLUE với độ lệch chuẩn qua năm lần chạy trong ngoặc đơn. Giá trị in đậm biểu thị phương pháp hiệu suất tốt nhất trong mỗi tác vụ.]

[Bảng 7: Điểm số trung bình chi tiết và độ đàn hồi hiệu suất của tham số (trong ngoặc đơn) trên GLUE cho các mô hình MHE và các baseline với kiến trúc chỉ-encoder sử dụng MLM làm mục tiêu huấn luyện trước. Giá trị gạch chân biểu thị phương pháp hiệu suất tốt nhất và giá trị in đậm biểu thị phương pháp có PEoP tốt nhất trong mỗi tác vụ.]

--- TRANG 17 ---
[Bảng 8: Điểm số trung bình chi tiết và độ đàn hồi hiệu suất của tham số (trong ngoặc đơn) trên SUPER GLUE cho các mô hình MHE và các baseline với kiến trúc chỉ-encoder sử dụng MLM làm mục tiêu huấn luyện trước. Giá trị gạch chân biểu thị phương pháp hiệu suất tốt nhất và giá trị in đậm biểu thị phương pháp có PEoP tốt nhất trong mỗi tác vụ.]

[Bảng 9: Kết quả cho các mô hình chỉ-decoder trên tập dev GLUE với độ lệch chuẩn qua năm lần chạy trong ngoặc đơn. Giá trị in đậm biểu thị phương pháp hiệu suất tốt nhất trong mỗi tác vụ.]

[Bảng 10: Điểm số trung bình chi tiết và độ đàn hồi hiệu suất của tham số (trong ngoặc đơn) trên GLUE cho các mô hình MHE và các baseline với kiến trúc chỉ-decoder sử dụng MLM làm mục tiêu huấn luyện trước. Giá trị gạch chân biểu thị phương pháp hiệu suất tốt nhất và giá trị in đậm biểu thị phương pháp có PEoP tốt nhất trong mỗi tác vụ.]

--- TRANG 18 ---
[Bảng 11: Chi tiết siêu tham số được sử dụng trong huấn luyện trước.]

[Bảng 12: Chi tiết siêu tham số được sử dụng trong fine-tuning.]

--- TRANG 19 ---
[Bảng 13: Sử dụng bộ nhớ (tính bằng byte) và tỷ lệ tiết kiệm bộ nhớ (so với MHA) mỗi attention block cho MHE của chúng tôi và các baseline khác. MHA biểu thị BERT-base ở đây.]

[Bảng 14: Kết quả của metric đánh giá trên hai benchmark phân loại văn bản (GLUE, SUPER GLUE) và hai benchmark mô hình hóa ngôn ngữ (WIKITEXT-103 và PENN TREEBANK) với tỷ lệ duy trì hiệu suất (PRR) cho MHA và MHE-MUL trên các kích thước mô hình khác nhau.]

[Bảng 15: Kết quả điểm BLEU trên tác vụ dịch máy WMT-14 Anh sang Đức với tỷ lệ duy trì hiệu suất (PRR) cho MHA và MHE-MUL trên các kích thước mô hình khác nhau.]
