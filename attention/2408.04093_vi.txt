# TREE ATTENTION: GIáº¢I MÃƒ NHáº¬N THá»¨C TOPOLOGY CHO
ATTENTION DÃ€I NGá»ŒN Cáº¢N TRÃŠN Cá»¤M GPU

Vasudev Shyam1âˆ—, Jonathan Pilault1âˆ—, Emily Shepperd2, Quentin Anthony1, Beren Millidge1
1Zyphra,22EleutherAI
jonathan.pilault@gmail.com, vasu@zyphra.com

## TÃ“M Táº®T

Self-attention lÃ  hoáº¡t Ä‘á»™ng toÃ¡n há»c cá»‘t lÃµi cá»§a cÃ¡c kiáº¿n trÃºc transformer hiá»‡n Ä‘áº¡i vÃ  cÅ©ng lÃ  má»™t nÃºt tháº¯t cá»• chai tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ do Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a nÃ³ trong Ä‘á»™ dÃ i chuá»—i. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i suy ra hÃ m nÄƒng lÆ°á»£ng vÃ´ hÆ°á»›ng mÃ  gradient cá»§a nÃ³ tÃ­nh toÃ¡n khá»‘i self-attention, do Ä‘Ã³ lÃ m sÃ¡ng tá» ná»n táº£ng lÃ½ thuyáº¿t cá»§a self-attention. CÃ´ng thá»©c cá»§a chÃºng tÃ´i tiáº¿t lá»™ ráº±ng viá»‡c rÃºt gá»n qua trá»¥c chuá»—i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n hiá»‡u quáº£ song song thÃ´ng qua má»™t tree reduction. Thuáº­t toÃ¡n cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c gá»i lÃ  Tree Attention, Ä‘á»ƒ song song hÃ³a tÃ­nh toÃ¡n attention chÃ­nh xÃ¡c qua nhiá»u GPU cho phÃ©p giáº£i mÃ£ cross-device Ä‘Æ°á»£c thá»±c hiá»‡n nhanh hÆ¡n tiá»‡m cáº­n (lÃªn Ä‘áº¿n 8Ã— nhanh hÆ¡n trong cÃ¡c thá»­ nghiá»‡m cá»§a chÃºng tÃ´i) so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tiÃªn tiáº¿n nhÆ° Ring Attention, Ä‘á»“ng thá»i cÅ©ng yÃªu cáº§u Ã­t khá»‘i lÆ°á»£ng giao tiáº¿p hÆ¡n Ä‘Ã¡ng ká»ƒ vÃ  phÃ¡t sinh 2Ã— Ã­t bá»™ nhá»› Ä‘á»‰nh hÆ¡n. ChÃºng tÃ´i chá»©ng minh ráº±ng Tree Attention tÄƒng tá»‘c giáº£i mÃ£ lÃªn Ä‘áº¿n 4x trÃªn Llama 3.1-8B vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho nhiá»u thiáº¿t láº­p pháº§n cá»©ng vÃ  máº¡ng khÃ¡c nhau nhÆ° nÃºt H100 DGX, nÃºt AMD MI300x, vÃ  NVIDIA RTX 4090 káº¿t ná»‘i PCIe. MÃ£ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c cÃ´ng khai táº¡i Ä‘Ã¢y: https://github.com/Zyphra/tree_attention

## 1 GIá»šI THIá»†U

Hoáº¡t Ä‘á»™ng self-attention lÃ  khá»‘i xÃ¢y dá»±ng tÃ­nh toÃ¡n cá»‘t lÃµi cá»§a kiáº¿n trÃºc transformer Bahdanau et al. (2014); Vaswani et al. (2017), Ä‘Ã£ trá»Ÿ thÃ nh má»™t kiáº¿n trÃºc Ä‘Æ°á»£c á»©ng dá»¥ng rá»™ng rÃ£i vÃ  cÃ³ hiá»‡u quáº£ cao hiá»‡n Ä‘ang Ä‘Æ°á»£c Ã¡p dá»¥ng quy mÃ´ lá»›n cho ngÃ´n ngá»¯ Brown et al. (2020); Kaplan et al. (2020); Hoffmann et al. (2022); Team et al. (2023); Achiam et al. (2023); Pilault et al. (2023), thá»‹ giÃ¡c Dosovitskiy et al. (2020), Ã¢m thanh Betker (2023), vÃ  ra quyáº¿t Ä‘á»‹nh Chen et al. (2021); Reed et al. (2022). Tuy nhiÃªn, Ä‘á»™ phá»©c táº¡p thá»i gian báº­c hai cá»§a self-attention cÃ³ nghÄ©a lÃ  cáº§n tÃ i nguyÃªn Ä‘Ã¡ng ká»ƒ Ä‘á»ƒ huáº¥n luyá»‡n vÃ  sinh tá»« cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) dá»±a trÃªn transformer, Ä‘áº·c biá»‡t cho cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ dÃ i ngá»¯ cáº£nh lá»›n.

Trong quÃ¡ trÃ¬nh inference, khá»‘i attention pháº§n lá»›n quyáº¿t Ä‘á»‹nh cÃ¡c yÃªu cáº§u tÃ­nh toÃ¡n vÃ  bá»™ nhá»›, trá»Ÿ nÃªn Ä‘Ã²i há»i nhiá»u hÆ¡n khi Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o tÄƒng. Máº·c dÃ¹ LLM sinh ra má»™t token táº¡i má»™t thá»i Ä‘iá»ƒm, toÃ n bá»™ chuá»—i cÃ¡c token trong quÃ¡ khá»© váº«n pháº£i Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› vÃ  sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘iá»ƒm attention trong quÃ¡ trÃ¬nh sinh. VÃ¬ attention thá»±c hiá»‡n má»™t viá»‡c khá»›p tÆ°Æ¡ng Ä‘á»“ng cá»§a má»—i biá»ƒu diá»…n token vá»›i má»i token khÃ¡c, nÃ³ phÃ¡t sinh Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n báº­c hai vá» máº·t flops.

ÄÃ£ cÃ³ nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y trong viá»‡c huáº¥n luyá»‡n LLM Ä‘á»ƒ xá»­ lÃ½ ngá»¯ cáº£nh cá»±c dÃ i (lÃªn Ä‘áº¿n 1M token) Chen et al. (2023); kai (2023); Peng et al. (2023). CÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng kháº£ nÄƒng má»›i vá» máº·t cháº¥t lÆ°á»£ng nhÆ° há»c trong ngá»¯ cáº£nh quy mÃ´ cá»±c lá»›n cá»§a toÃ n bá»™ táº­p dá»¯ liá»‡u nhá» Ä‘Æ°á»£c giá»¯ trong prompt Reid et al. (2024); Lee et al. (2024); Bertsch et al. (2024). ChÃºng cÅ©ng cÃ³ thá»ƒ trÃ¡nh viá»‡c Ä‘Æ°a dá»¯ liá»‡u liÃªn tá»¥c Ä‘a phÆ°Æ¡ng thá»©c qua má»™t sÆ¡ Ä‘á»“ token hÃ³a cÃ³ tá»•n tháº¥t Reid et al. (2024); Team (2024) báº±ng cÃ¡ch hoáº¡t Ä‘á»™ng trá»±c tiáº¿p á»Ÿ cáº¥p Ä‘á»™ byte Xue et al. (2022); Wu et al. (2024). Tuy nhiÃªn váº¥n Ä‘á» lÃ  viá»‡c thá»±c hiá»‡n inference trÃªn cÃ¡c ngá»¯ cáº£nh dÃ i nhÆ° váº­y ráº¥t tá»‘n kÃ©m.

Äá»ƒ tÄƒng tá»‘c inference vÃ  giáº£m nháº¹ yÃªu cáº§u bá»™ nhá»›, cÃ¡c cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘Ã£ cá»‘ gáº¯ng thay Ä‘á»•i chÃ­nh cÆ¡ cháº¿ attention, hoáº·c báº±ng cÃ¡ch tuyáº¿n tÃ­nh hÃ³a nÃ³ Katharopoulos et al. (2020), hoáº·c xáº¥p xá»‰ nÃ³ báº±ng má»™t kernel map Choromanski et al. (2020b); Peng et al. (2021); Arora et al. (2024), Ä‘iá»u nÃ y giáº£m Ä‘á»™ phá»©c táº¡p xuá»‘ng tuyáº¿n tÃ­nh vá»›i chi phÃ­ lÃ  giáº£m tÃ­nh biá»ƒu Ä‘áº¡t. Nhá»¯ng ngÆ°á»i khÃ¡c Ä‘Ã£ phÃ¡t minh ra cÃ¡c kiáº¿n trÃºc trá»™n chuá»—i thay tháº¿ nhÆ° cÃ¡c mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cÃ³ thá»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£ trong thá»i gian tuyáº¿n tÃ­nh vÃ  bá»™ nhá»› khÃ´ng Ä‘á»•i Gu & Dao (2023); Dao & Gu (2024); Katsch (2023); Sun et al. (2023); Glorioso et al. (2024).

[HÃ¬nh 1: Topology Ring vÃ  Tree Attention. Do tÃ­nh cháº¥t káº¿t há»£p cá»§a cÃ¡c hoáº¡t Ä‘á»™ng logsumexp vÃ  max cá»§a Tree Attention (HÃ¬nh 1(a)), cÃ³ thá»ƒ cáº¥u trÃºc viá»‡c rÃºt gá»n qua chuá»—i nhÆ° má»™t cÃ¢y, yÃªu cáº§u Ã­t bÆ°á»›c giao tiáº¿p tiá»‡m cáº­n hÆ¡n Ring Attention (HÃ¬nh 1(b)) cÅ©ng nhÆ° Ã­t bá»™ nhá»› vÃ  khá»‘i lÆ°á»£ng giao tiáº¿p hÆ¡n.]

CÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c sá»­ dá»¥ng thuáº­t toÃ¡n hiá»‡u quáº£ Ä‘á»ƒ giáº£m gÃ¡nh náº·ng tÃ­nh toÃ¡n cá»§a attention trong khi giá»¯ nguyÃªn tÃ­nh toÃ¡n cá»‘t lÃµi. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y bao gá»“m memory-efficient attention Rabe & Staats (2021), Flash Attention Dao et al. (2022) vÃ  Flash Decoding fla (2024), cung cáº¥p má»™t táº­p há»£p cÃ¡c kernel nháº­n thá»©c IO Ä‘á»ƒ Ã¡nh xáº¡ hoáº¡t Ä‘á»™ng attention lÃªn tÃ i nguyÃªn pháº§n cá»©ng GPU má»™t cÃ¡ch cá»±c ká»³ hiá»‡u quáº£, giáº£m Ä‘Ã¡ng ká»ƒ overhead bá»™ nhá»› yÃªu cáº§u. CÃ¡c cÃ´ng trÃ¬nh tiáº¿p theo Character AI (2024); Kang et al. (2024); Liu et al. (2024); Nawrot et al. (2024) khÃ¡m phÃ¡ viá»‡c nÃ©n hoáº·c giáº£m KV cache cáº§n thiáº¿t trong quÃ¡ trÃ¬nh sinh. Cuá»‘i cÃ¹ng, Ring Attention Liu et al. (2023) Ä‘á» xuáº¥t má»™t cÃ¡ch Ä‘á»ƒ song song hÃ³a tÃ­nh toÃ¡n attention qua trá»¥c chuá»—i giá»¯a cÃ¡c GPU, do Ä‘Ã³ cho phÃ©p ngá»¯ cáº£nh dÃ i hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i cÃ³ thá»ƒ phá»¥c vá»¥ trÃªn má»™t GPU Ä‘Æ¡n. VÃ¬ phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i lÃ  má»™t tÃ­nh toÃ¡n chÃ­nh xÃ¡c cá»§a attentionÂ¹, nÃ³ lÃ  má»™t plugin thay tháº¿ cho báº¥t ká»³ cÆ¡ cháº¿ song song chuá»—i Ä‘a GPU nÃ o nhÆ° cÃ¡c cÆ¡ cháº¿ Ring Attention tiÃªn tiáº¿n. Báº±ng cÃ¡ch táº­n dá»¥ng hÃ m nÄƒng lÆ°á»£ng chÃ­nh xÃ¡c cho khá»‘i self-attention, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ tÄƒng tá»‘c inference cho cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng ngá»¯ cáº£nh dÃ i khi keys vÃ  values Ä‘Æ°á»£c chia sáº» qua nhiá»u GPU dá»c theo trá»¥c chuá»—i.

Thuáº­t toÃ¡n Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i Ä‘á»ƒ tÃ­nh toÃ¡n attention thÃ´ng qua gradient cá»§a hÃ m nÄƒng lÆ°á»£ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn má»™t chiáº¿n lÆ°á»£c tÃ­nh toÃ¡n song song hiá»‡u quáº£ vÃ  giao tiáº¿p tree reduction. Äáº·c biá»‡t, cÃ´ng thá»©c nÃ y cho phÃ©p chÃºng tÃ´i thiáº¿t káº¿ má»™t thuáº­t toÃ¡n nhanh hÆ¡n tiá»‡m cáº­n Ä‘á»ƒ thá»±c hiá»‡n giáº£i mÃ£ trong Ä‘Ã³ sá»‘ bÆ°á»›c giao tiáº¿p tá»· lá»‡ logarithmic vá»›i sá»‘ thiáº¿t bá»‹, thay vÃ¬ tuyáº¿n tÃ­nh nhÆ° trong cÃ¡c phÆ°Æ¡ng Ã¡n thay tháº¿ nhÆ° Ring Attention Liu et al. (2023). PhÆ°Æ¡ng phÃ¡p nháº­n thá»©c topology cá»§a chÃºng tÃ´i Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1 vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p song song attention hÃ ng Ä‘áº§u nhÆ° Ring Attention trÃªn nhiá»u thiáº¿t bá»‹.

## 2 CÃ”NG TRÃŒNH LIÃŠN QUAN

Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a self-attention, Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Vaswani et al. (2017), Ä‘áº·t ra thá»­ thÃ¡ch cho cÃ¡c chuá»—i dÃ i do sá»± phá»¥ thuá»™c báº­c hai vÃ o Ä‘á»™ dÃ i chuá»—i, ğ‘‚(ğ‘›Â²Â·ğ‘‘). Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, cÃ¡c cÆ¡ cháº¿ xáº¥p xá»‰ attention nhÆ° Linformer (Wang et al., 2020) vÃ  Performer (Choromanski et al., 2020a) giáº£m Ä‘á»™ phá»©c táº¡p xuá»‘ng tuyáº¿n tÃ­nh ğ‘‚(ğ‘›) sá»­ dá»¥ng cÃ¡c phÃ©p chiáº¿u low-rank vÃ  xáº¥p xá»‰ kernelized trÃªn má»™t thiáº¿t bá»‹ Ä‘Æ¡n. CÃ¡c mÃ´ hÃ¬nh thÆ°a thá»›t nhÆ° Longformer (Beltagy et al., 2020) vÃ  BigBird (Zaheer et al., 2020) tá»‘i Æ°u hÃ³a thÃªm cÃ¡c tÃ­nh toÃ¡n báº±ng cÃ¡ch háº¡n cháº¿ attention vÃ o cÃ¡c cá»­a sá»• Ä‘á»‹a phÆ°Æ¡ng hoáº·c máº«u thÆ°a thá»›t, giáº£m Ä‘Ã¡ng ká»ƒ nhu cáº§u tÃ i nguyÃªn trong khi duy trÃ¬ hiá»‡u suáº¥t cho cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ. Tuy nhiÃªn cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° váº­y cung cáº¥p xáº¥p xá»‰ cho cÆ¡ cháº¿ attention trong khi chÃºng tÃ´i tÃ¬m cÃ¡ch song song hÃ³a tÃ­nh toÃ¡n attention chÃ­nh xÃ¡c qua trá»¥c chuá»—i.

CÃ´ng trÃ¬nh lÃ½ thuyáº¿t cÅ©ng Ä‘Ã£ Ä‘Ã³ng gÃ³p vÃ o viá»‡c cáº£i thiá»‡n hiá»‡u quáº£ cá»§a cáº£ phÆ°Æ¡ng phÃ¡p chÃ­nh xÃ¡c vÃ  xáº¥p xá»‰. CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn kernel, nhÆ° nhá»¯ng phÆ°Æ¡ng phÃ¡p cá»§a Tsai et al. (2019), Ä‘á» xuáº¥t cÃ¡c cÃ´ng thá»©c thay tháº¿ cho self-attention cÃ³ hiá»‡u quáº£ tÃ­nh toÃ¡n. CÃ¡c kháº£o sÃ¡t nhÆ° Tay et al. (2020) nháº¥n máº¡nh nhá»¯ng tiáº¿n bá»™ nÃ y, nháº¥n máº¡nh sá»± phá»‘i há»£p giá»¯a cÃ¡c chiáº¿n lÆ°á»£c song song hÃ³a vÃ  cÃ¡c ká»¹ thuáº­t thÆ°a thá»›t hoáº·c xáº¥p xá»‰, Ä‘áº£m báº£o self-attention váº«n cÃ³ thá»ƒ má»Ÿ rá»™ng ngay cáº£ trong mÃ´i trÆ°á»ng tÃ­nh toÃ¡n háº¡n cháº¿. CÅ©ng cáº§n lÆ°u Ã½ ráº±ng Duman Keles et al. (2022) Ä‘Ã£ thiáº¿t láº­p cÃ¡c giá»›i háº¡n dÆ°á»›i vá» Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a self-attention, chá»©ng minh ráº±ng viá»‡c Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p thá»i gian dÆ°á»›i báº­c hai lÃ  khÃ´ng kháº£ thi trá»« khi Giáº£ thuyáº¿t Thá»i gian HÃ m mÅ© Máº¡nh (SETH) lÃ  sai.

NgoÃ i cÃ¡c phÆ°Æ¡ng phÃ¡p xáº¥p xá»‰, má»™t sá»‘ phÆ°Æ¡ng phÃ¡p táº­p trung vÃ o viá»‡c song song hÃ³a cÃ¡c tÃ­nh toÃ¡n attention chÃ­nh xÃ¡c. FlashAttention (Dao et al., 2022), cháº³ng háº¡n, tá»• chá»©c láº¡i tÃ­nh toÃ¡n attention thÃ nh cÃ¡c khá»‘i nhá» hÆ¡n, hiá»‡u quáº£ bá»™ nhá»› táº­n dá»¥ng há»‡ thá»‘ng phÃ¢n cáº¥p bá»™ nhá»› GPU Ä‘á»ƒ cho phÃ©p xá»­ lÃ½ song song attention chÃ­nh xÃ¡c nhanh hÆ¡n, vÃ  báº±ng cÃ¡ch Ä‘Ã³ giáº£m Ä‘á»™ phá»©c táº¡p bá»™ nhá»› tá»« báº­c hai xuá»‘ng tuyáº¿n tÃ­nh. CÃ¡c ká»¹ thuáº­t khÃ¡c sá»­ dá»¥ng cÃ¡c hoáº¡t Ä‘á»™ng ma tráº­n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a vÃ  chiáº¿n lÆ°á»£c tiling Ä‘á»ƒ phÃ¢n phá»‘i cÃ¡c tÃ­nh toÃ¡n attention qua cÃ¡c core hoáº·c thread má»™t cÃ¡ch hiá»‡u quáº£ (Shen et al., 2021). Trong khi cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y nháº±m tá»‘i Ä‘a hÃ³a throughput trong khi duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c cá»§a attention chÃ­nh xÃ¡c, chÃºng táº­p trung vÃ o viá»‡c tÄƒng tá»‘c tÃ­nh toÃ¡n attention trÃªn thiáº¿t bá»‹ Ä‘Æ¡n. VÃ¬ chÃºng tÃ´i song song hÃ³a attention chÃ­nh xÃ¡c qua nhiá»u thiáº¿t bá»‹, Ring Attention (Liu et al., 2023) cÃ³ thá»ƒ so sÃ¡nh nháº¥t vá»›i cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i. Cuá»‘i cÃ¹ng, theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, khÃ´ng cÃ³ ká»¹ thuáº­t nÃ o khÃ¡c khÃ¡m phÃ¡ giáº£i mÃ£ song song Ä‘a thiáº¿t bá»‹ nhÆ° chÃºng tÃ´i Ä‘Ã£ lÃ m trong bÃ i bÃ¡o nÃ y.

## 3 SELF-ATTENTION

Hoáº¡t Ä‘á»™ng self-attention cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° má»™t táº­p há»£p cÃ¡c tÃ¬m kiáº¿m tÆ°Æ¡ng Ä‘á»“ng dot product giá»¯a queries vÃ  keys. CÃ¡c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c rÃºt gá»n dá»c theo trá»¥c chuá»—i vÃ  softmaxed, sao cho Ä‘á»‘i vá»›i má»™t query cho trÆ°á»›c, cÃ³ má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a cÃ¡c tÆ°Æ¡ng Ä‘á»“ng cá»§a má»—i key cho trÆ°á»›c. Sau Ä‘Ã³ chÃºng ta láº¥y ká»³ vá»ng cá»§a cÃ¡c vector value vá»›i phÃ¢n phá»‘i nÃ y. ChÃºng ta kÃ½ hiá»‡u cÃ¡c queries Ä‘Æ°á»£c gÃ¡n cho má»™t chuá»—i cÃ³ Ä‘á»™ dÃ i ğ‘ lÃ  {ğ‘â‚, ğ‘=1,Â·Â·Â·ğ‘}, trong Ä‘Ã³ má»—i query lÃ  má»™t vector cÃ³ kÃ­ch thÆ°á»›c ğ‘‘ Ä‘áº¡i diá»‡n cho hidden dimension, ğ‘â‚âˆˆâ„áµˆ, vÃ  tÆ°Æ¡ng tá»± cÃ¡c keys vÃ  values {(ğ‘˜â‚,ğ‘£â‚), ğ‘=1,Â·Â·Â·ğ‘}. Attention cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ°

ğ‘§â‚ = âˆ‘áµ¢â‚Œâ‚á´º softmax(ğ‘â‚Â·ğ‘˜áµ¢áµ€)ğ‘£áµ¢.

Viá»‡c tÃ­nh toÃ¡n attention má»™t cÃ¡ch ngÃ¢y thÆ¡ theo cÃ¡ch nÃ y yÃªu cáº§u cá»¥ thá»ƒ hÃ³a ma tráº­n ğ‘ğ‘˜ vá»›i chi phÃ­ tÃ­nh toÃ¡n vÃ  bá»™ nhá»› báº­c hai trong Ä‘á»™ dÃ i chuá»—i. Memory-efficient attention Rabe & Staats (2021) lÃ  má»™t cÃ¡ch láº·p Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c tÆ°Æ¡ng Ä‘á»“ng softmax mÃ  khÃ´ng bao giá» pháº£i cá»¥ thá»ƒ hÃ³a ma tráº­n attention Ä‘áº§y Ä‘á»§. NÃ³ thá»±c hiá»‡n cÃ¡c hoáº¡t Ä‘á»™ng sau, má»™t query (hoáº·c má»™t chunk queries) táº¡i má»™t thá»i Ä‘iá»ƒm:

ğ‘ áµ¢â½Ê²â¾ = exp(ğ‘â±¼Â·ğ‘˜áµ¢) (1)
ğ‘›áµ¢â½Ê²â¾ = ğ‘›áµ¢â‚‹â‚â½Ê²â¾ + ğ‘£áµ¢ğ‘ áµ¢â½Ê²â¾ (2)
ğ‘‘áµ¢â½Ê²â¾ = ğ‘‘áµ¢â‚‹â‚â½Ê²â¾ + ğ‘ áµ¢â½Ê²â¾ (3)

Sau Ä‘Ã³, má»™t khi cÃ¡c giÃ¡ trá»‹ ğ‘£ vÃ  máº«u sá»‘ softmax ğ‘‘ Ä‘Æ°á»£c tÃ­nh toÃ¡n, chÃºng ta chia Ä‘á»ƒ cÃ³ Ä‘iá»ƒm softmaxed cuá»‘i cÃ¹ng ğ‘§â½Ê²â¾ = ğ‘›â½Ê²â¾/ğ‘‘â½Ê²â¾ cho má»i chá»‰ sá»‘ query ğ‘—. Viá»‡c tÃ­nh toÃ¡n attention theo cÃ¡ch láº·p nÃ y giáº£m Ä‘Ã¡ng ká»ƒ bá»™ nhá»› yÃªu cáº§u.

Flash Attention Dao et al. (2022) sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng tá»± Ä‘á»ƒ giáº£m chi phÃ­ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cá»§a attention, nhÆ°ng thuáº­t toÃ¡n khÃ´ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho tÃ­nh toÃ¡n Ä‘a GPU. Flash Attention thá»±c hiá»‡n thuáº­t toÃ¡n láº·p cá»§a Rabe & Staats (2021) theo cÃ¡ch blockwise, sá»­ dá»¥ng cÃ¡c primitive tÃ­nh toÃ¡n song song khá»‘i cÃ³ sáºµn bÃªn trong tensor cores cá»§a GPU Ä‘Æ¡n. NgoÃ i ra, nÃ³ Ä‘á»‹nh kÃ­ch thÆ°á»›c chÃ­nh xÃ¡c cÃ¡c khá»‘i sao cho chÃºng cÃ³ thá»ƒ vá»«a vÃ o SRAM cá»§a GPU cho toÃ n bá»™ tÃ­nh toÃ¡n attention, thá»±c hiá»‡n hiá»‡u quáº£ kernel fusion vÃ  ngÄƒn cháº·n nhiá»u hoáº¡t Ä‘á»™ng IO khÃ´ng cáº§n thiáº¿t.

## 4 SELF-ATTENTION NHÆ¯ GRADIENT Cá»¦A Má»˜T HÃ€M NÄ‚NG LÆ¯á»¢NG

Theo thÃ nh cÃ´ng phá»• biáº¿n cá»§a kiáº¿n trÃºc transformer, Ä‘Ã£ cÃ³ ná»— lá»±c Ä‘Ã¡ng ká»ƒ Ä‘á»ƒ hiá»ƒu toÃ¡n há»c vá» báº£n cháº¥t vÃ  Ã½ nghÄ©a cá»§a hoáº¡t Ä‘á»™ng attention vÃ  liÃªn káº¿t nÃ³ vá»›i cÃ¡c mÃ´ hÃ¬nh nÄƒng lÆ°á»£ng (Krotov & Hopfield, 2016; Krotov, 2021; Millidge et al., 2022; Hoover et al., 2024), nhÆ° Hopfield Networks (Ramsauer et al., 2020; D'Amico & Negri, 2024). Ramsauer et al. (2020) tiÃªn phong trong lÄ©nh vá»±c nÃ y báº±ng cÃ¡ch thá»±c hiá»‡n má»™t phÃ¢n tÃ­ch tÆ°Æ¡ng tá»± nhÆ°ng khÃ¡c biá»‡t Ä‘á»ƒ liÃªn há»‡ self-attention vá»›i cÃ¡c máº¡ng Hopfield hiá»‡n Ä‘áº¡i, cung cáº¥p má»™t diá»…n giáº£i má»›i vÃ  sÃ¢u sáº¯c vá» self-attention nhÆ° thá»±c hiá»‡n tÃ¬m kiáº¿m bá»™ nhá»› hetero-associative sá»­ dá»¥ng má»™t hÃ m tÆ°Æ¡ng Ä‘á»“ng phi tuyáº¿n máº¡nh. CÃ´ng trÃ¬nh nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c má»Ÿ rá»™ng bá»Ÿi Hoover et al. (2023), ngÆ°á»i Ä‘Ã£ suy ra má»™t phiÃªn báº£n modified cá»§a transformer dá»±a trÃªn má»™t hÃ m nÄƒng lÆ°á»£ng. Tuy nhiÃªn, trong khi tá»« lÃ¢u Ä‘Ã£ biáº¿t ráº±ng hoáº¡t Ä‘á»™ng softmax cÃ³ thá»ƒ Ä‘Æ°á»£c suy ra nhÆ° gradient cá»§a hÃ m vÃ´ hÆ°á»›ng sau:

âˆ‚/âˆ‚z_j log(âˆ‘áµƒâ‚Œâ‚â¿ exp(zâ‚)) = e^z_j / âˆ‘áµƒâ‚Œâ‚â¿ e^zâ‚ = softmax(z_j), (4)

Ä‘Æ°á»£c biáº¿t Ä‘áº¿n nhÆ° log-sum-exp, má»™t hÃ m tÆ°Æ¡ng Ä‘Æ°Æ¡ng cho khá»‘i self-attention váº«n chÆ°a Ä‘Æ°á»£c suy ra.

ChÃºng tÃ´i phÃ¡t triá»ƒn trong bÃ i bÃ¡o nÃ y má»™t liÃªn káº¿t giá»¯a attention vÃ  cÃ¡c hÃ m nÄƒng lÆ°á»£ng báº±ng cÃ¡ch giá»›i thiá»‡u má»™t vector nguá»“n phá»¥ trá»£ Î¶, Ä‘áº¡i diá»‡n cho "Ä‘Ã³ng gÃ³p bÃªn ngoÃ i" cho nÄƒng lÆ°á»£ng cá»§a há»‡ thá»‘ng (Hopfield, 1982). Nguá»“n Î¶ lÃ  tham sá»‘ mÃ  chÃºng ta tÃ­nh gradient cá»§a hÃ m nÄƒng lÆ°á»£ng vÃ´ hÆ°á»›ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c hoáº¡t Ä‘á»™ng self-attention. NhÆ° chÃºng ta sáº½ tháº¥y, chÃºng ta cáº§n nguá»“n Ä‘á»ƒ viáº¿t ra hÃ m sinh cá»§a cÃ¡c moment cá»§a phÃ¢n phá»‘i vÃ¬ viá»‡c láº¥y gradient theo Î¶ sáº½ cho hoáº¡t Ä‘á»™ng self-attention chÃ­nh xÃ¡c.

Hiá»ƒu biáº¿t nÃ y cho phÃ©p chÃºng ta Ä‘Æ°a ra quan sÃ¡t sau:

**Quan sÃ¡t 1.** Attention cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° gradient cá»§a má»™t hÃ m nÄƒng lÆ°á»£ng vÃ´ hÆ°á»›ng ğ¹(Î¶) theo nguá»“n Î¶, sao cho:

âˆ‘áµƒâ‚Œâ‚á´º softmax(qÂ·kâ‚)vâ‚ = âˆ‚F/âˆ‚Î¶|_{Î¶=0}, (5)

trong Ä‘Ã³ hÃ m sinh moment (tá»©c lÃ  hÃ m nÄƒng lÆ°á»£ng) ğ¹(Î¶) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ :

F(Î¶) = log âˆ‘â‚ exp(qÂ·kâ‚áµ€ + Î¶Â·vâ‚áµ€). (6)

Báº±ng chá»©ng cá»§a Quan sÃ¡t 1 cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Phá»¥ lá»¥c C.1. Xin lÆ°u Ã½ ráº±ng cÃ´ng thá»©c nÃ y cÅ©ng cho phÃ©p Ä‘Æ°a ra diá»…n giáº£i Bayesian cá»§a Attention trong Phá»¥ lá»¥c C.2 vÃ  thÃºc Ä‘áº©y thuáº­t toÃ¡n Tree Attention cá»§a chÃºng tÃ´i trong Pháº§n 5 tiáº¿p theo.

## 5 TREE ATTENTION

Trong pháº§n nÃ y chÃºng tÃ´i cho tháº¥y cÃ¡ch cÃ´ng thá»©c cá»§a hoáº¡t Ä‘á»™ng attention nhÆ° gradient cá»§a má»™t hÃ m nÄƒng lÆ°á»£ng Ä‘á» xuáº¥t má»™t chiáº¿n lÆ°á»£c song song hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n nÃ³. Hiá»ƒu biáº¿t chÃ­nh lÃ  táº­n dá»¥ng má»™t thuáº­t toÃ¡n hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n nÄƒng lÆ°á»£ng, vÃ  sau Ä‘Ã³ vi phÃ¢n nÃ³ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t thuáº­t toÃ¡n hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n attention.

### 5.1 TÃNH TOÃN HÃ€M NÄ‚NG LÆ¯á»¢NG HIá»†U QUáº¢

HÃ£y táº­p trung vÃ o trÆ°á»ng há»£p giáº£i mÃ£ vá»›i KV cache trong má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ causal trong Ä‘Ã³ chÃºng ta cÃ³ má»™t query vÃ  ğ‘ keys vÃ  values. Trong trÆ°á»ng há»£p nÃ y, hÃ m nÄƒng lÆ°á»£ng lÃ :

F_dec = log âˆ‘áµƒâ‚Œâ‚á´º exp(qÂ·kâ‚áµ€ + Î¶Â·vâ‚áµ€) â‰¡ logsumexp_a({qÂ·kâ‚áµ€ + Î¶Â·vâ‚áµ€, a=1,Â·Â·Â·,N}). (7)

Má»™t sá»± tháº­t quan trá»ng lÃ  cáº£ logsumexp_a vÃ  max_a Ä‘á»u lÃ  cÃ¡c hoáº¡t Ä‘á»™ng káº¿t há»£p:

logsumexp_a({T_a, logsumexp_a({R_a, S_a})}) = logsumexp_a({logsumexp_a({T_a, R_a}), S_a}),
max_a({max_a({T_a, R_a}), S_a}) = max_a({T_a, max_a({R_a, S_a})}).

ChÃºng ta cÃ³ thá»ƒ chá»©ng minh ráº±ng tÃ­nh cháº¥t káº¿t há»£p nÃ y cho phÃ©p cÃ¡c reduction nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n hiá»‡u quáº£ song song vá»›i Ä‘á»™ phá»©c táº¡p thá»i gian logarithmic, miá»…n lÃ  chÃºng ta cÃ³ Ä‘á»§ nhiá»u worker song song:

**Äá»‹nh lÃ½ 1.** Äá»™ phá»©c táº¡p thá»i gian cá»§a má»™t hoáº¡t Ä‘á»™ng reduction liÃªn quan Ä‘áº¿n má»™t hÃ m káº¿t há»£p, nhÆ° logsumexp_a hoáº·c max_a, trÃªn má»™t máº£ng cÃ³ kÃ­ch thÆ°á»›c ğ‘ sá»­ dá»¥ng ğ‘ processor song song lÃ  ğ‘‚(ğ‘/ğ‘ + log ğ‘). Khi sá»‘ processor ğ‘ báº±ng ğ‘, Ä‘á»™ phá»©c táº¡p thá»i gian Ä‘Æ°á»£c giáº£m xuá»‘ng ğ‘‚(log ğ‘).

Báº±ng chá»©ng cá»§a Äá»‹nh lÃ½ 1 trong Phá»¥ lá»¥c E.

Káº¿t há»£p káº¿t quáº£ nÃ y, vÃ  cho Ã¢, bÌ‚ âˆˆ {1,Â·Â·Â·,t} lÃ  cÃ¡c chá»‰ sá»‘ intra-chunk, chÃºng ta cÃ³ Thuáº­t toÃ¡n 1 song song cao nhÆ° sau:

**Thuáº­t toÃ¡n 1** Single Query Energy Forward (tÃ­nh toÃ¡n logsumexp)
1: Chia k,v âˆˆ â„á´ºË£áµˆÊ° thÃ nh ğ‘ chunks {k_Ã¢, v_Ã¢, Ã¢ âˆˆ {1,Â·Â·Â·,N/p}} cÃ³ kÃ­ch thÆ°á»›c t = N/p
2: PhÃ¢n tÃ¡n má»™t báº£n sao cá»§a q, Î¶, vÃ  má»—i k_Ã¢, v_Ã¢ Ä‘áº¿n má»—i processor trong sá»‘ ğ‘ processors.
3: TÃ­nh toÃ¡n song song r_Ã¢ = qÂ·k_Ã¢áµ€ + Î¶Â·v_Ã¢áµ€
4: TÃ­nh toÃ¡n m = Reduce(max, r_Ã¢) báº±ng cÃ¡ch thá»±c hiá»‡n tree reduction.
5: PhÃ¢n tÃ¡n m Ä‘áº¿n má»i thiáº¿t bá»‹ vÃ  cáº­p nháº­t r_Ã¢ â†’ r_Ã¢ - m.
6: TÃ­nh toÃ¡n lse = Reduce(logsumexp, r_Ã¢) báº±ng cÃ¡ch thá»±c hiá»‡n tree reduction.
7: LÆ°u lse, m cho gradient w.r.t Î¶.
8: Tráº£ vá» lse

### 5.2 GIáº¢I MÃƒ SONG SONG HIá»†U QUáº¢

Má»™t trong nhá»¯ng hiá»ƒu biáº¿t cá»‘t lÃµi cá»§a automatic differentiation lÃ  gradient cá»§a má»™t hÃ m âˆ‡_x f(x) cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n vá»›i cÃ¹ng Ä‘á»™ phá»©c táº¡p thá»i gian nhÆ° tÃ­nh toÃ¡n f(x) Vieira (2016). Tuy nhiÃªn cáº£nh bÃ¡o lÃ  náº¿u hÃ m cÃ³ má»™t computational graph sÃ¢u, thÃ¬ dáº¥u chÃ¢n bá»™ nhá»› cá»§a viá»‡c tÃ­nh toÃ¡n gradient tÄƒng theo Ä‘á»™ sÃ¢u Ä‘Ã³ vÃ¬ backpropagation yÃªu cáº§u lÆ°u trá»¯ cÃ¡c giÃ¡ trá»‹ cá»§a cÃ¡c tensor trung gian. Trong trÆ°á»ng há»£p cá»§a chÃºng ta, computational graph liÃªn quan Ä‘áº¿n viá»‡c tÃ­nh toÃ¡n nÄƒng lÆ°á»£ng lÃ  nÃ´ng vÃ  do Ä‘Ã³ overhead bá»™ nhá»› lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  náº¿u chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n nÄƒng lÆ°á»£ng hiá»‡u quáº£, chÃºng ta cÃ³ Ä‘Æ°á»£c má»™t thuáº­t toÃ¡n hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n gradient cá»§a nÃ³ (tá»©c lÃ  hoáº¡t Ä‘á»™ng self-attention) má»™t cÃ¡ch tá»± Ä‘á»™ng.

Trong trÆ°á»ng há»£p cá»§a chÃºng ta, chÃºng ta muá»‘n tÃ­nh toÃ¡n gradient cá»§a hÃ m nÄƒng lÆ°á»£ng theo Î¶_A vÃ  sau Ä‘Ã³ Ä‘áº·t nÃ³ báº±ng khÃ´ng. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i cÃ¡c engine automatic differentiation Ä‘Ã£ Ä‘áº·t Î¶ lÃ  má»™t tensor cá»§a cÃ¡c sá»‘ khÃ´ng tá»« Ä‘áº§u. Tuy nhiÃªn chÃºng ta cÃ³ thá»ƒ thá»±c hiá»‡n thá»§ cÃ´ng má»™t gradient theo Î¶ pass cá»§a Thuáº­t toÃ¡n 1 á»Ÿ trÃªn mÃ  khÃ´ng cá»¥ thá»ƒ hÃ³a Î¶ trong Thuáº­t toÃ¡n 2 dÆ°á»›i Ä‘Ã¢y. LÆ°u Ã½ Ä‘áº·c biá»‡t ráº±ng khi chÃºng ta Ä‘áº·t Î¶_A = 0, A âˆˆ {1,Â·Â·Â·,d_h} thÃ¬ lse chá»‰ liÃªn quan Ä‘áº¿n logsumexp cá»§a dot product giá»¯a queries vÃ  keys.

**Thuáº­t toÃ¡n 2** Tree Decoding (sá»­ dá»¥ng atomic operation trÃªn má»—i thiáº¿t bá»‹)
1: Chia k,v âˆˆ â„á´ºË£áµˆÊ° thÃ nh ğ‘ chunks {k_Ã¢, v_Ã¢, Ã¢ âˆˆ {1,Â·Â·Â·,N/p}} cÃ³ kÃ­ch thÆ°á»›c t = N/p
2: TÃ­nh toÃ¡n m vÃ  lse sá»­ dá»¥ng Thuáº­t toÃ¡n 1.
3: PhÃ¢n tÃ¡n má»™t báº£n sao cá»§a q, m vÃ  lse, vÃ  má»—i k_Ã¢, v_Ã¢ Ä‘áº¿n má»—i processor trong sá»‘ ğ‘ processors.
4: TÃ­nh toÃ¡n song song r_Ã¢ = qÂ·k_Ã¢áµ€ - m
5: TÃ­nh toÃ¡n R_Ã¢ = exp(r_Ã¢)/exp(lse)Â·v_Ã¢ = exp(r_Ã¢ - lse)Â·v_Ã¢
6: TÃ­nh toÃ¡n z = Reduce(sum, R_Ã¢)
7: Tráº£ vá» z

LÆ°u Ã½ á»Ÿ Ä‘Ã¢y ráº±ng báº±ng cÃ¡ch lÆ°u trá»¯ lse, m cho backward pass, hoáº¡t Ä‘á»™ng reduction duy nháº¥t cÃ²n láº¡i cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n lÃ  hoáº¡t Ä‘á»™ng á»Ÿ dÃ²ng 5 cá»§a thuáº­t toÃ¡n trÃªn. Reduction Ä‘Æ¡n nÃ y máº¥t thá»i gian ğ‘‚(N/p) Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c tá»•ng Ä‘á»‹a phÆ°Æ¡ng trÃªn má»—i thiáº¿t bá»‹ vÃ  thá»i gian log p Ä‘á»ƒ giao tiáº¿p vÃ  káº¿t há»£p cÃ¡c káº¿t quáº£ partial, vÃ  do Ä‘Ã³ chÃºng ta cÃ³ cÃ¹ng Ä‘á»™ phá»©c táº¡p tiá»‡m cáº­n nhÆ° viá»‡c tÃ­nh toÃ¡n logsumexp.

Trong thá»±c táº¿, chÃºng tÃ´i thá»±c hiá»‡n forward vÃ  gradient w.r.t. Î¶ trong má»™t hÃ m duy nháº¥t tráº£ vá» cáº£ giÃ¡ trá»‹ vÃ  gradient cá»§a hÃ m nÄƒng lÆ°á»£ng. Do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ káº¿t há»£p Thuáº­t toÃ¡n 1 vÃ  2 thÃ nh Thuáº­t toÃ¡n 3 giáº£i mÃ£ song song hiá»‡u quáº£ sau:

**Thuáº­t toÃ¡n 3** Tree Decoding (sá»­ dá»¥ng Flash Attention 2 trÃªn má»—i thiáº¿t bá»‹)
1: Chia k,v âˆˆ â„á´ºË£áµˆÊ° giá»¯a ğ‘ GPU, má»—i GPU vá»›i má»™t chunk {k_Ã¢, v_Ã¢, Ã¢ âˆˆ {1,Â·Â·Â·,N/p}} cÃ³ kÃ­ch thÆ°á»›c t = N/p vÃ  phÃ¢n tÃ¡n q Ä‘áº¿n má»—i GPU.
2: Sá»­ dá»¥ng Flash Attention 2 Ä‘á»ƒ tÃ­nh toÃ¡n o = (âˆ‘_Ã¢ exp(qÂ·k_Ã¢áµ€)v_Ã¢)/(âˆ‘_bÌ‚ exp(qÂ·k_bÌ‚áµ€)) vÃ  lse = log âˆ‘_bÌ‚ exp(qÂ·k_bÌ‚áµ€).
3: TÃ­nh toÃ¡n láº¡i global max m = Allreduce(max, lse).
4: Láº¥y numerator vÃ  denominator Ä‘á»‹a phÆ°Æ¡ng báº±ng cÃ¡ch tÃ­nh toÃ¡n: n = o*exp(lse - m), d = exp(lse - m).
5: TÃ­nh toÃ¡n numerator vÃ  denominator toÃ n cáº§u vá»›i: n_g = Allreduce(sum, n), d_g = Allreduce(sum, d).
6: Tráº£ vá» káº¿t quáº£ z = n_g/d_g.

Thuáº­t toÃ¡n nÃ y yÃªu cáº§u ba hoáº¡t Ä‘á»™ng Allreduce tá»•ng cá»™ng, cÃ³ nghÄ©a lÃ  Ä‘á»™ phá»©c táº¡p thá»i gian yÃªu cáº§u lÃ  ğ‘‚(3(N/p + log p)).

### 5.3 CÃC HOáº T Äá»˜NG COLLECTIVE HIá»†U QUáº¢ Sá»¬ Dá»¤NG TOPOLOGY-AWARENESS

**Communication overheads** Trong khi phÃ¢n tÃ­ch lÃ½ thuyáº¿t á»Ÿ trÃªn chá»‰ ra ráº±ng chÃºng ta nÃªn tháº¥y speedups khi sá»­ dá»¥ng tree-based reductions, Ä‘iá»u nÃ y khÃ´ng nháº¥t thiáº¿t Ä‘Æ°á»£c Ä‘áº£m báº£o trong thá»±c táº¿ do cÃ¡c overhead tiá»m nÄƒng khÃ¡c nhau. Äáº·c biá»‡t, láº­p luáº­n cá»§a chÃºng tÃ´i vá» Ä‘á»™ phá»©c táº¡p thá»i gian cá»§a thuáº­t toÃ¡n Tree Decoding Ä‘á» xuáº¥t giáº£ Ä‘á»‹nh ráº±ng giao tiáº¿p cá»§a cÃ¡c káº¿t quáº£ partial lÃ  tá»©c thá»i, Ä‘iá»u mÃ  trong thá»±c táº¿ khÃ´ng bao giá» xáº£y ra. Thá»±c táº¿, khi chÃºng ta má»Ÿ rá»™ng Ä‘á»™ dÃ i chuá»—i, hoáº·c sá»‘ GPU Ä‘áº·c biá»‡t Ä‘áº¿n thiáº¿t láº­p multi-node, thá»i gian giao tiáº¿p lÃ  Ä‘Ã³ng gÃ³p chá»§ Ä‘áº¡o cho tá»•ng thá»i gian thá»±c thi. Tuy nhiÃªn, quan trá»ng lÃ , ngoÃ i lá»£i Ã­ch tiá»‡m cáº­n, Tree Attention cÃ³ lá»£i tá»« viá»‡c táº­n dá»¥ng topology hai cáº¥p tiÃªu chuáº©n trong cÃ¡c cá»¥m GPU hiá»‡n Ä‘áº¡i.

[HÃ¬nh 2: NCCL Send/Recv giá»¯a hai GPU H100 intra-node vÃ  inter-node. CÃ¡c cá»¥m GPU cung cáº¥p topology hai táº§ng trong Ä‘Ã³ bÄƒng thÃ´ng intra-node cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i inter-node. CÃ¡c thuáº­t toÃ¡n nhÆ° Tree Attention khai thÃ¡c topology nÃ y báº±ng cÃ¡ch giáº£m yÃªu cáº§u giao tiáº¿p inter-node, cho phÃ©p overlap tá»‘t hÆ¡n cá»§a giao tiáº¿p vá»›i tÃ­nh toÃ¡n.]

ChÃºng tÃ´i benchmark thuáº­t toÃ¡n cá»§a mÃ¬nh vá»›i má»™t thuáº­t toÃ¡n attention song song chuá»—i Ä‘Æ°á»£c Ä‘á» xuáº¥t trÆ°á»›c Ä‘Ã³ gá»i lÃ  Ring Attention. NhÆ° thuáº­t toÃ¡n cá»§a chÃºng tÃ´i, Ring Attention giáº£ Ä‘á»‹nh ráº±ng chuá»—i Ä‘Æ°á»£c chia sáº» qua cÃ¡c GPU vÃ  thá»±c hiá»‡n tÃ­nh toÃ¡n attention mÃ  khÃ´ng thu tháº­p táº¥t cáº£ chuá»—i vÃ o má»™t thiáº¿t bá»‹ duy nháº¥t. Thay vÃ o Ä‘Ã³, nÃ³ giao tiáº¿p cÃ¡c shard cá»§a keys vÃ  values theo cÃ¡ch point-to-point giá»¯a cÃ¡c GPU lÃ¡ng giá»ng Ä‘Æ°á»£c sáº¯p xáº¿p logic trong topology vÃ²ng. Giao tiáº¿p nÃ y Ä‘Æ°á»£c overlap vá»›i tÃ­nh toÃ¡n cá»§a shard Ä‘á»‹a phÆ°Æ¡ng cá»§a output. NgÆ°á»£c láº¡i vá»›i chiáº¿n lÆ°á»£c nÃ y, thuáº­t toÃ¡n cá»§a chÃºng tÃ´i phÃ¢n tÃ¡n query vÃ  giao tiáº¿p káº¿t quáº£ partial qua táº¥t cáº£ GPU khi thá»±c hiá»‡n hoáº¡t Ä‘á»™ng AllReduce, nhÆ°ng khÃ´ng di chuyá»ƒn cÃ¡c shard key vÃ  value giá»¯a cÃ¡c GPU. Do Ä‘Ã³, trong trÆ°á»ng há»£p giáº£i mÃ£, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ lá»£i tá»« viá»‡c cÃ³ khá»‘i lÆ°á»£ng giao tiáº¿p tháº¥p hÆ¡n vÃ  chá»‹u Ã­t overhead chi phÃ­ giao tiáº¿p hÆ¡n Ring Attention.

**Implications of network bandwidth hierarchy** Ring Attention vá» báº£n cháº¥t khÃ´ng nháº­n thá»©c topology, vÃ  chá»‰ má»Ÿ rá»™ng trong má»™t máº¡ng bÄƒng thÃ´ng Ä‘á»“ng nháº¥t. Tuy nhiÃªn, Ä‘iá»u nÃ y mÃ¢u thuáº«n vá»›i topology máº¡ng hai cáº¥p cá»§a cÃ¡c cá»¥m GPU hiá»‡n Ä‘áº¡i, sá»­ dá»¥ng cÃ¡c interconnect bÄƒng thÃ´ng cao trong cÃ¡c node (NVLINK hoáº·c PCIe) vÃ  cÃ¡c interconnect bÄƒng thÃ´ng tÆ°Æ¡ng Ä‘á»‘i tháº¥p hÆ¡n qua cÃ¡c node (InfiniBand hoáº·c Ethernet). CÃ¡c interconnect khÃ¡c nhau ráº¥t nhiá»u vá» bÄƒng thÃ´ng vÃ  latency (xem HÃ¬nh 2). Do Ä‘Ã³, Ring Attention bá»‹ bottleneck bá»Ÿi interconnect cháº­m nháº¥t, vÃ  khÃ´ng thá»ƒ luÃ´n overlap tÃ­nh toÃ¡n attention vá»›i giao tiáº¿p. ChÃºng tÃ´i tháº£o luáº­n Ä‘iá»ƒm nÃ y thÃªm trong 6.3 Tree Attention cáº£i thiá»‡n Ring Attention báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c máº«u giao tiáº¿p nháº­n thá»©c topology máº¡ng Ä‘á»ƒ tÄƒng overlap cá»§a tÃ­nh toÃ¡n vÃ  giao tiáº¿p, vÃ  giáº£m bottleneck kháº£ nÄƒng má»Ÿ rá»™ng nÃ y vá» giao tiáº¿p tá»« tÃ­nh toÃ¡n attention phÃ¢n tÃ¡n.

Trong thá»±c táº¿, cÃ¡c thÆ° viá»‡n giao tiáº¿p collective nhÆ° NCCL cá»‘ gáº¯ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n chiáº¿n lÆ°á»£c giao tiáº¿p Ä‘Ãºng dá»±a trÃªn cÃ¡c cÃ¢n nháº¯c nhÆ° khá»‘i lÆ°á»£ng dá»¯ liá»‡u vÃ  topology máº¡ng. Trong cÃ¡c cá»¥m DGX, Ä‘á»‘i vá»›i cÃ¡c hoáº¡t Ä‘á»™ng collective trong má»™t node, ring reduce Ä‘Æ°á»£c thá»±c hiá»‡n trong khi tree reduction Ä‘Æ°á»£c thá»±c hiá»‡n qua cÃ¡c node. ChÃºng ta tháº¥y ráº±ng do Ä‘Ã³ viá»‡c sá»­ dá»¥ng cÃ¡c hoáº¡t Ä‘á»™ng collective tÃ­ch há»£p nhÆ° Allreduce dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t hÆ¡n khi giáº£i mÃ£ tá»« ngá»¯ cáº£nh dÃ i qua nhiá»u GPU so vá»›i viá»‡c Ã©p buá»™c máº«u giao tiáº¿p point to point cá»§a Ring Attention. ChÃºng tÃ´i cho tháº¥y cÃ¡ch chiáº¿n lÆ°á»£c sau vÆ°á»£t trá»™i Ring Attention khi giáº£i mÃ£ tá»« ngá»¯ cáº£nh ráº¥t dÃ i qua nhiá»u GPU.

Trong cÃ¡c thÃ­ nghiá»‡m thá»±c nghiá»‡m, chÃºng tÃ´i sá»­ dá»¥ng Flash Attention 2 (Dao, 2023) trong má»—i thiáº¿t bá»‹, cáº£ cho thuáº­t toÃ¡n cá»§a chÃºng tÃ´i vÃ  cho Ring AttentionÂ². ChÃºng tÃ´i cung cáº¥p má»™t implementation JAX Ä‘Æ¡n giáº£n cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i trong Phá»¥ lá»¥c D. LÆ°u Ã½ ráº±ng phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i nhÃ¢n báº£n Flash Decoding (fla, 2024) ngoáº¡i trá»« trong trÆ°á»ng há»£p Ä‘Ã³, viá»‡c song song hÃ³a xáº£y ra á»Ÿ cáº¥p Ä‘á»™ cá»§a cÃ¡c streaming multiprocessor (SM) khÃ¡c nhau trong má»™t GPU trong khi chÃºng tÃ´i song song hÃ³a giá»¯a cÃ¡c GPU khÃ¡c nhau. Táº¥t cáº£ cÃ¡c tÃ­nh toÃ¡n Ä‘Æ°á»£c thá»±c hiá»‡n trong BF16.

## 6 Káº¾T QUáº¢

TÆ°Æ¡ng tá»± nhÆ° Ring Attention, Tree Attention lÃ  má»™t tÃ­nh toÃ¡n chÃ­nh xÃ¡c cá»§a attention. VÃ¬ cÃ¡c metric huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ giá»‘ng nhÆ° Ä‘á»‘i vá»›i attention, káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i táº­p trung chá»§ yáº¿u vÃ o latency trong pháº§n 6.1, sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh trong pháº§n 6.2 vÃ  khá»‘i lÆ°á»£ng giao tiáº¿p trong pháº§n 6.3. VÃ¬ thuáº­t toÃ¡n cá»§a chÃºng tÃ´i tÃ­nh toÃ¡n káº¿t quáº£ giá»‘ng há»‡t sá»‘ nhÆ° forward pass cá»§a attention tiÃªu chuáº©n, káº¿t quáº£ hiá»‡u suáº¥t cá»§a chÃºng tÃ´i chuyá»ƒn giao liá»n máº¡ch Ä‘áº¿n cÃ¡c kiáº¿n trÃºc transformer.

ChÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m trong Pháº§n 6.1 Ä‘áº¿n 6.3 trÃªn má»™t cá»¥m DGX H100 gá»“m 16 node, má»—i node chá»©a 8 GPU H100. Táº¥t cáº£ GPU trong node Ä‘Æ°á»£c káº¿t ná»‘i qua topology NVLINK 4.0 all-to-all (900GBps). CÃ¡c node Ä‘Æ°á»£c káº¿t ná»‘i vá»›i nhau qua 8 interconnect InfiniBand NDR má»—i node (1 trÃªn má»—i GPU), má»—i interconnect cung cáº¥p 400 Gbps (dáº«n Ä‘áº¿n bÄƒng thÃ´ng injection node tá»•ng há»£p 3.2 Tbps).

ChÃºng tÃ´i cÅ©ng cho tháº¥y so sÃ¡nh Ring Attention vÃ  Tree Attention khi Ä‘Æ°á»£c sá»­ dá»¥ng trong mÃ´ hÃ¬nh Llama 3 (Grattafiori et al., 2024) trong Pháº§n 6.4 vÃ  C.3 trÃªn cÃ¡c loáº¡i GPU vÃ  interconnect khÃ¡c nhau: 8 GPU H100 vá»›i NVLINK 4.0, 8 GPU AMD MI300X vá»›i AMD infinity fabric cho giao tiáº¿p intra-node vÃ  RoCE cho giao tiáº¿p inter-node, vÃ  2 GPU RTX 4090 vá»›i interconnect PCIe.

### 6.1 LATENCY

Vá» máº·t tÃ­nh há»¯u Ã­ch thá»±c táº¿, nghiÃªn cá»©u cá»§a chÃºng tÃ´i vá» hÃ m nÄƒng lÆ°á»£ng Ä‘Ã£ lÃ m sÃ¡ng tá» má»™t kháº£ nÄƒng song song hÃ³a chÆ°a Ä‘Æ°á»£c chÃº Ã½ trÆ°á»›c Ä‘Ã¢y bÃªn trong tÃ­nh toÃ¡n attention â€“ Ä‘Ã³ lÃ  reduction cá»§a logsumexp qua chiá»u chuá»—i, cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° má»™t Allreduce song song. NhÆ° Ä‘Ã£ nÃªu trong Äá»‹nh lÃ½ 1, vá» máº·t lÃ½ thuyáº¿t cÃ³ thá»ƒ thá»±c hiá»‡n attention, má»—i query nhÆ° cÃ¡c hoáº¡t Ä‘á»™ng song song N/p + log(p) thay vÃ¬ N, trong Ä‘Ã³ sá»‘ háº¡ng logarithmic tá»· lá»‡ vá»›i sá»‘ thiáº¿t bá»‹ cÃ³ sáºµn Ä‘á»ƒ song song hÃ³a. Khi attention Ä‘Æ°á»£c chia sáº» qua nhiá»u thiáº¿t bá»‹, speedup tiá»‡m cáº­n nÃ y táº¡o ra speedup Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p thay tháº¿ cho giáº£i mÃ£.

Äá»ƒ kiá»ƒm tra thá»±c nghiá»‡m lá»£i Ã­ch lÃ½ thuyáº¿t cá»§a phÆ°Æ¡ng phÃ¡p Tree Attention, chÃºng tÃ´i tÃ­nh toÃ¡n latency báº±ng cÃ¡ch Ä‘o thá»i gian cáº§n thiáº¿t Ä‘á»ƒ thá»±c hiá»‡n giáº£i mÃ£ cho cÃ¡c Ä‘á»™ dÃ i chuá»—i khÃ¡c nhau vÃ  sá»‘ node H100 thay Ä‘á»•i. ChÃºng tÃ´i so sÃ¡nh Tree Attention vá»›i thá»i gian thá»±c thi Ring Attention cá»§a chÃ­nh chÃºng tÃ´i trong HÃ¬nh 3. Cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»u sá»­ dá»¥ng Flash Attention 2 Dao (2023) cho tÃ­nh toÃ¡n attention trÃªn GPU cÃ¡ nhÃ¢n. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i benchmark trÃªn má»™t khá»‘i attention tiÃªu chuáº©n gá»“m 16 head cÃ³ chiá»u 128 qua cÃ¡c Ä‘á»™ dÃ i chuá»—i khÃ¡c nhau.

Káº¿t quáº£ latency cá»§a chÃºng tÃ´i cho tháº¥y cÃ¡ch Tree Attention cáº£i thiá»‡n so vá»›i Ring Attention khi chÃºng ta tÄƒng Ä‘á»™ dÃ i chuá»—i trong HÃ¬nh 3(a) vÃ  tÄƒng sá»‘ GPU trong HÃ¬nh 3(b). Äá»ƒ lÃ m ná»•i báº­t tá»‘t hÆ¡n cÃ¡c xu hÆ°á»›ng thá»i gian thá»±c thi vá»›i Ä‘á»™ dÃ i chuá»—i tÄƒng, chÃºng tÃ´i cÅ©ng Ä‘Ã£ thÃªm thá»i gian thá»±c thi tÆ°Æ¡ng Ä‘á»‘i cá»§a cáº£ hai phÆ°Æ¡ng phÃ¡p so vá»›i thá»i gian thá»±c thi cá»§a ring attention á»Ÿ Ä‘á»™ dÃ i chuá»—i 80k. Vá»›i thá»i gian thá»±c thi tÆ°Æ¡ng Ä‘á»‘i trong HÃ¬nh 3(a), chÃºng ta nháº­n tháº¥y ráº±ng thá»i gian thá»±c thi cá»§a Tree attention trá»Ÿ nÃªn pháº³ng khi sá»‘ GPU tÄƒng, trong khi thá»i gian thá»±c thi tÆ°Æ¡ng Ä‘á»‘i cá»§a Ring Attention tiáº¿p tá»¥c tÄƒng. NhÆ° cÃ¡c Ä‘á»“ thá»‹ chá»©ng minh, khi chÃºng ta má»Ÿ rá»™ng Ä‘á»™ dÃ i chuá»—i hoáº·c sá»‘ GPU, khoáº£ng cÃ¡ch giá»¯a thá»i gian thá»±c thi Tree Attention vÃ  Ring Attention má»Ÿ rá»™ng tiá»‡m cáº­n.

ÄÃ¡ng chÃº Ã½, Tree Attention Ä‘áº¡t Ä‘Æ°á»£c speedup gáº§n Ã—8 khi chÃºng tÃ´i sá»­ dá»¥ng 128 GPU trÃªn Ä‘á»™ dÃ i chuá»—i 5.12M. ChÃºng tÃ´i mong Ä‘á»£i xu hÆ°á»›ng nÃ y tiáº¿p tá»¥c cho cÃ¡c Ä‘á»™ dÃ i chuá»—i lá»›n hÆ¡n. Xin lÆ°u Ã½ ráº±ng cá»¥m DGX cá»§a chÃºng tÃ´i Ä‘Æ°á»£c táº¡o thÃ nh tá»« 16 node má»—i node cÃ³ 8 GPU. Káº¿t quáº£ cho 8 GPU sá»­ dá»¥ng má»™t node, cho 64 GPU sá»­ dá»¥ng 8 node vÃ  cho 128 GPU sá»­ dá»¥ng 16 node.

### 6.2 CHI PHÃ Bá»˜ NHá»š

[HÃ¬nh 4: Sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh cá»§a má»™t khá»‘i attention Ä‘Æ¡n vá»›i Tree Attention vs Ring Attention khi Ä‘Æ°á»£c chia sáº» giá»¯a hai RTX 4090. Káº¿t quáº£ Ä‘Æ°á»£c láº¥y báº±ng JAX memory profiler trÃªn má»™t GPU. Sá»± khÃ¡c biá»‡t vá» bá»™ nhá»› Ä‘á»‰nh tá»· lá»‡ vá»›i kÃ­ch thÆ°á»›c áº©n vÃ  Ä‘á»™ dÃ i chuá»—i.]

Äá»ƒ thá»±c hiá»‡n Ring Attention vá»›i KV cache phÃ¢n tÃ¡n, cáº§n pháº£i broadcast query tÆ°Æ¡ng á»©ng vá»›i pháº§n tá»­ cuá»‘i cÃ¹ng cá»§a chuá»—i trá»Ÿ láº¡i táº¥t cáº£ cÃ¡c thiáº¿t bá»‹, nhÆ° Ä‘Æ°á»£c nÃªu trong bÆ°á»›c 2 cá»§a Thuáº­t toÃ¡n 1. Má»—i thiáº¿t bá»‹ sau Ä‘Ã³ sáº½ giá»¯ má»™t tuple (q, k_Ã¢, v_Ã¢), trong Ä‘Ã³ Ã¢ lÃ  chá»‰ sá»‘ chunk, bao gá»“m vector query vÃ  má»™t chunk Ä‘á»‹a phÆ°Æ¡ng cá»§a keys vÃ  values cá»¥ thá»ƒ cho chunk chuá»—i trÃªn thiáº¿t bá»‹ Ä‘Ã³. Chi phÃ­ bá»™ nhá»› Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c Ä‘á»‘i tÆ°á»£ng nÃ y giá»‘ng nhÆ° Ä‘á»‘i vá»›i Tree Decoding.

NgoÃ i ra, Ring Attention pháº£i lÆ°u trá»¯ k_Ã¢', v_Ã¢' Ä‘áº¿n tá»« thiáº¿t bá»‹ lÃ¡ng giá»ng vÃ  chunk cá»§a output o cÃ³ cÃ¹ng hÃ¬nh dáº¡ng vá»›i query Ä‘Æ°á»£c giá»¯ bá»Ÿi thiáº¿t bá»‹ Ä‘Ã³. NgÆ°á»£c láº¡i, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i yÃªu cáº§u lÆ°u trá»¯ thay vÃ o Ä‘Ã³ chá»‰ chunk giao tiáº¿p cá»§a numerator n, denominator d vÃ  max m. ChÃºng tÃ´i khÃ´ng pre-allocate má»™t tensor output mÃ  thay vÃ o Ä‘Ã³ chá»‰ tráº£ vá» káº¿t quáº£ cá»§a viá»‡c thá»±c hiá»‡n Allreduce cho numerator chia cho Allreduced denominator. TÃ³m láº¡i chÃºng ta cÃ³ cÃ¡c chi phÃ­ bá»™ nhá»› Ä‘á»‰nh sau cho Ring vÃ  Tree attention:

Mem_ring = 4btd + 2bd (8)
Mem_tree = 2btd + 2bd + 2bn_h, (9)

trong Ä‘Ã³ d = d_h Ã— n_h, cho head size d_h vÃ  n_h sá»‘ head, b biá»ƒu thá»‹ batch size vÃ  t = N/p. NhÆ° váº­y, miá»…n lÃ  2bn_h â‰¤ 2btd, Ä‘iá»u mÃ  háº§u nhÆ° luÃ´n luÃ´n xáº£y ra trong cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i luÃ´n cÃ³ chi phÃ­ bá»™ nhá»› Ä‘á»‰nh tháº¥p hÆ¡n so vá»›i Ring Attention.

ChÃºng tÃ´i Ä‘o thá»±c nghiá»‡m viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh cho phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÃ  Ring Attention Ä‘á»ƒ cho tháº¥y ráº±ng thá»±c sá»± viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ã­t hÆ¡n Ä‘Ã¡ng ká»ƒ cho Tree Attention trong HÃ¬nh 4. NhÆ° Ä‘Æ°á»£c dá»± Ä‘oÃ¡n bá»Ÿi lÃ½ thuyáº¿t, viá»‡c má»Ÿ rá»™ng kÃ­ch thÆ°á»›c áº©n hoáº·c Ä‘á»™ dÃ i chuá»—i má»Ÿ rá»™ng viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘á»‰nh Ring Attention khoáº£ng 2Ã— nhanh hÆ¡n Tree Attention. VÃ­ dá»¥, viá»‡c tÄƒng gáº¥p Ä‘Ã´i kÃ­ch thÆ°á»›c áº©n tá»« 2048 lÃªn 4096, tÄƒng gáº¥p Ä‘Ã´i khoáº£ng cÃ¡ch bá»™ nhá»› Ä‘á»‰nh giá»¯a hai phÆ°Æ¡ng phÃ¡p, tá»« 524MB lÃªn 1040MB.

### 6.3 KHá»I LÆ¯á»¢NG GIAO TIáº¾P

Äá»‘i vá»›i chiáº¿n lÆ°á»£c giao tiáº¿p P2P cá»§a Ring Attention, tá»•ng khá»‘i lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c giao tiáº¿p giá»¯a cÃ¡c thiáº¿t bá»‹ (theo Ä‘Æ¡n vá»‹ sá»‘ pháº§n tá»­ tensor) má»—i láº§n láº·p tá»· lá»‡ vá»›i p vÃ  Ä‘Æ°á»£c cho bá»Ÿi:

V_ring = 2btd Ã— p (10)

trong Ä‘Ã³ p lÃ  sá»‘ thiáº¿t bá»‹. Yáº¿u tá»‘ Ä‘áº§u tiÃªn Ä‘áº¿n tá»« viá»‡c Ä‘áº¿m tá»•ng sá»‘ pháº§n tá»­ Ä‘Æ°á»£c giao tiáº¿p tÆ°Æ¡ng á»©ng vá»›i {(k_Ã¢, v_Ã¢), Ã¢=1,Â·Â·Â·,t}, tá»©c lÃ 

numel({(k_Ã¢, v_Ã¢), Ã¢=1,Â·Â·Â·,t}) = 2btd. (11)

Chiáº¿n lÆ°á»£c Allreduce chÃºng tÃ´i sá»­ dá»¥ng trong Tree Decoding yÃªu cáº§u khá»‘i lÆ°á»£ng sau Anthony et al. (2024):

V_Allreduce = 2 Ã— (p-1)/p Ã— numel. (12)

ChÃºng tÃ´i giao tiáº¿p má»™t shard cá»§a numerator, denominator vÃ  max, yÃªu cáº§u:

numel(n,d,m) = bd + 2bn_h. (13)

LÆ°u Ã½ ráº±ng chÃºng tÃ´i Ä‘áº§u tiÃªn thá»±c hiá»‡n trÃªn thiáº¿t bá»‹ cÃ¡c reduction Ä‘á»‹a phÆ°Æ¡ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c numerator vÃ  denominator Ä‘á»‹a phÆ°Æ¡ng trÃªn má»—i thiáº¿t bá»‹ Ä‘iá»u nÃ y do Ä‘Ã³ lÃ m cho t, tá»©c lÃ  kÃ­ch thÆ°á»›c cá»§a chunk chuá»—i Ä‘á»‹a phÆ°Æ¡ng khÃ´ng xuáº¥t hiá»‡n trong biá»ƒu thá»©c trÃªn. Sau Ä‘Ã³ chÃºng ta cÃ³ Ä‘Æ°á»£c:

V_Tree = 2(p-1)/p Ã— (bd + 2bn_h). (14)

PhÃ¢n tÃ­ch lÃ½ thuyáº¿t cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng má»—i láº§n láº·p thuáº­t toÃ¡n cá»§a chÃºng tÃ´i duy trÃ¬ khá»‘i lÆ°á»£ng giao tiáº¿p tháº¥p hÆ¡n Ring Attention. Tuy nhiÃªn lÆ°u Ã½ ráº±ng Ring Attention khi Ä‘Æ°á»£c thá»±c hiá»‡n trong thiáº¿t láº­p huáº¥n luyá»‡n vá»›i nhiá»u query overlap giao tiáº¿p vÃ  tÃ­nh toÃ¡n Ä‘á»ƒ áº©n chi phÃ­ giao tiáº¿p cá»§a nÃ³. Tuy nhiÃªn, viá»‡c overlap giao tiáº¿p vÃ  tÃ­nh toÃ¡n trong trÆ°á»ng há»£p giáº£i mÃ£ lÃ  khÃ´ng kháº£ thi vÃ¬ tÃ­nh toÃ¡n attention trÃªn má»™t GPU Ä‘Æ¡n nhanh nhÆ° tháº¿ nÃ o so vá»›i thá»i gian giao tiáº¿p chunk cá»§a keys vÃ  values giá»¯a hai thiáº¿t bá»‹.

Cá»¥ thá»ƒ, hÃ£y láº¥y vÃ­ dá»¥ vá» giáº£i mÃ£ tá»« ngá»¯ cáº£nh cÃ³ Ä‘á»™ dÃ i 640000 Ä‘Æ°á»£c chia giá»¯a 8 GPU trong má»™t node. HÃ£y láº¥y kÃ­ch thÆ°á»›c áº©n lÃ  2048 vÃ  cá»‘ Ä‘á»‹nh kiá»ƒu dá»¯ liá»‡u cá»§a chÃºng ta lÃ  bfloat16. Má»—i thiáº¿t bá»‹ cho giáº£i mÃ£ máº¥t O(10â»âµ) giÃ¢y Ä‘á»ƒ thá»±c hiá»‡n tÃ­nh toÃ¡n Flash Attention. Thá»i gian Ä‘á»ƒ di chuyá»ƒn keys vÃ  values cÃ³ kÃ­ch thÆ°á»›c tÆ°Æ¡ng á»©ng giá»¯a cÃ¡c GPU liá»n ká» nhÆ° trong HÃ¬nh 2 lÃ  khoáº£ng O(10â»Â³) giÃ¢y. Latency phÃ¡t sinh giá»¯a cÃ¡c node tháº­m chÃ­ cÃ²n lá»›n hÆ¡n vÃ  do Ä‘Ã³ overlap khÃ´ng kháº£ thi do sá»± chÃªnh lá»‡ch nÃ y vá» timescale.

### 6.4 HIá»†U SUáº¤T Vá»šI MÃ” HÃŒNH LLAMA TRANSFORMER

Äá»ƒ cho tháº¥y ráº±ng Tree attention cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c á»©ng dá»¥ng tháº¿ giá»›i thá»±c, chÃºng tÃ´i cÅ©ng Ä‘o throughput end-to-end vá»›i mÃ´ hÃ¬nh Llama 3.1 8B Grattafiori et al. (2024) trÃªn cÃ¡c chuá»—i prompt cÃ³ Ä‘á»™ dÃ i 32k, 64k, 128k vÃ  256k sá»­ dá»¥ng ring attention hoáº·c tree attention cho giáº£i mÃ£ (vá»›i prefill) 10 token trong Báº£ng 1. ChÃºng tÃ´i cháº¡y cÃ¡c thÃ­ nghiá»‡m nÃ y trÃªn 8 GPU H100 trong má»™t cá»¥m DGX (káº¿t ná»‘i vá»›i NVLink) cÅ©ng nhÆ° 4 GPU MI300X trong má»™t cá»¥m AMD Ä‘Æ°á»£c káº¿t ná»‘i vá»›i AMD infinity fabric. Trong Báº£ng 2 cá»§a Phá»¥ lá»¥c C.3, chÃºng tÃ´i cÅ©ng cho tháº¥y káº¿t quáº£ throughput tÆ°Æ¡ng tá»± trÃªn 2 GPU RTX 4090 Ä‘Æ°á»£c káº¿t ná»‘i vá»›i PCIe. Trong táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p chÃºng ta tháº¥y ráº±ng Tree attention cho giáº£i mÃ£ cÃ³ latency tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i Ring Attention cho giáº£i mÃ£ vá»›i giai Ä‘oáº¡n prefill. Ring Attention nhanh hÆ¡n lÃªn Ä‘áº¿n Ã—4 khi sá»­ dá»¥ng 8x H100 vÃ  lÃªn Ä‘áº¿n Ã—3 nhanh hÆ¡n khi sá»­ dá»¥ng 4x MI300x. ChÃºng tÃ´i mong Ä‘á»£i khoáº£ng cÃ¡ch nÃ y tÄƒng khi chÃºng ta tÄƒng sá»‘ node.

Trong khi chÃºng tÃ´i Ä‘Ã£ tháº£o luáº­n trÆ°á»›c Ä‘Ã¢y ráº±ng Ring Attention hoáº¡t Ä‘á»™ng tá»‘t nháº¥t khi Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i Ring Topology cá»§a cÃ¡c cá»¥m TPU, Báº£ng 1 vÃ  2 cho tháº¥y ráº±ng káº¿t quáº£ Tree Attention tá»•ng quÃ¡t hÃ³a tá»‘t cho cÃ¡c loáº¡i há»‡ thá»‘ng khÃ¡c nhau, sá»‘ GPU, giao thá»©c giao tiáº¿p vÃ  topology máº¡ng.

**Báº£ng 1:** So sÃ¡nh Thá»i gian Giáº£i mÃ£ Trung bÃ¬nh (tÃ­nh báº±ng giÃ¢y) vá»›i giai Ä‘oáº¡n prefill, sá»­ dá»¥ng mÃ´ hÃ¬nh Llama 3.1 8B vá»›i Tree Attention (cá»§a chÃºng tÃ´i) vÃ  Ring Attention (SOTA) qua cÃ¡c Ä‘á»™ dÃ i chuá»—i vÃ  loáº¡i GPU khÃ¡c nhau. Káº¿t quáº£ trung bÃ¬nh vÃ  standard error (Â±) Ä‘Æ°á»£c tÃ­nh toÃ¡n sá»­ dá»¥ng 10 láº§n cháº¡y thá»­.

| Äá»™ dÃ i Chuá»—i | 8x H100s |  | Speedup | 4x MI300x |  | Speedup |
|-------------|----------|----------|---------|-----------|----------|---------|
|  | Tree Attn | Ring Attn |  | Tree Attn | Ring Attn |  |
| 32k | 0.60Â±0.15 | 2.57Â±0.35 | Ã—4 | 1.05Â±0.01 | 3.57Â±0.25 | Ã—3 |
| 64k | 1.08Â±0.10 | 4.42Â±0.38 | Ã—4 | 2.36Â±0.01 | 7.33Â±0.25 | Ã—3 |
| 128k | 2.68Â±0.28 | 6.38Â±0.58 | Ã—2 | 6.43Â±0.25 | 16.40Â±0.40 | Ã—3 |
| 256k | 2.89Â±0.62 | 8.19Â±1.07 | Ã—3 | 15.30Â±4.93 | 35.12Â±5.02 | Ã—2 |

## 7 THáº¢O LUáº¬N VÃ€ Káº¾T LUáº¬N

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘Ã£ suy ra hÃ m nÄƒng lÆ°á»£ng cho self-attention vÃ  chá»©ng minh cÃ¡ch tÃ­nh toÃ¡n Ä‘áº¡o hÃ m cá»§a hÃ m nÃ y cung cáº¥p má»™t phÆ°Æ¡ng phÃ¡p má»›i vÃ  hiá»‡u quáº£ Ä‘á»ƒ tÃ­nh toÃ¡n attention song song. Lá»£i tháº¿ nÃ y Ä‘áº·c biá»‡t rÃµ rÃ ng khi thá»±c hiá»‡n giáº£i mÃ£ qua nhiá»u thiáº¿t bá»‹, trong trÆ°á»ng há»£p Ä‘Ã³ Tree Attention cá»§a chÃºng tÃ´i cho phÃ©p chÃºng tÃ´i vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i SOTA Ring Attention vá»›i má»™t thuáº­t toÃ¡n vÆ°á»£t trá»™i tiá»‡m cáº­n, vá»›i speedup Ã—8 khi chÃºng tÃ´i sá»­ dá»¥ng 128 GPU trÃªn Ä‘á»™ dÃ i chuá»—i 5.12M. ChÃºng tÃ´i cÅ©ng tháº¥y ráº±ng hoáº¡t Ä‘á»™ng AllReduce mÃ  chÃºng tÃ´i sá»­ dá»¥ng liÃªn quan Ä‘áº¿n viá»‡c gá»­i cÃ¡c Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c rÃºt gá»n má»™t pháº§n, Ä‘iá»u nÃ y giáº£m Ä‘Ã¡ng ká»ƒ khá»‘i lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c giao tiáº¿p cÅ©ng nhÆ° yÃªu cáº§u bá»™ nhá»› Ä‘á»‰nh. Trong má»™t á»©ng dá»¥ng tháº¿ giá»›i thá»±c, sá»­ dá»¥ng mÃ´ hÃ¬nh Llama 3.1 vá»›i 1B vÃ  8B tham sá»‘, chÃºng tÃ´i tháº¥y ráº±ng giáº£i mÃ£ vá»›i giai Ä‘oáº¡n prefill sá»­ dá»¥ng Tree Attention mang láº¡i cho chÃºng tÃ´i speedup Ã—3-5 so vá»›i Ring Attention. HÆ¡n ná»¯a, báº±ng cÃ¡ch kiá»ƒm tra phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i trÃªn cÃ¡c loáº¡i cá»¥m GPU khÃ¡c nhau bao gá»“m AMD MI300x, chÃºng tÃ´i cho tháº¥y ráº±ng Tree Attention tá»•ng quÃ¡t hÃ³a ráº¥t tá»‘t cho cÃ¡c giao thá»©c giao tiáº¿p vÃ  topology máº¡ng khÃ¡c nhau.

## TÃ€I LIá»†U THAM KHáº¢O

[Pháº§n tÃ i liá»‡u tham kháº£o Ä‘Æ°á»£c giá»¯ nguyÃªn nhÆ° trong báº£n gá»‘c]

## PHá»¤ Lá»¤C A: CÃ”NG TRÃŒNH LIÃŠN QUAN KHÃC

CÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘Ã£ cá»‘ gáº¯ng má»™t cÃ´ng thá»©c Bayesian cá»§a attention báº±ng cÃ¡ch suy ra má»™t mÃ´ hÃ¬nh sinh xÃ¡c suáº¥t khá»›p vá»›i cÃ¡c hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c thá»±c hiá»‡n trong má»™t hoáº¡t Ä‘á»™ng self-attention Singh & Buckley (2023). Äiá»u nÃ y theo má»™t dÃ²ng cÃ´ng trÃ¬nh liÃªn há»‡ self-attention vá»›i kiáº¿n trÃºc Hopfield Network Ä‘Æ°á»£c nghiÃªn cá»©u ká»¹ Ramsauer et al. (2020). Ã tÆ°á»Ÿng lÃ  trong khi Hopfield Network Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a vá» máº·t Ä‘á»™ng lá»±c há»c trÃªn má»™t landscape nÄƒng lÆ°á»£ng, cÃ¹ng má»™t hÃ¬nh áº£nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t diá»…n giáº£i Bayesian báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh hÃ m nÄƒng lÆ°á»£ng vá»›i má»™t hÃ m free energy biáº¿n phÃ¢n vÃ  do Ä‘Ã³ suy ra mÃ´ hÃ¬nh sinh ngáº§m trong cáº­p nháº­t self-attention.

Äáº·c biá»‡t, xem xÃ©t hÃ m nÄƒng lÆ°á»£ng Ä‘Æ°á»£c Ä‘á» xuáº¥t trong Hoover et al. (2023), Ä‘Ã³ lÃ  logsumexp. VÃ¬ gradient trong quy táº¯c cáº­p nháº­t cá»§a bÃ i bÃ¡o Ä‘Ã³ Ä‘Æ°á»£c láº¥y theo Ä‘áº§u vÃ o cá»§a khá»‘i, hÃ m káº¿t quáº£ lÃ  má»™t phiÃªn báº£n modified cá»§a hoáº¡t Ä‘á»™ng self-attention. TÆ°Æ¡ng tá»±, quy táº¯c cáº­p nháº­t trong Ramsauer et al. (2020) yÃªu cáº§u viá»‡c tying cá»§a má»™t sá»‘ weight nháº¥t Ä‘á»‹nh (K vÃ  V) trong hoáº¡t Ä‘á»™ng attention. Äiá»u nÃ y háº¡n cháº¿ derivation Hopfield Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a cÃ¡c máº¡ng lÆ°u trá»¯ auto-associative, trong khi transformer attention lÃ  hetero-associative.

Má»™t cÃ´ng trÃ¬nh liÃªn quan Ä‘Ã¡ng chÃº Ã½ khÃ¡c lÃ  Feng et al. (2024), trong Ä‘Ã³ cÃ¡c tÃ¡c giáº£ Ä‘Æ°a ra nhá»¯ng quan sÃ¡t tÆ°Æ¡ng tá»± nhÆ° chÃºng tÃ´i trong pháº§n 5 vá» cÃ¡ch cÃ¡c hoáº¡t Ä‘á»™ng káº¿t há»£p trong tÃ­nh toÃ¡n attention cÃ³ thá»ƒ Ä‘Æ°á»£c song song hÃ³a hiá»‡u quáº£ Ä‘á»ƒ thÃºc Ä‘áº©y má»™t kiáº¿n trÃºc RNN modified dá»±a trÃªn attention cho mÃ´ hÃ¬nh hÃ³a chuá»—i.

Trong khi hÃ m nÄƒng lÆ°á»£ng nÃ y tá»± nÃ³ chá»§ yáº¿u lÃ  má»™t tÃ² mÃ² toÃ¡n há»c vÃ  lÃ½ thuyáº¿t, chÃºng tÃ´i chá»©ng minh dÆ°á»›i Ä‘Ã¢y ráº±ng khi káº¿t há»£p vá»›i automatic differentiation, cÃ´ng thá»©c cá»§a chÃºng tÃ´i tá»± nhiÃªn dáº«n Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n song song cÃ³ hiá»‡u quáº£ cao Ä‘á»ƒ tÃ­nh toÃ¡n attention vÃ  thá»±c hiá»‡n giáº£i mÃ£, Ä‘áº·c biá»‡t qua nhiá»u thiáº¿t bá»‹.

## PHá»¤ Lá»¤C B: THÃŠM Ná»€N Táº¢NG Vá»€ HOáº T Äá»˜NG TREE REDUCTION

Má»™t hoáº¡t Ä‘á»™ng tree reduction lÃ  má»™t chiáº¿n lÆ°á»£c phÃ¢n cáº¥p Ä‘á»ƒ thá»±c hiá»‡n má»™t hoáº¡t Ä‘á»™ng reduction (vÃ­ dá»¥, tá»•ng, tÃ­ch, maximum, minimum) trÃªn má»™t táº­p há»£p cÃ¡c pháº§n tá»­ dá»¯ liá»‡u má»™t cÃ¡ch hiá»‡u quáº£, Ä‘áº·c biá»‡t trong tÃ­nh toÃ¡n song song. PhÆ°Æ¡ng phÃ¡p nÃ y giáº£m Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n tá»•ng thá»ƒ vÃ  cho phÃ©p sá»­ dá»¥ng hiá»‡u quáº£ cÃ¡c tÃ i nguyÃªn xá»­ lÃ½ song song. ÄÃ¢y lÃ  cÃ¡ch nÃ³ hoáº¡t Ä‘á»™ng:

â€¢ **Chia váº¥n Ä‘á» thÃ nh cÃ¡c tÃ¡c vá»¥ nhá» hÆ¡n:** Dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Æ°á»£c chia thÃ nh cÃ¡c chunk nhá» hÆ¡n, vÃ  hoáº¡t Ä‘á»™ng reduction Ä‘Æ°á»£c thá»±c hiá»‡n theo cáº·p giá»¯a cÃ¡c pháº§n tá»­ liá»n ká» trong cÃ¡c chunk nÃ y.

â€¢ **Táº¡o thÃ nh cáº¥u trÃºc giá»‘ng cÃ¢y:** Káº¿t quáº£ tá»« cáº¥p Ä‘á»™ reduction Ä‘áº§u tiÃªn tá»± chÃºng Ä‘Æ°á»£c reduced theo cáº·p trong cáº¥p Ä‘á»™ tiáº¿p theo. Äiá»u nÃ y tiáº¿p tá»¥c cho Ä‘áº¿n khi toÃ n bá»™ dataset Ä‘Æ°á»£c reduced thÃ nh má»™t káº¿t quáº£ duy nháº¥t.

â€¢ **Tá»•ng há»£p láº·p hoáº·c Ä‘á»‡ quy:** Viá»‡c tá»•ng há»£p thÆ°á»ng theo máº«u cÃ¢y nhá»‹ phÃ¢n, nhÆ°ng cÃ¡c sá»‘ fan-in khÃ¡c (vÃ­ dá»¥, k-ary trees) cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng. Má»—i node trong cÃ¢y Ä‘áº¡i diá»‡n cho má»™t káº¿t quáº£ reduction partial, vÃ  root cá»§a cÃ¢y giá»¯ káº¿t quáº£ cuá»‘i cÃ¹ng.

VÃ¬ cáº¥u trÃºc cÃ¢y cÃ³ Ä‘á»™ sÃ¢u logarithmic Ä‘á»‘i vá»›i tá»•ng sá»‘ node, má»™t tree reduction cÃ³ thá»ƒ giáº£m tiá»‡m cáº­n sá»‘ bÆ°á»›c tá»•ng cáº§n thiáº¿t Ä‘á»ƒ thá»±c hiá»‡n má»™t hoáº¡t Ä‘á»™ng khi cÃ³ thá»ƒ tá»•ng há»£p káº¿t quáº£ partial, vÃ  ngoÃ i ra dá»… dÃ ng cho viá»‡c song song hÃ³a vÃ¬ k-ary trees cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a Ä‘á»ƒ khá»›p vá»›i sá»‘ processor cÃ³ sáºµn cho xá»­ lÃ½ song song. NgoÃ i ra, nhiá»u topology máº¡ng hiá»‡n táº¡i nhÆ° NVLINK vÃ  Infiniband cá»§a Nvidia, do lá»£i tháº¿ tá»± nhiÃªn cá»§a cáº¥u trÃºc cÃ¢y, Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i topology nhÆ° váº­y cÃ³ nghÄ©a lÃ  cÃ¡c hoáº¡t Ä‘á»™ng cÃ¢y lÃ  tá»± nhiÃªn vÃ  hiá»‡u quáº£ Ä‘á»ƒ thá»±c hiá»‡n.

## PHá»¤ Lá»¤C C: ATTENTION NHÆ¯ GRADIENT Cá»¦A Má»˜T HÃ€M NÄ‚NG LÆ¯á»¢NG VÃ€ DIá»„N GIáº¢I BAYESIAN

### C.1 Báº°NG CHá»¨NG Cá»¦A QUAN SÃT 1

á» Ä‘Ã¢y, chÃºng tÃ´i cho tháº¥y cÃ¡ch hoáº¡t Ä‘á»™ng self-attention cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° gradient cá»§a má»™t hÃ m nÄƒng lÆ°á»£ng. Äáº·c biá»‡t, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a má»™t hÃ m vÃ´ hÆ°á»›ng phá»¥ thuá»™c vÃ o keys, queries, values vÃ  ngoÃ i ra vÃ o má»™t vector phá»¥ trá»£ mÃ  chÃºng tÃ´i gá»i lÃ  nguá»“n Î¶. Nguá»“n lÃ  tham sá»‘ mÃ  chÃºng ta tÃ­nh gradient cá»§a hÃ m vÃ´ hÆ°á»›ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c hoáº¡t Ä‘á»™ng self-attention. ChÃºng ta cáº§n nguá»“n Ä‘á»ƒ viáº¿t ra hÃ m sinh cá»§a cÃ¡c moment cá»§a phÃ¢n phá»‘i á»Ÿ trÃªn. NÃ³ cÅ©ng lÃ  biáº¿n mÃ  chÃºng ta cÃ³ thá»ƒ Taylor-expand hÃ m sinh vÃ  trÃ­ch xuáº¥t cÃ¡c moment nhÆ° cÃ¡c há»‡ sá»‘ cá»§a cÃ¡c Ä‘Æ¡n thá»©c cá»§a Î¶ xuáº¥t hiá»‡n trong chuá»—i Taylor. Má»™t cÃ¡ch rÃµ rÃ ng, chÃºng ta muá»‘n tÃ¬m má»™t hÃ m F(q,k,v,Î¶) sao cho:

âˆ‘áµƒâ‚Œâ‚á´º softmax(qÂ·kâ‚)vâ‚ = âˆ‚F/âˆ‚Î¶|_{Î¶=0}. (15)

Thuáº­t ngá»¯ nÃ y Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cÃ´ng trÃ¬nh vá» cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn nÄƒng lÆ°á»£ng trong machine learning Beal (2003); LeCun et al. (2006); Song & Kingma (2021). Má»™t tÃ³m táº¯t cÃ¡c biáº¿n vÃ  chá»‰ sá»‘ Ä‘Æ°á»£c cung cáº¥p trong phá»¥ lá»¥c G.

ChÃºng tÃ´i Ä‘áº§u tiÃªn cho tháº¥y cÃ¡ch hÃ m nÄƒng lÆ°á»£ng Ä‘Æ°á»£c cho bá»Ÿi hÃ m sinh cumulant liÃªn quan Ä‘áº¿n phÃ¢n phá»‘i Ä‘Æ°á»£c cho bá»Ÿi Ä‘iá»ƒm attention. Láº¥y cáº£m há»©ng tá»« cÆ¡ há»c thá»‘ng kÃª, nÆ¡i má»™t hÃ m sinh cumulant tÆ°Æ¡ng tá»± Ä‘á»‹nh nghÄ©a Helmholtz Free energy (Landau & Lifshitz, 1958), chÃºng tÃ´i gá»i hÃ m sinh cumulant cá»§a chÃºng tÃ´i lÃ  hÃ m nÄƒng lÆ°á»£ng cho self-attention.

HÃ£y táº­p trung vÃ o trÆ°á»ng há»£p vá»›i má»™t query Ä‘Æ¡n. NhÆ° Ä‘Ã£ lÆ°u Ã½ á»Ÿ trÃªn, chÃºng tÃ´i táº­n dá»¥ng thá»±c táº¿ ráº±ng hoáº¡t Ä‘á»™ng attention cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° tÃ­nh toÃ¡n giÃ¡ trá»‹ ká»³ vá»ng cá»§a cÃ¡c vector v trong phÃ¢n phá»‘i Ä‘Æ°á»£c thiáº¿t láº­p bá»Ÿi Ä‘iá»ƒm attention z:

z = âŸ¨vâŸ© = âˆ‘áµƒâ‚Œâ‚á´º Pâ‚vâ‚ = (âˆ‘áµƒâ‚Œâ‚á´º e^{qÂ·k_a^T} vâ‚)/(âˆ‘áµ¢â‚Œâ‚á´º e^{qÂ·k_i^T}). (16)

Máº­t Ä‘á»™ xÃ¡c suáº¥t Ä‘Æ°á»£c cho bá»Ÿi:

Pâ‚ = e^{qÂ·k_a^T}/(âˆ‘áµ¢â‚Œâ‚á´º e^{qÂ·k_i^T}). (17)

ThÆ°á»ng thÃ¬ máº«u sá»‘ hoáº·c há»‡ sá»‘ chuáº©n hÃ³a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh vá»›i cÃ¡i gá»i lÃ  hÃ m phÃ¢n vÃ¹ng:

Z = âˆ‘áµƒâ‚Œâ‚á´º e^{qÂ·k_a^T}. (18)

BÃ¢y giá» chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n moment Ä‘áº§u tiÃªn cá»§a phÃ¢n phá»‘i xÃ¡c suáº¥t Ä‘Æ°á»£c cho á»Ÿ trÃªn báº±ng cÃ¡ch giá»›i thiá»‡u má»™t nguá»“n, Î¶ âˆˆ â„áµˆ. Trong trÆ°á»ng há»£p cá»§a chÃºng ta, vá»›i Î¶, chÃºng ta cÃ³ thá»ƒ má»Ÿ rá»™ng hÃ m phÃ¢n vÃ¹ng thÃ nh hÃ m:

Z(Î¶) = âˆ‘áµƒâ‚Œâ‚á´º e^{qÂ·k_a^T + Î¶Â·v_a^T}. (19)

BÃ¢y giá», chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n báº¥t ká»³ moment nÃ o cá»§a phÃ¢n phá»‘i nhÆ° há»‡ sá»‘ Taylor thá»© n cá»§a Z(Î¶)
âˆ€Aâ‚,Aâ‚‚,Â·Â·Â· âˆˆ {1,Â·Â·Â·,d_h}:

âŸ¨vâ‚â‚Â·Â·Â·vâ‚â‚™âŸ© = (1/Z) âˆ‚â¿Z(Î¶)/(âˆ‚Î¶â‚â‚Â·Â·Â·âˆ‚Î¶â‚â‚™)|_{Î¶=0}. (20)

NÃ³i cÃ¡ch khÃ¡c, chÃºng ta cÃ³ thá»ƒ viáº¿t Z(Î¶) nhÆ°:

Z(Î¶) = Z[1 + âŸ¨vâŸ©Î¶ + (1/2!)âŸ¨vâ‚â‚vâ‚â‚‚âŸ©Î¶â‚â‚Î¶â‚â‚‚ + Â·Â·Â·] (21)

Do Ä‘Ã³, moment Ä‘áº§u tiÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ°:

âŸ¨vâŸ© = (1/Z) âˆ‚Z/âˆ‚Î¶|_{Î¶=0}, (22)

cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° gradient cá»§a log cá»§a Z(Î¶):

âŸ¨vâŸ© = âˆ‚/âˆ‚Î¶ log Z(Î¶)|_{Î¶=0}. (23)

Äáº¡i lÆ°á»£ng nÃ y lÃ  hÃ m sinh, hay cÃ²n gá»i lÃ  free energy:

F = log âˆ‘â‚ exp(qÂ·k_a^T + Î¶Â·v_a^T). (24)

Äá»ƒ tÃ­nh toÃ¡n causal self-attention, chÃºng tÃ´i giá»›i thiá»‡u N nguá»“n Î¶áµ¢ má»—i nguá»“n âˆˆ â„áµˆ vÃ  láº¥y

F_tot = âˆ‘áµ¢â‚Œâ‚á´º Fáµ¢ = âˆ‘áµ¢â‚Œâ‚á´º log âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T + Î¶áµ¢Â·v_a^T). (25)

Viá»‡c cáº¯t ngáº¯n tá»•ng bÃªn trong lÃªn Ä‘áº¿n chá»‰ sá»‘ i lÃ  do causal masking.

BÃ¢y giá», Ä‘á»ƒ tÃ­nh toÃ¡n pháº§n tá»­ thá»© i cá»§a causal self-attention, chÃºng ta vi phÃ¢n theo Î¶áµ¢ vÃ  Ä‘áº·t nÃ³ báº±ng khÃ´ng:

âˆ‚F_tot/âˆ‚Î¶áµ¢,â‚|_{Î¶áµ¢=0,âˆ€i} = (âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T)vâ‚,â‚)/(âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T)). (26)

Viá»‡c tá»•ng quÃ¡t hÃ³a cho trÆ°á»ng há»£p multi-head attention lÃ  Ä‘Æ¡n giáº£n. Trong trÆ°á»ng há»£p nÃ y, cÃ³ má»™t key, query vÃ  value cho má»—i head. Äá»‘i vá»›i n_h tá»•ng sá»‘ head, hÃ m sinh cÃ³ dáº¡ng:

F_tot = âˆ‘áµ¢â‚Œâ‚á´º âˆ‘â‚•â‚Œâ‚â¿Ê° Fáµ¢,â‚•, (27)

trong Ä‘Ã³

Fáµ¢,â‚• = log âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢,â‚•Â·k_{h,a}^T + Î¶â‚•,áµ¢Â·v_{h,a}^T). (28)

Weight projection output Ä‘Æ°á»£c bao gá»“m trong Ä‘á»‹nh nghÄ©a cá»§a vâ±¼ á»Ÿ Ä‘Ã¢y, cÃ³ nghÄ©a lÃ 

váµ¦,â‚ = xáµ¦,BÌ„(W^O W^V)â‚,BÌ„ (29)

trong Ä‘Ã³ W^O âˆˆ â„áµˆÊ°Ã—â„áµˆáµ‰áµáµ‡ biá»ƒu thá»‹ má»™t slice kÃ­ch thÆ°á»›c head cá»§a weight projection output vÃ  BÌ„ âˆˆ {1,Â·Â·Â·,d_h} tráº£i dÃ i cÃ¡c chá»‰ sá»‘ intra-head. Trong kÃ½ hiá»‡u chá»‰ sá»‘ á»Ÿ trÃªn, cÃ¡c chá»‰ sá»‘ head Ä‘Æ°á»£c gáº¡ch ngang trong khi cÃ¡c chá»‰ sá»‘ khÃ´ng gian embedding khÃ´ng Ä‘Æ°á»£c gáº¡ch ngang. ChÃºng tÃ´i tiáº¿p tá»¥c táº­p trung vÃ o trÆ°á»ng há»£p single-head, vÃ¬ nÃ³ lÃ m cho trÃ¬nh bÃ y Ä‘Æ¡n giáº£n hÆ¡n, vÃ  viá»‡c tá»•ng quÃ¡t hÃ³a multi-head lÃ  ngay láº­p tá»©c. LÆ°u Ã½ ráº±ng chÃºng tÃ´i chá»©ng minh ráº±ng phÆ°Æ¡ng phÃ¡p hÃ m nÄƒng lÆ°á»£ng cá»§a chÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ tÃ­nh Ä‘áº¿n safe softmax trong Phá»¥ lá»¥c F.

### C.2 DIá»„N GIáº¢I BAYESIAN

Thá»±c táº¿ lÃ  cÃ³ thá»ƒ suy ra hoáº¡t Ä‘á»™ng self-attention nhÆ° viá»‡c tá»‘i thiá»ƒu hÃ³a má»™t hÃ m nÄƒng lÆ°á»£ng ngá»¥ Ã½ ráº±ng cÃ³ thá»ƒ cung cáº¥p má»™t diá»…n giáº£i Bayesian vá» self-attention báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh má»™t hÃ m likelihood vÃ  cho tháº¥y ráº±ng chÃºng ta cÃ³ thá»ƒ cÃ³ Ä‘Æ°á»£c forward pass cá»§a khá»‘i attention tá»« viá»‡c tÃ­nh toÃ¡n Æ°á»›c lÆ°á»£ng maximum a posteriori cá»§a likelihood nÃ y.

Äáº·c biá»‡t, chÃºng tÃ´i Ä‘á» xuáº¥t nhÆ° sau cho hÃ m log-likelihood:

Î“(Î¶,z) = âˆ‘áµ¢â‚Œâ‚á´º âˆ‘â‚â‚Œâ‚áµˆ záµ¢,â‚Î¶áµ¢,â‚ - F(Î¶,x). (30)

ChÃºng tÃ´i kÃ½ hiá»‡u báº±ng x Ä‘áº§u vÃ o cho khá»‘i self-attention tá»« Ä‘Ã³ chÃºng ta cÃ³ Ä‘Æ°á»£c q,k,v tá»« viá»‡c nhÃ¢n nÃ³ vá»›i cÃ¡c weight W^Q, W^K, W^V tÆ°Æ¡ng á»©ng. HÃ£y tá»‘i thiá»ƒu hÃ³a Ä‘iá»u trÃªn theo Î¶ vÃ  z Ä‘á»“ng thá»i:

âˆ‚Î“/âˆ‚Î¶áµ¢,â‚ = 0, âˆ‚Î“/âˆ‚záµ¢,â‚ = 0. (31)

CÃ¡c Ä‘iá»u kiá»‡n nÃ y Ä‘Æ°á»£c viáº¿t rÃµ rÃ ng Ä‘á»c

Î¶*áµ¢,â‚ = 0, z*áµ¢,â‚ = âˆ‚F/âˆ‚Î¶áµ¢,â‚. (32)

ÄÆ°a Ä‘iá»u kiá»‡n Ä‘áº§u tiÃªn vÃ o Ä‘iá»u kiá»‡n thá»© hai dáº«n Ä‘áº¿n attention forward pass:

z*áµ¢,â‚ = (âˆ‘áµƒâ‚Œâ‚â± e^{qáµ¢Â·k_a^T} vâ‚,â‚)/(âˆ‘áµ¦â‚Œâ‚â± e^{qáµ¢Â·k_b^T}). (33)

TÃ³m láº¡i, Ä‘iá»u nÃ y cÃ³ nghÄ©a chÃºng ta cÃ³ thá»ƒ cÃ³ Ä‘Æ°á»£c gradient w.r.t. Î¶ tá»« Æ°á»›c lÆ°á»£ng MAP cá»§a likelihood sau:

z*áµ¢,â‚, Î¶*áµ¢,â‚ = argmax_{Î¶,z} e^{-Î“(Î¶,z)}. (34)

HÆ¡n ná»¯a, má»™t thá»§ tá»¥c nhÆ° váº­y cho phÃ©p chÃºng ta xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh dá»±a trÃªn nÄƒng lÆ°á»£ng liÃªn quan Ä‘áº¿n hÃ m self-attention.

### C.3 THÃŠM Káº¾T QUáº¢ HIá»†U SUáº¤T Vá»šI MÃ” HÃŒNH LLAMA TRANSFORMER

Äá»ƒ má»Ÿ rá»™ng cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i trong pháº§n 6.4, vÃ  Ä‘á»ƒ chá»©ng minh ráº±ng Tree Attention cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ nh cÃ´ng cho má»™t loáº¡t cÃ¡c thiáº¿t láº­p pháº§n cá»©ng, chÃºng tÃ´i cÅ©ng thÃ­ nghiá»‡m vá»›i viá»‡c cháº¡y Llama3.2-1B trÃªn thiáº¿t láº­p dual NVIDIA RTX 4090. Hai 4090 Ä‘Æ°á»£c káº¿t ná»‘i qua máº¡ng PCIe. Ngay cáº£ trong trÆ°á»ng há»£p nÃ y, chÃºng tÃ´i quan sÃ¡t Ä‘Æ°á»£c speedup 4x Ä‘Ã¡ng ká»ƒ (tÄƒng lÃªn 5x á»Ÿ Ä‘á»™ dÃ i chuá»—i dÃ i hÆ¡n) cá»§a Tree Attention so vá»›i Ring Attention cho giáº£i mÃ£ autoregressive.

**Báº£ng 2:** So sÃ¡nh Thá»i gian Giáº£i mÃ£ Trung bÃ¬nh (tÃ­nh báº±ng giÃ¢y) vá»›i giai Ä‘oáº¡n prefill sá»­ dá»¥ng mÃ´ hÃ¬nh Llama 3.2 1B vá»›i Tree Attention (cá»§a chÃºng tÃ´i) vÃ  Ring Attention (SOTA) qua cÃ¡c Ä‘á»™ dÃ i chuá»—i khÃ¡c nhau cho 4090s. Káº¿t quáº£ trung bÃ¬nh vÃ  standard error (Â±) Ä‘Æ°á»£c tÃ­nh toÃ¡n sá»­ dá»¥ng 10 láº§n cháº¡y thá»­.

| Äá»™ dÃ i Chuá»—i | Tree Attention | Ring Attention | Speedup |
|-------------|----------------|----------------|---------|
|  | Thá»i gian (s) | Thá»i gian (s) |  |
| 8000 | 0.34 Â±0.05 | 1.38Â±0.07 | Ã—4 |
| 16000 | 0.58 Â±0.07 | 2.77Â±0.04 | Ã—5 |
| 20000 | 0.74 Â±0.01 | 3.47Â±0.04 | Ã—5 |
| 32000 | 1.01 Â±0.02 | 5.45Â±0.03 | Ã—5 |

## PHá»¤ Lá»¤C D: MÃƒ JAX

DÆ°á»›i Ä‘Ã¢y lÃ  phÆ°Æ¡ng thá»©c tree_flash_decode. Codebase Ä‘áº§y Ä‘á»§ cá»§a chÃºng tÃ´i cÃ³ sáºµn táº¡i Ä‘Ã¢y: https://anonymous.4open.science/r/tree_attention-7C32.

```python
import jax
from jax import lax
import jax.numpy as jnp
from functools import partial
from jax.sharding import Mesh,NamedSharding, PartitionSpec as P
from jax.experimental import mesh_utils
from jax.experimental.shard_map import shard_map
from flash_attn_jax.flash import _flash_mha_vjp

in_specs=(P(None, None, None, None), P(None, 'i', None, None), P(None, 'i', None, None))
out_specs=P(None, None, None)

@jax.jit
@partial(shard_map, mesh=mesh, in_specs=in_specs, out_specs=out_specs, check_rep=False)
def tree_flash_decode(q, k, v):
    def flash_num_lse(q, k, v, config=dict(softmax_scale=1.0, is_causal=False, window_size=(-1, -1))):
        tup = _flash_mha_vjp.fwd(q, k, v, config)
        res,lse = tup[1][3],tup[1][4]
        return res,lse
    
    loc_res, loc_lse = flash_num_lse(q, k, v)
    a_max_global = lax.pmax(loc_lse, axis_name='i')
    num_global = lax.psum(loc_res *jnp.exp(loc_lse - a_max_global), axis_name='i')
    den_global = lax.psum(jnp.exp(loc_lse - a_max_global), axis_name='i')
    return (num_global / den_global)
```

HÃ m sá»­ dá»¥ng Flash Attention 2 Dao (2023) Ä‘á»ƒ tÃ­nh toÃ¡n numerator vÃ  denominator Ä‘á»‹a phÆ°Æ¡ng, cáº£ hai Ä‘Æ°á»£c tÃ­ch lÅ©y giá»¯a cÃ¡c thiáº¿t bá»‹ sá»­ dá»¥ng má»™t Allreduce (Ä‘Ã³ lÃ  nhá»¯ng gÃ¬ psum vÃ  pmax gá»i). NCCL xÃ¡c Ä‘á»‹nh theo máº«u nÃ o cÃ¡c káº¿t quáº£ nÃ y Ä‘Æ°á»£c giao tiáº¿p.

## PHá»¤ Lá»¤C E: Báº°NG CHá»¨NG Äá»ŠNH LÃ 1

ChÃºng tÃ´i chá»©ng minh Ä‘á»‹nh lÃ½ 1 dÆ°á»›i Ä‘Ã¢y.

**Báº±ng chá»©ng.**

**TrÆ°á»ng há»£p Tuáº§n tá»±:** TrÃªn má»™t GPU Ä‘Æ¡n, hoáº¡t Ä‘á»™ng reduction trÃªn má»™t máº£ng cÃ³ kÃ­ch thÆ°á»›c N cÃ³ Ä‘á»™ phá»©c táº¡p thá»i gian O(N) vÃ¬ processor pháº£i xá»­ lÃ½ tuáº§n tá»± tá»«ng pháº§n tá»­.

**Xá»­ lÃ½ Song song vá»›i p Processor:** Chia máº£ng cÃ³ kÃ­ch thÆ°á»›c N thÃ nh p chunk, má»—i chunk cÃ³ kÃ­ch thÆ°á»›c N/p. Má»—i processor thá»±c hiá»‡n hoáº¡t Ä‘á»™ng reduction trÃªn chunk cá»§a nÃ³ má»™t cÃ¡ch Ä‘á»™c láº­p. Äá»™ phá»©c táº¡p thá»i gian cho má»—i processor lÃ  O(N/p).

**Káº¿t há»£p Káº¿t quáº£ Partial:** CÃ¡c káº¿t quáº£ partial tá»« p processor cáº§n Ä‘Æ°á»£c káº¿t há»£p. Sá»­ dá»¥ng máº«u cÃ¢y cho reduction, cÃ¡c káº¿t quáº£ partial cÃ³ thá»ƒ Ä‘Æ°á»£c reduced trong O(log p) bÆ°á»›c. Má»—i bÆ°á»›c liÃªn quan Ä‘áº¿n viá»‡c káº¿t há»£p cÃ¡c cáº·p káº¿t quáº£, giáº£m má»™t ná»­a sá»‘ káº¿t quáº£ á»Ÿ má»—i bÆ°á»›c cho Ä‘áº¿n khi chá»‰ cÃ²n má»™t káº¿t quáº£.

**Tá»•ng Äá»™ phá»©c táº¡p Thá»i gian:** Tá»•ng Ä‘á»™ phá»©c táº¡p thá»i gian lÃ  tá»•ng cá»§a cÃ¡c Ä‘á»™ phá»©c táº¡p thá»i gian Ä‘á»ƒ xá»­ lÃ½ cÃ¡c chunk vÃ  káº¿t há»£p káº¿t quáº£:

O(N/p) + O(log p).

Äiá»u nÃ y chá»©ng minh ráº±ng Ä‘á»™ phá»©c táº¡p thá»i gian cá»§a má»™t reduction liÃªn quan Ä‘áº¿n má»™t hoáº¡t Ä‘á»™ng káº¿t há»£p trÃªn má»™t máº£ng cÃ³ kÃ­ch thÆ°á»›c N lÃ  O(N/p + log p) khi sá»­ dá»¥ng p processor song song, vÃ  nÃ³ giáº£m xuá»‘ng O(log N) khi sá»‘ processor báº±ng kÃ­ch thÆ°á»›c cá»§a máº£ng. â–¡

## PHá»¤ Lá»¤C F: TÃNH TOÃN SAFE SOFTMAX

Trong khi, vá» máº·t toÃ¡n há»c, attention sá»­ dá»¥ng hoáº¡t Ä‘á»™ng softmax, trong thá»±c táº¿ Ä‘iá»u nÃ y thÆ°á»ng khÃ´ng á»•n Ä‘á»‹nh vá» máº·t sá»‘ há»c khi sá»­ dá»¥ng cÃ¡c hoáº¡t Ä‘á»™ng Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i tháº¥p. Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, má»™t hÃ m tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t toÃ¡n há»c, 'safe softmax' thay vÃ o Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng, trá»« táº¥t cáº£ dot product trong exponential báº±ng max. Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ Ä‘Æ°á»£c exponentiated nhá» hÆ¡n 1 vÃ  do Ä‘Ã³ Ã­t cÃ³ kháº£ nÄƒng bÃ¹ng ná»• vÃ  gÃ¢y ra sá»± khÃ´ng á»•n Ä‘á»‹nh sá»‘ há»c. á» Ä‘Ã¢y, chÃºng tÃ´i chá»©ng minh ráº±ng phÆ°Æ¡ng phÃ¡p hÃ m nÄƒng lÆ°á»£ng cá»§a chÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ tÃ­nh Ä‘áº¿n safe softmax.

Giáº£ sá»­ chÃºng ta so sÃ¡nh hÃ m sinh cá»§a chÃºng ta

F_tot = âˆ‘áµ¢ log âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T + Î¶â‚Â·v_a^T) (35)

vÃ  má»™t hÃ m Ä‘Æ°á»£c sá»­a Ä‘á»•i nháº¹:

F'_tot = âˆ‘áµ¢ log âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T + Î¶áµ¢Â·v_a^T - máµ¢). (36)

Khi chÃºng ta láº¥y Ä‘áº¡o hÃ m cá»§a hai Ä‘áº¡i lÆ°á»£ng nÃ y, chÃºng ta tháº¥y ráº±ng chÃºng ta cÃ³ Ä‘Æ°á»£c cÃ¹ng káº¿t quáº£:

âˆ‚F_tot/âˆ‚Î¶áµ¢|_{Î¶áµ¢=0} = âˆ‚F'_tot/âˆ‚Î¶áµ¢|_{Î¶áµ¢=0}. (37)

Äá»ƒ tháº¥y nÃ³ má»™t cÃ¡ch rÃµ rÃ ng:

âˆ‚F'_tot/âˆ‚Î¶áµ¢|_{Î¶áµ¢=0} = (âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T - máµ¢)vâ‚)/(âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T - máµ¢)) (38)
= (âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T)vâ‚)/(âˆ‘áµƒâ‚Œâ‚â± exp(qáµ¢Â·k_a^T)).

ThÃ´ng thÆ°á»ng, khi tÃ­nh toÃ¡n softmax theo cÃ¡ch online, thá»§ tá»¥c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n trong Ä‘Ã³ máµ¢ lÃ  row max cá»§a qÂ·k^T. Sá»± dá»‹ch chuyá»ƒn nÃ y lÃ m cho tá»•ng cá»§a exponentials khÃ´ng dáº«n Ä‘áº¿n overflow.

## PHá»¤ Lá»¤C G: KÃ HIá»†U CHO CÃC PHÆ¯Æ NG TRÃŒNH

ÄÃ¢y lÃ  tÃ³m táº¯t cÃ¡c biáº¿n vÃ  chá»‰ sá»‘ khÃ¡c nhau sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c pháº§n tiáº¿p theo:

**Báº¢NG I: TÃªn biáº¿n.**

| Biáº¿n | MÃ´ táº£ |
|------|-------|
| x | Äáº§u vÃ o Attention |
| q,k,v | Vector Query, key vÃ  value |
| Î“ | Log-likelihood Attention |
| Î¶ | Vector nguá»“n |
| m | Max cá»§a qÂ·k^T |
| Z | HÃ m phÃ¢n vÃ¹ng |
| z | Vector activation |
| n | Numerator attention |
| d | Denominator attention |
| lse | Logsumexp Ä‘iá»ƒm attention |
| F | HÃ m sinh |
| P | Máº­t Ä‘á»™ xÃ¡c suáº¥t Ä‘iá»ƒm attention |

**Báº¢NG II: TÃªn chá»‰ sá»‘ vÃ  pháº¡m vi.**

| Chá»‰ sá»‘ | MÃ´ táº£ |
|--------|-------|
| N | Äá»™ dÃ i chuá»—i |
| d | Chiá»u embedding |
| d_h | Chiá»u head |
| p | Sá»‘ thiáº¿t bá»‹ |
| t | KÃ­ch thÆ°á»›c chunk N/p |
| b | KÃ­ch thÆ°á»›c batch |
| a,i,j âˆˆ {1,Â·Â·Â·,N} | Chá»‰ sá»‘ chuá»—i |
| A,B âˆˆ {1,Â·Â·Â·,d} | Chá»‰ sá»‘ embedding |
| Ä€,BÌ„ âˆˆ {1,Â·Â·Â·,d_h} | Chá»‰ sá»‘ intra-head |
| h âˆˆ {1,Â·Â·Â·,n_h} | Chá»‰ sá»‘ head |
| Ã¢,bÌ‚ âˆˆ {1,Â·Â·Â·,t} | Chá»‰ sá»‘ intra chunk |
