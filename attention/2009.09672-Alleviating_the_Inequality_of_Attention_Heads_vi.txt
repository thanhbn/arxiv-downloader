I'll translate this academic paper from English to Vietnamese while maintaining the exact same structure and format.# 2009.09672.pdf
# Đã được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2009.09672.pdf
# Kích thước tệp: 416337 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Giảm thiểu Bất bình đẳng của các Đầu Chú ý
cho Dịch thuật Máy Nơ-ron
Zewei Sun1,, Shujian Huang2,3, Xin-Yu Dai2, Jiajun Chen2
1ByteDance AI Lab
2Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới, Đại học Nanjing
3Phòng thí nghiệm Peng Cheng, Trung Quốc
sunzewei.v@bytedance.com ,{huangsj,daixinyu,chenjj}@nju.edu.cn
Tóm tắt
Các nghiên cứu gần đây cho thấy rằng các đầu chú ý
trong Transformer không bình đẳng (Voita et al.,
2019; Michel et al., 2019). Chúng tôi liên quan hiện
tượng này đến việc huấn luyện mất cân bằng của
cơ chế chú ý đa đầu và sự phụ thuộc của mô hình
vào các đầu cụ thể. Để giải quyết vấn đề này, chúng
tôi đề xuất một phương pháp che giấu đơn giản:
HeadMask, theo hai cách cụ thể. Các thí nghiệm
cho thấy rằng cải thiện dịch thuật được đạt được
trên nhiều cặp ngôn ngữ. Các phân tích thực nghiệm
tiếp theo cũng hỗ trợ giả định của chúng tôi và
xác nhận tính hiệu quả của phương pháp.

1 Giới thiệu
Gần đây, ngày càng có nhiều cấu trúc mạng mới
của dịch thuật máy nơ-ron (NMT) được đề xuất (Bahdanau et al., 2015; Barone et al.,
2017; Gehring et al., 2017; Vaswani et al., 2017),
trong đó Transformer (Vaswani et al., 2017)
đạt được kết quả tốt nhất. Một điểm khác biệt
quan trọng giữa Transformer và các mô hình
dịch thuật khác là cơ chế chú ý đa đầu của nó.

Một số hiện tượng thú vị của các đầu chú ý
được phát hiện gần đây. Voita et al. (2019)
phát hiện rằng chỉ có một tập con nhỏ các đầu
dường như quan trọng cho tác vụ dịch thuật và
đại đa số các đầu có thể bị loại bỏ mà không
ảnh hưởng nghiêm trọng đến hiệu suất. Michel et al. (2019) cũng
phát hiện rằng một số đầu có thể bị loại bỏ khỏi
các mô hình transformer đã huấn luyện mà không
làm giảm đáng kể về mặt thống kê hiệu suất kiểm tra. Hóa ra
không phải tất cả các đầu đều quan trọng như nhau.

Chúng tôi suy đoán rằng điều này có thể được
quy cho việc huấn luyện mất cân bằng của cơ chế
chú ý đa đầu, vì một số đầu không được huấn luyện
đầy đủ và đóng góp ít cho mô hình. Tuy nhiên,
điều này có thể trở thành nút thắt cổ chai cho
toàn bộ mô hình. Để tương tự, nếu một cầu thủ
bóng đá quen sử dụng chân phải và dành nhiều
cơ hội tập luyện hơn cho nó, nó sẽ
* Công việc được thực hiện khi đang ở NJU mạnh hơn và mạnh hơn. Kết quả là, chân phải
được phụ thuộc nhiều hơn, trong khi chân trái
nhận được ít tập luyện hơn và dần dần trở thành
hạn chế.

Trong bài báo này, chúng tôi trước tiên xác nhận
thực nghiệm sự bất bình đẳng trong cơ chế chú ý
đa đầu. Sau đó, một phương pháp huấn luyện mới
với hai biến thể được đề xuất để tránh nút thắt
cổ chai và cải thiện hiệu suất dịch thuật. Các
phân tích sâu hơn cũng được thực hiện để xác minh
giả định.

2 Bất bình đẳng Đầu
Theo Michel et al. (2019), chúng tôi định nghĩa
tầm quan trọng của một đầu chú ý như

Ih=ExX∂L(x)
∂h    (1)

trong đó L(x) là mất mát trên mẫu x và h là
biến che đầu có giá trị trong {0, 1}. Theo
trực giác, nếu đầu h quan trọng, chuyển đổi h sẽ
có tác động đáng kể đến mất mát. Áp dụng
quy tắc chuỗi cho biểu thức cuối cùng cho Ih:

Ih=ExX∑Atth(x)T∂L(x)
∂Atth(x)    (2)

Điều này tương đương với phương pháp khai triển
Taylor từ Molchanov et al. (2017). Trong
Transformer base (Vaswani et al., 2017), có 3
loại chú ý (chú ý tự bộ mã hoá, chú ý tự bộ
giải mã, chú ý bộ mã hoá-bộ giải mã) với 6
lớp mỗi loại và 8 đầu mỗi lớp. Do đó,
nó lên đến 144 đầu. Chúng tôi chia chúng thành 8
nhóm với 18 đầu (12.5%) mỗi nhóm theo
tầm quan trọng Ih của chúng, trong đó, 1-18 là
quan trọng nhất và cứ thế.

Chúng tôi sau đó che các nhóm khác nhau của
các đầu. Như được hiển thị trong Hình 1, che
một nhóm các đầu không quan trọng có ít tác
động đến chất lượng dịch thuật trong khi che
các đầu quan trọng dẫn đến sự giảm đáng kể
về hiệu suất. Đáng ngạc nhiên, gần một nửa
các đầu không quan trọng, vì gần như không
có sự khác biệt nào dù chúng được che hay không.arXiv:2009.09672v2  [cs.CL]  31 Aug 2022

--- TRANG 2 ---
Không có
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Che các nhóm đầu khác nhau (sắp xếp theo tầm quan trọng)20253035404550BLEUBLEU với các che đầu khác nhau
Đường cơ sởHình 1: Che các đầu trong cùng nhóm. Những đầu quan trọng
có ý nghĩa nhiều hơn so với những đầu không quan trọng.

Chúng tôi cũng dần dần che nhiều đầu hơn theo
từng nhóm theo thứ tự tăng dần và giảm dần,
tương ứng. Như được hiển thị trong Hình 2,
đường bắt đầu với các đầu không quan trọng
giảm chậm hơn nhiều so với đường bắt đầu với
các đầu quan trọng. Nó hoàn toàn minh họa
sự bất bình đẳng của các đầu khác nhau.

012.5% 25% 37.5% 50% 62.5% 75% 87.5% 100%
Tỷ lệ phủ che đầu01020304050BLEU
BLEU với tỷ lệ phủ che đầu khác nhau
Các đầu không quan trọng trước
Các đầu quan trọng trước
Hình 2: Che tất cả các đầu theo thứ tự tăng dần và
giảm dần. Các đường cong giảm khác nhau rất nhiều.

Hình 1 và Hình 2 minh chứng thêm sự bất bình
đẳng về tầm quan trọng của các đầu chú ý. Một
giả định đơn giản để giải thích là một số đầu
tình cờ có được nhiều cơ hội cập nhật hơn trong
giai đoạn đầu, khiến cho mô hình học cách phụ
thuộc vào chúng dần dần. Kết quả là, mô hình
ngày càng tạo ra mối liên hệ mạnh mẽ với những
đầu cụ thể này trong khi sự phụ thuộc cục bộ
này ngăn cản các đầu chú ý còn lại khỏi việc
huấn luyện đầy đủ và hạn chế khả năng tổng thể.

3 HeadMask
Vì vấn đề liên quan đến việc huấn luyện không
công bằng của các đầu chú ý, việc cân bằng
rõ ràng các cơ hội huấn luyện là điều tự nhiên
đối với chúng tôi. Chúng tôi đề xuất một phương
pháp đơn giản: HeadMask, che một số đầu trong
quá trình huấn luyện theo hai cách cụ thể.

3.1 Che Ngẫu nhiên
Cách đầu tiên là chọn ngẫu nhiên các đầu và
che chúng trong mỗi batch. Nó đảm bảo mỗi
đầu có cơ hội huấn luyện tương đối bình đẳng
và tránh sự phụ thuộc một phần, như được hiển
thị trong Thuật toán 1. Đối với tương tự bóng đá,
nó giống như huấn luyện các chân ngẫu nhiên,
làm cho cả hai nhận được cùng một lượng luyện tập.

Thuật toán 1 HeadMask: Che Ngẫu nhiên
Đầu vào: q; k; v cho chú ý, số lượng che n
Đầu ra: ngữ cảnh đã che
1: for batch in datasets do
2:     heads = random.sample(all_heads, n)
3:     for head in heads do
4:         head = 0
5:     end for
6:     context = attn( )
7: end for

3.2 Che Những Đầu Quan trọng
Cách thứ hai là che những đầu quan trọng nhất.
Bằng cách buộc mô hình bỏ qua các đầu quan trọng,
chúng tôi hy vọng có nhiều cơ hội huấn luyện
hơn được giao cho các đầu yếu hơn. Đối với
tương tự bóng đá, nó có nghĩa là huấn luyện
chân trái nhiều hơn nếu chân phải chiếm ưu thế.
Và một khi đảo ngược, huấn luyện ngược lại.
Ý tưởng chính của nó là về việc ngăn chặn
việc huấn luyện nghiện. Cụ thể, mạng đầu tiên
tiến hành tính toán feed-forward và lan truyền
ngược mà không cập nhật tham số để tạo ra tầm
quan trọng của các đầu. Và sau khi chọn các
đầu quan trọng nhất bằng cách sắp xếp, che
chúng. Trong quá trình huấn luyện, chúng tôi
chỉ sử dụng phần còn lại của mạng để đạt được
mất mát cuối cùng và cập nhật tham số, như
được hiển thị trong thuật toán 2.

Thuật toán 2 HeadMask: Che Những Đầu Quan trọng
Đầu vào: q; k; v cho chú ý, số lượng che n
Đầu ra: ngữ cảnh đã che
1: for batch in datasets do
2:     calculate L by feed-forward
3:     back propagation without updating params
4:     calculate importance of all heads I
5:     heads = argmaxn(I)
6:     for head in heads do
7:         head = 0
8:     end for
9:     context = attn( )
10:    calculate L by feed-forward
11:    back propagation and update params
12: end for

--- TRANG 3 ---
4 Thí nghiệm
4.1 Tập dữ liệu và Hệ thống
Chúng tôi tiến hành thí nghiệm trên bốn tập dữ liệu,
bao gồm ba tập tài nguyên thấp (ít hơn 1 triệu).
Chúng tôi sử dụng BPE (Sennrich et al., 2016) cho
Zh-En (Zheng et al., 2018) và Ro-En, áp dụng
các phiên bản đã tiền xử lý từ Luong và Manning (2015)
cũng như các cài đặt của Huang et al. (2017) cho
Vi-En, và tuân theo các cài đặt joint-BPE của
Sennrich et al. (2017) cho Tr-EN. Thông tin chi
tiết hơn trong Bảng 1.

Tập dữ liệu    Quy mô    Dev         Test
NIST Zh-En     1.34M     MT03        MT04/05/06
WMT16 Ro-En    608K      newstest2015 newstest2016
IWSLT15 Vi-En  133K      tst2012     tst2013
WMT17 Tr-En    207K      newstest2016 newstest2017

Bảng 1: Thông tin của các tập dữ liệu của chúng tôi

Chúng tôi tuân theo cài đặt Transformer base (Vaswani
et al., 2017; Sun et al., 2022). Tham số được tối
ưu hóa bởi Adam (Kingma and Ba, 2015), với
β₁ = 0.9, β₂ = 0.98, và ε = 10⁻⁹. Tốc độ
học được lên lịch theo Vaswani et al. (2017),
với warmup_steps = 4000. Làm mịn nhãn (Szegedy et al., 2016)
với giá trị ε = 0.1 và dropout (Srivastava et al., 2014)
với giá trị = 0.1 cũng được áp dụng.

So sánh Chúng tôi so sánh đường cơ sở với việc
che ngẫu nhiên (Random-N) và che những đầu quan
trọng (Impt-N), trong đó N là số lượng che. Trong
bài báo này, chúng tôi chủ yếu sử dụng N = 18 (12.5%).

4.2 Kết quả
Như được hiển thị trong Bảng 2,3,4, ngoại trừ
các thí nghiệm Vi-En, Impt-18 mang lại cải thiện
trên tất cả các hướng ngôn ngữ và đạt kết quả
tốt nhất trong thí nghiệm Ro → En. Và Random-18
đạt được cải thiện ổn định trên tất cả các cặp
và rõ ràng tốt hơn Impt-18. Có vẻ như chiến lược
che tích cực ở các đầu quan trọng có thể quá khắc
nghiệt và ngược lại hạn chế mô hình. Và phương
pháp ngẫu nhiên chuyên nghiệp hơn trong việc
xây dựng một mô hình huấn luyện hợp lý. Tóm lại,
việc giảm thiểu việc huấn luyện mất cân bằng
giữa các đầu chú ý có thể cải thiện hiệu quả
chất lượng dịch thuật.

Tập kiểm tra    MT04      MT05      MT06
Đường cơ sở     46.62     43.46     43.09
Impt-18         46.94 (+0.28) 44.19 (+0.73) 43.16 (+0.07)
Random-18       47.04 (+0.42) 44.33 (+0.87) 43.88 (+0.79)

Bảng 2: Kết quả thí nghiệm Zh → En

Hướng           Ro → En   Vi → En   Tr → En
Đường cơ sở     32.17     26.49     17.29
Impt-18         32.95 (+0.78) 26.36 (-0.13) 17.48 (+0.19)
Random-18       32.85 (+0.68) 26.85 (+0.36) 17.56 (+0.27)

Bảng 3: Kết quả thí nghiệm Ro/Vi/Tr → En

Hướng           En → Ro   En → Vi   En → Tr
Đường cơ sở     31.98     28.07     15.74
Impt-18         32.47 (+0.49) 28.06 (-0.01) 16.10 (+0.36)
Random-18       32.64 (+0.66) 28.46 (+0.39) 16.16 (+0.42)

Bảng 4: Kết quả thí nghiệm En → Ro/Vi/Tr

4.3 Phân tích Thống kê
4.3.1 Phân phối Phẳng hơn
Để đánh giá việc điều chỉnh huấn luyện của các đầu,
chúng tôi kiểm tra phân phối tầm quan trọng của đầu.
Như được hiển thị trong Hình 3, các phương pháp
của chúng tôi làm cho phân phối tầm quan trọng phẳng
hơn. Và phương sai và trung bình tổng thể cũng được
tính toán, như được hiển thị trong Bảng 5,6. So
với Đường cơ sở, Impt-18 và Random-18 giảm
đáng kể phương sai của các đầu chú ý, đạt được
mục tiêu huấn luyện bình đẳng hơn. Và trung bình
cũng giảm, điều này chứng minh sự suy giảm của
sự phụ thuộc vào mỗi đầu riêng lẻ. Cụ thể hơn,
Impt-18 có thể giải quyết tốt hơn sự mất cân bằng,
vì nó ngăn chặn tốt sự xuất hiện của các đầu "siêu".

1       18      36      54      72      90      108     126     144
Các đầu được sắp xếp theo tầm quan trọng01020304050607080Tầm quan trọngPhân phối tầm quan trọng của đầu
Đường cơ sở
(a) Đường cơ sở

1       18      36      54      72      90      108     126     144
Các đầu được sắp xếp theo tầm quan trọng01020304050607080Tầm quan trọngPhân phối tầm quan trọng của đầu
Random-18 (b) Random-18

1       18      36      54      72      90      108     126     144
Các đầu được sắp xếp theo tầm quan trọng01020304050607080Tầm quan trọngPhân phối tầm quan trọng của đầu
Impt-18 (c) Impt-18

Hình 3: Phân phối tầm quan trọng của các đầu chú ý. Các phương pháp của chúng tôi làm cho toàn bộ phân phối phẳng hơn nhiều.

--- TRANG 4 ---
Không có
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Che các nhóm đầu khác nhau (sắp xếp theo tầm quan trọng)20253035404550BLEUBLEU với các che đầu khác nhau
Đường cơ sở(a) Đường cơ sở

Không có
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Che các nhóm đầu khác nhau (sắp xếp theo tầm quan trọng)20253035404550BLEUBLEU với các che đầu khác nhau
Random-18 (b) Random-18

Không có
127-144 109-12691-10873-90 55-72 37-54 19-36 1~18
Che các nhóm đầu khác nhau (sắp xếp theo tầm quan trọng)20253035404550BLEUBLEU với các che đầu khác nhau
Impt-18 (c) Impt-18

Hình 4: Các phương pháp của chúng tôi duy trì đáng kể hiệu suất ngay cả khi các đầu quan trọng bị che.

0
12.5%25%37.5%50%62.5%75%87.5% 100%
Tỷ lệ phủ che đầu01020304050BLEU
BLEU với tỷ lệ phủ che đầu khác nhau
Đường cơ sở      (các đầu không quan trọng trước)
Đường cơ sở      (các đầu quan trọng trước)
Random-18 (các đầu quan trọng trước)
Impt-18       (các đầu quan trọng trước)
(a) Che 18 đầu trong huấn luyện

0
12.5%25%37.5%50%62.5%75%87.5% 100%
Tỷ lệ phủ che đầu01020304050BLEU
BLEU với tỷ lệ phủ che đầu khác nhau
Đường cơ sở      (các đầu không quan trọng trước)
Đường cơ sở      (các đầu quan trọng trước)
Random-36 (các đầu quan trọng trước)
Impt-36       (các đầu quan trọng trước) (b) Che 36 đầu trong huấn luyện

0
12.5%25%37.5%50%62.5%75%87.5% 100%
Tỷ lệ phủ che đầu01020304050BLEU
BLEU với tỷ lệ phủ che đầu khác nhau
Đường cơ sở      (các đầu không quan trọng trước)
Đường cơ sở      (các đầu quan trọng trước)
Random-54 (các đầu quan trọng trước)
Impt-54       (các đầu quan trọng trước) (c) Che 54 đầu trong huấn luyện

Hình 5: Khi số lượng đầu bị che tăng lên, các đường cong giảm bắt đầu với các đầu quan trọng đang di chuyển lên.

Hướng      Zh2En   Ro2En   Vi2En   Tr2En
Đường cơ sở 77.28   552.93  100.73  1767.70
Random-18   33.21   255.98  48.28   900.70
Impt-18     9.13    72.73   14.13   188.87

Bảng 5: Các phương pháp của chúng tôi giảm đáng kể Phương sai của
tầm quan trọng đầu, minh họa sự cải thiện bình đẳng của các đầu.

Hướng      Zh2En   Ro2En   Vi2En   Tr2En
Đường cơ sở 27.15   47.18   17.96   83.79
Random-18   19.62   39.96   14.86   74.05
Impt-18     18.95   37.30   18.96   85.12

Bảng 6: Các phương pháp của chúng tôi giảm Trung bình của tầm
quan trọng đầu, minh họa sự giảm thiểu phụ thuộc vào mỗi đầu.

4.3.2 Phụ thuộc Yếu hơn
Chúng tôi lặp lại các thí nghiệm che các nhóm
đầu khác nhau. Như được hiển thị trong Hình 4,
chất lượng dịch thuật vẫn được duy trì ngay cả
khi các đầu quan trọng bị che, chứng minh sự
phụ thuộc vào chúng đã giảm. Và Impt-18 hoạt
động ổn định hơn vì nó đã quen với những tình
huống như vậy.

4.3.3 Mô hình Mạnh mẽ hơn
Chúng tôi cũng lặp lại các thí nghiệm che tất cả
các đầu, như được hiển thị trong Hình 5. Hai
đường giữa ban đầu nằm ở cùng vị trí với đường
dưới. Khi số lượng đầu bị che trong huấn luyện
(N) tăng lên, chúng dần dần di chuyển lên và
tiếp cận đường trên nơi các đầu không quan trọng
được che trước. Nó cho thấy các phương pháp
của chúng tôi làm cho mô hình ít phụ thuộc vào
các đầu quan trọng và trở nên mạnh mẽ hơn.

5 Công trình Liên quan
Gần đây, nhiều công trình phân tích về cơ chế
chú ý đa đầu ra đời (Raganato and Tiedemann,
2018; Tang et al., 2018; Voita et al., 2019;
Michel et al., 2019; Sun et al., 2020; Behnke
and Heafield, 2020). Và đối với sự bất bình đẳng
của các mạng, một số nghiên cứu tập trung vào
mức mô hình (Frankle and Carbin, 2019; Sun
et al., 2021), mức lớp (Zhang et al., 2019),
và mức nơ-ron (Bau et al., 2019). Đối với thuật
toán che, cũng có các công trình ở mức lớp (Fan
et al., 2020), mức từ (Provilkov et al., 2019),
và mức nơ-ron (Srivastava et al., 2014). Khác
với họ, chúng tôi chủ yếu nghiên cứu mức chú ý
và tiến hành phân tích thống kê.

6 Kết luận
Trong bài báo này, chúng tôi xác nhận thực nghiệm
sự bất bình đẳng của các đầu chú ý trong Transformer
và đưa ra một giả định về việc huấn luyện mất cân
bằng. Tương ứng, chúng tôi đề xuất một phương pháp
cụ thể theo hai cách để giải quyết vấn đề. Các thí
nghiệm cho thấy cải thiện trên nhiều cặp ngôn ngữ.
Và phân tích chi tiết cho thấy sự giảm thiểu của
vấn đề và tính hiệu quả của các kỹ thuật của chúng tôi.

--- TRANG 5 ---
7 Lời cảm ơn
Chúng tôi muốn cảm ơn các nhà bình luận ẩn danh
vì những nhận xét sâu sắc của họ. Shujian Huang
là tác giả liên lạc. Công trình này được hỗ trợ
bởi Quỹ Khoa học Quốc gia Trung Quốc
(Số 6217020152).

Tài liệu tham khảo
Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. 2015. Dịch thuật máy nơ-ron bằng cách học
đồng thời căn chỉnh và dịch. Trong ICLR.

Antonio Valerio Miceli Barone, Jindrich Helcl, Rico
Sennrich, Barry Haddow, và Alexandra Birch.
2017. Kiến trúc sâu cho dịch thuật máy nơ-ron.
Trong WMT.

Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir
Durrani, Fahim Dalvi, và James Glass. 2019. Xác
định và kiểm soát các nơ-ron quan trọng trong
dịch thuật máy nơ-ron. Trong ICLR.

Maximiliana Behnke và Kenneth Heafield. 2020. Mất
các đầu trong xổ số: Cắt tỉa chú ý transformer
trong dịch thuật máy nơ-ron. Trong EMNLP.

Angela Fan, Edouard Grave, và Armand Joulin. 2020.
Giảm độ sâu transformer theo yêu cầu với dropout
có cấu trúc. Trong ICLR.

Jonathan Frankle và Michael Carbin. 2019. Giả thuyết
vé số: Tìm các mạng nơ-ron thưa thớt, có thể
huấn luyện. Trong ICLR.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, và Yann Dauphin. 2017. Học chuỗi đến
chuỗi tích chập. Trong ICML.

Po-Sen Huang, Chong Wang, Dengyong Zhou, và
Li Deng. 2017. Dịch thuật máy nơ-ron dựa trên
cụm từ. arXiv, abs/1706.05565.

Diederick P Kingma và Jimmy Ba. 2015. Adam: Một
phương pháp tối ưu hóa ngẫu nhiên. Trong ICLR.

Minh-Thang Luong và Christopher D Manning. 2015.
Hệ thống dịch thuật máy nơ-ron Stanford cho
các miền ngôn ngữ nói. Trong IWSLT.

Paul Michel, Omer Levy, và Graham Neubig. 2019.
Liệu mười sáu đầu có thực sự tốt hơn một đầu không?
Trong NeurIPS.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo
Aila, và Jan Kautz. 2017. Cắt tỉa mạng nơ-ron
tích chập cho suy luận hiệu quả tài nguyên. Trong
ICLR.

Ivan Provilkov, Dmitrii Emelianenko, và Elena Voita.
2019. Bpe-dropout: Chính quy hóa từ con đơn giản
và hiệu quả. arXiv, abs/1910.13267.

Alessandro Raganato và Jörg Tiedemann. 2018.
Một phân tích về biểu diễn bộ mã hoá trong
dịch thuật máy dựa trên transformer. Trong BlackboxNLP@EMNLP.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, Antonio Valerio Miceli Barone, và Philip Williams.
2017. Hệ thống MT nơ-ron của Đại học Edinburgh
cho wmt17. Trong WMT.

Rico Sennrich, Barry Haddow, và Alexandra Birch.
2016. Dịch thuật máy nơ-ron của các từ hiếm
với đơn vị từ con. Trong ACL.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, và Ruslan Salakhutdinov. 2014. Dropout: một cách đơn giản để ngăn
chặn mạng nơ-ron quá khớp. JMLR, 15(1):1929–
1958.

Zewei Sun, Shujian Huang, Hao-Ran Wei, Xin-yu Dai,
và Jiajun Chen. 2020. Tạo ra bản dịch đa dạng
bằng cách thao tác cơ chế chú ý đa đầu. Trong AAAI.

Zewei Sun, Mingxuan Wang, và Lei Li. 2021. Dịch
thuật đa ngôn ngữ thông qua ghép các mô hình
ngôn ngữ được tiền huấn luyện. Trong EMNLP.

Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi
Zhao, Shujian Huang, Jiajun Chen, và Lei Li. 2022.
Suy nghĩ lại về dịch thuật máy nơ-ron mức tài liệu.
Trong ACL.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, và Zbigniew Wojna. 2016. Suy
nghĩ lại kiến trúc inception cho thị giác máy tính.
Trong CVPR.

Gongbo Tang, Rico Sennrich, và Joakim Nivre. 2018.
Một phân tích về cơ chế chú ý: Trường hợp phân
biệt nghĩa từ trong dịch thuật máy nơ-ron. Trong
WMT.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. 2017. Chú ý là tất
cả những gì bạn cần. Trong NIPS.

Elena Voita, David Talbot, F. Moiseev, Rico Sennrich,
và Ivan Titov. 2019. Phân tích cơ chế tự chú ý
đa đầu: Các đầu chuyên biệt làm công việc nặng,
phần còn lại có thể được cắt tỉa. Trong ACL.

Biao Zhang, Ivan Titov, và Rico Sennrich. 2019. Cải
thiện transformer sâu với khởi tạo theo tỉ lệ độ
sâu và chú ý được hợp nhất. Trong EMNLP-IJCNLP.

Zaixiang Zheng, Shujian Huang, Zewei Sun, Rongxiang Weng, Xinyu Dai, và Jiajun Chen. 2018.
Học cách phân biệt tiếng ồn để kết hợp thông tin
bên ngoài trong dịch thuật máy nơ-ron. arXiv,
abs/1810.10317.Đã hoàn thành dịch toàn bộ bài báo học thuật từ tiếng Anh sang tiếng Việt, giữ nguyên cấu trúc và định dạng ban đầu.Đã hoàn thành dịch toàn bộ bài báo học thuật từ tiếng Anh sang tiếng Việt, giữ nguyên cấu trúc và định dạng ban đầu.