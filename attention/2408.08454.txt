# 2408.08454.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/attention/2408.08454.pdf
# File size: 1063809 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention
Zohaib Khan*, Muhammad Khaquan*, Omer Tafveez, Burhanuddin Samiwala, Agha Ali Raza
Lahore University of Management Sciences
{24100074, 25100095, 25020254, 25100255, agha.ali.raza }@lums.edu.pk
Abstract
The Transformer architecture has revolutionized deep learn-
ing through its Self-Attention mechanism, which effectively
captures contextual information. However, the memory foot-
print of Self-Attention presents significant challenges for
long-sequence tasks. Grouped Query Attention (GQA) ad-
dresses this issue by grouping queries and mean-pooling
the corresponding key-value heads—reducing the number of
overall parameters and memory requirements in a flexible
manner without adversely compromising model accuracy. In
this work, we introduce enhancements to GQA, focusing on
two novel approaches that deviate from the static nature of
grouping: Key-Distributed GQA (KDGQA) andDynamic
Key-Distributed GQA (DGQA) , which leverage informa-
tion from the norms of the key heads to inform query alloca-
tion. Specifically, KDGQA looks at the ratios of the norms of
the key heads during each forward pass, while DGQA exam-
ines the ratios of the norms as they evolve through training.
Additionally, we present Perturbed GQA (PGQA) as a case-
study, which introduces variability in (static) group formation
via subtracting noise from the attention maps. Our experi-
ments with up-trained Vision Transformers, for Image Classi-
fication on datasets such as CIFAR-10, CIFAR-100, Food101,
and Tiny ImageNet, demonstrate the promise of these variants
in improving upon the original GQA through more informed
and adaptive grouping mechanisms: specifically ViT-L expe-
riences accuracy gains of up to 8% when utilizing DGQA
in comparison to GQA and other variants. We further ana-
lyze the impact of the number of Key-Value Heads on perfor-
mance, underscoring the importance of utilizing query-key
affinities. Code is available on GitHub.1
1 Introduction
Since their inception in 2017, Transformers (Vaswani et al.
2023) have become the state-of-the-art approach in language
modeling tasks (Brown et al. 2020; Touvron et al. 2023).
More recently, they have been adapted for computer vision
tasks such as image classification and segmentation, bypass-
ing the large dependence on convolutions (Dosovitskiy et al.
2021; Liu et al. 2021; Dong et al. 2022). The scalability of
Transformers has also been empirically established by the
scaling laws (Hoffmann et al. 2022) which suggest bigger
is indeed better in this paradigm. This expansion in model
*These authors contributed equally.
1https://github.com/zohaib-khan5040/key-driven-gqa
Figure 1: An overview of the relevant mechanisms. MHA
associates one query to one key-value head. GQA asso-
ciates a single key-value head to subgroups of queries
such that the group sizes remain constant and the grouping
is performed in a static/uniform manner. Our approaches,
KDGQA/DGQA, perform the grouping according to the
norms of the key heads in a non-uniform manner - note how
the darker keys (indicating higher norms) have more queries
assigned to them in comparison to the lighter ones.
size usually occurs by increasing the number of layers, or
the dimensionality of the projections.
Increasing model size however comes at the cost of in-
creased memory and computational requirements. Larger
models also cause an inference bottleneck since the com-
putational complexity of Self-Attention scales quadratically
with number of tokens. These limitations, coupled with the
fact that most large transformer based models are at times
over-parameterised (Hoffmann et al. 2022) has set the stage
for various model compression techniques specifically tar-
geting the Self-Attention module.
One of the first popular schemes was Multi-Query At-
tention (Shazeer 2019) which suggests that in each self-
attention block, using only a single key head significantly
reduces parameters without causing a sizeable drop in ac-
curacy. Later, Grouped-Query Attention (GQA) was intro-
duced (Ainslie et al. 2023) as an interpolation between
Multi-Head Attention (MHA) and Multi-Query Attention
(MQA). GQA was shown to achieve performance close to
MHA, while being nearly as fast as MQA, using only a sin-arXiv:2408.08454v2  [cs.CV]  28 Aug 2024

--- PAGE 2 ---
gle key and value head per subgroup of query heads. Since
then, there have been multiple approaches that modify the
GQA architecture to further mitigate the drop in accuracy
(Javadi et al. 2023; Chinnakonduru and Mohapatra 2024;
Chen et al. 2024).
We propose our own extensions onto the GQA architec-
ture which try to soften the accuracy drop by making the
grouping mechanism more dynamic and informed in nature,
as applied to image classification for Vision Transformers
(Dosovitskiy et al. 2021). Specifically, we explore the possi-
bility and the promise of using a simple heuristic as the L2-
norms (as measures of importance ) of the key heads to guide
the group allocation process, during training, in approaches
we call Key-Distributed GQA (KDGQA) andDynamic
Key-Distributed GQA (DGQA) - the former limiting its
knowledge of the norms to the current forward pass, while
the latter examines the ratios as they evolve through train-
ing (Figure 1 illustrates the difference in grouping). We also
look into Perturbed GQA (PGQA) as an interesting case
of introducing Gaussian Noise to the GQA attention maps.
2 Related Work
The global nature of Self-Attention, where there are an equal
number of queries and keys/values, has made the training
and deployment of transformers increasingly computation-
ally expensive. GQA mitigated this to some extent, allowing
for forming fixed-size groups of query heads and associating
a single key-value head to each group (Ainslie et al. 2023):
the paper discusses utilizing existing MHA checkpoints, and
converting them to the GQA architecture by mean-pooling
the key and value heads associated with each group, re-
spectively. Many works have recently risen that tinker with
the connectivity and forward pass dynamics of the Atten-
tion mechanism, squeezing out more performance from the
model, oftentimes with the same number of parameters.
Grouped Query Key Value Attention (Javadi et al.
2023) emerges as a unifying approach that encapsulates
variations like Multi-Key Value Attention (MKV A) and
Grouped Key Value Attention (GKV A). While MKV A and
GKV A introduce novel strategies by grouping keys and val-
ues into distinct groups and sharing queries within these
groups, GQKV A partitions queries and key-value pairs, op-
timizing both the parameter count and computational effi-
ciency. It explores the combinatorial approach of splitting
Q matrices into ggroups and the K, V matrices into gkv
groups, allowing for a flexible number of queries, keys, and
values within each partition. This method uses dot-product
attention for each unique combination of these groups ( Qi,
KVj), ensuring distinct outputs across multiple heads.
Chen et al. (2024) propose AsymGQA and investigate
the non-uniform allocation of heads by incorporating sim-
ilar projections into uniformly constructed clusters. For in-
stance, consider a scenario where the set of keys Kis par-
titioned into three groups, each containing two keys. In this
approach, the similarity of a randomly selected key is com-
puted with respect to all keys in the other groups. The key is
then inserted into the group with the highest similarity. This
was also experimented on the activation level, where the out-
put of each layer was used for computing similarity to formclusters of heads, which performed better than Weight-level
clustering.
Similarly, Joshi et al. (2024) propose Quality and
Capacity-aware GQA (QCQA) , which uses a minimiza-
tion loss referred to as the Weight-Sharing Error, which is
the distance between the KandVhead distributions to min-
imize the drop in accuracy due to grouping of queries. The
approach follows a two-stage search framework, the first
forming of the groups of queries Qof each layer indepen-
dently. This is followed by evaluating groupings of each
layer using Weight Sharing Error and KV cache as the fitness
functions to drop layers with the largest impact on accuracy.
Layers that exhibit significant damage to the accuracy are re-
tained as MHA forms while the others are grouped, finaliz-
ing the selection of layers that will preserve these gourpings
with non selected layers configured to MHA. The first stage
focuses on optimizing groups for each layer individually,
while the second stage utilizes the Non-dominated Sorting
Genetic Algorithm II to iteratively refine these groupings.
Weighted GQA (Chinnakonduru and Mohapatra 2024)
introduces a learnable scalar or vector parameter to the K
andVheads for (w1,k, w2,k..wh,k)and(w1,v, w2,v..wh,v).
In other words, composite keys and values are formed where
each column of KandVis a linear combination of the
key and value matrices K1, V1through Kh, Vhwhere the
combination is controlled by a specific set of weight vector.
The weighted sum of this operation is used to modify K, V ,
which is then used for self-attention.
3 Method
This section describes our approaches to diverge from the
default uniform nature of the query allocation in Grouped-
Query Attention.
3.1 Key-Distributed GQA (KDGQA)
This approach was our first attempt at allocating query heads
to each group in a more dynamic manner. In KDGQA, rather
than uniformly assigning queries to groups, we utilize the
magnitudes of the key vectors, measured by their L2-norms
as a proxy for their importance , to inform the distribution
of queries across different groups. We posit that allocating a
number of query heads proportional to their relative impor-
tance (within that layer) is beneficial for model performance.
Given a set of key vectors K∈RG×dk, where Gis the
number of groups (or key-value heads) and dkis the dimen-
sionality of the key vectors, the norm of each key vector can
be computed as follows:
ng=∥Kg∥2=vuutdkX
i=1K2
g,i forg= 1, . . . , G (1)
Here, ngis the norm of the g-th key vector Kg.
To ensure that the norms are scaled within a range of [0,1]
for easier interpretation and allocation, we apply min-max
scaling to the vector of norms n= [n1, n2, . . . , n G]:
ˆng=ng−min(n)
max(n)−min(n)(2)

--- PAGE 3 ---
Where:
•ˆngis the scaled norm for the g-th group.
•min(n)andmax(n)represent the minimum and maxi-
mum values in the vector n.
We find that the min-max scaling as described in equa-
tion 2 helps with compressing the norms into a tighter range,
amplifying the differences and leading to more non-uniform
group sizes, in comparison to directly taking the ratios.
Finally, the number of queries, Qg, allocated to each key
head can be determined by normalizing the scaled norms
and multiplying by the total number of queries Nq:
Qg=$
ˆng·NqPG
g=1ˆng%
(3)
Where:
•Nqis the total number of queries.
•Qgrepresents the number of queries allocated to the g-th
group.
KDGQA carries out this procedure for finding the num-
ber of query heads to allocate, then forming the groups in a
left-to-right fashion, during each forward pass, irrespective
of training or inference, which causes a small overhead we
discuss later (Section 5.2).
3.2 Dynamic Key-Distributed GQA (DGQA)
Dynamic Key-Distributed Grouped Query Attention
(DGQA) extends the principles of KDGQA by not just
considering the norms of key vectors at one point in time,
but by dynamically adjusting these measures of importance
during the training process. The core idea of DGQA is to
allow the grouping of queries to adapt based on the evolving
characteristics of the key vectors as the model learns. This
dynamic adaptation aims to more effectively capture and
leverage the underlying structure of the data, leading to
improved performance in attention mechanisms.
The underlying mechanism relies upon defining a window
size: a fixed number of iterations at the start of which we
update the query allocations and stick with for the duration
of the interval. To quantify some measure of how the keys
evolve (on the basis of their magnitudes), we propose two
approaches:
1.Difference-based Evolution.
We cache the norms of the key heads (as calculated in
equation 1) at the start of each window/interval t,c(t)
g. As
training progresses, we quantify the importance of each
group by measuring the absolute difference between the
cached norm (coming from the previous interval) and the
current one:
∆n(t)
g=n(t)
g−c(t−1)
g forg= 1, . . . , G (4)
where ∆n(t)
grepresents the change in the norm for the
g-th key head and the t-th interval.
We then use this difference to form our measure of im-
portance, dg, for each key head:
dg=∆n(t)
g (5)The cached values are then updated when the next inter-
val starts:
c(t)
g=n(t)
g forg= 1, . . . , G (6)
2.Exponential Moving Average-based Evolution.
This approach is very similar to the previous one in terms
of setting a window-size, using a cache of the norms,
and updating the cache. The main difference lies in the
measure of importance, dg: instead of taking an absolute
difference of the norms, we take an exponential moving
average (EMA) of the two:
c(t)
g=α·n(t)
g+(1−α)·c(t−1)
g forg= 1, . . . , G (7)
where αis a hyperparameter that determines the impor-
tance of the current value over the moving average in de-
termining the new average.
This new cached value would be used for determining the
query allocations (note we don’t need to take the absolute
value here):
dg=c(t)
g forg= 1, . . . , G (8)
Both approaches simply seek to compute dg(g=
1, . . . , G ). Using this, the number of queries allocated in
each group, Qg, can be computed in the same fashion as
equation 3, taking the place of ˆng.
Our experiments have established the superiority of the
EMA variant over the difference variant: 84% vs 71% re-
spectively, for a window size of 300, on CIFAR-100 for 10
epochs, from a converted MHA checkpoint. This can be at-
tributed to the following (Figure 2):
•Smoother Adaptation to Changing Magnitudes
The averaging effect of EMA helps mitigate the influ-
ence of outliers or abrupt changes, leading to more robust
and reliable head allocations compared to the difference
method, especially in scenarios where the norms of keys
evolve rapidly.
•Reduced Sensitivity to Noise
By incorporating past information into the current up-
date, EMA reduces sensitivity to noisy or transient pat-
terns in the data.
For the rest of the paper, note that unless specified oth-
erwise, the DGQA approach refers to the EMA variant. We
performed a grid search to find the best performing window
size on CIFAR-100, which was found to be 300 - this means
that the model’s query allocation scheme is updated every
300 training steps, which can be impacted by the size of the
dataset. We elected to skip this finer search due to time con-
straints.
3.3 Perturbed GQA (PGQA)
Through experiments, MHA has clear evidence of each head
being more similar to itself than the other heads, while
GQA fails to maintain this coherency. Oftentimes, the at-
tention output of a head in GQA can be more similar to the
other heads in its group, than itself leading to an intra-group
similarity bias. Although the scaling laws of large models

--- PAGE 4 ---
Figure 2: Standard deviation of the EMA-variant incurs
fewer spiky, supporting the idea that it is more robust to tran-
sient noise.
show little to no performance degradation, the similarity bias
contributes to GQA lagging behind MHA in performance.
Therefore, we propose Perturbed GQA that removes this
similarity bias by introducing a Gaussian Noise matrix in
the attention output.
The noise matrix is computed separately for each group g
by computing mean µgand standard deviation σgfor each
group’s attention output ˆA. A random matrix Rgis gener-
ated of the same shape as ˆAgand is normalized to match the
statistics of ˆAgas follows:
Gaussian g=σg∗(Rg−µ(Rg))
σ(Rg)+µgforg= 1, . . . , G
(9)
where Gaussian gis the Gaussian Noise matrix computed
for each group. To maintain sparsity and avoid disrupting the
attention patterns, we set the diagonal elements of the matrix
to zero.
The obtained Gaussian Noise Matrix is subtracted from
the attention output of each group. Meanwhile, the group
allocation is carried with the same fashion from left-right as
GQA.
Figure 3 shows that PGQA helps us mitigate the intra-
group similarity bias. However, this bias mitigation also
causes the similarity of each head with itself to diminish.
In Appendix C, we also discuss the head similarity patterns
observed in DGQA which approximates a weighted average
of the heat maps from MHA and GQA.
4 Experiments
4.1 Models and Datasets
We elected to use Vision Transformers (Dosovitskiy et al.
2021) in our experiments, following an experimentation
scheme similar to Javadi et al. (2023). This choice is fa-
vorable since pre-trained Vision Transformers come in three
model sizes, which can easily fit in consumer-level hard-
ware. Approximate values for the model sizes, and the num-
ber of heads they were configured to train with, are shown
in Table 1.
Figure 3: PGQA mitigates similarity bias from GQA, how-
ever it wipes out self-similarity patterns seen in attention
map of MHA.
This flexibility allows for a more robust evaluation when
paired with our choice of datasets: CIFAR-10, CIFAR-100,
Food101, and Tiny ImageNet (Krizhevsky (2009); Bossard,
Guillaumin, and Van Gool (2014); mnmoustafa (2017)).
4.2 Checkpoint Conversion and Uptraining
The ”uptraining” scheme proposed in the GQA paper by
Ainslie et al. (2023) describes adapting pre-trained MHA
checkpoints to the GQA architecture: this involves the con-
struction of each group’s key and value heads by mean-
pooling all the original heads within that group, followed
by continued pre-training to close the gap between GQA
and MHA. We perform the checkpoint conversion using
MHA checkpoints of ViT from the timm library (Wightman
2019).
With the converted model, we perform up-training in a
fashion similar to what was described in the original ViT
paper. We use the AdamW optimizer with a learning rate
of1×10−4, with β1= 0.9,β2= 0.999,ϵ= 1×10−8
and a weight decay of 0.01. We train for 1epoch on the
ImageNet-1k dataset (Russakovsky et al. 2015), with over
1.3M training images. This is carried out with a batch size
of32on a single NVIDIA GeForce RTX 4090 24GB GPU.
Note that since all models have the same number of param-
eters, keeping the size (small, base, large) and dataset fixed,
the only difference in uptraining FLOPs stem from the ad-
ditional computations in finding the number of queries to
allocate to each group.
For up-training, we apply a random augmentation to
the images before they are resized to an image size of
(224,224,3). This is to mitigate overfitting which is a big
challenge when training ViTs - augmentations are prefer-
able in comparison to regularization strategies as discussed
by Steiner et al. (2022).
Note that for uptraining and for fine-tuning, we elected
to pit the best versions of our variants against the best ver-
sion of GQA, i.e. with the maximum number of key-value
heads. This allows for maximally narrowing the gap between
GQA (plus its variants) and MHA: with Hheads for a given
model, we go withH
2key-value heads for fairness, since that

--- PAGE 5 ---
Model Number of Parameters (M) Size (MB) Number of Heads
ViT-S 30 77 6
ViT-B 75 300 12
ViT-L 300 1060 16
Table 1: Model parameters, sizes, and number of heads. Note that (1) the number of heads come from pretrained model’s
configuration, as it is a hyperparameter that is set before pretraining, and (2) the Size in MB comes from our uptraining
experiments on ImageNet-1k.
Figure 4: Scaling the number of Key-Value heads, and how
it impacts the loss. Each data point represents a variant on
ViT-B, with 12 heads, that was finetuned on CIFAR-100 for
5 epochs, without any uptraining for the sake of time. It is
evident that the models perform better when they have the
maximal number of Key-Value heads.
is the ceiling of what is allowed with GQA. We addition-
ally examine the impact on varying the number of key-value
heads on performance in Figure 4.
4.3 Fine-tuning
To evaluate our models, we load in the model checkpoints
from the uptraining phase. It is important to note that all
models undergo the same number of training steps , so there
is no advantage given to our variants despite the modifica-
tions made. No extra opportunity is given to our variants to
recover model performance.
Each model is fine-tuned for 5epochs. The optimizer,
hardware, augmentation strategy, and batch size is the same
as the uptraining runs, with the exception of the learning rate
which is set to 1×10−5for fine-tuning.
5 Results and Discussion
5.1 Evaluation
The results from the uptraining runs are shown in Table 2.
Our core results are shown in Table 3, where we uptrain each
model and variant, and Table 4, where we use the uptrained
GQA checkpoint and only introduce our modifications dur-
ing fine-tuning. For these tables, GQA and its variants have
the following configuration: ViT-S uses 3 key-value heads,
ViT-B uses 6 key-value heads, and ViT-L uses 8 key-value
heads. Our benchmark is the GQA variant, and the MHA
variants’ performance have been provided for reference.Model Variant Accuracy
ViT-SGQA 65.95
KDGQA 51.85
DGQA 65.66
PGQA 59.55
ViT-BGQA 48.43
KDGQA 43.69
DGQA 45.53
PGQA 44.59
ViT-LGQA 52.49
KDGQA 37.21
DGQA 59.13
PGQA 49.94
Table 2: Validation accuracies after 1 epoch of uptraining on
ImageNet-1k. Models outperforming GQA in its category
are highlighted in bold .
From Table 3, the most important note is that DGQA con-
sistently outperforms the GQA benchmark when using an
uptrained ViT-L architecture, and by rather large margins,
by up to 8%as is the case for Tiny ImageNet.
It is very interesting to note that for ViT-S and ViT-B,
DGQA always underperforms in comparison to GQA - we
wished to include these failed experiments in our evalua-
tion as to highlight how much of an impact the number of
key-value heads and size of the model impact the change
in performance. Since DGQA was designed as an improve-
ment over KDGQA, it is no surprise that the latter performs
poorly in our final evaluation, as was the same pattern in
preliminary experiments.
Another interesting pattern is in the complexity of the
classification task: if we hone in on ViT-L, we find (approxi-
mately) that there is a 1.5%gain in accuracy for CIFAR-10,
5%gains for CIFAR-100 and Food101 (note they have 100
and 101 classes respectively), and 8%for Tiny ImageNet
(which has 200 classes). This, in tandem with the note from
the previous paragraph, lead us to postulate that the true ben-
efits of new approaches, derived from branching off a pre-
trained base, only come forward on larger scales, i.e. with
larger models and bigger datasets.
While PGQA may underperform the benchmark in our
evaluations, it follows the same pattern above (narrower
margin when scaling up), and presents an intriguing case
study. Despite its subpar performance, PGQA provides valu-
able insights into the (lack of) resilience of attention mech-

--- PAGE 6 ---
Model VariantAccuracy
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-SMHA 98.42 90.94 87.29 86.51
GQA 97.27 84.70 80.95 80.23
KDGQA 94.05 71.32 72.04 63.66
DGQA 97.27 84.55 80.77 80.16
PGQA 95.81 78.42 76.55 73.81
ViT-BMHA 98.67 91.91 90.17 91.04
GQA 93.32 71.78 72.42 62.87
KDGQA 91.33 67.84 68.01 57.90
DGQA 91.95 69.63 70.59 59.55
PGQA 91.84 67.86 68.31 59.27
ViT-LMHA 99.10 94.05 91.94 92.60
GQA 94.81 76.41 74.57 67.73
KDGQA 88.87 64.50 62.90 52.49
DGQA 96.36 81.67 79.93 75.83
PGQA 93.21 75.03 73.33 66.78
Table 3: Test accuracies of the finetuning runs, post-uptraining for each variant . The models outperforming GQA in its category
are highlighted in bold .
Model VariantAccuracy
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-SMHA 98.42 90.94 87.29 86.51
GQA 97.27 84.70 80.95 80.23
KDGQA 93.57 64.34 71.02 65.83
DGQA 97.09 84.54 80.81 80.03
PGQA 95.81 78.68 75.44 75.79
ViT-BMHA 98.67 91.91 90.17 91.04
GQA 93.32 71.78 72.42 62.87
KDGQA 90.36 65.64 65.99 56.42
DGQA 92.80 70.17 71.70 61.42
PGQA 91.85 68.19 68.73 59.75
ViT-LMHA 99.10 94.05 91.94 92.60
GQA 94.81 76.41 74.57 67.73
KDGQA 86.69 63.34 67.18 61.67
DGQA 93.77 75.83 73.79 66.04
PGQA 94.19 73.66 71.91 66.84
Table 4: Test accuracies of the finetuning runs, post-uptraining using only the GQA checkpoint .
anisms. It suggests that even small perturbations can impact
attention distributions, especially when the model is already
tuned to operate in a specific manner. This finding under-
lines the importance of extensive uptraining when making
such modifications to pretrained models, as the forward pass
dynamics are significantly altered, necessitating additional
adjustments to recover model performance.
From Table 4, it can be seen that our variants perform
worse on an absolute scale, irrespective of comparisons to
GQA. This highlights that special uptraining must be con-
ducted for variants that are modifications to GQA, coming
from conversions to MHA checkpoints.5.2 Inference Time
Table 5 presents a comparison of inference time for base
models with GQA serving as the baseline. PGQA exhibits
the highest increase in inference time by 3.31%, while
DGQA and KDGQA show minimal increases of 0.45% and
0.21% respectively.
5.3 Sensitivity to Training
Our results clearly show that asymmetric groupings andac-
tivation informed allocations can help improve performance.
However, there are many considerations when aiming to im-
prove mechanisms for existing models.
From the previous subsection, we again point out that the

--- PAGE 7 ---
Model Inference Time in Seconds % Difference
GQA 0.192415 -
DGQA 0.193276 0.4477%
KDGQA 0.192812 0.2064%
PGQA 0.198783 3.3098%
Table 5: Inference time comparison between ViT-B variants
averaged across batch size of 288, single NVIDIA GeForce
RTX 4090 24GB GPU.
scale/size of the dataset and model play a significant role
in determining whether the variants will have the capac-
ity to outperform the benchmark. With larger models, there
are a larger number of heads, which means there is a big-
ger ceiling on the number of key-value heads we can tinker
with. A larger number of key-value heads allows informed-
allocation mechanisms to really shine, since having too few
key-value heads implies there is a much narrower margin
for error in forming groups. This would likely be less of an
issue when applying to Transformers for natural-language
tasks since many popular benchmarks and models are gen-
erally very large in scale (Hoffmann et al. 2022).
This is not an isolated occurence of the sensitivity of a
new approach to the scale of the model and dataset. Doso-
vitskiy et al. (2021) make similar notes in their paper on the
Vision Transformer when discussing the pretraining data re-
quirements: ViT-L underperforms in comparison to ViT-B
on ImageNet-1k, performs similarly on ImageNet-21k, and
surpasses on JFT-300M. We hypothesize that this pattern
could be extrapolated to our study in that much more calibra-
tion is required for activation-informed grouping approaches
to surpass the static grouping approach.
It is also worth pointing out again that KDGQA and
DGQA have the same fundamental idea, but the latter takes
a slightly different perspective in one step of the computa-
tion. Despite this small nuance, it is very clear that DGQA
is significantly better than KDGQA, and that too only the
EMA variant.
Another challenge we experienced was that of the learn-
ing rates. We found that a learning rate of 1×10−3was too
high for our variants specifically, breaking their performance
on all datasets. A lower, and more popular, learning rate of
3×10−4performed better for PGQA, but still broke DGQA.
We finally started seeing reasonable results with 1×10−4,
and settled on using 1×10−5(see Appendix A).
The final ingredient in the recipe for training a vari-
ant of a (converted) pretrained model was the uptrain-
ing dataset . For preliminary experiments, we tinkered with
training GQA and its variants
1. From scratch,
2. Converting from a MHA checkpoint but not uptraining,
3. Uptraining on CINIC-10 for 10 epochs,
4. Uptraining on ImageNet-1k for 1 epoch.
We found that the results using our variants were rather
poor when the models were not uptrained properly: specif-
ically when we used a modified version of CINIC-10 (Dar-low et al. 2018), which had 270000 training images. The
lack of variety and the raw low-resolution made this a
poor choice, which became evident when we uptrained on
ImageNet-1k for just one epoch. Though the number of
training steps in the fourth setup was less than a half that
of the third, the impact of the quality of uptraining was
significant since that is when we began to see our variants
outperform the GQA benchmark. Due to time and resource
constraints, we were unable to experiment with uptraining
on ImageNet-21k or JFT-300M, or longer calibration on
ImageNet-1k.
We explore this idea in more depth in Appendix B.
6 Conclusion
We introduce KDGQA, DGQA, and PGQA: strategies for
moving past the static grouping strategy as was found in
GQA. We find that DGQA, being a refined version of
KDGQA, significantly outperforms uptrained GQA on ViT-
L, and shows larger gains in performance for more sophis-
ticated datasets. This shows the promise of calibrating for
more informed and adaptive grouping mechanisms.
7 Future Works
Since our experiments demonstrated the viability of improv-
ing the performance of Transformers using GQA through
a more informed grouping mechanism and diverging from
uniform allocations, the first step would be to extend these
to Decoders. With the popularity of Natural Language tasks
nowadays, it would be an interesting series of experiments to
tinker with LLMs that have billions of parameters and deal
with tasks that have a much higher intrinsic dimensionality,
such as language modeling. Based on our experiences, we
believe that this combination could lead to even larger per-
formance gains with the usage of informed grouping mech-
anisms.
Even within the model-agnostic framework, there are in-
teresting directions we plan to explore. Firstly, we note that
using the norms of the Keys, or manipulating the atten-
tion maps, still does not actually allow the activations to
communicate with each other. Specifically, we plan to ex-
plore whether it is possible to leverage the affinities between
the queries and keys, and the keys and values, to improve
upon the grouping mechanism even more. We could also di-
vert from just activation-informed approaches, and look at
weight-informed approaches, to dictate the allocations in-
dependently of the input. These are directions in the same
neighborhood as Chen et al. (2024), but we wish to take less
of a stochastic approach to finding optimal groupings and
give both the variants and the benchmark the same number
of training steps for a fair evaluation.
References
Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y .;
Lebr ´on, F.; and Sanghai, S. 2023. GQA: Training Gener-
alized Multi-Query Transformer Models from Multi-Head
Checkpoints. arXiv:2305.13245.

--- PAGE 8 ---
Bossard, L.; Guillaumin, M.; and Van Gool, L. 2014. Food-
101 – Mining Discriminative Components with Random
Forests. In European Conference on Computer Vision .
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Models
are Few-Shot Learners. arXiv:2005.14165.
Chen, Y .; Zhang, C.; Gao, X.; Mullins, R. D.; Con-
stantinides, G. A.; and Zhao, Y . 2024. Optimised
Grouped-Query Attention Mechanism for Transformers.
arXiv:2406.14963.
Chinnakonduru, S. S.; and Mohapatra, A. 2024.
Weighted Grouped Query Attention in Transformers.
arXiv:2407.10855.
Darlow, L. N.; Crowley, E. J.; Antoniou, A.; and Storkey,
A. J. 2018. CINIC-10 is not ImageNet or CIFAR-10.
arXiv:1810.03505.
Dong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;
Chen, D.; and Guo, B. 2022. CSWin Transformer: A Gen-
eral Vision Transformer Backbone with Cross-Shaped Win-
dows. arXiv:2107.00652.
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.
An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale. arXiv:2010.11929.
Hoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.;
Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.;
Welbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.;
van den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.; Si-
monyan, K.; Elsen, E.; Rae, J. W.; Vinyals, O.; and Sifre, L.
2022. Training Compute-Optimal Large Language Models.
arXiv:2203.15556.
Javadi, F.; Ahmed, W.; Hajimolahoseini, H.; Ataiefard, F.;
Hassanpour, M.; Asani, S.; Wen, A.; Awad, O. M.; Liu,
K.; and Liu, Y . 2023. GQKV A: Efficient Pre-training
of Transformers by Grouping Queries, Keys, and Values.
arXiv:2311.03426.
Joshi, V .; Laddha, P.; Sinha, S.; Omer, O. J.; and Sub-
ramoney, S. 2024. QCQA: Quality and Capacity-aware
grouped Query Attention. arXiv:2406.10247.
Krizhevsky, A. 2009. Learning multiple layers of features
from tiny images. Technical report.
Liu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;
and Guo, B. 2021. Swin Transformer: Hierarchical Vision
Transformer using Shifted Windows. arXiv:2103.14030.
mnmoustafa, M. A. 2017. Tiny ImageNet.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;
Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;
Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Vi-
sual Recognition Challenge. International Journal of Com-
puter Vision (IJCV) , 115(3): 211–252.Shazeer, N. 2019. Fast Transformer Decoding: One Write-
Head is All You Need. arXiv:1911.02150.
Steiner, A.; Kolesnikov, A.; Zhai, X.; Wightman, R.; Uszko-
reit, J.; and Beyer, L. 2022. How to train your ViT? Data,
Augmentation, and Regularization in Vision Transformers.
arXiv:2106.10270.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,
G. 2023. LLaMA: Open and Efficient Foundation Language
Models. arXiv:2302.13971.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2023. At-
tention Is All You Need. arXiv:1706.03762.
Wightman, R. 2019. PyTorch Image Models. https://github.
com/rwightman/pytorch-image-models.

--- PAGE 9 ---
A Learning Rates
Following the notes from our primary evaluation, we wanted
to show the impact of learning rates on the models and the
variants. A learning rate of 1×10−4was where training be-
gan to stabilize across all models and variants - we found
that lower learning rates (up to 1×10−5) were more benefi-
cial for fine-tuning.
Table 6 shows the results from the same setup as Table 3,
but with a higher learning rate. We find that some of the ViT-
S variants only slightly outperform the GQA benchmark,
though not by significant margins, and that DGQA outper-
forms GQA consistently when using ViT-L.
If numbers are compared across these two tables, it can
be found that the models perform better with lower learn-
ing rates - the gains in performance by DGQA are also more
significant, indicating that these models are much more sen-
sitive to the training setup.
B Impact of Uptraining Methodology
In our preliminary experiments, we explored the feasibility
of reducing the computational cost of uptraining by utiliz-
ing the CINIC-10 dataset for model calibration, as opposed
to the more resource-intensive ImageNet-1k. We uptrained
on a variant of CINIC-10, which contained 270000 images,
for10epochs, which was just shy of double the amount of
training steps required for 1epoch on ImageNet-1k.
The low-resolution, lack of variety in classes, and small
size of the dataset was shown to be far from optimal for our
models. For brevity, we elected to evaluate all models using
the corresponding GQA checkpoints only, as can be seen in
Table 7.
Since CINIC-10 contains the same 10 classes from
CIFAR-10, we find that the models obviously perform better
on CIFAR-10. However, the shortcomings of the dataset are
reflected when we move on to the remaining three datasets,
where the models fail to generalize horrifically, experiencing
significant drops in accuracy when compared to Table 4.
This experiment shows that the quality of uptraining is
very important, and that it was a necessary step to switch
to ImageNet-1k. We believe that uptraining on ImageNet-
21k and JFT-300M could improve the quality of the final
fine-tuned models even more, but due to resource and time
constraints, we were unable to explore this avenue.
C Head Similarity Bias
In MHA, the output of each head shows no dominant simi-
larity with outputs from any other head. In fact, we see dis-
similarity between outputs of a head compared to heads that
are further away from it.
However in GQA, output of heads are highly similar with
other heads in the group causing an intra-group similarity
bias which, among other reasons, might be contributing to a
drop in accuracy in GQA compared to MHA.
We initially introduced PGQA as a way to mitigate this
intra-group similarity bias, however we observed in Figure 3
that PGQA also wipes out other trends observed in the heat
map of MHA. DGQA on the other hand proves to be an in-
teresting middle ground between GQA and MHA (Figure 5).We show in Figure 6 that the heat map of DGQA is close to
a weighted average of GQA and MHA. This way, DGQA is
able to largely mitigate the intra-group similarity bias while
also retaining dissimilarity trends between heads as seen in
heat map of MHA.
D Complex Data Needs more Non-Uniform
Groups
We trained the same ViT from scratch with the same hy-
per parameters (Batch Size of 32, a learning rate of 1e−4,
AdamW optimiser, and window size of 100) for 1000 train-
ing steps on MNIST and CIFAR-10 datasets. Our experi-
ments in Figure 7 show that the more complex, feature dense
CIFAR-10 dataset chooses a higher proportion of its allo-
cations (which change every 100 training steps) to be non-
uniform.
This simple experiment shows the dependence of group
allocations on the dataset, however the static nature of GQA
does not allow for these dynamic allocations which are pro-
vided by our DGQA mechanism.
E Correlation between Number of
Parameters and Non-Uniform Groups
We repeated the experiments from Section D in the ap-
pendix but we progressively increase number of parameters
by increasing the embedding dimensionality but keeping the
number of layers constant. Figure 8 shows that while num-
ber of layers remain constant, simply increasing parameters
does not necessarily lead to an increase in the number of
non-uniform groups relative to total group allocations.
However, when we repeat these experiments again but this
time, we keep the embedding dimensionality constant but in-
crease the number of layers and hence the number of param-
eters too. Figure9 shows a much stronger upward trend de-
picting that increasing the number of layers leads to a larger
percentage of non-uniform group allocations.

--- PAGE 10 ---
Model VariantAccuracy
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-SMHA 96.29 85.87 84.93 78.61
GQA 95.73 83.77 82.22 76.38
KDGQA 93.14 76.53 77.78 66.64
DGQA 96.10 83.57 82.30 76.16
PGQA 95.19 81.03 80.03 73.02
ViT-BMHA 95.08 81.86 81.13 73.04
GQA 92.80 73.58 73.75 62.99
KDGQA 91.61 71.35 71.34 59.60
DGQA 91.91 73.42 73.29 60.98
PGQA 90.93 72.06 72.02 60.65
ViT-LMHA 96.08 84.97 83.24 78.19
GQA 94.05 76.61 76.46 65.62
KDGQA 87.33 61.24 66.85 54.33
DGQA 95.11 80.85 79.08 71.91
PGQA 92.71 70.37 72.61 65.20
Table 6: Test accuracies of the finetuning runs, post-uptraining on ImageNet-1k with a learning rate of 1×10−4, using variant-
specific checkpoints. The models outperforming GQA in its category are highlighted in bold .
Model VariantAccuracy
CIFAR-10 CIFAR-100 Food101 Tiny ImageNet
ViT-SGQA 98.13 72.78 61.83 53.88
KDGQA 95.01 62.71 53.83 44.61
DGQA 98.15 72.91 60.88 53.49
PGQA 97.68 66.42 54.02 48.37
ViT-BGQA 96.74 67.36 56.46 49.26
KDGQA 95.23 61.28 50.14 41.74
DGQA 96.57 66.44 54.83 46.72
PGQA 96.25 63.52 50.34 44.41
ViT-LGQA 97.95 77.31 66.80 59.85
KDGQA 97.08 71.38 60.17 54.37
DGQA 97.78 76.77 65.53 59.31
PGQA 97.77 74.11 63.08 56.77
Table 7: Test accuracies of the finetuning runs, post-uptraining on CINIC-10 with a learning rate of 1×10−5, using the GQA-
only checkpoint.
Figure 5: DGQA is an interesting middle ground between GQA and MHA. Unlike PGQA, heat map of DGQA does not wipe
out patterns observed in MHA.

--- PAGE 11 ---
Figure 6: DGQA roughly approximates a weighted average between GQA and MHA. DGQA preserves trends from MHA while
substantially alleviating the intra-group similarity bias afflicting GQA.
Figure 7: Model architecture being the same, it seems like
the more complicated CIFAR10 prefers more non-uniform
groups during training.
Figure 8: The %of non-uniform groups show no clearly pat-
tern when we increase parameters while keeping the layers
constant.
Figure 9: The %of non-uniform groups increase as we in-
crease the number of layers in the model.
