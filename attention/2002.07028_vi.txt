# 2002.07028.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2002.07028.pdf
# Kích thước tệp: 713171 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Nút thắt Thấp-hạng trong Mô hình Attention Đa-đầu
Srinadh Bhojanapalli* Chulhee Yun†Ankit Singh Rawat‡Sashank J. Reddi§
Sanjiv Kumar¶
Tóm tắt
Kiến trúc Transformer dựa trên Attention đã tạo ra những tiến bộ đáng kể trong lĩnh vực xử lý ngôn ngữ tự nhiên.
Ngoài các kỹ thuật tiền huấn luyện mới, những cải tiến gần đây chủ yếu dựa vào việc làm việc với chiều nhúng tương đối lớn hơn cho các token. Thật không may, điều này dẫn đến các mô hình quá lớn để có thể sử dụng trong các tác vụ downstream. Trong bài báo này, chúng tôi xác định một trong những yếu tố quan trọng góp phần vào yêu cầu kích thước nhúng lớn. Cụ thể, phân tích của chúng tôi nhấn mạnh rằng việc điều chỉnh tỷ lệ giữa số lượng đầu và kích thước mỗi đầu trong kiến trúc hiện tại tạo ra một nút thắt thấp-hạng trong các đầu attention, gây ra hạn chế này. Chúng tôi tiếp tục xác thực điều này trong các thí nghiệm của mình. Như một giải pháp, chúng tôi đề xuất đặt kích thước đầu của một đơn vị attention bằng độ dài chuỗi đầu vào, và độc lập với số lượng đầu, dẫn đến các lớp attention đa-đầu có khả năng biểu diễn mạnh mẽ hơn một cách chứng minh được. Chúng tôi thực nghiệm cho thấy điều này cho phép chúng ta huấn luyện các mô hình với chiều nhúng tương đối nhỏ hơn và có độ mở rộng hiệu suất tốt hơn.

1 Giới thiệu
Các kiến trúc dựa trên Attention, chẳng hạn như Transformers, đã hiệu quả cho các tác vụ mô hình hóa chuỗi như dịch máy [Gehring et al., 2017, Vaswani et al., 2017], trả lời câu hỏi, phân loại câu [Radford et al., 2018, Devlin et al., 2018] và tạo tài liệu [Liu et al., 2018]. Những mô hình này đã nổi lên như những lựa chọn thay thế tốt hơn cho các mô hình tái phát - RNNs [Sutskever et al., 2014], LSTMs [Hochreiter and Schmidhuber, 1997] và GRUs [Cho et al., 2014]. Điều này chủ yếu do cấu trúc feed forward của chúng, loại bỏ nút thắt xử lý tuần tự cho dữ liệu chuỗi, làm cho chúng dễ huấn luyện hơn so với các mô hình tái phát. Các mô hình self attention cũng đã tìm thấy ứng dụng trong thị giác [Wang et al., 2018], mạng đối kháng [Zhang et al., 2018], học tăng cường [Zambaldi et al., 2018, Li, 2017] và nhận dạng giọng nói [Chiu et al., 2018].

Những tiến bộ gần đây trong việc sử dụng các mô hình self attention trong các tác vụ ngôn ngữ tự nhiên đã được thực hiện bằng cách đầu tiên sử dụng một tác vụ mô hình hóa ngôn ngữ để tiền huấn luyện các mô hình và sau đó tinh chỉnh các mô hình đã học trên các tác vụ downstream cụ thể. Radford et al. [2018] và Devlin et al. [2018] đã sử dụng Transformers để tiền huấn luyện một mô hình ngôn ngữ và cho thấy rằng mô hình được tinh chỉnh vượt trội hơn LSTMs trên nhiều tác vụ hiểu ngôn ngữ tự nhiên và trả lời câu hỏi. Ví dụ, BERT [Devlin et al., 2018], một mô hình transformer 24 lớp, được cho thấy đạt được hiệu suất state of the art trên một số tác vụ NLP, bao gồm trên tập dữ liệu SQuAD. Những tiến bộ này, ngoài các tác vụ tiền huấn luyện mới lạ, dựa vào các mô hình lớn hơn với kích thước nhúng lớn hơn. Mô hình BERT sử dụng kích thước nhúng 1024 [Devlin et al., 2018]; GPT-2 sử dụng các mô hình với kích thước nhúng lên đến 1600 [Radford et al., 2019].

Một khối Transformer duy nhất bao gồm hai thành phần chính: một lớp self attention đa-đầu theo sau bởi một lớp feed forward [Vaswani et al., 2017]. Một đầu duy nhất trong một lớp attention đa-đầu, tính toán self attention giữa các token trong chuỗi đầu vào, sau đó sử dụng nó để tính toán một trung bình có trọng số của các nhúng cho mỗi token. Mỗi đầu chiếu dữ liệu vào một không gian con chiều thấp hơn, và tính toán self attention trong không gian con này. Kích thước chiếu này cho mỗi đầu thường được gọi là kích thước đầu.

*Google Research NY . bsrinadh@google.com
†MIT. Dựa trên công việc thực hiện tại Google Research New York. chulheey@mit.edu
‡Google Research NY . ankitsrawat@google.com
§Google Research NY . sashank@google.com
¶Google Research NY . sanjivk@google.com
1arXiv:2002.07028v1  [cs.LG]  17 Feb 2020

--- TRANG 2 ---
# đầu 8 16 32
# tham số 336M 336M 336M
SQuAD - F1 90.89±0.15 90.61±0.14 90.45±0.08
SQuAD - EM 84.1±0.34 83.75±0.27 83.48±0.13
MNLI 85±0.2 84.5±0.4 84.4±0.2
Bảng 1: Hiệu suất của BERT LARGE [Devlin et al., 2018], một Transformer 24 lớp với kích thước nhúng 1024, bị suy giảm với số lượng đầu tăng lên sau 8 đầu.

Để giữ số lượng tham số cố định trong lớp attention bất kể số lượng đầu, quy tắc phổ biến là điều chỉnh kích thước đầu với 1/(số lượng đầu). Quy tắc này ban đầu được đề xuất trong Vaswani et al. [2017] và đã trở thành một quy tắc tiêu chuẩn de facto trong các mô hình attention đa-đầu [Radford et al., 2018, Devlin et al., 2018]. Tuy nhiên, việc tăng số lượng đầu làm giảm kích thước đầu, làm giảm khả năng biểu diễn của các đầu riêng lẻ. Chúng tôi chứng minh rằng việc giảm kích thước đầu xuống một giá trị dưới độ dài chuỗi đầu vào làm hại khả năng biểu diễn của mỗi đầu (xem Định lý 1). Điều này là do kích thước đầu nhỏ hơn tạo ra một ràng buộc hạng trên các ma trận chiếu trong mỗi đầu, và giới hạn khả năng biểu diễn của chúng. Chúng tôi thực sự nhận thấy hiệu ứng này trong thực tế: trong khi hiệu suất cải thiện với việc tăng số lượng đầu ở ban đầu [Devlin et al., 2018], chúng tôi nhận thấy sự sụt giảm hiệu suất khi số lượng đầu tăng vượt quá một ngưỡng nhất định, như thấy trong Bảng 1 và Hình 1 (xem thêm Bảng 4(A) trong Vaswani et al. [2017]).

Để tránh làm tổn hại hiệu suất, các mô hình hiện tại cho phép nhiều đầu bằng cách tăng kích thước nhúng, từ đó tăng kích thước đầu. Tuy nhiên, kích thước nhúng lớn hơn, ngoài việc tăng số lượng tham số, làm cho việc sử dụng mô hình và các nhúng đã học trong các tác vụ downstream trở nên đắt đỏ, vì kích thước mô hình downstream tỷ lệ với kích thước nhúng của các token. Ví dụ, thời gian suy luận và bộ nhớ cần thiết trong các tác vụ truy xuất thường tăng tuyến tính với kích thước nhúng.

Trong bài báo này, chúng tôi đề xuất đặt kích thước đầu của các đơn vị attention bằng độ dài chuỗi đầu vào. Trong khi đây là một thay đổi siêu tham số đơn giản trong kiến trúc Transformer, chúng tôi cho thấy rằng việc đặt giá trị này một cách thích hợp là quan trọng để tránh nút thắt thấp-hạng (xem Định lý 1), và để cải thiện khả năng biểu diễn (xem Định lý 2). Kích thước đầu cố định này cũng độc lập với cả số lượng đầu và kích thước nhúng của mô hình. Điều này cho phép chúng ta huấn luyện các mô hình với kích thước nhúng tương đối nhỏ hơn (do đó ít tham số hơn) mà không ảnh hưởng đến kích thước đầu. Một lợi ích khác của kích thước đầu cố định là không giống như cài đặt tiêu chuẩn yêu cầu số lượng đầu phải là một yếu tố của kích thước nhúng, chúng ta tự do đặt một số lượng đầu tùy ý như yêu cầu cho tác vụ.

Thú vị thay, chúng tôi lưu ý rằng cách tiếp cận đơn giản nhưng mới lạ này của việc cố định kích thước đầu trong Transformers đa-đầu dẫn đến hiệu suất vượt trội về mặt thực nghiệm. Chúng tôi đánh giá Transformers được huấn luyện với kích thước đầu cố định này trên mô hình hóa ngôn ngữ (tập dữ liệu LM1B), suy luận ngôn ngữ tự nhiên (tập dữ liệu MNLI) và các tác vụ trả lời câu hỏi (tập dữ liệu SQuAD). Chúng tôi cho thấy rằng việc cố định kích thước đầu cho phép chúng ta huấn luyện Transformers với độ mở rộng hiệu suất tốt hơn và kích thước nhúng nhỏ hơn. Chúng tôi cho thấy rằng với kích thước đầu cố định, Transformers được huấn luyện với kích thước nhúng 512 có thể phù hợp với hiệu suất của BERT LARGE [Devlin et al., 2018], một Transformer với kích thước nhúng 1024 (xem Hình 2). Chúng tôi tiếp tục trình bày kết quả thí nghiệm đánh giá tác động của các lựa chọn khác nhau về kích thước đầu và kích thước nhúng trong Phần 4.

Đóng góp của chúng tôi trong bài báo này nằm ở việc xác định và chứng minh một cách nghiêm ngặt nút thắt hạng thấp trong các mô hình attention đa-đầu, và cho thấy rằng việc cố định kích thước đầu bằng độ dài chuỗi đầu vào dẫn đến một mô hình tốt hơn một cách nghiêm ngặt, cả về mặt lý thuyết và thực nghiệm. Các đóng góp của bài báo này được tóm tắt dưới đây.

•Chúng tôi phân tích khả năng biểu diễn của lớp self attention đa-đầu và chứng minh nút thắt thấp-hạng mà kích thước đầu đặt ra trên các đơn vị attention (Định lý 1).

•Chúng tôi đề xuất đặt kích thước đầu bằng độ dài chuỗi đầu vào, và cho thấy rằng việc cố định kích thước đầu cải thiện một cách nghiêm ngặt khả năng biểu diễn của các lớp attention đa-đầu so với quy tắc tiêu chuẩn để đặt kích thước đầu (Định lý 2). Điều này cho phép chúng ta vừa tăng số lượng đầu mỗi lớp vừa giảm kích thước nhúng, mà không làm tổn hại hiệu suất. Chúng tôi phát triển một cách tiếp cận xây dựng mới lạ dựa trên construction để chứng minh kết quả này, có thể hữu ích trong việc phân tích các biến thể khác của kiến trúc Transformer.

•Chúng tôi thực nghiệm cho thấy rằng với kích thước đầu cố định, Transformers có thể được huấn luyện với độ mở rộng hiệu suất tốt hơn và

--- TRANG 3 ---
kích thước nhúng nhỏ hơn trên ba tác vụ NLP tiêu chuẩn.

1.1 Các Công trình Liên quan
Với tầm quan trọng của các mô hình self attention, đã có công việc cố gắng cả cải thiện hiệu suất và tăng tốc tính toán trong Transformers. Ott et al. [2018] và You et al. [2019] giảm độ chính xác và sử dụng huấn luyện batch lớn để giảm thời gian huấn luyện của các mô hình attention. Child et al. [2019] đề xuất các mô hình self attention thưa thớt để tăng tốc tính toán trong lớp attention cho các tác vụ tạo dữ liệu chuỗi dài. Họ cho thấy rằng những mô hình attention thưa thớt này có thể được huấn luyện trên các tác vụ với độ dài chuỗi lớn hơn 10k mà không hy sinh độ chính xác. Dehghani et al. [2018] đề xuất một mạng Transformer tái phát theo chiều sâu tái sử dụng các tham số qua các lớp. Họ cho thấy rằng sự thay đổi này làm cho các mạng Transformer trở thành Turing complete ngay cả với trọng số có độ chính xác hữu hạn. Yang et al. [2019] đề xuất một cách mới để tăng độ dài chuỗi hiệu quả mà Transformer chú ý đến, bằng cách tái sử dụng các nhúng trung gian qua các chuỗi. Họ cho thấy rằng kiến trúc đã sửa đổi hoạt động tốt hơn trên các tác vụ yêu cầu tính toán ngữ cảnh trên độ dài chuỗi dài hơn. Chúng tôi lưu ý rằng hầu hết những sửa đổi này dựa vào self attention đa-đầu, cùng một khối xây dựng của Transformers. Công việc của chúng tôi đang nghiên cứu lớp attention đa-đầu cơ bản này, và đề xuất một cách mới để đặt kích thước đầu, có thể dễ dàng áp dụng cùng với bất kỳ sửa đổi kiến trúc nào ở trên.

Wu et al. [2019] đề xuất thay thế lớp self-attention bằng các convolution động nhẹ và cho thấy hiệu suất được cải thiện trên dịch máy và mô hình hóa ngôn ngữ. Mặc dù mô hình kết quả có thời gian suy luận nhanh hơn, nó vẫn cần sử dụng kích thước nhúng lớn (1024), lớn như các mô hình attention gốc. Chúng tôi tin rằng các kỹ thuật trong bài báo này có thể được kết hợp với những kết quả này để nhận ra cả kích thước nhúng nhỏ hơn và thời gian suy luận nhanh hơn.

Sun et al. [2019] thực hiện tìm kiếm kiến trúc neural bằng phương pháp tiến hóa trên các mô hình sequence to sequence và tìm thấy một kiến trúc transformer tiến hóa, ngoài các đơn vị attention đa-đầu, còn có bộ lọc convolution và các đơn vị tuyến tính có cổng. Các sửa đổi được đề xuất của chúng tôi gần gũi hơn với Transformers về tinh thần và có thể được sử dụng làm đơn vị hạt giống cho tìm kiếm kiến trúc này.

Yang et al. [2017] đã nghiên cứu tác động của ràng buộc hạng gây ra bởi các kích thước chiếu nhỏ trong việc tính toán mất mát softmax. Tình huống trong các lớp self attention hơi khác. Trong khi khả năng biểu diễn của mỗi đầu giảm với việc giảm kích thước đầu, cùng lúc chúng ta đang tăng số lượng đầu, có thể triệt tiêu điều này và tăng khả năng tổng thể của lớp. Như chúng tôi cho thấy trong Định lý 2, quy tắc kích thước đầu phổ biến thực sự giới hạn khả năng biểu diễn của lớp attention đa-đầu.

Yun et al. [2019] nghiên cứu khả năng biểu diễn của Transformers và cho thấy chúng là các bộ xấp xỉ phổ quát của các hàm sequence to sequence. Tuy nhiên họ không nghiên cứu nút thắt hạng thấp gây ra bởi quy tắc kích thước đầu phổ biến và mối liên hệ của nó với kích thước nhúng.

Voita et al. [2019], Michel et al. [2019] nghiên cứu tầm quan trọng của các đầu khác nhau trong một lớp attention. Họ quan sát rằng, trong quá trình suy luận, nhiều đầu trong mỗi lớp có thể được cắt tỉa với ít ảnh hưởng đến dự đoán. Tuy nhiên, họ vẫn cần nhiều đầu trong quá trình huấn luyện.

Child et al. [2019], Correia et al. [2019] áp đặt cấu trúc thưa thớt trên lớp attention trong quá trình huấn luyện để cải thiện cả khả năng diễn giải và hiệu suất. Việc cố định kích thước đầu thực tế sẽ làm cho việc học các mẫu thưa thớt như vậy dễ dàng hơn, vì ràng buộc hạng thấp không cho phép một đầu biểu diễn tất cả các mẫu thưa thớt có thể. Do đó việc kết hợp những kỹ thuật này có thể cho phép huấn luyện các mô hình attention thưa thớt với kích thước nhúng nhỏ hơn.

2 Kiến trúc Transformer và Phân tích
Trong phần này, chúng tôi trình bày kiến trúc Transformer và phân tích khả năng biểu diễn của self attention đa-đầu, một thành phần chính của khối Transformer.

Đầu vào của một mạng Transformer là một chuỗi n token. Thông thường, mỗi token được chuyển đổi thành một nhúng token có chiều d bởi một lớp nhúng. Chúng ta để X ∈ R^{d×n} là ma trận nhúng tương ứng với n token trong chuỗi đầu vào.

--- TRANG 4 ---
2.1 Attention Đơn-đầu
Khối Transformer là sự kết hợp của một lớp self attention theo sau bởi một lớp feed forward [Vaswani et al., 2017]. Cả hai lớp đều có một kết nối bỏ qua và sử dụng Layer Normalization (LN) [Ba et al., 2016]. Cụ thể, đối với các nhúng token X, dot product attention được tính như sau.

Attention(X) = W_v X Softmax((W_k X)^T (W_q X) / √d_k)
= W_v X P                                                    (1)

Ở đây W_q ∈ R^{d_q×d}, W_k ∈ R^{d_k×d} và W_v ∈ R^{d_v×d} đại diện cho các ma trận chiếu liên quan đến query, key và value tương ứng trong một đơn vị attention [Vaswani et al., 2017]. Đối với một đơn vị attention đơn-đầu, chúng ta có d_q = d_k = d_v = d. Trong dot-product attention (cf. (1)), P nhằm nắm bắt ngữ cảnh của đầu vào cho một token đã cho dựa trên các token còn lại trong chuỗi đầu vào. Sau đó, đầu ra của lớp attention có dạng sau.

LN(X + W_o Attention(X));                                    (2)

trong đó LN(·) đại diện cho thao tác layer-normalization. Cho mô-đun attention, như được định nghĩa trong (1), việc đặt câu hỏi về khả năng biểu diễn các ngữ cảnh P tùy ý cho một chuỗi đầu vào X cho trước là tự nhiên.

Trong kết quả sau đây, chúng tôi thiết lập rằng đối với một kích thước chiếu đủ lớn, một đơn vị attention có thể biểu diễn bất kỳ cặp dữ liệu (X, P) nào. Chúng tôi cũng cho thấy rằng mô hình không thể biểu diễn ngữ cảnh tùy ý khi d nhỏ hơn n, tạo ra một nút thắt thấp-hạng.

Định lý 1 (Định lý Biểu diễn). Nếu d_q = d_k = d ≥ n, thì cho bất kỳ ma trận X ∈ R^{d×n} có hạng cột đầy đủ và một ma trận stochastic cột dương n×n tùy ý P, luôn tồn tại các ma trận chiếu d×d W_q và W_k sao cho

Softmax((W_k X)^T (W_q X) / √d_k) = P.                       (3)

Nếu d_q = d_k = d < n, tồn tại X và P sao cho (3) không đúng cho tất cả W_q và W_k.

Kết quả này cho thấy rằng chiều chiếu d_q = d_k = d cần phải lớn hơn độ dài chuỗi n để đơn vị attention có thể biểu diễn bất kỳ ngữ cảnh P mong muốn nào. Mặc dù kết quả này mô tả trường hợp chuỗi ví dụ đơn lẻ, nó nhấn mạnh một tính chất cơ bản của kiến trúc mô hình rằng việc giảm kích thước chiếu dưới một ngưỡng nhất định tạo ra một nút thắt.

Chứng minh Định lý 1. Trường hợp d ≥ n. Để chứng minh phần đầu của kết quả, chúng tôi trình bày một cấu trúc tường minh của W_k và W_q cho phép chúng ta tạo ra P từ X bằng dot product attention. Vì X có hạng cột đầy đủ, tồn tại một nghịch đảo trái X^y = (X^T X)^{-1} X^T ∈ R^{n×d} sao cho X^y X = I_n. Đặt W_k = Ŵ_k X^y và W_q = Ŵ_q X^y. Khi đó

X^T W_k^T W_q X = X^T (X^y)^T Ŵ_k^T Ŵ_q X^y X = I_n Ŵ_k^T Ŵ_q I_n
= Ŵ_k^T Ŵ_q = Ŵ_{kq}.                                      (4)

Bây giờ việc lựa chọn W_q và W_k ở trên đã xử lý sự phụ thuộc vào X, chúng ta sẽ chọn một Ŵ_{kq} phụ thuộc vào P và hoàn thành cấu trúc. Dưới đây chúng ta biểu diễn thao tác Softmax trên các tích trong query và key. Lưu ý rằng Softmax ở đây là toán tử theo cột tính toán điểm attention cho mỗi query. Bằng cách sử dụng (4), chúng ta thu được

Softmax((W_k X)^T (W_q X) / √d_k) = Softmax[Ŵ_{kq} / √d_k]
= exp(Ŵ_{kq} / √d_k) D_{Ŵ_{kq}}^{-1},

trong đó D_{Ŵ_{kq}} là một ma trận chéo n×n sao cho

(D_{Ŵ_{kq}})_{ii} = Σ_{j=1}^n exp((Ŵ_{kq})_{ji} / √d_k)
= (1^T exp(Ŵ_{kq} / √d_k))_i.

--- TRANG 5 ---
Do đó, chúng ta có thể thiết lập kết quả mong muốn bằng cách cho thấy rằng luôn tồn tại một Ŵ_{kq} thỏa mãn phương trình điểm cố định sau.

exp(Ŵ_{kq} / √d_k) = P D_{Ŵ_{kq}}.                         (5)

Cho P, để xây dựng Ŵ_{kq} như vậy, chúng ta chọn một ma trận chéo dương tùy ý D_0, và đặt

Ŵ_{kq} = √d_k log(P D_0).                                   (6)

Vì P là một ma trận dương, Ŵ_{kq} như vậy luôn tồn tại. Tiếp theo, chúng ta xác minh rằng cấu trúc này thực sự thỏa mãn phương trình điểm cố định (cf. (5)). Lưu ý rằng

D_{Ŵ_{kq}} = Diag(1^T exp(Ŵ_{kq} / √d_k))
= Diag(1^T P D_0)
= D_0.                                                      (7)

Phương trình cuối cùng tuân theo thực tế rằng P là một ma trận stochastic cột. Bây giờ, sử dụng (6) và (7),

exp(Ŵ_{kq} / √d_k) = P D_0 = P D_{Ŵ_{kq}}.

Điều này hoàn thành phần đầu của chứng minh.

Trường hợp d < n. Xem xét trường hợp d = 1 và n = 2. Khi đó X ∈ R^{1×2} và W_q và W_k ∈ R^{1×1}. Đặt X = [1, 0]. Khi đó

Softmax((W_k X)^T (W_q X) / √d_k) = Softmax([1, 0]^T W_k^T W_q [1, 0] / √d_k)
= Softmax[W_k W_q  0 ]
                [0      0 ]

Ma trận này rõ ràng không thể được sử dụng để tạo ra P có các phần tử khác biệt trong cột thứ hai, ví dụ, P = [0.5  0.75]
                                                                                                                      [0.5  0.25]

2.2 Attention Đa-đầu
Như đã thảo luận trong Phần 2.1, một đơn vị attention cập nhật nhúng của một token đầu vào dựa trên trung bình có trọng số của các nhúng của tất cả các token trong chuỗi, sử dụng ngữ cảnh P (cf.(1)). Vaswani et al. [2017] đã đề xuất cơ chế attention Đa-đầu tăng khả năng biểu diễn của một lớp attention, trong đó nhiều đơn vị attention hoạt động trên các chiếu chiều thấp khác nhau của đầu vào, với mỗi đơn vị attention được gọi là một đầu. Điều này được theo sau bởi việc nối các đầu ra từ các đầu khác nhau. Cụ thể, việc tính toán bên trong một attention Đa-đầu với h đầu có dạng sau:

head(X)_i = W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h)) ∈ R^{d/h×n}

MultiHead(X) = Concat[head(X)_1; ...; head(X)_h] ∈ R^{d×n}.

Đầu ra của lớp attention Đa-đầu sau đó trở thành

Z = LN(X + W_o MultiHead(X));                               (8)

trong đó W_o ∈ R^{d×d}. Đối với một mô hình với h đầu, các ma trận chiếu query, key và value {W_q^i}, {W_k^i} và {W_v^i} là các ma trận d/h×d. Do đó, mỗi đầu chiếu đầu vào lên một không gian con d/h-chiều để tính toán ngữ cảnh, và giữ số lượng tham số cố định mỗi lớp. Việc sử dụng MultiHead đã dẫn đến hiệu suất tốt hơn về mặt thực nghiệm so với lớp attention đơn đầu [Vaswani et al., 2017].

--- TRANG 6 ---
(a) LM1B
(b) LM1B
Hình 1: Hiệu suất của Transformers được huấn luyện với quy tắc kích thước đầu phổ biến (d_p = d/h) (baseline) so với kích thước đầu cố định (d_p = 32) trên tác vụ mô hình hóa ngôn ngữ (LM1B) trên tập test. Chúng tôi huấn luyện các mô hình baseline với kích thước nhúng từ 256 đến 512. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 256 và kích thước đầu là 32, và thay đổi số lượng đầu từ 4 đến 70, trong khi phù hợp với số lượng tham số. Các biểu đồ rõ ràng chỉ ra rằng việc cố định kích thước đầu cho phép chúng ta huấn luyện Transformers với kích thước nhúng nhỏ hơn (biểu đồ (b)), và với độ mở rộng hiệu suất tốt hơn (biểu đồ (a)). Lưu ý rằng đối với perplexity, giá trị thấp hơn là tốt hơn.

2.3 Nút thắt Thấp-hạng
Trong khi việc tăng số lượng đầu dường như mang lại cho mô hình khả năng biểu diễn nhiều hơn, cùng lúc chúng ta đang giảm kích thước đầu, có thể làm giảm khả năng biểu diễn. Khi số lượng đầu h lớn hơn d/n, đơn vị attention bên trong mỗi đầu chiếu lên một chiều nhỏ hơn n, tạo ra một nút thắt thấp-hạng và mất khả năng biểu diễn các vectơ ngữ cảnh tùy ý (cf. Định lý 1). Thú vị thay, điều này phù hợp với quan sát thực nghiệm trong Bảng 1 rằng việc tăng h vượt quá 8 dẫn đến suy giảm hiệu suất trong BERT LARGE [Devlin et al., 2018]; lưu ý rằng d = 1024 và n = 128 cho hầu hết giai đoạn tiền huấn luyện của BERT LARGE.

Vì độ dài chuỗi được cố định từ dữ liệu/tác vụ có sẵn, cách duy nhất để tăng số lượng đầu mà không tạo ra nút thắt thấp-hạng là bằng cách tăng kích thước nhúng d. Đây là một hạn chế cơ bản của quy tắc kích thước đầu hiện đang thống trị, rằng chúng ta cần tăng kích thước nhúng để hỗ trợ nhiều đầu hơn.

Thật không may, việc tăng kích thước nhúng dẫn đến yêu cầu tính toán và bộ nhớ cao hơn để huấn luyện và lưu trữ mô hình. Hơn nữa, vì việc sử dụng các nhúng đã học từ các mô hình dựa trên Transformer cho các tác vụ downstream là phổ biến [Devlin et al., 2018], kích thước nhúng lớn hơn tăng kích thước mô hình và tính toán cần thiết cho tất cả các tác vụ downstream.

3 Attention Đa-đầu Cố định
Trong phần này, chúng tôi đề xuất cố định kích thước đầu của Transformer, cho phép chúng ta tận hưởng lợi thế của khả năng biểu diễn cao hơn của nhiều đầu mà không yêu cầu kích thước nhúng phải lớn. Chìa khóa là tách rời sự phụ thuộc giữa kích thước chiếu trong một đầu và kích thước nhúng của mô hình. Các ma trận chiếu bây giờ chiếu lên các không gian con có chiều cố định d_p bất kể số lượng đầu h. Cách tiếp cận này trong đó d_p độc lập với d và h dẫn đến cơ chế attention sau.

fixedhead(X)_i = V_v^i X Softmax((V_k^i X)^T (V_q^i X) / √d_p) ∈ R^{d_p×n}

FixedMultiHead(X) = Concat[fixedhead(X)_1; ...; fixedhead(X)_h] ∈ R^{d_p h×n}.

--- TRANG 7 ---
Lưu ý rằng các ma trận chiếu được sử dụng ở đây {V_q^i}, {V_k^i} và {V_v^i} là các ma trận d_p×d. Với V_o ∈ R^{d×hd_p}, đầu ra của lớp attention đa-đầu mới này có dạng sau.

Z = LN(X + V_o FixedMultiHead(X)).

Sửa đổi này làm cho mỗi đầu attention tương tự hơn với một đơn vị ẩn trong mạng feed forward hoặc một bộ lọc trong mạng convolutional, và cho phép chúng ta thay đổi số lượng đầu mà không lo lắng về việc giảm khả năng biểu diễn mỗi đầu. Nhược điểm là, không giống như MultiHead tiêu chuẩn, số lượng tham số mỗi lớp tăng với số lượng đầu. Tuy nhiên, sửa đổi này cho phép chúng ta huấn luyện một mô hình với kích thước nhúng nhỏ hơn mà không có nút thắt thấp-hạng, cuối cùng cho phép chúng ta giảm tổng số lượng tham số trong mô hình.

3.1 MultiHead vs. FixedMultiHead Attention
Cho một lớp MultiHead, chúng ta luôn có thể biểu diễn nó bằng một lớp FixedMultiHead, bất cứ khi nào chúng ta có kích thước đầu d_p ≥ d/h. Trong khi điều này cho thấy rằng việc tăng số lượng đầu h vượt quá d/d_p làm cho các đầu riêng lẻ của FixedMultiHead có khả năng biểu diễn như những đầu trong MultiHead, không rõ ràng liệu FixedMultiHead có nghiêm ngặt biểu diễn hơn hay không. Liệu lớp FixedMultiHead có thể biểu diễn các hàm mà lớp MultiHead tiêu chuẩn không thể biểu diễn?

Trong phần này, chúng tôi cho thấy rằng thực sự, trong chế độ đa-đầu, lớp FixedMultiHead nghiêm ngặt tốt hơn lớp MultiHead tiêu chuẩn về mặt khả năng biểu diễn.

Xem xét các đơn vị attention đa-đầu tiêu chuẩn trong (8).

f_W(X) = W_o MultiHead(X).

Chúng ta ký hiệu tập hợp tất cả các ma trận tham số là W. Tương tự, xem xét hàm được biểu diễn bởi các đơn vị attention kích thước đầu cố định:

g_V(X) = V_o FixedMultiHead(X).

Đặt V là tập hợp tất cả các ma trận tham số này. Chúng ta định nghĩa F và G là lớp các hàm f_W(·) và g_V(·), tương ứng. Như đã lưu ý ở trên, nếu d_p ≥ d/h, chúng ta có F ⊆ G.

Định lý sau đây cho thấy rằng ngay cả đối với các ví dụ đơn giản trong G, các hàm trong F không thể biểu diễn chúng; điều này đã cho thấy rằng F là một tập con nghiêm ngặt của G.

Định lý 2. Đặt n ≥ 2, d ≥ d_p, và h > d/d_p. Xem xét một lớp attention FixedMultiHead g_V(·) với các tham số thỏa mãn các điều kiện sau:

V_o = [V_v^1; ...; V_v^h] có hạng đầy đủ, và (V_k^i)^T V_q^i = U, cho tất cả i = 1, ..., h, trong đó U là một ma trận hạng-d_p.

Khi đó, với bất kỳ f_W ∈ F, tồn tại X ∈ R^{d×n} sao cho f_W(X) ≠ g_V(X).

Vì ||f_W(X) - g_V(X)|| là một hàm liên tục của X, sự tồn tại của X như vậy ngụ ý rằng tích phân của chuẩn của hiệu (tức là, lỗi xấp xỉ) là nghiêm ngặt dương. Chúng tôi lưu ý rằng các giả định về V_k^i và V_q^i trong Định lý trên được thực hiện để cung cấp một chứng minh đơn giản và xây dựng; thực tế, việc MultiHead (F) không thể biểu diễn những lớp attention đơn giản như vậy cho thấy rằng tình huống có thể tồi tệ hơn đối với các hàm phức tạp hơn.

Định lý 2 cho thấy rằng khả năng biểu diễn của lớp hàm attention FixedMultiHead nghiêm ngặt vượt trội so với lớp hàm attention MultiHead tiêu chuẩn. Do đó quy tắc giảm kích thước đầu với số lượng đầu đang giới hạn khả năng biểu diễn của MultiHead, trong khi việc sử dụng kích thước đầu cố định sẽ tăng khả năng biểu diễn của các lớp attention.

4 Thí nghiệm
Mục tiêu của phần này là cho thấy rằng việc đặt kích thước đầu theo cách có nguyên tắc dẫn đến hiệu suất tốt hơn so với việc sử dụng quy tắc phổ biến. Chúng tôi lại lưu ý rằng trong khi đây là một thay đổi siêu tham số đơn giản đối với Transformer, việc đặt điều này bằng độ dài chuỗi đầu vào như được thể hiện trong phân tích của chúng tôi, cho phép chúng ta huấn luyện các mô hình tốt hơn với kích thước nhúng nhỏ hơn.

--- TRANG 8 ---
(a) SQuAD F1
(b) SQuAD EM
(c) MNLI
Hình 2: So sánh các mô hình Transformer 24 lớp được huấn luyện với quy tắc kích thước đầu phổ biến BERT LARGE (baseline) so với mô hình kích thước đầu cố định trên các tập dev SQuAD và MNLI. Chúng tôi thay đổi kích thước nhúng của các mô hình baseline từ 512 đến 1024. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 512 và kích thước đầu là 128, với số lượng đầu thay đổi từ 8 đến 32, trong khi phù hợp với số lượng tham số. Việc cố định kích thước đầu cho phép chúng ta huấn luyện các mô hình với kích thước nhúng nhỏ hơn là 512 và với hiệu suất tốt hơn.

(a)
(b)
Hình 3: Các nghiên cứu ablation trên LM1B: (a) Chúng tôi cố định kích thước nhúng của tất cả các mô hình là 256 và thay đổi khả năng của Transformers được huấn luyện với quy tắc kích thước đầu phổ biến (baseline) bằng cách tăng kích thước của các lớp feedforward. Đối với các mô hình kích thước đầu cố định, chúng tôi cố định kích thước đầu là 32, vì vậy mô hình kích thước đầu cố định 8 đầu giống với mô hình baseline 8 đầu. Chúng tôi nhận thấy rằng một lần nữa với quy tắc tiêu chuẩn, việc tăng số lượng đầu vượt quá 16 làm tổn hại hiệu suất, trong khi với kích thước đầu cố định, việc tăng số lượng đầu cải thiện hiệu suất một cách đơn điệu. (b) Chúng tôi cho thấy tác động của kích thước đầu lên hiệu suất với số lượng đầu khác nhau. Cả hai biểu đồ đều rõ ràng cho thấy lợi thế trong việc có một cách bổ sung để điều chỉnh khả năng của Transformers với kích thước nhúng cố định.

Trong phần này, chúng tôi trình bày các thí nghiệm của mình trên ba tác vụ NLP tiêu chuẩn, mô hình hóa ngôn ngữ (LM1B), trả lời câu hỏi (SQuAD), và suy luận câu (MNLI), để chứng minh: 1) Việc tăng số lượng đầu trong Transformers vượt quá một điểm nhất định làm tổn hại hiệu suất với quy tắc kích thước đầu phổ biến, nhưng luôn giúp ích với các lớp attention kích thước đầu cố định; 2) Việc tách rời kích thước đầu khỏi kích thước nhúng cho phép chúng ta huấn luyện các mô hình với kích thước nhúng nhỏ hơn; và 3) Việc đặt kích thước đầu một cách thích hợp trong Transformers cho phép chúng ta huấn luyện các mô hình với độ mở rộng hiệu suất tốt hơn. Chúng tôi đầu tiên mô tả thiết lập thí nghiệm của mình theo sau bởi kết quả và các nghiên cứu ablation về các sửa đổi được đề xuất.

4.1 Thiết lập và Tập dữ liệu
Đối với tác vụ mô hình hóa ngôn ngữ, chúng tôi sử dụng tập dữ liệu benchmark một tỷ từ (LM1B) [Chelba et al., 2013]. Tập dữ liệu này có khoảng 30M ví dụ huấn luyện và khoảng 300k ví dụ trong tập test. Chúng tôi sử dụng một tokenizer sub-word với

--- TRANG 9 ---
# đầu 8 12 16 32
# tham số 168M 193M 218M 319M
SQuAD - F1 89.6±0.17 90.25±0.21 90.43±0.14 90.95±0.14
SQuAD - EM 82.73±0.21 83.18±0.24 83.59±0.06 84.4±0.29
MNLI 83.5±0.2 84.2±0.2 83.9±0.2 84.9±0.2
(A) Tăng số lượng đầu

kích thước đầu 32 64 128 256
# tham số 130M 142M 168M 218M
SQuAD - F1 88.53±0.06 89.51±0.15 89.6±0.17 90.33±0.23
SQuAD - EM 81.19±0.21 82.41±0.32 82.73±0.21 83.36±0.48
MNLI 82.5±0.1 83.4±0.3 83.5±0.2 83.9±0.2
(B) Tăng kích thước đầu

Bảng 2: Các nghiên cứu ablation trên SQuAD và MNLI: (A) Transformer 24 lớp với kích thước đầu cố định là 128 và kích thước nhúng 512 cho thấy sự cải thiện độ chính xác với việc tăng số lượng đầu. (B) Mô hình kích thước đầu cố định với kích thước nhúng 512 và 8 đầu cho thấy sự cải thiện độ chính xác với việc tăng kích thước đầu. Điều này cho thấy rằng thực sự kích thước đầu là một tham số kiểm soát khả năng quan trọng trong kiến trúc self attention.

32k từ vựng và giới hạn đầu vào ở độ dài chuỗi 256. Chúng tôi huấn luyện một mô hình Transformer 6 lớp với bộ tối ưu hóa ADAM bằng thư viện tensor2tensor [Vaswani et al., 2018]. Thiết lập thí nghiệm chi tiết được trình bày trong Phần C.

Multi-Genre Natural Language Inference (MNLI) là một tác vụ suy luận ở cấp độ câu, được thiết kế để kiểm tra khả năng hiểu ngôn ngữ tự nhiên [Williams et al., 2018]. Cho một câu tiền đề và một câu giả thuyết, mục tiêu là dự đoán liệu giả thuyết có suy ra, mâu thuẫn hay trung lập với tiền đề. Chúng tôi báo cáo độ chính xác phân loại cho tác vụ này. Stanford Question Answering Dataset (SQuAD) là một tập dữ liệu trả lời câu hỏi, trong đó cho một đoạn văn và một câu hỏi, mục tiêu là dự đoán chuỗi từ trong đoạn văn tạo thành câu trả lời cho câu hỏi [Rajpurkar et al., 2016]. Đây là một tác vụ cấp độ từ khó hơn, so với tác vụ phân loại câu. Chúng tôi báo cáo cả điểm Exact Match (EM) và F1 cho tác vụ này. Tất cả kết quả trong phần này được báo cáo trên tập Dev, chưa được sử dụng trong bất kỳ lựa chọn thí nghiệm nào trong bài báo này.

Đối với hai tác vụ sau này, chúng tôi theo cách tiếp cận hai giai đoạn của việc đầu tiên tiền huấn luyện trên một tác vụ mô hình hóa ngôn ngữ và sau đó tinh chỉnh các mô hình trên dữ liệu tác vụ. Chúng tôi theo cùng thiết lập thí nghiệm cho cả tiền huấn luyện và tinh chỉnh như BERT [Devlin et al., 2018], và sử dụng codebase của họ¹. Chúng tôi đầu tiên tiền huấn luyện các mô hình của mình bằng mô hình ngôn ngữ có mặt nạ và các mục tiêu dự đoán câu tiếp theo, và sau đó tinh chỉnh mô hình đã tiền huấn luyện cho các tác vụ riêng lẻ [Devlin et al., 2018]. Đối với tiền huấn luyện, chúng tôi sử dụng tập dữ liệu English Wikipedia và BooksCorpus [Zhu et al., 2015]. Đầu vào của các mô hình được tokenize bằng biểu diễn WordPiece với 30000 token trong từ vựng. Chúng tôi trình bày các lựa chọn thí nghiệm chính trong Phần C, và tham khảo độc giả đến Devlin et al. [2018] để có mô tả đầy đủ về thiết lập.

Lựa chọn kích thước đầu. Sửa đổi được đề xuất của chúng tôi giới thiệu kích thước đầu d_p như một siêu tham số mô hình mới. Chúng tôi chọn kích thước đầu là 128 cho các thí nghiệm BERT của mình, vì hầu hết việc tiền huấn luyện được thực hiện với dữ liệu độ dài chuỗi 128. Trong khi chúng tôi có các nghiên cứu ablation (cf. Bảng 2(B)) cho thấy kích thước đầu lớn hơn cải thiện hiệu suất, có sự đánh đổi giữa việc tăng kích thước đầu vs số lượng đầu vs các lớp. Chúng tôi nhận thấy rằng có kích thước đầu đủ lớn, ví dụ, phù hợp với độ dài chuỗi tiền huấn luyện, tốt hơn là có kích thước nhúng lớn hơn.

4.2 Kết quả
Đối với tập thí nghiệm đầu tiên của chúng tôi, chúng tôi muốn xem liệu Transformers được huấn luyện với kích thước đầu cố định và kích thước nhúng nhỏ hơn có thể phù hợp với hiệu suất của việc huấn luyện với quy tắc kích thước đầu tiêu chuẩn nhưng với kích thước nhúng lớn hơn. Như một baseline cho tác vụ mô hình hóa ngôn ngữ, chúng tôi huấn luyện Transformers với kích thước nhúng tăng từ 256 đến 512

¹https://github.com/google-research/bert

--- TRANG 10 ---
với số lượng đầu khác nhau. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 256 và kích thước đầu là 32, với số lượng đầu tăng từ 4 đến 70. Chúng tôi nhận thấy rằng Transformers với kích thước đầu cố định và kích thước nhúng 256 có hiệu suất tốt hơn các mô hình baseline với kích thước nhúng 512 (xem Hình 1). Chúng tôi lặp lại thí nghiệm tương tự trên hai tác vụ khác, trong đó đối với baseline chúng tôi huấn luyện BERT LARGE, một Transformer 24 lớp, 16 đầu với quy tắc kích thước đầu tiêu chuẩn, với kích thước nhúng từ 512 đến 1024. Chúng tôi so sánh nó với mô hình kích thước đầu cố định, với kích thước nhúng 512 và kích thước đầu 128, với số lượng đầu tăng từ 8 đến 32. Chúng tôi lại nhận thấy rằng Transformers được huấn luyện với kích thước đầu cố định và kích thước nhúng 512 có hiệu suất tốt hơn baseline, BERT LARGE (xem Hình 2).

Lưu ý rằng việc đơn giản cố gắng tăng kích thước đầu của Transformers bằng cách giảm số lượng đầu không cải thiện hiệu suất, vì việc giảm số lượng đầu làm giảm khả năng biểu diễn của mô hình (xem Hình 4 trong Phụ lục). Do đó, cả kích thước đầu và số lượng đầu cần được đặt đủ cao để có hiệu suất tốt hơn.

4.3 Ablation
Tăng đầu. Từ Bảng 1 và Hình 1a chúng ta có thể thấy rằng việc tăng số lượng đầu làm tổn hại hiệu suất của Transformer sau một số lượng nhất định. Chúng tôi lặp lại cùng thí nghiệm với Transformer kích thước đầu cố định, và trình bày kết quả trong Bảng 2(A) và Hình 3a. Kết quả cho thấy rằng hiệu suất của mô hình đã sửa đổi cải thiện một cách đơn điệu khi số lượng đầu tăng. Điều này là do khả năng mô hình (một hàm của kích thước đầu) không còn bị giảm với việc tăng số lượng đầu.

Tăng kích thước đầu. Trong Bảng 2(B) và Hình 3b, chúng tôi trình bày so sánh giữa các mô hình với kích thước đầu khác nhau. Điều này cho thấy rằng những cải thiện trong hiệu suất của các mô hình kích thước đầu cố định thực sự đến từ việc điều chỉnh kích thước đầu của các lớp query, key và value trong đơn vị attention. Bảng cho thấy một xu hướng rõ ràng của hiệu suất tốt hơn với kích thước đầu lớn hơn, cho thấy rằng nó thực sự là một yếu tố quan trọng trong hiệu suất của các mô hình attention.

5 Kết luận
Trong bài báo này, chúng tôi đã nghiên cứu khả năng biểu diễn của các mô hình self attention đa-đầu và chứng minh nút thắt thấp-hạng dẫn đến từ kích thước đầu nhỏ trong attention đa-đầu. Chúng tôi đã cho thấy rằng kích thước nhúng lớn hơn được sử dụng trong các mô hình hiện tại là hệ quả của nút thắt thấp-hạng này trong các lớp attention đa-đầu. Chúng tôi đề xuất thay vào đó sử dụng các đơn vị attention kích thước đầu cố định, với kích thước đầu được đặt bằng độ dài chuỗi đầu vào, để tránh nút thắt này. Chúng tôi đã cho thấy rằng điều này cho phép chúng ta tăng số lượng đầu mà không tăng kích thước nhúng. Như một hệ quả, chúng ta có thể huấn luyện Transformers với kích thước nhúng nhỏ hơn và ít tham số hơn, với hiệu suất tốt hơn. Trong tương lai, sẽ thú vị khi thí nghiệm với việc thay đổi kích thước đầu trong một khối attention và qua các lớp. Điều này đòi hỏi hiểu biết sâu hơn về vai trò của mỗi lớp trong việc tính toán ngữ cảnh, đây là một hướng thú vị cho công việc tương lai.

Tài liệu tham khảo
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, and P. Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.

R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4774–4778. IEEE, 2018.

--- TRANG 11 ---
K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111, 2014.

G. M. Correia, V. Niculae, and A. F. Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184, 2019.

M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1243–1252. JMLR. org, 2017.

S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

Y. Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.

P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650, 2019.

M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 1–9, 2018.

A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. Technical report, OpenAI, 2019.

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

H. Sun, X. Tan, J.-W. Gan, H. Liu, S. Zhao, T. Qin, and T.-Y. Liu. Token-level ensemble distillation for grapheme-to-phoneme conversion. arXiv preprint arXiv:1904.03446, 2019.

I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112, 2014.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.

A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.

E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.

X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7794–7803, 2018.

--- TRANG 12 ---
A. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.

F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.

Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.

Y. You, J. Li, J. Hseu, X. Song, J. Demmel, and C.-J. Hsieh. Reducing bert pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962, 2019.

C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.

V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.

H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.

Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.

--- TRANG 13 ---
A Ký hiệu
Kích thước nhúng d
Số lượng lớp l
Số lượng đầu h
Độ dài chuỗi n
Kích thước từ vựng v
Kích thước đầu d_p

B Chứng minh
Chứng minh Định lý 2. Đầu tiên hãy viết lại các lớp MultiHead và FixedMultiHead như sau. Lớp MultiHead có thể được viết lại như

f_W(X) = W_o MultiHead(X) = Σ_{i=1}^h W_o^i W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h)),

trong đó W_o^i là các ma trận d×d/h và W_v^i, W_k^i, và W_q^i là các ma trận d/h×d. Chúng ta ký hiệu tập hợp tất cả các ma trận tham số là W.

Tương tự, viết lại lớp attention kích thước đầu cố định như

g_V(X) = V_o FixedMultiHead(X) = Σ_{i=1}^h V_o^i V_v^i X Softmax((V_k^i X)^T (V_q^i X) / √d_p),

trong đó V_o^i ∈ R^{d×d_p}, và V_v^i, V_k^i, V_q^i ∈ R^{d_p×d}. Đặt V là tập hợp tất cả các ma trận này.

Đại cương của chứng minh cơ bản là phân tích trường hợp: chúng ta chia các giá trị có thể của W thành ba loại, và cho thấy trong mỗi trường hợp tồn tại một X sao cho f_W(X) ≠ g_V(X). Đây là ba trường hợp:

•Trường hợp 1: Σ_{i=1}^h W_o^i W_v^i ≠ Σ_{i=1}^h V_o^i V_v^i.

•Trường hợp 2: Σ_{i=1}^h W_o^i W_v^i = Σ_{i=1}^h V_o^i V_v^i, và tồn tại i ∈ {1, ..., h} sao cho U = √d_p (W_k^i)^T (W_q^i) / √(d/h) không phải là skew-symmetric.

•Trường hợp 3: Σ_{i=1}^h W_o^i W_v^i = Σ_{i=1}^h V_o^i V_v^i, và tất cả U = √d_p (W_k^i)^T (W_q^i) / √(d/h) đều là skew-symmetric.

Trường hợp 1. Trong trường hợp đầu tiên, chúng ta có thể chọn bất kỳ v sao cho (Σ_{i=1}^h W_o^i W_v^i - Σ_{i=1}^h V_o^i V_v^i)v ≠ 0. Chọn X = v1^T = [v v ⋯ v]. Khi đó, lưu ý rằng với bất kỳ ma trận stochastic cột P, chúng ta có XP = X. Do đó,

Σ_{i=1}^h W_o^i W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h)) - Σ_{i=1}^h V_o^i V_v^i X Softmax((V_k^i X)^T (V_q^i X) / √d_p)
= Σ_{i=1}^h W_o^i W_v^i X - Σ_{i=1}^h V_o^i V_v^i X = (Σ_{i=1}^h W_o^i W_v^i - Σ_{i=1}^h V_o^i V_v^i)v1^T ≠ 0.

Trường hợp 2. Trong các trường hợp mà Σ_{i=1}^h W_o^i W_v^i = Σ_{i=1}^h V_o^i V_v^i, vì Σ_{i=1}^h V_o^i V_v^i có hạng đầy đủ theo giả định và mỗi W_o^i W_v^i có hạng tối đa d/h, nó tuân theo rằng tất cả các cột trong W_o^i ∈ R^{d×d/h} phải độc lập tuyến tính. Do đó, với bất kỳ v ≠ 0, {W_o^i W_v^i v, i = 1, ..., h} là một tập hợp các vectơ độc lập tuyến tính, vì mỗi W_o^i W_v^i v là một tổ hợp tuyến tính của d/h vectơ cột của W_o^i độc lập tuyến tính với các vectơ cột khác trong W_o^j, j ≠ i.

--- TRANG 14 ---
Bây giờ xem xét bất kỳ v ∈ R^d, và X = ve_1^T, trong đó e_1 = (1, 0, ..., 0) ∈ R^n. Định nghĩa φ(t) = exp(t)/(exp(t) + n - 1). Khi đó, chúng ta có

g_V(X) = Σ_{i=1}^h V_o^i V_v^i X Softmax([X^T UX / √d_p])

= Σ_{i=1}^h V_o^i V_v^i X Softmax([v^T Uv / √d_p  0 ⋯ 0; 0  0 ⋯ 0; ⋮  ⋮ ⋱ ⋮; 0  0 ⋯ 0])

= (Σ_{i=1}^h V_o^i V_v^i) φ(v^T Uv / √d_p)[v; v/n; ⋮; v/n]

= (Σ_{i=1}^h W_o^i W_v^i) φ(v^T Uv / √d_p)[v; v/n; ⋮; v/n].

Tương tự, chúng ta có thể tính toán

f_W(X) = Σ_{i=1}^h W_o^i W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h))

= Σ_{i=1}^h W_o^i W_v^i φ(v^T (W_k^i)^T W_q^i v / √(d/h))[v; v/n; ⋮; v/n].

Lưu ý rằng tất cả các cột của f_W(X) và g_V(X), từ cột thứ hai đến cột cuối cùng, đều giống nhau. Bây giờ chúng ta so sánh các cột đầu tiên:

f_W(X)_{:,1} - g_V(X)_{:,1} = Σ_{i=1}^h [φ(v^T (W_k^i)^T W_q^i v / √(d/h)) - φ(v^T Uv / √d_p)] W_o^i W_v^i v.

Nhớ lại rằng với bất kỳ v ≠ 0, W_o^i W_v^i v là độc lập tuyến tính, vì vậy f_W(X)_{:,1} - g_V(X)_{:,1} = 0 khi và chỉ khi tất cả

φ(v^T (W_k^i)^T W_q^i v / √(d/h)) - φ(v^T Uv / √d_p)

đều bằng không. Tuy nhiên, vì tồn tại i ∈ {1, ..., h} sao cho U = √d_p (W_k^i)^T (W_q^i) / √(d/h) không phải là skew-symmetric, chúng ta có thể chọn v để thỏa mãn v^T (W_k^i)^T W_q^i v / √(d/h) ≠ v^T Uv / √d_p, do đó làm cho

φ(v^T (W_k^i)^T W_q^i v / √(d/h)) - φ(v^T Uv / √d_p) ≠ 0,

do đó f_W(X)_{:,1} - g_V(X)_{:,1} ≠ 0.

Trường hợp 3. Bây giờ xem xét bất kỳ X = [v_1  v_2  0 ⋯ 0], trong đó v_1 và v_2 sẽ được chọn sau này. Định nghĩa φ_1(t_1, t_2) = exp(t_1)/(exp(t_1) + exp(t_2) + n - 2), φ_2(t_1, t_2) = exp(t_2)/(exp(t_1) + exp(t_2) + n - 2). Khi đó, chúng ta có

g_V(X) = Σ_{i=1}^h V_o^i V_v^i X Softmax([v_1^T Uv_1 / √d_p  v_1^T Uv_2 / √d_p  0 ⋯ 0; v_2^T Uv_1 / √d_p  v_2^T Uv_2 / √d_p  0 ⋯ 0; 0  0  0 ⋯ 0; ⋮  ⋮  ⋮ ⋱ ⋮; 0  0  0 ⋯ 0]).

Do đó, cột đầu tiên của g_V(X) có thể được viết như

g_V(X)_{:,1} = Σ_{i=1}^h W_o^i W_v^i [φ_1(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)v_1 + φ_2(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)v_2].

--- TRANG 15 ---
Tương tự, cột đầu tiên của f_W(X) là

f_W(X)_{:,1} = Σ_{i=1}^h W_o^i W_v^i [φ_1(v_1^T (W_k^i)^T W_q^i v_1 / √(d/h), v_2^T (W_k^i)^T W_q^i v_1 / √(d/h))v_1 + φ_2(v_1^T (W_k^i)^T W_q^i v_1 / √(d/h), v_2^T (W_k^i)^T W_q^i v_1 / √(d/h))v_2].

Vì U = √d_p (W_1^k)^T (W_1^q) / √(d/h) là skew-symmetric theo giả định, chúng ta có v_1^T [U / √d_p - (W_1^k)^T (W_1^q) / √(d/h)]v_1 = 0 với tất cả v_1. Nhớ lại rằng U có hạng-d_p theo giả định, vì vậy U / √d_p - (W_1^k)^T (W_1^q) / √(d/h) có hạng ít nhất d_p ≥ d/h ≥ 1, vì vậy chúng ta có thể chọn bất kỳ v_1 sao cho

[U / √d_p - (W_1^k)^T (W_1^q) / √(d/h)]v_1 ≠ 0.

Nếu cả [U / √d_p]v_1 và [(W_1^k)^T (W_1^q) / √(d/h)]v_1 đều khác không, chúng ta luôn có thể chọn ṽ_2 sao cho ṽ_2^T [U / √d_p]v_1 > 0 và ṽ_2^T [(W_1^k)^T (W_1^q) / √(d/h)]v_1 < 0. Điều này có nghĩa là nếu chúng ta chọn v_2 = αṽ_2 và tỷ lệ α → ∞,

φ_1(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p) → 0; φ_2(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p) → 1;

φ_1(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) → exp(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h)) / (exp(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h)) + n - 2);

φ_2(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) → 0.

Khi đó, xem xét hiệu f_W(X)_{:,1} - g_V(X)_{:,1}. Nhớ lại rằng với bất kỳ v, W_1^o W_1^v v độc lập với {W_i^o W_i^v v, i ≠ 1}. Điều này có nghĩa là, để cho thấy f_W(X)_{:,1} - g_V(X)_{:,1} ≠ 0, đủ để cho thấy rằng

[φ_1(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) - φ_1(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)]W_1^o W_1^v v_1 + [φ_2(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) - φ_2(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)]W_1^o W_1^v v_2 ≠ 0.

Nếu chúng ta tỷ lệ v_2 = αṽ_2 với α đủ lớn, số hạng thứ hai sẽ thống trị số hạng thứ nhất và số hạng thứ nhất sẽ không bao giờ có thể triệt tiêu số hạng thứ hai. Do đó, bằng cách chọn α > 0 đủ lớn, chúng ta có thể đảm bảo rằng tổng là khác không.

Ngay cả trong trường hợp một trong [U / √d_p]v_1 và [(W_1^k)^T (W_1^q) / √(d/h)]v_1 bằng không (giả sử [(W_1^k)^T (W_1^q) / √(d/h)]v_1 = 0), chúng ta có thể chọn ṽ_2 = [U / √d_p]v_1 và sử dụng một lập luận tỷ lệ tương tự. Bằng cách chọn α > 0 đủ lớn và v_2 = αṽ_2, người ta có thể cho thấy rằng hiệu f_W(X)_{:,1} - g_V(X)_{:,1} là khác không.

C Thiết lập thí nghiệm
Đối với các thí nghiệm của chúng tôi với mô hình hóa ngôn ngữ (tập dữ liệu LM1B), chúng tôi huấn luyện các mô hình Transformer 6 lớp. Chúng tôi sử dụng kích thước batch 4096 và huấn luyện cho 250k bước. Chúng tôi sử dụng tốc độ học 0.1 với warm up tuyến tính cho 10k bước đầu tiên. Chúng tôi giảm tốc độ học với căn bậc hai của số bước. Chúng tôi huấn luyện các mô hình baseline, với quy tắc kích thước đầu phổ biến, với chiều nhúng thay đổi từ 256 đến 512. Chúng tôi cố định chiều rộng của lớp feed forward trong Transformer là 1024. Ngoài ra, chúng tôi sử dụng weight decay 0.01 và dropout với xác suất 0.1 trên tất cả các lớp.

--- TRANG 16 ---
Đối với các thí nghiệm của chúng tôi với BERT, chúng tôi theo cùng thiết lập thí nghiệm như trong [Devlin et al., 2018]. Chúng tôi trình bày các chi tiết chính ở đây và tham khảo độc giả đến [Devlin et al., 2018]. Chúng tôi huấn luyện với kích thước batch 1024 cho 450k bước với đầu vào có độ dài chuỗi n = 128 theo sau bởi 50k bước với đầu vào có độ dài chuỗi 512. Ngược lại, bài báo BERT sử dụng kích thước batch 512, và thực hiện tiền huấn luyện cho 900K bước với đầu vào độ dài chuỗi 128 và 100k bước với đầu vào độ dài chuỗi 512. Chúng tôi huấn luyện bằng ADAM với tốc độ học 1e-4, và lịch trình warm up và decay tuyến tính như trong BERT. Chúng tôi sử dụng 5k bước warm up cho giai đoạn đầu tiên, và re-warm up 3k bước cho giai đoạn thứ hai [You et al., 2019]. Một lần nữa, chúng tôi sử dụng weight decay 0.01 và dropout với xác suất 0.1 trên tất cả các lớp.

Đối với tác vụ mô hình hóa ngôn ngữ, việc huấn luyện được thực hiện trên 4 chip TPUv2 trong vài giờ. Đối với các mô hình BERT, việc huấn luyện được thực hiện trên 16 chip TPUv3 trong giai đoạn đầu tiên và 64 chip TPUv3 cho giai đoạn thứ hai. Tiền huấn luyện với cấu hình này mất từ 2 đến 3 ngày. Chúng tôi không cố gắng tìm các siêu tham số tối ưu cho kiến trúc kích thước đầu cố định, và sử dụng cùng siêu tham số như được sử dụng để huấn luyện các mô hình BERT.

D Kết quả thí nghiệm bổ sung

Hình 4: Hiệu suất của Transformers được huấn luyện với quy tắc kích thước đầu phổ biến (baseline) so với các mô hình kích thước đầu cố định (d_p) cho tác vụ mô hình hóa ngôn ngữ (LM1B) trên tập test. Không giống như Hình 1, chúng tôi thay đổi cả kích thước nhúng và số lượng đầu của các mô hình baseline để giữ kích thước đầu cố định ở 32. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 256 và kích thước đầu là 32, và thay đổi số lượng đầu từ 4 đến 70, trong khi phù hợp với số lượng tham số. Biểu đồ một lần nữa rõ ràng chỉ ra lợi thế của các mô hình kích thước đầu cố định. Vấn đề chính với các mô hình baseline là việc cố định kích thước đầu ở 32 buộc số lượng đầu phải nhỏ khi kích thước nhúng nhỏ. Việc giảm số lượng đầu dưới ngưỡng nhất định làm tổn hại hiệu suất của Transformer.

--- TRANG 17 ---
# đầu 8 12 16 20
# tham số 214M 252M 290M 327M
SQuAD - F1 90.35±0.14 90.48±0.09 90.92±0.14 90.89±0.08
SQuAD - EM 83.37±0.12 83.67±0.03 84.16±0.35 84.29±0.16
MNLI 84.4±0.2 84.4±0.2 84.7±0.1 85.1±0.4
(A) Tăng số lượng đầu

Bảng 3: (A): Transformer 24 lớp được huấn luyện với kích thước đầu cố định là 128 và kích thước nhúng 768 cho thấy sự cải thiện độ chính xác với việc tăng số lượng đầu.# 2002.07028.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2002.07028.pdf
# Kích thước tệp: 713171 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Nút thắt Thấp-hạng trong Mô hình Attention Đa-đầu
Srinadh Bhojanapalli* Chulhee Yun†Ankit Singh Rawat‡Sashank J. Reddi§
Sanjiv Kumar¶
Tóm tắt
Kiến trúc Transformer dựa trên Attention đã tạo ra những tiến bộ đáng kể trong lĩnh vực xử lý ngôn ngữ tự nhiên.
Ngoài các kỹ thuật tiền huấn luyện mới, những cải tiến gần đây chủ yếu dựa vào việc làm việc với chiều nhúng tương đối lớn hơn cho các token. Thật không may, điều này dẫn đến các mô hình quá lớn để có thể sử dụng trong các tác vụ downstream. Trong bài báo này, chúng tôi xác định một trong những yếu tố quan trọng góp phần vào yêu cầu kích thước nhúng lớn. Cụ thể, phân tích của chúng tôi nhấn mạnh rằng việc điều chỉnh tỷ lệ giữa số lượng đầu và kích thước mỗi đầu trong kiến trúc hiện tại tạo ra một nút thắt thấp-hạng trong các đầu attention, gây ra hạn chế này. Chúng tôi tiếp tục xác thực điều này trong các thí nghiệm của mình. Như một giải pháp, chúng tôi đề xuất đặt kích thước đầu của một đơn vị attention bằng độ dài chuỗi đầu vào, và độc lập với số lượng đầu, dẫn đến các lớp attention đa-đầu có khả năng biểu diễn mạnh mẽ hơn một cách chứng minh được. Chúng tôi thực nghiệm cho thấy điều này cho phép chúng ta huấn luyện các mô hình với chiều nhúng tương đối nhỏ hơn và có độ mở rộng hiệu suất tốt hơn.

1 Giới thiệu
Các kiến trúc dựa trên Attention, chẳng hạn như Transformers, đã hiệu quả cho các tác vụ mô hình hóa chuỗi như dịch máy [Gehring et al., 2017, Vaswani et al., 2017], trả lời câu hỏi, phân loại câu [Radford et al., 2018, Devlin et al., 2018] và tạo tài liệu [Liu et al., 2018]. Những mô hình này đã nổi lên như những lựa chọn thay thế tốt hơn cho các mô hình tái phát - RNNs [Sutskever et al., 2014], LSTMs [Hochreiter and Schmidhuber, 1997] và GRUs [Cho et al., 2014]. Điều này chủ yếu do cấu trúc feed forward của chúng, loại bỏ nút thắt xử lý tuần tự cho dữ liệu chuỗi, làm cho chúng dễ huấn luyện hơn so với các mô hình tái phát. Các mô hình self attention cũng đã tìm thấy ứng dụng trong thị giác [Wang et al., 2018], mạng đối kháng [Zhang et al., 2018], học tăng cường [Zambaldi et al., 2018, Li, 2017] và nhận dạng giọng nói [Chiu et al., 2018].

Những tiến bộ gần đây trong việc sử dụng các mô hình self attention trong các tác vụ ngôn ngữ tự nhiên đã được thực hiện bằng cách đầu tiên sử dụng một tác vụ mô hình hóa ngôn ngữ để tiền huấn luyện các mô hình và sau đó tinh chỉnh các mô hình đã học trên các tác vụ downstream cụ thể. Radford et al. [2018] và Devlin et al. [2018] đã sử dụng Transformers để tiền huấn luyện một mô hình ngôn ngữ và cho thấy rằng mô hình được tinh chỉnh vượt trội hơn LSTMs trên nhiều tác vụ hiểu ngôn ngữ tự nhiên và trả lời câu hỏi. Ví dụ, BERT [Devlin et al., 2018], một mô hình transformer 24 lớp, được cho thấy đạt được hiệu suất state of the art trên một số tác vụ NLP, bao gồm trên tập dữ liệu SQuAD. Những tiến bộ này, ngoài các tác vụ tiền huấn luyện mới lạ, dựa vào các mô hình lớn hơn với kích thước nhúng lớn hơn. Mô hình BERT sử dụng kích thước nhúng 1024 [Devlin et al., 2018]; GPT-2 sử dụng các mô hình với kích thước nhúng lên đến 1600 [Radford et al., 2019].

Một khối Transformer duy nhất bao gồm hai thành phần chính: một lớp self attention đa-đầu theo sau bởi một lớp feed forward [Vaswani et al., 2017]. Một đầu duy nhất trong một lớp attention đa-đầu, tính toán self attention giữa các token trong chuỗi đầu vào, sau đó sử dụng nó để tính toán một trung bình có trọng số của các nhúng cho mỗi token. Mỗi đầu chiếu dữ liệu vào một không gian con chiều thấp hơn, và tính toán self attention trong không gian con này. Kích thước chiếu này cho mỗi đầu thường được gọi là kích thước đầu.

*Google Research NY . bsrinadh@google.com
†MIT. Dựa trên công việc thực hiện tại Google Research New York. chulheey@mit.edu
‡Google Research NY . ankitsrawat@google.com
§Google Research NY . sashank@google.com
¶Google Research NY . sanjivk@google.com
1arXiv:2002.07028v1  [cs.LG]  17 Feb 2020

--- TRANG 2 ---
# đầu 8 16 32
# tham số 336M 336M 336M
SQuAD - F1 90.89±0.15 90.61±0.14 90.45±0.08
SQuAD - EM 84.1±0.34 83.75±0.27 83.48±0.13
MNLI 85±0.2 84.5±0.4 84.4±0.2
Bảng 1: Hiệu suất của BERT LARGE [Devlin et al., 2018], một Transformer 24 lớp với kích thước nhúng 1024, bị suy giảm với số lượng đầu tăng lên sau 8 đầu.

Để giữ số lượng tham số cố định trong lớp attention bất kể số lượng đầu, quy tắc phổ biến là điều chỉnh kích thước đầu với 1/(số lượng đầu). Quy tắc này ban đầu được đề xuất trong Vaswani et al. [2017] và đã trở thành một quy tắc tiêu chuẩn de facto trong các mô hình attention đa-đầu [Radford et al., 2018, Devlin et al., 2018]. Tuy nhiên, việc tăng số lượng đầu làm giảm kích thước đầu, làm giảm khả năng biểu diễn của các đầu riêng lẻ. Chúng tôi chứng minh rằng việc giảm kích thước đầu xuống một giá trị dưới độ dài chuỗi đầu vào làm hại khả năng biểu diễn của mỗi đầu (xem Định lý 1). Điều này là do kích thước đầu nhỏ hơn tạo ra một ràng buộc hạng trên các ma trận chiếu trong mỗi đầu, và giới hạn khả năng biểu diễn của chúng. Chúng tôi thực sự nhận thấy hiệu ứng này trong thực tế: trong khi hiệu suất cải thiện với việc tăng số lượng đầu ở ban đầu [Devlin et al., 2018], chúng tôi nhận thấy sự sụt giảm hiệu suất khi số lượng đầu tăng vượt quá một ngưỡng nhất định, như thấy trong Bảng 1 và Hình 1 (xem thêm Bảng 4(A) trong Vaswani et al. [2017]).

Để tránh làm tổn hại hiệu suất, các mô hình hiện tại cho phép nhiều đầu bằng cách tăng kích thước nhúng, từ đó tăng kích thước đầu. Tuy nhiên, kích thước nhúng lớn hơn, ngoài việc tăng số lượng tham số, làm cho việc sử dụng mô hình và các nhúng đã học trong các tác vụ downstream trở nên đắt đỏ, vì kích thước mô hình downstream tỷ lệ với kích thước nhúng của các token. Ví dụ, thời gian suy luận và bộ nhớ cần thiết trong các tác vụ truy xuất thường tăng tuyến tính với kích thước nhúng.

Trong bài báo này, chúng tôi đề xuất đặt kích thước đầu của các đơn vị attention bằng độ dài chuỗi đầu vào. Trong khi đây là một thay đổi siêu tham số đơn giản trong kiến trúc Transformer, chúng tôi cho thấy rằng việc đặt giá trị này một cách thích hợp là quan trọng để tránh nút thắt thấp-hạng (xem Định lý 1), và để cải thiện khả năng biểu diễn (xem Định lý 2). Kích thước đầu cố định này cũng độc lập với cả số lượng đầu và kích thước nhúng của mô hình. Điều này cho phép chúng ta huấn luyện các mô hình với kích thước nhúng tương đối nhỏ hơn (do đó ít tham số hơn) mà không ảnh hưởng đến kích thước đầu. Một lợi ích khác của kích thước đầu cố định là không giống như cài đặt tiêu chuẩn yêu cầu số lượng đầu phải là một yếu tố của kích thước nhúng, chúng ta tự do đặt một số lượng đầu tùy ý như yêu cầu cho tác vụ.

Thú vị thay, chúng tôi lưu ý rằng cách tiếp cận đơn giản nhưng mới lạ này của việc cố định kích thước đầu trong Transformers đa-đầu dẫn đến hiệu suất vượt trội về mặt thực nghiệm. Chúng tôi đánh giá Transformers được huấn luyện với kích thước đầu cố định này trên mô hình hóa ngôn ngữ (tập dữ liệu LM1B), suy luận ngôn ngữ tự nhiên (tập dữ liệu MNLI) và các tác vụ trả lời câu hỏi (tập dữ liệu SQuAD). Chúng tôi cho thấy rằng việc cố định kích thước đầu cho phép chúng ta huấn luyện Transformers với độ mở rộng hiệu suất tốt hơn và kích thước nhúng nhỏ hơn. Chúng tôi cho thấy rằng với kích thước đầu cố định, Transformers được huấn luyện với kích thước nhúng 512 có thể phù hợp với hiệu suất của BERT LARGE [Devlin et al., 2018], một Transformer với kích thước nhúng 1024 (xem Hình 2). Chúng tôi tiếp tục trình bày kết quả thí nghiệm đánh giá tác động của các lựa chọn khác nhau về kích thước đầu và kích thước nhúng trong Phần 4.

Đóng góp của chúng tôi trong bài báo này nằm ở việc xác định và chứng minh một cách nghiêm ngặt nút thắt hạng thấp trong các mô hình attention đa-đầu, và cho thấy rằng việc cố định kích thước đầu bằng độ dài chuỗi đầu vào dẫn đến một mô hình tốt hơn một cách nghiêm ngặt, cả về mặt lý thuyết và thực nghiệm. Các đóng góp của bài báo này được tóm tắt dưới đây.

•Chúng tôi phân tích khả năng biểu diễn của lớp self attention đa-đầu và chứng minh nút thắt thấp-hạng mà kích thước đầu đặt ra trên các đơn vị attention (Định lý 1).

•Chúng tôi đề xuất đặt kích thước đầu bằng độ dài chuỗi đầu vào, và cho thấy rằng việc cố định kích thước đầu cải thiện một cách nghiêm ngặt khả năng biểu diễn của các lớp attention đa-đầu so với quy tắc tiêu chuẩn để đặt kích thước đầu (Định lý 2). Điều này cho phép chúng ta vừa tăng số lượng đầu mỗi lớp vừa giảm kích thước nhúng, mà không làm tổn hại hiệu suất. Chúng tôi phát triển một cách tiếp cận xây dựng mới lạ dựa trên construction để chứng minh kết quả này, có thể hữu ích trong việc phân tích các biến thể khác của kiến trúc Transformer.

•Chúng tôi thực nghiệm cho thấy rằng với kích thước đầu cố định, Transformers có thể được huấn luyện với độ mở rộng hiệu suất tốt hơn và

--- TRANG 3 ---
kích thước nhúng nhỏ hơn trên ba tác vụ NLP tiêu chuẩn.

1.1 Các Công trình Liên quan
Với tầm quan trọng của các mô hình self attention, đã có công việc cố gắng cả cải thiện hiệu suất và tăng tốc tính toán trong Transformers. Ott et al. [2018] và You et al. [2019] giảm độ chính xác và sử dụng huấn luyện batch lớn để giảm thời gian huấn luyện của các mô hình attention. Child et al. [2019] đề xuất các mô hình self attention thưa thớt để tăng tốc tính toán trong lớp attention cho các tác vụ tạo dữ liệu chuỗi dài. Họ cho thấy rằng những mô hình attention thưa thớt này có thể được huấn luyện trên các tác vụ với độ dài chuỗi lớn hơn 10k mà không hy sinh độ chính xác. Dehghani et al. [2018] đề xuất một mạng Transformer tái phát theo chiều sâu tái sử dụng các tham số qua các lớp. Họ cho thấy rằng sự thay đổi này làm cho các mạng Transformer trở thành Turing complete ngay cả với trọng số có độ chính xác hữu hạn. Yang et al. [2019] đề xuất một cách mới để tăng độ dài chuỗi hiệu quả mà Transformer chú ý đến, bằng cách tái sử dụng các nhúng trung gian qua các chuỗi. Họ cho thấy rằng kiến trúc đã sửa đổi hoạt động tốt hơn trên các tác vụ yêu cầu tính toán ngữ cảnh trên độ dài chuỗi dài hơn. Chúng tôi lưu ý rằng hầu hết những sửa đổi này dựa vào self attention đa-đầu, cùng một khối xây dựng của Transformers. Công việc của chúng tôi đang nghiên cứu lớp attention đa-đầu cơ bản này, và đề xuất một cách mới để đặt kích thước đầu, có thể dễ dàng áp dụng cùng với bất kỳ sửa đổi kiến trúc nào ở trên.

Wu et al. [2019] đề xuất thay thế lớp self-attention bằng các convolution động nhẹ và cho thấy hiệu suất được cải thiện trên dịch máy và mô hình hóa ngôn ngữ. Mặc dù mô hình kết quả có thời gian suy luận nhanh hơn, nó vẫn cần sử dụng kích thước nhúng lớn (1024), lớn như các mô hình attention gốc. Chúng tôi tin rằng các kỹ thuật trong bài báo này có thể được kết hợp với những kết quả này để nhận ra cả kích thước nhúng nhỏ hơn và thời gian suy luận nhanh hơn.

Sun et al. [2019] thực hiện tìm kiếm kiến trúc neural bằng phương pháp tiến hóa trên các mô hình sequence to sequence và tìm thấy một kiến trúc transformer tiến hóa, ngoài các đơn vị attention đa-đầu, còn có bộ lọc convolution và các đơn vị tuyến tính có cổng. Các sửa đổi được đề xuất của chúng tôi gần gũi hơn với Transformers về tinh thần và có thể được sử dụng làm đơn vị hạt giống cho tìm kiếm kiến trúc này.

Yang et al. [2017] đã nghiên cứu tác động của ràng buộc hạng gây ra bởi các kích thước chiếu nhỏ trong việc tính toán mất mát softmax. Tình huống trong các lớp self attention hơi khác. Trong khi khả năng biểu diễn của mỗi đầu giảm với việc giảm kích thước đầu, cùng lúc chúng ta đang tăng số lượng đầu, có thể triệt tiêu điều này và tăng khả năng tổng thể của lớp. Như chúng tôi cho thấy trong Định lý 2, quy tắc kích thước đầu phổ biến thực sự giới hạn khả năng biểu diễn của lớp attention đa-đầu.

Yun et al. [2019] nghiên cứu khả năng biểu diễn của Transformers và cho thấy chúng là các bộ xấp xỉ phổ quát của các hàm sequence to sequence. Tuy nhiên họ không nghiên cứu nút thắt hạng thấp gây ra bởi quy tắc kích thước đầu phổ biến và mối liên hệ của nó với kích thước nhúng.

Voita et al. [2019], Michel et al. [2019] nghiên cứu tầm quan trọng của các đầu khác nhau trong một lớp attention. Họ quan sát rằng, trong quá trình suy luận, nhiều đầu trong mỗi lớp có thể được cắt tỉa với ít ảnh hưởng đến dự đoán. Tuy nhiên, họ vẫn cần nhiều đầu trong quá trình huấn luyện.

Child et al. [2019], Correia et al. [2019] áp đặt cấu trúc thưa thớt trên lớp attention trong quá trình huấn luyện để cải thiện cả khả năng diễn giải và hiệu suất. Việc cố định kích thước đầu thực tế sẽ làm cho việc học các mẫu thưa thớt như vậy dễ dàng hơn, vì ràng buộc hạng thấp không cho phép một đầu biểu diễn tất cả các mẫu thưa thớt có thể. Do đó việc kết hợp những kỹ thuật này có thể cho phép huấn luyện các mô hình attention thưa thớt với kích thước nhúng nhỏ hơn.

2 Kiến trúc Transformer và Phân tích
Trong phần này, chúng tôi trình bày kiến trúc Transformer và phân tích khả năng biểu diễn của self attention đa-đầu, một thành phần chính của khối Transformer.

Đầu vào của một mạng Transformer là một chuỗi n token. Thông thường, mỗi token được chuyển đổi thành một nhúng token có chiều d bởi một lớp nhúng. Chúng ta để X ∈ R^{d×n} là ma trận nhúng tương ứng với n token trong chuỗi đầu vào.

--- TRANG 4 ---
2.1 Attention Đơn-đầu
Khối Transformer là sự kết hợp của một lớp self attention theo sau bởi một lớp feed forward [Vaswani et al., 2017]. Cả hai lớp đều có một kết nối bỏ qua và sử dụng Layer Normalization (LN) [Ba et al., 2016]. Cụ thể, đối với các nhúng token X, dot product attention được tính như sau.

Attention(X) = W_v X Softmax((W_k X)^T (W_q X) / √d_k)
= W_v X P                                                    (1)

Ở đây W_q ∈ R^{d_q×d}, W_k ∈ R^{d_k×d} và W_v ∈ R^{d_v×d} đại diện cho các ma trận chiếu liên quan đến query, key và value tương ứng trong một đơn vị attention [Vaswani et al., 2017]. Đối với một đơn vị attention đơn-đầu, chúng ta có d_q = d_k = d_v = d. Trong dot-product attention (cf. (1)), P nhằm nắm bắt ngữ cảnh của đầu vào cho một token đã cho dựa trên các token còn lại trong chuỗi đầu vào. Sau đó, đầu ra của lớp attention có dạng sau.

LN(X + W_o Attention(X));                                    (2)

trong đó LN(·) đại diện cho thao tác layer-normalization. Cho mô-đun attention, như được định nghĩa trong (1), việc đặt câu hỏi về khả năng biểu diễn các ngữ cảnh P tùy ý cho một chuỗi đầu vào X cho trước là tự nhiên.

Trong kết quả sau đây, chúng tôi thiết lập rằng đối với một kích thước chiếu đủ lớn, một đơn vị attention có thể biểu diễn bất kỳ cặp dữ liệu (X, P) nào. Chúng tôi cũng cho thấy rằng mô hình không thể biểu diễn ngữ cảnh tùy ý khi d nhỏ hơn n, tạo ra một nút thắt thấp-hạng.

Định lý 1 (Định lý Biểu diễn). Nếu d_q = d_k = d ≥ n, thì cho bất kỳ ma trận X ∈ R^{d×n} có hạng cột đầy đủ và một ma trận stochastic cột dương n×n tùy ý P, luôn tồn tại các ma trận chiếu d×d W_q và W_k sao cho

Softmax((W_k X)^T (W_q X) / √d_k) = P.                       (3)

Nếu d_q = d_k = d < n, tồn tại X và P sao cho (3) không đúng cho tất cả W_q và W_k.

Kết quả này cho thấy rằng chiều chiếu d_q = d_k = d cần phải lớn hơn độ dài chuỗi n để đơn vị attention có thể biểu diễn bất kỳ ngữ cảnh P mong muốn nào. Mặc dù kết quả này mô tả trường hợp chuỗi ví dụ đơn lẻ, nó nhấn mạnh một tính chất cơ bản của kiến trúc mô hình rằng việc giảm kích thước chiếu dưới một ngưỡng nhất định tạo ra một nút thắt.

Chứng minh Định lý 1. Trường hợp d ≥ n. Để chứng minh phần đầu của kết quả, chúng tôi trình bày một cấu trúc tường minh của W_k và W_q cho phép chúng ta tạo ra P từ X bằng dot product attention. Vì X có hạng cột đầy đủ, tồn tại một nghịch đảo trái X^y = (X^T X)^{-1} X^T ∈ R^{n×d} sao cho X^y X = I_n. Đặt W_k = Ŵ_k X^y và W_q = Ŵ_q X^y. Khi đó

X^T W_k^T W_q X = X^T (X^y)^T Ŵ_k^T Ŵ_q X^y X = I_n Ŵ_k^T Ŵ_q I_n
= Ŵ_k^T Ŵ_q = Ŵ_{kq}.                                      (4)

Bây giờ việc lựa chọn W_q và W_k ở trên đã xử lý sự phụ thuộc vào X, chúng ta sẽ chọn một Ŵ_{kq} phụ thuộc vào P và hoàn thành cấu trúc. Dưới đây chúng ta biểu diễn thao tác Softmax trên các tích trong query và key. Lưu ý rằng Softmax ở đây là toán tử theo cột tính toán điểm attention cho mỗi query. Bằng cách sử dụng (4), chúng ta thu được

Softmax((W_k X)^T (W_q X) / √d_k) = Softmax[Ŵ_{kq} / √d_k]
= exp(Ŵ_{kq} / √d_k) D_{Ŵ_{kq}}^{-1},

trong đó D_{Ŵ_{kq}} là một ma trận chéo n×n sao cho

(D_{Ŵ_{kq}})_{ii} = Σ_{j=1}^n exp((Ŵ_{kq})_{ji} / √d_k)
= (1^T exp(Ŵ_{kq} / √d_k))_i.

--- TRANG 5 ---
Do đó, chúng ta có thể thiết lập kết quả mong muốn bằng cách cho thấy rằng luôn tồn tại một Ŵ_{kq} thỏa mãn phương trình điểm cố định sau.

exp(Ŵ_{kq} / √d_k) = P D_{Ŵ_{kq}}.                         (5)

Cho P, để xây dựng Ŵ_{kq} như vậy, chúng ta chọn một ma trận chéo dương tùy ý D_0, và đặt

Ŵ_{kq} = √d_k log(P D_0).                                   (6)

Vì P là một ma trận dương, Ŵ_{kq} như vậy luôn tồn tại. Tiếp theo, chúng ta xác minh rằng cấu trúc này thực sự thỏa mãn phương trình điểm cố định (cf. (5)). Lưu ý rằng

D_{Ŵ_{kq}} = Diag(1^T exp(Ŵ_{kq} / √d_k))
= Diag(1^T P D_0)
= D_0.                                                      (7)

Phương trình cuối cùng tuân theo thực tế rằng P là một ma trận stochastic cột. Bây giờ, sử dụng (6) và (7),

exp(Ŵ_{kq} / √d_k) = P D_0 = P D_{Ŵ_{kq}}.

Điều này hoàn thành phần đầu của chứng minh.

Trường hợp d < n. Xem xét trường hợp d = 1 và n = 2. Khi đó X ∈ R^{1×2} và W_q và W_k ∈ R^{1×1}. Đặt X = [1, 0]. Khi đó

Softmax((W_k X)^T (W_q X) / √d_k) = Softmax([1, 0]^T W_k^T W_q [1, 0] / √d_k)
= Softmax[W_k W_q  0 ]
                [0      0 ]

Ma trận này rõ ràng không thể được sử dụng để tạo ra P có các phần tử khác biệt trong cột thứ hai, ví dụ, P = [0.5  0.75]
                                                                                                                      [0.5  0.25]

2.2 Attention Đa-đầu
Như đã thảo luận trong Phần 2.1, một đơn vị attention cập nhật nhúng của một token đầu vào dựa trên trung bình có trọng số của các nhúng của tất cả các token trong chuỗi, sử dụng ngữ cảnh P (cf.(1)). Vaswani et al. [2017] đã đề xuất cơ chế attention Đa-đầu tăng khả năng biểu diễn của một lớp attention, trong đó nhiều đơn vị attention hoạt động trên các chiếu chiều thấp khác nhau của đầu vào, với mỗi đơn vị attention được gọi là một đầu. Điều này được theo sau bởi việc nối các đầu ra từ các đầu khác nhau. Cụ thể, việc tính toán bên trong một attention Đa-đầu với h đầu có dạng sau:

head(X)_i = W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h)) ∈ R^{d/h×n}

MultiHead(X) = Concat[head(X)_1; ...; head(X)_h] ∈ R^{d×n}.

Đầu ra của lớp attention Đa-đầu sau đó trở thành

Z = LN(X + W_o MultiHead(X));                               (8)

trong đó W_o ∈ R^{d×d}. Đối với một mô hình với h đầu, các ma trận chiếu query, key và value {W_q^i}, {W_k^i} và {W_v^i} là các ma trận d/h×d. Do đó, mỗi đầu chiếu đầu vào lên một không gian con d/h-chiều để tính toán ngữ cảnh, và giữ số lượng tham số cố định mỗi lớp. Việc sử dụng MultiHead đã dẫn đến hiệu suất tốt hơn về mặt thực nghiệm so với lớp attention đơn đầu [Vaswani et al., 2017].

--- TRANG 6 ---
(a) LM1B
(b) LM1B
Hình 1: Hiệu suất của Transformers được huấn luyện với quy tắc kích thước đầu phổ biến (d_p = d/h) (baseline) so với kích thước đầu cố định (d_p = 32) trên tác vụ mô hình hóa ngôn ngữ (LM1B) trên tập test. Chúng tôi huấn luyện các mô hình baseline với kích thước nhúng từ 256 đến 512. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 256 và kích thước đầu là 32, và thay đổi số lượng đầu từ 4 đến 70, trong khi phù hợp với số lượng tham số. Các biểu đồ rõ ràng chỉ ra rằng việc cố định kích thước đầu cho phép chúng ta huấn luyện Transformers với kích thước nhúng nhỏ hơn (biểu đồ (b)), và với độ mở rộng hiệu suất tốt hơn (biểu đồ (a)). Lưu ý rằng đối với perplexity, giá trị thấp hơn là tốt hơn.

2.3 Nút thắt Thấp-hạng
Trong khi việc tăng số lượng đầu dường như mang lại cho mô hình khả năng biểu diễn nhiều hơn, cùng lúc chúng ta đang giảm kích thước đầu, có thể làm giảm khả năng biểu diễn. Khi số lượng đầu h lớn hơn d/n, đơn vị attention bên trong mỗi đầu chiếu lên một chiều nhỏ hơn n, tạo ra một nút thắt thấp-hạng và mất khả năng biểu diễn các vectơ ngữ cảnh tùy ý (cf. Định lý 1). Thú vị thay, điều này phù hợp với quan sát thực nghiệm trong Bảng 1 rằng việc tăng h vượt quá 8 dẫn đến suy giảm hiệu suất trong BERT LARGE [Devlin et al., 2018]; lưu ý rằng d = 1024 và n = 128 cho hầu hết giai đoạn tiền huấn luyện của BERT LARGE.

Vì độ dài chuỗi được cố định từ dữ liệu/tác vụ có sẵn, cách duy nhất để tăng số lượng đầu mà không tạo ra nút thắt thấp-hạng là bằng cách tăng kích thước nhúng d. Đây là một hạn chế cơ bản của quy tắc kích thước đầu hiện đang thống trị, rằng chúng ta cần tăng kích thước nhúng để hỗ trợ nhiều đầu hơn.

Thật không may, việc tăng kích thước nhúng dẫn đến yêu cầu tính toán và bộ nhớ cao hơn để huấn luyện và lưu trữ mô hình. Hơn nữa, vì việc sử dụng các nhúng đã học từ các mô hình dựa trên Transformer cho các tác vụ downstream là phổ biến [Devlin et al., 2018], kích thước nhúng lớn hơn tăng kích thước mô hình và tính toán cần thiết cho tất cả các tác vụ downstream.

3 Attention Đa-đầu Cố định
Trong phần này, chúng tôi đề xuất cố định kích thước đầu của Transformer, cho phép chúng ta tận hưởng lợi thế của khả năng biểu diễn cao hơn của nhiều đầu mà không yêu cầu kích thước nhúng phải lớn. Chìa khóa là tách rời sự phụ thuộc giữa kích thước chiếu trong một đầu và kích thước nhúng của mô hình. Các ma trận chiếu bây giờ chiếu lên các không gian con có chiều cố định d_p bất kể số lượng đầu h. Cách tiếp cận này trong đó d_p độc lập với d và h dẫn đến cơ chế attention sau.

fixedhead(X)_i = V_v^i X Softmax((V_k^i X)^T (V_q^i X) / √d_p) ∈ R^{d_p×n}

FixedMultiHead(X) = Concat[fixedhead(X)_1; ...; fixedhead(X)_h] ∈ R^{d_p h×n}.

--- TRANG 7 ---
Lưu ý rằng các ma trận chiếu được sử dụng ở đây {V_q^i}, {V_k^i} và {V_v^i} là các ma trận d_p×d. Với V_o ∈ R^{d×hd_p}, đầu ra của lớp attention đa-đầu mới này có dạng sau.

Z = LN(X + V_o FixedMultiHead(X)).

Sửa đổi này làm cho mỗi đầu attention tương tự hơn với một đơn vị ẩn trong mạng feed forward hoặc một bộ lọc trong mạng convolutional, và cho phép chúng ta thay đổi số lượng đầu mà không lo lắng về việc giảm khả năng biểu diễn mỗi đầu. Nhược điểm là, không giống như MultiHead tiêu chuẩn, số lượng tham số mỗi lớp tăng với số lượng đầu. Tuy nhiên, sửa đổi này cho phép chúng ta huấn luyện một mô hình với kích thước nhúng nhỏ hơn mà không có nút thắt thấp-hạng, cuối cùng cho phép chúng ta giảm tổng số lượng tham số trong mô hình.

3.1 MultiHead vs. FixedMultiHead Attention
Cho một lớp MultiHead, chúng ta luôn có thể biểu diễn nó bằng một lớp FixedMultiHead, bất cứ khi nào chúng ta có kích thước đầu d_p ≥ d/h. Trong khi điều này cho thấy rằng việc tăng số lượng đầu h vượt quá d/d_p làm cho các đầu riêng lẻ của FixedMultiHead có khả năng biểu diễn như những đầu trong MultiHead, không rõ ràng liệu FixedMultiHead có nghiêm ngặt biểu diễn hơn hay không. Liệu lớp FixedMultiHead có thể biểu diễn các hàm mà lớp MultiHead tiêu chuẩn không thể biểu diễn?

Trong phần này, chúng tôi cho thấy rằng thực sự, trong chế độ đa-đầu, lớp FixedMultiHead nghiêm ngặt tốt hơn lớp MultiHead tiêu chuẩn về mặt khả năng biểu diễn.

Xem xét các đơn vị attention đa-đầu tiêu chuẩn trong (8).

f_W(X) = W_o MultiHead(X).

Chúng ta ký hiệu tập hợp tất cả các ma trận tham số là W. Tương tự, xem xét hàm được biểu diễn bởi các đơn vị attention kích thước đầu cố định:

g_V(X) = V_o FixedMultiHead(X).

Đặt V là tập hợp tất cả các ma trận tham số này. Chúng ta định nghĩa F và G là lớp các hàm f_W(·) và g_V(·), tương ứng. Như đã lưu ý ở trên, nếu d_p ≥ d/h, chúng ta có F ⊆ G.

Định lý sau đây cho thấy rằng ngay cả đối với các ví dụ đơn giản trong G, các hàm trong F không thể biểu diễn chúng; điều này đã cho thấy rằng F là một tập con nghiêm ngặt của G.

Định lý 2. Đặt n ≥ 2, d ≥ d_p, và h > d/d_p. Xem xét một lớp attention FixedMultiHead g_V(·) với các tham số thỏa mãn các điều kiện sau:

V_o = [V_v^1; ...; V_v^h] có hạng đầy đủ, và (V_k^i)^T V_q^i = U, cho tất cả i = 1, ..., h, trong đó U là một ma trận hạng-d_p.

Khi đó, với bất kỳ f_W ∈ F, tồn tại X ∈ R^{d×n} sao cho f_W(X) ≠ g_V(X).

Vì ||f_W(X) - g_V(X)|| là một hàm liên tục của X, sự tồn tại của X như vậy ngụ ý rằng tích phân của chuẩn của hiệu (tức là, lỗi xấp xỉ) là nghiêm ngặt dương. Chúng tôi lưu ý rằng các giả định về V_k^i và V_q^i trong Định lý trên được thực hiện để cung cấp một chứng minh đơn giản và xây dựng; thực tế, việc MultiHead (F) không thể biểu diễn những lớp attention đơn giản như vậy cho thấy rằng tình huống có thể tồi tệ hơn đối với các hàm phức tạp hơn.

Định lý 2 cho thấy rằng khả năng biểu diễn của lớp hàm attention FixedMultiHead nghiêm ngặt vượt trội so với lớp hàm attention MultiHead tiêu chuẩn. Do đó quy tắc giảm kích thước đầu với số lượng đầu đang giới hạn khả năng biểu diễn của MultiHead, trong khi việc sử dụng kích thước đầu cố định sẽ tăng khả năng biểu diễn của các lớp attention.

4 Thí nghiệm
Mục tiêu của phần này là cho thấy rằng việc đặt kích thước đầu theo cách có nguyên tắc dẫn đến hiệu suất tốt hơn so với việc sử dụng quy tắc phổ biến. Chúng tôi lại lưu ý rằng trong khi đây là một thay đổi siêu tham số đơn giản đối với Transformer, việc đặt điều này bằng độ dài chuỗi đầu vào như được thể hiện trong phân tích của chúng tôi, cho phép chúng ta huấn luyện các mô hình tốt hơn với kích thước nhúng nhỏ hơn.

--- TRANG 8 ---
(a) SQuAD F1
(b) SQuAD EM
(c) MNLI
Hình 2: So sánh các mô hình Transformer 24 lớp được huấn luyện với quy tắc kích thước đầu phổ biến BERT LARGE (baseline) so với mô hình kích thước đầu cố định trên các tập dev SQuAD và MNLI. Chúng tôi thay đổi kích thước nhúng của các mô hình baseline từ 512 đến 1024. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 512 và kích thước đầu là 128, với số lượng đầu thay đổi từ 8 đến 32, trong khi phù hợp với số lượng tham số. Việc cố định kích thước đầu cho phép chúng ta huấn luyện các mô hình với kích thước nhúng nhỏ hơn là 512 và với hiệu suất tốt hơn.

(a)
(b)
Hình 3: Các nghiên cứu ablation trên LM1B: (a) Chúng tôi cố định kích thước nhúng của tất cả các mô hình là 256 và thay đổi khả năng của Transformers được huấn luyện với quy tắc kích thước đầu phổ biến (baseline) bằng cách tăng kích thước của các lớp feedforward. Đối với các mô hình kích thước đầu cố định, chúng tôi cố định kích thước đầu là 32, vì vậy mô hình kích thước đầu cố định 8 đầu giống với mô hình baseline 8 đầu. Chúng tôi nhận thấy rằng một lần nữa với quy tắc tiêu chuẩn, việc tăng số lượng đầu vượt quá 16 làm tổn hại hiệu suất, trong khi với kích thước đầu cố định, việc tăng số lượng đầu cải thiện hiệu suất một cách đơn điệu. (b) Chúng tôi cho thấy tác động của kích thước đầu lên hiệu suất với số lượng đầu khác nhau. Cả hai biểu đồ đều rõ ràng cho thấy lợi thế trong việc có một cách bổ sung để điều chỉnh khả năng của Transformers với kích thước nhúng cố định.

Trong phần này, chúng tôi trình bày các thí nghiệm của mình trên ba tác vụ NLP tiêu chuẩn, mô hình hóa ngôn ngữ (LM1B), trả lời câu hỏi (SQuAD), và suy luận câu (MNLI), để chứng minh: 1) Việc tăng số lượng đầu trong Transformers vượt quá một điểm nhất định làm tổn hại hiệu suất với quy tắc kích thước đầu phổ biến, nhưng luôn giúp ích với các lớp attention kích thước đầu cố định; 2) Việc tách rời kích thước đầu khỏi kích thước nhúng cho phép chúng ta huấn luyện các mô hình với kích thước nhúng nhỏ hơn; và 3) Việc đặt kích thước đầu một cách thích hợp trong Transformers cho phép chúng ta huấn luyện các mô hình với độ mở rộng hiệu suất tốt hơn. Chúng tôi đầu tiên mô tả thiết lập thí nghiệm của mình theo sau bởi kết quả và các nghiên cứu ablation về các sửa đổi được đề xuất.

4.1 Thiết lập và Tập dữ liệu
Đối với tác vụ mô hình hóa ngôn ngữ, chúng tôi sử dụng tập dữ liệu benchmark một tỷ từ (LM1B) [Chelba et al., 2013]. Tập dữ liệu này có khoảng 30M ví dụ huấn luyện và khoảng 300k ví dụ trong tập test. Chúng tôi sử dụng một tokenizer sub-word với

--- TRANG 9 ---
# đầu 8 12 16 32
# tham số 168M 193M 218M 319M
SQuAD - F1 89.6±0.17 90.25±0.21 90.43±0.14 90.95±0.14
SQuAD - EM 82.73±0.21 83.18±0.24 83.59±0.06 84.4±0.29
MNLI 83.5±0.2 84.2±0.2 83.9±0.2 84.9±0.2
(A) Tăng số lượng đầu

kích thước đầu 32 64 128 256
# tham số 130M 142M 168M 218M
SQuAD - F1 88.53±0.06 89.51±0.15 89.6±0.17 90.33±0.23
SQuAD - EM 81.19±0.21 82.41±0.32 82.73±0.21 83.36±0.48
MNLI 82.5±0.1 83.4±0.3 83.5±0.2 83.9±0.2
(B) Tăng kích thước đầu

Bảng 2: Các nghiên cứu ablation trên SQuAD và MNLI: (A) Transformer 24 lớp với kích thước đầu cố định là 128 và kích thước nhúng 512 cho thấy sự cải thiện độ chính xác với việc tăng số lượng đầu. (B) Mô hình kích thước đầu cố định với kích thước nhúng 512 và 8 đầu cho thấy sự cải thiện độ chính xác với việc tăng kích thước đầu. Điều này cho thấy rằng thực sự kích thước đầu là một tham số kiểm soát khả năng quan trọng trong kiến trúc self attention.

32k từ vựng và giới hạn đầu vào ở độ dài chuỗi 256. Chúng tôi huấn luyện một mô hình Transformer 6 lớp với bộ tối ưu hóa ADAM bằng thư viện tensor2tensor [Vaswani et al., 2018]. Thiết lập thí nghiệm chi tiết được trình bày trong Phần C.

Multi-Genre Natural Language Inference (MNLI) là một tác vụ suy luận ở cấp độ câu, được thiết kế để kiểm tra khả năng hiểu ngôn ngữ tự nhiên [Williams et al., 2018]. Cho một câu tiền đề và một câu giả thuyết, mục tiêu là dự đoán liệu giả thuyết có suy ra, mâu thuẫn hay trung lập với tiền đề. Chúng tôi báo cáo độ chính xác phân loại cho tác vụ này. Stanford Question Answering Dataset (SQuAD) là một tập dữ liệu trả lời câu hỏi, trong đó cho một đoạn văn và một câu hỏi, mục tiêu là dự đoán chuỗi từ trong đoạn văn tạo thành câu trả lời cho câu hỏi [Rajpurkar et al., 2016]. Đây là một tác vụ cấp độ từ khó hơn, so với tác vụ phân loại câu. Chúng tôi báo cáo cả điểm Exact Match (EM) và F1 cho tác vụ này. Tất cả kết quả trong phần này được báo cáo trên tập Dev, chưa được sử dụng trong bất kỳ lựa chọn thí nghiệm nào trong bài báo này.

Đối với hai tác vụ sau này, chúng tôi theo cách tiếp cận hai giai đoạn của việc đầu tiên tiền huấn luyện trên một tác vụ mô hình hóa ngôn ngữ và sau đó tinh chỉnh các mô hình trên dữ liệu tác vụ. Chúng tôi theo cùng thiết lập thí nghiệm cho cả tiền huấn luyện và tinh chỉnh như BERT [Devlin et al., 2018], và sử dụng codebase của họ¹. Chúng tôi đầu tiên tiền huấn luyện các mô hình của mình bằng mô hình ngôn ngữ có mặt nạ và các mục tiêu dự đoán câu tiếp theo, và sau đó tinh chỉnh mô hình đã tiền huấn luyện cho các tác vụ riêng lẻ [Devlin et al., 2018]. Đối với tiền huấn luyện, chúng tôi sử dụng tập dữ liệu English Wikipedia và BooksCorpus [Zhu et al., 2015]. Đầu vào của các mô hình được tokenize bằng biểu diễn WordPiece với 30000 token trong từ vựng. Chúng tôi trình bày các lựa chọn thí nghiệm chính trong Phần C, và tham khảo độc giả đến Devlin et al. [2018] để có mô tả đầy đủ về thiết lập.

Lựa chọn kích thước đầu. Sửa đổi được đề xuất của chúng tôi giới thiệu kích thước đầu d_p như một siêu tham số mô hình mới. Chúng tôi chọn kích thước đầu là 128 cho các thí nghiệm BERT của mình, vì hầu hết việc tiền huấn luyện được thực hiện với dữ liệu độ dài chuỗi 128. Trong khi chúng tôi có các nghiên cứu ablation (cf. Bảng 2(B)) cho thấy kích thước đầu lớn hơn cải thiện hiệu suất, có sự đánh đổi giữa việc tăng kích thước đầu vs số lượng đầu vs các lớp. Chúng tôi nhận thấy rằng có kích thước đầu đủ lớn, ví dụ, phù hợp với độ dài chuỗi tiền huấn luyện, tốt hơn là có kích thước nhúng lớn hơn.

4.2 Kết quả
Đối với tập thí nghiệm đầu tiên của chúng tôi, chúng tôi muốn xem liệu Transformers được huấn luyện với kích thước đầu cố định và kích thước nhúng nhỏ hơn có thể phù hợp với hiệu suất của việc huấn luyện với quy tắc kích thước đầu tiêu chuẩn nhưng với kích thước nhúng lớn hơn. Như một baseline cho tác vụ mô hình hóa ngôn ngữ, chúng tôi huấn luyện Transformers với kích thước nhúng tăng từ 256 đến 512

¹https://github.com/google-research/bert

--- TRANG 10 ---
với số lượng đầu khác nhau. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 256 và kích thước đầu là 32, với số lượng đầu tăng từ 4 đến 70. Chúng tôi nhận thấy rằng Transformers với kích thước đầu cố định và kích thước nhúng 256 có hiệu suất tốt hơn các mô hình baseline với kích thước nhúng 512 (xem Hình 1). Chúng tôi lặp lại thí nghiệm tương tự trên hai tác vụ khác, trong đó đối với baseline chúng tôi huấn luyện BERT LARGE, một Transformer 24 lớp, 16 đầu với quy tắc kích thước đầu tiêu chuẩn, với kích thước nhúng từ 512 đến 1024. Chúng tôi so sánh nó với mô hình kích thước đầu cố định, với kích thước nhúng 512 và kích thước đầu 128, với số lượng đầu tăng từ 8 đến 32. Chúng tôi lại nhận thấy rằng Transformers được huấn luyện với kích thước đầu cố định và kích thước nhúng 512 có hiệu suất tốt hơn baseline, BERT LARGE (xem Hình 2).

Lưu ý rằng việc đơn giản cố gắng tăng kích thước đầu của Transformers bằng cách giảm số lượng đầu không cải thiện hiệu suất, vì việc giảm số lượng đầu làm giảm khả năng biểu diễn của mô hình (xem Hình 4 trong Phụ lục). Do đó, cả kích thước đầu và số lượng đầu cần được đặt đủ cao để có hiệu suất tốt hơn.

4.3 Ablation
Tăng đầu. Từ Bảng 1 và Hình 1a chúng ta có thể thấy rằng việc tăng số lượng đầu làm tổn hại hiệu suất của Transformer sau một số lượng nhất định. Chúng tôi lặp lại cùng thí nghiệm với Transformer kích thước đầu cố định, và trình bày kết quả trong Bảng 2(A) và Hình 3a. Kết quả cho thấy rằng hiệu suất của mô hình đã sửa đổi cải thiện một cách đơn điệu khi số lượng đầu tăng. Điều này là do khả năng mô hình (một hàm của kích thước đầu) không còn bị giảm với việc tăng số lượng đầu.

Tăng kích thước đầu. Trong Bảng 2(B) và Hình 3b, chúng tôi trình bày so sánh giữa các mô hình với kích thước đầu khác nhau. Điều này cho thấy rằng những cải thiện trong hiệu suất của các mô hình kích thước đầu cố định thực sự đến từ việc điều chỉnh kích thước đầu của các lớp query, key và value trong đơn vị attention. Bảng cho thấy một xu hướng rõ ràng của hiệu suất tốt hơn với kích thước đầu lớn hơn, cho thấy rằng nó thực sự là một yếu tố quan trọng trong hiệu suất của các mô hình attention.

5 Kết luận
Trong bài báo này, chúng tôi đã nghiên cứu khả năng biểu diễn của các mô hình self attention đa-đầu và chứng minh nút thắt thấp-hạng dẫn đến từ kích thước đầu nhỏ trong attention đa-đầu. Chúng tôi đã cho thấy rằng kích thước nhúng lớn hơn được sử dụng trong các mô hình hiện tại là hệ quả của nút thắt thấp-hạng này trong các lớp attention đa-đầu. Chúng tôi đề xuất thay vào đó sử dụng các đơn vị attention kích thước đầu cố định, với kích thước đầu được đặt bằng độ dài chuỗi đầu vào, để tránh nút thắt này. Chúng tôi đã cho thấy rằng điều này cho phép chúng ta tăng số lượng đầu mà không tăng kích thước nhúng. Như một hệ quả, chúng ta có thể huấn luyện Transformers với kích thước nhúng nhỏ hơn và ít tham số hơn, với hiệu suất tốt hơn. Trong tương lai, sẽ thú vị khi thí nghiệm với việc thay đổi kích thước đầu trong một khối attention và qua các lớp. Điều này đòi hỏi hiểu biết sâu hơn về vai trò của mỗi lớp trong việc tính toán ngữ cảnh, đây là một hướng thú vị cho công việc tương lai.

Tài liệu tham khảo
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, and P. Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.

R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4774–4778. IEEE, 2018.

--- TRANG 11 ---
K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111, 2014.

G. M. Correia, V. Niculae, and A. F. Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184, 2019.

M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1243–1252. JMLR. org, 2017.

S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

Y. Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.

P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? arXiv preprint arXiv:1905.10650, 2019.

M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 1–9, 2018.

A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. Technical report, OpenAI, 2019.

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

H. Sun, X. Tan, J.-W. Gan, H. Liu, S. Zhao, T. Qin, and T.-Y. Liu. Token-level ensemble distillation for grapheme-to-phoneme conversion. arXiv preprint arXiv:1904.03446, 2019.

I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112, 2014.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.

A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. Sepassi, N. Shazeer, and J. Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.

E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.

X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7794–7803, 2018.

--- TRANG 12 ---
A. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101.

F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.

Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.

Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.

Y. You, J. Li, J. Hseu, X. Song, J. Demmel, and C.-J. Hsieh. Reducing bert pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962, 2019.

C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.

V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.

H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018.

Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.

--- TRANG 13 ---
A Ký hiệu
Kích thước nhúng d
Số lượng lớp l
Số lượng đầu h
Độ dài chuỗi n
Kích thước từ vựng v
Kích thước đầu d_p

B Chứng minh
Chứng minh Định lý 2. Đầu tiên hãy viết lại các lớp MultiHead và FixedMultiHead như sau. Lớp MultiHead có thể được viết lại như

f_W(X) = W_o MultiHead(X) = Σ_{i=1}^h W_o^i W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h)),

trong đó W_o^i là các ma trận d×d/h và W_v^i, W_k^i, và W_q^i là các ma trận d/h×d. Chúng ta ký hiệu tập hợp tất cả các ma trận tham số là W.

Tương tự, viết lại lớp attention kích thước đầu cố định như

g_V(X) = V_o FixedMultiHead(X) = Σ_{i=1}^h V_o^i V_v^i X Softmax((V_k^i X)^T (V_q^i X) / √d_p),

trong đó V_o^i ∈ R^{d×d_p}, và V_v^i, V_k^i, V_q^i ∈ R^{d_p×d}. Đặt V là tập hợp tất cả các ma trận này.

Đại cương của chứng minh cơ bản là phân tích trường hợp: chúng ta chia các giá trị có thể của W thành ba loại, và cho thấy trong mỗi trường hợp tồn tại một X sao cho f_W(X) ≠ g_V(X). Đây là ba trường hợp:

•Trường hợp 1: Σ_{i=1}^h W_o^i W_v^i ≠ Σ_{i=1}^h V_o^i V_v^i.

•Trường hợp 2: Σ_{i=1}^h W_o^i W_v^i = Σ_{i=1}^h V_o^i V_v^i, và tồn tại i ∈ {1, ..., h} sao cho U = √d_p (W_k^i)^T (W_q^i) / √(d/h) không phải là skew-symmetric.

•Trường hợp 3: Σ_{i=1}^h W_o^i W_v^i = Σ_{i=1}^h V_o^i V_v^i, và tất cả U = √d_p (W_k^i)^T (W_q^i) / √(d/h) đều là skew-symmetric.

Trường hợp 1. Trong trường hợp đầu tiên, chúng ta có thể chọn bất kỳ v sao cho (Σ_{i=1}^h W_o^i W_v^i - Σ_{i=1}^h V_o^i V_v^i)v ≠ 0. Chọn X = v1^T = [v v ⋯ v]. Khi đó, lưu ý rằng với bất kỳ ma trận stochastic cột P, chúng ta có XP = X. Do đó,

Σ_{i=1}^h W_o^i W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h)) - Σ_{i=1}^h V_o^i V_v^i X Softmax((V_k^i X)^T (V_q^i X) / √d_p)
= Σ_{i=1}^h W_o^i W_v^i X - Σ_{i=1}^h V_o^i V_v^i X = (Σ_{i=1}^h W_o^i W_v^i - Σ_{i=1}^h V_o^i V_v^i)v1^T ≠ 0.

Trường hợp 2. Trong các trường hợp mà Σ_{i=1}^h W_o^i W_v^i = Σ_{i=1}^h V_o^i V_v^i, vì Σ_{i=1}^h V_o^i V_v^i có hạng đầy đủ theo giả định và mỗi W_o^i W_v^i có hạng tối đa d/h, nó tuân theo rằng tất cả các cột trong W_o^i ∈ R^{d×d/h} phải độc lập tuyến tính. Do đó, với bất kỳ v ≠ 0, {W_o^i W_v^i v, i = 1, ..., h} là một tập hợp các vectơ độc lập tuyến tính, vì mỗi W_o^i W_v^i v là một tổ hợp tuyến tính của d/h vectơ cột của W_o^i độc lập tuyến tính với các vectơ cột khác trong W_o^j, j ≠ i.

--- TRANG 14 ---
Bây giờ xem xét bất kỳ v ∈ R^d, và X = ve_1^T, trong đó e_1 = (1, 0, ..., 0) ∈ R^n. Định nghĩa φ(t) = exp(t)/(exp(t) + n - 1). Khi đó, chúng ta có

g_V(X) = Σ_{i=1}^h V_o^i V_v^i X Softmax([X^T UX / √d_p])

= Σ_{i=1}^h V_o^i V_v^i X Softmax([v^T Uv / √d_p  0 ⋯ 0; 0  0 ⋯ 0; ⋮  ⋮ ⋱ ⋮; 0  0 ⋯ 0])

= (Σ_{i=1}^h V_o^i V_v^i) φ(v^T Uv / √d_p)[v; v/n; ⋮; v/n]

= (Σ_{i=1}^h W_o^i W_v^i) φ(v^T Uv / √d_p)[v; v/n; ⋮; v/n].

Tương tự, chúng ta có thể tính toán

f_W(X) = Σ_{i=1}^h W_o^i W_v^i X Softmax((W_k^i X)^T (W_q^i X) / √(d/h))

= Σ_{i=1}^h W_o^i W_v^i φ(v^T (W_k^i)^T W_q^i v / √(d/h))[v; v/n; ⋮; v/n].

Lưu ý rằng tất cả các cột của f_W(X) và g_V(X), từ cột thứ hai đến cột cuối cùng, đều giống nhau. Bây giờ chúng ta so sánh các cột đầu tiên:

f_W(X)_{:,1} - g_V(X)_{:,1} = Σ_{i=1}^h [φ(v^T (W_k^i)^T W_q^i v / √(d/h)) - φ(v^T Uv / √d_p)] W_o^i W_v^i v.

Nhớ lại rằng với bất kỳ v ≠ 0, W_o^i W_v^i v là độc lập tuyến tính, vì vậy f_W(X)_{:,1} - g_V(X)_{:,1} = 0 khi và chỉ khi tất cả

φ(v^T (W_k^i)^T W_q^i v / √(d/h)) - φ(v^T Uv / √d_p)

đều bằng không. Tuy nhiên, vì tồn tại i ∈ {1, ..., h} sao cho U = √d_p (W_k^i)^T (W_q^i) / √(d/h) không phải là skew-symmetric, chúng ta có thể chọn v để thỏa mãn v^T (W_k^i)^T W_q^i v / √(d/h) ≠ v^T Uv / √d_p, do đó làm cho

φ(v^T (W_k^i)^T W_q^i v / √(d/h)) - φ(v^T Uv / √d_p) ≠ 0,

do đó f_W(X)_{:,1} - g_V(X)_{:,1} ≠ 0.

Trường hợp 3. Bây giờ xem xét bất kỳ X = [v_1  v_2  0 ⋯ 0], trong đó v_1 và v_2 sẽ được chọn sau này. Định nghĩa φ_1(t_1, t_2) = exp(t_1)/(exp(t_1) + exp(t_2) + n - 2), φ_2(t_1, t_2) = exp(t_2)/(exp(t_1) + exp(t_2) + n - 2). Khi đó, chúng ta có

g_V(X) = Σ_{i=1}^h V_o^i V_v^i X Softmax([v_1^T Uv_1 / √d_p  v_1^T Uv_2 / √d_p  0 ⋯ 0; v_2^T Uv_1 / √d_p  v_2^T Uv_2 / √d_p  0 ⋯ 0; 0  0  0 ⋯ 0; ⋮  ⋮  ⋮ ⋱ ⋮; 0  0  0 ⋯ 0]).

Do đó, cột đầu tiên của g_V(X) có thể được viết như

g_V(X)_{:,1} = Σ_{i=1}^h W_o^i W_v^i [φ_1(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)v_1 + φ_2(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)v_2].

--- TRANG 15 ---
Tương tự, cột đầu tiên của f_W(X) là

f_W(X)_{:,1} = Σ_{i=1}^h W_o^i W_v^i [φ_1(v_1^T (W_k^i)^T W_q^i v_1 / √(d/h), v_2^T (W_k^i)^T W_q^i v_1 / √(d/h))v_1 + φ_2(v_1^T (W_k^i)^T W_q^i v_1 / √(d/h), v_2^T (W_k^i)^T W_q^i v_1 / √(d/h))v_2].

Vì U = √d_p (W_1^k)^T (W_1^q) / √(d/h) là skew-symmetric theo giả định, chúng ta có v_1^T [U / √d_p - (W_1^k)^T (W_1^q) / √(d/h)]v_1 = 0 với tất cả v_1. Nhớ lại rằng U có hạng-d_p theo giả định, vì vậy U / √d_p - (W_1^k)^T (W_1^q) / √(d/h) có hạng ít nhất d_p ≥ d/h ≥ 1, vì vậy chúng ta có thể chọn bất kỳ v_1 sao cho

[U / √d_p - (W_1^k)^T (W_1^q) / √(d/h)]v_1 ≠ 0.

Nếu cả [U / √d_p]v_1 và [(W_1^k)^T (W_1^q) / √(d/h)]v_1 đều khác không, chúng ta luôn có thể chọn ṽ_2 sao cho ṽ_2^T [U / √d_p]v_1 > 0 và ṽ_2^T [(W_1^k)^T (W_1^q) / √(d/h)]v_1 < 0. Điều này có nghĩa là nếu chúng ta chọn v_2 = αṽ_2 và tỷ lệ α → ∞,

φ_1(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p) → 0; φ_2(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p) → 1;

φ_1(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) → exp(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h)) / (exp(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h)) + n - 2);

φ_2(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) → 0.

Khi đó, xem xét hiệu f_W(X)_{:,1} - g_V(X)_{:,1}. Nhớ lại rằng với bất kỳ v, W_1^o W_1^v v độc lập với {W_i^o W_i^v v, i ≠ 1}. Điều này có nghĩa là, để cho thấy f_W(X)_{:,1} - g_V(X)_{:,1} ≠ 0, đủ để cho thấy rằng

[φ_1(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) - φ_1(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)]W_1^o W_1^v v_1 + [φ_2(v_1^T (W_1^k)^T W_1^q v_1 / √(d/h), v_2^T (W_1^k)^T W_1^q v_1 / √(d/h)) - φ_2(v_1^T Uv_1 / √d_p, v_2^T Uv_1 / √d_p)]W_1^o W_1^v v_2 ≠ 0.

Nếu chúng ta tỷ lệ v_2 = αṽ_2 với α đủ lớn, số hạng thứ hai sẽ thống trị số hạng thứ nhất và số hạng thứ nhất sẽ không bao giờ có thể triệt tiêu số hạng thứ hai. Do đó, bằng cách chọn α > 0 đủ lớn, chúng ta có thể đảm bảo rằng tổng là khác không.

Ngay cả trong trường hợp một trong [U / √d_p]v_1 và [(W_1^k)^T (W_1^q) / √(d/h)]v_1 bằng không (giả sử [(W_1^k)^T (W_1^q) / √(d/h)]v_1 = 0), chúng ta có thể chọn ṽ_2 = [U / √d_p]v_1 và sử dụng một lập luận tỷ lệ tương tự. Bằng cách chọn α > 0 đủ lớn và v_2 = αṽ_2, người ta có thể cho thấy rằng hiệu f_W(X)_{:,1} - g_V(X)_{:,1} là khác không.

C Thiết lập thí nghiệm
Đối với các thí nghiệm của chúng tôi với mô hình hóa ngôn ngữ (tập dữ liệu LM1B), chúng tôi huấn luyện các mô hình Transformer 6 lớp. Chúng tôi sử dụng kích thước batch 4096 và huấn luyện cho 250k bước. Chúng tôi sử dụng tốc độ học 0.1 với warm up tuyến tính cho 10k bước đầu tiên. Chúng tôi giảm tốc độ học với căn bậc hai của số bước. Chúng tôi huấn luyện các mô hình baseline, với quy tắc kích thước đầu phổ biến, với chiều nhúng thay đổi từ 256 đến 512. Chúng tôi cố định chiều rộng của lớp feed forward trong Transformer là 1024. Ngoài ra, chúng tôi sử dụng weight decay 0.01 và dropout với xác suất 0.1 trên tất cả các lớp.

--- TRANG 16 ---
Đối với các thí nghiệm của chúng tôi với BERT, chúng tôi theo cùng thiết lập thí nghiệm như trong [Devlin et al., 2018]. Chúng tôi trình bày các chi tiết chính ở đây và tham khảo độc giả đến [Devlin et al., 2018]. Chúng tôi huấn luyện với kích thước batch 1024 cho 450k bước với đầu vào có độ dài chuỗi n = 128 theo sau bởi 50k bước với đầu vào có độ dài chuỗi 512. Ngược lại, bài báo BERT sử dụng kích thước batch 512, và thực hiện tiền huấn luyện cho 900K bước với đầu vào độ dài chuỗi 128 và 100k bước với đầu vào độ dài chuỗi 512. Chúng tôi huấn luyện bằng ADAM với tốc độ học 1e-4, và lịch trình warm up và decay tuyến tính như trong BERT. Chúng tôi sử dụng 5k bước warm up cho giai đoạn đầu tiên, và re-warm up 3k bước cho giai đoạn thứ hai [You et al., 2019]. Một lần nữa, chúng tôi sử dụng weight decay 0.01 và dropout với xác suất 0.1 trên tất cả các lớp.

Đối với tác vụ mô hình hóa ngôn ngữ, việc huấn luyện được thực hiện trên 4 chip TPUv2 trong vài giờ. Đối với các mô hình BERT, việc huấn luyện được thực hiện trên 16 chip TPUv3 trong giai đoạn đầu tiên và 64 chip TPUv3 cho giai đoạn thứ hai. Tiền huấn luyện với cấu hình này mất từ 2 đến 3 ngày. Chúng tôi không cố gắng tìm các siêu tham số tối ưu cho kiến trúc kích thước đầu cố định, và sử dụng cùng siêu tham số như được sử dụng để huấn luyện các mô hình BERT.

D Kết quả thí nghiệm bổ sung

Hình 4: Hiệu suất của Transformers được huấn luyện với quy tắc kích thước đầu phổ biến (baseline) so với các mô hình kích thước đầu cố định (d_p) cho tác vụ mô hình hóa ngôn ngữ (LM1B) trên tập test. Không giống như Hình 1, chúng tôi thay đổi cả kích thước nhúng và số lượng đầu của các mô hình baseline để giữ kích thước đầu cố định ở 32. Chúng tôi huấn luyện các mô hình kích thước đầu cố định với kích thước nhúng cố định là 256 và kích thước đầu là 32, và thay đổi số lượng đầu từ 4 đến 70, trong khi phù hợp với số lượng tham số. Biểu đồ một lần nữa rõ ràng chỉ ra lợi thế của các mô hình kích thước đầu cố định. Vấn đề chính với các mô hình baseline là việc cố định kích thước đầu ở 32 buộc số lượng đầu phải nhỏ khi kích thước nhúng nhỏ. Việc giảm số lượng đầu dưới ngưỡng nhất định làm tổn hại hiệu suất của Transformer.

--- TRANG 17 ---
# đầu 8 12 16 20
# tham số 214M 252M 290M 327M
SQuAD - F1 90.35±0.14 90.48±0.09 90.92±0.14 90.89±0.08
SQuAD - EM 83.37±0.12 83.67±0.03 84.16±0.35 84.29±0.16
MNLI 84.4±0.2 84.4±0.2 84.7±0.1 85.1±0.4
(A) Tăng số lượng đầu

Bảng 3: (A): Transformer 24 lớp được huấn luyện với kích thước đầu cố định là 128 và kích thước nhúng 768 cho thấy sự cải thiện độ chính xác với việc tăng số lượng đầu.