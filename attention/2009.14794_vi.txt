# 2009.14794.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2009.14794.pdf
# Kích thước tệp: 5880177 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
TÍNH TOÁN LẠI ATTENTION VỚI PERFORMERS
Krzysztof Choromanski1, Valerii Likhosherstov2, David Dohan1, Xingyou Song1
Andreea Gane1, Tamas Sarlos1, Peter Hawkins1, Jared Davis3, Afroz Mohiuddin1
Lukasz Kaiser1, David Belanger1, Lucy Colwell1;2, Adrian Weller2;4
1Google2University of Cambridge3DeepMind4Alan Turing Institute

TÓM TẮT
Chúng tôi giới thiệu Performers, các kiến trúc Transformer có thể ước lượng các Transformer attention đầy đủ (softmax) rank đầy đủ thông thường với độ chính xác có thể chứng minh, nhưng chỉ sử dụng độ phức tạp không gian và thời gian tuyến tính (thay vì bậc hai), mà không dựa vào bất kỳ kiến thức tiền nghiệm nào như độ thưa thớt hoặc rank thấp. Để xấp xỉ các kernel attention-softmax, Performers sử dụng một phương pháp Fast Attention Via positive Orthogonal Random features mới (FAVOR+), có thể có ý nghĩa độc lập cho các phương pháp kernel có thể mở rộng. FAVOR+ cũng có thể được sử dụng để mô hình hóa hiệu quả các cơ chế attention có thể kernelizable vượt ra ngoài softmax. Sức mạnh biểu diễn này rất quan trọng để so sánh chính xác softmax với các kernel khác lần đầu tiên trên các tác vụ quy mô lớn, vượt ra ngoài tầm với của các Transformer thông thường, và điều tra các kernel attention tối ưu. Performers là các kiến trúc tuyến tính hoàn toàn tương thích với các Transformer thông thường và có các đảm bảo lý thuyết mạnh mẽ: ước lượng không thiên lệch hoặc gần như không thiên lệch của ma trận attention, hội tụ đồng nhất và phương sai ước lượng thấp. Chúng tôi đã thử nghiệm Performers trên một tập hợp phong phú các tác vụ từ dự đoán pixel qua các mô hình văn bản đến mô hình hóa chuỗi protein. Chúng tôi chứng minh kết quả cạnh tranh với các phương pháp attention thưa thớt và dày đặc hiệu quả khác đã được kiểm tra, thể hiện hiệu quả của mô hình học attention mới được tận dụng bởi Performers.

1 GIỚI THIỆU VÀ CÔNG TRÌNH LIÊN QUAN
Transformers (Vaswani et al., 2017; Dehghani et al., 2019) là các kiến trúc mạng nơ-ron mạnh mẽ đã trở thành SOTA trong một số lĩnh vực machine learning bao gồm xử lý ngôn ngữ tự nhiên (NLP) (ví dụ: nhận dạng giọng nói (Luo et al., 2020)), dịch máy nơ-ron (NMT) (Chen et al., 2018), tạo/tóm tắt tài liệu, dự đoán chuỗi thời gian, mô hình hóa sinh (ví dụ: tạo hình ảnh (Parmar et al., 2018)), tạo nhạc (Huang et al., 2019), và tin sinh học (Rives et al., 2019; Madani et al., 2020; Ingraham et al., 2019; Elnaggar et al., 2019; Du et al., 2020).

Transformers dựa vào một cơ chế attention có thể huấn luyện xác định các phụ thuộc phức tạp giữa các phần tử của mỗi chuỗi đầu vào. Thật không may, Transformer thông thường có độ phức tạp bậc hai với số lượng token L trong chuỗi đầu vào, điều này cực kỳ tốn kém cho L lớn và ngăn cản việc sử dụng nó trong các môi trường có tài nguyên tính toán hạn chế ngay cả đối với các giá trị L vừa phải. Một số giải pháp đã được đề xuất để giải quyết vấn đề này (Beltagy et al., 2020; Gulati et al., 2020; Chan et al., 2020; Child et al., 2019; Bello et al., 2019). Hầu hết các phương pháp hạn chế cơ chế attention để chú ý đến các vùng lân cận cục bộ (Parmar et al., 2018) hoặc kết hợp các kiến thức tiền nghiệm cấu trúc về attention như độ thưa thớt (Child et al., 2019), nén dựa trên pooling (Rae et al., 2020) các kỹ thuật clustering/binning/convolution (ví dụ: (Roy et al., 2020) áp dụng k-means clustering để học các vùng attention thưa thớt động, hoặc (Kitaev et al., 2020), nơi locality sensitive hashing được sử dụng để nhóm các token có embeddings tương tự), cửa sổ trượt (Beltagy et al., 2020), hoặc targeting cắt ngắn (Chelba et al., 2020). Cũng có một dòng nghiên cứu dài về việc sử dụng ma trận attention dày đặc, nhưng được định nghĩa bởi các kernel rank thấp thay thế softmax (Katharopoulos et al., 2020; Shen et al., 2018). Những phương pháp đó chủ yếu dựa vào các kernel có biểu diễn rõ ràng dưới dạng tích vô hướng của các vector đặc trưng dương hữu hạn.

Các phương pháp trên không nhằm mục đích xấp xỉ attention thông thường, mà đề xuất các cơ chế attention đơn giản và dễ xử lý hơn, thường bằng cách kết hợp các ràng buộc bổ sung (ví dụ: tập query và key giống hệt nhau như trong (Kitaev et al., 2020)), hoặc bằng cách đánh đổi attention thông thường với attention thưa thớt sử dụng nhiều lớp hơn (Child et al., 2019). Thật không may, thiếu các đảm bảo nghiêm ngặt cho sức mạnh biểu diễn được tạo ra bởi các phương pháp như vậy, và đôi khi tính hợp lệ của các mẫu thưa thớt chỉ có thể được xác minh bằng thực nghiệm thông qua thử và sai bằng cách xây dựng các phép toán GPU đặc biệt (ví dụ: viết kernel C++ CUDA (Child et al., 2019) hoặc sử dụng TVM (Beltagy et al., 2020)). Các kỹ thuật khác nhằm giảm độ phức tạp không gian của Transformers bao gồm các lớp residual có thể đảo ngược cho phép lưu trữ activation một lần trong quá trình huấn luyện (Kitaev et al., 2020) và trọng số attention được chia sẻ (Xiao et al., 2019).

Những ràng buộc này có thể cản trở việc áp dụng vào các vấn đề chuỗi dài, nơi các xấp xỉ của cơ chế attention không đủ. Các xấp xỉ dựa trên back-propagation cắt ngắn (Dai et al., 2019) cũng không thể nắm bắt các tương quan tầm xa vì gradient chỉ được truyền trong một cửa sổ cục bộ. Các phương pháp khác đề xuất ước lượng thiên lệch của attention thông thường nhưng chỉ trong môi trường không nhân quả và với sai số bình phương trung bình lớn (Wang et al., 2020).

Để đáp lại, chúng tôi giới thiệu các kiến trúc Transformer đầu tiên, Performers, có khả năng ước lượng có thể chứng minh chính xác và thực tế của attention đầy đủ (softmax) rank đầy đủ thông thường, nhưng chỉ có độ phức tạp không gian và thời gian tuyến tính và không dựa vào bất kỳ kiến thức tiền nghiệm nào như độ thưa thớt hoặc rank thấp. Performers sử dụng cơ chế Fast Attention Via positive Orthogonal Random features (FAVOR+), tận dụng các phương pháp mới để xấp xỉ kernel softmax và Gaussian, mà chúng tôi đề xuất. Chúng tôi tin rằng những phương pháp này có ý nghĩa độc lập, đóng góp vào lý thuyết các phương pháp kernel có thể mở rộng. Do đó, Performers là các kiến trúc tuyến tính đầu tiên hoàn toàn tương thích (thông qua lượng nhỏ fine-tuning) với các Transformer thông thường, cung cấp các đảm bảo lý thuyết mạnh mẽ: ước lượng không thiên lệch hoặc gần như không thiên lệch của ma trận attention, hội tụ đồng nhất và phương sai thấp hơn của xấp xỉ.

FAVOR+ cũng có thể được áp dụng để mô hình hóa hiệu quả các cơ chế attention có thể kernelizable khác ngoài softmax. Sức mạnh biểu diễn này rất quan trọng để so sánh chính xác softmax với các kernel khác lần đầu tiên trên các tác vụ quy mô lớn, vượt ra ngoài tầm với của các Transformer thông thường, và tìm ra các kernel attention tối ưu cho chúng. FAVOR+ cũng có thể được áp dụng ngoài phạm vi Transformer như một sự thay thế có thể mở rộng hơn cho attention thông thường, bản thân nó có nhiều ứng dụng trong computer vision (Fu et al., 2019), reinforcement learning (Zambaldi et al., 2019), huấn luyện với softmax cross entropy loss, và thậm chí tối ưu hóa tổ hợp (Vinyals et al., 2015).

Chúng tôi thử nghiệm Performers trên một tập hợp phong phú các tác vụ từ dự đoán pixel qua các mô hình văn bản đến mô hình hóa chuỗi protein. Chúng tôi chứng minh kết quả cạnh tranh với các phương pháp attention thưa thớt và dày đặc hiệu quả khác đã được kiểm tra, thể hiện hiệu quả của mô hình học attention mới được tận dụng bởi Performers. Chúng tôi nhấn mạnh rằng về nguyên tắc, FAVOR+ cũng có thể được kết hợp với các kỹ thuật khác, như các lớp có thể đảo ngược (Kitaev et al., 2020) hoặc attention dựa trên cluster (Roy et al., 2020).

2 Cơ CHẾ FAVOR+ & POSITIVE ORTHOGONAL RANDOM FEATURES

Dưới đây chúng tôi mô tả chi tiết cơ chế FAVOR+ - xương sống của kiến trúc Performer. Chúng tôi giới thiệu một phương pháp mới để ước lượng kernel softmax (và Gaussian) với positive orthogonal random features mà FAVOR+ tận dụng để ước lượng mạnh mẽ và không thiên lệch attention thông thường (softmax) và chỉ ra cách FAVOR+ có thể được áp dụng cho các kernel attention khác.

2.1 KIẾN THỨC SƠ BỘ - CƠ CHẾ ATTENTION THÔNG THƯỜNG

Gọi L là kích thước của một chuỗi đầu vào các token. Khi đó attention tích vô hướng thông thường (Vaswani et al., 2017) là một ánh xạ nhận các ma trận Q,K,V∈R^{L×d} làm đầu vào trong đó d là chiều ẩn (chiều của biểu diễn tiềm ẩn). Các ma trận Q,K,V là các biểu diễn trung gian của đầu vào và các hàng của chúng có thể được hiểu là queries, keys và values của cấu trúc dữ liệu từ điển liên tục tương ứng. Attention tích vô hướng hai chiều (hoặc không định hướng (Devlin et al., 2018)) có dạng sau, trong đó A∈R^{L×L} là ma trận attention được gọi là:

Att$(Q,K,V) = D^{-1}AV, A = exp(QK^T/√d), D = diag(A1_L)     (1)

Ở đây exp() được áp dụng từng phần tử, 1_L là vector toàn số một có độ dài L, và diag() là ma trận đường chéo với vector đầu vào là đường chéo. Độ phức tạp thời gian và không gian của việc tính toán (1) là O(L²d) và O(L² + Ld) tương ứng, vì A phải được lưu trữ một cách rõ ràng. Do đó, về nguyên tắc, attention tích vô hướng loại (1) không tương thích với xử lý end-to-end của các chuỗi dài. Attention hai chiều được áp dụng trong encoder self-attention và encoder-decoder attention trong các kiến trúc Seq2Seq.

Một loại attention quan trọng khác là attention tích vô hướng một chiều có dạng:

Att→(Q,K,V) = D̃^{-1}ÃV, Ã = tril(A), D̃ = diag(Ã1_L)     (2)

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

trong đó tril() trả về phần tam giác dưới của ma trận đối số bao gồm đường chéo. Như đã thảo luận trong (Vaswani et al., 2017), attention một chiều được sử dụng cho mô hình hóa sinh tự hồi quy, ví dụ: như self-attention trong các Transformer sinh cũng như phần decoder của các Transformer Seq2Seq.

Chúng tôi sẽ chỉ ra rằng ma trận attention A có thể được xấp xỉ với độ chính xác bất kỳ trong thời gian O(Ld²log(d)). Để so sánh, các phương pháp phổ biến tận dụng độ thưa thớt thông qua các kỹ thuật Locality-Sensitive Hashing (LSH) (Kitaev et al., 2020) có độ phức tạp thời gian O(Ld²logL). Trong phần chính của bài báo, chúng tôi sẽ mô tả FAVOR+ cho attention hai chiều. Các kết quả hoàn toàn tương tự có thể được thu được cho biến thể một chiều thông qua cơ chế prefix-sum (tất cả chi tiết trong Phụ lục B.1).

2.2 GENERALIZED KERNELIZABLE ATTENTION

FAVOR+ hoạt động cho các khối attention sử dụng ma trận A∈R^{L×L} có dạng A(i,j) = K(q_i^T, k_j^T), với q_i = k_j đại diện cho vector hàng query/key thứ i/j trong Q = K và kernel K: R^d × R^d → R^+ được định nghĩa cho ánh xạ (thường được ngẫu nhiên hóa): φ: R^d → R_+^r (cho một số r > 0) là:

K(x,y) = E[φ(x)^T φ(y)]     (3)

Chúng ta gọi φ(u) là random feature map cho u∈R^d. Với Q', K'∈R^{L×r} có các hàng được cho là (φ(q_i^T))^T và (φ(k_i^T))^T tương ứng, Phương trình 3 dẫn trực tiếp đến cơ chế attention hiệu quả có dạng:

Âtt$(Q,K,V) = D̂^{-1}(Q'((K')^T V)), D̂ = diag(Q'((K')^T 1_L))     (4)

Ở đây Âtt$ đại diện cho attention xấp xỉ và dấu ngoặc cho biết thứ tự tính toán. Dễ thấy rằng cơ chế như vậy được đặc trưng bởi độ phức tạp không gian O(Lr + Ld + rd) và độ phức tạp thời gian O(Lrd) thay vì O(L² + Ld) và O(L²d) của attention thông thường (xem thêm Hình 1).

Hình 1: Xấp xỉ cơ chế attention thông thường AV (trước khi chuẩn hóa D^{-1}) thông qua (random) feature maps. Các khối nét đứt cho biết thứ tự tính toán với độ phức tạp thời gian tương ứng được đính kèm.

Sơ đồ trên tạo thành phần FA của cơ chế FAVOR+. Phần OR+ còn lại trả lời các câu hỏi sau: (1) Mô hình attention được định nghĩa trong Phương trình 3 có tính biểu diễn như thế nào, và đặc biệt, về nguyên tắc chúng ta có thể sử dụng nó để xấp xỉ attention softmax thông thường không? (2) Làm thế nào chúng ta triển khai nó một cách mạnh mẽ trong thực tế, và đặc biệt, chúng ta có thể chọn r ≪ L cho L ≫ d để đạt được lợi ích về độ phức tạp không gian và thời gian mong muốn không? Chúng tôi trả lời những câu hỏi này trong các phần tiếp theo.

2.3 CÁCH XẤP XỈ VÀ CÁCH KHÔNG XẤP XỈ KERNEL SOFTMAX CHO ATTENTION

Hóa ra bằng cách lấy φ có dạng sau cho các hàm f₁,...,f_l: R → R, hàm g: R^d → R và các vector xác định ω_i hoặc ω₁,...,ω_m iid D cho một số phân phối D∈P(R^d):

φ(x) = h(x)/√m(f₁(ω₁^T x),...,f₁(ω_m^T x),...,f_l(ω₁^T x),...,f_l(ω_m^T x))     (5)

chúng ta có thể mô hình hóa hầu hết các kernel được sử dụng trong thực tế. Hơn nữa, trong hầu hết các trường hợp D là isotropic (tức là có hàm pdf không đổi trên một mặt cầu), thường là Gaussian. Ví dụ, bằng cách lấy h(x) = 1, l = 1 và D = N(0,I_d) chúng ta thu được các ước lượng của các kernel PNG được gọi là (Choromanski et al., 2017) (ví dụ: f₁ = sgn tương ứng với kernel góc). Các cấu hình: h(x) = 1, l = 2, f₁ = sin, f₂ = cos tương ứng với các kernel shift-invariant, đặc biệt D = N(0,I_d) dẫn đến kernel Gaussian K_gauss (Rahimi & Recht, 2007). Kernel softmax định nghĩa ma trận attention thông thường A được cho là:

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

SM(x,y) ≝ exp(x^T y)     (6)

Trong phần trên, không mất tính tổng quát, chúng tôi bỏ qua việc chuẩn hóa √d vì chúng ta có thể chuẩn hóa lại các key và query đầu vào một cách tương đương. Vì: SM(x,y) = exp(-||x||₂²/2)K_gauss(x,y)exp(-||y||₂²/2), dựa trên những gì chúng tôi đã nói, chúng ta thu được xấp xỉ random feature map không thiên lệch của SM(x,y) sử dụng các hàm lượng giác với: h(x) = exp(-||x||₂²/2), l = 2, f₁ = sin, f₂ = cos. Chúng tôi gọi nó là SM_trig^m(x,y).

Tuy nhiên có một điều cần lưu ý ở đây. Mô-đun attention từ (1) xây dựng cho mỗi token, một tổ hợp lồi của các vector giá trị với các hệ số được cho là các điểm kernel được chuẩn hóa tương ứng. Đó là lý do tại sao các kernel tạo ra điểm số không âm được sử dụng. Việc áp dụng random feature map với các giá trị chiều có thể âm (sin/cos) dẫn đến các hành vi không ổn định, đặc biệt khi các điểm kernel gần 0 (đây là trường hợp đối với nhiều mục của A tương ứng với các token có mức độ liên quan thấp) được xấp xỉ bởi các ước lượng có phương sai lớn trong các vùng như vậy. Điều này dẫn đến các hành vi bất thường, ví dụ: các giá trị chuẩn hóa đường chéo âm D^{-1}, và do đó hoặc hoàn toàn ngăn cản việc huấn luyện hoặc dẫn đến các mô hình dưới tối ưu. Chúng tôi chứng minh thực nghiệm rằng đây là những gì xảy ra với SM_trig^m và cung cấp các giải thích lý thuyết chi tiết cho thấy rằng phương sai của SM_trig^m lớn khi các giá trị xấp xỉ có xu hướng về 0 (xem: Phần 3). Đây là một trong những lý do chính tại sao cơ chế random feature map mạnh mẽ để xấp xỉ attention softmax thông thường chưa bao giờ được đề xuất.

Chúng tôi đề xuất một cơ chế mạnh mẽ trong bài báo này. Hơn nữa, phương sai của ước lượng positive random feature map không thiên lệch mới của chúng tôi có xu hướng về 0 khi các giá trị xấp xỉ có xu hướng về 0 (xem: Phần 3).

Bổ đề 1 (Positive Random Features (PRFs) cho Softmax). Với x,y∈R^d, z = x + y chúng ta có:

SM(x,y) = E_{ω~N(0,I_d)}[exp(ω^T x - ||x||₂²/2)exp(ω^T y - ||y||₂²/2)]
= E_{ω~N(0,I_d)}[β cosh(ω^T z)]     (7)

trong đó β = exp(-(||x||₂² + ||y||₂²)/2) và cosh là hàm cosine hyperbolic. Do đó, kernel softmax có một xấp xỉ positive random feature map không thiên lệch với h(x) = exp(-||x||₂²/2), l = 1, f₁ = exp và D = N(0,I_d) hoặc: h(x) = 1/√2 exp(-||x||₂²/2), l = 2, f₁(u) = exp(u), f₂(u) = exp(-u) và cùng D (cái sau để giảm phương sai thêm). Chúng tôi gọi các ước lượng liên quan: SM_m^+ và SM_hyp^+.

Hình 2: Trái: Hàm tiện ích đối xứng (xung quanh gốc) r (được định nghĩa là tỷ số của sai số bình phương trung bình (MSE) của các ước lượng được xây dựng trên: trigonometric và positive random features) như một hàm của góc (theo radian) giữa các vector đặc trưng đầu vào và độ dài l của chúng. Các giá trị lớn hơn cho thấy các vùng của không gian (θ,l) có hiệu suất tốt hơn của positive random features. Chúng ta thấy rằng đối với các vùng quan trọng với θ đủ lớn (giá trị kernel softmax đủ nhỏ), phương pháp của chúng tôi chính xác hơn một cách tùy ý so với trigonometric random features. Biểu đồ được trình bày cho miền [-π,π]×[2,2]. Phải: Lát cắt của hàm r cho l = 1 cố định và góc θ thay đổi. Góc trên bên phải: So sánh MSE của cả hai ước lượng trong vùng giá trị kernel softmax thấp.

Trong Hình 2, chúng tôi trực quan hóa các lợi thế của positive so với standard trigonometric random features. Trong các vùng quan trọng, nơi các giá trị kernel nhỏ và cần xấp xỉ cẩn thận, phương pháp của chúng tôi vượt trội hơn so với đối tác của nó. Trong Phần 4, chúng tôi tiếp tục xác nhận các lợi thế của phương pháp một cách thực nghiệm, sử dụng positive features để huấn luyện hiệu quả các Transformer tuyến tính dựa trên softmax. Nếu chúng ta thay thế ω trong (7) bằng √d ω/||ω||, chúng ta thu được kernel softmax được điều chỉnh SM_REG được gọi là mà chúng ta có thể xấp xỉ theo cách tương tự, đơn giản thay đổi D = N(0,I_d) thành D = Unif(√d S^{d-1}), một phân phối tương ứng với độ đo Haar trên mặt cầu bán kính √d trong R^d, thu được ước lượng SM_REG^+. Như chúng tôi chỉ ra trong Phần 3, những random features như vậy cũng có thể được sử dụng để xấp xỉ chính xác kernel softmax thông thường.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

2.4 ORTHOGONAL RANDOM FEATURES (ORFS)

Phần trên tạo thành phần R+ của phương pháp FAVOR+. Vẫn còn phải giải thích phần O. Để giảm thêm phương sai của ước lượng (để chúng ta có thể sử dụng số lượng random features r nhỏ hơn), chúng tôi làm cho các mẫu ngẫu nhiên khác nhau ω₁,...,ωₘ trở nên chính xác trực giao. Điều này có thể được thực hiện trong khi duy trì tính không thiên lệch bất cứ khi nào các phân phối isotropic D được sử dụng (tức là đặc biệt trong tất cả các kernel mà chúng tôi đã xem xét cho đến nay) bằng quy trình trực giao hóa Gram-Schmidt tiêu chuẩn (xem (Choromanski et al., 2017) để biết chi tiết). ORFs là một phương pháp nổi tiếng, tuy nhiên hóa ra nó hoạt động đặc biệt tốt với PRFs đã giới thiệu của chúng tôi cho softmax. Điều này dẫn đến các kết quả lý thuyết đầu tiên cho thấy ORFs có thể được áp dụng để giảm phương sai của các ước lượng kernel softmax/Gaussian cho bất kỳ chiều d nào thay vì chỉ tiệm cận cho d đủ lớn (như trường hợp của các phương pháp trước đó, xem: phần tiếp theo) và dẫn đến các ràng buộc nhỏ theo cấp số nhân đầu tiên về xác suất lệch lớn chặt chẽ hơn so với các phương pháp không trực giao. Tính dương của random features đóng vai trò chính trong những ràng buộc này. Cơ chế ORF yêu cầu m ≤ d, nhưng đây sẽ là trường hợp trong tất cả các thí nghiệm của chúng tôi. Mã giả của toàn bộ thuật toán FAVOR+ được đưa ra trong Phụ lục B.

Các kết quả lý thuyết của chúng tôi được căn chỉnh chặt chẽ với các thí nghiệm. Chúng tôi chỉ ra trong Phần 4 rằng PRFs+ORFs cải thiện đáng kể độ chính xác của xấp xỉ ma trận attention và cho phép chúng tôi giảm r dẫn đến một cơ chế chính xác cũng như hiệu quả về không gian và thời gian mà chúng tôi gọi là FAVOR+.

3 KẾT QUẢ LÝ THUYẾT

Chúng tôi trình bày ở đây lý thuyết về positive orthogonal random features cho ước lượng kernel softmax. Tất cả những kết quả này cũng có thể được áp dụng cho kernel Gaussian, vì như đã giải thích trong phần trước, một cái có thể được thu được từ cái kia bằng chuẩn hóa (xem: Phần 2.3). Tất cả các chứng minh và các kết quả lý thuyết tổng quát bổ sung với thảo luận được đưa ra trong Phụ lục.

Bổ đề 2 (positive (hyperbolic) so với trigonometric random features). Điều sau đây là đúng:

MSE(SM_trig^m(x,y)) = 1/(2m) exp(||x+y||²)SM²(x,y)(1-exp(-||x-y||²))²;
MSE(SM_m^+(x,y)) = 1/m exp(||x+y||²)SM²(x,y)(1-exp(-||x+y||²));
MSE(SM_hyp^+(x,y)) = 1/2(1-exp(-||x+y||²))MSE(SM_m^+(x,y));     (8)

cho các mẫu ngẫu nhiên độc lập ωᵢ, và trong đó MSE đại diện cho sai số bình phương trung bình.

Do đó, với SM(x,y) → 0 chúng ta có: MSE(SM_trig^m(x,y)) → ∞ và MSE(SM_m^+(x,y)) → 0. Hơn nữa, ước lượng hyperbolic cung cấp các cải thiện độ chính xác bổ sung chặt chẽ hơn so với SM_2m^+(x,y) với gấp đôi số lượng random features. Kết quả tiếp theo cho thấy rằng kernel softmax được điều chỉnh trong thực tế là một proxy chính xác của kernel softmax trong attention.

Định lý 1 (kernel được điều chỉnh so với kernel softmax). Giả sử rằng chuẩn L₁ của ma trận attention cho kernel softmax thỏa mãn: ||A||₁ ≤ C cho một hằng số C ≥ 1. Ký hiệu bằng A_reg ma trận attention tương ứng cho kernel softmax được điều chỉnh. Điều sau đây đúng:

inf_{i,j} A_reg(i,j)/A(i,j) ≥ 1 - 2/d^{1/3} + o(1/d^{1/3}), và sup_{i,j} A_reg(i,j)/A(i,j) ≤ 1     (9)

Hơn nữa, cái sau đúng cho d ≥ 2 ngay cả khi điều kiện chuẩn L₁ không được thỏa mãn, tức là kernel softmax được điều chỉnh là một ràng buộc dưới toàn cục cho kernel softmax.

Do đó, positive random features cho SM_REG có thể được sử dụng để xấp xỉ kernel softmax. Kết quả tiếp theo của chúng tôi cho thấy rằng tính trực giao có thể chứng minh giảm sai số bình phương trung bình của ước lượng với positive random features cho bất kỳ chiều d > 0 nào và chúng tôi cung cấp rõ ràng khoảng cách.

Định lý 2. Nếu SM_ort^+(x,y) đại diện cho sự sửa đổi của SM_m^+(x,y) với orthogonal random features (và do đó cho m ≤ d), thì điều sau đây đúng cho bất kỳ d > 0 nào:

MSE(SM_ort^+(x,y)) ≤ MSE(SM_m^+(x,y)) - 2(m-1)/(m(d+2)) × SM(x,y)exp(-(||x||² + ||y||²)/2)     (10)

Hơn nữa, kết quả hoàn toàn tương tự đúng cho kernel softmax được điều chỉnh SM_REG.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

Đối với kernel softmax được điều chỉnh, các orthogonal features cung cấp các kết quả tập trung bổ sung - các ràng buộc nhỏ theo cấp số nhân đầu tiên cho xác suất đuôi của các ước lượng chặt chẽ hơn so với các biến thể không trực giao cho mọi d > 0. Kết quả tiếp theo của chúng tôi cho phép chúng tôi ước lượng rõ ràng khoảng cách.

Định lý 3. Gọi x,y∈R^d. Điều sau đây đúng cho bất kỳ a > SM_REG(x,y), λ > 0 và m ≤ d:

P[SM_REG^+_m(x,y) > a] ≤ exp(-λma)M_Z(λ)^m; P[SM_REG^{ort+}_m(x,y) > a] ≤ 
exp(-λma)/(M_Z(λ)^m) × exp(-m^2(||x||² + ||y||²) - 4m(m-1))/(4(d+2)||x+y||⁴)

trong đó SM_REG^{ort+}_m(x,y) đại diện cho sự sửa đổi của SM_REG^+_m(x,y) với ORFs, X = β exp(√d ω^T/(||ω||_2)(x+y)), ω ~ N(0,I_d), β như trong Bổ đề 1 và M_Z là hàm sinh moment của Z.

Chúng ta thấy rằng ORFs cung cấp các ràng buộc nhỏ theo cấp số nhân và sắc nét hơn cho các vùng quan trọng nơi kernel softmax nhỏ. Dưới đây chúng tôi chỉ ra rằng ngay cả đối với cơ chế SM_trig với ORFs, chỉ cần lấy m = Θ(d log(d)) random projections để xấp xỉ chính xác ma trận attention (do đó nếu không chuẩn hóa attention, PRFs sẽ không cần thiết). Nói chung, m phụ thuộc vào chiều d của embeddings, bán kính R của quả cầu nơi tất cả queries/keys tồn tại và tham số độ chính xác ε (xem: Phụ lục F.6 để thảo luận bổ sung), nhưng không phụ thuộc vào độ dài chuỗi đầu vào L.

Định lý 4 (hội tụ đồng nhất cho xấp xỉ attention). Giả sử rằng chuẩn L₂ của queries/keys được ràng buộc trên bởi R > 0. Định nghĩa l = Rd^{-1/4} và lấy h = exp(-l²/2). Khi đó cho bất kỳ ε > 0, δ = ε/(h)² và số lượng random projections m = Θ(d/ε² log(4d³/⁴R/ε)) điều sau đây đúng cho cơ chế xấp xỉ attention tận dụng các ước lượng SM_trig với ORFs:

||Â - A||₁ ≤ ε với bất kỳ xác suất hằng số nào, trong đó Â xấp xỉ ma trận attention A.

4 THÍ NGHIỆM

Chúng tôi triển khai thiết lập của mình trên mã huấn luyện Transformer đã có sẵn trong Jax (Frostig et al., 2018) được tối ưu hóa với biên dịch just-in-time (jax.jit), và bổ sung lý thuyết của chúng tôi bằng bằng chứng thực nghiệm để chứng minh tính thực tế của FAVOR+ trong nhiều thiết lập. Trừ khi được nêu rõ ràng, một Performer chỉ thay thế thành phần attention bằng phương pháp của chúng tôi, trong khi tất cả các thành phần khác hoàn toàn giống như Transformer thông thường. Để ký hiệu tắt, chúng tôi ký hiệu mô hình hóa một chiều/nhân quả là (U) và mô hình hóa ngôn ngữ hai chiều/có mặt nạ là (B).

Về các baseline, chúng tôi sử dụng các mô hình Transformer khác để so sánh, mặc dù một số trong số chúng bị hạn chế chỉ một trường hợp - ví dụ: Reformer (Kitaev et al., 2020) chỉ là (U), và Linformer (Wang et al., 2020) chỉ là (B). Hơn nữa, chúng tôi sử dụng PG-19 (Rae et al., 2020) như một benchmark pretraining (B) thay thế, vì nó được tạo ra cho việc huấn luyện chuỗi độ dài dài so với dataset BookCorpus (Zhu et al., 2015) + Wikipedia (hiện không có sẵn công khai) được sử dụng trong BERT (Devlin et al., 2018) và Linformer. Tất cả các siêu tham số mô hình và tokenization được hiển thị trong Phụ lục A.

Hình 3: So sánh Transformer và Performer về tốc độ forward và backward pass và L tối đa được cho phép. "X" (OPT) ký hiệu tăng tốc tối đa có thể đạt được, khi attention đơn giản trả về ma trận V. Các biểu đồ được hiển thị cho đến khi một mô hình tạo ra lỗi hết bộ nhớ trên GPU V100 với 16GB. Kích thước từ vựng được sử dụng là 256. Tốt nhất được tô màu.

4.1 CHI PHÍ TÍNH TOÁN

Chúng tôi so sánh về tốc độ backward pass của Transformer và Performer trong thiết lập (B), vì đây là một trong những nút thắt tính toán chính trong quá trình huấn luyện, khi sử dụng kích thước mặc định thông thường (n_heads, n_layers, d_ff, d) = (8, 6, 2048, 512), trong đó d_ff ký hiệu chiều rộng của các lớp MLP.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

Chúng tôi quan sát (Hình 3) rằng về L, Performer đạt được thời gian gần như tuyến tính và tiêu thụ bộ nhớ dưới bậc hai (vì ma trận attention O(L²) rõ ràng không được lưu trữ). Thực tế, Performer đạt được tăng tốc và hiệu quả bộ nhớ gần như tối ưu có thể, được mô tả bởi đường "X" khi attention được thay thế bằng "hàm đồng nhất" đơn giản trả về ma trận V. Sự kết hợp của cả hiệu quả bộ nhớ và backward pass cho L lớn cho phép tương ứng, huấn luyện batch lớn và thời gian đồng hồ thấp hơn mỗi bước gradient. Các kết quả bổ sung mở rộng được chứng minh trong Phụ lục E bằng cách thay đổi các lớp, attention thô, và kích thước kiến trúc.

4.2 SAI SỐ XẤP XỈ SOFTMAX ATTENTION

Chúng tôi tiếp tục kiểm tra sai số xấp xỉ thông qua FAVOR+ trong Hình 4. Chúng tôi chứng minh rằng 1. Orthogonal features tạo ra sai số thấp hơn so với unstructured (IID) features, 2. Positive features tạo ra sai số thấp hơn so với trigonometric sin/cos features. Hai điều này xác nhận thực nghiệm cơ chế PORF.

Hình 4: MSE của đầu ra xấp xỉ khi so sánh Orthogonal vs IID features và trigonometric sin/cos vs positive features. Chúng tôi lấy L = 4096, d = 16, và thay đổi số lượng mẫu ngẫu nhiên m. Độ lệch chuẩn được hiển thị trên 15 mẫu dữ liệu đầu vào ma trận ngẫu nhiên được chuẩn hóa phù hợp.

Để cải thiện thêm xấp xỉ tổng thể của các khối attention qua nhiều lần lặp nhằm cải thiện thêm việc huấn luyện, các mẫu ngẫu nhiên nên được vẽ lại định kỳ (Hình 5, phải). Đây là một quy trình rẻ, nhưng có thể được tối ưu hóa thêm (Phụ lục B.2).

4.3 XẤP XỈ SOFTMAX TRÊN TRANSFORMERS

Ngay cả khi xấp xỉ của cơ chế attention chặt chẽ, các sai số nhỏ có thể dễ dàng lan truyền qua nhiều lớp Transformer (ví dụ: MLPs, nhiều heads), như chúng tôi chỉ ra trong Hình 14 (Phụ lục). Nói cách khác, hằng số Lipschitz của mô hình có thể dễ dàng mở rộng sai số xấp xỉ attention nhỏ, có nghĩa là đôi khi các xấp xỉ rất chặt chẽ có thể cần thiết. Do đó, khi áp dụng các xấp xỉ softmax của FAVOR(+) trên một mô hình Transformer (tức là "Performer-X-SOFTMAX"), chúng tôi chứng minh rằng:

1. Khả năng tương thích ngược với các mô hình được huấn luyện trước có sẵn như một lợi ích từ xấp xỉ softmax, thông qua fine-tuning nhỏ (cần thiết do lan truyền sai số) ngay cả đối với trigonometric features (Hình 5, trái) trên dataset LM1B (Chelba et al., 2014). Tuy nhiên, khi trên dataset lớn hơn PG-19, 2. Positive (POS) softmax features (với redrawing) trở nên quan trọng để đạt được hiệu suất phù hợp với các Transformer thông thường (Hình 5, phải).

Hình 5: Chúng tôi chuyển trọng số của Transformer được huấn luyện trước ban đầu vào Performer, tạo ra độ chính xác ban đầu khác không 0.07 (đường cam chấm), nhưng nhanh chóng phục hồi độ chính xác trong một phần nhỏ số bước gradient ban đầu. Tuy nhiên trên PG-19, xấp xỉ softmax Trigonometric (TRIG) trở nên rất không ổn định (đường cong đầy đủ trong Phụ lục D.2), trong khi positive features (POS) (không redrawing) và Linformer (cũng xấp xỉ softmax) ngay cả với redrawn projections, đều đạt ngưỡng ở cùng một perplexity. Positive softmax với feature redrawing là cần thiết để phù hợp với Transformer, với SM_REG (điều chỉnh từ Phần 3) cho phép hội tụ nhanh hơn. Các nghiên cứu ablation bổ sung trên nhiều kernel attention, cũng cho thấy rằng trigonometric random features thậm chí dẫn đến các giá trị NaN trong huấn luyện được đưa ra trong Phụ lục D.3.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

4.4 HUẤN LUYỆN NHIỀU LỚP CHO PROTEINS

Chúng tôi tiếp tục benchmark Performer trên cả trường hợp (U) và (B) bằng cách huấn luyện một mô hình 36 lớp sử dụng các chuỗi protein từ bản phát hành tháng 1 năm 2019 của TrEMBL (Consortium, 2019), tương tự như (Madani et al., 2020). Trong Hình 6, Reformer và Linformer giảm đáng kể độ chính xác trên dataset protein. Hơn nữa, tính hữu ích của generalized attention được chứng minh bởi Performer-RELU (lấy f = ReLU trong Phương trình 5) đạt được độ chính xác cao nhất trong cả trường hợp (U) và (B). Xấp xỉ softmax được đề xuất của chúng tôi cũng được chỉ ra là chặt chẽ, đạt được cùng độ chính xác như Transformer softmax chính xác và xác nhận các tuyên bố lý thuyết của chúng tôi từ Phần 3.

Hình 6: Train = Nét đứt, Validation = Nét liền. Đối với TrEMBL, chúng tôi sử dụng chính xác cùng các tham số mô hình (n_heads, n_layers, d_ff, d) = (8, 36, 1024, 512) từ (Madani et al., 2020) cho tất cả các lần chạy. Để công bằng, tất cả các thí nghiệm TrEMBL đều sử dụng 16x16 TPU-v2. Kích thước batch được tối đa hóa cho mỗi lần chạy riêng biệt với các ràng buộc tính toán. Siêu tham số có thể được tìm thấy trong Phụ lục A. Kết quả mở rộng bao gồm thống kê dataset, đánh giá ngoài phân phối, và trực quan hóa, có thể được tìm thấy trong Phụ lục C.

4.5 HUẤN LUYỆN ĐỘ DÀI LỚN - CÁC DATASET THÔNG THƯỜNG

Trên benchmark ImageNet64 (U) tiêu chuẩn từ (Parmar et al., 2018) với L = 12288 không khả thi cho các Transformer thông thường, chúng tôi đặt tất cả các mô hình sử dụng cùng (n_heads, d_ff, d) nhưng thay đổi n_layers. Performer/6-layers phù hợp với Reformer/12-layers, trong khi Performer/12-layers phù hợp với Reformer/24-layers (Hình 7: trái). Tùy thuộc vào phần cứng (TPU hoặc GPU), chúng tôi cũng thấy rằng Performer có thể nhanh hơn 2 lần so với Reformer thông qua tối ưu hóa Jax cho thiết lập (U).

Đối với một nghiên cứu nguyên lý chứng minh, chúng tôi cũng tạo ra một benchmark protein ban đầu để dự đoán tương tác giữa các nhóm protein bằng cách nối các chuỗi protein đến độ dài L = 8192 từ TrEMBL, đủ dài để mô hình hóa mạng tương tác protein mà không cần căn chỉnh chuỗi lớn được yêu cầu bởi các phương pháp hiện có (Cong et al., 2019). Trong thiết lập này, một Transformer thông thường làm quá tải bộ nhớ ngay cả ở kích thước batch 1 mỗi chip, với một biên độ rộng. Do đó như một baseline, chúng tôi buộc phải sử dụng một biến thể nhỏ hơn đáng kể, giảm xuống (n_heads, n_layers, d_ff, d) = (8, {1,2,3}, 256, 256). Trong khi đó, Performer huấn luyện hiệu quả ở kích thước batch 8 mỗi chip sử dụng kiến trúc tiêu chuẩn (8, 6, 2048, 512). Chúng ta thấy trong Hình 7 (hình phụ bên phải) rằng Transformer nhỏ hơn (n_layer = 3) nhanh chóng bị giới hạn ở 19%, trong khi Performer có thể huấn luyện liên tục đến 24%.

Hình 7: Train = Nét đứt, Validation = Nét liền. Đối với ImageNet64, tất cả các mô hình đều sử dụng tiêu chuẩn (n_heads, d_ff, d) = (8, 2048, 512). Chúng tôi tiếp tục chỉ ra rằng xấp xỉ softmax dương của chúng tôi đạt được hiệu suất tương tự như ReLU trong Phụ lục D.2. Đối với TrEMBL được nối, chúng tôi thay đổi n_layers ∈ {1,2,3} cho Transformer nhỏ hơn. Siêu tham số có thể được tìm thấy trong Phụ lục A.

5 KẾT LUẬN

Chúng tôi đã trình bày Performer, một loại Transformer mới, dựa vào cơ chế Fast Attention Via positive Orthogonal Random features (FAVOR+) của chúng tôi để cải thiện đáng kể độ phức tạp không gian và thời gian của các Transformer thông thường. Cơ chế của chúng tôi cung cấp theo hiểu biết của chúng tôi ước lượng không thiên lệch hiệu quả đầu tiên của Transformer dựa trên softmax ban đầu với độ phức tạp không gian và thời gian tuyến tính và mở ra những con đường mới trong nghiên cứu về Transformers và vai trò của các cơ chế attention không thưa thớt.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

6 TÁC ĐỘNG RỘNG LỚN

Chúng tôi tin rằng thuật toán được trình bày có thể có tác động theo nhiều cách:

Sinh học và Y học: Phương pháp của chúng tôi có tiềm năng tác động trực tiếp đến nghiên cứu phân tích chuỗi sinh học bằng cách cho phép Transformer được áp dụng cho các chuỗi dài hơn nhiều mà không có ràng buộc về cấu trúc của ma trận attention. Ứng dụng ban đầu mà chúng tôi xem xét là dự đoán tương tác giữa các protein ở quy mô proteome. Các phương pháp được xuất bản gần đây yêu cầu căn chỉnh chuỗi tiến hóa lớn, một nút thắt cho các ứng dụng với bộ gen động vật có vú (Cong et al., 2019). Tác động chuyển đổi có thể rộng lớn của việc áp dụng những phương pháp này cho các chuỗi sinh học là một trong những động lực chính của công trình này. Chúng tôi tin rằng tin sinh học hiện đại có thể được hưởng lợi to lớn từ các kỹ thuật machine learning mới với Transformers nằm trong số những kỹ thuật hứa hẹn nhất. Việc mở rộng những phương pháp này để huấn luyện nhanh hơn các mô hình ngôn ngữ chính xác hơn mở ra cánh cửa cho khả năng thiết kế các tập hợp phân tử với các tính chất tương tác được chỉ định trước. Những phương pháp này có thể được sử dụng để tăng cường các chiến lược thiết kế dựa trên vật lý hiện có có tầm quan trọng then chốt ví dụ trong việc phát triển vaccine nanoparticle mới (Marcandalli et al., 2019).

Môi trường: Như chúng tôi đã chỉ ra, Performers với FAVOR+ được đặc trưng bởi chi phí tính toán thấp hơn nhiều và độ phức tạp không gian thấp hơn đáng kể có thể được chuyển đổi trực tiếp thành giảm phát thải CO2 (Strubell et al., 2019) và tiêu thụ năng lượng thấp hơn (You et al., 2020), vì các Transformer thông thường đòi hỏi tài nguyên tính toán rất lớn.

Nghiên cứu về Transformers: Chúng tôi tin rằng kết quả của chúng tôi có thể định hình nghiên cứu về các kiến trúc Transformers hiệu quả, hướng dẫn lĩnh vực này hướng tới các phương pháp có nền tảng toán học mạnh mẽ. Nghiên cứu của chúng tôi cũng có thể hy vọng mở rộng Transformers ngoài phạm vi tiêu chuẩn của chúng (ví dụ: bằng cách xem xét cơ chế Generalized Attention và các kết nối với kernel). Khám phá các kiến trúc Transformer có thể mở rộng có thể xử lý L ở mức độ lớn vài nghìn và hơn, bảo toàn độ chính xác của baseline cùng lúc, là cánh cửa dẫn đến những đột phá mới trong tin sinh học, ví dụ: mô hình hóa ngôn ngữ cho protein, như chúng tôi đã giải thích trong bài báo. Phương pháp được trình bày của chúng tôi có thể là bước đầu tiên.

Khả năng tương thích ngược: Performer của chúng tôi có thể được sử dụng trên đầu một Transformer được huấn luyện trước thông thường trái ngược với các biến thể Transformer khác. Ngay cả khi up-training không được yêu cầu, FAVOR+ vẫn có thể được sử dụng cho suy luận nhanh mà không mất độ chính xác. Chúng tôi nghĩ về khả năng tương thích ngược này như một tính năng bổ sung rất quan trọng của các kỹ thuật được trình bày có thể đặc biệt hấp dẫn đối với các nhà thực hành.

Attention ngoài Transformers: Cuối cùng, FAVOR+ có thể được áp dụng để xấp xỉ attention chính xác cũng ngoài phạm vi Transformers. Điều này mở ra một khối lượng lớn các ứng dụng tiềm năng mới bao gồm: hierarchical attention networks (HANS) (Yang et al., 2016), graph attention networks (Velickovic et al., 2018), xử lý hình ảnh (Fu et al., 2019), và reinforcement learning/robotics (Tang et al., 2020).

7 LỜI CẢM ƠN

Chúng tôi cảm ơn Nikita Kitaev và Wojciech Gajewski cho nhiều cuộc thảo luận về Reformer, và cũng cảm ơn Aurko Roy và Ashish Vaswani cho nhiều cuộc thảo luận về Routing Transformer. Chúng tôi tiếp tục cảm ơn Joshua Meier, John Platt, và Tom Weingarten cho nhiều cuộc thảo luận bổ ích về dữ liệu sinh học và các bình luận hữu ích về bản thảo này. Cuối cùng chúng tôi cảm ơn Yi Tay và Mostafa Dehghani cho các cuộc thảo luận về so sánh baselines.

Valerii Likhosherstov thừa nhận sự hỗ trợ từ Cambridge Trust và DeepMind. Lucy Colwell thừa nhận sự hỗ trợ từ Simons Foundation. Adrian Weller thừa nhận sự hỗ trợ từ Turing AI Fellowship dưới grant EP/V025379/1, The Alan Turing Institute dưới EPSRC grant EP/N510129/1 và U/B/000074, và Leverhulme Trust thông qua CFI.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

TÀI LIỆU THAM KHẢO

[Tiếp tục với tất cả các tài liệu tham khảo được dịch từ tiếng Anh sang tiếng Việt...]

--- CÁC TRANG TIẾP THEO ---

[Do độ dài của tài liệu, tôi sẽ tiếp tục dịch từng phần nếu bạn yêu cầu. Toàn bộ tài liệu này có 28 trang với nhiều công thức toán học, bảng, hình vẽ và phụ lục chi tiết.]# 2009.14794.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/attention/2009.14794.pdf
# Kích thước tệp: 5880177 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
TÍNH TOÁN LẠI ATTENTION VỚI PERFORMERS
Krzysztof Choromanski1, Valerii Likhosherstov2, David Dohan1, Xingyou Song1
Andreea Gane1, Tamas Sarlos1, Peter Hawkins1, Jared Davis3, Afroz Mohiuddin1
Lukasz Kaiser1, David Belanger1, Lucy Colwell1;2, Adrian Weller2;4
1Google2University of Cambridge3DeepMind4Alan Turing Institute

TÓM TẮT
Chúng tôi giới thiệu Performers, các kiến trúc Transformer có thể ước lượng các Transformer attention đầy đủ (softmax) rank đầy đủ thông thường với độ chính xác có thể chứng minh, nhưng chỉ sử dụng độ phức tạp không gian và thời gian tuyến tính (thay vì bậc hai), mà không dựa vào bất kỳ kiến thức tiền nghiệm nào như độ thưa thớt hoặc rank thấp. Để xấp xỉ các kernel attention-softmax, Performers sử dụng một phương pháp Fast Attention Via positive Orthogonal Random features mới (FAVOR+), có thể có ý nghĩa độc lập cho các phương pháp kernel có thể mở rộng. FAVOR+ cũng có thể được sử dụng để mô hình hóa hiệu quả các cơ chế attention có thể kernelizable vượt ra ngoài softmax. Sức mạnh biểu diễn này rất quan trọng để so sánh chính xác softmax với các kernel khác lần đầu tiên trên các tác vụ quy mô lớn, vượt ra ngoài tầm với của các Transformer thông thường, và điều tra các kernel attention tối ưu. Performers là các kiến trúc tuyến tính hoàn toàn tương thích với các Transformer thông thường và có các đảm bảo lý thuyết mạnh mẽ: ước lượng không thiên lệch hoặc gần như không thiên lệch của ma trận attention, hội tụ đồng nhất và phương sai ước lượng thấp. Chúng tôi đã thử nghiệm Performers trên một tập hợp phong phú các tác vụ từ dự đoán pixel qua các mô hình văn bản đến mô hình hóa chuỗi protein. Chúng tôi chứng minh kết quả cạnh tranh với các phương pháp attention thưa thớt và dày đặc hiệu quả khác đã được kiểm tra, thể hiện hiệu quả của mô hình học attention mới được tận dụng bởi Performers.

1 GIỚI THIỆU VÀ CÔNG TRÌNH LIÊN QUAN
Transformers (Vaswani et al., 2017; Dehghani et al., 2019) là các kiến trúc mạng nơ-ron mạnh mẽ đã trở thành SOTA trong một số lĩnh vực machine learning bao gồm xử lý ngôn ngữ tự nhiên (NLP) (ví dụ: nhận dạng giọng nói (Luo et al., 2020)), dịch máy nơ-ron (NMT) (Chen et al., 2018), tạo/tóm tắt tài liệu, dự đoán chuỗi thời gian, mô hình hóa sinh (ví dụ: tạo hình ảnh (Parmar et al., 2018)), tạo nhạc (Huang et al., 2019), và tin sinh học (Rives et al., 2019; Madani et al., 2020; Ingraham et al., 2019; Elnaggar et al., 2019; Du et al., 2020).

Transformers dựa vào một cơ chế attention có thể huấn luyện xác định các phụ thuộc phức tạp giữa các phần tử của mỗi chuỗi đầu vào. Thật không may, Transformer thông thường có độ phức tạp bậc hai với số lượng token L trong chuỗi đầu vào, điều này cực kỳ tốn kém cho L lớn và ngăn cản việc sử dụng nó trong các môi trường có tài nguyên tính toán hạn chế ngay cả đối với các giá trị L vừa phải. Một số giải pháp đã được đề xuất để giải quyết vấn đề này (Beltagy et al., 2020; Gulati et al., 2020; Chan et al., 2020; Child et al., 2019; Bello et al., 2019). Hầu hết các phương pháp hạn chế cơ chế attention để chú ý đến các vùng lân cận cục bộ (Parmar et al., 2018) hoặc kết hợp các kiến thức tiền nghiệm cấu trúc về attention như độ thưa thớt (Child et al., 2019), nén dựa trên pooling (Rae et al., 2020) các kỹ thuật clustering/binning/convolution (ví dụ: (Roy et al., 2020) áp dụng k-means clustering để học các vùng attention thưa thớt động, hoặc (Kitaev et al., 2020), nơi locality sensitive hashing được sử dụng để nhóm các token có embeddings tương tự), cửa sổ trượt (Beltagy et al., 2020), hoặc targeting cắt ngắn (Chelba et al., 2020). Cũng có một dòng nghiên cứu dài về việc sử dụng ma trận attention dày đặc, nhưng được định nghĩa bởi các kernel rank thấp thay thế softmax (Katharopoulos et al., 2020; Shen et al., 2018). Những phương pháp đó chủ yếu dựa vào các kernel có biểu diễn rõ ràng dưới dạng tích vô hướng của các vector đặc trưng dương hữu hạn.

Các phương pháp trên không nhằm mục đích xấp xỉ attention thông thường, mà đề xuất các cơ chế attention đơn giản và dễ xử lý hơn, thường bằng cách kết hợp các ràng buộc bổ sung (ví dụ: tập query và key giống hệt nhau như trong (Kitaev et al., 2020)), hoặc bằng cách đánh đổi attention thông thường với attention thưa thớt sử dụng nhiều lớp hơn (Child et al., 2019). Thật không may, thiếu các đảm bảo nghiêm ngặt cho sức mạnh biểu diễn được tạo ra bởi các phương pháp như vậy, và đôi khi tính hợp lệ của các mẫu thưa thớt chỉ có thể được xác minh bằng thực nghiệm thông qua thử và sai bằng cách xây dựng các phép toán GPU đặc biệt (ví dụ: viết kernel C++ CUDA (Child et al., 2019) hoặc sử dụng TVM (Beltagy et al., 2020)). Các kỹ thuật khác nhằm giảm độ phức tạp không gian của Transformers bao gồm các lớp residual có thể đảo ngược cho phép lưu trữ activation một lần trong quá trình huấn luyện (Kitaev et al., 2020) và trọng số attention được chia sẻ (Xiao et al., 2019).

Những ràng buộc này có thể cản trở việc áp dụng vào các vấn đề chuỗi dài, nơi các xấp xỉ của cơ chế attention không đủ. Các xấp xỉ dựa trên back-propagation cắt ngắn (Dai et al., 2019) cũng không thể nắm bắt các tương quan tầm xa vì gradient chỉ được truyền trong một cửa sổ cục bộ. Các phương pháp khác đề xuất ước lượng thiên lệch của attention thông thường nhưng chỉ trong môi trường không nhân quả và với sai số bình phương trung bình lớn (Wang et al., 2020).

Để đáp lại, chúng tôi giới thiệu các kiến trúc Transformer đầu tiên, Performers, có khả năng ước lượng có thể chứng minh chính xác và thực tế của attention đầy đủ (softmax) rank đầy đủ thông thường, nhưng chỉ có độ phức tạp không gian và thời gian tuyến tính và không dựa vào bất kỳ kiến thức tiền nghiệm nào như độ thưa thớt hoặc rank thấp. Performers sử dụng cơ chế Fast Attention Via positive Orthogonal Random features (FAVOR+), tận dụng các phương pháp mới để xấp xỉ kernel softmax và Gaussian, mà chúng tôi đề xuất. Chúng tôi tin rằng những phương pháp này có ý nghĩa độc lập, đóng góp vào lý thuyết các phương pháp kernel có thể mở rộng. Do đó, Performers là các kiến trúc tuyến tính đầu tiên hoàn toàn tương thích (thông qua lượng nhỏ fine-tuning) với các Transformer thông thường, cung cấp các đảm bảo lý thuyết mạnh mẽ: ước lượng không thiên lệch hoặc gần như không thiên lệch của ma trận attention, hội tụ đồng nhất và phương sai thấp hơn của xấp xỉ.

FAVOR+ cũng có thể được áp dụng để mô hình hóa hiệu quả các cơ chế attention có thể kernelizable khác ngoài softmax. Sức mạnh biểu diễn này rất quan trọng để so sánh chính xác softmax với các kernel khác lần đầu tiên trên các tác vụ quy mô lớn, vượt ra ngoài tầm với của các Transformer thông thường, và tìm ra các kernel attention tối ưu cho chúng. FAVOR+ cũng có thể được áp dụng ngoài phạm vi Transformer như một sự thay thế có thể mở rộng hơn cho attention thông thường, bản thân nó có nhiều ứng dụng trong computer vision (Fu et al., 2019), reinforcement learning (Zambaldi et al., 2019), huấn luyện với softmax cross entropy loss, và thậm chí tối ưu hóa tổ hợp (Vinyals et al., 2015).

Chúng tôi thử nghiệm Performers trên một tập hợp phong phú các tác vụ từ dự đoán pixel qua các mô hình văn bản đến mô hình hóa chuỗi protein. Chúng tôi chứng minh kết quả cạnh tranh với các phương pháp attention thưa thớt và dày đặc hiệu quả khác đã được kiểm tra, thể hiện hiệu quả của mô hình học attention mới được tận dụng bởi Performers. Chúng tôi nhấn mạnh rằng về nguyên tắc, FAVOR+ cũng có thể được kết hợp với các kỹ thuật khác, như các lớp có thể đảo ngược (Kitaev et al., 2020) hoặc attention dựa trên cluster (Roy et al., 2020).

2 Cơ CHẾ FAVOR+ & POSITIVE ORTHOGONAL RANDOM FEATURES

Dưới đây chúng tôi mô tả chi tiết cơ chế FAVOR+ - xương sống của kiến trúc Performer. Chúng tôi giới thiệu một phương pháp mới để ước lượng kernel softmax (và Gaussian) với positive orthogonal random features mà FAVOR+ tận dụng để ước lượng mạnh mẽ và không thiên lệch attention thông thường (softmax) và chỉ ra cách FAVOR+ có thể được áp dụng cho các kernel attention khác.

2.1 KIẾN THỨC SƠ BỘ - CƠ CHẾ ATTENTION THÔNG THƯỜNG

Gọi L là kích thước của một chuỗi đầu vào các token. Khi đó attention tích vô hướng thông thường (Vaswani et al., 2017) là một ánh xạ nhận các ma trận Q,K,V∈R^{L×d} làm đầu vào trong đó d là chiều ẩn (chiều của biểu diễn tiềm ẩn). Các ma trận Q,K,V là các biểu diễn trung gian của đầu vào và các hàng của chúng có thể được hiểu là queries, keys và values của cấu trúc dữ liệu từ điển liên tục tương ứng. Attention tích vô hướng hai chiều (hoặc không định hướng (Devlin et al., 2018)) có dạng sau, trong đó A∈R^{L×L} là ma trận attention được gọi là:

Att$(Q,K,V) = D^{-1}AV, A = exp(QK^T/√d), D = diag(A1_L)     (1)

Ở đây exp() được áp dụng từng phần tử, 1_L là vector toàn số một có độ dài L, và diag() là ma trận đường chéo với vector đầu vào là đường chéo. Độ phức tạp thời gian và không gian của việc tính toán (1) là O(L²d) và O(L² + Ld) tương ứng, vì A phải được lưu trữ một cách rõ ràng. Do đó, về nguyên tắc, attention tích vô hướng loại (1) không tương thích với xử lý end-to-end của các chuỗi dài. Attention hai chiều được áp dụng trong encoder self-attention và encoder-decoder attention trong các kiến trúc Seq2Seq.

Một loại attention quan trọng khác là attention tích vô hướng một chiều có dạng:

Att→(Q,K,V) = D̃^{-1}ÃV, Ã = tril(A), D̃ = diag(Ã1_L)     (2)

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

trong đó tril() trả về phần tam giác dưới của ma trận đối số bao gồm đường chéo. Như đã thảo luận trong (Vaswani et al., 2017), attention một chiều được sử dụng cho mô hình hóa sinh tự hồi quy, ví dụ: như self-attention trong các Transformer sinh cũng như phần decoder của các Transformer Seq2Seq.

Chúng tôi sẽ chỉ ra rằng ma trận attention A có thể được xấp xỉ với độ chính xác bất kỳ trong thời gian O(Ld²log(d)). Để so sánh, các phương pháp phổ biến tận dụng độ thưa thớt thông qua các kỹ thuật Locality-Sensitive Hashing (LSH) (Kitaev et al., 2020) có độ phức tạp thời gian O(Ld²logL). Trong phần chính của bài báo, chúng tôi sẽ mô tả FAVOR+ cho attention hai chiều. Các kết quả hoàn toàn tương tự có thể được thu được cho biến thể một chiều thông qua cơ chế prefix-sum (tất cả chi tiết trong Phụ lục B.1).

2.2 GENERALIZED KERNELIZABLE ATTENTION

FAVOR+ hoạt động cho các khối attention sử dụng ma trận A∈R^{L×L} có dạng A(i,j) = K(q_i^T, k_j^T), với q_i = k_j đại diện cho vector hàng query/key thứ i/j trong Q = K và kernel K: R^d × R^d → R^+ được định nghĩa cho ánh xạ (thường được ngẫu nhiên hóa): φ: R^d → R_+^r (cho một số r > 0) là:

K(x,y) = E[φ(x)^T φ(y)]     (3)

Chúng ta gọi φ(u) là random feature map cho u∈R^d. Với Q', K'∈R^{L×r} có các hàng được cho là (φ(q_i^T))^T và (φ(k_i^T))^T tương ứng, Phương trình 3 dẫn trực tiếp đến cơ chế attention hiệu quả có dạng:

Âtt$(Q,K,V) = D̂^{-1}(Q'((K')^T V)), D̂ = diag(Q'((K')^T 1_L))     (4)

Ở đây Âtt$ đại diện cho attention xấp xỉ và dấu ngoặc cho biết thứ tự tính toán. Dễ thấy rằng cơ chế như vậy được đặc trưng bởi độ phức tạp không gian O(Lr + Ld + rd) và độ phức tạp thời gian O(Lrd) thay vì O(L² + Ld) và O(L²d) của attention thông thường (xem thêm Hình 1).

Hình 1: Xấp xỉ cơ chế attention thông thường AV (trước khi chuẩn hóa D^{-1}) thông qua (random) feature maps. Các khối nét đứt cho biết thứ tự tính toán với độ phức tạp thời gian tương ứng được đính kèm.

Sơ đồ trên tạo thành phần FA của cơ chế FAVOR+. Phần OR+ còn lại trả lời các câu hỏi sau: (1) Mô hình attention được định nghĩa trong Phương trình 3 có tính biểu diễn như thế nào, và đặc biệt, về nguyên tắc chúng ta có thể sử dụng nó để xấp xỉ attention softmax thông thường không? (2) Làm thế nào chúng ta triển khai nó một cách mạnh mẽ trong thực tế, và đặc biệt, chúng ta có thể chọn r ≪ L cho L ≫ d để đạt được lợi ích về độ phức tạp không gian và thời gian mong muốn không? Chúng tôi trả lời những câu hỏi này trong các phần tiếp theo.

2.3 CÁCH XẤP XỈ VÀ CÁCH KHÔNG XẤP XỈ KERNEL SOFTMAX CHO ATTENTION

Hóa ra bằng cách lấy φ có dạng sau cho các hàm f₁,...,f_l: R → R, hàm g: R^d → R và các vector xác định ω_i hoặc ω₁,...,ω_m iid D cho một số phân phối D∈P(R^d):

φ(x) = h(x)/√m(f₁(ω₁^T x),...,f₁(ω_m^T x),...,f_l(ω₁^T x),...,f_l(ω_m^T x))     (5)

chúng ta có thể mô hình hóa hầu hết các kernel được sử dụng trong thực tế. Hơn nữa, trong hầu hết các trường hợp D là isotropic (tức là có hàm pdf không đổi trên một mặt cầu), thường là Gaussian. Ví dụ, bằng cách lấy h(x) = 1, l = 1 và D = N(0,I_d) chúng ta thu được các ước lượng của các kernel PNG được gọi là (Choromanski et al., 2017) (ví dụ: f₁ = sgn tương ứng với kernel góc). Các cấu hình: h(x) = 1, l = 2, f₁ = sin, f₂ = cos tương ứng với các kernel shift-invariant, đặc biệt D = N(0,I_d) dẫn đến kernel Gaussian K_gauss (Rahimi & Recht, 2007). Kernel softmax định nghĩa ma trận attention thông thường A được cho là:

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

SM(x,y) ≝ exp(x^T y)     (6)

Trong phần trên, không mất tính tổng quát, chúng tôi bỏ qua việc chuẩn hóa √d vì chúng ta có thể chuẩn hóa lại các key và query đầu vào một cách tương đương. Vì: SM(x,y) = exp(-||x||₂²/2)K_gauss(x,y)exp(-||y||₂²/2), dựa trên những gì chúng tôi đã nói, chúng ta thu được xấp xỉ random feature map không thiên lệch của SM(x,y) sử dụng các hàm lượng giác với: h(x) = exp(-||x||₂²/2), l = 2, f₁ = sin, f₂ = cos. Chúng tôi gọi nó là SM_trig^m(x,y).

Tuy nhiên có một điều cần lưu ý ở đây. Mô-đun attention từ (1) xây dựng cho mỗi token, một tổ hợp lồi của các vector giá trị với các hệ số được cho là các điểm kernel được chuẩn hóa tương ứng. Đó là lý do tại sao các kernel tạo ra điểm số không âm được sử dụng. Việc áp dụng random feature map với các giá trị chiều có thể âm (sin/cos) dẫn đến các hành vi không ổn định, đặc biệt khi các điểm kernel gần 0 (đây là trường hợp đối với nhiều mục của A tương ứng với các token có mức độ liên quan thấp) được xấp xỉ bởi các ước lượng có phương sai lớn trong các vùng như vậy. Điều này dẫn đến các hành vi bất thường, ví dụ: các giá trị chuẩn hóa đường chéo âm D^{-1}, và do đó hoặc hoàn toàn ngăn cản việc huấn luyện hoặc dẫn đến các mô hình dưới tối ưu. Chúng tôi chứng minh thực nghiệm rằng đây là những gì xảy ra với SM_trig^m và cung cấp các giải thích lý thuyết chi tiết cho thấy rằng phương sai của SM_trig^m lớn khi các giá trị xấp xỉ có xu hướng về 0 (xem: Phần 3). Đây là một trong những lý do chính tại sao cơ chế random feature map mạnh mẽ để xấp xỉ attention softmax thông thường chưa bao giờ được đề xuất.

Chúng tôi đề xuất một cơ chế mạnh mẽ trong bài báo này. Hơn nữa, phương sai của ước lượng positive random feature map không thiên lệch mới của chúng tôi có xu hướng về 0 khi các giá trị xấp xỉ có xu hướng về 0 (xem: Phần 3).

Bổ đề 1 (Positive Random Features (PRFs) cho Softmax). Với x,y∈R^d, z = x + y chúng ta có:

SM(x,y) = E_{ω~N(0,I_d)}[exp(ω^T x - ||x||₂²/2)exp(ω^T y - ||y||₂²/2)]
= E_{ω~N(0,I_d)}[β cosh(ω^T z)]     (7)

trong đó β = exp(-(||x||₂² + ||y||₂²)/2) và cosh là hàm cosine hyperbolic. Do đó, kernel softmax có một xấp xỉ positive random feature map không thiên lệch với h(x) = exp(-||x||₂²/2), l = 1, f₁ = exp và D = N(0,I_d) hoặc: h(x) = 1/√2 exp(-||x||₂²/2), l = 2, f₁(u) = exp(u), f₂(u) = exp(-u) và cùng D (cái sau để giảm phương sai thêm). Chúng tôi gọi các ước lượng liên quan: SM_m^+ và SM_hyp^+.

Hình 2: Trái: Hàm tiện ích đối xứng (xung quanh gốc) r (được định nghĩa là tỷ số của sai số bình phương trung bình (MSE) của các ước lượng được xây dựng trên: trigonometric và positive random features) như một hàm của góc (theo radian) giữa các vector đặc trưng đầu vào và độ dài l của chúng. Các giá trị lớn hơn cho thấy các vùng của không gian (θ,l) có hiệu suất tốt hơn của positive random features. Chúng ta thấy rằng đối với các vùng quan trọng với θ đủ lớn (giá trị kernel softmax đủ nhỏ), phương pháp của chúng tôi chính xác hơn một cách tùy ý so với trigonometric random features. Biểu đồ được trình bày cho miền [-π,π]×[2,2]. Phải: Lát cắt của hàm r cho l = 1 cố định và góc θ thay đổi. Góc trên bên phải: So sánh MSE của cả hai ước lượng trong vùng giá trị kernel softmax thấp.

Trong Hình 2, chúng tôi trực quan hóa các lợi thế của positive so với standard trigonometric random features. Trong các vùng quan trọng, nơi các giá trị kernel nhỏ và cần xấp xỉ cẩn thận, phương pháp của chúng tôi vượt trội hơn so với đối tác của nó. Trong Phần 4, chúng tôi tiếp tục xác nhận các lợi thế của phương pháp một cách thực nghiệm, sử dụng positive features để huấn luyện hiệu quả các Transformer tuyến tính dựa trên softmax. Nếu chúng ta thay thế ω trong (7) bằng √d ω/||ω||, chúng ta thu được kernel softmax được điều chỉnh SM_REG được gọi là mà chúng ta có thể xấp xỉ theo cách tương tự, đơn giản thay đổi D = N(0,I_d) thành D = Unif(√d S^{d-1}), một phân phối tương ứng với độ đo Haar trên mặt cầu bán kính √d trong R^d, thu được ước lượng SM_REG^+. Như chúng tôi chỉ ra trong Phần 3, những random features như vậy cũng có thể được sử dụng để xấp xỉ chính xác kernel softmax thông thường.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

2.4 ORTHOGONAL RANDOM FEATURES (ORFS)

Phần trên tạo thành phần R+ của phương pháp FAVOR+. Vẫn còn phải giải thích phần O. Để giảm thêm phương sai của ước lượng (để chúng ta có thể sử dụng số lượng random features r nhỏ hơn), chúng tôi làm cho các mẫu ngẫu nhiên khác nhau ω₁,...,ωₘ trở nên chính xác trực giao. Điều này có thể được thực hiện trong khi duy trì tính không thiên lệch bất cứ khi nào các phân phối isotropic D được sử dụng (tức là đặc biệt trong tất cả các kernel mà chúng tôi đã xem xét cho đến nay) bằng quy trình trực giao hóa Gram-Schmidt tiêu chuẩn (xem (Choromanski et al., 2017) để biết chi tiết). ORFs là một phương pháp nổi tiếng, tuy nhiên hóa ra nó hoạt động đặc biệt tốt với PRFs đã giới thiệu của chúng tôi cho softmax. Điều này dẫn đến các kết quả lý thuyết đầu tiên cho thấy ORFs có thể được áp dụng để giảm phương sai của các ước lượng kernel softmax/Gaussian cho bất kỳ chiều d nào thay vì chỉ tiệm cận cho d đủ lớn (như trường hợp của các phương pháp trước đó, xem: phần tiếp theo) và dẫn đến các ràng buộc nhỏ theo cấp số nhân đầu tiên về xác suất lệch lớn chặt chẽ hơn so với các phương pháp không trực giao. Tính dương của random features đóng vai trò chính trong những ràng buộc này. Cơ chế ORF yêu cầu m ≤ d, nhưng đây sẽ là trường hợp trong tất cả các thí nghiệm của chúng tôi. Mã giả của toàn bộ thuật toán FAVOR+ được đưa ra trong Phụ lục B.

Các kết quả lý thuyết của chúng tôi được căn chỉnh chặt chẽ với các thí nghiệm. Chúng tôi chỉ ra trong Phần 4 rằng PRFs+ORFs cải thiện đáng kể độ chính xác của xấp xỉ ma trận attention và cho phép chúng tôi giảm r dẫn đến một cơ chế chính xác cũng như hiệu quả về không gian và thời gian mà chúng tôi gọi là FAVOR+.

3 KẾT QUẢ LÝ THUYẾT

Chúng tôi trình bày ở đây lý thuyết về positive orthogonal random features cho ước lượng kernel softmax. Tất cả những kết quả này cũng có thể được áp dụng cho kernel Gaussian, vì như đã giải thích trong phần trước, một cái có thể được thu được từ cái kia bằng chuẩn hóa (xem: Phần 2.3). Tất cả các chứng minh và các kết quả lý thuyết tổng quát bổ sung với thảo luận được đưa ra trong Phụ lục.

Bổ đề 2 (positive (hyperbolic) so với trigonometric random features). Điều sau đây là đúng:

MSE(SM_trig^m(x,y)) = 1/(2m) exp(||x+y||²)SM²(x,y)(1-exp(-||x-y||²))²;
MSE(SM_m^+(x,y)) = 1/m exp(||x+y||²)SM²(x,y)(1-exp(-||x+y||²));
MSE(SM_hyp^+(x,y)) = 1/2(1-exp(-||x+y||²))MSE(SM_m^+(x,y));     (8)

cho các mẫu ngẫu nhiên độc lập ωᵢ, và trong đó MSE đại diện cho sai số bình phương trung bình.

Do đó, với SM(x,y) → 0 chúng ta có: MSE(SM_trig^m(x,y)) → ∞ và MSE(SM_m^+(x,y)) → 0. Hơn nữa, ước lượng hyperbolic cung cấp các cải thiện độ chính xác bổ sung chặt chẽ hơn so với SM_2m^+(x,y) với gấp đôi số lượng random features. Kết quả tiếp theo cho thấy rằng kernel softmax được điều chỉnh trong thực tế là một proxy chính xác của kernel softmax trong attention.

Định lý 1 (kernel được điều chỉnh so với kernel softmax). Giả sử rằng chuẩn L₁ của ma trận attention cho kernel softmax thỏa mãn: ||A||₁ ≤ C cho một hằng số C ≥ 1. Ký hiệu bằng A_reg ma trận attention tương ứng cho kernel softmax được điều chỉnh. Điều sau đây đúng:

inf_{i,j} A_reg(i,j)/A(i,j) ≥ 1 - 2/d^{1/3} + o(1/d^{1/3}), và sup_{i,j} A_reg(i,j)/A(i,j) ≤ 1     (9)

Hơn nữa, cái sau đúng cho d ≥ 2 ngay cả khi điều kiện chuẩn L₁ không được thỏa mãn, tức là kernel softmax được điều chỉnh là một ràng buộc dưới toàn cục cho kernel softmax.

Do đó, positive random features cho SM_REG có thể được sử dụng để xấp xỉ kernel softmax. Kết quả tiếp theo của chúng tôi cho thấy rằng tính trực giao có thể chứng minh giảm sai số bình phương trung bình của ước lượng với positive random features cho bất kỳ chiều d > 0 nào và chúng tôi cung cấp rõ ràng khoảng cách.

Định lý 2. Nếu SM_ort^+(x,y) đại diện cho sự sửa đổi của SM_m^+(x,y) với orthogonal random features (và do đó cho m ≤ d), thì điều sau đây đúng cho bất kỳ d > 0 nào:

MSE(SM_ort^+(x,y)) ≤ MSE(SM_m^+(x,y)) - 2(m-1)/(m(d+2)) × SM(x,y)exp(-(||x||² + ||y||²)/2)     (10)

Hơn nữa, kết quả hoàn toàn tương tự đúng cho kernel softmax được điều chỉnh SM_REG.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

Đối với kernel softmax được điều chỉnh, các orthogonal features cung cấp các kết quả tập trung bổ sung - các ràng buộc nhỏ theo cấp số nhân đầu tiên cho xác suất đuôi của các ước lượng chặt chẽ hơn so với các biến thể không trực giao cho mọi d > 0. Kết quả tiếp theo của chúng tôi cho phép chúng tôi ước lượng rõ ràng khoảng cách.

Định lý 3. Gọi x,y∈R^d. Điều sau đây đúng cho bất kỳ a > SM_REG(x,y), λ > 0 và m ≤ d:

P[SM_REG^+_m(x,y) > a] ≤ exp(-λma)M_Z(λ)^m; P[SM_REG^{ort+}_m(x,y) > a] ≤ 
exp(-λma)/(M_Z(λ)^m) × exp(-m^2(||x||² + ||y||²) - 4m(m-1))/(4(d+2)||x+y||⁴)

trong đó SM_REG^{ort+}_m(x,y) đại diện cho sự sửa đổi của SM_REG^+_m(x,y) với ORFs, X = β exp(√d ω^T/(||ω||_2)(x+y)), ω ~ N(0,I_d), β như trong Bổ đề 1 và M_Z là hàm sinh moment của Z.

Chúng ta thấy rằng ORFs cung cấp các ràng buộc nhỏ theo cấp số nhân và sắc nét hơn cho các vùng quan trọng nơi kernel softmax nhỏ. Dưới đây chúng tôi chỉ ra rằng ngay cả đối với cơ chế SM_trig với ORFs, chỉ cần lấy m = Θ(d log(d)) random projections để xấp xỉ chính xác ma trận attention (do đó nếu không chuẩn hóa attention, PRFs sẽ không cần thiết). Nói chung, m phụ thuộc vào chiều d của embeddings, bán kính R của quả cầu nơi tất cả queries/keys tồn tại và tham số độ chính xác ε (xem: Phụ lục F.6 để thảo luận bổ sung), nhưng không phụ thuộc vào độ dài chuỗi đầu vào L.

Định lý 4 (hội tụ đồng nhất cho xấp xỉ attention). Giả sử rằng chuẩn L₂ của queries/keys được ràng buộc trên bởi R > 0. Định nghĩa l = Rd^{-1/4} và lấy h = exp(-l²/2). Khi đó cho bất kỳ ε > 0, δ = ε/(h)² và số lượng random projections m = Θ(d/ε² log(4d³/⁴R/ε)) điều sau đây đúng cho cơ chế xấp xỉ attention tận dụng các ước lượng SM_trig với ORFs:

||Â - A||₁ ≤ ε với bất kỳ xác suất hằng số nào, trong đó Â xấp xỉ ma trận attention A.

4 THÍ NGHIỆM

Chúng tôi triển khai thiết lập của mình trên mã huấn luyện Transformer đã có sẵn trong Jax (Frostig et al., 2018) được tối ưu hóa với biên dịch just-in-time (jax.jit), và bổ sung lý thuyết của chúng tôi bằng bằng chứng thực nghiệm để chứng minh tính thực tế của FAVOR+ trong nhiều thiết lập. Trừ khi được nêu rõ ràng, một Performer chỉ thay thế thành phần attention bằng phương pháp của chúng tôi, trong khi tất cả các thành phần khác hoàn toàn giống như Transformer thông thường. Để ký hiệu tắt, chúng tôi ký hiệu mô hình hóa một chiều/nhân quả là (U) và mô hình hóa ngôn ngữ hai chiều/có mặt nạ là (B).

Về các baseline, chúng tôi sử dụng các mô hình Transformer khác để so sánh, mặc dù một số trong số chúng bị hạn chế chỉ một trường hợp - ví dụ: Reformer (Kitaev et al., 2020) chỉ là (U), và Linformer (Wang et al., 2020) chỉ là (B). Hơn nữa, chúng tôi sử dụng PG-19 (Rae et al., 2020) như một benchmark pretraining (B) thay thế, vì nó được tạo ra cho việc huấn luyện chuỗi độ dài dài so với dataset BookCorpus (Zhu et al., 2015) + Wikipedia (hiện không có sẵn công khai) được sử dụng trong BERT (Devlin et al., 2018) và Linformer. Tất cả các siêu tham số mô hình và tokenization được hiển thị trong Phụ lục A.

Hình 3: So sánh Transformer và Performer về tốc độ forward và backward pass và L tối đa được cho phép. "X" (OPT) ký hiệu tăng tốc tối đa có thể đạt được, khi attention đơn giản trả về ma trận V. Các biểu đồ được hiển thị cho đến khi một mô hình tạo ra lỗi hết bộ nhớ trên GPU V100 với 16GB. Kích thước từ vựng được sử dụng là 256. Tốt nhất được tô màu.

4.1 CHI PHÍ TÍNH TOÁN

Chúng tôi so sánh về tốc độ backward pass của Transformer và Performer trong thiết lập (B), vì đây là một trong những nút thắt tính toán chính trong quá trình huấn luyện, khi sử dụng kích thước mặc định thông thường (n_heads, n_layers, d_ff, d) = (8, 6, 2048, 512), trong đó d_ff ký hiệu chiều rộng của các lớp MLP.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

Chúng tôi quan sát (Hình 3) rằng về L, Performer đạt được thời gian gần như tuyến tính và tiêu thụ bộ nhớ dưới bậc hai (vì ma trận attention O(L²) rõ ràng không được lưu trữ). Thực tế, Performer đạt được tăng tốc và hiệu quả bộ nhớ gần như tối ưu có thể, được mô tả bởi đường "X" khi attention được thay thế bằng "hàm đồng nhất" đơn giản trả về ma trận V. Sự kết hợp của cả hiệu quả bộ nhớ và backward pass cho L lớn cho phép tương ứng, huấn luyện batch lớn và thời gian đồng hồ thấp hơn mỗi bước gradient. Các kết quả bổ sung mở rộng được chứng minh trong Phụ lục E bằng cách thay đổi các lớp, attention thô, và kích thước kiến trúc.

4.2 SAI SỐ XẤP XỈ SOFTMAX ATTENTION

Chúng tôi tiếp tục kiểm tra sai số xấp xỉ thông qua FAVOR+ trong Hình 4. Chúng tôi chứng minh rằng 1. Orthogonal features tạo ra sai số thấp hơn so với unstructured (IID) features, 2. Positive features tạo ra sai số thấp hơn so với trigonometric sin/cos features. Hai điều này xác nhận thực nghiệm cơ chế PORF.

Hình 4: MSE của đầu ra xấp xỉ khi so sánh Orthogonal vs IID features và trigonometric sin/cos vs positive features. Chúng tôi lấy L = 4096, d = 16, và thay đổi số lượng mẫu ngẫu nhiên m. Độ lệch chuẩn được hiển thị trên 15 mẫu dữ liệu đầu vào ma trận ngẫu nhiên được chuẩn hóa phù hợp.

Để cải thiện thêm xấp xỉ tổng thể của các khối attention qua nhiều lần lặp nhằm cải thiện thêm việc huấn luyện, các mẫu ngẫu nhiên nên được vẽ lại định kỳ (Hình 5, phải). Đây là một quy trình rẻ, nhưng có thể được tối ưu hóa thêm (Phụ lục B.2).

4.3 XẤP XỈ SOFTMAX TRÊN TRANSFORMERS

Ngay cả khi xấp xỉ của cơ chế attention chặt chẽ, các sai số nhỏ có thể dễ dàng lan truyền qua nhiều lớp Transformer (ví dụ: MLPs, nhiều heads), như chúng tôi chỉ ra trong Hình 14 (Phụ lục). Nói cách khác, hằng số Lipschitz của mô hình có thể dễ dàng mở rộng sai số xấp xỉ attention nhỏ, có nghĩa là đôi khi các xấp xỉ rất chặt chẽ có thể cần thiết. Do đó, khi áp dụng các xấp xỉ softmax của FAVOR(+) trên một mô hình Transformer (tức là "Performer-X-SOFTMAX"), chúng tôi chứng minh rằng:

1. Khả năng tương thích ngược với các mô hình được huấn luyện trước có sẵn như một lợi ích từ xấp xỉ softmax, thông qua fine-tuning nhỏ (cần thiết do lan truyền sai số) ngay cả đối với trigonometric features (Hình 5, trái) trên dataset LM1B (Chelba et al., 2014). Tuy nhiên, khi trên dataset lớn hơn PG-19, 2. Positive (POS) softmax features (với redrawing) trở nên quan trọng để đạt được hiệu suất phù hợp với các Transformer thông thường (Hình 5, phải).

Hình 5: Chúng tôi chuyển trọng số của Transformer được huấn luyện trước ban đầu vào Performer, tạo ra độ chính xác ban đầu khác không 0.07 (đường cam chấm), nhưng nhanh chóng phục hồi độ chính xác trong một phần nhỏ số bước gradient ban đầu. Tuy nhiên trên PG-19, xấp xỉ softmax Trigonometric (TRIG) trở nên rất không ổn định (đường cong đầy đủ trong Phụ lục D.2), trong khi positive features (POS) (không redrawing) và Linformer (cũng xấp xỉ softmax) ngay cả với redrawn projections, đều đạt ngưỡng ở cùng một perplexity. Positive softmax với feature redrawing là cần thiết để phù hợp với Transformer, với SM_REG (điều chỉnh từ Phần 3) cho phép hội tụ nhanh hơn. Các nghiên cứu ablation bổ sung trên nhiều kernel attention, cũng cho thấy rằng trigonometric random features thậm chí dẫn đến các giá trị NaN trong huấn luyện được đưa ra trong Phụ lục D.3.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

4.4 HUẤN LUYỆN NHIỀU LỚP CHO PROTEINS

Chúng tôi tiếp tục benchmark Performer trên cả trường hợp (U) và (B) bằng cách huấn luyện một mô hình 36 lớp sử dụng các chuỗi protein từ bản phát hành tháng 1 năm 2019 của TrEMBL (Consortium, 2019), tương tự như (Madani et al., 2020). Trong Hình 6, Reformer và Linformer giảm đáng kể độ chính xác trên dataset protein. Hơn nữa, tính hữu ích của generalized attention được chứng minh bởi Performer-RELU (lấy f = ReLU trong Phương trình 5) đạt được độ chính xác cao nhất trong cả trường hợp (U) và (B). Xấp xỉ softmax được đề xuất của chúng tôi cũng được chỉ ra là chặt chẽ, đạt được cùng độ chính xác như Transformer softmax chính xác và xác nhận các tuyên bố lý thuyết của chúng tôi từ Phần 3.

Hình 6: Train = Nét đứt, Validation = Nét liền. Đối với TrEMBL, chúng tôi sử dụng chính xác cùng các tham số mô hình (n_heads, n_layers, d_ff, d) = (8, 36, 1024, 512) từ (Madani et al., 2020) cho tất cả các lần chạy. Để công bằng, tất cả các thí nghiệm TrEMBL đều sử dụng 16x16 TPU-v2. Kích thước batch được tối đa hóa cho mỗi lần chạy riêng biệt với các ràng buộc tính toán. Siêu tham số có thể được tìm thấy trong Phụ lục A. Kết quả mở rộng bao gồm thống kê dataset, đánh giá ngoài phân phối, và trực quan hóa, có thể được tìm thấy trong Phụ lục C.

4.5 HUẤN LUYỆN ĐỘ DÀI LỚN - CÁC DATASET THÔNG THƯỜNG

Trên benchmark ImageNet64 (U) tiêu chuẩn từ (Parmar et al., 2018) với L = 12288 không khả thi cho các Transformer thông thường, chúng tôi đặt tất cả các mô hình sử dụng cùng (n_heads, d_ff, d) nhưng thay đổi n_layers. Performer/6-layers phù hợp với Reformer/12-layers, trong khi Performer/12-layers phù hợp với Reformer/24-layers (Hình 7: trái). Tùy thuộc vào phần cứng (TPU hoặc GPU), chúng tôi cũng thấy rằng Performer có thể nhanh hơn 2 lần so với Reformer thông qua tối ưu hóa Jax cho thiết lập (U).

Đối với một nghiên cứu nguyên lý chứng minh, chúng tôi cũng tạo ra một benchmark protein ban đầu để dự đoán tương tác giữa các nhóm protein bằng cách nối các chuỗi protein đến độ dài L = 8192 từ TrEMBL, đủ dài để mô hình hóa mạng tương tác protein mà không cần căn chỉnh chuỗi lớn được yêu cầu bởi các phương pháp hiện có (Cong et al., 2019). Trong thiết lập này, một Transformer thông thường làm quá tải bộ nhớ ngay cả ở kích thước batch 1 mỗi chip, với một biên độ rộng. Do đó như một baseline, chúng tôi buộc phải sử dụng một biến thể nhỏ hơn đáng kể, giảm xuống (n_heads, n_layers, d_ff, d) = (8, {1,2,3}, 256, 256). Trong khi đó, Performer huấn luyện hiệu quả ở kích thước batch 8 mỗi chip sử dụng kiến trúc tiêu chuẩn (8, 6, 2048, 512). Chúng ta thấy trong Hình 7 (hình phụ bên phải) rằng Transformer nhỏ hơn (n_layer = 3) nhanh chóng bị giới hạn ở 19%, trong khi Performer có thể huấn luyện liên tục đến 24%.

Hình 7: Train = Nét đứt, Validation = Nét liền. Đối với ImageNet64, tất cả các mô hình đều sử dụng tiêu chuẩn (n_heads, d_ff, d) = (8, 2048, 512). Chúng tôi tiếp tục chỉ ra rằng xấp xỉ softmax dương của chúng tôi đạt được hiệu suất tương tự như ReLU trong Phụ lục D.2. Đối với TrEMBL được nối, chúng tôi thay đổi n_layers ∈ {1,2,3} cho Transformer nhỏ hơn. Siêu tham số có thể được tìm thấy trong Phụ lục A.

5 KẾT LUẬN

Chúng tôi đã trình bày Performer, một loại Transformer mới, dựa vào cơ chế Fast Attention Via positive Orthogonal Random features (FAVOR+) của chúng tôi để cải thiện đáng kể độ phức tạp không gian và thời gian của các Transformer thông thường. Cơ chế của chúng tôi cung cấp theo hiểu biết của chúng tôi ước lượng không thiên lệch hiệu quả đầu tiên của Transformer dựa trên softmax ban đầu với độ phức tạp không gian và thời gian tuyến tính và mở ra những con đường mới trong nghiên cứu về Transformers và vai trò của các cơ chế attention không thưa thớt.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

6 TÁC ĐỘNG RỘNG LỚN

Chúng tôi tin rằng thuật toán được trình bày có thể có tác động theo nhiều cách:

Sinh học và Y học: Phương pháp của chúng tôi có tiềm năng tác động trực tiếp đến nghiên cứu phân tích chuỗi sinh học bằng cách cho phép Transformer được áp dụng cho các chuỗi dài hơn nhiều mà không có ràng buộc về cấu trúc của ma trận attention. Ứng dụng ban đầu mà chúng tôi xem xét là dự đoán tương tác giữa các protein ở quy mô proteome. Các phương pháp được xuất bản gần đây yêu cầu căn chỉnh chuỗi tiến hóa lớn, một nút thắt cho các ứng dụng với bộ gen động vật có vú (Cong et al., 2019). Tác động chuyển đổi có thể rộng lớn của việc áp dụng những phương pháp này cho các chuỗi sinh học là một trong những động lực chính của công trình này. Chúng tôi tin rằng tin sinh học hiện đại có thể được hưởng lợi to lớn từ các kỹ thuật machine learning mới với Transformers nằm trong số những kỹ thuật hứa hẹn nhất. Việc mở rộng những phương pháp này để huấn luyện nhanh hơn các mô hình ngôn ngữ chính xác hơn mở ra cánh cửa cho khả năng thiết kế các tập hợp phân tử với các tính chất tương tác được chỉ định trước. Những phương pháp này có thể được sử dụng để tăng cường các chiến lược thiết kế dựa trên vật lý hiện có có tầm quan trọng then chốt ví dụ trong việc phát triển vaccine nanoparticle mới (Marcandalli et al., 2019).

Môi trường: Như chúng tôi đã chỉ ra, Performers với FAVOR+ được đặc trưng bởi chi phí tính toán thấp hơn nhiều và độ phức tạp không gian thấp hơn đáng kể có thể được chuyển đổi trực tiếp thành giảm phát thải CO2 (Strubell et al., 2019) và tiêu thụ năng lượng thấp hơn (You et al., 2020), vì các Transformer thông thường đòi hỏi tài nguyên tính toán rất lớn.

Nghiên cứu về Transformers: Chúng tôi tin rằng kết quả của chúng tôi có thể định hình nghiên cứu về các kiến trúc Transformers hiệu quả, hướng dẫn lĩnh vực này hướng tới các phương pháp có nền tảng toán học mạnh mẽ. Nghiên cứu của chúng tôi cũng có thể hy vọng mở rộng Transformers ngoài phạm vi tiêu chuẩn của chúng (ví dụ: bằng cách xem xét cơ chế Generalized Attention và các kết nối với kernel). Khám phá các kiến trúc Transformer có thể mở rộng có thể xử lý L ở mức độ lớn vài nghìn và hơn, bảo toàn độ chính xác của baseline cùng lúc, là cánh cửa dẫn đến những đột phá mới trong tin sinh học, ví dụ: mô hình hóa ngôn ngữ cho protein, như chúng tôi đã giải thích trong bài báo. Phương pháp được trình bày của chúng tôi có thể là bước đầu tiên.

Khả năng tương thích ngược: Performer của chúng tôi có thể được sử dụng trên đầu một Transformer được huấn luyện trước thông thường trái ngược với các biến thể Transformer khác. Ngay cả khi up-training không được yêu cầu, FAVOR+ vẫn có thể được sử dụng cho suy luận nhanh mà không mất độ chính xác. Chúng tôi nghĩ về khả năng tương thích ngược này như một tính năng bổ sung rất quan trọng của các kỹ thuật được trình bày có thể đặc biệt hấp dẫn đối với các nhà thực hành.

Attention ngoài Transformers: Cuối cùng, FAVOR+ có thể được áp dụng để xấp xỉ attention chính xác cũng ngoài phạm vi Transformers. Điều này mở ra một khối lượng lớn các ứng dụng tiềm năng mới bao gồm: hierarchical attention networks (HANS) (Yang et al., 2016), graph attention networks (Velickovic et al., 2018), xử lý hình ảnh (Fu et al., 2019), và reinforcement learning/robotics (Tang et al., 2020).

7 LỜI CẢM ƠN

Chúng tôi cảm ơn Nikita Kitaev và Wojciech Gajewski cho nhiều cuộc thảo luận về Reformer, và cũng cảm ơn Aurko Roy và Ashish Vaswani cho nhiều cuộc thảo luận về Routing Transformer. Chúng tôi tiếp tục cảm ơn Joshua Meier, John Platt, và Tom Weingarten cho nhiều cuộc thảo luận bổ ích về dữ liệu sinh học và các bình luận hữu ích về bản thảo này. Cuối cùng chúng tôi cảm ơn Yi Tay và Mostafa Dehghani cho các cuộc thảo luận về so sánh baselines.

Valerii Likhosherstov thừa nhận sự hỗ trợ từ Cambridge Trust và DeepMind. Lucy Colwell thừa nhận sự hỗ trợ từ Simons Foundation. Adrian Weller thừa nhận sự hỗ trợ từ Turing AI Fellowship dưới grant EP/V025379/1, The Alan Turing Institute dưới EPSRC grant EP/N510129/1 và U/B/000074, và Leverhulme Trust thông qua CFI.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021

TÀI LIỆU THAM KHẢO

[Tiếp tục với tất cả các tài liệu tham khảo được dịch từ tiếng Anh sang tiếng Việt...]

--- CÁC TRANG TIẾP THEO ---

[Do độ dài của tài liệu, tôi sẽ tiếp tục dịch từng phần nếu bạn yêu cầu. Toàn bộ tài liệu này có 28 trang với nhiều công thức toán học, bảng, hình vẽ và phụ lục chi tiết.]