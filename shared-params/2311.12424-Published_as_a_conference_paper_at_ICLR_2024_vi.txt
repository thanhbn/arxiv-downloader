# 2311.12424.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/shared-params/2311.12424.pdf
# Kích thước tệp: 2824503 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
TRANSFORMER VÒNG LẶP TỐT HƠN TRONG VIỆC HỌC
CÁC THUẬT TOÁN HỌC TẬP
Liu Yang, Kangwook Lee, Robert D. Nowak & Dimitris Papailiopoulos
Đại học Wisconsin, Madison, USA
{liu.yang, kangwook.lee, rdnowak }@wisc.edu, dimitris@papail.io
TÓM TẮT
Transformer đã chứng minh hiệu quả trong việc giải quyết các bài toán khớp dữ liệu trong ngữ cảnh từ các mô hình (tiềm ẩn) khác nhau, như được báo cáo bởi Garg và cộng sự (2022). Tuy nhiên, việc thiếu cấu trúc lặp lại tự nhiên trong kiến trúc transformer tạo ra thách thức trong việc mô phỏng các thuật toán lặp, vốn thường được sử dụng trong các phương pháp học máy truyền thống. Để giải quyết vấn đề này, chúng tôi đề xuất việc sử dụng kiến trúc transformer vòng lặp và phương pháp huấn luyện tương ứng, với mục tiêu tích hợp các đặc tính lặp vào kiến trúc transformer. Kết quả thực nghiệm cho thấy transformer vòng lặp đạt được hiệu suất tương đương với transformer tiêu chuẩn trong việc giải quyết các bài toán khớp dữ liệu khác nhau, trong khi chỉ sử dụng ít hơn 10% số tham số.¹

1 GIỚI THIỆU
Transformer (Vaswani và cộng sự, 2017; Brown và cộng sự, 2020; Devlin và cộng sự, 2019) đã nổi lên như mô hình được ưa chuộng trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) và các lĩnh vực khác đòi hỏi mô hình hóa chuỗi-sang-chuỗi. Bên cạnh hiệu suất tối tân trong các nhiệm vụ xử lý ngôn ngữ tự nhiên, các mô hình ngôn ngữ lớn (LLM) như GPT-3 (Brown và cộng sự, 2020) và PaLM (Chowdhery và cộng sự, 2022) cũng thể hiện khả năng học trong ngữ cảnh: chúng có thể thích ứng với các nhiệm vụ hạ lưu khác nhau dựa trên một gợi ý ngắn, do đó bỏ qua nhu cầu tinh chỉnh mô hình bổ sung. Khả năng thú vị này của học trong ngữ cảnh đã thu hút sự quan tâm của cộng đồng nghiên cứu, dẫn đến nhiều nghiên cứu (Min và cộng sự, 2022; Olsson và cộng sự, 2022; Li và cộng sự, 2023). Tuy nhiên, các cơ chế cơ bản cho phép những transformer này thực hiện học trong ngữ cảnh vẫn chưa rõ ràng.

Trong nỗ lực hiểu hành vi học trong ngữ cảnh của LLM, Garg và cộng sự (2022) đã điều tra hiệu suất của transformer, khi được huấn luyện từ đầu, trong việc giải quyết các bài toán học lớp hàm cụ thể trong ngữ cảnh. Đáng chú ý, transformer thể hiện hiệu suất mạnh trên tất cả các nhiệm vụ, bằng hoặc thậm chí vượt qua các bộ giải truyền thống. Dựa trên điều này, Akyürek và cộng sự (2022) đã khám phá khả năng của mô hình dựa trên transformer trong việc giải quyết bài toán học hồi quy tuyến tính, diễn giải nó như một dạng ngầm định của các thuật toán học đã được thiết lập. Nghiên cứu của họ bao gồm cả góc độ lý thuyết và thực nghiệm để hiểu cách transformer học các hàm này. Tiếp theo, von Oswald và cộng sự (2022) đã chứng minh thực nghiệm rằng, khi được huấn luyện để dự đoán đầu ra hàm tuyến tính, một transformer chỉ có tự chú ý tuyến tính tự nhiên học thực hiện một bước của giảm gradient để giải quyết nhiệm vụ hồi quy tuyến tính trong ngữ cảnh. Trong khi cách tiếp cận và lý thuyết nền tảng được trình bày bởi von Oswald và cộng sự (2022) có triển vọng, tồn tại khoảng cách đáng kể giữa kiến trúc đơn giản hóa mà họ kiểm tra và transformer giải mã tiêu chuẩn được sử dụng trong thực tế. Thách thức huấn luyện một transformer giải mã tiêu chuẩn từ đầu, chỉ với những sửa đổi kiến trúc nhỏ, để nhân bản hiệu quả thuật toán học vẫn là một câu hỏi mở.

Trong học máy truyền thống, các thuật toán lặp thường được sử dụng để giải quyết hồi quy tuyến tính. Tuy nhiên, các phương pháp được sử dụng bởi transformer tiêu chuẩn không được cấu trúc tự nhiên cho tính toán lặp. Kiến trúc transformer vòng lặp, được nghiên cứu rộng rãi trong tài liệu như Giannou và cộng sự (2023), cung cấp một hướng đi hứa hẹn để thu hẹp khoảng cách này. Ngoài lợi thế tự nhiên trong việc giải quyết vấn đề theo cách lặp, transformer vòng lặp cũng chia nhỏ nhiệm vụ thành các nhiệm vụ con đơn giản hơn, có thể dẫn đến tiết kiệm đáng kể trong tham số mô hình.

¹Mã nguồn của chúng tôi có tại https://github.com/Leiay/looped_transformer .

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Để minh họa cách chia nhỏ nhiệm vụ dẫn đến tiết kiệm tham số, hãy xem xét việc sử dụng mô hình transformer để giải quyết nhiệm vụ hồi quy tuyến tính, cụ thể là min_w ∥Xw−y∥²₂ (Hình 1). Để huấn luyện transformer trên nhiệm vụ này, chúng ta nhập một chuỗi gợi ý được định dạng như (x₁,w^T x₁, . . . ,x_k,w^T x_k,x_test). Ở đây w đại diện cho các tham số của mô hình hồi quy tuyến tính, {x₁,···,x_k} là k mẫu trong ngữ cảnh, và x_test là mẫu kiểm tra. Transformer có thể cố gắng dự đoán y_test bằng cách xấp xỉ nghiệm bình phương tối thiểu thông thường trong một lần chuyển tiếp duy nhất. Việc tính toán nghịch đảo ma trận, như yêu cầu trong nghiệm bình phương tối thiểu thông thường (X^T X)^{-1}X^T y, khó khăn hơn cho transformer học so với phép nhân ma trận (Charton, 2021; von Oswald và cộng sự, 2022). Điều này được cho là do số lượng lớp và đầu tăng lên cần thiết trong phép nghịch đảo (Giannou và cộng sự, 2023). Tuy nhiên, giảm gradient cung cấp một giải pháp thay thế cho hồi quy tuyến tính, chỉ yêu cầu phép nhân ma trận: X^T(Xw−y), nhưng được áp dụng lặp lại.

Thiết lập Vấn đề: Giải quyết bài toán bình phương tối thiểu tuyến tính sử dụng Transformer chỉ giải mã (TF). Cho w∈R^d, {x_i}^k_{i=1}, mỗi x_i∈R^d, y_i=w^T x_i
nghiệm dạng đóng, ví dụ ŵ=(X^T X)^{-1}X^T y
gợi ý P=(x₁,y₁,x₂,y₂,...,x_k,y_k,x_test) đầu ra ŷ=(ŷ₁,ŷ₂,...,ŷ_k,ŷ_test)

Bài báo này: 
Transformer → Bộ giải TF đã học → Transformer vòng lặp
Bộ giải truyền thống: thuật toán lặp, ví dụ giảm gradient: w_{t+1}←w_t−η·(X^T Xw_t−X^T y)

Hình 1: Làm thế nào một transformer có thể được huấn luyện để học một thuật toán học lặp? Ở đây chúng ta xem xét nhiệm vụ huấn luyện transformer để giải quyết hồi quy tuyến tính trong ngữ cảnh. Gợi ý được cung cấp (x₁, y₁,x₂, y₂,· · ·,x_k, y_k,x_test) được đưa vào transformer giải mã. Mục tiêu là giảm tổn thất bình phương giữa ŷ_test được dự đoán dựa trên gợi ý này, và giá trị mục tiêu f(x_test). Garg và cộng sự (2022) đã chứng minh rằng transformer giải mã có thể học giải quyết hồi quy tuyến tính, có thể liên quan đến việc học xấp xỉ nghiệm bình phương tối thiểu. Trong nghiên cứu này, chúng tôi nhằm huấn luyện transformer để học các thuật toán học lặp. Mục tiêu của chúng tôi là đạt được hiệu suất ngang bằng với transformer tiêu chuẩn nhưng với ít tham số hơn. Để làm điều này, chúng tôi giới thiệu kiến trúc transformer vòng lặp và phương pháp huấn luyện đi kèm.

Với động lực này, chúng tôi đặt câu hỏi sau:
Liệu transformer vòng lặp có thể mô phỏng các thuật toán học lặp hiệu quả hơn so với transformer tiêu chuẩn, không đệ quy?

Trong các lớp hàm cụ thể mà chúng tôi kiểm tra, câu trả lời là tích cực. Những phát hiện sơ bộ của chúng tôi về việc sử dụng transformer vòng lặp để giải quyết nhiệm vụ hồi quy tuyến tính được minh họa trong Hình 2. Phần còn lại của bài báo được tổ chức như sau. Trong Mục 4, chúng tôi phát triển một phương pháp huấn luyện cho transformer vòng lặp để mô phỏng hiệu suất mong muốn của thuật toán lặp. Tiếp theo, trong Mục 5, chúng tôi so sánh hiệu suất thực nghiệm của transformer tiêu chuẩn và vòng lặp và phân tích sự cân bằng giữa chúng. Các đóng góp và phát hiện của chúng tôi được nêu dưới đây:

Phương pháp huấn luyện cho Transformer Vòng lặp. Chúng tôi đề xuất một phương pháp huấn luyện cho transformer vòng lặp, nhằm mô phỏng hiệu quả các thuật toán lặp. Giả định cho transformer vòng lặp mô phỏng một thuật toán hội tụ là khi số lần lặp vòng tăng lên, hiệu suất của transformer vòng lặp nên cải thiện hoặc hội tụ. Phù hợp với giả định này, chúng tôi đi sâu vào thiết kế cấu trúc của transformer vòng lặp, cũng như điều tra số lần lặp vòng cần thiết trong quá trình huấn luyện. Những điều tra này dẫn đến việc hình thành phương pháp huấn luyện của chúng tôi.

Hiệu suất của Transformer Vòng lặp cho học trong ngữ cảnh. Dựa trên thuật toán huấn luyện được đề xuất, bằng chứng thực nghiệm chứng minh rằng transformer vòng lặp có thể được huấn luyện từ đầu để học trong ngữ cảnh dữ liệu được tạo từ các hàm tuyến tính, hàm tuyến tính thưa, cây quyết định, và mạng nơ-ron 2 lớp. Trong số các lớp hàm khác nhau được kiểm tra, transformer vòng lặp liên tục vượt trội hơn transformer tiêu chuẩn, đặc biệt khi dữ liệu được tạo từ các hàm tuyến tính thưa hoặc cây quyết định. Những phát hiện của chúng tôi gợi ý khả năng transformer vòng lặp hiệu quả hơn trong việc mô phỏng các thuật toán học trong ngữ cảnh, cụ thể cho các nhiệm vụ học được khám phá trong bài báo này.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Đồ thị hiển thị hiệu suất của transformer vòng lặp qua các lần lặp và so sánh với các phương pháp khác]

Hình 2: Transformer vòng lặp có thể mô phỏng các thuật toán học lặp, cung cấp hiệu suất tương đương với transformer tiêu chuẩn với tham số giảm. Chúng tôi huấn luyện transformer vòng lặp để giải quyết hồi quy tuyến tính trong ngữ cảnh. (Trái): Trong khi được huấn luyện cho 30 lần lặp vòng, transformer vòng lặp trong quá trình suy luận đạt được nghiệm điểm cố định ổn định vượt quá các lần lặp vòng được huấn luyện. (Phải): Transformer vòng lặp được huấn luyện phù hợp với hiệu suất của transformer 12 lớp tiêu chuẩn và gần khớp với bộ giải bình phương tối thiểu, trong khi chỉ sử dụng 1/12 tham số của transformer.

2 CÁC CÔNG TRÌNH LIÊN QUAN

Mô hình Chia sẻ Trọng số. Chia sẻ trọng số là đặc tính cố hữu của các mô hình hồi quy như RNN (Bengio và cộng sự, 2003) và LSTM (Hochreiter & Schmidhuber, 1997). Những mô hình chuỗi này tái sử dụng trọng số theo hướng xử lý chuỗi, cho phép độ dài chuỗi đầu vào có thể vô hạn. Mô hình transformer (Vaswani và cộng sự, 2017; Devlin và cộng sự, 2019) thường có độ dài ngữ cảnh cố định. Để chứa ngữ cảnh dài hơn mà không tăng tham số, một số công trình (Dai và cộng sự, 2019; Wang và cộng sự, 2019; Hutchins và cộng sự, 2022) đã sử dụng transformer trong khung hồi quy để cho phép lần chuyển tiếp thích ứng (Dai và cộng sự, 2019) hoặc mở rộng độ dài ngữ cảnh (Hutchins và cộng sự, 2022). Hơn nữa, một số công trình (Lan và cộng sự, 2019; Jaegle và cộng sự, 2021; Dehghani và cộng sự, 2018) cũng đề xuất chia sẻ trọng số dọc theo lần chuyển tiếp của mô hình. Việc chia sẻ trọng số này có thể trên một số lượng hạn chế các lớp, hoặc với tiêu chí dừng để thoát khỏi đệ quy nếu cần. Ở một cực độ là mô hình ngầm định (Bai và cộng sự, 2019), nơi hàm được áp dụng lặp lại một số lần vô hạn. Các ứng dụng của những mô hình ngầm định này được đề cập trong đoạn tiếp theo.

Mô hình Ngầm định Sâu. Các Mô hình Ngầm định Sâu (Bai và cộng sự, 2018; 2019; 2020; Winston & Kolter, 2020; Bai và cộng sự, 2022) sử dụng các bộ giải hộp đen để tìm nghiệm điểm cố định cho các mô hình sâu ngầm định. Sau đó, Bai và cộng sự (2021) đề xuất chính quy hóa Jacobian để ổn định quá trình huấn luyện. Tuy nhiên, cách tiếp cận này đòi hỏi điều chỉnh siêu tham số rộng rãi và vẫn gặp thách thức về tính ổn định. Các mô hình ngầm định đã được chứng minh để giải quyết các bài toán toán học với khả năng ngoại suy (Decugis và cộng sự, 2022). Đây là một trường hợp đặc biệt của việc sử dụng mô hình hồi quy để giải quyết các nhiệm vụ toán học, như chúng ta sẽ kiểm tra kỹ hơn trong đoạn tiếp theo.

Sử dụng Transformer để Giải quyết Nhiệm vụ Toán học hoặc Lý luận. Một số công trình (Ongie và cộng sự, 2020; Doncevic và cộng sự, 2022; Zhang và cộng sự, 2022; Zhou và cộng sự, 2022b; Wei và cộng sự, 2022; Zhou và cộng sự, 2022a; Bansal và cộng sự, 2022; Charton, 2022; Goel và cộng sự, 2022; Zhang và cộng sự, 2023b; Dziri và cộng sự, 2023; Lee và cộng sự, 2023; Liu và cộng sự, 2022) đã khám phá khả năng của mạng nơ-ron sâu, bao gồm các mô hình hồi quy, trong việc giải quyết các nhiệm vụ toán học hoặc lý luận liên quan đến các quá trình lặp. Những nhiệm vụ này liên quan đến các trạng thái hoặc lựa chọn hữu hạn. Hơn nữa, Zhang và cộng sự (2023b) nghiên cứu cách transformer có thể mô phỏng bài toán đệ quy cấu trúc, trong khi Ongie và cộng sự (2020) điều tra việc sử dụng các mô hình đệ quy trong giải quyết các bài toán nghịch đảo trong hình ảnh. Mặt khác, bài toán hồi quy liên quan đến không gian đầu ra liên tục. Garg và cộng sự (2022) điều tra khả năng của transformer trong việc học các lớp hàm liên tục. Akyürek và cộng sự (2022) nghiên cứu chuyển pha của nghiệm thu được bởi transformer với độ sâu khác nhau khi được huấn luyện trên các bài toán hồi quy tuyến tính và hồi quy tuyến tính nhiễu. Raventos và cộng sự (2023) nghiên cứu mối quan hệ giữa số lượng nhiệm vụ gặp phải trong quá trình huấn luyện và hiệu suất của transformer học các bài toán hồi quy ridge trong ngữ cảnh. Tuy nhiên, công trình của chúng tôi sử dụng cùng kiến trúc như trong Garg và cộng sự (2022)–một mô hình giải mã với chú ý phi tuyến–và nhằm cung cấp bằng chứng thực nghiệm và hiểu biết về động lực học học tập của mô hình này.

Hiểu Cách Transformer Học Trong Ngữ cảnh để Giải quyết Bài toán Khớp Dữ liệu. Ngoài tiếp cận vấn đề này từ góc độ thực nghiệm, một số nghiên cứu đã kiểm tra nó từ góc độ lý thuyết, tập trung vào việc xây dựng transformer và mạng chú ý tuyến tính (loại trừ các đối tác softmax). Họ kết nối khả năng học của transformer cho nhiệm vụ hồi quy tuyến tính với các bản cập nhật giảm gradient ngầm định trên dự đoán gợi ý truy vấn (von Oswald và cộng sự, 2022; Ahn và cộng sự, 2023; Zhang và cộng sự, 2023a; Mahankali và cộng sự, 2023; Ding và cộng sự, 2023).

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

von Oswald và cộng sự (2022) đã chứng minh thực nghiệm rằng khi tối thiểu hóa khoảng cách ℓ2 giữa dự đoán và nhãn thực, một bước của giảm gradient là nghiệm tối ưu cho transformer tự chú ý tuyến tính (LSA) một lớp để học khi giải quyết bài toán hồi quy tuyến tính, trong khi Ahn và cộng sự (2023); Zhang và cộng sự (2023a; 2024) và Mahankali và cộng sự (2023) cung cấp giải thích lý thuyết cho điều này. Ngoài LSA 1 lớp, Fu và cộng sự (2023); Giannou và cộng sự (2024) đã chứng minh rằng transformer đa lớp có thể thực hiện tối ưu hóa bậc cao hơn. Hơn nữa, một hướng công trình khác (Muller và cộng sự, 2021; Xie và cộng sự, 2021; Akyürek và cộng sự, 2022; Bai và cộng sự, 2023) cung cấp hiểu biết lý thuyết về khả năng học trong ngữ cảnh của transformer thông qua lăng kính của bộ ước lượng Bayesian. Li và cộng sự (2023) nghiên cứu học trong ngữ cảnh từ góc độ lỗi tổng quát hóa. Lin & Lee (2024) điều tra các chế độ hoạt động kép của học trong ngữ cảnh.

3 THIẾT LẬP VẤN ĐỀ

Cho F biểu thị một lớp hàm được định nghĩa trên R^d. Cho D_F biểu thị một phân phối xác suất trên F và cho D_X biểu thị một phân phối xác suất trên R^d. Một gợi ý học ngẫu nhiên P được tạo như sau. Một hàm f được lấy mẫu từ D_F và các đầu vào {x₁,···,x_k} cũng như mẫu kiểm tra x_test được lấy mẫu từ D_X. Đầu ra của x_i được tính bởi f(x_i). Gợi ý sau đó là P = (x₁, f(x₁),···,x_k, f(x_k),x_test) và mục tiêu của hệ thống học là dự đoán f(x_test). Cho M là một hệ thống học và cho M(P) biểu thị dự đoán của nó (lưu ý nó không được cho f một cách rõ ràng). Hiệu suất của M được đo bằng lỗi bình phương ℓ(M(P), f(x_test)) = (M(P)−f(x_test))². Trong công trình này, chúng tôi tập trung vào các hệ thống học dựa trên transformer và so sánh chúng với các hệ thống học đã biết khác tùy thuộc vào nhiệm vụ. Cụ thể, chúng tôi kiểm tra mô hình giải mã GPT-2 (Radford và cộng sự, 2019) với L lớp. Theo mặc định, L = 12 cho transformer không vòng lặp, theo thiết lập của Garg và cộng sự (2022), và L = 1 cho transformer vòng lặp.

4 THUẬT TOÁN HUẤN LUYỆN CHO TRANSFORMER VÒNG LẶP

Trong phần này, chúng tôi đi sâu vào lựa chọn thiết kế cho transformer vòng lặp mô phỏng thuật toán. Đối với transformer vòng lặp mô phỏng thuật toán, chúng tôi dự kiến các đặc tính sau: 1) Khi các lần lặp vòng tiến triển, transformer vòng lặp nên duy trì hoặc cải thiện hiệu suất; 2) các lần lặp vòng có khả năng tiếp tục vô hạn mà không suy giảm hiệu suất.

Chiến lược Huấn luyện. Dựa trên thiết lập vấn đề trong Mục 3, cho gợi ý cho transformer là P = (x₁, f(x₁),···,x_k, f(x_k),x_test), với P_i biểu thị tiền tố gợi ý với i mẫu trong ngữ cảnh P_i = (x₁, f(x₁),···,x_i, f(x_i),x_{i+1}). Đầu ra của transformer vòng lặp sau t lần lặp vòng là

Y_t(P_i|θ) = M_θ(M_θ(···M_θ(Y_i^0+P_i) +P_i)···+P_i)
         t lần lặp

trong đó transformer M được tham số hóa bởi θ, và Y_i^0 là một tensor không với cùng hình dạng như P_i. Sau đó chúng tôi huấn luyện transformer bằng cách tối thiểu hóa tổn thất kỳ vọng sau:

min_θ E_P[1/(b-b₀) ∑_{t=b₀}^b 1/(k+1) ∑_{i=0}^k ℓ(Y_t(P_i|θ), f(x_{i+1}))]

trong đó chúng tôi chỉ đo tổn thất của transformer trên tất cả các tiền tố gợi ý với lần lặp vòng b₀ đến b, với b₀ = max(b−T,0). Tổn thất cắt ngắn này được lấy cảm hứng từ lan truyền ngược cắt ngắn qua thời gian (Hochreiter & Schmidhuber, 1997; Hinton & Sutskever, 2013) để tiết kiệm tính toán.

Cấu hình Mô hình và Số lượng Tham số. Để so sánh với transformer không vòng lặp, chúng tôi sử dụng transformer giống hệt với mô hình được mô tả trong Garg và cộng sự (2022), ngoại trừ số lượng lớp. Cụ thể, chúng tôi sử dụng mô hình GPT-2 với kích thước embedding D = 256 và h = 8 đầu chú ý. Transformer tiêu chuẩn (không vòng lặp) có L = 12 lớp, và transformer vòng lặp có L = 1 lớp. Về số lượng tham số, transformer không vòng lặp bao gồm 9.48M tham số, và transformer vòng lặp sử dụng 0.79M. Chúng tôi tuân theo cùng chiến lược huấn luyện: huấn luyện với bộ tối ưu hóa Adam, tốc độ học 0.0001, không suy giảm trọng số hoặc chính quy hóa rõ ràng khác (như cắt gradient, hoặc tăng cường dữ liệu).

Các Yếu tố Chính để Tìm Nghiệm Điểm Cố định. Khi triển khai huấn luyện kiến trúc vòng lặp này, hai yếu tố chính cần xem xét: 1) việc tiêm đầu vào trong kiến trúc vòng lặp (Mục 4.1), và 2) số lần lặp vòng tối đa trong quá trình huấn luyện (Mục 4.2). Các phần tiếp theo sẽ minh họa tác động của hai yếu tố này, sử dụng hồi quy tuyến tính làm nhiệm vụ cụ thể cho học trong ngữ cảnh.

4.1 TIÊM ĐẦU VÀO CHO TRANSFORMER VÒNG LẶP

Tái sử dụng một số ký hiệu, cho P đại diện cho đầu vào cho mô hình transformer M(·), và cho Y_t là đầu ra sau khi áp dụng M(·) cho t lần lặp. Trong dạng tổng quát, transformer vòng lặp có thể được biểu diễn như Y_{t+1} = M(Y_t;P), ∀t. Một số nghiên cứu (Lan và cộng sự, 2019; Dehghani và cộng sự, 2018) đã điều tra một biến thể cụ thể được gọi là dạng gắn trọng số: Y_{t+1} = M(Y_t) với Y₀ = P, và t < T < ∞ cho một hằng số T nào đó.

Tuy nhiên, khi t tiến đến vô cùng, ảnh hưởng của đầu vào ban đầu Y₀ giảm dần, và nghiệm trở nên cơ bản là ngẫu nhiên hoặc không thể dự đoán (Bai và cộng sự, 2019; Bansal và cộng sự, 2022). Để kết hợp đầu vào P vào nghiệm của transformer vòng lặp, chúng tôi đề xuất thiết lập Y_{t+1} = M(Y_t+P). Cần lưu ý rằng các phương pháp tiêm đầu vào không giới hạn chỉ ở phép cộng. Trong Hình 3, chúng tôi hiển thị kết quả khi huấn luyện transformer vòng lặp với (mặc định) tiêm đầu vào hoặc không có (gọi là gắn trọng số). Trong quá trình suy luận, transformer gắn trọng số sẽ nhanh chóng phân kỳ sau các lần lặp vòng được huấn luyện.

[HÌNH VẼ: Đồ thị so sánh hiệu suất với và không có tiêm đầu vào]

Hình 3: Lỗi kiểm tra cho bài toán hồi quy tuyến tính sử dụng transformer vòng lặp, so sánh các mô hình với tiêm đầu vào mặc định với những mô hình không có. Không có tiêm đầu vào, hiệu suất của transformer suy giảm vượt quá các lần lặp vòng được huấn luyện.

4.2 LỰA CHỌN SỐ LẦN LẶP VÒNG

Để đánh giá hiệu suất transformer vòng lặp tại lần lặp vòng thứ b, chúng ta phải thực hiện tính toán này b lần liên tiếp. Điều này có thể được so sánh với kiến trúc transformer có b lớp hiệu quả, mặc dù với trọng số được chia sẻ xuyên suốt các lớp này. Việc chọn giá trị b đòi hỏi sự cân bằng. Một b cao hơn dẫn đến thời gian huấn luyện và suy luận dài hơn, trong khi b thấp hơn hạn chế tiềm năng của mô hình. Hơn nữa, như được biểu thị trong Phương trình 1, tổn thất được cắt ngắn, chỉ có đầu ra của T lần lặp vòng đóng góp vào hàm tổn thất.

Tương tự, có sự cân bằng khi chọn T: một T nhỏ hơn dẫn đến sử dụng bộ nhớ giảm nhưng ảnh hưởng tiêu cực đến hiệu suất, trong khi T lớn hơn có thể gây ra gradient dao động, làm cho quá trình huấn luyện không ổn định. Phần này tập trung vào việc lựa chọn tối ưu các giá trị b và T (Hình 4) cho nhiệm vụ hồi quy tuyến tính. Chúng tôi phân tích tính phù hợp của các giá trị b trong tập {12,20,30,40,50}, và các giá trị T trong {5,10,15} cho hàm tuyến tính.

[HÌNH VẼ: Biểu đồ đánh giá với các giá trị b và T khác nhau]

Hình 4: Đánh giá transformer vòng lặp trên học trong ngữ cảnh các hàm tuyến tính với b và T khác nhau trong quá trình huấn luyện (b và T được định nghĩa trong Phương trình 1). Hình từ trái sang phải được huấn luyện với T = 5,10,15, và các màu khác nhau biểu thị các giá trị b khác nhau (được biểu thị trong chú thích). Các đường liền nét với nhiều màu khác nhau mô tả cách transformer vòng lặp, được huấn luyện với giá trị b cụ thể, hoạt động khi số lần lặp vòng tăng trong quá trình suy luận. Đường gạch ngang tương ứng biểu thị giá trị b.

Hình 4 gợi ý rằng giá trị b thấp hơn có thể khiến transformer vòng lặp phân kỳ sau các lần lặp vòng được huấn luyện, dẫn đến nghiệm điểm cố định kém bền vững hơn. Mặt khác, giá trị b cao hơn cho phép mô hình định vị nghiệm điểm cố định tránh được phân kỳ sau các lần lặp vòng được huấn luyện.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Tuy nhiên, vượt quá một giá trị b nhất định sẽ bắt đầu suy giảm tốc độ hội tụ của lần lặp vòng. Một sự cân bằng tương tự áp dụng cho việc lựa chọn T; một T thấp hơn có thể làm giảm hiệu suất, trong khi T lớn hơn có thể gây ra tính không ổn định trong huấn luyện, một hiện tượng được ghi nhận trong tài liệu huấn luyện mô hình hồi quy (Jaeger, 2005). Transformer vòng lặp với b = 12 phù hợp với độ sâu hiệu quả của transformer không vòng lặp được nghiên cứu trong Garg và cộng sự (2022). Tuy nhiên, nó không thể nhân bản hiệu suất của transformer tiêu chuẩn (không vòng lặp).

Một quan sát bổ sung là transformer vòng lặp luôn phát hiện nghiệm điểm cố định bão hòa trước lần lặp được huấn luyện b. Sự bão hòa này của nghiệm điểm cố định xảy ra do mục tiêu tổn thất, đòi hỏi transformer vòng lặp phù hợp với mục tiêu trong vòng b bước. Do đó, việc chọn giá trị b nhỏ hơn sẽ tăng tốc hội tụ đến nghiệm điểm cố định. Tuy nhiên, vượt quá một giá trị b nhất định, tốc độ hội tụ đạt bão hòa bất kể sự tăng trong b. Ví dụ, huấn luyện với T = 15, b = 40,50 cho tốc độ hội tụ tương tự.

Lựa chọn Tối ưu của b và T. Mục tiêu của chúng tôi là huấn luyện hiệu quả transformer vòng lặp cho nhiệm vụ trong phân phối. Điều này đòi hỏi xác định giá trị b tối thiểu ngăn chặn phân kỳ sau các lần lặp vòng huấn luyện. Để tối thiểu hóa sử dụng bộ nhớ, chúng tôi ưa thích giá trị T nhỏ hơn. Được hướng dẫn bởi những tiêu chí này, chúng tôi áp dụng b = 20 và T = 15 cho nhiệm vụ hồi quy tuyến tính.

5 KẾT QUẢ THỰC NGHIỆM

5.1 THIẾT LẬP THỰC NGHIỆM

Chúng tôi tập trung vào các bài toán khớp dữ liệu được tạo bởi mô hình tuyến tính với kích thước vấn đề d = 20, và mẫu trong ngữ cảnh k = 40. Các tham số được lấy mẫu từ N(0, I_d), và các mẫu trong ngữ cảnh x_i ∼ N(0, I_d) cũng vậy. Khi đo hiệu suất, chúng tôi đánh giá 1280 gợi ý và báo cáo khoảng tin cậy 90% trên 1000 thử nghiệm bootstrap. Để đảm bảo lỗi không thay đổi theo kích thước vấn đề, chúng tôi cũng thực hiện chuẩn hóa lỗi như được chỉ định trong Garg và cộng sự (2022).

Huấn luyện Có Lịch trình. Chúng tôi tuân theo chương trình huấn luyện về d và k, như được chỉ định trong Garg và cộng sự (2022). Bên cạnh đó, chúng tôi cũng thực hiện lịch trình trên tham số b, trong đó b được tăng dần trong quá trình huấn luyện. Kết quả là, cửa sổ tổn thất cắt ngắn bắt đầu tại [1, T] và sau đó chuyển theo thời gian đến [b−T, b]. Đáng chú ý rằng việc tối thiểu hóa lỗi bình phương trung bình giữa lần lặp vòng [1, T] và mục tiêu có thể khó khăn hơn so với [b−T, b]. Do đó, chúng tôi tránh gọi cách tiếp cận này là "học chương trình", vì thuật ngữ đó thường đề cập đến việc bắt đầu với nhiệm vụ ít khó khăn hơn và dần dần chuyển sang những nhiệm vụ thách thức hơn. Thay vào đó, chiến lược này có thể được xem như một phương pháp khởi động huấn luyện mô hình transformer trong cấu trúc vòng lặp. Nói chung, quyết định sử dụng hoặc không sử dụng lịch trình không ảnh hưởng đáng kể đến kết quả. Tuy nhiên, trong một số trường hợp, chúng tôi quan sát thấy kết quả tốt hơn đạt được bằng cách huấn luyện với lịch trình cho b. Chi tiết hơn có thể tìm thấy trong Phụ lục E.

5.2 TRANSFORMER VÒNG LẶP CÓ THỂ HỌC TRONG NGỮ CẢNH HỒI QUY TUYẾN TÍNH

Transformer Vòng lặp Phù hợp với Hiệu suất của Transformer Tiêu chuẩn khi Đánh giá Trong Phân phối. Như được chỉ ra trong Hình 2 (phải), transformer vòng lặp được huấn luyện với b = 20 và T = 15 có thể phù hợp với hiệu suất của transformer tiêu chuẩn, gần như phù hợp với hiệu suất của bộ giải tối ưu truyền thống: bình phương tối thiểu thông thường.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Ba biểu đồ so sánh hiệu suất của transformer trong các điều kiện khác nhau]

Hình 6: Đánh giá transformer được huấn luyện trên các hàm tuyến tính, được kiểm tra dưới các điều kiện khác nhau: a) đầu vào có hiệp phương sai lệch, b) hàm tuyến tính nhiễu, và c) đầu vào được thu phóng.

Transformer Vòng lặp Học Hiệu quả với Ít Gợi ý Riêng biệt Trong Phân phối Hơn. Chúng tôi đánh giá thêm độ phức tạp mẫu của huấn luyện transformer. Cụ thể, chúng tôi hỏi cần bao nhiêu gợi ý riêng biệt cần được nhìn thấy trong quá trình huấn luyện để transformer hoặc transformer vòng lặp học được hàm. Để làm điều này, thay vì tạo hàm tuyến tính một cách tức thời trong mỗi lần lặp, chúng tôi tạo tập dữ liệu huấn luyện trước khi huấn luyện. Do đó, trong quá trình huấn luyện, transformer có thể gặp cùng một gợi ý nhiều lần. Để tách biệt tác động của học chương trình, chúng tôi vô hiệu hóa chiến lược này và tập trung vào nhiệm vụ hồi quy tuyến tính với kích thước vấn đề d = 10, mẫu trong ngữ cảnh k = 20. Kết quả được trình bày trong Hình 5. Do ít tham số hơn trong transformer vòng lặp, nó có thể học hàm với ít gợi ý/hàm riêng biệt hơn so với transformer tiêu chuẩn. Kết quả thêm về các kích thước vấn đề khác nhau được trình bày trong Phụ lục C.

[HÌNH VẼ: Biểu đồ hiệu suất với số lượng gợi ý khác nhau]

Hình 5: Hiệu suất của transformer trên các hàm tuyến tính với d = 10 và k = 21, khi được huấn luyện với số lượng gợi ý/hàm riêng biệt khác nhau.

Transformer Vòng lặp Thể hiện Thiên hướng Quy nạp Nhất định khi Đánh giá Ngoài Phân phối. Nhớ lại rằng transformer được huấn luyện với x ∼ N(0, I_d). Bây giờ chúng tôi đánh giá nó trên: a) đầu vào có hiệp phương sai lệch, b) đầu vào có nhiễu, và c) đầu vào mẫu trong ngữ cảnh và đầu vào truy vấn nằm trong các góc phần tám khác nhau. Kết quả được hiển thị trong Hình 6. Thay vì thành thạo thuật toán thực được áp dụng cho bài toán hồi quy tuyến tính bất kể sự thay đổi trong phân phối đầu vào, transformer vòng lặp thể hiện thiên hướng quy nạp ưa chuộng các nghiệm đơn giản hơn khi so sánh với transformer không vòng lặp. Điều này dẫn đến việc xử lý tốt hơn (a) đầu vào hiệp phương sai lệch.

Trong nhiệm vụ (b) hồi quy tuyến tính nhiễu, transformer vòng lặp hiển thị đỉnh giảm trong đường cong giảm kép, tương tự như tác động của việc áp dụng chính quy hóa nhỏ cho mô hình cơ sở để giải quyết hồi quy tuyến tính nhiễu. Bhattamishra và cộng sự (2022) cũng báo cáo thiên hướng đơn giản của transformer ngẫu nhiên hướng đến độ nhạy thấp trong cơ sở hàm boolean. Tuy nhiên, thiên hướng quy nạp này hướng đến sự đơn giản có thể ảnh hưởng tiêu cực đến hiệu suất khi có sự thay đổi thu phóng trong phân phối đầu vào. Như được mô tả trong Hình 6 (c), trong quá trình suy luận, nếu chúng ta lấy mẫu x sao cho x = 3z, và z ∼ N(0, I), transformer vòng lặp hoạt động kém hơn so với transformer không vòng lặp. Kết quả ngoại suy thêm có thể tìm thấy trong Phụ lục D.

5.3 TÁC ĐỘNG CỦA CÁC BIẾN THỂ KIẾN TRÚC MÔ HÌNH LÊN HIỆU SUẤT TRANSFORMER VÒNG LẶP

Trong phần này, chúng tôi khám phá tác động của việc thay đổi số lượng lớp (L), đầu (h), và kích thước embedding (D) lên transformer vòng lặp. Những thực nghiệm này được huấn luyện với b = 30, T = 15.

Thay đổi Số lượng Lớp. Trong Hình 7 (trái), chúng tôi vẽ lỗi bình phương cho transformer với L lớp, và transformer vòng lặp với L lớp, áp dụng t lần lặp vòng. Từ hình, chúng tôi quan sát rằng khi L tăng, hội tụ đạt được nhanh hơn. Để phù hợp với hiệu suất của transformer tiêu chuẩn với L lớp, transformer vòng lặp cần nhiều hơn L lần lặp vòng, tức là độ sâu hiệu quả của transformer vòng lặp vượt quá transformer tiêu chuẩn. Ví dụ, transformer vòng lặp với một lớp đơn cần khoảng 20 lần lặp để đạt được hiệu suất của transformer 8 lớp. Điều này gợi ý rằng transformer và transformer vòng lặp học các biểu diễn khác nhau tại mỗi lớp (lần lặp).

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Hai biểu đồ so sánh hiệu suất với các cấu hình khác nhau]

Hình 7: Cho D là kích thước embedding của transformer, L là số lượng lớp, và h là số lượng đầu. Đối với các hàm tuyến tính với kích thước vấn đề d = 20, và mẫu trong ngữ cảnh k = 40, chúng tôi kiểm tra: (trái) transformer tiêu chuẩn (không vòng lặp) và transformer vòng lặp với D = 256 và h = 8, nhưng L thay đổi, và (phải) transformer vòng lặp với L, D, h thay đổi.

Để đi sâu hơn vào động lực học học tập của mỗi lớp trong cả transformer và transformer vòng lặp, chúng tôi sử dụng kỹ thuật thăm dò mô hình được đề xuất bởi Akyürek và cộng sự (2022); Alain & Bengio (2016). Tái sử dụng một số ký hiệu, chúng tôi biểu diễn đầu ra của lớp thứ t của transformer tiêu chuẩn hoặc t lần lặp vòng của transformer vòng lặp như Y_t. Một mô hình MLP được huấn luyện trên Y_t để học các đầu dò mục tiêu, cụ thể là X^T y cho thành phần gradient và w_ols cho nghiệm bình phương tối thiểu tối ưu. Lỗi bình phương trung bình cho thăm dó mô hình được minh họa trong Hình 8. Trong khi transformer tiêu chuẩn ban đầu nắm bắt các biểu diễn liên quan đến X^T y và w_ols, và sau đó chuyển tập trung sang các thống kê khác, transformer vòng lặp liên tục tinh chỉnh các biểu diễn của nó liên quan đến các đầu dò mục tiêu qua các lần lặp vòng. Chi tiết của thăm dò mô hình được trình bày trong Phụ lục F.

[HÌNH VẼ: Ba biểu đồ so sánh cách transformer và transformer vòng lặp mã hóa thông tin]

Hình 8: Transformer và transformer vòng lặp mã hóa thông tin qua các lớp/lần lặp như thế nào? Chúng tôi huấn luyện mô hình thăm dò MLP 2 lớp để khôi phục đầu dò mục tiêu từ đầu ra của transformer tại lớp/lần lặp vòng thứ t. Đường gạch ngang chỉ ra lỗi đầu dò tối thiểu thu được trong nhiệm vụ kiểm soát, nơi tham số hồi quy tuyến tính w được cố định là 1. Trái ngược với transformer tiêu chuẩn, giải mã đầu dò mục tiêu tối ưu khoảng lớp thứ 10, transformer vòng lặp dần dần tinh chỉnh biểu diễn của nó, cải thiện việc giải mã đầu dò mục tiêu với mỗi lần lặp vòng tiếp theo.

Thay đổi Số lượng Đầu và Kích thước Embedding. Hình 7 (phải) minh họa lỗi bình phương của transformer vòng lặp cho các kết hợp khác nhau của L, D, và h. Với L = 1 và h = 8 được giữ không đổi, việc tăng D dẫn đến hội tụ được cải thiện, bão hòa tại D = 256. Khi giữ D = 256 và h = 8 không đổi và thay đổi L, transformer vòng lặp cho thấy hiệu suất hội tụ tương tự. Sự khác biệt chính là trong tốc độ hội tụ. Điều chỉnh chỉ h với L và D cố định, lỗi của transformer vòng lặp giảm khi h tăng từ 2 đến 8. Tuy nhiên, lỗi tăng khi h tiến từ 8 đến 32. Ở đây, chúng tôi tuân theo triển khai tiêu chuẩn của GPT-2², nơi kích thước embedding mỗi đầu được định nghĩa là D/h. Định nghĩa này ngụ ý sự cân bằng trong transformer giữa số lượng đầu và kích thước embedding mỗi đầu khi h thay đổi.

5.4 CÁC LỚP HÀM PHỨC TẠP HƠN

Trong phần này, chúng tôi chuyển tập trung đến các lớp hàm phức tạp hơn: a) mô hình tuyến tính thưa với d = 20, k = 100, và mục nhập không thưa s = 3; b) cây quyết định với độ sâu = 4 và kích thước đầu vào d = 20; c) mạng nơ-ron ReLU 2 lớp với 100 nơ-ron ẩn và kích thước đầu vào d = 20. Đối với những hàm này, các tham số được lấy mẫu từ N(0, I_d), và các mẫu trong ngữ cảnh x_i ∼ N(0, I_d). Thiết lập này tuân theo phương pháp được mô tả trong Garg và cộng sự (2022), và được chi tiết thêm trong Phụ lục A. Chúng tôi cũng tiến hành thực nghiệm trên các tập dữ liệu OpenML, đại diện tốt hơn cho các tình huống thực tế. Chi tiết thiết lập thực nghiệm và kết quả cho các tập dữ liệu OpenML (Vanschoren và cộng sự, 2013) có sẵn trong Phụ lục G.

Số Lần lặp Vòng Tối ưu trong Transformer Vòng lặp cho Các Hàm Khác nhau. Trong Mục 4.2, chúng tôi nhấn mạnh rằng đối với transformer vòng lặp được huấn luyện trên nhiệm vụ hồi quy tuyến tính, giá trị b lớn hơn và T thích hợp tăng cường khả năng phát hiện nghiệm điểm cố định ổn định vượt quá các lần lặp vòng được huấn luyện. Quan sát này nhất quán qua các hàm khác nhau, nhưng các giá trị b và T tối ưu có thể thay đổi. Hình 9 mô tả hiệu suất của transformer vòng lặp được huấn luyện với T = 10 nhưng các giá trị b thay đổi trên các hàm khác nhau.

So sánh với Hình 4 cho thấy rằng các nhiệm vụ có độ phức tạp cao hơn, như hồi quy tuyến tính (đòi hỏi nhiều tham số hơn hồi quy tuyến tính thưa), cần các giá trị b và T tăng để đạt được hội tụ ổn định trong huấn luyện. Ví dụ, với T = 10, hồi quy tuyến tính phân kỳ tại b = 20, trong khi hồi quy tuyến tính thưa hội tụ đến điểm cố định. Các giá trị b và T cần thiết có thể chỉ ra độ phức tạp mà transformer gặp phải khi giải quyết nhiệm vụ cụ thể. Thú vị, trong khi học mạng nơ-ron 2 lớp khó khăn hơn (Chen và cộng sự, 2022), nó yêu cầu ít b và T hơn cho transformer vòng lặp so với các nhiệm vụ tưởng chừng đơn giản hơn, như hồi quy tuyến tính. Kết quả đầy đủ của transformer vòng lặp được huấn luyện với các b và T khác nhau được trình bày trong Phụ lục B.

[HÌNH VẼ: Ba biểu đồ đánh giá transformer vòng lặp trên các nhiệm vụ khác nhau]

Hình 9: Đánh giá transformer vòng lặp trên học trong ngữ cảnh dữ liệu được tạo từ a) hàm tuyến tính thưa, b) cây quyết định ngẫu nhiên, và c) mạng nơ-ron ReLU 2 lớp với T = 10 và b khác nhau trong huấn luyện (b được định nghĩa trong Phương trình 1). Các đường liền nét với nhiều màu khác nhau mô tả cách transformer vòng lặp, được huấn luyện với giá trị b cụ thể, hoạt động khi số lần lặp vòng tăng trong quá trình suy luận. Đường gạch ngang tương ứng biểu thị giá trị b.

Transformer Vòng lặp Phù hợp hoặc Vượt trội Transformer Tiêu chuẩn. Thông qua tìm kiếm siêu tham số toàn diện, chúng tôi xác định các giá trị tối ưu cho b và T, như được chi tiết trong Phụ lục B. Cụ thể, đối với dữ liệu được tạo từ a) các hàm tuyến tính thưa, chúng tôi sử dụng b = 20 và T = 10; b) cây quyết định, b = 70 và T = 15; và c) mạng nơ-ron ReLU 2 lớp, b = 12 và T = 5. Sau đó chúng tôi so sánh hiệu suất của transformer vòng lặp với transformer tiêu chuẩn và các bộ giải cơ sở khác, như được mô tả trong Hình 10. Trong tất cả các nhiệm vụ chúng tôi kiểm tra, transformer vòng lặp liên tục phù hợp với transformer tiêu chuẩn, ngoại trừ các nhiệm vụ hồi quy tuyến tính thưa, nơi transformer vòng lặp vượt trội hơn transformer tiêu chuẩn, và thậm chí cả Lasso (Tibshirani, 1996). Bằng vượt trội, chúng tôi không đề cập đến độ chính xác của nghiệm cuối cùng mà đến xu hướng tương đối với số lượng mẫu trong ngữ cảnh. Ví dụ, với 40 mẫu trong ngữ cảnh, Lasso đạt được nghiệm với lỗi 1.32e-04, Bình phương Tối thiểu đạt 8.21e-14, transformer vòng lặp đạt 6.12e-04, và transformer tiêu chuẩn đạt 0.0017. Tuy nhiên, với ít hơn 10 mẫu trong ngữ cảnh, transformer vòng lặp có lỗi nhỏ hơn nghiệm Lasso. Ở đây chúng tôi thực hiện tìm kiếm siêu tham số cho Lasso trên α ∈ {1e−4,1e−3,0.01,0.1,1}, trong đó α là tham số ℓ1, và chọn α = 0.01 tốt nhất.

Chúng tôi suy đoán rằng cải thiện hiệu suất này là kết quả của thiên hướng quy nạp của transformer vòng lặp, ưa chuộng các nghiệm đơn giản hơn (tức là thưa hơn) so với các đối tác phức tạp hơn. Khi giải quyết các vấn đề thưa, thiên hướng cố hữu này tăng cường hiệu suất của transformer vòng lặp.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Ba biểu đồ đánh giá hiệu suất của transformer được huấn luyện]

Hình 10: Đánh giá hiệu suất của transformer được huấn luyện trên học trong ngữ cảnh dữ liệu từ: a) hàm tuyến tính thưa, b) cây quyết định ngẫu nhiên, và c) mạng nơ-ron ReLU 2 lớp. Transformer vòng lặp phù hợp hoặc thậm chí vượt qua hiệu suất của transformer chỉ sử dụng 1/12 số tham số được sử dụng bởi transformer sau.

Phân tích Hiệu suất của Transformer Vòng lặp qua Các Mức độ Thưa Thay đổi trong Hồi quy Tuyến tính Thưa. Transformer vòng lặp thể hiện hiệu suất tăng cường trong nhiệm vụ hồi quy tuyến tính thưa so với transformer không vòng lặp. Để điều tra thêm điều này, chúng tôi kiểm tra nhiệm vụ dưới các mức độ thưa thay đổi, đại diện cho các độ khó nhiệm vụ khác nhau. Để cho phép transformer vòng lặp xử lý tất cả các mức độ phức tạp, chúng tôi chọn tham số b = 30 và T = 15. Như được trình bày trong Hình 11, kết quả cho thấy rằng qua tất cả các mức độ thưa của vector trọng số cho hàm tuyến tính thưa, transformer vòng lặp liên tục vượt trội hơn transformer không vòng lặp, đặc biệt khi số lượng mẫu trong ngữ cảnh bị hạn chế.

[HÌNH VẼ: Biểu đồ hiệu suất với các mức độ thưa khác nhau]

Hình 11: Hiệu suất của transformer vòng lặp trong việc giải quyết các hàm tuyến tính thưa với kích thước vấn đề d = 20. Chúng tôi kiểm tra các mức độ thưa s = 1,3,6,9,12, nơi chỉ có s mục nhập là khác không. Qua tất cả các mức độ thưa, transformer vòng lặp liên tục phù hợp hoặc vượt trội hơn transformer không vòng lặp, đạt được các nghiệm chính xác hơn với ít mẫu trong ngữ cảnh hơn.

6 THẢO LUẬN VÀ HƯỚNG NGHIÊN CỨU TƯƠNG LAI

Hiểu biết Toán học về Transformer Vòng lặp. Sức mạnh biểu đạt của các mô hình hồi quy đã được điều tra rộng rãi trong tài liệu: Định lý 3 trong Bai và cộng sự (2019) chứng minh rằng bất kỳ mạng nơ-ron L lớp truyền thống nào có thể được biểu diễn bởi mạng gắn trọng số, tiêm đầu vào. Hơn nữa, Giannou và cộng sự (2023) khám phá tiềm năng của transformer vòng lặp bộ mã hóa để hoạt động như một máy tính vạn năng. Công trình của chúng tôi tập trung vào việc xác thực thực nghiệm kiến trúc vòng lặp, cung cấp phương pháp huấn luyện thực tế cho transformer giải mã vòng lặp trong học trong ngữ cảnh. Điều tra tính biểu đạt của transformer này trình bày hướng nghiên cứu tương lai hứa hẹn.

Transformer Vòng lặp Mô phỏng Phương pháp Lặp Điểm Cố định Trong Phân phối Huấn luyện. Thông qua phương pháp huấn luyện được thiết kế, transformer vòng lặp thành công xấp xỉ các nghiệm điểm cố định vượt quá các lần lặp nó được huấn luyện, hiệu quả mô phỏng phương pháp lặp điểm cố định trong phân phối huấn luyện. Tuy nhiên, những nghiệm này đi kèm với sàn lỗi cố hữu và có hiệu suất hạn chế trên các gợi ý ngoài phân phối. Kết quả là, transformer vòng lặp được huấn luyện của chúng tôi không thể xác định một thuật toán thực sự tổng quát hóa cho bất kỳ phân phối đầu vào nào, mà chỉ mô phỏng thuật toán với phân phối đầu vào cụ thể nó được huấn luyện. Vượt qua hạn chế này, và tăng cường bộ giải xuất phát từ transformer để phù hợp với khả năng tổng quát hóa của các thuật toán thông thường trình bày hướng đi hứa hẹn cho nghiên cứu tương lai.

Lựa chọn b và T và Khái niệm Độ khó Nhiệm vụ. Như được thảo luận trong Mục 5.4, việc tăng số lần lặp vòng (b) và kích thước cửa sổ tổn thất cắt ngắn (T) cho phép transformer vòng lặp tìm nghiệm điểm cố định không phân kỳ vượt quá các lần lặp vòng được huấn luyện. Các giá trị tối ưu cho b và T được xác định thông qua điều chỉnh siêu tham số, chọn các tham số tối thiểu cần thiết để ngăn chặn phân kỳ trong transformer vòng lặp. Những giá trị tối ưu này của b và T phản ánh độ phức tạp của một nhiệm vụ từ góc độ của transformer. Một nhiệm vụ cơ bản phức tạp hơn sẽ yêu cầu các giá trị b và T lớn hơn để đạt được hội tụ đến nghiệm điểm cố định, trong khi các nhiệm vụ đơn giản hơn sẽ yêu cầu các giá trị nhỏ hơn. Do đó, các tham số b và T có thể được sử dụng như một chỉ số để đánh giá mức độ khó khăn của nhiệm vụ cho transformer. Mở rộng điều này, trong thực tế nơi các nhiệm vụ thể hiện các mức độ phức tạp khác nhau, một chiến lược vòng lặp thích ứng có thể có lợi. Những chiến lược này có thể bao gồm các lần lặp vòng thích ứng cấp token, như được đề xuất bởi Dehghani và cộng sự (2018), hoặc suy luận đa nhiệm vụ trong ngữ cảnh.

Lựa chọn b và T và Sự cân bằng Bộ nhớ-Tính toán của chúng. Chi phí tính toán của việc huấn luyện transformer vòng lặp gắn liền với các giá trị được chọn của b và T. Cụ thể, b xác định thời gian chuyển tiếp của transformer, vì transformer vòng lặp phải đánh giá đầu ra lần lặp vòng thứ b bằng cách triển khai b lần. T ảnh hưởng đến yêu cầu bộ nhớ trong quá trình huấn luyện, vì tổn thất được tính toán trên T lần lặp. Nếu mục tiêu là xấp xỉ nghiệm điểm cố định sử dụng transformer vòng lặp, người ta có thể tránh việc điều chỉnh chuyên sâu của b và T. Thay vào đó, chọn b lớn hơn, T được chọn phù hợp, và kết hợp các chiến lược chính quy hóa như huấn luyện độ chính xác hỗn hợp, cắt gradient, và suy giảm trọng số có thể có lợi. Những chiến lược chính quy hóa này có thể giảm nhẹ các vấn đề tính không ổn định liên quan đến T. Chúng tôi bỏ qua những phương pháp chính quy hóa này để duy trì so sánh công bằng với các phương pháp huấn luyện transformer được nêu trong Garg và cộng sự (2022).

7 KẾT LUẬN

Được thúc đẩy bởi việc ứng dụng rộng rãi của các thuật toán lặp trong việc giải quyết các bài toán khớp dữ liệu qua các lớp hàm khác nhau, chúng tôi đã phân tích lựa chọn kiến trúc vòng lặp và chiến lược huấn luyện, sau đó phát triển phương pháp huấn luyện cho transformer vòng lặp. Phương pháp này cho phép transformer vòng lặp xấp xỉ các nghiệm điểm cố định vượt quá các lần lặp vòng được huấn luyện ban đầu. Chúng tôi hy vọng công trình này có thể đóng góp cho cộng đồng trong việc hiểu và phát triển các chiến lược huấn luyện tốt hơn cho transformer trong việc giải quyết các bài toán khớp dữ liệu.

TÀI LIỆU THAM KHẢO

Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, và Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. ArXiv, abs/2306.00297, 2023. URL https://api.semanticscholar.org/CorpusID:258999480.

Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, và Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. ArXiv, abs/2211.15661, 2022. URL https://api.semanticscholar.org/CorpusID:254043800.

Guillaume Alain và Yoshua Bengio. Understanding intermediate layers using linear classifier probes. ArXiv, abs/1610.01644, 2016. URL https://api.semanticscholar.org/CorpusID:9794990.

Shaojie Bai, J. Zico Kolter, và Vladlen Koltun. Trellis networks for sequence modeling. ArXiv, abs/1810.06682, 2018.

Shaojie Bai, J. Zico Kolter, và Vladlen Koltun. Deep equilibrium models. ArXiv, abs/1909.01377, 2019.

Shaojie Bai, Vladlen Koltun, và J. Zico Kolter. Multiscale deep equilibrium models. ArXiv, abs/2006.08656, 2020.

Shaojie Bai, Vladlen Koltun, và J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In International Conference on Machine Learning, 2021.

Shaojie Bai, Vladlen Koltun, và J. Zico Kolter. Neural deep equilibrium solvers. In International Conference on Learning Representations, 2022.

Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, và Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. ArXiv, abs/2306.04637, 2023. URL https://api.semanticscholar.org/CorpusID:259095794.

Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Ali Sami Emam, Furong Huang, Micah Goldblum, và Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. ArXiv, abs/2202.05826, 2022.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, và Christian Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, 2003.

S. Bhattamishra, Arkil Patel, Varun Kanade, và Phil Blunsom. Simplicity bias in transformers and their ability to learn sparse boolean functions. ArXiv, abs/2211.12316, 2022.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.

François Charton. Linear algebra with transformers. Trans. Mach. Learn. Res., 2022, 2021.

François Charton. What is my math transformer doing? - three results on interpretability and generalization. ArXiv, abs/2211.00170, 2022.

Sitan Chen, Aravind Gollakota, Adam R. Klivans, và Raghu Meka. Hardness of noise-free learning for two-hidden-layer neural networks. ArXiv, abs/2202.05258, 2022. URL https://api.semanticscholar.org/CorpusID:246706042.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1–240:113, 2022. URL https://api.semanticscholar.org/CorpusID:247951931.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, và Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. ArXiv, abs/1901.02860, 2019.

Juliette Decugis, Max Emerling, Ashwin Ganesh, Alicia Y. Tsai, và Laurent El Ghaoui. On the abilities of mathematical extrapolation with implicit models. 2022.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, và Lukasz Kaiser. Universal transformers. ArXiv, abs/1807.03819, 2018.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.

Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, và Radu Soricut. Causallm is not optimal for in-context learning. 2023. URL https://api.semanticscholar.org/CorpusID:260887420.

Danimir T. Doncevic, Alexander Mitsos, Yu Guo, Qianxiao Li, Felix Dietrich, Manuel Dahmen, và Ioannis G. Kevrekidis. A recursively recurrent neural network (r2n2) architecture for learning iterative algorithms. ArXiv, abs/2211.12386, 2022.

Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, và Yejin Choi. Faith and fate: Limits of transformers on compositionality. ArXiv, abs/2305.18654, 2023. URL https://api.semanticscholar.org/CorpusID:258967391.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Deqing Fu, Tian-Qi Chen, Robin Jia, và Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023.

Shivam Garg, Dimitris Tsipras, Percy Liang, và Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022.

Angeliki Giannou, Shashank Rajput, Jy yong Sohn, Kangwook Lee, Jason D. Lee, và Dimitris Papailiopoulos. Looped transformers as programmable computers. ArXiv, abs/2301.13196, 2023.

Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, và Jason D Lee. How well can transformers emulate in-context newton's method? arXiv preprint arXiv:2403.03183, 2024.

Surbhi Goel, Sham M. Kakade, Adam Tauman Kalai, và Cyril Zhang. Recurrent convolutional neural networks learn succinct learning algorithms. ArXiv, abs/2209.00735, 2022.

Geoffrey E. Hinton và Ilya Sutskever. Training recurrent neural networks. 2013.

Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.

DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, và Behnam Neyshabur. Block-recurrent transformers. ArXiv, abs/2203.07852, 2022.

Herbert Jaeger. A tutorial on training recurrent neural networks, covering bppt, rtrl, ekf and the "echo state network" approach - semantic scholar. 2005.

Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, và João Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning, 2021.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. ArXiv, abs/1909.11942, 2019.

Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, và Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.

Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, và Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. 2023.

Ziqian Lin và Kangwook Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024.

Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, và Cyril Zhang. Transformers learn shortcuts to automata. ArXiv, abs/2210.10749, 2022.

Arvind V. Mahankali, Tatsunori Hashimoto, và Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. ArXiv, abs/2307.03576, 2023.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, và Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Conference on Empirical Methods in Natural Language Processing, 2022.

Samuel Muller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, và Frank Hutter. Transformers can do bayesian inference. ArXiv, abs/2112.10510, 2021.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, T. J. Henighan, Benjamin Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, John Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom B. Brown, Jack Clark, Jared Kaplan, Sam McCandlish, và Christopher Olah. In-context learning and induction heads. ArXiv, abs/2209.11895, 2022.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Greg Ongie, Ajil Jalal, Christopher A. Metzler, Richard Baraniuk, Alexandros G. Dimakis, và Rebecca M. Willett. Deep learning techniques for inverse problems in imaging. IEEE Journal on Selected Areas in Information Theory, 1:39–56, 2020.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

Allan Raventos, Mansheej Paul, Feng Chen, và Surya Ganguli. The effects of pretraining task diversity on in-context learning of ridge regression. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the royal statistical society series b-methodological, 58:267–288, 1996.

Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, và Luis Torgo. Openml: networked science in machine learning. SIGKDD Explorations, 15(2):49–60, 2013. doi: 10.1145/2641190.2641198. URL http://doi.acm.org/10.1145/2641190.264119.

Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. In NIPS, 2017.

Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, và Max Vladymyrov. Transformers learn in-context by gradient descent. ArXiv, abs/2212.07677, 2022.

Z. Wang, Yao Ma, Zitao Liu, và Jiliang Tang. R-transformer: Recurrent neural network enhanced transformer. ArXiv, abs/1907.05572, 2019.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, và Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.

Ezra Winston và J. Zico Kolter. Monotone operator equilibrium networks. ArXiv, abs/2006.08591, 2020.

Sang Michael Xie, Aditi Raghunathan, Percy Liang, và Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. ArXiv, abs/2111.02080, 2021.

Ruiqi Zhang, Spencer Frei, và Peter L. Bartlett. Trained transformers learn linear models in-context. ArXiv, abs/2306.09927, 2023a.

Ruiqi Zhang, Jingfeng Wu, và Peter L Bartlett. In-context learning of a linear transformer block: Benefits of the mlp component and one-step gd initialization. arXiv preprint arXiv:2402.14951, 2024.

Shizhuo Dylan Zhang, Curt Tigges, Stella Rose Biderman, Maxim Raginsky, và Talia Ringer. Can transformers learn to solve problems recursively? 2023b.

Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, và Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. ArXiv, abs/2206.04301, 2022.

Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, và Ed Huai hsin Chi. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625, 2022a.

Hattie Zhou, Azade Nova, H. Larochelle, Aaron C. Courville, Behnam Neyshabur, và Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. ArXiv, abs/2211.09066, 2022b.

A THIẾT LẬP CHI TIẾT CHO CÁC LỚP HÀM

Để hoàn thiện, chúng tôi chi tiết thiết lập của các lớp hàm ở đây, tuân thủ các thiết lập trong Garg và cộng sự (2022). Đầu vào x ∼ N(0, I) nhất quán qua tất cả các lớp hàm.

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hàm Tuyến tính. Đối với hàm tuyến tính với kích thước d, chúng tôi lấy mẫu tham số của hàm tuyến tính w ∼ R^d theo N(0, I).

Hàm Tuyến tính Thưa. Chúng tôi tuân theo cùng thiết lập phân phối như trong hàm tuyến tính, và chúng tôi chọn đều s trong số d kích thước để khác không.

Cây Quyết định. Chúng tôi sử dụng cây quyết định nhị phân với độ sâu là 4. Tại mỗi nút không phải lá, quyết định rẽ trái hay phải dựa trên dấu của một tọa độ được chọn ngẫu nhiên của đầu vào x. Tọa độ này được chọn ngẫu nhiên đều. Các nút lá được khởi tạo theo phân phối chuẩn N(0, I). Việc đánh giá một đầu vào x liên quan đến việc di chuyển từ gốc đến nút lá, di chuyển trái cho tọa độ âm và phải cho tọa độ dương, với đầu ra là giá trị của nút lá đạt được.

Mạng nơ-ron ReLU 2 lớp. Chúng tôi khởi tạo trọng số của mạng theo N(0, I), và đặt kích thước lớp ẩn là 100.

B LỰA CHỌN SỐ LẦN LẶP VÒNG VÀ TÁC ĐỘNG CỦA NÓ LÊN HIỆU SUẤT TRONG NGỮ CẢNH

Trong phần này, chúng tôi trình bày kết quả (Hình 12) của hiệu suất transformer vòng lặp khi được huấn luyện với b và T khác nhau cho các lớp hàm sau: a) hàm tuyến tính với kích thước vấn đề d = 20 và mẫu trong ngữ cảnh k = 40, b) hàm tuyến tính thưa với kích thước vấn đề d = 20, mẫu trong ngữ cảnh k = 40, và mục nhập không thưa s = 3, c) cây quyết định với kích thước vấn đề d = 20, độ sâu là 4, và mẫu trong ngữ cảnh k = 100, cũng như d) mạng nơ-ron ReLU 2 lớp với kích thước vấn đề d = 20, 100 nơ-ron ẩn, và mẫu trong ngữ cảnh k = 100.

Nhớ lại rằng hàm mục tiêu chúng tôi nhằm tối thiểu hóa trong quá trình huấn luyện transformer vòng lặp là:

min_θ E_P[1/(b-b₀) ∑_{t=b₀}^b 1/(k+1) ∑_{i=0}^k ℓ(Y_t(P_i|θ), f(x_{i+1}))]

trong đó P_i biểu thị tiền tố gợi ý với i mẫu trong ngữ cảnh P_i = (x₁, f(x₁),···,x_i, f(x_i),x_{i+1}), Y_t(P_i|θ) là đầu ra của transformer vòng lặp với tham số θ sau t lần lặp vòng. b₀ = max(b−T,0). Lựa chọn b ảnh hưởng đến số lần lặp vòng mà transformer vòng lặp cần triển khai trong quá trình huấn luyện, và T biểu thị kích thước cửa sổ cắt ngắn cho tính toán tổn thất.

Đối với hồi quy tuyến tính, hồi quy tuyến tính thưa, và nhiệm vụ mạng nơ-ron ReLU 2 lớp, chúng tôi điều tra các giá trị b khác nhau trong {12,20,30,40,50}, và đối với nhiệm vụ thách thức hơn – cây quyết định, chúng tôi tìm kiếm trên các giá trị b trong {12,50,70,100}. Đối với tất cả các nhiệm vụ, chúng tôi điều tra các giá trị T trong {5,10,15}. Các giá trị T lớn hơn như T = 20 sẽ dẫn đến tính không ổn định huấn luyện nếu được huấn luyện mà không có kỹ thuật ổn định nào như suy giảm trọng số, độ chính xác hỗn hợp, hoặc cắt chuẩn gradient. Nhưng một khi được sử dụng với những kỹ thuật đó, việc huấn luyện với T lớn cũng an toàn. Trong Hình 12 và Hình 13, chúng tôi trình bày kết quả của transformer vòng lặp, khi được huấn luyện với b và T khác nhau, hiệu suất trên các nhiệm vụ khác nhau, liên quan đến số lần lặp vòng, và liên quan đến các mẫu trong ngữ cảnh.

Đối với mỗi nhiệm vụ, kết quả nhất quán: huấn luyện với T nhỏ hơn sẽ cho lỗi bình phương trung bình lớn hơn trong nghiệm học trong ngữ cảnh. Khi T được giữ không đổi, b lớn hơn sẽ cho phép transformer vòng lặp tìm nghiệm điểm cố định. Khi b tăng, hiệu suất cải thiện cho đến khi đạt điểm bão hòa.

Qua các nhiệm vụ, chúng ta có thể thấy mô hình rõ ràng của lựa chọn b và T liên quan đến mức độ khó khăn của nhiệm vụ. Hồi quy tuyến tính thưa, có ít tham số hơn trong lớp hàm của nó, yêu cầu ít b và T hơn để tìm nghiệm hội tụ so với hồi quy tuyến tính. Nhiệm vụ cây quyết định yêu cầu học 2^{depth+1}−1 số tham số (trong trường hợp của chúng tôi, 31 tham số). Do đó nó yêu cầu b lớn hơn để tìm nghiệm điểm cố định. Trái với trực giác, mạng nơ-ron ReLU 2 lớp, chứa tổng cộng 21×100 số tham số, chỉ yêu cầu b = 12 để tìm nghiệm điểm cố định. Lý do tại sao dữ liệu được tạo từ mạng nơ-ron ReLU 2 lớp chỉ yêu cầu giá trị b nhỏ để tìm nghiệm điểm cố định vẫn chưa rõ ràng. Chúng tôi suy đoán rằng giá trị b là một chỉ báo về độ khó khăn của nhiệm vụ đối với transformer vòng lặp để giải quyết.

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Các biểu đồ đánh giá transformer vòng lặp với các thiết lập b và T khác nhau]

Hình 12: Chúng tôi đánh giá transformer vòng lặp trên học trong ngữ cảnh của a) hàm tuyến tính, và b) hàm tuyến tính thưa với b và T khác nhau trong quá trình huấn luyện (b và T được định nghĩa trong Phương trình 1). Đối với mỗi khối hình, từ trái sang phải là với T = 5,10,15. (trên) Hiệu suất liên quan đến số lần lặp vòng khác nhau, (dưới) hiệu suất liên quan đến số lượng mẫu trong ngữ cảnh khác nhau.

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Tiếp tục các biểu đồ đánh giá cho cây quyết định và mạng nơ-ron]

Hình 13: Chúng tôi đánh giá transformer vòng lặp trên học trong ngữ cảnh của a) cây quyết định ngẫu nhiên, và b) mạng nơ-ron ReLU 2 lớp với b và T khác nhau trong quá trình huấn luyện (b và T được định nghĩa trong Phương trình 1). Đối với mỗi khối hình, từ trái sang phải là với T = 5,10,15. (trên) Hiệu suất liên quan đến số lần lặp vòng khác nhau, (dưới) hiệu suất liên quan đến số lượng mẫu trong ngữ cảnh khác nhau.

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Hiệu suất với số lượng gợi ý khác nhau cho các kích thước vấn đề khác nhau]

Hình 14: Hiệu suất của transformer tiêu chuẩn và transformer vòng lặp trên bài toán hồi quy tuyến tính với kích thước vấn đề d = 5,10,20, khi được huấn luyện với số lượng gợi ý/hàm riêng biệt khác nhau.

C TÁC ĐỘNG CỦA SỐ LƯỢNG GỢI Ý / HÀM RIÊNG BIỆT ĐƯỢC NHÌN THẤY TRONG QUÁ TRÌNH HUẤN LUYỆN

Trong Mục 5, chúng tôi đã điều tra tác động của số lượng gợi ý riêng biệt trong quá trình huấn luyện transformer cho kích thước vấn đề d = 10 và mẫu trong ngữ cảnh k = 20. Trong phần này, chúng tôi mở rộng kết quả để bao gồm (1) d = 5, k = 10, và (2) d = 20, k = 40. Điều quan trọng cần lưu ý là tất cả các thực nghiệm được tiến hành mà không sử dụng học chương trình cho d và k. Đối với transformer vòng lặp, chúng tôi đặt b = 30, và T = 15, và chúng tôi không áp dụng lịch trình trong các lần lặp vòng cũng vậy. Đối với d = 5, chúng tôi điều tra tốc độ học trong {0.0001}, đối với d = 10 với tốc độ học {0.00001, 0.0001}, và đối với d = 20, chúng tôi khám phá tốc độ học {0.00001, 0.0001, 0.001}, chọn kết quả với hiệu suất tốt nhất. Kết quả được trình bày trong Hình 14. Qua các kích thước vấn đề khác nhau, transformer vòng lặp thể hiện hiệu quả mẫu tốt hơn so với transformer vanilla.

D HÀNH VI NGOẠI SUY CỦA TRANSFORMER VÒNG LẶP VÀ TRANSFORMER KHÔNG VÒNG LẶP

Chúng tôi tuân theo thiết lập từ Garg và cộng sự (2022) về việc kiểm tra hành vi ngoài phân phối. Nhớ lại rằng cả transformer và transformer vòng lặp đều được huấn luyện để giải quyết các nhiệm vụ hồi quy tuyến tính trong ngữ cảnh. Trong quá trình huấn luyện, các đầu vào được lấy mẫu từ N(0, I_d), và trọng số hàm tuyến tính w ∼ N(0, I_d). Ở đây d là 20, và số lượng mẫu trong ngữ cảnh là k = 40. Bây giờ chúng tôi đánh giá những mô hình này trên: a) đầu vào có hiệp phương sai lệch; b) đầu vào nằm trong không gian con d/2 chiều; c) đầu vào có nhiễu; d) truy vấn vuông góc với mẫu trong ngữ cảnh; e) truy vấn bằng một trong các mẫu trong ngữ cảnh; và f) đầu vào mẫu trong ngữ cảnh và đầu vào truy vấn nằm trong các góc phần tám khác nhau. Kết quả được trình bày trong Hình 15.

Từ hình, rõ ràng rằng transformer vòng lặp, trong một số trường hợp ngoài phân phối, thể hiện hiệu suất tương tự như transformer tiêu chuẩn với ít lớp hơn. Ví dụ, trong trường hợp hồi quy tuyến tính nhiễu, transformer với số lượng lớp L = 4 thể hiện đỉnh giảm khi d mẫu trong ngữ cảnh được trình bày. Xu hướng này tương tự như transformer vòng lặp, đặc biệt khi giá trị b lớn hơn. Trong một số trường hợp khác, transformer vòng lặp thể hiện hiệu suất giống như transformer tiêu chuẩn với số lượng lớp cao hơn. Ví dụ, khi các gợi ý là (a) có hiệp phương sai lệch, (b) trong không gian con d/2 chiều, hoặc (f) các mẫu trong ngữ cảnh và gợi ý nằm trong các góc phần tám khác nhau, đối với transformer tiêu chuẩn, L càng lớn, hiệu suất càng tốt. Hơn nữa, transformer vòng lặp có thể phù hợp hoặc thậm chí vượt trội hơn transformer tiêu chuẩn với tới L = 20 số lượng lớp. Nhớ lại rằng đối với nhiệm vụ hồi quy tuyến tính, khi b < 20, transformer vòng lặp không thể tìm nghiệm điểm cố định. Do đó, chúng tôi không bao gồm transformer vòng lặp với b < 20 trong so sánh. Trong các trường hợp khác, transformer vòng lặp hoạt động tương tự như transformer tiêu chuẩn. Điều này rõ ràng trong các trường hợp như (d) khi các mẫu trong ngữ cảnh và truy vấn nằm trong không gian vuông góc, và (e) khi truy vấn khớp với một trong các mẫu trong ngữ cảnh.

Chúng tôi đánh giá thêm hiệu suất ngoài phân phối của transformer vòng lặp bằng cách thu phóng gợi ý x hoặc thu phóng tham số của hàm tuyến tính w trong quá trình suy luận. Kết quả được mô tả trong Hình 16. Như được minh họa trong hình, transformer vòng lặp thể hiện hiệu suất tương đương với transformer tiêu chuẩn với ít lớp hơn. Ví dụ, khi x được thu phóng bởi 2, transformer vòng lặp hoạt động tương tự như transformer tiêu chuẩn với L = 4.

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: Các biểu đồ đánh giá transformer trên các điều kiện ngoài phân phối khác nhau]

Hình 15: Đánh giá transformer với số lượng lớp L khác nhau, và transformer vòng lặp, với số lượng lớp là L = 1, được huấn luyện đến số lần lặp vòng tối đa b trong quá trình huấn luyện. Transformer được huấn luyện trên nhiệm vụ hồi quy tuyến tính, nhưng được đánh giá trên các nhiệm vụ ngoài phân phối khác nhau.

[HÌNH VẼ: Đánh giá với đầu vào và tham số được thu phóng]

Hình 16: Đánh giá transformer với số lượng lớp L khác nhau, và transformer vòng lặp, với số lượng lớp là L = 1, được huấn luyện đến số lần lặp vòng tối đa b trong quá trình huấn luyện. Transformer được huấn luyện trên nhiệm vụ hồi quy tuyến tính, nhưng được đánh giá trên gợi ý x được thu phóng hoặc tham số hàm tuyến tính w được thu phóng.

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[HÌNH VẼ: So sánh hiệu suất với và không có lịch trình]

Hình 17: Hiệu suất của transformer vòng lặp liên quan đến số lượng mẫu trong ngữ cảnh thay đổi. Chúng tôi so sánh transformer vòng lặp được huấn luyện trong việc giải quyết (a) hồi quy tuyến tính, (b) hồi quy tuyến tính thưa, (c) cây quyết định, và (d) mạng nơ-ron ReLU 2 lớp, khi được huấn luyện với (đường liền nét) hoặc không có (đường gạch ngang) lịch trình. Việc áp dụng lịch trình trên b có tác động đến hiệu suất khi huấn luyện transformer vòng lặp trên dữ liệu được tạo từ cây quyết định, với việc áp dụng lịch trình dẫn đến hiệu suất tốt hơn một chút.

E TÁC ĐỘNG CỦA LỊCH TRÌNH

Trong Mục 5.1, chúng tôi thảo luận về lịch trình huấn luyện cho các lần lặp vòng – chúng tôi sẽ dần dần tăng b trong quá trình huấn luyện. Kết quả là, cửa sổ tổn thất cắt ngắn bắt đầu tại [1, T], và sau đó dần dần chuyển đến [b−T, b]. Trong phần này, chúng tôi điều tra tác động của lịch trình này trong quá trình huấn luyện. Cụ thể, chúng tôi so sánh transformer vòng lặp huấn luyện trên bốn lớp hàm: a) hàm tuyến tính; b) hàm tuyến tính thưa; c) cây quyết định; d) mạng nơ-ron ReLU 2 lớp, khi được huấn luyện với và không có lịch trình. Khi huấn luyện, chúng tôi tuân thủ b và T được chọn như được chỉ ra trong Mục 5.4. Kết quả được trình bày trong Hình 17. Như được chỉ ra trong hình, trong hầu hết các trường hợp, hiệu suất của transformer vòng lặp không khác biệt khi được huấn luyện với hoặc không có lịch trình. Tuy nhiên, đối với nhiệm vụ cây quyết định, transformer vòng lặp được huấn luyện với lịch trình thể hiện hiệu suất tốt hơn so với khi được huấn luyện không có lịch trình. Do đó, trong suốt các thực nghiệm của chúng tôi, chúng tôi chọn sử dụng lịch trình cho các lần lặp vòng.

F CHI TIẾT CỦA THĂM DÒ MÔ HÌNH

Trong Mục 5.3, chúng tôi phân tích đầu ra của lớp thứ t của transformer tiêu chuẩn hoặc lần lặp vòng thứ t của transformer vòng lặp bằng cách huấn luyện mô hình thăm dò trên chúng. Trong phần này, chúng tôi trình bày chi tiết của các thực nghiệm thăm dò mô hình.

Tái sử dụng ký hiệu, chúng tôi biểu thị đầu ra tại lớp thứ t hoặc lần lặp vòng như Y_t. Y_t có hình dạng k×D, trong đó k biểu thị độ dài chuỗi, và D biểu thị kích thước embedding của transformer. Trong các thực nghiệm của chúng tôi, chúng tôi điều tra hai đầu dò mục tiêu: (a) X^T y, biểu thị thành phần gradient, và w_ols, biểu thị nghiệm bình phương tối thiểu tối ưu. Cho kích thước vấn đề là d, các đầu dò mục tiêu v có hình dạng d×1. Để đơn giản, chúng tôi bỏ qua kích thước batch.

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Trước khi đưa biểu diễn Y_t vào mô hình thăm dò, chúng tôi trước tiên tính tổng có trọng số trên chiều chuỗi. Cho s ∈ R^{k×1}, chúng tôi thực hiện các phép toán sau:

α = softmax(s)                                                   (2)
Z_t = α^T Y_t                                                    (3)

Với Z_t ∈ R^{1×D}, chúng tôi sử dụng mô hình perceptron đa lớp (MLP) f_MLP để thăm dò đầu ra. Mô hình MLP bao gồm hai lớp tuyến tính, với ReLU ở giữa, với d_hidden biểu thị số lượng nơ-ron ẩn trong MLP:

v̂ = f_MLP(Z_t)

Khi huấn luyện s và f_MLP, chúng tôi nhằm tối thiểu hóa lỗi bình phương trung bình giữa v̂ và v. Đối với đầu ra của mỗi lớp, và mỗi độ dài ngữ cảnh, chúng tôi huấn luyện s và f_MLP riêng biệt để thăm dò mục tiêu. Khi trình bày kết quả, chúng tôi tính toán tổn thất trung bình trên các độ dài ngữ cảnh khác nhau. Ngoài ra, để chuẩn hóa lỗi, chúng tôi chia lỗi bình phương trung bình cho kích thước vấn đề d.

Trong các thực nghiệm của chúng tôi, chúng tôi đặt D = 256, d = 20, và d_hidden = 64. Chúng tôi tối ưu hóa mô hình thăm dò với bộ tối ưu hóa Adam và tốc độ học 0.001, được tìm thấy là tốt nhất trong tập {0.01, 0.001, 0.0001}.

Cấu hình Nhiệm vụ Kiểm soát. Để làm nổi bật tầm quan trọng của mô hình thăm dò đã học, chúng tôi tuân theo thiết lập trong Akyürek và cộng sự (2022), và huấn luyện mô hình thăm dò với trọng số hàm tuyến tính w được cố định tại 1. Trong trường hợp này, mô hình thăm dò sẽ chỉ học thống kê của đầu vào X, loại bỏ nhu cầu học trong ngữ cảnh. Sau đó trong quá trình suy luận, chúng tôi lấy mẫu ngẫu nhiên w ∼ N(0, I_d) và ghi lại lỗi thăm dò kết quả. Lỗi thăm dò tối thiểu cho các nhiệm vụ kiểm soát được hiển thị như các đường gạch ngang trong Hình 8. Từ hình, khoảng cách giữa nhiệm vụ kiểm soát và kết quả thăm dò mô hình chỉ ra rằng mô hình thăm dò sử dụng thông tin trong ngữ cảnh thu được trong đầu ra của mỗi lớp/lần lặp vòng, gợi ý rằng lỗi thăm dò có ý nghĩa.

G THỰC NGHIỆM OPENML

Để thiết lập kết nối với các ứng dụng thực tế và xác thực thêm cách tiếp cận của chúng tôi, chúng tôi đã tiến hành các thực nghiệm bổ sung sử dụng 10 tập dữ liệu từ OpenML (Vanschoren và cộng sự, 2013), mà chúng tôi tin rằng đại diện tốt hơn cho các tình huống thực tế. Chi tiết của các tập dữ liệu được sử dụng được trình bày trong Bảng 1.

[BẢNG: Chi tiết 10 tập dữ liệu OpenML với ID, tên, số đặc trưng số và số thể hiện]

Bảng 1: Tập dữ liệu OpenML được sử dụng trong các thực nghiệm. Chúng tôi chỉ sử dụng các đặc trưng số cho các đặc trưng đầu vào.

Cho S = {720,725,737,803,807,816,819,847,871,42192} là tập tất cả các tập dữ liệu. Các tập dữ liệu chúng tôi kiểm tra có nhãn nhị phân 0/1. Chúng tôi huấn luyện transformer và transformer vòng lặp trên 9 tập dữ liệu và đánh giá khả năng học trong ngữ cảnh của nó trên tập dữ liệu kiểm tra chưa thấy. Cả transformer và transformer vòng lặp đều có kích thước embedding 256, 8 đầu. Transformer có 12 lớp, và transformer vòng lặp có 1 lớp, được huấn luyện đến số lần lặp vòng tối đa b = 30, và kích thước cửa sổ T = 15.

Trong quá trình huấn luyện, chúng tôi lấy mẫu gợi ý đều từ 9 tập dữ liệu, trong đó đối với mỗi gợi ý, chúng tôi trước tiên chọn ngẫu nhiên một tập huấn luyện, sau đó chọn ngẫu nhiên k + 1 mẫu từ tập huấn luyện này, với k là số lượng mẫu trong ngữ cảnh. Trong quá trình kiểm tra, chúng tôi áp dụng cách tiếp cận tương tự cho mỗi mẫu kiểm tra, chọn k mẫu trong ngữ cảnh từ tập dữ liệu kiểm tra, với việc cẩn thận loại trừ chính mẫu kiểm tra khỏi những cặp trong ngữ cảnh này. Độ chính xác kiểm tra được trình bày trong Bảng 2.

Như kết quả chỉ ra, transformer vòng lặp thể hiện hiệu suất tương đương, và trong một số trường hợp, tốt hơn so với transformer tiêu chuẩn trong việc giải quyết các tập dữ liệu OpenML này. Chúng tôi tin rằng những thực nghiệm này cung cấp đại diện thực tế hơn cho các ứng dụng thực tế.

[BẢNG: Độ chính xác kiểm tra cho transformer và transformer vòng lặp trên các tập dữ liệu khác nhau]

Bảng 2: Độ chính xác kiểm tra cho transformer và transformer vòng lặp, cho các id tập dữ liệu kiểm tra khác nhau. Transformer vòng lặp thể hiện hiệu suất tương đương, và trong một số trường hợp, tốt hơn so với transformer vanilla.
