# 2309.08968.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/shared-params/2309.08968.pdf
# Kích thước tệp: 488413 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Sorted LLaMA: Khai Phá Tiềm Năng của Các Lớp Trung Gian trong Mô Hình Ngôn Ngữ Lớn cho Suy Luận Động

Parsa Kavehzadeh2, Mojtaba Valipour1, Marzieh Tahaei2,
Ali Ghodsi1, Boxing Chen2, và Mehdi Rezagholizadeh2
1Đại học Waterloo
2Phòng thí nghiệm Noah's Ark của Huawei
{mojtaba.valipour, ali.ghodsi}@uwaterloo.ca,
{parsa.kavehzadeh, mehdi.rezagholizadeh, marzieh.tahaei, boxing.chen}@huawei.com

Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa xử lý ngôn ngữ tự nhiên (NLP) bằng cách xuất sắc trong việc hiểu và tạo ra văn bản giống con người. Tuy nhiên, việc triển khai rộng rãi chúng có thể cực kỳ tốn kém. SortedNet là một kỹ thuật huấn luyện gần đây để cho phép suy luận động bằng cách tận dụng tính mô-đun trong mạng và sắp xếp các mô hình con dựa trên tính toán/độ chính xác theo cách lồng nhau. Chúng tôi mở rộng SortedNet cho các tác vụ NLP tạo sinh, làm cho các mô hình ngôn ngữ lớn trở nên động mà không cần bất kỳ Huấn luyện Trước nào và chỉ bằng cách thay thế Tinh chỉnh Tiêu chuẩn (SFT) bằng Tinh chỉnh Sắp xếp (SoFT). Phương pháp của chúng tôi tăng cường hiệu quả mô hình, loại bỏ nhu cầu về nhiều mô hình cho các tình huống khác nhau trong quá trình suy luận. Chúng tôi chỉ ra rằng phương pháp này có thể khai phá sức mạnh của các lớp trung gian của transformer trong việc tạo ra đầu ra mục tiêu. Các mô hình con của chúng tôi vẫn là các thành phần tích hợp của mô hình gốc, giảm thiểu yêu cầu lưu trữ và chi phí chuyển đổi giữa các ngân sách tính toán/độ trễ khác nhau. Hiệu quả của phương pháp đề xuất đã được chứng minh bằng cách áp dụng nó để tinh chỉnh LLaMA 2 13B trên bộ dữ liệu Stanford Alpaca để theo dõi hướng dẫn và TriviaQA cho trả lời câu hỏi sách đóng. Kết quả của chúng tôi cho thấy hiệu suất vượt trội của các mô hình con so với Tinh chỉnh Tiêu chuẩn và SFT+ICT (Early-Exit), tất cả đều đạt được với việc tinh chỉnh hiệu quả và không sử dụng thêm bộ nhớ trong quá trình suy luận.

1 Giới thiệu

Các mô hình ngôn ngữ lớn đang cách mạng hóa cách chúng ta tương tác với thông tin trong thế giới ngày nay (Hoffmann et al., 2022; Brown et al., 2020; Penedo et al., 2023; Scao et al., 2022). Các mô hình mới liên tục xuất hiện, thể hiện khả năng của chúng trong việc hiểu và quan trọng hơn, trong việc tạo ra văn bản giống con người. Đáng chú ý, các mô hình như ChatGPT, LLaMA 2 70B (Touvron et al., 2023b), và Falcon 180B (Almazrouei et al., 2023) đã có tác động sâu sắc đến tính ứng dụng của các mô hình ngôn ngữ lớn (LLM). Tuy nhiên, việc triển khai những mô hình ngôn ngữ rộng lớn này có thể trở nên cực kỳ tốn kém.

Điều khiến kỷ nguyên mới của các mô hình giống ChatGPT này trở nên khác biệt là khả năng thực hiện một loạt các tác vụ cực kỳ đa dạng trong xử lý ngôn ngữ tự nhiên (NLP), lý luận và nhiều hơn nữa, tất cả thông qua bản sao hành vi (Wei et al., 2021; Wang et al., 2022). Thực tế, một mô hình duy nhất có thể tận dụng khả năng học ngữ cảnh mạnh mẽ được cung cấp bởi Tinh chỉnh Tiêu chuẩn để giải quyết nhiều tác vụ, trải dài từ hiểu ngôn ngữ đến lý luận phức tạp. Mặc dù việc sử dụng thống nhất này đơn giản hóa việc triển khai các mô hình này như các trợ lý tổng quát, nó vẫn rất kém hiệu quả. Việc cho phép suy luận động, nơi các tài nguyên tính toán được phân bổ cho một truy vấn nhất định thay đổi trong thời gian suy luận, có thể tăng cường đáng kể tính thực tế của việc sử dụng các mô hình như vậy trong các tình huống thời gian thực. Điều này cho phép sử dụng các mô hình nhỏ hơn khi ngân sách bị hạn chế hoặc độ trễ là quan trọng. Điều quan trọng cần lưu ý là các chiến lược suy luận động cho các mô hình lớn với số lượng tham số đáng kể không nên yêu cầu tải các mô hình khác nhau trong quá trình suy luận.

Nghiên cứu trước đây đã khám phá các phương pháp để huấn luyện các mô hình động có khả năng thích ứng với các ràng buộc tài nguyên đang phát triển (Cai et al., 2019; Hou et al., 2020; Xin et al., 2020; Fan et al., 2019). Tuy nhiên, các phương pháp hiện có thường dựa vào các quy trình huấn luyện phức tạp hoặc yêu cầu sửa đổi kiến trúc mô hình gốc. SortedNet (Valipour et al., 2023) giới thiệu một phương pháp mới để huấn luyện mạng nơ-ron sâu tận dụng tính mô-đun vốn có của các mạng này để xây dựng các mô hình con với tải tính toán khác nhau. Phương pháp này sắp xếp các mô hình con theo thứ bậc dựa trên đặc điểm tính toán/độ chính xác của chúng, tạo điều kiện cho việc triển khai hiệu quả trong quá trình suy luận. Hơn nữa, nó sử dụng một sơ đồ cập nhật hiệu quả kết hợp lấy mẫu mô hình con ngẫu nhiên với tích lũy gradient để giảm thiểu chi phí huấn luyện. Do đó, với một vòng huấn luyện duy nhất, nhiều mô hình có thể được thu được trong một mô hình duy nhất.

Mặc dù phương pháp SortedNet chủ yếu được áp dụng cho các tác vụ thị giác và hiểu ngôn ngữ, xét đến tác động đáng kể của các mô hình ngôn ngữ tạo sinh trong bối cảnh AI ngày nay, hiệu quả của phương pháp này cho các tác vụ tạo sinh trong NLP là đáng quan tâm. Thực tế, việc có thể làm cho một mô hình ngôn ngữ lớn trở nên động mà không cần Huấn luyện Trước và chỉ với chi phí của một vòng Tinh chỉnh Tiêu chuẩn có thể mở ra cánh cửa cho việc suy luận hiệu quả của các mô hình này mà không phát sinh thêm chi phí liên quan đến các phương pháp nén mô hình phổ biến như chưng cất kiến thức và cắt tỉa, trong số các phương pháp khác. Hơn nữa, vì tất cả các mô hình kết quả đều là thành phần của mô hình gốc, yêu cầu lưu trữ và chi phí liên quan đến việc chuyển đổi giữa các nhu cầu tính toán khác nhau trở nên tối thiểu. Nếu không, việc quản lý nhiều mô hình cho các tình huống khác nhau trong quá trình suy luận trở nên không thực tế.

Trong nghiên cứu này, chúng tôi thách thức phương pháp thông thường chỉ dựa vào các nhúng ngữ cảnh của lớp cuối cùng và sử dụng Tinh chỉnh Sắp xếp (SoFT) thay cho Tinh chỉnh Tiêu chuẩn để tăng cường hiệu suất của các mô hình này trên nhiều lớp. Bằng cách làm như vậy, chúng tôi hướng đến việc cung cấp những hiểu biết mới về hiệu quả và hiệu suất của các lớp giữa trong việc tạo ra kết quả chất lượng cao cho các tác vụ hạ nguồn cụ thể. Phương pháp đề xuất của chúng tôi có thể tối ưu hóa các mô hình con này ngoài mô hình chính, cuối cùng tăng cường hiệu suất tổng thể của chúng. Trong bài báo này, chúng tôi tìm cách trả lời các câu hỏi sau thông qua đánh giá có hệ thống:

i) Các lớp trung gian từ Tinh chỉnh Tiêu chuẩn của một mô hình ngôn ngữ lớn có tạo ra các đầu ra chính xác và có ý nghĩa không? ii) Tinh chỉnh Tiêu chuẩn có thể hiện hành vi được sắp xếp, có nghĩa là các lớp sau tạo ra kết quả chính xác và có ý nghĩa hơn các lớp trước không? Nếu có, đến mức độ nào? iii) Làm thế nào chúng ta có thể tăng cường hành vi được sắp xếp này với chi phí tối thiểu?

Để trả lời các câu hỏi này, chúng tôi sử dụng LLaMA 2 13B và thực hiện cả Tinh chỉnh Tiêu chuẩn (SFT) và Tinh chỉnh Sắp xếp (SoFT) trên các bộ dữ liệu Stanford Alpaca (Taori et al., 2023) và TriviaQA (Joshi et al., 2017). Đối với Tinh chỉnh Sắp xếp, chúng tôi nhắm đến 8 mô hình con và chia sẻ đầu LLM giữa chúng để đảm bảo tính ngang bằng về chi phí. Chúng tôi sử dụng điểm chuẩn PandaLM (Wang et al., 2023) để đánh giá hiệu suất của các mô hình con trên bộ dữ liệu Alpaca. Những phát hiện của chúng tôi chứng minh hiệu suất vượt trội của SoFT so với SFT và thậm chí so với các phương pháp đòi hỏi bộ nhớ như Early Exit (Xin et al., 2020). Những đóng góp của bài báo này có thể được tóm tắt như sau:

• Mở rộng phương pháp SortedNet để tinh chỉnh các mô hình ngôn ngữ tự hồi quy cho các tác vụ tạo sinh bằng cách chia sẻ một lớp đầu LLM duy nhất giữa các mô hình con.

• Tạo ra 8 mô hình con lồng nhau, từ 12 đến 40 lớp, từ LLaMA2 13B bằng cách áp dụng Tinh chỉnh Sắp xếp trên bộ dữ liệu Stanford Alpaca và điểm chuẩn TriviaQA với chi phí tương đương với Tinh chỉnh Tiêu chuẩn.

• Đánh giá hiệu suất của các mô hình con của LLaMA 2 và chứng minh hiệu quả của SoFT trong việc tăng cường khả năng của các lớp trung gian cho việc tạo văn bản và trả lời câu hỏi thông qua đánh giá toàn diện.

2 Nghiên cứu liên quan

Phần này giới thiệu ngắn gọn các bài báo có liên quan nhất đến công việc của chúng tôi.

Mô hình Nhiều-trong-Một Các mạng nơ-ron sâu (DNN) thường được tham số hóa quá mức, thúc đẩy các nhà nghiên cứu khám phá cách sử dụng các tham số của mô hình hiệu quả hơn. Số lượng tham số nhiều hơn dẫn đến chi phí triển khai cao hơn cho các mạng nơ-ron. Hơn nữa, trong thực tế, các DNN được tham số hóa quá mức này được kỳ vọng sẽ phục vụ các khách hàng với yêu cầu và tài nguyên tính toán khác nhau. Để giải quyết những nhu cầu đa dạng này, người ta có thể nghĩ đến việc huấn luyện các mô hình có kích thước khác nhau, điều này có thể cực kỳ tốn kém (về mặt huấn luyện và bộ nhớ), hoặc một lựa chọn khác là huấn luyện các mạng nhiều-trong-một (Cai et al., 2019). Các giải pháp nhiều-trong-một nhằm huấn luyện một mạng cùng với một số mạng con của nó đồng thời cho các tác vụ cụ thể. Ví dụ, chúng ta có thể xem xét phương pháp Early-Exit (Xin et al., 2020), trong đó một đầu dự đoán được tinh chỉnh trên các lớp trung gian cụ thể trong một mạng. Một phương pháp khác là Layer Drop (Fan et al., 2019), huấn luyện một mạng ở bất kỳ độ sâu nào bằng cách loại bỏ ngẫu nhiên các lớp trong quá trình huấn luyện. Mặc dù cả Early-Exit và Layer Drop đều là các giải pháp đơn giản, chúng không phải là tối ưu về mặt hiệu suất. Trong Early-Exit, chúng ta chỉ huấn luyện lớp dự đoán đầu ra trên đầu mỗi lớp trung gian, và lớp này có thể không có đủ khả năng để duy trì hiệu suất tốt. Ngược lại, Layer Drop gặp khó khăn từ số lượng lớn các mô hình con có thể có trong huấn luyện, điều này làm cho quá trình huấn luyện trở nên kiệt sức và không tối ưu. Hơn nữa, phương pháp này yêu cầu điều chỉnh mức độ loại bỏ lớp trong quá trình huấn luyện. Siêu tham số bổ sung này, tỷ lệ loại bỏ lớp trong quá trình huấn luyện xác định kích thước và cài đặt tốt nhất của mô hình tại thời điểm suy luận. Việc lệch khỏi tỷ lệ loại bỏ huấn luyện tại thời điểm suy luận có thể dẫn đến sụt giảm đáng kể về hiệu suất.

Cai et al. (2019) trong Once for All (OFA) đã đề xuất một giải pháp thay thế cho tìm kiếm kiến trúc mạng nơ-ron (NAS). OFA yêu cầu huấn luyện mô hình và tất cả các mô hình con có thể theo cách tiến bộ tùy ý tiếp theo là một giai đoạn tìm kiếm riêng biệt. Dyna-BERT (Hou et al., 2020) là một công việc khác nhắm đến việc huấn luyện các mô hình BERT động được huấn luyện trước nhiều-trong-một trong hai giai đoạn: đầu tiên, chưng cất từ mạng chính đến các mạng thích ứng chiều rộng và sau đó chưng cất từ các mạng thích ứng chiều rộng đến các mạng thích ứng độ sâu. Cả mạng thích ứng chiều rộng và độ sâu đều có một tập hợp chiều rộng và độ sâu được xác định trước hạn chế cho các mô hình con. Mặc dù cả OFA và DynaBERT đều cho thấy kết quả thành công, các giải pháp của chúng khó có thể áp dụng cho các LLM đa tỷ tham số vì quy trình huấn luyện đa giai đoạn phức tạp và yêu cầu tìm kiếm và chưng cất kiến thức của chúng. SortedNet (Valipour et al., 2023) là một phương pháp gần đây hình thành và huấn luyện các mô hình con của một mạng theo cách được sắp xếp mà không yêu cầu bất kỳ tìm kiếm nào trong quá trình huấn luyện hoặc suy luận. SortedNet đã cho thấy hiệu suất vượt trội so với các phương pháp được đề cập trước đây về mặt đơn giản, hiệu suất, khả năng mở rộng và tổng quát hóa. Xem xét những lợi ích này, chúng tôi nhắm đến việc triển khai thuật toán huấn luyện SortedNet để phát triển các LLM nhiều-trong-một.

Mô hình Ngôn ngữ Lớn Nhiều-trong-Một (LLM) Các mô hình ngôn ngữ lớn gần đây đã thu hút sự chú ý đáng kể trong tài liệu (Touvron et al., 2023a; Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Ouyang et al., 2022). Trong thực tế, các LLM này phục vụ người dùng với các tác vụ, kỳ vọng và yêu cầu ngân sách tính toán khác nhau (Sun et al., 2022). Có hai loại phương pháp thích ứng để làm cho LLM phù hợp với yêu cầu của khách hàng: đầu tiên là cái gọi là tinh chỉnh hiệu quả tham số (PEFT), và thứ hai là nén mô hình. Trong PEFT, mô hình xương sống cốt lõi vẫn giữ nguyên, và chúng ta chỉ cập nhật các tham số adapter nhỏ hơn nhiều (ví dụ: LoRA (Hu et al., 2021), KRONA (Edalati et al., 2022), Adapter (Houlsby et al., 2019; Pfeiffer et al., 2020), DyLoRA (Valipour et al., 2022), Ladder Side-Tuning (Sung et al., 2022)) và Compacter (Karimi Mahabadi et al., 2021). Trong nén mô hình, mô hình lớn hơn được nén bằng cách sử dụng bất kỳ giải pháp nén mô hình nào như chưng cất kiến thức (Hinton et al., 2015; Hsieh et al., 2023; Wu et al., 2023), cắt tỉa (Bansal et al., 2023), và lượng tử hóa (Prato et al., 2019; Dettmers et al., 2023), một khảo sát liên quan tốt có thể được tìm thấy trong (Zhu et al., 2023). Mặc dù các giải pháp PEFT khá phổ biến với LLM, chúng không cung cấp LLM có kích thước động. Các giải pháp nén mô hình có thể cung cấp các mô hình với kích thước khác nhau, nhưng chúng cần huấn luyện từng mô hình nén riêng biệt, và chúng không phải là các mô hình nhiều-trong-một.

Theo hiểu biết tốt nhất của chúng tôi, công việc này là nỗ lực đầu tiên để thu được các LLM tạo sinh nhiều-trong-một bằng cách áp dụng Tinh chỉnh Sắp xếp cho mô hình LLaMA 13B. Xem xét những lợi ích của các mạng nhiều-trong-một và ứng dụng ngày càng tăng của LLM, chúng tôi hy vọng rằng điều này sẽ giúp cộng đồng xây dựng các mô hình ngôn ngữ lớn hiệu quả hơn có thể được triển khai thích ứng trong quá trình suy luận bằng các phương pháp như SoFT.

3 Phương pháp

Bài báo này tập trung vào việc làm cho các LLM tạo sinh trở thành nhiều-trong-một bằng cách khai phá tiềm năng của các lớp trung gian thông qua phương pháp SortedNet (Valipour et al., 2023).

Hãy xem xét một mô hình ngôn ngữ f(x;θ) với các tham số θ và đầu vào x. Sau đây là quy trình huấn luyện được sắp xếp:

Hình thành Mạng Con Đầu tiên, chúng ta cần hình thành các mạng con của LLM. Vì sự đơn giản và không mất tính tổng quát, chúng tôi tập trung vào các mạng con theo chiều sâu. Giả sử mạng con fn(x;θn) đề cập đến n lớp đầu tiên của f(x;θ). Trong bài báo này, mô hình ngôn ngữ được xem xét là LLaMA2 13B. Vì LLaMA2 bao gồm 40 lớp, chúng tôi định nghĩa các mạng con là n∈B={12,16,20,24,28,32,36,40}.

Tính toán Đầu ra của Mạng Con Đầu ra của mỗi mô hình con sẽ được dự đoán bằng cách sử dụng đầu dự đoán đầu ra được chia sẻ từ lớp cuối cùng (mạng gốc). Hãy nhớ rằng trong mô hình LLaMA, có một lớp RMSNorm (Zhang và Sennrich, 2019) trước đầu dự đoán đầu ra. RMSNorm này được thêm vào trước đầu dự đoán được chia sẻ của mỗi mô hình con. Việc chuẩn hóa này có thể là một yếu tố quan trọng giúp Sorted LLaMA tổng quát hóa tốt hơn cho tất cả các mô hình con.

Hàm Mục tiêu Gọi Ln(x;θn) là tổn thất cho mô hình con thứ n cho batch đầu vào x. Để huấn luyện mạng, chúng tôi định nghĩa tổn thất là tổng của các tổn thất của tất cả các mô hình con này:

L=∑n∈BLn(x;θn)/|B|                    (1)

Đối với các thí nghiệm được thực hiện trong bài báo, |B| = 8. Lưu ý rằng các mô hình con này có các tham số được chia sẻ thông qua kiểu lồng nhau tức là θ1⊂θ2...⊂θn.

Bộ dữ liệu Huấn luyện Chúng tôi sử dụng bộ dữ liệu Stanford Alpaca (Taori et al., 2023), bao gồm các minh họa của 52K ví dụ theo dõi hướng dẫn. Chúng tôi cũng sử dụng điểm chuẩn TriviaQA miền mở QA (Joshi et al., 2017) bao gồm 110K cặp câu hỏi-trả lời sách đóng.

Đánh giá Trong bài báo này, ngoài việc nhúng lớp cuối cùng, chúng tôi đánh giá chất lượng của các nhúng của đầu ra trung gian từ khối 1 đến n. Điểm chuẩn PandaLM (Wang et al., 2023) so sánh đầu ra của các mô hình con khác nhau. PandaLM triển khai một mô hình ngôn ngữ lớn (LLaMA 7b được Tinh chỉnh) để đánh giá chất lượng của văn bản được tạo từ hai nguồn. PandaLM cung cấp một tập xác thực gồm 170 hướng dẫn, để đánh giá các mô hình mục tiêu cho các tác vụ theo dõi hướng dẫn. Để đảm bảo rằng thứ tự của các phản hồi của mô hình không ảnh hưởng đến phán đoán của bộ đánh giá PandaLM, chúng tôi báo cáo điểm số trung bình trong cả hai tình huống Mô hình 1 trước và Mô hình 2 trước. Đầu ra của đánh giá PandaLM là số lần thắng, ký hiệu là W, số lần thua, ký hiệu là L, và số lần hòa, ký hiệu là T, trong tập xác thực. Điểm số cuối cùng được báo cáo đã được tính bằng công thức sau:

Score = (W-L)/T                        (2)

Điểm số cuối cùng là một số giữa -1 và 1, trong đó 1 đại diện cho tỷ lệ thắng cao và -1 có nghĩa là hiệu suất kém của mô hình.

Chúng tôi sử dụng độ chính xác (khớp chính xác) làm thước đo đánh giá cho điểm chuẩn TriviaQA.

Đường cơ sở Mục tiêu chính của LLM trong bài báo này là tuân theo các hướng dẫn được cung cấp bởi một truy vấn. Do đó, theo thiết lập của Alpaca (Taori et al., 2023), chúng tôi tinh chỉnh LLaMA2 13B trên Bộ dữ liệu Stanford Alpaca với hai thiết lập: (1) Tinh chỉnh Tiêu chuẩn thường xuyên (SFT) như đường cơ sở, tập trung chỉ vào việc huấn luyện lớp cuối cùng của mạng như thực hành phổ biến trong tài liệu; (2) Tinh chỉnh Sắp xếp (SoFT), tính toán tổn thất cho nhiều đầu ra từ lớp 12 đến lớp 40 (lớp cuối cùng) với bốn khoảng, và huấn luyện nhiều mô hình đồng thời như được giải thích trong phần trước.

4 Thí nghiệm

Phần này đi sâu vào các chi tiết cụ thể của thí nghiệm và phân tích được cung cấp để hiểu rõ hơn về tác động của Tinh chỉnh Sắp xếp đối với hiệu suất của một mô hình ngôn ngữ lớn như LLaMA2 (Touvron et al., 2023b). Chi tiết của thiết lập thí nghiệm được sử dụng cho các thí nghiệm này có sẵn trong phụ lục A.1. Trước khi đi vào kết quả, chúng tôi sẽ định nghĩa một số ký hiệu mà chúng tôi đã sử dụng cho các thiết lập khác nhau trong các thí nghiệm của chúng tôi:

• SoFT/SFT: Chúng tôi đầu tiên huấn luyện mô hình với các mô hình SoFT hoặc SFT và sử dụng các mô hình con sau khi huấn luyện mà không có bất kỳ huấn luyện thêm nào của đầu mô hình ngôn ngữ cho các lớp trung gian.

• SFT+Tinh chỉnh Bộ phân loại Trung gian (ICT): Chúng tôi đầu tiên huấn luyện mô hình với mô hình SFT và sau đó tinh chỉnh thêm đầu mô hình ngôn ngữ độc quyền cho mỗi mô hình con trong khi giữ trọng số của chúng cố định. SFT+ICT cũng được biết đến như Early-Exit (Xin et al., 2020) trong tài liệu.

• Tinh chỉnh Được Trích xuất: Chúng tôi trích xuất các mô hình con từ các trọng số đã học của mô hình được huấn luyện trước gốc và huấn luyện từng mô hình con riêng biệt.

4.1 Tác động của việc sắp xếp thông tin qua các lớp của mô hình tạo sinh là gì?

Như đã đề cập trước đây, chúng tôi đã tạo ra các phản hồi cho tất cả các lớp n∈B cho cả hai mô hình được huấn luyện dựa trên SFT và SoFT. Sau đó, chúng tôi tiến hành so sánh theo cặp giữa tất cả các mô hình con trong hai mô hình được huấn luyện bằng cách sử dụng bộ đánh giá PandaLM. Như các kết quả đề xuất trong Hình 1, huấn luyện được sắp xếp đáng kể khai phá tiềm năng của các lớp trung gian trong việc tạo ra đầu ra mong muốn. Một số ví dụ được tạo có thể được tìm thấy trong Bảng 1.

Sorted LLaMA (aka SoFT) đang vượt trội so với tinh chỉnh thường xuyên (SFT) trong gần như tất cả các so sánh lớp với một biên độ có ý nghĩa, như được thể hiện thông qua đánh giá tự động trong Hình 1.

Có thể lưu ý rằng hiệu suất Lớp 12 của SFT hơi tốt hơn so với Lớp 12 của Sorted LLaMA. Chúng tôi cho rằng điều này xảy ra bởi vì các đầu ra của các lớp đầu trong SFT chủ yếu là vô nghĩa (xem Bảng 1 như một ví dụ), và bộ đánh giá PandaLM đã không được huấn luyện trên dữ liệu như vậy. Do đó, kết quả đánh giá tự động cho lớp này không có ý nghĩa. Để điều tra thêm lý do đằng sau kết quả cho các mô hình con đầu, chúng tôi đã tiến hành đánh giá của con người trên 6 ô của hai bảng trong Hình 1 (Lớp 12 của SFT và SFT+ICT so với Lớp 12,16, và 20 SoFT) để xác minh tuyên bố của chúng tôi. Chúng tôi quan sát thấy rằng các mô hình con đầu SoFT có thể vượt trội đáng kể so với mô hình con lớp 12 của cả mô hình SFT và SFT+ICT, chứng minh tác động tiêu cực của văn bản vô nghĩa đối với hiệu suất bộ đánh giá PandaLM. Khi chúng ta đi đến các lớp cao hơn trong SFT, văn bản được tạo trở nên có ý nghĩa, điều này làm cho việc so sánh với đối tác lớp Sorted LLaMA trở nên hợp lý hơn.

Hơn nữa, để cải thiện kết quả SFT, lấy cảm hứng từ Early-Exit (Xin et al., 2020), chúng tôi cũng thử tình huống trong đó một đầu bộ phân loại riêng biệt được dành riêng cho tất cả các mô hình con của SFT. Phương pháp này đã được giới thiệu trong phần ký hiệu như SFT+ICT. Các đầu phân loại này đã được huấn luyện một epoch bổ sung sau khi tinh chỉnh SFT trong khi giữ mô hình cơ sở cố định. Lưu ý rằng cài đặt này gặp phải overhead bộ nhớ đáng kể trong quá trình tinh chỉnh và suy luận so với phương pháp SoFT của chúng tôi. Thực tế, số lượng tham số bổ sung cho SFT+ICT (Early Exit) là |B| −1×D×V, trong đó |B| là số lượng mô hình con, D là kích thước ẩn của mô hình, và V là kích thước từ vựng. Đối với LLaMA 2 13B, điều này tương đương với 1B tham số bổ sung.

Kết quả so sánh được sắp xếp với early exit được hiển thị trong hình 1 (Trái). Mặc dù có nhiều tham số hơn, SFT+ICT (Early-Exit) hoạt động kém hơn so với tinh chỉnh được sắp xếp của chúng tôi cho hầu hết các mô hình con. Theo kết quả, mô hình con trong Sorted LLaMA với 36 lớp hoạt động gần như tốt bằng tinh chỉnh thường xuyên của mô hình kích thước đầy đủ. Điều này thể hiện khả năng ấn tượng của mô hình đề xuất của chúng tôi để tạo ra các mô hình con nhỏ mạnh mẽ hoạt động tương tự như mô hình gốc.

Một thí nghiệm khác đã được tiến hành trong phụ lục A.3, điều tra thêm tác động của thời gian huấn luyện dài hơn cho SoFT. Kết quả cho thấy rằng mô hình của chúng tôi vẫn đang được huấn luyện thiếu, và chúng tôi có thể quan sát một cải thiện đáng kể trong hiệu suất Sorted LLaMA với thời gian huấn luyện dài hơn.

Hơn nữa, chúng tôi so sánh hiệu suất của các mô hình con Sorted LLaMA với khả năng thực tế của các mô hình này bằng cách tinh chỉnh các mô hình con riêng biệt và báo cáo kết quả trong cả thời gian huấn luyện bằng nhau và thời gian huấn luyện nhiều hơn cho SoFT. Chúng tôi trích xuất 4 mô hình con (Lớp 12, Lớp 20, Lớp 28, và Lớp 36) và mỗi lần tinh chỉnh đầy đủ mô hình con được trích xuất riêng biệt trong hai epoch trên bộ dữ liệu Alpaca. Hình 2 và Bảng 9 cho thấy sự so sánh giữa Tinh chỉnh Được Trích xuất và các mô hình con SoFT. Phần đầu tiên trong Bảng 9 cho thấy thiết lập ngân sách huấn luyện bằng nhau (2 Epoch) so sánh trong đó SFT thể hiện hiệu suất hơi tốt hơn so với các mô hình con SoFT tương tự. Huấn luyện thêm SoFT sẽ dẫn đến các mô hình con được sắp xếp tốt hơn trong đó SoFT vượt trội so với các mô hình con được tinh chỉnh đầy đủ, chứng minh tác động tích cực của SoFT đối với hiệu suất của các mô hình con thấp hơn.

Phân tích overhead tính toán của SoFT có thể được tìm thấy trong A.2.

4.2 SoFT hoạt động như thế nào đối với các miền khác?

Chúng tôi đánh giá thêm Sorted LLaMA trong một miền khác với việc theo dõi hướng dẫn, chọn điểm chuẩn TriviaQA (Joshi et al., 2017) để đánh giá hiệu suất của các mô hình con trong trả lời câu hỏi miền mở sách đóng.

Hình 3 cho thấy hiệu suất của SoFT và ba đường cơ sở (SFT, Tinh chỉnh Được Trích xuất và SFT+ICT) trong các checkpoint khác nhau thông qua quy trình huấn luyện trên điểm chuẩn TriviaQA. Các mô hình con SoFT cho thấy hiệu suất vượt trội đáng kể so với các đối tác SFT và SFT+ICT trong tất cả các mô hình con. Tương tự như PandaLM, khoảng cách giữa hiệu suất mô hình đầy đủ SoFT và SFT là nhỏ trong TriviaQA, điều này có thể nhấn mạnh khả năng SoFT trong việc duy trì hiệu suất mô hình đầy đủ so với SFT. Chúng tôi cũng đã thực hiện Tinh chỉnh Được Trích xuất trên các mô hình con trung gian trong 2 Epoch và kết quả chứng minh hiệu suất gần của các lớp trung gian SoFT với các đối tác Tinh chỉnh Được Trích xuất.

4.3 SoFT có thể tăng tốc tạo văn bản như thế nào?

Cải thiện Lấy mẫu Suy đoán Suy luận Suy đoán (SD) là một kỹ thuật được giới thiệu bởi (Chen et al., 2023) để tăng tốc độ giải mã văn bản trong các mô hình lớn. Phương pháp này sử dụng một mô hình mục tiêu lớn và các mô hình bản thảo nhỏ hơn để tạo token nhanh hơn. Chúng ta có thể xác minh các token được tạo bởi mô hình lớn song song. Chúng tôi đã sử dụng cùng một mô hình cho Sorted LLaMA như chúng tôi đã sử dụng các mô hình con trước đó như bản thảo và mô hình kích thước đầy đủ như mô hình mục tiêu. Vì các tham số đã được chia sẻ giữa các mô hình lớn và bản thảo trong thiết lập này, chúng ta có thể tránh bất kỳ overhead bộ nhớ bổ sung nào, không giống như Lấy mẫu Suy đoán tiêu chuẩn. Bảng 2 báo cáo kết quả suy luận của việc sử dụng giải mã suy đoán trên điểm chuẩn Alpaca và TriviaQA trong SoFT bằng cách sử dụng ba mô hình con khác nhau như bản thảo (Lớp 12, 16, và 20).

Như được hiển thị, việc kết hợp Giải mã suy đoán và Sorted LLaMA có thể tăng tốc tạo token lên đến 1.16× so với giải mã tự hồi quy bình thường trong PandaLM với sự sụt giảm hiệu suất không đáng kể. Do độ dài trung bình ngắn của câu trả lời trong TriviaQA, giải mã suy đoán không dẫn đến tăng tốc vì quá trình tạo bản thảo không tìm thấy cơ hội nào để tăng tốc suy luận.

Suy luận Động Nhận thức Thể hiện Chúng tôi cũng sử dụng động các mô hình con SoFT để tăng tốc độ tạo văn bản trong quá trình suy luận. Dựa trên độ tin cậy của các token được dự đoán của mô hình con, chúng tôi quyết định mô hình con nào nên tạo token nào. Với mỗi token, các mô hình con sẽ xử lý token theo thứ tự kích thước (đầu tiên là mô hình con nhỏ nhất 12, sau đó là 16, v.v.). Bất cứ nơi nào trong quy trình này, độ tin cậy của token được dự đoán bởi một mô hình con đạt đến một ngưỡng được xác định trước, token được dự đoán sẽ được chọn làm token tiếp theo và thoát khỏi mô hình. Chúng tôi cũng đã triển khai một cơ chế bộ nhớ cache thích ứng để sử dụng bộ nhớ cache KV trong tình huống không tầm thường này nơi mỗi token có thể thoát từ một lớp khác nhau. Bảng 2 cho thấy rằng Suy luận Động Nhận thức Thể hiện có thể tăng tốc phương pháp tự hồi quy bình thường trong tất cả các điểm chuẩn lên đến 1.34× trong PandaLM và 1.12× trong TriviaQA. Hơn nữa, suy luận động có thể dẫn đến hiệu suất tốt hơn trong PandaLM và TriviaQA so với giải mã suy đoán.

4.4 Phân tích

4.4.1 So sánh giữa phân phối xác suất đã học của SoFT và SFT

Tinh chỉnh được sắp xếp nhằm làm cho hiệu suất của các mô hình con tương tự như mô hình đầy đủ. Để khám phá hiệu quả của SoFT trong việc thu hẹp khoảng cách giữa các mô hình con và mô hình đầy đủ trong tác vụ theo dõi hướng dẫn, chúng tôi đo lường sự tương tự giữa phân phối xác suất của mỗi token trong mỗi mô hình con so với mô hình đầy đủ bằng cách sử dụng phân kỳ Kullback–Leibler (KL). Hình 4 (Trái) so sánh phân phối xác suất của các mô hình con Sorted LLaMA và SFT ở các vị trí đầu ra khác nhau.

Hình 4a (Trái) so sánh các lớp SFT khác nhau và lớp cuối cùng Sorted LLaMA. Hình này cho thấy rằng chỉ có phân phối đầu ra kích thước đầy đủ của SFT gần với mô hình kích thước đầy đủ được sắp xếp, trong khi phân phối của các lớp khác phân kỳ nhanh hơn trong các bước đầu tiên so với SoFT. Điều này được mong đợi vì đầu mô hình ngôn ngữ không quen với biểu diễn đã học của các lớp giữa trong SFT. Trong phần tiếp theo, chúng tôi so sánh các biểu diễn đã học của các mô hình con khác nhau để hiểu rõ hơn tác động của SoFT.

Hình 4b (Trái) so sánh phân phối đầu ra của tất cả các lớp được sắp xếp với lớp SFT cuối cùng. So với Hình 4a (Trái), Hình 4b (Trái) Sorted LLaMA có thể bảo tồn phân phối đầu ra gần với mô hình kích thước đầy đủ SFT ngay cả trong các lớp thấp hơn cho các token đầu ra ban đầu.

Sự so sánh giữa lớp cuối cùng và các lớp 12 đến 36 trong mô hình SFT được hiển thị trong Hình 5a (Trái). Rõ ràng từ hình này rằng phân phối đầu ra phân kỳ nhanh chóng so với lớp cuối cùng sau khi tạo một vài token ban đầu, ngay cả trong các lớp cao hơn như 36 và 32. Điều quan trọng cần lưu ý rằng đánh giá này được tạo ra mà không điều chỉnh đầu bộ phân loại.

Cuối cùng, Hình 5b (Trái) chứng minh rằng trong Sorted LLaMA, phân phối khả năng của kết quả được tạo ra ngày càng trở nên tương tự hơn với mô hình kích thước đầy đủ khi chúng ta tiến gần hơn đến lớp cuối cùng.

4.4.2 So sánh giữa biểu diễn đã học của SoFT và SFT

Trong quá trình tinh chỉnh thường xuyên, không có kết nối giữa đầu mô hình ngôn ngữ và các mô hình con có thể tăng cường sự phân kỳ của phân phối xác suất trong Hình 4 (Trái). Để khắc phục điều này, chúng tôi tiến hành một thí nghiệm khác để so sánh biểu diễn trạng thái ẩn trong các lớp cuối cùng và giữa ngay trước khi chuyển các trạng thái ẩn đến đầu mô hình ngôn ngữ. Hình 4 (Phải) so sánh biểu diễn trạng thái ẩn đã học của các mô hình con SFT và Sorted LLaMA ở các vị trí khác nhau trong đầu ra. Điều này sẽ làm cho phân tích độc lập với đầu mô hình ngôn ngữ. Chúng tôi sử dụng độ tương tự cosine để đo lường sự khác biệt giữa hai biểu diễn. Như được hiển thị bằng các bản đồ nhiệt, độ tương tự cosine có tương quan cao với so sánh KL-Divergence được giải thích trong phần trước.

Hình 4a (Phải) so sánh tất cả các mô hình con SFT với lớp cuối cùng Sorted về độ tương tự biểu diễn ẩn. Một lần nữa, tương tự như phân tích phân phối xác suất, độ tương tự giữa mô hình con SFT và lớp cuối cùng Sorted có xu hướng mờ dần ngay lập tức sau khi tạo ra vài token đầu tiên, trong khi Hình 4b chứng minh khả năng của các mô hình con Sorted LLaMA trong việc bảo tồn các biểu diễn đã học gần tương tự với các trạng thái ẩn lớp cuối cùng SFT.

Hình 5a (Phải) mô tả bản đồ nhiệt của độ tương tự cosine trạng thái ẩn giữa các mô hình con SFT khác nhau so với lớp cuối cùng SFT. Tương tự như biểu đồ bên trái của nó, độ tương tự nhanh chóng giảm sau vài token, và sự mờ dần này đáng kể hơn trong các lớp trước đó.

Mặt khác, Hình 5b (Phải) cho thấy rằng các biểu diễn của các mô hình con Sorted vẫn tương tự với lớp cuối cùng Sorted ngay cả sau khi tạo nhiều token ban đầu.

5 Kết luận

Công việc này trình bày sorted LLaMA, một mô hình ngôn ngữ nhiều-trong-một cho suy luận động thu được bằng cách sử dụng Tinh chỉnh Sắp xếp (SoFT) thay vì Tinh chỉnh Tiêu chuẩn. Sorted LLaMA khai phá khả năng tiềm năng của các lớp trung gian, cung cấp sự thích ứng động mà không cần huấn luyện trước hoặc chi phí bổ sung liên quan đến nén mô hình. Nó trình bày một hướng đi đầy hứa hẹn để tối ưu hóa các mô hình ngôn ngữ tạo sinh trong NLP. Phương pháp của chúng tôi làm cho việc triển khai các mô hình này hiệu quả hơn. Vì tất cả các mô hình con vẫn là các thành phần tích hợp của mô hình gốc, gánh nặng của yêu cầu lưu trữ và chi phí chuyển đổi giữa các nhu cầu tính toán khác nhau được giảm thiểu, làm cho việc quản lý nhiều mô hình trong quá trình suy luận trở thành một thực tế thiết thực.

Đánh giá có hệ thống của chúng tôi về các điểm chuẩn theo dõi hướng dẫn và trả lời câu hỏi đã thách thức sự khôn ngoan thông thường bằng cách trao quyền cho các lớp giữa tạo ra kết quả chất lượng cao. Điều này, đến lượt nó, cho phép suy luận động của LLM với một phương pháp tinh chỉnh rất hiệu quả (SoFT), cuối cùng tối ưu hóa việc sử dụng LLM. Kết quả khuyến khích của chúng tôi cho thấy khả năng đầy hứa hẹn của SortedNet (Valipour et al., 2023) để huấn luyện nhiều mô hình ngôn ngữ với kích thước khác nhau cùng một lúc.

6 Hạn chế

Mặc dù cho thấy hiệu quả của phương pháp SortedNet cho các mô hình ngôn ngữ lớn, nghiên cứu thêm là cần thiết để hiểu rõ hơn về phạm vi ứng dụng của nó trong LLM. Ví dụ, áp dụng phương pháp này trong quá trình huấn luyện trước, sắp xếp các chiều mô hình khác như các đầu attention và chiều ẩn, và điều tra tác động của việc chọn một kiến trúc cụ thể có thể cung cấp các hướng nghiên cứu tiềm năng trong tương lai. Nghiên cứu của chúng tôi có thể hơi thiên về đánh giá tự động, đòi hỏi điều tra thêm thông qua đánh giá của con người.

Lời cảm ơn

Chúng tôi cảm ơn Mindspore, một framework tính toán học sâu mới, cho sự hỗ trợ một phần của công việc này.

Tài liệu tham khảo

[Danh sách các tài liệu tham khảo được dịch theo format tương tự như bản gốc...]

A Phụ lục

A.1 Thiết lập Thí nghiệm

[Phần phụ lục được dịch chi tiết...]

[Các bảng và hình ảnh được duy trì nguyên layout và format...]
