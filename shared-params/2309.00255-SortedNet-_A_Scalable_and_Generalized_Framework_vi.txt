SortedNet: Khung Công Tác Có Thể Mở Rộng và Tổng Quát để Huấn Luyện Mạng Nơ-ron Sâu Mô-đun

Mojtaba Valipour1,2, Mehdi Rezagholizadeh2, Hossein Rajabzadeh1,2,
Parsa Kavehzadeh2,Marzieh Tahaei2,Boxing Chen2,Ali Ghodsi1
1Đại học Waterloo,2Phòng thí nghiệm Noah's Ark của Huawei

Tóm tắt
Mạng nơ-ron sâu (DNNs) phải phục vụ đa dạng người dùng với những nhu cầu hiệu suất và ngân sách khác nhau, dẫn đến thực tiễn tốn kém việc huấn luyện, lưu trữ và duy trì nhiều mô hình cụ thể cho người dùng/tác vụ. Có những giải pháp trong tài liệu để giải quyết vấn đề với các mô hình động đơn lẻ hoặc nhiều-trong-một thay vì nhiều mạng riêng lẻ; tuy nhiên, chúng gặp phải sự sụt giảm hiệu suất đáng kể, thiếu khái quát hóa trên các kiến trúc mô hình khác nhau hoặc các chiều khác nhau (ví dụ: độ sâu, độ rộng, khối attention), yêu cầu tìm kiếm mô hình nặng trong quá trình huấn luyện và huấn luyện số lượng hạn chế các mô hình con. Để giải quyết những hạn chế này, chúng tôi đề xuất SortedNet, một giải pháp huấn luyện tổng quát và có thể mở rộng để khai thác tính mô-đun vốn có của DNNs. Nhờ vào kiến trúc lồng nhau tổng quát (mà chúng tôi gọi là kiến trúc được sắp xếp trong bài báo này) với các tham số chia sẻ và sơ đồ cập nhật mới kết hợp việc lấy mẫu ngẫu nhiên mô hình con và cơ chế tích lũy gradient mới, SortedNet cho phép huấn luyện các mô hình con đồng thời cùng với việc huấn luyện mô hình chính (không có bất kỳ chi phí huấn luyện hoặc suy luận bổ sung đáng kể nào), đơn giản hóa việc lựa chọn mô hình động, tùy chỉnh triển khai trong quá trình suy luận và giảm đáng kể yêu cầu lưu trữ mô hình. Tính linh hoạt và khả năng mở rộng của SortedNet được xác thực thông qua các kiến trúc và tác vụ khác nhau bao gồm LLaMA, BERT, RoBERTa (tác vụ NLP), ResNet và MobileNet (phân loại hình ảnh) thể hiện sự vượt trội so với các phương pháp huấn luyện động hiện có. Ví dụ, chúng tôi giới thiệu một phương pháp tự suy đoán thích ứng mới dựa trên huấn luyện có sắp xếp để tăng tốc giải mã mô hình ngôn ngữ lớn. Hơn nữa, SortedNet có thể huấn luyện tới 160 mô hình con cùng một lúc, đạt được ít nhất 96% hiệu suất của mô hình gốc.

1 Giới thiệu
Mạng nơ-ron sâu (DNNs) ngày càng thu hút sự quan tâm và trở nên phổ biến hơn Sarker [2021]. Sự phổ biến này dẫn đến nhu cầu và yêu cầu ngày càng tăng từ người dùng mà các mô hình này phải đáp ứng. Người dùng tiền huấn luyện hoặc tinh chỉnh nhiều mô hình với các kích thước khác nhau để giải quyết nhu cầu hiệu suất và tính toán của các tác vụ và thiết bị đích của họ (với bộ nhớ và sức mạnh tính toán khác nhau cho dù được triển khai trên đám mây hay thiết bị biên). Tuy nhiên, việc phát triển, lưu trữ, duy trì và triển khai nhiều mô hình riêng lẻ cho tập hợp người dùng đa dạng có thể rất khó khăn và tốn kém Devvrit et al. [2023]. Hơn nữa, trong kỷ nguyên của các mô hình tiền huấn luyện khổng lồ Devlin et al. [2018], Liu et al. [2019] và các mô hình ngôn ngữ lớn Brown et al. [2020], Chowdhery et al. [2022], nhu cầu tính toán có thể thay đổi đáng kể từ tác vụ này sang tác vụ khác. Do đó, có nhu cầu ngày càng tăng cho các mô hình có thể tự thích nghi với các điều kiện động, trong khi mạng nơ-ron thông thường sẽ không thể giải quyết những trường hợp như vậy Xin et al. [2020], Yu et al. [2018a].

Mặt khác, DNNs thể hiện kiến trúc mô-đun theo các chiều khác nhau, như các lớp và khối theo độ sâu, và các nơ-ron, kênh và đầu attention theo độ rộng. Tính mô-đun vốn có này cho phép trích xuất các mô hình con với hình dạng tương tự như mô hình gốc. Tuy nhiên, tính mô-đun này chưa được triển khai trong các phương pháp huấn luyện thông thường, và do đó, hiệu suất của các mô hình con kém hơn so với mô hình chính. Do đó, thách thức nằm ở việc khai thác toàn bộ tiềm năng của tính mô-đun trong mạng nơ-ron sâu, cho phép sử dụng hiệu quả các mô hình con để nâng cao hiệu suất của chúng và cho phép triển khai thực tế trong các tình huống thực tế.

Thay vì huấn luyện các mô hình riêng lẻ, chúng ta có thể tận dụng các mô hình con của DNNs và huấn luyện chúng cùng với các mô hình chính để có được mạng nhiều-trong-một với các mô hình con có thể được sử dụng cho các tác vụ khác nhau. Có nhiều phương pháp trong tài liệu để huấn luyện các mô hình con Cai et al. [2020], Xin et al. [2020], Hou et al. [2020]. Những kỹ thuật này tuy hiệu quả nhưng có những thiếu sót nhất định: thường sử dụng quy trình huấn luyện phức tạp kết hợp với chưng cất kiến thức (cần phải huấn luyện một mô hình giáo viên riêng biệt) Hou et al. [2020], yêu cầu sửa đổi kiến trúc Nunez et al. [2023], chỉ hoạt động cho các kiến trúc cụ thể Cai et al. [2019], không thể xử lý nhiều hơn một số rất nhỏ các mô hình con, cần tìm kiếm mô hình nặng (ví dụ: tìm kiếm kiến trúc mạng nơ-ron) trong quá trình huấn luyện hoặc suy luận Cai et al. [2020], liên quan đến tối ưu hóa mô hình con dư thừa Fan et al. [2019], hoặc cho thấy hiệu suất kém cho mô hình chính hoặc mô hình con Xin et al. [2020].

Để giải quyết những vấn đề này, chúng tôi đề xuất SortedNet, một giải pháp huấn luyện tổng quát và có thể mở rộng để khai thác tính mô-đun vốn có của DNNs trên các chiều khác nhau. Như tên của phương pháp chúng tôi ngụ ý, nó chọn các mô hình con theo cách được sắp xếp (một phiên bản tổng quát của kiến trúc lồng nhau) trong mô hình chính để tránh tìm kiếm nặng trong hoặc sau khi huấn luyện. Trái ngược với các mô hình lồng nhau trong đó các mô hình con nhỏ hơn luôn được bao bọc hoàn toàn bởi các mô hình con lớn hơn, phiên bản được sắp xếp tổng quát của chúng tôi nới lỏng ràng buộc lồng nhau nhưng gắn nguồn gốc của các mô hình con với nguồn gốc của mô hình chính trên bất kỳ chiều đích nào (để biết thêm chi tiết xem phần 3.1).

Cấu hình được sắp xếp này với các tham số chia sẻ thực thi trật tự thường xuyên và tính nhất quán trong kiến thức được học bởi các mô hình con. Một tùy chọn để sắp xếp các mô hình con là dựa trên yêu cầu tính toán và độ chính xác của chúng, điều này sẽ cho phép chúng ta trích xuất các mô hình con mong muốn mà không yêu cầu tìm kiếm rộng rãi tại thời điểm kiểm tra. Việc sử dụng thứ tự sắp xếp xác định trước đảm bảo rằng mỗi mô hình con được nhắm tới sở hữu chi phí tính toán duy nhất, loại bỏ hiệu quả việc tối ưu hóa các mô hình con dư thừa khỏi quá trình huấn luyện.

Để huấn luyện các mô hình con được sắp xếp, chúng tôi đề xuất một sơ đồ cập nhật mới kết hợp việc lấy mẫu ngẫu nhiên các mô hình con với tích lũy gradient. Chúng tôi đã thử nghiệm giải pháp SortedNet thành công trên các kiến trúc và tác vụ khác nhau như mô hình ngôn ngữ lớn dựa trên bộ giải mã LLaMA (13B) Touvron et al. [2023] trên tác vụ lý luận toán học GSM8K Cobbe et al. [2021], các mô hình dựa trên bộ mã hóa BERT Devlin et al. [2018] và RoBERTa Liu et al. [2019] trên tập hợp các tác vụ hiểu ngôn ngữ GLUE Wang et al. [2018], ResNet He et al. [2015] và MobileNet Sandler et al. [2018] trên tác vụ phân loại hình ảnh CIFAR-10. Các nghiên cứu thực nghiệm toàn diện của chúng tôi trên các kiến trúc, tác vụ và tính động khác nhau dọc theo các chiều khác nhau từ độ rộng và độ sâu đến đầu attention và lớp nhúng cho thấy sự vượt trội và khả năng tổng quát hóa của phương pháp đề xuất so với các phương pháp huấn luyện động hiện đại. Hơn nữa, SortedNet cung cấp một số lợi ích, bao gồm yêu cầu lưu trữ tối thiểu và khả năng suy luận động (tức là chuyển đổi giữa các ngân sách tính toán khác nhau) trong quá trình suy luận.

Để tóm tắt, những đóng góp chính của bài báo này là:
• Giới thiệu một giải pháp nhiều-trong-một để cấu hình các mô hình con theo cách được sắp xếp và huấn luyện chúng đồng thời với một số khía cạnh độc đáo như khả năng mở rộng (huấn luyện nhiều mô hình con), tính tổng quát (CNN, Transformers, độ sâu, độ rộng), và không cần tìm kiếm (không cần tìm kiếm trong quá trình huấn luyện hoặc suy luận giữa các mô hình con) và duy trì hiệu suất cạnh tranh cho mô hình chính.
• Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên hướng tới huấn luyện mô hình nhiều-trong-một dọc theo các chiều khác nhau đồng thời (và thậm chí không phải ở các giai đoạn khác nhau).
• Chúng tôi triển khai huấn luyện có sắp xếp của mình trong việc tăng tốc giải mã của các mô hình ngôn ngữ lớn bằng cách sử dụng phương pháp tự suy đoán có thể dẫn đến tăng tốc suy luận khoảng 2 lần cho LLaMA 13B.
• Thể hiện các lý giải lý thuyết và bằng chứng thực nghiệm về hiệu quả của phương pháp đề xuất. Vượt trội hơn các phương pháp hiện đại trong huấn luyện động trên CIFAR10 Krizhevsky et al. [2009] với việc mở rộng số lượng mô hình con lên 160 và đạt được ít nhất 96% hiệu suất của mô hình gốc. Hơn nữa, cho thấy kết quả thành công trong huấn luyện động của các mô hình BERT, RoBERTa và LLaMA.

2 Công trình liên quan

Trong phần này, chúng tôi xem xét ngắn gọn các công trình hiện có liên quan nhất đến ý tưởng SortedNet của chúng tôi. Một tóm tắt về những giải pháp này và cách chúng khác nhau có thể được tìm thấy trong Bảng 1. Để biết thêm chi tiết, vui lòng tham khảo phụ lục A.

Slimmable Networks Yu et al. [2018b] Slimmable networks là một phương pháp huấn luyện có thể điều chỉnh độ rộng. Nó được đề xuất đặc biệt cho các kiến trúc CNN và do đó, việc xem xét cẩn thận mô-đun chuẩn hóa batch cho các kích thước độ rộng khác nhau là cần thiết. Trái ngược với slimmable networks, SortedNet của chúng tôi bao gồm nhiều kiến trúc hơn và hoạt động trong cả chiều độ sâu và độ rộng.

Early Exit Xin et al. [2020] là một trong những kỹ thuật cơ bản phổ biến nhất, thêm một bộ phân loại vào các lớp trung gian của một mô hình đã được huấn luyện. Các tham số của mô hình chính được đóng băng và các bộ phân loại được cập nhật trong một quy trình tinh chỉnh riêng biệt. Mặc dù giải pháp này tương đối đơn giản, hiệu suất của các mô hình con chậm chạp đáng kể so với mô hình chính.

Dayna-BERT Hou et al. [2020] trình bày một phương pháp nén động cho các mô hình BERT tiền huấn luyện, cho phép điều chỉnh linh hoạt kích thước mô hình, cả về độ sâu và độ rộng, trong quá trình suy luận. DynaBERT khác với chúng tôi ở các khía cạnh sau: đầu tiên, trong DynaBERT, chỉ có một số ít mô hình con có chức năng; thứ hai, DynaBERT yêu cầu một mô hình giáo viên đã được huấn luyện và sử dụng chưng cất kiến thức (KD); thứ ba, DynaBERT cần tìm kiếm để tìm một mô hình con tối ưu; cuối cùng, DynaBERT phụ thuộc vào kiến trúc.

Layer-drop Fan et al. [2019] là một huấn luyện dropout có cấu trúc cho phép cắt tỉa lớp tại thời điểm suy luận. Tương tự như DynaBERT, nó được áp dụng cho các mô hình ngôn ngữ tiền huấn luyện; tuy nhiên, trái ngược với DynaBERT, Layer-drop chỉ nhắm vào độ sâu của mạng nơ-ron chứ không phải độ rộng của chúng.

Once-for-All (OFA) Cai et al. [2020] nhắm vào suy luận hiệu quả trên các thiết bị khác nhau. Nó đầu tiên huấn luyện một mạng hỗ trợ nhiều mô hình con với các đặc tính độ trễ/độ chính xác khác nhau; sau đó nó tìm kiếm giữa các mô hình con khả thi theo yêu cầu độ chính xác và độ trễ của thiết bị đích của chúng. OFA khác với giải pháp của chúng tôi ở: đầu tiên, nó có bản chất huấn luyện tiến bộ trái ngược với tổn thất ngẫu nhiên hoặc tổng hợp của chúng tôi; thứ hai, nó cần giáo viên và KD; thứ ba, nó yêu cầu tìm kiếm kiến trúc mạng nơ-ron (NAS) riêng biệt tại thời điểm suy luận; thứ tư, OFA dành cho các mô hình dựa trên CNN; cuối cùng, nó không có bất kỳ giả định cụ thể nào để cấu hình mô hình con (xem Hình 4 để biết thêm chi tiết).

Learning Compressible Subspace (LCS) Nunez et al. [2023] là một kỹ thuật nén thích ứng dựa trên huấn luyện không gian con có thể nén của mạng nơ-ron. Mặc dù LCS không yêu cầu bất kỳ huấn luyện lại nào tại thời điểm suy luận, giải pháp này có một số hạn chế khác bao gồm: đầu tiên, nó cần gấp đôi bộ nhớ tại thời điểm huấn luyện; thứ hai, việc lựa chọn trọng số ban đầu và hàm nén không rõ ràng và tùy ý (để lại như một siêu tham số); thứ ba, nó chỉ được thử trên CNNs; thứ tư, tương tự như Layer-drop, không gian tìm kiếm của các mô hình con là rất lớn khiến việc huấn luyện không tối ưu.

MatFormer Devvrit et al. [2023] là một giải pháp nhiều-trong-một chỉ tiền huấn luyện dựa trên tổn thất tổng hợp cho các mô hình dựa trên Transformer. MatFormer chỉ hoạt động dọc theo chiều độ rộng của khối FFN trong Transformers và không thể xử lý nhiều hơn một số rất ít mô hình con.

3 Phương pháp đề xuất

3.1 Một góc nhìn tổng quát và có thể mở rộng

Trong phần công trình liên quan, chúng tôi đã thảo luận về một số phương pháp liên quan đến huấn luyện mạng nhiều-trong-một. Những phương pháp này khác nhau về kiến trúc đích, tổn thất huấn luyện, số lượng tham số huấn luyện, cấu hình của các mô hình con (ngẫu nhiên, lồng nhau, hoặc được sắp xếp), số lượng mô hình con được huấn luyện, và sự phụ thuộc vào tìm kiếm hoặc huấn luyện lại trước khi triển khai. Phương pháp SortedNet của chúng tôi có thể được xem như một phiên bản đơn giản, tổng quát và có thể mở rộng của những giải pháp hiện có này. Những lợi ích này chủ yếu xuất phát từ cấu hình được sắp xếp của các mô hình con với các tham số chia sẻ của chúng và huấn luyện ngẫu nhiên của chúng tôi. Để huấn luyện mạng nhiều-trong-một, chúng ta cần chỉ định một số lựa chọn thiết kế: đầu tiên, cách hình thành các mô hình con và cấu hình của chúng; thứ hai, các kiến trúc đích là gì; và thứ ba, cách huấn luyện các mô hình con cùng với mô hình chính.

Thiết kế các mô hình con SortedNet áp đặt một sự thiên lệch quy nạp cho huấn luyện dựa trên giả định rằng các tham số của mô hình con có kiến trúc đồng tâm gắn với nguồn gốc dọc theo mỗi chiều (mà chúng tôi gọi là kiến trúc được sắp xếp). Cấu hình được sắp xếp này với các tham số chia sẻ thực thi một trật tự thường xuyên và tính nhất quán trong kiến thức được học bởi các mô hình con (xem Hình 1).

Kiến trúc được sắp xếp so với lồng nhau Trong công trình này, chúng tôi giới thiệu thuật ngữ kiến trúc được sắp xếp để mở rộng và tổng quát hóa khái niệm kiến trúc lồng nhau. Trái ngược với các mô hình lồng nhau trong đó các mô hình con nhỏ hơn luôn được bao bọc hoàn toàn bởi các mô hình con lớn hơn, các mô hình con được sắp xếp của chúng tôi sẽ được gắn với nguồn gốc (chỉ số bắt đầu) của mỗi chiều một cách độc lập.

Hãy xem xét một mạng nơ-ron nhiều-trong-một f(x;θ(n)) với các tham số θ(n) và đầu vào x được tạo thành từ n mô hình con f(x;θ(i))|n−1 i=0, trong đó θ(i) đại diện cho các trọng số của mô hình con thứ i. Chúng tôi định nghĩa một tập hợp vũ trụ chứa tất cả các mô hình con duy nhất: Θ ={θ(0), θ(1), ..., θ(n)}.

Thiết lập một thứ tự Giả sử rằng chúng ta muốn nhắm vào D={Dim 1, Dim 2, ..., Dim K} nhiều chiều trong-một trong mô hình của chúng ta. Sau đó, hãy bắt đầu với Θ =∅ và xây dựng các mô hình con một cách lặp lại. Trong vấn đề này, tại mỗi lần lặp t trong quá trình huấn luyện, chúng ta có các thủ tục lấy mẫu và cắt ngắn dọc theo bất kỳ chiều được nhắm tới nào:

θ∗t=∩|D|j=1θDim j↓btj(n) trong đó btj∼PBj
Nếu θ∗t/∈Θ:Θ←Θ∪ {θ∗t}(1)

trong đó Dim j↓btj chỉ ra rằng chúng ta đã cắt ngắn θ(n) dọc theo chiều Dim j từ chỉ số 1 đến chỉ số btj tại lần lặp t. btj được lấy mẫu từ một phân phối PBj với tập hỗ trợ Bj={1,2, ..., dj} để tạo thành mô hình con thứ i. dj đề cập đến chỉ số tối đa của chiều thứ j.

Quá trình lặp này sẽ được thực hiện trong quá trình huấn luyện và tập hợp n mô hình con duy nhất Θ sẽ được xây dựng.

Để minh họa quá trình tốt hơn, hãy xem một trường hợp đơn giản như BERT base nơi chúng ta muốn tạo một mạng nhiều-trong-một trên các chiều độ rộng và độ sâu, D={Depth, Width}. Trong trường hợp này, chúng ta có 12 lớp và kích thước chiều ẩn là 768. Giả sử rằng Depth tương ứng với j=1 và Width tương ứng với j=2 trong Eq. 1. Để đơn giản, hãy sử dụng phân phối đồng nhất rời rạc để lấy mẫu các chỉ số trên hai chiều này. Để tạo mô hình con đầu tiên (i=1), chúng ta cần lấy mẫu b11 đồng nhất từ tập hợp các số tự nhiên trong phạm vi từ 1 đến 12: B1={1,2, ...,12}; và chúng ta cần lấy mẫu b12 từ phạm vi từ 1 đến 768: B2={1,2,3, ...,768}. Lưu ý rằng chúng ta thậm chí có thể chọn một tập con của B1 và B2 làm tập hỗ trợ cho phân phối xác suất lấy mẫu. Sau hai lần lấy mẫu này, chúng ta sẽ có hai tập hợp tham số bị cắt ngắn: θDepth ↓b11 và θWidth ↓b12. Giao của hai tham số bị cắt ngắn này sẽ cho chúng ta mô hình con đầu tiên: θ1=θDepth ↓b11∩θWidth ↓b12.

Mô hình huấn luyện Huấn luyện thông thường của mạng nơ-ron liên quan đến việc cải thiện hiệu suất của toàn bộ mô hình và thường việc huấn luyện này không nhận thức được hiệu suất của các mô hình con. Trên thực tế, trong tình huống này, nếu chúng ta trích xuất và triển khai các mô hình con của mô hình lớn đã được huấn luyện trên một tác vụ đích, chúng ta sẽ trải nghiệm sự sụt giảm đáng kể về hiệu suất của những mô hình con này so với mô hình chính. Tuy nhiên trong SortedNet, chúng tôi đề xuất một phương pháp huấn luyện cho phép huấn luyện các mô hình con cùng với mô hình chính theo cách ngẫu nhiên. Mô hình SortedNet dẫn đến những lợi ích sau:

• Trích xuất mô hình con không cần tìm kiếm: sau khi huấn luyện, bằng cách sắp xếp tầm quan trọng của các mô hình con, mô hình con tốt nhất cho một ngân sách nhất định có thể được chọn mà không cần tìm kiếm.
• Bất cứ lúc nào: Mỗi mô hình con nhỏ hơn là một tập con của một mô hình lớn hơn, điều này làm cho việc chuyển đổi giữa các mô hình con khác nhau hiệu quả. Điều này dẫn đến một tính năng quan trọng của SortedNet của chúng tôi được gọi là bất cứ lúc nào, đó là một mạng có thể tạo ra đầu ra của nó ở bất kỳ giai đoạn nào của quá trình tính toán.
• Hiệu quả bộ nhớ: chúng tôi huấn luyện một mạng nhiều-trong-một trong đó các mô hình con đều là một phần của một checkpoint duy nhất, điều này giảm thiểu yêu cầu lưu trữ.

Vì mục đích hiệu quả, trong quá trình huấn luyện của chúng tôi, lớp cuối cùng, ví dụ: lớp phân loại, được chia sẻ giữa tất cả các mô hình con; ngoài ra, chúng ta có thể thêm một lớp phân loại riêng biệt cho mỗi mô hình con. Để đơn giản và hiệu quả, chúng tôi chọn phương án trước, tức là sử dụng lớp phân loại chia sẻ.

3.2 Thuật toán SortedNet

Trong phần này, chúng tôi mô tả thuật toán huấn luyện đề xuất của chúng tôi. Để huấn luyện một SortedNet với n mô hình con, tại mỗi lần lặp trong quá trình huấn luyện, một chỉ số ngẫu nhiên cần được lấy mẫu từ một phân phối xác định trước: bij∼PBj. Sau khi tìm thấy mô hình con đích θ∗t tại mỗi lần lặp, chúng ta có thể sử dụng một trong những mục tiêu sau để cập nhật các tham số của mô hình con được chọn:

• (Tổn thất ngẫu nhiên) Chỉ huấn luyện mô hình con được chọn f(x, θ∗t):
minθ∗t L≜L(y, f(x, θ∗t)) trong đó L là hàm tổn thất để huấn luyện mô hình trên một tác vụ nhất định (ví dụ: L có thể là tổn thất entropy chéo thông thường) và y đề cập đến nhãn sự thật cơ bản.

• (Tổng ngẫu nhiên) Huấn luyện mô hình con f(x, θ∗t) và tất cả các mô hình con được nhắm tới của nó dọc theo mỗi chiều. Hãy giả sử rằng Θ⊥(θ∗t) là tập hợp vũ trụ cho tất cả các mô hình con được nhắm tới của θ∗t. Sau đó hàm tổn thất có thể được định nghĩa là:
minΘ⊥(θ∗t) L≜∑θ∈Θ⊥(θ∗t) L(y, f(x, θ))

Theo cách này, một mô hình con hoặc một tập con của các mô hình con được cập nhật trong mỗi lần lặp. Ngoài ra, người ta có thể chọn huấn luyện tất cả các mô hình con tại mỗi lần lặp, điều này tốn kém ở quy mô lớn.

3.3 Tại sao SortedNet hoạt động?

Trong Phụ lục B, chúng tôi cung cấp lý giải lý thuyết cho sự hội tụ tham số của các mô hình con trong các tình huống huấn luyện giống hệt nhau và cũng cung cấp ranh giới hiệu suất giữa các mô hình con được huấn luyện và mạng tương ứng tương tự được huấn luyện độc lập.

Hội tụ Giả sử ˆf là một mô hình con của một mạng lớn hơn, và f là một kiến trúc mô hình giống hệt được huấn luyện độc lập. Chúng tôi nhằm hiểu mối quan hệ giữa các tham số của hai mạng này, θ cho ˆf và ϕ cho f, khi chúng được huấn luyện trong các điều kiện giống hệt nhau. Giả sử rằng các gradient của các hàm tổn thất cho ˆf và f, là L-Lipschitz liên tục, và tốc độ học là η, chúng tôi chỉ ra rằng

∥θt+1−ϕt+1∥ ≤(1 +ηL)∥θt−ϕt∥. (2)

Điều này chỉ ra rằng sự khác biệt trong các tham số của ˆf và f được điều chỉnh bởi hằng số Lipschitz L và tốc độ học η, gợi ý rằng các tham số nên giữ gần nhau trong suốt quá trình huấn luyện, đặc biệt khi sự khác biệt giữa các gradient của các hàm tổn thất của hai mạng là không đáng kể.

Ranh giới hiệu suất Hơn nữa, chúng tôi muốn tìm một ranh giới hiệu suất giữa một mô hình con được huấn luyện (với các tham số tối ưu θ∗) và mô hình riêng lẻ tương ứng của nó (với các tham số tối ưu ϕ∗). Hãy giả sử rằng ϕ∗=θ∗+ ∆θ. Chúng tôi chỉ ra trong Phụ lục B rằng độ lệch ∆f=f(x;ϕ∗)−ˆf(x;θ∗) trong giá trị hàm từ giá trị tối ưu của nó do nhiễu tham số ∆θ được giới hạn bởi 12L∥∆θ∥2 dưới giả định về tính liên tục L-Lipschitz của gradient.

∆f≈12∆θTH(x;θ∗)∆θ≤12L∥∆θ∥2(3)

Kết quả này ngụ ý rằng độ lệch của giá trị hàm tăng nhiều nhất theo cấp bậc hai với kích thước của nhiễu tham số.

4 Thí nghiệm

Trong phần này, chúng tôi tiến hành một tập hợp các thí nghiệm để cho thấy hiệu quả và tầm quan trọng của giải pháp của chúng tôi. Chi tiết về các siêu tham số cho mỗi thí nghiệm có thể được tìm thấy trong Phụ lục C.3.

4.1 SortedNet có thể mở rộng không?

Để cho thấy rằng phương pháp đề xuất của chúng tôi có thể mở rộng, chúng tôi thiết kế một thí nghiệm cố gắng huấn luyện 160 mô hình khác nhau trên nhiều chiều (độ rộng và độ sâu) cùng một lúc. Làm cơ sở, chúng tôi huấn luyện mạng lớn nhất (một MobileNetV2), và báo cáo hiệu suất tốt nhất của mô hình. Vì hiệu suất của mô hình kém cho tất cả các mô hình con khác (ít hơn 12%), chúng tôi huấn luyện lớp phân loại thêm 5 epoch trước khi đánh giá mỗi mô hình con cho cơ sở và báo cáo hiệu suất tốt nhất.

Như kết quả gợi ý trong Hình 2-a, phương pháp của chúng tôi có thể nắm bắt hiệu suất tối đa cho nhiều mô hình con này theo cách zero-shot. Trong mỗi ô, chúng tôi báo cáo hiệu suất của mô hình con ở trên và tỷ lệ phục hồi của mô hình so với mô hình lớn nhất (trong ví dụ này, 95.45). Mặc dù chia sẻ trọng số trên tất cả các mô hình, chia sẻ bộ phân loại và đánh giá zero-shot, phương pháp đề xuất đã bảo tồn tới 96% hiệu suất của mô hình lớn nhất, điều này rất khích lệ. Huấn luyện thêm bộ phân loại cho phương pháp đề xuất của chúng tôi sẽ dẫn đến hiệu suất thậm chí tốt hơn như được hiển thị trong Phụ lục C.5 (cải thiện khoảng ∼2 đến 15% cho các mô hình con khác nhau). Ngoài ra, chúng tôi cũng thử sắp xếp độ sâu và độ rộng bằng phương pháp đề xuất riêng lẻ, được báo cáo trong Hình 2-a lần lượt là D. Only và W. Only. Trên độ rộng, SortedNet thành công bảo tồn tới 99% hiệu suất của mạng lớn nhất.

4.2 Chúng ta có thể tìm thấy các mô hình con tốt nhất bằng SortedNet không?

Như được hiển thị trong Hình 2-b, dựa trên hiệu suất của các mô hình trong thí nghiệm trước được hiển thị trong Hình 2-a, chúng tôi chọn một tập con của các mạng có hiệu suất tốt nhất (độ rộng >60% và độ sâu >13 khối), và huấn luyện lại mạng từ đầu bằng SortedNet để cho thấy tỷ lệ thành công của phương pháp đề xuất của chúng tôi. Như được hiển thị, SortedNet thành công bảo tồn tới 99% hiệu suất của việc huấn luyện thông thường của mạng lớn nhất. Chúng ta cũng có thể làm cho quá trình lựa chọn này hoàn toàn tự động bằng cách sắp xếp hiệu suất của tất cả các mô hình con sau khi đánh giá và lọc ra một tập con của các mô hình có hiệu suất tốt nhất thực hiện tốt hơn một ngưỡng mong muốn. Như có thể thấy trong Hình 2-c, có một tập hợp các mô hình con thực hiện tốt hơn 80%. Để hiểu rõ hơn mô hình, chúng tôi đã chú thích một số điểm bằng cách sử dụng "D W" làm mẫu cho thấy cho mỗi mô hình độ rộng và độ sâu tương ứng.

4.3 Chúng ta có thể tổng quát hóa SortedNet không?

Trong một thí nghiệm khác, như được hiển thị trong Bảng 2, chúng tôi chứng minh sự vượt trội của phương pháp ngẫu nhiên của chúng tôi so với các phương pháp hiện đại như LCS (được hiển thị là LCS p trong bảng) Nunez et al. [2023], Slimmable Neural Network (NS) Yu et al. [2018a], và Universally Slimmable Networks (US) Yu and Huang [2019]. Để làm cho so sánh công bằng, chúng tôi cân bằng số lượng cập nhật gradient cho tất cả các mô hình. Chúng tôi cũng cố gắng loại bỏ tác động của thiết kế kiến trúc như việc lựa chọn các lớp chuẩn hóa. Do đó, chúng tôi đã cố gắng so sánh các phương pháp bằng các kỹ thuật chuẩn hóa lớp khác nhau như BatchNorm Ioffe and Szegedy [2015] và InstanceNorm Ulyanov et al. [2016]. Ngoài ra, chúng tôi đảm bảo rằng các phương pháp bổ sung như Chưng cất Kiến thức không có tác động đến kết quả vì những phương pháp này có thể được áp dụng và cải thiện kết quả độc lập với phương pháp. Như được hiển thị trong bảng, SortedNet thể hiện hiệu suất trung bình vượt trội so với các phương pháp khác, chỉ ra sự tổng quát hóa của nó trên các cài đặt khác nhau như các chuẩn khác nhau. Đáng chú ý là chúng tôi nhận ra bản chất bất ngờ của kết quả LCS-p-BN trong Bảng 2. Tuy nhiên, những kết quả này phù hợp với các quan sát của bài báo LCS gốc Nunez et al. [2023] (xem Hình 3 của bài báo LCS). Các tác giả LCS Nunez et al. [2023] cũng đưa ra giả thuyết rằng sự sụt giảm này gây ra bởi thống kê batch norm không chính xác. Để giải quyết điều này, họ đề xuất điều chỉnh kiến trúc thành GroupNorm. Phương pháp SortedNet của chúng tôi, mặt khác, không bị ảnh hưởng bởi vấn đề này, do đó không yêu cầu những sửa đổi như vậy.

4.4 Mở rộng Sorted Net cho Mô hình Ngôn ngữ Tiền huấn luyện

Trong thí nghiệm này, mục tiêu là áp dụng SortedNet cho một mô hình transformer tiền huấn luyện và đánh giá hiệu suất trên benchmark GLUE Wang et al. [2018]. Làm cơ sở, chúng tôi chọn RoBERTa Liu et al. [2019] để chứng minh tính linh hoạt của thuật toán của chúng tôi. Trong Bảng 3, chúng tôi sắp xếp tất cả các lớp của RoBERTa-base. Như kết quả chứng minh, phương pháp đề xuất của chúng tôi trung bình thực hiện tốt hơn cơ sở với biên độ đáng kể (∼23%). Tuy nhiên, mô hình lớn nhất có sự sụt giảm hiệu suất nhỏ (ít hơn 2%). Thật thú vị là kiến trúc transformer có thể bảo tồn hiệu suất của các mô hình con đến một mức độ nhất định mà không cần huấn luyện bổ sung. Tuy nhiên, thuật toán của chúng tôi cải thiện hiệu suất của những mô hình con này khoảng 10 đến 40% xấp xỉ. Một cài đặt phức tạp hơn (sắp xếp trên các mô hình Bert), đã được điều tra trong Phụ lục C.6.

4.5 Tăng tốc suy luận của Mô hình Ngôn ngữ Lớn dựa trên Bộ giải mã bằng SortedNet

Để thêm thể hiện khả năng mở rộng và tổng quát hóa của SortedNet trong các tình huống thực tế hơn, chúng tôi tinh chỉnh một LLaMA-13b Touvron et al. [2023] trên GSM8K Cobbe et al. [2021], một trong những tác vụ lý luận toán học thách thức. Chúng tôi chọn 12, 16, 20, 24, 28, 32, 36, và 40 lớp đầu tiên của LLaMA để xây dựng các mô hình con của chúng tôi. Để cân bằng số lượng cập nhật, chúng tôi huấn luyện mô hình dựa trên tổn thất ngẫu nhiên 8 lần nhiều hơn tổn thất tổng hợp, vì chúng tôi có 8 mô hình và mỗi lần truyền tiến trong tổn thất ngẫu nhiên là 1/8 của tổn thất tổng hợp. Trong bảng 4, chúng tôi báo cáo hiệu suất của một tập con các mô hình con và tăng tốc mà người ta có thể đạt được bằng cách sử dụng các kỹ thuật lấy mẫu khác nhau như Giải mã Tự hồi quy, Giải mã Suy đoán và Kỹ thuật Thoát Sớm Thích ứng dựa trên độ tin cậy Varshney et al. [2023], và Giải mã Tự Suy đoán Được sắp xếp, sử dụng thích ứng các mô hình con trung gian để tạo ra các token nháp trong thuật toán Giải mã Suy đoán (Hình 3). Như được hiển thị, việc kết hợp SortedNet và giải mã suy đoán có thể cải thiện hiệu quả thời gian cho mỗi token lên đến 2.09 lần nhanh hơn so với việc sử dụng tự hồi quy cho mô hình kích thước đầy đủ. Ngoài ra, chúng tôi làm nổi bật chi tiết của mỗi siêu tham số thí nghiệm trong Phụ lục C.3 và phân tích thêm đã được cung cấp trong Phụ lục C.7 để hiểu rõ hơn về hành vi của phương pháp sortedNet.

Kết luận

Tóm lại, bài báo này đề xuất một phương pháp mới để huấn luyện mạng nơ-ron động tận dụng tính mô-đun của mạng nơ-ron sâu để chuyển đổi hiệu quả giữa các mô hình con trong quá trình suy luận. Phương pháp của chúng tôi sắp xếp các mô hình con dựa trên tính toán/độ chính xác của chúng và huấn luyện chúng bằng một sơ đồ cập nhật hiệu quả lấy mẫu ngẫu nhiên các mô hình con trong khi tích lũy gradient. Bản chất ngẫu nhiên của phương pháp đề xuất của chúng tôi đang giúp thuật toán của chúng tôi tổng quát hóa tốt hơn và tránh những lựa chọn tham lam để tối ưu hóa mạnh mẽ nhiều mạng cùng một lúc. Chúng tôi chứng minh thông qua các thí nghiệm rộng rãi rằng phương pháp của chúng tôi vượt trội hơn các phương pháp huấn luyện động trước đây và tạo ra các mô hình con chính xác hơn trên các kiến trúc và tác vụ khác nhau. Kiến trúc được sắp xếp của mô hình động được đề xuất trong công trình này phù hợp với suy luận hiệu quả mẫu bằng cách cho phép các mẫu dễ dàng hơn thoát khỏi quá trình suy luận tại các lớp trung gian. Khám phá hướng này có thể là một lĩnh vực thú vị cho công việc tương lai.
