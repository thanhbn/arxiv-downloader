# 2301.13196.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/shared-params/2301.13196.pdf
# File size: 2075697 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Looped Transformers as Programmable Computers
Angeliki Giannouw*, Shashank Rajputw, Jy-yong Sohnw,
Kangwook Leew, Jason D. Leep, Dimitris Papailiopoulosw
pPrinceton University
wUniversity of Wisconsin-Madison
January 31, 2023
Abstract
We present a framework for using transformer networks as universal computers by program-
ming them with speciﬁc weights and placing them in a loop. Our input sequence acts as a
punchcard, consisting of instructions and memory for data read/writes. We demonstrate that
a constant number of encoder layers can emulate basic computing blocks, including embed-
ding edit operations, non-linear functions, function calls, program counters, and conditional
branches. Using these building blocks, we emulate a small instruction-set computer. This
allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer
transformer. We show how this transformer, instructed by its input, can emulate a basic
calculator, a basic linear algebra library, and in-context learning algorithms that employ back-
propagation. Our work highlights the versatility of the attention mechanism, and demonstrates
that even shallow transformers can execute full-ﬂedged, general-purpose programs.
1 Introduction
Transformers (TFs) have become a popular choice for a wide range of machine learning tasks,
achieving state-of-the-art results in ﬁelds such as natural language processing and computer
vision [Vaswani et al., 2017, Khan et al., 2022, Yuan et al., 2021, Dosovitskiy et al., 2020]. One
key reason for their success is their ability to capture higher-order relationships and long-range
dependencies across tokens, through attention. This allows TFs to model contextual information
and makes them effective in tasks such as machine translation and language modeling, where
they have consistently outperformed other methods [Vaswani et al., 2017, Kenton and Toutanova,
2019].
Language models with billions of parameters, such as GPT-3 (175B parameters Brown et al.
[2020]) and PaLM (540B parameters Chowdhery et al. [2022]), have achieved state-of-the-art
*Equal contribution. The title of this paper was not created by a transformer, but we can’t guarantee the same for
this footnote.
1arXiv:2301.13196v1  [cs.LG]  30 Jan 2023

--- PAGE 2 ---
Looped Transformers as Programmable Computers
performance on many natural language processing tasks. Interestingly, some of these large
language models (LLMs) can also perform in-context learning, adapting to and performing a
speciﬁc task, on-the-ﬂy , based on a brief prompt and a few examples. The ability to perform
in-context learning (ICL) arises without explicit training for it, and allows these large models to
efﬁciently perform new tasks without requiring weight updates.
Surprisingly, through in-context learning LLMs can perform algorithmic tasks and reasoning,
as demonstrated in several works including Nye et al. [2021], Wei et al. [2022c], Lewkowycz
et al. [2022], Wei et al. [2022b], Zhou et al. [2022], Dasgupta et al. [2022], Chung et al. [2022].
For example, Zhou et al. [2022] showed that LLMs can successfully perform addition on unseen
examples when prompted with a multidigit addition algorithm and a few examples of addition.
These results suggest that LLMs can apply algorithmic principles and perform pre-instructed
commands on a given input at inference time, as if interpreting natural language as code .
Constructive arguments have demonstrated that Transformers can simulate Turing Machines
with enough depth or recursive links between attention layers Pérez et al. [2021], Pérez et al. [2019],
Wei et al. [2022a]. This demonstrates the potential of transformer networks to precisely follow
algorithmic instructions speciﬁed by the input. Yet, these constructions are more generalized and
do not provide insight into how to create Transformers that can carry out particular algorithmic
tasks, or compile programs in a higher-level programming language.
More specialized designs can however allow TFs to execute higher level programs. For
example, in Weiss et al. [2021], the authors design a computational model and a programming
language that maps simple selection and aggregation commands on indexed input tokens. This
language can be used to create several interesting algorithms, such as counting tokens, sorting,
creating histograms, and recognizing Dyck- klanguages. Programs written in Restricted Access
Sequence Processing Language (RASP) can then be mapped into transformer networks, which
typically scale in size with the size of the program.
Another line of research has demonstrated methods for selecting the weights of a Transformer
model to function as an optimization algorithm for learning linear regression models on-the-ﬂy,
performing implicit training at inference time when given training data as input [Akyürek et al.,
2022, von Oswald et al., 2022]. These methods typically require a number of layers proportional
to the number of iterations of the learning algorithm and are limited to a small set of loss functions
and models.
The ability to program transformer models to emulate the abstract computation of a Turing
Machine, the specialized commands of languages like RASP, and the speciﬁc algorithms of in-
context learning, highlights the potential for transformer networks as versatile programmable
computers. Our research aims to explore this promising prospect, uncovering how the mechanics
of attention can enable the emulation of a general-purpose computer inspired by instruction-set
architectures.
Our Contributions: In this paper, we demonstrate that transformer networks can simulate
complex algorithms and programs by hardcoding them with speciﬁc weights and placing them in
a loop. We do this by reverse engineering attention to emulate basic computing blocks, such as
edit operations on the input sequence, nonlinear functions, function calls, program counters and
conditional branches. Our paper demonstrates the importance of using a single loop or recursion
to connect the transformer’s output sequence back to its input, avoiding the need for a deep model.
2

--- PAGE 3 ---
Looped Transformers as Programmable Computers
We accomplish this by designing a transformer that can execute programs written in a gen-
eralized version of a single instruction, known as SUBLEQ (A,B,C), i.e.,SUBtract and branch if
Less-than or EQual to zero. SUBLEQ is a single instruction language, deﬁning a one-instruction
set computer (OISC, pronounced “whisk”). SUBLEQ consists of 3 memory address operands and
when executed it subtracts the value at memory address A from the value at memory address B, and
stores the result in B. If the result in B is less than or equal to zero, the execution jumps to address
C, otherwise it proceeds to the next instruction. Programs written in SUBLEQ language use only
this command, yet this single instruction is capable of deﬁning a universal computer [Mavaddat
and Parhami, 1988, Esolangs].
input embedding sequence
 scratchpad
  memory
instructions
         pointers 
Transformer
Figure 1: A sketch of the looped transformer architec-
ture, where the input sequence stores the commands,
memory where the data is read/written from, and a
scratchpad where intermediate results are stored. The
input is processed by the network and the output is
used as the new input, allowing the network to itera-
tively update an implicit state and perform complex
computations.We construct explicit transformers that im-
plement SUBLEQ -like programs, of a more
ﬂexible single instruction which we call FLEQ
which takes the form
mem[c] =fm(mem[a];mem[b])
if mem [ﬂag]0
goto instructionp
wherefmcan be selected from a set of func-
tions (matrix multiplication/non-linear func-
tions/polynomials/etc), which we can hardcode
into the network. The depth of a looped trans-
former that can execute FLEQ programs is not
dependent on the depth of the program or the
number of lines of code, but rather on the depth
required to implement a single FLEQ instruc-
tion, which is constant. This is achieved by
running the transformer in cycles over the input sequence, similar to how a CPU operates.
Using this framework, we demonstrate the ability to emulate a variety of functions at inference
time, including a basic calculator, a basic linear algebra library (matrix transpose, multiplication,
inversion, power iteration) and an in-context learning algorithm that implements backpropagation
on implicit fully-connected networks. The input sequence, or the prompt, acts as a punchcard
that includes the program in the form of instructions that the transformer needs to execute, while
providing space for storing and processing the variables used in the program. The transformer
networks used to execute these programs are all of depth smaller or equal to thirteen, and the exact
weight matrices for all these models are provided. The following informal theorem summarizes
our main ﬁndings:
Theorem 1 (Informal) .There exists a looped transformer with less than 13 layers that can emulate
a general purpose computer (see Sec. 5), a basic calculator (see Sec. 7), numerical linear algebra
methods, such as approximate matrix inverse and power iteration (see Sec. 8), and in-context
learning algorithms, such as SGD, on neural networks (See Sec. 9).
The precise size of the transformers constructed in this paper is also summarized in Section 1.
3

--- PAGE 4 ---
Looped Transformers as Programmable Computers
# Layers # Heads Formal Statement
SUBLEQ 9 2 Lemma. 4
Matrix Inversion 13 1 Lemma. 12
Power Iteration 13 1 Lemma. 13
SGD 13 1 Lemma. 15
Table 1: Looped transformer sizes required to successfully emulate the functionalities of a one instruc-
tion set computer (OISC), perform basic calculations, run numerical linear algebra algorithms, and in-
context learning using Stochastic Gradient Descent on a neural network. The width of these networks
depends on the complexity of the functions implemented, and typically range from O(log( length_input ) +
embedding_dimension )to at most polynomial in the approximation error required when implementing
arbitrary loss functions for in-context learning.
Our research highlights the ﬂexibility of the attention mechanism and the importance of even a
single loop making it possible to design models that can emulate complex iterative algorithms and
execute general programs. It further demonstrates the ability of transformer models to efﬁciently
perform complex mathematical and algorithmic tasks. It is conceivable that modern transformers,
such as GPT-3, utilize similar internal subroutines when performing various tasks. In a way, these
models may possess the ability to elicit a speciﬁc skill or algorithm, akin to a function call, when
given in-context examples and instructions. However, this hypothesis should be taken with caution,
as the way we design our constructions shares no similarities with how real-world language models
are trained.
We hope that our study will encourage further research into the potential of attention mecha-
nisms, and the ability of language models to execute algorithmic instructions. Our proposed designs
can aid in determining the minimal transformer network size required to perform speciﬁc algorith-
mic tasks. Additionally, we hope that our ﬁndings will contribute to the development of methods
to enhance the capabilities of trained language models by utilizing smaller, reverse-engineered
transformer networks for speciﬁc algorithmic tasks
2 Prior Work
Our work is inspired by the recent results on the expressive power of Transformer networks and
their in-context learning capabilities.
In [Pérez et al., 2021, Pérez et al., 2019, Wei et al., 2022a] the authors explore the computational
properties of Transformers establishing that they are Turing complete, meaning that they can
simulate a Turing machine. The constructions typically require high/inﬁnite precision (apart from
that of Wei et al. [2022a]), and recursion around attention layers. In Yun et al. [2019], the authors
prove that given access to sufﬁcient width/depth TFs can act as universal sequence to sequence
approximators.
In Weiss et al. [2021], the authors propose a computational model for the transformer-encoder
in the form of a domain-speciﬁc language called the Restricted Access Sequence Processing
Language (RASP). The model maps the basic components of a TF encoder into simple primitives.
Examples of tasks that could be learned by a Transformer are provided, and the maximum number
of heads and layers necessary to encode a task in a transformer are analyzed.
4

--- PAGE 5 ---
Looped Transformers as Programmable Computers
In a recent and related work, Lindner et al. [2023] suggests using transformer networks as
programmable units and introduces a compiler called Tracr which utilizes RASP. However, the
expressivity limitations and unclear Turing completeness of the language are discussed in Weiss
et al. [2021], Merrill et al. [2022], Lindner et al. [2023]. Our approach, in contrast, demonstrates
the potential of transformer networks to serve as universal computers, enabling the implementation
of arbitrary nonlinear functions and emulating iterative, non-linear algorithms. Furthermore, our
framework allows the depth of our transformers to not scale in proportion to the lines of code
that they execute, allowing the implementation of iterative algorithms, expanding the potential
applications.
In Garg et al. [2022] the authors demonstrate that standard Transformers ( e.g., GPT-2) can be
trained from scratch to perform in-context learning of linear functions and more complex model
classes, such as two-layer neural networks, with performance that matches or exceeds task-speciﬁc
learning algorithms. A useful element of their analysis is the fact that language is completely
removed from the picture, and they perform all operations on the level of vector embeddings. This
allows a higher abstraction level than using language as an input, and in fact is what also allows us
to obtain our derivations.
Motivated by the above experimental work, in Akyürek et al. [2022], the authors investigate
the hypothesis that TF-based in-context learners emulate standard learning algorithms implicitly
at inference time. The authors provide evidence for this hypothesis by constructing transformers
that implement SGD for linear models, showing that trained in-context learners closely match the
predictors computed by these algorithms.
In a similar vein, von Oswald et al. [2022] argues that training Transformers on auto-regressive
tasks is closely related to gradient-based meta-learning formulations. The authors also provide a
hard-coded weight construction showing the equivalence between data transformations induced
by a single linear self-attention layer and gradient descent on a regression loss. The authors
empirically show that when training linear attention TFs on simple regression tasks, the models
learned by GD and Transformers have intriguing similarities.
In Liu et al. [2022], the authors test the hypothesis that TFs can perform algorithmic reasoning
using fewer layers than the number of reasoning steps, in the context of ﬁnite automata. The authors
characterized “shortcut solutions” that allow shallow Transformer models to exactly replicate the
computation of an automaton on an input sequence, and showed that these solutions can be learned
through standard training methods. As is expected this hypothesis is only true for a certain family
of automata, as the general existence of shortcut solutions would imply the collapse of complexity
classes that are widely believed not to be identical.
Other experimental studies have utilized recursion in transformer architectures in a similar
manner to our constructions, although in our case we only utilize a single recursive link that feeds
the output of the transformer back as an input [Hutchins et al., 2022, Shen et al., 2022, Dehghani
et al., 2018].
3 Preliminaries
The transformer architecture. Our work follows a similar problem setting as previous studies
(e.g. Yun et al. [2019], Garg et al. [2022], Akyürek et al. [2022], von Oswald et al. [2022]) in
which the input sequence consists of d-dimensional embedding vectors rather than tokens. This
5

--- PAGE 6 ---
Looped Transformers as Programmable Computers
simpliﬁes our results without sacriﬁcing generality, as an embedding layer can map tokens to the
desired vector constructions.
The input to each layer, X2Rdn, is a vector representation of a sequence of ntokens, where
each token is a d-dimensional column. In this paper, the terms “token” and “column” may be used
interchangeably.
A transformer layer outputs f(X), wherefis deﬁned as follows:
Attn( X) =X+HX
i=1ViXS(X>Ki>QiX) (1a)
f(X) = Attn( X) +W2ReLU (W1Attn( X) +b11>
n) +b21>
n (1b)
whereSis the softmax function applied on the columns of the input matrix, i.e.,
[S(X;)]i;j=eXi;j
Pn
k=1eXk;j;
where0is the temperature parameter, ReLU (x) =x1x>0is the ReLU activation, and 1n
is the all ones vector of length n. We refer to the K;Q;andVmatrices as the key, query, and
value matrices respectively1; the superscript ithat appears on the weight matrices indicates those
corresponding to the i-th attention head.Consistent with previous literature, the ﬁrst equation
Eq. (1a) represents the attention layer. We refer to the combination of attention and ReLU layers
as a single transformer layer.
Iterative computation through a simple loop. In the following sections, we utilize TF networks
with multiple transformer layers. Let us refer to the output of such a multilayer TF as TF(W;X),
where for simplicity Wis the collection of all weight matrices required to deﬁne such a multi-layer
TF.
Algorithm 1
Looped Transformer
1:fori= 1 :Tdo
2: X TF(W;X)
3:end forWe use our constructions recursively, and feed the output
back as an input sequence, allowing the network to perform it-
erative computation through a simple ﬁxed-point like iteration.
This recursive transformer is similar to past work on adding
recursion to TF networks. We refer to these simple recursive
TFs as Looped Transformers .
Feeding the output back to its input is similar to how a
traditional computer processes machine code, where it continually reads/writes data in memory, by
executing one instruction at a time. The input sequence Xincludes the instructions and memory.
Similar to how a CPU processes each line of code in a program, the transformer network processes
parts of the input sequence to perform complex computations. Like a CPU, the TF acts as a
self-contained computational unit. The use of loops in this process is analogous to how CPUs
operate using cycles.
1We’d like to note that typically the weight matrices are denoted as WQ;WK;WVbut to make notation cleaner,
we use instead Q;K;V.
6

--- PAGE 7 ---
Looped Transformers as Programmable Computers
While the analogy between TFs and CPUs can be entertaining, there are also many differences
in implementation. It is important to keep these differences in mind and not rely too heavily on the
analogy. The results obtained from using TFs as computational units do not require the analogy to
be valid.
To be able to build compute boxes out of a TF network, it is crucial to format the input sequence
Xin a way that separates memory, a cache-like scratchpad, and commands.
Input sequence format. The input to our transformer network has the following abstract form:
X=S M C
p1:::psps+1:::ps+mps+m+1:::pn;
(2)
where Srepresents the portion of the input that serves as a “scratchpad,” Mrepresents the portion
that acts as memory that can be read from and written to, and Crepresents the portion that contains
the commands provided by the user. The p1;:::;pnare positional encodings for the ncolumns,
which will be described in more detail in the following paragraph, and will be used as pointers to
data and instructions. The structure of our input sequence bares similarities to that of Wei et al.
[2022a], Akyürek et al. [2022] that also use scratchspace, and have a separate part for the input
data.
Scratchpad. The scratchpad is a crucial component of our constructions. This is the central
location where the inputs and outputs of all computation are recorded. It is perhaps useful to think
of this as an analogue to a CPU’s cache memory. It functions as a temporary workspace where
data is copied, transformed, and manipulated in order to perform a wide variety of operations,
ranging from simple arithmetic to more complex tasks such as matrix inversion. Regardless of the
speciﬁc computation that is performed, the data necessary for the operation is always transferred
from the memory to the scratchpad, and once the computation is completed, the data is transferred
back to the memory. This allows the TF to perform the necessary calculations in a designated area,
separate from other parts of the input sequence.
Memory. All the compute boxes we create require memory to perform speciﬁc actions. The
memory component of the input sequence serves as a storage location for data. This data can take
various forms, including scalars, vectors, and matrices, and is subject to manipulation through
various operations. When computation is needed, the data is ﬁrst copied from the memory to the
scratchpad, where it is updated and transformed as necessary. Once the computation is complete,
the updated data is then returned and copied back to the memory for future use or reference. In
this way, the memory serves as a central repository for all relevant data, allowing it to be accessed
and manipulated as needed.
Commands. Our framework implements a set of commands within a transformer network; these
serve as instructions that guide the internal functioning of the transformer, similar to a low-level
programming language. These commands include indicators for memory locations and operation
directives, allowing the TF to execute complex computations and tasks in a consecutive and
organized manner.
7

--- PAGE 8 ---
Looped Transformers as Programmable Computers
4 Building Transformer Blocks towards General Computation
/u1D5B3/u1D5A5lex(X)
/u1D5B3/u1D5A5PC(X)
/u1D5B3/u1D5A5jump (X)
Figure 2: A sketch of the three transformer blocks used
as building blocks to implement a small instruction-set
computer. These blocks handle edits in the input sequence
(such as moving or copying from one block to another),
keep track of the program counter, and execute a program
counter jump if a speciﬁed condition is met.To build general compute boxes using
transformer networks, specialized com-
pute blocks are required. These blocks
will be assembled to create the desired end
functionality. In this section, we highlight
various operations that transformer layers
can perform. These operations will serve
the building blocks to create more com-
plex routines and algorithms. These op-
erations are designed to be interoperable
with each other, leveraging the ability of
attention to perform various tasks, such as
producing approximate permutation matri-
ces and approximating general functions
through sigmoid activations.
In the following sections, we focus on
the fundamental components necessary to
emulate a general-purpose computer, re-
serving the examination of how attention
can replicate sigmoid-based functions in the sections that follow.
4.1 Positional Encodings, Program Counter, and Data Pointers
To aid the transformer in locating the position of each token, each column of Xis appended with
positional encodings that is based on the column index. In this case, similar to Wei et al. [2022a],
the positional encodings is the binary representation of the column index, which is appended to
each column to keep the encoding dimension low, i.e., logarithmic in the sequence length. This
approach to using positional encodings is slightly different from the typical method of adding them
to the encodings of the input sequence. However, in this case, appending them as sufﬁxes to the
encodings allows for cleaner arguments and constructions.
In particular, the encoding for token/column indexed by iis alog(n)-dimensional1binary
vector pi21log(n), wherenis the length of the input sequence. Using the standard binary
representation of an integer i, meaningi=Plog(n) 1
k=0 2kbk, the positional encoding vector
piis set to 1at indexjif the binary representation of ihas0at thej-th index, i.e.,bi= 0,
otherwise it is +1. As a result, we have pT
ipi= log(n)and by Cauchy-Schwarz inequality,
pT
ipj<jpijjpjj=p
log(n)p
log(n) = log(n)wheneveri6=j, since pi;pjdiffer in at least one
coordinate.
In the applications presented, the transformer often needs to execute iterative algorithms or
go through a sequence of commands. To achieve this, we utilize a program counter that iterates
through the commands. The counter contains the encoding of the location where the next command
is stored. Additionally, a command may have data pointers that point to the location of the data the
command needs to read and write to. Both the program counter and data pointers utilize the same
8

--- PAGE 9 ---
Looped Transformers as Programmable Computers
positional encodings as discussed in the previous paragraph. Using binary vectors as positional
encodings allows us to easily increment the program counter by 1 (or any other amount) using the
feed forward ReLU layers in the transformer architecture (1). This is formalized in the following
lemma, for the proof see Lemma 16.
Lemma 1. Given twod-dimensional binary vectors representing two non-negative integers, there
exists a 1-hidden layer feedforward network with ReLU activation, containing 8dactivations in
the hidden layer and dneurons in the output layer, that can output the binary vector representation
of their sum, as long as the sum is less than 2d+1.
Our positional encoding scheme can also be used to point to speciﬁc data locations for reading
or writing, as discussed in the following section. This is achieved by using the same binary vectors
as positional encodings for both the program counter and data pointers. Furthermore, this technique
for pointing to speciﬁc data locations enables the transformer to effectively read and write from/to
data during the execution of the algorithm or sequence of commands that is build to implement.
4.2read /write : Copying Data/Instructions to/from the Scratchpad
Figure 3: A sketch of the read operation. Arrows show command blocks being copied from the part of the
input that is allocated to commands to the scratchpad. Typically an instruction is another set of pointers.
Positional encodings and counters are used for tracking what is copied where.
As previously stated, the scratchpad serves as a temporary memory for storing all information
needed for computation. This includes copying commands and data to it, performing computation,
and writing results back to memory. This process has similarities with the copy/write mechanism
developed in Akyürek et al. [2022].
The following lemma states that the command pointed to by the program counter or the
data from a location speciﬁed in the current command can be copied to the scratchpad for further
computation. The location of the program counter is conventionally placed right below the contents
of the scratchpad, but it can be changed arbitrarily. Keeping it in a speciﬁc location throughout the
entire computation helps retain a good organization of the construction.
Lemma 2 (read ).A transformer with one layer, one head, and width of O(logn+d), wheredis
the dimension of the data vectors and nis the length of the input, can read data/command vectors
9

--- PAGE 10 ---
Looped Transformers as Programmable Computers
from the input to the scratchpad from the location pointed to by the position embedding vector in
the scratchpad.
Proof. Consider a simpliﬁed input where the scratchpad only has one column, and we have
positional encodings, denoted as pi, that point to the location where data or commands should be
copied from. In this case, the operation we want to perform is as follows:
X=2
66666640v2vi
v100
pi00
0 p 2pi
0 00
1 0::: 0:::3
7777775  !2
66666640v2vi
vi00
pi00
0 p 2pi
0 00
1 0::: 0:::3
7777775;
which moves data/command embedding vector vifrom the memory/command part of the input to
the scratchpad. The ﬁrst row contains the data to be read, the second row has the data written in
the scratchpad, the third row contains the program counter, the fourth row contains the positional
encodings, the ﬁfth row is used by for temporary storage and the last row is just a bit that indicates
whether the column is in the scratchpad or not.
We use the following key and query matrices: K=Q=
0 0 I I 0 0
;so that the key
and query become equal to KX=QX=
pip2pi
;and hence,
(KX)>QX=2
666664p>
ipip>
ip2:::
p>
2pip>
2p2:::
.........
p>
ipip>
ip2:::
.........3
777775
Recall that piis alog(n)-dimensional1vector such that pT
ipi= log(n)and each pT
ipj
log(n) 1forj6=i. We show in the appendix that if we apply the softmax with temperature
logn3
, we haveS((KX)>QX)to be annnmatrix of the following form
2
66666666641
20 01
20
0 1 000
0 0 100
.....................
1
20 01
20
.....................
0 0 0013
7777777775+M=e1+ei
2e2e3e1+ei
2
+M;
where eiis theith column of the identity matrix, kMk1, andis as deﬁned in Appendix B. For
the purpose of the proof, we ignore the error term M, because it can be reduced arbitrarily by
increasing the temperature (it can be made precisely equal to 0, if we consider hardmax instead of
softmax), and overall does not limit us from deriving arbitrarily small error bounds.
10

--- PAGE 11 ---
Looped Transformers as Programmable Computers
Next we set the output and value weight matrices as follows
V=2
66666640 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
I I 0 0 0 0
0 0 0 0 0 03
7777775:
Using this, the output of the head is
X+VXS((KX)>QX) =2
66666640 v2 vi
v10 0
pi0 0
0 p 2 pi
v1+vi
2v2v1+vi
2
1 0::: 0:::3
7777775
Each column above has the following form:
2
666666664v0
orig
v1
orig
vorig
p(0)
p(1)
vnew
b3
777777775;
where v(0)
origandv(1)
origare the original value vectors (present in the top two row blocks) contained
in that column, p(0)andp(1)are the corresponding embeddings of each column, vnewis the new
value, andbis the bit indicating whether the column is part of the scratchpad or not.
The feedforward layers have the following form:
v(1)
orig:=v(1)
orig+ReLU (C(b 1)1+ 2vnew 2v(1)
orig) ReLU (C(b 1)1 2vnew+ 2v(1)
orig)
vnew:=vnew ReLU (vnew) +ReLU ( vnew) =0;
whereCis a large positive constant. The ﬁrst equation is performing the operation of subtracting
vnewfromvorigbut only when the sum and difference of C(b 1)1andvneware positive, otherwise
the subtraction does not occur. The second equation is resetting the value of vnewto zero after
it has been copied to vorig, where ReLU ( vnew)is the rectiﬁed linear unit (ReLU) applied to the
negative of vnew.
It can be veriﬁed that the output of the feedforward layers would then be the desired result
X=2
66666640v2vi
vi00
pi00
0 p 2pi
0 00
1 0::: 0:::3
7777775:
11

--- PAGE 12 ---
Looped Transformers as Programmable Computers
The next lemma explains that the vector vstored in the scratchpad can be copied to a designated
location in memory, as speciﬁed within the scratchpad itself. This allows for the transfer of data
from the scratchpad to a speciﬁc location in memory for further use or storage.
Figure 4: A sketch of the write operation. Arrows show data blocks being copied from the scratchpad to a
designated location in the part of the input allocated for memory. Positional encodings are used for tracking
the destination location and ensuring data is written at the correct memory location.
Lemma 3 (write ).A transformer network with a single layer, one head, and width O(logn+d),
wheredis the dimension of the data vectors and nis the length of the input, can effectively write a
data vector stored in the scratchpad to a speciﬁc location in the input, as designated by a positional
encoding vector in the scratchpad.
Proof. We want to achieve the following operation
X=2
66666640v2vi
v100
pi00
0 p 2pi
0 00
1 0::: 0:::3
7777775  !2
66666640v2v1
v100
pi00
0 p 2pi
0 00
1 0::: 0:::3
7777775;
The construction for this is identical to the one for read (see the proof of Lemma 2), except that
the feedforward layers are outputting the following:
v(0)
orig:=v(0)
orig+ReLU ( Cb1+ 2vnew 2v(0)
orig) +ReLU ( Cb1 2vnew+ 2v(0)
orig)
vnew:=vnew ReLU (vnew) +ReLU ( vnew) =0;
whereCis a large positive constant. The ﬁrst equation updates the value of a vector vorigin
memory with the value of a vector vnewfrom the scratchpad. The second equation is resetting the
12

--- PAGE 13 ---
Looped Transformers as Programmable Computers
new vector in the scratchpad to zero. It can be veriﬁed that the output of the feedforward layers
would be
X=2
66666640v2v1
v100
pi00
0 p 2pi
0 00
1 0::: 0:::3
7777775:
4.3ifhconditionithen gotohinstructioni: Conditional branching
In this subsection, we will implement a conditional branching instruction that evaluates a condition
and sets the program counter to a speciﬁed location if the condition is true, or increments the
program counter by 1 if the condition is false. The form of the command is as follows: if
mem[a]0,then goto i, where mem [a]is a value of some location in the memory part of
the input sequence. This command has two parts: evaluating the inequality and modifying the
program counter accordingly.
The ﬁrst thing we do is read from mem [a], as described in the previous subsection. Then, we
evaluate the inequality. Let us say that “ﬂag” is the truth value of the inequality. Since we assume
that for such conditional branching command, mem [a]contains an integer, the following ReLU
network can be used to compute the ﬂag:
ﬂag= 1 ReLU (mem[a]) +ReLU (mem[a] 1): (3)
In Section 5.1, we consider mem[a]to be vectors contain the binary 1representation of
integers. There we use 2’s complement convention to represent negative integers. Let the vector
be[bN::: b 1], wherebNis the most signiﬁcant bit and b1the least signiﬁcant. As we explain in
that section, the sign of bNindicates whether the integer is negative or positive (The number is
negative ifbN= +1 and non-negative otherwise). Hence, the ﬂag is 1 if bN= +1 or if all the bits
are 1(which is the case when mem[a]represents the integer 0).
ﬂag=ReLU (bN) +ReLU 
1 +N NX
i=1bi!
(4)
Let the current Program Counter be pPC, which points to a given command. Thus, if ﬂag is 1,
we want the program counter to “jump” and become pi, else if ﬂag is 0the program counter will
be incremented by one, and set to be pPC+1.
Consider that the simpliﬁed input currently has the following scratchpad
2
664 ::: 
ag 0:::0 0
pPC0:::0 0
pi0:::0 03
775;
13

--- PAGE 14 ---
Looped Transformers as Programmable Computers
where00are inconsequential values. The incremented pointer, pPC+1, can be computed using the
pointer incrementing operation that we described in the Subsection 4.1, using one feedforward
layer of (1b).Then,
pnext= 2ReLU (pPC+1 1ag) + 2 ReLU (pi 1(1 ag)) 1;
where 1is the all ones vector. Notice that we can implement this with just the feed forward layers
of Eq. (1b). To account for the residual connection we can add the expression  ReLU (pPC) +
ReLU ( pPC)in the equation above.
Hence, this entire operation requires 3 feed forward layers of Eq. (1b), and hence 2 transformer
layers. Note that to ensure that the attention layer of the transformer do not modify the input, we
simply set the Vmatrix to zero in (1a).
5 Emulating a Generalized One-instruction Set Computer
5.1 A SUBLEQ Transformer
Mavaddat and Parhami [1988] showed that there exists an instruction such that any computer
program can be translated to a program consisting of instantiation of this single instructions. A
variant of such an instruction is SUBLEQ , where different registers, or memory locations are
accessed. The way that SUBLEQ works is simple. It accesses two registers in memory, takes
the difference of their contents and stores it back to one of the registers, and then if the result
is negative it jumps to a different predeﬁned line of code, or continues on the next instruction
from the current line of code.2A computer that is built to execute SUBLEQ programs is called an
One-Instruction Set Computer, and is a universal computer, i.e., it is Turing Complete , if given
access to inﬁnite memory.
Algorithm 2 SUBLEQ (a,b,c)
1:mem[b] =mem[b] -mem[a]
2:ifmem[b]0then
3: goto instructionc
4:elsegoto next instruction
5:end if
The following describes the construction of a looped transformer that can execute a program
written in a speciﬁc set of instructions. The transformer keeps track of the lines of code, memory
locations, and a program counter, using the memory part of the input as memory registers and the
command part as lines of code/instructions. The scratchpad is used to record the additions and
pointers involved in each instruction, and the read, write, and conditional branch operations are
utilized.
2This version of the SUBLEQ instruction is a slightly restricted version of the original instruction; here we separate
the memory / registers from the instructions. We show that this restriction does not make our version computationally
less powerful by proving in Appendix C that our version is also Turing Complete.
14

--- PAGE 15 ---
Looped Transformers as Programmable Computers
Figure 5: Graphical representation of the building blocks necessary to implement the OISC instruction.
The ﬁrst two blocks transfer the data/command to the scratchpad, the second and third implement the
substraction and store the result, while the last one implements the if goto command that completes the
instruction.
Lemma 4. There exists a looped transformer architecture that can run SUBLEQ programs. This
architecture has nine layers, two heads, and a width of O(log(n) +N), wherenis the length of the
input sequence that is proportional to the length of the program and memory used by the emulated
OISC, andNis the number of bits we use to store each integer. The integers are considered to be
in the range [ 2N 1+ 1;2N 1 1]
Before we present our construction some observations are in place.
The importance of loops. The use of a loop outside the transformer is crucial as it allows the
computer to keep track of the program counter and execute the instructions in the correct order.
Without this loop, the size of the transformer would have to scale with the number of lines of code,
making the implementation impractical. Note that the overall complexity of running a SUBLEQ
program is going to scale with the number of lines of code, which is to be expected given standard
complexity theoretic assumptions on the circuit depth of functions. Note however that the depth of
the looped transfromer itself does not scale with the size of the program.
Can we avoid the logarithmic width scaling? Finally note, that the width of the transformer
scales logarithmically with the length of the program, and memory used. This is a side-effect
of the bit-complexity of our positional encodings, and could be overcome by considering higher
bit-complexity.
OISC as a basis for a more ﬂexible attention-based computer. The following construction
describes an implementation of a fully functioning one-instruction set computer (OISC) using
a transformer architecture. The memory stores integers and the instructions are executed in
a sequential manner. The key to this construction is the reverse engineering of the attention
mechanism to perform read/write operations and taking full advantage of each piece of the
transformer architecture, including the feedforward layers. This implementation serves as the
foundation for a more general attention-based computer presented in the next subsection, where
the subtraction of two contents of memory can be replaced with a general function, allowing for
the implementation of arbitrary iterative algorithms.
Proof of Lemma 4. Looking at Algorithm 2, note that each instruction can be speciﬁed by just 3
indices,a;b; andc. Since we use binary representation of indices to form positional encodings
and pointers, each of these indices can be represented by a logndimensional vector. We represent
15

--- PAGE 16 ---
Looped Transformers as Programmable Computers
each instruction by simply concatenating these embedding vectors to form a 3 logndimensional
vector as follows:
c=2
4pa
pb
pc3
5:
The input then takes the following form:
X=2
66666640 0 0 cs+m+1cs+m+2:::cn 1cEOF
0 0 M 0 0 ::: 0 0
00 0 0 0 ::: 0 0
pPC0 0 0 0 ::: 0 0
0p2:sps+1:s+mps+m+1ps+m+2:::pn 1pn
112:s0s+1:s+m0s+m+10s+m+2 0n 10n3
7777775(5)Commands
EOF Block of memory
Scratchpad
Program Counter
Encodings
Indicator of the scratchpad
where ci2R3 log(n),M2RNmandX2R(8 log(n)+3N+1)n. The ﬁrstscolumns constitute
the scratchpad, the next mconstitute the memory section, and the last n m scolumns contain
the instructions.
The program counter, pPCpoints to the next instruction that is to be executed, and hence it
is initialized to the ﬁrst instruction as pPC:=ps+m+1. The contents of the memory section are
Ndimensional1binary vectors which represent the corresponding integers. We follow the
2’s complement convention to represent the integers, described as follows. Let’s say the bits
representing an integer are bN;:::;b 1, withbNbeing the most signiﬁcant bit. Then,
1. IfbN= 1, then the integer is considered positive with the valuePN 1
i=12i 1bi+1
2.
2.IfbN= +1 , then the integer is considered negative with the value  2N 1+PN 1
i=12i 1bi+1
2.
Step 1 - Read the instruction cPC.The ﬁrst thing to do is to read and copy the instruction
pointed to by pPCin the scratchpad. The current instruction is located at column index PC, and
is pointed to by the current program counter pPC. The instruction, cPCconsists of three pointers,
each of length logn. In particular we copy the elements at the location (1 : 3 log(n);PC)to the
location (3 log(n) + 4 : 6 log( n) + 3;1). This can be done using the read operation as described
in Section 4.2. Hence, after this operation, the input looks as follows:
X=2
666666666640 0 0 c 1 c2:::cn m scEOF
0 0 M 0 0 ::: 0 0
0 0 0 0 0 ::: 0 0
0 0 0 0 0 ::: 0 0
cPC0 0 0 0 ::: 0 0
pPC0 0 0 0 ::: 0 0
0 p 2:sps+1:s+mps+m+1ps+m+2::: pn 1pn
1 1 2:s0s+1:s+m0s+m+10s+m+2::: 0n 1 0n3
77777777775
16

--- PAGE 17 ---
Looped Transformers as Programmable Computers
=2
6666666666666640 0 0 c 1 c2:::cn m s 1cEOF
0 0 M 0 0 ::: 0 0
0 0 0 0 0 ::: 0 0
0 0 0 0 0 ::: 0 0
pa0 0 0 0 ::: 0 0
pb0 0 0 0 ::: 0 0
pc0 0 0 0 ::: 0 0
pPC0 0 0 0 ::: 0 0
0 p 2:sps+1:s+mps+m+1ps+m+2::: pn 1 pn
1 1 2:s0s+1:s+m0s+m+10s+m+2::: 0n 1 0n3
777777777777775
This step can be done in one layer.
Step 2 - Read the data required by the instruction. We need to read the data that the columns
a;bcontain. To do so, we again use the read operation on the pointers pa;pb. Note that we need
two heads for this operation, one each for reading aandb. The resulting output sequence looks
like
X=2
6666666666666640 0 0 c 1 c2:::cn m s 1cEOF
0 0 M 0 0 ::: 0 0
mem[a]0 0 0 0 ::: 0 0
mem[b]0 0 0 0 ::: 0 0
pa 0 0 0 0 ::: 0 0
pb 0 0 0 0 ::: 0 0
pc 0 0 0 0 ::: 0 0
pPC 0 0 0 0 ::: 0 0
0 p 2:sps+1:s+mps+m+1ps+m+2::: pn 1 pn
1 1 2:s0s+1:s+m0s+m+10s+m+2::: 0n 1 0n3
777777777777775: (6)
This step can be done in one layer.
Step 3 - Perform subtraction. Letxdenote a column of the input X. Let it have the following
structure:
x=2
6666666666664

br
bs




3
7777777777775;
where each entry above represents the corresponding column element of the matrix Xin(6). Thus,
br=mem[a];bs=mem[b]for the ﬁrst column, and br=bs=0otherwise.
17

--- PAGE 18 ---
Looped Transformers as Programmable Computers
Hence, to perform bs r, we ﬁrst need to compute the binary representation of  r, which is
b r, and then simply add it to bs. To compute b r, which is the 2’s complement of br, we just
need to ﬂip the bits of brand add 1. Bit ﬂipping a 1bit can be done with a neuron simply as
bﬂipped = 2ReLU ( b) 1. For adding 1, we can use Lemma 16. Hence, each of these operations
can be done using 1 ReLU layer of width O(N), and so we need 2 transformer layers to perform
this (Here we make the intermediate attention layers become the identity mapping by setting their
value matrices to 0). Finally, we need one more ReLU layer to add bstob r, hence bringing the
total to 3 transformer layers.
This results in the following:
X=2
6666666666666640 0 0 c 1 c2:::cn m s 1cEOF
0 0 M 0 0 ::: 0 0
0 0 0 0 0 ::: 0 0
mem[b] mem[a]0 0 0 0 ::: 0 0
pa 0 0 0 0 ::: 0 0
pb 0 0 0 0 ::: 0 0
pc 0 0 0 0 ::: 0 0
pPC 0 0 0 0 ::: 0 0
0 p 2:sps+1:s+mps+m+1ps+m+2::: pn 1 pn
1 1 2:s0s+1:s+m0s+m+10s+m+2::: 0n 1 0n3
777777777777775
Note that since this can be done in the feedforward layers of the previous step, this does not require
an additional layer.
Step 4 - Write the result back to memory. Writing mem[b] mem[a]back to location bcan be
done using the pointer pband the set of embeddings and applying the write operation described
in Section 4.2. This operation requires one layer.
Step 5 - Conditional branching. We ﬁrst use Eq. (4) as described in Section 4.3 to create the
ﬂag, which is 1ifmem[b] mem[a]0and0otherwise. This can be done using the Eq. (1b) of
the transformer. Thus, we have
X=2
6666666666666640 0 0 c 1 c2:::cn m s 1cEOF
0 0 M 0 0 ::: 0 0
0 0 0 0 0 ::: 0 0
ﬂag 0 0 0 0 ::: 0 0
pa0 0 0 0 ::: 0 0
pb0 0 0 0 ::: 0 0
pc0 0 0 0 ::: 0 0
pPC0 0 0 0 ::: 0 0
0 p 2:sps+1:s+mps+m+1ps+m+2::: pn 1 pn
1 1 2:s0s+1:s+m0s+m+10s+m+2::: 0n 1 0n3
777777777777775(7)
This operation requires one layer.
Next we use the construction described in Section 4.3 to choose, depending on the value of
the ﬂag, whether we want to increment the current program counter or we want to jump in the
commandc. Similar to Section 4.3, this step needs 2 layers of transformers.
18

--- PAGE 19 ---
Looped Transformers as Programmable Computers
Step 6 - Error Correction. Note that some of the steps above we incur some error while reading
and writing due to the fact that we are using softmax instead of hardmax. This error can be made
arbitrarily small by increasing the temperature of the softmax. In this step, we push the error down
to zero. Note that all the elements of Xcan only be one of f 1;0;1g, with some additive error
from reads and writes as explained before. Assume that the temperature is set high enough that the
error is at most <0:5. Then, a noisy bit bcan be ﬁxed using the following ReLU:
bnoiseless =1
1 2(ReLU (b+ 1 ) ReLU (b+))
+1
1 2(ReLU (b ) ReLU (b 1 +)) 1:
This operation can be done with a single layer of transformer.
Step 7 - Program Termination. The special command cEOFis used to signal the end of a
program to the transformer. This command is made up of three encodings: ps+1,ps+2, andpn.
The ﬁrst encoding, ps+1, points to the ﬁrst entry in the memory, which we hard-code to contain
the value 0. The second encoding, ps+2, points to the second entry in the memory, which is
hard-codeded to contain the value  1. The third encoding, pn, points to itself, signaling the end of
the program and preventing further execution of commands. Hence, on executing this command,
the next command pointer is set to point to this command again. This ensures that the transformer
maintains the ﬁnal state of the input.
•For this, we ensure that the last instruction in each program is cEOF, and that mem[s+ 1] = 0
andmem[s+ 2] = 1.
• For this case a=s+ 1,b=s+ 2, andc=n.
•The memory is updated with the value mem[b] = mem[b] mem[a]. Since mem[a] = 0
here, the memory remains unchanged.
•Since mem[b]0here, the branch is always true and thus the pointer for the next instruction
is again set to point to cEOF.
5.2FLEQ : A More Flexible Attention-based Computer
In this section, we introduce FLEQ , a generalization of SUBLEQ that deﬁnes a more ﬂexible
reduced-instruction set computer. This implied set of additional instructions is based on a more
advanced version of SUBLEQ that allows for the implementation of multiple functions within
the same transformer network. This is achieved by generalizing the previous OISC construction
to include not just addition of registers, but any function from a set of Mpredeﬁned functions
implementable by a transformer network. In the following, we use the term FLEQ to refer
interchangably to the instruction, the language, and the attention-based computer it deﬁnes.
19

--- PAGE 20 ---
Looped Transformers as Programmable Computers
The design of FLEQ allows for the implementation of complex and sophisticated algorithms
by generating more general functions beyond simple subtraction, such as matrix multiplication,
computation of square roots, activation functions, etc. This not only increases the ﬂexibility
of the system, but also makes it possible to implement nonlinear computations, linear algebra
calculations, and iterative optimization algorithms for in-context learning while containing the
length of the corresponding programs.
Deﬁnition 1. LetTibe a transformer network of the form (1)withli-layers,hi-heads and di-
mensionality r. We call this a “transformer-based function block” if it implements a function
f(A;B)where the input and output sequence format is assumed to be the following: A2Rdhdw
is assumed to be provided in the ﬁrst set of dcolumns (columns 1tod) andB2Rdhdwthe second
set ofdcolumns (columns d+ 1to2d); after passing the input through the lilayers, the output of
f(A;B)2Rdhdwis stored in the third dcolumns (columns 2d+1to3d), wheredis the maximum
size that the input could have and it is a constant that we determine. Note that dh;dwd. Finally,
the sequence length of the block is s3d. Similarly to d,sis a predetermined constant.
The parameters A;Bcan be scalars, vectors or matrices as long as they can ﬁt within a dd
matrix. Hence, the above deﬁnition is minimally restrictive, with the only main constraint being the
input and output locations. More details about the input and output requirements will be explained
towards the end of this subsection.
Theorem 2. GivenMdifferent transformer-based function blocks T1;;TM, there exists a
transformerTof the form (1)with number of layers 9 + maxfl1;;lMg, a number ofPM
i=1hi
heads , and dimensionality O(Md+ logn)such that running it recurrently Ttimes can run
Tinstructions of any program where each instruction is FLEQ (a;b;c;m; ﬂag;p;dh;dw), and
executes the following:
mem[c] =fm(mem[a];mem[b]) ; ifmem[ﬂag]0goto instruction p (8)
Herenis the total length of the program and we assume that mem[ﬂag]is an integer. The
parameters dh;dware explained in Remark 1 below.
Remark 1. Note that, the transformer TcontainsMtransformer-based function blocks and
each one may use different input parameters. We thus deﬁne with dthe max length that each of
the parameters A;B;C(stored in locations a;b;c ) as in Deﬁnition 1 can have; this is a global
constant and it is ﬁxed for all the different instances that we can create. Now, dh;dwrefer to the
maximum dimension that the parameters can have in a speciﬁc instance of the transformer T; the
rest of the columns d dwand rowsd dhare set to zero.
The proof of this theorem can be found in Appendix D. Below we explain some of our design
choices.
Execution cycle of the uniﬁed attention-based computer. In each iteration of the looped
transformer, one instruction is fetched from the set of instructions in the input according to the
program counter. The instruction is then copied to the scratchpad. Depending on the function to
be implemented, a different function block location is used to locally record the results of that
20

--- PAGE 21 ---
Looped Transformers as Programmable Computers
function. Once the result is calculated, it is copied back to a speciﬁed memory location provided
by the instruction. The execution cycle is similar to the one-instruction set computer (OISC) in the
previous section, with the main difference being that for each instruction, we can choose from a
pre-selected list of functions that take inputs in the form of arbitrary arrays of numbers, such as
matrices, vectors, and scalars.
The format of the input sequence. In Fig. 6, we illustrate the input Xto our looped transformer,
which can execute a program written as a series of FLEQ instructions. Note that Xis divided
into three sections: Scratchpad, Memory, and Instructions. As in the left bottom part of Fig. 6,
we allocate a separate part of the scratchpad for each of the Mfunctions that are internally
implemented by the transformer. For example, if we have matrix multiplication and element-wise
square root as two functions, we would allocate a different function block for each one.
Figure 6: The structure of input X, to execute FLEQ commands.
This design may not be the most efﬁcient, but our goal is to demonstrate the possibilities of
looped transformers. Additionally, since the number of different functions is typically small in
the applications we have in mind, the design does not signiﬁcantly increase in size. The choice to
reserve different function blocks for each predeﬁned function is for convenience, as it allows for
separate treatment of functions without worrying about potentially overlapping results. We believe
that a design with a single function block is feasible, but it would signiﬁcantly complicate the rest
of the transformer construction.
Instruction format. The instruction in Theorem 2 is essentially a composition of the following
two components: the function call to fmand the conditional branching (if ... goto ...). The
instruction, located at the top right side of Fig. 6 contains the following components:
21

--- PAGE 22 ---
Looped Transformers as Programmable Computers
2
66666666664pa
pb
pc
pm
pﬂag
pp
dh
dw3
77777777775(9)Pointers to parameters of fm
Position to write result
Pointer to function block
Position of ﬂag
Next instruction
Dimensions of the inputs and the output
The goal of each positional encoding vector in Eq. (9) is to point to the corresponding space
of the input where each component required by the instruction is located. To be speciﬁc, paand
pbpoint to the locations that the inputs aandbare located, pcpoints to the location to which we
will record the ﬁnal result of the function fm. Similarly, pmpoints to the function block in the
scratchpad that the intermediate computations required for fmare recording, pﬂagpoints to the
variable that we check if it is non-positive (the result is used for conditional branching), and pp
points to the address of the line of code that we would jump if the variable in pointed by pﬂagis
non-positive.
Execute a function; Jump to command. Recall that the ﬁrst four parameters ( a;b;c;m ) of
FLEQ, as well as the last two (dh;dw)are related to the implementation of the function block,
while the other two ( ﬂag;p) are related with the conditional branching. Since there is no overlap
between the two components of each instruction, it is possible to use each of these components
independently. By having a ﬁxed location ﬂag0where mem[ﬂag0]is always set to 1, we can have
the simpler command FLEQ (a;b;c;m; ﬂag0;p;dh;dw)which implements
mem[c] =fm(mem[a];mem[b]):
Further, by having ﬁxed locations a0;b0;c0which are not used elsewhere in the program, and hence
inconsequential, we can have the simpler command FLEQ (a0;b0;c0;m;ﬂag;p;dh;dw)which
implements
ifmem[ﬂag]0goto instruction p:
Using this, we get the following corollary:
Corollary 1. The Uniﬁed Attention Based Computer presented in Theorem 2 can run programs
where each instruction can be either of the following two simple instructions:
•mem[c] =fm(mem[a];mem[b])
• ifmem[ﬂag]0goto instruction p
Format of Transformer-Based Function Blocks. Recall that each function block is located at
the bottom left part of the input X, as shown in Fig. 6. Each transformer-based function block is
expected to operate using the following format of the input:
22

--- PAGE 23 ---
Looped Transformers as Programmable Computers
•The number of rows in the input is r, while the number of columns is sands3d. Here
swill dictate the total maximum number of columns that any transformer-based function
block needs to operate. The reason that smight be larger than 3dhas to do with the fact that
some blocks may need some extra scratchpad space to perform some calculations.
•The function block speciﬁes the dimensions of input and output. Say they are dhdw,
wheredh;dwd. These will be part of the instruction which calls this function inside the
FLEQ framework, as in (9).
•Suppose each function block has two inputs ( A2RdhdwandB2Rdhdw) and one output
f(A;B) =C2Rdhdw. As in (10), the function block is divided into four parts: (1) the
ﬁrst input Ais placed in the ﬁrst dhrows and the ﬁrst dwcolumns, (2) the second input B
is placed in the ﬁrst dhrows and the columns d+ 1 :d+dw, (3) the output f(A;B) =C
is in the ﬁrst dhrows and the columns 2d+ 1 : 2d+dwcolumns and 4) the rest s 3d
column used as scratchpad space for performing necessary calculations. Note that the unused
columns are set to zero.
•The lastr dhrows can be used by the transformer-based function block in any way, e.g.,
to store any additional positional encodings.
We put the format of the input of each transformer-based function block in(10). The ﬁrst input
A= [z1
a;;zdwa]of the function is zero padded and stored in the ﬁrst dcolumns. Similarly,
the second input B= [z1
b;;zdw
b]is stored in the next dcolumns. The output/result of the
function block C= [z1
c;;zdwc]is located in the next dcolumns while we have some extra
s 3dcolumns which can be used as scratchpad.

z1
a:::zdwa0z1
b:::zdw
b0z1
c:::zdwc0:::0
:::   :::   ::: :::
(10)InputA InputB OutputC=f(A;B)
Let us consider the case where we wish to multiply a matrix A2Rdd,with a vector b2Rd1.
The resulting output matrix would look as follows:

A b 0 A>b 0 0
:
Computational concerns: Do we need full attention? In our construction, the computational
complexity of each layer depends on the number of embedding vectors that each part of the input
has to attend to. Typically, this is quite sparse, as only a few of them need global attention. In
our speciﬁc construction, only the columns within the scratchpad require global attention. By
focusing only on these columns, we can reduce the computational complexity of the attention
mechanism from O(n2d)toO(nd), where n is the number of input sequences, dis the dimension
of the embedding vectors.
This reduction in computational complexity is achieved by limiting the attention mechanism
to only the columns within the scratchpad, which helps to improve the overall efﬁciency of the
model. Additionally, since the computational complexity grows linearly with the number of
input sequences, rather than quadratically, it enables us to scale the model to handle larger input
sequences.
23

--- PAGE 24 ---
Looped Transformers as Programmable Computers
6 Functions in the Uniﬁed Template Form
In this section, we demonstrate how to implement a variety of nonlinear functions and basic linear
algebra operations using transformers. These techniques will be crucial in the construction of
iterative algorithms in the following sections. Each transformer-based function block in this section
ﬁts in our uniﬁed template in terms of input/output parameters’ locations. We note here that each
transformer-based function block might have its own positional encodings used to transfer the
output in the correct place or perform some read/write operations and they are part of the
design of the block.
6.1 Encoding Non-linear Functions within the Attention Mechanism
One key ingredient of our constructions is encoding various functions within the attention mech-
anism. We do this by forcing the softmax to act as a sigmoid function and by storing multiple
coefﬁcients in the query and value weight matrices. As far as we know, this is the ﬁrst work that
shows how general non-linear functions can be emulated by attention layers. This allows us to
create linear combinations of sigmoids that can be accessed by an indicator vector in the input.
Our analysis is based on the result of Barron [1993] which we present below.
Deﬁnition 2. Let C;Bbe the set of functions deﬁned in a bounded domain B,f:B!R;BRd
with a proper extension to Rdsuch that they have Cbounded Fourier integral, i.e.,R
supx2BjwxjF(dw)Cholds where F(dw)is the magnitude of the Fourier distribution.
Deﬁnition 3. Given >0;C > 0and a bounded set B, let
G;=f((aTx+b)) :jj2C;kakB1;jbj1g
wherekakB= supx2BfxTagandis the sigmoid function, i.e.,(x) =1
1+e x.
Theorem 3 (Theorem 3 in Barron [1993]) .Every function f2 C;Bwithf(0) = 0 and can be
approximated by a linear combination of sigmoids fi2G;,i= 1;:::m . Ifm1=2lnmthe
error scales as f(x) mX
i=1fi(x)O1
m1=2
;x2B
To encodeNdifferent functions, we use the index j2[N]and writecji;ajifor the coefﬁcients
of the sigmoids that approximate them or
fj(x) =mX
i=1cji(xTaji)forj= 1;:::;N
We here note that the terms ;bcan be incorporated in the term aijby adding an extra coefﬁcient
of1inxand multiplying everything with .
We are now able to present the lemma on approximating functions using transformer blocks, in
a format that is consistent with the FLEQ design outlined in the previous section.
24

--- PAGE 25 ---
Looped Transformers as Programmable Computers
Lemma 5. Fix>0and consider an input of the form
X=2
66664e0 x 0 0 0
0 0 0 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2dp2d+1p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775:
wheredis chosen according to the FLEQ construction from the previous section and Nis the
number of functions we encode . e=ej2RNis an indicator vector signifying the function we
wish to execute. Then there exists a transformer-based function block with 3 layers, mheads and
dimensionality r= 2 log(d) +d+ 1 =O(d)such that
f(X) =2
66664   Pm
i=1cji(xTaji) +
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2d p2d+1 p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775
wheredenoted inconsequential values that will be ignored downstream. This implies that
arbitrary function g2 C;Bcan be well approximated by attention layers.
Remark 2. Notice that in this case we don’t use any extra scratchpad space and thus s= 3d;
however if this function block was to be used with another one that needs s>3dscratchpad space,
we would simply zero pad the input of Lemma 5 and ignore these columns. The same holds for the
rest of the transformer-based function blocks and we will not mention it from now on.
In the expressionPm
i=1cji(xTaji), the number head is equal to the number of terms we
need.We show in the appendix that we can actually encode these mterms in the dimension of the
transformer architecture with just one head (See Corollary 6). The choice of which result to use
can depend on the speciﬁc design and can affect both accuracy and efﬁciency of the implemented
transformer network.
The proof of this Lemma is given in Appendix A.2.
6.2 Matrix Transposition and Multiplication by Linearizing the Softmax
We assume that a ddmatrix Ain the input Xis represented by a sequence of length d, and
each of these dcolumns has drows. While this representation has the advantage that it is well
suited for the matrix multiplication operation (as we will see in the next sub-section), a vectorized
form of the matrix is more suited to create transpose. This is how we implement the transpose; we
ﬁrst vectorize the matrix A, then with a ﬁxed permutation of the columns we create its vectorized
version of a transpose.
Lemma 6. Fix>0and consider an input of the following form
X=2
664A 0 0 ::: 0
0 0 0 ::: 0
p1:dp1:dp1:d:::p1:d
P0
1P0
2P0
3:::P0
d3
775:
25

--- PAGE 26 ---
Looped Transformers as Programmable Computers
where A2Rdd; then there exists transformer-based function block with 4 layers, 1 head and
dimensionality r= 2d+ 2 logd=O(d)that outputs the following matrix
X=2
664A0A0A0:::A0
0 0 0 ::: 0
p1:dp1:dp1:d:::p1:d
P0
1P0
2P0
3:::P0
d3
775:
where A0=A>+M, for somekMk1. The errordepends on the choice of the temperature
, as it is a consequence of the read/write operations.
In order for matrix multiplication to ﬁt in our uniﬁed template, we need to show for example
for the result of A>B, where A2RkmandB2Rknwithk;m;n < d we can achieve the
following:A 0 B 0 0 0
0 0 0 0 0 0
  !    A>B
0 0 0 0 0 0
The idea we leverage is the linearization of the softmax, i.e., for a column vector z= [x;C]for
some large constant Cwe have that
S(z) = [x+;]
The erroris controlled by the constant C.
Lemma 7. LetA2RkmandB2Rkn; then for any >0there exists a transformer-based
function block with 2 layers, 1 head and dimensionality r=O(d)that outputs the multiplication
A>B+M, for somekMk1.
The implementation of B>A,A>AandB>Bare simple corollaries of the lemma presented
above and we will freely use them in the subsequent sections. In Appendix A.2, we provide the
exact form of the input Xfor implementing matrix transposition/multiplication, as well as the
proof of the corresponding Lemmas.
6.3 Advantage of attention over fully-connected networks
It is possible to implement the functions and overall lexicographic functionality presented in
previous sections using fully connected networks, as they are also universal function approximators.
However, it is easy to demonstrate a depth separation between attention-based networks and fully
connected networks. For example, to compute simple functions like polynomials of x(e.g.,x2),
a ReLU network with a depth proportional to log(1=)is required, where is the quality of
approximation, e.g., as showed in [Perekrestenko et al., 2018]. In contrast, we have shown how x2
can be implemented in essentially 2layers. This simple depth separation argument highlights the
constant vs scaling depth required for several functionalities in fully connected networks versus
attention-based networks. It is important to note that although these constructions are easy to
demonstrate their existence, constructing them is not straightforward. In this work, we provide
hardcoded attention layers that precisely do that, making it easier to implement these functionalities
in practice.
26

--- PAGE 27 ---
Looped Transformers as Programmable Computers
7 A Basic Calculator
We show that the FLEQ transformer introduced in Section 5.2, can be used to build a simple
calculator. This transformer consists of six transformer-based function blocks that implement
addition, substraction, multiplication, percentage, division and square root. The formal statement
is written as below.
Theorem 4. There exists a transformer with 12layers,mheads and dimensionality O(logn)that
uses the Uniﬁed Attention Based Computer framework in Section 5.2 to implement a calculator
which can perform addition, subtraction, multiplication, and computing the inverse, square root
and percentage. For computing the inverse and square root, the operand needs to be in the
range [ eO(m); ~
(1pm)][[~
(1pm);eO(m)]and[0;O(m2)]respectively, and the returned output is
correct up to an error of O(1=pm)andO(1=m)respectively. Here, nis the number of operations
to be performed.
Remark 3. In the proof of this theorem, we use Lemma 5 to approximate the square root and
the inversion function. That lemma provides error guarantees in terms of the number of heads
m. We prove Corollary 6 in the appendix which provides equivalent error guarantees, but where
the error decreases with the dimension dof the transformer. Depending on the design choices of
the transformer, either of the results can be used, and the calculator’s error guarantee will also
change accordingly.
We show how one can implement a calculator in our FLEQ framework in Algorithm 3.
Algorithm 3 A sample program for executing a basic calculator functionality. The following
algorithm performsp
1=(((a+b) c)d)
100
Require: mem[p] =a;mem[q] =b;mem[r] =c;mem[s] =d. .The location of the inputs.
1:mem[t] =fadd(mem[p];mem[q]) .mem[t] =a+b.
2:mem[t] =fsub(mem[t];mem[r]) .mem[t] = (a+b) c.
3:mem[t] =fmul(mem[t];mem[s]) .mem[t] = ((a+b) c)d.
4:mem[t] =finv(mem[t]) .mem[t] = 1=((a+b) c)d.
5:mem[t] =fsqrt(mem[t]) .mem[t] =p
1=((a+b) c)d.
6:mem[t] =fperc(mem[t]) .mem[t] =p
1=((a+b) c)d
100.
Looking at the algorithm, it is clear that for proving the theorem above, it is sufﬁcient to imple-
ment the 6 functions (addition, subtraction, multiplication, inversion, square root and percentage)
using the transformer-based function blocks deﬁned in Deﬁnition 1. We start with two lemmas,
which can be proved by constructing transformers that add and subtract in a similar way to the
OISC transformer constructed in Section 5.1.
Lemma 8 (addition ).There exists a transformer-based function block with 3 layers, 1 head
and dimensionality O(1)which can implement f(a;b) =a+b.
27

--- PAGE 28 ---
Looped Transformers as Programmable Computers
Proof. Consider the input in the form of Eq. (10)
X=2
66664a 0b 0 0 0
0 0 0 0 0 0
p2d+10 0 0 0 0
0 p 2:dpd+1pd+2:2dp2d+1p2d+2:3d
1 0 0 0 0 03
77775(11)
We can perform the following transformation
a0b000
0 0 0 0 0 0
  !2
4a0b000
a0b000
0 0 0 0 0 03
5 (12)
  !2
4a00000
00b000
0 0 0 0 0 03
5 (13)
  !2
4a+b00000
000000
0 0 0 0 0 03
5 (14)
  !2
4a+b000a+b0
00b0 00
0 0 0 0 0 03
5 (15)
The ﬁrst and second step are implemented with one feed-forward layer each. The third step with
the Section 4.2. We have ignored the last three rows since we don’t change them and we only use
them for the last step.
Lemma 9 (subtraction ).There exists a transformer-based function block with 3 layers, 1
head and dimensionality O(1)which can implement f(a;b) =a b.
This lemma can be proved in the exact same way as the previous one. In addition, we can use
the theory presented in Lemma 7 to get the following corollaries:
Corollary 2 (multiplication ).There exists a transformer-based function block with 2 layers,
1 head and dimensionality O(d)which can implement f(a;b) =ab.
Corollary 3 (percentage ).There exists a transformer-based function block with 2 layers, 1
head and dimensionality O(1)which can implement f(a) =a=100 =a0:01.
To implement inversion function, we introduce the following lemma.
Lemma 10. Given;2[0;1], andC1there exists a function fof the form f(x) =Pm
i=1ci(wix+bi);whereis the sigmoid function, such that
8x2[;C];f(x) 1
x;
as long asd= 

log(1=())
+ logC
.
28

--- PAGE 29 ---
Looped Transformers as Programmable Computers
We can use this lemma along with the result presented in Lemma 5 to get the following
corollary:
Corollary 4 (inversion ).There exists a transformer-based function block with 3 layers and m
heads which can implement f(a) =1
aup to error ~O(1pm)for alla2[~
(1pm);~O(em)].
Note that using Corollary 2 (multiplication) and Corollary 4 (inversion), the operation of
division can be implemented as well. Next, we move on to showing the way of implementing
square root.
Lemma 11. Given2[0;1], andC1there exists a function fof the formf(x) =Pm
i=1ci(wix+
bi);whereis the sigmoid function such that
8x2[0;C];f(x) px;
as long asm= 
p
C

.
We can use this lemma along with the result presented in Lemma 5 to get the following
corollary:
Corollary 5 (sqrt ).There exists a transformer-based function block with 3 layers and m heads
which can implement f(a) =paup to errorO(1=m)for alla2[0;O(m2)].
The functions f:x!1
x(inversion) and f:x!px(square root) since they can be
approximated by sums of sigmoids, they can directly be encoded in the standard transformer-based
function block form through Lemma 5.
What other functions can our calculator implement? We have included some of the most
commonly used operations in calculators in our construction, but it can be extended to include a
wider variety of operations such as algebraic and trigonometric functions. When implementing
these functions within our transformer architecture, there are typically two choices that can be
made. One option is to approximate the target function f(x)using sigmoids. Another option is to
use an iterative numerical algorithm where the next output yis calculated based on the previous
outputyand the goal is to minimize the difference between the calculated output and the target
functionf(x). This algorithm takes the form yk+1=g(yk), wheregis typically an algebraic
function. The desired accuracy is achieved when the difference between the calculated output and
target function is less than or equal to a certain tolerance .
8 Linear Algebra
In Section 6, we demonstrated the implementation of matrix transpose and matrix multiplication
as transformer-based function blocks. Utilizing these implementations, we proceed to execute two
iterative algorithms for determining the inverse of a matrix through the Newton-Raphson Method
and identifying the eigenvector corresponding to the maximum eigenvalue through the Power
Iteration method.
29

--- PAGE 30 ---
Looped Transformers as Programmable Computers
Linear algebra using Transformers In the study conducted by Charton [2021], the author
implemented some standard matrix method operations using a transformer-based architecture.
Four distinct encoding schemes were proposed and applied to nine different operations, ranging
from matrix multiplication to eigenvalue decomposition. We ﬁnd that the size of the networks in
Charton [2021] is comparable to that of ours.
As an example we compare the required network size of ours and Charton [2021], for the
task of transposing a matrix of size 3030: our construction uses a transformer with 1 layer, 1
head and width of 168, while the transformer in Charton [2021] has 1 layer, 8 heads and width
of 256. Notice that the number of layers, heads and width reported above may seem different
with Lemma 6; however, in the proof of Lemma 6 we ﬁrst vectorize the matrix ( 1layer), then
we implement the ﬁxed permutation using Lemma 3 ( 1layer) and ﬁnally we use another 2layers
to bring back the matrix in its original representation. If the matrix is given to us, as in Charton
[2021], in its transposed form then we only need one layer and the two sets of encodings to perform
the ﬁxed permutation. Since the maximum size of the matrix is 3030, the sequence length is
n= 302and thus the size of each of the encodings will be 10, leading to an input with width
210 + 1 = 21 . This will lead to a total width of 168, due to the ReLU layer in Lemma 16, for
adding two binary vectors, having a width eight times the input’s width.
We intend to further investigate our constructions, by implementing them and evaluating the
errors involved as a function of the constants used in the proof of Lemma 7 and the temperature in
Lemma 2, in future work.
Matrix Inversion. We can use the Uniﬁed Attention Based Computer to write a program for
Matrix Inversion using the functions for matrix multiplications and a function for subtraction. We
do so by implementing Newton’s algorithm for matrix inversion using our uniﬁed framework. The
pseudo code for the algorithm is as follows:
Algorithm 4 Pseudocode for running Newton’s algorithm for Matrix inversion for Titerations.
1:X T=A
2:fori= T;:::; 0do
3: Xi+1=Xi(2I AXi)
4:end for
Lemma 12. Consider a matrix A2Rdd, then for any >0there exists a transformer with 13
layers, 1 head and dimensionality r=O(d)that emulates Algorithm 4 with output X(transf )
1 that
satisﬁeskX(transf )
1 X1k.
Proof. The proof of this lemma is the code using the FLEQ instruction provided below ( Algo-
rithm 5). Let fmul,fsubandftransp be the functions that implement multiplication, substraction and
transpose respectively. Then, the following code runs Newton’s algorithm for matrix inversion.
30

--- PAGE 31 ---
Looped Transformers as Programmable Computers
Algorithm 5 Program to compute the approximate inverse using our Uniﬁed Attention Based
Computer
Require: mem[a] =A. .This is the location of the input.
Require: mem[p] = 2I,mem[x] =I,mem[y] =0,mem[q] = 1. .Constants.
Require: mem[t] = T. .Iteration counter, iinitialized as i:= T.
1:mem[x] =fmul(mem[x];mem[a]). .Initializes the result, X T:=A.
2:mem[a] =ftransp(mem[a];mem[y]) .Transpose A.
3:mem[y] =fmul(mem[a];mem[x]). .First sub-step of Newton’s algorithm, Y:=AXi
4:mem[y] =fsub(mem[p];mem[y]). .Second sub-step of Newton’s algorithm, Y:= 2I Y
5:mem[y] =ftransp(mem[y];mem[q]). .Transpose of Y.
6:mem[x] =fmul(mem[x];mem[y]). .Updating the result, Xi+1:=XiY
7:mem[t] =fsub(mem[t];mem[q]). .Increment counter, i:=i+ 1.
8:ifmem[t]0goto instruction 3. .Keep looping back as long as i0.
9:EOF. .End of File command.
Power Iteration. The Power Iteration algorithm (Algorithm 6) is used for ﬁnding the dominant
eigenvalue, the one that has the maximum absolute value, and corresponding eigenvector of a
diagonalizable matrix. The algorithm starts with an initial approximation of the eigenvector and
converges linearly to the eigenvector associated with the dominant eigenvalue; below we provide
the pseudocode.
Algorithm 6 Power Iteration
Input: A;T
1:Initializeb0=1
2:fork= 0;:::;T 1do
3: bk+1=Abk
4:end for
5:b=bT
kbTk
The last step in the algorithm above needs a normalization by the norm of bT. While we
can computekbTk2easily and precisely using the matrix multiplication function block (since
kbTk2=b>
TbT), computing the norm and taking its inverse using the function block from
Section 7 would induce error. Hence, we use the following Newton’s algorithm that converges
quadratically.
Algorithm 7 Newton’s algorithm to compute inverse square root: 1=p
S
Input:S
1:Initializex0= 1
2:fork= 0;:::;T do
3:xk+1=xk 3
2 S
2x2
k
4:end for
31

--- PAGE 32 ---
Looped Transformers as Programmable Computers
Lemma 13. Consider a matrix A2Rdd, then for any  >0there exists a transformer with
13layers, 1 head and dimensionality r=O(d)that emulates Algorithm 6 for T=O(log 1=)
iterations with output b(transf )
T+1 that satisﬁeskb(transf )
T+1 bT+1k.
Proof. The proof consists of translating each step of the pseudocode for Algorithm 6 and Algo-
rithm 7 to commands of our uniﬁed framework.
Algorithm 8 Program to simulate Power Iteration using our Uniﬁed Attention Based Computer
Require: mem[a] =A,mem[b] =1,mem[inv_norm ] = 1 ..Location of matrix and initialization.
Require: mem[q] = 1 ,mem[p] = 0 ,mem[r] = 0:5,mem[s] = 1:5 .Constants.
Require: mem[t1] =mem[t2] = T+ 1,
1:mem[a] =ftransp(mem[a];mem[p]). .Transpose of A.
2:mem[b] =fmul(mem[a];mem[b]). .Inner product: Abk.
3:mem[t] =fadd(mem[t1];mem[q]). .Increment counter, i:=i+ 1.
4:ifmem[t1]0goto instruction 2. .Keep looping back as long as i0.
5:mem[norm_square ] =fmul(mem[b];mem[b]). .CalculatekbTk2.
Code for Algorithm 7 begins.
6:mem[y] =fmul(mem[inv_norm ];mem[inv_norm ]). .Calculatex2
k.
7:mem[y] =fmul(mem[norm_square ];mem[y]). .CalculateSx2
k.
8:mem[y] =fmul(mem[r];mem[y]). .CalculateSx2
k=2.
9:mem[y] =fsub(mem[s];mem[y]). .Calculate (3 Sx2
k)=2.
10:mem[inv_norm ] =fmul(mem[inv_norm ];mem[y]). .Updatexk+1:=xk(3 Sx2
k)=2.
11:mem[t2] =fadd(mem[t2];mem[q]). .Increment counter, j:=j+ 1.
12:ifmem[t2]0goto instruction 6. .Keep looping back as long as j0.
Code for Algorithm 7 ends.
13:mem[b] =fmul(mem[b];mem[inv_norm ]). .b:=bT=kbTk.
14:EOF. .End of File command.
What other numerical linear algebra algorithms can transformers implement? The algo-
rithms presented above serve as proof of concept for the potential to build small linear algebra
libraries using our transformer construction. As demonstrated, the size of the looped transformer is
constant regardless of the depth. To implement iterative numerical algorithms, additional functions
can be incorporated into our architecture. For instance, QR decomposition, Gauss-Seidel, Arnoldi
iteration, or Lanczos algorithm can be implemented. While we have not included detailed code for
these speciﬁc algorithms, the above examples should provide sufﬁcient insight on how to do so.
9 Emulating Learning Algorithms at Inference Time
In this section we demonstrate the ability of our uniﬁed template to emulate Stochastic Gradient
Descent (SGD). We begin by examining the case of linear models, before extending our results to
the implementation of the backpropagation algorithm for two layer neural networks. Utilizing this
as a “function” which we call at each step, we demonstrate the application of SGD in updating the
implicit weights of a model.
32

--- PAGE 33 ---
Looped Transformers as Programmable Computers
Our work demonstrates that looped transformers can effectively perform in-context learning for
a wide range of models and achieve high levels of accuracy, given access to a sufﬁcient number of
inference calls/loops. Previous research, such as Akyürek et al. [2022] and Garg et al. [2022], has
limited in-context learning to a single inference call of a transformer model deeper than ours, which
restricts the types of models that can be learned and the level of accuracy that can be achieved. To
implement complex iterative programs like SGD, either a looped structure transformer or one that
grows in size with the program’s depth is required, unless widely believed complexity conjectures
are falsiﬁed. Additionally, this is the ﬁrst work to show that transformers can implement SGD on
more general loss functions and models beyond linear regression.
Stochastic Gradient Descent in linear models. In Algorithm 9 we provide the program for run-
ning SGD in linear models; that is we perform updates of the form: wt+1=wt PD
i=1(w>xi 
yi)xi, where wis the weight vector, (xi;yi)is the feature-label pair of the i th data point, and 
is the step-size. The program iterates through the Ddata points that the user gives and cycles back
to the ﬁrst point after one pass is completed. The step-size is given as input by the user.
Lemma 14. Let >0, there exists a transformer with 13 layers, 1 head and dimensionality
O(log(D) +d)that uses the Uniﬁed Attention Based Computer framework in Section 5.2 to
implementTiterations of SGD on a weight vector w2Rd, over a set ofDdata points (xi;yi)2
Rd+1,i= 1;:::;Dwith error up to . The step size is given as a parameter to the program.
Remark 4. The error is controlled by two parameters: the temperature and the constants used
in the proof of Lemma 7. Implementing arbitrary loss functions fand thus updates of the form
wt+1=wt PD
i=1f0(w>xi yi)xiwould introduce an extra error as a result of Barron’s
theorem (Theorem 3) applied in Lemma 5. Speciﬁcally, we would need in general poly(TD)heads,
in order to ensure control over this approximation error. However, if the derivative f0(x)of the loss
functionf(x)is a sum of sigmoids, the number of heads will be equal to the number of sigmoids
required, and there will be no error associated with this aspect of the construction.
33

--- PAGE 34 ---
Looped Transformers as Programmable Computers
Algorithm 9 Program to simulate SGD using our Uniﬁed Attention Based Computer
Require: mem[w] =w,mem[] =. .Location of the weight and step-size.
Require: mem[x0+i 1] =xi,i= 1;:::;D. .Location of the data points.
Require: mem[y0+i 1] =yi,i= 1;:::;D. .Location of the labels.
Require: px=x0. .pxis a pointer to the ﬁrst data.
Require: py=y0. .pyis a pointer to the ﬁrst label.
Require: pPC=instr 1. .Program Counter points to ﬁrst instruction.
Require: mem[q] = 1 ,mem[p] = 0 ,mem[z] =n. .Constants.
Require: mem[j] = D. .Within epoch iteration counter initialized to  n.
Require: mem[k] = T. .Epoch counter initialized to  T.
1:( instr 1)mem[temp ] =fmul(mem[px];mem[w]). .Inner product: w>xi.
2:( instr 2)mem[temp ] =fsub(mem[temp ];mem[py]). .Substract the label: w>xi yi.
3:( instr 3)mem[temp ] =fmul(mem[px];mem[temp ]). .Multiply with the data point xi.
4:mem[temp ] =fmul(mem[temp ];mem[]). .Multiply with the step-size.
5:mem[w] =fsub(mem[w];mem[temp ]). .Subtract from wone gradient step.
6:mem[instr 1] =fincr_pointer (mem[instr 1]): . Increment pointer.
7:mem[instr 2] =fincr_pointer (mem[instr 2]): . Increment pointer.
8:mem[instr 3] =fincr_pointer (mem[instr 3]): . Increment pointer.
9:mem[j] =fadd(mem[j];mem[q]). .Increment within epoch iteration counter by 1.
10:ifmem[j]0goto 1. .Cycle back to the ﬁrst data point.
11:mem[j] = D: . Reset counter.
12:mem[instr 1] =freset_pointer (mem[instr 1];x0): . Reset pointer.
13:mem[instr 2] =freset_pointer (mem[instr 2];y0): . Reset pointer.
14:mem[instr 3] =freset_pointer (mem[instr 3];x0): . Reset pointer.
15:mem[k] =fadd(mem[k];mem[q]). .Increment epoch counter by 1.
16:ifmem[k]0goto 1. .Cycle back to the ﬁrst data point.
17:EOF. .End of File command.
The following will detail the essential procedures for implementing the Stochastic Gradient
Descent algorithm. We employ three pointers, namely pPC,pxandpyin our algorithm. The ﬁrst
one, referred to as program counter, is used to iterate through the commands; after one pass over
all data points is completed, the program counter is reset to the ﬁrst instruction (line 16), until T
full passes have been completed. The second and third ones, referred to as data and label pointer
respectively, iterate through the features and labels one by one. The increment of the pointer px
needs to occur in both instructions 1 and 3, as to in the next iteration they have been updated from
instri(px;w;temp )!instri(px+ 1;w;temp ),i= 1;3. The same holds for the pointer pyin
line 7. Finally, we reset the two pointers in lines 13,14 to cycle back in the ﬁrst feature and label.
To enhance understanding, we note that lines 6-8 modify the instructions themselves; instead
of doing this we could have Dcopies of the lines 1-3, each one with parameters pointers of a
different (feature,label) pair. In that case the number of instructions would have been 7D.
Notice that the functions fincr_pointer andfreset_pointer can be directly implemented using Lemma 16.
Backpropagation and SGD. We will now generalize the result of Lemma 14 to two layer neural
networks with non-linear activation functions; we demonstrate in Algorithm 12 how this can be
achieved if the activation function is the sigmoid function.
34

--- PAGE 35 ---
Looped Transformers as Programmable Computers
Closest to this section is the work of Akyürek et al. [2022], where the authors prove that
constant number of layers is needed to perform one step SGD in linear models, using decoder only
transformer architecture.
Algorithm 10 Backpropagation
Loss function: J(x) =1
2x2
Input: W12Rmd,b12Rm,W22Rm1,b22Rx2Rd,y2R
1:Compute z=W1x+b1.
2:Compute a=(z).
3:Computeo=W2a+b2.
4:Compute2= (o y).
5:Compute1=0(z)W2(o y).
6:Compute@J
@W2=2a>.
7:Compute@J
@b2=2.
8:Compute@J
@W1=1x>.
9:Compute@J
@b1=1.
Lemma 15. Let >0, there exists a transformer with 13 layers, 1 head and dimensionality
O(log(D) +d)that uses the Uniﬁed Attention Based Computer framework in Section 5.2 to
implementTiterations of SGD on a two layer neural network, over a set of Ddata points
(xi;yi)2Rd+1,i= 1;:::;Dwith error up to . The step size is given as a parameter to the
program.
Remark 5. The program we provide in Algorithm 11 is implemented as an independent function,
which we call multiple times. Speciﬁcally, in line 1 of Algorithm 12 we call the algorithm for
backpropagation at each iteration with a different data point. In terms of our construction, this
translates to different instructions which will be in total O(D), each one with parameters pointers
to a different data point. However, as in Algorithm 9 the utilization of a pointer that changes the
instructions themselves, would result in a program of constant length; we did not do this in order
to contain the total length of the program.
Remark 6. If we want to account for different activation functions we can use Lemma 5 to express
the activation function and its derivative as sums of sigmoids. The number of heads would need to
be in that case poly(TD)to ensure control over the error induced by the approximation.
35

--- PAGE 36 ---
Looped Transformers as Programmable Computers
Algorithm 11 Program to simulate Backpropagation for two layer Neural Networks
Input: pw1;pw2;pb1;pb2 .Pointers to weights and biases.
Input: px;py .Pointer to data point and label.
Input:. .Pointer to step size.
Require: mem[q] = 1 ,mem[p] = 0 ,mem[r] = 1,mem[m] =m. .Constants.
Require: mem[k] = 1 . .Iteration counter, k:= 1.
Require: pz=z1
T. .Pointer forz.
Require: p=1
1;T. .Pointer for1.
1:( instr 1)mem[temp ] =ftrans(mem[pw1];mem[p]). .Create W>
1.
2:mem[z] =fmul(mem[temp ];mem[px]). .Multiply: W1x.
3:mem[z] =fadd(mem[z];mem[pb1]). .Add the bias: Compute z.
4:mem[a] =fsigmoids (mem[z];mem[q]). .Compute a=(z).
5:mem[temp ] =ftrans(mem[pw2];mem[p]). .Create W>
2.
6:mem[o] =fmul(mem[temp ];mem[a]). .Multiply: W2a.
7:mem[o] =fadd(mem[o];mem[pb2]). .Add bias: Compute o.
8:mem[2] =fsub(mem[o];mem[py]). .Compute2.
9:mem[1] =fmul(mem[pw2];mem[2]). .Multiply W22.
10:mem[flag] =fsub(mem[k];mem[m]). .Createk m.
11:mem[pz] =ftrans(mem[z];mem[p]). .Store zto consecutive memory cells.
12:mem[p] =ftrans(mem[1];mem[p]). .Store1to consecutive memory cells.
13:ifmem[flag]0goto 20. .If we iterated all the elements goto next command.
14:( instr 14)mem[temp0] =fsigmoids (mem[p];mem[pz]). .Create(zi).
15:mem[temp00] =fsub(mem[q];mem[temp0]). .Create 1 (zi).
16:mem[temp0] =fmul(mem[temp0];mem[temp00]). .Create0(zi) =(zi)(1 (zi)).
17:( instr 17)mem[p] =fmul(mem[temp0];mem[p]). .Create0(zi)(W2)i(o y).
18:mem[instr 14] =fincr_pointer (mem[instr 14]). .Point to next element of z.
19:mem[instr 17] =fincr_pointer (mem[instr 17]). .Point to next element of 1.
20:mem[k] =fadd(mem[k];mem[q]). .Increment counter, k:=k+ 1.
21:Ifmem[p]0goto 13. .Loop back.
22:mem[instr 1] =freset_pointer (mem[instr 14];z1
>). .Reset pointer.
23:mem[instr 15] =freset_pointer (mem[instr 15];1
1;>). .Reset pointer.
24:mem[grad _W2] =fmul(mem[2];mem[a]). .Create@J
@W2.
25:mem[grad _b2] =fmul(mem[2];mem[q]). .Create@J
@b2.
26:mem[grad _W1] =fmul(mem[1];mem[px]). .Create@J
@W1.
27:mem[grad _b1] =fmul(mem[1];mem[q]). .Create@J
@b1.
28:mem[temp ] =fmul(mem[gradW2];mem[]). .Multiply with step-size.
29:mem[pw2] =fsub(mem[pw2];mem[temp ]). .Update W2.
30:mem[temp ] =fmul(mem[gradW1];mem[]). .Multiply with step-size.
31:mem[pw1] =fsub(mem[pw1];mem[temp ]). .Update W1.
32:mem[temp ] =fmul(mem[gradb2];mem[]). .Multiply with step-size.
33:mem[pb2] =fsub(mem[pb2];mem[temp ]). .Update b2.
34:mem[temp ] =fmul(mem[gradb1];mem[]). .Multiply with step-size.
35:mem[pb1] =fsub(mem[pb1];mem[temp ]). .Update b1.
36

--- PAGE 37 ---
Looped Transformers as Programmable Computers
Algorithm 12 Program to simulate SGD using our Uniﬁed Attention Based Computer
Require: mem[w1] =W1;mem[w2] =W2. .Location weights and biases.
Require: mem[b1] =b1;mem[b2] =b2. .Location of biases.
Require: mem[x0+i 1] =xi,i= 1;:::;D. .Location of the data points.
Require: mem[y0+i 1] =yi,i= 1;:::;D. .Location of the labels.
Require: mem[z] =e: . Indicator for the choice of loss function
Require: px=x0. .pxis a pointer to the ﬁrst data.
Require: py=y0. .pyis a pointer to the ﬁrst label.
Require: pPC=instr 1. .Program Counter points to ﬁrst instruction.
Require: mem[q] = 1 ,mem[p] = 0 ,mem[z] =n. .Constants.
Require: mem[j] = D. .Within epoch iteration counter initialized to  n.
Require: mem[k] = T. .Epoch counter initialized to  T.
1:Backpropagation (w1;w2;b1;b2;px;py).Perform one step of SGD using Backpropagation
2:mem[j] =fadd(mem[j];mem[q]). .Increment within epoch iteration counter by 1.
3:px=fincr_pointer (px). .Show to next data point.
4:py=fincr_pointer (py) .Show to next label.
5:ifmem[j]0goto 1. .Cycle back until all data points are iterated.
6:mem[j] = D: . Reset counter.
7:px=freset_pointer (px;x0): . Reset pointer.
8:py=freset_pointer (py;y0): . Reset pointer.
9:mem[instr 3] =freset_pointer (mem[instr 3];x0): . Reset pointer.
10:mem[k] =fadd(mem[k];mem[q]). .Increment epoch counter by 1.
11:ifmem[k]0goto 1. .Cycle back to the ﬁrst data point.
12:EOF. .End of File command.
Generalizing to arbitrary depth. Our algorithm above is designed to emulate backpropagation
on a neural network that contains only one hidden layer. However, it is important to note that this
construction can be generalized to networks of arbitrary depth, with the caveat that the length of
the code will scale with the number of layers in the network. This is because each line of code in
our algorithm represents one cycle of the looped transformer, and the number of cycles required is
directly proportional to the depth of the network. It’s important to note that the number of cycles
of the looped transformer will be equal to the depth of the network. So the cost of this algorithm
is proportional to looping the transformer network as many times as the depth of the network.
This means that as the network becomes deeper, the computational cost of training it using our
algorithm will also increase.
10 Conclusion and Open Problems
In this paper, we have shown that transformer networks can be used as universal computers by
programming them with speciﬁc weights and placing them in a loop. We demonstrate that a
constant number of encoder layers can emulate basic computing blocks, such as lexicographic
operations, non-linear functions, function calls, program counters, and conditional branches. We
construct a one-instruction set computer (OISC) and use it to map iterative algorithms to programs
that can be executed by a transformer network. Our results include constant-depth transformers that
37

--- PAGE 38 ---
Looped Transformers as Programmable Computers
emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context
learning algorithm. Our ﬁndings reveal the potential of transformer networks as programmable
compute units and offer insight into the mechanics of attention.
Our study sheds light on the versatility of the attention mechanism and how even a single
loop can enable the creation of models that can mimic complex iterative algorithms and execute
general programs. Our ﬁndings also reveal the ability of transformer models to effectively perform
intricate mathematical and algorithmic tasks. It is possible that advanced transformer models like
GPT-3 use similar internal subroutines when given in-context examples and instructions. In a
sense, these models may have the ability to call upon a speciﬁc skill or algorithm, similar to a
function call, when given contextual examples and instructions. The unique aspect of this is that
the programming language of transformers is in natural language, rather than traditional code.
This opens up the possibility of using natural language commands to control and program these
models, further expanding their potential as programmable computers.
In conclusion, there are several open problems that warrant further exploration in the ﬁeld of
programmable computers using transformer networks. One of the most intriguing possibilities is
the potential to fuse hardcoded models with larger pretrained transformers, in order to harness the
strengths of both. Additionally, as our constructions currently do not take into account the language
aspect of the input, it would be interesting to investigate ways to tokenize input commands in
order to map them to natural language. Another promising avenue for research is the potential
for model distillation, in which larger networks could learn the skills performed by these looped
transformers. Additionally, experimental validation through the creation of even smaller networks,
trained on input-output pairs as well as internal representations, could provide further insight into
the capabilities of these designs. Finally considering what architecture changes would make the
above designs easier to implement and train, could lead to new insights in the ﬁeld.
References
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learn-
ing algorithm is in-context learning? investigations with linear models. arXiv preprint
arXiv:2211.15661 , 2022.
A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory , 39(3):930–945, 1993. doi: 10.1109/18.256500.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
François Charton. Linear algebra with transformers. arXiv preprint arXiv:2112.01898 , 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. 2022.
38

--- PAGE 39 ---
Looped Transformers as Programmable Computers
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-ﬁnetuned language
models. arXiv preprint arXiv:2210.11416 , 2022.
Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran,
James L McClelland, and Felix Hill. Language models show human-like content effects on
reasoning. arXiv preprint arXiv:2207.07051 , 2022.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819 , 2018.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. In International
Conference on Learning Representations , 2020.
Esolangs. Subleq. URL https://esolangs.org/wiki/Subleq .
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-
context? a case study of simple function classes. In Advances in Neural Information Processing
Systems , 2022.
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-
recurrent transformers. arXiv preprint arXiv:2203.07852 , 2022.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of NAACL-HLT , pages
4171–4186, 2019.
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and
Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR) , 54(10s):
1–41, 2022.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay
Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving
quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858 , 2022.
David Lindner, János Kramár, Matthew Rahtz, Thomas McGrath, and Vladimir Mikulik. Tracr:
Compiled transformers as a laboratory for interpretability. arXiv preprint arXiv:2301.05062 ,
2023.
Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers
learn shortcuts to automata. arXiv preprint arXiv:2210.10749 , 2022.
Farhad Mavaddat and Behrooz Parhami. Urisc: the ultimate reduced instruction set computer.
International Journal of Electrical Engineering Education , 25(4):327–334, 1988.
39

--- PAGE 40 ---
Looped Transformers as Programmable Computers
William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth
threshold circuits. Transactions of the Association for Computational Linguistics , 10:843–856,
2022.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. 2021.
Dmytro Perekrestenko, Philipp Grohs, Dennis Elbrächter, and Helmut Bölcskei. The universal
approximation power of ﬁnite-width deep relu networks. arXiv preprint arXiv:1806.01528 ,
2018.
Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing-complete. Journal of
Machine Learning Research , 22(75):1–35, 2021. URL http://jmlr.org/papers/v22/
20-302.html .
Jorge Pérez, Javier Marinkovi ´c, and Pablo Barceló. On the turing completeness of modern neural
network architectures, 2019. URL https://arxiv.org/abs/1901.03429 .
Zhiqiang Shen, Zechun Liu, and Eric Xing. Sliced recursive transformer. In European Conference
on Computer Vision , pages 727–744. Springer, 2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordv-
intsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
descent. arXiv preprint arXiv:2212.07677 , 2022.
Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on
approximating turing machines with transformers. Advances on Neural Information Processing
Systems (NeurIPS) , 2022a.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
language models. arXiv preprint arXiv:2206.07682 , 2022b.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022c.
Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International
Conference on Machine Learning , pages 11080–11090. PMLR, 2021.
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi
Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on
imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages
558–567, 2021.
40

--- PAGE 41 ---
Looped Transformers as Programmable Computers
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are
transformers universal approximators of sequence-to-sequence functions? In International
Conference on Learning Representations , 2019.
Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and
Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint
arXiv:2211.09066 , 2022.
41

--- PAGE 42 ---
Looped Transformers as Programmable Computers
A Ommited proofs
A.1 Addition of pointers.
Lemma 16. There exists a 1-hidden layer feedforward, ReLU network, with 8dactivations in the
hidden layer and dneurons in the output layer that when given two d-dimensional binary vectors
representing two non-negative integers, can output the binary vector representation of their sum,
as long as the sum is less than 2d+1.
Proof. For the purpose of explaining this proof, we use the f0;1gdbinary representation of the
integers, instead of the f1gdbinary representation. However, since the conversion of a bit
between the two representations can be done easily using simple afﬁne transformation, the proof
will also work for the f1gdbinary representation.
Let the two integers be a,band letc:=a+b. We assume that c <2d. Futher, let a1be
the least signiﬁcant bit of a,adthe most signiﬁcant, and aibe thei-th most signiﬁcant bit, and
similarly for bandc. Further, let a[i]represent the integer formed by considering only the least i
signiﬁcant bits of a.
Note thatciis only dependent on the least ibits ofaandb, and not on the more signiﬁcant bits
ofaorb. In particular, cionly depends on a[i]+b[i]. Deﬁnes:=a[i]+b[i], and note that ci=si.
Further note that s<2i+1and hence can be represented in i+ 1bits. Then, whenever ci= 1, there
can be two cases: (si+1= 1;si= 1) ; or(si+1= 0;si= 1) . This can be equivalently written as
ci= 1iffs2[2i 1;2i 1][[32i 1;2i+1 1]. This can be computed by the following ReLU:
ci= (ReLU (s 2i 1+ 1) ReLU (s 2i 1)) + ( ReLU (2i s) ReLU (2i s 1)) 1
+ (ReLU (s 32i 1+ 1) ReLU (s 32i 1)):
Thus, each bit of ccan be computed using 6 neurons. Hence, computing the entire sum needs
8dactivations, as to substract the residual.
A.2 Non-linear functions as sum of sigmoids
Lemma 17. Consider an input of the form
X=2
66664e0 x 0 0 0
0 0 0 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2dp2d+1p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
777752RN+dx3d:
wheredis chosen,Nis the number of functions we encode and dxis the dimension of x.e=ejan
indicator vector of the function we want to choose. Then there exists a transformer-based function
block with 3 layers, mheads and dimensionality O(d)such that
f(X) =2
66664   Pm
i=1cji(xTaji)
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2d p2d+1 p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775
42

--- PAGE 43 ---
Looped Transformers as Programmable Computers
wheredenoted inconsequential values that will be ignored downstream.
Proof. The ﬁrst thing we do is to move the xto the second row block, as follows:
X=2
66664e0 x 0 0 0
0 0 0 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2dp2d+1p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775!2
66664e0 0 0 0 0
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2dp2d+1p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775
This can be done using a ReLU feedforward layer that performs this using the last row of the input
as the indicator bit for the column containing x.
Then we want to create the following transformation
2
66664e0 0 0 0 0
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2dp2d+1p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775  !2
66664   Pm
i=1cji(xTaji)
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2d p2d+1 p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775
The proof follows that of Lemma 5. We again ignore the last three rows by setting the corresponding
rows in the key, query and values weight matrices to be zero. Let
Qi=0 Id
0 0
;Ki=[a1i:::aNi]0
0 0
;Vi=[c1i::: cNi]0
0 0
We note that for the purpose of this proof, each aihas one extra element at the end equal to
 log(3d 1), while the vectors xwill have the last element equal to one. Then we will have
S((KiX)T(QiX)) =2
6666664a>
ji0
0 0
0 0
0 0
0 0
0 03
77777750 0 x 0 0 0
0 0 0 0 0 0
=2
66666640 0 a>
jix 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 03
7777775
=2
6666664 (xTaji)  
     
     
     
     
     3
7777775
43

--- PAGE 44 ---
Looped Transformers as Programmable Computers
sincea>
jix=a>
jix log 3d 1and thusea>
jix=(3d 1 +ea>
jix) =(a>
jix)with a slight abuse of
notation over the inner product a>
jixto account for the extra corrections bias term. Thus,
VXS((KX)T(QX)) =2
66664 cji(xTaji)  
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 03
77775
By summing over all heads and adding the residual we get
2
66664 Pm
i=1cji(xTaji)  
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d 0 p d+2:2dp2d+1p2d+2:3d
0 0 2:d 1 0 d+2:2d 0 0 2d+2:3d3
77775
Finally, we use an extra layer similarly to Lemma 3 to write the result in the desired output. Hence,
we get2
66664   Pm
i=1cji(xTaji)
0 0 x 0 0 0
0 0 p 2d+1 0 0 0
p1p2:d0 pd+2:2d p2d+1 p2d+2:3d
0 0 2:d 1 0d+2:2d 0 0 2d+2:3d3
77775
However, we have another way of controlling the input, which is by the size of each attention
mechanism, that is directly controlled by the dimension dof the embedding.
Lemma 18. Consider an input of the form
X=2
664x::: x
0::: 0
1 e1:::1 em
e1::: em3
775(16)
where eiis the one hot vector with 1in thei th position and x2Rd. Letmbe the number of
sigmoids we need to represent a function, then there exists a one layer transformer with 1 head
such that
Attn( X) =2
664x::: x
(a>
1x):::  (a>
mx)
1 e1:::1 em
e1::: em3
775(17)
Proof. Let
K=2
640>00>e>
1............
0>00>e>
m3
75;Q=2
6664a>
10 Ce>
10>
............
a>
m0 Ce>
m0>
0>00>0>3
7775(18)
44

--- PAGE 45 ---
Looped Transformers as Programmable Computers
Hence,
KX=Id;QX=2
666664a>
1x C+a>
1x::: C+a>
1x
 C+a>
2x a>
2x::: C+a>
2x
............
 C+a>
mx C+a>
mx::: a>
mx
0 0 ::: 03
777775(19)
After applying softmax we get,
s((KX)>QX)2
666664(a>
1x) 0 ::: 0
0(a>
2x)::: 0
............
0 0 :::  (a>
mx)
 0:::3
777775;
for large enough C. Next we set
V=2
6640000::: 0
000c1::: cm
0000::: 0
0000::: 03
775
thus resulting in
VX=2
6666640 0::: 0 0
c1c2::: cm0
0 0::: 0 0
...............
0 0::: 0 03
777775
Hence, we get
VXs((KX)>QX) =2
666666640::: 0
c1(a>
1x)::: cm(a>
mx)
0::: 0
............
0::: 0
0::: 03
77777775;
and
X+VXs((KX)>QX) =2
664x::: x
c1(a>
1x)::: cm(a>
mx)
1 e1::: 1 em
e1::: em3
775:
45

--- PAGE 46 ---
Looped Transformers as Programmable Computers
Corollary 6. Consider an input of the form
X=2
664x::: 0
0::: 0
1 e1:::1 em
e1::: em3
775(20)
wheremis the number of sigmoids we use and eiis an indicator vector and x2Rd; then there
exists a 3 layer transformer with 1 head such that
Attn( X) =Pm
i=1(a>
ix):::Pm
i=1(a>
ix)
0::: 0
(21)
Proof. Given the input
X=2
664x::: 0
0::: 0
1 e1:::1 em
e1::: em3
775; (22)
we set the query and key matrices as follows:
K=Q=
0>01 1
:
Then, we get
(KX)>QX=2
64d ::: d
...:::...
d ::: d3
75:
Setting the value matrix to
2
664dI 0 0 0
0 0 0 0
0 0 0 0
0 0 0 03
775;
we get
VXS((KX)>QX) =2
664x:::x
0::: 0
0:::0
0:::03
775:
Hence, the output of the attention layer is:
X+VXS((KX)>QX) =2
6642x::: x
0::: 0
1 e1:::1 em
e1::: em3
775:
46

--- PAGE 47 ---
Looped Transformers as Programmable Computers
Note that using the embeddings in the last rows and a feedforward network can be used to produce
the following
2
664x::: x
0::: 0
1 e1:::1 em
e1::: em3
775:
Now, passing this into the transformer of Lemma 18 will result in
Attn( X) =2
664x::: x
c1(a>
1x)::: cm(a>
mx)
1 e1::: 1 em
e1::: em3
775: (23)
For the third layer, we set the key and query matrices as follows
K=Q=
0>01 1
:
Then, we get
(KX)>QX=2
64m ::: m
...:::...
m ::: m3
75:
Setting the value matrix to
2
6640 0 0 0
0m0 0
0 0 0 0
0 0 0 03
775;
we get
VXS((KX)>QX) =2
6640::: 0Pm
i=1ci(a>
ix):::Pm
i=1ci(a>
ix)
0::: 0
0::: 03
775:
Hence, the output of the attention layer is:
X+VXS((KX)>QX) =2
664x::: xPm
i=1ci(a>
ix):::Pm
i=1ci(a>
ix)
1 e1::: 1 em
e1::: em3
775:
Finally, the feedforward layers can be used to move the results to the ﬁrst row.
47

--- PAGE 48 ---
Looped Transformers as Programmable Computers
A.3 Matrix Transposition
Lemma 19. Fix>0and consider an input of the following form
X=2
664A 0 0 ::: 0
0 0 0 ::: 0
p1:dp1:dp1:d:::p1:d
P0
1P0
2P0
3:::P0
d3
775:
where A2Rdd; then there exists transformer-based function block with 4 layers, 1 head and
dimensionality r= 2d+ 2 logd=O(d)that outputs the following matrix
X=2
664A0A0A0:::A0
0 0 0 ::: 0
p1:dp1:dp1:d:::p1:d
P0
1P0
2P0
3:::P0
d3
775:
where A0=A>+M, for somekMk1.
Proof. We can vectorize the matrix Ainto ad2dimensional vector using the attention mechanism,
as shown in Eq. (24). Notice that once we have the matrix in this form we can implement its
transpose with a ﬁxed permutation of the columns of the matrix to get the vectorized form of A>.
Once we have the transpose in vector form, we matricize it back to get the matrix transform using
the attention mechanism. We explain the details of this process below:
Vectorization: We assume that the input is of the following form, where Ais the matrix to be
vectorized.
X=2
664A 0::: 0
0 0::: 0
p1:dp1:d:::p1:d
P0
1P0
2:::P0
d3
775:
Here, P0
irepresents a matrix of dcolumns, where each column is pi.
The ﬁrst layer uses the p1:dencodings to make dcopies of the matrix A, as follows:
X=2
664A 0::: 0
A A:::A
p1:dp1:d:::p1:d
P0
1P0
2:::P0
d3
775:
The feed forward part of the second layer then uses the encodings p0
ito vectorize the matrix in
the second row block as follows:
X=2
66664A ::: 0A(1;1)::: A (1;d)
0::: 0
:::A(d;1)::: A (d;d)
0::: 0
p1:d::: p1:d
P0
1 ::: P0
d3
77775: (24)
48

--- PAGE 49 ---
Looped Transformers as Programmable Computers
This is achieved, by explicitly deﬁning a neural network that keeps the i th row if the correspond-
ing encoding is P0
iand place it in the d+ 1row.
Transposition in the vector form: Once we have the matrix vectorized as the second row block
of the scratchpad, the following key and query matrices
K=0 0 I 0
0 0 0 I
;Q=0 0 0 I
0 0 I 0
;
results in the head outputting the following, which is the vectorized form of A>(in the second row
block)
XS((KX)>(QX)) =2
66664 :::A(1;1)::: A (d;1)
0::: 0
:::A(1;d)::: A (d;d)
0::: 0
P0
1 ::: P0
d
p1:d::: p1:d3
77775:
Then, using the following value matrix gives
V=2
6640 0 0 0
0 I 0 0
0 0 0 0
0 0 0 03
775;
VXS((KX)>(QX)) =2
666640 ::: 0A(1;1)::: A (d;1)
0::: 0
:::A(1;d)::: A (d;d)
0::: 0
0 ::: 0
0 ::: 03
77775;
Adding back the X(see (1)), results in
X+VXS((KX)>(QX)) =2
66664A ::: 0A(1;1)::: A (d;1)
0::: 0
:::A(1;d)::: A (d;d)
0::: 0
p1:d::: p1:d
P0
1 ::: P0
d3
77775:
Using the feedforward layers and the encodings P0
i, we get
X=2
66664A ::: 0A(1;1)::: A (d;1)
0::: 0
:::0::: 0
A(1;d)::: A (d;d)
p1:d::: p1:d
P0
1 ::: P0
d3
77775:
Using an attention layer and the ﬁrst row of encodings, we get
X=2
664A>:::A>
0::: 0
p1:d:::p1:d
P0
1:::P0
d3
775:
49

--- PAGE 50 ---
Looped Transformers as Programmable Computers
A.4 Matrix Multiplication by Linearizing the Softmax
We will show how we can implement matrix multiplication so that it will ﬁt our uniﬁed template.
To do so, we need to show for example for the result of A>B, where A2RkmandB2Rkn
withk;m;n<d we can achieve the following:
A 0 B 0 0
0 0 0 0 0
  !    A>B
0 0 0 0 0 0
Lemma 20. LetA2RkmandB2Rkn; then for any >0there exists a transformer-based
function block with 2 layers, 1 head and dimensionality r=O(d)that outputs the multiplication
ATBT+M, for somekMk1.
Corollary 7. LetA2RkmandB2Rkn; then for any >0there exists a transformer-based
function block with 2 layers, 1 head and dimensionality r=O(d)that outputs the multiplication
B>A+M, for somekMk1.
Corollary 8. LetA2RkmandB2Rkn; then for any >0there exists a transformer-based
function block with 2 layers, 1 head and dimensionality r=O(d)that outputs the multiplication
B>B+M, for somekMk1.
Corollary 9. LetA2RkmandB2Rkn; then for any >0there exists a transformer-based
function block with 2 layers, 1 head and dimensionality r=O(d)that outputs the multiplication
A>A+M, for somekMk1.
We will prove just the ﬁrst of these results and the rest are a simple corollary of it.
Proof. LetM2R2d2d,A2RkmandB2Rknbe the following matrices:
M=A 0 B 0
0 0 0 0
:
The zeros pad the rows and columns to ensure that the matrix Mis2d2d. Then, consider the
input matrix to be of the following form:
X=2
6666664M 0 0
0 11>0
I 0 0
p(1)
p(2)
0 1T03
7777775
where 12R2dis the all ones vector. The identity matrix Iand the all ones matrix 11>are part
of the design of the input and they are always ﬁxed. For now we ignore the encodings and the
last row, by setting the corresponding rows of the key,query and value weight matrices to be zero.
These rows will be used to copy the output to the place that we want.
50

--- PAGE 51 ---
Looped Transformers as Programmable Computers
Focusing on the rest of the rows, we set the key and query weight matrices to be
K=I;Q=2
4cI 0 0
0 0CI
0 I 03
5;V=2
40 0 0
0 0neCDd
0 0 03
5
whereDd2R2d2dis the diagonal matrix with the ﬁrst ddiagonal elements 1, and the rest 0. Thus
we have
(KX)>QX=2
4M 0 0
0 11>0
I 0 03
5>2
4cM 0 0
CI 0 0
0 11>03
5 (25)
=2
4cM>M 11>0
C11>0 0
0 0 03
5 (26)
Each of the ﬁrst 2dcolumns above looks as follows

cz1icz2i::: czniC1>0
After we apply the softmax sper column, we get
s(czij) =eczij
Pn
j=1eczij+n(eC+ 1)
wheren= 2d,zijis the (i;j)element of the matrix M>M. Let`()be the transformation above
then we have
VXS((KX)>QX) =2
40 0 0
neCDd0 0
0 0 03
52
4`(cM>M) 
  
  3
5
=2
40 0 0
neCDd`(cM>M) 
0 0 03
5
2
40 0 0
11>+cM>M 
0 0 03
5
and by adding back the residual we have
X=2
4M 0 0
11>+cM>M 
I 0 03
5
51

--- PAGE 52 ---
Looped Transformers as Programmable Computers
for small enough cand large enough C. This is because
neCecxij
Pn
j=1ecxij+n(eC+ 1)=ecxij1
1 +Pn
j=1ecxij C logn+n
= (1 +cxij+O((cxij)2))(1 ecxij C logn+O(e2(cxij C logn)))
= (1 +cxij+O((cxij)2))(1 ecxij C logn)
(1 +cxij)
We now use the feedforward layers to perform the following transform
X=2
4  
M>M 
  3
5
=2
6666664    
A>A 0 A>B 0
0 0 0 0 
B>A 0 B>B 0
0 0 0 0 
    3
7777775
Now if p(1)=
0 0 p 2d+1:2d+n0 0
andp(2)=
p1:npn+1:d0 pd+n+1:2dp2d:3d
we
can copy A>Bto the desired place using Lemma 2.
B Error Analysis
In all of this section we assume that each element of the input matrix Xhas valuesvibounded by
some constant G,i.e.,jvijG.
The error in the read /write operation. The positional encodings as we have already men-
tioned have the following properties: piis anlog(n)dimensional1vector which is the binary rep-
resentation of iwith 1in the place of 0. Hence, we have p>
ipi= log(n)and each p>
ipj<log(n)
fori6=j.
Each time a copy is implemented from one column to another, we create a permutation matrix
(a matrix of zeros and ones) which then multiplies the input matrix X2Rdnfrom the right and
results in permutations of the column space. We thus focus on just one column of the nnmatrix
that is created after we apply the softmax. Let zbe this column of the matrix, ideally we want
to output in one position 1and in the rest 0. In the place that we want to output 1, say thea th
position, we have the inner product za=p>
ipifor somei2[n]. The rest of the elements in the
same column would be zbp>
ipjfori6=janda6=b. Then,
[S((KX)>QX)]i;i=ep>
ipi
ep>
ipi+P
j6=iep>
ipj
=1
1 +P
j6=iep>
ipj=ep>
ipi
52

--- PAGE 53 ---
Looped Transformers as Programmable Computers
Sincep>
ipj<p>
ipi fori6=j, we have that
[S((KX)>QX)]i;i1
1 +ne 
1
1 +elogn 
1 elogn 
1 +elogn 
1 elogn 
Thus, fori6=j,[S((KX)>QX)]i;jelogn . This implies that there exist i,i= 1;:::;n such
that
za= 1 "a;for some"aelogn 
zb="bforb6=aand for some "belogn 
Hence, we have that
z=z+"
where zis the targeted vector and "is the vector containing the errors "a;"b.
Now let xibe thei th row of the input matrix X, then we have
Xz=Xz+X"
=Xz+2
64hx1;"i
...
hxd;"i3
75
In the general case that all the columns will change, let P=S((KX)>QX)andPbe the
targeted matrix then we have that
XP=XP+XE
where E=
"1::: "n
is the matrix containing all the errors and so
kXP XPk= max
1jndX
i=1jhxi;"jij
Gn2delogn 
elogGdn3 
Thus, if>logGdn3
we have that
kXP XPk
53

--- PAGE 54 ---
Looped Transformers as Programmable Computers
The error in Matrix Multiplication . This error has already been calculated in Appendix A.4,
however we explicitly deﬁne it here as follows:
neCecxij
Pn
j=1ecxij+n(eC+ 1)=ecxij1
1 +Pn
j=1ecxij C logn+n
= (1 +cxij+O((cxij)2))(1 ecxij C logn+O(e2(cxij C logn)))
Letc=1
C1Gfor some constant C1andC= logC2
2for someC2then we have
A=neCecxij
Pn
j=1ecxij+n(eC+ 1)
=ecxij1
1 +Pn
j=1ecxij C logn+n
= (1 +cxij+2
1x2
ij
G2)(1 ecxij2
n+e2cxij2
2
n2)
= (1 +cxij)(1 ecxij2
n+e2cxij2
2
n2) +2
1x2
ij
G2(1 ecxij2
n+e2cxij2
2
n2)
Thus,
jA (1 +cxij)j=j (1 +cxij)ecxij2
n+e2cxij2
2
n2+2
1x2
ij
G2(1 ecxij2
n+e2cxij2
2
n2)j
2
1(e1=C12
n+ 2e21=C12
2
n2) +e1=C12
n(1 +1
C1)
4e1=C12
n
Hence if2==4and1=C1log(n)we have that the total error is less than .
Function approximation. The error in Lemma 5 is an immediate consequence of Theorem 3
and it is proportional to 1=pm, wheremis the number of heads we are using.
Accumulation of error after Toperations. Fix an>0and assume that in the t th iteration
the input is Xt=X
t+tMt, where X
tis the ideal input 0<t<t
TandMtis a matrix such
thatkMtk 1, we will show that Xt+1=X
t+1+t+1Mt+1, where X
t+1is the ideal input,
0<t+1<(t+ 1)
TandMt+1is a matrix such that kMt+1k1.
• Matrix Multiplication with a matrix A,kAk13will have the following result:
AXt+0=AX
t+tAMt+0M0=X
t+1+ (t+0)Mt+1
3Notice that this can be assumed without loss of generality, since we can normalize all the errors with the maximum
norm of a matrix to the power of T.
54

--- PAGE 55 ---
Looped Transformers as Programmable Computers
where0is controlled by the constants we use in the design of the function block and Mt+1
is some matrix with kMt+1k1. If now0<
T, our claim follows.
•Read/Write operations will result to an error of
XtP=XtP+0M0=X
tP+tMtP+0M0
Notice that as before, since kM0k1andkMtPk1and thus we have Xt+1=XtP=
X
t+1+t+1Mt+1, wheret+1=t+0. Again if0
Tthe result follows.
• The result for function approximation follows in a similar way.
Csubleq is Turing Complete
In this section, we show that our slightly restricted version of the original SUBLEQ instruction
[Mavaddat and Parhami, 1988] is indeed also Turing complete. To do this, we will utilize Minsky
machines, which are also Turing complete. A Minksy machine comprises of registers and a list of
instructions, where each instruction can be either of the following two instructions
•add(a):mem[a] :=mem[a] + 1 , go to the next instruction.
•sub(a, n): If mem[a] == 0 , go to instruction n. Otherwise mem[a] :=mem[a] 1, go to the
next instruction.
Given a program written in a language above, we translate it into an equivalent one written in
ourSUBLEQ language. For this, we initialize three ﬁxed locations / registers c 1;c0, andc+1such
thatmem[c 1] := 1,mem[c0] := 0 , andmem[c+1] := +1 ; as well as an extra register mem[b]. We
translate the program instruction-by-instruction. Assume that we have translated the ﬁrst i 1
instructions. Let j 1be the index of the last (translated) SUBLEQ instruction, that is, the index
of the next SUBLEQ instruction will be j. Then, for the i-th instruction in the Minsky machine
language, we translate it into our language as follows:
•Case 1, The i-th instruction of the Minsky machine program is add(a). This is equivalent
toSUBLEQ (a;c 1;j+ 1) , and hence the jinstruction in our program will simply be
SUBLEQ (a;c 1;j+ 1).
•Case 2, The i-th instruction in the Minsky machine program is sub(a;n). This would be
equivalent to the sequence of the following 5 SUBLEQ instructions.
Algorithm 13 Translation for sub(a;n)
Instr.j :SUBLEQ (b;b;j + 1)
Instr.j+ 1:SUBLEQ (b;a;j + 3)
Instr.j+ 2:SUBLEQ (a;c+1;j+ 5)
Instr.j+ 3:SUBLEQ (a;c0;n0)
Instr.j+ 4:SUBLEQ (a;c+1;j+ 5)
55

--- PAGE 56 ---
Looped Transformers as Programmable Computers
Heren0is the index of the translation of the n-th instruction of the Minsky machine program.
This can be computed as a function of the number of add andsub instructions up to
instructionn. The correctness of the above can be veriﬁed by considering the three cases:
mem[a]1,mem[a] 1;andmem[a] = 0 .
D Single Instruction Set
Each instruction consists of the following tuple: (pa;pb;pc;pﬂag;pm;pp), and does the following
1.mem [c] =fm(mem [a];mem [b])
2. ifmem [ﬂag](0;0)0goto instruction p
Here, locations a;b, andccan contain either scalars, or d-dimensional vectors or ddmatrices,
andmem[ﬂag](0;0)is the 1-st entry of mem[ﬂag]if it is a vector / matrix, else it is mem[ﬂag]if a
scalar.
This can be implemented using the following steps (each may use a separate layer of trans-
former):
At the beginning of each iteration, the scratchpad starts with storing the pointer to the next
instruction pt.
1. Read the command (pa;pb;pc;pﬂag;pp;pm)from the location to the scratchpad.
2.Copy thedddata at locations a;bto the scratchpad memory scratchMem (assume the
data isddeven if actually scalar or vector, the fmimplementation will handle that)
3. Copy the data to the i-th function row block using the feed forward layer.
4. Once in the correct row block, fm(mem[a];mem[b])is computed
5.Feedforward layers copy back the data from i-th row block to the scratchpad memory
scratchMem .
6. Write result from scratchpad memory to pc.
7. ifmem[ﬂag](0;0)0storeppin the scratchpad, else pt+1
56

--- PAGE 57 ---
Looped Transformers as Programmable Computers
Figure 7: The structure of input X
The structure of the input Xis shown in Fig. 7. It has ncolumns and O(Md+ logn)rows. It
is partitioned into 3 column blocks: the Scratchpad block, the Memory block, and the Instructions
block. The Memory block is the storage and is the location where all the variables are stored. Each
variable can be either a scalar, vector or matrix, as long as the number of rows in it are no larger
thand. For example, if a variable is a ddmatrix, it is stored in dconsecutive columns in the
block, where each column has length d. The address of this variable is the index of its ﬁrst column
in the input X. The Instructions block contains instructions, where each instruction is a vector of
the form
c=2
666666666666666664pa
pb
pc
pm
pﬂag
pp
dh
dw
b(1)
mask
b(2)
mask
b(3)
mask3
777777777777777775;
which encodes the following logic:
mem[c] =fm(mem[a];mem[b]) ; ifmem[ﬂag]0goto instruction p:
pa;pb;pc;pp;andpﬂagare all binary1vectors that point to the locations a;b;c;p; andﬂag
respectively. These are simply the binary representations of the integers a;b;c;p andﬂag, and
hence have length log2neach. Similarly, pmis the binary vector representation of the integer m,
and hence has length log2M, whereMis the number of functions we implement. The bmaskis
mask bit used while writing the output back to memory.
The scratchpad has scolumns. The length sdepends on the maximum number of columns
needed by the function blocks to operate, and can be as low as O(1)for scalar and vector functions,
O(d)for matrix functions, and can be as high as O(d2)if functions like matrix vectorization are
one of theMfunctions. The Scratchpad consists of the following parts:
57

--- PAGE 58 ---
Looped Transformers as Programmable Computers
• The program counter is a row block with log2nrows andscolumns and takes the form:

pipipi:
This signiﬁes that the current program counter points to the i-th instruction. Using this, the
i-th instruction is read into all the scolumns of ‘Current Instruction’ row block.
•The Current Instruction row block has O(logn)rows andscolumns, and each column
initially contains the i-th instruction once it is read. Then, the instructions in each column
are slightly modiﬁed depending on the column index, to read memory blocks pointed to in
the instruction. The memory blocks are read into the ‘Scratchpad Memory’.
•The Scratchpad Memory is a temporary location where the data is ﬁrst read into from the
Memory column block, before it is moved to the correct function’s Function Block, using
the function index encoding pmin the instruction.
•The encodings row block has O(logn)rows andncolumns, and is used to index every
column in the input X. It contains the binary 1vector encodings of the column index for
each column. The details of this row block are explained later.
•The Function Blocks are custom transformer blocks that can be added in a plug-n-play
manner to the Uniﬁed Attention Based Computer depending on what ‘elementary’ functions
the user wants the computer to have access to.
X=2
666666666666666666640 0:::0 zs+1:::zm+scm+s+1
0
:::cn
0
ptpt:::pt:::  :::
c1
t c2
t:::cs
t:::  :::
z1
atz2
at:::zs
at0::: 0 0 ::: 0
z1
btz2
bt:::zs
bt0::: 0 0 ::: 0
z1
ctz2
ct:::zs
ct0::: 0 0 ::: 0
0 0:::0 ps+1:::pm+spm+s+1::: pn
p1p2:::ps0::: 0 0 ::: 0
f1mem::: ::: :::::::::
.....................:::
fMmem::: ::: :::::::::3
77777777777777777775
58

--- PAGE 59 ---
Looped Transformers as Programmable Computers
D.1 Step 1
In this step, we need to copy the t-th instruction, pointed to by the program counter pt, to the
scratchpad’s Current Instruction block. We denote the instruction by ctwhere
ct=2
666666666666666664pat
pbt
pct
pﬂagt
ppt
pmt
dh
dw
b(1)
mask
b(2)
mask
b(3)
mask3
777777777777777775
For this step, we only consider the following relevant subset of rows of the matrix X:
X=2
6640 0:::0 ::: cm+s+1:::cn
ptpt:::pt:::  :::
c1
tc2
t:::cs
t:::  :::
0 0:::0 ps+1:::pm+spm+s+1:::pn3
775
The other rows will not be used or changed during this operation because we can simply set the
corresponding rows of the K;V;Qmatrices to 0 for all heads and setting the feed forward layers
to also pass the corresponding rows unchanged.
At the beginning of execution of each command, the Current Instruction row block would be
empty, so the input would look like
X=2
664 :::   ::: cm+s+1:::cn
ptpt:::pt:::  :::
0 0:::0 0::: 0 0 :::
0 0:::0 ps+1:::pm+spm+s+1:::pn3
775
Then, consider an attention head with the following K;Q;Vmatrices:
K=
0 0 0 I
;Q=
0 I 0 0
;V=2
6640 0 0 0
0 0 0 0
I 0 0 0
0 0 0 03
775
This will result in
X=2
664 :::   ::: cm+s+1:::cn
ptpt:::pt:::  :::
ctct:::ct:::  :::
0 0:::0 ps+1:::pm+spm+s+1:::pn3
775:
59

--- PAGE 60 ---
Looped Transformers as Programmable Computers
We apply Lemma 16 on the row blocks
ctct:::ct::: :::
p1p2:::ps0:::0 0:::0
to construct feedforward layers that convert cttoci
t, where
ci
t=2
666666666666666664pat+i
pbt+i d
pct+i 2d
pﬂagt
ppt
pmt
dh
dw
b(1)
mask= 1 (idw)
b(2)
mask= 1 (i>d)+ 1 (id+dw) 1
b(3)
mask= 1 (i>2d)+ 1 (i2d+dw) 13
777777777777777775:
Note that the last three elements can be created using the following ReLU:
b(1)
mask=ReLU (2d+dw i+ 1) ReLU (2d+dw i)
b(2)
mask=ReLU (i d) ReLU (i d 1) + ReLU (d+dw i+ 1) ReLU (d+dw i) 1
b(3)
mask=ReLU (i 2d) ReLU (i 2d 1) + ReLU (2d+dw i+ 1) ReLU (2d+dw i) 1:
At the end of this step, we get the following:
X=2
6640 0:::0 ::: cm+s+1:::cn
ptpt:::pt:::  :::
c0
tc1
t:::cs
t:::  :::
0 0:::0 ps+1:::pm+spm+s+1:::pn3
775;
D.2 Step 2
Use three heads, one each for pa;pbandpc.
Using the vectors pat+i;pbt+i d, andpct+i 2dwe copy the data (using one head each and a
similar technique as last step) to get the following in the Scratchpad Memory:
2
4zat:::zat+d::: ::: ::: :::
:::zbt:::zbt+d::: ::: :::
::: :::zct:::zct+s 2d::: :::3
5
Using the mask bits at the end of ci
t, we get
2
6664zat:::zat+dw 10 zbt:::zbt+dw 10 zct:::zct+dw 10 0:::0:::
0::: 0 0 0 ::: 0 0 0 ::: 0 0 0:::0:::
0::: 0 0 0 ::: 0 0 0 ::: 0 0 0:::0:::3
7775(27)
60

--- PAGE 61 ---
Looped Transformers as Programmable Computers
zi[1 :d] =ReLU (zi[1 :d] C(1 b(1)
mask)1) ReLU ( zi[1 :d] C(1 b(1)
mask)1)
+ReLU (zi[d+ 1 : 2d] C(1 b(2)
mask)1) ReLU ( zi[d+ 1 : 2d] C(1 b(1)
mask)1)
+ReLU (zi[2d+ 1 : 3d] C(1 b(1)
mask)1) ReLU ( zi[2d+ 1 : 3d] C(1 b(1)
mask)1);
zi[d+ 1 : 3d] =0;
whereCis a large positive constant.
Using the same mask bits, we also mask the row containing the output data pointers for c:
h
0:::0 0:::0 pct:::pct+dw 10:::0 0:::0 0:::0i
(28)
D.3 Step 3
The following feedforward ReLU layer can move the data to the correct function blocks:
fkmem[1 :dh] = ( ReLU (z[1 :dh] C((1 b(1)
mask b(2)
mask)1+ logM p>
kpm))
 ReLU ( z[1 :dh] C((1 b(1)
mask b(2)
mask)1+ logM p>
kpm)));
whereCis a large positive constant.
D.4 Step 4
Each of the Mfunctions have their own attention heads, which are constructed to be copies of
their transformer based function blocks. The results after the attention are written back into their
respective row blocks. Since the row blocks are separate, the feedforward layers of each of the
transformer based function blocks also work in parallel to store the ﬁnal results in the respective
row blocks.
D.5 Step 5
Similar to Step 3 we use the following feedforward ReLU layer to move the data from the function
block back into the scratchpad memory
z[1 :dh] =z[1 :dh] +MX
k=1
ReLU ((fkmem[1 :dh] z[1 :dh]) C((1 b(3)
mask)1+ logM p>
kpm))
 ReLU ( (fkmem[1 :dh] z[1 :dh]) C((1 b(3)
mask)1+ logM p>
kpm))
;
whereCis a large positive constant.
61

--- PAGE 62 ---
Looped Transformers as Programmable Computers
D.6 Step 6
For this step we focus on the encoding row block, memory storage row block and the following
rows in the input (see (28), (27)):
2
66666666640:::0 0:::0 0::: 0 0:::0 zs+1:::zm+s2
4cm+s+1
03
5:::2
4cn
03
5
0:::0 0:::0 znew
ct:::znew
ct+dw0:::0 0::: 0 0 ::: 0
0:::0 0:::0 pct:::pct+dw0:::0 0::: 0 0 ::: 0
0:::0 0:::0 0::: 0 0:::0 ps:::pm 1 pm:::pn 13
7777777775
We set the Key and Query weight matrices as follows:
K=Q=2
66666640
0
I
I3
7777775:
V=2
66666640 0 0 0
I I 0 0
0 0 0 0
0 0 0 03
7777775
VXS((KX)>QX)
=2
66666666664::: 0::: 0:::0::: 0 0 ::: 0 0 ::: :::
:::dnew
ct+dct
2:::dnew
ct+dw+dct+dw
2:::d0:::dct 1dnew
ct+dct
2:::dnew
ct+dw+dct+dw
2dct+dw+1::: :::
::: 0::: 0:::0::: 0 0 ::: 0 0 ::: :::
::: 0::: 0:::0::: 0 0 ::: 0 0 ::: :::3
77777777775
Finally, we use the feedforward layers similar to the proof of Lemma 3 to write back [dnew
ct:::dnew
ct+dw]
to the correct rows.
D.7 Step 7
This step is identical to Section 4.3.
62

--- PAGE 63 ---
Looped Transformers as Programmable Computers
E Calculator
Lemma 21. Given two constants ;2[0;1], there exists a 1 hidden layer neural network fwith
threshold activation and dactivations in the hidden layer, such that
8x2[ C; ][[;C];f(x) 1
x;
as long asd= 
(log(1=())
+ logC).
Proof. We partition [;C]into the following intervals
[;(1 +));[(1 +);(1 +)(1 +(1 +))):::;[ai;ai(1 +ai));:::;
that is, if an interval begins at a, then it ends at a(1+a). Note that for any point x2[ai;ai(1+ai))
1
x 1
ai=1
ai 1
x
<1
ai 1
ai(1 +ai)
=
1 +ai<:
Hence two output activations of the form1
ai1xai 1
ai1x<ai(1+ai)can be used to approximate1
xin
[ai;ai(1 +ai)).
Thus, all that remains is to compute the number of such intervals, and using that we get the
number of output activations in the hidden layer. Towards that end, if the i-th interval begins at ai,
ai=ai 1(1 +ai 1)ai 1(1 +) =(1 +)i 2:
Hence,
8i2 +log 1=()
log(1 +);ai1
:
Noting that log(1 +)>
2for;2[0;1], we get that
8i2 +2 log 1=()
;ai1:
Once we have that ai1
, the number of further partitions needed to reach Cwould beO(logC)
as shown below:
aj=aj 1(1 +aj 1)aj 1
1 +1

= 2aj 1:
Hence, the total number of partitions needed is O(log(1=())
+ logC).
We can similarly approximate 1=xon[ C; ]with the same number of output activations.
63

--- PAGE 64 ---
Looped Transformers as Programmable Computers
Lemma 22. Given2[0;1], there exists a 1 hidden layer neural network fwith threshold
activation and dactivations in the hidden layer, such that
8x2[0;C];f(x) px;
as long asd= 
(p
C
).
Proof. We partition [0;C]into the following intervals
[0;2));[2;42):::;[i22;(i+ 1)22);::::
Note that for any point x2[i22;(i+ 1)22)
jpx p
i22j<p
(i+ 1)22 p
i22=:
Hence two output activations of the form i1xi22 i1x<(i+1)22can be used to approximatepx
in[i22;(i+ 1)22).
Thus, all that remains is to compute the number of such intervals, and using that we get the
number of output activations in the hidden layer. It is easy to see that the total number of intervals
needed would bep
C
.
64
