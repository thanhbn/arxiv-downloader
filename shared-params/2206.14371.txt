# 2206.14371.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/shared-params/2206.14371.pdf
# File size: 2535875 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Matryoshka: Stealing Functionality of Private ML
Data by Hiding Models in Model
Xudong Pan, Yifan Yan, Shengyao Zhang, Mi Zhang, Min Yang
School of Computer Science
Fudan University, China
fxdpan18,yanyf20g@fudan.edu.cn, shengyaozhang21@m.fudan.edu.cn, fmizhang,m yangg@fudan.edu.cn
Abstract ‚ÄîHigh-quality private machine learning (ML) data
becomes a key competitive factor for AI corporations. Despite
being carefully stored in local data centers, a private dataset
may still suffer from piracy once a deep neural network (DNN)
model trained on the dataset is made public or accessible as a
prediction API. If it is the innate tension between the open model
interface and the private dataset that leaves an exploitable attack
surface, we wonder if a private dataset with no exposed interface
can be impregnable .
In this paper, we present a novel insider attack called Ma-
tryoshka , which employs an irrelevant scheduled-to-publish DNN
model as a carrier model for covert transmission of multiple secret
models which memorize the functionality of private ML data
stored in local data centers. Instead of treating the parameters
of the carrier model as bit strings and applying conventional
steganography, we devise a novel parameter sharing approach
which exploits the learning capacity of the carrier model for
information hiding. Matryoshka simultaneously achieves: (i) High
Capacity ‚Äì With almost no utility loss of the carrier model,
Matryoshka can hide a 26larger secret model or 8secret
models of diverse architectures spanning different application
domains in the carrier model, neither of which can be done with
existing steganography techniques; (ii) Decoding EfÔ¨Åciency ‚Äì once
downloading the published carrier model, an outside colluder
can exclusively decode the hidden models from the carrier model
with only several integer secrets and the knowledge of the hidden
model architecture; (iii) Effectiveness ‚Äì Moreover, almost all the
recovered models have similar performance as if it were trained
independently on the private data; (iv) Robustness ‚Äì Information
redundancy is naturally implemented to achieve resilience against
common post-processing techniques on the carrier before its
publishing; (v) Covertness ‚Äì A model inspector with different
levels of prior knowledge could hardly differentiate a carrier
model from a normal model.
Index Terms ‚Äîdeep learning, data privacy, covert transmission
I. I NTRODUCTION
‚ÄúThe availability of large datasets are key enablers of deep
learning ‚Äù, as stated by Bengio et al. in the Turing lecture [1].
High-quality private machine learning (ML) datasets become
critical assets for AI corporations to train performant ML mod-
els [2]. As the dataset preparation is highly time-consuming
and labor-intensive, it is common for relevant parties to hold
the private ML data as conÔ¨Ådential properties [3].
Despite the ML data being carefully curated in local data
centers isolated from the open network [4], recent research on
ML privacy [5]‚Äì[9] reveals, once a deep neural network (DNN)
Ô¨Ånishes its training process on a private dataset, the model
Fig. 1. The attack scenario of Matryoshka.
immediately becomes an exploitable source for privacy breach.
By interacting with the trained model in a full-knowledge
manner (i.e., with known parameters, model architecture,
etc.) or via the prediction API, attacks are known to infer
global/individual sensitive attributes of the training data [10]‚Äì
[13], infer data membership [14], [15], or even steal the full
functionality of the private datasets from the trained model
[5], [6], [8]. The success of these previous attacks reÔ¨Çect the
tension between the conÔ¨Ådential data and the openly accessible
trained model. We wonder, if blocking this source of leakage,
whether a private dataset with no exposed open interfaces is
immune to data stealing attacks.
Our Work. We are the Ô¨Årst to discover the severe leakage of
data functionality can happen even for private ML datasets
with no exposed public interface . Instead of exploiting a
trained model from the outside, our work proposes a novel
insider attack which can be conducted by DNN engineers
who have access to the target private dataset(s) and is respon-
sible for using an irrelevant dataset to train an independent
scheduled-to-publish model (i.e., the carrier model). In pop-
ular data security models adopted by many AI corporations,
potential attackers with such a role is common (¬ßIII-B). In
other words, our work for the Ô¨Årst time reveals a new attack
surface where an attacker can steal the functionality of private
ML data with no contact to any models trained on the victim
dataset(s).
As Fig.1 shows, to break the privacy of ML data with no ex-
posed interface, our attack employs an irrelevant scheduled-to-
publish DNN model as a carrier model for covert transmission
of multiple task-oriented models trained on different private
datasets (i.e., the victims) for functionality stealing. Once an
outside colluder decodes these secret models from the carrier
model, the obtained task-oriented models would immediately
share the similar functionality as if trained on the privatearXiv:2206.14371v1  [stat.ML]  29 Jun 2022

--- PAGE 2 ---
data. We name this new attack class as Matryoshka (nesting
dolls in Russian1), which literally reÔ¨Çects our novel attack
methodology of hiding models in model , ormodel hiding .
The key challenge of model hiding is where to Ô¨Ånd the
sufÔ¨Åcient encoding capacity for the millions or even billions
of Ô¨Çoating-point parameters in a secret DNN. Simply treating
the parameters of the carrier model as bit strings, existing
steganography techniques [16]‚Äì[20] are strictly upper bound
in their encoding capacity, which is far smaller than the size of
the carrier model. This bottleneck substantially weakens their
Ô¨Çexibility in model hiding. For example, they are unable to
encode a secret model in conventional carrier medium (e.g.,
texts or images), to encode a larger secret model in a small
carrier model, or to encode multiple secret models in one
carrier model simultaneously for different attack purposes.
Moreover, previous studies usually attempt to encode highly
heterogeneous data into the parameters of the carrier model.
Without special designs, the performance of the carrier model
may decrease to an unacceptable level to raise doubts. Finally,
the robustness of the existing data hiding techniques may also
inÔ¨Çuence the secret models when the potential obfuscation is
conducted on the carrier model [21].
At the core of Matryoshka, we devise a new parameter
sharing mechanism which instead exploits the enormous learn-
ing capacity of DNN to implement a much larger encoding
capacity for model hiding. Intuitively, our parameter sharing
mechanism is based on a data structure called ParamPool ,
which stores an array of learnable parameters that are reusable ,
i.e., they are designed to Ô¨Åll into different layers of a given
DNN for multiple times, and allow cyclic access. In other
words, multiple DNNs, including either the secret models
and the carrier model, can be generated from the same
ParamPool by recycling the parameters inside (¬ßV-B). To
encode the secret models in one carrier model, we jointly
train the carrier model along with the secret models with
the parameters generated from the ParamPool. During the
learning process, the error propagates and accumulates to
the corresponding parameter in the ParamPool, where the
update Ô¨Ånally happens. Our insight behind using ParamPool
for parameter sharing comes from a pilot study which observes
quite similar parameter distribution of a wide range of well-
trained DNNs on various application domains (¬ßV-C). After
the carrier model is published online, the outside colluder
can decode the ParamPool from the carrier model with a
small number of secret integer keys, and assemble the secret
model(s) for privacy breach (¬ßV-D).
Our Contributions. As a newly discovered threat to private
ML data, our proposed Matryoshka attack implements the
following key properties for effective and covert transmission
of secret models which steal the data functionality.
High Capacity : With almost no utility loss ( <1%),
Matryoshka can hide a 26larger secret model, or 8secret
models of different architectures spanning image, speech and
1By convention, Matryoshka dolls inside (i.e., the secret models) is more
precious than the outer doll which holds them (i.e. the carrier model).text applications at one time in the carrier model, neither of
which is feasible with existing steganography techniques.
Decoding EfÔ¨Åciency : The secret models can be efÔ¨Åciently
decoded and assembled from the carrier model with only
several integer secrets exclusively known to the colluder,
provided that the secret models have standard architectures.
Effectiveness : Almost all the recovered secret models either
have similar performance as when they are independently
trained on the private dataset.
Robustness : Matryoshka naturally implements information
redundancy [22] via the usage of a smaller ParamPool, which
helps in achieving resilience against possible post-processing
techniques on the carrier model before its publishing.
Covertness : Besides, we also provide a preliminary discus-
sion on the covertness of Matryoshka when a detector with
different levels of prior knowledge on the attack attempts to
differentiate a carrier model from a normal one (¬ßVI-F), which
we hope may foster future defense studies.
II. R ELATED WORK
Privacy Attacks on Training Data. Recently, trained ML
models are proved to be highly exploitable for attackers to
steal private information about the training data. One branch
of attacks focuses more on the privacy of the training data as
a whole. Inferring global sensitive information of the training
data, model inversion attack [10], [13] and property inference
attack [11], [23] respectively target at revealing the class
representatives of a training data (e.g., the average face of an
identity when attacking a face recognition model) and whether
a Ô¨Årst-order predicate holds on the training data (e.g., whether
a face dataset contains no whites). Stealing the functionality of
a private training dataset, model extraction attack constructs a
surrogate model by distilling the black-box prediction API [5]‚Äì
[7], [24] or directly reverse-engineers the model parameters by
exploiting the property of the rectiÔ¨Åed linear units [8], [25],
[26]. With Ô¨Åner attack granularity, another branch of attacks
aims at breaking the privacy of single training samples. For
instance, membership inference attack instead turns to infer
whether a query sample belongs to the training set via white-
box access [14], by knowing the predicted conÔ¨Ådence values
[15], or by label-only exposures [27], [28] from a trained
model. Differently, recent years further witness works that
infer sample-level sensitive information or even reconstruct
raw training samples from intermediate computation results
(e.g., features [29] and gradients [12], [30], [31]) and model
outputs [32]‚Äì[35]. In contrast, instead of exploiting a well-
trained DNN for probing its training data, we for the Ô¨Årst time
explore and reveal the risk of a private training dataset with
even no exposed interface, which we hope can substantially
advance more comprehensive access control mechanisms.
Data Hiding in Neural Networks. Data hiding, or steganog-
raphy, is a long-standing research area in our community
[16], [21]. With the recent development of open-source model
supply chains, several research works also exploit DNN as a
new medium for hiding binary information. At the early stage,
Uchida et al. [20] and Song et al. [19] concurrently explore
2

--- PAGE 3 ---
the idea of hiding data in DNN yet with different research
focuses. To protect the intellectual property of DNN, Uchida et
al. propose to embed a secret random binary message into the
model parameters via conventional steganography techniques
(e.g., least-signiÔ¨Åcant bits or sign encoding). By verifying
whether a model contains the binary message, the ownership is
established. Later, this seminal work catalyzes the orthogonal
study of DNN watermarking [36]‚Äì[38]. Instead of model
protection, Song et al. aim at hiding sensitive information
about the private data into the model parameters during its
training. SpeciÔ¨Åcally, they directly convert a subset of sensitive
data inputs into a binary form and encode them into the model
parameters again with almost the same set of conventional
steganography techniques in [20].
III. P RELIMINARY
A. Background on Machine Learning (ML)
Notions in ML. In our work, we call a private database is a ML
dataset if it is prepared for training ML models. In this paper,
we mainly focus on supervised learning task, which is deÔ¨Åned
in a spaceXY . Here,Xis the input space, composed of raw
data including but not limited to images, texts, or audios, and
Yis the label space. A learning model f(;)(:=f) :X!Y
aims to build the relation from an element in Xto the label in
Y, which consists of all the possible values in the annotation.
For example, in a K-class image classiÔ¨Åcation task, Xconsists
of a set of images to be classiÔ¨Åed, while Y=f1;:::;Kg, i.e.,
the possible classes. Finally, the training process of a learning
modelfinvolves the optimization of a loss function , denoted
as`(;), with respect to the parameters of the learning model.
Basics of Deep Learning. The past decade has witnessed the
evolution of deep learning (DL) techniques in most important
ML tasks [39], which show substantial improvements over
the classical ML techniques, and have already reach deep
into a wide range of mission-critical applications in real
world [40]‚Äì[42]. The core of DL is deep neural networks
(DNNs), a family of learning models which are composed of
a number of processing layers (e.g., linear/activation layers,
convolutional/pooling layers, etc.) that transform the data rep-
resentation through multiple abstraction levels [43]. Usually, a
layer in DNN is composed of a set of learnable parameters ,
which are iteratively updated to optimize the loss function via
off-the-shelf gradient-based optimizers (e.g., Adam [44]).
As a Ô¨Ånal remark, we highlight proper parameter initial-
ization is indispensable to attain a near-optimal DNN model
throughout the training. Learnable parameters of different
roles (i.e., weight, bias and scale) are usually initialized with
distinct schemes and may also differ in the constraints on
their values. For example, a weight scalar is usually initialized
from Guassian, a bias scalar is initialized as 0, while a scale
scalar is 1with a nonnegative constraint. This technical detail
inÔ¨Çuences our subsequent attack designs.
B. Data Security Models in AI Corporations
As essential backgrounds for discussing insider attacks,
we provide below a Ô¨Åeld study on typical data securitymechanisms in AI corporations.
Common Mechanisms . Data Leakage Prevention Systems
(DLPS) are one of the most mainstream security solutions in
industry. DLPS can identify, monitor and protect conÔ¨Ådential
data and detect illegal data transmission based on predeÔ¨Åned
rules [45]. According to Securosis, DLPS which are practically
deployed inside corporations usually have rigorous restriction
on data transfer to unauthorized endpoint devices (e.g., Wi-Fi,
Bluetooth, USB) or to the outside network without permission
[46], i.e., it is impossible for a naive insider attacker to directly
transmit private datasets from the local network to the outside
without being detected. With DLPS blocking potential data
leakage to the outside, various access control policies are
implemented by AI corporations to manage their owned pri-
vate datasets. In the following, we introdce two typical modes
named as the Willing-to-Share mode and the Application-then-
Authorization mode, according to our industry partners.
‚Ä¢ The Willing-to-Share Mode. For start-up corporations which
focus on one killer application (e.g., object detection [47]),
they prefer to organize the datasets in a more shareable mode.
According to our Ô¨Åeld study, a majority of the groups organize
all the datasets (either from public or private sources) in their
local distributed Ô¨Åle system, which enables fast access to
speciÔ¨Åc datasets according to one‚Äôs requirement and facilitates
swift development of novel DL techniques.
‚Ä¢ The Application-then-Authorization Mode. More estab-
lished AI corporations prefer to implement a more conser-
vative way of managing ML datasets. According to Google‚Äôs
common security whitepaper [4], each employer should Ô¨Årst
apply for the authorization before accessing certain data re-
sources, including the access to private datasets. However,
considering the ever-evolving paradigms in deep learning,
employees with ulterior motives may fabricate reasons such
as the requirements of data augmentation [48] or the purpose
of multimodal learning [49] to apply for relevant and irrelevant
private datasets, which is common in social engineering [50].
Under the umberalla of DLPS, corporations may be less
precautious about the unnecessary access to certain private
datasets, as DLPS ought to forbid any attempts of transferring
the private datasets away from the local network.
Summary : An insider has multiple ways to access
private datasets, which are strictly forbidden from
being transferred out of the local network with no
authorization.
IV. S ECURITY SETTINGS
Attack DeÔ¨Ånition. Following existing works on breaking
training data privacy via well-trained DNN models (¬ßII),
we deÔ¨Åne a target private dataset is stolen in the sense of
functionality leakage , i.e., the attacker attains a trained model
which has nearly the same functionality as if the model were
trained on the target private dataset.
Threat Model. Our Matryoshka attack involves an inside
attacker and an outside colluder. The colluder can also be
3

--- PAGE 4 ---
played as the attacker him/herself. Formally, we have the
following assumptions on the ability of the insider and the
outsider: (i) Accessible Targets : The insider has access to the
target private dataset(s); (ii) Existent Carrier : The insider is
assigned with the task of training a DNN, which is scheduled
to undergo an open sourcing process, on a non-target dataset;
(iii)Receivable Carrier : The outsider knows which model to
download after the publishing; (iv) Secure Collusion : The in-
sider and the outsider can collude on several integer values and
model architectures via a secure channel (e.g., a rendezvous).
Below, we brieÔ¨Çy discuss the rationale behind our threat
model. First, the accessibility of the target datasets is sup-
ported by our Ô¨Åeld study in ¬ßIII-B, where an insider can
either freely access the target private dataset in the Willing-to-
Share mode, or pretend to have a need on the target datasets
to accomplish the assigned task at hand in the Application-
then-Authorization mode. Second, the existence of a carrier
is realistic in the AI industry nowadays, where the trend of
open sourcing pretrained DNNs becomes increasingly com-
mon for corporations with either educational or commercial
purposes [51], [52]. Working as a DL engineer, the insider
has a chance to be assigned with such a task. Besides, the
third and the fourth assumptions can be easily fulÔ¨Ålled by
off-line communication between the inside attacker and the
outside colluder during non-working time. As we later show,
in Matryoshka the secret keys which are required to decode
the hidden information from the carrier model only consist of:
a website link to the ofÔ¨Åcial downloading site, several integer
values (i.e., random seeds) and the architectures of the secret
models which can be chosen as well-known architectures that
already standardized in popular DL frameworks to ease the
collusion. This allows the collusion to be conducted via a
rendezvous, which establishes a secure channel.
Attack Taxonomy and Goals. To realize the covert transmis-
sion of secret models via a scheduled-to-publish DNN, our
proposed Matryoshka attack is expected to meet the following
attack goals: (i) Capacity: As a prerequisite of invoking covert
transmission, we require the secret models can be encoded
into the carrier model without causing obvious degradation
of its normal utility. Intuitively, a hiding scheme has higher
capacity if it encodes more secret information with the same
level of performance degradation. (ii) Decoding EfÔ¨Åciency:
This requirement measures how much additional knowledge
is required to successfully decode the secret models from the
carrier model. If it surpasses the memorization ability of the
insider, the additional knowledge should be communicated via
an additional round of covert transmission, increasing the risk
of being detected. (iii) Effectiveness: After the secret infor-
mation is decoded from our carrier model, the information is
expected to have almost no distortion (i.e., effective transmis-
sion). SpeciÔ¨Åcally, in our attack, we require the decoded secret
models are effective in stealing the target private dataset(s) in
terms of leaking functionality or leaking sensitive data. (iv)
Robustness: Using the language of information theory, this
requirement expects the covert transmission remains effective
even if the transmission channel has noises. In our context,we require the colluder can still decode the secret models
and thus the privacy of the target datasets when the carrier
model undergoes common post-processing techniques (e.g.,
pruning and Ô¨Ånetuning). (v) Covertness: Finally, to reify the
requirement of covertness, a third party should not be able to
detect whether a published model contains secret models or
not. Otherwise, the transmission would be canceled and the
inside attacker will be detected according to the logging of
dataset access.
Existing Data Hiding Techniques for Model Hiding. Data
hiding is a long-standing and fruitful research area in security
[21]. Many well-known algorithms and tools are devised to
encode binary messages in multimedia contents such as image
[16], text [17], and audio [18]. Recently, there are also several
research works which explore applying well-established data
hiding techniques, including least-signiÔ¨Åcant-bit (LSB [53]),
correlated value and sign encoding, to embed secret binary
watermark [20], [38], a subset of training data in binary form
into a trained DNN [19] or a virus [54].
However, existing data hiding techniques mostly view the
carrier medium, whether common multimedia contents or
parameters in DNN, as bit strings for encoding the secrets.
Hence, the size of the carrier medium poses a strict capacity
limit on the information that can be hidden. On the one
hand, common multimedia contents can hardly afford to hide
a DNN which is usually composed of millions of Ô¨Çoating-
point parameters. For example, the storage of a ResNet-18
for ImageNet is over 45MB, while, for image, the maximally
tolerable embedding rate is only 0:4bits per pixel [55]. On the
other hand, applying existing techniques to hide information
in a carrier DNN cannot fully exploit its potential capacity.
Compared with conventional multimedia, a carrier DNN is
much larger than traditional multimedia and has stronger
resilience against the modiÔ¨Åcation of multiple LSB positions
[56], [57]. Despite this, previous encoding schemes are only
able to use about 20%50% size of a DNN for information
hiding. For example, Song et al. Ô¨Ånd most existing data hiding
techniques can hardly hide over 500 raw gray-scale images
of3232resolution ( 5:85MB in total) if the performance
degradation of a carrier ResNet-18 as is required to be bound
by3%[19]. In our viewpoint, to directly encode raw data
inputs into the carrier model confront the challenges from
the distribution misalignment between the two heterogeneous
data types (¬ßV-C). If existing techniques are applied for covert
transmission of secret models, the Ô¨Çexibility and efÔ¨Åciency of
our Matryoshka attack would be severely hurt: Either a larger
secret model or multiple secret models for different attack
purposes are unable to be hidden in the carrier model.
Summary : (i) Multimedia carrier usually have insuf-
Ô¨Åcient capacity for model hiding. (ii) Existing tech-
niques have not fully exploited the encoding capacity
of a carrier DNN.
4

--- PAGE 5 ---
Fig. 2. Overview of the methodology of Matryoshka.
V. T HEMATRYOSHKA ATTACK
A. Attack Overview
Below, we Ô¨Årst present our design and insights of a Param-
Pool, which is at the core of the encoding (decoding) of the
secret models into (from) the carrier model.
Design of ParamPool. In general, a ParamPool Pmaintains
an array ofjPjlearnable scalar parameters. Corresponding
to the different types (e.g., weight, bias, scale) of learnable
parameters in DNN, the parameters in a weight pool are also
grouped in disjoint groups, i.e., P=Pw[Pb[Ps. Each group
implements different random schemes when initialized. Given
an integer secret v, a ParamPool Pimplements the following
primitives to interact with a DNN f(;):
(i)Fill(P,f,v)!P (;v): This primitive replaces each
original parameter in fwith a parameter of the same type
which is selected from the ParamPool Pwith replacement
according to the secret key v. We useP(;v)to denote the
parameters of a DNN fÔ¨Ålled byPunder the secret integer
v. In this sense,P(;v)is viewed as a hash map from the
original parameters to the ParamPool parameters.
(ii)Propagate (P,f(;),v)!; : After an optimization
step onf, this primitive collects the weight update on
each parameter in a DNN and propagates them to the
corresponding positions in the ParamPool with P(;v).
(iii)Update (P)!;: After the weight update is accumulated
in the update buffer of the corresponding parameter in P,
this primitive updates each parameter in Pby aggregating
its update buffer and then reset it.
(iv) Decode (f,v)!P: This primitive decodes the the
ParamPool from a DNN faccording to the secret key v.
Intuitively, with the above primitives, a ParamPool can be used
as a proxy to the full life cycle of a DNN.
Attack Pipeline. Fig.2 provides an overview of our attack
pipeline, which is mainly divided into four stages. In the
following, we denote the carrier DNN as C.
Stage 1: Initialization of ParamPool. First, a ParamPool P
is initialized from the carrier DNN by colllecting and grouping
all the learnable parameters (Option I) or from scratch withan attacker-speciÔ¨Åed size for each group (Option II) . Later
we show Option I is more advantageous in evading detection
(¬ßVI-F) while Option II helps the carrier model to be more
robust even transmitted in a noisy channel, i.e., model post-
processing (¬ßVI-E).
Stage 2: Construction of the Secret Tasks (¬ßV-B). In
the next stage, we specify the secret learning tasks according
to the attack purposes and the nature of the target datasets
D1;:::;DNto steal (N1). For stealing the functionality
of a supervised learning dataset, we choose an off-the-shelf
DNN architecture to Ô¨Åt the private dataset with a proper
loss`k. Differently, for stealing a sensitive subset of private
data inputs, we choose a DNN which is trained to map a
sequence of noise vectors to the attacker-interested data inputs
in a one-to-one way. The noise vectors are randomly sampled
from a secret distribution with a Ô¨Åxed integer seed, which
is exclusively known to the insider and the colluder. Finally,
the attacker chooses an integer secret vkand invokes the Fill
primitive to replace the parameters in each secret model with
the ones inPby invoking Fill(P,fk,vk).
Stage 3: Joint Training for Model Hiding (¬ßV-C) Combin-
ing with the learning task (DC;`C)of the carrier model C, the
attacker jointly trains the carrier model and the secret models
by optimizing the parameters in the ParamPool P. Concisely,
in each iteration, we synchronously calculate the parameter
updates in each model and then invoke Propagate (P,fk) to
accumulate the updates in each model to the corresponding
update buffers. Subsequently, we invoke the Update primitive
and resume the joint training to the next iteration. When the
training Ô¨Ånishes, the attacker removes all the traces in the code
base and replaces the corresponding parameters in the carrier
model with the values from the Ô¨Ånal ParamPool. We denote
the Ô¨Ånal carrier model as C.
Stage 4: Decoding Secret Models from the Carrier
Model (¬ßV-D). After the carrier model Cis published, the
outsider colluder downloads C. With the knowledge of the
architectures of the secret models and the secret key vk
communicated via a secure channel, the outsider Ô¨Årst invokes
Decode (C,vk) to decode the ParamPool Pfrom the carrier
5

--- PAGE 6 ---
model. Then, the colluder assembles the secret models from
the decoded ParamPool. In the following, we present the
detailed technical designs for each stage above.
B. Construction of the Secret Tasks
Functionality-Oriented Learning Tasks. For stealing the
functionality of a private dataset, the secret learning task
is constructed as if the attacker is training a usable model
fk(;k) :Xk! Ykon the private dataset Dk:=
f(xi;yi)gMk
i=1belonging toXkYk. To minimize the re-
quired information for collusion, we propose to use stan-
dardized neural architectures in deep learning frameworks.
With the loss function `k, we formally construct the secret
learning task for functionality stealing as minkLk(k) :=
1=jDkjP
(xi;yi)2Dk`k(fk(xi;k);yi).
Fill Secret/Carrier Models with ParamPool. With the carrier
modelCand an initialized ParamPool P, we Ô¨Årst specify
a random integer vkfor each secret model fkfrom all the
possible indices of the ParamPool P, i.e.,f1;2;:::;jPjg. This
prevents the secret models from being decoded by any party
except for the colluder. Then we invoke the primitive Fill(P,
fk,vk) to replace the original parameters in fkby parameters
inP. Intuitively, the Fill primitive loops over all the scalar
parameters in the target model fk(;)and assign it with
the value of a parameter selected from the ParamPool. As
Fig.2 shows, the parameter selection cursor on each parameter
group (e.g., the weight group Pw) cycles from a starting
index derived from the integer secret (e.g., vkmodjPwj).
For the carrier model, we choose vC= 0 if the ParamPool
is initialized directly from the carrier model (i.e., Option II in
¬ßV-A), or otherwise, we sample an integer secret vCas well
for the carrier model. The obtained model is denoted as ~fk
with the substituted parameters as P(k;vk).
C. Joint Training for Model Hiding
After the secret and the carrier models are Ô¨Ålled, the
ParamPool Pis ready to become a proxy to the training
process of each secret/open learning task. Without loss of
generality, the carrier model Cis supposed to be trained on
a supervised learning task DC:=f(xi;yi)gMC
i=1with a loss
function`C. Formally, the model hiding process aims to solve
the following joint learning objective:
min
P1
jDCjX
(xi;yi)2DC`k(C(xi;P(C;vC));yi)
+1
NNX
k=1Lk(P(k;vk)): (1)
Intuitively, the above objective requires Pto reach a good
consensus on Nsecret tasks and the open task. For example,
whenN= 1, it means that the sets of local optimum for fC
andf1should intersect with one another to guarantee a near-
optimal ParamPool is attainable. For the Ô¨Årst time, we empiri-
cally propose a joint training algorithm in Algorithm 1 which
solves the above learning objective to construct a near-optimal
ParamPool. Each secret model assembled from the optimizedParamPool exhibits similar utility compared with an identical
model which is independently trained (¬ßVI-C). However, to
the best of our knowledge, existing deep learning theories
can hardly help with the explanation of this phenomenon. To
provide a tentative explanation, we provide the following pilot
study on the optimal parameter distributions across models.
A Pilot Study on Parameter Distribution. SpeciÔ¨Åcally,
we download 8pretrained models (i.e., ffk(;kg8
k=1) from
Pytorch Hub [58], with their names listed in the legend of
Fig.3. The selected models cover typical applications (e.g.,
classiÔ¨Åcation, generation, object detection) and span image,
audio and text data. In the following, we focus on the param-
eters of the weight type. Without loss of generality, similar
arguments are applicable to the bias and the scale. First, we
calculate the normalized histogram of the parameters of the
weight type in each model on a range of [ 1;1]withn= 100
bins. In our experiments, the range is validated to cover all
the weight parameters. We plot the weight histograms in
Fig.3(a) and use Hw(k) :=f(w1;p1(k));:::; (wn;pn(k))g
to denote the histogram of the weights in k. To analyze the
feasibility of our ParamPool-based weight sharing scheme, we
measure the following optimal transportation distance (OTD)
between the weight histograms Hw(j)andHw(k)[59]:
OTD(Hw(j);Hw(k)) = minnX
l=1nX
m=1dlmplm (2)
s.t.,nX
l=1plmpm(k);nX
m=1plmpl(j);nX
m=1nX
l=1plm= 1;
(3)
wheredij=jwi wjj, i.e., the distance between the i-
th and thej-th bin centers. We call the fp
jkgto solve the
linear programming above as the optimal transportation (OT)
scheme . In general, the OTD between two distributions mea-
sures the minimal distance when the density in one distribution
is transported to another [60]. In our context, viewing the
ParamPool as the intermediate of mapping a model fjto a
different model fk, we present the following analytical result.
Theorem 1 (Existence of Near-Optimal ParamPool) .For a
pair of learning tasks (Dj;fj;`j)and(Dj;fj;`j)such that

jandkare the corresponding minimizers to the training
loss, if OTD (H(i);H(j))<(whereiterates infw;b;sg,
i.e., weight, bias, scale), then there always exist a ParamPool
Pof sizejPjand two integer secrets vj;vksuch that
jP(
j;vj)[i] 
j[i]j<=2 +O(jPj 1)
jP(
j;vk)[i] 
k[i]j<=2 +O(jPj 1) (4)
where
j[i]denotes the i-th scalar parameter in 
j.
In plain words, the above theorem argues that the exis-
tence of a near-optimal ParamPool which minimizes the loss
functions on both individual task is strongly related with the
OTD between the parameter distributions. Intuitively, when
an OTD (H(j);H(k))is smaller than , it means that a
scalar weight parameter winfjcan always Ô¨Ånd a substitute
6

--- PAGE 7 ---
(a)(b)Fig. 3. (a) The weight histogram of 8pretrained models of high diversity.
(b) The empirical values for the pairwise OTD among the 8models.
parameterw0such thatjw w0j<  due to the existence of
the OT scheme. Therefore, with the parameters of fj;fk, one
can always construct a ParamPool and the mapping relations
from the models to the ParamPool such that the shared weight
differs from the optimized weight by . The following provides
the proof for this theorem.
Proof for Theorem 1. First, we provide a constructive proof
to the case when the ParamPool has an unlimited size, which
is formally stated as the following lemma.
Lemma 1. If OTD (H(i);H(j))<  (whereiterates in
fw;b;sg), then there always exist a ParamPool P0and two
integer secrets vjandvksuch that, for each i,jP0(
j;vj)[i] 

j[i]j<=2;jP0(
k;vk)[i] 
k[i]j<=2:
Proof. By the deÔ¨Ånition of OTD (H(i);H(j))< , there
exists an optimal transportation scheme p
jksuch that for each

j[i], the corresponding parameter 
k[p
jk(i)]lies in the-ball
of
j[i]. Therefore, for the index iof the parameter 
jand the
indexp
jk(i)of the parameter 
k, thel-th ParamPool parameter
P0[l]can always be constructed to satisfy j
j[i] P0[l]j<
=2andj
k[p
jk(i)] P0[l]j<=2(e.g., by choosing P0[l] =
(
j[i] +
k[p
jk(i)])=2). Therefore, a mapping rule from 
ito
P0isi!l, while, for 
j, it isp
jk(i)!l. Without loss of
generality, we assume 
iis no smaller than 
j. By iterating
overias above, we construct a ParamPool P0of the same
size as
i, and the mapping relations P0(
i;vi) =fi!lg,
P0(
j;vi) =fp
jk(i)!lg. One can easily show there exist
integersvi;vjconforming to the above mapping relation.
If the size of 
jis no smaller than jPj, our theorem is
already valid according to the lemma above. Otherwise, we
can extend the above lemma to the case by clustering the
parameters in the unlimited ParamPool we construct above
to satisfy the size constraint. Technically, considering the
ParamPoolP0in Lemma 1, we next prove the existence of
a smaller ParamPool of size jPjwhich well approximates P0
in the following sense.
Lemma 2. There exists a ParamPool Pand a mapping
relationfrom the index of P0to the index of Psuch that
jP0[i] P[(j)]j<O(jPj 1).Proof. We prove the lemma again via construction. By di-
viding the parameter range (w.l.o.g., we denote the range as
[ M=2;M=2]) injPjbins of equal width (i.e., M=jPj), we
put the parameters in P0into each bin. Then we construct the
smaller ParamPool by collecting all the jPjbin centers, and
the mapping relation as the belonging relation to the bins. The
the lemma is proved.
Finally, we leverage the triangle inequality to such that
j
j[i] P[l]j<j
j[i] P0[l]j+jP0[l] P0[(l)]j< = 2 +
O(jPj 1). Finally, by contracting on the original mapping
relations inP0(
j;vi)with the clustering relation , we
construct the mapping relation from 
jto the ParamPool P.
Similar arguments hold for 
k.
Finally, we empirically calculate the pairwise OTD among
the8models based on the histogram, and report the results
in the heatmap form in Fig.3(b). The results show, the upper
boundis0:038on average with a standard deviation of 0:008
(i.e. theis smaller than 2%of the weight range of length
2), which would hardly affect the model performance [61].
To summarize, our pilot study relates the existence of a near-
optimal ParamPool to the scale of the OTD between the dis-
tribution of the optimized parameters. For future exploration,
we leave open a more comprehensive theoretical treatment on
how such a near-optimal ParamPool is possible to be attained
with our proposed algorithm.
Searching for the Optimized ParamPool. To solve the
joint learning objective in Section V-C, our proposed attack
executes the following training iteration recurrently. Denote
the ParamPool at the t-th iteration as Pt. Concisely, at the
t-th iteration, we iterate over all the Nsecret tasks and the
normal task to conduct the following key procedures:
(i)For thek-th secret tasks, we Ô¨Årst invoke Fill(Pt,fk,vk)
to instantiate fkwith the current values of the ParamPool.
(ii)Then, we forward a training batch via the model
fk(;Pt(k;vk)), back-propagate the loss Lkapproximated
on the training batch, and conduct one optimization step on
the parameters of fkwith an optimizer (e.g., Adam [44]).
(iii)Finally, we collect the weight update on each parameter
and follow the mapping relation in P(;vk)to propagate
the update to the corresponding ParamPool parameter. The
above procedures are also conducted on the carrier model.
The above procedures describe the Propagate primitive on
each secret/open task (L4-15 in Algorithm 1).
The Ô¨Ånal step in one training iteration is to invoke the
Update primitive on the ParamPool (L16-18 in Algorithm
1). Technically, for each parameter in Pt, we maintain a
buffer to store the weight updates from each task. The update
buffer is aggregated to obtain the global update value on the
corresponding scalar parameter in Pt. In our experiments, we
Ô¨Ånd aggregation by average is already sufÔ¨Åcient to achieve
effective attacks. After the parameter is updated, we clear the
update buffers and resume the next training iteration.
As a Ô¨Ånal remark, the training process on the ParamPool
terminates when (i) the validation performance of the carrier
7

--- PAGE 8 ---
Algorithm 1 Thet-th iteration during the joint training process
on the ParamPool
Input:Pt(the current ParamPool), f(fk;~Dk;`k;Optk)gN
k=0
(the secret and open tasks). BFor simplicity, the index 0
denotes the open task on the carrier model.
Output:Pt+1(the updated ParamPool)
1:foreach parameter wPinPtdo
2:wP.buffer fg
3:end for
4:parallel for k= 0;1;:::;N do
5:P(;vk) Fill(Pt,fk,vk)
6: Sample a training batch Bfrom ~Dk.
7: ~Lk 1
jBjP
(x;y)2B`k(fk(x;P(k;vk));y).BApprox-
imate the loss Lkon a randomly sampled mini-batch.
8: ~Lk.Backward()
9: k Optk.Step()
10: foreach scalar parameter winkdo
11: ifwbelongs to afweight, scale, bias gparameter
then
12:P(w;vk).buffer.Append( w)BPropagate the up-
date to the corresponding ParamPool parameter.
13: end if
14: end for
15:end parallel
16:foreach parameter wpinPtdo
17:wp wp+Average(wp.buffer)
18:end for
19:returnPt+1.
model meets the task requirement, and (ii) at least one of
the secret tasks converge. This condition ensures the carrier
model to smoothly transition to the next stage of a publishing
process with certain secret models encoded. We denote the
Ô¨Ånal ParamPool as P. To wind up the model hiding phase,
the attacker clears up any traces of malicious training code
and irrelevant intermediate outcomes, memorizes the secret
keys (i.e., the random seeds fskgfor generating noises, the
starting indicesfvkgand the architecture name for each task,
the size of each group in the ParamPool) via a secure medium,
and instantiates the carrier model by Fill(P,fC,vC).
D. Decoding Secret Models from the Carrier Model
Recovering the ParamPool. After the carrier model is pub-
lished online with open access, the attacker immediately
notiÔ¨Åes the outside colluder to download the carrier model and
decode the secret models from the carrier model. SpeciÔ¨Åcally,
after colluding on the secret keys with the attacker via a
secure channel (e.g., an in-person rendezvous), the outsider
Ô¨Årst decodes the ParamPool based on the colluded knowledge
of the ParamPool sizes and the starting index vC, i.e., by the
primitive Decode (C;vC). SpeciÔ¨Åcally, the decoding procedure
differs according to the ParamPool initialization:
(i)If initialized from the carrier model directly, the Param-
Pool can be easily recovered by the colluder via collecting
each parameter in the carrier model into different groups.(ii)Otherwise, if the ParamPool is created from scratch,
the colluder Ô¨Årst collects the parameters of different groups
from the carrier model. Then, the attacker slices, e.g., the
weight parameters into segments of length jPwj. The last
segment may need additional zero padding to hold the same
length. Finally, the attacker conducts a fusion operation on
theNdecoded segments, right-shift the fusion result by vk
modjPwj, and permute it with a permutation inverse to the
one in Fillto recover the Ô¨Ånal Pwin the ParamPool. Similar
operations are conducted on the bias and the scale groups.
We introduce the fusion mechanism on the Ndecoded
segments to implement resilience against post-processing on
the carrier model (¬ßVI-E). For example, when the colluder
Ô¨Ånds the carrier model is pruned, the fusion mechanism
selects the non-zero value from each ParamPool copy to
restore the pruned values.
More details on the decoding procedure are provided in
Algorithm 2. To determine which decoding algorithm to use,
the outsider simply compares the known jPjwith the number
of learnable parameters in the carrier model.
Algorithm 2 The Decode (C,vc) Primitive
Input:C(the carrier model), vc(the integer secret of the
carrier model), Nw;Nb;Ns(the length of Pw;Pb;Ps).
Output:P=Pw[Pb[Ps(the decoded ParamPool).
1:Pw list();Pb list();Ps list()
2:foreach scalar parameter winCdo
3: ifwbelongs to afweight, scale, biasgparameter then
4: switch (type(w))do
5: case weight :
6:Pw.Append(w.data)
7: case bias:
8:Pb.Append(w.data)
9: case scale :
10:Ps.Append(w.data)
11: end switch
12: else
13: CONTINUE
14: end if
15:end for
16:vw;vb;vs vcmodNw;vcmodNb;vcmodNs
17:Generate a random permutation w(b;s) of integers
from 0 toNw(Nb;Ns) with seeds vw(vb;vs).
18:Pw;Pb;Ps Fusion (Pw;Pb;Ps) BRecover the
parameter by selecting the non-zero value from each ParamPool
copy, if needed.
19:Right-shift each parameter group by vw;vb;vs.
20:Pw;Pb;Ps  1
wPw; 1
bPb; 1
sPs
21:returnP=Pw[Pb[Ps
Assembling the Secret Models. Finally, the colluder recovers
the secret models f1;f2;:::;fNby invoking the Fillprimitive
with the decoded ParamPool in the previous part. In this way,
the attack objective is attained.
VI. E VALUATION RESULTS
A. Overview of Evaluation
Datasets & Architectures. We evaluate the performance of
Matroyoshka on a diversity of datasets and tasks from multiple
application domains including image, audio, text, time series
8

--- PAGE 9 ---
TABLE I
EFFECTIVENESS OF OUR MATRYOSHKA ATTACK IN TRANSMITTING SINGLE SECRET MODELS VIA A RESNET-18 ONCIFAR-10.
Domain TaskSecret Tasks Secret Model Carrier Model
Dataset/Model/Metric # Params Perf-IPerf-II Normal Perf-IPerf-II Normal
ImageClassiÔ¨Åcation GTSRB [62]/VGG-16 [63]/ACC 14.74M -0.51% 0.25% 98.45% 0.03% -0.61%
95.50%Object Detection VOC‚Äô07 [64]/MobileNetV1-SSD [65]/MAP 9.47M -1.42% -16.63% 51.57% 0.12% -0.61%
Text ClassiÔ¨Åcation IMDB [66]/TextCNN [67]/ACC 6.69M -5.66% -9.08% 90.85% -0.16% -0.41%
Audio ClassiÔ¨Åcation SpeechCommand [68]/M5 [69]/ACC 26.92K -1.39% -1.11% 96.86% -0.32% -1.14%
Time Series ClassiÔ¨Åcation UWave [70]/RNN/MSE 1.67K -1.14% -4.54% 86.36% -0.67% -0.40%
HealthcareClassiÔ¨Åcation DermaMNIST [71]/LeNet-5 [72]/ACC 61.98K -4.78% -5.74% 74.21% -0.34% -0.47%
Regression Warfarin [73]/MLP/MSE 0.5K -0.51 -0.86 8.23 0.00% -0.35%
and healthcare. The general information is presented in Secret
Tasks columns of Table II. Due to ethical concerns, we
conduct all our experiments on public datasets. To faciliate
future research, we open source our experimental code in
https://anonymous.4open.science/r/hiding models-EAF8.
Evaluation Metrics. We measure the effectiveness of
functionality-oriented stealing with Performance Difference
(Perf). For the carrier model, Perf measures the difference
between the carrier model encoded with secret models and an
independently trained carrier model. For a secret model, Perf
measures the performance difference between the decoded
secret model and a model of the same architecture while
independently trained on the private dataset. A lower Perf
means model hiding causes more performance overhead to the
carrier model or the secret model, which implies the carrier
model has lower capacity. Moreover, a lower carrier model
Perf means model hiding is less covert, while a lower secret
model Perf indicates the attack is less effective. SpeciÔ¨Åcally,
the7secret tasks involve the following performance metrics:
ACC : Accuracy (ACC) is the standard evaluation metric
for classiÔ¨Åcation tasks. ACC measures the proportion of
correctly predicted samples. It formally writes ACC =PN
i=11ff(xi)=yig
N, whereNis the total number of the test
samples.
MAP : Mean average precision (MAP) is the common
evaluation metric for the object detection task, which is the
average of AP (average precision) over all object classes.
MSE : Mean Square Error (MSE) measures the L2dif-
ference between the predicted output and the ground-truth
output after being averaged over the coordinates. Formally,
MSE writes MSE(f(x);y) =jjf(x) yjj2=dim(y), where
dim(y)is the dimension of the ground-truth output.
B. Hiding Single Secret Model
First, we validate the effectiveness of our proposed Ma-
tryoshka attack in hiding a single secret model in the carrier.
SpeciÔ¨Åcally, we choose the secret model from diverse sources
while Ô¨Åxing the carrier model as a ResNet-18 on CIFAR-10.
We evaluate both the cases when the ParamPool is initialized
from the carrier model (marked as I) or initialized with a
customized size 5smaller than the carrier model (marked
asII). Table I reports the performance difference of thecarrier and the secret models when comparing with the normal
counterparts independently trained (the Normal columns).
Results & Analysis. First, from the Perf-I column for the
carrier model in Table I, we observe that Matryoshka only
sacriÔ¨Åces no more than 1%on the normal utility of the carrier
model for covertly transmitting each of the 7secret models
of diverse nature. According to [74], a 1%degradation level
is indistinguishable to the Ô¨Çuctuation in model performance
caused by randomness in training and ensures the attack
stealthiness. Meanwhile, from the Perf-I column for the
secret models, we observe that the performance degradation
on the decoded secret models is controlled to be less than 6%
uniformly, which indicates a successful functionality stealing
[5], [6]. Moreover, although the carrier model is normally
trained for image classiÔ¨Åcation, we do not observe clear
evidence that the carrier model is better in hiding models
from the same domain. For example, the Perf is slightly
larger on the medical imaging dataset DermaMNIST than the
audio dataset SpeechCommand (i.e.,  4:78%< 1:39%).
According to our analysis in ¬ßV-C, this may be explained by
the observation that an independently optimized M5 model
on SpeechCommand has a more similar distribution to the
ResNet-18 compared with a LeNet-5 trained on DermaMNIST
(i.e., 0:021<0:053in terms of OTD). Finally, by comparing
thePerf-I and the Perf-II columns in Table I, we observe
the advatange of initializing the ParamPool directly from the
carrier model, i.e., Option I, in attack effectiveness. This can
be explained by the more learnable parameters provided by the
carrier and the more delicate random value scheme adopted
by the deep learning framework to initialize the carrier model
[75], [76]. Therefore, in the following experiments, we mainly
evaluate the Option I intialization. Yet, as we have discussed,
initialization with a smaller ParamPool, i.e., Option II, has its
own advantages in increasing the robustness of Matryoshka,
which is later validated in ¬ßVI-E.
C. Hiding Multiple Secret Models
Next, we substantially extend the case of single model
hiding to covertly transmitting multiple secret models in the
carrier model at one time. SpeciÔ¨Åcally, we select a represen-
tative set of 6secret models in the Model column of Table II,
spanning all the 5application domains we cover. We leverage
the Matryoshka attack with a ParamPool initialized from the
9

--- PAGE 10 ---
carrier model ResNet-18. Table II reports the performance
difference of the carrier and the secret models.
TABLE II
EFFECTIVENESS OF MATRYOSHKA IN TRANSMITTING MULTIPLE SECRET
MODELS (THE TABLE EXCLUDES 2AUXILIARY MODELS FOR STEALING
NON -LEARNABLE PARAMETERS ).
Domain Model # Params Perf NormalSecret ModelsImage MobileNetV1-SSD 9.47M -1.22% 51.57%
Text TextCNN 6.69M -5.61% 90.85%
Audio M5 26.92K -1.89% 96.86%
Time Series RNN 1.67K 2.28% 86.36%
HealthcareLeNet-5 61.98K -6.13% 74.21%
MLP 0.5K -0.27 8.23
# Params in Total = 16.25M - -
Carrier Model ResNet-18 11.17M -0.41% 95.50%
Results & Analysis. As Table II shows, Matryoshka also
shows high effectiveness in covertly transmitting multiple
models at one time. When we encode all the 6models and the
2auxiliary models in a ResNet-18, the Perf on the carrier
model remains less than 1%as when encoding a single secret
model. This phenomenon is more striking in this case because
the carrier model has to encode multiple models which has at
least30% more parameters than itself (i.e., 11:17M vs. 16:25M
plus the auxiliary model). Moreover, compared with the results
in Table I, we observe that hiding multiple secret models
neither incur more utility losses on the secret models. For
example, the Perf on TextCNN is 5:61% and5:65% when
it is hidden with or without other 5models, while, on RNN,
the performance even increases by near 2%when it is hidden
with other models. In contrast, most conventional information
hiding approaches, including LSB and sign encoding, are
unable to hide a larger carrier in a smaller one by design
[21] and hence cannot attain the same hiding capability as
Matryoshka.
D. Exploring the Hiding Capacity
On the Size of the Secret Model. To explore the capacity
limit of Matryoshka, we further validate the effectiveness in
hiding a secret model typically larger than the carrier model
by multiple times. SpeciÔ¨Åcally, we initialize the ParamPool
with a(i.e.,2(0;1:0]) proportion of parameters from the
full carrier mode, i.e., a ResNet-18 on CIFAR-10 [77]. We
choose the secret model as a VGG-16 on GTSRB [62], which
contains 14:74M parameters, 1:32of the full carrier model.
Then we vary the proportion from 1:0to0:01and conduct
Matryoshka with the ParamPool of different sizes. Fig.4(a)
reports the accuracy curves of the secret and the carrier model,
where the blue curve reports the size ratio between the secret
and the carrier model, and the dashed horizontal lines report
the accuracy of the normal counterparts.
Results & Analysis. As Fig.4(a) shows, Matryoshka is very
effective in hiding a large secret model with a much smaller
number of parameters from the carrier. For example, when the
size of the ParamPool is reduced to 5%of the full size, the
# Params in Carrier# Params in SecretFig. 4. The effectiveness of Matryoshka (a) when the ratio between the
carrier model and the secret model decreases and (b) when the number of
secret models increases. The dashed horizontal lines report the performance
of the normal counterparts.
carrier model is over 26smaller than the carrier in terms of
the parameter number. Even under this situation, the accuracy
of the secret model and the carrier model remains in a 1%
margin of the normal performance.
From our perspective, Matryoshka‚Äôs high capacity should be
attributed to our novel parameter sharing mechanism, which
sufÔ¨Åciently exploits the distribution similarity between model
parameters: When a smaller subset of parameters are randomly
sampled from the full carrier model, it can still approximate
the distribution of the secret model with a tolerable error. As
is expected, when the ParamPool size is further reduced to 1%
(i.e., 131smaller), the distortion caused by model hiding is
enlarged, i.e., the accuracy of the carrier and the secret models
decreases resp. by about 5%and15%.
On the Number of the Secret Models. We also present the
following experiment to explore Matryoshka‚Äôs behavior when
the number of secret tasks Nincreases. To let Nhave a near-
linear relation to the required hiding capacity, we construct
each secret task as a four-layer FCN with the hidden layer sizes
randomly sampled from [100;300] on a permutated MNIST
[78] with pixels in each input are permuted with a Ô¨Åxed
random permutation speciÔ¨Åc to one task. This ensures each
secret model is independent from one another [79]. The carrier
model is an FCN of the average scale, i.e., ( 784-200-200-10),
on its own permuted MNIST. Fig.4(b) reports the curve of
the average performance of the secret models when Nscales
from 10to400, where the dashed line reports the accuracy of
an independently trained model which shares the architecture
of the carrier model, and the error bars report the standard
deviation of the performance of all Nsecret models.
Results & Analysis. As Fig.4(b) shows, initially the FCN
carrier model encodes 10models of the same scale with
almost no performance degradation. As the number of tasks
continually increases, the accuracy of the carrier model and the
secret model decreases only by 1:5%whenNscales to 100,
and remains over 90% whenNreaches 400. Combined with
the results in ¬ßVI-C, Matryoshka does allow a carrier model
to encode multiple secret models of different architectures and
for different tasks , with only slight degradation on the normal
utility. To the best of knowledge, previous works only explore
the superposition of multiple models of the same architecture
in one model in the area of continual learning [79].
10

--- PAGE 11 ---
E. Robustness against Popular Post-Processing Techniques
Finally, we evaluate the robustness of Matryoshka when the
carrier model undergoes popular model post-processing tech-
niques, including parameter pruning and model Ô¨Åne-tuning.
SpeciÔ¨Åcally, we consider the following settings.
(1) Weight Pruning : Aproportion of weight parameters
with the smallest absolute values are set to be 0in each layer
[80]. In our experiments, we vary from 0:1to0:5.
(2) Partial Finetuning : To Ô¨Ånetune the last Klayers of an
H-layer carrier model, the parameters of the Ô¨Årst H Klayers
are Ô¨Åxed, while the parameters of the last Klayers are further
updated by optimizing the learning objective of the carrier
model on the training data.
In our experiments, we choose ResNet-18 on CIFAR-10
as the carrier model, with the three models in Table I on
GTSRB (image), IMDB (text) and SpeechCommand (audio)
as the secret model respectively. For model Ô¨Åne-tuning, we
Ô¨Åne-tune the parameters until the last linear layer, the four
basic blocks, and the Ô¨Årst convolutional layer in the carrier
model respectively. To test fusion techniques at the decoding
stage, we initialize the ParamPool with a customized size 5
smaller than the carrier model. This allows us to implement a
ParamPool restoration algorithm to recover the pruned parame-
ter by selecting the non-zero value from each ParamPool copy.
For Ô¨Åne-tuning, as the Ô¨Årst copy of the ParamPool would only
be modiÔ¨Åed when the model is Ô¨Åne-tuned to the Ô¨Årst several
layers, we always decode the Ô¨Årst copy as the ParamPool. Fig.5
reports the performance of the carrier and the secret models
under different conÔ¨Ågurations on SpeechCommand.
0 0.1 0.2 0.3 0.4 0.5
Ratio of Pruning ()
20406080100ACC (%)
(a)
Carrier
Secret (w/. fusion)
Secret (w/o. fusion)
1 2 3 4 5 6
Finetuning Until (K)80859095100
(b)
Carrier
Secret
Fig. 5. Performance of the carrier/secret models under different levels of (a)
weight pruning and (b) model Ô¨Ånetuning on SpeechCommand. The dashed
horizontal lines report the performance of the normal counterparts.
Results & Analysis. As Fig.5(a) shows, the performance of
the secret model remains high even until the carrier model
goes through a huge utility loss due to the large weight pruning
ratio. For example, when reaches 0:3, the performance of
the carrier model decreases by over 10% compared with the
normal performance (marked in the green dashed line), while
the accuracy of the secret model stays within the 1%margin
of the normal utility. Comparing the performance of the secret
model with and without the fusion smechanisms, we conclude
that the robustness of Matryoshka largely comes from the
information redunduncy which is innately implemented in
our design of ParamPool. Consequently, only if a ParamPool
parameter is not always in the smallest for all the layers, our
decoding algorithm can always recover its value by referring
to the unpruned copy. Similarly from Fig.5(b), robustness is
00.1 0.2 0.3 0.4 0.5
Ratio of Pruning ()
020406080100ACC (%)
(a)
Carrier
Secret (w/. fusion)
Secret (w/o. fusion)
1 2 3 4 5 6
Finetuning Until (K)0255075100
(b)
Carrier
SecretFig. 6. Performance of the carrier/secret models under different levels of (a)
weight pruning and (b) model Ô¨Ånetuning on IMDB.
00.1 0.2 0.3 0.4 0.5
Ratio of Pruning ()
020406080100ACC (%)
(a)
Carrier
Secret (w/. fusion)
Secret (w/o. fusion)
1 2 3 4 5 6
Finetuning Until (K)80859095100
(b)
Carrier
Secret
Fig. 7. Performance of the carrier/secret models under different levels of (a)
weight pruning and (b) model Ô¨Ånetuning on GTSRB.
also observed when the carrier model is Ô¨Ånetuned until the Ô¨Årst
layer. Fig.6&7 present similar experimental phenomenon when
hiding secret models on texts (IMDB) and images (GTSRB).
Besides the above post-processing techniques we cover,
we admit that Matryoshka may be less robust when the
architecture of the carrier model is modiÔ¨Åed. However, it is not
a common practice of post-processing an already optimized
model by modifying its architecture, except for some special
cases that the corporation wants to further optimize the storage
and computing efÔ¨Åciency by model compression (e.g., pruning
or quantization). Nevertheless, even with model compression,
the compressed version is usually released together with the
raw full-size model on most third-party platforms [81], [82].
Moreover, post-processing is usually conducted by the users
after he/she downloads the model and would hardly be con-
ducted by the corporation itself in case of degrading the
model utility. In other words, during typical model publishing
procedures, the model parameters stay frozen once the training
process Ô¨Ånishes, so as the architecture of the carrier model.
Therefore, the colluder is very likely to obtain the carrier
model as in the original form prepared by the insider.
F . Evading Potential Detection Strategies
To further understand the covertness of our Matryoshka
attack, we discuss several potential detection strategies and
show our covert transmission strategy can evade them naturally
or with slightly more adaptive designs. Following the same
security model in steganography [21], we mainly consider a
detector who is able to inspect the published model as a white
box to determine whether the model is a carrier model. The
detection is assumed to happen before the publishing process.
If the detector is conÔ¨Ådent of the existence of encoded secret
models, the transmission would be discarded. However, as a
11

--- PAGE 12 ---
trade-off, when an innocent model is wrongly recognized, the
publishing process would be unexpectedly delayed.
Detection by Parameter Distribution. First, we discuss
statistics-based steganalysis, which is a classical strategy on
multimedia contents [21] and may be possibly adopted by
a detector who has no knowledge of the attack mechanism.
SpeciÔ¨Åcally, the detector compares the model Cunder inspec-
tion with a set of normal models in terms of the parameter
distribution to determine whether there is abnormality. In our
preliminary study, we mainly consider the situation that the
detector has a set of models which are irrelevant with C. We
do not consider the case that the detector has a large set of
independently trained models of the same architecture as Cfor
doing statistical testing or learning-based detection [83]. From
our viewpoint, this would rarely happen in real world as the
purpose of training Cis for publishing, which means such a
model has not been trained or been available previously.
SpeciÔ¨Åcally, we Ô¨Årst prepare a carrier model and a normal
model of the same architecture. Then, we calculate the dissimi-
larity between them with 8irrelevant models which we use in
¬ßV-C. We use the OTD between the parameter distributions
to measure the model dissimilarity. Fig.8(a) compares the
distribution of the OTD on two types of model pairs, namely,
(carrier ,irrelevant ) and ( normal ,irrelevant ), where the dis-
tributions of dissimilarity metrics are almost indistinguishable
from one another on either weights or biases.
Detection by Repetitive Patterns. A more knowledgeable
detector who knows the construction procedure of the Param-
Pool may attempt to analyze the patterns in model parameters
for detection. According to ¬ßV-A, when the ParamPool is
directly constructed from the carrier model, there should be
no abnormal pattern in the carrier model, as each parameter
is independent from one another. Besides, according to our
previous results, the parameter distribution in the carrier model
is also indistinguishable from a normal model. However, an ex-
ploitable pattern does exist when the ParamPool is constructed
from scratch and has a smaller size than the carrier model.
As Fig.8(b) shows, a sequence of parameters would occur
for multiple times when we collect the learnable parameters
in the carrier model sequentially. The detector can exploit
this property to run a repetitive subsequence identiÔ¨Åcation
algorithm for detection, which has a complexity of O(n2)
(withnthe number of parameters in the carrier model).
Yet, to evade this heuristic-based detection is also straight-
forward. As Fig.8(b) shows, when the model hiding process
terminates, the attacker can actively Ô¨Ånetune the carrier model
on the training dataset while freezing one copy of ParamPool
to eliminate the repetitive parameter patterns. After the active
Ô¨Ånetuning, neither the carrier model nor the secret models
is expected to exhibit performance degradation by design. In
our viewpoint, the only cost of this evasion strategy is, after
the Ô¨Ånetuning, only one copy of ParamPool exists, and no
information redundancy can be used to improve the resilience
of the secret in the carrier model. We recommend the attacker
to determine whether to apply active Ô¨Ånetuning or not based
on the priority of covertness and robustness.
Fig. 8. (a) Comparison of distributions of parameter dissimilarity, where the
dashed lines mark the quantiles. (b) A partial Ô¨Åne-tuning strategy to evade
repetitive pattern based detection.
Detection by Monitoring System Usages. Finally, we
discuss the case that the detector monitors the real-time system
information of the computing devices to determine whether
there is an undergoing Matryoshka attack, which would be
typically conducted by a system administrator who is in charge
of the computing cluster. Under this situation, the detector may
observe side-channel information such as the CPU, GPU and
memory loads to recognize any undergoing attack (e.g., detect-
ing cryptojacking [84]). Although the joint training algorithm
of Matryoshka does exhibit a simultaneously high utilization
rate of the three types of computing resources, such a pattern
of resource utilization is very common for deep learning
projects. For example, using multiprocessing when streaming
the training batches would incur both high CPU and memory
use, while using data parallelism for accelerating the training
of a DNN model would cause high utilization rates on multiple
GPUs. In other words, a detector can hardly differentiate
between an undergoing Matryoshka and a resource-intensive
training task from the real-time system usage statistics only.
In summary, the detector in these cases would face a
huge risk of classifying an innocent model wrongly as a
carrier model and would therefore absent from alarming and
discarding any scheduled-to-publish model. In consideration of
the practical threats imposed by our attacks on private datasets
even with no exposed interface, we strongly encourage the
study of effective defense mechanisms as a future work.
VII. C ONCLUSION
In this paper, we design a novel insider attack called
Matryoshka which for the Ô¨Årst time reveals the possibility of
breaking the privacy of ML data even with no exposed inter-
face. Via our novel attack technique of hiding models in model,
Matryoshka successfully steals both the functionality aof the
private dataset(s) only if the attacker is authorized to access
the target dataset in the local network. We provide extensive
evaluation results to show Matryoshka is efÔ¨Åcient, robust and
covert in transmitting the secret models via the scheduled-to-
published DNN and exhibits a much higher hiding capacity
than known approaches. We hope our discovered vulnerability
would alarm AI corporations of the potential risks of any
unnecessary access to private datasets even if they are safely
stored in the local network.
12

--- PAGE 13 ---
REFERENCES
[1] Y . Bengio, Y . LeCun, and G. E. Hinton, ‚ÄúDeep learning for ai,‚Äù
Communications of the ACM , vol. 64, pp. 58 ‚Äì 65, 2021.
[2] ‚ÄúBuilding a national ai research resource: A blueprint for the national
research cloud,‚Äù https://hai.stanford.edu/sites/default/Ô¨Åles/2021-10/HAI
NRCR 2021 0.pdf.
[3] D. L. Wenskay, ‚ÄúIntellectual property protection for neural networks,‚Äù
Neural Networks , 1990.
[4] ‚ÄúGoogle‚Äôs Approach to IT Security,‚Äù https://static.
googleusercontent.com/media/1.9.22.221/en//enterprise/pdf/whygoogle/
google-common-security-whitepaper.pdf.
[5] F. Tram `er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, ‚ÄúStealing
machine learning models via prediction apis,‚Äù in USENIX Security
Symposium , 2016.
[6] J. R. Correia-Silva, R. Berriel, C. S. Badue, A. F. de Souza, and
T. Oliveira-Santos, ‚ÄúCopycat cnn: Stealing knowledge by persuading
confession with random non-labeled data,‚Äù 2018 International Joint
Conference on Neural Networks (IJCNN) , pp. 1‚Äì8, 2018.
[7] V . Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan,
‚ÄúExploring connections between active learning and model extraction,‚Äù
inUSENIX Security Symposium , 2020.
[8] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot,
‚ÄúHigh accuracy and high Ô¨Ådelity extraction of neural networks,‚Äù in
USENIX Security Symposium , 2020.
[9] H. Hu and J. Pang, ‚ÄúStealing machine learning models: Attacks and
countermeasures for generative adversarial networks,‚Äù Annual Computer
Security Applications Conference , 2021.
[10] M. Fredrikson, S. Jha, and T. Ristenpart, ‚ÄúModel inversion attacks that
exploit conÔ¨Ådence information and basic countermeasures,‚Äù in CCS,
2015.
[11] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, ‚ÄúProperty
inference attacks on fully connected neural networks using permutation
invariant representations,‚Äù in CCS, 2018.
[12] L. Melis, C. Song, E. D. Cristofaro, and V . Shmatikov, ‚ÄúExploiting unin-
tended feature leakage in collaborative learning,‚Äù 2019 IEEE Symposium
on Security and Privacy (SP) , pp. 691‚Äì706, 2019.
[13] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,
‚ÄúPrivacy in pharmacogenetics: An end-to-end case study of personalized
warfarin dosing,‚Äù vol. 2014, pp. 17‚Äì32, 2014.
[14] R. Shokri, M. Stronati, C. Song, and V . Shmatikov, ‚ÄúMembership
inference attacks against machine learning models,‚Äù Security & Privacy ,
pp. 3‚Äì18, 2017.
[15] A. Salem, Y . Zhang, M. Humbert, M. Fritz, and M. Backes, ‚ÄúMl-leaks:
Model and data independent membership inference attacks and defenses
on machine learning models,‚Äù NDSS , 2019.
[16] A. Cheddad, J. Condell, K. Curran, and P. M. Kevitt, ‚ÄúDigital image
steganography: Survey and analysis of current methods,‚Äù Signal Pro-
cess. , vol. 90, pp. 727‚Äì752, 2010.
[17] Z. Yang, X. Guo, Z.-M. Chen, Y . Huang, and Y .-J. Zhang, ‚ÄúRnn-stega:
Linguistic steganography based on recurrent neural networks,‚Äù IEEE
Transactions on Information Forensics and Security , vol. 14, pp. 1280‚Äì
1295, 2019.
[18] F. Djebbar and B. Ayad, ‚ÄúComparative study of digital audio steganog-
raphy techniques,‚Äù EURASIP Journal on Audio, Speech, and Music
Processing , vol. 2012, pp. 1‚Äì16, 2012.
[19] C. Song, T. Ristenpart, and V . Shmatikov, ‚ÄúMachine learning models that
remember too much,‚Äù Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security , 2017.
[20] Y . Uchida, Y . Nagai, S. Sakazawa, and S. Satoh, ‚ÄúEmbedding water-
marks into deep neural networks,‚Äù Proceedings of the 2017 ACM on
International Conference on Multimedia Retrieval , 2017.
[21] N. Provos and P. Honeyman, ‚ÄúHide and seek: An introduction to
steganography,‚Äù IEEE Secur. Priv. , vol. 1, pp. 32‚Äì44, 2003.
[22] J. von Neumann, ‚ÄúProbabilistic logic and the synthesis of reliable
organisms from unreliable components,‚Äù Automata studies , vol. 34, pp.
43‚Äì98, 1956.
[23] G. Ateniese, L. V . Mancini, A. Spognardi, A. Villani, D. Vitali, and
G. Felici, ‚ÄúHacking smart machines with smarter ones: How to extract
meaningful data from machine learning classiÔ¨Åers,‚Äù Int. J. Secur. Net-
works , vol. 10, pp. 137‚Äì150, 2015.
[24] T. Orekondy, B. Schiele, and M. Fritz, ‚ÄúKnockoff nets: Stealing function-
ality of black-box models,‚Äù 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 4949‚Äì4958, 2019.[25] S. J. Oh, M. Augustin, M. Fritz, and B. Schiele, ‚ÄúTowards reverse-
engineering black-box neural networks,‚Äù in ICLR , 2018.
[26] N. Carlini, M. Jagielski, and I. Mironov, ‚ÄúCryptanalytic extraction of
neural network models,‚Äù in CRYPTO , 2020.
[27] Z. Li and Y . Zhang, ‚ÄúMembership leakage in label-only exposures,‚Äù
Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security , 2021.
[28] C. A. Choquette-Choo, F. Tram `er, N. Carlini, and N. Papernot, ‚ÄúLabel-
only membership inference attacks,‚Äù in ICML , 2021.
[29] X. Pan, M. Zhang, S. Ji, and M. Yang, ‚ÄúPrivacy risks of general-purpose
language models,‚Äù 2020 IEEE Symposium on Security and Privacy (SP) ,
pp. 1314‚Äì1331, 2020.
[30] L. Zhu, Z. Liu, and S. Han, ‚ÄúDeep leakage from gradients,‚Äù in NeurIPS ,
2019.
[31] J. Geiping, H. Bauermeister, H. Dr ¬®oge, and M. Moeller, ‚ÄúInverting
gradients - how easy is it to break privacy in federated learning?‚Äù in
NeurIPS , 2020.
[32] N. Carlini, C. Liu, ¬¥U. Erlingsson, J. Kos, and D. X. Song, ‚ÄúThe
secret sharer: Evaluating and testing unintended memorization in neural
networks,‚Äù in USENIX Security Symposium , 2019.
[33] A. Salem, A. Bhattacharyya, M. Backes, M. Fritz, and Y . Zhang,
‚ÄúUpdates-leak: Data set inference and reconstruction attacks in online
learning,‚Äù USENIX Security , 2020.
[34] A. Radhakrishnan, M. Belkin, and C. Uhler, ‚ÄúOverparameterized neural
networks implement associative memory,‚Äù Proceedings of the National
Academy of Sciences of the United States of America , vol. 117, pp.
27 162 ‚Äì 27 170, 2020.
[35] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee,
A. Roberts, T. B. Brown, D. X. Song, ¬¥U. Erlingsson, A. Oprea, and
C. Raffel, ‚ÄúExtracting training data from large language models,‚Äù in
USENIX Security Symposium , 2021.
[36] Y . Adi, C. Baum, M. Ciss ¬¥e, B. Pinkas, and J. Keshet, ‚ÄúTurning
your weakness into a strength: Watermarking deep neural networks by
backdooring,‚Äù in USENIX Security Symposium , 2018.
[37] H. Chen, B. D. Rouhani, C. Fu, J. Zhao, and F. Koushanfar, ‚ÄúDeepmarks:
A secure Ô¨Ångerprinting framework for digital rights management of deep
learning models,‚Äù Proceedings of the 2019 on International Conference
on Multimedia Retrieval , 2019.
[38] T. Wang and F. Kerschbaum, ‚ÄúRiga: Covert and robust white-box water-
marking of deep neural networks,‚Äù Proceedings of the Web Conference
2021 , 2021.
[39] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning . MIT Press,
2016.
[40] J. B. Heaton, N. G. Polson, and J. H. Witte, ‚ÄúDeep learning for Ô¨Ånance:
Deep portfolios,‚Äù Econometric Modeling: Capital Markets - Portfolio
Theory eJournal , 2016.
[41] A. Esteva, B. Kuprel, R. A. Novoa, J. M. Ko, S. M. Swetter, H. M.
Blau, and S. Thrun, ‚ÄúDermatologist-level classiÔ¨Åcation of skin cancer
with deep neural networks,‚Äù Nature , 2017.
[42] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, ‚ÄúYou only
look once: UniÔ¨Åed, real-time object detection,‚Äù 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , pp. 779‚Äì788,
2016.
[43] I. J. Goodfellow, Y . Bengio, and A. C. Courville, ‚ÄúDeep learning,‚Äù
Nature , vol. 521, pp. 436‚Äì444, 2015.
[44] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
CoRR , vol. abs/1412.6980, 2015.
[45] S. Alneyadi, E. Sithirasenan, and V . Muthukkumarasamy, ‚ÄúA survey on
data leakage prevention systems,‚Äù J. Netw. Comput. Appl. , vol. 62, pp.
137‚Äì152, 2016.
[46] ‚ÄúUnderstanding and Selecting a Data Loss Prevention Solution,‚Äù https:
//securosis.com/assets/library/reports/DLP-Whitepaper.pdf.
[47] S. Ren, K. He, R. B. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-
time object detection with region proposal networks,‚Äù IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 39, pp. 1137‚Äì1149,
2015.
[48] C. Shorten and T. M. Khoshgoftaar, ‚ÄúA survey on image data augmen-
tation for deep learning,‚Äù Journal of Big Data , vol. 6, pp. 1‚Äì48, 2019.
[49] T. Baltruaitis, C. Ahuja, and L.-P. Morency, ‚ÄúMultimodal machine learn-
ing: A survey and taxonomy,‚Äù IEEE Transactions on Pattern Analysis
and Machine Intelligence , vol. 41, pp. 423‚Äì443, 2019.
[50] K. Krombholz, H. Hobel, M. Huber, and E. R. Weippl, ‚ÄúAdvanced social
engineering attacks,‚Äù J. Inf. Secur. Appl. , vol. 22, pp. 113‚Äì122, 2015.
13

--- PAGE 14 ---
[51] ‚ÄúOpen Sourcing BERT,‚Äù https://ai.googleblog.com/2018/11/
open-sourcing-bert-state-of-art-pre.html.
[52] ‚ÄúGitHub Homepage of OpenAI,‚Äù https://github.com/openai.
[53] C. W. Kurak and J. McHugh, ‚ÄúA cautionary note on image downgrad-
ing,‚Äù [1992] Proceedings Eighth Annual Computer Security Application
Conference , pp. 153‚Äì159, 1992.
[54] T. Liu, Z. Liu, Q. Liu, W. Wen, W. Xu, and M. Li, ‚ÄúStegonet: Turn
deep neural network into a stegomalware,‚Äù Annual Computer Security
Applications Conference , 2020.
[55] ‚ÄúBOSSBase Stegnoanalysis Dataset,‚Äù http://agents.fel.cvut.cz/boss/
index.php?mode=VIEW&tmpl=materials.
[56] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. G. Howard, H. Adam,
and D. Kalenichenko, ‚ÄúQuantization and training of neural networks for
efÔ¨Åcient integer-arithmetic-only inference,‚Äù 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 2704‚Äì2713, 2018.
[57] R. Krishnamoorthi, ‚ÄúQuantizing deep convolutional networks for efÔ¨Å-
cient inference: A whitepaper,‚Äù ArXiv , vol. abs/1806.08342, 2018.
[58] ‚ÄúPyTorch Hub,‚Äù https://pytorch.org/hub/.
[59] Y . Rubner, C. Tomasi, and L. J. Guibas, ‚ÄúThe earth mover‚Äôs distance as
a metric for image retrieval,‚Äù International Journal of Computer Vision ,
vol. 40, pp. 99‚Äì121, 2004.
[60] C. Villani, Optimal transport: old and new . Springer Science &
Business Media, 2008, vol. 338.
[61] T.-W. Weng, P. Zhao, S. Liu, P.-Y . Chen, X. Lin, and L. Daniel, ‚ÄúTowards
certiÔ¨Åcated model robustness against weight perturbations,‚Äù in AAAI ,
2020.
[62] S. Houben, J. Stallkamp, J. Salmen, M. Schlipsing, and C. Igel,
‚ÄúDetection of trafÔ¨Åc signs in real-world images: The German TrafÔ¨Åc
Sign Detection Benchmark,‚Äù in International Joint Conference on Neural
Networks , no. 1288, 2013.
[63] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556 , 2014.
[64] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman, ‚ÄúThe pascal visual object classes challenge:
A retrospective,‚Äù International Journal of Computer Vision , vol. 111,
no. 1, pp. 98‚Äì136, Jan. 2015.
[65] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, ‚ÄúMobilenets: EfÔ¨Åcient convo-
lutional neural networks for mobile vision applications,‚Äù arXiv preprint
arXiv:1704.04861 , 2017.
[66] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts,
‚ÄúLearning word vectors for sentiment analysis,‚Äù in Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies . Portland, Oregon, USA: Association
for Computational Linguistics, June 2011, pp. 142‚Äì150. [Online].
Available: http://www.aclweb.org/anthology/P11-1015
[67] Y . Zhang and B. Wallace, ‚ÄúA sensitivity analysis of (and practitioners‚Äô
guide to) convolutional neural networks for sentence classiÔ¨Åcation,‚Äù
arXiv preprint arXiv:1510.03820 , 2015.
[68] P. Warden, ‚ÄúSpeech Commands: A Dataset for Limited-V ocabulary
Speech Recognition,‚Äù ArXiv e-prints , Apr. 2018. [Online]. Available:
https://arxiv.org/abs/1804.03209
[69] W. Dai, C. Dai, S. Qu, J. Li, and S. Das, ‚ÄúVery deep convolutional neural
networks for raw waveforms,‚Äù in 2017 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , 2017, pp. 421‚Äì
425.
[70] J. Liu, L. Zhong, J. Wickramasuriya, and V . Vasudevan, ‚Äúuwave:
Accelerometer-based personalized gesture recognition and its applica-
tions,‚Äù Pervasive and Mobile Computing , vol. 5, no. 6, pp. 657‚Äì675,
2009.
[71] J. Yang, R. Shi, D. Wei, Z. Liu, L. Zhao, B. Ke, H. PÔ¨Åster, and
B. Ni, ‚ÄúMedmnist v2: A large-scale lightweight benchmark for 2d and
3d biomedical image classiÔ¨Åcation,‚Äù arXiv preprint arXiv:2110.14795 ,
2021.
[72] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, ‚ÄúGradient-based learning
applied to document recognition,‚Äù Proceedings of the IEEE , vol. 86,
no. 11, pp. 2278‚Äì2324, 1998.
[73] G. Truda and P. Marais, ‚ÄúEvaluating warfarin dosing models on
multiple datasets with a novel software framework and evolutionary
optimisation,‚Äù Journal of Biomedical Informatics , p. 103634, 2020.
[Online]. Available: http://www.sciencedirect.com/science/article/pii/
S1532046420302628[74] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, ‚ÄúBadnets: Evaluating
backdooring attacks on deep neural networks,‚Äù IEEE Access , vol. 7,
pp. 47 230‚Äì47 244, 2019.
[75] X. Glorot and Y . Bengio, ‚ÄúUnderstanding the difÔ¨Åculty of training deep
feedforward neural networks,‚Äù in AISTATS , 2010.
[76] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDelving deep into rectiÔ¨Åers:
Surpassing human-level performance on imagenet classiÔ¨Åcation,‚Äù 2015
IEEE International Conference on Computer Vision (ICCV) , pp. 1026‚Äì
1034, 2015.
[77] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
2009.
[78] I. J. Goodfellow, M. Mirza, X. Da, A. C. Courville, and Y . Bengio, ‚ÄúAn
empirical investigation of catastrophic forgeting in gradient-based neural
networks,‚Äù ICLR , 2014.
[79] B. Cheung, A. Terekhov, Y . Chen, P. Agrawal, and B. A. Olshausen,
‚ÄúSuperposition of many models into one,‚Äù ArXiv , vol. abs/1902.05522,
2019.
[80] S. Han, J. Pool et al. , ‚ÄúLearning both weights and connections for
efÔ¨Åcient neural network,‚Äù ArXiv , 2015.
[81] ‚ÄúModels - Machine Learning - Apple Developer,‚Äù https://developer.
apple.com/machine-learning/models/.
[82] ‚ÄúAvailable Models-PaddleLite,‚Äù https://paddle-
lite.readthedocs.io/zh/latest/introduction/support model list.html,
accessed: 2021-05-21.
[83] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, ‚ÄúDetecting
ai trojans using meta neural analysis,‚Äù 2021 IEEE Symposium on Security
and Privacy (SP) , pp. 103‚Äì120, 2021.
[84] R. Ning, C. Wang, C. Xin, J. Li, L. Zhu, and H. Wu, ‚ÄúCapjack: Capture
in-browser crypto-jacking by deep capsule network through behavioral
analysis,‚Äù IEEE INFOCOM 2019 - IEEE Conference on Computer
Communications , pp. 1873‚Äì1881, 2019.
14
