# 2302.10798.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/shared-params/2302.10798.pdf
# File size: 1379958 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Learning a Consensus Sub-Network with
Polarization Regularization and One Pass Training
Xiaoying Zhi, Varun Babbar, Rundong Liu, Pheobe Sun,
Fran Silavong, Ruibo Shi, Sean Moran
JPMorgan Chase, 25 Bank Street, London, E145JP, UK.
Contributing authors: xiaoying.zhi@jpmchase.com;
varun.babbar@jpmchase.com; eric.liu@jpmchase.com;
pheobe.sun@jpmchase.com; fran.silavong@jpmchase.com;
ruibo.shi@jpmchase.com; sean.j.moran@jpmchase.com;
Abstract
The subject of green AI has been gaining attention within the deep learning
community given the recent trend of ever larger and more complex neural network
models. Existing solutions for reducing the computational load of training at
inference time usually involve pruning the network parameters. Pruning schemes
often create extra overhead either by iterative training and fine-tuning for static
pruning or repeated computation of a dynamic pruning graph. We propose a
new parameter pruning strategy for learning a lighter-weight sub-network that
minimizes the energy cost while maintaining comparable performance to the fully
parameterised network on given downstream tasks. Our proposed pruning scheme
is green-oriented, as it only requires a one-off training to discover the optimal static
sub-networks by dynamic pruning methods. The pruning scheme consists of a
binary gating module and a polarizing loss function to uncover sub-networks with
user-defined sparsity. Our method enables pruning and training simultaneously,
which saves energy in both the training and inference phases and avoids extra
computational overhead from gating modules at inference time. Our results on
CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our scheme can remove
≈50%of connections in deep networks with ≤1%reduction in classification
accuracy. Compared to other related pruning methods, our method demonstrates
a lower drop in accuracy for equivalent reductions in computational cost.
Keywords: Neural Architecture Pruning, Machine Learning, Computer Vision.
1arXiv:2302.10798v5  [cs.LG]  10 Jan 2025

--- PAGE 2 ---
1 Introduction
Large, sparse and over-parameterised models bring state-of-the-art (SOTA) performance
on many tasks but require significantly more computational power and therefore
energy compared to conventional machine learning models [ 1]. For example, the vision
transformer model (ViT-L16) with 307M parameters can achieve 99.42% accuracy on
the CIFAR-10 dataset and 87.76% on the ImageNet dataset [ 2]. The training of ViT-L16
model requires 680 TPUv3-core-days1and 3672kWh energy consumption, equivalent
to 32.5% of the annual energy consumption of an average US household [2–4].
Network pruning is a promising research direction that helps achieve greener AI
models, based on the assumption that we can safely remove parameters from over-
parameterized networks, without significantly degrading the network performance [ 5].
There are two common types of network pruning methods - static anddynamic . Static
network pruning generates a unified sub-network for all data, while dynamic pruning
computes different sub-networks for each data sample. Static network pruning often
requires a pre-defined neuron importance measure which determines which trained
neurons should be pruned [ 6–10]. Further fine-tuning or regrowing of the selected
sub-network are often involved after training, which can potentially lead to further
improvement in performance [ 11–13]. Dynamic pruning, on the other hand, applies a
parameterized and learnable gating function that computes the neuron importance on
the fly, leading to a different computational graph for each data sample. The training
phase optimizes the learnable gating functions with an empirical loss, and the inference
phase computes the appropriate sub-network through forward propagation through
the gating modules [14–17].
From a green AI perspective neither the dynamic or static pruning approaches are
ideal. Dynamic pruning is not optimal for parallel computing due to the necessary index-
ing operations at inference time. Furthermore, dynamic pruning introduces overhead
from the necessary connection importance computations. Static pruning can reduce
computational resources at inference time, but the iterative pruning-and-fine-tuning
process consumes more computational resources during the training phase. One-shot
pruning after training is no better than the iterative procedure as its effectiveness
heavily depends on the assumed priors, which lack verification in their validity prior to
training [18].
Our proposed pruning method forms a compressed network without the cost of sig-
nificant additional training resources. It achieves this by simultaneously optimizing the
network structure and parameters in a single pass. We posit that this new paradigm
brings forward a significant improvement for the efficiency desiderata in pruning meth-
ods, particularly in resource-constrained environments. Particularly, the simultaneous
optimization is realized with a light-weight trainable binary gating module along with a
polarising regularizer . The polarising regularizer produces a stable sub-network that
performs well for all data points towards the end of training. Inference time is reduced
as the smaller static sub-network is ready-to-use. We verify the pruning scheme on
two types of pruning (layer and channel) on different ResNets [ 19], applied to three
1Multiplication of the number of used TPUv3 cores and the training time in days
2

--- PAGE 3 ---
datasets with different size and number of classes (CIFAR-10, CIFAR-100, and Tiny
Imagenet[20, 21]). Comparisons with competitive existing baselines are presented.
1.1 Green and Red AI: Raising Awareness of the Energy Cost
of AI
Schwartz et al. [1]were among the first authors to define the concepts of green AI
(environmentally friendly AI) and red AI (heavily energy-consuming AI), suggesting
that AI models should be evaluated beyond accuracy by taking into account their
carbon emission and electricity usage, elapsed time, parameter count, and floating
point operations (FPOs/FLOPs). Patterson et al. [22]and Dodge et al. [23]propose
frameworks that quantify the carbon emission resulting from application of specific
AI models on various common devices. In order to reduce the carbon emission from
model training, different approaches have been commonly used. For example, model
quantization can be used to reduce the elapsed time and processor memory usage [ 24],
while network distillation and network pruning approaches can be used to reduce the
number of parameters and total FLOPs [25].
1.2 Network Pruning
Network pruning aims to rank the importance of the edges in a neural network model
in order to find a sub-network with the most important edges. There are generally
two approaches to achieve this goal: static ordynamic methodologies. Static network
pruning finds a unified sub-network at the end of the training, and is usually followed-
up by a fine-tuning procedure to further improve the sub-network performance. This
pruning scheme relies on the calculated importance scores of the edges of interest. The
edge importance can be calculated, for example, by the magnitude or the influence of
an edge to the final output.
In convolutional neural networks (CNNs), pruning is usually achieved in three
dimensions: depth (layers), width (channels), and resolution (feature maps) [ 26]. Exper-
iments on static feature map pruning [ 27] and channel pruning [ 28] demonstrated a
30% reduction in FLOPs or a 2.5 ×reduction in GPU time with only negligible perfor-
mance degradation or even improvement in some cases. Cai et al. [29]expanded the
problem to multi-stage pruning to make the pruning approach adaptable to different
size requirements. This goal was achieved by training and fine-tuning the sub-networks
with incremental size reduction while making sure the model accuracy stays the same
each time the size requirement is reduced.
Dynamic pruning, on the other hand, aims to find input dependent sub-networks.
Input-dependent elements are usually added to the original network to compute the
importance of edges of interest. Veit and Belongie [14]proposed an adaptive inference
graph that computes the importance of the CNN layers with a probabilistic and
learnable gating module before each layer. Lin et al. [30]proposed a similar framework
to prune the CNN channels by using reinforcement learning to train an optimal channel
importance scoring mechanism.
Combining both static and dynamic pruning methods can achieve synergistic gains
in computational efficiency. A combined approach can benefit from the compatibility of
3

--- PAGE 4 ---
static pruning with parallel computing, saving energy especially on GPU computation.
This approach can also leverage the ability of dynamic pruning to induce networks
that are input data adaptive. For example, Lee [31]proposed a sub-differentiable
sparsification method where parameters can potentially be zeroed after optimization
under stochastic gradient descent. However, the non-unified sub-networks still cause
excess indexing computation in parallel computing. Our work focuses on the problem
of finding a unified sub-network for data following a certain distribution, by using
the dynamically pruned sub-networks as intermediate states. Recent work on unifying
sub-networks has involved pruning a graph representation of the neural network [ 32,33]
Additionally, our findings corroborate recent work on the existence of “lottery
tickets”, i.e. pruned sub-networks that can achieve similar accuracy as the original
network [ 34]. To generate such networks, Frankle and Carbin [34]develop the IMP
(Iterative Magnitude Pruning) scheme that involves iterative pruning and training over
multiple rounds until convergence. This is different from our proposed method, which
performs simultaneous training and pruning in one training session and is therefore
computationally cheaper to train.
1.3 Discrete Stochasticity and Gradient Estimation
To obtain a stable sub-network structure through gradient-based optimiza-
tion i.e. binary activation statuses for each connection, a gating module is needed
with differentiable and discrete latent variables. Discrete variables often require a
relaxation or estimation due to the incompatibility of the discretisation function with
back-propagation ( e.g. zero gradients everywhere).
A well-known estimation approach is the Gumbel-Softmax (GS) estimator [ 35]. The
Gumbel-Softmax is a continuous probability distribution that can be tuned to approxi-
mate a discrete categorical distribution. The gradients w.r.t. the categorical output
distribution is well-defined for the GS distribution. This technique is often applied to
generative sequence models requiring sampling under multinomial distributions [ 36–38].
An arguably simpler, but still effective approach is the straight-through estimator
(STE) [ 39], which binarizes the stochastic output based on a threshold in the forward
pass, and heuristically copies the gradient of next layer to the estimator. Experiments
show that neural networks gated by the STE give the lowest error rate among other
differentiable gates (multinomial and non-multinomial) [ 39]. We describe the STE in
more detail in Section 3.2.
1.4 Sparsity Regularizers
For the purposes of parameter pruning a sparsity regularizer is often used to control
the pruning ratio during training. The l1andl2regularizers are two most common.
However, standard regularization functions can lead to unnecessary pruning and
mistaken estimation of network connectivity importance. Regularizers that are more
sensitive to network structure include l2,0andl2,1structured sparsity regularization
[40], grouped on samples or on feature maps [41] etc.
Srinivas and Srinivas and Babu [42]proposed a binarizing regularizer that encourages
each network connection to approach either 1 or 0 for all samples. The binarizing
4

--- PAGE 5 ---
mechanism can also be extended to continuous activation rates. For example, Zhuang
et al. [43]integrated a polarization regularizer into network pruning to force the
deactivation of neurons. Networks pruned under this setting achieve the highest accuracy
even at high pruning rate compared to other pruning schemes.
2Problem Setup: Joint Parameter and Architecture
Learning
We denote a neural network with full connectivity in the form of a graph as Φ := ( V, E),
where Vis a set of nodes, Eis the set of edges E:={e(x,y),∀x, y∈V}. A sub-network
with partial connectivity can thus be represented as Φ′= (V, E′) where E′⊆E. We
also denote the transformation of a network as fθ(·)≡f(·;θ), where θdenotes all the
parameters in a network. Each edge e∈Eis associated with a weight θe. For the full
network θ=θΦ, and for the sub-network θ=θΦ′. A sub-network can be expressed in
terms of a full-network using an activation matrix Wewith certain elements zeroed, i.e:
θΦ′=W⊤
eθΦ, (1)
where we,c∈ {0,1}for every entry in the edge activation matrix Weis binary, and
θΦ′is computed from a Hadamard product. In network pruning, we aim to find a
sub-network Φ′and the optimal network parameters θ∗
Φ′simultaneously. We estimate
the optimal solution θ∗
Φ′with ˆθ∗
Φ′by minimising the empirical loss. We further reuse
the settings in Eq.1 to reform the objective below: i.e.
min
θΦ′,Φ′L(f(x;θΦ′),y).
min
θΦ,WeL(f(x;W⊤
eθΦ),y).(2)
3 Methodology
In practice, the edge activation matrix Weis not learned as a whole and each entry
in the matrix is not independent. When training a sequential network the activation
of earlier connections can affect the outputs of later connections, and thus also affect
gradient back-propagation. A naive binary/categorical Wewould prevent gradients
from propagating back, as a function with constant value has zero gradient. Therefore,
a gradient estimator is needed as the core gating element of each connectivity. We
choose the straight-through estimator (STE), as introduced in Section 3.2, as this core
element.
3.1 Network Architectures
Figure 1 illustrates the design for the gating module integration into a ResNet. Our
pruning scheme has slightly different workflows for the training phase and the testing
phase. In training, the gating modules with learnable dense layers are trained as part
of the network. At inference (for validation or test), the resultant Weis loaded, which
5

--- PAGE 6 ---
Fig. 1 : Illustration of a gating module with binary decision as integrated into the
original residual model. The learnable gating modules are trained as per other parts
of the network. At inference, the gate decisions are pre-loaded, and only the network
parameters whose gate decision is open are loaded and computed.
decides the subset of parameters to be selected – only the connections with a non-zero
we,cwill be loaded for parameters and included in the forward pass.
The choice of ResNet as the base network is based on the necessity of residual
connections to avoid potential computational path termination in the middle of the
network due to a deactivated full layer. For ResNet, we focus on CNN-centered layer
and channel (feature map) pruning. However, we also argue that this methodology has
the potential be applied to any type of connection, even in less structured pruning
(e.g. selected kernel-to-kernel connections between convolutional layers) and leave
empirical evidence as a direction for future work. While our method has similarities
with dropout-based methods in ResNets, these involve pruning specific connections
between nodes. From an architectural standpoint this doesn’t necessarily reduce the
number of FLOPs as there is no reduction in the number of matrix multiplications
required. In contrast removing entire channels / layers has this desired effect.
3.2 Straight-through Estimator
We chose the straight-through estimator (STE) as the binary head for the gating
module. The forward path of STE is a hard thresholding function:
STE (x) =(
1,ifx >0
0,ifx≤0. (3)
The backward gradient reflects why this estimator is known as “straight-through”:
∂L
∂x=∂L
∂STE (x)·∂STE (x)
∂x=(
∂L
∂STE (x),if|x| ≤1
0, if|x|>1, (4)
where the insensitive state is triggered when |x|>1. This is to avoid a possible scenario
where a large gradient makes the STE output value stay at either 1 or 0 permanently.
A clear advantage of the STE as the gating head is that it constitutes a lightweight
module for both forward and backward propagation. In the forward pass, no other
computation than a sign check is needed. In the backward pass no computation is
needed. The gradient estimation, often viewed as a coarse approximation of the true
gradient under noise, has been proved to positively correlate with the population
gradient, and therefore gradient descent helps to minimize the empirical loss [44].
6

--- PAGE 7 ---
3.3 Polarising Regularization for Unified Activation
During the dynamic pruning-style training, the matrix We(x) might not be the same for
allx∈ X. To encourage a unified edge activation matrix that We(x) =We(x′),∀x, x′∈
X, we introduce a polarisation regularizer Rpolar({We(x)|x∈ X} ). The complete loss
function is:
L(f(x),y) =Ltask(f(x),y) +λRpolar(We(x)) (5)
where Ltaskis the task loss, e.g. cross-entropy loss for classification tasks and mean-
squared error for regression tasks, and λis the scale factor for polarisation regularizer.
The general form of Rpolar(We(x)) is in the form of an inverted parabola. Supposing
We(x)∈R|C|is flattened for all covered connections c∈ C:
Rpolar(We(x)) :=1
|C|(1−¯We(x))⊤¯We(x), (6)
where ¯We(x) =1
|X|P
x∈XWe(x) is the averaged edge activation matrix over all data
samples. Given the range of ¯We,c∈[0,1], this form of inverted parabola ensures that
an equivalent optimum can be reached when ¯We,creaches either boundary of the range.
Specifically, in our ResNet layer-pruning scenario, the regularisation term is written as:
Rpolar :=1
|L|X
ly∈L(1−¯gly)¯gly, (7)
where ¯gly=1
|X|P
x∈Xgly(x)∈[0,1] is the average of the gating module outputs over
all input samples of the layer. Similarly, in our ResNet channel-pruning scenario, the
regularisation term is written as:
Rpolar :=1
|L|X
ly∈L1
|C|X
ch∈C(1−¯gly,ch)¯gly,ch (8)
where ¯gly,ch=1
|X|P
x∈Xgly,ch(x)∈[0,1] is the average of the gating module outputs
over all input samples of the channel ch∈ Cin the layer ly∈L..
4 Experiments
4.1 Datasets and Architecture Specifications
We test the effectiveness of our proposed method on the ResNet architecture [ 19] on
the CIFAR-10, CIFAR-100, and Tiny Imagenet datasets, with 10, 100, and 200 classes
respectively. We chose the three datasets to benchmark our work as they have widely-
acknowledged test results on most variants of ResNets. Testing on both datasets shows
the effectiveness of our method under both simple and complex data distributions.
Our experiments on the CIFAR datasets are conducted on one NVIDIA T4 GPU
with 16GB memory. The batch size is set to 256 for CIFAR-10 and 64 for CIFAR-100.
Our training is done under a staged decaying learning rate for 350 epochs (although
7

--- PAGE 8 ---
convergence can usually be achieved before 250 epochs). The initial learning rate for
both dataset is 0.1, and at each next stage will decrease to 10%. On CIFAR-10, the
learning rate is adjusted at epochs 60, 120, and 160. On CIFAR-100, the learning rate
is adjusted at epochs 125, 190, and 250. We chose stochastic gradient descent (SGD)
as the optimizer, with a momentum of 0.9 and a weight decay of 5 ×104. The networks
and training procedures are implemented in PyTorch. When randomness is involved,
we set the random seed to 1.
Our experiments on the Tiny Imagenet dataset are conducted on four NVIDIA A10
GPU with 24GB memory each. Our training is done with the SGD optimizer under a
staged decaying learning rate for 1200 epochs. The learning rate is set to 0.2 initially,
then decreases to 10% of the previous value at 600 and 900 epochs. Our batch size is
100. In addition, we applied Puzzle mix [ 45] for both baseline network training and
pruned network training to improve the classification accuracy on Tiny Imagenet.
We apply the network to the image classification task. The pruned networks are
evaluated by top-1 accuracy and FLOPs (floating-point operations). The FLOPs count
is approximated by the fvcore package2.
4.2 Comparison with Existing Pruning Baselines
We first compare the performance of our method with na¨ ıve baselines and other methods
in the literature that are related to ours. We follow the training specifications laid out in
Section 4.1. For these experiments, we choose our most performant pruning architecture
based on empirical results on the test set shown later in this section, where we use
the layer pruning scheme, the Gumbell softmax (section 4.4) and set λpolar =↑(as in
Table 6). Furthermore, we choose the ResNet56 as the base architecture in particular
to allow for the easy comparison with other pruning schemes seen in the literature.
In addition to our original loss function, we now wish to control the pruning rate
of our method. This is achieved by adding an additional sparsity regularization term
in the loss function that provides a signal for the polarization regulariser to keep fewer
gates open. The resulting loss function is:
L(f(x),y) =Ltask(f(x),y) +λpolarRpolar(We(x))
+λactRact(We(x)) (9)
where:
Ract(We(x)) =1
|L|X
ly∈L¯gly (10)
is the overall average layer activation, where the average is taken across layers ly∈
Land inputs x∈ X. We vary λact∈[0,1] to change the pruning rate ( i.e. the
% of parameters pruned). In the context of our work, this is defined as:
Ex∈XP
l#Parameters in layer l: Gate gl(x) = 0
#Total Parameters×100
(11)
2https://github.com/facebookresearch/fvcore
8

--- PAGE 9 ---
where the expectation over inputs accounts for the fact that some pruning schemes
may not achieve perfect sub-network unification.
We consider the following na¨ ıve baselines, due to their empirical performance in
the literature:
•Na¨ ıve Dropout ResNet-56: A standard classifier but with
k∈ {20%,30%,50%,60,80%}parameters randomly pruned during testing.
•Na¨ ıve Layer Pruned ResNet-56: The same classifier but with
k∈ {20%,30%,50%,60,80%}layer activations randomly set to 0 during testing.
Fig. 2 : Comparison of our method with some na¨ ıve baselines on CIFAR-10 with
ResNet-56. Left: Average pruning rate at inference vs Top-1 accuracy. Right % FLOPs
reduction at inference vs Top-1 accuracy. The naive dropout method does not reduce
FLOPs because it still involves computation through the “dropped” nodes - hence the
omission.
A visualized comparison with the na¨ ıve baselines is shown in Figure 2, demonstrating
our pruning method’s ability to maintain network performance with a high pruning
rate on the test set. Further benchmarking our method, we consider the methods that
also followed the idea of simultaneous pruning and learning, as listed in Table 1. Figure
3 shows the performance of our scheme against methods found in the literature, all
of which use the ResNet-56 as the base network. We note that our scheme provides
competitive results not only in terms of the absolute accuracy, but also the accuracy
drop resulting from pruning.
4.3 Investigating Chanel and Layer pruning
ResNets contain residual layers which consist of two convolutional layers and a
bypasssing residual connection.
In channel pruning, we experiment on two layer designs and three positions of the
gating module. Specifically, we experimented with gating module architectures that
consist of one and two dense layers, where Table 2 shows the detailed design. For the
9

--- PAGE 10 ---
Method Unpruned Accuracy Pruned Accuracy % FLOPS Reduction Accuracy Drop
Ours 93.43 92.42±0.14 41.81±4.01 1.01±0.14
AMC [46] 92.80 91.90 50.00 1.10
Importance [47] 93.60 91.90 39.90 1.14
SFP [48] 93.59 92.26 52.60 1.33
CP [49] 92.80 91.80 50.00 1.00
PFEC [50] 93.04 91.31 27.60 1.73
VCP [51] 93.04 92.26 20.30 0.78
Table 1 : The performance of our method over 5 trials against some established,
related methods in the literature for ≈50% FLOPs reduction ( Dataset : CIFAR-
10,Model : ResNet-56). We note that our method offers a competitive trade-off
between accuracy and FLOPs while being simple to implement. For SFP, we
consider only the pre-trained variant for fair comparison as the fine-tuning
variant in the paper incurs extra computational costs that are not necessarily
considered.
Fig. 3 : Comparison between our scheme and related methods in literature on CIFAR-
10 with ResNet-56 at inference. Left: Pruning rate vs Top-1 accuracy. Right % FLOPs
reduction vs Top-1 accuracy drop.
gating module positions, we experimented on placing the gating module before the
first convolutional layer, between the two convolution layers, and immediately after the
second convolutional layer. Figure 5 visualises the aforementioned decisions. Table 3
shows the pruning results on CIFAR-10 under different gating module architectures and
positions. For the gating module architectures (recall Table 2), results show that while
all designs achieve a similar channel pruning ratio, the design with two dense layers
and placed at the end of each residual layer (2FC-after) achieves the best classification
accuracy that is significantly higher than most others. However, the design with 1
dense layer placed between two convolution layers (1FC-middle) also achieves a similar
accuracy. Henceforth, we use our channel pruning baseline to be the 2FC-after design,
as per Table 3, as it showed the best performance in classification accuracy and channel
pruning ratio on the test set.
10

--- PAGE 11 ---
Layer Gating Module Channel Gating Module (K=2)
avgpool 2d (output size=channel in) flatten()
dense (in dim=channel in, out dim=16) dense(in dim=out channel*feature size, out dim=16)
batch norm 1d() batch norm 1d()
ReLU() ReLU()
dense (in dim=16, out dim=1) dense(in dim=16, out dim=1)
STE() STE()
Table 2 : Layer and channel gating module design. Left: “channel in” is
the input channel number for the first convolution layer in the residual
layer. Right : “mid channel” is the output channel number for the first
convolution layer, equal to the input channel number for the second
convolution layer. “feature size” is the dimension of a flattened feature
map. For other K values, we simply vary the dense layer number.
Model top-1 accuracy (%) (rel) gate open ratio (%)
baseline 93.68 (0) 100.00
1FC-before 69.71 (-23.97) 47.82
1FC-middle 90.60 (-3.08) 47.77
1FC-after 83.89 (-9.79) 41.91
2FC-before 85.39 (-8.29) 48.93
2FC-middle 82.94 (-10.74) 49.36
2FC-after 91.01 (-2.67) 48.76
Table 3 : Channel pruning results under different gating module
specifications on the CIFAR-10 dataset. Numbers in brackets
for top-1 accuracy show the relative difference (rel) from the
baseline model. “ {K}FC” refers to K={1,2}dense layer(s) in
gating module and “before”, “middle”, and “after” for the three
gating module positions, illustrated in Figure 5.
In layer pruning, our design decisions are simplified. We add the gating module
before the two convolutional layers, as seen in Figure 4, to decide whether the layer is
to be computed. Table 2 shows the detailed design of the gating module.
We now proceed to compare the channel and layer pruning schemes on the CIFAR-
10, CIFAR-100, and Tiny Imagenet datases, as summarised in Table 4. We observe
that the layer pruning scheme can save at least 14% of computations (FLOPs) while
sacrificing accuracy of less than 2.5%. Under the channel pruning scheme, we can save
at least 22% computations (FLOPs) while sacrificing accuracy of less than 3%.
In general, we note that the layer pruned models perform better than channel
pruned models ( i.e. there is a lower accuracy drop) for all three datasets, even when
relative differences in FLOPs are taken into account. We believe this is because under
the design of ResNet, the intermediate feature maps in each residual layer is sufficiently
information-compact, and any removal on the feature maps can lead to information
loss. The pruning ratio of channel pruning is positive though, with an almost 50%
FLOPs reduction.
11

--- PAGE 12 ---
Fig. 4 : Illustration of layer-pruning gating modules in ResNet.
(a) Channel prun-
ing before the first
convolution layer.
(b) Channel pruning between two convolution lay-
ers.
(c) Channel prun-
ing after the
second convolution
layer.
Fig. 5 : Illustration of channel-pruning gating modules in ResNet: The gating module
(a) before the first convolution layer; (b) between two convolutional layers; (c) after
the second convolution layer. K=1 or 2 in our experiments.
4.4 Ablation Studies for Gradient Estimation
We test the individual utility of the two major modules, STE and polarisation reg-
ularizer, through ablation studies. To test the utility of STE, we replace STE with
sampling from Bernoulli distribution and with Gumbel-softmax. When sampling from
Bernoulli distribution, we set up a threshold equal to the mean of the gating module’s
outputs right after the last dense layer ( i.e. right before the original STE). If the
output is larger than the mean, we keep the layer; otherwise we prune the layer.
12

--- PAGE 13 ---
Dataset Model Top-1 accuracy (%) Gate open ratio (%) FLOPs (M)(rel)
CIFAR-10 baseline 93.68 (0) 100.00 255.3 (1)
layer pruned 92.82 (-0.86) 53.70 137.7 (0.54)
channel pruned 91.01 (-2.67) 48.76 189.9 (0.74)
CIFAR-100 baseline 71.85 (0) 100.00 255.3 (1)
layer pruned 70.01 (-1.84) 66.67 171.1 (0.67)
channel pruned 66.91 (-4.94) 51.14 135.41 (0.52)
Tiny Imagenet baseline 61.43 (0) 100.00 556.6 (1)
layer pruned 59.14 (-2.29) 87.50 480.9 (0.86)
channel pruned 58.70 (-2.73) 66.60 437.84 (0.78)
Table 4 : Results of pruned networks on CIFAR-10, CIFAR-100, and Tiny Imagenet datasets.
Numbers in brackets for top-1 accuracy shows the relative difference from the baseline model.
FLOPs is counted in millions (M). Numbers in brackets for FLOPs shows the relative ratio from
the baseline model. Baseline model is ResNet110 for CIFAR-10 and CIFAR-100. Baseline model is
PreActResNet18 for Tiny Imagenet.
Function Top-1 accuracy (%) Rpolar
STE 92.86 0.00
Gumbel-softmax 94.05 0.0039
Bernoulli 91.96 0.2454
Table 5 : Results of different gating functions on
CIFAR-10. Rpolar taken at the end of a training
session, each with the same number of epochs.
A larger Rpolar corresponds to less unified sub-
networks after convergence. These experiments
were performed on ResNet110 with λpolar = 3.
Table 5 shows results from the three gating functions, experimented on CIFAR-
10. We observed that other than STE, no other gating head function can result in a
perfectly unified and stable sub-network. STE has utility here in terms of stabilising the
evolution of the dynamic sub-networks and retaining expected performance. However,
we note that the Gumbel-softmax has a potential to achieve a better task performance
while keeping a set of lightly dynamic sub-networks indicated by the low level of
Rpolar. We leave to future work a study on how a suitable unification of the resultant
dynamic sub-networks from Gumbel-softmax can be achieved to further improve the
performance over STE for this task.
13

--- PAGE 14 ---
Spec Top-1 accuracy (%) (rel) gate open ratio (%)
baseline 93.68 (0) 100.00
λpolar = 0 90.79 (-2.89) 18.52 - 53.70
λpolar = 1 92.44 (-1.24) 53.70
λpolar = 2 91.67 (-2.01) 40.74
λpolar = 3 91.85 (-1.83) 57.41
λpolar↑ 93.18 (-0.50) 55.56
Table 6 : Results of pruned networks under different λpolar
values on CIFAR-10 (ResNet110). ” λpolar↑” uses the settings
ofλpolar = 0 for the first 125 epochs; λ= 2 for the next 65
epochs; and λ= 3 for the resting epochs until end of training.
Fig. 6 : Layer opening ratio during training under different λ. Each row represents one
layer among the 54 layers. Each column represents one epoch. For λ∈ {0,1,2,3}, we
include the first 50 training epochs. For λ↑, the change in λis separated by the pink
line.
To understand the trade-off between pruning aggressiveness, accuracy retention,
and computational savings, we experimented on a series of pruning aggressiveness levels,
realized by tuning the hyperparamter λpolar. We experimented on λpolar values varying
from 0 to 3. We also tested the effect of gradually increasing regularizer weight λpolar
during a training session in order to verify whether a partially trained network would
affect the pruning results. Figure 6 shows the layer pruning evolution under different
λpolar settings and Table 6 shows the performance of the resulting sub-networks.
14

--- PAGE 15 ---
The layer pruning evolution figures show that a higher pruning aggressiveness level
(a higher λpolar) accelerates convergence to a unified sub-network than a lower level.
On the other hand, Table 6 shows that a higher opening ratio does not necessarily
bring more computational savings and accuracy retention. Increasing the pruning
aggressiveness (introducing the penalty term) at a later stage in training, however,
shows higher accuracy than other constant penalty terms, with higher computational
savings than other experiments.
5 Discussions
5.1 Computation Complexity
Similar to any extra gating modules, the introduced gating module from our method
adds an overhead on the overall computational complexity during training. The time
complexity of STE of a single input for forward path is O(1) according to Eq. 3. For
backward path, according to Eq. 4, the STE on each dimension brings only O(1) extra
computations than the original backward path.
Taking the setup in Table 2 (left column) as an example, a gating module of K
fully connected layers, ddimensions per layer, and |C|channels as input introduces an
overhead of O(dK|C|).
Merging the above gating module into network in Figure 4, for a network of N
layers and |C|channels per layer, the inserted gating modules introduces an overhead
ofO(dNK|C|).
In our experiment where K= 1 or 2, the final overhead is shrinked to O(dN|C|),
within which only dis dependent on the gating module’s design. A practically small
value of dis helpful in controlling the overall computational overhead.
The above overhead analysis is only applicable for training the network. After
training, the optimized network parameters include the gating module’s output, and a
layer is either activated or deactivated at inference for arbitrary inputs. The overhead
is thus excluded at inference. Our experiment (Table 6) show that the pruned model
can use only 55.56% of neurons to achieve only 0.5% of performance drop.
6 Conclusion
We proposed a network pruning scheme that maintains performance while being more
computationally efficient. Through simultaneous parameter and structure optimization,
our pruning scheme finds a stable sub-network with similar performance on the
same downstream task as the full (unpruned) network. The scheme consists of a
differentiable, lightweight binary gating module and regularisers to enforce data-
invariance of the pruned sub-networks. Experiments on layer and channel pruning
show results competitive with (or indeed better than) similar methods in literature.
With fine tuning of our uncovered sub-networks, we anticipate further performance
improvements - however, this is beyond the scope of our work, which aims to emphasise
maximal performance gains within limited computational budgets. We look forward to
testing the applicability of our pruning scheme to other base networks, datasets and
15

--- PAGE 16 ---
tasks, hoping that our encouraging results facilitate the transition from red AI towards
energy efficient and green deep learning models.
7 Disclaimer
This paper was prepared for informational purposes by the Applied Innovation of
AI team of JPMorgan Chase & Co. This paper is not a product of the Research
Department of JPMorgan Chase & Co. or its affiliates. Neither JPMorgan Chase &
Co. nor any of its affiliates makes any explicit or implied representation or warranty
and none of them accept any liability in connection with this paper, including, without
limitation, with respect to the completeness, accuracy, or reliability of the information
contained herein and the potential legal, compliance, tax, or accounting effects thereof.
This document is not intended as investment research or investment advice, or as a
recommendation, offer, or solicitation for the purchase or sale of any security, financial
instrument, financial product or service, or to be used in any way for evaluating the
merits of participating in any transaction. The described work is a prototype and is
not a production deployed system.
References
[1]Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O.: Green ai. Commun. ACM
63(12), 54–63 (2020) https://doi.org/10.1145/3381831
[2]Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
arXiv (2020). https://doi.org/10.48550/ARXIV.2010.11929 . https://arxiv.org/
abs/2010.11929
[3]Jouppi, N.P., Yoon, D.H., Kurian, G., Li, S., Patil, N., Laudon, J., Young, C.,
Patterson, D.: A domain-specific supercomputer for training deep neural networks.
Commun. ACM 63(7), 67–78 (2020) https://doi.org/10.1145/3360307
[4]EIA US: 2020 Average Monthly Bill- Residential. https://www.eia.gov/electricity/
sales revenue price/pdf/table5 a.pdf. Accessed: 2022-07-19 (2021)
[5]Denil, M., Shakibi, B., Dinh, L., Ranzato, M., Freitas, N.: Predicting Parameters
in Deep Learning. arXiv (2013). https://doi.org/10.48550/ARXIV.1306.0543 .
https://arxiv.org/abs/1306.0543
[6]Shafiee, M.S., Shafiee, M.J., Wong, A.: Dynamic Representations Toward Efficient
Inference on Deep Neural Networks by Decision Gates. arXiv (2018). https:
//doi.org/10.48550/ARXIV.1811.01476 . https://arxiv.org/abs/1811.01476
16

--- PAGE 17 ---
[7]Luo, J., Zhang, H., Zhou, H., Xie, C., Wu, J., Lin, W.: Thinet: Pruning cnn filters
for a thinner net. IEEE Transactions on Pattern Analysis ‘I&’ Machine Intelligence
41(10), 2525–2538 (2019) https://doi.org/10.1109/TPAMI.2018.2858232
[8]Cheong, R.: transformers . zip : Compressing transformers with pruning and
quantization. (2019)
[9]Zhang, M.S., Stadie, B.: One-Shot Pruning of Recurrent Neural Networks by
Jacobian Spectrum Evaluation. arXiv (2019). https://doi.org/10.48550/ARXIV.
1912.00120 . https://arxiv.org/abs/1912.00120
[10]Lin, Z., Liu, J., Yang, Z., Hua, N., Roth, D.: Pruning redundant mappings in
transformer models via spectral-normalized identity prior. In: Findings of the
Association for Computational Linguistics: EMNLP 2020, pp. 719–730. Association
for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.
findings-emnlp.64 . https://aclanthology.org/2020.findings-emnlp.64
[11] Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both Weights and Connections
for Efficient Neural Networks. arXiv (2015). https://doi.org/10.48550/ARXIV.
1506.02626 . https://arxiv.org/abs/1506.02626
[12]Zhu, M., Tang, Y., Han, K.: Vision Transformer Pruning. arXiv (2021). https:
//doi.org/10.48550/ARXIV.2104.08500 . https://arxiv.org/abs/2104.08500
[13]Hou, Z., Qin, M., Sun, F., Ma, X., Yuan, K., Xu, Y., Chen, Y.-K., Jin, R., Xie, Y.,
Kung, S.-Y.: CHEX: CHannel EXploration for CNN Model Compression. arXiv
(2022). https://doi.org/10.48550/ARXIV.2203.15794 . https://arxiv.org/abs/2203.
15794
[14]Veit, A., Belongie, S.: Convolutional Networks with Adaptive Inference Graphs.
arXiv (2017). https://doi.org/10.48550/ARXIV.1711.11503 . https://arxiv.org/
abs/1711.11503
[15]Gao, X., Zhao, Y., Dudziak, L., Mullins, R., Xu, C.-z.: Dynamic Channel Pruning:
Feature Boosting and Suppression. arXiv (2018). https://doi.org/10.48550/ARXIV.
1810.05331 . https://arxiv.org/abs/1810.05331
[16]Bejnordi, B.E., Blankevoort, T., Welling, M.: Batch-Shaping for Learning Condi-
tional Channel Gated Networks. arXiv (2019). https://doi.org/10.48550/ARXIV.
1907.06627 . https://arxiv.org/abs/1907.06627
[17]Yin, H., Vahdat, A., Alvarez, J., Mallya, A., Kautz, J., Molchanov, P.: A-ViT:
Adaptive Tokens for Efficient Vision Transformer. arXiv (2021). https://doi.org/
10.48550/ARXIV.2112.07658 . https://arxiv.org/abs/2112.07658
[18]Lee, N., Ajanthan, T., Torr, P.H.S.: SNIP: Single-shot Network Pruning based on
Connection Sensitivity. arXiv (2018). https://doi.org/10.48550/ARXIV.1810.02340
17

--- PAGE 18 ---
. https://arxiv.org/abs/1810.02340
[19]He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition.
arXiv (2015). https://doi.org/10.48550/ARXIV.1512.03385 . https://arxiv.org/
abs/1512.03385
[20]Krizhevsky, A.: Learning multiple layers of features from tiny images. (2009).
https://www.cs.toronto.edu/ \texttildelowkriz/learning-features-2009-TR.pdf
[21]Le, Y., Yang, X.S.: Tiny imagenet visual recognition challenge. (2015). https:
//api.semanticscholar.org/CorpusID:16664790
[22]Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So,
D., Texier, M., Dean, J.: Carbon Emissions and Large Neural Network Training.
arXiv (2021). https://doi.org/10.48550/ARXIV.2104.10350 . https://arxiv.org/
abs/2104.10350
[23] Dodge, J., Prewitt, T., Combes, R.T.D., Odmark, E., Schwartz, R., Strubell, E.,
Luccioni, A.S., Smith, N.A., DeCario, N., Buchanan, W.: Measuring the Carbon
Intensity of AI in Cloud Instances. arXiv (2022). https://doi.org/10.48550/ARXIV.
2206.05229 . https://arxiv.org/abs/2206.05229
[24]Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: A Survey
of Quantization Methods for Efficient Neural Network Inference. arXiv (2021).
https://doi.org/10.48550/ARXIV.2103.13630 . https://arxiv.org/abs/2103.13630
[25]Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowledge in a Neural Network.
arXiv (2015). https://doi.org/10.48550/ARXIV.1503.02531 . https://arxiv.org/
abs/1503.02531
[26]Wang, W., Chen, M., Zhao, S., Chen, L., Hu, J., Liu, H., Cai, D., He, X.,
Liu, W.: Accelerate CNNs from Three Dimensions: A Comprehensive Pruning
Framework. arXiv (2020). https://doi.org/10.48550/ARXIV.2010.04879 . https:
//arxiv.org/abs/2010.04879
[27]Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning Filters for
Efficient ConvNets. arXiv (2016). https://doi.org/10.48550/ARXIV.1608.08710 .
https://arxiv.org/abs/1608.08710
[28]He, Y., Zhang, X., Sun, J.: Channel Pruning for Accelerating Very Deep Neural
Networks. arXiv (2017). https://doi.org/10.48550/ARXIV.1707.06168 . https:
//arxiv.org/abs/1707.06168
[29]Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Network
and Specialize it for Efficient Deployment. arXiv (2019). https://doi.org/10.48550/
ARXIV.1908.09791 . https://arxiv.org/abs/1908.09791
18

--- PAGE 19 ---
[30]Lin, J., Rao, Y., Lu, J., Zhou, J.: Runtime neural pruning. In: Guyon, I.,
Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett,
R. (eds.) Advances in Neural Information Processing Systems, vol. 30. Cur-
ran Associates, Inc., ??? (2017). https://proceedings.neurips.cc/paper/2017/file/
a51fb975227d6640e4fe47854476d133-Paper.pdf
[31]Lee, Y.: Differentiable Sparsification for Deep Neural Networks. arXiv (2019).
https://doi.org/10.48550/ARXIV.1910.03201 . https://arxiv.org/abs/1910.03201
[32]Wortsman, M., Farhadi, A., Rastegari, M.: Discovering Neural Wirings. arXiv
(2019). https://doi.org/10.48550/ARXIV.1906.00586 . https://arxiv.org/abs/1906.
00586
[33]Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., Rastegari, M.: What’s
Hidden in a Randomly Weighted Neural Network? arXiv (2019). https://doi.org/
10.48550/ARXIV.1911.13299 . https://arxiv.org/abs/1911.13299
[34]Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In: 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, ??? (2019).
https://openreview.net/forum?id=rJl-b3RcF7
[35]Jang, E., Gu, S., Poole, B.: Categorical Reparameterization with Gumbel-Softmax.
arXiv (2016). https://doi.org/10.48550/ARXIV.1611.01144 . https://arxiv.org/
abs/1611.01144
[36]Kusner, M.J., Hern´ andez-Lobato, J.M.: GANS for Sequences of Discrete Elements
with the Gumbel-softmax Distribution. arXiv (2016). https://doi.org/10.48550/
ARXIV.1611.04051 . https://arxiv.org/abs/1611.04051
[37]Shen, J., Zhen, X., Worring, M., Shao, L.: Variational multi-task learning with
gumbel-softmax priors. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang,
P.S., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems,
vol. 34, pp. 21031–21042. Curran Associates, Inc., ??? (2021). https://proceedings.
neurips.cc/paper/2021/file/afd4836712c5e77550897e25711e1d96-Paper.pdf
[38]Chang, J., Zhang, X., Guo, Y., Meng, G., Xiang, S., Pan, C.: Differentiable
Architecture Search with Ensemble Gumbel-Softmax. arXiv (2019). https://doi.
org/10.48550/ARXIV.1905.01786 . https://arxiv.org/abs/1905.01786
[39]Bengio, Y., L´ eonard, N., Courville, A.: Estimating or Propagating Gradients
Through Stochastic Neurons for Conditional Computation. arXiv (2013). https:
//doi.org/10.48550/ARXIV.1308.3432 . https://arxiv.org/abs/1308.3432
[40]Lin, S., Ji, R., Li, Y., Deng, C., Li, X.: Towards Compact ConvNets via Structure-
Sparsity Regularized Filter Pruning. arXiv (2019). https://doi.org/10.48550/
ARXIV.1901.07827 . https://arxiv.org/abs/1901.07827
19

--- PAGE 20 ---
[41]Li, Y., Gu, S., Mayer, C., Van Gool, L., Timofte, R.: Group Sparsity: The Hinge
Between Filter Pruning and Decomposition for Network Compression. arXiv (2020).
https://doi.org/10.48550/ARXIV.2003.08935 . https://arxiv.org/abs/2003.08935
[42]Srinivas, S., Babu, R.V.: Learning Neural Network Architectures using Back-
propagation. arXiv (2015). https://doi.org/10.48550/ARXIV.1511.05497 . https:
//arxiv.org/abs/1511.05497
[43]Zhuang, T., Zhang, Z., Huang, Y., Zeng, X., Shuang, K., Li, X.: Neuron-
level structured pruning using polarization regularizer. In: Larochelle, H.,
Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in
Neural Information Processing Systems, vol. 33, pp. 9865–9877. Curran
Associates, Inc., ??? (2020). https://proceedings.neurips.cc/paper/2020/file/
703957b6dd9e3a7980e040bee50ded65-Paper.pdf
[44]Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., Xin, J.: Understanding Straight-
Through Estimator in Training Activation Quantized Neural Nets. arXiv (2019).
https://doi.org/10.48550/ARXIV.1903.05662 . https://arxiv.org/abs/1903.05662
[45]Kim, J.-H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In: International Conference on Machine Learning
(ICML) (2020)
[46]He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., Han, S.: AMC: AutoML for model
compression and acceleration on mobile devices. In: Computer Vision – ECCV 2018,
pp. 815–832. Springer, ??? (2018). https://doi.org/10.1007/978-3-030-01234-2 48 .
https://doi.org/10.1007/978-3-030-01234-2 48
[47]Dekhovich, A., Tax, D.M.J., Sluiter, M.H.F., Bessa, M.A.: Neural network relief:
a pruning algorithm based on neural activity. CoRR abs/2109.10795 (2021)
2109.10795
[48]He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft filter pruning for accelerating
deep convolutional neural networks. In: International Joint Conference on Artificial
Intelligence (IJCAI), pp. 2234–2240 (2018)
[49]He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural
networks. In: 2017 IEEE International Conference on Computer Vision (ICCV),
pp. 1398–1406 (2017). https://doi.org/10.1109/ICCV.2017.155
[50]Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient
convnets. ArXiv abs/1608.08710 (2016)
[51]Zhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W., Tian, Q.: Variational convo-
lutional neural network pruning. In: 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 2775–2784 (2019). https://doi.org/
10.1109/CVPR.2019.00289
20
