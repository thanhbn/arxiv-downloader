# 2302.10798.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/shared-params/2302.10798.pdf
# Kích thước tệp: 1379958 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học một Mạng Con Đồng Thuận với
Chính Quy Hóa Phân Cực và Huấn Luyện Một Lượt
Xiaoying Zhi, Varun Babbar, Rundong Liu, Pheobe Sun,
Fran Silavong, Ruibo Shi, Sean Moran
JPMorgan Chase, 25 Bank Street, London, E145JP, UK.
Tác giả đóng góp: xiaoying.zhi@jpmchase.com;
varun.babbar@jpmchase.com; eric.liu@jpmchase.com;
pheobe.sun@jpmchase.com; fran.silavong@jpmchase.com;
ruibo.shi@jpmchase.com; sean.j.moran@jpmchase.com;

Tóm tắt
Chủ đề về AI xanh đã thu hút sự chú ý trong cộng đồng học sâu do xu hướng gần đây của các mô hình mạng nơ-ron ngày càng lớn và phức tạp hơn. Các giải pháp hiện có để giảm tải tính toán của việc huấn luyện tại thời điểm suy luận thường liên quan đến việc cắt tỉa các tham số mạng. Các sơ đồ cắt tỉa thường tạo ra chi phí phụ bổ sung hoặc bằng việc huấn luyện và tinh chỉnh lặp đi lặp lại cho cắt tỉa tĩnh hoặc tính toán lặp lại của một đồ thị cắt tỉa động. Chúng tôi đề xuất một chiến lược cắt tỉa tham số mới để học một mạng con nhẹ hơn nhằm giảm thiểu chi phí năng lượng trong khi duy trì hiệu suất tương đương với mạng được tham số hóa đầy đủ trên các nhiệm vụ cuối cụ thể. Sơ đồ cắt tỉa được đề xuất của chúng tôi có định hướng xanh, vì nó chỉ yêu cầu một lần huấn luyện để khám phá các mạng con tĩnh tối ưu bằng các phương pháp cắt tỉa động. Sơ đồ cắt tỉa bao gồm một mô-đun cổng nhị phân và một hàm mất mát phân cực để khám phá các mạng con với độ thưa thớt do người dùng định nghĩa. Phương pháp của chúng tôi cho phép cắt tỉa và huấn luyện đồng thời, điều này tiết kiệm năng lượng trong cả giai đoạn huấn luyện và suy luận và tránh chi phí tính toán phụ từ các mô-đun cổng tại thời điểm suy luận. Kết quả của chúng tôi trên CIFAR-10, CIFAR-100, và Tiny Imagenet cho thấy rằng sơ đồ của chúng tôi có thể loại bỏ ≈50% kết nối trong các mạng sâu với độ giảm ≤1% trong độ chính xác phân loại. So với các phương pháp cắt tỉa liên quan khác, phương pháp của chúng tôi cho thấy độ giảm thấp hơn trong độ chính xác cho việc giảm tương đương trong chi phí tính toán.
Từ khóa: Cắt Tỉa Kiến Trúc Mạng Nơ-ron, Học Máy, Thị Giác Máy Tính.
1arXiv:2302.10798v5  [cs.LG]  10 Jan 2025

--- TRANG 2 ---
1 Giới thiệu
Các mô hình lớn, thưa thớt và được tham số hóa quá mức mang lại hiệu suất tiên tiến nhất (SOTA) trên nhiều nhiệm vụ nhưng yêu cầu sức mạnh tính toán và do đó năng lượng nhiều hơn đáng kể so với các mô hình học máy thông thường [1]. Ví dụ, mô hình biến đổi thị giác (ViT-L16) với 307M tham số có thể đạt được độ chính xác 99.42% trên tập dữ liệu CIFAR-10 và 87.76% trên tập dữ liệu ImageNet [2]. Việc huấn luyện mô hình ViT-L16 yêu cầu 680 TPUv3-core-days¹ và tiêu thụ năng lượng 3672kWh, tương đương với 32.5% mức tiêu thụ năng lượng hàng năm của một hộ gia đình trung bình ở Mỹ [2–4].

Cắt tỉa mạng là một hướng nghiên cứu đầy hứa hẹn giúp đạt được các mô hình AI xanh hơn, dựa trên giả định rằng chúng ta có thể loại bỏ an toàn các tham số từ các mạng được tham số hóa quá mức, mà không làm giảm đáng kể hiệu suất mạng [5]. Có hai loại phương pháp cắt tỉa mạng phổ biến - tĩnh và động. Cắt tỉa mạng tĩnh tạo ra một mạng con thống nhất cho tất cả dữ liệu, trong khi cắt tỉa động tính toán các mạng con khác nhau cho mỗi mẫu dữ liệu. Cắt tỉa mạng tĩnh thường yêu cầu một thước đo tầm quan trọng nơ-ron được định nghĩa trước để xác định những nơ-ron đã được huấn luyện nào nên được cắt tỉa [6–10]. Việc tinh chỉnh hoặc phát triển lại thêm của mạng con được chọn thường được thực hiện sau huấn luyện, điều này có thể dẫn đến cải thiện thêm về hiệu suất [11–13]. Mặt khác, cắt tỉa động áp dụng một hàm cổng có thể học và được tham số hóa để tính toán tầm quan trọng nơ-ron một cách tức thời, dẫn đến một đồ thị tính toán khác nhau cho mỗi mẫu dữ liệu. Giai đoạn huấn luyện tối ưu hóa các hàm cổng có thể học với một mất mát thực nghiệm, và giai đoạn suy luận tính toán mạng con phù hợp thông qua lan truyền thuận qua các mô-đun cổng [14–17].

Từ quan điểm AI xanh, cả hai phương pháp cắt tỉa động và tĩnh đều không lý tưởng. Cắt tỉa động không tối ưu cho tính toán song song do các hoạt động lập chỉ mục cần thiết tại thời điểm suy luận. Hơn nữa, cắt tỉa động đưa ra chi phí phụ từ các tính toán tầm quan trọng kết nối cần thiết. Cắt tỉa tĩnh có thể giảm tài nguyên tính toán tại thời điểm suy luận, nhưng quá trình cắt tỉa-và-tinh chỉnh lặp đi lặp lại tiêu thụ nhiều tài nguyên tính toán hơn trong giai đoạn huấn luyện. Cắt tỉa một lần sau huấn luyện không tốt hơn quy trình lặp vì hiệu quả của nó phụ thuộc nhiều vào các tiên nghiệm được giả định, thiếu xác minh về tính hợp lệ của chúng trước khi huấn luyện [18].

Phương pháp cắt tỉa được đề xuất của chúng tôi tạo thành một mạng nén mà không có chi phí của các tài nguyên huấn luyện bổ sung đáng kể. Nó đạt được điều này bằng cách đồng thời tối ưu hóa cấu trúc và tham số mạng trong một lần. Chúng tôi cho rằng mô hình mới này mang lại cải thiện đáng kể cho các yêu cầu hiệu quả trong các phương pháp cắt tỉa, đặc biệt trong các môi trường hạn chế tài nguyên. Cụ thể, việc tối ưu hóa đồng thời được thực hiện với một mô-đun cổng nhị phân có thể huấn luyện nhẹ cùng với một bộ chính quy hóa phân cực. Bộ chính quy hóa phân cực tạo ra một mạng con ổn định hoạt động tốt cho tất cả các điểm dữ liệu về cuối quá trình huấn luyện. Thời gian suy luận được giảm vì mạng con tĩnh nhỏ hơn sẵn sàng sử dụng. Chúng tôi xác minh sơ đồ cắt tỉa trên hai loại cắt tỉa (lớp và kênh) trên các ResNet khác nhau [19], áp dụng cho ba tập dữ liệu với kích thước và số lớp khác nhau (CIFAR-10, CIFAR-100, và Tiny Imagenet [20, 21]). Các so sánh với các đường cơ sở cạnh tranh hiện có được trình bày.

1.1 AI Xanh và Đỏ: Nâng Cao Nhận Thức về Chi Phí Năng Lượng của AI

Schwartz và cộng sự [1] là những tác giả đầu tiên định nghĩa các khái niệm về AI xanh (AI thân thiện với môi trường) và AI đỏ (AI tiêu thụ năng lượng lớn), đề xuất rằng các mô hình AI nên được đánh giá vượt ra ngoài độ chính xác bằng cách xem xét phát thải carbon và sử dụng điện, thời gian trôi qua, số lượng tham số, và các hoạt động điểm nổi (FPOs/FLOPs). Patterson và cộng sự [22] và Dodge và cộng sự [23] đề xuất các khung công tác định lượng phát thải carbon từ việc áp dụng các mô hình AI cụ thể trên các thiết bị phổ biến khác nhau. Để giảm phát thải carbon từ việc huấn luyện mô hình, các phương pháp khác nhau đã được sử dụng thường xuyên. Ví dụ, lượng tử hóa mô hình có thể được sử dụng để giảm thời gian trôi qua và sử dụng bộ nhớ bộ xử lý [24], trong khi các phương pháp chưng cất mạng và cắt tỉa mạng có thể được sử dụng để giảm số lượng tham số và tổng FLOPs [25].

1.2 Cắt Tỉa Mạng

Cắt tỉa mạng nhằm xếp hạng tầm quan trọng của các cạnh trong một mô hình mạng nơ-ron để tìm một mạng con với các cạnh quan trọng nhất. Thường có hai phương pháp để đạt được mục tiêu này: các phương pháp luận tĩnh hoặc động. Cắt tỉa mạng tĩnh tìm một mạng con thống nhất ở cuối quá trình huấn luyện, và thường được theo sau bởi một quy trình tinh chỉnh để cải thiện thêm hiệu suất mạng con. Sơ đồ cắt tỉa này dựa vào các điểm số tầm quan trọng được tính toán của các cạnh quan tâm. Tầm quan trọng cạnh có thể được tính toán, ví dụ, bằng cường độ hoặc ảnh hưởng của một cạnh đến đầu ra cuối cùng.

Trong các mạng nơ-ron tích chập (CNN), cắt tỉa thường được thực hiện trong ba chiều: độ sâu (lớp), độ rộng (kênh), và độ phân giải (bản đồ đặc trưng) [26]. Các thí nghiệm về cắt tỉa bản đồ đặc trưng tĩnh [27] và cắt tỉa kênh [28] đã chứng minh giảm 30% trong FLOPs hoặc giảm 2.5× trong thời gian GPU với chỉ sự suy giảm hiệu suất không đáng kể hoặc thậm chí cải thiện trong một số trường hợp. Cai và cộng sự [29] mở rộng vấn đề thành cắt tỉa đa giai đoạn để làm cho phương pháp cắt tỉa có thể thích ứng với các yêu cầu kích thước khác nhau. Mục tiêu này được đạt được bằng cách huấn luyện và tinh chỉnh các mạng con với việc giảm kích thước tăng dần trong khi đảm bảo độ chính xác mô hình vẫn giữ nguyên mỗi khi yêu cầu kích thước được giảm.

Mặt khác, cắt tỉa động nhằm tìm các mạng con phụ thuộc đầu vào. Các yếu tố phụ thuộc đầu vào thường được thêm vào mạng gốc để tính toán tầm quan trọng của các cạnh quan tâm. Veit và Belongie [14] đề xuất một đồ thị suy luận thích ứng tính toán tầm quan trọng của các lớp CNN với một mô-đun cổng xác suất và có thể học trước mỗi lớp. Lin và cộng sự [30] đề xuất một khung công tác tương tự để cắt tỉa các kênh CNN bằng cách sử dụng học tăng cường để huấn luyện một cơ chế chấm điểm tầm quan trọng kênh tối ưu.

Kết hợp cả các phương pháp cắt tỉa tĩnh và động có thể đạt được lợi ích hiệp đồng trong hiệu quả tính toán. Một phương pháp kết hợp có thể hưởng lợi từ khả năng tương thích của cắt tỉa tĩnh với tính toán song song, tiết kiệm năng lượng đặc biệt trên tính toán GPU. Phương pháp này cũng có thể tận dụng khả năng của cắt tỉa động để tạo ra các mạng thích ứng với dữ liệu đầu vào. Ví dụ, Lee [31] đề xuất một phương pháp thưa thớt hóa có thể phân vi để các tham số có thể được làm về không sau tối ưu hóa dưới gradient descent ngẫu nhiên. Tuy nhiên, các mạng con không thống nhất vẫn gây ra tính toán lập chỉ mục dư thừa trong tính toán song song. Công trình của chúng tôi tập trung vào vấn đề tìm một mạng con thống nhất cho dữ liệu theo một phân phối nhất định, bằng cách sử dụng các mạng con được cắt tỉa động như các trạng thái trung gian. Công trình gần đây về việc thống nhất các mạng con đã liên quan đến việc cắt tỉa một biểu diễn đồ thị của mạng nơ-ron [32,33]

Ngoài ra, các phát hiện của chúng tôi xác nhận công trình gần đây về sự tồn tại của "vé số", tức là các mạng con được cắt tỉa có thể đạt được độ chính xác tương tự như mạng gốc [34]. Để tạo ra các mạng như vậy, Frankle và Carbin [34] phát triển sơ đồ IMP (Iterative Magnitude Pruning) bao gồm việc cắt tỉa và huấn luyện lặp đi lặp lại qua nhiều vòng cho đến khi hội tụ. Điều này khác với phương pháp được đề xuất của chúng tôi, thực hiện huấn luyện và cắt tỉa đồng thời trong một phiên huấn luyện và do đó rẻ hơn về mặt tính toán để huấn luyện.

1.3 Ngẫu Nhiên Rời Rạc và Ước Lượng Gradient

Để có được một cấu trúc mạng con ổn định thông qua tối ưu hóa dựa trên gradient tức là trạng thái kích hoạt nhị phân cho mỗi kết nối, một mô-đun cổng cần thiết với các biến tiềm ẩn có thể phân biệt và rời rạc. Các biến rời rạc thường yêu cầu một sự nới lỏng hoặc ước lượng do sự không tương thích của hàm rời rạc hóa với lan truyền ngược (ví dụ: gradient bằng không ở khắp mọi nơi).

Một phương pháp ước lượng nổi tiếng là bộ ước lượng Gumbel-Softmax (GS) [35]. Gumbel-Softmax là một phân phối xác suất liên tục có thể được điều chỉnh để xấp xỉ một phân phối categorical rời rạc. Các gradient đối với phân phối đầu ra categorical được định nghĩa rõ ràng cho phân phối GS. Kỹ thuật này thường được áp dụng cho các mô hình chuỗi sinh sản yêu cầu lấy mẫu dưới các phân phối đa thức [36–38].

Một phương pháp đơn giản hơn nhưng vẫn hiệu quả là bộ ước lượng straight-through (STE) [39], nhị phân hóa đầu ra ngẫu nhiên dựa trên một ngưỡng trong lượt đi thuận, và theo kinh nghiệm sao chép gradient của lớp tiếp theo cho bộ ước lượng. Các thí nghiệm cho thấy rằng các mạng nơ-ron được điều khiển bởi STE cho tỷ lệ lỗi thấp nhất trong số các cổng có thể phân biệt khác (đa thức và không đa thức) [39]. Chúng tôi mô tả STE chi tiết hơn trong Phần 3.2.

1.4 Bộ Chính Quy Hóa Thưa Thớt

Cho mục đích cắt tỉa tham số, một bộ chính quy hóa thưa thớt thường được sử dụng để kiểm soát tỷ lệ cắt tỉa trong quá trình huấn luyện. Các bộ chính quy hóa l1 và l2 là hai loại phổ biến nhất. Tuy nhiên, các hàm chính quy hóa tiêu chuẩn có thể dẫn đến việc cắt tỉa không cần thiết và ước lượng sai về tầm quan trọng kết nối mạng. Các bộ chính quy hóa nhạy cảm hơn với cấu trúc mạng bao gồm chính quy hóa thưa thớt có cấu trúc l2,0 và l2,1 [40], được nhóm trên các mẫu hoặc trên các bản đồ đặc trưng [41] v.v.

Srinivas và Srinivas và Babu [42] đề xuất một bộ chính quy hóa nhị phân hóa khuyến khích mỗi kết nối mạng tiếp cận 1 hoặc 0 cho tất cả các mẫu. Cơ chế nhị phân hóa cũng có thể được mở rộng cho các tỷ lệ kích hoạt liên tục. Ví dụ, Zhuang và cộng sự [43] tích hợp một bộ chính quy hóa phân cực vào việc cắt tỉa mạng để buộc việc ngừng kích hoạt các nơ-ron. Các mạng được cắt tỉa dưới thiết lập này đạt được độ chính xác cao nhất ngay cả ở tỷ lệ cắt tỉa cao so với các sơ đồ cắt tỉa khác.

2 Thiết Lập Vấn Đề: Học Tham Số và Kiến Trúc Kết Hợp

Chúng tôi ký hiệu một mạng nơ-ron với kết nối đầy đủ dưới dạng đồ thị là Φ := (V, E), trong đó V là một tập hợp các nút, E là tập hợp các cạnh E:={e(x,y),∀x, y∈V}. Một mạng con với kết nối một phần do đó có thể được biểu diễn là Φ′= (V, E′) trong đó E′⊆E. Chúng tôi cũng ký hiệu việc biến đổi của một mạng là fθ(·)≡f(·;θ), trong đó θ ký hiệu tất cả các tham số trong một mạng. Mỗi cạnh e∈E được liên kết với một trọng số θe. Đối với mạng đầy đủ θ=θΦ, và đối với mạng con θ=θΦ′. Một mạng con có thể được biểu thả dưới dạng một mạng đầy đủ sử dụng một ma trận kích hoạt We với một số yếu tố được làm về không, tức là:

θΦ′=W⊤eθΦ, (1)

trong đó we,c∈ {0,1} cho mọi mục trong ma trận kích hoạt cạnh We là nhị phân, và θΦ′ được tính từ một tích Hadamard. Trong việc cắt tỉa mạng, chúng tôi nhằm tìm một mạng con Φ′ và các tham số mạng tối ưu θ∗Φ′ đồng thời. Chúng tôi ước lượng giải pháp tối ưu θ∗Φ′ với ˆθ∗Φ′ bằng cách giảm thiểu mất mát thực nghiệm. Chúng tôi tiếp tục sử dụng các thiết lập trong Eq.1 để cải tổ mục tiêu dưới đây: tức là

min θΦ′,Φ′L(f(x;θΦ′),y).
min θΦ,WeL(f(x;W⊤eθΦ),y). (2)

3 Phương Pháp Luận

Trong thực tế, ma trận kích hoạt cạnh We không được học như một toàn thể và mỗi mục trong ma trận không độc lập. Khi huấn luyện một mạng tuần tự, việc kích hoạt các kết nối trước có thể ảnh hưởng đến đầu ra của các kết nối sau, và do đó cũng ảnh hưởng đến lan truyền ngược gradient. Một We nhị phân/categorical ngây thơ sẽ ngăn các gradient lan truyền ngược, vì một hàm với giá trị không đổi có gradient bằng không. Do đó, một bộ ước lượng gradient được cần thiết như yếu tố cổng cốt lõi của mỗi kết nối. Chúng tôi chọn bộ ước lượng straight-through (STE), như được giới thiệu trong Phần 3.2, như yếu tố cốt lõi này.

3.1 Kiến Trúc Mạng

Hình 1 minh họa thiết kế cho việc tích hợp mô-đun cổng vào một ResNet. Sơ đồ cắt tỉa của chúng tôi có quy trình làm việc hơi khác nhau cho giai đoạn huấn luyện và giai đoạn kiểm tra. Trong huấn luyện, các mô-đun cổng với các lớp dày đặc có thể học được huấn luyện như một phần của mạng. Tại suy luận (cho xác thực hoặc kiểm tra), We kết quả được tải, điều này quyết định tập con của các tham số được chọn – chỉ các kết nối với we,c khác không sẽ được tải cho các tham số và được bao gồm trong lượt đi thuận.

Việc lựa chọn ResNet như mạng cơ sở dựa trên sự cần thiết của các kết nối dư để tránh khả năng chấm dứt đường tính toán ở giữa mạng do một lớp đầy đủ bị ngừng kích hoạt. Đối với ResNet, chúng tôi tập trung vào việc cắt tỉa lớp và kênh (bản đồ đặc trưng) lấy CNN làm trung tâm. Tuy nhiên, chúng tôi cũng lập luận rằng phương pháp luận này có tiềm năng được áp dụng cho bất kỳ loại kết nối nào, ngay cả trong việc cắt tỉa ít có cấu trúc hơn (ví dụ: các kết nối kernel-đến-kernel được chọn giữa các lớp tích chập) và để lại bằng chứng thực nghiệm như một hướng cho công việc tương lai. Trong khi phương pháp của chúng tôi có điểm tương đồng với các phương pháp dựa trên dropout trong ResNets, những phương pháp này liên quan đến việc cắt tỉa các kết nối cụ thể giữa các nút. Từ quan điểm kiến trúc, điều này không nhất thiết làm giảm số lượng FLOPs vì không có sự giảm số lượng phép nhân ma trận cần thiết. Ngược lại, việc loại bỏ toàn bộ kênh / lớp có hiệu ứng mong muốn này.

3.2 Bộ Ước Lượng Straight-through

Chúng tôi chọn bộ ước lượng straight-through (STE) làm đầu nhị phân cho mô-đun cổng. Đường đi thuận của STE là một hàm ngưỡng cứng:

STE (x) = (1, nếu x > 0; 0, nếu x ≤ 0). (3)

Gradient ngược phản ánh tại sao bộ ước lượng này được biết đến như "straight-through":

∂L/∂x = ∂L/∂STE (x) · ∂STE (x)/∂x = (∂L/∂STE (x), nếu |x| ≤ 1; 0, nếu |x| > 1), (4)

trong đó trạng thái không nhạy cảm được kích hoạt khi |x| > 1. Điều này là để tránh một tình huống có thể xảy ra trong đó một gradient lớn làm cho giá trị đầu ra STE ở lại 1 hoặc 0 vĩnh viễn.

Một ưu điểm rõ ràng của STE như đầu cổng là nó tạo thành một mô-đun nhẹ cho cả lan truyền thuận và ngược. Trong lượt đi thuận, không cần tính toán nào khác ngoài kiểm tra dấu. Trong lượt đi ngược không cần tính toán. Việc ước lượng gradient, thường được xem như một xấp xỉ thô của gradient thực dưới nhiễu, đã được chứng minh có tương quan tích cực với gradient quần thể, và do đó gradient descent giúp giảm thiểu mất mát thực nghiệm [44].

3.3 Chính Quy Hóa Phân Cực cho Kích Hoạt Thống Nhất

Trong quá trình huấn luyện theo kiểu cắt tỉa động, ma trận We(x) có thể không giống nhau cho tất cả x∈X. Để khuyến khích một ma trận kích hoạt cạnh thống nhất mà We(x) = We(x′), ∀x, x′∈X, chúng tôi giới thiệu một bộ chính quy hóa phân cực Rpolar({We(x)|x∈X}). Hàm mất mát hoàn chỉnh là:

L(f(x),y) = Ltask(f(x),y) + λRpolar(We(x)) (5)

trong đó Ltask là mất mát nhiệm vụ, ví dụ: mất mát cross-entropy cho các nhiệm vụ phân loại và sai số bình phương trung bình cho các nhiệm vụ hồi quy, và λ là hệ số tỷ lệ cho bộ chính quy hóa phân cực.

Dạng tổng quát của Rpolar(We(x)) ở dạng một parabol đảo ngược. Giả sử We(x)∈R|C| được làm phẳng cho tất cả các kết nối được bao phủ c∈C:

Rpolar(We(x)) := 1/|C|(1−W̄e(x))⊤W̄e(x), (6)

trong đó W̄e(x) = 1/|X|∑x∈XWe(x) là ma trận kích hoạt cạnh trung bình qua tất cả các mẫu dữ liệu. Cho phạm vi của W̄e,c∈[0,1], dạng parabol đảo ngược này đảm bảo rằng một tối ưu tương đương có thể được đạt đến khi W̄e,c đạt đến một trong hai biên của phạm vi.

Cụ thể, trong tình huống cắt tỉa lớp ResNet của chúng tôi, số hạng chính quy hóa được viết là:

Rpolar := 1/|L|∑ly∈L(1−ḡly)ḡly, (7)

trong đó ḡly = 1/|X|∑x∈Xgly(x)∈[0,1] là trung bình của các đầu ra mô-đun cổng qua tất cả các mẫu đầu vào của lớp. Tương tự, trong tình huống cắt tỉa kênh ResNet của chúng tôi, số hạng chính quy hóa được viết là:

Rpolar := 1/|L|∑ly∈L 1/|C|∑ch∈C(1−ḡly,ch)ḡly,ch (8)

trong đó ḡly,ch = 1/|X|∑x∈Xgly,ch(x)∈[0,1] là trung bình của các đầu ra mô-đun cổng qua tất cả các mẫu đầu vào của kênh ch∈C trong lớp ly∈L.

4 Thí Nghiệm

4.1 Đặc Tả Tập Dữ Liệu và Kiến Trúc

Chúng tôi kiểm tra hiệu quả của phương pháp được đề xuất của chúng tôi trên kiến trúc ResNet [19] trên các tập dữ liệu CIFAR-10, CIFAR-100, và Tiny Imagenet, với lần lượt 10, 100, và 200 lớp. Chúng tôi chọn ba tập dữ liệu để đánh giá công việc của chúng tôi vì chúng có kết quả kiểm tra được công nhận rộng rãi trên hầu hết các biến thể của ResNets. Kiểm tra trên cả hai tập dữ liệu cho thấy hiệu quả của phương pháp của chúng tôi dưới cả phân phối dữ liệu đơn giản và phức tạp.

Các thí nghiệm của chúng tôi trên các tập dữ liệu CIFAR được tiến hành trên một GPU NVIDIA T4 với bộ nhớ 16GB. Kích thước batch được đặt thành 256 cho CIFAR-10 và 64 cho CIFAR-100. Huấn luyện của chúng tôi được thực hiện dưới một tỷ lệ học giảm theo giai đoạn cho 350 epoch (mặc dù hội tụ thường có thể đạt được trước 250 epoch). Tỷ lệ học ban đầu cho cả hai tập dữ liệu là 0.1, và ở mỗi giai đoạn tiếp theo sẽ giảm xuống 10%. Trên CIFAR-10, tỷ lệ học được điều chỉnh tại các epoch 60, 120, và 160. Trên CIFAR-100, tỷ lệ học được điều chỉnh tại các epoch 125, 190, và 250. Chúng tôi chọn stochastic gradient descent (SGD) làm bộ tối ưu hóa, với momentum 0.9 và weight decay 5×10^4. Các mạng và quy trình huấn luyện được triển khai trong PyTorch. Khi có sự ngẫu nhiên, chúng tôi đặt seed ngẫu nhiên thành 1.

Các thí nghiệm của chúng tôi trên tập dữ liệu Tiny Imagenet được tiến hành trên bốn GPU NVIDIA A10 với mỗi GPU có bộ nhớ 24GB. Huấn luyện của chúng tôi được thực hiện với bộ tối ưu hóa SGD dưới một tỷ lệ học giảm theo giai đoạn cho 1200 epoch. Tỷ lệ học được đặt thành 0.2 ban đầu, sau đó giảm xuống 10% của giá trị trước đó tại 600 và 900 epoch. Kích thước batch của chúng tôi là 100. Ngoài ra, chúng tôi áp dụng Puzzle mix [45] cho cả huấn luyện mạng cơ sở và huấn luyện mạng được cắt tỉa để cải thiện độ chính xác phân loại trên Tiny Imagenet.

Chúng tôi áp dụng mạng cho nhiệm vụ phân loại hình ảnh. Các mạng được cắt tỉa được đánh giá bằng độ chính xác top-1 và FLOPs (các hoạt động điểm nổi). Số lượng FLOPs được xấp xỉ bởi gói fvcore².

4.2 So Sánh với Các Đường Cơ Sở Cắt Tỉa Hiện Có

Trước tiên chúng tôi so sánh hiệu suất của phương pháp chúng tôi với các đường cơ sở ngây thơ và các phương pháp khác trong tài liệu liên quan đến của chúng tôi. Chúng tôi tuân theo các đặc tả huấn luyện được đặt ra trong Phần 4.1. Cho các thí nghiệm này, chúng tôi chọn kiến trúc cắt tỉa hiệu suất tốt nhất của chúng tôi dựa trên kết quả thực nghiệm trên tập kiểm tra được hiển thị sau trong phần này, nơi chúng tôi sử dụng sơ đồ cắt tỉa lớp, Gumbell softmax (phần 4.4) và đặt λpolar = ↑ (như trong Bảng 6). Hơn nữa, chúng tôi chọn ResNet56 như kiến trúc cơ sở cụ thể để cho phép so sánh dễ dàng với các sơ đồ cắt tỉa khác được thấy trong tài liệu.

Ngoài hàm mất mát gốc của chúng tôi, bây giờ chúng tôi muốn kiểm soát tỷ lệ cắt tỉa của phương pháp chúng tôi. Điều này được đạt được bằng cách thêm một số hạng chính quy hóa thưa thớt bổ sung trong hàm mất mát cung cấp một tín hiệu cho bộ chính quy hóa phân cực để giữ ít cổng mở hơn. Hàm mất mát kết quả là:

L(f(x),y) = Ltask(f(x),y) + λpolarRpolar(We(x)) + λactRact(We(x)) (9)

trong đó:

Ract(We(x)) = 1/|L|∑ly∈Lḡly (10)

là kích hoạt lớp trung bình tổng thể, trong đó trung bình được lấy qua các lớp ly∈L và đầu vào x∈X. Chúng tôi thay đổi λact∈[0,1] để thay đổi tỷ lệ cắt tỉa (tức là % tham số được cắt tỉa). Trong bối cảnh công việc của chúng tôi, điều này được định nghĩa là:

Ex∈X∑l #Tham số trong lớp l: Cổng gl(x) = 0 / #Tổng Tham Số × 100 (11)

trong đó kỳ vọng qua các đầu vào tính đến thực tế rằng một số sơ đồ cắt tỉa có thể không đạt được việc thống nhất mạng con hoàn hảo.

Chúng tôi xem xét các đường cơ sở ngây thơ sau, do hiệu suất thực nghiệm của chúng trong tài liệu:

• ResNet-56 Dropout Ngây Thơ: Một bộ phân loại tiêu chuẩn nhưng với k∈{20%, 30%, 50%, 60%, 80%} tham số được cắt tỉa ngẫu nhiên trong quá trình kiểm tra.

• ResNet-56 Cắt Tỉa Lớp Ngây Thơ: Cùng bộ phân loại nhưng với k∈{20%, 30%, 50%, 60%, 80%} kích hoạt lớp được đặt ngẫu nhiên thành 0 trong quá trình kiểm tra.

Hình 2: So sánh phương pháp của chúng tôi với một số đường cơ sở ngây thơ trên CIFAR-10 với ResNet-56. Trái: Tỷ lệ cắt tỉa trung bình tại suy luận so với độ chính xác Top-1. Phải: % giảm FLOPs tại suy luận so với độ chính xác Top-1. Phương pháp dropout ngây thơ không giảm FLOPs vì nó vẫn liên quan đến tính toán qua các nút "đã bị drop" - do đó bị bỏ qua.

Một so sánh trực quan với các đường cơ sở ngây thơ được hiển thị trong Hình 2, chứng minh khả năng của phương pháp cắt tỉa của chúng tôi trong việc duy trì hiệu suất mạng với tỷ lệ cắt tỉa cao trên tập kiểm tra. Đánh giá thêm phương pháp của chúng tôi, chúng tôi xem xét các phương pháp cũng theo ý tưởng cắt tỉa và học đồng thời, như được liệt kê trong Bảng 1. Hình 3 cho thấy hiệu suất của sơ đồ của chúng tôi so với các phương pháp được tìm thấy trong tài liệu, tất cả đều sử dụng ResNet-56 như mạng cơ sở. Chúng tôi lưu ý rằng sơ đồ của chúng tôi cung cấp kết quả cạnh tranh không chỉ về độ chính xác tuyệt đối, mà còn về sự giảm độ chính xác do cắt tỉa.

4.3 Điều Tra Cắt Tỉa Kênh và Lớp

ResNets chứa các lớp dư gồm hai lớp tích chập và một kết nối dư bỏ qua.

Trong cắt tỉa kênh, chúng tôi thí nghiệm trên hai thiết kế lớp và ba vị trí của mô-đun cổng. Cụ thể, chúng tôi thí nghiệm với các kiến trúc mô-đun cổng gồm một và hai lớp dày đặc, trong đó Bảng 2 hiển thị thiết kế chi tiết. Đối với các vị trí mô-đun cổng, chúng tôi thí nghiệm đặt mô-đun cổng trước lớp tích chập đầu tiên, giữa hai lớp tích chập, và ngay sau lớp tích chập thứ hai. Hình 5 trực quan hóa các quyết định nói trên. Bảng 3 hiển thị kết quả cắt tỉa trên CIFAR-10 dưới các kiến trúc và vị trí mô-đun cổng khác nhau. Đối với các kiến trúc mô-đun cổng (nhớ lại Bảng 2), kết quả cho thấy rằng trong khi tất cả các thiết kế đạt được tỷ lệ cắt tỉa kênh tương tự, thiết kế với hai lớp dày đặc và được đặt ở cuối mỗi lớp dư (2FC-after) đạt được độ chính xác phân loại tốt nhất cao hơn đáng kể so với hầu hết các thiết kế khác. Tuy nhiên, thiết kế với 1 lớp dày đặc được đặt giữa hai lớp tích chập (1FC-middle) cũng đạt được độ chính xác tương tự. Từ đây, chúng tôi sử dụng đường cơ sở cắt tỉa kênh của chúng tôi là thiết kế 2FC-after, theo Bảng 3, vì nó cho thấy hiệu suất tốt nhất về độ chính xác phân loại và tỷ lệ cắt tỉa kênh trên tập kiểm tra.

Trong cắt tỉa lớp, các quyết định thiết kế của chúng tôi được đơn giản hóa. Chúng tôi thêm mô-đun cổng trước hai lớp tích chập, như được thấy trong Hình 4, để quyết định liệu lớp có được tính toán hay không. Bảng 2 hiển thị thiết kế chi tiết của mô-đun cổng.

Bây giờ chúng tôi tiến hành so sánh các sơ đồ cắt tỉa kênh và lớp trên các tập dữ liệu CIFAR-10, CIFAR-100, và Tiny Imagenet, như được tóm tắt trong Bảng 4. Chúng tôi quan sát thấy rằng sơ đồ cắt tỉa lớp có thể tiết kiệm ít nhất 14% tính toán (FLOPs) trong khi hy sinh độ chính xác dưới 2.5%. Dưới sơ đồ cắt tỉa kênh, chúng tôi có thể tiết kiệm ít nhất 22% tính toán (FLOPs) trong khi hy sinh độ chính xác dưới 3%.

Nói chung, chúng tôi lưu ý rằng các mô hình được cắt tỉa lớp hoạt động tốt hơn các mô hình được cắt tỉa kênh (tức là có sự giảm độ chính xác thấp hơn) cho tất cả ba tập dữ liệu, ngay cả khi sự khác biệt tương đối trong FLOPs được tính đến. Chúng tôi tin rằng điều này là do dưới thiết kế của ResNet, các bản đồ đặc trưng trung gian trong mỗi lớp dư đủ thông tin-compact, và bất kỳ việc loại bỏ nào trên các bản đồ đặc trưng có thể dẫn đến mất thông tin. Tỷ lệ cắt tỉa của cắt tỉa kênh là tích cực, với gần 50% giảm FLOPs.

4.4 Nghiên Cứu Khử Thành Phần cho Ước Lượng Gradient

Chúng tôi kiểm tra tiện ích cá nhân của hai mô-đun chính, STE và bộ chính quy hóa phân cực, thông qua các nghiên cứu khử thành phần. Để kiểm tra tiện ích của STE, chúng tôi thay thế STE bằng lấy mẫu từ phân phối Bernoulli và với Gumbel-softmax. Khi lấy mẫu từ phân phối Bernoulli, chúng tôi thiết lập một ngưỡng bằng trung bình của các đầu ra mô-đun cổng ngay sau lớp dày đặc cuối cùng (tức là ngay trước STE gốc). Nếu đầu ra lớn hơn trung bình, chúng tôi giữ lớp; nếu không chúng tôi cắt tỉa lớp.

Bảng 5 hiển thị kết quả từ ba hàm cổng, được thí nghiệm trên CIFAR-10. Chúng tôi quan sát thấy rằng ngoài STE, không có hàm đầu cổng nào khác có thể tạo ra một mạng con hoàn toàn thống nhất và ổn định. STE có tiện ích ở đây về mặt ổn định hóa sự tiến hóa của các mạng con động và giữ lại hiệu suất dự kiến. Tuy nhiên, chúng tôi lưu ý rằng Gumbel-softmax có tiềm năng đạt được hiệu suất nhiệm vụ tốt hơn trong khi giữ một tập hợp các mạng con động nhẹ được chỉ ra bởi mức độ thấp của Rpolar. Chúng tôi để lại cho công việc tương lai một nghiên cứu về cách một sự thống nhất phù hợp của các mạng con động kết quả từ Gumbel-softmax có thể được đạt được để cải thiện thêm hiệu suất so với STE cho nhiệm vụ này.

Để hiểu sự đánh đổi giữa tính tích cực cắt tỉa, giữ lại độ chính xác, và tiết kiệm tính toán, chúng tôi thí nghiệm trên một loạt các mức độ tích cực cắt tỉa, được thực hiện bằng cách điều chỉnh siêu tham số λpolar. Chúng tôi thí nghiệm trên các giá trị λpolar thay đổi từ 0 đến 3. Chúng tôi cũng kiểm tra hiệu ứng của việc tăng dần trọng số bộ chính quy hóa λpolar trong một phiên huấn luyện để xác minh liệu một mạng được huấn luyện một phần có ảnh hưởng đến kết quả cắt tỉa hay không. Hình 6 hiển thị sự tiến hóa cắt tỉa lớp dưới các thiết lập λpolar khác nhau và Bảng 6 hiển thị hiệu suất của các mạng con kết quả.

Các hình tiến hóa cắt tỉa lớp cho thấy rằng một mức độ tích cực cắt tỉa cao hơn (một λpolar cao hơn) tăng tốc hội tụ đến một mạng con thống nhất hơn một mức độ thấp hơn. Mặt khác, Bảng 6 cho thấy rằng một tỷ lệ mở cao hơn không nhất thiết mang lại nhiều tiết kiệm tính toán và giữ lại độ chính xác hơn. Tăng tính tích cực cắt tỉa (giới thiệu số hạng phạt) ở giai đoạn sau trong huấn luyện, tuy nhiên, cho thấy độ chính xác cao hơn các số hạng phạt không đổi khác, với tiết kiệm tính toán cao hơn các thí nghiệm khác.

5 Thảo Luận

5.1 Độ Phức Tạp Tính Toán

Tương tự như bất kỳ mô-đun cổng phụ nào, mô-đun cổng được giới thiệu từ phương pháp của chúng tôi thêm một chi phí phụ trên độ phức tạp tính toán tổng thể trong quá trình huấn luyện. Độ phức tạp thời gian của STE của một đầu vào đơn cho đường đi thuận là O(1) theo Eq. 3. Đối với đường đi ngược, theo Eq. 4, STE trên mỗi chiều chỉ mang lại các tính toán O(1) phụ so với đường đi ngược gốc.

Lấy thiết lập trong Bảng 2 (cột trái) làm ví dụ, một mô-đun cổng của K lớp kết nối đầy đủ, d chiều mỗi lớp, và |C| kênh làm đầu vào giới thiệu một chi phí phụ O(dK|C|).

Hợp nhất mô-đun cổng nói trên vào mạng trong Hình 4, cho một mạng của N lớp và |C| kênh mỗi lớp, các mô-đun cổng được chèn vào giới thiệu một chi phí phụ O(dNK|C|).

Trong thí nghiệm của chúng tôi nơi K = 1 hoặc 2, chi phí phụ cuối cùng được thu nhỏ thành O(dN|C|), trong đó chỉ có d phụ thuộc vào thiết kế của mô-đun cổng. Một giá trị d nhỏ trong thực tế hữu ích trong việc kiểm soát chi phí tính toán tổng thể.

Phân tích chi phí phụ ở trên chỉ áp dụng cho việc huấn luyện mạng. Sau khi huấn luyện, các tham số mạng được tối ưu hóa bao gồm đầu ra của mô-đun cổng, và một lớp được kích hoạt hoặc ngừng kích hoạt tại suy luận cho các đầu vào tùy ý. Do đó chi phí phụ được loại trừ tại suy luận. Thí nghiệm của chúng tôi (Bảng 6) cho thấy rằng mô hình được cắt tỉa có thể chỉ sử dụng 55.56% nơ-ron để đạt được chỉ 0.5% giảm hiệu suất.

6 Kết Luận

Chúng tôi đề xuất một sơ đồ cắt tỉa mạng duy trì hiệu suất trong khi hiệu quả tính toán hơn. Thông qua tối ưu hóa tham số và cấu trúc đồng thời, sơ đồ cắt tỉa của chúng tôi tìm một mạng con ổn định với hiệu suất tương tự trên cùng nhiệm vụ cuối như mạng đầy đủ (không được cắt tỉa). Sơ đồ bao gồm một mô-đun cổng nhị phân có thể phân biệt, nhẹ và các bộ chính quy hóa để thực thi tính bất biến dữ liệu của các mạng con được cắt tỉa. Các thí nghiệm về cắt tỉa lớp và kênh cho thấy kết quả cạnh tranh với (hoặc thực sự tốt hơn) các phương pháp tương tự trong tài liệu. Với việc tinh chỉnh các mạng con được khám phá của chúng tôi, chúng tôi dự đoán cải thiện hiệu suất thêm - tuy nhiên, điều này nằm ngoài phạm vi công việc của chúng tôi, nhằm nhấn mạnh tối đa lợi ích hiệu suất trong ngân sách tính toán hạn chế. Chúng tôi mong muốn kiểm tra khả năng áp dụng của sơ đồ cắt tỉa của chúng tôi cho các mạng cơ sở, tập dữ liệu và nhiệm vụ khác, hy vọng rằng các kết quả khuyến khích của chúng tôi sẽ tạo điều kiện cho việc chuyển đổi từ AI đỏ sang các mô hình học sâu hiệu quả năng lượng và xanh.

7 Tuyên Bố Miễn Trách Nhiệm

Bài báo này được chuẩn bị cho mục đích thông tin bởi nhóm Đổi Mới Ứng Dụng AI của JPMorgan Chase & Co. Bài báo này không phải là sản phẩm của Bộ phận Nghiên cứu của JPMorgan Chase & Co. hoặc các chi nhánh của nó. Không JPMorgan Chase & Co. hay bất kỳ chi nhánh nào của nó đưa ra bất kỳ tuyên bố rõ ràng hoặc ngụ ý nào và không ai trong số họ chấp nhận bất kỳ trách nhiệm nào liên quan đến bài báo này, bao gồm, không giới hạn, về tính đầy đủ, chính xác, hoặc độ tin cậy của thông tin chứa trong đó và các hiệu ứng pháp lý, tuân thủ, thuế, hoặc kế toán tiềm năng. Tài liệu này không được dự định như nghiên cứu đầu tư hoặc tư vấn đầu tư, hoặc như một khuyến nghị, đề nghị, hoặc lời mời gọi mua hoặc bán bất kỳ chứng khoán, công cụ tài chính, sản phẩm tài chính hoặc dịch vụ nào, hoặc để được sử dụng bằng bất kỳ cách nào để đánh giá giá trị của việc tham gia vào bất kỳ giao dịch nào. Công việc được mô tả là một nguyên mẫu và không phải là một hệ thống triển khai sản xuất.

Tài Liệu Tham Khảo

[1] Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O.: Green ai. Commun. ACM 63(12), 54–63 (2020) https://doi.org/10.1145/3381831

[2] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv (2020). https://doi.org/10.48550/ARXIV.2010.11929 . https://arxiv.org/abs/2010.11929

[3] Jouppi, N.P., Yoon, D.H., Kurian, G., Li, S., Patil, N., Laudon, J., Young, C., Patterson, D.: A domain-specific supercomputer for training deep neural networks. Commun. ACM 63(7), 67–78 (2020) https://doi.org/10.1145/3360307

[4] EIA US: 2020 Average Monthly Bill- Residential. https://www.eia.gov/electricity/sales revenue price/pdf/table5 a.pdf. Truy cập: 2022-07-19 (2021)

[5] Denil, M., Shakibi, B., Dinh, L., Ranzato, M., Freitas, N.: Predicting Parameters in Deep Learning. arXiv (2013). https://doi.org/10.48550/ARXIV.1306.0543 . https://arxiv.org/abs/1306.0543

[6] Shafiee, M.S., Shafiee, M.J., Wong, A.: Dynamic Representations Toward Efficient Inference on Deep Neural Networks by Decision Gates. arXiv (2018). https://doi.org/10.48550/ARXIV.1811.01476 . https://arxiv.org/abs/1811.01476

[7] Luo, J., Zhang, H., Zhou, H., Xie, C., Wu, J., Lin, W.: Thinet: Pruning cnn filters for a thinner net. IEEE Transactions on Pattern Analysis 'I&' Machine Intelligence 41(10), 2525–2538 (2019) https://doi.org/10.1109/TPAMI.2018.2858232

[8] Cheong, R.: transformers.zip: Compressing transformers with pruning and quantization. (2019)

[9] Zhang, M.S., Stadie, B.: One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation. arXiv (2019). https://doi.org/10.48550/ARXIV.1912.00120 . https://arxiv.org/abs/1912.00120

[10] Lin, Z., Liu, J., Yang, Z., Hua, N., Roth, D.: Pruning redundant mappings in transformer models via spectral-normalized identity prior. In: Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 719–730. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.findings-emnlp.64 . https://aclanthology.org/2020.findings-emnlp.64

[11] Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both Weights and Connections for Efficient Neural Networks. arXiv (2015). https://doi.org/10.48550/ARXIV.1506.02626 . https://arxiv.org/abs/1506.02626

[12] Zhu, M., Tang, Y., Han, K.: Vision Transformer Pruning. arXiv (2021). https://doi.org/10.48550/ARXIV.2104.08500 . https://arxiv.org/abs/2104.08500

[13] Hou, Z., Qin, M., Sun, F., Ma, X., Yuan, K., Xu, Y., Chen, Y.-K., Jin, R., Xie, Y., Kung, S.-Y.: CHEX: CHannel EXploration for CNN Model Compression. arXiv (2022). https://doi.org/10.48550/ARXIV.2203.15794 . https://arxiv.org/abs/2203.15794

[14] Veit, A., Belongie, S.: Convolutional Networks with Adaptive Inference Graphs. arXiv (2017). https://doi.org/10.48550/ARXIV.1711.11503 . https://arxiv.org/abs/1711.11503

[15] Gao, X., Zhao, Y., Dudziak, L., Mullins, R., Xu, C.-z.: Dynamic Channel Pruning: Feature Boosting and Suppression. arXiv (2018). https://doi.org/10.48550/ARXIV.1810.05331 . https://arxiv.org/abs/1810.05331

[16] Bejnordi, B.E., Blankevoort, T., Welling, M.: Batch-Shaping for Learning Conditional Channel Gated Networks. arXiv (2019). https://doi.org/10.48550/ARXIV.1907.06627 . https://arxiv.org/abs/1907.06627

[17] Yin, H., Vahdat, A., Alvarez, J., Mallya, A., Kautz, J., Molchanov, P.: A-ViT: Adaptive Tokens for Efficient Vision Transformer. arXiv (2021). https://doi.org/10.48550/ARXIV.2112.07658 . https://arxiv.org/abs/2112.07658

[18] Lee, N., Ajanthan, T., Torr, P.H.S.: SNIP: Single-shot Network Pruning based on Connection Sensitivity. arXiv (2018). https://doi.org/10.48550/ARXIV.1810.02340 . https://arxiv.org/abs/1810.02340

[19] He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition. arXiv (2015). https://doi.org/10.48550/ARXIV.1512.03385 . https://arxiv.org/abs/1512.03385

[20] Krizhevsky, A.: Learning multiple layers of features from tiny images. (2009). https://www.cs.toronto.edu/ \texttildelowkriz/learning-features-2009-TR.pdf

[21] Le, Y., Yang, X.S.: Tiny imagenet visual recognition challenge. (2015). https://api.semanticscholar.org/CorpusID:16664790

[22] Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: Carbon Emissions and Large Neural Network Training. arXiv (2021). https://doi.org/10.48550/ARXIV.2104.10350 . https://arxiv.org/abs/2104.10350

[23] Dodge, J., Prewitt, T., Combes, R.T.D., Odmark, E., Schwartz, R., Strubell, E., Luccioni, A.S., Smith, N.A., DeCario, N., Buchanan, W.: Measuring the Carbon Intensity of AI in Cloud Instances. arXiv (2022). https://doi.org/10.48550/ARXIV.2206.05229 . https://arxiv.org/abs/2206.05229

[24] Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: A Survey of Quantization Methods for Efficient Neural Network Inference. arXiv (2021). https://doi.org/10.48550/ARXIV.2103.13630 . https://arxiv.org/abs/2103.13630

[25] Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowledge in a Neural Network. arXiv (2015). https://doi.org/10.48550/ARXIV.1503.02531 . https://arxiv.org/abs/1503.02531

[26] Wang, W., Chen, M., Zhao, S., Chen, L., Hu, J., Liu, H., Cai, D., He, X., Liu, W.: Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework. arXiv (2020). https://doi.org/10.48550/ARXIV.2010.04879 . https://arxiv.org/abs/2010.04879

[27] Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning Filters for Efficient ConvNets. arXiv (2016). https://doi.org/10.48550/ARXIV.1608.08710 . https://arxiv.org/abs/1608.08710

[28] He, Y., Zhang, X., Sun, J.: Channel Pruning for Accelerating Very Deep Neural Networks. arXiv (2017). https://doi.org/10.48550/ARXIV.1707.06168 . https://arxiv.org/abs/1707.06168

[29] Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Network and Specialize it for Efficient Deployment. arXiv (2019). https://doi.org/10.48550/ARXIV.1908.09791 . https://arxiv.org/abs/1908.09791

[30] Lin, J., Rao, Y., Lu, J., Zhou, J.: Runtime neural pruning. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., ??? (2017). https://proceedings.neurips.cc/paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf

[31] Lee, Y.: Differentiable Sparsification for Deep Neural Networks. arXiv (2019). https://doi.org/10.48550/ARXIV.1910.03201 . https://arxiv.org/abs/1910.03201

[32] Wortsman, M., Farhadi, A., Rastegari, M.: Discovering Neural Wirings. arXiv (2019). https://doi.org/10.48550/ARXIV.1906.00586 . https://arxiv.org/abs/1906.00586

[33] Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., Rastegari, M.: What's Hidden in a Randomly Weighted Neural Network? arXiv (2019). https://doi.org/10.48550/ARXIV.1911.13299 . https://arxiv.org/abs/1911.13299

[34] Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable neural networks. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, ??? (2019). https://openreview.net/forum?id=rJl-b3RcF7

[35] Jang, E., Gu, S., Poole, B.: Categorical Reparameterization with Gumbel-Softmax. arXiv (2016). https://doi.org/10.48550/ARXIV.1611.01144 . https://arxiv.org/abs/1611.01144

[36] Kusner, M.J., Hernández-Lobato, J.M.: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution. arXiv (2016). https://doi.org/10.48550/ARXIV.1611.04051 . https://arxiv.org/abs/1611.04051

[37] Shen, J., Zhen, X., Worring, M., Shao, L.: Variational multi-task learning with gumbel-softmax priors. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P.S., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems, vol. 34, pp. 21031–21042. Curran Associates, Inc., ??? (2021). https://proceedings.neurips.cc/paper/2021/file/afd4836712c5e77550897e25711e1d96-Paper.pdf

[38] Chang, J., Zhang, X., Guo, Y., Meng, G., Xiang, S., Pan, C.: Differentiable Architecture Search with Ensemble Gumbel-Softmax. arXiv (2019). https://doi.org/10.48550/ARXIV.1905.01786 . https://arxiv.org/abs/1905.01786

[39] Bengio, Y., Léonard, N., Courville, A.: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv (2013). https://doi.org/10.48550/ARXIV.1308.3432 . https://arxiv.org/abs/1308.3432

[40] Lin, S., Ji, R., Li, Y., Deng, C., Li, X.: Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning. arXiv (2019). https://doi.org/10.48550/ARXIV.1901.07827 . https://arxiv.org/abs/1901.07827

[41] Li, Y., Gu, S., Mayer, C., Van Gool, L., Timofte, R.: Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression. arXiv (2020). https://doi.org/10.48550/ARXIV.2003.08935 . https://arxiv.org/abs/2003.08935

[42] Srinivas, S., Babu, R.V.: Learning Neural Network Architectures using Backpropagation. arXiv (2015). https://doi.org/10.48550/ARXIV.1511.05497 . https://arxiv.org/abs/1511.05497

[43] Zhuang, T., Zhang, Z., Huang, Y., Zeng, X., Shuang, K., Li, X.: Neuron-level structured pruning using polarization regularizer. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, pp. 9865–9877. Curran Associates, Inc., ??? (2020). https://proceedings.neurips.cc/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf

[44] Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., Xin, J.: Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets. arXiv (2019). https://doi.org/10.48550/ARXIV.1903.05662 . https://arxiv.org/abs/1903.05662

[45] Kim, J.-H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In: International Conference on Machine Learning (ICML) (2020)

[46] He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., Han, S.: AMC: AutoML for model compression and acceleration on mobile devices. In: Computer Vision – ECCV 2018, pp. 815–832. Springer, ??? (2018). https://doi.org/10.1007/978-3-030-01234-2 48 . https://doi.org/10.1007/978-3-030-01234-2 48

[47] Dekhovich, A., Tax, D.M.J., Sluiter, M.H.F., Bessa, M.A.: Neural network relief: a pruning algorithm based on neural activity. CoRR abs/2109.10795 (2021) 2109.10795

[48] He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft filter pruning for accelerating deep convolutional neural networks. In: International Joint Conference on Artificial Intelligence (IJCAI), pp. 2234–2240 (2018)

[49] He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural networks. In: 2017 IEEE International Conference on Computer Vision (ICCV), pp. 1398–1406 (2017). https://doi.org/10.1109/ICCV.2017.155

[50] Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient convnets. ArXiv abs/1608.08710 (2016)

[51] Zhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W., Tian, Q.: Variational convolutional neural network pruning. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2775–2784 (2019). https://doi.org/10.1109/CVPR.2019.00289
