# 2310.07707.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/shared-params/2310.07707.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1136961 byte

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
MatFormer: Transformer Lá»“ng Nhau cho Suy Luáº­n ÄÃ n Há»“i
Devvritâˆ—âˆ†â‹„Sneha Kuduguntaâˆ—â€ â‹„Aditya Kusupatiâˆ—â€ â‹„+
Tim Dettmersâ€ Kaifeng Chenâ‹„Inderjit Dhillonâ‹„âˆ†Yulia Tsvetkovâ€ Hannaneh Hajishirziâ€ 
Sham Kakadeâ€¡Ali Farhadiâ€ Prateek Jainâ‹„+
â‹„Google DeepMindâˆ†University of Texas at Austinâ€ University of Washingtonâ€¡Harvard University
TÃ³m táº¯t
CÃ¡c mÃ´ hÃ¬nh ná»n táº£ng Ä‘Æ°á»£c Ã¡p dá»¥ng trong má»™t phá»• rá»™ng cÃ¡c cÃ i Ä‘áº·t vá»›i cÃ¡c rÃ ng buá»™c suy luáº­n khÃ¡c nhau, tá»« cÃ¡c cá»¥m Ä‘a gia tá»‘c lá»›n Ä‘áº¿n cÃ¡c thiáº¿t bá»‹ di Ä‘á»™ng Ä‘á»™c láº­p cÃ³ tÃ i nguyÃªn háº¡n cháº¿. Tuy nhiÃªn, chi phÃ­ Ä‘Ã¡ng ká»ƒ liÃªn quan Ä‘áº¿n viá»‡c huáº¥n luyá»‡n nhá»¯ng mÃ´ hÃ¬nh nÃ y thÆ°á»ng giá»›i háº¡n sá»‘ lÆ°á»£ng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh duy nháº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c cung cáº¥p. Do Ä‘Ã³, cÃ¡c nhÃ  thá»±c hÃ nh buá»™c pháº£i chá»n má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p tá»‘i Æ°u vá»›i cÃ¡c yÃªu cáº§u Ä‘á»™ trá»… vÃ  chi phÃ­ cá»¥ thá»ƒ cá»§a há». ChÃºng tÃ´i trÃ¬nh bÃ y MatFormer2, má»™t kiáº¿n trÃºc Transformer má»›i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p suy luáº­n Ä‘Ã n há»“i trÃªn cÃ¡c rÃ ng buá»™c triá»ƒn khai Ä‘a dáº¡ng. MatFormer Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng cÃ¡ch káº¿t há»£p má»™t cáº¥u trÃºc khá»‘i Máº¡ng Truyá»n Tiáº¿n Lá»“ng Nhau (FFN) trong má»™t mÃ´ hÃ¬nh Transformer tiÃªu chuáº©n. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i tá»‘i Æ°u hÃ³a cÃ¡c tham sá»‘ cá»§a nhiá»u khá»‘i FFN lá»“ng nhau vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau, cho phÃ©p trÃ­ch xuáº¥t hÃ ng trÄƒm mÃ´ hÃ¬nh nhá» hÆ¡n chÃ­nh xÃ¡c mÃ  khÃ´ng phÃ¡t sinh chi phÃ­ tÃ­nh toÃ¡n bá»• sung. ChÃºng tÃ´i xÃ¡c thá»±c thá»±c nghiá»‡m hiá»‡u quáº£ cá»§a MatFormer trÃªn cÃ¡c lá»›p mÃ´ hÃ¬nh khÃ¡c nhau (bá»™ giáº£i mÃ£ vÃ  bá»™ mÃ£ hÃ³a) vÃ  cÃ¡c phÆ°Æ¡ng thá»©c (ngÃ´n ngá»¯ vÃ  thá»‹ giÃ¡c), chá»©ng minh tiá»m nÄƒng cá»§a nÃ³ cho viá»‡c triá»ƒn khai thá»±c táº¿. ChÃºng tÃ´i chá»‰ ra ráº±ng má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ MatFormer chá»‰ giáº£i mÃ£ 850M (MatLM) cho phÃ©p chÃºng tÃ´i trÃ­ch xuáº¥t nhiá»u mÃ´ hÃ¬nh nhá» hÆ¡n tá»« 582M Ä‘áº¿n 850M tham sá»‘, má»—i mÃ´ hÃ¬nh thá»ƒ hiá»‡n máº¥t mÃ¡t xÃ¡c thá»±c tá»‘t hÆ¡n vÃ  Ä‘Ã¡nh giÃ¡ downstream má»™t láº§n so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p. HÆ¡n ná»¯a, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng cÃ¡c bá»™ mÃ£ hÃ³a nhá» hÆ¡n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« má»™t bá»™ mÃ£ hÃ³a MatFormer-based ViT (MatViT) phá»• quÃ¡t báº£o tá»“n cáº¥u trÃºc khÃ´ng gian metric cho viá»‡c truy xuáº¥t thÃ­ch á»©ng quy mÃ´ lá»›n. Cuá»‘i cÃ¹ng, chÃºng tÃ´i chá»©ng minh ráº±ng viá»‡c giáº£i mÃ£ suy Ä‘oÃ¡n vá»›i cÃ¡c mÃ´ hÃ¬nh con chÃ­nh xÃ¡c vÃ  nháº¥t quÃ¡n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« MatFormer cÃ³ thá»ƒ dáº«n Ä‘áº¿n giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ trá»… suy luáº­n. Trang web dá»± Ã¡n.

1 Giá»›i thiá»‡u
CÃ¡c mÃ´ hÃ¬nh ná»n táº£ng lá»›n [49,45,17] Ä‘Æ°á»£c triá»ƒn khai trong nhiá»u cÃ i Ä‘áº·t khÃ¡c nhau vá»›i cÃ¡c yÃªu cáº§u tÃ­nh toÃ¡n vÃ  Ä‘á»™ chÃ­nh xÃ¡c khÃ¡c nhau nhÆ° pháº£n há»“i thá»i gian thá»±c trÃªn Ä‘iá»‡n thoáº¡i di Ä‘á»™ng hoáº·c trÃªn GPU Ä‘a cá»¥m cho phá»¥c vá»¥ lÃ´ quy mÃ´ web. Tuy nhiÃªn, cÃ¡c há» mÃ´ hÃ¬nh Ä‘iá»ƒn hÃ¬nh chá»‰ cung cáº¥p má»™t vÃ i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau. VÃ­ dá»¥, há» Llama-2 cung cáº¥p cÃ¡c mÃ´ hÃ¬nh vá»›i 7B, 13B, 34B vÃ  70B tham sá»‘ [59]. VÃ¬ váº­y, cÃ¡c nhÃ  thá»±c hÃ nh buá»™c pháº£i chá»n má»™t mÃ´ hÃ¬nh nhá» hÆ¡n (vÃ  thÆ°á»ng Ã­t chÃ­nh xÃ¡c hÆ¡n) so vá»›i ngÃ¢n sÃ¡ch Ä‘á»™ trá»…/chi phÃ­ cá»§a há». Thay vÃ o Ä‘Ã³, ngÆ°á»i ta cÃ³ thá»ƒ sá»­ dá»¥ng nÃ©n/cáº¯t tá»‰a Ä‘á»ƒ phÃ¹ há»£p vá»›i má»™t mÃ´ hÃ¬nh lá»›n hÆ¡n trong má»™t ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n nháº¥t Ä‘á»‹nh [19, 36, 53], nhÆ°ng Ä‘iá»u Ä‘Ã³ yÃªu cáº§u huáº¥n luyá»‡n bá»• sung.

ChÃºng tÃ´i giá»›i thiá»‡u MatFormer, má»™t kiáº¿n trÃºc Transformer [61] Ä‘Ã n há»“i báº£n Ä‘á»‹a vÆ°á»£t qua thÃ¡ch thá»©c nÃ y. MatFormer cho phÃ©p huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh phá»• quÃ¡t cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t hÃ ng trÄƒm mÃ´ hÃ¬nh con nhá» hÆ¡n mÃ  khÃ´ng cÃ³ báº¥t ká»³ chi phÃ­ huáº¥n luyá»‡n bá»• sung nÃ o (HÃ¬nh 1). MatFormer lÃ  má»™t kiáº¿n trÃºc tá»•ng quÃ¡t

âˆ—ÄÃ³ng gÃ³p ká»¹ thuáº­t ngang nhau.+Aditya Kusupati vÃ  Prateek Jain dáº«n dáº¯t dá»± Ã¡n.
LiÃªn há»‡: devvrit@cs.utexas.edu,{snehakudugunta,kusupati,prajain}@google.com
2MatFormer viáº¿t táº¯t cá»§a Matryoshka Transformer, pháº£n Ã¡nh cáº¥u trÃºc lá»“ng nhau cá»‘ há»¯u cá»§a mÃ´ hÃ¬nh.
Há»™i nghá»‹ láº§n thá»© 38 vá» CÃ¡c Há»‡ thá»‘ng Xá»­ lÃ½ ThÃ´ng tin Tháº§n kinh (NeurIPS 2024).arXiv:2310.07707v2 [cs.LG] 15 Dec 2024

--- TRANG 2 ---
Huáº¥n luyá»‡n Add & Norm 
ğ¼ ( )
Add & Norm 
Attention 
FFN 
Suy luáº­n
Khá»‘i MatFormer
MatLM-S MatLM-M MatLM-L MatLM-XL 
Trá»™n & Káº¿t há»£p 

HÃ¬nh 1: MatFormer giá»›i thiá»‡u cáº¥u trÃºc lá»“ng nhau vÃ o khá»‘i FFN cá»§a Transformer & huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh con, cho phÃ©p trÃ­ch xuáº¥t miá»…n phÃ­ hÃ ng trÄƒm mÃ´ hÃ¬nh con chÃ­nh xÃ¡c cho suy luáº­n Ä‘Ã n há»“i.

kiáº¿n trÃºc cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c bá»™ mÃ£ hÃ³a vÃ  bá»™ giáº£i mÃ£, báº¥t kháº£ tri vá» miá»n vÃ  tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c quy trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh ná»n táº£ng tiÃªu chuáº©n.

MatFormer tuÃ¢n theo nguyÃªn táº¯c há»c biá»ƒu diá»…n matryoshka [34], Ä‘á»ƒ giá»›i thiá»‡u cáº¥u trÃºc con lá»“ng nhau bÃªn trong khá»‘i Transformer tiÃªu chuáº©n. ChÃ­nh thá»©c, MatFormer Ä‘á»‹nh nghÄ©a cÃ¡c khá»‘i Transformer Ti, sao cho T1âŠ‚T2âŠ‚ Â·Â·Â· âŠ‚ Tg, trong Ä‘Ã³ g lÃ  sá»‘ lÆ°á»£ng khá»‘i transformer lá»“ng nhau, vÃ  quan há»‡ TiâŠ‚Ti+1 chá»‰ ra ráº±ng cÃ¡c tham sá»‘ cá»§a Ti Ä‘Æ°á»£c chá»©a trong nhá»¯ng tham sá»‘ cá»§a Ti+1. MatFormer cÃ³ thá»ƒ táº¡o ra cáº¥u trÃºc con nhÆ° váº­y trong cáº£ khá»‘i attention vÃ  máº¡ng truyá»n tiáº¿n (FFN) cá»§a Transformer (xem HÃ¬nh 1). HÃ£y xem xÃ©t, cháº³ng háº¡n, má»™t khá»‘i FFN cÃ³ dff neuron trong lá»›p áº©n. Khi Ä‘Ã³, MatFormer táº¡o ra cáº¥u trÃºc matryoshka trÃªn nhá»¯ng neuron nÃ y, trong Ä‘Ã³ Ti chá»©a mi neuron Ä‘áº§u tiÃªn vÃ  1â‰¤m1< m 2Â·Â·Â·< m g=dff biá»ƒu thá»‹ sá»‘ lÆ°á»£ng neuron cho má»—i má»©c Ä‘á»™ chi tiáº¿t hoáº·c mÃ´ hÃ¬nh con. Trá»±c giÃ¡c, Ä‘iá»u nÃ y ngá»¥ Ã½ ráº±ng m1 neuron Ä‘áº§u tiÃªn lÃ  nhá»¯ng neuron "quan trá»ng nháº¥t" vÃ¬ chÃºng thuá»™c vá» táº¥t cáº£ cÃ¡c khá»‘i, tiáº¿p theo lÃ  m2âˆ’m1 neuron tiáº¿p theo, vÃ  cá»© tháº¿.

KhÃ¡c vá»›i cÃ¡c cÃ´ng trÃ¬nh liÃªn quan (Pháº§n 2), máº·c dÃ¹ chá»‰ tá»‘i Æ°u hÃ³a cho g má»©c Ä‘á»™ chi tiáº¿t, chÃºng tÃ´i cÃ³ thá»ƒ trÃ­ch xuáº¥t sá»‘ mÃ´ hÃ¬nh con tÄƒng theo cáº¥p sá»‘ nhÃ¢n sau huáº¥n luyá»‡n. Sá»­ dá»¥ng cÃ¡c khá»‘i MatFormer Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n T1, . . . , T g táº¡i má»—i lá»›p, ngÆ°á»i ta cÃ³ thá»ƒ táº¡o thÃ nh cÃ¡c mÃ´ hÃ¬nh má»›i báº±ng Trá»™n & Káº¿t há»£p (Pháº§n 3.3), tá»©c lÃ  báº±ng cÃ¡ch láº¥y má»™t káº¿t há»£p tÃ¹y Ã½ cá»§a cÃ¡c khá»‘i nÃ y qua cÃ¡c lá»›p. VÃ­ dá»¥, trong lá»›p Ä‘áº§u tiÃªn, ngÆ°á»i ta cÃ³ thá»ƒ chá»n Tg, khá»‘i lá»›n nháº¥t, chá»n T2 trong lá»›p thá»© hai, vÃ  cá»© tháº¿, táº¡o thÃ nh gl mÃ´ hÃ¬nh khÃ¡c nhau (trong Ä‘Ã³ l lÃ  sá»‘ lÆ°á»£ng lá»›p). ÄÃ¡ng ngáº¡c nhiÃªn, trong nhiá»u cÃ i Ä‘áº·t vÃ  cho cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh khÃ¡c nhau, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c trÃ­ch xuáº¥t thá»±c sá»± chÃ­nh xÃ¡c, vá»›i Ä‘á»™ chÃ­nh xÃ¡c tá»· lá»‡ thuáº­n vá»›i kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c trÃ­ch xuáº¥t.

ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ chá»‰ giáº£i mÃ£ dá»±a trÃªn Matformer (MatLM) lÃªn Ä‘áº¿n 850M tham sá»‘ vÃ  quan sÃ¡t tháº¥y ráº±ng: (a) MatLM Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng vá»›i g má»©c Ä‘á»™ chi tiáº¿t cÃ¡ch nhau theo cáº¥p sá»‘ nhÃ¢n vÆ°á»£t trá»™i vá» máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘Ã¡nh giÃ¡ downstream má»™t láº§n cá»§a g mÃ´ hÃ¬nh cÆ¡ sá»Ÿ tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p tá»« Ä‘áº§u, (b) cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c trÃ­ch xuáº¥t cá»§a chÃºng tÃ´i sá»­ dá»¥ng Trá»™n & Káº¿t há»£p náº±m trÃªn Ä‘Æ°á»ng cong Ä‘Ã¡nh Ä‘á»•i Ä‘á»™ chÃ­nh xÃ¡c-so vá»›i-tham sá»‘ Ä‘Æ°á»£c táº¡o ra bá»Ÿi g mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng, (c) thÃ´ng qua cÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng luáº­t máº¥t mÃ¡t so vá»›i tÃ­nh toÃ¡n cho cÃ¡c mÃ´ hÃ¬nh MatFormer khÃ¡c nhau váº«n tÆ°Æ¡ng tá»± nhÆ° cÃ¡c mÃ´ hÃ¬nh Transformer vanilla qua cÃ¡c má»©c Ä‘á»™ chi tiáº¿t khÃ¡c nhau vÃ  (d) cÃ¡c mÃ´ hÃ¬nh con Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« MatLM cÃ³ hÃ nh vi nháº¥t quÃ¡n cao ráº¥t mong muá»‘n cho cÃ¡c tá»‘i Æ°u hÃ³a suy luáº­n vÃ  triá»ƒn khai qua cÃ¡c quy mÃ´.

ChÃºng tÃ´i nghiÃªn cá»©u thÃªm cÃ¡c mÃ´ hÃ¬nh MatFormer-based ViT (MatViT) vÃ  Ä‘Æ°a ra nhá»¯ng quan sÃ¡t tÆ°Æ¡ng tá»±. VÃ­ dá»¥, MatViT-L/16 cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh ViT-L/16 tiÃªu chuáº©n trÃªn ImageNet-1K, vÃ  cÃ¡c mÃ´ hÃ¬nh con Ä‘Æ°á»£c trÃ­ch xuáº¥t Ä‘á»u khá»›p hoáº·c tháº­m chÃ­ hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p. HÆ¡n ná»¯a, chÃºng tÃ´i chá»©ng minh ráº±ng, do tÃ­nh nháº¥t quÃ¡n cao, cÃ¡c mÃ´ hÃ¬nh MatViT cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° "bá»™ mÃ£ hÃ³a Ä‘Ã n há»“i" cho viá»‡c truy xuáº¥t hÃ¬nh áº£nh thÃ­ch á»©ng. NghÄ©a lÃ , khÃ´ng gian metric cá»§a má»™t hÃ¬nh áº£nh Ä‘Æ°á»£c mÃ£ hÃ³a bá»Ÿi mÃ´ hÃ¬nh MatViT phá»• quÃ¡t (tá»©c lÃ  lá»›n nháº¥t) Ä‘Æ°á»£c báº£o tá»“n gáº§n Ä‘Ãºng bá»Ÿi cÃ¡c mÃ´ hÃ¬nh con lá»“ng nhau. Do Ä‘Ã³, dá»±a trÃªn Ä‘á»™ phá»©c táº¡p truy váº¥n, táº£i há»‡ thá»‘ng vÃ  cÃ¡c cÃ¢n nháº¯c khÃ¡c nhau, chÃºng tÃ´i cÃ³ thá»ƒ sá»­ dá»¥ng má»™t trong nhá»¯ng bá»™ mÃ£ hÃ³a MatViT Ä‘Æ°á»£c trÃ­ch xuáº¥t táº¡i thá»i Ä‘iá»ƒm suy luáº­n Ä‘á»ƒ truy xuáº¥t trÃªn má»™t corpus cá»‘ Ä‘á»‹nh Ä‘Æ°á»£c mÃ£ hÃ³a bá»Ÿi mÃ´ hÃ¬nh phá»• quÃ¡t â€“ cung cáº¥p hÆ¡n 40% Ã­t chi phÃ­ tÃ­nh toÃ¡n hÆ¡n vá»›i <0.5% giáº£m Ä‘á»™ chÃ­nh xÃ¡c.

ChÃºng tÃ´i Ä‘Ã³ng gÃ³p nhá»¯ng Ä‘iá»ƒm chÃ­nh sau:

--- TRANG 3 ---
1. ChÃºng tÃ´i giá»›i thiá»‡u MatFormer, káº¿t há»£p cáº¥u trÃºc con lá»“ng nhau trong Transformer tiÃªu chuáº©n vÃ  tá»‘i Æ°u hÃ³a táº¥t cáº£ g má»©c Ä‘á»™ chi tiáº¿t Ä‘á»ƒ táº¡o ra má»™t mÃ´ hÃ¬nh Ä‘Ã n há»“i phá»• quÃ¡t duy nháº¥t.

2. ChÃºng tÃ´i giá»›i thiá»‡u Trá»™n & Káº¿t há»£p, má»™t heuristic Ä‘Æ¡n giáº£n khÃ´ng cÃ³ chi phÃ­ tÃ­nh toÃ¡n tÃ¬m ra cÃ¡c mÃ´ hÃ¬nh con tá»‘i Æ°u trong má»™t ngÃ¢n sÃ¡ch tham sá»‘ nháº¥t Ä‘á»‹nh, vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p NAS phá»©c táº¡p hÆ¡n. Äiá»u nÃ y mang láº¡i hÃ ng trÄƒm mÃ´ hÃ¬nh con chÃ­nh xÃ¡c vÃ  nháº¥t quÃ¡n mÃ  khÃ´ng cÃ³ báº¥t ká»³ chi phÃ­ huáº¥n luyá»‡n nÃ o (Pháº§n 3).

3. MatFormer tá»•ng quÃ¡t hÃ³a hiá»‡u quáº£ cho cáº£ mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ giáº£i mÃ£ (MatLM) vÃ  bá»™ mÃ£ hÃ³a thá»‹ giÃ¡c (MatViT), má»Ÿ rá»™ng Ä‘Ã¡ng tin cáº­y vÃ  chÃ­nh xÃ¡c nhÆ° Transformer tiÃªu chuáº©n, Ä‘á»“ng thá»i cho phÃ©p táº¡o tá»± Ä‘á»™ng há»“i quy nhanh hÆ¡n Ä‘Ã¡ng ká»ƒ vÃ  truy xuáº¥t dÃ y Ä‘áº·c thÃ­ch á»©ng quy mÃ´ lá»›n (Pháº§n 4).

2 CÃ´ng trÃ¬nh liÃªn quan
Transformer [61] Ä‘Ã£ trá»Ÿ thÃ nh kiáº¿n trÃºc mÃ´ hÃ¬nh thá»‘ng nháº¥t cho cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng [7] qua cÃ¡c phÆ°Æ¡ng thá»©c nhÆ° ngÃ´n ngá»¯ [8], thá»‹ giÃ¡c [17] vÃ  Ã¢m thanh [47]. Máº·c dÃ¹ máº¡nh máº½, khá»‘i Transformer tiÃªu chuáº©n khÃ´ng Ä‘Ã n há»“i báº£n Ä‘á»‹a theo cÃ¡ch cho phÃ©p triá»ƒn khai thÃ­ch á»©ng vÃ  linh hoáº¡t quy mÃ´ lá»›n qua cÃ¡c rÃ ng buá»™c tÃ i nguyÃªn khÃ¡c nhau. Äá»ƒ phá»¥c vá»¥ cho vÃ´ sá»‘ yÃªu cáº§u triá»ƒn khai, cÃ¡c giáº£i phÃ¡p hiá»‡n cÃ³ bao gá»“m huáº¥n luyá»‡n má»™t há» mÃ´ hÃ¬nh vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau [49,2], cÃ¡c ká»¹ thuáº­t hiá»‡u quáº£ sau hoc nhÆ° lÆ°á»£ng tá»­ hÃ³a [19], cáº¯t tá»‰a [36], vÃ  chÆ°ng cáº¥t [53]. Tuy nhiÃªn, nhá»¯ng giáº£i phÃ¡p nÃ y thÆ°á»ng cá»¥ thá»ƒ cho rÃ ng buá»™c Ä‘Æ¡n láº» hiá»‡n táº¡i vÃ  yÃªu cáº§u huáº¥n luyá»‡n bá»• sung cho má»—i trÆ°á»ng há»£p sá»­ dá»¥ng downstream má»›i. Äiá»u nÃ y khiáº¿n chÃºng xa vá»›i viá»‡c trá»Ÿ thÃ nh má»™t giáº£i phÃ¡p thá»±c sá»± Ä‘Ã n há»“i cho triá»ƒn khai thÃ­ch á»©ng. Cuá»‘i cÃ¹ng, cÃ¡c LLM dá»±a trÃªn Transformer thÆ°á»ng Ä‘Æ°á»£c tÄƒng tá»‘c trong quÃ¡ trÃ¬nh suy luáº­n vá»›i cÃ¡c ká»¹ thuáº­t nhÆ° giáº£i mÃ£ suy Ä‘oÃ¡n [39,12] â€“ cÃ³ lá»£i tá»« viá»‡c cÃ¡c mÃ´ hÃ¬nh nhÃ¡p nhá» hÆ¡n & mÃ´ hÃ¬nh xÃ¡c minh lá»›n hÆ¡n cÃ³ hÃ nh vi tÆ°Æ¡ng tá»± â€“ hoáº·c thoÃ¡t sá»›m [54] Ä‘á»ƒ cho phÃ©p triá»ƒn khai thá»i gian thá»±c.

Viá»‡c cÃ³ Ä‘Æ°á»£c nhiá»u mÃ´ hÃ¬nh nhá» hÆ¡n tá»« má»™t mÃ´ hÃ¬nh duy nháº¥t Ä‘Ã£ Ä‘Æ°á»£c khÃ¡m phÃ¡ trong quÃ¡ khá»© [66,65,9,23,10] vá»›i háº§u háº¿t cÃ´ng trÃ¬nh táº­p trung vÃ o cÃ¡c bá»™ mÃ£ hÃ³a CNN. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i táº­p trung vÃ o Transformer trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ giáº£i mÃ£ vÃ  cÃ¡c mÃ´ hÃ¬nh thá»‹ giÃ¡c Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n. Cá»¥ thá»ƒ, OFA [9] huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh CNN giÃ¡o viÃªn vÃ  sá»­ dá»¥ng chÆ°ng cáº¥t Ä‘á»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh con Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn (khÃ´ng lá»“ng nhau) trong mÃ´ hÃ¬nh CNN há»c sinh phá»• quÃ¡t. HÆ¡n ná»¯a, OFA táº­p trung vÃ o cÃ¡c mÃ´ hÃ¬nh quy mÃ´ nhá» Ä‘á»ƒ triá»ƒn khai trÃªn cÃ¡c thiáº¿t bá»‹ cuá»‘i. NgÆ°á»£c láº¡i, MatFormer khÃ´ng yÃªu cáº§u chÆ°ng cáº¥t, do Ä‘Ã³ sá»­ dá»¥ng Ã­t bá»™ nhá»› hÆ¡n Ä‘Ã¡ng ká»ƒ vÃ  sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh lá»“ng nhau. Viá»‡c sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh lá»“ng nhau cho phÃ©p chÃºng tÃ´i lÆ°u trá»¯ nhiá»u mÃ´ hÃ¬nh cÃ¹ng nhau mÃ  khÃ´ng tÄƒng Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n bá»™ nhá»› cá»§a mÃ´ hÃ¬nh, Ä‘iá»u nÃ y cÃ³ lá»£i cho cÃ¡c tÃ¬nh huá»‘ng mÃ  chÃºng tÃ´i muá»‘n Ä‘á»‹nh tuyáº¿n cÃ¡c truy váº¥n thÃ´ng qua cÃ¡c máº¡ng con khÃ¡c nhau. Máº¡ng cÃ³ thá»ƒ thu nhá» [66] tá»‘i Æ°u hÃ³a chung vÃ  cung cáº¥p cÃ¡c Ä‘á»™ rá»™ng thiáº¿t láº­p trÆ°á»›c háº¡n cháº¿. Máº¡ng cÃ³ thá»ƒ thu nhá» phá»• quÃ¡t [65] má»Ÿ rá»™ng Ä‘iá»u nÃ y Ä‘á»ƒ láº¥y máº«u tá»« má»™t khÃ´ng gian tÃ¬m kiáº¿m liÃªn tá»¥c cá»§a cÃ¡c mÃ´ hÃ¬nh con vÃ  tá»‘i Æ°u hÃ³a chÃºng chung. NgÆ°á»£c láº¡i, MatFormer láº¥y máº«u vÃ  chá»‰ tá»‘i Æ°u hÃ³a má»™t trong nhá»¯ng má»©c Ä‘á»™ chi tiáº¿t thiáº¿t láº­p trÆ°á»›c. HAT [63] huáº¥n luyá»‡n má»™t máº¡ng phá»• quÃ¡t chá»‰ Ä‘á»ƒ há»c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘á»‘i cho cÃ¡c kiáº¿n trÃºc khÃ¡c nhau. Äá»ƒ triá»ƒn khai, cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng NAS Ä‘á»ƒ tÃ¬m kiáº¿n trÃºc tá»‘i Æ°u vÃ  huáº¥n luyá»‡n nÃ³ tá»« Ä‘áº§u trÆ°á»›c khi phá»¥c vá»¥. NgÆ°á»£c láº¡i, MatFormer khÃ´ng yÃªu cáº§u huáº¥n luyá»‡n bá»• sung vÃ  cÃ¡c máº¡ng con chÃ­nh xÃ¡c cÃ³ thá»ƒ Ä‘Æ°á»£c thu Ä‘Æ°á»£c báº±ng cÃ¡ch sá»­ dá»¥ng Trá»™n & Káº¿t há»£p (Pháº§n 3.3) mang láº¡i káº¿t quáº£ tá»‘t nhÆ° NAS mÃ  khÃ´ng cÃ³ sá»± phá»©c táº¡p bá»• sung. DynaBERT [27] huáº¥n luyá»‡n chung má»™t táº­p há»£p cá»‘ Ä‘á»‹nh cÃ¡c mÃ´ hÃ¬nh con, khÃ´ng giá»›i thiá»‡u báº¥t ká»³ chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m nÃ o vÃ  chá»‰ tháº£o luáº­n vá» viá»‡c sá»­ dá»¥ng cÃ¡c má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng lÃ m mÃ´ hÃ¬nh con. Káº¿t quáº£ cá»§a viá»‡c tá»‘i Æ°u hÃ³a chung cá»§a táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t, DynaBERT mang láº¡i Ã­t cáº­p nháº­t gradient hÆ¡n vÃ  do Ä‘Ã³ hiá»‡u suáº¥t dÆ°á»›i tá»‘i Æ°u so vá»›i MatFormer trong khi sá»­ dá»¥ng cÃ¹ng tÃ­nh toÃ¡n vÃ  bá»™ nhá»› (Pháº§n 4).

HÆ¡n ná»¯a, chÃºng tÃ´i nháº¥n máº¡nh ráº±ng trong khi háº§u háº¿t cÃ´ng trÃ¬nh trong lÄ©nh vá»±c nÃ y tá»‘i Æ°u hÃ³a cho má»™t sá»‘ mÃ´ hÃ¬nh theo cáº¥p sá»‘ nhÃ¢n, chÃºng tÃ´i tá»‘i Æ°u hÃ³a má»™t sá»‘ lÆ°á»£ng nhá» máº¡ng con (g= 4) Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t sá»‘ mÃ´ hÃ¬nh theo cáº¥p sá»‘ nhÃ¢n táº¡i thá»i Ä‘iá»ƒm suy luáº­n dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ cho cÃ¡c táº­p dá»¯ liá»‡u lá»›n. HÆ¡n

Báº£ng 1: So sÃ¡nh MatFormer vá»›i cÃ¡c ká»¹ thuáº­t cÃ³ thá»ƒ so sÃ¡nh qua huáº¥n luyá»‡n vÃ  suy luáº­n. ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng trÃ¡i ngÆ°á»£c vá»›i cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y, MatFormer yÃªu cáº§u tá»‘i Æ°u hÃ³a Ã­t mÃ´ hÃ¬nh hÆ¡n Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t sá»‘ mÃ´ hÃ¬nh theo cáº¥p sá»‘ nhÃ¢n táº¡i thá»i Ä‘iá»ƒm suy luáº­n mÃ  khÃ´ng cáº§n háº­u huáº¥n luyá»‡n hoáº·c NAS. HÆ¡n ná»¯a, cÃ¡c máº¡ng con MatFormer Ä‘Æ°á»£c lá»“ng nhau, cho phÃ©p truy xuáº¥t thÃ­ch á»©ng & colocation cá»§a cÃ¡c mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh suy luáº­n. á» Ä‘Ã¢y, l lÃ  sá»‘ lÆ°á»£ng lá»›p trong mÃ´ hÃ¬nh vÃ  exp(l) chá»‰ cáº¥p sá»‘ nhÃ¢n trong l.

N(MÃ´ hÃ¬nh ÄÆ°á»£c Tá»‘i Æ°u hÃ³a) N(MÃ´ hÃ¬nh Thu Ä‘Æ°á»£c) Lá»“ng nhau? Lá»±a chá»n MÃ´ hÃ¬nh Cáº§n Háº­u huáº¥n luyá»‡n? Kiáº¿n trÃºc MÃ´ hÃ¬nh Giáº£i mÃ£?
MatFormer O(1) exp(l) âœ“ Trá»™n & Káº¿t há»£p Ã— Transformer âœ“
OFA [9] exp(l) exp(l) Ã— NAS Ã— CNN Ã—
Máº¡ng Thu nhá» [66] O(1) O(1) âœ“ - Ã— CNN Ã—
HAT [63] exp(l) exp(l) Ã— NAS âœ“ CNN enc-dec
Máº¡ng ÄÃ£ sáº¯p xáº¿p [60] exp(l) exp(l) âœ“ - Ã— Cáº£ hai Ã—
DynaBERT [27] O(1) O(1) âœ“ - Ã— Transformer Ã—

--- TRANG 4 ---
gáº§n Ä‘Ã¢y, má»™t sá»‘ cÃ´ng trÃ¬nh nÃ y Ä‘Ã£ Ä‘Æ°á»£c má»Ÿ rá»™ng cho cÃ¡c bá»™ mÃ£ hÃ³a Transformer [11,52] Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c mÃ´ hÃ¬nh con trong cáº£ cÃ i Ä‘áº·t tÄ©nh hoáº·c Ä‘á»™ng. NhÆ°ng chÃºng hoáº·c tháº¥t báº¡i trong viá»‡c má»Ÿ rá»™ng thÃªm cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ giáº£i mÃ£ ([52]) hoáº·c hoáº¡t Ä‘á»™ng dÆ°á»›i tá»‘i Æ°u so vá»›i MatFormer (Pháº§n 4) do nhá»¯ng khÃ¡c biá»‡t trong phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n cá»§a chÃºng. Trong khi khÃ´ng á»Ÿ khÃ´ng gian trá»ng sá»‘, há»c biá»ƒu diá»…n matryoshka [34] & FlexiViT [5] thá»ƒ hiá»‡n tÃ­nh Ä‘Ã n há»“i trong khÃ´ng gian Ä‘áº§u ra & Ä‘áº§u vÃ o tÆ°Æ¡ng á»©ng báº±ng cÃ¡ch bao quÃ¡t suÃ´n sáº» cÃ¡c rÃ ng buá»™c triá»ƒn khai vá»›i chi phÃ­ tá»‘i thiá»ƒu. MatFormer, ngÆ°á»£c láº¡i, xÃ¢y dá»±ng dá»±a trÃªn nhá»¯ng cÃ´ng trÃ¬nh nÃ y báº±ng cÃ¡ch táº¡o ra cáº¥u trÃºc lá»“ng nhau trong khÃ´ng gian trá»ng sá»‘ thay vÃ¬ cho phÃ©p cÃ¡c mÃ´ hÃ¬nh Transformer thá»±c sá»± Ä‘Ã n há»“i vÃ  thÃ­ch á»©ng (giáº£i mÃ£ & mÃ£ hÃ³a) bao quÃ¡t táº¥t cáº£ cÃ¡c Ä‘Ã¡nh Ä‘á»•i Ä‘á»™ chÃ­nh xÃ¡c-so vá»›i-tÃ­nh toÃ¡n (tÄ©nh hoáº·c Ä‘á»™ng) vá»›i nhá»¯ng thay Ä‘á»•i vÃ  chi phÃ­ huáº¥n luyá»‡n tá»‘i thiá»ƒu (HÃ¬nh 1). SortedNet [60] lÃ  má»™t cÃ´ng trÃ¬nh Ä‘á»“ng thá»i vá»›i má»¥c tiÃªu tÆ°Æ¡ng tá»± tá»‘i Æ°u hÃ³a nhiá»u mÃ´ hÃ¬nh con Ä‘Æ°á»£c láº¥y máº«u (giá»‘ng nhÆ° cÃ´ng trÃ¬nh trÆ°á»›c) khÃ´ng giá»‘ng nhÆ° viá»‡c tá»‘i Æ°u hÃ³a má»™t vÃ i (thÆ°á»ng lÃ  4) mÃ´ hÃ¬nh con lá»“ng nhau cá»§a MatFormer. ChÃºng tÃ´i cÅ©ng lÆ°u Ã½ FLEXTRON [64], má»™t cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y xÃ¢y dá»±ng dá»±a trÃªn MatFormer báº±ng cÃ¡ch má»Ÿ rá»™ng tÃ­nh Ä‘Ã n há»“i lá»“ng nhau trong cáº£ MLP vÃ  Attention Head Ä‘á»“ng thá»i, vÃ¡ Ä‘uÃ´i kiá»ƒu huáº¥n luyá»‡n Matformer, vÃ  bao gá»“m má»™t bá»™ Ä‘á»‹nh tuyáº¿n Ä‘á»ƒ tá»± Ä‘á»™ng Ä‘á»‹nh tuyáº¿n má»—i token trong cÃ¡c má»©c Ä‘á»™ chi tiáº¿t khÃ¡c nhau trong má»—i lá»›p.

Trong Báº£ng 1, chÃºng tÃ´i tÃ³m táº¯t nhá»¯ng khÃ¡c biá»‡t giá»¯a MatFormer vÃ  cÃ´ng trÃ¬nh liÃªn quan Ä‘Ã£ tháº£o luáº­n. Trong sá»‘ nhá»¯ng cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i Ä‘Ã£ xÃ¡c Ä‘á»‹nh hai Ã½ tÆ°á»Ÿng trung tÃ¢m thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng: huáº¥n luyá»‡n chung nhiá»u mÃ´ hÃ¬nh con vÃ  láº¥y máº«u cÃ¡c mÃ´ hÃ¬nh con ngáº«u nhiÃªn. ChÃºng tÃ´i coi DynaBERT [27] vÃ  Once-for-All (OFA) [9] lÃ  nhá»¯ng cÃ´ng trÃ¬nh liÃªn quan nháº¥t tÆ°Æ¡ng á»©ng vÃ  so sÃ¡nh chÃºng vá»›i MatFormer trong Pháº§n 4.

3 MatFormer
Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a cáº¥u trÃºc con lá»“ng nhau cá»§a MatFormer (Pháº§n 3.1) vÃ  tháº£o luáº­n vá» quy trÃ¬nh huáº¥n luyá»‡n cá»§a nÃ³ cho g má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh Ä‘Ã£ chá»n (Pháº§n 3.2). Sau Ä‘Ã³ chÃºng tÃ´i tháº£o luáº­n vá» suy luáº­n Ä‘Ã n há»“i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Trá»™n & Káº¿t há»£p (Pháº§n 3.3) tá»« MatFormer cÃ¹ng vá»›i cÃ¡c cÃ¢n nháº¯c triá»ƒn khai cá»§a nÃ³.

3.1 Cáº¥u trÃºc MatFormer
MatFormer Ä‘á»‹nh nghÄ©a g khá»‘i Transformer Ti, sao cho T1âŠ‚T2âŠ‚ Â·Â·Â· âŠ‚ Tg trong Ä‘Ã³ TiâŠ‚Ti+1 chá»‰ ra ráº±ng cÃ¡c tham sá»‘ cá»§a Ti Ä‘Æ°á»£c chá»©a trong nhá»¯ng cá»§a Ti+1. Máº·c dÃ¹ cÃ³ thá»ƒ Ã¡p Ä‘áº·t cáº¥u trÃºc nhÆ° váº­y trÃªn báº¥t ká»³ pháº§n nÃ o cá»§a Transformer, chÃºng tÃ´i chá»n khá»‘i FFN Ä‘á»ƒ Ä‘á»‹nh nghÄ©a phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh vÃ  trÃ¬nh bÃ y pháº§n lá»›n cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, vÃ¬ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  chi phÃ­ tÃ­nh toÃ¡n cá»§a Transformer bá»‹ chi phá»‘i (khoáº£ng 60% cho LLM vÃ  ViT) bá»Ÿi khá»‘i FFN (xem Phá»¥ lá»¥c C, vÃ  Phá»¥ lá»¥c F.2 cho cÃ¡c thÃ­ nghiá»‡m Ã¡p dá»¥ng MatFormer cho khá»‘i attention cá»§a Transformer). VÃ¬ váº­y, trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i táº­p trung vÃ o viá»‡c táº¡o ra cáº¥u trÃºc con lá»“ng nhau cá»§a MatFormer trong khá»‘i FFN. Sau Ä‘Ã³ chÃºng tÃ´i xáº¿p chá»“ng cÃ¡c khá»‘i riÃªng láº» (cho l lá»›p) Ä‘á»ƒ táº¡o thÃ nh g mÃ´ hÃ¬nh lá»“ng nhau (M1...g) vá»›i cÃ¡c tham sá»‘ chia sáº» tá»©c lÃ  MiâŠ‚ M i+1.

Khá»‘i FFN Transformer cÃ³ má»™t lá»›p áº©n duy nháº¥t vá»›i dff neuron vÃ  cáº£ Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra trong Rdmodel, vÃ  tá»· lá»‡ FFN cá»‘ Ä‘á»‹nh := dff/dmodel (thÆ°á»ng â‰¥4). MatFormer giá»›i thiá»‡u cáº¥u trÃºc lá»“ng nhau matryoshka vá»›i g má»©c Ä‘á»™ chi tiáº¿t trÃªn biá»ƒu diá»…n áº©n cá»§a khá»‘i FFN. Cá»¥ thá»ƒ, má»™t khá»‘i con lá»“ng nhau cá»§a Transformer, Ti chá»©a mi neuron Ä‘áº§u tiÃªn cá»§a FFN vÃ  1â‰¤m1< Â·Â·Â·< m g=dff biá»ƒu thá»‹ sá»‘ lÆ°á»£ng neuron cho má»—i má»©c Ä‘á»™ chi tiáº¿t hoáº·c mÃ´ hÃ¬nh con. VÃ¬ váº­y, tÃ¹y thuá»™c vÃ o má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c chá»n, hoáº¡t Ä‘á»™ng FFN cá»§a Ti tá»©c lÃ  TFFN i trÃªn má»™t Ä‘áº§u vÃ o xâˆˆRdmodel lÃ :

TFFN i(x) =Ïƒ(xÂ·W1[0 :mi]âŠ¤)Â·W2[0 :mi], (1)

trong Ä‘Ã³ cÃ¡c ma tráº­n trá»ng sá»‘ cá»§a FFN lÃ  W1,W2âˆˆRdffÃ—dmodel vÃ  cÃ¡c háº¡ng tá»­ bias Ä‘Æ°á»£c bá» qua Ä‘á»ƒ Ä‘Æ¡n giáº£n. W1[0 :k] biá»ƒu thá»‹ ma tráº­n con vá»›i k hÃ ng Ä‘áº§u tiÃªn cá»§a W1. Cuá»‘i cÃ¹ng, Ïƒ lÃ  má»™t hÃ m phi tuyáº¿n thÆ°á»ng Ä‘Æ°á»£c Ä‘áº·t thÃ nh GELU [24] hoáº·c ReLU bÃ¬nh phÆ°Æ¡ng [56]. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i chá»n g= 4 má»©c Ä‘á»™ chi tiáº¿t cÃ¡ch nhau theo cáº¥p sá»‘ nhÃ¢n vá»›i tá»· lá»‡ FFN cá»§a {0.5,1,2,4} tá»©c lÃ  cÃ¡c neuron áº©n lá»“ng nhau cÃ³ kÃ­ch thÆ°á»›c {dff 8,dff 4,dff 2, dff}.

ChÃºng tÃ´i cÃ³ g mÃ´ hÃ¬nh con lá»“ng nhau M1âŠ‚ M 2. . . ,âŠ‚ M g trong Ä‘Ã³ Miâ†[Ti]Ã—l, tá»©c lÃ  Mi Ä‘Æ°á»£c táº¡o thÃ nh báº±ng cÃ¡ch xáº¿p chá»“ng Ti cho l lá»›p. CÃ¡c ma tráº­n nhÃºng Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra Ä‘Æ°á»£c chia sáº» qua cÃ¡c mÃ´ hÃ¬nh.

ChÃºng tÃ´i lÆ°u Ã½ ráº±ng chÃºng ta cÃ³ thá»ƒ táº¡o thÃ nh má»™t cáº¥u trÃºc con tÆ°Æ¡ng tá»± trÃªn cÃ¡c Ä‘áº§u attention, vá»›i cÃ¡c Ä‘áº§u Ä‘Æ°á»£c tá»• chá»©c tá»« "quan trá»ng nháº¥t" Ä‘áº¿n "Ã­t quan trá»ng nháº¥t", trong Ä‘Ã³ cÃ¡c Ä‘áº§u quan trá»ng hÆ¡n Ä‘Æ°á»£c chia sáº» bá»Ÿi nhiá»u mÃ´ hÃ¬nh con hÆ¡n. NghÄ©a lÃ , chÃºng tÃ´i sá»­ dá»¥ng mi Ä‘áº§u attention Ä‘áº§u tiÃªn cho má»©c Ä‘á»™ chi tiáº¿t thá»© i. ChÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ giá»›i thiá»‡u cáº¥u trÃºc con nÃ y trong token embedding (dmodel) Ä‘Æ°á»£c cung cáº¥p cho má»—i khá»‘i Transformer.

--- TRANG 5 ---
3.2 Huáº¥n luyá»‡n
Äá»‘i vá»›i má»™t mÃ´ hÃ¬nh Transformer M, lÆ°á»£t truyá»n tiáº¿n trÃªn má»™t Ä‘áº§u vÃ o x Ä‘Æ°á»£c kÃ½ hiá»‡u bá»Ÿi M(x) vÃ  Ä‘á»ƒ L biá»ƒu thá»‹ hÃ m máº¥t mÃ¡t giá»¯a Ä‘áº§u ra vÃ  má»¥c tiÃªu y: L(M(x), y).

MatFormer dá»±a vÃ o má»™t chiáº¿n lÆ°á»£c huáº¥n luyá»‡n Ä‘Æ¡n giáº£n lÃ  láº¥y máº«u ngáº«u nhiÃªn g mÃ´ hÃ¬nh con lá»“ng nhau qua huáº¥n luyá»‡n. Äá»ƒ lÃ m Ä‘iá»u nÃ y, cho má»—i bÆ°á»›c chÃºng tÃ´i láº¥y máº«u ngáº«u nhiÃªn má»™t má»©c Ä‘á»™ chi tiáº¿t Matformer i= 1,2..., g vÃ  huáº¥n luyá»‡n cho nÃ³ sá»­ dá»¥ng cÃ¡c bá»™ tá»‘i Æ°u hÃ³a dá»±a trÃªn gradient ngáº«u nhiÃªn tiÃªu chuáº©n [55]:

LSAMPLING (x, y) =L(Mi(x), y), (2)

trong Ä‘Ã³ Mi lÃ  táº­p tham sá»‘ cá»§a mÃ´ hÃ¬nh con má»©c Ä‘á»™ chi tiáº¿t thá»© i, vá»›i Mi Ä‘Æ°á»£c chá»n tá»« má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t {p1, p2...pg}. Äá»‘i vá»›i háº§u háº¿t cÃ¡c thÃ­ nghiá»‡m trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i láº¥y máº«u Ä‘á»u má»—i mÃ´ hÃ¬nh con - trong Phá»¥ lá»¥c F.3, chÃºng tÃ´i tháº¥y ráº±ng viá»‡c Ä‘iá»u chá»‰nh phÃ¢n phá»‘i xÃ¡c suáº¥t nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh con máº¡nh hÆ¡n.

Huáº¥n luyá»‡n MatFormer dáº«n Ä‘áº¿n g mÃ´ hÃ¬nh con lá»“ng nhau chÃ­nh xÃ¡c M1...g bÃªn trong mÃ´ hÃ¬nh MatFormer phá»• quÃ¡t (Mg), vÃ  cÅ©ng cho phÃ©p trÃ­ch xuáº¥t hÃ ng trÄƒm mÃ´ hÃ¬nh con nhá» hÆ¡n dá»c theo Ä‘Æ°á»ng cong Ä‘á»™ chÃ­nh xÃ¡c-so vá»›i-tÃ­nh toÃ¡n Ä‘Æ°á»£c váº½ bá»Ÿi g mÃ´ hÃ¬nh con Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a rÃµ rÃ ng (Pháº§n 3.3). Nhá»¯ng mÃ´ hÃ¬nh nÃ y xuáº¥t hiá»‡n miá»…n phÃ­ sá»­ dá»¥ng Trá»™n & Káº¿t há»£p trong quÃ¡ trÃ¬nh suy luáº­n vÃ  giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ huáº¥n luyá»‡n kháº¥u hao trÃªn má»—i mÃ´ hÃ¬nh thu Ä‘Æ°á»£c thÃ´ng qua MatFormer. PhÆ°Æ¡ng phÃ¡p nÃ y dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh con nhá» hÆ¡n cÃ³ hÃ nh vi nháº¥t quÃ¡n cao (Pháº§n 3.4) vá»›i mÃ´ hÃ¬nh phá»• quÃ¡t.

3.3 Trá»™n & Káº¿t há»£p
Táº¡i thá»i Ä‘iá»ƒm suy luáº­n, viá»‡c trÃ­ch xuáº¥t má»™t trong g mÃ´ hÃ¬nh con M1âŠ‚ M 2. . . ,âŠ‚ M g báº±ng cÃ¡ch xáº¿p chá»“ng khá»‘i Transformer tÆ°Æ¡ng á»©ng Ti qua cÃ¡c lá»›p lÃ  táº§m thÆ°á»ng. Tuy nhiÃªn, báº±ng cÃ¡ch chá»n cÃ¡c má»©c Ä‘á»™ chi tiáº¿t khÃ¡c nhau cho má»—i lá»›p MatFormer, cÃ³ thá»ƒ táº¡o ra má»™t sá»‘ lÆ°á»£ng lá»›n cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n chÃ­nh xÃ¡c miá»…n phÃ­ theo tá»• há»£p. ChÃºng tÃ´i gá»i quy trÃ¬nh Ä‘Æ¡n giáº£n nÃ y lÃ  Trá»™n & Káº¿t há»£p vÃ  quan sÃ¡t tháº¥y ráº±ng nhá»¯ng má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh bá»• sung nÃ y â€“ chÆ°a bao giá» Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a rÃµ rÃ ng â€“ cÃ³ hiá»‡u suáº¥t cao.

Äá»‘i vá»›i má»™t ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n hoáº·c tham sá»‘ nháº¥t Ä‘á»‹nh, cÃ³ nhiá»u mÃ´ hÃ¬nh con cÃ³ thá»ƒ. Má»™t chiáº¿n lÆ°á»£c phá»• biáº¿n Ä‘á»ƒ chá»n má»™t mÃ´ hÃ¬nh con tá»‘i Æ°u lÃ  TÃ¬m kiáº¿m Kiáº¿n trÃºc Tháº§n kinh (NAS) [48,68]. Tuy nhiÃªn, Ä‘iá»u nÃ y tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n (Phá»¥ lá»¥c D.2). Vá»›i Trá»™n & Káº¿t há»£p, chÃºng tÃ´i Ä‘á» xuáº¥t viá»‡c tÄƒng dáº§n kÃ­ch thÆ°á»›c khá»‘i con vá»›i "Ä‘á»™ dá»‘c nhá» nháº¥t". Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i khuyáº¿n nghá»‹ chá»n cÃ¡c khá»‘i con vá»›i nhá»¯ng thay Ä‘á»•i má»©c Ä‘á»™ chi tiáº¿t tá»‘i thiá»ƒu qua cÃ¡c lá»›p, Ä‘áº£m báº£o ráº±ng kÃ­ch thÆ°á»›c cá»§a lá»›p thá»© j Ã­t nháº¥t báº±ng cá»§a lá»›p thá»© i cho j > i. Äá»ƒ Ä‘Æ°a ra má»™t vÃ­ dá»¥ cá»¥ thá»ƒ, chÃºng tÃ´i tháº¥y ráº±ng má»™t mÃ´ hÃ¬nh con sá»­ dá»¥ng má»©c Ä‘á»™ chi tiáº¿t g2 cho má»™t ná»­a cÃ¡c lá»›p sau Ä‘Ã³ g3 cho pháº§n cÃ²n láº¡i cÃ³ thá»ƒ sáº½ tá»‘t hÆ¡n má»™t mÃ´ hÃ¬nh con sá»­ dá»¥ng g1 vÃ  g4 cho kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tÆ°Æ¡ng tá»±. Heuristic cá»§a chÃºng tÃ´i Ä‘Æ°á»£c há»— trá»£ bá»Ÿi phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n, trong Ä‘Ã³ má»—i máº¡ng con Ä‘Æ°á»£c láº¥y máº«u duy trÃ¬ má»©c Ä‘á»™ chi tiáº¿t lá»›p nháº¥t quÃ¡n qua mÃ´ hÃ¬nh. Do Ä‘Ã³, mÃ´ hÃ¬nh thÃ­ch á»©ng tá»‘t nháº¥t vá»›i cÃ¡c cáº¥u hÃ¬nh trong Ä‘Ã³ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t lá»›p hoáº·c Ä‘á»“ng nháº¥t hoáº·c hiá»ƒn thá»‹ sá»± biáº¿n Ä‘á»•i tá»‘i thiá»ƒu. Trá»±c giÃ¡c nÃ y cÅ©ng Ä‘Æ°á»£c há»— trá»£ bá»Ÿi NAS, dá»± Ä‘oÃ¡n cÃ¡c cáº¥u hÃ¬nh cÃ¢n báº±ng hÆ¡n so vá»›i cÃ¡c cáº¥u hÃ¬nh lá»‡ch (Phá»¥ lá»¥c D.1).

Trong sá»‘ nhá»¯ng cáº¥u hÃ¬nh "cÃ¢n báº±ng" nÃ y, chÃºng tÃ´i tháº¥y thá»±c nghiá»‡m ráº±ng cáº¥u hÃ¬nh tÄƒng vá»›i Ä‘á»™ dá»‘c tá»‘i thiá»ƒu hoáº¡t Ä‘á»™ng tá»‘t nháº¥t. Trong Pháº§n 4.1.1 vÃ  Phá»¥ lá»¥c D.1, chÃºng tÃ´i chá»‰ ra ráº±ng Trá»™n & Káº¿t há»£p hoáº¡t Ä‘á»™ng Ã­t nháº¥t lÃ  tá»‘t nhÆ° viá»‡c sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p NAS dá»±a trÃªn tÃ¬m kiáº¿m tiáº¿n hÃ³a [48] nhÆ° Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi OFA [9].

TÃ³m láº¡i, chÃºng tÃ´i tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng Trá»™n & Káº¿t há»£p lÃ  má»™t heuristic Ä‘Æ¡n giáº£n, ráº» vÃ  hiá»‡u quáº£ Ä‘á»ƒ chá»n má»™t mÃ´ hÃ¬nh con cÃ³ hiá»‡u suáº¥t cao cho má»™t ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n nháº¥t Ä‘á»‹nh (Pháº§n 4.1.1 & 4.2). ChÃºng tÃ´i cung cáº¥p thÃªm chi tiáº¿t vÃ  trá»±c giÃ¡c trong Phá»¥ lá»¥c D.1.

3.4 Triá»ƒn khai
Thiáº¿t káº¿ cá»§a MatFormer cÃ³ lá»£i cho cáº£ khá»‘i lÆ°á»£ng cÃ´ng viá»‡c tÄ©nh vÃ  Ä‘á»™ng:

Khá»‘i lÆ°á»£ng cÃ´ng viá»‡c TÄ©nh Äá»ƒ má»Ÿ rá»™ng vÃ­ dá»¥ vá» cÃ¡c mÃ´ hÃ¬nh Llama-2 trong Pháº§n 1, má»™t thiáº¿t láº­p triá»ƒn khai cÃ³ thá»ƒ cÃ³ ngÃ¢n sÃ¡ch Ä‘á»™ trá»… Ä‘á»ƒ há»— trá»£ mÃ´ hÃ¬nh Llama 40B tham sá»‘, nhÆ°ng chá»‰ cÃ³ thá»ƒ lÆ°u trá»¯ má»™t biáº¿n thá»ƒ 34B vÃ¬ mÃ´ hÃ¬nh lá»›n hÆ¡n tiáº¿p theo (70B) cÃ³ Ä‘á»™ trá»… cao hÆ¡n Ä‘Ã¡ng ká»ƒ. Huáº¥n luyá»‡n má»™t tham sá»‘ 40B tá»« Ä‘áº§u sáº½ yÃªu cáº§u 4.8âˆ—1023 FLOP, khi huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh 34B vÃ  70B Ä‘Ã£ tá»‘n 4.08âˆ—1023 vÃ  8.4âˆ—1023 FLOP tÆ°Æ¡ng á»©ng. VÃ¬ váº­y, ngÆ°á»i ta sáº½ cáº§n pháº£i cháº¥p nháº­n má»™t mÃ´ hÃ¬nh Ã­t chÃ­nh xÃ¡c hÆ¡n máº·c dÃ¹ cÃ³ ngÃ¢n sÃ¡ch Ä‘á»™ trá»… lá»›n hÆ¡n. Vá»›i Matformer, ngÆ°á»i ta cÃ³ thá»ƒ cÃ³ Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh 40B cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao vá»›i 0 FLOP huáº¥n luyá»‡n bá»• sung. ChÃ­nh xÃ¡c hÆ¡n, Ä‘á»‘i vá»›i khá»‘i lÆ°á»£ng cÃ´ng viá»‡c tÄ©nh, trong Ä‘Ã³ tÃ i nguyÃªn tÃ­nh toÃ¡n Ä‘Æ°á»£c biáº¿t trÆ°á»›c vÃ  cÃ¡c Ä‘áº§u vÃ o váº«n tÆ°Æ¡ng Ä‘á»‘i tÆ°Æ¡ng tá»± vá» Ä‘á»™ khÃ³, ngÆ°á»i ta cÃ³ thá»ƒ chá»n mÃ´ hÃ¬nh con tÄ©nh chÃ­nh xÃ¡c nháº¥t cho cÃ¡c rÃ ng buá»™c sá»­ dá»¥ng Trá»™n & Káº¿t há»£p.

--- TRANG 6 ---
Khá»‘i lÆ°á»£ng cÃ´ng viá»‡c Äá»™ng Äá»‘i vá»›i khá»‘i lÆ°á»£ng cÃ´ng viá»‡c Ä‘á»™ng, trong Ä‘Ã³ tÃ i nguyÃªn tÃ­nh toÃ¡n hoáº·c Ä‘á»™ khÃ³ Ä‘áº§u vÃ o thay Ä‘á»•i ngay láº­p tá»©c, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh MatFormer phá»• quÃ¡t Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘á»™ng mÃ´ hÃ¬nh con tá»‘i Æ°u cho má»—i token hoáº·c truy váº¥n. Äiá»u nÃ y hoáº¡t Ä‘á»™ng Ä‘áº·c biá»‡t tá»‘t cho MatFormer vÃ¬ táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh con Ä‘Æ°á»£c trÃ­ch xuáº¥t cÃ³ tÃ­nh nháº¥t quÃ¡n hÃ nh vi cao vá»›i mÃ´ hÃ¬nh MatFormer phá»• quÃ¡t (Pháº§n 4.1) â€“ giáº£m thiá»ƒu sá»± trÃ´i dáº¡t qua cÃ¡c dá»± Ä‘oÃ¡n tá»« cÃ¡c mÃ´ hÃ¬nh con khÃ¡c nhau. ChÃºng tÃ´i Ä‘o tÃ­nh nháº¥t quÃ¡n giá»¯a hai mÃ´ hÃ¬nh táº¡o sinh lÃ  tá»· lá»‡ pháº§n trÄƒm token khá»›p Ä‘Æ°á»£c táº¡o ra bá»Ÿi chÃºng cho cÃ¹ng má»™t tiá»n tá»‘ hoáº·c sá»­ dá»¥ng phÃ¢n ká»³ KL cá»§a Ä‘áº§u ra mÃ´ hÃ¬nh nhá» hÆ¡n vá»›i Ä‘áº§u ra mÃ´ hÃ¬nh lá»›n hÆ¡n â€“ Ä‘iá»u nÃ y tÃ­nh Ä‘áº¿n cÃ¡c chiáº¿n lÆ°á»£c láº¥y máº«u tiá»m nÄƒng trong giáº£i mÃ£. TÃ­nh nháº¥t quÃ¡n cao nÃ y dáº«n Ä‘áº¿n tÄƒng tá»‘c thá»i gian suy luáº­n vÆ°á»£t trá»™i cho cÃ¡c ká»¹ thuáº­t nhÆ° giáº£i mÃ£ suy Ä‘oÃ¡n [39] (Pháº§n 4.1.1) vÃ  cÃ³ thá»ƒ há»— trá»£ trong viá»‡c giáº£m trÃ´i dáº¡t dá»± Ä‘oÃ¡n giá»¯a cÃ¡c triá»ƒn khai Ä‘a ná»n táº£ng. ChÃºng tÃ´i cÅ©ng chá»‰ ra ráº±ng tÃ­nh nháº¥t quÃ¡n mÃ´ hÃ¬nh cao hÆ¡n cÅ©ng há»— trá»£ báº£o tá»“n cáº¥u trÃºc khÃ´ng gian metric trong cÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a (Pháº§n 4.2.2). HÆ¡n ná»¯a, vá»›i kiáº¿n trÃºc lá»“ng nhau cá»§a MatFormer, colocation mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»‡u quáº£ bá»™ nhá»› hÆ¡n.

4 ThÃ­ nghiá»‡m
Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ thá»±c nghiá»‡m MatFormer qua cÃ¡c phÆ°Æ¡ng thá»©c (ngÃ´n ngá»¯ trong Pháº§n 4.1 vÃ  thá»‹ giÃ¡c trong Pháº§n 4.2) vÃ  cÃ¡c lá»›p mÃ´ hÃ¬nh (giáº£i mÃ£ vÃ  mÃ£ hÃ³a). ChÃºng tÃ´i chá»©ng minh triá»ƒn khai Ä‘Ã n há»“i cá»§a cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn MatFormer (Pháº§n 4.1.1 & 4.2) cho cÃ¡c nhiá»‡m vá»¥ tá»« Ä‘Ã¡nh giÃ¡ táº¡o sinh má»™t láº§n Ä‘áº¿n truy xuáº¥t hÃ¬nh áº£nh thÃ­ch á»©ng. NgoÃ i ra, chÃºng tÃ´i cÅ©ng Ä‘iá»u tra hÃ nh vi má»Ÿ rá»™ng Ä‘Ã¡ng tin cáº­y [29] cá»§a cÃ¡c mÃ´ hÃ¬nh MatFormer (Pháº§n 4.1.2).

4.1 MatLM: MÃ´ hÃ¬nh NgÃ´n ngá»¯ MatFormer
CÃ i Ä‘áº·t ThÃ­ nghiá»‡m: ChÃºng tÃ´i xÃ¢y dá»±ng MÃ´ hÃ¬nh NgÃ´n ngá»¯ chá»‰ giáº£i mÃ£ dá»±a trÃªn MatFormer â€“ MatLM â€“ vÃ  so sÃ¡nh chÃºng vá»›i cÃ¡c Ä‘á»‘i tÃ¡c Transformer vanilla tÆ°Æ¡ng á»©ng (LM) [41]. Äá»‘i vá»›i má»—i mÃ´ hÃ¬nh MatLM vá»›i dmodel cá»‘ Ä‘á»‹nh, chÃºng tÃ´i tá»‘i Æ°u hÃ³a cho g= 4 má»©c Ä‘á»™ chi tiáº¿t lá»“ng nhau Ä‘Æ°á»£c biá»ƒu thá»‹ bá»Ÿi tá»· lá»‡ FFN cá»§a {0.5,1,2,4}â€“ tá»©c lÃ  chá»‰ cÃ³ kÃ­ch thÆ°á»›c biá»ƒu diá»…n áº©n cá»§a khá»‘i FFN thay Ä‘á»•i. ChÃºng tÃ´i kÃ½ hiá»‡u cÃ¡c mÃ´ hÃ¬nh con nÃ y lÃ  MatLM â€“ {S, M, L, XL} theo thá»© tá»± tÄƒng dáº§n cá»§a kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  gá»i MatLM-XL lÃ  MatLM phá»• quÃ¡t.

Äá»‘i vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Transformer vanilla vá»›i cÃ¡c kiáº¿n trÃºc cÃ³ thá»ƒ so sÃ¡nh. NghÄ©a lÃ , Ä‘á»‘i vá»›i má»—i MatLM, chÃºng tÃ´i huáº¥n luyá»‡n 4 mÃ´ hÃ¬nh cÆ¡ sá»Ÿ riÃªng biá»‡t vá»›i tá»· lá»‡ FFN cá»§a {0.5,1,2,4} Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  Baseline â€“ {S, M, L, XL}. NgoÃ i ra, chÃºng tÃ´i Ä‘iá»u chá»‰nh OFA [9] vÃ  DynaBERT [27] cho thiáº¿t láº­p mÃ´ hÃ¬nh ngÃ´n ngá»¯ cá»§a chÃºng tÃ´i, vÃ  so sÃ¡nh nhá»¯ng cÃ¡i Ä‘Ã³ vá»›i MatFormer á»Ÿ cÃ¹ng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ nhá»¯ng mÃ´ hÃ¬nh nÃ y trÃªn máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh trÃªn 25 nhiá»‡m vá»¥ tiáº¿ng Anh [8,22,3]. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng khÃ´ng cÃ³ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n bá»• sung nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y so vá»›i cÃ¡c Baseline Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p. Vui lÃ²ng xem Phá»¥ lá»¥c B Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t vá» huáº¥n luyá»‡n, Ä‘Æ°á»ng cÆ¡ sá»Ÿ, Ä‘Ã¡nh giÃ¡ vÃ  cÃ¡c táº­p dá»¯ liá»‡u.

[HÃ¬nh 2 Ä‘Æ°á»£c giá»¯ nguyÃªn vá»›i cÃ¡c biá»ƒu Ä‘á»“ vá» máº¥t mÃ¡t xÃ¡c thá»±c, Ä‘Ã¡nh giÃ¡ má»™t láº§n vÃ  tÃ­nh nháº¥t quÃ¡n]

HÃ¬nh 2: Äiá»ƒm máº¥t mÃ¡t xÃ¡c thá»±c & Ä‘Ã¡nh giÃ¡ downstream má»™t láº§n cho mÃ´ hÃ¬nh MatLM 850M & cÃ¡c mÃ´ hÃ¬nh cÆ¡ sá»Ÿ. Trá»™n & Káº¿t há»£p giÃºp táº¡o ra cÃ¡c mÃ´ hÃ¬nh chÃ­nh xÃ¡c vÃ  nháº¥t quÃ¡n hÆ¡n tá»« MatLM náº±m trÃªn Ä‘Æ°á»ng cong hiá»‡u suáº¥t-so vá»›i-tÃ­nh toÃ¡n Ä‘Æ°á»£c bao phá»§ bá»Ÿi cÃ¡c mÃ´ hÃ¬nh con Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a rÃµ rÃ ng.

Káº¿t quáº£ so sÃ¡nh vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ: Äá»ƒ thá»ƒ hiá»‡n hiá»‡u quáº£ cá»§a MatFormer so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh MatLM 850M vá»›i cÃ¡c Ä‘á»‘i tÃ¡c cÆ¡ sá»Ÿ tÆ°Æ¡ng á»©ng trong HÃ¬nh 2.

Tá»•ng thá»ƒ, trong HÃ¬nh 2a vÃ  2b chÃºng tÃ´i quan sÃ¡t táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh con má»©c Ä‘á»™ chi tiáº¿t cá»§a MatLM vÆ°á»£t trá»™i hÆ¡n cÃ¡c Ä‘á»‘i tÃ¡c cÆ¡ sá»Ÿ cá»§a chÃºng. Cá»¥ thá»ƒ, chÃºng tÃ´i tháº¥y ráº±ng DynaBERT thá»ƒ hiá»‡n má»™t khoáº£ng cÃ¡ch log perplexity Ä‘Ã¡ng ká»ƒ 0.01 so vá»›i MatFormer trÃªn mÃ´ hÃ¬nh 850M. LÃ½ do cÆ¡ báº£n lÃ  DynaBERT sá»­ dá»¥ng tá»‘i Æ°u hÃ³a chung cá»§a táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t, dáº«n Ä‘áº¿n Ã­t cáº­p nháº­t gradient hÆ¡n vÃ  do Ä‘Ã³ hiá»‡u suáº¥t dÆ°á»›i tá»‘i Æ°u so vá»›i MatFormer. DynaBERT sáº½ yÃªu cáº§u hÆ¡n 15% tÃ­nh toÃ¡n bá»• sung Ä‘á»ƒ hoáº¡t Ä‘á»™ng tá»‘t nhÆ° MatLM. OFA, tÆ°Æ¡ng tá»± nhÆ° MatFormer, duy trÃ¬ má»™t mÃ´ hÃ¬nh phá»• quÃ¡t duy nháº¥t nhÆ°ng

--- TRANG 7 ---
sá»­ dá»¥ng láº¥y máº«u máº¡ng con ngáº«u nhiÃªn trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cá»§a nÃ³. Äiá»u nÃ y dáº«n Ä‘áº¿n viá»‡c láº¥y máº«u Ã­t mÃ´ hÃ¬nh gáº§n vá»›i má»©c Ä‘á»™ chi tiáº¿t S vÃ  XL, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m hÆ¡n trong cháº¿ Ä‘á»™ nÃ y. Khoáº£ng cÃ¡ch hiá»‡u suáº¥t thá»ƒ hiá»‡n nhÆ° má»™t Ä‘Æ°á»ng cong máº¥t mÃ¡t hÃ¬nh chuÃ´ng (HÃ¬nh 2a), lÃ m ná»•i báº­t nhá»¯ng thiáº¿u sÃ³t cá»§a OFA trong viá»‡c xá»­ lÃ½ sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a duy trÃ¬ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh phá»• quÃ¡t (XL) vÃ  tÃ­nh Ä‘Ã n há»“i mÃ´ hÃ¬nh. NgoÃ i ra, viá»‡c huáº¥n luyá»‡n OFA Ä‘Ã²i há»i chiáº¿n lÆ°á»£c NAS phá»©c táº¡p Ä‘á»ƒ lá»±a chá»n mÃ´ hÃ¬nh con tá»‘i Æ°u. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng NAS á»Ÿ quy mÃ´ lá»›n lÃ  tá»‘n kÃ©m vÃ  cÃ³ lá»—i, mÃ  chÃºng tÃ´i tháº£o luáº­n thÃªm trong Phá»¥ lá»¥c D.2. ChÃºng tÃ´i giá»›i thiá»‡u ngÆ°á»i Ä‘á»c Ä‘áº¿n Phá»¥ lá»¥c B.4 Ä‘á»ƒ tháº£o luáº­n chi tiáº¿t hÆ¡n vá» hiá»‡u suáº¥t MatFormer so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ, vÃ  nhá»¯ng Æ°u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a má»—i phÆ°Æ¡ng phÃ¡p.

4.1.1 Suy luáº­n ÄÃ n há»“i vá»›i MatLM
CÃ¡c mÃ´ hÃ¬nh con MatLM chÃ­nh xÃ¡c cho má»i rÃ ng buá»™c miá»…n phÃ­ vá»›i Trá»™n & Káº¿t há»£p. Trá»™n & Káº¿t há»£p cho phÃ©p MatLM cung cáº¥p cÃ¡c mÃ´ hÃ¬nh chÃ­nh xÃ¡c cho báº¥t ká»³ rÃ ng buá»™c tÃ­nh toÃ¡n nÃ o giá»¯a S vÃ  XL, ngoÃ i cÃ¡c má»©c Ä‘á»™ chi tiáº¿t cá»‘ Ä‘á»‹nh {S, M, L, XL}. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a Trá»™n & Káº¿t há»£p trÃªn mÃ´ hÃ¬nh MatLM 850M tham sá»‘, so sÃ¡nh máº¥t mÃ¡t xÃ¡c thá»±c vÃ  hiá»‡u suáº¥t downstream vá»›i cÃ¡c mÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p {S, M, L, XL}. HÃ¬nh 2a chá»©ng minh ráº±ng Trá»™n & Káº¿t há»£p Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c Ä‘Ã¡nh Ä‘á»•i máº¥t mÃ¡t-so vá»›i-tÃ­nh toÃ¡n tá»‘i Æ°u mÃ  khÃ´ng cÃ³ chi phÃ­ bá»• sung. NgoÃ i ra, cÃ¡c Ä‘Ã¡nh giÃ¡ downstream trong HÃ¬nh 2b cá»§ng cá»‘ xu hÆ°á»›ng nÃ y. Trong cÃ¡c tÃ¬nh huá»‘ng triá»ƒn khai chá»‰ cÃ³ 55% tÃ i nguyÃªn tÃ­nh toÃ¡n cho má»™t mÃ´ hÃ¬nh MatLM-XL, má»™t mÃ´ hÃ¬nh con Trá»™n & Káº¿t há»£p xáº¥p xá»‰ hiá»‡u suáº¥t cá»§a XL chá»‰ vá»›i khoáº£ng 1% giáº£m Ä‘á»™ chÃ­nh xÃ¡c, so vá»›i 2% giáº£m khi sá»­ dá»¥ng mÃ´ hÃ¬nh MatLM-M. Äiá»u nÃ y lÃ m ná»•i báº­t hiá»‡u quáº£ cá»§a Trá»™n & Káº¿t há»£p trong viá»‡c táº¡o ra nhiá»u mÃ´ hÃ¬nh tá»‘i Æ°u, nhÆ° Ä‘Æ°á»£c minh há»a bá»Ÿi cÃ¡c trÆ°á»ng há»£p Ä‘Æ°á»£c chá»n dá»c theo cÃ¡c Ä‘Æ°á»ng cong hiá»‡u suáº¥t.

ChÃºng tÃ´i thá»­ nghiá»‡m vá»›i má»™t sá»‘ heuristic Ä‘á»ƒ chá»n máº¡ng con tá»‘t nháº¥t, nhÆ°ng liÃªn tá»¥c quan sÃ¡t tháº¥y ráº±ng viá»‡c dáº§n dáº§n sá»­ dá»¥ng cÃ¡c má»©c Ä‘á»™ chi tiáº¿t lá»›n hÆ¡n trong cÃ¡c lá»›p sÃ¢u hÆ¡n hoáº¡t Ä‘á»™ng tá»‘t nháº¥t (Pháº§n 3.3). ChÃºng tÃ´i tháº¥y ráº±ng heuristic nÃ y tá»‘t hÆ¡n cÃ¡c ká»¹ thuáº­t tÃ¬m kiáº¿m dá»±a trÃªn tiáº¿n hÃ³a [48], Ä‘Æ°á»£c sá»­ dá»¥ng trong OFA [9] trong HÃ¬nh 2a & 2b. ChÃºng tÃ´i cÅ©ng tháº¥y ráº±ng viá»‡c Ã¡p dá»¥ng NAS cho MatFormer khÃ´ng mang láº¡i lá»£i Ã­ch so vá»›i Trá»™n & Káº¿t há»£p trong HÃ¬nh 6. ChÃºng tÃ´i tháº£o luáº­n thÃªm chi tiáº¿t vá» Trá»™n & Káº¿t há»£p trong Phá»¥ lá»¥c D.1.

CÃ¡c mÃ´ hÃ¬nh con MatLM tÄƒng tá»‘c giáº£i mÃ£ suy Ä‘oÃ¡n. Giáº£i mÃ£ suy Ä‘oÃ¡n táº­n dá»¥ng má»™t LM nháº¹ chÃ­nh xÃ¡c nhÆ° má»™t mÃ´ hÃ¬nh nhÃ¡p Ä‘á»ƒ táº¡o ra má»™t vÃ i token tá»± há»“i quy, tiáº¿p theo lÃ  xÃ¡c minh nhá»¯ng báº£n nhÃ¡p nÃ y vá»›i má»™t mÃ´ hÃ¬nh lá»›n hÆ¡n thÃ´ng qua giáº£i mÃ£ song song trÃªn cÃ¡c token Ä‘Æ°á»£c táº¡o ra. Khi báº£n nhÃ¡p khÃ´ng chÃ­nh xÃ¡c, mÃ´ hÃ¬nh nhÃ¡p Ä‘Æ°á»£c quay láº¡i vÃ  Ä‘áº·t láº¡i thÃ nh Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh lá»›n hÆ¡n. Äiá»u nÃ y dáº«n Ä‘áº¿n tÄƒng tá»‘c suy luáº­n Ä‘Ã¡ng ká»ƒ cho cÃ¹ng Ä‘á»™ chÃ­nh xÃ¡c nhÆ° mÃ´ hÃ¬nh lá»›n. ChÃºng tÃ´i hÆ°á»›ng ngÆ°á»i Ä‘á»c Ä‘áº¿n bÃ i bÃ¡o gá»‘c Ä‘á»ƒ giáº£i thÃ­ch chi tiáº¿t hÆ¡n [39].

Sá»± cháº­m láº¡i cá»§a thuáº­t toÃ¡n nÃ y báº¯t nguá»“n tá»« cÃ¡c trÆ°á»ng há»£p mÃ  dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh nhá» hÆ¡n khÃ´ng Ä‘á»“ng Ã½ vá»›i mÃ´ hÃ¬nh lá»›n hÆ¡n. Má»™t mÃ´ hÃ¬nh nhÃ¡p nháº¥t quÃ¡n hÆ¡n Ä‘Ã¡ng ká»ƒ vá»›i mÃ´ hÃ¬nh xÃ¡c minh lá»›n hÆ¡n sáº½ dáº«n Ä‘áº¿n Ã­t quay láº¡i cÃ¡c dá»± Ä‘oÃ¡n nhÃ¡p hÆ¡n vÃ  do Ä‘Ã³ Ä‘á»™ trá»… tháº¥p hÆ¡n. NhÆ° tháº¥y trong HÃ¬nh 2c, cÃ¡c mÃ´ hÃ¬nh con MatLM cÃ³ thá»ƒ nháº¥t quÃ¡n hÆ¡n Ä‘áº¿n 11.5% so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ vá»›i mÃ´ hÃ¬nh XL tÆ°Æ¡ng á»©ng cá»§a chÃºng. Khoáº£ng cÃ¡ch Ä‘Ã¡ng ká»ƒ váº«n tá»“n táº¡i ngay cáº£ trong biáº¿n thá»ƒ phÃ¢n ká»³ KL cá»§a tÃ­nh nháº¥t quÃ¡n vá»›i Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh XL (xem HÃ¬nh 7 trong Phá»¥ lá»¥c). TÃ­nh nháº¥t quÃ¡n Ä‘Æ°á»£c cáº£i thiá»‡n nÃ y cÃ¹ng vá»›i nhu cáº§u chá»‰ má»™t mÃ´ hÃ¬nh phá»• quÃ¡t duy nháº¥t Ä‘áº·t MatLM á»Ÿ vá»‹ trÃ­ thuáº­n lá»£i Ä‘á»ƒ cáº£i thiá»‡n cÃ¡c ká»¹ thuáº­t yÃªu cáº§u mÃ´ hÃ¬nh nhÃ¡p vÃ  xÃ¡c minh nhÆ° giáº£i mÃ£ suy Ä‘oÃ¡n.

Báº£ng 2: TÄƒng tá»‘c thá»i gian suy luáº­n so vá»›i má»™t mÃ´ hÃ¬nh 850M tiÃªu chuáº©n thÃ´ng qua giáº£i mÃ£ suy Ä‘oÃ¡n sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh nhÃ¡p 393M (S) vÃ  mÃ´ hÃ¬nh xÃ¡c minh 850M (XL).

Giáº£i mÃ£ Suy Ä‘oÃ¡n LAMBADA TriviaQA
Baseline 1.10Ã— 1.08Ã—
MatLM 1.14Ã— 1.11Ã—
+chia sáº» bá»™ nhá»› Ä‘á»‡m attention 1.16Ã— 1.14Ã—

Báº£ng 2 hiá»ƒn thá»‹ tÄƒng tá»‘c thá»i gian suy luáº­n tá»« giáº£i mÃ£ suy Ä‘oÃ¡n sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh con S vÃ  XL cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ 850M Ä‘á»ƒ soáº¡n nhÃ¡p vÃ  xÃ¡c minh tÆ°Æ¡ng á»©ng. Giáº£i mÃ£ suy Ä‘oÃ¡n vá»›i cÃ¡c LM cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p dáº«n Ä‘áº¿n tÄƒng tá»‘c lÃªn Ä‘áº¿n 10% so vá»›i giáº£i mÃ£ tá»± há»“i quy tiÃªu chuáº©n cá»§a mÃ´ hÃ¬nh 850M-XL. NhÆ°ng giáº£i mÃ£ suy Ä‘oÃ¡n dá»±a trÃªn MatLM nhanh hÆ¡n Ä‘áº¿n 6% so vá»›i giáº£i mÃ£ suy Ä‘oÃ¡n truyá»n thá»‘ng. TÄƒng tá»‘c bá»• sung nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho chá»§ yáº¿u cho báº£n cháº¥t nháº¥t quÃ¡n hÆ¡n cá»§a cÃ¡c mÃ´ hÃ¬nh soáº¡n nhÃ¡p vÃ  xÃ¡c minh dá»±a trÃªn MatLM vÃ  Ä‘Æ°á»£c tÄƒng cÆ°á»ng thÃªm bá»Ÿi kháº£ nÄƒng chia sáº» bá»™ nhá»› Ä‘á»‡m attention qua cÃ¡c mÃ´ hÃ¬nh tá»« MatLM mÃ  khÃ´ng kháº£ thi cho cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ (xem Phá»¥ lá»¥c C.1). Cuá»‘i cÃ¹ng, MatLM giáº£m thÃªm chi phÃ­ bá»™ nhá»› cho suy luáº­n báº±ng cÃ¡ch loáº¡i bá» nhu cáº§u cÃ³ hai mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh triá»ƒn khai háº¡n cháº¿ tÃ i nguyÃªn.

--- TRANG 8 ---
4.1.2 MatLM Má»Ÿ rá»™ng cÅ©ng tá»‘t nhÆ° Transformer LMs Vanilla

[HÃ¬nh 3 Ä‘Æ°á»£c giá»¯ nguyÃªn vá»›i cÃ¡c biá»ƒu Ä‘á»“ vá» máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘á»™ chÃ­nh xÃ¡c cho cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau]

HÃ¬nh 3: ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh MatLM chá»‰ giáº£i mÃ£ khÃ¡c nhau á»Ÿ má»™t dáº£i kÃ­ch thÆ°á»›c tá»« 78M Ä‘áº¿n 850M tham sá»‘ vÃ  quan sÃ¡t xu hÆ°á»›ng má»Ÿ rá»™ng cá»§a táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t (S, M, L, XL) cho máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ downstream má»™t láº§n. ChÃºng tÃ´i tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh MatLM-XL qua cÃ¡c quy mÃ´ báº¯t chÆ°á»›c xu hÆ°á»›ng huáº¥n luyá»‡n cá»§a cÃ¡c mÃ´ hÃ¬nh Baseline-XL. ThÃº vá»‹ lÃ  chÃºng tÃ´i cÅ©ng lÆ°u Ã½ ráº±ng máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘Ã¡nh giÃ¡ downstream tuÃ¢n theo xu hÆ°á»›ng má»Ÿ rá»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh XL qua táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t.

BÃ¢y giá» chÃºng tÃ´i Ä‘Ã£ thiáº¿t láº­p ráº±ng má»™t mÃ´ hÃ¬nh MatLM 850M vÃ  cÃ¡c mÃ´ hÃ¬nh con cá»§a nÃ³ Ã­t nháº¥t lÃ  chÃ­nh xÃ¡c nhÆ° cÃ¡c LM Transformer cÆ¡ sá»Ÿ, chÃºng tÃ´i muá»‘n kiá»ƒm tra kháº£ nÄƒng má»Ÿ rá»™ng cá»§a viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh MatLM. VÃ¬ váº­y, chÃºng tÃ´i nghiÃªn cá»©u cÃ¡c tÃ­nh cháº¥t má»Ÿ rá»™ng [29,25] cá»§a MatLMs vÃ  so sÃ¡nh chÃºng vá»›i cÃ¡c LM Transformer cÆ¡ sá»Ÿ vanilla Ä‘Æ°á»£c huáº¥n luyá»‡n cho cÃ¹ng sá»‘ lÆ°á»£ng token. ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh tá»« 78M Ä‘áº¿n 850M tham sá»‘ trÃªn 10B Ä‘áº¿n 80B token (trÃªn má»—i má»©c Ä‘á»™ chi tiáº¿t) vÃ  váº½ máº¥t mÃ¡t xÃ¡c thá»±c cho MatLM â€“ {S, M, L, XL} so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p trong HÃ¬nh 9.

Báº£ng 3: CÃ¡c tham sá»‘ Ä‘Æ°á»£c khá»›p cho phÆ°Æ¡ng trÃ¬nh má»Ÿ rá»™ng: Loss (N, D ) =aÂ·(ND )b+c

a b c
Baseline 14.08 -0.10 0.89
Matformer 21.60 -0.13 1.33

Äáº§u tiÃªn, trong HÃ¬nh 3a, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh MatLM-XL qua cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh má»Ÿ rá»™ng Ä‘Ã¡ng tin cáº­y nhÆ° cÃ¡c LM Baseline-XL cho máº¥t mÃ¡t so vá»›i sá»‘ lÆ°á»£ng tham sá»‘. HÃ¬nh 3b thÃº vá»‹ cho tháº¥y ráº±ng táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t {S, M, L, XL}, cá»§a MatLM vÃ  Baseline tuÃ¢n theo cÃ¹ng xu hÆ°á»›ng má»Ÿ rá»™ng. Do Ä‘Ã³, chÃºng tÃ´i khá»›p má»™t luáº­t má»Ÿ rá»™ng theo sá»‘ lÆ°á»£ng tham sá»‘ khÃ´ng nhÃºng (N) vÃ  token huáº¥n luyá»‡n (D) cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh con cÃ³ thá»ƒ cho cáº£ MatLMs vÃ  cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ trong Báº£ng 3. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng cÃ¡c tham sá»‘ Ä‘Æ°á»£c khá»›p cá»±c ká»³ tÆ°Æ¡ng tá»±, gá»£i Ã½ ráº±ng MatLM má»Ÿ rá»™ng tÆ°Æ¡ng tá»± nhÆ° LM Transformer vanilla. Trong HÃ¬nh 3c chÃºng tÃ´i cÅ©ng tháº¥y ráº±ng cÃ¡c Ä‘Ã¡nh giÃ¡ downstream cho MatLM tá»‘t hÆ¡n 0.3% so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ, vá»›i cÃ¡c mÃ´ hÃ¬nh con nhá» hÆ¡n vÆ°á»£t trá»™i hÆ¡n cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ á»Ÿ quy mÃ´ lÃªn Ä‘áº¿n 1.4%. Cuá»‘i cÃ¹ng, HÃ¬nh 9f trong Phá»¥ lá»¥c cho tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh con MatLM nháº¥t quÃ¡n hÆ¡n vá»›i mÃ´ hÃ¬nh XL cá»§a chÃºng so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c cÆ¡ sá»Ÿ qua cÃ¡c quy mÃ´.

ChÃºng tÃ´i lÆ°u Ã½ ráº±ng cÃ¡c luáº­t má»Ÿ rá»™ng khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c cÃ¡ch MatLM Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho nhiá»u mÃ´ hÃ¬nh con vÃ  tháº­m chÃ­ cÃ³ cÃ¡c mÃ´ hÃ¬nh con hiá»‡u suáº¥t cao chÆ°a Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a rÃµ rÃ ng (Pháº§n 4.1.1) ChÃºng tÃ´i Ä‘á»ƒ láº¡i cÃ¡c cÃ´ng thá»©c náº¯m báº¯t nhá»¯ng tinh táº¿ nÃ y cho cÃ´ng viá»‡c tÆ°Æ¡ng lai vÃ  tháº£o luáº­n thÃªm Ä‘iá»u nÃ y trong Phá»¥ lá»¥c E.1. ChÃºng tÃ´i cung cáº¥p káº¿t quáº£ Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c chia theo má»©c Ä‘á»™ chi tiáº¿t trong Phá»¥ lá»¥c E.

[HÃ¬nh 4 Ä‘Æ°á»£c giá»¯ nguyÃªn vá»›i biá»ƒu Ä‘á»“ vá» Ä‘á»™ chÃ­nh xÃ¡c cho cÃ¡c biáº¿n thá»ƒ MatViT]

HÃ¬nh 4: CÃ¡c biáº¿n thá»ƒ MatViT khá»›p hoáº·c vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh ViT tiÃªu chuáº©n trÃªn phÃ¢n loáº¡i ImageNet-1K vÃ  cung cáº¥p cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c trÃ­ch xuáº¥t miá»…n phÃ­ bao quÃ¡t Ä‘Æ°á»ng cong Ä‘á»™ chÃ­nh xÃ¡c-tÃ­nh toÃ¡n thÃ´ng qua Trá»™n & Káº¿t há»£p.

--- TRANG 9 ---
[HÃ¬nh 5 Ä‘Æ°á»£c giá»¯ nguyÃªn vá»›i biá»ƒu Ä‘á»“ vá» Ä‘á»™ chÃ­nh xÃ¡c 1-NN]

HÃ¬nh 5: MatViT báº£n Ä‘á»‹a cho phÃ©p cÃ¡c bá»™ mÃ£ hÃ³a Ä‘Ã n há»“i cho truy xuáº¥t thÃ­ch á»©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng cho tÃ­nh toÃ¡n phÃ­a truy váº¥n thá»i gian thá»±c trong khi duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c máº¡nh trÃªn ImageNet-1K, khÃ´ng giá»‘ng nhÆ° cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ.

4.2 MatViT: Vision Transformers MatFormer
ChÃºng tÃ´i má»Ÿ rá»™ng MatFormer cho cÃ¡c mÃ´ hÃ¬nh mÃ£ hÃ³a thá»‹ giÃ¡c mÃ¡y tÃ­nh dá»±a trÃªn Vision Transformer (ViT) [21]. MatFormer-based ViT (MatViT) cho phÃ©p suy luáº­n Ä‘Ã n há»“i cho cÃ¡c nhiá»‡m vá»¥ cÆ¡ báº£n nhÆ° phÃ¢n loáº¡i hÃ¬nh áº£nh vÃ  truy xuáº¥t. ChÃºng tÃ´i huáº¥n luyá»‡n biáº¿n thá»ƒ MatFormer cá»§a cÃ¡c mÃ´ hÃ¬nh ViT-B/16 vÃ  ViT-L/16 tiÃªu chuáº©n â€“ MatViT-B/16 vÃ  MatViT-L/16 Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i g= 4 má»©c Ä‘á»™ chi tiáº¿t lá»“ng nhau (tá»· lá»‡ FFN cá»§a {0.5,1,2,4}). CÃ¡c mÃ´ hÃ¬nh B/16 Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn ImageNet-1K [50] vá»›i AugReg [57] trong khi cÃ¡c mÃ´ hÃ¬nh L/16 Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn ImageNet-21K [18] tiáº¿p theo lÃ  tinh chá»‰nh trÃªn ImageNet-1K. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng thiáº¿t láº­p huáº¥n luyá»‡n vÃ  siÃªu tham sá»‘ tá»‘i Æ°u cá»§a cÃ¡c biáº¿n thá»ƒ ViT tiÃªu chuáº©n tá»« thÆ° viá»‡n Scenic [16].

4.2.1 PhÃ¢n loáº¡i HÃ¬nh áº£nh
Äá»‘i vá»›i phÃ¢n loáº¡i hÃ¬nh áº£nh, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cáº£ mÃ´ hÃ¬nh ViT & MatViT trÃªn ImageNet-1K. HÃ¬nh 4a cho tháº¥y ráº±ng cÃ¡c má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a rÃµ rÃ ng trong MatViT dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh chÃ­nh xÃ¡c nhÆ° cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p cho B/16. Tuy nhiÃªn Ä‘á»‘i vá»›i L/16, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4b, chÃºng ta tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh MatViT chÃ­nh xÃ¡c hÆ¡n Ä‘áº¿n 0.35% so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ cho cÃ¹ng chi phÃ­ suy luáº­n.

Sau Ä‘Ã³ chÃºng tÃ´i khÃ¡m phÃ¡ viá»‡c sá»­ dá»¥ng MatFormer á»Ÿ cÃ¡c giai Ä‘oáº¡n huáº¥n luyá»‡n khÃ¡c nhau vá»›i má»™t lÆ°á»›i 2Ã—2 cá»§a cÃ¡c cáº·p tiá»n huáº¥n luyá»‡n-tinh chá»‰nh (Báº£ng 7 trong Phá»¥ lá»¥c G.1) vÃ  tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng MatFormer trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n giÃºp mang láº¡i cÃ¡c bá»™ mÃ£ hÃ³a chÃ­nh xÃ¡c vÃ  linh hoáº¡t hÆ¡n cho viá»‡c sá»­ dá»¥ng downstream. HÆ¡n ná»¯a, tinh chá»‰nh sá»­ dá»¥ng MatFormer tÄƒng cÆ°á»ng triá»ƒn khai Ä‘Ã n há»“i tÃ¹y thuá»™c vÃ o cÃ¡c rÃ ng buá»™c hiá»‡n táº¡i thÃ´ng qua Trá»™n & Káº¿t há»£p.

Bá»™ mÃ£ hÃ³a ThÃ­ch á»©ng vá»›i Trá»™n & Káº¿t há»£p. HÆ¡n ná»¯a, Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c mÃ´ hÃ¬nh Trá»™n & Káº¿t há»£p cá»§a chÃºng tÃ´i gáº§n nhÆ° náº±m trÃªn Ä‘Æ°á»ng ná»‘i Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng. Trong cÃ¡c tÃ¬nh huá»‘ng mÃ , cháº³ng háº¡n, má»™t á»©ng dá»¥ng cÃ³ thá»ƒ lÆ°u trá»¯ mÃ´ hÃ¬nh B/16 50M tham sá»‘, MatViT cÃ³ thá»ƒ cung cáº¥p mÃ´ hÃ¬nh chÃ­nh xÃ¡c hÆ¡n 0.8% so vá»›i cÃ¡ch tiáº¿p cáº­n hiá»‡n táº¡i sáº½ lÆ°u trá»¯ mÃ´ hÃ¬nh cÆ¡ sá»Ÿ lá»›n nháº¥t vá»›i â‰¤50M tham sá»‘. Trong quÃ¡ trÃ¬nh triá»ƒn khai, mÃ´ hÃ¬nh MatViT phá»• quÃ¡t cÃ³ thá»ƒ Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› vÃ  tÃ¹y thuá»™c vÃ o cÃ¡c rÃ ng buá»™c tÃ­nh toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t má»™t mÃ´ hÃ¬nh nhá» hÆ¡n thÃ­ch á»©ng Ä‘á»ƒ tá»‘i Ä‘a hÃ³a Ä‘á»™ chÃ­nh xÃ¡c vá»›i cÃ¡c tÃ i nguyÃªn cÃ³ sáºµn táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³. Hiá»‡n táº¡i, chÃºng tÃ´i tÃ¬m cÃ¡c mÃ´ hÃ¬nh Trá»™n & Káº¿t há»£p trÃªn Ä‘Æ°á»ng cong Ä‘á»™ chÃ­nh xÃ¡c-tÃ­nh toÃ¡n thÃ´ng qua suy luáº­n nhanh trÃªn táº­p xÃ¡c thá»±c. Máº·c dÃ¹ tÆ°Æ¡ng Ä‘á»‘i cÃ³ thá»ƒ má»Ÿ rá»™ng, Ä‘iá»u nÃ y chá»‰ ra nhu cáº§u phÃ¢n bá»• ngÃ¢n sÃ¡ch tá»‘i Æ°u qua cÃ¡c lá»›p trong máº¡ng nÆ¡-ron [33].

4.2.2 Truy xuáº¥t HÃ¬nh áº£nh ThÃ­ch á»©ng
Má»¥c tiÃªu cá»§a truy xuáº¥t hÃ¬nh áº£nh lÃ  tÃ¬m nhá»¯ng hÃ¬nh áº£nh tÆ°Æ¡ng tá»± vá» máº·t ngá»¯ nghÄ©a â€“ vÃ­ dá»¥ hÃ¬nh áº£nh tá»« cÃ¹ng má»™t lá»›p â€“ sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n thu Ä‘Æ°á»£c tá»« má»™t bá»™ mÃ£ hÃ³a Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n [13]. CÃ¡ch tiáº¿p cáº­n tiÃªu chuáº©n lÃ  mÃ£ hÃ³a cÃ¡c hÃ¬nh áº£nh cÆ¡ sá»Ÿ dá»¯ liá»‡u cÅ©ng nhÆ° hÃ¬nh áº£nh truy váº¥n vá»›i cÃ¹ng bá»™ mÃ£ hÃ³a vÃ  cháº¡y truy xuáº¥t lÃ¡ng giá»ng gáº§n nháº¥t cho embedding truy váº¥n. Trong khi chÃºng ta cÃ³ thá»ƒ nhÃºng hÃ¬nh áº£nh cÆ¡ sá»Ÿ dá»¯ liá»‡u vá»›i má»™t bá»™ mÃ£ hÃ³a Ä‘áº¯t tiá»n, bá»™ mÃ£ hÃ³a truy váº¥n nÃ³i chung pháº£i lÃ  thá»i gian thá»±c. HÆ¡n ná»¯a, cÃ i Ä‘áº·t mÃ£ hÃ³a truy váº¥n cÃ³ thá»ƒ Ä‘a dáº¡ng, vÃ­ dá»¥ xá»­ lÃ½ trÃªn thiáº¿t bá»‹ so vá»›i cloud, táº£i truy váº¥n biáº¿n Ä‘á»•i vÃ  Ä‘á»™ phá»©c táº¡p truy váº¥n. CÃ¡c giáº£i phÃ¡p hiá»‡n táº¡i cÃ³ má»™t bá»™ mÃ£ hÃ³a cá»‘ Ä‘á»‹nh do Ä‘Ã³ thá»a hiá»‡p vá» Ä‘á»™ chÃ­nh xÃ¡c hoáº·c chi phÃ­ cho cÃ¡c cÃ i Ä‘áº·t khÃ¡c nhau.

Vá»›i báº£n cháº¥t Ä‘Ã n há»“i cá»§a MatViT, nÃ³ lÃ  má»™t á»©ng viÃªn tá»‘t cho bá»™ mÃ£ hÃ³a truy váº¥n. Tuy nhiÃªn, truy xuáº¥t cÅ©ng yÃªu cáº§u ráº±ng cÃ¡c mÃ´ hÃ¬nh con báº£o tá»“n khoáº£ng cÃ¡ch giá»¯a cÆ¡ sá»Ÿ dá»¯ liá»‡u cá»‘ Ä‘á»‹nh (vá»›i bá»™ mÃ£ hÃ³a lá»›n) vÃ  embedding truy váº¥n qua táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t. Náº¿u chÃºng ta chá»‰ sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ViT cÆ¡ sá»Ÿ nhá» hÆ¡n cho mÃ£ hÃ³a truy váº¥n, nhá»¯ng khoáº£ng cÃ¡ch nÃ y khÃ´ng Ä‘Æ°á»£c báº£o tá»“n vÃ  dáº«n Ä‘áº¿n gáº§n nhÆ° 0 Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t (xem HÃ¬nh 5).

--- TRANG 10 ---
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cáº£ bá»™ mÃ£ hÃ³a ViT vÃ  MatViT trÃªn ImageNet-1K cho truy xuáº¥t hÃ¬nh áº£nh. ChÃºng tÃ´i tÃ­nh Ä‘á»™ chÃ­nh xÃ¡c 1-lÃ¡ng giá»ng gáº§n nháº¥t (NN) sá»­ dá»¥ng vector biá»ƒu diá»…n cá»§a token [CLS] (cÅ©ng xem Phá»¥ lá»¥c G.2). HÃ¬nh 5 cho tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh con Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« MatViT cÃ³ thá»ƒ báº£o tá»“n gáº§n Ä‘Ãºng khoáº£ng cÃ¡ch vÃ  cung cáº¥p tÃ­nh linh hoáº¡t Ä‘Ã¡ng ká»ƒ hÆ¡n. VÃ­ dá»¥, vá»›i máº¥t mÃ¡t <0.5% Ä‘á»™ chÃ­nh xÃ¡c, MatViT-L/16 cÃ³ thá»ƒ giáº£m chi phÃ­ tÃ­nh toÃ¡n 40%. Äiá»u nÃ y tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh Trá»™n & Káº¿t há»£p 175M tham sá»‘ trong HÃ¬nh 5b, nhá» hÆ¡n 40% so vá»›i mÃ´ hÃ¬nh XL 300M, vÃ  cÃ³ <0.5% giáº£m Ä‘á»™ chÃ­nh xÃ¡c. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, Ä‘Ã¢y lÃ  káº¿t quáº£ Ä‘áº§u tiÃªn thuá»™c loáº¡i nÃ y vÃ  má»Ÿ ra má»™t loáº¡t rá»™ng cÃ¡c chiáº¿n lÆ°á»£c suy luáº­n thÃ­ch á»©ng cho tÃ¬m kiáº¿m ngá»¯ nghÄ©a quy mÃ´ lá»›n.

5 Káº¿t luáº­n
Trong cÃ´ng trÃ¬nh nÃ y chÃºng tÃ´i Ä‘Ã£ trÃ¬nh bÃ y MatFormer, má»™t kiáº¿n trÃºc Transformer Ä‘Ã n há»“i báº£n Ä‘á»‹a cho phÃ©p huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh phá»• quÃ¡t duy nháº¥t cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t hÃ ng trÄƒm mÃ´ hÃ¬nh con nhá» hÆ¡n chÃ­nh xÃ¡c vá»›i chi phÃ­ bá»• sung báº±ng khÃ´ng táº¡i thá»i Ä‘iá»ƒm triá»ƒn khai. ChÃºng tÃ´i tháº¥y ráº±ng MÃ´ hÃ¬nh NgÃ´n ngá»¯ MatFormer (MatLM) khá»›p perplexity & Ä‘á»™ chÃ­nh xÃ¡c má»™t láº§n cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p. Thá»±c táº¿, MatLM chá»©ng minh má»™t Ä‘Æ°á»ng cong má»Ÿ rá»™ng máº¥t mÃ¡t-so vá»›i-tÃ­nh toÃ¡n thÃº vá»‹ gáº§n nhÆ° Ä‘á»™c láº­p vá»›i má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c huáº¥n luyá»‡n chá»‰ ra khÃ¡i quÃ¡t hÃ³a máº¡nh máº½ cho cÃ¡c mÃ´ hÃ¬nh cá»±c lá»›n. Cuá»‘i cÃ¹ng, cÃ¡c mÃ´ hÃ¬nh con MatFormer cho phÃ©p tÄƒng tá»‘c suy luáº­n Ä‘a dáº¡ng nhÆ° táº¡o tá»± há»“i quy nhanh hÆ¡n vá»›i giáº£i mÃ£ suy Ä‘oÃ¡n vÃ  bá»™ mÃ£ hÃ³a truy váº¥n Ä‘Ã n há»“i cho truy xuáº¥t dÃ y Ä‘áº·c thÃ­ch á»©ng qua cÃ¡c phÆ°Æ¡ng thá»©c. ChÃºng tÃ´i tin ráº±ng viá»‡c Ä‘á»‹nh tuyáº¿n Ä‘á»™ng nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘á»ƒ thay Ä‘á»•i Ä‘á»™ trá»… suy luáº­n [32,40,20], vÃ  phÃ¡t triá»ƒn cÃ¡c tá»‘i Æ°u hÃ³a pháº§n cá»©ng cáº§n thiáº¿t lÃ  má»™t lÄ©nh vá»±c há»©a háº¹n cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

Lá»i cáº£m Æ¡n
ChÃºng tÃ´i biáº¿t Æ¡n Aishwarya P S, Yashas B.L. Samaga, Varun Yerram, Lovish Madaan, Anurag Arnab vÃ¬ há»— trá»£ thiáº¿t láº­p cÃ¡c quy trÃ¬nh huáº¥n luyá»‡n, Matthew Wallingford, Praneeth Netrapalli, Orhan Firat, Rohan Anil, Tom Duerig, Luke Zettlemoyer, Manish Gupta, Rahul Sukthankar vÃ  Jeff Dean vÃ¬ nhá»¯ng tháº£o luáº­n há»¯u Ã­ch, há»— trá»£ vÃ  pháº£n há»“i.

ChÃºng tÃ´i cÅ©ng ghi nháº­n tÃ i nguyÃªn tÃ­nh toÃ¡n vÃ  há»— trá»£ tá»« HYAK táº¡i University of Washington, FAS RC táº¡i Harvard University, Kempner Institute vÃ  má»™t giáº£i thÆ°á»Ÿng tÃ­n dá»¥ng GCP cho khÃ¡m phÃ¡ giai Ä‘oáº¡n Ä‘áº§u cá»§a dá»± Ã¡n nÃ y. Ali Farhadi ghi nháº­n tÃ i trá»£ tá»« cÃ¡c giáº£i thÆ°á»Ÿng NSF IIS 1652052, IIS 1703166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, vÃ  quÃ  táº·ng tá»« Allen Institute for Artificial Intelligence vÃ  Google. Sham Kakade ghi nháº­n tÃ i trá»£ tá»« Office of Naval Research theo giáº£i thÆ°á»Ÿng N00014-22-1-2377. CÃ´ng viá»‡c nÃ y Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n má»™t pháº§n nhá» mÃ³n quÃ  tá»« Chan Zuckerberg Initiative Foundation Ä‘á»ƒ thÃ nh láº­p Kempner Institute for the Study of Natural and Artificial Intelligence. Yulia Tsvetkov ghi nháº­n há»— trá»£ tá»« National Science Foundation theo CAREER Grant No. IIS2142739, cÃ¡c khoáº£n tÃ i trá»£ NSF No. IIS2125201 vÃ  IIS2203097, vÃ  tÃ i trá»£ quÃ  táº·ng tá»« Google, MSR, vÃ  OpenAI. Hannaneh Hajishirzi ghi nháº­n tÃ i trá»£ thÃ´ng qua má»™t mÃ³n quÃ  tá»« Allen Institute for Artificial Intelligence.

TÃ i liá»‡u tham kháº£o
[1] O. Ahia, J. Kreutzer, vÃ  S. Hooker. The low-resource double bind: An empirical study of pruning for low-resource machine translation. arXiv preprint arXiv:2110.03036, 2021.

[2] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.

[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

[4] J. Berant, A. K. Chou, R. Frostig, vÃ  P. Liang. Semantic parsing on freebase from question-answer pairs. In Conference on Empirical Methods in Natural Language Processing, 2013. URL https://api.semanticscholar.org/CorpusID:6401679.

[5] L. Beyer, P. Izmailov, A. Kolesnikov, M. Caron, S. Kornblith, X. Zhai, M. Minderer, M. Tschannen, I. Alabdulmohsin, vÃ  F. Pavetic. Flexivit: One model for all patch sizes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14496â€“14506, 2023.

--- TRANG 11 ---
[6] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, vÃ  Y. Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.

[7] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.

[9] H. Cai, C. Gan, T. Wang, Z. Zhang, vÃ  S. Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.

[10] H. Cai, C. Gan, J. Lin, vÃ  S. Han. Network augmentation for tiny deep learning. arXiv preprint arXiv:2110.08890, 2021.

[11] A. Chavan, Z. Shen, Z. Liu, Z. Liu, K.-T. Cheng, vÃ  E. P. Xing. Vision transformer slimming: Multi-dimension searching in continuous optimization space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4931â€“4941, 2022.

[12] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, vÃ  J. Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

[13] W. Chen, Y. Liu, W. Wang, E. M. Bakker, T. Georgiou, P. Fieguth, L. Liu, vÃ  M. S. Lew. Deep learning for instance retrieval: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, et al. Palm: Scaling language modeling with pathways, 2022.

[15] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, vÃ  O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.

[16] M. Dehghani, A. Gritsenko, A. Arnab, M. Minderer, vÃ  Y. Tay. Scenic: A jax library for computer vision research and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21393â€“21398, 2022.

[17] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480â€“7512. PMLR, 2023.

[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, vÃ  L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248â€“255. Ieee, 2009.

[19] T. Dettmers vÃ  L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pages 7750â€“7774. PMLR, 2023.

[20] D. Ding, A. Mallick, C. Wang, R. Sim, S. Mukherjee, V. Ruhle, L. V. Lakshmanan, vÃ  A. H. Awadallah. Hybrid llm: Cost-efficient and quality-aware query routing. arXiv preprint arXiv:2404.14618, 2024.

[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[22] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, vÃ  C. Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.

--- TRANG 12 ---
[23] M. Grimaldi, L. Mocerino, A. Cipolletta, vÃ  A. Calimera. Dynamic convnets on tiny devices via nested sparsity. IEEE Internet of Things Journal, 10(6):5073â€“5082, 2022.

[24] D. Hendrycks vÃ  K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

[25] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[26] S. Hooker, N. Moorosi, G. Clark, S. Bengio, vÃ  E. Denton. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058, 2020.

[27] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, vÃ  Q. Liu. Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33:9782â€“9793, 2020.

[28] M. Joshi, E. Choi, D. Weld, vÃ  L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601â€“1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.

[29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, vÃ  D. Amodei. Scaling laws for neural language models. 2020.

[30] S. Kim, C. Hooper, T. Wattanawong, M. Kang, R. Yan, H. Genc, G. Dinh, Q. Huang, K. Keutzer, M. W. Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.

[31] T. Kudo vÃ  J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.

[32] S. Kudugunta, Y. Huang, A. Bapna, M. Krikun, D. Lepikhin, M.-T. Luong, vÃ  O. Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. arXiv preprint arXiv:2110.03742, 2021.

[33] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, vÃ  A. Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544â€“5555. PMLR, 2020.

[34] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233â€“30249, 2022.

[35] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, vÃ  S. Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452â€“466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026.

[36] F. Lagunas, E. Charlaix, V. Sanh, vÃ  A. M. Rush. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838, 2021.

[37] G. Lai, Q. Xie, H. Liu, Y. Yang, vÃ  E. Hovy. Race: Large-scale reading comprehension dataset from examinations, 2017.

[38] H. J. Levesque, E. Davis, vÃ  L. Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, page 552â€“561. AAAI Press, 2012. ISBN 9781577355601.

[39] Y. Leviathan, M. Kalman, vÃ  Y. Matias. Fast inference from transformers via speculative decoding. 2023.

--- TRANG 13 ---
[40] M. Li, S. Gururangan, T. Dettmers, M. Lewis, T. Althoff, N. A. Smith, vÃ  L. Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306, 2022.

[41] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, vÃ  N. Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

[42] T. Mihaylov, P. Clark, T. Khot, vÃ  A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.

[43] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, vÃ  J. Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839â€“849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.

[44] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, vÃ  D. Kiela. Adversarial nli: A new benchmark for natural language understanding, 2020.

[45] R. OpenAI. Gpt-4 technical report. arXiv, pages 2303â€“08774, 2023.

[46] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, vÃ  R. FernÃ¡ndez. The lambada dataset: Word prediction requiring a broad discourse context, 2016.

[47] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, vÃ  I. Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492â€“28518. PMLR, 2023.

[48] E. Real, A. Aggarwal, Y. Huang, vÃ  Q. V. Le. Regularized evolution for image classifier architecture search, 2019.

[49] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.

[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211â€“252, 2015.

[51] K. Sakaguchi, R. L. Bras, C. Bhagavatula, vÃ  Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.

[52] M. Salehi, S. Mehta, A. Kusupati, A. Farhadi, vÃ  H. Hajishirzi. Sharcs: Efficient transformers through routing with dynamic width sub-networks. Findings of Empirical Methods in Natural Language Processing, 2023.

[53] V. Sanh, L. Debut, J. Chaumond, vÃ  T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[54] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran, Y. Tay, vÃ  D. Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456â€“17472, 2022.

[55] N. Shazeer vÃ  M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596â€“4604. PMLR, 2018.

[56] D. R. So, W. MaÅ„ke, H. Liu, Z. Dai, N. Shazeer, vÃ  Q. V. Le. Primer: Searching for efficient transformers for language modeling. arXiv preprint arXiv:2109.08668, 2021.

[57] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, vÃ  L. Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.

--- TRANG 14 ---
[58] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[59] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[60] M. Valipour, M. Rezagholizadeh, H. Rajabzadeh, M. Tahaei, B. Chen, vÃ  A. Ghodsi. Sortednet, a place for every network and every network in its place: Towards a generalized solution for training many-in-one neural networks. arXiv preprint arXiv:2309.00255, 2023.

[61] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, vÃ  I. Polosukhin. Attention is all you need. 2023.

[62] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, vÃ  S. R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020.

[63] H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, vÃ  S. Han. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187, 2020.

[64] J. Yu vÃ  T. Huang. Universally slimmable networks and improved training techniques, 2019. URL https://arxiv.org/abs/1903.05134.

[65] J. Yu vÃ  T. S. Huang. Universally slimmable networks and improved training techniques. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1803â€“1811, 2019.

[66] J. Yu, L. Yang, N. Xu, J. Yang, vÃ  T. Huang. Slimmable neural networks. arXiv preprint arXiv:1812.08928, 2018.

[67] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, vÃ  Y. Choi. Hellaswag: Can a machine really finish your sentence?, 2019.

[68] B. Zoph vÃ  Q. V. Le. Neural architecture search with reinforcement learning, 2017.

--- TRANG 15 ---
A TuyÃªn bá»‘ TÃ¡c Ä‘á»™ng Rá»™ng hÆ¡n
TÃ­nh Ä‘Ã n há»“i cá»§a MatFormer cho phÃ©p viá»‡c sá»­ dá»¥ng nÃ³ trong cÃ¡c tÃ¬nh huá»‘ng triá»ƒn khai Ä‘a dáº¡ng. Äiá»u nÃ y giáº£m rÃ o cáº£n cho cÃ¡c nhÃ  thá»±c hÃ nh sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo cÃ¡c tÃ¬nh huá»‘ng triá»ƒn khai cá»§a há». Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng váº«n Ä‘áº¯t Ä‘á», vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n nháº¥t mÃ  chÃºng tÃ´i tháº£o luáº­n Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 256 lÃµi TPU-v4 trong 3 ngÃ y. HÆ¡n ná»¯a, máº·c dÃ¹ chÃºng tÃ´i bÃ¡o cÃ¡o Ä‘iá»ƒm máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ downstream trÃªn nhiá»u nhiá»‡m vá»¥ khÃ¡c nhau, chÃºng tÃ´i thá»«a nháº­n kháº£ nÄƒng MatFormer cÃ³ thá»ƒ cÃ³ tÃ¡c Ä‘á»™ng báº¥t lá»£i Ä‘áº¿n bias [26] hoáº·c cÃ¡c miá»n/ngÃ´n ngá»¯ thiá»ƒu sá»‘ [1].

B Chi tiáº¿t Triá»ƒn khai
B.1 Kiáº¿n trÃºc vÃ  Huáº¥n luyá»‡n
Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i huáº¥n luyá»‡n má»™t dáº£i MatLM khÃ¡c nhau tá»« kÃ­ch thÆ°á»›c 78M Ä‘áº¿n 850M cho 10B-80B token â€“ chÃºng tÃ´i má»Ÿ rá»™ng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh Ä‘á»u vá»›i sá»‘ lÆ°á»£ng token huáº¥n luyá»‡n [25]. Äá»‘i vá»›i má»—i má»©c Ä‘á»™ chi tiáº¿t MatLM, chÃºng tÃ´i cÅ©ng huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh Transformer vanilla cÆ¡ sá»Ÿ tÆ°Æ¡ng á»©ng. NghÄ©a lÃ , Ä‘á»‘i vá»›i má»—i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh chÃºng tÃ´i huáº¥n luyá»‡n Baseline-XL, L, M, S vá»›i dff= 4âˆ—dmodel,2âˆ—dmodel, dmodel, dmodel/2. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh cÃ³ 16 lá»›p, 16 Ä‘áº§u attention, vÃ  tá»· lá»‡ dmodel:dff lÃ  1:4. ChÃºng tÃ´i huáº¥n luyá»‡n tá»« vá»±ng 256k sá»­ dá»¥ng thÆ° viá»‡n SentencePiece [31], sá»­ dá»¥ng Ä‘á»™ dÃ i ngá»¯ cáº£nh tá»‘i Ä‘a 1024 token, vÃ  kÃ­ch thÆ°á»›c lÃ´ 1M token. ChÃºng tÃ´i tiá»n huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh 850M trÃªn 256 chip v3 TPU. ChÃºng tÃ´i cung cáº¥p thÃªm chi tiáº¿t vá» nhá»¯ng mÃ´ hÃ¬nh nÃ y trong Báº£ng 4. Äá»ƒ biáº¿t thÃªm chi tiáº¿t vá» thiáº¿t láº­p huáº¥n luyá»‡n, chÃºng tÃ´i hÆ°á»›ng ngÆ°á»i Ä‘á»c Ä‘áº¿n [58].

Báº£ng 4: Chi tiáº¿t mÃ´ hÃ¬nh cho cÃ¡c quy mÃ´ mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 4.1, vá»›i sá»± phÃ¢n chia sá»‘ lÆ°á»£ng tham sá»‘ tá»•ng, sá»‘ lÆ°á»£ng tham sá»‘ khÃ´ng nhÃºng vÃ  sá»‘ lÆ°á»£ng tham sá»‘ FFN cho má»—i má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh.

Sá»‘ lÆ°á»£ng Tham sá»‘ (Ä‘áº§y Ä‘á»§ / cáº¯t) Tham sá»‘ KhÃ´ng NhÃºng (Ä‘áº§y Ä‘á»§ / cáº¯t) Tham sá»‘ FFN (Ä‘áº§y Ä‘á»§) dmodel N(token)
78M (74M / 72M / 71M) 12.6M (8.4M/6.3M/ 5.3M) 8.4M 256 10B
180M (164M / 157M / 152M) 50M (33.7M/25.3M/21.1M) 33.6M 512 20B
310M (272M / 253M / 244M) 113M (75M/56M/47M) 75.6M 768 30B
463M (397M / 363M / 346M) 201M (134M/100M/84M) 134M 1024 40B
850M (696M / 620M / 582M) 453M (302M/227M/189M) 302M 1536 80B

B.2 ÄÃ¡nh giÃ¡ Downstream
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh LM Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p há»£p 25 nhiá»‡m vá»¥ tiáº¿ng Anh tÆ°Æ¡ng tá»± nhÆ° [8,22,14,3], bao gá»“m:

1. Nhiá»‡m vá»¥ Tráº£ lá»i CÃ¢u há»i SÃ¡ch ÄÃ³ng Miá»n Má»Ÿ: TriviaQA [28], Natural Questions [35], vÃ  WebQuestions [4].
2. Nhiá»‡m vá»¥ Cloze vÃ  hoÃ n thÃ nh: LAMBADA [46], HellaSwag [67], vÃ  StoryCloze [43].
3. Nhiá»‡m vá»¥ kiá»ƒu Winograd: Winograd [38] vÃ  WinoGrande [51].
4. Äá»c hiá»ƒu: RACE [37].
5. LÃ½ luáº­n thÆ°á»ng thá»©c: PIQA [6], ARC [15], vÃ  OpenBookQA [42].
6. SuperGLUE [62]
7. Suy luáº­n ngÃ´n ngá»¯ tá»± nhiÃªn: Adversarial NLI [44].

Äá»‘i vá»›i táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t tÆ°Æ¡ng á»©ng vá»›i má»—i mÃ´ hÃ¬nh, chÃºng tÃ´i trÃ¬nh bÃ y sá»‘ Ä‘Ã¡nh giÃ¡ cÃ¹ng vá»›i máº¥t mÃ¡t log perplexity táº­p phÃ¡t triá»ƒn trÃªn táº¥t cáº£ 25 nhiá»‡m vá»¥ trong Báº£ng 9 Ä‘áº¿n 13.

B.3 Chi tiáº¿t Triá»ƒn khai Baseline
Baseline Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p: CÃ¡c baseline Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u Ä‘á»™c láº­p, trong Ä‘Ã³ má»—i má»©c Ä‘á»™ chi tiáº¿t trong má»™t kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh nháº¥t Ä‘á»‹nh sá»­ dá»¥ng X token Ä‘Æ°á»£c Ä‘á» cáº­p trong Báº£ng 4. VÃ­ dá»¥, Ä‘á»‘i vá»›i mÃ´ hÃ¬nh 850M X = 80B token. Do Ä‘Ã³, Baseline-{S, M, L, XL} xá»­ lÃ½ tá»•ng cá»™ng 4X token. CÃ¡c baseline Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u dáº«n Ä‘áº¿n cÃ¹ng sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng. NghÄ©a lÃ , khÃ´ng cÃ³ cÃ¡ch nÃ o Ä‘á»ƒ trá»±c tiáº¿p cÃ³ Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh con cÃ³ kÃ­ch thÆ°á»›c giá»¯a cÃ¡c má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c huáº¥n luyá»‡n mÃ  khÃ´ng sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p bá»• sung nhÆ° chÆ°ng cáº¥t, cáº¯t tá»‰a, v.v.

--- TRANG 16 ---
OFA: OFA [9] huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh lá»›n tá»« Ä‘áº§u, sau Ä‘Ã³ Ä‘Ã³ng bÄƒng nÃ³ vÃ  báº¯t Ä‘áº§u má»™t láº§n cháº¡y khÃ¡c cá»§a má»™t "mÃ´ hÃ¬nh phá»• quÃ¡t" cÃ³ kÃ­ch thÆ°á»›c tÆ°Æ¡ng tá»± nÆ¡i nÃ³ láº¥y máº«u cÃ¡c máº¡ng con ngáº«u nhiÃªn vÃ  tá»‘i Æ°u hÃ³a chÃºng vá»›i chÆ°ng cáº¥t vÃ  máº¥t mÃ¡t sá»± tháº­t cÆ¡ báº£n. Ã tÆ°á»Ÿng trung tÃ¢m cá»§a cÃ´ng trÃ¬nh nÃ y lÃ  láº¥y máº«u cÃ¡c mÃ´ hÃ¬nh con ngáº«u nhiÃªn vÃ  tá»‘i Æ°u hÃ³a chÃºng. ChÆ°ng cáº¥t lÃ  má»™t thÃ nh pháº§n trá»±c giao cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c thÃªm vÃ o MatFormer. Äá»ƒ sá»­ dá»¥ng tÃ­nh toÃ¡n vÃ  bá»™ nhá»› cÃ³ thá»ƒ so sÃ¡nh vá»›i Baseline (Pháº§n 4.1), chÃºng tÃ´i sá»­a Ä‘á»•i phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ chá»‰ tá»‘i Æ°u hÃ³a w.r.t. máº¥t mÃ¡t sá»± tháº­t cÆ¡ báº£n - nhÆ° cÅ©ng Ä‘Æ°á»£c thá»±c hiá»‡n trong trÆ°á»ng há»£p cá»§a MatFormer. Tá»•ng thá»ƒ, OFA sá»­ dá»¥ng 4X token, giá»‘ng nhÆ° táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t baseline Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u. Trong má»—i bÆ°á»›c, OFA láº¥y máº«u má»™t mÃ´ hÃ¬nh ngáº«u nhiÃªn, trong Ä‘Ã³ má»—i lá»›p cá»§a nÃ³ cÃ³ má»©c Ä‘á»™ chi tiáº¿t {S, M, L, XL} Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn, vÃ  tá»‘i Æ°u hÃ³a nÃ³.

Äá»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c mÃ´ hÃ¬nh con theo má»™t rÃ ng buá»™c chi phÃ­ nháº¥t Ä‘á»‹nh, OFA Ä‘á» xuáº¥t sá»­ dá»¥ng NAS [48] Ä‘á»ƒ tÃ¬m kiáº¿m kiáº¿n trÃºc tá»‘i Æ°u. Cá»¥ thá»ƒ, OFA láº¥y máº«u má»™t táº­p há»£p ngáº«u nhiÃªn cÃ¡c cáº·p (kiáº¿n trÃºc, máº¥t mÃ¡t) tá»« mÃ´ hÃ¬nh phá»• quÃ¡t. Sau Ä‘Ã³, nÃ³ huáº¥n luyá»‡n má»™t bá»™ dá»± Ä‘oÃ¡n trÃªn cáº·p nÃ y, láº¥y kiáº¿n trÃºc lÃ m Ä‘áº§u vÃ o vÃ  dá»± Ä‘oÃ¡n máº¥t mÃ¡t. Cuá»‘i cÃ¹ng, nÃ³ sá»­ dá»¥ng má»™t thÃ³i quen tÃ¬m kiáº¿m dá»±a trÃªn tiáº¿n hÃ³a [48] Ä‘á»ƒ tÃ¬m kiáº¿m kiáº¿n trÃºc tá»‘i Æ°u trong má»™t rÃ ng buá»™c cá»¥ thá»ƒ, sá»­ dá»¥ng bá»™ dá»± Ä‘oÃ¡n trong thÃ³i quen tÃ¬m kiáº¿m cá»§a nÃ³. ThÃªm chi tiáº¿t vá» NAS vÃ  so sÃ¡nh vá»›i MatFormer Trá»™n & Káº¿t há»£p Ä‘Æ°á»£c trÃ¬nh bÃ y trong Pháº§n D.

DynaBERT: DynaBERT [27] cÃ³ g má»©c Ä‘á»™ chi tiáº¿t cá»‘ Ä‘á»‹nh cá»§a cÃ¡c mÃ´ hÃ¬nh con - tÆ°Æ¡ng tá»± nhÆ° MatFormer. TÆ°Æ¡ng tá»± nhÆ° OFA, DynaBERT cÅ©ng sá»­ dá»¥ng máº¥t mÃ¡t chÆ°ng cáº¥t. Äá»ƒ phÃ¢n tÃ­ch cÃ³ thá»ƒ so sÃ¡nh vÃ  duy trÃ¬ tÃ­nh toÃ¡n/bá»™ nhá»› cÃ³ thá»ƒ so sÃ¡nh vá»›i baseline, chÃºng tÃ´i chá»‰ tá»‘i Æ°u hÃ³a DynaBERT wrt máº¥t mÃ¡t sá»± tháº­t cÆ¡ báº£n, tÆ°Æ¡ng tá»± nhÆ° MatFormer.

Sá»± khÃ¡c biá»‡t quan trá»ng giá»¯a DynaBERT vÃ  MatFormer náº±m á»Ÿ phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n cá»§a nÃ³. Trong má»—i bÆ°á»›c, DynaBERT láº¥y 4 lÃ´ dá»¯ liá»‡u, tá»‘i Æ°u hÃ³a má»—i lÃ´ vá»›i má»—i má»©c Ä‘á»™ chi tiáº¿t {S, M, L, XL} vÃ  trung bÃ¬nh nhá»¯ng máº¥t mÃ¡t nÃ y. TÆ°Æ¡ng tá»± nhÆ° Matformer vÃ  OFA, nÃ³ xá»­ lÃ½ tá»•ng cá»™ng 4X token. Máº·c dÃ¹, vÃ¬ nÃ³ tá»‘i Æ°u hÃ³a trÃªn 4 lÃ´ dá»¯ liá»‡u trong má»™t bÆ°á»›c duy nháº¥t, nÃ³ cháº¡y trong má»™t pháº§n tÆ° sá»‘ bÆ°á»›c cá»§a MatFormer hoáº·c OFA, vÃ  cÃ¹ng sá»‘ bÆ°á»›c nhÆ° baseline.

Má»™t sá»± khÃ¡c biá»‡t quan trá»ng giá»¯a Ä‘Ã³ng gÃ³p cá»§a cÃ´ng trÃ¬nh DynaBERT vÃ  MatFormer lÃ  chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m. Trong MatFormer, chÃºng tÃ´i Ä‘Ã£ giá»›i thiá»‡u heuristic Trá»™n & Káº¿t há»£p, tÃ­nh toÃ¡n má»™t mÃ´ hÃ¬nh con tá»‘i Æ°u vá»›i má»™t rÃ ng buá»™c chi phÃ­ mÃ  khÃ´ng cÃ³ báº¥t ká»³ chi phÃ­ nÃ o. NgÆ°á»£c láº¡i, DynaBERT khÃ´ng giá»›i thiá»‡u báº¥t ká»³ chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m nÃ o vÃ  chá»‰ tháº£o luáº­n vá» viá»‡c sá»­ dá»¥ng cÃ¡c má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng lÃ m mÃ´ hÃ¬nh con.

B.4 So sÃ¡nh vá»›i Baseline
á» Ä‘Ã¢y chÃºng tÃ´i thá»±c hiá»‡n má»™t cuá»™c tháº£o luáº­n chi tiáº¿t hÆ¡n vá» so sÃ¡nh giá»¯a MatFormer vÃ  cÃ¡c baseline Ä‘Æ°á»£c sá»­ dá»¥ng trong Pháº§n 4.

1. MatFormer vs Baseline Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u: Äá»ƒ Baseline-{S, M, L, XL} káº¿t há»£p xá»­ lÃ½ 4X token, trong Ä‘Ã³ má»—i má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn X token Ä‘á»™c láº­p. VÃ¬ MatFormer chá»‰ duy trÃ¬ má»™t mÃ´ hÃ¬nh, chÃºng tÃ´i huáº¥n luyá»‡n nÃ³ cho 4X token, trong má»—i bÆ°á»›c láº¥y máº«u má»™t trong MatLM-{S, M, L, XL}. Do Ä‘Ã³, tá»•ng tÃ­nh toÃ¡n vÃ  bá»™ nhá»› Ä‘á»‰nh váº«n giá»‘ng nhÆ° Baseline vÃ¬ chÃºng tÃ´i hiá»‡u quáº£ huáº¥n luyá»‡n MatLM-{S, M, L, XL} cho X token má»—i cÃ¡i â€“ khá»›p vá»›i cáº¥u hÃ¬nh huáº¥n luyá»‡n Baseline. NhÆ°ng vÃ¬ chÃºng tÃ´i duy trÃ¬ má»™t mÃ´ hÃ¬nh duy nháº¥t, cÃ¡c tham sá»‘ Ä‘Æ°á»£c chia sáº» Ä‘Æ°á»£c huáº¥n luyá»‡n lÃªn Ä‘áº¿n 4X token miá»…n phÃ­. VÃ­ dá»¥, má»—i khi chÃºng tÃ´i huáº¥n luyá»‡n má»©c Ä‘á»™ chi tiáº¿t XL/L/M, vÃ¬ má»©c Ä‘á»™ chi tiáº¿t S Ä‘Æ°á»£c chá»©a trong chÃºng, nÃ³ cÅ©ng nháº­n Ä‘Æ°á»£c cáº­p nháº­t gradient. Do Ä‘Ã³, MatLM-S hiá»‡u quáº£ xá»­ lÃ½ 4X token, MatLM-M xá»­ lÃ½ 3X token, MatLM-L xá»­ lÃ½ 2X token, vÃ  MatLM-XL xá»­ lÃ½ X token. ChÃºng ta cÅ©ng cÃ³ thá»ƒ tháº¥y Ä‘iá»u nÃ y chuyá»ƒn Ä‘á»•i thÃ nh Máº¥t mÃ¡t XÃ¡c thá»±c (HÃ¬nh 2a) â€“ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t nhá» hÆ¡n vÆ°á»£t trá»™i hÆ¡n Baseline vá»›i má»™t biÃªn Ä‘á»™ lá»›n. Trong khi má»©c Ä‘á»™ chi tiáº¿t lá»›n hÆ¡n khá»›p vá» hiá»‡u suáº¥t.

2. MatFormer vs DynaBERT: DynaBERT, tÆ°Æ¡ng tá»± nhÆ° MatFormer, huáº¥n luyá»‡n g máº¡ng con lá»“ng nhau, nhÆ°ng khÃ¡c biá»‡t quan trá»ng trong phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n cá»§a nÃ³. NÃ³ thá»±c hiá»‡n tá»‘i Æ°u hÃ³a chung - xá»­ lÃ½ Ä‘á»“ng thá»i 4 lÃ´ dá»¯ liá»‡u - 1 lÃ´ thÃ´ng qua má»—i má»©c Ä‘á»™ chi tiáº¿t {S, M, L, XL}. Máº¥t mÃ¡t cuá»‘i cÃ¹ng Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a lÃ  trung bÃ¬nh cá»§a máº¥t mÃ¡t cá»§a má»—i má»©c Ä‘á»™ chi tiáº¿t. NgÆ°á»£c láº¡i, MatFormer láº¥y máº«u vÃ  tá»‘i Æ°u hÃ³a cÃ¡c má»©c Ä‘á»™ chi tiáº¿t riÃªng láº» trong cÃ¡c bÆ°á»›c riÃªng biá»‡t, dáº«n Ä‘áº¿n gáº¥p bá»‘n láº§n cáº­p nháº­t gradient cho cÃ¹ng má»™t lÆ°á»£ng dá»¯ liá»‡u vÃ  tÃ­nh toÃ¡n. Sá»± khÃ¡c biá»‡t trong phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n nÃ y chuyá»ƒn Ä‘á»•i thÃ nh má»™t khoáº£ng cÃ¡ch hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ: DynaBERT thá»ƒ hiá»‡n má»™t khoáº£ng cÃ¡ch log perplexity Ä‘Ã¡ng ká»ƒ 0.01 so vá»›i MatFormer trÃªn mÃ´ hÃ¬nh 850M. Ngay cáº£ vá»›i 15% tÄƒng tÃ­nh toÃ¡n, DynaBERT váº«n thua kÃ©m hiá»‡u suáº¥t cá»§a MatFormer. HÆ¡n ná»¯a, khÃ´ng giá»‘ng nhÆ° MatFormer, cung cáº¥p tÃ­nh linh hoáº¡t trong viá»‡c lá»±a chá»n kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh con thÃ´ng qua phÆ°Æ¡ng phÃ¡p Trá»™n & Káº¿t há»£p cá»§a nÃ³, DynaBERT thiáº¿u má»™t chiáº¿n lÆ°á»£c láº¥y máº«u chuyÃªn dá»¥ng.

--- TRANG 17 ---
[HÃ¬nh 6 Ä‘Æ°á»£c giá»¯ nguyÃªn]

HÃ¬nh 6: ChÃºng tÃ´i tÃ¬m kiáº¿m cáº¥u hÃ¬nh kiáº¿n trÃºc tá»‘i Æ°u sá»­ dá»¥ng tÃ¬m kiáº¿m tiáº¿n hÃ³a [48] vÃ  vá»›i Trá»™n & Káº¿t há»£p. ChÃºng tÃ´i tháº¥y ráº±ng Trá»™n & Káº¿t há»£p mang láº¡i cÃ¡c kiáº¿n trÃºc náº±m trÃªn Ä‘Æ°á»ng cong pareto-tá»‘i Æ°u, vÃ  Ã­t nháº¥t lÃ  tá»‘t nhÆ° nhá»¯ng cÃ¡i Ä‘Æ°á»£c tÃ¬m tháº¥y báº±ng cÃ¡ch sá»­ dá»¥ng tÃ¬m kiáº¿m tiáº¿n hÃ³a.

3. MatFormer vs OFA: OFA duy trÃ¬ má»™t mÃ´ hÃ¬nh phá»• quÃ¡t duy nháº¥t tÆ°Æ¡ng tá»± nhÆ° MatFormer, nhÆ°ng láº¥y máº«u ngáº«u nhiÃªn cÃ¡c máº¡ng con sao cho má»—i lá»›p lÃ  má»™t trong {S, M, L, XL}. TÆ°Æ¡ng tá»± nhÆ° MatFormer vÃ  DynaBERT, nÃ³ táº­n dá»¥ng viá»‡c xá»­ lÃ½ dá»¯ liá»‡u 4X bá»Ÿi má»™t mÃ´ hÃ¬nh duy nháº¥t. NhÆ°ng do láº¥y máº«u, khÃ´ng Ä‘á»§ mÃ´ hÃ¬nh nhá» hoáº·c lá»›n (gáº§n vá»›i má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh S vÃ  XL) Ä‘Æ°á»£c láº¥y máº«u, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m hÆ¡n trong nhá»¯ng vÃ¹ng nÃ y. NghÄ©a lÃ , láº¥y máº«u cÃ¡c mÃ´ hÃ¬nh tá»« kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh S Ä‘áº¿n kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh XL, háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c láº¥y máº«u gáº§n vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh M-L. Äiá»u nÃ y cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c tháº¥y trong Ä‘Æ°á»ng cong Máº¥t mÃ¡t, nÆ¡i chÃºng ta tháº¥y má»™t Ä‘Æ°á»ng cong giá»‘ng nhÆ° chuÃ´ng cÃ³ máº¥t mÃ¡t cao hÆ¡n nhiá»u gáº§n vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh S vÃ  XL, vÃ  khá»›p vá»›i hiá»‡u suáº¥t cá»§a MatLM á»Ÿ giá»¯a. Má»™t nhÆ°á»£c Ä‘iá»ƒm khÃ¡c cá»§a OFA lÃ  vÃ¬ cÃ¡c mÃ´ hÃ¬nh ngáº«u nhiÃªn Ä‘Æ°á»£c láº¥y máº«u, ngÆ°á»i ta khÃ´ng thá»ƒ sá»­ dá»¥ng ká»¹ thuáº­t Trá»™n & Káº¿t há»£p Ä‘Æ¡n giáº£n nhÆ° trong matFormer. Thay vÃ o Ä‘Ã³, má»™t chiáº¿n lÆ°á»£c NAS phá»©c táº¡p hÆ¡n lÃ  cáº§n thiáº¿t Ä‘á»ƒ tÃ¬m cÃ¡c mÃ´ hÃ¬nh con tá»‘i Æ°u. Äá»ƒ huáº¥n luyá»‡n má»™t NAS khi tÃ¬m kiáº¿m cÃ¡c mÃ´ hÃ¬nh lá»›n nhÆ° váº­y lÃ  tá»‘n kÃ©m vÃ  cÃ³ lá»—i, nhÆ° chÃºng tÃ´i tháº£o luáº­n trong Phá»¥ lá»¥c D.2.

C Chi phÃ­ Huáº¥n luyá»‡n vÃ  Suy luáº­n
ChÃºng tÃ´i hiá»‡n táº¡i thá»±c hiá»‡n nhá»¯ng thay Ä‘á»•i vÃ  tá»‘i Æ°u hÃ³a tá»‘i thiá»ƒu Ä‘á»‘i vá»›i cÃ¡c script huáº¥n luyá»‡n cá»§a kiáº¿n trÃºc Transformer vanilla. NÃ³i cÃ¡ch khÃ¡c, chÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng má»™t triá»ƒn khai cho cáº£ Baselime vÃ  MatFormer, ngoáº¡i trá»« viá»‡c sá»­ dá»¥ng cÃ¡c lÃ¡t cáº¯t cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau cá»§a khá»‘i FFN cho má»—i lÆ°á»£t truyá»n tiáº¿n. Thá»i gian tÆ°á»ng cho huáº¥n luyá»‡n MatLM cÃ¹ng chi phÃ­ nhÆ° huáº¥n luyá»‡n táº¥t cáº£ 4 Ä‘á»‘i tÃ¡c cÆ¡ sá»Ÿ má»©c Ä‘á»™ chi tiáº¿t. Trong quÃ¡ trÃ¬nh phá»¥c vá»¥, chÃºng tÃ´i quan sÃ¡t tháº¥y tá»· lá»‡ Ä‘á»™ trá»… FFN mÃ´ hÃ¬nh 850M so vá»›i Ä‘á»™ trá»… attention = 56:44. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng tá»· lá»‡ Ä‘á»™ trá»… FFN:MHA nÃ y phá»¥ thuá»™c ráº¥t nhiá»u vÃ o quy mÃ´ vÃ  Ä‘á»™ dÃ i chuá»—i. Cá»¥ thá»ƒ hÆ¡n, Ä‘á»‘i vá»›i má»™t Ä‘á»™ dÃ i chuá»—i nháº¥t Ä‘á»‹nh, Ä‘á»™ trá»… FFN chi phá»‘i Ä‘á»™ trá»… tá»•ng thá»ƒ á»Ÿ quy mÃ´, trong khi chi phÃ­ cá»§a cÃ¡c Ä‘áº§u attention tÄƒng vá»›i Ä‘á»™ dÃ i chuá»—i. ChÃºng tÃ´i giá»›i thiá»‡u ngÆ°á»i Ä‘á»c Ä‘áº¿n Kim et al. [30] Ä‘á»ƒ minh há»a rá»™ng rÃ£i hÆ¡n vá» Ä‘iá»u nÃ y. ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng máº·c dÃ¹ chÃºng tÃ´i Ä‘Ã£ huáº¥n luyá»‡n má»™t MatFormer vÃ  so sÃ¡nh thá»i gian huáº¥n luyá»‡n cá»§a nÃ³ vá»›i Baseline káº¿t há»£p, chÃºng tÃ´i cÃ³ thá»ƒ trÃ­ch xuáº¥t nhiá»u mÃ´ hÃ¬nh hÆ¡n so vá»›i 4 má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh chÃºng tÃ´i Ä‘Ã£ huáº¥n luyá»‡n rÃµ rÃ ng.

C.1 Chia sáº» Attention Giáº£i mÃ£ Suy Ä‘oÃ¡n
Má»™t lá»£i Ã­ch bá»• sung cá»§a MatLM lÃ  bá»™ nhá»› Ä‘á»‡m attention Ä‘Æ°á»£c chia sáº» giá»¯a mÃ´ hÃ¬nh nhÃ¡p vÃ  mÃ´ hÃ¬nh xÃ¡c minh. Khi mÃ´ hÃ¬nh XL xÃ¡c minh báº£n nhÃ¡p cá»§a mÃ´ hÃ¬nh S, nÃ³ ghi Ä‘Ã¨ bá»™ nhá»› Ä‘á»‡m attention vá»›i biá»ƒu diá»…n tiá»m áº©n phong phÃº hÆ¡n cá»§a nÃ³ so vá»›i cÃ¡i Ä‘Æ°á»£c táº¡o ra bá»Ÿi mÃ´ hÃ¬nh soáº¡n nhÃ¡p. LÆ°u Ã½ ráº±ng 1) Ä‘iá»u nÃ y khÃ´ng liÃªn quan Ä‘áº¿n tÃ­nh toÃ¡n bá»• sung vÃ¬ MatLM cÃ³ má»™t mÃ´ hÃ¬nh phá»• quÃ¡t duy nháº¥t bao gá»“m cáº£ mÃ´ hÃ¬nh nhÃ¡p vÃ  mÃ´ hÃ¬nh xÃ¡c minh; 2) chia sáº» attention khÃ´ng kháº£ thi trong Baseline vÃ¬ chÃºng khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng cÃ¹ng nhau. Do Ä‘Ã³, biá»ƒu diá»…n tiá»m áº©n cá»§a má»™t mÃ´ hÃ¬nh khÃ¡ vÃ´ nghÄ©a Ä‘á»‘i vá»›i mÃ´ hÃ¬nh khÃ¡c. VÃ¬ váº­y, chia sáº» attention mang láº¡i cáº£i thiá»‡n thÃªm so vá»›i giáº£i mÃ£ suy Ä‘oÃ¡n vanilla nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2.

--- TRANG 18 ---
Báº£ng 5: TrÃªn mÃ´ hÃ¬nh MatFormer 850M, trong khi cháº¡y NAS chÃºng tÃ´i quan sÃ¡t tháº¥y nÃ³ Æ°a thÃ­ch cÃ¡c má»©c Ä‘á»™ chi tiáº¿t cÃ¢n báº±ng qua cÃ¡c lá»›p hÆ¡n lÃ  lá»‡ch. TrÃªn má»™t vÃ i rÃ ng buá»™c tham sá»‘ chÃºng tÃ´i liá»‡t kÃª cáº¥u hÃ¬nh heuristic MnM, vÃ  cáº¥u hÃ¬nh Ä‘Æ°á»£c dá»± Ä‘oÃ¡n vá»›i NAS.

NgÃ¢n sÃ¡ch Tham sá»‘ FFN Cáº¥u hÃ¬nh Trá»™n & Káº¿t há»£p & máº¥t mÃ¡t Cáº¥u hÃ¬nh Ä‘Æ°á»£c dá»± Ä‘oÃ¡n NAS & máº¥t mÃ¡t
226M [M,M,M,M,M,M,M,M,M,M,M,M,M,M,M,M]; 2.931 [S,M,M,M,M,M,M,S,M,M,M,M,M,M,M,L]; 2.944
245M [M,M,M,M,M,M,M,M,M,M,M,M,L,L,L,L]; 2.926 [M,M,M,M,M,L,M,M,M,L,M,L,M,M,M,L]; 2.932
278M [M,M,M,M,M,L,L,L,L,L,L,L,L,L,L,L]; 2.9 [M,L,L,L,L,L,L,L,L,L,M,L,M,M,M,L]; 2.91
377M [L,L,L,L,XL,XL,XL,XL,XL,XL,XL,XL,XL,XL,XL,XL]; 2.865 [L,L,XL,L,XL,L,XL,L,XL,XL,XL,L,XL,L,L,XL]; 2.874

D Ká»¹ thuáº­t TÃ¬m kiáº¿m
D.1 Trá»™n & Káº¿t há»£p
Äá»‘i vá»›i má»™t ngÃ¢n sÃ¡ch tÃ­nh toÃ¡n hoáº·c tham sá»‘ nháº¥t Ä‘á»‹nh, nhiá»u mÃ´ hÃ¬nh con cÃ³ thá»ƒ Ä‘Ã¡p á»©ng rÃ ng buá»™c. Má»™t chiáº¿n lÆ°á»£c phá»• biáº¿n cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n viá»‡c sá»­ dá»¥ng TÃ¬m kiáº¿m Kiáº¿n trÃºc Tháº§n kinh (NAS) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh kiáº¿n trÃºc tá»‘i Æ°u trong táº­p há»£p con nÃ y; tuy nhiÃªn, cÃ¡ch tiáº¿p cáº­n nÃ y cÃ³ thá»ƒ tá»‘n kÃ©m má»™t cÃ¡ch cáº¥m Ä‘oÃ¡n. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t heuristic Trá»™n & Káº¿t há»£p Ä‘Æ¡n giáº£n hÆ¡n, xÃ¡c Ä‘á»‹nh cÃ¡c mÃ´ hÃ¬nh con tá»‘i Æ°u náº±m trÃªn Ä‘Æ°á»ng cong pareto-tá»‘i Æ°u Ä‘á»™ chÃ­nh xÃ¡c-so vá»›i-tham sá»‘.

Heuristic Trá»™n & Káº¿t há»£p khuyáº¿n nghá»‹ chá»n cÃ¡c má»©c Ä‘á»™ chi tiáº¿t lá»›p vá»›i nhá»¯ng thay Ä‘á»•i tá»‘i thiá»ƒu qua cÃ¡c lá»›p, Ä‘áº£m báº£o ráº±ng má»©c Ä‘á»™ chi tiáº¿t cá»§a lá»›p thá»© j Ã­t nháº¥t báº±ng cá»§a lá»›p thá»© i cho j > i. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  má»™t sá»± tÄƒng dáº§n má»©c Ä‘á»™ chi tiáº¿t vá»›i "Ä‘á»™ dá»‘c nhá» nháº¥t" thá»±c nghiá»‡m mang láº¡i hiá»‡u suáº¥t tá»‘t nháº¥t trong sá»‘ táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh trá»™n-vÃ -káº¿t há»£p Ä‘Æ°á»£c kiá»ƒm tra. Heuristic cá»§a chÃºng tÃ´i Ä‘Æ°á»£c há»— trá»£ bá»Ÿi phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n, trong Ä‘Ã³ má»—i cáº¥u hÃ¬nh má»©c Ä‘á»™ chi tiáº¿t Ä‘Æ°á»£c láº¥y máº«u duy trÃ¬ má»©c Ä‘á»™ chi tiáº¿t lá»›p nháº¥t quÃ¡n qua mÃ´ hÃ¬nh. Do Ä‘Ã³, mÃ´ hÃ¬nh thÃ­ch á»©ng tá»‘t nháº¥t vá»›i cÃ¡c cáº¥u hÃ¬nh trong Ä‘Ã³ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t lá»›p hoáº·c Ä‘á»“ng nháº¥t hoáº·c hiá»ƒn thá»‹ sá»± biáº¿n Ä‘á»•i tá»‘i thiá»ƒu. VÃ­ dá»¥, má»™t cáº¥u hÃ¬nh [M, M, M, M, L, L, L] qua cÃ¡c lá»›p hiá»‡u quáº£ hÆ¡n [S, S, S, S, S, XL, XL] máº·c dÃ¹ cÃ³ sá»‘ lÆ°á»£ng tham sá»‘ tÆ°Æ¡ng tá»±, vÃ¬ nÃ³ duy trÃ¬ má»™t phÃ¢n phá»‘i má»©c Ä‘á»™ chi tiáº¿t cÃ¢n báº±ng hÆ¡n so vá»›i má»™t phÃ¢n phá»‘i lá»‡ch.

Trá»±c giÃ¡c nÃ y cÅ©ng Ä‘Æ°á»£c há»— trá»£ bá»Ÿi káº¿t quáº£ tá»« má»™t phÆ°Æ¡ng phÃ¡p TÃ¬m kiáº¿m Kiáº¿n trÃºc Tháº§n kinh (NAS) dá»±a trÃªn tÃ¬m kiáº¿m tiáº¿n hÃ³a [48], tÆ°Æ¡ng tá»± nhÆ° Ä‘Æ°á»£c sá»­ dá»¥ng trong [9]. CÃ¡c thÃ­ nghiá»‡m NAS cá»§a chÃºng tÃ´i, má»™t vÃ i láº§n cháº¡y chÃºng tÃ´i trÃ¬nh bÃ y trong Báº£ng 5, chá»‰ ra má»™t sá»± Æ°a thÃ­ch cho cÃ¡c cáº¥u hÃ¬nh cÃ¢n báº±ng dÆ°á»›i cÃ¡c rÃ ng buá»™c khÃ¡c nhau, phÃ¹ há»£p vá»›i nhá»¯ng phÃ¡t hiá»‡n tá»« heuristic Trá»™n & Káº¿t há»£p cá»§a chÃºng tÃ´i.

ChÃºng tÃ´i Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡c cáº¥u hÃ¬nh cÃ¢n báº±ng khÃ¡c nhau, bao gá»“m:
1. TÄƒng-Giáº£m: Má»©c Ä‘á»™ chi tiáº¿t tÄƒng cho Ä‘áº¿n Ä‘iá»ƒm giá»¯a vÃ  sau Ä‘Ã³ giáº£m, vÃ­ dá»¥ [M, M, L, L, L, M, M].
2. Giáº£m-TÄƒng: NgÆ°á»£c láº¡i cá»§a heuristic trÆ°á»›c, trong Ä‘Ã³ má»©c Ä‘á»™ chi tiáº¿t giáº£m sau Ä‘Ã³ tÄƒng.
3. TÄƒng: Má»™t chuá»—i khÃ´ng giáº£m cá»§a cÃ¡c má»©c Ä‘á»™ chi tiáº¿t lá»›p, cháº³ng háº¡n nhÆ° [M, M, M, M, L, L, L].
4. Giáº£m: NgÆ°á»£c láº¡i cá»§a heuristic TÄƒng.

Trong sá»‘ nhá»¯ng cáº¥u hÃ¬nh nÃ y, heuristic TÄƒng liÃªn tá»¥c vÆ°á»£t trá»™i hÆ¡n nhá»¯ng cÃ¡i khÃ¡c. Do Ä‘Ã³, chÃºng tÃ´i khuyáº¿n nghá»‹ Ã¡p dá»¥ng má»™t chiáº¿n lÆ°á»£c chá»n cÃ¡c má»©c Ä‘á»™ chi tiáº¿t tÄƒng qua cÃ¡c lá»›p vá»›i Ä‘á»™ dá»‘c tá»‘i thiá»ƒu Ä‘á»ƒ cÃ³ hiá»‡u suáº¥t mÃ´ hÃ¬nh hiá»‡u quáº£.

TrÃªn MatLM 850M, chÃºng tÃ´i so sÃ¡nh cÃ¡c kiáº¿n trÃºc Ä‘Æ°á»£c dá»± Ä‘oÃ¡n bá»Ÿi Trá»™n & Káº¿t há»£p vÃ  thuáº­t toÃ¡n NAS, trong HÃ¬nh 6. ChÃºng tÃ´i tháº¥y ráº±ng heuristic cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng Ã­t nháº¥t lÃ  tá»‘t nhÆ° NAS, dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh náº±m trÃªn Ä‘Æ°á»ng cong pareto-tá»‘i Æ°u. LÆ°u Ã½ ráº±ng Trá»™n & Káº¿t há»£p khÃ´ng yÃªu cáº§u huáº¥n luyá»‡n bá»• sung cá»§a má»™t mÃ´-Ä‘un hoáº·c chi phÃ­ trong viá»‡c tÃ­nh toÃ¡n mÃ´ hÃ¬nh con tá»‘i Æ°u trong má»™t rÃ ng buá»™c chi phÃ­.

D.2 NAS
TÃ¬m kiáº¿m Kiáº¿n trÃºc Tháº§n kinh [68,48] lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»± nhiÃªn Ä‘á»ƒ tÃ¬m kiáº¿m má»™t máº¡ng tá»‘i Æ°u trong sá»‘ cÃ¡c kiáº¿n trÃºc cÃ³ thá»ƒ trong má»™t ngÃ¢n sÃ¡ch rÃ ng buá»™c. Thá»±c táº¿, OFA [9] sá»­ dá»¥ng má»™t ká»¹ thuáº­t dá»±a trÃªn tÃ¬m kiáº¿m tiáº¿n hÃ³a [48] cÃ¹ng vá»›i má»™t bá»™ dá»± Ä‘oÃ¡n máº¥t mÃ¡t kiáº¿n trÃºc (Phá»¥ lá»¥c B.3) trong thÃ³i quen tÃ¬m kiáº¿m cá»§a nÃ³. Trong khi trÃªn cÃ¡c mÃ´ hÃ¬nh quy mÃ´ nhá» hÆ¡n, cÃ³ thá»ƒ dá»… dÃ ng triá»ƒn khai phÆ°Æ¡ng phÃ¡p NAS tÆ°Æ¡ng tá»± nhÆ° OFA lÃ m, trÃªn quy mÃ´ lá»›n hÆ¡n nÃ³ trá»Ÿ nÃªn tá»‘n kÃ©m má»™t cÃ¡ch cáº¥m Ä‘oÃ¡n vÃ  cÃ³ lá»—i.

--- TRANG 19 ---
Äá»ƒ cÃ³ Ä‘Æ°á»£c máº¥t mÃ¡t tÆ°Æ¡ng á»©ng vá»›i má»™t kiáº¿n trÃºc, ngÆ°á»i ta cáº§n cháº¡y trÃªn má»™t táº­p há»£p Ä‘Æ°á»£c giá»¯ láº¡i Ä‘á»§ lá»›n Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t giÃ¡ trá»‹ trung bÃ¬nh. VÃ­ dá»¥, trong cÃ¡c láº§n cháº¡y cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng má»™t táº­p há»£p Ä‘Æ°á»£c giá»¯ láº¡i khoáº£ng 200M token. á» quy mÃ´ lá»›n, viá»‡c liÃªn tá»¥c cháº¡y cÃ¡c mÃ´ hÃ¬nh lá»›n trÃªn táº­p há»£p Ä‘Æ°á»£c giá»¯ láº¡i nÃ y Ä‘á»ƒ thu tháº­p cÃ¡c giÃ¡ trá»‹ máº¥t mÃ¡t trá»Ÿ nÃªn tá»‘n kÃ©m. VÃ­ dá»¥, Ä‘á»ƒ thu tháº­p 16k cáº·p (kiáº¿n trÃºc, máº¥t mÃ¡t), chÃºng tÃ´i Ä‘Ã£ sá»­ dá»¥ng 256 chip TPUv-4 trong khoáº£ng 2 ngÃ y.

Khi chÃºng tÃ´i cÃ³ táº­p dá»¯ liá»‡u, chÃºng tÃ´i chia nÃ³ thÃ nh tá»· lá»‡ 60%-40% train-eval. Sau khi huáº¥n luyá»‡n bá»™ dá»± Ä‘oÃ¡n máº¥t mÃ¡t kiáº¿n trÃºc, chÃºng tÃ´i Ä‘Ã£ kiá»ƒm tra hiá»‡u suáº¥t cá»§a nÃ³ trÃªn táº­p há»£p Ä‘Æ°á»£c giá»¯ láº¡i vÃ  nháº­n tháº¥y lá»—i bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh âˆ¼5eâˆ’5.

Khi chÃºng ta chuyá»ƒn sang cháº¿ Ä‘á»™ mÃ´ hÃ¬nh lá»›n, khoáº£ng cÃ¡ch hiá»‡u suáº¥t giá»¯a cÃ¡c mÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau giáº£m. VÃ­ dá»¥, khoáº£ng cÃ¡ch giá»¯a Baseline-S vÃ  Baseline-XL 78M tham sá»‘, cÃ³ kÃ­ch thÆ°á»›c 71M vÃ  78M, lÃ  4.01 vs 3.83. ÄÃ³ lÃ  má»™t khoáº£ng cÃ¡ch 0.18 máº¥t mÃ¡t cho chá»‰ 7M tham sá»‘ khÃ¡c biá»‡t. NhÆ°ng trong trÆ°á»ng há»£p mÃ´ hÃ¬nh 850M, Baseline-S vÃ  Baseline-XL cÃ³ kÃ­ch thÆ°á»›c 582M vs 850M, máº¥t mÃ¡t tÆ°Æ¡ng á»©ng lÃ  3.017 vs 2.84. VÃ¬ váº­y, má»™t sá»± khÃ¡c biá»‡t 0.177 cho 268M tham sá»‘ khÃ¡c biá»‡t. Xu hÆ°á»›ng nÃ y má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n, vÃ  khoáº£ng cÃ¡ch giáº£m giá»¯a cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh khÃ¡c nhau. VÃ¬ váº­y, viá»‡c huáº¥n luyá»‡n má»™t bá»™ dá»± Ä‘oÃ¡n Ä‘á»™ chÃ­nh xÃ¡c kiáº¿n trÃºc trá»Ÿ nÃªn ngÃ y cÃ ng thÃ¡ch thá»©c vÃ  cÃ³ lá»—i, vÃ¬ nÃ³ pháº£i há»c phÃ¢n biá»‡t cÃ¡c kiáº¿n trÃºc vá»›i nhá»¯ng khÃ¡c biá»‡t ráº¥t nhá» trong giÃ¡ trá»‹ máº¥t mÃ¡t cá»§a chÃºng.

Trong thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cho OFA, chÃºng tÃ´i láº¥y tá»‘t nháº¥t cá»§a cÃ¡c cáº¥u hÃ¬nh mÃ´ hÃ¬nh Ä‘Æ°á»£c dá»± Ä‘oÃ¡n bá»Ÿi NAS vÃ  cáº¥u hÃ¬nh mÃ´ hÃ¬nh trong sá»‘ cÃ¡c Ä‘iá»ƒm Ä‘Æ°á»£c láº¥y máº«u cho má»™t ngÃ¢n sÃ¡ch tÆ°Æ¡ng á»©ng.

E Luáº­t Má»Ÿ rá»™ng cho Bá»™ giáº£i mÃ£ NgÃ´n ngá»¯
ChÃºng tÃ´i cung cáº¥p káº¿t quáº£ Ä‘Æ°á»£c chia theo má»©c Ä‘á»™ chi tiáº¿t cho máº¥t mÃ¡t xÃ¡c thá»±c, Ä‘iá»ƒm trung bÃ¬nh trÃªn cÃ¡c nhiá»‡m vá»¥ Ä‘Ã¡nh giÃ¡, vÃ  tÃ­nh nháº¥t quÃ¡n trong HÃ¬nh 9, 10, vÃ  11 tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng khoáº£ng cÃ¡ch trong máº¥t mÃ¡t xÃ¡c thá»±c vÃ  Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã¡nh giÃ¡ giá»¯a MatLM vÃ  Baseline dÆ°á»ng nhÆ° lÃ  háº±ng sá»‘. Äá»‘i vá»›i tÃ­nh nháº¥t quÃ¡n, khoáº£ng cÃ¡ch dÆ°á»ng nhÆ° giáº£m vá»›i quy mÃ´, nhÆ°ng ngÆ°á»i ta sáº½ cáº§n má»Ÿ rá»™ng cÃ¡c mÃ´ hÃ¬nh nhiá»u báº­c Ä‘á»™ lá»›n vÆ°á»£t quÃ¡ nhá»¯ng gÃ¬ cÃ³ thá»ƒ ngÃ y nay Ä‘á»ƒ cÃ¡c baseline cÃ³ tÃ­nh nháº¥t quÃ¡n so sÃ¡nh Ä‘Æ°á»£c vá»›i MatLM.

E.1 Luáº­t má»Ÿ rá»™ng cá»§a MatFormers vs Transformers.
Luáº­t má»Ÿ rá»™ng lÃ  nhá»¯ng cÃ´ng cá»¥ thiáº¿t yáº¿u Ä‘á»ƒ Æ°á»›c tÃ­nh cháº¥t lÆ°á»£ng khi chi phÃ­ huáº¥n luyá»‡n hoáº·c suy luáº­n Ä‘Æ°á»£c tÄƒng lÃªn. Luáº­t má»Ÿ rá»™ng cÃ³ thá»ƒ giÃºp chÃºng ta xem xÃ©t cÃ¡c sáº¯c thÃ¡i khÃ¡c nhau cá»§a huáº¥n luyá»‡n vÃ  triá»ƒn khai nhÆ° chi phÃ­ huáº¥n luyá»‡n tá»•ng thá»ƒ trong FLOP, hiá»‡u quáº£ dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  tham sá»‘, vÃ  sá»­ dá»¥ng FLOP trung bÃ¬nh suy luáº­n so vá»›i Ä‘á»™ trá»… cho cÃ¡c triá»ƒn khai.

Má»‘i quan há»‡ má»Ÿ rá»™ng cá»§a MatFormer so vá»›i Transformer vá»«a Ä‘Æ¡n giáº£n vá»«a phá»©c táº¡p. ÄÆ¡n giáº£n, vÃ¬ cÃ¡c Ä‘Æ°á»ng cong má»Ÿ rá»™ng MatFormer cho tiá»n huáº¥n luyá»‡n tÆ°Æ¡ng tá»± nhÆ° cá»§a Transformer â€“ vÃ¬ váº­y MatFormer chá»‰ yÃªu cáº§u má»™t lÆ°á»£ng tÃ­nh toÃ¡n tÆ°Æ¡ng tá»± vÃ  cÃ¹ng siÃªu tham sá»‘ hoáº¡t Ä‘á»™ng cho Transformer cÃ³ hiá»‡u quáº£ cho MatFormer. Má»‘i quan há»‡ má»Ÿ rá»™ng phá»©c táº¡p Ä‘áº¿n tá»« thá»±c táº¿ ráº±ng MatFormer cho phÃ©p huáº¥n luyá»‡n nhiá»u mÃ´ hÃ¬nh vá»›i má»™t láº§n cháº¡y huáº¥n luyá»‡n duy nháº¥t mÃ  Ä‘á»‹nh tÃ­nh khÃ¡c vá»›i Transformer vÃ  khÃ³ tÃ­nh vÃ o cÃ¡c phÆ°Æ¡ng trÃ¬nh má»Ÿ rá»™ng. Vá» máº·t hiá»‡u quáº£, náº¿u chÃºng ta so sÃ¡nh FLOP huáº¥n luyá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ trÃ­ch xuáº¥t tá»« MatFormer, thÃ¬ viá»‡c huáº¥n luyá»‡n MatFormer má»™t mÃ¬nh cÃ³ lá»£i tháº¿ rÃµ rÃ ng trong báº¥t ká»³ trÆ°á»ng há»£p nÃ o mÃ  táº¥t cáº£ cÃ¡c tham sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Transformer tiÃªu chuáº©n trÃªn cÃ¹ng táº­p dá»¯ liá»‡u vÆ°á»£t quÃ¡ 2.58P, trong Ä‘Ã³ P lÃ  sá»‘ lÆ°á»£ng tham sá»‘ cá»§a MatFormer vÃ  mÃ´ hÃ¬nh Transformer lá»›n nháº¥t. Äiá»u nÃ y lÃ  do MatFormer sá»­ dá»¥ng 2.58 láº§n FLOP hÆ¡n má»—i token cho má»™t láº§n cháº¡y huáº¥n luyá»‡n so vá»›i Transformer: 4Ã— FLOP hÆ¡n cho cÃ¡c tham sá»‘ lá»›p attention vÃ  {1 + 1/2 + 1/4 + 1/8 = 1.875}Ã— FLOP hÆ¡n cho cÃ¡c lá»›p MLP.

F PhÃ¢n tÃ­ch ThÃªm vá» Bá»™ giáº£i mÃ£ NgÃ´n ngá»¯
F.1 PhÃ¢n ká»³ KL Giá»¯a cÃ¡c MÃ´ hÃ¬nh S, M, L vÃ  XL
HÃ¬nh 7 thá»ƒ hiá»‡n phÃ©p tÃ­nh tÃ­nh nháº¥t quÃ¡n mÆ°á»£t mÃ  hÆ¡n giá»¯a hai mÃ´ hÃ¬nh táº¡o sinh Ä‘Æ°á»£c Ä‘o báº±ng phÃ¢n ká»³ KL cá»§a Ä‘áº§u ra mÃ´ hÃ¬nh nhá» hÆ¡n vá»›i Ä‘áº§u ra mÃ´ hÃ¬nh lá»›n hÆ¡n. TÆ°Æ¡ng tá»± nhÆ° metric tÃ­nh nháº¥t quÃ¡n khá»›p chÃ­nh xÃ¡c kiá»ƒu cá»©ng Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i bÃ¡o chÃ­nh, cÃ³ má»™t khoáº£ng cÃ¡ch Ä‘Ã¡ng ká»ƒ giá»¯a tÃ­nh nháº¥t quÃ¡n cá»§a cÃ¡c mÃ´ hÃ¬nh con MatLM vá»›i mÃ´ hÃ¬nh MatLM-XL vÃ  giá»¯a Ä‘Ã³ vá»›i cÃ¡c mÃ´ hÃ¬nh cÆ¡ sá»Ÿ tÆ°Æ¡ng á»©ng. Äiá»u nÃ y chá»‰ ra cÃ¡ch cÃ¡c chiáº¿n lÆ°á»£c láº¥y máº«u dá»±a trÃªn xÃ¡c suáº¥t Ä‘áº§u ra khÃ´ng thay Ä‘á»•i tÃ­nh nháº¥t quÃ¡n hÃ nh vi giá»¯a hai mÃ´ hÃ¬nh vÃ  ráº±ng nÃ³ váº«n tuÃ¢n theo xu hÆ°á»›ng táº¡o ra token vá»›i xÃ¡c suáº¥t cao nháº¥t. KhÃ¡i niá»‡m tÃ­nh nháº¥t quÃ¡n mÆ°á»£t mÃ  hÆ¡n nÃ y láº­p luáº­n cho viá»‡c báº£o tá»“n cáº¥u trÃºc khÃ´ng gian metric vá»›i viá»‡c ma tráº­n phÃ¢n loáº¡i/nhÃºng Ä‘áº§u ra Ä‘Æ°á»£c chia sáº» qua táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh con cá»§a MatLM.

--- TRANG 20 ---
Báº£ng 6: Äá»‘i vá»›i mÃ´ hÃ¬nh 850M, chÃºng tÃ´i thá»­ nghiá»‡m vá»›i viá»‡c sá»­a Ä‘á»•i {pS, pM, pL, pXL} Ä‘á»ƒ láº¥y máº«u cÃ¡c mÃ´ hÃ¬nh con tá»« má»™t phÃ¢n phá»‘i khÃ´ng Ä‘á»“ng nháº¥t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  bÃ¡o cÃ¡o káº¿t quáº£ qua táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t. ChÃºng tÃ´i tháº¥y ráº±ng táº¥t cáº£ cÃ¡c chiáº¿n lÆ°á»£c tÄƒng trá»ng sá»‘ máº¥t mÃ¡t cho má»©c Ä‘á»™ chi tiáº¿t lá»›n nháº¥t hoáº¡t Ä‘á»™ng tá»‘t, vá»›i sá»± suy giáº£m khiÃªm tá»‘n trÃªn cÃ¡c má»©c Ä‘á»™ chi tiáº¿t M vÃ  S.

XÃ¡c suáº¥t MÃ´ hÃ¬nh S M L XL
Baseline N/A 3.017 2.971 2.910 2.840
MatFormer 0.44/0.31/0.15/0.10 2.963 2.925 2.899 2.877
0.31/0.27/0.22/0.20 2.977 2.929 2.890 2.857
0.25/0.25/0.25/0.25 2.970 2.934 2.886 2.846
0.20/0.22/0.24/0.34 3.000 2.939 2.885 2.836
0.17/0.20/0.22/0.41 3.010 2.943 2.884 2.829

báº£o tá»“n cho ráº±ng ma tráº­n phÃ¢n loáº¡i/nhÃºng Ä‘áº§u ra Ä‘Æ°á»£c chia sáº» qua táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh con cá»§a MatLM.

[HÃ¬nh 7 Ä‘Æ°á»£c giá»¯ nguyÃªn]

HÃ¬nh 7: Biáº¿n thá»ƒ mÆ°á»£t mÃ  hÆ¡n cá»§a tÃ­nh nháº¥t quÃ¡n Ä‘o phÃ¢n ká»³ KL giá»¯a cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n vÃ  mÃ´ hÃ¬nh XL tÆ°Æ¡ng á»©ng. Metric nÃ y, khÃ´ng giá»‘ng nhÆ° biáº¿n thá»ƒ khá»›p chÃ­nh xÃ¡c, cÅ©ng tÃ­nh Ä‘áº¿n cÃ¡c chiáº¿n lÆ°á»£c láº¥y máº«u khÃ¡c nhau trÃªn phÃ¢n phá»‘i Ä‘áº§u ra trong quÃ¡ trÃ¬nh triá»ƒn khai. Trong hÃ¬nh nÃ y, chÃºng tÃ´i váº½ phÃ¢n ká»³ KL cá»§a cÃ¡c má»©c Ä‘á»™ chi tiáº¿t S, M, L Ä‘á»‘i vá»›i XL cho mÃ´ hÃ¬nh 850M tham sá»‘.

F.2 Sá»­ dá»¥ng MatFormers cho Khá»‘i con Attention
ChÃºng tÃ´i thá»­ nghiá»‡m vá»›i viá»‡c Ã¡p dá»¥ng MRL cho khá»‘i con attention cá»§a khá»‘i Transformer. Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i tá»‘i Æ°u hÃ³a chung 4 máº¡ng con vá»›i sá»‘ lÆ°á»£ng Ä‘áº§u attention khÃ¡c nhau nattn, 3âˆ—nattn/4, nattn/2, nattn/4 cÃ³ kÃ­ch thÆ°á»›c d/nattn má»—i cÃ¡i. Trong HÃ¬nh 8, chÃºng tÃ´i váº½ máº¥t mÃ¡t xÃ¡c thá»±c cho nhá»¯ng mÃ´ hÃ¬nh nÃ y (MatLM-Attn), cÃ¡c baseline tÆ°Æ¡ng á»©ng cá»§a chÃºng vÃ  cÃ¡c máº¡ng con Trá»™n & Káº¿t há»£p. ChÃºng tÃ´i tháº¥y ráº±ng tÆ°Æ¡ng tá»± nhÆ° cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trÃªn MatLM Trá»™n & Káº¿t há»£p, Trá»™n & Káº¿t há»£p giÃºp cÃ³ Ä‘Æ°á»£c nhiá»u mÃ´ hÃ¬nh MatLM-Attn trÃªn Ä‘Æ°á»ng cong máº¥t mÃ¡t-so vá»›i-tÃ­nh toÃ¡n tá»‘i Æ°u.

F.3 Äiá»u chá»‰nh XÃ¡c suáº¥t Láº¥y máº«u
Trong Báº£ng 6, chÃºng tÃ´i thá»­ nghiá»‡m vá»›i viá»‡c Ä‘iá»u chá»‰nh xÃ¡c suáº¥t láº¥y máº«u cho cÃ¡c má»©c Ä‘á»™ chi tiáº¿t riÃªng láº» Ä‘á»ƒ Ä‘iá»u tra sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a kÃ­ch thÆ°á»›c má»©c Ä‘á»™ chi tiáº¿t vÃ  hiá»‡u suáº¥t. Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i Ä‘iá»u chá»‰nh {p1, p2, p3, p4}, vÃ  tháº¥y ráº±ng táº¥t cáº£ cÃ¡c chiáº¿n lÆ°á»£c tÄƒng trá»ng sá»‘ máº¥t mÃ¡t cho má»©c Ä‘á»™ chi tiáº¿t lá»›n nháº¥t hoáº¡t Ä‘á»™ng tá»‘t, vá»›i sá»± suy giáº£m khiÃªm tá»‘n trÃªn cÃ¡c má»©c Ä‘á»™ chi tiáº¿t M vÃ  S.

G PhÃ¢n tÃ­ch ThÃªm vá» Bá»™ mÃ£ hÃ³a Thá»‹ giÃ¡c
G.1 TÃ¡ch Hiá»‡u á»©ng cá»§a MatFormer trÃªn Tiá»n huáº¥n luyá»‡n vÃ  Tinh chá»‰nh
Báº£ng 7 Ä‘iá»u tra hiá»‡u á»©ng cá»§a MatFormer trÃªn cÃ¡c giai Ä‘oáº¡n tiá»n huáº¥n luyá»‡n vÃ  tinh chá»‰nh cá»§a mÃ´ hÃ¬nh ViT-L/16. ViT-L/16 thÆ°á»ng Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn ImageNet-21K vÃ  sau Ä‘Ã³ tinh chá»‰nh trÃªn ImageNet-1K cho Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng. Báº£ng 7 cho tháº¥y ráº±ng cÃ³ MatFormer trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n táº¡o ra má»™t mÃ´ hÃ¬nh tá»‘t hÆ¡n cho viá»‡c tinh chá»‰nh downstream so vá»›i tiá»n huáº¥n luyá»‡n ViT thÃ´ng thÆ°á»ng. Äá»“ng thá»i, tinh chá»‰nh má»™t ViT Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n vanilla vá»›i MatFormer dáº«n Ä‘áº¿n tÃ­nh linh hoáº¡t Ä‘Æ°á»£c táº¡o ra vÃ o mÃ´ hÃ¬nh. Máº·c dÃ¹ Ã­t chÃ­nh xÃ¡c hÆ¡n Ä‘áº¿n 2% so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c cá»§a nÃ³ á»Ÿ má»™t sá»‘ má»©c Ä‘á»™ chi tiáº¿t, má»™t MatViT Ä‘Æ°á»£c tinh chá»‰nh Ä‘Ã£ há»c cÃ¡ch phÃ¢n bá»• láº¡i thÃ´ng tin Ä‘á»ƒ cung cáº¥p cÃ¡c mÃ´ hÃ¬nh lá»“ng nhau máº¡nh. Xem xÃ©t ráº±ng Ä‘iá»u nÃ y khÃ´ng Ä‘Ã¡ng ká»ƒ so vá»›i chi phÃ­ tiá»n huáº¥n luyá»‡n, cÃ³ thá»ƒ láº¥y mÃ´ hÃ¬nh ViT Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n lá»›n nháº¥t vÃ  tinh chá»‰nh vá»›i MatFormer Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t biáº¿n thá»ƒ MatViT cÃ³ thá»ƒ triá»ƒn khai.

--- TRANG 21 ---
[HÃ¬nh 8 Ä‘Æ°á»£c giá»¯ nguyÃªn]

HÃ¬nh 8: Máº¥t mÃ¡t xÃ¡c thá»±c cho cÃ¡c mÃ´ hÃ¬nh MatLM-Attn 850M & baseline.

Báº£ng 7: LÆ°á»›i 2Ã—2 cá»§a cÃ¡c cáº·p Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ (Ä‘á»™ chÃ­nh xÃ¡c top-1 (%)) cÃ¡c hiá»‡u á»©ng cá»§a MatFormer vÃ  huáº¥n luyá»‡n tiÃªu chuáº©n trÃªn tiá»n huáº¥n luyá»‡n (PT) trÃªn ImageNet-21K vÃ  tinh chá»‰nh (FT) trÃªn ImageNet-1K sá»­ dá»¥ng kiáº¿n trÃºc L/16. Sá»­ dá»¥ng MatFormer trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n giÃºp mang láº¡i cÃ¡c bá»™ mÃ£ hÃ³a chÃ­nh xÃ¡c hÆ¡n vÃ  Ä‘Ã n há»“i cho viá»‡c sá»­ dá»¥ng downstream.

PTâ†“/ FTâ†’ # Tham sá»‘ (M) ViT MatViT
ViT 306 85.26 85.57
206 85.12 84.27
156 85.02 82.79
131 84.42 82.1
MatViT 306 85.58 85.61
206 â€“ 85.40
156 â€“ 85.02
131 â€“ 84.41

G.2 ÄÃ¡nh giÃ¡ Truy xuáº¥t HÃ¬nh áº£nh Truyá»n thá»‘ng
Báº£ng 8 thá»ƒ hiá»‡n Ä‘Ã¡nh giÃ¡ truy xuáº¥t hÃ¬nh áº£nh truyá»n thá»‘ng trÃªn ImageNet-1K nÆ¡i bá»™ mÃ£ hÃ³a truy váº¥n vÃ  tÃ i liá»‡u giá»‘ng nhau cho truy xuáº¥t lÃ¡ng giá»ng gáº§n nháº¥t. ÄÃ¡nh giÃ¡ dá»±a trÃªn 1-lÃ¡ng giá»ng gáº§n nháº¥t (NN) tuÃ¢n theo cháº·t cháº½ káº¿t quáº£ phÃ¢n loáº¡i má»™t-so vá»›i-táº¥t cáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4. Cáº£ hai biáº¿n thá»ƒ MatViT B/16 vÃ  L/16 cÃ³ cÃ¡c mÃ´ hÃ¬nh con cÃ³ hiá»‡u suáº¥t truy xuáº¥t tá»‘t hoáº·c tá»‘t hÆ¡n so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p cá»§a chÃºng. Cá»¥ thá»ƒ, truy xuáº¥t dá»±a trÃªn MatViT cÃ³ thá»ƒ chÃ­nh xÃ¡c hÆ¡n Ä‘áº¿n 0.5% so vá»›i cÃ¡c baseline trong khi má»™t mÃ´ hÃ¬nh con MatViT 200M tham sá»‘ cÃ³ thá»ƒ chÃ­nh xÃ¡c hÆ¡n baseline ViT 300M tham sá»‘.

--- TRANG 22 ---
Báº£ng 8: Äá»™ chÃ­nh xÃ¡c truy xuáº¥t hÃ¬nh áº£nh 1-NN (%) khi bá»™ mÃ£ hÃ³a truy váº¥n vÃ  tÃ i liá»‡u lÃ  cÃ¹ng má»™t mÃ´ hÃ¬nh. TÆ°Æ¡ng tá»± nhÆ° káº¿t quáº£ phÃ¢n loáº¡i hÃ¬nh áº£nh, cÃ¡c biáº¿n thá»ƒ MatViT hoáº·c khá»›p hoáº·c vÆ°á»£t trá»™i hÆ¡n cÃ¡c Ä‘á»‘i tÃ¡c ViT tiÃªu chuáº©n tÆ°Æ¡ng á»©ng. LÆ°u Ã½ ráº±ng táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n cá»§a má»™t mÃ´ hÃ¬nh nháº¥t Ä‘á»‹nh trong MatViT Ä‘Æ°á»£c trÃ­ch xuáº¥t miá»…n phÃ­ trong khi cÃ¡c baseline pháº£i Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng cho cÃ¡c rÃ ng buá»™c.

Bá»™ mÃ£ hÃ³a # Tham sá»‘ (M) ViT MatViT
B/16 85 77.46 77.38
57 76.58 76.41
43 74.90 74.49
36 71.44 71.72
L/16 300 83.17 83.67
200 82.92 83.23
150 82.81 82.89
125 82.22 82.14

[CÃ¡c hÃ¬nh 9, 10, 11 Ä‘Æ°á»£c giá»¯ nguyÃªn vá»›i biá»ƒu Ä‘á»“ vá» xu hÆ°á»›ng má»Ÿ rá»™ng]

HÃ¬nh 9: ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh MatLM chá»‰ giáº£i mÃ£ khÃ¡c nhau á»Ÿ má»™t dáº£i kÃ­ch thÆ°á»›c tá»« 78M Ä‘áº¿n 850M tham sá»‘ vÃ  quan sÃ¡t xu hÆ°á»›ng má»Ÿ rá»™ng cho má»—i má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh trÃªn máº¥t mÃ¡t xÃ¡c thá»±c. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng khoáº£ng cÃ¡ch giá»¯a MatLM vÃ  baseline dÆ°á»ng nhÆ° lÃ  háº±ng sá»‘ táº¡i má»—i má»©c Ä‘á»™ chi tiáº¿t. TÃ­nh nháº¥t quÃ¡n giá»¯a cÃ¡c mÃ´ hÃ¬nh con cá»§a cÃ¡c má»©c Ä‘á»™ chi tiáº¿t vÃ  cÃ¡c mÃ´ hÃ¬nh XL cho tháº¥y hiá»‡u á»©ng cá»§a viá»‡c huáº¥n luyá»‡n chung MatFormer trÃªn viá»‡c Ä‘áº£m báº£o báº£n Ä‘á»‹a hÃ nh vi tÆ°Æ¡ng tá»± qua cÃ¡c mÃ´ hÃ¬nh con.

--- TRANG 23 ---
[HÃ¬nh 10 Ä‘Æ°á»£c giá»¯ nguyÃªn]

HÃ¬nh 10: ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh MatLM chá»‰ giáº£i mÃ£ khÃ¡c nhau á»Ÿ má»™t dáº£i kÃ­ch thÆ°á»›c tá»« 78M Ä‘áº¿n 850M tham sá»‘ vÃ  quan sÃ¡t xu hÆ°á»›ng má»Ÿ rá»™ng cho má»—i má»©c Ä‘á»™ chi tiáº¿t mÃ´ hÃ¬nh cho Ä‘iá»ƒm trung bÃ¬nh trÃªn Ä‘Ã¡nh giÃ¡ má»™t láº§n. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng khoáº£ng cÃ¡ch giá»¯a MatLM vÃ  baseline váº«n giá»¯ nguyÃªn vá»›i quy mÃ´, vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline cho táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ chi tiáº¿t cho cÃ¡c mÃ´ hÃ¬nh lá»›n nháº¥t.

--- TRANG 24 ---
[HÃ¬nh 11 Ä‘Æ°á»£c giá»¯ nguyÃªn]

HÃ¬nh 11: ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh MatLM chá»‰ giáº£i mÃ£ khÃ¡c nhau á»Ÿ má»™t dáº£i kÃ­ch thÆ°á»›c tá»« 78M Ä‘áº¿n 850M tham sá»‘ vÃ  quan sÃ¡t xu hÆ°á»›ng má»Ÿ rá»™ng cho má»—i mÃ´ hÃ¬nh con S, M, L cho tÃ­nh nháº¥t quÃ¡n vá»›i mÃ´ hÃ¬nh XL. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng khoáº£ng cÃ¡ch giá»¯a MatLM vÃ  baseline giáº£m vá»›i quy mÃ´, nhÆ°ng ngÆ°á»i ta sáº½ cáº§n má»Ÿ rá»™ng baseline nhiá»u báº­c Ä‘á»™ lá»›n Ä‘á»ƒ cÃ³ tÃ­nh nháº¥t quÃ¡n so sÃ¡nh Ä‘Æ°á»£c vá»›i MatLM.

--- TRANG 25 ---
[CÃ¡c báº£ng 9-13 Ä‘Æ°á»£c giá»¯ nguyÃªn vá»›i sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ downstream vÃ  máº¥t mÃ¡t]

--- TRANG 26 ---
[Tiáº¿p tá»¥c cÃ¡c báº£ng Ä‘Ã¡nh giÃ¡]

--- TRANG 27 ---
[Tiáº¿p tá»¥c cÃ¡c báº£ng Ä‘Ã¡nh giÃ¡]

--- TRANG 28 ---
[Báº£ng 14 vÃ  danh sÃ¡ch kiá»ƒm tra NeurIPS Ä‘Æ°á»£c giá»¯ nguyÃªn]

Danh sÃ¡ch Kiá»ƒm tra BÃ i bÃ¡o NeurIPS
1. TuyÃªn bá»‘
CÃ¢u há»i: CÃ¡c tuyÃªn bá»‘ chÃ­nh Ä‘Æ°á»£c Ä‘Æ°a ra trong tÃ³m táº¯t vÃ  giá»›i thiá»‡u cÃ³ pháº£n Ã¡nh chÃ­nh xÃ¡c Ä‘Ã³ng gÃ³p vÃ  pháº¡m vi cá»§a bÃ i bÃ¡o khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: Táº¥t cáº£ cÃ¡c tuyÃªn bá»‘ trong tÃ³m táº¯t Ä‘Æ°á»£c há»— trá»£ bá»Ÿi káº¿t quáº£ thá»±c nghiá»‡m trong Pháº§n 4.

2. Háº¡n cháº¿
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ tháº£o luáº­n vá» nhá»¯ng háº¡n cháº¿ cá»§a cÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi cÃ¡c tÃ¡c giáº£ khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: ChÃºng tÃ´i tháº£o luáº­n Ä‘iá»u nÃ y xuyÃªn suá»‘t báº£n tháº£o.

3. Giáº£ Ä‘á»‹nh vÃ  Chá»©ng minh LÃ½ thuyáº¿t
CÃ¢u há»i: Äá»‘i vá»›i má»—i káº¿t quáº£ lÃ½ thuyáº¿t, bÃ i bÃ¡o cÃ³ cung cáº¥p táº­p há»£p Ä‘áº§y Ä‘á»§ cÃ¡c giáº£ Ä‘á»‹nh vÃ  má»™t chá»©ng minh hoÃ n chá»‰nh (vÃ  chÃ­nh xÃ¡c) khÃ´ng?
Tráº£ lá»i: [NA].
LÃ½ do: KhÃ´ng cÃ³ káº¿t quáº£ lÃ½ thuyáº¿t.

4. Kháº£ nÄƒng TÃ¡i táº¡o Káº¿t quáº£ Thá»±c nghiá»‡m
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ tiáº¿t lá»™ Ä‘áº§y Ä‘á»§ táº¥t cáº£ thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ tÃ¡i táº¡o cÃ¡c káº¿t quáº£ thá»±c nghiá»‡m chÃ­nh cá»§a bÃ i bÃ¡o Ä‘áº¿n má»©c áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c tuyÃªn bá»‘ vÃ /hoáº·c káº¿t luáº­n chÃ­nh cá»§a bÃ i bÃ¡o khÃ´ng (báº¥t ká»ƒ mÃ£ vÃ  dá»¯ liá»‡u cÃ³ Ä‘Æ°á»£c cung cáº¥p hay khÃ´ng)?
Tráº£ lá»i: [CÃ³]
LÃ½ do: Chi tiáº¿t triá»ƒn khai Ä‘Æ°á»£c cung cáº¥p trong Phá»¥ lá»¥c B. HÆ¡n ná»¯a, mÃ£ Ä‘á»ƒ tÃ¡i táº¡o Pháº§n 4.2 sáº½ Ä‘Æ°á»£c mÃ£ nguá»“n má»Ÿ táº¡i liÃªn káº¿t.

5. Truy cáº­p Má»Ÿ Ä‘áº¿n Dá»¯ liá»‡u vÃ  MÃ£
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ cung cáº¥p truy cáº­p má»Ÿ Ä‘áº¿n dá»¯ liá»‡u vÃ  mÃ£, vá»›i hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tÃ¡i táº¡o trung thá»±c cÃ¡c káº¿t quáº£ thá»±c nghiá»‡m chÃ­nh, nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong tÃ i liá»‡u bá»• sung khÃ´ng?
Tráº£ lá»i: [KhÃ´ng]
LÃ½ do: Chi tiáº¿t triá»ƒn khai Ä‘Æ°á»£c cung cáº¥p trong Phá»¥ lá»¥c B. Máº·c dÃ¹ mÃ£ Ä‘á»ƒ tÃ¡i táº¡o Pháº§n 4.2 Ä‘Ã£ Ä‘Æ°á»£c mÃ£ nguá»“n má»Ÿ, mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u Ä‘á»™c quyá»n.

6. CÃ i Ä‘áº·t/Chi tiáº¿t Thá»±c nghiá»‡m
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ chá»‰ Ä‘á»‹nh táº¥t cáº£ cÃ¡c chi tiáº¿t huáº¥n luyá»‡n vÃ  kiá»ƒm tra (vÃ­ dá»¥, phÃ¢n chia dá»¯ liá»‡u, siÃªu tham sá»‘, cÃ¡ch chÃºng Ä‘Æ°á»£c chá»n, loáº¡i bá»™ tá»‘i Æ°u hÃ³a, v.v.) cáº§n thiáº¿t Ä‘á»ƒ hiá»ƒu káº¿t quáº£ khÃ´ng?
Tráº£ lá»i: [CÃ³]

--- TRANG 29 ---
LÃ½ do: Chi tiáº¿t triá»ƒn khai Ä‘Æ°á»£c cung cáº¥p trong Phá»¥ lá»¥c B, vá»›i trÃ­ch dáº«n cÃ¡c mÃ´ hÃ¬nh mÃ  nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn.

7. Ã nghÄ©a Thá»‘ng kÃª ThÃ­ nghiá»‡m
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ bÃ¡o cÃ¡o thanh lá»—i Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a phÃ¹ há»£p vÃ  chÃ­nh xÃ¡c hoáº·c thÃ´ng tin thÃ­ch há»£p khÃ¡c vá» Ã½ nghÄ©a thá»‘ng kÃª cá»§a cÃ¡c thÃ­ nghiá»‡m khÃ´ng?
Tráº£ lá»i: [KhÃ´ng]
LÃ½ do: Äiá»u nÃ y chÆ°a Ä‘Æ°á»£c thá»±c hiá»‡n do chi phÃ­ tÃ­nh toÃ¡n Ä‘áº¯t Ä‘á» cá»§a viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»« Ä‘áº§u.

8. TÃ i nguyÃªn TÃ­nh toÃ¡n ThÃ­ nghiá»‡m
CÃ¢u há»i: Äá»‘i vá»›i má»—i thÃ­ nghiá»‡m, bÃ i bÃ¡o cÃ³ cung cáº¥p thÃ´ng tin Ä‘áº§y Ä‘á»§ vá» tÃ i nguyÃªn mÃ¡y tÃ­nh (loáº¡i cÃ´ng nhÃ¢n tÃ­nh toÃ¡n, bá»™ nhá»›, thá»i gian thá»±c hiá»‡n) cáº§n thiáº¿t Ä‘á»ƒ tÃ¡i táº¡o cÃ¡c thÃ­ nghiá»‡m khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: ThÃ´ng tin nÃ y Ä‘Æ°á»£c cung cáº¥p trong Phá»¥ lá»¥c B.

9. Quy táº¯c Äáº¡o Ä‘á»©c
CÃ¢u há»i: NghiÃªn cá»©u Ä‘Æ°á»£c tiáº¿n hÃ nh trong bÃ i bÃ¡o cÃ³ tuÃ¢n thá»§, á»Ÿ má»i khÃ­a cáº¡nh, vá»›i Quy táº¯c Äáº¡o Ä‘á»©c NeurIPS https://neurips.cc/public/EthicsGuidelines khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: TÃ¡c Ä‘á»™ng rá»™ng hÆ¡n Ä‘Ã£ Ä‘Æ°á»£c tháº£o luáº­n trong Phá»¥ lá»¥c A.

10. TÃ¡c Ä‘á»™ng Rá»™ng hÆ¡n
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ tháº£o luáº­n cáº£ tÃ¡c Ä‘á»™ng xÃ£ há»™i tÃ­ch cá»±c tiá»m nÄƒng vÃ  tÃ¡c Ä‘á»™ng xÃ£ há»™i tiÃªu cá»±c cá»§a cÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: TÃ¡c Ä‘á»™ng rá»™ng hÆ¡n Ä‘Ã£ Ä‘Æ°á»£c tháº£o luáº­n trong Phá»¥ lá»¥c A.

11. Biá»‡n phÃ¡p Báº£o vá»‡
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ mÃ´ táº£ cÃ¡c biá»‡n phÃ¡p báº£o vá»‡ Ä‘Ã£ Ä‘Æ°á»£c Ä‘Æ°a ra cho viá»‡c phÃ¡t hÃ nh cÃ³ trÃ¡ch nhiá»‡m dá»¯ liá»‡u hoáº·c mÃ´ hÃ¬nh cÃ³ nguy cÆ¡ cao bá»‹ láº¡m dá»¥ng khÃ´ng (vÃ­ dá»¥, mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n, trÃ¬nh táº¡o hÃ¬nh áº£nh, hoáº·c táº­p dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p)?
Tráº£ lá»i: [NA]
LÃ½ do: KhÃ´ng cÃ³ táº­p dá»¯ liá»‡u hoáº·c mÃ´ hÃ¬nh nÃ o Ä‘Æ°á»£c phÃ¡t hÃ nh vá»›i bÃ i bÃ¡o nÃ y.

12. Giáº¥y phÃ©p cho TÃ i sáº£n Hiá»‡n cÃ³
CÃ¢u há»i: Nhá»¯ng ngÆ°á»i táº¡o ra hoáº·c chá»§ sá»Ÿ há»¯u ban Ä‘áº§u cá»§a tÃ i sáº£n (vÃ­ dá»¥, mÃ£, dá»¯ liá»‡u, mÃ´ hÃ¬nh), Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i bÃ¡o, cÃ³ Ä‘Æ°á»£c ghi cÃ´ng Ä‘Ãºng cÃ¡ch vÃ  giáº¥y phÃ©p vÃ  Ä‘iá»u khoáº£n sá»­ dá»¥ng Ä‘Æ°á»£c Ä‘á» cáº­p rÃµ rÃ ng vÃ  Ä‘Æ°á»£c tÃ´n trá»ng Ä‘Ãºng cÃ¡ch khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: CÃ¡c mÃ´ hÃ¬nh thá»‹ giÃ¡c vÃ  dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng trong nÃ y Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng khai trong má»™t thá»i gian dÃ i. CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ káº¿ thá»«a cÃ¡c giáº¥y phÃ©p thÃ­ch há»£p cá»§a bÃ i bÃ¡o Lamda [58] trong khi chÃºng tÃ´i huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u Ä‘á»™c quyá»n.

13. TÃ i sáº£n Má»›i
CÃ¢u há»i: CÃ¡c tÃ i sáº£n má»›i Ä‘Æ°á»£c giá»›i thiá»‡u trong bÃ i bÃ¡o cÃ³ Ä‘Æ°á»£c tÃ i liá»‡u hÃ³a tá»‘t vÃ  tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p cÃ¹ng vá»›i cÃ¡c tÃ i sáº£n khÃ´ng?
Tráº£ lá»i: [CÃ³]
LÃ½ do: MÃ£ Ä‘á»ƒ tÃ¡i táº¡o cÃ¡c thÃ­ nghiá»‡m sáº½ Ä‘Æ°á»£c phÃ¡t hÃ nh cho camera ready.

HÆ°á»›ng dáº«n:
â€¢ Tráº£ lá»i NA cÃ³ nghÄ©a lÃ  bÃ i bÃ¡o khÃ´ng phÃ¡t hÃ nh tÃ i sáº£n má»›i.
â€¢ CÃ¡c nhÃ  nghiÃªn cá»©u nÃªn truyá»n Ä‘áº¡t chi tiáº¿t cá»§a táº­p dá»¯ liá»‡u/mÃ£/mÃ´ hÃ¬nh nhÆ° má»™t pháº§n cá»§a bÃ i ná»™p cá»§a há» thÃ´ng qua cÃ¡c máº«u cÃ³ cáº¥u trÃºc. Äiá»u nÃ y bao gá»“m chi tiáº¿t vá» huáº¥n luyá»‡n, giáº¥y phÃ©p, háº¡n cháº¿, v.v.

--- TRANG 30 ---
â€¢ BÃ i bÃ¡o nÃªn tháº£o luáº­n liá»‡u vÃ  cÃ¡ch thá»©c Ä‘á»“ng Ã½ Ä‘Æ°á»£c thu tháº­p tá»« nhá»¯ng ngÆ°á»i cÃ³ tÃ i sáº£n Ä‘Æ°á»£c sá»­ dá»¥ng.
â€¢ Táº¡i thá»i Ä‘iá»ƒm ná»™p bÃ i, hÃ£y nhá»› áº©n danh tÃ i sáº£n cá»§a báº¡n (náº¿u cÃ³ thá»ƒ Ã¡p dá»¥ng). Báº¡n cÃ³ thá»ƒ táº¡o má»™t URL áº©n danh hoáº·c bao gá»“m má»™t tá»‡p zip áº©n danh.

14. Crowdsourcing vÃ  NghiÃªn cá»©u vá»›i Äá»‘i tÆ°á»£ng Con ngÆ°á»i
CÃ¢u há»i: Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m crowdsourcing vÃ  nghiÃªn cá»©u vá»›i Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i, bÃ i bÃ¡o cÃ³ bao gá»“m toÃ n bá»™ vÄƒn báº£n hÆ°á»›ng dáº«n Ä‘Æ°á»£c Ä‘Æ°a cho ngÆ°á»i tham gia vÃ  áº£nh chá»¥p mÃ n hÃ¬nh, náº¿u cÃ³ thá»ƒ Ã¡p dá»¥ng, cÅ©ng nhÆ° chi tiáº¿t vá» bá»“i thÆ°á»ng (náº¿u cÃ³) khÃ´ng?
Tráº£ lá»i: [NA]
LÃ½ do: BÃ i bÃ¡o khÃ´ng liÃªn quan Ä‘áº¿n crowdsourcing hoáº·c nghiÃªn cá»©u vá»›i Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i.

HÆ°á»›ng dáº«n:
â€¢ Tráº£ lá»i NA cÃ³ nghÄ©a lÃ  bÃ i bÃ¡o khÃ´ng liÃªn quan Ä‘áº¿n crowdsourcing hoáº·c nghiÃªn cá»©u vá»›i Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i.
â€¢ Bao gá»“m thÃ´ng tin nÃ y trong tÃ i liá»‡u bá»• sung lÃ  tá»‘t, nhÆ°ng náº¿u Ä‘Ã³ng gÃ³p chÃ­nh cá»§a bÃ i bÃ¡o liÃªn quan Ä‘áº¿n Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i, thÃ¬ cÃ ng nhiá»u chi tiáº¿t cÃ ng tá»‘t nÃªn Ä‘Æ°á»£c bao gá»“m trong bÃ i bÃ¡o chÃ­nh.
â€¢ Theo Quy táº¯c Äáº¡o Ä‘á»©c NeurIPS, nhá»¯ng ngÆ°á»i lao Ä‘á»™ng tham gia vÃ o thu tháº­p dá»¯ liá»‡u, quáº£n lÃ½, hoáº·c lao Ä‘á»™ng khÃ¡c nÃªn Ä‘Æ°á»£c tráº£ Ã­t nháº¥t má»©c lÆ°Æ¡ng tá»‘i thiá»ƒu trong nÆ°á»›c cá»§a ngÆ°á»i thu tháº­p dá»¯ liá»‡u.

15. PhÃª duyá»‡t Há»™i Ä‘á»“ng ÄÃ¡nh giÃ¡ Thá»ƒ cháº¿ (IRB) hoáº·c TÆ°Æ¡ng Ä‘Æ°Æ¡ng cho NghiÃªn cá»©u vá»›i Äá»‘i tÆ°á»£ng Con ngÆ°á»i
CÃ¢u há»i: BÃ i bÃ¡o cÃ³ mÃ´ táº£ cÃ¡c rá»§i ro tiá»m áº©n mÃ  ngÆ°á»i tham gia nghiÃªn cá»©u pháº£i chá»‹u, liá»‡u nhá»¯ng rá»§i ro Ä‘Ã³ cÃ³ Ä‘Æ°á»£c tiáº¿t lá»™ cho Ä‘á»‘i tÆ°á»£ng khÃ´ng, vÃ  liá»‡u phÃª duyá»‡t Há»™i Ä‘á»“ng ÄÃ¡nh giÃ¡ Thá»ƒ cháº¿ (IRB) (hoáº·c phÃª duyá»‡t/Ä‘Ã¡nh giÃ¡ tÆ°Æ¡ng Ä‘Æ°Æ¡ng dá»±a trÃªn yÃªu cáº§u cá»§a quá»‘c gia hoáº·c thá»ƒ cháº¿ cá»§a báº¡n) cÃ³ Ä‘Æ°á»£c thu tháº­p khÃ´ng?
Tráº£ lá»i: [NA]
LÃ½ do: BÃ i bÃ¡o khÃ´ng liÃªn quan Ä‘áº¿n crowdsourcing hoáº·c nghiÃªn cá»©u vá»›i Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i.

HÆ°á»›ng dáº«n:
â€¢ Tráº£ lá»i NA cÃ³ nghÄ©a lÃ  bÃ i bÃ¡o khÃ´ng liÃªn quan Ä‘áº¿n crowdsourcing hoáº·c nghiÃªn cá»©u vá»›i Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i.
â€¢ TÃ¹y thuá»™c vÃ o quá»‘c gia nÆ¡i nghiÃªn cá»©u Ä‘Æ°á»£c tiáº¿n hÃ nh, phÃª duyá»‡t IRB (hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng) cÃ³ thá»ƒ Ä‘Æ°á»£c yÃªu cáº§u cho báº¥t ká»³ nghiÃªn cá»©u Ä‘á»‘i tÆ°á»£ng con ngÆ°á»i nÃ o. Náº¿u báº¡n cÃ³ Ä‘Æ°á»£c phÃª duyá»‡t IRB, báº¡n nÃªn nÃªu rÃµ Ä‘iá»u nÃ y trong bÃ i bÃ¡o.
â€¢ ChÃºng tÃ´i nháº­n ra ráº±ng cÃ¡c thá»§ tá»¥c cho Ä‘iá»u nÃ y cÃ³ thá»ƒ khÃ¡c nhau Ä‘Ã¡ng ká»ƒ giá»¯a cÃ¡c thá»ƒ cháº¿ vÃ  Ä‘á»‹a Ä‘iá»ƒm, vÃ  chÃºng tÃ´i mong Ä‘á»£i cÃ¡c tÃ¡c giáº£ tuÃ¢n thá»§ Quy táº¯c Äáº¡o Ä‘á»©c NeurIPS vÃ  cÃ¡c hÆ°á»›ng dáº«n cho thá»ƒ cháº¿ cá»§a há».
â€¢ Äá»‘i vá»›i bÃ i ná»™p ban Ä‘áº§u, khÃ´ng bao gá»“m báº¥t ká»³ thÃ´ng tin nÃ o cÃ³ thá»ƒ phÃ¡ vá»¡ tÃ­nh áº©n danh (náº¿u cÃ³ thá»ƒ Ã¡p dá»¥ng), cháº³ng háº¡n nhÆ° thá»ƒ cháº¿ tiáº¿n hÃ nh Ä‘Ã¡nh giÃ¡.
