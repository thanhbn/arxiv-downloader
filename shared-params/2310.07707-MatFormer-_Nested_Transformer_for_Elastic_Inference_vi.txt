# 2310.07707.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/shared-params/2310.07707.pdf
# Kích thước tệp: 1136961 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
MatFormer: Transformer Lồng Nhau cho Suy Luận Đàn Hồi
Devvrit∗∆⋄Sneha Kudugunta∗†⋄Aditya Kusupati∗†⋄+
Tim Dettmers†Kaifeng Chen⋄Inderjit Dhillon⋄∆Yulia Tsvetkov†Hannaneh Hajishirzi†
Sham Kakade‡Ali Farhadi†Prateek Jain⋄+
⋄Google DeepMind∆University of Texas at Austin†University of Washington‡Harvard University
Tóm tắt
Các mô hình nền tảng được áp dụng trong một phổ rộng các cài đặt với các ràng buộc suy luận khác nhau, từ các cụm đa gia tốc lớn đến các thiết bị di động độc lập có tài nguyên hạn chế. Tuy nhiên, chi phí đáng kể liên quan đến việc huấn luyện những mô hình này thường giới hạn số lượng kích thước mô hình duy nhất có thể được cung cấp. Do đó, các nhà thực hành buộc phải chọn một mô hình có thể không phù hợp tối ưu với các yêu cầu độ trễ và chi phí cụ thể của họ. Chúng tôi trình bày MatFormer2, một kiến trúc Transformer mới được thiết kế để cung cấp suy luận đàn hồi trên các ràng buộc triển khai đa dạng. MatFormer đạt được điều này bằng cách kết hợp một cấu trúc khối Mạng Truyền Tiến Lồng Nhau (FFN) trong một mô hình Transformer tiêu chuẩn. Trong quá trình huấn luyện, chúng tôi tối ưu hóa các tham số của nhiều khối FFN lồng nhau với các kích thước khác nhau, cho phép trích xuất hàng trăm mô hình nhỏ hơn chính xác mà không phát sinh chi phí tính toán bổ sung. Chúng tôi xác thực thực nghiệm hiệu quả của MatFormer trên các lớp mô hình khác nhau (bộ giải mã và bộ mã hóa) và các phương thức (ngôn ngữ và thị giác), chứng minh tiềm năng của nó cho việc triển khai thực tế. Chúng tôi chỉ ra rằng một mô hình ngôn ngữ MatFormer chỉ giải mã 850M (MatLM) cho phép chúng tôi trích xuất nhiều mô hình nhỏ hơn từ 582M đến 850M tham số, mỗi mô hình thể hiện mất mát xác thực tốt hơn và đánh giá downstream một lần so với các đối tác được huấn luyện độc lập. Hơn nữa, chúng tôi quan sát thấy rằng các bộ mã hóa nhỏ hơn được trích xuất từ một bộ mã hóa MatFormer-based ViT (MatViT) phổ quát bảo tồn cấu trúc không gian metric cho việc truy xuất thích ứng quy mô lớn. Cuối cùng, chúng tôi chứng minh rằng việc giải mã suy đoán với các mô hình con chính xác và nhất quán được trích xuất từ MatFormer có thể dẫn đến giảm đáng kể độ trễ suy luận. Trang web dự án.

1 Giới thiệu
Các mô hình nền tảng lớn [49,45,17] được triển khai trong nhiều cài đặt khác nhau với các yêu cầu tính toán và độ chính xác khác nhau như phản hồi thời gian thực trên điện thoại di động hoặc trên GPU đa cụm cho phục vụ lô quy mô web. Tuy nhiên, các họ mô hình điển hình chỉ cung cấp một vài mô hình được huấn luyện độc lập với các kích thước khác nhau. Ví dụ, họ Llama-2 cung cấp các mô hình với 7B, 13B, 34B và 70B tham số [59]. Vì vậy, các nhà thực hành buộc phải chọn một mô hình nhỏ hơn (và thường ít chính xác hơn) so với ngân sách độ trễ/chi phí của họ. Thay vào đó, người ta có thể sử dụng nén/cắt tỉa để phù hợp với một mô hình lớn hơn trong một ngân sách tính toán nhất định [19, 36, 53], nhưng điều đó yêu cầu huấn luyện bổ sung.

Chúng tôi giới thiệu MatFormer, một kiến trúc Transformer [61] đàn hồi bản địa vượt qua thách thức này. MatFormer cho phép huấn luyện một mô hình phổ quát có thể được sử dụng để trích xuất hàng trăm mô hình con nhỏ hơn mà không có bất kỳ chi phí huấn luyện bổ sung nào (Hình 1). MatFormer là một kiến trúc tổng quát

∗Đóng góp kỹ thuật ngang nhau.+Aditya Kusupati và Prateek Jain dẫn dắt dự án.
Liên hệ: devvrit@cs.utexas.edu,{snehakudugunta,kusupati,prajain}@google.com
2MatFormer viết tắt của Matryoshka Transformer, phản ánh cấu trúc lồng nhau cố hữu của mô hình.
Hội nghị lần thứ 38 về Các Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2024).arXiv:2310.07707v2 [cs.LG] 15 Dec 2024

--- TRANG 2 ---
Huấn luyện Add & Norm 
𝞼 ( )
Add & Norm 
Attention 
FFN 
Suy luận
Khối MatFormer
MatLM-S MatLM-M MatLM-L MatLM-XL 
Trộn & Kết hợp 

Hình 1: MatFormer giới thiệu cấu trúc lồng nhau vào khối FFN của Transformer & huấn luyện tất cả các mô hình con, cho phép trích xuất miễn phí hàng trăm mô hình con chính xác cho suy luận đàn hồi.

kiến trúc có thể được áp dụng cho các bộ mã hóa và bộ giải mã, bất khả tri về miền và tương thích với các quy trình huấn luyện mô hình nền tảng tiêu chuẩn.

MatFormer tuân theo nguyên tắc học biểu diễn matryoshka [34], để giới thiệu cấu trúc con lồng nhau bên trong khối Transformer tiêu chuẩn. Chính thức, MatFormer định nghĩa các khối Transformer Ti, sao cho T1⊂T2⊂ ··· ⊂ Tg, trong đó g là số lượng khối transformer lồng nhau, và quan hệ Ti⊂Ti+1 chỉ ra rằng các tham số của Ti được chứa trong những tham số của Ti+1. MatFormer có thể tạo ra cấu trúc con như vậy trong cả khối attention và mạng truyền tiến (FFN) của Transformer (xem Hình 1). Hãy xem xét, chẳng hạn, một khối FFN có dff neuron trong lớp ẩn. Khi đó, MatFormer tạo ra cấu trúc matryoshka trên những neuron này, trong đó Ti chứa mi neuron đầu tiên và 1≤m1< m 2···< m g=dff biểu thị số lượng neuron cho mỗi mức độ chi tiết hoặc mô hình con. Trực giác, điều này ngụ ý rằng m1 neuron đầu tiên là những neuron "quan trọng nhất" vì chúng thuộc về tất cả các khối, tiếp theo là m2−m1 neuron tiếp theo, và cứ thế.

Khác với các công trình liên quan (Phần 2), mặc dù chỉ tối ưu hóa cho g mức độ chi tiết, chúng tôi có thể trích xuất số mô hình con tăng theo cấp số nhân sau huấn luyện. Sử dụng các khối MatFormer đã được huấn luyện T1, . . . , T g tại mỗi lớp, người ta có thể tạo thành các mô hình mới bằng Trộn & Kết hợp (Phần 3.3), tức là bằng cách lấy một kết hợp tùy ý của các khối này qua các lớp. Ví dụ, trong lớp đầu tiên, người ta có thể chọn Tg, khối lớn nhất, chọn T2 trong lớp thứ hai, và cứ thế, tạo thành gl mô hình khác nhau (trong đó l là số lượng lớp). Đáng ngạc nhiên, trong nhiều cài đặt và cho các kích thước mô hình khác nhau, chúng tôi quan sát thấy rằng các mô hình được trích xuất thực sự chính xác, với độ chính xác tỷ lệ thuận với kích thước của mô hình được trích xuất.

Chúng tôi huấn luyện các Mô hình Ngôn ngữ chỉ giải mã dựa trên Matformer (MatLM) lên đến 850M tham số và quan sát thấy rằng: (a) MatLM được huấn luyện rõ ràng với g mức độ chi tiết cách nhau theo cấp số nhân vượt trội về mất mát xác thực và đánh giá downstream một lần của g mô hình cơ sở tương ứng được huấn luyện độc lập từ đầu, (b) các mô hình được trích xuất của chúng tôi sử dụng Trộn & Kết hợp nằm trên đường cong đánh đổi độ chính xác-so với-tham số được tạo ra bởi g mô hình được huấn luyện rõ ràng, (c) thông qua các thí nghiệm mở rộng, chúng tôi quan sát thấy rằng luật mất mát so với tính toán cho các mô hình MatFormer khác nhau vẫn tương tự như các mô hình Transformer vanilla qua các mức độ chi tiết khác nhau và (d) các mô hình con được trích xuất từ MatLM có hành vi nhất quán cao rất mong muốn cho các tối ưu hóa suy luận và triển khai qua các quy mô.

Chúng tôi nghiên cứu thêm các mô hình MatFormer-based ViT (MatViT) và đưa ra những quan sát tương tự. Ví dụ, MatViT-L/16 cải thiện độ chính xác của mô hình ViT-L/16 tiêu chuẩn trên ImageNet-1K, và các mô hình con được trích xuất đều khớp hoặc thậm chí hoạt động tốt hơn so với các đường cơ sở được huấn luyện độc lập. Hơn nữa, chúng tôi chứng minh rằng, do tính nhất quán cao, các mô hình MatViT có thể được sử dụng như "bộ mã hóa đàn hồi" cho việc truy xuất hình ảnh thích ứng. Nghĩa là, không gian metric của một hình ảnh được mã hóa bởi mô hình MatViT phổ quát (tức là lớn nhất) được bảo tồn gần đúng bởi các mô hình con lồng nhau. Do đó, dựa trên độ phức tạp truy vấn, tải hệ thống và các cân nhắc khác nhau, chúng tôi có thể sử dụng một trong những bộ mã hóa MatViT được trích xuất tại thời điểm suy luận để truy xuất trên một corpus cố định được mã hóa bởi mô hình phổ quát – cung cấp hơn 40% ít chi phí tính toán hơn với <0.5% giảm độ chính xác.

Chúng tôi đóng góp những điểm chính sau:

--- TRANG 3 ---
1. Chúng tôi giới thiệu MatFormer, kết hợp cấu trúc con lồng nhau trong Transformer tiêu chuẩn và tối ưu hóa tất cả g mức độ chi tiết để tạo ra một mô hình đàn hồi phổ quát duy nhất.

2. Chúng tôi giới thiệu Trộn & Kết hợp, một heuristic đơn giản không có chi phí tính toán tìm ra các mô hình con tối ưu trong một ngân sách tham số nhất định, vượt trội hơn các phương pháp NAS phức tạp hơn. Điều này mang lại hàng trăm mô hình con chính xác và nhất quán mà không có bất kỳ chi phí huấn luyện nào (Phần 3).

3. MatFormer tổng quát hóa hiệu quả cho cả mô hình ngôn ngữ chỉ giải mã (MatLM) và bộ mã hóa thị giác (MatViT), mở rộng đáng tin cậy và chính xác như Transformer tiêu chuẩn, đồng thời cho phép tạo tự động hồi quy nhanh hơn đáng kể và truy xuất dày đặc thích ứng quy mô lớn (Phần 4).

2 Công trình liên quan
Transformer [61] đã trở thành kiến trúc mô hình thống nhất cho các mô hình nền tảng [7] qua các phương thức như ngôn ngữ [8], thị giác [17] và âm thanh [47]. Mặc dù mạnh mẽ, khối Transformer tiêu chuẩn không đàn hồi bản địa theo cách cho phép triển khai thích ứng và linh hoạt quy mô lớn qua các ràng buộc tài nguyên khác nhau. Để phục vụ cho vô số yêu cầu triển khai, các giải pháp hiện có bao gồm huấn luyện một họ mô hình với các kích thước khác nhau [49,2], các kỹ thuật hiệu quả sau hoc như lượng tử hóa [19], cắt tỉa [36], và chưng cất [53]. Tuy nhiên, những giải pháp này thường cụ thể cho ràng buộc đơn lẻ hiện tại và yêu cầu huấn luyện bổ sung cho mỗi trường hợp sử dụng downstream mới. Điều này khiến chúng xa với việc trở thành một giải pháp thực sự đàn hồi cho triển khai thích ứng. Cuối cùng, các LLM dựa trên Transformer thường được tăng tốc trong quá trình suy luận với các kỹ thuật như giải mã suy đoán [39,12] – có lợi từ việc các mô hình nháp nhỏ hơn & mô hình xác minh lớn hơn có hành vi tương tự – hoặc thoát sớm [54] để cho phép triển khai thời gian thực.

Việc có được nhiều mô hình nhỏ hơn từ một mô hình duy nhất đã được khám phá trong quá khứ [66,65,9,23,10] với hầu hết công trình tập trung vào các bộ mã hóa CNN. Trong công trình này, chúng tôi tập trung vào Transformer trong các mô hình ngôn ngữ chỉ giải mã và các mô hình thị giác được tiền huấn luyện. Cụ thể, OFA [9] huấn luyện một mô hình CNN giáo viên và sử dụng chưng cất để tinh chỉnh các mô hình con được lấy mẫu ngẫu nhiên (không lồng nhau) trong mô hình CNN học sinh phổ quát. Hơn nữa, OFA tập trung vào các mô hình quy mô nhỏ để triển khai trên các thiết bị cuối. Ngược lại, MatFormer không yêu cầu chưng cất, do đó sử dụng ít bộ nhớ hơn đáng kể và sử dụng các mô hình lồng nhau. Việc sử dụng các mô hình lồng nhau cho phép chúng tôi lưu trữ nhiều mô hình cùng nhau mà không tăng đáng kể dấu chân bộ nhớ của mô hình, điều này có lợi cho các tình huống mà chúng tôi muốn định tuyến các truy vấn thông qua các mạng con khác nhau. Mạng có thể thu nhỏ [66] tối ưu hóa chung và cung cấp các độ rộng thiết lập trước hạn chế. Mạng có thể thu nhỏ phổ quát [65] mở rộng điều này để lấy mẫu từ một không gian tìm kiếm liên tục của các mô hình con và tối ưu hóa chúng chung. Ngược lại, MatFormer lấy mẫu và chỉ tối ưu hóa một trong những mức độ chi tiết thiết lập trước. HAT [63] huấn luyện một mạng phổ quát chỉ để học hiệu suất tương đối cho các kiến trúc khác nhau. Để triển khai, các tác giả sử dụng NAS để tìm kiến trúc tối ưu và huấn luyện nó từ đầu trước khi phục vụ. Ngược lại, MatFormer không yêu cầu huấn luyện bổ sung và các mạng con chính xác có thể được thu được bằng cách sử dụng Trộn & Kết hợp (Phần 3.3) mang lại kết quả tốt như NAS mà không có sự phức tạp bổ sung. DynaBERT [27] huấn luyện chung một tập hợp cố định các mô hình con, không giới thiệu bất kỳ chiến lược tìm kiếm nào và chỉ thảo luận về việc sử dụng các mức độ chi tiết được huấn luyện rõ ràng làm mô hình con. Kết quả của việc tối ưu hóa chung của tất cả các mức độ chi tiết, DynaBERT mang lại ít cập nhật gradient hơn và do đó hiệu suất dưới tối ưu so với MatFormer trong khi sử dụng cùng tính toán và bộ nhớ (Phần 4).

Hơn nữa, chúng tôi nhấn mạnh rằng trong khi hầu hết công trình trong lĩnh vực này tối ưu hóa cho một số mô hình theo cấp số nhân, chúng tôi tối ưu hóa một số lượng nhỏ mạng con (g= 4) để có được một số mô hình theo cấp số nhân tại thời điểm suy luận dẫn đến độ chính xác tốt hơn đáng kể cho các tập dữ liệu lớn. Hơn

Bảng 1: So sánh MatFormer với các kỹ thuật có thể so sánh qua huấn luyện và suy luận. Chúng tôi nhấn mạnh rằng trái ngược với công trình trước đây, MatFormer yêu cầu tối ưu hóa ít mô hình hơn để có được một số mô hình theo cấp số nhân tại thời điểm suy luận mà không cần hậu huấn luyện hoặc NAS. Hơn nữa, các mạng con MatFormer được lồng nhau, cho phép truy xuất thích ứng & colocation của các mô hình trong quá trình suy luận. Ở đây, l là số lượng lớp trong mô hình và exp(l) chỉ cấp số nhân trong l.

N(Mô hình Được Tối ưu hóa) N(Mô hình Thu được) Lồng nhau? Lựa chọn Mô hình Cần Hậu huấn luyện? Kiến trúc Mô hình Giải mã?
MatFormer O(1) exp(l) ✓ Trộn & Kết hợp × Transformer ✓
OFA [9] exp(l) exp(l) × NAS × CNN ×
Mạng Thu nhỏ [66] O(1) O(1) ✓ - × CNN ×
HAT [63] exp(l) exp(l) × NAS ✓ CNN enc-dec
Mạng Đã sắp xếp [60] exp(l) exp(l) ✓ - × Cả hai ×
DynaBERT [27] O(1) O(1) ✓ - × Transformer ×

--- TRANG 4 ---
gần đây, một số công trình này đã được mở rộng cho các bộ mã hóa Transformer [11,52] để trích xuất các mô hình con trong cả cài đặt tĩnh hoặc động. Nhưng chúng hoặc thất bại trong việc mở rộng thêm cho các mô hình ngôn ngữ chỉ giải mã ([52]) hoặc hoạt động dưới tối ưu so với MatFormer (Phần 4) do những khác biệt trong phương pháp huấn luyện của chúng. Trong khi không ở không gian trọng số, học biểu diễn matryoshka [34] & FlexiViT [5] thể hiện tính đàn hồi trong không gian đầu ra & đầu vào tương ứng bằng cách bao quát suôn sẻ các ràng buộc triển khai với chi phí tối thiểu. MatFormer, ngược lại, xây dựng dựa trên những công trình này bằng cách tạo ra cấu trúc lồng nhau trong không gian trọng số thay vì cho phép các mô hình Transformer thực sự đàn hồi và thích ứng (giải mã & mã hóa) bao quát tất cả các đánh đổi độ chính xác-so với-tính toán (tĩnh hoặc động) với những thay đổi và chi phí huấn luyện tối thiểu (Hình 1). SortedNet [60] là một công trình đồng thời với mục tiêu tương tự tối ưu hóa nhiều mô hình con được lấy mẫu (giống như công trình trước) không giống như việc tối ưu hóa một vài (thường là 4) mô hình con lồng nhau của MatFormer. Chúng tôi cũng lưu ý FLEXTRON [64], một công trình gần đây xây dựng dựa trên MatFormer bằng cách mở rộng tính đàn hồi lồng nhau trong cả MLP và Attention Head đồng thời, vá đuôi kiểu huấn luyện Matformer, và bao gồm một bộ định tuyến để tự động định tuyến mỗi token trong các mức độ chi tiết khác nhau trong mỗi lớp.

Trong Bảng 1, chúng tôi tóm tắt những khác biệt giữa MatFormer và công trình liên quan đã thảo luận. Trong số những công trình này, chúng tôi đã xác định hai ý tưởng trung tâm thường được sử dụng: huấn luyện chung nhiều mô hình con và lấy mẫu các mô hình con ngẫu nhiên. Chúng tôi coi DynaBERT [27] và Once-for-All (OFA) [9] là những công trình liên quan nhất tương ứng và so sánh chúng với MatFormer trong Phần 4.

3 MatFormer
Trong phần này, chúng tôi định nghĩa cấu trúc con lồng nhau của MatFormer (Phần 3.1) và thảo luận về quy trình huấn luyện của nó cho g mức độ chi tiết mô hình đã chọn (Phần 3.2). Sau đó chúng tôi thảo luận về suy luận đàn hồi sử dụng các mô hình Trộn & Kết hợp (Phần 3.3) từ MatFormer cùng với các cân nhắc triển khai của nó.

3.1 Cấu trúc MatFormer
MatFormer định nghĩa g khối Transformer Ti, sao cho T1⊂T2⊂ ··· ⊂ Tg trong đó Ti⊂Ti+1 chỉ ra rằng các tham số của Ti được chứa trong những của Ti+1. Mặc dù có thể áp đặt cấu trúc như vậy trên bất kỳ phần nào của Transformer, chúng tôi chọn khối FFN để định nghĩa phương pháp của mình và trình bày phần lớn các thí nghiệm của chúng tôi, vì kích thước mô hình và chi phí tính toán của Transformer bị chi phối (khoảng 60% cho LLM và ViT) bởi khối FFN (xem Phụ lục C, và Phụ lục F.2 cho các thí nghiệm áp dụng MatFormer cho khối attention của Transformer). Vì vậy, trong công trình này, chúng tôi tập trung vào việc tạo ra cấu trúc con lồng nhau của MatFormer trong khối FFN. Sau đó chúng tôi xếp chồng các khối riêng lẻ (cho l lớp) để tạo thành g mô hình lồng nhau (M1...g) với các tham số chia sẻ tức là Mi⊂ M i+1.

Khối FFN Transformer có một lớp ẩn duy nhất với dff neuron và cả đầu vào và đầu ra trong Rdmodel, và tỷ lệ FFN cố định := dff/dmodel (thường ≥4). MatFormer giới thiệu cấu trúc lồng nhau matryoshka với g mức độ chi tiết trên biểu diễn ẩn của khối FFN. Cụ thể, một khối con lồng nhau của Transformer, Ti chứa mi neuron đầu tiên của FFN và 1≤m1< ···< m g=dff biểu thị số lượng neuron cho mỗi mức độ chi tiết hoặc mô hình con. Vì vậy, tùy thuộc vào mức độ chi tiết được chọn, hoạt động FFN của Ti tức là TFFN i trên một đầu vào x∈Rdmodel là:

TFFN i(x) =σ(x·W1[0 :mi]⊤)·W2[0 :mi], (1)

trong đó các ma trận trọng số của FFN là W1,W2∈Rdff×dmodel và các hạng tử bias được bỏ qua để đơn giản. W1[0 :k] biểu thị ma trận con với k hàng đầu tiên của W1. Cuối cùng, σ là một hàm phi tuyến thường được đặt thành GELU [24] hoặc ReLU bình phương [56]. Trong công trình này, chúng tôi chọn g= 4 mức độ chi tiết cách nhau theo cấp số nhân với tỷ lệ FFN của {0.5,1,2,4} tức là các neuron ẩn lồng nhau có kích thước {dff 8,dff 4,dff 2, dff}.

Chúng tôi có g mô hình con lồng nhau M1⊂ M 2. . . ,⊂ M g trong đó Mi←[Ti]×l, tức là Mi được tạo thành bằng cách xếp chồng Ti cho l lớp. Các ma trận nhúng đầu vào và đầu ra được chia sẻ qua các mô hình.

Chúng tôi lưu ý rằng chúng ta có thể tạo thành một cấu trúc con tương tự trên các đầu attention, với các đầu được tổ chức từ "quan trọng nhất" đến "ít quan trọng nhất", trong đó các đầu quan trọng hơn được chia sẻ bởi nhiều mô hình con hơn. Nghĩa là, chúng tôi sử dụng mi đầu attention đầu tiên cho mức độ chi tiết thứ i. Chúng tôi cũng có thể giới thiệu cấu trúc con này trong token embedding (dmodel) được cung cấp cho mỗi khối Transformer.

--- TRANG 5 ---
3.2 Huấn luyện
Đối với một mô hình Transformer M, lượt truyền tiến trên một đầu vào x được ký hiệu bởi M(x) và để L biểu thị hàm mất mát giữa đầu ra và mục tiêu y: L(M(x), y).

MatFormer dựa vào một chiến lược huấn luyện đơn giản là lấy mẫu ngẫu nhiên g mô hình con lồng nhau qua huấn luyện. Để làm điều này, cho mỗi bước chúng tôi lấy mẫu ngẫu nhiên một mức độ chi tiết Matformer i= 1,2..., g và huấn luyện cho nó sử dụng các bộ tối ưu hóa dựa trên gradient ngẫu nhiên tiêu chuẩn [55]:

LSAMPLING (x, y) =L(Mi(x), y), (2)

trong đó Mi là tập tham số của mô hình con mức độ chi tiết thứ i, với Mi được chọn từ một phân phối xác suất {p1, p2...pg}. Đối với hầu hết các thí nghiệm trong bài báo này, chúng tôi lấy mẫu đều mỗi mô hình con - trong Phụ lục F.3, chúng tôi thấy rằng việc điều chỉnh phân phối xác suất này có thể dẫn đến các mô hình con mạnh hơn.

Huấn luyện MatFormer dẫn đến g mô hình con lồng nhau chính xác M1...g bên trong mô hình MatFormer phổ quát (Mg), và cũng cho phép trích xuất hàng trăm mô hình con nhỏ hơn dọc theo đường cong độ chính xác-so với-tính toán được vẽ bởi g mô hình con được tối ưu hóa rõ ràng (Phần 3.3). Những mô hình này xuất hiện miễn phí sử dụng Trộn & Kết hợp trong quá trình suy luận và giảm đáng kể chi phí huấn luyện khấu hao trên mỗi mô hình thu được thông qua MatFormer. Phương pháp này dẫn đến các mô hình con nhỏ hơn có hành vi nhất quán cao (Phần 3.4) với mô hình phổ quát.

3.3 Trộn & Kết hợp
Tại thời điểm suy luận, việc trích xuất một trong g mô hình con M1⊂ M 2. . . ,⊂ M g bằng cách xếp chồng khối Transformer tương ứng Ti qua các lớp là tầm thường. Tuy nhiên, bằng cách chọn các mức độ chi tiết khác nhau cho mỗi lớp MatFormer, có thể tạo ra một số lượng lớn các mô hình nhỏ hơn chính xác miễn phí theo tổ hợp. Chúng tôi gọi quy trình đơn giản này là Trộn & Kết hợp và quan sát thấy rằng những mức độ chi tiết mô hình bổ sung này – chưa bao giờ được tối ưu hóa rõ ràng – có hiệu suất cao.

Đối với một ngân sách tính toán hoặc tham số nhất định, có nhiều mô hình con có thể. Một chiến lược phổ biến để chọn một mô hình con tối ưu là Tìm kiếm Kiến trúc Thần kinh (NAS) [48,68]. Tuy nhiên, điều này tốn kém về mặt tính toán (Phụ lục D.2). Với Trộn & Kết hợp, chúng tôi đề xuất việc tăng dần kích thước khối con với "độ dốc nhỏ nhất". Cụ thể hơn, chúng tôi khuyến nghị chọn các khối con với những thay đổi mức độ chi tiết tối thiểu qua các lớp, đảm bảo rằng kích thước của lớp thứ j ít nhất bằng của lớp thứ i cho j > i. Để đưa ra một ví dụ cụ thể, chúng tôi thấy rằng một mô hình con sử dụng mức độ chi tiết g2 cho một nửa các lớp sau đó g3 cho phần còn lại có thể sẽ tốt hơn một mô hình con sử dụng g1 và g4 cho kích thước mô hình tương tự. Heuristic của chúng tôi được hỗ trợ bởi phương pháp huấn luyện, trong đó mỗi mạng con được lấy mẫu duy trì mức độ chi tiết lớp nhất quán qua mô hình. Do đó, mô hình thích ứng tốt nhất với các cấu hình trong đó các mức độ chi tiết lớp hoặc đồng nhất hoặc hiển thị sự biến đổi tối thiểu. Trực giác này cũng được hỗ trợ bởi NAS, dự đoán các cấu hình cân bằng hơn so với các cấu hình lệch (Phụ lục D.1).

Trong số những cấu hình "cân bằng" này, chúng tôi thấy thực nghiệm rằng cấu hình tăng với độ dốc tối thiểu hoạt động tốt nhất. Trong Phần 4.1.1 và Phụ lục D.1, chúng tôi chỉ ra rằng Trộn & Kết hợp hoạt động ít nhất là tốt như việc sử dụng các phương pháp NAS dựa trên tìm kiếm tiến hóa [48] như được sử dụng bởi OFA [9].

Tóm lại, chúng tôi thấy rằng việc sử dụng Trộn & Kết hợp là một heuristic đơn giản, rẻ và hiệu quả để chọn một mô hình con có hiệu suất cao cho một ngân sách tính toán nhất định (Phần 4.1.1 & 4.2). Chúng tôi cung cấp thêm chi tiết và trực giác trong Phụ lục D.1.

3.4 Triển khai
Thiết kế của MatFormer có lợi cho cả khối lượng công việc tĩnh và động:

Khối lượng công việc Tĩnh Để mở rộng ví dụ về các mô hình Llama-2 trong Phần 1, một thiết lập triển khai có thể có ngân sách độ trễ để hỗ trợ mô hình Llama 40B tham số, nhưng chỉ có thể lưu trữ một biến thể 34B vì mô hình lớn hơn tiếp theo (70B) có độ trễ cao hơn đáng kể. Huấn luyện một tham số 40B từ đầu sẽ yêu cầu 4.8∗1023 FLOP, khi huấn luyện một mô hình 34B và 70B đã tốn 4.08∗1023 và 8.4∗1023 FLOP tương ứng. Vì vậy, người ta sẽ cần phải chấp nhận một mô hình ít chính xác hơn mặc dù có ngân sách độ trễ lớn hơn. Với Matformer, người ta có thể có được một mô hình 40B có độ chính xác cao với 0 FLOP huấn luyện bổ sung. Chính xác hơn, đối với khối lượng công việc tĩnh, trong đó tài nguyên tính toán được biết trước và các đầu vào vẫn tương đối tương tự về độ khó, người ta có thể chọn mô hình con tĩnh chính xác nhất cho các ràng buộc sử dụng Trộn & Kết hợp.

--- TRANG 6 ---
Khối lượng công việc Động Đối với khối lượng công việc động, trong đó tài nguyên tính toán hoặc độ khó đầu vào thay đổi ngay lập tức, chúng ta có thể sử dụng mô hình MatFormer phổ quát để trích xuất động mô hình con tối ưu cho mỗi token hoặc truy vấn. Điều này hoạt động đặc biệt tốt cho MatFormer vì tất cả các mô hình con được trích xuất có tính nhất quán hành vi cao với mô hình MatFormer phổ quát (Phần 4.1) – giảm thiểu sự trôi dạt qua các dự đoán từ các mô hình con khác nhau. Chúng tôi đo tính nhất quán giữa hai mô hình tạo sinh là tỷ lệ phần trăm token khớp được tạo ra bởi chúng cho cùng một tiền tố hoặc sử dụng phân kỳ KL của đầu ra mô hình nhỏ hơn với đầu ra mô hình lớn hơn – điều này tính đến các chiến lược lấy mẫu tiềm năng trong giải mã. Tính nhất quán cao này dẫn đến tăng tốc thời gian suy luận vượt trội cho các kỹ thuật như giải mã suy đoán [39] (Phần 4.1.1) và có thể hỗ trợ trong việc giảm trôi dạt dự đoán giữa các triển khai đa nền tảng. Chúng tôi cũng chỉ ra rằng tính nhất quán mô hình cao hơn cũng hỗ trợ bảo tồn cấu trúc không gian metric trong các mô hình mã hóa (Phần 4.2.2). Hơn nữa, với kiến trúc lồng nhau của MatFormer, colocation mô hình có thể hiệu quả bộ nhớ hơn.

4 Thí nghiệm
Trong phần này, chúng tôi đánh giá thực nghiệm MatFormer qua các phương thức (ngôn ngữ trong Phần 4.1 và thị giác trong Phần 4.2) và các lớp mô hình (giải mã và mã hóa). Chúng tôi chứng minh triển khai đàn hồi của các mô hình dựa trên MatFormer (Phần 4.1.1 & 4.2) cho các nhiệm vụ từ đánh giá tạo sinh một lần đến truy xuất hình ảnh thích ứng. Ngoài ra, chúng tôi cũng điều tra hành vi mở rộng đáng tin cậy [29] của các mô hình MatFormer (Phần 4.1.2).

4.1 MatLM: Mô hình Ngôn ngữ MatFormer
Cài đặt Thí nghiệm: Chúng tôi xây dựng Mô hình Ngôn ngữ chỉ giải mã dựa trên MatFormer – MatLM – và so sánh chúng với các đối tác Transformer vanilla tương ứng (LM) [41]. Đối với mỗi mô hình MatLM với dmodel cố định, chúng tôi tối ưu hóa cho g= 4 mức độ chi tiết lồng nhau được biểu thị bởi tỷ lệ FFN của {0.5,1,2,4}– tức là chỉ có kích thước biểu diễn ẩn của khối FFN thay đổi. Chúng tôi ký hiệu các mô hình con này là MatLM – {S, M, L, XL} theo thứ tự tăng dần của kích thước mô hình và gọi MatLM-XL là MatLM phổ quát.

Đối với các đường cơ sở, chúng tôi huấn luyện các mô hình Transformer vanilla với các kiến trúc có thể so sánh. Nghĩa là, đối với mỗi MatLM, chúng tôi huấn luyện 4 mô hình cơ sở riêng biệt với tỷ lệ FFN của {0.5,1,2,4} được ký hiệu là Baseline – {S, M, L, XL}. Ngoài ra, chúng tôi điều chỉnh OFA [9] và DynaBERT [27] cho thiết lập mô hình ngôn ngữ của chúng tôi, và so sánh những cái đó với MatFormer ở cùng kích thước mô hình. Chúng tôi đánh giá những mô hình này trên mất mát xác thực và độ chính xác trung bình trên 25 nhiệm vụ tiếng Anh [8,22,3]. Chúng tôi lưu ý rằng không có bộ nhớ và tính toán bổ sung nào được sử dụng trong quá trình huấn luyện các phương pháp này so với các Baseline được huấn luyện độc lập. Vui lòng xem Phụ lục B để biết thêm chi tiết về huấn luyện, đường cơ sở, đánh giá và các tập dữ liệu.

[Hình 2 được giữ nguyên với các biểu đồ về mất mát xác thực, đánh giá một lần và tính nhất quán]

Hình 2: Điểm mất mát xác thực & đánh giá downstream một lần cho mô hình MatLM 850M & các mô hình cơ sở. Trộn & Kết hợp giúp tạo ra các mô hình chính xác và nhất quán hơn từ MatLM nằm trên đường cong hiệu suất-so với-tính toán được bao phủ bởi các mô hình con được tối ưu hóa rõ ràng.

Kết quả so sánh với đường cơ sở: Để thể hiện hiệu quả của MatFormer so với đường cơ sở, chúng tôi đánh giá mô hình MatLM 850M với các đối tác cơ sở tương ứng trong Hình 2.

Tổng thể, trong Hình 2a và 2b chúng tôi quan sát tất cả các mô hình con mức độ chi tiết của MatLM vượt trội hơn các đối tác cơ sở của chúng. Cụ thể, chúng tôi thấy rằng DynaBERT thể hiện một khoảng cách log perplexity đáng kể 0.01 so với MatFormer trên mô hình 850M. Lý do cơ bản là DynaBERT sử dụng tối ưu hóa chung của tất cả các mức độ chi tiết, dẫn đến ít cập nhật gradient hơn và do đó hiệu suất dưới tối ưu so với MatFormer. DynaBERT sẽ yêu cầu hơn 15% tính toán bổ sung để hoạt động tốt như MatLM. OFA, tương tự như MatFormer, duy trì một mô hình phổ quát duy nhất nhưng

--- TRANG 7 ---
sử dụng lấy mẫu mạng con ngẫu nhiên trong quá trình huấn luyện của nó. Điều này dẫn đến việc lấy mẫu ít mô hình gần với mức độ chi tiết S và XL, dẫn đến hiệu suất kém hơn trong chế độ này. Khoảng cách hiệu suất thể hiện như một đường cong mất mát hình chuông (Hình 2a), làm nổi bật những thiếu sót của OFA trong việc xử lý sự đánh đổi giữa duy trì chất lượng mô hình phổ quát (XL) và tính đàn hồi mô hình. Ngoài ra, việc huấn luyện OFA đòi hỏi chiến lược NAS phức tạp để lựa chọn mô hình con tối ưu. Tuy nhiên, việc sử dụng NAS ở quy mô lớn là tốn kém và có lỗi, mà chúng tôi thảo luận thêm trong Phụ lục D.2. Chúng tôi giới thiệu người đọc đến Phụ lục B.4 để thảo luận chi tiết hơn về hiệu suất MatFormer so với đường cơ sở, và những ưu điểm và nhược điểm của mỗi phương pháp.

4.1.1 Suy luận Đàn hồi với MatLM
Các mô hình con MatLM chính xác cho mọi ràng buộc miễn phí với Trộn & Kết hợp. Trộn & Kết hợp cho phép MatLM cung cấp các mô hình chính xác cho bất kỳ ràng buộc tính toán nào giữa S và XL, ngoài các mức độ chi tiết cố định {S, M, L, XL}. Chúng tôi đánh giá hiệu quả của Trộn & Kết hợp trên mô hình MatLM 850M tham số, so sánh mất mát xác thực và hiệu suất downstream với các mô hình cơ sở được huấn luyện độc lập {S, M, L, XL}. Hình 2a chứng minh rằng Trộn & Kết hợp đạt được các đánh đổi mất mát-so với-tính toán tối ưu mà không có chi phí bổ sung. Ngoài ra, các đánh giá downstream trong Hình 2b củng cố xu hướng này. Trong các tình huống triển khai chỉ có 55% tài nguyên tính toán cho một mô hình MatLM-XL, một mô hình con Trộn & Kết hợp xấp xỉ hiệu suất của XL chỉ với khoảng 1% giảm độ chính xác, so với 2% giảm khi sử dụng mô hình MatLM-M. Điều này làm nổi bật hiệu quả của Trộn & Kết hợp trong việc tạo ra nhiều mô hình tối ưu, như được minh họa bởi các trường hợp được chọn dọc theo các đường cong hiệu suất.

Chúng tôi thử nghiệm với một số heuristic để chọn mạng con tốt nhất, nhưng liên tục quan sát thấy rằng việc dần dần sử dụng các mức độ chi tiết lớn hơn trong các lớp sâu hơn hoạt động tốt nhất (Phần 3.3). Chúng tôi thấy rằng heuristic này tốt hơn các kỹ thuật tìm kiếm dựa trên tiến hóa [48], được sử dụng trong OFA [9] trong Hình 2a & 2b. Chúng tôi cũng thấy rằng việc áp dụng NAS cho MatFormer không mang lại lợi ích so với Trộn & Kết hợp trong Hình 6. Chúng tôi thảo luận thêm chi tiết về Trộn & Kết hợp trong Phụ lục D.1.

Các mô hình con MatLM tăng tốc giải mã suy đoán. Giải mã suy đoán tận dụng một LM nhẹ chính xác như một mô hình nháp để tạo ra một vài token tự hồi quy, tiếp theo là xác minh những bản nháp này với một mô hình lớn hơn thông qua giải mã song song trên các token được tạo ra. Khi bản nháp không chính xác, mô hình nháp được quay lại và đặt lại thành đầu ra của mô hình lớn hơn. Điều này dẫn đến tăng tốc suy luận đáng kể cho cùng độ chính xác như mô hình lớn. Chúng tôi hướng người đọc đến bài báo gốc để giải thích chi tiết hơn [39].

Sự chậm lại của thuật toán này bắt nguồn từ các trường hợp mà dự đoán của mô hình nhỏ hơn không đồng ý với mô hình lớn hơn. Một mô hình nháp nhất quán hơn đáng kể với mô hình xác minh lớn hơn sẽ dẫn đến ít quay lại các dự đoán nháp hơn và do đó độ trễ thấp hơn. Như thấy trong Hình 2c, các mô hình con MatLM có thể nhất quán hơn đến 11.5% so với các đường cơ sở với mô hình XL tương ứng của chúng. Khoảng cách đáng kể vẫn tồn tại ngay cả trong biến thể phân kỳ KL của tính nhất quán với đầu ra của mô hình XL (xem Hình 7 trong Phụ lục). Tính nhất quán được cải thiện này cùng với nhu cầu chỉ một mô hình phổ quát duy nhất đặt MatLM ở vị trí thuận lợi để cải thiện các kỹ thuật yêu cầu mô hình nháp và xác minh như giải mã suy đoán.

Bảng 2: Tăng tốc thời gian suy luận so với một mô hình 850M tiêu chuẩn thông qua giải mã suy đoán sử dụng một mô hình nháp 393M (S) và mô hình xác minh 850M (XL).

Giải mã Suy đoán LAMBADA TriviaQA
Baseline 1.10× 1.08×
MatLM 1.14× 1.11×
+chia sẻ bộ nhớ đệm attention 1.16× 1.14×

Bảng 2 hiển thị tăng tốc thời gian suy luận từ giải mã suy đoán sử dụng các mô hình con S và XL của mô hình ngôn ngữ 850M để soạn nháp và xác minh tương ứng. Giải mã suy đoán với các LM cơ sở được huấn luyện độc lập dẫn đến tăng tốc lên đến 10% so với giải mã tự hồi quy tiêu chuẩn của mô hình 850M-XL. Nhưng giải mã suy đoán dựa trên MatLM nhanh hơn đến 6% so với giải mã suy đoán truyền thống. Tăng tốc bổ sung này có thể được quy cho chủ yếu cho bản chất nhất quán hơn của các mô hình soạn nháp và xác minh dựa trên MatLM và được tăng cường thêm bởi khả năng chia sẻ bộ nhớ đệm attention qua các mô hình từ MatLM mà không khả thi cho các đường cơ sở (xem Phụ lục C.1). Cuối cùng, MatLM giảm thêm chi phí bộ nhớ cho suy luận bằng cách loại bỏ nhu cầu có hai mô hình trong quá trình triển khai hạn chế tài nguyên.

--- TRANG 8 ---
4.1.2 MatLM Mở rộng cũng tốt như Transformer LMs Vanilla

[Hình 3 được giữ nguyên với các biểu đồ về mất mát xác thực và độ chính xác cho các mô hình khác nhau]

Hình 3: Chúng tôi huấn luyện các mô hình MatLM chỉ giải mã khác nhau ở một dải kích thước từ 78M đến 850M tham số và quan sát xu hướng mở rộng của tất cả các mức độ chi tiết (S, M, L, XL) cho mất mát xác thực và điểm đánh giá downstream một lần. Chúng tôi thấy rằng các mô hình MatLM-XL qua các quy mô bắt chước xu hướng huấn luyện của các mô hình Baseline-XL. Thú vị là chúng tôi cũng lưu ý rằng mất mát xác thực và đánh giá downstream tuân theo xu hướng mở rộng của các mô hình XL qua tất cả các mức độ chi tiết.

Bây giờ chúng tôi đã thiết lập rằng một mô hình MatLM 850M và các mô hình con của nó ít nhất là chính xác như các LM Transformer cơ sở, chúng tôi muốn kiểm tra khả năng mở rộng của việc huấn luyện các mô hình MatLM. Vì vậy, chúng tôi nghiên cứu các tính chất mở rộng [29,25] của MatLMs và so sánh chúng với các LM Transformer cơ sở vanilla được huấn luyện cho cùng số lượng token. Chúng tôi huấn luyện các mô hình từ 78M đến 850M tham số trên 10B đến 80B token (trên mỗi mức độ chi tiết) và vẽ mất mát xác thực cho MatLM – {S, M, L, XL} so với các đường cơ sở được huấn luyện độc lập trong Hình 9.

Bảng 3: Các tham số được khớp cho phương trình mở rộng: Loss (N, D ) =a·(ND )b+c

a b c
Baseline 14.08 -0.10 0.89
Matformer 21.60 -0.13 1.33

Đầu tiên, trong Hình 3a, chúng tôi quan sát thấy rằng việc huấn luyện các mô hình MatLM-XL qua các kích thước mô hình mở rộng đáng tin cậy như các LM Baseline-XL cho mất mát so với số lượng tham số. Hình 3b thú vị cho thấy rằng tất cả các mức độ chi tiết {S, M, L, XL}, của MatLM và Baseline tuân theo cùng xu hướng mở rộng. Do đó, chúng tôi khớp một luật mở rộng theo số lượng tham số không nhúng (N) và token huấn luyện (D) cho tất cả các mô hình con có thể cho cả MatLMs và các đường cơ sở trong Bảng 3. Chúng tôi quan sát thấy rằng các tham số được khớp cực kỳ tương tự, gợi ý rằng MatLM mở rộng tương tự như LM Transformer vanilla. Trong Hình 3c chúng tôi cũng thấy rằng các đánh giá downstream cho MatLM tốt hơn 0.3% so với các đường cơ sở, với các mô hình con nhỏ hơn vượt trội hơn các đường cơ sở ở quy mô lên đến 1.4%. Cuối cùng, Hình 9f trong Phụ lục cho thấy rằng các mô hình con MatLM nhất quán hơn với mô hình XL của chúng so với các đối tác cơ sở qua các quy mô.

Chúng tôi lưu ý rằng các luật mở rộng không nắm bắt được cách MatLM đã được tối ưu hóa cho nhiều mô hình con và thậm chí có các mô hình con hiệu suất cao chưa được tối ưu hóa rõ ràng (Phần 4.1.1) Chúng tôi để lại các công thức nắm bắt những tinh tế này cho công việc tương lai và thảo luận thêm điều này trong Phụ lục E.1. Chúng tôi cung cấp kết quả đầy đủ được chia theo mức độ chi tiết trong Phụ lục E.

[Hình 4 được giữ nguyên với biểu đồ về độ chính xác cho các biến thể MatViT]

Hình 4: Các biến thể MatViT khớp hoặc vượt trội hơn các mô hình ViT tiêu chuẩn trên phân loại ImageNet-1K và cung cấp các mô hình được trích xuất miễn phí bao quát đường cong độ chính xác-tính toán thông qua Trộn & Kết hợp.

--- TRANG 9 ---
[Hình 5 được giữ nguyên với biểu đồ về độ chính xác 1-NN]

Hình 5: MatViT bản địa cho phép các bộ mã hóa đàn hồi cho truy xuất thích ứng có thể được sử dụng cho tính toán phía truy vấn thời gian thực trong khi duy trì độ chính xác mạnh trên ImageNet-1K, không giống như các đường cơ sở.

4.2 MatViT: Vision Transformers MatFormer
Chúng tôi mở rộng MatFormer cho các mô hình mã hóa thị giác máy tính dựa trên Vision Transformer (ViT) [21]. MatFormer-based ViT (MatViT) cho phép suy luận đàn hồi cho các nhiệm vụ cơ bản như phân loại hình ảnh và truy xuất. Chúng tôi huấn luyện biến thể MatFormer của các mô hình ViT-B/16 và ViT-L/16 tiêu chuẩn – MatViT-B/16 và MatViT-L/16 được huấn luyện với g= 4 mức độ chi tiết lồng nhau (tỷ lệ FFN của {0.5,1,2,4}). Các mô hình B/16 được huấn luyện trên ImageNet-1K [50] với AugReg [57] trong khi các mô hình L/16 được tiền huấn luyện trên ImageNet-21K [18] tiếp theo là tinh chỉnh trên ImageNet-1K. Tất cả các mô hình sử dụng thiết lập huấn luyện và siêu tham số tối ưu của các biến thể ViT tiêu chuẩn từ thư viện Scenic [16].

4.2.1 Phân loại Hình ảnh
Đối với phân loại hình ảnh, chúng tôi đánh giá cả mô hình ViT & MatViT trên ImageNet-1K. Hình 4a cho thấy rằng các mức độ chi tiết được tối ưu hóa rõ ràng trong MatViT dẫn đến các mô hình chính xác như các đường cơ sở được huấn luyện độc lập cho B/16. Tuy nhiên đối với L/16, như được hiển thị trong Hình 4b, chúng ta thấy rằng các mô hình MatViT chính xác hơn đến 0.35% so với đường cơ sở cho cùng chi phí suy luận.

Sau đó chúng tôi khám phá việc sử dụng MatFormer ở các giai đoạn huấn luyện khác nhau với một lưới 2×2 của các cặp tiền huấn luyện-tinh chỉnh (Bảng 7 trong Phụ lục G.1) và thấy rằng việc sử dụng MatFormer trong quá trình tiền huấn luyện giúp mang lại các bộ mã hóa chính xác và linh hoạt hơn cho việc sử dụng downstream. Hơn nữa, tinh chỉnh sử dụng MatFormer tăng cường triển khai đàn hồi tùy thuộc vào các ràng buộc hiện tại thông qua Trộn & Kết hợp.

Bộ mã hóa Thích ứng với Trộn & Kết hợp. Hơn nữa, độ chính xác của các mô hình Trộn & Kết hợp của chúng tôi gần như nằm trên đường nối độ chính xác của các mức độ chi tiết được huấn luyện rõ ràng. Trong các tình huống mà, chẳng hạn, một ứng dụng có thể lưu trữ mô hình B/16 50M tham số, MatViT có thể cung cấp mô hình chính xác hơn 0.8% so với cách tiếp cận hiện tại sẽ lưu trữ mô hình cơ sở lớn nhất với ≤50M tham số. Trong quá trình triển khai, mô hình MatViT phổ quát có thể được lưu trữ trong bộ nhớ và tùy thuộc vào các ràng buộc tính toán được sử dụng để trích xuất một mô hình nhỏ hơn thích ứng để tối đa hóa độ chính xác với các tài nguyên có sẵn tại thời điểm đó. Hiện tại, chúng tôi tìm các mô hình Trộn & Kết hợp trên đường cong độ chính xác-tính toán thông qua suy luận nhanh trên tập xác thực. Mặc dù tương đối có thể mở rộng, điều này chỉ ra nhu cầu phân bổ ngân sách tối ưu qua các lớp trong mạng nơ-ron [33].

4.2.2 Truy xuất Hình ảnh Thích ứng
Mục tiêu của truy xuất hình ảnh là tìm những hình ảnh tương tự về mặt ngữ nghĩa – ví dụ hình ảnh từ cùng một lớp – sử dụng các biểu diễn thu được từ một bộ mã hóa được tiền huấn luyện [13]. Cách tiếp cận tiêu chuẩn là mã hóa các hình ảnh cơ sở dữ liệu cũng như hình ảnh truy vấn với cùng bộ mã hóa và chạy truy xuất láng giềng gần nhất cho embedding truy vấn. Trong khi chúng ta có thể nhúng hình ảnh cơ sở dữ liệu với một bộ mã hóa đắt tiền, bộ mã hóa truy vấn nói chung phải là thời gian thực. Hơn nữa, cài đặt mã hóa truy vấn có thể đa dạng, ví dụ xử lý trên thiết bị so với cloud, tải truy vấn biến đổi và độ phức tạp truy vấn. Các giải pháp hiện tại có một bộ mã hóa cố định do đó thỏa hiệp về độ chính xác hoặc chi phí cho các cài đặt khác nhau.

Với bản chất đàn hồi của MatViT, nó là một ứng viên tốt cho bộ mã hóa truy vấn. Tuy nhiên, truy xuất cũng yêu cầu rằng các mô hình con bảo tồn khoảng cách giữa cơ sở dữ liệu cố định (với bộ mã hóa lớn) và embedding truy vấn qua tất cả các mức độ chi tiết. Nếu chúng ta chỉ sử dụng các mô hình ViT cơ sở nhỏ hơn cho mã hóa truy vấn, những khoảng cách này không được bảo tồn và dẫn đến gần như 0 độ chính xác truy xuất (xem Hình 5).

--- TRANG 10 ---
Chúng tôi đánh giá cả bộ mã hóa ViT và MatViT trên ImageNet-1K cho truy xuất hình ảnh. Chúng tôi tính độ chính xác 1-láng giềng gần nhất (NN) sử dụng vector biểu diễn của token [CLS] (cũng xem Phụ lục G.2). Hình 5 cho thấy rằng các mô hình con được trích xuất từ MatViT có thể bảo tồn gần đúng khoảng cách và cung cấp tính linh hoạt đáng kể hơn. Ví dụ, với mất mát <0.5% độ chính xác, MatViT-L/16 có thể giảm chi phí tính toán 40%. Điều này tương ứng với mô hình Trộn & Kết hợp 175M tham số trong Hình 5b, nhỏ hơn 40% so với mô hình XL 300M, và có <0.5% giảm độ chính xác. Theo hiểu biết của chúng tôi, đây là kết quả đầu tiên thuộc loại này và mở ra một loạt rộng các chiến lược suy luận thích ứng cho tìm kiếm ngữ nghĩa quy mô lớn.

5 Kết luận
Trong công trình này chúng tôi đã trình bày MatFormer, một kiến trúc Transformer đàn hồi bản địa cho phép huấn luyện một mô hình phổ quát duy nhất có thể được sử dụng để trích xuất hàng trăm mô hình con nhỏ hơn chính xác với chi phí bổ sung bằng không tại thời điểm triển khai. Chúng tôi thấy rằng Mô hình Ngôn ngữ MatFormer (MatLM) khớp perplexity & độ chính xác một lần của các mô hình được huấn luyện độc lập. Thực tế, MatLM chứng minh một đường cong mở rộng mất mát-so với-tính toán thú vị gần như độc lập với mức độ chi tiết được huấn luyện chỉ ra khái quát hóa mạnh mẽ cho các mô hình cực lớn. Cuối cùng, các mô hình con MatFormer cho phép tăng tốc suy luận đa dạng như tạo tự hồi quy nhanh hơn với giải mã suy đoán và bộ mã hóa truy vấn đàn hồi cho truy xuất dày đặc thích ứng qua các phương thức. Chúng tôi tin rằng việc định tuyến động những mô hình này để thay đổi độ trễ suy luận [32,40,20], và phát triển các tối ưu hóa phần cứng cần thiết là một lĩnh vực hứa hẹn cho công việc tương lai.

Lời cảm ơn
Chúng tôi biết ơn Aishwarya P S, Yashas B.L. Samaga, Varun Yerram, Lovish Madaan, Anurag Arnab vì hỗ trợ thiết lập các quy trình huấn luyện, Matthew Wallingford, Praneeth Netrapalli, Orhan Firat, Rohan Anil, Tom Duerig, Luke Zettlemoyer, Manish Gupta, Rahul Sukthankar và Jeff Dean vì những thảo luận hữu ích, hỗ trợ và phản hồi.

Chúng tôi cũng ghi nhận tài nguyên tính toán và hỗ trợ từ HYAK tại University of Washington, FAS RC tại Harvard University, Kempner Institute và một giải thưởng tín dụng GCP cho khám phá giai đoạn đầu của dự án này. Ali Farhadi ghi nhận tài trợ từ các giải thưởng NSF IIS 1652052, IIS 1703166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, và quà tặng từ Allen Institute for Artificial Intelligence và Google. Sham Kakade ghi nhận tài trợ từ Office of Naval Research theo giải thưởng N00014-22-1-2377. Công việc này đã được thực hiện một phần nhờ món quà từ Chan Zuckerberg Initiative Foundation để thành lập Kempner Institute for the Study of Natural and Artificial Intelligence. Yulia Tsvetkov ghi nhận hỗ trợ từ National Science Foundation theo CAREER Grant No. IIS2142739, các khoản tài trợ NSF No. IIS2125201 và IIS2203097, và tài trợ quà tặng từ Google, MSR, và OpenAI. Hannaneh Hajishirzi ghi nhận tài trợ thông qua một món quà từ Allen Institute for Artificial Intelligence.

Tài liệu tham khảo
[1] O. Ahia, J. Kreutzer, và S. Hooker. The low-resource double bind: An empirical study of pruning for low-resource machine translation. arXiv preprint arXiv:2110.03036, 2021.

[2] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.

[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

[4] J. Berant, A. K. Chou, R. Frostig, và P. Liang. Semantic parsing on freebase from question-answer pairs. In Conference on Empirical Methods in Natural Language Processing, 2013. URL https://api.semanticscholar.org/CorpusID:6401679.

[5] L. Beyer, P. Izmailov, A. Kolesnikov, M. Caron, S. Kornblith, X. Zhai, M. Minderer, M. Tschannen, I. Alabdulmohsin, và F. Pavetic. Flexivit: One model for all patch sizes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14496–14506, 2023.

--- TRANG 11 ---
[6] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, và Y. Choi. Piqa: Reasoning about physical commonsense in natural language, 2019.

[7] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[9] H. Cai, C. Gan, T. Wang, Z. Zhang, và S. Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.

[10] H. Cai, C. Gan, J. Lin, và S. Han. Network augmentation for tiny deep learning. arXiv preprint arXiv:2110.08890, 2021.

[11] A. Chavan, Z. Shen, Z. Liu, Z. Liu, K.-T. Cheng, và E. P. Xing. Vision transformer slimming: Multi-dimension searching in continuous optimization space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4931–4941, 2022.

[12] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, và J. Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

[13] W. Chen, Y. Liu, W. Wang, E. M. Bakker, T. Georgiou, P. Fieguth, L. Liu, và M. S. Lew. Deep learning for instance retrieval: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, et al. Palm: Scaling language modeling with pathways, 2022.

[15] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, và O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.

[16] M. Dehghani, A. Gritsenko, A. Arnab, M. Minderer, và Y. Tay. Scenic: A jax library for computer vision research and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21393–21398, 2022.

[17] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480–7512. PMLR, 2023.

[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.

[19] T. Dettmers và L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pages 7750–7774. PMLR, 2023.

[20] D. Ding, A. Mallick, C. Wang, R. Sim, S. Mukherjee, V. Ruhle, L. V. Lakshmanan, và A. H. Awadallah. Hybrid llm: Cost-efficient and quality-aware query routing. arXiv preprint arXiv:2404.14618, 2024.

[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[22] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, và C. Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.

--- TRANG 12 ---
[23] M. Grimaldi, L. Mocerino, A. Cipolletta, và A. Calimera. Dynamic convnets on tiny devices via nested sparsity. IEEE Internet of Things Journal, 10(6):5073–5082, 2022.

[24] D. Hendrycks và K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

[25] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

[26] S. Hooker, N. Moorosi, G. Clark, S. Bengio, và E. Denton. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058, 2020.

[27] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, và Q. Liu. Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33:9782–9793, 2020.

[28] M. Joshi, E. Choi, D. Weld, và L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.

[29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, và D. Amodei. Scaling laws for neural language models. 2020.

[30] S. Kim, C. Hooper, T. Wattanawong, M. Kang, R. Yan, H. Genc, G. Dinh, Q. Huang, K. Keutzer, M. W. Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023.

[31] T. Kudo và J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.

[32] S. Kudugunta, Y. Huang, A. Bapna, M. Krikun, D. Lepikhin, M.-T. Luong, và O. Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. arXiv preprint arXiv:2110.03742, 2021.

[33] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, và A. Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544–5555. PMLR, 2020.

[34] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:30233–30249, 2022.

[35] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, và S. Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026.

[36] F. Lagunas, E. Charlaix, V. Sanh, và A. M. Rush. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838, 2021.

[37] G. Lai, Q. Xie, H. Liu, Y. Yang, và E. Hovy. Race: Large-scale reading comprehension dataset from examinations, 2017.

[38] H. J. Levesque, E. Davis, và L. Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12, page 552–561. AAAI Press, 2012. ISBN 9781577355601.

[39] Y. Leviathan, M. Kalman, và Y. Matias. Fast inference from transformers via speculative decoding. 2023.

--- TRANG 13 ---
[40] M. Li, S. Gururangan, T. Dettmers, M. Lewis, T. Althoff, N. A. Smith, và L. Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306, 2022.

[41] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, và N. Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.

[42] T. Mihaylov, P. Clark, T. Khot, và A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.

[43] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, và J. Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839–849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.

[44] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, và D. Kiela. Adversarial nli: A new benchmark for natural language understanding, 2020.

[45] R. OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.

[46] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, và R. Fernández. The lambada dataset: Word prediction requiring a broad discourse context, 2016.

[47] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, và I. Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492–28518. PMLR, 2023.

[48] E. Real, A. Aggarwal, Y. Huang, và Q. V. Le. Regularized evolution for image classifier architecture search, 2019.

[49] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.

[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015.

[51] K. Sakaguchi, R. L. Bras, C. Bhagavatula, và Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.

[52] M. Salehi, S. Mehta, A. Kusupati, A. Farhadi, và H. Hajishirzi. Sharcs: Efficient transformers through routing with dynamic width sub-networks. Findings of Empirical Methods in Natural Language Processing, 2023.

[53] V. Sanh, L. Debut, J. Chaumond, và T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[54] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran, Y. Tay, và D. Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456–17472, 2022.

[55] N. Shazeer và M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596–4604. PMLR, 2018.

[56] D. R. So, W. Mańke, H. Liu, Z. Dai, N. Shazeer, và Q. V. Le. Primer: Searching for efficient transformers for language modeling. arXiv preprint arXiv:2109.08668, 2021.

[57] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, và L. Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.

--- TRANG 14 ---
[58] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[59] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[60] M. Valipour, M. Rezagholizadeh, H. Rajabzadeh, M. Tahaei, B. Chen, và A. Ghodsi. Sortednet, a place for every network and every network in its place: Towards a generalized solution for training many-in-one neural networks. arXiv preprint arXiv:2309.00255, 2023.

[61] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin. Attention is all you need. 2023.

[62] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, và S. R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2020.

[63] H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, và S. Han. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187, 2020.

[64] J. Yu và T. Huang. Universally slimmable networks and improved training techniques, 2019. URL https://arxiv.org/abs/1903.05134.

[65] J. Yu và T. S. Huang. Universally slimmable networks and improved training techniques. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1803–1811, 2019.

[66] J. Yu, L. Yang, N. Xu, J. Yang, và T. Huang. Slimmable neural networks. arXiv preprint arXiv:1812.08928, 2018.

[67] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, và Y. Choi. Hellaswag: Can a machine really finish your sentence?, 2019.

[68] B. Zoph và Q. V. Le. Neural architecture search with reinforcement learning, 2017.

--- TRANG 15 ---
A Tuyên bố Tác động Rộng hơn
Tính đàn hồi của MatFormer cho phép việc sử dụng nó trong các tình huống triển khai đa dạng. Điều này giảm rào cản cho các nhà thực hành sử dụng các mô hình nền tảng được điều chỉnh theo các tình huống triển khai của họ. Huấn luyện các mô hình nền tảng vẫn đắt đỏ, với các mô hình lớn nhất mà chúng tôi thảo luận được huấn luyện trên 256 lõi TPU-v4 trong 3 ngày. Hơn nữa, mặc dù chúng tôi báo cáo điểm mất mát xác thực và điểm đánh giá downstream trên nhiều nhiệm vụ khác nhau, chúng tôi thừa nhận khả năng MatFormer có thể có tác động bất lợi đến bias [26] hoặc các miền/ngôn ngữ thiểu số [1].

B Chi tiết Triển khai
B.1 Kiến trúc và Huấn luyện
Đối với các thí nghiệm của chúng tôi, chúng tôi huấn luyện một dải MatLM khác nhau từ kích thước 78M đến 850M cho 10B-80B token – chúng tôi mở rộng kích thước mô hình đều với số lượng token huấn luyện [25]. Đối với mỗi mức độ chi tiết MatLM, chúng tôi cũng huấn luyện một mô hình Transformer vanilla cơ sở tương ứng. Nghĩa là, đối với mỗi kích thước mô hình chúng tôi huấn luyện Baseline-XL, L, M, S với dff= 4∗dmodel,2∗dmodel, dmodel, dmodel/2. Tất cả các mô hình có 16 lớp, 16 đầu attention, và tỷ lệ dmodel:dff là 1:4. Chúng tôi huấn luyện từ vựng 256k sử dụng thư viện SentencePiece [31], sử dụng độ dài ngữ cảnh tối đa 1024 token, và kích thước lô 1M token. Chúng tôi tiền huấn luyện các mô hình 850M trên 256 chip v3 TPU. Chúng tôi cung cấp thêm chi tiết về những mô hình này trong Bảng 4. Để biết thêm chi tiết về thiết lập huấn luyện, chúng tôi hướng người đọc đến [58].

Bảng 4: Chi tiết mô hình cho các quy mô mô hình được sử dụng để tiến hành các thí nghiệm được mô tả trong Phần 4.1, với sự phân chia số lượng tham số tổng, số lượng tham số không nhúng và số lượng tham số FFN cho mỗi mức độ chi tiết mô hình.

Số lượng Tham số (đầy đủ / cắt) Tham số Không Nhúng (đầy đủ / cắt) Tham số FFN (đầy đủ) dmodel N(token)
78M (74M / 72M / 71M) 12.6M (8.4M/6.3M/ 5.3M) 8.4M 256 10B
180M (164M / 157M / 152M) 50M (33.7M/25.3M/21.1M) 33.6M 512 20B
310M (272M / 253M / 244M) 113M (75M/56M/47M) 75.6M 768 30B
463M (397M / 363M / 346M) 201M (134M/100M/84M) 134M 1024 40B
850M (696M / 620M / 582M) 453M (302M/227M/189M) 302M 1536 80B

B.2 Đánh giá Downstream
Chúng tôi đánh giá tất cả các mô hình LM được huấn luyện trên tập hợp 25 nhiệm vụ tiếng Anh tương tự như [8,22,14,3], bao gồm:

1. Nhiệm vụ Trả lời Câu hỏi Sách Đóng Miền Mở: TriviaQA [28], Natural Questions [35], và WebQuestions [4].
2. Nhiệm vụ Cloze và hoàn thành: LAMBADA [46], HellaSwag [67], và StoryCloze [43].
3. Nhiệm vụ kiểu Winograd: Winograd [38] và WinoGrande [51].
4. Đọc hiểu: RACE [37].
5. Lý luận thường thức: PIQA [6], ARC [15], và OpenBookQA [42].
6. SuperGLUE [62]
7. Suy luận ngôn ngữ tự nhiên: Adversarial NLI [44].

Đối với tất cả các mức độ chi tiết tương ứng với mỗi mô hình, chúng tôi trình bày số đánh giá cùng với mất mát log perplexity tập phát triển trên tất cả 25 nhiệm vụ trong Bảng 9 đến 13.

B.3 Chi tiết Triển khai Baseline
Baseline được huấn luyện độc lập: Các baseline được huấn luyện từ đầu độc lập, trong đó mỗi mức độ chi tiết trong một kích thước mô hình nhất định sử dụng X token được đề cập trong Bảng 4. Ví dụ, đối với mô hình 850M X = 80B token. Do đó, Baseline-{S, M, L, XL} xử lý tổng cộng 4X token. Các baseline được huấn luyện từ đầu dẫn đến cùng số lượng mô hình được huấn luyện rõ ràng. Nghĩa là, không có cách nào để trực tiếp có được một mô hình con có kích thước giữa các mức độ chi tiết được huấn luyện mà không sử dụng các phương pháp bổ sung như chưng cất, cắt tỉa, v.v.

--- TRANG 16 ---
OFA: OFA [9] huấn luyện một mô hình lớn từ đầu, sau đó đóng băng nó và bắt đầu một lần chạy khác của một "mô hình phổ quát" có kích thước tương tự nơi nó lấy mẫu các mạng con ngẫu nhiên và tối ưu hóa chúng với chưng cất và mất mát sự thật cơ bản. Ý tưởng trung tâm của công trình này là lấy mẫu các mô hình con ngẫu nhiên và tối ưu hóa chúng. Chưng cất là một thành phần trực giao cũng có thể được thêm vào MatFormer. Để sử dụng tính toán và bộ nhớ có thể so sánh với Baseline (Phần 4.1), chúng tôi sửa đổi phương pháp để chỉ tối ưu hóa w.r.t. mất mát sự thật cơ bản - như cũng được thực hiện trong trường hợp của MatFormer. Tổng thể, OFA sử dụng 4X token, giống như tất cả các mức độ chi tiết baseline được huấn luyện từ đầu. Trong mỗi bước, OFA lấy mẫu một mô hình ngẫu nhiên, trong đó mỗi lớp của nó có mức độ chi tiết {S, M, L, XL} được lấy mẫu ngẫu nhiên, và tối ưu hóa nó.

Để có được các mô hình con theo một ràng buộc chi phí nhất định, OFA đề xuất sử dụng NAS [48] để tìm kiếm kiến trúc tối ưu. Cụ thể, OFA lấy mẫu một tập hợp ngẫu nhiên các cặp (kiến trúc, mất mát) từ mô hình phổ quát. Sau đó, nó huấn luyện một bộ dự đoán trên cặp này, lấy kiến trúc làm đầu vào và dự đoán mất mát. Cuối cùng, nó sử dụng một thói quen tìm kiếm dựa trên tiến hóa [48] để tìm kiếm kiến trúc tối ưu trong một ràng buộc cụ thể, sử dụng bộ dự đoán trong thói quen tìm kiếm của nó. Thêm chi tiết về NAS và so sánh với MatFormer Trộn & Kết hợp được trình bày trong Phần D.

DynaBERT: DynaBERT [27] có g mức độ chi tiết cố định của các mô hình con - tương tự như MatFormer. Tương tự như OFA, DynaBERT cũng sử dụng mất mát chưng cất. Để phân tích có thể so sánh và duy trì tính toán/bộ nhớ có thể so sánh với baseline, chúng tôi chỉ tối ưu hóa DynaBERT wrt mất mát sự thật cơ bản, tương tự như MatFormer.

Sự khác biệt quan trọng giữa DynaBERT và MatFormer nằm ở phương pháp huấn luyện của nó. Trong mỗi bước, DynaBERT lấy 4 lô dữ liệu, tối ưu hóa mỗi lô với mỗi mức độ chi tiết {S, M, L, XL} và trung bình những mất mát này. Tương tự như Matformer và OFA, nó xử lý tổng cộng 4X token. Mặc dù, vì nó tối ưu hóa trên 4 lô dữ liệu trong một bước duy nhất, nó chạy trong một phần tư số bước của MatFormer hoặc OFA, và cùng số bước như baseline.

Một sự khác biệt quan trọng giữa đóng góp của công trình DynaBERT và MatFormer là chiến lược tìm kiếm. Trong MatFormer, chúng tôi đã giới thiệu heuristic Trộn & Kết hợp, tính toán một mô hình con tối ưu với một ràng buộc chi phí mà không có bất kỳ chi phí nào. Ngược lại, DynaBERT không giới thiệu bất kỳ chiến lược tìm kiếm nào và chỉ thảo luận về việc sử dụng các mức độ chi tiết được huấn luyện rõ ràng làm mô hình con.

B.4 So sánh với Baseline
Ở đây chúng tôi thực hiện một cuộc thảo luận chi tiết hơn về so sánh giữa MatFormer và các baseline được sử dụng trong Phần 4.

1. MatFormer vs Baseline được huấn luyện từ đầu: Để Baseline-{S, M, L, XL} kết hợp xử lý 4X token, trong đó mỗi mức độ chi tiết được huấn luyện trên X token độc lập. Vì MatFormer chỉ duy trì một mô hình, chúng tôi huấn luyện nó cho 4X token, trong mỗi bước lấy mẫu một trong MatLM-{S, M, L, XL}. Do đó, tổng tính toán và bộ nhớ đỉnh vẫn giống như Baseline vì chúng tôi hiệu quả huấn luyện MatLM-{S, M, L, XL} cho X token mỗi cái – khớp với cấu hình huấn luyện Baseline. Nhưng vì chúng tôi duy trì một mô hình duy nhất, các tham số được chia sẻ được huấn luyện lên đến 4X token miễn phí. Ví dụ, mỗi khi chúng tôi huấn luyện mức độ chi tiết XL/L/M, vì mức độ chi tiết S được chứa trong chúng, nó cũng nhận được cập nhật gradient. Do đó, MatLM-S hiệu quả xử lý 4X token, MatLM-M xử lý 3X token, MatLM-L xử lý 2X token, và MatLM-XL xử lý X token. Chúng ta cũng có thể thấy điều này chuyển đổi thành Mất mát Xác thực (Hình 2a) – các mức độ chi tiết nhỏ hơn vượt trội hơn Baseline với một biên độ lớn. Trong khi mức độ chi tiết lớn hơn khớp về hiệu suất.

2. MatFormer vs DynaBERT: DynaBERT, tương tự như MatFormer, huấn luyện g mạng con lồng nhau, nhưng khác biệt quan trọng trong phương pháp huấn luyện của nó. Nó thực hiện tối ưu hóa chung - xử lý đồng thời 4 lô dữ liệu - 1 lô thông qua mỗi mức độ chi tiết {S, M, L, XL}. Mất mát cuối cùng được tối ưu hóa là trung bình của mất mát của mỗi mức độ chi tiết. Ngược lại, MatFormer lấy mẫu và tối ưu hóa các mức độ chi tiết riêng lẻ trong các bước riêng biệt, dẫn đến gấp bốn lần cập nhật gradient cho cùng một lượng dữ liệu và tính toán. Sự khác biệt trong phương pháp huấn luyện này chuyển đổi thành một khoảng cách hiệu suất đáng kể: DynaBERT thể hiện một khoảng cách log perplexity đáng kể 0.01 so với MatFormer trên mô hình 850M. Ngay cả với 15% tăng tính toán, DynaBERT vẫn thua kém hiệu suất của MatFormer. Hơn nữa, không giống như MatFormer, cung cấp tính linh hoạt trong việc lựa chọn kích thước mô hình con thông qua phương pháp Trộn & Kết hợp của nó, DynaBERT thiếu một chiến lược lấy mẫu chuyên dụng.

--- TRANG 17 ---
[Hình 6 được giữ nguyên]

Hình 6: Chúng tôi tìm kiếm cấu hình kiến trúc tối ưu sử dụng tìm kiếm tiến hóa [48] và với Trộn & Kết hợp. Chúng tôi thấy rằng Trộn & Kết hợp mang lại các kiến trúc nằm trên đường cong pareto-tối ưu, và ít nhất là tốt như những cái được tìm thấy bằng cách sử dụng tìm kiếm tiến hóa.

3. MatFormer vs OFA: OFA duy trì một mô hình phổ quát duy nhất tương tự như MatFormer, nhưng lấy mẫu ngẫu nhiên các mạng con sao cho mỗi lớp là một trong {S, M, L, XL}. Tương tự như MatFormer và DynaBERT, nó tận dụng việc xử lý dữ liệu 4X bởi một mô hình duy nhất. Nhưng do lấy mẫu, không đủ mô hình nhỏ hoặc lớn (gần với mức độ chi tiết mô hình S và XL) được lấy mẫu, dẫn đến hiệu suất kém hơn trong những vùng này. Nghĩa là, lấy mẫu các mô hình từ kích thước mô hình S đến kích thước mô hình XL, hầu hết các mô hình được lấy mẫu gần với kích thước mô hình M-L. Điều này cũng có thể được thấy trong đường cong Mất mát, nơi chúng ta thấy một đường cong giống như chuông có mất mát cao hơn nhiều gần với kích thước mô hình S và XL, và khớp với hiệu suất của MatLM ở giữa. Một nhược điểm khác của OFA là vì các mô hình ngẫu nhiên được lấy mẫu, người ta không thể sử dụng kỹ thuật Trộn & Kết hợp đơn giản như trong matFormer. Thay vào đó, một chiến lược NAS phức tạp hơn là cần thiết để tìm các mô hình con tối ưu. Để huấn luyện một NAS khi tìm kiếm các mô hình lớn như vậy là tốn kém và có lỗi, như chúng tôi thảo luận trong Phụ lục D.2.

C Chi phí Huấn luyện và Suy luận
Chúng tôi hiện tại thực hiện những thay đổi và tối ưu hóa tối thiểu đối với các script huấn luyện của kiến trúc Transformer vanilla. Nói cách khác, chúng tôi sử dụng cùng một triển khai cho cả Baselime và MatFormer, ngoại trừ việc sử dụng các lát cắt có kích thước khác nhau của khối FFN cho mỗi lượt truyền tiến. Thời gian tường cho huấn luyện MatLM cùng chi phí như huấn luyện tất cả 4 đối tác cơ sở mức độ chi tiết. Trong quá trình phục vụ, chúng tôi quan sát thấy tỷ lệ độ trễ FFN mô hình 850M so với độ trễ attention = 56:44. Chúng tôi lưu ý rằng tỷ lệ độ trễ FFN:MHA này phụ thuộc rất nhiều vào quy mô và độ dài chuỗi. Cụ thể hơn, đối với một độ dài chuỗi nhất định, độ trễ FFN chi phối độ trễ tổng thể ở quy mô, trong khi chi phí của các đầu attention tăng với độ dài chuỗi. Chúng tôi giới thiệu người đọc đến Kim et al. [30] để minh họa rộng rãi hơn về điều này. Chúng tôi nhấn mạnh rằng mặc dù chúng tôi đã huấn luyện một MatFormer và so sánh thời gian huấn luyện của nó với Baseline kết hợp, chúng tôi có thể trích xuất nhiều mô hình hơn so với 4 mức độ chi tiết mô hình chúng tôi đã huấn luyện rõ ràng.

C.1 Chia sẻ Attention Giải mã Suy đoán
Một lợi ích bổ sung của MatLM là bộ nhớ đệm attention được chia sẻ giữa mô hình nháp và mô hình xác minh. Khi mô hình XL xác minh bản nháp của mô hình S, nó ghi đè bộ nhớ đệm attention với biểu diễn tiềm ẩn phong phú hơn của nó so với cái được tạo ra bởi mô hình soạn nháp. Lưu ý rằng 1) điều này không liên quan đến tính toán bổ sung vì MatLM có một mô hình phổ quát duy nhất bao gồm cả mô hình nháp và mô hình xác minh; 2) chia sẻ attention không khả thi trong Baseline vì chúng không được huấn luyện rõ ràng cùng nhau. Do đó, biểu diễn tiềm ẩn của một mô hình khá vô nghĩa đối với mô hình khác. Vì vậy, chia sẻ attention mang lại cải thiện thêm so với giải mã suy đoán vanilla như được hiển thị trong Bảng 2.

--- TRANG 18 ---
Bảng 5: Trên mô hình MatFormer 850M, trong khi chạy NAS chúng tôi quan sát thấy nó ưa thích các mức độ chi tiết cân bằng qua các lớp hơn là lệch. Trên một vài ràng buộc tham số chúng tôi liệt kê cấu hình heuristic MnM, và cấu hình được dự đoán với NAS.

Ngân sách Tham số FFN Cấu hình Trộn & Kết hợp & mất mát Cấu hình được dự đoán NAS & mất mát
226M [M,M,M,M,M,M,M,M,M,M,M,M,M,M,M,M]; 2.931 [S,M,M,M,M,M,M,S,M,M,M,M,M,M,M,L]; 2.944
245M [M,M,M,M,M,M,M,M,M,M,M,M,L,L,L,L]; 2.926 [M,M,M,M,M,L,M,M,M,L,M,L,M,M,M,L]; 2.932
278M [M,M,M,M,M,L,L,L,L,L,L,L,L,L,L,L]; 2.9 [M,L,L,L,L,L,L,L,L,L,M,L,M,M,M,L]; 2.91
377M [L,L,L,L,XL,XL,XL,XL,XL,XL,XL,XL,XL,XL,XL,XL]; 2.865 [L,L,XL,L,XL,L,XL,L,XL,XL,XL,L,XL,L,L,XL]; 2.874

D Kỹ thuật Tìm kiếm
D.1 Trộn & Kết hợp
Đối với một ngân sách tính toán hoặc tham số nhất định, nhiều mô hình con có thể đáp ứng ràng buộc. Một chiến lược phổ biến có thể liên quan đến việc sử dụng Tìm kiếm Kiến trúc Thần kinh (NAS) để xác định kiến trúc tối ưu trong tập hợp con này; tuy nhiên, cách tiếp cận này có thể tốn kém một cách cấm đoán. Thay vào đó, chúng tôi đề xuất một heuristic Trộn & Kết hợp đơn giản hơn, xác định các mô hình con tối ưu nằm trên đường cong pareto-tối ưu độ chính xác-so với-tham số.

Heuristic Trộn & Kết hợp khuyến nghị chọn các mức độ chi tiết lớp với những thay đổi tối thiểu qua các lớp, đảm bảo rằng mức độ chi tiết của lớp thứ j ít nhất bằng của lớp thứ i cho j > i. Điều này có nghĩa là một sự tăng dần mức độ chi tiết với "độ dốc nhỏ nhất" thực nghiệm mang lại hiệu suất tốt nhất trong số tất cả các cấu hình trộn-và-kết hợp được kiểm tra. Heuristic của chúng tôi được hỗ trợ bởi phương pháp huấn luyện, trong đó mỗi cấu hình mức độ chi tiết được lấy mẫu duy trì mức độ chi tiết lớp nhất quán qua mô hình. Do đó, mô hình thích ứng tốt nhất với các cấu hình trong đó các mức độ chi tiết lớp hoặc đồng nhất hoặc hiển thị sự biến đổi tối thiểu. Ví dụ, một cấu hình [M, M, M, M, L, L, L] qua các lớp hiệu quả hơn [S, S, S, S, S, XL, XL] mặc dù có số lượng tham số tương tự, vì nó duy trì một phân phối mức độ chi tiết cân bằng hơn so với một phân phối lệch.

Trực giác này cũng được hỗ trợ bởi kết quả từ một phương pháp Tìm kiếm Kiến trúc Thần kinh (NAS) dựa trên tìm kiếm tiến hóa [48], tương tự như được sử dụng trong [9]. Các thí nghiệm NAS của chúng tôi, một vài lần chạy chúng tôi trình bày trong Bảng 5, chỉ ra một sự ưa thích cho các cấu hình cân bằng dưới các ràng buộc khác nhau, phù hợp với những phát hiện từ heuristic Trộn & Kết hợp của chúng tôi.

Chúng tôi đã khám phá các cấu hình cân bằng khác nhau, bao gồm:
1. Tăng-Giảm: Mức độ chi tiết tăng cho đến điểm giữa và sau đó giảm, ví dụ [M, M, L, L, L, M, M].
2. Giảm-Tăng: Ngược lại của heuristic trước, trong đó mức độ chi tiết giảm sau đó tăng.
3. Tăng: Một chuỗi không giảm của các mức độ chi tiết lớp, chẳng hạn như [M, M, M, M, L, L, L].
4. Giảm: Ngược lại của heuristic Tăng.

Trong số những cấu hình này, heuristic Tăng liên tục vượt trội hơn những cái khác. Do đó, chúng tôi khuyến nghị áp dụng một chiến lược chọn các mức độ chi tiết tăng qua các lớp với độ dốc tối thiểu để có hiệu suất mô hình hiệu quả.

Trên MatLM 850M, chúng tôi so sánh các kiến trúc được dự đoán bởi Trộn & Kết hợp và thuật toán NAS, trong Hình 6. Chúng tôi thấy rằng heuristic của chúng tôi hoạt động ít nhất là tốt như NAS, dẫn đến các mô hình nằm trên đường cong pareto-tối ưu. Lưu ý rằng Trộn & Kết hợp không yêu cầu huấn luyện bổ sung của một mô-đun hoặc chi phí trong việc tính toán mô hình con tối ưu trong một ràng buộc chi phí.

D.2 NAS
Tìm kiếm Kiến trúc Thần kinh [68,48] là một phương pháp tự nhiên để tìm kiếm một mạng tối ưu trong số các kiến trúc có thể trong một ngân sách ràng buộc. Thực tế, OFA [9] sử dụng một kỹ thuật dựa trên tìm kiếm tiến hóa [48] cùng với một bộ dự đoán mất mát kiến trúc (Phụ lục B.3) trong thói quen tìm kiếm của nó. Trong khi trên các mô hình quy mô nhỏ hơn, có thể dễ dàng triển khai phương pháp NAS tương tự như OFA làm, trên quy mô lớn hơn nó trở nên tốn kém một cách cấm đoán và có lỗi.

--- TRANG 19 ---
Để có được mất mát tương ứng với một kiến trúc, người ta cần chạy trên một tập hợp được giữ lại đủ lớn để có được một giá trị trung bình. Ví dụ, trong các lần chạy của chúng tôi, chúng tôi đã sử dụng một tập hợp được giữ lại khoảng 200M token. Ở quy mô lớn, việc liên tục chạy các mô hình lớn trên tập hợp được giữ lại này để thu thập các giá trị mất mát trở nên tốn kém. Ví dụ, để thu thập 16k cặp (kiến trúc, mất mát), chúng tôi đã sử dụng 256 chip TPUv-4 trong khoảng 2 ngày.

Khi chúng tôi có tập dữ liệu, chúng tôi chia nó thành tỷ lệ 60%-40% train-eval. Sau khi huấn luyện bộ dự đoán mất mát kiến trúc, chúng tôi đã kiểm tra hiệu suất của nó trên tập hợp được giữ lại và nhận thấy lỗi bình phương trung bình ∼5e−5.

Khi chúng ta chuyển sang chế độ mô hình lớn, khoảng cách hiệu suất giữa các mô hình có kích thước khác nhau giảm. Ví dụ, khoảng cách giữa Baseline-S và Baseline-XL 78M tham số, có kích thước 71M và 78M, là 4.01 vs 3.83. Đó là một khoảng cách 0.18 mất mát cho chỉ 7M tham số khác biệt. Nhưng trong trường hợp mô hình 850M, Baseline-S và Baseline-XL có kích thước 582M vs 850M, mất mát tương ứng là 3.017 vs 2.84. Vì vậy, một sự khác biệt 0.177 cho 268M tham số khác biệt. Xu hướng này mở rộng đến các mô hình lớn hơn, và khoảng cách giảm giữa các kích thước mô hình khác nhau. Vì vậy, việc huấn luyện một bộ dự đoán độ chính xác kiến trúc trở nên ngày càng thách thức và có lỗi, vì nó phải học phân biệt các kiến trúc với những khác biệt rất nhỏ trong giá trị mất mát của chúng.

Trong thí nghiệm của chúng tôi cho OFA, chúng tôi lấy tốt nhất của các cấu hình mô hình được dự đoán bởi NAS và cấu hình mô hình trong số các điểm được lấy mẫu cho một ngân sách tương ứng.

E Luật Mở rộng cho Bộ giải mã Ngôn ngữ
Chúng tôi cung cấp kết quả được chia theo mức độ chi tiết cho mất mát xác thực, điểm trung bình trên các nhiệm vụ đánh giá, và tính nhất quán trong Hình 9, 10, và 11 tương ứng. Chúng tôi quan sát thấy rằng khoảng cách trong mất mát xác thực và độ chính xác đánh giá giữa MatLM và Baseline dường như là hằng số. Đối với tính nhất quán, khoảng cách dường như giảm với quy mô, nhưng người ta sẽ cần mở rộng các mô hình nhiều bậc độ lớn vượt quá những gì có thể ngày nay để các baseline có tính nhất quán so sánh được với MatLM.

E.1 Luật mở rộng của MatFormers vs Transformers.
Luật mở rộng là những công cụ thiết yếu để ước tính chất lượng khi chi phí huấn luyện hoặc suy luận được tăng lên. Luật mở rộng có thể giúp chúng ta xem xét các sắc thái khác nhau của huấn luyện và triển khai như chi phí huấn luyện tổng thể trong FLOP, hiệu quả dữ liệu huấn luyện và tham số, và sử dụng FLOP trung bình suy luận so với độ trễ cho các triển khai.

Mối quan hệ mở rộng của MatFormer so với Transformer vừa đơn giản vừa phức tạp. Đơn giản, vì các đường cong mở rộng MatFormer cho tiền huấn luyện tương tự như của Transformer – vì vậy MatFormer chỉ yêu cầu một lượng tính toán tương tự và cùng siêu tham số hoạt động cho Transformer có hiệu quả cho MatFormer. Mối quan hệ mở rộng phức tạp đến từ thực tế rằng MatFormer cho phép huấn luyện nhiều mô hình với một lần chạy huấn luyện duy nhất mà định tính khác với Transformer và khó tính vào các phương trình mở rộng. Về mặt hiệu quả, nếu chúng ta so sánh FLOP huấn luyện tương đương của tất cả các mô hình có thể trích xuất từ MatFormer, thì việc huấn luyện MatFormer một mình có lợi thế rõ ràng trong bất kỳ trường hợp nào mà tất cả các tham số được sử dụng để huấn luyện các mô hình Transformer tiêu chuẩn trên cùng tập dữ liệu vượt quá 2.58P, trong đó P là số lượng tham số của MatFormer và mô hình Transformer lớn nhất. Điều này là do MatFormer sử dụng 2.58 lần FLOP hơn mỗi token cho một lần chạy huấn luyện so với Transformer: 4× FLOP hơn cho các tham số lớp attention và {1 + 1/2 + 1/4 + 1/8 = 1.875}× FLOP hơn cho các lớp MLP.

F Phân tích Thêm về Bộ giải mã Ngôn ngữ
F.1 Phân kỳ KL Giữa các Mô hình S, M, L và XL
Hình 7 thể hiện phép tính tính nhất quán mượt mà hơn giữa hai mô hình tạo sinh được đo bằng phân kỳ KL của đầu ra mô hình nhỏ hơn với đầu ra mô hình lớn hơn. Tương tự như metric tính nhất quán khớp chính xác kiểu cứng được sử dụng trong bài báo chính, có một khoảng cách đáng kể giữa tính nhất quán của các mô hình con MatLM với mô hình MatLM-XL và giữa đó với các mô hình cơ sở tương ứng. Điều này chỉ ra cách các chiến lược lấy mẫu dựa trên xác suất đầu ra không thay đổi tính nhất quán hành vi giữa hai mô hình và rằng nó vẫn tuân theo xu hướng tạo ra token với xác suất cao nhất. Khái niệm tính nhất quán mượt mà hơn này lập luận cho việc bảo tồn cấu trúc không gian metric với việc ma trận phân loại/nhúng đầu ra được chia sẻ qua tất cả các mô hình con của MatLM.

--- TRANG 20 ---
Bảng 6: Đối với mô hình 850M, chúng tôi thử nghiệm với việc sửa đổi {pS, pM, pL, pXL} để lấy mẫu các mô hình con từ một phân phối không đồng nhất trong quá trình huấn luyện và báo cáo kết quả qua tất cả các mức độ chi tiết. Chúng tôi thấy rằng tất cả các chiến lược tăng trọng số mất mát cho mức độ chi tiết lớn nhất hoạt động tốt, với sự suy giảm khiêm tốn trên các mức độ chi tiết M và S.

Xác suất Mô hình S M L XL
Baseline N/A 3.017 2.971 2.910 2.840
MatFormer 0.44/0.31/0.15/0.10 2.963 2.925 2.899 2.877
0.31/0.27/0.22/0.20 2.977 2.929 2.890 2.857
0.25/0.25/0.25/0.25 2.970 2.934 2.886 2.846
0.20/0.22/0.24/0.34 3.000 2.939 2.885 2.836
0.17/0.20/0.22/0.41 3.010 2.943 2.884 2.829

bảo tồn cho rằng ma trận phân loại/nhúng đầu ra được chia sẻ qua tất cả các mô hình con của MatLM.

[Hình 7 được giữ nguyên]

Hình 7: Biến thể mượt mà hơn của tính nhất quán đo phân kỳ KL giữa các mô hình nhỏ hơn và mô hình XL tương ứng. Metric này, không giống như biến thể khớp chính xác, cũng tính đến các chiến lược lấy mẫu khác nhau trên phân phối đầu ra trong quá trình triển khai. Trong hình này, chúng tôi vẽ phân kỳ KL của các mức độ chi tiết S, M, L đối với XL cho mô hình 850M tham số.

F.2 Sử dụng MatFormers cho Khối con Attention
Chúng tôi thử nghiệm với việc áp dụng MRL cho khối con attention của khối Transformer. Cụ thể hơn, chúng tôi tối ưu hóa chung 4 mạng con với số lượng đầu attention khác nhau nattn, 3∗nattn/4, nattn/2, nattn/4 có kích thước d/nattn mỗi cái. Trong Hình 8, chúng tôi vẽ mất mát xác thực cho những mô hình này (MatLM-Attn), các baseline tương ứng của chúng và các mạng con Trộn & Kết hợp. Chúng tôi thấy rằng tương tự như các thí nghiệm của chúng tôi trên MatLM Trộn & Kết hợp, Trộn & Kết hợp giúp có được nhiều mô hình MatLM-Attn trên đường cong mất mát-so với-tính toán tối ưu.

F.3 Điều chỉnh Xác suất Lấy mẫu
Trong Bảng 6, chúng tôi thử nghiệm với việc điều chỉnh xác suất lấy mẫu cho các mức độ chi tiết riêng lẻ để điều tra sự đánh đổi giữa kích thước mức độ chi tiết và hiệu suất. Cụ thể hơn, chúng tôi điều chỉnh {p1, p2, p3, p4}, và thấy rằng tất cả các chiến lược tăng trọng số mất mát cho mức độ chi tiết lớn nhất hoạt động tốt, với sự suy giảm khiêm tốn trên các mức độ chi tiết M và S.

G Phân tích Thêm về Bộ mã hóa Thị giác
G.1 Tách Hiệu ứng của MatFormer trên Tiền huấn luyện và Tinh chỉnh
Bảng 7 điều tra hiệu ứng của MatFormer trên các giai đoạn tiền huấn luyện và tinh chỉnh của mô hình ViT-L/16. ViT-L/16 thường được tiền huấn luyện trên ImageNet-21K và sau đó tinh chỉnh trên ImageNet-1K cho đánh giá cuối cùng. Bảng 7 cho thấy rằng có MatFormer trong quá trình tiền huấn luyện tạo ra một mô hình tốt hơn cho việc tinh chỉnh downstream so với tiền huấn luyện ViT thông thường. Đồng thời, tinh chỉnh một ViT được tiền huấn luyện vanilla với MatFormer dẫn đến tính linh hoạt được tạo ra vào mô hình. Mặc dù ít chính xác hơn đến 2% so với các đối tác của nó ở một số mức độ chi tiết, một MatViT được tinh chỉnh đã học cách phân bổ lại thông tin để cung cấp các mô hình lồng nhau mạnh. Xem xét rằng điều này không đáng kể so với chi phí tiền huấn luyện, có thể lấy mô hình ViT được tiền huấn luyện lớn nhất và tinh chỉnh với MatFormer để có được một biến thể MatViT có thể triển khai.

--- TRANG 21 ---
[Hình 8 được giữ nguyên]

Hình 8: Mất mát xác thực cho các mô hình MatLM-Attn 850M & baseline.

Bảng 7: Lưới 2×2 của các cặp để đánh giá (độ chính xác top-1 (%)) các hiệu ứng của MatFormer và huấn luyện tiêu chuẩn trên tiền huấn luyện (PT) trên ImageNet-21K và tinh chỉnh (FT) trên ImageNet-1K sử dụng kiến trúc L/16. Sử dụng MatFormer trong quá trình tiền huấn luyện giúp mang lại các bộ mã hóa chính xác hơn và đàn hồi cho việc sử dụng downstream.

PT↓/ FT→ # Tham số (M) ViT MatViT
ViT 306 85.26 85.57
206 85.12 84.27
156 85.02 82.79
131 84.42 82.1
MatViT 306 85.58 85.61
206 – 85.40
156 – 85.02
131 – 84.41

G.2 Đánh giá Truy xuất Hình ảnh Truyền thống
Bảng 8 thể hiện đánh giá truy xuất hình ảnh truyền thống trên ImageNet-1K nơi bộ mã hóa truy vấn và tài liệu giống nhau cho truy xuất láng giềng gần nhất. Đánh giá dựa trên 1-láng giềng gần nhất (NN) tuân theo chặt chẽ kết quả phân loại một-so với-tất cả được hiển thị trong Hình 4. Cả hai biến thể MatViT B/16 và L/16 có các mô hình con có hiệu suất truy xuất tốt hoặc tốt hơn so với các đối tác được huấn luyện độc lập của chúng. Cụ thể, truy xuất dựa trên MatViT có thể chính xác hơn đến 0.5% so với các baseline trong khi một mô hình con MatViT 200M tham số có thể chính xác hơn baseline ViT 300M tham số.

--- TRANG 22 ---
Bảng 8: Độ chính xác truy xuất hình ảnh 1-NN (%) khi bộ mã hóa truy vấn và tài liệu là cùng một mô hình. Tương tự như kết quả phân loại hình ảnh, các biến thể MatViT hoặc khớp hoặc vượt trội hơn các đối tác ViT tiêu chuẩn tương ứng. Lưu ý rằng tất cả các mô hình nhỏ hơn của một mô hình nhất định trong MatViT được trích xuất miễn phí trong khi các baseline phải được huấn luyện rõ ràng cho các ràng buộc.

Bộ mã hóa # Tham số (M) ViT MatViT
B/16 85 77.46 77.38
57 76.58 76.41
43 74.90 74.49
36 71.44 71.72
L/16 300 83.17 83.67
200 82.92 83.23
150 82.81 82.89
125 82.22 82.14

[Các hình 9, 10, 11 được giữ nguyên với biểu đồ về xu hướng mở rộng]

Hình 9: Chúng tôi huấn luyện các mô hình MatLM chỉ giải mã khác nhau ở một dải kích thước từ 78M đến 850M tham số và quan sát xu hướng mở rộng cho mỗi mức độ chi tiết mô hình trên mất mát xác thực. Chúng tôi quan sát thấy rằng khoảng cách giữa MatLM và baseline dường như là hằng số tại mỗi mức độ chi tiết. Tính nhất quán giữa các mô hình con của các mức độ chi tiết và các mô hình XL cho thấy hiệu ứng của việc huấn luyện chung MatFormer trên việc đảm bảo bản địa hành vi tương tự qua các mô hình con.

--- TRANG 23 ---
[Hình 10 được giữ nguyên]

Hình 10: Chúng tôi huấn luyện các mô hình MatLM chỉ giải mã khác nhau ở một dải kích thước từ 78M đến 850M tham số và quan sát xu hướng mở rộng cho mỗi mức độ chi tiết mô hình cho điểm trung bình trên đánh giá một lần. Chúng tôi quan sát thấy rằng khoảng cách giữa MatLM và baseline vẫn giữ nguyên với quy mô, vượt trội hơn các baseline cho tất cả các mức độ chi tiết cho các mô hình lớn nhất.

--- TRANG 24 ---
[Hình 11 được giữ nguyên]

Hình 11: Chúng tôi huấn luyện các mô hình MatLM chỉ giải mã khác nhau ở một dải kích thước từ 78M đến 850M tham số và quan sát xu hướng mở rộng cho mỗi mô hình con S, M, L cho tính nhất quán với mô hình XL. Chúng tôi quan sát thấy rằng khoảng cách giữa MatLM và baseline giảm với quy mô, nhưng người ta sẽ cần mở rộng baseline nhiều bậc độ lớn để có tính nhất quán so sánh được với MatLM.

--- TRANG 25 ---
[Các bảng 9-13 được giữ nguyên với số liệu đánh giá downstream và mất mát]

--- TRANG 26 ---
[Tiếp tục các bảng đánh giá]

--- TRANG 27 ---
[Tiếp tục các bảng đánh giá]

--- TRANG 28 ---
[Bảng 14 và danh sách kiểm tra NeurIPS được giữ nguyên]

Danh sách Kiểm tra Bài báo NeurIPS
1. Tuyên bố
Câu hỏi: Các tuyên bố chính được đưa ra trong tóm tắt và giới thiệu có phản ánh chính xác đóng góp và phạm vi của bài báo không?
Trả lời: [Có]
Lý do: Tất cả các tuyên bố trong tóm tắt được hỗ trợ bởi kết quả thực nghiệm trong Phần 4.

2. Hạn chế
Câu hỏi: Bài báo có thảo luận về những hạn chế của công việc được thực hiện bởi các tác giả không?
Trả lời: [Có]
Lý do: Chúng tôi thảo luận điều này xuyên suốt bản thảo.

3. Giả định và Chứng minh Lý thuyết
Câu hỏi: Đối với mỗi kết quả lý thuyết, bài báo có cung cấp tập hợp đầy đủ các giả định và một chứng minh hoàn chỉnh (và chính xác) không?
Trả lời: [NA].
Lý do: Không có kết quả lý thuyết.

4. Khả năng Tái tạo Kết quả Thực nghiệm
Câu hỏi: Bài báo có tiết lộ đầy đủ tất cả thông tin cần thiết để tái tạo các kết quả thực nghiệm chính của bài báo đến mức ảnh hưởng đến các tuyên bố và/hoặc kết luận chính của bài báo không (bất kể mã và dữ liệu có được cung cấp hay không)?
Trả lời: [Có]
Lý do: Chi tiết triển khai được cung cấp trong Phụ lục B. Hơn nữa, mã để tái tạo Phần 4.2 sẽ được mã nguồn mở tại liên kết.

5. Truy cập Mở đến Dữ liệu và Mã
Câu hỏi: Bài báo có cung cấp truy cập mở đến dữ liệu và mã, với hướng dẫn đầy đủ để tái tạo trung thực các kết quả thực nghiệm chính, như được mô tả trong tài liệu bổ sung không?
Trả lời: [Không]
Lý do: Chi tiết triển khai được cung cấp trong Phụ lục B. Mặc dù mã để tái tạo Phần 4.2 đã được mã nguồn mở, mô hình ngôn ngữ đã được huấn luyện trên dữ liệu độc quyền.

6. Cài đặt/Chi tiết Thực nghiệm
Câu hỏi: Bài báo có chỉ định tất cả các chi tiết huấn luyện và kiểm tra (ví dụ, phân chia dữ liệu, siêu tham số, cách chúng được chọn, loại bộ tối ưu hóa, v.v.) cần thiết để hiểu kết quả không?
Trả lời: [Có]

--- TRANG 29 ---
Lý do: Chi tiết triển khai được cung cấp trong Phụ lục B, với trích dẫn các mô hình mà những mô hình này được xây dựng dựa trên.

7. Ý nghĩa Thống kê Thí nghiệm
Câu hỏi: Bài báo có báo cáo thanh lỗi được định nghĩa phù hợp và chính xác hoặc thông tin thích hợp khác về ý nghĩa thống kê của các thí nghiệm không?
Trả lời: [Không]
Lý do: Điều này chưa được thực hiện do chi phí tính toán đắt đỏ của việc huấn luyện các mô hình ngôn ngữ từ đầu.

8. Tài nguyên Tính toán Thí nghiệm
Câu hỏi: Đối với mỗi thí nghiệm, bài báo có cung cấp thông tin đầy đủ về tài nguyên máy tính (loại công nhân tính toán, bộ nhớ, thời gian thực hiện) cần thiết để tái tạo các thí nghiệm không?
Trả lời: [Có]
Lý do: Thông tin này được cung cấp trong Phụ lục B.

9. Quy tắc Đạo đức
Câu hỏi: Nghiên cứu được tiến hành trong bài báo có tuân thủ, ở mọi khía cạnh, với Quy tắc Đạo đức NeurIPS https://neurips.cc/public/EthicsGuidelines không?
Trả lời: [Có]
Lý do: Tác động rộng hơn đã được thảo luận trong Phụ lục A.

10. Tác động Rộng hơn
Câu hỏi: Bài báo có thảo luận cả tác động xã hội tích cực tiềm năng và tác động xã hội tiêu cực của công việc được thực hiện không?
Trả lời: [Có]
Lý do: Tác động rộng hơn đã được thảo luận trong Phụ lục A.

11. Biện pháp Bảo vệ
Câu hỏi: Bài báo có mô tả các biện pháp bảo vệ đã được đưa ra cho việc phát hành có trách nhiệm dữ liệu hoặc mô hình có nguy cơ cao bị lạm dụng không (ví dụ, mô hình ngôn ngữ được tiền huấn luyện, trình tạo hình ảnh, hoặc tập dữ liệu được thu thập)?
Trả lời: [NA]
Lý do: Không có tập dữ liệu hoặc mô hình nào được phát hành với bài báo này.

12. Giấy phép cho Tài sản Hiện có
Câu hỏi: Những người tạo ra hoặc chủ sở hữu ban đầu của tài sản (ví dụ, mã, dữ liệu, mô hình), được sử dụng trong bài báo, có được ghi công đúng cách và giấy phép và điều khoản sử dụng được đề cập rõ ràng và được tôn trọng đúng cách không?
Trả lời: [Có]
Lý do: Các mô hình thị giác và dữ liệu được sử dụng trong này đã được công khai trong một thời gian dài. Các mô hình ngôn ngữ kế thừa các giấy phép thích hợp của bài báo Lamda [58] trong khi chúng tôi huấn luyện trên dữ liệu độc quyền.

13. Tài sản Mới
Câu hỏi: Các tài sản mới được giới thiệu trong bài báo có được tài liệu hóa tốt và tài liệu được cung cấp cùng với các tài sản không?
Trả lời: [Có]
Lý do: Mã để tái tạo các thí nghiệm sẽ được phát hành cho camera ready.

Hướng dẫn:
• Trả lời NA có nghĩa là bài báo không phát hành tài sản mới.
• Các nhà nghiên cứu nên truyền đạt chi tiết của tập dữ liệu/mã/mô hình như một phần của bài nộp của họ thông qua các mẫu có cấu trúc. Điều này bao gồm chi tiết về huấn luyện, giấy phép, hạn chế, v.v.

--- TRANG 30 ---
• Bài báo nên thảo luận liệu và cách thức đồng ý được thu thập từ những người có tài sản được sử dụng.
• Tại thời điểm nộp bài, hãy nhớ ẩn danh tài sản của bạn (nếu có thể áp dụng). Bạn có thể tạo một URL ẩn danh hoặc bao gồm một tệp zip ẩn danh.

14. Crowdsourcing và Nghiên cứu với Đối tượng Con người
Câu hỏi: Đối với các thí nghiệm crowdsourcing và nghiên cứu với đối tượng con người, bài báo có bao gồm toàn bộ văn bản hướng dẫn được đưa cho người tham gia và ảnh chụp màn hình, nếu có thể áp dụng, cũng như chi tiết về bồi thường (nếu có) không?
Trả lời: [NA]
Lý do: Bài báo không liên quan đến crowdsourcing hoặc nghiên cứu với đối tượng con người.

Hướng dẫn:
• Trả lời NA có nghĩa là bài báo không liên quan đến crowdsourcing hoặc nghiên cứu với đối tượng con người.
• Bao gồm thông tin này trong tài liệu bổ sung là tốt, nhưng nếu đóng góp chính của bài báo liên quan đến đối tượng con người, thì càng nhiều chi tiết càng tốt nên được bao gồm trong bài báo chính.
• Theo Quy tắc Đạo đức NeurIPS, những người lao động tham gia vào thu thập dữ liệu, quản lý, hoặc lao động khác nên được trả ít nhất mức lương tối thiểu trong nước của người thu thập dữ liệu.

15. Phê duyệt Hội đồng Đánh giá Thể chế (IRB) hoặc Tương đương cho Nghiên cứu với Đối tượng Con người
Câu hỏi: Bài báo có mô tả các rủi ro tiềm ẩn mà người tham gia nghiên cứu phải chịu, liệu những rủi ro đó có được tiết lộ cho đối tượng không, và liệu phê duyệt Hội đồng Đánh giá Thể chế (IRB) (hoặc phê duyệt/đánh giá tương đương dựa trên yêu cầu của quốc gia hoặc thể chế của bạn) có được thu thập không?
Trả lời: [NA]
Lý do: Bài báo không liên quan đến crowdsourcing hoặc nghiên cứu với đối tượng con người.

Hướng dẫn:
• Trả lời NA có nghĩa là bài báo không liên quan đến crowdsourcing hoặc nghiên cứu với đối tượng con người.
• Tùy thuộc vào quốc gia nơi nghiên cứu được tiến hành, phê duyệt IRB (hoặc tương đương) có thể được yêu cầu cho bất kỳ nghiên cứu đối tượng con người nào. Nếu bạn có được phê duyệt IRB, bạn nên nêu rõ điều này trong bài báo.
• Chúng tôi nhận ra rằng các thủ tục cho điều này có thể khác nhau đáng kể giữa các thể chế và địa điểm, và chúng tôi mong đợi các tác giả tuân thủ Quy tắc Đạo đức NeurIPS và các hướng dẫn cho thể chế của họ.
• Đối với bài nộp ban đầu, không bao gồm bất kỳ thông tin nào có thể phá vỡ tính ẩn danh (nếu có thể áp dụng), chẳng hạn như thể chế tiến hành đánh giá.
