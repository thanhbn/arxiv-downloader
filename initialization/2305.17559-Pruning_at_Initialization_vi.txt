# Pruning at Initialization -
A Sketching Perspective
Noga Bar
Tel Aviv University
Tel Aviv, Israel
nogabar@mail.tau.ac.ilRaja Giryes
Tel Aviv University
Tel Aviv, Israel
raja@tauex.tau.ac.il

# Tóm tắt
Giả thuyết vé số (LTH) đã tăng sự chú ý đến việc cắt tỉa mạng nơ-ron tại khởi tạo. Chúng tôi nghiên cứu vấn đề này trong thiết lập tuyến tính. Chúng tôi cho thấy việc tìm một mặt nạ thưa tại khởi tạo tương đương với bài toán sketching được giới thiệu cho phép nhân ma trận hiệu quả. Điều này cung cấp cho chúng tôi các công cụ để phân tích vấn đề LTH và có được những hiểu biết sâu sắc về nó. Cụ thể, sử dụng mặt nạ được tìm thấy tại khởi tạo, chúng tôi giới hạn lỗi xấp xỉ của mô hình tuyến tính được cắt tỉa tại cuối quá trình huấn luyện. Chúng tôi biện minh lý thuyết cho bằng chứng thực nghiệm trước đây rằng việc tìm kiếm mạng thưa có thể độc lập với dữ liệu. Bằng cách sử dụng góc nhìn sketching, chúng tôi đề xuất một cải tiến chung cho các thuật toán hiện tại để cắt tỉa tại khởi tạo, mà chúng tôi cho thấy có lợi trong trường hợp độc lập với dữ liệu.

## 1 Giới thiệu
Việc cắt tỉa một mạng nơ-ron tại khởi tạo, nơi các trọng số được loại bỏ trước khi huấn luyện, với tổn hại tối thiểu đến hiệu suất mạng có thể có lợi cho hiệu quả huấn luyện. Nó cũng có thể được sử dụng để có được những hiểu biết sâu sắc về huấn luyện mạng nơ-ron và khả năng biểu đạt nói chung. Theo giả thuyết vé số (LTH) [23], một mạng có thể chứa các mạng con cực kỳ thưa tại khởi tạo đạt được hiệu suất tương đương hoặc thậm chí tốt hơn khi được huấn luyện riêng biệt. Thuật toán gốc được đề xuất để tìm vé chiến thắng là không hiệu quả và yêu cầu nhiều lần huấn luyện cho đến khi hội tụ. Tuy nhiên, những người khác đã đề xuất cắt tỉa tại khởi tạo theo cách hiệu quả hơn [39, 61, 63, 17, 2, 38].

Hầu hết các công trình đề xuất các hàm tính điểm để tìm một mạng con tại khởi tạo dựa trên dữ liệu và nhiệm vụ cụ thể [39,63,17,2]. Tuy nhiên, bằng chứng cho thấy vé số chiến thắng độc lập với dữ liệu. Cụ thể, đã được chỉ ra rằng vé chiến thắng có thể được chuyển giao giữa các tập dữ liệu và nhiệm vụ [19,47,13,57]. Hơn nữa, công trình trước đây đã cho thấy các phương pháp cắt tỉa có thể có hiệu suất tốt với dữ liệu huấn luyện bị hỏng [60]. Ngoài ra, công trình của SynFlow đã chứng minh rằng một mạng thậm chí có thể được cắt tỉa tại khởi tạo mà không cần sử dụng dữ liệu và hàm mất mát cụ thể theo nhiệm vụ [61].

Trong công trình này, chúng tôi nhằm mục đích giải thích thành công của LTH trong thiết lập không giám sát mà không có dữ liệu. Phân tích lý thuyết trước đây tập trung vào một phiên bản mạnh hơn của LTH [56]. Theo nghiên cứu, mạng nơ-ron sâu có một mạng con thưa có khả năng hiệu suất tốt ngay cả khi không huấn luyện trong thiết lập có giám sát. Cụ thể, họ cho thấy đối với một kiến trúc mạng nơ-ron sâu (DNN) cho trước, khởi tạo và dữ liệu đích (có nhãn), có một mạng con đạt được độ chính xác cao mà không cần huấn luyện. Giải thích lý thuyết cho LTH mạnh được phát triển bằng cách ước tính hàm được tạo ra bởi mạng lớn và dày đặc chỉ sử dụng các mạng con thưa trong đó [43,55]. Kết quả này đã được tổng quát hóa thêm và đã được chỉ ra rằng khởi tạo có thể tương thích với một tập hợp các hàm trên tập huấn luyện [10]. Trong khi các kết quả trên cung cấp một giải thích hấp dẫn cho thành công của LTH trong trường hợp có giám sát, giải thích của họ về sự tồn tại của mặt nạ giả định dữ liệu có sẵn để thực hiện cắt tỉa và không cần huấn luyện. Nghiên cứu của chúng tôi, mặt khác, tập trung vào thiết lập không giám sát, nơi cắt tỉa được thực hiện sử dụng dữ liệu ngẫu nhiên và áp dụng cho các tham số khác so với những tham số tại khởi tạo (có thể sau khi huấn luyện).

Để phân tích của chúng tôi, chúng tôi rút ra một kết nối giữa cắt tỉa tại khởi tạo và một thuật toán sketching nổi tiếng [18]. Ban đầu, thuật toán được đề xuất cho phép nhân ma trận hiệu quả trong khi giảm thiểu lỗi của xấp xỉ phép nhân. Chúng tôi chọn tập trung vào trường hợp tuyến tính và phân tích nó để có được những hiểu biết sâu sắc về trường hợp tổng quát, đây là một thực hành phổ biến khi phân tích mạng nơ-ron [4, 9, 58, 5, 40, 3, 50, 68].

Quan sát chính trong công trình của chúng tôi là trong trường hợp tuyến tính, thuật toán sketching tương ứng với bài toán cắt tỉa tại khởi tạo. Dựa trên mối quan hệ này, chúng tôi mở rộng phân tích sketching về lỗi xấp xỉ tại khởi tạo cho trường hợp của một vector chưa biết. Trong trường hợp của chúng tôi, vector này có thể được hiểu là vector đã học tại cuối quá trình huấn luyện. Chúng tôi tập trung vào sự tương ứng giữa sketching và cắt tỉa mà không có dữ liệu. Điều này cho phép chúng tôi phân tích hiệu suất của các thuật toán cắt tỉa hoạt động trong chế độ không giám sát, tức là không có dữ liệu huấn luyện có nhãn. Chúng tôi phát triển một giới hạn cho thấy lỗi trong cắt tỉa mà không có dữ liệu phụ thuộc vào khoảng cách giữa các trọng số của mạng tuyến tính tại khởi tạo và tại cuối quá trình huấn luyện.

Khoảng cách này được biết là nhỏ trong các NN thực tế và đặc biệt trong chế độ Neural Tangent Kernel (NTK) [37,32,6], nơi độ rộng mạng tăng đến vô cùng. Dưới các giả định phổ biến trong thiết lập NTK, chúng tôi xem xét các đặc trưng đầu ra mạng như các đặc trưng ngẫu nhiên tuyến tính. Dựa trên các giả định này, chúng tôi chứng minh rằng mặt nạ được tìm thấy bằng sketching cũng xấp xỉ các đặc trưng đầu ra mạng.

Được trang bị các kết quả lý thuyết, chúng tôi chuyển sang nghiên cứu các thuật toán thực tế cho bài toán cắt tỉa tại khởi tạo mà không có dữ liệu. Chúng tôi xem xét phương pháp SynFlow hiện đại và không giám sát và cho thấy trong trường hợp tuyến tính nó có sự tương đồng lớn với sketching khi được áp dụng với một vector ngẫu nhiên. Điều này cung cấp cho chúng tôi một giải thích khả dĩ cho thành công của SynFlow. Chúng tôi cũng phân tích kết nối của phương pháp SNIP có giám sát với sketching và sử dụng nó để đề xuất một phiên bản không giám sát của SNIP.

Các mối quan hệ chúng tôi rút ra cũng cho thấy cắt tỉa thành công có tương quan cao với việc chọn các trọng số có độ lớn lớn tại khởi tạo. Chúng tôi xác thực thực nghiệm các phát hiện của mình cho cả SynFlow và thuật toán cắt tỉa độ lớn lặp (IMP) (phương pháp được đề xuất trong công trình LTH gốc) cho thấy cả hai đều có xu hướng duy trì các trọng số có độ lớn lớn sau khi cắt tỉa.

Để xác thực thêm phân tích của chúng tôi với dữ liệu ngẫu nhiên, chúng tôi cho thấy đối với các thuật toán cắt tỉa đã biết khác nhau sử dụng dữ liệu đầu vào [23,63,39] rằng hiệu suất của chúng chỉ giảm nhẹ khi đầu vào được thay thế bằng đầu vào hoàn toàn ngẫu nhiên. Điều này phù hợp với bằng chứng trước đây rằng các phương pháp cắt tỉa không khai thác dữ liệu [60].

Dựa trên các kết quả trên, chúng tôi đề xuất một cải tiến chung cho các thuật toán cắt tỉa hiện tại trong trường hợp không giám sát. Thay vì cắt tỉa trọng số bằng cách loại bỏ các điểm số thấp nhất, các mặt nạ sketching được ngẫu nhiên hóa dựa trên một số xác suất. Chúng tôi kiểm tra hiệu ứng của việc thay thế tiêu chí ngưỡng nghiêm ngặt của cắt tỉa bằng một tiêu chí ngẫu nhiên hóa trong đó mặt nạ được lấy mẫu dựa trên điểm số phương pháp cắt tỉa. Chiến lược này cho thấy cải tiến trong hầu hết các trường hợp cho các kiến trúc mạng và tập dữ liệu khác nhau, cụ thể là CIFAR-10, CIFAR-100 [35] và Tiny-ImageNet [66].

## 2 Công trình liên quan
Phương pháp nghiên cứu chính cho việc cắt tỉa mạng nơ-ron sâu theo quy trình huấn luyện → cắt tỉa → tinh chỉnh [49,46,36,29,28,27]. Những phương pháp này yêu cầu huấn luyện đến khi hội tụ, sau đó cắt tỉa và cuối cùng tinh chỉnh. Do đó, chúng có khả năng tiết kiệm tính toán tại thời gian suy luận. Một phương pháp khác cho việc cắt tỉa là làm thưa mô hình trong quá trình huấn luyện nơi huấn luyện và cắt tỉa được thực hiện đồng thời [12,11,42,8,45,48, và nhiều hơn nữa]. Trong trường hợp này, giai đoạn tinh chỉnh được bỏ qua. Những phương pháp này có khả năng hạn chế để cải thiện hiệu quả huấn luyện và lợi ích chủ yếu dành cho thời gian suy luận. Cuối cùng, cắt tỉa tại khởi tạo, là trọng tâm của chúng tôi, nhằm mục đích đặt các tham số bằng không tại khởi tạo. Những phương pháp như vậy có thể cải thiện hiệu quả về tham số cho cả huấn luyện và suy luận [19]. Ngoài ra, nó có thể được sử dụng cho tìm kiếm kiến trúc mạng nơ-ron [44,64,1] và để có được hiểu biết lý thuyết sâu sắc hơn về DNN [7].

Trong công trình này, chúng tôi thảo luận về cắt tỉa mạng bằng cách đơn giản đặt trọng số bằng không (cắt tỉa không cấu trúc). Tuy nhiên, một phương pháp khác là loại bỏ hoàn toàn các nơ-ron của mạng (cắt tỉa có cấu trúc) [52,33,15,14] (để có một khảo sát toàn diện xem [51]). Những phương pháp này thường bao gồm nhiều tham số hơn những phương pháp không cấu trúc nhưng thường có lợi hơn cho thời gian tính toán trên phần cứng tiêu chuẩn. Tuy nhiên, các mô hình được cắt tỉa không cấu trúc cũng có những lợi thế này vì nó có thể dẫn đến giảm tính toán cho một số phần cứng trong khi duy trì số lượng tham số thấp [19]. Ngoài ra, một phương pháp ban đầu được trình bày cho cắt tỉa không cấu trúc sử dụng tính điểm đã được mở rộng cho cắt tỉa có cấu trúc tại khởi tạo [62]. Do đó, các khái niệm được trình bày trong công trình này có thể được mở rộng cho cắt tỉa có cấu trúc.

Do không gian tìm kiếm theo cấp số nhân, việc cắt tỉa DNN tại khởi tạo là một nhiệm vụ đầy thử thách. Trước LTH, đã được đề xuất sử dụng SNIP để cắt tỉa NN trong khi duy trì khả năng kết nối của nó theo độ lớn của gradient [39]. SNIP đã được cải thiện bằng cách sử dụng nó một cách lặp [17] hoặc bằng cách áp dụng nó sau một vài bước huấn luyện [2]. Đã được đề xuất để cải thiện sự lan truyền tín hiệu trong DNN tại khởi tạo [38] và tìm một mặt nạ trong khi bảo tồn dòng gradient sử dụng ma trận Hessian [63]. Một thuật toán cắt tỉa thưa cho đa tạp chiều cao được bao gồm trong [70], cũng như một xác thực lý thuyết về tính khả thi của các mạng con. Những phương pháp này dựa vào gradient của các tham số của chúng đối với dữ liệu đầu vào. Tuy nhiên, một nghiên cứu phát hiện rằng một số trong số này khó khai thác thông tin từ dữ liệu huấn luyện [60] gợi ý rằng nó có thể không cần thiết để tìm các mạng con thưa tốt. Một công trình khác, đã nới lỏng nhu cầu giám sát và sử dụng mô hình giáo viên-học sinh với dữ liệu không nhãn [41].

Giả thuyết vé số (LTH) [23] đã dẫn đến sự quan tâm tăng lên đối với mạng nơ-ron thưa. Nghiên cứu gốc đề xuất một thuật toán toàn diện và có giám sát để tìm vé chiến thắng. Có những công trình giảm sự phụ thuộc vào dữ liệu khi tìm kiếm vé chiến thắng. Đã được chứng minh rằng việc tìm kiếm vé ở giai đoạn đầu của huấn luyện, trước khi hội tụ, dẫn đến hiệu suất được cải thiện và tiết kiệm chi phí tính toán [67,31,24]. Đã được chỉ ra thêm rằng huấn luyện LT với các tập dữ liệu một phần dẫn đến vé chiến thắng khá tốt [71]. Cắt tỉa sớm đã được chứng minh lý thuyết là có lợi [65]. Cùng với các phương pháp cắt tỉa khác tại khởi tạo, đã được chỉ ra rằng các vé chiến thắng không phụ thuộc nhiều vào dữ liệu huấn luyện [60].

Các công trình khác đã chứng minh tính tổng quát của vé chiến thắng và cho thấy một mạng được cắt tỉa duy nhất có thể được chuyển giao qua các tập dữ liệu và đạt được hiệu suất tốt sau khi tinh chỉnh [19,47,13] và thậm chí LT có thể được sử dụng cho các tập dữ liệu không tự nhiên [57]. Sự tồn tại của các vé chiến thắng toàn cầu phù hợp với nhiều hàm đã được chỉ ra lý thuyết trong [10]. LTH đã được tăng cường và đã được đề xuất rằng một mạng con thưa trong khởi tạo mạng nơ-ron có độ chính xác cao mà không cần huấn luyện [56]. LTH mạnh sau đó đã được nghiên cứu lý thuyết [43,55,53,20,21]. Lưu ý rằng LTH mạnh yêu cầu một mạng lớn hơn trước khi cắt tỉa. Điều này đã được nới lỏng bằng cách khởi tạo các trọng số một cách lặp [16]. Những công trình này chỉ giới hạn lỗi xấp xỉ tại khởi tạo và giả định dữ liệu có nhãn.

SynFlow [61], một phương pháp nổi bật được đề xuất cho cắt tỉa tại khởi tạo, và không phụ thuộc vào dữ liệu hoặc nhiệm vụ. Chúng tôi xem xét trong nghiên cứu của mình sự tương đương của nó với một phương pháp sketching khi dữ liệu đầu vào cho SynFlow là ngẫu nhiên thay vì một vector tất cả các số một. Ngoài ra, nghiên cứu khác về SynFlow đề xuất xem xét các đường dẫn hoạt động trong mạng tại khởi tạo [54,25] và thiết lập các phương pháp cắt tỉa độc lập với dữ liệu. Họ công thức hóa phương pháp của mình theo các đường dẫn trong neural tangent kernel.

Chúng tôi thiết lập sự tương đương giữa bài toán sketching và cắt tỉa. Để đạt được điều này, chúng tôi sử dụng một kỹ thuật Monte-Carlo nổi tiếng cho xấp xỉ hiệu quả phép nhân ma trận và nén được thiết kế để xử lý các ma trận lớn [18]. Chúng tôi phân tích các phương pháp cắt tỉa mạng thông qua 'ống kính sketching', điều này dẫn đến một giới hạn lỗi xấp xỉ của mặt nạ tại khởi tạo. Loại kết quả này tương tự như các giới hạn xấp xỉ cho LTH mạnh nhưng sử dụng một kỹ thuật chứng minh khác phân tích trường hợp không giám sát và "yếu".

## 3 Sketching và Cắt tỉa tại Khởi tạo
Ký hiệu. Dữ liệu đầu vào chúng tôi nhằm mục đích mô hình hóa là xi∈Rd cho i= 1, ..., n và tương ứng mỗi ví dụ có nhãn yi∈R. Chúng tôi liên kết đầu vào như một ma trận X∈Rd×n nơi xi là các cột của nó và y∈Rn là vector chứa các nhãn. Hàng thứ i trong ma trận A được ký hiệu là A(i) và cột thứ i là A(i). Trừ khi có quy định khác, ∥x∥ là chuẩn Euclidean của x.

Phát biểu bài toán. Trong công trình này, chúng tôi liên kết đến các đặc trưng tuyến tính nơi chúng tôi nhằm mục đích tìm vector xấp xỉ dữ liệu, tức là w sao cho XTw≃y. Mối quan tâm chính của chúng tôi là làm thưa w trong khi giảm thiểu lỗi bình phương trung bình của các đặc trưng. Bài toán tối ưu có thể được viết như
min
m,s.t.∥m∥0≤s∥XTw−XT(w⊙m)∥, (1)
nơi ∥·∥0 là số lượng phần tử khác không trong m và ⊙ đại diện cho phép nhân từng phần tử. Rõ ràng, khi mặt nạ được áp dụng cho một vector, nó đúng rằng ∥w⊙m∥0≤s.

Kết nối của sketching và cắt tỉa tại khởi tạo. Chúng tôi tập trung vào một phương pháp sketching được trình bày ban đầu cho phép nhân ma trận hiệu quả [18]. Mục tiêu trong đó là nhân chỉ một tập con nhỏ các cột và hàng, trong khi gây hại ít nhất có thể đến độ chính xác xấp xỉ. Đóng góp chính của phương pháp là trong cách tập con này được chọn. Chúng tôi điều chỉnh nó để phù hợp với thiết lập đặc trưng tuyến tính, nơi phép nhân ma trận được giảm thành phép nhân của đầu vào cho trước X∈Rd×n và một vector w∈Rd. Điều này dẫn đến các đặc trưng tuyến tính XTw.

**Thuật toán 1** Sketching cho mặt nạ [18].
Đầu vào: Xác suất p∈Rd và mật độ s∈Z+.
Khởi tạo: Đặt mặt nạ m= 0.
cho t= 1, ..., s làm
it∼p
mit=mit+1
spit
kết thúc cho
Đầu ra: Mặt nạ m.

Lưu ý rằng việc chọn hàng/cột trong phép nhân ma trận tương đương với việc chọn một mặt nạ cho các phần tử trong w cho phép nhân ma trận-vector. Được cho một lựa chọn các phần tử, các phần tử tương thích với mặt nạ là khác không trong khi những phần tử khác là không. Các phần tử không trong m dẫn đến việc bỏ qua toàn bộ các cột trong ma trận. Ví dụ, xấp xỉ của phép nhân ma trận A với vector b được đặt mặt nạ với m thỏa mãn Ab≃A(b⊙m) =A({i,mi̸=0})b({i,mi̸=0}).

Điều này có nghĩa là bất kỳ thuật toán sketching nào nén phép nhân ma trận bằng cách chọn hàng/cột cũng có thể được sử dụng như một thủ tục đặt mặt nạ cho một vector trong phép nhân ma trận-vector.

Để tìm một mặt nạ m cho vector w như được mô tả trong phát biểu bài toán (Phương trình (1)), chúng tôi sử dụng thuật toán sketching được trình bày trong Thuật toán 1. Thuật toán lấy mẫu một phần tử ngẫu nhiên theo phân phối p và đặt phần tử này là khác không cho đến khi đạt được mật độ mong muốn. Lưu ý rằng các chỉ số it được lấy mẫu i.i.d. với hoàn lại và rằng phân phối của p được sử dụng cho cả lấy mẫu và tỷ lệ. Do khả năng lấy mẫu cùng một phần tử nhiều hơn một lần, m kết quả có độ chi tiết của ∥m∥0≤s và nó không nhất thiết đúng rằng ∥m∥0=s. Ngoài ra, không phải tất cả các phần tử khác không trong mặt nạ đều bằng nhau.

Bởi [18, Lemma 4] (được phát biểu lại trong Lemma 4.1), xác suất tối ưu để giảm thiểu lỗi là
Pr
w,X[i] =∥X(i)∥|wi|
Pd
j=1∥X(j)∥|wj|, i= 1, ..., d. (2)

Xác suất tỷ lệ thuận với chuẩn của các hàng của X, và nó tương ứng với một phần tử cụ thể tại mỗi ví dụ xk qua tất cả các đầu vào, k= 1, ..., n. Nhớ lại rằng X(i)∈Rn. Do đó, việc đặt mặt nạ một phần tử i trong w có nghĩa là bỏ qua tất cả các phần tử thứ i trong dữ liệu xk,k= 1, ..., n. Lưu ý rằng khi lấy mẫu với (2) có nhiều khả năng chọn các trọng số có độ lớn lớn hơn khi cho trước dữ liệu. Hình 1a và 2 cho thấy thực nghiệm rằng các phương pháp cắt tỉa DNN cũng có xu hướng giữ các trọng số có độ lớn cao hơn.

## 4 Lỗi Xấp xỉ Cắt tỉa
Sử dụng mối quan hệ giữa cắt tỉa và sketching được vẽ ra trong Mục 3, chúng tôi có thể sử dụng các công cụ phân tích từ sketching để kiểm tra các tính chất của cắt tỉa. Tất cả các chứng minh đều ở Phụ lục B.

Trong cắt tỉa tại khởi tạo, vector ban đầu w0 được sử dụng để tìm mặt nạ, và sau đó chúng tôi xấp xỉ các đặc trưng với cùng mặt nạ và một vector chưa biết khác w⋆. Thông thường, w⋆ là các trọng số đã học tại cuối quá trình huấn luyện. Các đặc trưng là XTw⋆ và chúng tôi nhằm mục đích giảm thiểu ∥XTw⋆−XT(w⋆⊙m)∥. Lưu ý rằng vì m được chọn ngẫu nhiên, chúng tôi giới hạn giá trị kỳ vọng của lỗi xấp xỉ.

Để đơn giản, chúng tôi sử dụng ký hiệu p0
i=Prw0,X[i] như được chi tiết trong Phương trình (2). p0 là xác suất tối ưu để lấy mẫu mặt nạ tại khởi tạo. Trong phân tích của chúng tôi, X có thể được cho hoặc phân phối ngẫu nhiên trong khi trọng số w được giả định là đã biết.

Đầu tiên, chúng tôi trình bày một phiên bản đơn giản hóa của phiên bản được tìm thấy trong bài báo sketching gốc [18] giả định dữ liệu X được cho. Chúng tôi diễn đạt lại lemma để phù hợp với trường hợp nhân vector-ma trận.

**Lemma 4.1.** [Phiên bản đơn giản hóa của [18, Lemma 4]] Giả sử X∈Rd×n,w0∈Rd và s∈Z+ thì sử dụng Thuật toán 1 với p0 cho m thì lỗi là
Em|X[∥XTw0−XT(w0⊙m)∥2]=1s(dXk=1∥X(k)∥|w0k|)2−1s∥XTw0∥2

Lemma giới hạn lỗi của xấp xỉ thưa tại khởi tạo khi dữ liệu được cho. Nó tỷ lệ thuận với 1s, có nghĩa là khi mật độ của mặt nạ tăng thì lỗi giảm, như mong đợi. Kết nối này sẽ nhất quán trong suốt phân tích của chúng tôi. Tiếp theo, chúng tôi thiết lập một giới hạn của lỗi khi X được rút ra từ phân phối chuẩn i.i.d., nơi chúng tôi cũng quan sát hành vi này.

**Lemma 4.2.** Giả sử X∈Rd×n∼1√nN(0, I),w0∈Rd và s∈Z+ thì khi sử dụng Thuật toán 1 với p0 để vẽ m, lỗi có thể được giới hạn như sau
EX[∥XTw0−XT(w0⊙m)∥2]≤1s∥w0∥2.

Lưu ý rằng các giới hạn lỗi trên đúng tại khởi tạo. Tuy nhiên, trong thiết lập cắt tỉa tại khởi tạo, không giống như phép nhân ma trận, chúng tôi không nhằm mục đích áp dụng mặt nạ trên một tham số đã biết w0 mà trên một tham số chưa biết w⋆ được thiết lập sau một thủ tục học. Do đó, chúng tôi giới hạn lỗi xấp xỉ với mặt nạ khi chúng tôi áp dụng nó cho một vector chưa biết w⋆ và dữ liệu phân phối chuẩn.

Trước khi chúng tôi thiết lập kết quả chính của mình, chúng tôi cung cấp một lemma về lỗi, khi mặt nạ được tìm thấy với một vector ban đầu w0 và dữ liệu X̃ nhưng được áp dụng trên dữ liệu đầu vào khác X và vector trọng số w⋆.

**Lemma 4.3.** Giả sử X,X̃∈Rd×n,w0, w⋆∈Rd và s∈Z+ thì khi sử dụng Thuật toán 1 với p0 và X̃ cho m, lỗi có thể được giới hạn như sau
Em[∥XTw⋆−XT(w⋆⊙m)∥2]≤1sdXk=1Pdj=1∥X̃(j)∥|w0j|∥X̃(k)∥|w0k|∥X(k)∥2(w⋆k)2

Tiếp theo, chúng tôi cho thấy kết quả chính về lỗi của mặt nạ cho các tham số chưa biết và dữ liệu ngẫu nhiên.

**Định lý 4.4.** Giả sử X∈Rd×n∼1√nN(0, I),w0, w⋆∈Rd và s∈Z+ thì khi sử dụng Thuật toán 1 với p0 cho m lỗi có thể được giới hạn như sau
EX[∥XTw⋆−XT(w⋆⊙m)∥2]≤1s∥w0∥1(∥w⋆−w0∥2∥w0∥∞+ 2∥w⋆−w0∥1+∥w0∥1).

Để đảm bảo rằng giới hạn của Định lý 4.4 là có ý nghĩa, chúng tôi so sánh nó với một cơ sở đơn giản: Giới hạn lỗi khi mặt nạ được lấy mẫu đồng đều ngẫu nhiên.

**Lemma 4.5.** Giả sử X∈Rd×n∼1√nN(0, I),w0∈Rd và s∈Z+. Thì khi chọn m đồng đều ngẫu nhiên lỗi có thể được giới hạn như sau
EX[∥XTw⋆−XT(w⋆⊙m)∥2]≤ds∥w⋆∥2=ds(∥w⋆−w0∥2+ 2w⋆Tw0)−ds∥w0∥2.

Theo Lemma 4.5, chúng tôi có một yếu tố chiều bổ sung d được tránh trong Định lý 4.4. Lưu ý rằng Định lý 4.4 giới hạn lỗi kỳ vọng chỉ như một hàm của khoảng cách giữa vector cuối cùng w⋆,w0 và các tính chất của khởi tạo. Trong Mục 7, chúng tôi sử dụng tuyên bố rằng DNN dưới các giả định NTK phổ biến không thay đổi nhiều trong quá trình huấn luyện. Do đó, giới hạn lỗi là hợp lý.

## 5 SynFlow và Sketching
Trong mục này chúng tôi phân tích lý thuyết về kết nối giữa SynFlow [61] và sketching trong trường hợp tuyến tính. Chúng tôi tính toán các điểm số được đưa ra bởi SynFlow và xử lý chúng như xác suất.

SynFlow thực hiện một lượt truyền tiến trên mô hình với một vector các số một làm đầu vào và các giá trị tuyệt đối của các tham số được khởi tạo. Các điểm số sau đó được tính toán sau khi cộng tất cả các đặc trưng đầu ra:
RSF=1Tf(1;|w|), (3)
nơi 1 là một vector tất cả các số một và phép nhân với nó dẫn đến cộng tất cả các đầu ra. Để thiết lập kết nối với sketching, chúng tôi phân tích các điểm số SynFlow với ∥X(i)∥ làm đầu vào thay vì 1:
RSF=1Tf(∥X(i)∥;|w|). (4)

Lưu ý rằng trong trường hợp mô hình tuyến tính, nơi w∈Rd, nó đúng rằng
RSF=(∥X(1)∥,∥X(2)∥, ...,∥X(i)∥, ...,∥X(d)∥)T|w|.

Do đó, hóa ra điểm số SynFlow cho mỗi trọng số trong w là:
∂RSF∂|w|i⊙ |w|i=∥X(i)∥|w|i,
điều này mang lại rằng khi nhìn vào các điểm số như một phân phối pi∝ ∥X(i)∥|w|i. Lưu ý rằng trong sketching, xác suất p0i, như được định nghĩa trong Phương trình (2), tỷ lệ thuận với cùng một số hạng và mang lại lỗi xấp xỉ kỳ vọng thấp nhất [18]. Do đó, nó có nghĩa là trong thiết lập đặc trưng tuyến tính, các điểm số SynFlow và xác suất tối ưu cho sketching chia sẻ cùng các tính chất. Do đó, nếu một người sử dụng ngẫu nhiên hóa trên mặt nạ với các điểm số SynFlow như trong Thuật toán 1, chúng ta nhận được thuật toán sketching. Vì chúng tôi đã thiết lập sự tương đương giữa sketching và SynFlow, tất cả các tuyên bố trong Mục 4 đều đúng cho nó trong trường hợp tuyến tính. Lưu ý rằng trong Định lý 4.4, chúng tôi giả định dữ liệu chuẩn ngẫu nhiên làm đầu vào. Theo Phương trình (4), các đầu vào SynFlow là √Σni=11nx2i, nơi xi là chuẩn ngẫu nhiên, điều này ngụ ý một vector phân phối χ cho đầu vào của nó. Để tóm tắt, mối quan hệ với sketching đề xuất rằng chúng ta nên áp dụng SynFlow với lấy mẫu ngẫu nhiên và phân phối χ cho đầu vào của nó. Chúng tôi chứng minh lợi thế của phương pháp này trong Mục 8. Lấy mẫu các phần tử dựa trên p có cùng độ phức tạp tính toán như tìm các trọng số với s điểm số lớn nhất.

## 6 SNIP và Sketching
Trong mục này chúng tôi chuyển sang phân tích một phương pháp nổi tiếng khác của cắt tỉa tại khởi tạo có tên SNIP [39], nhằm mục đích ước tính tầm quan trọng của mỗi trọng số theo độ lớn và độ lớn gradient của nó tại khởi tạo. Độ lớn của gradient được tính toán đối với dữ liệu đầu vào, D. SNIP gán điểm số saliency sau đây của mặt nạ tại chỉ số j:
gj(w;D) =∂L(m⊙w;D)∂mj,
nơi L là hàm mất mát được sử dụng cho huấn luyện và giá trị của mặt nạ trước khi cắt tỉa là 1 trong tất cả các phần tử của nó. Chúng tôi phân tích trường hợp hàm mất mát ℓ1 cho đầu vào X. Trước đây, một biện minh thống kê cho việc sử dụng hàm mất mát ℓ1 cho phân loại DNN đã được chứng minh [34] và đã được chỉ ra thêm rằng việc sử dụng hàm mất mát ℓ1 có thể có lợi cho phân loại robust [26]. Điểm số saliency của trọng số trong mô hình tuyến tính là

gj(w;X, y) =1n∂Σni=1|xTi(w⊙m−yi)|∂mj=1n|wj|ΣnixiSign(xTiw−yi)xij.

Đối với dữ liệu chỉ có một phần tử khác không trong mỗi hàng của X, tức là ∥X(j)∥0= 1, nó đúng rằng xác suất được tạo ra là pj=|wj||xij,s.t.xij̸=0|Σdk=1|wk||xik,s.t.xik̸=0|=|wj|∥X(j)∥Σdk=1|wk|∥X(k)∥. Xác suất được tạo ra trong trường hợp này là tối ưu từ góc độ sketching. Do đó, trong trường hợp không giám sát, chúng ta nên áp dụng SNIP với dữ liệu thưa ngẫu nhiên và lấy mẫu ngẫu nhiên của mặt nạ. Chúng tôi xác thực kết luận này một cách thực nghiệm trong Mục 8.

## 7 Neural Tangent Kernel Pruning
Chúng tôi chuyển sang áp dụng các kết quả sketching của mình cho chế độ NTK [37,32,6], được giới thiệu để xem xét động lực huấn luyện của DNN. Dưới giả định độ rộng vô hạn, các bước gradient descent trên một DNN trở nên có thể theo dõi phân tích và tương tự như học với một kernel đã biết.

Định nghĩa NTK. Chúng tôi sử dụng cùng ký hiệu như xuất hiện trong [37]: θt∈Rd là vector hóa của các tham số tại thời điểm t; ft(x)∈Rm là đầu ra của mô hình tại thời điểm t; (X,Y) là các vector đầu vào và nhãn (chúng tôi giả định rằng Yi∈R); ft(X) là một nối của tất cả các đầu ra; n là khái niệm độ rộng của mô hình; J(θt) =∇θft(X)∈R|X|m×d là Jacobian; Θ̂t=∇θft(X)∇θft(X)T=1nJ(θt)J(θt)T là NTK thực nghiệm. Chúng tôi đề cập đến kernel phân tích như Θ = lim n→∞Θ̂0.

Để các tuyên bố trong [37] đúng, chúng tôi sử dụng các giả định dựa trên bài báo gốc: (i) NTK thực nghiệm hội tụ theo xác suất đến NTK phân tích: Θ̂0→n→∞Θ; (ii) NTK phân tích Θ có hạng đầy đủ. 0< λmin≤λmax<∞ và cho ηcritical = 2(λmin+λmax)−1; (iii) (X,Y) là một tập compact và cho x,x̃∈ X,x̸=x̃; và (iv) Jacobian là locally Lipschitz như được định nghĩa trong Định nghĩa 7.1 (ban đầu được phát biểu trong [37, Lemma 2]):

**Định nghĩa 7.1 (Local Lipschitzness của Jacobian)** .Ký hiệu B(θ0, C) ={θ:∥θ−θ0∥ ≤C}. Jacobian là locally Lipschitz nếu tồn tại K > 0 sao cho cho mọi C >0, với xác suất cao trên khởi tạo ngẫu nhiên, điều sau đây đúng cho θ,θ̃∈B(θ0, C)
∥J(θ)−J(θ̃)∥F≤K∥θ−θ̃∥ và ∥J(θ)∥F≤K.

Lưu ý rằng các hằng số local Lipschitzness thường được xác định bởi các hàm kích hoạt DNN.

NTK Pruning. Chúng tôi tập trung vào xấp xỉ tuyến tính hóa flin_t(x) =∇θtft(x)θt của các đặc trưng mô hình. Chúng tôi nhằm mục đích tìm một mặt nạ để áp dụng trên θt theo các đặc trưng tuyến tính của mạng tại khởi tạo, flin_0(X) và θ0, sử dụng sketching như trong Thuật toán 1. Do đó, chúng tôi có thể sử dụng phân tích trước đây của mình để giới hạn lỗi được tạo ra bởi mặt nạ trên các đặc trưng tại thời điểm t, flin_t(x). Định lý sau đây dựa trên các giới hạn NTK trong [37, Theorem G.4]. Chứng minh định lý của chúng tôi ở Phụ lục B.6.

**Định lý 7.2.** Dưới các giả định [i-iv], δ0>0 và η0≤ηcritical, cho F(A) =Σdi=11∥A(i)∥ và m∈Rd là mặt nạ s-dense được tìm thấy với Thuật toán 1 với p theo flin_0(X) và θ0 (Phương trình (2)). Thì tồn tại R0>0,K > 1 sao cho cho mọi n≥N điều sau đây đúng với xác suất >1−δ0 trên khởi tạo ngẫu nhiên khi áp dụng GD với tốc độ học η0:
Em[∥flin_t(X)− ∇θtft(X)(θt⊙m)∥2]≤1sK3∥θ0∥1F(J(θ0))(∥θ0∥1+F(θ0)9K4R20λ2min+ 6√dK3R0λmin).

Lưu ý rằng đối với mật độ mặt nạ s=O(√d), lỗi của việc sử dụng mặt nạ tại thời điểm t chỉ phụ thuộc vào khởi tạo và các hằng số từ các giả định NTK. Ngoài ra, ∇θtft(X)(θt⊙m) là các đầu ra tuyến tính hóa được đặt mặt nạ của mô hình tại thời điểm t. Chúng tôi chứng minh định lý trên sử dụng giới hạn trên chuẩn Jacobian và tuyên bố rằng dưới các điều kiện của [37, Theorem G.4] được đảm bảo rằng khoảng cách của các tham số tại thời điểm t, θt, từ các tham số tại khởi tạo, θ0, được giới hạn.

## 8 Thí nghiệm
Chúng tôi nghiên cứu những hiểu biết lý thuyết của mình một cách thực nghiệm trên các DNN thưa. Chúng tôi đã thử nghiệm nhiều phương pháp cắt tỉa tại khởi tạo: SynFlow [61], SNIP [39], GraSP [63] và Iterative Magnitude Purning (IMP), thuật toán được đề xuất để tìm vé số chiến thắng [23].

**Bảng 3:** Kết quả với SynFlow và ngẫu nhiên hóa mặt nạ cho Tiny-ImageNet.
Model Density SynFlow SynFlow + random mask
ResNet-18 10% 58.3 60.64
5% 57.63 58.57
2% 54.62 55.56
WideResNet 10% 59.25 60.22
5% 57.49 58.84
2% 54.8 55.54

Chúng tôi sử dụng CIFAR-10/100 [35] và Tiny-ImageNet [66] với VGG-19 [59], ResNet-20 [30] và WideResNet-20-32 [69]. Chúng tôi sử dụng SGD với momentum, kích thước batch 128, và tốc độ học 0.1 nhân với 0.1 sau epoch 80 và 120. Chúng tôi huấn luyện các mô hình trong 160 epoch với weight decay 10−4. Đối với Tiny-ImageNet, chúng tôi sử dụng phiên bản sửa đổi của ResNet-18 và WideResNet-18. Mã của chúng tôi dựa trên các repository của [22,61,60]. Tất cả các thí nghiệm được thực hiện trên một NVIDIA GeForce RTX 2080 Ti duy nhất.

Chúng tôi tìm thấy mặt nạ SynFlow sử dụng 100 vòng lặp. Chúng tôi sử dụng SNIP và GraSP với kích thước batch 256. Đối với IMP, chúng tôi sử dụng 1000 vòng lặp warmup cho VGG-19 và 2000 vòng lặp cho ResNet-20 với CIFAR-10 và 6000 cho CIFAR-100 với ResNet20. Trong các thí nghiệm không giám sát, chúng tôi các ví dụ được rút ra từ phân phối chuẩn với cùng kỳ vọng và độ lệch chuẩn của tập dữ liệu gốc. Tất cả các kết quả bao gồm trung bình và độ lệch chuẩn của 3 seed khác nhau.

Ngẫu nhiên hóa mặt nạ được thực hiện trong ba bước. Đầu tiên là tính toán ngưỡng theo số lượng trọng số còn lại mong muốn. Thứ hai là tính toán số lượng trọng số còn lại trong mỗi lớp đối với hard thresholding. Cuối cùng, bước thứ ba là chọn ngẫu nhiên một mặt nạ cho mỗi lớp theo độ chi tiết được tìm thấy trong bước trước đó và các điểm số của phương pháp cắt tỉa. Chúng tôi ngẫu nhiên hóa mặt nạ theo cách này do các hạn chế tính toán.

Hình 1a cho thấy rằng độ lớn của các trọng số được chọn bởi SynFlow và IMP lớn hơn một lựa chọn ngẫu nhiên đồng đều của các tham số. Lưu ý rằng SynFlow có độ thiên vị mạnh hơn đối với độ lớn lớn hơn so với IMP. Hình 2 trình bày một trường hợp, nơi mặt nạ IMP có chuẩn cực kỳ cao so với các mặt nạ ngẫu nhiên và chuẩn cao được duy trì qua các mức độ thưa. Chúng tôi kiểm tra hiệu ứng của việc ngẫu nhiên hóa mặt nạ trên các điểm số được chọn. Hình 1b trình bày một so sánh histogram của các điểm số của các trọng số được chọn bằng SynFlow có và không có ngẫu nhiên hóa so với phân phối của tất cả các điểm số trong mạng. Chúng tôi báo cáo các điểm số của VGG-19 với CIFAR-10 và để đơn giản chúng tôi trình bày kết quả chỉ trên một tập con 1000 trọng số được chọn đồng đều ngẫu nhiên.

Để cho thấy hiệu ứng của các thay đổi được sử dụng trong phân tích lý thuyết, chúng tôi đã thử nghiệm SynFlow với ngẫu nhiên hóa mặt nạ và thay thế vector tất cả các số một bằng dữ liệu ngẫu nhiên phân phối χ. Chúng tôi tạo ra phân phối χ theo chuẩn ℓ2 của một vector với các biến phân phối chuẩn và chiều n= 128. Chúng tôi ngẫu nhiên hóa dữ liệu tại mỗi vòng lặp cắt tỉa. Để xác thực thực nghiệm kết luận trong Mục 6 cho mối quan hệ giữa SNIP và sketching, chúng tôi đã thử nghiệm SNIP với dữ liệu thưa ngẫu nhiên. Chúng tôi chọn ngẫu nhiên cho mỗi vị trí pixel đầu vào, một hình ảnh duy nhất trong batch nơi pixel là khác không. Lưu ý rằng các hình ảnh được rút ra từ phân phối chuẩn và không phải là hình ảnh tự nhiên.

Bảng 1 cho thấy rằng việc thay thế thresholding bằng một mặt nạ ngẫu nhiên và thay thế đầu vào bằng phân phối được rút ra từ phân tích của chúng tôi cải thiện kết quả trong hầu hết các trường hợp khi so sánh với SynFlow gốc và SNIP độc lập với dữ liệu ngây thơ. Đối với SynFlow, việc sử dụng ngẫu nhiên hóa mặt nạ dẫn đến hiệu suất được cải thiện. Sự kết hợp của dữ liệu thưa và ngẫu nhiên hóa mặt nạ dẫn đến độ chính xác vượt trội cho SNIP. Nó cho thấy rằng lấy mẫu mặt nạ và dữ liệu của chúng tôi có thể được sử dụng và cải thiện các phương pháp cắt tỉa khác dựa trên nó, ví dụ [2,17]. Kết quả bổ sung với SNIP có giám sát gốc được tìm thấy trong Phụ lục A. Hầu hết các kết quả độ chính xác với mặt nạ ngẫu nhiên và dữ liệu thưa vượt trội hơn SNIP ngay cả với đầu vào có nhãn.

Tuyên bố của chúng tôi rằng việc tìm một mạng con thưa tốt tại khởi tạo không nhất thiết phụ thuộc vào dữ liệu được kiểm tra bằng một thí nghiệm nơi các mặt nạ được học với đầu vào hoàn toàn ngẫu nhiên và sau đó được huấn luyện lại với dữ liệu có giám sát thực. Bảng 2 báo cáo kết quả độ chính xác với CIFAR-10/100. Chúng ta có thể thấy rằng các mặt nạ ngẫu nhiên hóa tạo ra kết quả tương đương và thường tốt hơn so với các phương pháp cắt tỉa với dữ liệu ngẫu nhiên. Ngoài ra, có thể thấy rằng, thực sự, hiệu suất của các phương pháp cắt tỉa phụ thuộc vào dữ liệu chỉ được giải thích một phần bởi việc sử dụng dữ liệu đầu vào có giám sát, như được chỉ ra trong [60]. Lưu ý rằng dự kiến sẽ thấy một số suy giảm về độ chính xác vì những phương pháp này được thiết kế để hoạt động với dữ liệu có nhãn.

Đối với huấn luyện với Tiny-ImageNet, sự cải thiện trong hiệu suất là rõ ràng với ngẫu nhiên hóa mặt nạ được lấy cảm hứng từ sketching, xem Bảng 3. Sự cải thiện về độ chính xác là khoảng 1% cho tất cả các mức độ thưa và kiến trúc được thử nghiệm. Nhìn chung, ngẫu nhiên hóa các mặt nạ dẫn đến một state-of-the-art mới với SynFlow.

## 9 Kết luận và Nghiên cứu Tương lai
Công trình này trình bày và phân tích nhiệm vụ cắt tỉa tại khởi tạo từ góc độ sketching. Dựa trên nghiên cứu lý thuyết của chúng tôi, chúng tôi có được một biện minh mới cho tuyên bố rằng các mạng con thưa tốt độc lập với dữ liệu. Ngoài ra, chúng tôi áp dụng các ý tưởng từ sketching cho DNN. Các thay đổi được đề xuất của chúng tôi có thể được thêm vào các phương pháp cắt tỉa hiện tại và tương lai.

Trong khi chúng tôi đã đề xuất một kết nối mới giữa cắt tỉa tại khởi tạo và sketching, khung làm việc của chúng tôi có một số hạn chế. Chúng tôi áp dụng phương pháp của mình, chỉ trong thiết lập cắt tỉa tại khởi tạo. Người ta có thể điều tra phương pháp được đề xuất của chúng tôi cho các thiết lập khác của cắt tỉa sau hoặc trong quá trình huấn luyện và các phương pháp cắt tỉa có cấu trúc. Ngoài ra, chúng tôi chỉ nghiên cứu một phương pháp sketching. Chúng tôi để lại cho nghiên cứu tương lai việc khai thác các phương pháp sketching phức tạp hơn, có thể dẫn đến những hiểu biết và cải tiến mới cho việc cắt tỉa DNN. Ngoài ra, nghiên cứu tương lai có thể được thực hiện để tổng quát hóa khung đặc trưng tuyến tính khá đơn giản thành một mô hình sâu hơn nơi không gian tìm kiếm cho mặt nạ thậm chí lớn hơn. Không gian tìm kiếm trong trường hợp này có thể là mơ hồ, tức là hai mặt nạ khác nhau có thể dẫn đến cùng đầu ra [72]. Chúng tôi tin rằng mối quan hệ chúng tôi vẽ ra ở đây có thể được sử dụng trong nhiều hướng khác ngoài những hướng được chỉ ra ở trên và đóng góp vào hiểu biết về DNN và cắt tỉa.

## Lời cảm ơn
Công trình này được hỗ trợ bởi hội đồng nghiên cứu châu Âu dưới Grant ERC-StG 757497 và bởi Trung tâm AI và Khoa học Dữ liệu Đại học Tel Aviv (TAD).

## Tài liệu tham khảo
[1]Mohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas D Lane. Zero-cost proxies for lightweight nas. arXiv preprint arXiv:2101.08134 , 2021.

[2]Milad Alizadeh, Shyam A Tailor, Luisa M Zintgraf, Joost van Amersfoort, Sebastian Farquhar, Nicholas Donald Lane, and Yarin Gal. Prospect pruning: Finding trainable weights at initialization using meta-gradients. arXiv preprint arXiv:2202.08132 , 2022.

[3]Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations , 2019.

[4]Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning , pages 244–253. PMLR, 2018.

[5]Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In Advances in Neural Information Processing Systems (NeurIPS) , pages 7413– 7424, 2019.

[6]Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning , pages 322–332. PMLR, 2019.

[7]Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In International Conference on Machine Learning , pages 254–263. PMLR, 2018.

[8]Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations , 2018.

[9]Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. In International Conference on Learning Representations , 2018.

[10]Rebekka Burkholz, Nilanjana Laha, Rajarshi Mukherjee, and Alkis Gotovos. On the existence of universal lottery tickets. In International Conference on Learning Representations , 2021.

[11]Miguel A Carreira-Perpinán and Yerlan Idelbayev. "learning-compression" algorithms for neural netpruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8532–8541, 2018.

[12]Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. Advances in neural information processing systems , 1, 1988.

[13]Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models. In CVPR, 2021.

[14]Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances in Neural Information Processing Systems , 34:19637–19651, 2021.

[15]Tianyi Chen, Luming Liang, Tianyu DING, Zhihui Zhu, and Ilya Zharkov. OTOv2: Automatic, generic, user-friendly. In The Eleventh International Conference on Learning Representations , 2023.

[16]Daiki Chijiwa, Shin'ya Yamaguchi, Yasutoshi Ida, Kenji Umakoshi, and Tomohiro Inoue. Pruning randomly initialized neural networks with iterative randomization. arXiv:2106.09269 , 2021.

[17]Pau de Jorge, Amartya Sanyal, Harkirat S Behl, Philip HS Torr, Gregory Rogez, and Puneet K Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081 , 2020.

[18]Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing , 36(1):132–157, 2006.

[19]Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In ICML. PMLR, 2020.

[20]Jonas Fischer and Rebekka Burkholz. Towards strong pruning for lottery tickets with non-zero biases.arXiv preprint arXiv:2110.11150 , 2021.

[21]Jonas Fischer and Rebekka Burkholz. Plant'n'seek: Can you find the winning ticket? In International Conference on Learning Representations , 2022.

[22] Jonathan Frankle. Openlth: A framework for lottery tickets and beyond, 2020.

[23]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.

[24]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In ICML, pages 3259–3269. PMLR, 2020.

[25]Thomas Gebhart, Udit Saxena, and Paul Schrater. A unified paths perspective for pruning at initialization. arXiv preprint arXiv:2101.10552 , 2021.

[26]Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence , 2017.

[27]Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. Advances in neural information processing systems , 29, 2016.

[28]Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems , 28, 2015.

[29]Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks , pages 293–299. IEEE, 1993.

[30]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.

[31]Nathan Hubens, Matei Mancas, Bernard Gosselin, Marius Preda, and Titus Zaharia. One-cycle pruning: Pruning convnets under a tight training budget. arXiv:2107.02086 , 2021.

[32]Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems , 31, 2018.

[33]Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference . BMVA Press, 2014.

[34]Katarzyna Janocha and Wojciech Marian Czarnecki. On loss functions for deep neural networks in classification. arXiv preprint arXiv:1702.05659 , 2017.

[35] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

[36]Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems , 2, 1989.

[37]Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems , 32, 2019.

[38]Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip H. S. Torr. A signal propagation perspective for pruning neural networks at initialization. In ICLR, 2020.

[39]Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.

[40]Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In International Conference on Learning Representations , 2021.

[41]Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent transfer. In International Conference on Machine Learning , pages 6336–6347. PMLR, 2020.

[42]Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l_0 regularization. In International Conference on Learning Representations , 2018.

[43]Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. In ICML. PMLR, 2020.

[44]Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without training. In International Conference on Machine Learning , pages 7588–7598. PMLR, 2021.

[45]Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications , 9(1):2383, 2018.

[46]Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440 , 2016.

[47]Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. In NeurIPS , 2019.

[48]Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In International Conference on Machine Learning , pages 4646–4655. PMLR, 2019.

[49]Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. Advances in neural information processing systems , 1, 1988.

[50]Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear diagonal neural networks. In International Conference on Machine Learning , volume 162, pages 16270–16295, 17–23 Jul 2022.

[51]James O' Neill. An overview of neural network compression. arXiv preprint arXiv:2006.03669 , 2020.

[52]Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing neural networks. Advances in neural information processing systems , 28, 2015.

[53]Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic pruning is all you need. Advances in Neural Information Processing Systems , 33:2925–2934, 2020.

[54]Shreyas Malakarjun Patil and Constantine Dovrolis. Phew: Constructing sparse networks that learn fast and generalize well without training data. In International Conference on Machine Learning , pages 8432–8442. PMLR, 2021.

[55]Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient. arXiv:2006.07990 , 2020.

[56]Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What's hidden in a randomly weighted neural network? In CVPR, pages 11893–11902, 2020.

[57]Matthia Sabatelli, Mike Kestemont, and Pierre Geurts. On the transferability of winning tickets in non-natural image datasets. arXiv:2005.05232 , 2020.

[58]Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. In Conference on Learning Theory , volume 99, pages 2691–2713, 25–28 Jun 2019.

[59]Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014.

[60]Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. arXiv:2009.11094 , 2020.

[61]Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in Neural Information Processing Systems , 33:6377–6389, 2020.

[62]Joost van Amersfoort, Milad Alizadeh, Sebastian Farquhar, Nicholas Lane, and Yarin Gal. Single shot structured pruning before training. arXiv preprint arXiv:2007.00389 , 2020.

[63]Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376 , 2020.

[64]Colin White, Arber Zela, Robin Ru, Yang Liu, and Frank Hutter. How powerful are performance predictors in neural architecture search? Advances in Neural Information Processing Systems , 34:28454–28469, 2021.

[65]Cameron R Wolfe, Qihan Wang, Junhyung Lyle Kim, and Anastasios Kyrillidis. Provably efficient lottery ticket discovery. arXiv:2108.00259 , 2021.

[66] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny imagenet challenge. Technical report , 2017.

[67]Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv:1909.11957 , 2019.

[68]Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural networks. In International Conference on Learning Representations , 2021.

[69]Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 , 2016.

[70]Zeru Zhang, Jiayin Jin, Zijie Zhang, Yang Zhou, Xin Zhao, Jiaxiang Ren, Ji Liu, Lingfei Wu, Ruoming Jin, and Dejing Dou. Validating the lottery ticket hypothesis with inertial manifold theory.Advances in Neural Information Processing Systems , 34:30196–30210, 2021.

[71]Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang. Efficient lottery ticket finding: Less data is more. In ICML, pages 12380–12390. PMLR, 2021.

[72]Léon Zheng, Elisa Riccietti, and Rémi Gribonval. Identifiability in two-layer sparse matrix factorization. complexity , 20(19):28–29, 2021.

## A Kết quả với SNIP có Giám sát

**Bảng 4:** Kết quả độ chính xác với SNIP có giám sát gốc và SNIP không giám sát với dữ liệu thưa và ngẫu nhiên hóa mặt nạ. Phía trên là kết quả với CIFAR-10 và phía dưới là kết quả với CIFAR-100.

Model VGG-19 ResNet20
Density 10% 5% 2% 10% 5% 2%
SNIP (supervised) 92.62±0.09 91.66 ±0.25 90.92 ±0.16 86.29±0.92 82.55 ±1.02 78.16 ±0.8
SNIP + sparse data + rand. mask 93.16±0.36 92.79 ±0.78 92.03 ±0.9187.02±0.65 83.72 ±0.73 79.39 ±0.49
SNIP (supervised) 71.80±0.66 71.07 ±0.2356.05±13.97 67.03±0.04 61.80 ±0.38 49.42 ±0.65
SNIP + sparse data + rand. mask 72.49±0.33 71.51 ±0.45 53.05±15.3867.52±0.10 62.68 ±0.32 51.76 ±0.36

## B Chứng minh cho Sketching và SynFlow
Trong mục này chúng tôi bao gồm các chứng minh của các tuyên bố được trình bày trong bài báo chính về SynFlow và sketching.

### B.1 Các Lemma Hữu ích và Chứng minh
Đầu tiên chúng tôi trình bày các lemma và chứng minh hữu ích được sử dụng để chứng minh các lemma và định lý trong bài báo.

Chúng tôi tính toán kỳ vọng của mặt nạ với một số vector w⋆ chưa biết.

**Lemma B.1.** Cho m được tìm thấy với Thuật toán 1 và p0. Thì cho w⋆∈Rd nó đúng,
E[(XT(w⋆⊙m))i] = (XTw⋆)i.

**Chứng minh Lemma B.1.** Cố định một chỉ số i và ký hiệu biến ngẫu nhiên Y0⋆t=Xitiw⋆itsp0it (it là ngẫu nhiên và do đó p0it là ngẫu nhiên), nó đúng rằng Σst=1Y0⋆t=Σdt=1Xitiw⋆it·1sp0it= (XT(w⋆⊙m))i.

Eit[Y0⋆t]=Σdk=1p0kXkiw⋆ksp0k=1s(XT(i)w⋆) =1s(XTw⋆)i.

Tổng trên s biến ngẫu nhiên chúng ta có
Eit[(XT(w⋆⊙m))i]=Σst=1E[Y0⋆t]= (XTw⋆)i.

Chúng tôi tính toán phương sai của mỗi phần tử mặt nạ khi mặt nạ được áp dụng trên w⋆.

**Lemma B.2.** Cho m được tìm thấy với Thuật toán 1 với p0. Thì cho w⋆∈Rd nó đúng,
Var[XT(w⋆⊙m)i]=1sΣdk=1(Xki)2(w⋆k)2p0k−1s(XTw⋆)2i

**Chứng minh Lemma B.2.** Theo phương sai của các biến độc lập
Var[XT(w⋆⊙m)i]=Σst=1Var[Y0⋆t]=Σst=1E[(Y0⋆t)2]−E[Y0⋆t]2.

Sau đó chúng tôi tập trung vào tính toán E[(Y0⋆t)2],
E[(Y0⋆t)2] =Σdk=1(Xki)2(w⋆k)2s2p0k.

Sau đó việc sử dụng Lemma B.2 kết thúc chứng minh.

### B.2 Chứng minh của Lemma 4.3
Lemma sau đây hỗ trợ Định lý 4.4 và Định lý 7.2. Lemma này thiết lập phương sai của mỗi phần tử được đặt mặt nạ khi mặt nạ được tìm thấy với w0 và X̃ nhưng được áp dụng trên dữ liệu đầu vào và vector trọng số khác.

**Lemma 4.3.** Giả sử X,X̃∈Rd×n,w0, w⋆∈Rd và s∈Z+ thì khi sử dụng Thuật toán 1 với p0 và X̃ cho m lỗi có thể được giới hạn
Em[∥XTw⋆−XT(w⋆⊙m)∥2]=1sΣdk=11p0k∥X(k)∥2(w⋆k)2−1s∥XTw⋆∥2
≤1sΣdk=1Σdj=1∥X̃(j)∥|w0j|∥X̃(k)∥|w0k|∥X(k)∥2(w⋆k)2

**Chứng minh.** m được rút ra i.i.d.
Em[∥XTw⋆−XT(w⋆⊙m)∥2]=Σni=1Em[(XTw⋆−XT(w⋆⊙m))2i]
=Σni=1(XTw⋆)2i−2(XTw⋆)iEm[(XT(w⋆⊙m))i]+Em[(XT(w⋆⊙m))2i]

Lemma B. 1=Σni=1Varm[(XT(w⋆⊙m))i]

Lemma B. 2=1sΣdk=11p0k(Σni=1X2ki)(w⋆k)2−1s∥XTw⋆∥2
≤1sΣdk=11p0k(Σni=1X2ki)(w⋆k)2

định nghĩa p0=1sΣdk=1Σdj=1∥X̃(j)∥|w0j|∥X̃(k)∥|w0k|∥X(k)∥2(w⋆k)2

Bây giờ, chúng tôi có thể lặp lại các lemma và định lý từ bài báo chính và trình bày chứng minh của chúng.

### B.3 Chứng minh của Lemma 4.2

**Lemma 4.2.** Giả sử X∈Rd×n∼1√nN(0, I),w0∈Rd và s∈Z+ thì khi sử dụng Thuật toán 1 với p0 cho m lỗi có thể được giới hạn
EX[∥XTw0−XT(w0⊙m)∥2]≤1s∥w0∥2

**Chứng minh.**
EX[∥XTw0−XT(w0⊙m)∥2]=EX[Em|X[∥XTw0−XT(w0⊙m)∥2]]

Lemma 4.1= EX[1s(Σdk=1∥X(k)∥|w0k|)2−1s∥XTw0∥2]
≤EX[1s(Σdk=1∥X(k)∥|w0k|)2]
≤1sEX[Σdk=1∥X(k)∥2(w0k)2]
=1sΣdk=1EX[∥X(k)∥2](w0k)2=nsn∥w0∥2

### B.4 Chứng minh của Định lý 4.4

**Định lý 4.4.** Giả sử X∈Rd×n∼1√nN(0, I),w0, w⋆∈Rd và s∈Z+ thì khi sử dụng Thuật toán 1 với p0 cho m lỗi có thể được giới hạn
EX[∥XTw⋆−XT(w⋆⊙m)∥2]≤1s∥w0∥1(∥w⋆−w0∥2∥w0∥∞+ 2∥w⋆−w0∥1+∥w0∥1).

**Chứng minh.** Chúng tôi tính toán kỳ vọng trên X. Vì các hàng X là i.i.d. chúng tôi thay thế E∥X(k)∥ trong E∥X(.)∥ vì đối với tất cả k chuẩn là như nhau.

EX[∥XTw⋆−XT(w⋆⊙m)∥2]

Lemma 4.3= EX[1sΣdk=1Σdj=1∥X(j)∥|w0j||w0k|∥X(k)∥(w⋆k)2]
=1sΣdk=1(w⋆k)2EX[∥X(k)∥2]+1|w0k|Σj≠k|w0j|EX[∥X(k)∥]EX[∥X(j)∥]
=1sΣdk=1EX[∥X(.)∥2](w⋆k)2+EX[∥X(.)∥]2(w⋆k)2|w0k|Σj≠k|w0j|

Lưu ý rằng theo phân phối X,
EX[∥X(.)∥2]=E[Σni=11nx2i]= 1,
EX[∥X(.)∥]2=1nE[√Σni=1x2i]2=2nΓ((n+ 1)/2)2Γ(n/2)2≤1.

Nơi Γ(·) là hàm gamma.

Tính toán kết quả như một hàm của ∥w⋆−w0∥:
1sΣdk=1(w⋆k)2+(w⋆k)2|w0k|Σj≠k|w0j|=1sΣdk=1(w⋆k)2|w0k|Σdj=1|w0j|
=1s∥w0∥1Σdk=1(w⋆k)2|w0k|
=1s∥w0∥1Σdk=1(w⋆k−w0k)2|w0k|+ 2sign(w0k)(w⋆k)− |w0k|
≤1s∥w0∥1(∥w⋆−w0∥2∥w0∥∞+ 2∥w⋆∥1− ∥w0∥1)
≤1s∥w0∥1(∥w⋆−w0∥2∥w0∥∞+ 2∥w⋆−w0∥1+∥w0∥1)

Nơi bất đẳng thức cuối cùng đúng từ bất đẳng thức tam giác ngược và phép cộng và trừ của ∥w0∥1.

### B.5 Chứng minh của Lemma 4.5

**Lemma 4.5.** Giả sử X∈Rd×n∼1√nN(0, I),w0∈Rd và s∈Z+, thì khi chọn m đồng đều ngẫu nhiên lỗi có thể được giới hạn
EX[∥XTw⋆−XT(w⋆⊙m)∥2]≤ds∥w⋆∥2=ds(∥w⋆−w0∥2+ 2w⋆Tw0)−ds∥w0∥2.

**Chứng minh.** Chứng minh này có cùng dạng như Định lý 4.4 và chúng tôi bắt đầu bằng việc thiết lập sự tương đương với Lemma B.1 và B.2. Để duy trì ước tính của các đặc trưng, khi mi̸= 0 nó đúng rằng mi=ds (như được phát biểu trong Thuật toán 1, mi=1spi).

Em[(XT(w⋆⊙m)i]=Σst=11d(XTw⋆)ids= (XTw⋆)i

Tiếp theo chúng tôi phân tích phương sai Varm[(XT(w⋆⊙m))i]. Cố định một chỉ số i và cho YUni⋆t =Xitiw⋆itspUnit=dXitiw⋆its. Chúng tôi nhìn vào

Var[XT(w⋆⊙m)i]=Σst=1Var[YUni⋆t]=Σst=1E[(YUni⋆t)2]−E[YUni⋆t]2.

Em[YUni⋆t]2]=Σdk=1X2ki(w⋆k)2sds

Nhìn chung chúng tôi có thể kết luận rằng
Varm[(XT(w⋆⊙m))i]=dsΣdk=1X2ki(w⋆k)2−1s(XTw⋆)2i.

Cuối cùng chúng tôi tính toán kỳ vọng trên X và chúng ta có
EX,m[∥XTw⋆−XT(w⋆⊙m)∥2]≤ds∥w⋆∥2=ds(∥w⋆−w0∥2+ 2w⋆Tw0)−ds∥w0∥2

### B.6 Chứng minh của Định lý 7.2

**Định lý 7.2.** Dưới các giả định [1-4], δ0>0 và η0≤ηcritical. Cho m∈Rd là mặt nạ s-dense được tìm thấy với Thuật toán 1 với p theo flint0(X) và θ0 (Phương trình (2)). Thì tồn tại R0>0,K > 1 sao cho cho mọi n≥N điều sau đây đúng với xác suất >1−δ0 trên khởi tạo ngẫu nhiên khi áp dụng GD với η0
Em[∥flint(X)− ∇θtft(X)(θt⊙m)∥2]≤1sK3∥θ0∥1F(J(θ0))·(∥θ0∥1+F(θ0)9K4R20λ2min+ 6√dK3R0λmin),
nơi F(A) =Σdi=11∥A(i)∥.

**Chứng minh.** Chúng tôi bắt đầu với việc tính toán lỗi xấp xỉ kỳ vọng của các đặc trưng tại thời điểm t sử dụng Lemma 4.3. Lưu ý rằng chúng tôi sử dụng jacobian tại khởi tạo J(θ0) và các tham số ban đầu θ0 để tìm mặt nạ m.

Em[∥flint(X)− ∇θtft(X)(θt⊙m)∥2]
=Em[∥J(θt)θt−J(θt)(θt⊙m)∥2]
≤1sΣdk=1∥J(θt)(k)∥2∥J(θ0)(k)∥θ2tk|θ0k|Σdj=1∥J(θ0)(j)∥|θ0j|
≤1s(Σdj=1∥J(θ0)(j)∥|θ0j|)(Σdk=1∥J(θt)(k)∥2∥J(θ0)(k)∥)(Σdk=1θ2tk|θ0k|)

Tiếp theo chúng tôi giới hạn từng số hạng.
Σdk=1θ2tk|θ0k|=Σdk=1(θtk−θ0k)2|θ0k|+ 2θtksign(θ0k)− |θ0k|
≤ (Σdk=11|θ0k|)∥θt−θ0∥2+ 2∥θt∥1− ∥θ0∥1
≤ (Σdk=11|θ0k|)∥θt−θ0∥2+ 2∥θt−θ0∥1+∥θ0∥1

Nơi chuyển đổi cuối cùng được thực hiện chúng tôi sử dụng giới hạn trong Định lý G.4 và ∥a∥1≤√d∥a∥2 chúng ta có,
Σdk=1θ2tk|θ0k|≤ ∥θ0∥1+ (Σdk=11|θ0k|)3KR0λmin2+ 6√dKR0λmin

Chúng tôi giới hạn số hạng tiếp theo,
(Σdj=1∥J(θ0)(j)∥|θ0j|)(Σdk=1∥J(θt)(k)∥2∥J(θ0)(k)∥)
≤(Σdj=1∥J(θ0)(j)∥|θ0j|)(Σdk=11∥J(θ0)(k)∥)(Σdk=1∥J(θt)(k)∥2)
=(Σdj=1∥J(θ0)(j)∥|θ0j|)(Σdk=11∥J(θ0)(k)∥)∥J(θt)∥2F
≤(Σdj=1∥J(θ0)(j)∥|θ0j|)(Σdk=11∥J(θ0)(k)∥)K2

Chúng tôi cũng có thể giới hạn,
(Σdj=1∥J(θ0)(j)∥|θ0j|)≤Σdj=1∥J(θ0)∥F|θ0j| ≤K∥θ0∥1

Nhìn chung chúng tôi có thể giới hạn lỗi với trạng thái của mô hình tại khởi tạo
Em[∥J(θt)θt−J(θt)(θt⊙m)∥2]
≤1sK3∥θ0∥1(Σdk=11∥J(θ0)(k)∥)(∥θ0∥1+ (Σdk=11|θ0k|)3K2R0λmin2+ 6√dK3R0λmin)
≤1sK3∥θ0∥1F(J(θ0))(∥θ0∥1+F(θ0)9K4R20λ2min+ 6√dK3R0λmin)

Nơi F(A) =Σdi=11∥A(i)∥.
