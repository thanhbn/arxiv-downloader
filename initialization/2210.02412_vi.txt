# Tại sao Tỉa thưa Ngẫu nhiên là Tất cả Những gì Chúng ta Cần để Bắt đầu Thưa
Advait Gadhikar1Sohom Mukherjee1Rebekka Burkholz1

Tóm tắt
Mặt nạ ngẫu nhiên định nghĩa các mô hình mạng nơ-ron thưa một cách hiệu quả đáng ngạc nhiên, như đã được chứng minh thực nghiệm. Các mạng thưa kết quả thường có thể cạnh tranh với các kiến trúc dày đặc và các thuật toán tỉa vé số may mắn tiên tiến, mặc dù chúng không dựa vào các vòng lặp tỉa-huấn luyện tốn kém về mặt tính toán và có thể được rút ra ban đầu mà không có chi phí tính toán đáng kể. Chúng tôi đưa ra lời giải thích lý thuyết về cách mặt nạ ngẫu nhiên có thể xấp xỉ các mạng mục tiêu tùy ý nếu chúng rộng hơn một yếu tố logarit trong nghịch đảo độ thưa 1/log(1/sparsity). Yếu tố tham số hóa quá mức này là cần thiết ít nhất đối với các mạng ngẫu nhiên 3 lớp, điều này làm sáng tỏ hiệu suất giảm dần quan sát được của các mạng ngẫu nhiên ở độ thưa cao hơn. Tuy nhiên, ở mức độ thưa vừa phải đến cao, kết quả của chúng tôi hàm ý rằng các mạng thưa hơn được chứa trong các mạng nguồn ngẫu nhiên để bất kỳ sơ đồ huấn luyện dày đặc đến thưa nào cũng có thể được chuyển thành một sơ đồ hiệu quả hơn về mặt tính toán từ thưa đến thưa bằng cách hạn chế việc tìm kiếm vào một mặt nạ ngẫu nhiên cố định. Chúng tôi chứng minh tính khả thi của phương pháp này trong các thí nghiệm cho các phương pháp tỉa khác nhau và đề xuất các lựa chọn đặc biệt hiệu quả về tỷ lệ thưa theo lớp ban đầu của mạng nguồn ngẫu nhiên. Như một trường hợp đặc biệt, chúng tôi cũng chỉ ra về mặt lý thuyết và thực nghiệm rằng các mạng nguồn ngẫu nhiên cũng chứa các vé số may mắn mạnh. Mã của chúng tôi có sẵn tại https://github.com/RelationalML/sparse_to_sparse.

1. Giới thiệu
Những đột phá ấn tượng được đạt được bởi học sâu phần lớn được quy cho việc tham số hóa quá mức rộng rãi của các mạng nơ-ron sâu, vì nó dường như có nhiều lợi ích cho khả năng biểu diễn và tối ưu hóa của chúng (Belkin và cộng sự, 2019). Tuy nhiên, xu hướng kết quả hướng tới các mô hình và tập dữ liệu ngày càng lớn hơn áp đặt chi phí tính toán và năng lượng ngày càng tăng mà khó có thể đáp ứng. Điều này đặt ra câu hỏi: Liệu mức độ tham số hóa quá mức cao này có thực sự cần thiết không?

Huấn luyện các kiến trúc mạng nơ-ron sâu quy mô nhỏ hoặc thưa tổng quát từ đầu vẫn là một thách thức đối với các sơ đồ khởi tạo tiêu chuẩn (Li và cộng sự, 2016; Han và cộng sự, 2015). Tuy nhiên, (Frankle & Carbin, 2019) gần đây đã chứng minh rằng tồn tại các kiến trúc thưa có thể được huấn luyện để giải quyết các vấn đề điểm chuẩn tiêu chuẩn một cách cạnh tranh. Theo Giả thuyết Vé số May mắn (LTH) của họ, các mạng khởi tạo ngẫu nhiên dày đặc chứa các mạng con có thể được huấn luyện độc lập đến độ chính xác kiểm tra tương đương với mạng dày đặc gốc. Các mạng con như vậy, các vé số may mắn (LTs), kể từ đó đã được thu được bằng các thuật toán tỉa yêu cầu các vòng lặp tỉa-huấn luyện lại tốn kém về mặt tính toán (Frankle & Carbin, 2019; Tanaka và cộng sự, 2020) hoặc các quy trình học mặt nạ (Savarese và cộng sự, 2020; Sreenivasan và cộng sự, 2022b). Trong khi những điều này có thể dẫn đến lợi ích tính toán trong thời gian huấn luyện và suy luận và giảm yêu cầu bộ nhớ (Hassibi và cộng sự, 1993; Han và cộng sự, 2015), mục tiêu thực sự vẫn là xác định các kiến trúc thưa có thể huấn luyện được trước khi huấn luyện, vì điều này có thể dẫn đến tiết kiệm tính toán đáng kể. Tuy nhiên, các phương pháp tỉa tại thời điểm khởi tạo đương thời (Lee và cộng sự, 2018; Wang và cộng sự, 2020; Tanaka và cộng sự, 2020; Fischer & Burkholz, 2022; Frankle và cộng sự, 2021) đạt được hiệu suất kém cạnh tranh hơn. Vì lý do đó, điều đáng chú ý là ngay cả các phương pháp lặp đi lặp lại tiên tiến cũng khó có thể vượt trội hơn một giải pháp thay thế đơn giản, rẻ về mặt tính toán và độc lập với dữ liệu: tỉa ngẫu nhiên tại thời điểm khởi tạo (Su và cộng sự, 2020). Liu và cộng sự (2021) đã cung cấp bằng chứng thực nghiệm có hệ thống cho hiệu quả 'bất hợp lý' của nó trong nhiều thiết lập, bao gồm các kiến trúc và dữ liệu phức tạp, quy mô lớn.

Chúng tôi giải thích về mặt lý thuyết tại sao chúng có thể hiệu quả bằng cách chứng minh rằng một mạng được che mặt ngẫu nhiên có thể xấp xỉ một mạng mục tiêu tùy ý nếu nó rộng hơn một yếu tố logarit trong độ thưa của nó 1/log(1/sparsity). Bằng cách suy ra một cận dưới về độ rộng yêu cầu của một mạng ER 1 lớp ẩn ngẫu nhiên, chúng tôi tiếp tục chỉ ra rằng mức độ tham số hóa quá mức này là cần thiết nói chung. Điều này hàm ý rằng các mạng ngẫu nhiên thưa có thuộc tính xấp xỉ hàm phổ quát như các mạng dày đặc và ít nhất cũng có khả năng biểu đạt như các mạng mục tiêu tiềm năng. Tuy nhiên, nó cũng làm nổi bật các hạn chế của việc tỉa ngẫu nhiên trong trường hợp độ thưa cực cao, vì yêu cầu độ rộng sau đó tỷ lệ xấp xỉ như 1/log(1/sparsity) ≈ 1/(1-sparsity) (xem thêm Hình 2 cho một ví dụ). Trong thực tế, chúng tôi quan sát sự suy giảm tương tự về hiệu suất đối với các mức độ thưa cao.

Ngay cả đối với độ thưa vừa phải đến cao, tính ngẫu nhiên của các kết nối dẫn đến một số lượng đáng kể các trọng số dư thừa không cần thiết cho việc biểu diễn của một mạng mục tiêu. Hiểu biết này cho thấy rằng, một mặt, việc tỉa bổ sung có thể tăng cường thêm độ thưa của cấu trúc mạng nơ-ron kết quả, vì các mặt nạ ngẫu nhiên có thể không thưa một cách tối ưu. Mặt khác, bất kỳ phương pháp huấn luyện dày đặc đến thưa nào cũng sẽ không cần phải bắt đầu từ một mạng dày đặc mà cũng có thể bắt đầu huấn luyện từ một mạng ngẫu nhiên thưa hơn và do đó được chuyển thành một phương pháp học thưa đến thưa. Ý tưởng chính được minh họa trong Hình 1 và được xác minh trong các thí nghiệm mở rộng với các phương pháp tỉa vé số may mắn và thưa hóa liên tục khác nhau. Kết quả chính của chúng tôi cũng có thể được hiểu là lời biện minh lý thuyết cho Huấn luyện Thưa Động (DST) (Evci và cộng sự, 2020; Liu và cộng sự, 2021; Bellec và cộng sự, 2018), điều này tỉa các mạng ngẫu nhiên có độ thưa vừa phải. Tuy nhiên, nó tiếp tục dựa vào các bước nối lại cạnh đôi khi yêu cầu tính toán gradient của mạng dày đặc tương ứng (Evci và cộng sự, 2020). Các hạn chế suy ra của chúng tôi về việc tỉa ngẫu nhiên chỉ ra rằng việc nối lại này có thể cần thiết ở độ thưa cực cao nhưng có thể không cần thiết cho các điểm khởi đầu ngẫu nhiên thưa vừa phải, như chúng tôi cũng nhấn mạnh trong các thí nghiệm bổ sung.

Huấn luyện thưa với các mạng được che mặt ngẫu nhiên (ER): Một biểu diễn trực quan của hàm ý chính của lý thuyết chúng tôi - huấn luyện thưa đến thưa có thể hiệu quả bằng cách bắt đầu từ một mạng được che mặt ngẫu nhiên (ER).

Như một trường hợp đặc biệt của ý tưởng chính để tỉa các mạng ngẫu nhiên, chúng tôi cũng xem xét các vé số may mắn mạnh (SLTs) (Zhou và cộng sự, 2019; Ramanujan và cộng sự, 2020). Đây là các mạng con của các mạng nguồn lớn, khởi tạo ngẫu nhiên, không yêu cầu bất kỳ huấn luyện thêm nào sau khi tỉa. Các bằng chứng tồn tại lý thuyết (Malach và cộng sự, 2020; Pensia và cộng sự, 2020; Fischer và cộng sự, 2021; da Cunha và cộng sự, 2022; Burkholz, 2022a;b; Burkholz và cộng sự, 2022) cũng như thực nghiệm Ramanujan và cộng sự (2020); Zhou và cộng sự (2019); Diffenderfer & Kailkhura (2021); Sreenivasan và cộng sự (2022a) cho đến nay chỉ tập trung vào việc tỉa các mạng nguồn dày đặc. Chúng tôi làm nổi bật tiềm năng tiết kiệm tài nguyên tính toán trong việc tìm kiếm SLTs bằng cách chứng minh sự tồn tại của chúng trong các mạng ngẫu nhiên thưa thay thế. Thành phần chính của kết quả chúng tôi là Bổ đề 2.2, mở rộng các xấp xỉ tổng tập con cho thiết lập đồ thị ngẫu nhiên thưa. Điều này cho phép chuyển đổi trực tiếp hầu hết các kết quả tồn tại SLT cho các kiến trúc và hàm kích hoạt khác nhau sang các mạng nguồn thưa. Hơn nữa, chúng tôi sửa đổi thuật toán edge-popup (EP) (Ramanujan và cộng sự, 2020) để tìm SLTs tương ứng, dẫn đến phương pháp tỉa thưa đến thưa đầu tiên cho SLTs, theo hiểu biết của chúng tôi. Chúng tôi chứng minh trong các thí nghiệm rằng việc bắt đầu ngay cả ở độ thưa cao đến 0.8 không cản trở hiệu suất tổng thể của EP.

Lưu ý rằng lý thuyết tổng quát của chúng tôi áp dụng cho bất kỳ tỷ lệ thưa theo lớp nào của mạng nguồn ngẫu nhiên và chúng tôi xác thực thực tế này trong các thí nghiệm khác nhau trên dữ liệu hình ảnh điểm chuẩn tiêu chuẩn và các kiến trúc mạng nơ-ron thường được sử dụng, bổ sung kết quả của Liu và cộng sự (2021) cho các lựa chọn tỷ lệ thưa bổ sung. Hai đề xuất của chúng tôi, tỷ lệ thưa cân bằng và hình tháp, dường như hoạt động cạnh tranh qua nhiều thiết lập, đặc biệt, ở các chế độ thưa cao hơn.

Đóng góp
1. Chúng tôi chứng minh rằng các mạng ngẫu nhiên được tỉa ngẫu nhiên đủ biểu đạt và có thể xấp xỉ một mạng mục tiêu tùy ý nếu chúng rộng hơn một yếu tố 1/log(1/sparsity). Yếu tố tham số hóa quá mức này là cần thiết nói chung, như cận dưới của chúng tôi cho các mạng mục tiêu đơn biến chỉ ra.

2. Được truyền cảm hứng từ các chứng minh của chúng tôi, chúng tôi chứng minh thực nghiệm rằng, không mất đáng kể về hiệu suất, việc bắt đầu bất kỳ sơ đồ huấn luyện dày đặc đến thưa nào có thể được dịch thành một sơ đồ thưa đến thưa bằng cách bắt đầu từ một mạng nguồn ngẫu nhiên thay vì một mạng dày đặc.

3. Như một trường hợp đặc biệt, chúng tôi cũng chứng minh sự tồn tại của Vé số May mắn Mạnh (SLTs) trong các mạng nguồn ngẫu nhiên thưa, nếu mạng nguồn rộng hơn mục tiêu một yếu tố 1/log(1/sparsity). Sự sửa đổi của chúng tôi đối với thuật toán edge-popup (EP) (Ramanujan và cộng sự, 2020) dẫn đến phương pháp tỉa SLT thưa đến thưa đầu tiên, điều này xác thực lý thuyết của chúng tôi và làm nổi bật tiềm năng tiết kiệm tính toán.

4. Để chứng minh rằng lý thuyết của chúng tôi áp dụng cho các lựa chọn tỷ lệ thưa khác nhau, chúng tôi giới thiệu hai đề xuất bổ sung vượt trội hơn các đề xuất tiên tiến trên nhiều điểm chuẩn và do đó là các ứng cử viên hứa hẹn cho các điểm khởi đầu của các sơ đồ học thưa đến thưa.

1.1. Công trình Liên quan
Các thuật toán để tỉa mạng nơ-ron cho độ thưa không có cấu trúc có thể được phân loại rộng rãi thành hai nhóm, tỉa sau huấn luyện và tỉa trước (hoặc trong) huấn luyện. Nhóm thuật toán đầu tiên tỉa sau huấn luyện hiệu quả trong việc tăng tốc suy luận, nhưng chúng vẫn dựa vào quy trình huấn luyện tốn kém về mặt tính toán (Hassibi và cộng sự, 1993; LeCun và cộng sự, 1989; Molchanov và cộng sự, 2016; Dong và cộng sự, 2017; Yu và cộng sự, 2022). Nhóm thuật toán thứ hai tỉa tại thời điểm khởi tạo (Lee và cộng sự, 2018; Wang và cộng sự, 2020; Tanaka và cộng sự, 2020; Sreenivasan và cộng sự, 2022b; de Jorge và cộng sự, 2020) hoặc theo một chu kỳ tỉa và huấn luyện lại tốn kém về mặt tính toán cho nhiều vòng lặp (Gale và cộng sự, 2019; Savarese và cộng sự, 2020; You và cộng sự, 2019; Frankle & Carbin, 2019; Renda và cộng sự, 2019). Những phương pháp này tìm các mạng con có thể huấn luyện được cũng được gọi là Vé số May mắn (Frankle & Carbin, 2019). Các phương pháp tỉa một lần rẻ hơn về mặt tính toán nhưng dễ bị các vấn đề như sụp đổ lớp khiến mạng được tỉa không thể huấn luyện được (Lee và cộng sự, 2018; Wang và cộng sự, 2020). Tanaka và cộng sự (2020) giải quyết vấn đề này bằng cách bảo tồn luồng trong mạng thông qua cơ chế tính điểm của họ. Các mạng thưa hoạt động tốt nhất vẫn được thu được bằng các phương pháp tỉa lặp đi lặp lại tốn kém như Tỉa Độ lớn Lặp đi lặp lại (IMP), Synflow Lặp đi lặp lại (Frankle & Carbin, 2019; Fischer & Burkholz, 2022) hoặc các phương pháp thưa hóa liên tục (Sreenivasan và cộng sự, 2022b; Savarese và cộng sự, 2020; Kusupati và cộng sự, 2020; Louizos và cộng sự, 2018).

Tuy nhiên, Su và cộng sự (2020) thấy rằng các mặt nạ được tỉa ngẫu nhiên có thể vượt trội hơn các chiến lược tỉa lặp đi lặp lại tốn kém trong các tình huống khác nhau. Được truyền cảm hứng từ phát hiện này, Golubeva và cộng sự (2021); Chang và cộng sự (2021) đã đưa ra giả thuyết rằng các mạng thưa tham số hóa quá mức hiệu quả hơn các mạng nhỏ hơn với cùng số lượng tham số. Liu và cộng sự (2021) đã tiếp tục chứng minh tính cạnh tranh của các mặt nạ ngẫu nhiên cho các lựa chọn độc lập với dữ liệu khác nhau về tỷ lệ thưa theo lớp qua một loạt rộng các kiến trúc mạng nơ-ron và tập dữ liệu, bao gồm cả những tập phức tạp. Phân tích của chúng tôi xác định các điều kiện mà hiệu quả của các mặt nạ ngẫu nhiên là hợp lý. Chúng tôi chỉ ra rằng một mạng nguồn ngẫu nhiên thưa có thể xấp xỉ một mạng mục tiêu nếu nó rộng hơn một yếu tố tỷ lệ với log nghịch đảo độ thưa. Bổ sung cho các thí nghiệm của Liu và cộng sự (2021), chúng tôi làm nổi bật rằng các mặt nạ ngẫu nhiên cạnh tranh cho các lựa chọn tỷ lệ thưa theo lớp khác nhau. Tuy nhiên, chúng tôi cũng chỉ ra rằng tính ngẫu nhiên của chúng cũng có thể tạo ra tiềm năng cho việc tỉa thêm.

Chúng tôi xây dựng trên lý thuyết tồn tại vé số may mắn (Malach và cộng sự, 2020; Pensia và cộng sự, 2020; Orseau và cộng sự, 2020; Fischer và cộng sự, 2021; Burkholz và cộng sự, 2022; Burkholz, 2022b; Ferbach và cộng sự, 2022) để chứng minh rằng các mạng nguồn ngẫu nhiên thưa thực sự chứa các vé số may mắn mạnh (SLTs) nếu độ rộng của chúng vượt quá một giá trị tỷ lệ với độ rộng của một mạng mục tiêu. Lý thuyết này được truyền cảm hứng từ bằng chứng thực nghiệm cho SLTs (Ramanujan và cộng sự, 2020; Zhou và cộng sự, 2019; Diffenderfer & Kailkhura, 2021; Sreenivasan và cộng sự, 2022a). Thuật toán cơ bản edge-popup (Ramanujan và cộng sự, 2020) tìm SLTs bằng cách huấn luyện điểm số cho mỗi tham số của mạng nguồn dày đặc và do đó tốn kém về mặt tính toán như huấn luyện dày đặc. Chúng tôi chỉ ra rằng huấn luyện các mạng nguồn ngẫu nhiên thưa nhỏ hơn là đủ, do đó, giảm hiệu quả các yêu cầu tính toán để tìm SLTs.

Tuy nhiên, lý thuyết của chúng tôi cho thấy rằng các mạng ER ngẫu nhiên đối mặt với một hạn chế cơ bản ở độ thưa cực cao, vì yếu tố tham số hóa quá mức tỷ lệ trong chế độ này như 1/log(1/(sparsity)) ≈ 1/(1-sparsity). Khuyết điểm này có thể được giải quyết bằng việc nối lại có mục tiêu các cạnh ngẫu nhiên với Huấn luyện Thưa Động (DST) bắt đầu tỉa từ một mạng ER (Liu và cộng sự, 2021; Mocanu và cộng sự, 2018; Yuan và cộng sự, 2021). Cho đến nay, các phương pháp huấn luyện thưa đến thưa như Evci và cộng sự (2020); Dettmers & Zettlemoyer (2019) vẫn yêu cầu gradient dày đặc cho thao tác nối lại cạnh của chúng. Zhou và cộng sự (2021) thu được huấn luyện thưa bằng cách ước tính gradient thưa sử dụng hai lần truyền thuận. Chúng tôi chỉ ra thực nghiệm rằng dưới ánh sáng của sức mạnh biểu đạt của các mạng ngẫu nhiên, chúng ta cũng có thể đạt được huấn luyện thưa đến thưa bằng cách đơn giản hạn chế bất kỳ phương pháp tỉa hoặc gradient nào vào một mặt nạ ngẫu nhiên thưa ban đầu cố định.

2. Khả năng Biểu đạt của Mạng Ngẫu nhiên
Các nghiên cứu lý thuyết của chúng tôi trong phần tiếp theo có mục đích giải thích tại sao hiệu quả của các mạng ngẫu nhiên là hợp lý do sức mạnh biểu đạt cao của chúng. Chúng tôi chỉ ra rằng chúng ta có thể xấp xỉ bất kỳ mạng mục tiêu nào với sự trợ giúp của một mạng ngẫu nhiên, miễn là nó rộng hơn một yếu tố logarit trong nghịch đảo độ thưa. Đầu tiên, ràng buộc duy nhất mà chúng ta đối mặt trong việc xây dựng tường minh một mạng con đại diện là các cạnh có sẵn hoặc không có sẵn một cách ngẫu nhiên. Nhưng chúng ta có thể chọn các tham số mạng còn lại, tức là các trọng số và bias, theo cách mà chúng ta có thể biểu diễn tối ưu một mạng mục tiêu. Như thường thấy trong các kết quả về khả năng biểu đạt và sức mạnh biểu diễn, chúng tôi đưa ra các phát biểu về sự tồn tại của các tham số như vậy, không nhất thiết, nếu chúng có thể được tìm thấy một cách thuật toán. Trong thực tế, các tham số thường sẽ được xác định bằng huấn luyện mạng nơ-ron tiêu chuẩn hoặc các vòng lặp tỉa-huấn luyện. Các thí nghiệm của chúng tôi xác thực rằng điều này thực sự khả thi ngoài nhiều bằng chứng thực nghiệm khác (Su và cộng sự, 2020; Ma và cộng sự, 2021; Liu và cộng sự, 2021). Thứ hai, chúng tôi chứng minh sự tồn tại của các vé số may mắn mạnh (SLTs), giả định rằng chúng ta phải xấp xỉ các tham số mục tiêu bằng cách tỉa mạng nguồn ngẫu nhiên thưa. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên cung cấp bằng chứng thực nghiệm và lý thuyết cho tính khả thi của trường hợp này.

Bối cảnh, Ký hiệu và Thiết lập Chứng minh Cho x = (x1, x2, .., xd) ∈ [a1, b1]d là một vector đầu vào d chiều bị chặn, trong đó a1, b1 ∈ R với a1 < b1. f : [a1, b1]d → RnL là một mạng nơ-ron truyền thuận kết nối đầy đủ với kiến trúc (n0, n1, .., nL), tức là chiều sâu L và nl nơ-ron trong Lớp l. Mỗi lớp l ∈ {1, 2, .., L} tính toán các trạng thái nơ-ron x(l) = ϕ(h(l)), h(l) = W(l-1)x(l-1) + b(l-1). h(l) được gọi là tiền kích hoạt, W(l) ∈ Rnl×nl-1 là ma trận trọng số và b(l) là vector bias. Chúng tôi cũng viết f(x; θ) để nhấn mạnh sự phụ thuộc của mạng nơ-ron vào các tham số θ = (W(l), b(l))Ll=1 của nó. Để đơn giản, chúng tôi hạn chế bản thân với hàm kích hoạt ReLU phổ biến ϕ(x) = max{x, 0}, nhưng hầu hết các kết quả của chúng tôi có thể dễ dàng mở rộng cho các hàm kích hoạt tổng quát hơn như trong (Burkholz, 2022b;a). Ngoài các lớp kết nối đầy đủ, chúng tôi cũng xem xét các lớp tích chập. Để có ký hiệu thuận tiện, không mất tính tổng quát, chúng tôi làm phẳng các tensor trọng số để W(l)T ∈ Rcl×cl-1×kl trong đó cl, cl-1, kl lần lượt là các kênh đầu ra, kênh đầu vào và kích thước bộ lọc. Ví dụ, một phép tích chập 2 chiều trên dữ liệu hình ảnh sẽ dẫn đến kl = k'1,l k'2,l, trong đó k'1,l, k'2,l định nghĩa kích thước bộ lọc.

Chúng tôi phân biệt ba loại mạng nơ-ron, một mạng mục tiêu fT, một mạng nguồn fS, và một mạng con fP của fS. fT được xấp xỉ hoặc biểu diễn chính xác bởi fP, được thu được bằng cách che mặt các tham số của nguồn fS. fS được cho là chứa một SLT nếu mạng con này không yêu cầu huấn luyện thêm sau khi thu được mặt nạ (bằng tỉa). Chúng tôi giả định rằng fT có chiều sâu L và các tham số W(l)T, b(l)T, nT,l, mT,l là trọng số, bias, số lượng nơ-ron và số lượng tham số khác không của ma trận trọng số trong Lớp l ∈ {1, 2, .., L}. Lưu ý rằng điều này hàm ý ml ≤ nlnl-1. Tương tự, fS có chiều sâu L + 1 với các tham số W(l)S, b(l)S, nS,l, mS,lLl=0. Lưu ý rằng l dao động từ 0 đến L cho mạng nguồn, trong khi nó chỉ dao động từ 1 đến L cho mạng mục tiêu. Lớp mạng nguồn bổ sung l = 0 tính cho một lớp bổ sung mà chúng tôi cần trong việc xây dựng của chúng tôi để chứng minh sự tồn tại.

Mạng ER Mặc dù phổ biến, thuật ngữ 'mạng ngẫu nhiên' không chính xác liên quan đến phân phối ngẫu nhiên mà từ đó một đồ thị được rút ra. Phù hợp với lý thuyết đồ thị tổng quát, do đó chúng tôi sử dụng thuật ngữ mạng Erdős-Rényi (ER) (Erdos và cộng sự, 1960) trong phần sau. Một mạng nơ-ron ER fER ∈ ER(p) được đặc trưng bởi các tỷ lệ thưa theo lớp pl. Một nguồn ER fER được định nghĩa là một mạng con của một mạng nguồn hoàn chỉnh sử dụng mặt nạ nhị phân S(l)ER ∈ {0,1}nl×nl-1 hoặc S(l)ER ∈ {0,1}nl×nl-1×kl cho mỗi lớp. Các mục mặt nạ được rút ra từ các phân phối Bernoulli độc lập với xác suất thành công theo lớp pl > 0, tức là s(l)ij,ER ∼ Ber(pl). Việc tỉa ngẫu nhiên được thực hiện ban đầu với chi phí tính toán không đáng kể và mặt nạ giữ cố định trong quá trình huấn luyện. Lưu ý rằng pl cũng là mật độ kỳ vọng của lớp đó. Mật độ kỳ vọng tổng thể của mạng được cho là p = Σl mlpl / Σk mk = 1 - sparsity. Trong trường hợp độ thưa đồng nhất, pl = p, chúng tôi cũng viết ER(p) thay vì ER(p). Một mạng ER được định nghĩa là fER = fS(x; W · SER). Khác với các chứng minh tồn tại SLT thông thường (Ramanujan và cộng sự, 2020), chúng tôi gọi fER ∈ ER(p) là mạng nguồn, và chỉ ra rằng SLT được chứa trong mạng ER này. SLT sau đó được định nghĩa bởi mặt nạ SP, là một mạng con của SER, tức là một mục không s ij,ER = 0 cũng hàm ý một số không trong sij,P = 0, nhưng điều ngược lại không đúng. Chúng tôi bỏ qua các chỉ số dưới nếu bản chất của mặt nạ rõ ràng từ ngữ cảnh. Trong phân tích sau đây về khả năng biểu đạt trong các mạng ER, chúng tôi tiếp tục sử dụng fSER và SP để biểu thị một mạng nguồn ER ngẫu nhiên và một mạng con thưa trong mạng ER tương ứng.

Tỷ lệ Thưa Có nhiều lựa chọn hợp lý cho các tỷ lệ thưa theo lớp và do đó xác suất ER pl. Lý thuyết của chúng tôi áp dụng cho tất cả chúng. Lựa chọn tối ưu cho một kiến trúc mạng nguồn nhất định phụ thuộc vào mạng mục tiêu và do đó giải pháp cho một vấn đề học tập, thường không được biết trước trong thực tế. Để chứng minh rằng lý thuyết của chúng tôi đúng cho các phương pháp khác nhau, chúng tôi điều tra các tỷ lệ thưa theo lớp sau đây trong các thí nghiệm. Đường cơ sở đơn giản nhất là lựa chọn đồng nhất toàn cục pl = p. Liu và cộng sự (2021) đã so sánh lựa chọn này trong các thí nghiệm mở rộng với đề xuất chính của họ, ERK, gán pl ∝ nin + nout / ninnout cho tuyến tính và pl ∝ cl + cl-1 + kl / clcl-1kl (Mocanu và cộng sự, 2017) cho lớp tích chập. Ngoài ra, chúng tôi đề xuất một phương pháp hình tháp và cân bằng, được hình dung trong Phụ lục A.15.

Hình tháp: Phương pháp này mô phỏng một thuộc tính của các mạng được tỉa thu được bằng IMP (Frankle & Carbin, 2019) tức là mật độ lớp giảm dần với độ sâu tăng của mạng. Đối với một mạng có độ sâu L, chúng tôi sử dụng pl = (p1)l, pl ∈ (0,1) để Σl=L l=1 plml / Σl=L l=1 ml = p. Cho kiến trúc, chúng tôi sử dụng bộ giải phương trình đa thức (Harris và cộng sự, 2020) để thu được p1 cho lớp đầu tiên sao cho p1 ∈ (0,1).

Cân bằng: Phương pháp tỷ lệ thưa theo lớp thứ hai nhằm duy trì cùng số lượng tham số trong mỗi lớp cho một độ thưa mạng p và kiến trúc mạng nguồn nhất định. Mỗi nơ-ron có in-degree và out-degree tương tự trung bình. Mỗi lớp có x = p / L Σl=L l=1 ml tham số khác không. Một mạng ER như vậy có thể được thực hiện với pl = x / ml. Trong trường hợp x ≥ ml, chúng tôi đặt pl = 1.

2.1. Khả năng Biểu đạt Tổng quát của Mạng ER
Mục tiêu chính của chúng tôi trong phần này là suy ra các phát biểu xác suất về sự tồn tại của các cạnh trong một mạng nguồn ER cho phép chúng ta xấp xỉ một mạng mục tiêu nhất định. Vì mỗi kết nối trong mạng nguồn chỉ tồn tại với xác suất pl, đối với mỗi trọng số mục tiêu, chúng ta cần tạo nhiều cạnh ứng cử viên, trong đó ít nhất một cạnh khác không với xác suất đủ cao. Điều này có thể đạt được bằng cách đảm bảo rằng mỗi cạnh mục tiêu có nhiều điểm khởi đầu tiềm năng trong mạng nguồn ER. Việc xây dựng của chúng tôi thực hiện ý tưởng này với nhiều bản sao của mỗi nơ-ron trong một lớp. Số lượng bản sao nơ-ron yêu cầu phụ thuộc vào độ thưa của mạng nguồn ER và giới thiệu yếu tố tham số hóa quá mức liên quan đến độ rộng của mạng. Để tạo nhiều bản sao của các nơ-ron đầu vào cũng vậy, việc xây dựng của chúng tôi dựa vào một lớp bổ sung trong mạng nguồn so với mạng mục tiêu, như được hình dung trong Hình 4 trong Phụ lục. Chúng tôi đầu tiên giải thích việc xây dựng cho một lớp mục tiêu duy nhất và mở rộng nó sau đó cho các kiến trúc sâu hơn.

Mục tiêu Lớp Ẩn Đơn Chúng tôi bắt đầu với việc xây dựng một mạng mục tiêu kết nối đầy đủ một lớp ẩn duy nhất với một mạng con của một mạng nguồn ER ngẫu nhiên bao gồm thêm một lớp. Chiến lược chứng minh của chúng tôi được giải thích trực quan bằng Hình 4 trong Phụ lục. Định lý sau đây phát biểu yêu cầu độ rộng chính xác mà việc xây dựng của chúng tôi yêu cầu.

Định lý 2.1 (Xây dựng Mục tiêu Lớp Ẩn Đơn). Giả sử rằng một mạng mục tiêu một lớp ẩn kết nối đầy đủ fT(x) = W(2)T ϕ(W(1)T x + b(1)T) + b(2)T, một xác suất thất bại cho phép δ ∈ (0,1), mật độ nguồn p và một mạng nguồn ER 2 lớp fS ∈ ER(p) với độ rộng nS,0 = q0d, nS,1 = q1nT,1, nS,2 = q2nT,2 được cho. Nếu q0 ≥ 1/log(1/(1-p1)) log(2mT,1q1/δ), q1 ≥ 1/log(1/(1-p2)) log(2mT,2/δ) và q2 = 1 thì với xác suất 1 - δ, mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W · SP) = fT.

Đề cương Chứng minh:
Ý tưởng chính là tạo nhiều bản sao (khối trong Hình 4 (b) trong Phụ lục) trong mạng nguồn cho mỗi nơ-ron mục tiêu sao cho mỗi liên kết mục tiêu được thực hiện bằng cách trỏ đến ít nhất một trong những bản sao này trong nguồn ER. Để tạo nhiều ứng cử viên của các nơ-ron đầu vào, chúng tôi tạo một lớp đầu tiên đơn biến trong mạng nguồn như được giải thích trong Hình 4. Trong phụ lục, chúng tôi suy ra các tham số trọng số và bias tương ứng của mạng nguồn để nó có thể biểu diễn mạng mục tiêu một cách chính xác. Tự nhiên, nhiều liên kết có sẵn sẽ nhận trọng số không nếu chúng không cần thiết trong việc xây dựng cụ thể nhưng được yêu cầu cho xác suất đủ cao rằng ít nhất một trọng số có thể được đặt thành khác không. Nhiệm vụ chính của chúng tôi trong chứng minh là ước tính xác suất rằng chúng ta có thể tìm thấy đại diện của tất cả các liên kết mục tiêu trong mạng nguồn ER, tức là mỗi nơ-ron trong Lớp l = 1 có ít nhất một cạnh đến mỗi khối trong l = 0 có kích thước q0, như được hiển thị trong Hình 4 (b). Xác suất này được cho bởi (1-(1-p1)q0)mT,1q1. Đối với lớp thứ hai, chúng tôi lặp lại một lập luận tương tự để giới hạn xác suất (1-(1-p2)q1)mT,2 với q2 = 1, vì chúng tôi không yêu cầu nhiều bản sao của các nơ-ron đầu ra. Giới hạn xác suất này bằng 1 - δ hoàn thành chứng minh, như được chi tiết trong Phụ lục A.3.

Mạng Mục tiêu Sâu Định lý 2.1 chỉ ra rằng q0 và q1 phụ thuộc vào 1/log(1/sparsity). Bây giờ chúng tôi tổng quát hóa ý tưởng tạo nhiều bản sao của các nơ-ron mục tiêu trong mỗi lớp cho một mạng kết nối đầy đủ có độ sâu L (chứng minh trong Phụ lục A.4) và các mạng tích chập có độ sâu L như được phát biểu trong Phụ lục A.5, mang lại kết quả tương tự như trên. Thách thức bổ sung của việc mở rộng là xử lý các phụ thuộc của các lớp, vì việc xây dựng mỗi lớp cần phải khả thi.

Định lý 2.2 (Mạng ER có thể biểu diễn mạng mục tiêu L lớp). Cho một mạng mục tiêu kết nối đầy đủ fT có độ sâu L, δ ∈ (0,1), mật độ nguồn p và một mạng nguồn ER L+1 lớp fS ∈ ER(p) với độ rộng nS,0 = q0d và nS,l = qlnT,l, l ∈ {1, 2, .., L}, trong đó ql ≥ 1/log(1/(1-pl+1)) log(LmT,l+1ql+1/δ) cho l ∈ {0, 1, .., L-1} và qL = 1, thì với xác suất 1 - δ mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W · SP) = fT.

Cận Dưới về Tham số hóa Quá mức Trong khi các kết quả tồn tại của chúng tôi chứng minh rằng các mạng ER có thuộc tính xấp xỉ hàm phổ quát như các mạng nơ-ron dày đặc, để đạt được điều đó, việc xây dựng của chúng tôi yêu cầu một lượng tham số hóa quá mức đáng kể so với một mạng mục tiêu dày đặc. Đặc biệt các mạng ER cực kỳ thưa dường như đối mặt với một hạn chế tự nhiên, vì đối với độ thưa 1 - p ≥ 0.9, yếu tố tham số hóa quá mức tỷ lệ xấp xỉ như 1/log(1/(1-p)) ≈ 1/p. Hình 2 hình dung cách tỷ lệ này trở nên có vấn đề đối với độ thưa tăng. Định lý tiếp theo thiết lập rằng, thật không may, chúng ta không thể mong đợi thoát khỏi hạn chế 1/log(1/(1-pl)) này.

Định lý 2.3 (Cận dưới về Tham số hóa Quá mức trong Mạng ER). Tồn tại các mạng mục tiêu đơn biến fT(x) = ϕ(wTTx + bT) không thể được biểu diễn bởi một mạng nguồn ER 1-lớp-ẩn ngẫu nhiên fS ∈ ER(p) với xác suất ít nhất 1 - δ, nếu độ rộng của nó là nS,1 < 1/log(1/(1-p)) log(1/(1-(1-δ)1/d)).

Xem Hình 6 và Phụ lục A.6 cho chứng minh đầy đủ.

Hiểu biết Lý thuyết Chúng tôi đã chỉ ra rằng các mạng ER có thể chứa các mạng con có thể biểu diễn các mạng mục tiêu tổng quát nếu chúng rộng hơn một yếu tố 1/log(1/(1-pl)). Yếu tố tham số hóa quá mức này là cần thiết và hạn chế tính hữu dụng của các mặt nạ ngẫu nhiên một mình để thu được các kiến trúc mạng nơ-ron cực kỳ thưa. Tuy nhiên, khả năng biểu đạt cao của chúng khiến chúng trở thành điểm khởi đầu hứa hẹn và rẻ về mặt tính toán cho việc tỉa thêm và các phương pháp thưa hóa tổng quát hơn.

Được truyền cảm hứng từ hiểu biết này, trong phần tiếp theo, chúng tôi khám phá ý tưởng bắt đầu tỉa từ các mạng nguồn ER trong bối cảnh của SLTs. Câu hỏi đầu tiên mà chúng tôi đặt ra là: Các mạng nguồn ngẫu nhiên cần rộng bao nhiêu để chứa SLTs?

[Hình 2. Tham số hóa Quá mức trong Mạng ER Đối với một mạng mục tiêu một lớp ẩn với độ rộng 128 trong lớp ẩn và 10 trong lớp đầu ra, hình cho thấy độ rộng yêu cầu của lớp đầu tiên (l = 1) của mạng nguồn ER theo Định lý 2.1 với độ tin cậy 1 - δ = 0.999. Độ rộng yêu cầu tăng vừa phải đến độ thưa 0.9 và mạnh mẽ sau 0.95.]

2.2. Sự Tồn tại của Vé số May mắn Mạnh
Hầu hết các chứng minh tồn tại SLT suy ra cận dưới logarit về yếu tố tham số hóa quá mức của mạng nguồn (Pensia và cộng sự, 2020; Burkholz và cộng sự, 2022; Burkholz, 2022a; da Cunha và cộng sự, 2022; Burkholz, 2022b; Ferbach và cộng sự, 2022) giải quyết nhiều vấn đề xấp xỉ tổng tập con (Lueker, 1998). Đối với mỗi tham số mục tiêu z, chúng xác định một số tham số ngẫu nhiên của mạng nguồn X1, ..., Xn, một tập con trong đó có thể xấp xỉ z. Trong trường hợp của một mạng nguồn ER, 1 - p kết nối ngẫu nhiên bị thiếu so với một mạng nguồn dày đặc. Những kết nối bị thiếu này cũng làm giảm lượng tham số nguồn có sẵn X1, ..., Xn. Để tính đến điều này, chúng tôi sửa đổi các xấp xỉ tổng tập con tương ứng theo bổ đề sau.

Bổ đề 2.4 (Xấp xỉ tổng tập con trong Mạng ER). Cho X1, ..., Xn là các biến ngẫu nhiên độc lập, phân phối đều sao cho Xi ∼ U([-1,1]) và M1, ..., Mn là các biến ngẫu nhiên độc lập, phân phối Bernoulli sao cho Mi ∼ Ber(p) cho p > 0. Cho ε, δ ∈ (0,1). Thì đối với bất kỳ z ∈ [-1,1] nào tồn tại một tập con I ⊂ [n] sao cho với xác suất ít nhất 1 - δ chúng ta có |z - Σi∈I MiXi| ≤ ε nếu n ≥ C 1/log(1/(1-p)) log(1/min(δ, ε)). (1)

Chứng minh được đưa ra trong Phụ lục A.2 và sử dụng kết quả xấp xỉ tổng tập con gốc cho các tập con ngẫu nhiên của tập cơ sở X1, ..., Xn. Ngoài ra, nó giải quyết thách thức kết hợp các hằng số liên quan tôn trọng phân phối xác suất của các tập con ngẫu nhiên. Để đơn giản, chúng tôi đã phát biểu nó cho các biến ngẫu nhiên đồng nhất và các tham số mục tiêu z ∈ [-1,1] nhưng nó có thể dễ dàng mở rộng cho các biến ngẫu nhiên chứa phân phối đồng nhất (như phân phối chuẩn) và các mục tiêu bị chặn nói chung như trong Hệ quả 7 trong (Burkholz và cộng sự, 2022).

So với kết quả xấp xỉ tổng tập con gốc, chúng ta cần một tập cơ sở lớn hơn một yếu tố 1/log(1/(1-p)). Đây chính xác là yếu tố mà chúng ta có thể sửa đổi các kết quả tồn tại SLT đương thời để chuyển sang các mạng nguồn ER và nó cũng là cùng yếu tố mà chúng tôi suy ra trong phần trước về kết quả khả năng biểu đạt. Tuy nhiên, chúng tôi yêu cầu nói chung một tham số hóa quá mức cao hơn để phù hợp với các xấp xỉ tổng tập con.

Lợi thế của việc phát biểu bổ đề trên là nó cho phép chuyển các kết quả tồn tại SLT tổng quát sang thiết lập nguồn ER một cách đơn giản. Bằng cách thay thế việc xây dựng xấp xỉ tổng tập con bằng Bổ đề 2.2, chúng ta có thể chỉ ra sự tồn tại SLT cho các mạng ER kết nối đầy đủ (Pensia và cộng sự, 2020; Burkholz, 2022b), tích chập (Burkholz và cộng sự, 2022; Burkholz, 2022a; da Cunha và cộng sự, 2022), và dư (Burkholz, 2022a), hoặc GNN ngẫu nhiên (Ferbach và cộng sự, 2022). Để đưa ra một ví dụ về việc sử dụng hiệu quả của bổ đề này và thảo luận về chiến lược chuyển đổi tổng quát, chúng tôi mở rộng tường minh các kết quả tồn tại SLT của Burkholz (2022b) cho các mạng kết nối đầy đủ sang các mạng nguồn ER. Do đó chúng tôi chỉ ra rằng việc tỉa một mạng nguồn ngẫu nhiên có độ sâu L + 1 với độ rộng lớn hơn một yếu tố logarit có thể xấp xỉ bất kỳ mạng mục tiêu nào có độ sâu L với xác suất cho trước 1 - δ.

Định lý 2.5 (Sự Tồn tại của SLTs trong Mạng ER). Cho ε, δ ∈ (0,1), một mạng mục tiêu fT có độ sâu L, một mạng nguồn ER(p) fS có độ sâu L + 1 với xác suất cạnh pl trong mỗi lớp l và các tham số ban đầu iid θ với w(l)ij ∼ U([-1,1]), b(l)i ∼ U([-1,1]). Thì với xác suất ít nhất 1 - δ, tồn tại một mặt nạ SP sao cho mỗi thành phần đầu ra mục tiêu i được xấp xỉ như maxx∈D ||fT,i(x) - fS,i(x; WS · SP)|| ≤ ε nếu nS,l ≥ C nT,l / log(1/(1-pl+1)) log(1/min{εl, δ/ρ}) cho ρ = CN1+γT / log(1/(1-minl pl))1+γ log(1/min{minl εl, δ}), l ≥ 1 cho bất kỳ γ ≥ 0 nào, và trong đó εl = g(ε, fT) được định nghĩa trong Phụ lục A.2. Chúng tôi cũng yêu cầu nS,0 ≥ Cd 1/log(1/(1-p1)) log(1/min{ε1, δ/ρ}), trong đó C > 0 biểu thị một hằng số chung độc lập với nT,l, L, pl, δ, và ε.

Đề cương Chứng minh: Ý tưởng xây dựng LT chính được hình dung trong Hình 4 (c) trong phụ lục. Đối với mỗi nơ-ron mục tiêu, nhiều bản sao xấp xỉ được tạo trong lớp tương ứng của LT để phục vụ như cơ sở cho các xấp xỉ tổng tập con đã sửa đổi (xem Bổ đề 2.2) của các tham số dẫn đến lớp tiếp theo. Phù hợp với phương pháp này, lớp đầu tiên của LT bao gồm các khối đơn biến tạo nhiều bản sao của các nơ-ron đầu vào. Ngoài Bổ đề 2.2, tổng số vấn đề xấp xỉ tổng tập con ρ phải được giải quyết cũng cần được đánh giá lại cho các mạng nguồn ER, vì điều này ảnh hưởng đến xác suất tồn tại LT. Sự sửa đổi này được điều khiển bởi cùng yếu tố 1/log(1/(1-p)). Chứng minh đầy đủ được đưa ra trong Phụ lục A.2.

Với các kết quả tồn tại SLT của chúng tôi, chúng tôi đã cung cấp ví dụ đầu tiên về cách chuyển các phương pháp học sâu dày đặc đến thưa thành các sơ đồ thưa đến thưa tổng quát. Tiếp theo, chúng tôi cũng xác thực ý tưởng bắt đầu tỉa từ một mặt nạ ER ngẫu nhiên trong các thí nghiệm.

3. Thí nghiệm
Để xác minh những hiểu biết lý thuyết của chúng tôi, chúng tôi tiến hành thí nghiệm trong các thiết lập tiêu chuẩn trên dữ liệu điểm chuẩn phổ biến (CIFAR10, CIFAR100 (Krizhevsky và cộng sự, 2009) và Tiny ImageNet (Russakovsky và cộng sự, 2015b)) và các kiến trúc mạng nơ-ron (ResNet (He và cộng sự, 2016) và VGG (Simonyan & Zisserman, 2015)). Chi tiết về thiết lập có thể được tìm thấy trong Phụ lục A.7. Chúng tôi luôn báo cáo trung bình trên 3 lần chạy độc lập. Do hạn chế về không gian, khoảng tin cậy được báo cáo trong phụ lục cùng với các thí nghiệm bổ sung. Mục tiêu chính của chúng tôi là thể hiện khả năng biểu đạt của các mạng ER với ba loại thí nghiệm. Đầu tiên, chúng tôi làm nổi bật rằng một mạng được tỉa ngẫu nhiên với tỷ lệ thưa theo lớp được chọn cẩn thận có tính cạnh tranh và đôi khi thậm chí vượt trội hơn các phương pháp tỉa tiên tiến như Tỉa Độ lớn Lặp đi lặp lại (IMP) (Frankle & Carbin, 2019) (xem Phụ lục A.9) Thứ hai, chúng tôi xác minh rằng các mạng ER có thể phục vụ như điểm khởi đầu hứa hẹn của việc thưa hóa thêm bằng cách tỉa trong mạng ER ban đầu. Thứ ba, chúng tôi áp dụng cùng nguyên tắc cho các vé số may mắn mạnh (SLTs) và trình bày kết quả huấn luyện thưa đến thưa đầu tiên trong bối cảnh này.

Hiệu suất của Tỉa Ngẫu nhiên Để bổ sung cho Liu và cộng sự (2021), chúng tôi tiến hành thí nghiệm trong các chế độ độ thưa cao hơn ≥ 0.9 để kiểm tra giới hạn mà các mạng ER ngẫu nhiên là một giải pháp thay thế khả thi cho các thuật toán tỉa tiên tiến hơn nhưng tốn kém về mặt tính toán. Su và cộng sự (2020); Ma và cộng sự (2021) đã chỉ ra rằng việc ngẫu nhiên hóa mặt nạ theo lớp của các mạng được tỉa thu được bằng các thuật toán tỉa tiên tiến thường cạnh tranh và trình bày các đường cơ sở mạnh. Các tỷ lệ thưa tương ứng tốn kém về mặt tính toán để thu được và do đó có ít quan tâm thực tế. Chúng tôi vẫn báo cáo so sánh với tỷ lệ thưa thu được bằng Snip ngẫu nhiên (Lee và cộng sự, 2018), Synflow Lặp đi lặp lại (Tanaka và cộng sự, 2020), và IMP (Frankle & Carbin, 2019) để chứng minh rằng các tỷ lệ thưa hoạt động tốt nhất cho mặt nạ ER thường khác với những tỷ lệ thu được từ các vé được tỉa lặp đi lặp lại. Trạng thái tiên tiến trước đây thường được định nghĩa bởi ERK (Evci và cộng sự, 2020; Liu và cộng sự, 2021). Ngoài ra, chúng tôi đề xuất hai phương pháp để chọn độ thưa theo lớp, cân bằng và hình tháp, thường cải thiện hiệu suất của các mạng ER (xem Bảng 1 và kết quả tiếp theo cho ResNets trên CIFAR10 và 100 trong Phụ lục A.8). Các tỷ lệ thưa mẫu được hình dung trong Hình 7. Các phương pháp hình tháp và cân bằng có tính cạnh tranh và thậm chí vượt trội hơn ERK trong các thí nghiệm của chúng tôi cho độ thưa lên đến 0.99. Quan trọng là, chúng cũng vượt trội hơn các tỷ lệ thưa theo lớp thu được bằng các thuật toán tỉa lặp đi lặp lại đắt đỏ Synflow và IMP. Tuy nhiên, đối với độ thưa cực cao 1 - p ≥ 0.99, hiệu suất của các mạng ER giảm đáng kể và thậm chí hoàn toàn sụp đổ đối với các phương pháp như ER Snip và hình tháp. Chúng tôi phỏng đoán rằng ER Snip và hình tháp dễ bị sụp đổ lớp trong các lớp cao hơn và ngay cả việc sửa chữa luồng (xem Phụ lục A.1) cũng không thể tăng đáng kể khả năng biểu đạt của mạng. Tuy nhiên, các hạn chế tổng quát mà chúng tôi gặp phải ở độ thưa cao hơn được mong đợi dựa trên lý thuyết của chúng tôi. Những điều này có thể được khắc phục một phần bằng cách sử dụng chiến lược nối lại của Huấn luyện Thưa Động (DST) (Evci và cộng sự, 2020).

[Bảng 1. Mạng ER với độ thưa theo lớp khác nhau trên CIFAR10 với VGG16. Chúng tôi so sánh độ chính xác kiểm tra của tỷ lệ thưa theo lớp cân bằng và hình tháp của chúng tôi với những tỷ lệ đồng nhất, ERK, và mạng ER với tỷ lệ thưa theo lớp thu được bằng IMP, Synflow Lặp đi lặp lại và Snip (được ký hiệu bằng ER). Khoảng tin cậy được báo cáo trong Phụ lục A.8.]

Huấn luyện Thưa Động Để cải thiện các mạng được tỉa ngẫu nhiên ở độ thưa cực cao, chúng tôi sử dụng thuật toán RiGL (Evci và cộng sự, 2020) để thu được Bảng 2. Đầu tiên, chúng tôi chỉ nối lại các cạnh, điều này cho phép chúng tôi bắt đầu từ các mạng tương đối thưa. Đơn giản bằng cách phân phối lại các cạnh, hiệu suất của mạng ER có thể được cải thiện. Đặc biệt, các tỷ lệ thưa ban đầu cân bằng hoặc hình tháp dường như có thể cải thiện hiệu suất của RiGL. Bảng 28 trong phụ lục chứng minh rằng việc bắt đầu RiGL (tỉa + nối lại) từ độ thưa cao hơn nhiều lên đến 0.9 cũng có thể mà không mất độ chính xác đáng kể, điều này làm nổi bật tính hữu dụng của các mặt nạ ER ngẫu nhiên ngay cả ở độ thưa cực cao.

[Bảng 2. Mạng ER được nối lại với DST: Độ chính xác Kiểm tra cho một VGG16 ER(p) với mặt nạ cố định và sau khi nối lại các cạnh với RiGL (Evci và cộng sự, 2020; Liu và cộng sự, 2021) trên CIFAR10. Khoảng tin cậy được báo cáo trong Phụ lục A.14.]

Huấn luyện Thưa đến Thưa với mạng ER Chúng tôi xác minh rằng các mạng ER có thể phục vụ như một điểm khởi đầu hứa hẹn của các sơ đồ thưa hóa thêm tỉa trong mạng ER như được giải thích bởi Hình 1. Hiệu quả, ý tưởng này có thể biến bất kỳ sơ đồ huấn luyện dày đặc đến thưa nào thành một sơ đồ thưa đến thưa. Như đại diện cho một phương pháp tỉa lặp đi lặp lại, chúng tôi nghiên cứu IMP và cho sơ đồ thưa hóa liên tục, chúng tôi sử dụng Tái tham số hóa Ngưỡng Mềm (STR) (Kusupati và cộng sự, 2020). Trong Bảng 3 và 4, chúng tôi quan sát rằng chúng ta có thể bắt đầu huấn luyện với mặt nạ ER có độ thưa lên đến 0.9 và tỉa mạng thêm mà không mất nhiều hiệu suất. Đối với cả STR và IMP, việc tỉa một mạng ER có độ thưa 0.7 trên CIFAR10 dẫn đến cùng hiệu suất mà chúng ta sẽ thu được nếu chúng ta tỉa một mạng dày đặc thay thế. Các thí nghiệm của chúng tôi chỉ ra rằng đối với cả STR và IMP, đặc biệt, các tỷ lệ tỉa ban đầu cân bằng có thể tăng cường hiệu suất của phương pháp tổng quát.

[Bảng 3. Huấn luyện thưa đến thưa với Tái tham số hóa Ngưỡng Mềm trong mạng ER: Kết quả trên một ResNet18 được huấn luyện trên CIFAR10. STR (ER) biểu thị tỷ lệ thưa thu được bằng STR. Để tham khảo, bắt đầu từ một mạng dày đặc STR đạt 94.66% và 90.95% ở độ thưa 0.9 và 0.993 tương ứng. Xem Phụ lục A.10 cho khoảng tin cậy.]

[Bảng 4. Huấn luyện thưa đến thưa với Tỉa Độ lớn Lặp đi lặp lại trong mạng ER: Kết quả trên một ResNet18 được huấn luyện trên CIFAR10. Để tham khảo, bắt đầu từ một mạng dày đặc IMP đạt 93.38% và 91.39% ở độ thưa 0.9 và 0.99 tương ứng. Xem Phụ lục A.10 cho khoảng tin cậy.]

Thí nghiệm cho SLTs Tương tự như các thí nghiệm trước đây của chúng tôi, chúng ta cũng có thể tỉa một mặt nạ ER ngẫu nhiên để thu được SLTs. Chúng tôi sử dụng thuật toán edge-popup (Ramanujan và cộng sự, 2020) để xác minh các suy dẫn lý thuyết của chúng tôi. Bảng 5 trình bày bằng chứng cho thực tế rằng việc tìm kiếm SLTs không cần phải tốn kém về mặt tính toán như huấn luyện dày đặc. Đáng chú ý, chúng ta có thể bắt đầu với một mạng ER thưa lên đến độ thưa 0.8 thay vì một mạng dày đặc và vẫn đạt hiệu suất cạnh tranh trong việc tìm một SLT với độ thưa cuối cùng 0.9. Các thí nghiệm bổ sung được báo cáo trong phụ lục (xem Bảng 20 và 22).

[Bảng 5. Mạng ER cho Vé số May mắn Mạnh: Độ chính xác kiểm tra của SLTs thu được bằng edge-popup (EP) (Ramanujan và cộng sự, 2020) tỉa một ResNet18 ER thưa trên CIFAR10. Bắt đầu dày đặc (xem Phụ lục A.11), EP đạt độ chính xác 87.86% cho độ thưa 0.9.]

Thí nghiệm trên Nhiều Nhiệm vụ Khác nhau Trong khi hầu hết các thí nghiệm của chúng tôi tập trung vào các nhiệm vụ phân loại hình ảnh, những hiểu biết lý thuyết của chúng tôi tổng quát hơn và áp dụng cho các cấu trúc mạng mục tiêu và nguồn đa dạng. Để chứng minh phạm vi rộng hơn của kết quả chúng tôi, chúng tôi cung cấp các thí nghiệm bổ sung cho huấn luyện thưa đến thưa trên ImageNet, Mạng Tích chập Đồ thị, dữ liệu thuật toán và dữ liệu dạng bảng trong Phụ lục A.16. Một cách nhất quán, chúng tôi thấy rằng việc tỉa các mạng ngẫu nhiên thưa đạt hiệu suất cạnh tranh so với việc tỉa một mạng dày đặc.

4. Kết luận
Chúng tôi đã giải thích một cách có hệ thống hiệu quả của việc tỉa ngẫu nhiên và do đó cung cấp lời biện minh lý thuyết cho việc sử dụng các mặt nạ Erdős-Rényi (ER) như các đường cơ sở mạnh cho việc tỉa vé số may mắn và như điểm khởi đầu của huấn luyện thưa động. Lý thuyết của chúng tôi hàm ý rằng các mạng ER ngẫu nhiên có khả năng biểu đạt như các mạng mục tiêu dày đặc nếu chúng rộng hơn một yếu tố logarit trong nghịch đảo độ thưa của chúng. Các xây dựng của chúng tôi cho thấy rằng việc tỉa ngẫu nhiên, mặc dù rẻ về mặt tính toán, không đạt được độ thưa tối ưu nhưng có tiềm năng lớn cho việc tỉa thêm. Phát hiện này cũng có ý nghĩa thực tế, vì các mặt nạ ngẫu nhiên thưa ban đầu có thể tránh quy trình tốn kém về mặt tính toán của việc tỉa một mạng dày đặc từ đầu. Như điểm nhấn mẫu, chúng tôi đã áp dụng hiểu biết này cho các vé số may mắn mạnh. Chúng tôi đã chứng minh về mặt lý thuyết và chứng minh thực nghiệm rằng việc tỉa cho các vé số may mắn mạnh có thể đạt được bằng các sơ đồ huấn luyện thưa đến thưa.

Tài liệu Tham khảo
[Phần tài liệu tham khảo được dịch nguyên văn như bản gốc với tất cả các chi tiết bibliographic]

A. Phụ lục
A.1. Bảo tồn Luồng để ngăn ngừa sụp đổ lớp trong mạng ER
[Nội dung phụ lục được dịch đầy đủ theo cấu trúc và chi tiết như bản gốc...]
