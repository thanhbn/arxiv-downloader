# 2305.17559.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/initialization/2305.17559.pdf
# File size: 634434 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pruning at Initialization -
A Sketching Perspective
Noga Bar
Tel Aviv University
Tel Aviv, Israel
nogabar@mail.tau.ac.ilRaja Giryes
Tel Aviv University
Tel Aviv, Israel
raja@tauex.tau.ac.il
Abstract
The lottery ticket hypothesis (LTH) has increased attention to pruning neural
networks at initialization. We study this problem in the linear setting. We show
that finding a sparse mask at initialization is equivalent to the sketching problem
introduced for efficient matrix multiplication. This gives us tools to analyze the
LTH problem and gain insights into it. Specifically, using the mask found at
initialization, we bound the approximation error of the pruned linear model at the
end of training. We theoretically justify previous empirical evidence that the search
for sparse networks may be data independent. By using the sketching perspective,
we suggest a generic improvement to existing algorithms for pruning at initialization,
which we show to be beneficial in the data-independent case.
1 Introduction
Pruning a neural network at initialization, where weights are removed ahead of training, with
minimal harm to network performance can be beneficial for training efficiency. It can also be
used to gain insights into neural network training and expressivity as a whole. According to
the lottery ticket hypothesis (LTH) [ 23], a network may contain extremely sparse subnetworks at
initialization that achieve comparable or even better performance when trained in isolation. The
original algorithm suggested for finding the winning ticket was inefficient and required multiple
trainings until convergence. Others, however, suggested pruning at initialization in a more efficient
manner [39, 61, 63, 17, 2, 38].
Most works propose scoring functions for finding a subnetwork at initialization that rely on the
specific data and task at hand [ 39,63,17,2]. Yet, evidence suggests that the winning lottery ticket
is independent of data. Specifically, it has been shown that the winning ticket can be transferred
between datasets and tasks [ 19,47,13,57]. Moreover, previous work has shown that pruning methods
can have good performance with corrupted training data [ 60]. Furthermore, the work of SynFlow
has demonstrated that a network can even be pruned at initialization without the use of data and a
task-specific loss [61].
In this work, we aim at explaining the success of LTH in the unsupervised setting without data.
Previous theoretical analysis focused on a stronger version of the LTH [ 56]. According to the study,
deep neural networks have a sparse subnetwork capable of good performance even without training in
a supervised setting. Specifically, they show that for a given deep neural network (DNN) architecture,
initialization and target data (with labels), there is a subnetwork that achieves high accuracy without
the need to train. The theoretical explanation for the strong LTH is developed by estimating the
function produced by the large and dense network using only the sparse subnetworks in it [ 43,55].
This result was further generalized and it has been shown that the initialization can be compatible
with a set of functions over the training set [ 10]. While the above results provide an intriguing
explanation for the success of LTH in the supervised case, their explanation for the existence of the
mask assumes the data is available for performing the pruning and that no training is required. OurarXiv:2305.17559v1  [cs.LG]  27 May 2023

--- PAGE 2 ---
study, on the other hand, focuses on the unsupervised setting, where pruning is performed using
random data and applied to other parameters than the ones at initialization (possibly after training).
For our analysis, we draw a connection between pruning at initialization and a well known sketching
algorithm [ 18]. Originally, the algorithm was suggested for efficient matrix multiplication while
minimizing the error of the multiplication approximation. We choose to focus on the linear case and
analyse it in order to gain insights into the general case, which is a common practice when analyzing
neural networks [4, 9, 58, 5, 40, 3, 50, 68].
The key observation in our work is that in the linear case, the sketching algorithm corresponds to
the pruning at initialization problem. Based on this relationship, we extend the sketching analysis of
the approximation error at initialization for the case of an unknown vector. In our case, this vector
can be interpreted as the learned vector at the end of training. We focus on the correspondence
between sketching and pruning without data. This allows us to analyze the performance of pruning
algorithms that operates in the unsupervised regime, i.e., without labeled training data. We develop
a bound that shows that the error in pruning without data depends on the distance between the
weights of the linear network at initialization and at the end of training.
This distance is known to be small in practical NNs and especially in the Neural Tanget Kernel
(NTK) regime [ 37,32,6], where the network width grows to infinity. Under common assumptions
in NTK settings, we consider network output features as linear random features. Based on these
assumptions, we prove that the mask found by sketching also approximates the network output
features.
Equipped with the theoretical results, we turn to study practical algorithms for the problem of
pruning at initialization without data. We consider the state-of-the-art and unsupervised SynFlow
method and show that in the linear case it bears great similarity to sketching when is applied with a
random vector. This provides us with a possible explanation for SynFlow’s success. We also analyse
the connection of the supervised SNIP method to sketching and use it to suggest an unsupervised
version of SNIP.
The relations we draw also suggest that successful pruning is highly correlated with selecting weights
that have large magnitude at initialization. We empirically validate our findings for both SynFlow
and the iterative magnitude pruning (IMP) algorithm (the method suggested in the original LTH
work) showing that they both have the tendency to maintain large magnitude weights after pruning.
To further validate our analysis with random data, we show that for various known pruning algorithms
that make use of the input data [ 23,63,39] that their performance only mildly degrades when the
input is replaced with completely random input. This stands in line with previous evidence that
pruning methods do not exploit the data [60].
Based on the above results, we suggest a general improvement to existing pruning algorithms in the
unsupervised case. Instead of pruning weights by removing the lowest scores, sketching masks are
randomized based on some probability. We test the effect of replacing the strict threshold criterion of
pruning with a randomized one in which the mask is sampled based on the pruning method scoring.
This strategy shows improvement in most cases for various network architectures and datasets,
namely, CIFAR-10, CIFAR-100 [35] and Tiny-ImageNet [66].
2 Related Work
The main research approach for pruning deep neural networks follows the train →prune→fine-tune
pipeline [ 49,46,36,29,28,27]. Those methods require training until convergence, then pruning and
finally fine tuning. Thus, they have the ability to save computations at inference time. Another
approach for pruning is sparsifying the model during training where training and pruning are
performed simultaneously [ 12,11,42,8,45,48, and more]. In this case, the fine tuning stage is
omitted. Those methods have limited ability to improve training efficiency and the gain is mainly
for inference time. Lastly, pruning at initialization, which is our focus, aims to zero parameters at
initialization. Such methods can improve efficiency in parameters for both training and inference
[19]. Additionally, it can be used for neural architecture search [ 44,64,1] and for gaining a deeper
theoretical understanding of DNN [7].
2

--- PAGE 3 ---
In this work, we discuss pruning network by simply zeroing weights (unstructured pruning). Yet,
another approach is to remove complete neurons of the network (structured pruning) [ 52,33,15,14]
(for a comprehensive survey see [ 51]). These methods generally include more parameters than
unstructured ones but are usually more beneficial for computational time on standard hardware.
However, unstructured prunedmodels alsohavethis advantages asit can lead toreduced computations
for some hardware while maintaining a lownumber of parameters [ 19]. In addition, a methodoriginally
presented for unstructured pruning that employs scoring has been extended for structured pruning
at initialization [ 62]. Therefore, the concepts presented in this work can potentially be extended to
structured pruning as well.
Due to the exponential search space, pruning DNNs at initialization is a challenging task. Before
LTH, it was suggested to use SNIP which prunes the NN while maintaining its connectivity according
to the magnitude of the gradient [ 39]. SNIP has been improved by using it iteratively [ 17] or by
applying it after a few training steps [ 2]. It was proposed to improve the signal propagation in
the DNN at initialization [ 38] and to find a mask while preserving gradient flow using the Hessian
matrix [63]. A sparse pruning algorithm for high-dimensional manifolds is included in [ 70], as well
as a theoretical validation of subnetworks’ viability. Those methods rely on the gradient of their
parameters w.r.t. the input data. Yet, a study found that some of these hardly exploit information
from the training data [ 60] hinting that it may not be required for finding good sparse subnetworks.
Another work, relaxed the need for supervision and employed a teacher-student model with unlabeled
data [41].
The lottery ticket hypothesis (LTH) [ 23] has led to an increased interest in sparse neural networks.
The original study suggested an exhaustive and supervised algorithm for finding the winning ticket.
There are works which reduce the dependency on data when searching for the winning ticket. It
was demonstrated that looking for tickets at early stages of training, prior to convergence, leads to
improved performance and saves computational overhead [ 67,31,24]. It was further shown that
training the LT with partial datasets leads to decent winning tickets [ 71]. Early pruning has been
theoretically proven to be beneficial [ 65]. Together with other pruning methods at initialization, it
was shown that the winning tickets do not rely heavily on training data [60].
Other works demonstrated the generality of the winning ticket and showed that a single pruned
network can be transferred across datasets and achieve good performance after fine-tuning [ 19,47,13]
and even that LT can be used for non-natural datasets [ 57]. The existence of universal winning
tickets that fit multiple functions was shown theoretically in [ 10]. LTH was strengthened and it
was suggested that a sparse subnetwork within the neural network initialization has high accuracy
without training [ 56]. The strong LTH was later theoretically studied [ 43,55,53,20,21]. Note that
strong LTH requires a larger network before pruning. This was relaxed by initializing the weights
iteratively [ 16]. These works only bound the approximation error at initialization and assume labeled
data.
SynFlow [ 61], a prominent method proposed for pruning at initialization, and is not data or task
dependent. We examine in our study its equivalence to a sketching method when the input data to
SynFlow is random rather than a vector of all ones. Additionally, other research on SynFlow proposed
to examine active paths in networks at initialization [ 54,25] and established data independent pruning
methods. They formulate their method according to paths in the neural tangent kernel.
We establish equivalence between the sketching problem and pruning. To this end, we use a well
known Monte-Carlo technique for efficient approximation of matrix multiplication and compression
designed for handling large matrices [ 18]. We analyze network pruning methods through the ‘sketching
lens’, which leads to an approximation error bound of the mask at initialization. The type of result
is similar to the bounds of approximation for the strong LTH but using a different proof technique
that analyze the unsupervised and “weak” case.
3 Sketching and Pruning at Initialization
Notations. The input data we aim to model is xi∈Rdfori= 1, ..., nand respectively each example
has its label yi∈R. We relate to the input as a matrix X∈Rd×nwhere xiare its columns and
3

--- PAGE 4 ---
y∈Rnis the vector containing the labels. The ith row in a matrix Ais denoted as A(i)and the ith
column as A(i). Unless otherwise stated, ∥x∥is the Euclidean norm of x.
Problem statement. In this work, we relate to linear features where we aim to find vector that
approximate the data, i.e. wsuch that XTw≃y. Our main interest is to sparsify wwhile minimizing
the mean squared error of the features. The optimization problem can be written as
min
m,s.t.∥m∥0≤s∥XTw−XT(w⊙m)∥, (1)
where ∥·∥0is the number of non-zeros in mand⊙stands for entry-wise multiplication. Clearly, when
the mask is applied to a vector, it holds that ∥w⊙m∥0≤s.
Connection of sketching and pruning at initialization. We focus on a sketching approach
presented initially for efficient matrix multiplication [ 18]. The goal in it is multiplying only a small
subset of columns and rows, while harming the approximation accuracy as little as possible. The
main contribution of the approach is in the way this subset is chosen. We adapt it to match the
linear features settings, where the matrix multiplication is reduced to be a multiplication of given
input X∈Rd×nand a vector w∈Rd. This results in the linear features XTw.
Algorithm 1 Sketching for mask [ 18].
Input:Probability p∈Rdand den-
sitys∈Z+.
Initialization Set mask m= 0.
fort= 1, ..., sdo
it∼p
mit=mit+1
spitend for
Output: The mask m.Note that choosing rows/columns in matrix multiplication
is equivalent to choosing a mask for entries in wfor matrix-
vector multiplication. Given a choice of the entries, the
entries that are compatible with the mask are non-zero while
others are zero. The zero entries in mleads to ignoring whole
columns in the matrix. For example, the approximation of
the multiplication of a matrix Awith the vector bmasked
with msatisfies Ab≃A(b⊙m) =A({i,mi̸=0})b({i,mi̸=0}).
This means that any sketching algorithm that compresses
matrix multiplication by selecting rows/columns can also be
used as a masking procedure for a vector in matrix-vector
multiplication.
In order to find a mask mfor the vector was described in the problem statement (Equation (1)), we
use the sketching algorithm presented in Algorithm 1. The algorithm samples an entry randomly
according to the distribution pand sets this entry to be non-zero until the desired density is achieved.
Note that the indices itare sampled i.i.d. with return and that the distribution of pis used both
for sampling and scaling. Due to the possibility of sampling the same entry more than once, the
resulting mhas granularity of ∥m∥0≤sand it is not necessarily holds that ∥m∥0=s. Additionally,
not all non-zero entries in the mask are equal.
By [18, Lemma 4] (restated in Lemma 4.1), the optimal probability for minimizing the error is
Pr
w,X[i] =∥X(i)∥|wi|
Pd
j=1∥X(j)∥|wj|, i= 1, ..., d. (2)
The probability is proportional to the norms of the rows of X, and it corresponds to a specific entry
at each example xkacross all inputs, k= 1, ..., n. Recall that X(i)∈Rn. Thus, masking an entry i
inwmeans ignoring all the ith entries in the data xk,k= 1, ..., n. Note that when sampling with (2)
it is more likely to choose weights with larger magnitudes given the data. Figures 1a and 2 show
empirically that DNNs’ pruning methods also tend to keep higher magnitude weights.
4 Pruning Approximation Error
Using the relationship between pruning and sketching drawn in Section 3, we can use the analysis
tools from sketching for examining the properties of pruning. All proofs are in the Appendix B.
In pruning at initialization, the initial vector w0is used for finding the mask, and then we approximate
the features with the same mask and another unknown vector w⋆. Typically, w⋆is the learned weights
at the end of training. The features are XTw⋆and we aim to minimize ∥XTw⋆−XT(w⋆⊙m)∥.
Note that since mis chosen randomly we bound the expected value of the approximation error.
For simplicity, we use the notation of p0
i=Prw0,X[i]as detailed in Equation (2). p0is the optimal
4

--- PAGE 5 ---
0.3
 0.2
 0.1
 0.0 0.1 0.2 0.30.000.020.040.060.080.10Probabilityuniform
SynFlow
IMP
0.20
 0.15
 0.10
 0.05
 0.00 0.05 0.10 0.15 0.200.000.010.020.030.040.050.06Probabilityuniform
SynFlow
IMP(a) Weights histogram in sparse subnetworks at initialization for
VGG-19 and CIFAR-10. Note that SynFlow and IMP have bias to
large magnitude weights compared to a uniformly random mask.
0.00 0.01 0.02 0.03 0.04 0.0502004006008001000SynFlow scores
SynFlow mask
SynFlow + randomize mask(b) Histogram of scores of NN before
pruning and the weights chosen by
SynFlow w./w.o. randomization.
0 10 20 30 40 50 60 70 80
Remaining Weights [%]05101520weight frobenius
lt layer 1
random layer 1
lt layer 2
random layer 2
lt layer 3
random layer 3
(a) Norms of winning lottery tickets and random
masks with multiple sparsities.
1.0 1.5 2.0 2.5 3.0 3.5 4.0
weight frobenius0.000.010.020.030.040.050.060.07Probabilitylayer
0
1
2(b) Norms of winning lottery tickets compared to
10,000 random masks with 1.2% density.
Figure 2: Winning tickets norms vs. random tickets with Fashion-MNIST and a fully connected NN.
probability to sample the mask at initialization. In our analysis, Xmay be given or randomly
distributed while the weighting wis assumed to be known.
First, we present a simplified version of the one found in the original sketching paper [ 18] assuming
the data Xare given. We rephrase the lemma to match the vector-matrix multiplication case.
Lemma 4.1. [Simplified version of [ 18, Lemma 4]] Suppose X∈Rd×n,w0∈Rdands∈Z+then
using Algorithm 1 with p0formthen the error is
Em|Xh
∥XTw0−XT(w0⊙m)∥2i
=1
s dX
k=1∥X(k)∥|w0
k|!2
−1
s∥XTw0∥
The lemma bounds the error of sparse approximation at initialization when the data is given. It
is proportional to1
s, which means that as the density of the mask grows the error decreases, as
expected. This connection will be consistent throughout our analysis. Next, we establish a bound of
the error when Xis drawn from a normal i.i.d. distribution, where we also observe this behavior.
Lemma 4.2. Suppose X∈Rd×n∼1√nN(0, I),w0∈Rdands∈Z+then when using Algorithm 1
withp0for drawing m, the error can be bounded as follows
EXh
∥XTw0−XT(w0⊙m)∥2i
≤1
s∥w0∥2.
Note that the above error bounds holds at initialization. Yet, in the pruning at initialization setting,
unlike matrix multiplication, we do not aim to apply a mask on a known parameter w0but on some
5

--- PAGE 6 ---
Table 1: Accuracy results for vanilla SynFlow [ 61], SynFlow with χdistributed input and with mask
randomization; and accuracy results for SNIP [ 39] with normal random data, sparse input data and
mask randomization. Above are results with CIFAR-10 and below are results with CIFAR-100.
Model Density SynFlow SynFlow + χSynFlow +
rand. maskSynFlow +
rand. mask + χSNIP +
normal dataSNIP +
rand. mask +
normal. dataSNIP +
sparse dataSNIP +
sparse data +
rand. mask
VGG-1910% 93.01±0.1993.12±0.1293.06±0.1293.12±0.1892.15±0.07 92.55 ±0.57 92.85 ±0.0693.16±0.36
5%92.68 ±0.1192.52±0.20 92.44 ±0.11 92.63 ±0.291.53±0.04 92.18 ±0.78 92.02 ±0.1892.79±0.78
2% 91.68±0.28 91.32 ±0.11 91.63 ±0.1691.71±0.2890.35±0.31 91.04 ±0.14 90.97 ±0.1192.03±0.91
ResNet-2010% 86.55±0.18 86.70 ±0.3286.74 ±0.22 86.59±0.29 85.69±0.37 86.77 ±0.41 85.85 ±0.0787.02±0.65
5% 83.19±0.36 83.28 ±0.3183.55 ±0.38 83.45±0.42 82.37±0.69 83.59 ±0.4 82.29 ±0.0183.72±0.73
2% 77.06±0.35 77.10 ±0.1277.74 ±0.51 77.74 ±0.43 77.2±0.56 78.99 ±0.73 76.98 ±0.6679.39±0.49
VGG-1910% 69.90±0.26 69.84 ±0.15 69.97 ±0.4370.24±0.2972.75±0.2272.67±0.01 72.13 ±0.08 72.49 ±0.33
5% 68.25±0.66 68.39 ±0.3168.65 ±0.33 68.32±0.08 71.48±0.72 71.34 ±0.03 71.05 ±0.1171.51±0.45
2% 65.98±0.38 65.68 ±0.23 65.87 ±0.5066.00±0.22 1.00 1.00 48.48 ±13.8853.05±15.38
ResNet-2010% 50.66±0.78 50.97 ±0.5152.29 ±0.64 51.80±0.60 66.90±0.33 67.11 ±0.62 67.32 ±0.1367.52±0.10
5% 41.12±0.92 40.87 ±0.7642.92 ±0.53 42.44±0.48 60.97±0.45 61.03 ±0.03 61.94 ±0.6062.68±0.32
2% 23.39±1.04 23.52 ±0.2724.89 ±0.75 24.35±0.72 47.61±4.15 48.86 ±0.07 50.30 ±0.0451.76±0.36
unknown parameter w⋆established after a learning procedure. Thus, we bound the approximation
error with the mask when we apply it to some unknown vector w⋆and normally distributed data.
Before we establish our main result, we provide a lemma regarding the error, when the mask is found
with some initial vector w0and data ˜Xbut applied on other input data Xand weight vector w⋆.
Lemma 4.3. Suppose X,˜X∈Rd×n,w0, w⋆∈Rdands∈Z+then when using Algorithm 1 with p0
and ˜Xform, the error can be bounded as follows
Emh
∥XTw⋆−XT(w⋆⊙m)∥2i
≤1
sdX
k=1Pd
j=1∥˜X(j)∥|w0
j|
∥˜X(k)∥|w0
k|∥X(k)∥2w⋆
k2
Next, we show the main result about the error of the mask for unknown parameters and random
data.
Theorem 4.4. Suppose X∈Rd×n∼1√nN(0, I),w0, w⋆∈Rdands∈Z+then when using
Algorithm 1 with p0formthe error can be bounded as follows
EXh
∥XTw⋆−XT(w⋆⊙m)∥2i
≤1
s∥w0∥1 
∥w⋆−w0∥2
∥w0∥∞+ 2∥w⋆−w0∥1+∥w0∥1!
.
To ensure that the bound of Theorem 4.4 is meaningful, we compare it to a simple baseline: The
error bound when the mask is sampled uniformly at random.
Lemma 4.5. Suppose X∈Rd×n∼1√nN(0, I),w0∈Rdands∈Z+. Then when choosing m
uniformly at random the error can be bounded as follows
EXh
∥XTw⋆−XT(w⋆⊙m)∥2i
≤d
s∥w⋆∥2=d
s
∥w⋆−w0∥2+ 2w⋆Tw0
−d
s∥w0∥2.
According to Lemma 4.5, we have an additional dimension factor dthat is avoided in Theorem 4.4.
Note that Theorem 4.4 bounds the expected error only as a function of the distance between the
final vector w⋆,w0and the properties of the initialization. In Section 7, we use the claim that DNNs
under common NTK assumptions do not change much during training. Thus, the error bound is
reasonable.
5 SynFlow and Sketching
In this section we analyse theoretically the connection between SynFlow [ 61] and sketching in the
linear case. We calculate the scores given by SynFlow and treat them as probabilities.
SynFlow performs a forward pass on the model with a vector of ones as input and the absolute
values of the initialized parameters. The scores are then calculated after summing up all the output
features:
RSF=1Tf(1;|w|), (3)
6

--- PAGE 7 ---
where1is an all-one vector and the multiplication with it leads to summing all outputs. In order to
establish the connection to sketching, we analyse SynFlow scores with ∥X(i)∥as input instead of 1:
RSF=1Tf(∥X(i)∥;|w|). (4)
Note that in the linear model case, where w∈Rd, it holds that
RSF= 
∥X(1)∥,∥X(2)∥, ...,∥X(i)∥, ...,∥X(d)∥
|w|.
Hence, it turns out that the SynFlow score for each weight in wis:
∂RSF
∂|w|i⊙ |w|i=∥X(i)∥|w|i,
which yields that when looking at the scores as a distribution pi∝ ∥X(i)∥|w|i. Note that in sketching,
the probability p0
i, as defined in Equation (2), is proportional to the same term and yields the
lowest expected approximation error [ 18]. Thus, it means that in the linear features setting the
SynFlow scores and the optimal probability for sketching share the same properties. Hence, if one
uses randomization over the mask with SynFlow scores as in Algorithm 1, we get the sketching
algorithm. Since we established the equivalence between sketching and SynFlow, all the claims
in Section 4 hold for it in the linear case. Note that in Theorem 4.4, we assume random normal
data as input. According to Equation (4), SynFlow inputs areqPn
i=11
nx2
i, where xiare random
normal, which implies a χdistribution vector for its input. To summarize, the relation to sketching
suggests that we should apply SynFlow with random sampling and χdistribution for its input. We
demonstrate the advantage of this approach in Section 8. Sample the entries based on phave the
same computation complexity as finding the weights with the largest sscores.
6 SNIP and Sketching
In this section we turn to analyze another well-known method of pruning at initialization named
SNIP [39], which aims to estimate the importance of each weight according to its magnitude and
gradient magnitude at initialization. The magnitude of the gradient is calculated with respect to
input data, D. SNIP assigns the following saliency score of mask at index j:
gj(w;D) =∂L(m⊙w;D)
∂mj,
where Lis the loss used for training and the value of the mask before pruning is 1in all its entries.
We analyse the case of ℓ1loss for inputs X. Previously, a statistical justification for using ℓ1loss for
DNN classification was proven [ 34] and it was further shown that using ℓ1loss can be beneficial for
robust classification [26]. The weights’ sailency score in the linear model is
gj(w;X, y) =1
n∂Pn
i=1|xT
i(w⊙m−yi)|
∂mj=1
n|wj|nX
i=1sign(xT
iw−yi)xij.
For data which has only one non-zero entry in each row of X, i.e.∥X(j)∥0= 1, it holds that the
induces probability is pj=|wj||xij,s.t.xij̸=0|
Pd
k=1|wk||xik,s.t.xik̸=0|=|wj|∥X(j)∥Pd
k=1|wk|∥X(k)∥. The induced probability in this
case is optimal from a sketching perspective. Thus, in the unsupervised case, we should apply SNIP
with random sparse data and random sampling of the mask. We validate this conclusion empirically
in Section 8.
7 Neural Tangent Kernel Pruning
We turn to apply our sketching results to the NTK regime [ 37,32,6], which was introduced for
examining the training dynamics of DNNs. Under infinite width assumption, the gradient descent
steps over a DNN become analytically tractable and similar to learning with a known kernel.
7

--- PAGE 8 ---
Table 2: Accuracy results with GraSP [ 63] and Iterative Magnitude Pruning (IMP) [ 23] with random
data used for pruning and mask randomization that replaces the thresholding. Reported accuracy
results with CIFAR-10 (top) and CIFAR-100 (bottom). We provide the supervised version of the
methods as reference and we boldface the best method with random data.
Model VGG-19 ResNet-20
Density 10% 5% 2% 10% 5% 2%
Random 91.14±0.08 89.24 ±0.22 86.28 ±0.04 85.04±0.12 72.08 ±1.38 10.0
Magnitude 93.05±0.22 92.23 ±0.9 91.28 ±0.43 84.88±0.7 81.97 ±0.52 75.15 ±0.86
GraSP (supervised) 92.21±0.69 91.68 ±1.21 90.92 ±1.28 87.28±0.04 84.37 ±0.21 79.53 ±0.06
GraSP + normal data 92.23±0.9591.23±0.91 90.9 ±1.63 86.3±0.46 83.61 ±0.1379.59±0.1
GraSP + normal data + rand. mask 92.12±0.8891.57±1.1 91.22 ±1.0886.59±0.29 84.35 ±1.3478.36±0.16
IMP (supervised) 93.56±0.19 92.64 ±0.19 89.63 ±1.57 89.63±0.24 85.99 ±1.3 82.46 ±0.73
IMP+rand. data 91.53±0.32 91.42 ±0.1990.58±0.6985.46±0.1183.07±0.11 77.97 ±0.04
IMP+rand. data+rand mask 91.57±0.03 91.45 ±0.12 90.55±0.785.5±0.15 82.77±0.13 77.54 ±0.49
Random 66.02±0.13 62.7 ±0.57 58.28 ±0.25 51.09±0.43 26.61 ±3.51 1.0 ±0.0
Magnitude 69.43±0.49 68.09 ±0.21 65.18 ±0.47 52.03±0.24 45.64 ±0.4 32.9 ±0.65
GraSP (supervised) 70.64±0.43 70.03 ±0.55 67.78 ±0.42 67.24±0.23 63.08 ±0.33 53.18 ±0.94
GraSP+normal data 70.52±0.18 70.06 ±0.29 68.07 ±0.16 67.37±0.02 63.66 ±0.0654.15±0.63
GraSP+normal data+rand. mask 71.15±0.32 70.08 ±0.01 68.53 ±0.1867.39±0.46 64.34 ±0.1553.79±0.16
IMP (supervised) 70.88±0.96 69.87 ±1.28 67.9 ±1.36 58.38±2.97 50.39 ±0.16 40.88 ±0.22
IMP + rand. data 66.84±0.1666.23±0.1754.81±6.87 51.66±0.68 43.84 ±0.17 32.92 ±0.65
IMP + rand. data + rand. mask 67.1±0.21 65.3±0.2359.05±0.1452.05±0.6 44.68 ±0.38 33.37 ±0.74
The NTK assumptions allows representing the DNN features in a linearized manner. Hence, we can
apply our previous results with sketching. Before doing that we provide some basic NTK definitions.
NTK definitions. We use the same notations as appeared in [ 37]:θt∈Rdis the vectorization of the
parameters at time t;ft(x)∈Rmis the output of the model at time t;(X,Y)are the input vectors
and labels (we assume that Yi∈R);ft(X)is a concatenation of all outputs; nis the notion of width
of the model; J(θt) =∇θft(X)∈R|X|m×dis the Jacobian; ˆΘt=∇θft(X)∇θft(X)T=1
nJ(θt)J(θt)T
is the empirical NTK. We refer to the analytic kernel as Θ = lim n→∞ˆΘ0.
For the claims in [ 37] to hold we use assumptions that are based on the original paper: (i)The
empirical NTK converges in probability to the analytic NTK: ˆΘ0→n→∞Θ;(ii)The analytic NTK
Θis full rank. 0< λ min≤λmax<∞and let ηcritical = 2(λmin+λmax)−1;(iii) (X,Y)is a compact
set and for x,˜x∈ X,x̸=˜x; and(iv)The Jacobian is locally Lipschitz as defined in Definition 7.1
(originally stated in [37, Lemma 2]):
Definition 7.1 (Local Lipschitzness of the Jacobian) .Denote B(θ0, C) ={θ:∥θ−θ0∥ ≤C}. The
Jacobian is locally Lipschitz if there exists K > 0such that for every C >0, with high probability
over random initialization the following holds for θ,˜θ∈B(θ0, C)
∥J(θ)−J(˜θ)∥F≤K∥θ−˜θ∥and∥J(θ)∥F≤K.
Note that local Lipschitzness constants are usually determined by the DNN activation functions.
NTK Pruning. We focus on the linearized approximation flin
t(x) =∇θtft(x)θtof the model
features. We aim to find a mask to apply on θtaccording to the linear features of the network at
initialization, flin
0(X)andθ0, using sketching as in Algorithm 1. Thus, we can use our previous
analysis to bound the error induced by the mask on the features at time t,flin
t(x). The following
theorem relies on the NTK bounds in [37, Theorem G.4]. Our theorem proof is in Appendix B.6.
Theorem 7.2. Under assumptions [i-iv], δ0>0andη0≤ηcritical, let F(A) =Pd
i=11
∥A(i)∥and
m∈Rdbe as-dense mask found with Algorithm 1 with paccording to flin
0(X)andθ0(Equation (2)).
Then there exist R0>0,K > 1such that for every n≥Nthe following holds with probability
>1−δ0over random initialization when applying GD with learning rate of η0:
Emh
∥flin
t(X)− ∇ θtft(X)(θt⊙m)∥2i
≤1
sK3∥θ0∥1F(J(θ0))
∥θ0∥1+F(θ0)9K4R2
0
λ2
min+ 6√
dK3R0
λmin
.
Note that for a mask density of s=O(√
d), the error of using the mask at time tonly depends on
the initialization and constants from the NTK assumptions. Additionally, ∇θtft(X)(θt⊙m)are the
8

--- PAGE 9 ---
masked linearized outputs of the model at time t. We prove the above theorem using the bound on
the Jacobian norm and the claim that under the conditions of [ 37, Theorem G.4] it is guaranteed that
the distance of the parameters at time t,θt, from the parameters at initialization, θ0, is bounded.
8 Experiments
We study our theoretical insights empirically on sparse DNNs. We tested multiple pruning at
initialization methods: SynFlow [ 61], SNIP [ 39], GraSP [ 63] and Iterative Magnitude Purning (IMP),
the algorithm suggested for finding the winning lottery ticket [23].
Table 3: Result with SynFlow and mask
randomization for Tiny-ImageNet.
Model Density SynFlowSynFlow +
random mask
ResNet-1810% 58.3 60.64
5% 57.63 58.57
2% 54.62 55.56
WideResNet10% 59.25 60.22
5% 57.49 58.84
2% 54.8 55.54We use CIFAR-10/100 [ 35] and Tiny-ImageNet [ 66] with
VGG-19 [ 59], ResNet-20 [ 30] and WideResNet-20-32 [ 69].
We employ SGD with momentum, batch size 128, and
learning rate 0.1multiplied by 0.1after epoch 80and120.
We train the models for 160epochs with weight decay
10−4. For Tiny-ImageNet, we use a modified version of
ResNet-18 and WideResNet-18. Our code is based on the
repositories of [ 22,61,60]. All the experiments performed
on a single NVIDIA GeForce RTX 2080 Ti.
We found the SynFlow mask using 100iterations. We
employ SNIP and GraSP with batch size of 256. For IMP,
we use 1000iterations of warmup for VGG-19 and 2000
iterations for ResNet-20 with CIFAR-10 and 6000for CIFAR-100 with ResNet20. In the unsupervised
experiments, we examples are drawn from a normal distribution with the same expectation and
standard deviation of the original dataset. All the results include the average and standard deviation
of 3 different seeds.
The mask randomization is performed in three steps. The first is to compute the threshold according
to the number of desired remaining weights. The second is to compute the number of remaining
weights within each layer with respect to hard thresholding. Finally, the third step is to randomly
select a mask for each layer according to the granularity found in the previous step and the scores of
the pruning method. We randomize the mask in this manner due to computational limitations.
Figure 1a shows that the magnitudes of the weights chosen by SynFlow and IMP are larger than
a uniform random choice of parameters. Note that SynFlow has a stronger bias towards larger
magnitudes than IMP. Figure 2 presents a case, where the IMP mask has an extremely high norms
compared to random masks and the high norm is maintained across sparsities. We test the effect of
randomizing the mask on the chosen scores. Figure 1b presents a comparison of the histogram of
scores of weights chosen with SynFlow with and without randomization compared to the distribution
of all the scores in the network. We report the scores of VGG-19 with CIFAR-10 and for simplicity
we present the results only on a subset of 1000weights chosen uniformly at random.
To show the effect of changes used in the theoretical analysis we tested SynFlow with mask ran-
domization and replacing the all-one vector with χdistributed random data. We generate the χ
distribution according to the ℓ2norms of a vector with normally distributed variables and dimension
n= 128. We randomize data at each pruning iteration. In order to validate empirically the conclusion
in Section 6 for the relation between SNIP and sketching, we tested SNIP with sparse random data.
We choose randomly for each input pixel location, a single image in the batch where the pixel is
non-zero. Note that the images are drawn from a normal distribution and are not natural images.
Table 1 shows that replacing the thresholding with a random mask and replacing the input with the
distribution derived from our analysis improve results in most cases when compared to the original
SynFlow and naive data-independent SNIP. For SynFlow, the use of mask randomization leads to
improved performance. The combination of sparse data and mask randomization leads to superior
accuracy for SNIP. It suggests that our sampling of mask and data can be used and improve other
pruning methods based on it, e.g. [ 2,17]. Additional results with the original supervised SNIP found
in the Appendix A. Most accuracy results with random masks and sparse data outperform SNIP
even with labeled input.
9

--- PAGE 10 ---
Our claim that finding a good sparse subnetwork at initialization does not necessarily depend on the
data is tested by an experiment where the masks are learned with completely random input and then
retrained with real supervised data. Table 2 reports the accuracy results with CIFAR-10/100. We
can see that randomized masks produce comparable and usually better results than pruning methods
with random data. Also, it can be seen that, indeed, the performance of data dependent pruning
methods is only partially explained by the use of supervised input data, as shown in [ 60]. Note that
it is expected to see some degradation in accuracy since those methods are designed to work with
labeled data.
For training with Tiny-ImageNet the improvement in performance is clear with the mask randomiza-
tion inspired by sketching, see Table 3. The improvement in accuracy is around 1%for all sparsities
and architectures tested. Overall, randomizing the masks leads to a new the state-of-the-art with
SynFlow.
9 Conclusion and Future Work
This work presents and analyzes the task of pruning at initialization from the perspective of sketching.
Based on our theoretical study, we gain a new justification for the claim that good sparse subnetworks
are data independent. Additionally, we apply ideas from sketching to DNNs. Our proposed changes
can be added to existing and future pruning methods.
While we have proposed a novel connection between pruning at initialization and sketching, our
frameworkhassomelimitations. Weapplyourapproach, onlyinthesettingofpruningatinitialization.
One might investigate our suggested approach to other settings of pruning after or during training
and structured pruning methods. In addition, we have studied only one sketching approach. We
leave for future work exploiting more complex sketching methods, which possibly can lead to new
insights and improvements to DNN pruning. Also, future research may be conducted to generalize
the rather simple linear features framework to a deeper model where the search space for the mask is
even larger. The search space in this case might be ambiguous, i.e., two different masks can lead
to the same output [ 72]. We believe that the relationship we draw here can be used in many other
directions besides those indicated above and contribute to DNN understanding and pruning.
Acknowledgements
This work is supported by the European research council under Grant ERC-StG 757497 and by Tel
Aviv University Center for AI and Data Science (TAD).
References
[1]Mohamed S Abdelfattah, Abhinav Mehrotra, Łukasz Dudziak, and Nicholas D Lane. Zero-cost
proxies for lightweight nas. arXiv preprint arXiv:2101.08134 , 2021.
[2]Milad Alizadeh, Shyam A Tailor, Luisa M Zintgraf, Joost van Amersfoort, Sebastian Far-
quhar, Nicholas Donald Lane, and Yarin Gal. Prospect pruning: Finding trainable weights at
initialization using meta-gradients. arXiv preprint arXiv:2202.08132 , 2022.
[3]Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In International Conference on Learning Representations ,
2019.
[4]Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning , pages
244–253. PMLR, 2018.
[5]Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems (NeurIPS) , pages 7413–
7424, 2019.
10

--- PAGE 11 ---
[6]Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning , pages 322–332. PMLR, 2019.
[7]Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds
for deep nets via a compression approach. In International Conference on Machine Learning ,
pages 254–263. PMLR, 2018.
[8]Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring:
Training very sparse deep networks. In International Conference on Learning Representations ,
2018.
[9]Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations , 2018.
[10]Rebekka Burkholz, Nilanjana Laha, Rajarshi Mukherjee, and Alkis Gotovos. On the existence
of universal lottery tickets. In International Conference on Learning Representations , 2021.
[11]Miguel A Carreira-Perpinán and Yerlan Idelbayev. “learning-compression” algorithms for neural
netpruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,
pages 8532–8541, 2018.
[12]Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. Advances in
neural information processing systems , 1, 1988.
[13]Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and
Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training
in computer vision models. In CVPR, 2021.
[14]Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin
Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning
framework. Advances in Neural Information Processing Systems , 34:19637–19651, 2021.
[15]Tianyi Chen, Luming Liang, Tianyu DING, Zhihui Zhu, and Ilya Zharkov. OTOv2: Automatic,
generic, user-friendly. In The Eleventh International Conference on Learning Representations ,
2023.
[16]Daiki Chijiwa, Shin’ya Yamaguchi, Yasutoshi Ida, Kenji Umakoshi, and Tomohiro Inoue. Pruning
randomly initialized neural networks with iterative randomization. arXiv:2106.09269 , 2021.
[17]Pau de Jorge, Amartya Sanyal, Harkirat S Behl, Philip HS Torr, Gregory Rogez, and Puneet K
Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. arXiv
preprint arXiv:2006.09081 , 2020.
[18]Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for matrices
i: Approximating matrix multiplication. SIAM Journal on Computing , 36(1):132–157, 2006.
[19]Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the
lottery: Making all tickets winners. In ICML. PMLR, 2020.
[20]Jonas Fischer and Rebekka Burkholz. Towards strong pruning for lottery tickets with non-zero
biases.arXiv preprint arXiv:2110.11150 , 2021.
[21]Jonas Fischer and Rebekka Burkholz. Plant’n’seek: Can you find the winning ticket? In
International Conference on Learning Representations , 2022.
[22] Jonathan Frankle. Openlth: A framework for lottery tickets and beyond, 2020.
[23]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.
[24]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode
connectivity and the lottery ticket hypothesis. In ICML, pages 3259–3269. PMLR, 2020.
11

--- PAGE 12 ---
[25]Thomas Gebhart, Udit Saxena, and Paul Schrater. A unified paths perspective for pruning at
initialization. arXiv preprint arXiv:2101.10552 , 2021.
[26]Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise
for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence , 2017.
[27]Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns.
Advances in neural information processing systems , 29, 2016.
[28]Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. Advances in neural information processing systems , 28, 2015.
[29]Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In IEEE international conference on neural networks , pages 293–299. IEEE, 1993.
[30]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[31]Nathan Hubens, Matei Mancas, Bernard Gosselin, Marius Preda, and Titus Zaharia. One-cycle
pruning: Pruning convnets under a tight training budget. arXiv:2107.02086 , 2021.
[32]Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems , 31, 2018.
[33]Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural
networks with low rank expansions. In Proceedings of the British Machine Vision Conference .
BMVA Press, 2014.
[34]Katarzyna Janocha and Wojciech Marian Czarnecki. On loss functions for deep neural networks
in classification. arXiv preprint arXiv:1702.05659 , 2017.
[35] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
[36]Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural
information processing systems , 2, 1989.
[37]Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems , 32, 2019.
[38]Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip H. S. Torr. A signal
propagation perspective for pruning neural networks at initialization. In ICLR, 2020.
[39]Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
[40]Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient
descent for matrix factorization: Greedy low-rank learning. In International Conference on
Learning Representations , 2021.
[41]Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent
transfer. In International Conference on Machine Learning , pages 6336–6347. PMLR, 2020.
[42]Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks
through l_0 regularization. In International Conference on Learning Representations , 2018.
[43]Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery
ticket hypothesis: Pruning is all you need. In ICML. PMLR, 2020.
[44]Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without
training. In International Conference on Machine Learning , pages 7588–7598. PMLR, 2021.
[45]Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connec-
tivity inspired by network science. Nature communications , 9(1):2383, 2018.
12

--- PAGE 13 ---
[46]Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440 , 2016.
[47]Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all:
generalizing lottery ticket initializations across datasets and optimizers. In NeurIPS , 2019.
[48]Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural
networks by dynamic sparse reparameterization. In International Conference on Machine
Learning , pages 4646–4655. PMLR, 2019.
[49]Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. Advances in neural information processing systems , 1, 1988.
[50]Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias
of the step size in linear diagonal neural networks. In International Conference on Machine
Learning , volume 162, pages 16270–16295, 17–23 Jul 2022.
[51]James O’ Neill. An overview of neural network compression. arXiv preprint arXiv:2006.03669 ,
2020.
[52]Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P Vetrov. Tensorizing
neural networks. Advances in neural information processing systems , 28, 2015.
[53]Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Logarithmic pruning is all you need.
Advances in Neural Information Processing Systems , 33:2925–2934, 2020.
[54]Shreyas Malakarjun Patil and Constantine Dovrolis. Phew: Constructing sparse networks that
learn fast and generalize well without training data. In International Conference on Machine
Learning , pages 8432–8442. PMLR, 2021.
[55]Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopou-
los. Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient.
arXiv:2006.07990 , 2020.
[56]Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad
Rastegari. What’s hidden in a randomly weighted neural network? In CVPR, pages 11893–11902,
2020.
[57]Matthia Sabatelli, Mike Kestemont, and Pierre Geurts. On the transferability of winning tickets
in non-natural image datasets. arXiv:2005.05232 , 2020.
[58]Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear
neural networks. In Conference on Learning Theory , volume 99, pages 2691–2713, 25–28 Jun
2019.
[59]Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 , 2014.
[60]Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee.
Sanity-checking pruning methods: Random tickets can win the jackpot. arXiv:2009.11094 , 2020.
[61]Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. Advances in Neural Information
Processing Systems , 33:6377–6389, 2020.
[62]Joost van Amersfoort, Milad Alizadeh, Sebastian Farquhar, Nicholas Lane, and Yarin Gal. Single
shot structured pruning before training. arXiv preprint arXiv:2007.00389 , 2020.
[63]Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. arXiv preprint arXiv:2002.07376 , 2020.
[64]Colin White, Arber Zela, Robin Ru, Yang Liu, and Frank Hutter. How powerful are performance
predictors in neural architecture search? Advances in Neural Information Processing Systems ,
34:28454–28469, 2021.
13

--- PAGE 14 ---
[65]Cameron R Wolfe, Qihan Wang, Junhyung Lyle Kim, and Anastasios Kyrillidis. Provably
efficient lottery ticket discovery. arXiv:2108.00259 , 2021.
[66] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny imagenet challenge. Technical report , 2017.
[67]Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G
Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Towards more
efficient training of deep networks. arXiv:1909.11957 , 2019.
[68]Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in
training linear neural networks. In International Conference on Learning Representations , 2021.
[69]Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146 , 2016.
[70]Zeru Zhang, Jiayin Jin, Zijie Zhang, Yang Zhou, Xin Zhao, Jiaxiang Ren, Ji Liu, Lingfei Wu,
Ruoming Jin, and Dejing Dou. Validating the lottery ticket hypothesis with inertial manifold
theory.Advances in Neural Information Processing Systems , 34:30196–30210, 2021.
[71]Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang. Efficient lottery ticket finding:
Less data is more. In ICML, pages 12380–12390. PMLR, 2021.
[72]Léon Zheng, Elisa Riccietti, and Rémi Gribonval. Identifiability in two-layer sparse matrix
factorization. complexity , 20(19):28–29, 2021.
14

--- PAGE 15 ---
A Results with Supervised SNIP
Table 4: Accuracy results with the original supervised SNIP and unsupervised SNIP with sparse data
and mask randomization. Above are results with CIFAR-10 and below are results with CIFAR-100.
Model VGG-19 ResNet20
Density 10% 5% 2% 10% 5% 2%
SNIP (supervised) 92.62±0.09 91.66 ±0.25 90.92 ±0.16 86.29±0.92 82.55 ±1.02 78.16 ±0.8
SNIP + sparse data + rand. mask 93.16±0.36 92.79 ±0.78 92.03 ±0.9187.02±0.65 83.72 ±0.73 79.39 ±0.49
SNIP (supervised) 71.80±0.66 71.07 ±0.2356.05±13.97 67.03±0.04 61.80 ±0.38 49.42 ±0.65
SNIP + sparse data + rand. mask 72.49±0.33 71.51 ±0.45 53.05±15.3867.52±0.10 62.68 ±0.32 51.76 ±0.36
B Proofs for Sketching and SynFlow
In this section we include the proofs of the claims presented in the main paper regarding the SynFlow
and sketching.
B.1 Useful Lemmas and Proofs
First we present useful lemmas and proofs that are used to prove the lemmas and theorems in the
paper.
We calculate the expectation of the mask with some unknown w⋆vector.
Lemma B.1. Formfound with Algorithm 1 and p0. Then for w⋆∈Rdit holds,
E
(XT(w⋆⊙m))i
= (XTw⋆)i
.
Lemma B.1. Fix an index iand denote the random variable Y0⋆
t=Xitiw⋆
it
sp0
it(itis random and hence
p0
itis random), it holds thatPs
t=1Y0⋆
t=Pd
t=1Xitiw⋆
it·1
sp0
it= (XT(w⋆⊙m))i.
Eit
Y0⋆
t
=dX
k=1p0
kXkiw⋆
k
sp0
k=1
s(XT
(i)w⋆) =1
s(XTw⋆)i.
Sum over srandom variables we have
Eit
(XT(w⋆⊙m))i
=sX
t=1E
Y0⋆
t
= (XTw⋆)i.
We calculate the variance of each mask entry when the mask is applied on w⋆.
Lemma B.2. Formfound with Algorithm 1 with p0. Then for w⋆∈Rdit holds,
Var 
XT(w⋆⊙m)
i
=1
sdX
k=1(Xki)2(w⋆
k)2
p0
k−1
s 
XTw⋆2
i
Lemma B.2. According to the variance of independent variables
Var 
XT(w⋆⊙m)
i
=sX
t=1Var
Y0⋆
t
=sX
t=1E[(Y0⋆
t)2]−E[Y0⋆
t]2.
Then we focus on E[(Y0⋆
t)2]calculation,
E[(Y0⋆
t)2] =dX
k=1(Xki)2(w⋆
k)2
s2p0
k.
Then the use of Lemma B.2 concludes the proof.
15

--- PAGE 16 ---
B.2 Proof of Lemma 4.3
The following lemma supports Theorem 4.4 and Theorem 7.2. This lemma establish the variance
of each masked entry when the mask is found with w0and ˜Xbut applied on other input data and
weight vector.
Lemma 4.3. Suppose X,˜X∈Rd×n,w0, w⋆∈Rdands∈Z+then when using Algorithm 1 with p0
and ˜Xformthe error can be bounded
Emh
∥XTw⋆−XT(w⋆⊙m)∥2i
=1
sdX
k=11
p0
k∥X(k)∥2w⋆
k2−1
s∥XTw⋆∥2
≤1
sdX
k=1Pd
j=1∥˜X(j)∥|w0
j|
∥˜X(k)∥|w0
k|∥X(k)∥2w⋆
k2
Proof. mis drawn i.i.d.
Emh
∥XTw⋆−XT(w⋆⊙m)∥2i
=nX
i=1Em
(XTw⋆−XT(w⋆⊙m))2
i
=nX
i=1(XTw⋆)2
i−2(XTw⋆)iEm
(XT(w⋆⊙m))i
+Em
(XT(w⋆⊙m))2
i
Lemma B. 1=nX
i=1Varm
(XT(w⋆⊙m))i
Lemma B. 2=1
sdX
k=11
p0
k nX
i=1Xki2!
(w⋆
k)2−1
s∥XTw⋆∥2
≤1
sdX
k=11
p0
k nX
i=1Xki2!
(w⋆
k)2
p0definition=1
sdX
k=1Pd
j=1∥˜X(j)∥|w0
j|
∥˜X(k)∥|w0
k|∥X(k)∥2w⋆
k2
Now, we can repeat the lemmas and theorem from the main paper and present their proofs.
B.3 Proof of Lemma 4.2
Lemma 4.2. Suppose X∈Rd×n∼1√nN(0, I),w0∈Rdands∈Z+then when using Algorithm 1
withp0formthe error can be bounded
EXh
∥XTw0−XT(w0⊙m)∥2i
≤1
s∥w0∥2
16

--- PAGE 17 ---
Proof.
EXh
∥XTw0−XT(w0⊙m)∥2i
=EXh
Em|Xh
∥XTw0−XT(w0⊙m)∥2ii
]
Lemma 4.1= EX
1
s dX
k=1∥X(k)∥|w0
k|!2
−1
s∥XTw0∥

≤EX
1
s dX
k=1∥X(k)∥|w0
k|!2

≤1
sEX"dX
k=1∥X(k)∥2(w0
k)2#
=1
sdX
k=1EXh
∥X(k)∥2i
(w0
k)2=n
sn∥w0∥2
B.4 Proof of Theorem 4.4
Theorem 4.4. Suppose X∈Rd×n∼1√nN(0, I),w0, w⋆∈Rdands∈Z+then when using
Algorithm 1 with p0formthe error can be bounded
EXh
∥XTw⋆−XT(w⋆⊙m)∥2i
≤1
s∥w0∥1 
∥w⋆−w0∥2
∥w0∥∞+ 2∥w⋆−w0∥1+∥w0∥1!
.
Proof.We calculate the expectation over X. Since Xrows are i.i.d. we replace E∥X(k)∥inE∥X(.)∥
since for all kthe norm is the same.
EXh
∥XTw⋆−XT(w⋆⊙m)∥2iLemma 4.3= EX"
1
sdX
k=1Pd
j=1∥X(j)∥|w0
j|
|w0
k|∥X(k)∥w⋆
k2#
=1
sdX
k=1w⋆
k2
EXh
∥X(k)∥2i
+1
|w0
k|X
j̸=k|w0
j|EX
∥X(k)∥
EX
∥X(j)∥

=1
sdX
k=1EXh
∥X(.)∥2i
w⋆
k2+EX
∥X(.)∥2w⋆
k2
|w0
k|X
j̸=k|w0
j|
Note that according to Xdistribution,
EXh
∥X(.)∥2i
=E"nX
i=11
nx2
i#
= 1,
EX
∥X(.)∥2=1
nE
vuutnX
i=1x2
i
2
=2
nΓ((n+ 1)/2)2
Γ(n/2)2≤1.
Where Γ(·)is the gamma function.
17

--- PAGE 18 ---
Calculate the result as a function of ∥w⋆−w0∥:
1
sdX
k=1w⋆
k2+w⋆
k2
|w0
k|X
j̸=k|w0
j|=1
sdX
k=1w⋆
k2
|w0
k|dX
j=1|w0
j|
=1
s∥w0∥1dX
k=1w⋆
k
|w0
k|
=1
s∥w0∥1dX
k=1(w⋆
k−w0
k)2
|w0
k|+ 2sign(w0
k)w⋆
k− |w0
k|
≤1
s∥w0∥1 
∥w⋆−w0∥2
∥w0∥∞+ 2∥w⋆∥1− ∥w0∥1!
≤1
s∥w0∥1 
∥w⋆−w0∥2
∥w0∥∞+ 2∥w⋆−w0∥1+∥w0∥1!
Where the last inequality holds from the reverse triangle inequality and addition and subtraction of
∥w0∥1.
B.5 Proof of Lemma 4.5
Lemma 4.5. Suppose X∈Rd×n∼1√nN(0, I),w0∈Rdands∈Z+, then when choosing m
uniformly at random the error can be bounded
EXh
∥XTw⋆−XT(w⋆⊙m)∥2i
≤d
s∥w⋆∥2=d
s
∥w⋆−w0∥2+ 2w⋆Tw0
−d
s∥w0∥2.
Proof.This proof is in the same form as Theorem 4.4 and we start by establishing the equivalence to
Lemmas B.1 and B.2. In order to maintain the estimation of the features, when mi̸= 0it holds that
mi=d
s(as stated in Algorithm 1, mi=1
spi).
Em
(XT(w⋆⊙m)i
=sX
t=11
d(XTw⋆)id
s= (XTw⋆)i
Next we analyse the variance Varm
(XT(w⋆⊙m))i
. Fix an index iand let YUni⋆
t =Xitiw⋆
it
spUni
it=
dXitiw⋆
it
sWe look at
Var 
XT(w⋆⊙m)
i
=sX
t=1Var
YUni⋆
t
=sX
t=1E[(YUni⋆
t)2]−E[YUni⋆
t]2.
Emh
YUni⋆
t2i
=dX
k=1X2
ki(w⋆
k)2
sd
s
Overall we can conclude that
Varm
(XT(w⋆⊙m))i
=d
sdX
k=1X2
ki(w⋆
k)2−1
s(XTw⋆)2
i.
Finally we calculate the expectation over Xand we have
EX,m
∥XTw⋆−XT(w⋆⊙m)∥
≤d
s∥w⋆∥2=d
s
∥w⋆−w0∥2+ 2w⋆Tw0
−d
s∥w0∥2
18

--- PAGE 19 ---
B.6 Proof of Theorem 7.2
Theorem 7.2. Under [1-4] assumptions, δ0>0andη0≤ηcritical. Let m∈Rdbe a s-dense
mask found with Algorithm 1 with paccording to flin
0(X)andθ0(Equation (2)). Then there exist
R0>0,K > 1such that for every n≥Nthe following holds with probability >1−δ0over random
initialization when applying GD with η0
Emh
∥flin
t(X)− ∇ θtft(X)(θt⊙m)∥2i
≤1
sK3∥θ0∥1F(J(θ0))·

∥θ0∥1+F(θ0)9K4R2
0
λ2
min+ 6√
dK3R0
λmin
,
where F(A) =Pd
i=11
∥A(i)∥.
Proof.We start with computing the expected error of approximation of the features at time tusing
Lemma 4.3. Note that we use the jacobian at initialization J(θ0)and the initial parameters θ0to
find the mask m.
Em
∥flin
t(X)− ∇ θtft(X)(θt⊙m)∥
=Emh
∥J(θt)θt−J(θt)(θt⊙m)∥2i
≤1
sdX
k=1∥J(θt)(k)∥2
∥J(θ0)(k)∥θ2
tk
|θ0k|dX
j=1∥J(θ0)(j)∥|θ0j|
≤1
s
dX
j=1∥J(θ0)(j)∥|θ0j|
 dX
k=1∥J(θt)(k)∥2
∥J(θ0)(k)∥! dX
k=1θ2
tk
|θ0k|!
Next we bound each term.
dX
k=1θ2
tk
|θ0k|=dX
k=1(θtk−θ0k)2
|θ0k|+ 2θtksign(θ0k)− |θ0k|
≤ dX
k=11
|θ0k|!
∥θt−θ0∥2+ 2∥θt∥1− ∥θ0∥1
≤ dX
k=11
|θ0k|!
∥θt−θ0∥2+ 2∥θt−θ0∥1+∥θ0∥1
Where the last transition is done we the reverse triangle inequality and subtraction and addition of
∥θ0∥1. Hence when using the bound in Theorem G.4 and ∥a∥1≤√
d∥a∥2we have,
dX
k=1θ2
tk
|θ0k|≤ ∥θ0∥1+ dX
k=11
|θ0k|!3KR 0
λmin2
+ 6√
dKR 0
λmin
We bound the next term,

dX
j=1∥J(θ0)(j)∥|θ0j|
 dX
k=1∥J(θt)(k)∥2
∥J(θ0)(k)∥!
≤
dX
j=1∥J(θ0)(j)∥|θ0j|
 dX
k=11
∥J(θ0)(k)∥! dX
k=1∥J(θt)(k)∥2!
=
dX
j=1∥J(θ0)(j)∥|θ0j|
 dX
k=11
∥J(θ0)(k)∥!
∥J(θt)∥2
F
≤
dX
j=1∥J(θ0)(j)∥|θ0j|
 dX
k=11
∥J(θ0)(k)∥!
K2
19

--- PAGE 20 ---
We can also bound,

dX
j=1∥J(θ0)(j)∥|θ0j|
≤dX
j=1∥J(θ0)∥F|θ0j| ≤K∥θ0∥1
Overall we can bound the error with the state of the model at initialization
Emh
∥J(θt)θt−J(θt)(θt⊙m)∥2i
≤
≤1
sK3∥θ0∥1 dX
k=11
∥J(θ0)(k)∥! 
∥θ0∥1+ dX
k=11
|θ0k|!3K2R0
λmin2
+ 6√
dK3R0
λmin!
≤1
sK3∥θ0∥1F(J(θ0))
∥θ0∥1+F(θ0)9K4R2
0
λ2
min+ 6√
dK3R0
λmin
Where F(A) =Pd
i=11
∥A(i)∥.
20
