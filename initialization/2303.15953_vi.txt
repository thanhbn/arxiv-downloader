# Các Mạng Con Được Khởi Tạo Ngẫu Nhiên với Tái Chế Trọng Số Lặp Đi Lặp Lại
Matt Gorbett1Darrell Whitley1

## Tóm tắt
Giả thuyết Vé Số Đa Giải thưởng khẳng định rằng các mạng neural được khởi tạo ngẫu nhiên chứa một số mạng con đạt được độ chính xác tương đương với các mô hình được huấn luyện đầy đủ có cùng kiến trúc. Tuy nhiên, các phương pháp hiện tại yêu cầu mạng phải được tham số hóa quá mức đủ. Trong công trình này, chúng tôi đề xuất một sửa đổi cho hai thuật toán tiên tiến (Edge-Popup và Biprop) để tìm các mạng con có độ chính xác cao mà không cần thêm chi phí lưu trữ hoặc mở rộng. Thuật toán, Tái Chế Trọng Số Lặp Đi Lặp Lại, xác định các tập con của các trọng số quan trọng trong một mạng được khởi tạo ngẫu nhiên để tái sử dụng trong lớp. Về mặt thực nghiệm, chúng tôi cho thấy sự cải thiện trên các kiến trúc mạng nhỏ hơn và tỷ lệ cắt tỉa cao hơn, phát hiện ra rằng độ thưa thớt của mô hình có thể được tăng lên thông qua việc "tái chế" các trọng số hiện có. Ngoài Tái Chế Trọng Số Lặp Đi Lặp Lại, chúng tôi bổ sung cho Giả thuyết Vé Số Đa Giải thưởng với một phát hiện tương hỗ: các mạng con có độ chính xác cao, được khởi tạo ngẫu nhiên tạo ra các mặt nạ đa dạng, mặc dù được tạo ra với cùng siêu tham số và chiến lược cắt tỉa. Chúng tôi khám phá các cảnh quan của những mặt nạ này, cho thấy tính biến đổi cao.

## 1. Giới thiệu
Giả thuyết Vé Số (Frankle & Carbin, 2019) đã chứng minh rằng các Mạng Neural Sâu (DNN) được khởi tạo ngẫu nhiên chứa các mạng con thưa thớt mà khi được huấn luyện riêng lẻ, đạt được độ chính xác tương đương với một mạng dày đặc được huấn luyện đầy đủ có cùng cấu trúc. Kết quả của giả thuyết này chỉ ra rằng các DNN được tham số hóa quá mức không còn cần thiết nữa; thay vào đó, việc tìm các mạng con thưa thớt "vé trúng" có thể mang lại các mô hình có độ chính xác cao. Hệ quả của các vé trúng là phong phú trong sử dụng thực tế: chúng ta có thể huấn luyện DNN với chi phí tính toán giảm (Morcos et al., 2019) bao gồm tiêu thụ bộ nhớ và thời gian suy luận, và ngoài ra cho phép dân chủ hóa rộng rãi các DNN với dấu chân carbon thấp.

Mở rộng Giả thuyết Vé Số, Ramanujan et al. (Ramanujan et al., 2020) đã báo cáo một phát hiện đáng chú ý: chúng ta không cần huấn luyện mạng neural nào cả để tìm vé trúng. Thuật toán của họ, Edge-Popup, đã phát hiện các mạng con thưa thớt trong các DNN được khởi tạo ngẫu nhiên mà đạt được độ chính xác tương đương với các mô hình được huấn luyện đầy đủ. Hiện tượng này đã được chứng minh toán học trong Giả thuyết Vé Số Mạnh (Malach et al., 2020). Về mặt thực tế, phát hiện này cho thấy rằng tối ưu hóa trọng số dựa trên gradient không cần thiết để một mạng neural đạt được độ chính xác cao. Hơn nữa, nó cho phép chúng ta vượt qua những khó khăn của việc thưa thớt hóa dựa trên gradient, chẳng hạn như bị kẹt tại cực tiểu địa phương và lan truyền ngược không tương thích (Diffenderfer & Kailkhura, 2021). Cuối cùng, các mạng con "vé trúng" được khởi tạo ngẫu nhiên đã được chứng minh là mạnh mẽ hơn các phương pháp cắt tỉa khác (Diffenderfer et al., 2021).

Mặc dù có khám phá hấp dẫn này, nó cũng đánh dấu một hạn chế chính đối với công việc hiện có: các DNN được khởi tạo ngẫu nhiên yêu cầu một số lượng lớn tham số để đạt được độ chính xác cao. Nói cách khác, để đạt được cùng mức hiệu suất như các mạng dày đặc được huấn luyện với tối ưu hóa trọng số, các mô hình được khởi tạo ngẫu nhiên cần nhiều tham số hơn, và do đó cần nhiều không gian bộ nhớ hơn. Các công trình tiếp theo đã nới lỏng các ràng buộc được đề xuất bởi Giả thuyết Vé Số Mạnh (Pensia et al., 2020; Orseau et al., 2020), cho thấy về mặt toán học rằng chiều rộng mạng chỉ cần rộng hơn logarithmic so với các mạng dày đặc. Chijiwa et. al (Chijiwa et al., 2021) đã đề xuất một sửa đổi thuật toán cho Edge-Popup, ngẫu nhiên hóa lặp đi lặp lại (IteRand), cho thấy rằng chúng ta có thể giảm chiều rộng mạng cần thiết cho việc cắt tỉa trọng số xuống cùng mức với một mô hình được huấn luyện đầy đủ lên đến các hệ số hằng số.

Ngoài những phát hiện này, Giả thuyết Vé Số Đa Giải thưởng (Diffenderfer & Kailkhura, 2021) đã cho thấy có một số mạng con (Vé Đa Giải thưởng (MPT)) trong các mô hình được khởi tạo ngẫu nhiên đạt được độ chính xác cao so với các mạng dày đặc. Quan trọng hơn, các tác giả đã chuyển đổi phát hiện này thành Mạng Neural Nhị phân (BNN), nơi họ đề xuất một thuật toán mới (Biprop) để xác định vé trúng trong các BNN được khởi tạo ngẫu nhiên. Hàm ý của phát hiện này cho phép nén cực kỳ các mô hình lớn, được tham số hóa quá mức.

Trong công trình này, chúng tôi đề xuất một thuật toán để tìm các mạng con thưa thớt chính xác trong các DNN và BNN được khởi tạo ngẫu nhiên. Cách tiếp cận của chúng tôi khai thác các trọng số hiện có trong một lớp mạng, xác định các tập con của các trọng số tầm thường và thay thế chúng bằng các trọng số có ảnh hưởng đến một mạng con mạnh. Chúng tôi trình bày kết quả của mình trong Hình 9, cho thấy sự cải thiện trên nhiều kiến trúc và tỷ lệ cắt tỉa khác nhau. Ngoài ra, chúng tôi cung cấp xác nhận cho Giả thuyết Vé Số Đa Giải thưởng, cho thấy bằng chứng rằng các mạng con được tạo ra dưới điều khiển siêu tham số chặt chẽ thể hiện cấu trúc khác biệt cơ bản.

Đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất một thuật toán mới, Tái Chế Trọng Số Lặp Đi Lặp Lại, cải thiện khả năng tìm các mạng con thưa thớt có độ chính xác cao trong các mạng neural được khởi tạo ngẫu nhiên. Thuật toán này là một cải tiến cho cả Edge-Popup (cho DNN) cũng như Biprop (BNN). Thuật toán xác định k trọng số ngoại lai trong một lớp mô hình và thay thế chúng bằng k giá trị trọng số liên quan.

• Chúng tôi kiểm tra các MPT được tạo ra dưới điều khiển siêu tham số nghiêm ngặt, cho thấy rằng, dưới điều kiện gần như giống hệt nhau, các MPT hiển thị cấu trúc mặt nạ đa dạng. Những kết quả này chỉ ra rằng, không chỉ tồn tại nhiều vé số trong các mạng neural được khởi tạo ngẫu nhiên, mà thực sự là sự phong phú của các vé số.

## 2. Bối cảnh
Trong phần này, chúng tôi xem xét các phương pháp tiên tiến hiện tại để cắt tỉa các mạng neural được khởi tạo ngẫu nhiên và nhị phân được khởi tạo ngẫu nhiên.

**DNN được Khởi tạo Ngẫu nhiên** Cho một mạng neural f(x;θ) với các lớp θ₁,...θₗ, các tham số trọng số θ ∈ Rⁿ được lấy mẫu ngẫu nhiên từ phân phối D trên R, và tập dữ liệu x, chúng ta có thể biểu diễn một mạng con của f(x;θ) là f(x;θ⊙M), trong đó M ∈ {0,1}ⁿ là một mặt nạ nhị phân và ⊙ là tích Hadamard.

Edge-popup (Ramanujan et al., 2020) tìm M trong một DNN được khởi tạo ngẫu nhiên bằng cách tối ưu hóa tham số đánh điểm trọng số S ∈ Rⁿ trong đó S ~ D_score. S_i có thể được hiểu trực quan như một điểm số quan trọng được tính cho mỗi trọng số θᵢ. Thuật toán lấy siêu tham số tỷ lệ cắt tỉa p ∈ [0,1], và trong lần truyền tiến tính M tại M_i là:

M_i = {1 nếu j|S_j| ∈ {σ(i)ₖⱼ=₁[k ≤ |p·100|]}
      {0 ngược lại                                    (1)

trong đó σ sắp xếp các chỉ số {θᵢ}ⁿᵢ=₁ ∈ S sao cho |S_σ(i)| ≥ |S_σ(i+1)|. Nói cách khác, các mặt nạ được tính tại mỗi trọng số bằng cách lấy giá trị tuyệt đối của điểm số cho mỗi lớp, và đặt mặt nạ thành 1 nếu giá trị điểm số tuyệt đối nằm trong top 100*p%, ngược lại họ đặt mặt nạ thành zero. Họ sử dụng ước tính thẳng (Bengio et al., 2013) để lan truyền ngược qua mặt nạ và cập nhật S thông qua SGD.

Chijiwa et. al (Chijiwa et al., 2021) đã cải thiện thuật toán Edge-Popup với thuật toán IteRand. Họ cho thấy rằng bằng cách ngẫu nhiên hóa lại các trọng số mạng bị cắt tỉa trong quá trình huấn luyện, có thể tìm được các mạng con tốt hơn. Họ chứng minh kết quả của mình về mặt lý thuyết sử dụng một định lý xấp xỉ chỉ ra rằng các phép toán ngẫu nhiên hóa lại hiệu quả làm giảm số lượng tham số cần thiết để đạt được các mạng con có độ chính xác cao.

Thuật toán IteRand chủ yếu được điều khiển bởi hai siêu tham số: K_per và tỷ lệ ngẫu nhiên hóa lại r. K_per điều khiển tần suất các trọng số sẽ được ngẫu nhiên hóa lại. Siêu tham số thứ hai, r, biểu thị một ngẫu nhiên hóa lại một phần của các trọng số bị cắt tỉa. Để đạt được kết quả tốt nhất, các tác giả đặt r thành 0.1, có nghĩa là ngẫu nhiên hóa lại 10% các trọng số bị cắt tỉa.

**Thuật toán 1 Edge-Popup với IteRand**
1: **Require:** D_weight; S ~ D_score, p, K_per, r
2: **Input:** Dataset (X,Y)
3: **function** EDGEPOPUP(S, M, f(x))
4:   **for each** l ∈ L **do**
5:     **if** |s_i| ∈ top k|S_l| **then** M_i = 1 **else** M_i = 0 **end if**
6:   **end for**
7:   **return** S, M
8: **end function**
9: **for** i=1 ..., N-1 **do**
10:   x, y ← MINIBATCH(X, Y)
11:   S, M ← EDGE-POPUP(S, M, f(x))
12:   **if** i mod K_per = 0 **then**
13:     Rerandomize(θ, M)
14:   **end if**
15: **end for**

**BNN được Khởi tạo Ngẫu nhiên** Bổ sung cho các phát hiện được báo cáo trong phần trước, Diffenderfer và Kailkhura (Diffenderfer & Kailkhura, 2021) đã mô tả một phương pháp mới để tìm các mạng con có độ chính xác cao trong các mô hình có trọng số nhị phân. Phát hiện này cung cấp cho chúng ta khả năng lưu trữ các trọng số kích thước bit thay vì số dấu phẩy động (32 bit), dẫn đến nén đáng kể các mô hình lớn. Trong phần này, chúng tôi tóm tắt thuật toán Biprop.

Chúng ta bắt đầu với một sửa đổi của hàm được mô tả trong phần trước, thay thế θ ∈ Rⁿ bằng các trọng số nhị phân B ∈ {-1,1}. Hàm mạng kết quả trở thành f(x; B⊙M), với mặt nạ M trên các trọng số nhị phân. Hơn nữa, Biprop giới thiệu tham số tỷ lệ α ∈ R, sử dụng các trọng số dấu phẩy động trước khi nhị phân hóa (Martinez et al., 2020). Tham số được học α điều chỉnh lại các trọng số nhị phân thành {-α, α}, và hàm mạng kết quả trở thành f(x; α(B⊙M)). Tham số α được cập nhật với |αM|₁ = ||αM||₁, với M được nhân với α cho gradient descent (ước tính thẳng vẫn được sử dụng cho lan truyền ngược). Trong thời gian kiểm tra, tham số alpha đã học đơn giản chỉ điều chỉnh một vector trọng số được nhị phân hóa. Kết quả là, chỉ cần biểu diễn bit của các trọng số tại các giá trị mặt nạ dương (±1 trong đó M = 1), giảm đáng kể bộ nhớ, lưu trữ và chi phí suy luận.

Về mặt thực nghiệm, Diffenderfer và Kailkhura (Diffenderfer & Kailkhura, 2021) có thể tạo ra các mạng con nhị phân có độ chính xác cao sử dụng Biprop trên một loạt các kiến trúc mạng, và chứng minh kết quả này về mặt lý thuyết trên các mô hình có tham số hóa quá mức đủ. Trong phần tiếp theo, chúng tôi cho thấy cách chúng ta có thể sửa đổi thuật toán này, cũng như Edge-Popup, với Tái Chế Trọng Số để đạt được hiệu suất tăng lên.

## 3. Tái Chế Trọng Số Lặp Đi Lặp Lại
Trong phần này, chúng tôi chi tiết Tái Chế Trọng Số Lặp Đi Lặp Lại, trước tiên tóm tắt phương pháp luận đằng sau cách tiếp cận, và sau đó chi tiết thiết lập thí nghiệm và kết quả. Cuối cùng chúng tôi thực hiện phân tích thực nghiệm về thuật toán, với kết quả cho thấy rằng Tái Chế Trọng Số Lặp Đi Lặp Lại nhấn mạnh việc giữ các giá trị trọng số có chuẩn cao tương tự như kỹ thuật cắt tỉa L1 truyền thống.

### 3.1. Phương pháp
Chúng tôi xem xét f(x;θ) như một mạng neural l-lớp với các kích hoạt ReLU, tập dữ liệu x ∈ Rⁿ với các tham số trọng số θ ~ D_weight. Chúng tôi đóng băng θ và ngoài ra tắt số hạng bias cho mỗi l. Mô hình của chúng tôi được khởi tạo tương tự như Edge-Popup và Biprop: một tham số điểm số S cho mỗi θ, trong đó S_i học tầm quan trọng của θ_i. Ngoài ra, chúng tôi đặt tỷ lệ cắt tỉa p ∈ [0,1]. D_weight được khởi tạo sử dụng khởi tạo Kaiming Normal (không có tỷ lệ fan) cho Biprop và Khởi tạo Hằng số Có dấu cho Edge-Popup. Hơn nữa, D_score được khởi tạo với Kaiming Uniform với seed 0, ngoại trừ trong Phần 4, nơi chúng tôi khám phá các khởi tạo S khác nhau.

Tái chế trọng số hoạt động trên cơ sở lặp đi lặp lại, tương tự như IteRand (Chijiwa et al., 2021). Chúng tôi định nghĩa hai siêu tham số, K_per và r, trong đó K_per là tần suất chúng tôi thay đổi trọng số, và r là tỷ lệ tái chế. Trong giai đoạn tái chế, chúng tôi tính k là số lượng trọng số chúng tôi muốn thay đổi trong một lớp cho trước là |θ_l|r, trong đó |θ_l| là kích thước của S tại lớp l và r ∈ [0,1]. Chúng tôi truy xuất các tập con S^low_l, S^high_l ⊆ S_l chứa k điểm số tuyệt đối thấp nhất và k điểm số tuyệt đối cao nhất tại mỗi lớp:

S^low_l = {σ(i)^k_{i=1}}, S^high_l = {σ(i)^j_{i=j-k}} (2)

trong đó σ sắp xếp {θ_i}^j_{i=1} ∈ S sao cho |S_σ(i)| ≤ |S_σ(i+1)|. Ở đây, {θ_i}^j_{i=1} tương đương với các giá trị chỉ số liên kết với tập {|S_j|}^j_{i=1}. Tiếp theo, chúng tôi truy xuất các giá trị trọng số liên kết với S^high_l và S^low_l, với {θ_i,...,θ_k} ∈ S = {θ_i,...,θ_k}. Cuối cùng, chúng tôi đặt θ^low_l = θ^high_l. Hiệu quả, thuật toán Tái Chế Trọng Số Lặp Đi Lặp Lại tìm các giá trị S và chỉ số liên kết của chúng (trong đó θ_i ∈ S = θ_i) và truy xuất giá trị trọng số liên kết với chỉ số, cho cả điểm số S cao và thấp. Thuật toán thay thế các giá trị trọng số S thấp bằng các giá trị trọng số S cao, loại bỏ các giá trị trọng số S thấp. Thuật toán 2 biểu thị phương trình dưới dạng mã giả.

**Thuật toán 2 Tái Chế Trọng Số. Thay thế dòng 14 trong Thuật toán 1 bằng phương pháp sau**
1: **function** WEIGHT RECYCLE(S, θ)
2:   **for each** l ∈ L **do** . lớp có kích thước j
3:     k ← |θ_l|r . Tính số lượng trọng số cần thay đổi
4:     S^high_l ← highest k|S_l| . Truy xuất chỉ số của top k giá trị abs(score)
5:     S^low_l ← lowest k|S_l| . Truy xuất chỉ số của bottom k giá trị abs(score)
6:     θ_l[S^low_l] ← θ_l[S^high_l] . Thay thế θ_l thấp bằng θ_l cao
7:   **end for**
8:   **return** S, M
9: **end function**

### 3.2. Thiết lập Thí nghiệm
Để bắt đầu, chúng tôi sử dụng các kiến trúc mô hình và tập dữ liệu tương tự như ba công trình trước đó. Conv-2 đến Conv-8 là các CNN giống VGG (Simonyan & Zisserman, 2015) với độ sâu d = 2 đến 8. Chúng tôi cũng sử dụng các tương tự "rộng" của chúng, giới thiệu một tham số tỷ lệ tại mỗi lớp để ảnh hưởng đến chiều rộng cụ thể của mỗi chiều rộng lớp w = 0.1 đến 1. Ngoài ra, chúng tôi sử dụng ResNets (He et al., 2015), sử dụng các kết nối bỏ qua và chuẩn hóa batch. Chúng tôi kiểm tra các mô hình trên cả tập dữ liệu CIFAR-10 và ImageNet. Biến đổi không affine được sử dụng cho tất cả các thí nghiệm CIFAR-10, và ResNets sử dụng chuẩn hóa batch đã học tương tự như (Diffenderfer & Kailkhura, 2021). Chúng tôi áp dụng các tỷ lệ cắt tỉa tương tự như các công trình trước đó {0.2, 0.4, 0.5, 0.6, 0.8, 0.9}, và ngoài ra kiểm tra phương pháp của chúng tôi ở các tỷ lệ cắt tỉa trên 0.9. Trong các thí nghiệm Tái Chế Trọng Số Lặp Đi Lặp Lại, chúng tôi sử dụng ba khởi tạo khác nhau, và báo cáo độ chính xác trung bình, với các thanh lỗi biểu thị độ chính xác thấp nhất và cao nhất.

Chúng tôi so sánh hiệu suất của Tái Chế Trọng Số Lặp Đi Lặp Lại với các thuật toán Edge-Popup, Biprop và IteRand sử dụng cùng siêu tham số. Đối với mỗi thuật toán cơ sở, chúng tôi sử dụng các siêu tham số mang lại kết quả tốt nhất trong các bài báo gốc: khởi tạo Hằng số Có dấu cho Edge-Popup/IteRand, và Kaiming Normal với tỷ lệ fan cho Biprop. Đối với thuật toán của chúng tôi, chúng tôi sử dụng các chiến lược khởi tạo tương tự này, ngoại trừ đối với Biprop với Tái Chế Trọng Số, chúng tôi không sử dụng tỷ lệ fan vì điều này mang lại kết quả tốt hơn một chút. Ngoài ra, đối với IteRand, chúng tôi sử dụng cùng K_per và r như trong bài báo: K_per = 1 (một lần mỗi epoch), với r = 0.1. Đối với thuật toán của chúng tôi, chúng tôi chọn K_per = 10 và r = 0.2 cho tất cả các mô hình. Chúng tôi thấy rằng tái chế ít thường xuyên hơn mang lại kết quả tốt hơn, giả thuyết rằng tái chế quá thường xuyên tạo ra các giá trị dư thừa.

### 3.3. Kết quả
Trong phần này, chúng tôi kiểm tra tác động của tham số hóa quá mức mạng và tỷ lệ cắt tỉa đối với hiệu suất mạng con, với mục tiêu xác minh thực nghiệm Tái Chế Trọng Số Lặp Đi Lặp Lại so với các thuật toán Edge Popup (Ramanujan et al., 2020), Biprop (Diffenderfer & Kailkhura, 2021), và IteRand (Chijiwa et al., 2021). Chúng tôi theo các công trình trước đó và kiểm tra các mạng neural với độ sâu và chiều rộng khác nhau, và ngoài ra kiểm tra mỗi thuật toán ở các tỷ lệ cắt tỉa cao.

**Thay đổi Độ sâu** Trong Hình 9, chúng tôi thay đổi độ sâu của các kiến trúc VGG từ 2 đến 8 và so sánh độ chính xác kiểm tra ở các tỷ lệ cắt tỉa khác nhau. Chúng tôi quan sát thấy một lợi thế rõ ràng khi sử dụng Tái Chế Trọng Số Lặp Đi Lặp Lại: ở mỗi tỷ lệ cắt tỉa và kiến trúc mô hình, Tái Chế Trọng Số Lặp Đi Lặp Lại vượt trội hơn cả Biprop và Edge-Popup. Ngoài ra, Biprop với Tái Chế Trọng Số thường vượt trội hơn Edge-Popup với Tái Chế Trọng Số ở các tỷ lệ cắt tỉa cao hơn. Tái Chế Trọng Số Lặp Đi Lặp Lại vượt trội hơn các mô hình dày đặc trong mỗi kiến trúc ngoại trừ khi 90% trọng số đã bị cắt tỉa. Đáng chú ý, chúng tôi phát hiện ra rằng Tái Chế Trọng Số Lặp Đi Lặp Lại có thể đạt được độ chính xác vượt quá mô hình dày đặc trong các kiến trúc Conv-2. Đây là quan sát đầu tiên như vậy trên một mô hình độ sâu thấp - nghiên cứu Edge-Popup và Biprop báo cáo độ chính xác kiểm tra gần mô hình dày đặc, tuy nhiên không bao giờ vượt quá một cách rõ ràng. Hơn nữa, Biprop+Tái Chế Trọng Số Lặp Đi Lặp Lại có thể đạt được độ chính xác kiểm tra 80.23% chỉ với 20% trọng số.

**Thay đổi Chiều rộng** Chúng tôi cũng xem xét chiều rộng mạng như một yếu tố để kiểm soát tham số hóa mạng. Trước đó đã cho thấy rằng khi chúng ta tăng chiều rộng, cơ hội tìm vé trúng của chúng ta tăng lên. Edge-Popup tìm vé trúng ở các hệ số chiều rộng lớn hơn 1, trong khi Biprop báo cáo vé trúng xung quanh hệ số chiều rộng 1.

Trong Hình 2, chúng tôi chứng minh hiệu quả của Tái Chế Trọng Số Lặp Đi Lặp Lại trên các mạng có hệ số chiều rộng nhỏ hơn một. Kết quả cho thấy rằng trong mỗi kiến trúc, chúng ta có thể tìm vé trúng chỉ ở 50% chiều rộng. Về mặt thực tế, trong một kiến trúc Conv-4, điều này tương đương với chỉ 25% tham số so với Conv-4 với hệ số chiều rộng 1 (600k so với 2.4m). Ngoài ra, kiến trúc Conv-4 của chúng tôi với hệ số chiều rộng 0.5 đạt được độ chính xác 86.5% so với 86.66% cho một Conv-4 dày đặc với hệ số chiều rộng 1.

**Thay đổi Tỷ lệ Cắt tỉa** Trong Hình 3, chúng tôi chứng minh kết quả của Biprop, Edge-Popup, IteRand và Tái Chế Trọng Số Lặp Đi Lặp Lại (Biprop) trên các DNN với tỷ lệ cắt tỉa trên 80%. Tái Chế Trọng Số Lặp Đi Lặp Lại cho thấy kết quả thuận lợi với số lượng tham số hạn chế. Đáng chú ý, thuật toán luôn vượt trội hơn IteRand ở các tỷ lệ cắt tỉa tích cực từ 80% đến 99%. Ở các tỷ lệ cắt tỉa khiêm tốn hơn (20%-60%), Tái Chế Trọng Số có thể so sánh với IteRand, mà chúng tôi tóm tắt trong Phần 3.4.

Trong kiến trúc ResNet18 (11 triệu tham số), thuật toán của chúng tôi có thể tìm vé trúng chỉ với 5% trọng số ngẫu nhiên. Những kết quả này là bằng chứng tiếp theo cho thấy tham số hóa quá mức giúp trong việc xác định các mạng con hiệu suất cao.

**Kết quả ImageNet** Trong Bảng 1, chúng tôi làm nổi bật kết quả của thuật toán trên tập dữ liệu ImageNet. Chúng tôi chọn kiến trúc ResNet50 chứa 25.5 triệu tham số tổng cộng. Chúng tôi huấn luyện mỗi thuật toán cơ sở với tỷ lệ cắt tỉa 70%, tương tự như các bài báo trước đó. Kết quả cho IteRand và Edge-Popup nằm trong khoảng 0.1% so với kết quả của các bài báo gốc.

Kết quả cho thấy rằng thuật toán của chúng tôi hoạt động tốt dưới các tỷ lệ cắt tỉa tích cực hơn so với Edge-Popup, IteRand và Biprop, tương tự như những gì chúng tôi tìm thấy trong Phần 3.3. Cụ thể, thuật toán của chúng tôi hoạt động tương tự hoặc tốt hơn các thuật toán trước đó với ít hơn 2.5 triệu tham số.

| Thuật toán | Cắt tỉa % | # Tham số | Độ chính xác (%) |
|-----------|----------|-----------|------------------|
| Edge-Popup | 70% | 7.6M | 67.13 |
| Edge-Popup+IteRand | 70% | 7.6M | 69.11 |
| Edge-Popup+IWR | 70% | 7.6M | 69.02 |
| Edge-Popup+IWR | 80% | 5.1M | 68.87 |
| Biprop | 70% | 7.6M | 67.76 |
| Biprop+IteRand | 70% | 7.6M | 43.76 |
| Biprop+IWR | 70% | 7.6M | 69.85 |
| Biprop+IWR | 80% | 5.1M | 68.65 |

**Bảng 1.** Kết quả ImageNet trên ResNet50: Chúng tôi kiểm tra các thuật toán cắt tỉa khác nhau trên tập dữ liệu ImageNet với kiến trúc ResNet50. Kết quả cho thấy Tái Chế Trọng Số Lặp Đi Lặp Lại (**in đậm**) đạt được kết quả tương tự Edge-Popup, Biprop và IteRand với ít hơn 2.5 triệu tham số.

### 3.4. Phân tích
Trong phần này, chúng tôi cung cấp biện minh thực nghiệm cho Tái Chế Trọng Số Lặp Đi Lặp Lại.

**Tác động của Trọng số Ngẫu nhiên** Trước tiên chúng tôi thực hiện một nghiên cứu loại bỏ về tác động của trọng số ngẫu nhiên bằng cách đánh giá xem liệu S^low có thể được thay thế bằng bất kỳ tập con nào của trọng số. Cụ thể, để biện minh cho việc tái sử dụng các trọng số "quan trọng" như được xác định bởi S, chúng tôi thay thế S^high tại l bằng S^high_l = {σ(i)^2k_{i=k+1}}. Hiệu quả, điều này tái chế các giá trị trọng số được coi là ở tầng thứ hai của "không quan trọng" như được đo bởi tham số S.

Trong các thí nghiệm của chúng tôi, chúng tôi huấn luyện một kiến trúc Conv-6 với cả thuật toán Edge-Popup và Biprop Tái Chế Trọng Số với 3 khởi tạo khác nhau để so sánh hiệu quả của cách tiếp cận với thuật toán cơ sở. Chúng tôi sử dụng cùng siêu tham số như các thí nghiệm ban đầu, ngoại trừ chúng tôi sử dụng khởi tạo Kaiming Normal cho Edge-Popup.

Độ chính xác Biprop giảm từ 90.9% (±0.2) xuống 89.7% (±0.5), và độ chính xác Edge-Popup giảm từ 88.9% (±0.3) xuống 87.15% (±0.5). Kết quả của những thí nghiệm này chỉ ra một lợi ích khi tái chế các trọng số quan trọng cao so với các trọng số ngẫu nhiên khác. Cuối cùng, chúng tôi lưu ý rằng tái chế trọng số hiệu quả về mặt tính toán hơn so với ngẫu nhiên hóa lại trọng số.

**Chuẩn trên Mô hình Được Huấn luyện Trước** Trong phần này, chúng tôi nghiên cứu các chuẩn Frobenius của các thuật toán cắt tỉa khác nhau. Trước tiên chúng tôi huấn luyện một mạng Conv-8 dày đặc sử dụng quy trình huấn luyện tiêu chuẩn trên tập dữ liệu CIFAR-10, và sau đó áp dụng các thuật toán Edge-Popup, Biprop và Tái Chế Trọng Số Lặp Đi Lặp Lại để cắt tỉa mô hình đã huấn luyện. Các chuẩn Frobenius của mỗi lớp mô hình và thuật toán được mô tả trong Hình 6, với mỗi thuật toán sử dụng tỷ lệ cắt tỉa 50% ngoại trừ mạng dày đặc.

Phân tích các chuẩn của trọng số không bị cắt tỉa (được mô tả bằng "+") so với trọng số bị cắt tỉa ("-") cho thấy rằng mỗi thuật toán thể hiện các chuẩn cao hơn trong mặt nạ trọng số không bị cắt tỉa so với trọng số bị cắt tỉa của nó, ngoại trừ Tái Chế Trọng Số Lặp Đi Lặp Lại. Thú vị là, ngay cả Biprop, sử dụng trọng số dấu phẩy động trước khi nhị phân hóa, cũng thể hiện các chuẩn cao hơn trong trọng số không bị cắt tỉa. Mặt khác, Tái Chế Trọng Số Lặp Đi Lặp Lại thể hiện các chuẩn tương tự trong cả trọng số bị cắt tỉa và không bị cắt tỉa.

Kết quả của phân tích này chỉ ra rằng các giá trị trọng số có chuẩn cao được chọn một cách tự nhiên trong cả Edge-Popup và Biprop. Chúng tôi cho thấy rằng Tái Chế Trọng Số nhấn mạnh việc tái sử dụng các giá trị trọng số có chuẩn cao, tạo ra một không gian tìm kiếm của các ứng viên tốt so với một quần thể được khởi tạo ngẫu nhiên. Chúng tôi cho thấy các kết quả tương tự trên các mạng được khởi tạo ngẫu nhiên trong Phụ lục, với mỗi thuật toán chọn các trọng số có chuẩn cao. IteRand thể hiện kết quả tương tự, tuy nhiên chúng tôi loại trừ những kết quả này trong Hình 6 để mục đích hình ảnh hóa.

**Tái Chế Trọng Số Lặp Đi Lặp Lại so với IteRand** So sánh Tái Chế Trọng Số Lặp Đi Lặp Lại với IteRand mang lại những cải thiện không đáng kể về mặt thống kê trong thuật toán Edge-Popup sử dụng khởi tạo Hằng số Có dấu. Chúng tôi lập luận rằng bất kỳ ngẫu nhiên hóa trọng số nào cũng hoạt động tốt dưới khởi tạo này bởi vì các giá trị trọng số hằng số làm cho việc tái chế ít liên quan hơn. Tuy nhiên, tái chế trọng số hoạt động tốt như, nếu không tốt hơn một chút so với IteRand với chi phí tính toán ít hơn. Cuối cùng, tái chế trọng số vượt trội hơn IteRand ở các tỷ lệ cắt tỉa tích cực, như được mô tả trong Hình 3.

Ngoài những lập luận này, chúng tôi lưu ý một số hạn chế chính đối với IteRand: 1) **Chi phí lưu trữ bổ sung.** Bằng cách ngẫu nhiên hóa lại các trọng số một cách lặp đi lặp lại, chúng ta cần lưu các hạt giống ngẫu nhiên bổ sung và mặt nạ nhị phân mỗi K_per trong quá trình huấn luyện. Trong môi trường tính toán hạn chế, điều này có thể trở thành một yếu tố hạn chế. Trong công trình gốc, K_per được đặt ở các giá trị tích cực: một đến mười lần mỗi epoch. 2) **Ngẫu nhiên hóa lại trọng số định kỳ tạo ra một mô hình được tham số hóa quá mức một cách giả tạo.** Nếu các trọng số bị cắt tỉa được khởi tạo lại với tỷ lệ r mỗi K_per cho K epochs, một mạng với n trọng số cần n + K/K_per(n_pr) giá trị trọng số để đạt được độ chính xác cao. Tái Chế Trọng Số Lặp Đi Lặp Lại thay vào đó cho thấy rằng các giá trị trọng số liên quan tồn tại như một tập con của n tham số gốc, và xác định những giá trị đó để tái sử dụng. 3) **Trong bài báo gốc, IteRand chỉ được kiểm tra trên Edge-Popup.** Chúng tôi đã triển khai IteRand trên thuật toán Biprop và không thể đạt được kết quả thành công trên hơn nửa tá cấu hình. Bảng 1 (Biprop) mô tả những kết quả này.

## 4. Vé Trúng Dồi Dào
Giả thuyết Vé Số Đa Giải thưởng (Diffenderfer & Kailkhura, 2021) khẳng định rằng một mạng neural được khởi tạo ngẫu nhiên chứa một số vé trúng đạt được độ chính xác tương đương với các mô hình được huấn luyện trọng số có cùng kiến trúc. Trong phần này, chúng tôi đánh giá thêm giả thuyết này bằng cách đặt câu hỏi sau: Cho một mạng được tham số hóa quá mức đủ, liệu chúng ta có thể tìm nhiều vé trúng (MPT) dưới điều khiển siêu tham số nghiêm ngặt?

Mặc dù các MPT đã được chứng minh là có thể về mặt lý thuyết (Diffenderfer & Kailkhura, 2021), kết quả thực nghiệm chủ yếu bị hạn chế trong việc cho thấy sự tồn tại của các MPT bằng cách thay đổi tỷ lệ cắt tỉa. Mặc dù điều này là bằng chứng đủ cho chứng minh, chúng tôi thay vào đó tìm cách đánh giá xem liệu các vé trúng có thể hiện cấu trúc khác nhau trong một môi trường bị ràng buộc. Cụ thể, chúng tôi hạn chế các siêu tham số như tỷ lệ cắt tỉa để đánh giá tính không đồng nhất của các MPT.

Chúng tôi giả thuyết rằng các vé trúng, tức là các cấu trúc mặt nạ duy nhất, tồn tại với số lượng lớn hơn những gì đã được báo cáo trước đây. Để minh họa điều này, hãy xem xét lớp nhỏ nhất của mạng Conv-6 (lớp đầu tiên) chứa 1,258 trọng số. Khi hạn chế việc tìm kiếm mặt nạ đến một tỷ lệ cắt tỉa cụ thể, giả sử 95%, có C(1,258, 63) mặt nạ có thể để chọn, một con số thiên văn học.

**Thí nghiệm** Chúng tôi sử dụng mạng Conv-6 vì nó tương đối nhỏ gọn (2.26m tham số) và cũng tạo ra vé trúng ở nhiều tỷ lệ cắt tỉa. Để thực hiện điều khiển siêu tham số, chúng tôi khởi tạo mỗi mô hình với một cấu hình giống hệt nhau; ngoài ra, chúng tôi seed mỗi lần chạy để a) khởi tạo cùng trọng số b) thực hiện cùng luồng huấn luyện (tức là các batch giống hệt nhau trong cả dữ liệu và thứ tự), và c) tạo điều kiện nhất quán trên các thư viện và thiết bị (ví dụ: NumPy/PyTorch, CPU/GPU). Chúng tôi đặt torch CUDA backend thành 'deterministic', như được khuyến nghị trong tài liệu. Sửa đổi siêu tham số duy nhất của chúng tôi là seed cho tham số điểm số S, mà chúng tôi tăng lên một cho mỗi mô hình tiếp theo.

Chúng tôi huấn luyện các mô hình sử dụng các thuật toán Edge-Popup và Biprop tiêu chuẩn. Ở tỷ lệ cắt tỉa 50%, chúng tôi huấn luyện 15 mô hình cho mỗi thuật toán, và ở tỷ lệ cắt tỉa 75% và 90%, chúng tôi huấn luyện 5 mô hình cho mỗi thuật toán. Chúng tôi lưu ý rằng mỗi mô hình bị cắt tỉa đạt được độ chính xác kiểm tra tương tự như các mô hình khác có cùng thuật toán và tỷ lệ cắt tỉa: Edge-Popup ở tỷ lệ cắt tỉa 50% (μ = 89.57% ± 0.2), tỷ lệ cắt tỉa 75% (μ = 86.75% ± 0.1), tỷ lệ cắt tỉa 90% (μ = 79.73% ± 0.1). Biprop ở tỷ lệ cắt tỉa 50% (μ = 89.7% ± 0.2), tỷ lệ cắt tỉa 75% (μ = 88.56% ± 0.2), tỷ lệ cắt tỉa 90% (μ = 82.56% ± 0.1). Đối với Biprop, tham số tỷ lệ α được chuyển đổi thành một cho mỗi mặt nạ để tính toán đẳng thức mặt nạ.

Chúng tôi đánh giá độ tương tự của các mặt nạ nhị phân sử dụng Hệ số Khớp Đơn giản (SMC) (Rand, 1971), và JI (Jaccard, 1912; Tanimoto, 1956). SMC đo tỷ lệ phần trăm tổng của các mặt nạ khớp, trong khi JI đo tỷ lệ phần trăm của các mặt nạ bằng một, loại trừ sự vắng mặt chung khỏi mẫu số (M₁₁/(M₀₁ + M₁₀ + M₁₁)).

**Kết quả** Trong Hình 5, chúng tôi cho thấy bản đồ nhiệt của các hệ số JI ở tỷ lệ cắt tỉa 50% cũng như JI cấp lớp cho mỗi thuật toán. Kết quả trên các hệ số, lớp mô hình và tỷ lệ cắt tỉa chỉ ra rằng các mặt nạ được tạo ra với các seed điểm số khác nhau tạo ra các cấu trúc tương phản. Ví dụ, ma trận tương tự trong Hình 5 và thống kê tóm tắt trong Bảng 2 cho thấy tất cả các kết hợp thuật toán mang lại trung bình JI nhỏ hơn 0.29, một sự khác biệt lớn giữa các mặt nạ trong mọi trường hợp. Các giá trị SMC mang lại điểm số cao hơn (lên đến 0.84 trong Bảng 2), nhưng điều này được mong đợi ở các tỷ lệ cắt tỉa cao hơn vì hầu hết các mặt nạ sẽ có các số không khớp. Một sự phân biệt rõ ràng về tính duy nhất là JI ở các tỷ lệ cắt tỉa cao hơn: ở 90% cắt tỉa, có độ chung rất thấp giữa các mặt nạ dương được chọn cho các mạng con (0.05 và 0.09).

Kết quả cũng chỉ ra một số điểm tương tự. Hình 3.4 so sánh mỗi mô hình với 14 mô hình khác cho mỗi thuật toán, với điểm số JI nằm trong 1/100 của một chữ số thập phân với nhau. Chúng tôi suy đoán điều này là kết quả của thuật toán và tỷ lệ cắt tỉa: vì mỗi lớp bị giới hạn đến một tỷ lệ cắt tỉa cụ thể, tỷ lệ khớp của nó so với cùng lớp trong một mô hình khác sẽ bị ràng buộc bởi tỷ lệ cắt tỉa. Một điểm tương tự thứ hai có thể được thấy trong các hệ số theo lớp mang lại các mẫu tương tự trên các mô hình. Lớp đầu tiên thể hiện độ tương tự cao nhất trong tất cả các trường hợp, chỉ ra cấu trúc mặt nạ cần thiết để học đầu vào tập dữ liệu là quan trọng. Các lớp giữa mang lại các mặt nạ đa dạng, cho thấy rằng các trọng số ở những lớp này thể hiện tiện ích có thể thay thế được nhiều hơn. Cuối cùng, lớp cuối cùng thường mang lại các hệ số tương tự cao hơn, chỉ ra tầm quan trọng của các trọng số cụ thể cho phân loại.

Về mặt thực tế, phân tích này cung cấp một số hướng cho nghiên cứu tương lai. Thứ nhất, sự tồn tại của các MPT dưới điều khiển siêu tham số cho thấy bằng chứng rằng tổng số lượng MPT có thể lớn. Hiểu được số lượng lý thuyết này có thể hướng dẫn chúng ta trong việc tìm kiếm các mô hình tốt hơn. Và thứ hai, đánh giá sự tương tự của các trọng số trên các MPT có thể giúp chúng ta hiểu các đặc tính mong muốn vốn có đối với các mạng con thành công.

| Thuật toán | Cắt tỉa | SMC | JI |
|-----------|---------|-----|----|
| Edge-Popup | 50% | 0.51 | 0.25 |
| Edge-Popup | 75% | 0.63 | 0.13 |
| Edge-Popup | 90% | 0.82 | 0.05 |
| Biprop | 50% | 0.58 | 0.29 |
| Biprop | 75% | 0.68 | 0.18 |
| Biprop | 90% | 0.84 | 0.09 |

**Bảng 2.** Thống kê tương tự mặt nạ: Thống kê của các kết hợp mô hình với các thuật toán và tỷ lệ cắt tỉa khác nhau.

## 5. Công trình Liên quan

**Cắt tỉa Mạng Truyền thống** Hiệu quả của các mạng neural thưa thớt đã được chứng minh lần đầu tiên bởi Lecun et. al (LeCun et al., 1989). Với sự ra đời của deep learning, kích thước và hiệu quả của các mô hình ML nhanh chóng trở thành một hạn chế quan trọng. Tự nhiên, nghiên cứu nhằm vào việc giảm kích thước (Han et al., 2015; Hinton et al., 2014), và hạn chế tiêu thụ điện năng và năng lượng (Yang et al., 2017).

**Giả thuyết Vé Số** Giả thuyết Vé Số đã phát hiện ra rằng các mạng dày đặc chứa các mạng con được khởi tạo ngẫu nhiên mà khi được huấn luyện riêng, đạt được độ chính xác tương đương với mô hình dày đặc gốc. Tuy nhiên, cách tiếp cận này yêu cầu huấn luyện một mạng dày đặc để xác định vé trúng. Công trình tiếp theo đã xác định các chiến lược để cắt tỉa DNN mà không cần mô hình được huấn luyện trước sử dụng lựa chọn tham lam tiến (Ye et al., 2020), khoảng cách mặt nạ (You et al., 2022), kỹ thuật bảo tồn luồng (Wang et al., 2019a; Tanaka et al., 2020), và tầm quan trọng kênh (Wang et al., 2019b).

**Mạng Neural Ngẫu nhiên hóa** Quan trọng đối với công việc được mô tả trong (Ramanujan et al., 2020; Chijiwa et al., 2021; Malach et al., 2020), các mạng neural ngẫu nhiên hóa (Gallicchio & Scardapane, 2020) cũng đã được khám phá trong các kiến trúc nông. Một số ứng dụng khám phá ngẫu nhiên hóa, bao gồm liên kết chức năng vector ngẫu nhiên (Needell et al., 2020; Pao et al., 1994; Pao & Takefuji, 1992), đặc trưng ngẫu nhiên cho xấp xỉ kernel (Le et al., 2013; Rahimi & Recht, 2007; Hamid et al.), tính toán hồ chứa (Lukošević & Jaeger, 2009), và mạng cấu hình ngẫu nhiên (Wang & Li, 2017).

**Mạng Neural Nhị phân** Các BNN được nghiên cứu trong bài báo này thuộc lớp các mạng neural được lượng tử hóa. Giống như cắt tỉa, lượng tử hóa là một cách tiếp cận tự nhiên cho nén mô hình. Các kỹ thuật phổ biến để tạo ra các mạng được lượng tử hóa bao gồm lượng tử hóa sau huấn luyện với huấn luyện lại (Gysel et al., 2018; Dettmers, 2016) và huấn luyện nhận thức lượng tử hóa (Gupta et al., 2015). Trong thuật toán Biprop được đề xuất trong (Diffenderfer & Kailkhura, 2021), các mạng neural nhị phân nhận thức lượng tử hóa được huấn luyện với tham số α, cho phép các trọng số dấu phẩy động học một tham số tỷ lệ trước khi nhị phân hóa (Martinez et al., 2020).

## 6. Thảo luận
Trong công trình này, chúng tôi đề xuất một thuật toán mới để tìm các mạng con có độ chính xác cao trong các mô hình được khởi tạo ngẫu nhiên. Tái Chế Trọng Số Lặp Đi Lặp Lại thành công trên cả DNN (Edge-Popup) cũng như BNN (Biprop). Kết quả của chúng tôi chỉ ra rằng các mạng nhỏ hơn có thể đạt được độ chính xác cao hơn so với suy nghĩ trước đây. Về mặt thực tế, điều này cho phép chúng ta tạo ra các mô hình chính xác và nén trong môi trường tính toán hạn chế.

Ngoài ra, chúng tôi cho thấy bằng chứng của các MPT dồi dào bằng cách tạo ra các mạng con đa dạng với các siêu tham số gần như giống hệt nhau. Điều này cung cấp một số hướng cho điều tra thêm: 1) Suy ra các giới hạn lý thuyết về tổng số MPT trong một kiến trúc cho trước 2) Khám phá các đặc tính của các trọng số không bị cắt tỉa để hiểu rõ hơn về tối ưu hóa trọng số, và 3) Khám phá cắt tỉa trọng số trong các miền vấn đề khác nhau như NLP.

## Tài liệu Tham khảo

[Phần tài liệu tham khảo được giữ nguyên với định dạng gốc do chứa nhiều thông tin chi tiết về tác giả, tên bài báo, tạp chí, năm xuất bản, etc.]
