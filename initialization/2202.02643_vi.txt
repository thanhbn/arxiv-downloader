# 2202.02643.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/initialization/2202.02643.pdf
# Kích thước tệp: 3998713 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
HIỆU QUẢ KHÔNG HỢP LÝ CỦA RANDOM PRUNING : SỰ TRỞ LẠI CỦA BASELINE NGÂY THƠ NHẤT CHO SPARSE TRAINING

Shiwei Liu1, Tianlong Chen2, Xiaohan Chen2, Li Shen3
Decebal Constantin Mocanu1,4,Zhangyang Wang2,Mykola Pechenizkiy1,
1Eindhoven University of Technology,2University of Texas at Austin
3JD Explore Academy,4University of Twente,
{s.liu3,m.pechenizkiy}@tue.nl ,
{tianlong.chen,xiaohan.chen,atlaswang}@utexas.edu ,
d.c.mocanu@utwente.nl ,mathshenli@gmail.com

TÓM TẮT
Random pruning có lẽ là cách ngây thơ nhất để đạt được sparsity trong mạng neural, nhưng đã bị coi là không cạnh tranh được bởi các phương pháp post-training pruning hoặc sparse training. Trong bài báo này, chúng tôi tập trung vào sparse training và làm nổi bật một phát hiện có lẽ trái ngược với trực giác, rằng random pruning tại initialization có thể khá mạnh mẽ cho sparse training của các mạng neural hiện đại. Mà không cần bất kỳ tiêu chí pruning tinh tế nào hoặc các cấu trúc sparsity được theo đuổi cẩn thận, chúng tôi chứng minh thực nghiệm rằng sparse training một mạng được pruned ngẫu nhiên từ đầu có thể khớp với hiệu suất của equivalent dense của nó. Có hai yếu tố chính góp phần vào sự hồi sinh này: (i) kích thước mạng quan trọng: khi các mạng dense gốc trở nên rộng hơn và sâu hơn, hiệu suất của việc training một mạng sparse được pruned ngẫu nhiên sẽ nhanh chóng tăng lên để khớp với mạng dense equivalent của nó, ngay cả ở tỷ lệ sparsity cao; (ii) các tỷ lệ sparsity layer-wise thích hợp có thể được chọn trước cho sparse training, điều này cho thấy là một booster hiệu suất quan trọng khác. Đơn giản như vẻ ngoài, một subnetwork được pruned ngẫu nhiên của Wide ResNet-50 có thể được sparse trained để vượt trội hơn một Wide ResNet-50 dense, trên ImageNet. Chúng tôi cũng quan sát thấy các mạng được pruned ngẫu nhiên như vậy vượt trội hơn các counterpart dense trong các khía cạnh thuận lợi khác, như out-of-distribution detection, uncertainty estimation, và adversarial robustness. Nhìn chung, kết quả của chúng tôi mạnh mẽ gợi ý rằng có chỗ lớn hơn dự kiến cho sparse training ở quy mô lớn, và lợi ích của sparsity có thể universal hơn ngoài pruning được thiết kế cẩn thận. Mã nguồn của chúng tôi có thể được tìm thấy tại https://github.com/VITA-Group/Random_Pruning .

1 GIỚI THIỆU
Hầu hết các đột phá gần đây trong deep learning được đạt được khá nhiều với độ phức tạp tăng lên của các mạng over-parameterized (Brown et al., 2020; Raffel et al., 2020; Dosovitskiy et al., 2021; Fedus et al., 2021. arXiv:2101.03961; Jumper et al., 2021; Berner et al., 2019). Điều được biết rõ là các mô hình lớn train tốt hơn (Neyshabur et al., 2019; Novak et al., 2018; Allen-Zhu et al., 2019), generalize tốt hơn (Hendrycks & Dietterich, 2019; Xie & Yuille, 2020; Zhao et al., 2018), và transfer tốt hơn (Chen et al., 2020b;a; 2021b). Tuy nhiên, sự gia tăng của các mô hình lớn làm trầm trọng thêm khoảng cách giữa nghiên cứu và thực tế vì nhiều ứng dụng trong đời thực đòi hỏi các mạng compact và hiệu quả.

Neural network pruning, kể từ khi được đề xuất bởi (Mozer & Smolensky, 1989; Janowsky, 1989), đã phát triển như kỹ thuật phổ biến nhất trong literature để giảm các yêu cầu tính toán và bộ nhớ của mạng neural. Trong vài năm qua, nhiều tiêu chí pruning đã được đề xuất, bao gồm magnitude (Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Mocanu et al., 2018), Hessian (LeCun et al., 1990; Hassibi & Stork, 1993), mutual information (Dai et al., 2018), Taylor expansion (Molchanov et al., 2016), movement (Sanh et al., 2020), connection sensitivity (Lee et al., 2019), v.v. Được thúc đẩy cho các scenario khác nhau, pruning có thể xảy ra sau training (Han et al., 2015; Frankle & Carbin, 2019; Molchanov et al., 2016; Lee et al., 2021), trong training (Zhu & Gupta, 2017; Gale et al., 2019; Louizos et al., 2018; You et al., 2020; Chen et al., 2021c;a), và thậm chí trước training (Mocanu et al., 2018; Lee et al., 2019; Gale et al., 2019; Wang et al., 2020; Tanaka et al., 2020). Chế độ cuối cùng có thể được phân loại thêm thành "static sparse training" (Mocanu et al., 2016; Gale et al., 2019; Lee et al., 2019; Wang et al., 2020) và "dynamic sparse training" (Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020a; Liu et al., 2021b;a).

Trong khi random pruning là một phương pháp universal có thể xảy ra ở bất kỳ giai đoạn nào của training, training một mạng được pruned ngẫu nhiên từ đầu có lẽ là cách hấp dẫn nhất, do tiềm năng tiết kiệm "end-to-end" của nó cho toàn bộ quá trình training bên cạnh inference. Do lý do này, chúng tôi tập trung vào random pruning cho sparse training trong bài báo này. Khi các phương pháp pruning mới nở rộ, random pruning tự nhiên trở thành "lower bound" thực nghiệm của hiệu suất của chúng vì các connection được chọn ngẫu nhiên mà không có bất kỳ lý do tốt nào. Có thể do cùng lý do này, kết quả của sparse training với random pruning (như baseline "dễ đánh bại" để hỗ trợ các phương pháp pruning mới phức tạp hơn) được báo cáo trong literature thật không may là mơ hồ, thường không nhất quán, và đôi khi casual. Ví dụ, nó được tìm thấy trong Liu et al. (2020b) rằng các mạng sparse được pruned ngẫu nhiên có thể được trained từ đầu để khớp với độ chính xác đầy đủ của mạng dense chỉ với 20% parameters, trong khi khoảng 80% parameters được yêu cầu để làm như vậy trong Frankle et al. (2021). Sự khác biệt có thể nảy sinh từ lựa chọn architecture, công thức training, hyperparameters phân phối/tỷ lệ layer-wise, và nhiều thứ khác.