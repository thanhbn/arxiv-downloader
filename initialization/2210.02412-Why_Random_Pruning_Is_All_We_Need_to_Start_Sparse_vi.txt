# Tại sao Cắt tỉa Ngẫu nhiên là Tất cả Những gì Chúng ta Cần để Bắt đầu Thưa thớt
Advait Gadhikar1Sohom Mukherjee1Rebekka Burkholz1

## Tóm tắt
Các mặt nạ ngẫu nhiên định nghĩa các mô hình mạng nơ-ron thưa thớt hiệu quả một cách đáng ngạc nhiên, như đã được chứng minh thực nghiệm. Các mạng thưa thớt thu được thường có thể cạnh tranh với các kiến trúc dày đặc và các thuật toán cắt tỉa vé số may mắn tiên tiến, mặc dù chúng không dựa vào các vòng lặp cắt tỉa-huấn luyện tốn kém về mặt tính toán và có thể được rút ra ban đầu mà không có chi phí tính toán đáng kể. Chúng tôi đưa ra một giải thích lý thuyết về cách các mặt nạ ngẫu nhiên có thể xấp xỉ các mạng đích tùy ý nếu chúng rộng hơn bởi một hệ số logarit trong nghịch đảo độ thưa thớt 1/log(1/sparsity). Hệ số quá tham số hóa này là cần thiết ít nhất cho các mạng ngẫu nhiên 3 tầng, điều này làm sáng tỏ hiệu suất giảm sút quan sát được của các mạng ngẫu nhiên ở độ thưa thớt cao hơn. Tuy nhiên, ở mức độ thưa thớt vừa phải đến cao, kết quả của chúng tôi ngụ ý rằng các mạng thưa thớt hơn được chứa trong các mạng nguồn ngẫu nhiên để bất kỳ sơ đồ huấn luyện dày đặc-đến-thưa thớt nào có thể được chuyển thành một sơ đồ thưa thớt đến thưa thớt hiệu quả về mặt tính toán hơn bằng cách ràng buộc tìm kiếm vào một mặt nạ ngẫu nhiên cố định. Chúng tôi chứng minh tính khả thi của phương pháp này trong các thí nghiệm cho các phương pháp cắt tỉa khác nhau và đề xuất các lựa chọn đặc biệt hiệu quả về tỷ lệ thưa thớt theo tầng ban đầu của mạng nguồn ngẫu nhiên. Như một trường hợp đặc biệt, chúng tôi chứng minh về mặt lý thuyết và thực nghiệm rằng các mạng nguồn ngẫu nhiên cũng chứa các vé số may mắn mạnh. Mã của chúng tôi có sẵn tại https://github.com/RelationalML/sparse_to_sparse.

## 1. Giới thiệu
Những đột phá ấn tượng đạt được bởi học sâu phần lớn được quy cho việc quá tham số hóa rộng rãi của các mạng nơ-ron sâu, vì nó dường như có nhiều lợi ích cho sức mạnh biểu diễn và tối ưu hóa của chúng (Belkin et al., 2019). Tuy nhiên, xu hướng kết quả hướng tới các mô hình và tập dữ liệu ngày càng lớn hơn áp đặt các chi phí tính toán và năng lượng tăng lên khó đáp ứng. Điều này đặt ra câu hỏi: Liệu mức độ quá tham số hóa cao này có thực sự cần thiết không?

Huấn luyện các kiến trúc mạng nơ-ron sâu quy mô nhỏ hoặc thưa thớt từ đầu vẫn là một thách thức đối với các sơ đồ khởi tạo tiêu chuẩn (Li et al., 2016; Han et al., 2015). Tuy nhiên, (Frankle & Carbin, 2019) gần đây đã chứng minh rằng tồn tại các kiến trúc thưa thớt có thể được huấn luyện để giải quyết các vấn đề điểm chuẩn tiêu chuẩn một cách cạnh tranh. Theo Giả thuyết Vé số May mắn (LTH) của họ, các mạng được khởi tạo ngẫu nhiên dày đặc chứa các mạng con có thể được huấn luyện riêng biệt đến một độ chính xác kiểm tra có thể so sánh với mạng dày đặc gốc. Các mạng con như vậy, các vé số may mắn (LT), kể từ đó đã được thu thập bởi các thuật toán cắt tỉa yêu cầu các vòng lặp cắt tỉa-huấn luyện lại tốn kém về mặt tính toán (Frankle & Carbin, 2019; Tanaka et al., 2020) hoặc các thủ tục học mặt nạ (Savarese et al., 2020; Sreenivasan et al., 2022b). Mặc dù những điều này có thể dẫn đến lợi ích tính toán ở thời gian huấn luyện và suy luận và giảm yêu cầu bộ nhớ (Hassibi et al., 1993; Han et al., 2015), mục tiêu thực sự vẫn là xác định các kiến trúc thưa thớt có thể huấn luyện trước khi huấn luyện, vì điều này có thể dẫn đến tiết kiệm tính toán đáng kể. Tuy nhiên, các phương pháp cắt tỉa tại khởi tạo đương đại (Lee et al., 2018; Wang et al., 2020; Tanaka et al., 2020; Fischer & Burkholz, 2022; Frankle et al., 2021) đạt hiệu suất ít cạnh tranh hơn. Vì lý do đó, thật đáng chú ý rằng ngay cả các phương pháp lặp lại tiên tiến cũng khó vượt qua một giải pháp thay thế đơn giản, rẻ về mặt tính toán và độc lập với dữ liệu: cắt tỉa ngẫu nhiên tại khởi tạo (Su et al., 2020). Liu et al. (2021) đã cung cấp bằng chứng thực nghiệm có hệ thống cho hiệu quả 'không hợp lý' của nó trong nhiều thiết lập, bao gồm các kiến trúc và dữ liệu phức tạp, quy mô lớn.

Chúng tôi giải thích về mặt lý thuyết tại sao chúng có thể hiệu quả bằng cách chứng minh rằng một mạng được che mặt nạ ngẫu nhiên có thể xấp xỉ một mạng đích tùy ý nếu nó rộng hơn bởi một hệ số logarit trong độ thưa thớt của nó 1/log(1/sparsity). Bằng cách rút ra một cận dưới về độ rộng yêu cầu của một mạng ER ngẫu nhiên 1 tầng ẩn, chúng tôi cũng chứng minh rằng mức độ quá tham số hóa này là cần thiết nói chung. Điều này ngụ ý rằng các mạng ngẫu nhiên thưa thớt có tính chất xấp xỉ hàm toàn cầu như các mạng dày đặc và ít nhất cũng biểu cảm như các mạng đích tiềm năng. Tuy nhiên, nó cũng làm nổi bật những hạn chế của cắt tỉa ngẫu nhiên trong trường hợp độ thưa thớt cực cao, vì yêu cầu độ rộng sau đó tăng khoảng 1/log(1/sparsity) ≈ 1/(1−sparsity) (xem thêm Hình 2 để có ví dụ). Trong thực tế, chúng tôi quan sát sự suy giảm tương tự về hiệu suất ở mức độ thưa thớt cao.

Ngay cả đối với độ thưa thớt vừa phải đến cao, tính ngẫu nhiên của các kết nối dẫn đến một số lượng đáng kể các trọng số thừa không cần thiết cho việc biểu diễn một mạng đích. Hiểu biết này cho thấy rằng, một mặt, việc cắt tỉa bổ sung có thể tăng cường thêm độ thưa thớt của cấu trúc mạng nơ-ron kết quả, vì các mặt nạ ngẫu nhiên có thể không tối ưu về độ thưa thớt. Mặt khác, bất kỳ phương pháp huấn luyện dày đặc-đến-thưa thớt nào sẽ không cần bắt đầu từ một mạng dày đặc mà cũng có thể bắt đầu huấn luyện từ một mạng ngẫu nhiên thưa thớt hơn và do đó được chuyển thành một phương pháp học thưa thớt-đến-thưa thớt. Ý tưởng chính được hình dung trong Hình 1.

[Hình 1. Huấn luyện thưa thớt với các mạng được che mặt nạ ngẫu nhiên (ER): Một biểu diễn trực quan của hàm ý chính của lý thuyết của chúng tôi - huấn luyện thưa thớt đến thưa thớt có thể hiệu quả bằng cách bắt đầu từ một mạng được che mặt nạ ngẫu nhiên (ER).]

và được xác minh trong các thí nghiệm rộng rãi với các phương pháp cắt tỉa vé số may mắn và thưa thớt hóa liên tục khác nhau. Kết quả chính của chúng tôi cũng có thể được hiểu là lý giải lý thuyết cho Huấn luyện Thưa thớt Động (DST) (Evci et al., 2020; Liu et al., 2021; Bellec et al., 2018), cắt tỉa các mạng ngẫu nhiên có độ thưa thớt vừa phải. Tuy nhiên, nó còn dựa vào các bước nối lại cạnh đôi khi yêu cầu tính toán gradient của mạng dày đặc tương ứng (Evci et al., 2020). Những hạn chế được rút ra của chúng tôi về cắt tỉa ngẫu nhiên chỉ ra rằng việc nối lại này có thể cần thiết ở độ thưa thớt cực cao nhưng có thể không cần thiết cho các điểm khởi đầu ngẫu nhiên thưa thớt vừa phải, như chúng tôi cũng làm nổi bật trong các thí nghiệm bổ sung.

Như một trường hợp đặc biệt của ý tưởng chính để cắt tỉa các mạng ngẫu nhiên, chúng tôi cũng xem xét các vé số may mắn mạnh (SLT) (Zhou et al., 2019; Ramanujan et al., 2020). Đây là các mạng con của các mạng nguồn lớn, được khởi tạo ngẫu nhiên, không yêu cầu bất kỳ huấn luyện thêm nào sau khi cắt tỉa. Các bằng chứng tồn tại lý thuyết (Malach et al., 2020; Pensia et al., 2020; Fischer et al., 2021; da Cunha et al., 2022; Burkholz, 2022a;b; Burkholz et al., 2022) cũng như thực nghiệm Ramanujan et al. (2020); Zhou et al. (2019); Diffenderfer & Kailkhura (2021); Sreenivasan et al. (2022a) cho đến nay chỉ tập trung vào việc cắt tỉa các mạng nguồn dày đặc. Chúng tôi làm nổi bật tiềm năng tiết kiệm tài nguyên tính toán trong việc tìm kiếm SLT bằng cách chứng minh sự tồn tại của chúng trong các mạng ngẫu nhiên thưa thớt thay vào đó. Thành phần chính của kết quả của chúng tôi là Bổ đề 2.2, mở rộng xấp xỉ tổng tập con sang thiết lập đồ thị ngẫu nhiên thưa thớt. Điều này cho phép chuyển giao trực tiếp hầu hết các kết quả tồn tại SLT cho các kiến trúc và hàm kích hoạt khác nhau sang các mạng nguồn thưa thớt. Hơn nữa, chúng tôi sửa đổi thuật toán edge-popup (EP) (Ramanujan et al., 2020) để tìm SLT tương ứng, dẫn đến phương pháp cắt tỉa thưa thớt-đến-thưa thớt đầu tiên cho SLT, theo hiểu biết của chúng tôi. Chúng tôi chứng minh trong các thí nghiệm rằng việc bắt đầu ngay cả ở độ thưa thớt cao đến 0.8 không cản trở hiệu suất tổng thể của EP.

Lưu ý rằng lý thuyết chung của chúng tôi áp dụng cho bất kỳ tỷ lệ thưa thớt theo tầng nào của mạng nguồn ngẫu nhiên và chúng tôi xác nhận thực tế này trong các thí nghiệm khác nhau trên dữ liệu hình ảnh điểm chuẩn tiêu chuẩn và các kiến trúc mạng nơ-ron thường được sử dụng, bổ sung cho kết quả của Liu et al. (2021) cho các lựa chọn tỷ lệ thưa thớt bổ sung. Hai đề xuất của chúng tôi, tỷ lệ thưa thớt cân bằng và hình kim tự tháp, dường như hoạt động cạnh tranh trong nhiều thiết lập, đặc biệt ở các chế độ thưa thớt cao hơn.

### Đóng góp
1. Chúng tôi chứng minh rằng các mạng ngẫu nhiên được cắt tỉa ngẫu nhiên đủ biểu cảm và có thể xấp xỉ một mạng đích tùy ý nếu chúng rộng hơn bởi một hệ số 1/log(1/sparsity). Hệ số quá tham số hóa này là cần thiết nói chung, như cận dưới của chúng tôi cho các mạng đích đơn biến chỉ ra.

2. Được truyền cảm hứng từ các chứng minh của chúng tôi, chúng tôi chứng minh thực nghiệm rằng, không có mất mát đáng kể về hiệu suất, việc bắt đầu bất kỳ sơ đồ huấn luyện dày đặc-đến-thưa thớt nào có thể được dịch thành một sơ đồ thưa thớt-đến-thưa thớt bằng cách bắt đầu từ một mạng nguồn ngẫu nhiên thay vì một mạng dày đặc.

3. Như một trường hợp đặc biệt, chúng tôi cũng chứng minh sự tồn tại của Vé số May mắn Mạnh (SLT) trong các mạng nguồn ngẫu nhiên thưa thớt, nếu mạng nguồn rộng hơn mạng đích bởi một hệ số 1/log(1/sparsity). Sự sửa đổi của chúng tôi đối với thuật toán edge-popup (EP) (Ramanujan et al., 2020) dẫn đến phương pháp cắt tỉa SLT thưa thớt-đến-thưa thớt đầu tiên, xác nhận lý thuyết của chúng tôi và làm nổi bật tiềm năng tiết kiệm tính toán.

4. Để chứng minh rằng lý thuyết của chúng tôi áp dụng cho các lựa chọn tỷ lệ thưa thớt khác nhau, chúng tôi giới thiệu hai đề xuất bổ sung vượt trội hơn các đề xuất tiên tiến trên nhiều điểm chuẩn và do đó là những ứng cử viên hứa hẹn cho các điểm khởi đầu của các sơ đồ học thưa thớt-đến-thưa thớt.

### 1.1. Công trình Liên quan
Các thuật toán để cắt tỉa mạng nơ-ron cho độ thưa thớt không có cấu trúc có thể được phân loại rộng rãi thành hai nhóm, cắt tỉa sau huấn luyện và cắt tỉa trước (hoặc trong) huấn luyện. Nhóm thuật toán đầu tiên cắt tỉa sau huấn luyện hiệu quả trong việc tăng tốc suy luận, nhưng chúng vẫn dựa vào một thủ tục huấn luyện tốn kém về mặt tính toán (Hassibi et al., 1993; LeCun et al., 1989; Molchanov et al., 2016; Dong et al., 2017; Yu et al., 2022). Nhóm thuật toán thứ hai cắt tỉa tại khởi tạo (Lee et al., 2018; Wang et al., 2020; Tanaka et al., 2020; Sreenivasan et al., 2022b; de Jorge et al., 2020) hoặc theo một chu kỳ cắt tỉa và huấn luyện lại tốn kém về mặt tính toán cho nhiều vòng lặp (Gale et al., 2019; Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019). Các phương pháp này tìm ra các mạng con có thể huấn luyện còn được biết đến là Vé số May mắn (Frankle & Carbin, 2019). Các phương pháp cắt tỉa một lần rẻ hơn về mặt tính toán nhưng dễ bị các vấn đề như sụp đổ tầng làm cho mạng được cắt tỉa không thể huấn luyện (Lee et al., 2018; Wang et al., 2020). Tanaka et al. (2020) giải quyết vấn đề này bằng cách bảo tồn luồng trong mạng thông qua cơ chế chấm điểm của họ. Các mạng thưa thớt hoạt động tốt nhất vẫn được thu được bằng các phương pháp cắt tỉa lặp lại tốn kém như Cắt tỉa Độ lớn Lặp lại (IMP), Synflow Lặp lại (Frankle & Carbin, 2019; Fischer & Burkholz, 2022) hoặc các phương pháp thưa thớt hóa liên tục (Sreenivasan et al., 2022b; Savarese et al., 2020; Kusupati et al., 2020; Louizos et al., 2018).

Tuy nhiên, Su et al. (2020) phát hiện rằng các mặt nạ được cắt tỉa ngẫu nhiên có thể vượt trội hơn các chiến lược cắt tỉa lặp lại tốn kém trong các tình huống khác nhau. Được truyền cảm hứng từ phát hiện này, Golubeva et al. (2021); Chang et al. (2021) đã đưa ra giả thuyết rằng các mạng thưa thớt quá tham số hóa hiệu quả hơn các mạng nhỏ hơn với cùng số lượng tham số. Liu et al. (2021) đã chứng minh thêm tính cạnh tranh của các mặt nạ ngẫu nhiên cho các lựa chọn tỷ lệ thưa thớt theo tầng độc lập với dữ liệu khác nhau trên một loạt rộng các kiến trúc mạng nơ-ron và tập dữ liệu, bao gồm các tập dữ liệu phức tạp. Phân tích của chúng tôi xác định các điều kiện mà hiệu quả của các mặt nạ ngẫu nhiên là hợp lý. Chúng tôi chỉ ra rằng một mạng nguồn ngẫu nhiên thưa thớt có thể xấp xỉ một mạng đích nếu nó rộng hơn bởi một hệ số tỷ lệ với log nghịch đảo độ thưa thớt. Bổ sung cho các thí nghiệm của Liu et al. (2021), chúng tôi làm nổi bật rằng các mặt nạ ngẫu nhiên cạnh tranh cho các lựa chọn tỷ lệ thưa thớt theo tầng khác nhau. Tuy nhiên, chúng tôi cũng chỉ ra rằng tính ngẫu nhiên của chúng cũng có thể tạo ra tiềm năng cho việc cắt tỉa thêm.

Chúng tôi xây dựng trên lý thuyết tồn tại vé số may mắn (Malach et al., 2020; Pensia et al., 2020; Orseau et al., 2020; Fischer et al., 2021; Burkholz et al., 2022; Burkholz, 2022b; Ferbach et al., 2022) để chứng minh rằng các mạng nguồn ngẫu nhiên thưa thớt thực sự chứa các vé số may mắn mạnh (SLT) nếu độ rộng của chúng vượt quá một giá trị tỷ lệ với độ rộng của mạng đích. Lý thuyết này đã được truyền cảm hứng từ bằng chứng thực nghiệm cho SLT (Ramanujan et al., 2020; Zhou et al., 2019; Diffenderfer & Kailkhura, 2021; Sreenivasan et al., 2022a). Thuật toán cơ bản edge-popup (Ramanujan et al., 2020) tìm SLT bằng cách huấn luyện điểm số cho mỗi tham số của mạng nguồn dày đặc và do đó tốn kém về mặt tính toán như huấn luyện dày đặc. Chúng tôi chỉ ra rằng việc huấn luyện các mạng nguồn ngẫu nhiên thưa thớt nhỏ hơn là đủ, do đó, giảm hiệu quả các yêu cầu tính toán để tìm SLT.

Tuy nhiên, lý thuyết của chúng tôi cho thấy rằng các mạng ER ngẫu nhiên đối mặt với một hạn chế cơ bản ở độ thưa thớt cực cao, vì hệ số quá tham số hóa tăng trong chế độ này như 1/log(1/(sparsity)) ≈ 1/(1−sparsity). Thiếu sót này có thể được giải quyết bằng việc nối lại có mục tiêu các cạnh ngẫu nhiên với Huấn luyện Thưa thớt Động (DST) bắt đầu cắt tỉa từ một mạng ER (Liu et al., 2021; Mocanu et al., 2018; Yuan et al., 2021). Cho đến nay, các phương pháp huấn luyện thưa thớt-đến-thưa thớt như Evci et al. (2020); Dettmers & Zettlemoyer (2019) vẫn yêu cầu gradient dày đặc cho hoạt động nối lại cạnh của chúng. Zhou et al. (2021) thu được huấn luyện thưa thớt bằng cách ước tính gradient thưa thớt sử dụng hai lần truyền xuôi. Chúng tôi chứng minh thực nghiệm rằng dưới ánh sáng của sức mạnh biểu cảm của các mạng ngẫu nhiên, chúng ta cũng có thể đạt được huấn luyện thưa thớt-đến-thưa thớt bằng cách đơn giản ràng buộc bất kỳ phương pháp cắt tỉa hoặc gradient nào vào một mặt nạ ngẫu nhiên thưa thớt ban đầu cố định.

## 2. Tính Biểu cảm của Mạng Ngẫu nhiên
Các nghiên cứu lý thuyết của chúng tôi trong phần tiếp theo có mục đích giải thích tại sao hiệu quả của các mạng ngẫu nhiên là hợp lý cho sức mạnh biểu cảm cao của chúng. Chúng tôi chỉ ra rằng chúng ta có thể xấp xỉ bất kỳ mạng đích nào với sự trợ giúp của một mạng ngẫu nhiên, với điều kiện nó rộng hơn bởi một hệ số logarit trong độ thưa thớt nghịch đảo. Đầu tiên, ràng buộc duy nhất mà chúng ta đối mặt trong việc xây dựng rõ ràng một mạng con đại diện là các cạnh có sẵn hoặc không có sẵn một cách ngẫu nhiên. Nhưng chúng ta có thể chọn các tham số mạng còn lại, tức là các trọng số và độ lệch, theo cách chúng ta có thể biểu diễn tối ưu một mạng đích. Như thông thường trong các kết quả về tính biểu cảm và sức mạnh biểu diễn, chúng tôi đưa ra các tuyên bố về sự tồn tại của các tham số như vậy, không nhất thiết, nếu chúng có thể được tìm thấy theo thuật toán. Trong thực tế, các tham số thường sẽ được xác định bằng huấn luyện mạng nơ-ron tiêu chuẩn hoặc các vòng lặp cắt tỉa-huấn luyện. Các thí nghiệm của chúng tôi xác nhận rằng điều này thực sự khả thi ngoài nhiều bằng chứng thực nghiệm khác (Su et al., 2020; Ma et al., 2021; Liu et al., 2021). Thứ hai, chúng tôi chứng minh sự tồn tại của các vé số may mắn mạnh (SLT), giả định rằng chúng ta phải xấp xỉ các tham số đích bằng cách cắt tỉa mạng nguồn ngẫu nhiên thưa thớt. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên cung cấp bằng chứng thực nghiệm và lý thuyết cho tính khả thi của trường hợp này.

### Bối cảnh, Ký hiệu và Thiết lập Chứng minh
Gọi x = (x₁, x₂, ..., xₐ) ∈ [a₁, b₁]ᵈ là một vectơ đầu vào d-chiều bị chặn, trong đó a₁, b₁ ∈ ℝ với a₁ < b₁. f: [a₁, b₁]ᵈ → ℝⁿᴸ là một mạng nơ-ron feed-forward kết nối đầy đủ với kiến trúc (n₀, n₁, ..., nᴸ), tức là độ sâu L và nₗ nơ-ron trong Tầng l. Mỗi tầng l ∈ {1, 2, ..., L} tính toán các trạng thái nơ-ron x⁽ˡ⁾ = φ(h⁽ˡ⁾), h⁽ˡ⁾ = W⁽ˡ⁻¹⁾x⁽ˡ⁻¹⁾ + b⁽ˡ⁻¹⁾. h⁽ˡ⁾ được gọi là tiền kích hoạt, W⁽ˡ⁾ ∈ ℝⁿˡˣⁿˡ⁻¹ là ma trận trọng số và b⁽ˡ⁾ là vectơ độ lệch. Chúng tôi cũng viết f(x; θ) để nhấn mạnh sự phụ thuộc của mạng nơ-ron vào các tham số θ = (W⁽ˡ⁾, b⁽ˡ⁾)ᴸₗ₌₁ của nó. Để đơn giản, chúng tôi giới hạn bản thân với hàm kích hoạt ReLU φ(x) = max{x, 0} phổ biến, nhưng hầu hết các kết quả của chúng tôi có thể được mở rộng dễ dàng cho các hàm kích hoạt tổng quát hơn như trong (Burkholz, 2022b;a). Ngoài các tầng kết nối đầy đủ, chúng tôi cũng xem xét các tầng tích chập. Để có ký hiệu thuận tiện, không mất tính tổng quát, chúng tôi làm phẳng các tensor trọng số để W⁽ˡ⁾ᵀ ∈ ℝᶜˡˣᶜˡ⁻¹ˣᵏˡ trong đó cₗ, cₗ₋₁, kₗ lần lượt là kênh đầu ra, kênh đầu vào và chiều bộ lọc. Ví dụ, một tích chập 2 chiều trên dữ liệu hình ảnh sẽ dẫn đến kₗ = k'₁,ₗk'₂,ₗ, trong đó k'₁,ₗ, k'₂,ₗ định nghĩa kích thước bộ lọc.

Chúng tôi phân biệt ba loại mạng nơ-ron, một mạng đích fT, một mạng nguồn fS, và một mạng con fP của fS. fT được xấp xỉ hoặc biểu diễn chính xác bởi fP, được thu được bằng cách che các tham số của nguồn fS. fS được cho là chứa một SLT nếu mạng con này không yêu cầu huấn luyện thêm sau khi thu được mặt nạ (bằng cắt tỉa). Chúng tôi giả định rằng fT có độ sâu L và các tham số W⁽ˡ⁾T, b⁽ˡ⁾T, nT,l, mT,l là trọng số, độ lệch, số nơ-ron và số tham số khác không của ma trận trọng số trong Tầng l ∈ {1, 2, ..., L}. Lưu ý rằng điều này ngụ ý ml ≤ nlnl-1. Tương tự, fS có độ sâu L + 1 với các tham số W⁽ˡ⁾S, b⁽ˡ⁾S, nS,l, mS,l}ᴸₗ₌₀. Lưu ý rằng l phạm vi từ 0 đến L cho mạng nguồn, trong khi nó chỉ phạm vi từ 1 đến L cho mạng đích. Tầng mạng nguồn thêm l = 0 giải thích cho một tầng thêm mà chúng ta cần trong việc xây dựng của chúng tôi để chứng minh sự tồn tại.

### Mạng ER
Mặc dù phổ biến, thuật ngữ 'mạng ngẫu nhiên' không chính xác về phân phối ngẫu nhiên mà từ đó một đồ thị được rút ra. Phù hợp với lý thuyết đồ thị chung, do đó chúng tôi sử dụng thuật ngữ mạng Erdős-Rényi (ER) (Erdos et al., 1960) trong phần sau. Một mạng nơ-ron ER fER ∈ ER(p) được đặc trưng bởi các tỷ lệ thưa thớt theo tầng pl. Một nguồn ER fER được định nghĩa là một mạng con của một mạng nguồn đầy đủ sử dụng một mặt nạ nhị phân S⁽ˡ⁾ER ∈ {0,1}ⁿˡˣⁿˡ⁻¹ hoặc S⁽ˡ⁾ER ∈ {0,1}ⁿˡˣⁿˡ⁻¹ˣᵏˡ cho mỗi tầng. Các mục mặt nạ được rút ra từ các phân phối Bernoulli độc lập với xác suất thành công theo tầng pl > 0, tức là s⁽ˡ⁾ij,ER ~ Ber(pl). Việc cắt tỉa ngẫu nhiên được thực hiện ban đầu với chi phí tính toán không đáng kể và mặt nạ giữ cố định trong quá trình huấn luyện. Lưu ý rằng pl cũng là mật độ kỳ vọng của tầng đó. Mật độ kỳ vọng tổng thể của mạng được cho là p = Σₗmₗpₗ/Σₖmₖ = 1 - sparsity. Trong trường hợp độ thưa thớt đồng nhất, pl = p, chúng tôi cũng viết ER(p) thay vì ER(p̄). Một mạng ER được định nghĩa là fER = fS(x; W · SER). Khác với các chứng minh tồn tại SLT thông thường (Ramanujan et al., 2020), chúng tôi tham chiếu đến fER ∈ ER(p) như mạng nguồn, và chỉ ra rằng SLT được chứa trong mạng ER này. SLT sau đó được định nghĩa bởi mặt nạ SP, là một mạng con của SER, tức là một mục không sij,ER = 0 cũng ngụ ý một số không trong sij,P = 0, nhưng điều ngược lại không đúng. Chúng tôi bỏ qua các chỉ số dưới nếu bản chất của mặt nạ rõ ràng từ ngữ cảnh. Trong phân tích tiếp theo về tính biểu cảm trong các mạng ER, chúng tôi tiếp tục sử dụng SER và SP để biểu thị một mạng nguồn ER ngẫu nhiên và một mạng con thưa thớt trong mạng ER tương ứng.

### Tỷ lệ Thưa thớt
Có nhiều lựa chọn hợp lý cho các tỷ lệ thưa thớt theo tầng và do đó các xác suất ER pl. Lý thuyết của chúng tôi áp dụng cho tất cả chúng. Lựa chọn tối ưu cho một kiến trúc mạng nguồn đã cho phụ thuộc vào mạng đích và do đó giải pháp cho một vấn đề học tập, thường không biết trước trong thực tế. Để chứng minh rằng lý thuyết của chúng tôi đúng cho các phương pháp khác nhau, chúng tôi nghiên cứu các tỷ lệ thưa thớt theo tầng sau trong các thí nghiệm. Đường cơ sở đơn giản nhất là lựa chọn đồng nhất toàn cầu pl = p. Liu et al. (2021) đã so sánh lựa chọn này trong các thí nghiệm rộng rãi với đề xuất chính của họ, ERK, gán pl ∝ nin+nout/ninnout cho một tầng tuyến tính và pl ∝ cl+cl-1+kl/clcl-1kl (Mocanu et al., 2017) cho một tầng tích chập. Ngoài ra, chúng tôi đề xuất một phương pháp kim tự tháp và cân bằng, được hình dung trong Phụ lục A.15.

**Kim tự tháp**: Phương pháp này mô phỏng một tính chất của các mạng được cắt tỉa thu được bởi IMP (Frankle & Carbin, 2019) tức là mật độ tầng giảm theo độ sâu tăng của mạng. Đối với một mạng có độ sâu L, chúng tôi sử dụng pl = (p1)ˡ, pl ∈ (0,1) để Σˡ⁼ᴸₗ₌₁plml/Σˡ⁼ᴸₗ₌₁ml = p. Cho kiến trúc, chúng tôi sử dụng một bộ giải phương trình đa thức (Harris et al., 2020) để thu được p1 cho tầng đầu tiên sao cho p1 ∈ (0,1).

**Cân bằng**: Phương pháp thưa thớt theo tầng thứ hai nhằm duy trì cùng số lượng tham số trong mọi tầng cho một độ thưa thớt mạng p và kiến trúc mạng nguồn đã cho. Mỗi nơ-ron có độ vào và ra tương tự trung bình. Mỗi tầng có x = p/L Σˡ⁼ᴸₗ₌₁ml tham số khác không. Một mạng ER như vậy có thể được thực hiện với pl = x/ml. Trong trường hợp x ≥ ml, chúng tôi đặt pl = 1.

### 2.1. Tính Biểu cảm Chung của Mạng ER
Mục tiêu chính của chúng tôi trong phần này là rút ra các tuyên bố xác suất về sự tồn tại của các cạnh trong một mạng nguồn ER cho phép chúng ta xấp xỉ một mạng đích đã cho. Vì mỗi kết nối trong mạng nguồn chỉ tồn tại với xác suất pl, cho mỗi trọng số đích, chúng ta cần tạo ra nhiều cạnh ứng cử viên, trong đó ít nhất một cạnh khác không với xác suất đủ cao. Điều này có thể đạt được bằng cách đảm bảo rằng mỗi cạnh đích có nhiều điểm khởi đầu tiềm năng trong mạng nguồn ER. Việc xây dựng của chúng tôi thực hiện ý tưởng này với nhiều bản sao của mỗi nơ-ron trong một tầng. Số lượng bản sao nơ-ron yêu cầu phụ thuộc vào độ thưa thớt của mạng nguồn ER và giới thiệu một hệ số quá tham số hóa liên quan đến độ rộng của mạng. Để tạo ra nhiều bản sao của các nơ-ron đầu vào cũng vậy, việc xây dựng của chúng tôi dựa vào một tầng bổ sung trong mạng nguồn so với mạng đích, như được hình dung trong Hình 4 trong Phụ lục. Chúng tôi đầu tiên giải thích việc xây dựng cho một tầng đích đơn và mở rộng sau đó sang các kiến trúc sâu hơn.

#### Đích Tầng Ẩn Đơn
Chúng tôi bắt đầu với việc xây dựng một mạng đích kết nối đầy đủ một tầng ẩn với một mạng con của một mạng nguồn ER ngẫu nhiên bao gồm thêm một tầng. Chiến lược chứng minh của chúng tôi được giải thích bằng hình ảnh trong Hình 4 trong Phụ lục. Định lý sau phát biểu yêu cầu độ rộng chính xác mà việc xây dựng của chúng tôi yêu cầu.

**Định lý 2.1** (Xây dựng Đích Tầng Ẩn Đơn). Giả sử rằng một mạng đích kết nối đầy đủ một tầng ẩn fT(x) = W⁽²⁾Tφ(W⁽¹⁾Tx + b⁽¹⁾T) + b⁽²⁾T, một xác suất thất bại cho phép δ ∈ (0,1), mật độ nguồn p̄ và một mạng nguồn ER 2 tầng fS ∈ ER(p̄) với độ rộng nS,0 = q0d, nS,1 = q1nT,1, nS,2 = q2nT,2 được cho. Nếu

q0 ≥ 1/log(1/(1-p1)) log(2mT,1q1/δ),
q1 ≥ 1/log(1/(1-p2)) log(2mT,2/δ) và q2 = 1

thì với xác suất 1-δ, mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W·SP) = fT.

**Phác thảo Chứng minh:**
Ý tưởng chính là tạo ra nhiều bản sao (khối trong Hình 4(b) trong Phụ lục) trong mạng nguồn cho mỗi nơ-ron đích sao cho mỗi liên kết đích được thực hiện bằng cách trỏ đến ít nhất một trong những bản sao này trong nguồn ER. Để tạo ra nhiều ứng cử viên của các nơ-ron đầu vào, chúng tôi tạo ra một tầng đầu tiên đơn biến trong mạng nguồn như được giải thích trong Hình 4. Trong phụ lục, chúng tôi rút ra các tham số trọng số và độ lệch tương ứng của mạng nguồn để nó có thể biểu diễn mạng đích một cách chính xác. Tự nhiên, nhiều liên kết có sẵn sẽ nhận trọng số không nếu chúng không cần thiết trong việc xây dựng cụ thể nhưng được yêu cầu cho xác suất đủ cao rằng ít nhất một trọng số có thể được đặt thành khác không. Nhiệm vụ chính của chúng tôi trong chứng minh là ước tính xác suất rằng chúng ta có thể tìm thấy đại diện của tất cả các liên kết đích trong mạng nguồn ER, tức là mỗi nơ-ron trong Tầng l = 1 có ít nhất một cạnh đến mỗi khối trong l = 0 có kích thước q0, như được hiển thị trong Hình 4(b). Xác suất này được cho bởi (1-(1-p1)^q0)^(mT,1q1). Đối với tầng thứ hai, chúng tôi lặp lại một lập luận tương tự để ràng buộc xác suất (1-(1-p2)^q1)^mT,2 với q2 = 1, vì chúng tôi không yêu cầu nhiều bản sao của các nơ-ron đầu ra. Ràng buộc xác suất này bởi 1-δ hoàn thành chứng minh, như được chi tiết trong Phụ lục A.3.

#### Mạng Đích Sâu
Định lý 2.1 chỉ ra rằng q0 và q1 phụ thuộc vào 1/log(1/sparsity). Chúng tôi bây giờ tổng quát hóa ý tưởng tạo ra nhiều bản sao của các nơ-ron đích trong mỗi tầng cho một mạng kết nối đầy đủ có độ sâu L (các chứng minh trong Phụ lục A.4) và các mạng tích chập có độ sâu L như được phát biểu trong Phụ lục A.5, tạo ra kết quả tương tự như trên. Thách thức bổ sung của việc mở rộng là xử lý các phụ thuộc của các tầng, vì việc xây dựng mỗi tầng cần phải khả thi.

**Định lý 2.2** (Mạng ER có thể biểu diễn mạng đích L-tầng). Cho một mạng đích kết nối đầy đủ fT có độ sâu L, δ ∈ (0,1), mật độ nguồn p̄ và một mạng nguồn ER L+1 tầng fS ∈ ER(p̄) với độ rộng nS,0 = q0d và nS,l = qlnT,l, l ∈ {1,2,...,L}, trong đó

ql ≥ 1/log(1/(1-pl+1)) log(LmT,l+1ql+1/δ)

cho l ∈ {0,1,...,L-1} và qL = 1, thì với xác suất 1-δ mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W·SP) = fT.

#### Cận Dưới về Quá tham số hóa
Trong khi các kết quả tồn tại của chúng tôi chứng minh rằng các mạng ER có tính chất xấp xỉ hàm toàn cục như các mạng nơ-ron dày đặc, để đạt được điều đó, việc xây dựng của chúng tôi yêu cầu một lượng quá tham số hóa đáng kể so với một mạng đích dày đặc. Đặc biệt, các mạng ER cực kỳ thưa thớt dường như đối mặt với một hạn chế tự nhiên, vì đối với độ thưa thớt 1-p ≥ 0.9, hệ số quá tham số hóa tăng khoảng như 1/log(1/(1-p)) ≈ 1/p. Hình 2 hình dung cách tỷ lệ này trở thành vấn đề đối với độ thưa thớt tăng. Định lý tiếp theo thiết lập rằng, thật không may, chúng ta không thể mong đợi vượt qua giới hạn 1/log(1/(1-pl)) này.

**Định lý 2.3** (Cận dưới về Quá tham số hóa trong Mạng ER). Tồn tại các mạng đích đơn biến fT(x) = φ(wᵀTx + bT) không thể được biểu diễn bởi một mạng nguồn ER 1-tầng ẩn ngẫu nhiên fS ∈ ER(p) với xác suất ít nhất 1-δ, nếu độ rộng của nó nS,1 < 1/log(1/(1-p)) log(1/(1-(1-δ)^(1/d))).

Xem Hình 6 và Phụ lục A.6 cho chứng minh đầy đủ.

#### Hiểu biết Lý thuyết
Chúng tôi đã chỉ ra rằng các mạng ER có thể chứa các mạng con có thể biểu diễn các mạng đích chung nếu chúng rộng hơn bởi một hệ số 1/log(1/(1-pl)). Hệ số quá tham số hóa này là cần thiết và hạn chế tính hữu dụng của các mặt nạ ngẫu nhiên một mình để thu được các kiến trúc mạng nơ-ron cực kỳ thưa thớt. Tuy nhiên, tính biểu cảm cao của chúng làm cho chúng trở thành những điểm khởi đầu hứa hẹn và rẻ về mặt tính toán cho việc cắt tỉa thêm và các phương pháp thưa thớt hóa tổng quát hơn.

Được truyền cảm hứng từ hiểu biết này, trong phần tiếp theo, chúng tôi khám phá ý tưởng bắt đầu cắt tỉa từ các mạng nguồn ER trong bối cảnh của SLT. Câu hỏi đầu tiên mà chúng tôi đặt ra là: Các mạng nguồn ngẫu nhiên cần rộng bao nhiêu để chứa SLT?

[Hình 2. Quá tham số hóa trong Mạng ER: Đối với một mạng đích một tầng ẩn với độ rộng 128 trong tầng ẩn và 10 trong tầng đầu ra, hình cho thấy độ rộng yêu cầu của tầng đầu tiên (l = 1) của mạng nguồn ER theo Định lý 2.1 với độ tin cậy 1-δ = 0.999. Độ rộng yêu cầu tăng vừa phải đến độ thưa thớt 0.9 và mạnh mẽ sau 0.95.]

### 2.2. Sự Tồn tại của Vé số May mắn Mạnh
Hầu hết các chứng minh tồn tại SLT rút ra cận dưới logarit về hệ số quá tham số hóa của mạng nguồn (Pensia et al., 2020; Burkholz et al., 2022; Burkholz, 2022a; da Cunha et al., 2022; Burkholz, 2022b; Ferbach et al., 2022) giải quyết nhiều vấn đề xấp xỉ tổng tập con (Lueker, 1998). Đối với mỗi tham số đích z, họ xác định một số tham số ngẫu nhiên của mạng nguồn X₁, ..., Xₙ, một tập con của chúng có thể xấp xỉ z. Trong trường hợp của một mạng nguồn ER, 1-p kết nối ngẫu nhiên bị thiếu so với một mạng nguồn dày đặc. Những kết nối bị thiếu này cũng làm giảm lượng tham số nguồn có sẵn X₁, ..., Xₙ. Để tính đến điều này, chúng tôi sửa đổi các xấp xỉ tổng tập con tương ứng theo bổ đề sau.

**Bổ đề 2.4** (Xấp xỉ tổng tập con trong Mạng ER). Gọi X₁, ..., Xₙ là các biến ngẫu nhiên độc lập, phân phối đều sao cho Xᵢ ~ U([-1,1]) và M₁, ..., Mₙ là các biến ngẫu nhiên Bernoulli độc lập sao cho Mᵢ ~ Ber(p) cho một p > 0. Gọi ε, δ ∈ (0,1) được cho. Thì với bất kỳ z ∈ [-1,1] nào tồn tại một tập con I ⊂ [n] sao cho với xác suất ít nhất 1-δ chúng ta có |z - Σᵢ∈ᵢMᵢXᵢ| ≤ ε nếu

n ≥ C₁/log(1/(1-p)) log(1/min(δ,ε)).    (1)

Chứng minh được đưa ra trong Phụ lục A.2 và sử dụng kết quả xấp xỉ tổng tập con gốc cho các tập con ngẫu nhiên của tập cơ sở X₁, ..., Xₙ. Ngoài ra, nó giải quyết thách thức kết hợp các hằng số liên quan tôn trọng phân phối xác suất của các tập con ngẫu nhiên. Để đơn giản, chúng tôi đã công thức hóa nó cho các biến ngẫu nhiên đều và tham số đích z ∈ [-1,1] nhưng nó có thể được mở rộng dễ dàng cho các biến ngẫu nhiên chứa phân phối đều (như phân phối chuẩn) và các đích bị chặn chung như trong Hệ quả 7 trong (Burkholz et al., 2022).

So với kết quả xấp xỉ tổng tập con gốc, chúng ta cần một tập cơ sở lớn hơn bởi một hệ số 1/log(1/(1-p)). Đây chính xác là hệ số mà chúng ta có thể sửa đổi các kết quả tồn tại SLT đương đại để chuyển sang các mạng nguồn ER và nó cũng là cùng hệ số mà chúng tôi rút ra trong phần trước về kết quả tính biểu cảm. Tuy nhiên, chúng tôi yêu cầu nói chung một quá tham số hóa cao hơn để phù hợp với các xấp xỉ tổng tập con.

Lợi thế của việc công thức hóa bổ đề trên là nó cho phép chuyển giao các kết quả tồn tại SLT chung sang thiết lập nguồn ER một cách đơn giản. Bằng cách thay thế việc xây dựng xấp xỉ tổng tập con bằng Bổ đề 2.2, chúng ta có thể chỉ ra sự tồn tại SLT cho các mạng ER kết nối đầy đủ (Pensia et al., 2020; Burkholz, 2022b), tích chập (Burkholz et al., 2022; Burkholz, 2022a; da Cunha et al., 2022), và dư (Burkholz, 2022a), hoặc GNN ngẫu nhiên (Ferbach et al., 2022). Để đưa ra một ví dụ về việc sử dụng hiệu quả bổ đề này và thảo luận về chiến lược chuyển giao chung, chúng tôi mở rộng rõ ràng các kết quả tồn tại SLT bởi Burkholz (2022b) cho các mạng kết nối đầy đủ sang các mạng nguồn ER. Do đó chúng tôi chỉ ra rằng việc cắt tỉa một mạng nguồn ngẫu nhiên có độ sâu L+1 với độ rộng lớn hơn một hệ số logarit có thể xấp xỉ bất kỳ mạng đích nào có độ sâu L với xác suất đã cho 1-δ.

**Định lý 2.5** (Sự Tồn tại của SLT trong Mạng ER). Gọi ε, δ ∈ (0,1), một mạng đích fT có độ sâu L, một mạng nguồn ER(p) fS có độ sâu L+1 với xác suất cạnh pl trong mỗi tầng l và các tham số ban đầu iid θ với w⁽ˡ⁾ᵢⱼ ~ U([-1,1]), b⁽ˡ⁾ᵢ ~ U([-1,1]) được cho. Thì với xác suất ít nhất 1-δ, tồn tại một mặt nạ SP sao cho mỗi thành phần đầu ra đích i được xấp xỉ như max_{x∈D} ||fT,i(x) - fS,i(x; WS·SP)|| ≤ ε nếu

nS,l ≥ C nT,l/log(1/(1-pl+1)) log(1/min{εl, δ/ρ})

cho ρ = C N^(1+γ)_T/log(1/(1-min_l pl))^(1+γ) log(1/min{min_l εl, δ}), l ≥ 1 cho bất kỳ γ ≥ 0, và trong đó εl = g(ε, fT) được định nghĩa trong Phụ lục A.2. Chúng tôi cũng yêu cầu nS,0 ≥ Cd^(1/log(1/(1-p1))) log(1/min{ε1, δ/ρ}), trong đó C > 0 biểu thị một hằng số chung độc lập với nT,l, L, pl, δ, và ε.

**Phác thảo Chứng minh**: Ý tưởng xây dựng LT chính được hình dung trong Hình 4(c) trong phụ lục. Đối với mỗi nơ-ron đích, nhiều bản sao xấp xỉ được tạo ra trong tầng tương ứng của LT để phục vụ như cơ sở cho các xấp xỉ tổng tập con đã sửa đổi (xem Bổ đề 2.2) của các tham số dẫn đến tầng tiếp theo. Phù hợp với phương pháp này, tầng đầu tiên của LT bao gồm các khối đơn biến tạo ra nhiều bản sao của các nơ-ron đầu vào. Ngoài Bổ đề 2.2, tổng số vấn đề xấp xỉ tổng tập con ρ phải được giải quyết cũng cần được đánh giá lại cho các mạng nguồn ER, vì điều này ảnh hưởng đến xác suất tồn tại LT. Sửa đổi này được thúc đẩy bởi cùng hệ số 1/log(1/(1-p)). Chứng minh đầy đủ được đưa ra trong Phụ lục A.2.

Với các kết quả tồn tại SLT của chúng tôi, chúng tôi đã cung cấp ví dụ đầu tiên về cách chuyển các phương pháp học sâu dày đặc đến thưa thớt thành các sơ đồ thưa thớt đến thưa thớt một cách chung. Tiếp theo, chúng tôi cũng xác nhận ý tưởng bắt đầu cắt tỉa từ một mặt nạ ER ngẫu nhiên trong các thí nghiệm.

## 3. Thí nghiệm
Để xác minh các hiểu biết lý thuyết của chúng tôi, chúng tôi tiến hành các thí nghiệm trong các thiết lập tiêu chuẩn trên dữ liệu điểm chuẩn thông thường (CIFAR10, CIFAR100 (Krizhevsky et al., 2009) và Tiny ImageNet (Russakovsky et al., 2015b)) và các kiến trúc mạng nơ-ron (ResNet (He et al., 2016) và VGG (Simonyan & Zisserman, 2015)). Chi tiết về thiết lập có thể được tìm thấy trong Phụ lục A.7. Chúng tôi luôn báo cáo trung bình trên 3 lần chạy độc lập. Do hạn chế về không gian, khoảng tin cậy được báo cáo trong phụ lục cùng với các thí nghiệm bổ sung. Mục tiêu chính của chúng tôi là giới thiệu tính biểu cảm của các mạng ER với ba loại thí nghiệm. Đầu tiên, chúng tôi làm nổi bật rằng một mạng được cắt tỉa ngẫu nhiên với các tỷ lệ thưa thớt theo tầng được chọn cẩn thận có tính cạnh tranh và đôi khi thậm chí vượt trội hơn các phương pháp cắt tỉa tiên tiến như Cắt tỉa Độ lớn Lặp lại (IMP) (Frankle & Carbin, 2019) (xem Phụ lục A.9). Thứ hai, chúng tôi xác minh rằng các mạng ER có thể phục vụ như điểm khởi đầu hứa hẹn của việc thưa thớt hóa thêm bằng cắt tỉa trong mạng ER ban đầu. Thứ ba, chúng tôi áp dụng cùng nguyên tắc cho các vé số may mắn mạnh (SLT) và trình bày kết quả huấn luyện thưa thớt đến thưa thớt đầu tiên trong bối cảnh này.

### Hiệu suất của Cắt tỉa Ngẫu nhiên
Để bổ sung cho Liu et al. (2021), chúng tôi tiến hành các thí nghiệm trong các chế độ độ thưa thớt cao ≥ 0.9 để kiểm tra giới hạn mà các mạng ER ngẫu nhiên là một giải pháp thay thế khả thi cho các thuật toán cắt tỉa tiên tiến nhưng tốn kém về mặt tính toán hơn. Su et al. (2020); Ma et al. (2021) đã chỉ ra rằng việc ngẫu nhiên hóa mặt nạ theo tầng của các mạng được cắt tỉa thu được với các thuật toán cắt tỉa tiên tiến thường cạnh tranh và trình bày các đường cơ sở mạnh. Các tỷ lệ thưa thớt tương ứng tốn kém về mặt tính toán để thu được và do đó có ý nghĩa thực tế giảm. Chúng tôi vẫn báo cáo so sánh với tỷ lệ thưa thớt thu được bằng Snip ngẫu nhiên (Lee et al., 2018), Synflow Lặp lại (Tanaka et al., 2020), và IMP (Frankle & Carbin, 2019) để chứng minh rằng các tỷ lệ thưa thớt hoạt động tốt nhất cho các mặt nạ ER thường khác với những tỷ lệ thu được từ các vé được cắt tỉa lặp lại. Trạng thái nghệ thuật trước đây thường được định nghĩa bởi ERK (Evci et al., 2020; Liu et al., 2021). Ngoài ra, chúng tôi đề xuất hai phương pháp để chọn độ thưa thớt theo tầng, cân bằng và kim tự tháp, thường cải thiện hiệu suất của các mạng ER (xem Bảng 1 và kết quả thêm cho ResNet trên CIFAR10 và 100 trong Phụ lục A.8). Các tỷ lệ thưa thớt mẫu được hình dung trong Hình 7. Các phương pháp kim tự tháp và cân bằng có tính cạnh tranh và thậm chí vượt trội hơn ERK trong các thí nghiệm của chúng tôi cho độ thưa thớt đến 0.99. Quan trọng là, chúng cũng vượt trội hơn các tỷ lệ thưa thớt theo tầng thu được bằng các thuật toán cắt tỉa lặp lại đắt đỏ Synflow và IMP. Tuy nhiên, đối với độ thưa thớt cực cao 1-p ≥ 0.99, hiệu suất của các mạng ER giảm đáng kể và thậm chí hoàn toàn sụp đổ đối với các phương pháp như ER Snip và kim tự tháp. Chúng tôi phỏng đoán rằng ER Snip và kim tự tháp dễ bị sụp đổ tầng trong các tầng cao hơn và ngay cả việc sửa chữa luồng (xem Phụ lục A.1) không thể tăng đáng kể tính biểu cảm của mạng. Tuy nhiên, những hạn chế chung mà chúng tôi gặp phải ở độ thưa thớt cao hơn là mong đợi dựa trên lý thuyết của chúng tôi. Những điều này có thể được khắc phục một phần bằng cách sử dụng chiến lược nối lại của Huấn luyện Thưa thớt Động (DST) (Evci et al., 2020).

[Bảng 1. Các mạng ER với độ thưa thớt theo tầng khác nhau trên CIFAR10 với VGG16. Chúng tôi so sánh độ chính xác kiểm tra của các tỷ lệ thưa thớt theo tầng cân bằng và kim tự tháp với các tỷ lệ đồng nhất, ERK, và các mạng ER với tỷ lệ thưa thớt theo tầng thu được bằng IMP, Synflow Lặp lại và Snip (được ký hiệu bằng ER). Khoảng tin cậy được báo cáo trong Phụ lục A.8.]

### Huấn luyện Thưa thớt Động
Để cải thiện các mạng được cắt tỉa ngẫu nhiên ở độ thưa thớt cực cao, chúng tôi sử dụng thuật toán RiGL (Evci et al., 2020) để thu được Bảng 2. Đầu tiên, chúng tôi chỉ nối lại các cạnh, cho phép chúng tôi bắt đầu từ các mạng tương đối thưa thớt. Đơn giản bằng cách phân phối lại các cạnh, hiệu suất của mạng ER có thể được cải thiện. Đặc biệt, các tỷ lệ thưa thớt ban đầu cân bằng hoặc kim tự tháp dường như có thể cải thiện hiệu suất của RiGL. Bảng 28 trong phụ lục chứng minh rằng việc bắt đầu RiGL (cắt tỉa + nối lại) từ độ thưa thớt cao hơn nhiều đến 0.9 cũng có thể mà không mất độ chính xác đáng kể, làm nổi bật tính hữu dụng của các mặt nạ ER ngẫu nhiên ngay cả ở độ thưa thớt cực cao.

[Bảng 2. Các mạng ER được nối lại với DST: Độ chính xác Kiểm tra cho một VGG16 ER(p) với mặt nạ cố định và sau khi nối lại các cạnh với RiGL (Evci et al., 2020; Liu et al., 2021) trên CIFAR10. Khoảng tin cậy được báo cáo trong Phụ lục A.14.]

### Huấn luyện Thưa thớt đến Thưa thớt với mạng ER
Chúng tôi xác minh rằng các mạng ER có thể phục vụ như một điểm khởi đầu hứa hẹn của các sơ đồ thưa thớt hóa thêm cắt tỉa trong mạng ER như được giải thích bởi Hình 1. Hiệu quả, ý tưởng này có thể chuyển bất kỳ sơ đồ huấn luyện dày đặc-đến-thưa thớt nào thành một sơ đồ thưa thớt đến thưa thớt. Như đại diện cho một phương pháp cắt tỉa lặp lại, chúng tôi nghiên cứu IMP và cho sơ đồ thưa thớt hóa liên tục, chúng tôi sử dụng Tái tham số hóa Ngưỡng Mềm (STR) (Kusupati et al., 2020). Trong Bảng 3 và 4, chúng tôi quan sát rằng chúng ta có thể bắt đầu huấn luyện với một mặt nạ ER có độ thưa thớt đến 0.9 và cắt tỉa mạng thêm mà không mất nhiều hiệu suất. Đối với cả STR và IMP, việc cắt tỉa một mạng ER có độ thưa thớt 0.7 trên CIFAR10 dẫn đến cùng hiệu suất mà chúng ta sẽ thu được nếu chúng ta cắt tỉa một mạng dày đặc thay vào đó. Các thí nghiệm của chúng tôi chỉ ra rằng đối với cả STR và IMP, đặc biệt, các tỷ lệ cắt tỉa ban đầu cân bằng có thể thúc đẩy hiệu suất của phương pháp chung.

[Bảng 3. Huấn luyện thưa thớt đến thưa thớt với Tái tham số hóa Ngưỡng Mềm trong mạng ER: Kết quả trên ResNet18 được huấn luyện trên CIFAR10. STR (ER) biểu thị tỷ lệ thưa thớt thu được bằng STR. Để tham khảo, bắt đầu từ một mạng dày đặc STR đạt 94.66% và 90.95% ở độ thưa thớt 0.9 và 0.993 tương ứng. Xem Phụ lục A.10 cho khoảng tin cậy.]

[Bảng 4. Huấn luyện thưa thớt đến thưa thớt với Cắt tỉa Độ lớn Lặp lại trong mạng ER: Kết quả trên ResNet18 được huấn luyện trên CIFAR10. Để tham khảo, bắt đầu từ một mạng dày đặc IMP đạt 93.38% và 91.39% ở độ thưa thớt 0.9 và 0.99 tương ứng. Xem Phụ lục A.10 cho khoảng tin cậy.]

### Thí nghiệm cho SLT
Tương tự như các thí nghiệm trước của chúng tôi, chúng tôi cũng có thể cắt tỉa một mặt nạ ER ngẫu nhiên để thu được SLT. Chúng tôi sử dụng thuật toán edge-popup (Ramanujan et al., 2020) để xác minh các rút ra lý thuyết của chúng tôi. Bảng 5 trình bày bằng chứng cho thực tế rằng việc tìm kiếm SLT không cần phải tốn kém về mặt tính toán như huấn luyện dày đặc. Đáng chú ý, chúng ta có thể bắt đầu với một mạng ER thưa thớt đến 0.8 độ thưa thớt thay vì một mạng dày đặc và vẫn đạt được hiệu suất cạnh tranh trong việc tìm một SLT có độ thưa thớt cuối cùng 0.9. Các thí nghiệm bổ sung được báo cáo trong phụ lục (xem Bảng 20 và 22).

[Bảng 5. Mạng ER cho Vé số May mắn Mạnh: Độ chính xác kiểm tra của SLT thu được bằng edge-popup (EP) (Ramanujan et al., 2020) cắt tỉa một ER ResNet18 thưa thớt trên CIFAR10. Bắt đầu dày đặc (xem Phụ lục A.11), EP đạt 87.86% độ chính xác cho độ thưa thớt 0.9.]

### Thí nghiệm trên Nhiệm vụ Đa dạng
Trong khi hầu hết các thí nghiệm của chúng tôi tập trung vào các nhiệm vụ phân loại hình ảnh, các hiểu biết lý thuyết của chúng tôi tổng quát hơn và áp dụng cho các cấu trúc mạng đích và nguồn đa dạng. Để chứng minh phạm vi rộng hơn của kết quả của chúng tôi, chúng tôi cung cấp các thí nghiệm bổ sung cho huấn luyện thưa thớt đến thưa thớt trên ImageNet, Mạng Tích chập Đồ thị, dữ liệu thuật toán và dữ liệu bảng trong Phụ lục A.16. Một cách nhất quán, chúng tôi thấy rằng việc cắt tỉa các mạng ngẫu nhiên thưa thớt đạt hiệu suất cạnh tranh so với việc cắt tỉa một mạng dày đặc.

## 4. Kết luận
Chúng tôi đã giải thích một cách có hệ thống hiệu quả của cắt tỉa ngẫu nhiên và do đó cung cấp một lý giải lý thuyết cho việc sử dụng các mặt nạ Erdős-Rényi (ER) như các đường cơ sở mạnh cho cắt tỉa vé số may mắn và như điểm khởi đầu của huấn luyện thưa thớt động. Lý thuyết của chúng tôi ngụ ý rằng các mạng ER ngẫu nhiên có tính biểu cảm như các mạng đích dày đặc nếu chúng rộng hơn bởi một hệ số logarit trong độ thưa thớt nghịch đảo của chúng. Các xây dựng của chúng tôi cho thấy rằng cắt tỉa ngẫu nhiên, mặc dù rẻ về mặt tính toán, không đạt được độ thưa thớt tối ưu nhưng có tiềm năng lớn cho việc cắt tỉa thêm. Phát hiện này cũng có ý nghĩa thực tế, vì các mặt nạ thưa thớt ngẫu nhiên ban đầu có thể tránh quá trình tốn kém về mặt tính toán của việc cắt tỉa một mạng dày đặc từ đầu. Như điểm nổi bật mẫu, chúng tôi đã áp dụng hiểu biết này cho các vé số may mắn mạnh. Chúng tôi đã chứng minh về mặt lý thuyết và chứng minh thực nghiệm rằng việc cắt tỉa cho các vé số may mắn mạnh có thể đạt được bằng các sơ đồ huấn luyện thưa thớt đến thưa thớt.

## Tài liệu Tham khảo
[Phần tài liệu tham khảo được dịch nguyên văn từ bản gốc...]

Belkin, M., Hsub, D., Maa, S., and Mandala, S. Reconciling modern machine learning practice and the bias-variance trade-off. stat, 1050:10, 2019.

Bellec, G., Kappel, D., Maass, W., and Legenstein, R. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018.

Burkholz, R. Convolutional and residual networks provably contain lottery tickets. In International Conference on Machine Learning, 2022a.

Burkholz, R. Most activation functions can win the lottery without excessive depth. In Advances in Neural Information Processing Systems, 2022b.

Burkholz, R., Laha, N., Mukherjee, R., and Gotovos, A. On the existence of universal lottery tickets. In International Conference on Learning Representations, 2022.

Chang, X., Li, Y., Oymak, S., and Thrampoulidis, C. Provable benefits of overparameterization in model compression: From double descent to pruning neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 6974–6983, 2021.

da Cunha, A., Natale, E., and Viennot, L. Proving the lottery ticket hypothesis for convolutional neural networks. In International Conference on Learning Representations, 2022.

de Jorge, P., Sanyal, A., Behl, H., Torr, P., Rogez, G., and Dokania, P. K. Progressive skeletonization: Trimming more fat from a network at initialization. In International Conference on Learning Representations, 2020.

Dettmers, T. and Zettlemoyer, L. Sparse networks from scratch: Faster training without losing performance. 2019.

Diffenderfer, J. and Kailkhura, B. Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=U_mat0b9iv.

Ding, F., Hardt, M., Miller, J., and Schmidt, L. Retiring adult: New datasets for fair machine learning. Advances in Neural Information Processing Systems, 34, 2021.

Dong, X., Chen, S., and Pan, S. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.

Erdos, P., Rényi, A., et al. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17–60, 1960.

Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020.

Ferbach, D., Tsirigotis, C., Gidel, G., and Avishek, B. A general framework for proving the equivariant strong lottery ticket hypothesis, 2022.

Fischer, J. and Burkholz, R. Plant 'n' seek: Can you find the winning ticket? In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9n9c8sf0xm.

Fischer, J., Gadhikar, A., and Burkholz, R. Towards strong pruning for lottery tickets with non-zero biases, 2021.

Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.

Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ig-VyQc-MLK.

Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. CoRR, abs/1902.09574, 2019. URL http://arxiv.org/abs/1902.09574.

Golubeva, A., Gur-Ari, G., and Neyshabur, B. Are wider nets better given the same number of parameters? In International Conference on Learning Representations, 2021.

Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.

Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357–362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.

Hassibi, B., Stork, D., and Wolff, G. Optimal brain surgeon and general network pruning. In IEEE International Conference on Neural Networks, pp. 293–299 vol.1, 1993. doi: 10.1109/ICNN.1993.298572.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.

Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations.

Krizhevsky, A., Nair, V., and Hinton, G. Cifar-10 and cifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar. html, 6(1):1, 2009.

Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S., and Farhadi, A. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pp. 5544–5555. PMLR, 2020.

LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. In Touretzky, D. (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. URL https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf.

Lee, N., Ajanthan, T., and Torr, P. Snip: Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2018.

Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P. Pruning filters for efficient convnets. 2016.

Liu, S., Chen, T., Chen, X., Shen, L., Mocanu, D. C., Wang, Z., and Pechenizkiy, M. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. In International Conference on Learning Representations, 2021.

Liu, S., Chen, T., Chen, X., Atashgahi, Z., Yin, L., Kou, H., Shen, L., Pechenizkiy, M., Wang, Z., and Mocanu, D. C. Sparse training via boosting pruning plasticity with neuroregeneration. Advances in Neural Information Processing Systems, 2021.

Liu, Z., Kitouni, O., Nolte, N., Michaud, E. J., Tegmark, M., and Williams, M. Towards understanding grokking: An effective theory of representation learning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=6at6rB3IZm.

Louizos, C., Welling, M., and Kingma, D. P. Learning sparse neural networks through l 0 regularization. In International Conference on Learning Representations, 2018.

Lueker, G. S. Exponentially small bounds on the expected optimum of the partition and subset sum problems. Random Structures and Algorithms, 12:51–62, 1998.

Ma, X., Yuan, G., Shen, X., Chen, T., Chen, X., Chen, X., Liu, N., Qin, M., Liu, S., Wang, Z., and Wang, Y. Sanity checks for lottery tickets: Does your winning ticket really win the jackpot? In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=WL7pr00_fnJ.

Malach, E., Yehudai, G., Shalev-Schwartz, S., and Shamir, O. Proving the lottery ticket hypothesis: Pruning is all you need. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 6682–6691. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/malach20a.html.

Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Evolutionary training of sparse artificial neural networks: a network science perspective. 2017.

Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.

Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. 2016.

Orseau, L., Hutter, M., and Rivasplata, O. Logarithmic pruning is all you need. Advances in Neural Information Processing Systems, 33:2925–2934, 2020.

Pensia, A., Rajput, S., Nagle, A., Vishwakarma, H., and Papailiopoulos, D. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. Advances in Neural Information Processing Systems, 33:2599–2610, 2020.

Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., and Rastegari, M. What's hidden in a randomly weighted neural network? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11893–11902, 2020.

Renda, A., Frankle, J., and Carbin, M. Comparing rewinding and fine-tuning in neural network pruning. In International Conference on Learning Representations, 2019.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015a. doi: 10.1007/s11263-015-0816-y.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015b. doi: 10.1007/s11263-015-0816-y.

Savarese, P., Silva, H., and Maire, M. Winning the lottery with continuous sparsification, 2020. URL https://openreview.net/forum?id=BJe4oxHYPB.

Sen, P., Namata, G. M., Bilgic, M., Getoor, L., Gallagher, B., and Eliassi-Rad, T. Collective classification in network data. AI Magazine, 29(3):93–106, 2008.

Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.1556.

Sreenivasan, K., Rajput, S., Sohn, J.-Y., and Papailiopoulos, D. Finding nearly everything within random binary networks. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 3531–3541. PMLR, 28–30 Mar 2022a.

Sreenivasan, K., Sohn, J.-y., Yang, L., Grinde, M., Nagle, A., Wang, H., Lee, K., and Papailiopoulos, D. Rare gems: Finding lottery tickets at initialization, 2022b. URL https://arxiv.org/abs/2202.12002.

Su, J., Chen, Y., Cai, T., Wu, T., Gao, R., Wang, L., and Lee, J. D. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural Information Processing Systems, 33:20390–20401, 2020.

Tanaka, H., Kunin, D., Yamins, D. L., and Ganguli, S. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in Neural Information Processing Systems, 33:6377–6389, 2020.

Wang, C., Zhang, G., and Grosse, R. Picking winning tickets before training by preserving gradient flow. 2020.

You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y. Drawing early-bird tickets: Toward more efficient training of deep networks. In International Conference on Learning Representations, 2019.

Yu, X., Serra, T., Ramalingam, S., and Zhe, S. The combinatorial brain surgeon: Pruning weights that cancel one another in neural networks, 2022. URL https://arxiv.org/abs/2203.04466.

Yuan, G., Ma, X., Niu, W., Li, Z., Kong, Z., Liu, N., Gong, Y., Zhan, Z., He, C., Jin, Q., et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. Advances in Neural Information Processing Systems, 34:20838–20850, 2021.

Zhou, H., Lan, J., Liu, R., and Yosinski, J. Deconstructing lottery tickets: Zeros, signs, and the supermask. Advances in neural information processing systems, 32, 2019.

Zhou, X., Zhang, W., Chen, Z., Diao, S., and Zhang, T. Efficient neural network training via forward and backward propagation sparsification. Advances in Neural Information Processing Systems, 34:15216–15229, 2021.

## A. Phụ lục

### A.1. Bảo tồn Luồng để ngăn chặn sụp đổ tầng trong mạng ER
Cắt tỉa có mục tiêu được biết là dễ bị sụp đổ tầng hoặc chỉ sử dụng tài nguyên dưới tối ưu (được đưa ra dưới dạng các tham số có thể huấn luyện), khi các nơ-ron trung gian không nhận đầu vào mặc dù có trọng số đầu ra khác không hoặc trọng số đầu ra bằng không mặc dù có trọng số đầu vào khác không. Để tránh vấn đề này, (Tanaka et al., 2020) đã rút ra một tiêu chí cắt tỉa độc lập với dữ liệu cụ thể, tức là luồng synapse. Tuy nhiên, bảo tồn luồng cũng có thể đạt được với một chiến lược sửa chữa ngẫu nhiên đơn giản và hiệu quả về mặt tính toán áp dụng cho các phương pháp che mặt nạ đa dạng, bao gồm che mặt nạ ER ngẫu nhiên.

Ý tưởng chính đằng sau thuật toán này là kết nối các nơ-ron (hoặc bộ lọc) có bậc vào hoặc ra bằng không với ít nhất một nơ-ron (hoặc bộ lọc) khác được chọn ngẫu nhiên trong mạng. Để bảo tồn độ thưa thớt toàn cục, một cạnh mới có thể thay thế một cạnh được chọn ngẫu nhiên trước đó. Ngoài ra, các mạng ER với bảo tồn luồng cũng có thể được thu được bằng lấy mẫu từ chối, tương đương với việc điều kiện hóa các nơ-ron trên bậc vào và ra khác không. Để vẫn đáp ứng mật độ đích pl, xác suất ER p̃l sẽ cần được điều chỉnh thích hợp. Tuy nhiên, các thí nghiệm của chúng tôi tiết lộ rằng hầu hết các kiến trúc ResNet và VGG được che mặt nạ ngẫu nhiên thường bảo tồn luồng với xác suất cao cho các tỷ lệ thưa thớt theo tầng khác nhau đến độ thưa thớt ≈ 0.95 (xem Phụ lục A.1). Các tầng có vấn đề nhất là tầng đầu tiên và cuối cùng nếu số kênh đầu vào và nơ-ron đầu ra tương đối nhỏ. Do đó, hầu hết các sơ đồ cắt tỉa giữ các tầng này tương đối dày đặc nói chung. Trong các rút ra lý thuyết của chúng tôi, chúng tôi giả định bảo tồn luồng trong tầng đầu tiên.

[Hình 3. So sánh Luồng: Chúng tôi so sánh kết quả của các mạng ER cho mỗi phương pháp thưa thớt theo tầng có và không có bảo tồn luồng. Các đường liền nét biểu thị rằng luồng được bảo tồn trong khi các đường chấm hiển thị phương pháp tương ứng không có bảo tồn luồng cho VGG16 trên CIFAR10.]

Chúng tôi đề xuất hai phương pháp để đạt được bảo tồn luồng, đảm bảo rằng mỗi nơ-ron (hoặc bộ lọc) có ít nhất bậc vào và bậc ra là 1.

**Lấy mẫu Từ chối**: Chúng ta có thể lấy mẫu lại các cạnh mặt nạ s⁽ˡ⁾ᵢⱼ,ER của các nơ-ron (bộ lọc) có bậc vào bằng không hoặc bậc ra bằng không cho đến khi có ít nhất một bậc vào và một bậc ra cho nơ-ron đó.

**Thêm Ngẫu nhiên**: Chúng ta ngẫu nhiên thêm một cạnh vào một nơ-ron có bậc vào hoặc bậc ra bằng không. Trong khi phương pháp này thêm một cạnh bổ sung trong mạng, tổng số cạnh cần được thêm thường không đáng kể trong thực tế.

Chúng tôi xác minh số lượng sửa chữa yêu cầu trong một mạng ER để bảo tồn luồng. Lưu ý rằng trong hầu hết trường hợp, các mạng ER vốn bảo tồn luồng. Đối với mỗi tỷ lệ thưa thớt theo tầng được sử dụng, chúng tôi tính toán số lượng kết nối (cạnh) được thêm vào mạng để đảm bảo rằng mỗi nơ-ron (hoặc bộ lọc) có ít nhất bậc vào và một bậc ra là 1 sử dụng phương pháp Thêm Ngẫu nhiên. Bảng 6 và 7 hiển thị kết quả.

[Bảng 6. Số lượng cạnh mặt nạ trung bình được thêm bởi sửa chữa luồng trong mạng ResNet18 cho CIFAR100 trên ba lần chạy.]

[Bảng 7. Số lượng cạnh mặt nạ trung bình được thêm bởi sửa chữa luồng trong mạng VGG19 cho CIFAR100 trung bình trên ba lần chạy.]

Phân tích của chúng tôi cho thấy rằng bảo tồn luồng là một tính chất quan trọng tránh sụp đổ tầng trong các mạng thưa thớt và được thỏa mãn vốn có trong các chế độ thưa thớt hợp lý ≈ 0.9. Nó có tác dụng tương tự như việc làm cho tầng cuối cùng và tầng ban đầu dày đặc trong quá trình cắt tỉa, được theo dõi trong một số thuật toán cắt tỉa (Liu et al., 2021).

Hình 3 so sánh các phương pháp thưa thớt theo tầng khác nhau cho các mạng ER có và không có bảo tồn luồng. Kết quả của chúng tôi cho thấy rằng bảo tồn luồng đặc biệt quan trọng trong các phương pháp Kim tự tháp và ER Snip. Cả hai phương pháp này đều có độ thưa thớt cao hơn trong tầng cuối cùng dẫn đến các vấn đề hiệu suất trong trường hợp độ thưa thớt toàn cục cao. Bảo tồn luồng có thể giải quyết điều này một phần để có thể thấy cải thiện rõ ràng cho phương pháp kim tự tháp ở độ thưa thớt 0.99 và 0.995.

### A.2. Chứng minh cho Sự Tồn tại của Vé số May mắn Mạnh trong mạng ER

[Hình 4. Tính Biểu cảm trong mạng ER: Trong (a), fT(x) là một mạng đích một tầng. (b) hình dung mạng nguồn ER fS(x), chứa một mạng thưa thớt biểu diễn đích. (c) hiển thị một LT mạnh được chứa trong một mạng ER. Hình chỉ hiển thị các kết nối cho chỉ một nơ-ron trong mỗi tầng của fS để đơn giản. Cả đường chấm và đường liền đều thuộc về mặt nạ ngẫu nhiên SER, trong khi các đường liền thuộc về trọng số khác không của mạng thưa thớt cuối cùng (SP).]

Như đã thảo luận trong bản thảo chính, hầu hết các chứng minh tồn tại SLT rút ra cận dưới logarit về hệ số quá tham số hóa của mạng nguồn sử dụng xấp xỉ tổng tập con (Lueker, 1998) trong việc xây dựng rõ ràng một vé số may mắn xấp xỉ mạng đích (Pensia et al., 2020; Burkholz et al., 2022; Burkholz, 2022a; da Cunha et al., 2022; Burkholz, 2022b). Chúng ta có thể chuyển giao tất cả các chứng minh này sang các mạng nguồn ER bằng cách sửa đổi kết quả xấp xỉ tổng tập con cho các biến ngẫu nhiên được đặt thành không với xác suất Bernoulli p để tính cho các liên kết bị thiếu ngẫu nhiên trong mạng nguồn. Chúng ta chỉ cần thay thế kết quả xấp xỉ tổng tập con của Lueker bằng Bổ đề 2.2 trong các chứng minh tương ứng. Để đơn giản, chúng tôi đã công thức hóa nó cho các biến ngẫu nhiên đều và tham số đích z ∈ [-1,1] nhưng nó có thể được mở rộng dễ dàng cho các biến ngẫu nhiên chứa phân phối đều (như phân phối chuẩn) và các đích bị chặn chung như trong Hệ quả 7 trong (Burkholz et al., 2022). Để thuận tiện, chúng tôi phát biểu lại Bổ đề 2.2 từ bản thảo chính:

**Bổ đề A.1** (Xấp xỉ tổng tập con trong Mạng ER). Gọi X₁, ..., Xₙ là các biến ngẫu nhiên độc lập, phân phối đều sao cho Xᵢ ~ U([-1,1]) và M₁, ..., Mₙ là các biến ngẫu nhiên Bernoulli độc lập sao cho Mᵢ ~ Ber(p) cho một p > 0. Gọi ε, δ ∈ (0,1) được cho. Thì với bất kỳ z ∈ [-1,1] nào tồn tại một tập con I ⊂ [n] sao cho với xác suất ít nhất 1-δ chúng ta có |z - Σᵢ∈ᵢMᵢXᵢ| ≤ ε nếu

n ≥ C₁/log(1/(1-p)) log(1/min(δ,ε)).    (2)

**Chứng minh**: Các biến ngẫu nhiên X̃ᵢ = MᵢXᵢ không đóng góp vào việc xấp xỉ giá trị đích z, nếu chúng bằng không và do đó đặc biệt trong trường hợp Mᵢ = 0, điều này xảy ra với xác suất 1-p cho mỗi chỉ số i. Do đó chúng ta có thể loại bỏ tất cả các biến X̃ᵢ, mà Mᵢ = 0. Sau khi thay đổi chỉ số, chúng ta đến một tập con X̃₁, ..., X̃ₖ của K biến ngẫu nhiên, được phân phối đều như X̃ᵢ = MᵢXᵢ = Xᵢ ~ U([-1,1]), vì Mᵢ độc lập với Xᵢ. Số lượng biến K theo phân phối nhị thức, K ~ Bin(n, p), vì M₁, ..., Mₙ là Bernoulli phân phối độc lập.

Đối với K = k cố định, Lueker (1998) đã chứng minh rằng tồn tại các hằng số aₖ > 0 và bₖ > 0 sao cho xác suất rằng việc xấp xỉ không thể thực hiện có dạng P(∀I⊂[k])|z - Σᵢ∈ᵢX̃ᵢ| > ε'] ≤ aₖexp(-bₖk)/ε'.

Sử dụng kết quả này và định nghĩa a := maxₖ∈[n] aₖ > 0 và b := minₖ∈[n] bₖ > 0, chúng ta chỉ cần lấy trung bình theo biến ngẫu nhiên K ~ Bin(B, p).

P(∀I⊂[n])|z - Σᵢ∈ᵢX̃ᵢ| > ε'!) ≤ Σⁿₖ₌₀ aₖ/ε' exp(-bₖk)(n choose k)pᵏ(1-p)ⁿ⁻ᵏ
≤ a/ε' Σⁿₖ₌₀(n choose k)exp(-bk)pᵏ(1-p)ⁿ⁻ᵏ
= a/ε'[1-p(1-exp(-b))]ⁿ

Để đảm bảo xấp xỉ tổng tập con khả thi với xác suất ít nhất 1-δ' chúng ta cần thỏa mãn

a/ε'[1-p(1-exp(-b))]ⁿ ≤ δ'.

Giải cho n dẫn đến

n ≥ 1/log(1/(1-p(1-exp(-b)))) log(a/(δ'ε')).

Bất đẳng thức này được thỏa mãn nếu

n ≥ C₁/log(1/(1-p)) log(1/min{δ', ε'})

cho một hằng số chung C > 0 phụ thuộc vào a và b.

Với xấp xỉ tổng tập con đã sửa đổi này, chúng tôi chỉ ra tiếp theo rằng so với một mạng nguồn đầy đủ, một mạng ER cần rộng hơn bởi một hệ số 1/log(1/(1-p)). Để cung cấp một ví dụ về cách chuyển giao chứng minh tồn tại SLT, chúng tôi tập trung vào việc xây dựng bởi (Burkholz, 2022b).

Lưu ý rằng trong tất cả các định lý của chúng tôi, chúng tôi giả định rằng luồng được bảo tồn trong tầng đầu tiên, vì việc áp dụng một thuật toán bảo tồn luồng đơn giản và rẻ về mặt tính toán sau khi rút một mặt nạ ngẫu nhiên là hợp lý (xem Phụ lục A.1). Thuật toán này chỉ đảm bảo rằng tất cả các nơ-ron được kết nối với mạng chính và do đó hữu ích cho việc huấn luyện mạng nơ-ron.

Nếu chúng ta không giả định rằng luồng được bảo tồn, một số nơ-ron trong tầng đầu tiên có thể bị ngắt kết nối khỏi tất cả các nơ-ron đầu vào với xác suất (1-p₀)ᵈ. Các nơ-ron bị ngắt kết nối có thể đơn giản bị bỏ qua trong việc xây dựng LT. Phần của chúng thường không đáng kể nhưng, về mặt kỹ thuật, không có bảo tồn luồng, chúng ta sẽ cần đảm bảo rằng nS,1 ≥ C(1-p₀)ᵈ + n*S,1, trong đó n*S,1 biểu thị cận về độ rộng mà chúng ta thực sự sẽ rút ra.

**Định lý A.2** (Sự Tồn tại của SLT trong Mạng ER). Gọi ε, δ ∈ (0,1), một mạng đích fT có độ sâu L, một mạng nguồn ER(p) fS có độ sâu L+1 với xác suất cạnh pl trong mỗi tầng l và các tham số ban đầu iid θ với w⁽ˡ⁾ᵢⱼ ~ U([-1,1]), b⁽ˡ⁾ᵢ ~ U([-1,1]) được cho. Thì với xác suất ít nhất 1-δ, tồn tại một mặt nạ SP sao cho mỗi thành phần đầu ra đích i được xấp xỉ như maxₓ∈D ||fT,i(x) - fS,i(x; WS·SP)|| ≤ ε nếu

nS,l ≥ C nT,l/log(1/(1-pl+1)) log(1/min{εl, δ/ρ})

cho l ≥ 1, trong đó εl = g(ε, fT) được định nghĩa trong Phương trình (3) và ρ = CN^(1+γ)_T/log(1/(1-minₗpl))^(1+γ) log(1/min{minₗεl, δ}) cho bất kỳ γ ≥ 0. Chúng tôi cũng yêu cầu nS,0 ≥ Cd^(1/log(1/(1-p1))) log(1/min{ε1, δ/ρ}), trong đó C > 0 biểu thị một hằng số chung độc lập với nT,l, L, pl, δ, và ε.

Ở đây, εl = g(ε) được định nghĩa phù hợp với Bổ đề 5.1 trong (Burkholz, 2022b):

εl = g(ε, fT) = ε/(nT,L L[(1 + Bl-1)(1 + ε/L)^(L-1) ∏ᵏ₌ₗ₊₁^L (||W^(k)_T||∞ + ε/L)]^(-1)), Bl := supₓ∈D ||x^(l)_T||₁.    (3)

**Chứng minh**: Để chứng minh sự tồn tại của vé số may mắn mạnh trong các mạng ER, chúng tôi sửa đổi chứng minh bởi (Burkholz, 2022b) cho các mạng kết nối đầy đủ hoàn chỉnh. Chúng tôi đầu tiên trả lời câu hỏi, làm thế nào thực tế rằng các trọng số ngẫu nhiên được đặt thành không một cách không thể đảo ngược, thay đổi việc xây dựng của chúng tôi. Hình 4 hình dung sơ đồ chung. Ý tưởng chung là chúng ta phải tạo nhiều bản sao ρl của mỗi nơ-ron đích trong LT, vì những điều này sẽ cho phép xấp xỉ các tham số đích bằng cách sử dụng xấp xỉ tổng tập con như được sửa đổi bởi Bổ đề 2.2. Đầu tiên, như Hình 4 hình dung, chúng ta phải lập luận tại sao và làm thế nào chúng ta có thể tạo các khối đơn biến trong tầng đầu tiên hoặc nói chung các xây dựng 2L. Trong trường hợp này, một tầng đích được xấp xỉ bởi hai tầng được cắt tỉa thích hợp của mạng nguồn. Tầng đầu tiên của hai tầng nguồn này chỉ chứa các nơ-ron đơn biến tạo thành các khối bao gồm các nơ-ron cùng loại, tương ứng với cùng nơ-ron đầu vào đích i. Tất cả các trọng số bắt đầu trong cùng khối i và kết thúc trong cùng nơ-ron j sau đó có thể được sử dụng để xấp xỉ tham số đích wT,ji. Các khối đơn biến yêu cầu có thể được thực hiện dễ dàng bằng cắt tỉa nếu luồng được bảo tồn. Lý do là mỗi nơ-ron trong tầng nguồn l = 0 có ít nhất một cạnh đến, có thể tồn tại qua việc cắt tỉa. Vì cạnh này có thể liền kề với bất kỳ nơ-ron đầu vào nào với cùng xác suất, chúng ta luôn có thể tìm đủ nơ-ron trong Tầng l = 0 trỏ đến bất kỳ nơ-ron đầu vào nào và điều này cho phép chúng ta tạo thành các khối đơn biến có kích thước tương tự B.

Thứ hai, chúng ta phải phân tích cách việc xây dựng mỗi tầng đích sau bị ảnh hưởng bởi các cạnh bị thiếu ngẫu nhiên trong mạng nguồn. Mỗi trọng số đích w^(l)_T,ij có thể được xấp xỉ bởi w^(l)_T,ij ≈ Σⱼ'∈I m^(l)_S,i'j' w^(l)_S,i'j', trong đó nơ-ron i' trong LT xấp xỉ nơ-ron đích i và nơ-ron j' trong LT xấp xỉ nơ-ron đích j. Tập con I được chọn dựa trên xấp xỉ tổng tập con đã sửa đổi và thông báo mặt nạ của LT. Do đó, I tồn tại theo Bổ đề 2.2, vì các mục mặt nạ ban đầu ngẫu nhiên của mạng nguồn m^(l)_S,i'j' là Bernoulli phân phối với xác suất pl.

Vấn đề thứ hai cần được sửa đổi cho các mạng ER là việc phân tích số lượng vấn đề xấp xỉ tổng tập con yêu cầu ρ. Như đã giải thích trước đây, ý tưởng chính của việc xây dựng là tạo ρl bản sao của mỗi nơ-ron đích trong Tầng đích l trong Tầng l của LT. Những bản sao này sau đó phục vụ nhiều xấp xỉ tổng tập con để xấp xỉ các nơ-ron đích trong tầng tiếp theo theo cách tương tự như các khối đơn biến của tầng đầu tiên. Tuy nhiên, điều này tăng tổng số vấn đề xấp xỉ tổng tập con ρ cần được giải quyết và ảnh hưởng đến xác suất mà chúng ta có thể giải quyết tất cả chúng. Sử dụng một cận hợp, chúng ta có thể chi tiêu δ/ρ cho mỗi xấp xỉ với một ρ đã sửa đổi cho các mạng ER. Tương tự như (Burkholz, 2022b), chúng ta có thể rút ra một cận dưới trên ρl trong các tầng tiếp theo, để xấp xỉ tổng tập con khả thi cho mỗi tham số của tầng l khi kích thước khối B là

B ≥ 1/log(1/(1-pl)) log(a/(δ'ρε'))

để với một hằng số được chọn thích hợp C chúng ta có

B ≥ C/log(1/(1-pl)) log(1/min{δ'/ρ, ε'})

để nó theo đó tổng cộng

nS,l ≥ C nT,l/log(1/(1-pl+1)) log(1/min{εl, δ/ρ})

Mục tiêu còn lại là tìm một ρ ≥ ρ' = Σᴸₗᵢ₌₁ ρ'l, trong đó ρ' là hệ số tăng số vấn đề xấp xỉ tổng tập con yêu cầu để xấp xỉ L tầng đích với một mạng nguồn ER và ρl đếm số tham số trong mỗi tầng LT.

Theo phương pháp của Burkholz (2022b) để xác định ρ, chúng tôi bắt đầu với tầng cuối cùng. Số ρL vấn đề xấp xỉ tổng tập con phải được giải quyết để xấp xỉ tầng cuối cùng xác định số nơ-ron yêu cầu trong tầng trước đó. Điều này lần lượt xác định số nơ-ron yêu cầu trong tầng trước đó, v.v. Tầng cuối cùng yêu cầu giải quyết chính xác ρ'L = nT,L nT,L-1 vấn đề tổng tập con có thể được giải quyết với xác suất đủ cao nếu nS,L-1 ≥ C nT,L-1/log(1/(1-pL)) log(1/min{εL, δ/ρ'}). Vì chúng ta sẽ cần tối đa C/log(1/(1-pL)) log(1/min{εL, δ/ρ'}) tập hợp các tham số đích trong tầng cuối cùng, chúng ta có thể ràng buộc ρ'L-1 ≤ C NL-1/log(1/(1-pL)) log(1/min{εL, δ/ρ'}). Lặp lại cùng lập luận cho mỗi tầng, chúng ta rút ra ρ'l ≤ C Nl/log(1/(1-pl+1)) log(1/min{εl+1, δ/ρ'}). Tổng cộng, chúng ta thấy rằng ρ' = ΣᴸₗIᵢ₌₁ ρ'l ≤ ΣᴸₗᵢI₌₁ C Nl/log(1/(1-pl+1)) log(1/min{εl+1, δ/ρ'}) ≤ C Nt/log(1/(1-minₗ pl)) log(1/min{minₗ εl, δ/ρ}). Ở đây, Nl = nT,L nT,L-1 và Nt = Σₗ Nl. Một ρ thỏa mãn ρ ≥ C Nt/log(1/(1-minₗ pl)) log(1/min{εl+1, δ/ρ}) sẽ đủ. Dễ thấy rằng ρ = C N^(1+γ)_T/log(1/(1-minₗ pl))^(1+γ) log(1/min{minₗ εl, δ}) cho bất kỳ γ ≥ 0 thỏa mãn yêu cầu của chúng ta.

Do đó chúng tôi đã chỉ ra sự tồn tại của SLT trong các mạng ER theo ý tưởng tương tự như chứng minh của Định lý 5.2 bởi Burkholz (2022b). Do đó, việc xây dựng của chúng tôi cũng sẽ áp dụng cho các hàm kích hoạt tổng quát hơn ReLU. Lưu ý rằng chúng ta cũng có thể theo chiến lược chứng minh của Pensia et al. (2020) để chỉ ra sự tồn tại của vé số may mắn mạnh trong các mạng ER. Sự khác biệt chính giữa các chứng minh của Burkholz (2022b) và Pensia et al. (2020) là cách tập cơ sở tổng tập con được tạo ra để xấp xỉ một tham số đích. Pensia et al. (2020) sử dụng hai tầng cho mỗi tầng trong đích và tạo một tập cơ sở để xấp xỉ mỗi trọng số đích trong khi Burkholz (2022b) đi xa hơn một bước và tạo nhiều xấp xỉ tổng tập con của mỗi trọng số đích để tránh việc xây dựng hai tầng. Trong cả hai trường hợp này, xấp xỉ tổng tập con cơ bản có thể được sửa đổi như được hiển thị ở trên cho các mạng ER và cùng chiến lược chứng minh như (Burkholz, 2022b) hoặc (Pensia et al., 2020) có thể được theo dõi. Tương tự, chúng ta cũng có thể mở rộng các chứng minh của chúng tôi sang các kiến trúc tích chập và dư (Burkholz, 2022a).

### A.3. Biểu diễn Mạng Đích Tầng Ẩn Đơn với mạng ER hai tầng

**Định lý A.3** (Xây dựng Đích Tầng Ẩn Đơn). Giả sử rằng một mạng đích kết nối đầy đủ một tầng ẩn fT(x) = W^(2)_T φ(W^(1)_T x + b^(1)_T) + b^(2)_T, một xác suất thất bại cho phép δ ∈ (0,1), mật độ nguồn p̄ và một mạng nguồn ER 2 tầng fS ∈ ER(p̄) với độ rộng nS,0 = q0d, nS,1 = q1nT,1, nS,2 = q2nT,2 được cho. Nếu

q0 ≥ 1/log(1/(1-p1)) log(2mT,1q1/δ),
q1 ≥ 1/log(1/(1-p2)) log(2mT,2/δ) và q2 = 1

thì với xác suất 1-δ, mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W·SP) = fT.

**Chứng minh của Định lý 2.1**: Một mạng hai tầng ẩn có thể xấp xỉ một mạng đích một tầng ẩn như được giải thích trong Phần 2.1. (q0, q1, q2) là các hệ số quá tham số hóa trong mỗi tầng trong mạng nguồn đảm bảo rằng chúng ta có thể tìm thấy các liên kết mà chúng ta cần trong mạng ER. Tại sao chúng ta cần bất kỳ hình thức quá tham số hóa nào? Khác với việc xây dựng SLT, chúng ta không cần sử dụng nhiều tham số để xấp xỉ một tham số đơn và do đó không sử dụng bất kỳ xấp xỉ tổng tập con nào. Chúng ta chọn các trọng số trong mạng ER sao cho chúng chính xác là các trọng số tương ứng của mạng đích. Tuy nhiên, chúng ta vẫn cần chứng minh rằng chúng ta có thể tìm thấy tất cả các mục khác không yêu cầu trong mặt nạ của chúng ta. Để tăng xác suất rằng một liên kết đích tồn tại, chúng ta cũng tạo nhiều bản sao của các nơ-ron đầu vào. Như trong việc xây dựng SLT, chúng ta cắt tỉa các nơ-ron trong tầng đầu tiên thành các nơ-ron đơn biến và chọn độ lệch đủ lớn để ReLU hoạt động về cơ bản như một hàm đồng nhất. p0 > 0 do đó có thể tùy ý, miễn là luồng được bảo tồn. Lưu ý rằng q2 = 1, vì các nơ-ron đầu ra cho nguồn và đích phải giống hệt nhau nT,2 = nS,2. Tầng cuối cùng (tầng đầu ra) trong đích chứa nT,2 nơ-ron và tầng áp chót nT,1. Trong mạng nguồn, chúng ta tạo q1 bản sao của mỗi nơ-ron trong tầng thứ hai của mạng đích sao cho nS,1 = q1 × nT,1. Mục tiêu của chúng ta là ràng buộc độ rộng của Tầng 1 trong mạng ER sao cho có ít nhất một cạnh khác không trong mạng ER cho mỗi trọng số đích khác không. Để ràng buộc dưới q1, mỗi trọng số khác không w^(2)_T,ij phải có ít nhất một trọng số khác không (cạnh) trong mạng nguồn với xác suất đủ cao, tức là mỗi nơ-ron trong tầng đầu ra nS,2 phải có một cạnh khác không đến mỗi khối trong tầng trước nS,1 như được giải thích trong Hình 4. Xác suất rằng ít nhất một cạnh như vậy tồn tại cho mỗi nơ-ron đầu ra được cho bởi (1-(1-p2)^q1)^mT,2.

Tương tự, chúng ta có thể tính toán xác suất rằng mỗi nơ-ron trong tầng thứ hai của nguồn nS,1 có ít nhất một cạnh khác không đến mỗi khối đơn biến trong tầng đầu tiên như (1-(1-p1)^q0)^(mT,1×q1). Vì việc xây dựng mỗi tầng độc lập với tầng khác, các xác suất trên có thể được nhân để thu được xác suất rằng chúng ta có thể biểu diễn toàn bộ mạng đích như

∏²ₗ₌₀(1-(1-pl)^(ql-1))^(mT,l×ql) ≥ 1-δ

Một cách để thỏa mãn bất đẳng thức trên là chia lỗi giữa hai số hạng tích,

(1-(1-p1)^q0)^(mT,1q1) ≥ (1-δ)^(1/2) và (1-(1-p2)^q1)^(mT,2q2) ≥ (1-δ)^(1/2)

Cả hai phương trình trên đều được thỏa mãn với 1-(1-p2)^q1 ≥ (1-δ)^(1/(2mT,2q2)) và 1-(1-p1)^q0 ≥ (1-δ)^(1/(2mT,1q1)). Chúng ta bây giờ có thể giải cho qi, i ∈ {0,1}

q0 ≥ 1/log(1/(1-p1)) log(2mT,1q1/δ) và
q1 ≥ 1/log(1/(1-p2)) log(2mT,2/δ), vì q2 = 1

Sau khi xác định một liên kết đại diện trong mạng nguồn ER cho mỗi trọng số đích, chúng ta tiếp theo định nghĩa các trọng số và độ lệch cho mạng nguồn ER. Mỗi liên kết đại diện trong mạng nguồn ER được gán trọng số của đích tương ứng của nó. Đối với tầng đầu tiên trong mạng nguồn, là một xây dựng đơn biến của đầu vào, các trọng số được định nghĩa là w^(0)_S,ij = 1 và độ lệch đủ lớn để tất cả các đầu vào có liên quan đi qua hàm kích hoạt ReLU như thể nó là đồng nhất:

w^(0)_S,ij = 1 ∀j ∈ {1,2,...,d} và i ∈ {1,2,...,nS,0},
b^(0)_S,i = -a1 nếu a1 ≤ 0
       = 0 nếu a1 > 0 cho mỗi i ∈ {1,2,...,nS,0}.

Nhớ rằng a1 được định nghĩa là cận dưới của mỗi thành phần đầu vào x. Chúng ta bù đắp cho độ lệch bổ sung này trong tầng cuối cùng. Bây giờ đối với tầng thứ hai, mỗi trọng số w^(1)_T,ij trong mạng đích được gán cho một trong các mục mặt nạ khác không trong mạng nguồn ER dẫn đến khối đầu vào tương ứng j và khối đầu ra i. Các trọng số thêm còn lại trong nguồn được đặt thành không.

w^(1)_S,i'j' = w^(1)_T,ij, i' ∈ {q1i, q1i+1,...,q1i+q1} và j' ∈ {q0j, q0j+1,...,q0j+q0}

cho một cặp i', j'. Các kết nối còn lại giữa i' và khối j có thể được cắt tỉa đi, tức là được che mặt nạ hoặc đặt thành không. Độ lệch của tầng thứ hai có thể được chọn để nó bù đắp cho độ lệch thêm được thêm vào trong việc xây dựng đơn biến của tầng đầu tiên:

∀i' ∈ {1,...,nS,1} b^(1)_S,i' = b^(1)_T,i - w^(1)_T,ij b^(0)_S,j'.

### A.4. Biểu diễn mạng đích có độ sâu L với mạng ER

Mở rộng hiểu biết của chúng tôi từ việc xây dựng 2 tầng của mạng nguồn trong phần trước, chúng tôi cung cấp một kết quả chung cho một mạng đích fT có độ sâu L và các mạng nguồn ER với các tỷ lệ thưa thớt theo tầng khác nhau pl. Trong khi chúng ta có thể xấp xỉ mỗi tầng đích riêng biệt với hai tầng nguồn ER, thay vào đó chúng tôi trình bày một xây dựng chỉ yêu cầu một tầng bổ sung để Ls = L + 1. Điều này phù hợp với phương pháp được sử dụng bởi Burkholz (2022b;a) cho SLT. Nhưng chúng ta phải giải quyết hai thách thức thêm. (a) Chúng ta cần đảm bảo rằng một số đủ các nơ-ron được kết nối với tầng trước với các cạnh khác không. (b) Chúng ta phải chỉ ra rằng số lượng khớp tiềm năng yêu cầu cho các nơ-ron đích ql không bùng nổ cho số lượng tầng tăng. Trên thực tế, nó chỉ tăng logarit trong các biến có liên quan.

**Định lý A.4** (Mạng ER có thể biểu diễn mạng đích L-tầng). Cho một mạng đích kết nối đầy đủ fT có độ sâu L, δ ∈ (0,1), mật độ nguồn p̄ và một mạng nguồn ER L+1 tầng fS ∈ ER(p̄) với độ rộng nS,0 = q0d và nS,l = qlnT,l, l ∈ {1,2,...,L}, trong đó

ql ≥ 1/log(1/(1-pl+1)) log(LmT,l+1ql+1/δ)

cho l ∈ {0,1,...,L-1} và qL = 1, thì với xác suất 1-δ mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W·SP) = fT.

**Chứng minh**: Một lần nữa chúng tôi theo cùng thủ tục tìm độ rộng nhỏ nhất cho mỗi tầng trong mạng nguồn sao cho có ít nhất một cạnh kết nối giữa một bản sao nơ-ron đích và một trong các bản sao trong tầng trước. Lặp lại lập luận này cho mỗi tầng bắt đầu từ tầng đầu ra theo thứ tự ngược lại cho chúng ta cận dưới về hệ số ql trong mỗi tầng l ∈ {0,1,...,L}. Chúng ta chọn các trọng số của mạng ER thưa thớt sao cho cho mỗi tham số đích có ít nhất một tham số khác không (không bị che mặt nạ) trong nguồn chính xác học giá trị yêu cầu.

Chúng ta bây giờ xây dựng một mạng nguồn fS(x) chứa một mạng con ngẫu nhiên sao chép fT(x) với xác suất 1-δ. Như được giải thích trong Phần A.3, chúng ta đầu tiên xây dựng một tầng đơn biến (với chỉ số l = 0) trong mạng nguồn giả định bảo tồn luồng.

Tiếp theo, chúng ta tính toán hệ số quá tham số hóa yêu cầu cho mỗi tầng trong mạng nguồn sử dụng cùng lập luận như A.3 bắt đầu từ tầng cuối cùng và làm việc ngược lại. Tầng đầu ra cuối cùng có cùng số nơ-ron trong cả nguồn và đích, nS,L = nT,L. Do đó, hệ số quá tham số hóa độ rộng yêu cầu là qL = 1. Trong mỗi tầng trung gian, chúng ta tạo các khối nơ-ron bao gồm ql bản sao của cùng nơ-ron đích. ql nên lớn bao nhiêu? Trong tầng áp chót, xác suất rằng mỗi nơ-ron trong tầng đầu ra có ít nhất một cạnh đến mỗi trong số qL-1 khối trong Tầng L-1 là (1-(1-pL)^(qL-1))^(mT,L×qL). Chúng ta có thể tính toán tương tự xác suất này cho mỗi tầng tất cả đường đến đầu vào trong mạng nguồn đảm bảo rằng có ít nhất một cạnh giữa một nơ-ron trong mỗi tầng và mỗi trong số ql-1 khối trong tầng trước. Xác suất rằng Tầng l có thể được xây dựng do đó là (1-(1-pl)^(ql-1))^(mT,l×ql). Những sự kiện này độc lập và nên giữ đồng thời với xác suất 1-δ. Bất đẳng thức sau công thức hóa lập luận của chúng ta

∏ᴸₗ₌₁(1-(1-pl)^(ql-1))^(mt,l×ql) ≥ 1-δ

Một cách để thỏa mãn phương trình trên sẽ là đảm bảo rằng

(1-(1-pl)^(ql-1))^(mT,l×ql) ≥ (1-δ)^(1/L)

cho mỗi tầng và do đó

(1-(1-pl)^(ql-1)) ≥ (1-δ)^(1/(mT,l×ql×L))

Bất đẳng thức này được thỏa mãn nếu

1-(1-pl)^(ql-1) ≥ 1-δ/(mT,l×ql×L)

Giải cho ql-1 dẫn đến

ql-1 ≥ log(δ/(mT,l×ql×L))/log(1-pl) = 1/log(1/(1-pl)) log(LmT,l×ql/δ).

Chúng ta có thể tính toán quá tham số hóa độ rộng yêu cầu cho mỗi tầng bắt đầu từ tầng cuối cùng, nơi chúng ta biết qL = 1. Lưu ý rằng ql phụ thuộc vào logarit của ql+1 của tầng tiếp theo, đảm bảo rằng ql không bùng nổ khi độ sâu tăng.

Sau khi đảm bảo rằng các cạnh yêu cầu tồn tại trong mạng ER để biểu diễn mỗi trọng số đích, chúng ta vẫn phải rút ra các lựa chọn tham số cụ thể. Sau đó nó theo rằng những lựa chọn trọng số này có thể được chọn, giả định chúng là kết quả của việc huấn luyện mạng. Tương tự như trường hợp một tầng ẩn, mỗi liên kết đại diện trong mạng nguồn ER được gán trọng số của đích tương ứng của nó và các trọng số trong tầng đơn biến đầu tiên được đặt thành 1. Các độ lệch trong tầng đơn biến được chọn để tất cả các đầu vào đi qua ReLU kích hoạt. Các độ lệch trong tầng tiếp theo bù đắp cho các độ lệch bổ sung trong tầng đầu tiên.

w^(0)_S,ij = 1 ∀j ∈ {1,2,...,d} và i ∈ {1,2,...,nS,0},
b^(0)_S,i = -a1 nếu a1 ≤ 0,
        = 0 nếu a1 > 0 cho mỗi i ∈ {1,2,...,nS,0}.

Đối với các tầng tiếp theo trong mạng nguồn l ∈ {1,2,...,L} các trọng số là w^(l)_S,i'j' = w^(l)_T,ij, trong đó j' được chọn ngẫu nhiên trong tất cả các kết nối không bị che mặt nạ của i' đến khối j và j' ∈ {ql-1j, ql-1j+1,...,ql-1j+ql-1} và i' ∈ {qli, qli+1,...,qli+ql}. Các kết nối còn lại giữa khối j và i' có thể được cắt tỉa đi hoặc các tham số trọng số được đặt thành không. Các độ lệch được đặt thành độ lệch đích tương ứng cho các tầng l ∈ {2,...,L}

∀i' ∈ {qli, qli+1,...,qli+ql} b^(l)_S,i' = b^(l)_T,i

nhưng tầng thứ hai l = 1 có một số hạng bổ sung để bù đắp cho độ lệch trong tầng đầu tiên (đơn biến):

∀i' ∈ {qli, qli+1,...,qli+ql} b^(1)_S,i' = b^(1)_T,i - w^(1)_T,ij b^(0)_S,j.

### A.5. Mạng ER cho Tầng Tích chập

Chúng ta cũng có thể mở rộng phân tích của chúng ta sang các mạng ER với các tầng tích chập, nơi số lượng kênh cần được quá tham số hóa bởi một hệ số 1/log(1/sparsity).

**Định lý A.5** (Mạng ER có thể biểu diễn mạng tích chập L tầng). Cho một mạng đích fT có độ sâu L với các tầng tích chập h^(l)_T,i = Σ^(cl-1)_(j=1) W^(l)_T,ij * x^(l-1)_ij + b^(l)_T,i, WT ∈ R^(cl×cl-1×kl), δ ∈ (0,1), một mật độ nguồn p và một mạng nguồn ER L+1 tầng fS ∈ ER(p) với các tầng tích chập h^(l)_S,i = Σ^(cl-1)_(j=1) W^(l)_S,ij * x^(l-1)_ij + b^(l)_S,i, WS ∈ R^(qlcl×cl-1×kl) trong đó

ql ≥ 1/log(1/(1-pl+1)) log(LmT,l+1ql+1/δ)

cho l ∈ {0,1,...,L-1} và qL = 1, thì với xác suất 1-δ mạng nguồn ngẫu nhiên fS chứa một mạng con SP sao cho fS(x, W·SP) = fT.

**Chứng minh**: Tương tự như trường hợp mạng nguồn ER kết nối đầy đủ, chúng ta tạo ql bản sao của mỗi kênh đầu ra của đích cl trong nguồn. Mỗi bản sao kênh trong nguồn được gán trọng số của kênh đích tương ứng.

Lưu ý rằng bất kỳ mục tensor nào dẫn đến cùng khối đều đủ, vì tích chập là một hoạt động song tuyến tính để Σ_(i'∈Ii) Wi'j * xi = Σ_(i'∈Ii) Wi'j * xi. Cụ thể, Σ_(i'∈Ii) W^(l)_S,i'j có thể biểu diễn một phần tử đích w^(l)_T,ij nếu ít nhất một trọng số w^(l)_S,i'j khác không.

Tính tuyến tính của tích chập cho phép chúng ta xây dựng một bộ lọc đích bằng cách kết hợp các phần tử được phân tán giữa các kênh đầu vào khác nhau trong mạng nguồn ER như được hiển thị trong Hình 5. Sử dụng cùng lập luận như trường hợp tầng kết nối đầy đủ, chúng ta ràng buộc xác suất rằng ít nhất một trong số ql kênh của mỗi phần tử bộ lọc trong tensor trọng số tích chập có một mục không bị che mặt nạ đến một kênh trong tầng tiếp theo. Như đối với các mạng kết nối đầy đủ, chúng ta có thể tạo các khối kênh tương ứng với các bản sao của cùng kênh đích. Tầng đầu tiên có thể được cắt tỉa xuống thành các bộ lọc tích chập đơn biến. Xác suất rằng mỗi tầng có thể được tái tạo trong mạng tích chập có thể được ràng buộc như:

(1-(1-pl)^(ql-1))^(mT,l×ql) ≥ (1-δ)^(1/L)

Lưu ý rằng đối với trọng số tích chập, mT,l là số tham số khác không trong W^(l)_T ∈ R^(cl×cl-1×kl). Quá tham số hóa độ rộng sau của các kênh đầu ra trong một mạng tích chập

ql-1 ≥ log(δ/(mT,l×ql×L))/log(1-pl) = 1/log(1/(1-pl)) log(LmT,l×ql/δ)

cho phép một mạng ER biểu diễn đích với xác suất 1-δ.

Các trọng số trong mạng tích chập bây giờ có thể được chọn như:

w^(l)_S,i'j'k = w^(l)_T,ijk, cho mỗi i' ∈ {qli, qli+1,...,qli+ql},

trong đó j' ∈ {ql-1j, ql-1j+1,...,ql-1j+ql-1} được chọn ngẫu nhiên trong tất cả các kết nối không bị che mặt nạ của i' đến khối j và các kết nối còn lại được cắt tỉa đi hoặc đặt thành không. Các độ lệch được đặt như trong chứng minh của Định lý 2.2.

[Hình 5. Xây dựng đích tích chập trong mạng ER: Đối với mỗi kênh đầu ra cT,l trong tensor trọng số tích chập đích W^(l)_T, chúng ta tạo ql bản sao trong tensor trọng số nguồn W^(l)_S như được hiển thị bên trái (a). Quá tham số hóa độ rộng được làm rõ thêm trong (b) nơi mỗi phần tử bộ lọc của một bộ lọc đầu ra đích có ql bản sao độc lập trong nguồn, ít nhất một trong số đó khác không (không bị che mặt nạ). Các hình vuông có màu trong (b) hiển thị các tham số khác không trong mạng nguồn ER.]

### A.6. Cận Dưới về Quá tham số hóa của mạng ER

Phân tích lý thuyết của chúng tôi gợi ý rằng các mạng ER yêu cầu quá tham số hóa độ rộng bởi một hệ số log(1/sparsity) để xấp xỉ một đích tùy ý. Chúng tôi cũng chỉ ra rằng chúng ta không thể làm tốt hơn đáng kể so với một độ rộng tỷ lệ với log(1/sparsity).

[Hình 6. Cận dưới của độ rộng của một mạng nguồn ER được hiển thị bên phải yêu cầu để biểu diễn mạng đích bên trái. Các cạnh liền trong nguồn bên phải là các cạnh khác không (không bị che mặt nạ) trong khi các đường chấm bị che mặt nạ đi trong một mạng nguồn ER.]

**Định lý A.6** (Cận dưới về quá tham số hóa trong mạng ER). Tồn tại các mạng đích đơn biến fT(x) = φ(w^T_T x + bT) không thể được biểu diễn bởi một mạng nguồn ER 1-tầng ẩn ngẫu nhiên fS ∈ ER(p) với xác suất ít nhất 1-δ, nếu độ rộng của nó nS,1 < 1/log(1/(1-p)) log(1/(1-(1-δ)^(1/d))).

**Chứng minh**: Ý tưởng chính là tìm độ rộng tối thiểu của một mạng một tầng ẩn ER(p) có thể xấp xỉ một đầu ra đơn đích fT(x) = φ(w^T_T x + bT). Mức tối thiểu này sẽ đạt được khi mỗi trọng số đích trong wT được xấp xỉ bởi chính xác một đường dẫn trong mạng ER từ đầu vào đến đầu ra (qua tầng ẩn). Chúng ta rút ra xác suất rằng đối với mỗi trọng số trong đích, có ít nhất một đường dẫn không bị che mặt nạ trong nguồn ER có thể biểu diễn trọng số này như được hiển thị trong Hình 6. Ràng buộc xác suất này sẽ cho chúng ta một cận dưới về độ rộng tối thiểu yêu cầu trong mạng ER để có thể biểu diễn mạng đích. Có nS,1 đường dẫn từ một nơ-ron đầu vào đến một nơ-ron đầu ra trong mạng nguồn và xác suất rằng mỗi đường dẫn này tồn tại là p², độc lập cho mỗi đường dẫn, vì cả liên kết đầu vào và đầu ra trong đường dẫn phải khác không và mỗi cạnh tồn tại độc lập. Bắt đầu từ nơ-ron đầu vào đầu tiên, xác suất rằng có ít nhất một đường dẫn từ đầu vào xi đến đầu ra là

1-(1-p²)^(nS,1).

Các đường dẫn tồn tại độc lập với nhau nếu chúng bắt đầu ở các nơ-ron đầu vào khác nhau. Do đó, xác suất rằng chúng ta có thể biểu diễn một nơ-ron đích tùy ý với d nơ-ron đầu vào là

[1-(1-p²)^(nS,1)]^d.

Để tìm độ rộng tối thiểu yêu cầu, chúng ta ràng buộc dưới xác suất này như:

[1-(1-p²)^(nS,1)]^d ≥ 1-δ

Giải bất đẳng thức này cho nS,1 chứng minh tuyên bố, vì chúng ta sẽ cần

nS,1 ≥ 1/log(1/(1-p²)) log(1/(1-(1-δ)^(1/d)))
     ≥ 1/log(1/(1-p)) log(1/(1-(1-δ)^(1/d))).

### A.7. Thiết lập Thí nghiệm

Chúng tôi tiến hành các thí nghiệm của mình với hai tập dữ liệu được xây dựng cho các nhiệm vụ phân loại hình ảnh: CIFAR10 và CIFAR100 (Krizhevsky et al., 2009). Các thí nghiệm trên Tiny Imagenet (Russakovsky et al., 2015b) được báo cáo trong Phụ lục A.13. Chúng tôi huấn luyện hai kiến trúc phổ biến, VGG16 (Simonyan & Zisserman, 2015) và ResNet18 (He et al., 2016), để phân loại hình ảnh trong tập dữ liệu CIFAR10. Trên tập dữ liệu CIFAR100 lớn hơn, chúng tôi sử dụng VGG19 và ResNet50. Mã của chúng tôi xây dựng trên công trình của (Liu et al., 2021; Tanaka et al., 2020; Kusupati et al., 2020) và có sẵn tại https://github.com/RelationalML/sparse_to_sparse. Tất cả các thí nghiệm của chúng tôi được chạy với 4 GPU Nvidia A100.

**Cắt tỉa Ngẫu nhiên**: Mỗi mô hình được huấn luyện sử dụng Stochastic Gradient Descent (SGD) với tỷ lệ học 0.1 và động lượng 0.9 với phân rã trọng số 0.0005 và kích thước batch 128. Chúng tôi sử dụng cùng siêu tham số như (Ma et al., 2021) và huấn luyện mỗi mô hình trong 160 epoch. Chúng tôi lặp lại tất cả các thí nghiệm của mình trên ba lần chạy và báo cáo trung bình và khoảng tin cậy tiêu chuẩn 0.95, có thể được tìm thấy trong phụ lục do hạn chế về không gian.

**Vé số May mắn Mạnh**: Đối với các thí nghiệm về vé số may mắn mạnh sử dụng edge popup, chúng tôi sử dụng phiên bản lặp lại của edge popup như được mô tả trong (Fischer & Burkholz, 2022). Chúng tôi khởi tạo một mạng thưa thớt và ủ độ thưa thớt lặp lại trong khi giữ mặt nạ cố định. Đối với ResNet18, chúng tôi sử dụng tỷ lệ học 0.1 và ủ trong 5 mức và 100 epoch cho mỗi mức. Kích thước batch là 128 và chúng tôi sử dụng SGD với động lượng 0.9 và phân rã trọng số 0.0005. Chúng tôi báo cáo hiệu suất sau một lần chạy cho mỗi thí nghiệm này do tính toán hạn chế.

**Huấn luyện Thưa thớt Động**: Trong các thí nghiệm DST, chúng tôi sử dụng cùng thiết lập như cắt tỉa ngẫu nhiên, và sửa đổi mặt nạ mỗi 100 vòng lặp. Đối với huấn luyện thưa thớt đến thưa thớt với DST, chúng tôi sử dụng độ lớn trọng số như điểm quan trọng cho cắt tỉa (với tỷ lệ cắt tỉa 0.5) và gradient cho tăng trưởng.

**Huấn luyện Thưa thớt đến Thưa thớt**: Đối với IMP cơ sở, chúng tôi cắt tỉa mạng bằng cách loại bỏ 20% tham số trong mỗi chu kỳ và huấn luyện trong 150 epoch trong mỗi chu kỳ với tỷ lệ học 0.1 và lịch trình LR cosin ủ tỷ lệ học xuống 0.01. Chúng tôi theo cùng thủ tục trong khi huấn luyện một mạng ER.

Đối với thưa thớt hóa liên tục với STR, chúng tôi sử dụng mã được cung cấp bởi các tác giả (Kusupati et al., 2020) và cùng siêu tham số cho cả ResNet18 và ResNet50 với giá trị sInit = -200 và sửa đổi tham số phân rã trọng số theo độ thưa thớt đích. 0.0005 cho độ thưa thớt đích 0.96 và 0.001 cho độ thưa thớt đích 0.995.

### A.8. Thí nghiệm bổ sung trên CIFAR10/100 cho Hiệu suất của Cắt tỉa Ngẫu nhiên

Cùng với VGG, chúng tôi báo cáo kết quả cho các tỷ lệ thưa thớt theo tầng khác nhau cho ResNet. Chúng tôi sử dụng ResNet 18 cho CIFAR10 và ResNet 50 cho CIFAR100.

[Bảng 8. Mạng ER với độ thưa thớt theo tầng khác nhau trên CIFAR10 với ResNet18.]

[Bảng 9. Mạng ER với độ thưa thớt theo tầng khác nhau trên CIFAR100 với ResNet50.]

[Bảng 10. Mạng ER với độ thưa thớt theo tầng khác nhau trên CIFAR10 với VGG16. Chúng tôi so sánh các tỷ lệ thưa thớt theo tầng cân bằng và kim tự tháp với đường cơ sở đồng nhất, ERK và các mạng ER với tỷ lệ thưa thớt theo tầng thu được bằng IMP, Synflow Lặp lại và Snip.]

[Bảng 11. Mạng ER với độ thưa thớt theo tầng khác nhau trên CIFAR100 với VGG19. Chúng tôi so sánh các tỷ lệ thưa thớt theo tầng cân bằng và kim tự tháp với đường cơ sở đồng nhất, ERK và các mạng ER với tỷ lệ thưa thớt theo tầng thu được bằng IMP, Synflow Lặp lại và Snip.]

### A.9. Đường cơ sở Huấn luyện Dày đặc trên CIFAR10

Để tham khảo, chúng tôi cung cấp các đường cơ sở của STR và IMP trên CIFAR10 với ResNet 18 bắt đầu từ một mạng dày đặc để đạt độ thưa thớt đích trong Bảng 13, 12.

[Bảng 12. Đường cơ sở IMP trên CIFAR10 cho ResNet18.]

[Bảng 13. Đường cơ sở STR trên CIFAR10 cho ResNet18.]

### A.10. Huấn luyện Thưa thớt đến Thưa thớt

Chúng tôi cung cấp các thí nghiệm bổ sung cho Huấn luyện Thưa thớt đến Thưa thớt với ResNet50 trên CIFAR100 trong Bảng 16. Chúng tôi cũng báo cáo khoảng tin cậy cho kết quả trong bài báo chính trong Bảng 3, 4.

[Bảng 14. Huấn luyện thưa thớt đến thưa thớt với Tái tham số hóa Ngưỡng Mềm trong mạng ER: Kết quả trên ResNet18 được huấn luyện trên CIFAR10.]

[Bảng 15. Huấn luyện thưa thớt đến thưa thớt với Cắt tỉa Độ lớn Lặp lại trong mạng ER: Kết quả trên ResNet18 được huấn luyện trên CIFAR10.]

[Bảng 16. Huấn luyện thưa thớt đến thưa thớt với Tái tham số hóa Ngưỡng Mềm trong mạng ER: Kết quả trên ResNet50 được huấn luyện trên CIFAR100.]

### A.11. Thí nghiệm bổ sung cho Vé số May mắn Mạnh trong mạng ER

Để chỉ ra thực nghiệm rằng các mạng ER có thể chứa SLT, chúng tôi sử dụng edge popup (Ramanujan et al., 2020) để tìm kiếm SLT trong ER ResNet18. Chúng tôi dần dần ủ độ thưa thớt của mạng ER với 5 mức như được đề xuất bởi (Fischer & Burkholz, 2022). Kết quả bắt đầu từ các mạng ER với độ thưa thớt ban đầu khác nhau được trình bày trong Bảng 18. Khoảng tin cậy của Bảng 5 được báo cáo trong Bảng 17. Để tham khảo, chúng tôi cũng báo cáo kết quả cơ sở cho các mạng dày đặc trong Bảng 19.

[Bảng 17. Mạng ER cho Vé số May mắn Mạnh: Kết quả trung bình và khoảng tin cậy tiêu chuẩn 0.95 cho việc huấn luyện mạng ER ResNet18 với edge popup (Ramanujan et al., 2020) trên CIFAR10 trên ba lần chạy. Mạng ER được ủ dần để đạt được SLT có độ thưa thớt cuối cùng (ban đầu → cuối cùng). Lưu ý rằng cột đầu tiên phục vụ như một cơ sở tức là bắt đầu từ một mạng dày đặc.]

[Bảng 18. Mạng ER cho SLT và độ thưa thớt cuối cùng khác nhau: Kết quả trung bình về việc huấn luyện mạng ER ResNet18 với edge popup (Ramanujan et al., 2020) trên CIFAR10. Mạng ER được khởi tạo với độ thưa thớt ban đầu đồng nhất, được ủ dần để đạt được SLT có độ thưa thớt cuối cùng (ban đầu → cuối cùng). Kết quả cơ sở cho các mạng ban đầu dày đặc được báo cáo trong Bảng 19.]

[Bảng 19. Cơ sở cho edge popup với ResNet18 trên CIFAR10: Kết quả để tìm SLT sử dụng edge popup bắt đầu từ một mạng dày đặc được hiển thị. Kết quả ER của chúng tôi bắt đầu từ một mạng thưa thớt có thể so sánh với các kết quả cơ sở này xác nhận hiệu quả của các mạng ER.]

Các thí nghiệm bổ sung cho ER VGG16 trên CIFAR10 được hiển thị trong Bảng 20 với kết quả cơ sở trong Bảng 21 cho các mạng ER với độ thưa thớt đồng nhất.

[Bảng 20. Mạng ER cho Vé số May mắn Mạnh: SLT trong các mạng ER VGG16 trên CIFAR10. Mạng ER được khởi tạo với độ thưa thớt ban đầu đồng nhất và ủ dần để đạt được SLT có độ thưa thớt cuối cùng (ban đầu → cuối cùng).]

[Bảng 21. Cơ sở cho edge popup với VGG16 trên CIFAR10: Kết quả cơ sở của Edge Popup để thu được SLT trên CIFAR10 với VGG16.]

#### A.11.1. SLT trong Mạng ER cho ResNet110 trên CIFAR100

Chúng tôi tìm SLT trong các mạng ER sử dụng thuật toán Edge Popup cho một mô hình Resnet110 lớn hơn cũng như được báo cáo trong Bảng 22, cho các mạng ER bắt đầu với độ thưa thớt đồng nhất.

[Bảng 22. Kết quả edge popup (SLT) trên các mạng ER với độ thưa thớt đồng nhất của Resnet110 trên CIFAR100. Kết quả được báo cáo cho một lần chạy do tính toán hạn chế.]

### A.12. Khả năng Mở rộng của Cắt tỉa Ngẫu nhiên với Resnet110 trên CIFAR100

Để giới thiệu khả năng mở rộng của các thuật toán được đề xuất, chúng tôi báo cáo thêm các thí nghiệm cho một mô hình lớn hơn, tức là Resnet110.

#### A.12.1. Mạng ER cho Cắt tỉa Ngẫu nhiên

Chúng tôi báo cáo kết quả cho các độ thưa thớt theo tầng khác nhau trong Bảng 23. Để tham khảo, chúng tôi cũng báo cáo kết quả về cắt tỉa với thuật toán cơ sở Cắt tỉa Độ lớn Lặp lại trong Bảng 24. Chúng tôi cũng tiến hành các thí nghiệm với thuật toán Synflow Lặp lại Tanaka et al. (2020) để cắt tỉa Resnet110 nhưng thuật toán thất bại đối với mô hình lớn như vậy.

[Bảng 23. Kết quả cho cắt tỉa ngẫu nhiên trên CIFAR100 với ResNet110. Kết quả là trung bình và độ lệch chuẩn được báo cáo trên ba lần chạy.]

[Bảng 24. Kết quả trên CIFAR100 với Resnet110 cắt tỉa với thuật toán cắt tỉa độ lớn lặp lại (IMP) để tham khảo. Chỉ một lần chạy IMP được thực hiện cho mỗi độ thưa thớt này.]

### A.13. Thí nghiệm về Cắt tỉa Ngẫu nhiên với Tiny Imagenet

Chúng tôi cũng báo cáo các thí nghiệm với các tỷ lệ thưa thớt theo tầng khác nhau trong các mạng ER cho tập dữ liệu Tiny Imagenet. Chúng tôi sử dụng VGG19 và ResNet20 và chỉ ra rằng các phương pháp thưa thớt theo tầng được đề xuất của chúng tôi cho các mạng ER có tính cạnh tranh cho tập dữ liệu này. Lưu ý rằng chúng tôi sử dụng tập xác thực được cung cấp bởi người tạo Tiny Imagenet (Russakovsky et al., 2015b) như một tập kiểm tra để đo hiệu suất tổng quát hóa của các mô hình được huấn luyện của chúng tôi.

Xem Bảng 25 và 26.

[Bảng 25. Kết quả cho các mạng ER trên Tiny Imagenet với VGG19]

[Bảng 26. Kết quả cho các mạng ER trên Tiny Imagenet với ResNet20]

### A.14. Huấn luyện Thưa thớt Động trên mạng ER

[Bảng 27. Mạng ER được nối lại với DST: Một mạng VGG16 ER(p) với độ thưa thớt = 1-p được khởi tạo và mặt nạ được sửa đổi bằng cách nối lại các cạnh với RiGL trên CIFAR10.]

Ngoài các thí nghiệm nối lại được hiển thị trong Bảng 2, chúng tôi sử dụng Huấn luyện Thưa thớt Động để cắt tỉa một mạng ER đã thưa thớt đến độ thưa thớt cao hơn và xem liệu điều này có thể đạt được cùng hiệu suất như thực hiện DST bắt đầu từ một mạng dày đặc hơn. Các thí nghiệm tương tự đã được hiển thị bởi (Liu et al., 2021). Tuy nhiên, chúng tôi báo cáo kết quả trên các mạng ER bắt đầu ở độ thưa thớt cao hơn nhiều. Kết quả của chúng tôi được hiển thị trong Bảng 28 có thể phù hợp với hiệu suất của (Liu et al., 2021) trong khi hiệu quả hơn vì chúng tôi bắt đầu ở độ thưa thớt cao hơn.

[Bảng 28. Huấn luyện thưa thớt đến thưa thớt với DST: Độ chính xác kiểm tra cuối cùng cho VGG16 trên CIFAR10 được báo cáo nơi mô hình được khởi tạo với một mạng ER có độ thưa thớt ban đầu nào đó và được cắt tỉa thêm đến độ thưa thớt cuối cùng (ban đầu → cuối cùng) trong khi sửa đổi mặt nạ sử dụng thuật toán RiGL (Evci et al., 2020).]

Đáng chú ý, chúng tôi quan sát rằng cũng có thể bắt đầu ở độ thưa thớt đến 0.95 và vẫn đạt được độ chính xác kiểm tra cạnh tranh, chỉ kém hơn một chút so với việc bắt đầu với độ thưa thớt 0.5.

### A.15. Hình dung độ thưa thớt theo tầng cho mạng ER

Chúng tôi báo cáo các tỷ lệ thưa thớt theo tầng cho các phương pháp được đề xuất được thảo luận trong Phần 3 so với ERK và Đồng nhất trong Bảng 7.

### A.16. Thí nghiệm Bổ sung trên Tập dữ liệu Đa dạng

Để giới thiệu tính linh hoạt của các hiểu biết của chúng tôi, chúng tôi trình bày các thí nghiệm bổ sung trên các tập dữ liệu đa dạng hơn với các miền ứng dụng khác nhau. Điều này xác minh khả năng áp dụng của huấn luyện thưa thớt đến thưa thớt trong bối cảnh rộng.

#### Thí nghiệm ImageNet

Bảng 29) thiết lập rằng huấn luyện thưa thớt đến thưa thớt cũng hoạt động trong bối cảnh dữ liệu quy mô lớn như ImageNet (Russakovsky et al., 2015a), trên đó chúng tôi huấn luyện một ResNet 50 thưa thớt sử dụng Cắt tỉa Độ lớn Lặp lại. Mạng ER thưa thớt được khởi tạo với tỷ lệ thưa thớt theo tầng cân bằng. Chúng tôi thấy rằng bắt đầu từ một mạng ER có 50% độ thưa thớt, IMP vẫn có thể tìm thấy một mạng 80% thưa thớt mà không mất hiệu suất.

[Hình 7. Mật độ theo tầng cho ResNet18 được huấn luyện trên CIFAR10 cho kết quả được báo cáo trong Bảng 8, 14 cho độ thưa thớt đích 0.9 tức là 10% tham số được giữ lại.]

[Bảng 29. Huấn luyện thưa thớt đến thưa thớt với IMP trên ImageNet: Độ chính xác của ResNet 50 được huấn luyện trên ImageNet. Tỷ lệ thưa thớt của mạng ER ban đầu được chọn là cân bằng.]

#### Mạng Tích chập Đồ thị
Chúng tôi chỉ ra rằng cắt tỉa ngẫu nhiên có thể cho phép huấn luyện thưa thớt trong Mạng Tích chập Đồ thị (GCN)(Kipf & Welling) (xem Bảng 30). Chúng tôi thích ứng thiết lập thí nghiệm và siêu tham số được cung cấp bởi https://github.com/meliketoy/graph-cnn.pytorch. Mỗi tầng trong GCN ngẫu nhiên ban đầu có độ thưa thớt đồng nhất. Đối với mỗi chu kỳ huấn luyện trong IMP, chúng tôi tăng độ thưa thớt 10% cho đến khi đạt được độ thưa thớt đích. Việc bắt đầu thưa thớt chỉ ảnh hưởng đến hiệu suất cuối cùng một cách nhẹ.

[Bảng 30. Huấn luyện thưa thớt đến thưa thớt với IMP trên GCN: Chúng tôi huấn luyện một GCN 2 tầng trên một nhiệm vụ phân loại nút trên tập dữ liệu CORA(Sen et al., 2008) trung bình trên 10 lần chạy. GCN 2 tầng dày đặc đạt 82.57(±0.005)% độ chính xác. Tỷ lệ thưa thớt của mạng ER ban đầu là đồng nhất.]

#### Dữ liệu Thuật toán
Chúng tôi kiểm tra lý thuyết của chúng tôi cho MLP trên dữ liệu thuật toán đã được nghiên cứu trong bối cảnh grokking trên Mô hình Đồ chơi được mô tả trong Phần 2 của Liu et al. (2022). Nhiệm vụ chính là học phép cộng thông qua ký hiệu, mà chúng tôi thích ứng thiết lập thí nghiệm gốc (https://github.com/ejmichaud/grokking-squared/blob/main/notebooks/erics-implementation.ipynb) để sử dụng huấn luyện thưa thớt đến thưa thớt (xem Bảng 31). Mỗi tầng của MLP có độ thưa thớt đồng nhất. Chúng tôi tăng độ thưa thớt 10% trong mỗi chu kỳ huấn luyện cho đến khi đạt được độ thưa thớt đích. Chúng tôi quan sát rằng đối với mỗi lần chạy riêng lẻ, huấn luyện thưa thớt đến thưa thớt đạt được cùng hiệu suất như đối tác dày đặc đến thưa thớt của nó. Chúng tôi giả thuyết rằng mô hình đồ chơi được quá tham số hóa nặng nề và do đó cho phép bắt đầu thưa thớt. Tuy nhiên, mỗi lần chạy riêng lẻ không nhất quán và có thể có hiệu suất khác nhau đáng kể có thể do chất lượng của dữ liệu huấn luyện được lấy mẫu ngẫu nhiên.

#### Dữ liệu Bảng
Dữ liệu bảng thường được nghiên cứu trong bối cảnh công bằng (Ding et al., 2021), xem thêm https://github.com/socialfoundations/folktables. Chúng tôi sử dụng một MLP bốn tầng với 256 đơn vị ẩn cho phân loại nhị phân về công bằng (xem Bảng 32). Mỗi tầng của MLP có độ thưa thớt đồng nhất. Chúng tôi huấn luyện mô hình với bộ tối ưu Adam (Kingma & Ba, 2015) và tỷ lệ học 0.01 trong 20 epoch trong mỗi chu kỳ huấn luyện và tăng độ thưa thớt 10% trong mỗi chu kỳ huấn luyện cho đến khi đạt được độ thưa thớt đích.

Chúng tôi thấy rằng việc bắt đầu thưa thớt không ảnh hưởng đến hiệu suất cuối cùng của mô hình.

[Bảng 31. Huấn luyện thưa thớt đến thưa thớt trên dữ liệu thuật toán với IMP. Chúng tôi huấn luyện một mô hình để học phép cộng của hai số một cách biểu tượng như được mô tả trong Liu et al. (2022). Bộ mã hóa và giải mã của mô hình được sử dụng trong Liu et al. (2022) được khởi tạo với tỷ lệ thưa thớt đồng nhất cho huấn luyện thưa thớt đến thưa thớt. Độ chính xác kiểm tra trung bình và khoảng tin cậy 0.95 được báo cáo cho 3 lần chạy độc lập.]

[Bảng 32. Huấn luyện thưa thớt đến thưa thớt trên dữ liệu bảng với IMP. Độ chính xác kiểm tra trung bình và khoảng tin cậy 0.95 được báo cáo cho 3 lần chạy độc lập.]
