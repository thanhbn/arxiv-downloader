# 2311.04251.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/initialization/2311.04251.pdf
# KÃ­ch thÆ°á»›c file: 1792733 bytes

===============================================
Ná»˜I DUNG FILE PDF
===============================================


--- TRANG 1 ---
MixtureGrowth: PhÃ¡t triá»ƒn Máº¡ng Neural báº±ng cÃ¡ch TÃ¡i káº¿t há»£p cÃ¡c Tham sá»‘ ÄÃ£ há»c
Chau Pham1*Piotr Teterwak1*Soren Nelson2*â€ Bryan A. Plummer1
Boston University1Physical Sciences Inc2
{chaupham,piotrt,bplum }@bu.edu snelson@psicorp.com
TÃ³m táº¯t
Háº§u háº¿t cÃ¡c máº¡ng neural sÃ¢u Ä‘Æ°á»£c huáº¥n luyá»‡n dÆ°á»›i cÃ¡c kiáº¿n trÃºc máº¡ng cá»‘ Ä‘á»‹nh vÃ  yÃªu cáº§u huáº¥n luyá»‡n láº¡i khi kiáº¿n trÃºc thay Ä‘á»•i. Náº¿u cáº§n má»Ÿ rá»™ng kÃ­ch thÆ°á»›c cá»§a máº¡ng, cáº§n pháº£i huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u, Ä‘iá»u nÃ y ráº¥t tá»‘n kÃ©m. Äá»ƒ trÃ¡nh Ä‘iá»u nÃ y, ngÆ°á»i ta cÃ³ thá»ƒ phÃ¡t triá»ƒn tá»« má»™t máº¡ng nhá» báº±ng cÃ¡ch thÃªm trá»ng sá»‘ ngáº«u nhiÃªn theo thá»i gian Ä‘á»ƒ dáº§n Ä‘áº¡t Ä‘Æ°á»£c kÃ­ch thÆ°á»›c máº¡ng má»¥c tiÃªu. Tuy nhiÃªn, cÃ¡ch tiáº¿p cáº­n ngÃ¢y thÆ¡ nÃ y khÃ´ng hiá»‡u quáº£ trong thá»±c táº¿ vÃ¬ nÃ³ mang quÃ¡ nhiá»u nhiá»…u vÃ o quÃ¡ trÃ¬nh phÃ¡t triá»ƒn. CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã£ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch táº­n dá»¥ng cÃ¡c trá»ng sá»‘ Ä‘Ã£ há»c vÃ  dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘á»ƒ táº¡o ra trá»ng sá»‘ má»›i thÃ´ng qua viá»‡c thá»±c hiá»‡n má»™t bÆ°á»›c phÃ¢n tÃ­ch tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giá»›i thiá»‡u MixtureGrowth, má»™t cÃ¡ch tiáº¿p cáº­n má»›i Ä‘á»ƒ phÃ¡t triá»ƒn máº¡ng trÃ¡nh Ä‘Æ°á»£c chi phÃ­ khá»Ÿi táº¡o trong cÃ¡c nghiÃªn cá»©u trÆ°á»›c. TrÆ°á»›c khi phÃ¡t triá»ƒn, má»—i lá»›p trong mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘Æ°á»£c táº¡o ra báº±ng má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u tham sá»‘. Trá»ng sá»‘ lá»›p má»›i Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch sá»­ dá»¥ng má»™t tá»• há»£p tuyáº¿n tÃ­nh má»›i cá»§a cÃ¡c máº«u hiá»‡n cÃ³ cho má»™t lá»›p. Má»™t máº·t, cÃ¡c máº«u nÃ y Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n cho nhiá»‡m vá»¥, cung cáº¥p má»™t khá»Ÿi táº¡o máº¡nh. Máº·t khÃ¡c, cÃ¡c há»‡ sá»‘ má»›i cung cáº¥p tÃ­nh linh hoáº¡t cho trá»ng sá»‘ lá»›p Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ há»c Ä‘iá»u gÃ¬ Ä‘Ã³ má»›i. ChÃºng tÃ´i cho tháº¥y cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i tÄƒng Ä‘á»™ chÃ­nh xÃ¡c top-1 so vá»›i hiá»‡n táº¡i tá»‘t nháº¥t 2-2.5% trÃªn cÃ¡c táº­p dá»¯ liá»‡u CIFAR-100 vÃ  ImageNet, trong khi Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Ã­t FLOPs hÆ¡n so vá»›i má»™t máº¡ng lá»›n hÆ¡n Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u. MÃ£ nguá»“n cÃ³ sáºµn táº¡i https://github.com/
chaudatascience/mixturegrowth .
1. Giá»›i thiá»‡u
Nhiá»u nhiá»‡m vá»¥ khÃ¡m phÃ¡ sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a hiá»‡u suáº¥t dá»± Ä‘oÃ¡n vÃ  Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n, cháº³ng háº¡n nhÆ° tÃ¬m kiáº¿m kiáº¿n trÃºc neural (NAS) [3, 12, 23, 39], chÆ°ng cáº¥t kiáº¿n thá»©c [16, 18], vÃ  cáº¯t tá»‰a tham sá»‘ [21, 44]. CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y dáº«n Ä‘áº¿n má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a tá»‘t trong quÃ¡ trÃ¬nh suy luáº­n nhÆ°ng thÆ°á»ng tá»‘n kÃ©m Ä‘á»ƒ huáº¥n luyá»‡n (vÃ­ dá»¥, [39]). Do Ä‘Ã³, má»™t sá»‘ nhÃ  nghiÃªn cá»©u Ä‘Ã£ khÃ¡m phÃ¡ má»™t quy trÃ¬nh thay tháº¿ trong Ä‘Ã³ ngÆ°á»i ta báº¯t Ä‘áº§u vá»›i má»™t máº¡ng nhá» vÃ  tÄƒng kÃ­ch thÆ°á»›c cá»§a nÃ³ theo thá»i gian (vÃ­ dá»¥, [14, 46]). Báº±ng cÃ¡ch huáº¥n luyá»‡n trÆ°á»›c má»™t mÃ´ hÃ¬nh nhá» hÆ¡n vÃ  ráº» hÆ¡n, cÃ¡ch tiáº¿p cáº­n nÃ y cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ há»c má»™t kiáº¿n trÃºc má»¥c tiÃªu vá»›i Ã­t phÃ©p toÃ¡n dáº¥u pháº©y Ä‘á»™ng (FLOPs) hÆ¡n so vá»›i huáº¥n luyá»‡n máº¡ng má»¥c tiÃªu tá»« Ä‘áº§u. ThÃ¡ch thá»©c chÃ­nh lÃ  quyáº¿t Ä‘á»‹nh cÃ¡ch khá»Ÿi táº¡o báº¥t ká»³ trá»ng sá»‘ lá»›p má»›i nÃ o sau má»™t bÆ°á»›c phÃ¡t triá»ƒn Ä‘á»ƒ trÃ¡nh quÃªn nhá»¯ng gÃ¬ máº¡ng Ä‘Ã£ há»c Ä‘Æ°á»£c trong khi váº«n Ä‘á»ƒ láº¡i khÃ´ng gian cho cáº£i thiá»‡n. NhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1a, cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o tham sá»‘ má»›i thÃ´ng qua má»™t bÆ°á»›c phÃ¢n tÃ­ch tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n Ä‘á»ƒ chá»n cÃ¡c tham sá»‘ giáº£m loss huáº¥n luyá»‡n [46] hoáº·c tÄƒng dÃ²ng gradient [14].

BÃ i bÃ¡o cá»§a chÃºng tÃ´i tÃ¬m cÃ¡ch tÃ¬m ra giáº£i phÃ¡p cho má»™t tÃ¬nh huá»‘ng trong Ä‘Ã³ chÃºng ta cÃ³ má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ vÃ  muá»‘n má»Ÿ rá»™ng nÃ³ thÃ nh má»™t mÃ´ hÃ¬nh lá»›n hÆ¡n. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t MixtureGrowth, má»™t khung phÃ¡t triá»ƒn máº¡ng má»›i táº¡o ra trá»ng sá»‘ má»›i báº±ng cÃ¡ch há»c cÃ¡ch tÃ¡i sá»­ dá»¥ng vÃ  káº¿t há»£p cÃ¡c tham sá»‘ tá»« máº¡ng nhá» hÆ¡n. CÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« thÃ nh cÃ´ng gáº§n Ä‘Ã¢y vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trá»™n máº«u [2, 31, 34, 41, 48], trong Ä‘Ã³ trá»ng sá»‘ lá»›p Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch sá»­ dá»¥ng má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u tham sá»‘ Ä‘Æ°á»£c chia sáº». Má»—i lá»›p cÃ³ Ä‘Æ°á»£c trá»ng sá»‘ tÃ¹y chá»‰nh báº±ng cÃ¡ch sá»­ dá»¥ng má»™t táº­p há»£p cÃ¡c há»‡ sá»‘ tuyáº¿n tÃ­nh duy nháº¥t, tá»« Ä‘Ã³ tÄƒng tÃ­nh biá»ƒu Ä‘áº¡t cá»§a cÃ¡c tham sá»‘ Ä‘Æ°á»£c chia sáº». CÃ¡c máº«u Ä‘Æ°á»£c chia sáº» vÃ  cÃ¡c há»‡ sá»‘ Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ¹ng nhau mÃ  khÃ´ng cáº§n thay Ä‘á»•i cÃ¡c hÃ m loss hoáº·c kiáº¿n trÃºc máº¡ng cho nhiá»‡m vá»¥ má»¥c tiÃªu. Khi so sÃ¡nh vá»›i cÃ¡c máº¡ng neural truyá»n thá»‘ng, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Ã£ bÃ¡o cÃ¡o cáº£i thiá»‡n hiá»‡u quáº£ tÃ­nh toÃ¡n vÃ  tham sá»‘ [2, 31, 41], giáº£m thá»i gian huáº¥n luyá»‡n [2,31], vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t nhiá»‡m vá»¥ [31,34,41,48].

Má»™t cÃ¡i nhÃ¬n tá»•ng quan cáº¥p cao vá» cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i Ä‘Æ°á»£c cung cáº¥p trong HÃ¬nh 1b. Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘áº·t giáº£ thuyáº¿t ráº±ng vÃ¬ trá»™n máº«u hiá»‡u quáº£ chia sáº» tham sá»‘ qua cÃ¡c lá»›p, nÃ³ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khá»Ÿi táº¡o trá»ng sá»‘ lá»›p má»›i sau má»™t bÆ°á»›c phÃ¡t triá»ƒn. ChÃºng tÃ´i Ä‘iá»u tra ba cÃ¢u há»i nghiÃªn cá»©u chÆ°a Ä‘Æ°á»£c khÃ¡m phÃ¡ trong cÃ¡c nghiÃªn cá»©u trÆ°á»›c. Äáº§u tiÃªn, trá»™n máº«u cung cáº¥p má»™t cÆ¡ cháº¿ má»›i Ä‘á»ƒ táº¡o ra trá»ng sá»‘ lá»›p má»›i sau khi phÃ¡t triá»ƒn. VÃ­ dá»¥, hÃ£y xem xÃ©t viá»‡c tÄƒng chiá»u rá»™ng cá»§a má»™t lá»›p g láº§n, dáº«n Ä‘áº¿n háº§u háº¿t cÃ¡c lá»›p trong máº¡ng má»Ÿ rá»™ng
1arXiv:2311.04251v1  [cs.LG]  7 Nov 2023

--- TRANG 2 ---
a. NghiÃªn cá»©u TrÆ°á»›c 
TrÃ¬nh Táº¡o 
Trá»ng sá»‘ Dá»¯ liá»‡u 
Trá»ng sá»‘ 
Hiá»‡n táº¡i TrÆ°á»›c phÃ¡t triá»ƒn 
Trá»ng sá»‘ 
Sau phÃ¡t triá»ƒn 
Trá»ng sá»‘ b. Mixture Growth (cá»§a chÃºng tÃ´i) 
Máº«u Tham sá»‘ 
ÄÆ°á»£c chia sáº» TrÃ¬nh Táº¡o 
Trá»ng sá»‘ ğ›‚cÅ© Há»‡ sá»‘ 
Tuyáº¿n tÃ­nh 
ğ›‚má»›i 
HÃ¬nh 1. So sÃ¡nh cÃ¡c chiáº¿n lÆ°á»£c khÃ¡c nhau trong viá»‡c phÃ¡t triá»ƒn máº¡ng neural. (a) Trong cÃ¡c nghiÃªn cá»©u trÆ°á»›c (vÃ­ dá»¥, [14,46]), cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn máº¡ng sá»­ dá»¥ng cÃ¡c biá»‡n phÃ¡p cá»¥c bá»™ dá»±a trÃªn loss Ä‘á»ƒ khá»Ÿi táº¡o trá»ng sá»‘ má»›i táº¡i thá»i Ä‘iá»ƒm phÃ¡t triá»ƒn. CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y phá»¥ thuá»™c vÃ o viá»‡c thá»±c hiá»‡n phÃ¢n tÃ­ch sá»­ dá»¥ng dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  trá»ng sá»‘ hiá»‡n táº¡i cá»§a cÃ¡c lá»›p hiá»‡n cÃ³, Ä‘Ã²i há»i chi phÃ­ tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ táº¡i bÆ°á»›c phÃ¡t triá»ƒn; (b)NgÆ°á»£c láº¡i, chÃºng tÃ´i Ä‘iá»u tra má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn trá»™n máº«u Ä‘á»ƒ phÃ¡t triá»ƒn máº¡ng. Cho má»™t máº¡ng nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n, chÃºng tÃ´i báº¯t Ä‘áº§u báº±ng viá»‡c huáº¥n luyá»‡n má»™t máº¡ng khÃ¡c cÃ³ kÃ­ch thÆ°á»›c báº±ng nhau. Sau Ä‘Ã³, cÃ¡c tham sá»‘ cá»§a nhá»¯ng máº¡ng nhá» nÃ y Ä‘Æ°á»£c chia sáº» qua cÃ¡c lá»›p báº±ng cÃ¡c sÆ¡ Ä‘á»“ trá»™n máº«u, trong Ä‘Ã³ trá»ng sá»‘ lá»›p bá»• sung Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c há»‡ sá»‘ tuyáº¿n tÃ­nh má»›i cho cÃ¡c máº«u hiá»‡n cÃ³.

cÃ³ g2 láº§n trá»ng sá»‘ cá»§a máº¡ng gá»‘c. VÃ¬ cÃ¡c trá»ng sá»‘ trong máº¡ng nhá» Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch sá»­ dá»¥ng má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u Ä‘Æ°á»£c chia sáº», chÃºng ta cÃ³ thá»ƒ táº¡o ra cÃ¡c trá»ng sá»‘ cá»§a máº¡ng lá»›n báº±ng cÃ¡ch khá»Ÿi táº¡o g2âˆ’1 táº­p há»‡ sá»‘ tuyáº¿n tÃ­nh má»›i vÃ  ná»‘i cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c táº¡o ra cho lá»›p lá»›n (Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2). ChÃºng tÃ´i tháº¥y ráº±ng viá»‡c khá»Ÿi táº¡o cáº©n tháº­n cÃ¡c há»‡ sá»‘ tuyáº¿n tÃ­nh má»›i nÃ y váº«n cáº§n thiáº¿t Ä‘á»ƒ cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t, vá»›i má»™t sá»‘ chiáº¿n lÆ°á»£c, cháº³ng háº¡n nhÆ° sao chÃ©p cÃ¡c há»‡ sá»‘ hiá»‡n cÃ³, cÃ³ tÃ¡c Ä‘á»™ng cÃ³ háº¡i Ä‘áº¿n viá»‡c huáº¥n luyá»‡n.

CÃ¢u há»i nghiÃªn cá»©u thá»© hai mÃ  chÃºng tÃ´i Ä‘iá»u tra lÃ  liá»‡u chÃºng ta nÃªn phÃ¡t triá»ƒn tá»« má»™t mÃ´ hÃ¬nh nhá» duy nháº¥t, hay viá»‡c káº¿t há»£p hai mÃ´ hÃ¬nh nhá» cÃ³ tá»‘t hÆ¡n khÃ´ng. ChÃºng tÃ´i láº­p luáº­n ráº±ng viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh nhá» thÃªm cung cáº¥p nhiá»u Ä‘a dáº¡ng hÆ¡n vá» cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ há»c cho quÃ¡ trÃ¬nh phÃ¡t triá»ƒn. Trong má»™t thiáº¿t láº­p mÃ  chÃºng ta phÃ¡t triá»ƒn tá»« má»™t mÃ´ hÃ¬nh cÃ³ chiá»u rá»™ng báº±ng má»™t ná»­a mÃ´ hÃ¬nh lá»›n, thÃ¬ má»™t máº¡ng nhá» sáº½ chá»‰ máº¥t khoáº£ng 25% FLOPs cá»§a máº¡ng lá»›n, lÃ m cho nÃ³ tÆ°Æ¡ng Ä‘á»‘i ráº».

CÃ¢u há»i nghiÃªn cá»©u cuá»‘i cÃ¹ng mÃ  chÃºng tÃ´i khÃ¡m phÃ¡, tiáº¿p theo cÃ¢u há»i trÆ°á»›c, lÃ  tÃ¬m thá»i Ä‘iá»ƒm tá»‘i Æ°u Ä‘á»ƒ dá»«ng huáº¥n luyá»‡n máº¡ng thá»© hai vÃ  báº¯t Ä‘áº§u huáº¥n luyá»‡n má»™t máº¡ng Ä‘Ã£ phÃ¡t triá»ƒn Ä‘áº§y Ä‘á»§. Vá»›i má»™t ngÃ¢n sÃ¡ch FLOPs cá»‘ Ä‘á»‹nh, phÃ¡t triá»ƒn sá»›m cho nhiá»u thá»i gian hÆ¡n Ä‘á»ƒ tá»‘i Æ°u hÃ³a táº¥t cáº£ cÃ¡c tham sá»‘ lá»›p trong máº¡ng lá»›n. Tuy nhiÃªn, Ä‘iá»u nÃ y cÅ©ng háº¡n cháº¿ kháº£ nÄƒng cá»§a máº¡ng thá»© hai, tá»« Ä‘Ã³ Ä‘áº·t ra cÃ¢u há»i vá» thá»i Ä‘iá»ƒm phÃ¡t triá»ƒn. ChÃºng tÃ´i tháº¥y ráº±ng cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i tÆ°Æ¡ng Ä‘á»‘i khÃ´ng nháº¡y cáº£m vá»›i giai Ä‘oáº¡n huáº¥n luyá»‡n chÃ­nh xÃ¡c mÃ  táº¡i Ä‘Ã³ bÆ°á»›c phÃ¡t triá»ƒn xáº£y ra, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t triá»ƒn muá»™n vá»›i tÃ¡c Ä‘á»™ng tá»‘i thiá»ƒu Ä‘áº¿n hiá»‡u suáº¥t nhiá»‡m vá»¥.

TÃ³m láº¡i, cÃ¡c Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i lÃ :
â€¢ ChÃºng tÃ´i phÃ¡t triá»ƒn MixtureGrowth, má»™t cÃ¡ch tiáº¿p cáº­n má»›i Ä‘á»ƒ phÃ¡t triá»ƒn má»™t máº¡ng neural theo thá»i gian báº±ng cÃ¡ch ná»‘i cÃ¡c trá»ng sá»‘ cá»§a máº¡ng cÅ© (nhá») vá»›i trá»ng sá»‘ má»›i Ä‘Æ°á»£c táº¡o ra tá»« cÃ¡c há»‡ sá»‘ tuyáº¿n tÃ­nh Ä‘Æ°á»£c khá»Ÿi táº¡o má»›i Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ káº¿t há»£p cÃ¡c máº«u tham sá»‘ Ä‘Æ°á»£c chia sáº» hiá»‡n cÃ³.

â€¢ CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y MixtureGrowth lÃ  má»™t phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn hiá»‡u quáº£ báº±ng cÃ¡ch vÆ°á»£t trá»™i Ä‘á»™ chÃ­nh xÃ¡c top-1 cá»§a cÃ¡c nghiÃªn cá»©u trÆ°á»›c 2-2.5% trÃªn CIFAR-100 vÃ  ImageNet, chá»©ng minh tÃ­nh hiá»‡u quáº£ cá»§a cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i.

â€¢ ChÃºng tÃ´i phÃ¢n tÃ­ch cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i Ä‘á»ƒ phÃ¡t triá»ƒn máº¡ng neural theo cÃ¡c trá»¥c nhÆ° Ä‘á»™ nháº¡y cáº£m vá»›i Ä‘iá»ƒm phÃ¡t triá»ƒn, Ä‘áº·c Ä‘iá»ƒm cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ há»c, vÃ  tÃ¡c Ä‘á»™ng cá»§a viá»‡c káº¿t há»£p hai mÃ´ hÃ¬nh nhá» so vá»›i phÃ¡t triá»ƒn tá»« má»™t mÃ´ hÃ¬nh duy nháº¥t.

2. NghiÃªn cá»©u LiÃªn quan
PhÃ¡t triá»ƒn Máº¡ng Neural tá»« lÃ¢u Ä‘Ã£ lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u tÃ­ch cá»±c; Trong nghiÃªn cá»©u ban Ä‘áº§u, Ash et al. [1] Ä‘Ã£ thÃªm cÃ¡c neuron trong cÃ¡c há»‡ thá»‘ng má»™t lá»›p, sau Ä‘Ã³ Ä‘Æ°á»£c má»Ÿ rá»™ng bá»Ÿi Fahlman et al. [15] cho cháº¿ Ä‘á»™ máº¡ng sÃ¢u. Viá»‡c huáº¥n luyá»‡n trÆ°á»›c cÃ¡c máº¡ng neural má»™t cÃ¡ch tham lam báº±ng cÃ¡ch xáº¿p chá»“ng MÃ¡y Boltzmann Bá»‹ háº¡n cháº¿ [4, 20] Ä‘á»ƒ xÃ¢y dá»±ng Máº¡ng Niá»m tin SÃ¢u [19] Ä‘Ã£ trá»Ÿ nÃªn phá»• biáº¿n. Äiá»u nÃ y Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hÃ³a bá»Ÿi Vincent et al. [43] vá»›i Bá»™ Tá»± mÃ£ hÃ³a Khá»­ nhiá»…u Xáº¿p chá»“ng. Gáº§n Ä‘Ã¢y hÆ¡n, Wen et al. [45] Ä‘Ã£ Ä‘á» xuáº¥t cÃ¡c chÃ­nh sÃ¡ch phÃ¡t triá»ƒn Ä‘á»™ sÃ¢u Ä‘á»ƒ tá»± Ä‘á»™ng phÃ¡t triá»ƒn máº¡ng cho Ä‘áº¿n khi hiá»‡u suáº¥t ngá»«ng cáº£i thiá»‡n. Maile et al. [28] Ä‘á» xuáº¥t khá»Ÿi táº¡o trá»ng sá»‘ má»›i Ä‘á»ƒ tá»‘i Ä‘a hÃ³a tÃ­nh trá»±c giao. Tuy nhiÃªn, cÃ¡c nghiÃªn cá»©u nÃ y nháº±m tá»± Ä‘á»™ng hÃ³a viá»‡c lá»±a chá»n kÃ­ch thÆ°á»›c máº¡ng báº±ng phÃ¡t triá»ƒn tiáº¿n bá»™, trÃ¡i ngÆ°á»£c vá»›i viá»‡c giáº£m thiá»ƒu chi phÃ­ huáº¥n luyá»‡n cho má»™t kÃ­ch thÆ°á»›c máº¡ng nháº¥t Ä‘á»‹nh.

Trong nghiÃªn cá»©u liÃªn quan hÆ¡n, Wu et al. [47] vÃ  Chen et al. [7] Ä‘Ã£ sao chÃ©p trá»ng sá»‘ cá»§a máº¡ng Ä‘á»ƒ tÄƒng dáº§n chiá»u rá»™ng cá»§a nÃ³, trong khi Net2Net [7] táº­n dá»¥ng cÃ¡c biáº¿n Ä‘á»•i báº£o toÃ n hÃ m Ä‘á»ƒ chuyá»ƒn giao kiáº¿n thá»©c khi phÃ¡t triá»ƒn. CÃ¡c cÃ¡ch tiáº¿p cáº­n nÃ y sao chÃ©p trá»ng sá»‘ sao cho hÃ m trÆ°á»›c phÃ¡t triá»ƒn Ä‘Æ°á»£c báº£o toÃ n, vÃ  sau Ä‘Ã³ nhiá»…u loáº¡n hoáº·c thÃªm nhiá»…u Ä‘á»ƒ phÃ¡ vá»¡ tÃ­nh Ä‘á»‘i xá»©ng Ä‘á»ƒ phÃ¡t triá»ƒn. Firefly [46] cÅ©ng há»c má»™t máº¡ng phÃ¡t triá»ƒn quÃ¡ má»©c vÃ  sau Ä‘Ã³ cáº¯t tá»‰a cÃ¡c trá»ng sá»‘ khÃ´ng cÃ³ thÃ´ng tin. GradMax [14] vÃ  NeST [9] khá»Ÿi táº¡o trá»ng sá»‘ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a chuáº©n gradient. Máº·c dÃ¹ cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y hiá»‡u quáº£ hÆ¡n so vá»›i huáº¥n luyá»‡n láº¡i tá»« Ä‘áº§u, nhiá»u trong sá»‘ chÃºng váº«n yÃªu cáº§u chi phÃ­ tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ Ä‘á»ƒ phÃ¢n tÃ­ch cÃ¡ch máº¡ng nÃªn khá»Ÿi táº¡o máº¡ng má»›i.

Trá»™n Máº«u cáº£i thiá»‡n viá»‡c chia sáº» tham sá»‘ cá»©ng, trá»±c tiáº¿p tÃ¡i sá»­ dá»¥ng tham sá»‘ qua cÃ¡c lá»›p [8, 50], báº±ng cÃ¡ch
2

--- TRANG 3 ---
!!âˆˆ#â€¦%!ğ›¼#!#ğ‘¡!#InputOutput Máº¡ng 1(Ä‘Ã£ huáº¥n luyá»‡n)TrÆ°á»›c PhÃ¡t triá»ƒn 
PhÃ¡t triá»ƒnğ‘‡!!!âˆˆ#â€¦%"ğ›¼&!#ğ‘¡!&InputOutput Máº¡ng 2!!âˆˆ#â€¦%!ğ›¼#!#ğ‘¡!#!!âˆˆ#â€¦%"ğ›¼&!#ğ‘¡!&!!âˆˆ#â€¦%!ğ›¼#!&ğ‘¡!#!!âˆˆ#â€¦%"ğ›¼&!&ğ‘¡!&Sau PhÃ¡t triá»ƒn InputOutput 
ğ‘‡"ğ‘‡"ğ‘‡!HÃ¬nh 2. Minh há»a quÃ¡ trÃ¬nh phÃ¡t triá»ƒn trong cÃ¡c mÃ´ hÃ¬nh MixtureGrowth vá»›i g= 2 (tá»©c lÃ , gáº¥p Ä‘Ã´i kÃ­ch thÆ°á»›c): ChÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡ch phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh cÃ³ trá»ng sá»‘ Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« cÃ¡c tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u, mÃ  chÃºng tÃ´i gá»i lÃ  MÃ´ hÃ¬nh Trá»™n Máº«u (xem Má»¥c 3.1). TrÆ°á»›c khi phÃ¡t triá»ƒn, giáº£ sá»­ ráº±ng ngÆ°á»i ta Ä‘Ã£ cÃ³ má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ (kÃ½ hiá»‡u lÃ  máº¡ng 1, bÃªn trÃ¡i) vÃ  muá»‘n má»Ÿ rá»™ng nÃ³. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i trÆ°á»›c tiÃªn huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh khÃ¡c cÃ³ kÃ­ch thÆ°á»›c báº±ng nhau sá»­ dá»¥ng má»™t táº­p há»£p máº«u khÃ¡c (máº¡ng 2) trong má»™t sá»‘ epoch. Sau Ä‘Ã³, chÃºng tÃ´i káº¿t há»£p trá»ng sá»‘ cá»§a hai máº¡ng sao cho trá»ng sá»‘ cá»§a chÃºng Ä‘Æ°á»£c xáº¿p trong cÃ¡c gÃ³c dá»c theo Ä‘Æ°á»ng chÃ©o. Trong vÃ­ dá»¥ nÃ y, chÃºng tÃ´i táº¡o thÃ nh má»™t mÃ´ hÃ¬nh lá»›n gáº¥p 4 láº§n vÃ  tiáº¿p tá»¥c huáº¥n luyá»‡n trÃªn mÃ´ hÃ¬nh Ä‘Ã£ phÃ¡t triá»ƒn Ä‘áº§y Ä‘á»§ (mÃ´ hÃ¬nh má»¥c tiÃªu, ngoÃ i cÃ¹ng bÃªn pháº£i). CÃ¡c gÃ³c pháº§n tÆ° chia sáº» máº«u nhÆ°ng cÃ³ cÃ¡c há»‡ sá»‘ tá»• há»£p tuyáº¿n tÃ­nh khÃ¡c nhau Ä‘Æ°á»£c trÃ¬nh bÃ y dÆ°á»›i dáº¡ng cÃ¡c sáº¯c thÃ¡i khÃ¡c nhau cá»§a cÃ¹ng má»™t mÃ u. CÃ¡c hÃ¬nh vuÃ´ng tÆ°Æ¡ng á»©ng vá»›i trá»ng sá»‘ lá»›p cÃ³ Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra tÆ°Æ¡ng á»©ng vá»›i chiá»u Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. LÆ°u Ã½ ráº±ng táº¥t cáº£ máº«u vÃ  há»‡ sá»‘ alpha Ä‘á»u lÃ  tham sá»‘ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c.

biá»ƒu diá»…n trá»ng sá»‘ lá»›p nhÆ° má»™t há»—n há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u Ä‘Æ°á»£c chia sáº» (vÃ­ dá»¥, [2, 31, 34, 41, 48]). Savarese et al. [34] tháº¥y ráº±ng há» cÃ³ thá»ƒ sá»­ dá»¥ng trá»™n máº«u Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t báº±ng cÃ¡ch chia sáº» qua nhiá»u lá»›p trong má»™t máº¡ng duy nháº¥t. CÃ¡c máº¡ng Ä‘áº³ng cáº¥u [33, 51] cÃ³ thá»ƒ Ä‘áº©y viá»‡c chia sáº» tham sá»‘ giá»¯a cÃ¡c lá»›p xa hÆ¡n báº±ng cÃ¡ch rÃ ng buá»™c máº¡ng cÃ³ cÃ¡c lá»›p cÃ³ hÃ¬nh dáº¡ng vÃ  kÃ­ch thÆ°á»›c giá»‘ng há»‡t nhau. Plummer et al. [31] loáº¡i bá» nhu cáº§u cÃ³ nhiá»u lá»›p giá»‘ng há»‡t nhau báº±ng cÃ¡ch nÃ¢ng cáº¥p/háº¡ cáº¥p máº«u khi Ä‘Æ°á»£c chia sáº» giá»¯a cÃ¡c lá»›p cÃ³ hÃ¬nh dáº¡ng khÃ¡c nhau. ThÃº vá»‹ lÃ , cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y thá»ƒ hiá»‡n sá»± tÆ°Æ¡ng Ä‘á»“ng vá»›i khÃ¡i niá»‡m há»c táº­p mÃ´-Ä‘un, trong Ä‘Ã³ cÃ¡c lá»›p máº¡ng Ä‘Æ°á»£c chia sáº» qua nhiá»u nhiá»‡m vá»¥, nhÆ° Ä‘Æ°á»£c khÃ¡m phÃ¡ trong nghiÃªn cá»©u trÆ°á»›c [29, 30]. Trong khi há»c táº­p mÃ´-Ä‘un dá»±a vÃ o trá»ng sá»‘ lá»›p Ä‘Æ°á»£c chia sáº» Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho tá»«ng nhiá»‡m vá»¥ riÃªng láº», trá»™n máº«u, ngÆ°á»£c láº¡i, káº¿t há»£p cÃ¡c máº«u Ä‘Æ°á»£c chia sáº» Ä‘á»ƒ táº¡o ra trá»ng sá»‘ lá»›p má»™t cÃ¡ch Ä‘á»™ng. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i má»Ÿ rá»™ng nghiÃªn cá»©u vá» trá»™n máº«u báº±ng cÃ¡ch Ä‘áº·t ra cÃ¡c cÃ¢u há»i nghiÃªn cá»©u má»›i chÆ°a Ä‘Æ°á»£c giáº£i quyáº¿t trong cÃ¡c nghiÃªn cá»©u trÆ°á»›c. Cá»¥ thá»ƒ, cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» trá»™n máº«u giáº£ Ä‘á»‹nh máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n giá»¯ nguyÃªn kÃ­ch thÆ°á»›c theo thá»i gian. Tuy nhiÃªn, trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i táº­n dá»¥ng cáº¥u trÃºc Ä‘á»™c Ä‘Ã¡o cá»§a trá»™n máº«u Ä‘á»ƒ táº¡o ra trá»ng sá»‘ má»›i khi má»Ÿ rá»™ng máº¡ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

Thu nhá» Máº¡ng Neural thay Ä‘á»•i kÃ­ch thÆ°á»›c cá»§a má»™t mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n thÆ°á»ng nháº±m tÄƒng hiá»‡u quáº£ tÃ­nh toÃ¡n táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra. CÃ³ hai phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho viá»‡c nÃ y: chÆ°ng cáº¥t vÃ  cáº¯t tá»‰a máº¡ng neural. Trong chÆ°ng cáº¥t máº¡ng neural, má»¥c tiÃªu lÃ  chuyá»ƒn giao kiáº¿n thá»©c tá»« má»™t máº¡ng lá»›n hÆ¡n sang má»™t máº¡ng nhá» hÆ¡n. Ká»¹ thuáº­t nÃ y, Ä‘Æ°á»£c giá»›i thiá»‡u trong Hinton et al. [18], thay tháº¿ cÃ¡c nhÃ£n má»™t-hot báº±ng cÃ¡c dá»± Ä‘oÃ¡n cá»§a má»™t máº¡ng lá»›n hÆ¡n (tá»©c lÃ  máº¡ng giÃ¡o viÃªn). Theo cÃ¡ch nÃ y, kiáº¿n thá»©c tá»« máº¡ng giÃ¡o viÃªn Ä‘Æ°á»£c chuyá»ƒn giao cho máº¡ng nhá» hÆ¡n gá»i lÃ  máº¡ng há»c sinh. VÃ­ dá»¥ vá» cÃ¡c nghiÃªn cá»©u tiáº¿p theo bao gá»“m cÃ¡c phÆ°Æ¡ng phÃ¡p chÆ°ng cáº¥t táº­p há»£p thÃ nh má»™t mÃ´ hÃ¬nh duy nháº¥t [36] vÃ  lÃ m ná»•i báº­t táº§m quan trá»ng cá»§a viá»‡c há»c sinh vÃ  giÃ¡o viÃªn tháº¥y cÃ¡c tÄƒng cÆ°á»ng giá»‘ng há»‡t nhau trong cÃ¡c giai Ä‘oáº¡n huáº¥n luyá»‡n dÃ i [6]. Trong cáº¯t tá»‰a tham sá»‘, má»¥c tiÃªu lÃ  tÃ¬m vÃ  loáº¡i bá» cÃ¡c trá»ng sá»‘ khÃ´ng quan trá»ng (vÃ­ dá»¥, [27,38,42]), dáº«n Ä‘áº¿n má»™t máº¡ng hiá»‡u quáº£ hÆ¡n vá» máº·t tÃ­nh toÃ¡n vá»›i chi phÃ­ tÄƒng thá»i gian huáº¥n luyá»‡n vÃ¬ nÃ³ giáº£ Ä‘á»‹nh ngÆ°á»i ta Ä‘Ã£ huáº¥n luyá»‡n máº¡ng Ä‘Æ°á»£c tham sá»‘ hÃ³a Ä‘áº§y Ä‘á»§ cho Ä‘áº¿n khi há»™i tá»¥. TrÃ¡i ngÆ°á»£c vá»›i cÃ¡c nhiá»‡m vá»¥ nÃ y, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  giáº£m thá»i gian huáº¥n luyá»‡n cá»§a máº¡ng lá»›n báº±ng cÃ¡ch phÃ¡t triá»ƒn má»™t máº¡ng neural theo thá»i gian.

3. PhÃ¡t triá»ƒn thÃ´ng qua khá»Ÿi táº¡o tham sá»‘ Ä‘Æ°á»£c chia sáº»
Cho má»™t kiáº¿n trÃºc máº¡ng neural nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c FS vá»›i cÃ¡c lá»›p â„“S1,...,N, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  táº¡o ra trá»ng sá»‘ cá»§a má»™t kiáº¿n trÃºc máº¡ng neural lá»›n hÆ¡n FL vá»›i cÃ¡c lá»›p â„“L1,...,M. Theo [14, 46], chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c thiáº¿t láº­p trong Ä‘Ã³ cÃ¡c máº¡ng cÃ³ cÃ¹ng sá»‘ lÆ°á»£ng lá»›p (tá»©c lÃ , M=N), máº·c dÃ¹ cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¡t triá»ƒn theo Ä‘á»™ sÃ¢u [31,34]. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i thay Ä‘á»•i chiá»u rá»™ng cá»§a má»—i lá»›p, tá»©c lÃ , |â„“Li|>|â„“Si|, trong Ä‘Ã³ toÃ¡n tá»­ | Â· | tráº£ vá» sá»‘ lÆ°á»£ng trá»ng sá»‘ trong má»™t lá»›p. Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, Ä‘áº·t kÃ­ch thÆ°á»›c cá»§a FL gáº¥p Ä‘Ã´i FS, máº·c dÃ¹ cÃ¡c thiáº¿t láº­p khÃ¡c cÃ³ thá»ƒ vá»›i nhá»¯ng thay Ä‘á»•i nhá» trong viá»‡c xÃ¢y dá»±ng trá»ng sá»‘ lá»›p (vÃ­ dá»¥, nhÆ° trong [31, 41]).

Nhiá»‡m vá»¥ khi Ä‘Ã³ lÃ  táº¡o ra trá»ng sá»‘ cho FL cho phÃ©p nÃ³ há»™i tá»¥ nhanh chÃ³ng sá»­ dá»¥ng nhá»¯ng gÃ¬ Ä‘Ã£ Ä‘Æ°á»£c há»c trong FS. CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c xáº¿p háº¡ng dá»±a trÃªn cáº£ hiá»‡u quáº£ tÃ­nh toÃ¡n cá»§a viá»‡c cÃ³ Ä‘Æ°á»£c mÃ´ hÃ¬nh FL Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ cÅ©ng nhÆ° cháº¥t lÆ°á»£ng cá»§a giáº£i phÃ¡p cuá»‘i cÃ¹ng Ä‘Æ°á»£c Ä‘o báº±ng hiá»‡u suáº¥t nhiá»‡m vá»¥.

CÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» phÃ¡t triá»ƒn máº¡ng neural dá»±a vÃ o viá»‡c thá»±c hiá»‡n má»™t phÃ¢n tÃ­ch tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n cho tá»«ng lá»›p vá» gradient hoáº·c loss huáº¥n luyá»‡n Ä‘á»ƒ khá»Ÿi táº¡o báº¥t ká»³ tham sá»‘ má»›i nÃ o trong FL (vÃ­ dá»¥, [14, 46]). ChÃºng tÃ´i trÃ¡nh váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch
3

--- TRANG 4 ---
há»c cÃ¡ch táº¡o ra trá»ng sá»‘ má»›i báº±ng cÃ¡ch sá»­ dá»¥ng má»™t trÃ¬nh táº¡o trá»ng sá»‘ dá»±a trÃªn cÃ¡c tham sá»‘ Ä‘Æ°á»£c chia sáº». LÆ°u Ã½ ráº±ng chÃºng tÃ´i Ä‘á» cáº­p Ä‘áº¿n trá»ng sá»‘ nhÆ° ma tráº­n cÃ¡c sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi má»™t phÃ©p toÃ¡n lá»›p, cháº³ng háº¡n nhÆ° cÃ¡c bá»™ lá»c Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c lá»›p tÃ­ch cháº­p, vÃ  tham sá»‘ nhÆ° cÃ¡c ma tráº­n Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a báº±ng cÃ¡ch sá»­ dá»¥ng lan truyá»n ngÆ°á»£c. Trong cÃ¡c máº¡ng neural truyá»n thá»‘ng, trá»ng sá»‘ vÃ  tham sá»‘ thÆ°á»ng Ä‘á» cáº­p Ä‘áº¿n cÃ¹ng má»™t táº­p há»£p ma tráº­n, nhÆ°ng trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº¡o ra trá»ng sá»‘ lá»›p báº±ng cÃ¡ch sá»­ dá»¥ng tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n. CÃ¡c tham sá»‘ lÃ  cÃ¡c tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u lÃ  káº¿t quáº£ cá»§a má»™t quÃ¡ trÃ¬nh mÃ  chÃºng tÃ´i Ä‘á» cáº­p Ä‘áº¿n nhÆ° trá»™n máº«u [31,34,41] mÃ  chÃºng tÃ´i xem xÃ©t ngáº¯n gá»n trong Má»¥c 3.1. Tiáº¿p theo, chÃºng tÃ´i tháº£o luáº­n vá» chiáº¿n lÆ°á»£c cá»§a chÃºng tÃ´i Ä‘á»ƒ há»c cÃ¡c máº«u tham sá»‘ biá»ƒu Ä‘áº¡t hÆ¡n báº±ng cÃ¡ch sá»­ dá»¥ng káº¿t há»£p mÃ´ hÃ¬nh (Má»¥c 3.2). Cuá»‘i cÃ¹ng, chÃºng tÃ´i giá»›i thiá»‡u má»™t quÃ¡ trÃ¬nh khá»Ÿi táº¡o báº¥t ká»³ trá»ng sá»‘ má»›i nÃ o cáº§n thiáº¿t bá»Ÿi FL báº±ng cÃ¡ch xáº¿p cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c máº«u ban Ä‘áº§u Ä‘Æ°á»£c há»c tá»« máº¡ng nhá» FS cá»§a chÃºng tÃ´i (Má»¥c 3.3).

3.1. Táº¡o trá»ng sá»‘ vá»›i trá»™n máº«u
CÃ¡c máº¡ng trá»™n máº«u (vÃ­ dá»¥, [2, 31, 34, 41, 48]) táº¡o ra trá»ng sá»‘ lá»›p báº±ng cÃ¡ch káº¿t há»£p cÃ¡c máº«u tham sá»‘ vÃ  Ä‘Ã£ cho tháº¥y chÃºng cung cáº¥p hiá»‡u quáº£ tham sá»‘ vÃ  hiá»‡u suáº¥t vÆ°á»£t trá»™i so vá»›i cÃ¡c máº¡ng neural truyá»n thá»‘ng. ChÃºng tÃ´i sá»­ dá»¥ng thiáº¿t láº­p trong Ä‘Ã³ má»—i máº«u tham sá»‘ T1,..,t cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c vá»›i lá»›p tÆ°Æ¡ng á»©ng trong máº¡ng neural nhá» FS (tá»©c lÃ , |Tki|=|â„“Si|), máº·c dÃ¹ cÃ¡c máº«u tham sá»‘ cÃ³ kÃ­ch thÆ°á»›c khÃ¡c nhau cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng [41]. Do Ä‘Ã³, trá»ng sá»‘ cá»§a lá»›p thá»© i, kÃ½ hiá»‡u lÃ  â„“Si, Ä‘Æ°á»£c táº¡o ra thÃ´ng qua,

â„“Si=âˆ‘kâˆˆ1,...,tÎ±kiTki, (1)

trong Ä‘Ã³ Î±ki lÃ  má»™t tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n Ä‘iá»u khiá»ƒn Ä‘Ã³ng gÃ³p cá»§a máº«u Tki trong viá»‡c táº¡o ra trá»ng sá»‘ cho lá»›p â„“Si.

ChÃºng tÃ´i báº¯t Ä‘áº§u báº±ng viá»‡c huáº¥n luyá»‡n trÆ°á»›c máº¡ng nhá» FS cá»§a chÃºng tÃ´i báº±ng cÃ¡ch sá»­ dá»¥ng trá»™n máº«u. Theo [34], chÃºng tÃ´i chia sáº» máº«u giá»¯a báº¥t ká»³ cáº·p lá»›p nÃ o cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c (vá»›i nhÃ³m tá»‘i Ä‘a gá»“m hai lá»›p liÃªn tiáº¿p chia sáº» máº«u) cho cÃ¡c trá»ng sá»‘ má»›i. VÃ­ dá»¥, náº¿u Ti vÃ  Ti+1 Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra trá»ng sá»‘ cho cÃ¡c lá»›p giá»‘ng há»‡t nhau â„“Si vÃ  â„“Si+1, thÃ¬ chÃºng Ä‘á» cáº­p Ä‘áº¿n cÃ¹ng má»™t táº­p há»£p tham sá»‘ Ä‘Æ°á»£c chia sáº», vÃ  chÃºng sáº½ khÃ´ng chia sáº» máº«u vá»›i lá»›p â„“Si+2, ngay cáº£ khi cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c.

Tá»•ng sá»‘ tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n so vá»›i má»™t máº¡ng neural tiÃªu chuáº©n chá»‰ tÄƒng bá»Ÿi sá»‘ lÆ°á»£ng há»‡ sá»‘ Î±ki Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra trá»ng sá»‘ (dáº«n Ä‘áº¿n hÃ ng trÄƒm tham sá»‘ thÃªm trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n). Cáº£ há»‡ sá»‘ Î±ki vÃ  máº«u Ti Ä‘á»u Ä‘Æ°á»£c há»c cÃ¹ng nhau thÃ´ng qua lan truyá»n ngÆ°á»£c tá»« loss nhiá»‡m vá»¥, khÃ´ng cÃ³ cÃ¡c Ä‘iá»u khoáº£n loss bá»• sung so vá»›i má»™t máº¡ng neural tiÃªu chuáº©n Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. LÆ°u Ã½ ráº±ng quÃ¡ trÃ¬nh táº¡o trá»ng sá»‘ nÃ y thá»±c sá»± giá»›i thiá»‡u má»™t sá»‘ chi phÃ­ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, nhÆ°ng sá»± gia tÄƒng nÃ y lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ, chá»‰ chiáº¿m 0.03% FLOPs trong má»™t láº§n forward pass cho kÃ­ch thÆ°á»›c batch 64 máº«u [31].

3.2. Há»c cÃ¡c máº«u máº¡nh máº½ Ä‘á»ƒ phÃ¡t triá»ƒn
Má»™t sá»± khÃ¡c biá»‡t chÃ­nh giá»¯a MixtureGrowth cá»§a chÃºng tÃ´i vÃ  cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» trá»™n máº«u (vÃ­ dá»¥, [2, 31, 34, 41, 48]) lÃ  kiáº¿n trÃºc trong giai Ä‘oáº¡n huáº¥n luyá»‡n Ä‘áº§u tiÃªn cá»§a chÃºng tÃ´i khÃ´ng pháº£i lÃ  kiáº¿n trÃºc máº¡ng cuá»‘i cÃ¹ng. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i sá»­ dá»¥ng má»™t quy trÃ¬nh máº¡ng neural tiÃªu chuáº©n Ä‘á»ƒ há»c má»™t máº¡ng nhá» vÃ  sau Ä‘Ã³ phÃ¡t triá»ƒn thÃ nh má»™t máº¡ng lá»›n hÆ¡n, tÆ°Æ¡ng tá»± nhÆ° cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» phÃ¡t triá»ƒn máº¡ng (vÃ­ dá»¥, [14, 46]). CÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» phÃ¡t triá»ƒn máº¡ng sáº½ khá»Ÿi táº¡o cÃ¡c tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n má»›i báº±ng cÃ¡ch sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p heuristic Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« máº¡ng nhá». Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n khá»Ÿi táº¡o khÃ´ng thuáº­n lá»£i vÃ¬ tÃ­nh biá»ƒu Ä‘áº¡t cá»§a máº¡ng lá»›n lá»›n hÆ¡n nhiá»u (do cÃ³ nhiá»u trá»ng sá»‘ hÆ¡n) so vá»›i máº¡ng nhá». CÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» phÃ¡t triá»ƒn máº¡ng cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° táº­p trung vÃ o viá»‡c cá»‘ gáº¯ng khÃ´i phá»¥c (má»™t pháº§n) tÃ­nh biá»ƒu Ä‘áº¡t bá»• sung bá»‹ thiáº¿u trong máº¡ng nhá». NgÆ°á»£c láº¡i, cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i giáº£ Ä‘á»‹nh ráº±ng nhiá»u máº«u mÃ  chÃºng tÃ´i Ä‘Ã£ há»c trong FS sáº½ tá»•ng quÃ¡t hÃ³a cho cÃ¡c trá»ng sá»‘ má»›i cáº§n thiáº¿t sau khi phÃ¡t triá»ƒn thÃ nh máº¡ng má»¥c tiÃªu FL.

ChÃºng tÃ´i khÃ¡m phÃ¡ hai biáº¿n thá»ƒ cá»§a phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i: má»™t chÃºng tÃ´i phÃ¡t triá»ƒn trá»±c tiáº¿p tá»« má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n duy nháº¥t vÃ  tÃ¡i sá»­ dá»¥ng cÃ¡c máº«u cho trá»ng sá»‘ má»›i sau khi phÃ¡t triá»ƒn, vÃ  má»™t khÃ¡c trong Ä‘Ã³ chÃºng tÃ´i há»c má»™t mÃ´ hÃ¬nh thÃªm sá»­ dá»¥ng má»™t táº­p há»£p máº«u Ä‘á»™c láº­p trÆ°á»›c khi phÃ¡t triá»ƒn. ChÃºng tÃ´i gá»i cÃ¡ch tiáº¿p cáº­n thá»© hai nÃ y lÃ  káº¿t há»£p mÃ´ hÃ¬nh, Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi Ã½ tÆ°á»Ÿng ráº±ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p há»c cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»™c láº­p Ä‘Ã¡ng ká»ƒ [11, 26]. Do Ä‘Ã³, viá»‡c káº¿t há»£p hai mÃ´ hÃ¬nh táº¡o ra má»™t biá»ƒu diá»…n Ä‘áº·c trÆ°ng máº¡nh máº½ hÆ¡n, tá»« Ä‘Ã³ tÄƒng hiá»‡u suáº¥t. Tá»« má»™t cáº·p máº¡ng nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n FS1, FS2, chÃºng tÃ´i sá»­ dá»¥ng chÃºng Ä‘á»ƒ khá»Ÿi táº¡o vÃ  huáº¥n luyá»‡n máº¡ng má»¥c tiÃªu FL cá»§a chÃºng tÃ´i, mÃ  chÃºng tÃ´i tháº£o luáº­n trong pháº§n tiáº¿p theo.

3.3. Táº¡o trá»ng sá»‘ cho máº¡ng má»¥c tiÃªu
Sau khi cÃ³ Ä‘Æ°á»£c cáº·p máº¡ng nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c FS1, FS2 sá»­ dá»¥ng cÃ¡ch tiáº¿p cáº­n trong Má»¥c 3.2, má»¥c tiÃªu lÃ  sá»­ dá»¥ng cÃ¡c máº¡ng nÃ y Ä‘á»ƒ xÃ¢y dá»±ng trá»ng sá»‘ cá»§a máº¡ng lá»›n FL. Tá»« FS, chÃºng ta muá»‘n phÃ¡t triá»ƒn nÃ³ lá»›n hÆ¡n g láº§n Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c FL. Do Ä‘Ã³, háº§u háº¿t cÃ¡c lá»›p trong máº¡ng lá»›n yÃªu cáº§u g2 láº§n trá»ng sá»‘. Ngoáº¡i lá»‡ duy nháº¥t lÃ  lá»›p Ä‘áº§u tiÃªn vÃ  cuá»‘i cÃ¹ng, chá»‰ lá»›n hÆ¡n g láº§n vÃ¬ kÃ­ch thÆ°á»›c cá»§a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra khÃ´ng thay Ä‘á»•i. HÃ¬nh 2 minh há»a má»™t cÃ¡ch xáº¿p cho trá»ng sá»‘ cá»§a má»—i lá»›p vá»›i g= 2.

VÃ¬ cÃ¡c máº¡ng nhá» cá»§a chÃºng tÃ´i Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng trá»™n máº«u, chÃºng tÃ´i cÃ³ thá»ƒ táº¡o ra trá»ng sá»‘ má»›i báº±ng cÃ¡ch sá»­ dá»¥ng má»™t táº­p há»£p há»‡ sá»‘ má»›i (Î± trong PhÆ°Æ¡ng trÃ¬nh 1). Do Ä‘Ã³, náº¿u chÃºng ta cáº§n táº¡o ra g2 Ã´ trá»ng sá»‘ cho má»™t lá»›p, thÃ¬ chÃºng ta sáº½ há»c g2 há»‡ sá»‘ Ä‘áº·c trÆ°ng cho Ã´, nhÆ°ng cÃ¡c máº«u sáº½ Ä‘Æ°á»£c chia sáº» vá»›i cÃ¡c Ã´ khÃ¡c Ä‘Æ°á»£c táº¡o ra tá»« cÃ¹ng má»™t máº¡ng con nhá». Trong sá»‘ g2 há»‡ sá»‘ Ä‘áº·c trÆ°ng cho Ã´ nÃ y, chÃºng ta cÃ³ thá»ƒ táº¡o ra g2âˆ’2 Ã´ má»›i báº±ng cÃ¡ch tÃ¡i sá»­ dá»¥ng cÃ¡c máº¡ng con nhá» hiá»‡n cÃ³ FS1, FS2 cá»§a chÃºng tÃ´i, nhÆ°ng chÃºng ta pháº£i khá»Ÿi táº¡o cÃ¡c há»‡ sá»‘ má»›i. ChÃºng tÃ´i Ä‘iá»u tra nhiá»u chiáº¿n lÆ°á»£c Ä‘á»ƒ khá»Ÿi táº¡o cÃ¡c há»‡ sá»‘ nÃ y Ä‘Æ°á»£c mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y.
4

--- TRANG 5 ---
Há»‡ sá»‘ ngáº«u nhiÃªn Ä‘Æ¡n giáº£n khá»Ÿi táº¡o cÃ¡c há»‡ sá»‘ má»›i má»™t cÃ¡ch ngáº«u nhiÃªn tá»« phÃ¢n phá»‘i chuáº©n tiÃªu chuáº©n. Báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a cÃ¹ng nhau táº¥t cáº£ cÃ¡c há»‡ sá»‘ vÃ  máº«u Ä‘Æ°á»£c chia sáº», máº¡ng neural tÃ¬m ra cÃ¡c vÃ¹ng loss tháº¥p má»›i sau khi phÃ¡t triá»ƒn. Tuy nhiÃªn, váº«n khÃ´ng cháº¯c cháº¯n liá»‡u má»™t khá»Ÿi táº¡o ngáº«u nhiÃªn cÃ³ lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ tÃ­nh Ä‘a dáº¡ng cho viá»‡c phÃ¡t triá»ƒn máº¡ng hay khÃ´ng.

Sao chÃ©p há»‡ sá»‘ Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi Ã½ tÆ°á»Ÿng ráº±ng toÃ n bá»™ khÃ´ng gian con cá»§a trá»ng sá»‘ Ä‘Æ°á»£c bao trÃ¹m bá»Ÿi má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u khÃ´ng tÆ°Æ¡ng á»©ng vá»›i loss tháº¥p. Viá»‡c tÃ¬m cÃ¡c tá»• há»£p loss tháº¥p má»›i mÃ  khÃ´ng thá»±c hiá»‡n phÃ¢n tÃ­ch ká»¹ lÆ°á»¡ng lÃ  thÃ¡ch thá»©c, vÃ¬ váº­y thay vÃ o Ä‘Ã³ chÃºng tÃ´i sao chÃ©p cÃ¡c há»‡ sá»‘ cá»§a cÃ¡c máº¡ng con FS1, FS2 lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u ban Ä‘áº§u cho cÃ¡c trá»ng sá»‘ má»›i. CÃ¡c há»‡ sá»‘ Ä‘Æ°á»£c sao chÃ©p nÃ y khÃ´ng Ä‘Æ°á»£c chia sáº», vÃ¬ váº­y máº·c dÃ¹ chÃºng báº¯t Ä‘áº§u nhÆ° cÃ¡c giÃ¡ trá»‹ giá»‘ng nhau, chÃºng Ä‘Æ°á»£c thay Ä‘á»•i Ä‘á»™c láº­p trong khi huáº¥n luyá»‡n FL. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° cung cáº¥p má»™t khá»Ÿi táº¡o báº¯t Ä‘áº§u lÃ m tÄƒng pháº£n á»©ng cá»§a má»—i máº¡ng con g láº§n trong má»i lá»›p sau khi phÃ¡t triá»ƒn.

Há»‡ sá»‘ trá»±c giao sá»­ dá»¥ng cÃ¡c há»‡ sá»‘ trá»±c giao khi chia sáº» máº«u Ä‘á»ƒ khuyáº¿n khÃ­ch tÃ­nh Ä‘a dáº¡ng trong viá»‡c táº¡o ra trá»ng sá»‘, Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Savarese et al. [34]. NÃ³i cÃ¡ch khÃ¡c, khi má»—i táº­p há»£p há»‡ sá»‘ Î± cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t vector, táº¥t cáº£ cÃ¡c vector Î± nhÆ° váº­y Ä‘Æ°á»£c liÃªn káº¿t vá»›i cÃ¹ng má»™t máº«u Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘á»ƒ trá»±c giao theo [35].

4. ThÃ­ nghiá»‡m
Thiáº¿t láº­p ThÃ­ nghiá»‡m. ChÃºng tÃ´i giáº£ Ä‘á»‹nh má»™t máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ³ sáºµn Ä‘á»ƒ phÃ¡t triá»ƒn. Trong khi phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i báº¯t Ä‘áº§u vá»›i má»™t mÃ´ hÃ¬nh trá»™n máº«u nhá», cÃ¡c baseline sá»­ dá»¥ng cÃ¡c máº¡ng nhá» theo thiáº¿t láº­p riÃªng cá»§a chÃºng. Äá»ƒ so sÃ¡nh cÃ´ng báº±ng, táº¥t cáº£ cÃ¡c máº¡ng nhá» cÃ³ cÃ¹ng FLOPs (25% cá»§a máº¡ng má»¥c tiÃªu).

Chá»‰ sá»‘. ChÃºng tÃ´i xáº¿p háº¡ng cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i top-1 trung bÃ¬nh trÃªn ba láº§n cháº¡y trÃªn táº¥t cáº£ cÃ¡c táº­p dá»¯ liá»‡u. NgoÃ i ra, chÃºng tÃ´i bÃ¡o cÃ¡o sá»‘ lÆ°á»£ng phÃ©p toÃ¡n dáº¥u pháº©y Ä‘á»™ng (FLOPs) cáº§n thiáº¿t Ä‘á»ƒ huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh. Chá»‰ sá»‘ nÃ y so sÃ¡nh chi phÃ­ tÃ­nh toÃ¡n cho cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau báº¥t ká»ƒ pháº§n cá»©ng cá»§a chÃºng. Do Ä‘Ã³, nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a má»™t máº¡ng [13, 32, 40]. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i chuáº©n hÃ³a Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n qua cÃ¡c phÆ°Æ¡ng phÃ¡p báº±ng cÃ¡ch chá»‰ Ä‘á»‹nh má»™t mÃ´ hÃ¬nh huáº¥n luyá»‡n trong cÃ¹ng FLOPs Ä‘á»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng. ChÃºng tÃ´i sá»­ dá»¥ng thÆ° viá»‡n fvcore1 Ä‘á»ƒ tÃ­nh FLOPs.

Baseline. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p baseline sau.
â€¢Random khá»Ÿi táº¡o báº¥t ká»³ trá»ng sá»‘ má»›i nÃ o má»™t cÃ¡ch ngáº«u nhiÃªn trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn. Äiá»u nÃ y Ä‘á» cáº­p Ä‘áº¿n viá»‡c táº¡o ra trá»ng sá»‘ trá»±c tiáº¿p, thay vÃ¬ khá»Ÿi táº¡o ngáº«u nhiÃªn cÃ¡c há»‡ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c máº¡ng trá»™n máº«u (Ä‘Æ°á»£c mÃ´ táº£ trong Má»¥c 3.3).

â€¢Net2Net [7] dá»±a trÃªn khá»Ÿi táº¡o báº£o toÃ n hÃ m Ä‘á»ƒ khá»Ÿi táº¡o cÃ¡c trá»ng sá»‘ má»›i theo cÃ¡ch sao cho dá»± Ä‘oÃ¡n khÃ´ng thay Ä‘á»•i. Äiá»u nÃ y Ä‘áº£m báº£o mÃ´ hÃ¬nh má»›i Ä‘Æ°á»£c phÃ¡t triá»ƒn cÃ³ thá»ƒ cÃ³ cÃ¹ng kháº£ nÄƒng nhÆ° mÃ´ hÃ¬nh nhá» ngÄƒn cháº·n sá»± giáº£m hiá»‡u suáº¥t táº¡i Ä‘iá»ƒm phÃ¡t triá»ƒn.

1https://github.com/facebookresearch/fvcore

â€¢Firefly [46] phÃ¡t triá»ƒn má»™t máº¡ng báº±ng cÃ¡ch sá»­ dá»¥ng cáº¯t tá»‰a. Cá»¥ thá»ƒ, há» phÃ¡t triá»ƒn quÃ¡ má»©c máº¡ng, tá»‘i Æ°u hÃ³a nÃ³ trong vÃ i bÆ°á»›c, vÃ  sau Ä‘Ã³ cáº¯t tá»‰a máº¡ng báº±ng cÃ¡ch sá»­ dá»¥ng Xáº¥p xá»‰ Taylor cá»§a hÃ m loss Ä‘á»ƒ Æ°á»›c tÃ­nh Ä‘iá»ƒm quan trá»ng cá»§a cÃ¡c neuron má»›i. LÆ°u Ã½ ráº±ng cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i trÃ¡nh chi phÃ­ nÃ y báº±ng cÃ¡ch há»c cÃ¡ch táº¡o ra trá»ng sá»‘ má»›i báº±ng cÃ¡ch tÃ¡i sá»­ dá»¥ng cÃ¡c tham sá»‘ Ä‘Æ°á»£c chia sáº» cá»§a chÃºng tÃ´i, lÃ m cho bÆ°á»›c phÃ¡t triá»ƒn hiá»‡u quáº£ hÆ¡n Ä‘Ã¡ng ká»ƒ vá» máº·t tÃ­nh toÃ¡n.

â€¢GradMax [14] Ä‘á» xuáº¥t má»™t cÃ¡ch tiáº¿p cáº­n tÃ¬m cÃ¡ch tá»‘i Ä‘a hÃ³a gradient cá»§a cÃ¡c neuron má»›i. Äiá»u nÃ y Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi quan sÃ¡t ráº±ng khá»Ÿi táº¡o vá»›i gradient lá»›n dáº«n Ä‘áº¿n tiáº¿n bá»™ nhanh chÃ³ng trong bÃ i toÃ¡n há»c táº­p. GradMax tá»‘i Ä‘a hÃ³a gradient cá»§a loss Ä‘á»‘i vá»›i cÃ¡c tham sá»‘ má»›i. NÃ³ giáº£i quyáº¿t Ä‘iá»u nÃ y má»™t cÃ¡ch xáº¥p xá»‰ vá»›i SVD táº¡i thá»i Ä‘iá»ƒm phÃ¡t triá»ƒn, lÃ m cho bÆ°á»›c phÃ¡t triá»ƒn tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n khi so sÃ¡nh vá»›i cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i. ChÃºng tÃ´i sá»­ dá»¥ng mÃ£ cá»§a tÃ¡c giáº£ trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i2, mÃ  chÃºng tÃ´i cÅ©ng sá»­ dá»¥ng nhÆ° viá»‡c triá»ƒn khai cá»§a chÃºng tÃ´i cho cÃ¡c phÆ°Æ¡ng phÃ¡p Random vÃ  Firefly.

4.1. Táº­p dá»¯ liá»‡u vÃ  chi tiáº¿t triá»ƒn khai
Xem pháº§n bá»• sung cho cÃ¡c chi tiáº¿t khÃ´ng Ä‘Æ°á»£c mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y.

CIFAR-10 vÃ  CIFAR-100 [25] chá»©a 60K hÃ¬nh áº£nh cá»§a 10 vÃ  100 loáº¡i, tÆ°Æ¡ng á»©ng. Má»—i táº­p dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh 50K hÃ¬nh áº£nh Ä‘á»ƒ huáº¥n luyá»‡n vÃ  10K hÃ¬nh áº£nh Ä‘á»ƒ kiá»ƒm tra. ChÃºng tÃ´i sá»­ dá»¥ng kiáº¿n trÃºc WideResnet [49] Ä‘á»ƒ phÃ¡t triá»ƒn tá»« WRN-28-5 thÃ nh WRN-28-10. ChÃºng tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh trong 200 epoch trÃªn má»™t GPU duy nháº¥t sá»­ dá»¥ng stochastic gradient descent. Theo cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» phÃ¡t triá»ƒn máº¡ng [14], Batch Normalization [22] chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng trÆ°á»›c má»—i khá»‘i Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng.

ImageNet [10] chá»©a 1K loáº¡i vá»›i 1.2M hÃ¬nh áº£nh Ä‘á»ƒ huáº¥n luyá»‡n, 50K Ä‘á»ƒ xÃ¡c thá»±c, vÃ  100K Ä‘á»ƒ kiá»ƒm tra. ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ResNet-50 [17] trong 90 epoch vá»›i tá»‘c Ä‘á»™ há»c 0.1, giáº£m 0.1 táº¡i 30, 60, vÃ  80 epoch. ChÃºng tÃ´i sá»­ dá»¥ng stochastic gradient descent vá»›i momentum 0.9, kÃ­ch thÆ°á»›c batch 256 vá»›i cross entropy loss. TrÆ°á»›c khi phÃ¡t triá»ƒn, má»—i lá»›p cÃ³ má»™t ná»­a sá»‘ kÃªnh cá»§a máº¡ng cuá»‘i cÃ¹ng.

4.2. Káº¿t quáº£
Báº£ng 1 bÃ¡o cÃ¡o hiá»‡u suáº¥t phÃ¡t triá»ƒn má»™t máº¡ng neural trÃªn CIFAR-10, CIFAR-100, vÃ  ImageNet. ChÃºng tÃ´i tháº¥y ráº±ng MixtureGrowth cÃ³ thá»ƒ cÃ³ hiá»‡u suáº¥t tá»‘t hÆ¡n so vá»›i huáº¥n luyá»‡n kiáº¿n trÃºc má»¥c tiÃªu tá»« Ä‘áº§u (Máº¡ng má»¥c tiÃªu) vá»›i má»™t ná»­a FLOPs trÃªn táº­p dá»¯ liá»‡u CIFAR-100. Äá»‘i vá»›i ImageNet, cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i cÃ³ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Máº¡ng má»¥c tiÃªu chá»‰ sá»­ dá»¥ng má»™t pháº§n ba FLOPs. Khi xem xÃ©t Má»¥c tiÃªu á»Ÿ cÃ¹ng FLOPs, nÃ³ vÆ°á»£t trá»™i Large Match (tá»©c lÃ  Máº¡ng Má»¥c tiÃªu nhÆ°ng Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ¹ng FLOPs nhÆ° cÃ¡c baseline phÃ¡t triá»ƒn máº¡ng) báº±ng má»™t biÃªn Ä‘á»™ Ä‘Ã¡ng ká»ƒ. Do Ä‘Ã³, cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i cÃ³ kháº£ nÄƒng sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh nhá» hiá»‡n cÃ³ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh hiá»‡u suáº¥t cao trong thá»i gian Ã­t hÆ¡n so vá»›i má»™t máº¡ng tiÃªu chuáº©n.
2https://github.com/google-research/growneuron
5

--- TRANG 6 ---
PhÆ°Æ¡ng phÃ¡p CIFAR-10 [25] CIFAR-100 [25] FLOPs Norm ImageNet [10] FLOPs Norm
(a)Large (Máº¡ng má»¥c tiÃªu) 96.88% 81.39% 1.00X 76.49% 1.00X
Large Match 96.62% 80.95% 0.45X 73.85% 0.35X
Small 96.53% 80.29% 0.25X 72.48% 0.25X
Small Match 96.60% 80.30% 0.45X 72.89% 0.35X
(b)Random 95.61% 79.08% 0.45X 71.47% 0.35X
Net2Net [7] 95.47% 79.20% 0.45X 72.29% 0.35X
Firefly [46] 95.62% 78.90% 0.45X 71.30% 0.35X
GradMax [14] 95.73% 79.05% 0.45X 71.73% 0.35X
MixtureGrowth w/o fusion 96.05% 79.44% 0.45X 73.02% 0.35X
MixtureGrowth (cá»§a chÃºng tÃ´i) 96.66% 81.50% 0.45X 74.51% 0.35X

Báº£ng 1. So sÃ¡nh phÃ¡t triá»ƒn máº¡ng sá»­ dá»¥ng Ä‘á»™ chÃ­nh xÃ¡c top-1 trung bÃ¬nh trÃªn ba láº§n cháº¡y. (a)Hiá»‡u suáº¥t huáº¥n luyá»‡n kiáº¿n trÃºc máº¡ng má»¥c tiÃªu (sau phÃ¡t triá»ƒn) (Large) tá»« Ä‘áº§u (WRN-28-10 cho cáº£ hai táº­p dá»¯ liá»‡u CIFAR vÃ  ResNet-50 cho ImageNet), cÅ©ng nhÆ° hiá»‡u suáº¥t cá»§a kiáº¿n trÃºc nhá» (trÆ°á»›c phÃ¡t triá»ƒn) (Small) (WRN-28-5 cho cáº£ hai táº­p dá»¯ liá»‡u CIFAR vÃ  ResNet-50 vá»›i má»™t ná»­a sá»‘ kÃªnh nhÆ° máº¡ng lá»›n cho ImageNet); (b)Hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn máº¡ng, trong Ä‘Ã³ cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i SOTA hiá»‡n táº¡i.

0.30 0.35 0.40 0.45 0.50 0.55
FLOPs Norm798081Accuracy (%)
Gradmax
Fireï¬‚y
Random
Net2Net
MixtureGrowth (cá»§a chÃºng tÃ´i)
Máº¡ng Má»¥c tiÃªu
(a)CÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn theo FLOPs
0.26 0.30 0.36 0.40 0.45
FLOPs Norm74767880Accuracy (%)Orthogonal coef.
Random coef.
Coef. copying (b)Khá»Ÿi táº¡o há»‡ sá»‘

HÃ¬nh 3. Äá»™ chÃ­nh xÃ¡c Top-1 trÃªn CIFAR-100. (a)CÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn máº¡ng dÆ°á»›i FLOPs khÃ¡c nhau. ÄÆ°á»ng Ä‘á»©t nÃ©t biá»ƒu diá»…n hiá»‡u suáº¥t cá»§a Máº¡ng má»¥c tiÃªu Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u sá»­ dá»¥ng lá»‹ch trÃ¬nh huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§. ChÃºng tÃ´i tháº¥y MixtureGrowth khÃ´ng chá»‰ vÆ°á»£t trá»™i Random, FireFly [46], vÃ  GradMax [14], mÃ  cÃ²n vÆ°á»£t trá»™i máº¡ng má»¥c tiÃªu vá»›i má»™t ná»­a FLOPs; (b)Hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o há»‡ sá»‘ khÃ¡c nhau (Má»¥c 3.3) tá»« Ä‘iá»ƒm phÃ¡t triá»ƒn. Xem Má»¥c 4.3 Ä‘á»ƒ tháº£o luáº­n.

CÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» phÃ¡t triá»ƒn máº¡ng (Báº£ng 1b) gáº·p khÃ³ khÄƒn trong viá»‡c tÄƒng hiá»‡u suáº¥t sau khi phÃ¡t triá»ƒn khi so sÃ¡nh vá»›i viá»‡c Ä‘Æ¡n giáº£n huáº¥n luyá»‡n má»™t máº¡ng nhá» thÃ´ng thÆ°á»ng (Báº£ng 1a), kÃ½ hiá»‡u lÃ  Small vÃ  Small Match (tá»©c lÃ  máº¡ng Small nhÆ°ng Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ¹ng FLOPs nhÆ° cÃ¡c baseline phÃ¡t triá»ƒn máº¡ng). LÆ°u Ã½ ráº±ng so vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c, chÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡c kiáº¿n trÃºc máº¡ng lá»›n hÆ¡n trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i (vÃ­ dá»¥, ResNet-50 trÃªn ImageNet trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i so vá»›i VGG11 [37] Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trong [14]), cho tháº¥y cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c nÃ y gáº·p thÃ¡ch thá»©c trong viá»‡c tá»•ng quÃ¡t hÃ³a cho cÃ¡c máº¡ng lá»›n hÆ¡n. NgÆ°á»£c láº¡i, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ cho phÃ©p mÃ´ hÃ¬nh cÃ³ Ä‘Æ°á»£c sá»± cáº£i thiá»‡n sau khi phÃ¡t triá»ƒn vÃ  tháº­m chÃ­ vÆ°á»£t qua mÃ´ hÃ¬nh má»¥c tiÃªu trÃªn CIFAR-100 vá»›i má»™t ná»­a FLOPs. Khi so sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn khÃ¡c, cÃ¡ch tiáº¿p cáº­n MixtureGrowth cá»§a chÃºng tÃ´i cÃ³ Ä‘Æ°á»£c khoáº£ng 1% hiá»‡u suáº¥t cao hÆ¡n trÃªn CIFAR-10, 2.5% trÃªn CIFAR-100, vÃ  2% trÃªn ImageNet.

HÃ¬nh 3a chá»©ng minh hiá»‡u quáº£ vÃ  tÃ­nh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i so vá»›i bá»‘n baseline: Random [5], Net2Net [7], Firefly [46], vÃ  Gradmax [14] qua cÃ¡c ngÃ¢n sÃ¡ch FLOPs khÃ¡c nhau. Sá»­ dá»¥ng WRN-28-10 lÃ m máº¡ng má»¥c tiÃªu, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i liÃªn tá»¥c vÆ°á»£t trá»™i cÃ¡c baseline vá»›i cÃ¹ng ngÃ¢n sÃ¡ch trong khi ngang báº±ng hoáº·c tháº­m chÃ­ vÆ°á»£t trá»™i máº¡ng má»¥c tiÃªu vá»›i má»™t ná»­a FLOPs.

Báº£ng 3 so sÃ¡nh phÃ¡t triá»ƒn trong má»™t bÆ°á»›c duy nháº¥t sá»­ dá»¥ng MixtureGrowth vá»›i phÃ¡t triá»ƒn nhiá»u láº§n vá»›i lá»‹ch trÃ¬nh phÃ¡t triá»ƒn dÃ i Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c nghiÃªn cá»©u trÆ°á»›c [14]. Trong thiáº¿t láº­p nÃ y, chÃºng tÃ´i khÃ´ng chá»‰ cÃ³ Ä‘Æ°á»£c sá»± tÄƒng hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ so vá»›i hiá»‡n táº¡i tá»‘t nháº¥t, mÃ  cÃ²n lÃ m nhÆ° váº­y trong má»™t ná»­a FLOPs. CÃ¡c FLOPs bá»• sung cáº§n thiáº¿t bá»Ÿi cÃ¡c nghiÃªn cá»©u trÆ°á»›c lÃ  do chi phÃ­ phÃ¡t triá»ƒn cá»§a chÃºng (Ä‘Æ°á»£c tháº£o luáº­n á»Ÿ Ä‘áº§u Má»¥c 4).

4.3. Tháº£o luáº­n
ChÃºng ta nÃªn khá»Ÿi táº¡o cÃ¡c há»‡ sá»‘ tuyáº¿n tÃ­nh nhÆ° tháº¿ nÃ o? CÃ¢u há»i nghiÃªn cá»©u Ä‘áº§u tiÃªn mÃ  chÃºng tÃ´i khÃ¡m phÃ¡ lÃ  cÃ¡ch khá»Ÿi táº¡o hiá»‡u quáº£ cÃ¡c há»‡ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra trá»ng sá»‘ lá»›p má»›i sau
6

--- TRANG 7 ---
Thá»i gian Huáº¥n luyá»‡n Máº¡ng Thá»© hai (FLOPs)
Táº­p dá»¯ liá»‡u 0.0 0.4 0.5 0.6 0.7 0.8 0.9 1.0
CIFAR-100 79.24% 80.16% 80.46% 81.08% 80.9% 81.09% 81.44 % 81.27
ImageNet 73.02% 73.74% 73.75% 74.09% 74.16% 74.51 % 74.33% 74.31

Báº£ng 2. Ablation vá» cÃ¡c Ä‘iá»ƒm phÃ¡t triá»ƒn mÃ  chÃºng tÃ´i dá»«ng huáº¥n luyá»‡n máº¡ng thá»© hai Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§. Cá»™t Ä‘áº§u tiÃªn lÃ  trÆ°á»ng há»£p cá»±c Ä‘oan mÃ  chÃºng tÃ´i phÃ¡t triá»ƒn mÃ  khÃ´ng huáº¥n luyá»‡n máº¡ng thá»© hai (tá»©c lÃ  MixtureGrowth w/out fusion). Pháº§n cÃ²n láº¡i cho tháº¥y hiá»‡u suáº¥t cá»§a MixtureGrowth trÃªn cÃ¡c Ä‘iá»ƒm phÃ¡t triá»ƒn khÃ¡c nhau. Thá»i gian Huáº¥n luyá»‡n Máº¡ng Thá»© hai chá»‰ ra tá»· lá»‡ FLOPs huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n máº¡ng thá»© hai, vÃ­ dá»¥, 0.5 cÃ³ nghÄ©a lÃ  chÃºng tÃ´i huáº¥n luyá»‡n máº¡ng thá»© hai má»™t ná»­a huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ trÆ°á»›c khi phÃ¡t triá»ƒn. ChÃºng tÃ´i tháº¥y ráº±ng phÃ¡t triá»ƒn muá»™n mÃ  chÃºng tÃ´i gáº§n nhÆ° huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ máº¡ng thá»© hai trÆ°á»›c khi phÃ¡t triá»ƒn cÃ³ Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t nháº¥t (in Ä‘áº­m). Táº¥t cáº£ cÃ¡c láº§n cháº¡y bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c top-1 á»Ÿ cÃ¹ng 0.35X FLOPs.

khi phÃ¡t triá»ƒn. Trong Má»¥c 3.3 chÃºng tÃ´i mÃ´ táº£ ba phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ khá»Ÿi táº¡o cÃ¡c há»‡ sá»‘ má»›i nÃ y. ChÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t cá»§a chÃºng trong Báº£ng 4, nÆ¡i chÃºng tÃ´i tháº¥y khá»Ÿi táº¡o trá»±c giao, giÃºp thÃºc Ä‘áº©y tÃ­nh Ä‘a dáº¡ng nhiá»u nháº¥t trong cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c táº¡o ra, vÆ°á»£t trá»™i cáº£ khá»Ÿi táº¡o ngáº«u nhiÃªn vÃ  sao chÃ©p há»‡ sá»‘. ÄÃ¡ng chÃº Ã½, khá»Ÿi táº¡o ngáº«u nhiÃªn cÅ©ng hoáº¡t Ä‘á»™ng tá»‘t vÃ  tiáº¿p cáº­n hiá»‡u suáº¥t cá»§a khá»Ÿi táº¡o trá»±c giao. LÆ°u Ã½, Ä‘á»‘i vá»›i káº¿t quáº£ sao chÃ©p há»‡ sá»‘, chÃºng tÃ´i cÅ©ng Ä‘Ã£ thá»­ nghiá»‡m thÃªm má»™t Ã­t nhiá»…u ngáº«u nhiÃªn nhá» Ä‘á»ƒ phÃ¡ vá»¡ tÃ­nh Ä‘á»‘i xá»©ng, nhÆ°ng nÃ³ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n hiá»‡u suáº¥t.

Äá»ƒ cÃ³ cÃ¡i nhÃ¬n sÃ¢u sáº¯c vá» cÃ¡ch cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o há»‡ sá»‘ hoáº¡t Ä‘á»™ng táº¡i bÆ°á»›c phÃ¡t triá»ƒn, chÃºng tÃ´i váº½ Ä‘á»™ chÃ­nh xÃ¡c top-1 báº¯t Ä‘áº§u tá»« bÆ°á»›c phÃ¡t triá»ƒn cho Ä‘áº¿n cuá»‘i trong HÃ¬nh 3b. ChÃºng tÃ´i tháº¥y ráº±ng cáº£ trá»±c giao vÃ  ngáº«u nhiÃªn Ä‘á»u cÃ³ thá»ƒ duy trÃ¬ hiá»‡u suáº¥t nhiá»‡m vá»¥ máº·c dÃ¹ Ä‘Æ°á»£c cung cáº¥p trá»ng sá»‘ má»›i táº¡i thá»i Ä‘iá»ƒm phÃ¡t triá»ƒn vÃ  báº¯t Ä‘áº§u cáº£i thiá»‡n hiá»‡u suáº¥t vá»›i máº¡ng lá»›n. Tuy nhiÃªn, sao chÃ©p há»‡ sá»‘ gáº·p khÃ³ khÄƒn táº¡i bÆ°á»›c phÃ¡t triá»ƒn vÃ¬ hiá»‡u suáº¥t cá»§a nÃ³ giáº£m Ä‘Ã¡ng ká»ƒ. ChÃºng tÃ´i nghi ngá» Ä‘iá»u nÃ y cÃ³ thá»ƒ do sá»± dÆ° thá»«a gÃ¢y ra bá»Ÿi viá»‡c tÃ¡i sá»­ dá»¥ng vÃ  sao chÃ©p cÃ¹ng cÃ¡c há»‡ sá»‘, lÃ m cho viá»‡c há»c cÃ¡c trá»ng sá»‘ thÃ´ng tin má»›i trá»Ÿ nÃªn khÃ³ khÄƒn.

PhÃ¡t triá»ƒn báº±ng cÃ¡ch káº¿t há»£p hai mÃ´ hÃ¬nh nhá» cÃ³ hiá»‡u quáº£ hÆ¡n so vá»›i phÃ¡t triá»ƒn tá»« má»™t mÃ´ hÃ¬nh duy nháº¥t khÃ´ng? CÃ¢u há»i thá»© hai mÃ  chÃºng tÃ´i khÃ¡m phÃ¡ lÃ  liá»‡u chÃºng ta cÃ³ thá»ƒ táº­n dá»¥ng viá»‡c cÃ³ hai máº¡ng nhá» trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn hay khÃ´ng. CÃ³ má»™t cáº·p máº¡ng sáº½ cáº£i thiá»‡n tÃ­nh máº¡nh máº½ nhÆ° Ä‘Ã£ tháº£o luáº­n trong Má»¥c 3.2. Báº£ng 1 so sÃ¡nh phÃ¡t triá»ƒn cÃ³ vÃ  khÃ´ng cÃ³ káº¿t há»£p mÃ´ hÃ¬nh (2 hÃ ng cuá»‘i), nÆ¡i chÃºng tÃ´i tháº¥y ráº±ng káº¿t há»£p hai mÃ´ hÃ¬nh cÃ³ Ä‘Æ°á»£c sá»± tÄƒng nháº¥t quÃ¡n sau khi chuáº©n hÃ³a theo FLOPs.

Äiá»ƒm phÃ¹ há»£p trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘á»ƒ phÃ¡t triá»ƒn lÃ  gÃ¬? CÃ¢u há»i trÆ°á»›c lÃ m sÃ¡ng tá» táº§m quan trá»ng cá»§a viá»‡c cÃ³ má»™t mÃ´ hÃ¬nh thá»© hai Ä‘Æ°á»£c huáº¥n luyá»‡n. Tuy nhiÃªn, chÃºng ta nÃªn huáº¥n luyá»‡n máº¡ng thá»© hai trong bao lÃ¢u váº«n lÃ  cÃ¢u há»i má»Ÿ. Náº¿u chÃºng ta huáº¥n luyá»‡n máº¡ng thá»© hai trong má»™t lá»‹ch trÃ¬nh dÃ i, thÃ¬ khÃ´ng cÃ²n nhiá»u ngÃ¢n sÃ¡ch Ä‘á»ƒ tá»‘i Æ°u hÃ³a cÃ¡c tham sá»‘ cá»§a máº¡ng lá»›n. NgÆ°á»£c láº¡i, náº¿u chÃºng ta phÃ¡t triá»ƒn nÃ³ quÃ¡ sá»›m, thÃ¬ máº¡ng thá»© hai khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§, tá»« Ä‘Ã³ cÃ³ thá»ƒ dáº«n Ä‘áº¿n sá»± giáº£m hiá»‡u suáº¥t tá»•ng thá»ƒ. Tá»« gÃ³c Ä‘á»™ nÃ y, chÃºng ta cÃ³ thá»ƒ coi MixtureGrowth w/out fusion nhÆ° má»™t trÆ°á»ng há»£p cá»±c Ä‘oan cá»§a phÃ¡t triá»ƒn sá»›m mÃ  chÃºng ta khÃ´ng huáº¥n luyá»‡n máº¡ng thá»© hai mÃ  thay vÃ o Ä‘Ã³ phÃ¡t triá»ƒn

081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0(a)
081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0 (b)
081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0
(c)
081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0 (d)

HÃ¬nh 4. CKA cá»§a cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau táº¡i lá»›p tÃ­ch cháº­p cuá»‘i cÃ¹ng theo thá»i gian. (a) Máº¡ng Má»¥c tiÃªu: Má»™t máº«u mÆ°á»£t mÃ  trá»Ÿ nÃªn sÃ¡ng hÆ¡n dá»c theo Ä‘Æ°á»ng chÃ©o vá» phÃ­a cuá»‘i huáº¥n luyá»‡n khi huáº¥n luyá»‡n tá»« Ä‘áº§u; (b)Random: Máº«u tá»‘i vÃ  nhiá»…u hÆ¡n so vá»›i má»¥c tiÃªu khi phÃ¡t triá»ƒn vá»›i trá»ng sá»‘ ngáº«u nhiÃªn; (c)MixtureGrowth w/out fusion: KhÃ´ng cÃ³ káº¿t há»£p mÃ´ hÃ¬nh, máº«u cÃ³ pháº§n tÆ°Æ¡ng tá»± nhÆ° Má»¥c tiÃªu, nhÆ°ng cÃ¡c biá»ƒu diá»…n giá»¯a cÃ¡c epoch cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng nhá» hÆ¡n; (d)MixtureGrowth: Trong mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ cá»§a chÃºng tÃ´i, chÃºng tÃ´i tháº¥y má»™t sá»± thay Ä‘á»•i pha rÃµ rÃ ng táº¡i Ä‘iá»ƒm phÃ¡t triá»ƒn (epoch 63), gá»£i Ã½ viá»‡c há»c nhanh chÃ³ng xáº£y ra sau khi káº¿t há»£p 2 mÃ´ hÃ¬nh nhá».

thÃ nh má»™t máº¡ng Ä‘áº§y Ä‘á»§ trá»±c tiáº¿p. Báº£ng 2 cho tháº¥y hiá»‡u suáº¥t cá»§a cÃ¡c Ä‘iá»ƒm phÃ¡t triá»ƒn khÃ¡c nhau dÆ°á»›i cÃ¹ng ngÃ¢n sÃ¡ch FLOPs trÃªn cÃ¡c táº­p dá»¯ liá»‡u CIFAR-100 vÃ  ImageNet. ChÃºng tÃ´i quan sÃ¡t ráº±ng phÃ¡t triá»ƒn muá»™n mÃ  máº¡ng thá»© hai Ä‘Æ°á»£c huáº¥n luyá»‡n gáº§n nhÆ° Ä‘áº§y Ä‘á»§ cho hiá»‡u suáº¥t tá»‘t nháº¥t. Äiá»u nÃ y xÃ¡c nháº­n giáº£ Ä‘á»‹nh cá»§a chÃºng tÃ´i ráº±ng phÃ¡t triá»ƒn vá»›i hai máº¡ng máº¡nh máº½ hÆ¡n.

4.4. PhÃ¢n tÃ­ch Ä‘áº·c trÆ°ng
ChÃºng tÃ´i sá»­ dá»¥ng Centered Kernel Alignment (CKA) [24] Ä‘á»ƒ phÃ¢n tÃ­ch sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng qua quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Äá»ƒ phÃ¢n tÃ­ch nÃ y, chÃºng tÃ´i thiáº¿t láº­p MixtureGrowth báº±ng cÃ¡ch huáº¥n luyá»‡n 2 máº¡ng nhá» trong 63 epoch, sau Ä‘Ã³ huáº¥n luyá»‡n máº¡ng Ä‘Ã£ phÃ¡t triá»ƒn Ä‘áº§y Ä‘á»§ sau Ä‘Ã³ cho Ä‘áº¿n khi Ä‘áº¡t 100 epoch tá»•ng cá»™ng. Äá»‘i vá»›i Mixture-
7

--- TRANG 8 ---
PhÆ°Æ¡ng phÃ¡p Top-1 Acc. FLOPs Norm
(a) Random 79.12% 0.77X
Firefly [46] 78.94% 0.77X
GradMax [14] 79.24% 0.77X
(b) MixtureGrowth 81.50% 0.45X

Báº£ng 3. So sÃ¡nh phÃ¡t triá»ƒn máº¡ng trÃªn CIFAR-100 .(a)Sá»­ dá»¥ng lá»‹ch trÃ¬nh phÃ¡t triá»ƒn dÃ i Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c tÃ¡c giáº£ cá»§a GradMax; (b)MixtureGrowth vá»›i phÃ¡t triá»ƒn Ä‘Æ¡n Ä‘á»ƒ tham kháº£o.

PhÆ°Æ¡ng phÃ¡p Top-1 Accuracy
Random coefficients 81.41%
Coefficient copying 79.05%
Orthogonal coefficients 81.50 %

Báº£ng 4. Ablation vá» cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o há»‡ sá»‘ (Ä‘Æ°á»£c mÃ´ táº£ trong Má»¥c 3.3) cho MixtureGrowth trÃªn CIFAR-100. Khá»Ÿi táº¡o trá»±c giao hoáº¡t Ä‘á»™ng tá»‘t nháº¥t, gá»£i Ã½ táº§m quan trá»ng cá»§a viá»‡c cÃ³ cÃ¡c trá»ng sá»‘ Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘á»™c láº­p.

0246810121416182022242628
Small Net 1's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net 2's layer
0.00.20.40.60.81.0
(a)
0246810121416182022242628
Large Net's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net's layer
0.00.20.40.60.81.0 (b)
0246810121416182022242628
Large Net's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net 1's layer
0.00.20.40.60.81.0
(c)
0246810121416182022242628
Large Net's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net 2's layer
0.00.20.40.60.81.0 (d)

HÃ¬nh 5. CKA giá»¯a cÃ¡c lá»›p cá»§a MixtureGrowth. (a) MixtureGrowth: Máº¡ng Nhá» 1 so vá»›i Máº¡ng Nhá» 2; (b)MixtureGrowth w/o model fusion: Máº¡ng Lá»›n so vá»›i Máº¡ng Nhá» 1; (c)MixtureGrowth vá»›i model fusion: Máº¡ng Lá»›n so vá»›i Máº¡ng Nhá» 1; (d)MixtureGrowth vá»›i model fusion: Máº¡ng Lá»›n so vá»›i Máº¡ng Nhá» 2; ChÃºng tÃ´i phÃ¢n tÃ­ch sá»± tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c biá»ƒu diá»…n cho má»—i cáº·p lá»›p cá»§a WRN-28-10 trÃªn táº­p dá»¯ liá»‡u CIFAR-100. Xem Má»¥c 4.4 Ä‘á»ƒ tháº£o luáº­n.

Growth w/out fusion, chá»‰ má»™t mÃ´ hÃ¬nh nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n trong 63 epoch, vÃ  sau Ä‘Ã³ chÃºng tÃ´i phÃ¡t triá»ƒn. Máº¡ng má»¥c tiÃªu Ä‘á» cáº­p Ä‘áº¿n má»™t máº¡ng Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u trong 100 epoch.

HÃ¬nh 4 cho tháº¥y cÃ¡c Ä‘áº·c trÆ°ng phÃ¡t triá»ƒn nhÆ° tháº¿ nÃ o trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n theo thá»i gian sá»­ dá»¥ng quy trÃ¬nh CKA Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ trÃªn. Trong HÃ¬nh 4d, minh há»a sá»± tÆ°Æ¡ng Ä‘á»“ng Ä‘áº·c trÆ°ng, má»™t sá»± thay Ä‘á»•i pha ráº¥t rÃµ rÃ ng trong Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cÃ³ thá»ƒ nhÃ¬n tháº¥y táº¡i bÆ°á»›c phÃ¡t triá»ƒn á»Ÿ epoch 63. Äiá»u nÃ y chá»‰ ra ráº±ng viá»‡c há»c táº¡i Ä‘iá»ƒm phÃ¡t triá»ƒn ráº¥t nhanh, gá»£i Ã½ chÃºng tÃ´i hiá»‡u quáº£ káº¿t há»£p hai mÃ´ hÃ¬nh Ä‘á»™c láº­p trong bÆ°á»›c phÃ¡t triá»ƒn. Khá»Ÿi táº¡o trá»ng sá»‘ ngáº«u nhiÃªn (HÃ¬nh 4b) pháº£n Ã¡nh má»™t sá»± thay Ä‘á»•i tÆ°Æ¡ng tá»± Ä‘Æ°á»£c tháº¥y trong HÃ¬nh 4d, nhÆ°ng vá»›i nhiá»u máº«u nhiá»…u hÆ¡n. NgÆ°á»£c láº¡i, huáº¥n luyá»‡n máº¡ng má»¥c tiÃªu tá»« Ä‘áº§u (HÃ¬nh 4a), vÃ  MixtureGrowth w/out fusion (HÃ¬nh 4c) cÃ³ má»™t sá»± chuyá»ƒn Ä‘á»•i dáº§n dáº§n hÆ¡n tá»« Ä‘áº§u huáº¥n luyá»‡n vá» phÃ­a cuá»‘i mÃ  khÃ´ng cÃ³ cÃ¡c gÃ³c pháº§n tÆ° rÃµ rÃ ng nhÆ° MixtureGrowth. Do Ä‘Ã³, viá»‡c há»c trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn khÃ´ng nhanh chÃ³ng, mÃ  dáº§n dáº§n thay vÃ o Ä‘Ã³. Äiá»u nÃ y cung cáº¥p cÃ¡i nhÃ¬n sÃ¢u sáº¯c vá» tÃ­nh hiá»‡u quáº£ cá»§a cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i, Ä‘áº·c biá»‡t vá»›i ngÃ¢n sÃ¡ch FLOP tháº¥p.

HÃ¬nh 5 minh há»a cÃ¡ch cÃ¡c máº¡ng Ä‘Ã£ thay Ä‘á»•i tá»« Ä‘iá»ƒm phÃ¡t triá»ƒn Ä‘áº¿n biá»ƒu diá»…n cuá»‘i cÃ¹ng vá»›i CKA. Trong HÃ¬nh 5a, chÃºng tÃ´i so sÃ¡nh hai máº¡ng nhá» ngay trÆ°á»›c khi phÃ¡t triá»ƒn. ChÃºng tÃ´i quan sÃ¡t ráº±ng má»™t sá»‘ lá»›p qua cÃ¡c máº¡ng cÃ³ cÃ¡c Ä‘áº·c trÆ°ng tÆ°Æ¡ng tá»± (mÃ u sÃ¡ng trÃªn Ä‘Æ°á»ng chÃ©o, Ä‘áº¡i diá»‡n cho cÃ¡c Ä‘áº·c trÆ°ng ráº¥t tÆ°Æ¡ng tá»± tá»« cÃ¹ng má»™t lá»›p), vÃ  má»™t sá»‘ lá»›p Ä‘áº§u vÃ  cuá»‘i khÃ¡c nhau (mÃ u tá»‘i). ChÃºng tÃ´i tháº¥y má»™t máº«u tÆ°Æ¡ng tá»± phÃ¡t triá»ƒn tá»« má»™t mÃ´ hÃ¬nh duy nháº¥t (tá»©c lÃ , MixtureGrowth w/out fusion, HÃ¬nh 5b). Äiá»u nÃ y gá»£i Ã½ ráº±ng sau khi phÃ¡t triá»ƒn (w/o fusion), máº¡ng lá»›n há»c cÃ¡c Ä‘áº·c trÆ°ng bá»• sung khÃ¡c vá»›i máº¡ng nhá» theo cÃ¡ch tÆ°Æ¡ng tá»± nhÆ° hai máº¡ng nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p. NgÆ°á»£c láº¡i, trong MixtureGrowth w/fusion, tá»« HÃ¬nh 5c vÃ  5d, chÃºng tÃ´i tháº¥y máº¡ng Ä‘áº§y Ä‘á»§ chia sáº» nhiá»u sá»± tÆ°Æ¡ng Ä‘á»“ng hÆ¡n vá»›i má»—i máº¡ng nhá» trong cÃ¡c lá»›p Ä‘áº§u vÃ  cuá»‘i so vá»›i MixtureGrowth w/out fusion. Do Ä‘Ã³, Ã­t viá»‡c há»c xáº£y ra sau khi káº¿t há»£p hai máº¡ng nhá», vÃ  huáº¥n luyá»‡n nháº¹ sau khi phÃ¡t triá»ƒn lÃ  Ä‘á»§.

5. Káº¿t luáº­n
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t MixtureGrowth, má»™t phÆ°Æ¡ng phÃ¡p má»›i Ä‘á»ƒ phÃ¡t triá»ƒn máº¡ng vá»›i trá»ng sá»‘ Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« cÃ¡c tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c máº«u Ä‘Æ°á»£c chia sáº» sá»­ dá»¥ng káº¿t há»£p mÃ´ hÃ¬nh Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng trong cÃ¡c máº«u Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cá»§a chÃºng tÃ´i. ChÃºng tÃ´i phÃ¢n tÃ­ch cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i báº±ng cÃ¡ch tráº£ lá»i má»™t sá»‘ cÃ¢u há»i vá» tÃ¡c Ä‘á»™ng cá»§a khá»Ÿi táº¡o há»‡ sá»‘ tuyáº¿n tÃ­nh vÃ  tÃ¬m má»™t Ä‘iá»ƒm phÃ¡t triá»ƒn tá»‘t. ChÃºng tÃ´i tháº¥y ráº±ng khÃ´ng cÃ³ káº¿t há»£p mÃ´ hÃ¬nh, MixtureGrowth Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c cao hÆ¡n so vá»›i hiá»‡n táº¡i tá»‘t nháº¥t. Tuy nhiÃªn, vá»›i káº¿t há»£p, lá»£i Ã­ch so vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c tÄƒng lÃªn, dáº«n Ä‘áº¿n cáº£i thiá»‡n 2-2.5% so vá»›i hiá»‡n táº¡i tá»‘t nháº¥t trÃªn CIFAR-100 vÃ  ImageNet. ChÃºng tÃ´i tin ráº±ng MixtureGrowth Ä‘áº¡i diá»‡n cho má»™t bÆ°á»›c tiáº¿n thÃº vá»‹ trong viá»‡c há»c phÃ¡t triá»ƒn máº¡ng báº±ng cÃ¡ch táº­n dá»¥ng trá»™n máº«u Ä‘á»ƒ táº¡o ra trá»ng sá»‘ má»›i.

Lá»i cáº£m Æ¡n TÃ i liá»‡u nÃ y dá»±a trÃªn cÃ´ng viá»‡c Ä‘Æ°á»£c há»— trá»£, má»™t pháº§n, bá»Ÿi DARPA dÆ°á»›i sá»‘ thá»a thuáº­n HR00112020054 vÃ  NSF dÆ°á»›i giáº£i thÆ°á»Ÿng DBI-2134696. Báº¥t ká»³ Ã½ kiáº¿n, phÃ¡t hiá»‡n, vÃ  káº¿t luáº­n hoáº·c khuyáº¿n nghá»‹ nÃ o Ä‘á»u cá»§a (cÃ¡c) tÃ¡c giáº£ vÃ  khÃ´ng nháº¥t thiáº¿t pháº£n Ã¡nh quan Ä‘iá»ƒm cá»§a cÃ¡c cÆ¡ quan há»— trá»£.
8

--- TRANG 9 ---
TÃ i liá»‡u tham kháº£o
[1] Timur Ash. Dynamic node creation in backpropagation networks. Connection science , 1(4):365â€“375, 1989. 2
[2] Hessam Bagherinezhad, Mohammad Rastegari, and Ali Farhadi. Lcnn: Lookup-based convolutional neural network. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2017. 1, 3, 4
[3] Pouya Bashivan, Mark Tensen, and James J DiCarlo. Teacher guided architecture search. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5320â€“5329, 2019. 1
[4] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. Advances in neural information processing systems , 19, 2006. 2
[5] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysÅ‚aw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680 , 2019. 6
[6] Lucas Beyer, Xiaohua Zhai, Am Â´elie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10925â€“10934, 2022. 3
[7] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641 , 2015. 2, 5, 6
[8] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning , pages 160â€“167, 2008. 2
[9] Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. Nest: A neural network synthesis tool based on a grow-and-prune paradigm. IEEE Transactions on Computers , 68(10):1487â€“1497, 2019. 2
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248â€“255. Ieee, 2009. 5, 6, 11
[11] Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems , pages 1â€“15. Springer, 2000. 4
[12] Xuanyi Dong and Yi Yang. One-shot neural architecture search via self-evaluated template network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3681â€“3690, 2019. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. 5
[14] Utku Evci, Max Vladymyrov, Thomas Unterthiner, Bart van Merri Â¨enboer, and Fabian Pedregosa. Gradmax: Growing neural networks using gradient information. arXiv preprint arXiv:2201.05125 , 2022. 1, 2, 3, 4, 5, 6, 8, 11, 12
[15] Scott Fahlman and Christian Lebiere. The cascade-correlation learning architecture. Advances in neural information processing systems , 2, 1989. 2
[16] J Gou, B Yu, SJ Maybank, and D Tao. Knowledge distillation: A survey. corr. arXiv preprint arXiv:2006.05525 , 2020. 1
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016. 5, 11
[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2(7), 2015. 1, 3
[19] Geoffrey E Hinton. Deep belief networks. Scholarpedia , 4(5):5947, 2009. 2
[20] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation , 18(7):1527â€“1554, 2006. 2
[21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res. , 22(241):1â€“124, 2021. 1
[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML) , 2015. 5, 11
[23] Manas R Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K Adams, Pranav Khaitan, Jiahui Liu, and Quoc V Le. Neural input search for large scale recommendation models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 2387â€“2397, 2020. 1
[24] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.. Similarity of neural network representations revisited. In International Conference on Machine Learning , pages 3519â€“3529. PMLR, 2019. 7
[25] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 5, 6, 11
[26] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems , 30, 2017. 4
[27] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems , 2, 1989. 3
[28] Kaitlin Maile, Emmanuel Rachelson, Herv Â´e Luga, and Dennis George Wilson. When, where, and how to add new neurons to anns. In International Conference on Automated Machine Learning , pages 18â€“1. PMLR, 2022. 2
[29] Elliot Meyerson and Risto Miikkulainen. Beyond shared hierarchies: Deep multitask learning through soft layer ordering. In International Conference on Learning Representations , 2018. 3
[30] Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli Â´c, and Edoardo Maria Ponti. Modular deep learning. arXiv preprint arXiv:2302.11529 , 2023. 3
9

--- TRANG 10 ---
[31] Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, and Kate Saenko. Neural parameter allocation search. In International Conference on Learning Representations (ICLR) , 2022. 1, 3, 4
[32] Adria Ruiz and Jakob Verbeek. Anytime inference with distilled hierarchical neural ensembles. In Proceedings of the AAAI Conference on Artificial Intelligence , 2021. 5
[33] Mark Sandler, Jonathan Baccash, Andrey Zhmoginov, and Andrew Howard. Non-discriminative data or weak model? on the relative importance of data and model resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops , pages 0â€“0, 2019. 3
[34] Pedro Savarese and Michael Maire. Learning implicitly recurrent CNNs through parameter sharing. In International Conference on Learning Representations , 2019. 1, 3, 4, 5
[35] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013. 5
[36] Zhiqiang Shen and Marios Savvides. Meal v2: Boosting vanilla resnet-50 to 80 arXiv preprint arXiv:2009.08453 , 2020. 3
[37] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. 6, 12
[38] Nikko Str Â¨om. Sparse connection and pruning in large dynamic artificial neural networks. In Fifth European Conference on Speech Communication and Technology , 1997. 3
[39] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2820â€“2828, 2019. 1
[40] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning , pages 6105â€“6114. PMLR, 2019. 5
[41] Piotr Teterwak, Soren Nelson, Nikoli Dryden, Dina Bashkirova, Kate Saenko, and Bryan A. Plummer. Learning to compose superweights for neural parameter allocation search. In IEEE Winter Conference on Applications of Computer Vision (WACV) , 2024. 1, 3, 4
[42] Georg Thimm and Emile Fiesler. Evaluating pruning methods. In Proceedings of the International Symposium on Artificial neural networks , pages 20â€“25, 1995. 3
[43] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L Â´eon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research , 11(12), 2010. 2
[44] Huan Wang, Can Qin, Yue Bai, Yulun Zhang, and Yun Fu. Recent advances on neural network pruning at initialization. arXiv e-prints , pages arXivâ€“2103, 2021. 1
[45] Wei Wen, Feng Yan, Yiran Chen, and Hai Li. Autogrow: Automatic layer growing in deep convolutional networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 833â€“841, 2020. 2
[46] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks. Advances in Neural Information Processing Systems , 33:22373â€“22383, 2020. 1, 2, 3, 4, 5, 6, 8
[47] Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural architectures. Advances in neural information processing systems , 32, 2019. 2
[48] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. In Advances in Neural Information Processing Systems , 2019. 1, 3, 4
[49] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016 . British Machine Vision Association, 2016. 5, 11, 12
[50] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. InEuropean conference on computer vision , pages 94â€“108. Springer, 2014. 2
[51] Andrey Zhmoginov, Dina Bashkirova, and Mark Sandler. Compositional models: Multi-task learning and knowledge transfer with modular networks. arXiv preprint arXiv:2107.10963 , 2021. 3
10

--- TRANG 11 ---
Phá»¥ lá»¥c
6. Chi tiáº¿t Triá»ƒn khai
ChÃºng tÃ´i thiáº¿t láº­p cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trÃªn cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i hÃ¬nh áº£nh trong Ä‘Ã³ má»¥c tiÃªu lÃ  nháº­n biáº¿t Ä‘á»‘i tÆ°á»£ng trong hÃ¬nh áº£nh. Äiá»u nÃ y Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng Ä‘á»™ chÃ­nh xÃ¡c top 1, tá»©c lÃ  tá»· lá»‡ pháº§n trÄƒm cÃ¡c láº§n mÃ´ hÃ¬nh cÃ³ thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c loáº¡i cá»§a hÃ¬nh áº£nh. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÃ  cÃ¡c baseline trÃªn CIFAR-10 vÃ  CIFAR100 [25], bao gá»“m 60K hÃ¬nh áº£nh cá»§a 10 vÃ  100 loáº¡i tÆ°Æ¡ng á»©ng, vÃ  ImageNet [10], chá»©a 1.2M hÃ¬nh áº£nh cá»§a 1,000 loáº¡i.

6.1. CIFAR-10 vÃ  CIFAR-100
CIFAR-10 vÃ  CIFAR100 [25] bao gá»“m 60K hÃ¬nh áº£nh cá»§a 10 vÃ  100 loáº¡i tÆ°Æ¡ng á»©ng. Trong cáº£ hai táº­p dá»¯ liá»‡u, chÃºng tÃ´i chia dá»¯ liá»‡u thÃ nh 50K hÃ¬nh áº£nh Ä‘á»ƒ huáº¥n luyá»‡n vÃ  10K hÃ¬nh áº£nh Ä‘á»ƒ kiá»ƒm tra. ChÃºng tÃ´i thá»±c hiá»‡n thÃ­ nghiá»‡m sá»­ dá»¥ng kiáº¿n trÃºc Wide Residual Network (WRN) [49], lÃ  phiÃªn báº£n sá»­a Ä‘á»•i cá»§a máº¡ng residual [17]. ChÃºng tÃ´i kÃ½ hiá»‡u mÃ´ hÃ¬nh WRN nhÆ° WRN-n-k, trong Ä‘Ã³ n lÃ  tá»•ng sá»‘ lá»›p tÃ­ch cháº­p, vÃ  k lÃ  há»‡ sá»‘ má»Ÿ rá»™ng. WRN tÄƒng chiá»u rá»™ng cá»§a má»—i lá»›p báº±ng há»‡ sá»‘ k trong khi giáº£m Ä‘á»™ sÃ¢u Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a máº¡ng residual truyá»n thá»‘ng. Trong thÃ­ nghiá»‡m nÃ y, chÃºng tÃ´i chá»n WRN-28-10 Ä‘Æ°á»£c Ä‘iá»u chá»‰nh tá»« Savarese et al.3. Máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n trong tá»•ng cá»™ng 200 epoch trÃªn má»™t GPU duy nháº¥t (Nvidia Titan V 12G) sá»­ dá»¥ng stochastic gradient descent vá»›i momentum 0.9, tá»‘c Ä‘á»™ há»c 0.1, Ä‘Æ°á»£c giáº£m xuá»‘ng 0 vá»›i lá»‹ch trÃ¬nh cosine, weight decay 5Ã—10âˆ’4, vÃ  kÃ­ch thÆ°á»›c batch 128. Äá»‘i vá»›i hÃ m loss, chÃºng tÃ´i sá»­ dá»¥ng cross entropy loss. Äá»ƒ cÃ³ so sÃ¡nh cÃ´ng báº±ng, chÃºng tÃ´i theo cÃ¹ng thiáº¿t láº­p trong nghiÃªn cá»©u trÆ°á»›c [14], trong Ä‘Ã³ Batch Normalization [22] chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng trÆ°á»›c má»—i khá»‘i. CÃ¡c tham sá»‘ cá»¥ thá»ƒ cho phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c Ä‘áº·t nhÆ° sau. Tá»•ng ngÃ¢n sÃ¡ch tham sá»‘ cho trá»™n máº«u Ä‘Æ°á»£c Ä‘áº·t lÃ  36.5M, báº±ng vá»›i sá»‘ lÆ°á»£ng tham sá»‘ trong WRN-28-10 (máº¡ng má»¥c tiÃªu). LÆ°u Ã½ ráº±ng ngÃ¢n sÃ¡ch tham sá»‘ trong phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ linh hoáº¡t, tá»©c lÃ , nÃ³ cÃ³ thá»ƒ cÃ³ thiáº¿t láº­p mÃ  mÃ´ hÃ¬nh cÃ³ Ã­t hoáº·c nhiá»u tham sá»‘ hÆ¡n nhá» vÃ o cÃ¡c sÆ¡ Ä‘á»“ trá»™n máº«u, xem Má»¥c 8 Ä‘á»ƒ so sÃ¡nh cÃ¡c ngÃ¢n sÃ¡ch tham sá»‘ khÃ¡c nhau. Äá»‘i vá»›i má»—i lá»›p, sá»‘ lÆ°á»£ng máº«u Ä‘Æ°á»£c Ä‘áº·t lÃ  2. Ngoáº¡i trá»« lá»›p Ä‘áº§u tiÃªn (conv 1) vÃ  cuá»‘i cÃ¹ng (Fully connected), táº¥t cáº£ cÃ¡c há»‡ sá»‘ alpha Ä‘á»u cÃ³ thá»ƒ huáº¥n luyá»‡n. Äá»ƒ huáº¥n luyá»‡n MixtureGrowth, cho má»™t mÃ´ hÃ¬nh WRN-28-5 Ä‘Æ°á»£c huáº¥n luyá»‡n, chÃºng tÃ´i báº¯t Ä‘áº§u báº±ng viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh WRN-28-5 khÃ¡c trong e epoch, trong Ä‘Ã³ 0â‰¤e < 200 lÃ  má»™t siÃªu tham sá»‘. Sau Ä‘Ã³, cÃ¡c mÃ´ hÃ¬nh nhá» nÃ y Ä‘Æ°á»£c káº¿t há»£p vÃ  sá»­ dá»¥ng Ä‘á»ƒ khá»Ÿi táº¡o cho viá»‡c phÃ¡t triá»ƒn thÃ nh mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ (tá»©c lÃ , WRN-28-10), lá»›n hÆ¡n 4 láº§n vá» sá»‘ lÆ°á»£ng trá»ng sá»‘. ChÃºng tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ thÃªm má»™t sá»‘ epoch, tÃ¹y thuá»™c vÃ o ngÃ¢n sÃ¡ch FLOPs.

3https://github.com/lolemacs/soft-sharing

ÄÃ¡ng chÃº Ã½ ráº±ng GradMax [14] thu nhá» lá»›p tÃ­ch cháº­p Ä‘áº§u tiÃªn táº¡i má»—i khá»‘i xuá»‘ng 4, dáº«n Ä‘áº¿n cÃ³ má»™t máº¡ng nhá» vá»›i cÃ¡c lá»›p rá»™ng vÃ  háº¹p xen káº½. NgÆ°á»£c láº¡i, MixtureGrowth giáº£m cáº£ chiá»u rá»™ng Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra trong má»—i lá»›p xuá»‘ng 2 Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c phiÃªn báº£n nhá» hÆ¡n cá»§a máº¡ng. Máº·c dÃ¹ cÃ¡c máº¡ng nhá» trong phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vÃ  cÃ¡c baseline hÆ¡i khÃ¡c nhau do thiáº¿t láº­p cá»§a má»—i phÆ°Æ¡ng phÃ¡p, táº¥t cáº£ chÃºng Ä‘á»u cÃ³ cÃ¹ng FLOPs (khoáº£ng 0.25X FLOPs Norm) Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng.

6.2. ImageNet
ImageNet [10] chá»©a 1,000 loáº¡i vá»›i 1.2M hÃ¬nh áº£nh Ä‘á»ƒ huáº¥n luyá»‡n, 50K Ä‘á»ƒ xÃ¡c thá»±c, vÃ  100K Ä‘á»ƒ kiá»ƒm tra. ChÃºng tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh sá»­ dá»¥ng kiáº¿n trÃºc ResNet-50 [17] trong 90 epoch trÃªn 4 GPU (NVIDIA RTX A6000 48G) vá»›i tá»‘c Ä‘á»™ há»c 0.1, Ä‘Æ°á»£c giáº£m 0.1 táº¡i 30, 60, vÃ  80 epoch vá»›i bá»™ láº­p lá»‹ch cosine. ChÃºng tÃ´i sá»­ dá»¥ng stochastic gradient descent vá»›i momentum 0.9, kÃ­ch thÆ°á»›c batch 256, vÃ  cross entropy lÃ m hÃ m loss. Trong MixtureGrowth, chÃºng tÃ´i chia sáº» máº«u giá»¯a hai lá»›p liÃªn tiáº¿p náº¿u chÃºng cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c. Tá»•ng tham sá»‘ cho trá»™n máº«u trong phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i lÃ  25.6M, báº±ng vá»›i mÃ´ hÃ¬nh má»¥c tiÃªu. ÄÃ¡ng chÃº Ã½ ráº±ng trong GradMax [14], máº¡ng nhá» mÃ  há» sá»­ dá»¥ng yÃªu cáº§u nhiá»u FLOPs hÆ¡n so vá»›i máº¡ng Ä‘Æ°á»£c sá»­ dá»¥ng trong phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i (0.3X FLOPs Norm cho GradMax so vá»›i 0.26X FLOPs Norm cho chÃºng tÃ´i), do thiáº¿t láº­p cá»§a cÃ¡c tÃ¡c giáº£ Ä‘Æ°á»£c mÃ´ táº£ trong pháº§n trÆ°á»›c. Tuy nhiÃªn, táº¥t cáº£ cÃ¡c thiáº¿t láº­p khÃ¡c váº«n giá»‘ng nhÆ° nhá»¯ng gÃ¬ Ä‘Ã£ Ä‘á» cáº­p cho CIFAR á»Ÿ trÃªn.

Äá»ƒ huáº¥n luyá»‡n phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i, chÃºng tÃ´i báº¯t Ä‘áº§u vá»›i má»™t máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ³ Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra cá»§a má»—i lá»›p báº±ng má»™t ná»­a kÃ­ch thÆ°á»›c cá»§a máº¡ng má»¥c tiÃªu. ChÃºng tÃ´i huáº¥n luyá»‡n má»™t máº¡ng khÃ¡c cÃ³ kÃ­ch thÆ°á»›c báº±ng nhau trong e epoch, trong Ä‘Ã³ 0â‰¤e < 90 lÃ  má»™t siÃªu tham sá»‘. Sau Ä‘Ã³, cÃ¡c máº¡ng nhá» nÃ y Ä‘Æ°á»£c káº¿t há»£p Ä‘á»ƒ phÃ¡t triá»ƒn thÃ nh má»™t máº¡ng Ä‘áº§y Ä‘á»§. ChÃºng tÃ´i huáº¥n luyá»‡n máº¡ng Ä‘Ã£ phÃ¡t triá»ƒn Ä‘áº§y Ä‘á»§ thÃªm má»™t sá»‘ epoch cho Ä‘áº¿n khi háº¿t ngÃ¢n sÃ¡ch FLOPs.

7. ThÃ­ nghiá»‡m vá»›i VGG-11
BÃªn cáº¡nh WideResnet [49], chÃºng tÃ´i thá»±c hiá»‡n thÃ­ nghiá»‡m trÃªn cÃ¡c há» kiáº¿n trÃºc khÃ¡c nhau. Báº£ng 5 cho tháº¥y hiá»‡u suáº¥t cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i vÃ  cÃ¡c baseline trÃªn táº­p dá»¯ liá»‡u CIFAR-100 khi phÃ¡t triá»ƒn tá»« 2 máº¡ng nhá». Firefly gáº·p khÃ³ khÄƒn trong viá»‡c há»™i tá»¥, dáº«n Ä‘áº¿n bá»‹ loáº¡i khá»i báº£ng. Tuy nhiÃªn, Random vÃ  GradMax cho káº¿t quáº£ tÆ°Æ¡ng tá»±, vá»›i khoáº£ng 48% Ä‘á»™ chÃ­nh xÃ¡c. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n cÃ¡c baseline vá»›i biÃªn Ä‘á»™ lá»›n (âˆ¼7.5%).

8. NgÃ¢n sÃ¡ch tham sá»‘ tháº¥p
Trá»™n máº«u cho phÃ©p chÃºng tÃ´i chia sáº» tham sá»‘ qua cÃ¡c lá»›p, giáº£m sá»‘ lÆ°á»£ng tham sá»‘ trong máº¡ng mÃ  khÃ´ng cáº§n thay Ä‘á»•i kiáº¿n trÃºc cá»§a nÃ³ (nhÆ° chiá»u rá»™ng
11

--- TRANG 12 ---
0.2 0.3 0.4 0.5 0.6 0.7
Number of Parameters (Norm)95.295.495.695.8Accuracy (%)
MÃ´ hÃ¬nh Má»¥c tiÃªu
MixtureGrowth (cá»§a chÃºng tÃ´i)(a) CIFAR-10
0.2 0.3 0.4 0.5 0.6 0.7
Number of Parameters (Norm)77.578.078.579.079.5Accuracy (%)
MÃ´ hÃ¬nh Má»¥c tiÃªu
MixtureGrowth (cá»§a chÃºng tÃ´i) (b) CIFAR-100

HÃ¬nh 6. So sÃ¡nh MixtureGrowth vá»›i mÃ´ hÃ¬nh má»¥c tiÃªu dÆ°á»›i thiáº¿t láº­p tham sá»‘ tháº¥p. Hiá»‡u suáº¥t Ä‘Æ°á»£c Ä‘o báº±ng Ä‘á»™ chÃ­nh xÃ¡c top-1 trung bÃ¬nh trÃªn 3 láº§n cháº¡y.

HÃ¬nh 7. MixtureGrowth vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn khÃ¡c nhau: Minh há»a PhÃ¡t triá»ƒn Dá»c vÃ  PhÃ¡t triá»ƒn Ngang. CÃ¡c gÃ³c pháº§n tÆ° chia sáº» máº«u nhÆ°ng cÃ³ cÃ¡c há»‡ sá»‘ tá»• há»£p tuyáº¿n tÃ­nh khÃ¡c nhau Ä‘Æ°á»£c trÃ¬nh bÃ y dÆ°á»›i dáº¡ng cÃ¡c sáº¯c thÃ¡i khÃ¡c nhau cá»§a cÃ¹ng má»™t mÃ u.

Báº£ng 5. So sÃ¡nh phÃ¡t triá»ƒn máº¡ng trÃªn CIFAR-100 sá»­ dá»¥ng kiáº¿n trÃºc VGG-11 [37]. (a) Hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh baseline trong Ä‘Ã³ Firefly khÃ´ng Ä‘Æ°á»£c bao gá»“m do khÃ´ng há»™i tá»¥ (b) Hiá»‡u suáº¥t cá»§a MixtureGrowth

PhÆ°Æ¡ng phÃ¡p Top-1 Acc. Total FLOPs Norm
(a) Random 48.52% 1.3X
GradMax [14] 48.79% 1.3X
(b) MixtureGrowth (cá»§a chÃºng tÃ´i) 56.17% 0.6X

vÃ  Ä‘á»™ sÃ¢u). Äá»ƒ so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i vá»›i cÃ¡c mÃ´ hÃ¬nh má»¥c tiÃªu dÆ°á»›i ngÃ¢n sÃ¡ch tham sá»‘ tháº¥p, chÃºng tÃ´i giáº£m chiá»u rá»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh má»¥c tiÃªu sao cho sá»‘ lÆ°á»£ng tham sá»‘ cá»§a chÃºng khá»›p vá»›i phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i. HÃ¬nh 6a minh há»a hiá»‡u suáº¥t cá»§a MixtureGrowth khi so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh má»¥c tiÃªu nhá» trÃªn CIFAR-10 vá»›i cÃ¹ng sá»‘ lÆ°á»£ng tham sá»‘, trong Ä‘Ã³

Báº£ng 6. So sÃ¡nh phÆ°Æ¡ng phÃ¡p phÃ¡t triá»ƒn khÃ¡c nhau cá»§a MixtureGrowth trÃªn CIFAR-100 sá»­ dá»¥ng kiáº¿n trÃºc WRN-28-10. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh cá»§a ba láº§n cháº¡y cho má»—i phÆ°Æ¡ng phÃ¡p.

PhÆ°Æ¡ng phÃ¡p Top-1 Acc. FLOPs Norm
PhÃ¡t triá»ƒn ngang 80.66% 0.35X
PhÃ¡t triá»ƒn dá»c 80.82 % 0.35X

MixtureGrowth liÃªn tá»¥c vÆ°á»£t trá»™i cÃ¡c mÃ´ hÃ¬nh má»¥c tiÃªu dÆ°á»›i ngÃ¢n sÃ¡ch tham sá»‘ tháº¥p. ChÃºng tÃ´i tháº¥y quan sÃ¡t tÆ°Æ¡ng tá»± trÃªn táº­p dá»¯ liá»‡u CIFAR-100, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong 6b. ChÃºng tÃ´i sá»­ dá»¥ng kiáº¿n trÃºc WRN [49] Ä‘á»ƒ so sÃ¡nh cáº£ hai táº­p dá»¯ liá»‡u.

9. PhÃ¡t triá»ƒn Ngang vÃ  Dá»c
Táº¡i bÆ°á»›c phÃ¡t triá»ƒn, chÃºng tÃ´i phÃ¡t triá»ƒn tá»« 2 máº¡ng nhá» Ä‘Æ°á»£c huáº¥n luyá»‡n thÃ nh má»™t máº¡ng lá»›n. Cho cÃ¡c máº¡ng Ä‘Æ°á»£c huáº¥n luyá»‡n lÃ  2 gÃ³c pháº§n tÆ° Ä‘Æ°á»ng chÃ©o, cÃ³ 2 cÃ¡ch Ä‘á»ƒ má»Ÿ rá»™ng nÃ³ thÃ nh 4 gÃ³c pháº§n tÆ°. Lá»±a chá»n Ä‘áº§u tiÃªn lÃ  PhÃ¡t triá»ƒn dá»c, trong Ä‘Ã³ cÃ¡c gÃ³c pháº§n tÆ° cÃ³ cÃ¹ng Ä‘áº§u ra chia sáº» máº«u (HÃ¬nh 7, trÃªn pháº£i). CÃ¡ch khÃ¡c lÃ  phÃ¡t triá»ƒn ngang trong Ä‘Ã³ cÃ¡c gÃ³c pháº§n tÆ° cÃ³ cÃ¹ng Ä‘áº§u vÃ o sá»­ dá»¥ng cÃ¹ng táº­p há»£p máº«u (HÃ¬nh 7, dÆ°á»›i pháº£i). Báº£ng 6 so sÃ¡nh hiá»‡u suáº¥t cá»§a 2 chiáº¿n lÆ°á»£c phÃ¡t triá»ƒn trÃªn táº­p dá»¯ liá»‡u CIFAR-100. ChÃºng tÃ´i nháº­n tháº¥y ráº±ng PhÃ¡t triá»ƒn dá»c vÆ°á»£t trá»™i má»™t chÃºt so vá»›i PhÃ¡t triá»ƒn ngang vá» hiá»‡u suáº¥t.
12
