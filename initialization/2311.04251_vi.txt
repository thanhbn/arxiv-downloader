# 2311.04251.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/initialization/2311.04251.pdf
# Kích thước file: 1792733 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
MixtureGrowth: Phát triển Mạng Neural bằng cách Tái kết hợp các Tham số Đã học
Chau Pham1*Piotr Teterwak1*Soren Nelson2*†Bryan A. Plummer1
Boston University1Physical Sciences Inc2
{chaupham,piotrt,bplum }@bu.edu snelson@psicorp.com
Tóm tắt
Hầu hết các mạng neural sâu được huấn luyện dưới các kiến trúc mạng cố định và yêu cầu huấn luyện lại khi kiến trúc thay đổi. Nếu cần mở rộng kích thước của mạng, cần phải huấn luyện lại từ đầu, điều này rất tốn kém. Để tránh điều này, người ta có thể phát triển từ một mạng nhỏ bằng cách thêm trọng số ngẫu nhiên theo thời gian để dần đạt được kích thước mạng mục tiêu. Tuy nhiên, cách tiếp cận ngây thơ này không hiệu quả trong thực tế vì nó mang quá nhiều nhiễu vào quá trình phát triển. Các nghiên cứu trước đã giải quyết vấn đề này bằng cách tận dụng các trọng số đã học và dữ liệu huấn luyện để tạo ra trọng số mới thông qua việc thực hiện một bước phân tích tốn kém về mặt tính toán. Trong bài báo này, chúng tôi giới thiệu MixtureGrowth, một cách tiếp cận mới để phát triển mạng tránh được chi phí khởi tạo trong các nghiên cứu trước. Trước khi phát triển, mỗi lớp trong mô hình của chúng tôi được tạo ra bằng một tổ hợp tuyến tính của các mẫu tham số. Trọng số lớp mới được phát triển được tạo ra bằng cách sử dụng một tổ hợp tuyến tính mới của các mẫu hiện có cho một lớp. Một mặt, các mẫu này đã được huấn luyện cho nhiệm vụ, cung cấp một khởi tạo mạnh. Mặt khác, các hệ số mới cung cấp tính linh hoạt cho trọng số lớp được thêm vào để học điều gì đó mới. Chúng tôi cho thấy cách tiếp cận của chúng tôi tăng độ chính xác top-1 so với hiện tại tốt nhất 2-2.5% trên các tập dữ liệu CIFAR-100 và ImageNet, trong khi đạt được hiệu suất tương đương với ít FLOPs hơn so với một mạng lớn hơn được huấn luyện từ đầu. Mã nguồn có sẵn tại https://github.com/
chaudatascience/mixturegrowth .
1. Giới thiệu
Nhiều nhiệm vụ khám phá sự đánh đổi giữa hiệu suất dự đoán và độ phức tạp tính toán, chẳng hạn như tìm kiếm kiến trúc neural (NAS) [3, 12, 23, 39], chưng cất kiến thức [16, 18], và cắt tỉa tham số [21, 44]. Các phương pháp này dẫn đến một mô hình được tối ưu hóa tốt trong quá trình suy luận nhưng thường tốn kém để huấn luyện (ví dụ, [39]). Do đó, một số nhà nghiên cứu đã khám phá một quy trình thay thế trong đó người ta bắt đầu với một mạng nhỏ và tăng kích thước của nó theo thời gian (ví dụ, [14, 46]). Bằng cách huấn luyện trước một mô hình nhỏ hơn và rẻ hơn, cách tiếp cận này cũng có thể được sử dụng để học một kiến trúc mục tiêu với ít phép toán dấu phẩy động (FLOPs) hơn so với huấn luyện mạng mục tiêu từ đầu. Thách thức chính là quyết định cách khởi tạo bất kỳ trọng số lớp mới nào sau một bước phát triển để tránh quên những gì mạng đã học được trong khi vẫn để lại không gian cho cải thiện. Như được minh họa trong Hình 1a, các nghiên cứu trước đã khám phá các phương pháp khởi tạo tham số mới thông qua một bước phân tích tốn kém về mặt tính toán để chọn các tham số giảm loss huấn luyện [46] hoặc tăng dòng gradient [14].

Bài báo của chúng tôi tìm cách tìm ra giải pháp cho một tình huống trong đó chúng ta có một mô hình được huấn luyện đầy đủ và muốn mở rộng nó thành một mô hình lớn hơn. Để làm điều này, chúng tôi đề xuất MixtureGrowth, một khung phát triển mạng mới tạo ra trọng số mới bằng cách học cách tái sử dụng và kết hợp các tham số từ mạng nhỏ hơn. Cách tiếp cận của chúng tôi được lấy cảm hứng từ thành công gần đây với các phương pháp trộn mẫu [2, 31, 34, 41, 48], trong đó trọng số lớp được tạo ra bằng cách sử dụng một tổ hợp tuyến tính của các mẫu tham số được chia sẻ. Mỗi lớp có được trọng số tùy chỉnh bằng cách sử dụng một tập hợp các hệ số tuyến tính duy nhất, từ đó tăng tính biểu đạt của các tham số được chia sẻ. Các mẫu được chia sẻ và các hệ số được huấn luyện cùng nhau mà không cần thay đổi các hàm loss hoặc kiến trúc mạng cho nhiệm vụ mục tiêu. Khi so sánh với các mạng neural truyền thống, các phương pháp này đã báo cáo cải thiện hiệu quả tính toán và tham số [2, 31, 41], giảm thời gian huấn luyện [2,31], và cải thiện hiệu suất nhiệm vụ [31,34,41,48].

Một cái nhìn tổng quan cấp cao về cách tiếp cận của chúng tôi được cung cấp trong Hình 1b. Cụ thể, chúng tôi đặt giả thuyết rằng vì trộn mẫu hiệu quả chia sẻ tham số qua các lớp, nó cũng có thể được sử dụng để khởi tạo trọng số lớp mới sau một bước phát triển. Chúng tôi điều tra ba câu hỏi nghiên cứu chưa được khám phá trong các nghiên cứu trước. Đầu tiên, trộn mẫu cung cấp một cơ chế mới để tạo ra trọng số lớp mới sau khi phát triển. Ví dụ, hãy xem xét việc tăng chiều rộng của một lớp g lần, dẫn đến hầu hết các lớp trong mạng mở rộng
1arXiv:2311.04251v1  [cs.LG]  7 Nov 2023

--- TRANG 2 ---
a. Nghiên cứu Trước 
Trình Tạo 
Trọng số Dữ liệu 
Trọng số 
Hiện tại Trước phát triển 
Trọng số 
Sau phát triển 
Trọng số b. Mixture Growth (của chúng tôi) 
Mẫu Tham số 
Được chia sẻ Trình Tạo 
Trọng số 𝛂cũ Hệ số 
Tuyến tính 
𝛂mới 
Hình 1. So sánh các chiến lược khác nhau trong việc phát triển mạng neural. (a) Trong các nghiên cứu trước (ví dụ, [14,46]), các phương pháp phát triển mạng sử dụng các biện pháp cục bộ dựa trên loss để khởi tạo trọng số mới tại thời điểm phát triển. Các phương pháp này phụ thuộc vào việc thực hiện phân tích sử dụng dữ liệu huấn luyện và trọng số hiện tại của các lớp hiện có, đòi hỏi chi phí tính toán đáng kể tại bước phát triển; (b)Ngược lại, chúng tôi điều tra một phương pháp dựa trên trộn mẫu để phát triển mạng. Cho một mạng nhỏ được huấn luyện, chúng tôi bắt đầu bằng việc huấn luyện một mạng khác có kích thước bằng nhau. Sau đó, các tham số của những mạng nhỏ này được chia sẻ qua các lớp bằng các sơ đồ trộn mẫu, trong đó trọng số lớp bổ sung được tạo ra bằng cách sử dụng các hệ số tuyến tính mới cho các mẫu hiện có.

có g2 lần trọng số của mạng gốc. Vì các trọng số trong mạng nhỏ được tạo ra bằng cách sử dụng một tổ hợp tuyến tính của các mẫu được chia sẻ, chúng ta có thể tạo ra các trọng số của mạng lớn bằng cách khởi tạo g2−1 tập hệ số tuyến tính mới và nối các trọng số được tạo ra cho lớp lớn (được minh họa trong Hình 2). Chúng tôi thấy rằng việc khởi tạo cẩn thận các hệ số tuyến tính mới này vẫn cần thiết để có hiệu suất tốt nhất, với một số chiến lược, chẳng hạn như sao chép các hệ số hiện có, có tác động có hại đến việc huấn luyện.

Câu hỏi nghiên cứu thứ hai mà chúng tôi điều tra là liệu chúng ta nên phát triển từ một mô hình nhỏ duy nhất, hay việc kết hợp hai mô hình nhỏ có tốt hơn không. Chúng tôi lập luận rằng việc huấn luyện một mô hình nhỏ thêm cung cấp nhiều đa dạng hơn về các đặc trưng đã học cho quá trình phát triển. Trong một thiết lập mà chúng ta phát triển từ một mô hình có chiều rộng bằng một nửa mô hình lớn, thì một mạng nhỏ sẽ chỉ mất khoảng 25% FLOPs của mạng lớn, làm cho nó tương đối rẻ.

Câu hỏi nghiên cứu cuối cùng mà chúng tôi khám phá, tiếp theo câu hỏi trước, là tìm thời điểm tối ưu để dừng huấn luyện mạng thứ hai và bắt đầu huấn luyện một mạng đã phát triển đầy đủ. Với một ngân sách FLOPs cố định, phát triển sớm cho nhiều thời gian hơn để tối ưu hóa tất cả các tham số lớp trong mạng lớn. Tuy nhiên, điều này cũng hạn chế khả năng của mạng thứ hai, từ đó đặt ra câu hỏi về thời điểm phát triển. Chúng tôi thấy rằng cách tiếp cận của chúng tôi tương đối không nhạy cảm với giai đoạn huấn luyện chính xác mà tại đó bước phát triển xảy ra, và có thể được sử dụng để phát triển muộn với tác động tối thiểu đến hiệu suất nhiệm vụ.

Tóm lại, các đóng góp của chúng tôi là:
• Chúng tôi phát triển MixtureGrowth, một cách tiếp cận mới để phát triển một mạng neural theo thời gian bằng cách nối các trọng số của mạng cũ (nhỏ) với trọng số mới được tạo ra từ các hệ số tuyến tính được khởi tạo mới được sử dụng để kết hợp các mẫu tham số được chia sẻ hiện có.

• Các thí nghiệm của chúng tôi cho thấy MixtureGrowth là một phương pháp phát triển hiệu quả bằng cách vượt trội độ chính xác top-1 của các nghiên cứu trước 2-2.5% trên CIFAR-100 và ImageNet, chứng minh tính hiệu quả của cách tiếp cận của chúng tôi.

• Chúng tôi phân tích cách tiếp cận của chúng tôi để phát triển mạng neural theo các trục như độ nhạy cảm với điểm phát triển, đặc điểm của các đặc trưng đã học, và tác động của việc kết hợp hai mô hình nhỏ so với phát triển từ một mô hình duy nhất.

2. Nghiên cứu Liên quan
Phát triển Mạng Neural từ lâu đã là một lĩnh vực nghiên cứu tích cực; Trong nghiên cứu ban đầu, Ash et al. [1] đã thêm các neuron trong các hệ thống một lớp, sau đó được mở rộng bởi Fahlman et al. [15] cho chế độ mạng sâu. Việc huấn luyện trước các mạng neural một cách tham lam bằng cách xếp chồng Máy Boltzmann Bị hạn chế [4, 20] để xây dựng Mạng Niềm tin Sâu [19] đã trở nên phổ biến. Điều này được đơn giản hóa bởi Vincent et al. [43] với Bộ Tự mã hóa Khử nhiễu Xếp chồng. Gần đây hơn, Wen et al. [45] đã đề xuất các chính sách phát triển độ sâu để tự động phát triển mạng cho đến khi hiệu suất ngừng cải thiện. Maile et al. [28] đề xuất khởi tạo trọng số mới để tối đa hóa tính trực giao. Tuy nhiên, các nghiên cứu này nhằm tự động hóa việc lựa chọn kích thước mạng bằng phát triển tiến bộ, trái ngược với việc giảm thiểu chi phí huấn luyện cho một kích thước mạng nhất định.

Trong nghiên cứu liên quan hơn, Wu et al. [47] và Chen et al. [7] đã sao chép trọng số của mạng để tăng dần chiều rộng của nó, trong khi Net2Net [7] tận dụng các biến đổi bảo toàn hàm để chuyển giao kiến thức khi phát triển. Các cách tiếp cận này sao chép trọng số sao cho hàm trước phát triển được bảo toàn, và sau đó nhiễu loạn hoặc thêm nhiễu để phá vỡ tính đối xứng để phát triển. Firefly [46] cũng học một mạng phát triển quá mức và sau đó cắt tỉa các trọng số không có thông tin. GradMax [14] và NeST [9] khởi tạo trọng số để tối đa hóa chuẩn gradient. Mặc dù các phương pháp này hiệu quả hơn so với huấn luyện lại từ đầu, nhiều trong số chúng vẫn yêu cầu chi phí tính toán đáng kể để phân tích cách mạng nên khởi tạo mạng mới.

Trộn Mẫu cải thiện việc chia sẻ tham số cứng, trực tiếp tái sử dụng tham số qua các lớp [8, 50], bằng cách
2

--- TRANG 3 ---
!!∈#…%!𝛼#!#𝑡!#InputOutput Mạng 1(đã huấn luyện)Trước Phát triển 
Phát triển𝑇!!!∈#…%"𝛼&!#𝑡!&InputOutput Mạng 2!!∈#…%!𝛼#!#𝑡!#!!∈#…%"𝛼&!#𝑡!&!!∈#…%!𝛼#!&𝑡!#!!∈#…%"𝛼&!&𝑡!&Sau Phát triển InputOutput 
𝑇"𝑇"𝑇!Hình 2. Minh họa quá trình phát triển trong các mô hình MixtureGrowth với g= 2 (tức là, gấp đôi kích thước): Chúng tôi khám phá cách phát triển các mô hình có trọng số được xây dựng từ các tổ hợp tuyến tính của các mẫu, mà chúng tôi gọi là Mô hình Trộn Mẫu (xem Mục 3.1). Trước khi phát triển, giả sử rằng người ta đã có một mô hình được huấn luyện đầy đủ (ký hiệu là mạng 1, bên trái) và muốn mở rộng nó. Để làm điều này, chúng tôi trước tiên huấn luyện một mô hình khác có kích thước bằng nhau sử dụng một tập hợp mẫu khác (mạng 2) trong một số epoch. Sau đó, chúng tôi kết hợp trọng số của hai mạng sao cho trọng số của chúng được xếp trong các góc dọc theo đường chéo. Trong ví dụ này, chúng tôi tạo thành một mô hình lớn gấp 4 lần và tiếp tục huấn luyện trên mô hình đã phát triển đầy đủ (mô hình mục tiêu, ngoài cùng bên phải). Các góc phần tư chia sẻ mẫu nhưng có các hệ số tổ hợp tuyến tính khác nhau được trình bày dưới dạng các sắc thái khác nhau của cùng một màu. Các hình vuông tương ứng với trọng số lớp có đầu vào và đầu ra tương ứng với chiều đầu vào và đầu ra. Lưu ý rằng tất cả mẫu và hệ số alpha đều là tham số có thể học được.

biểu diễn trọng số lớp như một hỗn hợp tuyến tính của các mẫu được chia sẻ (ví dụ, [2, 31, 34, 41, 48]). Savarese et al. [34] thấy rằng họ có thể sử dụng trộn mẫu để tăng hiệu suất bằng cách chia sẻ qua nhiều lớp trong một mạng duy nhất. Các mạng đẳng cấu [33, 51] có thể đẩy việc chia sẻ tham số giữa các lớp xa hơn bằng cách ràng buộc mạng có các lớp có hình dạng và kích thước giống hệt nhau. Plummer et al. [31] loại bỏ nhu cầu có nhiều lớp giống hệt nhau bằng cách nâng cấp/hạ cấp mẫu khi được chia sẻ giữa các lớp có hình dạng khác nhau. Thú vị là, các phương pháp này thể hiện sự tương đồng với khái niệm học tập mô-đun, trong đó các lớp mạng được chia sẻ qua nhiều nhiệm vụ, như được khám phá trong nghiên cứu trước [29, 30]. Trong khi học tập mô-đun dựa vào trọng số lớp được chia sẻ được điều chỉnh cho từng nhiệm vụ riêng lẻ, trộn mẫu, ngược lại, kết hợp các mẫu được chia sẻ để tạo ra trọng số lớp một cách động. Trong bài báo này, chúng tôi mở rộng nghiên cứu về trộn mẫu bằng cách đặt ra các câu hỏi nghiên cứu mới chưa được giải quyết trong các nghiên cứu trước. Cụ thể, các nghiên cứu trước về trộn mẫu giả định mạng được huấn luyện giữ nguyên kích thước theo thời gian. Tuy nhiên, trong nghiên cứu này, chúng tôi tận dụng cấu trúc độc đáo của trộn mẫu để tạo ra trọng số mới khi mở rộng mạng trong quá trình huấn luyện.

Thu nhỏ Mạng Neural thay đổi kích thước của một mô hình trong quá trình huấn luyện thường nhằm tăng hiệu quả tính toán tại thời điểm kiểm tra. Có hai phương pháp được sử dụng rộng rãi cho việc này: chưng cất và cắt tỉa mạng neural. Trong chưng cất mạng neural, mục tiêu là chuyển giao kiến thức từ một mạng lớn hơn sang một mạng nhỏ hơn. Kỹ thuật này, được giới thiệu trong Hinton et al. [18], thay thế các nhãn một-hot bằng các dự đoán của một mạng lớn hơn (tức là mạng giáo viên). Theo cách này, kiến thức từ mạng giáo viên được chuyển giao cho mạng nhỏ hơn gọi là mạng học sinh. Ví dụ về các nghiên cứu tiếp theo bao gồm các phương pháp chưng cất tập hợp thành một mô hình duy nhất [36] và làm nổi bật tầm quan trọng của việc học sinh và giáo viên thấy các tăng cường giống hệt nhau trong các giai đoạn huấn luyện dài [6]. Trong cắt tỉa tham số, mục tiêu là tìm và loại bỏ các trọng số không quan trọng (ví dụ, [27,38,42]), dẫn đến một mạng hiệu quả hơn về mặt tính toán với chi phí tăng thời gian huấn luyện vì nó giả định người ta đã huấn luyện mạng được tham số hóa đầy đủ cho đến khi hội tụ. Trái ngược với các nhiệm vụ này, mục tiêu của chúng tôi là giảm thời gian huấn luyện của mạng lớn bằng cách phát triển một mạng neural theo thời gian.

3. Phát triển thông qua khởi tạo tham số được chia sẻ
Cho một kiến trúc mạng neural nhỏ được huấn luyện trước FS với các lớp ℓS1,...,N, mục tiêu của chúng tôi là tạo ra trọng số của một kiến trúc mạng neural lớn hơn FL với các lớp ℓL1,...,M. Theo [14, 46], chúng tôi đánh giá các thiết lập trong đó các mạng có cùng số lượng lớp (tức là, M=N), mặc dù cách tiếp cận của chúng tôi có thể được sử dụng để phát triển theo độ sâu [31,34]. Thay vào đó, chúng tôi thay đổi chiều rộng của mỗi lớp, tức là, |ℓLi|>|ℓSi|, trong đó toán tử | · | trả về số lượng trọng số trong một lớp. Trong các thí nghiệm của chúng tôi, đặt kích thước của FL gấp đôi FS, mặc dù các thiết lập khác có thể với những thay đổi nhỏ trong việc xây dựng trọng số lớp (ví dụ, như trong [31, 41]).

Nhiệm vụ khi đó là tạo ra trọng số cho FL cho phép nó hội tụ nhanh chóng sử dụng những gì đã được học trong FS. Các phương pháp được xếp hạng dựa trên cả hiệu quả tính toán của việc có được mô hình FL được huấn luyện đầy đủ cũng như chất lượng của giải pháp cuối cùng được đo bằng hiệu suất nhiệm vụ.

Các nghiên cứu trước về phát triển mạng neural dựa vào việc thực hiện một phân tích tốn kém về mặt tính toán cho từng lớp về gradient hoặc loss huấn luyện để khởi tạo bất kỳ tham số mới nào trong FL (ví dụ, [14, 46]). Chúng tôi tránh vấn đề này bằng cách
3

--- TRANG 4 ---
học cách tạo ra trọng số mới bằng cách sử dụng một trình tạo trọng số dựa trên các tham số được chia sẻ. Lưu ý rằng chúng tôi đề cập đến trọng số như ma trận các số được sử dụng bởi một phép toán lớp, chẳng hạn như các bộ lọc được sử dụng bởi các lớp tích chập, và tham số như các ma trận được tối ưu hóa bằng cách sử dụng lan truyền ngược. Trong các mạng neural truyền thống, trọng số và tham số thường đề cập đến cùng một tập hợp ma trận, nhưng trong bài báo này, chúng tôi tạo ra trọng số lớp bằng cách sử dụng tham số có thể huấn luyện. Các tham số là các tổ hợp tuyến tính của các mẫu là kết quả của một quá trình mà chúng tôi đề cập đến như trộn mẫu [31,34,41] mà chúng tôi xem xét ngắn gọn trong Mục 3.1. Tiếp theo, chúng tôi thảo luận về chiến lược của chúng tôi để học các mẫu tham số biểu đạt hơn bằng cách sử dụng kết hợp mô hình (Mục 3.2). Cuối cùng, chúng tôi giới thiệu một quá trình khởi tạo bất kỳ trọng số mới nào cần thiết bởi FL bằng cách xếp các trọng số được tạo ra bằng cách sử dụng các mẫu ban đầu được học từ mạng nhỏ FS của chúng tôi (Mục 3.3).

3.1. Tạo trọng số với trộn mẫu
Các mạng trộn mẫu (ví dụ, [2, 31, 34, 41, 48]) tạo ra trọng số lớp bằng cách kết hợp các mẫu tham số và đã cho thấy chúng cung cấp hiệu quả tham số và hiệu suất vượt trội so với các mạng neural truyền thống. Chúng tôi sử dụng thiết lập trong đó mỗi mẫu tham số T1,..,t có cùng kích thước với lớp tương ứng trong mạng neural nhỏ FS (tức là, |Tki|=|ℓSi|), mặc dù các mẫu tham số có kích thước khác nhau cũng có thể được sử dụng [41]. Do đó, trọng số của lớp thứ i, ký hiệu là ℓSi, được tạo ra thông qua,

ℓSi=∑k∈1,...,tαkiTki, (1)

trong đó αki là một tham số có thể huấn luyện điều khiển đóng góp của mẫu Tki trong việc tạo ra trọng số cho lớp ℓSi.

Chúng tôi bắt đầu bằng việc huấn luyện trước mạng nhỏ FS của chúng tôi bằng cách sử dụng trộn mẫu. Theo [34], chúng tôi chia sẻ mẫu giữa bất kỳ cặp lớp nào có cùng kích thước (với nhóm tối đa gồm hai lớp liên tiếp chia sẻ mẫu) cho các trọng số mới. Ví dụ, nếu Ti và Ti+1 được sử dụng để tạo ra trọng số cho các lớp giống hệt nhau ℓSi và ℓSi+1, thì chúng đề cập đến cùng một tập hợp tham số được chia sẻ, và chúng sẽ không chia sẻ mẫu với lớp ℓSi+2, ngay cả khi có cùng kích thước.

Tổng số tham số có thể huấn luyện so với một mạng neural tiêu chuẩn chỉ tăng bởi số lượng hệ số αki được sử dụng để tạo ra trọng số (dẫn đến hàng trăm tham số thêm trong các thí nghiệm của chúng tôi trong quá trình huấn luyện). Cả hệ số αki và mẫu Ti đều được học cùng nhau thông qua lan truyền ngược từ loss nhiệm vụ, không có các điều khoản loss bổ sung so với một mạng neural tiêu chuẩn được sử dụng trong quá trình huấn luyện. Lưu ý rằng quá trình tạo trọng số này thực sự giới thiệu một số chi phí trong quá trình huấn luyện, nhưng sự gia tăng này là không đáng kể, chỉ chiếm 0.03% FLOPs trong một lần forward pass cho kích thước batch 64 mẫu [31].

3.2. Học các mẫu mạnh mẽ để phát triển
Một sự khác biệt chính giữa MixtureGrowth của chúng tôi và các nghiên cứu trước về trộn mẫu (ví dụ, [2, 31, 34, 41, 48]) là kiến trúc trong giai đoạn huấn luyện đầu tiên của chúng tôi không phải là kiến trúc mạng cuối cùng. Thay vào đó, chúng tôi sử dụng một quy trình mạng neural tiêu chuẩn để học một mạng nhỏ và sau đó phát triển thành một mạng lớn hơn, tương tự như các nghiên cứu trước về phát triển mạng (ví dụ, [14, 46]). Các nghiên cứu trước về phát triển mạng sẽ khởi tạo các tham số có thể huấn luyện mới bằng cách sử dụng một phương pháp heuristic được xây dựng từ mạng nhỏ. Điều này có thể dẫn đến khởi tạo không thuận lợi vì tính biểu đạt của mạng lớn lớn hơn nhiều (do có nhiều trọng số hơn) so với mạng nhỏ. Các phương pháp của các nghiên cứu trước về phát triển mạng có thể được xem như tập trung vào việc cố gắng khôi phục (một phần) tính biểu đạt bổ sung bị thiếu trong mạng nhỏ. Ngược lại, cách tiếp cận của chúng tôi giả định rằng nhiều mẫu mà chúng tôi đã học trong FS sẽ tổng quát hóa cho các trọng số mới cần thiết sau khi phát triển thành mạng mục tiêu FL.

Chúng tôi khám phá hai biến thể của phương pháp của chúng tôi: một chúng tôi phát triển trực tiếp từ một mô hình được huấn luyện duy nhất và tái sử dụng các mẫu cho trọng số mới sau khi phát triển, và một khác trong đó chúng tôi học một mô hình thêm sử dụng một tập hợp mẫu độc lập trước khi phát triển. Chúng tôi gọi cách tiếp cận thứ hai này là kết hợp mô hình, được minh họa trong Hình 2. Phương pháp của chúng tôi được thúc đẩy bởi ý tưởng rằng các mô hình được huấn luyện độc lập học các đặc trưng độc lập đáng kể [11, 26]. Do đó, việc kết hợp hai mô hình tạo ra một biểu diễn đặc trưng mạnh mẽ hơn, từ đó tăng hiệu suất. Từ một cặp mạng nhỏ được huấn luyện FS1, FS2, chúng tôi sử dụng chúng để khởi tạo và huấn luyện mạng mục tiêu FL của chúng tôi, mà chúng tôi thảo luận trong phần tiếp theo.

3.3. Tạo trọng số cho mạng mục tiêu
Sau khi có được cặp mạng nhỏ được huấn luyện trước FS1, FS2 sử dụng cách tiếp cận trong Mục 3.2, mục tiêu là sử dụng các mạng này để xây dựng trọng số của mạng lớn FL. Từ FS, chúng ta muốn phát triển nó lớn hơn g lần để có được FL. Do đó, hầu hết các lớp trong mạng lớn yêu cầu g2 lần trọng số. Ngoại lệ duy nhất là lớp đầu tiên và cuối cùng, chỉ lớn hơn g lần vì kích thước của đầu vào và đầu ra không thay đổi. Hình 2 minh họa một cách xếp cho trọng số của mỗi lớp với g= 2.

Vì các mạng nhỏ của chúng tôi đã được huấn luyện bằng cách sử dụng trộn mẫu, chúng tôi có thể tạo ra trọng số mới bằng cách sử dụng một tập hợp hệ số mới (α trong Phương trình 1). Do đó, nếu chúng ta cần tạo ra g2 ô trọng số cho một lớp, thì chúng ta sẽ học g2 hệ số đặc trưng cho ô, nhưng các mẫu sẽ được chia sẻ với các ô khác được tạo ra từ cùng một mạng con nhỏ. Trong số g2 hệ số đặc trưng cho ô này, chúng ta có thể tạo ra g2−2 ô mới bằng cách tái sử dụng các mạng con nhỏ hiện có FS1, FS2 của chúng tôi, nhưng chúng ta phải khởi tạo các hệ số mới. Chúng tôi điều tra nhiều chiến lược để khởi tạo các hệ số này được mô tả dưới đây.
4

--- TRANG 5 ---
Hệ số ngẫu nhiên đơn giản khởi tạo các hệ số mới một cách ngẫu nhiên từ phân phối chuẩn tiêu chuẩn. Bằng cách tối ưu hóa cùng nhau tất cả các hệ số và mẫu được chia sẻ, mạng neural tìm ra các vùng loss thấp mới sau khi phát triển. Tuy nhiên, vẫn không chắc chắn liệu một khởi tạo ngẫu nhiên có làm tăng đáng kể tính đa dạng cho việc phát triển mạng hay không.

Sao chép hệ số được thúc đẩy bởi ý tưởng rằng toàn bộ không gian con của trọng số được bao trùm bởi một tổ hợp tuyến tính của các mẫu không tương ứng với loss thấp. Việc tìm các tổ hợp loss thấp mới mà không thực hiện phân tích kỹ lưỡng là thách thức, vì vậy thay vào đó chúng tôi sao chép các hệ số của các mạng con FS1, FS2 làm điểm khởi đầu ban đầu cho các trọng số mới. Các hệ số được sao chép này không được chia sẻ, vì vậy mặc dù chúng bắt đầu như các giá trị giống nhau, chúng được thay đổi độc lập trong khi huấn luyện FL. Điều này có thể được xem như cung cấp một khởi tạo bắt đầu làm tăng phản ứng của mỗi mạng con g lần trong mọi lớp sau khi phát triển.

Hệ số trực giao sử dụng các hệ số trực giao khi chia sẻ mẫu để khuyến khích tính đa dạng trong việc tạo ra trọng số, được giới thiệu bởi Savarese et al. [34]. Nói cách khác, khi mỗi tập hợp hệ số α có thể được coi là một vector, tất cả các vector α như vậy được liên kết với cùng một mẫu được khởi tạo để trực giao theo [35].

4. Thí nghiệm
Thiết lập Thí nghiệm. Chúng tôi giả định một mạng được huấn luyện có sẵn để phát triển. Trong khi phương pháp của chúng tôi bắt đầu với một mô hình trộn mẫu nhỏ, các baseline sử dụng các mạng nhỏ theo thiết lập riêng của chúng. Để so sánh công bằng, tất cả các mạng nhỏ có cùng FLOPs (25% của mạng mục tiêu).

Chỉ số. Chúng tôi xếp hạng các phương pháp dựa trên độ chính xác phân loại top-1 trung bình trên ba lần chạy trên tất cả các tập dữ liệu. Ngoài ra, chúng tôi báo cáo số lượng phép toán dấu phẩy động (FLOPs) cần thiết để huấn luyện một mô hình. Chỉ số này so sánh chi phí tính toán cho các mô hình khác nhau bất kể phần cứng của chúng. Do đó, nó được sử dụng rộng rãi để đánh giá hiệu quả của một mạng [13, 32, 40]. Đối với các thí nghiệm của chúng tôi, chúng tôi chuẩn hóa độ phức tạp tính toán qua các phương pháp bằng cách chỉ định một mô hình huấn luyện trong cùng FLOPs để đảm bảo so sánh công bằng. Chúng tôi sử dụng thư viện fvcore1 để tính FLOPs.

Baseline. Chúng tôi sử dụng các phương pháp baseline sau.
•Random khởi tạo bất kỳ trọng số mới nào một cách ngẫu nhiên trong quá trình phát triển. Điều này đề cập đến việc tạo ra trọng số trực tiếp, thay vì khởi tạo ngẫu nhiên các hệ số được sử dụng trong các mạng trộn mẫu (được mô tả trong Mục 3.3).

•Net2Net [7] dựa trên khởi tạo bảo toàn hàm để khởi tạo các trọng số mới theo cách sao cho dự đoán không thay đổi. Điều này đảm bảo mô hình mới được phát triển có thể có cùng khả năng như mô hình nhỏ ngăn chặn sự giảm hiệu suất tại điểm phát triển.

1https://github.com/facebookresearch/fvcore

•Firefly [46] phát triển một mạng bằng cách sử dụng cắt tỉa. Cụ thể, họ phát triển quá mức mạng, tối ưu hóa nó trong vài bước, và sau đó cắt tỉa mạng bằng cách sử dụng Xấp xỉ Taylor của hàm loss để ước tính điểm quan trọng của các neuron mới. Lưu ý rằng cách tiếp cận của chúng tôi tránh chi phí này bằng cách học cách tạo ra trọng số mới bằng cách tái sử dụng các tham số được chia sẻ của chúng tôi, làm cho bước phát triển hiệu quả hơn đáng kể về mặt tính toán.

•GradMax [14] đề xuất một cách tiếp cận tìm cách tối đa hóa gradient của các neuron mới. Điều này được thúc đẩy bởi quan sát rằng khởi tạo với gradient lớn dẫn đến tiến bộ nhanh chóng trong bài toán học tập. GradMax tối đa hóa gradient của loss đối với các tham số mới. Nó giải quyết điều này một cách xấp xỉ với SVD tại thời điểm phát triển, làm cho bước phát triển tốn kém về mặt tính toán khi so sánh với cách tiếp cận của chúng tôi. Chúng tôi sử dụng mã của tác giả trong các thí nghiệm của chúng tôi2, mà chúng tôi cũng sử dụng như việc triển khai của chúng tôi cho các phương pháp Random và Firefly.

4.1. Tập dữ liệu và chi tiết triển khai
Xem phần bổ sung cho các chi tiết không được mô tả dưới đây.

CIFAR-10 và CIFAR-100 [25] chứa 60K hình ảnh của 10 và 100 loại, tương ứng. Mỗi tập dữ liệu được chia thành 50K hình ảnh để huấn luyện và 10K hình ảnh để kiểm tra. Chúng tôi sử dụng kiến trúc WideResnet [49] để phát triển từ WRN-28-5 thành WRN-28-10. Chúng tôi huấn luyện mô hình trong 200 epoch trên một GPU duy nhất sử dụng stochastic gradient descent. Theo các nghiên cứu trước về phát triển mạng [14], Batch Normalization [22] chỉ được sử dụng trước mỗi khối để so sánh công bằng.

ImageNet [10] chứa 1K loại với 1.2M hình ảnh để huấn luyện, 50K để xác thực, và 100K để kiểm tra. Chúng tôi huấn luyện các mô hình ResNet-50 [17] trong 90 epoch với tốc độ học 0.1, giảm 0.1 tại 30, 60, và 80 epoch. Chúng tôi sử dụng stochastic gradient descent với momentum 0.9, kích thước batch 256 với cross entropy loss. Trước khi phát triển, mỗi lớp có một nửa số kênh của mạng cuối cùng.

4.2. Kết quả
Bảng 1 báo cáo hiệu suất phát triển một mạng neural trên CIFAR-10, CIFAR-100, và ImageNet. Chúng tôi thấy rằng MixtureGrowth có thể có hiệu suất tốt hơn so với huấn luyện kiến trúc mục tiêu từ đầu (Mạng mục tiêu) với một nửa FLOPs trên tập dữ liệu CIFAR-100. Đối với ImageNet, cách tiếp cận của chúng tôi có hiệu suất tương đương với Mạng mục tiêu chỉ sử dụng một phần ba FLOPs. Khi xem xét Mục tiêu ở cùng FLOPs, nó vượt trội Large Match (tức là Mạng Mục tiêu nhưng được huấn luyện với cùng FLOPs như các baseline phát triển mạng) bằng một biên độ đáng kể. Do đó, cách tiếp cận của chúng tôi có khả năng sử dụng các mô hình nhỏ hiện có để có được một mô hình hiệu suất cao trong thời gian ít hơn so với một mạng tiêu chuẩn.
2https://github.com/google-research/growneuron
5

--- TRANG 6 ---
Phương pháp CIFAR-10 [25] CIFAR-100 [25] FLOPs Norm ImageNet [10] FLOPs Norm
(a)Large (Mạng mục tiêu) 96.88% 81.39% 1.00X 76.49% 1.00X
Large Match 96.62% 80.95% 0.45X 73.85% 0.35X
Small 96.53% 80.29% 0.25X 72.48% 0.25X
Small Match 96.60% 80.30% 0.45X 72.89% 0.35X
(b)Random 95.61% 79.08% 0.45X 71.47% 0.35X
Net2Net [7] 95.47% 79.20% 0.45X 72.29% 0.35X
Firefly [46] 95.62% 78.90% 0.45X 71.30% 0.35X
GradMax [14] 95.73% 79.05% 0.45X 71.73% 0.35X
MixtureGrowth w/o fusion 96.05% 79.44% 0.45X 73.02% 0.35X
MixtureGrowth (của chúng tôi) 96.66% 81.50% 0.45X 74.51% 0.35X

Bảng 1. So sánh phát triển mạng sử dụng độ chính xác top-1 trung bình trên ba lần chạy. (a)Hiệu suất huấn luyện kiến trúc mạng mục tiêu (sau phát triển) (Large) từ đầu (WRN-28-10 cho cả hai tập dữ liệu CIFAR và ResNet-50 cho ImageNet), cũng như hiệu suất của kiến trúc nhỏ (trước phát triển) (Small) (WRN-28-5 cho cả hai tập dữ liệu CIFAR và ResNet-50 với một nửa số kênh như mạng lớn cho ImageNet); (b)Hiệu suất của các phương pháp phát triển mạng, trong đó cách tiếp cận của chúng tôi vượt trội SOTA hiện tại.

0.30 0.35 0.40 0.45 0.50 0.55
FLOPs Norm798081Accuracy (%)
Gradmax
Fireﬂy
Random
Net2Net
MixtureGrowth (của chúng tôi)
Mạng Mục tiêu
(a)Các phương pháp phát triển theo FLOPs
0.26 0.30 0.36 0.40 0.45
FLOPs Norm74767880Accuracy (%)Orthogonal coef.
Random coef.
Coef. copying (b)Khởi tạo hệ số

Hình 3. Độ chính xác Top-1 trên CIFAR-100. (a)Các phương pháp phát triển mạng dưới FLOPs khác nhau. Đường đứt nét biểu diễn hiệu suất của Mạng mục tiêu được huấn luyện từ đầu sử dụng lịch trình huấn luyện đầy đủ. Chúng tôi thấy MixtureGrowth không chỉ vượt trội Random, FireFly [46], và GradMax [14], mà còn vượt trội mạng mục tiêu với một nửa FLOPs; (b)Hiệu suất của các phương pháp khởi tạo hệ số khác nhau (Mục 3.3) từ điểm phát triển. Xem Mục 4.3 để thảo luận.

Các nghiên cứu trước về phát triển mạng (Bảng 1b) gặp khó khăn trong việc tăng hiệu suất sau khi phát triển khi so sánh với việc đơn giản huấn luyện một mạng nhỏ thông thường (Bảng 1a), ký hiệu là Small và Small Match (tức là mạng Small nhưng được huấn luyện với cùng FLOPs như các baseline phát triển mạng). Lưu ý rằng so với các nghiên cứu trước, chúng tôi khám phá các kiến trúc mạng lớn hơn trong các thí nghiệm của chúng tôi (ví dụ, ResNet-50 trên ImageNet trong nghiên cứu của chúng tôi so với VGG11 [37] được đánh giá trong [14]), cho thấy các phương pháp trước này gặp thách thức trong việc tổng quát hóa cho các mạng lớn hơn. Ngược lại, phương pháp của chúng tôi có thể cho phép mô hình có được sự cải thiện sau khi phát triển và thậm chí vượt qua mô hình mục tiêu trên CIFAR-100 với một nửa FLOPs. Khi so sánh với các phương pháp phát triển khác, cách tiếp cận MixtureGrowth của chúng tôi có được khoảng 1% hiệu suất cao hơn trên CIFAR-10, 2.5% trên CIFAR-100, và 2% trên ImageNet.

Hình 3a chứng minh hiệu quả và tính hiệu quả của phương pháp của chúng tôi so với bốn baseline: Random [5], Net2Net [7], Firefly [46], và Gradmax [14] qua các ngân sách FLOPs khác nhau. Sử dụng WRN-28-10 làm mạng mục tiêu, phương pháp của chúng tôi liên tục vượt trội các baseline với cùng ngân sách trong khi ngang bằng hoặc thậm chí vượt trội mạng mục tiêu với một nửa FLOPs.

Bảng 3 so sánh phát triển trong một bước duy nhất sử dụng MixtureGrowth với phát triển nhiều lần với lịch trình phát triển dài được sử dụng bởi các nghiên cứu trước [14]. Trong thiết lập này, chúng tôi không chỉ có được sự tăng hiệu suất đáng kể so với hiện tại tốt nhất, mà còn làm như vậy trong một nửa FLOPs. Các FLOPs bổ sung cần thiết bởi các nghiên cứu trước là do chi phí phát triển của chúng (được thảo luận ở đầu Mục 4).

4.3. Thảo luận
Chúng ta nên khởi tạo các hệ số tuyến tính như thế nào? Câu hỏi nghiên cứu đầu tiên mà chúng tôi khám phá là cách khởi tạo hiệu quả các hệ số được sử dụng để tạo ra trọng số lớp mới sau
6

--- TRANG 7 ---
Thời gian Huấn luyện Mạng Thứ hai (FLOPs)
Tập dữ liệu 0.0 0.4 0.5 0.6 0.7 0.8 0.9 1.0
CIFAR-100 79.24% 80.16% 80.46% 81.08% 80.9% 81.09% 81.44 % 81.27
ImageNet 73.02% 73.74% 73.75% 74.09% 74.16% 74.51 % 74.33% 74.31

Bảng 2. Ablation về các điểm phát triển mà chúng tôi dừng huấn luyện mạng thứ hai để huấn luyện mô hình đầy đủ. Cột đầu tiên là trường hợp cực đoan mà chúng tôi phát triển mà không huấn luyện mạng thứ hai (tức là MixtureGrowth w/out fusion). Phần còn lại cho thấy hiệu suất của MixtureGrowth trên các điểm phát triển khác nhau. Thời gian Huấn luyện Mạng Thứ hai chỉ ra tỷ lệ FLOPs huấn luyện đầy đủ được sử dụng để huấn luyện mạng thứ hai, ví dụ, 0.5 có nghĩa là chúng tôi huấn luyện mạng thứ hai một nửa huấn luyện đầy đủ trước khi phát triển. Chúng tôi thấy rằng phát triển muộn mà chúng tôi gần như huấn luyện đầy đủ mạng thứ hai trước khi phát triển có được hiệu suất tốt nhất (in đậm). Tất cả các lần chạy báo cáo độ chính xác top-1 ở cùng 0.35X FLOPs.

khi phát triển. Trong Mục 3.3 chúng tôi mô tả ba phương pháp khác nhau để khởi tạo các hệ số mới này. Chúng tôi báo cáo hiệu suất của chúng trong Bảng 4, nơi chúng tôi thấy khởi tạo trực giao, giúp thúc đẩy tính đa dạng nhiều nhất trong các trọng số được tạo ra, vượt trội cả khởi tạo ngẫu nhiên và sao chép hệ số. Đáng chú ý, khởi tạo ngẫu nhiên cũng hoạt động tốt và tiếp cận hiệu suất của khởi tạo trực giao. Lưu ý, đối với kết quả sao chép hệ số, chúng tôi cũng đã thử nghiệm thêm một ít nhiễu ngẫu nhiên nhỏ để phá vỡ tính đối xứng, nhưng nó không ảnh hưởng đáng kể đến hiệu suất.

Để có cái nhìn sâu sắc về cách các phương pháp khởi tạo hệ số hoạt động tại bước phát triển, chúng tôi vẽ độ chính xác top-1 bắt đầu từ bước phát triển cho đến cuối trong Hình 3b. Chúng tôi thấy rằng cả trực giao và ngẫu nhiên đều có thể duy trì hiệu suất nhiệm vụ mặc dù được cung cấp trọng số mới tại thời điểm phát triển và bắt đầu cải thiện hiệu suất với mạng lớn. Tuy nhiên, sao chép hệ số gặp khó khăn tại bước phát triển vì hiệu suất của nó giảm đáng kể. Chúng tôi nghi ngờ điều này có thể do sự dư thừa gây ra bởi việc tái sử dụng và sao chép cùng các hệ số, làm cho việc học các trọng số thông tin mới trở nên khó khăn.

Phát triển bằng cách kết hợp hai mô hình nhỏ có hiệu quả hơn so với phát triển từ một mô hình duy nhất không? Câu hỏi thứ hai mà chúng tôi khám phá là liệu chúng ta có thể tận dụng việc có hai mạng nhỏ trong quá trình phát triển hay không. Có một cặp mạng sẽ cải thiện tính mạnh mẽ như đã thảo luận trong Mục 3.2. Bảng 1 so sánh phát triển có và không có kết hợp mô hình (2 hàng cuối), nơi chúng tôi thấy rằng kết hợp hai mô hình có được sự tăng nhất quán sau khi chuẩn hóa theo FLOPs.

Điểm phù hợp trong quá trình huấn luyện để phát triển là gì? Câu hỏi trước làm sáng tỏ tầm quan trọng của việc có một mô hình thứ hai được huấn luyện. Tuy nhiên, chúng ta nên huấn luyện mạng thứ hai trong bao lâu vẫn là câu hỏi mở. Nếu chúng ta huấn luyện mạng thứ hai trong một lịch trình dài, thì không còn nhiều ngân sách để tối ưu hóa các tham số của mạng lớn. Ngược lại, nếu chúng ta phát triển nó quá sớm, thì mạng thứ hai không được huấn luyện đầy đủ, từ đó có thể dẫn đến sự giảm hiệu suất tổng thể. Từ góc độ này, chúng ta có thể coi MixtureGrowth w/out fusion như một trường hợp cực đoan của phát triển sớm mà chúng ta không huấn luyện mạng thứ hai mà thay vào đó phát triển

081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0(a)
081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0 (b)
081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0
(c)
081624324048566472808896
training epoch0
8
16
24
32
40
48
56
64
72
80
88
96training epoch
0.00.20.40.60.81.0 (d)

Hình 4. CKA của các mô hình khác nhau tại lớp tích chập cuối cùng theo thời gian. (a) Mạng Mục tiêu: Một mẫu mượt mà trở nên sáng hơn dọc theo đường chéo về phía cuối huấn luyện khi huấn luyện từ đầu; (b)Random: Mẫu tối và nhiễu hơn so với mục tiêu khi phát triển với trọng số ngẫu nhiên; (c)MixtureGrowth w/out fusion: Không có kết hợp mô hình, mẫu có phần tương tự như Mục tiêu, nhưng các biểu diễn giữa các epoch có độ tương đồng nhỏ hơn; (d)MixtureGrowth: Trong mô hình đầy đủ của chúng tôi, chúng tôi thấy một sự thay đổi pha rõ ràng tại điểm phát triển (epoch 63), gợi ý việc học nhanh chóng xảy ra sau khi kết hợp 2 mô hình nhỏ.

thành một mạng đầy đủ trực tiếp. Bảng 2 cho thấy hiệu suất của các điểm phát triển khác nhau dưới cùng ngân sách FLOPs trên các tập dữ liệu CIFAR-100 và ImageNet. Chúng tôi quan sát rằng phát triển muộn mà mạng thứ hai được huấn luyện gần như đầy đủ cho hiệu suất tốt nhất. Điều này xác nhận giả định của chúng tôi rằng phát triển với hai mạng mạnh mẽ hơn.

4.4. Phân tích đặc trưng
Chúng tôi sử dụng Centered Kernel Alignment (CKA) [24] để phân tích sự tương đồng của các đặc trưng qua quá trình huấn luyện. Để phân tích này, chúng tôi thiết lập MixtureGrowth bằng cách huấn luyện 2 mạng nhỏ trong 63 epoch, sau đó huấn luyện mạng đã phát triển đầy đủ sau đó cho đến khi đạt 100 epoch tổng cộng. Đối với Mixture-
7

--- TRANG 8 ---
Phương pháp Top-1 Acc. FLOPs Norm
(a) Random 79.12% 0.77X
Firefly [46] 78.94% 0.77X
GradMax [14] 79.24% 0.77X
(b) MixtureGrowth 81.50% 0.45X

Bảng 3. So sánh phát triển mạng trên CIFAR-100 .(a)Sử dụng lịch trình phát triển dài được sử dụng bởi các tác giả của GradMax; (b)MixtureGrowth với phát triển đơn để tham khảo.

Phương pháp Top-1 Accuracy
Random coefficients 81.41%
Coefficient copying 79.05%
Orthogonal coefficients 81.50 %

Bảng 4. Ablation về các phương pháp khởi tạo hệ số (được mô tả trong Mục 3.3) cho MixtureGrowth trên CIFAR-100. Khởi tạo trực giao hoạt động tốt nhất, gợi ý tầm quan trọng của việc có các trọng số được khởi tạo độc lập.

0246810121416182022242628
Small Net 1's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net 2's layer
0.00.20.40.60.81.0
(a)
0246810121416182022242628
Large Net's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net's layer
0.00.20.40.60.81.0 (b)
0246810121416182022242628
Large Net's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net 1's layer
0.00.20.40.60.81.0
(c)
0246810121416182022242628
Large Net's layer0
2
4
6
8
10
12
14
16
18
20
22
24
26
28Small Net 2's layer
0.00.20.40.60.81.0 (d)

Hình 5. CKA giữa các lớp của MixtureGrowth. (a) MixtureGrowth: Mạng Nhỏ 1 so với Mạng Nhỏ 2; (b)MixtureGrowth w/o model fusion: Mạng Lớn so với Mạng Nhỏ 1; (c)MixtureGrowth với model fusion: Mạng Lớn so với Mạng Nhỏ 1; (d)MixtureGrowth với model fusion: Mạng Lớn so với Mạng Nhỏ 2; Chúng tôi phân tích sự tương đồng của các biểu diễn cho mỗi cặp lớp của WRN-28-10 trên tập dữ liệu CIFAR-100. Xem Mục 4.4 để thảo luận.

Growth w/out fusion, chỉ một mô hình nhỏ được huấn luyện trong 63 epoch, và sau đó chúng tôi phát triển. Mạng mục tiêu đề cập đến một mạng đầy đủ được huấn luyện từ đầu trong 100 epoch.

Hình 4 cho thấy các đặc trưng phát triển như thế nào trong quá trình huấn luyện theo thời gian sử dụng quy trình CKA được mô tả ở trên. Trong Hình 4d, minh họa sự tương đồng đặc trưng, một sự thay đổi pha rất rõ ràng trong độ tương đồng có thể nhìn thấy tại bước phát triển ở epoch 63. Điều này chỉ ra rằng việc học tại điểm phát triển rất nhanh, gợi ý chúng tôi hiệu quả kết hợp hai mô hình độc lập trong bước phát triển. Khởi tạo trọng số ngẫu nhiên (Hình 4b) phản ánh một sự thay đổi tương tự được thấy trong Hình 4d, nhưng với nhiều mẫu nhiễu hơn. Ngược lại, huấn luyện mạng mục tiêu từ đầu (Hình 4a), và MixtureGrowth w/out fusion (Hình 4c) có một sự chuyển đổi dần dần hơn từ đầu huấn luyện về phía cuối mà không có các góc phần tư rõ ràng như MixtureGrowth. Do đó, việc học trong quá trình phát triển không nhanh chóng, mà dần dần thay vào đó. Điều này cung cấp cái nhìn sâu sắc về tính hiệu quả của cách tiếp cận của chúng tôi, đặc biệt với ngân sách FLOP thấp.

Hình 5 minh họa cách các mạng đã thay đổi từ điểm phát triển đến biểu diễn cuối cùng với CKA. Trong Hình 5a, chúng tôi so sánh hai mạng nhỏ ngay trước khi phát triển. Chúng tôi quan sát rằng một số lớp qua các mạng có các đặc trưng tương tự (màu sáng trên đường chéo, đại diện cho các đặc trưng rất tương tự từ cùng một lớp), và một số lớp đầu và cuối khác nhau (màu tối). Chúng tôi thấy một mẫu tương tự phát triển từ một mô hình duy nhất (tức là, MixtureGrowth w/out fusion, Hình 5b). Điều này gợi ý rằng sau khi phát triển (w/o fusion), mạng lớn học các đặc trưng bổ sung khác với mạng nhỏ theo cách tương tự như hai mạng nhỏ được huấn luyện độc lập. Ngược lại, trong MixtureGrowth w/fusion, từ Hình 5c và 5d, chúng tôi thấy mạng đầy đủ chia sẻ nhiều sự tương đồng hơn với mỗi mạng nhỏ trong các lớp đầu và cuối so với MixtureGrowth w/out fusion. Do đó, ít việc học xảy ra sau khi kết hợp hai mạng nhỏ, và huấn luyện nhẹ sau khi phát triển là đủ.

5. Kết luận
Trong bài báo này, chúng tôi đề xuất MixtureGrowth, một phương pháp mới để phát triển mạng với trọng số được xây dựng từ các tổ hợp tuyến tính của các mẫu được chia sẻ sử dụng kết hợp mô hình để tăng tính đa dạng trong các mẫu được huấn luyện trước của chúng tôi. Chúng tôi phân tích cách tiếp cận của chúng tôi bằng cách trả lời một số câu hỏi về tác động của khởi tạo hệ số tuyến tính và tìm một điểm phát triển tốt. Chúng tôi thấy rằng không có kết hợp mô hình, MixtureGrowth đã đạt được độ chính xác tương đương hoặc cao hơn so với hiện tại tốt nhất. Tuy nhiên, với kết hợp, lợi ích so với các nghiên cứu trước tăng lên, dẫn đến cải thiện 2-2.5% so với hiện tại tốt nhất trên CIFAR-100 và ImageNet. Chúng tôi tin rằng MixtureGrowth đại diện cho một bước tiến thú vị trong việc học phát triển mạng bằng cách tận dụng trộn mẫu để tạo ra trọng số mới.

Lời cảm ơn Tài liệu này dựa trên công việc được hỗ trợ, một phần, bởi DARPA dưới số thỏa thuận HR00112020054 và NSF dưới giải thưởng DBI-2134696. Bất kỳ ý kiến, phát hiện, và kết luận hoặc khuyến nghị nào đều của (các) tác giả và không nhất thiết phản ánh quan điểm của các cơ quan hỗ trợ.
8

--- TRANG 9 ---
Tài liệu tham khảo
[1] Timur Ash. Dynamic node creation in backpropagation networks. Connection science , 1(4):365–375, 1989. 2
[2] Hessam Bagherinezhad, Mohammad Rastegari, and Ali Farhadi. Lcnn: Lookup-based convolutional neural network. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2017. 1, 3, 4
[3] Pouya Bashivan, Mark Tensen, and James J DiCarlo. Teacher guided architecture search. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5320–5329, 2019. 1
[4] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. Advances in neural information processing systems , 19, 2006. 2
[5] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680 , 2019. 6
[6] Lucas Beyer, Xiaohua Zhai, Am ´elie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10925–10934, 2022. 3
[7] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641 , 2015. 2, 5, 6
[8] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning , pages 160–167, 2008. 2
[9] Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. Nest: A neural network synthesis tool based on a grow-and-prune paradigm. IEEE Transactions on Computers , 68(10):1487–1497, 2019. 2
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. 5, 6, 11
[11] Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems , pages 1–15. Springer, 2000. 4
[12] Xuanyi Dong and Yi Yang. One-shot neural architecture search via self-evaluated template network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3681–3690, 2019. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. 5
[14] Utku Evci, Max Vladymyrov, Thomas Unterthiner, Bart van Merri ¨enboer, and Fabian Pedregosa. Gradmax: Growing neural networks using gradient information. arXiv preprint arXiv:2201.05125 , 2022. 1, 2, 3, 4, 5, 6, 8, 11, 12
[15] Scott Fahlman and Christian Lebiere. The cascade-correlation learning architecture. Advances in neural information processing systems , 2, 1989. 2
[16] J Gou, B Yu, SJ Maybank, and D Tao. Knowledge distillation: A survey. corr. arXiv preprint arXiv:2006.05525 , 2020. 1
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016. 5, 11
[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2(7), 2015. 1, 3
[19] Geoffrey E Hinton. Deep belief networks. Scholarpedia , 4(5):5947, 2009. 2
[20] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation , 18(7):1527–1554, 2006. 2
[21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res. , 22(241):1–124, 2021. 1
[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML) , 2015. 5, 11
[23] Manas R Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K Adams, Pranav Khaitan, Jiahui Liu, and Quoc V Le. Neural input search for large scale recommendation models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 2387–2397, 2020. 1
[24] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.. Similarity of neural network representations revisited. In International Conference on Machine Learning , pages 3519–3529. PMLR, 2019. 7
[25] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 5, 6, 11
[26] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems , 30, 2017. 4
[27] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems , 2, 1989. 3
[28] Kaitlin Maile, Emmanuel Rachelson, Herv ´e Luga, and Dennis George Wilson. When, where, and how to add new neurons to anns. In International Conference on Automated Machine Learning , pages 18–1. PMLR, 2022. 2
[29] Elliot Meyerson and Risto Miikkulainen. Beyond shared hierarchies: Deep multitask learning through soft layer ordering. In International Conference on Learning Representations , 2018. 3
[30] Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli ´c, and Edoardo Maria Ponti. Modular deep learning. arXiv preprint arXiv:2302.11529 , 2023. 3
9

--- TRANG 10 ---
[31] Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, and Kate Saenko. Neural parameter allocation search. In International Conference on Learning Representations (ICLR) , 2022. 1, 3, 4
[32] Adria Ruiz and Jakob Verbeek. Anytime inference with distilled hierarchical neural ensembles. In Proceedings of the AAAI Conference on Artificial Intelligence , 2021. 5
[33] Mark Sandler, Jonathan Baccash, Andrey Zhmoginov, and Andrew Howard. Non-discriminative data or weak model? on the relative importance of data and model resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops , pages 0–0, 2019. 3
[34] Pedro Savarese and Michael Maire. Learning implicitly recurrent CNNs through parameter sharing. In International Conference on Learning Representations , 2019. 1, 3, 4, 5
[35] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013. 5
[36] Zhiqiang Shen and Marios Savvides. Meal v2: Boosting vanilla resnet-50 to 80 arXiv preprint arXiv:2009.08453 , 2020. 3
[37] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. 6, 12
[38] Nikko Str ¨om. Sparse connection and pruning in large dynamic artificial neural networks. In Fifth European Conference on Speech Communication and Technology , 1997. 3
[39] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2820–2828, 2019. 1
[40] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning , pages 6105–6114. PMLR, 2019. 5
[41] Piotr Teterwak, Soren Nelson, Nikoli Dryden, Dina Bashkirova, Kate Saenko, and Bryan A. Plummer. Learning to compose superweights for neural parameter allocation search. In IEEE Winter Conference on Applications of Computer Vision (WACV) , 2024. 1, 3, 4
[42] Georg Thimm and Emile Fiesler. Evaluating pruning methods. In Proceedings of the International Symposium on Artificial neural networks , pages 20–25, 1995. 3
[43] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L ´eon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research , 11(12), 2010. 2
[44] Huan Wang, Can Qin, Yue Bai, Yulun Zhang, and Yun Fu. Recent advances on neural network pruning at initialization. arXiv e-prints , pages arXiv–2103, 2021. 1
[45] Wei Wen, Feng Yan, Yiran Chen, and Hai Li. Autogrow: Automatic layer growing in deep convolutional networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 833–841, 2020. 2
[46] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks. Advances in Neural Information Processing Systems , 33:22373–22383, 2020. 1, 2, 3, 4, 5, 6, 8
[47] Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural architectures. Advances in neural information processing systems , 32, 2019. 2
[48] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. In Advances in Neural Information Processing Systems , 2019. 1, 3, 4
[49] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016 . British Machine Vision Association, 2016. 5, 11, 12
[50] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. InEuropean conference on computer vision , pages 94–108. Springer, 2014. 2
[51] Andrey Zhmoginov, Dina Bashkirova, and Mark Sandler. Compositional models: Multi-task learning and knowledge transfer with modular networks. arXiv preprint arXiv:2107.10963 , 2021. 3
10

--- TRANG 11 ---
Phụ lục
6. Chi tiết Triển khai
Chúng tôi thiết lập các thí nghiệm của chúng tôi trên các nhiệm vụ phân loại hình ảnh trong đó mục tiêu là nhận biết đối tượng trong hình ảnh. Điều này được đánh giá bằng độ chính xác top 1, tức là tỷ lệ phần trăm các lần mô hình có thể dự đoán chính xác loại của hình ảnh. Chúng tôi đánh giá phương pháp của chúng tôi và các baseline trên CIFAR-10 và CIFAR100 [25], bao gồm 60K hình ảnh của 10 và 100 loại tương ứng, và ImageNet [10], chứa 1.2M hình ảnh của 1,000 loại.

6.1. CIFAR-10 và CIFAR-100
CIFAR-10 và CIFAR100 [25] bao gồm 60K hình ảnh của 10 và 100 loại tương ứng. Trong cả hai tập dữ liệu, chúng tôi chia dữ liệu thành 50K hình ảnh để huấn luyện và 10K hình ảnh để kiểm tra. Chúng tôi thực hiện thí nghiệm sử dụng kiến trúc Wide Residual Network (WRN) [49], là phiên bản sửa đổi của mạng residual [17]. Chúng tôi ký hiệu mô hình WRN như WRN-n-k, trong đó n là tổng số lớp tích chập, và k là hệ số mở rộng. WRN tăng chiều rộng của mỗi lớp bằng hệ số k trong khi giảm độ sâu để cải thiện hiệu suất của mạng residual truyền thống. Trong thí nghiệm này, chúng tôi chọn WRN-28-10 được điều chỉnh từ Savarese et al.3. Mạng được huấn luyện trong tổng cộng 200 epoch trên một GPU duy nhất (Nvidia Titan V 12G) sử dụng stochastic gradient descent với momentum 0.9, tốc độ học 0.1, được giảm xuống 0 với lịch trình cosine, weight decay 5×10−4, và kích thước batch 128. Đối với hàm loss, chúng tôi sử dụng cross entropy loss. Để có so sánh công bằng, chúng tôi theo cùng thiết lập trong nghiên cứu trước [14], trong đó Batch Normalization [22] chỉ được sử dụng trước mỗi khối. Các tham số cụ thể cho phương pháp của chúng tôi được đặt như sau. Tổng ngân sách tham số cho trộn mẫu được đặt là 36.5M, bằng với số lượng tham số trong WRN-28-10 (mạng mục tiêu). Lưu ý rằng ngân sách tham số trong phương pháp của chúng tôi có thể linh hoạt, tức là, nó có thể có thiết lập mà mô hình có ít hoặc nhiều tham số hơn nhờ vào các sơ đồ trộn mẫu, xem Mục 8 để so sánh các ngân sách tham số khác nhau. Đối với mỗi lớp, số lượng mẫu được đặt là 2. Ngoại trừ lớp đầu tiên (conv 1) và cuối cùng (Fully connected), tất cả các hệ số alpha đều có thể huấn luyện. Để huấn luyện MixtureGrowth, cho một mô hình WRN-28-5 được huấn luyện, chúng tôi bắt đầu bằng việc huấn luyện một mô hình WRN-28-5 khác trong e epoch, trong đó 0≤e < 200 là một siêu tham số. Sau đó, các mô hình nhỏ này được kết hợp và sử dụng để khởi tạo cho việc phát triển thành mô hình đầy đủ (tức là, WRN-28-10), lớn hơn 4 lần về số lượng trọng số. Chúng tôi huấn luyện mô hình đầy đủ thêm một số epoch, tùy thuộc vào ngân sách FLOPs.

3https://github.com/lolemacs/soft-sharing

Đáng chú ý rằng GradMax [14] thu nhỏ lớp tích chập đầu tiên tại mỗi khối xuống 4, dẫn đến có một mạng nhỏ với các lớp rộng và hẹp xen kẽ. Ngược lại, MixtureGrowth giảm cả chiều rộng đầu vào và đầu ra trong mỗi lớp xuống 2 để đạt được phiên bản nhỏ hơn của mạng. Mặc dù các mạng nhỏ trong phương pháp của chúng tôi và các baseline hơi khác nhau do thiết lập của mỗi phương pháp, tất cả chúng đều có cùng FLOPs (khoảng 0.25X FLOPs Norm) để so sánh công bằng.

6.2. ImageNet
ImageNet [10] chứa 1,000 loại với 1.2M hình ảnh để huấn luyện, 50K để xác thực, và 100K để kiểm tra. Chúng tôi huấn luyện mô hình sử dụng kiến trúc ResNet-50 [17] trong 90 epoch trên 4 GPU (NVIDIA RTX A6000 48G) với tốc độ học 0.1, được giảm 0.1 tại 30, 60, và 80 epoch với bộ lập lịch cosine. Chúng tôi sử dụng stochastic gradient descent với momentum 0.9, kích thước batch 256, và cross entropy làm hàm loss. Trong MixtureGrowth, chúng tôi chia sẻ mẫu giữa hai lớp liên tiếp nếu chúng có cùng kích thước. Tổng tham số cho trộn mẫu trong phương pháp của chúng tôi là 25.6M, bằng với mô hình mục tiêu. Đáng chú ý rằng trong GradMax [14], mạng nhỏ mà họ sử dụng yêu cầu nhiều FLOPs hơn so với mạng được sử dụng trong phương pháp của chúng tôi (0.3X FLOPs Norm cho GradMax so với 0.26X FLOPs Norm cho chúng tôi), do thiết lập của các tác giả được mô tả trong phần trước. Tuy nhiên, tất cả các thiết lập khác vẫn giống như những gì đã đề cập cho CIFAR ở trên.

Để huấn luyện phương pháp của chúng tôi, chúng tôi bắt đầu với một mạng được huấn luyện có đầu vào và đầu ra của mỗi lớp bằng một nửa kích thước của mạng mục tiêu. Chúng tôi huấn luyện một mạng khác có kích thước bằng nhau trong e epoch, trong đó 0≤e < 90 là một siêu tham số. Sau đó, các mạng nhỏ này được kết hợp để phát triển thành một mạng đầy đủ. Chúng tôi huấn luyện mạng đã phát triển đầy đủ thêm một số epoch cho đến khi hết ngân sách FLOPs.

7. Thí nghiệm với VGG-11
Bên cạnh WideResnet [49], chúng tôi thực hiện thí nghiệm trên các họ kiến trúc khác nhau. Bảng 5 cho thấy hiệu suất của phương pháp chúng tôi và các baseline trên tập dữ liệu CIFAR-100 khi phát triển từ 2 mạng nhỏ. Firefly gặp khó khăn trong việc hội tụ, dẫn đến bị loại khỏi bảng. Tuy nhiên, Random và GradMax cho kết quả tương tự, với khoảng 48% độ chính xác. Phương pháp của chúng tôi hoạt động tốt hơn các baseline với biên độ lớn (∼7.5%).

8. Ngân sách tham số thấp
Trộn mẫu cho phép chúng tôi chia sẻ tham số qua các lớp, giảm số lượng tham số trong mạng mà không cần thay đổi kiến trúc của nó (như chiều rộng
11

--- TRANG 12 ---
0.2 0.3 0.4 0.5 0.6 0.7
Number of Parameters (Norm)95.295.495.695.8Accuracy (%)
Mô hình Mục tiêu
MixtureGrowth (của chúng tôi)(a) CIFAR-10
0.2 0.3 0.4 0.5 0.6 0.7
Number of Parameters (Norm)77.578.078.579.079.5Accuracy (%)
Mô hình Mục tiêu
MixtureGrowth (của chúng tôi) (b) CIFAR-100

Hình 6. So sánh MixtureGrowth với mô hình mục tiêu dưới thiết lập tham số thấp. Hiệu suất được đo bằng độ chính xác top-1 trung bình trên 3 lần chạy.

Hình 7. MixtureGrowth với các phương pháp phát triển khác nhau: Minh họa Phát triển Dọc và Phát triển Ngang. Các góc phần tư chia sẻ mẫu nhưng có các hệ số tổ hợp tuyến tính khác nhau được trình bày dưới dạng các sắc thái khác nhau của cùng một màu.

Bảng 5. So sánh phát triển mạng trên CIFAR-100 sử dụng kiến trúc VGG-11 [37]. (a) Hiệu suất của các mô hình baseline trong đó Firefly không được bao gồm do không hội tụ (b) Hiệu suất của MixtureGrowth

Phương pháp Top-1 Acc. Total FLOPs Norm
(a) Random 48.52% 1.3X
GradMax [14] 48.79% 1.3X
(b) MixtureGrowth (của chúng tôi) 56.17% 0.6X

và độ sâu). Để so sánh phương pháp của chúng tôi với các mô hình mục tiêu dưới ngân sách tham số thấp, chúng tôi giảm chiều rộng của các mô hình mục tiêu sao cho số lượng tham số của chúng khớp với phương pháp của chúng tôi. Hình 6a minh họa hiệu suất của MixtureGrowth khi so sánh với các mô hình mục tiêu nhỏ trên CIFAR-10 với cùng số lượng tham số, trong đó

Bảng 6. So sánh phương pháp phát triển khác nhau của MixtureGrowth trên CIFAR-100 sử dụng kiến trúc WRN-28-10. Chúng tôi báo cáo độ chính xác trung bình của ba lần chạy cho mỗi phương pháp.

Phương pháp Top-1 Acc. FLOPs Norm
Phát triển ngang 80.66% 0.35X
Phát triển dọc 80.82 % 0.35X

MixtureGrowth liên tục vượt trội các mô hình mục tiêu dưới ngân sách tham số thấp. Chúng tôi thấy quan sát tương tự trên tập dữ liệu CIFAR-100, như được hiển thị trong 6b. Chúng tôi sử dụng kiến trúc WRN [49] để so sánh cả hai tập dữ liệu.

9. Phát triển Ngang và Dọc
Tại bước phát triển, chúng tôi phát triển từ 2 mạng nhỏ được huấn luyện thành một mạng lớn. Cho các mạng được huấn luyện là 2 góc phần tư đường chéo, có 2 cách để mở rộng nó thành 4 góc phần tư. Lựa chọn đầu tiên là Phát triển dọc, trong đó các góc phần tư có cùng đầu ra chia sẻ mẫu (Hình 7, trên phải). Cách khác là phát triển ngang trong đó các góc phần tư có cùng đầu vào sử dụng cùng tập hợp mẫu (Hình 7, dưới phải). Bảng 6 so sánh hiệu suất của 2 chiến lược phát triển trên tập dữ liệu CIFAR-100. Chúng tôi nhận thấy rằng Phát triển dọc vượt trội một chút so với Phát triển ngang về hiệu suất.
12
