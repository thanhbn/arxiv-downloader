# 2202.02643.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/initialization/2202.02643.pdf
# Kích thước file: 3998713 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2022
HIỆU QUẢ KHÔNG HỢP LÝ CỦA VIỆC CẮT TỈA NGẪU NHIÊN: SỰ TRỞ LẠI CỦA ĐƯỜNG CƠ SỞ NGÂY THƠ NHẤT CHO HUẤN LUYỆN THƯA

Shiwei Liu1, Tianlong Chen2, Xiaohan Chen2, Li Shen3
Decebal Constantin Mocanu1,4,Zhangyang Wang2,Mykola Pechenizkiy1,
1Trường Đại học Công nghệ Eindhoven,2Trường Đại học Texas tại Austin
3Học viện Khám phá JD,4Trường Đại học Twente,
fs.liu3,m.pechenizkiy g@tue.nl ,
ftianlong.chen,xiaohan.chen,atlaswang g@utexas.edu ,
d.c.mocanu@utwente.nl ,mathshenli@gmail.com

TÓM TẮT
Cắt tỉa ngẫu nhiên có lẽ là cách ngây thơ nhất để đạt được độ thưa trong mạng nơ-ron, nhưng đã được coi là không cạnh tranh bởi việc cắt tỉa sau huấn luyện hoặc huấn luyện thưa. Trong bài báo này, chúng tôi tập trung vào huấn luyện thưa và nêu bật một phát hiện có thể phản trực giác, rằng cắt tỉa ngẫu nhiên tại khởi tạo có thể khá mạnh mẽ cho việc huấn luyện thưa của các mạng nơ-ron hiện đại. Không có bất kỳ tiêu chí cắt tỉa tinh vi nào hoặc cấu trúc thưa được theo đuổi cẩn thận, chúng tôi chứng minh thực nghiệm rằng huấn luyện thưa một mạng được cắt tỉa ngẫu nhiên từ đầu có thể khớp với hiệu suất của tương đương dày đặc của nó. Có hai yếu tố chính góp phần vào sự hồi sinh này: (i) kích thước mạng quan trọng: khi các mạng dày đặc ban đầu trở nên rộng hơn và sâu hơn, hiệu suất của việc huấn luyện một mạng thưa được cắt tỉa ngẫu nhiên sẽ nhanh chóng tăng lên để khớp với mạng dày đặc tương đương, ngay cả ở tỷ lệ thưa cao; (ii) tỷ lệ thưa theo lớp thích hợp có thể được chọn trước cho huấn luyện thưa, điều này cho thấy là một yếu tố tăng cường hiệu suất quan trọng khác. Đơn giản như vẻ ngoài, một mạng con được cắt tỉa ngẫu nhiên của Wide ResNet-50 có thể được huấn luyện thưa để vượt trội hơn một Wide ResNet-50 dày đặc, trên ImageNet. Chúng tôi cũng quan sát thấy các mạng được cắt tỉa ngẫu nhiên như vậy vượt trội hơn các đối tác dày đặc trong các khía cạnh thuận lợi khác, chẳng hạn như phát hiện ngoài phân phối, ước lượng không chắc chắn và độ bền chống đối kháng. Nhìn chung, kết quả của chúng tôi gợi ý mạnh mẽ rằng có không gian lớn hơn dự kiến cho huấn luyện thưa ở quy mô lớn, và lợi ích của độ thưa có thể phổ quát hơn vượt ra ngoài việc cắt tỉa được thiết kế cẩn thận. Mã nguồn của chúng tôi có thể được tìm thấy tại https://github.com/VITA-Group/Random_Pruning .

1 GIỚI THIỆU
Hầu hết các đột phá gần đây trong học sâu được đạt được khá dễ dàng với sự phức tạp tăng lên của các mạng có quá nhiều tham số (Brown et al., 2020; Raffel et al., 2020; Dosovitskiy et al., 2021; Fedus et al., 2021. arXiv:2101.03961; Jumper et al., 2021; Berner et al., 2019). Điều được biết rộng rãi là các mô hình lớn huấn luyện tốt hơn (Neyshabur et al., 2019; Novak et al., 2018; Allen-Zhu et al., 2019), tổng quát hóa tốt hơn (Hendrycks & Dietterich, 2019; Xie & Yuille, 2020; Zhao et al., 2018), và chuyển giao tốt hơn (Chen et al., 2020b;a; 2021b). Tuy nhiên, sự gia tăng của các mô hình lớn làm trầm trọng thêm khoảng cách giữa nghiên cứu và thực tiễn vì nhiều ứng dụng thực tế yêu cầu các mạng nhỏ gọn và hiệu quả.

Cắt tỉa mạng nơ-ron, kể từ khi được đề xuất bởi (Mozer & Smolensky, 1989; Janowsky, 1989), đã phát triển thành kỹ thuật phổ biến nhất trong tài liệu để giảm yêu cầu tính toán và bộ nhớ của mạng nơ-ron. Trong vài năm qua, nhiều tiêu chí cắt tỉa đã được đề xuất, bao gồm độ lớn (Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Mocanu et al., 2018), Hessian (LeCun et al., 1990; Hassibi & Stork, 1993), thông tin tương hỗ (Dai et al., 2018), khai triển Taylor (Molchanov et al., 2016), chuyển động (Sanh et al., 2020), độ nhạy kết nối (Lee et al., 2019), v.v. Được tạo động lực cho các tình huống khác nhau, cắt tỉa có thể xảy ra sau huấn luyện (Han et al., 2015; Frankle & Carbin, 2019; Molchanov et al., 2016; Lee et al., 2021), trong quá trình huấn luyện (Zhu & Gupta,

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2022
2017; Gale et al., 2019; Louizos et al., 2018; You et al., 2020; Chen et al., 2021c;a), và thậm chí trước huấn luyện (Mocanu et al., 2018; Lee et al., 2019; Gale et al., 2019; Wang et al., 2020; Tanaka et al., 2020). Chế độ cuối cùng có thể được phân loại thêm thành "huấn luyện thưa tĩnh" (Mocanu et al., 2016; Gale et al., 2019; Lee et al., 2019; Wang et al., 2020) và "huấn luyện thưa động" (Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020a; Liu et al., 2021b;a).

Trong khi cắt tỉa ngẫu nhiên là một phương pháp phổ quát có thể xảy ra ở bất kỳ giai đoạn nào của huấn luyện, huấn luyện một mạng được cắt tỉa ngẫu nhiên từ đầu có lẽ là cách hấp dẫn nhất, do tiềm năng tiết kiệm "đầu cuối đến cuối" cho toàn bộ quá trình huấn luyện bên cạnh suy luận. Do lý do này, chúng tôi tập trung vào cắt tỉa ngẫu nhiên cho huấn luyện thưa trong bài báo này. Khi các phương pháp cắt tỉa mới nở rộ, cắt tỉa ngẫu nhiên tự nhiên trở thành "giới hạn dưới" thực nghiệm của hiệu suất chúng vì các kết nối được chọn ngẫu nhiên mà không có lý do tốt nào. Có thể do cùng lý do, kết quả của huấn luyện thưa với cắt tỉa ngẫu nhiên (như đường cơ sở "dễ đánh bại" để hỗ trợ các phương pháp cắt tỉa mới phức tạp hơn) được báo cáo trong tài liệu thật không may là mơ hồ, thường không nhất quán, và đôi khi tùy tiện. Ví dụ, được phát hiện trong Liu et al. (2020b) rằng các mạng thưa được cắt tỉa ngẫu nhiên có thể được huấn luyện từ đầu để khớp với độ chính xác đầy đủ của mạng dày đặc với chỉ 20% tham số, trong khi khoảng 80% tham số được yêu cầu để làm như vậy trong Frankle et al. (2021). Sự khác biệt có thể phát sinh từ lựa chọn kiến trúc, công thức huấn luyện, siêu tham số phân phối/tỷ lệ theo lớp, v.v.

Trong hầu hết tài liệu cắt tỉa (Gale et al., 2019; Lee et al., 2019; Frankle et al., 2021; Tanaka et al., 2020), cắt tỉa ngẫu nhiên thường đề cập đến việc loại bỏ ngẫu nhiên cùng tỷ lệ tham số mỗi lớp, kết thúc với độ thưa theo lớp đồng đều. Tuy nhiên, các nhà nghiên cứu đã khám phá các độ thưa theo lớp được xác định trước khác, ví dụ: uniform +(Gale et al., 2019), đồ thị ngẫu nhiên Erdős-Rényi (ER) (Mocanu et al., 2018), và Erdős-Rényi-Kernel (ERK) (Evci et al., 2020a). Các độ thưa theo lớp này cũng phù hợp với danh mục cắt tỉa ngẫu nhiên, vì chúng không yêu cầu huấn luyện để có được các tỷ lệ thưa tương ứng. Chúng tôi đánh giá cắt tỉa ngẫu nhiên cho huấn luyện thưa với các tỷ lệ thưa theo lớp này, về các quan điểm khác nhau bên cạnh độ chính xác dự đoán.

Các phát hiện chính của chúng tôi trong quá trình nghiên cứu này được tóm tắt dưới đây:
• Chúng tôi phát hiện rằng kích thước mạng quan trọng đối với hiệu quả của huấn luyện thưa với cắt tỉa ngẫu nhiên. Với các mạng nhỏ, cắt tỉa ngẫu nhiên khó có thể khớp với độ chính xác đầy đủ ngay cả ở độ thưa nhẹ (10%, 20%). Tuy nhiên, khi các mạng trở nên rộng hơn và sâu hơn, hiệu suất của việc huấn luyện một mạng thưa được cắt tỉa ngẫu nhiên sẽ nhanh chóng tăng lên để khớp với mạng dày đặc tương đương, ngay cả ở tỷ lệ thưa cao.

• Chúng tôi tiếp tục xác định rằng tỷ lệ thưa theo lớp thích hợp có thể là một yếu tố tăng cường quan trọng để huấn luyện một mạng được cắt tỉa ngẫu nhiên từ đầu, đặc biệt đối với các mạng lớn. Chúng tôi điều tra một số tùy chọn để xác định trước tỷ lệ thưa theo lớp trước bất kỳ huấn luyện nào; một trong số chúng có thể đẩy hiệu suất của một mạng con thưa hoàn toàn ngẫu nhiên Wide ResNet-50 vượt trội hơn Wide ResNet-50 được huấn luyện dày đặc, trên ImageNet.

• Chúng tôi đánh giá có hệ thống hiệu suất của huấn luyện thưa với cắt tỉa ngẫu nhiên, và quan sát độ chính xác và độ bền đáng ngạc nhiên tốt. Độ chính xác đạt được bởi tỷ lệ ERK thậm chí có thể vượt qua những tiêu chí phức tạp đã học được, ví dụ: SNIP hoặc GraSP. Ngoài ra, các mạng được cắt tỉa ngẫu nhiên và huấn luyện thưa được phát hiện vượt trội hơn các mạng dày đặc thông thường trong các khía cạnh thuận lợi khác, chẳng hạn như phát hiện ngoài phân phối (OoD), độ bền chống đối kháng và ước lượng không chắc chắn.

2 CÔNG TRÌNH LIÊN QUAN

2.1 HUẤN LUYỆN THƯA TĨNH

Huấn luyện thưa tĩnh đại diện cho một lớp các phương pháp nhằm huấn luyện một mạng con thưa với mẫu kết nối thưa cố định trong quá trình huấn luyện. Chúng tôi chia huấn luyện thưa tĩnh thành cắt tỉa ngẫu nhiên và cắt tỉa không ngẫu nhiên theo việc kết nối có được chọn ngẫu nhiên hay không.

Cắt tỉa ngẫu nhiên. Huấn luyện thưa tĩnh với cắt tỉa ngẫu nhiên lấy mẫu các mặt nạ trong mỗi lớp theo cách ngẫu nhiên dựa trên độ thưa theo lớp được xác định trước. Phương pháp ngây thơ nhất là cắt tỉa mỗi lớp đồng đều với cùng tỷ lệ cắt tỉa, tức là cắt tỉa đồng đều (Mariet & Sra, 2016; He et al., 2017; Suau et al., 2019; Gale et al., 2019). Mocanu et al. (2016) đề xuất một tôpô không đồng đều và không có tỷ lệ, cho thấy hiệu suất tốt hơn so với đối tác dày đặc khi áp dụng cho máy Boltzmann hạn chế (RBMs). Sau đó, đồ thị mở rộng được giới thiệu để xây dựng CNN thưa và cho thấy hiệu suất tương đương so với CNN dày đặc tương ứng (Prabhu et al., 2018; Kepner & Robinett, 2019). Mặc dù ban đầu không được thiết kế cho huấn luyện thưa tĩnh, ER (Mocanu et al., 2018) và ERK (Evci et al., 2020a) là hai độ thưa theo lớp tiên tiến được giới thiệu từ lĩnh vực lý thuyết đồ thị với kết quả mạnh mẽ.

Cắt tỉa không ngẫu nhiên. Thay vì chọn trước tỷ lệ thưa cho mỗi lớp, nhiều công trình sử dụng các tiêu chí độ quan trọng được đề xuất để học các tỷ lệ thưa theo lớp trước huấn luyện, còn được gọi là cắt tỉa tại khởi tạo (PaI). Lee et al. (2019) đầu tiên giới thiệu SNIP chọn các kết nối quan trọng về mặt cấu trúc tại khởi tạo thông qua điểm số độ nhạy kết nối được đề xuất. Theo sau SNIP, nhiều tiêu chí hiệu quả đã được đề xuất để cải thiện hiệu suất của cắt tỉa không ngẫu nhiên tại khởi tạo, bao gồm nhưng không giới hạn ở dòng gradient (GraSP; Wang et al. (2020)), sức mạnh synaptic (SynFlow; Tanaka et al. (2020)), kernel tiếp tuyến nơ-ron (Liu & Zenke, 2020), và SNIP lặp (de Jorge et al., 2021; Verdenius et al., 2020). Su et al. (2020); Frankle et al. (2021) phát hiện rằng các phương pháp PaI hiện tại khó khai thác bất kỳ thông tin nào từ dữ liệu huấn luyện và rất bền vững với việc trộn mặt nạ, trong khi cắt tỉa độ lớn sau huấn luyện học cả hai, phản ánh một thách thức rộng lớn hơn vốn có đối với cắt tỉa tại khởi tạo.

2.2 HUẤN LUYỆN THƯA ĐỘNG

Trái ngược với huấn luyện thưa tĩnh, huấn luyện thưa động xuất phát từ các mạng con thưa được khởi tạo ngẫu nhiên, và đồng thời khám phá động kết nối thưa mới trong quá trình huấn luyện. Huấn luyện thưa động bắt đầu từ Huấn luyện Tiến hóa Thưa (SET) (Mocanu et al., 2018; Liu et al., 2020a) khởi tạo kết nối thưa với tôpô Erdős-Rényi (Erdős & Rényi, 1959) và định kỳ khám phá kết nối thưa thông qua sơ đồ cắt-và-phát triển trong quá trình huấn luyện. Mặc dù tồn tại nhiều tiêu chí cắt tỉa trong tài liệu, cắt tỉa độ lớn đơn giản thường hoạt động tốt trong lĩnh vực huấn luyện thưa động. Mặt khác, các tiêu chí được sử dụng để phát triển trọng số trở lại khác nhau từ phương pháp này sang phương pháp khác, bao gồm tính ngẫu nhiên (Mocanu et al., 2018; Mostafa & Wang, 2019), momentum (Dettmers & Zettlemoyer, 2019), gradient (Evci et al., 2020a; Jayakumar et al., 2020; Liu et al., 2021b). Bên cạnh sơ đồ cắt-và-phát triển, độ thưa theo lớp rất quan trọng để đạt được độ chính xác cao. (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019) phân bổ lại trọng số qua các lớp trong quá trình huấn luyện dựa trên heuristics hợp lý, thể hiện cải thiện hiệu suất. Evci et al. (2020a) mở rộng ER cho CNN và cho thấy mức tăng hiệu suất đáng kể đối với huấn luyện CNN thưa với tỷ lệ Erdős-Rényi-Kernel (ERK). Gần đây nhất, Liu et al. (2021a) bắt đầu từ một mạng con ở độ thưa nhỏ hơn và dần dần cắt tỉa nó đến độ thưa mục tiêu trong quá trình huấn luyện. Mạng con dày đặc hơn ban đầu cung cấp không gian khám phá lớn hơn cho DST ở giai đoạn huấn luyện sớm và do đó dẫn đến cải thiện hiệu suất, đặc biệt đối với độ thưa cực đoan.

Mặc dù huấn luyện thưa động đạt được hiệu suất huấn luyện thưa đầy hứa hẹn, nó thay đổi kết nối thưa trong quá trình huấn luyện và do đó nằm ngoài phạm vi của cắt tỉa ngẫu nhiên.

Trong khi các công trình trước đây đã quan sát thấy rằng cắt tỉa ngẫu nhiên có thể cạnh tranh hơn trong một số trường hợp nhất định (Mocanu et al., 2018; Liu et al., 2020b; Su et al., 2020; Frankle et al., 2021), họ không đưa ra hướng dẫn có nguyên tắc về khi nào và làm thế nào nó có thể trở nên tốt như vậy; cũng không cho thấy nó có thể khớp với hiệu suất của mạng dày đặc trên ImageNet. Đứng trên vai những người khổng lồ đó, công trình của chúng tôi tóm tắt các nguyên tắc bằng các nghiên cứu rộng rãi và nghiêm ngặt hơn, và chứng minh kết quả mạnh nhất cho đến nay, rằng Wide ResNet-50 thưa được cắt tỉa ngẫu nhiên có thể được huấn luyện thưa để vượt trội hơn Wide ResNet-50 dày đặc, trên ImageNet. Hơn nữa, so với các tỷ lệ thưa tùy tiện được sử dụng trong (Su et al., 2020), chúng tôi cho thấy rằng ERK (Evci et al., 2020a) và ERK+ được sửa đổi của chúng tôi là các tỷ lệ thưa có thể áp dụng tổng quát hơn luôn thể hiện hiệu suất cạnh tranh mà không cần thiết kế thưa theo lớp cẩn thận cho mọi kiến trúc. Cụ thể, tỷ lệ ERK+ đạt được độ chính xác tương tự với Wide ResNet-50 dày đặc trên ImageNet trong khi không có dữ liệu, không có feedforward và không có khởi tạo dày đặc.

3 PHƯƠNG PHÁP LUẬN

Chúng tôi tiến hành các thí nghiệm rộng rãi để đánh giá có hệ thống hiệu suất của cắt tỉa ngẫu nhiên. Các thiết lập thí nghiệm được mô tả dưới đây.

3.1 TỶ LỆ THƯA THEO LỚP

Ký hiệu sl là độ thưa của lớp l. Cắt tỉa ngẫu nhiên, cụ thể là, loại bỏ trọng số hoặc bộ lọc trong mỗi lớp một cách ngẫu nhiên đến độ thưa mục tiêu sl. Khác với các công trình học độ thưa theo lớp và trọng số mô hình cùng nhau trong quá trình huấn luyện bằng các kỹ thuật cắt tỉa toàn cục lặp (Frankle & Carbin, 2019), ngưỡng mềm (Kusupati et al., 2020), và tái tham số hóa động (Mostafa & Wang, 2019), v.v., độ thưa theo lớp của cắt tỉa ngẫu nhiên được xác định trước trước huấn luyện. Nhiều tỷ lệ thưa theo lớp được xác định trước trong tài liệu phù hợp cho cắt tỉa ngẫu nhiên, trong khi chúng có thể không được thiết kế ban đầu cho cắt tỉa ngẫu nhiên. Chúng tôi chọn 6 tỷ lệ thưa theo lớp sau đây để nghiên cứu. Tỷ lệ SNIP và tỷ lệ GraSP là hai tỷ lệ theo lớp mà chúng tôi mượn từ PaI.

ERK. Được giới thiệu bởi Mocanu et al. (2018), Erdős-Rényi (ER) làm thưa Perceptron Đa lớp (MLP) với tôpô ngẫu nhiên trong đó các lớp lớn hơn được phân bổ với độ thưa cao hơn so với các lớp nhỏ hơn. Evci et al. (2020a) tiếp tục đề xuất một biến thể tích chập (Erdős-Rényi-Kernel (ERK)) có tính đến kernel tích chập. Cụ thể, độ thưa của lớp tích chập được tỷ lệ tỷ lệ thuận với 1-nl-1+nl+wl+hl/nl-1nlwlhl trong đó nl đề cập đến số lượng nơ-ron/kênh trong lớp l; wl và hl là chiều rộng và chiều cao tương ứng.

ERK+. Chúng tôi sửa đổi ERK bằng cách buộc lớp kết nối đầy đủ cuối cùng là dày đặc nếu nó không phải, trong khi giữ nguyên số lượng tham số tổng thể. Làm như vậy cải thiện độ chính xác kiểm tra của Wide ResNet-50 trên ImageNet1 như được hiển thị trong Phụ lục F.

Uniform. Mỗi lớp được cắt tỉa với cùng tỷ lệ cắt tỉa để mạng được cắt tỉa kết thúc với phân phối thưa đồng đều, ví dụ: Zhu & Gupta (2017).

Uniform+. Thay vì sử dụng tỷ lệ thưa hoàn toàn đồng đều, Gale et al. (2019) giữ lớp tích chập đầu tiên dày đặc và duy trì ít nhất 20% tham số trong lớp kết nối đầy đủ cuối cùng.

Tỷ lệ SNIP. SNIP là một phương pháp PaI chọn trọng số quan trọng dựa trên điểm số độ nhạy kết nối |gw|, trong đó w và g là trọng số mạng và gradient, tương ứng. Các trọng số có điểm số thấp nhất trong một lần lặp được cắt tỉa trước huấn luyện. Mặc dù ban đầu không được thiết kế cho cắt tỉa ngẫu nhiên, chúng tôi điều chỉnh SNIP cho cắt tỉa ngẫu nhiên bằng cách chỉ giữ các tỷ lệ thưa theo lớp của nó, trong khi loại bỏ các vị trí mặt nạ của nó. Các vị trí mặt nạ mới (các phần tử khác không) được lấy mẫu lại thông qua phân phối đồng đều Uniform(0;1) với xác suất 1-sl. Các độ thưa theo lớp như vậy được thu được theo cách (hơi) dựa trên dữ liệu, nhưng rất hiệu quả trước huấn luyện mà không có bất kỳ cập nhật trọng số nào. Tỷ lệ SNIP sau đó được coi như một tỷ lệ lấy mẫu được xác định trước khác (tức là trước khi huấn luyện bắt đầu) cho cắt tỉa ngẫu nhiên.

Tỷ lệ GraSP. GraSP là một phương pháp tiên tiến khác tìm cách cắt tỉa tại khởi tạo. Cụ thể, GraSP loại bỏ các trọng số có ảnh hưởng ít nhất đến chuẩn gradient dựa trên điểm số của wHg, trong đó H là ma trận Hessian và g là gradient. Tương tự với SNIP, chúng tôi giữ các tỷ lệ thưa theo lớp của GraSP và lấy mẫu lại các vị trí mặt nạ của nó.

3.2 THIẾT LẬP THÍ NGHIỆM

Bảng 1: Tóm tắt các kiến trúc, tập dữ liệu và siêu tham số được sử dụng trong bài báo này.
Model Mode Data #Epoch Batch Size LR Momentum LR Decay, Epoch Weight Decay
ResNetsDense CIFAR-10/100 160 128 0.1 0.9 10 , [80, 120] 0.0005
Sparse CIFAR-10/100 160 128 0.1 0.9 10 , [80, 120] 0.0005
Wide ResNetsDense ImageNet 90 192*4 0.4 0.9 10 , [30, 60, 80] 0.0001
Sparse ImageNet 100 192*4 0.4 0.9 10 , [30, 60, 90] 0.0001

Kiến trúc và Tập dữ liệu. Các thí nghiệm chính của chúng tôi được tiến hành với phiên bản CIFAR của ResNet (He et al., 2016) với độ sâu và độ rộng khác nhau trên CIFAR-10/100 (Krizhevsky et al., 2009), phiên bản batch normalization của VGG (Simonyan & Zisserman, 2014) với độ sâu khác nhau trên CIFAR-10/100, và phiên bản ImageNet của ResNet và Wide ResNet-50 (Zagoruyko & Komodakis, 2016) trên ImageNet (Deng et al., 2009). Đối với ImageNet, chúng tôi theo thiết lập phổ biến trong huấn luyện thưa (Dettmers & Zettlemoyer, 2019; Evci et al., 2020b; Liu et al., 2021b) và huấn luyện các mô hình thưa trong 100 epoch. Tất cả các mô hình được huấn luyện với descent gradient ngẫu nhiên (SGD) với momentum. Chúng tôi chia sẻ tóm tắt về kiến trúc, tập dữ liệu và siêu tham số trong Bảng 1.

Chỉ số Đo lường. Trong hầu hết tài liệu cắt tỉa, độ chính xác kiểm tra là chất lượng cốt lõi mà các nhà nghiên cứu xem xét. Tuy nhiên, việc đánh giá các quan điểm khác cũng quan trọng đối với học thuật và công nghiệp

1Đối với các tập dữ liệu có ít lớp hơn, ví dụ: MNIST, CIFAR-10, ERK+ thu nhỏ lại thành ERK vì lớp kết nối đầy đủ cuối cùng đã dày đặc.

--- TRANG 5 ---
trước khi thay thế các mạng dày đặc bằng các mạng thưa trên quy mô lớn. Do đó, chúng tôi đánh giá toàn diện huấn luyện thưa với cắt tỉa ngẫu nhiên từ một quan điểm rộng hơn. Cụ thể, chúng tôi đánh giá cắt tỉa ngẫu nhiên từ các quan điểm về hiệu suất OoD (Hendrycks et al., 2021), độ bền chống đối kháng (Goodfellow et al., 2014), và ước lượng không chắc chắn (Lakshminarayanan et al., 2016). Xem Phụ lục A để biết chi tiết đầy đủ về các phép đo được sử dụng trong công trình này.

4 KẾT QUẢ THÍ NGHIỆM TRÊN CIFAR

Trong phần này, chúng tôi báo cáo kết quả của huấn luyện thưa với cắt tỉa ngẫu nhiên trên CIFAR-10/100 dưới các phép đo đánh giá khác nhau. Đối với mỗi phép đo, sự đánh đổi giữa độ thưa và chỉ số tương ứng được báo cáo. Hơn nữa, chúng tôi cũng thay đổi độ sâu và độ rộng của mô hình để kiểm tra cách hiệu suất thay đổi khi kích thước mô hình thay đổi. Kết quả được tính trung bình trên 3 lần chạy. Chúng tôi báo cáo kết quả trên CIFAR-10 của ResNet trong phần chính của bài báo này. Kết quả trên CIFAR-100 của ResNet, và CIFAR-10/100 của VGG được hiển thị trong Phụ lục D và Phụ lục E, tương ứng. Trừ khi được nêu khác, tất cả kết quả đều tương tự về chất lượng.

4.1 ĐỘ CHÍNH XÁC DỰ ĐOÁN CỦA CẮT TỈA NGẪU NHIÊN TRÊN CIFAR-10

Chúng tôi đầu tiên chứng minh hiệu suất của cắt tỉa ngẫu nhiên trên chỉ số phổ biến nhất - độ chính xác kiểm tra. Để tránh chồng chéo của nhiều đường cong, chúng tôi chia sẻ kết quả của GraSP trong Phụ lục C. Các quan sát chính như sau:

1 Hiệu suất của cắt tỉa ngẫu nhiên cải thiện với kích thước của mạng. Chúng tôi thay đổi độ sâu và độ rộng của ResNet và báo cáo độ chính xác kiểm tra trong Hình 1. Khi hoạt động trên các mạng nhỏ, ví dụ: ResNet-20 và ResNet-32, chúng tôi khó có thể tìm thấy các mạng con khớp ngay cả ở độ thưa nhẹ, tức là 10%, 20%. Với các mạng lớn hơn, ví dụ: ResNet-56 và ResNet-110, cắt tỉa ngẫu nhiên có thể khớp với hiệu suất dày đặc ở độ thưa 60% ÷ 70%. Hành vi tương tự cũng có thể được quan sát khi chúng tôi tăng độ rộng của ResNet-20 trong Hình 2. Xem Phụ lục B để biết kết quả trên các mô hình sâu hơn.

2 Sự khác biệt hiệu suất giữa các phương pháp cắt tỉa khác nhau trở nên không rõ ràng khi kích thước mô hình tăng. Mặc dù độ thưa đồng đều không khớp với độ chính xác đạt được bởi độ thưa không đồng đều (ERK và SNIP) với các mô hình nhỏ, độ chính xác kiểm tra của chúng tăng lên mức độ tương đương với độ thưa không đồng đều với các mô hình lớn, ví dụ: ResNet-110 và ResNet-20-56.

3 Cắt tỉa ngẫu nhiên với ERK thậm chí vượt trội hơn cắt tỉa với các phương pháp thông thạo (SNIP, GraSP). Không sử dụng bất kỳ thông tin nào, ví dụ: gradient và độ lớn, huấn luyện một mạng con được cắt tỉa ngẫu nhiên với tôpô ERK dẫn đến độ chính xác biểu cảm, thậm chí tốt hơn những tỷ lệ thưa được thiết kế tinh vi, tức là SNIP và GraSP (được hiển thị trong Phụ lục C).

4.2 ĐÁNH GIÁ RỘNG HÔN CỦA CẮT TỈA NGẪU NHIÊN TRÊN CIFAR-10

Nói chung, huấn luyện thưa với cắt tỉa ngẫu nhiên đạt được kết quả khá mạnh trên CIFAR-10 về ước lượng không chắc chắn, độ bền OoD, và độ bền chống đối kháng mà không có bất kỳ kỹ thuật phức tạp nào. Chúng tôi tóm tắt các quan sát chính như dưới đây:

1 Các mạng được cắt tỉa ngẫu nhiên có ước lượng không chắc chắn tốt hơn so với các đối tác dày đặc của chúng. Hình 3 cho thấy ResNet-20 được cắt tỉa ngẫu nhiên khớp hoặc thậm chí cải thiện ước lượng không chắc chắn của mạng dày đặc với toàn bộ phạm vi độ thưa. ECE của cắt tỉa ngẫu nhiên tăng với kích thước mô hình, phù hợp với phát hiện của mạng dày đặc trong Guo et al. (2017). Tuy nhiên, cắt tỉa ngẫu nhiên có khả năng lấy mẫu các mạng con khớp với độ thưa cao (ví dụ: 80%) trừ mô hình lớn nhất, ResNet-110. Kết quả của NLL được trình bày trong Phụ lục G.2, nơi các mạng được cắt tỉa ngẫu nhiên cũng khớp NLL của mạng dày đặc ở độ thưa cực cao.

2 Cắt tỉa ngẫu nhiên tạo ra các mạng con cực kỳ thưa nhưng bền vững trên OoD. Hình 4 vẽ kết quả của các mạng được huấn luyện trên CIFAR-10, được kiểm tra trên CIFAR-100. Kết quả được kiểm tra trên SVHN được báo cáo trong Phụ lục G.1. Kích thước mạng lớn cải thiện đáng kể hiệu suất OoD của cắt tỉa ngẫu nhiên. Khi kích thước mô hình tăng, cắt tỉa ngẫu nhiên có thể khớp với hiệu suất OoD của mạng dày đặc chỉ với 20% tham số. Một lần nữa, tỷ lệ SNIP và ERK vượt trội hơn độ thưa đồng đều trong thiết lập này.

3 Cắt tỉa ngẫu nhiên cải thiện độ bền chống đối kháng của các mô hình lớn. Độ bền chống đối kháng của các mô hình lớn (ví dụ: ResNet-56 và ResNet-110) cải thiện đáng kể với độ thưa nhẹ trong Phụ lục G.3. Một giải thích ở đây là, trong khi đạt được độ chính xác sạch cao, các mô hình lớn có quá nhiều tham số này bị overfit cao trên CIFAR-10, và do đó gặp khó khăn với hiệu suất kém trên các ví dụ đối kháng (được hiển thị bởi Tsipras et al. (2019); Zhang et al. (2019)). Độ thưa được tạo ra bởi cắt tỉa ngẫu nhiên đóng vai trò như một loại regularization rẻ có thể giảm thiểu vấn đề overfit này.

5 KẾT QUẢ THÍ NGHIỆM TRÊN IMAGENET

Chúng tôi đã học được từ Phần 4 rằng các mạng lớn hơn dẫn đến các mạng con ngẫu nhiên mạnh hơn trên CIFAR-10/100. Chúng tôi cũng quan tâm đến việc chúng ta có thể đi xa đến đâu với cắt tỉa ngẫu nhiên trên ImageNet (Deng et al., 2009), một tập dữ liệu không bão hòa mà trên đó các mạng nơ-ron sâu ít có quá nhiều tham số hơn so với trên CIFAR-10/100. Trong phần này, chúng tôi cung cấp một thí nghiệm quy mô lớn trên ImageNet với các ResNet khác nhau từ ResNet-18 đến ResNet-101 và Wide ResNet-50.

--- TRANG 7 ---
Tương tự với CIFAR, chúng tôi đánh giá huấn luyện thưa với cắt tỉa ngẫu nhiên từ các quan điểm khác nhau, bao gồm độ chính xác dự đoán, phát hiện OoD, độ bền chống đối kháng và ước lượng không chắc chắn. Chúng tôi chọn tỷ lệ thưa SNIP và ERK+ cho Wide ResNet-50, và tỷ lệ SNIP cho phần còn lại của các kiến trúc. Độ thưa của các mạng con được cắt tỉa ngẫu nhiên được đặt là [0.7, 0.5, 0.3]. Chúng tôi đầu tiên hiển thị sự đánh đổi giữa số lượng tham số và độ chính xác kiểm tra cho tất cả kiến trúc trong Hình 5-trên-trái. Hơn nữa, để hiểu rõ hơn về lợi ích tính toán mang lại bởi độ thưa, chúng tôi báo cáo sự đánh đổi giữa FLOPs kiểm tra và từng chỉ số đo lường trong phần còn lại của Hình 5.

Độ Chính Xác Dự Đoán. Sự đánh đổi số lượng tham số-độ chính xác và sự đánh đổi FLOPs-độ chính xác được báo cáo trong Hình 5-trên-trái và Hình 5-trên-giữa, tương ứng. Nhìn chung, chúng tôi quan sát một mẫu rất tương tự như kết quả của CIFAR được báo cáo trong Phần 4. Trên các mô hình nhỏ hơn như ResNet-18 và ResNet-34, cắt tỉa ngẫu nhiên không thể tìm thấy các mạng con khớp. Khi kích thước mô hình trở nên lớn hơn đáng kể (ResNet-101 và Wide ResNet-50), độ chính xác kiểm tra của cắt tỉa ngẫu nhiên nhanh chóng cải thiện và khớp với các mô hình dày đặc tương ứng (không chỉ các mô hình dày đặc nhỏ) ở độ thưa 30% ÷ 50%. Trong khi sự khác biệt hiệu suất giữa tỷ lệ SNIP và ERK trên CIFAR-10/100 là hơi mơ hồ, tỷ lệ SNIP luôn vượt trội hơn ERK+ với Wide Resnet-50 trên ImageNet với cùng số lượng tham số (xem Phụ lục F để biết thêm chi tiết). Bên cạnh đó, chúng tôi quan sát rằng cắt tỉa ngẫu nhiên nhận được mức tăng hiệu quả ngày càng lớn hơn (sự khác biệt giữa các giá trị trục x của mô hình thưa và mô hình dày đặc) với kích thước mô hình tăng trên ImageNet.

Trong khi trong Hình 5-trên-trái, một Wide ResNet-50 được cắt tỉa ngẫu nhiên với tỷ lệ SNIP (đường màu tím) có thể dễ dàng vượt trội hơn ResNet-50 dày đặc (ngôi sao xanh) 2% độ chính xác với cùng số lượng tham số, cái trước yêu cầu gấp đôi FLOPs so với cái sau trong Hình 5-trên-giữa. Quan sát này làm nổi bật tầm quan trọng của việc báo cáo FLOPs cần thiết cùng với độ thưa khi so sánh hai phương pháp cắt tỉa.

Đánh Giá Rộng hơn của Cắt Tỉa Ngẫu Nhiên trên ImageNet. Như được hiển thị trong phần còn lại của Hình 5, hiệu suất của đánh giá rộng hơn trên ImageNet cực kỳ tương tự với độ chính xác kiểm tra. Cắt tỉa ngẫu nhiên không thể khám phá các mạng con khớp với các mô hình nhỏ. Tuy nhiên, khi kích thước mô hình tăng, nó nhận được mức tăng hiệu suất lớn về các đánh giá quan trọng khác, bao gồm ước lượng không chắc chắn, hiệu suất phát hiện OoD và độ bền chống đối kháng.

5.1 HIỂU CẮT TỈA NGẪU NHIÊN THÔNG QUA DÒNG GRADIENT

Khoảng cách hiệu suất giữa tỷ lệ ERK và SNIP trên ImageNet đặt ra câu hỏi - độ thưa theo lớp của SNIP mang lại lợi ích gì cho huấn luyện thưa? Vì SNIP tính đến gradient khi xác định độ thưa theo lớp, chúng tôi chuyển sang dòng gradient với hy vọng tìm ra một số hiểu biết về câu hỏi này. Dạng của chuẩn gradient mà chúng tôi chọn là dòng gradient hiệu quả (Tessera et al., 2021) chỉ tính toán chuẩn gradient của các trọng số hoạt động. Chúng tôi đo chuẩn gradient của các mạng thưa được tạo ra bởi SNIP và ERK2 trong giai đoạn huấn luyện sớm. Kết quả được mô tả trong Hình 6.

Nhìn chung, chúng tôi thấy rằng tỷ lệ SNIP thực sự mang lại lợi ích cho chuẩn gradient ở giai đoạn ban đầu so với ERK. Chỉ xem xét SNIP (đường màu xanh lá) và ERK (đường màu cam), những cái có chuẩn gradient cao hơn ở đầu luôn đạt được độ chính xác cuối cùng cao hơn trên cả CIFAR-10 và ImageNet. Kết quả này phù hợp với công trình trước đây (Wang et al., 2020), tuyên bố rằng việc tăng chuẩn gradient ở đầu có thể dẫn đến độ chính xác cao hơn. Tuy nhiên, quan sát này không đúng đối với Wide ResNet-50 dày đặc trên ImageNet (đường màu xanh dương). Mặc dù mô hình dày đặc có chuẩn gradient thấp hơn so với SNIP, chúng luôn vượt trội hơn SNIP về độ chính xác với một khoảng cách lớn.

Thay vì tập trung vào giai đoạn ban đầu, chúng tôi lưu ý rằng chuẩn gradient sớm trong huấn luyện (giai đoạn phẳng sau khi chuẩn gradient giảm) có thể hiểu rõ hơn về hành vi của các mạng được huấn luyện dưới các tình huống khác nhau. Chúng tôi thực nghiệm thấy rằng khoảng cách hiệu suất cuối cùng giữa mạng thưa và mạng dày đặc có tương quan cao với khoảng cách chuẩn gradient sớm trong huấn luyện. Huấn luyện với các mạng nhỏ (ví dụ: ResNet-20) trên CIFAR-10 dẫn đến khoảng cách hiệu suất và chuẩn gradient lớn giữa mạng thưa và mạng dày đặc, trong khi hai khoảng cách này đồng thời biến mất khi được huấn luyện với các mạng lớn, ví dụ: ResNet-56 và ResNet-110. Hành vi tương tự có thể được quan sát trong Wide ResNet-50 trên ImageNet về độ thưa. Cả khoảng cách hiệu suất và chuẩn gradient đều giảm dần khi mức độ thưa giảm từ 85% xuống 60%. Điều này thực sự có ý nghĩa, vì các mạng lớn (hoặc kích thước mô hình lớn hoặc số lượng tham số lớn) có thể vẫn có quá nhiều tham số sau khi cắt tỉa, để tất cả các phương pháp cắt tỉa (bao gồm cắt tỉa ngẫu nhiên) có thể bảo tồn dòng gradient cùng với độ chính xác kiểm tra một cách bình đẳng.

Phát hiện của chúng tôi gợi ý rằng chỉ xem xét dòng gradient tại khởi tạo có thể không đủ cho huấn luyện thưa. Nhiều nỗ lực hơn nên được đầu tư để nghiên cứu các tính chất mà huấn luyện mạng thưa thiếu sau khởi tạo, đặc biệt là giai đoạn đầu của huấn luyện (Liu et al., 2021a). Các kỹ thuật thay đổi mẫu thưa trong quá trình huấn luyện (Mocanu et al., 2018; Evci et al., 2020a) và tua lại trọng số (Frankle et al., 2020; Renda et al., 2020) có thể phục vụ như những điểm bắt đầu tốt.

6 KẾT LUẬN

Trong công trình này, chúng tôi đánh giá lại có hệ thống đường cơ sở bị đánh giá thấp của huấn luyện thưa - cắt tỉa ngẫu nhiên. Kết quả của chúng tôi làm nổi bật một phát hiện phản trực giác, đó là, huấn luyện một mạng được cắt tỉa ngẫu nhiên từ đầu mà không có bất kỳ tiêu chí cắt tỉa tinh vi nào có thể khá hiệu quả. Với kích thước mạng và tỷ lệ thưa theo lớp thích hợp, cắt tỉa ngẫu nhiên có thể khớp với hiệu suất của mạng dày đặc ngay cả ở độ thưa cực đoan. Ấn tượng, một mạng con được cắt tỉa ngẫu nhiên của Wide ResNet-50 có thể được huấn luyện để vượt trội hơn một điểm chuẩn mạnh, Wide ResNet-50 dày đặc trên ImageNet. Hơn nữa, huấn luyện với cắt tỉa ngẫu nhiên vốn dĩ mang lại lợi ích đáng kể cho các khía cạnh mong muốn khác, chẳng hạn như phát hiện ngoài phân phối, ước lượng không chắc chắn và độ bền chống đối kháng. Bài báo của chúng tôi chỉ ra rằng ngoài hiệu suất hấp dẫn, các mô hình lớn cũng có độ bền mạnh đối với cắt tỉa. Ngay cả khi chúng ta cắt tỉa với sự ngẫu nhiên hoàn toàn, các mô hình lớn có thể bảo tồn hiệu suất của chúng tốt.

--- TRANG 10 ---
7 KHẢ NĂNG TÁI TẠO

Chúng tôi đã chia sẻ các kiến trúc, tập dữ liệu và siêu tham số được sử dụng trong bài báo này trong Phần 3.2. Bên cạnh đó, các phép đo và chỉ số khác nhau được sử dụng trong bài báo này được chia sẻ trong Phụ lục A. Chúng tôi đã phát hành mã của chúng tôi tại https://github.com/VITA-Group/Random_Pruning .

8 LỜI CẢM ƠN

Công trình này đã được thực hiện khi Shiwei Liu làm việc như một thực tập sinh tại JD Explore Academy. Công trình này được hỗ trợ một phần bởi Dự án Lớn Đổi mới Khoa học và Công nghệ 2030 - "Khoa học Não bộ và Nghiên cứu giống Não bộ" (Số 2021ZD0201402 và Số 2021ZD0201405). Z. Wang được hỗ trợ một phần bởi Viện AI NSF cho Nền tảng Học máy (IFML).

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo từ trang 10-15 được dịch nguyên văn]

--- TRANG 16 ---
A CÁC PHÉP ĐO VÀ CHỈ SỐ

Chính thức, hãy ký hiệu một mạng (thưa) f:X!Y được huấn luyện trên các mẫu từ phân phối D. f(x;θ) và f(x;m) đề cập đến mạng dày đặc và mạng được cắt tỉa, tương ứng. Độ chính xác kiểm tra của các mạng con được cắt tỉa thường được báo cáo như độ chính xác của chúng trên các truy vấn kiểm tra được rút từ D, tức là P(x;y)∼D(f(x;m) = y). Ngoài độ chính xác kiểm tra, chúng tôi cũng đánh giá cắt tỉa ngẫu nhiên từ quan điểm độ bền chống đối kháng (Goodfellow et al., 2014), hiệu suất OoD (Hendrycks et al., 2021), và ước lượng không chắc chắn (Lakshminarayanan et al., 2016).

Độ Bền Chống Đối Kháng. Mặc dù có khả năng đáng kể trong việc giải quyết các vấn đề phân loại, dự đoán của mạng nơ-ron sâu thường có thể bị đánh lừa bởi các nhiễu đối kháng nhỏ (Szegedy et al., 2013; Papernot et al., 2016). Các công trình trước đây đã cho thấy khả năng tìm ra điểm ngọt ngào của độ thưa và độ bền chống đối kháng (Guo et al., 2018; Ye et al., 2019; Gui et al., 2019; Hu et al., 2020). Là phương pháp có thể ngây thơ nhất để tạo ra độ thưa, chúng tôi cũng quan tâm đến việc liệu huấn luyện một mạng con được cắt tỉa ngẫu nhiên có thể cải thiện độ bền chống đối kháng của mạng sâu. Chúng tôi theo phương pháp cổ điển được đề xuất trong Goodfellow et al. (2014) và tạo ra các ví dụ đối kháng với Phương pháp Dấu Gradient Nhanh (FGSM). Cụ thể, dữ liệu đầu vào được nhiễu với ε sign(∇xL(θ;x;y)), trong đó ε đề cập đến cường độ nhiễu, được chọn là 8/255 trong bài báo của chúng tôi.

Hiệu suất ngoài phân phối. Việc điều tra tổng quát hóa ngoài phân phối (OoD) có tầm quan trọng đối với học máy trong cả lĩnh vực học thuật và công nghiệp. Vì giả định i.i.d. khó có thể được thỏa mãn, đặc biệt là những tình huống có rủi ro cao như chăm sóc sức khỏe và quân sự. Chúng tôi đánh giá liệu cắt tỉa ngẫu nhiên có mang lại lợi ích cho OoD. Theo các quy trình cổ điển (Augustin et al., 2020; Meinke & Hein, 2020), SVHN (Netzer et al., 2011), CIFAR-100, và CIFAR-10 với nhiễu Gaussian ngẫu nhiên (Hein et al., 2019) được áp dụng cho các mô hình được huấn luyện trên CIFAR-10; ImageNet-O như tập dữ liệu OoD cho các mô hình được huấn luyện trên ImageNet.

Ước lượng không chắc chắn. Trong các tình huống quan trọng về an ninh, ví dụ: tự lái, các bộ phân loại không chỉ phải chính xác mà còn nên chỉ ra khi chúng có thể sai (Guo et al., 2017). Để kiểm tra ảnh hưởng của độ thưa được tạo ra đối với ước lượng không chắc chắn, chúng tôi chọn hai chỉ số được sử dụng rộng rãi, lỗi hiệu chuẩn dự kiến (ECE) (Guo et al., 2017) và log-likelihood âm (NLL) (Friedman et al., 2001).

[Phần còn lại của các phụ lục được dịch tương tự...]
