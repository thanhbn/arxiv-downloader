# 2309.11000.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/speech/2309.11000.pdf
# File size: 1523721 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Towards Joint Modeling of Dialogue Response and Speech Synthesis
based on Large Language Model
Xinyu Zhou ( 周欣宇)1Delong Chen ( 陈德龙)2Yudong Chen ( 陈玉东)1
1Communication University of China
2Hong Kong University of Science and Technology
{xinyuzhou, chenyd}@cuc.edu.cn ,delong.chen@connect.ust.hk
Abstract
This paper explores the potential of construct-
ing an AI spoken dialogue system that " thinks
how to respond " and " thinks how to speak " si-
multaneously, which more closely aligns with
the human speech production process com-
pared to the current cascade pipeline of in-
dependent chatbot and Text-to-Speech (TTS)
modules. We hypothesize that Large Language
Models (LLMs) with billions of parameters
possess significant speech understanding ca-
pabilities and can jointly model dialogue re-
sponses and linguistic features. We conduct
two sets of experiments: 1) Prosodic struc-
ture prediction, a typical front-end task in TTS,
demonstrating the speech understanding abil-
ity of LLMs, and 2) Further integrating dia-
logue response and a wide array of linguistic
features using a unified encoding format. Our
results indicate that the LLM-based approach
is a promising direction for building unified
spoken dialogue systems.1
1 Introduction
As we are developing more advanced AI systems,
such as Large Language Model (LLM)-based chat-
bots like ChatGPT and GPT-4 (OpenAI, 2023),
we also hope to establish natural, seamless, and
efficient communication between humans and AI
systems. In addition to typing and reading through
the screen, the speech channel represents a valu-
able alternative for interpersonal exchange, given
its convenience and capacity to convey richer in-
formation than text alone. Recently, researchers
from both academia and the industry have made
successful attempts to concatenate AI chatbots with
off-the-shelf text-to-speech (TTS) (Tan et al., 2021)
Xinyu Zhou and Delong Chen contributed equally.
This work is partially supported by National Social Sci-
ence Fund of China (20&ZD295).
1Codes and datasets are publicly available at https://
github.com/XinyuZhou2000/Spoken_Dialogue .
Dialogue
Response
 Front 
End
Acoustic 
model
V ocoderAudio
WaveformLinguistic
FeaturesAcoustic
Features
Dialogue
ModelUser Input
Text-to-Speech (TTS) Model
Conceptualization
Formulation
Grammatical
EncodingPhonological
Encoding
Articulation
Dialogue
Response +
Linguistic
Features
 Acoustic 
model
V ocoderAudio
WaveformAcoustic
Features
Large Language Model
as Dialogue Model & Front -endUser Input
Audio
Waveform
(a) Two-stage Cascaded Pipeline
(b) Human Speech Production
(c) Our Unified FrameworkFigure 1: A high-level comparison of different speech
production processes. As noted by the red dotted boxes,
the novel LLM-based unified framework proposed in this
study can “think how to respond” and“think how to speak”
at the same time, which aligns better with the speech pro-
duction process of humans.
modules as in Figure 1 (a), representative applica-
tions include Siri, Xiaomi Xiaoai2and Call Annie3.
However, the expressivity and interactivity of
speech responses synthesized by these two-stage
cascaded models are heavily limited. The reasons
are two-fold. Firstly , TTS modules are usually
based on small language models ( e.g.,BERT model
with 0.1B parameters), which have limited capac-
ity for understanding complex dialogue contexts.
Secondly , the dialogue response generation mod-
ule (i.e.,the LLM Chatbot) and the TTS module
work independently. During speech synthesizing,
the TTS module can not access the information
from the dialogue context, which is proven to be
valuable for generating plausible and appropriate
speech responses.
The current two-stage pipeline also has a funda-
mental difference with our understandings of the
human speech production process (Levelt, 1993),
where the “grammatical encoding” and“phono-
logical encoding” are done in parallel within the
“conceptualization-formulation-articulation” pro-
2https://xiaoai.mi.com
3https://callannie.aiarXiv:2309.11000v1  [cs.CL]  20 Sep 2023

--- PAGE 2 ---
cess, as shown in Figure 1 (b). Inspired by this,
we want to explore the possibility of building an
AI speech dialogue system that “thinks how to re-
spond” and“thinks how to speak” at the same time.
In order to accomplish this goal, a model must pos-
sess a deep understanding of natural language and
dialogue context, exhibit extensive world knowl-
edge and commonsense, and demonstrate adequate
learnability to handle text-speech joint modeling.
We hypothesize LLMs with (hundreds) billions
of parameters (in comparison with BERT-based
TTS front-ends (Chen et al., 2022) with only 0.3B
parameters) are capable of achieving this goal. To
verify this, in this paper, we provide two groups
of experiments to demonstrate the possibility of
building such LLM-based unified speech dialogue
system.
Firstly , we get started with the prosodic struc-
ture prediction (Section 3), a typical task within the
TTS text analysis front-end, to showcase the speech
understanding ability of LLMs. Results show that
both prompting-based ChatGPT and fine-tuning
based ChatGLM (Zeng et al., 2022) model achieve
competitive performance against traditional meth-
ods. We also show that LLM can utilize linguistic
knowledge to improve prediction accuracy.
Secondly , we aim to further integrate a wide
array of linguistic features into the model, and
maintain LLM’s dialogue capability at the same
time (Section 4). To address the lack of a parallel
dataset of dialogue response and linguistic annota-
tions, we employ an automated dialogue context
generation approach inspired by LongForm (Kök-
sal et al., 2023), then train an LLM to produce both
dialogue response speech features at the same time.
Experiments show that LLM learns successfully.
2 Related Work
2.1 Human Speech Production
The process of human speech production is a long-
standing research area. In 1993, Levelt (Levelt,
1993) proposed an encoding model for human
speech production. First, concepts are generated,
followed by the selection of appropriate vocabu-
lary and the arrangement of these words according
to grammatical rules. Then, the phonetics of the
words are extracted in sequence, and motor pro-
grams are executed to initiate speech. The genera-
tion of spoken sentences is parallel and incremen-
tal, involving multiple stages of processing. Experi-
ments (Schnur, 2011; Jaeger et al., 2012) prove thatthe phonetic planning of words begins as the gram-
matical structure of a sentence unfolds. Although
there are many efforts to understand and explain
human speech production process, TTS methods
rarely take inspiration from these research results.
To our best knowledge, this is the first study that
attempts to build an AI system that imitates the si-
multaneous “grammatical encoding” and“phono-
logical encoding” process of human speech pro-
duction.
2.2 TTS Front-end and Expressivity
Typical TTS systems (Tan et al., 2021) usually con-
sist of three main modules: front-end, acoustic
model, and vocoder. The TTS Front-end models
convert text into linguistic features, and are primar-
ily BERT-based small language models, while the
power of LLM is not well validated in this task
yet. Hsu et al. (Hsu et al., 2021) and Stephenson
et al. (Stephenson et al., 2022) have demonstrated
that fine-tuning BERT can enhance the prosodic ex-
pression capabilities of TTS systems. Nevertheless,
issues such as homograph ambiguity, ineffective-
ness in stress, emotion and prosody still exist. Re-
cent studies have explored the use of interactional
resources (Chen, 2023), such as breathing (Székely
et al., 2020), laughter (Xin et al., 2023), phonation
type (Lameris et al., 2023), filled pauses and prolon-
gations (Li et al., 2023), to improve the spontaneity
and expressiveness. However, these studies have
only focused on one single interactional resource,
which limits their ability to capture rich and diverse
subtle variations in natural conversation.
2.3 LLMs for Speech Processing
Understanding and generating speech signals are
strongly related to natural language processing.
With the recent explosion of LLM, many re-
searchers in the field of speech processing also
attempt to use LLMs to benefit speech or audio
related tasks. AudioLM (Borsos et al., 2023)
leverages a masked language model to capture the
long-term structure and generate natural and co-
herent audio continuations given short prompts.
SpeechGPT (Zhang et al., 2023), a multi-modal
large language model, leverages its inherent ca-
pabilities to perceive and generate multi-modal
content. PromptTTS (Guo et al., 2023) and
PromptTTS2 (Leng et al., 2023) take prompts with
both style and content descriptions as input to syn-
thesize the corresponding speech.

--- PAGE 3 ---
Response
Text
Front 
End
Acoustic 
model
V ocoderAudio
WaveformLinguistic
FeaturesAcoustic
Features
Input : 结果今天早上一上班，萨摩耶很幽怨的趴在那，哈士奇却不见了。
As a result, when I went to work this morning, 
the Samoyed lay there resentfully, but the Husky disappeared.
结果
PW
Output :结果 #2今天 #1早上 #1一上班 #3，萨摩耶 #2很幽怨的 #1趴在那 #3，
哈士奇 #1却#1不见了 #4。Prosodic Word
(PW, #1) Prediction
Prosodic Phrase (PPH, 
#2) Prediction
Intonation Phrase 
(IPH, #3) Prediction
Pronunciation
Dictionary
Polyphone
DisambiguationPolyphone Characters:
•一: yi1 / yi2 / yi4
•耶:  ye1 / ye2   
•的:  de5 / di2 / di4 / di1
•了:  le5 / liao3
Output :jie2 guo3  jin1 tian1 zao3 shang 5
yi2 shang 4 ban1 sa4 mo2 ye1  hen3  you1 
yuan 4 de5 pa1 zai4 na4 ha1shi4qi2 que4  
bu2jian4le5 
Dialogue
ModelUser Input
今天
PW早上
PW一
PW上班
PW萨摩耶
PW很
PW幽怨的
PW趴在那
PW哈士奇
PW却
PW不见了
PW
结果
PPH今天早上一上班
PPH萨摩耶
PPH很幽怨的趴在那
PPH哈士奇却不见了
PPH
结果今天早上一上班
IPH萨摩耶很幽怨的趴在那
IPH哈士奇却不见了
IPH
Prosody Structure Prediction
Grapheme- to-Phoneme Input : 结果今天早上 一上班，萨摩耶 很
幽怨的趴在那，哈士奇却不见了 。Text Normalization
Word Segmentation
Part of Speech Tagging
…
Text-to-Speech (TTS) ModelFigure 2: Standard pipeline of current spoken dialog sys-
tems. A dialogue model generates a response to user input,
and the TTS model (front-end →acoustic model →vocoder)
converts text to audio subsequently.
3 Prosodic Structure Prediction based on
Large Language Model
Prosodic Structure Prediction (PSP) is a typical
task in the Chinese TTS front-end (Chen et al.,
2022), among others like grapheme-to-phoneme
prediction, text-normalization, word segmentation,
part-of-speech tagging, etc. As illustrated in Fig-
ure 2, a PSP model needs to identify multiple
levels of prosody hierarchy, including Prosodic
Word (PW), Prosodic Phrase (PPH), and Intona-
tion Phrase (IPH), which can be denoted as #1, #2,
and #3 respectively in the output sentence.
Prosodic structure is one of the most important
linguistic features in Chinese TTS, and it is strongly
related to the syntax of the sentence. In this section,
we want to validate whether the LLMs, which have
been well-proved to have superior semantic under-
standing abilities, but are trained on the text-only
corpus, can handle this speech-related task. In the
following, we present two methods for adapting
LLM to the PSP task: prompting (Section 3.1), and
fine-tuning (Section 3.2).
3.1 Prompting LLM for Prosodic Structure
Prediction
Prompting is the most convenient way to adapt an
instruction-following LLM to new tasks. In Fig-
ure 3, we present an overview of our proposed
prompt structure for PSP on LLM, which consists
of linguistic knowledge of Chinese prosodic struc-
ture, few-shot demonstrations as in-context learn-
You have learned thetheoretical knowledge ofprosodic levels, andlearned
therules ofprosodic level labeling from examples .
Next, please carefully understand thefollowing sentences, annotate the
prosodic hierarchy, and output theresulting sentences directly without
adding anyadditional content (such as'output :'orlinebreaks) .
Now thatyouareaprosodic hierarchy annotator, please learn thefollowing
about prosody hierarchies :
•Prosodic word (#1):Aprosodic word isoften adictionary word …
•Prosodic Phrases (#2):isamedium rhythmic module between…
•Intonation Phrases (#3):Intonation phrases arecomposed ofmultiple …
You have learned thetheory oftheprosodic hierarchy .Next, please
understand thefollowing example carefully .
Example 1
•Input :地藏菩萨铜像由洛阳铜加工集团金像公司负责监制。
•Output :地藏#1菩萨#1铜像 #3由 #1洛阳铜 #2加工 #1集团#2 金像#1公司
#2负责#1监制#4。
Example 2
•Input :冲锋在前的他， 遭遇对方拼命拒捕反抗， 右手肘粉碎性骨折 。
•Output :冲锋#1在前的他 #3，遭遇 #1对方 #2拼命 #1拒捕#1 反抗#3，右
手肘#2粉碎性#1骨折#4。
...
“结果今天早上一上班，萨摩耶很幽怨的趴在那，哈士奇却不见了。”
Your prediction is:System
Message
Linguistic
Knowledge
System
Message
Few-shot
Demonstrations
drawn from 
training split
System
Message
Input Text
结果#2今天#1早上#1一上班 #3，萨摩耶 #2很幽怨的#1 趴在那 #3，哈士
奇#1却#1不见了#4 。OpenAI ChatGPT
System
MessageFigure 3: Our proposed prompt structure for LLM
(ChatGPT)-based prosodic structure prediction. We in-
corporate expert linguistic knowledge and few-shot demon-
strations to enable LLMs to perform the prosodic structure
prediction task.
ing examples, input sentence, and interleaved sys-
tem messages to explain each part to the LLM.
Linguistic Knowledge contains formal defini-
tions of Chinese prosodic structure summarized
from recognized research literature (Cao, 2003).
It describes distinct characteristics and positions
within sentences and phrases of three levels of
prosodic structure in Chinese.
Few-shot Demonstration provides input-output
pairs to LLM for in-context learning. Examples
are either randomly drawn from the training split
or carefully selected based on the assessment of
their representativity and quality from the linguis-
tic perspective. The maximum number of few-shot
demonstrations is 16, as more examples would ex-
ceed the context window length of LLM.
3.2 Fine-tuning LLM for Prosodic Structure
Prediction
Context window length is a crucial limit for
prompting-based methods, as it prohibits the LLM
from learning from more (than 16) training exam-
ples. Furthermore, all model parameters remain
fixed and unlearnable, resulting in limited learning
capacity. To address these constraints, we propose
the fine-tuning of a Large Language Model (LLM)
to enhance Prosodic Structure Prediction learning
from a substantially larger number of training ex-

--- PAGE 4 ---
amples, up to 8,000.
It has been proved that using a Pretrained Lan-
guage Model (PLM) such as BERT (Devlin et al.,
2018) to be the initialization of the PSP model is
beneficial, such as SpanPSP (Chen et al., 2022), J-
TranPSP (Shen et al., 2022), and MLC-PSP (Chen
et al., 2023), our methodology of fine-tuning LLM
has some difference from them. Despite the differ-
ence in model scale (0.1B vs 6B), previous BERT-
based methods regard PSP as a token classifica-
tion problem, where the model needs to deter-
mine whether there is a prosodic boundary after
each character and what level is it. In contrast,
here we formalize PSP as a sequence-to-sequence
(Seq2seq) prediction task, where input xis the raw
sentence and the output yis a string of character
sequence with “# n” (n∈ {1,2,3}) notation of
prosodic structure.
We apply standard cross-entropy loss for auto-
regressive language modeling as the learning ob-
jective, and we only calculate the loss on output
tokens. We add a prefix cof “Please perform
prosodic prediction on the given sentence:” into
the input for better initialization. The following
is the loss function L(θ)of the LLM θ, where
Nis the number of training samples: L(θ) =
−PN
i=1logpθ(yi|xi, ci).
3.3 Experiment Setup
Dataset . We utilize the DataBaker open-source
Chinese Standard Mandarin Speech Corpus4,
which contains 10-hour speech recordings of
10,000 sentences with an average length of around
16 words per sentence. It was articulated by a single
Chinese female speaker. The corpus also encom-
passes diverse domains, including news, novels,
technology, entertainment, etc.
Furthermore, the dataset is enriched with various
linguistic annotations, including character, pinyin,
and prosodic hierarchy information, as well as
phoneme level interval and boundary data. An-
notations for prosodic hierarchy comprise PW (#1),
PPH(#2), IPH (#3), and the end of a sentence (#4).
We discard the #4 annotations as every sample is a
single sentence and only has a “#4” in the end. The
remaining labels collectively form a hierarchical
prosodic tree structure with three distinct layers.
We split 10k samples with an 8:1:1 ratio for train-
ing, validation, and testing. Few-shot demonstra-
tions are drawn from the 8k training split. For the
4https://www.data-baker.com/data/index/TNtts
41.3 64.0 64.9 72.7 75.1 74.6 74.4 77.4 77.4 78.6 78.3 80.3 80.1 
40455055606570758085
random
selected
random
selected
random
selected
random
selected
random
selected
random
selected
0 4 4 4 4 8 8 8 8 16 16 16 16Average F -Score
Selection / Number of Few -shot DemonstrationsMore Few -shot Demonstrations
w/o Linguistic Knowledge w/ Linguistic KnowledgeFigure 4: Ablation study of prompting ChatGPT based
PSP. We compared different numbers of few-shot demonstra-
tions, selection of few-shot demonstrations, and variants of
with (w/) or without (w/o) linguistic knowledge.
“random” selection setting, we sample demonstra-
tions randomly three times and report the averaged
performance.
Implementation Details . For the prompting-
based method, we test the OpenAI’s
text-davinci-002 API (ChatGPT) and the
ChatGLM2-6B model. For the fine-tuning-based
method, we only unitize the ChatGLM2-6B model
due to the limitation of computational resources.
We apply P-tuning-v2 (Liu et al., 2022) for
parameter-efficient fine-tuning using the official
codebase5. We used a single NVIDIA A100 GPU
for both training and testing.
3.4 Ablation Study
We first provide ablations for the prompting-based
approach. Following previous works on PSP tasks,
we use F-Score as the evaluation metric. As it
can be seen from Figure 4, the number of few-shot
demonstrations makes a significant impact. Four in-
context examples lead to +22.7% improvements to
zero-shot setting (41.3% →64.0%), while incorpo-
rating linguistic knowledge brings another around
+8.7% improvements ( →72.7%), and further, when
swapping random demonstrations to carefully se-
lected high-quality demonstrations, we receive an-
other +2.4% performance gain ( →75.1%).
We further ablate different levels of linguistic
knowledge in Table 2. It shows that linguistic ex-
pert knowledge plays a crucial role in the prediction
of Prosodic Phrase (#2) and Intonational Phrase
(#3). We hypothesize it is caused by different diffi-
culties of #1 to #3 predictions – #1 usually appears
at word boundaries, while identifying #2 and #3 is
5https://github.com/THUDM/ChatGLM2-6B/tree/
main/ptuning

--- PAGE 5 ---
Table 1: Benchmarking of PSP Models . We compared the F-Score of the traditional BERT-based method SpanPSP and our
newly proposed LLM-based methods (prompting or fine-tuning) using two LLMs with different scales (ChatGPT and ChatGLM).
Model (#Parameters) Variation PW #1 PPH #2 IPH #3 Average
SpanPSP (0.1B)Databaker Pretrained 96.35 69.34 65.64 77.11
PeopleDaily Pretrained 89.20 71.08 79.12 79.80
ChatGPT (175B)Knowledge Only 61.87 27.27 34.78 41.31
16 Random Examples 88.51 69.40 77.91 78.61
Knowledge + 16 Selected Examples 90.12 69.40 80.85 80.12
ChatGLM2-6BKnowledge + 16 Selected Examples N/A N/A N/A N/A
Fine-tuned 93.86 73.28 80.00 82.38
Table 2: Ablations of removing each level of linguistic
knowledge. Expert knowledge is especially useful for higher
levels of prosodic structure prediction ( i.e.,PPH and IPH).
Knowledge
AblationPW #1
F-ScorePPH #2
F-ScoreIPH #3
F-ScoreAverage
F-Score
w/o #1 88.54 64.66 78.30 77.17
w/o #2 87.57 61.63 79.09 76.10
w/o #3 87.72 64.69 78.14 76.85
Default (all) 88.14 65.03 79.52 77.56
not that straightforward.
3.5 Benchmarking LLM-based PSP
Baseline. SpanPSP (Chen et al., 2022) is a classical
character-level BERT-based model for the PSP task,
which is based on a relatively small language model
bert-base-chinese6with only 0.1B parameters.
We use their official checkpoints and codebase7for
evaluation.
We provide benchmarking results in Table 1. It
reveals that carefully crafted linguistic knowledge
and selected examples ( i.e.,“Knowledge + 16 Se-
lected Examples” variation) enable ChatGPT to out-
perform the traditional method SpanPSP (80.12%
vs. 79.80%), but such a prompting-based learn-
ing strategy failed (N/A) at smaller open-source
LLM (ChatGLM) due to its limited instruction-
following ability. However, it shows that fine-
tuning smaller LLM can outperform prompting
larger LLM (82.38% vs. 80.12%), as it can ac-
cess more training samples (8k training set vs. the
maximum of 16 in-context examples).
4 Joint Prediction of Dialogue Response
and Linguistic Features
In the last section, we have shown some positive
results proving LLMs are competitive at a typical
front-end task in Chinese TTS. Here in this section,
we want to go beyond just a single task in TTS, and
6https://huggingface.co/bert-base-chinese
7https://github.com/thuhcsi/SpanPSPvalidate the possibility of building a LLM-based
system that can handle versatile tasks in Chinese
TTS front-end (Figure 2), and also maintain its dia-
logue capability of generating coherent responses
to user queries simultaneously. By implementing
such a model that jointly predicts dialogue response
(i.e., “think how to respond” ) and linguistic fea-
tures ( i.e., “think how to speak” ) at the same time,
we could have an AI system that communicates
with humans in a way that is more similar to the
human speech production process (Levelt, 1993),
where the “grammatical encoding” and “phonolog-
ical encoding” are done in parallel.
4.1 Methodology
Dialogue Context Generation . Our objective is
to investigate the feasibility of constructing a uni-
fied model capable of simultaneously generating
coherent responses to user queries in dialogues and
diverse fine-grained linguistic features for TTS. Un-
fortunately, the DataBaker dataset only comprises
isolated sentence recordings, and there are not any
other datasets having dialogue context and parallel
speech recordings or annotations. Drawing inspi-
ration from the LongForm approach (Köksal et al.,
2023), we prompt ChatGPT to anticipate the di-
alogue context and transform it into a dataset of
single-turn dialogues:
Prompt for Dialogue Context Generation
### System Message:
Please generate the most likely sentence spoken by A
based on B’s response.
### User:
A:
B:“When I went to work this morning, the Samoyed
lay there resentfully, but the Husky disappeared. ”
### ChatGPT:
A:“What’s going on? Where did the Husky go?”
Linguistic Feature Extraction . As shown in
Figure 5 left, we automatically extract the follow-

--- PAGE 6 ---
萨摩 耶 忧闷 很 趴的 在 那，
#2 #1 #3
0.17
 0.18
 0.21
 0.24
 0.17
 0.19
 .
0.24
 0.16
 0.27
 Duration (s)
Pinyin
Character
6.11 D
6.78 D
7.23 D
7.87 D
Highest &LowestPitch (D -Value) 
of each Syllable/Character
Prosody 
Hierarchy
结果今天早上一上班，萨摩耶很幽怨的趴
在那，哈士奇却不见了。
{
"character": " 结", 
"pinyin": "jie2", 
"highest pitch ": 7.76, 
"lowest pitch": 7.31, "duration": 0.22, "prosody hierarchy": "null" 
},{
"character": " 果", 
"pinyin": "guo3", "highest pitch ": 7.58, 
"lowest pitch": 5.4, "duration": 0.25, "prosody hierarchy": "#2" 
},...
怎么回事？哈士奇去哪了？Input
ChatGPT -generated dialogue 
context
Output
Dialogue response + Json-
style linguistic featureChatGLM2-6B 
Fine-tuning (full-parameter)
Figure 5: Left: an overview of linguistic feature extraction. We automatically extract a wide array of linguistic features,
including character, duration, pinyin, prosody hierarchy, highest pitch and lowest pitch ( D-Value). Right: the illustration of data
formatting. We encode extracted linguistic features into JSON-formatted strings, such that they can be fed to LLM directly as
learning targets.
ing four categories of linguistic attributes: char-
acters, their corresponding duration, pinyin (pho-
netic transcription representing character pronun-
ciation), prosodic hierarchy, and the highest and
lowest pitch values ( D-Value). The use of D-value
is inspired by Shen Jiong’s theory (Shen, 1985):
the D-value is a logarithmic scale used to describe
pitch and quantifies the relationship between a pitch
(F) in Hertz (Hz) and a reference frequency ( F0).
It provides a measure of pitch variation, which
is especially useful for observing pitch contours
in speech. The formal definition of ( D-Value) is:
D= 5×log2(F/F 0).
Data Formatting . As shown in the right side
of Figure 5, we format extracted linguistic features
into a string of JSON-style dictionaries, and con-
catenate it with the response text generated by Chat-
GPT, together serving as the learning target. Such
implementation realizes joint learning in a seam-
less way and enjoys simplicity over the traditional
method, where different types of outputs (response,
various linguistic features) are usually produced
by different models, or one model with different
task-specific heads (Bai and Hu, 2021). Our ap-
proach also shares some similarities with recent
advances in LLM research, such as RT-2 (Brohan
et al., 2023) from Google DeepMind, where the
LLM are trained to produce not only natural lan-
guage output but also some continuous values.
4.2 Experiment Setup
Training . Empirically, we found that P-tuning (as
used in fine-tuning-based PSP in Section 3.2) failedTable 3: Evaluation result of LLM produced linguistic
features. Most of the output JSON-style is incorrect grammar
and parsable, and the majority of these parsable characters
can be matched with ground truth. However, we can observe
a notable train-test performance gap, meaning that the model
suffers from overfitting.
Parsable
SamplesMatched
CharactersMatched
PinyinMatched
Prosody
Training Split 95.90% 86.88% 98.79% 97.75%
Testing Split 89.70% 69.26% 86.29% 77.70%
to learn how to generate dialogue response and
JSON-style linguistic features. Therefore, for this
section, we turn to use full-parameter fine-tuning
to enable more learning capacity. We use 4-bit
quantization to boost memory efficiency, as JSON-
style encoding takes much longer context than that
in the PSP task (maximum 1.6k tokens vs. 128
tokens).
Testing . Based on our data formatting (Figure 5),
given a user utterance as input, the model will first
give its dialogue response, then the JSON-style
linguistic feature of each word in the response sen-
tence subsequently. However, this poses a chal-
lenge for the evaluation of the linguistic feature,
since for unseen testing quires, the LLM-outputted
response would be different from the ground truth
response, thus making them not comparable. To
solve this issue, we use the ground truth response
as a generation prefix, and then try to parse the gen-
erated dictionary and compare them with ground
truth linguistic features.

--- PAGE 7 ---
Training
Split
Testing
Split
Prosody Structure Lowest Pitch Highest Pitch DurationFigure 6: Evaluations results of fine-tuning ChatGLM2-6B on joint dialogue response and linguistic features. Visualiza-
tions show that the model fits the training data quite well, showing the feasibility of our proposed joint learning approach. But
possibly due to insufficient dataset scale, the generalization ability of the model is somewhat weak.
4.3 Experiment Results
In Table 3, we provide the evaluation result of 1k
testing samples and randomly sampled 1k training
samples. As can be seen, the model performed
quite well on the training set, achieving 95.90%
parsable samples, 86.88% matched characters, and
98.79% matched Pinyin, showing that it success-
fully fit the JSON-format data. When tested on
unseen samples, the model successfully generated
Json-style linguistic features with an 89.70% suc-
cess rate. However, due to the limited capacity of
the small LLM, missing characters were frequently
observed, resulting in only 69.26% of characters
from the ground truth being found in the gener-
ated results. Within those matched characters, the
model achieved an 86.29% success rate in produc-
ing matched Pinyin, and a 77.70% success rate of
matched prosody structure annotation.
In Figure 6, we visualize the model predic-
tions versus the ground truth of continuous values.
Again, we observed that the model effectively fit
the training set and demonstrated a certain level of
generalization ability when applied to new data.
5 Discussion
In this study, we presented two groups of exper-
iments to validate the possibility of building an
LLM-based spoken dialog system that “thinks how
to respond” and“thinks how to speak” at the same
time. In the first group of experiments, we proved
that LLM is a competitive prosodic structure predic-tor, which means that its rich world knowledge and
semantic understanding ability acquired from text-
only pretraining can transferred to benefit speech-
related tasks. Based on this observation, we fur-
ther involve many other linguistic features in our
second group of experiments and further proved
that it is possible for LLM to learn to generate di-
alogue response and speech features at the same
time. However, there are still several noticeable
limitations of this study, which are summarized
from the following three perspectives:
Model Perspective . The training cost of LLM
is high. Additionally, the auto-regressive decod-
ing of Json-style linguistic features is quite time-
consuming – processing a single sentence and its
linguistic features takes at least 15 seconds (for
long sentences, it could be 40+ seconds).
Data Perspective . The current training dataset
consists of only 8k samples, which is insuffi-
cient and has led to a substantial over-fitting phe-
nomenon. Speech style in the dataset is limited, as
it was sourced from a single speaker, primarily con-
taining formal read recordings, lacking the nuances
inherent in natural conversations.
Expressivity Perspective . According to interac-
tional linguistics studies (Couper-Kuhlen and Selt-
ing, 2017), finer-grained annotation system by com-
prehensively and meticulously annotating the col-
lected speech dataset with interactional resources
like voice quality, phonation type, breath patterns,
repair, interjection, pause, prolongation, etc, will
further increase the expressivity.

--- PAGE 8 ---
System Perspective . It is important to note that
so far this study does not include subsequent acous-
tic models and vocoders and is not able to generate
audio waveform, we only use speech linguistic fea-
tures to represent speech information.
References
Zilong Bai and Beibei Hu. 2021. A universal bert-based
front-end model for mandarin text-to-speech synthesis.
InICASSP 2021-2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages
6074–6078. IEEE.
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene
Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Rob-
lek, Olivier Teboul, David Grangier, Marco Tagliasacchi,
et al. 2023. Audiolm: a language modeling approach to au-
dio generation. IEEE/ACM Transactions on Audio, Speech,
and Language Processing .
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Cheb-
otar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny
Driess, Avinava Dubey, Chelsea Finn, et al. 2023. Rt-2:
Vision-language-action models transfer web knowledge to
robotic control. arXiv preprint arXiv:2307.15818 .
Jianfen Cao. 2003. Prediction of prosodic organization based
on grammatical infomation (in chinese). Journal of Chi-
nese Information Processing , (41-46).
Jie Chen, Changhe Song, Deyi Tuo, Xixin Wu, Shiyin Kang,
Zhiyong Wu, and Helen Meng. 2023. Improving mandarin
prosodic structure prediction with multi-level contextual
information. arXiv preprint arXiv:2308.16577 .
Xueyuan Chen, Changhe Song, Yixuan Zhou, Zhiyong Wu,
Changbin Chen, Zhongqin Wu, and Helen Meng. 2022. A
character-level span-based model for mandarin prosodic
structure prediction. In ICASSP 2022-2022 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 7602–7606. IEEE.
Yudong Chen. 2023. Studies of prosodic expressions in in-
teractive language: A review (in chinese). Contemporary
Linguistics , 25(197-222).
Elizabeth Couper-Kuhlen and Margret Selting. 2017. Interac-
tional linguistics: Studying language in social interaction .
Cambridge University Press.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2018. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 .
Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and
Xu Tan. 2023. Prompttts: Controllable text-to-speech with
text descriptions. In ICASSP 2023-2023 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 1–5. IEEE.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman
Mohamed. 2021. Hubert: Self-supervised speech repre-
sentation learning by masked prediction of hidden units.
IEEE/ACM Transactions on Audio, Speech, and Language
Processing , 29:3451–3460.
T Florian Jaeger, Katrina Furth, and Caitlin Hilliard. 2012.
Incremental phonological encoding during unscripted sen-
tence production. Frontiers in Psychology , 3:481.
Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich
Schütze. 2023. Longform: Optimizing instruction tuningfor long text generation with corpus extraction. arXiv
preprint arXiv:2304.08460 .
Harm Lameris, Shivam Mehta, Gustav Eje Henter, Joakim
Gustafson, and Éva Székely. 2023. Prosody-controllable
spontaneous tts with neural hmms. In ICASSP 2023-2023
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 1–5. IEEE.
Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian
Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying
Zhang, Kaitao Song, et al. 2023. Prompttts 2: Describ-
ing and generating voices with text prompt. arXiv preprint
arXiv:2309.02285 .
Willem JM Levelt. 1993. Speaking: From intention to articu-
lation . MIT press.
Weiqin Li, Shun Lei, Qiaochu Huang, Yixuan Zhou, Zhiyong
Wu, Shiyin Kang, and Helen Meng. 2023. Towards spon-
taneous style modeling with semi-supervised pre-training
for conversational text-to-speech synthesis. arXiv preprint
arXiv:2308.16593 .
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning
can be comparable to fine-tuning across scales and tasks. In
Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers) ,
pages 61–68.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Tatiana T Schnur. 2011. Phonological planning during sen-
tence production: Beyond the verb. Frontiers in Psychol-
ogy, 2:319.
Binbin Shen, Jian Luan, Shengyan Zhang, Quanbo Shen, and
Yujun Wang. 2022. J-tranpsp: A joint transition-based
model for prosodic structure prediction, word segmentation
and pos tagging. In 2022 13th International Symposium
on Chinese Spoken Language Processing (ISCSLP) , pages
280–284. IEEE.
Jiong Shen. 1985. The pitch range of tone and intonation in
Beijing dialect (in Chinese) , 84.
Brooke Stephenson, Laurent Besacier, Laurent Girin, and
Thomas Hueber. 2022. Bert, can he predict contrastive
focus? predicting and controlling prominence in neural tts
using a language model. arXiv preprint arXiv:2207.01718 .
Éva Székely, Gustav Eje Henter, Jonas Beskow, and Joakim
Gustafson. 2020. Breathing and speech planning in spon-
taneous speech synthesis. In ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 7649–7653. IEEE.
Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. 2021.
A survey on neural speech synthesis. arXiv preprint
arXiv:2106.15561 .
Detai Xin, Shinnosuke Takamichi, Ai Morimatsu, and Hiroshi
Saruwatari. 2023. Laughter synthesis using pseudo pho-
netic tokens with a large-scale in-the-wild laughter corpus.
arXiv preprint arXiv:2305.12442 .
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu
Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,
Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-
trained model. arXiv preprint arXiv:2210.02414 .
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang,
Yaqian Zhou, and Xipeng Qiu. 2023. Speechgpt: Empow-
ering large language models with intrinsic cross-modal
conversational abilities. arXiv preprint arXiv:2305.11000 .
