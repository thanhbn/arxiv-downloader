# Đánh giá tĩnh về việc hoàn thiện mã bởi các mô hình ngôn ngữ lớn

Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski,
Xiaopeng Li, Murali Krishna Ramanathan, Baishakhi Ray,
Parminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang
AWS AI Labs
{dhantian, kuvrun, tiayuche, zijwan, robkwiat, xiaopel
mkraman, rabaisha, parmib, sudipta, drot, bxiang}@amazon.com

## Tóm tắt

Các mô hình ngôn ngữ lớn được huấn luyện trên mã đã cho thấy tiềm năng lớn trong việc tăng năng suất của các nhà phát triển phần mềm. Một số tiêu chuẩn đánh giá dựa trên thực thi đã được đề xuất để đánh giá tính đúng đắn về mặt chức năng của mã được tạo ra bởi mô hình trên các bài toán lập trình đơn giản. Tuy nhiên, việc thực hiện cùng một đánh giá trên các dự án thực tế phức tạp là tốn kém khi xem xét chi phí thực thi. Ngược lại, các công cụ phân tích tĩnh như linter, có thể phát hiện lỗi mà không cần chạy chương trình, chưa được khám phá kỹ để đánh giá các mô hình tạo mã. Trong nghiên cứu này, chúng tôi đề xuất một khung đánh giá tĩnh để lượng hóa các lỗi tĩnh trong việc hoàn thiện mã Python, bằng cách tận dụng Cây Cú pháp Trừu tượng. So với đánh giá dựa trên thực thi, phương pháp của chúng tôi không chỉ hiệu quả hơn mà còn có thể áp dụng cho mã trong thực tế. Để thực nghiệm, chúng tôi thu thập ngữ cảnh mã từ các kho lưu trữ mã nguồn mở để tạo ra một triệu thân hàm sử dụng các mô hình công khai. Phân tích tĩnh của chúng tôi tiết lộ rằng Tên Không xác định và Biến Không sử dụng là những lỗi phổ biến nhất trong số các lỗi khác được tạo ra bởi các mô hình ngôn ngữ. Thông qua các nghiên cứu mở rộng, chúng tôi cũng cho thấy tác động của nhiệt độ lấy mẫu, kích thước mô hình và ngữ cảnh đối với các lỗi tĩnh trong việc hoàn thiện mã.

## 1 Giới thiệu

Việc hoàn thiện mã tự động bởi các mô hình ngôn ngữ lớn được huấn luyện trên nhiều kho lưu trữ mã đã chứng minh tiềm năng lớn trong việc tăng tốc phát triển phần mềm. Các dịch vụ trợ lý mã được hỗ trợ bởi những mô hình này cung cấp cho các nhà phát triển những gợi ý mã theo ngữ cảnh hiện tại trong thời gian thực. Tuy nhiên, đã được chỉ ra rằng khoảng 70% các gợi ý bị người dùng từ chối trong một nghiên cứu gần đây (Ziegler et al., 2022). Thậm chí tệ hơn, các khuyến nghị sai lệch có thể dẫn đến thất bại trong việc hoàn thành các nhiệm vụ lập trình (Vaithilingam et al., 2022). Do đó, việc hiểu được điểm yếu của các mô hình tạo mã hiện tại thông qua đánh giá và phân tích toàn diện là quan trọng.

**Hình 1:** Một ví dụ về hoàn thiện hàm, với lỗi Biến Không sử dụng (màu xám) trong ngữ cảnh, và lỗi Tên Không xác định (màu đỏ) trong việc hoàn thiện.

Gần đây, đánh giá dựa trên thực thi đã trở nên ngày càng phổ biến, nơi mã được tạo ra bởi mô hình được thực thi với các bài kiểm tra đơn vị để kiểm tra tính đúng đắn về mặt chức năng. Một số tiêu chuẩn đánh giá đã được đề xuất theo hướng này, như HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), MBXP (Athiwaratkun et al., 2022), CodeContests (Li et al., 2022), và DS-1000 (Lai et al., 2022). Mặc dù những tiêu chuẩn này rất đáng tin cậy và chính xác, chúng chỉ tập trung vào các bài toán thuật toán và khoa học dữ liệu được định nghĩa rõ ràng, không phản ánh nhu cầu trong phát triển phần mềm nói chung. Việc chạy đánh giá dựa trên thực thi với các cơ sở mã thực tế tuy nhiên là cực kỳ tốn kém vì mỗi dự án đều yêu cầu thiết lập khác nhau và chi phí tính toán có thể không giới hạn.

Trái ngược với phương pháp dựa trên thực thi, phân tích chương trình tĩnh (hoặc phân tích tĩnh) có thể phân tích các chương trình mà không cần thực thi chúng. Mặc dù phân tích tĩnh thường không thể xác định tính đúng đắn về mặt chức năng, nó bao gồm một tập hợp lớn các loại lỗi tĩnh, như tên không xác định hoặc biến không sử dụng được minh họa trong Hình 1. Quan trọng hơn, việc phân tích có thể rất nhanh và không yêu cầu bất kỳ thiết lập môi trường cụ thể nào cho dự án, điều này cho phép chúng tôi đánh giá các hoàn thiện mô hình cho mã thực tế phức tạp ở quy mô lớn. Các công cụ phân tích tĩnh như linter đã được sử dụng rộng rãi, ví dụ trong các trình soạn thảo mã, để kiểm tra mã do con người viết, nhưng giá trị của chúng trong việc đánh giá các mô hình tạo mã vẫn chưa được khám phá kỹ.

Trong nghiên cứu này, chúng tôi đề xuất một khung đánh giá tĩnh cho ngôn ngữ Python. Các đoạn mã đầu tiên được phân tích thành Cây Cú pháp Trừu tượng (AST) và sau đó được phân tích bởi Pyflakes, một công cụ phân tích tĩnh phổ biến cho Python. Để mô phỏng các trường hợp sử dụng thực tế của tự động hoàn thiện, chúng tôi thu thập mã từ các kho lưu trữ Github công khai để xây dựng một tập dữ liệu hoàn thiện hàm gồm 100K bài toán. Trong mỗi bài toán, chúng tôi ngẫu nhiên che đi một thân hàm trong một tệp Python và yêu cầu mô hình hoàn thiện nó dựa trên ngữ cảnh đi trước cho đến tiêu đề hàm. Sau đó chúng tôi đánh giá các mô hình công khai bằng cách lấy mẫu 10 hoàn thiện cho mỗi bài toán, dẫn đến một triệu lần tạo ra cho mỗi mô hình và nhiệt độ lấy mẫu, sẽ được kiểm tra bởi pipeline đánh giá tĩnh của chúng tôi.

Trong quá trình phân tích AST, chúng tôi thấy rằng hầu hết các lỗi phát sinh từ các lần tạo ra không hoàn chỉnh đã đạt đến giới hạn độ dài tối đa. Ngoài ra, các mô hình ở mọi kích thước đều hoạt động khá tốt trong việc tạo ra mã có thể phân tích được. Tiến về phía trước, phân tích Pyflakes tiết lộ rằng Tên Không xác định và Biến Không sử dụng là những lỗi tĩnh nổi bật nhất trong mã được tạo ra bởi mô hình. Chúng tôi cũng quan sát thấy nhiệt độ cao hơn luôn dẫn đến nhiều lỗi hơn. Việc mở rộng mô hình, mặc dù có thể giảm lỗi của nhiều loại, không cho thấy lợi ích rõ ràng trong việc ngăn chặn tên không xác định. Thông qua phân loại chi tiết hơn, chúng tôi thấy các mô hình lớn hơn tạo ra ít biến không xác định hơn nhưng nhiều phương thức không xác định hơn, điều này tạo ra kết quả hỗn hợp. Cuối cùng, chúng tôi chứng minh rằng các lỗi trong ngữ cảnh có thể dẫn đến các lỗi cùng loại trong việc tạo ra, có thể là hệ quả của khả năng học trong ngữ cảnh của các mô hình ngôn ngữ lớn.

Tóm lại, những đóng góp chính của chúng tôi bao gồm: (1) Chúng tôi đề xuất một khung đánh giá tĩnh cho việc hoàn thiện mã. (2) Đánh giá của chúng tôi trên các mô hình công khai tiết lộ các lỗi tĩnh phổ biến và cách chúng bị tác động bởi các yếu tố khác nhau như nhiệt độ, kích thước mô hình và ngữ cảnh.

## 2 Bối cảnh

**Tạo mã với Transformer** Trong những năm gần đây, việc huấn luyện các mô hình ngôn ngữ dựa trên Transformer trên mã nguồn đã trở nên ngày càng phổ biến (Feng et al., 2020; Ahmad et al., 2021; Wang et al., 2021; Lu et al., 2021; Guo et al., 2022) để hỗ trợ các nhiệm vụ kỹ thuật phần mềm (Iyer et al., 2018; Tufano et al., 2019). Đặc biệt, một số mô hình transformer chỉ giải mã đã được phát triển để hỗ trợ việc tạo mã, như Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2022), Incoder (Fried et al., 2022), và AlphaCode (Li et al., 2022). Những mô hình ngôn ngữ nhân quả được huấn luyện trước này có thể được sử dụng để dự đoán phần tiếp theo của mã đầu vào mà không cần bất kỳ việc tinh chỉnh nào.

**Cây Cú pháp Trừu tượng** Cây Cú pháp Trừu tượng (còn gọi là AST) được sử dụng để biểu diễn mã nguồn dưới dạng cây ngắn gọn. Bằng cách loại bỏ các chi tiết không cần thiết của mã cơ bản và cây phân tích tương ứng, AST chỉ trình bày nội dung cấu trúc chính của mã nguồn theo ngữ pháp ngôn ngữ (Aho et al., 2007).

**Phân tích Tĩnh** Phân tích tĩnh là một cách phổ biến để phát hiện lỗi phần mềm mà không cần thực thi chương trình (Ayewah et al., 2008; Chess and McGraw, 2004; Chess and West, 2007; Zheng et al., 2006). Các bộ phân tích tĩnh có xu hướng phát hiện lỗi bằng cách phân tích văn bản mã tĩnh, AST, tài liệu của nó, v.v. Người dùng thường cần chỉ định các mẫu lỗi và các bộ phân tích tĩnh sử dụng các phân tích AST, đồ thị và đường dẫn khác nhau để tìm những mẫu đó trong mã. Có rất nhiều công cụ phân tích tĩnh và chúng có thể phát hiện nhiều loại lỗi tùy thuộc vào các mẫu được chỉ định (Emanuelsson and Nilsson, 2008). Ví dụ, Linter là một công cụ phổ biến kiểm tra các lỗi phong cách lập trình và do đó, cố gắng thực thi một tiêu chuẩn lập trình (Van Oort et al., 2021).

## 3 Tập dữ liệu Hoàn thiện Hàm

Chúng tôi giới thiệu nhiệm vụ hoàn thiện hàm, đây là một trong những trường hợp sử dụng quan trọng nhất của các dịch vụ tự động hoàn thiện. Cho một đoạn mã đầu vào kết thúc bằng chữ ký hàm cộng với một docstring tùy chọn, mô hình được yêu cầu tạo ra thân hàm. Các nghiên cứu trước đây về hoàn thiện mã (Lu et al., 2021; Svyatkovskiy et al., 2019) chủ yếu tập trung vào việc hoàn thiện một dòng. Tuy nhiên, một dòng thường quá ngắn để tiết lộ khả năng của mô hình trong việc viết mã đúng cú pháp. Chúng tôi tin rằng hàm, với tư cách là khối xây dựng cơ bản trong hầu hết các ngôn ngữ lập trình, phục vụ mục đích này tốt hơn.

**Hình 2:** Pipeline đánh giá. Trái: Chúng tôi phân tích [ngữ cảnh] và [ngữ cảnh + tạo ra] thành AST. Nếu [ngữ cảnh] không thể phân tích được, chúng tôi dừng lại mà không báo cáo bất kỳ lỗi nào về việc tạo ra. Nếu [ngữ cảnh] có thể phân tích được, nhưng [ngữ cảnh + tạo ra] thì không, chúng tôi báo cáo lỗi AST trong việc tạo ra. Phải: Nếu cả hai đều có thể phân tích được, chúng tôi chạy Pyflakes trên các cây, báo cáo lỗi trong [ngữ cảnh] và lỗi trong [ngữ cảnh + tạo ra]. Lấy sự khác biệt cho chúng tôi các lỗi trong việc tạo ra.

Các nhà phát triển phần mềm sử dụng các mô hình tạo mã như các dịch vụ hộp đen trên một tập hợp đa dạng các dự án lập trình. Để mô phỏng tốt hơn kịch bản thực tế, chúng tôi xây dựng một tập đánh giá bằng cách lấy mẫu từ các kho lưu trữ Github công khai. Cụ thể, chúng tôi thu thập mã Python được cấp phép tự do trong các kho lưu trữ được tạo ra từ tháng 4 năm 2022 đến tháng 8 năm 2022. Tiêu chí lựa chọn loại trừ bất kỳ sự chồng chéo về thời gian nào giữa dữ liệu đánh giá của chúng tôi và dữ liệu huấn luyện của các mô hình được kiểm tra trong nghiên cứu này.

Các mã Python được thu thập được định dạng lại thành các bài toán hoàn thiện hàm. Đầu tiên chúng tôi sử dụng tree-sitter để phân tích toàn bộ tệp để xác định tất cả các hàm. Sau đó một hàm chứa docstring được chọn ngẫu nhiên. Mã từ đầu tệp cho đến cuối docstring được sử dụng làm ngữ cảnh, và thân hàm được coi là groundtruth. Phần còn lại của tệp bị loại bỏ. Tại thời điểm kiểm tra, chúng tôi nhắc mô hình với phần ngữ cảnh làm đầu vào, và để mô hình tạo ra thân hàm. Chúng tôi chỉ chọn các hàm có docstring để ngữ cảnh được xác định rõ và mô hình có thể tạo ra các hoàn thiện mã có ý nghĩa. Chúng tôi tiếp tục chọn các mẫu kiểm tra có độ dài ngữ cảnh từ 64 đến 768 token, và độ dài groundtruth ngắn hơn 256 token, để phù hợp với thiết lập tạo mô hình của chúng tôi. Tập đánh giá cuối cùng của chúng tôi bao gồm 100K bài toán hoàn thiện hàm.

## 4 Phân tích Lỗi Tĩnh

Chúng tôi đề xuất một pipeline đánh giá để phát hiện lỗi trong các hoàn thiện hàm được tạo ra bởi mô hình, được minh họa trong Hình 2. Giả sử mô hình tạo ra một hoàn thiện x cho ngữ cảnh đầu vào c. Chúng tôi không thể phân tích trực tiếp x vì đó là mã một phần không có ngữ cảnh. Trong khi đó, c cũng có thể chứa lỗi đặc biệt trong các trường hợp thực tế. Do đó, chúng tôi thực hiện phân tích của mình trong hai lượt. Đầu tiên chúng tôi kiểm tra c để tìm bất kỳ lỗi nào trong đầu vào cần được loại trừ, sau đó thực hiện một lượt khác trên mã đầy đủ (c, x), phép nối của ngữ cảnh và hoàn thiện mô hình. Bất kỳ lỗi nào được xác định trong (c, x) nhưng không có trong c phải phát sinh từ x, hay nói cách khác, được tạo ra bởi mô hình. Cụ thể hơn, chúng tôi tiến hành hai bước phân tích sau đây cho mã Python.

### 4.1 Phân tích AST

Trong bước đầu tiên, chúng tôi phân tích cả c và (c, x) thành cây cú pháp trừu tượng sử dụng mô-đun ast gốc của Python. Nếu mã có thể phân tích được, một AST sẽ được trả về. Ngược lại, một lỗi cú pháp được bắt. Dựa trên kết quả phân tích, chúng tôi thực hiện các hành động sau:

1. Nếu c không thể phân tích được, chúng tôi không thể kết luận bất kỳ lỗi nào trong việc tạo ra. Thực nghiệm cho thấy điều này hiếm khi xảy ra, như chúng tôi sẽ chỉ ra trong phần tiếp theo.

2. Nếu c có thể phân tích được nhưng (c, x) thì không, chúng tôi có thể xác nhận lỗi cú pháp được báo cáo là do việc tạo ra của mô hình gây ra. Tuy nhiên, lưu ý rằng chỉ một lỗi sẽ được trả về ngay cả khi có nhiều lỗi, do bản chất của việc phân tích AST.

3. Nếu cả c và (c, x) đều có thể phân tích được, không có lỗi AST nào trong việc tạo ra của mô hình. Các AST sẽ được sử dụng cho phân tích tĩnh trong bước tiếp theo.

### 4.2 Phân tích tĩnh với Pyflakes

Nếu cả c và (c, x) đều có thể được phân tích thành AST, chúng tôi thực hiện phân tích tĩnh sử dụng Pyflakes. Pyflakes là một công cụ phân tích tĩnh kiểm tra tệp nguồn Python để tìm lỗi bằng cách kiểm tra AST. Một lợi thế là việc phân tích không dựa vào các phụ thuộc của tệp nguồn, điều này quan trọng do sự đa dạng của các gói được sử dụng trong mã thực tế. Chúng tôi chạy Pyflakes trên c và (c, x) để xác định lỗi trong ngữ cảnh và trong mã đầy đủ. Các lỗi được phát hiện trong (c, x) nhưng không có trong c được coi là do việc hoàn thiện mô hình gây ra.

## 5 Thực nghiệm

Với pipeline được đề xuất, chúng tôi tiến hành phân tích lỗi cho các mô hình CodeGen (Nijkamp et al., 2022) trên tập kiểm tra được mô tả trong Phần 3, và trình bày kết quả phân tích.

### 5.1 Thiết lập Thực nghiệm

Chúng tôi đánh giá các mô hình CodeGen-mono ở tất cả các kích thước, từ 350M đến 16B. Chúng tôi tạo ra các hoàn thiện hàm sử dụng nucleus sampling với top-p 0.95. Nhiệt độ lấy mẫu được thay đổi từ 0.2 đến 0.8 cho mô hình 2B, và cố định ở 0.4 cho các mô hình còn lại. Chúng tôi lấy mẫu 10 lần tạo ra cho mỗi bài toán, dẫn đến một triệu hoàn thiện mã cho mỗi mô hình và nhiệt độ. Độ dài tạo ra tối đa là 256 token. Các hoàn thiện mã được tạo ra sau đó được chuyển qua pipeline đánh giá tĩnh của chúng tôi được xây dựng với Python 3.8 và Pyflakes 3.0.1. Việc đánh giá một triệu lần tạo ra chỉ mất vài giờ trên một luồng CPU duy nhất, và có thể được song song hóa hoàn toàn để tăng tốc.

### 5.2 Xác thực Đầu ra Mô hình

Mặc dù chúng tôi chủ yếu tập trung vào lỗi tĩnh trong nghiên cứu này, việc xác thực rằng các mô hình thực sự tạo ra mã có liên quan cũng quan trọng. Một phản ví dụ sẽ là tạo ra một dòng "return" duy nhất cho mọi chữ ký hàm, điều này đúng cú pháp nhưng hoàn toàn không có ý nghĩa. Để đạt được mục đích này, chúng tôi tính toán độ tương tự chỉnh sửa giữa việc tạo ra của mô hình và groundtruth, và so sánh với Pass@1 từ HumanEval (Chen et al., 2021) là một tiêu chuẩn dựa trên thực thi phổ biến để đánh giá các mô hình tạo mã. Cụ thể, cho cả hai tập dữ liệu, chúng tôi tạo ra 10 mẫu cho mỗi bài toán, và báo cáo độ tương tự chỉnh sửa trung bình hoặc tỷ lệ vượt qua trên tất cả các lần tạo ra. Như được hiển thị trong Bảng 1, các mô hình ở mọi kích thước và nhiệt độ đều có thể đạt được độ tương tự chỉnh sửa hợp lý trên tập dữ liệu hoàn thiện hàm, có nghĩa là các lần tạo ra có liên quan về mặt ngữ nghĩa. Hơn nữa, độ tương tự chỉnh sửa và HumanEval Pass@1 đều cải thiện khi mô hình mở rộng quy mô, nhấn mạnh rằng quy mô mô hình là rất quan trọng cho việc tạo mã chính xác.

**Bảng 1:** Độ tương tự chỉnh sửa trên tập dữ liệu hoàn thiện hàm và Pass@1 trên HumanEval, của các mô hình CodeGen qua các kích thước và nhiệt độ khác nhau. (1) Độ tương tự chỉnh sửa và HumanEval Pass@1 có tương quan dương qua các thiết lập khác nhau, điều này chứng minh độ tương tự chỉnh sửa có thể được sử dụng làm thước đo thay thế cho đánh giá mô hình. (2) Như mong đợi, các mô hình lớn hơn có độ tương tự chỉnh sửa tốt hơn (một chỉ số thay thế cho độ chính xác) trong nhiệm vụ hoàn thiện hàm.

| Mô hình | Nhiệt độ | Độ tương tự Chỉnh sửa | HumanEval Pass@1 |
|---------|----------|---------------------|------------------|
| CodeGen-16B | 0.4 | 72.07 | 31.83 |
| CodeGen-6B | | 68.76 | 26.46 |
| CodeGen-2B | | 64.83 | 23.72 |
| CodeGen-350M | | 56.47 | 12.62 |
| CodeGen-2B | 0.2 | 65.10 | 25.06 |
| | 0.4 | 64.83 | 23.72 |
| | 0.6 | 64.09 | 21.28 |
| | 0.8 | 62.62 | 17.56 |

Cuối cùng, mối tương quan dương mạnh giữa hai cột cuối cho thấy độ tương tự chỉnh sửa trên tập dữ liệu hoàn thiện hàm có thể được sử dụng làm thước đo thay thế để so sánh mô hình.

### 5.3 Kết quả AST

Chúng tôi chạy phân tích AST và thấy chỉ có 0.42% trường hợp với ngữ cảnh không thể phân tích được cần bị loại bỏ. Đối với phần còn lại, chúng tôi báo cáo tỷ lệ phần trăm các lần tạo ra có lỗi AST trong Bảng 2. Danh sách đầy đủ các loại lỗi được đưa vào Phụ lục A. Đối với mỗi loại, chúng tôi cũng hiển thị một ví dụ mã trong Phụ lục B.

Mặc dù có khoảng 7-8% các lần tạo ra không thể phân tích được, hầu hết các lỗi phân tích xảy ra ở cuối tệp (EOF), có nghĩa là mã được tạo ra không hoàn chỉnh do giới hạn 256 token tối đa. Việc mở rộng độ dài tạo ra có thể giúp giảm lỗi EOF, nhưng sẽ yêu cầu nhiều tính toán hơn và tăng độ trễ cảm nhận của dịch vụ tự động hoàn thiện. Mặt khác, các lỗi không phải EOF chỉ chiếm một phần rất nhỏ, thường khoảng 0.1-0.2%, cho thấy các mô hình CodeGen nói chung có thể tuân theo ngữ pháp cú pháp trừu tượng để tạo ra mã có thể phân tích được, bất kể kích thước mô hình và nhiệt độ.

**Phát hiện 1.** Mã được tạo ra bởi các mô hình, trừ khi không hoàn chỉnh, hầu hết đều có thể phân tích thành AST, bất kể kích thước mô hình hoặc nhiệt độ.

**Bảng 2:** Tỷ lệ phần trăm lỗi AST qua các kích thước mô hình và nhiệt độ khác nhau. Chúng tôi hiển thị (1) tổng lỗi AST; (2) lỗi ở cuối tệp (EOF); (3) lỗi không phải ở EOF; (4) top 3 lỗi không phải EOF. Các mô hình nói chung hoạt động tốt ở cấp độ AST ngoại trừ lỗi EOF gây ra bởi giới hạn độ dài tạo ra tối đa.

| Mô hình | Nhiệt độ | Tổng | EOF | Không EOF | Cú pháp Không hợp lệ | "print" Thiếu Dấu ngoặc | Đối số Từ khóa Lặp lại |
|---------|----------|------|-----|-----------|---------------------|------------------------|----------------------|
| CodeGen-16B | 0.4 | 7.330% | 7.236% | 0.094% | 0.042% | 0.041% | 0.004% |
| CodeGen-6B | | 7.446% | 7.253% | 0.193% | 0.081% | 0.094% | 0.006% |
| CodeGen-2B | | 7.272% | 7.177% | 0.095% | 0.052% | 0.018% | 0.008% |
| CodeGen-350M | | 8.703% | 8.593% | 0.110% | 0.041% | 0.016% | 0.028% |
| CodeGen-2B | 0.2 | 8.067% | 7.982% | 0.085% | 0.045% | 0.018% | 0.008% |
| | 0.4 | 7.272% | 7.177% | 0.095% | 0.052% | 0.018% | 0.008% |
| | 0.6 | 6.823% | 6.713% | 0.110% | 0.060% | 0.020% | 0.008% |
| | 0.8 | 7.496% | 7.337% | 0.159% | 0.085% | 0.029% | 0.014% |

Chúng tôi cũng hiển thị top-3 loại lỗi không phải EOF được xếp hạng theo tần suất, đó là Cú pháp Không hợp lệ, Thiếu Dấu ngoặc Print, và Đối số Từ khóa Lặp lại. Đáng chú ý, hai danh mục đầu thường liên quan đến phiên bản trình thông dịch Python. Để minh họa, print kiểu Python2 như print "abc" sẽ dẫn đến Thiếu Dấu ngoặc Print trong Python3. Một ví dụ khác là việc sử dụng async làm tên biến sẽ gây ra Cú pháp Không hợp lệ vì async đã trở thành từ khóa dành riêng kể từ Python3.7. Các mô hình học cách tạo ra những lỗi như vậy từ dữ liệu huấn luyện của chúng bao gồm mã được viết cho các phiên bản Python khác nhau. Trong nhiều trường hợp, khó để mô hình suy ra phiên bản trình thông dịch dự định trực tiếp từ ngữ cảnh hạn chế. Một hướng tương lai thú vị là hướng dẫn các mô hình tạo ra mã tương thích phiên bản cho môi trường đích.

**Phát hiện 2.** Không khớp phiên bản trình thông dịch là một trong những lý do chính cho các lỗi AST không phải EOF.

### 5.4 Kết quả Pyflakes

Chúng tôi trình bày tần suất của top 6 lỗi linter từ Pyflakes trong Bảng 3, với các ví dụ mã trong Phụ lục B. Mặc dù Pyflakes cũng tìm thấy các vấn đề khác trong mã, hầu hết chúng rất thưa thớt và do đó ít quan trọng hơn, chúng tôi để trong Phụ lục A. Lưu ý rằng một đoạn mã có thể chứa nhiều lỗi. Chúng tôi chỉ đếm mỗi loại một lần trong mỗi mẫu kiểm tra.

Trong số tất cả các lỗi, Tên Không xác định và Biến Không sử dụng là những lỗi phổ biến nhất, nơi mô hình hoặc gọi một biến không được định nghĩa, hoặc định nghĩa một biến nhưng không bao giờ sử dụng nó. Liên quan chặt chẽ là Import Không sử dụng, Định nghĩa lại Khi Không sử dụng và Cục bộ Không xác định, có thể được coi là các trường hợp đặc biệt của hai loại đầu tiên. Các mô hình đôi khi cũng sử dụng f-string một cách không cần thiết bằng cách không đưa ra bất kỳ placeholder nào. Điều đáng chỉ ra là không phải tất cả lỗi Pyflakes đều sẽ ảnh hưởng đến việc thực thi. Thực tế trong số sáu loại, chỉ có Tên Không xác định và Cục bộ Không xác định có thể gây ra vấn đề runtime. Tuy nhiên, tất cả những lỗi này có thể gây hại cho khả năng đọc và bảo trì, điều này rất quan trọng cho phát triển phần mềm. Do đó, việc giải quyết chúng để cải thiện chất lượng tự động hoàn thiện mã là quan trọng.

Qua các nhiệt độ lấy mẫu, chúng tôi quan sát trong mỗi cột rằng nhiều lỗi hơn được tạo ra dưới nhiệt độ cao hơn, điều này được mong đợi vì các lần tạo ra trong những trường hợp như vậy ít tự tin hơn.

**Phát hiện 3.** Nhiệt độ cao hơn luôn dẫn đến nhiều lỗi hơn của mọi loại.

**Bảng 3:** Tỷ lệ phần trăm lỗi Pyflakes qua các kích thước mô hình và nhiệt độ khác nhau. Nhiệt độ cao hơn luôn dẫn đến nhiều lỗi hơn trong mọi danh mục. Mặt khác, các mô hình lớn hơn không nhất thiết tạo ra ít lỗi hơn.

| Mô hình | Nhiệt độ | Tên Không xác định | Biến Không sử dụng | FString Thiếu Placeholder | Import Không sử dụng | Định nghĩa lại Khi Không sử dụng | Cục bộ Không xác định |
|---------|----------|-------------------|-------------------|---------------------------|---------------------|----------------------------------|---------------------|
| CodeGen-16B | 0.4 | 4.323% | 1.729% | 0.135% | 0.107% | 0.131% | 0.047% |
| CodeGen-6B | | 4.374% | 1.775% | 0.089% | 0.149% | 0.126% | 0.055% |
| CodeGen-2B | | 4.364% | 1.810% | 0.147% | 0.150% | 0.146% | 0.065% |
| CodeGen-350M | | 4.472% | 2.032% | 0.151% | 0.173% | 0.155% | 0.095% |
| CodeGen-2B | 0.2 | 4.206% | 1.751% | 0.125% | 0.139% | 0.139% | 0.067% |
| | 0.4 | 4.364% | 1.810% | 0.147% | 0.150% | 0.146% | 0.065% |
| | 0.6 | 4.711% | 2.000% | 0.188% | 0.170% | 0.159% | 0.076% |
| | 0.8 | 5.377% | 2.490% | 0.240% | 0.247% | 0.184% | 0.086% |

Tác động của kích thước mô hình đối với tỷ lệ lỗi ít nhất quán hơn. Đối với Biến Không sử dụng, Import Không sử dụng, và Cục bộ Không xác định, tỷ lệ lỗi thực sự giảm khi mô hình mở rộng quy mô. Tuy nhiên, ba danh mục còn lại không thể hiện mối tương quan như vậy. Chúng tôi điều tra lý do cơ bản cho kết quả hỗn hợp này đặc biệt trong trường hợp Tên Không xác định. Lưu ý rằng nếu một tên không xác định là một lời gọi hàm, nó có thể được định nghĩa sau đó bên ngoài phạm vi hoàn thiện hàm hiện tại. Mặc dù không được đảm bảo, mô hình có thể sửa lỗi này bằng cách cho phép tạo ra mã dài hơn thay vì chỉ một hàm. Ngược lại, việc sử dụng một biến mà không định nghĩa nó trước thường là một lỗi. Ngay cả trong một số trường hợp hiếm hoi khi định nghĩa biến được tạo ra chính xác sau khi sử dụng, thứ tự như vậy thường ít được ưa thích hơn về mặt phong cách lập trình.

**Hình 3:** Số lượng biến không xác định so với hàm không xác định. Các mô hình lớn hơn tạo ra nhiều hàm không xác định hơn nhưng ít biến không xác định hơn.

Trong Hình 3, chúng tôi chia nhỏ các tên không xác định thành biến và hàm. Chúng tôi thấy rằng các mô hình lớn hơn tạo ra ít biến không xác định hơn, nhưng nhiều hàm không xác định hơn, điều này chứng minh rằng mối tương quan giữa số lượng lỗi và kích thước mô hình thay đổi đối với các loại lỗi khác nhau.

**Phát hiện 4.** Mặc dù các mô hình lớn hơn là những trình tạo mã chính xác hơn (Nijkamp et al., 2022), việc mở rộng quy mô kích thước mô hình không dẫn đến giảm số lượng lỗi cho tất cả các danh mục lỗi.

### 5.5 Tương quan với Lỗi trong Ngữ cảnh

Chúng tôi tiếp tục nghiên cứu mối tương quan giữa lỗi trong ngữ cảnh và trong việc tạo ra. Ký hiệu c là ngữ cảnh đầu vào, x là việc tạo ra của mô hình, e là loại lỗi. Chúng tôi viết e∈c có nghĩa là c chứa một lỗi thuộc loại e. Đối với mọi e, chúng tôi tính toán P(e∈x|e∈c), tỷ lệ lỗi tạo ra khi ngữ cảnh chứa cùng loại lỗi. Chúng tôi cũng báo cáo tỷ lệ tương đối P(e∈x|e∈c)/P(e∈x|e/∈c) để đo lường tác động của ngữ cảnh. Từ Bảng 4, nếu mô hình quan sát lỗi trong ngữ cảnh, nó có nhiều khả năng tạo ra cùng loại lỗi trong việc tạo ra, và tỷ lệ lỗi có thể được khuếch đại từ 7∼200 lần tùy thuộc vào loại. Điều này có thể là một hệ quả không mong muốn của khả năng học trong ngữ cảnh của các mô hình ngôn ngữ lớn.

**Bảng 4:** Tương quan giữa lỗi trong ngữ cảnh và trong việc tạo ra cho mô hình 2B. Hai cột đầu tiên cho thấy lỗi trong ngữ cảnh có thể khuếch đại lỗi trong việc tạo ra; cột cuối cho thấy không phải tất cả lỗi tạo ra đều có thể quy cho ngữ cảnh. Các mô hình khác có kết quả tương tự.

| Loại lỗi | P(e∈x\|e∈c) | P(e∈x\|e∈c)/P(e∈x\|e/∈c) | P(e∈c\|e∈x) |
|-----------|-------------|---------------------------|-------------|
| Tên Không xác định | 26.33% | 7.80 | 25.99% |
| Biến Không sử dụng | 14.13% | 8.45 | 8.56% |
| FString Thiếu Placeholders | 20.63% | 215.50 | 35.08% |
| Định nghĩa lại Khi Không sử dụng | 2.44% | 21.16 | 22.30% |
| Cục bộ Không xác định | 7.00% | 108.68 | 1.08% |

Chúng tôi cũng tính toán P(e∈c|e∈x) để hiển thị có bao nhiều lỗi tạo ra xảy ra đồng thời với lỗi ngữ cảnh. Như được chỉ ra bởi cột cuối của Bảng 4, mặc dù lỗi ngữ cảnh có thể khuếch đại đáng kể lỗi tạo ra, sự đồng xuất hiện của hai điều này không chiếm một phần lớn. Điều này ngụ ý ngữ cảnh có vấn đề không phải là yếu tố duy nhất cho việc tạo ra có vấn đề, và thường là trường hợp các mô hình tạo ra lỗi ngay cả với ngữ cảnh đúng.

**Phát hiện 5.** Lỗi trong ngữ cảnh nói chung dẫn đến nhiều lỗi hơn trong việc tạo ra.

## 6 Thảo luận

Chúng tôi trình bày một khung đánh giá tĩnh cho các hoàn thiện mã được tạo ra bởi các mô hình ngôn ngữ lớn. Bằng cách sử dụng khung được đề xuất, chúng tôi tiến hành phân tích lỗi của các mô hình CodeGen trên một tập đánh giá Python thực tế quy mô lớn. Thực nghiệm của chúng tôi tiết lộ các lỗi tĩnh phổ biến được tạo ra bởi các mô hình được huấn luyện trước, cũng như xu hướng tần suất của chúng qua các kích thước mô hình và nhiệt độ lấy mẫu. Bằng cách chỉ ra điểm yếu của các mô hình hiện tại, chúng tôi hy vọng nghiên cứu của mình cũng làm sáng tỏ các hướng tương lai hướng tới việc tạo mã chính xác hơn.

Có một vài hạn chế của nghiên cứu này. Thứ nhất, chúng tôi tập trung vào việc tạo mã từ trái sang phải mà không xem xét ngữ cảnh phía bên phải và xuyên tệp, có thể được sử dụng để xác định các danh mục lỗi rộng hơn với độ chính xác được cải thiện. Thứ hai, mỗi công cụ phân tích tĩnh có những hạn chế riêng. Do đó, phân tích được trình bày bị giới hạn bởi độ chính xác và phạm vi bao phủ của Pyflakes để phát hiện các vấn đề mã nhất định.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên theo định dạng gốc]

## Phụ lục A Các Danh mục Lỗi Đầy đủ

Ngoài những lỗi được thảo luận trong Phần 5, chúng tôi liệt kê tất cả các danh mục lỗi có thể được phát hiện trong mã được tạo ra bởi mô hình trong các thực nghiệm của chúng tôi, với tần suất tối thiểu 0.001% bởi bất kỳ mô hình nào (tức là 10 quan sát trong tổng số 1 triệu lần tạo ra).

**Lỗi AST (lỗi EOF được chỉ ra bằng dấu sao):**
1. *unexpected EOF while parsing
2. *EOL while scanning string literal
3. *invalid syntax at EOF
4. *EOF while scanning triple-quoted string literal
5. invalid syntax not at EOF
6. missing parentheses in call to "print"
7. keyword argument repeated
8. leading zeros in decimal integer literals are not permitted; use an o prefix for octal integers
9. unmatched ")"
10. cannot assign to function call
11. positional argument follows keyword argument
12. expression cannot contain assignment

**Vấn đề Pyflakes:**
1. undefined name
2. unused variable
3. f-string missing placeholder
4. unused import
5. redefined while unused
6. indentation error
7. import shadowed by loop var
8. raise not implemented
9. invalid print syntax
10. is literal
11. string dot format extra positional argument
12. multi value repeated key literal
13. percent format positional count mismatch
14. tab error
15. string dot format extra named arguments
16. import star not permitted
17. percent format unsupported format character
18. assert tuple
19. percent format extra named arguments

## Phụ lục B Ví dụ cho Các Loại Lỗi Hàng đầu

Dưới đây chúng tôi liệt kê một ví dụ mã cho mỗi danh mục lỗi được hiển thị trong Bảng 2 và 3. Theo định nghĩa của nhiệm vụ hoàn thiện hàm, trong mỗi ví dụ, ngữ cảnh là từ đầu cho đến cuối docstring của hàm cuối cùng, và hoàn thiện mô hình là thân của hàm cuối cùng.

[Các ví dụ mã được giữ nguyên như trong bản gốc]
