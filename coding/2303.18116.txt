# 2303.18116.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2303.18116.pdf
# File size: 715868 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2303.18116v1  [cs.CL]  31 Mar 2023Pair Programming with Large Language Models
for Sampling and Estimation of Copulas
Jan G´ orecki
Department of Informatics and Mathematics, Silesian Unive rsity in Opava,
Univerzitn´ ı n´ amˇ est´ ı 1934/3, 733 40 Karvin´ a, Czech Rep ublic
gorecki@opf.slu.cz
Abstract
Without writing a single line of code by a human, an example Mo nte
Carlo simulation based application for stochastic depende nce modeling
with copulas is developed using a state-of-the-art large la nguage model
(LLM) ﬁne-tuned for conversations. This includes interact ion with Chat-
GPTinnaturallanguage andusingmathematical formalism, w hich, under
careful supervision by a human-expert, led to producing a wo rking code in
MATLAB, Python and Rfor sampling from a given copula model, evalu-
ation of the model’s density, performing maximum likelihoo d estimation,
optimizing the code for parallel computing for CPUs as well a s for GPUs,
and visualization of the computed results. In contrast to ot her emerging
studies thatassess theaccuracy ofLLMs likeChatGPT on task s from ase-
lected area, this work rather investigates ways how to achie ve a successful
solution of a standard statistical task in a collaboration o f a human-expert
and artiﬁcial intelligence (AI). Particularly, through ca reful prompt en-
gineering, we separate successful solutions generated by C hatGPT from
unsuccessful ones, resulting in a comprehensive list of rel ated pros and
cons. It is demonstrated that if the typical pitfalls are avo ided, we can
substantially beneﬁt from collaborating with an AI partner . For example,
we show that if ChatGPT is not able to provide a correct soluti on due to a
lack of or incorrect knowledge, the human-expert can feed it with the cor-
rect knowledge, e.g., in the form of mathematical theorems a nd formulas,
and make it to apply the gained knowledge in order to provide a solution
that is correct. Such ability presents an attractive opport unity to achieve
a programmed solution even for users with rather limited kno wledge of
programming techniques.
Keywords: human-AI collaboration, analytically intractable problems,
prompt engineering, natural language
MSC classiﬁcation: 65C60, 68N19, 68T50
1 Introduction
The recent progress in solving natural language processing (NLP) tasks using
largelanguagemodels(LLMs) resultedin models with previouslyunsee n quality
of text generation and contextual understanding. These models , such as BERT
(Devlin et al., 2018), RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al.,
1

--- PAGE 2 ---
2020), are capable of performing a wide range of NLP tasks, includin g text
classiﬁcation, question-answering, text summarization, and more . With more
than 100 million users registered in two months after release for pub lic test-
ing through a web portal1, ChatGPT2is the LLM that currently most res-
onates in the artiﬁcial intelligent (AI) community. This conversation al AI is
ﬁne-tuned from the GPT-3.5 series with reinforcement learning fro m human
feedback (Christiano et al., 2017; Stiennon et al., 2020), using ne arly the same
methods as InstructGPT (Ouyang et al., 2022), but with slight diﬀer ences in
the data collection setup. In March 2023, the ChatGPT’s developer released
a new version of GPT-3.5, GPT-4 (OpenAI, 2023). In the time of the writing
of this paper, GPT-4 was not freely available, so our results do not in clude its
outputs. However, as a technical report for some of the model’s p roperties is
available, we add the relevant information where appropriate.
A particular result of the ChatGPT’s ﬁne-tuning is that it can genera te
corresponding code in many programming languages given a task des cription in
natural language. This can be exploited in pair programming (Williams, 2001)
with ChatGPT, which then oﬀers several beneﬁts, including:
•Enhanced productivity: ChatGPT can help automate repetitive and time-
consuming programming tasks, freeing up time for developers to fo cus on
higher-level problem-solving and creative work. On average a time s aving
of 55% was reported for the task of writing an HTTP server in JavaS cript
in the study conducted by the GitHub Next team3for GitHub Copilot4.
The latter is another code suggestion tool that generates code s nippets
based on natural language descriptions, powered by an LLM similar t o
ChatGPT, Codex (Chen et al., 2021).
•Improved code quality: Pair programming with ChatGPT can help iden-
tify errors and bugs in the code before they become bigger problem s.
ChatGPT can also suggest improvements to code architecture and design.
•Knowledge sharing: ChatGPT can help less experienced developers le arn
from more experienced team members by providing suggestions and guid-
ance.
•Better code documentation: ChatGPT can help create more detaile d and
accurate code documentation by generating comments and annot ations
based on the code.
•Accessibility: ChatGPT can make programmingmore accessible to peo ple
who may not have a programming background, allowing them to collab-
orate with developers and contribute to projects in a meaningful w ay.
For example, having developed a new theory that requires computa tions,
it might be appealing and time-eﬀective for researchers to use tools like
1https://www.demandsage.com/chatgpt-statistics/
2https://openai.com/blog/chatgpt/
3https://github.blog/2022-09-07-...
4https://github.com/features/copilot
2

--- PAGE 3 ---
ChatGPT to implement the solution without the need to involve typically
expensive manpower in software engineering.
Currently, there appear severalstudies that assessthe accur acy of LLMs like
ChatGPT based on a set of tasks from a particular area. For examp le, multi-
ple aspects of mathematical skills of ChatGPT are evaluated in Friede r et al.
(2023), with the main observation that it is not yet ready to deliver h igh-quality
proofs or calculations consistently. In Katz et al. (2023), a prelimin ary version
ofGPT-4 wasexperimentally evaluated againstprior generationsof GPT on the
entire Uniform Bar Examination (UBE)5, and it is reported that GPT-4 signif-
icantly outperforms both human test-takers and prior models, de monstrating
a 26% increase over the GPT-3.5-based model and beating humans in ﬁve of
seven subject areas. In Bang et al. (2023), an extensive evaluat ion of ChatGPT
using 21 data sets covering 8 diﬀerent NLP tasks such as summariza tion, senti-
ment analysis and question answering is presented. The authors fo und that, on
the one hand, ChatGPT outperforms LLMs with so-called zero-sho t learning
(Brown et al., 2020) on most tasks and even out-performs ﬁne-t uned models
on some tasks. On the other hand, they conclude that ChatGPT su ﬀers from
hallucination problems like other LLMs and it generates more extrinsic hallu-
cinations from its parametric memory as it does not have access to a n external
knowledge base. Interestingly, the authors observed in several tasks that the
possibility of interaction with ChatGPT enables human collaboration wit h the
underlying LLM to improve its performance.
The latterobservationis themain focus ofthis work. Ratherthan e valuating
the accuracy of LLMs, we investigate ways to beneﬁt from pair pro gramming
with an AI partner in order to achieve a successful solution of a tas k requiring
intensive computations. Despite many impressive recent achieveme nts of state-
of-the-art LLMs, achieving a functional code is far from being str aightforward;
one of many unsuccessful attempts is reported at freeCodeCam p6. Importantly,
successful attempts are also emerging. In Maddigan and Susnjak (2023), the
authors report that LLMs together with the proposed prompts c an oﬀer a re-
liable approach to rendering visualisations from natural language qu eries, even
when queries are highly misspeciﬁed and underspeciﬁed. However, in many
areas, including computationally intensive solutions of analytically intr actable
statistical problems, a study that demonstrates beneﬁts from p air programming
with an AI partner is missing.
This work ﬁlls this gap and considers applications involving copulas(Nelsen,
2006; Joe, 2014) as models for stochastic dependence between r andom vari-
ables. These applications are known for their analytical intractabilit y, hence,
the Monte Carlo (MC) approach is most widely used to compute the inv olved
quantities of interest. As the MC approach often involves large com putation ef-
forts, conductinga MC study requiresoneto implement all underlyin gconcepts.
We demonstrate how to make ChatGPT produce a working implementa tion for
such an application by interacting with it in a natural languageand usin g math-
5https://www.ncbex.org/exams/ube/
6https://www.freecodecamp.org/news/pair-programming. ..
3

--- PAGE 4 ---
ematical formalism. To fully illustrate the coding abilities of ChatGPT, t he hu-
man role is pushed to an extreme, and all the mentioned tasks are imp lemented
without a single line of code written by the human or tweaking the gene rated
code in any way. It is important to emphasize that even if the applicat ion under
consideration relates to a speciﬁc area of probability and statistics , our obser-
vations apply in a wider scope as the tasks we consider (sampling from a given
(copula) model, evaluation of the model’s density, performing maximu m likeli-
hood estimation, optimizing the code forparallel computing and visua lizationof
the computed results) commonly appear in many statistical applicat ions. Also,
we do not present just one way to achieve a successful solution fo r a given task.
Mostofthe successfulsolutionsarecomplemented with examplesd emonstrating
which adjustments of our prompts for ChatGPT turn unsuccessf ul solutions to
successful ones. This results in a comprehensive list of related pro s and cons,
suggesting that if the typical pitfalls are avoided, we can substant ially beneﬁt
from a collaboration with LLMs like ChatGPT. Particularly, we demonst rate
that if ChatGPT is not able to provide a correct solution due to limitatio ns
in its knowledge, it is possible to feed it with the necessary knowledge, and
make ChatGPT apply this knowledge to provide a correct solution. Ha ving all
the sub-tasks of the main task successfully coded in a particular pr ogramming
language, we also demonstrate how to fully exploit several impressiv e abilities
of ChatGPT. For example, by a simple high-level prompt like “Now code it
in Python.”, ChatGPT correctly transpiles the code from one to ano ther pro-
gramming language in a few seconds. Also, if an error in the code prod uced by
ChatGPT is encountered during execution, it is demonstrated that ChatGPT is
not only able to identify the error, but even immediately produces a c orrected
version after the error message is copy-pasted to ChatGPT’s web interface.
The paper is organized as follows. Section 2 presents the tasks we c onsider
and sets up the way we interact with ChatGPT. Section 3 presents t he develop-
ment of the task via pair programming with ChatGPT. Section 4 summa rizes
the pros and cons observed during the task development, including a discussion
on how to mitigate the latter, and Section 5 concludes.
2 Methodology
2.1 The task
Let (xij)∈Rn×dbe a sample of size nfrom a random vector ( X1,...,X d)∼F,
whereFis a joint distribution function with the continuous univariate mar-
ginsF1,...,F dandcopula(Sklar, 1959) Cimplicitly given by F(x1,...,x d) =
C(F1(x1),...,F d(xd)) forx1,...,x d∈R. An explicit formulafor CisC(u1,...,
ud) =F(F−1
1(u1),...,F−1
d(ud)), u1,...,u d∈[0,1]. A typical application in-
volving the MC approach and copulas assumes that Cis unknown but belongs
to a parametric family of copula models {Cθ:θ∈Θ}, where Θ is an open
subset of Rpfor some integer p≥1. The following steps are then considered:
1. Estimate the true but unknown parameter θ0∈Θ ofCθ0=C, e.g., using
4

--- PAGE 5 ---
the pseudo maximum likelihood (ML) estimator
ˆθ= argmax
θ∈Θn/summationdisplay
i=1logcθ(ˆui1,...,ˆuid), (1)
wherecθis the density of Cθ, θ∈Θ, ˆuij=ˆFj(xij) andˆFjis an estimate
ofFjfori∈ {1,...,n}, j∈ {1,....d}. For some copula families, e.g.,
for Archimedean ones, evaluation of cθfor large dis already a challenge;
see Hofert et al. (2013). For pair-copula constructions, the main challenge
lies in computing (1), see Hobæk Haﬀ (2013) or Schellhase and Spanhe l
(2018), typically done using numerical methods like gradient descen t.
2. Generate a sample ( vij)∈[0,1]N×dfromCˆθ, typically with N≫n.
For several popular copula families, this task is also challenging, and in -
volvesdiﬀerenttechniquesforeﬃcientlysamplingfrom Cˆθ; see,e.g.,Hofert
(2010);Hofert et al.(2018)forsamplingtechniquesrelatedtoAr chimedean
and Archimax copulas and their hierarchical extensions.
3. Compute a sample from an analytically intractable distribution, e.g., from
the distribution of ¯X=1
d/summationtextd
j=1Xj. Comparedto the previoustwo points,
this is a trivial task, we just need to evaluate ¯ xi=1
d/summationtextd
i=1ˆF−1
j(vij), i∈
{1,...,N}.
4. Computethedesiredquantitybasedon ¯ x1,...,¯xN. Forexample,if X1,...,
Xdrepresent risk factor changes, a quantile of the distribution func -
tion of¯Xrepresents the Value-at-Risk VaRα, commonly used in quan-
titative risk management; see McNeil et al. (2015). Approximating V aR
(X1+···+Xd) is also trivial as it just involves computing the order
statistics ¯ x(1)≤ ··· ≤¯x(N)and then picking out ¯ x(⌈αN⌉), whereα∈[0,1]
is a desired conﬁdence level. In the same realm, the quantity known a s
expected shortfall involves computing the average of the values ¯ xithat are
larger than VaR α, so again a computationally trivial task.
In order to clearly see that the code generated by ChatGPT indeed works as
expected without the need of an experienced programmer, we dev iate a bit
from the above outline, while keeping the non-trivial tasks, i.e., the s ampling
and estimation. We thus prompt ChatGPT to generate code that do es the
following:
1. Generate a sample from Cθ0, whereθ0∈R.
2. Based on this sample, compute the ML estimator ˆθof the true parameter
θ0using (1).
Then, we repeat these two steps for several values of θ0, e.g., linearly spaced on
some convenient interval of R. If the plot of the pairs of ( θ0,ˆθ) is close to the
identity( θ−θ)plot, thenonehasstrongevidenceofacorrectstatisticalsamp ling
and estimation procedure. Finally, to allow for scaling, we ask ChatGP T to
optimizethegeneratedcodeforparallelcomputingonCPUsaswella sonGPUs.
5

--- PAGE 6 ---
2.2 The communication protocol
When interacting with ChatGPT, we use the web portal provided by it s devel-
opment team7. Also, we set up and follow this communication protocol :
1. We prompt ChatGPT to generate code for solving a selected task in nat-
ural language and using mathematical formalism, that is, we specify the
task in plain text and do not use any speciﬁc formal language. For fo rmu-
las, we use plain text like psi(t) = (1 + t)ˆ(-1/theta) .
2. If the solution generated by ChatGPT is wrong, that is, does not solve
the given task, we communicate the problem to ChatGPT, and ask it t o
provide us with a corrected solution.
3. If this corrected solution is still wrong, we feed ChatGPT with the knowl-
edge necessary to complete the task successfully, e.g., we provide it with
theorems andformulasin plain text. Foran example, see the third pr ompt
in Section 3.4.
In this way, we simulate an interaction between two humans, e.g., a clie nt sends
by email a task to a software engineer, and we play the role of the clie nt and
ChatGPT the role of the software engineer. As it is typical that the client is
not aware of all details required to solve the task at the beginning of the inter-
action, such a communication protocol may be frequently observe d in practice.
The client starts by providing the (subjectively) most important fe atures of the
problemin orderto minimize her/hisinitial eﬀort, and then, if necessa ry, she/he
adds more details to get a more precise solution. Importantly, this c ommunica-
tion protocol led to a successful completion of the aforementione d tasks, which
is reported in Section 3.
With regards to passing ChatGPT the required knowledge, it is import ant
to realize that ChatGPT does nothave any memory to remember the previ-
ous conversation with a user. Instead, the trick for ChatGPT to a ppear to
remember previous conversations is to feed it the entire conversa tion history as
a single prompt. This means that when a user sends a message, the p revious
conversationhistory is appended to the prompt and then fed to Ch atGPT. This
prompt engineering technique is widely used in conversational AI sys tems to
improve the model’s ability to generate coherent and contextually ap propriate
responses. However, it is just a trick used to create the illusionof memory in
ChatGPT.
If the previous conversation is too long (larger than 4096 tokens8, where a
token is roughly 3/4 of an English word9), it may not ﬁt entirely within the
context window that ChatGPT uses to generate responses. In su ch cases, the
model may only have access to a partial view of the conversation his tory, which
7chat.openai.com
8https://platform.openai.com/docs/models/gpt-3-5
9The total count of tokens in a piece of text can be precisely me asured by
https://platform.openai.com/tokenizer
6

--- PAGE 7 ---
can result in the model seeming like it has forgotten some parts of th e conver-
sation. To mitigate this issue, conversational AI designers often u se techniques
like truncating or summarizing the conversation history to ensure t hat it ﬁts
within the context window. The way we solve this problem in our example
task is re-introducing the parts that we referred to to ChatGPT. For example,
when transpiling the code from Python (Appendix A) to R(Appendix B), we
ﬁrst copy-paste the Python code to ChatGPT’s web interface and then ask it to
transpile it to R. Without having this technical limitation in mind, it is unlikely
to get a correct answer/solution if we refer to the conversion par t that does
not ﬁt within the context window. Finally note that according to its te chnical
report, GPT-4 uses the context window that is 8x larger than of Ch atGPT, so
it can contain roughly 25,000 words. This suggests that the limitation imposed
by the context window length will become less and less of a concern.
2.3 The copula family
In order to make the example task speciﬁed in Section 2.1 precise, we choose
the parametric family {Cθ:θ∈Θ}to be the popular family of Clayton copulas
(Clayton, 1978), given by
Cθ(u1,u2) = (max( u−θ
1+u−θ
2−1,0))−1
θ, (2)
whereu1,u2∈[0,1] and−1≤θ <∞, θ/ne}ationslash= 0. This family of copulas is used in a
wide variety of applications. To mention several recent ones, e.g., H uang et al.
(2022) use it to analyse the correlation between the residual serie s of a long
short-term memory neural network and a wind-speed series. Par ticularly, the
maximum likelihood estimation of the copula parameter is utilized, i.e., the pro-
cedure that ChatGPT implements here in Section 3.3. In the simulation study
in Michimae and Emura (2022), where copula-based competing risks m odels for
latent failure times are proposed, the authors utilize sampling from C layton
copulas, i.e., the procedure that ChatGPT implements here in Section 3.4.
For simplicity, as well as the fact that the models with θ <0, that is, those
with negative dependence, are rarely used in practice, we restrict toθ∈(0,∞),
which allows one to rewrite (2) to
Cθ(u1,u2) = (u−θ
1+u−θ
2−1)−1
θ. (3)
The technical reason for choosing this family is its simple analytical fo rm, which
makes easier for the reader to track all the formulas we ask for an d get from
ChatGPT,e.g., the probabilitydensityfunction (PDF). Another rea sonisChat-
GPT’s relatively limited knowledge of this family. By contrast, e.g., for t he
most popular family of Gaussian copulas, ChatGPT was not able to gen erate a
sampling algorithm without being fed with some necessary theory. Th e latter
simulates a realistic situation when ChatGPT is facing a new theory/co ncept,
e.g., one recently developed by the user. However, we would like to en courage
the reader to experiment with any family of interest or even with a ta sk that
diﬀers from our example.
7

--- PAGE 8 ---
3 Pair programming with ChatGPT
Pair programming (Williams, 2001) is a software development techniqu e in
which two programmers work together at one workstation. One, t hedriver,
writes code while the other, the navigator reviews each line of code as it is
typed in and considers the “strategic” direction of the work. In wh at follows,
we, the human-part of the pair, put ourselvesin the role of the nav igator, giving
speciﬁc tasks to the driver, ChatGPT, representing the AI-part of the pair.
We begin prompting ChatGPT with tasks that require rather less lines of
code in order to not overwhelm the reader with the amount of result s, while still
demonstrating as many abilities of ChatGPT as possible. Then, we gra dually
move to larger tasks until we get all of them implemented. Our promp ts are
insans serif . Responses of ChatGPT are either in verbatim , if the response is
code, or in italic, otherwise. In its ﬁrst occurrence, we indicate this explicitly.
Note that as ChatGPT is quite a loquacious LLM, we mostly limit the lengt h
of its responses, otherwise we would be overwhelmed with answers o f unneces-
sary detail. For the same reason, when asking ChatGPT for code, w e mostly
omit its comments on the produced code. Also note that when comme nting
on ChatGPT’s responses, we speak of it as if it were a human, e.g., it “u nder-
stands”, “knows” or “is aware of” something, which should be inter preted by
the reader in the sense that ChatGPT produced a response that ( typically very
well)mimicsa corresponding human reaction. As ChatGPT’s responses are by
default non-deterministic, i.e., giving it the same prompt again, the re sponse
might slightly diﬀer. To take this feature into account, we re-gener ate the re-
sponse for each of our prompts three times, and if these respons es are factually
diﬀerent from each other, we indicate it accordingly. Finally note tha t the whole
interaction is conducted in one chat window. Once we observe that C hatGPT
starts to forget the previous context due to the reasons descr ibed in Section 2.2,
we re-introduce it as we describe in the same section.
In the rest of this section, we ﬁrst investigate the knowledge of Ch atGPT on
the topic under consideration, and then we prompt it to generate c ode for eval-
uation of the density of the Clayton copula, for ML estimation of its p arameter,
for sampling from the copula, for creating a visualization of the exam ple Monte
Carlo approach, and for optimizing the code for parallel computatio ns.
3.1 Warm up
(Prompt:) What is the Clayton bivariate copula, why is it inte resting, which is
itsanalyticalformandisitanArchimedeancopula? Respond tothesequestions
in at most 100 words.
8

--- PAGE 9 ---
(Response:) The Clayton copula is a bivariate copula func-
tion that is widely used in statistics and ﬁnance to model de-
pendence between variables. It is interesting because it ha s a sim-
ple form and can capture positive dependence between variab les while al-
lowing for asymmetric tails. The analytical form of the Clay ton cop-
ula is C(u, v; theta) = (max(uˆ(-theta) + vˆ(-theta) - 1, 0))ˆ (-
1/theta), where theta is the Clayton copula parameter. The C layton cop-
ula is an Archimedean copula, meaning that it can be expresse d as the gen-
erator function of a particular class of symmetric distribu tions.
We see that ChatGPT can save our time by quickly and concisely summa rizing
basic facts about the topic of our interest. We can also limit the size o f the
answer, which is satisﬁed in this 91 words long answer. The informatio n about
the positive dependence probably follows from what we have already stated
before: the negatively dependent models are rarely used in practic e, which is
probably reﬂected in ChatGPT’s training data. However, several d etails of
the answer can be discussed. In lines 3 and 4, “random variables” ins tead of
just “variables” would be more precise. From the last sentence, it f ollows that
an Archimedean copula can be expressed as the generator functio n for some
symmetric distributions. This is at least confusing as Archimedean co pulas are
rather a particular class of copulas admitting a certain functional f ormbased
on so-called generator functions. Finally, symmetric distributions have their
precise meaning: such a distribution is unchanged when, in the contin uous case,
its probability density function is reﬂected around a vertical line at s ome value
of the random variable represented by the distribution. Whereas A rchimedean
copulas posses a kind of symmetry following from their exchangeabilit y, they do
not belong to symmetric distributions.
To investigate the limits of ChatGPT’s knowledge, let us prompt it with t wo
further questions. According to the previous response, we can s peculate that it
has limited knowledge on the Clayton models with negative dependence .
If the Clayton copula parameter is negative, e.g., theta = -1/2, is the corre-
sponding Clayton copula a model for positive dependence bet ween variables?
Respond with one word.
No.
Correction: Yes.
9

--- PAGE 10 ---
Yes.
We prompted ChatGPT to answer the same question three times and got con-
tradicting answers. The ﬁrst answer is correct, however, after asking again,
ChatGPT changed its mind. Before commenting on that, let us try on ce again,
with a bit more complex concept.
If the Clayton copula parameter is positive, e.g., theta = 1, is t he survival
copula of the corresponding Clayton copula a model with lowe r or with upper
tail dependence?
When the Clayton copula parameter is positive, i.e., theta >0, the sur-
vival copula of the corresponding Clayton copula is a model w ith lower tail
dependence.
If (U1,U2)∼C, then the survival copula of Cis the distribution of (1 −U1,1−
U2), and thus the properties of the lower tail of Care the properties of the
upper tail of the survival copula. Hence, we got an incorrect answ er. After
asking again, we got this response.
For the Clayton copula, if the parameter theta is positive, t he survival copula
will have upper tail dependence. This is because the Clayton copula exhibits
positive lower tail dependence, which means that when one va riable has a
very low value, the other variable tends to have a low value as well. This
implies that when one variable has a very high survival time, the other vari-
able also tends to have a high survival time, leading to upper tail dependence
in the survival copula.
Again, we got contradicting answers. Based on this observation, t he reader
could raise the following question.
Would you agree with the following statement? ChatGPT is a la rge language
model, and thus, if we drastically simplify it, it basically makes inference on
which word comes next. However, it makes no inference on the u nderlying
concepts.
As an AI language model, ChatGPT is indeed designed to make pr edictions
on the next word based on the context and patterns in the input text. How-
ever, it is also capable of capturing and inferring underlyi ng concepts from
10

--- PAGE 11 ---
the text, to some extent. While it is true that ChatGPT is not c apable of
understanding concepts in the same way as humans do, it can st ill recognize
patterns and relationships between words and phrases, and u se this infor-
mation to make inferences and generate responses that reﬂec t some level of
understanding of the underlying concepts.
This response conﬁrms what we have seen so far, hence, any user should take
these limitations into account with the utmost seriousness and be ex tremely
careful when asking ChatGPT for some reasoning. The examples ab ove also
well illustrate that the current version of ChatGPT is deﬁnitely notan appro-
priate tool for reasoning, which is as also observed by Frieder et al. (2023) and
Bang et al. (2023). However, this by no means implies that it cannot s erve as a
helpful AI partner for pair programming.
3.2 The density
It can be easily shown that the density cθofCθis
cθ(u,v) =∂2Cθ(u,v)
∂u∂v= (θ+1)(uv)−(θ+1)(u−θ+v−θ−1)−1
θ−2(4)
forθ >0. Before we ask ChatGPT to generate code evaluating the Clayton
copula density, it would be good to ask for a plain formula.
What is the analytical form of the probability density funct ion of the bivariate
Clayton copula? Output the formula using LaTeX.
cθ(u,v) =∂2Cθ(u,v)
∂u∂v=θ+1
θ/parenleftbigg
u−θ−1v−θ−1(u−θ+v−θ−1)−θ+2
θ+1/parenrightbigg
.
Out of many factually distinct responses, we present this one to illus trate how
dangerous it could be to believe that ChatGPT knows or can derive th e right
formula. Even if it looks quite similar to (4), this is notthe density of Cθ.
This formula is not correct. Which is the analytical form of t he probability
density function of the bivariate Clayton copula? Output th e formula using
LaTeX.
c(u,v) =/bracketleftBig
(u−θ+v−θ−1)−1
θ−2/bracketrightBig
·/parenleftbig
u−θv−θ/parenrightbig−1
θ−1
11

--- PAGE 12 ---
Another formula quite similar to (4), but also this one is incorrect. As already
mentioned before, ChatGPT is not a good option when it comes to rea son-
ing. So, to get the right formula, some symbolic tool is deﬁnitely pref erred.
However, note that ChatGPT plugins10has been recently announced, and par-
ticularly Code interpreter, which is an experimental ChatGPT model that can
use Python, handle uploads and downloads, allows for symbolic compu tations.
Even if not freely available yet, this might be also a possible way to mitiga te
the problem, directly in the ChatGPT environment.
Following our communication protocol, let us feed ChatGPT with the rig ht
formula, and ask for a corresponding function in three programmin g languages:
1) MATLAB11, which represents a proprietary software, 2) Python12, an open-
source software popular in the AI community, and R13, an open-source software
popular in the statistical community. Note that in cases when the ou tput is too
wide, we adjust it in order to ﬁt on the page; otherwise we do not adj ust it in
any other way.
This is the correct formula: c(u,v) = (1 + theta) * (u * v)ˆ(-the ta-1) *
(uˆ(-theta) + vˆ(-theta) - 1)ˆ(-2-1/theta). Code it in MATL AB as Clayton-
CopulaDensity(u, v, theta).
Here’s the MATLAB code for the probability density function of the Clay-
ton copula with parameter theta evaluated at (u, v):
function dens = ClaytonCopulaDensity(u, v, theta)
dens = (1 + theta) * (u * v)^(-theta - 1) * ...
(u^(-theta) + v^(-theta) - 1)^(-2 - 1/theta);
end
You can call this function with your desired values of u, v, an d theta to eval-
uate the density of the Clayton copula at those values.
After feeding ChatGPT with the right formula, it immediately generat ed a
functional code. Notice that we used quite a natural and relaxed f orm of con-
versation, e.g., like in an email.
Code ClaytonCopulaDensity in Python.
def ClaytonCopulaDensity(u, v, theta):
density = ((1 + theta) * (u * v)**(-theta - 1) *
10https://openai.com/blog/chatgpt-plugins
11We used version R2020a.
12Version 3.9.
13Version 4.2.2.
12

--- PAGE 13 ---
(u**(-theta) + v**(-theta) - 1)**(-2 - 1/theta))
return density
And in R.
ClaytonCopulaDensity <- function(u, v, theta) {
density <- ((1 + theta) * (u * v)^(-theta - 1) *
(u^(-theta) + v^(-theta) - 1)^(-2 - 1/theta))
return(density)
}
As ChatGPT takes into account the previous conversation, we cou ld aﬀord to
be extremely concise with our prompts and still get correct solution s. In what
follows,weaskforcodeonlyin MATLABtosavespace. However,the equivalent
code in Python and Ris shown in the appendices, where all the functions can
be easily identiﬁed by their names.
3.3 The estimation
Having this ClaytonCopulaDensity(u, v, theta) function, c ould you write a
MATLAB code for the maximum likelihoood estimator of the par ameter theta?
function thetaMLE = ClaytonCopulaMLE(u, v)
loglik = @(theta) sum(log(ClaytonCopulaDensity(u, v, the ta)));
thetaMLE = fminsearch(@(theta) -loglik(theta), 1);
end
By contrast to our struggles with the PDF, we immediately got a corr ect solu-
tion. This may be due to the fact that code snippets computing ML es timators
occur more frequently in ChatGPT’s training data. This pattern (th e more gen-
eral the task, the more frequently we receive a working solution on ﬁrst trial) is
observed also in other examples throughout this work.
3.4 The sampling
In MATLAB, code a function ClaytonSample(theta, n) that gene rates a sample
of size n from the bivariate Clayton copula with parameter th eta.
13

--- PAGE 14 ---
function u = ClaytonSample(theta, n)
u1 = rand(n, 1);
u2 = rand(n, 1);
v1 = (u1.^(-theta)+u2.^(-theta)-1).^(-1/theta);
v2 = (u1.^(-theta)+u2.^(-theta)-1).^(-1/theta);
u = [v1, v2];
end
We see that u1andu2are drawn from the standard uniform distribution, which
is typically used in such sampling algorithms. However, then these are just used
as arguments of (3), resulting in two identical one-dimensional sam ples from
the distribution of C(U1,U2), where U1andU2are two independent random
variables with the standard uniform distribution. Note if the random vector
(U1,U2) would be distributed according to C, then the distribution function of
C(U1,U2) would be the distribution known as Kendall function; see Joe (2014 ,
pp. 419–422). So we are witnessing a mixture of approaches relate d to copula
modelling,which, however,donotleadtoacorrectsolution. Asample generated
with this code for θ= 2 is shown at the left-hand side of Figure 1, and it is
clearlynota sample from the Clayton copula Cθwith parameter θ= 2.
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0 0.2 0.4 0.6 0.8 100.20.40.60.81
0 0.2 0.4 0.6 0.8 100.20.40.60.81
Figure 1: A sample from the random vector ( C(U1,U2),C(U1,U2)) (left). A
sample from another incorrect sampling trial of ChatGPT (center) . A sample
generated by the correct sampling procedure based on theory fe d to ChatGPT
(right). The sample was obtained by calling ClaytonSample(2, 1000) .
In ClaytonSample(theta, n), v1 and v2 are two identical vecto rs. Hence, u =
[v1, v2] cannot be a sample from the bivariate Clayton copula with parameter
theta. Could you code a correct version of ClaytonSample(th eta, n)?
function u = ClaytonSample(theta, n)
14

--- PAGE 15 ---
w = rand(n, 1);
v = rand(n, 1);
for i = 1:n
v(i) = (w(i)^(-theta) * (1 - v(i))^ ...
(-theta/(1+theta)))^(-1/theta);
end
u = [w, v]
In the second try, ChatGPT also failed to produce a correct solutio n; see the
sample generated by this code at the center of Figure 1. Here, the reasoning
behind is even less clear. These two trivial examples again illustrate th at we
must be extremely careful about results produced by ChatGPT. O n the other
hand, this gives us a chance to show that ChatGPT can interactively learn new
concepts by feeding it with the necessary theory as we will demonst rate next.
A standard approach for eﬃcient sampling from Archimedean copula s was
introducedin Marshall and Olkin(1988). LetusfeedChatGPTwithth e related
theory and ask it for a correct solution.
Let psi(t) = (1+ t)ˆ(-1/theta)be a functionwith parametert hetain (0, infty).
Let E1 and E2 be two independent samples from the standard exp onential
distribution, and V be an independent sample from the Gamma d istribution
with the shape parameter1/thetaand the scaleparameter 1. T hen (psi(E1/V),
psi(E2/V)) is a sample from the bivariate Clayton copula wit h parameter theta.
Codea MATLAB functionClaytonSample(theta, n), which gene ratesn samples
from the bivariate Clayton copula with parameter theta usin g the provided
theory.
function u = ClaytonSample(theta, n)
% Generate two samples from exponential distribution
E = exprnd(1, 2, n);
% Generate a sample from gamma distribution
V = gamrnd(1/theta, 1, 1, n);
% Define the function psi(t)
psi = @(t) (1 + t).^(-1/theta);
% Apply the transformation
u = [psi(E(1,:) ./ V); psi(E(2,:) ./ V)]’;
end
Apart from producing a correct solution, which generated the sam ple at the
right-hand side of Figure 1, this is a clear demonstration of the ability of Chat-
GPT to learnduring the interaction with the user (having in mind that this
capacity is just a quite convincing illusion enabled by the prompt engine ering
15

--- PAGE 16 ---
technique applied to ChatGPT, as discussed in Section 2.2). In contr ast to the
previous example, where it “only” translated the fed formula for th e PDF to a
more-or-less similarly looking code, this example shows that ChatGPT is able
to understand even relatively complex concepts.
Such an ability makes ChatGPT a feasible tool also in cases when it face s
unknown concepts. This is essential, e.g., in cases when a new theory devel-
oped by a researcher is about to be coded. Also, notice that ChatG PT saves
us time by mapping our concepts, e.g., the standard exponential an d gamma
distribution, toexistingfunctionsintheconsideredprogramminglan guage. Par-
ticularly, without explicitly asking, ChatGPT avoided a loop iterating ov er 1 to
nin which one sample from the copula would be generated, which is typica lly
slow, but generates directly nsamples of E1andE2from the standard expo-
nential distribution ( exprnd(1, 2, n) , where1denotes the parameter of the
exponential distribution). We can thus avoid probably the most bor ing part of
coding when we are browsing the documentation for a particular fun ction in
available libraries.
Finally, let us perform a simple check of all the previously generated f unc-
tions.
Generate a script that draws a sample of size 1000 from the Cla yton copula
with theta = 2 and then computes an estimate theta hat of theta using the
ML estimator.
U = ClaytonSample(2, 1000);
theta_hat = ClaytonCopulaMLE(U(:,1), U(:,2));
Being in the context of the previous conversation, ChatGPT exploit ed the
ClaytonCopulaMLE function generated in Section 3.3. After executing the sim-
ple check code, we got the following error.
Error using *
Incorrect dimensions for matrix multiplication. Check tha t
the number of columns in the first matrix matches the
number of rows in the second matrix. To
perform elementwise multiplication, use ’.*’.
Error in ClaytonCopulaDensity (line 2)
dens = (1 + theta) * (u * v)^(-theta - 1) * ...
Error in ClaytonCopulaMLE>@(theta)sum(log(...
ClaytonCopulaDensity(u,v,theta))) (line 2)
loglik = @(theta) sum(log(ClaytonCopulaDensity(u, v, the ta)));
16

--- PAGE 17 ---
Error in ClaytonCopulaMLE>@(theta)-loglik(theta) (line 3)
thetaMLE = fminsearch(@(theta) -loglik(theta), 1);
Error in fminsearch (line 200)
fv(:,1) = funfcn(x,varargin{:});
Error in ClaytonCopulaMLE (line 3)
thetaMLE = fminsearch(@(theta) -loglik(theta), 1);
We copy-pasted the error message to ChatGPT, e.g., gave it the pr ompt
We got this error message: ***
where***was the error message, and got the following response.
The error message suggests that the code is trying to per-
form matrix multiplication with the * operator, but the dime n-
sions of the matrices do not match. This is likely happen-
ing in the line dens = (1 + theta) * (u * v)ˆ(-theta - 1) * ....
To ﬁx this, we need to use element-wise multiplication with t he .* op-
erator instead of matrix multiplication with *. Here is the c orrected ver-
sion of the function:
function dens = ClaytonCopulaDensity(u, v, theta)
dens = (1 + theta) * (u .* v).^(-theta - 1) .* ...
(u.^(-theta) + v.^(-theta) - 1).^(-2 - 1/theta);
end
Note the use of .* instead of * and .ˆ instead of ˆ to indicate el ement-
wise operations.
ChatGPT not only detected what is wrong, but provided a correcte d solution
together with an explanation of the problem. Given that these trivia l mistakes
occur quite often in daily practice, such help that points out to the p roblem or
even solves it can signiﬁcantly save time. After executing the simple c heck with
the correctedversionof ClaytonCopulaDensity ,we gotthetahat = 2.12 . So
far so good.
3.5 The visualization
17

--- PAGE 18 ---
Write MATLAB code that computes and creates a plot of 20 pairs of (theta,
thetahat), where theta is from (0.1, 3) and theta hat is the ML estimate of
theta (computed using ClaytonCopulaMLE(u, v)) based on a sa mple of size
1000 from the Clayton bivariate copula with parameter theta (generated using
U = ClaytonSample(theta, n))
n = 1000;
thetas = linspace(0.1, 3, 20);
theta_hats = zeros(size(thetas));
for i = 1:length(thetas)
U = ClaytonSample(thetas(i), n);
theta_hats(i) = ClaytonCopulaMLE(U(:,1), U(:,2));
end
plot(thetas, theta_hats, ’o’);
hold on;
plot([0, 3], [0, 3], ’--’);
xlim([0, 3]);
ylim([0, 3]);
xlabel(’\theta’);
ylabel(’\theta_{hat}’);
title([’Maximum likelihood estimates for ’ ...
’Clayton copula parameter’]);
legend(’Estimates’, ’True values’);
The plot generated by the response is depicted on the left-hand sid e of Figure 2.
Asthe(θ,ˆθ)pairsareclosetotheidentity, itgivesanevidencethatallprevious ly
generatedcodeworksproperly. Wewouldliketohighlightthatevenif ChatGPT
is not instructed to:
1. The parameters θarelinearlyspaced in the desired interval. This is a
typical choice for many visualizations.
2. It shows perfect identity by a line, also typically considered in an ide al
benchmark. This clearly demonstrates that ChatGPT at least part ially
understands the underlying concepts, i.e., that we are estimating t he true
value of some parameter.
3. Typical time-consuming trivialities like limits, labels, title and legend ar e
also shown.
All in all, this is the type of tasks where the user can substantially ben eﬁt from
a collaboration with tools like ChatGPT.
18

--- PAGE 19 ---
0 0.5 1 1.5 2 2.5 300.511.522.53hatMaximum likelihood estimates for Clayton copula parameter
Estimates
True values
0.0 0.5 1.0 1.5 2.0 2.5 3.0
T rue theta0.00.51.01.52.02.53.0Estimated theta
0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.5 1.0 1.5 2.0 2.5 3.0
True thetaEstimated theta
Figure 2: The plots produced by the code generated by ChatGPT in M ATLAB
(top-left), Python (top-right) and R(bottom).
However, we would like to note that two iterations of our last prompt were
done before we got the presented one. In the ﬁrst one, we omitte d the con-
tent of the two parentheses with the function names. In that cas e, the output of
ClaytonSample didnotmatchthedimensionoftheinputof ClaytonCopulaMLE .
In the second iteration, we added those parentheses to the prom pt, but without
“U =”, and got a very similar error. We copy-pasted the error message to
ChatGPT, but this time it was not succesful to provide a correct ve rsion. Fi-
nally, we added that “ U =” in the prompt with the intuition that ChatGPT is
not aware of the right dimensions of the output, and this way we got a working
solution. The main take-away from this example is that obtaining a cor rect
solution is an iterative process requiring careful prompt engineerin g. The con-
nection to human-human collaboration is clear: until the software e ngineer does
not understand what the researcher exactly wants, she/he will p robably deliver
unsatisfactory solutions.
19

--- PAGE 20 ---
Code the last functionality from MATLAB in Python.
The response is shown in Appendix A and the corresponding plot is dep icted
at right-hand side of Figure 2. When a project is migrated from one p ro-
gramming language to another, this ability of ChatGPT could limit the re lated
eﬀorts/expenses dramatically.
And in R.
In this case, the transpilation to Rwas not successful as we again got a problem
with the dimensions matching of the input of ClaytonCopulaMLE and output of
ClaytonSample . We were also unsuccessful with copy-pasting the errormessage
to ChatGPT. Clearly, we could be more precise by specifying these dim ensions.
However, we were deliberately underspecifying the task in order to illustrate
what separates successful solutions from unsuccessful ones.
To resolve the task, we ﬁnally used a rather “brute-force” appro ach consist-
ing of copy-pasting the whole code in Python to ChatGPT and asking it for the
same code on R. The response is shown in Appendix B and the corresponding
plot is depicted at the bottom of Figure 2. Another way we obtained a working
solution was to feed ChatGPT with explicit information about the dimen sions
of the inputs and outputs, i.e., using a similar approach as before by a dding “U
=”. On the one hand, this approach is more elegant then the brute-f orce one,
on the other hand, the brute-force approachallowedus to get a w orkingsolution
in less time. A choice between them thus depends on the user’s priorit ies.
Basically we now have an implementation of our example task in every lan -
guage supported by ChatGPT. Note that an implementation of samp ling from
the Clayton copula in Python appeared relatively recently as a seriou s eﬀort
presented in Boulin (2022). Here, we got an equivalent implementatio n com-
pletely created by an AI. Clearly, with the abilities of ChatGPT, achiev ing a
solution in distinct programming languages will become much less valuab le.
3.6 The parallelization
Note that as the codes for the following tasks in Python and Rwere rather long,
we present only the MATLAB versions.
3.6.1 CPUs
In MATLAB, optimize the code for computing that 20 pairs of (th eta,
thetahat) for parallel computing on CPUs. Create a demonstration that shows
how the time for performing the same job decreases as the numb er of workers
increases.
20

--- PAGE 21 ---
% Define the range of theta values
thetas = linspace(0.1, 3, 20);
% Define the number of samples
n = 1000;
% Define the range of workers to use
numWorkers = 1:4;
% Initialize an array to store the execution times
times = zeros(size(numWorkers));
for i = 1:length(numWorkers)
% Start a timer
tic
% Run the loop in parallel using parfor
parfor (j = 1:length(thetas), numWorkers(i))
% Generate a sample from the Clayton copula with
% parameter theta
U = ClaytonSample(thetas(j), n);
% Estimate the parameter theta using maximum likelihood
ClaytonCopulaMLE(U(:,1), U(:,2));
end
% Stop the timer and record the execution time
times(i) = toc;
end
% Plot the execution times
plot(numWorkers, times, ’o-’)
xlabel(’Number of workers’)
ylabel(’Execution time (s)’)
We directly got a working solution, which generated the plot shown at the left-
hand side of Figure 3. Let us highlight several points:
•Being in the context of the conversation, we could aﬀord to be extr emely
concise and use just one word (“ that” in the ﬁrst sentence) to point at the
functionality we request; natural and time-saving.
•We just prompted ChatGPT with a very high-level request, Create a
demonstration , and got exactly what we wanted. Originally, we used, say,
the traditional way of thinking, which involves being very speciﬁc whe n
designing a task. For example, “create an array for time measurem ents,
21

--- PAGE 22 ---
1 2 3 4
Number of workers0.10.150.20.250.30.350.4Execution time (s)
0 10 20 30
Number of workers00.511.522.5Execution time (s)
Figure 3: Two plots demonstrating the improvement in run-time when adding
new workers, generated by the code produced by ChatGPT. The o utput of the
code when executed on a local machine is shown on the left side. On th e right
side, the same when executed on an available server with 32 CPUs. Fo r the
latter, we adjusted the code with numWorkers = 1:32 andn = 10000 .
measure time for this and that, store it in the array and then plot it in the
way that...”. Since such tasks probably occur many times in ChatGPT’s
training data, overly specifying is not necessary, which, again, can save a
lot of time.
•Notice the 4 workers in numWorkers = 1:4 . It is a common number of
CPUs of an averageoﬃce PC. This number is thus not just a pure ran dom
guess, but rather a most occurring one in ChatGPT’s training data.
Note that apart from our oﬃce PC, we executed the code on an ava ilable server
with 32 CPUs, which generated the plot at the right-hand side of Figu re 3. As
only 20 theta values ( thetas) are considered, we observe that the improvement
in time does not continue from the 21st worker onwards.
3.6.2 GPUs
In MATLAB, optimize the function ClaytonSample for parallel computing on
GPUs. Call the optimized function ClaytonSampleGPUs. Then create a script
that measures the run-time of ClaytonSample(2, 1000000)an d of ClaytonSam-
pleGPUs(2, 1000000).
We got two pieces of code as response. First, ChatGPT provided an optimiza-
tion ofClaytonSample for GPUs.
22

--- PAGE 23 ---
function [U, V] = ClaytonSampleGPUs(theta, n)
% Generate random variables on the GPU
E1 = gpuArray(single(exprnd(1, n, 1)));
E2 = gpuArray(single(exprnd(1, n, 1)));
V = gpuArray(single(gamrnd(1/theta, 1, n, 1)));
psi = @(t) (1+t).^(-1/theta);
U = gather(psi(E1./V));
V = gather(psi(E2./V));
end
Then, a code for its demonstration followed.
% Define parameters
n = 1000000;
theta = 2;
% Time CPU version
tic
data_cpu = ClaytonSample(theta, n);
time_cpu = toc;
% Time GPU version
tic
data_gpu = ClaytonSampleGPUs(theta, n);
time_gpu = toc;
% Display results
disp([’CPU time: ’ num2str(time_cpu) ’ seconds’]);
disp([’GPU time: ’ num2str(time_gpu) ’ seconds’]);
After executing the latter, the output was:
CPU time: 0.17493 seconds
GPU time: 0.093107 seconds
Ascanbeobserved,theoptimizationof ClaytonSample ,ClaytonSampleGPUs ,
is based on the addition of gpuArray , which assures that the sampling from the
standard exponential and gamma distributions as well as the remain ing com-
putations are performed directly on available GPUs. The outputs UandVare
then gathered from the GPUs onto the client by gather. As the output of the
demonstration part of the code shows, this roughly halved the non -optimized
run-time.
Apartfromamoreeﬃcientimplementation, ChatGPTalsosavesourt imeby
exemptingusfrominspectingwhetherornoteachinvolvedfunction issupported
on GPUs. We should also not forget the educational aspect of the m atter.
As these optimization techniques are rather advanced, an inexper ienced user
23

--- PAGE 24 ---
genuinely learns from these outputs (having in mind they might not alw ays
be correct). For example, without explicitly mentioning it in our promp t, the
values of thetaandnare stored in separate variables before they are used
inClaytonSample andClaytonSampleGPUs . This belongs to proper coding
techniques.
4 Summary and discussion
During the development of the working code solving our example task , we ob-
served a considerable list of advantages from which we can beneﬁt w hile pair
programming with ChatGPT. In particular:
1. ChatGPT can save time by quickly and concisely summarizing basic fa cts
about the topic of our interest, e.g., formulas or application example s, as
illustrated in Section 3.1.
2. If ChatGPT is not able to provide a correct solution due to a lack of or
incorrect knowledge, we can feed it with the correct knowledge, an d make
it use it to provide a correct solution. In Section 3.2, this approach le d
ChatGPT to produce a function evaluating the PDF of the copula mod el
in three diﬀerent programming languages. In Section 3.4, a working c ode
for sampling from the copula model is generated once ChatGPT was f ed
by the related non-trivial theory. Particularly the latter example s hows
that ChatGPT is able to understand even relatively complex concept s,
and clearly demonstrates that it can be applied in cases when it faces
unknown concepts.
3. ChatGPT saves time by mapping simple concepts, e.g., sampling from
the standard exponential and gamma distributions, to existing cod e (li-
braries, APIs, or functions) available for a given programming langu age,
as illustrated in Section 3.4.
4. The more common the task to solve, the more successful ChatG PT in
generating a correct solution. This is illustrated, e.g., in Section 3.3,
whereweimmediatelyobtainedcodeimplementingthemaximallikelihood
estimator by a simple prompt like write code for the maximum likelihood
estimator of that parameter . Another example is the transpilation of the
MATLABsolutiontoPythoninSection3.5, ortheoptimizationofexistin g
code for parallel computing on CPUs and GPUs in Section 3.6.
5. ChatGPT can help in cases when an error is thrown after executin g the
generated code. In Section 3.4, we have seen that it not only detec ted
what was wrong, but provided a corrected solution. Apart from sa ving
time needed to search and ﬁx the error, this can be crucial particu larly for
less experienced programmers, who could ﬁnd the error too comple x and
eventually give up. ChatGPT helped us roughly with 1/3 of the errors we
encountered. Even if not perfect, this is substantially better tha n no help
at all.
24

--- PAGE 25 ---
6. ChatGPT can help with creating visualizations. In Section 3.5, it gen er-
ated a visualization suggesting that all previously generated code is cor-
rect. Even if we have not asked for it, the visualization included all th e
typical trivia like labels, benchmarks, limits, legends, etc.
7. ChatGPT at least partially understands the underlying concepts of what
we are doing. Without asking it to do so, it added to the visualization
the plot of the identity (see Section 3.5), suggesting that it is aware of us
trying to estimating the true value of some parameter.
8. ChatGPT can transpile code from one programming language to an other
also with high-level prompts like Code it in Python andAnd in R , demon-
stratedinSection3.5. Thesamesectionalsoshowsthatifthetrans pilation
fails (which happened with the transpilation to R), it is possible to use a
quick “brute-force” solution that also accomplished the task.
9. ChatGPT can optimize the already generated code, e.g., for para llel com-
putations. By prompting optimize that for parallel computing on CPUs , we
immediately got the optimized version of the sample-estimate proced ure
developed in Section 3.5; see Section 3.6.1. The same section also show s
that a high-level prompt like Create a demonstration of this optimization
can result in code showing the impact of the optimization, again includin g
the typically tedious but necessary trivia like labels, etc. Similarly, suc h
an optimization together with a simple demonstration was generated also
for computations on GPUs; see Section 3.6.2.
10. ChatGPTfollowspropercodingtechniques,sotheusercangen uinelylearn
them too. We observed that the produced code is properly commen ted,
indented, modularized, avoids code duplicities, etc.
11. ChatGPT helps the user to get familiar with the produced code fa ster.
When providing code, ChatGPT typically surrounds it by further info r-
mation explaining its main features. To save space, we mostly cut this
out, however, an example can be found, e.g., in Section 3.4 in connect ion
to the error message thrown by the simple check code.
We have also seen that pair programming with ChatGPT brings severa l disad-
vantages, which should be carefully considered. Let us summarize t hem and
discuss possibilities to mitigate them:
1. ChatGPT in its current version (early 2023) is poor in reasoning; s ee
Section 3.1. On two examples, we demonstrated how it responses wit h
contradicting answers to the same question. We particularly highligh t
the case when it ﬁrst answered yesand then noto the same question.
Also, we demonstrated how dangerous this could be in quantitative r ea-
soning, where it generated incorrect formulas that looked very sim ilar to
correct ones; see the PDF derivation in Section 3.2. In order to mitig ate
this problem, a lot of eﬀort can be currently observed. One of the m ost
25

--- PAGE 26 ---
promising examples in the direction of quantitative reasoning is Minerv a
(Lewkowycz et al., 2022), an LLM based on the PaLM general langua ge
models (Chowdhery et al., 2022) with up to 540 billion of parameters.
This model, released in June 2022, gained its attention by scoring 50%
on questions in the MATH data set, which was a signiﬁcant improvemen t
of the state-of-the-art performance on STEM evaluation datas ets; see Ta-
ble 3 therein. In other works, the authors develop models ﬁne-tun ed for
understanding mathematical formulas (Peng et al., 2021), or emp loy deep
neural networks in mathematical tasks like symbolic integration or s olving
diﬀerential equations (Lample and Charton, 2019). Another way o f miti-
gatingthe problemcanbe tryingtoexploit atmaximum the currentLL Ms
by carefully adjusting the prompt in order to get more reliable answe rs.
This increasingly popular technique, called prompt engineering , involves
special techniques to improve reliability when the model fails on a task14,
andcansubstantiallyimprovethesolution, e.g., forsimple mathproble ms,
just by adding “ Let’s think step by step. ” at the end of the prompt. Note
thatwetriedthistechniqueintheexampleconsideringthetaildepen dence
of the survival Clayton copula in Section 3.1, however, without succ ess,
probably because the underlying concepts go beyond simple mathem atics.
2. If ChatGPT lacks the necessary knowledge or possesses incorr ect knowl-
edge, it may generate an incorrect solution without any indication to the
user. As illustrated in Section 3.4, after asking it for code for samplin g
from a Clayton copula model, ChatGPT ﬁrst generated two routines ,
which were resembling proper sampling algorithms, but were entirely in -
correct. Due to the opacity of the current state-of-the-art L LMs that con-
tain tens or even hundreds of billions of parameters, the correctn ess of the
solution can hardly be guaranteed in all cases. While there may be eﬀo rts
to develop more explainable LLMs, it is unlikely that the fundamental
challenges related to the complexity of language and the massive amo unts
of data required for training will be completely overcome. Therefor e, it
is essential for a human expert in the ﬁeld to alwaysverify the output
generated by the model.
3. Speciﬁcally, ChatGPT tends to be less successful in producing ac curate
solutions for tasks that are less common. This means that the oppo site of
advantage 4. also applies. In Section 3.2, this is demonstrated thro ugh the
probability density function (PDF) of the copula model. In Section 3.4 ,
through the sampling algorithm. To solve these issues, we provided t he
required theory to ChatGPT, which led to a correct solution, see th e same
two sections.
4. ChatGPT does not have any memory. If the conversation is too lo ng, and
thus does not ﬁt within ChatGPT’s context window, it seems that the
model has forgotten some parts of the conversation. This, toge ther with
ways how to mitigate this issue, has already been discussed in Section 2.2.
14https://github.com/openai/openai-cookbook/blob/main /techniques toimprove reliability.md
26

--- PAGE 27 ---
Apart from ChatGPT, there are several other language models th at are ca-
pable of generating code solutions from natural language inputs. O ne notable
example is AlphaCode (Li et al., 2022), which achieved on average a ra nking of
top54.3%incompetitions with morethan5,000participantson recent program-
ming competitions on the platform Codeforces. Recently, AlphaCod e has been
made publicly available15, including example solutions from the mentioned con-
test. Another example is OpenAI Codex16, already mentioned in the introduc-
tion. In contrast to ChatGPT, these models have been developed p articularly
for code generation. On the one hand, it is thus possible that one ca n generate
solutions that are better than those generated with ChatGPT. It would thus be
interesting future research to compare, e.g., the successfulnes s of these models
for solving the tasks considered in this work.
On the other hand, ChatGPT might be more convenient for many use rs
than these models as it allows for interaction during the coding proce ss. Unlike
AlphaCode and OpenAI Codex, which generate code snippets based on natural
language inputs without any further interaction, ChatGPT allows us ers to pro-
vide feedback and adjust the generated code in real-time. This inte raction can
be beneﬁcial for several reasons. First, it allows users to clarify t heir intent and
ensures that the generated code aligns with their goals. For examp le, as we have
seen in Section 3.4 that considers the sampling from a Clayton copula m odel,
if a user requests a speciﬁc functionality and the generated code d oes not quite
match what they had in mind, the user can provide feedback to Chat GPT to
adjust the code accordingly. Second, the interaction with ChatGP T can help
users learn more about programming and improve their coding skills. B y en-
gaging in a dialogue with ChatGPT, users can gain insights into the logic a nd
structure of the code they are generating, and learn how to impro ve their code
in the future. For example, in Section 3.6.2, we could genuinely learn ho w to
convert existing code for parallel computing on GPUs. Finally, the int eraction
with ChatGPT can help users troubleshoot errors and debug their c ode more
eﬀectively. As we have seen in Section 3.4, ChatGPT can recognize co mmon
programming mistakes, and provide feedback that helps users to id entify and
ﬁx errors in their code. These reasons, together with the fact th at ChatGPT
can be conveniently accessed through a web portal, led us to choos e ChatGPT
as our pair programming AI partner.
5 Conclusion
In a human-AI collaboration, we developed working code that impleme nts sam-
pling from a copula model, estimation of its parameter, visualization su ggesting
that the last two tasks worked properly, and a parallelization of the code for
CPUs as well as for GPUs. To illustrate the coding abilities of the AI par t,
represented by ChatGPT, all the mentioned tasks were implemente d without
a single line of code written by the human. In addition to presenting ho w to
15https://github.com/deepmind/code contests
16https://openai.com/blog/openai-codex/
27

--- PAGE 28 ---
achieve a successful solution for a given task, we also showed addit ional exam-
ples demonstrating which modiﬁcations of our prompts for ChatGPT turned
failed solutions to successful ones. This resulted in a comprehensiv e list of re-
lated pros and cons, suggesting that if typical pitfalls can be avoide d, we can
substantially beneﬁt from a collaboration with an AI partner like Chat GPT.
Acknowledgments
The author thanks the Czech Science Foundation (GA ˇCR) for ﬁnancial sup-
port for this work through grant 21-03085S. The author also tha nks to Martin
Holeˇ naandMariusHofertforconstructivecommentsandrecomm endationsthat
deﬁnitely helped to improve the readability and quality of the paper.
Appendix A The solution in Python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import fmin
def ClaytonSample(theta, n):
E1 = np.random.exponential(size=n)
E2 = np.random.exponential(size=n)
V = np.random.gamma(1/theta, 1, size=n)
psi = lambda t: (1+t)**(-1/theta)
U = psi(E1/V)
V = psi(E2/V)
return np.column_stack((U, V))
def ClaytonCopulaDensity(u, v, theta):
return ((1+theta)*(u*v)**(-theta-1)*
(u**(-theta)+v**(-theta)-1)**(-2-1/theta))
def ClaytonCopulaMLE(data):
n = data.shape[0]
u, v = data[:, 0], data[:, 1]
loglik = lambda theta: np.sum(np.log(
ClaytonCopulaDensity(u, v, theta)))
thetaMLE = fmin(lambda theta: -loglik(theta), 1,
disp=False)
return thetaMLE[0]
thetas = np.linspace(0.1, 3, 20)
28

--- PAGE 29 ---
theta_hats = []
for theta in thetas:
data = ClaytonSample(theta, 1000)
theta_hat = ClaytonCopulaMLE(data)
theta_hats.append(theta_hat)
plt.plot(thetas, theta_hats, ’o’)
plt.plot(thetas, thetas, ’--’)
plt.xlabel(’True theta’)
plt.ylabel(’Estimated theta’)
plt.show()
An example of a redundant code is n = data.shape[0] . As can be observed,
the variable nhas no use in ClaytonCopulaMLE .
Appendix B The solution in R
ClaytonSample <- function(theta, n) {
E1 <- rexp(n)
E2 <- rexp(n)
V <- rgamma(n, 1/theta, 1)
psi <- function(t) (1+t)^(-1/theta)
U <- psi(E1/V)
V <- psi(E2/V)
return(cbind(U, V))
}
ClaytonCopulaDensity <- function(u, v, theta) {
return((1+theta)*(u*v)^(-theta-1)*
(u^(-theta)+v^(-theta)-1)^(-2-1/theta))
}
ClaytonCopulaMLE <- function(data) {
u <- data[,1]
v <- data[,2]
loglik <- function(theta) -sum(log(
ClaytonCopulaDensity(u, v, theta)))
thetaMLE <- optimize(loglik, c(0.1, 3), maximum=FALSE)$m inimum
return(thetaMLE)
}
thetas <- seq(0.1, 3, length.out=20)
29

--- PAGE 30 ---
theta_hats <- c()
for (theta in thetas) {
data <- ClaytonSample(theta, 1000)
theta_hat <- ClaytonCopulaMLE(data)
theta_hats <- c(theta_hats, theta_hat)
}
plot(thetas, theta_hats, pch=19, col="blue", ylim=c(0,3 ),
xlab="True theta", ylab="Estimated theta")
abline(a=0, b=1, lty=2)
Interestingly, even if this code is a direct transpilation of the code f rom Ap-
pendix A, the redundant code from the Python version of ClaytonCopulaMLE
is not present. This hints on the ability of ChatGPT to keep only the co de that
is relevant.
References
Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., J i,
Z., Yu, T., Chung, W., et al. (2023). A multitask, multilingual, multimodal
evaluation of chatgpt on reasoning, hallucination, and interactivity .arXiv
preprint arXiv:2302.04023 .
Boulin, A. (2022). Sample from copula: a coppy module. arXiv preprint
arXiv:2203.17177 .
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Lan guage
models are few-shot learners. Advances in neural information processing sys-
tems, 33:1877–1901.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., E d-
wards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evalu ating
large language models trained on code. arXiv preprint arXiv:2107.03374 .
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts , A.,
Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022). P alm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 .
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amode i, D.
(2017). Deep reinforcement learning from human preferences. Advances in
neural information processing systems , 30.
Clayton, D. G. (1978). A model for association in bivariate life tables and its
application in epidemiological studies of familial tendency in chronic dise ase
incidence. Biometrika , 65:141–151.
30

--- PAGE 31 ---
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: P re-
training of deep bidirectional transformers for language underst anding.arXiv
preprint arXiv:1810.04805 .
Frieder, S., Pinchetti, L., Griﬃths, R.-R., Salvatori, T., Lukasiewicz, T ., Pe-
tersen, P. C., Chevalier, A., and Berner, J. (2023). Mathematical capabilities
of chatgpt. arXiv preprint arXiv:2301.13867 .
Hobæk Haﬀ, I. (2013). Parameter estimation for pair-copula cons tructions.
Hofert, M. (2010). Sampling nested Archimedean copulas with applic ations to
CDO pricing.
Hofert, M., Huser, R., and Prasad, A. (2018). Hierarchical Archim ax copulas.
Journal of Multivariate Analysis , 167:195–211.
Hofert, M., M¨ achler, M., and McNeil, A. J. (2013). Archimedean cop ulas in
high dimensions: Estimators and numerical challenges motivated by ﬁ nancial
applications. Journal de la Soci´ et´ e Fran¸ caise de Statistique , 154(1):25–63.
Huang, Y., Zhang, B., Pang, H., Wang, B., Lee, K. Y., Xie, J., and Jin, Y.
(2022). Spatio-temporal wind speed prediction based on clayton c opula func-
tion with deep learning fusion. Renewable Energy , 192:526–536.
Joe, H. (2014). Dependence Modeling with Copulas . CRC Press.
Katz, D. M., Bommarito, M. J., Gao, S., and Arredondo, P. (2023). G PT-4
passes the bar exam. Available at SSRN 4389233 .
Lample, G. and Charton, F. (2019). Deep learning for symbolic math ematics.
arXiv preprint arXiv:1912.01412 .
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ra-
masesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. (2022 ).
Solvingquantitativereasoningproblemswithlanguagemodels. arXiv preprint
arXiv:2206.14858 .
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles,
T., Keeling, J., Gimeno, F., Dal Lago, A., et al. (2022). Competition-lev el
code generation with alphacode. Science, 378(6624):1092–1097.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., and Stoyanov, V. (2019). RoBERTa: A robustly o ptimized
bert pretraining approach. arXiv preprint arXiv:1907.11692 .
Maddigan, P. and Susnjak, T. (2023). Chat2vis: Generating data visualisations
via natural language using chatgpt, codex and gpt-3 large languag e models.
arXiv preprint arXiv:2302.02094 .
Marshall, A. W. and Olkin, I. (1988). Families of multivariate distributio ns.
Journal of the American Statistical Association , 83(403):834–841.
31

--- PAGE 32 ---
McNeil, A., Frey, R., and Embrechts, P. (2015). Quantitative risk management:
Concepts, techniques and tools . Princeton university press.
Michimae, H. and Emura, T. (2022). Likelihood inference for copula m odels
based on left-truncated and competing risks data from ﬁeld studie s.Mathe-
matics, 10(13):2163.
Nelsen, R. B. (2006). An Introduction to Copulas . Springer-Verlag, 2nd edition.
OpenAI (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774 .
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P.,
Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training lan-
guage models to follow instructions with human feedback. arXiv preprint
arXiv:2203.02155 .
Peng, S., Yuan, K., Gao, L., and Tang, Z. (2021). Mathbert: A pre-
trained model for mathematical formula understanding. arXiv preprint
arXiv:2105.00377 .
Schellhase, C. and Spanhel, F. (2018). Estimating non-simpliﬁed vine copulas
using penalized splines. Statistics and Computing , 28:387–409.
Sklar, A. (1959). Fonctions de r´ epartition a n dimensions et leurs m arges.
Publications de l’Institut Statistique de l’Universit´ e d e Paris, 8:229–231.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radfo rd, A.,
Amodei, D., and Christiano, P.F. (2020). Learningto summarizewith human
feedback. Advances in Neural Information Processing Systems , 33:3008–3021.
Williams, L. (2001). Integrating pair programming into a software de velopment
process. In Proceedings 14th Conference on Software Engineering Educa -
tion and Training. ’In search of a software engineering prof ession’ (Cat. No.
PR01059) , pages 27–36. IEEE.
32
