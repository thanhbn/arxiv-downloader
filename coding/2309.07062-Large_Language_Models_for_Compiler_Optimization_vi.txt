# 2309.07062.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2309.07062.pdf
# Kích thước tệp: 1205118 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mô hình Ngôn ngữ Lớn cho Tối ưu hóa Trình biên dịch
Chris Cummins†∗, Volker Seeker†, Dejan Grubisic†,
Mostafa Elhoushi, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle,
Kim Hazelwood, Gabriel Synnaeve, Hugh Leather†
Meta AI Youwei Liang
UC San Diego

Tóm tắt —Chúng tôi khám phá ứng dụng mới lạ của Mô hình Ngôn ngữ Lớn trong tối ưu hóa mã. Chúng tôi trình bày một mô hình transformer 7B tham số được đào tạo từ đầu để tối ưu hóa assembly LLVM cho kích thước mã. Mô hình nhận đầu vào là assembly chưa tối ưu và xuất ra danh sách các tùy chọn trình biên dịch để tối ưu hóa chương trình tốt nhất. Quan trọng là, trong quá trình đào tạo, chúng tôi yêu cầu mô hình dự đoán số lượng lệnh trước và sau tối ưu hóa, cũng như chính mã đã tối ưu. Những nhiệm vụ học tập phụ trợ này cải thiện đáng kể hiệu suất tối ưu hóa của mô hình và nâng cao độ sâu hiểu biết của mô hình.

Chúng tôi đánh giá trên một bộ test lớn các chương trình. Phương pháp của chúng tôi đạt được cải thiện 3.0% trong việc giảm số lượng lệnh so với trình biên dịch, vượt trội hai baseline tiên tiến yêu cầu hàng nghìn lần biên dịch. Hơn nữa, mô hình cho thấy khả năng lý luận mã mạnh mẽ một cách đáng ngạc nhiên, tạo ra mã có thể biên dịch được 91% thời gian và hoàn toàn mô phỏng đầu ra của trình biên dịch 70% thời gian.

I. GIỚI THIỆU
Có sự quan tâm ngày càng tăng đối với Mô hình Ngôn ngữ Lớn (LLM) cho các lĩnh vực kỹ thuật phần mềm như tạo mã [1–9], dịch mã [10–12], và kiểm thử mã [13–15]. Các mô hình như Code Llama [9], Codex [8], và ChatGPT [16] có hiểu biết thống kê tốt về mã và đề xuất các phần hoàn thiện có khả năng cho mã chưa hoàn thành, khiến chúng hữu ích cho việc chỉnh sửa và tạo phần mềm. Tuy nhiên, có vẻ như chúng chưa được đào tạo cụ thể để tối ưu hóa mã. Ví dụ, ChatGPT sẽ thực hiện các điều chỉnh nhỏ cho chương trình như gắn thẻ biến để lưu trữ dưới dạng thanh ghi, và thậm chí sẽ thử các tối ưu hóa đáng kể hơn như vector hóa, mặc dù nó dễ bị nhầm lẫn và mắc lỗi, thường dẫn đến mã không chính xác.

Các nghiên cứu trước về tối ưu hóa mã được hướng dẫn bởi machine learning đã sử dụng các đặc trưng được xây dựng thủ công [17–19], đến các mạng neural đồ thị (GNN) [20, 21]. Tuy nhiên, trong tất cả các trường hợp, cách thể hiện chương trình đầu vào cho thuật toán machine learning là không đầy đủ, mất một số thông tin trên đường đi. Ví dụ, MLGO [17] sử dụng các đặc trưng số để cung cấp gợi ý cho inline hàm, nhưng không thể tái tạo trung thực đồ thị gọi hàm hoặc luồng điều khiển, v.v. PrograML [21] tạo đồ thị của chương trình để truyền cho GNN, nhưng nó loại trừ các giá trị cho hằng số và một số thông tin kiểu dữ liệu ngăn cản việc tái tạo lệnh với độ chính xác.

Trong nghiên cứu này, chúng tôi đặt câu hỏi: liệu Mô hình Ngôn ngữ Lớn có thể học cách tối ưu hóa mã không? LLM có thể chấp nhận các chương trình nguồn, như vốn có, với biểu diễn hoàn chỉnh, không mất mát. Sử dụng văn bản làm biểu diễn đầu vào và đầu ra cho một bộ tối ưu machine learning có những thuộc tính mong muốn: văn bản là giao diện phổ quát, di động và dễ tiếp cận, và không giống các phương pháp trước đây không chuyên biệt cho bất kỳ nhiệm vụ cụ thể nào.

Chúng tôi bắt đầu điều tra về sức mạnh tối ưu hóa mã của LLM bằng cách tái tạo các phép biến đổi tối ưu hóa có trong trình biên dịch, nhắm đến trình biên dịch chuẩn công nghiệp LLVM [22]. Bộ tối ưu của LLVM cực kỳ phức tạp và chứa hàng nghìn quy tắc, thuật toán và heuristic trong hơn 1M dòng mã C++. Kỳ vọng của chúng tôi là mặc dù LLM đã cho thấy tiến bộ lớn trong các nhiệm vụ dịch ngôn ngữ tự nhiên và tạo mã, chúng sẽ không thể mô phỏng một hệ thống phức tạp như vậy. Hiểu và áp dụng các tối ưu hóa trình biên dịch đòi hỏi nhiều cấp độ lý luận, khả năng tính toán số học, và áp dụng các thuật toán cấu trúc dữ liệu và đồ thị phức tạp, đây là những khả năng mà LLM đã cho thấy thiếu hụt [23, 24].

Chúng tôi nghĩ đây sẽ là một bài báo về những thất bại rõ ràng của LLM sẽ làm động lực cho những ý tưởng thông minh trong tương lai để khắc phục những thất bại đó. Chúng tôi hoàn toàn bất ngờ khi phát hiện rằng trong nhiều trường hợp, một LLM được đào tạo đầy đủ không chỉ có thể dự đoán các tối ưu hóa tốt nhất để áp dụng cho mã đầu vào, mà nó còn có thể trực tiếp thực hiện các tối ưu hóa mà không cần dựa vào trình biên dịch!

Phương pháp của chúng tôi đơn giản. Chúng tôi bắt đầu với kiến trúc LLM 7B tham số, lấy từ LLaMa 2 [25], và khởi tạo nó từ đầu. Sau đó chúng tôi đào tạo nó trên hàng triệu ví dụ assembly LLVM, kết hợp với các tùy chọn trình biên dịch tốt nhất được tìm thấy bởi tìm kiếm cho mỗi assembly, cũng như assembly kết quả từ việc thực hiện những tối ưu hóa đó. Chỉ từ những ví dụ này, mô hình học cách tối ưu hóa mã với độ chính xác đáng chú ý.

Đóng góp duy nhất của chúng tôi là ứng dụng đầu tiên của LLM để tối ưu hóa mã. Chúng tôi xây dựng LLM chỉ nhằm mục đích tối ưu hóa trình biên dịch và cho thấy chúng đạt được cải thiện 3.0% trong việc giảm kích thước mã với một lần biên dịch so với trình biên dịch, so với phương pháp dựa trên tìm kiếm đạt 5.0% với 2.5e9 lần biên dịch và so với các phương pháp ML tiên tiến nhất gây hồi quy và yêu cầu hàng nghìn lần biên dịch. Chúng tôi cung cấp các thí nghiệm phụ trợ và ví dụ mã để đặc trưng hóa thêm tiềm năng và giới hạn của LLM cho lý luận mã. Nhìn chung, chúng tôi thấy hiệu quả của chúng đáng chú ý và nghĩ rằng những kết quả này sẽ được cộng đồng quan tâm. arXiv:2309.07062v1 [cs.PL] 11 Sep 2023

--- TRANG 2 ---
Hình 1: Tổng quan về phương pháp của chúng tôi, hiển thị đầu vào mô hình (Prompt) và đầu ra (Answer) trong quá trình đào tạo và suy luận. Prompt chứa mã chưa tối ưu. Answer chứa danh sách pass tối ưu hóa, số lượng lệnh, và mã đã tối ưu. Trong quá trình suy luận, chúng tôi chỉ tạo danh sách pass tối ưu hóa để đưa vào trình biên dịch, đảm bảo mã đã tối ưu là chính xác.

Bảng I: Dữ liệu đào tạo. Mỗi hàm LLVM-IR được autotuned và sử dụng để tạo cặp (Prompt, Answer). Cột ntokens hiển thị số token khi prompt được mã hóa bằng tokenizer Llama 2 [25].

n functions | unoptimized instruction count | size on disk | ntokens
Handwritten | 610,610 | 8,417,799 | 653.5 MB | 214,746,711
Synthetic | 389,390 | 13,775,149 | 352.3 MB | 158,435,151
Total | 1,000,000 | 16,411,249 | 1.0 GB | 373,181,862

II. SẮP XẾP PASS VỚI LLM

Trong nghiên cứu này, chúng tôi nhắm đến sắp xếp pass của trình biên dịch. Nhiệm vụ sắp xếp pass là chọn từ tập hợp các pass biến đổi tối ưu hóa có sẵn trong trình biên dịch danh sách các pass sẽ tạo ra kết quả tốt nhất cho một mã đầu vào cụ thể. Thao tác thứ tự pass đã được chứng minh có tác động đáng kể đến cả hiệu suất runtime và kích thước mã [19, 26].

Các phương pháp machine learning cho nhiệm vụ này đã cho thấy kết quả tốt trước đây, nhưng gặp khó khăn trong việc tổng quát hóa qua các chương trình khác nhau [27]. Các nghiên cứu trước thường cần biên dịch các chương trình mới hàng chục hoặc hàng trăm lần để thử các cấu hình khác nhau và tìm ra tùy chọn hoạt động tốt nhất, khiến chúng không thực tế cho sử dụng thực tế. Chúng tôi giả định rằng một mô hình ngôn ngữ lớn với khả năng lý luận đầy đủ sẽ có thể học cách đưa ra quyết định tối ưu hóa tốt mà không cần điều này.

Hầu hết nghiên cứu trước về LLM cho mã hoạt động trên các ngôn ngữ nguồn như Python. Thay vào đó, cho bài toán sắp xếp pass, chúng tôi yêu cầu lý luận ở cấp độ thấp hơn của assembly trình biên dịch, được gọi là Biểu diễn Trung gian (IR).

Bảng II: Dữ liệu test.
n functions | unoptimized instruction count | -Oz instruction count
AI-SOCO [31] | 8,929 | 97,800 | 47,578
ExeBench [32] | 26,806 | 386,878 | 181,277
POJ-104 [33] | 310 | 8,912 | 4,492
Transcoder [12] | 17,392 | 289,689 | 129,611
CSmith [34] | 33,794 | 647,815 | 138,276
YARPGen [35] | 12,769 | 285,360 | 144,539
Total | 100,000 | 1,716,354 | 645,773

Mặc dù tồn tại các tập dữ liệu được tuyển chọn của ngôn ngữ nguồn để pretrain LLM (ví dụ [28–30]), IR trình biên dịch không tạo thành một phần đáng kể của các tập dữ liệu này, và mặc dù các mô hình như ChatGPT cho thấy một số triển vọng hiểu biết, khả năng lý luận về IR của chúng kém hơn nhiều so với ngôn ngữ nguồn.

Chúng tôi nhắm đến tối ưu hóa thứ tự pass LLVM cho kích thước mã như trong các nghiên cứu trước [17, 27], sử dụng số lượng lệnh IR làm proxy (không hoàn hảo) cho kích thước binary. Phương pháp này bất khả tri với trình biên dịch và metric tối ưu hóa được chọn, và chúng tôi dự định nhắm đến hiệu suất runtime trong tương lai. Hiện tại, tối ưu hóa cho kích thước mã đơn giản hóa việc thu thập dữ liệu đào tạo.

A. Prompts

Chúng tôi trình bày mô hình với LLVM-IR chưa tối ưu (như được phát ra bởi frontend clang) và yêu cầu nó tạo ra danh sách các pass tối ưu hóa nên được áp dụng cho nó. Hình 1 hiển thị định dạng của prompt đầu vào và văn bản đầu ra.

Trong nghiên cứu này, chúng tôi nhắm đến LLVM 10 và sử dụng các cờ tối ưu hóa từ opt. Có 122 pass tối ưu hóa để chọn

--- TRANG 3 ---
lựa và các pass có thể được chọn nhiều hơn một lần trong một chuỗi duy nhất. Chúng tôi cũng bao gồm 6 meta-flags (-O0, -O1, -O2, -O3, -Oz, và -Os) mà mỗi cái chỉ có thể xuất hiện một lần mỗi danh sách pass. Danh sách pass có thể có bất kỳ độ dài nào, mặc dù trong các thí nghiệm của chúng tôi, chúng tôi thấy thường lên đến 9 pass, cho không gian tìm kiếm tổ hợp khoảng 10^18.

Như hiển thị trong Hình 1, chúng tôi cũng bao gồm hai nhiệm vụ phụ trợ: i) tạo số lượng lệnh của mã trước và sau khi áp dụng tối ưu hóa và ii) tạo IR đầu ra sau khi áp dụng tối ưu hóa. Chúng tôi giả định rằng những điều này sẽ cho phép quyết định sắp xếp pass tốt hơn bằng cách buộc hiểu sâu về cơ chế tối ưu hóa mã. Chúng tôi xác minh điều này thực nghiệm trong Phần V-B.

Mặc dù mô hình được đào tạo để tạo số lượng lệnh và IR đã tối ưu, chúng tôi không cần những nhiệm vụ phụ trợ đó để triển khai. Tất cả những gì chúng tôi cần làm là tạo danh sách pass mà sau đó chúng tôi thực thi bằng trình biên dịch. Do đó chúng tôi tránh được các vấn đề về tính đúng đắn làm phiền các kỹ thuật yêu cầu đầu ra của mô hình phải đáng tin cậy [10–12, 36].

B. Chuẩn hóa LLVM-IR

Chúng tôi chuẩn hóa LLVM-IR được sử dụng để đào tạo LLM bằng các quy tắc sau: chúng tôi loại bỏ comments, debug metadata và attributes, và đảm bảo khoảng trắng nhất quán bằng cách đưa IR qua một lexer tùy chỉnh giữ lại newlines nhưng chuẩn hóa khoảng trắng khác và loại bỏ indentation. Chúng tôi làm điều này để giảm độ dài của LLVM-IR để tận dụng tối đa kích thước đầu vào hạn chế của LLM (Phần III-A). Mã trong Hình 1 đã được xử lý theo cách này.

III. MÔ HÌNH

Chúng tôi sử dụng kiến trúc transformer phổ biến [37]. Transformer là một mạng neural nhân tạo sử dụng self-attention trên một cửa sổ ngữ cảnh có kích thước cố định.

Văn bản đầu vào đầu tiên được tokenize thành các từ và đơn vị từ phụ. Chúng được nhúng vào các biểu diễn vector liên tục và cung cấp làm đầu vào cho encoder của transformer, nơi các cơ chế self-attention nắm bắt mối quan hệ ngữ cảnh giữa các token để khuyến khích mô hình hiểu và xử lý cấu trúc ngữ nghĩa của văn bản đầu vào.

Văn bản đầu ra được tạo ra bằng cách tạo từng token một cách lặp lại. Decoder nhận đầu vào đã được mã hóa cùng với bất kỳ token nào được tạo trước đó và sử dụng self-attention để dự đoán token tiếp theo trong chuỗi. Chúng tôi lấy mẫu greedy trong quá trình giải mã để chọn chuỗi token có khả năng nhất. Quá trình này tiếp tục cho đến khi một token kết thúc chuỗi được tạo ra hoặc đạt đến độ dài tối đa được định trước.

A. Kiến trúc Mô hình

Chúng tôi sử dụng cùng kiến trúc mô hình và tokenizer Byte Pair Encoding (BPE) [38] như Llama 2 [25], nhưng đào tạo mô hình của chúng tôi từ đầu. Chúng tôi sử dụng cấu hình nhỏ nhất của Llama 2: 32 attention heads, 4,096 hidden dimensions, và 32 layers, tổng cộng 7B tham số.

Độ dài tối đa của cặp (prompt, answer) được định nghĩa bởi độ dài chuỗi. Trong nghiên cứu này, chúng tôi sử dụng độ dài chuỗi

(a) Hiệu suất của danh sách pass được tạo.
(b) Độ chính xác trong dự đoán số lượng lệnh.
(c) Metrics mã được tối ưu hóa bởi mô hình.

Hình 2: Hiệu suất trên tập validation holdout trong quá trình đào tạo. Chúng tôi đánh giá hiệu suất mỗi 250 bước đào tạo (131M token train). Bằng với -Oz đạt được ở 393M token và hiệu suất đỉnh ở 10.9B token.

2,048 token. Tokenizer Llama 2 đạt trung bình 2.02 ký tự mỗi token khi mã hóa LLVM-IR, vậy điều này cung cấp giới hạn trên xấp xỉ về LLVM-IR dài nhất chúng tôi có thể đào tạo ở 2KB (vì 2KB prompt và 2KB answer ≈ 2,048 token).

B. Dữ liệu Đào tạo

Chúng tôi tập hợp một corpus lớn các hàm LLVM-IR chưa tối ưu, được tóm tắt trong Bảng I. Chúng tôi trích xuất các hàm từ tập dữ liệu mã C/C++ viết tay có sẵn công khai và bổ sung điều này với mã tổng hợp được tạo bởi các trình tạo test trình biên dịch C/C++. Tổng cộng, corpus đào tạo của chúng tôi bao gồm 1,000,000 hàm IR được khử trùng, tổng cộng 373M token đào tạo. Chúng tôi hoạt động ở cấp độ hàm IR riêng lẻ thay vì toàn bộ modules để tối đa hóa lượng dữ liệu chúng tôi có thể vừa trong độ dài chuỗi 2,048-token.

Để tìm danh sách các pass tối ưu hóa sẽ tạo ra số lượng lệnh nhỏ nhất, chúng tôi sử dụng autotuning. Autotuner của chúng tôi kết hợp tìm kiếm ngẫu nhiên và broadcasting kết quả all-to-all giữa các hàm, lấy cảm hứng từ công trình của Liang et. al. [20].

--- TRANG 4 ---
Bảng III: Hiệu suất của các phương pháp khác nhau để sắp xếp pass trên tập test các hàm LLVM-IR chưa thấy từ Bảng II. Tất cả metrics đều w.r.t. -Oz. Instructions saved được tổng qua các hàm được cải thiện và instructions regressed được tổng qua các hàm bị hồi quy. Overall improvement là tổng số instruction count savings w.r.t -Oz. Autotuner đạt hiệu suất tốt nhất nhưng yêu cầu 2.5B lần biên dịch bổ sung (949 CPU-days). Phương pháp của chúng tôi đạt 60% lợi ích của autotuner mà không gọi trình biên dịch một lần nào.

additional compilations | functions improved | functions regressed | instructions saved | instructions regressed | overall improvement
Autotuner | 2,522,253,069 | 6,764 | 0 | 30,948 | 0 | 5.03%
AutoPhase [39] | 4,500,000 | 1,558 | 8,400 | 6,522 | 32,357 | -3.85%
Coreset-NVP [20] | 442,747 | 3,985 | 6,072 | 16,064 | 28,405 | -1.88%
Our Approach | 0 | 4,136 | 526 | 21,935 | 3,095 | 3.01%

Bảng IV: Mở rộng các mô hình trong Bảng III với "-Oz backup". Nếu mô hình dự đoán danh sách pass khác -Oz, nó cũng đánh giá -Oz và chọn tốt nhất. Điều này ngăn hồi quy w.r.t -Oz với chi phí lần biên dịch bổ sung.

additional compilations | overall improvement
AutoPhase [39] | 4,600,000 | 1.02%
Coreset-NVP [20] | 542,747 | 2.55%
Our Approach | 5,721 | 3.52%

Đối với mỗi hàm, chúng tôi chạy tìm kiếm ngẫu nhiên trong một khoảng thời gian cố định (780 giây) và sau đó tối thiểu hóa danh sách pass tốt nhất bằng cách lặp loại bỏ các pass riêng lẻ được chọn ngẫu nhiên để xem liệu chúng có đóng góp vào số lượng lệnh không. Nếu không, chúng được loại bỏ. Sau khi thực hiện điều này trên từng hàm, chúng tôi tổng hợp tập hợp các danh sách pass tốt nhất duy nhất và broadcast chúng qua tất cả các hàm khác. Do đó, nếu một danh sách pass được tìm thấy hoạt động tốt trên một hàm, nó được thử trên tất cả các hàm khác.

Tổng cộng, autotuner biên dịch mỗi chương trình đào tạo trung bình 37,424 lần, đạt được cải thiện 5.8% trong giảm số lượng lệnh so với baseline sắp xếp pass cố định trong trình biên dịch được cung cấp bởi -Oz. Đối với mục đích của chúng tôi, autotuning này phục vụ như một tiêu chuẩn vàng cho việc tối ưu hóa mỗi hàm. Mặc dù việc tiết kiệm số lượng lệnh được phát hiện bởi autotuner là đáng kể, chi phí tính toán để đạt được những thắng lợi này là 9,016 CPU days. Mục tiêu của nghiên cứu này là đạt được một phần hiệu suất của autotuner bằng cách sử dụng mô hình dự đoán không yêu cầu chạy trình biên dịch hàng nghìn lần.

C. Đào tạo

Bắt đầu từ trọng số được khởi tạo ngẫu nhiên, chúng tôi đào tạo mô hình trong 30,000 bước trên 64 V100 với tổng thời gian đào tạo 620 GPU days. Chúng tôi sử dụng optimizer AdamW [40] với giá trị β1 và β2 là 0.9 và 0.95. Chúng tôi sử dụng lịch học tập cosine với 1,000 bước warm-up, learning rate đỉnh 1e−5, và learning rate cuối cùng 1/10 của đỉnh. Chúng tôi sử dụng batch size 256 và mỗi batch chứa 524,288 token với tổng 15.7B token đào tạo. Toàn bộ 30,000 bước đào tạo là 7.7 epochs (lần lặp qua corpus đào tạo).

Trong quá trình đào tạo, chúng tôi đánh giá mô hình trên tập validation holdout gồm 1,000 IR chưa thấy được xử lý theo cách tương tự như tập đào tạo. Chúng tôi đánh giá mỗi 250 bước.

IV. ĐÁNH GIÁ

Trong phần này, chúng tôi đánh giá khả năng của mô hình tạo danh sách pass cho mã chưa thấy và thực hiện tối ưu hóa đúng.

A. Kết quả Đào tạo

Hình 2 hiển thị hiệu suất trong quá trình đào tạo khi được đánh giá trên tập validation holdout gồm 1,000 hàm LLVM-IR chưa thấy. Hiệu suất validation đỉnh được mô hình đạt tại 10.9B token đào tạo.

Ở hiệu suất đỉnh, mã được tối ưu hóa bằng cách sử dụng chuỗi pass do mô hình tạo chứa ít hơn 4.4% lệnh so với khi được tối ưu hóa bằng sắp xếp pass tích hợp của trình biên dịch (-Oz). Autotuner đạt được giảm số lượng lệnh lớn hơn là 5.6%, nhưng điều này yêu cầu 27 triệu lần biên dịch của tập validation. Mô hình đưa ra dự đoán của nó mà không gọi trình biên dịch một lần nào.

Hình 2b hiển thị lỗi của số lượng lệnh đầu vào và đầu ra được dự đoán. Dự đoán số lượng lệnh cho mã chưa tối ưu nhanh chóng tiến đến độ chính xác gần hoàn hảo. Dự đoán số lượng lệnh đầu ra tỏ ra thách thức hơn, đạt Mean Average Percentage Error (MAPE) 5.9%.

Hình 2c đánh giá chất lượng của mã được tạo bằng ba metrics. Điểm BLEU [41] hiển thị sự tương tự giữa mã do mô hình tạo và mã ground-truth tham chiếu được tạo bởi trình biên dịch sử dụng danh sách pass được tạo. Code compiles là tần suất mã do mô hình tạo biên dịch mà không có lỗi. Exact match theo dõi tần suất mã do mô hình tạo khớp từng ký tự của mã do trình biên dịch tạo khi được tối ưu hóa bằng danh sách pass được tạo (tức là bao nhiêu lần BLEU=1).

Ở hiệu suất đỉnh, mô hình đạt tỷ lệ tạo mã biên dịch mà không có lỗi ấn tượng 90.5%. Hơn nữa, điểm BLEU 0.952 cho thấy mã được tối ưu hóa bởi mô hình xấp xỉ chặt chẽ của trình biên dịch, và tần suất exact match là 70%. Để so sánh, một baseline đơn giản sao chép mã chưa tối ưu ra đầu ra sẽ đạt điểm BLEU 0.531 và tần suất exact match 0%, chứng minh rằng thao tác đáng kể của mã đầu vào là cần thiết để đạt điểm số cao như vậy.

Đến cuối đào tạo, hiệu suất trên tập validation đã đạt ngưỡng. Chúng tôi sử dụng checkpoint hoạt động tốt nhất và chuyển sang đánh giá quy mô lớn hơn 100× cho phần còn lại của đánh giá.

B. So sánh với State-of-the-Art

Trong thí nghiệm này, chúng tôi thực hiện đánh giá quy mô lớn về khả năng của LLM dự đoán danh sách pass so với các baseline.

--- TRANG 5 ---
Hình 3: Tần suất các pass xuất hiện trong danh sách pass cho mỗi chương trình test 100,000 (trái), và độ dài của danh sách pass (phải). -Oz là điểm bắt đầu cho autotuner và là kết quả chủ đạo, là kết quả tốt nhất được tìm thấy cho 93.2% chương trình test autotuned và xuất hiện thêm trong 0.6% danh sách pass như một phần của chuỗi dài hơn. Phân phối pass do mô hình tạo theo dõi autotuner nhưng hơi dự đoán quá -Oz (94.3%) và bao gồm 9 pass mà autotuner sử dụng trên tập đào tạo nhưng không trên tập test. Kết quả được sắp xếp theo tần suất autotuner giảm dần.

define i32 @f1(i8 %0) {
%2 = alloca i32, align 4
%3 = alloca i8, align 1
store i8 %0, i8 *%3, align 1
%4 = load i8, i8 *%3, align 1
%5 = zext i8 %4 to i32
%6 = icmp sge i32 %5, 65
br i1 %6, label %7, label %15
7:
%8 = load i8, i8 *%3, align 1
%9 = zext i8 %8 to i32
%10 = icmp sle i32 %9, 90
br i1 %10, label %11, label %15
11:
%12 = load i8, i8 *%3, align 1
%13 = zext i8 %12 to i32
<snip 21 lines...>
33:
%34 = load i32, i32 *%2, align 4
ret i32 %34
}

(a) Mã đầu vào (39 lệnh).

define i32 @f1(i8 %0) {
%2 = zext i8 %0 to i32
%.off = add i8 %0, 191
%3 = icmp ult i8 %.off, 26
br i1 %3, label %4, label %6
4:
%5 = add nsw i32 %2, 191
br label %10
6:
%.reload16.off = add nsw i32 %2, 159
%7 = icmp ult i32 %.reload16.off, 26
br i1 %7, label %10, label %8
8:
%9 = icmp eq i8 %0, 32
%. = select i1 %9, i32 26, i32 1
br label %10
10:
%.0.reg2mem.0 = phi i32 [%5, %4],
[%., %8], [%.reload16.off, %6]
ret i32 %.0.reg2mem.0
}

(b) Mã autotuned (14 lệnh) sử dụng passes:
-reg2mem -instcombine -Os -O1.

define i32 @f1(i8 %0) {
%2 = zext i8 %0 to i32
%.off = add i8 %0, 191
%3 = icmp ult i8 %.off, 26
br i1 %3, label %6, label %._crit_edge
._crit_edge:
%.off24 = add i8 %0, 159
%4 = icmp ult i8 %.off24, 26
br i1 %4, label %6, label %._crit_edge9
._crit_edge9:
%5 = icmp eq i8 %0, 32
%spec.select = select i1 %5,
i32 26, i32 1
ret i32 %spec.select
6:
%.sink = phi i32 [191, %1],
[159, %._crit_edge]
%7 = add nsw i32 %.sink, %2
ret i32 %7
}

(c) Mã được tối ưu hóa bởi mô hình (13 lệnh) và
danh sách pass: -reg2mem -simplifycfg -mem2reg
-jump-threading -Os.

Listing 1: Một ví dụ hàm IR nơi mô hình đề xuất danh sách pass tốt hơn autotuner, mặc dù chưa bao giờ thấy mã này trước đây. Đối với hàm này, autotuner đã thử 26k thứ tự pass khác nhau. Danh sách pass được tạo bởi mô hình xuất hiện 5 lần trong tập đào tạo 1,000,000 ví dụ.

Datasets Chúng tôi tổng hợp một bộ dataset benchmark rộng cho đánh giá, được tóm tắt trong Bảng II. Chúng tôi khử trùng và loại trừ các hàm IR giống hệt với những hàm chúng tôi đã đào tạo. Dữ liệu test của chúng tôi bao gồm mã từ nhiều lĩnh vực khác nhau bao gồm các cuộc thi lập trình (AI-SOCO [31], POJ-104 [33]), trình tạo test case trình biên dịch (CSmith [34], YARPGen [35]), và mã có sẵn công khai khác (ExeBench [32], Transcoder [12]).

Baselines Chúng tôi so sánh phương pháp của mình với ba baseline: AutoPhase [39], Coreset-NVP [20], và Autotuner.

AutoPhase [39] là phương pháp reinforcement learning trong đó agent được đào tạo bằng Proximal Policy Optimization [42] để chọn chuỗi các pass tối ưu hóa sẽ tối đa hóa tổng tiết kiệm số lượng lệnh trong một episode có độ dài cố định. Ở mỗi bước, chương trình được tối ưu hóa được thể hiện cho agent dưới dạng vector 56 chiều của số lượng lệnh và các thuộc tính khác. Chúng tôi tái tạo môi trường của [39] nhưng sử dụng triển khai và chế độ đào tạo mở rộng từ [27] trong đó agent được đào tạo cho 100,000 episodes. Chúng tôi đào tạo agent trên cùng dữ liệu như mô hình ngôn ngữ của chúng tôi (Bảng I) và đánh giá hiệu suất agent định kỳ trong quá trình đào tạo trên tập validation holdout. Như trong các nghiên cứu trước, chúng tôi sử dụng action space và episode length là 45.

Coreset-NVP [20] là kỹ thuật kết hợp tìm kiếm lặp với mô hình cost đã học. Đầu tiên, tìm kiếm greedy được chạy trên 17,500 benchmarks để xác định Core set của danh sách pass tốt nhất. Sau đó Neural Value Prediction (NVP) được đào tạo trên kết quả của tìm kiếm này, sử dụng đồ thị ProGraML [21] được xử lý bởi Graph Convolutional Network làm biểu diễn chương trình. Tại suy luận, Coreset-NVP dự đoán reward chuẩn hóa và thử một vài chuỗi pass đầu tiên với reward chuẩn hóa cao nhất. Tổng số pass nó được phép thử cho mỗi benchmark là 45, theo các nghiên cứu trước. Chúng tôi sử dụng trọng số mô hình do tác giả cung cấp để thực hiện suy luận trên tập test của chúng tôi.

Cuối cùng, chúng tôi so sánh nó với Autotuner mà chúng tôi đã sử dụng để tạo dữ liệu đào tạo. Chúng tôi autotuned dataset test theo cách tương tự như dữ liệu đào tạo, được mô tả trong Phần III-B.

Results Bảng III tóm tắt kết quả. Phương pháp của chúng tôi vượt trội -Oz, AutoPhase, và Coreset-NVP trên tất cả datasets. Nhìn chung, hàng nghìn nỗ lực tối ưu hóa được cung cấp cho autotuner cho phép nó khám phá danh sách pass hoạt động tốt nhất.

AutoPhase và Coreset-NVP đều có thể xác định danh sách pass vượt trội -Oz nhưng có tác động âm tổng thể lên số lượng lệnh do số lượng lớn hồi quy. Chúng tôi đề xuất một phần mở rộng "-Oz backup" đơn giản để khắc phục điều này: nếu mô hình dự đoán danh sách pass khác -Oz, chúng tôi cũng chạy -Oz và chọn tốt nhất trong hai tùy chọn. Điều này ngăn hồi quy w.r.t. -Oz, nhưng tăng số lần biên dịch bổ sung bằng số lần mô hình dự đoán danh sách pass khác -Oz.

Bảng IV hiển thị kết quả của các kỹ thuật khi được đánh giá theo cách này. Mặc dù điều này không giúp các mô hình tìm thêm cải thiện, việc thiếu hồi quy có nghĩa là AutoPhase và Coreset-NVP giờ đạt được cải thiện tổng thể so với -Oz, mặc dù vẫn ít hơn LLM có hoặc không có -Oz backup.

C. Đánh giá Danh sách Pass được Tạo

Hình 3 hiển thị tần suất các pass được chọn bởi autotuner và mô hình của chúng tôi từ thí nghiệm trước. Phân phối các pass được chọn bởi mô hình theo dõi rộng rãi autotuner. -Oz là pass tối ưu thường xuyên nhất. Loại trừ -Oz, danh sách pass do mô hình tạo có độ dài trung bình 3.4 (max 10), và danh sách pass autotuner có độ dài trung bình 3.1 (max 9). 105 danh sách pass được tạo bởi mô hình không bao giờ xuất hiện trong dữ liệu đào tạo.

Trong 710 trường hợp, danh sách pass do mô hình tạo vượt trội autotuner trên tập test, mặc dù cải thiện thường nhỏ. Listing 1 hiển thị ví dụ nơi danh sách pass do mô hình tạo

Hình 5: Cải thiện so với -Oz theo kích thước đầu vào. Mã lớn hơn tối ưu hóa nhiều hơn.

Bảng V: Lỗi trình biên dịch của mã được tối ưu hóa bởi mô hình trên 100,000 đầu vào chưa thấy.

error category | n
type error | 5,777
instruction forward referenced | 1,521
undefined value | 1,113
invalid redefinition | 616
syntax error | 280
invalid value for constant | 144
undefined function | 112
index error | 98
other | 83
Total | 9,744

đơn giản hóa luồng điều khiển thành ít blocks hơn, tiết kiệm thêm một lệnh.

Hình 4 phân tích cải thiện của mỗi phương pháp sắp xếp pass theo dataset benchmark. Cải thiện lớn nhất so với -Oz được tìm thấy trong datasets POJ-104 và Transcoder, cả hai đều tổng hợp một lượng lớn mã viết tay, trong khi YARPGen, một trình tạo chương trình ngẫu nhiên để test trình biên dịch, có ít cơ hội cải thiện so với -Oz nhất.

Chúng tôi phát hiện có mối tương quan mạnh giữa kích thước chương trình đầu vào và cải thiện hiệu suất tiềm năng so với -Oz được tìm thấy bởi cả autotuner và mô hình. Hình 5 vẽ xu hướng này, hiển thị rõ ràng rằng các chương trình lớn hơn có nhiều cơ hội cải thiện so với -Oz hơn.

D. Đánh giá Mã được Tạo

Trong phần này, chúng tôi đánh giá chất lượng mã do mô hình tạo. Để làm điều này, chúng tôi chạy nhiệm vụ đào tạo phụ trợ tạo mã đã tối ưu cho tất cả 100k hàm trong tập test. Lưu ý rằng điều này không cần thiết để tạo danh sách pass được đánh giá trong phần trước. Chúng tôi đã thực hiện các chỉnh sửa nhỏ cho các mẫu mã trong phần này để ngắn gọn như bỏ qua các câu lệnh thừa và rút ngắn tên định danh.

Trong 90.3% trường hợp, IR đã tối ưu do mô hình tạo biên dịch được, và trong 68.4% trường hợp IR đầu ra khớp từng ký tự ground truth được tạo bởi trình biên dịch. Chúng tôi phân loại các lớp lỗi khác nhau cho 9.7% trường hợp nơi IR được tạo không biên dịch được trong Bảng V, và Listing 2 cung cấp ví dụ mã.

--- TRANG 7 ---
error: '%15' defined with type 'i32' but expected 'i1'
%or.cond = or i1 %14, %15

(a) Mô hình định nghĩa %15 như một integer nhưng sau đó thử sử dụng nó như bool (type error).

error: constant expression type mismatch
@.str = private unnamed_addr constant [ 493 x i8 ]
c"<snip 492 chars ...> ", align 1

(b) Mô hình bỏ qua một ký tự duy nhất khi chép một string-literal 493 ký tự từ mã đầu vào (type error).

error: floating point constant invalid for type
%1 = tail call i32 @f1(float -0.47799998483256463 ,
float -1.8159999847412109 )

(c) LLVM yêu cầu giá trị thập phân chính xác cho hằng số floating-point. Những giá trị do mô hình tạo này có thập phân lặp lại trong nhị phân nên bị từ chối (invalid value for constant).

Listing 2: Lỗi trình biên dịch trong mã được tối ưu hóa bởi mô hình.

define hidden signext i8 @f1() #0 {
%1 = alloca i64, align 8
store i64 3718042838174166437 , i64 *%1, align 8
%2 = load i64, i64 *%1, align 8
%3 = trunc i64 %2 to i8
ret i8 %3
}

(a) Mã đầu vào chưa tối ưu.

define hidden signext i8
@f1() #0 {
ret i8 165
}

(b) Mã đã tối ưu mong muốn.

define hidden signext i8
@f1() #0 {
ret i8 1
}

(c) Mã do mô hình tạo.

Listing 3: Ví dụ nơi mô hình tạo mã có thể biên dịch nhưng thất bại trong việc tính toán câu trả lời đúng cho biểu thức số. Tạo ra kết quả đúng cho biểu thức này yêu cầu lý luận toán học không tầm thường.

Run passes -instcombine -simplifycfg to reduce
instruction count from 14to7:
define dso_local i32 @f1(i32 %0) {
%2 = load i64, i64 *getelementptr inbounds(
%struct.t2, %struct.t2 *@gvar, i64 0, i32 0), align 8
%3 = icmp eq i64 %2, 0
%4 = icmp eq i32 %0, 0
%or.cond = or i1 %3, %4
%5 = load i32, i32 *@S64_MAX, align 4
%6 = select i1 %or.cond, i32 %5, i32 %0
ret i32 %6
}

Listing 4: Ví dụ nơi mô hình tạo mã được tối ưu hóa đúng nhưng thất bại trong việc tạo danh sách pass cần thiết để tạo ra mã mong muốn. Mã được tối ưu hóa bởi mô hình và dự đoán số lượng lệnh khớp với hiệu suất của autotuner, nhưng mô hình bỏ qua pass -mem2reg cần thiết để đạt được mã này. Danh sách pass do mô hình tạo cho 10 lệnh thay vì 7.

define i32 @f1(
i32 %0,
i32 %1
) align 2 {
br label %3
3:
%i = phi i32 [%7, %6], [2, %2]
%4 = mul nsw i32 %i, %i
%5 = icmp sgt i32 %4, %1
br i1 %5, label %8, label %6
6:
%7 = add i32 %i, 1
br label %3
8:
ret i32 2
}

(a) Mã đã tối ưu mong muốn.

int f1(int x, int y) {
int i = 2;
while (i *i < y) {
i += 1;
}
return 2;
}

(b) Mã C tương đương (viết tay).

define i32 @f1(
i32 %0,
i32 %1
) align 2 {
ret i32 2
}

(c) Mã được tối ưu hóa bởi mô hình.

Listing 5: Ví dụ về tối ưu hóa không an toàn bởi mô hình. Chương trình đầu vào 33 lệnh (không hiển thị) chứa vòng lặp không phải lúc nào cũng an toàn để tối ưu hóa loại bỏ. Ví dụ, khi y = INT_MAX vòng lặp không bao giờ kết thúc.

Hình 6: Chất lượng mã được tối ưu hóa bởi mô hình như một hàm của hiệu suất danh sách pass được tạo. Chất lượng mã thấp hơn khi danh sách pass hoạt động tệ hơn -Oz. Mã được tối ưu hóa bởi mô hình giống ground truth ít hơn (điểm BLEU thấp hơn), mã ít có khả năng biên dịch hơn, và mô hình gặp khó khăn trong việc ước tính số lượng lệnh (lỗi cao hơn). Error bars hiển thị khoảng tin cậy 95%.

Thách thức nhất để đánh giá là 21.9% trường hợp nơi mã được tối ưu hóa bởi mô hình biên dịch được nhưng không khớp từng ký tự với trình biên dịch. Có hai thách thức: thứ nhất là các metrics precision văn bản như điểm BLEU nhạy cảm với sự khác biệt trong mã như tên biến và thứ tự toán hạng giao hoán không ảnh hưởng đến hành vi của mã. Các công cụ như LLVM-Canon [43] có thể giúp ở đây nhưng đi kèm với bộ nhược điểm riêng. Tuy nhiên, trong nhiều trường hợp, không rõ liệu hành vi của hai IR có giống nhau không, vì vậy thách thức thứ hai chúng tôi đối mặt là đánh giá tương đương ngữ nghĩa. Vì không phải tất cả datasets chúng tôi sử dụng để test đều cung cấp script driver và datasets đầu vào cho mã của chúng, chúng tôi không thể sử dụng kiểm tra tương đương dựa trên thực thi như differential testing [44].

Listing 3 hiển thị ví dụ mã do mô hình tạo có ngữ nghĩa chương trình không chính xác. Ở đây, 8 bit thấp của literal 64-bit được truncate và trả về. Trình biên dịch thực hiện tính toán này và thay thế giá trị đúng. Mô hình

--- TRANG 8 ---
Hình 7: Đào tạo mô hình để dự đoán các pass tối ưu hóa đơn lẻ. Subplot trên đánh giá chất lượng mã được tạo cho pass tương ứng (sắp xếp theo điểm BLEU). Subplot dưới hiển thị tần suất pass tương ứng đóng góp vào cải thiện hoặc hồi quy số lượng lệnh so với -Oz.

Hình 8: Ablating tác động của kích thước dữ liệu đào tạo và nhiệm vụ co-training phụ trợ tạo mã đã tối ưu (ký hiệu No Aux). Kích thước dữ liệu được đo như số ví dụ đào tạo. Đồ thị hiển thị hiệu suất trên tập validation holdout trong quá trình đào tạo.

Bảng VI: Thí nghiệm ablation. Chúng tôi đánh giá tác động của việc thay đổi kích thước dữ liệu đào tạo và đào tạo mô hình để tạo mã đã tối ưu. Chúng tôi đào tạo mỗi mô hình trong 30k bước và báo cáo hiệu suất của checkpoint mô hình tốt nhất trên tập validation holdout gồm 1,000 hàm IR chưa thấy.

n training examples | generate optimized code? | overall improvement
1,000,000 | ✓ | 4.95% (—)
500,000 | ✓ | 3.91% (-21%)
250,000 | ✓ | 3.74% (-24%)
1,000,000 | × | 4.15% (-16%)

nhận ra rằng biểu thức có thể được tính toán tại thời điểm biên dịch nhưng thất bại trong việc tính toán giá trị đúng. Loại lý luận toán học này là điểm yếu đã biết của LLM [24].

Đôi khi mô hình tạo mã được tối ưu hóa đúng nhưng thất bại trong việc tạo danh sách pass cần thiết để đạt được nó. Listing 4 hiển thị một ví dụ như vậy. Một lớp lỗi khác là khi mô hình thực hiện tối ưu hóa không an toàn bằng cách thất bại trong việc phân tích mã đầu vào. Listing 5 hiển thị ví dụ.

Chúng tôi quan sát mối liên hệ thú vị giữa chất lượng danh sách pass và mã đã tối ưu tương ứng, được hiển thị trong Hình 6. Khi mô hình tạo ra danh sách pass hoạt động kém, chất lượng của mã được tạo thấp hơn.

V. THÍ NGHIỆM BỔ SUNG

Trong phần trước, chúng tôi đánh giá hiệu suất của LLM được đào tạo để tối ưu hóa LLVM-IR cho kích thước mã. Trong phần này, chúng tôi xây dựng các mô hình bổ sung để hiểu rõ hơn các thuộc tính của LLM cho tối ưu hóa mã. Tất cả mô hình sử dụng cùng kiến trúc và tham số như trong Phần III.

A. Ablation của Kích thước Dataset

Chúng tôi ablate đóng góp của kích thước dataset bằng cách đào tạo hai mô hình bổ sung và thay đổi lượng dữ liệu đào tạo từ 50% (500k ví dụ) xuống 25% (250k ví dụ) bằng dropout ngẫu nhiên. Hình 8 hiển thị tiến trình trong quá trình đào tạo các mô hình. Đối với kích thước dataset 50% và 25%, các mô hình bắt đầu overfit tập đào tạo sau khoảng 8B token đào tạo. Bảng VI hiển thị hiệu suất đỉnh của mỗi cấu hình. Với 50% và 25% dữ liệu đào tạo, hiệu suất downstream giảm lần lượt 21% và 24%.

B. Ablation của Nhiệm vụ Tối ưu hóa Mã

Chúng tôi đào tạo mô hình để tạo không chỉ danh sách pass mà còn mã đã tối ưu kết quả từ danh sách pass này. Có thể mong đợi điều này làm suy giảm hiệu suất mô hình – không chỉ phải học dự đoán danh sách pass tốt, mà còn cách tạo ra mã được tối ưu hóa đúng, một nhiệm vụ khó hơn. Thực tế, chúng tôi tin điều này quan trọng cho hiệu suất mô hình. Bằng cách buộc LLM học ngữ nghĩa của LLVM-IR, chúng tôi cho phép chúng đưa ra quyết định tối ưu hóa tốt hơn.

Để ablate điều này, chúng tôi đào tạo mô hình chỉ tạo danh sách pass mà không có mã đã tối ưu tương ứng. Chúng tôi giữ nguyên data mix và tất cả tham số khác. Hình 8 và Bảng VI hiển thị rằng không đào tạo mô hình tạo mã đã tối ưu, hiệu suất downstream giảm 16%.

--- TRANG 9 ---
Optimize the following LLVM-IR using -name-anon-globals:
@0 = private
@anon.2ef3bda806391c61822366a2a59f2569.0 = private
@anon.95277a486ffed0b6ba33ab3385b3d7bd.0 = private
,→unnamed_addr constant [14 x i8] c" <snip> ", align 1
define dso_local i32 @f1(i8 *%0) {
%2 = call i32 @f2(i8 *%0, i8 *getelementptr inbounds(
,→[14 x i8], [14 x i8] *
,→@0,
@anon.2ef3bda806391c61822366a2a59f2569.0,
@anon.95277a486ffed0b6ba33ab3385b3d7bd.0,
,→i64 0, i64 0))
ret i32 %2
}

(a) Thất bại do thông tin không đầy đủ. Pass -name-anon-globals sử dụng tên module để tính hash. Thiếu điều này, mô hình tạo hash ngẫu nhiên.

Optimize the following LLVM-IR using -instcombine:
@var_12 = external dso_local global i64, align 8
@var_13 = external dso_local global i32, align 4
@var_14 = external dso_local global i32, align 4
define dso_local void @f1(i64 %arg) {
%tmp = alloca i64, align 8
store i64 %arg, i64 *%tmp, align 8
%tmp1 = load i64, i64 *%tmp, align 8
%tmp2 = sub i64 0, %tmp1
%tmp3 = sub i64 0, %tmp2
store i64 %tmp3, i64 *@var_12, align 8
store i64 %arg, i64 *@var_12, align 8
store i64 0, i64 *@var_12, align 8
store i32 1, i32 *@var_13, align 4
store i32 0, i32 *@var_14, align 4
ret void
}

(b) Thất bại phân tích data-flow. Mô hình loại bỏ đúng các lệnh thừa nhưng thay thế giá trị sai cho biến. Mã được tối ưu hóa bởi mô hình biên dịch được và có điểm BLEU cao, nhưng không chính xác.

Listing 6: Ví dụ thất bại từ thí nghiệm dịch pass. Chúng tôi kết hợp đầu vào mô hình (đỏ), ground-truth (xanh), và văn bản do mô hình tạo (xanh lá) thành một unified diff duy nhất để ngắn gọn. Văn bản đen chung cho cả ba.

C. Đánh giá Dịch Pass Đơn lẻ

Trong các phần trước, chúng tôi đào tạo LLM để điều phối các pass tối ưu hóa để tạo ra mã được tối ưu hóa tốt nhất. Trong phần này, chúng tôi đánh giá khả năng của LLM mô phỏng các tối ưu hóa khác nhau trong chính chúng. Đối với thí nghiệm này, đầu vào mô hình là IR chưa tối ưu và tên của pass tối ưu hóa để áp dụng, đầu ra là IR sau khi áp dụng pass này.

Dataset Chúng tôi tạo dataset mới cho nhiệm vụ này sử dụng 60 pass tối ưu hóa và áp dụng chúng ngẫu nhiên cho các chương trình từ Bảng I. Chúng tôi tăng cường dataset mã chưa tối ưu với mã được tối ưu một phần bằng cách đầu tiên chạy chuỗi các pass được chọn ngẫu nhiên trên IR chưa tối ưu trước pass đích mong muốn. Chúng tôi thu thập 10,000 ví dụ (prompt, answer) duy nhất cho mỗi 60 pass với tổng 600k ví dụ.

Model Chúng tôi đào tạo mô hình mới từ đầu trên dataset dịch pass này. Nó đạt hiệu suất đỉnh sau 11B token đào tạo (74 GPU days).

Results Hình 7 tóm tắt hiệu suất mô hình. Điểm BLEU trung bình trên tất cả pass là 0.846, với khớp chính xác từng ký tự 73.7% thời gian và mã có thể biên dịch 82.3% thời gian. Chúng tôi cũng vẽ tần suất mà mỗi tối ưu hóa xuất hiện trong danh sách pass do mô hình tạo đã cải thiện hoặc làm hồi quy hiệu suất so với -Oz trong Bảng III. Chúng tôi không tìm thấy mối tương quan giữa metrics chất lượng mã và tần suất của nó trong danh sách pass được tạo.

Như có thể thấy, nhiều pass được học gần hoàn hảo trong khi những pass khác tỏ ra thách thức hơn. Trong số các pass hoạt động kém, một số gợi ý về những cải thiện đơn giản cho biểu diễn trong khi những pass khác là kết quả của những hạn chế sâu sắc hơn trong lý luận của mô hình. Listing 6a hiển thị ví dụ từ pass -name-anon-globals, đây là pass tiện ích đơn giản đổi tên các biến global ẩn danh bằng hash của tên module. Vì chúng tôi không cung cấp tên module trong prompt, LLM buộc phải tạo ra các giá trị ngẫu nhiên. Chúng tôi sẽ thêm tên module vào prompts để giải quyết điều này.

Listing 6b hiển thị ví dụ từ pass -instcombine. Đây là pass phức tạp được triển khai trong hơn 4.5k dòng mã C++ trong LLVM. Chúng tôi thấy rằng mô hình xác định đúng các lệnh để kết hợp, nhưng mắc lỗi trong phân tích luồng dữ liệu và thay thế giá trị không chính xác. Đây là tối ưu hóa quan trọng thường xuyên xuất hiện trong danh sách pass vượt trội -Oz. Chúng tôi sẽ khám phá phương pháp active learning trong đó nhiều ví dụ hơn được cung cấp cho các pass phức tạp và khó.

--- TRANG 10 ---
Cuối cùng, chúng tôi trình bày ví dụ về tối ưu hóa mô hình đúng trong Listing 7. Ví dụ kết hợp nhiều thao tác mã không tầm thường: phân bổ thanh ghi, đơn giản hóa đồ thị luồng điều khiển, và kết hợp lệnh. Chúng tôi trực quan hóa đồ thị luồng điều khiển và dữ liệu để giúp diễn giải các thay đổi mà mô hình đã thực hiện. Ngay cả ở quy mô của những hàm IR nhỏ này, chúng tôi thấy sự nắm bắt tinh vi về ngữ nghĩa LLVM-IR được LLM thể hiện đáng chú ý. Mô hình đã học thực hiện những tối ưu hóa này hoàn toàn từ các ví dụ, mà không truy cập vào triển khai trình biên dịch.

Listing 7: Ví dụ về tạo IR đã tối ưu đúng. Mô hình thực hiện nhiều tối ưu hóa phức tạp bao gồm đơn giản hóa luồng điều khiển và thay thế các khối mã if-then-else bằng lệnh.

VI. THẢO LUẬN

Chúng tôi đã cho thấy LLM có thể mô phỏng gần hoàn hảo nhiều tối ưu hóa trình biên dịch và vượt trội các phương pháp trước, nhưng có những hạn chế. Phần này nhằm cung cấp thảo luận thực tế về giới hạn và hướng nghiên cứu tương lai.

A. Cửa sổ Ngữ cảnh

Hạn chế chính của LLM là độ dài chuỗi đầu vào hạn chế (cửa sổ ngữ cảnh). Trong nghiên cứu này, chúng tôi nhắm đến cửa sổ ngữ cảnh 2k-token và chia IR thành các hàm riêng lẻ để tối đa hóa lượng mã chúng tôi có thể vừa trong cửa sổ ngữ cảnh. Điều này không mong muốn vì nhiều lý do. Thứ nhất, nó hạn chế ngữ cảnh có sẵn cho mô hình khi đưa ra quyết định tối ưu hóa; thứ hai, nó ngăn tối ưu hóa intra-function; thứ ba, chúng tôi không thể tối ưu hóa mã không vừa trong cửa sổ ngữ cảnh. Hình 5 gợi ý rằng các chương trình lớn hơn có nhiều cơ hội tối ưu hóa thú vị hơn.

Các nhà nghiên cứu đang áp dụng cửa sổ ngữ cảnh ngày càng tăng [45], nhưng cửa sổ ngữ cảnh hữu hạn vẫn là mối quan tâm chung với LLM. Khi các kỹ thuật mới để xử lý chuỗi dài tiếp tục phát triển, chúng tôi dự định kết hợp chúng và áp dụng cho tối ưu hóa mã, ví dụ như biến thể interpolation vị trí của Code Llama [46] là RoPE base period scaling [9] hoặc các kỹ thuật ngoại suy độ dài gần đây [47].

B. Lý luận Toán học và Logic

Trình biên dịch thực hiện nhiều phép tính. Bất cứ khi nào có thể, các biểu thức được đánh giá tại thời điểm biên dịch để tối thiểu hóa công việc tại runtime và để phơi bày thêm cơ hội tối ưu hóa. Chúng tôi thấy các ví dụ về LLM gặp khó khăn với loại lý luận này, ví dụ như constant folding thất bại (Listing 3) và phân tích luồng dữ liệu thất bại (Listing 6b).

Chúng tôi nghĩ rằng phương pháp chain-of-thought [48] trong đó các mô hình được dạy phân tách các vấn đề lý luận phức tạp thành các bước tăng dần sẽ tỏ ra hiệu quả. Chúng tôi đã thực hiện bước đầu tiên theo hướng này bằng cách chia tối ưu hóa thành các pass riêng lẻ trong Phần V-C. Chúng tôi cũng dự định tập trung đào tạo vào curriculum của số học và logic, và đào tạo LLM sử dụng công cụ để tính toán kết quả trung gian [49, 50].

C. Tốc độ Suy luận

Trình biên dịch nhanh. Mất hai bậc độ lớn thời gian hơn để mô hình tạo danh sách pass so với thời gian trình biên dịch thực thi nó. Mặc dù điều này nhanh hơn nhiều so với autotuner mà nó được đào tạo, nó vẫn là overhead có thể tỏ ra cấm đoán cho một số ứng dụng. Điều đó chưa nói gì đến sự khác biệt trong tài nguyên tính toán cần thiết để đánh giá heuristic trình biên dịch so với LLM 7B tham số chạy trên nhiều GPU.

Ngoài batching và quantization tích cực [51], việc tăng tốc suy luận đáng kể có thể đạt được bằng cách chuyên biệt hóa từ vựng cho trường hợp sử dụng. Ví dụ, chúng tôi có thể giảm toàn bộ chuỗi con của pass thành các phần tử từ vựng đơn lẻ bằng Byte Pair Encoding để tại thời điểm suy luận ít token hơn cần được tạo.

VII. NGHIÊN CỨU LIÊN QUAN

Sắp xếp pass trình biên dịch cho hiệu suất đã được khai thác trong nhiều thập kỷ [26, 52, 53]. Qua các năm đã có nhiều phương pháp sử dụng machine learning [18–20, 39, 54, 55]. Việc áp dụng machine learning trong trình biên dịch không giới hạn ở thứ tự pass và đã được áp dụng cho nhiều vấn đề khác [17, 56–59]. Không ai đã áp dụng LLM cho bài toán sắp xếp pass, chúng tôi là người đầu tiên làm điều này.

Dịch máy neural là lĩnh vực mới nổi sử dụng mô hình ngôn ngữ để biến đổi mã từ ngôn ngữ này sang ngôn ngữ khác. Các ví dụ trước bao gồm biên dịch C sang assembly [11], assembly sang C [36, 60], và transpilation source-to-source [10]. Trong các nghiên cứu này, tính đúng đắn của mã không thể được đảm bảo. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng tạo mã chỉ như nhiệm vụ học tập phụ trợ – tính đúng đắn được cung cấp bởi trình biên dịch.

Mô hình ngôn ngữ đã tìm thấy việc áp dụng rộng rãi cho các nhiệm vụ coding, mặc dù ít mô hình hoạt động ở cấp độ IR trình biên dịch. Gallagher et al. đào tạo kiến trúc RoBERTA trên LLVM-IR cho mục đích xác định điểm yếu mã [61] và Transcoder-IR [12] sử dụng LLVM-IR như điểm pivot cho dịch source-to-source. Cả hai đều không sử dụng LLM để tối ưu hóa như chúng tôi.

Nhiều mô hình ngôn ngữ đã được đào tạo trên mã nguồn bao gồm CodeBERT [62], GraphCodeBERT [63], và CodeT5 [64] được đào tạo để thực hiện nhiều nhiệm vụ bao gồm tìm kiếm mã, tóm tắt mã, và tạo tài liệu. LLM được đào tạo trên mã nguồn cũng đã được sử dụng cho fuzzing chương trình [13, 14, 65], tạo test [15], và sửa chữa chương trình tự động [66–68]. Một số lượng lớn ứng dụng hữu ích đã được khám phá cho mô hình ngôn ngữ, tuy nhiên, đây là nghiên cứu đầu tiên nơi LLM được sử dụng cụ thể để tối ưu hóa mã.

Hầu hết LLM được đào tạo ít nhất một phần trên mã [3, 5, 25, 69]. Một số LLM được đào tạo tương tự như các mô hình tổng quát nhưng đặc biệt nhắm đến ngôn ngữ lập trình và có thể được sử dụng cho hoàn thiện mã như Codex [8] cung cấp năng lượng cho Copilot [70]. Việc giới thiệu khả năng fill-in-the-middle đặc biệt hữu ích cho các trường hợp sử dụng hoàn thiện mã thực tế và đã trở nên phổ biến trong các mô hình mã gần đây như InCoder [6], SantaCoder [4], StarCoder [1], và Code Llama [9]. Code Llama cũng được đào tạo để tuân theo hướng dẫn và tạo mã cũng như giải thích các chức năng của nó.

Mặc dù corpus đào tạo multi-terabyte cho các mô hình này chứa một số assembly, chúng tôi tin rằng việc khám phá tập trung về giá trị của LLM trong lĩnh vực trình biên dịch sẽ có giá trị cho cộng đồng. Bài báo này nhằm cung cấp điều đó.

--- TRANG 11 ---
VIII. KẾT LUẬN

Chúng tôi trình bày các bước đầu tiên hướng tới LLM cho tối ưu hóa mã. Chúng tôi xây dựng mô hình có thể dự đoán chiến lược tối ưu hóa tốt cho LLVM-IR chưa thấy. Kết quả đầy hứa hẹn, mặc dù chúng tôi đối mặt với thách thức về độ dài chuỗi hạn chế chúng tôi hoạt động trên các đoạn chương trình nhỏ, và trong lý luận số học hạn chế khả năng của mô hình dự đoán kết quả của tối ưu hóa. Chúng tôi hy vọng truyền cảm hứng cho cộng đồng nghiên cứu vượt ra ngoài LLM cho tạo mã max-likelihood đơn giản và đi vào tối ưu hóa mã nhận thức hiệu suất.

TÀI LIỆU THAM KHẢO
[1] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, et al. "StarCoder: may the source be with you!" Trong: arXiv:2305.06161 (2023).
[2] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, et al. "Competition-Level Code Generation with AlphaCode". Trong: Science 378.6624 (2022).
[3] OpenAI. "GPT-4 Technical Report". Trong: arXiv:2303.08774 (2023).
[4] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, L. K. Umapathi, C. J. Anderson, et al. "SantaCoder: don't reach for the stars!" Trong: arXiv:2301.03988 (2023).
[5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, et al. "PaLM: Scaling Language Modeling with Pathways". Trong: arXiv:2204.02311 (2022).
[6] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W. -t. Yih, L. Zettlemoyer, và M. Lewis. "InCoder: A Generative Model for Code Infilling and Synthesis". Trong: arXiv:2204.05999 (2023).
[7] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, et al. "Textbooks Are All You Need". Trong: arXiv:2306.11644 (2023).
[8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, et al. "Evaluating Large Language Models Trained on Code". Trong: arXiv:2107.03374 (2021).
[9] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, et al. "Code Llama: Open Foundation Models for Code". Trong: arXiv:2308.12950 (2023).
[10] M.-A. Lachaux, B. Roziere, L. Chanussot, và G. Lample. "Unsupervised Translation of Programming Languages". Trong: arXiv:2006.03511 (2020).

[Tiếp tục với tất cả các tài liệu tham khảo còn lại...]
