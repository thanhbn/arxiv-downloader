# Tạo Test Hiệu Quả Sử Dụng Mô Hình Ngôn Ngữ Lớn Đã Được Huấn Luyện Trước và Mutation Testing

Arghavan Moradi Dakhela,∗, Amin Nikanjama, Vahid Majdinasaba, Foutse Khomhaand Michel
C. Desmaraisa
aDepartment of Computer and Software Engineering, Polytechnique Montreal, Montreal, H3T 1J4, Quebec, Canada

## TÓM TẮT

**Bối cảnh:** Một trong những giai đoạn quan trọng trong chu kỳ phát triển phần mềm là kiểm thử phần mềm. Kiểm thử giúp xác định các lỗi tiềm ẩn và giảm chi phí bảo trì. Mục tiêu của các công cụ tạo test tự động là làm dễ dàng việc phát triển test bằng cách đề xuất các test hiệu quả trong việc phát hiện lỗi. Gần đây, các nhà nghiên cứu đã tận dụng Mô Hình Ngôn Ngữ Lớn (LLM) của code để tạo ra các unit test. Trong khi độ bao phủ code của các test được tạo ra thường được đánh giá, tài liệu đã thừa nhận rằng độ bao phủ có mối tương quan yếu với hiệu quả của test trong việc phát hiện lỗi.

**Mục tiêu:** Để cải thiện hạn chế này, trong bài báo này, chúng tôi giới thiệu MuTAP (Mutation Test case generation using Augmented Prompt) để cải thiện hiệu quả của các test case được tạo bởi LLM về mặt tiết lộ lỗi bằng cách tận dụng mutation testing.

**Phương pháp:** Mục tiêu của chúng tôi được đạt được bằng cách bổ sung prompt với các mutant sống sót, vì những mutant này làm nổi bật các hạn chế của test case trong việc phát hiện lỗi. MuTAP có khả năng tạo ra các test case hiệu quả trong trường hợp không có mô tả ngôn ngữ tự nhiên về Chương Trình Được Kiểm Thử (PUT). Chúng tôi sử dụng các LLM khác nhau trong MuTAP và đánh giá hiệu suất của chúng trên các benchmark khác nhau.

**Kết quả:** Kết quả của chúng tôi cho thấy phương pháp được đề xuất có thể phát hiện thêm tới 28% các đoạn code có lỗi do con người viết. Trong số này, 17% vẫn không được phát hiện bởi cả công cụ tạo test tự động hoàn toàn hiện đại nhất (tức là Pynguin) và các phương pháp học zero-shot/few-shot trên LLM. Hơn nữa, MuTAP đạt được Điểm Mutation (MS) là 93.57% trên code buggy tổng hợp, vượt trội hơn tất cả các phương pháp khác trong đánh giá của chúng tôi.

**Kết luận:** Những phát hiện của chúng tôi cho thấy rằng mặc dù LLM có thể phục vụ như một công cụ hữu ích để tạo test case, chúng cần các bước xử lý hậu kỳ cụ thể để tăng cường hiệu quả của các test case được tạo ra, những test case này có thể gặp phải lỗi cú pháp hoặc chức năng và có thể không hiệu quả trong việc phát hiện một số loại lỗi nhất định và kiểm thử các trường hợp đặc biệt trong PUT.

## 1. Giới thiệu

Kiểm thử là một bước quan trọng nhưng tốn kém trong chu kỳ phát triển phần mềm. Tạo ra các test hiệu quả là một nhiệm vụ tốn thời gian và tẻ nhạt đối với các nhà phát triển. Unit test là thiết yếu vì chúng tạo thành nền tảng của kim tự tháp tự động hóa test. Unit test kiểm tra xem một function hoặc một component có hoạt động như mong đợi trong môi trường cô lập hay không. Một unit test bao gồm hai component: component đầu tiên là một tập hợp các test input cho Chương Trình Được Kiểm Thử (PUT), trong khi component thứ hai là test oracle chỉ ra hành vi dự định (output) của PUT và do đó có khả năng phơi bày lỗi bằng cách xác minh tính đúng đắn của PUT trên các test input. Một test oracle có thể ở dạng assertion.

Việc tự động tạo unit test là một chủ đề quan trọng trong Kỹ Thuật Phần Mềm (SE). Nó nhằm mục đích giảm nỗ lực kiểm thử của nhà phát triển. Phát triển unit test chất lượng tốt có thể ngăn ngừa lỗi trong sản phẩm phần mềm. Có các công cụ khác nhau để tự động tạo unit test và test suite dựa trên các generator test ngẫu nhiên, dynamic symbolic execution, hoặc các phương pháp dựa trên tìm kiếm. Tuy nhiên, những kỹ thuật này có một số nhược điểm và thường tạo ra các test không có assertion hoặc assertion quá chung chung, hoặc test với assertion không thể đánh giá hiệu quả hành vi dự định của PUT.

Xem xét những thiếu sót này, các nhà nghiên cứu gần đây đã khám phá khả năng tận dụng các kỹ thuật tổng hợp code dựa trên Machine Learning để tạo ra unit test tốt hơn. Cụ thể, những phương pháp này đã khám phá tiềm năng của Mô Hình Ngôn Ngữ Lớn (LLM) với kiến trúc transformer, chẳng hạn như Codex, đã đạt được hiệu suất tốt trong tổng hợp chương trình tự động. Trong số những nỗ lực như vậy, Bareiß et al. đánh giá hiệu suất của Codex cho việc tạo test case bằng cách sử dụng phương pháp học few-shot. Những phát hiện của họ trên một tập hợp hạn chế 18 phương thức Java cho thấy rằng phương pháp của họ có thể so sánh được với việc tạo test có hướng dẫn phản hồi. ATHENATEST tận dụng mô hình transformer BART sau khi fine-tune nó trên một tập hợp các function Java thực và các test tương ứng của chúng. Họ cũng báo cáo đạt được độ bao phủ có thể so sánh với EvoSuite sau khi đánh giá năm dự án Java. Lemieux et al. đề xuất CODAMOSA sử dụng các test case được tạo bởi Codex để cải thiện các kỹ thuật kiểm thử dựa trên tìm kiếm, bao gồm chỉ tiền tố (input) của một test case mà không có bất kỳ test oracle nào. Kết quả được báo cáo của họ thu được trên 27 dự án Python cho thấy CODAMOSA vượt trội hơn kỹ thuật dựa trên tìm kiếm cơ sở, Pynguin và Codex về độ bao phủ code. Mặc dù kết quả sơ bộ của những nghiên cứu này và những nghiên cứu khác có triển vọng, không có nghiên cứu nào trong số này cố gắng cải thiện khả năng phát hiện lỗi của các test được tạo ra. Hơn nữa, đã được thừa nhận trong tài liệu rằng trong khi độ bao phủ test là một metric hữu ích để đánh giá chất lượng test, nó có mối tương quan yếu với hiệu quả của test trong việc phát hiện lỗi.

Mutation Testing (MT) là một kỹ thuật kiểm thử white box để đánh giá khả năng của một test trong việc tiết lộ lỗi. MT đã được nghiên cứu rộng rãi và sử dụng thành công trong SE để đánh giá hiệu quả của các test case. MT bao gồm việc tiêm các thay đổi nhân tạo dựa trên các lỗi thực vào một PUT, dẫn đến các phiên bản đột biến của PUT được gọi là mutant. Càng nhiều test case kill mutant, test case đó càng hiệu quả trong việc xác định lỗi thực. Các mutant sống sót làm nổi bật điểm yếu của một test case và mục tiêu cuối cùng là để các test case có thể phát hiện tất cả mutant, tức là kill chúng. Mutant không chỉ hữu ích để đánh giá hiệu quả của test case mà còn có thể được sử dụng như một phương tiện để thiết kế các test case hiệu quả hơn.

Trong bài báo này, chúng tôi trình bày nghiên cứu đầu tiên tận dụng MT để tăng cường và đánh giá hiệu quả của các test case được tạo bởi LLM cho các chương trình Python về khả năng tiết lộ lỗi. Phương pháp của chúng tôi nhằm mục đích tối ưu hóa test case cho việc phát hiện lỗi thay vì độ bao phủ code. Kỹ thuật được đề xuất của chúng tôi, MuTAP, sử dụng một LLM làm Component chính (LLMC) và bắt đầu bằng cách đưa một prompt vào LLMC để tạo test case. Prompt ban đầu bao gồm PUT và hướng dẫn tạo test case bằng cách sử dụng học zero-shot và few-shot. Tiếp theo, MuTAP đánh giá cú pháp của các test case được tạo ra và re-prompt LLMC của nó để sửa chữa bất kỳ vấn đề cú pháp nào được phát hiện. Sau khi sửa lỗi cú pháp, MuTAP tiến hành đánh giá hành vi dự định của các test case được tạo ra. Điều này được đạt được bằng cách so sánh output của test oracle trên các test input nhất định với các giá trị trả về mong đợi của PUT bằng cách sử dụng cùng các test input, từ đó sửa chữa bất kỳ hành vi không mong muốn nào trong test oracle.

Sau đó, MuTAP áp dụng MT để kiểm tra hiệu quả của test case trong việc kill mutant của PUT. Vì các mutant sống sót làm nổi bật hạn chế của các test case được tạo ra, MuTAP re-prompt LLMC của nó để tạo test case mới cho các PUT có mutant sống sót bằng cách bổ sung prompt ban đầu với cả test case ban đầu và các mutant sống sót. MuTAP dừng quá trình bổ sung prompt ban đầu khi hoặc là các test case cuối cùng có thể phát hiện hiệu quả tất cả mutant hoặc không còn mutant sống sót nào chưa được sử dụng để bổ sung prompt ban đầu.

Chúng tôi sử dụng hai loại LLM làm LLMC của MuTAP: Codex, được thiết kế cho các nhiệm vụ liên quan đến code, và llama-2-chat, được tối ưu hóa cho các trường hợp sử dụng dialog và đủ linh hoạt để đáp ứng một loạt các nhiệm vụ, bao gồm cả lập trình. Chúng tôi đánh giá MuTAP trên cả lỗi tổng hợp của 164 PUT và 1710 chương trình buggy được thu thập từ một benchmark sửa chữa lỗi Python. Kết quả của chúng tôi chỉ ra rằng phương pháp được đề xuất tạo ra các test case hiệu quả với Điểm Mutation (MS, tỷ lệ mutant bị kill trên tổng số mutant) trung bình là 93.57%, vượt trội hơn cả Pynguin (một công cụ tạo test tự động hoàn toàn hiện đại) và các kỹ thuật học zero-shot/few-shot LLM thông thường. Hơn nữa, phương pháp của chúng tôi phát hiện thêm tới 468 (28%) đoạn code buggy do con người viết so với các phương pháp có thể so sánh khác trong đánh giá của chúng tôi. Đáng chú ý, nó xác định 79 (17%) đoạn code buggy của con người mà không có kỹ thuật nào khác có thể phát hiện. Tóm lại, bài báo này đóng góp những điều sau:

• Chúng tôi trình bày nghiên cứu đầu tiên về việc tận dụng MT để tạo test case với LLM.
• Chúng tôi đề xuất một kỹ thuật học dựa trên prompt để cải thiện hiệu quả của test case bằng cách bổ sung prompt với cả test case ban đầu và mutant sống sót của một PUT.
• Chúng tôi đánh giá hiệu quả của các test được tạo ra trong việc phát hiện lỗi trong các phiên bản buggy thực và tổng hợp của PUT.
• Chúng tôi làm cho kỹ thuật được đề xuất, MuTAP, có sẵn công khai trực tuyến cho các nhà nghiên cứu/thực hành khác để nhân rộng hoặc xây dựng dựa trên công việc của chúng tôi.

Phần còn lại của bài báo này được tổ chức như sau. Phần 2 giới thiệu một ví dụ thúc đẩy. Phần 3 mô tả các bước khác nhau của phương pháp của chúng tôi. Chúng tôi trình bày thiết lập thực nghiệm, câu hỏi nghiên cứu và kết quả thực nghiệm trong Phần 4. Chúng tôi thảo luận các phát hiện và các trường hợp sử dụng tiềm năng của phương pháp của chúng tôi trong Phần 5. Các mối đe dọa đến tính hợp lệ của kết quả được xem xét trong Phần 6. Chúng tôi xem xét ngắn gọn các công trình liên quan trong Phần 7. Cuối cùng, chúng tôi kết luận bài báo trong Phần 8; làm nổi bật một số hướng cho các công việc tương lai.

## 2. Ví dụ Thúc đẩy

Trong phần này, chúng tôi trình bày một ví dụ trong Hình 1 cho thấy cách phương pháp được đề xuất của chúng tôi tạo ra các test case hiệu quả. Giả sử chúng ta có 10 mutant {SM0, SM1, ..., SM9} cho Chương Trình Được Kiểm Thử, PUT trong Hình 1. Mục tiêu của kỹ thuật được đề xuất của chúng tôi, MuTAP (Mutation Test case generation using Augmented Prompt), là tạo ra các test case hiệu quả cho PUT theo cách đảm bảo kill số lượng mutant tối đa.

Function any_int() trong Hình 1 nhận 3 input và trả về True nếu tất cả 3 input là số nguyên, đồng thời một trong các input bằng tổng của hai input khác. Ngược lại, nó trả về False. Trong bước đầu tiên, MuTAP sử dụng prompt ban đầu, 1, để chạy một truy vấn trên LLM Component (LLMC) và tạo ra các test case ban đầu cho Chương Trình Được Kiểm Thử (PUT) này. Component 2 trong Hình 1 cho thấy các test case ban đầu được tạo bởi LLMC sau bước tinh chỉnh. Chúng tôi đặt tên nó là Initial Unit Test, IUT. Trong Phần 3, chúng tôi thảo luận chi tiết bước tinh chỉnh (sửa cú pháp và hành vi dự định) của phương pháp chúng tôi. IUT kill 6 trong số 10 mutant của PUT. 4 mutant còn lại tiết lộ điểm yếu của test được tạo ra, có nghĩa là IUT cần các test case mới với assertion để kill các lỗi được tiêm trong 4 mutant đó.

Để giải quyết hạn chế này và tạo ra các test case hiệu quả hơn, MuTAP bổ sung prompt ban đầu với hai component mới; component đầu tiên là phản hồi của mô hình đối với prompt ban đầu sau khi sửa cú pháp và hành vi dự định của nó, IUT, và component thứ hai là mutant component, 3 trong Hình 1. MuTAP khởi tạo việc xây dựng mutant component bằng cách sử dụng "Survived Mutant" đầu tiên của PUT mà chúng tôi gọi là SM0. Phần tô sáng màu đỏ trong SM0 cho thấy lỗi được tiêm trong PUT. Lỗi được tiêm thay đổi câu lệnh thứ hai trong điều kiện của if bên trong trong PUT theo cách mà tổng của input đầu tiên và cuối cùng của function any_int() không bằng input giữa nữa. Vì không có test case nào trong IUT để xác minh rằng input giữa của nó, y, bằng tổng của input đầu tiên và cuối cùng của nó, x và z, IUT không thể kill mutant này.

MuTAP sử dụng sự nối của ba component này: 1, 2, và 3 để re-prompt LLMC. Component 4 trong Hình 1, cho thấy tập hợp test case mới được tạo bởi LLMC được nối vào IUT sau bước tinh chỉnh. Chúng tôi đặt tên nó là Augmented Unit Test, AUT0. Unit test có thêm hai assertion so với IUT và một trong số chúng, được tô sáng màu đỏ, kill mutant, SM0.

MuTAP áp dụng AUT0 cho các mutant của PUT một lần nữa. Nếu có bất kỳ mutant sống sót nào còn lại, MuTAP lặp lại quá trình bổ sung bằng cách cập nhật mutant component với một mutant sống sót khác nếu nó chưa được sử dụng để bổ sung prompt trước đó. MuTAP sử dụng từng mutant riêng lẻ vì đôi khi các test case mới giải quyết một mutant cũng có thể kill các mutant sống sót còn lại. Hơn nữa, do độ dài hạn chế của prompt và độ dài không cố định của mutant, việc áp dụng từng mutant sống sót riêng biệt là một phương pháp thực tế hơn. Hình 1 3′ cho thấy một ví dụ về cách mutant component được cập nhật bằng cách sử dụng một mutant sống sót khác. Chúng tôi gọi mutant này là SM1. Unit test, 4′, cho thấy một tập hợp test case mới bao gồm một assertion phát hiện SM1. MuTAP lặp lại quá trình bổ sung cho đến khi hoặc là các test case cuối cùng có thể kill tất cả mutant, hoặc không còn mutant sống sót nào chưa được sử dụng để bổ sung prompt ban đầu.

Các test case cuối cùng được tạo bởi kỹ thuật được đề xuất của chúng tôi, MuTAP, kill 9 trong số 10 mutant của ví dụ này, PUT, và nó tăng MS cho PUT từ 60% (6 trong số 10) lên 90% (9 trong số 10). Kết quả này có thể được so sánh với công cụ tạo test tự động hiện đại cho ngôn ngữ lập trình Python, Pynguin, tạo ra một test case cho PUT với chỉ 40% MS. Công cụ này sử dụng kỹ thuật tạo dựa trên tìm kiếm và ngẫu nhiên đột biến các giá trị test trong một test case để tạo ra các test case mới. Bản chất ngẫu nhiên của phương pháp này dẫn đến cơ hội thấp tạo ra một test case mới có thể kill các mutant sống sót của PUT.

## 3. Phương pháp

Trong phần này, chúng tôi thảo luận các bước khác nhau của phương pháp của chúng tôi. Hình 2 cho thấy tổng quan về phương pháp được đề xuất của chúng tôi và Thuật toán 1 trình bày chuỗi các bước khác nhau của nó.

### 3.1. Prompt Ban đầu

LLM có khả năng thực hiện những nhiệm vụ mà chúng đã được huấn luyện. Fine-tuning LLM để thực hiện một nhiệm vụ mới rất tốn kém về mặt tính toán. Ngoài ra, có những LLM như Codex cho thấy hiệu suất rất tốt trong việc tạo code nhưng vì chúng là closed-source, việc fine-tuning chúng cho một nhiệm vụ mới là không thể.

Học dựa trên prompt là một kỹ thuật hiệu quả để điều chỉnh LLM cho các nhiệm vụ mới. Một prompt là sự kết hợp của ngữ cảnh ngôn ngữ tự nhiên và/hoặc ngôn ngữ lập trình và được sử dụng như một input cho LLM. Có các nghiên cứu cho thấy rằng việc đặt một hướng dẫn ngôn ngữ tự nhiên như một gợi ý (học zero-shot) hoặc một số ví dụ (học few-shot) trong prompt làm tăng khả năng của LLM trong việc thực hiện một nhiệm vụ mới.

MuTAP sử dụng cả học zero-shot và few-shot để xây dựng prompt ban đầu và gọi LLMC trên chúng riêng biệt. Bước này được hiển thị trong Thuật toán 2. Cụ thể hơn, chúng tôi sử dụng zero-shot và few-shot như sau:

• zero-shot: Prompt ban đầu được tạo bởi kỹ thuật zero-shot chứa ba đơn vị, theo phương pháp trong [29]. Component được chỉ ra bởi 1 trong Hình 1 cho thấy một ví dụ về prompt như vậy. Đơn vị đầu tiên trong component này là một hướng dẫn bằng ngôn ngữ tự nhiên có tên INS1 và nó làm rõ nhiệm vụ bằng cách hỏi: "Generate test case for the following code". Đơn vị thứ hai là Chương Trình Được Kiểm Thử (PUT) và đơn vị cuối cùng là một tập hợp hướng dẫn bằng ngôn ngữ lập trình có tên INS2. INS2 hoạt động như một gợi ý để chỉ ra output mong muốn cho LLMC. Sự nối của (INS1, PUT, INS2) xây dựng prompt ban đầu cho học zero-shot (Dòng 2 trong Thuật toán 2).

• few-shot: Việc tạo prompt dựa trên học few-shot sử dụng một chuỗi input và output mong đợi liên quan đến nhiệm vụ downstream. Có các phương pháp khác nhau để trình bày cặp input và output trong prompt. Chúng tôi tuân theo phương pháp trong [1] để xây dựng prompt ban đầu với chiến lược few-shot trong MuTAP. Xem xét độ dài token tối đa có thể cho LLMC (4k token trong nghiên cứu của chúng tôi), prompt few-shot bao gồm hai ví dụ minh họa khác nhau về một Method (M) và một Unit Test (UT) như sau (Dòng 5 trong Thuật toán 2):

<code>M_1</code>\n<test>UT_1</test>\n
<code>M_2</code>\n<test>UT_2</test>\n
<code>PUT_i</code>\n<test>

Không có mô tả ngôn ngữ tự nhiên về PUT trong prompt ban đầu vì những mô tả như vậy có thể không luôn có sẵn, và MuTAP dựa vào khả năng của LLMC để tổng hợp ngữ cảnh code. MuTAP gọi prompt ban đầu, zero-shot hoặc few-shot, trên LLMC và sau đó chuyển output được suy luận đến bước tiếp theo (Dòng 2 trong Thuật toán 1).

### 3.2. Tinh chỉnh

Trong phần này, chúng tôi mô tả quá trình tinh chỉnh các test case được tạo ra trong MuTAP bao gồm việc sửa lỗi cú pháp và sửa chữa hành vi dự định. Các chi tiết được hiển thị trong Thuật toán 3.

#### 3.2.1. Sửa Cú pháp

Các test case được tạo bởi LLMC có thể có lỗi cú pháp (thiếu dấu ngoặc, dòng chưa hoàn thành, v.v.). Vì MuTAP cần thực thi function test để điều tra MT và bổ sung prompt, các mẫu có lỗi cú pháp trở nên không hiệu quả. Tuy nhiên, đôi khi một thay đổi nhỏ trong output của LLMC có thể sửa lỗi cú pháp và chuyển đổi nó thành một test case có thể thực thi được.

MuTAP sử dụng khả năng của LLMC để sửa lỗi cú pháp, tương tự như các nghiên cứu khác. Để làm như vậy, LLMC được gọi trên một prompt mới để sửa lỗi cú pháp trong output riêng của nó (Thủ tục SyntaxFixer trong Thuật toán 3). Prompt sửa cú pháp bao gồm hai phần. Phần đầu tiên là một hướng dẫn ngôn ngữ tự nhiên, INSfix, "Fix the syntax errors in the following code snippet", và phần thứ hai là function test được tạo bởi LLMC trên prompt ban đầu (Dòng 7-8 trong Thuật toán 3). Nếu lỗi cú pháp vẫn tồn tại ngay cả sau khi re-prompt LLMC, MuTAP sử dụng parser Python để xác định dòng lỗi. Sau đó nó giữ lại các dòng đứng trước dòng có vấn đề, đảm bảo chúng không có lỗi cú pháp (Dòng 13 trong Thuật toán 3).

#### 3.2.2. Sửa chữa Hành vi Dự định

Dựa trên prompt ban đầu, LLMC tạo ra các test case khác nhau được serialize như một assertion oracle bằng cách gọi PUT trên các input nhất định và so sánh output trả về của PUT với output mong đợi hoặc ground truth, ví dụ, {assert add(2,2) == 4}. Tuy nhiên, có thể LLMC tạo ra các test case đang assert các giá trị trả về sai. Điều đó có nghĩa là đối với một số test case, LLMC không tạo ra output trả về mong đợi của PUT. Việc thiếu mô tả ngôn ngữ tự nhiên về PUT trong prompt ban đầu có thể dẫn đến việc tạo ra các test case không phản ánh chính xác hành vi dự định của method.

Assertion với các giá trị trả về sai có thể fail trên mutant, không phải vì phát hiện lỗi, mà vì hành vi không mong muốn của assertion. Những failure này gây nhầm lẫn về hiệu quả của test case. Vì vậy, bước này của MuTAP nhằm mục đích sửa chữa hành vi dự định của assertion oracle trong các test case (Thủ tục IntendedBehaviorFixer trong Thuật toán 3).

Đối với mỗi assertion trong test, MuTAP chạy PUT trên các test input và so sánh output trả về của PUT với asserting output. Nếu output trả về của PUT giống với asserting output trong oracle, thì MuTAP coi nó như một assertion oracle với hành vi dự định đúng. Ngược lại, nó sửa chữa những assertion đó bằng cách thay thế asserting output bằng output mong đợi của PUT (Dòng 22-27 trong Thuật toán 3). MuTAP bỏ qua những assertion mà các loại input fail trên PUT, ví dụ, nếu PUT mong đợi một list số nguyên nhưng test input là một string.

Kết quả cuối cùng của bước này được gọi là Initial Unit Test (IUT) là một tập hợp các test case được tạo bởi LLMC sau khi tinh chỉnh như được hiển thị bởi 2 trong Hình 1.

### 3.3. Mutation Testing (MT)

MT đánh giá chất lượng và hiệu quả của test case. Mutant được xây dựng bằng cách tiêm lỗi nhân tạo vào PUT để mô phỏng khiếm khuyết. Nếu test case fail trên một mutant, chúng ta coi nó như một mutant bị kill, ngược lại, nó sống sót, có nghĩa là các test case trong unit test không thể phát hiện nó. Sự hiện diện của các mutant sống sót làm nổi bật những thiếu sót của test case, gợi ý cần thêm một test case mới hoặc cải thiện một test case hiện có. Điểm Mutation (MS) thể hiện hiệu quả của test case bằng cách tính tỷ lệ mutant bị kill trong tất cả mutant của một PUT.

Thuật toán 4 trình bày chi tiết của bước này. Lấy cảm hứng từ [32], MuTAP sử dụng MutPy để tạo ra các mutant khác nhau cho mỗi PUT và tính toán MS (Dòng 3-7 trong Thuật toán 4). Thực thi test case trên mỗi mutant bao gồm việc thực hiện một số thiết lập sơ bộ. Với mục đích này, MuTAP sử dụng "setuptools.find_packages" tích hợp của Python để định vị và cài đặt các package cần thiết, như "math", "numPy", "pandas", "pytest", và những package khác. Ngoài ra, MuTAP triển khai các function setup chịu trách nhiệm tạo các thư mục tạm thời, được sử dụng trong quá trình thực thi test case trên mutant. Sau khi thực thi test case trên mutant và tính toán MS, MuTAP đúng cách tear down thiết lập bằng cách xóa thư mục tạm thời.

Như được hiển thị trên Dòng 5-9 trong Thuật toán 1, nếu MS của một PUT đạt 100%, MuTAP chuyển test case đến bước tối thiểu hóa oracle (Phần 3.5), ngược lại, nó thu thập danh sách các mutant sống sót và chuyển mutant đến bước bổ sung prompt (Phần 3.4).

### 3.4. Bổ sung Prompt

Thuật toán 5 cho thấy chi tiết của bước này. Nếu có bất kỳ mutant sống sót nào từ bước trước, MuTAP bổ sung prompt ban đầu, zero-shot hoặc few-shot, bằng cách thêm bốn component mới (Dòng 3 trong Thuật toán 5). Component đầu tiên là IUT, unit test ban đầu được tạo bởi LLMC sau khi tinh chỉnh. Component thứ hai là một hướng dẫn bằng ngôn ngữ tự nhiên có tên INS3 làm rõ thiếu sót của IUT bằng "The test function, test(), cannot detect the fault in the following code". Component thứ ba là một trong các mutant sống sót của PUT, có tên SM. Component cuối cùng, INS4 là một hướng dẫn bằng ngôn ngữ tự nhiên và lập trình: ngữ cảnh ngôn ngữ tự nhiên làm rõ nhiệm vụ bằng cách yêu cầu "Provide a new test case to detect the fault in prior code" và ngữ cảnh ngôn ngữ lập trình chỉ hoạt động như một gợi ý để hướng dẫn LLMC tạo output. Một ví dụ được hiển thị bởi 3 trong Hình 1.

MuTAP re-prompt LLMC và lặp lại bước tinh chỉnh trên output được tạo ra (Dòng 4-5 trong Thuật toán 5). Sau đó, nó nối các test case mới được tạo ra vào IUT mà chúng ta gọi là Augmented Unit Test (AUT). AUT được chuyển đến bước MT (Dòng 7 trong Thuật toán 5). MuTAP lặp lại đệ quy bổ sung prompt cho đến khi hoặc là các test case cuối cùng kill tất cả mutant (MS=100%) hoặc không có mutant sống sót nào chưa được sử dụng trong quá trình bổ sung (Dòng 8 trong Thuật toán 5). Một ví dụ về việc cập nhật mutant component trong Hình 1, 3 được thay đổi thành 3′ bằng cách thay thế SM0 bằng SM1. 4′ chỉ ra các test case được tạo ra với LLMC sau khi lặp lại quá trình trên mutant sống sót tiếp theo.

### 3.5. Tối thiểu hóa Oracle

Các test case được tạo bởi LLMC thường bao gồm các assertion dư thừa. Ngoài ra, quá trình bổ sung có thể thêm nhiều assertion dư thừa hơn vào unit test cuối cùng. Trình bày tất cả chúng (với sự dư thừa) như output cuối cùng có thể gây nhầm lẫn cho các nhà phát triển. Trong bước cuối cùng, tương tự như các công cụ trước đó tạo ra test oracle được điều khiển bởi mutation, MuTAP tối thiểu hóa số lượng assertion bằng cách sử dụng kỹ thuật Greedy để loại bỏ các assertion dư thừa không cải thiện MS. Bước này được trình bày trong Thuật toán 6. MuTAP bắt đầu bằng cách theo dõi số lượng mutant mà mỗi assertion kill và sau đó chọn test case chứa assertion kill số lượng mutant tối đa. Quá trình này sau đó được lặp lại bằng cách thêm các test case chứa các assertion tiếp theo phát hiện nhiều mutant nhất (Dòng 4-10 trong Thuật toán 6). Nếu việc thêm assertion mới này tăng MS, MuTAP giữ test case và assertion của nó. Ngược lại, test case sẽ bị loại bỏ như dư thừa.

## 4. Đánh giá

Trong phần này, chúng tôi mô tả các đánh giá mà chúng tôi đã thiết kế và tiến hành để điều tra các câu hỏi nghiên cứu sau:

**RQ1** Các test case được tạo bởi MuTAP hiệu quả như thế nào so với các test case được tạo bởi các công cụ tạo test tự động?

**RQ2** Các phần khác nhau của MuTAP hoạt động như thế nào?

**RQ3** Hiệu suất của MuTAP đối với mỗi loại mutation là gì?

### 4.1. Thiết lập Thực nghiệm

Trong phần này, chúng tôi trình bày thiết lập thực nghiệm của chúng tôi. Cụ thể, chúng tôi mô tả công cụ tạo test tự động được sử dụng để so sánh kết quả của chúng tôi, làm rõ LLMC của MuTAP và thiết lập của nó, và chỉ ra các baseline và benchmark dataset được sử dụng trong các thí nghiệm của chúng tôi.

Chúng tôi đã tiến hành thí nghiệm trên cluster Cedar của Compute Canada, cung cấp CPU 32 lõi, bộ nhớ 1TB, và một GPU v100l với 32GB GPU Memory, và trên một hệ thống chạy Linux 5.15.0-69-generic với CPU AMD FX(tm)-6300 Six-Cores, bộ nhớ 512GB, và 16GB Memory.

#### 4.1.1. Tham số Thực nghiệm

Chúng tôi gọi prompt ban đầu, zero-shot hoặc few-shot, trên LLMC lên đến 10 lần và thu thập các output đáp ứng hai tiêu chí như các test case ứng viên: ứng viên phải bao gồm hai từ khóa assert và tên function của PUT. Nếu sau 10 lần chạy, LLMC không thể tạo ra output chứa hai từ khóa đó, chúng tôi coi nhiệm vụ đó như một nhiệm vụ có vấn đề hoặc một nhiệm vụ mà MuTAP không thể tạo test case.

Về bước sửa cú pháp, chúng tôi chạy prompt sửa cú pháp trên LLMC lên đến 10 lần. Nếu lỗi cú pháp vẫn không được giải quyết ngay cả sau 10 lần lặp, MuTAP sử dụng parser Python để định vị dòng lỗi. Sau đó nó giữ lại các dòng đứng trước dòng buggy, đảm bảo chúng không có lỗi cú pháp. Nếu việc loại bỏ dòng dẫn đến việc không còn test case nào (tất cả test case đều không thể compile), chúng tôi phân loại nhiệm vụ đó là có vấn đề.

#### 4.1.2. Công cụ So sánh

Pynguin là một công cụ tạo test tự động hoàn toàn nổi tiếng cho ngôn ngữ lập trình được gõ động như Python. Nó sử dụng các thuật toán dựa trên tìm kiếm khác nhau hướng tới việc thỏa mãn các tiêu chí độ bao phủ code, tức là độ bao phủ nhánh. Pynguin đầu tiên lấy một code Python (method, module, v.v.) làm input và thu thập thông tin của nó như loại biến, tên method, và dependencies. Sau đó nó sử dụng một trong các thuật toán tạo test dựa trên tìm kiếm (MIO, MOSA, DynaMOSA, v.v.) để tạo test case. Nó ngẫu nhiên đột biến (xóa, chèn, thay thế) các giá trị và câu lệnh khác nhau trong test case để tạo ra các test case mới và thực thi chúng trên PUT để đảm bảo tính đúng đắn của chúng. Cuối cùng, nó tạo ra assertion cho test case bằng cách sử dụng một MT engine.

Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng Pynguin 0.17.0. với DynaMOSA. Theo đánh giá của Pynguin, DynaMOSA cho thấy hiệu suất tốt nhất so với các thuật toán khác trong việc tạo test case với công cụ này. Chúng tôi đặt timeout của việc tạo test là 600 giây là thiết lập mặc định của công cụ.

#### 4.1.3. Large Language Model Component (LLMC)

Chúng tôi sử dụng hai LLM khác nhau làm LLMC của MuTAP. LLM đầu tiên là Codex của OpenAI, được thiết kế đặc biệt cho các nhiệm vụ tạo code. Chúng tôi sử dụng Code-davinci-002, với temperature là 0.8. Temperature thấp hơn gây ra ít biến thể hơn trong output của mô hình trong khi temperature cao hơn tăng biến thể của output và sau đó là cơ hội tạo ra các test case hữu ích qua các lần lặp khác nhau. Đánh giá của CODAMOSA cho thấy rằng 0.8 là một temperature hợp lý để tạo ra các test case hữu ích với Codex.

LLM thứ hai là llama-2-chat của Meta, đã được tinh chỉnh lặp đi lặp lại bằng cách sử dụng Reinforcement Learning with Human Feedback (RLHF) và phù hợp cho các trường hợp sử dụng dialog. Tương tự như Codex, chúng tôi đã cấu hình temperature của mô hình là 0.8. Hơn nữa, mô hình cung cấp ba vai trò riêng biệt trong prompt: system, user, và assistant. Những vai trò này phục vụ mục đích làm rõ mỗi component của prompt cho mô hình bằng cách gán các component cụ thể cho mỗi vai trò. Các kết hợp khác nhau của những vai trò này có thể được sử dụng trong mỗi prompt để điều chỉnh tương tác với mô hình theo các yêu cầu cụ thể.

Trong các thí nghiệm của chúng tôi, vai trò của system được định nghĩa là {You are a Python coding assistant. Always answer with Python code.}, cho tất cả các loại prompt, bao gồm zero-shot, few-shot, và augmented prompt. Để xử lý prompt zero-shot, chúng tôi chỉ đặt nội dung vai trò của user là sự nối của (INS1, PUTi, INS2). Đối với prompt few-shot, chúng tôi định nghĩa nội dung của vai trò assistant là một tập hợp các ví dụ minh họa về Method (M) và Unit Test (UT), trong khi nội dung vai trò user được đặt thành PUTi. Đối với augmented prompt, các component khác nhau của nó được thiết lập như sau:

{user: Initial Prompt,
assistant: IUT,
user: concat(INS3, SMi, INS4)}

Đối với cả hai LLM, số lượng token được tạo ra tối đa được đặt thành 250 để tạo test case và 20 token để sửa cú pháp, dựa trên các nghiên cứu trước về các nhiệm vụ tương tự. Stop word được định nghĩa là quote (") cho zero-shot và là </test> cho prompt few-shot. Đối với các hyperparameter còn lại, chúng tôi giữ các giá trị mặc định của mô hình.

Để tránh overfitting trên dữ liệu benchmark, MuTAP lặp lại tất cả prompt trên Codex hoặc llama-2-chat lên đến 10 lần. Nếu sau 10 lần chạy, yêu cầu tạo test case không được thỏa mãn, MuTAP coi nó như một nhiệm vụ có vấn đề hoặc chưa được giải quyết.

Điều quan trọng cần lưu ý là MuTAP không giới hạn ở hai mô hình này, và LLMC của nó có thể được thay thế bằng bất kỳ LLM nào khác theo yêu cầu.

#### 4.1.4. Baseline

Ngoài Pynguin, chúng tôi đề xuất hai baseline cho mỗi LLM để đánh giá phương pháp được đề xuất của chúng tôi, MuTAP.

**Before-refining:** Baseline đầu tiên là output của prompt ban đầu trên LLMC (Codex hoặc llama-2-chat), mà không sửa lỗi cú pháp hoặc sửa chữa hành vi dự định. Vì assertion với các giá trị trả về không mong muốn có thể fail trên mutant hoặc buggy code và trình bày hiệu quả không hợp lệ, chúng tôi bỏ qua những assertion đó trong baseline này để tránh tác dụng phụ này. Nếu output của mô hình có lỗi cú pháp, chúng tôi coi nó như một test sai và do đó coi nhiệm vụ đó như một nhiệm vụ có vấn đề hoặc chưa được giải quyết.

**After-refining:** Baseline thứ hai là output của prompt ban đầu trên LLMC (Codex hoặc llama-2-chat), sau khi áp dụng các bước sau: Refining (Phần 3.2) và Oracle Minimization (Phần 3.5).

#### 4.1.5. Mutant Generator

Để áp dụng MT, chúng tôi cần tạo ra các phiên bản mutant khác nhau của một PUT bằng cách tiêm lỗi vào các dòng khác nhau của nó. Với mục đích này, chúng tôi sử dụng MutPy phiên bản 2.0. MutPy là một công cụ MT cho code trong Python 3.3+. Nó được hưởng lợi từ các mutation operator khác nhau để tạo ra mutant. Danh sách các mutation operator được sử dụng trong thí nghiệm của chúng tôi với các ví dụ tương ứng được hiển thị trong Bảng 1. MutPy tiêm một operator tại một thời điểm để tạo ra mutant nếu operator có thể áp dụng trên PUT.

#### 4.1.6. Benchmark Dataset

Để tiến hành các thí nghiệm của chúng tôi, chúng tôi sử dụng hai benchmark khác nhau. Benchmark đầu tiên là HumanEval là một benchmark để đánh giá LLM tạo code. Nó có 164 bài toán lập trình do con người viết ở mức độ dễ đến trung bình. Mỗi bài toán có các thuộc tính khác nhau như mô tả và giải pháp tham chiếu. Chúng tôi sử dụng giải pháp tham chiếu của mỗi nhiệm vụ như một PUT.

Benchmark thứ hai, Refactory, là một benchmark cho việc sửa chữa lỗi Python. Nó có 1710 bài nộp buggy của sinh viên cho 5 bài tập của một khóa học lập trình Python. Mỗi bài tập có một giải pháp tham chiếu đúng mà chúng tôi sử dụng như PUT. Lợi thế của dataset này là các đoạn code buggy được tạo bởi con người cho chúng tôi cơ hội đánh giá test case được tạo bởi MuTAP trên lỗi thực và so sánh nó với Pynguin và baseline của chúng tôi.

### 4.2. Kết quả Thực nghiệm

Trong phần này, chúng tôi thảo luận các phát hiện của chúng tôi cho mỗi RQ.

#### 4.2.1. RQ1: Các test case được tạo bởi MuTAP hiệu quả như thế nào so với các test case được tạo bởi các công cụ tạo test tự động?

Vì nghiên cứu của chúng tôi tập trung vào MT để cải thiện hiệu quả của test case, chúng tôi so sánh MuTAP với Pynguin và baseline của chúng tôi về MS, số lượng mutant bị kill, và số lượng PUT với 100% MS. Đáng chú ý rằng chúng tôi chỉ xem xét PUT với test case đúng để tính toán MS trung bình cho mỗi phương pháp. Vì lý do này, chúng tôi báo cáo tổng số mutant bị kill và tổng số PUT với 100% MS để so sánh công bằng.

Bảng 2 cho thấy kết quả thu được cho benchmark HumanEval. Trước khi sửa cú pháp và sửa chữa hành vi dự định (before-refining), các test case được tạo bởi Codex và llama-2-chat không đúng cho 73 và 68 (trong số 164) PUT, tương ứng, khi sử dụng prompt ban đầu zero-shot. Tuy nhiên, chúng quản lý để kill 295 và 318 mutant (trong số 1260), tương ứng.

Prompt ban đầu có tác động rõ rệt hơn đến output của Codex so với llama-2-chat. Chuyển prompt ban đầu sang few-shot giảm số lượng PUT không có test case xuống 39, đồng thời tăng số lượng mutant bị kill lên 508 khi sử dụng Codex làm LLMC. Mặt khác, khi sử dụng llama-2-chat, số lượng PUT không có test case giảm xuống 60, và số lượng mutant bị kill tăng từ 318 lên 325. Sự khác biệt về hiệu suất này có thể được quy cho việc llama-2-chat phù hợp hơn cho dialog prompt, và việc sử dụng prompt với một cặp input và output minh họa, không có ngữ cảnh ngôn ngữ tự nhiên, không cải thiện hiệu suất của mô hình một cách đáng kể.

Ngược lại, Pynguin, như công cụ tạo test tự động hiện đại, vượt trội hơn output của cả hai LLM, before-refining, bằng cách kill 649 mutant và thất bại trong việc tạo test case cho 31 nhiệm vụ.

Sau khi áp dụng các bước xử lý hậu kỳ của sửa cú pháp và sửa chữa hành vi dự định, MuTAP với cả hai LLM hoạt động tốt hơn Pynguin về việc kill nhiều mutant hơn. Đáng chú ý, khi sử dụng cả prompt zero-shot và few-shot, llama-2-chat có thể tạo test case đúng cho tất cả PUT, after-refining. Tuy nhiên, hiệu quả của chúng về việc kill mutant được đo ở 84.04% và 85.16% với prompt zero-shot và few-shot, tương ứng.

Mặt khác, MS của test case được tạo bởi Codex sau khi tinh chỉnh là 76.82% và 82.73% với prompt zero-shot và few-shot, tương ứng. Mặc dù có sự cải thiện này, Codex vẫn thất bại trong việc tạo test case đúng cho 30 (với zero-shot) và 27 (với few-shot) PUT sau khi tinh chỉnh.

MuTAP, tăng cường hiệu quả của test case được tạo bởi cả hai LLM, Codex và llama-2-chat, đạt được MS là 89.13% và 91.98% với prompt zero-shot, và MS là 92.02% và 93.57% với prompt few-shot, tương ứng. Đặc biệt, MuTAP với prompt few-shot khi sử dụng llama-2-chat làm LLMC của nó quản lý để kill 1179 mutant trong số 1260 và tạo test case với MS=100% cho tới 70% PUT, thể hiện sự cải thiện đáng kể về hiệu quả của test case so với Pynguin với 649 mutant bị kill và 28.22% PUT với MS=100%.

Bảng 3 cho thấy kết quả trên các chương trình buggy thực của con người từ benchmark Refactory xác nhận các phát hiện của chúng tôi trên HumanEval. Để đánh giá MuTAP trên code buggy thực, chúng tôi áp dụng các bước sau. Đầu tiên, chúng tôi tạo ra mutant của mỗi PUT trong dataset này. Thứ hai, chúng tôi tiến hành quá trình bổ sung prompt và hoàn thiện test case cho mỗi PUT. Sau đó, chúng tôi áp dụng test case được tạo bởi MuTAP trên code buggy của sinh viên trong Refactory, tiếp theo là test case được tạo bởi Pynguin và LLM After-refining, để đánh giá hiệu quả của test case được tạo bởi các phương pháp khác nhau trong việc phát hiện code buggy.

MuTAP với học few-shot trong khi sử dụng llama-2-chat làm LLMC của nó xác định thêm 468 code buggy so với Pynguin (với MS là 94.91% so với 67.54%) và 111 code buggy hơn so với After-refining (với MS là 94.91% so với 82.51%). Hơn nữa, MuTAP phát hiện 79 code buggy không được phát hiện bởi cả Pynguin hoặc test case của llama-2-chat After-refining. Khi sử dụng Codex, MuTAP phát hiện 73 code buggy bị bỏ lỡ bởi cả Pynguin và test case của Codex After-refining. Hơn nữa, MuTAP xuất sắc trong việc tạo ra các test case hiệu quả hơn, với trung bình 2.6 test case sau khi áp dụng tối ưu hóa greedy.

Nhìn chung, MuTAP sử dụng cả llama-2-chat và Codex thể hiện hiệu suất tốt hơn so với Pynguin về việc kill mutant và phát hiện code buggy. Hiệu quả của những test case này trong việc phát hiện khiếm khuyết được cải thiện thông qua các bước xử lý hậu kỳ của tinh chỉnh và bổ sung prompt.

**Phát hiện 1:** MuTAP tạo ra các test case hiệu quả hơn so với Pynguin và học zero-shot và few-shot thông thường trên LLM. Số lượng test case của MuTAP không lớn hơn nhiều so với output của các phương pháp khác sau khi tối thiểu hóa. Ngoài ra, LLM với thiết lập dialog hoạt động tốt hơn trên augmented prompt. Kết luận, hiệu quả của test case được tạo bởi LLM có thể được tăng cường thông qua bổ sung prompt bằng cách sử dụng mutant sống sót và tinh chỉnh xử lý hậu kỳ.

#### 4.2.2. RQ2: Các phần khác nhau của MuTAP hoạt động như thế nào?

**Syntax Fixer:** Trung bình, tỷ lệ phần trăm test case có lỗi cú pháp là 38.98% và 26.48% khi sử dụng prompt zero-shot và few-shot, tương ứng, với Codex. Khi sử dụng llama-2-chat, tỷ lệ phần trăm này là 33.85% và 26.32% với prompt zero-shot và few-shot, tương ứng.

Khi xem xét lỗi cú pháp, ba yếu tố góp phần giảm chúng trong output của LLM. Yếu tố đầu tiên là loại prompt ban đầu. Như được hiển thị trong Bảng 4 trên benchmark HumanEval, học few-shot dẫn đến ít lỗi cú pháp hơn trong output của cả hai LLM. Cụ thể, khi sử dụng Codex, tỷ lệ phần trăm lỗi cú pháp giảm từ 44.79% xuống 29.03% after-refining, và đối với MuTAP, nó giảm từ 33.17% xuống 23.93%. Với llama-2-chat làm LLMC, tỷ lệ phần trăm lỗi cú pháp giảm từ 38.03% xuống 26.99% sau tinh chỉnh, và từ 29.66% xuống 25.64% cho MuTAP.

Yếu tố có tác động thứ hai, cũng là yếu tố chính, là component Syntax Fixing. Như được hiển thị trong Bảng 4, khi sử dụng Codex, component này trong MuTAP trung bình sửa 14.5% lỗi cú pháp bằng cách sử dụng LLMC và giải quyết 81.37% lỗi cú pháp bằng cách bỏ qua các dòng gây ra lỗi. Mặt khác, khi sử dụng llama-2-chat làm LLMC của MuTAP, component Syntax Fixing, trung bình, giải quyết 32.31% lỗi cú pháp thông qua re-prompting LLMC, và 60.73% lỗi bằng cách bỏ qua các dòng có vấn đề.

Yếu tố cuối cùng góp phần vào việc cải thiện lỗi cú pháp trong test case là quá trình bổ sung prompt trong MuTAP. Bằng cách bổ sung prompt với IUT, sự xuất hiện của lỗi cú pháp trong output của Codex với kỹ thuật zero-shot giảm từ 44.79% xuống 33.17%. Tương tự, với llama-2-chat và prompt zero-shot, tỷ lệ phần trăm lỗi cú pháp giảm từ 38.03% xuống 29.66%. Bổ sung prompt với IUT cung cấp các ví dụ minh họa về test case và phục vụ mục đích tương tự như các ví dụ minh họa trong prompt học few-shot, hiệu quả giảm lỗi cú pháp trong output của LLM.

Phát hiện của chúng tôi trên benchmark Refactory cho thấy MuTAP tạo test case với lỗi cú pháp chỉ trong một PUT (trong số 5) sử dụng Codex và học zero-shot. Hơn nữa, không có lỗi cú pháp nào trong số đó có thể được sửa bằng re-prompting LLMC. Mặt khác, đối với cả hai loại prompt ban đầu, lỗi cú pháp giảm xuống không sử dụng llama-2-chat.

**Intended Behavior Repair:** Trong trường hợp sửa chữa hành vi dự định, hai yếu tố riêng biệt góp phần giảm tỷ lệ lỗi trong assertion oracle. Như được hiển thị trong Bảng 5, bước Intended Behavior Repair, khi sử dụng Codex làm LLMC, trung bình, sửa 83.98% và 89.86% hành vi không đúng trong after-refining và MuTAP, tương ứng. Khi sử dụng llama-2-chat, bước này sửa 84.35% và 95.96% hành vi không mong muốn trong after-refining và MuTAP, tương ứng.

Ngoài bước Intended Behavior Repair, bước bổ sung prompt trong MuTAP giảm đáng kể sự xuất hiện của hành vi không mong muốn trong test case. Ví dụ, khi sử dụng Codex với prompt zero-shot, assertion với hành vi không mong muốn, như các giá trị trả về sai, giảm từ 63.63% xuống 19.38%. Tương tự, với llama-2-chat và sử dụng prompt few-shot, assertion với hành vi không mong muốn giảm từ 63.25% xuống 10.75%. Lý do đằng sau sự cải thiện này có thể được quy cho việc sử dụng IUT (Initial Unit Test) trong MuTAP để bổ sung prompt ban đầu. Những IUT này đã thể hiện hành vi dự định của PUT, từ đó hỗ trợ LLM trong việc đề xuất test case với ít hành vi không mong muốn hơn (tức là ít giá trị trả về sai hơn). Ngoài ra, trên benchmark Refactory, MuTAP đã sửa tất cả assertion với hành vi không đúng trên output của augmented prompt.

Không giống như lỗi cú pháp, loại prompt không giúp đáng kể với hành vi không mong muốn trong assertion. Sự kết hợp của bước Intended Behavior Repair và quá trình bổ sung prompt cải thiện hiệu quả của test case, đảm bảo rằng chúng phù hợp với hành vi dự định của PUT.

**Surviving Mutants Representation:** Chúng tôi cũng điều tra tác động của thứ tự mutant sống sót đến MS trong quá trình bổ sung prompt. Hình 3 minh họa hiệu ứng của việc bổ sung prompt với thứ tự ngẫu nhiên của mutant sống sót qua 5 lần chạy cho tất cả PUT. Để so sánh này, chúng tôi ngẫu nhiên chọn một trong các mutant sống sót của mỗi PUT với MS < 100% và sử dụng nó để bổ sung prompt ban đầu. Sau đó chúng tôi tính toán MS trung bình cho tất cả PUT. Tiếp theo, chúng tôi ngẫu nhiên chọn mutant sống sót thứ hai cho các PUT còn lại với MS < 100% (nếu có), lặp lại quá trình bổ sung, và tính toán MS trung bình cho tất cả PUT một lần nữa. Chúng tôi tiếp tục lặp lại quá trình này cho đến khi hoặc là không còn PUT nào với MS < 100% hoặc không còn mutant sống sót nào chưa được sử dụng trong quá trình tranh luận.

Như được hiển thị trong Hình 3, mỗi điểm dữ liệu đại diện cho MS trung bình cho tất cả PUT qua 5 lần chạy của việc lựa chọn ngẫu nhiên mutant sống sót. Đáng chú ý, hơn 90% MS được đạt được bằng cách chỉ sử dụng một nửa số mutant sống sót, và sự cải thiện trong MS dừng lại sau một số lần lặp nhất định của bước bổ sung trong các LLM khác nhau. Ví dụ, khi sử dụng Codex làm LLMC, trong học zero-shot, MS ngừng cải thiện mặc dù, trung bình, 27 mutant sống sót (trong số 226) không được sử dụng trong bước bổ sung prompt. Tương tự, trong học few-shot, con số này bằng 24 (trong số 106).

Kết quả của chúng tôi cho RQ2 thể hiện rằng test case được tạo bởi LLM, bất kể loại prompt, cần xử lý hậu kỳ, như sửa cú pháp hoặc sửa chữa hành vi dự định, để hoạt động đúng cách và phát hiện lỗi hiệu quả. Ngoài ra, thứ tự của mutant sống sót để bổ sung prompt không tác động đáng kể đến mức tăng MS.

**Phát hiện 2:** Syntax Fixing và Intended Behavior Repair sửa tới 95.94% và 89.86% lỗi cú pháp và chức năng trong test case, tương ứng. Bổ sung prompt trong MuTAP giảm hành vi không mong muốn trong output của LLM một cách đáng kể (44.36% sử dụng Codex và 52.5% sử dụng llama-2-chat). Hơn nữa, chỉ một số lượng nhỏ mutant (tới 27) không góp phần vào việc cải thiện MS.

#### 4.2.3. RQ3: Hiệu suất của MuTAP đối với mỗi loại mutation là gì?

Trong RQ này, chúng tôi đánh giá hiệu suất của MuTAP trong các loại mutant khác nhau. Chúng tôi báo cáo tổng số và số lượng mutant bị kill bởi mỗi phương pháp trên benchmark HumanEval trong Bảng 6. Hiệu suất của tất cả kỹ thuật cho mỗi loại mutant được báo cáo để giúp so sánh. Tổng số mutant trong mỗi loại khác nhau cho mỗi phương pháp vì số lượng PUT có vấn đề không giống nhau cho tất cả phương pháp. MS cho mỗi loại/phương pháp chỉ ra tỷ lệ mutant bị kill trong tổng số mutant trong loại đó. Có một số loại mutant phổ biến hơn (nhiều mẫu hơn trong những loại đó) như AOR, COI, và ROR (một ví dụ cho mỗi loại mutant được hiển thị trong Bảng 1). Số lượng mutant trong mỗi loại phụ thuộc vào PUT. Ví dụ, trong HumanEval, có ít PUT với exception handling. Do đó, có ít mutant trong EHD.

Nói chung, MuTAP cho thấy hiệu suất tốt hơn hoặc tương tự trong tất cả các loại mutant so với Pynguin và output của LLM After-refining của cả hai LLM. Xem xét ASR như một ví dụ, MuTAP cho thấy hiệu suất cao nhất trên loại mutant này trong tất cả các phương pháp. Ví dụ, test case được tạo bởi Pynguin xác định 45 mutant trong category này trong khi test case được tạo bởi MuTAP sử dụng llama-2-chat và prompt few-shot xác định 79 mutant trong category này (trong số 84).

Đối với một trong các loại mutant, BCR, là loại hiếm trong benchmark của chúng tôi, MuTAP và After-refining với cả prompt ban đầu zero-shot và few-shot, cùng với việc sử dụng Codex, cho thấy hiệu suất giống nhau. Tuy nhiên, khi sử dụng llama-2-chat, MuTAP vượt trội hơn những cái khác bằng cách kill nhiều mutant hơn của loại này. Đối với một loại mutant hiếm khác trong dataset của chúng tôi, EHD, đáng chú ý rằng Codex, mặc dù sử dụng cả hai loại prompt ban đầu và quá trình bổ sung, thất bại trong việc tạo test case để phát hiện hai mutant có trong category này. Ngược lại, MuTAP với prompt ban đầu few-shot và llama-2-chat thành công kill tất cả mutant trong category này.

**Phát hiện 3:** Các test case được tạo bởi MuTAP đều hiệu quả hoặc hiệu quả hơn trong việc kill các loại mutant khác nhau so với những test case được tạo bởi Pynguin và phương pháp baseline. Ngoài ra, việc sử dụng LLM với thiết lập dialog có thể tăng số lượng kill mutant trong các loại mutant khác nhau trong khi áp dụng bổ sung prompt.

## 5. Thảo luận

### 5.1. Tạo Test Case Tự động

MuTAP tận dụng khả năng tổng hợp code của LLM và sử dụng học dựa trên prompt để hỗ trợ nhà phát triển tạo ra các test case hiệu quả mà không cần fine-tuning LLM tốn kém về mặt tính toán.

LLM có thể tạo ra các test case hiệu quả hơn những test case được tạo bởi Pynguin về việc tiết lộ lỗi. Listing 1 cho thấy một test case mẫu được tạo bởi Pynguin cho PUT của ví dụ thúc đẩy của chúng tôi trong Phần 2. Trong khi Pynguin tạo ra test input như số nguyên ngẫu nhiên và đột biến những giá trị đó để tạo ra test case mới, LLM tạo ra các test case trông tự nhiên hơn và có mối tương quan với loại input/output và chức năng của PUT. Tuy nhiên, test case được tạo bởi LLM cần xử lý hậu kỳ để trở nên hiệu quả hơn trong việc phát hiện lỗi. Kết quả của chúng tôi cho thấy rằng việc bổ sung prompt với mutant sống sót và tinh chỉnh test case (cú pháp và hành vi dự định) giúp LLM tạo ra test case hiệu quả hơn về phát hiện lỗi.

Nhà phát triển có thể sử dụng MuTAP để tạo ra test case hiệu quả về phát hiện lỗi, với sự trợ giúp của LLM. Ngoài ra, MuTAP có thể được tích hợp vào component tạo test của GitHub Copilot lab để đề xuất test case hiệu quả hơn cho nhà phát triển. Vì mutant có thể được tạo ra tự động, bổ sung prompt có thể được áp dụng mà không có sự tham gia của con người.

### 5.2. Thời gian Thực thi

API mở của Codex có giới hạn về số lượng request (20 mỗi phút) và số lượng token (40,000 mỗi phút). Vì lý do này, thí nghiệm của chúng tôi cần dừng gọi API thỉnh thoảng để không vượt quá giới hạn. Kết quả là, chúng tôi trình bày phân tích thời gian xử lý bằng cách sử dụng llama-2-chat. Thời gian xử lý tổng thể của MuTAP trên dataset HumanEval trong khi sử dụng llama-2-chat trung bình là 39.75 giây với học zero-shot (với tối thiểu 16.16 và tối đa 56.66 giây) và 42.11 giây với prompt few-shot (với tối thiểu 18.2 và tối đa 64.2 giây) mỗi nhiệm vụ. Nó bao gồm trung bình xây dựng và gọi prompt ban đầu trên LLMC với trung bình 10.26 giây, sửa cú pháp bao gồm gọi prompt sửa cú pháp trên LLMC với 10.3 giây, sửa chữa hành vi dự định ở 0.38 giây, tính toán MS ở 1.7 giây, tạo augmented prompt và gọi chúng trên LLM với 12.05 giây và tối ưu hóa greedy với 1.4 giây. Đáng chú ý rằng sau bước bổ sung prompt, MuTAP phải lặp lại các quá trình sửa cú pháp, sửa chữa hành vi dự định, và các bước greedy đã được bao gồm trong thời gian xử lý tổng thể. Trong tất cả các bước của MuTAP, những bước tốn thời gian nhất là những bước đòi hỏi suy luận output từ LLM. Ngược lại, thời gian xử lý tổng thể trên cùng benchmark với Pynguin để hoàn thành tìm kiếm không gian cần thiết trung bình là 44.16 giây với tối thiểu 2.7 và tối đa 10 phút là timeout mặc định của công cụ.

### 5.3. Lợi ích của dialog LLM

Các phát hiện của chúng tôi chỉ ra rằng thiết lập dialog của llama-2-chat cung cấp cho MuTAP sự linh hoạt để gán các vai trò riêng biệt cho mỗi component của augmented prompt. Ví dụ, bằng cách gán IUT cho vai trò assistant trong quá trình bổ sung prompt, khả năng lặp lại các test ban đầu trong output được tạo ra giảm, trong khi cơ hội tạo ra test case mới để phát hiện mutant sống sót tăng. Listing 2 minh họa một ví dụ về cách llama-2-chat hiệu quả tổng hợp sự khác biệt của PUT và một trong các mutant sống sót của nó, giải thích sự khác biệt, và sau đó tạo ra một test case mới để phát hiện lỗi.

### 5.4. Metric Đánh giá

Các nghiên cứu trước đây liên quan đến việc tạo assertion thông qua LLM đã sử dụng "exact match" như một trong các metric đánh giá của chúng. Exact match tính toán tỷ lệ phần trăm test case được tạo bởi LLM (output được suy luận) khớp từ vựng với ground truth test case (output mong đợi). Tuy nhiên, CIDAR đã thảo luận về sự không đầy đủ của exact match như một metric phù hợp để đánh giá assertion được tạo bởi LLM. Lý do này là mô hình thường tạo ra assertion đúng về mặt ngữ nghĩa nhưng có thể không khớp chính xác với ground truth. Trong nghiên cứu của chúng tôi, MuTAP thực thi mỗi test case bao gồm assertion, cả trên PUT và trên mutant của nó để đánh giá tính đúng đắn và hiệu quả của chúng, báo cáo MS của chúng. MS là một metric thường được sử dụng trong các nghiên cứu trước và nó phục vụ như một metric hiệu quả để đánh giá chất lượng của test oracle. Trong khi, trong bài báo này, chúng tôi tập trung vào việc cải thiện hiệu quả của test case về phát hiện lỗi, có các metric khác như test coverage có thể đánh giá các khía cạnh chất lượng khác của một test case. Cải thiện MS không nhất thiết dẫn đến coverage tốt và test coverage có mối tương quan yếu với hiệu quả của test trong phát hiện lỗi và bị thách thức như một thước đo hiệu quả test trong việc tiết lộ lỗi, điều này có thể làm cho phương pháp được đề xuất của chúng tôi khó hoạt động tốt trên cả hai metric.

Hơn nữa, kết quả được trình bày trong [46] chỉ ra rằng khoảng 60% test case được tạo bởi Codex gặp phải vấn đề biên dịch do lỗi cú pháp. Việc kết hợp các bước sửa cú pháp và sửa chữa hành vi dự định trong phương pháp được đề xuất của chúng tôi, MuTAP, tăng cường đáng kể tính hữu ích của các test được tạo bởi LLM.

### 5.5. Mutant sống sót

Chúng tôi bổ sung prompt tại mỗi lần lặp cho mỗi PUT với một mutant sống sót duy nhất. Số lượng mutant trung bình cho tất cả PUT trong HumanEval và Refactory là 6.6 và 4.2 và số lượng mutant sống sót trung bình là 3.6 và 1.8, tương ứng. Sử dụng sự kết hợp của mutant sống sót để bổ sung prompt có thể tác động đến tốc độ đạt 100% MS. Tuy nhiên, không phải tất cả mutant sống sót được sử dụng trong bổ sung prompt đều góp phần cải thiện MS, đôi khi test case mới giải quyết một mutant cũng có thể kill các mutant sống sót còn lại.

## 6. Mối đe dọa đến Tính hợp lệ

**Tính hợp lệ nội bộ.**

Trong nghiên cứu này, chúng tôi đã sử dụng hai kỹ thuật học dựa trên prompt khác nhau: zero-shot và few-shot. Tuy nhiên, chúng tôi không khám phá tác động tiềm năng của việc thay đổi hướng dẫn ngôn ngữ tự nhiên hoặc ví dụ minh họa (cho học few-shot) trong prompt của chúng tôi. Sửa đổi những hướng dẫn này hoặc sử dụng các ví dụ minh họa khác nhau phù hợp hơn với chức năng của PUT có thể tăng cường kết quả. Như được thể hiện bởi kết quả của chúng tôi trong RQ2, việc bao gồm IUT trong prompt trong các bước bổ sung giảm các trường hợp hành vi không mong muốn trong test oracle. Ngược lại, việc sử dụng, ví dụ, hướng dẫn ngôn ngữ tự nhiên dài có thể có tác động bất lợi đến kết quả.

Để sửa lỗi cú pháp trong test case thông qua re-prompting LLMC, chúng tôi đã sử dụng phương pháp được trình bày trong [52]. Chúng tôi không tích hợp thông tin bổ sung về lỗi cú pháp như thông báo lỗi hoặc dòng lỗi vào prompt. Đáng xem xét rằng việc kết hợp thông tin bổ sung về lỗi cú pháp có thể tăng cường hiệu suất của LLMC để sửa những lỗi cú pháp này.

Ngoài ra, chúng tôi thừa nhận rằng thuật toán greedy được sử dụng trong phương pháp của chúng tôi để tối thiểu hóa số lượng test oracle có thể không phải là giải pháp tối ưu nhất để tối thiểu hóa test oracle trong khi tối đa hóa MS. Tuy nhiên, các nghiên cứu trước đây sử dụng cùng phương pháp để tối thiểu hóa số lượng assertion đã thể hiện hiệu quả của nó trong việc giảm số lượng test oracle trong test case, cùng với tính dễ triển khai.

Cuối cùng, trong số các loại assertion khác nhau, chúng tôi chỉ tập trung vào việc tạo ra những assertion nguyên thủy trong nghiên cứu này. Các loại assertion khác có thể được khám phá trong các nghiên cứu tương lai.

Chúng tôi sử dụng khái niệm killability mutant và phát hiện lỗi như metric để đánh giá hiệu quả của test case, vì mục tiêu chính của testing là phát hiện lỗi. Coverage đã được sử dụng trong các nghiên cứu khác để đánh giá chất lượng test case. Tuy nhiên, đã được chứng minh rằng mặc dù có mối tương quan giữa coverage và phát hiện lỗi, chúng có thể không nhất quán phù hợp trong việc xếp hạng các chiến lược testing khác nhau, như được quan sát trong lĩnh vực fuzz testing.

**Tính hợp lệ cấu trúc.** Chúng tôi sử dụng khái niệm kill mutant và phát hiện lỗi như metric để đánh giá hiệu quả của test case, vì mục tiêu chính của testing là tiết lộ lỗi. Coverage đã được sử dụng trong các nghiên cứu khác để đánh giá chất lượng test case. Đã được chỉ ra rằng mặc dù có mối tương quan giữa coverage và tìm lỗi, chúng không đồng ý về xếp hạng của các tester khác nhau, như trong không gian fuzz testing.

Điều quan trọng cần lưu ý là các lỗi có trong mutant là nhân tạo và có thể không tương ứng trực tiếp với lỗi trong thế giới thực. Để giải quyết mối quan ngại này, chúng tôi đã sử dụng dataset Refactory, một benchmark sửa chữa lỗi chứa các chương trình lỗi thực được phát triển bởi sinh viên.

**Tính hợp lệ bên ngoài.** Cho các thí nghiệm của chúng tôi, chúng tôi đã sử dụng hai dataset chứa các nhiệm vụ lập trình Python, điều này có thể gây ra thách thức bên ngoài đối với tính hợp lệ của các phát hiện của chúng tôi. Yêu cầu về các chương trình Python có thể thực thi là thiết yếu để chạy các test được tạo ra đối với cả phiên bản chính xác và buggy (thực hoặc đột biến) của PUT và xem xét này đã hướng dẫn lựa chọn dataset của chúng tôi. Tuy nhiên, vì chúng tôi không đưa ra bất kỳ giả định cụ thể nào khi chọn dataset, kết quả của chúng tôi có thể được mở rộng cho các chương trình Python khác.

Cuối cùng, cần thừa nhận rằng kỹ thuật được đề xuất và các đánh giá được tiến hành trong bài báo này về mặt khái niệm có thể thích ứng với các ngôn ngữ ngoài Python. Tuy nhiên, việc triển khai hiện tại của MuTAP được thiết kế riêng cho các chương trình Python, có nghĩa là kết quả hiện tại của chúng tôi không thể được mở rộng để bao gồm các ngôn ngữ lập trình khác.

**Tính hợp lệ độ tin cậy.** Với mục đích cho phép các nhà nghiên cứu khác nhân rộng hoặc mở rộng nghiên cứu của chúng tôi, chúng tôi cung cấp một gói nhân rộng. Tuy nhiên, việc cải tiến liên tục của LLM có thể gây ra thách thức trong việc đạt được nhân rộng chính xác kết quả của chúng tôi.

## 7. Công trình liên quan

Tác giả trong [7] đã nghiên cứu tác động của học few-shot trên các nhiệm vụ downstream khác nhau, bao gồm tạo test case và test oracle. Họ so sánh hiệu suất của học few-shot với các công cụ tạo test tự động. Điều tra được tiến hành trên một tập hợp khác nhau các phương thức Java có nguồn gốc từ các benchmark khác nhau. Kết quả chỉ ra rằng LLM có khả năng tạo ra test case và test oracle khớp chính xác (về mặt từ vựng) với ground truth test trong các dự án benchmark. Hơn nữa, test coverage của chúng được tìm thấy là có thể so sánh với test case được tạo bởi các công cụ tạo test tự động.

Schäfer et al. [41] đã thực hiện nỗ lực tạo test case bằng cách prompting Codex. Điều tra của họ tập trung vào 25 gói JavaScript. Prompt trong nghiên cứu của họ bao gồm việc triển khai PUT và cũng là các ví dụ sử dụng API được trích xuất từ tài liệu. Trong các trường hợp mà test case không thành công trên PUT, phương pháp của họ kết hợp thông báo lỗi gặp phải vào prompt và re-prompt Codex. Phát hiện của họ thể hiện rằng quá trình tăng cường prompt với thông tin bổ sung như vậy đã tạo điều kiện cho Codex tạo ra test case đúng với coverage đầy đủ.

LIBRO [27] đã sử dụng các báo cáo vấn đề (cả tiêu đề và nội dung) như prompt few-shot để tạo ra test case tái tạo lỗi. Các test case cuối cùng được kết hợp vào các test class thích hợp và được xếp hạng dựa trên tính hợp lệ của chúng. Kết quả tiết lộ sự tăng cường trong việc tạo ra test case đúng để tái tạo lỗi so với các công cụ hiện đại.

CEDAR [35], thay vì sử dụng các ví dụ minh họa cố định trong học few-shot, nhằm mục đích truy xuất các ví dụ minh họa liên quan đến mỗi PUT và kết hợp chúng vào prompt. Họ đánh giá phương pháp của mình dựa trên khớp từ vựng, được gọi là "exact match", giữa assertion được tạo ra và ground truth trong một benchmark. Trong khi phương pháp được đề xuất của họ thể hiện hiệu suất tăng cường trong việc đạt được khớp chính xác giữa assertion và ground truth, nó cần một pull rộng lớn các mẫu code để lựa chọn ví dụ minh họa thích hợp cho mỗi PUT.

ATHENATEST [49] đã sử dụng mô hình transformer BART, mà họ đã fine-tune bằng cách sử dụng một tập hợp các function Java và các test tương ứng của chúng. Họ báo cáo test coverage có thể so sánh với những coverage của EvoSuite khi đánh giá việc tạo test case cho năm dự án Java.

TOGA [14] tham gia vào việc fine-tuning CodeBERT bằng cách sử dụng docstring của PUT cùng với tiền tố của một test case có assertion được che. Mục tiêu của họ là tổng hợp assertion. Sau đó, họ xây dựng toàn bộ test oracle bằng cách kết hợp ngữ pháp test oracle và tạo ra một tập hợp assertion. Tập hợp này sau đó được xếp hạng thông qua một neural network ranker dựa trên khớp từ vựng của chúng với ground truth test oracle. Mặc dù họ báo cáo kết quả tương tự như EvoSuite trong phát hiện lỗi, trọng tâm của họ chỉ là tổng hợp assertion. Tuy nhiên, tổng hợp assertion không phải là thách thức nhưng tạo ra test oracle hiệu quả và có ý nghĩa đặt ra một thách thức đáng kể.

CODAMOSA đã kết hợp test case được tạo bởi Codex với những test case có nguồn gốc từ Pynguin trong các trường hợp mà việc tạo test case của Pynguin dừng lại và thất bại trong việc tăng cường test coverage. CODAMOSA đạt được test coverage cao hơn trên các benchmark Python khác nhau so với Pynguin. Đáng chú ý rằng, tương tự như các nghiên cứu khác, CODAMOSA chỉ tập trung vào cải thiện test coverage, và các test case được tạo ra của nó thiếu assertion oracle để phát hiện lỗi trong chương trình.

Hai nghiên cứu bổ sung đã sử dụng Codex để đồng thời tạo code và test case tương ứng dựa trên mô tả vấn đề đã cho. Sau đó, họ sử dụng những test case này để lọc ra các đề xuất buggy được tạo bởi Codex. Để tạo code, họ sử dụng mô tả vấn đề như một prompt, và để tạo test case, họ sử dụng cùng mô tả vấn đề cùng với PUT và hướng dẫn ngôn ngữ tự nhiên.

Mặc dù nghiên cứu trước đây đã khám phá các chiến lược đa dạng để tạo test case bằng cách sử dụng LLM như Codex và đánh giá chúng về test coverage hoặc khớp từ vựng với ground truth test, không có nghiên cứu nào trong số này đặc biệt tập trung vào việc tận dụng MT để tăng cường hiệu quả của các test case được tạo ra.

## 8. Kết luận

Trong bài báo này, chúng tôi đã đề xuất MuTAP như một phương tiện cải thiện và đánh giá khả năng của LLM được huấn luyện trước để tạo ra test case hiệu quả. MuTAP đầu tiên prompt LLMC của nó để tạo test case bằng cách sử dụng học zero-shot và few-shot. Sau khi xác định và sửa chữa bất kỳ lỗi cú pháp và giá trị trả về tiềm ẩn nào trong các test case được tạo ra, MuTAP đánh giá hiệu quả của chúng bằng cách tiến hành MT. Sau đó, nó sử dụng các mutant sống sót của mỗi PUT, nếu có, cũng như test case ban đầu không đầy đủ để bổ sung prompt ban đầu. Nó re-prompt LLMC của mình bằng cách sử dụng augmented prompt để tái tạo test case mới có khả năng phát hiện mutant sống sót.

Chúng tôi đã đánh giá hiệu quả của test case được tạo bởi LLM để xác định lỗi trong các chương trình buggy thực và tổng hợp. Trung bình, test case được tạo bởi MuTAP thành công xác định 86.72% code buggy trong một benchmark sửa chữa lỗi khi sử dụng LLM được thiết kế cho tạo code, Codex. Khi sử dụng LLM với thiết lập dialog, llama-2-chat, MuTAP cải thiện thêm hiệu suất của nó, phát hiện 94.06% code buggy, vượt trội hơn cả công cụ tạo test tự động và kỹ thuật học zero-shot và few-shot trên LLM. Điều này nhấn mạnh lợi thế của việc sử dụng LLM làm cốt lõi của công cụ tạo test tự động, vì các công cụ tạo tự động thông thường như Pynguin thiếu quyền truy cập vào những insight được nhúng trong mutant sống sót.

Mặc dù phiên bản hiện tại của MuTAP sử dụng hai LLM khác nhau để tạo test case cho các chương trình Python, thiết kế và phương pháp đánh giá của nó về cơ bản có thể thích ứng với các ngôn ngữ lập trình và mô hình khác nhau. Do đó, như công việc tương lai, nó có thể dễ dàng được mở rộng để bao gồm các ngôn ngữ lập trình khác hoặc kết hợp LLM mới.
