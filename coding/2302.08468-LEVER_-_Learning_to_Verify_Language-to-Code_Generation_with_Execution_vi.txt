# 2302.08468.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2302.08468.pdf
# Kích thước tệp: 1891989 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LEVER : Học cách Xác minh Sinh code từ Ngôn ngữ với Thực thi
Ansong Ni1†Srini Iyer2Dragomir Radev1Ves Stoyanov2Wen-tau Yih2Sida I. Wang2 *Xi Victoria Lin2 *

Tóm tắt
Sự ra đời của các mô hình ngôn ngữ lớn được huấn luyện trên 
code (code LLMs) đã dẫn đến tiến bộ đáng kể trong sinh code 
từ ngôn ngữ tự nhiên. Các phương pháp tiên tiến nhất trong 
lĩnh vực này kết hợp giải mã LLM với việc cắt tỉa mẫu và 
xếp hạng lại sử dụng các test case hoặc heuristics dựa trên 
kết quả thực thi. Tuy nhiên, việc thu thập test cases cho 
nhiều ứng dụng thực tế của ngôn ngữ-sang-code là thách thức, 
và heuristics không thể nắm bắt tốt các đặc trưng ngữ nghĩa 
của kết quả thực thi, như kiểu dữ liệu và phạm vi giá trị, 
vốn thường chỉ ra tính đúng đắn của chương trình. Trong 
công trình này, chúng tôi đề xuất LEVER , một phương pháp 
đơn giản để cải thiện sinh code từ ngôn ngữ bằng cách học 
xác minh các chương trình được sinh ra với kết quả thực thi 
của chúng. Cụ thể, chúng tôi huấn luyện các verifier để xác 
định xem một chương trình được lấy mẫu từ LLMs có đúng hay 
không dựa trên đầu vào ngôn ngữ tự nhiên, chính chương trình 
đó và kết quả thực thi của nó. Các chương trình được lấy mẫu 
được xếp hạng lại bằng cách kết hợp điểm xác minh với xác 
suất sinh của LLM, và marginalize trên các chương trình có 
cùng kết quả thực thi. Trên bốn dataset thuộc các lĩnh vực 
table QA, math QA và lập trình Python cơ bản, LEVER liên 
tục cải thiện so với các code LLMs cơ sở (4.6% đến 10.9% 
với code-davinci-002) và đạt được kết quả state-of-the-art 
mới trên tất cả chúng.

1. Giới thiệu
Khả năng ánh xạ ngôn ngữ tự nhiên sang code thực thi được 
là nền tảng của nhiều ứng dụng AI như giao diện cơ sở dữ 
liệu (Pasupat & Liang, 2015; Yu et al., 2018; Shi et al., 
2020), điều khiển robot (Zhou et al., 2021; Shridhar et al., 
2020) và trợ lý ảo (Agashe et al., 2019; Lai et al., 2022). 
Những tiến bộ gần đây về các mô hình ngôn ngữ lớn (LLMs) 
(Brown et al., 2020; Wei et al., 2021; Chowdhery et al., 
2022), đặc biệt là những mô hình được pre-train trên code 
(code LLMs) (Chen et al., 2021a; Fried et al., 2022; Nijkamp 
et al., 2022; Li et al., 2022a), đã cho thấy tiềm năng lớn 
trong các tác vụ này với học few-shot trong ngữ cảnh (Shi 
et al., 2022; Chen et al., 2022a; Zhang et al., 2022). Tuy 
nhiên hiệu suất của chúng vẫn còn xa hoàn hảo (Chen et al., 
2021a). Xét đến chi phí tính toán để finetune các mô hình 
như vậy, việc khám phá các cách để cải thiện chúng mà không 
thay đổi tham số là hấp dẫn.

Một quan sát then chốt là trong khi LLMs gặp khó khăn với 
độ chính xác trong setting few-shot, nó thường tạo ra đầu 
ra đúng khi đủ số mẫu được rút ra. Các công trình trước đây 
đã cho thấy rằng majority voting và lọc bằng test cases có 
thể tăng đáng kể hiệu suất của chúng khi các mẫu được rút 
ra ở quy mô lớn (Chen et al., 2021a; Austin et al., 2021; 
Li et al., 2022a). Shen et al. (2021) và Cobbe et al. (2021) 
tiếp tục chứng minh hiệu quả của việc huấn luyện một verifier 
và sử dụng điểm xác minh để xếp hạng lại các giải pháp ứng 
viên cho các bài toán toán học thế giới. So với các phương 
pháp chỉ dựa vào tính nhất quán thực thi và cắt tỉa lỗi, 
các verifier được huấn luyện có thể sử dụng các đặc trưng 
ngữ nghĩa phong phú trong các giải pháp mô hình, như kiểu 
dữ liệu, phạm vi giá trị, và thuộc tính biến, có thể là chỉ 
báo mạnh về tính đúng đắn của các chương trình. Trong khi 
Cobbe et al. (2021) và các công trình tiếp theo (Li et al., 
2022b; Kadavath et al., 2022) tập trung vào xác minh các 
giải pháp ngôn ngữ tự nhiên bởi LMs, một câu hỏi tự nhiên 
là liệu cùng phương pháp có thể được áp dụng cho các giải 
pháp chương trình hay không.

Trong công trình này, chúng tôi đề xuất học xác minh (LEVER 
🌏🤖) sinh code từ ngôn ngữ bởi các code LLMs, với sự giúp 
đỡ của thực thi. Cụ thể hơn, chúng tôi huấn luyện một verifier 
học cách phân biệt và từ chối các chương trình không đúng 
dựa trên biểu diễn kết hợp của mô tả ngôn ngữ tự nhiên, 
dạng bề mặt chương trình và kết quả thực thi của nó. Chúng 
tôi tiếp tục kết hợp xác suất xác minh với xác suất sinh của 
LLM và marginalize trên các chương trình có cùng kết quả 
thực thi. Chúng tôi sử dụng xác suất tổng hợp này làm điểm 
xếp hạng lại và xuất ra các chương trình thực thi để có kết 
quả có xác suất cao nhất.

Chúng tôi tiến hành các thí nghiệm mở rộng trên bốn benchmark 
ngôn ngữ-sang-code khác nhau thuộc các lĩnh vực text-to-SQL 
semantic parsing, table QA, math reasoning và lập trình 
Python cơ bản. Kết quả thí nghiệm với ba code LLMs khác 
nhau cho thấy LEVER liên tục cải thiện độ chính xác thực thi 
của các chương trình được sinh ra. Đáng chú ý, LEVER kết 
hợp với code-davinci-002 cải thiện so với các baseline mạnh 
sử dụng cắt tỉa lỗi thực thi từ 4.6% đến 10.9%, và đạt được 
kết quả state-of-the-art mới trên tất cả bốn benchmark, mà 
không sử dụng kiến trúc mô hình cụ thể cho tác vụ hoặc phương 
pháp prompting. Các nghiên cứu ablation cho thấy kết quả 
thực thi là then chốt cho việc xác minh và LEVER cũng mang 
lại cải thiện đáng kể trong các setting ít tài nguyên và 
weakly-supervised.12

2. Phương pháp
Bây giờ chúng tôi giới thiệu công thức chi tiết và quy trình 
huấn luyện của LEVER. Các thành phần chính được minh họa 
trong Hình 1.

2.1. Sinh Code từ Ngôn ngữ với Code LLMs
Đầu vào cho một tác vụ ngôn ngữ-sang-code thường bao gồm 
mô tả ngôn ngữ tự nhiên (NL) và tùy chọn một số ngữ cảnh 
lập trình (ví dụ, kho dữ liệu, assertions, v.v.). Chúng ta 
ký hiệu đầu vào như vậy là x. Cho x, một mô hình sinh 
P(y|x) sinh ra một chương trình y sau đó được thực thi thông 
qua một executor E(·) để thu được kết quả3 E(y). Đối với 
học few-shot với LMs lớn, việc sinh cũng thường được điều 
kiện trên một tập cố định m exemplars, {(xi, yi)}i<m. Do 
đó sinh code từ ngôn ngữ few-shot với code LLMs có thể được 
công thức hóa như:

PLM(y|x) = P(y|prompt(x,{(xi, yi)}i<m)), (1)

trong đó prompt(x,{(xi, yi)}i<m) là một biểu diễn chuỗi 
của đầu vào tổng thể. Tìm kiếm greedy thường được sử dụng 
để tìm chương trình có xác suất sinh (gần đúng) cao nhất, 
tức là, ˆygreedy ≈ arg max y PLM(y|x).

2.2. Xếp hạng lại các Ứng viên Chương trình
Quan sát then chốt thúc đẩy phương pháp của chúng tôi là 
một tập mẫu có kích thước hợp lý từ PLM(y|x) thường bao 
gồm các chương trình đúng. Điều này gợi ý rằng xếp hạng lại 
các ứng viên chương trình có thể mang lại cải thiện kết quả 
đáng kể. Ý tưởng của xếp hạng lại discriminative (Shen et al., 
2004; Collins & Koo, 2005) là học một hàm scoring R(x,ˆy) 
đo lường khả năng ˆy là đầu ra tốt nhất cho đầu vào x. Cho 
R(·), bộ xếp hạng lại xuất ra chương trình có điểm xếp hạng 
lại cao nhất trong tập ứng viên S:

ˆyrerank = arg max ˆy∈S R(x,ˆy) (2)

Tiếp theo chúng tôi giới thiệu cách chúng tôi áp dụng một 
verifier được huấn luyện để xác minh và xếp hạng lại các 
ứng viên chương trình được lấy mẫu từ code LLMs sao cho 
ˆyrerank tốt hơn ˆygreedy.

Lấy mẫu Chương trình từ Code LLMs. Cho đầu vào x, thay vì 
thực hiện tìm kiếm greedy, chúng tôi thu được k chương trình 
từ PLM(y|x) với temperature sampling, tức là, {ˆyi}k i=1∼ 
PLM(y|x). Vì cùng chương trình có thể được lấy mẫu nhiều 
hơn một lần, chúng tôi thực hiện deduplication để tạo thành 
một tập n ứng viên chương trình duy nhất S={ˆyi}n i=1, trong 
đó n≤k. Chúng tôi chọn làm sampling thay vì beam search chủ 
yếu vì hai lý do: 1) công trình gần đây cho thấy rằng beam 
search cho sinh code thường dẫn đến hiệu suất tồi tệ hơn do 
các chương trình degenerated (Austin et al., 2021; Zhang et al., 
2022); và 2) beam search không có sẵn hoặc được triển khai 
hiệu quả cho tất cả LLMs mà chúng tôi test trên (ví dụ, Codex).

Xác minh với Thực thi. Chúng tôi sử dụng một phép nối đơn 
giản của mô tả vấn đề x, chương trình ứng viên ˆy và một 
biểu diễn của kết quả thực thi E(ˆy) làm đầu vào cho bộ 
xếp hạng lại. Lấy cảm hứng từ công trình gần đây (Cobbe et al., 
2021; Li et al., 2022b), chúng tôi tham số hóa bộ xếp hạng 
lại discriminative của chúng tôi như một mô hình xác minh 
(tức là, phân loại nhị phân) Pθ(v|x,ˆy,E(ˆy)), trong đó 
v∈ {0,1}. Trong thực tế, bộ xếp hạng lại có thể được triển 
khai sử dụng bất kỳ kiến trúc phân loại nhị phân nào. Chúng 
tôi báo cáo các thí nghiệm sử dụng T5 (Raffel et al., 2020) 
và RoBERTa (Liu et al., 2019) trong §B.2.

Cho một đầu vào x và một chương trình ứng viên ˆy∈S, chúng 
tôi thu được xác suất xếp hạng lại như xác suất kết hợp của 
sinh và vượt qua xác minh:

PR(ˆy, v=1|x) = PLM(ˆy|x)·Pθ(v=1|x,ˆy,E(ˆy)) (3)

Tổng hợp Kết quả Thực thi. Vì các chương trình có cùng 
ngữ nghĩa có thể có các dạng bề mặt khác nhau, chúng tôi 
tiếp tục tổng hợp xác suất xếp hạng lại của các chương trình 
trong S thực thi cùng kết quả. Bằng cách này, chúng tôi nới 
lỏng sự phụ thuộc vào dạng bề mặt và tập trung vào kết quả 
thực thi thay thế. Hàm scoring cuối cùng cho xếp hạng lại 
do đó là:

R(x,ˆy) = PR(E(ˆy), v=1|x) = ∑ y∈S,E(y)=E(ˆy) PR(y, v=1|x)

Vì có thể có nhiều chương trình chia sẻ cùng kết quả thực 
thi có xác suất cao nhất, chúng tôi phá vỡ tie ngẫu nhiên 
trong trường hợp này khi xuất ra các chương trình.

2.3. Học các Verifiers
Các phần trước mô tả cách sử dụng một verifier tại thời 
điểm suy luận. Tiếp theo chúng tôi giới thiệu quy trình huấn 
luyện của nó.

Tạo Dữ liệu Huấn luyện. Đối với các dataset ngôn ngữ-sang-code, 
mỗi ví dụ thường là một triplet (x, y∗, z∗), trong đó 
z∗=E(y∗) là kết quả thực thi vàng và y∗ là chương trình 
vàng. Vì việc chú thích các chương trình đòi hỏi chuyên môn 
lĩnh vực, đối với một số dataset mà kết quả cuối cùng có 
thể được thu thập trực tiếp, chỉ z∗ nhưng không có y∗ được 
cung cấp để học (Artzi & Zettlemoyer, 2013; Cheng & Lapata, 
2018; Goldman et al., 2018). Điều này được biết đến như 
setting weakly-supervised. Để thu thập dữ liệu huấn luyện, 
chúng tôi thu được một tập n ứng viên chương trình duy nhất 
ˆS={ˆyi}n i=1 cho mỗi đầu vào x trong tập huấn luyện, bằng 
cách trước tiên lấy mẫu k chương trình từ PLM(ˆy|x) và sau 
đó loại bỏ tất cả các chương trình trùng lặp, tương tự như 
thời điểm suy luận. Sau đó cho mỗi ứng viên chương trình 
ˆy∈S, chúng tôi thu được nhãn xác minh nhị phân của nó bằng 
cách so sánh kết quả thực thi ˆz=E(ˆy) với kết quả thực thi 
vàng4 z∗, tức là, v= 1(ˆz=z∗). Đối với các dataset chứa 
chương trình vàng y∗, chúng tôi thêm (x, y∗, z∗, v=1) như 
một ví dụ huấn luyện xác minh bổ sung, và chúng tôi bỏ qua 
bước này cho các dataset weakly-supervised. Bằng cách này, 
chúng tôi tạo ra một tập các ví dụ huấn luyện xác minh 
{(x,ˆyi,ˆzi, vi)|ˆyi∈S} cho mỗi đầu vào x.

Mục tiêu Học. Cho tập các ví dụ huấn luyện xác minh này, 
chúng tôi công thức hóa loss cho đầu vào x với hàm negative 
log-likelihood, được chuẩn hóa bởi số lượng ứng viên chương 
trình

Lθ(x, S) = −1/|S|·∑ ˆyi∈S log Pθ(vi|x,ˆyi,ˆzi) (4)

Bước chuẩn hóa là quan trọng để ngăn một ví dụ có số lượng 
lớn ứng viên chương trình duy nhất thống trị việc học.

3. Thiết lập Thí nghiệm
3.1. Datasets
Chúng tôi tiến hành thí nghiệm trên bốn dataset ngôn ngữ-sang-code 
thuộc các lĩnh vực semantic parsing, table QA, math reasoning 
và lập trình python cơ bản. Các setting chính của bốn dataset 
này được hiển thị trong Bảng 1. Các setting chi tiết hơn 
cho xác minh trong Bảng 7 của Phụ lục.

▷Spider (Yu et al., 2018) là một dataset semantic parsing 
về sinh truy vấn SQL từ câu hỏi ngôn ngữ tự nhiên. Với 7k 
dữ liệu huấn luyện song song, nó cũng lý tưởng cho finetuning 
generators;

▷WikiTableQuestions (WikiTQ) (Pasupat & Liang, 2015) là 
một dataset trả lời câu hỏi bảng, mà chúng tôi cố gắng giải 
quyết bằng cách sinh và thực thi các truy vấn SQL trên các 
bảng nguồn. Chúng tôi sử dụng các bảng được tiền xử lý từ 
Shi et al. (2020) và áp dụng các truy vấn SQL được chú thích 
của họ để thêm chương trình vàng cho dataset ban đầu 
weakly-supervised;

▷GSM8k (Cobbe et al., 2021) là một benchmark để giải các 
bài toán từ toán học cấp độ tiểu học. Theo công trình trước 
đây (Chowdhery et al., 2022; Chen et al., 2022b; Gao et al., 
2022), chúng tôi tiếp cận benchmark này bằng cách sinh các 
chương trình Python từ câu hỏi trong NL, vốn sẽ tạo ra câu 
trả lời đúng khi thực thi. Dataset gốc chỉ có ngôn ngữ tự 
nhiên và không có giải pháp chương trình, do đó nó 
weakly-supervised cho ngôn ngữ-sang-code;

▷MBPP (Austin et al., 2021) chứa các chương trình lập trình 
Python cơ bản được phát biểu bằng ngôn ngữ tự nhiên. Mỗi 
ví dụ được trang bị 3 test cases để kiểm tra tính đúng đắn 
của các chương trình. Theo công trình trước đây (Shi et al., 
2022; Zhang et al., 2022), chúng tôi sử dụng test case đầu 
tiên như một phần của prompt cho mô hình để sinh chữ ký 
hàm đúng và sử dụng cả ba để đánh giá tính đúng đắn.

3.2. Code LLMs
Chúng tôi đánh giá LEVER với ba code LLMs khác nhau:

▷Codex (Chen et al., 2021a) là một họ code LLMs có các 
kích thước khác nhau được phát triển bởi OpenAI. Cụ thể, 
chúng tôi sử dụng API code-davinci-0025 thông qua Python 
bindings chính thức của nó.

▷InCoder (Fried et al., 2022) là một họ code LLMs lên đến 
6B tham số được huấn luyện trên một corpus lớn code với 
giấy phép permissive. Chúng tôi thí nghiệm với InCoder-6B 
và sử dụng nó cho sinh từ trái sang phải.

▷CodeGen (Nijkamp et al., 2022) là một họ code LLMs và 
chúng tôi đánh giá phiên bản CodeGen-16B-multi. Mặc dù 
các tệp SQL không được bao gồm trong corpus huấn luyện 
cho CodeGen, chúng tôi thấy nó vẫn hoạt động khá tốt trên 
các tác vụ sinh SQL có thể vì các truy vấn SQL được trộn 
lẫn với các tệp nguồn của các ngôn ngữ lập trình khác.

3.3. Baselines và Metric Đánh giá
Baselines. Chúng tôi so sánh LEVER với các phương pháp 
baseline sau đây để sinh chương trình sử dụng code LLMs.

▷Greedy: Chọn token có khả năng cao nhất mỗi bước giải mã.

▷Maximum Likelihood (ML): Từ k ứng viên chương trình được 
lấy mẫu, chọn chương trình có log-probability sinh cao nhất, 
tức là, log PLM(ˆy|x) (hoặc log-probability sinh chuẩn hóa 
như log PLM(ˆy|x)/|ˆy|). Chúng tôi xác định thực nghiệm 
sử dụng tập phát triển có nên sử dụng xác suất chuẩn hóa 
cho mỗi dataset. Chi tiết hơn có thể được tìm thấy trong 
Phụ lục A.

▷Error Pruning + ML (EP + ML): Cắt tỉa các ứng viên chương 
trình có lỗi thực thi; sau đó chọn chương trình có maximum 
likelihood;

▷Error Pruning + Voting (EP + Voting): Bỏ phiếu đa số trên 
kết quả thực thi trong các chương trình không lỗi, và chọn 
kết quả thực thi được bỏ phiếu nhiều nhất và các chương 
trình tương ứng của nó.

Chúng tôi tập trung so sánh với baseline EP+ML, vì đây là 
một phương pháp xếp hạng lại đơn giản khai thác thực thi 
và mang lại kết quả cạnh tranh nhất quán trên các dataset 
và code LLMs khác nhau.

Metric đánh giá. Theo công trình trước đây (Xie et al., 2022; 
Liu et al., 2021; Ni et al., 2022; Zhang et al., 2022), chúng 
tôi sử dụng độ chính xác thực thi làm metric đánh giá chính 
cho tất cả datasets, đo lường tỷ lệ phần trăm các ví dụ 
mang lại kết quả thực thi vàng hoặc vượt qua tất cả test cases.

3.4. Chi tiết Triển khai
Huấn luyện verifier. Chúng tôi tạo dữ liệu huấn luyện xác 
minh bằng cách lấy mẫu từ LLMs trên tập huấn luyện, sử 
dụng ngân sách lấy mẫu được mô tả trong Bảng 1. Thống kê 
thêm của dữ liệu huấn luyện kết quả có thể được tìm thấy 
trong Bảng 7 trong Phụ lục. Khi học các verifiers, như được 
hiển thị trong Eq. 4, loss huấn luyện được tính bằng cách 
lấy trung bình trên tất cả các mẫu chương trình cho mỗi ví 
dụ. Vì chúng tôi batch các mẫu chương trình cho cùng ví dụ 
lại với nhau, kích thước batch hiệu quả cũng sẽ được nhân 
với kích thước mẫu. Điều này có thể gây vấn đề khi kích 
thước mẫu trở nên lớn (lên đến 100 trong thí nghiệm của 
chúng tôi) vì chúng có thể không thể vừa vào bộ nhớ GPU 
cùng lúc. Do đó, chúng tôi down-sample các chương trình 
được sử dụng để học mỗi ví dụ trong mỗi iteration. Việc 
down-sampling ngẫu nhiên xảy ra ở đầu mỗi epoch huấn 
luyện để các verifiers có thể thấy các chương trình khác 
nhau mỗi epoch. Kích thước batch chi tiết và hệ số 
downsampling có thể được tìm thấy trong Bảng 7 trong Phụ lục.

Biểu diễn kết quả thực thi. Đầu vào cho verifier là một 
phép nối của đầu vào tác vụ, ứng viên chương trình và kết 
quả thực thi của nó. Đối với Spider và WikiTQ, chúng tôi 
sử dụng các bảng kết quả được tuyến tính hóa từ thực thi 
SQL làm kết quả thực thi. Đối với GSM8k, chúng tôi sử 
dụng giá trị của biến có tên "answer" sau khi thực thi chương 
trình làm kết quả thực thi. Đối với MBPP, chúng tôi sử 
dụng kiểu và giá trị (được cast thành chuỗi) được trả về 
bởi các hàm. Tất cả lỗi thực thi được biểu diễn như "ERROR: 
[reason]", chẳng hạn như "ERROR: Time out". Ví dụ về 
những đầu vào verifier này cho các dataset khác nhau có 
thể được tìm thấy trong Bảng 11.

Lựa chọn mô hình verifier. Chúng tôi sử dụng tập phát triển 
để chọn mô hình verifier tốt nhất. Chúng tôi chọn T5-base 
cho Spider, T5-large cho WikiTQ và MBPP, và RoBERTa-large 
cho GSM8k làm LM cơ sở cho các verifiers sử dụng trong 
các thí nghiệm chính6. Quy trình lựa chọn được chi tiết 
trong §B.2. Đối với các mô hình T5 (Raffel et al., 2020), 
chúng tôi huấn luyện chúng để xuất ra token "yes/no" cho 
mỗi ví dụ tích cực/tiêu cực được cho đầu vào verifier, và 
chúng tôi lấy xác suất sinh "yes" làm xác suất xác minh 
trong suy luận. Đối với RoBERTa (Liu et al., 2019), chúng 
tôi thêm một lớp tuyến tính trên đỉnh head [CLS], theo thực 
hành tiêu chuẩn của phân loại chuỗi với các mô hình chỉ-encoder 
(Devlin et al., 2019).

Chi tiết về lấy mẫu LLM, xây dựng prompt few-shot và thiết 
lập cụ thể cho dataset có thể được tìm thấy trong Phụ lục A.

4. Kết quả Chính
Chúng tôi hiển thị hiệu suất của LEVER kết hợp với Codex-Davinci 
và so sánh nó với hiệu suất finetuning và few-shot state-of-the-art 
từ công trình trước đây cho Spider (Bảng 2), WikiTQ (Bảng 3), 
GSM8k (Bảng 4) và MBPP (Bảng 5). Ngoài ra, chúng tôi cũng 
đánh giá LEVER với các mô hình InCoder và CodeGen trên Spider 
và GSM8k (Bảng 6).

4.1. Hiệu quả của LEVER.
LEVER liên tục cải thiện hiệu suất của tất cả code LLMs 
trên tất cả tác vụ, mang lại cải thiện từ 6.6% (Spider) đến 
17.3% (WikiTQ) so với các baseline giải mã greedy cho 
Codex-Davinci. Đối với các mô hình yếu hơn như InCoder 
và CodeGen, chúng tôi quan sát cải thiện lên đến 30.0% cho 
Spider và 15.0% cho GSM8k. Hơn nữa, LEVER kết hợp với 
Codex-Davinci cũng đạt được kết quả state-of-the-art mới 
trên tất cả bốn datasets, với cải thiện từ 1.2% (WikiTQ) 
đến 2.0% (MBPP). Trên dataset text-to-SQL thách thức, 
Spider, nơi state-of-the-art trước đây được đạt được bằng 
cách finetuning một mô hình T5-3B được tăng cường với 
relational-aware self-attention, chúng tôi đạt được kết quả 
tốt hơn với Codex-Davinci + LEVER, nơi verifier được finetuned 
sử dụng mô hình T5-base. LEVER cũng cải thiện kết quả 
tốt nhất trước đây trên Spider sử dụng InCoder và CodeGen, 
lần lượt 13.2% và 20.6%.

5. Phân tích
5.1. Mở rộng Ví dụ Huấn luyện
Chúng tôi hiển thị cách hiệu suất của LEVER thay đổi với 
ít ví dụ huấn luyện hơn trong Hình 3, sử dụng Spider làm 
ví dụ. Kết quả thêm trên WikiTQ và GSM8k trong §B.3. 
Các cải thiện với LEVER so với LLMs cơ sở vẫn nhất quán 
ngay cả khi chỉ có 250 ví dụ được đưa ra, với cải thiện từ 
1.7% đến 10.0% trên các datasets và LLMs khác nhau. Điều 
này cho thấy LEVER có thể hoạt động trong các setting ít 
tài nguyên. Hơn nữa, xu hướng cũng khác nhau cho các 
datasets và code LLMs khác nhau, ví dụ, khi sử dụng Codex 
làm LLM, hiệu suất của LEVER giảm 6.4% cho WikiTQ và 
chỉ 3.2% cho Spider. Tuy nhiên, cũng trên Spider, hiệu 
suất giảm 6.9% và 5.3% cho InCoder và CodeGen. Điều này 
cho thấy có nhiều ví dụ huấn luyện hơn cho LEVER có tác 
động lớn hơn đối với các datasets khó hơn và LMs yếu hơn.

Với Hình 3, chúng tôi cũng so sánh hiệu suất của LEVER 
với các mô hình T5 được finetuned trực tiếp để sinh cho 
cùng số lượng ví dụ huấn luyện. Trong khi xác minh có thể 
được học với chỉ hàng trăm ví dụ, hiệu suất của các mô 
hình T5 finetuned giảm mạnh khi có ít ví dụ huấn luyện 
hơn. Như một ví dụ, cho 500 ví dụ, một verifier T5-base 
trên InCoder/CodeGen vượt trội hơn generator T5-3B finetuned 
khoảng 7%.

5.2. Mở rộng Kích thước Mẫu
Vì việc rút mẫu từ LLMs có thể tốn kém về mặt tính toán, 
ở đây chúng tôi nghiên cứu cách kích thước mẫu trong thời 
gian huấn luyện và suy luận ảnh hưởng đến hiệu suất. Như 
chúng ta có thể thấy từ Hình 4a, trong thời gian suy luận, 
khi giảm kích thước mẫu từ 50 xuống 10 chương trình mỗi 
ví dụ, hiệu suất của LEVER giảm 1.8% (Spider) đến 5.2% 
(WikiTQ). Điều này chỉ ra rằng LEVER nhạy cảm với kích 
thước mẫu tại thời điểm suy luận, điều này được mong đợi 
vì nó cũng ảnh hưởng mạnh đến kết quả oracle (tức là, 
upper-bound cho xếp hạng lại). Để so sánh, Hình 4b cho 
thấy LEVER rất không nhạy cảm với kích thước mẫu để 
cung cấp dữ liệu huấn luyện, với khoảng cách hiệu suất 
đều dưới 1% cho ba datasets. Tổng thể, kết quả cho thấy 
ngân sách lấy mẫu cao hơn giúp ích hơn tại thời điểm test.

5.3. Hiệu chuẩn Verifier và Generator
Chúng tôi nghiên cứu mức độ hiệu chuẩn tốt của verifier 
và generator trong việc nhận diện các chương trình đúng. 
Lý tưởng, các mẫu chương trình đúng sẽ được đưa ra xác 
suất cao hơn do đó chúng ta sẽ quan sát tỷ lệ phần trăm 
cao hơn các chương trình đúng khi nó gần với đỉnh. Để 
làm điều này, chúng tôi sắp xếp điểm dự đoán của verifier, 
generator và LEVER (như trong Eq. 3), và di chuyển ngưỡng 
percentile và đo lường tỷ lệ phần trăm chương trình đúng 
trong các chương trình được xếp hạng cao. Theo Hình 5, 
các verifiers nói chung được hiệu chuẩn tốt hơn so với 
generators, đặc biệt khi ngưỡng ở percentiles thấp hơn. 
Điều này chỉ ra rằng dễ dàng hơn cho các verifiers nhận 
diện những lỗi rõ ràng trong các chương trình với kết quả 
thực thi như một phần đầu vào của chúng. Thú vị, khi phân 
biệt giữa các chương trình được xếp hạng cao, các verifiers 
được hiệu chuẩn kém trong ba trong số bốn datasets được 
test7. Tuy nhiên, các generators nói chung được hiệu chuẩn 
tốt hơn trong vùng này, và kết hợp xác suất của verifier 
và generator mang lại kết quả tốt nhất trên tất cả bốn 
benchmarks. Cụ thể hơn, trên dataset GSM8k, nơi hiệu 
chuẩn của cả hai mô hình khá kém cho các chương trình 
được xếp hạng cao, xác suất kết hợp của chúng bất ngờ 
được hiệu chuẩn tốt, cho thấy hai mô hình bổ sung cho 
nhau trên dataset này.

5.4. Phân tích Định lượng
Chúng tôi trình bày một phân tích định lượng về lý do LEVER 
thành công hoặc thất bại trong việc cải thiện hiệu suất của 
LLMs. Theo Hình 6, khi LEVER xếp hạng lại một chương 
trình để thay thế chương trình khác có xác suất sinh cao 
hơn, thường là vì kết quả thực thi cung cấp thông tin quan 
trọng như lỗi thực thi, kiểu biến và phạm vi. Điều này 
nhất quán với phát hiện của chúng tôi trong §4.2 về tầm 
quan trọng của kết quả thực thi đối với LEVER. Cũng đáng 
chú ý rằng có những trường hợp khi LEVER vẫn có thể xếp 
hạng lại chương trình đúng khi kết quả thực thi không lỗi 
có cùng kiểu và phạm vi với chương trình greedy, tức là, 
trong danh mục "others". Giả thuyết của chúng tôi là đây 
là khi chính chương trình trở thành đặc trưng chính cho 
các verifiers khai thác. Ngoài ra, khi LEVER thất bại trong 
việc xếp hạng các chương trình đúng lên đỉnh, lý do phổ 
biến nhất là không tìm thấy chương trình đúng trong các 
mẫu (tức là, upper-bound được đạt tới), điều này đặc biệt 
là trường hợp cho các LMs yếu hơn. Lý do phổ biến thứ 
hai cho LEVER thất bại là kết quả thực thi của chương 
trình không đúng khi xếp hạng lại có cùng kiểu và phạm 
vi như chương trình đúng trong các mẫu. Trong trường hợp 
này, kết quả thực thi không cung cấp thông tin phong phú 
cho các verifiers do đó LEVER thất bại trong việc cải thiện 
code LLMs.

6. Công trình Liên quan
Sinh Code từ Ngôn ngữ. Việc dịch ngôn ngữ tự nhiên sang 
code là một thách thức lâu đời qua tất cả thời đại của 
trí tuệ nhân tạo, bao gồm các hệ thống dựa trên quy tắc 
(Woods, 1973; Templeton & Burger, 1983), dự đoán có cấu 
trúc (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; 
Gulwani & Marron, 2014) và deep learning (Xiao et al., 
2016; Dong & Lapata, 2016; Rabinovich et al., 2017; Zhong 
et al., 2017; Lin et al., 2017). Gần đây, các mô hình ngôn 
ngữ code được pre-train (Chen et al., 2021a; Wang et al., 
2021; Fried et al., 2022; Nijkamp et al., 2022; OpenAI, 
2022) đã chứng minh hiệu suất mạnh mẽ đáng ngạc nhiên 
trong vấn đề này trên các ngôn ngữ lập trình (Lin et al., 
2018; Yu et al., 2018; Austin et al., 2021; Cobbe et al., 
2021; Li et al., 2022a). Một số phương pháp được đề xuất 
để tinh chỉnh lựa chọn mẫu LLM, bao gồm thực thi test 
case (Li et al., 2022a), tương tự cross-sample (Chen et al., 
2021a; Li et al., 2022a; Shi et al., 2022) và lọc dựa trên 
maximum mutual information (Zhang et al., 2022). Công 
trình của chúng tôi đề xuất một module xác minh có thể 
học được để đánh giá đầu ra mẫu của LLMs để cải thiện 
thêm hiệu suất của chúng.

Sinh Code với Thực thi. Công trình sinh code trước đây 
đã khai thác kết quả thực thi theo những cách khác nhau. 
Các phương pháp học weakly-supervised (Berant et al., 
2013; Pasupat & Liang, 2015; Guu et al., 2017) mô hình 
hóa các chương trình như các biến tiềm ẩn và sử dụng kết 
quả thực thi để dẫn xuất tín hiệu giám sát. Kết quả thực 
thi trung gian được sử dụng để hướng dẫn tìm kiếm chương 
trình cả trong thời gian huấn luyện (Chen et al., 2019; 
2021b) và suy luận (Wang et al., 2018). Khi lấy mẫu ở 
quy mô lớn, majority voting dựa trên kết quả thực thi đã 
được cho thấy hiệu quả cho lựa chọn ứng viên (Li et al., 
2022a; Cobbe et al., 2021). Shi et al. (2022) tổng quát 
hóa nguyên tắc này bằng cách chọn các mẫu có consensus 
tối đa với các mẫu khác trong kết quả thực thi. Chúng tôi 
đề xuất huấn luyện một mô hình xác minh để đánh giá tính 
đúng đắn của sinh code có tính đến kết quả thực thi.

Học Xác minh. Công trình trước đây đã cho thấy hiệu quả 
của các verifiers được học cho lọc mẫu trong các lĩnh vực 
như math QA (Shen et al., 2021; Cobbe et al., 2021) và 
commonsense QA (Li et al., 2022b), nơi giải pháp chủ yếu 
được mô tả bằng ngôn ngữ tự nhiên. Trong khi việc huấn 
luyện các verifiers độc lập với generator phổ biến hơn 
(Cobbe et al., 2021; Li et al., 2022b), Shen et al. (2021) 
finetuned cả hai cùng lúc. Công trình trước đây cũng đã 
sử dụng các LMs cơ sở khác nhau cho các verifiers. Cobbe 
et al. (2021) sử dụng GPT-3 (Brown et al., 2020) trong 
khi Li et al. (2022b) sử dụng DeBERTa (He et al., 2020). 
Bên cạnh các verifiers cụ thể cho tác vụ, Kadavath et al. 
(2022) cho thấy các LMs lớn có thể tự xác minh đầu ra của 
chúng trong setting few-shot cho một loạt các tác vụ. Chen 
et al. (2022a) và các công trình khác (Tufano et al., 2020; 
Li et al., 2022a) sử dụng LMs để sinh test cases thay vì 
trực tiếp đánh giá tính đúng đắn của các chương trình đầu 
ra. Để so sánh, setting của LEVER gần với Li et al. (2022b) 
vì chúng tôi huấn luyện verifier riêng biệt và sử dụng LM 
nhỏ hơn nhiều cho nó (khoảng 0.5% kích thước tham số 
generator). Chúng tôi báo cáo tập đánh giá toàn diện đầu 
tiên trên các tác vụ ngôn ngữ-sang-code, sử dụng kết quả 
thực thi chương trình8.

Xếp hạng lại Discriminative. Các phương pháp xếp hạng 
lại discriminative từ lâu đã được sử dụng để cải thiện thêm 
hiệu suất của các tác vụ sinh chuỗi, bao gồm tóm tắt (Wan 
et al., 2015), dịch máy (Shen et al., 2004; Lee et al., 2021), 
sinh phản hồi đối thoại (Olabiyi et al., 2018) và gần đây 
hơn, sinh code (Yin & Neubig, 2019). LEVER có thể được 
xem như một framework xếp hạng lại discriminative.

7. Hạn chế
Trong công trình này, chúng tôi sử dụng thông tin thực thi 
để xác minh các chương trình trong LEVER. Tuy nhiên, việc 
thực thi các chương trình phụ thuộc vào ít nhất một tập 
đầu vào (ví dụ, đối số cho một hàm) và ngữ cảnh thực thi 
đầy đủ (ví dụ, cơ sở dữ liệu), có thể không được cung cấp 
cho một số ứng dụng nhất định. Hơn nữa, chúng ta không 
thể luôn giả định rằng các chương trình được sinh bởi mô 
hình là an toàn để thực thi. Ngoài ra, chúng tôi sử dụng 
PASS@1 làm metric đánh giá chính trong các thí nghiệm. 
Trong khi nó lý tưởng cho các ứng dụng như text-to-SQL 
và math reasoning nơi người dùng chỉ tìm kiếm câu trả 
lời cho câu hỏi của họ, các metrics như PASS@k hoặc N@k 
có thể cung cấp góc nhìn khác nhau cho các tác vụ lập 
trình tổng quát như MBPP.

8. Kết luận
Chúng tôi đề xuất LEVER, một phương pháp đơn giản để 
cải thiện code LLMs trên các tác vụ ngôn ngữ-sang-code, 
bằng cách học các mô hình xác minh riêng biệt để đánh 
giá tính đúng đắn của các chương trình được sinh ra, có 
tính đến kết quả thực thi của chúng. Chúng tôi cho thấy 
rằng có thể huấn luyện các verifiers khoảng 0.5% kích 
thước của generators sử dụng các dataset benchmark có 
giám sát. Thay vì trực tiếp thực hiện rejection sampling 
dựa trên đầu ra verifier, chúng tôi cho thấy tốt hơn là 
trộn xác suất sinh và xác minh cho xếp hạng lại mẫu. 
LEVER liên tục cải thiện hiệu suất của code LLMs trên bốn 
tác vụ ngôn ngữ-sang-code, và đạt được kết quả state-of-the-art 
mới trên tất cả chúng. Phân tích thêm cho thấy kết quả 
thực thi chương trình là then chốt cho xác minh và phương 
pháp được đề xuất có thể tổng quát hóa trên các LLMs khác nhau.

Lời cảm ơn
Các tác giả muốn cảm ơn Xi Ye, Tianyi Zhang, Mengzhou 
Xia, Luke Zettlemoyer, và các nhà đánh giá ẩn danh cho 
cuộc thảo luận và bình luận hữu ích.

Tài liệu tham khảo
[Tôi sẽ dịch phần References nếu cần, nhưng nó rất dài và chủ yếu là các tên riêng và tiêu đề công trình khoa học]
