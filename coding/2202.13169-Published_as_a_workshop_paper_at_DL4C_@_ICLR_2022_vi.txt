# 2202.13169.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2202.13169.pdf
# Kích thước tệp: 548945 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
ĐÁNH GIÁ HỆ THỐNG CÁC MÔ HÌNH NGÔN NGỮ LỚN
CỦA MÃ NGUỒN
Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn
Trường Khoa học Máy tính
Đại học Carnegie Mellon
ffangzhex,ualon,gneubig g@cs.cmu.edu, vhellendoorn@cmu.edu
TÓM TẮT
Các mô hình ngôn ngữ lớn (LM) của mã nguồn gần đây đã cho thấy tiềm năng to lớn
trong việc hoàn thành mã và tổng hợp mã từ các mô tả ngôn ngữ tự nhiên.
Tuy nhiên, các LM mã nguồn tiên tiến nhất hiện tại (ví dụ: Codex (Chen et al., 2021))
không được công khai, để lại nhiều câu hỏi về các quyết định thiết kế mô hình và dữ liệu của chúng. Chúng tôi nhằm mục đích lấp đầy một số khoảng trống này thông qua việc đánh giá có hệ thống các mô hình lớn nhất hiện có: Codex, GPT-J, GPT-Neo, GPT-NeoX-
20B, và CodeParrot, trên nhiều ngôn ngữ lập trình khác nhau. Mặc dù bản thân Codex
không phải là mã nguồn mở, chúng tôi phát hiện rằng các mô hình mã nguồn mở hiện có đạt được kết quả gần tương đương trong một số ngôn ngữ lập trình, mặc dù chủ yếu được nhắm mục tiêu cho việc mô hình hóa ngôn ngữ tự nhiên. Chúng tôi tiếp tục xác định một phần quan trọng còn thiếu dưới dạng một mô hình mã nguồn mở lớn được đào tạo độc quyền trên một tập dữ liệu đa ngôn ngữ của mã nguồn.
Chúng tôi phát hành một mô hình mới, PolyCoder, với 2,7 tỷ tham số dựa trên kiến trúc GPT-2, được đào tạo trên 249GB mã nguồn trên 12 ngôn ngữ lập trình
trên một máy duy nhất. Trong ngôn ngữ lập trình C, PolyCoder vượt trội hơn
tất cả các mô hình bao gồm cả Codex. Các mô hình được đào tạo của chúng tôi là mã nguồn mở và có sẵn công khai tại https://github.com/VHellendoorn/Code-LMs, cho phép nghiên cứu và ứng dụng trong tương lai trong lĩnh vực này.

1 GIỚI THIỆU
Các mô hình ngôn ngữ (LM) gán xác suất cho các chuỗi token, và được ứng dụng rộng rãi trong văn bản ngôn ngữ tự nhiên (Bengio et al., 2003; Baevski & Auli, 2018; Brown et al., 2020). Gần đây,
LM đã cho thấy hiệu suất ấn tượng trong việc mô hình hóa cả mã nguồn, được viết bằng các ngôn ngữ lập trình (Hindle et al., 2016; Hellendoorn & Devanbu, 2017; Alon et al., 2020; Karampatsis
et al., 2020). Những mô hình này xuất sắc trong các nhiệm vụ hạ nguồn hữu ích như hoàn thành mã (Raychev et al.,
2014) và tổng hợp mã từ các mô tả ngôn ngữ tự nhiên (Desai et al., 2016). Các mô hình ngôn ngữ lớn tiên tiến nhất hiện tại cho mã nguồn, chẳng hạn như Austin et al. (2021), đã cho thấy tiến bộ đáng kể cho việc hỗ trợ lập trình dựa trên AI. Đáng chú ý nhất, một trong những mô hình lớn nhất này,
Codex (Chen et al., 2021) đã được triển khai trong công cụ sản xuất thực tế GitHub Copilot¹, như một trợ lý phát triển trong IDE tự động tạo mã dựa trên ngữ cảnh của người dùng.

Mặc dù có thành công lớn của các mô hình ngôn ngữ lớn của mã nguồn, các mô hình mạnh nhất không được cung cấp công khai. Điều này ngăn cản việc ứng dụng các mô hình này bên ngoài các công ty có nguồn lực tốt và hạn chế nghiên cứu trong lĩnh vực này cho các tổ chức ít nguồn lực. Ví dụ, Codex cung cấp quyền truy cập không miễn phí vào đầu ra của mô hình thông qua các cuộc gọi API hộp đen², nhưng trọng số của mô hình và dữ liệu đào tạo không có sẵn. Điều này ngăn cản các nhà nghiên cứu tinh chỉnh và điều chỉnh mô hình này cho các lĩnh vực và nhiệm vụ khác ngoài hoàn thành mã. Việc thiếu quyền truy cập vào các thành phần bên trong của mô hình cũng ngăn cản cộng đồng nghiên cứu nghiên cứu các khía cạnh quan trọng khác của những mô hình này, chẳng hạn như khả năng diễn giải, chưng cất mô hình để triển khai hiệu quả hơn, và kết hợp các thành phần bổ sung như truy xuất.

Một số mô hình ngôn ngữ được đào tạo trước cỡ trung bình đến lớn có sẵn công khai, chẳng hạn như GPT-
Neo (Black et al., 2021), GPT-J (Wang & Komatsuzaki, 2021) và GPT-NeoX (Black et al., 2022).
¹https://copilot.github.com/
²https://openai.com/blog/openai-codex/
1arXiv:2202.13169v3 [cs.PL] 4 May 2022

--- TRANG 2 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
Mặc dù được đào tạo trên hỗn hợp của nhiều loại văn bản khác nhau bao gồm các bài báo tin tức, diễn đàn trực tuyến,
và chỉ một lựa chọn khiêm tốn các kho lưu trữ phần mềm (GitHub) (Gao et al., 2020), các mô hình ngôn ngữ này có thể được sử dụng để tạo mã nguồn với hiệu suất hợp lý Chen et al. (2021). Ngoài ra, có một số mô hình ngôn ngữ mã nguồn mở được đào tạo chỉ trên mã nguồn. Ví dụ, CodeParrot (Tunstall et al., 2022) được đào tạo trên 180 GB mã Python.

Với sự đa dạng về kích thước mô hình và sơ đồ đào tạo liên quan đến các mô hình này và thiếu so sánh giữa chúng, tác động của nhiều quyết định thiết kế mô hình và đào tạo vẫn chưa rõ ràng.
Ví dụ, chúng ta không biết việc lựa chọn chính xác dữ liệu mà Codex và các mô hình riêng tư khác được đào tạo; tuy nhiên, chúng ta biết rằng một số mô hình công khai (ví dụ: GPT-J) được đào tạo trên hỗn hợp ngôn ngữ tự nhiên và mã trong nhiều ngôn ngữ lập trình, trong khi các mô hình khác (ví dụ: CodeParrot) được đào tạo chỉ trên mã trong một ngôn ngữ lập trình cụ thể. Các mô hình đa ngôn ngữ có khả năng cung cấp khả năng tổng quát hóa tốt hơn, bởi vì các ngôn ngữ lập trình khác nhau chia sẻ các từ khóa và thuộc tính tương tự, như được chứng minh bởi thành công của các mô hình đa ngôn ngữ cho ngôn ngữ tự nhiên (Conneau & Lample,
2019) và cho mã nguồn (Zügner et al., 2021). Điều này có thể gợi ý rằng các LM đa ngôn ngữ có thể tổng quát hóa trên các ngôn ngữ, vượt trội hơn các mô hình đơn ngôn ngữ và hữu ích cho việc mô hình hóa các ngôn ngữ lập trình ít nguồn lực, nhưng điều này vẫn chưa được xác minh thực nghiệm.

Trong bài báo này, chúng tôi trình bày một đánh giá có hệ thống các mô hình mã nguồn hiện có - Codex, GPT-J, GPT-Neo,
GPT-NeoX, và CodeParrot - trên nhiều ngôn ngữ lập trình khác nhau. Chúng tôi nhằm mục đích làm sáng tỏ thêm về
bối cảnh các quyết định thiết kế mô hình hóa mã bằng cách so sánh và đối chiếu các mô hình này, cũng như cung cấp một liên kết quan trọng còn thiếu: cho đến nay, không có mô hình ngôn ngữ mã nguồn mở lớn nào được đào tạo độc quyền trên mã từ nhiều ngôn ngữ lập trình. Chúng tôi cung cấp ba mô hình như vậy, từ 160M đến 2,7B tham số, mà chúng tôi phát hành dưới tên chung "PolyCoder". Đầu tiên, chúng tôi thực hiện một so sánh rộng rãi về các thiết lập đào tạo và đánh giá giữa PolyCoder, các mô hình mã nguồn mở, và Codex. Thứ hai, chúng tôi đánh giá các mô hình trên điểm chuẩn HumanEval (Chen et al.,
2021) và so sánh cách các mô hình có kích thước và bước đào tạo khác nhau mở rộng, và cách các nhiệt độ khác nhau ảnh hưởng đến chất lượng tạo. Cuối cùng, vì HumanEval chỉ đánh giá việc tổng hợp từ ngôn ngữ tự nhiên sang Python, chúng tôi tuyển chọn một tập dữ liệu đánh giá chưa thấy³ trong mỗi ngôn ngữ trong 12 ngôn ngữ, để đánh giá độ phức tạp của các mô hình khác nhau. Chúng tôi phát hiện rằng mặc dù Codex được cho là tập trung vào Python (Chen et al. (2021) §3.1), Codex hoạt động tốt một cách đáng ngạc nhiên trong các ngôn ngữ lập trình khác, và thậm chí tốt hơn GPT-J và GPT-NeoX được đào tạo trên Pile (Gao et al., 2020).
Tuy nhiên, trong ngôn ngữ lập trình C, mô hình PolyCoder của chúng tôi đạt được độ phức tạp thấp hơn tất cả các mô hình này, bao gồm cả Codex.

Mặc dù hầu hết các mô hình hiện tại hoạt động kém hơn Codex, chúng tôi hy vọng rằng nghiên cứu có hệ thống này giúp nghiên cứu tương lai trong lĩnh vực này thiết kế các mô hình hiệu quả và hiệu suất hơn. Quan trọng hơn, thông qua việc đánh giá có hệ thống các mô hình khác nhau này, chúng tôi khuyến khích cộng đồng nghiên cứu và phát hành các mô hình ngôn ngữ quy mô trung bình-lớn cho mã nguồn, để đáp ứng các mối quan tâm được thể hiện bởi Hellendoorn & Sawant (2021):

[...] xu hướng bùng nổ này trong chi phí để đạt được tiên tiến nhất đã để lại khả năng đào tạo và kiểm tra các mô hình như vậy bị giới hạn cho một số ít các công ty công nghệ lớn - và vượt xa nguồn lực của hầu như tất cả các phòng thí nghiệm học thuật.

Chúng tôi tin rằng nỗ lực của chúng tôi là một bước tiến đáng kể hướng tới việc dân chủ hóa các mô hình ngôn ngữ lớn của mã nguồn.

2 CÔNG TRÌNH LIÊN QUAN
Cốt lõi của việc mô hình hóa mã là công việc đang diễn ra về tiền đào tạo các mô hình ngôn ngữ (LM). Tiền đào tạo quy mô lớn của LM đã có tác động đáng kinh ngạc đến xử lý ngôn ngữ tự nhiên trong những năm gần đây (Han
et al., 2021). Hình 1 cung cấp tổng quan về cách các mô hình khác nhau so sánh về kích thước và tính khả dụng.

2.1 PHƯƠNG PHÁP TIỀN ĐÀO TẠO
Chúng tôi thảo luận ba phương pháp tiền đào tạo phổ biến được sử dụng trong mô hình hóa ngôn ngữ mã nguồn. Minh họa cho các phương pháp này được hiển thị trong Hình 2.

³Tập đào tạo chính xác mà Codex được đào tạo trên là không rõ.
2

--- TRANG 3 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
# tham số500M1B5B10B50B100B
CodeGPTCodeBERTCodeT5 CuBERT PLBART
CodeParrotGPT-NeoPolyCoderGPT-J Codex
GPT-NeoX Austin '21
Hình 1: Các mô hình ngôn ngữ hiện có của mã nguồn, kích thước và tính khả dụng của chúng (mã nguồn mở so với
không phải mã nguồn mở).

# tìm kiếm nhị phân đệ quy MASK0
defbinarySearch (arr, left, right, x): 
mid =(left +MASK1
ifarrMASK2==x: 
returnmid
MASK0tìm kiếm nhị phân đệ quy MASK1 right ) //2
MASK2 [ mid ]# tìm kiếm nhị phân đệ quy 
defbinarySearch (arr, left, right, x): 
mid =(left +MASK) //2
ifarr[mid] ==x: 
returnmid
right# tìm kiếm nhị phân đệ quy 
defbinarySearch (arr, left, right, x): 
mid =(left + ???
right
Mô hình Ngôn ngữ Từ Trái sang Phải Mô hình Ngôn ngữ Che Mặt Mô hình Bộ mã hóa-Giải mã
Hình 2: Ba loại mô hình ngôn ngữ được đào tạo trước.

Mô hình Ngôn ngữ Từ Trái sang Phải (Hình 2, trái) Tự hồi quy, LM từ trái sang phải, dự đoán xác suất của một token dựa trên các token trước đó. Trong mô hình hóa mã, CodeGPT (124M) (Lu et al., 2021),
CodeParrot (1.5B) (Tunstall et al., 2022), GPT-Neo (2.7B) (Black et al., 2021), GPT-J (6B) (Wang &
Komatsuzaki, 2021), Codex (12B) (Chen et al., 2021), GPT-NeoX (20B) (Black et al., 2022), và
Google's (137B) (Austin et al., 2021) thuộc về danh mục này. Bản chất từ trái sang phải của các mô hình này làm cho chúng cực kỳ hữu ích cho các nhiệm vụ tạo chương trình, chẳng hạn như hoàn thành mã. Mặt khác, vì mã thường không được viết trong một lần duy nhất, từ trái sang phải, nên không dễ dàng để tận dụng ngữ cảnh xuất hiện "sau" vị trí tạo. Trong bài báo này, chúng tôi tập trung vào họ mô hình này và sẽ thảo luận các mô hình hiện có chi tiết hơn trong các phần sau.

Mô hình Ngôn ngữ Che Mặt (Hình 2, giữa) Trong khi các mô hình ngôn ngữ tự hồi quy mạnh mẽ cho việc mô hình hóa xác suất của chuỗi, bản chất một chiều của chúng làm cho chúng ít phù hợp hơn cho việc tạo ra các biểu diễn chuỗi toàn bộ hiệu quả cho các nhiệm vụ hạ nguồn như phân loại.
Một hàm mục tiêu hai chiều phổ biến được sử dụng rộng rãi trong học biểu diễn là mô hình ngôn ngữ che mặt (Devlin et al., 2018), nơi mục tiêu là dự đoán các phần văn bản bị che dựa trên ngữ cảnh xung quanh. CodeBERT (125M) (Feng et al., 2020) và CuBERT (345M) (Kanade et al.,
2020) là các ví dụ của những mô hình như vậy trong mã. Trong các ngữ cảnh lập trình, những phương pháp này cung cấp các biểu diễn hữu ích của một chuỗi mã cho các nhiệm vụ hạ nguồn như phân loại mã, phát hiện nhân bản và phát hiện lỗi.

Mô hình Bộ mã hóa-Giải mã (Hình 2, phải) Một mô hình bộ mã hóa-giải mã đầu tiên sử dụng một bộ mã hóa để mã hóa một chuỗi đầu vào, và sau đó sử dụng một LM từ trái sang phải để giải mã một chuỗi đầu ra được điều kiện hóa trên chuỗi đầu vào. Các mục tiêu tiền đào tạo phổ biến bao gồm dự đoán khoảng che (Raffel et al.,
2019) nơi chuỗi đầu vào được che ngẫu nhiên với nhiều mặt nạ và chuỗi đầu ra là nội dung bị che theo thứ tự, và tái tạo chuỗi khử nhiễu (Lewis et al., 2019) nơi đầu vào là một chuỗi bị hỏng và đầu ra là chuỗi gốc. Những mô hình được đào tạo trước này hữu ích trong nhiều nhiệm vụ chuỗi-đến-chuỗi (Raffel et al., 2019). Trong mã, CodeT5 (220M) (Wang
et al., 2021), và PLBART (406M) (Ahmad et al., 2021) sử dụng hai mục tiêu được đề cập ở trên tương ứng, và hoạt động tốt trong các nhiệm vụ tạo có điều kiện hạ nguồn như bình luận mã, hoặc tạo mã từ ngôn ngữ tự nhiên.
3

--- TRANG 4 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
2.2 DỮ LIỆU TIỀN ĐÀO TẠO
Một số mô hình (ví dụ: CodeParrot và CodeT5) được đào tạo chỉ trên mã GitHub, với các tập dữ liệu được trích xuất bằng cách sử dụng tập dữ liệu GitHub của Google BigQuery⁴, hoặc CodeSearchNet (Husain et al., 2019). Những mô hình khác (ví dụ: GPT-Neo và GPT-J) được đào tạo trên "the Pile" (Gao et al., 2020), một tập dữ liệu lớn chứa hỗn hợp các văn bản ngôn ngữ tự nhiên và mã từ nhiều lĩnh vực khác nhau, bao gồm các bản sao Stack Exchange, tài liệu phần mềm, và các kho lưu trữ GitHub phổ biến (>100 sao). Các tập dữ liệu mà các mô hình độc quyền khác (Codex, Google's) được đào tạo trên là không rõ. Một mục tiêu của nghiên cứu của chúng tôi là cố gắng làm sáng tỏ những tập dữ liệu nào có thể hữu ích nhất cho việc tiền đào tạo các mô hình mã.

3 THIẾT LẬP ĐÁNH GIÁ
Chúng tôi đánh giá tất cả các mô hình sử dụng cả điểm chuẩn ngoại tại và nội tại, như mô tả dưới đây.

Đánh giá Ngoại tại Một trong những nhiệm vụ hạ nguồn phổ biến nhất cho việc mô hình hóa mã là tạo mã từ một mô tả ngôn ngữ tự nhiên. Theo Chen et al. (2021), chúng tôi đánh giá tất cả các mô hình trên tập dữ liệu HumanEval. Tập dữ liệu chứa 164 lời nhắc với các mô tả dưới dạng bình luận mã và định nghĩa hàm, bao gồm tên đối số và tên hàm, và các trường hợp kiểm tra để đánh giá xem mã được tạo có đúng không. Để tạo mã từ một lời nhắc, chúng tôi sử dụng cùng chiến lược lấy mẫu như Chen et al. (2021), sử dụng softmax với tham số nhiệt độ softmax (x=T).
Chúng tôi đánh giá sử dụng một phạm vi rộng nhiệt độ T= [0:2;0:4;0:6;0:8] để kiểm soát độ tin cậy của dự đoán của mô hình. Tương tự như Codex, chúng tôi sử dụng lấy mẫu nucleus (Holtzman et al., 2019) với top-p= 0:95. Chúng tôi lấy mẫu token từ mô hình cho đến khi gặp một trong các chuỗi dừng sau đây chỉ ra kết thúc của một phương thức:⁵'nnclass ', 'nndef ', 'nn#', 'nnif', hoặc ' nnprint '. Chúng tôi lấy mẫu ngẫu nhiên 100 ví dụ cho mỗi lời nhắc trong tập dữ liệu đánh giá.

Đánh giá Nội tại Để đánh giá hiệu suất nội tại của các mô hình khác nhau, chúng tôi tính toán độ phức tạp cho mỗi ngôn ngữ trên một tập các kho lưu trữ GitHub chưa thấy. Để ngăn chặn rò rỉ dữ liệu từ đào tạo đến kiểm tra cho các mô hình như GPT-Neo và GPT-J, chúng tôi loại bỏ các kho lưu trữ trong tập dữ liệu đánh giá của chúng tôi xuất hiện trong phần GitHub của tập dữ liệu đào tạo Pile⁶. Để đánh giá Codex, chúng tôi sử dụng API của OpenAI⁷, chọn engine code-davinci-001. Chúng tôi lưu ý rằng dữ liệu mà mô hình này được đào tạo trên là không rõ, vì vậy chúng tôi không thể ngăn chặn rò rỉ dữ liệu từ tập đào tạo đến tập kiểm tra cho Codex. Chúng tôi lấy mẫu 100 tệp ngẫu nhiên cho mỗi ngôn ngữ lập trình trong 12 ngôn ngữ trong tập dữ liệu đánh giá của chúng tôi. Để làm cho độ phức tạp có thể so sánh được trên các phương pháp token hóa khác nhau được sử dụng trong các mô hình khác nhau, chúng tôi sử dụng Pygments⁸ để chuẩn hóa đều tổng log-likelihood của mỗi mô hình, khi tính toán độ phức tạp.⁹

4 CÁC MÔ HÌNH ĐƯỢC SO SÁNH
4.1 CÁC MÔ HÌNH HIỆN CÓ
Như đã thảo luận trong Phần 2, chúng tôi chủ yếu tập trung vào các mô hình ngôn ngữ được đào tạo trước tự hồi quy từ trái sang phải, phù hợp nhất cho các nhiệm vụ hoàn thành mã.

Chúng tôi đánh giá Codex, vì nó hiện đang được triển khai trong thế giới thực và có hiệu suất ấn tượng trong hoàn thành mã (Chen et al., 2021). Codex sử dụng mô hình ngôn ngữ GPT-3 (Brown et al., 2020) như kiến trúc mô hình cơ bản của nó. Codex được đào tạo trên một tập dữ liệu trải dài 179GB (sau khi khử trùng lặp) bao phủ hơn 54 triệu kho lưu trữ Python công khai có được từ GitHub vào tháng 5 năm 2020. Như được phản ánh trong kết quả ấn tượng của nó trong các ngôn ngữ lập trình khác ngoài Python, chúng tôi nghi ngờ rằng Codex cũng được đào tạo trên các tập dữ liệu lớn của các ngôn ngữ lập trình bổ sung. Mô hình có sẵn để truy vấn thông qua API không miễn phí.

⁴https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-
the-open-source-code
⁵Sự vắng mặt của khoảng trắng, có ý nghĩa trong Python, báo hiệu một lối thoát khỏi thân phương thức.
⁶https://github.com/EleutherAI/github-downloader
⁷https://beta.openai.com/docs/engines/codex-series-private-beta
⁸https://pygments.org/docs/lexers/
⁹Mỗi mô hình sử dụng tokenizer gốc của nó để dự đoán token tiếp theo. Chúng tôi chỉ sử dụng tokenizer chung để tính toán độ phức tạp dựa trên tổng log-likelihood.
4

--- TRANG 5 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
Đối với các mô hình mã nguồn mở, chúng tôi so sánh GPT-Neo, GPT-J và GPT-NeoX, các biến thể lớn nhất có
2,7, 6 và 20 tỷ tham số, tương ứng. GPT-NeoX là mô hình ngôn ngữ được đào tạo trước mã nguồn mở lớn nhất có sẵn. Những mô hình này được đào tạo trên tập dữ liệu Pile, vì vậy chúng là đại diện tốt của các mô hình được đào tạo trên cả văn bản ngôn ngữ tự nhiên từ nhiều lĩnh vực khác nhau và mã nguồn từ GitHub. Chúng tôi cũng so sánh CodeParrot với tối đa 1,5 tỷ tham số, một mô hình chỉ được đào tạo trên mã Python từ GitHub. CodeParrot tuân theo quy trình được sử dụng trong Chen et al.
(2021) đã có được hơn 20 triệu tệp Python từ cơ sở dữ liệu Github của Google BigQuery, dẫn đến một tập dữ liệu 180GB, có thể so sánh với dữ liệu đào tạo Python của Codex, nhưng bản thân mô hình nhỏ hơn nhiều.

Không có mô hình ngôn ngữ mã nguồn mở lớn nào được đào tạo gần như độc quyền trên mã từ nhiều ngôn ngữ lập trình. Để lấp đầy khoảng trống này, chúng tôi đào tạo một mô hình 2,7 tỷ, PolyCoder, trên hỗn hợp các kho lưu trữ từ GitHub trong 12 ngôn ngữ lập trình khác nhau.

Ngôn ngữ Kho lưu trữ Tệp Kích thước Trước Lọc Kích thước Sau Lọc
C 10,749 3,037,112 221G 55G
C# 9,511 2,514,494 30G 21G
C++ 13,726 4,289,506 115G 52G
Go 12,371 1,416,789 70G 15G
Java 15,044 5,120,129 60G 41G
JavaScript 25,144 1,774,174 66G 22G
PHP 9,960 1,714,058 21G 13G
Python 25,446 1,550,208 24G 16G
Ruby 5,826 674,343 5.0G 4.1G
Rust 4,991 304,842 5.2G 3.5G
Scala 1,497 245,100 2.2G 1.8G
TypeScript 12,830 1,441,926 12G 9.2G
Tổng 147,095 24,082,681 631.4G 253.6G
Bảng 1: Thống kê tập dữ liệu đào tạo.

4.2 DỮ LIỆU CỦA POLYCODER
Thu thập Tập dữ liệu Mã Thô GitHub là một nguồn tuyệt vời cho mã nguồn có sẵn công khai của nhiều ngôn ngữ lập trình khác nhau. Chúng tôi sao chép các kho lưu trữ phổ biến nhất cho 12 ngôn ngữ lập trình phổ biến với ít nhất 50 sao (dừng lại ở khoảng 25K cho mỗi ngôn ngữ để tránh độ lệch quá nặng về các ngôn ngữ lập trình phổ biến) từ GitHub vào tháng 10 năm 2021. Đối với mỗi dự án, mỗi tệp thuộc về ngôn ngữ chính của dự án đó được trích xuất, tạo ra tập đào tạo ban đầu. Tập dữ liệu ban đầu, chưa được lọc này trải dài 631GB và 38,9 triệu tệp.

Tiền xử lý Dữ liệu Chiến lược tiền xử lý dữ liệu chi tiết so sánh với các mô hình khác được phân tích trong Bảng 2. Nói chung, chúng tôi cố gắng tuân theo các quyết định thiết kế của Codex, mặc dù có một phần mơ hồ trong mô tả về tiền xử lý dữ liệu của nó.

Khử trùng lặp và Lọc Tương tự như Codex và CodeParrot, các tệp rất lớn (>1MB) và rất ngắn (<100 token) được lọc ra, giảm kích thước của tập dữ liệu 33%, từ 631GB xuống 424GB. Điều này chỉ giảm tổng số tệp 8%, cho thấy rằng một số lượng nhỏ tệp chịu trách nhiệm cho một phần lớn của tập dữ liệu.¹⁰

Allamanis (2019) đã chỉ ra rằng sự trùng lặp mã thường biểu hiện trong các tập dữ liệu mã ảnh hưởng xấu đến việc mô hình hóa ngôn ngữ của mã. Do đó, chúng tôi khử trùng lặp tệp dựa trên hash của nội dung của chúng, điều này làm giảm số lượng tệp gần 30%, và kích thước tập dữ liệu thêm 29%, để lại 24,1 triệu tệp và 254GB dữ liệu.

Tổng thể, việc lọc các tệp rất lớn và rất ngắn cộng với khử trùng lặp, đã giảm số lượng tệp 38%, và kích thước tập dữ liệu 61%, gần bằng với việc giảm kích thước tập dữ liệu 70% được báo cáo bởi CodeParrot. Một khác biệt chính vẫn còn là các phương pháp khác sử dụng lọc tinh tế hơn

¹⁰Codex cũng đề cập đến việc loại bỏ các tệp "tự động tạo", nhưng định nghĩa của điều này không rõ ràng, vì vậy chúng tôi bỏ qua bước này.
5

--- TRANG 6 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
PolyCoder CodeParrot Codex
Khử trùng Chính xác Chính xác Không rõ, đề cập "duy nhất"
Lọc Tệp>1 MB, <100 to-
kenTệp>1MB, độ dài dòng tối đa >
1000, độ dài dòng trung bình >100,
tỷ lệ ký tự chữ và số<0.25, chứa từ
"auto-generated" hoặc tương tự trong
5 dòng đầuTệp>1MB, độ dài dòng tối đa >
1000, độ dài dòng trung bình >100,
tự động tạo (chi tiết không rõ),
chứa tỷ lệ nhỏ ký tự chữ và số (chi
tiết không rõ)
Token hóa Đào tạo tokenizer GPT-2 trên
một tập con ngẫu nhiên 5%
(tất cả ngôn ngữ)Đào tạo tokenizer GPT-2 trên
split đào tạoTokenizer GPT-3, thêm các token
nhiều khoảng trắng để giảm
token khoảng trắng dư thừa
Bảng 2: So sánh các chiến lược tiền xử lý dữ liệu của các mô hình khác nhau.

chiến lược, chẳng hạn như giới hạn độ dài dòng tối đa hoặc độ dài dòng trung bình, lọc các tệp có thể tự động tạo, v.v. Ví dụ, Chen et al. (2021) chỉ lọc 11% dữ liệu đào tạo của họ.
Thống kê tập dữ liệu được hiển thị trong Bảng 1, thể hiện kích thước dữ liệu cho mỗi ngôn ngữ trước và sau khi lọc. Tập dữ liệu của chúng tôi chứa ít mã Python hơn (chỉ 16G) so với Codex hoặc CodeParrot, và thay vào đó bao phủ nhiều ngôn ngữ lập trình khác nhau.

Tokenizer Chúng tôi đào tạo một tokenizer GPT-2 (sử dụng BPE (Sennrich et al., 2015)) trên một tập con ngẫu nhiên 5% của tất cả dữ liệu tiền đào tạo, chứa tất cả các ngôn ngữ. Codex sử dụng một tokenizer GPT-3 được đào tạo sẵn, với việc bổ sung các token nhiều khoảng trắng để giảm độ dài chuỗi sau token hóa, vì các khoảng trắng liên tiếp phổ biến hơn trong mã so với trong văn bản.

4.3 ĐÀO TẠO POLYCODER
Xét đến ngân sách của chúng tôi, chúng tôi chọn GPT-2 (Radford et al., 2019) làm kiến trúc mô hình của chúng tôi. Để nghiên cứu hiệu ứng mở rộng kích thước mô hình, chúng tôi đào tạo 3 mô hình có kích thước khác nhau, với 2,7 tỷ, 400 triệu và 160 triệu tham số, với mô hình lớn nhất 2,7B ngang bằng với GPT-Neo để so sánh công bằng. Mô hình 2,7 tỷ là một mô hình Transformer 32 lớp, 2.560 chiều, với cửa sổ ngữ cảnh tối đa 2048 token, được đào tạo với kích thước batch 128 chuỗi (262K token). Mô hình được đào tạo trong 150K bước. Mô hình 400 triệu là một biến thể 24 lớp, 1.024 chiều, và mô hình 160 triệu là một biến thể 12 lớp, 768 chiều, các yếu tố khác giống hệt. Chúng tôi sử dụng bộ công cụ GPT-NeoX¹¹ để đào tạo mô hình hiệu quả song song với 8 GPU Nvidia RTX 8000 trên một máy duy nhất. Thời gian thực sử dụng để đào tạo mô hình 2,7B lớn nhất là khoảng 6 tuần. Trong cấu hình mặc định, mô hình này nên đào tạo trong 320K bước, điều này không khả thi với nguồn lực của chúng tôi. Thay vào đó, chúng tôi điều chỉnh sự suy giảm tốc độ học để giảm một nửa số này và đào tạo cho đến 150K bước (gần hội tụ). Các đường cong mất mát đào tạo và xác thực cho các mô hình có kích thước khác nhau được hiển thị trong Hình 3. Chúng ta thấy rằng ngay cả sau khi đào tạo trong 150K bước, các mất mát xác thực vẫn đang giảm. Điều này, kết hợp với lịch trình đào tạo ngắn hơn và sự suy giảm tốc độ học nhanh hơn, báo hiệu mạnh mẽ rằng các mô hình vẫn đang thiếu phù hợp và có thể được hưởng lợi từ việc đào tạo lâu hơn.

Chúng tôi so sánh thiết lập đào tạo và siêu tham số với CodeParrot và Codex trong Bảng 3. Do chi phí tính toán cao, chúng tôi không thể thực hiện tìm kiếm siêu tham số. Hầu hết các siêu tham số giống với những siêu tham số được sử dụng trong đào tạo mô hình GPT-2 tương ứng của chúng¹² để cung cấp một mặc định tốt đối với kích thước mô hình tương ứng. Một số khác biệt chính bao gồm kích thước cửa sổ ngữ cảnh để cho phép nhiều token hơn như ngữ cảnh, kích thước batch và token được đào tạo, cũng như khởi tạo mô hình có hoặc không có kiến thức ngôn ngữ tự nhiên.

6

--- TRANG 7 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
PolyCoder (2.7B) CodeParrot (1.5B) Codex (12B)
Khởi tạo Mô hình Từ đầu Từ đầu Khởi tạo từ GPT-3
Kiến thức NL Học từ bình luận
trong mãHọc từ bình luận
trong mãKiến thức ngôn ngữ tự nhiên
từ GPT-3
Tốc độ Học 1.6e-4 2.0e-4 1e-4
Optimizer AdamW AdamW AdamW
Adam betas 0.9, 0.999 0.9, 0.999 0.9, 0.95
Adam eps 1e-8 1e-8 1e-8
Weight Decay - 0.1 0.1
Warmup Steps 1600 750 175
Learning Rate Decay Cosine Cosine Cosine
Batch Size (#tokens) 262K 524K 2M
Training Steps 150K bước, 39B token 50K bước, 26B token 100B token
Context Window 2048 1024 4096
Bảng 3: So sánh các quyết định thiết kế và siêu tham số trong việc đào tạo các mô hình mã khác nhau.

0 25 50 75 100 125 150
×1000 bước0.500.751.001.251.50Training Loss2.7B
400M
160M
(a) Đào tạo
0 25 50 75 100 125 150
×1000 bước0.500.751.001.251.50Validation Loss2.7B
400M
160M (b) Xác thực
Hình 3: Mất mát đào tạo và xác thực trong quá trình đào tạo 150K bước.

5 KẾT QUẢ
5.1 ĐÁNH GIÁ NGOẠI TẠI
Kết quả tổng thể được hiển thị trong Bảng 4.¹³ Các số được thu được bằng cách lấy mẫu với các nhiệt độ khác nhau và chọn giá trị tốt nhất cho mỗi chỉ số. Trong số các mô hình hiện có, PolyCoder kém hơn GPT-Neo có kích thước tương tự và thậm chí Codex 300M nhỏ hơn. Tổng thể, PolyCoder đứng sau Codex,
GPT-Neo/J, trong khi hoạt động mạnh hơn CodeParrot. PolyCoder, được đào tạo chỉ trên mã, tụt hậu so với một mô hình có kích thước tương tự (GPT-Neo 2.7B) được đào tạo trên Pile, một hỗn hợp văn bản ngôn ngữ tự nhiên và mã. Nhìn vào các cột bên phải nhất trong Bảng 4 cung cấp một lời giải thích tiềm năng: về tổng số token Python thấy trong quá trình đào tạo, tất cả các mô hình vượt trội đáng kể so với chúng tôi. Điều này một phần là vì họ sử dụng tỷ lệ mã Python cao hơn (chúng tôi nhằm cân bằng khối lượng dữ liệu trên các ngôn ngữ lập trình), và một phần vì hạn chế về nguồn lực, dẫn đến PolyCoder không quan sát toàn bộ dữ liệu đào tạo của nó. Ngoài ra, sự pha trộn ngôn ngữ tự nhiên trong tập dữ liệu đào tạo cũng có thể giúp mô hình hóa ngôn ngữ mã, đặc biệt với các văn bản liên quan đến mã như các bản sao Stack Exchange được bao gồm.

So với GPT-Neo (2.7B), PolyCoder đã thấy ít token Python hơn, nhưng nhiều token mã hơn trong các ngôn ngữ lập trình khác, gợi ý rằng việc chuyển giao từ các ngôn ngữ khác sang Python giúp đạt được hiệu suất tương tự. Điều này cho thấy rằng nghiên cứu tương lai có thể được hưởng lợi từ việc pha trộn mã trong các ngôn ngữ lập trình khác nhau, cũng như văn bản ngôn ngữ tự nhiên.

¹¹https://github.com/EleutherAI/gpt-neox
¹²https://github.com/EleutherAI/gpt-neox/tree/main/configs
¹³Do kích thước mô hình lớn của GPT-NeoX (20B) và ngân sách tính toán hạn chế, chúng tôi không bao gồm nó trong thí nghiệm HumanEval.
7

--- TRANG 8 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
Mô hình Pass@1 Pass@10 Pass@100 Token Đào tạo Token Mã Token Python
PolyCoder (160M) 2.13% 3.35% 4.88% 39B 39B 2.5B
PolyCoder (400M) 2.96% 5.29% 11.59% 39B 39B 2.5B
PolyCoder (2.7B) 5.59% 9.84% 17.68% 39B 39B 2.5B
CodeParrot (110M) 3.80% 6.57% 12.78% 26B 26B 26B
CodeParrot (1.5B) 3.58% 8.03% 14.96% 26B 26B 26B
GPT-Neo (125M) 0.75% 1.88% 2.97% 300B 22.8B 3.1B
GPT-Neo (1.3B) 4.79% 7.47% 16.30% 380B 28.8B 3.9B
GPT-Neo (2.7B) 6.41% 11.27% 21.37% 420B 31.9B 4.3B
GPT-J (6B) 11.62% 15.74% 27.74% 402B 30.5B 4.1B
Codex (300M) 13.17% 20.37% 36.27% 100B* 100B* 100B*
Codex (2.5B) 21.36% 35.42% 59.50% 100B* 100B* 100B*
Codex (12B) 28.81% 46.81% 72.31% 100B* 100B* 100B*
*Codex được khởi tạo với một mô hình được đào tạo trước khác, GPT-3.
Bảng 4: Kết quả của các mô hình khác nhau trên điểm chuẩn HumanEval, và số lượng các loại token khác nhau thấy trong quá trình đào tạo.

1071081091010
#Tham số051015202530Pass@1 (%)
PolyCoder
CodeParrot
GPT-Neo/J
Codex
(a) Pass@1
1071081091010
#Tham số010203040Pass@10 (%)
PolyCoder
CodeParrot
GPT-Neo/J
Codex (b) Pass@10
1071081091010
#Tham số0204060Pass@100 (%)
PolyCoder
CodeParrot
GPT-Neo/J
Codex (c) Pass@100
Hình 4: Hiệu ứng mở rộng của hiệu suất HumanEval trên các mô hình khác nhau.

Hiệu ứng Mở rộng Để hiểu thêm về hiệu ứng của số lượng tham số mô hình đối với hiệu suất hoàn thành mã HumanEval, chúng tôi hiển thị tỷ lệ phần trăm Pass@1, Pass@10 và Pass@100 đối với kích thước mô hình trong Hình 4. Chúng ta có thể thấy rằng hiệu suất của các mô hình Codex tốt hơn đáng kể so với tất cả các mô hình mã nguồn mở khác trên tất cả số lượng tham số.
Hiệu suất trên điểm chuẩn HumanEval tăng tuyến tính với độ lớn (thang log) của số lượng tham số trong mô hình. Các hiệu ứng mở rộng tương tự có thể được tìm thấy trên các mô hình PolyCoder và GPT-Neo/J. Thú vị là, các mô hình CodeParrot chỉ được đào tạo trên Python dường như đã đạt đến hiệu suất bão hòa đối với việc tăng số lượng tham số, nơi tập dữ liệu đào tạo tập trung vào Python có thể có một số hiệu ứng. Với số lượng tham số cao hơn (2.7B), hiệu suất của PolyCoder có xu hướng kém hơn so với GPT-Neo/J. So sánh GPT-Neo/J được đào tạo trên tập dữ liệu Pile chứa hỗn hợp văn bản, các bản sao Stack Exchange và dữ liệu GitHub, với PolyCoder được đào tạo chỉ trên các kho lưu trữ GitHub của các ngôn ngữ lập trình phổ biến, chúng tôi đưa ra giả thuyết rằng văn bản được thêm vào, đặc biệt là văn bản trong các lĩnh vực kỹ thuật và kỹ thuật phần mềm, có thể quan trọng để mô hình lớn hơn thúc đẩy hiệu suất. Chúng tôi cũng so sánh sự khác biệt hiệu suất giữa mô hình được đào tạo sau 100K bước so với mô hình sau 150K bước trong Phụ lục A, và thấy rằng đào tạo lâu hơn giúp mô hình lớn hơn nhiều hơn vì nó vẫn thiếu phù hợp.

Hiệu ứng Nhiệt độ Tất cả các kết quả trên được thu được bằng cách lấy mẫu mô hình ngôn ngữ với các nhiệt độ khác nhau và chọn giá trị tốt nhất cho mỗi chỉ số. Chúng tôi cũng quan tâm đến cách các lựa chọn nhiệt độ khác nhau ảnh hưởng đến chất lượng tạo cuối cùng. Chúng tôi tóm tắt kết quả trong Hình 5. Xu hướng chung là đối với Pass@1, nhiệt độ thấp hơn tốt hơn, và đối với Pass@100, nhiệt độ cao hơn sẽ giúp ích, trong khi đối với Pass@10 nhiệt độ ở giữa phù hợp hơn. Chúng tôi đưa ra giả thuyết rằng điều này là vì nhiệt độ cao hơn trong quá trình tạo làm cho mô hình ít tự tin hơn trong dự đoán của nó và do đó cho phép khám phá nhiều hơn và đầu ra đa dạng hơn, dẫn đến độ chính xác tốt hơn ở Pass@100. Nhiệt độ quá cao (0.8) cũng có hại nếu mô hình đủ khả năng.
8

--- TRANG 9 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
01234
C C# C++ Go Java JavaScript PHP Python Ruby Rust Scala TypeScriptCodex* PolyCoder 2.7B GPT-Neo 2.7B GPT-J 6B GPT-NeoX 20B CodeParrot
* Vì tập đào tạo chính xác của Codex không rõ, nó có thể bao gồm các tệp từ những tập kiểm tra này
làm cho kết quả của Codex quá lạc quan.
Hình 6: So sánh độ phức tạp trên tập dữ liệu đánh giá của chúng tôi của các mô hình khác nhau trên các ngôn ngữ lập trình khác nhau. Lưu ý rằng trục y được giới hạn ở 4; entropy của CodeParrot trên tất cả các ngôn ngữ khác ngoài Python cao hơn nhiều so với hiển thị ở đây (xem Bảng 5).

0.2 0.4 0.6 0.8
Nhiệt độ2.55.07.510.012.515.017.5Tỷ lệ Pass (%)
Pass@1
Pass@10
Pass@100
Hình 5: Hiệu suất HumanEval với
các nhiệt độ softmax khác nhau trong
quá trình tạo.Ngược lại, nhiệt độ thấp hơn làm cho mô hình
đầu ra rất tự tin trong dự đoán của nó và do đó sẽ
phù hợp hơn để tạo ra rất ít ví dụ đúng, và do đó
hiệu suất tốt hơn cho Pass@1. Trong Phụ lục B
chúng tôi lặp lại những thí nghiệm này với các mô hình nhỏ hơn
cũng vậy. Điều này cho thấy tầm quan trọng của nhiệt độ và
nhu cầu điều chỉnh nó riêng lẻ cho các kịch bản tạo
khác nhau.

5.2 ĐÁNH GIÁ NỘI TẠI
Kết quả độ phức tạp trên các tập dữ liệu đánh giá được hiển thị
trong Hình 6, với các số chi tiết trong Phụ lục C. Biểu đồ
giới hạn điểm độ phức tạp ở 4 vì CodeParrot hoạt động
kém trong các ngôn ngữ khác ngoài Python. Quan trọng
là lưu ý rằng mặc dù độ phức tạp của Codex thấp hơn
các mô hình khác trong hầu hết các ngôn ngữ, Codex có thể đã được đào tạo trên các tập kiểm tra, và kết quả của nó do đó quá lạc quan.

Đáng chú ý, PolyCoder vượt trội hơn Codex và tất cả các mô hình khác trong ngôn ngữ C. So sánh chỉ các mô hình mã nguồn mở, PolyCoder hoạt động tốt hơn GPT-Neo 2.7B có kích thước tương tự trong C, JavaScript, Rust, Scala và TypeScript.

Trong 11 ngôn ngữ khác ngoài C, tất cả các mô hình mã nguồn mở khác, bao gồm cả của chúng tôi, đều kém hơn đáng kể (độ phức tạp cao hơn) so với Codex. Chúng tôi đưa ra giả thuyết rằng điều này là do thực tế PolyCoder được đào tạo trên hỗn hợp không cân bằng của các ngôn ngữ khác nhau, với C và C++ có liên quan chặt chẽ và hai ngôn ngữ chiếm ưu thế nhất trong toàn bộ tập dữ liệu đào tạo (Phần 4.2). Do đó, khối lượng lớn hơn tổng cộng (vì các tệp dài) làm cho C trở thành ngôn ngữ được PolyCoder "ưa thích" nhất. Lý do tại sao PolyCoder không vượt trội hơn Codex trong C++ có thể là do sự phức tạp của ngôn ngữ C++ và kích thước cửa sổ ngữ cảnh dài hơn đáng kể của Codex (4096, so với 2048 của PolyCoder), hoặc vì Codex có thể được đào tạo trên nhiều dữ liệu đào tạo C++ hơn.

Với cùng tập dữ liệu tiền đào tạo, lợi ích từ mô hình 2.7B (GPT-Neo) lên mô hình 6B (GPT-J) là đáng kể trên tất cả các ngôn ngữ. Tuy nhiên, khi tăng kích thước mô hình thêm lên 20B, sự cải thiện khác nhau trên các ngôn ngữ khác nhau. Ví dụ, hiệu suất trên Go, Java, Rust, Scala, TypeScript không tăng đáng kể khi kích thước mô hình tăng gấp 3 lần. Điều này cho thấy rằng đối với một số ngôn ngữ lập trình, và với lượng dữ liệu đã cho, khả năng của GPT-J là đủ. Thú vị là, những ngôn ngữ này dường như trùng khớp với các ngôn ngữ mà PolyCoder vượt trội hơn một mô hình có kích thước tương tự được đào tạo trên Pile. Điều này có thể gợi ý rằng đối với các ngôn ngữ mà các mô hình lớn hơn không cung cấp lợi ích bổ sung, việc đào tạo mô hình chỉ sử dụng mã có thể là đủ hoặc hơi hữu ích hơn so với đào tạo trên cả ngôn ngữ tự nhiên và mã.
9

--- TRANG 10 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
Chúng ta có thể thấy rằng khi so sánh các mô hình khác nhau, xu hướng độ phức tạp cho Python tương quan tốt với hiệu suất điểm chuẩn HumanEval của đánh giá ngoại tại (Phần 5.1). Điều này cho thấy rằng độ phức tạp là một chỉ số hữu ích và chi phí thấp để ước tính các chỉ số hạ nguồn khác.

6 KẾT LUẬN
Trong bài báo này, chúng tôi thực hiện một đánh giá có hệ thống các mô hình ngôn ngữ lớn cho mã. Hiệu suất nói chung được hưởng lợi từ các mô hình lớn hơn và thời gian đào tạo dài hơn. Chúng tôi cũng tin rằng kết quả tốt hơn của GPT-Neo so với PolyCoder trong một số ngôn ngữ cho thấy rằng đào tạo trên văn bản ngôn ngữ tự nhiên và mã có thể có lợi cho việc mô hình hóa mã. Để giúp nghiên cứu tương lai trong lĩnh vực này, chúng tôi phát hành PolyCoder, một mô hình ngôn ngữ mã nguồn mở lớn cho mã, được đào tạo độc quyền trên mã trong 12 ngôn ngữ lập trình khác nhau. Trong ngôn ngữ lập trình C, PolyCoder đạt được độ phức tạp thấp hơn tất cả các mô hình bao gồm cả Codex.

TÀI LIỆU THAM KHẢO
Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Uniﬁed pre-training for
program understanding and generation. In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies , pp. 2655–2668, Online, June 2021. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/2021.naacl-main.211 .

Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In
Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms,
and Reﬂections on Programming and Software , pp. 143–153, 2019.

Uri Alon, Roy Sadaka, Omer Levy, và Eran Yahav. Structural language models of code. In
International Conference on Machine Learning , pp. 245–256. PMLR, 2020.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732 , 2021.

Alexei Baevski và Michael Auli. Adaptive input representations for neural language modeling.
arXiv preprint arXiv:1809.10853 , 2018.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, và Christian Jauvin. A neural probabilistic
language model. Journal of machine learning research , 3(Feb):1137–1155, 2003.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorﬂow, March 2021. URL https://doi.or
g/10.5281/zenodo.5297715 . If you use this software, please cite it using these metadata.

Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu
Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, và Samuel Weinbach. GPT-NeoX-20B: An
open-source autoregressive language model. 2022.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,
Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374 , 2021.

Alexis Conneau và Guillaume Lample. Cross-lingual language model pretraining. Advances in
Neural Information Processing Systems , 32:7059–7069, 2019.

Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, và
Subhajit Roy. Program synthesis using natural language. In Proceedings of the 38th International
Conference on Software Engineering , pp. 345–356, 2016.
10

--- TRANG 11 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural
languages. arXiv preprint arXiv:2002.08155 , 2020.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027 , 2020.

Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
Wentao Han, Minlie Huang, et al. Pre-trained models: Past, present and future. AI Open , 2021.

Vincent J Hellendoorn và Premkumar Devanbu. Are deep neural networks the best choice for
modeling source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering , pp. 763–773, 2017.

Vincent J. Hellendoorn và Anand Ashok Sawant. The growing cost of deep learning for source
code. Commun. ACM , 65(1):31–33, dec 2021. ISSN 0001-0782. doi: 10.1145/3501261. URL
https://doi.org/10.1145/3501261 .

Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, và Premkumar Devanbu. On the naturalness
of software. Communications of the ACM , 59(5):122–131, 2016.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751 , 2019.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. Code-
searchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 ,
2019.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, và Kensen Shi. Learning and evaluating
contextual embedding of source code. In International Conference on Machine Learning , pp.
5110–5121. PMLR, 2020.

Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, và Andrea Janes. Big
code!= big vocabulary: Open-vocabulary models for source code. In 2020 IEEE/ACM 42nd
International Conference on Software Engineering (ICSE) , pp. 1073–1085. IEEE, 2020.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, và Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 ,
2019.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, và Shujie LIU. CodeXGLUE: A machine learning benchmark dataset for code
understanding and generation. In Thirty-ﬁfth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 1) , 2021. URL https://openreview.net/forum?id=
6lE4dQXaUcb .

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683 , 2019.

Veselin Raychev, Martin Vechev, và Eran Yahav. Code completion with statistical language models.
InProceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and
Implementation , pp. 419–428, 2014.
11

--- TRANG 12 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
Rico Sennrich, Barry Haddow, và Alexandra Birch. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909 , 2015.

Lewis Tunstall, Leandro von Werra, và Thomas Wolf. Natural Language Processing with Trans-
formers . " O'Reilly Media, Inc.", 2022.

Ben Wang và Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-transformer-jax , May 2021.

Yue Wang, Weishi Wang, Shaﬁq Joty, và Steven CH Hoi. Codet5: Identiﬁer-aware uniﬁed pre-trained
encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 ,
2021.

Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, và Stephan Günnemann.
Language-agnostic representation learning of source code from structure and context. In Inter-
national Conference on Learning Representations , 2021. URL https://openreview.net/for
um?id=Xh5eMZVONGF .

A HIỆU ỨNG MỞ RỘNG: ĐÀO TẠO LÂU HƠN
Chúng tôi so sánh sự khác biệt hiệu suất giữa mô hình được đào tạo sau 100K bước so với mô hình sau 150K bước trong Hình 7. Chúng ta có thể thấy rằng trong mô hình 2.7B lớn hơn, bằng cách đào tạo mô hình lâu hơn đến 150K bước, hiệu suất tăng đều, với Pass@100 tăng nhiều nhất. Tuy nhiên, đối với một mô hình nhỏ hơn như mô hình 400M, bằng cách đào tạo mô hình lâu hơn đến 100K bước, các cải thiện được giảm bớt và Pass@100 giảm. Điều này cho thấy rằng với mô hình lớn hơn, đào tạo lâu hơn có thể cung cấp thúc đẩy bổ sung trong hiệu suất. Điều này phù hợp với quan sát từ đường cong đào tạo (Hình 3) cũng vậy.

100 110 120 130 140 150
×1000 bước5.07.510.012.515.017.5Tỷ lệ Pass (%)
Pass@1
Pass@10
Pass@100
(a) Mô hình 2.7B
100 110 120 130 140 150
×1000 bước24681012Tỷ lệ Pass (%)
Pass@1
Pass@10
Pass@100 (b) Mô hình 400M
Hình 7: So sánh hiệu suất HumanEval sau khi đào tạo mô hình lâu hơn.

B HIỆU ỨNG NHIỆT ĐỘ: CÁC MÔ HÌNH NHỎ HƠN
Chúng tôi hiển thị cách nhiệt độ ảnh hưởng đến hiệu suất HumanEval trên mô hình của cả ba kích thước trong Hình 8.
Chúng tôi thấy rằng đối với một mô hình lớn hơn, ví dụ: mô hình 2.7B, nhiệt độ cao đến 0.8 thực sự làm tổn hại hiệu suất cho Pass@100, cho thấy rằng nếu mô hình đủ tốt, nhiệt độ rất cao có thể làm cho đầu ra quá đa dạng, do đó làm tổn hại tính chính xác. Điều này cho thấy tầm quan trọng của nhiệt độ và nhu cầu điều chỉnh nó riêng lẻ cho khả năng mô hình khác nhau và các kịch bản tạo khác nhau.

C KẾT QUẢ ĐỘ PHỨC TẠP CHI TIẾT
Chúng tôi hiển thị độ phức tạp chi tiết của các mô hình khác nhau trên các ngôn ngữ khác nhau trong Bảng 5. Số lượng token được hiển thị trong bảng được thu được sau khi token hóa mã trong mỗi ngôn ngữ bằng cách sử dụng các lexer tương ứng của chúng, bởi Pygments. Số lượng token này được sử dụng để chuẩn hóa điểm độ phức tạp để làm cho chúng có thể so sánh được trên các mô hình. Lưu ý rằng CodeParrot chỉ được đào tạo trên dữ liệu Python và do đó hoạt động kém trong các ngôn ngữ khác.
12

--- TRANG 13 ---
Xuất bản như một bài báo hội thảo tại DL4C @ ICLR 2022
0.2 0.4 0.6 0.8
Nhiệt độ2.55.07.510.012.515.017.5Tỷ lệ Pass (%)
Pass@1
Pass@10
Pass@100
(a) Mô hình 2.7B
0.2 0.4 0.6 0.8
Nhiệt độ24681012Tỷ lệ Pass (%)
Pass@1
Pass@10
Pass@100 (b) Mô hình 400M
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Nhiệt độ12345Tỷ lệ Pass (%)
Pass@1
Pass@10
Pass@100 (c) Mô hình 160M
Hình 8: Hiệu suất HumanEval sử dụng các nhiệt độ softmax khác nhau trong quá trình tạo.

Ngôn ngữ #token Codex* PolyCoder 2.7B GPT-Neo 2.7B GPT-J 6B GPT-NeoX CodeParrot
C 55,333 2.55 2.33 3.69 2.82 2.37 19.23
C# 67,306 1.72 2.58 2.49 2.20 2.12 7.16
C++ 69,627 1.95 2.99 2.87 2.47 2.32 8.48
Go 79,947 1.39 2.57 2.19 1.89 1.85 10.00
Java 65,484 1.94 2.92 2.78 2.49 2.47 6.79
JavaScript 54,620 2.17 3.06 3.07 2.73 2.62 9.23
PHP 45,682 1.98 3.70 3.61 2.81 2.45 19.91
Python 79,653 1.47 3.18 3.00 2.68 2.61 2.95
Ruby 46,537 1.39 3.96 3.77 3.13 2.89 14.26
Rust 107,717 1.96 3.24 3.30 2.92 2.92 8.68
Scala 65,756 1.75 3.87 3.88 3.37 3.33 12.91
TypeScript 55,895 2.40 3.61 3.90 3.43 3.41 12.54
* Vì tập đào tạo chính xác của Codex không rõ, nó có thể đã được đào tạo trên những tập kiểm tra này,
và kết quả của Codex quá lạc quan.
Bảng 5: Độ phức tạp của các mô hình khác nhau cho các ngôn ngữ lập trình khác nhau trên tập dữ liệu đánh giá của chúng tôi.
13
