# 2209.07753.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2209.07753.pdf
# Kích thước tệp: 3200561 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Code as Policies: Các chương trình mô hình ngôn ngữ cho điều khiển thể hiện
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng
Robotics at Google

Tóm tắt — Các mô hình ngôn ngữ lớn (LLM) được huấn luyện trên việc hoàn thành mã đã được chứng minh có khả năng tổng hợp các chương trình Python đơn giản từ docstring [1]. Chúng tôi nhận thấy rằng những LLM viết mã này có thể được tái sử dụng để viết mã chính sách robot, dựa trên các lệnh ngôn ngữ tự nhiên. Cụ thể, mã chính sách có thể biểu thị các hàm hoặc vòng lặp phản hồi xử lý đầu ra nhận thức (ví dụ, từ các bộ phát hiện đối tượng [2], [3]) và tham số hóa các API nguyên thủy điều khiển. Khi được cung cấp làm đầu vào một số lệnh ngôn ngữ ví dụ (được định dạng như nhận xét) theo sau bởi mã chính sách tương ứng (thông qua prompting few-shot), LLM có thể nhận các lệnh mới và tự động tái tổ hợp các lời gọi API để tạo mã chính sách mới tương ứng. Bằng cách liên kết các cấu trúc logic cổ điển và tham chiếu các thư viện bên thứ ba (ví dụ, NumPy, Shapely) để thực hiện số học, LLM được sử dụng theo cách này có thể viết các chính sách robot mà (i) thể hiện lý luận không gian-hình học, (ii) tổng quát hóa cho các hướng dẫn mới, và (iii) quy định các giá trị chính xác (ví dụ, vận tốc) cho các mô tả mơ hồ ("nhanh hơn") tùy thuộc vào ngữ cảnh (tức là, thường thức hành vi). Bài báo này trình bày Code as Policies: một công thức hóa tập trung vào robot của các chương trình được tạo bởi mô hình ngôn ngữ (LMP) có thể đại diện cho các chính sách phản ứng (ví dụ, bộ điều khiển trở kháng), cũng như các chính sách dựa trên điểm đường (pick and place dựa trên thị giác, điều khiển dựa trên quỹ đạo), được chứng minh trên nhiều nền tảng robot thực. Trung tâm của phương pháp chúng tôi là prompting phân cấp code-gen (định nghĩa đệ quy các hàm chưa xác định), có thể viết mã phức tạp hơn và cũng cải thiện state-of-the-art để giải quyết 39,8% bài toán trên benchmark HumanEval [1]. Mã và video có sẵn tại https://code-as-policies.github.io

I. GIỚI THIỆU

Robot sử dụng ngôn ngữ cần nó được grounded (hoặc situated) để tham chiếu thế giới vật lý và xây dựng cầu nối giữa từ ngữ, nhận thức và hành động [4]. Các phương pháp cổ điển ground ngôn ngữ bằng cách sử dụng phân tích từ vựng để trích xuất các biểu diễn ngữ nghĩa thông báo cho các chính sách [5]–[7], nhưng chúng thường gặp khó khăn trong việc xử lý các hướng dẫn chưa thấy. Các phương pháp gần đây hơn học grounding end-to-end (ngôn ngữ thành hành động) [8]–[10], nhưng chúng yêu cầu lượng lớn dữ liệu huấn luyện, có thể tốn kém để thu thập trên robot thực.

Trong khi đó, tiến bộ gần đây trong xử lý ngôn ngữ tự nhiên cho thấy rằng các mô hình ngôn ngữ lớn (LLM) được pre-train trên dữ liệu quy mô Internet [11]–[13] thể hiện các khả năng out-of-the-box [14]–[16] có thể được áp dụng cho robot sử dụng ngôn ngữ, ví dụ như lập kế hoạch một chuỗi các bước từ hướng dẫn ngôn ngữ tự nhiên [16]–[18] mà không cần tinh chỉnh mô hình bổ sung. Những bước này có thể được grounded trong khả năng robot thực từ các hàm giá trị giữa một tập hợp cố định các kỹ năng tức là các chính sách được pre-train với behavior cloning hoặc reinforcement learning [19]–[21]. Mặc dù đầy hứa hẹn, sự trừu tượng này ngăn cản LLM trực tiếp ảnh hưởng đến vòng lặp phản hồi nhận thức-hành động, khiến việc ground ngôn ngữ theo những cách (i) tổng quát hóa các chế độ phản hồi chia sẻ nhận thức và hành động như từ "đặt quả táo xuống trên quả cam" thành "đặt quả táo xuống khi bạn thấy quả cam", (ii) biểu thị các tiên nghiệm thường thức trong điều khiển như "di chuyển nhanh hơn", "đẩy mạnh hơn", hoặc (iii) hiểu các mối quan hệ không gian "di chuyển quả táo một chút sang trái" trở nên khó khăn. Kết quả là, việc tích hợp mỗi kỹ năng mới (và chế độ grounding) yêu cầu dữ liệu bổ sung và tái huấn luyện – do đó gánh nặng dữ liệu vẫn tồn tại, mặc dù được chuyển cho việc thu thập kỹ năng. Điều này dẫn chúng tôi đặt câu hỏi: làm thế nào LLM có thể được áp dụng vượt ra ngoài chỉ lập kế hoạch một chuỗi kỹ năng?

Ở đây, chúng tôi nhận thấy rằng các LLM viết mã [1], [11], [22] thành thạo trong việc đi xa hơn: điều phối lập kế hoạch, logic chính sách và điều khiển. LLM được huấn luyện trên việc hoàn thành mã đã được chỉ ra có khả năng tổng hợp các chương trình Python từ docstring. Chúng tôi nhận thấy rằng những mô hình này có thể được tái sử dụng để viết mã chính sách robot, dựa trên các lệnh ngôn ngữ tự nhiên (được định dạng như nhận xét). Mã chính sách có thể biểu thị các hàm hoặc vòng lặp phản hồi xử lý đầu ra nhận thức (ví dụ, các bộ phát hiện đối tượng từ vựng mở [2], [3]) và tham số hóa các API nguyên thủy điều khiển (xem Hình 1). Khi được cung cấp một số lệnh ngôn ngữ ví dụ theo sau bởi mã chính sách tương ứng (thông qua few-shot prompting, được tô màu xám), LLM có thể nhận các lệnh mới (màu xanh lá) và tự động tái tổ hợp các lời gọi API để tạo mã chính sách mới (được làm nổi bật) tương ứng:

# nếu bạn thấy một quả cam, di chuyển lùi lại.
if detect_object("orange"):
    robot.set_velocity(x=-0.1, y=0, z=0)

# di chuyển sang phải cho đến khi bạn thấy quả táo.
while not detect_object("apple"):
    robot.set_velocity(x=0, y=0.1, z=0)

Các mô hình viết mã có thể biểu thị nhiều phép toán số học cũng như các vòng lặp phản hồi được grounded trong ngôn ngữ. Chúng không chỉ tổng quát hóa cho các hướng dẫn mới, mà sau khi được huấn luyện trên hàng tỷ dòng mã và nhận xét, cũng có thể quy định các giá trị chính xác (ví dụ, vận tốc) cho các mô tả mơ hồ ("nhanh hơn" và "sang trái") tùy thuộc vào ngữ cảnh – để gợi ra thường thức hành vi:

# làm lại nhưng nhanh hơn, sang trái, và với một quả chuối.
while not detect_object("banana"):
    robot.set_velocity(x=0, y=-0.2, z=0)

Biểu diễn mã như chính sách kế thừa một số lợi ích từ LLM: không chỉ khả năng diễn giải ngôn ngữ tự nhiên, mà còn khả năng tham gia vào đối thoại người-robot và Q&A đơn giản bằng cách sử dụng "say(text)" như một API nguyên thủy hành động có sẵn:

# nói cho tôi biết tại sao bạn ngừng di chuyển.
robot.say("Tôi ngừng di chuyển vì tôi thấy một quả chuối.")

--- TRANG 2 ---

Người dùng
Lấy lon coke từ bàn và đặt nó ở giữa các loại trái cây trên bàn.

Người dùng  
Đợi cho đến khi bạn thấy một quả trứng và đặt nó vào đĩa xanh lá

Người dùng
Vẽ một kim tự tháp nhỏ hơn một chút bên trái kim tự tháp

Người dùng
Đặt vật thể tối nhất vào đĩa có quả táo

Người dùng
Vẽ một hình vuông xung quanh loại trái cây ngọt hơn

Người dùng
Cất lon coke và quả táo vào thùng tương ứng của chúng

(b) (c) (d)

Người dùng
Đặt các khối vào bát với màu sắc không khớp

Người dùng
Đặt các khối trong một đường thẳng dài 20 cm và 10 cm phía dưới bát xanh dương

(a)

Hình 2: Code as Policies có thể tuân theo hướng dẫn ngôn ngữ tự nhiên trên các lĩnh vực và robot đa dạng: thao tác trên bàn (a)-(b), vẽ hình 2D (c), và thao tác di động trong nhà bếp với robot từ Everyday Robots (d). Phương pháp của chúng tôi cho phép robot thực hiện lý luận không gian-hình học, phân tích các mối quan hệ đối tượng, và hình thành các hành vi nhiều bước sử dụng các mô hình có sẵn và few-shot prompting mà không cần huấn luyện bổ sung. Xem video đầy đủ và nhiều tác vụ hơn tại code-as-policies.github.io

Chúng tôi trình bày Code as Policies (CaP): một công thức hóa tập trung vào robot của các chương trình được tạo bởi mô hình ngôn ngữ (LMP) được thực thi trên các hệ thống thực. LMP Pythonic có thể biểu thị các chính sách phức tạp sử dụng:

• Các cấu trúc logic cổ điển như chuỗi, lựa chọn (if/else), và vòng lặp (for/while) để lắp ráp các hành vi mới tại runtime.
• Các thư viện bên thứ ba để nội suy điểm (NumPy), phân tích và tạo hình (Shapely) cho lý luận không gian-hình học, v.v.

LMP có thể là phân cấp: được prompt để định nghĩa đệ quy các hàm mới, tích lũy thư viện riêng của chúng theo thời gian, và tự kiến trúc một codebase động. Chúng tôi chứng minh trên một số hệ thống robot rằng LLM có thể tự động diễn giải các lệnh ngôn ngữ để tạo LMP đại diện cho các chính sách cấp thấp phản ứng (ví dụ, bộ điều khiển PD hoặc trở kháng), và các chính sách dựa trên điểm đường (ví dụ, cho pick and place dựa trên thị giác, hoặc điều khiển dựa trên quỹ đạo).

Những đóng góp chính của chúng tôi là: (i) code as policies: một công thức sử dụng LLM để viết mã robot, (ii) một phương pháp cho hierarchical code-gen cải thiện state-of-the-art trên cả bài toán robotics và code-gen tiêu chuẩn với 39,8% P@1 trên HumanEval [1], (iii) một benchmark mới để đánh giá các mô hình ngôn ngữ tương lai trên các bài toán code-gen robotics, và (iv) các ablation phân tích cách CaP cải thiện các metric tổng quát hóa [23] và nó tuân theo các quy luật scaling – mô hình lớn hơn hoạt động tốt hơn. Code as policies trình bày một phương pháp mới để liên kết từ ngữ, nhận thức và hành động; cho phép các ứng dụng trong tương tác người-robot, nhưng không phải không có hạn chế. Chúng tôi thảo luận về những điều này trong Phần V. Các prompt đầy đủ và đầu ra được tạo có trong Phụ lục, có thể được tìm thấy cùng với các kết quả bổ sung, video và mã tại code-as-policies.github.io

II. CÔNG TRÌNH LIÊN QUAN

Điều khiển robot thông qua ngôn ngữ có lịch sử lâu đời, bao gồm các demo sớm về tương tác người-robot thông qua phân tích từ vựng của ngôn ngữ tự nhiên [5]. Ngôn ngữ không chỉ phục vụ như một giao diện cho người không chuyên tương tác với robot [24], [25], mà còn như một phương tiện để mở rộng tổng quát hóa một cách compositional cho các tác vụ mới [9], [17]. Tài liệu rất rộng (chúng tôi tham khảo Tellex et al. [4] và Luketina et al. [26] cho các khảo sát toàn diện), nhưng các công trình gần đây rơi vào các danh mục rộng của diễn giải cấp cao (ví dụ, phân tích ngữ nghĩa [25], [27]–[32]), lập kế hoạch [14], [17], [18], và các chính sách cấp thấp (ví dụ, dựa trên mô hình [33]–[35], imitation learning [8], [9], [36], [37], hoặc reinforcement learning [38]–[42]). Trái lại, công trình của chúng tôi tập trung vào khía cạnh tạo mã của LLM và sử dụng các thủ tục được tạo như một cách biểu thị để điều khiển robot.

Các mô hình ngôn ngữ lớn thể hiện khả năng lý luận zero-shot ấn tượng: từ lập kế hoạch [14] đến viết chương trình toán [43]; từ giải quyết bài toán khoa học [44] đến sử dụng các verifier được huấn luyện [45] cho bài toán từ toán học. Những điều này có thể được cải thiện với các phương pháp prompting như Least-to-Most [46], Think-Step-by-Step [15] hoặc Chain-of-Thought [47]. Gần nhất với bài báo này là các công trình sử dụng khả năng LLM cho các agent robot mà không cần huấn luyện mô hình bổ sung. Ví dụ, Huang et al. phân tách các lệnh ngôn ngữ tự nhiên thành chuỗi các hành động có thể thực thi bằng text completion và dịch thuật ngữ nghĩa [14], trong khi SayCan [17] tạo các kế hoạch khả thi cho robot bằng cách jointly decode một LLM được trọng số bởi skill affordances [20] từ các hàm giá trị. Inner Monologue [18] mở rộng lập kế hoạch LLM bằng cách tích hợp đầu ra từ các detector thành công hoặc các mô hình ngôn ngữ thị giác khác và sử dụng phản hồi của chúng để tái lập kế hoạch. Socratic Models [16] sử dụng các mô hình ngôn ngữ thị giác để thay thế thông tin perceptual (màu xanh teal) vào các prompt ngôn ngữ tạo kế hoạch, và nó sử dụng các chính sách conditioned ngôn ngữ như cho grasping [36]. Ví dụ sau minh họa sự khác biệt định tính giữa phương pháp của chúng tôi so với các công trình trước đó đã đề cập. Khi được giao nhiệm vụ "di chuyển lon coke một chút sang phải":

LLM Plan [14], [17], [18]
1. Nhặt lon coke
2. Di chuyển một chút sang phải  
3. Đặt lon coke

Socratic Models Plan [16]
objects = [lon coke]
1. robot.grasp(lon coke) open vocab
2. robot.place_a_bit_right()

các kế hoạch được tạo bởi các công trình trước giả định tồn tại một kỹ năng cho phép robot di chuyển một vật thể một chút sang phải. Phương pháp của chúng tôi khác ở chỗ nó sử dụng LLM để trực tiếp tạo mã chính sách (kế hoạch được lồng bên trong) để chạy trên robot và tránh yêu cầu có các chính sách được định nghĩa trước để ánh xạ mọi bước trong kế hoạch:

Code as Policies (của chúng tôi)
while not obj_in_gripper("coke can"):
    robot.move_gripper_to("coke can")
    robot.close_gripper()
pos = robot.gripper.position
robot.move_gripper(pos.x, pos.y+0.1, pos.z)
robot.open_gripper()

Phương pháp của chúng tôi (CaP) không chỉ tận dụng các cấu trúc logic để chỉ định các vòng lặp phản hồi, mà còn tham số hóa (và viết các phần của) các nguyên thủy điều khiển cấp thấp. CaP giảm bớt nhu cầu thu thập dữ liệu và huấn luyện một tập hợp cố định các kỹ năng được định nghĩa trước hoặc các chính sách conditioned ngôn ngữ – những thứ tốn kém và thường vẫn domain-specific.

Tạo mã đã được khám phá với LLM [1], [48] và không có [49]. Program synthesis đã được chứng minh có khả năng vẽ các hình đơn giản [50] và tạo chính sách

--- TRANG 3 ---

giải quyết các tác vụ 2D [51]. Chúng tôi mở rộng trên những công trình này, cho thấy rằng (i) các LLM viết mã cho phép các khả năng lý luận mới (ví dụ, mã hóa các mối quan hệ không gian bằng cách dựa vào sự quen thuộc với các thư viện bên thứ ba) mà không cần huấn luyện bổ sung cần thiết trong các công trình trước [35], [36], [52]–[56], và (ii) hierarchical code-writing (lấy cảm hứng từ recursive summarization [57]) cải thiện state-of-the-art code generation. Chúng tôi cũng trình bày một benchmark code-gen chủ đề robotics mới để đánh giá các mô hình ngôn ngữ tương lai trong lĩnh vực robotics.

III. PHƯƠNG PHÁP

Trong phần này, chúng tôi đặc trưng mức độ mà các LLM được pre-train có thể được prompt để tạo mã như chính sách – được biểu diễn như một tập hợp các chương trình mô hình ngôn ngữ (LMP). Nói chung, chúng tôi sử dụng thuật ngữ LMP để tham chiếu bất kỳ chương trình nào được tạo bởi mô hình ngôn ngữ và được thực thi trên hệ thống. Công trình này điều tra Code as Policies, một lớp LMP ánh xạ từ hướng dẫn ngôn ngữ đến các đoạn mã mà (i) phản ứng với đầu vào perceptual (tức là từ cảm biến hoặc module trên cảm biến), (ii) tham số hóa các API nguyên thủy điều khiển, và (iii) được compile và thực thi trực tiếp trên robot, ví dụ:

# xếp các khối vào bát trống.
empty_bowl_name = parse_obj('empty bowl')
block_names = parse_obj('blocks')
obj_names = [empty_bowl_name] + block_names
stack_objs_in_order(obj_names=obj_names)

Hướng dẫn đầu vào được định dạng như nhận xét (màu xanh), có thể được cung cấp bởi con người hoặc được viết bởi một LMP khác. Đầu ra dự đoán từ LLM (được làm nổi bật) được mong đợi là mã Python hợp lệ, được tạo autoregressively [11], [12]. LMP được prompt few-shot với các ví dụ để tạo các chương trình con khác nhau có thể xử lý kết quả phát hiện đối tượng, xây dựng quỹ đạo, hoặc sắp xếp tuần tự các nguyên thủy điều khiển. LMP có thể được tạo hierarchically bằng cách tổ hợp các hàm đã biết (ví dụ, get_obj_names() sử dụng các module nhận thức) hoặc gọi các LMP khác để định nghĩa các hàm chưa xác định:

# định nghĩa hàm stack_objs_in_order(obj_names).
def stack_objs_in_order(obj_names):
    for i in range(len(obj_names) - 1):
        put_first_on_second(obj_names[i + 1], obj_names[i])

trong đó put_first_on_second là một nguyên thủy pick and place từ vựng mở hiện có (ví dụ, CLIPort [36]). Đối với các embodiment mới, những lời gọi hàm hoạt động này có thể được thay thế bằng các API điều khiển có sẵn đại diện cho không gian hành động (ví dụ, set_velocity) của agent. Hierarchical code-gen với tên biến verbose có thể được xem như một biến thể của chain of thought prompting [47] thông qua lập trình hàm. Các hàm được định nghĩa bởi LMP có thể tích lũy tiến triển theo thời gian, nơi các LMP mới có thể tham chiếu các hàm được xây dựng trước đó để mở rộng logic chính sách.

Để thực thi một LMP, đầu tiên chúng tôi kiểm tra rằng nó an toàn để chạy bằng cách đảm bảo không có câu lệnh import, biến đặc biệt bắt đầu với __, hoặc lời gọi exec và eval. Sau đó, chúng tôi gọi hàm exec của Python với mã làm chuỗi đầu vào và hai dictionary tạo thành phạm vi của việc thực thi mã đó: (i) globals, chứa tất cả API mà mã được tạo có thể gọi, và (ii) locals, một dictionary trống sẽ được điền với các biến và hàm mới được định nghĩa trong quá trình exec. Nếu LMP được mong đợi trả về một giá trị, chúng tôi lấy nó từ locals sau khi exec hoàn thành.

A. Prompting Language Model Programs

Prompt để tạo LMP chứa hai yếu tố:

1. Hints ví dụ như câu lệnh import thông báo cho LLM những API nào có sẵn và type hints về cách sử dụng những API đó.

import numpy as np
from utils import get_obj_names, put_first_on_second

2. Examples là các cặp instruction-to-code trình bày "demonstration" few-shot về cách các hướng dẫn ngôn ngữ tự nhiên nên được chuyển đổi thành mã. Những điều này có thể bao gồm thực hiện số học, gọi các API khác, và các tính năng khác của ngôn ngữ lập trình. Hướng dẫn được viết như nhận xét trực tiếp trước một khối mã giải pháp tương ứng. Chúng ta có thể duy trì một "session" LMP bằng cách incrementally append các hướng dẫn và phản hồi mới vào prompt, cho phép các hướng dẫn sau tham chiếu lại các hướng dẫn trước, như "undo hành động cuối cùng".

B. Ví dụ Language Model Programs (Cấp thấp)

LMP có lẽ được hiểu tốt nhất thông qua các ví dụ, mà phần sau xây dựng từ các hướng dẫn Python thuần túy đơn giản đến những hướng dẫn phức tạp hơn có thể hoàn thành các tác vụ robot. Tất cả các ví dụ và thí nghiệm trong bài báo này, trừ khi được nêu khác, sử dụng OpenAI Codex code-davinci-002 với temperature 0 (tức là, deterministic greedy token decoding). Ở đây, prompt (màu xám) bắt đầu với một Hint để chỉ ra chúng ta đang viết Python. Sau đó nó đưa ra một Example để chỉ định định dạng của các giá trị trả về, được gán cho một biến có tên ret_val. Hướng dẫn đầu vào màu xanh, và đầu ra được tạo được làm nổi bật:

# Python script
# get the variable a.
ret_val = a

# find the sum of variables a and b.
ret_val = a + b

# see if any number is divisible by 3 in a list called xs.
ret_val = any(x % 3 == 0 for x in xs)

Thư viện bên thứ ba. LLM viết mã Python lưu trữ kiến thức về nhiều thư viện phổ biến. LMP có thể được prompt để sử dụng những thư viện này để thực hiện các hướng dẫn phức tạp mà không cần viết tất cả mã như sử dụng NumPy để gợi lý luận không gian với tọa độ. Hints ở đây bao gồm câu lệnh import, và Examples định nghĩa các hướng cardinal. Tên biến cũng quan trọng để chỉ ra rằng pts_np và pt_np là các mảng NumPy. Các phép toán với vector 2D ngụ ý rằng các điểm cũng là 2D. Ví dụ:

import numpy as np
# move all points in pts_np toward the right.
ret_val = pts_np + [0.3, 0]

# move a pt_np toward the top.
ret_val = pt_np + [0, 0.3]

# get the left most point in pts_np.
ret_val = pts_np[np.argmin(pts_np[:, 0]), :]

# get the center of pts_np.
ret_val = np.mean(pts_np, axis=0)

# the closest point in pts_np to pt_np.
ret_val = pts_np[np.argmin(np.sum((pts_np - pt_np)**2, axis=1))]

Thư viện first-party. LMP cũng có thể sử dụng thư viện first-party (API nguyên thủy nhận thức hoặc điều khiển) không được tìm thấy trong dữ liệu huấn luyện nếu những hàm đó có tên có ý nghĩa và được cung cấp trong Hints/Examples. Ví dụ (prompt đầy đủ trong B.2):

--- TRANG 4 ---

from utils import get_pos, put_first_on_second
...
# move the purple bowl toward the left.
target_pos = get_pos('purple bowl') + [-0.3, 0]
put_first_on_second('purple bowl', target_pos)

objs = ['blue bowl', 'red block', 'red bowl', 'blue block']
# move the red block a bit to the right.
target_pos = get_pos('red block') + [0.1, 0]
put_first_on_second('red block', target_pos)

# put the blue block on the bowl with the same color.
put_first_on_second('blue block', 'blue bowl')

Hints import hai hàm cho một lĩnh vực robot: một để lấy vị trí 2D của một đối tượng theo tên (sử dụng bộ phát hiện đối tượng từ vựng mở [2]) và một khác để đặt đối tượng đầu tiên lên mục tiêu thứ hai, có thể là tên đối tượng hoặc vị trí 2D. Lưu ý khả năng thích ứng của LMP với các hướng dẫn mới — đầu tiên sửa đổi độ lớn chuyển động bằng cách sử dụng "một chút," trong khi thứ hai liên kết đối tượng với "cùng màu."

Lý luận ngôn ngữ có thể được prompt few-shot sử dụng LLM viết mã (prompt đầy đủ trong B.1) để như liên kết tên đối tượng với các mô tả ngôn ngữ tự nhiên ("khối màu biển"), danh mục ("bát"), hoặc ngữ cảnh quá khứ ("khối khác"):

objs = ['blue bowl', 'red block', 'red bowl', 'blue block']
# the bowls.
ret_val = ['blue bowl', 'red bowl']

# sea-colored block.
ret_val = 'blue block'

# the other block.
ret_val = 'red block'

C. Ví dụ Language Model Programs (Cấp cao)

Control flows. Ngôn ngữ lập trình cho phép sử dụng các cấu trúc điều khiển như câu lệnh if-else và loop. Trước đây chúng tôi đã chỉ ra LMP có thể biểu thị for-loop dưới dạng list comprehension. Ở đây chúng tôi chỉ ra cách chúng có thể viết while-loop có thể tạo thành một chính sách phản hồi đơn giản. Lưu ý rằng prompt (giống như trong B.2) không chứa Examples như vậy:

# while the red block is to the left of the blue bowl, move it to the right 5cm at a time.
while get_pos('red block')[0] < get_pos('blue bowl')[0]:
    target_pos = get_pos('red block') + [0.05, 0]
    put_first_on_second('red block', target_pos)

LMP có thể được tổ hợp thông qua các lời gọi hàm lồng nhau. Điều này cho phép bao gồm nhiều ví dụ few-shot hơn vào các prompt riêng lẻ để cải thiện độ chính xác và phạm vi chức năng, trong khi vẫn ở trong độ dài token đầu vào tối đa của LLM. Sau đây (prompt đầy đủ trong B.4) tạo phản hồi sử dụng parse_obj, một LMP khác liên kết tên đối tượng với các mô tả ngôn ngữ:

objs = ['red block', 'blue bowl', 'blue block', 'red bowl']
# while the left most block is the red block, move it toward the right.
block_name = parse_obj('the left most block')
while block_name == 'red block':
    target_pos = get_pos(block_name) + [0.3, 0]
    put_first_on_second(block_name, target_pos)
    block_name = parse_obj('the left most block')

LMP parse_obj (prompt đầy đủ trong Phụ lục B.5):

objs = ['red block', 'blue bowl', 'blue block', 'red bowl']
# the left most block.
block_names = ['red block', 'blue block']
block_positions = np.array([get_pos(name) for name in block_names])
left_block_name = block_names[np.argmin(block_positions[:, 0])]
ret_val = left_block_name

LMP có thể tạo hàm hierarchically để tái sử dụng trong tương lai:

import numpy as np
from utils import get_obj_bbox_xyxy

# define function: total = get_total(xs).
def get_total(xs):
    return np.sum(xs)

# define function: get_objs_bigger_than_area_th(obj_names, bbox_area_th).
def get_objs_bigger_than_area_th(obj_names, bbox_area_th):
    return [name for name in obj_names
            if get_obj_bbox_area(name) > bbox_area_th]

Tạo hàm có thể được thực hiện bằng cách phân tích mã được tạo bởi LMP, định vị các hàm chưa được định nghĩa, và gọi một LMP khác chuyên về tạo hàm để tạo những hàm đó. Điều này cho phép cả prompt và mã được tạo bởi LMP gọi các hàm chưa được định nghĩa. Kỹ sư prompt sẽ không còn cần cung cấp tất cả chi tiết triển khai trong Examples — một "bản phác thảo thô" của logic mã có thể đủ.

LMP cấp cao cũng có thể tuân theo các thực hành trừu tượng tốt và tránh "làm phẳng" tất cả logic mã onto một cấp độ. Ngoài việc làm cho mã kết quả dễ đọc hơn, điều này cải thiện hiệu suất tạo mã như được hiển thị trong Phần IV-A. Định vị các hàm chưa được định nghĩa cũng được thực hiện trong thân của các hàm được tạo. Lưu ý trong ví dụ trên, get_obj_bbox_area không phải là lời gọi API được cung cấp. Thay vào đó, nó có thể được tạo khi cần:

# define function: get_obj_bbox_area(obj_name).
def get_obj_bbox_area(obj_name):
    x1, y1, x2, y2 = get_obj_bbox_xyxy(obj_name)
    return (x2 - x1) * (y2 - y1)

Lưu ý prompt không chỉ định chính xác get_obj_bbox_xyxy trả về gì, nhưng tên gợi ý rằng nó chứa tọa độ xy tối thiểu và tối đa của bounding box axis-aligned, và LLM có thể suy ra điều này và tạo mã đúng.

Trong Python, chúng tôi triển khai tạo hàm hierarchical bằng cách phân tích abstract syntax tree của khối mã và kiểm tra các hàm không tồn tại trong phạm vi đã cho. Chúng tôi sử dụng LMP tạo hàm để viết những hàm chưa xác định này và thêm chúng vào phạm vi. Thủ tục này được lặp lại trên thân hàm được tạo, hierarchically tạo các hàm mới theo cách depth-first.

Kết hợp control flows, tổ hợp LMP, và tạo hàm hierarchical. Ví dụ sau cho thấy cách LMP có thể kết hợp những khả năng này để tuân theo các hướng dẫn phức tạp hơn và thực hiện một tác vụ trong lĩnh vực thao tác tabletop. Prompt được bỏ qua để ngắn gọn, nhưng chúng tương tự như những cái trước đó. LMP cấp cao tạo hành vi chính sách cấp cao và dựa vào parse_obj để lấy tên đối tượng bằng các mô tả ngôn ngữ:

objs = ['red block', 'blue bowl', 'blue block', 'red bowl']
# while there are blocks with area bigger than 0.2 that are left of the red bowl, move them toward the right.
block_names = parse_obj('blocks with area bigger than 0.2 that are left of the red bowl')
while len(block_names) > 0:
    for block_name in block_names:
        target_pos = get_pos(block_name) + np.array([0.1, 0])
        put_first_on_second(block_name, target_pos)
    block_names = parse_obj('blocks with area bigger than 0.2 that are left of the red bowl')

Sau đó, parse_obj sử dụng get_objs_bigger_than_area_th (hàm chưa được định nghĩa), để hoàn thành truy vấn. Hàm này được đưa ra thông qua câu lệnh import trong Hints của prompt parse_obj, nhưng nó không được triển khai. Tạo hàm hierarchical sau đó sẽ tạo hàm này như đã chứng minh ở trên.

--- TRANG 5 ---

objs = ['red block', 'blue bowl', 'blue block', 'red bowl']
# blocks with area bigger than 0.2 that are left of the red bowl.
block_names = ['red block', 'blue block']
red_bowl_pos = get_pos('red bowl')
use_block_names = [name for name in block_names
                   if get_pos(name)[0] < red_bowl_pos[0]]
use_block_names = get_objs_bigger_than_area_th(use_block_names, 0.2)
ret_val = use_block_names

Chúng tôi mô tả thêm về prompt engineering trong Phụ lục A.

D. Language Model Programs as Policies

Trong bối cảnh chính sách robot, LMP có thể tổ hợp logic phản hồi perception-to-control được đưa ra các hướng dẫn ngôn ngữ tự nhiên, nơi đầu ra cấp cao của (các) mô hình nhận thức (states) có thể được thao tác lập trình và được sử dụng để thông báo các tham số của API điều khiển cấp thấp (actions). Thông tin trước về các API nhận thức và điều khiển có sẵn có thể được hướng dẫn thông qua Examples và Hints. Những API này "ground" LMP vào hệ thống robot thế giới thực, và cải thiện trong các thuật toán nhận thức và điều khiển có thể trực tiếp dẫn đến cải thiện khả năng của các chính sách dựa trên LMP. Ví dụ, trong các thí nghiệm thế giới thực bên dưới, chúng tôi sử dụng các mô hình phát hiện đối tượng từ vựng mở được phát triển gần đây như ViLD [3] và MDETR [2] off-the-shelf để lấy vị trí và bounding box đối tượng.

Lợi ích của các chính sách dựa trên LMP là ba mặt: chúng (i) có thể thích ứng mã chính sách và tham số với các tác vụ và hành vi mới được chỉ định bởi các hướng dẫn ngôn ngữ tự nhiên chưa thấy, (ii) có thể tổng quát hóa cho các đối tượng và môi trường mới bằng cách bootstrapping off các hệ thống nhận thức từ vựng mở và/hoặc các mô hình saliency, và (iii) không yêu cầu bất kỳ thu thập dữ liệu bổ sung hoặc huấn luyện mô hình nào. Các kế hoạch và chính sách được tạo cũng có thể diễn giải được vì chúng được biểu diễn trong mã, cho phép sửa đổi và tái sử dụng dễ dàng. Sử dụng LMP cho tương tác người dùng cấp cao kế thừa lợi ích của LLM, bao gồm phân tích ngôn ngữ tự nhiên biểu thị với kiến thức thường thức, tính đến ngữ cảnh trước đó, khả năng đa ngôn ngữ, và tham gia vào đối thoại.

Trong phần thí nghiệm sau đây, chúng tôi chứng minh nhiều instantiation của LMP trên các robot khác nhau và các tác vụ khác nhau, thể hiện khả năng linh hoạt và dễ sử dụng của phương pháp.

IV. THÍ NGHIỆM

Mục tiêu của các thí nghiệm chúng tôi là ba mặt: (i) đánh giá tác động của việc sử dụng hierarchical code generation (trên các mô hình ngôn ngữ khác nhau) và phân tích các chế độ tổng quát hóa, (ii) so sánh Code as Policies (CaP) với baseline trong các tác vụ thao tác được hướng dẫn bằng ngôn ngữ mô phỏng, và (iii) chứng minh CaP trên các hệ thống robot khác nhau để thể hiện tính linh hoạt và dễ sử dụng của nó. Các thí nghiệm bổ sung có thể được tìm thấy trong Phụ lục, như tạo các bộ điều khiển phản ứng để cân bằng cartpole và thực hiện điều khiển trở kháng end-effector (Phụ lục F). Phụ lục cũng chứa prompt và phản hồi cho tất cả các thí nghiệm. Video và Colab Notebook tái tạo những thí nghiệm này có thể được tìm thấy trên website. Do khó khăn trong việc đánh giá các tác vụ mở và thiếu baseline có thể so sánh, đánh giá định lượng của hệ thống robot sử dụng CaP được giới hạn trong một tập hợp các tác vụ mô phỏng bị ràng buộc trong IV-D, trong khi ở IV-B, IV-C, và IV-E chúng tôi chứng minh phạm vi đầy đủ các lệnh được hỗ trợ của hệ thống mà không có đánh giá định lượng.

BẢNG I: Hierarchical code-generation giải quyết nhiều bài toán hơn trong RoboCodeGen (theo % tỷ lệ pass) và cải thiện với quy luật scaling (khi # tham số mô hình tăng).

GPT-3 [12] Codex [1]
Phương pháp 6.7B 175B cushman davinci
Flat 3 68 54 81
Hierarchical 5 84 57 95

BẢNG II: Hierarchical code-gen hoạt động tốt hơn (% tỷ lệ pass) trên các bài toán coding chung từ HumanEval [1]. Greedy là giải mã LLM với temperature=0. P@N đánh giá tính đúng đắn trên N mẫu với temperature=0.8.

Greedy P@1 P@10 P@100
Flat 45.7 34.9 75.1 90.9
Hierarchical 53.0 39.8 80.6 95.7

A. Hierarchical LMP trên Code-Generation Benchmarks

Chúng tôi đánh giá phương pháp tạo mã của chúng tôi trên hai benchmark code-generation: (i) RoboCodeGen chủ đề robotics và (ii) HumanEval [1], bao gồm các bài toán code-gen tiêu chuẩn.

RoboCodeGen: chúng tôi giới thiệu một benchmark mới với 37 bài toán tạo hàm có một số khác biệt chính từ các benchmark code-gen trước đó: (i) nó có chủ đề robotics với các câu hỏi về lý luận không gian (ví dụ, tìm điểm gần nhất với một tập hợp điểm), lý luận hình học (ví dụ, kiểm tra nếu một bounding box được chứa trong box khác), và điều khiển (ví dụ, điều khiển PD), (ii) sử dụng thư viện bên thứ ba (ví dụ NumPy) được phép và được khuyến khích, (iii) header hàm được cung cấp không có docstring hoặc type hint rõ ràng, vì vậy LLM cần suy ra và sử dụng các quy ước chung, và (iv) sử dụng các hàm chưa được định nghĩa cũng được phép, có thể được tạo với hierarchical code-gen. Các câu hỏi benchmark ví dụ có thể được tìm thấy trong Phụ lục E. Chúng tôi đánh giá trên bốn LLM có thể truy cập từ OpenAI API¹. Như với các benchmark tiêu chuẩn [1], metric đánh giá của chúng tôi là phần trăm mã được tạo vượt qua các unit test được viết bởi con người. Xem Bảng I. Các mô hình ngôn ngữ domain-specific (mô hình Codex) thường hoạt động tốt hơn. Trong mỗi family mô hình, hiệu suất cải thiện với các mô hình lớn hơn. Hierarchical hoạt động tốt hơn trên toàn bộ, cho thấy lợi ích của việc cho phép LLM chia nhỏ các hàm phức tạp thành các phần hierarchical và tạo mã cho mỗi phần riêng biệt.

Chúng tôi cũng phân tích cách hiệu suất tạo mã thay đổi trên năm loại tổng quát hóa được đề xuất trong [23]. Hierarchical giúp Productivity nhiều nhất, đó là khi hướng dẫn mới yêu cầu mã dài hơn, hoặc mã với nhiều lớp logic hơn so với những trong Examples. Tuy nhiên, những cải thiện này chỉ xảy ra với hai mô hình davinci, và không phải cushman, gợi ý rằng một mức độ khả năng tạo mã nhất định cần được đạt tới trước khi hierarchical code-gen có thể mang lại cải thiện thêm. Nhiều kết quả hơn trong Phụ lục E.2.

Đánh giá trong HumanEval [1] chứng minh rằng hierarchical code-gen cải thiện không chỉ mã chính sách, mà còn mã đa mục đích. Xem Bảng II. Số liệu đạt được cao hơn trong các công trình gần đây [1], [11], [58]. Chi tiết thêm trong Phụ lục D.

B. CaP: Vẽ hình thông qua Generated Waypoints

Trong lĩnh vực này, chúng tôi giao nhiệm vụ cho cánh tay robot UR5e thực vẽ các hình dạng khác nhau bằng cách tạo và theo dõi một chuỗi điểm đường 2D. Đối với

¹Hai mô hình text: mô hình GPT-3 6.7B [12] và InstructGPT 175B [22]. Hai mô hình Codex [1]: cushman và davinci, được huấn luyện để tạo mã. davinci lớn hơn và tốt hơn. Kích thước của các mô hình Codex không được công khai.

--- TRANG 6 ---

BẢNG III: Tỷ lệ thành công trên các họ tác vụ với 50 thử nghiệm mỗi tác vụ.

Train/Test Họ Tác vụ CLIPort [36] NL Planner CaP (của chúng tôi)
SA SI Long-Horizon 78.80 86.40 97.20
SA SI Spatial-Geometric 97.33 N/A 89.30
UA SI Long-Horizon 36.80 88.00 97.60
UA SI Spatial-Geometric 0.00 N/A 73.33
UA UI Long-Horizon 0.00 64.00 80.00
UA UI Spatial-Geometric 0.01 N/A 62.00

nhận thức, LMP được cung cấp API phát hiện vị trí đối tượng, được triển khai với bộ phát hiện đối tượng từ vựng mở off-the-shelf MDETR [2]. Đối với hành động, một API theo dõi quỹ đạo end-effector được cung cấp. Có bốn LMP: (i) phân tích lệnh người dùng, duy trì session, và gọi API hành động, (ii) phân tích tên đối tượng từ các mô tả ngôn ngữ, (iii) phân tích điểm đường từ các mô tả ngôn ngữ, và (iv) tạo hàm mới. Các ví dụ về việc thực thi thành công trên robot các lệnh ngôn ngữ chưa thấy có trong Hình 2c. Hệ thống có thể gợi lý luận không gian để vẽ các hình hoàn toàn mới từ lệnh ngôn ngữ. Các ví dụ bổ sung thể hiện khả năng phân tích kích thước chính xác, thao tác hình trước đó, và lệnh nhiều bước, cũng như prompt đầy đủ, có trong Phụ lục H.

C. CaP: Pick & Place Policies for Table-Top Manipulation

Lĩnh vực thao tác table-top giao nhiệm vụ cho cánh tay robot UR5e pick và place các đối tượng đồ chơi nhựa khác nhau trên bàn. Cánh tay được trang bị gripper hút và camera Intel Realsense D435 trong tay. Chúng tôi cung cấp API nhận thức phát hiện sự hiện diện của đối tượng, vị trí của chúng, và bounding box, thông qua MDETR [2]. Chúng tôi cũng cung cấp một nguyên thủy scripted pick một đối tượng và đặt nó trên vị trí mục tiêu. Prompt tương tự như những từ lĩnh vực cuối, ngoại trừ việc phân tích quỹ đạo được thay thế bằng phân tích vị trí. Các ví dụ về việc thực thi trên robot các lệnh ngôn ngữ chưa thấy có trong Hình 2 panel a và b, cho thấy khả năng lý luận về các mô tả đối tượng và mối quan hệ không gian. Các lệnh khác sử dụng ngữ cảnh lịch sử (ví dụ, "undo cái đó"), lý luận về đối tượng thông qua mô tả hình học (ví dụ, "nhỏ nhất") và không gian (ví dụ, "phải nhất") có trong Phụ lục I.

D. CaP: Table-Top Manipulation Simulation Evaluations

Chúng tôi đánh giá CaP trên môi trường thao tác table-top mô phỏng từ [16], [18]. Thiết lập giao nhiệm vụ cho cánh tay UR5e và gripper Robotiq 2F85 thao tác 10 khối màu và 10 bát màu. Chúng tôi kế thừa tất cả 8 tác vụ, được gọi là tác vụ "long-horizon" do tính chất nhiều bước của chúng (ví dụ, "đặt các khối vào bát khớp màu"). Chúng tôi định nghĩa 6 tác vụ mới yêu cầu khả năng lý luận không gian-hình học thách thức và chính xác hơn (ví dụ, "đặt các khối trong đường chéo"). Mỗi tác vụ được tham số hóa bởi một số thuộc tính (ví dụ, "nhặt <obj> và đặt nó trong <corner>"), được lấy mẫu trong mỗi thử nghiệm. Chúng tôi chia hướng dẫn tác vụ (I) và thuộc tính (A) thành danh mục "seen" (SI, SA) và "unseen" (UI, UA), nơi "seen" có nghĩa là nó được phép xuất hiện trong prompt hoặc được huấn luyện trên (trong trường hợp baseline có giám sát). Chi tiết thêm trong Phụ lục K. Chúng tôi xem xét hai baseline: (i) chính sách CLIPort [36] đa tác vụ conditioned ngôn ngữ được huấn luyện thông qua imitation learning trên 30k demonstration, và (ii) LLM planner được prompt few-shot sử dụng ngôn ngữ tự nhiên thay vì mã.

Kết quả trong Bảng III. CaP so sánh cạnh tranh với baseline CLIPort có giám sát trên các tác vụ với thuộc tính và hướng dẫn đã thấy, mặc dù chỉ được prompt few-shot với một ví dụ rollout cho mỗi tác vụ. Với thuộc tính tác vụ chưa thấy, hiệu suất của CLIPort suy giảm đáng kể, trong khi các phương pháp dựa trên LLM duy trì hiệu suất tương tự. Trên các tác vụ và thuộc tính chưa thấy, các hệ thống end-to-end như CLIPort gặp khó khăn để tổng quát hóa, và CaP vượt trội hơn lý luận LLM trực tiếp với ngôn ngữ (cũng được quan sát trong [20]). Hơn nữa, các planner ngôn ngữ tự nhiên [14], [16]–[18] không áp dụng được cho các tác vụ yêu cầu lý luận không gian-hình học số chính xác. Chúng tôi bổ sung cho thấy lợi ích của lý luận với mã so với ngôn ngữ tự nhiên (cả câu hỏi và trả lời trực tiếp và Chain of Thought [47]), cụ thể là khả năng của cái trước để thực hiện tính toán số chính xác, trong Phụ lục C.

E. CaP: Mobile Robot Navigation and Manipulation

Trong lĩnh vực này, một robot với base di động và cánh tay 7 DoF được giao nhiệm vụ thực hiện các tác vụ navigation và manipulation trong nhà bếp thế giới thực. Đối với nhận thức, LMP được cung cấp API phát hiện đối tượng được triển khai thông qua ViLD [3]. Đối với hành động, robot được cung cấp API để navigate đến vị trí và grasp đối tượng thông qua cả tên và tọa độ. Các ví dụ về việc thực thi trên robot các lệnh ngôn ngữ chưa thấy có trong Hình 2. Lĩnh vực này cho thấy rằng CaP có thể được triển khai trên các tác vụ thực tế trên các hệ thống robot khác nhau với các API khác nhau. Nó cũng minh họa khả năng tuân theo các lệnh phản ứng long-horizon với cấu trúc điều khiển cũng như lý luận không gian chính xác, không thể dễ dàng thực hiện bởi các công trình trước [16], [17], [36]. Xem prompt và ví dụ bổ sung trong Phụ lục J.

V. THẢO LUẬN VÀ HẠN CHẾ

CaP tổng quát hóa tại một lớp cụ thể trong stack robot: diễn giải hướng dẫn ngôn ngữ tự nhiên, xử lý đầu ra nhận thức, sau đó tham số hóa đầu vào chiều thấp cho các nguyên thủy điều khiển. Điều này phù hợp với các hệ thống có nhận thức và điều khiển được factorized, và nó trao một mức độ tổng quát hóa (thu được từ LLM được pre-train) mà không cần cường độ thu thập dữ liệu cần thiết cho việc học end-to-end. Phương pháp của chúng tôi cũng kế thừa khả năng LLM không liên quan đến viết mã như hỗ trợ hướng dẫn với ngôn ngữ không phải tiếng Anh hoặc emoji (Phụ lục L). CaP cũng có thể biểu thị các kế hoạch cross-embodied thực hiện cùng một tác vụ khác nhau tùy thuộc vào API có sẵn (Phụ lục M).

Tuy nhiên, khả năng này dễ vỡ với LLM hiện tại, và có thể yêu cầu những cái lớn hơn được huấn luyện trên mã domain-specific.

CaP ngày nay bị hạn chế bởi phạm vi của (i) những gì API nhận thức có thể mô tả (ví dụ, không có mô hình ngôn ngữ thị giác nào đến nay có thể mô tả liệu một quỹ đạo có "gập ghềnh" hay "hình C hơn"), và (ii) những nguyên thủy điều khiển nào có sẵn. Chỉ một số ít tham số nguyên thủy được đặt tên có thể được điều chỉnh mà không làm quá tải prompt. CaP cũng gặp khó khăn để diễn giải các lệnh dài hoặc phức tạp hơn đáng kể, hoặc hoạt động ở mức trừu tượng khác so với Examples đã cho. Trong lĩnh vực tabletop, sẽ khó cho LMP "xây nhà với các khối," vì không có Examples về xây dựng cấu trúc 3D phức tạp. Phương pháp của chúng tôi cũng giả định tất cả hướng dẫn đã cho là khả thi, và chúng tôi không thể biết liệu một phản hồi sẽ đúng trước.

LỜI CẢM ƠN

Cảm ơn đặc biệt đến Vikas Sindhwani, Vincent Vanhoucke cho phản hồi hữu ích về viết, Chad Boodoo cho hỗ trợ vận hành và phần cứng.

--- TRANG 7 ---

TÀI LIỆU THAM KHẢO

[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv:2107.03374, 2021.

[2] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, và N. Carion, "Mdetr-modulated detection for end-to-end multi-modal understanding," trong ICCV, 2021.

[3] X. Gu, T.-Y. Lin, W. Kuo, và Y. Cui, "Open-vocabulary object detection via vision and language knowledge distillation," arXiv:2104.13921, 2021.

[4] S. Tellex, N. Gopalan, H. Kress-Gazit, và C. Matuszek, "Robots that use language," Review of Control, Robotics, and Autonomous Systems, 2020.

[5] T. Winograd, "Procedures as a representation for data in a computer program for understanding natural language," MIT PROJECT MAC, 1971.

[6] J. Dzifcak, M. Scheutz, C. Baral, và P. Schermerhorn, "What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution," trong ICRA, 2009.

[7] Y. Artzi và L. Zettlemoyer, "Weakly supervised learning of semantic parsers for mapping instructions to actions," TACL, 2013.

[8] C. Lynch và P. Sermanet, "Language conditioned imitation learning over unstructured data," arXiv:2005.07648, 2020.

[9] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, và C. Finn, "Bc-z: Zero-shot task generalization with robotic imitation learning," trong CoRL, 2022.

[10] O. Mees, L. Hermann, E. Rosete-Beas, và W. Burgard, "Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks," RA-L, 2022.

[11] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," arXiv:2204.02311, 2022.

[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," NeurIPS, 2020.

[13] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., "Opt: Open pre-trained transformer language models," arXiv:2205.01068, 2022.

[14] W. Huang, P. Abbeel, D. Pathak, và I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," arXiv:2201.07207, 2022.

[15] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, và Y. Iwasawa, "Large language models are zero-shot reasoners," arXiv:2205.11916, 2022.

[16] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke et al., "Socratic models: Composing zero-shot multimodal reasoning with language," arXiv:2204.00598, 2022.

[17] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog et al., "Do as i can, not as i say: Grounding language in robotic affordances," arXiv:2204.01691, 2022.

[18] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, và B. Ichter, "Inner monologue: Embodied reasoning through planning with language models," trong arXiv:2207.05608, 2022.

[19] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, và J. Tompson, "Implicit behavioral cloning," trong CoRL, 2022.

[20] A. Zeng, "Learning visual affordances for robotic manipulation," Ph.D. dissertation, Princeton University, 2019.

[21] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., "Scalable deep reinforcement learning for vision-based robotic manipulation," trong CoRL, 2018.

[22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," arXiv:2203.02155, 2022.

[23] D. Hupkes, V. Dankers, M. Mul, và E. Bruni, "Compositionality decomposed: How do neural networks generalise?" JAIR, 2020.

[24] C. Breazeal, K. Dautenhahn, và T. Kanda, "Social robotics," Springer handbook of robotics, 2016.

[25] T. Kollar, S. Tellex, D. Roy, và N. Roy, "Toward understanding natural language directions," trong HRI, 2010.

[26] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foerster, J. Andreas, E. Grefenstette, S. Whiteson, và T. Rocktäschel, "A survey of reinforcement learning informed by natural language," trong IJCAI, 2019.

[27] M. MacMahon, B. Stankiewicz, và B. Kuipers, "Walk the talk: Connecting language, knowledge, and action in route instructions," AAAI, 2006.

[28] J. Thomason, S. Zhang, R. J. Mooney, và P. Stone, "Learning to interpret natural language commands through human-robot dialog," trong IJCAI, 2015.

[29] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, và N. Roy, "Understanding natural language commands for robotic navigation and mobile manipulation," trong AAAI, 2011.

[30] D. Shah, B. Osinski, B. Ichter, và S. Levine, "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action," arXiv:2207.04429, 2022.

[31] C. Matuszek, E. Herbst, L. Zettlemoyer, và D. Fox, "Learning to parse natural language commands to a robot control system," trong Experimental robotics, 2013.

[32] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y. Jiang, H. Yedidsion, J. Hart, P. Stone, và R. Mooney, "Jointly improving parsing and perception for natural language commands through human-robot dialog," JAIR, 2020.

[33] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn et al., "Learning language-conditioned robot behavior from offline data and crowd-sourced annotation," trong CoRL, 2022.

[34] J. Andreas, D. Klein, và S. Levine, "Learning with latent language," arXiv:1711.00482, 2017.

[35] P. Sharma, B. Sundaralingam, V. Blukis, C. Paxton, T. Hermans, A. Torralba, J. Andreas, và D. Fox, "Correcting robot plans with natural language feedback," arXiv:2204.05186, 2022.

[36] M. Shridhar, L. Manuelli, và D. Fox, "Cliport: What and where pathways for robotic manipulation," trong CoRL, 2021.

[37] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, và H. Ben Amor, "Language-conditioned imitation learning for robot manipulation tasks," NeurIPS, 2020.

[38] Y. Jiang, S. S. Gu, K. P. Murphy, và C. Finn, "Language as an abstraction for hierarchical deep reinforcement learning," NeurIPS, 2019.

[39] P. Goyal, S. Niekum, và R. J. Mooney, "Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards," arXiv:2007.15543, 2020.

[40] G. Cideron, M. Seurin, F. Strub, và O. Pietquin, "Self-educated language agent with hindsight experience replay for instruction following," DeepMind, 2019.

[41] D. Misra, J. Langford, và Y. Artzi, "Mapping instructions and visual observations to actions with reinforcement learning," arXiv:1704.08795, 2017.

[42] A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, và O. Sigaud, "Grounding language to autonomously-acquired skills via goal generation," arXiv:2006.07185, 2020.

[43] I. Drori, S. Zhang, R. Shuttleworth, L. Tang, A. Lu, E. Ke, K. Liu, L. Chen, S. Tran, N. Cheng et al., "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level," PNAS, 2022.

[44] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al., "Solving quantitative reasoning problems with language models," arXiv:2206.14858, 2022.

[45] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, và J. Schulman, "Training verifiers to solve math word problems," arXiv:2110.14168, 2021.

[46] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, và E. Chi, "Least-to-most prompting enables complex reasoning in large language models," arXiv:2205.10625, 2022.

[47] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, và D. Zhou, "Chain of thought prompting elicits reasoning in large language models," arXiv:2201.11903, 2022.

[48] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., "Program synthesis with large language models," arXiv:2108.07732, 2021.

[49] K. Ellis, C. Wong, M. Nye, M. Sable-Meyer, L. Cary, L. Morales, L. Hewitt, A. Solar-Lezama, và J. B. Tenenbaum, "Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning," arXiv:2006.08381, 2020.

[50] L. Tian, K. Ellis, M. Kryven, và J. Tenenbaum, "Learning abstract structure for drawing by efficient motor program induction," NeurIPS, 2020.

[51] D. Trivedi, J. Zhang, S.-H. Sun, và J. J. Lim, "Learning to synthesize programs as interpretable and generalizable policies," NeurIPS, 2021.

[52] O. Mees và W. Burgard, "Composing pick-and-place tasks by grounding language," trong ISER, 2020.

[53] W. Liu, C. Paxton, T. Hermans, và D. Fox, "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects," trong ICRA, 2022.

[54] W. Yuan, C. Paxton, K. Desingh, và D. Fox, "Sornet: Spatial object-centric representations for sequential manipulation," trong CoRL, 2022.

[55] A. Bucker, L. Figueredo, S. Haddadin, A. Kapoor, S. Ma, và R. Bonatti, "Reshaping robot trajectories using natural language commands: A study of multi-modal data alignment using transformers," arXiv:2203.13411, 2022.

--- TRANG 8 ---

[56] A. Bobu, C. Paxton, W. Yang, B. Sundaralingam, Y.-W. Chao, M. Cakmak, và D. Fox, "Learning perceptual concepts by bootstrapping from human queries," RA-L, 2022.

[57] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, và P. Christiano, "Recursively summarizing books with human feedback," arXiv:2109.10862, 2021.

[58] F. F. Xu, U. Alon, G. Neubig, và V. J. Hellendoorn, "A systematic evaluation of large language models of code," trong MAPS, 2022.

[59] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, và D. Dwibedi, "Xirl: Cross-embodiment inverse reinforcement learning," trong CoRL. PMLR, 2022.

[60] A. Ganapathi, P. Florence, J. Varley, K. Burns, K. Goldberg, và A. Zeng, "Implicit kinematic policies: Unifying joint and cartesian action spaces in end-to-end robot learning," arXiv:2203.01983, 2022.

--- TRANG 9 ---

PHỤ LỤC

A. Prompt Engineering

Sử dụng LMP để hoàn thành tác vụ một cách đáng tin cậy thông qua tạo mã yêu cầu prompt engineering cẩn thận. Mặc dù những prompt này không cần dài, chúng cần phải liên quan và cụ thể. Ở đây, chúng tôi thảo luận một số hướng dẫn chung mà chúng tôi đã tuân theo khi phát triển prompt cho bài báo này.

Rất quan trọng để prompt chứa mã không có lỗi. Lỗi trong prompt dẫn đến phản hồi không đáng tin cậy và không chính xác. Ngược lại, nếu LMP đang viết mã không chính xác cho một Instruction đã cho, kỹ sư prompt nên trước tiên xác minh rằng prompt, đặc biệt là Examples gần nhất với Instruction, không có lỗi.

Để giảm lỗi liên quan đến lỗi syntax, một phương pháp đơn giản là viết prompt trong code editor với syntax highlighting.

Có nhiều trường hợp prompt chứa biến hoặc hàm có tên mơ hồ. Để tạo phản hồi đáng tin cậy trong những điều kiện này, Examples trong prompt nên xử lý những mơ hồ này một cách nhất quán. Nếu một biến có tên point được xử lý như đối tượng numpy.ndarray trong một Example và như đối tượng shapely.geometry.Point trong Example khác, LMP sẽ không thể "quyết định" quy ước nào để sử dụng, dẫn đến phản hồi không đáng tin cậy. Một cách khác để xử lý mơ hồ là cung cấp type hint không chính thức, như thêm _np vào tên biến để chỉ ra loại của nó, hoặc thêm nó vào tên hàm để chỉ ra loại của biến được trả về. Nói chung, tên biến và hàm cụ thể hơn cho kết quả nhất quán hơn.

Để sử dụng thư viện bên thứ ba, bao gồm câu lệnh import trong prompt có thể không cần thiết, vì chúng tôi thấy rằng LMP có thể tạo mã gọi NumPy và SciPy mà không có chúng. Tuy nhiên, câu lệnh import rõ ràng cải thiện độ tin cậy và tăng cơ hội LMP sử dụng những thư viện đó khi cần thiết. Để sử dụng thư viện first party, tên hàm có ý nghĩa tuân theo quy ước phổ biến (ví dụ, bắt đầu với set_ và get_) và chỉ định định dạng đối tượng trả về (ví dụ, get_bbox_xyxy) gây ra việc sử dụng chính xác hơn. Câu lệnh import trong Hints nên được định dạng như thể chúng ta đang import hàm. Ví dụ, trong Python điều này có nghĩa là sử dụng from utils import function_name thay vì import function_name. Nếu cái sau được sử dụng, LMP có thể xử lý tên được import như package, và mã được tạo có thể viết function_name.function_name().

Một loại thất bại LMP liên quan đến tính đúng đắn của tạo mã. Ví dụ, lỗi coding nhỏ khi gọi API nội bộ hoặc bên ngoài, như thiếu argument, có thể được sửa với Hint hoặc Example chứng minh cách sử dụng đúng. Giả định không chính xác về loại biến cũng có thể được sửa theo cách tương tự. Các thất bại coding khác có thể được giải quyết bằng tên hàm mô tả để khuyến khích việc sử dụng thư viện phù hợp (perform_function_with_np()) hoặc logic mã ngắn gọn (# implement in one line.). Mặc dù có thể sử dụng LLM để chỉnh sửa mã và sửa lỗi (ví dụ, bằng cách sử dụng API chỉnh sửa mã của OpenAI), theo kinh nghiệm của chúng tôi điều này cho kết quả không nhất quán (không phải lúc nào cũng có thể sửa lỗi, và đôi khi thay đổi những gì hàm đang làm), vì vậy chúng tôi không sử dụng phương pháp này trong các thí nghiệm.

B. Prompt Phần Phương pháp

1) Lý luận dựa trên ngôn ngữ: Prompt đầy đủ:

objs = ['green block', 'green bowl', 'yellow block', 'yellow bowl']
# the yellow block.
ret_val = 'yellow block'
# the blocks.
ret_val = ['green block', 'yellow block']

2) First-party: Prompt đầy đủ:

from utils import get_pos, put_first_on_second
objs = ['gray block', 'gray bowl']
# put the gray block on the gray bowl.
put_first_on_second('gray block', 'gray bowl')

objs = ['purple block', 'purple bowl']
# move the purple bowl toward the left.
target_pos = get_pos('purple bowl') + [-0.3, 0]
put_first_on_second('purple bowl', target_pos)

3) Kết hợp lý luận ngôn ngữ, thư viện bên thứ ba, và first-party.: Prompt đầy đủ:

import numpy as np
from utils import get_pos, put_first_on_second
objs = ['cyan block', 'cyan bowl', 'pink bowl']
# put the cyan block in cyan bowl.
put_first_on_second('cyan block', 'cyan bowl')

objs = ['gray block', 'silver block', 'gray bowl']
# place the top most block on the gray bowl.
names = ['gray block', 'silver block']
positions = np.array([get_pos(name) for name in names])
name = names[np.argmax(positions[:,1])]
put_first_on_second(name, 'gray bowl')

objs = ['purple block', 'purple bowl']
# put the purple bowl to the left of the purple block.
target_pos = get_pos('purple block') + [-0.3, 0]
put_first_on_second('purple bowl', target_pos)

4) LMP có thể được tổ hợp.: Prompt đầy đủ:

import numpy as np
from utils import get_pos, put_first_on_second, parse_obj
objs = ['yellow block', 'yellow bowl', 'gray block', 'gray bowl']
# move the sun colored block toward the left.
block_name = parse_obj('sun colored block')
target_pos = get_pos(block_name) + [-0.3, 0]
put_first_on_second(block_name, target_pos)

objs = ['white block', 'white bowl', 'yellow block', 'yellow bowl']
# place the block closest to the blue bowl on the other bowl.
block_name = parse_obj('the block closest to the blue bowl')
bowl_name = parse_obj('a bowl other than the blue bowl')
put_first_on_second(block_name, bowl_name)

5) prompt parse_obj.: Prompt đầy đủ:

import numpy as np
from utils import get_pos
objs = ['brown bowl', 'green block', 'brown block', 'green bowl']
# the blocks.
ret_val = ['brown block', 'green block']
# the sky colored block.
ret_val = 'blue block'

objs = ['orange block', 'cyan block', 'purple bowl', 'gray bowl']
# the right most block.
block_names = ['orange block', 'cyan block']
block_positions = np.array([
    get_pos(block_name) for block_name in block_names])
right_block_name = block_names[np.argmax(block_positions[:, 0])]
ret_val = right_block_name

C. Lý luận với Mã vs. Ngôn ngữ Tự nhiên

Để điều tra cách lý luận liên quan đến robot thông qua LLM có thể được thực hiện với LMP thay vì với ngôn ngữ tự nhiên, chúng tôi tạo một benchmark bao gồm hai tập hợp tác vụ: (i) lựa chọn đối tượng trong cảnh từ các mô tả không gian-hình học, và (ii) lựa chọn tọa độ vị trí từ các mô tả không gian-hình học. Lựa chọn đối tượng có 28 câu hỏi với lệnh như "tìm tên của khối gần nhất với bát xanh," nơi danh sách vị trí khối và bát được cung cấp như ngữ cảnh đầu vào trong prompt.

--- TRANG 10 ---

Lựa chọn vị trí có 23 câu hỏi với lệnh như "nội suy 3 điểm trên đường từ bát cyan đến bát xanh." Một câu trả lời được tạo bởi LLM cho lựa chọn vị trí được coi là đúng nếu tất cả tọa độ nằm trong vòng 1cm từ ground truth.

Chúng tôi đánh giá LMP so với hai biến thể lý luận với ngôn ngữ tự nhiên: (i) Vanilla, được đưa ra mô tả về setting (ví dụ, danh sách vị trí đối tượng) và câu hỏi, trực tiếp xuất ra câu trả lời (ví dụ, "Q: Khối trên cùng là gì?" → "A: khối đỏ"), và (ii) Chain of Thought (CoT) [47], thực hiện lý luận từng bước được đưa ra các ví dụ về các bước trung gian trong prompt (ví dụ, khuyến khích LLM liệt kê tọa độ y của tất cả khối trong cảnh trước khi xác định khối trên cùng).

BẢNG IV: Sử dụng mã cho lý luận không gian-hình học cho tỷ lệ thành công cao hơn (mean %) so với sử dụng ngôn ngữ tự nhiên vanilla hoặc chain-of-thought prompting.

Ngôn ngữ Tự nhiên Mã
Tác vụ Vanilla CoT [47] LMP (của chúng tôi)
Lựa chọn Đối tượng 39 68 96
Lựa chọn Vị trí 30 48 100
Tổng 35 58 98

Kết quả trong Bảng IV cho thấy LMP đạt độ chính xác trong khoảng 90 cao, vượt trội hơn CoT, vượt trội hơn Vanilla. CoT cho phép LLM lý luận về mối quan hệ và thứ tự (ví dụ tọa độ nào ở bên phải tọa độ khác), nhưng thất bại xảy ra đối với tính toán số chính xác và nhiều bước. Ngược lại, mã từ LMP có thể sử dụng Python để thực hiện những tính toán như vậy, và chúng thường tận dụng thư viện bên ngoài để thực hiện các phép toán phức tạp hơn (ví dụ, NumPy cho phép cộng vector). CoT và LMP không loại trừ lẫn nhau – có thể prompt tạo mã "từng bước" để giải quyết các tác vụ phức tạp hơn thông qua CoT, nhưng đây là hướng không được khám phá trong công trình này.

D. Kết quả Bổ sung CodeGen HumanEval

Ở đây chúng tôi cung cấp kết quả bổ sung cho các thí nghiệm HumanEval của chúng tôi. Tổng cộng, ba biến thể của mô hình Codex lớn hơn (code-davinci-002) được kiểm tra. Phương pháp của chúng tôi là Hier. CodeGen + Hier Prompts, nơi prompt khuyến khích LLM gọi các hàm chưa được định nghĩa bằng cách bao gồm những ví dụ như vậy. Để so sánh, chúng tôi đánh giá so với Flat CodeGen + No Prompt, về cơ bản chỉ sử dụng LLM trực tiếp, và Flat CodeGen + Flat Prompt, để so sánh công bằng với flat code-generation, vì phương pháp hierarchical của chúng tôi có prompt. Prompt chỉ chứa 2 Examples:

Prompt cho Flat CodeGen:

prompt_f_gen_flat = """
def get_total(xs: List[float]) -> float:
    \"\"\"Find the sum of a list of numbers called xs.
    \"\"\"
    return sum(xs)
# end of function

def get_abs_diff_between_means(xs0: List[float],
                               xs1: List[float]) -> float:
    \"\"\"Get the absolute difference between the means of two
    lists of numbers.
    \"\"\"
    m0 = sum(xs0) / len(xs0)
    m1 = sum(xs1) / len(xs1)
    return abs(m0 - m1)
# end of function
"""

Prompt cho Hierarchical CodeGen:

def get_total(xs: List[float]) -> float:
    \"\"\"Find the sum of a list of numbers called xs.
    \"\"\"
    return sum(xs)
# end of function

def get_abs_diff_between_means(xs0: List[float],
                               xs1: List[float]) -> float:
    \"\"\"Get the absolute difference between the means of two lists of
    numbers.
    \"\"\"
    m0 = get_mean(xs0)
    m1 = get_mean(xs1)
    return abs(m0 - m1)
# end of function

Lưu ý sự khác biệt duy nhất trong prompt hierarchical là sử dụng hàm chưa được định nghĩa get_mean thay vì tính toán mean trực tiếp. Điều này "cho phép" LLM tạo mã cũng gọi các hàm chưa được định nghĩa.

Chúng tôi báo cáo tỷ lệ pass khi sử dụng đầu ra có khả năng cao nhất ("greedy", được thực hiện bằng cách đặt temperature về 0), cũng như tỷ lệ pass cho ít nhất một giải pháp từ sampling số lượng giải pháp khác nhau (1, 10, và 100) với temperature đặt về 0.8, tương tự như những cái được sử dụng trong các công trình trước [1], [11], [58].

BẢNG V: Hierarchical code generation cũng hoạt động tốt hơn (theo % tỷ lệ pass) trên các bài toán coding chung từ benchmark HumanEval tiêu chuẩn [1]. Đối với các cột, Greedy có nghĩa là giải mã LLM với temperature=0, trong khi P@N có nghĩa là đánh giá tính đúng đắn trên N mẫu được giải mã từ LLM với temperature=0.8.

Greedy P@1 P@10 P@100
code-davinci-001 [11] - 36.0 - 81.7
PaLM Coder [11] - 36.0 - 88.4
Flat CodeGen + No Prompt 45.7 34.9 75.1 90.9
Flat CodeGen + Flat Prompts 50.6 36.6 77.6 93.3
Hier. CodeGen + Hier Prompts 53.0 39.8 80.6 95.7

Xem kết quả trong Bảng II. Trong tất cả trường hợp hierarchical code generation vượt trội hơn flat code generation, và các số liệu đạt được cao hơn những báo cáo trong các công trình gần đây [1], [11], [58]. Lưu ý rằng chúng tôi sử dụng code-davinci-002, trong khi các công trình trước sử dụng code-davinci-001, nhưng cải thiện tương đối với hierarchical nhất quán trên toàn bộ. Trong số 164 câu hỏi trong HumanEval, 6.5% dẫn đến hierarchical code generation, nhưng trong đó cả hai biến thể Flat CodeGen đều đạt 44% thành công, trong khi Hier CodeGen code đạt 56%. Trong khi tỷ lệ thành công khi sampling 100 phản hồi trên 90% trên toàn bộ, chúng tôi lưu ý rằng sampling nhiều giải pháp không thực tế cho LMP, cần thực hiện các tác vụ theo cách zero-shot mà không cần kỹ thuật unit test trước. Do đó, đối với LMP chúng tôi luôn đặt temperature về 0 và sử dụng đầu ra có khả năng cao nhất.

E. Robot Code-Generation Benchmark

1) Câu hỏi Ví dụ: Đây là bốn loại câu hỏi benchmark và ví dụ của chúng:

• Phép toán Vector với Numpy:
pts = interpolate_pts_np(start, end, n)

• Điều khiển đơn giản:
u = pd_control(x_curr, x_goal, x_dot, Kp, Kv)

• Thao tác hình với shapely:
circle = make_circle(radius, center)

• Sử dụng thư viện first-party:
ret_val = obj_shape_does_not_contain_others(obj_name,
                                          other_obj_names)

--- TRANG 11 ---

Hình 4: Hiệu suất Robot Code-Generation Benchmark trên các Loại Tổng quát hóa cho Flat (trên) và Hierarchical (giữa) Code-Generation, cũng như cải thiện hiệu suất được thực hiện bởi Hierarchical Code-Generation (dưới).

Đối với loại cuối, chúng tôi cung cấp import của các hàm first-party, như những hàm lấy thông tin hình học đối tượng theo tên, như Hints trong prompt.

2) Phân tích Tổng quát hóa: Chúng tôi phân tích cách hiệu suất tạo mã hoạt động trên năm loại tổng quát hóa được mô tả trong [23], nơi tổng quát hóa được đánh giá bằng cách so sánh các ví dụ được đưa ra trong prompt với các hướng dẫn mới được đưa ra trong benchmark. Chúng tôi đưa ra mô tả về năm loại tổng quát hóa được áp dụng cho benchmark của chúng tôi. Cụ thể, chúng tôi nói rằng giải quyết một bài toán trong benchmark chứng minh một loại tổng quát hóa cụ thể nếu hướng dẫn hoặc giải pháp của bài toán thỏa mãn các điều kiện sau:

• Systematicity: tái tổ hợp các phần của hướng dẫn hoặc đoạn mã của Examples.
• Productivity: có mã dài hơn hoặc chứa nhiều cấp độ hơn (ví dụ, lời gọi hàm hierarchical) so với Examples.
• Substitutivity: sử dụng từ đồng nghĩa hoặc thay thế từ của các danh mục tương tự từ Examples.
• Localism: tái sử dụng các phần hoặc khái niệm đã thấy cho mục đích khác nhau.
• Overgeneralization: sử dụng lời gọi API mới hoặc tính năng ngôn ngữ lập trình không thấy trong Examples.

Trong Hình 4 chúng tôi báo cáo kết quả trên ba mô hình. Biểu đồ trên cho thấy sử dụng prompt flat và flat code generation, trong khi dưới sử dụng prompt hierarchical và hierarchical code generation. Thứ hạng tương đối của ba mô hình nhất quán trên tất cả loại tổng quát hóa, với code-davinci-002 hoạt động tốt nhất. Mô hình nhỏ nhất, code-cushman-001, hoạt động đặc biệt kém trong Productivity và Localism, nhưng nó hoạt động tốt hơn nhiều trên Substitutivity. Thật vậy, hiệu suất Substitutivity cao trên tất cả mô hình có thể chỉ ra rằng các mô hình ngôn ngữ đặc biệt robust với việc thay thế từ có ý nghĩa tương tự hoặc từ các danh mục tương tự, trong khi tổng quát hóa cho các lệnh phức tạp hơn (câu trả lời dài hơn thông qua Productivity hoặc duy trì cấu trúc cục bộ thông qua Localism) thách thức hơn. Hierarchical code generation cũng mang lại cải thiện hiệu suất đáng kể cho cả hai mô hình davinci, đặc biệt trên Productivity, cho phép những mô hình này viết hàm dài hơn và đa lớp hơn. Tuy nhiên, những cải thiện này không rõ ràng đối với cushman, thực sự có tỷ lệ thành công Substitutivity và Localism thấp hơn, gợi ý việc đạt một mức độ khả năng tạo mã nhất định là cần thiết trước khi Hierarchical Code-Generation có thể mang lại cải thiện thêm.

F. CaP: Reactive Controllers for Toy Tasks

LMP có thể biểu thị các bộ điều khiển phản ứng, và chúng tôi chứng minh điều này trên: (i) cân bằng CartPole của OpenAI Gym và (ii) điều khiển trở kháng vị trí end-effector. Sử dụng cùng cấu trúc prompt, cả hai bộ điều khiển có thể được tạo zero-shot, phản ánh kiến thức đã được lưu trữ trong mô hình ngôn ngữ, có thể được gọi khi mục tiêu được chỉ định đủ trong function signature:

# define function: direction = keep_pole_upright_with_pd_control(x,
x_dot, theta, theta_dot).
# info: direction is 1 if going right, 0 if going left.
def keep_pole_upright_with_pd_control(x, x_dot, theta, theta_dot):
    # define constants.
    kp = 1
    kd = 1
    # define direction.
    direction = 1
    # define error.
    error = theta
    # define error_dot.
    error_dot = theta_dot
    # define control.
    control = kp * error + kd * error_dot
    # define direction.
    if control < 0:
        direction = 0
    return direction

LMP được tạo biểu thị một bộ điều khiển nhận đầu vào là vị trí của xe đẩy, góc của cột, và vận tốc của chúng, sau đó nó cân bằng cột bằng cách tác động xe đẩy bên dưới với các hành động rời rạc trái hoặc phải. Bộ điều khiển này chỉ là một cái đơn giản yêu cầu khởi tạo gần thẳng đứng, nhưng nó có thể ổn định một điểm cố định không ổn định.

Hình 5: LMP có thể cân bằng cartpole

LMP cũng có thể được prompt để biểu thị điều khiển trở kháng:

# define function: tau = ee_impedance_control(x_curr, x_goal,
x_dot, K_x_mat, D_x_mat, J).
def ee_impedance_control(x_curr, x_goal, x_dot, K_x_mat,
                        D_x_mat, J):
    x_err = x_goal - x_curr
    x_dot_err = -x_dot
    tau = np.matmul(J.T,
                    np.matmul(K_x_mat, x_err) + np.matmul(D_x_mat, x_dot_err))
    return tau

để di chuyển end-effector cánh tay robot về phía vị trí mục tiêu với joint torque. Bộ điều khiển có chức năng ở chỗ nó có thể điều khiển robot UR5e trong PyBullet, nhưng được đơn giản hóa ở chỗ nó không bù cho lực Coriolis hoặc trọng lực. Lưu ý nhu cầu bao gồm thông tin bổ sung về hướng đầu ra mong đợi cũng như gợi ý sử dụng điều khiển PD trong function signature. Không có những gợi ý này, hàm kết quả vẫn có thể trông hợp lý (ví dụ nó có thể xuất ra giá trị liên tục cho điều khiển thay vì rời rạc), nhưng nó sẽ không hoạt động cho API môi trường cụ thể này. Đối với tên của các gain đầu vào, _mat cần thiết để LMP xử lý chúng như ma trận thay vì scalar, và _x cần thiết để chỉ ra những gain này dành cho end-effector, không phải joint. Chúng tôi chứng minh việc sử dụng bộ điều khiển này bằng cách ra lệnh vị trí 3D end-effector của robot UR5e trong PyBullet. Gain PD mặc định là 1 cũng hoạt động trong lĩnh vực này mà không cần điều chỉnh bổ sung vì môi trường CartPole tương đối đơn giản. Các tác vụ điều khiển liên tục phức tạp hơn có thể yêu cầu thực sự điều chỉnh gain dựa trên phản hồi thực thi, điều mà phương pháp của chúng tôi không hỗ trợ hiện tại.

Cả hai ví dụ đều cho thấy có thể tạo các bộ điều khiển phản ứng đơn giản, nhưng cần nhiều công việc hơn để biểu thị những cái phức tạp hơn.

G. Visual Language Models

Đối với các thí nghiệm thế giới thực, chúng tôi sử dụng các mô hình phát hiện đối tượng từ vựng mở off-the-shelf, ViLD [3] và MDETR [2] để thực hiện phát hiện đối tượng, localization, và segmentation. Những cái này được gọi là mô hình ngôn ngữ thị giác vì chúng nhận đầu vào là mô tả ngôn ngữ tự nhiên (caption) của hình ảnh và cố gắng tìm đối tượng trong mô tả đó. ViLD được sử dụng cho lĩnh vực robot di động, trong khi MDETR được sử dụng cho lĩnh vực thao tác tabletop và vẽ whiteboard. Cả hai mô hình đều đưa ra bounding box axis-aligned trong hình ảnh cùng với mask segmentation per-pixel của các đối tượng được phát hiện. Để chuyển đổi những detection này thành tọa độ 3D cho cả nhận thức và hành động (ví dụ, nguyên thủy picking scripted), chúng tôi deproject các pixel tương ứng từ camera depth, có transform đến khung robot được đăng ký trước. Độ robust của các mô hình ngôn ngữ thị giác ngày nay vẫn có thể được cải thiện, và nhiều thất bại thế giới thực có thể được quy cho detection không chính xác. Ngoài ra, một mức độ prompt engineering cũng cần thiết cho VLM. Ví dụ, MDETR phát hiện khối đáng tin cậy hơn với từ "square" hơn "block," và áp dụng phương pháp của chúng tôi cho lĩnh vực mới sẽ yêu cầu một số prompt engineering cho mô hình ngôn ngữ thị giác.

H. Whiteboard Drawing

Trong lĩnh vực này, robot UR5e được giao nhiệm vụ vẽ và xóa các hình khác nhau được mô tả bằng ngôn ngữ tự nhiên trên whiteboard. Một marker khô được gắn cứng vào end-effector robot. Kích thước whiteboard, vị trí, và vị trí của tẩy được biết. Các đối tượng bổ sung có thể được thêm vào cảnh để các lệnh tham chiếu (ví dụ, vẽ vòng tròn xung quanh khối xanh). Trong demo của chúng tôi, chúng tôi sử dụng API speech-to-text và text-to-speech của Google Cloud để cho phép người dùng tương tác với hệ thống thông qua lệnh thoại và cũng nghe phản hồi của robot cho lệnh.

Prompt.
• draw_ui: UI cấp cao để phân tích lệnh người dùng và gọi hàm khác
https://code-as-policies.github.io/prompts/draw_ui.txt
• parse_obj_name: trả về tên đối tượng từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/parse_obj_name.txt
• parse_shape_pts: trả về chuỗi điểm đường 2D của hình từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/parse_shape_pts.txt
• transform_shape_pts: thực hiện biến đổi 2D trên chuỗi điểm 2D từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/transform_shape_pts.txt
• function_generation: định nghĩa hàm từ nhận xét
https://code-as-policies.github.io/prompts/fgen_simple.txt

API.
• get_obj_names() - lấy danh sách đối tượng có sẵn trong cảnh. những cái này được chỉ định trước.
• get_obj_pos(name) - lấy vị trí 2D của tâm đối tượng theo tên.
• draw(pts_2d) - vẽ hình bằng cách ra lệnh end-effector robot theo dõi chuỗi điểm trên whiteboard. Robot trước tiên di chuyển đến điểm phía trên điểm đầu tiên trong quỹ đạo, di chuyển xuống cho đến khi phát hiện tiếp xúc với whiteboard, và tiến hành theo dõi phần còn lại của quỹ đạo.
• erase(pts_2d) - xóa hình bằng cách ra lệnh end-effector robot trước tiên thiết lập tiếp xúc với tẩy (vị trí tẩy được hardcode) trước khi theo dõi phần còn lại của quỹ đạo.

Hướng dẫn. Những hướng dẫn này được đưa ra cho robot theo chuỗi từ whiteboard trống ban đầu. Xem video đầy đủ và mã được tạo trên website.

--- TRANG 12 ---

1) vẽ lục giác 5cm xung quanh giữa
2) vẽ đường chia đôi lục giác
3) làm cho cả hai lớn hơn
4) xóa lục giác và đường
5) vẽ mặt trời như vòng tròn ở góc trên phải
6) vẽ mặt đất như đường ở dưới cùng
7) vẽ kim tự tháp như tam giác trên mặt đất
8) vẽ kim tự tháp nhỏ hơn một chút bên trái
9) vẽ vòng tròn xung quanh các khối
10) vẽ hình vuông xung quanh loại trái cây ngọt hơn

I. Thao tác Tabletop Thế giới Thực

Trong lĩnh vực này, robot UR5e được giao nhiệm vụ thao tác đối tượng trên tabletop theo hướng dẫn ngôn ngữ tự nhiên. Robot được trang bị gripper hút, và nó chỉ có thể thực hiện hành động pick và place được tham số hóa bởi vị trí pick và place 2D top-down. Robot cũng được mong đợi trả lời câu hỏi về cảnh (ví dụ, có bao nhiêu khối?) bằng cách sử dụng API nhận thức được cung cấp. Trong demo của chúng tôi, chúng tôi sử dụng API speech-to-text và text-to-speech của Google Cloud để cho phép người dùng tương tác với hệ thống thông qua lệnh thoại và cũng nghe phản hồi và câu trả lời của robot cho lệnh và câu hỏi. Hiện tại, prompt chỉ hỗ trợ có một tập hợp đối tượng duy nhất. Đây không phải là hạn chế của CaP mà của hệ thống nhận thức - chúng tôi không có cách tốt để duy trì danh tính của đối tượng trùng lặp trên các detection VLM. Một hệ thống tinh vi hơn về theo dõi thế giới được cảm nhận có thể giải quyết vấn đề này.

Prompt.
• tabletop_ui: UI cấp cao để phân tích lệnh người dùng và gọi hàm khác
https://code-as-policies.github.io/prompts/tabletop_ui.txt
• parse_obj_name: trả về tên đối tượng từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/parse_obj_name.txt
• parse_position: trả về vị trí 2D từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/parse_position.txt
• parse_question: trả về phản hồi (có thể là số, boolean, hoặc chuỗi) cho câu hỏi ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/parse_question.txt
• function_generation: định nghĩa hàm từ nhận xét
https://code-as-policies.github.io/prompts/fgen.txt

API.
• get_obj_names() - lấy danh sách đối tượng có sẵn trong cảnh. những cái này được chỉ định trước.
• get_obj_pos(name) - lấy vị trí 2D của tâm đối tượng theo tên.
• is_obj_visible(name) - kiểm tra nếu đối tượng hiển thị theo tên.
• get_bbox(name) - lấy bounding box axis-aligned 2D của đối tượng theo tên. Điều này trong tọa độ base robot, không phải pixel.
• get_segmask(name) - lấy mask segmentation của detection đối tượng theo tên. Điều này trong pixel.
• get_color_rgb(name) - lấy màu RGB trung bình của crop detection đối tượng theo tên.
• get_corner_name(pos_2d) - lấy tên góc (ví dụ, góc trên phải) gần nhất với điểm 2d.
• get_side_name(pos_2d) - lấy tên cạnh (ví dụ, cạnh trái) gần nhất với điểm 2d.
• denormalize_xy(normalized_pos_2d) - chuyển đổi tọa độ 2D được chuẩn hóa (mỗi giá trị giữa 0 và 1) thành tọa độ 2D thực tế trong khung robot.
• put_first_on_second(obj_name, target) - pick đối tượng đầu tiên theo tên và đặt nó lên mục tiêu theo tên. Mục tiêu có thể là tên đối tượng khác hoặc vị trí 2D. Picking và placing được thực hiện bằng cách di chuyển gripper hút trực tiếp lên vị trí mong muốn, di chuyển xuống cho đến khi phát hiện tiếp xúc, sau đó tham gia hoặc tách suction cup.
• say(message) - sử dụng loa robot để phát ra tin nhắn.

Chúng tôi chứng minh CaP trên ba lĩnh vực trong setting thao tác tabletop. Hướng dẫn của mỗi lĩnh vực được liệt kê bên dưới và được thực hiện theo chuỗi. Xem video đầy đủ và mã được tạo trên website.

Hướng dẫn cho lĩnh vực 4 khối.
1) Đặt các khối trong đường ngang gần đỉnh
2) Di chuyển khối màu trời vào giữa khối đỏ và khối thứ hai từ trái
3) Tại sao bạn di chuyển khối xanh?
4) Khối nào bạn đã di chuyển?
5) Sắp xếp các khối trong hình vuông xung quanh giữa
6) Làm cho hình vuông lớn hơn
7) Undo cái đó
8) xoay hình vuông 45 độ
9) Bạn có thể ném khối không?
10) Di chuyển khối đỏ 5cm xuống dưới
11) Làm tương tự với các khối khác
12) Đặt các khối trên các góc khác nhau theo chiều kim đồng hồ bắt đầu ở góc trên phải

Hướng dẫn cho lĩnh vực 3 khối và 3 bát.
1) Đặt khối đỏ bên trái bát phải nhất
2) Bây giờ di chuyển nó đến cạnh xa nhất khỏi nó
3) Có bao nhiêu bát bên trái khối đỏ?
4) đặt các khối vào bát với màu không khớp
5) đặt các khối trong đường dọc dài 20 cm và 10 cm dưới bát xanh
6) tưởng tượng rằng các bát đại diện cho núi lửa, rừng, và đại dương
7) cũng tưởng tượng rằng các khối là phần của tòa nhà
8) bây giờ xây tháp trong rừng
9) cho tôi xem điều gì xảy ra khi núi lửa phun trào trên đại dương

Hướng dẫn cho lĩnh vực trái cây, chai, và đĩa.
1) Có bao nhiêu trái cây?
2) Nói tên chúng cho tôi
3) Có trái cây nào trên đĩa xanh không?
4) Di chuyển tất cả trái cây đến đĩa xanh và chai đến đĩa xanh dương
5) Di chuyển trái cây nhỏ nhất trở lại đĩa vàng
6) Đợi cho đến khi bạn thấy trứng và đặt nó trên đĩa xanh
7) Đặt vật thể tối nhất vào đĩa có táo

--- TRANG 13 ---

J. Mobile Robot

Thí nghiệm thao tác di động được thiết lập với robot từ Everyday Robots navigate và tương tác với đối tượng trong nhà bếp văn phòng thế giới thực. Robot có base di động và cánh tay 7DoF. Để triển khai API nhận thức, chúng tôi chủ yếu sử dụng cảm biến camera RGBD trên robot. Robot được hiển thị trong Hình 6.

Góc nhìn phía trước,
Tư thế pre-manipulation Hình ảnh RGBD, 640 x 512

Hình 6: Thiết lập Thí nghiệm cho thao tác di động với robot Everyday Robots.

Prompt.
• mobile_ui: UI cấp cao để phân tích lệnh người dùng và gọi hàm khác
https://code-as-policies.github.io/prompts/mobile_ui.txt
• parse_obj_name: trả về tên đối tượng từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/mobile_parse_obj_name.txt
• parse_position: trả về vị trí 2D từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/mobile_parse_pos.txt
• transform_traj: thực hiện biến đổi 2D trên chuỗi điểm 2D từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/mobile_transform_traj.txt
• function_generation: định nghĩa hàm từ nhận xét
https://code-as-policies.github.io/prompts/fgen_simple.txt

API.
• get_obj_names() - lấy danh sách đối tượng có sẵn trong cảnh. những cái này được chỉ định trước.
• get_obj_pos(name) - lấy vị trí 2D của tâm đối tượng theo tên.
• is_obj_visible(name) - trả về liệu robot có thấy đối tượng theo tên hay không.
• get_visible_obj_names() - trả về danh sách tên đối tượng hiện đang hiển thị.
• get_loc_names() - trả về danh sách tất cả tên vị trí được định nghĩa trước mà robot có thể navigate đến.
• get_obj_pos(name) - lấy vị trí 3D của đối tượng theo tên. Đối tượng này phải hiện đang hiển thị.
• get_loc_pos(name) - lấy vị trí 2D và góc 1D của vị trí được định nghĩa trước.
• get_robot_pos_and_angle - lấy vị trí 3D robot hiện tại và góc 1D (heading).
• goto_pos(pos_3d) - navigate đến vị trí 3D bằng cách chạy motion planner nội bộ của robot.
• goto_loc(name) - navigate đến vị trí theo tên bằng cách chạy motion planner nội bộ của robot.
• pick_obj(name) - pick đối tượng theo tên. Đối tượng phải hiện đang hiển thị. Điều này được triển khai như nguyên thủy picking scripted sử dụng detection đối tượng ViLD.
• place_at_pos(pos_3d) - đặt đối tượng đang giữ tại vị trí.
• place_at_obj(name) - đặt đối tượng đang giữ lên đối tượng khác theo tên.
• say(message) - sử dụng loa robot để phát ra tin nhắn.

Bên dưới chúng tôi liệt kê các lệnh được thực hiện trên nền tảng robot di động. Đầu tiên là các tác vụ liên quan đến navigation, trong khi thứ hai là liên quan đến manipulation. Đối với lệnh manipulation sau, lưu ý khả năng của CaP hình thành "bộ nhớ ngắn hạn" bằng cách ghi lại rõ ràng các biến (trong trường hợp này, vị trí quá khứ của robot) trong phạm vi thực thi Python và tham chiếu lại chúng sau này. Xem video và mã được tạo trên website.

Hướng dẫn Navigation Di động.
1) Di chuyển trong hình chữ nhật 3m x 2m xung quanh ghế văn phòng
2) Làm lại nhưng xoay 45 độ theo chiều kim đồng hồ
3) Đi trong hình vuông 1.5m xung quanh barstool nhiều lần theo cần thiết, kiểm tra mỗi bước nếu có chuối, chỉ ngừng di chuyển khi bạn thấy chuối
4) Theo convex hull chứa các ghế
5) Di chuyển qua lại giữa bàn và countertop 3 lần

Hướng dẫn Manipulation Di động.
1) Có bao nhiêu snack trên bàn?
2) Lấy chai nước từ bàn và đặt nó ở giữa trái cây trên bàn
3) Đây là thùng compost
4) Đây là thùng recycle
5) Đây là thùng landfill
6) Lon coke và táo ở trên bàn
7) Cất lon coke và táo vào thùng tương ứng của chúng

K. Đánh giá Thao tác Tabletop Mô phỏng

Tương tự như lĩnh vực tabletop thế giới thực, chúng tôi xây dựng môi trường tabletop mô phỏng, trong đó robot UR5e được trang bị gripper jaw Robotiq 2F85 được giao nhiệm vụ hoàn thành các tác vụ sắp xếp lại theo hướng dẫn ngôn ngữ tự nhiên. Các đối tượng bao gồm 10 khối màu khác nhau và 10 bát màu khác nhau. CaP được đề xuất được cung cấp API để truy cập danh sách đối tượng hiện diện và vị trí của chúng, thông qua detector đối tượng scripted, cũng như nguyên thủy chuyển động pick-and-place được tham số hóa bởi tọa độ hoặc tên đối tượng.

Prompt.
• tabletop_ui: UI cấp cao để phân tích lệnh người dùng và gọi hàm khác
https://code-as-policies.github.io/prompts/sim_tabletop_ui.txt

--- TRANG 14 ---

• parse_obj_name: trả về tên đối tượng từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/sim_parse_obj_name.txt
• parse_position: trả về vị trí 2D từ mô tả ngôn ngữ tự nhiên
https://code-as-policies.github.io/prompts/sim_parse_position.txt
• function_generation: định nghĩa hàm từ nhận xét
https://code-as-policies.github.io/prompts/fgen.txt

API.
• get_obj_names() - lấy danh sách đối tượng có sẵn trong cảnh. những cái này được chỉ định trước.
• get_obj_pos(name) - lấy vị trí 2D của tâm đối tượng theo tên.
• denormalize_xy(normalized_pos_2d) - chuyển đổi tọa độ 2D được chuẩn hóa (mỗi giá trị giữa 0 và 1) thành tọa độ 2D thực tế trong khung robot.
• put_first_on_second(obj_name, target) - pick đối tượng đầu tiên theo tên và đặt nó lên mục tiêu theo tên. Mục tiêu có thể là tên đối tượng khác hoặc vị trí 2D. Picking và placing được thực hiện bằng cách di chuyển gripper hút trực tiếp lên vị trí mong muốn, di chuyển xuống cho đến khi phát hiện tiếp xúc, sau đó tham gia hoặc tách suction cup.

Chúng tôi đánh giá CaP và baseline trên các tác vụ sau, nơi mỗi tác vụ tham chiếu đến template hướng dẫn duy nhất (ví dụ, "Pick up the <block> and place it in the corner <distance> to the <bowl>") được tham số hóa bởi một số thuộc tính (ví dụ, <block>). Chúng tôi chia các tác vụ thành hướng dẫn và thuộc tính thành danh mục "seen" và "unseen", nơi hướng dẫn hoặc thuộc tính "seen" được phép xuất hiện trong prompt hoặc được sử dụng để huấn luyện (trong trường hợp baseline có giám sát). Danh sách đầy đủ có thể được tìm thấy bên dưới. Lưu ý rằng chúng tôi nhóm thêm các hướng dẫn thành họ tác vụ "Long-Horizon" và "Spatial-Geometric". Hướng dẫn "Long-Horizon" là 1-5 trong Seen Instructions và 1-3 trong Unseen Instructions. Hướng dẫn "Spatial-Geometric" là 5-8 trong Seen Instructions và 4-6 trong Unseen Instructions.

Seen Instructions.
1) Pick up the <block1> and place it on the (<block2> or <bowl>)
2) Stack all the blocks
3) Put all the blocks on the <corner/side>
4) Put the blocks in the <bowl>
5) Put all the blocks in the bowls with matching colors
6) Pick up the block to the <direction> of the <bowl> and place it on the <corner/side>
7) Pick up the block <distance> to the <bowl> and place it on the <corner/side>
8) Pick up the <nth> block from the <direction> and place it on the <corner/side>

Unseen Instructions.
1) Put all the blocks in different corners
2) Put the blocks in the bowls with mismatched colors
3) Stack all the blocks on the <corner/side>
4) Pick up the <block1> and place it <magnitude> to the <direction> of the <bowl>
5) Pick up the <block1> and place it in the corner <distance> to the <bowl>
6) Put all the blocks in a <line> line

Seen Attributes.
1) <block>: blue block, red block, green block, orange block, yellow block
2) <bowl>: blue bowl, red bowl, green bowl, orange bowl, yellow bowl
3) <corner/side>: left side, top left corner, top side, top right corner
4) <direction>: top, left
5) <distance>: closest
6) <magnitude>: a little
7) <nth>: first, second
8) <line>: horizontal, vertical

Unseen Attributes.
1) <block>: pink block, cyan block, brown block, gray block, purple block
2) <bowl>: pink bowl, cyan bowl, brown bowl, gray bowl, purple bowl
3) <corner/side>: bottom right corner, bottom side, bottom left corner
4) <direction>: bottom, right
5) <distance>: farthest
6) <magnitude>: a lot
7) <nth>: third, fourth
8) <line>: diagonal

Trong Bảng VI chúng tôi cung cấp kết quả mô phỏng chi tiết báo cáo tỷ lệ thành công tác vụ cho các danh mục tác vụ fine-grained. Attributes tham chiếu đến trường <>, có giá trị có thể được thấy bởi phương pháp (ví dụ, tập training cho CLIPort, prompt cho phương pháp dựa trên ngôn ngữ). Instructions tham chiếu đến loại hướng dẫn templated được đưa ra trong mỗi hàng, cũng có thể được thấy hoặc không thấy. Tổng cộng 50 thử nghiệm được thực hiện mỗi tác vụ, mỗi cái với thuộc tính được lấy mẫu và cấu hình cảnh ban đầu (loại, số lượng, và vị trí khối và bát). Lưu ý rằng CLIPort bản thân nó (không có oracle) chỉ là chính sách phản hồi và nó không biết khi nào ngừng — trong trường hợp này chúng tôi chạy 10 hành động từ chính sách CLIPort và đánh giá thành công ở cuối. Để cải thiện hiệu suất CLIPort, chúng tôi sử dụng biến thể sử dụng thông tin oracle từ mô phỏng để ngừng chính sách khi phát hiện thành công (oracle termination).

L. Khả năng LLM Bổ sung

Sử dụng LLM lớn được pre-train cũng có nghĩa là chúng ta có thể tận dụng khả năng của nó ngoài viết mã. Ví dụ, Code as Policies có thể phân tích lệnh từ ngôn ngữ không phải tiếng Anh cũng như emoji. Xem Hình 7.

M. Ví dụ Cross Embodiment

CaP thể hiện mức độ hỗ trợ cross-embodiment [59], [60] bằng cách thực hiện cùng một tác vụ khác nhau tùy thuộc vào API hành động. Trong ví dụ bên dưới, chúng tôi đưa ra Hints của API hành động, và kế hoạch kết quả thay đổi tùy thuộc vào liệu robot có omnidirectional hay unidirectional. Chúng tôi lưu ý rằng khả năng này dễ vỡ với LLM hiện tại và không thể thích ứng đáng tin cậy với API rất khác nhau. Độ robust hơn có thể yêu cầu những cái lớn hơn được huấn luyện trên mã domain-specific.

--- TRANG 15 ---

BẢNG VI: Tỷ lệ thành công thao tác tabletop mô phỏng chi tiết (%) trên các kịch bản tác vụ khác nhau.

CLIPort (oracle termination) CLIPort (no oracle) NL Planner CaP (của chúng tôi)

Seen Attributes, Seen Instructions
Pick up the <object1> and place it on the (<object2> or <recepticle-bowl>) 88 44 98 100
Stack all the blocks 98 4 94 94
Put all the blocks on the <corner/side> 96 8 46 92
Put the blocks in the <recepticle-bowl> 100 22 94 100
Put all the blocks in the bowls with matching colors 12 14 100 100
Pick up the block to the <direction> of the <recepticle-bowl> and place it on the <corner/side> 100 80 N/A 72
Pick up the block <distance> to the <recepticle-bowl> and place it on the <corner/side> 92 54 N/A 98
Pick up the <nth> block from the <direction> and place it on the <corner/side> 100 38 N/A 98
Total 85.8 33.0 86.4 94.3
Long-Horizon Total 78.8 18.4 86.4 97.2
Spatial-Geometric Total 97.3 57.3 N/A 89.3

Unseen Attributes, Seen Instructions
Pick up the <object1> and place it on the (<object2> or <recepticle-bowl>) 12 10 98 100
Stack all the blocks 96 8 96 100
Put all the blocks on the <corner/side> 0 0 58 100
Put the blocks in the <recepticle-bowl> 46 0 88 96
Put all the blocks in the bowls with matching colors 30 26 100 92
Pick up the block to the <direction> of the <recepticle-bowl> and place it on the <corner/side> 0 0 N/A 60
Pick up the block <distance> to the <recepticle-bowl> and place it on the <corner/side> 0 0 N/A 100
Pick up the <nth> block from the <direction> and place it on the <corner/side> 0 0 N/A 60
Total 23.0 5.5 88.0 88.5
Long-Horizon Total 36.8 8.8 88.0 97.6
Spatial-Geometric total 0.0 0.0 N/A 73.3

Unseen Attributes, Unseen Instructions
Put all the blocks in different corners 0 0 60 98
Put the blocks in the bowls with mismatched colors 0 0 92 60
Stack all the blocks on the <corner/side> 0 0 40 82
Pick up the <object1> and place it <magnitude> to the <direction> of the <recepticle-bowl> 0 0 N/A 38
Pick up the <object1> and place it in the corner <distance> to the <recepticle-bowl> 4 0 N/A 58
Put all the blocks in a <line> 0 0 N/A 90
Total 0.7 0.0 64.0 71.0
Long-Horizon Total 0.0 0.0 64.0 80.0
Spatial-Geometric Total 1.3 0.0 N/A 62.0

Hình 7: LMP kế thừa lợi ích của LLM, như phân tích lệnh từ ngôn ngữ không phải tiếng Anh và emoji.

# omnidirectional robot.
# available actions: move_up(dist), move_right(dist), move_back(dist).
# make a U shape 5 meters wide and 10 meters long.
robot.move_back(dist=10)
robot.move_right(dist=5)
robot.move_up(dist=10)

# unidirectional robot.
# available actions: turn_left(deg), move_forward(dist).
# make a U shape 3 meters wide and 8 meters long.
robot.move_forward(dist=8)
robot.turn_left(deg=90)
robot.move_forward(dist=3)
robot.turn_left(deg=90)
robot.move_forward(dist=8)
