# 2306.00029.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2306.00029.pdf
# Kích thước file: 624380 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
CODETF: THƯ VIỆN TRANSFORMER MỘT ĐIỂM DỪNG CHO
CÁC CODE LLM TIÊN TIẾN
Nghi D. Q. Bui∗, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, Steven C.H. Hoi∗
Salesforce AI Research
https://github.com/salesforce/CodeTF
TÓM TẮT
Trí tuệ mã nguồn đóng vai trò quan trọng trong việc biến đổi kỹ thuật phần mềm hiện đại. Gần đây, các mô hình dựa trên học sâu, đặc biệt là các mô hình ngôn ngữ lớn (LLM) dựa trên Transformer, đã thể hiện tiềm năng đáng kể trong việc giải quyết những nhiệm vụ này bằng cách tận dụng dữ liệu mã nguồn mở khổng lồ và các tính năng ngôn ngữ lập trình. Tuy nhiên, việc phát triển và triển khai các mô hình như vậy thường đòi hỏi chuyên môn về cả học máy và kỹ thuật phần mềm, tạo ra rào cản cho việc áp dụng mô hình. Trong bài báo này, chúng tôi trình bày CodeTF, một thư viện Transformer mã nguồn mở cho các Code LLM tiên tiến và trí tuệ mã nguồn. Theo các nguyên tắc thiết kế mô-đun và khung mở rộng, chúng tôi thiết kế CodeTF với giao diện thống nhất để cho phép truy cập nhanh chóng và phát triển trên các loại mô hình, tập dữ liệu và nhiệm vụ khác nhau. Thư viện của chúng tôi hỗ trợ một bộ sưu tập các mô hình Code LLM được huấn luyện trước và các điểm chuẩn mã nguồn phổ biến, bao gồm giao diện chuẩn hóa để huấn luyện và phục vụ các Code LLM một cách hiệu quả, và các tính năng dữ liệu như bộ phân tích cú pháp đặc thù ngôn ngữ và các hàm tiện ích để trích xuất thuộc tính mã nguồn. Trong bài báo này, chúng tôi mô tả các nguyên tắc thiết kế, kiến trúc, các mô-đun và thành phần chính, và so sánh với các công cụ thư viện liên quan khác. Cuối cùng, chúng tôi hy vọng CodeTF có thể thu hẹp khoảng cách giữa học máy/AI sinh và kỹ thuật phần mềm, cung cấp một giải pháp mã nguồn mở toàn diện cho các nhà phát triển, nhà nghiên cứu và người thực hành.
Từ khóa Transformer ·mô hình ngôn ngữ lớn mã nguồn ·hiểu biết mã nguồn ·sinh mã nguồn ·trí tuệ mã nguồn

1 Giới thiệu
AI đã tạo ra những thay đổi mang tính biến đổi đối với các ngành kỹ thuật phần mềm trong những năm gần đây. Các phương pháp dựa trên học máy truyền thống cho các nhiệm vụ trí tuệ mã nguồn trong kỹ thuật phần mềm bao gồm các nhiệm vụ phân tích mã nguồn cơ bản. Những nhiệm vụ này bao gồm hiểu, phân tích và sửa đổi mã nguồn để cải thiện chất lượng và khả năng bảo trì của nó [1,2,3]. Trong những năm gần đây, các mô hình học sâu, đặc biệt là các mô hình ngôn ngữ lớn (LLM) dựa trên Transformer được huấn luyện trước trên dữ liệu mã nguồn quy mô lớn ("Code LLM") [4,5,6,7,8,9,10], đã cho thấy kết quả đầy hứa hẹn trong các nhiệm vụ trí tuệ mã nguồn đầy thử thách hơn, như sinh mã nguồn, hoàn thiện mã nguồn, tóm tắt mã nguồn và truy xuất mã nguồn [11,12,13,14]. Những mô hình này tận dụng một lượng lớn dữ liệu mã nguồn mở từ các nền tảng trực tuyến như Github [5,6,10], đôi khi được bổ sung với các tính năng ngôn ngữ lập trình [4,9,7], để học các biểu diễn ngữ cảnh có ý nghĩa trong mã nguồn. Thành công ban đầu trong việc áp dụng những mô hình này trong thực tế đã chứng minh những lợi ích tiềm năng to lớn cho xã hội và cụ thể hơn, cho các chuyên gia phát triển phần mềm để cải thiện năng suất và chất lượng công việc của họ.

Khi các LLM đã chứng minh giá trị to lớn đối với các nhà phát triển phần mềm, việc phát triển và triển khai những mô hình như vậy từ đầu vẫn là một nhiệm vụ khó khăn và tốn thời gian đối với phần lớn các nhà phát triển. Việc phát triển những mô hình như vậy thường đòi hỏi kinh nghiệm đáng kể về thiết kế và huấn luyện mô hình [15,16,17], thường được cung cấp bởi các chuyên gia học máy. Để triển khai những mô hình này, các nhà phát triển phần mềm chuyên nghiệp sau đó cần thiết để mở rộng và phục vụ các mô hình một cách hiệu quả trong các hệ thống phần mềm. Một trở ngại chính trong lĩnh vực này là tập hợp các giao diện không nhất quán giữa các mô hình, tập dữ liệu và nhiệm vụ ứng dụng, dẫn đến các nỗ lực lặp đi lặp lại cao trong việc phát triển và triển khai Code LLM.

Để giải quyết những thách thức này, chúng tôi xây dựng CodeTF, một thư viện toàn diện mã nguồn mở cho các LLM dựa trên Transformer và ứng dụng của chúng trong các nhiệm vụ trí tuệ mã nguồn. Hình 1 cho thấy tổng quan về CodeTF. Trong CodeTF, chúng tôi đã tạo ra một giao diện thống nhất

∗Liên hệ: {nghi.bui, shoi}@salesforce.com

--- TRANG 2 ---
Huấn luyện mô hình
Hỗ trợ tinh chỉnh mô hình hiệu quả tham số (Lora, Prefix-tuning, Prompt-tuning)

Phục vụ mô hình
Hỗ trợ lượng tử hóa mô hình cho suy luận mô hình hiệu quả: int8, float16, GPTQ 4-bit

Tiện ích mã nguồn
Các tiện ích mã nguồn tích hợp như bộ phân tích cú pháp AST đa ngôn ngữ cho hơn 15 ngôn ngữ lập trình

Tiện ích dữ liệu
Các tập dữ liệu mở rộng và hàm tiện ích tải cho các nhiệm vụ ứng dụng hạ nguồn khác nhau

Vườn mô hình
Hỗ trợ các Transformer LLM cho mã nguồn bao gồm CodeT5, CodeGen và CodeT5+

API tích hợp để dễ dàng áp dụng các mô hình và nhiệm vụ ứng dụng được hỗ trợ

Đánh giá mô hình
Hỗ trợ đánh giá mô hình trên các điểm chuẩn mã nguồn phổ biến như HumanEval, MBPP

Tải và xử lý dữ liệu nhanh chóng và dễ dàng để tái tạo kết quả hiệu suất mô hình

Hình 1: Tổng quan về CodeTF: Chúng tôi phát triển một thư viện toàn diện dựa trên Transformer để hỗ trợ phát triển và triển khai LLM cho trí tuệ mã nguồn. Thư viện chứa các tính năng để huấn luyện và phục vụ các mô hình ngôn ngữ, tiện ích mã nguồn để xử lý và thao tác dữ liệu mã nguồn, và các điểm chuẩn nghiên cứu phổ biến để đánh giá hiệu suất mô hình.

giao diện để cho phép truy cập dễ dàng và tùy chỉnh các thành phần riêng lẻ. Các thành phần chính như huấn luyện mô hình, suy luận và tập dữ liệu được xây dựng trên một mô-đun nền tảng được thiết kế đặc biệt cho dữ liệu và mô hình dựa trên mã nguồn. Nguyên tắc thiết kế này cho phép tích hợp chuẩn hóa và phát triển nhanh chóng từ bất kỳ mô hình và tập dữ liệu có sẵn nào.

Trong giao diện thống nhất của CodeTF này, thư viện của chúng tôi hỗ trợ một bộ sưu tập đa dạng các LLM dựa trên Transformer được huấn luyện trước [5,4,18] và các nhiệm vụ mã nguồn [12,13,11]. CodeTF hỗ trợ một phạm vi rộng các LLM của mã nguồn, bao gồm chỉ bộ mã hóa (CodeBERT [6], CodeBERTA), chỉ bộ giải mã (CodeParrot [19], Incoder [20], CodeGen [5], SantaCoder [21], StarCoder [22]), bộ mã hóa-giải mã (CodeT5 [4], CodeT5+ [18] CodeTF bao gồm một bộ sưu tập các tập dữ liệu phổ biến như HumanEval và APPS [12,13,11,23] và một giao diện để tải và phục vụ hiệu quả các mô hình được huấn luyện trước, mô hình tùy chỉnh và tập dữ liệu. Thông qua giao diện thống nhất, người dùng thư viện có thể không chỉ tái tạo và triển khai các mô hình tiên tiến một cách hiệu quả, mà còn tích hợp liền mạch các mô hình và điểm chuẩn mới khi cần thiết.

So với các lĩnh vực khác như thị giác và văn bản, dữ liệu mã nguồn thường đòi hỏi các quy trình tiền xử lý và thao tác nghiêm ngặt hơn do các quy tắc cú pháp khắt khe phải tuân thủ theo ngôn ngữ lập trình tương ứng của chúng. Do đó, CodeTF giới thiệu một bộ tính năng xử lý dữ liệu nâng cao bao gồm các bộ phân tích cú pháp Cây Cú pháp Trừu tượng (AST) cho nhiều ngôn ngữ lập trình tận dụng tree-sitter2, cùng với các tiện ích để trích xuất thuộc tính mã nguồn như tên phương thức, định danh, tên biến và bình luận mã nguồn. Những công cụ này đã được thiết kế tỉ mỉ để tạo điều kiện cho việc xử lý và thao tác hiệu quả dữ liệu mã nguồn trong quá trình huấn luyện, tinh chỉnh và đánh giá mô hình. Những tính năng như vậy là không thể thiếu để hỗ trợ việc tiền xử lý mã nguồn thành định dạng phù hợp cho các mô hình ngôn ngữ. Ví dụ, CodeT5 [4] đòi hỏi việc trích xuất tên hàm và xác định vị trí định danh cho phương pháp học đa mục tiêu của nó

Tóm lại, những đóng góp chính của chúng tôi với CodeTF là như sau:
•Một khung mô-đun và mở rộng cho các nhiệm vụ trí tuệ mã nguồn, cho phép người dùng dễ dàng tích hợp một phạm vi rộng các ngôn ngữ lập trình, mô hình và dữ liệu, khi cần thiết
•Một giao diện cho cả việc phục vụ và huấn luyện các mô hình được huấn luyện trước và mô hình tùy chỉnh, cho phép người dùng tận dụng các mô hình tiên tiến và tinh chỉnh chúng cho các trường hợp sử dụng cụ thể
•Một bộ sưu tập các kho mã nguồn phổ biến với các mô-đun tiền xử lý dữ liệu và trích xuất tính năng, hỗ trợ một phạm vi rộng các ngôn ngữ lập trình và nhiệm vụ mã nguồn và thúc đẩy khả năng tái sử dụng dữ liệu.
•Tài liệu chi tiết và ví dụ mã nguồn, tạo điều kiện cho quá trình học tập và áp dụng cho người dùng với các mức độ chuyên môn khác nhau.

Cuối cùng, chúng tôi hy vọng CodeTF sẽ trở thành một công cụ hữu ích cho cả nhà phát triển phần mềm và nhà nghiên cứu, thúc đẩy nhiều đổi mới hơn trong nghiên cứu trí tuệ mã nguồn và tạo điều kiện cho việc triển khai và ứng dụng rộng rãi hơn của Code LLM.

2https://github.com/tree-sitter/tree-sitter

--- TRANG 3 ---
Định dạng mã nguồn
Linter, Clang, …

Bộ phân tích cú pháp AST
Tree-sitter, PMD, ANTLR, …

Xử lý Token
NLTK, Spacy, …
…….

Huấn luyện phân tán
Quản lý thiết bị, song song dữ liệu, Deepspeed, …

Vòng lặp huấn luyện
Pytorch, Tensorflow, Keras,...

Cấu hình siêu tham số
Kích thước batch, optimizer, tốc độ học, …
…..

Lượng tử hóa
Bitsandbyes, GPTQ, Optimum, CTranslate2, …

Phục vụ phân tán
TorchServer, Triton, Faster Transformer, …

Triển khai mô hình
Flask, Fast API, KubeFlow, MLFlow
…..

Hậu xử lý
Spacy, Regex, ….

Triển khai metric
Pass@K, CodeBLEU
…..

Chuẩn bị dữ liệu

Huấn luyện

Phục vụ

Đánh giá

Người thực hành/Nhà nghiên cứu

Hình 2: Minh họa cách các nhà thực hành sử dụng Code LLM cho các vấn đề kỹ thuật phần mềm.

2 Thiết kế thư viện

Hình 3 cung cấp một tổng quan chi tiết về việc triển khai hệ thống CodeTF, làm nổi bật các thành phần thiết yếu của nó giúp người dùng có thể dễ dàng tham gia vào các nhiệm vụ liên quan đến mã nguồn khác nhau. Hệ thống của chúng tôi tuân theo kiến trúc mô-đun, tăng cường khả năng mở rộng bằng cách cho phép tích hợp liền mạch các ngôn ngữ lập trình, mô hình và tiện ích bổ sung được thiết kế riêng cho các yêu cầu cụ thể.

2.1 Động lực

Để minh họa động lực đằng sau thiết kế của CodeTF, chúng tôi trình bày các trường hợp sử dụng của các nhà thực hành và nhà nghiên cứu áp dụng Code LLM cho mục đích thực tế và nghiên cứu (xem Hình 2). Những trường hợp sử dụng này bao gồm bốn nhiệm vụ chính: Chuẩn bị dữ liệu, Huấn luyện, Phục vụ và Đánh giá.

Chuẩn bị dữ liệu: Trong nhiệm vụ đầu tiên, người dùng sử dụng Code LLM để hoàn thiện mã nguồn, dịch mã nguồn sang ngôn ngữ khác, dự đoán lỗi hoặc tinh chỉnh mã nguồn. Những nhiệm vụ này dựa vào việc đưa ra dự đoán dựa trên các đoạn mã nguồn đầu vào. Tuy nhiên, Code LLM thường được huấn luyện trước cho các nhiệm vụ dự đoán token tiếp theo, đòi hỏi tinh chỉnh trên các tập dữ liệu cụ thể cho các nhiệm vụ mong muốn. Điều này đòi hỏi các bước phức tạp để chuẩn bị dữ liệu mã nguồn, chẳng hạn như định dạng mã nguồn (ví dụ: sử dụng clang-formatter, ESLint), phân tích cú pháp mã nguồn thành Cây Cú pháp Trừu tượng (AST) (ví dụ: với tree-sitter, PMD), và xử lý các token mã nguồn (ví dụ: sử dụng NLTK, Spacy). Tiền xử lý mã nguồn để trích xuất thông tin quan trọng, thay vì sử dụng mã nguồn thô, thường dẫn đến kết quả tốt hơn, đặc biệt đối với các nhiệm vụ như dự đoán lỗi hoặc sửa chữa chương trình.

Huấn luyện/Tinh chỉnh: Một khi dữ liệu được chuẩn bị, người dùng có thể tiến hành huấn luyện hoặc tinh chỉnh Code LLM. Điều này bao gồm các nhiệm vụ bổ sung như viết vòng lặp huấn luyện (sử dụng các framework như PyTorch, Keras hoặc TensorFlow) trên các tập dữ liệu đã chuẩn bị. Hơn nữa, người dùng có thể cần xử lý quản lý thiết bị (GPU, TPU, CPU) trong một môi trường huấn luyện phân tán, đặc biệt khi tinh chỉnh các mô hình lớn. Huấn luyện Code LLM là một bước quan trọng để thích ứng các mô hình với các nhiệm vụ cụ thể và cải thiện hiệu suất của chúng trên lĩnh vực mục tiêu. Tinh chỉnh cho phép người dùng tận dụng các tập dữ liệu riêng của họ và tinh chỉnh các tham số của mô hình, giúp họ đạt được kết quả tốt hơn cho các trường hợp sử dụng cụ thể của mình.

Phục vụ: Một khi mô hình được huấn luyện, người dùng có thể muốn phục vụ mô hình để suy luận. Điều này đòi hỏi các nỗ lực như lượng tử hóa các mô hình thành phiên bản 8-bit hoặc 4-bit để tăng tốc độ suy luận, làm cho chúng hiệu quả hơn trong việc sử dụng tài nguyên. Lượng tử hóa giảm kích thước mô hình và cải thiện thời gian suy luận mà không hy sinh đáng kể độ chính xác.

--- TRANG 4 ---
Tập dữ liệu
Human Input CodeXGLUE

Các tính năng mã nguồn được trích xuất

Bộ tải dữ liệu

Bộ xử lý dữ liệu

Bộ làm sạch dữ liệu

Loại bỏ bình luận

Bộ phân tích cú pháp AST

Bộ trích xuất thuộc tính mã nguồn
…<>

HumanEval
…

BitsandByte GPTQ …

Huấn luyện mô hình đầy đủ

Prefix Tuning LORA

AdaLORA P-tuning …

pass@k CodeBLEU …

CodeT5-Summarization CodeT5-Text2Code CodeT5-Completion

CodeGen-Mono CodeGen-Multi …

Dữ liệu mã nguồn codetf.data_utility

codetf.model codetf.trainer

codetf.code_utility codetf.performance codetf.predict

Hình 3: Tổng quan về thiết kế hệ thống của CodeTF. Thiết kế mô-đun cải thiện khả năng mở rộng thư viện và cho phép người dùng dễ dàng tùy chỉnh và tích hợp các mô hình, dữ liệu và ngôn ngữ lập trình bổ sung khi cần thiết. Các thành phần chính trong CodeTF bao gồm: vườn mô hình - codetf.model, phục vụ mô hình - codetf.predict, huấn luyện mô hình - codetf.trainer, tiện ích dữ liệu - codetf.data_utility, tiện ích mã nguồn codetf.data_utility, và đánh giá codetf.performance.

Ngoài ra, việc triển khai các mô hình trong các môi trường cụ thể đòi hỏi thiết lập môi trường triển khai và đảm bảo tương thích với hệ thống mục tiêu. Điều này bao gồm cấu hình cơ sở hạ tầng cần thiết và xử lý logistics triển khai, chẳng hạn như quản lý tài nguyên máy chủ và giao tiếp mạng. Phục vụ Code LLM hiệu quả là rất quan trọng để tích hợp chúng vào các ứng dụng thế giới thực và cho phép người dùng tận dụng khả năng của chúng.

Đánh giá: Đối với các mô hình lớn, người dùng thường muốn đánh giá chất lượng của chúng so với các điểm chuẩn tiêu chuẩn. Ví dụ, khi đánh giá các mô hình được tinh chỉnh cho các nhiệm vụ sinh mã nguồn, người dùng có thể đánh giá tỷ lệ vượt qua (pass@k). Điều này bao gồm việc sử dụng mô hình để sinh ra các token mã nguồn và thực thi các đầu ra được sinh với các bài kiểm tra đơn vị. Tuy nhiên, các token mã nguồn được sinh có thể đòi hỏi các bước hậu xử lý, chẳng hạn như cắt bớt các thế hệ không hoàn chỉnh hoặc áp dụng các quy tắc định dạng để đảm bảo khả năng đọc mã nguồn. Hơn nữa, các metric cụ thể cho mã nguồn, như CodeBLEU và Edit Similarity, cũng được sử dụng để đánh giá hiệu suất của các mô hình một cách chính xác. Triển khai những metric này có thể là thách thức vì các nghiên cứu khác nhau có thể áp dụng các phương pháp đa dạng, cản trở khả năng tái tạo kết quả mô hình được huấn luyện trước và xác minh các mô hình mới được huấn luyện. Đảm bảo đánh giá đúng đắn các Code LLM cho phép người dùng có được hiểu biết sâu sắc về hiệu suất của chúng và đưa ra quyết định sáng suốt về sự phù hợp của chúng cho các nhiệm vụ cụ thể.

Thực hiện các nhiệm vụ trên một cách riêng lẻ đòi hỏi người dùng tích hợp các thư viện và công cụ khác nhau vào một codebase duy nhất, làm cho việc thúc đẩy khả năng sử dụng của Code LLM trong các công cụ cấp độ sản xuất trở nên thách thức. Các thư viện hiện có như HuggingFace Transformers (HF-T) cung cấp các giao diện thống nhất để xử lý các công cụ đa dạng và phức tạp để làm việc với LLM. Tuy nhiên, HF-T không đáp ứng đầy đủ các nhu cầu cụ thể của các nhiệm vụ trí tuệ mã nguồn. CodeTF giải quyết nhược điểm này bằng cách phục vụ như một lớp cấp cao hơn được xây dựng trên HuggingFace Transformers và các công cụ khác, được thiết kế đặc biệt để đáp ứng các yêu cầu của các nhiệm vụ trí tuệ mã nguồn. Phần tiếp theo làm nổi bật các nguyên tắc thiết kế chính của CodeTF.

2.2 Nguyên tắc thiết kế

Trong việc thiết kế CodeTF, chúng tôi tuân thủ một số nguyên tắc quan trọng hướng dẫn phương pháp của chúng tôi trong việc tạo ra một thư viện mạnh mẽ và lấy người dùng làm trung tâm cho các nhiệm vụ trí tuệ mã nguồn. Những nguyên tắc này phục vụ như nền tảng cho các lựa chọn thiết kế và chức năng được triển khai trong CodeTF, đảm bảo rằng nó đáp ứng các nhu cầu đa dạng của các nhà thực hành và nhà nghiên cứu.

--- TRANG 5 ---
1.Tính toàn diện: CodeTF cố gắng trở thành một thư viện toàn diện, bao quát các khía cạnh khác nhau của các mô hình ngôn ngữ lớn mã nguồn. Điều này bao gồm các chức năng như tải và phục vụ các mô hình tiên tiến trong các kiểu khác nhau (chỉ bộ mã hóa, chỉ bộ giải mã và bộ mã hóa-giải mã), huấn luyện trước và tinh chỉnh, đánh giá và thao tác mã nguồn cho mục đích huấn luyện. CodeTF phục vụ như một giải pháp một điểm dừng, bao quát những khía cạnh thiết yếu này.

2.Thân thiện với người dùng: CodeTF ưu tiên tính thân thiện với người dùng, đảm bảo rằng thư viện không chỉ hữu ích mà còn có thể tiếp cận được với một phạm vi rộng người dùng, từ người mới bắt đầu đến các nhà nghiên cứu nâng cao. Chúng tôi đơn giản hóa các quy trình cài đặt và thiết lập, giảm nhu cầu cấu hình phức tạp hoặc phụ thuộc. Mục tiêu là đảm bảo rằng người dùng có thể dễ dàng bắt đầu với CodeTF, bất kể kinh nghiệm hoặc chuyên môn trước đó của họ.

3.Khả năng sử dụng: Trong khi tính thân thiện với người dùng là về trải nghiệm ban đầu khi bắt đầu với CodeTF, khả năng sử dụng tập trung vào sự dễ dàng và hiệu quả của việc tương tác với thư viện một cách liên tục. Chúng tôi nhắm đến việc cung cấp một giao diện gắn kết và trực quan cho các nhiệm vụ trí tuệ mã nguồn khác nhau. Điều này bao gồm đơn giản hóa các nhiệm vụ phức tạp như thu thập dữ liệu, trích xuất thuộc tính mã nguồn, chuyển đổi dữ liệu cho các framework học sâu, quản lý GPU và cấu hình vòng lặp huấn luyện.

4.Khả năng mở rộng: Chúng tôi nhận ra bản chất phát triển nhanh chóng của Code LLM, với các mô hình mới sử dụng các phương pháp huấn luyện khác nhau và các điểm chuẩn bổ sung xuất hiện. Để thích ứng với những tiến bộ trong tương lai, chúng tôi thiết kế CodeTF theo các nguyên tắc kỹ thuật phần mềm như Lập trình Hướng đối tượng, đảm bảo khả năng mở rộng và linh hoạt.

5.Khả năng mở rộng quy mô: Quản lý khả năng mở rộng quy mô hệ thống trong quá trình huấn luyện và phục vụ Code LLM có thể là thách thức, đặc biệt trên các thiết bị và môi trường khác nhau. CodeTF đơn giản hóa quá trình này bằng cách tận dụng cơ sở hạ tầng có thể mở rộng và tối ưu hóa việc phân bổ tài nguyên.

6.Khả năng tái tạo: Khả năng tái tạo là một khía cạnh quan trọng của Code LLM, đặc biệt khi đánh giá hiệu suất của chúng trên các điểm chuẩn nổi tiếng như HumanEval [12], MBPP [23] và APPS [13]. Tuy nhiên, nhiều codebase mô hình được phát hành thiếu các script cần thiết để tái tạo kết quả, cản trở khả năng của cộng đồng nghiên cứu trong việc xác minh hiệu suất Code LLM. CodeTF giải quyết vấn đề này thông qua giao diện thống nhất có khả năng tải một phạm vi rộng Code LLM, cùng với giao diện Đánh giá tạo điều kiện cho khả năng tái tạo cho cộng đồng nghiên cứu.

2.3 Mô-đun

Với động lực và nguyên tắc thiết kế, chúng tôi đã thiết kế các mô-đun phù hợp với những mục tiêu này. Thư viện CodeTF bao gồm sáu mô-đun chính: Vườn mô hình, Phục vụ mô hình, Huấn luyện mô hình, Đánh giá, Tiện ích dữ liệu và Tiện ích mã nguồn.

•Vườn mô hình chứa các cấu hình cho các mô hình được huấn luyện trước hoặc tinh chỉnh nổi tiếng cho các nhiệm vụ cụ thể. Ba loại chính của Code LLM được xem xét: mô hình chỉ bộ giải mã (hoặc kiểu GPT), mô hình bộ mã hóa-giải mã (hoặc Seq2Seq), và mô hình chỉ bộ mã hóa (hoặc kiểu BERT).

•Mô-đun Phục vụ mô hình có thể tải các mô hình thông qua một giao diện, chỉ định loại mô hình (GPT, Seq2Seq, BERT), kích thước mô hình và các nhiệm vụ mà các mô hình được dự định (huấn luyện trước, tóm tắt, sinh, v.v.). Mô-đun có thể thực hiện dự đoán trên các đầu vào thô, chẳng hạn như các đoạn mã nguồn hoặc mô tả ngôn ngữ tự nhiên.

•Mô-đun Huấn luyện mô hình cung cấp các tiện ích để huấn luyện trước hoặc tinh chỉnh các mô hình, quản lý GPU và xử lý các cấu hình mạng nơ-ron. Nó nhận mô hình được tải từ mô-đun Phục vụ mô hình và khởi tạo các trọng số để huấn luyện.

•Mô-đun Tiện ích dữ liệu cung cấp các tiện ích để hỗ trợ mô-đun Huấn luyện mô hình trong việc tải các tập dữ liệu nổi tiếng. Những tập dữ liệu này được tiền xử lý ở các giai đoạn khác nhau thành các định dạng phù hợp để đầu vào cho mô-đun Huấn luyện mô hình.

•Để tạo điều kiện cho việc xử lý mã nguồn, mô-đun Tiện ích mã nguồn cung cấp các công cụ để thao tác dễ dàng mã nguồn. Điều này bao gồm tải bộ phân tích cú pháp AST để phân tích cú pháp mã nguồn và thực hiện duyệt trên AST để trích xuất các thuộc tính mã nguồn quan trọng, đây là một bước quan trọng trong tiền xử lý dữ liệu.

•Cuối cùng, mô-đun Đánh giá xác thực kết quả của các mô hình được huấn luyện trên các điểm chuẩn nổi tiếng. Nó có thể nhận các thể hiện được tải từ mô-đun Phục vụ mô hình và tính toán hiệu suất mô hình với các metric đánh giá.

Thêm chi tiết về từng mô-đun có thể được tìm thấy trong phần tiếp theo.

--- TRANG 6 ---
load_model_pipeline(args) registry CodeGen StarCoder 
CodeT5 CodeT5+ 
CodeBERT CodeBERTA CausalLM 
Seq2Seq 
BERT …
…
…Model 
Class

Hình 4: Tổng quan về pipeline tải mô hình trong CodeTF

3 Mô-đun và tiện ích

3.1 Vườn mô hình

Vườn mô hình - codetf.model cung cấp các cấu hình cho cả các checkpoint được huấn luyện trước và tinh chỉnh từ các LLM nổi tiếng, bao gồm các loại kiến trúc mô hình Transformer khác nhau. Cụ thể, CodeTF có thể hỗ trợ một phạm vi rộng các LLM: mô hình chỉ bộ mã hóa (CodeBERT [6], CodeBERTA [24]), mô hình chỉ bộ giải mã (CodeParrot [19], Incoder [20], CodeGen [5], SantaCoder [21], StarCoder [22]), và mô hình bộ mã hóa-giải mã (CodeT5 [4], CodeT5+ [18]). Mô-đun này đơn giản hóa việc truy cập các mô hình tiên tiến cho các nhiệm vụ trí tuệ mã nguồn, cho phép người dùng sử dụng những mô hình này trong các ứng dụng của họ.

Ngoài các checkpoint được huấn luyện trước, CodeTF cũng hỗ trợ các mô hình được tinh chỉnh cho các nhiệm vụ hạ nguồn cụ thể, như tóm tắt mã nguồn, sinh mã nguồn và hoàn thiện mã nguồn. Thư viện cho phép người dùng dễ dàng truy cập những mô hình này thông qua một giao diện lập trình thống nhất trên các nhiệm vụ khác nhau. Mỗi mô hình được đi kèm với một file cấu hình YAML chứa thông tin thiết yếu như URL Hugging Face, tokenizer, độ dài chuỗi tối đa, và nhiều hơn nữa. Bằng cách cung cấp một giao diện cho kho lưu trữ Hugging Face, mô-đun Vườn mô hình đảm bảo rằng người dùng có thể dễ dàng cập nhật với những tiến bộ mới nhất trong lĩnh vực, thúc đẩy việc áp dụng và triển khai các mô hình tiên tiến trên nhiều trường hợp sử dụng trí tuệ mã nguồn.

3.2 Mô-đun phục vụ mô hình

Mô-đun Phục vụ mô hình - codetf.predict cung cấp cho người dùng khả năng tải các checkpoint mô hình được huấn luyện trước hoặc tinh chỉnh từ Vườn mô hình và áp dụng những mô hình này cho nhiều nhiệm vụ khác nhau. CodeTF có thể hỗ trợ cả nhiều nhiệm vụ mã nguồn đầy thử thách, bao gồm tóm tắt mã nguồn, hoàn thiện mã nguồn, sinh mã nguồn từ văn bản và tinh chỉnh mã nguồn. Mô-đun Phục vụ mô hình đơn giản hóa việc triển khai các mô hình cho một loạt các nhiệm vụ trí tuệ mã nguồn bằng cách cung cấp một giao diện thuận tiện nhận bất kỳ đoạn mã nguồn mới nào làm đầu vào và trả về một dự đoán mô hình tương ứng.

Để tạo điều kiện cho một giao diện nhanh chóng và thân thiện với người dùng để triển khai và kiểm tra các mô hình được huấn luyện trước của chúng tôi, chúng tôi nhận ra tầm quan trọng của việc lượng tử hóa mô hình. Các mô hình Pytorch thô có thể cồng kềnh và tốn thời gian để cung cấp kết quả suy luận (ví dụ: khoảng 1,2 giây mỗi mẫu cho CodeGen-16B), làm cho việc lượng tử hóa trở nên thiết yếu để giảm thiểu kích thước mô hình trong khi duy trì hiệu suất thỏa đáng. CodeTF tích hợp BitsandByte [25], và GPTQ [26] như các lựa chọn lượng tử hóa đa dạng để phù hợp với các yêu cầu khác nhau. Hình 4 phác thảo quá trình tải mô hình. Ban đầu, một hàm nhập có tên load_model_pipeline được gọi nơi người dùng sẽ chỉ định loại mô hình cùng với các tham số khác, như tên của mô hình. 'Registry' là một mô-đun đăng ký lớp mô hình liên quan, bao gồm CausalLMModel, Seq2SeqModel, BERTModel. Mỗi lớp mô hình đại diện cho một loại kiến trúc mô hình ngôn ngữ khác nhau cho mã nguồn. Mỗi lớp mô hình được liên kết với một file cấu hình để chọn checkpoint được đặt trước được định nghĩa trong Vườn mô hình. Một khi lớp mô hình được khởi tạo, người dùng có thể sử dụng nó để đưa ra dự đoán dựa trên một đầu vào.

3.3 Mô-đun huấn luyện mô hình

Mô-đun Huấn luyện - codetf.trainer trang bị cho người dùng khả năng truy cập các checkpoint từ thẻ mô hình và điều chỉnh các mô hình của họ để tương thích với các tập dữ liệu hoặc nhiệm vụ hiện có. Mô-đun này cung cấp cho người dùng một giao diện thống nhất để dễ dàng tinh chỉnh LLM để phù hợp với ngân sách tính toán cụ thể và ứng dụng của họ. Ngoài việc tinh chỉnh mô hình thông thường, chúng tôi cung cấp cho người dùng, đặc biệt là những người dưới ràng buộc ngân sách tính toán, một tùy chọn để sử dụng các phương pháp tinh chỉnh hiệu quả tham số của chúng tôi.

--- TRANG 7 ---
BaseTrainer 
CausalLMTrainer Seq2SeqTrainer BERTTrainer 
LORA, AdaLORA, Prompt-Tuning CausaLMModel 
GPT, CodeGen, StarCoder, 
SantaCoder Seq2SeqModel 
CodeT5, CodeT5+ BertModel 
CodeBERT, CodeBERTA 
CausalLMConfig Seq2SeqConfig BertConfig

Hình 5: Tổng quan về Trainer của CodeTF: BaseTrainer là lớp cơ sở mà tất cả các trainer mô hình kế thừa. Ba lớp Trainer chính là CausalLMTrainer, Seq2SeqTrainer và BERTTrainer. Chúng tôi thiết kế những trainer này để tương thích với các họ mô hình ngôn ngữ (LLM) khác nhau cho mã nguồn, bao gồm CausalLMModel, Seq2SeqModel và BERTModel, tương ứng.

Để thúc đẩy việc tinh chỉnh hiệu quả tham số, chúng tôi áp dụng PEFT3 làm nền tảng. Chúng tôi tích hợp các kỹ thuật tinh chỉnh khác nhau như LORA [27], Prefix-Tuning [28], P-Tuning [29], Prompt Tuning [30] và AdaLORA [31]. Những kỹ thuật này đã chứng minh lợi ích đáng kể trong việc điều chỉnh LLM (với hàng tỷ tham số) trong khi giữ chi phí huấn luyện ở mức có thể chi trả. Bằng cách cung cấp những lựa chọn này cho việc tinh chỉnh mô hình, CodeTF trao quyền cho người dùng sửa đổi các mô hình được huấn luyện trước theo yêu cầu huấn luyện độc quyền của họ.

Hình 5 minh họa tổng quan về cách các Trainer được triển khai. Các lớp Trainer, bao gồm CausalLMTrainer, Seq2SeqTrainer và BERTTrainer, tất cả đều kế thừa từ một BaseTrainer. Những lớp trainer này tương ứng với các họ mô hình ngôn ngữ (LLM) khác nhau cho mã nguồn, bao gồm CausalLMModel, Seq2SeqModel và BERTModel, tương ứng. Các Trainer được gán với các cấu hình huấn luyện được định nghĩa trước cụ thể cho mỗi họ mô hình. Chúng tôi cung cấp thêm các cấu hình cho các phương pháp tinh chỉnh hiệu quả tham số từ PEFT của HuggingFace như một tùy chọn cho người dùng, cho phép họ tinh chỉnh hiệu quả các mô hình thông qua những cấu hình tích hợp này.

3.4 Mô-đun tiện ích dữ liệu

Mô-đun Tiện ích dữ liệu - codetf.data_utility cung cấp một bộ công cụ cho việc tiền xử lý dữ liệu, bao gồm tokenization, xử lý mã nguồn và bộ tải dữ liệu. Những tiện ích này đảm bảo rằng dữ liệu được chuẩn bị một cách thích hợp để sử dụng trong huấn luyện và suy luận, thúc đẩy hiệu suất mô hình hiệu quả và có thể tái tạo. Bằng cách cung cấp một bộ công cụ tiền xử lý dữ liệu toàn diện, mô-đun Tiện ích dữ liệu đơn giản hóa quá trình chuẩn bị dữ liệu mã nguồn cho các nhiệm vụ học máy khác nhau.

3.5 Mô-đun đánh giá

Mô-đun Đánh giá - codetf.performance cung cấp một giao diện thống nhất cung cấp nhiều metric hiệu suất được thiết kế riêng cho các nhiệm vụ trí tuệ mã nguồn. Những metric này bao gồm nhưng không giới hạn ở độ tương tự chỉnh sửa Levenshtein, pass@k [12,32] và CodeBLEU [33]. Bằng cách cung cấp một giao diện để đo lường những metric chuẩn hóa này, chúng tôi tìm cách đơn giản hóa quá trình đánh giá và tạo điều kiện cho khả năng tái tạo kết quả trên các điểm chuẩn phổ biến. Cuối cùng, giao diện thống nhất này được thiết kế để thúc đẩy hiểu biết tốt hơn và khả năng so sánh giữa các bài báo nghiên cứu khác nhau, thúc đẩy hợp tác và đổi mới trong cộng đồng nghiên cứu.

Chúng tôi cũng nhắm đến việc cung cấp một giao diện thống nhất cung cấp nhiều metric được thiết kế riêng cho các nhiệm vụ trí tuệ mã nguồn, bao gồm nhưng không giới hạn ở pass@k [12,32], Edit Similarity [34] và CodeBLEU [33]. Bằng cách cung cấp những metric chuẩn hóa này, chúng tôi tìm cách đơn giản hóa quá trình đánh giá và tạo điều kiện cho khả năng tái tạo kết quả trên các điểm chuẩn được công nhận rộng rãi. Ngoài ra, giao diện thống nhất này được thiết kế để thúc đẩy hiểu biết tốt hơn và khả năng so sánh giữa

3https://github.com/huggingface/peft

--- TRANG 8 ---
các bài báo nghiên cứu khác nhau, thúc đẩy hợp tác và đổi mới trong cộng đồng nghiên cứu. Về lâu dài, chúng tôi hình dung rằng giao diện thống nhất của chúng tôi cho các metric cụ thể cho mã nguồn sẽ phục vụ như một công cụ có giá trị cho các nhà nghiên cứu, cải thiện khả năng tổng quát hóa và ứng dụng mô hình, và cuối cùng thúc đẩy đổi mới trong lĩnh vực trí tuệ mã nguồn.

3.6 Tiện ích mã nguồn

Bên cạnh các hàm tiện ích thông thường liên quan đến huấn luyện và kiểm tra mô hình, chúng tôi cũng cung cấp mô-đun Tiện ích mã nguồn - codetf.code_utility, giúp người dùng thao tác dữ liệu mã nguồn. CodeTF cung cấp cho người dùng các hàm tích hợp để trích xuất các thuộc tính mã nguồn quan trọng, sử dụng tree-sitter4 làm bộ phân tích cú pháp cho 15 ngôn ngữ lập trình (bao gồm Java, Apex, C, C++, C#, Python, Scala, SOQL, SOSL, PHP, JavaScript, Haskell, Go, Kotlin, Ruby, Rust, Scala, Solidity và YAML). Tree-sitter là một công cụ tạo bộ phân tích cú pháp và thư viện phân tích cú pháp tăng dần có thể xây dựng cây cú pháp cụ thể cho một file mã nguồn và cập nhật hiệu quả cây cú pháp khi file nguồn được chỉnh sửa. Trong khi tất cả các ngôn ngữ được hỗ trợ đều sử dụng tree-sitter làm xương sống để phân tích cú pháp mã nguồn thành AST, mỗi ngôn ngữ dựa vào một bộ quy tắc cú pháp riêng biệt. Chúng tôi đã tập hợp các quy tắc cú pháp mã nguồn mở cho mỗi ngôn ngữ và xây dựng trước chúng thành các file ".so" tương thích với các hệ điều hành khác nhau. Hiện tại, chúng tôi hỗ trợ các hệ điều hành chính như Darwin, Linux và Windows. Những file ".so" này được gói với CodeTF và có thể dễ dàng được tải thông qua giao diện lập trình mà không cần cài đặt bổ sung.

Ngoài việc phân tích cú pháp, mô-đun Tiện ích mã nguồn cung cấp nhiều hàm hỗ trợ hữu ích khác như loại bỏ bình luận, trích xuất thuộc tính mã nguồn (ví dụ: bình luận, tên biến, tên phương thức), và nhiều hơn nữa. Mỗi ngôn ngữ lập trình kế thừa một lớp BaseCodeUtility, cho phép tạo ra các lớp tiện ích cụ thể theo ngôn ngữ (ví dụ: JavaCodeUtility, PythonCodeUtility, ApexCodeUtility) triển khai các hàm dựa trên thuộc tính của ngôn ngữ. Mô-đun này đảm bảo việc xử lý và thao tác mã nguồn hiệu quả, phục vụ các quy tắc cú pháp độc đáo của mỗi ngôn ngữ lập trình được hỗ trợ.

4 Ví dụ sử dụng

Giao diện thống nhất để tải mô hình và thực hiện suy luận CodeTF cung cấp giao diện thống nhất để tải các mô hình được hỗ trợ. Điều này hữu ích cho việc sử dụng sẵn có của suy luận mô hình, v.v. Trong ví dụ này, chúng tôi cho thấy cách tải một checkpoint mô hình CodeT5 cho nhiệm vụ tóm tắt mã nguồn cho chương trình Python

1from codetf.models import load_model_pipeline
2
3summarization_model = load_model_pipeline(model_name="codet5", task="sum-python",
4 model_type="base", is_eval=True,
5 load_in_8bit=True, weight_sharding=False)
6
7code_snippets = """
8 void bubbleSort(int arr[])
9 {
10 int n = arr.length;
11 for (int i = 0; i < n - 1; i++)
12 for (int j = 0; j < n - i - 1; j++)
13 if (arr[j] > arr[j + 1]) {
14 // swap arr[j+1] and arr[j]
15 int temp = arr[j];
16 arr[j] = arr[j + 1];
17 arr[j + 1] = temp;
18 }
19 }
20"""
21
22# Chương trình sắp xếp nổi bọt để sắp xếp một mảng số nguyên
23summaries = summarization_model.predict([code_snippets])

Giao diện thống nhất để tinh chỉnh mô hình CodeTF cung cấp một giao diện thống nhất để tinh chỉnh một mô hình dựa trên các checkpoint được hỗ trợ. Ví dụ sau đây cho thấy cách tải một mô hình CodeGen và tinh chỉnh nó bằng cách sử dụng một tập dữ liệu CodeXGLUE được tiền xử lý.

4https://github.com/tree-sitter/tree-sitter

--- TRANG 9 ---
1from codetf.trainer.causal_lm_trainer import CausalLMTrainer
2from codetf.data_utility.codexglue_dataset import CodeXGLUEDataset
3from codetf.models import load_model_pipeline
4from codetf.performance.evaluate import EvaluationMetric
5
6model_class = load_model_pipeline(model_name="causal-lm", task="pretrained",
7 model_type="codegen-350M-mono", is_eval=False,
8 load_in_8bit=False, weight_sharding=False)
9
10
11dataloader = CodeXGLUEDataset(tokenizer=model_class.get_tokenizer())
12train_dataset, test_dataset, val_dataset = dataloader.load(subset="text-to-code")
13
14evaluator = EvaluationMetric(metric="bleu", tokenizer=model_class.tokenizer)
15
16# peft có thể là ["lora", "prefixtuning"]
17trainer = CausalLMTrainer(train_dataset=train_dataset,
18 validation_dataset=val_dataset,
19 peft=None,
20 pretrained_model_or_path=model_class.get_model(),
21 tokenizer=model_class.get_tokenizer())
22trainer.train()

Giao diện thống nhất để đánh giá mô hình trên các điểm chuẩn nổi tiếng CodeTF cung cấp một giao diện thống nhất để đánh giá các mô hình so với các điểm chuẩn nổi tiếng trên nhiều metric khác nhau. Ví dụ sau đây cho thấy cách tải giao diện đánh giá và sử dụng metric pass@k để đánh giá một mô hình CodeGen trên điểm chuẩn Human-Eval.

1from codetf.models import load_model_pipeline
2from codetf.data_utility.human_eval_dataset import HumanEvalDataset
3from codetf.performance.model_evaluator import ModelEvaluator
4
5os.environ["HF_ALLOW_CODE_EVAL"] = "1"
6os.environ["TOKENIZERS_PARALLELISM"] = "true"
7
8model_class = load_model_pipeline(model_name="causal-lm", task="pretrained",
9 model_type="codegen-350M-mono", is_eval=True,
10 load_in_8bit=True, weight_sharding=False)
11
12dataset = HumanEvalDataset(tokenizer=model_class.get_tokenizer())
13prompt_token_ids, prompt_attention_masks, references = dataset.load()
14
15problems = TensorDataset(prompt_token_ids, prompt_attention_masks)
16
17evaluator = ModelEvaluator(model_class)
18pass_at_k = evaluator.evaluate_pass_k(problems=problems, unit_tests=references, k=[1,10,100])

5 Nghiên cứu liên quan

Trong phần này, chúng tôi cung cấp một tổng quan về nghiên cứu của LLM cho mã nguồn và sự phát triển liên quan của các thư viện/công cụ để hỗ trợ những mô hình này.

Mô hình ngôn ngữ lớn cho mã nguồn Các mô hình ngôn ngữ lớn (LLM) cho mã nguồn đã thu hút sự chú ý đáng kể trong những năm gần đây, được thúc đẩy bởi khả năng hỗ trợ một phạm vi rộng các nhiệm vụ hiểu biết mã nguồn như sinh mã nguồn [6,4,35], hoàn thiện mã nguồn [6,4,36], sửa chữa chương trình [37] và dịch mã nguồn [38]. Thành công của các mô hình ngôn ngữ lớn (LLM) như BERT [39] và GPT [40] trong xử lý ngôn ngữ tự nhiên (NLP) đã truyền cảm hứng cho các nhà nghiên cứu thích ứng các mô hình ngôn ngữ NLP cho mã nguồn [6,4,41,42,35,36,43,44,45,46]. Họ thường coi mã nguồn như văn bản ngôn ngữ tự nhiên và tận dụng các chiến lược huấn luyện trước như span corruption và causal LM từ lĩnh vực NLP, điều này đã dẫn đến kết quả tiên tiến mới trên một phạm vi rộng các nhiệm vụ liên quan đến mã nguồn.

Code LLM có thể được nhóm thành ba kiến trúc chính: mô hình chỉ bộ mã hóa [6,7,47], mô hình chỉ bộ giải mã [11,12,20,5], và mô hình bộ mã hóa-giải mã [48,4,49,50]. Mô hình chỉ bộ mã hóa xuất sắc trong các nhiệm vụ hiểu biết như

--- TRANG 10 ---
Bảng 1: So sánh các tính năng giữa CodeTF và HuggingFace Transformers (HF-T). Lưu ý rằng chúng tôi so sánh những thư viện này theo các tính năng liên quan đến lĩnh vực mã nguồn, làm nổi bật các chức năng mà HF-T có thể không hỗ trợ cụ thể.

Tính năng CodeTF (Của chúng tôi) HF-T
Giao diện mô hình và tập dữ liệu thống nhất ✓ ✓
Tinh chỉnh hiệu quả tham số thống nhất cho các nhiệm vụ trí tuệ mã nguồn ✓ ✓
Giao diện tiện ích mã nguồn thống nhất cho nhiều ngôn ngữ lập trình ✓
Giao diện metric thống nhất để đánh giá các điểm chuẩn trí tuệ mã nguồn ✓
Giao diện bộ tải dữ liệu thống nhất để xử lý các điểm chuẩn trí tuệ mã nguồn ✓
Thiết kế thư viện mô-đun ✓ ✓
Checkpoint mô hình được huấn luyện trước ✓ ✓
Checkpoint mô hình được tinh chỉnh theo nhiệm vụ cụ thể ✓ ✓

truy xuất mã nguồn [24], trong khi mô hình chỉ bộ giải mã phù hợp cho các nhiệm vụ sinh như tổng hợp chương trình [12,13]. Mặc dù mô hình bộ mã hóa-giải mã [4,48] có thể được thích ứng cho cả nhiệm vụ hiểu biết và sinh mã nguồn, chúng không phải lúc nào cũng vượt trội hơn mô hình chỉ bộ giải mã hoặc chỉ bộ mã hóa. Trong CodeTF, chúng tôi gói một phạm vi rộng các mô hình đại diện cho các kiến trúc khác nhau vào một giao diện thống nhất.

Thư viện thống nhất cho các nhiệm vụ trí tuệ mã nguồn Code LLM gần đây đã thu hút sự chú ý đáng kể để giải quyết các nhiệm vụ kỹ thuật phần mềm. Tuy nhiên, trí tuệ mã nguồn bao quát một phạm vi rộng hơn, kết hợp những tiến bộ mới nhất trong trí tuệ nhân tạo với các phương pháp kỹ thuật phần mềm truyền thống, như phân tích tĩnh, phân tích động, phân tích con trỏ và phương pháp hình thức, để giải quyết hiệu quả các nhiệm vụ kỹ thuật phần mềm phức tạp. Trong phiên bản đầu tiên của CodeTF, trọng tâm của chúng tôi nằm ở việc gói các LLM tiên tiến cho mã nguồn với các tiện ích bổ sung cho các phương pháp kỹ thuật phần mềm truyền thống, bao gồm các bộ phân tích cú pháp AST.

Một số thư viện khác với mục tiêu tương tự tồn tại trong ngành. NaturalCC [51] là một nền tảng được thiết kế để tạo điều kiện cho nghiên cứu phân tích big code dựa trên NLP để huấn luyện và tái tạo. Tuy nhiên, khả năng sử dụng của nó bị hạn chế do thiết kế không tối ưu và thách thức trong việc mở rộng khả năng của nó. HuggingFace Transformers [52] là một thư viện nổi tiếng rộng rãi cung cấp các giao diện thân thiện với người dùng để tải các mô hình ngôn ngữ được huấn luyện trước trên các lĩnh vực khác nhau (thị giác máy tính, xử lý ngôn ngữ tự nhiên, mã nguồn và chuỗi thời gian), thu hút sự chú ý đáng kể từ cộng đồng nghiên cứu. Tuy nhiên, bản chất tổng quát của nó có thể gây khó khăn cho người dùng tìm kiếm các tính năng được thiết kế riêng cho lĩnh vực mã nguồn. Cũng có các kho lưu trữ mã nguồn mở khác cho trí tuệ mã nguồn, như CodeT5 [4], CodeGeeX [53], CodeBERT [6] và CodeXGLUE [11]. Tuy nhiên, hầu hết những cái này không phải là thư viện thống nhất cho trí tuệ mã nguồn mà là các mô hình cụ thể với hướng dẫn về cách tải các checkpoint.

Bảng 5 tóm tắt so sánh giữa các tính năng chính của CodeTF với HuggingFace Transformers. Điều quan trọng cần lưu ý là HuggingFace Transformers (HF-T) là một thư viện toàn diện bao quát các mô hình ngôn ngữ tiên tiến và tiện ích cho nhiều lĩnh vực nghiên cứu. So sánh được cung cấp trong Bảng 5 tập trung duy nhất vào các tính năng liên quan đến lĩnh vực mã nguồn, làm nổi bật các khu vực mà HuggingFace Transformers có thể thiếu chức năng nhất định.

6 Kế hoạch tương lai & cải tiến

Chúng tôi tiếp tục tích cực cải thiện CodeTF như một thư viện mã nguồn mở một điểm dừng cho Code LLM và các nhiệm vụ trí tuệ mã nguồn. Chúng tôi có một số kế hoạch để mở rộng khả năng của nó và hỗ trợ các trường hợp sử dụng tiên tiến hơn và cải thiện khả năng tái tạo mô hình. Một số tính năng chính chúng tôi nhắm đến việc tích hợp trong tương lai bao gồm:

•Triển khai lượng tử hóa 4-bit như một phần của các mô hình được huấn luyện trước và tinh chỉnh, cho phép ngay cả các mô hình lớn như InstructCodeT5+ [18] chạy hiệu quả trên laptop thương mại hoặc máy trạm.

•Tiến hành đánh giá toàn diện các nhiệm vụ trí tuệ mã nguồn nổi tiếng trên các điểm chuẩn đã được thiết lập (CodeXGLUE, MBPP, Human-Eval và APPS). Do những tiến bộ nhanh chóng trong lĩnh vực, có sự thiếu hụt khả năng tái tạo hiệu suất của các mô hình tiên tiến, làm cho việc thích ứng và thúc đẩy hợp tác cho cộng đồng nghiên cứu trở nên thách thức.

•Nâng cao mô-đun Tiện ích mã nguồn bằng cách thêm hỗ trợ cho các ngôn ngữ lập trình khác, như Go, Rust, C# và nhiều hơn nữa. Chúng tôi cũng có kế hoạch bao gồm các tiện ích để trích xuất các tính năng hữu ích bổ sung từ mã nguồn, như đồ thị gọi, luồng điều khiển, luồng dữ liệu và các tính năng khác.

•Tích hợp một lựa chọn rộng hơn các mô hình ngôn ngữ được huấn luyện trước tiên tiến gần đây của mã nguồn vào CodeTF, củng cố thêm thư viện của chúng tôi như một tài nguyên toàn diện trong lĩnh vực.

--- TRANG 11 ---
7 Kết luận

Trong bài báo này, chúng tôi giới thiệu CodeTF, một thư viện Transformer mã nguồn mở một điểm dừng cho trí tuệ mã nguồn và Code LLM. Thư viện cung cấp một bộ công cụ mạnh mẽ và đa năng để phát triển và triển khai LLM cho các nhiệm vụ liên quan đến mã nguồn. Với kiến trúc mô-đun và bộ tính năng toàn diện, thư viện cho phép người dùng dễ dàng thực hiện nhiều nhiệm vụ liên quan đến mã nguồn, như tóm tắt mã nguồn, hoàn thiện, sinh và tinh chỉnh. Bằng cách cung cấp quyền truy cập vào các mô hình tiên tiến, khả năng tinh chỉnh và đánh giá, và một phạm vi các tập dữ liệu phổ biến, thư viện của chúng tôi trao quyền cho người dùng tận dụng những tiến bộ mới nhất trong nghiên cứu và phát triển trí tuệ mã nguồn.

8 Tác động rộng hơn và sử dụng có trách nhiệm

Trong khi các mô hình trong CodeTF cho thấy tiềm năng to lồn trong các nhiệm vụ liên quan đến mã nguồn khác nhau, chúng không cung cấp bảo đảm tuyệt đối về khả năng trí tuệ mã nguồn của chúng. Các tập dữ liệu và mô hình được huấn luyện trước được sử dụng trong CodeTF có thể mang theo những thiên kiến có thể dẫn đến hiểu lầm, kết quả không chính xác hoặc hành vi không mong muốn. Những thiên kiến này có thể có nhiều hình thức:

1.Thiên kiến ngôn ngữ: Mô hình có thể ưu tiên một số ngôn ngữ lập trình hơn những ngôn ngữ khác dựa trên tần suất của các ngôn ngữ trong dữ liệu huấn luyện. Ví dụ, nếu mô hình được huấn luyện chủ yếu trên mã nguồn Python, nó có thể gặp khó khăn trong việc sinh mã nguồn Java hoặc JavaScript chính xác và thành ngữ.

2.Thiên kiến cụ thể ứng dụng: Điều này xảy ra khi một mô hình được huấn luyện cho một ứng dụng hoặc lĩnh vực cụ thể được sử dụng trong một ứng dụng khác. Ví dụ, một mô hình được huấn luyện trên mã nguồn phát triển web có thể hoạt động kém khi được giao nhiệm vụ sinh mã nguồn hệ thống nhúng.

3.Thiên kiến thư viện và framework: Điều này đề cập đến khuynh hướng vốn có của mô hình hướng tới việc sử dụng các thư viện hoặc framework cụ thể do tần suất hiện diện của chúng trong tập dữ liệu huấn luyện. Ví dụ, nếu mô hình được huấn luyện chủ yếu trên dữ liệu sử dụng Pandas của Python để thao tác dữ liệu, nó có thể có xu hướng sử dụng Pandas ngay cả trong các tình huống mà các thư viện khác như NumPy hoặc cấu trúc Python gốc có thể hiệu quả hoặc phù hợp hơn.

4.Thiên kiến phiên bản ngôn ngữ: Các ngôn ngữ phần mềm phát triển, với các phiên bản mới (ví dụ: Python 2 sang Python 3) giới thiệu những thay đổi, sự lỗi thời và tính năng mới. Nếu tập dữ liệu huấn luyện không được cập nhật thường xuyên để phản ánh những thay đổi này, mô hình có thể sinh mã nguồn sử dụng các quy ước lỗi thời hoặc không còn được sử dụng của một ngôn ngữ.

5.Thiên kiến phong cách viết mã: Phong cách viết mã có thể khác nhau đáng kể giữa các lập trình viên cá nhân, nhóm hoặc cộng đồng. Nếu mô hình được huấn luyện chủ yếu trên một tập dữ liệu phản ánh một phong cách cụ thể, nó có thể sinh mã nguồn phù hợp với phong cách đó, có thể không phải là cách tối ưu hoặc ưa thích cho trường hợp sử dụng cụ thể đang được xem xét.

6.Thiên kiến giải pháp: Thường có thể có nhiều hơn một giải pháp hợp lệ cho một vấn đề lập trình. Mô hình có thể bị thiên kiến về các giải pháp mà nó đã tiếp xúc trong quá trình huấn luyện và có thể không thể sinh ra các giải pháp khác có thể hiệu quả hoặc thanh lịch hơn.

Ngoài những thiên kiến tiềm ẩn này, có một số cân nhắc quan trọng khác:

1.Tính bền vững: Hiệu quả năng lượng là một mối quan tâm đáng kể trong AI, đặc biệt với các mô hình quy mô lớn. Các mô hình được tối ưu hóa sinh mã nguồn hiệu quả hơn có thể giảm tài nguyên tính toán cần thiết để thực thi mã nguồn như vậy, từ đó giảm tiêu thụ năng lượng. Nghiên cứu đang diễn ra về các phương pháp huấn luyện AI tiết kiệm năng lượng hơn cũng có thể giảm dấu chân năng lượng của chính AI.

2.Ngôn ngữ bao hàm: Ngôn ngữ lập trình cần phải bao hàm khi lĩnh vực này ngày càng trở nên đa dạng. Các thuật ngữ không bao hàm có thể làm nản lòng và xúc phạm nhiều nhà phát triển. Công việc tương lai nên tập trung vào việc tạo ra các công cụ để xác định ngôn ngữ không bao hàm trong mã nguồn và đề xuất các lựa chọn thay thế phù hợp.

3.Mất việc làm và tự động hóa: Trong khi AI mang tiềm năng tự động hóa một số nhiệm vụ nhất định, điều thiết yếu là phải xem nó như một công cụ bổ sung thay vì thay thế nỗ lực của con người. Các công cụ nhà phát triển thường được thiết kế để xử lý các nhiệm vụ lặp đi lặp lại, giải phóng nhà phát triển để tập trung vào các vấn đề phức tạp. Tuy nhiên, điều quan trọng là đảm bảo các nhà phát triển không trở nên quá phụ thuộc vào những công cụ này và vẫn có thể lập trình hiệu quả một mình.

4.Kiểm soát và tự chủ của con người: Duy trì kiểm soát và giám sát của con người là rất quan trọng, đặc biệt trong các lĩnh vực quan trọng như sinh mã nguồn. Các kỹ thuật như khả năng giải thích và diễn giải trong AI, cùng với kiểm tra nghiêm ngặt, đảm bảo các hệ thống AI vẫn dưới sự kiểm soát của con người và hoạt động như mong đợi. Mục tiêu nên là tạo ra các hệ thống AI tăng cường khả năng của con người và làm việc cộng tác với con người, thay vì thay thế họ.

Người dùng CodeTF phải xem xét kỹ lưỡng các mô hình được huấn luyện trước và hệ thống tổng thể trước khi áp dụng chúng trong các ứng dụng thực tế. Chúng tôi cam kết tinh chỉnh thư viện bằng cách xác định và giải quyết những thiên kiến tiềm ẩn và

--- TRANG 12 ---
hành vi không phù hợp một cách liên tục. Chúng tôi khuyến khích các nhà nghiên cứu, kỹ sư phần mềm và chuyên gia AI sử dụng thư viện một cách có trách nhiệm cho các ứng dụng tăng cường chất lượng phần mềm và năng suất của nhà phát triển. Tuy nhiên, CodeTF không nên được sử dụng để phát triển các mô hình trí tuệ mã nguồn có thể dẫn đến khả năng phi đạo đức, như thao tác mã nguồn trái phép, vi phạm quyền riêng tư hoặc lan truyền các thực hành lập trình không an toàn. Khi AI trở nên tích hợp hơn vào phát triển phần mềm, điều thiết yếu là giải quyết những cân nhắc đạo đức và thực tế này. CodeTF cam kết hỗ trợ các thực hành AI có trách nhiệm và ngăn chặn những thiên kiến tiềm ẩn và hành vi không phù hợp trong tương lai.

Tài liệu tham khảo
[1]Simone Livieri, Yoshiki Higo, Makoto Matushita, và Katsuro Inoue. Very-large scale code clone analysis and visualization of open source programs using distributed ccfinder: D-ccfinder. Trong 29th International Conference on Software Engineering (ICSE'07), trang 106–115. IEEE, 2007.
[2]Carol V Alexandru và Harald C Gall. Rapid multi-purpose, multi-commit code analysis. Trong 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, tập 2, trang 635–638. IEEE, 2015.
[3]Boyuan Chen và Zhen Ming Jiang. Characterizing and detecting anti-patterns in the logging code. Trong 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE), trang 71–81. IEEE, 2017.
[4]Yue Wang, Weishi Wang, Shafiq R. Joty, và Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih, biên tập, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, trang 8696–8708. Association for Computational Linguistics, 2021.
[5]Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. Trong The Eleventh International Conference on Learning Representations, 2023.
[6]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và Ming Zhou. Codebert: A pre-trained model for programming and natural languages. Trong Trevor Cohn, Yulan He, và Yang Liu, biên tập, Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, tập EMNLP 2020 của Findings of ACL, trang 1536–1547. Association for Computational Linguistics, 2020.
[7]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, và Ming Zhou. Graphcodebert: Pre-training code representations with data flow. Trong ICLR. OpenReview.net, 2021.
[8]Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, và Jian Yin. Unixcoder: Unified cross-modal pre-training for code representation. Trong ACL (1), trang 7212–7225. Association for Computational Linguistics, 2022.
[9]Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, biên tập, Advances in Neural Information Processing Systems, 2022.
[10] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
[11] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. Trong NeurIPS Datasets and Benchmarks, 2021.
[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[13] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, và Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021.
[14] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Retrieval augmented code generation and summarization. Trong EMNLP (Findings), trang 2719–2734. Association for Computational Linguistics, 2021.

--- TRANG 13 ---
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, và Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
[16] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, và Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523–19536, 2022.
[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 35:30016–30030, 2022.
[18] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, và Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023.
[19] Frank F Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. Trong Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, trang 1–10, 2022.
[20] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.
[21] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.
[22] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.
[23] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[24] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. CoRR, abs/1909.09436, 2019.
[25] Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
[26] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[28] Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.
[29] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, và Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021.
[30] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
[31] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, và Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023.
[32] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. Competition-level code generation with alphacode. CoRR, abs/2203.07814, 2022.
[33] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, và Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. CoRR, abs/2009.10297, 2020.
[34] Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, và Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023.

--- TRANG 14 ---
[35] Ahmed Elnaggar, Wei Ding, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Silvia Severini, Florian Matthes, và Burkhard Rost. Codetrans: Towards cracking the language of silicon's code through self-supervised deep learning and high performance computing. arXiv preprint arXiv:2104.02443, 2021.
[36] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, và Tie-Yan Liu. How could neural networks understand programs? Trong International Conference on Machine Learning, trang 8476–8486. PMLR, 2021.
[37] Chunqiu Steven Xia, Yuxiang Wei, và Lingming Zhang. Practical program repair in the era of large pre-trained language models. arXiv preprint arXiv:2210.14179, 2022.
[38] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, và Guillaume Lample. Unsupervised translation of programming languages. Advances in Neural Information Processing Systems, 33:20601–20611, 2020.
[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), trang 4171–4186, 2019.
[40] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[41] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366, 2020.
[42] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Unified Pre-training for Program Understanding and Generation. Trong Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, và Yichao Zhou, biên tập, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, trang 2655–2668. Association for Computational Linguistics, 2021.
[43] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, và Kensen Shi. Learning and evaluating contextual embedding of source code. Trong International Conference on Machine Learning, trang 5110–5121. PMLR, 2020.
[44] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar T Devanbu, và Baishakhi Ray. Natgen: generative pre-training by "naturalizing" source code. Trong Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, trang 18–30, 2022.
[45] Toufique Ahmed và Premkumar Devanbu. Multilingual training for software engineering. Trong Proceedings of the 44th International Conference on Software Engineering, trang 1443–1455, 2022.
[46] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, và Bin Luo. Spt-code: sequence-to-sequence pre-training for learning source code representations. Trong Proceedings of the 44th International Conference on Software Engineering, trang 2006–2018, 2022.
[47] Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, và Jin Liu. CODE-MVP: Learning to represent source code from multiple views with contrastive pre-training. Trong Findings of the Association for Computational Linguistics: NAACL 2022, trang 1066–1077, Seattle, United States, July 2022. Association for Computational Linguistics.
[48] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Unified pre-training for program understanding and generation. Trong NAACL-HLT, trang 2655–2668. Association for Computational Linguistics, 2021.
[49] Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, và Bin Luo. Spt-code: Sequence-to-sequence pre-training for learning source code representations. Trong ICSE, trang 1–13. ACM, 2022.
[50] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Prem Devanbu, và Baishakhi Ray. Natgen: generative pre-training by "naturalizing" source code. Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022.
[51] Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Yulei Sui, Hongyu Zhang, Kazuma Hashimoto, Hai Jin, Guandong Xu, Caiming Xiong, et al. Naturalcc: an open-source toolkit for code intelligence. Trong Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, trang 149–153, 2022.
[52] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander

--- TRANG 15 ---
Rush. Transformers: State-of-the-art natural language processing. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, trang 38–45, Online, October 2020. Association for Computational Linguistics.
[53] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, và Jie Tang. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x, 2023.
