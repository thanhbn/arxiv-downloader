# 2308.03873.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2308.03873.pdf
# File size: 4327458 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Evaluating and Explaining Large Language Models
for Code Using Syntactic Structures
1stDavid N. Palacio
Department of Computer Science
William & Mary
Williamsburg, V A
danaderpalacio@wm.edu2ndAlejandro Velasco
Department of Computer Science
William & Mary
Williamsburg, V A
svelascodimate@wm.edu3rdDaniel Rodriguez-Cardenas
Department of Computer Science
William & Mary
Williamsburg, V A
dhrodriguezcar@wm.edu
4thKevin Moran
Department of Computer Science
University of Central Florida
Orlando, FL
kpmoran@ucf.edu5thDenys Poshyvanyk
Department of Computer Science
William & Mary
Williamsburg, V A
dposhyvanyk@wm.edu
Abstract —Large Language Models (LLMs) for code are a
family of high-parameter, transformer-based neural networks
pre-trained on massive datasets of both natural and program-
ming languages. These models are rapidly being employed in
commercial AI-based developer tools, such as GitHub CoPi-
lot. However, measuring and explaining their effectiveness on
programming tasks is a challenging proposition, given their
size and complexity. The methods for evaluating and explaining
LLMs for code are inextricably linked. That is, in order to
explain a model’s predictions, they must be reliably mapped
to fine-grained, understandable concepts. Once this mapping
is achieved, new methods for detailed model evaluations are
possible. However, most current explainability techniques and
evaluation benchmarks focus on model robustness or individual
task performance, as opposed to interpreting model predictions.
To this end, this paper introduces AST xplainer , an explain-
ability method specific to LLMs for code that enables both new
methods for LLM evaluation and visualizations of LLM predic-
tions that aid end-users in understanding model predictions. At
its core, AST xplainer provides an automated method for aligning
token predictions with AST nodes, by extracting and aggregating
normalized model logits within AST structures. To demonstrate
the practical benefit of AST xplainer , we illustrate the insights
that our framework can provide by performing an empirical
evaluation on 12 popular LLMs for code using a curated dataset
of the most popular GitHub projects. Additionally, we perform
a user study examining the usefulness of an AST xplainer -derived
visualization of model predictions aimed at enabling model users
to explain predictions. The results of these studies illustrate
the potential for AST xplainer to provide insights into LLM
effectiveness, and aid end-users in understanding predictions.
Index Terms —explainability, interpretability, large language
models, dl4se
I. I NTRODUCTION
The advent and proliferation of online open-source code
repositories and rapid advancements in transformer-based neu-
ral large language models (LLMs) have served as a catalyst
for the advancement of automated Software Engineering (SE)
tools with rapidly advancing effectiveness. LLMs for code
have demonstrated considerable proficiency across a diversearray of generative SE tasks, inclusive of, but not restricted
to, code completion [1], [2], program repair [3], [4], and
test case generation [5]. Moreover, these advancements are
rapidly being introduced into commercial developer tools such
as GitHub CoPilot [6] and Replit’s Ghostwriter [7].
However, the sheer complexity and size that enable the often
surprising effectiveness of LLMs for code is a double-edged
sword. That is, while these attributes enable LLMs to capture
important patterns in code that allow them to be applied
to a range of programming tasks, effectively explaining and
evaluating the capabilities of these models is a challenging
proposition — they effectively function as “black boxes” that
derive predictions from exceedingly complex internal model
mechanics. Current research in both designing LLMs for code
and in applying them to programming tasks typically makes
use of existing benchmarks ( e.g., CodeSearchNet [8], or Hu-
manEval [9]) and metrics that have been adapted from the field
of natural language processing (NLP) such as accuracy, BLEU,
METEOR, and ROUGE, as well as more recent metrics further
tailored for code such as CodeBLEU [10]. However, recent
work has illustrated the limitations of benchmarks such as
HumanEval [11], and there has been growing criticism of au-
tomated metrics within the NLP community [12]–[15]. These
deficiencies largely stem from the fact that such benchmarks
and metrics are often targeted at evaluating functional or
syntactic correctness of generated code or task performance,
but are not able to explain model predictions or capabilities
in an interpretable manner.
Methods for evaluating andexplaining LLMs for code are
inextricably linked to one another. An informative evaluation
requires some degree of explainability of model predictions,
such that model behavior can be understood at a fine-grained
level. However, the fundamental challenge in achieving ex-
plainability of LLMs for code lies in establishing a reliable
mapping mechanism that can bridge the gap between a given
model’s predictions and human-understandable programmingarXiv:2308.03873v1  [cs.SE]  7 Aug 2023

--- PAGE 2 ---
Fig. 1: The evaluative and explainability method AST xplainer
is composed of AsC -Eval , AsC -Causal , and AsC -Viz.
language (PL) concepts that can aid in explaining the model’s
decisions. As such, designing both effective evaluations and
interpretability techniques for LLMs of code requires that one
first establish this conceptual mapping.
To overcome the challenges in explaining and evaluating
LLMs for code we propose a novel method for enabling
a reliable conceptual mapping of LLM predictions to PL
concepts, called AST xplainer , which collects and aggregates
LLM token predictions into a construct that we call Abstract
Syntax Concepts ( AsC), derived from Abstract Syntax Trees
(ASTs). By explicitly mapping model predictions to code
structure, AST xplainer provides a fine-grained methodology
for examining how models perform relative to programming
language concepts, and can help model end-users reason about
why an LLM may have made a certain set of predictions.
AST xplainer ’s mapping of model predictions to AsCs enables
two new types of evaluations for LLMs of code, and one novel
interpretability technique that visualizes model AsCs to aid end
users ( i.e.,developers using LLMs to auto-complete code) in
understanding LLM predictions. Fig. 1 illustrates these three
main components of AST xplainer .
The first evaluation technique, called AsC -Eval is able
to estimate the structural performance of a predicted syntax
element in order to measure the uncertainty of the downstream
code generative process (e.g., for code completion). The
second evaluation technique called AsC -Causal , is capable
of generating causal explanations that link these structural
performance values with canonical model performance (i.e.,
Cross-Entropy Loss). Finally, AsC -Vizimplements a practical
interpretability technique by visualizing model LLM predic-
tion uncertainty, organized into AST structures, aiding end-
users in understanding the reliability of model predictions in
practice. We evaluate AsC -Eval and AsC -Causal through a
large-scale, comprehensive empirical study that evaluates 12
popular LLMs on a novel dataset of ≈10 million tokens
that are exclusive of the model’s training data. Furthermore,
to evaluate the effectiveness of AsC -Viz, we conduct a user
study examining the utility of multiple visualizations in aiding
developers to understand and explaining model predictions.
The results of our empirical study lead to novel insights
regarding the performance of LLMs for code, and user study
illustrates the promising utility of AsC -Viz.
The contributions of this paper are as the following:
•Anevaluative metric based on Abstract Syntax Concepts
and Next-token Predictions (AsC -Eval ).•An explainability method (AsC -Causal ) that links canon-
ical evaluations with our Abstract Syntax Concepts to
provide insights into why the cross-entropy loss is being
affected by structural elements of code data.
•A user study that shows how AST visualizations (AsC -
Viz) help to understand generated code.
•A benchmark to evaluate Abstract Syntax Concepts in
LLMs, which includes a curated dataset ( Galeras ) of 50K
python samples.
•Experimental data, curated datasets, source code, and
complementary statistical analysis used in this re-
search are published in an open-source repository,
which is available at https://github.com/WM-SEMERU/
CodeSyntaxConcept.
II. B ACKGROUND & R ELATED WORK
AST xplainer is an evaluative and explainability approach
to quantify the prediction uncertainty of LLMs for code.
LLMs are the result of scaling up billions of parameters for
context-aware word representations from pre-trained models
[16]. This section defines and formalizes the basic elements
of our approach. We provide a definition of LLMs and how to
evaluate them, the definition of Abstract Syntax Trees (ASTs)
and how they were employed for probing, and finally, the
explainability methods for LLMs.
A. Large Language Models for Code
Our research focused on LLMs because of their outstanding
performance on code-based generative tasks. While other
representations exist, such as graph-based models [17], [18],
we focus our discussion on sequence-based representations for
simplicity. The goal of sequence-based models is to statisti-
cally learn a representation of a software artifact ( e.g., snippet,
comments, or test cases). We refer to SE-specific sequence-
based data as a software corpus S. Given the sequential nature
ofS, we can decompose Sinto a desired granularity of tokens,
words, or sub-words [19] by using a transformation function
Γ(S) = w1, ..., w I(i.e., tokenizers ). This transformation
function is a tokenization method for converting a software
corpus into a sequence of discrete objects wifor1⩽i⩽I.
Note that wi∈V, where the vocabulary Vis a finite set.
Given this definition, a statistical language model is a
probability distribution Pover a fixed granularity of sequences
of software corpora S. We can factorize the joint distribu-
tion over the i−dimension as: P(S) = P(w1, ..., w I) =QI
i=1P(wi|w<i). Due to the discrete nature of the data, the
expression P(wi|w<i)can be estimated using a classifier.
The classifier, in our particular case, is a LLM [20]. Hence,
rather than using n-grams or Markov Models to approxi-
mate P(wi|w<i)[21], it is convenient to use a latent model
P(wi|w<i)≈P(wi|hi), where hiis known as a hidden state
that embeds the sequence information from past observations
up to the time step i.
Depending on how the sequence is processed, the hid-
den state hican be computed using either Encoder-Only ,
Encoder-Decoder , or Decoder-Only architectures according
2

--- PAGE 3 ---
to the transformers’ layers [22]. One popular bidirectional
objective function used widely in representation learning is
masked language modeling [23]. This function aims to predict
masked text pieces based on the surrounding context. Code-
BERT [24], CuBERT (345M) [25] CodeRoBERTa [26], and
GraphCodeBERT [27] are examples of Encoder-Only models
for code. In programming contexts, these methods provide
useful representations of code sequences for downstream tasks
such as code classification, clone and defect detection. CodeT5
[28] and PLBART [4] are examples of Encoder-Decoder
models. These models encode an input sequence and, then,
this encoded sequence is decoded with a different architec-
ture. Encoder-Decoder models are trained with the goal of
reconstructing masked input sequences [29]. Additionally, they
have been employed for SE tasks such as code summarization,
and code generation using masks [28]. Finally, Decoder-Only
models predict the probability of a token given a preceding
sequence. CodeGPT [30], CodeParrot [31], GPT-Neo [32],
GPT-J [33], Codex [34], GPT-NeoX [35], and Google’s left-
to-right decoder-only Transformer language models [22], [36]
are examples of Decoder-Only models for code.
Although our proposed approach AST xplainer was designed
to be compatible with either type of LLMs, this paper con-
centrated on Decoder-Only models due to their popularity
for code-based generative tasks [37]. These models share a
common property: the ability to connect previously processed
information to a present task, such as using an initial sequence
of tokens to predict new code tokens . The resulting auto-
completed sequence should be coherent with respect to the
context of the initial sequence. This property is known as the
ability to model long-range dependencies [38].
Definition 1: Decoder-Only Transformers. Decoder-Only
models update the hidden state hi=f(hi−1, w<i)using past
inputs w<iand a previous hidden state hi−1. In other words,
these models function in a feed-forward manner that predicts
future values from historical values directly. LLMs trained on
source code have the ability to generate tokens or sub-words
given a history. Hence, decoder-only models are employed as
generative models ˆwi∽P(wi|w<i) =σ(y)i=eywi
Σjeyj.
In the previous approximation, the predicted token wiiscon-
ditioned by the previous information. The term yjrepresents
thenon-normalized log-probabilities for each output token j.
We extracted and normalized these log-probabilities from the
last layer of LLMs to estimate the Next-token Predictions
(NtP) in AST xplainer (see Sec.III). This estimation relies on
the softmax function. The softmax σireturns a distribution
over predicted output classes, in this case, the classes are each
token in the previously introduced vocabulary V. It is expected
that the predictions contained in σiare influenced by previous
inputs of the sequence w<i.
B. ASTs Probing Approaches
Probing is a supervised analysis to determine which type
of parameters ( e.g., input code snippets, tokenization process,
number of hidden layers, and model size) influence the learn-
ing process in machine learning models [39]. The purpose ofprobing is to assess whether hidden representations of machine
learning models ( i.e.,LLMs) encode specific linguistic prop-
erties such as syntactic structures of programming languages.
For example, Lopez et al. [40] trained a linear classifier to
show that code syntactic structures are encoded in pre-trained
models in the form of Abstract Syntax Trees (ASTs). Lopez
et al.’s approach demonstrates that the middle layers of pre-
trained models contain ASTs’ information [40].
Nonetheless, instead of proposing another syntax probe, our
approach AST xplainer adapts AST information to evaluate and
explain LLMs (see Sec. III). ASTs are defined as a formal
representation of syntactical structures built upon linguistic
elements of PLs. ASTs are formed according to the production
rules defined in Context Free Grammar (CFGs). More pre-
cisely, production rules are functions that combine terminal
and non-terminal nodes into statements. Terminal nodes are
symbols in the source code ( e.g., tokens in region 3of
Fig.5), while non-terminal nodes encapsulate more than one
terminal node to define the structure of a statement ( e.g., nodes
containing children in region 2of Fig. 5).
When designing our approach AST xplainer (see Sec.III),
we leveraged meaningful and interpretable information defined
in Context-Free Grammars ( CFGs ).CFGs are a set of rules
containing the syntax and structural information of a language
[41]. Ultimately CFGs define instructions that specify how
different tokens ( i.e.,Lexemes) are put together to form valid
statements in every programming language.
Definition 2: Context Free Grammars. CFGGis ex-
pressed as G= (α, λ, ω, β )where αdenotes the finite set
of non-terminal symbols, λthe finite set of terminal symbols,
ωthe finite set of production rules and βthe start symbol.
The set of production rules ωfor any type of statement ( e.g.,
conditional, assignation, operator) is expressed in terms of the
terminal and non-terminal symbols.
C. Explainability for Code Generation
LLMs for code can be considered a black box because
of their uncertain behavior when predicting tokens. To esti-
mate such uncertainty, we can employ explainability methods
on LLMs. Explainability aims to understand how a model
operates and comes to decisions either by exploring inner
layers or performing perturbation analysis on the models’
inputs [42], [43]. For example, Gholizadeh et al. [44] propose
a local explainability technique, namely layer-wise relevant
propagation (LRP), that computes the importance of an inter-
pretable n-gram in classifying a text sequence. LRP calculates
a score with the sum of activated weights during the back-
propagation to identify the most influential n-grams. This
score is employed for explaining the importance of a given
n-gram for a canonical ( i.e., SVM) and a neural model( i.e.,
CNN). The authors demonstrated that LRP outperforms the
gradient-only-based and permutation-only-based explainability
techniques [44]. It is important to clarify that, in our research,
explainability andinterpretability are used interchangeably.
In the context of pre-trained models for code, Liu et al.
experimented with Encoder-Decoder models for code2code
3

--- PAGE 4 ---
and comment2code tasks ( e.g., T5, CodeText, and CodeTrans).
Their research aims at explaining why neural models generate
code sequences reliably by identifying tokens that contribute
the most to a sequence prediction [15]. Moreover, Vasconcelos
et al. propose a technique that highlights generated code using
an uncertainty threshold. Their approach points out fragments
of the sequence where developers can intervene upon the
uncertainty threshold [45]. On the other hand, we can explain
pre-trained models for code using structural information. For
instance, Wan et al. conducted an interpretability analysis
on Encoder-only models ( e.g., CodeBert and GraphCodeBert)
focusing on three aspects: 1) how the self-attention weights
align with the syntax structure, 2) whether the syntax structure
is encoded in the hidden layers, and 3) how pre-trained models
induce syntax structure [14].
Even though previous research has introduced explainabil-
ity techniques to analyze pre-trained models with structural
information, those techniques have been tested and designed
for modest-size Encoder-Only models ( i.e., less than 1B).
Conversely, our study AST xplainer proposes not only an
explainability technique that contextualizes canonical metrics
(i.e.,cross-entropy loss) based on causal inference (see Fig.4)
but also an evaluative metric (AsC -Eval ) for Decoder-only
LLMs that predicts ASTs terminal and non-terminal nodes.
More importantly, we introduce and control a set of con-
founders based on code features ( e.g., AST-levels, AST-nodes,
and number of tokens) to properly estimate the relationship
between AsC -Eval and canonical metrics (see Tab. II).
Kim et al. [13] introduce a formal mathematical structure
known as a function for explainability (φ). We use this
definition to formally describe what constitutes an explainable
method in SE. Most LLMs for code operate by predicting
tokens P(wi|di)that do not inherently match high-level con-
cepts a human can easily understand. Kim et al. claim that such
difficulty can be expressed mathematically as representing the
state of LLMs as a vector space ( ⃗ m). Conversely, humans or,
in our study, developers operate in a different vector space ⃗h,
which corresponds to an unknown set of human-interpretable
concepts (h). As such, our main challenge is to map ⃗ m→⃗h
bridging this gap between the disparate vector spaces. The key
insight of AST xplainer is the formalization of an explainability
function φfor LLMs of code.
Definition 3: Interpretability Function for Next Token
Predictions. Consider φ:⃗ m→⃗h. In this formulation, ⃗ m
represents an approximation of a model’s vector space as mea-
sured through token prediction performance at different gran-
ularity levels ( i.e., normalized log-probabilities). This vector
space approximation is then mapped to human-understandable
concepts ⃗hthat represent programming language syntactic
concepts ( i.e.,terminal and non-terminal nodes).
III. T HEASC-Eval COMPONENT
While LLMs have seen striking advances with regard to
code generation and other downstream SE tasks [46], [47],
researchers are still not able to evaluate what aspects of
code are actually statistically learned by these models. In thissection, we propose a new metric, AsC -Eval , to showcase
the statistical behavior of syntactic elements generated by
LLMs. Our proposed AsC -Eval comprises the basic units for
explainability (see Fig. 2) as Abstract Syntax Concepts ( AsC),
an alignment function δthat links tokens with ASTs, and an
aggregation function θthat estimates the prediction perfor-
mance of a terminal and non-terminal nodes. We propose an
explainability function φthat relies on the alignment function
δand the aggregation function θto perform the mapping
from log-probabilites ( i.e., NtP ) to developer-understandable
concepts ( i.e., AsC ).
A. Abstract Syntax Concepts (AsC)
AsC-Eval can be formally defined (see Def. 3) as an
explainability function φof token predictions of LLMs using
Context Free Grammars. We introduce the term Abstract
Syntax Concepts (AsC) to represent the terminal and non-
terminal symbols in a Context Free Grammar (see Def .2).
Specifically, to approximate a LLMs’ vector space, in ⃗ m, we
extract the last layer to calculate NtP, which is, in fact, a
generative measure of performance. Then in ⃗h, we map the
model’s prediction performance at the token level ( NtP) to
AsC (for which we define a set of categories H), to make
it easier to interpret what aspects of LLMs are effective or
erroneous at predicting.
In PLs, terminal and non-terminal nodes retain differ-
ent semantic meanings. For instance, ‘identifier’ and
‘string’ nodes correspond to a common Natural Language
concept category. As such, we can group nodes ninto se-
mantically meaningful categories H. Fig. 3 depicts some of
our proposed categories for Python. These categories will
allow AsC -Eval to assign semantic meaning to predicted AsC.
AsC are the fundamental mathematical units for enabling
the evaluation and explainability of LLMs. Figure 3 depicts
some of the concepts used to evaluate LLMs with AsC -Eval .
Concepts n∈Nare types of symbols defined by tree-
sitter’s CFG [48]. In summary, Each token in a sequence s
can be assigned to a category h∈ H . With our categories
H, researchers and developers can easily associate LLMs’
performance to particular structural code attributes. As such,
AsC-Eval allows for LLMs Next-token Predictions to be
explained in a developer-centric way.
Fig 2-A depicts the AST representation of a Python snippet
of a naive implementation of the function countCharts . This
function counts and returns the number of occurrences of a
given character for an input string. In the AST representation,
the leaf nodes correspond to the terminal tokens used in
the snippet, while the intermediate nodes correspond to non-
terminals. Our approach relies on the tree-sitter library [48]
to construct the AST representations of the snippets. Once
the AST has been parsed, we can access the information for
all nodes and retrieve useful properties such as their type,
children, and location.
4

--- PAGE 5 ---
Fig. 2: AsC -Eval Components. Left: Nodes are employed as “concepts”. Center: Each token is aligned to the end nodes of the
AST with an offset function. Right: Node probabilities are estimated with an aggregation function.
B. AST Alignment function ( δ)
Figure 2-B illustrates the process of aligning terminal and
non-terminal nodes in the AST representation with their corre-
sponding tokens. Prior to this alignment process, we split the
countCharts snippet sinto tokens using the model tokenizer
Γ(s) = ( w1, ..., w i). Since the tokenizer may produce a
sequence of tokens where each token does not necessarily
matches with a single terminal node, a single node in the
AST may contain more than one associated token. In fact,
intermediate nodes are aligned with a sub-sequence of the
original snippet rather than a single token. We define for this
purpose the alignment function δ:N→s<=iwhere s<=i
corresponds to a subsequence of a snippet and Nis the set
of terminal and non-terminal nodes. We leverage the offset
property of each AST node to conduct this process, in other
words, we search for all the tokens in sthat are located within
the offset range of each node. To illustrate how function δ
works, let’s consider the example in Figure 2-B, in the sub-
tree the terminal node ‘(’ is aligned with token (while
the sibling node ‘identifier’ is aligned with tokens str
ing. The parent node ‘parameters’ will be consequently
aligned with (str ing ,char acter ).
C. AST Aggregation function ( θ)
We design an aggregation function θthat computes our
proposed metric AsC -Eval , which represents how confident
a terminal or non-terminal node nis predicted by an LLM.
By relating these node predictions to an actual node symbol,
we gain an understanding of how well a studied model is
generating code . These AsC -Eval performance values can also
uncover specific long-range interactions and map them into an
AST visual structure (see Sec. V). AsC -Eval performs at two
levels of granularity depending on the scope of the analyzed
corpus S. We refer to such granularity as local and global
aggregation. Local aggregations operate for a code snippet,
while global aggregations operate for a corpus. Although local
aggregation can provide a AsC -Eval value for a single snippet,
this aggregation allows computing an average of aggregated
values at snippet granularity.
Figure 2-C shows the aggregation function used to compute
the prediction probability for each node. Once the tokens are
aligned with their corresponding nodes using δ, we traverse
the entire AST and aggregate the NtP probabilities of their
associated tokens. The aggregation function θcan take theform of a statistical average, median or max values depending
on the user configuration. In our study, we set the aggregation
θ:N→median (δ(N))for a subset of tokens s<=i.
For example, as illustrated in Fig. 2-C, the parent node
‘parameters’ has an associated average value of 0.23.
This parent node average was aggregated with its terminal
values: ‘(’ with0.07,‘identifier’ with0.4,‘,’ with
0.5,‘identifier’ with0.1, and ‘)’ with0.1.
IV. T HEASC-Causal COMPONENT
In this section, we show how AsC -Causal component can be
used to explain and contextualize other canonical metrics such
as the cross-entropy loss. To achieve that, we propose a causal
inference technique to estimate the impact of Abstract Syntax
Concepts ( AsC) predictions on overall LLM performance.
LLMs are more understandable when they reflect human
knowledge [13]. One way of determining whether an LLM
for code reflects human knowledge is testing it to see whether
or not it operates similar to how a developer would estimate
the prediction of a sequence [49]. For instance, consider
the situation where a developer inserts a ‘for statement’
in a snippet. Inherently, a developer mentally rationalizes
several things such as the concept of Iteration . If an LLM
is able to make a similar prediction, it suggests to us that
it has statistically learned some understanding of the syntax
structure of a programming cycle. We can consider that this
statistical behavior impacts the cross-entropy loss. This impact
indicates that Abstract Syntax Concepts ( AsC) are influencing
the quality of a LLM (see Def. 4). In order to estimate such
influence, we propose a causal inference technique based on
the do-calculus analysis [50]–[52]. For instance, in Eq. 1, we
compute a causal effect (Eq. 1a) and correlation (Eq. 1a) for
the concept treatment ‘for_statement’ impacting the cross-
entropy loss of a given LLM.
p(Y|do(t=forState. )) =X
z∈codeFeat.p(Y|z, t)p(t) (1a)
p(Y|t=forState. ) =X
z∈codeFeat.p(Y|z, t)p(t|z)(1b)
We can explain the prediction performance of LLMs us-
ing AsC -Eval values as treatment effects. These effects are
computed from a Structural Causal Model (SCM), which
5

--- PAGE 6 ---
Fig. 3: AsC -Eval for 10 AsC Categories and 2 LLMs ( mono-
lang [2B] andgpt-3 [125M] )
represents our assumptions about the underlying causal pro-
cess. In our study, these assumptions take the form of the
performance of each AsC (treatments T), code features (con-
founders Z), and the LLMs canonical performance (outcome
Y). The relationship or directionality information of these
causal variables is explicitly stated in the SCM (see Fig. 4).
The goal of the causal analysis is to determine the Average
Treatment Effect (ATE) that a treatment has on the outcomes
after controlling the confounding variables. In other words,
we want to estimate the probability p(Y|do(T))(see Eq. 1a)
to identify cases of spurious correlations (i.e.,association is
not causation) [53]. Note that the probability p(Y|do(T))is
different from p(Y|T)in Eq. 1b. We state that the probability
p(Y|T)represents the correlation between the variables Yand
Twithout controlling any confounder’s effects on treatments
or outcomes. In our study, we compute the Pearson correlation
ρ=p(Y|T). Conversely, the treatment effect p(Y|do(T))is
estimated with a liner regression after applying the the back-
Fig. 4: Structural Causal Model to estimate the Average
Treatment Effect of AsC to LLM Performance by controlling
code features (AsC -Causal ).
door criterion for controlling confounders [53].
Definition 4: AsC Causal Treatment Effects. Given a
Structural Causal Model where a set of variables PAdenotes
the parents ( i.e.,code features) of T, the treatment effect of T
(i.e., AsC ) on Y ( i.e.,cross-entropy loss) is given by
p(Y=y|do(T=t)) = (2a)
Σzp(Y=y|T=t, PA =z)p(PA=z) = (2b)
Σzp(T=t, Y=y, PA =z)/p(T=t|PA=z) (2c)
Based on the causal inference definition by Pearl et al. [50],
we propose a specific treatment effect for our Abstract Syntax
Concept AsC. Def. 2 depicts the statistical marginalization of
confounders. In simple terms, the Average Treatment Effect
comprises the pure impact of the treatment in the outcome
without the influence of confounding variables. These effects
represent the slope of the linear model obtained between the
treatment and the output after controlling for confounding. In
our study, we controlled for confounders such as sequence
size,number of nodes ,number of tree levels , and cyclomatic
complexity .
V. T HEASC-VizCOMPONENT
The visualization component AsC -Viz is a graphical ex-
plainability technique that displays the AsC -Eval performance
values of the terminal and non-terminal nodes for a single local
evaluation. We take advantage of the hierarchical structure
of PLs to visually accommodate AsC -Eval values obtained
in AsC -Eval into the AST. Fig. 5 illustrates how we ac-
commodate the AsC -Eval values for a code generation task
using our analyzed gpt-3 [1.3B] model. Region 1shows
a box with a prompt with an incomplete snippet followed
by a second box with generated tokens in blue. Then, in
region 2, the resulting auto-completed snippet is processed
with AsC -Eval and represented as an AST. Each node has
information about the AsC -Eval performance after applying
local aggregations θ. The nodes are color-coded. The highest
aggregated values ( i.e., best predictions) are displayed in
shades of blue. In contrast, nodes with the smallest values
(i.e.,worst predictions) are displayed in shades of red. Nodes,
6

--- PAGE 7 ---
in region 2, encapsulate the code tokens generated by the
LLM as presented in region 3. We refer to tokens linearly
organized as sequence representation .
VI. E XPERIMENTAL DESIGN
In order to illustrate the insights that AST xplainer can
enable, we present an empirical evaluation on 12 LLMs, which
shows how LLMs behave for each Abstract Syntax Concept,
and a user study, which assesses the usability of our approach.
This section details the methodological steps we followed to
configure, evaluate, and explain our selected LLMs.
RQ1AsC-Eval :To what extent do Large Language Models
for code predict syntactic structures?
RQ2AsC-Causal :How do Abstract Syntax Concepts impact
LLMs’ canonical prediction performance?
RQ3AsC-Viz:How useful is our AST evaluation method for
developers in a practical scenario?
A. Study Setup
Data Collection. Our selected LLMs were trained on Big-
Query [54], BigPython [55], and the Pile [56]. These datasets
include repositories and files from GitHub created before 2021.
However, in order to properly evaluate AsC -Eval , we must
avoid data contamination. That is, we need to avoid using
samples in the evaluation that LLMs have already been used
for training. For the same reason, we cannot evaluate our
approach using popular code datasets such as CodesearchNet
[8] or CodeXglue [57]. To solve this data contamination issue,
we collected 50k unique Python snippets and created a brand
new code dataset, called Galeras .Galeras contains only recent
commits performed from January 1st, 2022 to January 1st,
2023. We collected Python repositories from Github that have
more than one hundred stars and extracted snippets of code
from new and updated Python methods. We cleaned sample
duplicates using the commits’ history. Additionally, Galeras
includes information about the commit message, comments
on the method, the whole AST data structure of the method,
number of nodes, AST levels, AST errors, white spaces, lines
of code, cyclomatic complexity, and token counts.
Model Collection. We evaluated and explained a total of
12 open Decoder-Only LLMs filtered by popularity. Our
largest model has 2.7B parameters. Table I shows the LLMs
grouped into four different categories that correspond with the
fine-tuning strategy employed. The first category consists of
GPT-3-based models trained mostly on natural language ( i.e.,
Pile [56]). The second category includes models trained on
natural language but built upon the codegen architecture [58].
The third category consists of models trained on multiple
programming languages using BigQuery [54] on both gpt-2
and codegen architectures. The last category corresponds to
both Multi-Language-Type models fine-tuned on BigPython
[58], which we refer to as Mono-Language-Type , and gpt-2
models ( i.e.,codeparrot [31]).
Machine Configuration. We performed the experiments us-
ing 20.04 Ubuntu with an AMD EPYC 7532 32-Core CPU,
A100 NVIDA GPU with 40GB VRAM, and 1TB RAM. Forthe model inference process, we used HugginFace and Pytorch
[59], [60]. All models were loaded into the GPU of the
machine to boost the inference time.
B.RQ1The AsC-Eval Empirical Methodology
To answer RQ1, we generated the normalized log-
probabilities (see Sec.II) or Next Token Predictions ( NtP) for
each code snippet in S=Galeras . These log-probabilities were
extracted at inference time for each token position for the 12
LLMs. The log-probabilities distributions have a vector size of
|V|for each token position in s∈ S. These distributions are
processed to obtain the log-probability that actually matches
the expected token in a position i. Therefore, each token
position has an associated prediction value that we save for
generating the NtP sequence. Such Next-token Prediction
sequence is the input for the aggregation function θthat
generates the corresponding AsC -Eval values (see Sec. III).
Additionally, we computed the cross-entropy loss of each
snippet sin our dataset. To obtain the AsC -Eval Global value
in Tab. I and Fig. 6, we aggregated AsC -Eval performance
values ( i.e.,all available AsC) by LLM. The values per model
are bootstrapped with the median (size of 500 samplings)
to enable a fair comparison among models. Similarly, to
obtain the AsC -Eval per Abstract Syntax Concept Category
(e.g., Data Str, Decision, or Scope), we globally aggregated
performance values of tokens under these categories. We also
explored with Type Model aggregations (see Table. I).
C.RQ2The AsC-Causal Empirical Methodology
To answer RQ2, we compute both Pearson correlation ρ
values and causal treatment effects (see Def .4) for a subset
of 14 syntactic concepts (see Fig. 4). Specifically, we propose
the treatments ( T)Scope ,Exceptions ,Operator ,Decision ,
Data Structures ,Functional Programming ,Natural Language ,
Iterative ,Types , and Testing . Each AsC was correlated to
4 confounding variables ( i.e., Cyclo, AST Levels, #AST
Nodes, and Sequence Size) and the cross-entropy loss of
gpt-3 [125M] and mono-lang [2B] . We decided to explore
only edge case LLMs ( i.e., the best and worst models by
AsC-Eval performance) since we detected that the correlated
values were very similar across LLMs. On the other hand, we
estimated the probability of the treatment effect p(Y|do(T)
for each AsC and the cross-entropy loss by controlling the 4
previously mentioned confounders. This probability function
was estimated using the doWhy tool [53]. Table II summarizes
the treatment effects and correlations between the AsC -Eval
values locally aggregated (see Sec. III) and the cross-entropy
loss grouped by concept categories ( H).
D.RQ3Qualitative User-Study Methodology
To answer RQ3, we designed four surveys to understand the
perception of software practitioners in regard to the usability of
AsC-Eval and AsC -Viz. Our goal is to assess the effectiveness
of our AsC -Eval and AsC -Vizapproaches to explain why and
how certain source code tokens are predicted by LLMs trained
on code. Leveraging interpretability techniques to explain
7

--- PAGE 8 ---
Fig. 5: Local Evaluation for Code Completion (AsC -Viz).
TABLE I: Large Language Models characteristics and their associated AsC -Eval performance. Erroneous AsC -Eval values are
in red. Confident AsC -Eval values are in blue. Best global AsC -Eval is underlined.
Large Language Models (LLMs) AsC-Eval Performance (bootstrapped median)
Type Name Architecture Size Global Data Str. Decision Except. F. Prog. Iter. NL Oper. Scope Testing Types
gpt-neo-125m [32] gpt-3 125M 0.48 0.50 0.52 0.43 0.49 0.74 0.32 0.48 0.51 0.59 0.33
gpt-neo-1.3B [32] gpt-3 1.3B 0.59 0.60 0.61 0.53 0.62 0.79 0.43 0.57 0.68 0.68 0.44Natural L.
gpt-3gpt-neo-2.7B [32] gpt-3 2.7B 0.62 0.62 0.63 0.56 0.66 0.81 0.46 0.60 0.74 0.70 0.47
codegen-350M-nl [61] codegen 350M 0.55 0.56 0.57 0.45 0.57 0.77 0.39 0.54 0.64 0.64 0.40 Natural L.
codegen codegen-2B-nl [61] codegen 2B 0.65 0.65 0.65 0.58 0.68 0.82 0.48 0.61 0.78 0.72 0.50
codeparrot-small-multi [31] gpt-2 110M 0.57 0.54 0.55 0.64 0.60 0.60 0.40 0.54 0.71 0.67 0.42
codegen-350M-multi [61] codegen-350M-nl 350M 0.68 0.63 0.72 0.75 0.70 0.69 0.51 0.62 0.83 0.73 0.51Multi-
Languagecodegen-2B-multi [61] codegen-2B-nl 2B 0.79 0.74 0.79 0.83 0.81 0.77 0.65 0.74 0.91 0.80 0.71
codeparrot-small [31] gpt-2 110M 0.61 0.58 0.58 0.68 0.66 0.63 0.46 0.57 0.73 0.69 0.47
codeparrot [31] gpt-2 1.5B 0.71 0.67 0.67 0.80 0.76 0.70 0.59 0.66 0.82 0.74 0.64
codegen-350M-mono [61] codegen-350M-multi 350M 0.73 0.68 0.76 0.78 0.76 0.73 0.57 0.68 0.86 0.77 0.58Mono-
Language
codegen-2B-mono [61] codegen-2B-multi 2B 0.84 0.79 0.84 0.90 0.85 0.81 0.73 0.82 0.94 0.85 0.83
the decisions of such models can give software practitioners
insights into the behavior and the quality of the predictions.
We introduced a set of code exemplars with their cor-
responding AsC -Viz explanation. We asked the participants
to rate the explanations for four Python samples distributed
across treatments. We use a within-subjects design or repeated
measures design, in which every individual receives each of
the experimental treatments consecutively. Table III contains a
summary with the description of each survey. Each individual
survey has two sections. The first section of each survey
intends to gauge the proficiency of participants using Python
and their familiarity with language models for code generation
tasks. Participants were also asked about their knowledge of
representation of algorithms (AST) and the major problems
that they have faced when using LLMs for source code
generation.
In the second section, we provide four Python prompts
with an incomplete method along with the prediction of the
missing lines given by an LLM. Since our goal is to evaluate
the usability of AST xplainer rather than the perception of
the participant in regards to the model performance, we omitdetails about the model used for predictions ( gpt-3 [1.3B] ).
Each prompt is accompanied by a visualization ( i.e., AST-
partial, AST-complete, and sequence) that shows the AsC -Eval
and NtP values for the predicted tokens. Then we ask the
participant to assess the visualization and rate its usefulness.
The visualizations are separated into different surveys.
Figure 5 poses an example of a local evaluation of a code
completion task that was presented to the participants. The
survey asks to evaluate four different samples processed with
AsC-Eval . Some of the samples have syntactic errors. Each
survey comprises an incomplete Python method ( i.e.,prompt)
and a complete method with the highlighted portion of the
generated code (region 1in Fig. 5). For each sample in the
surveys, we presented a specific visualization. For instance,
surveys ( S2) and ( S3) contain an AST-based representation
similar to the one in Fig. 5. The AST-complete visualization
S3shows the terminal and non-terminal nodes (region 2in
Fig. 5). Nonetheless, the AST-partial visualization ( S2) only
shows the non-terminal nodes. Finally, a sequence-based visu-
alization of the generated logits for each token was presented
to the participants in the survey ( S1) (region 3in Fig. 5).
8

--- PAGE 9 ---
VII. R ESULTS & D ISCUSSION
A.RQ1Empirical AsC Performance Evaluation
In this RQ, we provide an empirical value (bootstrapped
median columns in Tab. I) of the prediction of Abstract Syntax
Concepts for the 12 LLMs. We set a threshold of 0.6as
an acceptable rate of prediction confidence for our AsC -
Eval metric. Fig. 3, for example, shows our best and worst
LLMs, mono-lang [2B] and gpt-3 [125M] respectively, at
every proposed Abstract Syntax Concept. We observe that, in
general, scaling the parameters of LLMs plays a fundamental
role in the prediction of AsC. The dashed green boxes show the
largest AsC -Eval performance increments from the worst to
the best concepts. Particularly, Exceptions ,Natural Language ,
Operators ,Types , and Decisions present the biggest jumps in
syntactic AsC -Eval performance.
Our empirical evaluation shows that AsC categories that
fulfill the 0.6threshold for the 12 LLMs are Scope with
the highest AsC -Eval performance of 0.94for the Mono-
Language-Type models, Iterations with 0.82forcodegen-nl
[2B] , and Testing with0.85formono-lang [2B] (see Table.I).
Conversely, we found some concept categories struggle with
AsC-Eval performance. We refer to these categories as erro-
neous since they are below 0.5. Those categories are mainly
Natural Language category with the largest average median of
0.46andData Types with the largest average median of 0.47
forNL GPT-3 .
0.2 0.4 0.6 0.8 1.0
AsC Eval performancegpt-3 [125M]
gpt-3 [1.3B]
gpt-3 [2.7B]
codegen-nl [350M]
codegen-nl [2B]
multi-lang [110M]
multi-lang [350M]
multi-lang [2B]
mono-lang [110M]
mono-lang [1.5B]
mono-lang [350M]
mono-lang [2B]LLMs
Fig. 6: AsC -Eval Performance grouped by specific LLMs and
AsC-Eval density by Model Type.
We believe that models poorly behave with low AsC -
Eval performance because category concepts such as Natural
Language andData Types require more context to be accu-
rately predicted. For instance, the ‘string’ concept requires
a larger window context before properly being predicted.
Similarly, the category Data Types is prone to be erroneous
since they may appear more frequently at the beginning of the
snippets compared to other categories. Also, bear in mind that
Data Types are less frequent concepts due to the dynamic typ-
ing for Python. In general, none of the evaluated architectures
performed well at predicting Data Types accurately except by
mono-lang [2B] , which was trained with a large number of
code samples.Table I depicts that Iteration category mostly surpasses the
threshold for all our models except for codeparrot-small-multi
with an average median AsC -Eval of0.6. From our smaller
models ( i.e.,in a range of millions of parameters), the lowest
average median obtained for gpt-3 [125M] is0.74, which also
surpasses the threshold. This outstanding behavior of NL GPT-
3models could be explained as Python reserved words for
iterations such as forandwhile also appear in natural language
with similar semantics.
Fig. 6 indicates that models trained on natural language
have more median variability than models fine-tuned on code
datasets. For instance, NL GPT-3 and NL Codegen report
values in a range from 0.2to0.9. Conversely, fine-tuned
models with code such as Mono-Language-Type has a lower
variability than NL GPT-3 and NL Codegen categories. For
example, mono-lang [2B] has a global avg. median AsC -
Eval of0.84and a variability range between 0.7and1.0,
outperforming the 0.6threshold. Furthermore, mono-lang [2B]
is our best model with an average global AsC -Eval of0.84.
On one hand, this suggests that fine-tuned models on code are
predicting AsC with higher confidence than natural language-
only models. On the other hand, although Multi-Language-
Type models exhibit high variability (from 0.5to0.9), their
average median AsC -Eval (i.e.,0.68formulti-lang [110M] )
is even better than natural language models ( i.e.,0.48with
variability from 0.2to0.8forgpt-3 [125M] ).
RQ1AsC-Eval: The prediction of syntactic structures highly
depends on LLMs’ parameter size and fine-tuning strategy.
More specifically our largest evaluated model mono-lang
[2B] , which was fine-tuned with the BigPython and BigQuery
datasets, obtains the highest global average AsC Performace
of0.84with the lowest variability.
B.RQ2Empirical Causal Evaluation
In this research question, we want to quantitatively demon-
strate that cross-entropy loss of LLMs tends to be negatively
impacted by the AsC -Eval values at snippet granularity. There-
fore, we can explain in lower granularity which parts of the
code LLM perform poorly (see red boxes in Tab. I). We
showcase empirical evidence that the previous statement holds
for both correlation ρand causal effect p(y|do(t))values. For
example, Table II shows that, in general, all Abstract Syntax
Concept Categories ( i.e.,Global Avg. AsC -Eval ) influence the
cross-entropy loss for our best ( i.e., mono-lang [2B] ) and worst
(i.e., gpt-3 [125M] ) models, with an average treatment effect
of1.78and−1.60respectively.
The most outstanding finding is that the Natural Language
category has the largest impact on the cross-entropy loss. For
example, the AsC concept ‘identifier’ has a causal effect
of−1.78forgpt-3 [125M] and−2.89formono-lang [2B] .
In contrast, Functional Programming categories present the
lowest impact on cross-entropy loss with a subtle ‘lambda’
positive causal effect of 0.2forgpt-3 [125M] . This subtle
positive effect was expected as NL-based LLMs have not been
fine-tuned on code corpora with ‘lambda’ expressions. In
9

--- PAGE 10 ---
addition, we want to highlight the moderate Pearson corre-
lation value between the ‘if statement’ concept and the
cyclomatic complexity for our best and worst models with the
same value of ρ= 0.58. This observation is consistent with
the definition of the cyclomatic complexity metric since this
metric takes into consideration the control flows induced by
conditional structures.
RQ2AsC-Causal: We can observe that cross-entropy loss
of LLMs tends to be negatively impacted by the AsC -Eval
values at snippet granularity. For instance, identifiers affect
both gpt-3 [2.7B] andmono-lang [2B] cross-entropy loss with
Average Treatment Effects of −1.78and−2.89respectively.
C.RQ3User-Study on AsC Visualization Results
In this RQ, we evaluate our AsC -Eval and AsC -Viz in
a practical scenario. We assessed how useful is the AST
visualization for practitioners after collecting a total of 40
responses (with sampling confidence of 90% and a margin
error of 15%). Each participant was randomly and uniformly
assigned to either the control or one of the survey groups.
Our user study (see Tab. III) revealed that a partial AST
visualization ( i.e., AST with only non-terminal nodes) is
preferred over the complete AST representation ( S3) and
sequence visualization ( S1). Furthermore, AST partial visu-
alization ( S2) is found to be particularly less complex by
participants with a proportion of 43% in the survey compared
to the control C. However, AST complete visualization ( S3)
is perceived to be more complex by participants with a
proportion of 67% compared to the control. However, we
observed that 43% of participants would like to use the
sequential visualization rather than not having any visual help
(i.e.,control group C) and only 29% of participants considered
sequential visualization easy to read and use. Surprisingly,
57% of participants found the sequence visualization to be
highly useful in explaining the behavior of the model. This
preference can be explained by considering the fact that only
42% of the participants have formal education in ML.
In all the surveys, participants agreed that visualizing Error
nodes in AsC -Viz with their corresponding AsC -Eval value
was very useful to understand the long-range dependencies of
the generative process. The participants highlight the fact that
AsC-Vizis useful to explain why the LLM generates an erro-
neous prediction. More specifically, the participants mentioned
that the color code for the AsC -Eval values helps them to
differentiate between predicted probabilities. The participants
also indicated that they would like the visualizations to include
information about the workflow of the program.
RQ3Usability of AsC-Eval: We found that the partial visu-
alization of the AST is the most readable representation to
showcase local aggregated predictions with 57% agreement.
Although AST partial visualization has mixed opinions about
itsusefulness with an agreement rate of 29%, the AST
complete visualization has an agreement rate of 50%.VIII. C ONCLUSION & F UTURE WORK
Our research proposes an Abstract Syntax Concept approach
to evaluate and explain the performance of Large Language
Models for code. We conducted a rigorous empirical eval-
uation on 12 popular LLMs using a curated dataset that
uncovered novel performance qualities of LLMs for code. Our
empirical evaluation revealed that mono-language outperforms
multi-language LLMs at predicting all types of syntax con-
cepts. This suggests the importance of fine-tuning strategies
over model size. In addition, we demonstrated that Abstract
Syntax Concepts influence the cross-entropy loss of LLMs
after controlling for code confounders. In fact, this influence
persists across models at different parameter sizes and fine-
tuning strategies. Additionally, we illustrated the utility of
visualizations built upon our defined Abstract Syntax Con-
cepts in a user study. We believe these results illustrate the
promise of blending explainability and evaluation techniques
for LLMs of code, and signal the potential for future work
to further integrate explainability techniques into future LLM
benchmarks.
REFERENCES
[1] V . Raychev, M. T. Vechev, and E. Yahav, “Code completion with
statistical language models,” Proceedings of the 35th ACM SIGPLAN
Conference on Programming Language Design and Implementation ,
2014.
[2] M. Ciniselli, N. Cooper, L. Pascarella et al. , “An empirical study
on the usage of BERT models for code completion,” CoRR , vol.
abs/2103.07115, 2021. [Online]. Available: https://arxiv.org/abs/2103.
07115
[3] Z. Chen, S. J. Kommrusch, M. Tufano et al. , “Sequencer: Sequence-
to-sequence learning for end-to-end program repair,” IEEE Transactions
on Software Engineering , pp. 1–1, 2019.
[4] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified
pre-training for program understanding and generation.” [Online].
Available: http://arxiv.org/abs/2103.06333
[5] C. Watson, M. Tufano, K. Moran et al. , “On learning meaningful
assert statements for unit test cases,” in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering , ser. ICSE ’20.
New York, NY , USA: Association for Computing Machinery, 2020,
p. 1398–1409. [Online]. Available: https://doi.org/10.1145/3377811.
3380429
[6] GitHub, “Github copilot · your ai pair programmer.” [Online]. Available:
https://copilot.github.com/
[7] Replit, “Replit Ghostwriter,” https://replit.com/site/ghostwriter, May
2023.
[8] H. Husain, H.-H. Wu, T. Gazit et al. , “CodeSearchNet chal-
lenge: Evaluating the state of semantic code search,” arXiv preprint
arXiv:1909.09436 , 2019.
[9] M. Chen, J. Tworek, H. Jun et al. , “Evaluating Large Language
Models Trained on Code,” Jul. 2021, arXiv:2107.03374 [cs]. [Online].
Available: http://arxiv.org/abs/2107.03374
[10] S. Ren, D. Guo, S. Lu et al. , “CodeBLEU: a Method for Automatic
Evaluation of Code Synthesis,” vol. 1949, no. Weaver 1955, 2020,
arXiv: 2009.10297. [Online]. Available: http://arxiv.org/abs/2009.10297
[11] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is your code generated by
chatgpt really correct? rigorous evaluation of large language models for
code generation,” 2023.
[12] C. Molnar, Interpretable Machine Learning , 2019, https://christophm.
github.io/interpretable-ml-book/.
[13] B. Kim, M. Wattenberg, J. Gilmer et al. , “Interpretability beyond
feature attribution: Quantitative Testing with Concept Activation Vectors
(TCA V),” 35th International Conference on Machine Learning, ICML
2018 , vol. 6, pp. 4186–4195, 2018.
10

--- PAGE 11 ---
TABLE II: AsC -Causal : Correlations and Causal Effects.
Variable Cyclo Complexity AST Levels # AST Nodes Sequence Size Cross-Entropy Loss
LLMs gpt-125 mono-2B gpt-125 mono-2B gpt-125 mono-2B gpt-125 mono-2B gpt-125 mono-2B
Category H AsC ρ=p(Y|T) ρ Causal Eff. ρ Causal Eff.
Global Avg. AsC-Eval 0.61 0.60 0.41 0.45 0.51 0.47 0.53 0.48 -0.38 -1.60 -0.38 -1.78
forstatement 0.32 0.32 0.30 0.30 0.18 0.19 0.23 0.19 -0.16 -0.10 -0.07 -0.01Iterativewhile statement 0.13 0.13 0.08 0.07 0.05 0.05 0.07 0.05 -0.05 -0.11 -0.03 -0.08
identifier 0.10 0.12 0.10 0.20 0.51 0.46 0.49 0.49 -0.56 -1.78 -0.80 -2.89 Natural
Language string 0.02 0.04 0.20 0.24 0.36 0.30 0.36 0.30 -0.31 -0.36 -0.43 -0.55
] 0.16 0.14 0.21 0.28 0.35 0.36 0.29 0.32 -0.16 -0.04 -0.22 -0.10 Scope
) 0.09 0.03 0.07 0.12 0.37 0.23 0.31 0.23 -0.37 -0.85 -0.54 -1.49
Decision ifstatement 0.58 0.58 0.29 0.28 0.20 0.21 0.27 0.23 -0.22 -0.21 -0.11 -0.11
comparison operator 0.34 0.38 0.18 0.20 0.30 0.30 0.30 0.32 -0.13 0.02 -0.11 0.00Operatorboolean operator 0.53 0.54 0.20 0.21 0.17 0.18 0.22 0.20 -0.10 0.01 -0.08 -0.09
forinclause 0.28 0.29 0.18 0.18 0.15 0.14 0.13 0.13 -0.03 0.09 -0.03 0.04
ifclause 0.20 0.22 0.10 0.10 0.06 0.06 0.07 0.06 -0.01 0.19 0.01 0.13Functional
Programminglambda -0.01 -0.01 0.12 0.12 0.18 0.18 0.16 0.16 -0.04 0.20 -0.05 0.06
TABLE III: User study visualization results
ID Group Visualization User study visualization results
C ControlPrompt
predicted tokens.Agreement%
useful%
readable%
complex
S1SequencePrompt
predicted tokens
NTP.Strongly 14 0 0
Agree 43 29 43
Neutral 29 29 29
Disagree 14 43 29
S2AST
partialPrompt
predicted tokens
partial AST
NTP.Strongly 14 14 0
Agree 29 43 14
Neutral 29 29 43
Disagree 29 14* 43
S3AST
completePrompt
predicted tokens
complete AST
NTP.Strongly 0 0 17
Agree 50 17 50
Neutral 33 33 17
Disagree 17 50 17
The * depicts the only “Strongly Disagree” reported in the survey
[14] Y . Wan, W. Zhao, H. Zhang et al. , “What Do They Capture?
– A Structural Analysis of Pre-Trained Language Models for
Source Code,” Feb. 2022, arXiv:2202.06840 [cs]. [Online]. Available:
http://arxiv.org/abs/2202.06840
[15] Y . Liu, C. Tantithamthavorn, Y . Liu, and L. Li, “On the Reliability
and Explainability of Automated Code Generation Approaches,” Feb.
2023, arXiv:2302.09587 [cs]. [Online]. Available: http://arxiv.org/abs/
2302.09587
[16] W. X. Zhao, K. Zhou, J. Li et al. , “A Survey of Large Language
Models,” Apr. 2023, arXiv:2303.18223 [cs]. [Online]. Available:
http://arxiv.org/abs/2303.18223
[17] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning
to represent programs with graphs,” in International Conference
on Learning Representations , 2018. [Online]. Available: https:
//openreview.net/forum?id=BJOFETxR-
[18] M. Allamanis, “The adverse effects of code duplication in machine
learning models of code,” in Onward! OOPLSA 2019 , 2019, pp.
143–153. [Online]. Available: https://doi.org/10.1145/3359591.3359735
[19] R. M. Karampatsis and C. Sutton, “Maybe deep neural networks are the
best choice for modeling source code,” arXiv , no. Lm, 2019.
[20] Y . Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic
language model,” Advances in Neural Information Processing Systems ,
vol. 3, pp. 1137–1155, 2003.
[21] R. M. Karampatsis, H. Babii, R. Robbes et al. , “Open-V ocabulary
Models for Source Code (Extended Abstract),” Proceedings - 2020
ACM/IEEE 42nd International Conference on Software Engineering:
Companion, ICSE-Companion 2020 , pp. 294–295, 2020.
[22] A. Vaswani, N. Shazeer, N. Parmar et al. , “Attention is all you need,” in
Proceedings of the 31st International Conference on Neural Information
Processing Systems , ser. NeurIPs’17. Red Hook, NY , USA: Curran
Associates Inc., 2017, p. 6000–6010.[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language understanding.”
[Online]. Available: http://arxiv.org/abs/1810.04805
[24] Z. Feng, D. Guo, D. Tang et al. , “CodeBERT: A pre-trained
model for programming and natural languages.” [Online]. Available:
http://arxiv.org/abs/2002.08155
[25] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, “Learning and
evaluating contextual embedding of source code.” [Online]. Available:
http://arxiv.org/abs/2001.00059
[26] Z. Lin, G. Li, J. Zhang et al. , “Xcode: Towards cross-language code
representation with large-scale pre-training,” vol. 31, no. 3, pp. 1–44.
[Online]. Available: https://dl.acm.org/doi/10.1145/3506696
[27] D. Guo, S. Ren, S. Lu et al. , “GraphCodeBERT: Pre-training
code representations with data flow.” [Online]. Available: http:
//arxiv.org/abs/2009.08366
[28] Y . Wang, W. Wang, S. Joty, and S. C. H. Hoi, “CodeT5: Identifier-aware
unified pre-trained encoder-decoder models for code understanding and
generation.” [Online]. Available: http://arxiv.org/abs/2109.00859
[29] M. Lewis, Y . Liu, N. Goyal et al. , “BART: Denoising sequence-to-
sequence pre-training for natural language generation, translation, and
comprehension.” [Online]. Available: http://arxiv.org/abs/1910.13461
[30] S. Lu, D. Guo, S. Ren et al. , “CodeXGLUE: A machine learning
benchmark dataset for code understanding and generation.” [Online].
Available: http://arxiv.org/abs/2102.04664
[31] L. Gao, S. Biderman, S. Black et al. , “Codeparrot,” 2021.
[Online]. Available: https://github.com/huggingface/blog/blob/main/
codeparrot.md
[32] S. Black, G. Leo, P. Wang et al. , “GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorflow,” Mar. 2021.
[Online]. Available: https://doi.org/10.5281/zenodo.5297715
[33] B. Wang and A. Komatsuzaki, “GPT-J-6B: A 6 Billion Param-
eter Autoregressive Language Model,” https://github.com/kingoflolz/
mesh-transformer-jax, May 2021.
[34] W. Zaremba, G. Brockman, and OpenAI, “Openai codex,” Aug 2021.
[Online]. Available: https://openai.com/blog/openai-codex/
[35] S. Black, S. Biderman, E. Hallahan et al. , “Gpt-neox-20b: An
open-source autoregressive language model,” 2022. [Online]. Available:
https://arxiv.org/abs/2204.06745
[36] J. Austin, A. Odena, M. Nye et al. , “Program synthesis with large
language models,” 2021.
[37] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn, “A
Systematic Evaluation of Large Language Models of Code,” May 2022,
arXiv:2202.13169 [cs]. [Online]. Available: http://arxiv.org/abs/2202.
13169
[38] A. Karpathy, J. Johnson, and F. Li, “Visualizing and understanding
recurrent networks,” CoRR , vol. abs/1506.02078, 2015. [Online].
Available: http://arxiv.org/abs/1506.02078
[39] S. Troshin and N. Chirkova, “Probing Pretrained Models of
Source Code,” Nov. 2022, arXiv:2202.08975 [cs]. [Online]. Available:
http://arxiv.org/abs/2202.08975
11

--- PAGE 12 ---
[40] J. A. H. L ´opez, M. Weyssow, J. S. Cuadrado, and H. Sahraoui, “AST-
Probe: Recovering abstract syntax trees from hidden representations
of pre-trained language models,” Sep. 2022, arXiv:2206.11719 [cs].
[Online]. Available: http://arxiv.org/abs/2206.11719
[41] J. E. Hopcroft, R. Motwani, and J. D. Ullman, Introduction to Automata
Theory, Languages, and Computation (3rd Edition) . USA: Addison-
Wesley Longman Publishing Co., Inc., 2006.
[42] V . Belle and I. Papantonis, “Principles and Practice of Explainable
Machine Learning,” 2020, arXiv: 2009.11698. [Online]. Available:
http://arxiv.org/abs/2009.11698
[43] C. Molnar, G. Casalicchio, and B. Bischl, “Interpretable Machine
Learning – A Brief History, State-of-the-Art and Challenges,”
no. 01, pp. 1–15, 2020, arXiv: 2010.09337. [Online]. Available:
http://arxiv.org/abs/2010.09337
[44] S. Gholizadeh and N. Zhou, “Model Explainability in Deep Learning
Based Natural Language Processing,” Jun. 2021, arXiv:2106.07410
[cs]. [Online]. Available: http://arxiv.org/abs/2106.07410
[45] H. Vasconcelos, G. Bansal, A. Fourney et al. , “Generation Probabilities
Are Not Enough: Exploring the Effectiveness of Uncertainty Highlight-
ing in AI-Powered Code Completions,” Feb. 2023, arXiv:2302.07248
[cs]. [Online]. Available: http://arxiv.org/abs/2302.07248
[46] M. Chen, J. Tworek, H. Jun et al. , “Evaluating Large Language Models
Trained on Code,” 2021. [Online]. Available: http://arxiv.org/abs/2107.
03374
[47] C. Watson, N. Cooper, D. Nader-Palacio et al. , “A systematic
literature review on the use of deep learning in software engineering
research,” CoRR , vol. abs/2009.06520, 2020. [Online]. Available:
https://arxiv.org/abs/2009.06520
[48] M. Brunsfeld, A. Hlynskyi, P. Thomson et al. , “tree-sitter/tree-sitter:
v0.20.8,” Apr. 2023. [Online]. Available: https://doi.org/10.5281/zenodo.
7798573
[49] D. N. Palacio, N. Cooper, A. Rodriguez et al. , “Toward a theory of
causation for interpreting neural code models,” 2023.
[50] J. Pearl, M. Glymour, and N. P.Jewell, Causal Inference in Statistics, A
Primer , 2016.
[51] J. Pearl, Causality: models, reasoning, and inference , 2009.
[52] J. Pearl and D. Mackenzie, The book of why: The New Science of Cause
and Effect , 2018.
[53] A. Sharma, V . Syrgkanis, C. Zhang, and E. Kıcıman, “DoWhy : Ad-
dressing Challenges in Expressing and Validating Causal Assumptions,”
2021.
[54] “Bigquery dataset.” [Online]. Available: https://cloud.google.com/
bigquery
[55] Anonymous, “Interpretingcodegeneration,” https://anonymous.4open.
science/r/InterpretingCodeGeneration-75E4/, 2022.
[56] L. Gao, S. Biderman, S. Black et al. , “The pile: An 800gb dataset of
diverse text for language modeling,” 2020.
[57] S. Lu, D. Guo, S. Ren et al. , “Codexglue: A machine learning
benchmark dataset for code understanding and generation,” CoRR , vol.
abs/2102.04664, 2021.
[58] E. Nijkamp, B. Pang, H. Hayashi et al. , “Codegen: An open large
language model for code with multi-turn program synthesis,” 2023.
[59] T. Wolf, L. Debut, V . Sanh et al. , “Transformers: State-of-the-
art natural language processing,” in Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing:
System Demonstrations . Online: Association for Computational
Linguistics, Oct. 2020, pp. 38–45. [Online]. Available: https:
//www.aclweb.org/anthology/2020.emnlp-demos.6
[60] A. Paszke, S. Gross, F. Massa et al. , “Pytorch: An imperative style,
high-performance deep learning library,” in Advances in Neural
Information Processing Systems 32 , H. Wallach, H. Larochelle,
A. Beygelzimer et al. , Eds. Curran Associates, Inc., 2019,
pp. 8024–8035. [Online]. Available: http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf
[61] E. Nijkamp, B. Pang, H. Hayashi et al. , “A conversational paradigm for
program synthesis,” arXiv preprint , 2022.
12
