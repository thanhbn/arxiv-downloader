# Chuyển giao Tri thức từ Ngôn ngữ Lập trình Giàu Tài nguyên sang Ít Tài nguyên cho Các Mô hình Ngôn ngữ Lớn Lập trình

FEDERICO CASSANO, Đại học Northeastern, Hoa Kỳ
JOHN GOUWAR, Đại học Northeastern, Hoa Kỳ  
FRANCESCA LUCCHETTI, Đại học Northeastern, Hoa Kỳ
CLAIRE SCHLESINGER, Đại học Northeastern, Hoa Kỳ
ANDERS FREEMAN, Đại học Wellesley, Hoa Kỳ
CAROLYN JANE ANDERSON, Đại học Wellesley, Hoa Kỳ
MOLLY Q FELDMAN, Đại học Oberlin, Hoa Kỳ
MICHAEL GREENBERG, Viện Công nghệ Stevens, Hoa Kỳ
ABHINAV JANGDA, Microsoft Research, Hoa Kỳ
ARJUN GUHA, Đại học Northeastern, Hoa Kỳ và Roblox, Hoa Kỳ

Trong vài năm qua, các Mô hình Ngôn ngữ Lớn về Lập trình (Code LLMs) đã bắt đầu có tác động đáng kể đến thực hành lập trình. Code LLMs cũng đang nổi lên như những khối xây dựng cho nghiên cứu trong ngôn ngữ lập trình và kỹ thuật phần mềm. Tuy nhiên, chất lượng mã được tạo ra bởi Code LLM thay đổi đáng kể theo ngôn ngữ lập trình. Code LLMs tạo ra kết quả ấn tượng trên các ngôn ngữ lập trình giàu tài nguyên được đại diện tốt trong dữ liệu huấn luyện của chúng (ví dụ, Java, Python, hoặc JavaScript), nhưng gặp khó khăn với các ngôn ngữ ít tài nguyên có dữ liệu huấn luyện hạn chế (ví dụ, OCaml, Racket, và một số ngôn ngữ khác).

Bài báo này trình bày một cách tiếp cận hiệu quả để tăng cường hiệu suất của Code LLMs trên các ngôn ngữ ít tài nguyên bằng cách sử dụng dữ liệu bán tổng hợp. Cách tiếp cận của chúng tôi, gọi là MultiPL-T, tạo ra các bộ dữ liệu chất lượng cao cho các ngôn ngữ ít tài nguyên, sau đó có thể được sử dụng để tinh chỉnh bất kỳ Code LLM đã được huấn luyện trước nào. MultiPL-T dịch dữ liệu huấn luyện từ các ngôn ngữ giàu tài nguyên thành dữ liệu huấn luyện cho các ngôn ngữ ít tài nguyên theo cách sau: 1) Chúng tôi sử dụng Code LLM để tổng hợp các bài kiểm tra đơn vị cho mã có chú thích từ ngôn ngữ nguồn giàu tài nguyên, lọc bỏ các bài kiểm tra lỗi và mã có độ bao phủ kiểm tra thấp. 2) Chúng tôi sử dụng Code LLM để dịch mã từ ngôn ngữ nguồn giàu tài nguyên sang ngôn ngữ đích ít tài nguyên. Điều này cho chúng tôi một kho dữ liệu huấn luyện ứng viên trong ngôn ngữ đích, nhưng nhiều bản dịch này là sai. 3) Chúng tôi sử dụng một trình biên dịch nhẹ để biên dịch các ca kiểm tra được tạo trong bước (1) từ ngôn ngữ nguồn sang ngôn ngữ đích, cho phép chúng tôi lọc những bản dịch rõ ràng sai. Kết quả là một kho dữ liệu huấn luyện trong ngôn ngữ đích ít tài nguyên nơi tất cả các mục đã được xác thực bằng các ca kiểm tra. Chúng tôi áp dụng cách tiếp cận này để tạo ra hàng chục nghìn mục huấn luyện mới, đã được xác thực cho năm ngôn ngữ ít tài nguyên: Julia, Lua, OCaml, R, và Racket, sử dụng Python làm ngôn ngữ nguồn giàu tài nguyên. Hơn nữa, chúng tôi sử dụng một Code LLM mở (StarCoderBase) với dữ liệu huấn luyện mở (The Stack), cho phép chúng tôi khử nhiễm các benchmark, huấn luyện mô hình mà không vi phạm giấy phép, và chạy các thí nghiệm mà không thể thực hiện được cách khác.

Sử dụng các bộ dữ liệu được tạo bằng MultiPL-T, chúng tôi trình bày các phiên bản tinh chỉnh của StarCoderBase và Code Llama cho Julia, Lua, OCaml, R, và Racket vượt trội hơn các bản tinh chỉnh khác của những mô hình cơ sở này trong nhiệm vụ ngôn ngữ tự nhiên sang mã. Chúng tôi cũng trình bày các bản tinh chỉnh Racket cho hai mô hình rất gần đây, DeepSeek Coder và StarCoder2, để chỉ ra rằng MultiPL-T tiếp tục vượt trội hơn các cách tiếp cận tinh chỉnh khác cho các ngôn ngữ ít tài nguyên. Cách tiếp cận MultiPL-T dễ áp dụng cho các ngôn ngữ mới, và hiệu quả hơn đáng kể cũng như hiệu quả hơn các lựa chọn thay thế như huấn luyện lâu hơn.

**Từ khóa bổ sung và Cụm từ**: Mô hình Ngôn ngữ Lớn được huấn luyện trên Mã

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn về Lập trình (Code LLMs) đang bắt đầu có tác động đáng kể đến cả lập trình viên chuyên nghiệp và nghiên cứu trong ngôn ngữ lập trình và kỹ thuật phần mềm. GitHub Copilot chỉ là một trong số các công cụ phổ biến được hỗ trợ bởi Code LLMs. Hơn nữa, Code LLMs cũng đang nổi lên như một khối xây dựng cho nghiên cứu. Tuy nhiên, chất lượng mã được tạo ra bởi Code LLM thay đổi đáng kể theo ngôn ngữ lập trình. Các mô hình ấn tượng nhất khi tạo ra mã trong các ngôn ngữ lập trình giàu tài nguyên như Python, JavaScript, và Java, nhưng gặp khó khăn trong các ngôn ngữ ít tài nguyên, như Racket và OCaml. Điều này đặt những lập trình viên dựa vào những ngôn ngữ này vào thế bất lợi, vì họ không nhận được những lợi ích tương tự mà Code LLMs có thể mang lại cho các ngôn ngữ giàu tài nguyên.

Vấn đề chính là hiệu suất của Code LLMs phụ thuộc vào lượng dữ liệu ngôn ngữ có sẵn để huấn luyện. Ví dụ, The Stack, là bộ huấn luyện cho một số Code LLMs đương đại, có 64GB Python, nhưng chỉ khoảng 1GB OCaml và 0.5GB Scheme/Racket. Như Hình 1 cho thấy, hiệu suất của StarCoderBase, một Code LLM mở được huấn luyện trên The Stack, thường tăng khi dữ liệu huấn luyện cho ngôn ngữ tăng.

Mục tiêu của chúng tôi trong bài báo này là nghiên cứu các phương pháp để huấn luyện thêm, hoặc tinh chỉnh, các Code LLMs đã được huấn luyện trước để cải thiện hiệu suất của chúng trên các ngôn ngữ ít tài nguyên. Cách tiếp cận rõ ràng là cố gắng tìm thêm dữ liệu, nhưng đối với một ngôn ngữ ít tài nguyên, việc tìm thêm dữ liệu là khó theo định nghĩa. Ví dụ, The Stack đã bao gồm tất cả mã có giấy phép tự do cho 358 ngôn ngữ lập trình từ GitHub tính đến năm 2022, và GitHub là kho lưu trữ mã nguồn mở lớn nhất. Một lựa chọn thay thế là huấn luyện lâu hơn (tức là, cho nhiều epoch hơn) trên dữ liệu hiện có. Tuy nhiên, trong bài báo này, chúng tôi chỉ ra rằng huấn luyện lâu hơn trên một số ngôn ngữ ít tài nguyên không chỉ không hiệu quả mà thực sự có thể làm hại hiệu suất (§3.1). Một lựa chọn thay thế khác là huấn luyện mô hình trên dữ liệu tổng hợp, là dữ liệu được tạo ra bởi chính một LLM. Những bộ dữ liệu tinh chỉnh tổng hợp này hiệu quả cho các ngôn ngữ lập trình giàu tài nguyên. Tuy nhiên, chúng tôi chỉ ra rằng dữ liệu tổng hợp không hoạt động cho các ngôn ngữ ít tài nguyên vì lý do trực quan rằng LLMs tạo ra các chương trình chất lượng kém trong các ngôn ngữ ít tài nguyên (§3.2).

**Cách tiếp cận của chúng tôi.** Trong bài báo này, chúng tôi trình bày một cách tiếp cận mới và hiệu quả để tinh chỉnh Code LLMs cho các ngôn ngữ lập trình ít tài nguyên dựa trên việc tạo ra dữ liệu huấn luyện bán tổng hợp. Cách tiếp cận của chúng tôi dựa vào một số thành phần chính. 1) Khối lượng lớn dữ liệu huấn luyện cho các ngôn ngữ lập trình giàu tài nguyên bao gồm rất nhiều mã được ghi chép tốt; 2) Code LLMs là những công cụ tạo kiểm tra đơn vị hiệu quả và hiệu suất, và chúng ta có thể kiểm tra rằng các bài kiểm tra được tạo ra là hợp lệ; 3) Chúng ta có thể biên dịch nhiều bài kiểm tra đơn vị sang một ngôn ngữ ít tài nguyên bằng một trình biên dịch đơn giản; 4) Code LLMs có thể dịch mã từ ngôn ngữ này sang ngôn ngữ khác, và mặc dù những bản dịch này có thể có lỗi, chúng ta có thể lọc chúng bằng các bài kiểm tra nói trên, thử lại cho đến khi các bài kiểm tra vượt qua, và thiết kế prompt để tăng khả năng dịch thành công. Kết hợp bốn ý tưởng này lại với nhau, chúng tôi phát triển một pipeline để chuyển giao dữ liệu huấn luyện qua nhiều ngôn ngữ lập trình mà chúng tôi gọi là MultiPL-T.

Hình 2 đưa ra cái nhìn tổng quan cấp cao về cách MultiPL-T tạo ra dữ liệu huấn luyện chất lượng cao cho một ngôn ngữ lập trình ít tài nguyên. Chúng tôi sử dụng một LLM để dịch mã từ một ngôn ngữ giàu tài nguyên (1○) sang các ngôn ngữ đích ít tài nguyên (2○). Tuy nhiên, bản dịch này không đáng tin cậy theo định nghĩa: LLMs kém trong việc tạo ra mã trong các ngôn ngữ lập trình ít tài nguyên. Nhưng, chúng ta có thể tận dụng tính ngẫu nhiên của việc tạo ra LLM để tạo ra một số bản dịch ứng viên cho mỗi mục và lọc ra những bản dịch lỗi bằng các ca kiểm tra đã được tổng hợp (4○). Chúng ta không thể tạo ra các bài kiểm tra trực tiếp từ mã ngôn ngữ ít tài nguyên (vì nó có khả năng bị lỗi). Thay vào đó, chúng ta tạo ra và xác thực các bài kiểm tra trong ngôn ngữ giàu tài nguyên (3○), và sau đó biên dịch những bài kiểm tra này sang ngôn ngữ ít tài nguyên (4○). Việc kết hợp các bước này cho ra dữ liệu huấn luyện trong ngôn ngữ ít tài nguyên vượt qua các bài kiểm tra đã được biên dịch cũng vượt qua trong ngôn ngữ giàu tài nguyên.

Các kho dữ liệu huấn luyện của các ngôn ngữ lập trình giàu tài nguyên (ví dụ, Python) là rất lớn, vì vậy chúng tôi sử dụng các bộ lọc heuristic tích cực để xây dựng một kho các hàm "chất lượng cao" (1○) trước khi thử bất kỳ bản dịch LLM nào. Đối với những hàm này, chúng tôi thấy rằng LLM tạo ra các bài kiểm tra một cách đáng tin cậy (3○), nhưng cần một số nỗ lực để đạt được độ bao phủ kiểm tra cao. Dịch các hàm (2○) chỉ đơn giản là prompting LLM, nhưng yêu cầu một số suy luận kiểu nhẹ để hiệu quả khi ngôn ngữ đích được nhập. Cuối cùng, để biên dịch các bài kiểm tra sang ngôn ngữ ít tài nguyên (4○), chúng tôi xây dựng dựa trên một toolchain hiện có, thêm hỗ trợ cho OCaml, các định dạng prompting mới, và hỗ trợ cho một nhiệm vụ dịch LLM lớn hơn ba bậc độ lớn so với những gì nó hỗ trợ ban đầu.

Sử dụng dữ liệu huấn luyện được tạo bởi MultiPL-T, chúng tôi trình bày các Code LLMs tinh chỉnh đạt được hiệu suất tiên tiến trên năm ngôn ngữ ít tài nguyên: Racket, OCaml, Lua, R, và Julia. Chúng tôi tập trung chủ yếu vào việc tinh chỉnh họ StarCoder của Code LLMs. Có các mô hình StarCoder có sẵn ở nhiều kích thước khác nhau, bao gồm một mô hình 1B tham số đủ nhẹ để chạy trên CPUs, và một mô hình có khả năng hơn 15B tham số, mà chúng tôi sử dụng làm trình tạo kiểm tra và dịch ngôn ngữ cho MultiPL-T. Các mô hình StarCoder cũng có dữ liệu huấn luyện mở, cho phép chúng tôi so sánh MultiPL-T với baseline của việc huấn luyện lâu hơn trên dữ liệu hiện có cho các ngôn ngữ ít tài nguyên. Chúng tôi cũng trình bày các phiên bản tinh chỉnh của các mô hình Code Llama 34B và 70B, và các bản tinh chỉnh Racket của các mô hình DeepSeek Coder và StarCoder2 được phát hành gần đây.

**Đóng góp.** Tóm lại, chúng tôi đưa ra những đóng góp sau:

(1) **MultiPL-T**, một cách tiếp cận hiệu quả để tạo ra dữ liệu bán tổng hợp cho các ngôn ngữ lập trình ít tài nguyên bằng cách sử dụng bản dịch được xác thực bằng kiểm tra của mã chất lượng cao trong các ngôn ngữ giàu tài nguyên.

(2) Các bộ dữ liệu tinh chỉnh hiệu quả cho Julia, Lua, OCaml, R, và Racket, bao gồm hàng chục nghìn hàm được ghi chép và kiểm tra được tạo bằng StarCoderBase-15B.

(3) Một bộ dữ liệu 133,168 hàm Python được trích xuất từ Stack, nơi mỗi hàm có tài liệu ngôn ngữ tự nhiên và một tập hợp các bài kiểm tra đã được xác thực với độ bao phủ cao. Bộ dữ liệu này có thể được sử dụng để tạo ra các tập tinh chỉnh cho các ngôn ngữ lập trình khác.

(4) Các phiên bản tinh chỉnh của StarCoderBase 1B và 15B cho Julia, Lua, OCaml, R, và Racket. Đối với những ngôn ngữ này, những mô hình tinh chỉnh này vượt trội hơn các bản tinh chỉnh trước đó của StarCoderBase trong nhiệm vụ ngôn ngữ tự nhiên sang mã.

(5) Các phiên bản tinh chỉnh của Code Llama 34B và 70B cho Julia, Lua, OCaml, R, và Racket. Đối với những ngôn ngữ này, những mô hình tinh chỉnh này vượt trội hơn các bản tinh chỉnh trước đó của Code Llama. Quan trọng hơn, đây là một kết quả không phổ biến khi dữ liệu được tạo ra từ một mô hình nhỏ hơn (StarCoderBase-15B) cải thiện hiệu suất của các mô hình lớn hơn và tốt hơn (Code Llama 34B và 70B).

(6) Các phiên bản tinh chỉnh của StarCoder2-15B và DeepSeek Coder 33B cho Racket cũng vượt trội hơn các mô hình tinh chỉnh khác. Đây là hai mô hình được phát hành rất gần đây.

(7) Một đánh giá kỹ lưỡng bao gồm a) so sánh MultiPL-T với baseline của việc huấn luyện thêm trên dữ liệu hiện có, b) đánh giá hiệu quả tinh chỉnh với MultiPL-T, c) kết quả trên các benchmark đa ngôn ngữ trước đây, d) một benchmark đa ngôn ngữ mới được thiết kế để kiểm tra học trong ngữ cảnh, e) đánh giá cách mã được tạo ra tuân thủ phong cách lập trình Racket phổ biến, f) tác động của khử trùng lặp dữ liệu, và g) tác động của tinh chỉnh trên dữ liệu nguồn Python.

## 2 Bối cảnh

Trong phần này, chúng tôi đưa ra cái nhìn tổng quan cấp cao về cách Code LLMs được huấn luyện và đánh giá. Chúng tôi sử dụng StarCoder làm ví dụ, vì đây là mô hình mà chúng tôi sử dụng cho hầu hết công việc của mình.

### 2.1 Huấn luyện và Tinh chỉnh Mô hình Ngôn ngữ Lớn về Lập trình

Một mô hình ngôn ngữ lớn (LLM) là một mạng neural được huấn luyện trên hàng trăm gigabyte hoặc thậm chí terabyte dữ liệu. Code LLMs được huấn luyện trên mã nguồn (và thường cả tài liệu ngôn ngữ tự nhiên), cho phép chúng tạo ra mã từ các bình luận, bình luận từ mã, thêm mã từ mã, v.v. Huấn luyện LLM đòi hỏi tài nguyên đáng kể: StarCoderBase được huấn luyện trên khoảng 800GB mã, mất ba tuần trên một cụm 512 GPU NVIDIA A100.

Cách duy nhất để xây dựng một tập huấn luyện ở quy mô này là cạo các kho lưu trữ mã công cộng. Có một số tập huấn luyện công cộng dựa trên GitHub, và The Stack là một ví dụ gần đây. The Stack v1.2 có 3TB mã nguồn có giấy phép tự do cho 358 ngôn ngữ lập trình. Nó được xây dựng vào năm 2022, và kể từ đó đã được sử dụng để huấn luyện một số Code LLMs, bao gồm StarCoderBase. Cụ thể, StarCoderBase được huấn luyện trên một tập con được lọc của The Stack bao gồm 86 ngôn ngữ lập trình.

**Họ mô hình StarCoder.** StarCoder là một họ mô hình có sẵn ở một số kích thước. Mô hình lớn nhất và có khả năng nhất trong họ được gọi là StarCoderBase, có 15B tham số. Có các phiên bản nhỏ hơn của StarCoderBase được huấn luyện trên chính xác cùng dữ liệu. Để tận dụng tài nguyên GPU hạn chế, chúng tôi sử dụng mô hình nhỏ nhất, StarCoderBase-1B, cho hầu hết các thí nghiệm trong bài báo này. Tuy nhiên, chúng tôi cũng chỉ ra rằng kết quả của chúng tôi tổng quát hóa sang StarCoderBase-15B. Ngoài ra còn có một mô hình trong họ StarCoder chỉ được đặt tên là StarCoder: đó là StarCoderBase-15B được chuyên biệt hóa để xuất sắc ở Python. Bài báo này sử dụng StarCoderBase-15B để dịch sang các ngôn ngữ ít tài nguyên, và StarCoder-15B để tạo ra kiểm tra Python.

**Họ mô hình Code Llama.** Họ mô hình Code Llama được phát hành gần đây và hoạt động tốt hơn các mô hình StarCoder trên các benchmark phổ biến. Mặc dù các tác giả tuyên bố rằng dữ liệu huấn luyện đến từ các bộ dữ liệu có thể truy cập công khai, họ không tiết lộ các bộ dữ liệu cụ thể được sử dụng, ngăn chúng tôi tiến hành đánh giá kỹ lưỡng trên Code Llama như chúng tôi làm với StarCoder và dữ liệu huấn luyện của nó. Hơn nữa, giấy phép Llama cấm sử dụng đầu ra mô hình để huấn luyện các mô hình không phải Llama, đó là lý do chúng tôi sử dụng StarCoder để tạo dữ liệu. Tuy nhiên, chúng tôi huấn luyện và đánh giá các mô hình Code Llama lớn hơn (34B và 70B).

**Tinh chỉnh.** Sau khi huấn luyện, một mô hình có thể được huấn luyện thêm, hoặc tinh chỉnh, với ít tài nguyên hơn đáng kể. Ví dụ, có một số phiên bản tinh chỉnh của StarCoderBase được huấn luyện với vài ngày thời gian GPU trên một lượng dữ liệu khiêm tốn. Hầu hết các phiên bản tinh chỉnh của StarCoderBase được thiết kế để làm cho mô hình thậm chí tốt hơn ở các ngôn ngữ giàu tài nguyên, như Python. Ngược lại, bài báo này trình bày các phiên bản tinh chỉnh của StarCoderBase tốt hơn đáng kể ở một số ngôn ngữ ít tài nguyên.

Thông thường người ta chưng cất dữ liệu từ một mô hình lớn hơn (ví dụ, GPT-4), để tinh chỉnh một mô hình nhỏ hơn. Tuy nhiên, chúng tôi chỉ ra rằng MultiPL-T có thể làm ngược lại: chúng tôi sử dụng dữ liệu được tạo ra từ StarCoderBase-15B để tinh chỉnh các mô hình lớn hơn, CodeLlama-34B và 70B. Đây là một hình thức giám sát yếu-tới-mạnh, nơi một mô hình nhỏ hơn được sử dụng để tạo dữ liệu để huấn luyện một mô hình lớn hơn, cho thấy khả năng mở rộng của MultiPL-T.

### 2.2 Nhiệm vụ Code LLM và Benchmarking Code LLMs

Một Code LLM có thể được nhắc để thực hiện nhiều nhiệm vụ khác nhau, bao gồm dịch mã, tạo kiểm tra, đột biến mã, chỉnh sửa mã, và nhiều hơn nữa. Bài viết này tập trung vào nhiệm vụ ngôn ngữ tự nhiên sang mã cho các ngôn ngữ lập trình ít tài nguyên. Việc làm cho Code LLMs tốt hơn ở các nhiệm vụ khác nằm ngoài phạm vi của bài viết này.

Hầu hết các benchmark Code LLM cho nhiệm vụ ngôn ngữ tự nhiên sang mã, bao gồm những cái chúng tôi sử dụng trong bài báo này, tuân theo định dạng được giới thiệu bởi benchmark Codex "HumanEval". Mỗi bài toán benchmark có hai phần: 1) một prompt cho LLM có chữ ký hàm và một bình luận, và 2) một bộ ca kiểm tra không được đưa cho LLM. Do đó mỗi bài toán được chạy trong hai bước: 1) LLM tạo ra một hàm từ prompt, và 2) hàm được tạo ra sau đó được kiểm tra với các bài kiểm tra ẩn, và tất cả các bài kiểm tra phải vượt qua để mã được tạo ra được coi là đúng.

Benchmark HumanEval có 164 bài toán cho Python. Tuy nhiên, có thể dịch cơ học hầu hết những bài toán này sang các ngôn ngữ lập trình khác (Hình 3). Dịch các bình luận và chữ ký hàm là đơn giản, nhưng cần một số cẩn thận để giới thiệu các kiểu cho các ngôn ngữ đích có kiểu. Dịch các ca kiểm tra cũng hóa ra dễ dàng, vì hầu hết tất cả các ca kiểm tra HumanEval đều có dạng f(v_in) = v_out, nơi v_in và v_out là các giá trị bậc nhất. Đây là cách tiếp cận được thực hiện bởi MultiPL-E và các công cụ tương tự để xây dựng các benchmark đa ngôn ngữ cho Code LLMs. Bài báo này sử dụng MultiPL-E, là benchmark duy nhất cho đến nay hỗ trợ Racket, và chúng tôi mở rộng nó để hỗ trợ OCaml cho bài báo này.

Code LLMs dường như tạo ra mã chất lượng cao hơn khi đầu ra của chúng được lấy mẫu. Vì việc lấy mẫu giới thiệu tính không xác định, chúng ta phải đánh giá đầu ra của chúng bằng cách tạo ra một số mẫu từ cùng một prompt. Metric được sử dụng rộng rãi nhất cho hiệu suất Code LLM là pass@k, là khả năng LLM tạo ra một chương trình vượt qua tất cả các bài kiểm tra ẩn ít nhất một lần từ k lần thử. Pass@k phải được ước tính từ n >> k mẫu. Khi k = 1 và có c thành công, pass@1 giống như tỷ lệ vượt qua (c/n). Chúng tôi sử dụng pass@1 làm metric cho tất cả các thí nghiệm benchmarking của chúng tôi, đây là thực hành phổ biến. Theo trực giác, pass@1 đo lường khả năng của Code LLM tạo ra một giải pháp đúng trong một lần thử duy nhất.

### 2.3 Tại sao StarCoder?

Trong phần còn lại của bài báo này, phần lớn công việc mà chúng tôi trình bày sử dụng họ StarCoder của Code LLMs vì những lý do sau.

(1) Tại thời điểm chúng tôi bắt đầu công việc này, StarCoder là Code LLM mở có giấy phép tự do, hoạt động tốt nhất có sẵn.

(2) Họ StarCoder bao gồm một mô hình 1B tham số khá nhỏ, phù hợp cho các thí nghiệm với ngân sách hạn chế.

(3) Mặc dù có những LLMs nguồn đóng tốt hơn khi chúng tôi bắt đầu, chúng (a) không hỗ trợ tinh chỉnh, (b) đắt hơn đáng kể để sử dụng ở quy mô công việc của chúng tôi, hoặc (c) cấm sử dụng đầu ra của chúng để tinh chỉnh các LLMs khác.

(4) StarCoder vẫn là Code LLM duy nhất với dữ liệu huấn luyện mở, mà chúng tôi sử dụng để tương quan hiệu suất mô hình với kích thước tập huấn luyện (Hình 1), đánh giá huấn luyện thêm (§3.1), và kiểm tra rằng các benchmark của chúng tôi không có trong dữ liệu huấn luyện. Sẽ không thể thực hiện công việc này với bất kỳ Code LLM nào khác.

Chúng tôi tinh chỉnh một số Code LLMs mới hơn với dữ liệu được tạo bởi StarCoder để chỉ ra rằng cách tiếp cận của chúng tôi vẫn hữu ích (§5).

## 3 Các lựa chọn thay thế cho MultiPL-T

Trước khi chúng tôi trình bày cách tiếp cận MultiPL-T, chúng tôi xem xét hai lựa chọn thay thế đơn giản hơn.

### 3.1 Huấn luyện Thêm trên Dữ liệu Tự nhiên

Cách đơn giản nhất để tăng hiệu suất của Code LLM trên một ngôn ngữ lập trình là huấn luyện nó thêm trên dữ liệu tự nhiên, là mã được viết bởi lập trình viên con người thay vì mã được tạo ra bằng các phương tiện khác (ví dụ, một LLM). Đây là cách tiếp cận được thực hiện để tạo ra StarCoder từ StarCoderBase. Mô hình sau là mô hình cơ sở, và mô hình trước được tinh chỉnh trên khoảng hai epoch bổ sung của tập con Python của The Stack. Mặc dù cách tiếp cận này hiệu quả cho các ngôn ngữ giàu tài nguyên, chúng tôi bây giờ chỉ ra rằng nó không hoạt động cho một số ngôn ngữ ít tài nguyên (Hình 4).

Trong Hình 4a, chúng tôi tinh chỉnh ba phiên bản của StarCoderBase-1B trên ba epoch nữa của Lua, OCaml, và Racket. Dữ liệu này từ The Stack. Tuy nhiên, tập con Racket của The Stack có chất lượng kém, vì vậy chúng tôi sử dụng tập con Scheme thay thế. The Stack có Lua nhiều hơn một bậc độ lớn so với OCaml và Racket. Hơn nữa, thậm chí dữ liệu Racket và OCaml trong The Stack cũng lớn hơn đáng kể so với các bộ dữ liệu tinh chỉnh chúng tôi sẽ phát triển với MultiPL-T. Do đó, những thí nghiệm này không thể so sánh trực tiếp với nhau, vì chúng huấn luyện trên lượng dữ liệu rất khác nhau. Tuy nhiên, chúng tôi có kết quả kém cho tất cả: hiệu suất của những mô hình tinh chỉnh này hầu như không tăng cho Racket và OCaml và thậm chí giảm cho Lua.

Trong Hình 4b, chúng tôi thực hiện một thí nghiệm khác với The Stack cho phép so sánh trực tiếp với MultiPL-T. Chúng tôi lấy mẫu ngẫu nhiên dữ liệu từ The Stack để có được khoảng cùng khối lượng dữ liệu mà chúng tôi tạo ra với MultiPL-T. Do đó, tinh chỉnh trên những bộ dữ liệu này sẽ sử dụng tài nguyên tính toán tương tự như tinh chỉnh một mô hình với dữ liệu MultiPL-T. Chúng tôi sử dụng dữ liệu này để tinh chỉnh ba phiên bản trên StarCoderBase-1B cho sáu epoch, và đánh giá các mô hình ở mỗi epoch. Chúng tôi vẫn có kết quả kém với The Stack: hiệu suất Lua và OCaml hầu như không tăng và hiệu suất Racket giảm. Ngược lại, tinh chỉnh với MultiPL-T sẽ chỉ ra những cải thiện đáng kể.

### 3.2 Tự Hướng dẫn cho Ngôn ngữ Lập trình Ít Tài nguyên

Một lựa chọn thay thế cho tinh chỉnh trên dữ liệu tự nhiên là tinh chỉnh trên dữ liệu được tạo bởi LLM. Cách tiếp cận thông thường là chọn tay một bộ dữ liệu seed của các chương trình, nhắc mô hình với mỗi seed để tạo ra thêm chương trình, và lặp lại cho đến khi một bộ dữ liệu đủ lớn đã được thu thập. Loại cách tiếp cận này đã được sử dụng thành công để tạo ra dữ liệu huấn luyện cho Code LLMs trong các ngôn ngữ giàu tài nguyên. Tuy nhiên, rõ ràng là những cách tiếp cận này giả định rằng LLM tốt trong việc tạo ra các chương trình hợp lý đúng và chất lượng cao. Chúng tôi quan tâm đến các ngôn ngữ mà mô hình kém, vì vậy không đáng ngạc nhiên khi tự hướng dẫn không hoạt động.

**Minh họa tự hướng dẫn.** Để minh họa cách tự hướng dẫn gặp sai lầm với các ngôn ngữ ít tài nguyên, chúng tôi sử dụng StarCoderBase-15B để tạo ra các hàm trong Racket, bắt chước bước đầu tiên của tự hướng dẫn. Chúng tôi nhắc mô hình với năm ví dụ viết tay (bao gồm trong tài liệu bổ sung) và để nó tạo ra năm hàm nữa (Hình 5). Chúng tôi thấy rằng bốn trong năm chương trình được tạo bởi mô hình có lỗi. Đây là tỷ lệ lỗi cao hơn nhiều so với những gì thấy rõ từ các bộ dữ liệu tự hướng dẫn cho các ngôn ngữ giàu tài nguyên.

**Một thí nghiệm tự hướng dẫn.** Trong tài liệu bổ sung, chúng tôi trình bày kết quả từ một thí nghiệm nơi chúng tôi tự hướng dẫn StarCoderBase-15B trên Racket và có được kết quả kém mong đợi.

Tự hướng dẫn và huấn luyện thêm trên dữ liệu công cộng hiện có không giúp Code LLMs hoạt động tốt hơn trên các ngôn ngữ lập trình ít tài nguyên. Do đó chúng tôi bây giờ chuyển sang cách tiếp cận MultiPL-T.

## 4 Cách tiếp cận của chúng tôi

Chúng tôi bây giờ trình bày cách tiếp cận MultiPL-T để tạo ra dữ liệu bán tổng hợp chất lượng cao cho các ngôn ngữ ít tài nguyên. Hình 6 mô tả hệ thống MultiPL-T, có một số giai đoạn. 1) Cho một bộ dữ liệu huấn luyện (The Stack), chúng tôi lọc dữ liệu từ một ngôn ngữ giàu tài nguyên (Python) để chọn mã phù hợp cho việc tạo kiểm tra tự động và dịch. The Stack có 60GB Python, và dịch và tạo kiểm tra đắt đỏ, vì vậy chúng tôi lọc khá tích cực. Chúng tôi chỉ chọn các hàm Python riêng lẻ có docstrings và vượt qua một trình kiểm tra kiểu heuristic (§4.1). 2) Cho bộ dữ liệu được lọc, chúng tôi sử dụng Code LLM (StarCoder-15B) để tạo ra bộ kiểm tra cho mỗi hàm. Chúng tôi xác thực các bài kiểm tra được tạo ra về tính đúng đắn và độ bao phủ mã, và thấy rằng Code LLM có thể được sử dụng như một trình tạo kiểm tra có khả năng cho mục đích của chúng tôi (§4.2). 3) Chúng tôi dịch mỗi hàm Python sang một ngôn ngữ đích L, bằng cách nhắc Code LLM để dịch mã. Bản dịch này có thể gặp sai lầm, đặc biệt vì Code LLM hoạt động kém trên ngôn ngữ đích ít tài nguyên. 4) Chúng tôi lọc các hàm L (từ Bước 1) để chỉ chọn những cái vượt qua các ca kiểm tra. Để làm như vậy, chúng tôi biên dịch các ca kiểm tra Python (từ Bước 2) sang ngôn ngữ L, sử dụng trình biên dịch ca kiểm tra Python-sang-L từ MultiPL-E. Trình biên dịch ca kiểm tra là một trình biên dịch truyền thống không gặp phải các ảo giác LLM: nếu nó không thể biên dịch một ca kiểm tra, nó báo hiệu lỗi, và chúng tôi loại bỏ mục huấn luyện nếu quá nhiều ca kiểm tra không thể biên dịch (§4.3). Kết quả cuối cùng do đó là một bộ dữ liệu của các mục huấn luyện mới cho ngôn ngữ L, có thể được sử dụng để tinh chỉnh bất kỳ LLM nào. Trong §5, chúng tôi thảo luận về cách chúng tôi sử dụng dữ liệu này để tinh chỉnh và đánh giá một số mô hình cho năm ngôn ngữ ít tài nguyên khác nhau. Phần còn lại của phần này mô tả các bước trên một cách sâu sắc.

### 4.1 Lọc Dữ liệu từ Ngôn ngữ Giàu Tài nguyên để Dịch và Tạo Kiểm tra

Bước đầu tiên trong MultiPL-T là lọc mã từ một ngôn ngữ giàu tài nguyên để phục vụ như nguồn dịch cho dữ liệu bán tổng hợp của chúng tôi. Chúng tôi sử dụng Python vì nó có sự đại diện cao nhất trong The Stack và vì MultiPL-E có thể biên dịch chữ ký hàm Python và ca kiểm tra sang một số ngôn ngữ ít tài nguyên. Tuy nhiên, cách tiếp cận của chúng tôi có thể dễ dàng được điều chỉnh để hoạt động với các ngôn ngữ giàu tài nguyên khác.

**Lọc Hàm Python Trước Dịch.** The Stack có 22 triệu hàm Python (Bảng 1). Tuy nhiên, không phải tất cả những cái này đều phù hợp cho dịch và xác thực dựa trên kiểm tra với MultiPL-T. Có thể ngây thơ cố gắng dịch và tạo kiểm tra cho tất cả 22M hàm. Tuy nhiên, vì làm như vậy yêu cầu GPUs, nó sẽ cực kỳ đắt đỏ. Thay vào đó, chúng tôi lọc tích cực 22M hàm xuống ~400,000 hàm bằng các bước sau:

(1) Chúng tôi loại trừ các hàm Python không có docstring hoặc sử dụng ký tự không phải ASCII. Có thể tổng quát hóa để bao gồm các hàm có bình luận liên quan. Tuy nhiên, chúng tôi vẫn kết thúc với hơn 5M hàm ứng viên với bộ lọc đơn giản này.

(2) Chúng tôi sử dụng trình kiểm tra Python Pyright để xác thực rằng mỗi hàm trả về một giá trị, chỉ sử dụng thư viện chuẩn Python, và do đó có khả năng đúng kiểu. Pyright sử dụng heuristics và không cố gắng trở nên sound. Điều này không ảnh hưởng đến MultiPL-T, vì chúng tôi chỉ sử dụng khả năng định kiểu như một heuristic cho chất lượng mã. Điều này thu hẹp 5M hàm xuống khoảng 460,000 hàm.

(3) Chúng tôi loại trừ các hàm Python có bình luận gợi ý việc triển khai không hoàn chỉnh (ví dụ "TODO"). Hóa ra một lượng đáng kể mã trên The Stack không hoàn chỉnh; những hàm này không có khả năng là dữ liệu huấn luyện hữu ích. Để tránh nhiễm dữ liệu, chúng tôi lọc ra các hàm có prompt hoặc giải pháp xuất hiện trong các benchmark Code LLM được sử dụng rộng rãi bằng cách tìm khớp chính xác của các prompts.

Bộ dữ liệu cuối cùng chứa 432,361 hàm Python. Với tập hàm hẹp hơn này, chúng tôi chuyển sang các bước tiếp theo yêu cầu GPUs.

### 4.2 Tạo Kiểm tra Đơn vị Python

Bước tiếp theo trong MultiPL-T là tạo ra kiểm tra đơn vị cho mỗi hàm Python. Sau đó chúng tôi sẽ biên dịch những kiểm tra đơn vị này sang các ngôn ngữ đích ít tài nguyên bằng cách sử dụng các trình dịch ca kiểm tra từ MultiPL-E.

Chúng tôi tạo ra kiểm tra đơn vị Python bằng các bước sau.

**Tạo kiểm tra.** Thay vì sử dụng một trình tạo kiểm tra truyền thống tổng hợp các kiểm tra từ mã, chúng tôi sử dụng Code LLM để tạo ra các ca kiểm tra bằng cách nhắc mô hình tạo ra một assertion. Code LLM điều kiện trên văn bản mã nguồn và do đó phục vụ như một detector yếu cho sự không nhất quán giữa mã và bình luận. Ví dụ, Hình 8 cho thấy một hàm sẽ là một mục huấn luyện tồi: mã đếm Y, nhưng có bình luận nói rằng nó đếm X. Khi chúng tôi tạo ra một số ca kiểm tra độc lập, chúng tôi kết thúc với các kiểm tra cho cả X và Y, điều này làm giảm tỷ lệ kiểm tra vượt qua. Điều kiện trên văn bản làm cho khả năng tạo ra các kiểm tra không nhất quán ít hơn, giảm khả năng các hàm sẽ bị lọc ra khỏi tập huấn luyện do độ bao phủ thấp (như mô tả bên dưới).

Chúng tôi nhắc StarCoder-15B để tạo ra năm bộ kiểm tra độc lập cho mỗi hàm với nhiệt độ cao (0.8) để có được một tập hợp đa dạng các kiểm tra ứng viên. Chúng tôi phân tích mỗi bộ kiểm tra được tạo ra và trích xuất tất cả các ca kiểm tra phù hợp cho dịch bằng MultiPL-E. Chúng tôi lấy tập hợp các ca kiểm tra khớp và chạy mỗi kiểm tra riêng biệt trong một container để xác minh rằng nó vượt qua, loại bỏ bất kỳ cái nào thất bại. Nếu không có kiểm tra đúng nào được tạo ra, chúng tôi loại bỏ hàm. Kết quả là một tập hợp gần 160,000 hàm Python với ít nhất một ca kiểm tra vượt qua. Hình 7a cho thấy phân phối kích thước bộ kiểm tra. Số trung vị ca kiểm tra mỗi hàm là 7, và trung bình là 12.1. Lưu ý rằng chúng tôi không lọc bất kỳ kiểm tra nào sau khi xác thực, vì vậy một hàm có thể có nhiều kiểm tra hơn thực sự cần thiết.

**Lọc trên độ bao phủ kiểm tra.** Cho bộ dữ liệu hàm Python với docstrings và bộ kiểm tra, bước tiếp theo của chúng tôi là lọc ra các hàm có độ bao phủ kiểm tra thấp. Chúng tôi sử dụng độ bao phủ dòng làm metric độ bao phủ và loại trừ tất cả các hàm có ít hơn 90% độ bao phủ dòng. Kết quả là một bộ dữ liệu 133,668 hàm với 90% độ bao phủ dòng từ kiểm tra.

Vì chúng tôi bắt đầu với gần 160,000 hàm, điều này ngụ ý rằng hầu hết các bộ kiểm tra được tạo ra hoạt động có độ bao phủ dòng cao. Thực tế, hầu hết các hàm có 100% độ bao phủ dòng (Hình 7b). Tiêu chí nghiêm ngặt này đảm bảo rằng các hàm trong tập cuối cùng của chúng tôi không chỉ đúng mà còn được kiểm tra toàn diện, củng cố độ tin cậy của bộ dữ liệu của chúng tôi. Trong bộ dữ liệu Python được lọc, hàm trung bình có 10.3 dòng mã (SD=10.7) và trung bình 3.6 nhánh (SD=4.1).

**Suy luận kiểu.** Các bước được mô tả ở trên đủ để tạo ra dữ liệu cho một ngôn ngữ đích ít tài nguyên không có kiểu (ví dụ, Racket hoặc Lua). Tuy nhiên, đối với một đích có kiểu (ví dụ, OCaml hoặc Julia), chúng tôi cũng cần suy luận kiểu vì hai lý do. Xem xét trường hợp chúng tôi nhắm đến OCaml. Đầu tiên, chúng tôi dựa vào LLM để tạo ra một thân hàm OCaml, chỉ cho một bình luận và header hàm (let f x =). Không có chú thích kiểu, tín hiệu duy nhất về kiểu mong muốn của f là tên định danh và bình luận. Với chú thích kiểu, LLM có khả năng cao hơn nhiều để tạo ra một hàm với kiểu mong đợi. Thứ hai, chúng tôi cần suy luận kiểu Python để biên dịch ca kiểm tra, mà chúng tôi minh họa trong Hình 9. Trong ví dụ này, chúng tôi có một hàm Python tiêu thụ một chuỗi tùy chọn. Chúng tôi có hai ca kiểm tra, một áp dụng hàm cho None và cái khác áp dụng hàm cho một chuỗi. Khi chúng tôi biên dịch những kiểm tra này sang OCaml, chúng tôi phải biến đổi kiểu đối số Python Union[str,None] thành kiểu OCaml string option. Chúng tôi chỉ có thể thực hiện biến đổi kiểu này sau khi chúng tôi đã suy luận kiểu Python.

Cách tiếp cận của chúng tôi đối với suy luận kiểu là đơn giản: chúng tôi suy ra kiểu dựa trên ca kiểm tra, bỏ qua thân hàm. Chúng tôi trích xuất kiểu instance của mỗi đối số và giá trị trả về mong đợi trong mỗi kiểm tra, tính toán kiểu union giữa các kiểu ở cùng vị trí giữa các kiểm tra. Ví dụ, nếu các ca kiểm tra áp dụng foo(1) và foo(None), chúng tôi suy luận Union[int, None] làm kiểu của đối số foo. Hơn nữa, chúng tôi đơn giản hóa Union[T, None] thành Optional[T] chuẩn hơn. Ví dụ, Union[int, int, None] sau đó sẽ được đơn giản hóa thành Optional[int]. Cách tiếp cận suy luận kiểu này chỉ có thể thất bại nếu mã không xác định, điều này không xảy ra trong bộ dữ liệu của chúng tôi.

Theo các bước trên tạo ra hai bộ dữ liệu hàm Python—một có và một không có chú thích kiểu—nơi mỗi hàm có một docstring và một bộ kiểm tra đơn vị đạt được độ bao phủ cao. Những bộ dữ liệu này có thể được sử dụng để tạo ra dữ liệu huấn luyện cho bất kỳ ngôn ngữ ít tài nguyên nào.

### 4.3 Dịch từ Ngôn ngữ Giàu Tài nguyên sang Ngôn ngữ Ít Tài nguyên

Cho một bộ dữ liệu các hàm Python có bình luận với bộ kiểm tra đơn vị độ bao phủ cao, mục tiêu tiếp theo của chúng tôi là dịch bộ dữ liệu từ Python sang một ngôn ngữ đích L và sử dụng kiểm tra để xác thực bản dịch.

**Dịch với Code LLM và MultiPL-E.** Chúng tôi sử dụng một phiên bản sửa đổi của MultiPL-E để dịch mỗi hàm Python thành một hàm tương đương trong ngôn ngữ đích ít tài nguyên. Chúng tôi xây dựng một prompt MultiPL-E với ba phần sau:

(1) **Docstring**: Chúng tôi biến docstring Python thành một bình luận trong ngôn ngữ đích. Toolchain MultiPL-E dịch giữa các định dạng bình luận khác nhau và cũng thay đổi tên kiểu phổ biến trong ngôn ngữ tự nhiên bằng các quy tắc đơn giản. Ví dụ, khi dịch từ Python sang OCaml, chúng tôi biến "dictionary" thành "association list".

(2) **Chữ ký hàm**: Chúng tôi biến chữ ký hàm Python thành chữ ký hàm trong ngôn ngữ đích. Bước này có thể liên quan đến việc dịch kiểu từ Python sang ngôn ngữ đích nếu chúng được yêu cầu.

(3) **Mã Python gốc**: Cuối cùng, chúng tôi thêm một bình luận (trong ngôn ngữ đích) chứa mã Python gốc. Chúng tôi thấy rằng thông tin bổ sung này tăng cơ hội mô hình tạo ra bản dịch đúng (§5.4.2).

Hình 6 làm nổi bật một ví dụ prompt và bộ kiểm tra để dịch một hàm sắp xếp giảm dần được viết bằng Python sang OCaml trong các chương trình được gán nhãn 1 và 2. MultiPL-E dịch các bình luận được viết bằng Python sang OCaml và dịch mỗi ca kiểm tra và chữ ký hàm từ Python sang OCaml. Mã Python gốc được thêm vào như một phần của bình luận.

Cho prompt này, chúng tôi sử dụng StarCoderBase-15B để tạo ra bản dịch của mỗi bài toán trong bộ dữ liệu Python của chúng tôi. Đối với tất cả ngôn ngữ của chúng tôi, chúng tôi tạo ra 50 bản dịch với nhiệt độ cao (0.8), để khuyến khích Code LLM tạo ra một tập hợp giải pháp ứng viên đa dạng hơn.

**Kiểm tra bản dịch với kiểm tra đã biên dịch.** Code LLM rất có khả năng tạo ra bản dịch lỗi; trong trường hợp của chúng tôi, điều này thậm chí có khả năng hơn, vì chúng tôi đặc biệt nhắm đến các ngôn ngữ mà Code LLM hoạt động kém. Chúng tôi giải quyết vấn đề này bằng cách dịch ca kiểm tra từ Python sang ngôn ngữ đích bằng một trình biên dịch đơn giản, đệ quy. MultiPL-E có một bộ trình biên dịch dịch các assertion Python đơn giản thành assertions trong 20+ ngôn ngữ lập trình khác. Các trình biên dịch hỗ trợ assertions là các đẳng thức đơn giản giữa các giá trị bậc nhất, cụ thể là dữ liệu Python nguyên tử và collections (lists, tuples, và dictionaries). Chúng tôi sử dụng những trình biên dịch này để dịch kiểm tra từ Python sang mỗi ngôn ngữ đích, loại bỏ các ca kiểm tra mà MultiPL-E không hỗ trợ. Nếu chúng tôi còn lại với không ca kiểm tra nào, chúng tôi loại bỏ hàm hoàn toàn.

Cho tập hợp 50 bản dịch được tạo ra cho mỗi hàm, chúng tôi chỉ chọn những giải pháp vượt qua tất cả kiểm tra. Điều này có thể bao gồm việc chọn một số giải pháp cho cùng một bài toán, điều này có lợi cho mô hình về mặt học các phong cách mã đa dạng.

### 4.4 Khử trùng lặp

Chúng tôi định nghĩa một tập hợp giải pháp là đa dạng khi chúng khác nhau về hình thức. Ví dụ, hai giải pháp hoạt động giống hệt nhau trên một bộ kiểm tra nhưng khác nhau trong triển khai của chúng–một sử dụng hàm đệ quy và cái khác một vòng lặp–được coi là đa dạng. Đảm bảo sự đa dạng giữa các giải pháp được tạo ra và xác minh là quan trọng để dạy các mô hình tinh chỉnh nhiều tính năng cú pháp và ngữ nghĩa của ngôn ngữ lập trình đích. Hơn nữa, các giải pháp dư thừa hoặc tương tự có thể làm giảm hiệu quả của một bộ dữ liệu.

Chỉ lấy mẫu lại với nhiệt độ cao không đảm bảo các giải pháp đa dạng: LLM vẫn có thể tạo ra các giải pháp gần như giống hệt nhau (ví dụ, với một số biến được đổi tên). Để giải quyết điều này, chúng tôi sử dụng một thuật toán khử trùng lặp dựa trên ROUGE-L. ROUGE-L là một metric chất lượng tóm tắt văn bản, và định lượng sự chồng chéo cú pháp giữa hai đoạn văn bản với một điểm số từ 0 đến 1 nơi 1 chỉ ra sự tương tự cao nhất. Chúng tôi sử dụng 0.6 làm ngưỡng tương tự để loại bỏ trùng lặp. Trước khi so sánh một cặp giải pháp, chúng tôi loại bỏ tất cả bình luận từ mã, vì nó có thể gây ra nhiễu trong quá trình khử trùng lặp.

Chạy ROUGE-L trên tất cả các cặp mục là cực kỳ đắt đỏ. Thay vào đó, chúng tôi sử dụng một heuristic phù hợp cho song song hóa (Thuật toán 1). Chúng tôi áp dụng ROUGE-L để khử trùng lặp các mục trong các nhóm có kích thước cố định (chúng tôi sử dụng kích thước 200). Ban đầu, các nhóm là giải pháp cho cùng một prompt, vì nhóm này có khả năng có nhiều trùng lặp. Sau đó chúng tôi nhóm lại các mục ngẫu nhiên và chạy khử trùng lặp theo nhóm một lần nữa. Số vòng khử trùng lặp tỷ lệ với tổng số mục: nhiều vòng hơn tăng khả năng trùng lặp sẽ được loại bỏ. Cuối cùng, điều này dẫn đến một tập hợp các giải pháp đa dạng, chính xác, và tương đương về mặt ngữ nghĩa cho mỗi prompt.

## 5 Đánh giá

Trong phần này, chúng tôi sử dụng MultiPL-T để tinh chỉnh nhiều mô hình khác nhau với kích thước khác nhau. Chúng tôi chứng minh kết quả tiên tiến trên các benchmark tiêu chuẩn cho nhiệm vụ ngôn ngữ tự nhiên sang mã (§5.2), đánh giá định tính và định lượng mới (§5.3), và ablations cho thấy tầm quan trọng của các quyết định thiết kế khác nhau trong MultiPL-T (§5.4).

### 5.1 Thiết lập Thí nghiệm và Triển khai

**Hyperparameters huấn luyện.** Chúng tôi tinh chỉnh tất cả mô hình với độ dài chuỗi 2,048 tokens. StarCoderBase-1B được tinh chỉnh cho bảy epochs với batch size 8, với learning rate 3×10^-5, 10 bước warmup, và cosine learning rate decay. Đối với StarCoderBase-15B, CodeLlama-34B, và CodeLlama-70B chúng tôi thực hiện những thay đổi cấu hình này: mười epochs, batch size 32, và learning rate 2×10^-5.

**Tài nguyên tính toán ước tính được sử dụng.** Công việc cho bài viết này được thực hiện trong vài tháng sử dụng GPU NVIDIA V100 (32GB), A100 (80GB), và H100 (80GB), khi chúng có sẵn trên một số cluster và server. Chúng tôi ước tính rằng chúng tôi đã dành khoảng 550 ngày thời gian GPU A100 (80GB) với phân tích sau:

• **Huấn luyện**: ~3,444 giờ tinh chỉnh một số phiên bản của CodeLlama-70B, CodeLlama-34B, StarCoderBase-15B, và StarCoderBase-1B. Bao gồm các mô hình được trình bày trong phần này và trong §3.

• **Đánh giá**: ~310 giờ chạy benchmarks, bao gồm MultiPL-E và benchmark học trong ngữ cảnh mới (§5.3.1).

• **Tạo Bộ dữ liệu**: ~9,984 giờ tạo ra các tập huấn luyện MultiPL-T. Đây là việc sử dụng tài nguyên đáng kể nhất nhưng có thể tái sử dụng cho phát triển mô hình tương lai. Sử dụng các mô hình lớn hơn StarCoderBase-15B để tạo ra sẽ tốn nhiều tài nguyên hơn đáng kể.

**Công cụ huấn luyện và đánh giá.** Chúng tôi thử nghiệm với một số công nghệ trong quá trình phát triển. Pipeline MultiPL-T cuối cùng sử dụng vLLM cho suy luận, DeepSpeed ZeRO để tinh chỉnh các mô hình lớn hơn, Transformers, và MultiPL-E với một số sửa đổi, như hỗ trợ OCaml.

### 5.2 Đánh giá trên Benchmarks Tiêu chuẩn

Hầu hết các benchmark Code LLM nhắm đến các ngôn ngữ lập trình giàu tài nguyên, chủ yếu là Python. Để đánh giá các LLMs tinh chỉnh của chúng tôi cho các ngôn ngữ ít tài nguyên, chúng tôi sử dụng và mở rộng MultiPL-E, là benchmark được sử dụng để đánh giá StarCoder, Code Llama, StableCode của Stability.ai, và một số Code LLMs khác. Như mô tả trong §2.2, MultiPL-E là một benchmark cho nhiệm vụ ngôn ngữ tự nhiên sang mã. Chúng tôi báo cáo kết quả MultiPL-E theo cách tiêu chuẩn: sử dụng metric pass@1, đơn giản là tỷ lệ vượt qua trung bình trên benchmark, nơi một completion vượt qua phải thực thi thành công tất cả kiểm tra.

**Chọn baselines và so sánh.** Trước khi trình bày kết quả, chúng tôi thảo luận về cách chúng tôi tin rằng các mô hình tinh chỉnh nên được đánh giá.

Điều được biết rõ là các mô hình lớn hơn hoạt động tốt hơn vì chúng có thể học các pattern phức tạp hơn. Hơn nữa, có một xu hướng gần đây của việc xây dựng các mô hình nhỏ hơn vượt trội hơn các mô hình lớn hơn bằng cách huấn luyện chúng trên nhiều dữ liệu hơn. Ví dụ, StarCoder2-15B cùng kích thước với StarCoderBase-15B, nhưng được huấn luyện trên 400% dữ liệu hơn và hoạt động tốt hơn 50% trên MultiPL-E.

Do đó, việc đạt được kết quả tinh chỉnh ấn tượng bằng cách chỉ đơn giản bắt đầu với một mô hình mới hơn, tốt hơn là tầm thường. Chúng tôi cho rằng cách đúng để đánh giá một cách tiếp cận tinh chỉnh là thực hiện tất cả những điều sau:

(1) So sánh mô hình tinh chỉnh mới với mô hình cơ sở. Điều này cho phép chúng ta hỏi, cách tiếp cận tinh chỉnh cải thiện hiệu suất baseline cho một mô hình cụ thể như thế nào?

(2) So sánh mô hình tinh chỉnh mới với các bản tinh chỉnh khác của cùng mô hình. Tinh chỉnh trên một bộ dữ liệu lớn có khả năng làm tốt hơn tinh chỉnh trên một bộ dữ liệu nhỏ hơn. Nhưng, điều này cho phép chúng ta hỏi, Một cách tiếp cận tinh chỉnh hiệu quả như thế nào so với cách khác?

(3) Tinh chỉnh nhiều mô hình cơ sở. Điều này cho phép chúng ta hỏi, cách tiếp cận tinh chỉnh có tổng quát hóa sang các kích thước mô hình và họ khác nhau không?

Do đó một cách tiếp cận tinh chỉnh nên cải thiện hiệu suất baseline trên một số họ mô hình khác nhau và qua một số kích thước mô hình, và chúng tôi sẽ chỉ ra rằng điều này đúng cho MultiPL-T.

#### 5.2.1 Tinh chỉnh StarCoderBase và Code Llama

Tinh chỉnh StarCoderBase và Code Llama với dữ liệu được tạo bởi MultiPL-T trên các ngôn ngữ đích của chúng tôi cải thiện hiệu suất trên MultiPL-E trên toàn bộ (Bảng 2). Đối với mỗi mô hình, chúng tôi tinh chỉnh một mô hình riêng biệt cho Julia, Lua, OCaml, R, và Racket. Chúng tôi checkpoint và đánh giá các mô hình ở mỗi epoch và báo cáo hiệu suất đỉnh. Không phải là mục tiêu của bài báo này để tối đa hóa điểm số MultiPL-E. Thực tế, phần tiếp theo gợi ý rằng sẽ dễ dàng cải thiện một số điểm số này bằng cách huấn luyện lâu hơn trên dữ liệu chúng tôi đã có hoặc bằng cách để MultiPL-T tạo ra thêm dữ liệu. Ngoài hiệu suất được cải thiện nói chung, chúng tôi rút ra một số kết luận khác:

(1) Racket và OCaml, là các ngôn ngữ ít tài nguyên nhất mà chúng tôi đánh giá, cho thấy những cải thiện tương đối lớn nhất. Ví dụ, các phiên bản tinh chỉnh của cả hai mô hình có điểm số gấp hơn đôi mô hình cơ sở của chúng, với những cải thiện đặc biệt lớn cho OCaml. Thậm chí các mô hình tinh chỉnh 1B hoạt động tương đương với các mô hình cơ sở 15B trong việc tạo ra các giải pháp đúng trong những ngôn ngữ này.

(2) Lua đạt được cải thiện tương đối 42% cho 1B và 17% cho 15B. Tuy nhiên, những cải thiện này đáng kể và đặt hiệu suất Lua của các mô hình tinh chỉnh ngang bằng với hiệu suất của các mô hình cơ sở trên các ngôn ngữ giàu tài nguyên nhất. Ví dụ, StarCoderBase-15B đạt 30.6 trên MultiPL-Python, và mô hình tinh chỉnh của chúng tôi đạt 31.0 trên MultiPL-Lua.

(3) Julia cũng cho thấy một cải thiện tương đối đáng kể 67% cho 15B, đạt được điểm số trên MultiPL-Lua vượt quá điểm số MultiPL-Python của mô hình cơ sở.

(4) Chúng tôi cũng tinh chỉnh và đánh giá CodeLlama-34B và CodeLlama-70B, nơi chúng tôi thấy những cải thiện đáng kể.

(5) Trái ngược với chưng cất, nơi một mô hình lớn hơn tạo ra dữ liệu huấn luyện cho một mô hình nhỏ, cách tiếp cận MultiPL-T cho phép một mô hình nhỏ hơn cải thiện hiệu suất của các mô hình lớn hơn nhiều. Cụ thể, chúng tôi sử dụng dữ liệu được tạo bởi StarCoderBase-15B và được xác thực bằng cách tiếp cận MultiPL-T để cải thiện hiệu suất của các mô hình gần 5 lần lớn hơn.

Tài liệu bổ sung bao gồm một đánh giá với benchmark MBPP. Trên MBPP, chúng tôi chỉ ra những cải thiện tương đối thậm chí lớn hơn với MultiPL-T.

#### 5.2.2 Tinh chỉnh StarCoder2 và DeepSeek Coder

Khi công việc này được thực hiện, Code Llama là Code LLM cơ sở hoạt động tốt nhất với trọng số mô hình mở. Tuy nhiên, hai mô hình cơ sở mới được phát hành gần đây: DeepSeek Coder và StarCoder2. Để chỉ ra rằng MultiPL-T vẫn có liên quan, chúng tôi tinh chỉnh DeepSeekCoder-33B và StarCoder2-15B trên bộ dữ liệu Racket của chúng tôi. Các mô hình nói trên là hai mô hình cơ sở hoạt động tốt nhất với kích thước của chúng tại thời điểm bài viết này được viết. Trong cả hai trường hợp, chúng tôi thấy rằng cách tiếp cận MultiPL-T tiếp tục hoạt động. Cả hai mô hình đều cho thấy cải thiện đáng kể trong khả năng Racket của chúng (Bảng 3).

Các bộ dữ liệu được trình bày trong bài viết này là một phần của corpus pretraining StarCoder2 như một trong số một số nguồn dữ liệu "(Leandro's) High Quality". Tất cả những gì chúng tôi có thể kết luận là chúng không làm hại hiệu suất trên các ngôn ngữ đích và việc tinh chỉnh thêm vẫn giúp ích (tương phản với §3.1).

#### 5.2.3 Tóm tắt

Sử dụng MultiPL-T, chúng tôi đã tinh chỉnh các mô hình từ bốn họ mô hình khác nhau và một phạm vi rộng kích thước mô hình (1B đến 70B tham số). Chúng tôi thấy rằng các mô hình tinh chỉnh của chúng tôi vượt trội hơn các mô hình cơ sở trên toàn bộ, với những cải thiện đáng kể nhất cho các ngôn ngữ ít tài nguyên nhất và các mô hình nhỏ đến trung bình.

Các mô hình lớn nhất (70B) không cho thấy nhiều như các mô hình nhỏ nhất. Tuy nhiên, các bộ dữ liệu của chúng tôi được tạo ra với một mô hình nhỏ hơn nhiều (15B), và việc chúng tôi có thể sử dụng một mô hình nhỏ hơn để cải thiện hiệu suất của một mô hình lớn hơn nhiều là bất thường.

Tại thời điểm viết, phiên bản tinh chỉnh DeepSeek Coder của chúng tôi vượt trội hơn mọi mô hình tinh chỉnh trên BigCode Models Leaderboard cho Racket. Các mô hình leaderboard được tinh chỉnh trên một bậc độ lớn dữ liệu hơn so với chúng tôi, cũng như các mô hình chưng cất các mô hình độc quyền như GPT-4. Do đó chúng tôi kết luận rằng đối với các ngôn ngữ ít tài nguyên, MultiPL-T hiệu quả hơn đáng kể về mặt dữ liệu so với các cách tiếp cận tinh chỉnh thay thế.

### 5.3 Nhiệm vụ Mới và Đánh giá Định tính

Phần trước sử dụng MultiPL-E để đánh giá, có một định dạng rất cụ thể (§2.2) và sử dụng kiểm tra đơn vị để kiểm tra tính đúng đắn. Code LLMs linh hoạt hơn nhiều và có các tiêu chí có thể chấp nhận không thể được ghi lại bằng kiểm tra đơn vị hoặc định dạng cụ thể. Chúng tôi giải quyết những điều này trong phần này.

#### 5.3.1 Đánh giá Học trong Ngữ cảnh

Một hạn chế của các bộ dữ liệu MultiPL-T là mỗi mục huấn luyện là một hàm đơn mà không có ngữ cảnh nào khác: chúng có thể sử dụng thư viện chuẩn, nhưng không thể phụ thuộc vào các hàm, lớp, hoặc thư viện bên thứ ba khác. Do đó có thể tin rằng việc tinh chỉnh một mô hình trên dữ liệu MultiPL-T sẽ làm cho nó overfit với định dạng này. Hơn nữa, các benchmark Code LLM thông thường, bao gồm MultiPL-E, sẽ không phơi bày vấn đề này, vì các nhiệm vụ benchmark chủ yếu liên quan đến việc tạo ra các hàm độc lập chỉ sử dụng thư viện chuẩn.

Để xác định xem loại overfitting này có phải là vấn đề không, chúng tôi xây dựng một benchmark đa ngôn ngữ mới với mười bốn bài toán. Chúng tôi quyết định xây dựng thủ công mỗi bài toán thay vì lấy nguồn từ các repositories để đảm bảo rằng các bài toán tương tự qua các ngôn ngữ, và quan trọng hơn, rằng chúng không phải là một phần của dữ liệu huấn luyện. Mỗi bài toán có các ca kiểm tra ẩn và một prompt tập luyện khả năng của mô hình sử dụng các kiểu do người dùng định nghĩa, hàm bậc cao, hàm helper, hoặc thư viện bên ngoài (Bảng 5). Chúng tôi dịch thủ công những prompts này thành OCaml, Racket, và Lua idiomatic.

Chúng tôi đánh giá StarCoderBase 1B và 15B trên benchmark mới này (Bảng 4). Kết quả gợi ý rằng mô hình OCaml-tuned 15B có thể đã overfit với định dạng MultiPL-T. Tuy nhiên, tất cả các mô hình tinh chỉnh khác làm tương tự hoặc tốt hơn. Chúng tôi suy đoán rằng tinh chỉnh trên một hỗn hợp dữ liệu tự nhiên và MultiPL-T sẽ giảm khả năng overfitting.

#### 5.3.2 Đánh giá Phong cách Lập trình

Một hạn chế tiềm năng của MultiPL-T là nó có thể tác động tiêu cực đến phong cách của mã được tạo ra, vì các mục huấn luyện được dịch từ Python. Chúng tôi nghiên cứu vấn đề này trong Racket bằng một quy trình đánh giá định tính. Chúng tôi phát triển một rubric chấm điểm phong cách Racket (Bảng 6) dựa trên hướng dẫn phong cách Racket và kinh nghiệm của chúng tôi trong việc dạy và chấm điểm các bài tập lập trình Racket. Rubric phác thảo các mục chấm điểm và các khoản khấu trừ tương ứng của chúng. Một số khoản khấu trừ được thiết kế cho các vấn đề phong cách phát sinh trong mã được viết bởi học sinh mới bắt đầu, như các dòng dài không cần thiết. Những cái khác phạt phong cách Lisp, như sử dụng car và cdr thay vì first và rest. Cuối cùng, khoản khấu trừ lớn nhất là cho việc sử dụng lặp mệnh lệnh khi một giải pháp hàm đơn giản có thể.

Chúng tôi sử dụng rubric của mình để chấm điểm các giải pháp HumanEval được tạo ra bởi StarCoderBase-1B trước và sau khi tinh chỉnh trên dữ liệu Racket MultiPL-T. Chúng tôi chỉ chấm điểm 35 bài toán mà cả hai mô hình có thể giải quyết ít nhất một lần (do đó chúng tôi bỏ qua một số bài toán chỉ có thể giải quyết sau khi tinh chỉnh). Vì chúng tôi tạo ra một số giải pháp cho mỗi bài toán, chúng tôi chọn giải pháp hoạt động phổ biến nhất, và trong trường hợp hòa, chúng tôi chọn cái có phong cách tốt nhất. Chúng tôi chấm điểm 70 chương trình Racket tổng cộng: 35 được tạo ra bởi mô hình cơ sở và 35 sau khi tinh chỉnh. Để tránh thiên vị, chúng tôi sử dụng hai người chấm điểm và ẩn danh quá trình lựa chọn và chấm điểm bằng cách gán ID ngẫu nhiên cho mỗi chương trình và theo dõi nguồn gốc của chúng trên một bảng tính ẩn. Hai người chấm điểm có kinh nghiệm dạy Racket đáng kể và chúng tôi thấy sự khác biệt tối thiểu giữa họ: điểm số của họ khác nhau hơn 1 điểm chỉ cho 15 trong số 70 chương trình ứng viên, và chỉ khác nhau 3–4 điểm cho hai chương trình.

Chúng tôi tính điểm tổng thể trung bình cho mô hình cơ sở và mô hình MultiPL-T trên 35 bài toán HumanEval. Chúng tôi thấy rằng mô hình cơ sở đạt được điểm phong cách 89.5% và mô hình tinh chỉnh 85.2%. Nói cách khác, điểm trung bình cho một chương trình mô hình cơ sở là 13.4/15 trong khi cho một mô hình tinh chỉnh là 12.8/15. Do đó tinh chỉnh dẫn đến một sự giảm nhẹ trong điểm phong cách Racket của chúng tôi.

Chúng tôi kiểm tra 17 chương trình đạt điểm cao hơn cho mô hình cơ sở so với mô hình tinh chỉnh. Chúng tôi thấy rằng mô hình tinh chỉnh có nhiều khả năng sử dụng các biểu thức if lồng nhau trong những chương trình này, cũng như thực hiện lặp nơi đệ quy có sẵn. Ngược lại, chúng tôi kiểm tra tám chương trình đạt điểm cao hơn trong mô hình MultiPL-T và thấy rằng mô hình cơ sở có nhiều khả năng sử dụng đệ quy trực tiếp với car/cdr trong khi mô hình tinh chỉnh sử dụng các abstraction danh sách Racket. Hình 11 cho thấy sự phân tích các loại khoản khấu trừ được gán cho mỗi bài toán cho mỗi mô hình, nơi các khoản khấu trừ được trung bình hóa giữa các người chấm điểm. Hình 12 cho thấy một cặp được chấm điểm nơi mô hình cơ sở đạt điểm tốt hơn về phong cách lập trình so với mô hình tinh chỉnh, và tài liệu bổ sung cho thấy một cặp được chấm điểm nơi điều ngược lại đúng: mô hình tinh chỉnh tạo ra một giải pháp sạch hơn so với mô hình cơ sở.

Phong cách lập trình sang một bên, trong cả hai trường hợp giải pháp chất lượng thấp hơn dài dòng hơn cần thiết nhiều.

Kết quả đánh giá của chúng tôi nhất quán với cách chúng tôi tạo ra dữ liệu huấn luyện từ mã Python. Nhìn chung, mặc dù tinh chỉnh làm giảm nhẹ khả năng của mô hình tạo ra mã Racket idiomatic, nó cũng tăng đáng kể khả năng tạo ra mã Racket đúng như chúng tôi đã chỉ ra trước đó. Hơn nữa, cả hai mô hình hoạt động tốt trên rubric phong cách Racket, điều này gợi ý rằng sự đánh đổi là tối thiểu.

### 5.4 Ablations MultiPL-T

Phần cuối cùng của đánh giá của chúng tôi là ablations để chứng minh hiệu quả của các phần khác nhau của pipeline MultiPL-T. Phần này trình bày kết quả cho một số lượng đáng kể các mô hình tinh chỉnh. Để quản lý chi phí, chúng tôi sử dụng StarCoderBase-1B cho phần lớn các thí nghiệm.

#### 5.4.1 Hiệu quả Tinh chỉnh

Chúng tôi bây giờ điều tra cách hiệu suất của StarCoderBase-1B trên MultiPL-E thay đổi trong quá trình tinh chỉnh. Trên ba ngôn ngữ (Lua, OCaml, và Racket), chúng tôi tinh chỉnh StarCoderBase-1B cho bảy epochs dữ liệu MultiPL-T và đánh giá ở mỗi epoch. Tuy nhiên, các bộ dữ liệu mà chúng tôi đã tạo ra không cân bằng về số lượng mục huấn luyện (Bảng 8). Để cân bằng tốt hơn các tập huấn luyện, chúng tôi lấy mẫu ngẫu nhiên 25,000 mục từ mỗi bộ dữ liệu để có các tập tinh chỉnh có kích thước tương tự cho mỗi ngôn ngữ.

Trong Hình 10, chúng tôi thấy rằng hiệu suất tăng đáng kể sau một epoch duy nhất của dữ liệu MultiPL-T cho các ngôn ngữ ít tài nguyên nhất (Racket và OCaml). Tuy nhiên, một ngôn ngữ giàu tài nguyên hơn (Lua) yêu cầu nhiều dữ liệu hơn để nhận ra thậm chí những cải thiện khiêm tốn. Những kết quả này là những gì người ta mong đợi: các ngôn ngữ ít tài nguyên hơn tận hưởng những cải thiện dễ dàng và hiệu quả từ tinh chỉnh với dữ liệu MultiPL-T.

Mặt khác, MultiPL-T yêu cầu nhiều tài nguyên tính toán hơn để tạo ra dữ liệu cho các ngôn ngữ ít tài nguyên, vì vậy khoảng cách hiệu quả tổng thể giữa các ngôn ngữ hẹp hơn so với hình vẽ gợi ý: nó không hiển thị chi phí tạo ra dữ liệu huấn luyện. Tuy nhiên, dữ liệu MultiPL-T chỉ cần được tạo ra một lần cho một ngôn ngữ nhất định và sau đó có thể được tái sử dụng để huấn luyện nhiều mô hình (§5.2).

#### 5.4.2 Dịch Versus Tạo sinh

MultiPL-T sử dụng StarCoderBase-15B để dịch các mục huấn luyện từ Python sang các ngôn ngữ ít tài nguyên. Tỷ lệ thành công của bản dịch này phụ thuộc vào cả chất lượng của mô hình đã được huấn luyện trước (được cố định) và chất lượng của prompt (mà chúng tôi thiết kế). Chúng tôi đã thử một số biến thể prompt trong quá trình phát triển và cuối cùng chọn một prompt bao gồm mã Python gốc trong một bình luận (§4.3).

Thực hiện một ablation hoàn chỉnh với tất cả năm ngôn ngữ và ~133,000 hàm sẽ cực kỳ đắt đỏ. Thay vào đó, chúng tôi chạy một thí nghiệm với một mẫu ngẫu nhiên 1,000 hàm Python nguồn. Chúng tôi sử dụng LLM để dịch 1,000 hàm này sang OCaml, Racket, và Lua với hai định dạng prompt khác nhau: có và không có nguồn Python gốc. Chúng tôi tạo ra 50 ứng viên cho mỗi prompt. Chúng tôi đánh giá hiệu suất bằng pass@50, là khả năng mô hình tạo ra ít nhất một giải pháp đúng trong 50 lần thử.

Như được hiển thị trong Bảng 7, việc thêm Python gốc vào prompt tăng đáng kể khả năng dịch thành công sang Racket và Lua, nhưng làm giảm nhẹ khả năng dịch thành công sang OCaml. Chúng tôi chỉ có thể suy đoán về lý do tại sao điều này xảy ra: Python có thể đang làm cho mô hình bị hiểu lầm và OCaml có vẻ xa hơn với Python so với Racket hoặc Lua.

#### 5.4.3 Kiểm tra là Quan trọng cho MultiPL-T

MultiPL-T dựa vào kiểm tra để xác thực các mục huấn luyện được dịch bởi LLM. Chúng tôi lý luận rằng vì chúng tôi đang dịch sang các ngôn ngữ lập trình mà LLM hoạt động kém trong tổng hợp, nó cũng có khả năng hoạt động kém trong dịch. Nhưng, để xác thực tuyên bố này, chúng tôi tinh chỉnh các mô hình trên dữ liệu Racket mà không có xác thực kiểm tra. Chúng tôi có kết quả kém như mong đợi (Bảng 9). Điều này cho thấy rằng nỗ lực chúng tôi thực hiện để tạo ra, xác thực, và biên dịch kiểm tra là cần thiết cho MultiPL-T.

#### 5.4.4 Tác động của Khử trùng lặp

Công việc trước đây đã chỉ ra rằng sự trùng lặp dữ liệu làm giảm hiệu suất trong khi tăng thời gian huấn luyện. Để chứng minh tác động của nó trong công việc của chúng tôi, chúng tôi thực hiện một thí nghiệm trên bộ dữ liệu nơi nó có tác động nhiều nhất—nơi khử trùng lặp loại bỏ nhiều dữ liệu nhất—đó là cho Lua. Điều này được mong đợi vì số lượng hàm gần như trùng lặp sẽ cao hơn khi LLM tạo dữ liệu dịch các hàm với tỷ lệ thành công cao hơn. Trong số các ngôn ngữ chúng tôi nhắm đến, LLM tạo dữ liệu của chúng tôi (StarCoderBase15-B) có tỷ lệ vượt qua cao nhất trên Lua.

Trước khử trùng lặp, bộ dữ liệu Lua có 1.4M hàm, nhưng sau khử trùng lặp chúng tôi còn lại với 48K (được báo cáo trong Bảng 8). Đối với thí nghiệm này, chúng tôi tinh chỉnh StarCoderBase-1B trên một GPU H100 duy nhất trên bộ dữ liệu chưa khử trùng lặp, với chính xác cùng hyperparameters chúng tôi đã sử dụng trước đó (§5). Chúng tôi thấy điều sau:

(1) Không có khử trùng lặp, StarCoderBase-1B tinh chỉnh có được điểm pass@1 17.1 trên MultiPL-E và mất 12 giờ để huấn luyện.

(2) Với khử trùng lặp, StarCoderBase-1B tinh chỉnh có được 17.3 pass@1 trên MultiPL-E và mất ít hơn 30 phút để huấn luyện.

Hai điểm pass@1 rất gần nhau. Nhưng, không có khử trùng lặp, huấn luyện mất đáng kể lâu hơn, vì sự gia tăng trong kích thước bộ dữ liệu.

Tác động sẽ đáng kể hơn với một mô hình lớn hơn. Ví dụ, chúng tôi mất 8 giờ trên 8xH100 GPUs để tinh chỉnh StarCoderBase-15B với một bộ dữ liệu đã khử trùng lặp. Không có khử trùng lặp, tinh chỉnh sẽ mất vài ngày. Tương tự, tinh chỉnh các mô hình 33B và 70B sẽ mất thậm chí lâu hơn. Do đó khử trùng lặp là một cách để quản lý thời gian và chi phí huấn luyện các mô hình lớn.

#### 5.4.5 Nhắm đến Ngôn ngữ Nguồn

Toolchain MultiPL-T hoàn chỉnh được thiết kế để tạo ra dữ liệu huấn luyện cho một ngôn ngữ đích ít tài nguyên từ một ngôn ngữ nguồn (Python). Tuy nhiên, cũng có thể đánh giá trực tiếp phần đầu tiên của toolchain MultiPL-T, xây dựng một bộ dữ liệu chất lượng cao cho ngôn ngữ nguồn (Python). Trong phần này, chúng tôi tinh chỉnh các LLMs được chọn trên Python bằng bộ dữ liệu 133,668 hàm Python được ghi chép, kiểm tra, có thể định kiểu, và được lọc trên độ bao phủ kiểm tra (§§ 4.1 và 4.2).

Chúng tôi tinh chỉnh và đánh giá StarCoderBase-1B trực tiếp trên bộ dữ liệu hàm Python này. Mô hình cơ sở có được điểm pass@1 15.1 trên Python, trong khi mô hình tinh chỉnh có được 15.0 pass@1. Do đó, rõ ràng là tinh chỉnh trực tiếp trên tập con Python này không tạo ra cùng những cải thiện mà chúng tôi thấy trên các ngôn ngữ ít tài nguyên sau khi dịch.

Chúng tôi giả thuyết rằng điều này xảy ra vì lý do sau: StarCoderBase-1B đã được huấn luyện trên ba epochs của dữ liệu Python này. Chúng tôi không tinh chỉnh mô hình trên dữ liệu mới, do đó có ít gì mới cho mô hình học, và điều này được phản ánh trong điểm pass@1 không thay đổi. Ngược lại, khi chúng tôi dịch dữ liệu này sang một ngôn ngữ ít tài nguyên bằng pipeline MultiPL-T hoàn chỉnh, chúng tôi đang hiệu quả tạo ra dữ liệu mới cho mô hình học.

## 6 Thảo luận

Chúng tôi đã chỉ ra rằng MultiPL-T là một phương pháp hiệu quả và hiệu suất để tạo ra dữ liệu huấn luyện bán tổng hợp cho các ngôn ngữ lập trình ít tài nguyên. Trong phần này, chúng tôi thảo luận về ý nghĩa của việc mở rộng MultiPL-T theo nhiều cách khác nhau.

**Tổng quát hóa sang các ngôn ngữ lập trình khác.** Chúng tôi hy vọng rõ ràng với người đọc rằng cách tiếp cận MultiPL-T đơn giản để tổng quát hóa sang nhiều ngôn ngữ lập trình hơn. Công việc cụ thể ngôn ngữ liên quan đến 1) dịch các bình luận và chữ ký hàm thành một prompt phù hợp, và 2) viết một trình biên dịch có thể dịch các assertion đơn giản từ nguồn sang đích. MultiPL-E đã hỗ trợ cả hai bước này cho 20+ ngôn ngữ lập trình, một số trong đó là ít tài nguyên, bao gồm D, Bash, MATLAB, Haskell, và Perl. Vì vậy, tạo ra các tập tinh chỉnh cho những ngôn ngữ này có thể chỉ là vấn đề chạy pipeline MultiPL-T trong vài ngày trên GPUs.

Một số kết hợp ngôn ngữ nguồn và đích có thể hiệu quả hơn những cái khác. Ví dụ, xem xét xây dựng dữ liệu huấn luyện bán tổng hợp cho Rust. Chúng tôi có thể trực tiếp tận dụng các bộ dữ liệu trong bài báo này và sử dụng Code LLM để dịch Python sang Rust. Tuy nhiên, chúng tôi suy đoán rằng sẽ hiệu quả hơn để sửa đổi MultiPL-T để hỗ trợ một ngôn ngữ nguồn gần với Rust hơn, như C++.

**Tổng quát hóa sang các LLMs khác.** Mặc dù bài báo này tập trung vào các bản tinh chỉnh của họ Code Llama và StarCoder của Code LLMs, các bộ dữ liệu của chúng tôi cũng có thể được sử dụng để tinh chỉnh các LLMs khác. Chúng tôi tinh chỉnh DeepSeek Coder và StarCoder2 trên Racket, và chúng tôi mong đợi kết quả của chúng tôi sẽ tổng quát hóa sang các ngôn ngữ khác.

**Tái tạo dữ liệu MultiPL-T.** Pipeline MultiPL-T trong bài báo này sử dụng StarCoderBase-15B. Rõ ràng là sử dụng một mô hình có khả năng hơn sẽ cải thiện tỷ lệ chuyển đổi, và do đó tạo ra kết quả tốt hơn. Một mô hình độc quyền như GPT-4 có khả năng tạo ra kết quả tốt nhất, nhưng làm như vậy sẽ vi phạm điều khoản sử dụng của nó. Một kết quả bất thường trong bài báo này là MultiPL-T có thể sử dụng một mô hình yếu hơn (StarCoderBase-15B) để cải thiện các mô hình tốt hơn nhiều (CodeLlama-70B, DeepSeekCoder-34B, và StarCoder2-15B) trên các ngôn ngữ ít tài nguyên.

**Ngôn ngữ "không tài nguyên".** MultiPL-T nhắm đến các ngôn ngữ ít tài nguyên, nhưng không có khả năng hoạt động như hiện tại cho các ngôn ngữ "không tài nguyên" mà StarCoderBase hoàn toàn không được huấn luyện. Có thể thông minh prompt mô hình với đủ thông tin về một ngôn ngữ không tài nguyên để bootstrap tạo dữ liệu. Nhưng, làm như vậy một cách hiệu quả có thể là thử thách.

**Khả năng kết hợp với tự hướng dẫn.** Mặc dù chúng tôi đã lập luận rằng tự hướng dẫn không có khả năng thành công trên một ngôn ngữ ít tài nguyên, tự hướng dẫn và MultiPL-T có thể được kết hợp với nhau một cách tự nhiên: có thể tạo ra một bộ dữ liệu chất lượng cao các hướng dẫn trong một ngôn ngữ giàu tài nguyên, và sau đó sử dụng MultiPL-T để dịch chúng sang một ngôn ngữ ít tài nguyên. Cho hiệu quả của WizardCoder và Magicoder ở Python, sự kết hợp này có vẻ có khả năng thành công.

**Các loại benchmark khác.** Rất nhiều nỗ lực đã được đưa vào việc đánh giá khả năng lập trình Python của LLMs, nhưng có ít benchmark hơn nhiều cho các ngôn ngữ ít tài nguyên. Ví dụ, có các benchmark Python cho các nhiệm vụ chuyên biệt, như khoa học dữ liệu, và với prompts được tác giả bởi các quần thể cụ thể, như lập trình viên mới bắt đầu. Chúng tôi cần phát triển những loại benchmark này cho các ngôn ngữ ít tài nguyên để thực sự hiểu khả năng và hạn chế của Code LLMs.

## 7 Công việc Liên quan

**Dịch mã với mô hình ngôn ngữ và kiểm tra đơn vị.** Một số dự án sử dụng mô hình ngôn ngữ để dịch mã giữa các ngôn ngữ lập trình và kiểm tra rằng các bản dịch được tạo ra là đúng bằng cách biên dịch các kiểm tra đơn vị hoạt động từ ngôn ngữ này sang ngôn ngữ khác. TransCoder-ST và CMTrans sử dụng những kỹ thuật này để tạo ra dữ liệu huấn luyện cho một mô hình dịch mã giữa Java, Python, và C++, trong khi MultiPL-E, MBXP, và BabelCode dịch các benchmark Code LLM từ Python sang 10+ ngôn ngữ lập trình. Một tính năng phân biệt của MultiPL-T là nó sử dụng một Code LLM đã được huấn luyện trước off-the-shelf (StarCoder) để cả tạo ra ca kiểm tra và dịch mã sang các ngôn ngữ ít tài nguyên. Khi những bài báo nói trên được viết, các Code LLMs mở tốt nhất có khả năng thấp hơn StarCoder rất nhiều: chúng được huấn luyện trên ít ngôn ngữ lập trình hơn sử dụng ít dữ liệu huấn luyện hơn, và chúng nhỏ hơn một bậc độ lớn. Do đó chúng tôi tin rằng cách tiếp cận MultiPL-T có khả năng đã thất bại. Mặc dù các mô hình nguồn đóng có khả năng có sẵn, chúng hoặc bị giới hạn tỷ lệ hoặc cực kỳ đắt đỏ cho quy mô tạo dữ liệu mà MultiPL-T cần. Ví dụ, Cassano et al. báo cáo rằng họ đã sử dụng một mô hình thương mại trong thời gian beta miễn phí, nhưng nó sẽ có giá $37,000 với các mô hình được phát hành tương đương. Tạo dữ liệu MultiPL-T yêu cầu một bậc độ lớn nhiều truy vấn hơn, làm cho nó cực kỳ đắt đỏ để sử dụng với các mô hình thương mại ở giá 2023.

**Mô hình ngôn ngữ đa ngôn ngữ về lập trình.** Huấn luyện mô hình trên một bộ dữ liệu mã được viết trong các ngôn ngữ lập trình tương tự với ngôn ngữ đích, được biết đến như học chuyển giao, là một phương pháp được nghiên cứu phổ biến để cải thiện hiệu suất mô hình trên một ngôn ngữ lập trình cụ thể. Chen et al. khám phá phương pháp này trong bối cảnh các ngôn ngữ ít tài nguyên bằng cách pretraining và tinh chỉnh các Transformers encoder-only nhỏ với các ngôn ngữ lập trình khác nhau. Sau đó họ đánh giá hiệu suất của chúng trên các nhiệm vụ tìm kiếm và tóm tắt mã cho Ruby, một ngôn ngữ có bộ dữ liệu nhỏ hơn 10 lần so với Python. Những phát hiện của họ gợi ý rằng pretraining đa ngôn ngữ có thể cải thiện hiệu suất tạo mã trong các ngôn ngữ ít tài nguyên. Tuy nhiên, chiến lược này có thể thất bại với các ngôn ngữ khác biệt đáng kể về cú pháp hoặc ngữ nghĩa, như Racket. Ví dụ, Baltaji et al. sử dụng một phương pháp tương tự để huấn luyện mô hình trên 41 cặp ngôn ngữ lập trình khác nhau, lưu ý rằng Scheme, tiền thân của Racket, hưởng lợi ít hơn từ học chuyển giao so với các ngôn ngữ khác. Trong những tình huống như vậy, các chiến lược tạo dữ liệu như MultiPL-T được coi là có khả năng đạt được thành công cao hơn.

Hơn nữa, những nghiên cứu này chủ yếu dựa vào điểm BLEU và các metric dựa trên cú pháp khác để đánh giá tính đúng đắn, được chỉ trích vì sự không đầy đủ của chúng trong các nhiệm vụ tạo mã. Trong công việc của chúng tôi, chúng tôi sử dụng xác thực dựa trên kiểm tra của cả dữ liệu huấn luyện và metrics đánh giá để đảm bảo tính đúng đắn của mã được tạo ra, là một thước đo đáng tin cậy hơn về hiệu suất mô hình.

Công việc của chúng tôi bắt đầu với quan sát rằng hiệu suất Code LLM có thể thay đổi đáng kể theo ngôn ngữ, và điều này đã được quan sát lặp lại trong công việc trước đây. Gần nhất với MultiPL-T là các benchmark cho nhiệm vụ ngôn ngữ tự nhiên sang mã. Tuy nhiên, các xu hướng tương tự xảy ra khi Code LLMs được sử dụng cho các nhiệm vụ khác, như fuzzing và dịch.

**Tinh chỉnh hướng dẫn.** Để có được một LLM thực hiện một nhiệm vụ mong muốn, người dùng phải prompt nó theo cách đúng. Có một số kỹ thuật để tinh chỉnh hướng dẫn LLMs để tuân theo tốt hơn các hướng dẫn tự nhiên, do con người viết. Một cách tiếp cận sử dụng các chú thích viên con người để viết các hướng dẫn mẫu và đưa ra phản hồi về một số lượng lớn các thế hệ mô hình, nhưng điều này đắt đỏ và yêu cầu tài nguyên đáng kể. Một cách tiếp cận rẻ hơn là có một LLM có khả năng tự hướng dẫn để tạo ra hướng dẫn từ một tập hợp tương đối nhỏ các hướng dẫn seed do con người viết. Evol-Instruct sử dụng một LLM để tạo ra các biến thể của hướng dẫn. Những kỹ thuật này đã được sử dụng để tạo ra các bộ dữ liệu để tinh chỉnh hướng dẫn Code LLMs. Những bộ dữ liệu này tập trung vào các ngôn ngữ giàu tài nguyên, và, như chúng tôi chỉ ra trong §3.2, chúng không có khả năng thành công cho các ngôn ngữ ít tài nguyên.

**Huấn luyện trên dữ liệu chất lượng cao.** Huấn luyện trên dữ liệu chất lượng cao là một cách hiệu quả để giảm cả kích thước của một LLM và khối lượng dữ liệu huấn luyện cần thiết, trong khi duy trì hiệu suất. Gunasekar et al. đạt được điểm HumanEval cao trên một mô hình nhỏ với một lượng khiêm tốn dữ liệu huấn luyện "chất lượng sách giáo khoa". Điều này bao gồm cả dữ liệu tự nhiên và tổng hợp được tạo ra bởi một mô hình có khả năng hơn. Công việc của họ nhắm đến Python, và chúng tôi lập luận trong §3 rằng cách tiếp cận ít có khả năng thành công với các ngôn ngữ ít tài nguyên.

**Code LLMs độc quyền.** Tại thời điểm viết, có các LLMs độc quyền hoạt động tốt hơn ở các nhiệm vụ lập trình so với các mô hình mở mà chúng tôi xây dựng dựa trên. Tuy nhiên, hầu hết những mô hình này chỉ hỗ trợ suy luận (tức là, chạy mô hình đã được huấn luyện) và không huấn luyện hoặc tinh chỉnh. Thậm chí khi tinh chỉnh có thể, vì những mô hình này được huấn luyện trên các tập dữ liệu đóng, chúng tôi sẽ không thể so sánh MultiPL-T với baseline tự nhiên của việc huấn luyện lâu hơn trên dữ liệu hiện có. Hơn nữa, một hạn chế đáng kể phát sinh từ các ràng buộc cấp phép độc quyền của những mô hình này. Nhiều giấy phép của chúng rõ ràng cấm sử dụng dữ liệu được tạo ra để huấn luyện các mô hình khác.

Thậm chí mặc dù, nhiều mô hình độc quyền cấm sử dụng dữ liệu được tạo ra của chúng để huấn luyện các mô hình khác, có các mô hình làm như vậy. Những mô hình này là chưng cất của các mô hình độc quyền lớn hơn nhiều, và không thể so sánh chúng với baseline tự nhiên, là huấn luyện lâu hơn trên dữ liệu hiện có, mà đối với những mô hình này là tập huấn luyện OpenAI độc quyền. Sức mạnh của cách tiếp cận MultiPL-T không phải là nó làm tốt hơn theo nghĩa tuyệt đối, mà là tinh chỉnh với dữ liệu MultiPL-T tốt hơn huấn luyện lâu hơn trên dữ liệu hiện có. Chúng tôi chỉ ra rằng điều này đúng cho StarCoderBase và tập huấn luyện của nó, và chỉ có thể làm như vậy vì cả hai đều mở.

## 8 Kết luận

Trong vài năm qua, Code LLMs đã nhanh chóng tiến vào ngày càng nhiều công cụ lập trình. Tuy nhiên, chất lượng và độ tin cậy của Code LLMs phụ thuộc cao vào ngôn ngữ: chúng có thể đáng chú ý trên các ngôn ngữ lập trình giàu tài nguyên, nhưng ít ấn tượng hơn khi làm việc với các ngôn ngữ ít tài nguyên. Có thể trong tương lai gần, một số lượng lớn lập trình viên sẽ mong đợi công nghệ dựa trên LLM chỉ hoạt động, giống như nhiều lập trình viên ngày nay mong đợi làm nổi bật cú pháp, phân tích liên tục, hoặc hoàn thành dựa trên kiểu trong môi trường lập trình của họ. Chúng tôi hy vọng rằng MultiPL-T—một phương pháp để tạo ra các bộ dữ liệu tinh chỉnh quy mô lớn, chất lượng cao trong các ngôn ngữ ít tài nguyên—sẽ giúp các ngôn ngữ ít tài nguyên cạnh tranh trong một thế giới nơi nhiều công cụ nhà phát triển dựa vào Code LLMs.

Dữ liệu tinh chỉnh MultiPL-T (và mã) cũng mở: chúng được xây dựng từ dữ liệu huấn luyện StarCoder (The Stack) và được tăng cường bởi chính StarCoder. Chúng tôi cố ý không sử dụng một mô hình độc quyền có khả năng hơn để tinh chỉnh một mô hình mở. Điều này cho phép chúng tôi chứng minh rằng tinh chỉnh trên dữ liệu MultiPL-T hiệu quả hơn và hiệu suất hơn huấn luyện lâu hơn trên dữ liệu hiện có. Chúng tôi đánh giá MultiPL-T theo một số cách khác, bao gồm một benchmark mới tập luyện học trong ngữ cảnh và một đánh giá định tính về phong cách lập trình. Khi so sánh với các bản tinh chỉnh khác của cùng mô hình cơ sở, MultiPL-T đạt được kết quả tiên tiến cho Julia, Lua, OCaml, R, và Racket.

## Tuyên bố Sẵn có Dữ liệu

Tất cả mã, dữ liệu, và mô hình từ bài báo này có sẵn với giấy phép mở. Mã: có sẵn trên GitHub và được lưu trữ trên Zenodo; Bộ dữ liệu: có sẵn trên Hugging Face; và Mô hình: có sẵn trên Hugging Face. Cụ thể:

(1) Một hướng dẫn để tái tạo kết quả trong bài viết này có sẵn tại doi.org/10.5281/zenodo.12453932.

(2) Các bộ dữ liệu MultiPL-T có sẵn tại doi.org/10.57967/hf/2941 với liên kết đến một số mô hình được tinh chỉnh trên những bộ dữ liệu này.

## Lời cảm ơn

Chúng tôi cảm ơn Loubna Ben Allal, Harm de Vries, Joydeep Biswas, Matthias Felleisen, Shriram Krishnamurthi, và Leandro von Werra cho những cuộc trò chuyện hữu ích. Cảm ơn Leandro von Werra vì đã bao gồm các bộ dữ liệu MultiPL-T trong corpus huấn luyện StarCoder2. Chúng tôi cũng cảm ơn các nhà phê bình OOPSLA cho phản hồi của họ. Chúng tôi đặc biệt cảm ơn ủy ban đánh giá artifact vì sự kiên nhẫn của họ trong việc xem xét artifact phức tạp của chúng tôi, cũng như các chủ tịch AEC (Guillaume Baudart và Sankha Narayan Guria) vì đã tạo điều kiện xem xét. Công việc này sử dụng tài nguyên tính toán được cung cấp bởi Northeastern Research Computing, New England Research Cloud, và Joydeep Biswas (UT Austin). Federico Cassano được liên kết với Roblox cho hầu hết công việc của anh ấy trên bài báo này. Công việc này được hỗ trợ một phần bởi National Science Foundation (SES-2326173, SES-2326174, và SES-2326175).
