# EarlyBIRD Bắt Được Bug: Về Việc Khai Thác Các Lớp Sớm của Các Mô Hình Encoder để Phân Loại Code Hiệu Quả Hơn

Anastasiia Grishina
anastasiia@simula.no
Simula Research Laboratory
Oslo, NorwayMax Hort
maxh@simula.no
Simula Research Laboratory
Oslo, NorwayLeon Moonen
leon.moonen@computer.org
Simula Research Laboratory &
BI Norwegian Business School
Oslo, Norway

TÓM TẮT
Việc sử dụng các kỹ thuật Xử Lý Ngôn Ngữ Tự Nhiên (NLP) hiện đại đã cho thấy có lợi cho các tác vụ kỹ thuật phần mềm, chẳng hạn như phát hiện lỗ hổng và suy luận kiểu. Tuy nhiên, việc huấn luyện các mô hình NLP sâu đòi hỏi tài nguyên tính toán đáng kể. Bài báo này khám phá các kỹ thuật nhằm đạt được việc sử dụng tài nguyên và thông tin có sẵn tốt nhất trong các mô hình này.

Chúng tôi đề xuất một phương pháp tổng quát, EarlyBIRD, để xây dựng các biểu diễn kết hợp của code từ các lớp sớm của một mô hình transformer được huấn luyện trước. Chúng tôi điều tra thực nghiệm tính khả thi của phương pháp này trên mô hình CodeBERT bằng cách so sánh hiệu suất của 12 chiến lược tạo biểu diễn kết hợp với thực hành tiêu chuẩn chỉ sử dụng lớp encoder cuối cùng.

Đánh giá của chúng tôi trên bốn bộ dữ liệu cho thấy một số kết hợp lớp sớm mang lại hiệu suất tốt hơn trong phát hiện lỗi, và một số kết hợp cải thiện phân loại đa lớp. Cụ thể hơn, chúng tôi đạt được cải thiện độ chính xác phát hiện trung bình +2 trên Devign chỉ với 3 trong số 12 lớp của CodeBERT và tăng tốc fine-tuning 3.3x. Những phát hiện này cho thấy các lớp sớm có thể được sử dụng để có được kết quả tốt hơn bằng cách sử dụng cùng tài nguyên, cũng như giảm sử dụng tài nguyên trong quá trình fine-tuning và suy luận.

KHÁI NIỆM CCS
•Phần mềm và kỹ thuật của nó ;•Phương pháp tính toán
→Mạng nơ-ron ;Xử lý ngôn ngữ tự nhiên ;

TỪ KHÓA
tính bền vững, tối ưu hóa mô hình, transformer, phân loại code, phát hiện lỗ hổng, AI4Code, AI4SE, ML4SE

1 GIỚI THIỆU
Tự động hóa các tác vụ kỹ thuật phần mềm (SE) hỗ trợ các nhà phát triển trong việc tạo và bảo trì mã nguồn. Gần đây, các mô hình học sâu (DL) đã được huấn luyện trên các corpus code nguồn mở lớn và được sử dụng để thực hiện các tác vụ phân tích code [3,8,27,38]. Được thúc đẩy bởi giả thuyết tính tự nhiên cho rằng code và ngôn ngữ tự nhiên có chung những tương đồng thống kê, các nhà nghiên cứu và nhà cung cấp công cụ đã bắt đầu huấn luyện các mô hình NLP sâu trên code và fine-tuning chúng cho các tác vụ SE [11]. Trong số những ứng dụng khác, các mô hình như vậy đã được áp dụng cho suy luận kiểu [17], phát hiện bản sao code [50], sửa chữa chương trình [9,15,47,48], và dự đoán lỗi [7,30,35,44]. Trong các phương pháp dựa trên NLP, các tác vụ SE thường được chuyển đổi thành các bài toán phân loại code. Ví dụ, phát hiện lỗ hổng phần mềm là một bài toán phân loại nhị phân, suy luận kiểu bug là một thiết lập phân loại đa lớp, và suy luận kiểu là một tác vụ phân loại đa nhãn đa lớp trong trường hợp một kiểu được dự đoán cho mỗi biến trong chương trình.

Hầu hết các mô hình NLP hiện đại được xây dựng trên kiến trúc transformer [42]. Kiến trúc này sử dụng cơ chế attention và bao gồm một encoder chuyển đổi một chuỗi đầu vào thành một biểu diễn thông qua một chuỗi các lớp, theo sau là các lớp decoder chuyển đổi biểu diễn này thành một chuỗi đầu ra. Mặc dù hiệu quả về khả năng học tập, thiết kế transformer dẫn đến các mô hình đa lớp cần lượng lớn dữ liệu để huấn luyện từ đầu. Một nhược điểm nổi tiếng của các mô hình này là việc sử dụng tài nguyên cao được yêu cầu cho huấn luyện do cả kích thước mô hình và dữ liệu. Trong khi một số mô hình được huấn luyện trước đã được công bố gần đây, việc fine-tuning các mô hình này cho các tác vụ cụ thể vẫn đòi hỏi tài nguyên tính toán bổ sung [27].

Bài báo này khám phá các kỹ thuật nhằm tối ưu hóa việc sử dụng tài nguyên và thông tin có sẵn trong các mô hình trong quá trình fine-tuning. Cụ thể, chúng tôi xem xét các mô hình white-box mở, mà từ đó có thể trích xuất trọng số từ mỗi lớp. Chúng tôi tập trung vào các mô hình chỉ có encoder, vì chúng thường được sử dụng cho các tác vụ phân loại SE, đặc biệt là các encoder dựa trên transformer. Thực hành tiêu chuẩn trong các mô hình encoder là lấy biểu diễn của chuỗi đầu vào từ lớp cuối cùng của mô hình [14], trong khi thông tin từ các lớp sớm hơn thường bị loại bỏ [21]. Tức là, trong khi các lớp sớm được sử dụng để tính toán các giá trị của lớp cuối cùng, chúng thường không được coi là các biểu diễn riêng lẻ của đầu vào theo cách mà lớp cuối cùng được coi. Để minh họa lượng thông tin bị loại bỏ tại thời điểm suy luận, khi fine-tuning một encoder 12 lớp, chẳng hạn như CodeBERT [14], để phát hiện bug, 92% embeddings code bị bỏ qua.¹ Tuy nhiên, đã được chứng minh đối với ngôn ngữ tự nhiên rằng các lớp sớm của một encoder nắm bắt các tính năng cú pháp cấp thấp tốt hơn các lớp sau [6,24,32,40], điều này có thể có lợi cho các tác vụ hạ nguồn.

Được truyền cảm hứng từ dòng nghiên cứu khai thác các lớp sớm của mô hình, chúng tôi đề xuất EarlyBIRD,² một phương pháp mới và tổng quát để xây dựng các biểu diễn kết hợp từ các lớp sớm của một mô hình encoder được huấn luyện trước. EarlyBIRD nhằm tận dụng tất cả thông tin có sẵn trong các mô hình encoder được huấn luyện trước hiện có trong quá trình fine-tuning để cải thiện kết quả hoặc đạt được kết quả cạnh tranh với việc sử dụng tài nguyên giảm trong quá trình phân loại code. Chúng tôi đánh giá thực nghiệm EarlyBIRD trên CodeBERT [14], một mô hình encoder được huấn luyện trước phổ biến cho code, và bốn bộ dữ liệu benchmark bao gồm ba tác vụ SE phổ biến: phát hiện lỗi với các bộ dữ liệu Devign và ReVeal [20,51], suy luận kiểu bug với dữ liệu từ Yasunaga et al. [47], và phân loại kiểu exception [7]. Đánh giá so sánh biểu diễn baseline sử dụng lớp encoder cuối cùng với kết quả thu được qua EarlyBIRD. Chúng tôi cả fine-tune encoder kích thước đầy đủ và phiên bản được tỉa của nó chỉ với một số lớp sớm có mặt trong mô hình. Kịch bản sau phân tích sự đánh đổi giữa việc chỉ sử dụng một mô hình một phần và tác động hiệu suất đối với các tác vụ SE.

Đóng góp: Trong bài báo này, chúng tôi đưa ra những đóng góp sau:
(1) Chúng tôi đề xuất EarlyBIRD, một phương pháp tạo các biểu diễn kết hợp của code sử dụng các lớp sớm của một mô hình encoder dựa trên transformer. Mục tiêu là đạt được hiệu suất phân loại code tốt hơn với việc sử dụng tài nguyên bằng nhau hoặc hiệu suất tương đương với việc sử dụng tài nguyên thấp hơn.

(2) Chúng tôi tiến hành một đánh giá thực nghiệm kỹ lưỡng về phương pháp được đề xuất. Chúng tôi cho thấy hiệu quả của việc sử dụng các biểu diễn EarlyBIRD kết hợp trong khi fine-tuning mô hình CodeBERT kích thước gốc trên bốn bộ dữ liệu phân loại code thực tế. Chúng tôi chạy EarlyBIRD với 10 khởi tạo ngẫu nhiên khác nhau của các tham số có thể huấn luyện không cố định và đánh dấu các biểu diễn EarlyBIRD mang lại cải thiện có ý nghĩa thống kê so với baseline.

(3) Chúng tôi điều tra việc sử dụng tài nguyên và hiệu suất của các mô hình được tỉa. Chúng tôi phân tích sự đánh đổi giữa việc loại bỏ các lớp sau của một mô hình và tác động này đối với hiệu suất phân loại.

Những phát hiện chính: Với EarlyBIRD, chúng tôi đạt được cải thiện hiệu suất so với biểu diễn code baseline với phần lớn các biểu diễn thu được từ các lớp sớm đơn lẻ trên tác vụ phát hiện lỗi và các kết hợp được chọn lọc trên phân loại kiểu bug và kiểu exception. Hơn nữa, trong số các mô hình kích thước giảm với các lớp sau được tỉa, chúng tôi đạt được cải thiện độ chính xác trung bình +2 trên Devign với tăng tốc fine-tuning 3.3x, cũng như cải thiện độ chính xác +0.4 với tăng tốc 3.7x trung bình cho ReVeal.

Phần còn lại của bài báo được tổ chức như sau. Chúng tôi trình bày công việc liên quan trong Phần 2 và cung cấp chi tiết nền tảng của nghiên cứu trong Phần 3. Phương pháp được mô tả trong Phần 4 theo sau là thiết lập thực nghiệm trong Phần 5. Chúng tôi trình bày và thảo luận kết quả trong Phần 6 và kết luận với Phần 7.

2 CÔNG VIỆC LIÊN QUAN
Ở đây, chúng tôi đưa ra một cái nhìn tổng quan về các mô hình ngôn ngữ cho các tác vụ SE và các mô hình encoder gần đây, cụ thể, cũng như các phương pháp khác nhau để sử dụng các lớp sớm của các mô hình encoder.

2.1 Transformers trong Kỹ Thuật Phần Mềm
Sự sẵn có của code nguồn mở và khả năng phần cứng tăng cường đã phổ biến việc huấn luyện và sử dụng Học Sâu, bao gồm NLP và Mô Hình Ngôn Ngữ Lớn (LLMs), cho các tác vụ SE. Đến nay, các mô hình NLP sâu đã được áp dụng trong ít nhất 18 tác vụ SE [28]. Các mô hình ngôn ngữ được huấn luyện trước có sẵn để fine-tuning trên các tác vụ SE phần lớn được xây dựng trên kiến trúc transformer, các mô hình sequence-to-sequence, và cơ chế attention [8,9,42]. Một benchmark được sử dụng rộng rãi để kiểm tra các kiến trúc học sâu khác nhau trên các tác vụ SE là CodeXGLUE [27]. Benchmark cung cấp dữ liệu, mã nguồn để đánh giá mô hình, và bảng xếp hạng lãnh đạo xếp hạng hiệu suất mô hình trên các tác vụ khác nhau [27].

Các tác vụ SE có thể được chuyển đổi thành phân loại chuỗi đầu vào và tạo code hoặc văn bản. Ví dụ về các tác vụ tạo sinh trong SE là hoàn thành code, sửa chữa code, tạo tài liệu từ code và ngược lại, và dịch giữa các ngôn ngữ lập trình khác nhau. Các tác vụ như vậy thường được tiếp cận với các mô hình dịch máy nơ-ron. Các mô hình transformer đầy đủ để dịch từ ngôn ngữ lập trình (PL) sang ngôn ngữ tự nhiên (NL) hoặc các tác vụ PL-PL bao gồm PLBART [1], PYMT5 [10], TFix [4], CodeT5 [43], Break-It-Fix-It [47]. Ngoài ra, các mô hình tạo sinh có thể bao gồm phần chỉ có decoder của transformer như trong các mô hình kiểu GPT. Trong trường hợp này, decoder vừa biểu diễn chuỗi đầu vào vừa chuyển đổi nó thành chuỗi đầu ra. Các mô hình dựa trên decoder cho code bao gồm, ví dụ, Codex và CodeGPT [8, 27].

Trong các tác vụ yêu cầu biểu diễn code hoặc tài liệu và phân loại tiếp theo của chúng, các kiến trúc chỉ có encoder được sử dụng thường xuyên hơn so với trong các tác vụ dịch. Ví dụ về các bài toán phân loại code là phát hiện bản sao code, phát hiện lỗi chung, chẳng hạn như sự hiện diện của toán hạng bị hoán đổi, tên biến sai, lỗi cú pháp, hoặc lỗ hổng bảo mật. Một số mô hình encoder cho code đã áp dụng một encoder hai chiều được sử dụng rộng rãi, BERT [12], để huấn luyện trước nó trên code, với một số sửa đổi của đầu vào. Theo cách này, các mô hình CodeBERT [14], GraphCodeBERT [16], CuBERT [20], và PolyglotCodeBERT [2] đã được tạo ra.

Chi tiết, mô hình CodeBERT 12 lớp dựa trên RoBERTa đã được huấn luyện trước trên các tác vụ NL-PL trong nhiều PLs và chỉ sử dụng các tính năng văn bản của code. Lưu ý rằng RoBERTa là một loại mô hình BERT với các siêu tham số và quy trình huấn luyện trước được tối ưu hóa [26]. Cùng với mô hình CodeGPT chỉ có decoder, mô hình CodeBERT chỉ có encoder đã được sử dụng làm baseline trong CodeXGLUE. GraphCodeBERT sử dụng cả thuộc tính văn bản và cấu trúc của code để mã hóa các biểu diễn của nó. PolyglotCodeBERT là phương pháp cải thiện fine-tuning của mô hình CodeBERT trên một bộ dữ liệu đa ngôn ngữ cho một tác vụ mục tiêu ngay cả khi tác vụ mục tiêu chỉ kiểm tra một PL.

Bài báo này tập trung vào các chiến lược fine-tuning mà, trái ngược với PolyglotCodeBERT, không tăng việc sử dụng tài nguyên cho fine-tuning. CuBERT là một encoder dựa trên transformer được huấn luyện trước 24 lớp được kiểm tra trên một số tác vụ phân loại code, bao gồm phân loại kiểu exception. Chúng tôi kiểm tra hiệu suất của các biểu diễn EarlyBIRD kết hợp được đề xuất trên phát hiện lỗi, bao gồm việc sử dụng một trong các benchmark CodeXGLUE, cũng như trên các tác vụ phân loại kiểu lỗi và exception. Tuy nhiên, mục tiêu của bài báo này là đạt được cải thiện so với mô hình baseline khi nó được fine-tune với các biểu diễn code kết hợp. Chúng tôi không nhằm so sánh kết quả với các mô hình khác, mà đề xuất một phương pháp có thể áp dụng cho các encoder dựa trên transformer cho mã nguồn và cho thấy các lợi ích hiệu suất của nó so với việc sử dụng cùng mô hình mà không có phương pháp được đề xuất.

2.2 Sử Dụng Các Lớp Encoder Sớm
Một số nghiên cứu đã khám phá các phương pháp khác nhau để sử dụng thông tin từ các lớp sớm của các mô hình DL để biểu diễn chuỗi, chẳng hạn như probing các lớp đơn lẻ, tỉa và tỷ lệ học tập biến đổi.

Một cách để tận dụng thông tin từ các lớp mô hình sớm là ưu tiên khác nhau cho các lớp trong khi fine-tuning các mô hình [19,39]. Ví dụ, chiến lược layer-wise learning rate decay (LLRD) và khởi tạo lại các lớp encoder muộn đã mang lại cải thiện so với fine-tuning tiêu chuẩn của BERT trên các tác vụ NLP [49]. Chiến lược LLRD ban đầu được phát triển để điều chỉnh các lớp encoder sau với tỷ lệ học tập lớn hơn. Theo cách này, các lớp sau có thể được thích ứng tốt hơn với một tác vụ hạ nguồn đang được xem xét, bởi vì các lớp sau được giả định là học các tính năng phức tạp cụ thể cho tác vụ của các chuỗi đầu vào [19]. Hơn nữa, Peters et al. [33] cho thấy rằng hiệu suất của fine-tuning cải thiện nếu các lớp encoder được cập nhật trong quá trình fine-tuning so với việc chỉ huấn luyện bộ phân loại trên các lớp encoder cố định (đông lạnh).

Tỉa các lớp sau của các mô hình transformer là một cách khác để chỉ xem xét các lớp sớm cho fine-tuning [13,31,36]. Sajjad et al. [36] điều tra cách hiệu suất của các mô hình transformer trên NLP bị ảnh hưởng khi giảm kích thước của chúng bằng cách tỉa các lớp. Họ xem xét sáu chiến lược tỉa, bao gồm loại bỏ từ các hướng khác nhau, loại bỏ lớp xen kẽ, hoặc loại bỏ các lớp dựa trên tầm quan trọng, cho bốn mô hình được huấn luyện trước: BERT [12], RoBERTa [26], XLNET [46], ALBERT [22]. Bằng cách tỉa các lớp mô hình, Sajjad et al. đã có thể giảm số lượng tham số xuống 60% của tập tham số ban đầu trong khi duy trì mức hiệu suất cao. Trong khi hiệu suất trên các tác vụ hạ nguồn khác nhau trong nghiên cứu của họ, các lớp thấp hơn là quan trọng để duy trì hiệu suất khi fine-tuning cho các tác vụ hạ nguồn. Nói cách khác, loại bỏ các lớp sớm hơn có hại cho hiệu suất. Nhìn chung, tỉa các lớp giảm kích thước mô hình và do đó giảm thời gian fine-tuning và suy luận. Phù hợp với công việc của Sajjad et al. [36], chúng tôi mở rộng các thí nghiệm của mình với việc tỉa các lớp sau và giữ các lớp sớm hơn có mặt trong mô hình (xem RQ2 trong Phần 6).

Việc sử dụng thông tin từ các lớp sớm đơn lẻ trong một số thí nghiệm EarlyBIRD cũng được truyền cảm hứng từ Peters et al. [32]. Trong nghiên cứu của họ, Peters et al. trình bày bằng chứng thực nghiệm rằng các mô hình ngôn ngữ học cú pháp và thông tin part-of-speech trên các lớp sớm hơn của một mạng nơ-ron, trong khi thông tin phức tạp hơn, chẳng hạn như ngữ nghĩa và mối quan hệ đồng tham chiếu, được nắm bắt tốt hơn bởi các lớp sâu hơn (sau). Trong một nghiên cứu khác, Karmakar và Robbes đã probed các mô hình được huấn luyện trước của code, bao gồm CodeBERT, trên các tác vụ hiểu thông tin cú pháp, độ phức tạp cấu trúc, độ dài code, và thông tin ngữ nghĩa [21]. Trong khi Karmakar và Robbes probed các lớp sớm đông lạnh của các mô hình khác nhau cho code trong một chiến lược duy nhất, chúng tôi sử dụng 12 chiến lược khác nhau để kết hợp các lớp sớm không đông lạnh trong quá trình fine-tuning và tập trung vào các tác vụ phát hiện bug hoặc phân loại kiểu bug. Tương tự, Hernández López et al. [18] probed các lớp khác nhau của năm mô hình được huấn luyện trước, bao gồm CodeBERT [14] và GraphCodeBERT [16], và phát hiện rằng hầu hết thông tin cú pháp được mã hóa trong các lớp giữa. Tính mới của nghiên cứu của chúng tôi so với Karmakar và Robbes là chúng tôi kết hợp các lớp sớm ngoài việc trích xuất từng lớp, trong khi Karmakar và Robbes trích xuất các biểu diễn lớp sớm và sử dụng chúng mà không soạn các biểu diễn mới.

3 ENCODERS CHO PHÂN LOẠI CODE
Trong phần này, chúng tôi trình bày nền tảng về các mô hình transformer và các cách sử dụng khác nhau của kiến trúc encoder-decoder—hoặc transformer đầy đủ—cũng như các biến thể chỉ có encoder và chỉ có decoder của nó. Bởi vì nghiên cứu của chúng tôi tập trung vào các mô hình nguồn mở chỉ có encoder có sẵn để fine-tuning, sự phân biệt giữa các loại transformer là cần thiết để hiểu phương pháp.

Trong các kịch bản tạo sinh sequence-to-sequence, mô hình transformer bao gồm một encoder đa lớp biểu diễn chuỗi đầu vào và một decoder tạo ra chuỗi đầu ra dựa trên biểu diễn chuỗi từ encoder và đầu ra có sẵn được tạo ra ở các bước trước [42]. Đối với các tác vụ phân loại mã nguồn, transformer thường được giảm xuống chỉ encoder của nó theo sau là một classification head, một thành phần được thêm vào encoder để phân loại biểu diễn thành các lớp khác nhau. Việc loại bỏ decoder cho phân loại được thúc đẩy bởi hiệu quả tài nguyên, bởi vì decoder về mặt khái niệm chỉ cần thiết để tạo token từ chuỗi đầu vào. Trong quá trình phân loại một đầu vào, encoder biểu diễn chuỗi và chuyển nó đến classification head. Dựa trên thiết kế này, một số encoder được huấn luyện trước đã được công bố trong những năm gần đây, chẳng hạn như BERT và RoBERTa được huấn luyện trước trên ngôn ngữ tự nhiên, và các mô hình tương tự được huấn luyện trước trên code, hoặc kết hợp code và ngôn ngữ tự nhiên [12,26]. Mục tiêu của việc huấn luyện trước trong kịch bản pre-train và fine-tune là nắm bắt các mẫu ngôn ngữ nói chung, để chúng có thể phục vụ như một nền tảng cho các tác vụ hạ nguồn cụ thể miền. Các mô hình được huấn luyện trước có thể được fine-tune trên các tác vụ hạ nguồn khác nhau trong NLP và SE.

Xử lý chuỗi đầu vào để phân loại bao gồm một số bước: tokenization, embedding ban đầu, mã hóa chuỗi với một encoder, và chuyển biểu diễn chuỗi qua một classification head. Tokenization chia chuỗi đầu vào, thêm các token đặc biệt, khớp các token với ID của chúng trong từ vựng của tokens, và thống nhất độ dài token kết quả cho các mẫu trong một bộ dữ liệu. Embedding chuyển đổi ID token một chiều thành một biểu diễn vector tĩnh đa chiều ban đầu của token và thường là một phần của mô hình encoder được huấn luyện trước. Biểu diễn này được cập nhật sử dụng cơ chế attention của encoder. Vì có attention, biểu diễn của đầu vào bị ảnh hưởng bởi tất cả các token trong chuỗi, vì vậy nó được ngữ cảnh hóa.

CodeBERT là một mô hình dựa trên RoBERTa với 12 lớp encoder được huấn luyện trước trên 6 ngôn ngữ lập trình (Python, Java, JavaScript, PHP, Ruby, và Go), cũng như các tác vụ text-to-code [14]. Việc huấn luyện trước được thực hiện trên các tác vụ masked language modeling (MLM) và replaced token detection (RTD). Các tác vụ này tương ứng huấn luyện mô hình để suy ra token nào bị che trong MLM, và trong RTD dự đoán liệu có token nào trong một chuỗi gốc bị hoán đổi với một token khác không nên có trong chuỗi. CodeBERT đầu ra một biểu diễn encoder hai chiều của chuỗi đầu vào, có nghĩa là mô hình xem xét ngữ cảnh từ các từ trước và sau để biểu diễn mỗi token trong chuỗi đầu vào.

Một mô hình được huấn luyện trước thường được phát hành với một tokenizer được huấn luyện trước. Tokenizer được huấn luyện trước đảm bảo rằng ID token tương ứng với những gì được xử lý trong quá trình huấn luyện trước. Tokenizer cũng thêm các token đặc biệt, chẳng hạn như token CLS ở đầu mỗi chuỗi đầu vào, các token PAD để thống nhất độ dài của các chuỗi đầu vào, và token EOS để biểu thị kết thúc của chuỗi đầu vào và bắt đầu của chuỗi padding [12]. Tất cả các token được chuyển đổi bởi mô hình trong mỗi lớp encoder. Trong số tất cả các token, biểu diễn token CLS từ lớp cuối cùng, được cập nhật bởi tất cả các lớp encoder, thường được sử dụng như một biểu diễn cho toàn bộ chuỗi.

Thực hành tiêu chuẩn sử dụng token CLS từ lớp encoder cuối cùng được thúc đẩy bởi quy trình huấn luyện trước. Ví dụ, trong MLM, mô hình dự đoán token bị che dựa trên biểu diễn token CLS từ lớp thứ 12 của BERT và CodeBERT. Tuy nhiên, lựa chọn token để biểu diễn toàn bộ chuỗi trong fine-tuning có thể khác. Ví dụ, trong PLBART [1], một mô hình transformer cho code với cả encoder và decoder, token EOS được sử dụng để biểu diễn chuỗi đầu vào. Trong bài báo này, chúng tôi đề xuất các cách khác nhau để biểu diễn chuỗi đầu vào và sử dụng thông tin từ các lớp sớm của mô hình một cách hiệu quả.

4 PHƯƠNG PHÁP
Trong bài báo này, kiến trúc của mô hình phân loại code bao gồm năm phần: (1) một tokenizer, (2) một lớp embedding, (3) một encoder với một số lớp, (4) một tập hợp các phép toán để kết hợp các biểu diễn chuỗi từ các lớp encoder với EarlyBIRD, và (5) một classification head. Đầu ra của mỗi bước được sử dụng làm đầu vào cho bước tiếp theo. Một cái nhìn tổng quan về kiến trúc được hiển thị trong Hình 1 và mô tả dưới đây. Sự khác biệt chính giữa kiến trúc này và kiến trúc phân loại được thảo luận trong Phần 3 là bước (4); kiến trúc tiêu chuẩn chỉ bao gồm các bước (1–3) và (5).

Các bước (1)–(3) sử dụng một tokenizer, embedder, và encoder được huấn luyện trước. EarlyBIRD được công thức hóa theo cách tổng quát và có thể được áp dụng cho bất kỳ encoder nào, nhưng cho các thí nghiệm của chúng tôi, chúng tôi cố định mô hình CodeBERT và tokenizer. Trong bước (4), chúng tôi kết hợp thông tin từ tất cả các lớp hoặc chỉ từ một số lớp sớm của encoder, trái ngược với baseline sử dụng lớp cuối cùng của encoder. Cuối cùng, classification head trong bước (5) bao gồm một lớp dropout và một lớp tuyến tính với softmax.

Mô hình encoder biểu diễn mỗi token của một chuỗi đầu vào với một vector có kích thước H, còn được gọi là hidden size. Đối với mỗi chuỗi đầu vào có độ dài S, và một hidden size H, chúng tôi có được một ma trận có kích thước S×H cho mỗi trong số L lớp của mô hình cơ sở như được hiển thị trong Hình 1. Ví dụ, kiến trúc CodeBERT được cố định với 12 lớp encoder, tức là L=12 cho mô hình đó. Tất cả thông tin có sẵn trong encoder cho một chuỗi đầu vào được lưu trữ trong một tensor có kích thước L×S×H. Các kết hợp EarlyBIRD phải tạo ra một vector R có kích thước H biểu diễn đầu vào, như được hiển thị trong Hình 1. Việc giữ biểu diễn code đầu ra có kích thước H được yêu cầu để cung cấp một so sánh công bằng của các biểu diễn EarlyBIRD kết hợp với biểu diễn code tiêu chuẩn thu được từ lớp cuối cùng. Theo cách này, chiều của classification head là giống nhau cho tất cả các kết hợp của các lớp sớm và có ảnh hưởng tối thiểu có thể trong quá trình fine-tuning.

Như một chiến lược để điều tra có hệ thống các biểu diễn kết hợp, chúng tôi tạo một grid-search trên ba phép toán điển hình để kết hợp đầu ra của các lớp mạng nơ-ron – maximum pooling (max pool), weighted sum và slicing – và hai chiều để áp dụng các phép toán: trên tokens và/hoặc layers. Đối với chiều tokens, chúng tôi sử dụng tất cả các token từ một lớp cụ thể hoặc chỉ token CLS. Trong số các lớp, chúng tôi slice một lớp, tính tổng hoặc lấy giá trị tối đa trên tất cả các lớp. Việc lựa chọn xem xét mọi token của một lớp được thúc đẩy bởi thực tế rằng các mô hình dựa trên transformer thể hiện các mức độ attention khác nhau cho các loại token khác nhau [29], điều này cho thấy rằng việc chỉ sử dụng token CLS có thể không phải là lựa chọn tốt nhất cho các tác vụ [37]. Chúng tôi cũng thử nghiệm với các kích thước khác nhau của mô hình. Các chiến lược kết hợp sử dụng tất cả các lớp của mô hình được huấn luyện trước được chia thành hai loại: các chiến lược sử dụng token CLS từ các lớp encoder; các chiến lược sử dụng nhiều token hơn chỉ CLS từ các lớp encoder.

Khi chúng tôi slice token CLS và áp dụng mỗi phép toán trên các lớp, chúng tôi có được các kết hợp CLS-token sau:
(i) baseline: token CLS từ lớp cuối cùng, tức là lớp số L;
(ii) token CLS từ một lớp³ số l, l∈{1,...,(L−1)};
(iii) max pool trên các token CLS từ tất cả các lớp {l}L l=1;
(iv) weighted sum trên các token CLS từ tất cả các lớp {l}L l=1.

Tập kết hợp thứ hai sử dụng các biểu diễn của tất cả các token trong các chuỗi đầu vào được tokenized, bao gồm token CLS. Chúng tôi đầu tiên áp dụng phép toán max pooling cho tất cả các token hoặc tất cả các lớp và sử dụng phần còn lại của các phép toán. Sau đó chúng tôi áp dụng weighted sum làm phép toán đầu tiên theo sau là max pool hoặc slicing của một lớp:
(v) max pool tokens từ một lớp số l, l∈{1,...,L};
(vi) max pool trên tất cả các lớp cho mỗi token trong chuỗi đầu vào, max pool trên tokens;
(vii) max pool trên tất cả các lớp cho mỗi token trong chuỗi đầu vào; weighted sum trên tokens;
(viii) max pool trên tất cả các token cho mỗi lớp số l, l∈{1,...,L}; weighted sum trên layers
(ix) weighted sum trên tokens từ một lớp số l, l∈{1,...,L};
(x) weighted sum trên tokens cho mỗi một lớp số l, l∈{1,...,L}; weighted sum trên tất cả các lớp;
(xi) weighted sum trên tất cả các lớp cho mỗi token trong chuỗi đầu vào; weighted sum trên tất cả các token.

Lưu ý rằng các trọng số trong weighted sums là các tham số có thể học được. Tuy nhiên, số lượng tham số có thể học được được thêm vào cho fine-tuning tạo thành 0.00042%⁴ của số lượng tham số có thể học được trong cấu hình baseline. Vì lý do này, chúng tôi đề cập rằng các mô hình với các kết hợp (ii-x) có cùng kích thước mô hình trong khi ghi nhớ overhead của các trọng số có thể học được trong weighted sums.

Ngoài các thí nghiệm với kết hợp token, chúng tôi cũng điều tra hiệu suất của mô hình với l<L lớp đầu tiên và kết hợp token baseline, được mô tả như sau:

³Chúng tôi sử dụng mỗi lớp l trong các kết hợp riêng biệt nếu chúng tôi ký hiệu l, l∈{1,...,L}, và chỉ định tập hợp các lớp {l}L l=1 nếu một số lớp được sử dụng cùng một lúc.
⁴Weighted sum trên tokens thêm S=512 trọng số có thể học được. Vì các trọng số của tổng được chia sẻ giữa các lớp, số lượng trọng số được thêm vào tối đa là L+H=524 trong số 124M trọng số có thể học được trong mô hình cơ sở. Các kết hợp không có weighted sums không thêm tham số có thể học được bổ sung vào mô hình cơ sở. Weighted sum trên layers thêm L=12 trọng số có thể học được cho CodeBERT.

(xii) token CLS từ lớp cuối cùng của mô hình với l<L lớp encoder.

Lưu ý rằng kết hợp baseline (i) với việc sử dụng token CLS từ lớp L tương ứng với (ii) và (xii) nếu l=L.

Các kết hợp được trình bày trong Hình 2. Các kết hợp tương tự được trình bày gần nhau hoặc được kết hợp trong cùng một hình ảnh nếu chúng chỉ có những khác biệt nhỏ và chia sẻ các phần chính. Ví dụ, trong Hình 2c, chúng tôi minh họa các kết hợp (iii) và (iv), bởi vì cả hai đều sử dụng token CLS từ tất cả các lớp được kết hợp sử dụng max pooling hoặc weighted sum. Các số La Mã chỉ ra các loại kết hợp được bảo toàn trong các mô tả dưới các hình hoặc trong chính các hình, nhưng thứ tự được thay đổi. Chúng tôi đề cập số kết hợp tương ứng với mô tả trong phần hiện tại, chẳng hạn như kết hợp baseline (i) trong Hình 2a hoặc kết hợp (ii) cho token CLS từ một lớp sớm trong Hình 2b. Chúng tôi làm nổi bật những phần nào của đầu ra lớp encoder được sử dụng cho mỗi kết hợp bằng màu sắc. Các ô trắng tương ứng với các token không được sử dụng trong các kết hợp lớp sớm. Mục tiêu của tất cả các kết hợp là có được một biểu diễn vector R cho mỗi mẫu code đầu vào. Ví dụ, trong Hình 2a, chúng tôi xem xét lớp cuối cùng L và chỉ trích xuất token CLS được đánh dấu là R.

Một lưu ý khác về các kết hợp EarlyBIRD liên quan đến việc sử dụng tất cả các token hoặc chỉ các token code. Các token code là những token tương ứng với các từ hoặc từ phụ đầu vào được tokenized và được hiển thị trong Hình 2 như tokeni1,...,tokeniN cho một chuỗi đầu vào i có kích thước iN. Đối với mỗi kết hợp sử dụng nhiều hơn chỉ một token CLS, tức là các kết hợp (v-xi), chúng tôi thử nghiệm chỉ với các token code, cũng như với tất cả các token, bao gồm CLS, EOS, và PAD. Động lực để kiểm tra các token code độc quyền xuất phát từ giả thuyết rằng thông tin trong các token đặc biệt có thể đưa nhiễu vào kết quả.

5 THIẾT LẬP THỰC NGHIỆM
Trong phần này, chúng tôi mô tả các bộ dữ liệu được sử dụng để đánh giá thực nghiệm và chi tiết triển khai của fine-tuning với phương pháp EarlyBIRD được đề xuất. Chúng tôi điều tra các kịch bản phân loại code nhị phân và đa tác vụ để khám phá khả năng tổng quát hóa của kết quả của chúng tôi.

5.1 Bộ Dữ Liệu cho Phân Loại Mã Nguồn
Chúng tôi fine-tune và kiểm tra mô hình CodeBERT sử dụng phương pháp EarlyBIRD trên bốn bộ dữ liệu. Các bộ dữ liệu bao gồm ba tác vụ: phát hiện lỗi, phân loại kiểu lỗi và phân loại kiểu exception — với tương ứng 2, 3, và 20 lớp. Chúng cũng chứa dữ liệu trong hai ngôn ngữ lập trình, C++ và Python. Ngoài ra, các bộ dữ liệu được chọn có kích thước tập con train tương tự. Theo cách này, chúng tôi nhằm giảm ảnh hưởng của việc tiếp xúc mô hình với các lượng dữ liệu huấn luyện khác nhau trong quá trình fine-tuning. Thống kê của các bộ dữ liệu được cung cấp trong Bảng 1. Chúng tôi báo cáo kích thước của các phần chia train/validation/test. Ngoài ra, chúng tôi tính toán số lượng token trung bình trong các chuỗi đầu vào sau khi tokenization với tokenizer CodeBERT được huấn luyện trước. Bởi vì kích thước chuỗi đầu vào tối đa cho mô hình CodeBERT được giới hạn ở S=512, số lượng token chỉ ra mô hình được truy cập bao nhiều thông tin hoặc bao nhiều thông tin bị cắt bỏ, trong trường hợp đầu vào dài.

Devign: Bộ dữ liệu này chứa các hàm trong C/C++ từ hai dự án nguồn mở được gán nhãn là dễ bị tổn thương hoặc không dễ bị tổn thương [51]. Chúng tôi tái sử dụng phần chia train/validation/test từ benchmark CodeXGLUE Defect detection.⁵ Bộ dữ liệu được cân bằng: tỷ lệ các hàm không dễ bị tổn thương là 54%.

ReVeal: Tương tự như Devign, ReVeal là một bộ dữ liệu phát hiện lỗ hổng của các hàm C/C++ [7]. Bộ dữ liệu không được cân bằng: nó chứa 90% đoạn code không dễ bị tổn thương. Cả hai bộ dữ liệu Devign và ReVeal đều chứa các hàm dễ bị tổn thương và không dễ bị tổn thương thực tế từ các dự án nguồn mở.

Break-It-Fix-It (BIFI): Bộ dữ liệu chứa các đoạn code cấp hàm trong Python với lỗi cú pháp [47]. Chúng tôi sử dụng các hàm buggy gốc và công thức hóa một tác vụ phân loại code thành ba lớp: Unbalanced Parentheses với 43% tổng số ví dụ code trong BIFI, Indentation Error với 31% mẫu code, Invalid Syntax chứa 26% mẫu. Phần chia train/test được cung cấp trong bộ dữ liệu được tái sử dụng, và tập validation được trích xuất như 10% của dữ liệu huấn luyện.

Exception Type: Bộ dữ liệu bao gồm các hàm ngắn trong Python với một token __HOLE__ được chèn vào thay cho một exception trong code.⁶ Tác vụ là dự đoán một trong 20 kiểu exception bị che cho mỗi hàm đầu vào và không được cân bằng. Bộ dữ liệu ban đầu được tạo từ ETH Py150 Open corpus⁷ như được mô tả trong bài báo gốc [20]. Chúng tôi tái sử dụng phần chia train/validation/test được cung cấp bởi các tác giả.

5.2 Triển Khai
Kiến trúc dựa trên tokenizer và mô hình encoder CodeBERT.⁸ Mô hình xác định độ dài chuỗi tối đa, hidden size, và có 12 lớp, vì vậy S=512, H=768, L=12. Các siêu tham số trong các thí nghiệm được đặt thành B=64, learning rate là 1e-5, và xác suất dropout là 0.1. Nếu mẫu đầu vào được tokenized dài hơn S=512, chúng tôi tỉa các token ở cuối để làm cho đầu vào phù hợp với mô hình. Chúng tôi chạy fine-tuning với optimizer Adam và kiểm tra cho mỗi kết hợp 10 lần với các seed khác nhau trong 10 epoch và báo cáo hiệu suất cho epoch tốt nhất trung bình trên 10 lần chạy. Epoch tốt nhất được xác định bằng cách đo độ chính xác trên tập validation. Chúng tôi sử dụng Python 3.7 và Cuda 11.6, và chạy các thí nghiệm trên một GPU Nvidia Volta A100.

5.3 Metrics Đánh Giá
Để trình bày tác động của các kết hợp lớp sớm, chúng tôi so sánh độ chính xác trên tập test cho tất cả các bộ dữ liệu, bởi vì nó cho phép chúng tôi so sánh kết quả của mình với các benchmark khác. Ngoài ra, chúng tôi báo cáo weighted F1-score được ký hiệu là F1(w) cho phân tích chi tiết của các kết hợp được chọn để tính đến sự mất cân bằng lớp. Để có được weighted F1-score, F1-score thường xuyên được tính toán cho mỗi nhãn và trung bình có trọng số của chúng được lấy. Các trọng số bằng với số lượng mẫu trong một lớp.

Chúng tôi cũng báo cáo kết quả của kiểm định Wilcoxon signed-rank trên các metrics tương ứng cho các kết hợp cho thấy cải thiện so với baseline [45]. Kiểm định Wilcoxon là một kiểm định phi tham số phù hợp cho thiết lập mà các biến thể mô hình khác nhau được kiểm tra trên cùng một tập test, bởi vì nó là một kiểm định ghép đôi. Kiểm định Wilcoxon kiểm tra giả thuyết null liệu hai mẫu ghép đôi liên quan có đến từ cùng một phân phối. Chúng tôi từ chối giả thuyết null nếu p-value nhỏ hơn α=0.05. Trong trường hợp chúng tôi có được cải thiện của một metric so với baseline với một kết hợp EarlyBIRD và giả thuyết null bị từ chối, chúng tôi kết luận rằng kết hợp hoạt động tốt hơn và kết quả có ý nghĩa thống kê. Đối với các mô hình được tỉa, chúng tôi tính toán thước đo hiệu ứng phi tham số A12 của Vargha và Delaney về sự thay đổi hiệu suất cho độ chính xác và F1(w) với các ngưỡng 0.71, 0.64 và 0.56 cho kích thước hiệu ứng lớn, trung bình và nhỏ [41].

5.4 Câu Hỏi Nghiên Cứu
Trong quá trình đánh giá thực nghiệm các biểu diễn code EarlyBIRD kết hợp của chúng tôi, chúng tôi giải quyết các câu hỏi nghiên cứu sau:

RQ1. Biểu Diễn Code Kết Hợp với Cùng Kích Thước Mô Hình: Hiệu quả của việc sử dụng các kết hợp (ii-xi) của các lớp sớm với cùng kích thước mô hình so với phương pháp baseline chỉ sử dụng token CLS từ lớp cuối cùng, tức là kết hợp (i), để biểu diễn code trên hiệu suất mô hình trong kịch bản phân loại code là gì? Mục tiêu là tìm ra liệu có loại kết hợp EarlyBIRD nào hoạt động tốt hơn một cách nhất quán cho các bộ dữ liệu và tác vụ khác nhau.

RQ2. Mô Hình Được Tỉa: Hiệu quả của việc giảm số lượng lớp encoder được huấn luyện trước trong các kết hợp (xii) trên việc sử dụng tài nguyên và hiệu suất mô hình trên các tác vụ phân loại code là gì? Trái ngược với RQ1, trong đó chúng tôi xem xét các kết hợp không giảm kích thước mô hình, câu hỏi nghiên cứu này được dành cho điều tra sự đánh đổi giữa việc sử dụng ít tài nguyên hơn với các mô hình kích thước giảm và sự biến đổi hiệu suất về các metrics phân loại.

Đối với cả hai câu hỏi nghiên cứu, chúng tôi đánh giá các biểu diễn kết hợp trên các kịch bản phân loại code nhị phân và đa tác vụ để khám phá khả năng tổng quát hóa của các kết quả thu được cho trường hợp nhị phân. Chúng tôi điều tra liệu có và những kết hợp nào dẫn đến hiệu suất tốt hơn, được tính trung bình trên 10 lần chạy với các seed khác nhau. Đối với các kết hợp cải thiện baseline trung bình, chúng tôi cũng khám phá liệu kết quả có ý nghĩa thống kê theo kiểm định Wilcoxon.

6 KẾT QUẢ VÀ THẢO LUẬN

6.1 EarlyBIRD với Mô Hình Kích Thước Cố Định
Để trả lời RQ1, chúng tôi khám phá các kết hợp một lớp, kết hợp đa lớp, và ước tính ý nghĩa thống kê của cải thiện hiệu suất.

6.1.1 Kết Hợp Tokens trong Các Lớp Sớm Được Chọn Đơn Lẻ. Hình 3 hiển thị một heatmap của sự khác biệt của độ chính xác trung bình thu được với mỗi kết hợp chỉ sử dụng một lớp sớm được chọn so với baseline. Ngoài ra, chúng tôi hiển thị giá trị của sự khác biệt trong độ chính xác trung bình cho mỗi loại kết hợp và số lớp. Lưu ý rằng thang đo là logarithmic và trong trường hợp cực đoan nhất kéo dài khoảng từ khoảng -37 đến +2. Các giá trị âm được hiển thị bằng màu đen, và các giá trị dương được hiển thị bằng màu trắng. Những khác biệt có ý nghĩa thống kê theo kiểm định Wilcoxon được đánh dấu bằng dấu sao (∗) bên cạnh giá trị. Các kết hợp tương ứng với baseline được đánh dấu bằng "bsln" và có sự khác biệt bằng không, theo định nghĩa. Kết quả cho weighted F1-score cho thấy một mẫu tương tự như những kết quả cho độ chính xác trung bình. Chúng được hình dung theo cùng một cách trong Hình 4.

Các hàng đầu tiên trong Hình 3a và 3b tương ứng với các kết hợp (ii) CLS token layer l. Với loại kết hợp này, cải thiện trung bình so với baseline được đạt được với phần lớn các lớp sớm. Cụ thể, chúng tôi có được cải thiện độ chính xác từ +0.2 đến +2.0 cho Devign trong 8 trong số 11 lớp, và cải thiện độ chính xác từ +0.1 đến +0.8 cho ReVeal trong 9 trong số 11 lớp. Động lực của sự thay đổi metric theo số lớp được chọn là khác nhau cho Devign và ReVeal. Chi tiết, hiệu suất trung bình của kết hợp (ii) là tốt nhất với lớp 3 trên Devign (cải thiện độ chính xác +2.0) và với lớp 1 cho ReVeal (cải thiện độ chính xác +0.8). Cải thiện tốt nhất về F1(w) khớp với lớp 3 cho Devign và với lớp 2 cho ReVeal, như được hiển thị trong Hình 4.

Max pooling trên tất cả các token có sẵn từ một lớp được chọn trong kết hợp (v) cũng đạt được cải thiện hiệu suất so với baseline, như được hiển thị trong các hàng 2 và 3 của Hình 3a, 3b. Nói chung, các lớp 4–11 mang lại độ chính xác cao hơn và các lớp 2–11 F1(w) cao hơn với max pooling cho Devign so với baseline. Đối với ReVeal, tất cả các lớp trừ lớp 11 dẫn đến độ chính xác trung bình tốt hơn và các lớp 2–10 có F1(w) trung bình cao hơn. Max pooling trên tất cả các token, bao gồm các token đặc biệt, đạt được cải thiện có ý nghĩa thống kê tốt nhất về độ chính xác +0.9 của tất cả các kết hợp cho ReVeal.

Weighted sum của tất cả các token hoặc chỉ các token code trong kết hợp (ix) không cải thiện hiệu suất baseline. Chúng tôi giả định rằng fine-tuning trong 10 epoch không đủ cho loại kết hợp này, bởi vì loss tại epoch 10 trên cả phần chia training và validation cao hơn cho các kết hợp (ix) so với các kết hợp với max pooling. Vì mục tiêu của nghiên cứu này là sử dụng cùng hoặc ít tài nguyên hơn cho fine-tuning, chúng tôi đã không fine-tune kết hợp này trong hơn 10 epoch.

Trong khi các kết hợp (ii) và (v) hoạt động tốt hơn cho phần lớn các lớp trên tác vụ phát hiện lỗi, phân loại đa lớp cho dự đoán kiểu bug hoặc exception không hưởng lợi từ các kết hợp ở cùng mức độ như tác vụ nhị phân. Chỉ max pooling của tokens của lớp encoder cuối cùng đạt được hiệu suất tốt hơn baseline cho các bộ dữ liệu BIFI (cải thiện độ chính xác +0.1, cải thiện weighted F1-score +0.1) và Exception Type (cải thiện độ chính xác +0.2, cải thiện weighted F1-score +0.1).

Tác động của việc sử dụng tất cả các token hoặc chỉ các token code phụ thuộc vào bộ dữ liệu. Sự khác biệt giữa hiệu suất của các kết hợp một lớp với max pooling của tất cả các token và chỉ các token code tạo thành 0.0-0.1 độ chính xác hoặc F1(w). Đối với các tác vụ đa lớp, kết quả trung bình cải thiện với việc sử dụng mỗi lớp sau trong mô hình. Chúng tôi có được cải thiện hiệu suất với kết hợp max pooling (v), trong khi các kết hợp một lớp khác không hoạt động tốt hơn baseline.

Kết quả hoạt động tốt nhất trên các bộ dữ liệu Devign và Exception Type classification có ý nghĩa thống kê theo kiểm định Wilcoxon. Đối với ReVeal, kết quả tốt thứ hai có ý nghĩa thống kê. Chúng tôi không có được cải thiện có ý nghĩa thống kê cho BIFI. Chúng tôi giải thích điều này bởi thực tế rằng metric baseline đã cao, tức là 96.7 độ chính xác. Việc đạt được cải thiện thường khó khăn hơn khi baseline hoạt động ở mức này.

Về bản chất, các kết hợp liên quan đến token CLS tương ứng với lớp đơn (ii), cũng như các kết hợp max pooling (v) hoạt động tốt hơn trung bình cho các bộ dữ liệu phát hiện lỗi Devign và Reveal. Tuy nhiên, chỉ kết hợp max pooling (v) của tokens từ lớp encoder cuối cùng vượt trội hơn baseline trung bình cho các bộ dữ liệu đa lớp BIFI và Exception Type. Weighted sum của tokens từ một lớp được chọn (ix) hoạt động tệ hơn baseline nếu được fine-tune cho cùng số epoch cho tất cả các tác vụ. Các tác vụ phân loại đa lớp yêu cầu thông tin từ lớp cuối cùng để có hiệu suất tốt hơn trong các thí nghiệm của chúng tôi, trong khi tác vụ nhị phân phát hiện lỗi cho phép chúng tôi sử dụng các lớp sớm và cải thiện hiệu suất so với baseline.

6.1.2 Kết Hợp Đa Lớp. Sự khác biệt hiệu suất trung bình với baseline của các kết hợp sử dụng các lớp sớm được hiển thị dưới dạng heatmaps trong Hình 5 và 6. Chúng tôi bao gồm giá trị của sự khác biệt hiệu suất trung bình và thêm dấu sao (∗) vào số nếu sự khác biệt có ý nghĩa thống kê. Một lần nữa, các giá trị âm được hiển thị bằng màu đen, và các giá trị dương được hiển thị bằng màu trắng.

Khi chúng tôi sử dụng tất cả thông tin từ các lớp có sẵn, cải thiện so với baseline ít hơn so với những gì được quan sát trong Phần 6.1.1, nơi một lớp cụ thể đã được sử dụng. Chi tiết, trong số các kết hợp liên quan đến token CLS từ tất cả các lớp sớm, không có kết hợp nào hoạt động tốt hơn baseline cho ReVeal, BIFI, hoặc các bộ dữ liệu Exception Type. Tuy nhiên, cải thiện tốt nhất (+0.6 độ chính xác) trong số các thí nghiệm với tất cả các lớp được có được trên Devign với weighted sum của token CLS trong kết hợp (iv), ít hơn so với cải thiện tối đa với các kết hợp từ một lớp sớm được chọn trong Phần 6.1.1. Cải thiện F1(w) được hiển thị trong Hình 6. Chúng tôi có được cải thiện F1(w) hơi tốt hơn cho Devign, không có cải thiện F1(w) cho bộ dữ liệu ReVeal không cân bằng. Sự khác biệt F1(w) trung bình với baseline cho các tác vụ đa lớp giống như sự khác biệt độ chính xác.

Nếu chúng tôi xem xét các kết hợp liên quan đến tất cả các token, kết hợp (vi) với hai phép toán max pooling vượt trội hơn baseline cho Devign, Reveal, và BIFI với cải thiện độ chính xác từ +0.1 đến +0.3. Không có kết hợp nào liên quan đến tất cả các lớp vượt trội hơn baseline trung bình cho bộ dữ liệu Exception Type.

Các kết hợp liên quan đến một max pooling và một weighted sum của tất cả các token hoạt động tệ hơn hoặc trung tính so với baseline. Các kết hợp chỉ với weighted sums hoạt động tệ hơn baseline trung bình.

Trả lời cho RQ1. EarlyBIRD đạt được cải thiện độ chính xác và F1-score có ý nghĩa thống kê cho các bộ dữ liệu phát hiện lỗi bằng cách sử dụng các kết hợp một lớp liên quan đến token CLS hoặc max pooling trên tất cả các token. Đối với phân loại kiểu bug và kiểu exception, max pooling của các token từ lớp encoder cuối cùng đã cải thiện hiệu suất. Weighted sum của tokens không cải thiện hiệu suất so với baseline.

6.2 Mô Hình Được Tỉa
Phần này dành cho các kết hợp của các lớp sớm được khởi tạo với l<L lớp sớm đầu tiên từ mô hình được huấn luyện trước và được fine-tune như các mô hình l-lớp — các kết hợp (xii). Chúng tôi bắt đầu bằng cách so sánh hiệu suất của việc sử dụng token CLS từ lớp l của mô hình kích thước đầy đủ, tức là kết hợp (ii), và sử dụng token CLS từ lớp l của mô hình có tổng cộng l lớp — kết hợp (xii). Hình 7 trình bày độ chính xác trung bình thu được với hai kết hợp này tùy thuộc vào lớp được sử dụng, cũng như kết hợp baseline sử dụng CLS từ lớp cuối cùng L=12 của CodeBERT. Trung bình, các mô hình được tỉa với kích thước giảm hoạt động ngang bằng với mô hình kích thước đầy đủ cho phát hiện lỗi trên bộ dữ liệu Devign cân bằng, và cho phân loại kiểu bug và kiểu exception. Tuy nhiên, hiệu suất của hai kết hợp tương tự phân kỳ cho bộ dữ liệu phát hiện lỗi không cân bằng ReVeal trong các lớp 4 và 6–11.

Quan trọng nhất, kết quả cho thấy rằng việc giảm kích thước mô hình và sử dụng token CLS từ lớp cuối cùng của mô hình giảm hoạt động ngang bằng với baseline cho tác vụ phát hiện lỗi. Cải thiện tốt nhất với mô hình giảm được đạt được với encoder 3 lớp cho Devign và encoder 1 lớp cho ReVeal. Kết quả này cho thấy rằng có thể vừa giảm tài nguyên vừa cải thiện hiệu suất của mô hình trong quá trình fine-tuning trên tác vụ phát hiện lỗi với cả bộ dữ liệu cân bằng và không cân bằng.

Để khám phá sự đánh đổi giữa việc sử dụng tài nguyên và suy giảm hiệu suất cho nhận dạng kiểu bug và kiểu exception, chúng tôi hiển thị tăng tốc trung bình của một epoch fine-tuning và mất mát hiệu suất so với baseline cho các bộ dữ liệu BIFI và Exception Type trong Bảng 2. Chúng tôi cũng báo cáo các giá trị tương ứng cho Devign và ReVeal, mà cho đó cả lợi ích và mất mát hiệu suất được chỉ ra. Tăng tốc được báo cáo như một hệ số tỷ lệ của thời gian baseline. Sự khác biệt metric được hiển thị như lợi ích hoặc mất mát của weighted F1-score và độ chính xác so với hiệu suất baseline. Các cải thiện có ý nghĩa thống kê được báo cáo in đậm, trong khi các mất mát không có ý nghĩa thống kê được đánh dấu bằng dấu sao (∗). Kích thước hiệu ứng A12 được chỉ ra bởi ba sắc thái xanh dương như màu ô, với sắc thái tối nhất chỉ ra hiệu ứng lớn (A12>0.71), sắc thái giữa chỉ ra hiệu ứng trung bình (A12>0.64), và sắc thái nhạt nhất chỉ ra hiệu ứng nhỏ (A12>0.56). Chúng tôi cũng gạch chân và thảo luận các kết quả được chọn cải thiện các giá trị metric và giảm việc sử dụng tài nguyên.

Phần lớn các kết hợp (xii) với các mô hình được tỉa vượt trội hơn baseline cho Devign và ReVeal. Hơn nữa, các mô hình với 2–10 lớp cho thấy cải thiện có ý nghĩa thống kê của cả hai metrics trên Devign, với mô hình 3 lớp đạt được cải thiện độ chính xác +2 với tăng tốc fine-tuning trung bình 3.3 lần với cùng phần cứng và phần mềm. Không chỉ mô hình 3 lớp cải thiện độ chính xác so với baseline CodeBERT lên 63.7, mà còn vượt trội hơn một số mô hình khác được kiểm tra trên Devign và được báo cáo trên benchmark CodeXGLUE [27]. Cụ thể, mô hình CodeBERT 3 lớp được tỉa của chúng tôi vượt trội hơn mô hình transformer đầy đủ PLBART [1], và các biểu diễn code2vec được huấn luyện trước trên cây cú pháp trừu tượng và token code một cách chung [1]. Tuy nhiên, mô hình được tỉa của chúng tôi không vượt trội hơn mô hình hoạt động tốt nhất được báo cáo trên CodeXGLUE, CoText, đạt được 66.62 độ chính xác [34].

Các mô hình với 1 và 11 lớp đạt được cải thiện độ chính xác có ý nghĩa thống kê cho ReVeal. Tuy nhiên, mô hình 1 lớp giảm điểm F1(w). Việc sử dụng lớp 11 không tác động đến tốc độ fine-tuning, trong khi mô hình 1 lớp mang lại gia tốc 3.7x của tốc độ fine-tuning baseline. Sự thiếu tăng tốc với mô hình 11 lớp có thể được giải thích bởi thực tế rằng số lượng tham số có thể huấn luyện không giảm tuyến tính với việc loại bỏ các lớp sau, vì lớp embedding bổ sung và classification head vẫn không thay đổi. Mô hình 2 lớp dẫn đến cải thiện tốt nhất của F1(w) có ý nghĩa thống kê. Mô hình 2 lớp cũng cải thiện độ chính xác trên ReVeal. Đối với Devign và ReVeal, các cải thiện có ý nghĩa thống kê có kích thước hiệu ứng lớn.

Đối với BIFI, chúng tôi có được giảm không có ý nghĩa thống kê của F1(w) và độ chính xác theo kiểm định Wilcoxon mang lại khoảng 1.2x tăng tốc của fine-tuning với mô hình 11 lớp. Nếu chúng tôi giảm số lượng lớp xuống 8, hiệu suất trên BIFI vẫn trong giới hạn (baseline metric −1), nhưng chúng tôi đạt được lên đến 1.7x tăng tốc trung bình của fine-tuning một epoch. Trong trường hợp sử dụng các mô hình với 1–10 lớp, chúng tôi quan sát một sự thay đổi có ý nghĩa thống kê của phân phối và giảm các giá trị metric.

Đối với bộ dữ liệu Exception Type không cân bằng, hiệu suất giảm nhanh hơn và tăng tốc ít nổi bật hơn so với BIFI. Sự thay đổi giá trị trung bình của các metrics cho tất cả các mô hình có ý nghĩa thống kê. Chi tiết, các metrics giảm -1.0 giá trị metric tuyệt đối tại 11 lớp với 1.1x tăng tốc fine-tuning và -1.8 với 10 lớp với 1.2x tăng tốc. Chúng tôi giải thích sự suy giảm mạnh hơn của hiệu suất kết hợp bởi các giá trị metric baseline thấp hơn (75.39 độ chính xác, 75.30 weighted F1-score) so với trường hợp BIFI (96.7 độ chính xác và weighted F1-score). Đối với BIFI, sự suy thoái không có ý nghĩa thống kê có kích thước hiệu ứng nhỏ. Tuy nhiên, đối với cả hai bộ dữ liệu BIFI và Exception Type, chúng tôi quan sát sự suy thoái hiệu suất của kích thước hiệu ứng lớn với các mô hình được tỉa.

Chúng tôi kết luận rằng đối với bộ dữ liệu BIFI với baseline hoạt động cao và 3 lớp, mất mát hiệu suất khi loại bỏ mỗi lớp ít hơn so với bộ dữ liệu phân loại Exception Type với hiệu suất baseline thấp hơn và 20 lớp. Việc sử dụng tài nguyên, có tương quan với thời gian dành cho điều chỉnh, giảm nhanh hơn cho BIFI so với Exception Type. Điều này được giải thích một phần bởi classification head lớn hơn cho bộ dữ liệu Exception Type, bởi vì bộ dữ liệu này có 20 lớp trái ngược với chỉ 3 lớp trong BIFI. Nói cách khác, chúng tôi quan sát rằng bộ dữ liệu BIFI có một baseline mạnh khó vượt qua với việc tỉa. Ngược lại, độ phức tạp của bộ dữ liệu Exception Type có thể ảnh hưởng đến kết quả theo hướng ngược lại: Hiệu suất baseline đã không mạnh lắm, và nó khó cải thiện thêm với chỉ các lớp sớm.

Trả lời cho RQ2. Chúng tôi có được cải thiện hiệu suất so với baseline cũng như tăng tốc fine-tuning cho cả hai bộ dữ liệu phát hiện lỗi bằng cách sử dụng token CLS từ lớp cuối cùng của các mô hình được tỉa. Đối với phân loại đa lớp, hiệu suất giảm khi tỉa mỗi lớp từ cuối mô hình. Sự giảm mạnh hơn cho bộ dữ liệu với 20 kiểu exception so với tác vụ với 3 kiểu bug.

6.3 Mối Đe Dọa Đối Với Tính Hợp Lệ
Mối đe dọa chính đối với tính hợp lệ bên ngoài là kết quả mang tính thực nghiệm và có thể không tổng quát hóa cho tất cả các thiết lập phân loại code, bao gồm các ngôn ngữ lập trình, tác vụ, và mô hình dựa trên encoder cho code khác. Chúng tôi đã kiểm tra các kết hợp EarlyBIRD trên code trong C để phát hiện lỗi và Python để phân loại kiểu bug và kiểu exception trong nghiên cứu này. Việc lựa chọn CodeBERT làm mô hình encoder và cấu trúc nội bộ của nó ảnh hưởng đến kết quả. Ví dụ, một mô hình encoder lấy các chuỗi đầu vào nhỏ hơn có thể hoạt động tệ hơn trên cùng các bộ dữ liệu, bởi vì các phần lớn hơn của các chuỗi code đầu vào phải được tỉa trong trường hợp này. Tính hợp lệ bên ngoài có thể được cải thiện bằng cách kiểm tra trên nhiều bộ dữ liệu và mô hình encoder hơn.

Các mối đe dọa đối với tính hợp lệ nội bộ liên quan đến sự phụ thuộc của các mô hình vào việc khởi tạo các tham số có thể huấn luyện và lựa chọn phương pháp. Classification head và weighted sums với các tham số có thể huấn luyện trong các thí nghiệm của chúng tôi phụ thuộc vào việc khởi tạo các tham số và có thể dẫn mô hình đến các cực tiểu địa phương khác nhau trong quá trình fine-tuning. Để giảm ảnh hưởng của các khởi tạo ngẫu nhiên khác nhau, chúng tôi đã fine-tune và kiểm tra tất cả các kết hợp EarlyBIRD 10 lần với các seed ngẫu nhiên khác nhau.

Ngoài ra, chúng tôi đã sử dụng kiểm định Wilcoxon để xác minh liệu các cải thiện đạt được có ý nghĩa thống kê. Tuy nhiên, kiểm định Wilcoxon chỉ ước tính liệu các phép đo của các giá trị baseline và kết hợp EarlyBIRD có được rút ra từ các phân phối khác nhau. Thời gian được báo cáo dành cho fine-tuning và các tăng tốc tương ứng có mục đích minh họa việc giảm sử dụng tài nguyên, và sẽ phụ thuộc vào phần cứng được sử dụng. Ngay cả khi sử dụng các hệ số tăng tốc cho các mô hình được tỉa, có khả năng những con số này sẽ khác trên các cấu hình phần cứng khác.

Chúng tôi đã triển khai các thuật toán và quy trình thống kê trong Python, với sự trợ giúp của các thư viện được sử dụng rộng rãi như PyTorch, NumPy và SciPy. Tuy nhiên, chúng tôi không thể đảm bảo sự vắng mặt của các lỗi triển khai có thể đã ảnh hưởng đến đánh giá của chúng tôi.

7 NHẬN XÉT KẾT LUẬN
Trong bài báo này, chúng tôi đã đề xuất EarlyBIRD, một phương pháp kết hợp các lớp sớm của các mô hình encoder cho code, và kiểm tra các kết hợp lớp sớm khác nhau trên các tác vụ kỹ thuật phần mềm về phát hiện lỗi, phân loại kiểu bug và kiểu exception. Nghiên cứu của chúng tôi được thúc đẩy bởi giả thuyết rằng các lớp sớm chứa thông tin có giá trị bị loại bỏ bởi thực hành tiêu chuẩn biểu diễn code với token CLS từ lớp encoder cuối cùng. EarlyBIRD cung cấp các cách để cải thiện hiệu suất của các mô hình hiện có với cùng việc sử dụng tài nguyên, cũng như để giảm sử dụng tài nguyên trong khi có được kết quả tương đương với baseline.

Kết quả: Sử dụng EarlyBIRD, chúng tôi có được các cải thiện có ý nghĩa thống kê so với baseline cho phần lớn các kết hợp liên quan đến một lớp encoder đơn lẻ trên phát hiện lỗi, và với các kết hợp EarlyBIRD được chọn lọc trên phân loại kiểu bug và kiểu exception. Max pooling của tokens từ các lớp đơn lẻ được chọn mang lại cải thiện hiệu suất cho tất cả các bộ dữ liệu. Cả hiệu suất phân loại và thời gian fine-tuning trung bình cho một epoch đều được cải thiện bằng cách tỉa mô hình được huấn luyện trước thành các lớp sớm của nó và sử dụng token CLS từ lớp cuối cùng của mô hình được tỉa. Đối với phát hiện lỗi, điều này dẫn đến tăng +2.0 độ chính xác và tăng tốc fine-tuning 3.3x trên Devign, và lên đến cải thiện độ chính xác +0.8 với tăng tốc 3.7x trên ReVeal. Các mô hình được tỉa không dẫn đến lợi ích hiệu suất phân loại đa lớp, nhưng chúng cho thấy tăng tốc fine-tuning và giảm tiêu thụ tài nguyên liên quan.

Kết quả cho thấy rằng các mô hình được tỉa với kích thước giảm hoặc hoạt động tốt hơn hoặc có thể dẫn đến giảm sử dụng tài nguyên trong quá trình fine-tuning với các mức biến đổi hiệu suất khác nhau, điều này cho thấy tiềm năng của EarlyBIRD trong các kịch bản hạn chế tài nguyên triển khai phát hiện lỗi và phân loại kiểu bug trong môi trường sản xuất. Ví dụ, EarlyBIRD đạt được tăng tốc 2.1x cho BIFI trong khi giảm độ chính xác từ 96.7 xuống 95.0.

Công việc Tương lai: Nghiên cứu có thể được mở rộng bằng cách điều tra sự tổng quát hóa cho các mô hình encoder khác. Chúng tôi đang trong quá trình nghiên cứu hiệu suất của EarlyBIRD với hai mô hình encoder mới: StarEncoder [23] và ContraBERT_C [25]. Một hướng khác cho nghiên cứu tương lai là liệu các loại kết hợp lớp và tỉa, như chúng tôi đã điều tra trong bài báo này cho các kiến trúc encoder, cũng là các kỹ thuật hiệu quả cho các kiến trúc decoder và encoder-decoder. Hơn nữa, sẽ thú vị khi thử nghiệm với các tác vụ phân loại code khác, chẳng hạn như phát hiện bug chung và dự đoán các kiểu lỗ hổng. Cái sau có thể được điều tra sử dụng các kiểu CWE từ Common Weakness Enumeration như được gắn nhãn trong bộ dữ liệu CVEfixes [5].

LỜI CẢM ƠN
Công việc này được hỗ trợ bởi Research Council of Norway thông qua dự án secureIT (IKTPLUSS #288787). Max Hort được hỗ trợ thông qua Chương trình Học bổng ERCIM 'Alain Bensoussan'. Đánh giá thực nghiệm được trình bày trong bài báo này đã được thực hiện trên Experimental Infrastructure for Exploration of Exascale Computing (eX3), được hỗ trợ tài chính bởi Research Council of Norway theo hợp đồng #270053, cũng như trên các tài nguyên được cung cấp bởi Sigma2, Cơ sở Hạ tầng Quốc gia cho Điện toán Hiệu suất Cao và Lưu trữ Dữ liệu ở Na Uy.

KHẢ NĂNG TRUY CẬP DỮ LIỆU
Để hỗ trợ khoa học mở và cho phép tái tạo và xác minh công việc của chúng tôi, tất cả các artifacts được cung cấp thông qua Zenodo tại URL sau: https://doi.org/10.5281/zenodo.7608802.

TÀI LIỆU THAM KHẢO
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211

[2] Toufique Ahmed và Premkumar Devanbu. 2022. Multilingual Training for Software Engineering. Trong Proceedings of the 44th International Conference on Software Engineering. ACM, Pittsburgh Pennsylvania, 1443–1455. https://doi.org/10.1145/3510003.3510049

[3] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, và Charles Sutton. 2018. A Survey of Machine Learning for Big Code and Naturalness. Comput. Surveys 51, 4 (July 2018), 81:1–81:37. https://doi.org/10.1145/3212695

[4] Berkay Berabi, Jingxuan He, Veselin Raychev, và Martin Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer. Trong International Conference on Machine Learning, Vol. 139. PMLR, Virtual Event, 780–791.

[5] Guru Bhandari, Amara Naseer, và Leon Moonen. 2021. CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software. Trong International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE). ACM, 30–39. https://doi.org/10.1145/3475960.3475985

[6] Terra Blevins, Omer Levy, và Luke Zettlemoyer. 2018. Deep RNNs Encode Soft Hierarchical Syntax. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Melbourne, Australia, 14–19. https://doi.org/10.18653/v1/p18-2003

[7] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, và Baishakhi Ray. 2022. Deep Learning Based Vulnerability Detection: Are We There Yet? IEEE Transactions on Software Engineering 48, 9 (Sept. 2022), 3280–3296. https://doi.org/10.1109/tse.2021.3087402

[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. https://doi.org/10.48550/arXiv.2107.03374 arXiv:2107.03374 [cs]

[9] Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, và Martin Monperrus. 2019. SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair. IEEE Transactions on Software Engineering 47, 9 (2019), 1943–1959. https://doi.org/10.1109/tse.2019.2940179

[10] Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, và Neel Sundaresan. 2020. PyMT5: Multi-Mode Translation of Natural Language and Python Code with Transformers. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 9052–9065. https://doi.org/10.18653/v1/2020.emnlp-main.728

[11] P. Devanbu. 2015. New Initiative: The Naturalness of Software. Trong Proceedings - International Conference on Software Engineering, Vol. 2. IEEE Computer Society, 543–546. https://doi.org/10.1109/icse.2015.190

[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL, Vol 1). Association for Computational Linguistics, 4171–4186. https://doi.org/10.18653/v1/n19-1423

[13] Angela Fan, Edouard Grave, và Armand Joulin. 2019. Reducing Transformer Depth on Demand with Structured Dropout. https://doi.org/10.48550/arXiv.1909.11556 arXiv:1909.11556 [cs, stat]

[14] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Trong Findings of the Association for Computational Linguistics: EMNLP 2020. Online, 1536–1547. https://doi.org/10.18653/v1/2020.findings-emnlp.139

[15] Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen, và Dinh Phung. 2022. VulRepair: A T5-based Automated Software Vulnerability Repair. Trong Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). Association for Computing Machinery, New York, NY, USA, 935–947. https://doi.org/10.1145/3540250.3549098

[16] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, và Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with Data Flow. Trong International Conference on Learning Representations, ICLR 2021. Virtual Event, Austria, 1–18. arXiv:2009.08366 [cs]

[17] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, và Miltiadis Allamanis. 2018. Deep Learning Type Inference. Trong Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). ACM, New York, NY, USA, 152–162. https://doi.org/10.1145/3236024.3236051

[18] José Antonio Hernández López, Martin Weyssow, Jesús Sánchez Cuadrado, và Houari Sahraoui. 2023. AST-Probe: Recovering Abstract Syntax Trees from Hidden Representations of Pre-Trained Language Models. Trong Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22). Association for Computing Machinery, New York, NY, USA, 1–11. https://doi.org/10.1145/3551349.3556900

[19] Jeremy Howard và Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Melbourne, Australia, 328–339. https://doi.org/10.18653/v1/p18-1031

[20] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, và Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code. Trong Proceedings of the 37th International Conference on Machine Learning. PMLR, 5110–5121. arXiv:2001.00059 [cs]

[21] Anjan Karmakar và Romain Robbes. 2021. What Do Pre-Trained Code Models Know about Code?. Trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). 1332–1336. https://doi.org/10.1109/ase51524.2021.9678927

[22] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. https://doi.org/10.48550/arXiv.1909.11942 arXiv:1909.11942 [cs]

[23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, và Harm de Vries. 2023. StarCoder: May the Source Be with You! https://doi.org/10.48550/arXiv.2305.06161 arXiv:2305.06161 [cs]

[24] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, và Noah A. Smith. 2019. Linguistic Knowledge and Transferability of Contextual Representations. https://doi.org/10.48550/arXiv.1903.08855 arXiv:1903.08855 [cs]

[25] Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, và Yang Liu. 2023. ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning. Trong Proceedings of the 45th International Conference on Software Engineering (ICSE '23). IEEE Press, Melbourne, Victoria, Australia, 2476–2487. https://doi.org/10.1109/icse48619.2023.00207

[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692 arXiv:1907.11692 [cs]

[27] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. Trong Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. 1–16. arXiv:2102.04664 [cs]

[28] Changan Niu, Chuanyi Li, Bin Luo, và Vincent Ng. 2022. Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code. https://doi.org/10.48550/arXiv.2205.11739 arXiv:2205.11739 [cs]

[29] Matteo Paltenghi và Michael Pradel. 2021. Thinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code. Trong 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). 867–879. https://doi.org/10.1109/ase51524.2021.9678712

[30] Cong Pan, Minyan Lu, và Biao Xu. 2021. An Empirical Study on Software Defect Prediction Using CodeBERT Model. Applied Sciences 11, 11 (May 2021), 4793. https://doi.org/10.3390/app11114793

[31] David Peer, Sebastian Stabinger, Stefan Engl, và Antonio Rodriguez-Sanchez. 2022. Greedy-Layer Pruning: Speeding up Transformer Models for Natural Language Processing. Pattern Recognition Letters 157 (May 2022), 76–82. https://doi.org/10.1016/j.patrec.2022.03.023 arXiv:2105.14839 [cs]

[32] Matthew Peters, Mark Neumann, Luke Zettlemoyer, và Wen-tau Yih. 2018. Dissecting Contextual Word Embeddings: Architecture and Representation. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 1499–1509. https://doi.org/10.18653/v1/d18-1179

[33] Matthew E. Peters, Sebastian Ruder, và Noah A. Smith. 2019. To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks. Trong Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019). Association for Computational Linguistics, Florence, Italy, 7–14. https://doi.org/10.18653/v1/w19-4302

[34] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Annibal, Alec Peltekian, và Yanfang Ye. 2021. CoTexT: Multi-task Learning with Code-Text Transformer. Trong Workshop on Natural Language Processing for Programming (NLP4Prog 2021). Association for Computational Linguistics, Online, 40–47. https://doi.org/10.18653/v1/2021.nlp4prog-1.5

[35] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood, và Marc McConley. 2018. Automated Vulnerability Detection in Source Code Using Deep Representation Learning. Trong International Conference on Machine Learning and Applications (ICMLA). IEEE, Orlando, FL, 757–762. https://doi.org/10.1109/icmla.2018.00120

[36] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, và Preslav Nakov. 2023. On the Effect of Dropping Layers of Pre-trained Transformer Models. Computer Speech & Language 77 (Jan. 2023), 101429. https://doi.org/10.1016/j.csl.2022.101429 arXiv:2004.03844 [cs]

[37] Rishab Sharma, Fuxiang Chen, Fatemeh Fard, và David Lo. 2022. An Exploratory Study on Code Attention in BERT. Trong Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension (ICPC '22). Association for Computing Machinery, New York, NY, USA, 437–448. https://doi.org/10.1145/3524610.3527921

[38] Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, và Federica Sarro. 2021. A Survey on Machine Learning Techniques for Source Code Analysis. https://doi.org/10.48550/arXiv.2110.09610 arXiv:2110.09610 [cs]

[39] Chi Sun, Xipeng Qiu, Yige Xu, và Xuanjing Huang. 2019. How to Fine-Tune BERT for Text Classification?. Trong Chinese Computational Linguistics (Lecture Notes in Computer Science), Maosong Sun, Xuanjing Huang, Heng Ji, Zhiyuan Liu, và Yang Liu (Eds.). Springer International Publishing, Cham, 194–206. https://doi.org/10.1007/978-3-030-32381-3_16

[40] Chi Sun, Xipeng Qiu, Yige Xu, và Xuanjing Huang. 2020. How to Fine-Tune BERT for Text Classification? https://doi.org/10.48550/arXiv.1905.05583 arXiv:1905.05583 [cs]

[41] András Vargha và Harold D. Delaney. 2000. A Critique and Improvement of the CL Common Language Effect Size Statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics 25, 2 (June 2000), 101–132. https://doi.org/10.3102/10769986025002101

[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention Is All You Need. Trong International Conference on Neural Information Processing Systems (NeurIPS), I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett (Eds.). Curran Associates, Inc., 5998–6008. arXiv:1706.03762 [cs]

[43] Yue Wang, Weishi Wang, Shafiq Joty, và Steven C.H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 8696–8708. https://doi.org/10.18653/v1/2021.emnlp-main.685

[44] Hongwei Wei, Guanjun Lin, Lin Li, và Heming Jia. 2021. A Context-Aware Neural Embedding for Function-Level Vulnerability Detection. Algorithms 14, 11 (Nov. 2021), 335. https://doi.org/10.3390/a14110335

[45] Frank Wilcoxon. 1992. Individual Comparisons by Ranking Methods. Trong Breakthroughs in Statistics: Methodology and Distribution, Samuel Kotz và Norman L. Johnson (Eds.). Springer, New York, NY, 196–202. https://doi.org/10.1007/978-1-4612-4380-9_16

[46] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, và Quoc V. Le. 2020. XLNet: Generalized Autoregressive Pretraining for Language Understanding. https://doi.org/10.48550/arXiv.1906.08237 arXiv:1906.08237 [cs]

[47] Michihiro Yasunaga và Percy Liang. 2021. Break-It-Fix-It: Unsupervised Learning for Program Repair. Trong International Conference on Machine Learning. PMLR, 12. arXiv:2106.06600 [cs]

[48] He Ye, Matias Martinez, và Martin Monperrus. 2022. Neural Program Repair with Execution-Based Backpropagation. Trong Proceedings of the 44th International Conference on Software Engineering (ICSE '22). Association for Computing Machinery, New York, NY, USA, 1506–1518. https://doi.org/10.1145/3510003.3510222

[49] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, và Yoav Artzi. 2021. Revisiting Few-sample BERT Fine-tuning. Trong NeurIPS 2021. 1–22. arXiv:2006.05987 [cs]

[50] Gang Zhao và Jeff Huang. 2018. DeepSim: Deep Learning Code Functional Similarity. Trong Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM, Lake Buena Vista FL USA, 141–151. https://doi.org/10.1145/3236024.3236068

[51] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, và Yang Liu. 2019. Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks. Trong International Conference on Neural Information Processing Systems (NeurIPS). Curran Associates, Inc., Vancouver, Canada., 11. arXiv:1909.03496 [cs]

Nhận ngày 2023-02-02; chấp nhận ngày 2023-07-27
