# 2311.14904.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2311.14904.pdf
# Kích thước tệp: 1258452 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẢN DỤNG LLM HỖ TRỢ LÀM SẠCH MÃ NGUỒN ĐỂ HUẤN LUYỆN CÁC BỘ SINH MÃ CHÍNH XÁC

Naman Jain, Tianjun Zhang, Wei-Lin Chiang, Joseph E. Gonzalez, Koushik Sen & Ion Stoica
Đại học California, Berkeley
{naman_jain,tianjunz,weichiang,jegonzal,ksen,istoica}@berkeley.edu

TÓM TẮT
Việc sinh mã từ ngôn ngữ tự nhiên là một lĩnh vực ứng dụng quan trọng của các LLM và đã nhận được sự quan tâm rộng rãi từ cộng đồng. Phần lớn các nghiên cứu liên quan đã tập trung độc quyền vào việc tăng số lượng và tính đúng đắn về mặt chức năng của các tập huấn luyện trong khi bỏ qua các yếu tố phong cách khác của chương trình. Gần đây hơn, chất lượng dữ liệu đã thu hút được nhiều quan tâm và nhiều công trình đã chứng minh tầm quan trọng của nó trong việc cải thiện hiệu suất. Trong công trình này, chúng tôi nghiên cứu chất lượng dữ liệu cho mã nguồn và phát hiện rằng việc làm cho mã có cấu trúc và dễ đọc hơn dẫn đến hiệu suất sinh mã cải thiện của hệ thống. Chúng tôi xây dựng một pipeline làm sạch dữ liệu mới sử dụng các nguyên tắc này để biến đổi các chương trình hiện có bằng cách 1.) đổi tên biến, 2.) mô-đun hóa và phân tách mã phức tạp thành các hàm phụ trợ nhỏ hơn, và 3.) chèn các kế hoạch dựa trên ngôn ngữ tự nhiên thông qua các phép biến đổi dựa trên LLM. Chúng tôi đánh giá phương pháp của mình trên hai benchmark sinh mã thuật toán đầy thử thách và phát hiện rằng việc tinh chỉnh CODELLAMA-7B trên các chương trình mô-đun hóa được biến đổi của chúng tôi cải thiện hiệu suất lên đến 30% so với việc tinh chỉnh trên tập dữ liệu gốc. Ngoài ra, chúng tôi chứng minh hiệu suất cải thiện từ việc sử dụng một lượng nhỏ hơn dữ liệu chất lượng cao, phát hiện rằng một mô hình được tinh chỉnh trên toàn bộ tập dữ liệu gốc bị vượt trội bởi một mô hình được huấn luyện trên 15% tập dữ liệu đã được làm sạch của chúng tôi. Ngay cả khi so sánh với các mô hình nguồn đóng, các mô hình của chúng tôi vượt trội hơn các mô hình ALPHA CODE lớn hơn nhiều (Li et al., 2022).

1 GIỚI THIỆU
Việc sinh mã từ ngôn ngữ tự nhiên đã chứng kiến những tiến bộ đáng kể trong những năm gần đây với sự ra đời của các mô hình ngôn ngữ lớn (LLM). Những tiến bộ này chủ yếu phát sinh từ việc huấn luyện trên dữ liệu quy mô web lớn và được đo lường dựa trên tính đúng đắn về mặt chức năng của các chương trình. Do đó, các khía cạnh khác như khả năng đọc, cấu trúc hóa, và tạo kiểu và cách chúng ảnh hưởng đến huấn luyện và chất lượng dữ liệu phần lớn bị bỏ qua bởi các công trình này. Mặt khác, nhiều công trình gần đây đã chứng minh hiệu quả của việc huấn luyện trên dữ liệu chất lượng cao hơn trong cả giai đoạn tiền huấn luyện (Li et al., 2023d) và tinh chỉnh (Zhou et al., 2023; Cao et al., 2023). Ngay cả trong lĩnh vực sinh mã, Gunasekar et al. (2023) đã chứng minh lợi ích của việc huấn luyện trên tập dữ liệu chất lượng "sách giáo khoa", được tạo ra tổng hợp bằng mô hình GPT-3.5-TURBO (Ouyang et al., 2022). Tuy nhiên, các công trình này không cung cấp hiểu biết về các yếu tố thực sự cải thiện chất lượng dữ liệu.

Trong công trình này, chúng tôi chỉ ra rằng việc sử dụng các chương trình tuân theo các thực hành lập trình tốt và cho phép khả năng đọc nhiều hơn dẫn đến hiệu suất sinh mã cải thiện so với việc sử dụng các chương trình không tuân theo các thực hành này. Chúng tôi sử dụng những hiểu biết này để xây dựng một pipeline làm sạch dữ liệu mã tự động mới biến đổi các chương trình trong khi duy trì tính đúng đắn về mặt chức năng bằng cách sử dụng các ví dụ đầu vào-đầu ra. Trái ngược với các công trình trước đây tuyển chọn các tập dữ liệu chất lượng cao bằng cách trực tiếp tạo ra dữ liệu mới bằng LLM, ở đây chúng tôi dịch các tập dữ liệu hiện có thành các phiên bản song song đã được làm sạch trong khi xác định các thuộc tính thực sự cải thiện chất lượng dữ liệu.

Chúng tôi sử dụng LLM để thực hiện các phép biến đổi được sử dụng trong phương pháp làm sạch dữ liệu của chúng tôi. Chúng tôi chứng minh rằng các mô hình được điều chỉnh theo hướng dẫn có thể nhận một thuộc tính chất lượng dữ liệu do người dùng xác định như một hướng dẫn ngôn ngữ tự nhiên và thực hiện phép biến đổi một cách chính xác. Phương pháp của chúng tôi tận dụng sự chênh lệch về độ khó giữa việc tạo ra một giải pháp và chỉnh sửa một giải pháp hiện có. Do đó, nó đặc biệt hiệu quả trong các lĩnh vực mà mô hình hiện tại gặp khó khăn trong việc tạo ra một giải pháp đúng nhưng có thể chỉnh sửa hiệu quả.

--- TRANG 2 ---
Hình 1: Tổng quan về phương pháp làm sạch mã của chúng tôi. Chúng tôi áp dụng các LLM được điều chỉnh theo hướng dẫn để biến đổi các tập dữ liệu hiện có bằng cách cung cấp các lời nhắc ngôn ngữ tự nhiên và sử dụng các ví dụ đầu vào-đầu ra để duy trì tính tương đương chức năng giữa các chương trình gốc và được biến đổi. Phương pháp làm sạch của chúng tôi hoạt động theo ba bước. Hình trên-trái mô tả chương trình gốc từ tập dữ liệu. Chương trình này đầu tiên trải qua việc đổi tên biến (hình trên-phải). Tiếp theo, chương trình đã đổi tên được phân tách thành các hàm phụ cấu thành và được chuyển đổi thành một chương trình mô-đun hóa (hình dưới-phải). Cuối cùng, chúng tôi tạo ra một kế hoạch ngôn ngữ tự nhiên từ chương trình mô-đun hóa bằng cách tóm tắt các hàm theo cách từ trên xuống (hình dưới-trái). Kế hoạch này được đặt trước chương trình như một bình luận. Hình giữa-trái trình bày phát biểu bài toán được cắt ngắn.

một giải pháp đã cho. Chúng tôi thực hiện các phép biến đổi làm sạch dữ liệu của mình trong ba lần lặp: 1) đổi tên biến 2) mô-đun hóa mã phức tạp thành các hàm phụ, và 3) thêm các chú thích lập kế hoạch. Hình 1 cung cấp tổng quan về phương pháp của chúng tôi. Lưu ý rằng bước đổi tên biến ở trên điều chỉnh tên biến để có liên quan theo ngữ cảnh (ví dụ: a thành root_u và d thành graph). Bước mô-đun hóa (được mô tả ở bên phải) xác định và phân tách chương trình gốc thành nhiều hàm phụ nhỏ hơn như find_root, merge_trees, build_graph, v.v. Sau đó nó triển khai các chương trình con này và lắp ráp chương trình mô-đun. Cuối cùng, bước lập kế hoạch của chúng tôi (được mô tả ở dưới) xây dựng một kế hoạch bằng cách tóm tắt các hàm theo cách từ trên xuống (bắt đầu từ main).

Chúng tôi đánh giá phương pháp của mình trong một lĩnh vực thích hợp, nhưng đầy thử thách, của việc sinh mã thuật toán. Mục tiêu là tạo ra một chương trình cho một phát biểu bài toán đã cho. Nhiệm vụ này đầy thử thách vì nó đòi hỏi cả lý luận thuật toán cấp cao và lập trình cấp thấp và được đánh giá bằng một metric tính đúng đắn về mặt chức năng nghiêm ngặt. Chúng tôi sử dụng hai benchmark sinh mã thuật toán nổi tiếng, cụ thể là APPS (Hendrycks et al., 2021) và CODE-CONTESTS (Li et al., 2022). Chúng tôi biến đổi các chương trình tương ứng trong các tập huấn luyện và thu được các tập dữ liệu song song từ phương pháp làm sạch của chúng tôi. Ngoài ra, chúng tôi sử dụng các ví dụ đầu vào-đầu ra để duy trì tính tương đương chức năng giữa các chương trình gốc và được biến đổi. Chúng tôi phân tích định tính tập dữ liệu được tạo ra và thấy rằng nó sử dụng

--- TRANG 3 ---
các hàm phụ trợ nhỏ hơn, mỗi hàm thường triển khai một thuật toán tiêu chuẩn hoặc chức năng chương trình chính, và cung cấp các phát hiện sâu sắc hơn trong Mục 4.1. Chúng tôi tiếp tục đánh giá tác động của các tập dữ liệu được biến đổi đối với hiệu suất trên nhiệm vụ sinh mã hạ nguồn của chúng tôi. Chúng tôi tinh chỉnh mô hình CODELLAMA-7B trên các tập dữ liệu thu thập khác nhau. Những phát hiện của chúng tôi tiết lộ rằng mô hình được tinh chỉnh trên tập dữ liệu mô-đun hóa của chúng tôi vượt trội hơn mô hình được tinh chỉnh trên tập dữ liệu gốc tương đương về mặt chức năng lên đến 30%.

Ngoài việc cải thiện hiệu suất, chúng tôi cũng chứng minh rằng việc cải thiện chất lượng dữ liệu giúp cải thiện hiệu quả dữ liệu. Cụ thể, một mô hình được tinh chỉnh trên toàn bộ tập dữ liệu gốc bị vượt trội bởi một mô hình được huấn luyện chỉ trên 15% tập dữ liệu đã được làm sạch của chúng tôi.

Tiếp theo chúng tôi nghiên cứu việc cải thiện lập kế hoạch trong một thiết lập học có giám sát tương tự như các công trình trước đây (Fu et al., 2023; Li et al., 2023b). Trong khi chúng tôi quan sát những cải thiện hạn chế trong lập kế hoạch, chúng tôi tách biệt khả năng lập kế hoạch so với khả năng lập trình và thấy rằng mô hình được tinh chỉnh của chúng tôi có khả năng sử dụng các kế hoạch được chú thích vàng, được trích xuất từ các giải pháp chân lý cơ bản để tạo ra các giải pháp chính xác cho các chương trình phức tạp. Điều này làm nổi bật việc lập kế hoạch cho các vấn đề phức tạp vẫn là một nút thắt cổ chai chính không có vẻ cải thiện bằng cách chỉ đơn giản là tăng các tập dữ liệu huấn luyện. Cuối cùng, so với các baseline hiện có, các mô hình được tinh chỉnh của chúng tôi vượt trội hơn các mô hình ALPHA CODE lớn hơn (Li et al., 2022).

2 PHƯƠNG PHÁP LUẬN
Trong mục này, chúng tôi trình bày phương pháp biến đổi dữ liệu tổng quát của chúng tôi và sau đó cụ thể hóa nó để thực hiện làm sạch dữ liệu mã.

2.1 CÁC PHÉP BIẾN ĐỔI CHO LÀM SẠCH DỮ LIỆU
Cho một tập dữ liệu D gồm N trường hợp di, sao cho D={di}N i=1. Để đạt được một đặc tả làm sạch dữ liệu mong muốn, người dùng bổ sung cung cấp một hướng dẫn làm sạch dữ liệu I, làm nổi bật một thuộc tính cần được sửa đổi. Tùy chọn, chúng tôi cũng sử dụng một bộ kiểm tra tương đương tiên tri (O) đảm bảo rằng trường hợp dữ liệu được biến đổi d̃i nhất quán với đầu vào gốc dựa trên một số metric mong muốn. Ví dụ, chúng ta có thể sử dụng khoảng cách chỉnh sửa hoặc tương đương chức năng dựa trên các ví dụ đầu vào-đầu ra làm bộ kiểm tra tiên tri của chúng ta.

Chúng tôi sử dụng một mô hình ngôn ngữ được tiền huấn luyện (ký hiệu là M) để tạo ra trường hợp được biến đổi (d̃i) bằng cách nhắc mô hình với hướng dẫn biến đổi (I) và câu trả lời gốc (y). Chúng ta có thể thực hiện nhắc zero-shot hoặc few-shot để thực hiện thao tác làm sạch dữ liệu. Cuối cùng, chúng tôi trích xuất trường hợp d̃i được tạo ra bởi M, và áp dụng bộ kiểm tra tương đương tiên tri (O) của chúng tôi để đảm bảo tính nhất quán với dữ liệu gốc. Nếu O(d̃i,di) = 0, tức là tiên tri báo cáo thất bại, chúng tôi từ chối đầu ra được tạo ra và thử lại ví dụ trong ngân sách lấy mẫu.

Trong khi phương pháp biến đổi của chúng tôi không cung cấp bất kỳ đảm bảo nào về chất lượng của phép biến đổi được thực hiện và dựa vào LLM, chúng tôi quan sát thực nghiệm rằng các LLM được điều chỉnh theo hướng dẫn có thể thực hiện các bước làm sạch dữ liệu không có cấu trúc khác nhau khá hiệu quả. Chúng tôi cung cấp một phân tích chi tiết về các đầu ra được tạo ra cho thiết lập sinh mã thuật toán của chúng tôi trong Mục 4.1. Cuối cùng, phù hợp với tài liệu hiện có về việc nhắc LLM, chúng tôi thấy rằng việc sử dụng các hướng dẫn đơn giản và chính xác, cấp thấp giúp cải thiện hiệu suất và độ chính xác của các mô hình trong việc thực hiện các thao tác. Do đó, đối với các thao tác làm sạch dữ liệu phức tạp (tái cấu trúc), chúng tôi thấy những cải thiện bằng cách chia nhỏ và thực hiện nhiều thao tác một cách lặp đi lặp lại (đổi tên tiếp theo bởi mô-đun hóa).

2.2 LÀM SẠCH DỮ LIỆU MÃ
Chúng tôi áp dụng phương pháp làm sạch dữ liệu dựa trên phép biến đổi của chúng tôi cho dữ liệu lập trình. Việc lập trình đòi hỏi cả kỹ năng lập trình cấp thấp và kỹ năng lý luận hoặc lập kế hoạch cấp cao. Do đó, chúng tôi đề xuất một pipeline làm sạch ba bước cải thiện khả năng đọc và cấu trúc chương trình nhắm mục tiêu vào kỹ năng lập trình cấp thấp và chèn dữ liệu kế hoạch dựa trên ngôn ngữ tự nhiên nhắm mục tiêu vào kỹ năng lý luận cấp cao. Các bước của chúng tôi được chi tiết dưới đây.

1. Đổi tên biến. Bước này đổi tên các biến trong chương trình, làm cho chúng mô tả và dễ theo dõi hơn. Hình 1 trên cung cấp một ví dụ về phép biến đổi này.

2. Mô-đun hóa các hàm. Phân tách vấn đề đã được xác định là một phương pháp chính để cải thiện khả năng lý luận của các mô hình (Zhou et al., 2022; Wang et al., 2023). Chúng tôi

--- TRANG 4 ---
chia APPS-INTRODUCTORY APPS-INTERVIEW APPS-COMPETITION CODE-CONTESTS
Số lượng bài toán huấn luyện 42 1247 361 7132
kiểm tra 702 2699 309 165
Số lượng test huấn luyện 1 1 9 200
kiểm tra 10 19 39 200
Số lượng giải pháp huấn luyện 736 18394 5060 98582

Bảng 1: Chi tiết về số lượng bài toán, số lượng trung vị của các trường hợp kiểm tra trên mỗi bài toán, và số lượng giải pháp trong các tập dữ liệu APPS và CODE-CONTESTS.

xác định các phân tách chương trình và biến đổi chương trình bằng cách trích xuất chức năng của chúng thành các hàm trợ giúp nhỏ hơn. Hình 1 bên phải cung cấp một ví dụ về phép biến đổi này.

3. Chú thích kế hoạch. Bước này tóm tắt các hàm trợ giúp trong chương trình đã được mô-đun hóa và đặt trước nó vào các chương trình dưới dạng một kế hoạch ngôn ngữ tự nhiên. Những mô tả ngôn ngữ tự nhiên này tương tự với các phương pháp nhắc được sử dụng để giải quyết các vấn đề lý luận như nhắc chuỗi suy nghĩ (Wei et al., 2022), parsel (Zelikman et al., 2023), v.v. Hình 1 dưới cung cấp một ví dụ về phép biến đổi này.

Ngoài ra, trong khi thực hiện các phép biến đổi này, chúng tôi sử dụng các trường hợp kiểm tra được cung cấp trong tập dữ liệu để xây dựng bộ kiểm tra tương đương tiên tri (O) của chúng tôi. Nó đảm bảo rằng các chương trình được biến đổi của chúng tôi duy trì tính tương đương chức năng với chương trình gốc.

3 THIẾT LẬP THỰC NGHIỆM
Trong mục này, chúng tôi chi tiết thiết lập thực nghiệm và triển khai của chúng tôi. Mục 3.1 phác thảo các benchmark và metric được sử dụng cho nhiệm vụ sinh mã thuật toán, trong khi Mục 3.2 và 3.3 đi sâu vào các chi tiết cụ thể của phương pháp làm sạch mã và các thí nghiệm tinh chỉnh tương ứng.

3.1 BENCHMARK
Chúng tôi sử dụng hai benchmark sinh mã thuật toán tiêu chuẩn, APPS và CODE-CONTESTS. Các benchmark cung cấp một tập hợp các phát biểu bài toán được mô tả bằng ngôn ngữ tự nhiên và các trường hợp kiểm tra tương ứng. Mục tiêu là tạo ra một chương trình giải quyết thành công bài toán. Việc đánh giá được thực hiện bằng cách sử dụng một metric tính đúng đắn về mặt chức năng nghiêm ngặt.

APPS (Hendrycks et al., 2021). Benchmark này bao gồm 10.000 bài toán, được chia đều giữa các tập huấn luyện và kiểm tra. Nó được lấy từ nhiều trang web lập trình cạnh tranh truy cập mở. Nó được chia thêm thành các tập con APPS-INTRODUCTORY, APPS-INTERVIEW, và APPS-COMPETITION dựa trên độ khó của bài toán. Trong nghiên cứu này, chúng tôi chỉ xem xét các bài toán được lấy từ một tập con các trang web cuộc thi dựa trên số lượng trường hợp kiểm tra được cung cấp.

CODE-CONTESTS (Li et al., 2022). Benchmark này bao gồm 13.328 bài toán trong tập huấn luyện và 165 bài toán trong tập kiểm tra. Chúng tôi chỉ sử dụng một tập con của phần chia huấn luyện bao gồm các giải pháp python thỏa mãn các trường hợp kiểm tra được cung cấp. Ngoài ra, vì tập huấn luyện cung cấp hơn một trăm giải pháp cho mỗi bài toán, chúng tôi thực hiện khử trùng lặp dựa trên LSH trên các giải pháp và giới hạn chúng tối đa 25 giải pháp cho mỗi bài toán.

Bảng 1 và Phụ lục A cung cấp thêm chi tiết về các tập dữ liệu cuối cùng của chúng tôi.

Metric. Chúng tôi đánh giá hiệu suất sinh mã của các mô hình bằng cách sử dụng metric PASS@K (Kulal et al., 2019; Chen et al., 2021), đánh giá tính đúng đắn về mặt chức năng của các chương trình được tạo ra. Đối với mỗi bài toán, chúng tôi tạo ra N giải pháp (trong đó N≥2K) và tính toán số lượng kịch bản mong đợi trong đó bài toán được giải quyết ít nhất một lần khi chọn ngẫu nhiên một mẫu con của K giải pháp. Chúng tôi thay đổi K trong {1,10,25} cho tập dữ liệu APPS và {1,10,100} cho benchmark CODE-CONTESTS. Chúng tôi trình bày thêm chi tiết về các siêu tham số lấy mẫu trong Phụ lục A.

3.2 CÁC PHÉP BIẾN ĐỔI DỮ LIỆU
Chúng tôi áp dụng phương pháp biến đổi dữ liệu của chúng tôi trên các tập dữ liệu APPS và CODE-CONTESTS. Trừ khi được chỉ định khác, chúng tôi sử dụng GPT-3.5-TURBO làm mô hình ngôn ngữ M mặc định để thực hiện các phép biến đổi và sử dụng nhiệt độ mặc định 0.3. Trong trường hợp thất bại, chúng tôi thử lại tối đa 5 lần lặp. Chúng tôi

--- TRANG 5 ---
Ký hiệu Tập dữ liệu Áp dụng Trên Hướng dẫn Biến đổi (I)
Base D_original - -
Rename D_rename D_original Đổi tên các biến trong chương trình để có tính mô tả, ý nghĩa và nhất quán
Modularize D_modular D_rename Tái cấu trúc chương trình trên làm cho nó mô-đun hóa hơn với các hàm trợ giúp nhỏ hơn và có ý nghĩa với tên mô tả tốt cho các hàm trợ giúp
Plan D_planning D_modular Tạo ra một mô tả ngôn ngữ tự nhiên cho các hàm sau đây trong chương trình

Bảng 2: Các tập dữ liệu được biến đổi tạo ra bởi phương pháp làm sạch mã của chúng tôi. Đối với mỗi phép biến đổi, chúng tôi đã cung cấp ký hiệu tương ứng, hướng dẫn biến đổi được sử dụng để thực hiện bước làm sạch và tập dữ liệu mà phép biến đổi được áp dụng lên.

thu được ba tập dữ liệu song song ở cuối quá trình làm sạch của chúng tôi, một cho mỗi việc đổi tên, mô-đun hóa, và lập kế hoạch (lưu ý rằng các phép biến đổi được áp dụng tuần tự). Bảng 2 cung cấp tóm tắt các tập dữ liệu được tạo ra cùng với các hướng dẫn được sử dụng để tạo ra chúng. Chúng tôi cung cấp chi tiết đầy đủ về các phép biến đổi trong Phụ lục B.

Chúng tôi cũng mô phỏng một phương pháp tạo dữ liệu tổng hợp trực tiếp đơn giản phần nào tương tự như Gunasekar et al. (2023). Cụ thể, chúng tôi tạo ra các giải pháp cho các bài toán huấn luyện bằng cách sử dụng mô hình GPT-3.5-TURBO. Chúng tôi sử dụng học trong ngữ cảnh với các ví dụ nhắc hai-shot được chọn từ tập dữ liệu D_modular của chúng tôi. Để đảm bảo các giải pháp đa dạng, chúng tôi sử dụng ba ví dụ few-shot riêng biệt và tạo ra tám giải pháp cho mỗi lời nhắc ở nhiệt độ 0.5. Ngoài ra, chúng tôi lọc các giải pháp về tính đúng đắn dựa trên các trường hợp kiểm tra chân lý cơ bản được cung cấp trong tập dữ liệu để đảm bảo chúng tôi không huấn luyện trên các chương trình không đúng. Vì nó giống một thiết lập giống như chưng cất, chúng tôi gọi tập dữ liệu này là D_distill.

3.3 CHI TIẾT THỰC NGHIỆM
Để đánh giá chất lượng của các tập dữ liệu được biến đổi, chúng tôi đo lường cách chúng tác động đến độ chính xác benchmark kiểm tra. Chúng tôi nghiên cứu cả học trong ngữ cảnh và tinh chỉnh bằng cách sử dụng các ví dụ từ các tập dữ liệu của chúng tôi.

Mô hình. Chúng tôi sử dụng mô hình CODELLAMA-7B (Rozière et al., 2023) trong tất cả các thí nghiệm của chúng tôi (được gọi là CL-7B về sau). Chúng tôi sử dụng checkpoint mô hình từ huggingface¹ và thực hiện suy luận theo lô thông qua VLLM (Kwon et al., 2023), cần thiết để tính toán metric PASS@K. Chúng tôi cũng trình bày các số từ CODE-DAVINCI-002 và GPT-3.5-TURBO khi có sẵn.

Học trong ngữ cảnh. Chúng tôi chọn hai cặp câu hỏi-câu trả lời từ các tập huấn luyện D_original và D_modular làm ví dụ học trong ngữ cảnh của chúng tôi. Để so sánh công bằng giữa hai đánh giá, chúng tôi sử dụng cùng một bài toán và các giải pháp tương ứng từ hai tập dữ liệu làm ví dụ. Các ví dụ được kết hợp với các dấu phân cách thích hợp và mô hình sau đó được nhắc với một bài toán mới. Lưu ý rằng những ví dụ học trong ngữ cảnh này tăng độ dài chuỗi hơn 2.000 token và làm chậm đáng kể việc suy luận.

Tinh chỉnh. Chúng tôi thực hiện tinh chỉnh đầy đủ trên mô hình CL-7B cơ sở trên các tập dữ liệu khác nhau. Chúng tôi huấn luyện các mô hình trong hai epoch trên tập dữ liệu APPS và một epoch trên tập dữ liệu CODE-CONTESTS bằng cách sử dụng tốc độ học 5e−5 và kích thước lô hiệu quả 256 trên 4 GPU A6000.

4 KẾT QUẢ THỰC NGHIỆM
Chúng tôi trình bày các kết quả thực nghiệm của chúng tôi trong mục này. Mục 4.1 đầu tiên cung cấp tổng quan định tính về các chương trình được biến đổi và Mục 4.2 trình bày các kết quả sinh mã chính.

4.1 PHÂN TÍCH CÁC CHƯƠNG TRÌNH ĐƯỢC BIẾN ĐỔI
Thống kê dữ liệu. Đối với tập dữ liệu CODE-CONTESTS, trong số 98.582 chương trình được trích xuất từ tập dữ liệu gốc (D_original), chúng tôi có thể biến đổi thành công 92.675 (94.0%) thành tập dữ liệu mô-đun hóa của chúng tôi (D_modular). Chúng tôi thu được tỷ lệ thành công tương tự cho tập dữ liệu APPS (chi tiết được hoãn lại đến phụ lục). Ngược lại, tập dữ liệu chưng cất (D_distill), được xây dựng bằng cách tạo ra các giải pháp trực tiếp bằng GPT-3.5-TURBO chỉ tìm thấy giải pháp đúng cho khoảng 50% các bài toán.

Phân tích các chương trình được biến đổi. Chúng tôi thấy rằng phương pháp biến đổi của chúng tôi phân tách các chương trình gốc bằng cách chèn ba hàm mới ở mức trung vị (~2.6 hàm trung bình). Để hiểu rõ hơn về việc phân tách, chúng tôi phân cụm các hàm bằng cách sử dụng tên hàm và chữ ký của chúng. Chúng tôi thấy rằng những hàm trợ giúp này thường triển khai logic chương trình chính, thuật toán tiêu chuẩn, và các tiện ích như xử lý đầu vào, đầu ra, và điều phối hàm chính. Thú vị là, chúng tôi cũng thấy rằng các hàm trợ giúp thường được tái sử dụng qua các bài toán, với các biến thể nhỏ trong triển khai. Ví dụ, năm hàm trợ giúp thường gặp nhất, dfs, build_graph, gcd, dp, và binary_search xuất hiện trong khoảng 3-8% các bài toán. Ngoài ra, chúng tôi phân tích định tính một trăm mẫu ngẫu nhiên từ các tập dữ liệu D_original và D_modular để xác định chất lượng của các phép biến đổi được thực hiện. Hình 4 đến 11 trong phụ lục cung cấp các ví dụ về những phép biến đổi như vậy. Chúng tôi thấy rằng hầu hết các phép biến đổi đều có ý nghĩa. Chúng cải thiện khả năng đọc của các chương trình và cũng tìm thấy việc phân tách phù hợp cho logic chương trình được mã hóa trong luồng điều khiển (xem Hình 4, 5, 6, 14 làm ví dụ). Tuy nhiên, trong một số trường hợp, các hàm trợ giúp được tạo ra có thể có tên không phù hợp (calculate_max_colors trong Hình 11) hoặc triển khai phức tạp được sao chép trực tiếp từ chương trình gốc (count_sequences trong Hình 12). Ngoài ra, đối với các chương trình đơn giản hơn (Hình 13), toàn bộ chức năng chương trình có thể được triển khai trong một hàm duy nhất và việc phân tách không cung cấp thông tin bổ sung nào. Cuối cùng, chúng tôi sử dụng GPT-4 làm trọng tài (Zheng et al., 2023) đánh giá để đánh giá định lượng các phép biến đổi về tính có ý nghĩa của chúng và về tính nhất quán của các chương trình gốc và được biến đổi. Phụ lục C.1 trình bày thiết lập toàn diện. Chúng tôi thấy rằng hơn 99% các phép biến đổi được coi là hữu ích trong đó chỉ 3-5% ví dụ được đánh giá là có thể làm tốt hơn. Tương tự, 99.4% các chương trình được biến đổi được đánh giá là nhất quán với các chương trình gốc. Kết quả đánh giá chi tiết hơn trong Bảng 6.

Không giống như mã được tạo ra, chúng tôi không thể hạn chế hoặc kiểm tra các kế hoạch ngôn ngữ tự nhiên được tạo ra. Do đó, chúng tôi thấy rằng đôi khi các kế hoạch có thể không chính xác và khác nhau về chi tiết. Trong khi việc sử dụng một mô hình tiền huấn luyện mạnh hơn như GPT-4 có thể giảm thiểu một số vấn đề này, chúng tôi tin rằng đây sẽ là một hướng tốt để áp dụng thứ gì đó tương tự như giám sát quy trình (Lightman et al., 2023).

4.2 KẾT QUẢ CHÍNH
Bảng 3 và 4 cung cấp các kết quả chính của chúng tôi trên các tập dữ liệu APPS và CODE-CONTESTS tương ứng. Chúng tôi hoãn lại các kết quả cho tập con APPS-COMPETITION đến Phụ lục C và làm nổi bật các phát hiện của chúng tôi dưới đây.

4.2.1 TÁC ĐỘNG CỦA MÔ-ĐUN HÓA
Chúng tôi thấy rằng phương pháp làm sạch dữ liệu của chúng tôi cải thiện hiệu suất của mô hình trên cả tập dữ liệu APPS và CODE-CONTESTS trong cả thiết lập học trong ngữ cảnh và tinh chỉnh.

¹https://huggingface.co/codellama/CodeLlama-7b-hf
²Kết quả mô hình được lấy từ Chen et al. (2022a)

--- TRANG 6 ---
APPS-INTRODUCTORY APPS-INTERVIEW
PASS@1 PASS@10 PASS@25 PASS@1 PASS@10 PASS@25
Học trong ngữ cảnh
CL-7B +D_original 14.2 29.2 38.4 1.8 7.3 10.4
CL-7B +D_modular 17.5 30.1 39.7 2.2 8.6 12.3
+3.3 +0.9 +1.3 +0.4 +1.3 +1.9
Tinh chỉnh
CL-7B +D_original 18.7 34.4 40.2 3.4 9.7 13.6
CL-7B +D_modular 22.7 36.9 42.6 4.2 11.0 15.0
+4.0 +2.5 +2.4 +0.8 +1.3 +1.4
CL-7B +D_planning 22.1 37.1 43.8 3.7 10.5 14.8
CL-7B +D_rename 19.2 36.6 42.9 4.0 10.7 14.6
CL-7B +D_distill 21.1 35.3 40.5 4.1 10.8 14.5
Mô hình đóng
CODE-DAVINCI-002² 22.1 50.2 58.7 4.1 16.8 23.8

Bảng 3: Kết quả trên tập dữ liệu APPS. Chúng tôi sử dụng mô hình CODELLAMA-7B (được gọi là CL-7B) dưới học trong ngữ cảnh và tinh chỉnh. Chúng tôi sử dụng các mẫu từ các tập dữ liệu gốc và được biến đổi của chúng tôi và thấy rằng các tập dữ liệu đã được làm sạch của chúng tôi cải thiện hiệu suất của mô hình hơn 20%. Các số được tô sáng màu xanh lá cây mô tả những cải thiện thu được từ việc sử dụng D_modular (so với D_original). Tương tự, việc sử dụng D_rename và D_planning cũng cung cấp những cải thiện, thường ít hơn so với việc sử dụng D_modular.

Học trong ngữ cảnh. Đầu tiên chúng tôi đánh giá hiệu suất của mô hình khi được cung cấp các ví dụ học trong ngữ cảnh hai-shot song song từ các tập dữ liệu D_original và D_modular. Chúng tôi thấy rằng PASS@1 cải thiện từ 14.2 lên 17.5 (cải thiện tương đối 23%) trên tập dữ liệu APPS-INTRODUCTORY và PASS@100 cải thiện từ 7.2 lên 9.3 (cải thiện tương đối 29%) trên tập dữ liệu CODE-CONTESTS. Những kết quả này chỉ ra rằng khả năng đọc tốt hơn và lập trình có cấu trúc tốt hơn hữu ích cho mô hình trong việc giải quyết nhiều bài toán hơn.

Tinh chỉnh. Tiếp theo, chúng tôi tinh chỉnh mô hình trên các tập dữ liệu D_original và D_modular và một lần nữa thấy những cải thiện hiệu suất mạnh mẽ từ phương pháp biến đổi của chúng tôi. Cụ thể, trên tập dữ liệu APPS-INTRODUCTORY, PASS@1 cải thiện từ 18.7 lên 22.7 (cải thiện tương đối 23%). Tương tự, metric PASS@25 của tập dữ liệu CODE-CONTESTS cải thiện từ 6.4 lên 8.4 (cải thiện tương đối 30%). Những kết quả này củng cố các phát hiện ở trên về tác động của việc làm sạch dữ liệu.

Thú vị là, chúng tôi cũng lưu ý rằng tinh chỉnh chỉ cung cấp những cải thiện khiêm tốn so với hiệu suất học trong ngữ cảnh. Chúng tôi giả định rằng điều này là do tính chất đầy thử thách của nhiệm vụ của chúng tôi.⁶

4.2.2 TÁC ĐỘNG CỦA CHÚ THÍCH LẬP KẾ HOẠCH
Các công trình trước đây đã chứng minh những thành công đáng kể trong việc cải thiện lý luận trong LLM (Yue et al., 2023; Magister et al., 2022; Fu et al., 2023) bằng cách thực hiện học có giám sát trên các bước lý luận hoặc lập kế hoạch ngôn ngữ tự nhiên. Chúng tôi thực hiện thí nghiệm tương tự, tinh chỉnh mô hình trên tập dữ liệu D_planning bao gồm các kế hoạch được tạo ra bởi phương pháp của chúng tôi dựa trên D_modular. Chúng tôi thấy rằng lập kế hoạch chỉ cung cấp một cải thiện khiêm tốn so với tập dữ liệu D_modular (PASS@25 cải thiện từ 42.6 lên 43.9 trên tập dữ liệu APPS-INTRODUCTORY) hoặc thường không có cải thiện nào cả.

Khi kiểm tra các giải pháp được tạo ra, chúng tôi thấy rằng thường các kế hoạch được tạo ra không chính xác hoặc sai, làm nổi bật rằng lập kế hoạch vẫn là một nút thắt cổ chai. Để tách biệt lập kế hoạch cấp cao với thành phần lập trình, chúng tôi phân tích hiệu suất của mô hình khi được cung cấp các kế hoạch chân lý cơ bản trên tập dữ liệu CODE-CONTESTS. Chúng tôi trích xuất những kế hoạch chân lý cơ bản này bằng cách áp dụng phương pháp biến đổi dữ liệu của chúng tôi trên tập kiểm tra (tương tự như cách tập huấn luyện D_planning được tạo ra). Bảng 5 cung cấp kết quả trên tập con 109 bài toán này từ tập dữ liệu CODE-CONTESTS mà chúng tôi có thể trích xuất các kế hoạch chân lý cơ bản (vì một số bài toán không có giải pháp python hợp lệ). Trong khi mô hình của chúng tôi được huấn luyện trên tập dữ liệu D_planning không có khả năng tổng hợp các kế hoạch mới, nó có thể tuân theo các kế hoạch được tạo ra một cách chính xác. Tất cả các metric đều cải thiện đáng kể, ví dụ PASS@100 cải thiện từ 17.8 lên 28.1, vượt xa hiệu suất của GPT-3.5-TURBO, một mô hình lớn hơn nhiều!

³Kết quả được lấy từ Li et al. (2022)
⁴Kết quả được lấy từ Zhang et al. (2023b)
⁵Kết quả được lấy từ Li et al. (2023c)

--- TRANG 7 ---
CODE-CONTESTS
PASS@10 PASS@25 PASS@100
Học trong ngữ cảnh
CL-7B +D_original 5.1 6.5 7.2
CL-7B +D_modular 4.9 6.6 9.3
-0.2 +0.1 +2.1
Tinh chỉnh
CL-7B +D_original 5 6.4 10.9
CL-7B +D_modular 6.1 8.3 12.4
+1.1 +1.9 +1.5
CL-7B +D_planning 5.3 7.0 10.8
CL-7B +D_rename 4.7 6.3 10.5
Mô hình đóng
ALPHA CODE-9B³ 5.0 7.0 10.0
ALPHA CODE-41B³ 5.0 7.0 10.0
CODE-DAVINCI-002⁴ 3.0 - 7.5
GPT-3.5-TURBO⁵ - - 18.2
+ BRAINSTORM⁵ - - 29.3

Bảng 4: Kết quả trên tập dữ liệu CODE-CONTESTS. Tương tự như các phát hiện trên tập dữ liệu APPS, chúng tôi thấy rằng phương pháp làm sạch dữ liệu của chúng tôi nói chung cải thiện hiệu suất với mô-đun hóa hoạt động đặc biệt tốt trong khi lập kế hoạch và đổi tên cung cấp những cải thiện từ nhỏ đến không có.

CODE-CONTESTS-PLAN
PASS@10 PASS@25 PASS@100
CL-7B +D_original 6.5 9.5 15.0
CL-7B +D_modular 8.8 11.8 17.8
CL-7B +D_planning 6.9 10.5 15.4
CL-7B +D_GT plan 17.9 22.3 28.1
+9.1 +10.5 +11.3

Bảng 5: Tác động của việc sử dụng kế hoạch chân lý cơ bản. Chúng tôi tách biệt khả năng lý luận cấp cao so với khả năng lập trình bằng cách trích xuất các kế hoạch chân lý cơ bản từ các giải pháp tương ứng với các bài toán kiểm tra. Chúng tôi thấy cải thiện đáng kể trong hiệu suất trên tập dữ liệu CODE-CONTESTS-PLAN, chỉ ra rằng mô hình được huấn luyện trên tập dữ liệu D_planning trong khi không có khả năng xây dựng kế hoạch đúng, có thể tuân theo những kế hoạch như vậy một cách chính xác.

Học trong ngữ cảnh. Đầu tiên chúng tôi đánh giá hiệu suất của mô hình khi được cung cấp các ví dụ học trong ngữ cảnh hai-shot song song từ các tập dữ liệu D_original và D_modular. Chúng tôi thấy rằng PASS@1 cải thiện từ 14.2 lên 17.5 (cải thiện tương đối 23%) trên tập dữ liệu APPS-INTRODUCTORY và PASS@100 cải thiện từ 7.2 lên 9.3 (cải thiện tương đối 29%) trên tập dữ liệu CODE-CONTESTS. Những kết quả này chỉ ra rằng khả năng đọc tốt hơn và lập trình có cấu trúc tốt hơn hữu ích cho mô hình trong việc giải quyết nhiều bài toán hơn.

Tinh chỉnh. Tiếp theo, chúng tôi tinh chỉnh mô hình trên các tập dữ liệu D_original và D_modular và một lần nữa thấy những cải thiện hiệu suất mạnh mẽ từ phương pháp biến đổi của chúng tôi. Cụ thể, trên tập dữ liệu APPS-INTRODUCTORY, PASS@1 cải thiện từ 18.7 lên 22.7 (cải thiện tương đối 23%). Tương tự, metric PASS@25 của tập dữ liệu CODE-CONTESTS cải thiện từ 6.4 lên 8.4 (cải thiện tương đối 30%). Những kết quả này củng cố các phát hiện ở trên về tác động của việc làm sạch dữ liệu.

Thú vị là, chúng tôi cũng lưu ý rằng tinh chỉnh chỉ cung cấp những cải thiện khiêm tốn so với hiệu suất học trong ngữ cảnh. Chúng tôi giả định rằng điều này là do tính chất đầy thử thách của nhiệm vụ của chúng tôi.⁶

4.2.2 TÁC ĐỘNG CỦA CHÚ THÍCH LẬP KẾ HOẠCH
Các công trình trước đây đã chứng minh những thành công đáng kể trong việc cải thiện lý luận trong LLM (Yue et al., 2023; Magister et al., 2022; Fu et al., 2023) bằng cách thực hiện học có giám sát trên các bước lý luận hoặc lập kế hoạch ngôn ngữ tự nhiên. Chúng tôi thực hiện thí nghiệm tương tự, tinh chỉnh mô hình trên tập dữ liệu D_planning bao gồm các kế hoạch được tạo ra bởi phương pháp của chúng tôi dựa trên D_modular. Chúng tôi thấy rằng lập kế hoạch chỉ cung cấp một cải thiện khiêm tốn so với tập dữ liệu D_modular (PASS@25 cải thiện từ 42.6 lên 43.9 trên tập dữ liệu APPS-INTRODUCTORY) hoặc thường không có cải thiện nào cả.

Khi kiểm tra các giải pháp được tạo ra, chúng tôi thấy rằng thường các kế hoạch được tạo ra không chính xác hoặc sai, làm nổi bật rằng lập kế hoạch vẫn là một nút thắt cổ chai. Để tách biệt lập kế hoạch cấp cao với thành phần lập trình, chúng tôi phân tích hiệu suất của mô hình khi được cung cấp các kế hoạch chân lý cơ bản trên tập dữ liệu CODE-CONTESTS. Chúng tôi trích xuất những kế hoạch chân lý cơ bản này bằng cách áp dụng phương pháp biến đổi dữ liệu của chúng tôi trên tập kiểm tra (tương tự như cách tập huấn luyện D_planning được tạo ra). Bảng 5 cung cấp kết quả trên tập con 109 bài toán này từ tập dữ liệu CODE-CONTESTS mà chúng tôi có thể trích xuất các kế hoạch chân lý cơ bản (vì một số bài toán không có giải pháp python hợp lệ). Trong khi mô hình của chúng tôi được huấn luyện trên tập dữ liệu D_planning không có khả năng tổng hợp các kế hoạch mới, nó có thể tuân theo các kế hoạch được tạo ra một cách chính xác. Tất cả các metric đều cải thiện đáng kể, ví dụ PASS@100 cải thiện từ 17.8 lên 28.1, vượt xa hiệu suất của GPT-3.5-TURBO, một mô hình lớn hơn nhiều!

⁶Lưu ý rằng các ví dụ trong ngữ cảnh thêm hơn 2.000 token vào tiền tố và dẫn đến việc giải mã chậm hơn nhiều

--- TRANG 8 ---
def read_grid():
n,m = input().split()
...
def remove_white_rows(grid):
row_indices = []
...
return grid
def remove_white_columns(grid):
column_indices = []
...
return grid
def main():
grid = read_grid()
grid = remove_white_rows(grid)
grid = remove_white_columns(grid)
print_grid(grid)
...

Hình 2: Ví dụ về một chương trình được tạo ra bởi mô hình của chúng tôi được huấn luyện trên tập dữ liệu D_modular. Nó giải quyết bài toán bằng cách sử dụng các hàm trợ giúp hoạt động trên các hàng và cột.

[THIS IS CHART: Graph showing "Effect of quality on data-efficiency of the model" with x-axis "Number of training samples" (5000-20000) and y-axis "PASS@1" (14-22), showing two lines labeled "D_mod" and "D_orig"]

Hình 3: Tác động của chất lượng đối với hiệu quả dữ liệu của mô hình. Tinh chỉnh trên 15% tập dữ liệu D_modular sạch dẫn đến hiệu suất tương tự như tinh chỉnh trên toàn bộ tập dữ liệu D_original.

Các kết quả hỗn hợp của chúng tôi đặt ra những câu hỏi quan trọng cho công việc tương lai về cải thiện lập kế hoạch trong LLM. Cụ thể, hiệu suất kém có thể được quy cho bất kỳ sự không chính xác nào trong các kế hoạch được tạo ra tự động. Các kỹ thuật tuyển chọn dữ liệu tương lai lọc hoặc tăng cường sự không chính xác này sẽ có giá trị. Thay vào đó, mô hình học có giám sát được tuân theo trong công trình này có thể không đủ để các mô hình tổng quát hóa lập kế hoạch trong các lĩnh vực phức tạp. Công việc tương lai có thể khám phá các thuật toán học khác, có thể dựa trên phương pháp mô-đun hóa của chúng tôi tự nhiên phân tách các chương trình.

4.2.3 CÁC NGHIÊN CỨU LOẠI BỎ
Tác động của kích thước dữ liệu. Ngoài việc cải thiện chất lượng của mô hình kết quả, chất lượng dữ liệu cũng được quy cho việc cải thiện hiệu quả dữ liệu. Chúng tôi đánh giá khía cạnh này bằng cách tinh chỉnh mô hình của chúng tôi trên các phần khác nhau của các tập dữ liệu D_original và D_modular và thấy kết quả tương tự. Hình 3 trình bày hiệu suất của mô hình như một hàm của kích thước tập huấn luyện. Như được hiển thị trong hình, việc huấn luyện chỉ trên 15% tập dữ liệu D_modular đạt được PASS@1 tương tự như tinh chỉnh trên toàn bộ D_original.

Tác động của đổi tên. Chúng tôi sử dụng đổi tên biến như một bước trung gian trong quá trình làm sạch của chúng tôi. Chúng tôi đánh giá hiệu suất của mô hình được tinh chỉnh chỉ trên tập dữ liệu D_rename và thấy rằng đổi tên cung cấp một số cải thiện hiệu suất khi so sánh với tinh chỉnh trên tập dữ liệu D_original. Ví dụ, PASS@1 cải thiện từ 17.2 lên 19.1 trên APPS-INTRODUCTORY. Tuy nhiên, đổi tên vẫn hoạt động kém hơn so với tinh chỉnh trên D_modular. Điều này làm nổi bật rằng ngoài chỉ có mã dễ đọc, phân tách chức năng cũng là một khía cạnh chính để cải thiện hiệu suất của chúng tôi.

Các phép biến đổi làm sạch so với chưng cất. Chúng tôi so sánh phương pháp biến đổi của chúng tôi với một baseline chưng cất trực tiếp nơi chúng tôi trực tiếp tạo ra các giải pháp bằng GPT-3.5-TURBO, được gọi là tập dữ liệu D_distill⁷. Điều này tương ứng với các phương pháp hướng dẫn hoặc tinh chỉnh LLM khác nhau (Xu et al., 2023; Li et al., 2023b) cung cấp một baseline mạnh cho việc làm sạch dữ liệu. Trên tập dữ liệu APPS-INTRODUCTORY, chúng tôi thấy rằng tinh chỉnh trên tập dữ liệu D_modular đạt được hiệu suất tốt hơn so với tập dữ liệu D_distill chứng minh lợi thế của làm sạch so với baseline tạo ra.

Lựa chọn mô hình biến đổi. Để đánh giá cách lựa chọn mô hình biến đổi ảnh hưởng đến hiệu suất, chúng tôi sử dụng mô hình GPT-4-TURBO để biến đổi trên một tập con của tập huấn luyện (thiết lập chi tiết trong Phụ lục C.3). GPT-4-TURBO, một mô hình mạnh hơn, thực hiện các phép biến đổi thành công và mô hình kết quả được huấn luyện trên phiên bản tập dữ liệu mô-đun hóa này đạt được độ chính xác cao hơn nữa. Ví dụ, PASS@10 cải thiện từ 33.0 khi sử dụng D_modular được xây dựng với GPT-3.5-TURBO lên 34.3 khi sử dụng D_modular được xây dựng với GPT-4-TURBO (kết quả đầy đủ trong Bảng 8).

4.2.4 SO SÁNH VỚI CÁC BASELINE KHÁC
Ngoài CL-7B, các mô hình được tinh chỉnh vượt trội hơn các baseline mạnh như ALPHA CODE trên tập dữ liệu CODE-CONTESTS nhưng vẫn thua các mô hình CODE-DAVINCI-002 và GPT-3.5-TURBO lớn hơn.

⁷Lưu ý rằng chúng tôi tạo ra những giải pháp này bằng cách sử dụng các ví dụ trong ngữ cảnh từ tập dữ liệu D_modular

--- TRANG 9 ---
4.2.5 NGHIÊN CỨU TRƯỜNG HỢP VỀ CHƯƠNG TRÌNH MÔ-ĐUN HÓA ĐƯỢC TẠO RA
Hình 2 cung cấp một ví dụ về một chương trình được tạo ra đúng bởi một mô hình được tinh chỉnh trên tập dữ liệu D_modular của chúng tôi. Bài toán đòi hỏi loại bỏ các hàng và cột chứa các ô có thuộc tính nhất định (tức là, nếu ô màu trắng). Giải pháp mô-đun hóa xác định đúng các bước cần thiết để giải quyết bài toán và triển khai chúng như các hàm trợ giúp riêng biệt, cung cấp mã dễ đọc.

5 CÔNG TRÌNH LIÊN QUAN
Điều chỉnh hướng dẫn. Điều chỉnh hướng dẫn đề cập đến quá trình tinh chỉnh một LLM cơ sở được tiền huấn luyện để thực hiện các nhiệm vụ đa mục đích và tuân theo hướng dẫn. Các công trình gần đây, Zhou et al. (2023); Cao et al. (2023); Chen et al. (2023a) đã chứng minh rằng một corpus hướng dẫn chất lượng cao nhỏ là đủ để đạt được hiệu suất điều chỉnh hướng dẫn tốt. Ở đây, chúng tôi thực hiện tinh chỉnh cụ thể cho nhiệm vụ của LLM và quan sát những cải thiện hiệu suất tương tự.

Dữ liệu tổng hợp cho LLM. Các công trình gần đây đã khám phá việc sử dụng các tập dữ liệu tổng hợp cho tinh chỉnh đa mục đích hoặc cụ thể cho nhiệm vụ của LLM. Những phương pháp này hoạt động bằng cách tạo ra các tập dữ liệu tổng hợp từ một LLM mạnh (như GPT-3.5-TURBO hoặc GPT-4) bằng cách sử dụng một tập hợp các nhiệm vụ hiện có (Taori et al., 2023; Chiang et al., 2023) hoặc tạo ra các nhiệm vụ mới bằng cách sử dụng self-instruct (Wang et al., 2022) hoặc các phương pháp evol-instruct (Xu et al., 2023). Điều này cũng đã được áp dụng cho tinh chỉnh cụ thể cho nhiệm vụ – trong lý luận thông thường (West et al., 2022), tóm tắt văn bản (Sclar et al., 2022), lý luận toán học (Luo et al., 2023a; Yue et al., 2023), sử dụng công cụ (Patil et al., 2023), lập trình (Luo et al., 2023b), và lý luận đa mục đích Li et al. (2023b); Zelikman et al. (2022).

Cụ thể hơn, Yue et al. (2023) tuyển chọn corpus đa dạng của các bài toán toán học với chú thích chuỗi suy nghĩ hoặc chương trình suy nghĩ (Chen et al., 2022b) cho lý luận toán học tương tự như các kế hoạch của chúng tôi. Gunasekar et al. (2023) đề xuất tiền huấn luyện các mô hình trên "sách giáo khoa" lập trình được tạo ra tổng hợp từ GPT-3.5-TURBO. Haluptzok et al. (2023) tương tự tạo ra các câu đố lập trình và giải pháp tương ứng từ các mô hình ngôn ngữ. Công trình của chúng tôi cũng nghiên cứu tuyển chọn dữ liệu tổng hợp cho không gian sinh mã. Tuy nhiên, thay vì trực tiếp tạo ra dữ liệu bằng LLM, chúng tôi xác định các mẫu lập trình tốt và làm sạch các tập dữ liệu hiện có bằng cách sử dụng chúng.

Sinh mã thuật toán. Sinh mã là một lĩnh vực rộng lớn và được bao phủ trong Phụ lục D. Chúng tôi chỉ thảo luận các công trình sinh mã thuật toán liên quan ở đây. Hendrycks et al. (2021) phát hành tập dữ liệu APPS trong khi Li et al. (2022) phát hành tập dữ liệu CODE-CONTESTS với các mô hình ALPHA CODE. Zhang et al. (2023c) đề xuất một thuật toán giải mã dựa trên tìm kiếm lookahead để cải thiện lý luận trong LLM và trực giao với công trình của chúng tôi. Chen et al. (2022a); Zhang et al. (2023b) đề xuất CODET và ALGO, sử dụng các test được tạo ra (bằng LLM hoặc giải pháp brute-force) để xếp hạng lại các giải pháp được tạo ra. Zelikman et al. (2023) đề xuất phương pháp PARSEL sử dụng mô hình CODE-DAVINCI-002 để đầu tiên tạo ra một kế hoạch trong ngôn ngữ đặc tả vấn đề cấp cao của họ và sau đó tạo ra một chương trình sử dụng nó. Li et al. (2023a) cũng nghiên cứu tách biệt khả năng lập kế hoạch và sinh mã cho các LLM nguồn đóng, tương tự như các thí nghiệm của chúng tôi trên các mô hình mở. Cuối cùng, công trình gần đây Le et al. (2023) đề xuất một phương pháp dựa trên nhắc cho sinh mã mô-đun.

6 THẢO LUẬN VÀ KẾT LUẬN
Truyền thống, chất lượng dữ liệu đã được liên kết với tính đúng đắn về mặt chức năng, bỏ qua các khía cạnh phong cách phong phú khác nhau giữa các chương trình. Trong công trình này, chúng tôi chứng minh rằng những khía cạnh này như khả năng đọc và cấu trúc chương trình thực sự tác động đến hiệu suất của mô hình được huấn luyện trên các nhiệm vụ hạ nguồn và do đó cũng đóng góp vào chất lượng dữ liệu. Tiếp theo, chúng tôi đề xuất một pipeline làm sạch dữ liệu mới chứng minh rằng LLM có thể được sử dụng để biến đổi các tập dữ liệu hiện có để cải thiện chất lượng của chúng dựa trên hướng dẫn người dùng và bộ kiểm tra tương đương tiên tri. Trong khi các đánh giá của chúng tôi tập trung vào nhiệm vụ sinh mã thuật toán, chúng tôi tin rằng phương pháp này cũng sẽ hữu ích cho các lĩnh vực khác để cải thiện chất lượng dữ liệu. Cụ thể, ngay cả khi không có các bộ kiểm tra tượng trưng (như các trường hợp kiểm tra), chúng tôi tin rằng có cơ hội sử dụng các "tiên tri" đã học để đảm bảo tính nhất quán và chất lượng trong các lĩnh vực khác tương tự như cách sử dụng trong Sclar et al. (2022). Cuối cùng, ngoài việc cải thiện sinh mã thuật toán, chúng tôi tin rằng phương pháp mô-đun hóa của chúng tôi có thể có lợi cho các trường hợp sử dụng kỹ thuật phần mềm tổng quát (tạo test, gỡ lỗi, xác minh) nơi tính mô-đun có lợi.

--- TRANG 10 ---
Lời cảm ơn Công trình này được hỗ trợ một phần bởi các tài trợ NSF CCF-1900968, CCF-1908870 và bởi các nhà tài trợ và đối tác công nghiệp SKY Lab Astronomer, Google, IBM, Intel, Lacework, Microsoft, Đại học Mohamed Bin Zayed về Trí tuệ Nhân tạo, Nexla, Samsung SDS, Uber, và VMware. Bất kỳ ý kiến, phát hiện, kết luận, hoặc khuyến nghị nào trong bài báo này hoàn toàn thuộc về các tác giả và không nhất thiết phản ánh quan điểm của các nhà tài trợ. Ngoài ra, chúng tôi cảm ơn Alex Gu, Manish Shetty, và các reviewer ẩn danh vì những thảo luận hữu ích và phản hồi về bài báo.

TÀI LIỆU THAM KHẢO
Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B. Ashok, và Shashank Shet. Codeplan: Repository-level coding using llms and planning. In Neural Information Processing Systems Workshop on Foundation Models for Decision Making (FMDM-NeurIPS), November 2023.

Yihan Cao, Yanbin Kang, và Lichao Sun. Instruction mining: High-quality instruction data selection for large language models. arXiv preprint arXiv:2307.06290, 2023.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, và Weizhu Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022a.

Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, và Junbo Zhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning. arXiv preprint arXiv:2305.09246, 2023a.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, và Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023b.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, và Tushar Khot. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023.

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.

Patrick Haluptzok, Matthew Bowers, và Adam Tauman Kalai. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations, 2023.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, và Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021.

Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, và Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620, 2023.

Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, và Rahul Sharma. Jigsaw: Large language models meet program synthesis. In ICSE 2022.

--- TRANG 11 ---
Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, và Ge Li. Self-planning code generation with large language model. arXiv preprint arXiv:2303.06689, 2023.

Darren Key, Wen-Ding Li, và Kevin Ellis. I speak, you verify: Toward trustworthy neural program synthesis. arXiv preprint arXiv:2210.00848, 2022.

Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, và Percy S Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, và Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.

Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, và Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. ArXiv, abs/2211.11501, 2022.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Hoi. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.

Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, và Shafiq Joty. Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules. arXiv preprint arXiv:2310.08992, 2023.

Jierui Li, Szymon Tworkowski, Yingying Wu, và Raymond Mooney. Explaining competitive-level programming solutions using llms. arXiv preprint arXiv:2307.05337, 2023a.

Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, và Yejin Choi. Symbolic chain-of-thought distillation: Small models can also "think" step-by-step. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2665–2679, Toronto, Canada, July 2023b. Association for Computational Linguistics.

Xin-Ye Li, Jiang-Tian Xue, Zheng Xie, và Ming Li. Think outside the code: Brainstorming boosts large language models in code generation. arXiv preprint arXiv:2305.10679, 2023c.

Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, và Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023d.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, và Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.

Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu, Xiaohong Zhang, và Meng Yan. Improving chatgpt prompt for code generation. arXiv preprint arXiv:2305.08360, 2023a.

Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, và Deheng Ye. Rltf: Reinforcement learning from unit test feedback, 2023b.

Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, và Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023a.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, và Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023b.

--- TRANG 12 ---
Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, và Aliaksei Severyn. Teaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730–27744, 2022.

Shishir G. Patil, Tianjun Zhang, Xin Wang, và Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Melanie Sclar, Peter West, Sachin Kumar, Yulia Tsvetkov, và Yejin Choi. Referee: Reference-free sentence summarization with sharper controllability through symbolic knowledge distillation. arXiv preprint arXiv:2210.13800, 2022.

Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, và Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, và Chandan K. Reddy. Execution-based code generation using deep reinforcement learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.

Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. Repository-level prompt generation for large language models of code. In International Conference on Machine Learning, pp. 31693–31715. PMLR, 2023.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, và Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, và Yejin Choi. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4602–4625, Seattle, United States, July 2022. Association for Computational Linguistics.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.

Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov, và Charles Sutton. Natural language to code generation in interactive data science notebooks. In Anna Rogers, Jordan Boyd-Graber, và Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 126–173, Toronto, Canada, July 2023. Association for Computational Linguistics.

--- TRANG 13 ---
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, và Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.

Eric Zelikman, Yuhuai Wu, Jesse Mu, và Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.

Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, và Nick Haber. Parsel: A (de-) compositional framework for algorithmic reasoning with language models. arXiv preprint arXiv:2212.10561, 2023.

Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, và Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023a.

Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, và Lei Li. Algo: Synthesizing algorithmic programs with generated oracle verifiers. arXiv preprint arXiv:2305.14591, 2023b.

Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, và Chuang Gan. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510, 2023c.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Daniel Fried Zhiruo Wang, Shuyan Zhou và Graham Neubig. Execution-based evaluation for open-domain code generation. 2022.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.

Terry Yue Zhuo. Large language models are state-of-the-art evaluators of code generation. arXiv preprint arXiv:2304.14317, 2023.

--- TRANG 14 ---
A THIẾT LẬP THỰC NGHIỆM
Benchmark APPS. Vì một số bài toán trong tập dữ liệu APPS được lấy từ các trang web cung cấp các trường hợp kiểm tra không đủ hoặc không có, chúng tôi lọc các bài toán từ những nền tảng đó. Cụ thể, chúng tôi chỉ giữ lại các bài toán từ các trang web cuộc thi codeforces, codechef, và atcoder. Điều này cũng loại bỏ sự chênh lệch/chuyển đổi miền giữa các phần chia huấn luyện và kiểm tra đã được quan sát như một vấn đề trong tập dữ liệu APPS trong các công trình trước đây (Mục 4.1 trong Li et al. (2023c)). Trong khi chúng tôi giảm đáng kể kích thước tập huấn luyện của mình, tập kiểm tra của chúng tôi vẫn khá gần với tập kiểm tra chứa khoảng 3800 bài toán thay vì 5000 mặc định.

Benchmark CODE-CONTESTS. Benchmark CODE-CONTESTS gốc bao gồm 13.328 bài toán trong tập dữ liệu huấn luyện. Chúng tôi hạn chế tập dữ liệu chỉ với các bài toán có giải pháp python hợp lệ vượt qua các trường hợp kiểm tra. Tiếp theo, vì tập dữ liệu gốc cung cấp hơn 100 giải pháp cho mỗi bài toán, chúng tôi thực hiện khử trùng lặp dựa trên minhash trên các giải pháp (kích thước hash=64, số band=60, kích thước band=5) từ gaoya⁸ và giữ lại tối đa 25 giải pháp cho mỗi bài toán. Điều này dẫn đến khoảng 7k bài toán trong tập huấn luyện trải dài khoảng 98.5k giải pháp. Chúng tôi không thực hiện bất kỳ lọc nào trên tập kiểm tra.

Ngoài ra, chúng tôi lưu ý rằng một số giải pháp được cung cấp trong cả tập dữ liệu APPS và CODE-CONTESTS không vượt qua các trường hợp kiểm tra. Những trường hợp này đôi khi được gây ra bởi các chương trình không đúng, tương ứng với các giải pháp trong ngôn ngữ lập trình sai. Tuy nhiên, thường xuyên hơn điều này được gây ra bởi các bài toán trong những tập dữ liệu này hỗ trợ nhiều giải pháp đúng (ví dụ các giải pháp có thể trả về một danh sách các phần tử theo bất kỳ thứ tự nào). Các trường hợp kiểm tra được cung cấp chỉ kiểm tra một giải pháp đúng duy nhất và do đó dẫn đến nhiều giải pháp thất bại trong các trường hợp kiểm tra. Chúng tôi giữ lại những mẫu như vậy cho tập dữ liệu APPS nhỏ hơn và sử dụng các chương trình đúng gốc để khớp hành vi đầu ra thay vì các đầu ra được cung cấp.

Metric Chúng tôi sử dụng PASS@K để thực hiện các đánh giá của chúng tôi. Chúng tôi thực hiện lấy mẫu nucleus bằng VLLM với p= 0.95. Chúng tôi phác thảo các cấu hình lấy mẫu mặc định được sử dụng để tính toán các metric

1. PASS@1 - Chúng tôi sử dụng ngân sách lấy mẫu (N) = 10 và nhiệt độ = 0.1.
2. PASS@10 - Chúng tôi sử dụng ngân sách lấy mẫu (N) = 50 và nhiệt độ = 0.6.
3. PASS@25 - Chúng tôi sử dụng ngân sách lấy mẫu (N) = 50 và nhiệt độ = 0.6.
4. PASS@100 - Chúng tôi sử dụng ngân sách lấy mẫu (N) = 200 và nhiệt độ = 0.8.

Chi tiết tinh chỉnh Chúng tôi tinh chỉnh mô hình CODELLAMA-7B bằng deepspeed huggingface trainer. Chúng tôi sử dụng cấu hình huấn luyện sau cho các thí nghiệm chính của chúng tôi -

Tham số Huấn luyện Giá trị
LR 5e−5
Epochs 1 hoặc 2 tùy thuộc vào tập dữ liệu
Batch Size 256 (kết hợp tích lũy grad.)
Dtype bf16

⁸https://github.com/serega/gaoya

--- TRANG 15 ---
B TRIỂN KHAI CÁC PHÉP BIẾN ĐỔI MÃ
Chúng tôi triển khai phương pháp biến đổi mã của chúng tôi bằng cách sử dụng nhắc zero-shot với mô hình GPT-3.5-TURBO. Sau khi biến đổi, chúng tôi trích xuất mã được tạo ra và đánh giá tính đúng đắn về mặt chức năng của nó bằng cách sử dụng các trường hợp kiểm tra được cung cấp. Trong trường hợp chương trình không vượt qua, chúng tôi thử lại quy trình với tối đa 5 lần thử. Theo kinh nghiệm của chúng tôi, các mô hình được điều chỉnh theo hướng dẫn có thể tuân theo các lệnh chính xác và biến đổi các chương trình rất tốt.

B.1 ĐỔI TÊN
Chúng tôi sử dụng lời nhắc sau để thực hiện đổi tên.

CÂU HỎI:
{problem_statement}
TRẢ LỜI:
```python
{solution}
```
Đổi tên các biến trong chương trình để có tính mô tả, ý nghĩa và nhất quán. Không thay đổi ngữ nghĩa gốc của chương trình. Đặt chương trình trong dấu backtick như được hiển thị ở trên và nhớ sử dụng tên biến mô tả.

B.2 MÔ-ĐUN HÓA
Không giống như đổi tên, chúng tôi thực hiện hai vòng mô-đun hóa trong trường hợp chương trình được tạo ra bao gồm các triển khai hàm dài (gợi ý rằng hàm có thể được phân tách thêm). Chúng tôi sử dụng lời nhắc sau để thực hiện vòng mô-đun hóa đầu tiên

CÂU HỎI:
{problem_statement}
TRẢ LỜI:
```python
{renamed_solution}
```
Tái cấu trúc chương trình trên. Tuân theo các hướng dẫn
*làm cho chương trình mô-đun hóa hơn với các hàm trợ giúp nhỏ hơn và có ý nghĩa
*tên mô tả tốt cho các hàm trợ giúp
*có một hàm nhập cảnh gọi là 'main()'
*'main()' được gọi bên trong 'if __name__ == '__main__''
Không thay đổi ngữ nghĩa gốc của chương trình đáng kể và không cần thực hiện tối ưu hóa. Đặt chương trình trong dấu backtick như được hiển thị ở trên

Tiếp theo, trong trường hợp chương trình mô-đun hóa chứa một hàm có số dòng lớn hơn 20, chúng tôi tiếp tục nhắc mô hình trong khi báo hiệu hàm nào cần phân tách thêm. Điều này xảy ra trong khoảng 20-40% các giải pháp mô-đun hóa và chúng tôi sử dụng lời nhắc sau.

CÂU HỎI:
{problem_statement}
TRẢ LỜI:
```python
{modularized_solution}
```
Tái cấu trúc chương trình trên bằng cách mô-đun hóa nó và chia nhỏ các hàm dài và phức tạp thành các hàm trợ giúp nhỏ hơn có ý nghĩa. Đặc biệt tái cấu trúc và phân tách (các) hàm sau đây thành các hàm trợ giúp nhỏ hơn − {function_names_string}
Chỉ trả về chương trình được tái cấu trúc được đặt trong dấu backtick như được hiển thị ở trên. """

B.3 LẬP KẾ HOẠCH
Chúng tôi sử dụng lời nhắc sau để tạo ra các kế hoạch ngôn ngữ tự nhiên

--- TRANG 16 ---
CÂU HỎI:
{problem_statement}
TRẢ LỜI:
```python
{modularized_solution}
```
Tạo ra một tóm tắt cho các hàm và lớp sau đây trong chương trình trong vòng bốn dòng mỗi cái. Các tóm tắt nên có tính mô tả và hữu ích để hiểu chương trình (tuy nhiên vẫn ngắn gọn trong bốn dòng).

Các hàm và lớp là −
{list_of_function_names}

Tuân theo định dạng được cung cấp cho các tóm tắt trong khi có tính thông tin và ngắn gọn. Đặt các chữ ký trong dấu backtick như được hiển thị ở trên.

--- TRANG 17 ---
C KẾT QUẢ BỔ SUNG
C.1 ĐÁNH GIÁ CỦA TRỌNG TÀI GPT-4 CHO CÁC PHÉP BIẾN ĐỔI
Chúng tôi ở đây trình bày một số bằng chứng định lượng về những cải thiện được thực hiện từ phương pháp biến đổi của chúng tôi. Tuy nhiên, vì các phép biến đổi là sinh mã tự do, chúng tôi dựa vào việc sử dụng GPT-4 làm trọng tài, một phương pháp đánh giá đang trở nên phổ biến để đánh giá các đầu ra ngôn ngữ tự do (Zheng et al., 2023; Zhuo, 2023). Cụ thể, chúng tôi yêu cầu mô hình ngôn ngữ trả lời liệu mã được tái cấu trúc mô-đun hóa có tên biến tốt hơn, phân tách hàm tốt hơn, và nhất quán với chương trình gốc hay không. Mô hình có thể cung cấp câu trả lời trên thang từ 1-3 từ các câu hỏi so sánh và 0-1 cho câu hỏi về tính nhất quán. Lời nhắc sau mô tả phương pháp của chúng tôi

LỜI NHẮC HỆ THỐNG:
Vui lòng đóng vai trò là một trọng tài công bằng và đánh giá việc tái cấu trúc mã dưới đây. Bạn cần đánh giá xem chương trình được tái cấu trúc có sử dụng tên biến tốt hơn và đúng, tái cấu trúc việc triển khai thành các hàm trợ giúp nhỏ hơn đúng và tính nhất quán với chương trình gốc hay không. Đánh giá của bạn nên dựa trên tính đúng đắn và hữu ích của việc tái cấu trúc trong việc hiểu rõ hơn về bài toán và cũng nếu nó vẫn nhất quán với chương trình gốc, tức là nó tuân theo logic chương trình và thuật toán tương tự.

*Để đánh giá tên biến và phân tách hàm, vui lòng cho điểm từ 1 đến 3 trong đó 1 có nghĩa là việc tái cấu trúc không hữu ích chút nào, 2 có nghĩa là việc tái cấu trúc hơi hữu ích và 3 có nghĩa là việc tái cấu trúc rất hữu ích. Định dạng ví dụ

Lý luận tên biến: [[lý luận cho điểm tên biến, thường đánh giá xem tên biến có mô tả và có ý nghĩa hơn và phản ánh đúng mục đích của biến hay không]]
Tên biến: [[1]] hoặc [[2]] hoặc [[3]]

Lý luận phân tách hàm: [[lý luận cho điểm phân tách, thường đánh giá xem hàm nào quá dài, khả năng thực hiện thêm các trừu tượng, lựa chọn trừu tượng, tên hàm trợ giúp]]
Phân tách hàm: [[1]] hoặc [[2]] hoặc [[3]]

*Để đánh giá tính nhất quán, vui lòng cho điểm 0 nếu chương trình được tái cấu trúc không nhất quán với chương trình gốc và 1 nếu nó nhất quán. Định dạng ví dụ

Lý luận tính nhất quán: [[lý luận cho điểm tính nhất quán, thường đánh giá xem chương trình được tái cấu trúc có tuân theo logic chương trình và thuật toán tương tự như chương trình gốc hay không]]
Tính nhất quán: [[0]] hoặc [[1]]

CÂU HỎI:
{problem_statement}
GIẢI PHÁP GỐC:
{solution}
GIẢI PHÁP ĐÃ TÁI CẤU TRÚC:
{solution}

Trong khi đánh giá này có thể mang một số thiên kiến tinh tế, chúng tôi tin rằng nó vẫn cung cấp cho chúng tôi một tín hiệu để đánh giá chất lượng của các phép biến đổi. Để giảm chi phí, chúng tôi đánh giá GPT-4 làm trọng tài cho 1000 bài toán trong tập dữ liệu APPS⁹. GPT-4 tuân theo định dạng được đề xuất cho 998 giải pháp và chúng tôi trình bày kết quả về chúng trong Bảng 6. Kết quả chứng minh rằng hầu hết các phép biến đổi được áp dụng đều có ý nghĩa trong khi vẫn nhất quán với các giải pháp chân lý cơ bản gốc.

Phân bố điểm Trung bình
Tên biến {3 : 967, 2 : 28, 1 : 3} 2.96
Phân tách hàm {3 : 938, 2 : 59, 1 : 1} 2.93
Tính nhất quán {1 : 994, 0 : 4} 0.994

Bảng 6: Đánh giá GPT-4 làm trọng tài cho chất lượng của 998 ví dụ được biến đổi. Chúng tôi so sánh giải pháp gốc và chưa được sửa đổi bằng GPT-4 về tên biến được sử dụng, phân tách hàm, và tính nhất quán của giải pháp được sửa đổi và gốc. Kết quả chứng minh rằng hầu hết các phép biến đổi đều thành công và có ý nghĩa trong khi nhất quán với chương trình gốc.

⁹Lời nhắc của chúng tôi trải dài khoảng 1.5-2k token bao gồm bài toán, chương trình gốc và được tái cấu trúc dẫn đến chi phí cao

--- TRANG 18 ---
Để có cái nhìn sâu sắc hơn về các đánh giá GPT-4, chúng tôi xem xét các ví dụ nhận điểm thấp hơn. Các điểm và lý luận liên quan có vẻ có ý nghĩa. Ví dụ, trong Hình 14, chương trình mô-đun hóa đã dễ đọc hơn đáng kể so với chương trình được đổi tên gốc. Tuy nhiên, GPT-4 xác định rằng các hàm trợ giúp calculate_permutation_even và calculate_permutation_even thực tế giống nhau và có thể được trừu tượng hóa thêm. Lưu ý rằng phép biến đổi này là một tạo phẩm của việc chương trình gốc bao gồm cùng logic chương trình được phân bố qua hai điều kiện if cách xa nhau. Tương tự, trong Hình 15, GPT-4 xác định một số tên biến chưa được sửa đổi như t trong khi thừa nhận các cải thiện khác như sky thành heights cho nó xếp hạng 2. Xếp hạng 1 được cung cấp khi phép biến đổi không sửa đổi bất kỳ tên biến nào hoặc không phân tách các hàm hiện có, như được chứng minh bởi phân bố điểm, một sự xuất hiện hiếm. Thực tế, thường các ví dụ được đánh dấu với xếp hạng 2 thực sự cải thiện mã gốc theo những cách không tầm thường.¹⁰

C.2 KẾT QUẢ APPS-COMPETITION
Chúng tôi trình bày các kết quả trên tập dữ liệu APPS-COMPETITION ở đây.

APPS-COMPETITION
PASS@1 PASS@10 PASS@100
Tinh chỉnh
CL-7B +D_original 0.2 1.7 3.1
CL-7B +D_modular 0.5 2.3 3.2
+0.3 +0.6 +0.1
CODE-DAVINCI-002 0.3 2.9 5.7

Bảng 7: Kết quả trên tập dữ liệu APPS-COMPETITION.

C.3 NGHIÊN CỨU LOẠI BỎ VỀ LỰA CHỌN MÔ HÌNH
Chúng tôi sử dụng GPT-3.5-TURBO làm mô hình mặc định để thực hiện các phép biến đổi trong các thí nghiệm chính vì nó cung cấp sự cân bằng tốt giữa độ chính xác và chi phí thực hiện các phép biến đổi. Ở đây, để chứng minh tính tổng quát của phương pháp chúng tôi thực hiện một nghiên cứu loại bỏ bằng cách thay thế mô hình biến đổi bằng GPT-4-TURBO. Vì mô hình này đắt hơn khoảng 8-10 lần so với GPT-3.5-TURBO, chúng tôi thực hiện nghiên cứu loại bỏ này trên một tập con 5k chương trình được lấy mẫu từ tập dữ liệu.

Thiết lập thực nghiệm. Chúng tôi lặp lại các bước đổi tên và mô-đun hóa được mô tả trong Mục 3.2 bằng cách sử dụng mô hình GPT-4-TURBO. Chúng tôi gọi tập dữ liệu được biến đổi kết quả là D⁴_modular. Tiếp theo, để so sánh công bằng tập dữ liệu kết quả với tập dữ liệu gốc và mô-đun hóa được tạo ra bằng GPT-3.5-TURBO, chúng tôi lấy mẫu các chương trình gốc và được biến đổi song song tương ứng và gọi chúng là các tập dữ liệu D_original và D³·⁵_modular.

APPS-INTRODUCTORY
PASS@1 PASS@10 PASS@100
CL-7B +D_original 16.3 31.6 37.6
CL-7B +D³·⁵_modular 18.8 33.0 38.2
CL-7B +D⁴_modular 19.4 34.3 40.0
+0.6 +1.3 +1.8

Bảng 8: Nghiên cứu loại bỏ về lựa chọn mô hình được sử dụng để thực hiện các phép biến đổi. D³·⁵_modular đại diện cho tập dữ liệu được tạo ra bằng GPT-3.5-TURBO và D⁴_modular đại diện cho tập dữ liệu được tạo ra bằng GPT-4-TURBO. Chúng tôi thấy rằng hiệu suất của mô hình được huấn luyện trên tập dữ liệu D⁴_modular tốt hơn mô hình được huấn luyện trên tập dữ liệu D³·⁵_modular.

¹⁰Tò mò là, GPT-4 đôi khi trả về xếp hạng 2.5 thay vì số nguyên 2 hoặc 3. Chúng tôi làm tròn nó thành 2, do đó làm cho đánh giá của chúng tôi khắt khe hơn!

--- TRANG 19 ---
D CÔNG TRÌNH LIÊN QUAN BỔ SUNG
LLM mã đã được sử dụng cho nhiều lĩnh vực trong các dòng phương pháp khác nhau. Ở đây, chúng tôi trình bày một số phương pháp chính và khuyến nghị người đọc tham khảo Hou et al. (2023) để có một khảo sát chi tiết. Chen et al. (2021) phát hành mô hình CODE-DAVINCI-002 và đánh giá nó cho sinh mã. Kể từ đó, LLM đã được sử dụng cho nhiều lĩnh vực như khoa học dữ liệu (Jain et al.; Lai et al., 2022; Yin et al., 2023), API (Zhiruo Wang & Neubig, 2022; Patil et al., 2023), và kho lưu trữ (Zhang et al., 2023a; Bairi et al., 2023; Shrivastava et al., 2023). (Le et al., 2022; Shojaee et al., 2023; Liu et al., 2023b) sử dụng học tăng cường với phản hồi biên dịch/thực thi để tinh chỉnh LLM mã cho nhiệm vụ sinh mã (thuật toán).

Các công trình khác đã tiếp cận sinh mã từ các mặt trận khác nhau, khám phá lập kế hoạch (Jiang et al., 2023), sửa chữa (Chen et al., 2023b; Shinn et al., 2023), tạo test (Key et al., 2022; Chen et al., 2022a), và tối ưu hóa lời nhắc (Liu et al., 2023a).

--- TRANG 20 ---
E VÍ DỤ VỀ CHƯƠNG TRÌNH ĐƯỢC BIẾN ĐỔI

def main():
import sys
input=sys.stdin.readline
n,k=map(int,input().split())
ab=[list(map(int,input().split())) for _ in [0]*n]
g=[[] for _ in [0] *10]
[g[b-1].append(a) for a,b in ab]
[g[c].sort(reverse=True) for c in range(10)]
for c in range(10):
g[c]=[0]+g[c]
for c in range(10):
for i in range(2,len(g[c])):
g[c][i]+=g[c][i-1]+2 *(i-1)
dp=[0] *(k+1)
for c in range(10):
dp2=[0] *(k+1)
for i in range(len(g[c])):
for j in range(k+1-i):
dp2[i+j]=max(dp2[i+j],dp[j]+g[c][i])
dp=dp2
print(max(dp))
if __name__=='__main__':
main()

(a) Chương trình gốc

def read_input():
... (BỊ CẮT NGẮN)
return num_books, num_sell, book_info
def group_books_by_genre(num_books, book_info):
genre_books = [[] for _ in range(10)]
for price, genre in book_info:
genre_books[genre-1].append(price)
return genre_books
def sort_books_by_price(genre_books):
for genre in genre_books:
genre.sort(reverse=True)
return genre_books
def calculate_purchase_prices(genre_books):
for genre in genre_books:
genre.insert(0, 0)
for genre in genre_books:
for i in range(2, len(genre)):
genre[i] += genre[i-1] + 2 *(i-1)
return genre_books
def calculate_max_purchase_price(num_sell, genre_books):
dp = [0] *(num_sell+1)
for genre in genre_books:
dp2 = [0] *(num_sell+1)
for i in range(len(genre)):
for j in range(num_sell+1-i):
dp2[i+j] = max(dp2[i+j], dp[j] + genre[i])
dp = dp2
return max(dp)
def main():
num_books, num_sell, book_info = read_input()
genre_books = group_books_by_genre(num_books, book_info)
genre_books = sort_books_by_price(genre_books)
genre_books = calculate_purchase_prices(genre_books)
max_purchase_price = calculate_max_purchase_price(num_sell, genre_books)
print(max_purchase_price)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 4: Các chương trình gốc và được biến đổi

--- TRANG 21 ---
for _ in range(int(input())):
p, q = map(int,input().split())
c = q
d = p
i = 1
factor = []
while i *i <= q:
if q % i == 0:
factor.append(i)
if q//i != i:
factor.append(q//i)
i += 1
factor.sort(reverse=True)
factor.pop()
m = 1
for i in factor:
d = p
while d % c == 0:
d //= i
m = max(m, d)
print(m)

(a) Chương trình gốc

def get_factors(q):
factors = []
i = 1
while i *i <= q:
if q % i == 0:
factors.append(i)
if q//i != i:
factors.append(q//i)
i += 1
factors.sort(reverse=True)
factors.pop()
return factors
def get_largest_divisor(p, q, factors):
largest_divisor = 1
for i in factors:
dividend_copy = p
while dividend_copy % q == 0:
dividend_copy //= i
largest_divisor = max(largest_divisor, dividend_copy)
return largest_divisor
def main():
t = int(input())
for _ in range(t):
p, q = map(int, input().split())
factors = get_factors(q)
largest_divisor = get_largest_divisor(p, q, factors)
print(largest_divisor)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 5: Các chương trình gốc và được biến đổi

--- TRANG 22 ---
def ncr(n, r, p):
# khởi tạo tử số
# và mẫu số
num = den = 1
for i in range(r):
num = (num *(n - i)) % p
den = (den *(i + 1)) % p
return (num *pow(den, p - 2, p)) % p
a,b,n=map(int,input().split())
ans=0
p=10 **9+7
num=1
den=1
for i in range(n+1):
s=a*i+b*(n-i)
if i!=0:
num=(num *(n-i+1))%p
den=(den *(i))%p
am=True
while s!=0:
if s%10!=a and s%10!=b:
am=False
break
s//=10
if am:
ans=(ans+(num *pow(den,p-2,p))%p)%p
print(ans%p)

(a) Chương trình gốc

def ncr(n, r, p):
# tính nCr modulo p
# khởi tạo tử số và mẫu số
numerator = denominator = 1
for i in range(r):
numerator = (numerator *(n - i)) % p
denominator = (denominator *(i + 1)) % p
return (numerator *pow(denominator, p - 2, p)) % p
def is_good_number(number, a, b):
# kiểm tra xem một số có phải là số tốt hay không
while number != 0:
if number % 10 != a and number % 10 != b:
return False
number //= 10
return True
def count_excellent_numbers(a, b, n):
ans = 0
p = 10 **9 + 7
numerator = 1
denominator = 1
for i in range(n + 1):
sum_of_digits = a *i + b *(n - i)
if i != 0:
numerator = (numerator *(n - i + 1)) % p
denominator = (denominator *i) % p
if is_good_number(sum_of_digits, a, b):
ans = (ans + (numerator *pow(denominator, p - 2, p)) % p) % p
return ans % p
def main():
a, b, n = map(int, input().split())
result = count_excellent_numbers(a, b, n)
print(result)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 6: Các chương trình gốc và được biến đổi

--- TRANG 23 ---
import bisect
rev=[]
for i in range(1,10002):
if str(i)==str(i)[::-1]:rev.append(i)
n=int(input())
ind= bisect.bisect_left(rev,n)
if abs(n-rev[ind-1])<=abs(n-rev[ind]):
print(rev[ind-1])
else:
print(rev[ind])

(a) Chương trình gốc

import bisect
def generate_palindromes():
palindromes = []
for num in range(1, 10002):
if str(num) == str(num)[::-1]:
palindromes.append(num)
return palindromes
def find_closest_palindrome(palindromes, n):
index = bisect.bisect_left(palindromes, n)
if abs(n - palindromes[index - 1]) <= abs(n - palindromes[index]):
return palindromes[index - 1]
else:
return palindromes[index]
def main():
palindromes = generate_palindromes()
n = int(input())
closest_palindrome = find_closest_palindrome(palindromes, n)
print(closest_palindrome)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 7: Các chương trình gốc và được biến đổi

--- TRANG 24 ---
N = int(input())
T = [int(input()) for i in range(N)]
T.sort()
if N == 1:
ans = 0
else:
y = T[-1]
R = []
for x in range(1, y+1):
if y % x == 0:
R.append(x)
cur = ans = 0
for t in T:
while R[cur] < t:
cur += 1
ans += R[cur] - t
print(ans)

(a) Chương trình gốc

def find_divisors(max_interval):
divisors = []
for divisor in range(1, max_interval+1):
if max_interval % divisor == 0:
divisors.append(divisor)
return divisors
def calculate_min_sum(intervals, divisors):
current_index = min_sum = 0
for interval in intervals:
while divisors[current_index] < interval:
current_index += 1
min_sum += divisors[current_index] - interval
return min_sum
def main():
num_metronomes = int(input())
intervals = [int(input()) for i in range(num_metronomes)]
intervals.sort()
if num_metronomes == 1:
min_sum = 0
else:
max_interval = intervals[-1]
divisors = find_divisors(max_interval)
min_sum = calculate_min_sum(intervals, divisors)
print(min_sum)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 8: Các chương trình gốc và được biến đổi

--- TRANG 25 ---
def main():
from collections import defaultdict
n, colors = int(input()), input()[::2]
dsu, edges, d = list(range(n)), [], defaultdict(list)
for _ in range(n - 1):
u, v = map(int, input().split())
u -= 1
v -= 1
if colors[u] == colors[v]:
a, b = dsu[u], dsu[v]
while a != dsu[a]:
a = dsu[a]
while b != dsu[b]:
b = dsu[b]
if a < b:
dsu[b] = dsu[v] = a
else:
dsu[a] = dsu[u] = b
else:
edges.append(u)
edges.append(v)
for u, v in enumerate(dsu):
dsu[u] = dsu[v]
while edges:
u, v = dsu[edges.pop()], dsu[edges.pop()]
d[u].append(v)
d[v].append(u)
def bfs(x):
nxt, avail, t = [x], [True] *n, 0
while nxt:
t += 1
cur, nxt = nxt, []
for y in cur:
avail[y] = False
for y in d[y]:
if avail[y]:
nxt.append(y)
return t if x else cur[0]
print(bfs(bfs(0)) // 2)
if __name__ == '__main__':
main()

(a) Chương trình gốc

from collections import defaultdict
def find_root(vertex, dsu):
while vertex != dsu[vertex]:
vertex = dsu[vertex]
return vertex
def merge_trees(u, v, dsu):
root_u = find_root(u, dsu)
root_v = find_root(v, dsu)
if root_u < root_v:
dsu[root_v] = dsu[v] = root_u
else:
dsu[root_u] = dsu[u] = root_v
def build_graph(num_vertices, colors, edges):
dsu = list(range(num_vertices))
graph = defaultdict(list)
for u, v in edges:
if colors[u] == colors[v]:
merge_trees(u, v, dsu)
else:
graph[dsu[u]].append(dsu[v])
graph[dsu[v]].append(dsu[u])
return dsu, graph
def bfs(x, num_vertices, graph):
next_vertices = [x]
available = [True] *num_vertices
t = 0
while next_vertices:
t += 1
current_vertices, next_vertices = next_vertices, []
for y in current_vertices:
available[y] = False
for neighbor in graph[y]:
if available[neighbor]:
next_vertices.append(neighbor)
return t if x else current_vertices[0]
def main():
num_vertices = int(input())
colors = input()[::2]
edges = []
for _ in range(num_vertices - 1):
u, v = map(int, input().split())
u -= 1
v -= 1
edges.append((u, v))
dsu, graph = build_graph(num_vertices, colors, edges)
print(bfs(bfs(0, num_vertices, graph), num_vertices, graph) // 2)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 9: Các chương trình gốc và được biến đổi

--- TRANG 26 ---
import heapq
def dfs(graph, start):
n = len(graph)
dist = [-0 for i in range(n + 1)]
visited = [False for i in range(n + 1)]
visited[start] = True
stack = []
dist[start] = 0
heapq.heappush(stack, start)
while stack:
u = heapq.heappop(stack)
for v in graph[u]:
if not visited[v]:
visited[v] = True
dist[v] = dist[u] + 1
heapq.heappush(stack, v)
return dist
def solution():
n, m, d = map(int, input().strip().split())
p = list(map(int, input().strip().split()))
graph = [[] for i in range(n + 1)]
for i in range(n - 1):
a, b = map(int, input().strip().split())
graph[a].append(b)
graph[b].append(a)
dist = dfs(graph, 1)
max_distance = -1
u = -1
v = -1
for i in p:
if dist[i] > max_distance:
max_distance = dist[i]
u = i
distu = dfs(graph, u)
max_distance = -1
for i in p:
if distu[i] > max_distance:
max_distance = distu[i]
v = i
distv = dfs(graph, v)
affected = 0
for i in range(1, n + 1):
if 0 <= distu[i] <= d and 0 <= distv[i] <= d:
affected += 1
print(affected)
solution()

(a) Chương trình gốc

import heapq
def calculate_distances(graph, start):
n = len(graph)
distances = [-0 for i in range(n + 1)]
visited = [False for i in range(n + 1)]
visited[start] = True
stack = []
distances[start] = 0
heapq.heappush(stack, start)
while stack:
current_node = heapq.heappop(stack)
for neighbor in graph[current_node]:
if not visited[neighbor]:
visited[neighbor] = True
distances[neighbor] = distances[current_node] + 1
heapq.heappush(stack, neighbor)
return distances
def find_possible_book_locations():
n, m, d = map(int, input().strip().split())
affected_settlements = list(map(int, input().strip().split()))
graph = [[] for i in range(n + 1)]
for i in range(n - 1):
a, b = map(int, input().strip().split())
graph[a].append(b)
graph[b].append(a)
return calculate_possible_locations(n, m, d, affected_settlements, graph)
def calculate_possible_locations(n, m, d, affected_settlements, graph):
distances = calculate_distances(graph, 1)
max_distance, u = find_max_distance(distances, affected_settlements)
distances_u = calculate_distances(graph, u)
max_distance, v = find_max_distance(distances_u, affected_settlements)
distances_v = calculate_distances(graph, v)
return count_possible_locations(n, d, distances_u, distances_v)
def find_max_distance(distances, affected_settlements):
max_distance = -1
u = -1
for settlement in affected_settlements:
if distances[settlement] > max_distance:
max_distance = distances[settlement]
u = settlement
return max_distance, u
def count_possible_locations(n, d, distances_u, distances_v):
possible_locations = 0
for i in range(1, n + 1):
if 0 <= distances_u[i] <= d and 0 <= distances_v[i] <= d:
possible_locations += 1
return possible_locations
def main():
possible_locations = find_possible_book_locations()
print(possible_locations)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 10: Các chương trình gốc và được biến đổi

--- TRANG 27 ---
n = int(input())
dividers = list()
i = 2
while i *i <= n:
if n % i == 0:
dividers.append(i)
while n % i == 0:
n /= i
i += 1
if n > 1:
dividers.append(n)
if len(dividers) == 1:
print(dividers[0])
else:
print(1)

(a) Chương trình gốc

def find_dividers(path_length):
dividers = []
divisor = 2
while divisor *divisor <= path_length:
if path_length % divisor == 0:
dividers.append(divisor)
while path_length % divisor == 0:
path_length //= divisor
divisor += 1
if path_length > 1:
dividers.append(path_length)
return dividers
def calculate_max_colors(path_length):
dividers = find_dividers(path_length)
if len(dividers) == 1:
return dividers[0]
else:
return 1
def main():
path_length = int(input())
max_colors = calculate_max_colors(path_length)
print(max_colors)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 11: Các chương trình gốc và được biến đổi

--- TRANG 28 ---
import sys
readline = sys.stdin.readline
N, M = map(int, readline().split())
mod = 10 **9+7
dpscc = [[0] *(N+1) for _ in range(N+1)]
dpus = [[0] *(N+1) for _ in range(N+1)]
dpscc[1][0] = 1
for m in range(M):
dpscc2 = [[0] *(N+1) for _ in range(N+1)]
dpus2 = [[0] *(N+1) for _ in range(N+1)]
for i in range(1, N+1):
for j in range(N+1-i):
kscc = dpscc[i][j]
kus = dpus[i][j]
dpscc2[i][j] = (dpscc2[i][j] + i *kscc) % mod
dpus2[i][j] = (dpus2[i][j] + j *(kus+kscc)) % mod
dpscc2[i+j][0] = (dpscc2[i+j][0] + i *kus) % mod
if N-i-j:
dpus2[i][j+1] = (dpus2[i][j+1] + (N-i-j) *(kus+kscc)) % mod
dpscc = [d[:] for d in dpscc2]
dpus = [d[:] for d in dpus2]
print(dpscc[N][0])

(a) Chương trình gốc

import sys
def count_sequences(num_towns, num_days):
mod = 10 **9+7
dp_same_city_count = [[0] *(num_towns+1) for _ in range(num_towns+1)]
dp_unique_city_count = [[0] *(num_towns+1) for _ in range(num_towns+1)]
dp_same_city_count[1][0] = 1
for day in range(num_days):
dp_same_city_count2 = [[0] *(num_towns+1) for _ in range(num_towns+1)]
dp_unique_city_count2 = [[0] *(num_towns+1) for _ in range(num_towns+1)]
for i in range(1, num_towns+1):
for j in range(num_towns+1-i):
same_city_count = dp_same_city_count[i][j]
unique_city_count = dp_unique_city_count[i][j]
dp_same_city_count2[i][j] = (dp_same_city_count2[i][j] + i *same_city_count) % mod
dp_unique_city_count2[i][j] = (dp_unique_city_count2[i][j] + j *(unique_city_count+same_city_count)) % mod
dp_same_city_count2[i+j][0] = (dp_same_city_count2[i+j][0] + i *unique_city_count) % mod
if num_towns-i-j:
dp_unique_city_count2[i][j+1] = (dp_unique_city_count2[i][j+1] + (num_towns-i-j) *(unique_city_count+same_city_count)) % mod
dp_same_city_count = [d[:] for d in dp_same_city_count2]
dp_unique_city_count = [d[:] for d in dp_unique_city_count2]
return dp_same_city_count[num_towns][0]
def main():
num_towns, num_days = map(int, input().split())
result = count_sequences(num_towns, num_days)
print(result)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 12: Các chương trình gốc và được biến đổi

--- TRANG 29 ---
n,m = [int(i) for i in input().split()]
seg = {i:[] for i in range(1,n+1)}
for j in range(m):
a,b = [int(i) for i in input().split()]
seg[a].append(b)
seg[b].append(a)
tail = [0] *(n+1)
tail[1] = 1
for i in range(2,n+1):
temp = [tail[j] for j in seg[i]]+[0]
tail[i] = max(temp)+1
temp = [len(seg[i]) *tail[i] for i in range(1,n+1)]
print(max(temp))

(a) Chương trình gốc

def calculate_beauty(num_points, num_segments, segments):
tail_length = [0] *(num_points+1)
tail_length[1] = 1
for i in range(2, num_points+1):
temp = [tail_length[j] for j in segments[i]]+[0]
tail_length[i] = max(temp)+1
spine_length = [len(segments[i]) *tail_length[i] for i in range(1, num_points+1)]
return max(spine_length)
def main():
num_points, num_segments = [int(i) for i in input().split()]
segments = {i:[] for i in range(1, num_points+1)}
for j in range(num_segments):
point1, point2 = [int(i) for i in input().split()]
segments[point1].append(point2)
segments[point2].append(point1)
result = calculate_beauty(num_points, num_segments, segments)
print(result)
if __name__ == '__main__':
main()

(b) Chương trình được biến đổi

Hình 13: Các chương trình gốc và được biến đổi

--- TRANG 30 ---
for i in range(int(input())):
n,k=[int(i) for i in input().split()]
if(n%2==0):
if(k<(n *(n+1))//2 - 1 or k>3 *((n//2) **2) - 1):
print(-1)
elif(k==(n *(n+1))//2 - 1):
for i in range(1,n+1):
print(i,'',end='')
print()
else:
k,count,p,l,x = k-(n *(n+1))//2 + 1,0,0,[0 for i in range(n)],1
while(k>0):
p+=2 ;k, count = k-n+p ,count+1
for i in range(n,n-count+1,-1):
l[x]=i ;x+=2
k=-k ;
l[2*count - 1 +k],p = n-count+1 ,1
for i in range(n):
if(l[i]==0):
l[i]=p
p+=1
for i in l:
print(i,'',end='')
print()
else:
if(n==1):
print(1) if(k==0) else print(-1)
elif(k<(n *(n+1))//2 - 1 or k>3 *(n//2) *(n//2 + 1)):
print(-1)
elif(k==(n *(n+1))//2 - 1):
for i in range(1,n+1):
print(i,'',end='')
print()
else:
k,count,p,l,x = k-(n *(n+1))//2 + 1,0,0,[0 for i in range(n)],1
while(k>0):
p+=2 ; k,count = k-n+p ,count+1

(a) Chương trình gốc

def find_permutation(n, k):
if n % 2 == 0:
if k < n *(n + 1) // 2 - 1 or k > 3 *(n // 2) **2 - 1:
return None
elif k == n *(n + 1) // 2 - 1:
return list(range(1, n + 1))
else:
permutation = calculate_permutation_even(n, k)
return permutation
elif n == 1:
return [1] if k == 0 else None
elif k < n *(n + 1) // 2 - 1 or k > 3 *(n // 2)*(n // 2 + 1):
return None
elif k == n *(n + 1) // 2 - 1:
return list(range(1, n + 1))
else:
permutation = calculate_permutation_odd(n, k)
return permutation
def calculate_permutation_even(n, k):
k, count, p, l, x = k - n *(n + 1) // 2 + 1, 0, 0, [0 for i in range(n)], 1
while k > 0:
p += 2
k, count = k - n + p, count + 1
for i in range(n, n - count, -1):
l[x] = i
x += 2
k = -k
l[2 *count - 1 + k], p = n - count + 1, 1
for i in range(n):
if l[i] == 0:
l[i] = p
p += 1
return l
def calculate_permutation_odd(n, k):
k, count, p, l, x = k - n *(n + 1) // 2 + 1, 0, 0, [0 for i in range(n)], 1
while k > 0:
p += 2
k, count = k - n + p, count + 1
for i in range(n, n - count, -1):
l[x] = i
x += 2
k = -k
l[2 *count - 1 + k], p = n - count + 1, 1
for i in range(n):
if l[i] == 0:
l[i] = p
p += 1
return l
def main():
t = int(input())
for _ in range(t):
n, k = map(int, input().split())
permutation = find_permutation(n, k)
if permutation is not None:
print( *permutation)
else:
print(-1)
main()

(b) Chương trình được biến đổi

Hình 14: Các chương trình gốc và được biến đổi

--- TRANG 31 ---
import sys
num=int(sys.stdin.readline())
s=sys.stdin.readline().split()
sky=list(map(int,s))
sky.reverse()
cuts=0
change=0
t=False
i=1
while i<len(sky):
if sky[i]<=sky[i-1]:
for j in range(i-1,-1,-1):
if sky[j]<=sky[i]-(i-j):
break
else:
change+=sky[j]-(sky[i]-(i-j))
if change>=sky[i]:
change=sky[i]
t=True
break
cuts+=change
if t:
del sky[i]
t=False
i-=1
else:
for j in range(i-1,-1,-1):
if sky[j]<sky[i]-(i-j):
break
else:
sky[j]=sky[i]-(i-j)
i+=1
change=0
print(cuts)

(a) Chương trình gốc

import sys
def count_operations(heights):
cuts = 0
change = 0
t = False
i = 1
while i < len(heights):
if heights[i] <= heights[i - 1]:
change = calculate_change(heights, i)
cuts += change
if t:
del heights[i]
t = False
i -= 1
else:
update_heights(heights, i)
i += 1
change = 0
return cuts
def calculate_change(heights, i):
change = 0
t = False
for j in range(i - 1, -1, -1):
if heights[j] <= heights[i] - (i - j):
break
else:
change += heights[j] - (heights[i] - (i - j))
if change >= heights[i]:
change = heights[i]
t = True
break
return change
def update_heights(heights, i):
for j in range(i - 1, -1, -1):
if heights[j] < heights[i] - (i - j):
break
else:
heights[j] = heights[i] - (i - j)
def main():
num_sky_scrappers = int(sys.stdin.readline())
heights = list(map(int, sys.stdin.readline().split()))
heights.reverse()
cuts = count_operations(heights)
print(cuts)
main()

(b) Chương trình được biến đổi

Hình 15: Các chương trình gốc và được biến đổi
