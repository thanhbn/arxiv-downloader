# 2310.08588.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2310.08588.pdf
# File size: 7943894 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Octopus: Embodied Vision-Language
Programmer from Environmental Feedback
Jingkang Yang∗,1, Yuhao Dong∗,2,3, Shuai Liu∗,2,4, Bo Li∗,1,
Ziyue Wang†,1, Haoran Tan†,4, Chencheng Jiang†,5, Jiamu Kang†,3,
Yuanhan Zhang1, Kaiyang Zhou6, and and Ziwei Liu1,B
1S-Lab, Nanyang Technological University2Shanghai AI Laboratory
3Tsinghua University4BUPT5XJTU6Hong Kong Baptist University
{jingkang001, ziwei.liu}@ntu.edu.sg
Fig.1: Illustration of our vision-language programmer, Octopus, complete
a task in GTA environment. Given a task in the form of natural language, Octopus
relies on its egocentric vision to generate plans and the corresponding executable code.
Abstract. Large vision-language models (VLMs) have achieved sub-
stantial progress in multimodal perception and reasoning. When inte-
grated into an embodied agent, existing embodied VLM works either
output detailed action sequences at the manipulation level or only pro-
vide plans at an abstract level, leaving a gap between high-level planning
and real-world manipulation. To bridge this gap, we introduce Octopus ,
an embodied vision-language programmer that uses executable code gen-
eration as a medium to connect planning and manipulation. Octopus
is designed to 1)proficiently comprehend an agent’s visual and textual
task objectives, 2)formulate intricate action sequences, and 3)gener-
ate executable code. To facilitate Octopus model development, we in-
troduceOctoVerse : a suite of environments tailored for benchmarking
⋆Equal contribution,†Equal engineering contribution,BCorresponding author.arXiv:2310.08588v2  [cs.CV]  20 Oct 2024

--- PAGE 2 ---
2 J. Yang et al.
vision-based code generators on a wide spectrum of tasks, ranging from
mundane daily chores in simulators to sophisticated interactions in com-
plex video games such as Grand Theft Auto (GTA) and Minecraft. To
train Octopus, we leverage GPT-4 to control an explorative agent that
generates training data, i.e., action blueprints and corresponding exe-
cutable code. We also collect feedback that enables an enhanced train-
ing scheme called Reinforcement Learning with Environmental
Feedback (RLEF) . Through a series of experiments, we demonstrate
Octopus’s functionality and present compelling results, showing that the
proposed RLEF refines the agent’s decision-making. By open-sourcing
our simulation environments, dataset, and model architecture, we aspire
to ignite further innovation and foster collaborative applications within
the broader embodied AI community. The project page is available at
https://choiszt.github.io/Octopus/ .
1 Introduction
Theriseoflargelanguagemodels(LLMs)[11,16,45,49,57]ledtoasurgeinvision-
language models (VLMs) [4,5,33,35,38–40], enabling tasks such as image/video-
based descriptions [35], reasoning [15,41,64], and conversations [17,33]. In the
realm of embodied AI, notable efforts [10,20,35] have trained agents to process
visual input and relay motor control commands.
Another approach to interacting with the environment focuses on task execu-
tion through code invocations, mirroring the human System-I stimulation [21,31]
(automatic, intuitive actions) with predefined code, and leaving the System-II
processes [21,31] (planning and reasoning) for large models. For example, refer-
ring to Fig. 1, planning a car ride with a pet might entail a subconscious check-
list (e.g., getOutOf() the house, open()the car door), each action could be
implemented using specific techniques [8,26] such as imitation learning [24,30].
This programmatic paradigm has been, although not in vision, leveraged by
works [27,53,55,56] using LLMs to craft programs and trigger APIs. Game-
centric models like Voyager [58] have similarly employed GPT for function calls
within game engines, though they often parse data directly from their environ-
ments.
However, when incorporating visual perception, the programming paradigms
are largely unexplored. Primary initiatives [51,62] can only output plans, which
anchor their strategies only in initial environmental states or employ dynamic
scene graphs for LLM inputs, respectively. Despite their innovations, the over-
reliance on pre-trained vision models to convert vision content into language can
occasionally hinder the LLM’s planning performance. The conversion from plans
into real-world actions is still missing. While EmbodiedGPT [44] addresses the
problem by integrating vision-language modeling for planning and then transi-
tioning to manipulation using policy mapping, the capability of embodied vision-
language models to generate executable programs remains largely uncharted.
Our exploration aims to bridge this gap. An embodied vision-language pro-
grammer should integrate visual perspective with textual objectives to devise

--- PAGE 3 ---
Octopus: Embodied VLM from Environmental Feedback 3
action plans and executable code (Fig. 1). However, existing simulator envi-
ronments often lack the carefully designed functions necessary to support such
models effectively. These functions should balance usefulness and complexity to
avoid hindering the development of genuine embodied vision-language program-
mers. For example, the explore_until() function in Minecraft, which can lead
the player directly to specific blocks without relying on vision information, may
not be suitable for training these models.
To meet the requirement, we carefully design and develop OctoVerse, a suite
of environments consisting of diverse simulators, including (i) OctoGibson, built
upon the photorealistic OmniGibson [34], (ii) OctoMC, developed on the in-
finitely creative, pixel-style Minecraft platform [2], and (iii) OctoGTA, adapted
from the highly interactive and immersive Grand Theft Auto V (GTA-V) [1].
These environments enable the training and benchmarking of our embodied
vision-language programming model in a wide range of scenarios, from daily
household tasks to complex urban navigation and open-world exploration, while
the function calls are tailored to be vision-dependent.
Using the OctoVerse environment, we train Octopus by leveraging GPT-4
to collect data. We provide GPT-4 with system messages, environmental cues,
and objectives, enabling it to formulate action strategies and code. Simultane-
ously, the agent captures visual perspectives, forming the image-code pair for
Octopus training. During data collection, the agent receives simulator feedback,
distinguishingsuccessfulmovesfromunsuccessfulones.Weincorporatethisfeed-
back using Reinforcement Learning with Environmental Feedback (RLEF) and
fine-tune Octopus using Proximal Policy Optimization (PPO) [54]. Empirically,
Octopus demonstrates strong adaptability in various scenarios, outperforming
existingmodelsintaskplanning,codegeneration,andexecution.Theintegration
of RLEF further enhances Octopus’s performance, showcasing the effectiveness
of this training approach. In sum, our key contributions include:
– A Novel Vision-Language Programming Benchmark: Three diverse
embodied environments with designed tasks: (i) OctoGibson, which is devel-
oped upon OmniGibson [34], (ii) OctoMC that developed on Minecraft [2],
and (iii) OctoGTA, which is adapted from GTA-V [1].
– A New Vision-Language Programming Model: An embodied vision-
languageplannerandprogrammertrainedwithReinforcementLearningwith
Environmental Feedback (RLEF), demonstrates compelling results.
– Insights on Vision-Language Programming: We extensively explore
Octopus and share useful insights facilitating future research on visual plan-
ning and programming.
2 Related Work
2.1 Embodied AI Simulators
Embodied AI has advanced significantly with the development of diverse sim-
ulation environments, enabling research tasks such as visual exploration [50],

--- PAGE 4 ---
4 J. Yang et al.
Table 1: Related Work for OctoVerse - Overview of Embodied AI Environ-
ments.We select three environments into OctoVerse and carefully design executable
tasks and vision-dependent function calls (VC), in comparison to undesigned standard
function calls (C).
Simulator KinematicsContinuous
Extended
StatesFlexible
MaterialsDeformable
BodiesRealistic
Action
ExecutionGame- or
World-BasedFormulated
TasksFunction
Call
Type
OpenAIGym [9] ✓ × × × ✓ G × C
Matterport3D [13] × × × × × W × ×
AI2THOR [32] ✓ × × × × G × C
VirtualHome [47] × × × × × G × ×
House3D [61] × × × × × W × ×
Habitat 1.0 [52] ✓ × × × ✓ W × C
Robosuite [71] ✓ × × × ✓ W × C
RFUniverse [23] ✓ × ✓ ✓ ✓ W × C
Minecraft [2] ✓ × ✓ × ✓ G × C
OctoMC ✓ × ✓ × ✓ G ✓ VC
GTA [1] ✓ ✓ ✓ ✓ ✓ G × C
OctoGTA ✓ ✓ ✓ ✓ ✓ G ✓ VC
OmniGibson [34] ✓ ✓ ✓ ✓ ✓ W × C
OctoGibson ✓ ✓ ✓ ✓ ✓ W ✓ VC
navigation [59], and question-answering [18]. Several simulators, including AI2-
THOR [32], VirtualHome [47], Habitat-Sim [52], SAPIEN [63], and Omnigib-
son [34], provide realistic representations of the world for investigating embod-
ied AI challenges. OmniGibson [34] stands out for its high-fidelity simulation of
diverse indoor and outdoor environments. OctoGibson environment further en-
hances OmniGibson with carefully designed function calls and formulated tasks,
making it well-suited for vision-language programming.
Game-related simulators like Arade [7], CHALET [65], and VRKitchen [25]
also contribute significantly to embodied AI. Minecraft [2] has gained atten-
tion in reinforcement learning and game agents [6,22,42,58,67,68] but lacks the
necessary structure for vision-language programming. OctoMC addresses this by
providingdesignedfunctioncallsandformulatedtasks.IncontrasttoMinecraft’s
voxel-basedrepresentationsthatlimittransferabilitytoreal-worldenvironments,
GTA-V [1] offers a highly realistic environment. In this work, we introduce Oc-
toGTA as a new setting, leveraging GTA-V’s rich, open-world environment with
incorporated tasks and function calls, extending this platform for embodied AI
study.
2.2 Embodied AI with Large Models
TherecentwaveofresearchfocusesonmergingLLMswithembodiedAItasks[11,
45,49,57].Forinstance,VoxPoseraddressesroboticmanipulationproblemsthrough
unsupervised methods [29]. A group of projects, namely SayCan [3], Palm-E [20],
RT-2 [10], and EmbodiedGPT [44], effectively integrate visual or linguistic cues
with robot manipulation data. Outside the domain of robotic manipulation, ini-
tiatives like Voyager [58] and Smallville [46] harness the capabilities of GPT
to interface with game functions, relying on preset functions to manage intri-
cate manipulations. In a parallel vein, VisProg [27] leverages GPT-3 language
prompts to craft Python programs, opening the door to a multitude of fasci-
nating applications. While the proposed Octopus model also formulates plans

--- PAGE 5 ---
Octopus: Embodied VLM from Environmental Feedback 5
Table2:RelatedWorkforOctopus-OverviewingEmbodiedAIModels. The
proposed Octopus distinguishes itself from other models as a unified vision-language
model for both plan and code generation.
ModelsRelease
DateSupported
EnvironmentVision
ModelCode
GeneratorAction
w/ FeedbackLLM Training
Enabled
Text2Motion [37] Mar. 2023 Sim × ✓ ✓ ×
Instruct2Act [28] May 2023 Sim × ✓ × ×
Lang2Rewards [66] Jun. 2023 Sim × ✓ ✓ ×
VoxPoser [29] Jul. 2023 Sim ✓ × × ×
SayCan [3] Apr. 2022 Real ✓ × ✓ ×
PALM-E [20] Mar. 2023 Sim, Real ✓ × ✓ ✓
RT-2 [10] Jul. 2023 Real ✓ × ✓ ✓
SayPlan [51] Jun. 2023 Real × × ✓ ×
EmbodiedGPT [44] May 2023 Sim ✓ × ✓ ✓
TaPA [62] Jul. 2023 Sim × × × ✓
Voyager [58] May 2023 Game × ✓ ✓ ×
Steve-Eye [68] Dec 2023 Game ✓ × ✓ ✓
RoboScript [14] Feb 2024 Sim, Real × ✓ ✓ ×
Octopus - Sim, Game ✓ ✓ ✓ ✓
and code, its distinguishing feature is the seamless integration of visual input in
program and code generation. This also stands in contrast to other embodied
planners like TAPA [62] and SayPlan [51], which deploy separate vision mod-
ules to translate visual data into linguistic inputs for LLMs. Octopus excels as
a cohesive vision-language model, delivering not just plans but also code.
More discussion on additional related works (Vision Language Model, and
FeedbackinLargeLanguageModels)isincludedinthesupplementarymaterials.
3 The OctoVerse Environment
In this section, we introduce three simulator environments designed to train
and evaluate the Octopus model. For each environment, we will describe their
overall information, the special design considerations that ensure the tasks are
well-formulated and the callable functions are vision-dependent.
OctoGibson We built the environment on the foundation of OmniGibson [34],
an existing simulation framework that supports 1,000 daily activities across 50
scenes, featuring over 5,000 meticulously annotated objects. We incorporated
16 functions that the robot can execute, such as moveBot() andeasyGrasp() .
Within this environment, we meticulously crafted 476 tasks1, each with a well-
defined initial state and a definitive termination state, allowing for a straight-
forward assessment of task completion. Among these tasks, 367 are routine
tasks—simple and direct actions like “place a glass in a trash can,” marked
asFollow. Conversely, the remaining 109 are reasoning tasks that necessitate
deeper comprehension. An example is “buy a chocolate,” where the agent needs
1The full list of tasks and their categories are listed in the supplementary material.

--- PAGE 6 ---
6 J. Yang et al.
to know to pick a chocolate bar from the shelf and then place it, along with
money, on the checkout counter, denoted as Reasontasks.
To ensure our tasks are vision-aware, we deliberately constrain the usage of
certain functions, such as moveBot(object) , which moves the agent in front of
the given object. To avoid making the task too easy and vision-agnostic, we
limit the given parameter to a predefined set of large objects, such as tables
and cupboards, rather than small items like cups and glasses. In this case, if the
robot wants to pick up a cup, it needs to recognize whether the cup is on the
table or in the cupboard. A simple moveBot(object) call with an inappropriate
parameter would cause a runtime error. The full list of the functions is in the
Appendix.
OctoMC The OctoMC environment is built on Minecraft [2], a popular plat-
form for reinforcement learning and game agents. We integrated 6 functional
actions and crafted 40 tasks2, each designed to facilitate comprehensive observa-
tions and executions by the agent. These tasks are distributed across 10 differ-
ent biomes, including indoor, outdoor, and underground settings, under varying
weather conditions.
However, existing Minecraft environments often lack vision-formulated tasks
and the necessary structure for vision-language programming. For example, in
Voyager [58], the exploreUntil() function allows the player to navigate directly
to specific blocks without relying on vision information. This function works by
randomly exploring the environment within a certain range until the desired
object is found, at which point it returns a true value, enabling the agent to
interact with the located object, such as gold blocks or trees. While effective,
this approach is entirely automated and does not utilize visual information,
making it unsuitable for our vision-based objectives.
To address this limitation, we crafted a vision-dependent exploration func-
tion, teleport(yaw, distance) , which operates within the robot’s perceptual
range. This function ensures that the agent’s operations are vision-dependent
and require active perception and navigation within the environment. For in-
stance, to locate a tree block, the agent must actively navigate towards the
direction of the forest, relying on visual cues to guide its exploration.
OctoGTA The OctoGTA environment is built on GTA-V [1] with the help of
the active GTA modding community. We integrated 19 functions and methodi-
cally crafted 25 tasks3, such as “help NPC drive their boat back to shore” and
“mediate a fight between two NPCs,” spanning across 5 groups (shown in Fig. 2
(b)). Each task is assigned to 5 different locations within the game world.
Wehaveimplementedasetoffunctionsthatenabletheagenttointeractwith
the game world in a visually-aware manner. Similar to the design in Minecraft,
we get rid of functions like walkTo(location) that might trivialize the task of
reaching a particular building or landmark. Instead, we provide functions such
2Detailed tasks are listed in the supplementary material.
3We meticulously designed tasks to be friendly, ensuring they exclude any inappro-
priate or violent behaviors.

--- PAGE 7 ---
Octopus: Embodied VLM from Environmental Feedback 7
(a) OctoGibsonTasksCleaningCookingRecyclingAssemblingStoringRecyclingShoppingPlantingPackingSetupCleaningOutdoorTasksIndoorTasks80.7%19.3%11333395118107141149117
(b) OctoGTATasks(c) OctoMCTasks
explore_until(brick)walkTo(30m, 25degree)walkTo(car_id)turnPlayer(25degree)moveBot(cup_id)moveBot(table_id)goForward(30m)
✅
❌
✅
❌
✅
❌
minecraftkillplacesmeltNon-InteractiveInteractiveboat
mediatorpetcar5555540%60%normalhuman73234332212111311forest
forestplainplainbeach
junglesavannaforesttaiga
cavebeachplaintaigacavebeachforestplain
Fig.2: The Statistics of the OctoVerse Environment with Function Designs.
asgoForward(distance) andturnPlayer(degree) (Fig. 2 (b)). For tasks like
mediating a fight between two NPCs, the essential function stopFight() only
works when the player is within 5 meters of the fighting NPCs. These design
choices ensure that the agent’s operations are vision-dependent and require ac-
tive perception and navigation within the environment.
4 Octopus: The Embodied Vision-Language Programmer
In this section, we present the procedure for training Octopus. Starting from
collecting training data within the OctoVerse environment, Octopus builds upon
a VLM architecture of Otter [33] and includes specialized RLEF modules to
handle vision-language programming tasks. Fig. 4 illustrates the entire Octopus
training pipeline.
4.1 Training Data Collection
We use the automatic training data collection pipeline described here for Oc-
toGibson and OctoMC, with the latter using customized prompts inspired by
Voyager [58]. For OctoGTA, we rely on human labor to hand-craft the train-
ing dataset due to the difficulty of obtaining textual environment messages in
the GTA environment. In the following parts, we use OctoGibson as the pri-
mary example to illustrate the data collection pipeline. Note that the primary
task in organizing training data is to form a succinct pairing: “vision input +
current/historical states →next step plan + executable code”.
Environment Info Collection As shown in Fig. 4 (a) and Fig. 3, we for-
mat anenvironment message for each state, encompassing attributes like
Observed Objects ,Observed Relations ,Inventory , and more. Specifically,
the simulator can provide us with an exact scene graph at each state, shaping
the content for the first two parts. The inventory info is also accessible. The task,
e.g., “cooking bacon” in Fig. 3, is represented by the Task Goal .
Automation with GPT-4 After preparing the environment message, we
crafted a structured system message to ensure that the robot not only un-
derstands its input but also maintains a consistent output format. A detailed

--- PAGE 8 ---
8 J. Yang et al.
Utility Functions4ZTUFN.FTTBHFAction FunctionsRole ExplanationResponse Requirement&OWJSPONFOU.FTTBHFObserved ObjectsObserved RelationsInventoryTask GoalOriginal PlanningPrevious ActionExecution ErrorExplanationCurrent PlanningAction CodeTarget States
GPT-4
Init StateFridge OpenBacon OutFridge CloseAction FailsNear StoveBacon on PanCooked
'SPOU7JFX-FGU7JFX3JHIU7JFX#BDL7JFXEnvironment Message Explanation
open(fridge)close(fridge)
Near Fridge
toggleOn(stove)moveBot(fridge)easyGrasp(bacon)putOnTop(bacon,pan)moveBot(stove)putOnTop(bacon,pan)
Since our task goal is to cook bacon and the bacon is inside the fridge on a tray, the ﬁrst step should be to approach the fridge, open it, and take out the bacon. Once the bacon is retrieved, we can proceed to cook it on suitable appliances.1.Approachfridge.2.Openfridge.3.Retrievethetraycontainingthebaconfromthefridge.4.Placethebacononthestove.5.Toggleonthestovetocookthebacon.
(robot,nextto,fridge_xyejdx_0,1)def act(robot, env, camera):# Subtask 1: Approach the fridgefridge=registry(env, "fridge_xyejdx_0") MoveBot(env, robot, fridge, camera)
…
Fig.3: Data Collection Example for “Cook a Bacon” Task. GPT-4 perceives
the environment through the environmental message and produces anticipated plans
and code following the detailed system message . This code is subsequently executed
in the simulator, directing the agent to the subsequent state. For each state, we gather
the environmental message, wherein observed objects andrelations are substituted
by egocentric images to serve as the training input. The response from GPT-4 acts as
the training output. Environmental feedback, specifically the determination of whether
each target state is met, is documented for RLEF training.
examination of this prompt can be found in the appendix. Experiments have
shown that a well-articulated prompt enables GPT-4 to effectively generate ex-
ecutable code. It is important to note that the combined length of the system
and environment messages can be extremely long, which may cause standard
GPT-4 8K models to struggle with producing meaningful outputs. To address
this issue, we employ the more robust GPT-4 32K model. As illustrated in Fig. 3,
when GPT-4 receives a consistent system and environment message, it gener-
ates comprehensive outputs that include current scenario analysis, planning, and
actionable code, supporting the training process in Section 4.3.
Error Management Notably, GPT-4 collects training data under the main
task of guiding the agent to complete tasks. However, GPT-4 is not infallible.
Errors can manifest in multiple ways, ranging from syntax errors to physical
challenges in the simulator. For instance, in Fig. 3, between states #5 and #6,
the action failed due to the long distance between the agent (holding bacon) and
the pan. Such setbacks reset the task to its previous state. If a main task remains
incomplete after 10 steps, it is deemed unsuccessful, and we terminate this task
for budget concerns. However, all data pairs without syntax errors, regardless of
the task’s completion status, are valuable for refining instructions and improving
the model’s performance.
Environmental Feedback GPT-4’s continual trial-and-error approach while
guidingtheagenttowardtaskcompletionservesadualpurpose:collectingvision-

--- PAGE 9 ---
Octopus: Embodied VLM from Environmental Feedback 9
(a) Data Collection Pipeline(b) Octopus Training Pipeline
AgentVisionCode
RLEFSFT
Octopus(SFT + RLEF)Octopus (SFT only)ParseCapture
Scene GraphComposeEnvironment Message
CodeControlChange
Environment
Agent Vision
FeedbackSystem Message
Step 1: Collect Simulator Feedback
(c) Octopus (Otter model)(d) RLEF Training Environmental MessageVision EncoderPerceiverGated XATTNLM BlockGated XATTNLM Block…Step 2: Training Reward ModelStep 3: Training Octopus with Rewardstate #0state #1state #3state #2state #4(subtask or task fails)(subtask done)(task done)(subtask done)def app(bot):bacon=registry(env, bacon)easyGrasp(bot, bacon)donothing(env)
Reward: 1Reward: 0Reward: 11
Bcodecodecode…01…def app(bot):pan=registry(env, pan_2)putOnTop(bot, pan)donothing(env)def app(bot):table=registry(env, table)moveBot(bot, table)donothing(env)Plan & code
SASASBS-…IMGIMGIMG#1#2#N
Fig.4: How to train Octopus: data collection and training pipeline.
output pairs and generating a rich set of feedback data. The automatic annota-
tion of this feedback focuses on two levels: step-level and task-level judgments.
Step-level judgment assesses the alignment of post-execution states with their
target states. For instance, in Fig. 3, steps color-coded in green lead to positive
feedback. One can visualize the action sequence for task completion as a tree,
where each node indicates a step (subtask), encapsulating an action code. Ac-
companying each step is a binary value that denotes success or failure, giving
preference to the successful branch over its counterpart. Task-level judgment ,
on the other hand, gauges the successful execution of the overall task. If the task
is not completed as intended, every state within that task is labeled as nega-
tive, regardless of the status of the subtasks. This collated feedback data serves
as a foundation for our Reinforcement Learning with Environmental Feedback
(RLEF) methodology, which we discuss in greater detail in Section 4.4.
4.2 Model Architecture
The Octopus architecture (shown in Fig. 4 (c)) is heavily inspired by the Ot-
ter model [33], integrates the MPT-7B Language Decoder [43] andCLIP
VIT-L/14VisionEncoder [48].AdoptingdesignprinciplesfromFlamingo[4],
Octopus employs the Perceiver Resampler andCross-Gated Attention
modules to enhance vision-language synergy. This architecture enables Octo-
pus to excel in tasks requiring understanding of both visual and textual data.
The Octopus is also compatible with other VLMs such as LLaVA [40].
4.3 SFT: Supervised Finetuning with Instructions
We train the Octopus model on our collected dataset from OctoVerse DE=
{(Xv,Ti,Tr)}using token-level supervised fine-tuning (SFT) [45,57]. The Per-
ceiver Resampler transforms images Xvinto visual tokens that condition sub-
sequent layers via Cross-Gated Attention modules. The training objective is
next-token prediction, modeling the likelihood of a targeted response Tras:
p(Tr|Ti,Xv) =LY
l=1p(tl|Xv,Ti,Tr,<l). (1)

--- PAGE 10 ---
10 J. Yang et al.
Note that Tidenotes the instruction tokens and Tr,<ldenotes the response to-
kensbeforethecurrentpredictedtoken tl.Duringinference,tokensareconverted
into natural language via the language decoder’s text tokenizer.
Visual observations Xv={x0
F, . . . , x7
F, x0
B, x1
B}consist of 8 first-person view
(FPV) images and two bird’s-eye view (BEV) images for OctoGibson and Oc-
toGTA. OctoMC only takes 4 FPV images. The FPV captures the agent’s direct
observations, while the BEV provides a holistic understanding of the environ-
ment. The eight FPV images are captured every 45 degrees, ensuring a complete
360-degree perspective.
4.4 RLEF: Reinforcement Learning with Environmental Feedback
In OctoVerse, task progression can be visualized as a tree (Fig. 4 (d)), where
each node represents a sub-task with a binary value indicating success (1) or
failure (0). If a node (or sub-task) has a value of 1, it is a step in the correct
direction toward our end goal.
Tree-basedTaskRepresentation Accordingtotheenvironmentalfeedback
part in Sec. 4.1, environmental reward datasets DR= (X∗
v,T∗
i,Ti
r,Tj
r, c)are
organized, where Ti
randTj
rare responses sharing the same parent task T∗
i,
andcindicates the preferred response leading to task completion. This ensures
the reward mechanism favors the successfully executed branch. Note that even
if a parental node does not have multiple responses, we can still assign feedback
according to the rule in Sec. 4.1.
RewardModelConfiguration Asingle-modalCodeLLaMA-7Bmodelwith
an additional value head is fine-tuned on DRas the reward model rϕ. This text-
based model assesses state transitions ( T∗
i→Ti,j
r) to determine high-reward
transitions, assisting the agent in task execution and completion. The rationale
for using CodeLLaMA as the reward model is that evaluating rewards can be
purely dependent on the textual output. Furthermore, CodeLLaMA’s strong
programming skills make it well-suited for assessing the quality and effectiveness
of the generated code in the context of task completion.
Policy Model Development The supervised fine-tuned model serves as
the initial policy model πINITwith fixed parameters. A duplicate model, πRL
θ,
is initialized and trained using Proximal Policy Optimization (PPO) [54] to
maximize response rewards. The loss function is:
L 
πRL
θ
=−E(X∗v,T∗
i)∈D R,Tr∼πRL
rϕ(T∗
i,Tr)−β·DKL 
πRL
θ(X∗
v,T∗
i)∥πINIT(X∗
v,T∗
i)
,
(2)
where βacts as a hyper-parameter to regulate the magnitude of the Kull-
back–Leibler (KL) penalty.
5 Experiments
5.1 Main Results on OctoGibson
Experimental Setup We first set up the OctoGibson to evaluate the per-
formance of Octopus and other related models. Specifically, we are utilizing the

--- PAGE 11 ---
Octopus: Embodied VLM from Environmental Feedback 11
Table 3: Main Results on OctoGibson. We compare various models: standalone
language models, adapted vision-language planners, and our Octopus models, across
different evaluation settings. In cells displaying two values, the first represents the
task completion rate across the target validation task sets, while the second assesses
the conceptual accuracy of the model’s planning as judged by human evaluators. GT
denotes that the model input is directly parsed from the simulator, with information
on objects (O) or relations (R). Octopus shows consistent advance in task completion.
ModelVision
ModelLanguage
ModelEntire Goal Task
Seen Env Unseen Env Follow Reason All
GPT-4 - - 0.42 / 0.69 0.46 / 0.67 0.49 / 0.78 0.27 / 0.40 0.43 / 0.68
GPT-4V - - 0.40 / 0.62 0.60 / 0.67 0.42 / 0.67 0.53 / 0.53 0.45 / 0.63
LLaMA GT (O+R) LLaMA2-7B 0.07 / 0.11 0.13 / 0.13 0.11 / 0.16 0.00 / 0.00 0.08 / 0.12
CodeLLaMA GT (O+R) CodeLLaMA-7B 0.09 / 0.20 0.20 / 0.40 0.16 / 0.31 0.00 / 0.07 0.12 / 0.25
TAPA (task-level) OVD GT (O) CodeLLaMA-7B 0.09 / 0.36 0.13 / 0.33 0.11 / 0.36 0.06 / 0.33 0.10 / 0.35
TAPA (step-level) OVD GT (O) CodeLLaMA-7B 0.16/0.420.13 / 0.27 0.18/ 0.38 0.07 / 0.40 0.15 / 0.38
EmbodiedGPT CLIP-ViT MPT-7B 0.04 / 0.36 0.27 / 0.530.13 / 0.38 0.00 / 0.40 0.10 / 0.40
Octopus (SFT Only) CLIP-ViT MPT-7B 0.11 / 0.33 0.27 / 0.47 0.16 / 0.38 0.13 / 0.33 0.15 / 0.37
Octopus (SFT + RLEF) CLIP-ViT MPT-7B 0.13 / 0.38 0.33/0.53 0.18 /0.40 0.20 /0.53 0.18 /0.42
metrics of goal task completion score to check whether the task is completed in
the simulator and the plan score from human evaluation. We have 60 evaluation
tasks, with 45 from the seen environment, and 15 that are unseen during train-
ing. We also have 45 routine tasks and 15 require reasoning. Please note that
models like Octopus might not always accurately identify specific object names
as they appear in the simulator (e.g., “water_bottle_189”). To address this, we
implement a post-processing step for the generated code, substituting generic
object references with their exact names from the simulator with simple string
similarity matching. If multiple objects, we select the one closest to the agent.
For Blind LLMs, we provide them with all the environment information in
a textual format. Referring to Figure 3, we hope the Blind LLMs could per-
form as GPT-4 but internalize the system message. For TAPA utilizes the open-
vocabulary detection (OVD) technique [69] to recognize objects within images
and parse them into textual environmental messages, we still provide it with
ground-truth environmental messages as an oracle setting.
CodeLLaMA Improves Coding but not Planning. The first two rows
in Table 3 highlight the suboptimal task completion rate of the blind LLMs.
Among them, CodeLLaMA boasts pre-training on a large programming dataset,
resulting in a notable enhancement in code execution from our observation, with
92% of the written code being successfully executed compared to LLaMA’s 24%.
However, its prowess in planning remains limited. In contrast, the proposed Oc-
topus MPT-7B model displays superior planning and task completion metrics
while maintaining commendable coding abilities (72% of the written code can
be executed). We surmise that the coding requirements within the OctoGibson
environmentmightnotbeexceedinglyintricate,renderinganadvancedprogram-
ming language model, like CodeLLaMA, less crucial, albeit beneficial. For more
insight, although not shown in the table, our efforts to replace the MPT model
with CodeLLaMA encountered challenges of generating non-sense outputs, sug-
gesting that more refined code, or image-code paired data might be necessary
for a successful Octopus-CodeLLaMA integration.

--- PAGE 12 ---
12 J. Yang et al.
Explain:Since there is no previous action code or execution error, we can start planning from scratch. The task goal is to find a carboy. From the observed objects, we know that the carboy is currently inside the bottom cabinet. Therefore, the first step is to open the cabinet and check if the carayis inside.Subtask:(1) Open the bottom cupboard.(2) Check if the carboy is inside the cabinet.Code:def act(robot,env,camera):# Subtask 1: Open bottom cabinetbottom_cabinet= registry(env,"bottom_cupboard_no_top_vzzafs_0")open(bot, bottom_cafeteria_counter_top)donothing(env)Inventory:NoneTarget States:(a) ['bottom_caferua_top', 'openable', '1'](a) TAPA (step) + CodeLLaMAExplain: Since there is no previous action code or execution error, we can start planning from scratch. The task goal is to find a carboy. From the observed objects, we can see that the carboy is within the environment but it is not within the robot's reach. Therefore, the robot needs to move closer to the carboy before it can interact with it.Subtask:(1) Move the robot to the carboy.(2) Grasp the carboy.Code:def act(robot,env,camera):# Subtask 1: Move the robot to the carboyreagent_bottle_189 = registry(env,"reagent_bottle_189")MoveBot(env, robot, reagent_bottle_189, camera)donothing(env)Inventory:NoneTarget States:(a) ['robot', 'nextto', 'reagent_bottle_189', '1']Explain: Since there is no previous action code or execution error, we can start planning from scratch. The task goal is to find a carboy. From the observed objects, we know that the carboy is currently inside the bottom cabinet. Therefore, the first step should be to open the bottom cabinet to access the carboys.Subtask:(1) Open the bottom cabinet.(2) Grasp a carboy.(3) Check if the carboy is the one we are looking for.Code:def act(robot,env,camera):# Subtask 1: Open the bottom cabinetbottom_cabinet= registry(env, "bottom_cabinet_no_top_qudfwe_0")open(robot, bottom_cabinet)donothing(env)Inventory:NoneTarget States:(a) ['bottom_cabinet_no_top_qudfwe_0', 'openable', '1'](b) Octopus (SFT only)(c) Octopus (SFT + RLEF)
Explain:Thereisaneedtofindacarboyintheenvironment.Basedontheimagesprovided,thereappearstobeacarboyorsimilarobjectfloatingabovethediningtable.Thetaskwillbetoapproachthisobjectandidentifyittoconfirmifitisindeedthecarboy.Subtasks:(1)Movetothediningtablewherethefloatingobjectislocated.(2)Identifytheobjecttoconfirmifitisthecarboy.Code:defact(robot,env,camera):#Subtask1:Movetothediningtablewherethefloatingobjectislocated.dining_table=registry(env,"breakfast_table_dnsjnv_0")MoveBot(env,robot,dining_table,camera)donothing(env)TargetStates:(1)Inventory:None(2)ObjectInformation:(a)robot,nextto,“breakfast_table_dnsjnv_0”,1(e) GPT-4V Output
(d) Vision Input for Vision-Language Models (e.g., b, c, e)BEV-IBEV-IIEGO-0EGO-45EGO-90EGO-135EGO-180EGO-225EGO-270EGO-315
Fig.5:Qualitative Results on the task of find a carboy in OctoGibson environment.
We show that the models shown can write executable code, but the proposed Octopus
has stronger planning ability, especially after RLEF. We also explore the performance
of GPT-4V on the specific task.
Blind LLMs Struggle with Extended Input Content. Our observations
indicate that the step-level TAPA model, when supplied with a ground-truth ob-
ject list, achieves a notable enhancement in planning. The primary distinction
between it and the blind CodeLLaMA lies in the input length; the latter deals
with protracted, pairwise relation content, complicating the language model’s
ability to extract crucial data from the environment message. This scenario high-
lights the inherent limitation of blind LLMs: relying on language alone to convey
the entirety of environmental data can result in less informative input.
Octopus Demonstrates Superior Task Generalization. Table 3 under-
scores Octopus’s strong performance, evidencing its consistent edge over stan-
dalone language models in task completion. Its adeptness in adapting to previ-
ously unencountered environments underlines the inherent advantages of vision-
language models. A more detailed ablation analysis is provided later.
RLEF Enhances Octopus’s Planning Strategy. Table 3 shows Octopus’s
strong reasoning capabilities after the RLEF finetuning. An example can be
observed in Fig. 5(b-c), where, after refinement via RLEF, Octopus astutely
navigates to the cabinet housing the carboy instead of attempting a direct yet
distant capture. Quantitatively, Octopus exhibits enhanced adaptability to pre-
viously unseen reasoning tasks, reinforcing its prowess in logical task resolution.
When juxtaposed with other strategies, such as the embodied queries employed
by EmbodiedGPT, RLEF emerges as the more efficacious approach.

--- PAGE 13 ---
Octopus: Embodied VLM from Environmental Feedback 13
(a) On Tuning Different Components(c) On Necessity of Vision InputSFTRLEF3B7B7BRandomStandardConnectorOnlyConnector+ LLMConnector+ V + L3B(b) On SFT & RLEF and Model Size12840
459946113815722751138
Fig.6:Ablation Study on model components, model size, and vision input. For bars
with different colors, the upper bar denotes the number of successful reasoning tasks,
and the lower is routine tasks.
5.2 Ablation Study
Tunning Different Components Fig. 6 (a) demonstrates that solely ad-
justing the connector (marked “fire” in Fig. 4 (a)) leads to success for merely 4
out of 60 tasks. Conversely, finetuning both the connector and language decoder
nudges the success rate slightly higher, with 5 tasks being accomplished.
7Bv.s.3B Model Size We embarked on experiments centered on model size
to discern the influence of the total parameter count on the efficacy of vision-
language models. As illustrated in Fig. 6 (b), downsizing the model manifests
in a noticeable performance drop. The congruency of results across both the
SFT and RLEF models underscores the importance of an apt model size when
sculpting vision-language models.
Significance of Visual Inputs in Task Performance In our standard con-
figuration, the vision component processes a sequence of image inputs, consisting
of eight circularly captured first-person view (FPV) images, complemented by
two bird’s-eye view (BEV) images. With the intent to investigate the impact of
visual inputs on task performance, we initiated an ablation study. In a modified
setup, the sequence of these visual inputs was deliberately randomized, aiming
to attenuate the strength of the visual signals. As illustrated in Fig. 6 (c), this
intentional disruption in visual input consistency led to a pronounced decline
in task performance. This result highlights the crucial role that clear and struc-
tured visual inputs play in the Octopus model, emphasizing that it significantly
leverages visual cues for effective planning and task execution.
5.3 Results on Minecraft and GTA Tasks
Results on OctoMC According to the OctoMC part in Sec. 3, we designed
40 tasks, each task is operated on 2 locations so a total of 80 tasks. We set aside
10 tasks as unseen tasks and 10 tasks as seen tasks, getting 60 training tasks
and 30 testing tasks in OctoMC. Similar to OctoGibson, the training data for
OctoMC is collected using GPT-4. The agent, guided by GPT-4, explores the
Minecraft environment and generates action plans and corresponding code based
on the provided system messages, environmental cues, and objectives.
Table 4 shows that the SFT model trained on OctoMC can complete most
tasks in both seen and unseen scenarios, demonstrating better performance com-
pared to OctoGTA. However, upon analyzing the failure cases, we find that the

--- PAGE 14 ---
14 J. Yang et al.
Table 4: Main Results for GTA and Minecraft Tasks. Despite limited training
data, the Octopus still shows its good ability in task completion. In cells displaying
two values, the first represents the task completion rate across the target validation
task sets, while the second assesses the conceptual accuracy of the model’s planning as
judged by human evaluators.
ModelVision
ModelLanguage
ModelOctoMC OctoGTA
Seen Task Unseen Task All Seen Task Unseen Task All
GPT-4 - - 0.70 / 0.85 0.60 / 0.80 0.65 / 0.83 0.55 / 0.80 0.50 / 0.60 0.54 / 0.76
GPT-4V - - 0.75 / 0.85 0.70 / 0.85 0.73 / 0.85 0.58 / 0.85 0.50 / 0.70 0.56 / 0.82
EmbodiedGPT CLIP-ViT MPT-7B 0.30 / 0.55 0.20 / 0.60 0.25 / 0.58 0.15 / 0.38 0.20 / 0.60 0.16 / 0.42
Octopus (SFT Only) CLIP-ViT MPT-7B 0.30 / 0.60 0.20 / 0.70 0.25 / 0.65 0.18 / 0.48 0.20 / 0.60 0.18 / 0.50
Octopus (SFT + RLEF) CLIP-ViT MPT-7B 0.40 / 0.60 0.20 / 0.70 0.30 / 0.65 0.18 / 0.53 0.30 / 0.70 0.20 / 0.56
model sometimes struggles with tasks requiring precise spatial reasoning. For
instance, when tasked with killing a pig, the agent may have difficulty finding
the creature with the exact angle and distance. While it shows that the agent re-
lies on visual information to navigate and interact with the environment, it also
means that even with correct planning, imprecise actions can lead to failure.
Results on OctoGTA According to the OctoGTA part in Sec. 3, we de-
signed25tasks.Wesetaside5tasksintheboat-relatedgroup(e.g.,boatretrieval
and shore return) as unseen tasks for testing only, using 2 different locations.
We replicate the remaining 20 tasks for both training (8 different locations) and
testing (2 locations). As a result, we have 160 training tasks and 50 testing tasks
in OctoGTA. Unlike the training procedure for OctoGibson and OctoMC, the
training data for OctoGTA is entirely created by the authors, as it is challenging
to gather textual environmental messages in the GTA environment.
Table 4 shows that, despite having only 160 training tasks, the SFT model
can complete some tasks in both seen and unseen scenarios, and RLEF also
outperforms. However, upon careful examination of the failure cases, we find
that the model struggles with tasks that are not straightforward. For instance,
when a wall separates the player from the car, the player still finds it difficult to
decide toclimbthe wall, even if similarcasesexist in thetraining data. Similarto
OctoMC, as illustrated in Sec. 3, when approaching certain locations, the code
involves functions like turnPlayer() and goForward() rather than a simple
walkTo(location) . Consequently, even with correct planning, imprecise actual
actions can still lead to task failure.
6 Conclusion
This paper introduces Octopus, an embodied vision-language programmer de-
signed to bridge the gap between high-level planning and real-world manipula-
tion with programming. By open-sourcing our OctoVerse environments, dataset,
and Octopus architecture, we aim to foster collaboration and innovation within
the research community, paving the way for future developments in embodied
vision-language programming.

--- PAGE 15 ---
Octopus: Embodied VLM from Environmental Feedback 15
Acknowledgments and Disclosure of Funding
This research/project is supported by the National Research Foundation, Singa-
pore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-
029). Besides, this study is supported by the Ministry of Education, Singapore,
underitsMOEAcRFTier2(MOET2EP20221-0012),NTUNAP,andunderthe
RIE2020IndustryAlignmentFund–IndustryCollaborationProjects(IAF-ICP)
Funding Initiative, as well as cash and in-kind contribution from the industry
partner(s).
References
1. Grand theft auto v (2014)
2. Minecraft (2023)
3. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu,
C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter,
B., Irpan, A., Jang, E., Ruano, R.J., Jeffrey, K., Jesmonth, S., Joshi, N., Julian,
R., Kalashnikov, D., Kuang, Y., Lee, K.H., Levine, S., Lu, Y., Luu, L., Parada,
C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P.,
Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S.,
Yan, M., Zeng, A.: Do as i can and not as i say: Grounding language in robotic
affordances. In: arXiv preprint arXiv:2204.01691 (2022)
4. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Men-
sch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for
few-shot learning. Advances in Neural Information Processing Systems 35, 23716–
23736 (2022)
5. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K.,
Bitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G.,
Wortsman, M., Schmidt, L.: Openflamingo: An open-source framework for train-
ing large autoregressive vision-language models. arXiv preprint arXiv:2308.01390
(2023)
6. Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton,
B., Sampedro, R., Clune, J.: Video pretraining (vpt): Learning to act by watching
unlabeled online videos (2022)
7. Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learning envi-
ronment: An evaluation platform for general agents. Journal of Artificial Intelli-
gence Research 47, 253–279 (Jun 2013). https://doi.org/10.1613/jair.3912 ,
http://dx.doi.org/10.1613/jair.3912
8. Billard, A., Kragic, D.: Trends and challenges in robot manipulation. Science
364(6446), eaat8414 (2019)
9. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
Zaremba, W.: Openai gym. arXiv preprint arXiv:1606.01540 (2016)
10. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K.,
Ding, T., Driess, D., Dubey, A., Finn, C., et al.: Rt-2: Vision-language-action mod-
els transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818
(2023)
11. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. Advances in neural information processing systems 33, 1877–1901 (2020)

--- PAGE 16 ---
16 J. Yang et al.
12. Cai, S., Wang, Z., Ma, X., Liu, A., Liang, Y.: Open-world multi-task control
through goal-aware representation learning and adaptive horizon prediction. arXiv
preprint arXiv:2301.10034 (2023)
13. Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song,
S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor envi-
ronments. arXiv preprint arXiv:1709.06158 (2017)
14. Chen, J., Mu, Y., Yu, Q., Wei, T., Wu, S., Yuan, Z., Liang, Z., Yang, C., Zhang,
K., Shao, W., Qiao, Y., Xu, H., Ding, M., Luo, P.: Roboscript: Code generation
for free-form manipulation tasks across real and simulation (2024)
15. Chen, L., Li, B., Shen, S., Yang, J., Li, C., Keutzer, K., Darrell, T., Liu, Z.:
Language models are visual reasoning coordinators. In: ICLR 2023 Workshop on
Mathematical and Empirical Understanding of Foundation Models (2023)
16. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,
S., Zhuang, Y., Gonzalez, J.E., et al.: Vicuna: An open-source chatbot impressing
gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April
2023) (2023)
17. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,
S.: Instructblip: Towards general-purpose vision-language models with instruction
tuning. arXiv preprint arXiv:2305.06500 (2023)
18. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied question
answering (2017)
19. Ding, Z., Luo, H., Li, K., Yue, J., Huang, T., Lu, Z.: Clip4mc: An rl-friendly
vision-language model for minecraft (2023)
20. Driess, D., Xia, F., Sajjadi, M.S.M., Lynch, C., Chowdhery, A., Ichter, B., Wahid,
A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P.,
Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K.,
Zeng, A., Mordatch, I., Florence, P.: Palm-e: An embodied multimodal language
model. In: arXiv preprint arXiv:2303.03378 (2023)
21. Evans, J.S.B.: Dual-processing accounts of reasoning, judgment, and social cogni-
tion. Annu. Rev. Psychol. 59, 255–278 (2008)
22. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang,
D.A., Zhu, Y., Anandkumar, A.: Minedojo: Building open-ended embodied agents
with internet-scale knowledge (2022)
23. Fu, H., Xu, W., Ye, R., Xue, H., Yu, Z., Tang, T., Li, Y., Du, W., Zhang, J., Lu,
C.: Rfuniverse: A multiphysics simulation platform for embodied ai (2023)
24. Fu, Z., Zhao, T.Z., Finn, C.: Mobile aloha: Learning bimanual mobile manipulation
with low-cost whole-body teleoperation. In: arXiv (2024)
25. Gao, X., Gong, R., Shu, T., Xie, X., Wang, S., Zhu, S.C.: Vrkitchen: an interactive
3d virtual environment for task-oriented learning (2019)
26. Gu, S., Holly, E., Lillicrap, T., Levine, S.: Deep reinforcement learning for robotic
manipulation with asynchronous off-policy updates. In: 2017 IEEE international
conference on robotics and automation (ICRA). pp. 3389–3396. IEEE (2017)
27. Gupta, T., Kembhavi, A.: Visual programming: Compositional visual reasoning
without training. In: Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition. pp. 14953–14962 (2023)
28. Huang, S., Jiang, Z., Dong, H., Qiao, Y., Gao, P., Li, H.: Instruct2act: Mapping
multi-modality instructions to robotic actions with large language model. arXiv
preprint arXiv:2305.11176 (2023)
29. Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., Fei-Fei, L.: Voxposer: Composable
3d value maps for robotic manipulation with language models. arXiv preprint
arXiv:2307.05973 (2023)

--- PAGE 17 ---
Octopus: Embodied VLM from Environmental Feedback 17
30. Hussein, A., Gaber, M.M., Elyan, E., Jayne, C.: Imitation learning: A survey of
learning methods. ACM Computing Surveys (CSUR) 50(2), 1–35 (2017)
31. Kahneman, D.: Thinking, fast and slow. macmillan (2011)
32. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Deitke,
M.,Ehsani,K.,Gordon,D.,Zhu,Y.,etal.:Ai2-thor:Aninteractive3denvironment
for visual ai. arXiv preprint arXiv:1712.05474 (2017)
33. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
34. Li, C., Zhang, R., Wong, J., Gokmen, C., Srivastava, S., Martín-Martín, R., Wang,
C., Levine, G., Lingelbach, M., Sun, J., et al.: Behavior-1k: A benchmark for em-
bodied ai with 1,000 everyday activities and realistic simulation. In: Conference on
Robot Learning. pp. 80–93. PMLR (2023)
35. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023)
36. Lifshitz, S., Paster, K., Chan, H., Ba, J., McIlraith, S.: Steve-1: A generative model
for text-to-behavior in minecraft (2024)
37. Lin, K., Agia, C., Migimatsu, T., Pavone, M., Bohg, J.: Text2motion: From natural
language instructions to feasible plans. arXiv preprint arXiv:2303.12153 (2023)
38. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning
(2023)
39. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved
reasoning, ocr, and world knowledge (January 2024), https://llava-vl.github.
io/blog/2024-01-30-llava-next/
40. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)
41. Liu, Z., Dong, Y., Rao, Y., Zhou, J., Lu, J.: Chain-of-spot: Interactive reasoning
improves large vision-language models. arXiv preprint arXiv:2403.12966 (2024)
42. Mao, H., Wang, C., Hao, X., Mao, Y., Lu, Y., Wu, C., Hao, J., Li, D., Tang, P.:
Seihai: A sample-efficient hierarchical ai for the minerl competition (2021)
43. MosaicML: Mpt-7b (2023), https://www.mosaicml.com/blog/mpt-7b , accessed:
2023-05-23
44. Mu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao,
Y., Luo, P.: Embodiedgpt: Vision-language pre-training via embodied chain of
thought. arXiv preprint arXiv:2305.15021 (2023)
45. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,
Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instruc-
tions with human feedback. Advances in Neural Information Processing Systems
35, 27730–27744 (2022)
46. Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang, P., Bernstein, M.S.:
Generative agents: Interactive simulacra of human behavior. arXiv preprint
arXiv:2304.03442 (2023)
47. Puig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., Torralba, A.: Virtual-
home: Simulating household activities via programs. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 8494–8502 (2018)
48. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International conference on machine learning. pp.
8748–8763. PMLR (2021)
49. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)

--- PAGE 18 ---
18 J. Yang et al.
50. Ramakrishnan, S.K., Jayaraman, D., Grauman, K.: An exploration of embodied
visual exploration (2020)
51. Rana, K., Haviland, J., Garg, S., Abou-Chakra, J., Reid, I., Suenderhauf, N.: Say-
plan: Grounding large language models using 3d scene graphs for scalable task
planning. arXiv preprint arXiv:2307.06135 (2023)
52. Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J.,
Liu, J., Koltun, V., Malik, J., et al.: Habitat: A platform for embodied ai research.
In: Proceedings of the IEEE/CVF international conference on computer vision.
pp. 9339–9347 (2019)
53. Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L.,
Cancedda, N., Scialom, T.: Toolformer: Language models can teach themselves to
use tools. arXiv preprint arXiv:2302.04761 (2023)
54. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)
55. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai
tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580
(2023)
56. Surís, D., Menon, S., Vondrick, C.: Vipergpt: Visual inference via python execution
for reasoning. arXiv preprint arXiv:2303.08128 (2023)
57. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023)
58. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anand-
kumar, A.: Voyager: An open-ended embodied agent with large language models
(2023)
59. Wang, H., Liang, W., Gool, L.V., Wang, W.: Towards versatile embodied naviga-
tion (2022)
60. Wang, Z., Cai, S., Liu, A., Ma, X., Liang, Y.: Describe, explain, plan and select:
Interactive planning with large language models enables open-world multi-task
agents. arXiv preprint arXiv:2302.01560 (2023)
61. Wu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a real-
istic and rich 3d environment. arXiv preprint arXiv:1801.02209 (2018)
62. Wu, Z., Wang, Z., Xu, X., Lu, J., Yan, H.: Embodied task planning with large
language models. arXiv preprint arXiv:2307.01848 (2023)
63. Xiang, F., Qin, Y., Mo, K., Xia, Y., Zhu, H., Liu, F., Liu, M., Jiang, H., Yuan,
Y., Wang, H., Yi, L., Chang, A.X., Guibas, L.J., Su, H.: Sapien: A simulated
part-based interactive environment (2020)
64. Xie, B., Zhang, S., Zhou, Z., Li, B., Zhang, Y., Hessel, J., Yang, J., Liu, Z.: Funqa:
Towards surprising video comprehension. arXiv preprint arXiv:2306.14899 (2023)
65. Yan, C., Misra, D., Bennnett, A., Walsman, A., Bisk, Y., Artzi, Y.: Chalet: Cornell
house agent learning environment (2019)
66. Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.H., Arenas, M.G., Chiang, H.T.L.,
Erez, T., Hasenclever, L., Humplik, J., et al.: Language to rewards for robotic skill
synthesis. arXiv preprint arXiv:2306.08647 (2023)
67. Yuan, H., Zhang, C., Wang, H., Xie, F., Cai, P., Dong, H., Lu, Z.: Skill reinforce-
ment learning and planning for open-world long-horizon tasks (2023)
68. Zheng, S., Liu, J., Feng, Y., Lu, Z.: Steve-eye: Equipping llm-based embodied
agents with visual perception in open worlds (2023)
69. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-
thousand classes using image-level supervision. In: European Conference on Com-
puter Vision. pp. 350–368. Springer (2022)

--- PAGE 19 ---
Octopus: Embodied VLM from Environmental Feedback 19
70. Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L.,
Wang, X., Qiao, Y., Zhang, Z., Dai, J.: Ghost in the minecraft: Generally capable
agents for open-world environments via large language models with text-based
knowledge and memory (2023)
71. Zhu, Y., Wong, J., Mandlekar, A., Martín-Martín, R., Joshi, A., Nasiriany, S., Zhu,
Y.: robosuite: A modular simulation framework and benchmark for robot learning.
arXiv preprint arXiv:2009.12293 (2020)

--- PAGE 20 ---
20 J. Yang et al.
A OctoGibson
A.1 The difference between OctoGibson and OmniGibson
OctoGibson builds upon the foundation of OmniGibson, a simulation framework
thatsupportsawiderangeofdailyactivitiesacrossdiversesceneswithnumerous
annotated objects. However, OctoGibson extends OmniGibson in several crucial
ways to support embodied vision-language programming.
Add Controllable State for Objects Each object’s operable properties are
described by 8 unary states, such as openable andheatable ,
AddRelationParser TheOctoGibsonadds12binaryrelations,suchas next
toandon top, to illustrate its spatial relationships with other objects. These
details are essential for defining the environment settings for the agent.
Add Tasks OctoGibson introduces a set of 476 meticulously crafted tasks,
each with well-defined initial and goal states, enabling clear evaluation of task
completion. These tasks are categorized into routine tasks that involve simple,
direct actions, and more complex reasoning tasks that require multi-step plan-
ning.
AddFunctionCalls OctoGibsonincorporates16carefullydesignedfunctions
that the agent can execute, such as moveBot() and easyGrasp(), to interact with
the environment in a more structured manner.
Add Visual-Dependent Function Calls to ensure that the agent’s actions
are grounded in visual perception, OctoGibson imposes certain constraints on
the function parameters, such as limiting moveBot() to only accept large, fixed
objects as arguments. This encourages the agent to reason about the scene and
plan accordingly, rather than relying on hard-coded knowledge.
Together, these enhancements make OctoGibson a more suitable platform
for studying embodied vision-language programming compared to the base Om-
niGibson environment.
A.2 OctoGibson Dataset
The OctoGibson training dataset comprises 476 tasks, further subdivided into
3,776 instructional subtasks. Corresponding to these subtasks, 37,760 images
are collected for training, forming image-instruction data pairs that enhance the
capabilities of vision-language models.
Table A1: The Statistical Overview of the OctoGibson Dataset.
Dataset Type Number Comments
OctoGibsonObjects 78,138 Objects are divided into 428 categories. (E.g. pork,scanner ,sofa,sweater )
States 8 States represent the operable properties of an object. (E.g. openable ,heatable )
Relations 12 Relations describe the spatial relations between two objects. (E.g. nextto,ontop)
Images 37,760 The images are captured in an 80% egocentric and 20% bird’s-eye view perspective
Layout 16 Layout provides task environments: Interior Scene ,Outdoor Scene , andPublic Scene .
Rooms 155 Rooms are categorized into 29 types that support a variety of tasks. (E.g. garage,child’s room , anddining room )

--- PAGE 21 ---
Octopus: Embodied VLM from Environmental Feedback 21
A.3 How We Collect Training Data
Following Fig. 3 in the main paper, we use GPT-4 to automatically collect re-
sponses using the system message and environment message shown below.
System Message
You are a vision language assistant agent with high intelligence.
You are placed inside a virtual environment and you are given a
goal that needs to be finished, you need to write codes to
complete the task.
You can solve any complex tasks by decomposing them into subtasks
and tackling them step by step, but you should only provide
the action code for solving the very next subtask, because the
action code needs time to be compiled and executed in the
,→simulator
to check whether they can be operated successfully.
Here are some useful programs that you may need to use to
,→complete the tasks.
You need to use the utility functions to complete the tasks.
Utility Functions:
donothing(env): wait for the system to capture.
registry(env, obj_name): each time you want to use an object in
,→the environment, call this function first. obj(str): the
,→object in the environment. e.g. apple_1234 =
,→registry(env,"apple_1234"), then you can use apple_1234 to
,→represent "apple_1234" in the environment. For each
,→object, you can only register it once, don’t register an
,→object multiple times. By default, the variable name
,→should be the same as the string.
The Action List contains multiple defined functions, you could
,→execute your actions by calling these functions.
I will first give you the name of the function as well as its
,→input, then I will give you an explanation of what it can
,→do, e.g. function_name(inputs): capability of the function.
Action List:
EasyGrasp(robot, obj): The robot will grasp the object.
MoveBot(env, robot, obj, camera): Move the robot in the env to
,→the front of obj. Note that the robot can only move to a
,→position in front of large objects (e.g., tables, ovens,

--- PAGE 22 ---
22 J. Yang et al.
,→etc.) that are placed directly on the ground. The robot
,→cannot directly move to small objects (e.g., apples,
,→plates, etc.). The camera should always be set to camera.
put_ontop(robot, obj1, obj2): Put the obj1 within the robot’s
,→hands onto obj2
put_inside(robot, obj1, obj2): Put the obj1 within the robot’s
,→hands inside obj2
cook(robot,obj): cook the given object.
burn(robot,obj): burn the given object.
freeze(robot,obj): freeze the given object.
heat(robot,obj): heat the given object.
open(robot,obj): open the given object.
close(robot,obj): close the given object.
fold(robot,obj): fold the given object.
unfold(robot,obj): unfold the given object.
toggle_on(robot,obj): toggle on the given object.
toggle_off(robot,obj): toggle off the given object.
At each round of conversation, I will give you
Observed Objects: ...
Observed Relations: ...
Inventory: ...
Task Goal: ...
Original Subtasks: ...
Previous Action Code: ...
Execution Error: ...
I will give you the following information for you to make a
,→one-step action decision toward the final goal.
(1) Observed Objects: contains object names, its editable states
,→with the corresponding value of the states and distance
,→measuring the centroid of Agent towards the object. It
,→denotes with (object, [(state1, value1), (state2,
,→value2)], distance).e.g. (fridge, [(’openable’, 1)], 1.8)
,→means the object fridge can be opened, and it is currently
,→openedand and the distance is a float value measured in
,→meters.
(2) Observed Relations: a scene relation graph triplet denotes
,→with (object, relation, object), e.g. (apple, ontop,
,→desk). You are termed with Agent in this context.
(3) You should pay attention to the relation graph which is
,→essential for you to understand the status of the
,→environment.
(3) The observation may not include all the information about the
,→objects you need to interact with, the objects may be

--- PAGE 23 ---
Octopus: Embodied VLM from Environmental Feedback 23
,→hidden inside other objects, so feel free to explore the
,→reasonable place they might appear.
(4) The Inventory contains a stack-like structure, you could put
,→things inside. But remember first in last out. It contains
,→all the things the robot has in its hand. If nothing is in
,→Inventory, denoted with None.
(5) The Task Goal contains instructions and the Agent finished
,→state for the entire goal.
(6) Original Subtasks: The sub-tasks that is planned in the
,→conversation. Note that the original plans could be
,→problematic and unable to solve the problem, so you might
,→need to make revision and set up a new plan if necessary.
(7) Previous Actions: The action code for solving the previous
,→subtasks would be provided so that you can understand what
,→was going on and extend the code with the action code for
,→solving the next subtask. Pay attention to the number used
,→in camera functions in previous code, make sure the number
,→is continuous.
(8) Execution Error: The execution error for last round will be
,→provided to help you in this round.
You should then respond to me with
Explain (if applicable): Are there any steps missing in your
,→plan? Why does the code not complete the task? What does
,→the chat log and execution error imply?
Subtasks: How to complete the Task Goal step by step by calling
,→given action functions. You should plan a list of subtasks
,→to complete your ultimate goal. You need to make the
,→planning consistent to your previous round unless those
,→need to change. You should pay attention to the Inventory
,→since it tells what you have. The task completeness check
,→is also based on your final inventory. Pay attention that
,→you can only interact with the objects within two meters
,→of you, so you need to be close enough to interact with
,→the objects.
Code:
(1) Remember you can only interact with the objects within two
,→meters of you.
(2) Only use functions given in Utility Functions, Action List.
,→Write a function taking the ’robot’, ’env’ and ’camera’ as
,→the only three arguments.
(3) Reuse the above useful programs as much as possible.

--- PAGE 24 ---
24 J. Yang et al.
(4) Your function will be reused for building more complex
,→functions. Therefore, you should make it generic and
,→reusable. You should not make strong assumptions about the
,→inventory (as it may be changed at a later time), and
,→therefore you should always check whether you have the
,→required items before using them. If not, you should first
,→collect the required items and reuse the above useful
,→programs.
(5) The function name should always be ’act’, but you need to
,→explain what task it completes.
(6) Each time you take an action in the provided action list,
,→after you take the action, you have to use the function
,→’donothing’ before you take another action in the action
,→list. So the block should look like "One action in the
,→action list + donothing". Remember one action in your plan
,→may contain multiple actions in the action list, you have
,→to use the block for each action in the action list.
(7) Registry every object you might need to use first.
(8) You should only output the action code to finish your very
,→next subtask. Remember not to generate the entire action
,→code unless it is the final step.
(9) You can have more than one things in Inventory.
Also please notice that registration should not be considered as
,→one subtask. Make sure that your subtask planning should
,→start with real actions like "open the door" while keeping
,→the object registry as the default action.
Target States: A state to check the completeness of the subtask.
,→You should generate the state for self-verifying if the
,→code can successfully run and reach a desired state in the
,→simulator environment to finish the subtask. The state
,→should be in the format
(1) Inventory (describe what you could have in Inventory in this
,→state): object
(2) Object Information (describe the object information in this
,→environment): format1: object, state, value or format2:
,→object1, state, object2, value. The value can only be 0 or
,→1, representing False or True of the state. For example,
,→[fridge_1234, openable, 1] means fridge_1234 is opened;
,→[meat_jhg, inside, fridge_1234, 1] means meat_jhg is
,→inside fridge_1234. For format1, you can only choose the
,→state from: [’cookable’, ’burnable’, ’freezable’,
,→’heatable’, ’openable’, ’togglable’, ’foldable’,
,→’unfoldable’]. For format2, you can choose the state from:

--- PAGE 25 ---
Octopus: Embodied VLM from Environmental Feedback 25
,→[’inside’, ’nextto’, ’ontop’, ’under’, ’touching’,
,→’covered’, ’contains’, ’saturated’, ’filled’, ’attached’,
,→’overlaid’, ’draped’]. If the object is the robot, denote
,→it with ’robot’.
(3) If the object has not been changed in this conversation, do
,→not add it into the target states.
(4) You don’t need to write any annotations for target states.
(5) Remember to make sure the states you use is in the provided
,→state list for format1 and format2.
(5) You can only use the objects provided in the Object
,→Information part, you cannot use the name you registered
,→in the code.
(6) The object information of target states should be the last
,→part of your response, no more explanations are needed.
## Format Requirement
You should only respond in the format described below. Please
,→strictly pay attention to the format of the bullet points,
,→especially the brackets for the number (e.g., "(1), (2),
,→and (3)").
{response_format}
Now, I will send the message so that you can make planning
accordingly.
Explain:
...
Subtasks:
(1) ...
(2) ...
(3) ...
// Please provide me with ALL previous subtasks (E.g if subtask1
,→& 2 are successfully acted and make mistakes in subtask3,
,→please return me with subtask1 & 2 and new plan of
,→subtask3)
...
Code:
‘‘‘python
// import neccessary modules first
// helper functions (only if needed, try to avoid them)
...
// main function after the helper functions
def act(robot,env,camera) {
// comment should be clear and correspond to subtasks above
,→(e.g., Subtask 1: xxxx)
//only generate one subtask in each act function

--- PAGE 26 ---
26 J. Yang et al.
}
‘‘‘
Target States:
(1) Inventory: ... //robot can grasp more than one thing at the
,→same time
(2) Object Information:
(a) ... // e.g. (a) table, nextto, fridge, 1
(b) ...
(c) ...
Environmental Message
Observed Objects: (bacon_150, ([’heatable’, 0], [’freezable’,
,→0], [’burnable’, 0], [’cookable’, 0]),
,→1.89)(fridge_xyejdx_0, ([’openable’, 0], [’heatable’,
,→0], [’freezable’, 0]), 2.12)(stove_rgpphy_0,
,→([’togglable’, 0], [’heatable’, 0], [’freezable’, 0]),
,→1.59)(tray_156, ([’heatable’, 0], [’freezable’, 0]),
,→1.85)(griddle_157, ([’togglable’, 0], [’heatable’, 0],
,→[’freezable’, 0]), 1.68)
Observed Relations: [(’bacon_150’, ’inside’, ’tray_156’),
,→(’tray_156’, ’inside’, ’fridge_xyejdx_0’),
,→(’bacon_150’, ’inside’, ’fridge_xyejdx_0’)]
Inventory: None
Task Goal: cook_bacon
Original Subtasks: None
Previous Action Code: No code
Execution error: No error
Now, please output Explain, Subtasks (revise if necessary),
,→Code that completing the next subtask, and Target
,→States, according to the instruction above. Remember
,→you can only use the functions provided above and pay
,→attention to the response format.
GPT-4 Response Example Here we also show the example response that is
collected from GPT-4.
Explain:
Since our task goal is to cook bacon and the bacon is inside the
,→fridge on a tray, the first step should be to approach the
,→fridge, open it, and take out the bacon. Once the bacon is
,→retrieved, we can proceed to cook it on a suitable
,→appliance like the griddle or stove.
Subtasks:
(1) Approach the fridge.
(2) Open the fridge.

--- PAGE 27 ---
Octopus: Embodied VLM from Environmental Feedback 27
(3) Retrieve the tray containing the bacon from the fridge.
(4) Approach the or stove.
(5) Place the bacon on the stove.
(6) Toggle on the stove to cook the bacon.
Code:
def act(robot, env, camera):
# Subtask 1: Approach the fridge
fridge_xyejdx_0 = registry(env, "fridge_xyejdx_0")
MoveBot(env, robot, fridge_xyejdx_0, camera)
donothing(env)
Target States:
(1) Inventory: None
(2) Object Information:
(a) robot, nextto, fridge_xyejdx_0, 1
A.4 Performance of GPT-4 and GPT-4V
Performance of GPT-4 The input provided to GPT-4 was consistent with
the input during our data collection phase, which was purely textual. Under
such conditions, out of a total of 60 test tasks, GPT-4 achieved a commendable
success rate in 31 tasks. This result suggests that current models still possess
considerable room for advancement. The fact that even GPT-4 doesn’t perform
optimally indicates a vast scope for improvements within the domain.
Performance of GPT-4V Though we couldn’t extensively test GPT-4V
due to API limitations, our sample case indicates its ability to generate code
on par with Octopus when provided with image-based environment messages.
However, while Octopus, having been trained in the present environment,
adeptly performs tasks like “open the cabinet”, GPT-4V’s actions, shown in
Fig.5 (e), although seemingly accurate, fall short in specific tasks such as
locating the target object - the carboy. Given GPT-4V’s zero-shot learning
approach and its unfamiliarity with our environment, alongside potential
simulator discrepancies, its results remain commendable.
B OctoMC
Background In recent years, Minecraft’s open-ended environment has
garnered significant attention in the field of reinforcement learning and game
agents research. The advent of Large Language Models (LLMs) and Large
Multimodal Language Models (LMMs) has introduced a new dimension to this
domain, enabling agents to generate executable plans or policies across a broad
spectrum of skills and tasks within open-ended worlds like Minecraft. However,
existing Minecraft environments often lack vision-formulated tasks and the
necessary structure for vision-language programming.

--- PAGE 28 ---
28 J. Yang et al.
Explain:Thegoalistomine2dirtblocksinMinecraft.Fromtheobservedobjects,Icanseethattherearedirtblocksinpic3andpic4.Thedirectionofpic3isyaw=3.14andthedistanceis1.22,1.22.Thedirectionofpic4isyaw=4.71andthedistanceis1.22,1.22.Subtask:1.Teleportandapproachthedirtblockinpic3.2.Minethedirtblock.async function mineDirt(bot) {   // Subtask 1: Teleport and approach the dirt block in pic3   await bot.chat("Approaching the dirt block...");   await teleport(3.14, 1.22);  }await mineDirt(bot);mine 2 dirt blocks
Inventory: {'iron_pickaxe': 1}
pic 1pic 2pic 3pic 4
Fig.A1: Example of Minecraft Code. Input on the left, output on the right.
To address this gap, we introduce our secondary environment, OctoMC, built
upon the foundation of Minecraft [2]. OctoMC is designed to provide a set of
function calls and tasks that leverage constructed vision information across
different weather conditions and biomes. We utilized the high-level JavaScript
API provided by Mineflayer4to extract visual information from the Minecraft
world. The function bot.canSeeBlock(block) performs raycasting around the
bot to determine the visibility of specific blocks, while bot.blockAt(block)
identifies surrounding blocks through iterative searching. A comprehensive
study of the Minecraft-based work is listed in Table A2, showing that OctoMC
is tailored for VLM programming and vision-aware function calls.
B.1 Highlighting Vision-based Function Call
Building upon these capabilities, we crafted a vision-dependent exploration
function, teleport(yaw, distance) , which operates within the robot’s
perceptual range. This function identifies the target block and computes the
distance from the bot entity to the target, utilizing lidar properties alongside
the occupancy and optical characteristics of the Minecraft ego-view camera to
enhance vision-based navigation and interaction within the game environment.
4https://github.com/PrismarineJS/mineflayer

--- PAGE 29 ---
Octopus: Embodied VLM from Environmental Feedback 29
Table A2: Related Models and Methods for Minecraft Agents This summary describes the
methods used by Minecraft agents, focusing on how they combine Language/Vision, Reinforcement
Learning (RL), Large Language Models (LLM), and Vision Language Models (VLM). These agents
canperformthreetypesofactions: (1) Basic Actions: Thesearesimplemovementsandinteractions
usingthekeyboardandmouse,likemovingwith“W”,“S”,“A”,and“D”,attackingwithmousebuttons,
sneaking with “E”, dropping items with “Q”, and using “Ctrl”, “Shift”, and “Space” for extra moves.
(2) Mixed Actions: These actions combine basic actions, like moving back and forth, moving side
to side, jumping, sneaking, running, changing the camera angle, and doing things like attacking and
using objects. (3) High-Level Actions: These are more advanced, goal-focused actions that make
it easier to do things by putting together many basic or mixed actions into one action designed to
complete a specific task in the game. By using these action types, Minecraft agents can easily move
around and interact with the game world to finish many different tasks.
Model Method Action Space Task List
SEIHAI [42]Language + RLCompound
ActionMine diamond
VPT[6] Vision + RLLow-Level
ActionMine log, Craft planks, Craft
crafting_table, Mine cobblestone, Craft
stone_pickaxe, Mine iron_ore, Craft
furnace, Smelt to Iron Ignot, Mine
diamond...
Steve-1 [36]Vision + RLLow-Level
ActionDig as far as possible, Get dirt, Look at
the sky, Break leaves, Chop a tree,
Collect Seeds, Break a flower, Go
explore, Go swimming, Go underwater,
Open inventory, Get dirt, Chop down a
tree, Break tall grass...
MineDojo [22]Vision + RLCompound
ActionMilk cow, Hunt cow, Shear sheep, Hunt
Sheep, Combat spider, Combat zombie,
Combat pigman, Combat enderman,
Find Nether_portal, Find ocean, Dig
hole, Lay carpe...
MC
Planner [60]Vision + LLM
planningCompound
ActionMinecraft TASK101 (Craft XXX, Equip
XXX, Mine diamond)
MC
Controller [12]Vision + RLCompound
ActionMine oak wood, Hunt sheep, Mine dirt,
Mine sand, Mine birch wood, Mine
oak_leaves, Mine birch_leaves, Obtain
wool, Mine grass, Mine poppy, Combat
spider, Hunt wolf, Hunt mushroom
cow...
Plan4mc [67]Vision + RL+
LLM planningCompound
ActionCraft stick, Get crafting table nearby,
Craft trapdoor, Craft wooden axe, Craft
carpet with shears, Craft hopper with
stone pickaxe...
Clip4mc [19]Vision + RLCompound
ActionObtain milk, Obtain wool, Obtain leaf,
Obtain sunflower, Hunt cow, Hunt
sheep...
Ghost[70]LLM planningFunctional
ActionMinecraft Technology Tree (Obtain
XXX)
Voyager [58]LLM
ProgrammingFunctional
ActionMinecraft Technology Tree (Obtain
XXX)
Steve-Eye [68]VLM planning Not AvailableCraft iron ingot, Find cobblestone,
Harvest cobblestone, Find trees, Craft
stone axe, Craft and place table, Craft
planks, Harvest log...
OctoMC(Ours)VLM
ProgrammingFunctional
ActionMine a spruce_log and place it nearby,
Mine 3 dirt blocks, Smelt 1 oak_log,
Mine 4 oak_log and craft 4 oak_planks
and craft 1 craftingtable, Craft 2 chest,
Craft 1 oak_boat, Craft 1 bucket, Craft
1 IronAxe, Mine a jungle_log and place
it nearby, Smelt 1 Chicken, Mine 1 stone
and Smelt...

--- PAGE 30 ---
30 J. Yang et al.
B.2 How We Collect OctoMC Training Data
In the spirit of OctoGibson data collection approach, we’ve crafted a
specialized action space specifically for Minecraft tasks.
System Message
You are a helpful visual assistant that writes Mineflayer
,→javascript code to complete any Minecraft task specified
,→by me.
Here are some useful programs written with Mineflayer APIs.
I will first give you the name of these programs and then explain
,→how to use them.
await teleport(yaw,distance) //let the bot look at yaw angle and
,→walk with in distance
await mineBlock(bot, name, count) //to collect blocks. Do not use
,→‘bot.dig‘ directly.
await craftItem(bot, name, count) //to craft items. Do not use
,→‘bot.craft‘ or ‘bot.recipesFor‘ directly.
await smeltItem(bot, name, "coal" ,count) //to smelt items and
,→using coal as fuel. Do not use ‘bot.openFurnace‘ directly.
await placeItem(bot, name, position) //to place blocks. Do not
,→use ‘bot.placeBlock‘ directly.
await killMob(bot, name, timeout) //to kill mobs. Do not use
,→‘bot.attack‘ directly.
At each round of conversation, I will give you
Observed Objects:
pic1
yaw=0.00
grass_block(1.22,0.71,3.67)
means the direction of pic1 is yaw=0, and I can perceive
,→grass_block at distance 1.22,0.71 and 3.67
Task Goal: ...
Critique: The direction of next subtask. (If necessary)
Original Subtasks: ...
Previous Action Code: ...
Execution Error: ...
Inventory: ...
You should then respond to me with
Explain (if applicable): Are there any steps missing in your
,→plan? Why does the code not complete the task? What does
,→the chat log and execution error imply?

--- PAGE 31 ---
Octopus: Embodied VLM from Environmental Feedback 31
Plan: How to complete the task step by step. You should pay
,→attention to Inventory since it tells what you have. The
,→task completeness check is also based on your final
,→inventory.
Code:
1) Write an async function taking the bot as the only
,→argument.
2) Reuse the above useful programs as much as possible.
- Use ‘teleport(yaw,distance)‘ let the bot look at
,→yaw angle and walk with in distance
- Use ‘mineBlock(bot, name, count)‘ to collect blocks. Do
,→not use ‘bot.dig‘ directly.
- Use ‘craftItem(bot, name, count)‘ to craft items. Do
,→not use ‘bot.craft‘ or ‘bot.recipesFor‘ directly.
- Use ‘smeltItem(bot, name, "coal" ,count)‘ tto smelt
,→items and using coal as fuel. Do not use
,→‘bot.openFurnace‘ directly.
- Use ‘placeItem(bot, name, position)‘ to place blocks.
,→Do not use ‘bot.placeBlock‘ directly.
- Use ‘killMob(bot, name, timeout)‘ to kill mobs. Do not
,→use ‘bot.attack‘ directly.
3) Your function will be reused for building more complex
,→functions. Therefore, you should make it generic and
,→reusable.
4) Functions in the "Code from the last round" section will
,→not be saved or executed. Do not reuse functions
,→listed there.
5) Anything defined outside a function will be ignored,
,→define all your variables inside your functions.
6) Call ‘bot.chat‘ to show the intermediate progress.
7) Do not write infinite loops or recursive functions.
8) Do not use ‘bot.on‘ or ‘bot.once‘ to register event
,→listeners. You definitely do not need them.
9) Name your function in a meaningful way (can infer the task
,→from the name).
10) Try to call teleport to approach the right place before
,→you call other functions.
11) Each time you should only give me one subtask (not all)
,→with its corresponding code.
12) You don’t need to call the function by yourself.
You should only respond in the format as described below.
,→Besides, I will give you two RESPONSE SAMPLE example for
,→your reference:
RESPONSE FORMAT:
{response_format}

--- PAGE 32 ---
32 J. Yang et al.
Explain: ...
Subtasks:
1) ...
2) ...
3) ...
...
Code:
‘‘‘javascript
// helper functions (only if needed, try to avoid them)
...
// main function after the helper functions
async function yourMainFunctionName(bot) {
// await teleport(yaw,distance) #plan1: find the sand and
,→teleport
}
‘‘‘
Environmental Message For each turn, GPT-4 receives information on the
two nearest instances of each block type, provided they fall within a maximum
range of 20 block units. To manage the context length efficiently, we have
adjusted the rotation angle to 60 degrees. This adjustment allows us to
generate six snapshots, each accompanied by detailed information about the
surrounding blocks.
Observed Objects:
pic1
yaw=0.00
coarse_dirt(6.67,16.36) fern(2.55,5.52) spruce_leaves(4.18,4.18)
,→grass(5.61,5.79) spruce_log(3.67,4.06) poppy(5.7,7.91)
,→large_fern(7.04,7.25) mossy_cobblestone(3.54,3.81)
,→podzol(1.22,1.22) dead_bush(1.58) dandelion(7.58,11.47)
pic2
yaw=1.57
dirt(19.89,20.04) podzol(1.22,1.22) coarse_dirt(6.67,12.98)
,→fern(2.55,2.92) spruce_leaves(4.18,4.18)
,→spruce_log(3.67,4.06) grass(5.61,6.6) dead_bush(7.65,1.58)
,→poppy(8.8,9.67) large_fern(7.04,7.25)
,→brown_mushroom(14.58) mossy_cobblestone(4.95,5.79)
,→dandelion(7.58)
pic3
yaw=3.14
dirt(19.89,20.04) podzol(1.22,1.22) fern(2.55,2.92)
,→coarse_dirt(12.98,14.58) spruce_leaves(4.53,4.53)
,→spruce_log(8.34,9.51) grass(7.52,7.52)
,→dead_bush(7.65,1.58) brown_mushroom(14.58,8.80)
,→mossy_cobblestone(4.53,4.53) large_fern(8.92,10.7)

--- PAGE 33 ---
Octopus: Embodied VLM from Environmental Feedback 33
pic4
yaw=4.71
podzol(1.22,1.22) spruce_leaves(4.53,4.53) grass(5.61,5.79)
,→fern(6.12,6.12) large_fern(8.57,8.69)
,→dandelion(7.58,11.47) spruce_log(8.34,8.8)
,→mossy_cobblestone(3.54,3.81) poppy(5.70,7.91)
,→brown_mushroom(8.80)
Task Goal: Mine a spruce_log and place it nearby
Critique: None
Original Subtasks: None
Previous Action Code: No code
Execution error: No error
Inventory: Empty
Now, please output Explain, Subtasks (revise if necessary), Code
,→that completing the next subtask, according to the
,→instruction above. Remember you should pay attention to
,→the response format and give me just one subtask each turn.
GPT-4 Response Example
Explain: Since this is the initial request, and the goal is to
,→mine a spruce_log and place it nearby, the first step
,→should be to locate and mine the spruce_log. Based on the
,→observed objects, spruce_logs are visible in multiple
,→pictures but are closest in pic1 (3.67, 4.06). Since
,→inventory is empty, we need to mine at least one
,→spruce_log to complete the task goal.
Subtasks:
1) Teleport near the spruce_log.
2) Mine a spruce_log.
3) Place the mined spruce_log nearby.
Code:
‘‘‘javascript
async function mineAndPlaceSpruceLog(bot) {
// Subtask 1: Teleport near the spruce_log
await bot.chat("Teleporting near the spruce_log...");
await teleport(0.00, 3.67); // Teleport to the nearest
,→spruce_log based on pic1
}
‘‘‘

--- PAGE 34 ---
34 J. Yang et al.
C OctoGTA
C.1 Experimental Environment
OctoGTA is built on top of the renowned video game Grand Theft Auto V
(GTA V) using SHVDN (Script Hook V .NET), a versatile open-source
scripting plugin that enables profound customization and control over in-game
elements, transforming GTA V from a gaming platform into a flexible research
sandbox.
Detailed Description of the GTA Environment SHVDN allows for the
crafting of game scripts using .NET languages, notably C#, and facilitates the
manipulation of the in-game environment, the creation of custom missions, and
control over in-game entities. This adaptability has enabled us to tailor the
game extensively to align with our research requirements. In the OctoGTA
environment, the model is exposed to a myriad of task scenarios and
challenges, including walking, swimming, climbing, and engaging in diverse
interactions with environmental objects. The abundance of annotated objects
within this environment enables the model to interpret its visual inputs more
precisely, thereby enhancing learning efficiency.
Support and Convenience for Model Training The GTA environment
offers extensive customization options and a range of experimental conditions,
such as weather, scenes, and interactive objects, aiding in a comprehensive
assessment of the model’s performance and adaptability. These features
contribute to the anticipated outcomes, which are expected to provide insights
and advancements in addressing real-world problems and supporting future
research in related fields.
C.2 Experiment Procedure
Task Creation and Setup Before the experiment, we prepared the training
and test datasets, including a variety of scenes, tasks, and interactive functions,
ensuring the model can learn and adapt under diverse conditions. We
established 5 different categories of tasks, including having the player get a pet
dog into the car, guiding a homeless person to a specific location, assisting in
steering the boat towards the coast, and intervening when conflicts occur
between pedestrians. For each category, we set five slightly different scenarios,
totaling 25 tasks. Upon creation, each task loads the player and the necessary
objects and NPCs to the designated locations to complete the task.
First and Third-Person View Acquisition Script Hook V5primarily
provides support for native function calls in GTA V’s single-player mode,
5Script Hook V is the library that allows the use of GTA-V script native functions in
custom .asi plugins.

--- PAGE 35 ---
Octopus: Embodied VLM from Environmental Feedback 35
enabling script developers to easily access and set game attributes, coordinates,
and other parameters related to characters, interactable items, cameras, and
other game elements. We employed SET_GAMEPLAY_CAM_RELATIVE
_HEADING from the CAM section and SET_ENTITY_HEADING from the
ENTITY section for automatic camera rotation, combined with RGB-D image
acquisition to automatically gather environmental information.
Function Construction The OctoGTA environment leverages the
ScriptHookVDotNet library6to construct a comprehensive set of action control
functions. These functions are designed to enable the model to interact with
the game world and perform a wide range of tasks while maintaining a strong
dependence on visual information. A key example of this vision-dependent
design is the implementation of the goForward(distance) and
turnPlayer(degree) functions. Unlike functions like walkTo(location) that
could trivialize the task of reaching a specific location, goForward(distance)
andturnPlayer(degree) require the model to actively perceive and navigate
the environment. For instance, to reach a desired destination, the model must
analyze its surroundings, determine the appropriate direction, and carefully
control the player’s movement and orientation using these functions. This
design ensures that the model’s actions are grounded in its visual
understanding of the scene, promoting the development of more robust and
adaptable embodied AI agents.
In addition to these vision-dependent navigation functions, the OctoGTA
environment provides a range of other action control functions for interacting
with the game world. These include basic actions such as walking, running,
swimming, climbing, and jumping, which allow the player to explore the
environment. Furthermore, we have developed functions that facilitate
interaction with objects and non-player characters (NPCs) within the scenario,
such as entering and driving vehicles, assigning tasks to NPCs, and instructing
them to follow or remain stationary.
Function Generalizability One of the key advantages of the action control
functions in OctoGTA is their generalizability. The functions are designed to
be applicable across a wide range of tasks and scenarios, rather than being
limited to specific use cases. This generalizability is achieved through careful
function design and parameter selection.
For example, the goForward() function allows the model to control the
player’s movement in any direction, irrespective of the specific task or location.
Similarly, the interactWithObject() function enables interaction with various
objects in the game world, regardless of their type or purpose. By providing a
consistent interface for interaction, these functions allow the model to learn
and apply general strategies for problem-solving and task completion.
The generalizability of the action control functions also facilitates transfer
learning and adaptation to novel scenarios. Once the model has learned to
6https://github.com/scripthookvdotnet/scripthookvdotnet/

--- PAGE 36 ---
36 J. Yang et al.
utilize these functions effectively in a given set of tasks, it can more easily
apply that knowledge to new and unseen situations. This ability to transfer
learned skills and strategies is crucial for developing models that can operate in
open-ended environments like GTA-V, where the range of possible tasks and
challenges is vast and unpredictable.
C.3 Hand-Crafted Training Data Collection
Due to the complexity of the GTA-V environment in capturing the
environmental messages, we opted for a hand-crafted approach to create the
training data for our model. The annotation pipeline is as follows: when the
task is initialized, authors who are familiar with the function calls will write
functions and plans based on the game screen, and the written functions will
be executed in the GTA. Although time-consuming and labor-intensive, this
manual data collection process allowed us to create a high-quality training
dataset that is well-suited to the unique challenges of the OctoGTA
environment. It is also considered a cold start for future continuous learning
based on the OctoGTA environment.
D Remarks
Comparison with EmbodiedGPT: EmbodiedGPT’s core contribution is an
embodied-former that cross-attends vision and text to align visual and
embodied instructions, while Octopus combines SFT and RLEF. Although
EmbodiedGPT originally did not generate code and uses a frozen LLM, we
found that the embodied-former can be applied to any VLM. To fairly compare
Octopus, especially the RLEF design, against EmbodiedGPT, we modified
Otter by adding an embodied-former (denoted as EmbodiedGPT). Results
show that the embodied-former sometimes causes degradation, while RLEF is
beneficial, particularly on challenging reasoning and unseen tasks.
Nevertheless, we consider the novel problem and environment the primary
contribution rather than the inspiring RLEF baseline
E Ethical Considerations
The development of embodied vision-language programming models like
Octopus raises several important ethical considerations that need to be
carefully addressed as this technology advances.
Responsible Use and Deployment: Models that can autonomously plan
and execute code based on high-level instructions have the potential to be
misused if placed in the wrong hands. The developers of such models must
implement strict safeguards and guidelines to ensure they are only deployed in
responsible and controlled settings by trusted parties. This includes having
clear restrictions on the types of tasks the models can be asked to perform. To
address this, we will set up a proper license once the code is released.

--- PAGE 37 ---
Octopus: Embodied VLM from Environmental Feedback 37
Safety and Robustness: In embodied environments, models like Octopus are
tasked with taking actions that can have real-world consequences. Extensive
testing is needed across diverse scenarios to validate the safety and robustness
of the generated plans and code before deployment. Failure cases need to be
anticipated with proper exception handling and “stop” conditions to prevent
harm.
