# Bộ Xếp Hạng Mã Lệnh Thần Kinh Nhận Biết Lỗi
Jeevana Priya Inala Chenglong Wang Mei Yang Andres Codas
Mark Encarnación Shuvendu K Lahiri Madanlal Musuvathi Jianfeng Gao
Microsoft Research
{jinala,chenwang,meiyang,andres.codas,markenc,
shuvendu,madanm,jfgao}@microsoft.com

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã chứng minh khả năng ấn tượng trong việc tạo ra mã lệnh cho các tác vụ lập trình khác nhau. Trong nhiều trường hợp, LLM có thể tạo ra chương trình chính xác cho một tác vụ khi được cho nhiều lần thử. Do đó, một xu hướng gần đây là thực hiện lấy mẫu quy mô lớn các chương trình sử dụng một mô hình và sau đó lọc/xếp hạng các chương trình dựa trên việc thực thi chương trình trên một số lượng nhỏ các bài kiểm tra đơn vị đã biết để chọn một giải pháp ứng viên. Tuy nhiên, các phương pháp này giả định rằng các bài kiểm tra đơn vị được cho trước và giả định khả năng thực thi an toàn các chương trình được tạo ra (có thể thực hiện các thao tác nguy hiểm tùy ý như thao tác tệp). Cả hai giả định trên đều không thực tế trong phát triển phần mềm thực tế. Trong bài báo này, chúng tôi đề xuất CODERANKER, một bộ xếp hạng thần kinh có thể dự đoán tính chính xác của một chương trình được lấy mẫu mà không cần thực thi nó. CODERANKER của chúng tôi có khả năng nhận biết lỗi, tức là nó được huấn luyện để dự đoán các loại thông tin thực thi khác nhau như dự đoán loại lỗi biên dịch/runtime chính xác (ví dụ: IndexError hoặc TypeError). Chúng tôi chỉ ra rằng CODERANKER có thể tăng đáng kể độ chính xác pass@1 của các mô hình tạo mã khác nhau (bao gồm Codex [11], GPT-Neo, GPT-J) trên các bộ dữ liệu APPS [25], HumanEval [11] và MBPP [3].

## 1 Giới thiệu
Các mô hình ngôn ngữ lớn dựa trên transformer (LLM) có khả năng ấn tượng [19,7,16], bao gồm khả năng tạo ra mã lệnh [11,3,28,39,33]. Nhiệm vụ ở đây là lấy mô tả ngôn ngữ tự nhiên hoặc ngữ cảnh mã trước đó làm đầu vào và tạo ra toàn bộ chương trình trong ngôn ngữ lập trình đa năng như Python hoặc C++. Tạo ra toàn bộ chương trình là một nhiệm vụ khó khăn vì nó liên quan đến việc hiểu nhiệm vụ, tìm ra cách thực hiện nó, và viết mã mà không có bất kỳ lỗi cú pháp/runtime nào. Trên các bài toán lập trình khó hơn như các bài toán thi đấu lập trình, các mô hình hiện tại đạt độ chính xác rất thấp đặc biệt khi ngân sách lấy mẫu thời gian suy luận thấp. Ví dụ, trên bộ dữ liệu APPS [25], một mô hình tạo mã tiên tiến, Codex [11], đạt 4% độ chính xác nếu nó chỉ được phép lấy mẫu một chương trình cho mỗi tác vụ (gọi là pass@1), nhưng đạt 24% độ chính xác nếu nó được phép 100 mẫu cho mỗi tác vụ (gọi là pass@100—ít nhất một chương trình chính xác trong 100 mẫu) (xem Bảng 7). Quan sát này dẫn chúng ta đến một vấn đề nghiên cứu quan trọng về việc khám phá các phương pháp xếp hạng các chương trình được lấy mẫu để thu hẹp khoảng cách giữa hiệu suất pass@1 và pass@100.

Khi phân tích các chương trình được lấy mẫu thu được bằng LLM, chúng tôi thấy rằng một số trong số chúng có lỗi cú pháp và lỗi runtime, và một số thực thi để tạo ra các đầu ra không mong muốn. Do đó, các công trình trước [11,28] tập trung vào việc xếp hạng các chương trình bằng cách thực thi chúng trên một tập nhỏ các bài kiểm tra đơn vị (thường được giả định là được cung cấp như một phần của mô tả tác vụ). Tuy nhiên, có một số khuyết điểm đối với phương pháp này: Thứ nhất, ngay cả khi một chương trình vượt qua các bài kiểm tra được cho, nó vẫn có thể thất bại trên các bài kiểm tra đơn vị chưa biết. Thứ hai, có gánh nặng cho người dùng phải cung cấp bài kiểm tra đơn vị cho mọi tác vụ suy luận.

Thứ ba, để thực thi mã, tất cả các dependency phải được cài đặt đúng cách. Điều này đặc biệt có vấn đề trong các tình huống mà người dùng muốn nhận đề xuất mã trong một dự án có nhiều tệp và dependency (như sử dụng CoPilot trong môi trường VS Code [1]). Ngay cả khi các dependency này được thỏa mãn, nhiều tình huống lập trình thực tế liên quan đến mã không hoàn chỉnh đang trong quá trình phát triển tích cực nơi việc thực thi chỉ là không khả thi. Cuối cùng, mã được tạo ra bởi LLM có thể có rủi ro bảo mật (như xóa tệp trên đĩa) và do đó, việc thực thi mã như vậy cần các cơ chế cách ly nặng như môi trường sandbox.

Để giảm bớt các vấn đề trên với việc dựa vào thực thi mã, một số công trình gần đây [18,36] đề xuất sử dụng bộ xếp hạng dựa trên mạng nơ-ron để xếp hạng các chương trình được lấy mẫu từ LLM. Một mô hình xếp hạng về cơ bản là một bộ phân loại lấy đầu vào là mô tả tác vụ và một chương trình được lấy mẫu và dự đoán xác suất chương trình đó chính xác đối với mô tả tác vụ. Khi được cho nhiều chương trình (thu được bằng lấy mẫu), bộ xếp hạng sắp xếp lại chúng theo thứ tự giảm dần của xác suất dự đoán chương trình đó chính xác. Bộ xếp hạng về cơ bản đang cố gắng mô phỏng việc thực thi một chương trình trên một số bài kiểm tra đơn vị mà không thực sự thực thi mã trong quá trình suy luận. Dữ liệu huấn luyện được thu thập bằng cách thực thi cả chương trình đúng và sai được lấy mẫu từ chính mô hình tạo mã. Do đó, chúng ta chỉ cần bài kiểm tra đơn vị và khả năng thực thi trong bước tạo bộ dữ liệu thay vì trong quá trình suy luận như trong các phương pháp trước.

Trong khi các phương pháp trước nhắm đến các bài toán toán học để làm cho bài toán học tập dễ xử lý, trong bài báo này, chúng tôi thiết kế CODERANKER cho các tác vụ lập trình tổng quát phức tạp hơn trong Python. Một chương trình Python được lấy mẫu có thể thất bại theo nhiều cách khác nhau. Ví dụ, một chương trình khi được thực thi trên một bài kiểm tra đơn vị có thể dẫn đến lỗi biên dịch/runtime và có thể tạo ra nhiều loại đầu ra như số nguyên, chuỗi, danh sách và từ điển có thể tạo ra sự không khớp kiểu. Ngược lại trong lĩnh vực toán học, có ít cơ hội cho lỗi biên dịch/runtime và đầu ra thường chỉ là một số.

Phương pháp CODERANKER của chúng tôi dựa trên ý tưởng rằng một bộ xếp hạng thần kinh được huấn luyện để phân biệt giữa các chế độ thất bại khác nhau có thể hiểu chương trình và tác vụ tốt hơn, và do đó có thể làm tốt hơn trong việc xếp hạng các chương trình. Do đó, chúng tôi thiết kế CODERANKER nhận biết lỗi và chúng tôi điều tra tác động của nó đối với hiệu suất xếp hạng. Mỗi bộ xếp hạng nhận biết lỗi là một bộ phân loại được huấn luyện để dự đoán một/hai nhãn đa lớp được trích xuất từ thông tin phong phú thu được bằng cách thực thi các chương trình.

Chúng tôi sử dụng bộ dữ liệu APPS để tinh chỉnh/huấn luyện các mô hình tạo mã và CODERANKER. Trên bộ dữ liệu này, chúng tôi đã chỉ ra rằng CODERANKER cải thiện hiệu suất pass@1 của Codex (được sử dụng theo cách few-shot) từ 26% lên 39.6% trên tập validation và từ 3.8% lên 4.5% trên tập test. Chúng tôi cũng thấy rằng các bộ xếp hạng của chúng tôi có thể chuyển giao sang bộ dữ liệu khác mà không cần huấn luyện thêm. Trên bộ dữ liệu HumanEval, chúng tôi cải thiện pass@1 của Codex từ 26% lên 32% và trên bộ dữ liệu MBPP, chúng tôi cải thiện từ 36% lên 42%. Chúng tôi thấy các cải thiện hiệu suất tương tự với các mô hình tạo mã khác như GPT-J và GPT-Neo. So với bộ xếp hạng dựa trên bộ phân loại nhị phân đơn giản, CODERANKER nhận biết lỗi của chúng tôi đạt hiệu suất xếp hạng tốt hơn. Cuối cùng, chúng tôi điều tra tác động của việc trộn các bộ dữ liệu xếp hạng từ nhiều mô hình tạo mã, điều này mang lại cho chúng ta thêm một sự cải thiện trong hiệu suất.

## 2 Kiến thức cơ bản

### 2.1 Tạo mã
**Tác vụ:** Một tác vụ tạo mã G là một prompt, được biểu diễn như một chuỗi token, chỉ định tác vụ cần thực hiện. G thường là sự kết hợp của ngôn ngữ tự nhiên, các ví dụ đầu vào-đầu ra, và mã khởi tạo. Một giải pháp S cho tác vụ tạo mã là một chuỗi token cùng nhau tạo thành một chương trình để giải quyết tác vụ được cho. Các bộ dữ liệu hiện có cũng chứa một tập các cặp đầu vào-đầu ra được sử dụng để kiểm tra tính chính xác của chương trình được tạo ra. Hình 1 đưa ra một ví dụ về tác vụ tạo mã, giải pháp tham chiếu tương ứng, và các bài kiểm tra đơn vị được lấy từ bộ dữ liệu APPS [25].

**Mô hình:** Có một số LLM được huấn luyện trước hiện có trong tài liệu phù hợp để tạo mã theo cách few-shot hoặc sau khi tinh chỉnh. Một mô hình tạo mã F cung cấp một cách để chúng ta lấy mẫu các chương trình cho một tác vụ G cho trước là Si ~ PF(S|G) trong đó PF biểu thị phân phối xác suất được tạo ra bởi mô hình tạo mã F.

Trong bài báo này, chúng tôi nghiên cứu bốn mô hình tạo mã khác nhau—(i) Codex, (ii) GPT-J (6B), (iii) GPT-Neo 1.3B và (iv) GPT-Neo 125M (Bảng 1). Codex là mô hình tạo mã tiên tiến lớn nhất có sẵn công khai để truy vấn thông qua API và đã cho thấy hiệu suất ấn tượng trong tạo mã [11]. Nó được xây dựng trên kiến trúc mô hình ngôn ngữ GPT-3 và được huấn luyện trên 180 GB dữ liệu GitHub. GPT-Neo và GPT-J là các mô hình mã nguồn mở với số lượng tham số từ 125M đến 6B. Các mô hình này được huấn luyện trước trên bộ dữ liệu Pile (800 GB corpus ngôn ngữ tự nhiên với 8% dữ liệu GitHub). Vì các mô hình này là mã nguồn mở, có thể tinh chỉnh các mô hình này trên bộ dữ liệu downstream như bộ dữ liệu APPS. Ngoài các mô hình trên, còn có các mô hình tạo mã khác như AlphaCode [28] và mô hình của Google [3]. Trong khi phương pháp của chúng tôi có thể áp dụng cho bất kỳ mô hình nào trong số này, trong nghiên cứu này, chúng tôi xác thực hiệu quả của CODERANKER trên một tập các mô hình tạo mã tiên tiến có sẵn công khai, để tái tạo được.

**Chỉ số:** Các mô hình tạo mã được đánh giá dựa trên tính chính xác chức năng hơn là khớp chính xác/mờ với chương trình tham chiếu. Điều này là do các chỉ số dựa trên khớp không thể tính đến không gian lớn và phức tạp của các chương trình tương đương chức năng với chương trình tham chiếu. Tính chính xác chức năng được ước tính bằng cách kiểm tra xem chương trình được lấy mẫu có vượt qua một tập các bài kiểm tra đơn vị hay không. Các phương pháp trước đánh giá tính chính xác chức năng bằng chỉ số pass@k; cho k mẫu chương trình được tạo ra cho mỗi tác vụ, một tác vụ được coi là giải được nếu bất kỳ mẫu nào vượt qua các bài kiểm tra đơn vị. Chỉ số pass@k đo tỷ lệ tổng số tác vụ được giải quyết. Ngoài ra, chúng tôi định nghĩa chỉ số exec@k tính toán tỷ lệ các tác vụ mà có ít nhất một chương trình trong k mẫu thực thi mà không có bất kỳ lỗi biên dịch/runtime nào, tức là tạo ra giá trị không lỗi cho mỗi đầu vào test, nhưng có thể hoặc không khớp với đầu ra mong muốn.

### 2.2 Xếp hạng mã
Cho n chương trình được lấy mẫu sử dụng mô hình tạo mã, S1, ..., Sn ~ PF(S|G), mục tiêu của CODERANKER là tìm một thứ tự của các chương trình So1, ..., Son sao cho để tính toán ranked pass@k cho k ≤ n, một bài toán được coi là giải được nếu bất kỳ chương trình nào trong tập {So1, ..., Sok} vượt qua các bài kiểm tra đơn vị. Một mô hình CODERANKER R lấy đầu vào là tác vụ tạo mã G và chương trình được lấy mẫu Si và đưa ra điểm số si. Thứ tự xếp hạng của các chương trình được lấy mẫu được cho bởi So1, ..., Son sao cho so1 > so2 > ... > son.

## 3 Bộ Xếp Hạng Mã Thần Kinh Nhận Biết Lỗi

Một mô hình CODERANKER là một bộ phân loại được huấn luyện để phân loại một cặp ⟨G, Si⟩ là CORRECT hay không, trong đó CORRECT có nghĩa là Si thỏa mãn tác vụ G đối với các bài kiểm tra đơn vị của nó. Điểm số si được tính như si = PR(CORRECT|G, Si) trong đó PR là xác suất theo mô hình xếp hạng R. Xác suất này được trích xuất bằng cách sử dụng các giá trị thực từ lớp cuối cùng trước lớp SoftMax.

### 3.1 Bộ dữ liệu Xếp hạng Mã

Để huấn luyện mô hình CODERANKER, chúng ta cần bộ dữ liệu có cả chương trình CORRECT và WRONG (tức là không chính xác). Để thu thập các chương trình này, chúng ta sử dụng các mô hình tạo mã để lấy mẫu n = 100 chương trình cho mỗi tác vụ trong bộ dữ liệu huấn luyện. Sau đó chúng ta thực thi các chương trình được lấy mẫu trên các bài kiểm tra đơn vị tương ứng để tạo ra các nhãn phân loại. Theo quan sát từ [18], người ta phải cẩn thận để không huấn luyện quá mức các mô hình tạo mã cơ sở để đảm bảo tính đa dạng trong các chương trình được lấy mẫu. Do đó, chúng tôi chỉ tinh chỉnh các mô hình tạo mã cơ sở tối đa 2 epoch và chọn checkpoint dẫn đến loss validation thấp nhất (điều này không áp dụng cho mô hình Codex, được sử dụng theo cách few-shot). Bảng 2 cho thấy phân phối của các bộ dữ liệu xếp hạng thu được bằng cách sử dụng 4 mô hình tạo mã khác nhau cho các tác vụ trong bộ dữ liệu APPS. Như có thể mong đợi, bộ dữ liệu xếp hạng có sự mất cân bằng cao với khoảng 5X đến 40X nhiều điểm dữ liệu WRONG hơn điểm dữ liệu CORRECT và tỷ lệ này cao hơn đối với các mô hình nhỏ hơn như mô hình GPT-Neo.

**Bộ dữ liệu xếp hạng nhận biết lỗi:** Một bộ xếp hạng đơn giản là bộ được huấn luyện để dự đoán nhãn nhị phân CORRECT hoặc WRONG. Tuy nhiên, một chương trình thất bại vì nhiều lý do khác nhau và biết tại sao chương trình có thể thất bại là rất quan trọng để dự đoán liệu chương trình có CORRECT hay không. Do đó, chúng tôi thiết kế bộ dữ liệu xếp hạng nhận biết lỗi. Khi chúng ta thực thi một chương trình trên một tập các bài kiểm tra đơn vị, thông báo compiler nhiều hơn chỉ là một bit thông tin. Thực tế, khi bài kiểm tra đơn vị thất bại, chúng ta biết nó thất bại vì lỗi biên dịch/runtime (mà chúng ta gọi là lỗi thực thi) hay vì chương trình tạo ra đầu ra sai cho một đầu vào cụ thể (mà chúng ta gọi là lỗi ý định). Bảng 2 cũng cho thấy phân phối của lỗi ý định và lỗi thực thi trong các bộ dữ liệu xếp hạng. Một quan sát thú vị là tỷ lệ lỗi thực thi giảm đối với các mô hình tạo mã lớn hơn.

Có thể chia nhỏ hơn nữa các điểm dữ liệu WRONG. Trong lớp lỗi thực thi, thông báo compiler cho chúng ta biết chính xác loại lỗi thực thi (như IndexError hoặc TypeError hoặc TimeOutError) và dòng trong chương trình gây ra lỗi này. Bằng cách phân tích thông báo lỗi từ compiler Python, chúng tôi tạo ra 10 lớp lỗi thực thi thường gặp nhất như được hiển thị trong Bảng 4 và Hình 2. Tương tự, đối với lớp lỗi ý định, chúng ta biết đầu ra được tạo ra khác với đầu ra mong đợi như thế nào (như kiểu sai hoặc độ dài sai của đầu ra mảng). Chúng tôi thiết kế thủ công 9 lớp lỗi ý định thường gặp nhất bằng cách xem xét các đầu ra mong đợi và được tạo ra khác nhau từ các ví dụ huấn luyện (xem Bảng 5 và Hình 3). Các nhãn dựa trên thực thi chi tiết này tạo thành bộ dữ liệu nhận biết lỗi của chúng tôi. Bảng 3 cho thấy một vài mục dữ liệu với tất cả các nhãn. Hình 6 trong Phụ lục cho thấy phân phối của các lớp lỗi thực thi và lỗi ý định khác nhau cho bộ dữ liệu xếp hạng thu được bằng mô hình Codex.

### 3.2 Các Tác vụ Xếp hạng Mã

Bây giờ chúng tôi mô tả các tác vụ phân loại khác nhau được tạo ra từ bộ dữ liệu trên. Đối với mỗi tác vụ phân loại, đầu vào là một cặp đặc tả tác vụ tạo mã G và chương trình được tạo ra Si và đầu ra là một hoặc nhiều nhãn trong đó mỗi nhãn thuộc tập lớp được xác định trước. Chúng tôi mô tả các nhãn/lớp khác nhau của các tác vụ khác nhau dưới đây. Các tác vụ này được thiết kế để khám phá sự đánh đổi giữa việc có các lớp thất bại trừu tượng so với việc có các lớp thất bại chi tiết.

**Nhị phân (B):** Đầu ra là một nhãn nhị phân duy nhất với hai lớp {CORRECT, WRONG}.

**Tam phân (T):** Tác vụ tam phân chia lớp WRONG thành các lớp lỗi ý định và lỗi thực thi: điều này tạo thành tác vụ phân loại ba lớp với nhãn đầu ra {CORRECT, lỗi ý định, lỗi thực thi}.

**Nhận biết Lỗi Ý định (I):** Tác vụ nhận biết lỗi ý định chia lớp lỗi ý định trong tác vụ tam phân thành 9 lớp con khác nhau của nó, do đó có tổng cộng 11 lớp cho nhãn đầu ra.

**Nhận biết Lỗi Thực thi (E):** Tác vụ nhận biết lỗi thực thi tương tự như tác vụ nhận biết lỗi ý định nhưng thay vì chia lớp lỗi ý định, chúng ta bây giờ chia lớp lỗi thực thi thành 10 lớp con khác nhau của nó, do đó có tổng cộng 12 lớp cho nhãn đầu ra.

**Nhận biết Lỗi Thực thi + Dòng Lỗi (E+L):** Đây là tác vụ phân loại đa lớp và đa nhãn kết hợp hai tác vụ phân loại. Tác vụ đầu tiên là tác vụ nhận biết lỗi thực thi được mô tả ở trên. Tác vụ thứ hai là dự đoán dòng của mã tương ứng với lỗi thực thi. Các nhãn cho số dòng lỗi cũng bao gồm -1 để biểu thị không có lỗi thực thi.

### 3.3 Các Mô hình Xếp hạng Mã

Chúng tôi triển khai các mô hình CODERANKER nhận biết lỗi bằng cách tinh chỉnh mô hình CodeBERT [21] được huấn luyện trước.

**CodeBERT:** Đây là mô hình hiểu mã phong cách BERT tiên tiến được huấn luyện trước trên bộ dữ liệu CodeSearchNet [26] sử dụng sự kết hợp các mục tiêu mô hình hóa ngôn ngữ có mặt nạ và phát hiện token thay thế [17]. Nó lấy đầu vào là sự nối của hai đoạn với token phân cách đặc biệt, cụ thể là [CLS]; w1; w2; ...; wn; [SEP]; c1; c2; ...; cm; [EOS]. Thường thì một đoạn là văn bản ngôn ngữ tự nhiên, và đoạn khác là mã. [CLS] là token đặc biệt, biểu diễn ẩn cuối cùng của nó có thể được coi như biểu diễn chuỗi tổng hợp cho các tác vụ phân loại hoặc xếp hạng downstream.

**Thêm đầu phân loại:** Chúng tôi thêm đầu phân loại lên trên mô hình CodeBERT cơ sở bằng cách kết nối lớp tuyến tính và lớp softmax với biểu diễn ẩn của token đặc biệt [CLS]. Gọi C ∈ R^H là vector ẩn cuối cùng tương ứng với token [CLS] và gọi W ∈ R^(K×H) là trọng số của lớp phân loại mới được thêm vào trong đó K là số lớp trong tác vụ phân loại. Các logit cho đầu ra phân loại được tính như softmax(CW^T) và chúng tôi sử dụng hàm mất cross-entropy tiêu chuẩn để tinh chỉnh tất cả các trọng số.

**Thêm đầu dự đoán dòng:** Để dự đoán dòng tương ứng với lỗi thực thi, chúng tôi giới thiệu vector dòng lỗi S ∈ R^H trong quá trình tinh chỉnh. Xác suất của token xuống dòng ("\n") thứ i là dòng có lỗi được tính như tích vô hướng giữa Ti và S theo sau bởi softmax trên tất cả các token xuống dòng trong mã, tức là Pi = e^(ST_i) / Σ_j e^(ST_j) trong đó Ti là vector ẩn cuối cùng tương ứng với token xuống dòng thứ i. Chúng tôi bao gồm token xuống dòng ở đầu và cuối đầu vào để chỉ ra trường hợp không có dòng có lỗi trong mã (tức là mã không dẫn đến lỗi thực thi) và để chỉ ra trường hợp dòng có lỗi vượt quá những gì được mã hóa trong đầu vào (điều này xảy ra nếu ngữ cảnh tác vụ+mã không thể vừa trong giới hạn 512 token của CodeBERT), tương ứng.

## 4 Đánh giá

Tiếp theo chúng tôi đánh giá phương pháp CODERANKER của chúng tôi. Chúng tôi điều tra (1) cách CODERANKER nhận biết lỗi có thể cải thiện các mô hình tạo mã khác nhau trên các bộ dữ liệu mã khác nhau, (2) tác động của các tác vụ Xếp hạng Mã khác nhau, và (3) tác động của việc trộn các bộ dữ liệu xếp hạng được tạo ra bởi các mô hình tạo mã khác nhau.

### 4.1 Thiết lập Thí nghiệm

**Bộ dữ liệu tạo mã:** Chúng tôi xem xét ba bộ dữ liệu tạo mã hiện có cho đánh giá của chúng tôi: (1) APPS [25]: bộ sưu tập 5000 tác vụ huấn luyện và 5000 tác vụ test được thu thập từ các cuộc thi lập trình và bài toán phỏng vấn, (2) HumanEval [11]: tập 164 tác vụ test, và (3) MBPP [3]: tập 974 tác vụ lập trình Python cơ bản với 474 bài toán huấn luyện và 500 bài toán test.

Trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng bộ dữ liệu APPS để tinh chỉnh các mô hình tạo mã và các mô hình CODERANKER (vì nó là bộ dữ liệu lớn nhất). Nhưng chúng tôi đánh giá các mô hình này trên cả ba tập tác vụ test. Bộ dữ liệu APPS không đi kèm với bộ dữ liệu validation, vì vậy chúng tôi sử dụng tập 600 tác vụ từ bộ dữ liệu huấn luyện gốc để validation; những tác vụ này sau đó được loại trừ khỏi bộ dữ liệu huấn luyện. Vì chúng tôi quan tâm đến việc đánh giá rõ ràng khả năng của bộ xếp hạng để phân biệt mã CORRECT khỏi mã WRONG, chúng tôi chọn tập validation của mình chỉ bao gồm các bài toán mà mô hình Codex (theo cách few-shot) có thể tạo ra ít nhất một chương trình chính xác trong 100 mẫu của nó. Để tạo điều kiện chuyển giao các mô hình GPT-J và GPT-Neo được tinh chỉnh trên các bộ dữ liệu HumanEval và MBPP, chúng tôi thực hiện biến đổi lập trình nhỏ của các mô tả tác vụ để khớp với phong cách APPS.

**Chỉ số:** Chúng tôi sử dụng các chỉ số pass@1, pass@5, exec@1, ranked pass@1, ranked pass@5, và ranked exec@1 (giá trị cao hơn là tốt hơn). Xem Phần 2 cho định nghĩa của chúng. Chúng tôi cũng cho thấy chỉ số pass@100 để minh họa giá trị tối đa có thể cho các chỉ số pass@k và ranked pass@k. Các chỉ số này được đo bằng bộ ước tính không thiên lệch từ 100 mẫu như được đề xuất bởi [11].

**Thiết lập huấn luyện và siêu tham số:** Chúng tôi tinh chỉnh các mô hình tạo mã GPT-J và GPT-Neo trên bộ dữ liệu huấn luyện APPS trong 2 epoch với kích thước batch 256 và tốc độ học 1e-5, và chọn checkpoint có validation loss thấp nhất. Để suy luận, chúng tôi sử dụng lấy mẫu nhiệt độ với T = 0.8 cho mô hình Codex và T = 0.9 cho các mô hình GPT-J và GPT-Neo trừ khi được chỉ định khác. Chúng tôi chọn những nhiệt độ này để tối đa hóa tính đa dạng trong 100 mẫu, nhưng chúng tôi cũng tiến hành ablation với nhiệt độ thấp hơn trong Bảng 11. Đối với mỗi chương trình, chúng tôi lấy mẫu 512 token mới và cắt ngắn chương trình được tạo ra bằng chuỗi dừng đặc biệt mà chúng tôi sử dụng trong các prompt few-shot/tinh chỉnh.

Chúng tôi tinh chỉnh các mô hình CODERANKER trong 30 epoch với kích thước batch 512 và tốc độ học 1e-4, và chọn checkpoint dẫn đến chỉ số ranked pass@1 tốt nhất trên bộ dữ liệu validation. Chúng tôi sử dụng trọng số lớp để cân bằng các lớp khác nhau trong khi huấn luyện các bộ xếp hạng. Tất cả thí nghiệm được tiến hành trên GPU V100-32GB.

**Ký hiệu:** Chúng tôi sử dụng ký hiệu R^Y_DX để biểu thị mô hình CODERANKER được huấn luyện trên bộ dữ liệu thu được bằng cách sử dụng mô hình tạo mã X từ Bảng 2 và trên một trong năm tác vụ xếp hạng Y từ Phần 3.2.

### 4.2 Kết quả Chính: CODERANKER cải thiện các mô hình tạo mã

**Bộ dữ liệu validation APPS:** Đầu tiên, chúng tôi phân tích kết quả trên bộ dữ liệu validation APPS của 600 tác vụ. Bảng 6 cho thấy hiệu suất trên các chỉ số khác nhau cho 4 mô hình tạo mã khác nhau. Những kết quả này sử dụng mô hình CODERANKER tốt nhất cho mỗi mô hình tạo mã được hiển thị trong Bảng 10. Từ Bảng 6, chúng tôi thấy rằng CODERANKER cải thiện tất cả các chỉ số cho tất cả các mô hình tạo mã bất chấp kích thước khác nhau của chúng. Hiệu suất pass@1 tăng từ 5.1% đến 13.6% với CODERANKER và các mô hình có thể giải quyết khoảng 30 đến 80 tác vụ nhiều hơn khi chỉ được chọn một chương trình từ 100 mẫu. Một quan sát thú vị khác là mô hình GPT Neo 125M khi kết hợp với CODERANKER (một mô hình 125M khác) đánh bại mô hình GPT-J (với số tham số nhiều gấp 50 lần). Những kết quả này cho thấy hiệu quả của CODERANKER trong việc cải thiện các mô hình tạo mã.

**Bộ dữ liệu test APPS:** Kết quả của chúng tôi trên bộ dữ liệu test APPS của 5000 tác vụ được hiển thị trong Bảng 7. Các bài toán test khó hơn những bài trên tập validation, điều mà chúng ta có thể thấy qua các con số pass@100 và pass@1 nhỏ hơn. Do đó, sự cải thiện từ CODERANKER nhỏ hơn về quy mô, nhưng vẫn là sự cải thiện đáng kể; pass@1 của Codex tăng từ 3.8% lên 4.7% (thêm 35 bài toán).

**Bộ dữ liệu HumanEval và MBPP:** Chúng tôi đo khả năng chuyển giao của CODERANKER trên hai bộ dữ liệu khác nhau–HumanEval (kết quả trong Bảng 8) và MBPP (kết quả trong Bảng 9). Chúng ta có thể thấy lại rằng CODERANKER cải thiện hiệu suất của tất cả các mô hình tạo mã trên tất cả các chỉ số cho cả hai bộ dữ liệu (một ngoại lệ là pass@5 cho GPT-J trên MBPP). Hiệu suất pass@1 của mô hình Codex tăng 6% trên cả hai bộ dữ liệu. Những kết quả này cho thấy khả năng chuyển giao out-of-distribution của CODERANKER và cũng cho thấy rằng các lỗi được thực hiện bởi các mô hình tạo mã trên các tác vụ khác nhau là phổ quát.

**Exec@1 so với pass@1:** Trong tất cả các kết quả trên, các cải thiện trong chỉ số exec@1 cao hơn các cải thiện trong chỉ số pass@1; điều này cho thấy CODERANKER tốt hơn trong việc xác định lỗi thực thi trong mã hơn lỗi ý định, điều này được mong đợi vì thực thi mã được chỉ ra là một nhiệm vụ khó khăn vốn có đối với các mô hình ngôn ngữ [3, 34].

### 4.3 Ablation

**Tác động của nhiệt độ lấy mẫu:** Trên các bộ dữ liệu HumanEval và MBPP, chúng tôi cũng thí nghiệm với các nhiệt độ khác nhau để lấy mẫu 100 chương trình (xem Bảng 11). Như mong đợi, chúng tôi nhận thấy rằng pass@100 giảm với nhiệt độ thấp hơn, nhưng pass@1 tăng. Chúng tôi thấy rằng CODERANKER tiếp tục tăng hiệu suất pass@1 cho 3 trong 4 thiết lập và đạt 42.7% ranked pass@1 trên bộ dữ liệu HumanEval—kết quả tốt nhất được biết đến cho đến nay trên bộ dữ liệu này [16]. Trên bộ dữ liệu MBPP, dưới thiết lập lấy mẫu nhiệt độ thấp, chúng tôi thấy rằng CODERANKER giảm nhẹ hiệu suất pass@1—điều này chúng tôi cho là do sự khác biệt nhỏ giữa các chỉ số pass@1 và pass@100, điều này làm cho bộ xếp hạng đã học khó đánh bại sơ đồ xếp hạng ngẫu nhiên.

**Phân tích các tác vụ xếp hạng khác nhau:** Hình 4 cho thấy 4 đường cong huấn luyện; một cho mỗi mô hình tạo mã, X, hiển thị các đường cong ranked pass@1 validation cho mô hình X khi kết hợp với 5 bộ xếp hạng khác nhau R^Y_DX. Kết quả ở định dạng bảng có thể được tìm thấy trong Phụ lục. Từ các đường cong, chúng ta có thể nhận thấy rằng đối với các mô hình lớn hơn như Codex và GPT-J, các bộ xếp hạng được huấn luyện trên tác vụ phân loại tam phân R^T hoạt động tốt nhất, và đối với các mô hình nhỏ hơn như GPT-Neo 1.3B và GPT-Neo 125M, bộ xếp hạng được huấn luyện trên tác vụ phân loại nhận biết ý định R^I và bộ xếp hạng được huấn luyện trên tác vụ phân loại nhận biết lỗi thực thi + dòng lỗi R^{E+L} hoạt động tốt nhất, tương ứng. Chúng ta cũng có thể nhận thấy rằng các bộ xếp hạng nhị phân R^B hoạt động kém hơn đáng kể đặc biệt với các mô hình nhỏ hơn. Những kết quả này cho thấy rằng khi huấn luyện bộ xếp hạng trên bộ dữ liệu có nhiều mã WRONG hơn, các tác vụ phân loại khó hơn hoạt động như bộ điều chuẩn tốt hơn. Hình 7 trong Phụ lục cho thấy các đường cong huấn luyện tương tự cho chỉ số ranked exec@1; ở đây, chúng ta có thể thấy rằng R^E và R^{E+L} luôn đạt ranked exec@1 tốt nhất vì chúng được huấn luyện để xác định lỗi thực thi và R^B và R^I có hiệu suất tệ nhất.

**Phân tích các bộ dữ liệu xếp hạng khác nhau:** Trong thí nghiệm này, chúng tôi đo tác động của việc sử dụng các bộ xếp hạng được huấn luyện trên dữ liệu từ một mô hình tạo mã trên mô hình tạo mã khác và tác động của việc trộn các bộ dữ liệu xếp hạng này. Chúng tôi có 4 mô hình tạo mã khác nhau trong thí nghiệm này và do đó, 4 bộ dữ liệu xếp hạng khác nhau. Chúng tôi phân tích hai bộ dữ liệu trộn khác nhau—(i) mixed-small ngẫu nhiên lấy mẫu 25% của các bộ dữ liệu xếp hạng trên và kết hợp chúng, và (ii) mixed-large kết hợp tất cả 4 bộ dữ liệu xếp hạng thành một; bộ dữ liệu trước đại diện cho bộ dữ liệu có kích thước xấp xỉ bằng kích thước của các bộ dữ liệu xếp hạng riêng lẻ để so sánh công bằng, trong khi bộ dữ liệu sau sử dụng tất cả dữ liệu có sẵn. Hình 5 cho thấy ranked pass@1 của tất cả các kết hợp 4x6 dưới dạng bản đồ nhiệt. Mỗi cột đã được chuẩn hóa sao cho các giá trị cho cùng một mô hình là 1 (tức là các giá trị đường chéo là 1). Từ hình, trong số các bộ dữ liệu xếp hạng riêng lẻ, chúng ta nhận thấy rằng các bộ xếp hạng được huấn luyện với dữ liệu từ cùng mô hình tạo mã là tốt nhất và các bộ xếp hạng được huấn luyện với dữ liệu từ các mô hình tạo mã có sự khác biệt kích thước lớn là tệ nhất. Cuối cùng, các bộ xếp hạng được huấn luyện trên bộ dữ liệu mixed-small hoạt động hơi kém hơn (trừ mô hình tạo mã GPT-J) so với việc sử dụng bộ dữ liệu mô hình giống nhau, nhưng các bộ xếp hạng được huấn luyện trên bộ dữ liệu mixed-large có hiệu suất tổng thể tốt nhất. Những kết quả này cho thấy rằng trong khi thường tốt hơn khi sử dụng cùng mô hình để tạo ra bộ dữ liệu xếp hạng, chúng ta có thể sử dụng các mô hình nhỏ hơn (các mô hình ít tốn kém hơn) để bổ sung bộ dữ liệu xếp hạng để cải thiện hiệu suất hơn nữa.

Phân tích định tính/định lượng bổ sung về phương pháp CODERANKER có thể được tìm thấy trong Phụ lục.

## 5 Các Công trình Liên quan

**Các tác vụ/mô hình tạo mã:** Có một số mô hình tạo mã được khám phá trong tài liệu, từ kiến trúc chỉ-decoder [11,3,39,6,22] đến kiến trúc encoder-decoder [28,33,2,38] với các kích thước khác nhau. Tương tự, có một số bộ dữ liệu tác vụ từ nhiều lĩnh vực, bao gồm các bài toán từ toán học [3,18], tạo cell notebook Jupyter [10], các tác vụ lập trình thông thường [3,11,35] và các bài toán lập trình cấp độ thi đấu [28,25]. Trong bài báo này, chúng tôi đánh giá khả năng của CODERANKER để cải thiện hiệu suất của bốn mô hình tạo mã dựa trên decoder trên ba bộ dữ liệu lập trình. Phương pháp của chúng tôi không phụ thuộc vào kiến trúc mô hình tạo mã miễn là chúng tạo ra đầu ra bằng lấy mẫu và phương pháp của chúng tôi có thể được mở rộng dễ dàng sang các lĩnh vực khác nhau.

**Các tác vụ/mô hình hiểu mã:** Bên cạnh các tác vụ tạo mã, có một số tác vụ hiểu mã bao gồm tìm kiếm mã [4,8,23,26], phát hiện clone [37,30], tóm tắt mã [26], dịch mã [13,27,32] và phát hiện lỗi [9,5,15,40,5]. Các mô hình hiểu mã chỉ-encoder [29,21,2,38,24] được phát triển cho các tác vụ này. Tác vụ CODERANKER của chúng tôi có thể được coi như một tác vụ hiểu mã mới, và mô hình xếp hạng của chúng tôi được tinh chỉnh từ CodeBERT [21]. Các bộ dữ liệu phát hiện lỗi liên quan chặt chẽ đến của chúng tôi; sự khác biệt chính là công trình trước tập trung vào việc tìm lỗ hổng trong mã được viết bởi con người, trong khi chúng tôi tập trung vào việc phát hiện lỗi trong mã được tạo ra bởi mô hình.

**Lọc/xếp hạng cho các mô hình tạo mã:** Các công trình trước như [11,28] sử dụng thực thi để loại bỏ các hoàn thành mã trong quá trình suy luận. Trong khi [11] chỉ sử dụng các bài kiểm tra đơn vị được cung cấp như một phần của tác vụ, [28] cũng sử dụng mô hình thần kinh để tạo ra đầu vào cho các bài kiểm tra đơn vị và sử dụng thực thi để lọc ra các chương trình tạo ra cùng đầu ra trên những đầu vào đó. Tuy nhiên, các công trình này yêu cầu thực thi mã có thể dễ bị tấn công cho mọi tác vụ suy luận. Phương pháp của chúng tôi bỏ qua thực thi tại thời gian suy luận với bộ xếp hạng thần kinh. Tương tự như công trình của chúng tôi, [18,36] đề xuất bộ xếp hạng/xác minh dựa trên mạng nơ-ron cho các mô hình tạo mã; sự khác biệt chính là lĩnh vực (các tác vụ lập trình đa năng trong trường hợp của chúng tôi so với các bài toán toán học trong công trình trước) và chúng tôi huấn luyện các bộ xếp hạng thần kinh của mình để học tại sao/cách mã thất bại thay vì chỉ dự đoán nhãn nhị phân. Hơn nữa, [18,36] sử dụng các mô hình sinh như cơ sở xếp hạng của họ; trong công trình của chúng tôi, chúng tôi cho thấy lợi ích của các bộ xếp hạng sử dụng mô hình chỉ-encoder đơn giản với chỉ 125M tham số ([18] sử dụng các mô hình với ít nhất 3B tham số) (xem Phần A.2.3 trong Phụ lục để xem ablation của chúng tôi về các kiến trúc xếp hạng khác nhau).

**Sử dụng thực thi/phân tích tĩnh trong các ngữ cảnh khác:** Đã có các công trình khác về việc sử dụng thực thi để hướng dẫn tạo mã bằng cách điều kiện hóa việc tạo ra trên biểu diễn của các trạng thái chương trình [12,20]. Cũng có những nỗ lực thay thế quá trình thực thi bằng mô hình thần kinh trong những trường hợp này [14,34]. Một công trình liên quan khác là [31], cải thiện các mô hình tạo mã bằng cách sử dụng phân tích tĩnh để bổ sung đầu vào của mô hình và đã chỉ ra làm giảm drastically số lượng lỗi thực thi được thực hiện bởi các chương trình được tạo ra. Bài báo này đưa ra phương pháp thay thế bằng cách sử dụng mô hình xếp hạng thần kinh để loại bỏ các chương trình sai tại thời gian suy luận.

## 6 Kết luận và Hướng Phát triển Tương lai

Chúng tôi đã trình bày CODERANKER xếp hạng các chương trình được tạo ra bởi mô hình tạo mã mà không cần thực thi rõ ràng các chương trình. Các bộ xếp hạng của chúng tôi có khả năng nhận biết lỗi tức là được huấn luyện để dự đoán các lớp chi tiết của các chế độ thất bại và chúng tôi đã chỉ ra hiệu quả của các bộ xếp hạng nhận biết lỗi trong việc cải thiện các chỉ số pass@k và exec@k cho các mô hình tạo mã và tác vụ khác nhau.

Một trong những hạn chế chính là phương pháp CODERANKER không đúng, tức là bộ xếp hạng có thể phân loại chương trình chính xác là sai và ngược lại. Ngoài ra, chúng tôi phải chịu thêm thời gian suy luận để có hiệu suất tốt hơn, vì bây giờ chúng tôi phải tạo ra n >> k chương trình để lọc k chương trình để hiển thị cho người dùng. Phương pháp hiện tại của chúng tôi cũng dựa vào việc lấy mẫu toàn bộ chương trình từ mô hình tạo mã trước khi sử dụng CODERANKER. Trong tương lai, chúng tôi muốn điều tra việc xếp hạng/phân loại các chương trình một phần, điều này có thể giảm thời gian suy luận bằng cách loại bỏ các chương trình sai sớm. Một hướng tương lai khác cho công trình của chúng tôi là điều tra các kiến trúc mô hình xếp hạng khác như những kiến trúc có thể tận dụng cấu trúc mã tốt hơn và khám phá các tác vụ xếp hạng khác như tạo ra thông báo lỗi đầy đủ. Cuối cùng, sẽ thú vị khi điều tra khả năng chuyển giao của phương pháp CODERANKER của chúng tôi sang các tác vụ lập trình tổng quát (thay vì chỉ lập trình thi đấu). Một thách thức chính ở đây là thiếu bộ dữ liệu lập trình tổng quát cần được thu thập bằng cách thu thập từ các nguồn công cộng như GitHub. Các thí nghiệm sơ bộ của chúng tôi trên bộ dữ liệu lập trình tổng quát như vậy cho thấy rằng các mô hình tạo mã hiện có thường tạo ra nhiều chương trình có lỗi thực thi hơn là các chương trình có lỗi ý định. Quan sát này kết hợp với kết quả của chúng tôi trong bài báo này cho thấy CODERANKER thường tốt hơn trong việc xác định lỗi thực thi hơn lỗi ý định, chúng tôi rất lạc quan rằng phương pháp CODERANKER nhận biết lỗi cũng sẽ cải thiện việc tạo mã cho các tác vụ lập trình tổng quát.

**Lời cảm ơn:** Chúng tôi cảm ơn Todd Mytkowicz, Piali Choudhury, Rahee Gosh Peshwaria, Curtis von Veh, và Xiaodong Liu vì những thảo luận hữu ích về công trình này.
