# 2203.13474.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2203.13474.pdf
# File size: 747283 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2023
CODEGEN: ANOPEN LARGE LANGUAGE MODEL FOR
CODE WITH MULTI -TURN PROGRAM SYNTHESIS
Erik Nijkamp, Bo Pang, Hiroaki Hayashi,
Lifu Tu ,Huan Wang ,Yingbo Zhou ,Silvio Savarese ,Caiming Xiong
Salesforce Research
ABSTRACT
Program synthesis strives to generate a computer program as a solution to a given
problem speciﬁcation, expressed with input-output examples or natural language
descriptions. The prevalence of large language models advances the state-of-the-art
for program synthesis, though limited training resources and data impede open
access to such models. To democratize this, we train and release a family of large
language models up to 16.1B parameters, called CODEGEN, on natural language
and programming language data, and open source the training library JAX FORMER .
We show the utility of the trained model by demonstrating that it is competitive with
the previous state-of-the-art on zero-shot Python code generation on HumanEval.
We further investigate the multi-step paradigm for program synthesis, where a single
program is factorized into multiple prompts specifying subproblems. To this end,
we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB),
consisting of 115 diverse problem sets that are factorized into multi-turn prompts.
Our analysis on MTPB shows that the same intent provided to CODEGENin multi-
turn fashion signiﬁcantly improves program synthesis over that provided as a single
turn. We make the training library JAX FORMER and model checkpoints available
as open source contribution: https://github.com/salesforce/CodeGen .
1 I NTRODUCTION
Creating a program has typically involved a human entering code by hand. The goal of program
synthesis is to automate the coding process, and generate a computer program that satisﬁes the user’s
speciﬁed intent. Some have called it the holy grail of computer science (Manna & Waldinger, 1971;
Gulwani et al., 2017). Successful program synthesis would not only improve the productivity of
experienced programmers but also make programming accessible to a wider audience.
Two key challenges arise when striving to achieve program synthesis: (1) the intractability of the
search space, and (2) the difﬁculty of properly specifying user intent. To maintain an expressive search
space, one needs a large search space, which poses challenges in efﬁcient search. Previous work
(Joshi et al., 2002; Panchekha et al., 2015; Cheung et al., 2013) leverages domain-speciﬁc language
to restrict the search space; however, this limits the applicability of synthesized programs. On the
contrary, while being widely applicable, general-purpose programming languages ( e.g., C, Python)
introduce an even larger search space for possible programs. To navigate through the enormous
program space, we formulate the task as language modeling, learning a conditional distribution of the
next token given preceding tokens and leverage transformers (Vaswani et al., 2017) and large-scale
self-supervised pre-training. This approach has seen success across modalities (Devlin et al., 2019;
Lewis et al., 2020; Dosovitskiy et al., 2021). Likewise, prior works have developed pre-trained
language models for programming language understanding (Kanade et al., 2020; Feng et al., 2020).
To realize program synthesis successfully, users must employ some means to communicate their
intent to the models such as a logical expression (which speciﬁes a logical relation between inputs
Equal contribution.
Correspondence to: Erik Nijkamp (erik.nijkamp@salesforce.com), Bo Pang (b.pang@salesforce.com),
Hiroaki Hayashi (hiroakihayashi@salesforce.com), Yingbo Zhou (yingbo.zhou@salesforce.com), Caiming
Xiong (cxiong@salesforce.com).
1arXiv:2203.13474v5  [cs.LG]  27 Feb 2023

--- PAGE 2 ---
Published as a conference paper at ICLR 2023
and outputs of a program), pseudo-code, input-output examples, or a verbalized speciﬁcations in
natural language. On the one hand, a complete formal speciﬁcation enjoys the exact speciﬁcations
of user intent but may require domain expertise and effort from users to translate the intent to such
a form. On the other hand, speciﬁcation merely based on input-output examples is less costly but
may under-specify the intent, leading to inaccurate solutions. Previous work has beneﬁted from
various methods and their combinations as the input to program synthesis models, including pseudo-
code (Kulal et al., 2019), a part of a program and its documentation (Chen et al., 2021), or natural
language paragraph with input-output examples (Hendrycks et al., 2021). However, we argue that a
truly user-friendly form of intent is natural language text.
To overcome these challenges, we propose a multi-turn program synthesis approach, where a user
communicates with the synthesis system by progressively providing speciﬁcations in natural language
while receiving responses from the system in the form of synthesized subprograms, such that the user
together with the system complete the program in multiple steps. The following two considerations
motivate this approach.
First, we speculate that factorizing a potentially long and complicated speciﬁcation into multiple steps
would ease the understanding by a model and hence enhance program synthesis. In the multi-turn
approach, a model can focus on the speciﬁcation associated with one subprogram and avoid arduously
tracking the complicated dependency among subprograms. This effectively reduces the search space
besides the convenience of specifying user intent. Indeed, our speculations are conﬁrmed in our
experiments with higher quality synthesized programs through the multi-turn approach.
Second, code exhibits a weak pattern of interleaved natural and programming language, which may
be exploitable. Such a pattern is formed by programmers who explain the functionality of a program
with comments. With the language modeling objective, we hypothesize that the interleaving pattern
provides a supervision signal for the model to generate programs given natural language descriptions
over multiple turns. The signal is highly noisy or weak, because only a subset of data would exhibit
such a pattern, comments may be inaccurate or uninformative, and some of them may even be placed
at an irrelevant position. However, up-scaling the model and data size might overcome such weak
supervision, allowing the model to develop multi-turn program synthesis capacity. This enables user
intent to be expressed in multiple turns, that is, the intent can be decomposed and fulﬁlled part by
part while each turn can easily be expressed in natural language.
In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for
multi-turn program synthesis. To solve a problem in the benchmark, a model needs to synthesize
a program in multiple steps with a user who speciﬁes the intent in each turn in natural language.
Please refer to Figure 1 for an example where the model synthesizes a program to extract the user
name of an email address. Performance on the benchmark is measured by pass rate on expert-written
test cases. To the best of our knowledge, this is the ﬁrst multi-turn program synthesis benchmark,
which allows quantitative analysis of multi-turn program synthesis. With the emergence of multi-turn
program synthesis capacity in large language models that beneﬁts problem-solving, we believe this
benchmark will foster future research in program synthesis.
Our Contributions Our work shares the basic idea of adopting language models for program
synthesis with the recent and concurrent efforts (Chen et al., 2021; Austin et al., 2021; Li et al., 2022)
with a single-turn user intent speciﬁcation. In addition, we contribute with respect to four aspects:
• We study multi-turn program synthesis emerging in autoregressive models under scaling laws.
• We leverage this capacity to introduce a multi-turn program synthesis paradigm.
• We investigate its properties quantitatively with a novel multi-turn programming benchmark.1
• We will open source model checkpoints2and the custom training library: JAX FORMER .3
For program synthesis, no large-scale models competitive with Codex are available as open-source.
This hinders progress, given that the expensive compute resources required to train these models are
only accessible to a limited number of institutions. Our open source contribution allows a wide range
of researchers to study and advance these models, which may greatly facilitate research progress.
1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark
2Checkpoints: https://github.com/salesforce/CodeGen
3Training: https://github.com/salesforce/jaxformer
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2023
2 M ODEL TRAINING
To evaluate the emergence of multi-turn programming capabilities under scaling laws, we adopt stan-
dard transformer-based autoregressive language models, varying (1) the number of model parameters
(350M, 2.7B, 6.1B, 16.1B) and (2) the number of tokens of programming languages in the training
corpora. For scaling the training, a custom library JAX FORMER for TPU-v4 hardware was developed
and will be released as open-source, including the trained model weights.
2.1 D ATASETS
The family of CODEGENmodels is trained sequentially on three datasets: THEPILE,BIGQUERY ,
and B IGPYTHON .
The natural language dataset THEPILEis an 825:18GiB English text corpus collected by Gao et al.
(2020) for language modeling (MIT license). The dataset is constructed from 22 diverse high-quality
subsets, one of which is programming language data collected from GitHub repositories with >100
stars that constitute 7.6% of the dataset. Since the majority of THEPILEis English text, the resulting
models are called as natural language C ODEGENmodels (C ODEGEN-NL).
The multi-lingual dataset BIGQUERY is a subset of Google’s publicly available BigQuery dataset,
which consists of code (under open-source license) in multiple programming languages. For the multi-
lingual training, the following 6 programming languages are chosen: C, C++, Go, Java, JavaScript,
and Python. Thus, we refer to models trained on the BIGQUERY as multi-lingual CODEGENmodels
(CODEGEN-MULTI ).
The mono-lingual dataset BIGPYTHON contains a large amount of data in the programming language,
Python. We have compiled public, non-personal information from GitHub consisting of permissively
licensed Python code in October 2021. Consequently, we refer to models trained on BIGPYTHON as
mono-lingual C ODEGENmodels (C ODEGEN-MONO).
The pre-processing follows: (1) ﬁltering, (2) deduplication, (3) tokenization, (4) shufﬂing, and
(5) concatenation. For details on THEPILE, we refer to Gao et al. (2020). For BIGQUERY and
BIGPYTHON , we refer to Appendix A. Table 5 summarizes the statistics of the training corpora.
2.2 M ODELS
TheCODEGENmodels are in the form of autoregressive transformers with next-token prediction
language modeling as the learning objective trained on a natural language corpus and programming
language data curated from GitHub. The models are trained in various sizes with 350M, 2.7B, 6.1B,
and 16.1B parameters. The ﬁrst three conﬁgurations allow for direct comparison with open-sourced
large language models trained on text corpus, GPT-N EO(350M, 2.7B) (Black et al., 2021) and
GPT-J (6B) (Wang & Komatsuzaki, 2021). See Table 6 in Appendix A for model speciﬁcations.
TheCODEGENmodels are trained in a sequential nature over datasets. CODEGEN-NL is ﬁrst trained
onTHEPILE.CODEGEN-MULTI is initialized from CODEGEN-NL and trained on BIGQUERY .
Finally C ODEGEN-MONO is initialized from C ODEGEN-MULTI and trained on B IGPYTHON .
The emergence of program synthesis conditional on descriptions in natural language may stem from
the size of the models and data, training objective, and nature of the training data itself. This is called
emergence since we do not explicitly train the model on comment-code pairs. Similar phenomena are
observed in a wide range of natural language tasks where a large-scale unsupervised language model
can solve unseen tasks in a zero-shot fashion (Brown et al., 2020). The emergence phenomena or
surprising zero-shot generalization is often attributed to the large scale of the model and the data.
While our focus is not to reveal the underlying mechanism on why program synthesis capacity
emerges from simple language modeling, we make an attempt to provide an explanation given the
nature of our modeling approach and the training data. The data consists of regular code from
GitHub (without manual selection), for which some data exhibits a pattern of interleaved natural
and programming language, which we believe provides a noisy supervision signal for the program
synthesis capacity due to the next-token prediction training objective. However, we emphasize that
such a data pattern is highly noisy and weak, because only a subset of data exhibits such a pattern, e.g.,
comments may be inaccurate or uninformative, and some of them may even be placed at an irrelevant
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2023
Modelpass@ k[%]
k= 1 k= 10 k= 100
GPT-N EO350M 0.85 2.55 5.95
GPT-N EO2.7B 6.41 11.27 21.37
GPT-J 6B 11.62 15.74 27.74
CODEX 300M 13.17 20.37 36.27
CODEX 2.5B 21.36 35.42 59.50
CODEX 12B 28.81 46.81 72.31
code-cushman-00133.5 54.3 77.4
code-davinci-00139.0 60.6 84.1
code-davinci-00247.0 74.9 92.1
CODEGEN-NL 350M 2.12 4.10 7.38
CODEGEN-NL 2.7B 6.70 14.15 22.84
CODEGEN-NL 6.1B 10.43 18.36 29.85
CODEGEN-NL 16.1B 14.24 23.46 38.33
CODEGEN-MULTI 350M 6.67 10.61 16.84
CODEGEN-MULTI 2.7B 14.51 24.67 38.56
CODEGEN-MULTI 6.1B 18.16 28.71 44.85
CODEGEN-MULTI 16.1B 18.32 32.07 50.80
CODEGEN-MONO 350M 12.76 23.11 35.19
CODEGEN-MONO 2.7B 23.70 36.64 57.01
CODEGEN-MONO 6.1B 26.13 42.29 65.82
CODEGEN-MONO 16.1B 29.28 49.86 75.00
Table 1: Evaluation results on the HumanEval benchmark. Each pass@ k(wherek2f1;10;100g)
for each model is computed with three sampling temperatures ( t2f0:2;0:6;0:8}) and the highest
one among the three are displayed, which follows the evaluation procedure in Chen et al. (2021).
Results for the model marked withare from Chen et al. (2022).
position. Therefore, we believe two main factors contribute to the program synthesis capacity: 1)
large scale of model size and data size and 2) noisy signal in training data.
The scaling of such LLMs requires data and model parallelism. To address these requirements, a
training library JAX FORMER (https://github.com/salesforce/jaxformer ) was developed for
efﬁcient training on Google’s TPU-v4 hardware. We refer to Appendix A for further details on the
technical implementation and sharding schemes. Table 6 summarizes the hyper-parameters.
3 S INGLE -TURN EVALUATION
We ﬁrst evaluate our CODEGENusing an existing program synthesis benchmark: HumanEval (MIT
license) (Chen et al., 2021). HumanEval contains 164hand-written Python programming problems.
Each problem provides a prompt with descriptions of the function to be generated, function signature,
and example test cases in the form of assertions. The model needs to complete a function given the
prompt such that it can pass all provided test cases, thus measuring the performance by functional
correctness. Since a user intent is speciﬁed in a single prompt and provided to the model once, we
regard the evaluation on HumanEval as a single-turn evaluation, to distinguish it from the multi-turn
evaluation which we introduce in the next section. Following Chen et al. (2021), we recruit nucleus
sampling (Holtzman et al., 2020) with top- pwherep= 0:95.
3.1 HUMAN EVAL PERFORMANCE SCALES AS A FUNCTION OF MODEL SIZE AND DATA SIZE
We compare our models to the Codex models (Chen et al., 2021), which demonstrate the state-of-
the-art performance on HumanEval. Moreover, our models are compared to open-sourced large
language models, GPT-N EO(Black et al., 2021) and GPT-J (Wang & Komatsuzaki, 2021). These
are trained on T HEPILE(Gao et al., 2020), and thus similar to our C ODEGEN-NL models, in terms
of training data and model size. All models are evaluated with temperature t2f0:2;0:6;0:8g, and
we compute pass@ kwherek2f1;10;100gfor each model. For direct comparison to the results
by Chen et al. (2021), we choose the temperature that yields the best-performing pass@ kfor each
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2023
CODEGEN-MONO 350M 2.7B 6.1B 16.1B
Pass 3:780:23 3 :660:14 3 :350:13 3 :120:11
Non-Pass 5:180:19 4 :370:18 3 :880:13 3 :400:11
Table 2: Average prompt perplexity#(standard error) of CODEGEN-MONO models on pass and
non-pass problems.
k. The results of our models and baselines are summarized in Table 1. Our CODEGEN-NL models
(350M, 2.7B, 6.1B) outperform or perform on par with the respective GPT-N EOandGPT-J models.
Further training CODEGEN-NL on multilingual programming language data ( BIGQUERY ) leads to
CODEGEN-MULTI . The multilingual CODEGENmodels outperform the models trained on THEPILE
(GPT-N EO,GPT-J ,CODEGEN-NL) by a large margin. We then ﬁnetune CODEGEN-MULTI on a
Python-only dataset ( BIGPYTHON ), resulting in CODEGEN-MONO. The program synthesis capacity
is improved substantially. Therefore, the Python program synthesis capacity enhances as the amount
of Python training data increases. For almost all models, as expected, increasing the size of the model
improves overall performance.
Our Python-monolingual CODEGENmodels have competitive or improved performance, compared
to the current state-of-the-art models, Codex. CODEGEN-MONO 2.7B underperforms CODEX 2.5B
whenk= 100 but outperforms it when k2f1;10g. While it is only half the size, our CODEGEN-
MONO 6.1B demonstrates pass@k scores approaching those of the best-performing Codex, CODEX
12B. Our largest model C ODEGEN-MONO 16.1B is competitive or outperforms it depending on k.
3.2 B ETTER USERINTENT UNDERSTANDING YIELDS BETTER SYNTHESIZED PROGRAMS
The success of a program synthesis system highly depends on how well it understands user intent.
When the system is based on a language model, the perplexity of problem prompts provides a proxy
for the system’s understanding of user intent speciﬁcations. A low perplexity of an intent speciﬁcation
under a model indicates that this intent speciﬁcation is compatible with the knowledge learned by
the model from the training data. We investigate whether better prompt understanding, with lower
prompt perplexity as a proxy, leads to more functionally accurate programs.
We partition all problems into pass versus non-pass ones. A pass problem is one that at least one
sample from 200 samples passes all test cases, while for a non-pass problem none of the 200 samples
pass all test cases. We compute the average perplexity of the problem prompts of the pass problems
and that of the non-pass ones, based on samples from CODEGEN-MONO models. The results are
displayed in Table 2 (see Appendix F for the results on CODEGEN-NL andCODEGEN-MULTI ). The
prompts of the pass problems have lower perplexity than those of the non-pass ones. This ﬁnding
implies that program synthesis is more likely to be successful when the user intent speciﬁcation is
understood better by the model. Indeed, some training data contains interleaved sequences of natural
language comments and programs, where the comments describe the functionality of the following
program. We thus speculate that user intent speciﬁcations similar to such a pattern would be better
understood by the model, and hence lead to better program synthesis. Inspired by this pattern, we
propose to specify user intent in multiple turns such that the model focus on a partial problem at a
time, which would make user intent understanding by the model easier.
4 M ULTI -TURN EVALUATION
In this section, we propose and study a multi-step program synthesis paradigm where program
synthesis is decomposed into multiple steps and the system synthesizes a subprogram in each step. To
examine such a paradigm, we ﬁrst develop a Multi-Turn Programming Benchmark (MTPB). MTPB
consists of 115problems written by experts, each of which includes a multi-step descriptions in
natural language ( prompt ). To solve a problem, a model needs to synthesize functionally correct
subprograms (1) following the description at the current step and (2) considering descriptions and
synthesized subprograms at previous steps ( e.g., correct backreference of functions and/or variables
deﬁned in the previous steps). An illustrative example is shown in Figure 1.
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2023
Sample
ConcatenateTurn 1Turn 2Turn 3Turn 4Turn 5
“abc xyz”Execute# Import re and define a regular expression that matches an …import reemail_regex = re.compile(    "([a-zA-Z0-9_\-\.]+)@([a-zA-Z0-9_\-\.]+)\.([a-zA-Z]{2,5})")# Search for an email address in “… abc.xyz@example.com …” and …address = email_regex.search(“… abc.xyz@example.com …”)# Remove the substring starting from the @ symbol from "address".address = address.group(0)address = address[:address.find("@")]# Replace non-alphabetical symbols with a whitespace in "address".address = re.sub("[^a-zA-Z]+", " ", address)# Print out "address".print(address)Generation
Actual OutputEvaluation2
3
“abc xyz”Discourse
Expected OutputInputHumanImport re and deﬁne a regular expression that matches an email address.import reemail_regex = re.compile("[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+")Model
Modeladdress = email_regex.search(“… abc.xyz@example.com …”)Search for an email address in “{input}” and store the ﬁrst match to a variable "address".HumanRemove the substring starting from the @ symbol from "address".HumanReplace non-alphabetical symbols with a whitespace in "address".Humanaddress = address.group(0)address = address[:address.find("@")]Model
Modeladdress = re.sub("[^a-zA-Z]+", " ", address)Print out "address".HumanModelprint(address)1“… abc.xyz@example.com …”
✓
Figure 1: An illustrative example for the Multi-Turn Programming Benchmark, performing the task of
extracting the user name of an email address. 1Each problem consists of prompts piand unit tests,
where some prompts include templates ( i.e.{input} ) that are ﬁlled with test case inputs before it is
fed to the model. In the displayed example, the input is a string containing abc.xyz@example.com ,
which replaces {input} inp2, and the expected output is abc xyz .2Our model conditions on the
concatenation of interleaved past prompts and generated responses .3Generated responses from
each turn are concatenated and executed, where the output is compared to the answer.
4.1 B ENCHMARK CONSTRUCTION
We (4 authors) start by deﬁning4a set of 115problems requiring a diverse range of programming
knowledge, including math, array operations, string manipulations, algorithms, data science, and
problems that require other knowledge, such that the number of problems in each category is roughly
balanced.5For each problem, we construct a triplet consisting of multi-turn prompts P, test case
inputsI, and test case outputs O. Multi-turn prompts Pare designed following the two constraints:
(1) the problem is decomposed into 3 or more turns, (2) a single turn cannot be attributed to solving
the problem. For example, implementing a linear regression model could be phrased as “Perform
linear regression on x and y”. Since the main task is fully expressed in this prompt, understanding
this prompt is sufﬁcient to perform the task. We avoid such cases via manual inspection and distribute
problem-solving over turns. Together with the prompts, we task the problem author to prepare 5 sets
of test case inputs Iand outputsOto evaluate model outputs with functional correctness. To reduce
wrongly rewarding false positive solutions that give meaningless programs but pass the tests, we
examine and revise such cases to ensure the test quality.
Unlike HumanEval for which models are expected to complete a partially deﬁned function, MTPB
problems only provide the prompts, thereby models have to generate the solution from scratch.6
While the free-form generation may allow for more potential solutions, the lack of an entry point
to provide test case inputs makes it challenging to test the generated code on diverse test cases. To
overcome this challenge, we instead embed test case inputs within prompts. Speciﬁcally, prompts
are written with Python’s formatted string7where input values are substituted for the variable name
when a speciﬁc test case is applied to the problem. For example, a prompt, “Deﬁne a string named ‘s’
4Problem writing was performed in a closed book format, i.e.we are not allowed to consult with online
resources while writing the problems.
5See Appendix D for a complete listing.
6To guide sampling in Python, we preﬁx the prompt with: /numbersign.tosf Import libraries.\n import numpy as np .
7https://docs.python.org//three.tosf/reference/lexical_analysis.html/numbersign.tosff-strings
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2023
Data ModelPass Rate"[%]
350M 2.7B 6.1B 16.1B -
THEPILE GPT-N EO& GPT-J 0.79 8.17 18.86 - -
THEPILE CODEGEN-NL 0.23 15.31 19.37 30.33 -
BIGQUERY CODEGEN-MULTI 4.09 20.82 25.51 26.27 -
BIGPYTHON CODEGEN-MONO 16.98 38.72 43.52 47.34 -
- code-cushman-001 - - - - 56.77
- code-davinci-001 - - - - 55.28
- code-davinci-002 - - - - 59.86
Table 3: Evaluation results on the Multi-Turn Programming Benchmark. The multi-turn program
synthesis performance varies as a function of model size (columns) and code data size (rows).
PromptPPL#Pass Rate"[%]
350M 2.7B 6.1B 16.1B 350M 2.7B 6.1B 16.1B
Single-Turn 13:921:89 11 :671:46 10 :581:20 10 :250:99 5.75 25.43 28.48 38.74
Multi-Turn 10:090:62 8 :900:52 8 :180:43 8 :050:43 16.98 38.72 43.52 47.34
Table 4: Comparison between multi- and concatenated single-turn speciﬁcations on perplexity (PPL)
and program synthesis performance (as measured by pass rate) under C ODEGEN-MONO models.
with the value { var}.”, together with a test case input var /equal.tosf ‘Hello’ will be formatted into “Deﬁne
a string named ‘s’ with the value ‘Hello’.” Also see 1in Figure 1 for an example.
4.2 E XECUTION ENVIRONMENT AND SOLUTION EVALUATION
For execution, the history of pairs of prompts and generated completions is concatenated into a
self-contained program (see 3in Figure 1 for an example). The program is then executed in an
isolated Python environment following the single-turn HumanEval benchmark (Chen et al., 2021).
However, the problems in HumanEval are constructed in such a way that a known function signature
is completed, thus invocation of the generated code under a set of functional unit tests is trivial. In our
multi-turn case, no such entry point (or return value) is guaranteed to be generated. To circumvent
the issue of a missing return signature (or value), the last prompt of the multi-turn problems in MTPB
is always speciﬁed to print out the resulting state to the terminal. Then, the benchmark execution
environment overloads the Python print(args) function and stores args on a stack. If the sampled
code for the last prompt of a problem does not include the print() statement, which is a valid
convention to print on the terminal in Python or speciﬁcally Jupyter notebooks, then the AST of the
generated code will be mutated to inject an invocation of print() . Finally, a type-relaxed equivalence
check ( e.g., an implicit conversion between lists and tuples) of args against the predeﬁned gold
output of the problem is performed to determine test failure or success.
4.3 M ULTI -STEPPROGRAMMING CAPACITY SCALES WITH MODEL SIZE AND DATA SIZE
In this analysis, we investigate how the model size and data size affect the program synthesis capacity
in a multi-turn paradigm. In the MTPB, each problem has 5 test cases and we sample 40 samples
for each test case with each model, based on which the pass rate is computed for each problem.
The MTPB evaluation results (average pass rate) for our CODEGENmodels, baselines, and OpenAI
Codex models8are shown in Table 3. Clearly, the performance on the MTPB improves as a function
of the model size and data size. This suggests that the capacity of multi-step program synthesis scales
as a function of the model size and data size. The models are simply trained with an autoregressive
language modeling objective. While the model and the data scale up, multi-turn program synthesis
capacity emerges, that is, the capacity to synthesize programs in a multi-turn fashion.
8Accessed on November 10th, 2022.
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2023
350M 2.7B 6.1B 16.1B0510152025
14:19 14:63
0:19
 0:2522:06
19:6722:53
9:06
2:998:511:51
9:35
Number of Model ParametersDifference in Pass RatesEasy
Medium
Hard
Figure 2: Difference in average pass-rate of problems in single-turn and multi-turn formulation over
levels of problem difﬁculty. The improvement is sizable for most model sizes and difﬁculty levels,
except for easy problems with larger models.
4.4 B ETTER USERSPECIFICATION UNDERSTANDING WITH MULTI -TURN FACTORIZATION
We hypothesize that multi-turn factorization enhances the model’s understanding of user intent
speciﬁcations, which in turn lead to higher program synthesis capacity. To test this hypothesis,
we form a single-turn counterpart of multi-turn speciﬁcations by concatenating each speciﬁcation
into a single turn. As discussed in Section 3.2, we adopt the prompt perplexity as a proxy for user
intent understanding. Thus, we compare the perplexity of the multi-turn prompts and that of the
concatenated single-turn prompts under the four C ODEGEN-MONO models.
The average perplexity (see Appendix E for the calculation details) over all the problems in the MTPB
is displayed in the left panel of Table 4. For all models, the single-turn speciﬁcation has a higher
average perplexity than the multi-turn speciﬁcation. It implies that the multi-turn user speciﬁcations
can be better understood by the models. We notice that the average perplexity for both multi-turn and
single-turn intent speciﬁcations under larger models is slightly lower than that under smaller models,
indicating that the larger ones understand the user intent better than the smaller ones.
We compare the program synthesis pass rate with the multi-turn prompts to that with the concatenated
single-turn prompts. The results are shown in the right panel of Table 4. Multi-turn speciﬁcations
lead to close to or more than 10 percentage points over single-turn speciﬁcations for all model
sizes. Together with the perplexity analysis above, it appears that factorizing a user speciﬁcation into
multiple steps and leveraging the emerged capacity of large language models allow them to digest the
speciﬁcation more easily and synthesize programs more successfully.
Furthermore, we categorize the problems by difﬁculty level based on their average pass rates (“hard”
with less than 30%, “easy” with larger than 70%), and examine the interaction effect between difﬁculty
level and model size on the improvement by multi-turn factorization. See the results in Figure 2.
Across almost all model sizes and difﬁculty levels, multi-turn prompts lead to signiﬁcant improvement
over single-turn prompts and most improvements are nearly or higher than 10percentage points.
Interestingly, the larger models (6.1B and 16.1B) are invariant to multi-turn factorization for easy
problems (see the two short bars, 0:19% and 0:25%, in Figure 2). This implies that when the
problems can be easily understood by the model (due to the combined effect of easiness of the
problems and the high capacity of larger models), it is not necessary or beneﬁcial to factorize the
speciﬁcations. This is in fact consistent with our motivating assumption that factorizing complicated
speciﬁcations would ease problem understanding and improve program synthesis.
4.5 Q UALITATIVE EXAMPLES
To further understand the differences in model behavior over model sizes, we examine cases where
large models have contrasting performances to smaller models. We speciﬁcally select problems for
which CODEGEN-MONO 16.1B andCODEGEN-MONO 2.7B show a signiﬁcant discrepancy in
performance. On problems where CODEGEN-MONO 16.1B performed signiﬁcantly worse compared
toCODEGEN-MONO 2.7B , we observe that the larger model becomes inﬂexible due to taking
the prompt literally. For example, initializing a number always results in an integer, despite the
prompt asking to cast into a string (Figure 3), or the “return” keyword in a prompt triggers a function
deﬁnition while the intent is to directly generate an executable program (Figure 4). However in
general, larger-scale models overcome mistakes due to prompt misinterpretation by smaller models,
including assigning multiple variables at the same time (Figure 5) or understanding the concept of
anycomparison (Figure 6).
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2023
5 R ELATED WORK
Program Synthesis While program synthesis has a long history, two inherent challenges remain
unsolved: (1) intractability of the program space and (2) difﬁculty in accurately expressing user
intent (Manna & Waldinger, 1971; Gulwani et al., 2017). A large body of prior research attempted to
address (1) by exploring methods like stochastic search techniques (Parisotto et al., 2017; Schkufza
et al., 2013) and deductive top-down search (Gulwani, 2011; Polozov & Gulwani, 2015). However,
the scalability of these approaches is still limited. User intent can be expressed with various methods:
formal logical speciﬁcations, input-output examples, and natural language descriptions. Complete
and formal speciﬁcations require too much effort, while informal ones like input-output examples
often under-specify problems (Gulwani, 2011). Well-learned conditional distribution and language
understanding capacity owing to the large-scale model and data allows for efﬁcient solutions for
these two challenges. Several works investigate converting conversational intents into programmable
representations, such as SQL (Yu et al., 2019a;b) or dataﬂow graph (Andreas et al., 2020). Our
proposed benchmark requires the generation of Python, which is more general and complex.
Large Language Models Transformers capture dependency among sequence elements through
attention mechanism (Bahdanau et al., 2014) and are highly scalable. It has been successfully applied
to natural language processing (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), computer
vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior
works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al.,
2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but
these mostly focus on code retrieval, classiﬁcation, and program repair. Several recent and concurrent
efforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al.,
2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they
focus on generating code in a single turn, we propose to factorize the speciﬁcations into multiple turns
and demonstrate that it is highly effective to improve synthesis quality. It is worth pointing out that
Austin et al. (2021) explored reﬁning the code in multiple iterations, but it is essentially a single-turn
approach since a complete program is produced in every single turn. Prompting pre-trained language
models with intermediate information to improve task performance has attracted interest (Nye et al.,
2021; Wei et al., 2022). Our proposed MTPB also allows the model to leverage past turns as context.
Benchmarks for Program Synthesis To quantitatively evaluate program synthesis models, several
benchmarks have been proposed with different input forms. A popular input forms include preceding
code in the same line (Raychev et al., 2016), pseudo-code (Kulal et al., 2019), a docstring and
function signature (Chen et al., 2021), or problem description (Hendrycks et al., 2021). In most of
those cases, only directly relevant input information is given to the model. In contrast, a few previous
works instantiate benchmarks that measure the ability to generate programs given surrounding
program context beyond the target program, such as variables and other methods (Iyer et al., 2018) or
alternating “cells” of preceding code and text blocks (Agashe et al., 2019), while the primary focus
is to generate the target program itself. We propose a new benchmark that requires a progressive
generation of subprograms through multi-turn prompts.
6 C ONCLUSION
We study program synthesis with large causal language models trained on large corpora of code
data. The capacity to understand long context and generate coherent responses emerges from the
simple language modeling as the model size and data size scale up. Leveraging this capacity and
observing that better user intent understanding leads to better program synthesis, we propose a
multi-step program synthesis approach in which program synthesis is achieved through a multi-turn
speciﬁcation and code generation. Moreover, we develop the Multi-Turn Programming Benchmark
(MTPB) to investigate our models’ capacity on synthesizing programs in such a multi-step paradigm.
Our experiments show that the multi-step program synthesis capacity scales as a function of the
model size and data size. The intent speciﬁcations, which are speciﬁed in multiple steps, are digested
more easily by the models and lead to more accurate program synthesis. We open-source the training
code and the model checkpoints to facilitate future research and practical applications in this area.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2023
BROADER IMPACT AND ETHICAL CONSIDERATIONS
All variants of CODEGENare ﬁrstly pre-trained on the Pile, which includes a small portion of
profane language. Focusing on the GitHub data that best aligns our expected use case of program
synthesis, Gao et al. (2020) report that 0.1% of the data contained profane language, and has sentiment
biases against gender and certain religious groups. Thus, while we did not observe in our samples,
CODEGENmay generate such content as well. In addition to risks on natural language outputs
(e.g., docstrings), generated programs may include vulnerabilities and safety concerns, which are not
remedied in this work. Models should not be used in applications until being treated for these risks.
REFERENCES
Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. Juice: A large scale distantly supervised
dataset for open domain context-based code generation. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP) , pp. 5436–5446, 2019.
Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim,
Jordan DeLoach, Leah Dorner, Jason Eisner, et al. Task-oriented dialogue as dataﬂow synthesis.
Transactions of the Association for Computational Linguistics , 8:556–571, 2020.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732 , 2021.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorﬂow, March 2021. URL https://doi.org/
/one.tosf/zero.tosf./five.tosf/two.tosf/eight.tosf/one.tosf/zenodo./five.tosf/two.tosf/nine.tosf/seven.tosf/seven.tosf/one.tosf/five.tosf . If you use this software, please cite it using these metadata.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.
Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 , 2022.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
Alvin Cheung, Armando Solar-Lezama, and Samuel Madden. Optimizing database-backed applica-
tions with query synthesis. ACM SIGPLAN Notices , 48(6):3–14, 2013.
Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. Pymt5:
multi-mode translation of natural language and python code with transformers. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp.
9052–9065, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N/one.tosf/nine.tosf-/one.tosf/four.tosf/two.tosf/three.tosf .
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2023
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
InICLR , 2021. URL https://openreview.net/forum?id/equal.tosfYicbFdNTTy .
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and
natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020 ,
pp. 1536–1547, 2020.
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code inﬁlling
and synthesis. arXiv preprint arXiv:2204.05999 , 2022.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027 , 2020.
Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. ACM
Sigplan Notices , 46(1):317–330, 2011.
Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and
Trends® in Programming Languages , 4(1-2):1–119, 2017.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin
Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge
competence with APPS. In Thirty-ﬁfth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) , 2021. URL https://openreview.net/forum?id/equal.tosf
sD/nine.tosf/three.tosfGOzH/three.tosfi/five.tosf .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In ICLR , 2020. URL https://openreview.net/forum?id/equal.tosfrygGQyrFvH .
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in
programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing , pp. 1643–1652, Brussels, Belgium, October-November 2018. Association
for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https://aclanthology.org/
D/one.tosf/eight.tosf-/one.tosf/one.tosf/nine.tosf/two.tosf .
Rajeev Joshi, Greg Nelson, and Keith Randall. Denali: A goal-directed superoptimizer. ACM
SIGPLAN Notices , 37(5):304–314, 2002.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature , 596(7873):583–589, 2021.
Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating
contextual embedding of source code. In International Conference on Machine Learning , pp.
5110–5121. PMLR, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster) ,
2015. URL http://arxiv.org/abs//one.tosf/four.tosf/one.tosf/two.tosf./six.tosf/nine.tosf/eight.tosf/zero.tosf .
Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S
Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing
Systems , 32, 2019.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , pp. 7871–7880, 2020.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2023
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien
de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal,
Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,
Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with
alphacode, Feb 2022.
Zohar Manna and Richard J Waldinger. Toward automatic program synthesis. Communications of
the ACM , 14(3):151–165, 1971.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 ,
2021.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748 , 2018.
Pavel Panchekha, Alex Sanchez-Stern, James R Wilcox, and Zachary Tatlock. Automatically
improving accuracy for ﬂoating point expressions. ACM SIGPLAN Notices , 50(6):1–11, 2015.
Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet
Kohli. Neuro-symbolic program synthesis. In ICLR (Poster) , 2017. URL https://openreview.
net/forum?id/equal.tosfrJ/zero.tosfJwFcex .
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In International conference on machine learning , pp. 1310–1318. PMLR, 2013.
Oleksandr Polozov and Sumit Gulwani. Flashmeta: A framework for inductive program synthe-
sis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented
Programming, Systems, Languages, and Applications , pp. 107–126, 2015.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research , 21:1–67, 2020.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations
toward training trillion parameter models. In SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis , pp. 1–16. IEEE, 2020.
Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees.
ACM SIGPLAN Notices , 51(10):731–747, 2016.
Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. ACM SIGARCH
Computer Architecture News , 41(1):305–316, 2013.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 , 2019.
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with
rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.
Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. Expectation vs. experience: Evaluating
the usability of code generation tools powered by large language models. In CHI Conference on
Human Factors in Computing Systems Extended Abstracts , pp. 1–7, 2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems , pp. 5998–6008, 2017.
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-transformer-jax , May 2021.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2023
Yue Wang, Weishi Wang, Shaﬁq Joty, and Steven C.H. Hoi. Codet5: Identiﬁer-aware uniﬁed pre-
trained encoder-decoder models for code understanding and generation. In Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021 , 2021.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022.
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze
Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri,
Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard
Socher, Walter Lasecki, and Dragomir Radev. CoSQL: A conversational text-to-SQL challenge
towards cross-domain natural language interfaces to databases. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 1962–1979, Hong Kong,
China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1204.
URL https://aclanthology.org/D/one.tosf/nine.tosf-/one.tosf/two.tosf/zero.tosf/four.tosf .
Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,
Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent
Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. SParC: Cross-domain semantic
parsing in context. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pp. 4511–4523, Florence, Italy, July 2019b. Association for Computational Linguistics.
doi: 10.18653/v1/P19-1443. URL https://aclanthology.org/P/one.tosf/nine.tosf-/one.tosf/four.tosf/four.tosf/three.tosf .
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2023
A M ODEL TRAINING
To evaluate the emergence of multi-turn program synthesis capabilities under scaling laws, we
adopt standard transformer-based autoregressive language models, varying (1) the number of model
parameters (350M, 2.7B, 6.1B, 16.1B) and (2) the number of tokens of programming languages in the
training corpora. For scaling the models, a custom library JAX FORMER for training large language
models on TPU-v4 hardware was developed and will be released as open source, including the trained
model weights.
A.1 D ATASETS
Dataset Language Raw Size Final Size Final Tokens
THEPILENatural Language 825:18GiB 1159:04GiB 354:7B
Code 95:16GiB 95:16GiB 31:6B
BIGQUERYC 1772:1GiB 48:9GiB 19:7B
C++ 205:5GiB 69:9GiB 25:5B
Go 256:4GiB 21:4GiB 9:6B
Java 335:1GiB 120:3GiB 35:4B
JavaScript 1282:3GiB 24:7GiB 9:7B
Python 196:8GiB 55:9GiB 19:3B
BIGPYTHON Python 5558:1GiB 217:3GiB 71:7B
Table 5: Approximate statistics for training corpora along the pre-processing steps.
For each dataset, the pre-processing shares the following steps: (1) ﬁltering, (2) deduplication, (3)
tokenization, (4) shufﬂing, and (5) concatenation. For details on THEPILE, we refer to Gao et al.
(2020). For BIGQUERY andBIGPYTHON , in (1) ﬁles are ﬁltered by ﬁle extension, and ﬁles with
average lines length of <100 characters, a maximum line length of 1;000, and >90% of the characters
being decimal or hexadecimal digits are removed. For (2), exact duplicates based on their SHA-256
hash are removed, which amounts to a substantial portion of the raw data due to forks and copies
of repositories. For (3), the BPE vocabulary of GPT-2 is extended by special tokens representing
repeating tokens of tabs and white spaces. In the multi-lingual setting of BIGQUERY , a preﬁx is
prepended to indicate the name of the programming language. For (4), each year of data is randomly
shufﬂed. For (5), sequences are concatenated to ﬁll the context length of 2;048tokens with a special
token as a separator. Table 5 summarizes the statistics of the training corpora.
CODEGEN-NL models are randomly initialized and trained on THEPILE.CODEGEN-MULTI models
are initialized from CODEGEN-NL and then trained on the BIGQUERY .CODEGEN-MONO models
are initialized from C ODEGEN-MULTI and then trained on B IGPYTHON .
A.2 M ODELS
Our models are autoregressive transformers with the regular next-token prediction language modeling
as the learning objective. The family of CODEGENmodels is trained in various sizes with 350M, 2.7B,
6.1B, and 16.1B parameters. The ﬁrst three conﬁgurations allow for direct comparison with open-
sourced large language models trained on text corpus, GPT-N EO(350M, 2.7B) (Black et al., 2021)
andGPT-J (6B) (Wang & Komatsuzaki, 2021). See Table 6 in Appendix A for model speciﬁcations.
The architecture follows a standard transformer decoder with left-to-right causal masking. For the
positional encoding, we adopt rotary position embedding (Su et al., 2021). For the forward pass, we
execute the self-attention and feed-forward circuits in parallel for improved communication overhead
following Wang & Komatsuzaki (2021), that is, xt+1=xt+ mlp(ln(xt+ attn(ln(xt))))is altered
toxt+1=xt+ attn(ln(xt)) + mlp(ln( xt))for which the computation of self-attention, attn() , and
feed-forward, mlp() , with layer-norm, ln(), is simultaneous. The architecture and hyper-parameter
choices were optimized speciﬁcally for the hardware layout of TPU-v4.
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2023
Model Dataset Hyper-parameter 350M 2.7B 6.1B 16.1B
CODEGENNumber of layers 20 32 33 34
Number of heads 16 32 16 24
Dimensions per head 64 80 256 256
Context length 2,048 2,048 2,048 2,048
Batch size 500k 1M 2M 2M
Weight decay 0.1 0.1 0.1 0.1
CODEGEN-NL T HEPILELearning rate 3:0e 4 1:6e 4 1:2e 4 0:9e 4
Warm-up steps 3k 3k 3k 3k
Warm-up / Total steps 350k 350k 350k 350k
CODEGEN-MULTI BIGQUERYLearning rate 1:8e 4 0:8e 4 0:4e 4 0:5e 4
Warm-up steps 3k 3k 3k 3k
Total steps 150k 150k 150k 150k
CODEGEN-MONO BIGPYTHONLearning rate 1:8e 4 0:8e 4 0:4e 4 0:5e 4
Warm-up steps 3k 3k 3k 3k
Total steps 150k 150k 150k 150k
Table 6: Hyper-parameters for model speciﬁcation and optimization for the family of CODEGEN
models.
A.3 T RAINING
The scaling of large language models requires data and model parallelism. Google’s TPU-v4 hardware
with a high-speed toroidal mesh interconnect naturally allows for efﬁcient parallelism. To efﬁciently
utilize the hardware, the training of the models is implemented in JAX (Bradbury et al., 2018). For
parallel evaluation in JAX the pjit()9operator is adopted. The operator enables a paradigm named
single-program, multiple-data (SPMD) code, which refers to a parallelism technique where the same
computation is run on different input data in parallel on different devices.10Speciﬁcally, pjit()is the
API exposed for the XLA SPMD partitioner in JAX, which allows a given function to be evaluated in
parallel with equivalent semantics over a logical mesh of compute.
Our library JAX FORMER recruits a designated coordinator node to orchestrate the cluster of TPU-
VMs11with a custom TCP/IP protocol. For data parallelism, the coordinator partitions a batch and
distributes the partitions to the individual TPU-VMs. For model parallelism, two schemes for the
sharding of model parameters are supported: (1) Intra-TPU-VM, where parameters are sharded
across MXU cores12inside a physical TPU-v4 board and replicated across boards following Shoeybi
et al. (2019); Wang & Komatsuzaki (2021); (2) Inter-TPU-VM, where parameters are sharded across
TPU-v4 boards and activations are replicated following Rajbhandari et al. (2020).
Both intra-TPU-VM and inter-TPU-VM sharding schemes are implemented based on our speciﬁc
pjit() a logical mesh speciﬁcation (r;p;c )withrreplicas of the parameters, ppartitions of the
parameters, and clogical cores per board over nbTPU boards with each nclogical cores such that
dp=nbandrpc=nbnc.
The intra-TPU-VM scheme is adopted for models of size of less or equal to 6B parameters, the
total amount of model and optimizer parameters ﬁt into the combined HBM memory of a single
TPU-v4 board. For instance, a TPU-v4-512 slice with nb= 64 andnc= 4 would be conﬁgured
as(r;p;c ) = (64;1;4). That is, the parameters are being replicated across r= 64 boards with
p= 1 total inter-board partitions and intra-board parallelism across c= 4 logical chips. In this
conﬁguration, the mean gradient is accumulated across boards via with_sharding_constraint() ,
effectively emulating the behavior of the xmap()13operator.
9https://jax.readthedocs.io/en/latest/_modules/jax/experimental/pjit.html
10https://jax.readthedocs.io/en/latest/jax-/one.tosf/zero.tosf/one.tosf//zero.tosf/six.tosf-parallelism.html
11https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms
12Speciﬁcally, 4 TPU-v4 chips ( i.e., 8 physical which amount 4 logical or virtual MXU cores).
13https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.xmap.html
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2023
The inter-TPU-VM scheme is adopted for models exceeding the size of 6B parameters for which
the model and optimizer parameters have to be sharded across TPU-v4 boards. For instance, a
TPU-v4-512 slice with nb= 64 andnc= 4 would be conﬁgured as (r;p;c ) = (1;64;4). For
larger slices such as TPU-v4-1024 with nb= 128 , one may introduce redundancy in the parameter
sharding, e.g.,(r;p;c ) = (2;64;4). In this conﬁguration, the activations are replicated across boards
viawith_sharding_constraint() . Moreover, (r;p;c )allows for backwards compatibility for the
logical hardware layout transition from TPU-v3 with c= 8to TPU-v4 with c= 4by adjusting p
without the need for re-sharding.
For the optimization, Table 6 summarizes the hyper-parameters. We adopt the Adam (Kingma & Ba,
2015) optimizer with (1;2;) = (0:9;0:999;1e 08)and global gradient norm clipping (Pascanu
et al., 2013) of 1:0. The learning rate function over time follows GPT-3 (Brown et al., 2020)
with warm-up steps and cosine annealing. In summary, we mainly adopted the GPT-3 reference
conﬁgurations with minor variations accounting for TPU optimizations. We did not have the compute
capacity to optimize these hyper-parameters further.
B P ASS@kESTIMATOR
We use the unbiased estimator proposed in Chen et al. (2021) to compute pass@ k. For each task,
nksamples are sampled. In particular, we use n= 200 andk100. Supposecis the number of
correct samples, among the nsamples, which pass all the unit tests. Then the unbiased estimator is
deﬁned as follows:
pass@k=EProblems"
1  n c
k
 n
k#
(1)
Directly computing this estimator is numerically unstable. We use the numerically stable numpy
implementation introduced by Chen et al. (2021).
C T YPE-RELAXED EQUIVALENCE CHECK FOR MTPB E VALUATION
We perform the following type-relaxation before assessing the equivalence between model outputs
and the expected outputs.
•Convert numpy arrays into correspondingly typed lists of standard types ( e.g.np.int/three.tosf/two.tosf will
be cast to int).
•pandas series are converted and compared in numpy array format.
• For the rest, model outputs are cast into the type of gold standard outputs.
• Floating numbers are compared with "= 1e 6as the tolerance threshold.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2023
D L IST OF MTPB P ROBLEMS
Problem Name Problem Description Category
Sandwich string Append a string in the middle of another string. string
Normalize integer list Normalize a list of positive integers and print formatted percentages. math
Convert time Convert units of time. math
Squared Fibonacci Print the squared Fibonacci numbers. math
Compare counts Compare the count of positive and negative numbers in a given list. array
Pandas mean Construct and compute the mean of a pandas DataFrame. D.S.
Fizz buzz Solve the ﬁzz buzz problem. Algo.
Bi-grams Print the bi-grams of a sentence. string
Top note Print the name with top note out of a dictionary. dict
Hex to binary Convert hex to binary and reverse. math
Invert dict Detect an inversion of a given dictionary. dict
Class deﬁnition Create a POJO class. class
Longest number Print the longest number. math
Linear regression Fit linear regression model with speciﬁed function and sk-learn. D.S.
Encrypt and decrypt Rotate alphabet for encryption, then reverse the operation. Algo.
Dedup custom objects Implement a class with __hash__ and obtain a count unique objects. class
Drunken python Convert between integer and string without using built-in functions. string
Morse code Encode a string into morse code given its conversion rule. Algo.
Two-sum Implement the two-sum problem on a given input pair. Algo.
k-means Implement and run k-means on sampled points. D.S.
Even odd sum Print the sum of even and odd numbers in a list. math
Shift zeros Move all the zeros in a list to the right. array
Bootstrap 95% CI Calculate the bootstrap 95% conﬁdence interval of an array. D.S.
Sum even digits Sum the even digits between two numbers. math
Min-max diff Compute the difference between max and min numbers in a list. array
Distinct chars Print the sorted, case-insensitive unique characters of a string. string
Longer string Compare and print the longer string given two strings. string
Sum ﬂoat digits Sum numbers before and after the decimal point of a ﬂoat. math
Count vowels Count the number of vowels in a string. string
Factorial Compute the factorial of n. math
Max edge triangle Finds the maximum range of a triangle’s third edge. math
Factorial & remainder Compute the factorial and its remainder when divided. math
Sum polygon angles Sum the angles in a polygon. math
Sum string numbers Add together two numbers represented in string. string
Min-max sum Sum the range from the minimum to the maximum of a list. array
V owel overlap Find the number of overlapped vowels of two words. string
Sum negative Calculate the sum of negative numbers in a list. math
Load dataset Load from a ﬁle and print statistics. D.S.
Char length list Return a list of non-punctuation character lengths from words. string
Hex to RGB Convert a six hexadecimal digit string into list of RGB values. math
Majority vote Check if a certain element is the majority of a given list. array
Week later Print the formatted date of a week later given a date. string
Sorted word weights Check if the list of word weights (sum of ASCII values) are sorted. math
Create Palindrome Sum pairs of adjacent digits until the number is palindrome. string
Simulate Backspace Apply the backspace characters in a string and print the modiﬁed. string
Data manipulation Manipulate a pandas DataFrame and split into train and test set. D.S.
Sum non-overlap Sum the integers in a (min, max) range that don’t appear in a list. array
Detect digits Find if a string contains digits. array
Cascading functions Sequentially invoke function objects in a given list. math
Pluralize duplicates Pluralize duplicated words in a list. dict
Highest altitude Given relative altitudes , ﬁnd the highest altitude array
Truncate words Truncate a sentence so that it contains k words array
Single element Find the elements that appear one time in an array array
Remove elements Remove all the occurrences of an element in an array array
Check array sum Check whether the sum of an array is equal to a given value array
Table 7: Problems in MTPB, showing the problem 1 to 55. D.S. and Algo. refers to data science and
algorithm.
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2023
Problem Name Problem Description Category
Merge sorted lists Merge two sorted lists into one Algo.
Maximum subarray Find the max contiguous subarray and return the sum Algo.
Max square root integer Find the largest integer but smaller than the square root Algo.
Longest word Find the longest word in a word list Algo.
Sum unique elements Sum all the unique numbers in a list Algo.
Diagonal sum Compute the diagonal sum of a matrix D.S.
Matrix condition number Check condition number of a matrix is less than a threshold D.S.
Matrix multiplication sum Compute matrix multiplication sum of two matrices D.S.
Matrix determinant Compare two matrix determinants D.S.
Log-sum-exp Compute the log of sum exponential input D.S.
K nearest points Find the k nearest points to the origin array
Longest common preﬁx Find the longest common preﬁx of two strings Algo.
Duplicate elements Find duplicates in a list array
First unique character Find the ﬁrst non-repeating character in a string Algo.
Uncommon words Find uncommon words in two sentences Algo.
Average words length Compute the average word length of a sentence Algo.
Compare char freq Compare the character frequencies in two strings string
Reverse string Reverse a string string
Square Sum diff Difference between the square of sum and the sum of squares math
Cosine sim Compute the cosine similarity between two vectors math
Vector distance Compare vector distances to the origin math
Smallest standard dev. Find the smaller standard deviation given two lists D.S.
Smallest means Find the smaller mean given two lists D.S.
Coefﬁcient of variation Compute coefﬁcient of variation given a list D.S.
L1 norm Compute the L1 norm given a list D.S.
Z-statistic Compute z-statistic given a list D.S.
Move negatives Move all negative elements in a list to the end array
Remove alphabets Remove alphabetical characters in a string string
Largest norm Find the largest norm among n-dimensional points D.S.
F1 score Given two arrays (pred, gold), calculate the F1 score D.S.
Add Space Add spaces before capital letters string
Remove outlier Remove data points in the tail (2sigma) of normal distribution D.S.
Convert to categorical Convert values into categorical variables D.S.
Group by key Group items in an array using a provided function array
Max stock proﬁt Given an array of "prices", ﬁnd the max proﬁt array
Sum positions Sum of all position indices where a value appear array
Find missing num Find a missing number given a list and a max number array
Common num in matrix Common numbers among rows in a matrix array
Sum Collatz Obtain the sum of Collatz sequence starting from given number Algo.
Cup swap Name the location of a "ball" after cup swapping Algo.
Reverse digits Reverse digits in a number with a stack Algo.
Calculate arrows Calculate arrowheads left and right Algo.
Check interval num Check if the interval (max-min) is included in a list Algo.
Length encoding Encode a string by converting repeated chars with counts string
Convert email Use regex to match email addresses and remove special chars string
Second largest Print out the second largest element in an array array
Largest preﬁx sum Return the largest preﬁx sum in an array array
Closest element to zero Find the element which is the closest to 0 and print the distance array
Consecutive unique char Find the max length contiguous subarray with unique characters string
Highest frequency char Obtain the frequency of the most frequent character string
Longest palindrome Find the length of longest palindrome substring string
Count primes Calculate prime numbers in a range Algo.
Rotate array Rotate an array to the right k steps Algo.
Partition equal sets Check if an array can be split into two sets with equal sums Algo.
Square root integer Compute the integer part of square root math
Plus 1 Return the digits after an integer is added by 1 math
Check square sum Check whether one integer is a sum of two square numbers math
Compare standard dev. Determine whether standard deviation is less than 1 D.S.
Matrix size Calculate the sum of row and column numbers D.S.
Diff mean and median Calculate the difference between mean and median for an array D.S.
Table 8: Problems in MTPB, showing the problem 56 to 115. D.S. and Algo. refers to data science
and algorithm.
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2023
E P ERPLEXITY COMPUTATION FOR SINGLE -AND MULTI -TURN PROMPTS
Supposefpign
i=1is the set of prompts for a given problem, and fsign
i=1are thensub-programs syn-
thesized by a model P. Supposeci 1= [p1;s1;:::;pi 1;si 1]where [;]indicates concatenation,
the conditional probability of piisProbi=P(pijci 1), and then the perplexity for the multi-turn
prompts is computed as
PPL Multi turn= exp 
 1
mnX
i=1log Probi!
; (2)
wheremis the total number of tokens of all prompts fpign
i=1. Supposec= [p1;s1;:::;pn;sn], then
its probability is Prob =P(c), and the the perplexity for the single-turn prompts is computed as
PPL Single turn= exp
 1
mlog Prob
: (3)
F P ERPLEXITY COMPARISON FOR CODEGEN-NL AND CODEGEN-MULTI
CODEGEN-NL 350M 2.7B 6.1B
Pass 4:53 3 :25 2 :78
Non-Pass 4:96 3 :87 3 :65
Table 9: Average prompt perplexity#of C ODEGEN-NL models on pass and non-pass problems.
CODEGEN-MULTI 350M 2.7B 6.1B
Pass 4:78 3 :82 3 :82
Non-Pass 5:64 4 :85 4 :80
Table 10: Average prompt perplexity#ofCODEGEN-MULTI models on pass and non-pass problems.
G A DDITIONAL BENCHMARK RESULTS
Model pass@ 1pass@ 10 pass@ 100
CODEGEN-NL 350M 0.96 6.37 19.91
CODEGEN-NL 2.7B 5.34 24.63 48.95
CODEGEN-NL 6.1B 8.15 31.21 55.27
CODEGEN-NL 16.1B 10.92 38.43 62.76
CODEGEN-MULTI 350M 7.46 24.18 46.37
CODEGEN-MULTI 2.7B 18.06 45.80 65.34
CODEGEN-MULTI 6.1B 18.35 47.27 67.92
CODEGEN-MULTI 16.1B 20.94 51.61 70.02
CODEGEN-MONO 350M 14.59 41.49 63.00
CODEGEN-MONO 2.7B 27.31 59.19 74.24
CODEGEN-MONO 6.1B 32.48 64.20 76.81
CODEGEN-MONO 16.1B 35.28 67.32 80.09
INCODER 6B 21.30 46.50 66.20
code-cushman-001 45.90 66.90 79.90
code-davinci-001 51.80 72.80 84.10
code-davinci-002 58.10 76.70 84.50
Table 11: Pass rates on Mostly Basic Python Problems (MBPP).
We also evaluated our models on Mostly Basic Python Problems (MBPP) (Austin et al., 2021). The
results are displayed in Table 11. Following Chen et al. (2022), we sampled programs from the
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2023
sanitized MBPP for all of our models, with n= 100 and temperature = 0:8. The last four rows
are from the aforementioned paper. In general we observe the consistent trend of improving the
performance over different versions (NL, Multi, Mono), with our largest CODEGEN-MONO 16.1B
approaching the results from code-cushman-001. While we do not know whether any of OpenAI
models is the “Codex 12B” reported in Chen et al. (2021), we believe our model achieves reasonable
results on MBPP as well. We also note that our CODEGEN-MONO 6.1B signiﬁcantly outperformed
INCODER 6B.
H G ENERATED SAMPLES
H.1 C ASES WHERE CODEGEN-MONO 16.1B UNDER -PERFORMS
/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Initialize a variable "stack" with an
empty list, and "num" with /one.tosf/zero.tosf/zero.tosf as a
string.
stack /equal.tosf []
num /equal.tosf "/one.tosf/zero.tosf/zero.tosf"
/numbersign.tosf For each chracter in "num", append the
character to "stack".
for i in range(len(num)):
stack.append(num[i])
/numbersign.tosf Assign an empty string to a variable "
result", and concatenate characters
popped from the last element of "stack"
to "result" until "stack" is empty.
result /equal.tosf ""
while len(stack)!/equal.tosf /zero.tosf:
result /plus.tosf/equal.tosf stack.pop()
/numbersign.tosf Cast "result" as integer and print it out.
print(int(result))/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Initialize a variable "stack" with an
empty list, and "num" with /one.tosf/zero.tosf/zero.tosf as a
string.
stack /equal.tosf []
num /equal.tosf /one.tosf/zero.tosf/zero.tosf
/numbersign.tosf For each chracter in "num", append the
character to "stack".
for i in num:
stack.append(i)
/numbersign.tosf Assign an empty string to a variable "
result", and concatenate characters
popped from the last element of "stack"
to "result" until "stack" is empty.
result /equal.tosf ""
while len(stack) /greater.tosf /zero.tosf:
result /plus.tosf/equal.tosf stack.pop()
/numbersign.tosf Cast "result" as integer and print it out.
print(int(result))
CODEGEN-MONO 2.7B C ODEGEN-MONO 16.1B
Figure 3: Generated samples for “Reverse digits” problem. While being simplistic, we often observe
thatCODEGEN-MONO 16.1B assumes the type when assigning a number to a variable. Here on the
right, despite being explicitly speciﬁed to initialize a number as a string, the larger model fails to do
so. This causes an error in the following turn when the code attempts to iterate over num.
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2023
/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Assign the matrix "[[/three.tosf, /two.tosf], [/two.tosf, /three.tosf]]" to a
variable named "my_matrix".
my_matrix /equal.tosf [[/three.tosf, /two.tosf], [/two.tosf, /three.tosf]]
/numbersign.tosf Assign the number "/one.tosf" to a variable named
"t".
t /equal.tosf /one.tosf
/numbersign.tosf Compute the condition number of my_matrix
and store as result.
result /equal.tosf np.linalg.cond(my_matrix)
/numbersign.tosf Check whether the result is smaller than t
. If yes, return "True", otherwise
return "False".
if result /less.tosf t:
print("True")
else:
print("False")/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Assign the matrix "[[/three.tosf, /two.tosf], [/two.tosf, /three.tosf]]" to a
variable named "my_matrix".
my_matrix /equal.tosf [[/three.tosf, /two.tosf], [/two.tosf, /three.tosf]]
/numbersign.tosf Assign the number "/one.tosf" to a variable named
"t".
t /equal.tosf /one.tosf
/numbersign.tosf Compute the condition number of my_matrix
and store as result.
condition_number /equal.tosf np.linalg.cond(my_matrix)
/numbersign.tosf Check whether the result is smaller than t
. If yes, return "True", otherwise
return "False".
def check_condition_number(condition_number,
t):
if condition_number /less.tosf t:
return True
else:
return False
CODEGEN-MONO 2.7B C ODEGEN-MONO 16.1B
Figure 4: Generated samples for “Matrix condition number” problem. Both models generate programs
in the same scope until the last turn, where CODEGEN-MONO 16.1B generates in the form of function
deﬁnition. Noticeably, the larger model generates return keyword that appears in the prompt, but it
is only valid within a function.
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2023
H.2 C ASES WHERE CODEGEN-MONO 16.1B OUTPERFORMS
/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Define a variable "pos" with "B", "swap"
with [’AC’, ’CA’].
pos /equal.tosf [’B’,’swap’]
/numbersign.tosf Write a function "move" that takes two
strings x and y as input, and replace
any appearance of x in y with an empty
string, then return y.
def move(x,y):
return y.replace(x,’’)
/numbersign.tosf For each element in "swap", if it contains
"pos", call "move" on "pos" and the
current element and store the result to
"pos".
def swap(swap):
for i in range(len(swap)):
if swap[i] /equal.tosf/equal.tosf ’pos’:
swap[i] /equal.tosf move(pos[/zero.tosf],pos[/one.tosf])
return swap
/numbersign.tosf Print out "pos".
print(pos)/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Define a variable "pos" with "B", "swap"
with [’AC’, ’CA’].
pos /equal.tosf "B"
swap /equal.tosf [’AC’, ’CA’]
/numbersign.tosf Write a function "move" that takes two
strings x and y as input, and replace
any appearance of x in y with an empty
string, then return y.
def move(x, y):
return y.replace(x, "")
/numbersign.tosf For each element in "swap", if it contains
"pos", call "move" on "pos" and the
current element and store the result to
"pos".
for i in swap:
if pos in i:
pos /equal.tosf move(pos, i)
/numbersign.tosf Print out "pos".
print(pos)
CODEGEN-MONO 2.7B C ODEGEN-MONO 16.1B
Figure 5: Generated samples for “Cup swap” problem. Here, CODEGEN-MONO 2.7B mistakenly
treats “swap” as a part of a value t o be assigned, while CODEGEN-MONO 16.1B parses the intent
correctly and assigns two variables in parallel.
22

--- PAGE 23 ---
Published as a conference paper at ICLR 2023
/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Initialize the variable named lst/one.tosf with a
list [’abc’, ’abc/one.tosf/zero.tosf’].
lst/one.tosf /equal.tosf [’abc’, ’abc/one.tosf/zero.tosf’]
/numbersign.tosf Create a function called num_in_str() to
check whether a string contains a number
.
def num_in_str(string):
for i in string:
if i.isnumeric():
return True
else:
return False
/numbersign.tosf Call the function num_in_str() to find
strings in lst/one.tosf that have numbers and
assign them to a list named lst/two.tosf
lst/two.tosf /equal.tosf []
for i in lst/one.tosf:
if num_in_str(i):
lst/two.tosf.append(i)
/numbersign.tosf Print out lst/two.tosf
print(lst/two.tosf)/numbersign.tosf Import libraries.
import numpy as np
/numbersign.tosf Initialize the variable named lst/one.tosf with a
list [’abc’, ’abc/one.tosf/zero.tosf’].
lst/one.tosf /equal.tosf [’abc’, ’abc/one.tosf/zero.tosf’]
/numbersign.tosf Create a function called num_in_str() to
check whether a string contains a number
.
def num_in_str(s):
return any(i.isdigit() for i in s)
/numbersign.tosf Call the function num_in_str() to find
strings in lst/one.tosf that have numbers and
assign them to a list named lst/two.tosf
lst/two.tosf /equal.tosf [i for i in lst/one.tosf if num_in_str(i)]
/numbersign.tosf Print out lst/two.tosf
print(lst/two.tosf)
CODEGEN-MONO 2.7B C ODEGEN-MONO 16.1B
Figure 6: Generated samples for “Detect digits” problem. CODEGEN-MONO 2.7B fails to implement
thenum_in_str , resulting in checking only the ﬁrst character. In contrast, CODEGEN-MONO 16.1B
successfully utilizes anyfunction to scan all the characters in the given string.
I A DDITIONAL ANALYSES ON MTPB
We conducted additional analyses to illustrate the relationship generated program length and pass
rate and showed the results in Figure 7, Figure 8, and Figure 9. The relationship between generated
program length and prompt length is shown in Figure 10.
23

--- PAGE 24 ---
Published as a conference paper at ICLR 2023
Figure 7: Maximum Length of Completion versus Pass Rate.
Figure 8: Maximum Length of Completion versus Pass Rate.
Figure 9: Maximum Length of Completion versus Pass Rate.
24

--- PAGE 25 ---
Published as a conference paper at ICLR 2023
150200250T oken count relationship between 
 prompts and generated programs from CodeGen-16B-Mono
y=x
Turn
0 20 40 60 80 100
Number of tokens in prompt020406080100120140Number of tokens in generated program
Figure 10: Prompt Length versus Generated Program Length.
25
