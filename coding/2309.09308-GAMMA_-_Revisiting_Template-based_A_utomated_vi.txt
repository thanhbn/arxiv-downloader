GAMMA: Xem xét lại Sửa chữa Chương trình Tự động dựa trên Mẫu thông qua Dự đoán Mặt nạ

Quanjun Zhang
Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới
Đại học Nam Kinh, Trung Quốc
quanjun.zhang@smail.nju.edu.cn

Bowen Yu
Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới
Đại học Nam Kinh, Trung Quốc
201250070@smail.nju.edu.cn

Chunrong Fang∗
Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới
Đại học Nam Kinh, Trung Quốc
fangchunrong@nju.edu.cn

Weisong Sun
Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới
Đại học Nam Kinh, Trung Quốc
weisongsun@smail.nju.edu.cn

Tongke Zhang
Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới
Đại học Nam Kinh, Trung Quốc
201250032@smail.nju.edu.cn

Zhenyu Chen∗
Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới
Đại học Nam Kinh, Trung Quốc
zychen@nju.edu.cn

Tóm tắt—Sửa chữa chương trình tự động (APR) nhằm mục đích sửa các lỗi phần mềm mà không cần sự can thiệp của con người và đóng một vai trò quan trọng trong phát triển và bảo trì phần mềm. APR dựa trên mẫu đã được nghiên cứu rộng rãi và cho thấy những kết quả đầy hứa hẹn. Tuy nhiên, việc lựa chọn mã nguồn donor phù hợp, đây là một thành phần sửa chữa quan trọng để tạo ra các bản vá ứng cử viên, là một thách thức đối với APR dựa trên mẫu. Mã nguồn donor không phù hợp có thể gây ra việc tạo ra các bản vá có vẻ hợp lý nhưng không chính xác ngay cả với các mẫu sửa chữa đúng, hạn chế hiệu suất sửa chữa.

Trong bài báo này, chúng tôi nhằm mục đích xem xét lại APR dựa trên mẫu và đề xuất GAMMA, để trực tiếp tận dụng các mô hình ngôn ngữ được huấn luyện trước lớn cho việc tạo mã nguồn donor. Hiểu biết chính của chúng tôi là thay vì truy xuất mã nguồn donor trong tệp lỗi cục bộ, chúng ta có thể trực tiếp dự đoán các token mã chính xác dựa trên các đoạn mã ngữ cảnh và các mẫu sửa chữa bằng một nhiệm vụ cloze. Cụ thể, (1) GAMMA sửa đổi nhiều mẫu sửa chữa khác nhau từ các kỹ thuật APR dựa trên mẫu hiện đại (tức là TBar) và chuyển đổi chúng thành các mẫu mặt nạ. (2) GAMMA áp dụng một mô hình ngôn ngữ được huấn luyện trước để dự đoán mã chính xác cho mã bị che mặt nạ như một nhiệm vụ điền vào chỗ trống. Mặc dù ý tưởng của chúng tôi mang tính tổng quát và có thể được xây dựng trên nhiều mô hình ngôn ngữ được huấn luyện trước khác nhau, chúng tôi đã triển khai GAMMA như một công cụ APR thực tế dựa trên mô hình UniXcoder gần đây. Kết quả thực nghiệm cho thấy GAMMA sửa chữa chính xác 82 lỗi trên Defects4J-v1.2, đạt được cải thiện 20.59% (14 lỗi) và 26.15% (17 lỗi) so với phương pháp dựa trên mẫu hiện đại trước đó TBar và phương pháp dựa trên học máy Recoder. Hơn nữa, GAMMA sửa chữa 45 lỗi và 22 lỗi từ Defects4J-v2.0 và QuixBugs bổ sung, cho thấy khả năng tổng quát hóa của GAMMA trong việc giải quyết vấn đề overfitting tập dữ liệu. Chúng tôi cũng chứng minh rằng việc áp dụng các mô hình ngôn ngữ được huấn luyện trước khác có thể mang lại tiến bộ đáng kể, ví dụ, GAMMA dựa trên CodeBERT và ChatGPT có thể sửa 80 và 67 lỗi trên Defects4J-v1.2, cho thấy khả năng mở rộng của GAMMA. Tổng thể, nghiên cứu của chúng tôi nhấn mạnh tương lai đầy hứa hẹn của việc áp dụng các mô hình được huấn luyện trước để tạo ra các bản vá chính xác dựa trên các mẫu sửa chữa trong thực tế.

Từ khóa chỉ mục—Sửa chữa Chương trình Tự động, Mẫu Sửa chữa, Mô hình Được huấn luyện trước, LLM4SE

I. GIỚI THIỆU

Độ phức tạp và kích thước của các hệ thống phần mềm hiện đại đang liên tục gia tăng, dẫn đến số lượng lỗi phần mềm tăng vọt [1], [2]. Các lỗi phần mềm có tác động có hại đến việc phát triển phần mềm, vì chúng gây ra trải nghiệm khó chịu cho người dùng và đôi khi có thể gây ra tổn thất tài chính lớn cho các nhà phát triển [3]. Một lượng thời gian và ngân sách đáng kể được dành cho việc xác định và sửa các lỗi phần mềm này một cách thủ công [4]. Để tạo điều kiện cho quá trình gỡ lỗi thủ công, sửa chữa chương trình tự động (APR), nhằm mục đích tự động tạo ra các bản vá chính xác cho các đoạn mã lỗi đã được xác định, đang nhận được sự chú ý ngày càng tăng từ cả học thuật và công nghiệp [5], [6], như Meta [7], Google [8] và Microsoft [9], [10].

Trong tài liệu, nhiều kỹ thuật APR khác nhau đã được đề xuất để tạo ra các bản vá, như dựa trên heuristic [11], [12], dựa trên ràng buộc [13], [14], dựa trên mẫu [15], [16]. Trong số các kỹ thuật APR truyền thống này, APR dựa trên mẫu, sử dụng các mẫu sửa chữa được tạo thủ công bởi các chuyên gia để chuyển đổi các đoạn mã lỗi thành các đoạn mã chính xác, đã được nghiên cứu rộng rãi và được công nhận là hiện đại [17]–[19]. Các bản vá ứng cử viên thường được tạo ra bằng cách tận dụng hai loại thành phần sửa chữa (tức là, các mẫu sửa chữa và mã nguồn donor) được tìm thấy trong các cơ sở mã hiện có. Mẫu sửa chữa đại diện cho các hành động thay đổi mã phổ biến (ví dụ, chèn một câu lệnh If) và mã nguồn donor đại diện cho các đoạn mã (ví dụ, các token định danh như tên phương thức) để cụ thể hóa các bản vá được hướng dẫn bởi các mẫu trừu tượng. Một khối lượng lớn các nghiên cứu đã được dành cho các sơ đồ trích xuất mẫu, như các mẫu được trích xuất thủ công và các mẫu khai thác tự động [20]–[23]. Ví dụ, công cụ APR dựa trên mẫu hiện đại TBar [17] tập trung vào tệp lỗi cục bộ và tận dụng ngữ cảnh của mã lỗi để loại bỏ mã nguồn donor không liên quan. Các công trình trước đây [24], [25] đã chỉ ra rằng một số lượng đáng kể các lỗi không thể được sửa vì mã nguồn donor liên quan không có sẵn trong tệp cục bộ. Do đó, TBar có thể không tạo ra được các bản vá chính xác với mã nguồn donor không phù hợp mặc dù mẫu sửa chữa phù hợp với các hành động thay đổi mã chính xác.

Trong bài báo này, chúng tôi đề xuất một công cụ APR dựa trên mẫu mới được gọi là GAMMA bằng cách kết hợp những tiến bộ của các mẫu sửa chữa và các mô hình ngôn ngữ được huấn luyện trước. Hiểu biết chính là xem xét các mô hình được huấn luyện trước có thể thu được kiến thức tổng quát đầy đủ về ngôn ngữ lập trình từ tất cả các dự án mã nguồn mở có thể trong tự nhiên, chúng ta có thể trực tiếp sử dụng các mô hình như vậy để truy xuất mã nguồn donor liên quan từ mẫu sửa chữa và ngữ cảnh mã xung quanh. Cụ thể, chúng tôi đầu tiên thu thập và tóm tắt một tập hợp siêu của các mẫu sửa chữa được rút ra từ công việc dựa trên mẫu trước đây (ví dụ, TBar). Sau đó chúng tôi chuyển đổi các mẫu sửa chữa này thành các mẫu dựa trên điền lỗ hổng, thay thế mã nguồn donor bằng một số token bị che để được điền. Cuối cùng, chúng tôi thực hiện một nhiệm vụ dự đoán mặt nạ trên các mẫu sửa chữa dựa trên điền lỗ hổng với sự hỗ trợ của các mô hình được huấn luyện trước theo cách điền vào chỗ trống, tức là, dự đoán mã nguồn donor chính xác cho các token bị che. Mặc dù GAMMA về mặt khái niệm có thể tổng quát hóa cho nhiều mô hình được huấn luyện trước, chúng tôi đã triển khai GAMMA dựa trên một mô hình ngôn ngữ được huấn luyện trước gần đây, UniXcoder [26]. Không giống như các công cụ APR dựa trên mẫu hiện tại thường truy xuất các thành phần sửa chữa trong tệp lỗi cục bộ, GAMMA trực tiếp sử dụng kiến thức chung được huấn luyện trước với hàng triệu đoạn mã từ các dự án mã nguồn mở, cho phép nó cung cấp nhiều mã nguồn donor để sửa các lỗi khác nhau.

Chúng tôi tiến hành các thí nghiệm mở rộng để so sánh GAMMA với các phương pháp APR hiện đại (bao gồm cả truyền thống và dựa trên học máy) trên benchmark Defects4J-v1.2 được áp dụng rộng rãi. Kết quả thí nghiệm cho thấy GAMMA có thể vượt trội hơn tất cả các phương pháp APR hiện có, cải thiện số lượng lỗi được sửa chính xác lên 82 với độ chính xác 81.19%, và 14 lỗi duy nhất mà không có công việc trước đây nào có thể sửa, đây là một biên giới mới trong lĩnh vực APR. Bên cạnh đó, GAMMA sửa 45 và 22 lỗi trên Defects4J-v2.0 và QuixBugs bổ sung, nhiều hơn 27 và 5 so với kỹ thuật dựa trên học máy hiện đại Recoder, cho thấy GAMMA có thể giải quyết tốt vấn đề overfitting tập dữ liệu quan trọng. Hơn nữa, chúng tôi triển khai GAMMA với CodeBERT [27] và ChatGPT [28], và thấy 80 và 67 lỗi được sửa chính xác trên Defects4J-v1.2. Kết quả cho thấy GAMMA với các mô hình được huấn luyện trước khác có thể cung cấp thêm tiến bộ đáng kể, nhấn mạnh khả năng tổng quát hóa của GAMMA.

Tóm lại, những đóng góp của bài báo này như sau:
• Chiều hướng mới. Chúng tôi thu hẹp khoảng cách giữa những tiến bộ trong các mô hình được huấn luyện trước gần đây và APR dựa trên mẫu. Khác với APR dựa trên mẫu hiện có truy xuất mã nguồn donor từ các tệp lỗi cục bộ và APR dựa trên học máy hiện có tạo ra đoạn mã đã vá từ đầu, công việc của chúng tôi cho thấy chúng ta có thể tận dụng các mô hình được huấn luyện trước để tạo ra các token mã chính xác trong một mẫu sửa chữa cho trước. Quan trọng hơn, công việc của chúng tôi tiết lộ tiềm năng tận dụng các mô hình được huấn luyện trước để giải quyết vấn đề thành phần sửa chữa quan trọng trong APR dựa trên mẫu.

• Công cụ APR mới. Chúng tôi đề xuất GAMMA, tận dụng mô hình ngôn ngữ được huấn luyện trước lớn để tạo ra mã chính xác với sự hỗ trợ của các mẫu sửa chữa mà không cần bất kỳ cặp sửa lỗi lịch sử bổ sung nào để huấn luyện. Chúng tôi định nghĩa một tập hợp các mẫu sửa chữa ở định dạng điền vào chỗ trống và tận dụng mục tiêu huấn luyện trước gốc của các mô hình được huấn luyện trước để dự đoán các token thực tế bị che. Xem xét nhiệm vụ điền vào chỗ trống có thể tận dụng nhiều mô hình ngôn ngữ được huấn luyện trước khác nhau, GAMMA mang tính tổng quát về khái niệm và có thể được triển khai với các mô hình được huấn luyện trước khác nhau trong thực tế.

• Nghiên cứu mở rộng. Chúng tôi tiến hành một nghiên cứu thực nghiệm để điều tra hiệu quả của GAMMA so với các kỹ thuật APR truyền thống và dựa trên học máy hiện đại. Kết quả trên Defects4J-v1.2 được áp dụng rộng rãi cho thấy GAMMA có thể sửa 82 lỗi và 14 trong số chúng không thể được sửa bởi các công cụ APR hiện có, tạo ra một đường cơ sở mới cao hơn về hiệu suất sửa chữa. Quan trọng hơn, GAMMA sửa 45 và 22 lỗi trên Defects4J-v2.0 và QuixBugs mới phát triển, cho thấy GAMMA có thể tránh vấn đề overfitting tập dữ liệu quan trọng của các kỹ thuật APR hiện có. Hơn nữa, chúng tôi áp dụng các mô hình được huấn luyện trước khác nhau (ví dụ, ChatGPT) để điều tra thêm khả năng tổng quát hóa của GAMMA.

• Tạo phẩm có sẵn. Để hỗ trợ cộng đồng khoa học mở, chúng tôi phát hành các tài liệu liên quan (bao gồm mã nguồn, kết quả thí nghiệm và các bản vá chính xác) trong thí nghiệm của chúng tôi để sao chép và nghiên cứu tương lai [29].

II. BỐI CẢNH VÀ ĐỘNG CƠ

A. Sửa chữa Chương trình Tự động

Là một kỹ thuật đầy hứa hẹn để chuyển việc gỡ lỗi thủ công nặng nề sang việc tạo bản vá tự động hiệu quả, APR đã phát triển nhanh chóng và nhận được nhiều sự chú ý từ nhiều cộng đồng nghiên cứu, như kỹ thuật phần mềm, bảo mật phần mềm và trí tuệ nhân tạo [6], [30]. Quy trình làm việc của APR thường bao gồm ba giai đoạn: (1) định vị lỗi, tức là các kỹ thuật định vị lỗi có sẵn được sử dụng để xác định một danh sách được xếp hạng các phần tử mã đáng ngờ, với sự hỗ trợ của chúng APR có thể tập trung vào một vùng mã nhỏ, do đó giảm khối lượng công việc [31]; (2) tạo bản vá, tức là các bản vá ứng cử viên được tạo ra bằng cách áp dụng một tập hợp các quy tắc chuyển đổi cho các đoạn mã đáng ngờ [32]; và (3) xác thực bản vá, tức là các bộ test có sẵn được sử dụng làm thông số kỹ thuật chương trình để kiểm tra tính đúng đắn của các bản vá ứng cử viên [33]. Các bản vá ứng cử viên vượt qua tất cả các bộ test có sẵn được coi là những bản vá hợp lý. Các bản vá hợp lý tương đương về mặt ngữ nghĩa với bản vá của nhà phát triển bằng kiểm tra thủ công được coi là những bản vá chính xác; nếu không thì là những bản vá overfitting [34], [35].

Trong tài liệu, như thành phần cốt lõi của nghiên cứu APR, một khối lượng lớn nỗ lực nghiên cứu được dành cho việc tạo ra các bản vá từ các khía cạnh khác nhau, bao gồm truyền thống và dựa trên học máy. Cụ thể, các kỹ thuật APR truyền thống có thể được phân loại thành dựa trên heuristic [11], [12], dựa trên ràng buộc [13], [14], dựa trên mẫu [15], [16]. Trong số chúng, APR dựa trên mẫu được chứng minh đạt được hiệu suất tốt nhất, bao gồm hai thành phần sửa chữa, tức là các mẫu sửa chữa và mã nguồn donor. Các mẫu sửa chữa được tạo thủ công bởi các chuyên gia để biểu thị những thay đổi mã phổ biến, và mã nguồn donor được truy xuất trong các tệp lỗi để biểu thị các token mã chính xác thực tế. GAMMA nhằm mục đích sửa đổi mã nguồn donor quan trọng bằng cách sử dụng các mô hình ngôn ngữ được huấn luyện trước theo cách điền vào chỗ trống.

So với các kỹ thuật APR truyền thống, các kỹ thuật dựa trên học máy xử lý vấn đề sửa chữa chương trình như một nhiệm vụ dịch máy thần kinh (NMT), dịch một chuỗi mã từ ngôn ngữ nguồn (tức là, các đoạn mã lỗi) sang ngôn ngữ đích (tức là, các đoạn mã chính xác). Các mô hình sửa chữa NMT hiện có thường được xây dựng trên kiến trúc encoder-decoder [36]. Encoder trích xuất trạng thái ẩn của các đoạn mã lỗi với ngữ cảnh cần thiết, và decoder lấy trạng thái ẩn của encoder và tạo ra các đoạn mã chính xác [22], [37], [38]. Nhờ khả năng mạnh mẽ của DL trong việc học các mối quan hệ ẩn và phức tạp từ các corpus mã lớn, các kỹ thuật APR dựa trên học máy đã đạt được hiệu suất đáng kể trong vài năm gần đây. Mặc dù các kỹ thuật APR dựa trên học máy đã chứng minh tương lai đầy hứa hẹn của chúng, chúng vẫn bị hạn chế bởi chất lượng và số lượng các cặp sửa lỗi lịch sử để huấn luyện [18]. Chúng tôi xem GAMMA như một kỹ thuật APR dựa trên học máy mới cố gắng thúc đẩy các kỹ thuật APR truyền thống bằng cách sử dụng công nghệ học sâu. Tuy nhiên, khác với hầu hết APR dựa trên học máy hiện có xem việc tạo bản vá như một nhiệm vụ NMT end-to-end với một số lượng hạn chế các cặp sửa lỗi làm dữ liệu huấn luyện, GAMMA tích hợp các mô hình ngôn ngữ được huấn luyện trước vào APR dựa trên mẫu và chỉ dự đoán các token mã bị che với một kịch bản học zero-shot.

B. Mô hình Được huấn luyện trước

Gần đây, các mô hình ngôn ngữ được huấn luyện trước (ví dụ, UniXcoder [26] và ChatGPT [28]) đã cải thiện đáng kể hiệu suất trên một loạt các nhiệm vụ liên quan đến mã [39], [40]. Các mô hình này được huấn luyện trước bằng huấn luyện tự giám sát trên các corpus lớn không được gắn nhãn và sau đó được tinh chỉnh bằng huấn luyện có giám sát trên các corpus hạn chế để tăng cường hiệu suất trên nhiều nhiệm vụ downstream. Trong quá trình huấn luyện trước, một mục tiêu mô hình hóa ngôn ngữ bị che thường được sử dụng để rút ra các biểu diễn ngôn ngữ chung từ dữ liệu huấn luyện lớn không được gắn nhãn [41], tức là, một tỷ lệ nhỏ các token được thay thế bằng các token bị che, và mục tiêu huấn luyện là dự đoán các giá trị gốc của các token bị che.

Các mô hình được huấn luyện trước hiện có thường áp dụng kiến trúc encoder-decoder, trong đó cái trước mã hóa một chuỗi đầu vào thành một biểu diễn vector có độ dài cố định, và cái sau tạo ra một chuỗi đầu ra dựa trên biểu diễn đầu vào. Các mô hình này có thể được phân loại chung thành ba kiến trúc: chỉ encoder, chỉ decoder, và encoder-decoder [42]. Các mô hình chỉ encoder (ví dụ, CodeBERT [41]) thường huấn luyện trước một transformer hai chiều trong đó mỗi token có thể chú ý đến nhau. Các mô hình chỉ decoder (ví dụ, GPT [43]) được huấn luyện trước sử dụng mô hình hóa ngôn ngữ một chiều chỉ cho phép các token chú ý đến các token trước đó và chính chúng để dự đoán token tiếp theo. Các mô hình encoder-decoder (ví dụ, UniXcoder [26]) thường sử dụng các mục tiêu huấn luyện trước khử nhiễu làm hỏng đầu vào nguồn và yêu cầu decoder khôi phục chúng.

Trong công việc này, chúng tôi chọn UniXcoder để truy xuất mã nguồn donor thông qua một nhiệm vụ dự đoán mặt nạ. UnixCoder được huấn luyện trước sử dụng mục tiêu MLM có thể được sử dụng để trực tiếp tạo ra các token mã bị che từ mẫu sửa chữa phù hợp và ngữ cảnh mã xung quanh. Bên cạnh đó, CodeBERT và ChatGPT được sử dụng để điều tra khả năng tổng quát hóa của GAMMA.

C. Mẫu Sửa chữa

Các mẫu sửa chữa được sử dụng rộng rãi trong cộng đồng APR [17]. Một mẫu sửa chữa là một quy tắc chuyển đổi mã được định nghĩa trước đại diện cho một thay đổi mã phổ biến trong quá trình sửa lỗi. Hiểu biết đằng sau các mẫu sửa chữa là nhiều lỗi phần mềm có bản chất tương tự [22]. Do đó, với các mẫu sửa chữa được tóm tắt từ các lỗi trước đây, có thể tự động sửa một số mã bị lỗi khác [24].

Trong tài liệu, có một số chiến lược để truy cập các mẫu sửa chữa: (1) khai thác mẫu thủ công [44], [45], tức là, thông qua việc thực hiện phân tích trên các lỗi hiện có cũng như các bản vá liên quan của chúng, các nhà phát triển có kinh nghiệm có thể xác định những thay đổi mã tương tự và biến chúng thành các mẫu sửa chữa. (2) học máy [15], [20], tức là, các phương pháp học được sử dụng để các mẫu sửa chữa có thể được tạo ra tự động. (3) phân tích tĩnh [16], [46], tức là, các mẫu sửa chữa được tạo ra từ nhiều loại cảnh báo được đưa ra bởi các công cụ phân tích tĩnh.

Quá trình áp dụng một mẫu sửa chữa để tạo bản vá thường bao gồm hai bước. Đầu tiên, công cụ APR chọn một mẫu sửa chữa phù hợp dựa trên biểu diễn cây cú pháp trừu tượng (AST) của mã lỗi. Các mẫu sửa chữa được chọn theo các loại node trong AST, và các lỗi được sửa bằng cách đột biến các node đích. Thứ hai, công cụ APR tạo ra một phiên bản đã sửa chữa của mã lỗi bằng cách tìm kiếm và áp dụng mã nguồn donor liên quan cho mẫu sửa chữa.

D. Ví dụ Động cơ

Để minh họa tốt hơn hạn chế của APR dựa trên mẫu hiện có, chúng tôi trình bày thêm một ví dụ động cơ trong phần này. Như được hiển thị trong Listing 1, chúng tôi sử dụng một lỗi thực tế Closure-92 từ benchmark được sử dụng rộng rãi Defects4J-v1.2 làm ví dụ. Closure-92 biểu thị phiên bản lỗi thứ 92 của dự án Google Closure Compiler trong Defects4J-v1.2. Lỗi này được sửa thành công bởi GAMMA, trong khi TBar không tạo ra được bản vá chính xác. Để sửa lỗi này, tên phương thức "indexOf" được thay thế bằng "lastIndexOf". Mẫu sửa chữa được sử dụng ở đây là để đột biến tên phương thức. TBar áp dụng các mẫu sửa chữa đã chọn cho mã nguồn một cách ngây thơ. Trong trường hợp này, TBar tìm kiếm tất cả các phương thức xuất hiện trong tệp cục bộ nơi lỗi được định vị, và thay thế phương thức lỗi bằng tất cả các phương thức khác có cùng kiểu trả về từng cái một. Kết quả là, TBar không thể tạo ra tên phương thức không tồn tại trong tệp gốc, như "lastIndexOf" trong ví dụ này, hạn chế hiệu suất sửa chữa của nó. Khác với TBar, chúng tôi thay thế tên phương thức "indexOf" bằng một token mặt nạ (tức là <mask>) thay vào đó, và truy vấn mô hình được huấn luyện trước UniXcoder để điền vào mặt nạ với mẫu sửa chữa và ngữ cảnh tương ứng.

Dựa trên ví dụ, chúng ta có thể quan sát rằng, mặc dù mẫu sửa chữa chính xác được chọn, như một APR truyền thống hiện đại, TBar vẫn không tạo ra được bản vá chính xác với mã nguồn donor không phù hợp (tức là, "lastIndexOf"). Hiệu quả của APR dựa trên mẫu phụ thuộc lớn vào mã nguồn donor, đề cập đến các token mã (ví dụ, tên biến) có thể được kết hợp với mẫu sửa chữa để tạo ra một bản vá hoàn chỉnh. Mã nguồn donor có thể được truy cập trong các phạm vi khác nhau của chương trình lỗi (ví dụ một phương thức, một tệp, hoặc một gói), nhưng đối với một số lỗi, mã nguồn donor chính xác không thể được tìm thấy ngay cả khi toàn bộ chương trình được tìm kiếm. Ví dụ, công việc trước đây cho thấy một nửa số lỗi từ benchmark Defects4J không thể được sửa vì mã nguồn donor liên quan không có sẵn trong không gian tìm kiếm [24]. Do đó, những lỗi này không thể được sửa chính xác bởi các công cụ APR dựa trên mẫu như TBar [17], chỉ xác định mã nguồn donor trong tệp cục bộ. Với một không gian tìm kiếm lớn hơn (ví dụ, tìm kiếm mã nguồn donor từ các dự án khác), có thể có nhiều cơ hội hơn để sửa những lỗi này. Tuy nhiên, chiến lược như vậy dẫn đến vấn đề bùng nổ không gian tìm kiếm và thời gian tìm kiếm không thể chấp nhận được, giảm hiệu quả sửa chữa. Trong bài báo này, chúng tôi sử dụng các mô hình ngôn ngữ được huấn luyện trước để truy xuất mã nguồn donor liên quan. Các mô hình này đã học kiến thức ngôn ngữ lập trình từ một số lượng lớn chương trình trong tự nhiên, giúp có thể sửa các lỗi yêu cầu mã nguồn donor từ bên ngoài chương trình lỗi.

III. PHƯƠNG PHÁP

Để đánh giá hiệu quả của các thành phần sửa chữa, chúng tôi xây dựng GAMMA, một công cụ APR dựa trên mẫu kết hợp các mẫu sửa chữa được sử dụng thường xuyên và các mô hình ngôn ngữ được huấn luyện trước. Hình 1 trình bày quy trình làm việc của GAMMA. Cho một chương trình lỗi và một tập hợp các bộ test khiến chương trình thất bại, một danh sách các phần tử mã đáng ngờ được trả về bởi các phương pháp định vị lỗi (tức là, giai đoạn định vị lỗi). Dựa trên corpus mẫu sửa chữa hiện có, GAMMA sau đó chọn các mẫu sửa chữa phù hợp cho các phần tử đáng ngờ (tức là, giai đoạn lựa chọn mẫu) và truy vấn các mô hình được huấn luyện trước để truy xuất mã nguồn donor thông qua một mặt nạ dự đoán mặt nạ (tức là, giai đoạn tạo bản vá). GAMMA cuối cùng sử dụng các bộ test có sẵn làm oracle để kiểm tra các bản vá được tạo ra và trả về các bản vá hợp lý để kiểm tra thủ công (tức là, giai đoạn xác thực bản vá). Xem xét rằng định vị lỗi thường được phát triển như một lĩnh vực độc lập và các kỹ thuật APR hiện có sử dụng các công cụ định vị lỗi có sẵn trong pipeline sửa chữa, chúng tôi không thảo luận về định vị lỗi dưới đây. Chúng tôi mô tả vai trò và hoạt động của các giai đoạn khác cũng như tất cả các chi tiết triển khai cần thiết.

A. Định nghĩa Mẫu Mặt nạ

Trong tài liệu, nhiều mẫu sửa chữa được thiết kế dựa trên tóm tắt thủ công hoặc khai thác tự động. Dựa trên phương pháp dựa trên mẫu hiện đại TBar [17], chúng tôi kiểm tra thủ công tất cả các mẫu sửa chữa và chuyển đổi chúng thành các mẫu dựa trên điền lỗ hổng. Chúng tôi hiển thị các mẫu liên quan cũng như cách chúng được áp dụng cho mã lỗi, hoặc cách các câu lệnh với token mặt nạ được tạo ra dựa trên các mẫu này.

T1: Kiểm tra Biểu thức Cast. Thêm một kiểm tra instanceof xung quanh một câu lệnh khi nó chứa một biểu thức cast không được kiểm tra.
+ if(exp instanceof T){
var=(T) exp ...
+ }

T2: Đột biến Biểu thức Điều kiện: Đột biến một biểu thức trả về một giá trị boolean bằng cách loại bỏ một phần của biểu thức, thay thế nó bằng mặt nạ hoặc thêm mặt nạ mới.
Loại bỏ biểu thức:
- condExp1 op condExp2
+ condExp1
Cập nhật biểu thức:
- condExp1 op condexp2
+ condExp1 op <mask>
Thêm biểu thức:
- condExp1
+ condExp1 <mask>
trong đó condExp biểu thị các biểu thức điều kiện và Op biểu thị toán tử logic (tức là, ∥ hoặc &&).

T3: Đột biến Kiểu Dữ liệu: Sử dụng một hoặc nhiều mặt nạ để thay thế các kiểu dữ liệu trong khai báo biến hoặc các node biểu thức cast.
- T var = ...
+ <mask> var = ...
- ... (T) exp ...
+ ... (<mask>) exp ...
trong đó cả T đều biểu thị một kiểu dữ liệu và exp biểu thị biểu thức đang được cast (ví dụ, biến).

T4: Đột biến Biểu thức Literal: Thay thế các biểu thức literal, bao gồm literal số, literal chuỗi, literal boolean, v.v. bằng mặt nạ.
- ...literal...
+ ...<mask>...

T5: Đột biến Lời gọi Phương thức. Đột biến các biểu thức lời gọi phương thức bằng cách thay đổi tên phương thức hoặc đối số.
Thay thế tên phương thức:
- method(...)
+ <mask>(...)
Chèn đối số:
- method(arg)
+ method(arg,<mask>)
Loại bỏ đối số:
- method(arg1,arg2)
+ method(arg)
Thay thế đối số:
- method(arg)
+ method(<mask>)

T6: Kiểm tra Null Pointer: Thêm một kiểm tra null vào một câu lệnh chứa một biểu thức có thể là null.
Bỏ qua null point:
+ if (exp != null){
...exp...
+ }
Chèn return:
+ if (exp == null){
+ return <mask>;
+ }
...exp...
Continue:
+ if (exp == null){
+ continue;
+ }
...exp...
Ném exception:
+ if (exp == null){
+ throw new IllegalArgumentException();
+ }
...exp...
Gán lại:
+ if (exp == null){
+ exp=<mask>
+ }
...exp...

T7: Đột biến Toán tử: Thay thế một toán tử trong một câu lệnh bằng mặt nạ hoặc thay đổi độ ưu tiên của các phép toán.
Thay đổi độ ưu tiên:
- (exp1 op1 exp2) op2 exp3
+ exp1 op1 (exp2 op2 exp3)
Thay thế toán tử:
- exp1 op exp2
+ exp1 <mask> exp2

T8: Kiểm tra Phạm vi Mảng: Kiểm tra phạm vi của chỉ số trước khi truy cập một phần tử trong một mảng.
+ if (index<array.length) {
...array[index]...
+ }

T9: Đột biến Câu lệnh Return: Thay thế biểu thức (ví dụ, literal, biến và biểu thức điều kiện) được trả về trong một phương thức bằng mặt nạ.
- return exp;
+ return <mask>;

T10: Chèn Câu lệnh: Chèn câu lệnh return, câu lệnh try catch, câu lệnh if, lời gọi phương thức, hoặc đơn giản là một số mặt nạ vào câu lệnh hiện có.
Câu lệnh Return:
+ return <mask>;
statement;
Câu lệnh Try-catch:
+ try{
statement;
+ } catch(Exception e){}
Câu lệnh If:
+ if (<mask>) {
statement;
+ }
Câu lệnh đơn giản:
+ <mask>;
statement;

T11: Loại bỏ Câu lệnh: Trực tiếp xóa một hoặc nhiều câu lệnh lỗi từ mã gốc.
- statement;

T12: Thay thế Biến: Thay thế một biến trong một câu lệnh lỗi bằng mặt nạ.
- ...var...
+ ...<mask>...

T13: Di chuyển câu lệnh: Di chuyển một câu lệnh từ vị trí gốc của nó đến vị trí mới.
- statement;
...
+ statement;

B. Lựa chọn Mẫu

Dựa trên các mẫu sửa chữa mặt nạ đã định nghĩa, GAMMA xác định mẫu nào nên được áp dụng cho các câu lệnh lỗi đầu vào. Tương tự như TBar [17], GAMMA sử dụng một phương pháp khớp dựa trên AST với chiến lược depth-first. Chúng tôi đầu tiên tạo ra một AST cho đầu vào, và sau đó tất cả các node trong AST được duyệt qua. Nếu AST chứa một loại node được yêu cầu trong một mẫu cụ thể, chúng tôi áp dụng mẫu cho lỗi. Chúng tôi đưa ra một ví dụ về lỗi Closure-10 từ Defects4J-v1.2 trong Listing 2 và cách chúng tôi tạo ra bản vá của nó trong Hình 1. Trong AST của dòng đầu vào, chúng tôi tìm thấy một node Method Invocation trong AST của dòng đầu vào, vì vậy chúng tôi chọn mẫu T5: Mutate Method Invocation. Đối với mẫu Method name replacement thay đổi tên của một phương thức, sau khi định vị tên phương thức allResultsMatch, chúng tôi thay thế nó bằng một token mặt nạ, sẽ được dự đoán trong giai đoạn tiếp theo. Đáng chú ý là GAMMA được xây dựng dựa trên UniXCoder, có thể dự đoán một chuỗi các token mã dựa trên một token bị che. Do đó, chúng tôi không cần xem xét có bao nhiêu token nên được che trong quá trình tạo bản vá và chỉ sử dụng một token mặt nạ trong mẫu sửa chữa đã chọn, khác với các mô hình được huấn luyện trước khác, như CodeBERT được sử dụng trong AlphaRepair [18] (được thảo luận trong Phần V-C). Có thể có nhiều mẫu sửa chữa phù hợp cho một đoạn mã lỗi cùng một lúc. Trong trường hợp này, chúng tôi dừng việc lựa chọn các mẫu sửa chữa ngay khi bản vá chính xác đầu tiên được tạo ra.

C. Tạo Bản vá với Dự đoán Mặt nạ

Sau khi chọn một mẫu sửa chữa phù hợp cho một mã lỗi, chúng tôi sử dụng UniXcoder [26] để tạo ra các token mã chính xác thông qua định dạng điền vào chỗ trống. Để làm điều này, chúng tôi tận dụng mục tiêu huấn luyện gốc của mô hình hóa ngôn ngữ bị che trong UniXcoder. UniXcoder là một mô hình được huấn luyện trước tiên tiến cho các ngôn ngữ lập trình hỗ trợ các nhiệm vụ hiểu và tạo mã. Nó chứa một mục tiêu huấn luyện trước của Masked Language Modeling (MLM), được thiết kế để dự đoán một số token đã bị che đi. Chúng tôi tận dụng nhiệm vụ huấn luyện trước này để hoàn thành các cloze được tạo ra trong bước trước mà không cần tinh chỉnh nào, để có thể tạo ra các bản vá ứng cử viên cho các chương trình lỗi.

Việc dự đoán cho mặt nạ phụ thuộc lớn vào các token xung quanh mặt nạ. Nếu token chính xác xuất hiện trong đầu vào cho mô hình, nó có nhiều khả năng được chọn làm một trong những kết quả có thể. Độ chính xác của dự đoán mặt nạ khá hạn chế khi chỉ có một dòng lỗi bị che duy nhất được đưa ra mà không có bất kỳ ngữ cảnh nào của mã, nơi có thể có một số thông tin hữu ích cho việc sửa lỗi. Để có thêm ngữ cảnh của dòng lỗi trong mỗi lỗi, chúng tôi trích xuất phương thức chứa dòng đó và sử dụng toàn bộ phương thức với dòng lỗi bị che làm đầu vào cho UniXcoder. Xem xét rằng một số token trong dòng lỗi đã được thay thế bằng mặt nạ nhưng những token này cũng có thể chứa thông tin cần thiết cho dự đoán mặt nạ, trong dòng đầu tiên của đầu vào của chúng tôi, chúng tôi thêm dòng lỗi gốc dưới dạng bình luận (tức là, thêm "//" ở phía trước dòng). Dòng lỗi được bình luận theo sau bởi phương thức mà lỗi nằm trong đó cùng nhau tạo thành đầu vào cuối cùng. Đối với mỗi đầu vào, N bản vá ứng cử viên được tạo ra bởi mô hình UniXcoder. N là kích thước beam và là một tham số có thể điều chỉnh của GAMMA. Kích thước beam tương đối lớn tăng khả năng tạo ra các bản vá chính xác.

D. Xác thực Bản vá

Sau khi một bản vá ứng cử viên cho một lỗi nhất định được tạo ra bởi GAMMA, chúng tôi áp dụng các thay đổi tương ứng cho chương trình lỗi. Theo thực hành trong cộng đồng APR [18], [47], chúng tôi đầu tiên biên dịch lại chương trình đã vá và lọc ra bất kỳ bản vá nào không biên dịch được. Sau đó chúng tôi thực thi chương trình đã vá đối với bộ test có sẵn để xác định các bản vá hợp lý vượt qua thành công tất cả các bộ test. Đối với những bản vá hợp lý đó, chúng tôi kiểm tra chúng thủ công để đảm bảo các chương trình được sửa chính xác, tức là, liệu các bản vá có tương đương về mặt ngữ nghĩa với các bản vá của nhà phát triển hay không.

IV. THIẾT LẬP THỰC NGHIỆM

A. Câu hỏi Nghiên cứu

Trong bài báo này, chúng tôi nghiên cứu các câu hỏi nghiên cứu sau:
RQ1: Hiệu suất của GAMMA so với các phương pháp APR hiện đại như thế nào?
RQ2: Khả năng tổng quát hóa của GAMMA trong việc sửa chữa các lỗi thực tế bổ sung như thế nào?
RQ3: Khả năng mở rộng của GAMMA khi sử dụng các mô hình được huấn luyện trước tiên tiến khác như thế nào?

B. Benchmark

Để đánh giá hiệu suất sửa chữa, chúng tôi sử dụng benchmark tiêu chuẩn Defects4J-v1.2 [48] trong cộng đồng APR. Defects4J-v1.2 là một bộ sưu tập các lỗi thực tế từ các dự án mã nguồn mở và được áp dụng rộng rãi bởi các phương pháp APR truyền thống [17], [49] và dựa trên học máy hiện có [38], [39], [47], [50]. Cụ thể, Defects4J-v1.2 chứa 395 lỗi đã biết và có thể tái tạo, mỗi lỗi chứa một phiên bản lỗi và một phiên bản đã sửa, cũng như một bộ test tương ứng kích hoạt lỗi đó để xác thực bản vá. Đánh giá trên Defects4J-v1.2 có thể phản ánh hiệu suất của GAMMA trong một kịch bản gỡ lỗi thực tế và cung cấp đủ kết quả so sánh với hầu hết các kỹ thuật APR hiện có.

Bên cạnh đó, chúng tôi chọn Defects4J-v2.0 và QuixBugs làm các benchmark lỗi khác để đánh giá, nhằm điều tra khả năng tổng quát hóa của GAMMA. Defects4J-v2.0 cung cấp 420 lỗi thực tế bổ sung từ 17 dự án Java, được áp dụng bởi một số nghiên cứu APR gần đây [18], [50]. QuixBugs [51] là một tập dữ liệu sửa lỗi song song đa ngôn ngữ trong Python và Java được sử dụng trong [18], [39]. QuixBugs chứa 40 thuật toán cổ điển nhỏ với một lỗi trên một dòng, cùng với bộ test kích hoạt lỗi.

C. Baseline

Để cho phép đánh giá đầy đủ, chúng tôi so sánh GAMMA với cả các phương pháp APR truyền thống và dựa trên học máy. Chúng tôi chọn bảy công cụ APR dựa trên học máy gần đây, tức là AlphaRepair [18], Recoder [50], CURE [47], CoCoNuT [52], CIRCLE [39], DLFix [38], và SequenceR [53]. Chúng tôi cũng chọn hai công cụ APR dựa trên mẫu hiện đại TBar [17] và PraPR [54] làm đại diện cho APR truyền thống. Tổng cộng, chúng tôi đánh giá GAMMA đối với chín công cụ APR tiên tiến từ các danh mục khác nhau. Mặc dù cấu hình định vị lỗi là một phần quan trọng của APR, chúng tôi không xem xét nó trong thí nghiệm của chúng tôi vì các sai lệch tiềm ẩn mà định vị lỗi có thể mang lại. Theo các nghiên cứu APR gần đây [38], [39], [47], [50], chúng tôi áp dụng định vị lỗi hoàn hảo bằng cách nhập các dòng lỗi chính xác vào các kỹ thuật APR khác nhau để chuẩn hóa tác động của định vị lỗi đối với hiệu suất sửa chữa, được thảo luận trong Phần VI.

D. Chỉ số Đánh giá

Chúng tôi sử dụng hai chỉ số phổ biến để đánh giá hiệu suất của tất cả các phương pháp APR liên quan [5], [6], tức là bản vá hợp lý và bản vá chính xác. Cái đầu tiên sửa chức năng lỗi mà không làm hại chức năng chính xác khác (tức là, vượt qua tất cả các bộ test có sẵn), và cái thứ hai tương đương về mặt ngữ nghĩa hoặc cú pháp với bản vá của nhà phát triển (tức là, tổng quát hóa bộ test tiềm năng). Chúng tôi kiểm tra thủ công từng bản vá hợp lý để xác định xem nó có phải là bản vá chính xác hay không bằng cách tuân theo thực hành tiêu chuẩn trong nghiên cứu APR.

E. Chi tiết Triển khai

Ở giai đoạn lựa chọn mẫu sửa chữa, chúng tôi áp dụng Eclipse JDT để phân tích dòng đầu vào thành AST, và sau đó AST được duyệt qua để kiểm tra xem nó có chứa bất kỳ node nào được yêu cầu bởi một mẫu sửa chữa hay không. Có một số mẫu có thể phù hợp với tất cả các dòng lỗi đầu vào. Ví dụ, mẫu T10: Insert Statements chỉ yêu cầu thêm các câu lệnh xung quanh dòng lỗi và không đột biến bất kỳ node nào trong AST. Các mẫu như vậy được áp dụng trực tiếp cho tất cả các đầu vào mà không cần kiểm tra AST.

Trong giai đoạn dự đoán mặt nạ, chúng tôi chọn mô hình UniXcoder "unixcoder-base". Đây là một mô hình được huấn luyện trước trên các cặp ngôn ngữ tự nhiên-ngôn ngữ lập trình (NL-PL) và được báo cáo trong bài báo UniXcoder gốc [26]. Chúng tôi sử dụng chế độ encoder-decoder của mô hình để đưa ra dự đoán cho mỗi mặt nạ và tạo ra các bản vá ứng cử viên. Chúng tôi đặt kích thước beam là 250 do hạn chế của thiết bị của chúng tôi, nhỏ hơn 1000 được sử dụng trong CURE [47] và CoCoNuT [52]. Theo các phương pháp APR dựa trên học máy trước đây [18], [50], chúng tôi đặt giới hạn thời gian chạy 5 giờ để sửa một lỗi để thực hiện so sánh công bằng.

Tất cả các thí nghiệm được tiến hành trên một máy chủ Ubuntu 18.04.3 với hai GPU Tesla V100-SXM2.

V. ĐÁNH GIÁ VÀ KẾT QUẢ

A. So sánh với Hiện đại

Thiết kế Thực nghiệm. Trong phần này, chúng tôi nhằm mục đích đánh giá hiệu suất của GAMMA. Chúng tôi sử dụng 395 lỗi thực tế được trình bày trong tập dữ liệu Defects4J-v1.2 và so sánh GAMMA với các kỹ thuật APR hiện đại, bao gồm truyền thống và dựa trên học máy. Chúng tôi báo cáo hiệu suất của tất cả các kỹ thuật được so sánh dưới định vị lỗi hoàn hảo (tức là, câu lệnh lỗi ground-truth được biết đến bởi các kỹ thuật).

Kết quả. Bảng I trình bày số lượng lỗi mà các kỹ thuật APR khác nhau sửa thành công trên tập dữ liệu Defects4J-v1.2. Tổng thể, chúng tôi thấy rằng GAMMA vượt trội đáng kể so với các kỹ thuật APR được so sánh bao gồm cả kỹ thuật APR truyền thống và dựa trên học máy. GAMMA có thể tạo ra các bản vá chính xác cho 82 lỗi thực tế, nhiều hơn 20.59% (14 lỗi), 26.15% (17 lỗi) và 14.8% (8 lỗi) so với TBar, Recoder và AlphaRepair. Cụ thể, GAMMA sửa 11, 24, 16, 25, 3 và 3 lỗi cho các dự án Chart, Closure, Lang, Math, Mockito và Time, tương ứng, bốn trong số chúng có hiệu suất tốt nhất (in đậm trong Bảng I). Quan trọng hơn, chúng tôi thấy rằng GAMMA đạt được tỷ lệ chính xác 81.19% (82/101) cho các bản vá hợp lý, cao hơn 9.61% (68/95), 23.15% (65/112) và 13.30% (74/109) so với TBar, Recoder và AlphaRepair, cho thấy GAMMA có thể giảm nhẹ vấn đề overfitting bản vá lâu dài trong cộng đồng APR.

Phân tích Chồng lấp. Để điều tra mức độ GAMMA bổ sung cho các kỹ thuật APR hiện có, chúng tôi tiếp tục tính toán số lượng lỗi chồng lấp được sửa bởi các kỹ thuật khác nhau. Một kỹ thuật truyền thống có hiệu suất tốt nhất (tức là, TBar) và ba kỹ thuật dựa trên học máy có hiệu suất tốt nhất (tức là, AlphaRepair, CURE và Recoder) được chọn. Như được hiển thị trong Hình 2, GAMMA sửa 14 lỗi duy nhất mà các phương pháp APR khác không sửa được, nhiều hơn 11, 3, 8 và 10 so với TBar, AlphaRepair, CURE và Recoder, tương ứng. Quan trọng hơn, như một kỹ thuật APR dựa trên mẫu, có 22 lỗi được sửa chính xác duy nhất của GAMMA so với TBar, nhấn mạnh lợi ích của dự đoán mặt nạ được thực hiện bởi UniXcoder. Tổng thể, kết quả cho thấy GAMMA bổ sung cho những kỹ thuật APR có hiệu suất tốt nhất này, để tăng số lượng lỗi được sửa chính xác trong benchmark Defects4J-v1.2.

Nghiên cứu Trường hợp. Chúng tôi đã chứng minh hiệu suất vượt trội của GAMMA so với công cụ dựa trên mẫu hiện đại TBar, có liên quan nhất đến công việc của chúng tôi. Để điều tra thêm hiệu quả của GAMMA, chúng tôi cung cấp một số ví dụ về các lỗi mà GAMMA có thể sửa nhưng TBar thất bại. Listing 3 trình bày lỗi Math-94 từ Defects4J-v1.2. Math-94 có thể được sửa với mẫu sửa chữa của đột biến biểu thức điều kiện. Biểu thức điều kiện "u * v == 0" trong một câu lệnh if được thay thế bằng "(u == 0) ||(v == 0)". TBar xử lý mẫu này bằng cách thay thế biểu thức đáng ngờ bằng những biểu thức tương thích khác được thu thập từ cùng tệp cục bộ, trong khi GAMMA trực tiếp thay thế biểu thức bằng một token mặt nạ để chúng có thể được dự đoán sau bởi nhiệm vụ dự đoán mặt nạ từ mô hình được huấn luyện trước, giúp có thể tạo ra các biểu thức mới có thể sửa chính xác các lỗi.

Listing 4 trình bày một ví dụ tương tự về lỗi Closure-52 từ Defects4J-v1.2. Closure-52 biểu thị phiên bản lỗi thứ 52 của dự án Google Closure Compiler trong Defects4J. Để sửa lỗi này, chúng ta cần chèn một biểu thức điều kiện phụ mới vào biểu thức gốc. TBar không tạo ra được bản vá chính xác với phép toán và biến không phù hợp trong khi GAMMA có thể trực tiếp dự đoán biểu thức bị che với ngữ cảnh mã tương ứng.

B. Khả năng tổng quát hóa của GAMMA

Thiết kế Thực nghiệm. Chúng tôi đã chứng minh rằng GAMMA đạt được hiệu suất ấn tượng để sửa các lỗi thực tế từ benchmark Defects4J-v1.2 được áp dụng rộng rãi dựa trên các mẫu sửa chữa. Durieux et al. [55] chứng minh rằng tồn tại hiện tượng overfitting benchmark phổ biến trong đánh giá APR, tức là các công cụ APR thường hoạt động tốt hơn đáng kể trên Defects4J-v1.2 so với các benchmark khác. Trong phần này, theo công việc trước đây [18], [50], để đánh giá khả năng tổng quát hóa của GAMMA, chúng tôi tiếp tục tiến hành một số thí nghiệm mở rộng trên các dự án bổ sung để đánh giá thêm.

Kết quả. Bảng II trình bày kết quả so sánh của GAMMA với các baseline trên Defects4J-v2.0 và QuixBugs. Trong Defects4J-v2.0, theo một số công việc gần đây [18], [56], chúng tôi chỉ tập trung vào những lỗi có bản vá được giới hạn trong một vị trí duy nhất. Tổng thể, GAMMA tạo ra 45 bản vá chính xác trong 257 chương trình lỗi đã cho, vượt trội hơn cả phương pháp truyền thống và dựa trên học máy. Chúng tôi thấy rằng hiệu suất đạt được trên tập dữ liệu Defects4J-v2.0 thường thấp hơn so với hiệu suất đạt được trên tập dữ liệu Defects4J-v1.2. Ví dụ, AlphaRepair sửa 18.73% (74/395) lỗi từ Defects4J-v1.2 trong khi chỉ sửa 14.01% (36/257) lỗi từ Defects4J-v2.0. Dựa trên phân tích của chúng tôi về hai tập dữ liệu, lý do có thể là Defects4J-v2.0 chứa một tập hợp các dự án khó hơn cho APR với nhiều loại sửa chữa khác nhau so với Defects4J-v1.2. Mặc dù vậy, GAMMA có thể tạo ra nhiều hơn 9, 34 và 37 bản vá chính xác, đây là số lượng cao nhất trong tất cả các phương pháp. Chúng tôi cũng thấy rằng như một phương pháp dựa trên mẫu, TBar có thể tạo ra một số lượng lớn bản vá chính xác (68) cho Defects4J-v1.2, trong khi nó chỉ tạo ra một số lượng hạn chế bản vá chính xác (8) cho Defects4J-v2.0. Lý do có thể là hầu hết các mẫu sửa chữa được thiết kế để nhắm mục tiêu Defects4J-v1.2, có thể không tổng quát hóa cho các dự án chưa thấy khác, như Defects4J-v2.0; Bên cạnh đó, các phương pháp dựa trên học máy cũng gặp khó khăn khi chuyển sang một tập dữ liệu đánh giá khó hơn vì các mẫu chuyển đổi mã được học từ các tập dữ liệu huấn luyện có thể không có mặt trong Defects4J-v2.0. Ngược lại, GAMMA có thể giải quyết vấn đề khả năng tổng quát hóa mà không cần huấn luyện trên các tập dữ liệu lỗi cụ thể, điều này khiến nó ít bị ảnh hưởng bởi các vấn đề khả năng tổng quát hóa của các công cụ dựa trên mẫu truyền thống hoặc dựa trên học máy.

Ngoài Defects4J-v2.0, chúng tôi cũng cố gắng xác thực phương pháp của chúng tôi trên QuixBugs, trích xuất các lỗi từ Quixey Challenge và dịch chúng sang cả ngôn ngữ Java và Python. Vì các mẫu sửa chữa của chúng tôi được thiết kế cho Java, chúng tôi chỉ tập trung vào các chương trình Java trong QuixBugs theo công việc trước đây [17]. Bảng II cho thấy trong số 40 lỗi trong QuixBugs, 22 được sửa chính xác bởi GAMMA, nhấn mạnh hiệu suất cạnh tranh của GAMMA so với các phương pháp hiện đại. Đáng chú ý là hầu hết các mẫu được tóm tắt từ Defects4J-v1.2, có thể có nghĩa là một số mẫu không thể được áp dụng cho bất kỳ lỗi nào ngoại trừ những lỗi từ Defects4J-v1.2. Do đó, GAMMA có thể bị hạn chế bởi thiếu các mẫu sửa chữa hiệu quả hơn khi đến với các tập dữ liệu lỗi mới khác. Ví dụ, mặc dù nhiều loại mẫu cùng với các mẫu phụ được định nghĩa, một số mẫu không thể được sử dụng để sửa ít nhất một lỗi từ QuixBugs. Kết quả là, chúng tôi mong đợi khám phá các mẫu sửa chữa tổng quát hơn trong tương lai để cải thiện thêm hiệu suất của APR dựa trên mẫu.

C. Khả năng mở rộng của GAMMA

Thiết kế Thực nghiệm. Để điều tra thêm liệu hiệu suất của GAMMA có bị ảnh hưởng bởi các mô hình được huấn luyện trước khác nhau hay không, chúng tôi áp dụng hai mô hình tiên tiến khác để thực hiện nhiệm vụ dự đoán mặt nạ: CodeBERT và ChatGPT. CodeBERT [27] là một mô hình được huấn luyện trước cho ngôn ngữ lập trình và tự nhiên, và dự đoán mặt nạ là một trong những nhiệm vụ huấn luyện trước của nó. ChatGPT là một mô hình ngôn ngữ hiện đại đã cho thấy khả năng ấn tượng trong các cuộc trò chuyện với con người. Chúng tôi cũng sử dụng Defects4J-v1.2 làm benchmark nhưng thay thế UniXcoder bằng hai mô hình này trong quá trình điền mặt nạ để tìm hiểu mức độ các mô hình được huấn luyện trước ảnh hưởng đến hiệu quả của sửa chữa chương trình dựa trên mẫu.

Tương tự như UniXcoder, CodeBERT cũng có thể tạo ra dự đoán cho một token mặt nạ "<mask>" trong đoạn mã đã cho. Sự khác biệt giữa chúng là UniXcoder có thể dự đoán một số token liên tục cho một mặt nạ duy nhất trong khi CodeBERT chỉ có thể đưa ra một token cho một mặt nạ. Tuy nhiên, thường có nhiều hơn một token dưới một mặt nạ, vì vậy khi sử dụng CodeBERT, chúng tôi phải sử dụng số lượng mặt nạ liên tiếp khác nhau để che mã ban đầu và sau đó dự đoán chúng tuần tự. Chúng tôi không biết số lượng mặt nạ chính xác mà chúng tôi nên sử dụng (tức là, số lượng token trong mã đã sửa) vì có thể có rất nhiều khả năng trong bản vá. Vì vậy trong khi che, chúng tôi một cách ngây thơ thử tất cả các số mặt nạ từ 1 đến 20, đây là một phạm vi phù hợp với hầu hết các trường hợp. Trong mỗi lần lặp, một mặt nạ được dự đoán và một điểm số chung cho mỗi dự đoán được tính toán. Những dự đoán có điểm số cao nhất sẽ được chọn để thay thế mặt nạ, và mặt nạ tiếp theo sẽ được dự đoán theo các dự đoán trước đó. Chúng tôi đặt kích thước beam là 250, giống như được sử dụng trong UniXcoder, vì vậy trong mỗi lần lặp, CodeBERT sẽ đưa ra 250 dự đoán có thể nhất cho mặt nạ. Lấy quá trình sửa lỗi Defects4J Closure-10 làm ví dụ (được hiển thị trong Listing 2), bản vá của lỗi liên quan đến việc thay đổi tên phương thức. Để sửa lỗi này, tên phương thức allResultsMatch được thay thế bằng mặt nạ, và sau đó CodeBERT được yêu cầu đưa ra 250 dự đoán cho mặt nạ đầu tiên. Trong số các dự đoán, token "any" có điểm số tương đối cao, vì vậy nó thay thế mặt nạ đầu tiên và CodeBERT sẽ tiếp tục dự đoán mặt nạ tiếp theo cho đến khi tất cả mặt nạ được điền.

Khác với UniXcoder, ChatGPT được tinh chỉnh từ GPT-3.5 và closed-source. Chúng tôi có thể truy cập ChatGPT với API ChatGPT của gpt-3.5-turbo-0301, đây là phiên bản mới nhất có sẵn. Chúng tôi tương tác với ChatGPT thông qua các cuộc trò chuyện ngôn ngữ tự nhiên, tức là, gửi yêu cầu đến ChatGPT hoặc nhận phản hồi từ ChatGPT. Để điền mặt nạ với ChatGPT, chúng tôi đầu tiên đưa cho nó một số prompt, hướng dẫn nó trả lại các dự đoán cho mặt nạ. Theo các prompt, chúng tôi sau đó thêm dòng lỗi bị che cùng với ngữ cảnh của nó để tạo thành truy vấn hoàn chỉnh cho ChatGPT. Trong thí nghiệm của chúng tôi, đầu vào cho ChatGPT bắt đầu bằng một prompt "Next token prediction task, the first line is a comment to help prediction, just return 250 possible predictions for <mask> with highest probability: ", và sau đó ngữ cảnh lỗi chúng tôi đưa ra giống như đầu vào cho UniXcoder, bao gồm một dòng lỗi được bình luận và toàn bộ phương thức mà dòng lỗi thuộc về. Bên cạnh đó, nhờ lợi ích của prompt được thiết kế, chúng tôi không đặt số lượng token bị che trong mã lỗi, giống như UniXcoder.

Kết quả. Hình 3 trình bày kết quả sửa chữa của các mô hình ngôn ngữ được huấn luyện trước khác nhau. Tổng thể, sự kết hợp của ba mô hình có thể sửa 93 lỗi từ Defects4J-v1.2, cho thấy những mô hình này có thể được sử dụng cùng nhau bởi GAMMA để tăng thêm số lượng bản vá chính xác có thể được tạo ra. Cụ thể, chúng tôi thấy khi sử dụng CodeBERT để thực hiện nhiệm vụ dự đoán mặt nạ, 80 lỗi tổng cộng được sửa chính xác bởi GAMMA, chỉ ít hơn hai lỗi so với các lỗi mà GAMMA với UniXcoder sửa. Tuy nhiên, CodeBERT mất nhiều thời gian hơn để tạo ra các bản vá chính xác, vì số lượng mặt nạ nên được sử dụng trong việc sửa một lỗi là không thể dự đoán được, và chúng tôi phải chạy chương trình dự đoán mặt nạ trên cùng một lỗi và cùng một mẫu sửa chữa rất nhiều lần, mỗi lần với một số mặt nạ khác nhau. Ngược lại, UniXcoder dự đoán tuần hoàn token tiếp theo cho một mặt nạ cho đến khi một token EOF được tạo ra, vì vậy chỉ cần một mặt nạ để sửa lỗi. Chúng tôi cũng thấy rằng GAMMA với ChatGPT chỉ sửa 67 lỗi chính xác, không hoạt động tốt như GAMMA với UniXcoder và CodeBERT trong dự đoán mặt nạ. Lý do có thể nằm ở chỗ UniXcoder được huấn luyện trước với một mục tiêu mô hình hóa ngôn ngữ bị che, trong đó một số văn bản huấn luyện được che một cách nhân tạo và mục tiêu huấn luyện là dự đoán văn bản thực. Tuy nhiên, ChatGPT được thiết kế cho các cuộc trò chuyện ngôn ngữ tự nhiên và không rõ ChatGPT được huấn luyện trước như thế nào do nó là closed-source. Do đó, việc sử dụng UniXcoder để khôi phục các token mã bị che cho các đoạn mã lỗi trong phương pháp của chúng tôi là tự nhiên. Các nhà nghiên cứu tương lai nên khám phá thêm cách tận dụng ChatGPT tốt hơn (ví dụ, thiết kế các prompt khác) cho dự đoán mặt nạ.

VI. THREATS TO VALIDITY

Mối đe dọa đầu tiên đối với tính hợp lệ đến từ việc kiểm tra thủ công các bản vá chính xác. Để giảm nhẹ ảnh hưởng của thiên kiến tiềm ẩn, theo các công việc trước đây [39], [47], ba tác giả thủ công xác minh tất cả các bản vá hợp lý (tức là các bản vá vượt qua thành công test) dựa trên các bản vá ground truth (tức là các bản vá của nhà phát triển). Một bản vá hợp lý được coi là chính xác nếu cả ba tác giả xác định nó tương đương với một bản vá ground truth về mặt ngữ nghĩa. Để tạo điều kiện cho việc sao chép và xác minh các thí nghiệm của chúng tôi, chúng tôi cũng phát hành công khai các tài liệu liên quan (bao gồm tất cả mã nguồn và các bản vá chính xác) [29].

Mối đe dọa thứ hai đối với tính hợp lệ là cài đặt định vị lỗi. Chúng tôi đánh giá GAMMA và các baseline trên định vị lỗi hoàn hảo (tức là, vị trí lỗi ground-truth được biết) do hai lý do. Đầu tiên, các phương pháp định vị lỗi có sẵn ảnh hưởng đáng kể đến hiệu suất của các kỹ thuật APR, gây ra thiên kiến trong kết quả so sánh [57]. Định vị lỗi hoàn hảo giảm nhẹ ảnh hưởng của sự khác biệt trong các phương pháp định vị lỗi khác nhau đối với kết quả sửa chữa và cho phép đánh giá công bằng hiệu suất sửa chữa độc lập với phương pháp định vị lỗi được sử dụng. Thứ hai, các kỹ thuật APR gần đây nhất [39], [47], [52], [58], [59] chỉ được đánh giá với định vị lỗi hoàn hảo, điều này khiến công việc của chúng tôi cũng sử dụng định vị hoàn hảo để đảm bảo so sánh trực tiếp. Tuy nhiên, cài đặt so sánh này có thể mang lại thiên kiến trong hiệu suất sửa chữa vì kết quả định vị lỗi hoàn hảo thường không có sẵn trong thực tế. Mặc dù vậy, chúng tôi tin rằng định vị lỗi hoàn hảo có ít tác động đến kết quả của chúng tôi, vì định vị hoàn hảo có thể hiển thị hiệu suất thuần túy của các phương pháp APR khác nhau. Trong tương lai, chúng tôi cố gắng báo cáo hiệu suất sửa chữa của GAMMA và các baseline với cả định vị lỗi tự động (ví dụ, Ochiai [31]) và định vị lỗi hoàn hảo.

Mối đe dọa thứ ba đối với tính hợp lệ đến từ tiềm năng rò rỉ dữ liệu của các mô hình được huấn luyện trước. Trong thí nghiệm của chúng tôi, chúng tôi triển khai GAMMA dựa trên UniXCoder và đánh giá GAMMA trên benchmark được áp dụng rộng rãi Defects4J-v1.2. Xem xét rằng UniXCoder được huấn luyện trước với hàng triệu đoạn mã, có thể tồn tại một số lỗi trong benchmark đánh giá Defects4J-v1.2 xuất hiện trong tập dữ liệu huấn luyện trước của UniXcoder. Chúng tôi thực hiện kiểm tra thủ công để kiểm tra xem các lỗi được sửa bởi GAMMA có bị rò rỉ vào tập dữ liệu huấn luyện trước hay không. Cụ thể, chúng tôi truy vấn các tập dữ liệu huấn luyện trước bao gồm 2.3M hàm được ghép nối với bình luận và 4.1M mã unimodal từ CodeSearchNet. Kiểm tra thủ công được thực hiện bởi hai tác giả độc lập và được xác nhận bởi tác giả thứ ba. Chúng tôi thấy có ba lỗi bị rò rỉ vào tập huấn luyện trước, tức là Closure-73, Closure-126 và Time-19. Đối với ba lỗi này, chúng tôi thủ công nhiễu loạn mã lỗi (ví dụ, thay đổi tên biến, thêm mã chết) và thấy GAMMA vẫn có thể tạo ra các bản vá chính xác cho cả ba lỗi. Chúng tôi cũng thấy rằng nếu chúng tôi loại trừ ba lỗi chồng lấp, GAMMA vẫn vượt trội hơn các kỹ thuật APR hiện đại (79 so với 68 cho TBar, 79 so với 74 cho AlphaRepair). Do đó, chúng tôi tin tưởng rằng rò rỉ dữ liệu không phải là điểm quan trọng đối với kết luận của chúng tôi.

VII. CÔNG VIỆC LIÊN QUAN

A. Sửa chữa Chương trình Tự động

Các kỹ thuật APR hiện có có thể được chia thành bốn danh mục, tức là dựa trên heuristic [11], [12], dựa trên ràng buộc [13], [14], [60], dựa trên mẫu [15]–[17] và các kỹ thuật sửa chữa dựa trên học máy [38], [50], [52]. Công việc của chúng tôi liên quan đến APR dựa trên mẫu và dựa trên học máy, được thảo luận như sau.

APR dựa trên mẫu, tạo ra các bản vá với sự hỗ trợ của các mẫu sửa chữa, đại diện cho hiện đại trong số các kỹ thuật APR truyền thống. TBar [17] hệ thống thu thập và tóm tắt các mẫu sửa chữa từ tài liệu trước đây và điều tra hiệu quả của việc tạo bản vá dựa trên các mẫu này. Một số kỹ thuật khác khám phá các mẫu sửa chữa theo nhiều cách khác nhau. Ví dụ, PAR [44] thủ công trích xuất các mẫu sửa chữa từ 60.000 bản vá viết bởi con người. FixMiner [15] khai thác các mẫu sửa chữa với một chiến lược phân cụm lặp. AVATAR [16] tận dụng các mẫu sửa chữa từ các công cụ phát hiện lỗi tĩnh để tạo ra các bản vá. Khác với hầu hết các kỹ thuật APR dựa trên mẫu hiện có tập trung vào khai thác các mẫu sửa chữa, GAMMA là công việc đầu tiên nhằm mục đích giải quyết vấn đề mã nguồn donor bằng cách tích hợp các mô hình được huấn luyện trước với một nhiệm vụ điền vào chỗ trống.

Với các corpus mã nguồn mở lớn có sẵn, APR dựa trên học máy, áp dụng machine learning cho mục tiêu sửa lỗi, đang nhận được sự chú ý ngày càng tăng. Ví dụ, DLFix [38] sử dụng một mô hình mạng nơ-ron hồi quy (RNN) dựa trên cây để học từ các bản sửa lỗi và ngữ cảnh xung quanh dưới dạng cây cú pháp trừu tượng. CoCoNuT [52] giới thiệu một kiến trúc dịch máy thần kinh (NMT) nhận biết ngữ cảnh mới biểu diễn riêng biệt mã lỗi và ngữ cảnh. CURE [47] cố gắng phá vỡ giới hạn của các kỹ thuật dựa trên NMT hiện có bằng cách huấn luyện trước một mô hình ngôn ngữ lập trình trên một codebase lớn, giới thiệu một chiến lược tìm kiếm nhận biết mã mới và sử dụng tokenization subword để thu hẹp không gian tìm kiếm. Recoder [50] là một decoder chỉnh sửa có hướng dẫn cú pháp với tạo placeholder, cung cấp một kiến trúc provider/decider mới để đảm bảo rằng các bản vá có cú pháp chính xác được tạo ra. Khác với các kỹ thuật APR dựa trên học máy hiện có tạo ra các bản vá từ đầu với huấn luyện dữ liệu sửa lỗi, GAMMA nhằm mục đích trực tiếp dự đoán các token mã chính xác với sự hỗ trợ của các mẫu sửa chữa trong một kịch bản zero-shot.

Gần đây, tồn tại một số lượng ngày càng tăng các kỹ thuật APR dựa trên các mô hình được huấn luyện trước. Ví dụ, Yuan et al. [39] đề xuất CIRCLE, một framework sửa chữa chương trình dựa trên T5 được trang bị khả năng học liên tục trên nhiều ngôn ngữ. Xia et al. [18] đề xuất AlphaRepair, một phương pháp APR kiểu cloze dựa trên CodeBERT mà không cần tinh chỉnh trên dữ liệu sửa lỗi lịch sử. Trong công việc của chúng tôi, chúng tôi bao gồm CIRCLE và AlphaRepair làm baseline trong thí nghiệm. Sobania et al. [61] điều tra hiệu suất của ChatGPT trên benchmark QuixBugs. Mashhadi et al. [62] điều tra hiệu suất của việc tinh chỉnh CodeBERT để sửa các lỗi phần mềm từ ManySStuBs4J. Jiang et al. [58] khám phá hiệu suất của các mô hình được huấn luyện trước có và không có tinh chỉnh cho lĩnh vực sửa chữa chương trình. Xia et al. [59] tiếp tục trình bày một đánh giá mở rộng về các mô hình được huấn luyện trước gần đây để sửa các dự án thực tế và thấy rằng các mô hình được huấn luyện trước hiện đại có thể sửa một số lượng đáng kể lỗi. Ví dụ, CodeX, mô hình hiệu quả nhất, sửa 99 lỗi trong Defects4J-v1.2 với tổng cộng ba cài đặt sửa chữa. Chúng tôi loại trừ CodeX làm baseline trong thí nghiệm của chúng tôi do sự không chắc chắn về dữ liệu huấn luyện trong các mô hình được huấn luyện trước lớn black-box như vậy.

B. Mô hình Ngôn ngữ Được huấn luyện trước và Ứng dụng

Trong phần này, chúng tôi giới thiệu một số mô hình ngôn ngữ được huấn luyện trước điển hình và sau đó thảo luận về các ứng dụng của các mô hình ngôn ngữ được huấn luyện trước cho một số nhiệm vụ liên quan đến mã, ví dụ tìm kiếm mã.

1) Mô hình Ngôn ngữ Được huấn luyện trước: Các mô hình ngôn ngữ được huấn luyện trước đã cho thấy kết quả đầy hứa hẹn trên các nhiệm vụ NLP. BERT [63] là một mô hình điều kiện trên ngữ cảnh trái và phải trong tất cả các lớp để huấn luyện trước các biểu diễn hai chiều sâu từ văn bản không được gắn nhãn. GPT-3 [43] là một mô hình ngôn ngữ tự hồi quy có 175 tỷ tham số, vượt trội đáng kể số tham số trong các mô hình ngôn ngữ trước đây. ChatGPT [28] là mô hình ngôn ngữ phổ biến nhất hiện tại được tinh chỉnh từ GPT-3 và đang nhận được sự chú ý từ cả lĩnh vực khoa học và công nghiệp. Tính năng đáng chú ý nhất của ChatGPT là nó có thể tạo ra các phản hồi giống con người và giao tiếp với con người như những gì một con người thực sự có thể làm.

Được truyền cảm hứng từ thành công của các mô hình được huấn luyện trước trong NLP, nhiều nhà nghiên cứu áp dụng mô hình được huấn luyện trước cho các nhiệm vụ liên quan đến mã. Feng et al. [27] đề xuất một mô hình được huấn luyện trước hai phương thức (CodeBERT) cho cả ngôn ngữ lập trình và ngôn ngữ tự nhiên. CodeBERT được phát triển trên kiến trúc thần kinh dựa trên Transformer và được huấn luyện trước với nhiệm vụ mô hình hóa ngôn ngữ bị che, là dự đoán các token, và phát hiện token được thay thế. Guo et al. [26] trình bày UniXcoder, một mô hình được huấn luyện trước đa phương thức thống nhất cho ngôn ngữ lập trình. UniXcoder sử dụng các ma trận chú ý mặt nạ với các adapter tiền tố để kiểm soát hành vi của mô hình và tận dụng nội dung đa phương thức như AST và bình luận mã để tăng cường biểu diễn mã. Khác với những nghiên cứu này thiết kế các mô hình huấn luyện trước mới từ đầu, chúng tôi cố gắng thúc đẩy APR dựa trên mẫu dựa trên các mô hình được huấn luyện trước này.

2) Ứng dụng của Mô hình Được huấn luyện trước: Ngoài các mô hình được huấn luyện trước điển hình nói trên, các nhà nghiên cứu cũng đã áp dụng các mô hình được huấn luyện trước như vậy cho một số lĩnh vực liên quan đến mã (ví dụ, hoàn thành mã và sửa chữa chương trình). Mastropaolo et al. [64] trình bày một nghiên cứu thực nghiệm để điều tra việc sử dụng các mô hình được huấn luyện trước cho bốn nhiệm vụ liên quan đến mã, bao gồm sửa chữa chương trình, tiêm đột biến, tạo assertion và tóm tắt mã. Một chiến lược tương tự kết hợp các mẫu đột biến và các mô hình được huấn luyện trước được áp dụng trong kiểm thử đột biến. Ví dụ, Degiovanni et al. [65] giới thiệu µBERT, một công cụ kiểm thử đột biến dựa trên CodeBERT bằng cách che một token từ biểu thức và thay thế token bị che bằng token được dự đoán từ CodeBERT. Richter et al. [66] đề xuất một toán tử đột biến ngữ cảnh bằng cách sử dụng CodeBERT để tạo ra một phân phối phụ thuộc ngữ cảnh trên các thay thế token khả thi. Gần đây, Zhang et al. [67] tiến hành một nghiên cứu thực nghiệm mở rộng để điều tra hiệu suất của các mô hình được huấn luyện trước trong việc sửa chữa các lỗ hổng bảo mật và đề xuất một phương pháp tăng cường với học chuyển giao sửa lỗi. Mặc dù tồn tại một số nhiệm vụ SE (ví dụ, kiểm thử đột biến và sửa chữa chương trình) hưởng lợi từ các mô hình được huấn luyện trước, trong công việc này, chúng tôi thực hiện công việc đầu tiên sử dụng các mô hình được huấn luyện trước để trực tiếp dự đoán mã chính xác với sự hỗ trợ của các mẫu sửa chữa.

VIII. KẾT LUẬN

Trong công việc này, chúng tôi trình bày GAMMA, một công cụ APR dựa trên mẫu sáng tạo đồng hóa những tiến bộ của các mẫu sửa chữa và các mô hình được huấn luyện trước. GAMMA đầu tiên định nghĩa một tập hợp các mẫu sửa chữa mặt nạ bằng cách che các token mã lỗi với ngữ cảnh mã tương ứng. GAMMA sau đó sử dụng các mô hình được huấn luyện trước có sẵn để trực tiếp khôi phục mã chính xác với một nhiệm vụ dự đoán mặt nạ. Quan trọng hơn, GAMMA có thể được xây dựng trên nhiều mô hình được huấn luyện trước khác nhau dưới một cài đặt học zero-shot và chúng tôi triển khai nó như một công cụ APR thực tế sử dụng mô hình UniXcoder gần đây. Kết quả thực nghiệm trên tập dữ liệu Defects4J-v1.2 phổ biến đã cho thấy hiệu suất đầy hứa hẹn, ví dụ 82 lỗi được sửa bởi GAMMA, vượt trội hơn tất cả các kỹ thuật APR hiện đại. Chúng tôi cũng chứng minh rằng GAMMA có thể giải quyết tốt vấn đề overfitting tập dữ liệu, ví dụ 45 và 22 lỗi được sửa trong Defects4J-v2.0 và Quixbugs. Chúng tôi tiếp tục chứng minh rằng GAMMA có thể tổng quát hóa cho các mô hình ngôn ngữ được huấn luyện trước khác nhau, như CodeBERT và ChatGPT.

LỜI CẢM ƠN

Các tác giả muốn cảm ơn các nhà phê bình ẩn danh vì những bình luận sâu sắc của họ. Công việc này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (61932012, 62141215).
