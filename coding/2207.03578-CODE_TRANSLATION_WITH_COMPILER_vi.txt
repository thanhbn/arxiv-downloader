# DỊCH MÃ VỚI BIỂU DIỄN TRUNG GIAN CỦA TRÌNH BIÊN DỊCH

Marc SzafraniecBaptiste Rozière*Hugh Leather
François Charton Patrick Labatut Gabriel Synnaeve
Meta AI
{mszafraniec,broz}@meta.com

TÓM TẮT
Trong bài báo này, chúng tôi tận dụng biểu diễn trung gian cấp thấp của trình biên dịch (IR) để cải thiện việc dịch mã. Các transpiler truyền thống dựa vào thông tin cú pháp và quy tắc thủ công, điều này hạn chế khả năng ứng dụng của chúng và tạo ra mã có vẻ không tự nhiên. Việc áp dụng các phương pháp dịch máy thần kinh (NMT) cho mã đã thành công trong việc mở rộng tập hợp các chương trình mà ta có thể nhận được bản dịch có vẻ tự nhiên. Tuy nhiên, chúng xử lý mã như các chuỗi token văn bản, và vẫn không phân biệt đủ tốt giữa các đoạn mã tương tự nhưng có ngữ nghĩa khác nhau trong các ngôn ngữ khác nhau. Hậu quả là chất lượng dịch thuật thấp, làm giảm tính thực tiễn của NMT, và nhấn mạnh nhu cầu về một phương pháp tăng đáng kể độ chính xác của nó. Ở đây chúng tôi đề xuất bổ sung IR cho việc dịch mã, cụ thể là LLVM IR, với kết quả trên các ngôn ngữ C++, Java, Rust và Go. Phương pháp của chúng tôi cải thiện so với kỹ thuật tiên tiến nhất cho dịch mã không giám sát, tăng số lượng bản dịch chính xác thêm 11% trên mức trung bình, và lên đến 79% cho cặp Java !Rust với giải mã tham lam. Chúng tôi mở rộng các bộ kiểm tra trước đó cho dịch mã, bằng cách thêm hàng trăm hàm Go và Rust. Ngoài ra, chúng tôi huấn luyện các mô hình với hiệu suất cao cho bài toán giải biên dịch IR, tạo mã nguồn lập trình từ IR, và nghiên cứu việc sử dụng IR làm trục cho dịch thuật.

1 GIỚI THIỆU
Dịch mã tự động cho phép chuyển đổi các codebase cũ sang framework mới, hoặc các ngôn ngữ cấp cao (nhưng chậm) sang ngôn ngữ cấp thấp (và nhanh). Các giải pháp công nghiệp hiện tại, được gọi là transpiler hoặc transcompiler1, dựa vào các quy tắc thủ công được áp dụng một cách có hệ thống. Chúng tạo ra các bản dịch không thuần thục mà tỏ ra khó đọc đối với các lập trình viên con người. Đây là một hạn chế nghiêm trọng: mã được dịch nên dễ đọc và hiểu, vì cuối cùng nó sẽ được duy trì bởi các nhà phát triển con người.

Trong những năm gần đây, Dịch máy thần kinh (NMT) được đề xuất như một thay thế cho dịch mã dựa trên quy tắc (Roziere et al., 2020; Weisz et al., 2021; 2022). Các mô hình này, được huấn luyện từ mã có thể đọc được bởi con người hiện có, tạo ra các bản dịch thuần thục, dễ hiểu. Thật không may, các neural transpiler không đáng tin cậy, và thường thất bại trong việc dịch ngữ nghĩa của chương trình đầu vào một cách chính xác. Đây là một hạn chế nghiêm trọng, vì một số công việc con người được tiết kiệm bởi transpiler phải được tái đầu tư vào việc gỡ lỗi đầu ra của nó.

Chúng tôi đề xuất cải thiện độ tin cậy của NMT bằng cách tận dụng thông tin từ các chuỗi công cụ trình biên dịch. Khi xử lý mã nguồn, các trình biên dịch tạo ra Biểu diễn Trung gian (IR): mã giả không phụ thuộc ngôn ngữ mô tả ngữ nghĩa của chương trình. Việc bổ sung dữ liệu huấn luyện với IR tương ứng có thể mang lại lợi ích cho Neural Transpiler theo hai cách: nó giúp căn chỉnh embedding cho các ngôn ngữ khác nhau và cải thiện hiểu biết ngữ nghĩa về mã. Như được hiển thị trong Hình 1, điều này có thể cải thiện đáng kể chất lượng ngữ nghĩa của các bản dịch thần kinh.

Trong công trình này, chúng tôi tận dụng LLVM (Lattner và Adve, 2004) để bổ sung mã nguồn với Biểu diễn Trung gian tương ứng và huấn luyện các mô hình cho dịch mã và giải biên dịch. Chúng tôi so sánh nó với TransCoder, chỉ sử dụng mã và không có IR. Chúng tôi cũng thiết kế một baseline chỉ dùng IR, được gọi là phương pháp pivot, tạo ra bản dịch hoàn toàn bằng cách giải biên dịch IR được tạo từ ngôn ngữ nguồn sang ngôn ngữ đích khác. Chúng tôi thử nghiệm với bốn ngôn ngữ: C++ Java, Rust và Go, và cho thấy rằng việc sử dụng cả mã và IR cho phép cải thiện tương đối trung bình 11%. Hơn nữa, phương pháp của chúng tôi chỉ sử dụng IR tại thời điểm huấn luyện và không yêu cầu tính toán bổ sung tại thời điểm suy luận.

Các đóng góp chính của chúng tôi là:
• Chúng tôi triển khai một phương pháp dịch thuật bổ sung IR mới, tận dụng LLVM IR để cải thiện biểu diễn mã. Nó cho phép chúng tôi tăng số lượng bản dịch chính xác được tạo bởi TransCoder cho C++, Java, Go và Rust lên 11%. So với phương pháp pivot chỉ dùng IR của chúng tôi, cải thiện đạt 170%
• Phương pháp của chúng tôi đặc biệt hữu ích trong chế độ dữ liệu thấp: với cải thiện tương đối đạt 26% khi dịch sang Rust và 19% khi dịch từ nó.
• Chúng tôi mở rộng bộ dữ liệu đánh giá song song gồm 852 hàm bằng C++, Java và Python từ Roziere et al. (2020) với thêm 343 hàm nữa bằng Go và 280 hàm nữa bằng Rust, cùng với các test case tương ứng
• Ngoài ra, chúng tôi đạt được độ chính xác 78% khi giải biên dịch LLVM IR sang C++

2 BIỂU DIỄN TRUNG GIAN TRONG CÁC TRÌNH BIÊN DỊCH
Các trình biên dịch dịch các chương trình được viết bằng ngôn ngữ máy tính thành mã thực thi cho một máy cụ thể. Hầu hết các trình biên dịch bao gồm một front-end nhận mã nguồn làm đầu vào, và một back-end tạo ra mã nhị phân máy. Front-end tiến hành lex (tokenize) và phân tích chương trình. Sau đó, nó tạo ra một cây cú pháp trừu tượng (AST), và dịch nó thành một số Biểu diễn Trung gian (IR). Back-end chuyển đổi IR thành mã thực thi đặc trưng cho máy.

Trong các trình biên dịch hiện đại như LLVM (Lattner và Adve, 2004), IR là chung cho các ngôn ngữ đầu vào khác nhau (và do đó các front-end khác nhau). Nó cho phép áp dụng các phép biến đổi và tối ưu hóa không phụ thuộc vào đích trong một module middle-end độc lập với ngôn ngữ nguồn và máy đích. Điều này dẫn đến một cấu trúc trình biên dịch hiệu quả: các ngôn ngữ mới có thể được triển khai bằng cách viết lại front-end, và các máy đích mới bằng cách viết lại back-end.

Một số IR thường cùng tồn tại trong một trình biên dịch: mỗi giai đoạn trong chuỗi công cụ (Hình 2) giới thiệu một biểu diễn mới. Các IR giai đoạn đầu phụ thuộc vào ngôn ngữ (ví dụ như AST phản ánh cú pháp của ngôn ngữ nguồn). Các IR giai đoạn cuối thay thế các biến có tên bằng thanh ghi và phản ánh đặc điểm của kiến trúc đích. Trong công trình này, chúng tôi quan tâm đến các IR middle-end, độc lập với máy đích, và tương tự cho tất cả các ngôn ngữ nguồn (như các phương ngữ trong ngôn ngữ tự nhiên).

3 MỤC TIÊU HUẤN LUYỆN
Dịch máy không giám sát bao gồm việc học các embedding chuỗi đa ngôn ngữ, và tạo chuỗi ở bất kỳ ngôn ngữ đầu ra nào từ các embedding này (Lample et al., 2018a). Bây giờ chúng tôi trình bày các hàm mục tiêu cho những tác vụ này. Trong phần 3.1, chúng tôi xem xét ba mục tiêu cơ bản được sử dụng bởi TransCoder, hệ thống NMT baseline của chúng tôi. Trong phần 3.2, chúng tôi giới thiệu ba hàm mới tận dụng LLVM IR để cải thiện biểu diễn đa ngôn ngữ của mã nguồn, và hiệu suất của các mô hình dịch thuật của chúng tôi. Trong quá trình huấn luyện, chúng tôi xen kẽ giữa tất cả sáu mục tiêu, chạy từng cái với cùng số bước tối ưu hóa. Tại thời điểm suy luận, mô hình chỉ được cung cấp mã nguồn, tức là IR không cần thiết.

Một cách chính thức, hãy để x=x1: : : x Nso là câu nguồn, z(x)=z(x) 1: : : z(x) Nir là IR tương ứng, và y=y1: : : yNta là câu đích. Chúng tôi viết LCE(^y; y) =P i`CE( ^yi; yi), với `CE( ^yi; yi) là tổn thất cross-entropy theo cặp giữa ^yi và yi. Chúng tôi định nghĩa tổn thất dịch máy (hoặc tổn thất seq2seq) từ x đến y, LMT là tổng của log-likelihood âm của mỗi token yi, cho trước x và các token trước đó y0: : : yi1(lưu ý rằng x và y có thể có độ dài khác nhau):

LMT(x; y) =X i log (P(yijx; y 1: : : yi1))

3.1 HÀM MỤC TIÊU CHUNG
TransCoder (Roziere et al., 2020) học cách dịch giữa các ngôn ngữ lập trình bằng cách tận dụng ba mục tiêu không giám sát được phát triển cho ngôn ngữ tự nhiên (Lample et al., 2018b):

Mô hình hóa Ngôn ngữ có Mặt nạ (MLM) huấn luyện một encoder để dự đoán các đầu vào được che ngẫu nhiên. Nó thường được sử dụng để tiền huấn luyện embedding cho ngôn ngữ tự nhiên (Devlin et al., 2018; Liu et al., 2019) và lập trình (Kanade et al., 2020; Feng et al., 2020). MLM cho phép mô hình học cú pháp và ngữ nghĩa của các chương trình. Các mục tiêu thay thế đã được đề xuất cho ngôn ngữ lập trình (Guo et al., 2020; Lachaux et al., 2021; Ahmad et al., 2021; Wang et al., 2021). Chúng tôi không sử dụng chúng ở đây, vì MLM vẫn hiệu quả và dễ sử dụng trên một loạt rộng các ngôn ngữ lập trình. Ký hiệu mask (x) là phiên bản bị che của câu mã x, và enc(t) là đầu ra encoder, MLM sử dụng tổn thất sau:

LMLM =LCE(enc(mask (x)); x): (1)

Mã hóa Tự động Khử nhiễu (AE) huấn luyện một mô hình chuỗi sang chuỗi (seq2seq) để lấy lại một chuỗi gốc từ phiên bản bị hỏng. Việc làm hỏng được thực hiện bằng cách che các khoảng token được lấy mẫu ngẫu nhiên từ phân phối Poisson, cũng như loại bỏ và xáo trộn token. Nó sử dụng tổn thất sau (noise (x) biểu thị phiên bản bị hỏng của x):

LAE=LMT(noise (x); x): (2)

Dịch Ngược (BT). Dịch Ngược (Sennrich et al., 2015) sử dụng mô hình để tạo ra bản dịch nhiễu của câu đầu vào, và sau đó huấn luyện mô hình phục hồi đầu vào gốc từ bản dịch. Đây là một mục tiêu đơn giản nhưng mạnh mẽ cho dịch máy không giám sát (Lample et al., 2018a; Artetxe et al., 2018). Trong thực tế, đây là tổn thất bắt buộc để có được hiệu suất cạnh tranh, vì vậy đây là thành phần chính của tất cả các thí nghiệm của chúng tôi. Một cách chính thức, chúng tôi sử dụng mô hình để dịch chuỗi x thành ^y và huấn luyện mô hình đảo ngược quá trình dịch, sử dụng tổn thất:

LBT=LMT(^y; x) (3)

3.2 IR CHO BIỂU DIỄN MÃ
Biểu diễn trung gian (IR) cung cấp thông tin bổ sung về mã sẽ được dịch. Chúng tôi thêm chúng vào bộ dữ liệu huấn luyện, như được mô tả trong phần 4.2, và tận dụng chúng bằng cách thêm ba hàm mục tiêu mới vào những hàm được mô tả trong phần 3.1.

Mô hình hóa Ngôn ngữ Dịch thuật (TLM), lần đầu được giới thiệu trong Lample và Conneau (2019), phấn đấu tạo ra các biểu diễn chung cho các câu song song trong các ngôn ngữ khác nhau. Giống như mục tiêu mô hình hóa ngôn ngữ có mặt nạ (MLM), nó huấn luyện một encoder để dự đoán các đầu vào được che ngẫu nhiên. Tuy nhiên, TLM được huấn luyện trên các cặp câu song song, được nối với nhau và tách biệt bởi một token đặc biệt. Ở đây, chúng tôi nối các hàm trong ngôn ngữ nguồn của chúng và IR tương ứng, sử dụng embedding ngôn ngữ mã nguồn và IR, và huấn luyện encoder để dự đoán các token được che ngẫu nhiên. Điều này cho phép mô hình học các tương ứng giữa nguồn và IR. Tổn thất tương ứng là (⊕ biểu thị nối):

LTLM =LCE⊕ mask (x⊕z(x)); x⊕z(x) ⊕ (4)

Mã hóa Tự động Dịch thuật (TAE) tương đương với việc chuyển đổi mục tiêu TLM thành một bộ giải mã tự động khử nhiễu. Mã nguồn và IR tương ứng bị hỏng và che, và sau đó được nối thành một chuỗi (sử dụng embedding ngôn ngữ cho mã và IR, như trước đây). TAE sau đó được giao nhiệm vụ khôi phục bản gốc, sử dụng tổn thất sau:

LTAE =LMT⊕ noise (x)⊕noise (z(x)); x⊕z(x) ⊕ (5)

Tạo IR (MT) huấn luyện mô hình dịch mã nguồn thành IR tương ứng. Điều này cho phép encoder học biểu diễn mã nguồn từ ngữ nghĩa của IR. Tổn thất là:

LIRGen =LMT⊕ x; z(x) ⊕ (6)

Ba mục tiêu này cần cả mã nguồn và IR tương ứng. Tuy nhiên, chỉ một phần nhỏ các hàm và file trong bộ dữ liệu của chúng tôi có thể được biên dịch. Để giảm thiểu điều này, chúng tôi cũng huấn luyện các mô hình trên dữ liệu đơn ngôn ngữ đầy đủ sử dụng các mục tiêu MLM và AE được mô tả ở trên. Trong thiết lập này, mục tiêu dịch ngược (BT) giống như trong Roziere et al. (2020), và cho phép mô hình của chúng tôi dịch trực tiếp chỉ từ mã nguồn tại thời điểm suy luận.

3.3 CÁC TỔN THẤT BỔ SUNG: GIẢI BIÊN DỊCH IR VÀ PIVOT
Chúng tôi nghiên cứu hai cách sử dụng thay thế của biểu diễn trung gian: giải biên dịch IR, và dịch pivot IR. Giải biên dịch IR bao gồm việc phục hồi mã nguồn tương ứng với IR đã cho. Trong thực tế, nó đảo ngược các phép tính được thực hiện bởi trình biên dịch. IR Pivot là một phương pháp dịch thuật được xây dựng dựa trên giải biên dịch IR. Vì LLVM có thể biên dịch nhiều ngôn ngữ (C++, Java, Rust, Go) thành cùng một IR, một cách tiếp cận rõ ràng để dịch mã bao gồm việc giải biên dịch IR được tạo từ ngôn ngữ nguồn thành mã trong ngôn ngữ đích. Chúng tôi gọi phương pháp này là "IR pivot". Lưu ý rằng, trong khi các kỹ thuật IR cho biểu diễn mã chỉ sử dụng IR trong quá trình huấn luyện, cả phương pháp giải biên dịch và pivot đều cần IR cho suy luận.

Giải biên dịch. Trong tác vụ có giám sát này, chúng tôi sử dụng LLVM để tạo IR từ mã nguồn, và huấn luyện một mô hình ngôn ngữ để đảo ngược quá trình, tức là học dự đoán mã nguồn từ IR. Các mô hình được tiền huấn luyện sử dụng các mục tiêu MLM và AE, và giải biên dịch được học sử dụng tổn thất dịch máy:

LDecomp =LMT⊕ z(x); x ⊕ (7)

IR Pivot. Tác vụ này tận dụng IR như một trục cho dịch mã. Ví dụ, để dịch từ Rust sang C++, trước tiên chúng tôi sử dụng LLVM để biên dịch chương trình Rust thành IR và sau đó giải biên dịch IR sang C++ sử dụng neural decompiler. Trong thực tế, có sự khác biệt nhỏ giữa IR được tạo cho các ngôn ngữ khác nhau: Rust-IR và C++-IR hoạt động như các phương ngữ của LLVM-IR. Điều này thường dẫn đến hiệu suất kém của phương pháp IR Pivot. Chúng tôi giảm thiểu những vấn đề này bằng cách sử dụng nhiều kỹ thuật khác nhau, mà chúng tôi mô tả trong phần C của phụ lục.

4 DỮ LIỆU

4.1 DỮ LIỆU HUẤN LUYỆN
Dữ liệu huấn luyện của chúng tôi được trích xuất bằng Google BigQuery, lập chỉ mục hơn 2,8 triệu kho lưu trữ mã nguồn mở từ GitHub2. Chúng tôi chọn các dự án có giấy phép cho phép phân phối lại các phần một cách rõ ràng, và trích xuất tất cả các hàm C++, Java, Rust và Go riêng lẻ. Để học cách giải biên dịch IR, chúng tôi cũng sử dụng bộ dữ liệu CodeNet (Puri et al., 2021), một kho lưu trữ 14 triệu giải pháp lập trình cạnh tranh bằng 55 ngôn ngữ. Các mô hình của chúng tôi hoạt động ở cấp độ hàm: điều này giảm các lỗi biên dịch do thiếu phụ thuộc, đồng thời giữ độ dài chuỗi ngắn.

Bảng 1: Phạm vi bộ dữ liệu trên các ngôn ngữ, theo số hàm độc lập. Thêm chi tiết có thể được tìm thấy trong Bảng 7 ở phụ lục.

C++ Go Java Rust
Dữ liệu đơn ngôn ngữ 6.6 M 9.4 M 7.8 M 576.3 K
Dữ liệu Song song Mã / IR 344.4 K 384.4 K 2.2 M 19.2 K

4.2 TẠO BIỂU DIỄN TRUNG GIAN
Mặc dù hệ sinh thái LLVM rộng lớn, không phải mọi ngôn ngữ đều có front-end LLVM, và không phải mọi front-end đều có thể tạo ra LLVM IR ngay lập tức. Chúng tôi sử dụng clang++3Lattner và Adve (2004) từ chuỗi công cụ biên dịch C++ LLVM đã được thiết lập, JLang4 cho Java, Gollvm5 cho Go và rustc Matsakis và Klock II (2014) cho Rust. Đối với cùng một chương trình, được viết bằng các ngôn ngữ khác nhau, các front-end khác nhau có thể tạo ra IR khác nhau. Để giảm thiểu những biến thể này, chúng tôi xử lý mã nguồn như sau. Đầu tiên, chúng tôi tạo IR được tối ưu hóa kích thước nhất (cờ -Oz), làm cho IR đồng nhất hơn giữa các ngôn ngữ. Thứ hai, chúng tôi loại bỏ tất cả thông tin không cần thiết (ví dụ: header và footer với thuộc tính, thông tin debug, comment). Cuối cùng, tên block được chuẩn hóa và tên symbol được demangle để tạo thuận lợi cho việc phục hồi chúng. Các hàm không biên dịch được tại thời điểm này (ví dụ: do thiếu phụ thuộc) không được đưa vào bộ dữ liệu song song, như được thấy trong hàng cuối của Bảng 1.

4.3 ĐÁNH GIÁ
Đánh giá NMT truyền thống dựa vào các metric như BLEU, dựa trên sự trùng lặp n-gram. Tuy nhiên, khi xử lý ngôn ngữ lập trình, cú pháp và đặc biệt là biên dịch và đầu ra tính toán có thể khác nhau rất nhiều mặc dù có những thay đổi nhỏ trong mã. Ngược lại, mã tương đương ngữ nghĩa, chỉ khác nhau về tên biến hoặc thứ tự phép toán có thể có điểm BLEU thấp. Để tính đến điều này, chúng tôi sử dụng và nâng cao bộ kiểm tra độ chính xác tính toán từ Roziere et al. (2020), chứa 852 giải pháp lập trình cạnh tranh song song bằng C++, Java và Python. Sử dụng C2Rust, CxGo và một số làm sạch mã thủ công, chúng tôi dịch 280 hàm và bộ kiểm tra bằng Rust và 343 bằng Go để đo hiệu suất của các mô hình của chúng tôi trong những ngôn ngữ này. Chúng tôi đo hiệu suất của mình bằng metric độ chính xác tính toán (CA@1) (Kulal et al., 2019; Roziere et al., 2020), xem xét rằng một bản dịch là chính xác nếu nó vượt qua một loạt kiểm tra đơn vị.

5 KẾT QUẢ

5.1 CHI TIẾT THỰC NGHIỆM
Đối với TransCoder, chúng tôi xem xét một mô hình transformer chuỗi-đến-chuỗi (seq2seq) (Vaswani et al., 2017) với attention (Bahdanau et al., 2015; Sutskever et al., 2014) và cùng kiến trúc như Roziere et al. (2020). Mô hình của chúng tôi có 12 lớp (6 trong encoder và 6 trong decoder), 8 attention head, và chiều 1024. Đối với các mục tiêu thêm nhiễu và mặt nạ vào câu đầu vào, như MLM, TLM, AE, và TAE, chúng tôi chọn các token được che và nhiễu ngẫu nhiên ngay lập tức tại mỗi epoch. Chúng tôi che 15% token trong MLM và TLM. Trong AE và TAE, chúng tôi che 20% token. MLM được huấn luyện trên dòng dữ liệu, trong khi các mục tiêu khác được huấn luyện ở cấp độ hàm. Chúng tôi sử dụng optimizer Adam (Kingma và Ba, 2015) và bộ lập lịch tốc độ học căn bậc hai nghịch đảo, với tốc độ học ban đầu 105 trong hầu hết các thí nghiệm của chúng tôi. Các mô hình của chúng tôi được triển khai trong PyTorch sử dụng float độ chính xác hỗn hợp. Các mô hình được tiền huấn luyện được huấn luyện cho đến khi hội tụ. Các mô hình dịch thuật được trình bày trong Bảng 2 và 3 được huấn luyện trong một tuần trên 32 GPU NVIDIA V100.

5.2 BIỂU DIỄN MÃ ĐƯỢC TĂNG CƯỜNG BẰNG IR CHO DỊCH THUẬT
Các mô hình sử dụng tổ hợp của ba mục tiêu—TAE, TLM và MT—được giới thiệu để tận dụng IR, được huấn luyện để dịch giữa các cặp của bốn ngôn ngữ (C++, Java, Rust, Go). Hiệu suất trung bình của chúng khi dịch đến và từ mỗi ngôn ngữ được trình bày trong bảng 2. Thông tin bổ sung, bao gồm so sánh với TransCoder-ST cho C++ $Java, có thể được tìm thấy trong Bảng 3) ở phụ lục. Như một baseline, chúng tôi sử dụng mô hình TransCoder (Roziere et al., 2020), được huấn luyện với MLM trên cùng bộ dữ liệu.

Sử dụng giải mã tham lam, các mục tiêu TLM, TAE và MT mới, tận dụng IR, cải thiện hiệu suất cho mọi ngôn ngữ. Kết quả trung bình tốt nhất đạt được khi kết hợp tất cả chúng. So với TransCoder, chúng cải thiện hiệu suất trung bình 4,4% điểm (11% tương đối). Tác động lớn nhất được quan sát thấy trong chế độ dữ liệu thấp: dịch từ và sang Rust (một ngôn ngữ ít được đại diện trong tập huấn luyện của chúng tôi) được cải thiện 25,6% và 19,3% (tương đối). Về mặt chất lượng, chúng tôi quan sát thấy rằng IR giúp mô hình của chúng tôi dịch các kiểu khi các kiểu nguồn và đích được đại diện bởi các token khác nhau. Ví dụ, trong ví dụ đầu tiên của Bảng 1, nó dịch ngữ nghĩa của int một cách chính xác sử dụng i32 thay vì kiểu số nguyên không dấu (usize). Xem Phụ lục H để biết thêm phân tích về cách các mục tiêu của chúng tôi cải thiện word embedding.

So với các mô hình dịch tăng cường IR, phương pháp IR Pivot "hiển nhiên" tỏ ra đáng thất vọng, mặc dù nó đạt được hiệu suất không tầm thường. Nó phụ thuộc rất nhiều vào kích thước của tập huấn luyện: IR pivot hoạt động tương đối tốt khi dịch từ ngôn ngữ ít tài nguyên sang ngôn ngữ nhiều tài nguyên (ví dụ: từ Rust), và kém khi dịch sang ngôn ngữ ít tài nguyên (ví dụ: sang Rust).

5.3 KẾT QUẢ GIẢI BIÊN DỊCH
Để tính toán IR pivot, chúng tôi huấn luyện một neural decompiler để lấy lại mã nguồn từ IR. Chúng tôi thử hai cấu hình riêng biệt cho giải biên dịch: một decoder chia sẻ với 6 lớp cho tất cả các cặp ngôn ngữ / IR, hoặc bốn decoder riêng biệt với hai lớp mỗi cái (một cho mỗi ngôn ngữ). Sử dụng decoder chia sẻ cải thiện hiệu suất cho tất cả ngôn ngữ, và đặc biệt khi dữ liệu khan hiếm (ví dụ: Rust). Xem Bảng 5 trong phụ lục để biết thêm thông tin.

Chúng tôi so sánh hiệu suất của mô hình của chúng tôi với RetDec (Křoustek et al., 2017), một decompiler dựa trên quy tắc. Nó đạt được độ chính xác tính toán 68,75 trên bộ dữ liệu C++ của chúng tôi và điểm BLEU là 8,54. Để so sánh, mô hình của chúng tôi đạt được độ chính xác tính toán 77,9 và điểm BLEU là 63,6 trong cùng thiết lập. Đặc biệt, RetDec thất bại trong việc giải biên dịch các file LLVM được tạo từ mã C++, đặc biệt là các đoạn tận dụng các cấu trúc thư viện chuẩn như unordered_map hoặc std::allocator. Những hạn chế của RetDec, được triển khai bởi một đội gồm 24 nhà phát triển trong 7 năm6, cho thấy việc xây dựng các decompiler dựa trên quy tắc toàn diện khó khăn như thế nào, đặc biệt khi IR đến từ các ngôn ngữ hoặc công cụ khác nhau.

6 THẢO LUẬN
IR khác nhau và ngôn ngữ thông dịch Bốn ngôn ngữ được xem xét trong công trình này có các front-end có thể xuất ra LLVM Intermediary Representation. LLVM hiện tại bao phủ hơn 30 ngôn ngữ máy tính. Sử dụng IR làm pivot yêu cầu ngôn ngữ nguồn và đích có front-end sử dụng cùng IR. Điều này loại bỏ một số ngôn ngữ được sử dụng rộng rãi (ví dụ: Python). Sử dụng IR để cải thiện embedding ít hạn chế hơn: ngôn ngữ nguồn và đích có thể được huấn luyện trên IR khác nhau, và được căn chỉnh với back-translation. Trong bài báo này, chúng tôi tập trung vào các ngôn ngữ biên dịch, nhưng điều quan trọng cần lưu ý là Biểu diễn Trung gian thường có sẵn cho các ngôn ngữ thông dịch: các trình thông dịch hiện đại dịch mã nguồn thành byte-code, có thể phục vụ như một IR.

Pivot vs Embedding TransCoder là một mô hình không giám sát học cách căn chỉnh biểu diễn mã và dịch mã từ ngôn ngữ này sang ngôn ngữ khác. Nó dựa hoàn toàn trên mã nguồn và không sử dụng IR. Phương pháp pivot sử dụng các câu song song được tạo tự động để học giải biên dịch IR, và back-translation để thích ứng với các phương ngữ IR khác nhau. Phương pháp này học dịch chỉ sử dụng sự tương đồng ở cấp độ IR, và không sử dụng chính mã nguồn ngoại trừ để tính toán IR. Mặc dù nó kém hiệu suất hơn các phương pháp khác, nó hoạt động tương đối tốt khi có ít dữ liệu cho ngôn ngữ nguồn, vì IR có thể được tính toán sử dụng trình biên dịch dựa trên quy tắc. Tuy nhiên, nó yêu cầu tính toán IR tại thời điểm kiểm tra, có thể rườm rà. Thay vào đó, việc thêm các mục tiêu TLM, TAE, và MT vào các mục tiêu thường được sử dụng cho dịch mã không giám sát cho phép mô hình có được điều tốt nhất của cả hai thế giới. Nó có thể học biểu diễn đa ngôn ngữ của mã nguồn từ sự tương đồng trong IR và trong chính mã nguồn. Như được hiển thị trong Bảng 2, nó vượt trội hơn cả TransCoder và phương pháp pivot. Đồng thời, mô hình này không yêu cầu tính toán IR tại thời điểm kiểm tra, và dễ sử dụng như TransCoder.

Sử dụng mô hình của chúng tôi tại thời điểm suy luận. Các mục tiêu TLM, TAE và MT tự giám sát tăng cường IR của chúng tôi được thiết kế để cải thiện biểu diễn mã đa ngôn ngữ được sử dụng trong các mô hình dịch thuật. Tuy nhiên, tác vụ dịch thuật không yêu cầu tính toán các mục tiêu này. Do đó, chúng dẫn đến các mô hình dễ sử dụng như TransCoder: tính toán IR không bắt buộc tại thời điểm kiểm tra và mô hình tạo bản dịch trực tiếp từ hàm nguồn.

7 CÔNG TRÌNH LIÊN QUAN
Dịch Nguồn-đến-Nguồn. Nhiều phương pháp dựa trên quy tắc có sẵn cho transpilation, một danh mục có thể được tìm thấy trực tuyến1. Đặc biệt, C2Rust7 và CxGo8, cùng với các sửa đổi thủ công, là trung tâm cho chúng tôi trong việc dịch các bài kiểm tra đánh giá sang Go và Rust (Xem Phần 4.3). Tương tự, 2to39, một thư viện Python chuyển mã Python 2 sang Python 3, được sử dụng trong Aggarwal et al. (2015) để tạo bộ dữ liệu song song và huấn luyện mô hình học máy.

Dịch máy thần kinh cho mã bị cản trở bởi việc thiếu dữ liệu song song giữa các ngôn ngữ lập trình. Thật vậy, ngoài một vài cặp ngôn ngữ, như Java-C# (Nguyen et al., 2013; Chen et al., 2018), và các lĩnh vực cụ thể (ví dụ: mã lập trình cạnh tranh), rất khó thu thập các bộ dữ liệu lớn về mã tương đương ngữ nghĩa trong các ngôn ngữ khác nhau. TransCoder (Roziere et al., 2020) lấp đầy khoảng trống này bằng cách giới thiệu dịch máy không giám sát cho ngôn ngữ lập trình. Họ tận dụng các cơ sở mã đơn ngôn ngữ lớn để học dịch giữa C++, Python và Java với hiệu suất cao. Sau đó, DOBF (Lachaux et al., 2021) cải thiện phương pháp tiền huấn luyện mô hình được sử dụng trong TransCoder, và Roziere et al. (2022) sử dụng các unit test được tạo tự động để cải thiện hiệu suất dịch thuật giữa Java, C++ và Python. Gần đây, các mô hình ngôn ngữ lớn được huấn luyện trên mã, như Codex (Chen et al., 2021) và PALM (Chowdhery et al., 2022), đã được sử dụng cho dịch mã không giám sát.

Sử dụng mô hình Transcoder, Weisz et al. (2021) và Weisz et al. (2022) khảo sát các liên kết giữa con người và phương pháp NMT cho dịch mã. Họ xem các phương pháp dịch thần kinh như trợ giúp cho lập trình viên. Trong bối cảnh này, họ chứng minh rằng ngay cả các mô hình không hoàn hảo cũng có thể cải thiện chất lượng công việc của kỹ sư cho dịch mã, và kêu gọi cải thiện giao diện người-máy.

Giải biên dịch. Giống như transpilation, giải biên dịch thường được thực hiện sử dụng các phương pháp dựa trên quy tắc dựa vào khớp mẫu để phân tích cấu trúc control flow của chương trình. RetDec, một decompiler mã nguồn mở được tạo bởi Avast (Křoustek et al., 2017), có thể giải biên dịch một file thực thi sang C và ngôn ngữ giống Python qua LLVM IR. Các công cụ khác tồn tại, như Hex-Rays Decompiler10 và Brumley et al. (2013). Một đánh giá toàn diện về các phương pháp dựa trên quy tắc có thể được tìm thấy trong các bài báo như Liang et al. (2021a) và Katz et al. (2019). Với những phương pháp này, giải biên dịch có thể thất bại nếu mã quá phức tạp, hoặc nếu nó chứa các tính năng ngôn ngữ không được dịch rõ ràng. Hầu hết các phương pháp cũng tạo ra các chương trình không có cấu trúc, dựa vào một số lượng lớn câu lệnh goto để mô phỏng control flow của các ngôn ngữ lập trình cấp thấp hơn. Điều này đúng về mặt ngữ nghĩa, nhưng rất hiếm khi được tìm thấy trong mã được viết bởi con người.

Một vài công trình đã nghiên cứu việc sử dụng mạng thần kinh chuỗi-đến-chuỗi cho giải biên dịch thần kinh. Katz et al. (2019) sử dụng mạng LSTM để giải biên dịch LLVM IR và mã assembly sang C. Cách tiếp cận của họ tạo ra các template mã dựa trên IR, xác định cấu trúc của đầu ra. Sau đó, họ điền chúng với gán biến chính xác và giá trị số. Trong cùng hướng, Fu et al. (2019) cố gắng giải quyết các hạn chế của giải biên dịch thần kinh với hai giai đoạn tuần tự: tạo code sketch và sửa lỗi lặp đi lặp lại. Cuối cùng, Liang et al. (2021b) sử dụng một phương pháp gần với của chúng tôi, và huấn luyện các mô hình Transformer để dịch giữa mã nhị phân và C.

Biểu diễn trung gian gần như cùng tuổi với thiết kế trình biên dịch. IR đầu tiên, UNCOL (Strong et al., 1958) được giới thiệu vào giữa những năm 1950, cùng với ý tưởng tái sử dụng cùng trình biên dịch cho nhiều ngôn ngữ và máy. Năm 1960, NELIAC (một biến thể của ALGOL) (Huskey et al., 1960) là trình biên dịch có thể nhắm mục tiêu lại đầu tiên, có thể chuyển đổi sang các kiến trúc khác nhau. Feldman (1979) mô tả cách một trình biên dịch cho Fortran 77 có thể được thêm vào các trình biên dịch C của Johnson (1979) và Ritchie (1979). GCC (Stallman, 2001) giới thiệu Register Transfer Language (RTL) một IR cấp thấp lấy cảm hứng từ Davidson và Fraser (1980), và sau đó GENERIC và GIMPLE (Merrill, 2003), tiền thân của IR được sử dụng trong LLVM (Lattner và Adve, 2004).

8 KẾT LUẬN
Trong bài báo này, chúng tôi tận dụng LLVM IR để cải thiện dịch máy thần kinh cho mã nguồn. IR cung cấp một ngôn ngữ chung giàu ngữ nghĩa, mà mã C++, Go, Java và Rust đều có thể được biên dịch vào. Chúng tôi phát triển ba mục tiêu, được thiết kế để tận dụng IR cho biểu diễn đa ngôn ngữ tốt hơn của mã nguồn, dẫn đến cải thiện tương đối trung bình 11% cho dịch mã. Chúng tôi cũng cho thấy rằng các transformer chuỗi-đến-chuỗi hoạt động tốt cho giải biên dịch thần kinh, và sử dụng điều này cho dịch pivot.

Chúng tôi chỉ làm việc với LLVM IR, nhưng cách tiếp cận của chúng tôi có thể áp dụng rộng rãi cho bất kỳ cặp ngôn ngữ nào chia sẻ Biểu diễn Trung gian chung. Tổng quát hơn, bất kỳ IR nào cũng có thể giúp cải thiện biểu diễn mã bằng cách gắn chúng với ngữ nghĩa. Một hạn chế khác là quy mô của các chuỗi nguồn và đích hiện tại của chúng tôi. Như công việc tương lai, LLVM IR có thể được tạo ra ở quy mô lớn hơn bằng cách biên dịch toàn bộ dự án, điều này sẽ cải thiện đáng kể tỷ lệ phần trăm biên dịch IR thành công trong Bảng 1. Nhiều ngôn ngữ và IR hơn có thể được sử dụng, và những mở rộng đó có thể được hỗ trợ bởi các mô hình lớn hơn.
