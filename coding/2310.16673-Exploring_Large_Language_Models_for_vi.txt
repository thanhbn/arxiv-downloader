# 2310.16673.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2310.16673.pdf
# Kích thước tệp: 766277 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Khám phá các Mô hình Ngôn ngữ Lớn cho
Giải thích Mã
Paheli Bhattacharya1,*,†, Manojit Chakraborty1,†, Kartheek N S N Palepu1,
Vikas Pandey1, Ishan Dindorkar2, Rakesh Rajpurohit2và Rishabh Gupta1
1Trung tâm Nghiên cứu và Công nghệ Bosch, Bangalore, Ấn Độ
2Công nghệ Phần mềm Toàn cầu Bosch, Bangalore, Ấn Độ
Tóm tắt
Tự động hóa tài liệu mã thông qua văn bản giải thích có thể chứng minh là rất có lợi trong việc hiểu mã.
Các Mô hình Ngôn ngữ Lớn (LLMs) đã đạt được những bước tiến đáng kể trong Xử lý Ngôn ngữ Tự nhiên,
đặc biệt trong các tác vụ kỹ thuật phần mềm như tạo mã và tóm tắt mã. Nghiên cứu này đặc biệt đi sâu
vào tác vụ tạo ra các bản tóm tắt ngôn ngữ tự nhiên cho các đoạn mã, sử dụng các LLMs khác nhau. Các
phát hiện cho thấy rằng các LLMs Mã vượt trội hơn các đối tác chung của chúng, và các phương pháp zero-shot
mang lại kết quả vượt trội khi xử lý các bộ dữ liệu có phân phối khác biệt giữa tập huấn luyện và tập kiểm tra.
Từ khóa
Tạo Bình luận Mã, Tóm tắt Mã, Mô hình Ngôn ngữ Lớn, AI cho Kỹ thuật Phần mềm
1. Giới thiệu
Hiểu các mã cũ trong các kho mã lớn là một thách thức lớn trong lĩnh vực kỹ thuật phần mềm.
Liang et.al. [1] đã chỉ ra rằng chỉ có 15,4% mã Java GitHub được tài liệu hóa.
Điều này làm cho việc hiểu chức năng cơ bản trở nên khó khăn và tốn thời gian đối với các nhà phát triển [2,3].
Do đó, việc tự động hóa tác vụ tài liệu mã thông qua các giải thích có thể chứng minh là có lợi.

Các Mô hình Ngôn ngữ Lớn (LLMs) đã mang lại một bước đột phá tiến bộ trong Xử lý Ngôn ngữ Tự nhiên,
đặc biệt trong lĩnh vực AI Tạo sinh. LLMs đã được áp dụng trong nhiều tác vụ kỹ thuật phần mềm [4],
phổ biến trong tạo mã [5], tóm tắt mã [6] và tạo ca kiểm tra đơn vị [7].

Trong bài báo này, chúng tôi tập trung vào tác vụ giải thích mã – tạo ra ý định hoặc tóm tắt
bằng ngôn ngữ tự nhiên cho một đoạn mã cho trước. Chúng tôi đánh giá một bộ LLMs – cả LLMs chung [8]
và LLMs Mã [9] sử dụng các phương pháp zero-shot, few-shot và fine-tuning hướng dẫn.
Các thí nghiệm mở rộng trên bộ dữ liệu IRSE [10] dẫn đến những hiểu biết sau: (i) LLMs Mã
hoạt động tốt hơn LLMs chung cho tác vụ này. (ii) Các phương pháp zero-shot đạt kết quả tốt hơn
few-shot và fine-tuning, khi các tập huấn luyện và kiểm tra có phân phối khác biệt.
Diễn đàn Đánh giá Truy xuất Thông tin, ngày 15-18 tháng 12, 2023, Ấn Độ
*Tác giả liên hệ.
†Đóng góp bằng nhau
/envel⌢pe-⌢penpaheli.bhattacharya@bosch.com (P. Bhattacharya)
©2023 Bản quyền cho bài báo này thuộc về các tác giả. Sử dụng được phép theo Giấy phép Creative Commons Attribution 4.0 International (CC BY 4.0)
CEUR
Workshop
ProceedingsHTTP://ceur-ws.org
ISSN 1613-0073
CEUR Workshop Proceedings (CEUR-WS.org)arXiv:2310.16673v1 [cs.SE] 25 Oct 2023

--- TRANG 2 ---
Bảng 1
Ví dụ các điểm dữ liệu từ bộ dữ liệu IRSE và conala-train cùng với độ dài trung bình của chúng (#từ).
Bộ dữ liệu Kích thước Ví dụ Đoạn Mã Ví dụ Giải thích Mã Độ dài Trung bình
Mã Bình luận
IRSE100pattern = re.compile('\\s+')
sentence = re.sub(pattern, "", sentence)Đoạn mã này sử dụng module re (biểu thức chính quy) trong Python
để định nghĩa một mẫu khớp với một hoặc nhiều ký tự khoảng trắng.
Sau đó sử dụng hàm re.sub() để loại bỏ bất kỳ sự xuất hiện nào của
mẫu từ biến chuỗi 'sentence'. Kết quả là một phiên bản được sửa đổi
của 'sentence' với tất cả các ký tự khoảng trắng được loại bỏ.21.18 84.28
conala-train 1666 re.sub('[^A-Z]', ", s) loại bỏ các ký tự viết hoa trong chuỗi 's' 13.92 14.68

2. Công trình Liên quan
Giải thích mã [11], còn được gọi là tóm tắt mã [3,12] và tạo bình luận [13, 2], là một vấn đề quan trọng
trong lĩnh vực kỹ thuật phần mềm. Các phương pháp truyền thống [14,15, 16] cũng như các phương pháp
học sâu [13, 2] đã được thử nghiệm cho tác vụ này.

Các Mô hình Ngôn ngữ Lớn đã được sử dụng thành công trong một loạt các tác vụ tạo ngôn ngữ tự nhiên [17].
Khả năng zero shot và few shot của các hệ thống này làm cho chúng có khả năng thích ứng cao với bất kỳ
tác vụ NLP nào. Có một số LLMs mã nguồn mở cho lĩnh vực chung như LLama-2 [8], Alpaca [18] và Falcon [19].
Cũng có các LLMs Mã đã được huấn luyện hoặc fine-tuned trên dữ liệu cụ thể về mã (thường là các tệp mã nguồn,
bao gồm hơn 80 ngôn ngữ lập trình). Các LLMs phổ biến nhất cho mã là OpenAI CodeX và Co-pilot. Trong số
các mô hình mã nguồn mở, chúng ta có StarCoder [9], CodeUp [5], CodeLlama [20] và Llama-2-Coder [21].

Các Mô hình Ngôn ngữ Lớn đã được sử dụng cho Giải thích mã trong cài đặt few shot [3,22].
Ahmed et.al. [3] phát hiện rằng việc đưa ra các ví dụ few shot từ cùng một dự án cho kết quả tốt hơn
so với từ một dự án khác. Geng et.al. [22] chỉ ra rằng việc chọn các ví dụ có liên quan trong cài đặt
few shot là một tiêu chí thiết kế quan trọng.

3. Bộ dữ liệu
Trong công trình này, chúng tôi xem xét một bộ dữ liệu gồm 100 mẫu được phát hành tại track Truy xuất Thông tin
trong Kỹ thuật Phần mềm (IRSE) tại Diễn đàn Đánh giá Truy xuất Thông tin (FIRE) 2023 [10].
Mỗi mẫu trong bộ dữ liệu là một cặp (𝑐𝑜𝑑𝑒 𝑠𝑛𝑖𝑝𝑝𝑒𝑡, 𝑐𝑜𝑑𝑒 𝑒𝑥𝑝𝑙𝑎𝑛𝑎𝑡𝑖𝑜𝑛). Giải thích là một
mô tả ngôn ngữ tự nhiên biểu thị tác vụ mà đoạn mã đang thực hiện. Chúng tôi gọi bộ dữ liệu này
là "IRSE" trong phần còn lại của bài báo. Ngoài ra, chúng tôi sử dụng bộ dữ liệu conala-train [23]
có sẵn công khai như một nguồn dữ liệu thứ cấp cho few-shot và instruction finetuning.
Bộ dữ liệu này bao gồm 1666 mẫu duy nhất của các cặp (𝑐𝑜𝑑𝑒 𝑠𝑛𝑖𝑝𝑝𝑒𝑡, 𝑐𝑜𝑑𝑒 𝑒𝑥𝑝𝑙𝑎𝑛𝑎𝑡𝑖𝑜𝑛).
Bảng 1 cho thấy một số ví dụ từ cả hai bộ dữ liệu. Có thể quan sát thấy rằng trong khi các đoạn mã
có độ dài tương đương (lần lượt là 21 và 14 tokens), các giải thích mã trong bộ dữ liệu IRSE dài hơn
(độ dài trung bình = 84 từ) so với những cái trong tập conala-train (độ dài trung bình = 15 từ).

4. Đánh giá
Các mô tả văn bản được tạo bởi mô hình được đánh giá so với các giải thích ground truth
sử dụng các thước đo sau:
(i)Dựa trên Token: Điểm BLEU [24] kết hợp các điểm chính xác của n-grams (thường lên đến 4-
grams) sử dụng trung bình hình học có trọng số, với trọng số cao hơn được đưa cho các n-grams ngắn hơn. BLEU-1,

--- TRANG 3 ---
Bảng 2
Các mẫu prompt zero-shot được sử dụng cho Giải thích Mã. {𝑐𝑜𝑑𝑒} biểu thị đoạn mã truy vấn cần tạo giải thích.
# Prompt Mô hình
P1[INST] <>
Bạn là một chuyên gia về Lập trình. Dưới đây là một dòng mã python mô tả một tác vụ.
Chỉ trả về một dòng tóm tắt mô tả phù hợp tác vụ mà mã đang thực hiện. Bạn chỉ được viết
tóm tắt mà không có bất kỳ giải thích tiền tố hoặc hậu tố nào.
Ghi chú: Tóm tắt nên có tối thiểu 1 từ và có thể có trung bình 10 từ.
<>
{code} [/INST]Llama-2-70B-Chat
CodeLlama-13B-Instruct
CodeUp-13B-Chat
P2#Human: Bạn là một người tóm tắt mã hữu ích. Vui lòng mô tả bằng tiếng Anh đơn giản
mục đích của đoạn mã Python sau: { code}
#Assistant:StarCoder (15.5B)
Llama-2-Coder-7B
BLEU-2, và BLEU-N (cho bất kỳ số nguyên N nào) mở rộng đánh giá đến unigrams, bigrams, và
n-grams có độ dài khác nhau, tương ứng.
(ii)Dựa trên Ngữ nghĩa: Chúng tôi sử dụng thước đo này để đánh giá sự tương tự ngữ nghĩa giữa
giải thích được tạo bởi mô hình (𝑚) và giải thích ground truth (𝑔). Chúng tôi chiếu cả 𝑚 và 𝑔 vào
một không gian embedding liên tục, − →𝑒𝑚 và − →𝑒𝑔 tương ứng sử dụng mô hình CodeBERT [25] được huấn luyện trước.
Sau đó chúng tôi tính cosine similarity giữa các embeddings 𝑐𝑜𝑠𝑖𝑛𝑒 (− →𝑒𝑚,− →𝑒𝑔) để có được điểm số.

5. Phương pháp
Chúng tôi thí nghiệm với 5 LLMs (i) LLM Chung: mô hình Llama-2-70B-Chat [8], là mô hình lớn nhất,
mã nguồn mở có sẵn. (ii) LLM Mã – các mô hình Llama-2-Coder-7B [21], CodeLlama-13B-Instruct [20],
CodeUp-13B-Chat [5] và StarCoder [9] (15.5B), sử dụng các chiến lược zero-shot, few-shot và instruction fine-tuning,
được mô tả bên dưới:
(i) Zero-shot: Trong cài đặt này, chúng tôi trực tiếp prompt LLM để tạo output cho một đoạn mã
input cụ thể. Chúng tôi thí nghiệm với nhiều prompts, một số trong đó được liệt kê trong Bảng 2
như prompts P1 và P2.
Dựa trên các model cards, chúng tôi cung cấp mẫu prompt P1 cho các mô hình Llama-2-70B Chat,
CodeLlama-13B-Instruct và CodeUp-13B-Chat. Mẫu P2 được cung cấp cho các mô hình Star-
Coder và Llama-2-Coder-7B.
(ii) Few-shot: Trong prompting few shot, chúng tôi cung cấp một số ví dụ minh họa bản chất
của tác vụ. Đối với tác vụ giải thích mã [3] đề xuất sử dụng 10 ví dụ trong cài đặt few-shot.
Do đó, chúng tôi cung cấp 10 cặp (𝑐𝑜𝑑𝑒 𝑠𝑛𝑖𝑝𝑝𝑒𝑡, 𝑛𝑎𝑡𝑢𝑟𝑎𝑙 𝑙𝑎𝑛𝑔𝑢𝑎𝑔𝑒 𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑖𝑜𝑛)
được chọn ngẫu nhiên từ tập conala-train (tham khảo Phần 3).
(iii) Instruction Finetuning: Đối với instruction finetuning của LLMs, chúng tôi lấy mô hình CodeUp-13B-Chat [5].
Chúng tôi lấy mỗi mẫu từ bộ dữ liệu conala-train và tạo các instance huấn luyện dựa trên instruction
sử dụng định dạng sau:
Dưới đây là một hướng dẫn mô tả một tác vụ, được ghép nối với một input cung cấp thêm ngữ cảnh.
Viết một phản hồi hoàn thành phù hợp yêu cầu.
### Instruction: Dưới đây là một dòng mã python mô tả một tác vụ. Viết một dòng tóm tắt
mô tả phù hợp tác vụ mà mã đang thực hiện.
### Input: 𝑠𝑜𝑟𝑡𝑒𝑑 (𝑙, 𝑘𝑒𝑦 =𝑙𝑎𝑚𝑏𝑑𝑎𝑥 : (−𝑖𝑛𝑡(𝑥[1]), 𝑥[0]))

--- TRANG 4 ---
Bảng 3
Đánh giá hiệu suất của các LLMs khác nhau và ba phương pháp cho tác vụ giải thích mã trên
bộ dữ liệu IRSE. Chúng tôi báo cáo các thước đo dựa trên BLEU và CodeBERT.
Phương pháp LLM Dựa trên Token Dựa trên Ngữ nghĩa
BLEU1 BLEU2 BLEUN CodeBERT
Zero ShotLlama2-70B-Chat 0.019 0.008 0.004 0.338
CodeLlama-13B-Instruct 0.189 0.073 0.036 0.498
CodeUp-13B 0.010 0.003 0.001 0.310
StarCoder-15.5B 0.069 0.024 0.005 0.336
Llama-2-Coder-7B 0.189 0.075 0.023 0.475
Few ShotLlama2-70B-Chat 0.064 0.024 0.012 0.424
CodeLlama-13B-Instruct 0.164 0.073 0.044 0.483
CodeUp-13B 0.061 0.023 0.011 0.416
StarCoder-15.5B 0.020 0.006 0.002 0.347
Llama-2-Coder-7B 0.023 0.008 0.003 0.342
Instruction Finetuning
Zero ShotCodeUp-13B 0.047 0.011 0.005 0.429
### Output: Sắp xếp một danh sách lồng nhau theo hai phần tử
Chúng tôi tải mô hình CodeUp-13B-Chat với lượng tử hóa 4-bit sử dụng các phương pháp QLoRA [26]
và bitsandbytes [27]. Sau đó chúng tôi thực hiện parameter-efficient finetuning (PEFT) [28] của mô hình
sử dụng bộ dữ liệu đã chuẩn bị ở trên.

6. Kết quả
Bảng 3 cho thấy hiệu suất của 5 LLMs khác nhau qua ba phương pháp – zero-shot, few-shot và zero-shot
trên mô hình Instruction finetuned. CodeLlama-13B-Instruct và Llama-2-Coder-7B có hiệu suất zero-shot
tốt nhất so với các LLMs khác. Lưu ý rằng mặc dù mô hình Llama2 chung là lớn nhất về kích thước (70B),
nó có hiệu suất kém khi so sánh với các mô hình LLM Mã nhỏ hơn (13B, 7B). Điều này cho thấy rằng
các mô hình cụ thể theo lĩnh vực hoạt động tốt hơn những cái chung.

Trong khi chiến lược few shot được kỳ vọng mang lại hiệu suất tốt hơn zero-shot, trong nghiên cứu này
chúng tôi thấy rằng hiệu suất lại tệ hơn. Điều này chủ yếu là do các ví dụ few shot đã được chọn từ tập conala-train.
Như đã thảo luận trong Phần 3 và Bảng 1, độ dài giải thích mã trong bộ dữ liệu IRSE và bộ dữ liệu conala-train
khác nhau rất nhiều. Vì các LLMs thấy các ví dụ few shot từ conala-train, nó tạo ra các giải thích mã độ dài
ngắn hơn cho các mẫu input đến từ bộ dữ liệu IRSE. Sự không khớp phân phối train-test này khiến các mô hình
hoạt động tệ hơn trong kịch bản few shot so với zero-shot.

Các lập luận tương tự có thể được rút ra cho phương pháp Instruction finetuning+Zero shot, vì dữ liệu huấn luyện
đến từ bộ dữ liệu conala-train khác với bộ dữ liệu IRSE.

7. Kết luận
Trong công trình này, chúng tôi khám phá hiệu suất của 5 LLMs, cả chung và cụ thể theo mã, cho tác vụ
giải thích mã. Chúng tôi sử dụng các phương pháp zero-shot, few shot và instruction finetuning trên
các LLMs và đánh giá hiệu suất của chúng. Chúng tôi thấy rằng các LLMs Mã hoạt động tốt hơn các

--- TRANG 5 ---
LLMs chung lớn hơn. Ngoài ra, prompting zero-shot hoạt động tốt trong kịch bản khi chúng ta không có
đủ ví dụ để prompt/finetune mô hình.
Tài liệu tham khảo
[1]Y. Liang, K. Zhu, Automatic generation of text descriptive comments for code blocks, in:
Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[2]R. Sharma, F. Chen, F. Fard, Lamner: code comment generation using character language
model and named entity recognition, in: Proceedings of the 30th IEEE/ACM International
Conference on Program Comprehension, 2022, pp. 48–59.
[3]T. Ahmed, P. Devanbu, Few-shot training llms for project-specific code-summarization,
in: Proceedings of the 37th IEEE/ACM International Conference on Automated Software
Engineering, 2022, pp. 1–5.
[4]I. Ozkaya, Application of large language models to software engineering tasks: Opportu-
nities, risks, and implications, IEEE Software 40 (2023) 4–8.
[5]J. Jiang, S. Kim, Codeup: A multilingual code generation llama2 model with parameter-
efficient instruction-tuning, https://huggingface.co/deepse, 2023.
[6]M.-F. Wong, S. Guo, C.-N. Hang, S.-W. Ho, C.-W. Tan, Natural language generation and
understanding of big code for AI-assisted programming: A review, Entropy 25 (2023) 888.
URL: https://doi.org/10.3390%2Fe25060888. doi: 10.3390/e25060888 .
[7]M. Schäfer, S. Nadi, A. Eghbali, F. Tip, An empirical evaluation of using large language
models for automated unit test generation, 2023. arXiv:2302.06527 .
[8]H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and fine-tuned chat mod-
els, arXiv preprint arXiv:2307.09288 (2023). URL: https://huggingface.co/meta-llama/
Llama-2-70b-chat-hf.
[9]R. L. et.al., Starcoder: may the source be with you!, arXiv preprint arXiv:2305.06161 (2023).
URL: https://huggingface.co/bigcode/starcoder.
[10] S. Majumdar, S. Paul, D. Paul, A. Bandyopadhyay, B. Dave, S. Chattopadhyay, P. P. Das, P. D.
Clough, P. Majumder, Generative ai for software metadata: Overview of the information
retrieval in software engineering track at fire 2023, in: Forum for Information Retrieval
Evaluation, ACM, 2023.
[11] S. MacNeil, A. Tran, A. Hellas, J. Kim, S. Sarsa, P. Denny, S. Bernstein, J. Leinonen, Ex-
periences from using code explanations generated by large language models in a web
software development e-book, in: Proceedings of the 54th ACM Technical Symposium on
Computer Science Education V. 1, 2023, pp. 931–937.
[12] S. Iyer, I. Konstas, A. Cheung, L. Zettlemoyer, Summarizing source code using a neural
attention model, in: 54th Annual Meeting of the Association for Computational Linguistics
2016, Association for Computational Linguistics, 2016, pp. 2073–2083.
[13] X. Hu, G. Li, X. Xia, D. Lo, Z. Jin, Deep code comment generation, in: Proceedings of
the 26th Conference on Program Comprehension, Association for Computing Machinery,
2018, p. 200–210.
[14] S. Haiduc, J. Aponte, L. Moreno, A. Marcus, On the use of automated text summarization

--- TRANG 6 ---
techniques for summarizing source code, in: 2010 17th Working conference on reverse
engineering, IEEE, 2010, pp. 35–44.
[15] B. P. Eddy, J. A. Robinson, N. A. Kraft, J. C. Carver, Evaluating source code summarization
techniques: Replication and expansion, in: 2013 21st International Conference on Program
Comprehension (ICPC), IEEE, 2013, pp. 13–22.
[16] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, K. Vijay-Shanker, Automatic
generation of natural language summaries for java classes, in: 2013 21st International
conference on program comprehension (ICPC), IEEE, 2013, pp. 23–32.
[17] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, X. Hu, Harnessing the power of
llms in practice: A survey on chatgpt and beyond, arXiv preprint arXiv:2304.13712 (2023).
[18] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, T. B. Hashimoto,
Alpaca: A strong, replicable instruction-following model, Stanford Center for Research on
Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html 3 (2023) 7.
[19] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier,
E. Almazrouei, J. Launay, The RefinedWeb dataset for Falcon LLM: outperforming curated
corpora with web data, and web data only, arXiv preprint arXiv:2306.01116 (2023).
[20] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,
et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950
(2023). URL: https://huggingface.co/codellama.
[21] Manuel Romero, llama-2-coder-7b (revision d30d193), 2023. URL: https://huggingface.co/
mrm8488/llama-2-coder-7b. doi: 10.57967/hf/0931 .
[22] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, X. Liao, Large language models
are few-shot summarizers: Multi-intent comment generation via in-context learning (2024).
[23] P. Yin, B. Deng, E. Chen, B. Vasilescu, G. Neubig, Learning to mine aligned code and natural
language pairs from stack overflow, in: International Conference on Mining Software
Repositories, ACM, 2018, pp. 476–486. URL: https://conala-corpus.github.io/.
[24] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, Bleu: A method for automatic evaluation of
machine translation, Association for Computational Linguistics, USA, 2002, p. 311–318.
[25] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al.,
Codebert: A pre-trained model for programming and natural languages, in: Findings of
the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536–1547.
[26] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of
quantized llms, 2023. arXiv:2305.14314 .
[27] T. Dettmers, M. Lewis, S. Shleifer, L. Zettlemoyer, 8-bit optimizers via block-wise quanti-
zation, 9th International Conference on Learning Representations, ICLR (2022).
[28] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, B. Bossan, Peft: State-of-the-art
parameter-efficient fine-tuning methods, https://github.com/huggingface/peft, 2022.
