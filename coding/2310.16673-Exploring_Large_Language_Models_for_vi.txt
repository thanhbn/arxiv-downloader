# 2310.16673.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/coding/2310.16673.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 766277 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
KhÃ¡m phÃ¡ cÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n cho
Giáº£i thÃ­ch MÃ£
Paheli Bhattacharya1,*,â€ , Manojit Chakraborty1,â€ , Kartheek N S N Palepu1,
Vikas Pandey1, Ishan Dindorkar2, Rakesh Rajpurohit2vÃ  Rishabh Gupta1
1Trung tÃ¢m NghiÃªn cá»©u vÃ  CÃ´ng nghá»‡ Bosch, Bangalore, áº¤n Äá»™
2CÃ´ng nghá»‡ Pháº§n má»m ToÃ n cáº§u Bosch, Bangalore, áº¤n Äá»™
TÃ³m táº¯t
Tá»± Ä‘á»™ng hÃ³a tÃ i liá»‡u mÃ£ thÃ´ng qua vÄƒn báº£n giáº£i thÃ­ch cÃ³ thá»ƒ chá»©ng minh lÃ  ráº¥t cÃ³ lá»£i trong viá»‡c hiá»ƒu mÃ£.
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs) Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng bÆ°á»›c tiáº¿n Ä‘Ã¡ng ká»ƒ trong Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn,
Ä‘áº·c biá»‡t trong cÃ¡c tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m nhÆ° táº¡o mÃ£ vÃ  tÃ³m táº¯t mÃ£. NghiÃªn cá»©u nÃ y Ä‘áº·c biá»‡t Ä‘i sÃ¢u
vÃ o tÃ¡c vá»¥ táº¡o ra cÃ¡c báº£n tÃ³m táº¯t ngÃ´n ngá»¯ tá»± nhiÃªn cho cÃ¡c Ä‘oáº¡n mÃ£, sá»­ dá»¥ng cÃ¡c LLMs khÃ¡c nhau. CÃ¡c
phÃ¡t hiá»‡n cho tháº¥y ráº±ng cÃ¡c LLMs MÃ£ vÆ°á»£t trá»™i hÆ¡n cÃ¡c Ä‘á»‘i tÃ¡c chung cá»§a chÃºng, vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p zero-shot
mang láº¡i káº¿t quáº£ vÆ°á»£t trá»™i khi xá»­ lÃ½ cÃ¡c bá»™ dá»¯ liá»‡u cÃ³ phÃ¢n phá»‘i khÃ¡c biá»‡t giá»¯a táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra.
Tá»« khÃ³a
Táº¡o BÃ¬nh luáº­n MÃ£, TÃ³m táº¯t MÃ£, MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n, AI cho Ká»¹ thuáº­t Pháº§n má»m
1. Giá»›i thiá»‡u
Hiá»ƒu cÃ¡c mÃ£ cÅ© trong cÃ¡c kho mÃ£ lá»›n lÃ  má»™t thÃ¡ch thá»©c lá»›n trong lÄ©nh vá»±c ká»¹ thuáº­t pháº§n má»m.
Liang et.al. [1] Ä‘Ã£ chá»‰ ra ráº±ng chá»‰ cÃ³ 15,4% mÃ£ Java GitHub Ä‘Æ°á»£c tÃ i liá»‡u hÃ³a.
Äiá»u nÃ y lÃ m cho viá»‡c hiá»ƒu chá»©c nÄƒng cÆ¡ báº£n trá»Ÿ nÃªn khÃ³ khÄƒn vÃ  tá»‘n thá»i gian Ä‘á»‘i vá»›i cÃ¡c nhÃ  phÃ¡t triá»ƒn [2,3].
Do Ä‘Ã³, viá»‡c tá»± Ä‘á»™ng hÃ³a tÃ¡c vá»¥ tÃ i liá»‡u mÃ£ thÃ´ng qua cÃ¡c giáº£i thÃ­ch cÃ³ thá»ƒ chá»©ng minh lÃ  cÃ³ lá»£i.

CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLMs) Ä‘Ã£ mang láº¡i má»™t bÆ°á»›c Ä‘á»™t phÃ¡ tiáº¿n bá»™ trong Xá»­ lÃ½ NgÃ´n ngá»¯ Tá»± nhiÃªn,
Ä‘áº·c biá»‡t trong lÄ©nh vá»±c AI Táº¡o sinh. LLMs Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng trong nhiá»u tÃ¡c vá»¥ ká»¹ thuáº­t pháº§n má»m [4],
phá»• biáº¿n trong táº¡o mÃ£ [5], tÃ³m táº¯t mÃ£ [6] vÃ  táº¡o ca kiá»ƒm tra Ä‘Æ¡n vá»‹ [7].

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i táº­p trung vÃ o tÃ¡c vá»¥ giáº£i thÃ­ch mÃ£ â€“ táº¡o ra Ã½ Ä‘á»‹nh hoáº·c tÃ³m táº¯t
báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn cho má»™t Ä‘oáº¡n mÃ£ cho trÆ°á»›c. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ má»™t bá»™ LLMs â€“ cáº£ LLMs chung [8]
vÃ  LLMs MÃ£ [9] sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p zero-shot, few-shot vÃ  fine-tuning hÆ°á»›ng dáº«n.
CÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng trÃªn bá»™ dá»¯ liá»‡u IRSE [10] dáº«n Ä‘áº¿n nhá»¯ng hiá»ƒu biáº¿t sau: (i) LLMs MÃ£
hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n LLMs chung cho tÃ¡c vá»¥ nÃ y. (ii) CÃ¡c phÆ°Æ¡ng phÃ¡p zero-shot Ä‘áº¡t káº¿t quáº£ tá»‘t hÆ¡n
few-shot vÃ  fine-tuning, khi cÃ¡c táº­p huáº¥n luyá»‡n vÃ  kiá»ƒm tra cÃ³ phÃ¢n phá»‘i khÃ¡c biá»‡t.
Diá»…n Ä‘Ã n ÄÃ¡nh giÃ¡ Truy xuáº¥t ThÃ´ng tin, ngÃ y 15-18 thÃ¡ng 12, 2023, áº¤n Äá»™
*TÃ¡c giáº£ liÃªn há»‡.
â€ ÄÃ³ng gÃ³p báº±ng nhau
/envelâŒ¢pe-âŒ¢penpaheli.bhattacharya@bosch.com (P. Bhattacharya)
Â©2023 Báº£n quyá»n cho bÃ i bÃ¡o nÃ y thuá»™c vá» cÃ¡c tÃ¡c giáº£. Sá»­ dá»¥ng Ä‘Æ°á»£c phÃ©p theo Giáº¥y phÃ©p Creative Commons Attribution 4.0 International (CC BY 4.0)
CEUR
Workshop
ProceedingsHTTP://ceur-ws.org
ISSN 1613-0073
CEUR Workshop Proceedings (CEUR-WS.org)arXiv:2310.16673v1 [cs.SE] 25 Oct 2023

--- TRANG 2 ---
Báº£ng 1
VÃ­ dá»¥ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u tá»« bá»™ dá»¯ liá»‡u IRSE vÃ  conala-train cÃ¹ng vá»›i Ä‘á»™ dÃ i trung bÃ¬nh cá»§a chÃºng (#tá»«).
Bá»™ dá»¯ liá»‡u KÃ­ch thÆ°á»›c VÃ­ dá»¥ Äoáº¡n MÃ£ VÃ­ dá»¥ Giáº£i thÃ­ch MÃ£ Äá»™ dÃ i Trung bÃ¬nh
MÃ£ BÃ¬nh luáº­n
IRSE100pattern = re.compile('\\s+')
sentence = re.sub(pattern, "", sentence)Äoáº¡n mÃ£ nÃ y sá»­ dá»¥ng module re (biá»ƒu thá»©c chÃ­nh quy) trong Python
Ä‘á»ƒ Ä‘á»‹nh nghÄ©a má»™t máº«u khá»›p vá»›i má»™t hoáº·c nhiá»u kÃ½ tá»± khoáº£ng tráº¯ng.
Sau Ä‘Ã³ sá»­ dá»¥ng hÃ m re.sub() Ä‘á»ƒ loáº¡i bá» báº¥t ká»³ sá»± xuáº¥t hiá»‡n nÃ o cá»§a
máº«u tá»« biáº¿n chuá»—i 'sentence'. Káº¿t quáº£ lÃ  má»™t phiÃªn báº£n Ä‘Æ°á»£c sá»­a Ä‘á»•i
cá»§a 'sentence' vá»›i táº¥t cáº£ cÃ¡c kÃ½ tá»± khoáº£ng tráº¯ng Ä‘Æ°á»£c loáº¡i bá».21.18 84.28
conala-train 1666 re.sub('[^A-Z]', ", s) loáº¡i bá» cÃ¡c kÃ½ tá»± viáº¿t hoa trong chuá»—i 's' 13.92 14.68

2. CÃ´ng trÃ¬nh LiÃªn quan
Giáº£i thÃ­ch mÃ£ [11], cÃ²n Ä‘Æ°á»£c gá»i lÃ  tÃ³m táº¯t mÃ£ [3,12] vÃ  táº¡o bÃ¬nh luáº­n [13, 2], lÃ  má»™t váº¥n Ä‘á» quan trá»ng
trong lÄ©nh vá»±c ká»¹ thuáº­t pháº§n má»m. CÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng [14,15, 16] cÅ©ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p
há»c sÃ¢u [13, 2] Ä‘Ã£ Ä‘Æ°á»£c thá»­ nghiá»‡m cho tÃ¡c vá»¥ nÃ y.

CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng thÃ nh cÃ´ng trong má»™t loáº¡t cÃ¡c tÃ¡c vá»¥ táº¡o ngÃ´n ngá»¯ tá»± nhiÃªn [17].
Kháº£ nÄƒng zero shot vÃ  few shot cá»§a cÃ¡c há»‡ thá»‘ng nÃ y lÃ m cho chÃºng cÃ³ kháº£ nÄƒng thÃ­ch á»©ng cao vá»›i báº¥t ká»³
tÃ¡c vá»¥ NLP nÃ o. CÃ³ má»™t sá»‘ LLMs mÃ£ nguá»“n má»Ÿ cho lÄ©nh vá»±c chung nhÆ° LLama-2 [8], Alpaca [18] vÃ  Falcon [19].
CÅ©ng cÃ³ cÃ¡c LLMs MÃ£ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n hoáº·c fine-tuned trÃªn dá»¯ liá»‡u cá»¥ thá»ƒ vá» mÃ£ (thÆ°á»ng lÃ  cÃ¡c tá»‡p mÃ£ nguá»“n,
bao gá»“m hÆ¡n 80 ngÃ´n ngá»¯ láº­p trÃ¬nh). CÃ¡c LLMs phá»• biáº¿n nháº¥t cho mÃ£ lÃ  OpenAI CodeX vÃ  Co-pilot. Trong sá»‘
cÃ¡c mÃ´ hÃ¬nh mÃ£ nguá»“n má»Ÿ, chÃºng ta cÃ³ StarCoder [9], CodeUp [5], CodeLlama [20] vÃ  Llama-2-Coder [21].

CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng cho Giáº£i thÃ­ch mÃ£ trong cÃ i Ä‘áº·t few shot [3,22].
Ahmed et.al. [3] phÃ¡t hiá»‡n ráº±ng viá»‡c Ä‘Æ°a ra cÃ¡c vÃ­ dá»¥ few shot tá»« cÃ¹ng má»™t dá»± Ã¡n cho káº¿t quáº£ tá»‘t hÆ¡n
so vá»›i tá»« má»™t dá»± Ã¡n khÃ¡c. Geng et.al. [22] chá»‰ ra ráº±ng viá»‡c chá»n cÃ¡c vÃ­ dá»¥ cÃ³ liÃªn quan trong cÃ i Ä‘áº·t
few shot lÃ  má»™t tiÃªu chÃ­ thiáº¿t káº¿ quan trá»ng.

3. Bá»™ dá»¯ liá»‡u
Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i xem xÃ©t má»™t bá»™ dá»¯ liá»‡u gá»“m 100 máº«u Ä‘Æ°á»£c phÃ¡t hÃ nh táº¡i track Truy xuáº¥t ThÃ´ng tin
trong Ká»¹ thuáº­t Pháº§n má»m (IRSE) táº¡i Diá»…n Ä‘Ã n ÄÃ¡nh giÃ¡ Truy xuáº¥t ThÃ´ng tin (FIRE) 2023 [10].
Má»—i máº«u trong bá»™ dá»¯ liá»‡u lÃ  má»™t cáº·p (ğ‘ğ‘œğ‘‘ğ‘’ ğ‘ ğ‘›ğ‘–ğ‘ğ‘ğ‘’ğ‘¡, ğ‘ğ‘œğ‘‘ğ‘’ ğ‘’ğ‘¥ğ‘ğ‘™ğ‘ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›). Giáº£i thÃ­ch lÃ  má»™t
mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn biá»ƒu thá»‹ tÃ¡c vá»¥ mÃ  Ä‘oáº¡n mÃ£ Ä‘ang thá»±c hiá»‡n. ChÃºng tÃ´i gá»i bá»™ dá»¯ liá»‡u nÃ y
lÃ  "IRSE" trong pháº§n cÃ²n láº¡i cá»§a bÃ i bÃ¡o. NgoÃ i ra, chÃºng tÃ´i sá»­ dá»¥ng bá»™ dá»¯ liá»‡u conala-train [23]
cÃ³ sáºµn cÃ´ng khai nhÆ° má»™t nguá»“n dá»¯ liá»‡u thá»© cáº¥p cho few-shot vÃ  instruction finetuning.
Bá»™ dá»¯ liá»‡u nÃ y bao gá»“m 1666 máº«u duy nháº¥t cá»§a cÃ¡c cáº·p (ğ‘ğ‘œğ‘‘ğ‘’ ğ‘ ğ‘›ğ‘–ğ‘ğ‘ğ‘’ğ‘¡, ğ‘ğ‘œğ‘‘ğ‘’ ğ‘’ğ‘¥ğ‘ğ‘™ğ‘ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›).
Báº£ng 1 cho tháº¥y má»™t sá»‘ vÃ­ dá»¥ tá»« cáº£ hai bá»™ dá»¯ liá»‡u. CÃ³ thá»ƒ quan sÃ¡t tháº¥y ráº±ng trong khi cÃ¡c Ä‘oáº¡n mÃ£
cÃ³ Ä‘á»™ dÃ i tÆ°Æ¡ng Ä‘Æ°Æ¡ng (láº§n lÆ°á»£t lÃ  21 vÃ  14 tokens), cÃ¡c giáº£i thÃ­ch mÃ£ trong bá»™ dá»¯ liá»‡u IRSE dÃ i hÆ¡n
(Ä‘á»™ dÃ i trung bÃ¬nh = 84 tá»«) so vá»›i nhá»¯ng cÃ¡i trong táº­p conala-train (Ä‘á»™ dÃ i trung bÃ¬nh = 15 tá»«).

4. ÄÃ¡nh giÃ¡
CÃ¡c mÃ´ táº£ vÄƒn báº£n Ä‘Æ°á»£c táº¡o bá»Ÿi mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ so vá»›i cÃ¡c giáº£i thÃ­ch ground truth
sá»­ dá»¥ng cÃ¡c thÆ°á»›c Ä‘o sau:
(i)Dá»±a trÃªn Token: Äiá»ƒm BLEU [24] káº¿t há»£p cÃ¡c Ä‘iá»ƒm chÃ­nh xÃ¡c cá»§a n-grams (thÆ°á»ng lÃªn Ä‘áº¿n 4-
grams) sá»­ dá»¥ng trung bÃ¬nh hÃ¬nh há»c cÃ³ trá»ng sá»‘, vá»›i trá»ng sá»‘ cao hÆ¡n Ä‘Æ°á»£c Ä‘Æ°a cho cÃ¡c n-grams ngáº¯n hÆ¡n. BLEU-1,

--- TRANG 3 ---
Báº£ng 2
CÃ¡c máº«u prompt zero-shot Ä‘Æ°á»£c sá»­ dá»¥ng cho Giáº£i thÃ­ch MÃ£. {ğ‘ğ‘œğ‘‘ğ‘’} biá»ƒu thá»‹ Ä‘oáº¡n mÃ£ truy váº¥n cáº§n táº¡o giáº£i thÃ­ch.
# Prompt MÃ´ hÃ¬nh
P1[INST] <>
Báº¡n lÃ  má»™t chuyÃªn gia vá» Láº­p trÃ¬nh. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t dÃ²ng mÃ£ python mÃ´ táº£ má»™t tÃ¡c vá»¥.
Chá»‰ tráº£ vá» má»™t dÃ²ng tÃ³m táº¯t mÃ´ táº£ phÃ¹ há»£p tÃ¡c vá»¥ mÃ  mÃ£ Ä‘ang thá»±c hiá»‡n. Báº¡n chá»‰ Ä‘Æ°á»£c viáº¿t
tÃ³m táº¯t mÃ  khÃ´ng cÃ³ báº¥t ká»³ giáº£i thÃ­ch tiá»n tá»‘ hoáº·c háº­u tá»‘ nÃ o.
Ghi chÃº: TÃ³m táº¯t nÃªn cÃ³ tá»‘i thiá»ƒu 1 tá»« vÃ  cÃ³ thá»ƒ cÃ³ trung bÃ¬nh 10 tá»«.
<>
{code} [/INST]Llama-2-70B-Chat
CodeLlama-13B-Instruct
CodeUp-13B-Chat
P2#Human: Báº¡n lÃ  má»™t ngÆ°á»i tÃ³m táº¯t mÃ£ há»¯u Ã­ch. Vui lÃ²ng mÃ´ táº£ báº±ng tiáº¿ng Anh Ä‘Æ¡n giáº£n
má»¥c Ä‘Ã­ch cá»§a Ä‘oáº¡n mÃ£ Python sau: { code}
#Assistant:StarCoder (15.5B)
Llama-2-Coder-7B
BLEU-2, vÃ  BLEU-N (cho báº¥t ká»³ sá»‘ nguyÃªn N nÃ o) má»Ÿ rá»™ng Ä‘Ã¡nh giÃ¡ Ä‘áº¿n unigrams, bigrams, vÃ 
n-grams cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau, tÆ°Æ¡ng á»©ng.
(ii)Dá»±a trÃªn Ngá»¯ nghÄ©a: ChÃºng tÃ´i sá»­ dá»¥ng thÆ°á»›c Ä‘o nÃ y Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ sá»± tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a giá»¯a
giáº£i thÃ­ch Ä‘Æ°á»£c táº¡o bá»Ÿi mÃ´ hÃ¬nh (ğ‘š) vÃ  giáº£i thÃ­ch ground truth (ğ‘”). ChÃºng tÃ´i chiáº¿u cáº£ ğ‘š vÃ  ğ‘” vÃ o
má»™t khÃ´ng gian embedding liÃªn tá»¥c, âˆ’ â†’ğ‘’ğ‘š vÃ  âˆ’ â†’ğ‘’ğ‘” tÆ°Æ¡ng á»©ng sá»­ dá»¥ng mÃ´ hÃ¬nh CodeBERT [25] Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c.
Sau Ä‘Ã³ chÃºng tÃ´i tÃ­nh cosine similarity giá»¯a cÃ¡c embeddings ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ (âˆ’ â†’ğ‘’ğ‘š,âˆ’ â†’ğ‘’ğ‘”) Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c Ä‘iá»ƒm sá»‘.

5. PhÆ°Æ¡ng phÃ¡p
ChÃºng tÃ´i thÃ­ nghiá»‡m vá»›i 5 LLMs (i) LLM Chung: mÃ´ hÃ¬nh Llama-2-70B-Chat [8], lÃ  mÃ´ hÃ¬nh lá»›n nháº¥t,
mÃ£ nguá»“n má»Ÿ cÃ³ sáºµn. (ii) LLM MÃ£ â€“ cÃ¡c mÃ´ hÃ¬nh Llama-2-Coder-7B [21], CodeLlama-13B-Instruct [20],
CodeUp-13B-Chat [5] vÃ  StarCoder [9] (15.5B), sá»­ dá»¥ng cÃ¡c chiáº¿n lÆ°á»£c zero-shot, few-shot vÃ  instruction fine-tuning,
Ä‘Æ°á»£c mÃ´ táº£ bÃªn dÆ°á»›i:
(i) Zero-shot: Trong cÃ i Ä‘áº·t nÃ y, chÃºng tÃ´i trá»±c tiáº¿p prompt LLM Ä‘á»ƒ táº¡o output cho má»™t Ä‘oáº¡n mÃ£
input cá»¥ thá»ƒ. ChÃºng tÃ´i thÃ­ nghiá»‡m vá»›i nhiá»u prompts, má»™t sá»‘ trong Ä‘Ã³ Ä‘Æ°á»£c liá»‡t kÃª trong Báº£ng 2
nhÆ° prompts P1 vÃ  P2.
Dá»±a trÃªn cÃ¡c model cards, chÃºng tÃ´i cung cáº¥p máº«u prompt P1 cho cÃ¡c mÃ´ hÃ¬nh Llama-2-70B Chat,
CodeLlama-13B-Instruct vÃ  CodeUp-13B-Chat. Máº«u P2 Ä‘Æ°á»£c cung cáº¥p cho cÃ¡c mÃ´ hÃ¬nh Star-
Coder vÃ  Llama-2-Coder-7B.
(ii) Few-shot: Trong prompting few shot, chÃºng tÃ´i cung cáº¥p má»™t sá»‘ vÃ­ dá»¥ minh há»a báº£n cháº¥t
cá»§a tÃ¡c vá»¥. Äá»‘i vá»›i tÃ¡c vá»¥ giáº£i thÃ­ch mÃ£ [3] Ä‘á» xuáº¥t sá»­ dá»¥ng 10 vÃ­ dá»¥ trong cÃ i Ä‘áº·t few-shot.
Do Ä‘Ã³, chÃºng tÃ´i cung cáº¥p 10 cáº·p (ğ‘ğ‘œğ‘‘ğ‘’ ğ‘ ğ‘›ğ‘–ğ‘ğ‘ğ‘’ğ‘¡, ğ‘›ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘ğ‘™ ğ‘™ğ‘ğ‘›ğ‘”ğ‘¢ğ‘ğ‘”ğ‘’ ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn tá»« táº­p conala-train (tham kháº£o Pháº§n 3).
(iii) Instruction Finetuning: Äá»‘i vá»›i instruction finetuning cá»§a LLMs, chÃºng tÃ´i láº¥y mÃ´ hÃ¬nh CodeUp-13B-Chat [5].
ChÃºng tÃ´i láº¥y má»—i máº«u tá»« bá»™ dá»¯ liá»‡u conala-train vÃ  táº¡o cÃ¡c instance huáº¥n luyá»‡n dá»±a trÃªn instruction
sá»­ dá»¥ng Ä‘á»‹nh dáº¡ng sau:
DÆ°á»›i Ä‘Ã¢y lÃ  má»™t hÆ°á»›ng dáº«n mÃ´ táº£ má»™t tÃ¡c vá»¥, Ä‘Æ°á»£c ghÃ©p ná»‘i vá»›i má»™t input cung cáº¥p thÃªm ngá»¯ cáº£nh.
Viáº¿t má»™t pháº£n há»“i hoÃ n thÃ nh phÃ¹ há»£p yÃªu cáº§u.
### Instruction: DÆ°á»›i Ä‘Ã¢y lÃ  má»™t dÃ²ng mÃ£ python mÃ´ táº£ má»™t tÃ¡c vá»¥. Viáº¿t má»™t dÃ²ng tÃ³m táº¯t
mÃ´ táº£ phÃ¹ há»£p tÃ¡c vá»¥ mÃ  mÃ£ Ä‘ang thá»±c hiá»‡n.
### Input: ğ‘ ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘‘ (ğ‘™, ğ‘˜ğ‘’ğ‘¦ =ğ‘™ğ‘ğ‘šğ‘ğ‘‘ğ‘ğ‘¥ : (âˆ’ğ‘–ğ‘›ğ‘¡(ğ‘¥[1]), ğ‘¥[0]))

--- TRANG 4 ---
Báº£ng 3
ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c LLMs khÃ¡c nhau vÃ  ba phÆ°Æ¡ng phÃ¡p cho tÃ¡c vá»¥ giáº£i thÃ­ch mÃ£ trÃªn
bá»™ dá»¯ liá»‡u IRSE. ChÃºng tÃ´i bÃ¡o cÃ¡o cÃ¡c thÆ°á»›c Ä‘o dá»±a trÃªn BLEU vÃ  CodeBERT.
PhÆ°Æ¡ng phÃ¡p LLM Dá»±a trÃªn Token Dá»±a trÃªn Ngá»¯ nghÄ©a
BLEU1 BLEU2 BLEUN CodeBERT
Zero ShotLlama2-70B-Chat 0.019 0.008 0.004 0.338
CodeLlama-13B-Instruct 0.189 0.073 0.036 0.498
CodeUp-13B 0.010 0.003 0.001 0.310
StarCoder-15.5B 0.069 0.024 0.005 0.336
Llama-2-Coder-7B 0.189 0.075 0.023 0.475
Few ShotLlama2-70B-Chat 0.064 0.024 0.012 0.424
CodeLlama-13B-Instruct 0.164 0.073 0.044 0.483
CodeUp-13B 0.061 0.023 0.011 0.416
StarCoder-15.5B 0.020 0.006 0.002 0.347
Llama-2-Coder-7B 0.023 0.008 0.003 0.342
Instruction Finetuning
Zero ShotCodeUp-13B 0.047 0.011 0.005 0.429
### Output: Sáº¯p xáº¿p má»™t danh sÃ¡ch lá»“ng nhau theo hai pháº§n tá»­
ChÃºng tÃ´i táº£i mÃ´ hÃ¬nh CodeUp-13B-Chat vá»›i lÆ°á»£ng tá»­ hÃ³a 4-bit sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p QLoRA [26]
vÃ  bitsandbytes [27]. Sau Ä‘Ã³ chÃºng tÃ´i thá»±c hiá»‡n parameter-efficient finetuning (PEFT) [28] cá»§a mÃ´ hÃ¬nh
sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Ä‘Ã£ chuáº©n bá»‹ á»Ÿ trÃªn.

6. Káº¿t quáº£
Báº£ng 3 cho tháº¥y hiá»‡u suáº¥t cá»§a 5 LLMs khÃ¡c nhau qua ba phÆ°Æ¡ng phÃ¡p â€“ zero-shot, few-shot vÃ  zero-shot
trÃªn mÃ´ hÃ¬nh Instruction finetuned. CodeLlama-13B-Instruct vÃ  Llama-2-Coder-7B cÃ³ hiá»‡u suáº¥t zero-shot
tá»‘t nháº¥t so vá»›i cÃ¡c LLMs khÃ¡c. LÆ°u Ã½ ráº±ng máº·c dÃ¹ mÃ´ hÃ¬nh Llama2 chung lÃ  lá»›n nháº¥t vá» kÃ­ch thÆ°á»›c (70B),
nÃ³ cÃ³ hiá»‡u suáº¥t kÃ©m khi so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh LLM MÃ£ nhá» hÆ¡n (13B, 7B). Äiá»u nÃ y cho tháº¥y ráº±ng
cÃ¡c mÃ´ hÃ¬nh cá»¥ thá»ƒ theo lÄ©nh vá»±c hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n nhá»¯ng cÃ¡i chung.

Trong khi chiáº¿n lÆ°á»£c few shot Ä‘Æ°á»£c ká»³ vá»ng mang láº¡i hiá»‡u suáº¥t tá»‘t hÆ¡n zero-shot, trong nghiÃªn cá»©u nÃ y
chÃºng tÃ´i tháº¥y ráº±ng hiá»‡u suáº¥t láº¡i tá»‡ hÆ¡n. Äiá»u nÃ y chá»§ yáº¿u lÃ  do cÃ¡c vÃ­ dá»¥ few shot Ä‘Ã£ Ä‘Æ°á»£c chá»n tá»« táº­p conala-train.
NhÆ° Ä‘Ã£ tháº£o luáº­n trong Pháº§n 3 vÃ  Báº£ng 1, Ä‘á»™ dÃ i giáº£i thÃ­ch mÃ£ trong bá»™ dá»¯ liá»‡u IRSE vÃ  bá»™ dá»¯ liá»‡u conala-train
khÃ¡c nhau ráº¥t nhiá»u. VÃ¬ cÃ¡c LLMs tháº¥y cÃ¡c vÃ­ dá»¥ few shot tá»« conala-train, nÃ³ táº¡o ra cÃ¡c giáº£i thÃ­ch mÃ£ Ä‘á»™ dÃ i
ngáº¯n hÆ¡n cho cÃ¡c máº«u input Ä‘áº¿n tá»« bá»™ dá»¯ liá»‡u IRSE. Sá»± khÃ´ng khá»›p phÃ¢n phá»‘i train-test nÃ y khiáº¿n cÃ¡c mÃ´ hÃ¬nh
hoáº¡t Ä‘á»™ng tá»‡ hÆ¡n trong ká»‹ch báº£n few shot so vá»›i zero-shot.

CÃ¡c láº­p luáº­n tÆ°Æ¡ng tá»± cÃ³ thá»ƒ Ä‘Æ°á»£c rÃºt ra cho phÆ°Æ¡ng phÃ¡p Instruction finetuning+Zero shot, vÃ¬ dá»¯ liá»‡u huáº¥n luyá»‡n
Ä‘áº¿n tá»« bá»™ dá»¯ liá»‡u conala-train khÃ¡c vá»›i bá»™ dá»¯ liá»‡u IRSE.

7. Káº¿t luáº­n
Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ hiá»‡u suáº¥t cá»§a 5 LLMs, cáº£ chung vÃ  cá»¥ thá»ƒ theo mÃ£, cho tÃ¡c vá»¥
giáº£i thÃ­ch mÃ£. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p zero-shot, few shot vÃ  instruction finetuning trÃªn
cÃ¡c LLMs vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a chÃºng. ChÃºng tÃ´i tháº¥y ráº±ng cÃ¡c LLMs MÃ£ hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n cÃ¡c

--- TRANG 5 ---
LLMs chung lá»›n hÆ¡n. NgoÃ i ra, prompting zero-shot hoáº¡t Ä‘á»™ng tá»‘t trong ká»‹ch báº£n khi chÃºng ta khÃ´ng cÃ³
Ä‘á»§ vÃ­ dá»¥ Ä‘á»ƒ prompt/finetune mÃ´ hÃ¬nh.
TÃ i liá»‡u tham kháº£o
[1]Y. Liang, K. Zhu, Automatic generation of text descriptive comments for code blocks, in:
Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[2]R. Sharma, F. Chen, F. Fard, Lamner: code comment generation using character language
model and named entity recognition, in: Proceedings of the 30th IEEE/ACM International
Conference on Program Comprehension, 2022, pp. 48â€“59.
[3]T. Ahmed, P. Devanbu, Few-shot training llms for project-specific code-summarization,
in: Proceedings of the 37th IEEE/ACM International Conference on Automated Software
Engineering, 2022, pp. 1â€“5.
[4]I. Ozkaya, Application of large language models to software engineering tasks: Opportu-
nities, risks, and implications, IEEE Software 40 (2023) 4â€“8.
[5]J. Jiang, S. Kim, Codeup: A multilingual code generation llama2 model with parameter-
efficient instruction-tuning, https://huggingface.co/deepse, 2023.
[6]M.-F. Wong, S. Guo, C.-N. Hang, S.-W. Ho, C.-W. Tan, Natural language generation and
understanding of big code for AI-assisted programming: A review, Entropy 25 (2023) 888.
URL: https://doi.org/10.3390%2Fe25060888. doi: 10.3390/e25060888 .
[7]M. SchÃ¤fer, S. Nadi, A. Eghbali, F. Tip, An empirical evaluation of using large language
models for automated unit test generation, 2023. arXiv:2302.06527 .
[8]H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and fine-tuned chat mod-
els, arXiv preprint arXiv:2307.09288 (2023). URL: https://huggingface.co/meta-llama/
Llama-2-70b-chat-hf.
[9]R. L. et.al., Starcoder: may the source be with you!, arXiv preprint arXiv:2305.06161 (2023).
URL: https://huggingface.co/bigcode/starcoder.
[10] S. Majumdar, S. Paul, D. Paul, A. Bandyopadhyay, B. Dave, S. Chattopadhyay, P. P. Das, P. D.
Clough, P. Majumder, Generative ai for software metadata: Overview of the information
retrieval in software engineering track at fire 2023, in: Forum for Information Retrieval
Evaluation, ACM, 2023.
[11] S. MacNeil, A. Tran, A. Hellas, J. Kim, S. Sarsa, P. Denny, S. Bernstein, J. Leinonen, Ex-
periences from using code explanations generated by large language models in a web
software development e-book, in: Proceedings of the 54th ACM Technical Symposium on
Computer Science Education V. 1, 2023, pp. 931â€“937.
[12] S. Iyer, I. Konstas, A. Cheung, L. Zettlemoyer, Summarizing source code using a neural
attention model, in: 54th Annual Meeting of the Association for Computational Linguistics
2016, Association for Computational Linguistics, 2016, pp. 2073â€“2083.
[13] X. Hu, G. Li, X. Xia, D. Lo, Z. Jin, Deep code comment generation, in: Proceedings of
the 26th Conference on Program Comprehension, Association for Computing Machinery,
2018, p. 200â€“210.
[14] S. Haiduc, J. Aponte, L. Moreno, A. Marcus, On the use of automated text summarization

--- TRANG 6 ---
techniques for summarizing source code, in: 2010 17th Working conference on reverse
engineering, IEEE, 2010, pp. 35â€“44.
[15] B. P. Eddy, J. A. Robinson, N. A. Kraft, J. C. Carver, Evaluating source code summarization
techniques: Replication and expansion, in: 2013 21st International Conference on Program
Comprehension (ICPC), IEEE, 2013, pp. 13â€“22.
[16] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, K. Vijay-Shanker, Automatic
generation of natural language summaries for java classes, in: 2013 21st International
conference on program comprehension (ICPC), IEEE, 2013, pp. 23â€“32.
[17] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, X. Hu, Harnessing the power of
llms in practice: A survey on chatgpt and beyond, arXiv preprint arXiv:2304.13712 (2023).
[18] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, T. B. Hashimoto,
Alpaca: A strong, replicable instruction-following model, Stanford Center for Research on
Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html 3 (2023) 7.
[19] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier,
E. Almazrouei, J. Launay, The RefinedWeb dataset for Falcon LLM: outperforming curated
corpora with web data, and web data only, arXiv preprint arXiv:2306.01116 (2023).
[20] B. RoziÃ¨re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,
et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950
(2023). URL: https://huggingface.co/codellama.
[21] Manuel Romero, llama-2-coder-7b (revision d30d193), 2023. URL: https://huggingface.co/
mrm8488/llama-2-coder-7b. doi: 10.57967/hf/0931 .
[22] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, X. Liao, Large language models
are few-shot summarizers: Multi-intent comment generation via in-context learning (2024).
[23] P. Yin, B. Deng, E. Chen, B. Vasilescu, G. Neubig, Learning to mine aligned code and natural
language pairs from stack overflow, in: International Conference on Mining Software
Repositories, ACM, 2018, pp. 476â€“486. URL: https://conala-corpus.github.io/.
[24] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, Bleu: A method for automatic evaluation of
machine translation, Association for Computational Linguistics, USA, 2002, p. 311â€“318.
[25] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al.,
Codebert: A pre-trained model for programming and natural languages, in: Findings of
the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536â€“1547.
[26] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of
quantized llms, 2023. arXiv:2305.14314 .
[27] T. Dettmers, M. Lewis, S. Shleifer, L. Zettlemoyer, 8-bit optimizers via block-wise quanti-
zation, 9th International Conference on Learning Representations, ICLR (2022).
[28] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, B. Bossan, Peft: State-of-the-art
parameter-efficient fine-tuning methods, https://github.com/huggingface/peft, 2022.
