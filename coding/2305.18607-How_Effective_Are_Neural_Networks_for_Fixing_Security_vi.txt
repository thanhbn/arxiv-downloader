Hiệu quả của Mạng thần kinh trong việc Sửa chữa Lỗ hổng Bảo mật như thế nào

Yi Wu
Đại học Purdue
West Lafayette, USA
wu1827@purdue.edu

Nan Jiang
Đại học Purdue
West Lafayette, USA
jiang719@purdue.edu

Hung Viet Pham∗
Đại học York
Toronto, Canada
hvpham@yorku.ca

Thibaud Lutellier∗
Đại học Alberta
Camrose, Canada
lutellie@ualberta.ca

Jordan Davis
Đại học Purdue
West Lafayette, USA
davi1304@purdue.edu

Lin Tan
Đại học Purdue
West Lafayette, USA
lintan@purdue.edu

Petr Babkin
Nghiên cứu AI J.P. Morgan
Palo Alto, USA
petr.babkin@jpmorgan.com

Sameena Shah
Nghiên cứu AI J.P. Morgan
New York, USA
sameena.shah@jpmchase.com

TÓM TẮT

Việc sửa chữa lỗ hổng bảo mật là một nhiệm vụ khó khăn đang rất cần được tự động hóa. Hai nhóm kỹ thuật đã thể hiện tiềm năng: (1) các mô hình ngôn ngữ mã lệnh lớn (LLMs) đã được tiền huấn luyện trên mã nguồn cho các tác vụ như hoàn thành mã, và (2) các kỹ thuật sửa chữa chương trình tự động (APR) sử dụng các mô hình học sâu (DL) để tự động sửa lỗi phần mềm.

Bài báo này là nghiên cứu đầu tiên và so sánh khả năng sửa chữa lỗ hổng Java của LLMs và các mô hình APR dựa trên DL. Các đóng góp bao gồm việc chúng tôi (1) áp dụng và đánh giá năm LLMs (Codex, CodeGen, CodeT5, PLBART và InCoder), bốn LLMs được tinh chỉnh, và bốn kỹ thuật APR dựa trên DL trên hai benchmark lỗ hổng Java thực tế (Vul4J và VJBench), (2) thiết kế các phép biến đổi mã để giải quyết mối đe dọa trùng lặp dữ liệu huấn luyện và kiểm tra đối với Codex, (3) tạo một benchmark sửa chữa lỗ hổng Java mới VJBench, và phiên bản biến đổi VJBench-trans, để đánh giá tốt hơn các LLMs và kỹ thuật APR, và (4) đánh giá LLMs và kỹ thuật APR trên các lỗ hổng đã biến đổi trong VJBench-trans.

Các phát hiện của chúng tôi bao gồm (1) các LLMs và mô hình APR hiện tại sửa được rất ít lỗ hổng Java. Codex sửa được 10.2 (20.4%) lỗ hổng, nhiều nhất. Nhiều bản vá được tạo ra không thể biên dịch được. (2) Tinh chỉnh với dữ liệu APR tổng quát cải thiện khả năng sửa lỗ hổng của LLMs. (3) VJBench mới của chúng tôi cho thấy LLMs và mô hình APR không thể sửa nhiều loại Common Weakness Enumeration (CWE), như CWE-325 Thiếu bước mã hóa và CWE-444 Buôn lậu yêu cầu HTTP. (4) Codex vẫn sửa được 8.7 lỗ hổng đã biến đổi, vượt trội hơn tất cả các LLMs khác

∗Công việc này được thực hiện khi Hung Viet Pham và Thibaud Lutellier làm việc tại Đại học Waterloo.

Được phép tạo bản sao kỹ thuật số hoặc in cứng một phần hoặc toàn bộ công việc này cho mục đích cá nhân hoặc lớp học mà không tính phí với điều kiện các bản sao không được tạo ra hoặc phân phối để kiếm lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của bên thứ ba trong công việc này phải được tôn trọng. Đối với tất cả các mục đích sử dụng khác, liên hệ với chủ sở hữu/tác giả.

ISSTA '23, 17–21 tháng 7, 2023, Seattle, WA, USA
©2023 Bản quyền thuộc về chủ sở hữu/tác giả.
ACM ISBN 979-8-4007-0221-1/23/07.
https://doi.org/10.1145/3597926.3598135

và các mô hình APR trên lỗ hổng đã biến đổi. Kết quả kêu gọi các đổi mới để tăng cường sửa chữa lỗ hổng Java tự động như tạo dữ liệu huấn luyện sửa chữa lỗ hổng lớn hơn, tinh chỉnh LLMs với dữ liệu đó, và áp dụng biến đổi đơn giản hóa mã để hỗ trợ sửa chữa lỗ hổng.

KHÁI NIỆM CCS
•Phần mềm và kỹ thuật của nó →Kiểm thử và gỡ lỗi phần mềm ;Lập trình tự động ;•Phương pháp tính toán →Mạng thần kinh ;•Bảo mật và quyền riêng tư →Kỹ thuật bảo mật phần mềm .

TỪ KHÓA
Sửa chữa Chương trình Tự động, Mô hình Ngôn ngữ Lớn, Lỗ hổng, AI và Kỹ thuật Phần mềm

Định dạng Tham chiếu ACM:
Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, và Sameena Shah. 2023. Hiệu quả của Mạng thần kinh trong việc Sửa chữa Lỗ hổng Bảo mật như thế nào. Trong Kỷ yếu Hội nghị quốc tế lần thứ 32 ACM SIGSOFT về Kiểm thử và Phân tích Phần mềm (ISSTA '23), 17–21 tháng 7, 2023, Seattle, WA, USA. ACM, New York, NY, USA, 13 trang. https://doi.org/10.1145/3597926.3598135

1 GIỚI THIỆU

Lỗ hổng phần mềm, như tràn bộ đệm và SQL injection, có tác động nghiêm trọng đến nền kinh tế toàn cầu và có thể gây hại cho hàng triệu người dùng. Khi một lỗ hổng được phát hiện, việc sửa chữa nhanh chóng thường rất quan trọng để giảm thiểu khả năng bị khai thác. Tuy nhiên, các nghiên cứu gần đây [43,52] cho thấy thời gian trung bình để sửa một lỗ hổng (thời gian giữa việc phát hiện và sửa chữa) dao động từ 60 đến 79 ngày, vẫn còn quá dài và tạo cơ hội rộng rãi cho kẻ tấn công khai thác các lỗ hổng này. Ví dụ, đối với lỗ hổng Apache Log4Shell nghiêm trọng được báo cáo vào ngày 24 tháng 11 năm 2021, bản sửa lỗi đầu tiên được Apache triển khai 12 ngày sau báo cáo. Trong 12 ngày này, cả Cloudflare và Cisco đều báo cáo một số cuộc tấn công khai thác lỗ hổng [34]. Hơn nữa, bản sửa lỗi ban đầu tỏ ra không đủ, khiến Log4Shell vẫn còn lỗ hổng cho đến khi một bản sửa lỗi hoàn chỉnh được phát hành hơn một tháng sau đó. Do đó, cần có các giải pháp sửa lỗ hổng nhanh hơn.

Hầu hết các benchmark lỗ hổng và giải pháp sửa chữa lỗ hổng tập trung vào C/C++ [19,29–31,36,42,46,53,67] hoặc các tệp nhị phân [10,48,54,60,72]. Thiếu các giải pháp và benchmark cho Java, mặc dù nó là một ngôn ngữ lập trình được sử dụng rộng rãi (ngôn ngữ phổ biến thứ ba trong cộng đồng mã nguồn mở [32]) với nhiều lỗ hổng nghiêm trọng.

Java đã được sử dụng để triển khai các máy chủ quan trọng, bao gồm máy chủ web và dịch vụ (ví dụ: Tomcat, Spring, CFX, Log4J), đặc biệt dễ bị tấn công. Do đó, nhiều lỗ hổng quan trọng nhất có trong phần mềm Java. Ví dụ, Google đánh giá rằng lỗ hổng Log4Shell trong gói Log4J đã ảnh hưởng đến 17.000 dự án Maven [7], và Microsoft thậm chí báo cáo rằng các kẻ tấn công nhà nước đã khai thác lỗ hổng này [2].

Các benchmark và giải pháp cho các ngôn ngữ lập trình khác thường không hoạt động hoặc hoạt động kém khi sửa lỗ hổng Java. Ví dụ, lỗ hổng phổ biến nhất trong C/C++ là tràn bộ đệm [24,59]. Java, là một ngôn ngữ an toàn về kiểu, được thiết kế để tránh tràn bộ đệm. Do đó, hầu hết các kỹ thuật C/C++ tập trung vào lỗ hổng tràn bộ đệm không liên quan đến Java. Chúng ta cần các benchmark và kỹ thuật mới để sửa lỗ hổng bảo mật Java.

Thay vì xây dựng một kỹ thuật để tự động sửa lỗ hổng Java, chúng tôi nghiên cứu và so sánh không gian và tính khả thi của việc áp dụng hai loại kỹ thuật—sửa chữa chương trình tự động dựa trên học tập và LLMs—để tự động sửa lỗ hổng bảo mật Java. Đầu tiên, sửa chữa chương trình dựa trên học tập đã trở nên phổ biến [18,21,22,40,47,75,75,76,76]. Các phương pháp encoder-decoder này học từ một số lượng lớn các cặp lỗi và bản sửa của chúng (trong các dự án mã nguồn mở) để tự động sửa lỗi phần mềm Java chưa thấy. Sẽ thú vị khi nghiên cứu các mô hình sửa chữa chương trình dựa trên học tập hiệu quả như thế nào trong việc sửa một tập con của lỗi phần mềm, tức là lỗ hổng phần mềm.

Thứ hai, LLMs gần đây đã được áp dụng cho mã nguồn [17,25,37,40,50,63,73] và là các mô hình tiền huấn luyện đã được huấn luyện trên một lượng mã nguồn khổng lồ (ví dụ: toàn bộ GitHub). Khác với các mô hình APR, LLMs tiền huấn luyện học từ kho mã nguồn lớn (thay vì các cặp lỗi và bản sửa) cho các tác vụ khác nhau như gắn thẻ định danh và hoàn thành mã. Mặc dù học để thực hiện các tác vụ khác với sửa chữa, nghiên cứu gần đây [38,74] cho thấy LLMs tiền huấn luyện có khả năng cạnh tranh trong việc sửa lỗi Java tổng quát [41,44]. Sẽ thú vị khi nghiên cứu LLMs hiệu quả như thế nào cho một tác vụ khác, tức là sửa lỗ hổng phần mềm, khi chúng không thấy cách các lỗi được sửa.

Thứ ba, sẽ thú vị khi so sánh khả năng sửa lỗ hổng Java của các kỹ thuật APR dựa trên học sâu (DL) và LLMs. Các kỹ thuật APR dựa trên DL và LLMs đại diện cho hai góc độ áp dụng mô hình cho một tác vụ khác. Áp dụng kỹ thuật APR dựa trên DL để sửa lỗ hổng là sử dụng các mô hình học từ một tập dữ liệu tổng quát cho một tập con cụ thể của tập dữ liệu (lỗ hổng phần mềm là một loại lỗi phần mềm). Áp dụng LLMs để sửa lỗ hổng là sử dụng các mô hình học từ một định dạng dữ liệu khác (chuỗi mã) cho một định dạng khác (cặp mã lỗi và mã đã sửa). Vì LLMs không yêu cầu các cặp lỗi và bản sửa, LLMs thường được xây dựng từ dữ liệu lớn hơn bậc so với dữ liệu huấn luyện được sử dụng để huấn luyện các mô hình APR. Liệu nhiều dữ liệu hơn sẽ thắng hay việc khớp định dạng dữ liệu sẽ thắng?

Cuối cùng, LLMs tiền huấn luyện thường được tinh chỉnh để thích ứng với các tác vụ downstream khác nhau [8,26,33,65,73]. Một nghiên cứu gần đây [38] cho thấy tinh chỉnh cải thiện khả năng sửa lỗi của LLMs ít nhất 31%. Tuy nhiên, do thiếu dữ liệu lỗ hổng Java, việc tinh chỉnh LLMs để sửa lỗ hổng Java là không thực tế. Do đó, sẽ thú vị khi nghiên cứu LLMs được tinh chỉnh với dữ liệu APR tổng quát hiệu quả như thế nào trong việc sửa lỗ hổng phần mềm. Và khi so sánh với các kỹ thuật APR dựa trên DL, liệu nhiều dữ liệu cộng với tinh chỉnh sẽ thắng hay việc khớp định dạng dữ liệu sẽ thắng?

1.1 Phương pháp của chúng tôi

Chúng tôi thực hiện nghiên cứu đầu tiên để đánh giá và so sánh khả năng sửa lỗ hổng Java của các kỹ thuật APR và LLMs. Chúng tôi đánh giá năm LLMs (Codex [1], CodeT5 [73], CodeGen [55], PLBART [8] và InCoder [28]), bốn LLMs được tinh chỉnh với dữ liệu APR tổng quát, và bốn kỹ thuật APR (CURE [40], Recoder [76], RewardRepair [75], và KNOD [39]) trên hai benchmark lỗ hổng Java (Vul4J và VJBench mới mà chúng tôi tạo). Có hai thách thức chính.

Đầu tiên, có ít benchmark có sẵn để đánh giá các công cụ sửa chữa lỗ hổng Java. Trong khi Vul4J [16] chứa 79 lỗ hổng Java có thể tái tạo, chúng chỉ thuộc về 25 CWEs, tức là các loại lỗ hổng. Ngoài ra, 60% CWEs trong tập dữ liệu (15 loại lỗ hổng) chỉ được bao phủ bởi một lỗ hổng có thể tái tạo duy nhất.

Để giải quyết thách thức này, chúng tôi phát triển các benchmark mới. Chúng tôi phân tích toàn bộ Cơ sở dữ liệu Lỗ hổng Quốc gia (NVD) [4] để xác định các lỗ hổng Java thực tế có thể tái tạo phù hợp cho đánh giá sửa chữa lỗ hổng, và sử dụng chúng để tạo benchmark VJBench của chúng tôi. Các lỗ hổng này bao phủ thêm mười hai loại CWE không được bao gồm trong tập dữ liệu Vul4J và thêm nhiều lỗ hổng cho bốn loại CWE mà Vul4J chỉ có một lỗ hổng liên quan. Benchmark mới có thể hỗ trợ đánh giá các kỹ thuật sửa chữa lỗ hổng Java trong tương lai.

Thách thức thứ hai phát sinh từ việc Codex được huấn luyện trên một kho mã đáng kể được thu thập từ GitHub [17] và tập dữ liệu huấn luyện không được công bố. Vì các dự án trong Vul4J và VJBench là các kho công cộng trên GitHub, người ta không thể chắc chắn rằng các lỗ hổng trong Vul4J và VJBench không có trong dữ liệu huấn luyện của Codex. Đây là một mối đe dọa lớn đã biết đối với tính hợp lệ của đánh giá [11,69]. Trong khi tập dữ liệu HumanEval [17] không có trong dữ liệu huấn luyện của Codex, nó dành cho hoàn thành mã Python và không chứa lỗ hổng Java. Tạo các benchmark thực tế mới không chỉ tốn kém [16,41], mà còn có thể không thực tế nếu LLMs đã được huấn luyện trên tất cả các tập dữ liệu công cộng.

Giải pháp nỗ lực tốt nhất của chúng tôi để giảm thiểu thách thức này là biến đổi mã lỗ hổng trong các benchmark hiện có. Chúng tôi sử dụng hai loại biến đổi mã: đổi tên định danh và thay đổi cấu trúc mã. Các biến đổi này tạo ra các chương trình tương đương mới vẫn giữ lại lỗ hổng nhưng không được bao gồm trong bất kỳ tập dữ liệu mã nguồn mở nào mà Codex và các LLMs khác có thể đã thấy. Do đó, chúng tôi tạo VJBench-trans, một benchmark của các lỗ hổng đã biến đổi, bằng cách áp dụng hai chiến lược biến đổi trên các lỗ hổng từ Vul4J và VJBench.

1.2 Đóng góp

Bài báo của chúng tôi đóng góp như sau:
•Chúng tôi thực hiện nghiên cứu đầu tiên đánh giá khả năng sửa chữa của năm LLMs, bốn LLMs tinh chỉnh, và bốn kỹ thuật APR trên lỗ hổng Java thực tế từ hai benchmark Vul4J và VJBench mới của chúng tôi. Các phát hiện bao gồm:
–Các LLMs và kỹ thuật APR hiện tại sửa rất ít lỗ hổng Java. Codex sửa 10.2 (20.4%) lỗ hổng trung bình, thể hiện khả năng sửa tốt nhất. (Phần 6.1)
–Tinh chỉnh với dữ liệu APR tổng quát cải thiện khả năng sửa lỗ hổng của LLMs. InCoder tinh chỉnh sửa 9 lỗ hổng, thể hiện khả năng sửa cạnh tranh với Codex. (Phần 6.1)
–Codex có tỷ lệ biên dịch cao nhất là 79.7%. Các LLMs khác (tinh chỉnh hay không) và kỹ thuật APR có tỷ lệ biên dịch thấp (thấp nhất là 6.4% với CodeT5 và phần còn lại từ 24.5% đến 65.2%), cho thấy thiếu kiến thức về cú pháp. (Phần 6.1)
–LLMs và mô hình APR, ngoại trừ Codex, chỉ sửa các lỗ hổng yêu cầu thay đổi đơn giản, như xóa một dòng hoặc thay thế biến/phương thức. (Phần 6.2)
–VJBench mới của chúng tôi cho thấy LLMs và mô hình APR không thể sửa nhiều loại CWE bao gồm CWE-172 Lỗi mã hóa, CWE-325 Thiếu bước mã hóa, CWE-444 Buôn lậu yêu cầu HTTP, CWE-668 Phơi bày tài nguyên cho phạm vi sai, và CWE-1295 Thông báo debug tiết lộ thông tin không cần thiết. (Phần 6.2)

•Chúng tôi tạo hai benchmark lỗ hổng Java cho sửa chữa chương trình tự động: (1) VJBench, chứa 42 lỗ hổng Java thực tế có thể tái tạo bao phủ mười hai loại CWE mới, và (2) VJBench-trans, chứa 150 lỗ hổng Java đã biến đổi.
•Chúng tôi sử dụng biến đổi mã để giảm thiểu mối đe dọa rằng LLMs và Codex hộp đen có thể đã thấy các benchmark được đánh giá.
•Chúng tôi đánh giá khả năng sửa chữa của LLMs và kỹ thuật APR trên lỗ hổng đã biến đổi (VJBench-trans).
–Biến đổi mã làm cho LLMs và kỹ thuật APR sửa ít lỗ hổng hơn. Một số mô hình như Codex và CodeT5 tinh chỉnh mạnh mẽ hơn với biến đổi mã. Mặt khác, một số biến đổi làm cho lỗ hổng dễ sửa hơn. (Phần 6.3)
•Chúng tôi cung cấp hàm ý và gợi ý cho các hướng tương lai (Phần 6).

2 BENCHMARK MỚI CỦA LỖ HỔNG JAVA

Một benchmark APR Java phải chứa các lỗ hổng Java có thể tái tạo với test case phơi bày lỗ hổng. Trong khi có rất nhiều benchmark như vậy cho lỗi Java, bao gồm Defects4J [41], QuixBugs [44], Bugs.jar [66], và Bears [49], benchmark lỗ hổng Java duy nhất cho APR là Vul4J [16]. Vul4J chứa 79 lỗ hổng từ 51 dự án bao phủ 25 loại CWE. Mặc dù là bước đầu có giá trị, Vul4J cung cấp phạm vi hạn chế của các danh mục CWE như đã giải thích trong Giới thiệu. Ngoài ra, chỉ 35 trong số 79 lỗ hổng này áp dụng được cho việc đánh giá các hệ thống APR dựa trên học tập hiện đại [40,75,76] vì các mô hình APR này chỉ sửa lỗi một hunk. Cụ thể, 39 trong số 79 lỗ hổng là một hunk. Chúng tôi chỉ có thể tái tạo 35 trong số 39 lỗ hổng, vì hai lỗi không biên dịch được, và hai lỗi không thể tái tạo với container Docker do tác giả Vul4J cung cấp.

Để mở rộng benchmark này, chúng tôi thu thập lỗ hổng Java theo công việc trước [41]: i) Lỗ hổng chỉ nên liên quan đến mã nguồn Java, ii) Commit sửa lỗi nên chứa ít nhất một test case vượt qua trên V_fix nhưng thất bại trên V_bug, iii) Bản vá sửa lỗi chỉ nên bao gồm các thay đổi sửa lỗ hổng và không nên đưa vào các thay đổi không liên quan như tính năng hoặc tái cấu trúc, và iv) lỗ hổng chưa có trong Vul4J.

Chúng tôi tải xuống tất cả dữ liệu lỗ hổng có sẵn ở định dạng JSON vào ngày 13 tháng 5 năm 2022 từ NVD. Chúng tôi phân tích dữ liệu này và thu được danh sách 7.116 dự án GitHub bằng cách thu thập các URL tham chiếu của các lỗ hổng này. Chúng tôi loại trừ các dự án có ít hơn 50% mã của họ là Java, dẫn đến 400 dự án Java chứa 933 lỗ hổng duy nhất. Sau đó, chúng tôi cố gắng xác định các commit sửa lỗi cho mỗi trong số 933 lỗ hổng bằng cách kiểm tra thủ công các liên kết tham chiếu được cung cấp trong báo cáo lỗ hổng hoặc bằng cách tìm kiếm ID lỗ hổng trong kho GitHub nếu không có liên kết. Chúng tôi tìm thấy các commit sửa lỗ hổng cho 698 lỗ hổng. Sau đó, chúng tôi lọc thủ công 185 lỗ hổng mà các commit sửa lỗi chứa thay đổi không phải Java và 314 lỗ hổng không có test case trong commit sửa lỗi. Bây giờ chúng tôi có 199 lỗ hổng, mỗi lỗ hổng có test case và commit sửa lỗi Java tương ứng. Sau đó, chúng tôi tái tạo thành công 42 lỗ hổng Java không được bao gồm trong Vul4J, sử dụng các công cụ xây dựng như Maven hoặc Gradle.

Chúng tôi kết thúc với tập dữ liệu 42 lỗ hổng Java thực tế có thể tái tạo mới từ ba mươi dự án mã nguồn mở. Chi tiết, tập dữ liệu của chúng tôi bao gồm 27 lỗ hổng nhiều hunk từ hai mươi hai dự án và 15 lỗ hổng một hunk từ mười một dự án. Như Hình 1 cho thấy, 42 lỗ hổng này bao phủ tổng cộng 23 loại CWE. Hơn nữa, tập dữ liệu của chúng tôi giới thiệu 12 loại CWE mới (được ký hiệu bằng * trong Hình 1) không được bao gồm trong Vul4J và bổ sung bốn loại CWE (CWE-78, CWE-200, CWE-310, CWE-863) mà Vul4J chỉ có một ví dụ.

Bảng 1 mô tả 15 lỗ hổng một hunk mới thuộc mười hai loại CWE trong benchmark VJBench của chúng tôi. Có sáu loại CWE duy nhất mới không có trong Vul4J. Do đó, có 15 lỗ hổng từ VJBench và 35 lỗ hổng từ Vul4J, tổng cộng 50 lỗ hổng mà chúng tôi sử dụng trong nghiên cứu.

3 CÁC MÔ HÌNH NGÔN NGỮ LỚN VÀ KỸ THUẬT APR

3.1 Các Mô hình Ngôn ngữ Lớn

Chúng tôi chọn năm LLMs, tức là Codex, PLBART, CodeT5, CodeGen và InCoder, vì chúng (1) hiện đại, (2) có khả năng thực hiện các tác vụ tạo mã mà không cần sửa đổi các mô hình hoặc thành phần bổ sung (ví dụ: CodeBERT [26] GraphCodeBERT [33] bị loại trừ), và (3) được huấn luyện với đủ mã nguồn để chúng có thể hiểu mã ở một mức độ nào đó (ví dụ: chúng tôi loại trừ T5 [65], GPT-2 [64], GPT-Neo [13] và GPT-J [71], mà dữ liệu huấn luyện của chúng hơn 90% là văn bản). Trong công việc này, chúng tôi nghiên cứu LLMs trong hai cài đặt: như vốn có và tinh chỉnh với dữ liệu APR tổng quát.

3.1.1 Các Mô hình Ngôn ngữ Lớn Như Vốn có. Trong phần này, chúng tôi giới thiệu chi tiết về các LLMs được nghiên cứu và cách sử dụng chúng để sửa lỗ hổng. Bảng 3 cung cấp kích thước mô hình và thông tin dữ liệu huấn luyện của chúng.

Codex [17]: Codex là một mô hình ngôn ngữ dựa trên GPT-3 [15,17] với 12B tham số được huấn luyện trên cả ngôn ngữ tự nhiên và mã nguồn. Chúng tôi sử dụng mô hình davinci-002 (tính đến tháng 7 năm 2022), được cho là mô hình Codex chính xác nhất [1]. Chúng tôi tập trung vào chế độ chèn của Codex vì nó cung cấp kết quả tốt nhất trong nghiên cứu sơ bộ của chúng tôi trong số ba chế độ chính: hoàn thành, chèn và chỉnh sửa.

CodeT5 [73]: CodeT5 là một mô hình transformer encoder-decoder [70] được tiền huấn luyện với mục tiêu khử nhiễu nhận biết định danh và với các tác vụ tạo kép hai chiều. Nó được huấn luyện trên kho dữ liệu 5.2 triệu hàm mã và 8.3 triệu câu ngôn ngữ tự nhiên từ các kho mã nguồn mở trong sáu ngôn ngữ lập trình bao gồm Java. Trong công việc này, chúng tôi sử dụng mô hình CodeT5 lớn nhất được phát hành, có 770M tham số.

CodeGen [55]: Các mô hình CodeGen là một loạt các transformer decoder tự động hồi quy được huấn luyện cho tổng hợp chương trình hội thoại. Dữ liệu huấn luyện của chúng bao gồm 354.7B token ngôn ngữ tự nhiên từ tập dữ liệu THEPILE và 150.8B token ngôn ngữ lập trình được trích xuất từ một tập con của cơ sở dữ liệu Google BigQuery. Trong công việc này, chúng tôi áp dụng mô hình CodeGen chứa 6B tham số (mô hình lớn hơn với 16B tham số không được sử dụng do hạn chế của máy chúng tôi).

PLBART [8]: PLBART sử dụng kiến trúc transformer encoder-decoder với một lớp chuẩn hóa bổ sung trên encoder và decoder. Nó được tiền huấn luyện trên các hàm được trích xuất từ các kho Java và Python GitHub thông qua khử nhiễu tự động mã hóa. Hai mô hình PLBART có kích thước khác nhau có sẵn, và chúng tôi sử dụng mô hình lớn hơn chứa 400M tham số.

InCoder [28]: Các mô hình InCoder theo kiến trúc chỉ decoder của XGLM [45] và được tiền huấn luyện trên tác vụ dự đoán khoảng bị che. Dữ liệu tiền huấn luyện của nó đến từ các dự án mã nguồn mở trên GitHub và GitLab, và các bài đăng StackOverflow. Có hai mô hình InCoder có kích thước khác nhau được phát hành, và chúng tôi sử dụng mô hình lớn hơn chứa 6B tham số.

Định dạng Đầu vào: Bảng 2 minh họa định dạng đầu vào chúng tôi sử dụng cho mỗi mô hình. Đối với Codex, chúng tôi áp dụng định dạng đầu vào tương tự như được sử dụng trong công việc trước [58]. Prompt bao gồm mã lỗi được comment với từ gợi ý "BUG:" và "FIXED:" để chỉ ra vị trí của lỗi và hướng dẫn Codex tạo ra phiên bản mã đã sửa. Nếu số lượng token đầu vào vượt quá số lượng tối đa cho một mô hình, chúng tôi cắt mã và nhập mã xung quanh dòng lỗi. Vì không rõ các prompt dòng lỗi được comment sẽ ảnh hưởng đến khả năng sửa lỗi của mô hình như thế nào, chúng tôi thử nghiệm với đầu vào có và không có dòng lỗi được comment cho mỗi mô hình. Hình 2 cho thấy một ví dụ về đầu vào và đầu ra mong đợi của Codex với dòng lỗi được comment bằng /* BUG .. FIXED */.

3.1.2 Các Mô hình Ngôn ngữ Lớn Tinh chỉnh. Chúng tôi cũng nghiên cứu khả năng sửa lỗi của LLMs tinh chỉnh, vì tinh chỉnh là kỹ thuật phổ biến để thích ứng LLM tiền huấn luyện với tác vụ downstream cụ thể, như tóm tắt mã hoặc dịch mã [26,28,65,73]. Tuy nhiên, do thiếu lỗ hổng làm dữ liệu tinh chỉnh, chúng tôi sử dụng LLMs được tinh chỉnh với dữ liệu APR tổng quát, được chia sẻ bởi công việc hiện có [38]. Công việc trước [38] tinh chỉnh LLMs với tập dữ liệu huấn luyện chứa 143.666 instance được thu thập từ các dự án Java GitHub mã nguồn mở [76]. Mỗi instance dữ liệu là một cặp mã lỗi và mã đã sửa. Chi tiết, [38] sử dụng optimizer Adam với tốc độ học 1e-5, đặt kích thước batch là một và tinh chỉnh trong một epoch. LLMs tinh chỉnh được cho là điều chỉnh với tác vụ sửa lỗ hổng ở một mức độ nào đó do sự tương đồng giữa sửa lỗ hổng và sửa lỗi tổng quát. Chúng tôi thực hiện tìm kiếm và xác nhận rằng không có lỗ hổng nào chúng tôi nghiên cứu trong công việc này có trong dữ liệu huấn luyện APR được sử dụng để tinh chỉnh LLMs.

Chúng tôi không thể tinh chỉnh Codex, vì nó không cung cấp API tinh chỉnh nào và cũng không có Codex tinh chỉnh nào có sẵn. Dòng cuối của Bảng 2 mô tả định dạng đầu vào để sử dụng LLMs tinh chỉnh, trong đó các dòng lỗi được đưa ra như các dòng comment, và toàn bộ hàm được nhập vào LLMs tinh chỉnh để tạo ra các dòng đã vá [38].

3.2 Kỹ thuật APR

Chúng tôi chọn bốn kỹ thuật APR dựa trên học tập hiện đại được huấn luyện cho lỗi Java. Các kỹ thuật APR này cần phải là mã nguồn mở để chúng tôi có thể chạy chúng trên các benchmark lỗ hổng mới của chúng tôi.

CURE [40] áp dụng một mô hình ngôn ngữ nhỏ (tiền huấn luyện với 4.04M instance mã) cho kiến trúc encoder-decoder của CoCoNuT [47] để học cú pháp mã và đề xuất chiến lược nhận biết mã mới để loại bỏ các định danh không hợp lệ và tăng tỷ lệ biên dịch trong quá trình suy luận. CURE được huấn luyện với 2.72M instance APR.

Recoder [76] sử dụng mạng học sâu dựa trên cây được huấn luyện trên 82.87K instance huấn luyện APR. Nó tập trung vào việc tạo ra các chỉnh sửa để sửa đổi AST lỗi thành AST đã vá.

RewardRepair [75] bao gồm biên dịch trong tính toán hàm loss của mô hình để tăng số lượng bản vá có thể biên dịch (và đúng). Điều này khác với CURE vì hàm loss tăng số lượng bản vá có thể biên dịch trong quá trình huấn luyện. Tổng thể, RewardRepair được huấn luyện với 3.51M instance huấn luyện APR.

KNOD [39] đề xuất bộ giải mã cây ba giai đoạn mới để tạo ra AST đã vá, và cũng sử dụng chưng cất kiến thức miền để sửa đổi hàm loss để cho các mô hình học cú pháp và ngữ nghĩa mã. KNOD được huấn luyện với 576K instance huấn luyện APR, và là kỹ thuật APR dựa trên DL hiện đại.

4 BIẾN ĐỔI MÃ

Để giải quyết thách thức trùng lặp dữ liệu huấn luyện-kiểm thử, chúng tôi cần tạo ra các lỗ hổng và bản sửa của chúng mà các LLMs hoặc kỹ thuật APR hiện có chưa thấy. Chúng tôi tạo ra các lỗ hổng chưa thấy bằng cách biến đổi các lỗ hổng hiện có thành các dạng tương đương về mặt ngữ nghĩa. Không có mô hình APR và LLMs nào, bao gồm Codex, đã thấy mã lỗi đã biến đổi này và các bản sửa tương ứng trong tập huấn luyện của chúng. Chúng tôi áp dụng hai danh mục biến đổi cho Vul4J và VJBench, được mô tả dưới đây:

(1) Đổi tên Định danh: Để ngăn LLMs và mô hình APR chỉ đơn giản ghi nhớ các bản vá chính xác được liên kết với tên định danh, chúng tôi đổi tên các định danh trong mã lỗi và mã đã sửa tương ứng. Tất cả các biến, hàm và lớp được định nghĩa trong dự án được đổi tên bằng cách sử dụng từ đồng nghĩa cho tên định danh gốc theo đặc tả Java. Chúng tôi sử dụng từ đồng nghĩa để giữ nghĩa từ của các định danh gốc. Chúng tôi không đổi tên các định danh từ thư viện bên ngoài hoặc thư viện lớp Java mặc định, vì người ta thường không thể sửa đổi thư viện bên ngoài. Hình 3 cho thấy một ví dụ về đổi tên định danh cho Halo-1.

Chúng tôi đầu tiên sử dụng công cụ src2abs [6] để trích xuất tất cả tên biến, hàm và lớp trong hàm lỗi, và lọc ra những định danh từ Java hoặc thư viện bên thứ ba. Chúng tôi tokenize mỗi định danh dựa trên quy ước camel case hoặc snake case, sau đó sử dụng NLTK WordNet [3] để tạo từ đồng nghĩa cho mỗi từ. Sau đó, chúng tôi lắp ráp lại các từ đồng nghĩa này để tạo thành một định danh hoàn chỉnh. Chúng tôi xem xét và điều chỉnh thủ công các từ đồng nghĩa để đảm bảo chúng phù hợp với ngữ cảnh mã. Vì một số kỹ thuật APR cần trích xuất định danh từ toàn bộ dự án, chúng tôi đổi tên các định danh được sử dụng trong hàm lỗi trên toàn bộ dự án.

(2) Thay đổi Cấu trúc Mã: Chúng tôi định nghĩa sáu quy tắc biến đổi để thay đổi cấu trúc mã.
•Đảo điều kiện if: phủ định điều kiện if và hoán đổi các khối mã trong nhánh if và else.
•Biến đổi vòng lặp: chuyển đổi vòng lặp for thành vòng lặp while và ngược lại.
•Biến đổi câu lệnh điều kiện: chuyển biểu thức ternary (var = cond ? exprTrue: exprFalse;) thành câu lệnh if-else (if (cond) {var = exprTrue;} else {var = exprFalse;}), và chuyển đổi câu lệnh switch thành nhiều câu lệnh if và elseif, và ngược lại.
•Chuỗi hàm: hợp nhất nhiều lời gọi hàm thành một chuỗi gọi, hoặc ngược lại tách chuỗi gọi hàm thành các lời gọi hàm riêng biệt. Hình 4 cho thấy một ví dụ trong đó value.getClass().equals(...); được tách thành Class value_class = value.getClass(); và value_class.equals(...);.
•Truyền tham số hàm: Nếu một biến hoặc đối tượng được định nghĩa cục bộ chỉ được sử dụng làm tham số hàm, chúng tôi thay thế tham số hàm bằng câu lệnh định nghĩa của nó, hoặc chúng tôi trích xuất lời gọi hàm được truyền làm tham số hàm thành định nghĩa biến/đối tượng riêng biệt. Hình 5 cho thấy một ví dụ trong đó tham số parentPath.normalize() được trích xuất và khai báo là đối tượng cục bộ normalizedParentPath.
•Thay đổi thứ tự mã: thay đổi thứ tự các câu lệnh nếu việc thay đổi thứ tự không ảnh hưởng đến kết quả thực thi. Ví dụ, funcA(); int n =0; có thể được biến đổi thành int n = 0; funcA(); vì việc gọi funcA() và khai báo int n không ảnh hưởng lẫn nhau.

Đối với thay đổi cấu trúc mã, chúng tôi biến đổi thủ công hàm lỗi. Đối với mỗi hàm lỗi, chúng tôi áp dụng tất cả các biến đổi có thể áp dụng cùng một lúc. Chúng tôi tiếp tục xác nhận tính tương đương của lỗi đã biến đổi bằng cách tái tạo chúng bằng cùng một bộ test và áp dụng các bản vá tương đương về mặt ngữ nghĩa để vượt qua các test.

Benchmark mới (VJBench-trans): Tóm lại, để tạo ra các lỗi và bản vá mà LLMs chưa thấy trong tập huấn luyện của chúng, chúng tôi áp dụng ba bộ biến đổi (chỉ đổi tên định danh, chỉ thay đổi cấu trúc mã, và cả hai cùng lúc) cho VJBench và Vul4J, và tạo VJBench-trans chứa 3×50 = 150 lỗ hổng Java đã biến đổi. Chúng tôi tìm kiếm trên GitHub và Google mã đã biến đổi, và không tìm thấy mã công cộng nào giống với hàm lỗi đã biến đổi.

Khôi phục bản vá để đánh giá: Mã đã biến đổi vẫn thực tế và có thể đọc được bởi con người. Tuy nhiên, để dễ dàng đánh giá tính đúng đắn của các bản vá hợp lý, chúng tôi duy trì một từ điển lưu trữ ánh xạ giữa các định danh đã đổi tên và tên gốc của chúng. Đối với mỗi lỗ hổng, chúng tôi cũng viết một chương trình đã vá cho phiên bản đã biến đổi cấu trúc mã của nó, cung cấp tham chiếu cho người dùng tập dữ liệu trong tương lai.

5 THIẾT LẬP THÍ NGHIỆM

Hình 6 cung cấp tổng quan về nghiên cứu của chúng tôi. Đầu tiên, chúng tôi xây dựng tập dữ liệu lỗ hổng mới, VJBench, chứa 42 lỗ hổng mới. Chúng tôi sử dụng tập dữ liệu mới này và tập dữ liệu gốc (Vul4J) để đánh giá khả năng sửa lỗ hổng của các kỹ thuật APR dựa trên DL, LLMs và LLMs tinh chỉnh. Mỗi mô hình ngôn ngữ tạo ra 10 bản vá cho mỗi lỗi thông qua suy luận. Đối với mỗi mô hình APR, chúng tôi sử dụng kích thước beam search mặc định và xác thực 10 bản vá hàng đầu. Các bản vá được tạo ra sau đó được xác thực bằng test case và xác minh thủ công tất cả các bản vá vượt qua test case.

Sau đó, chúng tôi áp dụng biến đổi mã trên Vul4J và VJBench để tạo ra VJBench-trans. Cuối cùng, chúng tôi đánh giá tác động của biến đổi mã đối với khả năng sửa chữa lỗ hổng của tất cả LLMs, LLMs tinh chỉnh và kỹ thuật APR.

5.1 Tập dữ liệu

Trong công việc này, chúng tôi tập trung vào việc sửa lỗ hổng Java một hunk vì các mô hình APR dựa trên DL hiện đại được thiết kế để sửa lỗi một hunk. Chúng tôi lọc và thu được 35 lỗi một hunk từ tập dữ liệu Vul4J. Cùng với 15 lỗ hổng một hunk từ VJBench, chúng tôi có tổng cộng 50 lỗ hổng Java. Chúng tôi sử dụng định vị lỗi hoàn hảo cho các lỗ hổng Java này, tức là chúng tôi sử dụng các dòng mã được sửa đổi trong các bản vá của nhà phát triển làm dòng lỗi.

5.2 Thiết lập Mô hình Ngôn ngữ Lớn

Chúng tôi đánh giá mỗi LLM với hai thiết lập đầu vào: (1) các dòng lỗi được comment như một phần của đầu vào và (2) không có dòng lỗi. Chúng tôi quan sát thấy InCoder sửa nhiều lỗ hổng hơn khi đầu vào chứa comment dòng lỗi, trong khi các LLMs khác hoạt động tốt hơn không có dòng lỗi. Sau đó chúng tôi báo cáo thiết lập hoạt động tốt nhất cho mỗi mô hình trong phần còn lại của bài báo này. Đối với LLMs tinh chỉnh, chúng tôi tuân theo định dạng đầu vào với comment dòng lỗi được sử dụng trong [38] được mô tả trong Bảng 2.

Chúng tôi cấu hình mỗi mô hình để tạo ra 10 bản vá cho mỗi lỗ hổng. Đối với CodeT5, CodeGen, PLBART và InCoder, chúng tôi đặt kích thước beam search của chúng thành 10. Đối với Codex, chúng tôi đặt tham số n, số lượng ứng viên để tạo ra, thành 10. Xem xét tính ngẫu nhiên vốn có của phương pháp lấy mẫu được Codex áp dụng, chúng tôi chạy nó hai mươi lăm lần cho mỗi lỗ hổng để thu được kết quả trung bình. Chúng tôi chạy hai mươi lăm lần để kiểm soát biên độ lỗi nhỏ (≤0.3) ở mức tin cậy 95%. Chúng tôi đặt nhiệt độ lấy mẫu của Codex thành 0.6, được cho là có hiệu suất tốt nhất khi lấy mẫu mười ứng viên trong công việc trước [17]. Chúng tôi đặt số lượng token mới được tạo tối đa thành 400 cho Codex do giới hạn tốc độ yêu cầu của nó, và thành 512 cho tất cả LLMs khác.

5.3 Xác thực Bản vá

Chế độ chèn Codex tạo ra mã để chèn giữa prompt tiền tố và prompt hậu tố. Vì chúng tôi sử dụng mã trước và bao gồm comment dòng lỗi làm prompt tiền tố và mã sau comment dòng lỗi làm prompt hậu tố, chúng tôi thay thế mã lỗi gốc bằng mã mà Codex tạo ra. Tương tự, CodeT5 tạo ra mã để thay thế nhãn bị che trong đầu vào của nó. PLBART tạo ra toàn bộ hàm đã vá để thay thế toàn bộ hàm lỗi. CodeGen và InCoder là các mô hình hoàn thành tạo ra mã để hoàn thành prompt tiền tố đã cho. Chúng tôi lấy hàm hoàn chỉnh đầu tiên mà CodeGen và InCoder tạo ra để thay thế hàm lỗi gốc. Đối với tất cả LLMs tinh chỉnh, CodeT5, CodeGen, PLBART và InCoder tinh chỉnh trực tiếp tạo ra mã đã vá để thay thế mã lỗi.

Đối với mỗi LLM và kỹ thuật APR, chúng tôi đầu tiên xác thực 10 bản vá hàng đầu mà chúng tạo ra bằng cách sử dụng test case từ dự án. Theo công việc trước [40,47,75,76], các bản vá hợp lý là các bản vá vượt qua tất cả test case, trong khi các bản vá đúng là tương đương về mặt ngữ nghĩa với các bản vá của nhà phát triển, và các bản vá quá khớp là các bản vá vượt qua tất cả test case nhưng không đúng. Chúng tôi kiểm tra thủ công mỗi bản vá hợp lý để xác định xem nó có phải là bản vá đúng không.

6 KẾT QUẢ VÀ PHÁT HIỆN

Chúng tôi đánh giá khả năng sửa lỗ hổng của năm LLMs, bốn LLMs tinh chỉnh và bốn kỹ thuật APR dựa trên DL trên hai benchmark lỗ hổng Java thực tế.

6.1 RQ1: Khả năng Sửa Lỗ hổng

Chúng tôi chạy Codex hai mươi lăm lần và báo cáo số lượng lỗ hổng đã sửa trung bình với biên độ lỗi, vì việc tạo bản vá của Codex không xác định. Đối với các LLMs khác, chúng tôi chỉ chạy chúng một lần vì việc tạo bản vá của chúng xác định (Phần 5).

Bảng 4 cho thấy khả năng sửa chữa, tức là số lượng lỗ hổng mà mỗi phương pháp sửa chính xác, của năm LLMs, bốn LLMs tinh chỉnh và bốn mô hình APR. Chúng tôi xem xét mười bản vá hàng đầu vì một nghiên cứu gần đây cho thấy hầu hết các nhà phát triển chỉ sẵn sàng kiểm tra tối đa mười bản vá [57]. Kết quả trong Bảng 4 được báo cáo dưới dạng X/Y, trong đó X là số lượng lỗ hổng được sửa chính xác bởi mỗi kỹ thuật và Y là số lượng lỗ hổng được sửa hợp lý. Một lỗ hổng được sửa hợp lý bởi một mô hình nếu mô hình tạo ra một bản vá hợp lý (định nghĩa trong Phần 5.3).

6.1.1 LLMs so với Kỹ thuật APR. Chúng tôi đầu tiên so sánh việc sử dụng LLMs như vốn có với các kỹ thuật APR. Ở đây, LLMs như vốn có có nghĩa là chúng tôi áp dụng Codex và LLMs dưới học zero-shot và không tinh chỉnh. Kết quả của chúng tôi cho thấy Codex thể hiện khả năng sửa chữa tốt nhất. Trong tổng số 50 lỗ hổng trong Vul4J và VJBench, Codex sửa trung bình 10.2 lỗ hổng với biên độ lỗi 0.3 (ở độ tin cậy 95%). InCoder thể hiện khả năng tốt thứ hai, sửa 5 lỗ hổng. Các LLMs khác và kỹ thuật APR dựa trên DL chỉ sửa rất ít lỗ hổng. Nhìn chung, LLMs và kỹ thuật APR cho thấy khả năng sửa lỗ hổng rất hạn chế.

Phát hiện của chúng tôi về Codex hoạt động tốt nhất trong việc sửa lỗ hổng Java phù hợp với hiệu suất vượt trội của Codex trong việc sửa lỗi tổng quát [74] và trong các lĩnh vực khác [1,17,27,58], có thể do kích thước mô hình và kích thước dữ liệu huấn luyện lớn hơn đáng kể như được chỉ ra trong Bảng 3. Kết quả của chúng tôi cũng phù hợp với công việc gần đây [38] trong việc cho thấy LLMs không tinh chỉnh có khả năng sửa chữa cạnh tranh – InCoder sửa thêm ba lỗ hổng so với kỹ thuật APR tốt nhất (RewardRepair). Tuy nhiên, trong khi [38] cho thấy CodeGen, PLBART và InCoder như vốn có có thể sửa 18%-23% lỗi Java tổng quát của các benchmark APR, kết quả của chúng tôi cho thấy chúng chỉ có thể sửa 4%(2/50)-10%(5/50) lỗ hổng của Vul4J và VJBench. Trong thực tế, chỉ khoảng 1~7% lỗi là lỗ hổng, dẫn đến ít dữ liệu cho các mô hình học. Điều này có nghĩa là, đối với mạng thần kinh, việc sửa lỗ hổng khó khăn hơn lỗi tổng quát và yêu cầu kiến thức chuyên môn hơn.

Phát hiện 1: Các mô hình ngôn ngữ lớn và kỹ thuật APR hiện tại sửa rất ít lỗ hổng Java. Codex sửa 10.2 (20.4%) lỗ hổng trung bình, thể hiện khả năng sửa chữa tốt nhất.

6.1.2 LLMs Tinh chỉnh với Dữ liệu APR. Chúng tôi áp dụng LLMs được tinh chỉnh với dữ liệu APR tổng quát bởi [38] trên các benchmark lỗ hổng. Chúng tôi không thể tinh chỉnh Codex vì OpenAI không cung cấp API công cộng để tinh chỉnh. Bảng 4 cho thấy tất cả LLMs được tinh chỉnh sửa nhiều lỗ hổng hơn các mô hình gốc của chúng. Chi tiết, InCoder tinh chỉnh sửa 9 lỗ hổng, nhiều hơn 4 so với mô hình gốc của nó. Mô hình tốt thứ hai là CodeGen tinh chỉnh, sửa 8 lỗ hổng, nhiều hơn 6 so với mô hình gốc. CodeT5 tinh chỉnh và PLBART tinh chỉnh mỗi cái sửa thêm 3 và 2 lỗ hổng.

Nhìn chung, tinh chỉnh với dữ liệu APR tổng quát có thể cải thiện khả năng sửa chữa của LLMs đối với lỗ hổng. Đầu tiên, tinh chỉnh có thể thích ứng LLMs với các tác vụ APR tốt hơn, làm cho LLMs nhận thức được việc tạo ra các bản vá thay vì mã hoặc văn bản mở. Thứ hai, mặc dù lỗ hổng có đặc điểm đặc biệt (nguyên nhân gốc) so với lỗi tổng quát, một số lỗ hổng vẫn chia sẻ các mẫu sửa chữa tương tự với lỗi tổng quát, như thay thế tham số hàm bằng biến khác, có thể được học tốt trong quá trình tinh chỉnh. Do sự khan hiếm của dữ liệu lỗ hổng thực tế, kết quả của chúng tôi cho thấy việc tinh chỉnh LLMs với dữ liệu APR tổng quát có thể có lợi.

Phát hiện 2: Tinh chỉnh với dữ liệu APR tổng quát cải thiện khả năng sửa lỗ hổng của cả bốn LLMs. InCoder tinh chỉnh sửa 9 lỗ hổng, thể hiện khả năng sửa chữa cạnh tranh so với Codex.

Chúng tôi cũng đánh giá tỷ lệ biên dịch (tức là tỷ lệ các bản vá được tạo ra có thể biên dịch) để nghiên cứu chất lượng của các bản vá. Các bản vá không thể biên dịch không thể là bản vá đúng. Codex, mô hình tốt nhất tổng thể, có tỷ lệ biên dịch 79.7%, cao hơn đáng kể so với LLM tinh chỉnh tốt nhất, InCoder tinh chỉnh (55.2%) và mô hình APR tốt nhất, Recoder (57.6%). Tinh chỉnh cải thiện đáng kể tỷ lệ biên dịch của CodeT5 và CodeGen, từ 6.4% lên 46.8% và từ 35.8% lên 47.2% tương ứng. Mặt khác, tỷ lệ biên dịch của PLBART tinh chỉnh là 45.2%, thấp hơn một chút so với tỷ lệ biên dịch 47.8% của PLBART gốc. Mặc dù có tỷ lệ biên dịch 65.2% cao hơn của InCoder so với mô hình tinh chỉnh của nó, nó tạo ra 82.0% bản vá trùng lặp, trong khi InCoder tinh chỉnh tạo ra các bản vá với các sửa đổi đa dạng hơn dẫn đến nhiều bản sửa đúng hơn. Nhìn chung, so với tỷ lệ biên dịch của việc sửa lỗi tổng quát [38], các tỷ lệ biên dịch này của việc sửa lỗ hổng thấp hơn. PLBART, CodeGen và InCoder không tinh chỉnh khi sửa lỗi tổng quát cho thấy tỷ lệ biên dịch trung bình 65%–73% [38], vượt trội hơn cả mô hình gốc và tinh chỉnh của chúng khi sửa lỗ hổng.

Hình 7a cho thấy một ví dụ về các bản vá không thể biên dịch của Vul4J-12: Chữ ký hàm khai báo t là final, do đó giá trị của t không được phép thay đổi. Tuy nhiên, Codex không thể nắm bắt ràng buộc này, mặc dù chữ ký hàm chỉ cách dòng lỗi hai dòng. Do đó, nó tạo ra mã t-- để giảm giá trị của t làm cho bản vá không thể biên dịch. Tương tự, RewardR bỏ qua thực tế rằng v và vt đều có kiểu int, và gọi hàm equals không hợp lệ trên chúng. Hình 7b cho thấy một ví dụ khác về bản vá không thể biên dịch cho Vul4J-1: parseArray là một phương thức được định nghĩa trong lớp khác trong dự án chỉ chấp nhận hai hoặc ba tham số. Tất cả bốn LLMs tinh chỉnh tạo ra cùng bản vá không thể biên dịch trong đó chúng truyền null làm tham số thứ tư, vì chúng không có thông tin rằng parseArray không chấp nhận bốn tham số.

Những kết quả này cho thấy khả năng học cú pháp mã của LLMs có thể được cải thiện. Công việc gần đây [40,76] là những bước đi đúng hướng để thêm kiến thức miền vào các mô hình để giúp chúng học cú pháp và ngữ nghĩa mã. Một hướng khác là kỹ thuật prompt, như cung cấp chữ ký phương thức hoặc thông tin kiểu trong prompt để chỉ định các ràng buộc. Điều này sẽ cho phép LLMs sử dụng thông tin cú pháp từ toàn bộ dự án, thay vì bị giới hạn trong mã trong hàm lỗi.

Phát hiện 3: Codex có tỷ lệ biên dịch cao nhất là 79.7%. Các LLMs khác (tinh chỉnh hay không) và kỹ thuật APR có tỷ lệ biên dịch thấp (thấp nhất là 6.4% với CodeT5 và phần còn lại từ 24.5% đến 65.2%), cho thấy thiếu kiến thức cú pháp miền.

6.2 RQ2: LLMs và kỹ thuật APR dựa trên học tập sửa những loại lỗ hổng nào?

Bảng 5 cho thấy các lỗ hổng được sửa chính xác bởi LLMs, LLMs tinh chỉnh, và kỹ thuật APR. Tổng cộng, 16 lỗ hổng (thuộc mười danh mục CWE như được hiển thị trong cột CWE với mô tả của chúng trong cột Description) từ cả hai benchmark được sửa bởi ít nhất một trong các mô hình. ID của các lỗ hổng này được liệt kê dưới cột Vul. ID. Một số lỗ hổng thuộc về không có danh mục CWE cụ thể và được liệt kê là unk.

Vul4J-47 là một lỗ hổng mà chỉ Codex có thể sửa. Hình 8a cho thấy bản vá của nhà phát triển cho Vul4J-47 thuộc loại CWE-611 (Hạn chế không đúng về Tham chiếu Thực thể XML Bên ngoài) và CWE-918 (Giả mạo Yêu cầu Phía Máy chủ). Bản sửa đúng yêu cầu chèn câu lệnh xmlIn.setProperty(XMLInputFactory. SUPPORT_DTD, Boolean.FALSE) để vô hiệu hóa hỗ trợ Định nghĩa Kiểu Tài liệu (DTD), vì DTD có thể được sử dụng để thực hiện các cuộc tấn công giả mạo yêu cầu phía máy chủ (SSRF). Mã lỗi gốc chỉ vô hiệu hóa hỗ trợ cho các thực thể bên ngoài bằng cách đặt thuộc tính IS_SUPPORTING_EXTERNAL_ENTITIES thành false, không đủ để ngăn chặn cuộc tấn công. Hình 8b cho thấy một bản vá không đúng được tạo bởi CodeGen tinh chỉnh, thay thế Boolean.FALSE bằng Boolean.TRUE. Nói chung, ngoại trừ Codex, các LLMs và LLMs tinh chỉnh khác chỉ sửa các lỗ hổng yêu cầu sửa đổi đơn giản như xóa câu lệnh hoặc thay thế tên biến/phương thức.

Mặt khác, Codex sửa 15 trong số 16 lỗ hổng (hợp của tất cả lỗi, mà Codex tạo ra ít nhất một bản vá đúng trong hai mươi lăm lần chạy). Lỗ hổng duy nhất được sửa bởi LLMs khác nhưng không phải Codex là Vul4J-39 thuộc loại CWE-200 (Phơi bày Thông tin Nhạy cảm cho Tác nhân Không được Ủy quyền). Lỗ hổng này có thể được sửa bằng cách chỉ đơn giản xóa toàn bộ mã lỗi. Tuy nhiên, đối với Vul4J-39, Codex tạo ra các bản vá bằng cách áp dụng các sửa đổi khác nhau cho mã lỗi, thay vì xóa nó.

Ratpack-1, Vul4J-12, Vul4J-39 và Jenkins-2 là bốn lỗ hổng được sửa bởi số lượng mô hình nhiều nhất (6-7 trong số 13). Ratpack-1 (Hình 9) khi khởi tạo DefaultHttpHeaders, đặt tham số constructor thành false, vô hiệu hóa xác thực cho các giá trị header do người dùng cung cấp. Bản vá đúng chỉ đơn giản loại bỏ false hoặc thay đổi nó thành true để kích hoạt xác thực. Bản sửa cho Vul4J-12 (Hình 7a) là thay đổi từ khóa while thành if, và bản sửa cho cả Jenkins-2 và Vul4J-39 là chỉ đơn giản xóa câu lệnh if phơi bày thông tin nhạy cảm cho các tác nhân không được ủy quyền. Sự đơn giản của các bản vá này được thể hiện rõ từ số lượng mô hình có thể sửa chúng.

Phát hiện 4: Các mô hình ngôn ngữ lớn và kỹ thuật APR, ngoại trừ Codex, chỉ sửa các lỗ hổng yêu cầu thay đổi đơn giản, như xóa câu lệnh hoặc thay thế tên biến/phương thức.

Đáng ngạc nhiên, chín LLMs và bốn kỹ thuật APR không sửa được loại CWE mới nào mà VJBench thêm vào, điều này cho thấy VJBench của chúng tôi giúp tiết lộ những hạn chế của LLMs và kỹ thuật APR hiện tại trong việc sửa lỗ hổng Java. Điều này kêu gọi các kỹ thuật mới có thể sửa CWE-172, CWE-325, CWE-347, CWE-444, CWE-668, và CWE-1295. Ngoài ra, đối với CWE-611 được bao phủ bởi Vul4J-47 của Vul4J, chúng tôi thêm hai instance của loại CWE này (Quartz-1 và Retrofit-1) trong VJBench. Codex sửa Vul4J-47, nhưng không có LLMs và kỹ thuật APR nào sửa được Quartz-1 và Retrofit-1 bổ sung. Điều này cho thấy VJBench bổ sung cho Vul4J ngay cả trên các danh mục CWE mà Vul4J đã bao phủ.

Hình 10 cho thấy Retrofit-1 thuộc danh mục CWE-611. Không có mô hình nào sửa Retrofit-1. Bản vá đúng là ngăn chặn các cuộc tấn công XML External Entity bằng cách gọi xmlInputFactory.setProperty(...) để vô hiệu hóa hỗ trợ cho các thực thể bên ngoài và DTD. Nhưng vì LLMs không được cung cấp thông tin rằng lỗ hổng liên quan đến các cuộc tấn công XML External Entity (như được gợi ý bởi loại CWE), chúng chỉ thực hiện các thay đổi trên mã lỗi (Hình 10b) không liên quan đến thuộc tính XML. Hình 11 cho thấy Jenkins-1 thuộc CWE-325 (Thiếu bước mã hóa), một danh mục CWE mới mà VJBench thêm vào. Bản sửa đúng cho lỗi là thêm điều kiện if để kiểm tra quyền trước vòng lặp for để hạn chế quyền truy cập vào NodeMonitor. Như bản vá của Codex được hiển thị trong Hình 11, tất cả các mô hình không thể sửa lỗi vì chúng chỉ áp dụng các sửa đổi tổng quát cho vòng lặp for và không biết rằng lỗi liên quan đến hạn chế quyền. Hơn nữa, phương thức hasPermission và biến CONNECT được khai báo bên ngoài hàm lỗi, do đó các mô hình không có kiến thức về cách sử dụng của chúng. Điều này phản ánh hai vấn đề đối với LLMs để sửa lỗ hổng Java: (1) Chỉ với các dòng lỗi được chỉ ra, LLMs không thể tạo ra các bản vá nhắm vào lỗ hổng. Điều này cho thấy cần thiết cung cấp cho LLMs nhiều thông tin hơn về lỗ hổng, như loại CWE. (2) Cần nhiều thông tin cụ thể của dự án hơn cho LLMs để sửa lỗ hổng, tức là cung cấp cho LLMs các phương thức và biến liên quan được khai báo bên ngoài hàm lỗi.

Phát hiện 5: Benchmark VJBench mới của chúng tôi tiết lộ rằng các mô hình ngôn ngữ lớn và kỹ thuật APR không thể sửa nhiều loại CWE, bao gồm CWE-172 (Lỗi mã hóa), CWE-325 (Thiếu bước mã hóa), CWE-444 (Buôn lậu yêu cầu HTTP), CWE-668 (Phơi bày tài nguyên cho phạm vi sai), và CWE-1295 (Thông báo debug tiết lộ thông tin không cần thiết).

6.3 RQ3: Khả năng Sửa chữa trên Lỗ hổng Đã biến đổi

Để giảm thiểu mối đe dọa trùng lặp dữ liệu huấn luyện-kiểm thử, chúng tôi áp dụng biến đổi mã cho các benchmark để nghiên cứu khả năng tổng quát hóa của Codex và LLMs trên dữ liệu chưa thấy (Phần 4). Bảng 6 cho thấy số lượng lỗ hổng mà LLMs như vốn có, LLMs tinh chỉnh, và kỹ thuật APR có thể sửa trong bốn cài đặt: (1) Không biến đổi —tập dữ liệu lỗ hổng gốc, (2) Chỉ đổi tên —chỉ áp dụng đổi tên định danh, (3) Chỉ thay đổi cấu trúc mã —chỉ áp dụng thay đổi cấu trúc mã, và (4) Đổi tên + thay đổi cấu trúc mã —áp dụng cả hai biến đổi.

Nhìn chung, biến đổi mã làm cho LLMs (tinh chỉnh hay không) và kỹ thuật APR sửa ít lỗ hổng hơn. Ví dụ, InCoder tinh chỉnh sửa chín lỗ hổng trong Vul4J và VJBench (không biến đổi), nhưng chỉ sửa bốn lỗ hổng đã biến đổi hoàn toàn (Đổi tên + thay đổi cấu trúc mã). Tác động của biến đổi nhỏ hơn trên một số mô hình, ví dụ Codex và CodeT5 tinh chỉnh, thể hiện tính mạnh mẽ của các mô hình này chống lại biến đổi mã và khả năng học tổng quát hóa. Kết quả này, ở một mức độ nào đó, giải quyết mối đe dọa của dữ liệu huấn luyện không công khai của Codex và tiết lộ khả năng học và sửa lỗ hổng mạnh mẽ của Codex. Nhiều mô hình chỉ sửa hai hoặc ít lỗ hổng hơn mà không có biến đổi, do đó tác động của biến đổi không thể lớn đối với các mô hình này. Tuy nhiên, chúng tôi thấy một xu hướng chung trên hầu hết tất cả các mô hình rằng các biến đổi mã này làm cho các mô hình sửa ít lỗ hổng hơn.

Hình 12a cho thấy một ví dụ, Halo-1, mà bản sửa đúng là gọi normalize() trên pathToCheck để loại bỏ bất kỳ phần tử dư thừa nào trong đường dẫn tệp. Lỗi này có thể được sửa chính xác bởi Codex, CodeGen tinh chỉnh, và InCoder tinh chỉnh. Tuy nhiên, sau khi áp dụng cả hai biến đổi, chỉ Codex có thể sửa nó (Hình 12b).

Các biến đổi khác nhau có hiệu ứng khác nhau nhưng mỗi biến đổi ảnh hưởng đáng kể đến ít nhất một LLM. Ví dụ, mặc dù đổi tên định danh có hiệu ứng nhỏ trên CodeT5 tinh chỉnh, nó giảm số lượng lỗ hổng mà PLBART tinh chỉnh sửa xuống ba. Kết quả cho thấy biến đổi mã của chúng tôi hiệu quả trong việc kiểm tra khả năng tổng quát hóa của LLMs trên dữ liệu chưa thấy.

Một quan sát thú vị là một số mô hình sửa các lỗ hổng đã biến đổi mà chúng không thể sửa trong tập dữ liệu gốc. Đây là hiện tượng hợp lý vì biến đổi của chúng tôi có thể chuyển đổi đoạn mã thành dạng đơn giản hơn để các mô hình sửa. Ví dụ, Vul4J-30 là một lỗi mà không có mô hình nào sửa ở dạng gốc, nhưng phiên bản đã biến đổi của nó được sửa bởi tất cả bốn LLMs tinh chỉnh khi áp dụng biến đổi cấu trúc mã. Hình 13 cho thấy bản sửa của Vul4J-30 là gọi trim() trên String.valueOf(value). Lỗ hổng gốc khó sửa vì String.valueOf(value) là một phần của điều kiện if phức tạp. Tuy nhiên, sau biến đổi mã, String.valueOf(value) nổi bật như một câu lệnh đơn, dễ sửa hơn cho LLMs. Hiện tượng này cho thấy biến đổi mã tương đương có thể là một hướng đầy hứa hẹn để đơn giản hóa mã dễ bị tổn thương và tăng cường hiệu quả của việc sửa lỗ hổng.

Phát hiện 6: Biến đổi mã làm cho các mô hình ngôn ngữ lớn và kỹ thuật APR sửa ít lỗ hổng hơn. Một số mô hình như Codex và CodeT5 tinh chỉnh mạnh mẽ hơn với biến đổi mã. Mặt khác, một số biến đổi làm cho lỗ hổng dễ sửa hơn.

7 CÁC MỐI ĐE DỌA ĐỐI VỚI TÍNH HỢP LỆ

Lỗ hổng Java rất đa dạng. Khó để các benchmark đại diện cho tất cả chúng. Do đó, các phát hiện của chúng tôi có thể không tổng quát hóa cho tất cả lỗ hổng Java. Chúng tôi giải quyết mối đe dọa này bằng cách mở rộng benchmark lỗ hổng Java hiện có với tập dữ liệu lỗ hổng mới.

Chúng tôi dựa vào các bản vá của nhà phát triển để đánh giá xem một lỗ hổng có được sửa hay không. Các nhà phát triển có thể mắc lỗi trong việc sửa lỗ hổng. Do đó, ground truth của chúng tôi có thể không chính xác. Chúng tôi giảm thiểu mối đe dọa này bằng cách chỉ xem xét các lỗ hổng được công bố công khai trong tập dữ liệu NVD có thể tái tạo và bao gồm test case cho thấy phiên bản đã sửa không còn có thể khai thác.

Một mối đe dọa khác là Codex (và các LLMs khác) có thể đã được huấn luyện trên các bản vá lỗ hổng trong tập dữ liệu Vul4J và VJBench. Để giảm thiểu vấn đề này, chúng tôi áp dụng biến đổi mã để tạo ra các lỗ hổng tương đương về mặt ngữ nghĩa không được bao gồm trong tập dữ liệu huấn luyện của chúng. Sau đó chúng tôi áp dụng Codex để sửa chữa các chương trình đã biến đổi này để chứng minh rằng Codex thực sự có thể sửa chữa các lỗ hổng mới mà nó chưa thấy.

8 CÔNG VIỆC LIÊN QUAN

8.1 Kỹ thuật Sửa Lỗ hổng Dựa trên DL

Nhiều công việc sử dụng DL để sửa lỗ hổng. Các phương pháp encoder-decoder đã được đề xuất để sửa chữa lỗ hổng C: [29] tinh chỉnh mô hình CodeT5 với dữ liệu sửa chữa lỗ hổng C; [19] huấn luyện mô hình transformer trên tập dữ liệu sửa lỗi lớn và sau đó tinh chỉnh trên tập dữ liệu sửa lỗ hổng nhỏ, nhưng họ sử dụng độ chính xác chuỗi làm chỉ số đánh giá thay vì cài đặt APR thực tế. Công việc trước [35] áp dụng cả CodeBERT và GraphCodeBert để sửa lỗ hổng, nhưng họ chỉ đánh giá trên cơ sở dữ liệu lỗ hổng tổng hợp, bộ test Juliet 1.1 C/C++ [14], là benchmark chỉ để đánh giá các bộ phân tích tĩnh. Do đó, các lỗ hổng trong tập dữ liệu được cô lập và đơn giản hóa để phù hợp trong vài dòng và không đại diện cho lỗ hổng mã trong sản xuất. Công việc của chúng tôi khác vì chúng tôi sử dụng tập dữ liệu lỗ hổng thực tế cho đánh giá, làm cho kết quả của chúng tôi gần hơn với những gì các nhà nghiên cứu và nhà phát triển có thể mong đợi về chất lượng sửa chữa lỗ hổng LLM trong mã sản xuất thực tế.

Công việc trước [58] áp dụng LLMs với học zero-shot để sửa chữa bảy lỗ hổng C/Python thủ công và 12 lỗ hổng C thực tế. Họ khám phá hiệu quả của các mẫu prompt khác nhau và sử dụng công cụ phân tích tĩnh CodeQL hoặc bộ sanitizer C để phát hiện lỗ hổng để tích hợp các thông báo lỗi thu được vào prompt đầu vào. Công việc của chúng tôi khác với [58] trong một số khía cạnh chính. Đầu tiên, chúng tôi nghiên cứu không chỉ LLMs mà còn các công cụ APR dựa trên DL và LLMs được tinh chỉnh với dữ liệu APR tổng quát. Thứ hai, chúng tôi đánh giá phương pháp của mình trên tập dữ liệu lớn hơn gồm 50 lỗ hổng Java thực tế. Thứ ba, chúng tôi áp dụng biến đổi mã để giảm thiểu vấn đề rò rỉ dữ liệu và đề xuất hướng mới sử dụng biến đổi để đơn giản hóa việc sửa chữa cho một số lỗ hổng. Hầu hết lỗ hổng trong Vul4J và VJBench không thể được phát hiện bởi các công cụ phân tích bảo mật Java hiện đại, vì vậy chúng tôi không thể tích hợp thông báo lỗi trong prompt đầu vào như [58] đã làm.

8.2 Benchmark Lỗ hổng

Công việc trước đã đề xuất các benchmark và tập dữ liệu để giúp đánh giá các phương pháp sửa lỗ hổng. Maestro [61] đề xuất một nền tảng để đánh giá các công cụ trên lỗ hổng Java và C++. Vì Maestro không hỗ trợ chạy LLMs và mô hình APR, chúng tôi trực tiếp sử dụng cùng tập dữ liệu lỗ hổng Java, Vul4J [16], với tập dữ liệu VJBench mới của chúng tôi. Các benchmark và tập dữ liệu khác của lỗ hổng thực tế đã được đề xuất [12,24,56,62]. Tuy nhiên, các tập dữ liệu này chỉ chứa đoạn mã từ các commit sửa lỗi và không có test case. Do đó, các tập dữ liệu như vậy chỉ có thể hỗ trợ khớp mã khi đánh giá tính đúng đắn của bản vá, và không thể được sử dụng trong sửa chữa chương trình tự động trong thực tế.

8.3 LLMs cho Sửa chữa và Các Tác vụ Khác

Các nhà nghiên cứu sử dụng LLMs để cải thiện nhiều tác vụ kỹ thuật phần mềm như sửa chữa chương trình tự động [40,50,63], gợi ý hoàn thành tự động [25], và lập trình cặp [37]. Nhiều công việc cũng thảo luận về hàm ý của LLMs đối với các nhà phát triển phần mềm [23,27,51] và những hạn chế hiện tại của LLMs [9,20,68]. Công việc của chúng tôi khám phá một lĩnh vực ứng dụng khác của LLMs, với những thách thức riêng (lỗ hổng nổi tiếng khó sửa [52]) chưa được khám phá tốt.

9 KẾT LUẬN

Công việc này là nghiên cứu đầu tiên điều tra khả năng của LLMs và mô hình APR dựa trên DL trong việc sửa chữa lỗ hổng trong Java. Chúng tôi đánh giá năm LLMs, bốn LLMs tinh chỉnh, và bốn kỹ thuật APR dựa trên DL trên hai benchmark lỗ hổng Java thực tế bao gồm một cái mới mà chúng tôi tạo. Chúng tôi sử dụng biến đổi mã để giải quyết mối đe dọa trùng lặp dữ liệu huấn luyện và kiểm thử của LLMs và tạo ra benchmark sửa chữa lỗ hổng Java mới VJBench, và phiên bản đã biến đổi VJBench-trans. Chúng tôi thấy rằng các LLMs và mô hình APR hiện tại sửa rất ít lỗ hổng Java, và kêu gọi các đổi mới nghiên cứu mới để cải thiện sửa chữa lỗ hổng Java tự động như tạo ra các tập dữ liệu huấn luyện sửa chữa lỗ hổng lớn hơn, tinh chỉnh LLMs với dữ liệu đó, khám phá học few-shot, và tận dụng các biến đổi đơn giản hóa để cải thiện sửa chữa chương trình.

Gói tái tạo: Benchmark và artifact của chúng tôi có sẵn tại [5].

LỜI CẢM ƠN

Chúng tôi cảm ơn các reviewer vì những bình luận và gợi ý sâu sắc. Công việc này được tài trợ một phần bởi NSF 1901242, NSF 2006688, Giải thưởng Nghiên cứu Khoa J.P. Morgan AI, và Giải thưởng Nghiên cứu Meta/Facebook. Bất kỳ ý kiến, phát hiện và kết luận nào trong bài báo này chỉ của các tác giả và không nhất thiết phản ánh quan điểm của các nhà tài trợ.

TÀI LIỆU THAM KHẢO

[1] 2022. Codex. https://beta.openai.com/docs/guides/code
[2] Truy cập: 2022. Hướng dẫn ngăn chặn, phát hiện và săn lùng khai thác lỗ hổng Log4j 2. https://www.microsoft.com/en-us/security/blog/2021/12/11/guidance-for-preventing-detecting-and-hunting-for-cve-2021-44228-log4j-2-exploitation/.
[3] Truy cập: 2023. Tài liệu NLTK. https://www.nltk.org/howto/wordnet.html.
[4] Truy cập: 2023. Nguồn dữ liệu NVD. https://nvd.nist.gov/vuln/data-feeds.
[5] Truy cập: 2023. Gói tái tạo của công việc này. https://github.com/lin-tan/llm-vul.
[6] Truy cập: 2023. Kho GitHub src2abs. https://github.com/micheletufano/src2abs.
[7] Truy cập: 2023. Hiểu tác động của Lỗ hổng Apache Log4j. https://security.googleblog.com/2021/12/understanding-impact-of-apache-log4j.html.
[8] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Mô hình Encoder-Decoder Tiền huấn luyện Thống nhất Nhận biết Định danh cho Hiểu và Tạo ra Chương trình. Trong Kỷ yếu Hội nghị 2021 của Chương Bắc Mỹ của Hiệp hội Ngôn ngữ học Tính toán: Công nghệ Ngôn ngữ Con người. Hiệp hội Ngôn ngữ học Tính toán, Trực tuyến, 2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211
