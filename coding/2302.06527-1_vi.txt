# 2302.06527.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2302.06527.pdf
# Kích thước tệp: 784093 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
1
Đánh Giá Thực Nghiệm Việc Sử Dụng Mô Hình
Ngôn Ngữ Lớn cho Tự Động Tạo Unit Test
Max Schäfer, Sarah Nadi, Aryaz Eghbali, Frank Tip
Tóm tắt —Unit test đóng vai trò quan trọng trong việc đảm bảo tính đúng đắn của phần mềm. Tuy nhiên, việc tạo unit test thủ công là một công việc tốn nhiều công sức, tạo động lực cho nhu cầu tự động hóa. Mô hình Ngôn ngữ Lớn (LLM) gần đây đã được áp dụng cho nhiều khía cạnh khác nhau của phát triển phần mềm, bao gồm việc sử dụng được đề xuất cho tự động tạo unit test, nhưng vẫn yêu cầu đào tạo bổ sung hoặc học few-shot trên các ví dụ về test hiện có. Bài báo này trình bày một đánh giá thực nghiệm quy mô lớn về hiệu quả của LLM trong tự động tạo unit test mà không yêu cầu đào tạo bổ sung hoặc nỗ lực thủ công. Cụ thể, chúng tôi xem xét một phương pháp trong đó LLM được cung cấp các prompt bao gồm chữ ký và triển khai của hàm cần test, cùng với các ví dụ sử dụng được trích xuất từ tài liệu. Hơn nữa, nếu một test được tạo ra bị lỗi, phương pháp của chúng tôi cố gắng tạo một test mới khắc phục vấn đề bằng cách re-prompt mô hình với test bị lỗi và thông báo lỗi. Chúng tôi triển khai phương pháp của mình trong TESTPILOT, một công cụ tạo test thích ứng dựa trên LLM cho JavaScript tự động tạo unit test cho các phương thức trong API của một dự án cho trước.
Chúng tôi đánh giá TESTPILOT sử dụng LLM gpt3.5-turbo của OpenAI trên 25 gói npm với tổng cộng 1.684 hàm API. Các test được tạo đạt được độ phủ statement trung vị là 70,2% và độ phủ branch là 52,8%. Ngược lại, kỹ thuật tạo test JavaScript được hướng dẫn bởi phản hồi tiên tiến nhất, Nessie, chỉ đạt được 51,3% độ phủ statement và 25,6% độ phủ branch. Hơn nữa, các thí nghiệm với việc loại bỏ các phần thông tin được bao gồm trong prompt cho thấy tất cả các thành phần đều đóng góp vào việc tạo ra các bộ test hiệu quả. Chúng tôi cũng thấy rằng 92,8% các test được tạo bởi TESTPILOT có độ tương tự ≤50% với các test hiện có (được đo bằng khoảng cách chỉnh sửa chuẩn hóa), không có test nào là bản sao chính xác. Cuối cùng, chúng tôi chạy TESTPILOT với hai LLM bổ sung, LLM code-cushman-002 cũ hơn của OpenAI và StarCoder, một LLM mà quá trình đào tạo được ghi chép công khai. Nhìn chung, chúng tôi quan sát thấy kết quả tương tự với LLM trước (68,2% độ phủ statement trung vị), và kết quả phần nào tệ hơn với LLM sau (54,0% độ phủ statement trung vị), cho thấy hiệu quả của phương pháp bị ảnh hưởng bởi kích thước và tập đào tạo của LLM, nhưng không phụ thuộc cơ bản vào mô hình cụ thể.
Thuật ngữ chỉ mục —tạo test, JavaScript, mô hình ngôn ngữ
✦
1 GIỚI THIỆU
Unit test kiểm tra tính đúng đắn của các hàm riêng lẻ hoặc các đơn vị mã nguồn khác, và đóng vai trò quan trọng trong phát triển phần mềm hiện đại [1]–[3]. Tuy nhiên, việc tạo unit test bằng tay tốn nhiều thời gian và nhàm chán, khiến một số nhà phát triển bỏ qua việc viết test hoàn toàn [4].
Thực tế này đã tạo cảm hứng cho nghiên cứu rộng rãi về các kỹ thuật tự động tạo test bao gồm fuzzing [5], [6], tạo test ngẫu nhiên được hướng dẫn bởi phản hồi [7]–[11], thực thi symbolic động [12]–[15], và các kỹ thuật dựa trên tìm kiếm và tiến hóa [16], [17]. Ở mức độ cao, hầu hết các kỹ thuật này sử dụng các kỹ thuật phân tích tĩnh hoặc động để khám phá các đường dẫn luồng điều khiển và dữ liệu trong chương trình, và sau đó cố gắng tạo ra các test tối đa hóa độ phủ. Mặc dù chúng thường thành công trong việc tạo ra các test phát hiện lỗi, các kỹ thuật này có hai nhược điểm chính. Thứ nhất, các test được tạo ra thường ít có thể đọc được và hiểu được hơn so với các test được viết thủ công [18], [19], đặc biệt là do việc sử dụng các tên biến không trực quan [20]. Thứ hai, các test được tạo ra thường thiếu assertion [21], hoặc chỉ chứa các assertion rất chung chung (ví dụ, một biến được tham chiếu phải không null), hoặc quá nhiều assertion giả [22]. Mặc dù các test như vậy có thể cung cấp cảm hứng cho việc tạo thủ công các bộ test có độ phủ cao, chúng không trông tự nhiên và thường không thể được sử dụng nguyên văn.
Do những nhược điểm này, gần đây đã có sự quan tâm ngày càng tăng trong việc sử dụng các kỹ thuật tạo mã dựa trên học máy để tạo ra các unit test tốt hơn [23]–[29]. Cụ thể, các nỗ lực nghiên cứu này tận dụng LLM đã được đào tạo trên kho ngữ liệu lớn của văn bản ngôn ngữ tự nhiên và mã nguồn. Chúng tôi đặc biệt quan tâm đến các mô hình transformer sinh tạo mà, khi được cung cấp một đoạn văn bản hoặc mã nguồn (được gọi là prompt), sẽ dự đoán văn bản có khả năng theo sau nó (từ nay được gọi là completion). Hóa ra LLM giỏi trong việc tạo ra các completion trông tự nhiên cho cả ngôn ngữ tự nhiên và mã nguồn, và ở một mức độ nào đó "hiểu" ngữ nghĩa của ngôn ngữ tự nhiên và mã, dựa trên các mối quan hệ thống kê về khả năng thấy một từ cụ thể trong một ngữ cảnh cho trước. Một số LLM như BERT [30] hoặc GPT-3 [31] được đào tạo hoàn toàn trên văn bản được trích xuất từ sách và các nguồn công cộng khác, trong khi những LLM khác như OpenAI Codex [32] và AlphaCode [33] được đào tạo bổ sung trên mã nguồn có sẵn công khai để làm cho chúng phù hợp hơn với các tác vụ phát triển phần mềm [34]–[43].
Với các đặc tính của LLM, có lý khi kỳ vọng rằng chúng có thể tạo ra các test trông tự nhiên. Chúng không chỉ có khả năng tạo ra mã giống với những gì một nhà phát triển con người sẽ viết (bao gồm, ví dụ, tên biến hợp lý), mà LLM cũng có khả năng tạo ra các test chứa assertion, đơn giản vì hầu hết các test trong tập đào tạo của chúng đều có. Do đó, bằng cách tận dụng LLM, người ta có thể hy vọng đồng thời giải quyết hai nhược điểm của các kỹ thuật tạo test truyền thống. Mặt khác, người ta có lẽ không mong đợi LLM tạo ra các test bao gồm các trường hợp biên phức tạp hoặc thực hiện các đầu vào hàm bất thường, vì những điều này sẽ hiếm trong dữ liệu đào tạo, làm cho LLM phù hợp hơn cho việc tạo regression test hơn là tìm bug.

Đã có một số công việc khám phá về việc sử dụng LLM cho tạo test. Ví dụ, Bareiß et al. [25] đánh giá hiệu suất của Codex cho tạo test. Họ theo paradigm học few-shot trong đó prompt của họ bao gồm hàm cần test cùng với một ví dụ về hàm khác và test đi kèm để cho mô hình ý tưởng về test nên như thế nào. Trong một đánh giá hạn chế trên 18 phương thức Java, họ thấy rằng phương pháp này so sánh thuận lợi với tạo test được hướng dẫn bởi phản hồi [8]. Tương tự, ATHENATEST của Tufano et al. [26] tạo test sử dụng mô hình transformer BART [44] được tinh chỉnh trên tập đào tạo các hàm và test tương ứng của chúng. Họ đánh giá trên năm dự án Java, đạt được độ phủ tương đương với EvoSuite [17]. Mặc dù đây là những kết quả ban đầu đầy hứa hẹn, các phương pháp này, cũng như những phương pháp khác [29], [45], [46], dựa vào kho ngữ liệu đào tạo của các hàm và test tương ứng của chúng, điều này tốn kém để tuyển chọn và duy trì.

Trong bài báo này, chúng tôi khám phá tính khả thi của việc tự động tạo unit test sử dụng LLM có sẵn, không cần đào tạo bổ sung và với ít tiền xử lý nhất có thể. Theo Reynolds và McDonell [47], chúng tôi cho rằng việc cung cấp cho mô hình các ví dụ đầu vào-đầu ra hoặc thực hiện đào tạo bổ sung là không cần thiết và việc tạo prompt cẩn thận là đủ. Cụ thể, ngoài mã khung test, prompt của chúng tôi chứa (1) chữ ký của hàm cần test; (2) chú thích tài liệu của nó, nếu có; (3) các ví dụ sử dụng cho hàm được khai thác từ tài liệu, nếu có sẵn; (4) mã nguồn của nó. Cuối cùng, chúng tôi xem xét một thành phần thích ứng cho kỹ thuật của mình: mỗi test được tạo ra được thực thi, và nếu nó bị lỗi, LLM được prompt lại với một prompt đặc biệt bao gồm (5) test bị lỗi và thông báo lỗi mà nó tạo ra, điều này thường cho phép mô hình sửa test và làm cho nó pass.

Để tiến hành thí nghiệm, chúng tôi đã triển khai những kỹ thuật này trong một hệ thống có tên TESTPILOT, một bộ tạo test dựa trên LLM cho JavaScript. Chúng tôi chọn JavaScript làm ví dụ về một ngôn ngữ phổ biến mà việc tạo test sử dụng các phương pháp truyền thống là thách thức do thiếu thông tin kiểu tĩnh và ngữ nghĩa runtime khoan dung của nó [11]. Chúng tôi đánh giá phương pháp của mình trên 25 gói npm từ nhiều lĩnh vực khác nhau được lưu trữ trên cả GitHub và GitLab, với các mức độ phổ biến và lượng tài liệu có sẵn khác nhau. Những gói này có tổng cộng 1.684 hàm API mà chúng tôi cố gắng tạo test cho. Chúng tôi điều tra độ phủ đạt được bởi các test được tạo và chất lượng của chúng về tỷ lệ thành công, lý do thất bại, và liệu chúng có chứa assertion thực sự thực hiện chức năng từ gói đích hay không (assertion không tầm thường). Chúng tôi cũng đánh giá thực nghiệm hiệu ứng của các thành phần khác nhau trong chiến lược tạo prompt của chúng tôi cũng như liệu TESTPILOT có đang tạo ra các test đã được ghi nhớ trước đó từ dữ liệu đào tạo của LLM hay không.

Sử dụng mô hình gpt3.5-turbo hiện tại có khả năng và hiệu quả chi phí nhất của OpenAI,1 các test được tạo bởi TESTPILOT đạt được độ phủ statement trung vị là 70,2%, và độ phủ branch là 52,8%. Chúng tôi thấy rằng trung vị 61,4% các test được tạo chứa assertion không tầm thường, và những test không tầm thường này riêng lẻ đạt được độ phủ trung vị 61,6%, cho thấy các test được tạo chứa các oracle có ý nghĩa thực hiện chức năng từ gói đích. Khi kiểm tra sâu hơn, chúng tôi thấy rằng lý do phổ biến nhất khiến các test được tạo bị lỗi là vượt quá timeout hai giây mà chúng tôi áp dụng, thường là do không thể truyền đạt việc hoàn thành test cho framework testing. Chúng tôi thấy rằng, trung bình, phương pháp thích ứng có thể sửa 15,6% các test bị lỗi. Đánh giá thực nghiệm của chúng tôi cũng cho thấy rằng tất cả năm thành phần được bao gồm trong prompt đều cần thiết để tạo ra các bộ test có ý nghĩa với độ phủ cao. Loại bỏ bất kỳ thành phần nào trong số này đều dẫn đến tỷ lệ test bị lỗi cao hơn hoặc độ phủ giảm. Mặt khác, trong khi loại bỏ các ví dụ sử dụng từ prompt làm giảm hiệu quả của phương pháp, nó không làm cho phương pháp trở nên lỗi thời, cho thấy LLM có thể học từ sự hiện diện của mã test tương tự trong tập đào tạo của nó.

Cuối cùng, từ các thí nghiệm được tiến hành với LLM gpt3.5-turbo, chúng tôi lưu ý rằng độ phủ cao vẫn được đạt trên các gói có mã nguồn được lưu trữ trên GitLab (và do đó không phải là một phần của dữ liệu đào tạo của LLM). Hơn nữa, chúng tôi thấy rằng 60,0% các test được tạo sử dụng LLM gpt3.5-turbo có độ tương tự ≤40% với các test hiện có và 92,8% có độ tương tự ≤50%, không có test nào là bản sao chính xác. Điều này cho thấy các test được tạo không được sao chép nguyên văn từ tập đào tạo của LLM.

Về nguyên tắc, phương pháp tạo test đang được xem xét có thể được sử dụng với bất kỳ LLM nào. Tuy nhiên, hiệu quả của phương pháp có khả năng phụ thuộc vào kích thước và tập đào tạo của LLM. Để khám phá yếu tố này, chúng tôi tiến hành thêm các thí nghiệm với hai LLM bổ sung: mô hình độc quyền code-cushman-002 [48] trước đây được phát triển bởi OpenAI và StarCoder [49], một LLM mà quá trình đào tạo được ghi chép công khai. Chúng tôi quan sát thấy kết quả về chất lượng tương tự sử dụng code-cushman-002 (độ phủ trung vị 68,2% cho statement, 51,2% cho branch), và kết quả phần nào tệ hơn sử dụng StarCoder (54,0% và 37,5%).

Tóm lại, bài báo này có những đóng góp sau:
• Một kỹ thuật tạo test đơn giản trong đó unit test được tạo bằng cách lặp đi lặp lại truy vấn LLM với prompt chứa chữ ký của các hàm API cần test và, tùy chọn, nội dung, tài liệu, và các ví dụ sử dụng liên quan đến các hàm như vậy. Kỹ thuật này cũng có một thành phần thích ứng bao gồm trong prompt các thông báo lỗi được quan sát khi thực thi các test được tạo trước đó.

1. https://platform.openai.com/docs/models/gpt-3-5

--- TRANG 2 ---
• Một triển khai của kỹ thuật này cho JavaScript trong một công cụ có tên TESTPILOT, có sẵn dưới dạng phần mềm mã nguồn mở tại https://github.com/githubnext/testpilot.
• Một đánh giá thực nghiệm rộng rãi về TESTPILOT trên 25 gói npm, chứng minh hiệu quả của nó trong việc tạo ra các bộ test với độ phủ cao. Đánh giá của chúng tôi khám phá các khía cạnh sau:
– Chất lượng của các test được tạo về mặt assertion mà chúng chứa, và độ phủ của các test bao gồm assertion không tầm thường.
– Hiệu ứng của việc loại bỏ các thành phần prompt khác nhau.
– Độ tương tự của các test được tạo với các test hiện có.
– So sánh với Nessie [11], một kỹ thuật tạo test ngẫu nhiên được hướng dẫn bởi phản hồi tiên tiến cho JavaScript.
– So sánh hiệu ứng của LLM cơ bản trên các test được tạo bởi TESTPILOT.

Dữ liệu thô và phân tích cho tất cả các thí nghiệm của chúng tôi có thể được tìm thấy tại https://doi.org/10.6084/m9.figshare.23653371.

2 PHƯƠNG PHÁP
TESTPILOT tạo test sử dụng framework testing JavaScript phổ biến Mocha [50] với cú pháp BDD-style trong đó test được triển khai dưới dạng các hàm callback được truyền cho hàm it. Bộ test bao gồm một hoặc nhiều lời gọi it xảy ra trong hàm callback được truyền cho hàm describe. Assertion được kiểm tra sử dụng module assert built-in của Node.js.

Hình 1 minh họa cấu trúc của các test được tạo cho hàm f. Ở đây, dòng 1-3 là mã boilerplate để import các thư viện testing và Package under Test (PUT). Theo sau là một hoặc nhiều dòng được comment chứa metadata hàm được bao gồm trong prompt, như chúng tôi sẽ giải thích ngay. Dòng 7-8 bắt đầu định nghĩa bộ test sử dụng describe với một test duy nhất được định nghĩa dưới dạng hàm callback chấp nhận tham số done được truyền cho hàm it. Mã test sử dụng assert để kiểm tra assertion của nó, và cuối cùng gọi done() để báo hiệu hoàn thành. Điều này cần thiết cho các test bất đồng bộ có thể mất nhiều lần lặp của vòng lặp sự kiện JavaScript để hoàn thành. Gọi done() nhiều hơn một lần dẫn đến lỗi runtime, trong khi không gọi nó hoàn toàn khiến test bị lỗi với lỗi timeout.

Ý tưởng cơ bản của phương pháp chúng tôi là gửi phần đầu của khung test trên đến (nhưng không bao gồm) phần bắt đầu của mã test thực tế trên Dòng 9 (được làm nổi bật ở trên bằng màu xanh) dưới dạng prompt cho LLM. Vì LLM được đào tạo để hoàn thành một đoạn mã cho trước, do đó người ta có thể kỳ vọng nó sẽ tạo ra phần còn lại của test cho chúng ta. Comment có thể được bao gồm trong khung test để cung cấp thông tin bổ sung về hàm có thể hữu ích để hướng dẫn LLM tạo ra các test tốt hơn.

2.1 Kiến trúc TESTPILOT
Hình 2 trình bày kiến trúc cấp cao của TESTPILOT, bao gồm năm thành phần chính: Với PUT làm đầu vào, API explorer xác định các hàm để test; documentation miner trích xuất metadata về chúng; và prompt generator, test validator, và prompt refiner cộng tác để xây dựng prompt cho việc tạo test, lắp ráp các test hoàn chỉnh từ phản hồi của LLM, chạy chúng để xác định liệu chúng có pass hay không, và xây dựng thêm prompt để tạo ra nhiều test hơn. Bây giờ chúng tôi thảo luận chi tiết về từng thành phần này.

API Explorer: Thành phần này phân tích PUT để xác định API của nó, tức là tập hợp các hàm, phương thức, hằng số, v.v. mà gói expose cho client. Trong JavaScript, rất khó xác định API một cách tĩnh do bản chất rất động của ngôn ngữ. Do đó, tương tự như các công việc tạo test JavaScript khác [10], [11], chúng tôi theo đuổi một phương pháp dựa trên phân tích động. Cụ thể, chúng tôi load gói ứng dụng chính và áp dụng introspection để duyệt qua đồ thị đối tượng kết quả và xác định các thuộc tính được gắn với các hàm. Đối với mỗi hàm, chúng tôi ghi lại đường dẫn truy cập của nó (tức là chuỗi các thuộc tính phải được duyệt qua để đến được nó từ module chính), chữ ký của nó (mà trong trường hợp không có thông tin kiểu tĩnh đơn giản là một danh sách tên tham số), và định nghĩa của nó (tức là mã nguồn của nó). Đầu ra của API Explorer là một danh sách các hàm được mô tả bởi đường dẫn truy cập, chữ ký, và định nghĩa của chúng; các phần tử API khác bị bỏ qua.

Documentation Miner: Thành phần này trích xuất các đoạn mã và comment từ tài liệu được bao gồm với PUT, và liên kết chúng với các hàm API mà chúng liên quan đến. Mục đích là thu thập, cho mỗi hàm API, các comment và ví dụ mô tả mục đích và cách sử dụng dự định của nó. Trong cơ sở mã JavaScript, tài liệu thường được cung cấp dưới dạng các tệp Markdown (.md), trong đó các đoạn mã được nhúng dưới dạng các khối mã có rào chắn (tức là các khối được bao quanh bởi ba dấu backtick). Chúng tôi tìm tất cả các khối như vậy trong tất cả các tệp Markdown trong cơ sở mã, và liên kết với mỗi hàm tập hợp tất cả các đoạn mã mà textually chứa tên của hàm. Mặc dù đây là một heuristic đơn giản, các ví dụ mã có thể không hoàn chỉnh hoặc đúng về mặt cú pháp, vì vậy một phương pháp tinh vi hơn dựa trên parsing hoặc phân tích tĩnh không có khả năng hoạt động tốt. Chúng tôi cũng liên kết mỗi hàm API với doc comment (/**...*/) ngay trước nó, nếu có.

Ba thành phần còn lại là prompt generator, test validator, và prompt refiner, hoạt động cùng nhau để tạo và validate test cho tất cả các hàm API được xác định bởi API Explorer, sử dụng thông tin được cung cấp bởi Documentation Miner. Các hàm được xử lý từng cái một, và đối với mỗi hàm chỉ một test được tạo tại một thời điểm (thay vì tạo toàn bộ bộ test cùng một lúc). Điều này nhằm cho phép chúng tôi validate từng test riêng lẻ mà không bị can thiệp từ các test khác.

--- TRANG 3 ---
Prompt Generator: Thành phần này xây dựng prompt ban đầu để gửi cho LLM cho việc tạo test cho một hàm f cho trước. Như đã đề cập ở trên, ban đầu chúng tôi có (tối đa) bốn mẩu thông tin về f theo ý định của chúng tôi: chữ ký của nó, định nghĩa của nó, doc comment của nó, và các đoạn sử dụng của nó được trích xuất từ tài liệu. Mặc dù có vẻ tự nhiên khi xây dựng một prompt chứa tất cả thông tin này, trong thực tế đôi khi có thể xảy ra rằng prompt phức tạp hơn dẫn đến completion tệ hơn vì LLM bị nhầm lẫn bởi thông tin bổ sung. Do đó, chúng tôi theo một chiến lược khác: chúng tôi bắt đầu với một prompt ban đầu rất đơn giản chỉ bao gồm metadata ngoại trừ chữ ký hàm, và sau đó để prompt refiner mở rộng nó từng bước với thông tin bổ sung.

Test Validator: Tiếp theo, chúng tôi gửi các prompt được tạo cho LLM và chờ completion. Chúng tôi chỉ sử dụng bao nhiêu token cần thiết để tạo thành một test hợp lệ về mặt cú pháp. Vì không có đảm bảo rằng các completion được đề xuất bởi mô hình là hợp lệ về mặt cú pháp, test validator cố gắng sửa các lỗi cú pháp đơn giản như thiếu dấu ngoặc, và sau đó parse mã kết quả để kiểm tra xem nó có hợp lệ về mặt cú pháp hay không. Nếu không, test ngay lập tức được đánh dấu là failed. Ngược lại, nó được chạy sử dụng test runner Mocha để xác định liệu nó pass hay fail (do lỗi assertion hoặc một số lỗi runtime khác).

Mỗi completion được trả về có thể được nối với prompt để tạo ra một test ứng cử viên. Tuy nhiên, để cho phép chúng tôi loại bỏ các test trùng lặp được tạo từ các prompt khác nhau, chúng tôi hậu xử lý các test ứng cử viên như sau: chúng tôi strip comment chứa metadata hàm trong prompt và thay thế các mô tả trong các lời gọi describe và it bằng các chuỗi chung 'test suite' và 'test case', tương ứng.

Prompt Refiner: Prompt Refiner áp dụng một số chiến lược để tạo ra các prompt bổ sung để sử dụng cho việc truy vấn mô hình. Nhìn chung, chúng tôi sử dụng bốn prompt refiner như sau:
1) FnBodyIncluder: Nếu p không chứa định nghĩa của f, một prompt được tạo bao gồm nó.
2) DocCommentIncluder: Nếu f có doc comment nhưng p không bao gồm nó, một prompt với doc comment được tạo.
3) SnippetIncluder: Nếu các đoạn sử dụng cho f có sẵn nhưng p không bao gồm chúng, một prompt với đoạn được tạo.
4) RetryWithError: Nếu t failed với thông báo lỗi e, một prompt được xây dựng bao gồm: văn bản của test bị lỗi t theo sau bởi một comment // the test above fails with the following error: e, theo sau bởi một comment // fixed test. Chiến lược này chỉ được áp dụng một lần cho mỗi prompt, vì vậy nó không được cố gắng nếu p bản thân đã được tạo bởi chiến lược này.

Prompt được tinh chỉnh sau đó được sử dụng để xây dựng một test theo cách tương tự như prompt gốc. Tất cả các chiến lược được áp dụng độc lập và trong tất cả các kết hợp có thể, nhưng lưu ý rằng ba chiến lược đầu sẽ chỉ áp dụng tối đa một lần và chiến lược thứ tư sẽ không bao giờ áp dụng hai lần liên tiếp, do đó đảm bảo kết thúc.

2.2 Chi tiết Thuật toán
Bây giờ chúng tôi cung cấp chi tiết bổ sung về hai bước chính của phương pháp chúng tôi: khám phá API và tạo test.

Khám phá API: Thuật toán 1 cho thấy pseudocode minh họa cách tập hợp các hàm tạo thành API cho một gói được xác định. Thuật toán nhận một gói cần test, pkgName, và tạo ra một danh sách các cặp ⟨a,sig⟩ đại diện cho API của nó. Ở đây, a là một đường dẫn truy cập duy nhất đại diện cho một phương thức API, và sig là chữ ký của một hàm. Khái niệm đường dẫn truy cập của chúng tôi có dạng phần nào đơn giản hóa so với khái niệm gốc được đề xuất bởi Mezzetti et al. [51], và bao gồm tên gói theo sau bởi một chuỗi tên thuộc tính.

Chúng tôi dựa vào một phương pháp động để khám phá API của gói pkgName, bằng cách tạo một chương trình nhỏ import gói (dòng 2), và dựa vào khả năng introspective của JavaScript để xác định thuộc tính nào có mặt trong đối tượng gốc gói modObj được tạo bởi việc import pkgName và các kiểu của những thuộc tính này là gì. Khám phá các thuộc tính của modObj được xử lý bởi một hàm đệ quy explore bắt đầu tại đường dẫn truy cập đại diện cho gốc gói và duyệt qua đối tượng này một cách đệ quy, gọi một hàm phụ trợ khác extend để mở rộng đường dẫn truy cập khi việc duyệt đi xuống cấu trúc của đối tượng. Trong quá trình khám phá, nếu một đối tượng được gặp tại đường dẫn truy cập a có kiểu là một hàm với chữ ký sig, thì một cặp ⟨a,sig⟩ được ghi lại (dòng 10). Nếu kiểu của p là một đối tượng, thì các đối tượng được tham chiếu bởi các thuộc tính của nó được khám phá đệ quy (dòng 15-15), và nếu kiểu của p là một mảng, thì các thuộc tính của p cũng được khám phá đệ quy (dòng 17-19).

Tạo Test: Thuật toán 2 cho thấy pseudo-code cho bước tạo test. Thuật toán bắt đầu bằng cách khởi tạo tập prompts của các prompt được tạo, tập tests của các test passing được tạo, và tập seen chứa tất cả các test được tạo thành tập rỗng và bằng cách sử dụng Thuật toán 1 để có được tập apis của các cặp (đường dẫn truy cập, chữ ký) tạo thành API của gói (dòng 2-5). Sau đó, trên dòng 6-7, đối với mỗi cặp như vậy, một prompt cơ sở được xây dựng và thêm vào prompts, chỉ chứa đường dẫn truy cập và chữ ký, sử dụng template được minh họa trong Hình 1. Tiếp theo, dòng 9-27 tạo các prompt bổ sung bằng cách thêm nội dung hàm, các đoạn sử dụng ví dụ, và comment tài liệu được trích xuất từ mã vào các prompt được tạo trước đó. Ở đây, hàm refine mở rộng một prompt được tạo trước đó bằng cách thêm nội dung hàm, đoạn ví dụ, hoặc doc comment. Thứ tự mà mỗi loại thông tin, nếu được bao gồm, xuất hiện trong prompt được cố định như sau: đoạn ví dụ, thông báo lỗi từ test được tạo trước đó, doc comment, nội dung hàm, chữ ký.

Vòng lặp while trên dòng 29-44 mô tả một quá trình lặp để tạo test tiếp tục miễn là còn prompt chưa được xử lý. Trong mỗi lần lặp, một prompt được chọn và loại bỏ từ prompts, và LLM được truy vấn cho completion (dòng 31). Đối với mỗi completion được nhận, một test được xây dựng bằng cách nối prompt và completion (dòng 33) và các vấn đề cú pháp nhỏ được sửa như thêm ký tự '}' thiếu ở cuối test (dòng 34). Hơn nữa, chúng tôi loại bỏ comment từ test để cho phép khử trùng lặp các test chỉ khác nhau ở comment của chúng (dòng 35).

Nếu test kết quả hợp lệ về mặt cú pháp và test tương tự không được gặp trước đó, nó được thực thi (dòng 38). Ngược lại, chúng tôi không thực thi lại nó nhưng vẫn liên kết prompt với test đã thấy trước đó. Nếu test thực thi thành công, chúng tôi thêm nó vào tests (dòng 40). Nếu nó failed (do lỗi assertion, không kết thúc, hoặc vì exception không được bắt), và nếu test không được dẫn xuất từ prompt được xây dựng từ test bị lỗi trước đó (dòng 42), thì chúng tôi tạo một prompt mới chứa test bị lỗi và thông báo lỗi và thêm nó vào prompts.

Khi quá trình lặp kết thúc, tập tests được trả về (dòng 45).

2.3 Ví dụ
Để làm cho cuộc thảo luận cụ thể hơn, bây giờ chúng tôi sẽ cho thấy hai ví dụ về cách TESTPILOT tạo test.

Như ví dụ đầu tiên, chúng tôi xem xét gói npm countries-and-timezones.2 Khám phá API tiết lộ rằng gói này export một hàm getCountry với một tham số id duy nhất và tệp README.md của dự án cung cấp một ví dụ sử dụng.

Hình 3(a) cho thấy một test cho hàm này được tạo từ prompt được làm nổi bật ban đầu chỉ bao gồm chữ ký hàm, nhưng không có metadata khác. Test này bị lỗi khi thực thi đến assertion trên dòng 8 vì biểu thức country.name đánh giá thành "United States of America", khác với giá trị "United States" được mong đợi bởi assertion.

2. Xem https://www.npmjs.com/package/countries-and-timezones.

Tiếp theo, chúng tôi tinh chỉnh prompt này để bao gồm đoạn sử dụng như được hiển thị trong phần được làm nổi bật của Hình 3(b). Điều này cho phép LLM tạo ra một test kết hợp thông tin được cung cấp trong đoạn này, mà pass khi được thực thi.

Chúng tôi cho thấy một ví dụ khác trong Hình 4 từ quill-delta,3 một gói để đại diện và thao tác các thay đổi đối với tài liệu. Như trước, Hình 4(a) cho thấy prompt ban đầu cho phương thức concat của quill-delta, gộp hai tập thay đổi, và một test được tạo từ prompt này. Đáng chú ý là LLM có thể tạo ra một test đúng về mặt cú pháp cho quill-delta, nơi các tham số như

```
[{ insert: 'Hello '},
 { insert: ' ', attributes: { bold: true } },
 { insert: 'World! '}]
```

được truyền cho constructor ngay cả khi không có bất kỳ ví dụ sử dụng nào. Rất có thể, điều này là do quill-delta là một gói phổ biến với hơn 1,2 triệu lượt tải xuống hàng tuần, có nghĩa là LLM có khả năng đã thấy các ví dụ sử dụng nó trong tập đào tạo của mình.

3. Xem https://github.com/quilljs/delta.

--- TRANG 4 ---
Tuy nhiên, test trong Hình 4(a) bị lỗi vì khi đến assertion trên dòng 16 delta3.ops.length có giá trị 5, trong khi assertion mong đợi giá trị 6. Lý do cho việc assertion bị lỗi là thực tế rằng phương thức concat gộp các phần tử liền kề nếu chúng có cùng thuộc tính. Do đó, khi thực thi đến dòng 16, mảng delta3.ops sẽ giữ giá trị sau:

```
[
  { insert: 'Hello '},
  { insert: ' ', attributes: { bold: true } },
  { insert: 'World!Hello '},
  { insert: ' ', attributes: { bold: true } },
  { insert: 'World! '}
]
```

và do đó delta3.ops.length sẽ có giá trị 5.

Để đáp ứng với lỗi này, Prompt Refiner sẽ tạo prompt được hiển thị trong Hình 4(b) từ đó một test passing được tạo. Trong test này, giá trị mong đợi trong assertion đã được cập nhật thành 5, theo thông báo lỗi assertion. Lưu ý rằng tất cả các test này trông khá tự nhiên và tương tự như các test mà một nhà phát triển con người có thể viết, và chúng thực hiện các tình huống sử dụng điển hình (thay vì các trường hợp biên) của các hàm cần test.

3 CÂU HỎI NGHIÊN CỨU & THIẾT LẬP ĐÁNH GIÁ

3.1 Câu hỏi Nghiên cứu
Đánh giá của chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau.

RQ1 Các test được tạo bởi TESTPILOT đạt được bao nhiêu độ phủ statement và branch? Lý tưởng nhất, các test được tạo sẽ đạt được độ phủ cao để đảm bảo rằng hầu hết chức năng của API được thực hiện. Cho rằng mục tiêu của chúng tôi là tạo ra các bộ unit test hoàn chỉnh (thay vì tìm bug), chúng tôi đo độ phủ statement chỉ cho các test passing. Chúng tôi báo cáo độ phủ trên cả cấp độ gói và cấp độ hàm.

RQ2 Độ phủ của TESTPILOT so sánh như thế nào với Nessie [11]? Chúng tôi so sánh độ phủ của TESTPILOT với bộ tạo test JavaScript tiên tiến, Nessie, sử dụng phương pháp feedback-directed.

RQ3 Bao nhiêu test được tạo bởi TESTPILOT chứa assertion không tầm thường? Một test không có assertion hoặc với assertion tầm thường như assert.equal(true, true) vẫn có thể đạt được độ phủ cao. Tuy nhiên, các test như vậy không cung cấp oracle hữu ích. Chúng tôi kiểm tra các test được tạo và đo mức độ phổ biến của assertion không tầm thường.

RQ4 Các đặc điểm của các test bị lỗi của TESTPILOT là gì? Chúng tôi điều tra lý do đằng sau bất kỳ test được tạo bị lỗi nào.

RQ5 Mỗi loại thông tin khác nhau được bao gồm trong prompt đóng góp như thế nào vào hiệu quả của các test được tạo bởi TESTPILOT? Để điều tra xem tất cả thông tin được bao gồm trong prompt thông qua các refiner có cần thiết để tạo ra các test hiệu quả hay không, chúng tôi vô hiệu hóa mỗi refiner và báo cáo cách nó ảnh hưởng đến kết quả.

RQ6 Các test được tạo bởi TESTPILOT có được sao chép từ các test hiện có không? Vì gpt3.5-turbo được đào tạo trên mã GitHub, rất có thể LLM đã thấy các test của các gói đánh giá của chúng tôi trước đó và có thể chỉ đơn giản tạo ra các bản sao của các test mà nó "ghi nhớ". Chúng tôi điều tra độ tương tự giữa các test được tạo và bất kỳ test hiện có nào trong các gói đánh giá của chúng tôi.

RQ7 Độ phủ của các test được tạo bởi TESTPILOT phụ thuộc bao nhiêu vào LLM cơ bản? Để hiểu tính tổng quát của phương pháp tạo test dựa trên LLM và hiệu ứng của LLM cơ bản mà TESTPILOT dựa vào, chúng tôi so sánh độ phủ chúng tôi có được sử dụng gpt3.5-turbo với hai LLM khác: (1) mô hình code-cushman-002 của OpenAI [48], một trong những người tiền nhiệm của gpt3.5-turbo thuộc bộ LLM Codex [52] và đã phục vụ như mô hình chính đằng sau bản phát hành đầu tiên của GitHub Copilot [38], và (2) StarCoder [49], một LLM có sẵn công khai mà quá trình đào tạo được ghi chép đầy đủ.

3.2 Thiết lập Đánh giá
Để trả lời các câu hỏi nghiên cứu trên, chúng tôi sử dụng một benchmark của 25 gói npm. Bảng 1 cho thấy kích thước và số lượt tải xuống (độ phổ biến) của mỗi gói này. 10 gói đầu tiên được hiển thị trong bảng là cùng các gói được lưu trữ trên GitHub được sử dụng để đánh giá Nessie [11], một kỹ thuật tạo test feedback-directed gần đây cho JavaScript. Tuy nhiên, chúng tôi nhận thấy rằng 10 gói này chủ yếu tập trung vào các thư viện I/O phổ biến với style nặng về callback, vì vậy chúng tôi thêm 10 gói mới từ các lĩnh vực khác nhau (ví dụ: xử lý tài liệu và cấu trúc dữ liệu), style lập trình (chủ yếu là hướng đối tượng), cũng như các gói ít phổ biến hơn. Vì gpt3.5-turbo (cũng như các LLM khác mà chúng tôi thí nghiệm trong RQ7) được đào tạo trên các kho GitHub, chúng tôi phải giả định rằng tất cả các gói chủ đề của chúng tôi (và đặc biệt là các test của chúng) là một phần của tập đào tạo của mô hình. Vì lý do này, chúng tôi cũng bao gồm thêm 5 gói có mã nguồn được lưu trữ trên GitLab.4

Bảng 1 cho thấy 25 gói khác nhau về độ phổ biến (tải xuống/tuần) và kích thước (LOC), cũng như về số lượng hàm API mà chúng cung cấp và mức độ tài liệu có sẵn. Các cột "API functions" cho thấy số lượng hàm API có sẵn; số lượng và tỷ lệ các hàm API có ít nhất một đoạn mã ví dụ trong tài liệu ("w/ examples"); và số lượng và tỷ lệ các hàm API có comment tài liệu ("w/ comment"). Chúng tôi cũng cho thấy tổng số đoạn ví dụ có sẵn trong tài liệu của mỗi gói.

Để trả lời RQ1–RQ6, chúng tôi chạy TESTPILOT sử dụng LLM gpt3.5-turbo (phiên bản gpt-3.5-turbo-0301), lấy mẫu năm completion tối đa 100 token ở nhiệt độ zero,5 với tất cả các tùy chọn khác ở giá trị mặc định của chúng. Trong RQ7, chúng tôi sử dụng cùng cài đặt cho code-cushman-002 và StarCoder, ngoại trừ nhiệt độ lấy mẫu cho cái sau là 0,01 vì nó không hỗ trợ nhiệt độ zero.

Lưu ý rằng tạo test dựa trên LLM không có ngân sách tạo test per se vì nó không phải là một quá trình vô hạn. Thay vào đó, chúng tôi yêu cầu LLM tối đa năm completion cho mỗi prompt (nhưng mô hình có thể trả về ít hơn). Chúng tôi khử trùng lặp các test được trả về để tránh làm tăng số lượng test được tạo. Ví dụ, nếu hai prompt trả về cùng một test (modulo comment), chúng tôi chỉ ghi lại test này một lần nhưng theo dõi prompt nào dẫn đến việc tạo ra nó.

Mặc dù chúng tôi đặt nhiệt độ lấy mẫu thấp nhất có thể, vẫn có một số tính không xác định trong các phản hồi nhận được. Theo đó, chúng tôi chạy tất cả các thí nghiệm 10 lần. Tất cả các điểm dữ liệu per-package được báo cáo trong Phần 4 là giá trị trung vị trong 10 lần chạy này, ngoại trừ dữ liệu kiểu integer như số lượng test, chúng tôi sử dụng ceiling của giá trị trung vị. Đối với RQ6, không mất tính tổng quát, chúng tôi trình bày các số tương tự dựa trên lần chạy đầu tiên.

Chúng tôi sử dụng Istanbul/nyc [53] để đo độ phủ statement và branch và sử dụng giới hạn thời gian mặc định 2s per test của Mocha.

4. Chúng tôi đã kiểm tra các repo có tên tương tự để đảm bảo rằng chúng không được mirror trên GitHub.
5. Về mặt trực quan, nhiệt độ lấy mẫu kiểm soát tính ngẫu nhiên của các completion được tạo, với nhiệt độ thấp hơn có nghĩa là ít tính không xác định hơn. Các mô hình ngôn ngữ mã hóa đầu vào và đầu ra của chúng sử dụng từ vựng token, với các chuỗi ký tự xảy ra thường xuyên (như require, nhưng cũng các run liền kề của ký tự khoảng trắng) được đại diện bởi một token duy nhất.

--- TRANG 5 ---
4 KẾT QUẢ ĐÁNH GIÁ

4.1 RQ1: Độ phủ của TESTPILOT
Bảng 2 cho thấy số lượng test mà TESTPILOT tạo cho mỗi gói, số lượng (và tỷ lệ) test passing, và độ phủ tương ứng đạt được bởi các test passing. Hai cột đầu tiên của Bảng 2 cũng cho thấy độ phủ có được bằng cách chỉ đơn giản load gói (loading coverage). Đây là độ phủ chúng ta có "miễn phí" mà không có bất kỳ bộ test nào, mà chúng tôi cung cấp như một điểm tham chiếu để diễn giải kết quả của chúng tôi. Nhìn chung, 9.9%–80.0% các test được tạo bởi TESTPILOT là test passing, với trung vị 48.0% trên tất cả các gói. Bây giờ chúng tôi thảo luận về các phép đo độ phủ khác nhau của những test passing này.

Độ phủ Statement: Độ phủ statement per gói đạt được bởi các test passing dao động từ 33.9% đến 93.1%, với trung vị 70.2%. Chúng tôi lưu ý rằng trên tất cả các gói, độ phủ statement đạt được cao hơn nhiều so với loading coverage với chênh lệch 19.1%–88.2% và chênh lệch trung vị 53.7%.6

Độ phủ statement thấp nhất mà TESTPILOT đạt được là trên js-sdsl, ở mức 33.9%. Khi điều tra sâu hơn gói này, chúng tôi thấy rằng nó duy trì các ví dụ tài liệu xuất hiện trên trang web của nó dưới dạng các tệp markdown trong một kho riêng biệt.7 Bao gồm các đoạn ví dụ được trích xuất từ kho ngoài này làm tăng độ phủ đạt được lên 43.6%, điều này cho thấy tầm quan trọng của việc bao gồm các ví dụ sử dụng trong prompt. Chúng tôi kiểm tra hiệu ứng của thông tin được bao gồm trong prompt chi tiết trong RQ5 (Phần 4.5).

Đáng chú ý là độ phủ của TESTPILOT cho các dự án GitLab được liệt kê trong 5 hàng cuối của Bảng 2 dao động từ 51.4% đến 78.3%. Điều này chứng minh rằng TESTPILOT hiệu quả trong việc tạo ra các unit test có độ phủ cao cho các gói mà nó chưa thấy trong tập đào tạo của mình.

Độ phủ Branch: Chúng tôi cũng cho thấy độ phủ branch đạt được bởi các test passing trong Bảng 2. Chúng tôi thấy rằng độ phủ branch per gói là giữa 16.5% và 71.3%, với trung vị 52.8%. Tương tự như độ phủ statement, độ phủ branch đạt được cũng cao hơn nhiều so với loading coverage với chênh lệch 15.9%–71.3% và chênh lệch trung vị 50.0%.

Vì đạt được độ phủ branch thường khó hơn so với đạt được độ phủ statement, có thể mong đợi rằng độ phủ branch cho các test được tạo thấp hơn độ phủ statement. Tuy nhiên, chúng tôi lưu ý một trường hợp thú vị trong gitlab-js nơi sự khác biệt này có vẻ rõ rệt hơn (51.7% vs. 16.5%). Khi điều tra sâu hơn mã nguồn và tài liệu của nó, chúng tôi thấy rằng gitlab-js cung cấp nhiều tùy chọn cấu hình và tham số khác nhau để chỉ định kho GitLab để kết nối và sử dụng/truy vấn (ví dụ: url của nó, token xác thực, tham số tìm kiếm để sử dụng cho truy vấn). Việc xử lý các tùy chọn này được phản ánh trong logic branching chính trong mã. Mặc dù TESTPILOT có cố gắng tạo ra các test hợp lý gọi các endpoint khác nhau với các tùy chọn khác nhau, đôi khi nó gặp khó khăn trong việc tìm lời gọi hàm đúng để sử dụng, dẫn đến lỗi kiểu. Nói chung, một tỷ lệ lớn các test mà TESTPILOT tạo cho gói này bị lỗi, và do đó không đóng góp vào các số độ phủ của chúng tôi. Cũng đáng chú ý rằng việc test đúng cách gói như vậy sẽ yêu cầu mocking, nhưng chúng tôi không quan sát bất kỳ test được tạo nào sử dụng mocking. Trong tương lai, sẽ thú vị khi điều tra xem việc bao gồm các thư viện mocking trong prompt, hoặc thông tin liên quan đến mocking khác, có thể dẫn đến việc mô hình sử dụng mocking khi cần thiết hay không.

Độ phủ per function: Hình 5 cho thấy phân phối độ phủ statement per function cho mỗi gói. Mỗi box tương ứng với một trong các gói benchmark của chúng tôi và mỗi điểm dữ liệu trong một box đại diện cho độ phủ statement cho một hàm trong gói đó. Độ phủ statement trung vị per function cho mỗi gói được hiển thị bằng màu đỏ. Nhìn chung, độ phủ statement trung vị per function cho một dự án cho trước dao động từ 0.0%–100.0%, với trung vị 77.1%. Để đảm bảo rằng TESTPILOT không tạo ra các test có độ phủ cao chỉ cho các hàm nhỏ hơn, chúng tôi chạy test tương quan Pearson giữa độ phủ statement per function và kích thước hàm tương ứng (trong statement). Chúng tôi thấy không có tương quan có ý nghĩa thống kê giữa độ phủ và kích thước, cho thấy TESTPILOT không chỉ làm tốt cho các hàm nhỏ hơn.8

Như mong đợi, Hình 5 cho thấy rằng đối với hầu hết các gói, TESTPILOT làm tốt cho một số hàm trong khi đạt được độ phủ thấp cho những hàm khác. Hãy lấy jsonfile làm ví dụ. Trong Bảng 2, chúng ta thấy rằng độ phủ statement ở cấp gói của nó là 38.3%. Từ Hình 5, chúng ta thấy rằng độ phủ statement per function dao động từ 0% đến 100%, với trung vị gần 50%. Đi sâu vào dữ liệu, chúng tôi thấy rằng có hai hàm mà TESTPILOT không thể phủ, vì các test được tạo tương ứng của chúng bị lỗi hoặc do tham chiếu đến các tệp không tồn tại mà TESTPILOT bao gồm trong test hoặc vì chúng timeout. Tuy nhiên, các hàm mà TESTPILOT có thể phủ có độ phủ statement dao động từ 58%-100%. Chúng ta có thể quan sát hành vi tương tự với các gói phụ thuộc file system khác, như graceful-fs hoặc fs-extra. Ở đầu kia của phổ, chúng ta thấy zip-a-folder nơi TESTPILOT đạt được cả độ phủ statement cao ở cấp gói (84%) cũng như độ phủ statement cao ở cấp hàm trong Hình 5 nơi độ phủ tối thiểu per function là 75%.

Test Đóng góp Duy nhất: Để hiểu rõ hơn về sự đa dạng của các test được tạo, Bảng 2 cũng cho thấy có bao nhiêu test mà TESTPILOT tạo ra là đóng góp duy nhất, có nghĩa là chúng phủ ít nhất một statement mà không có test nào khác phủ. Trung vị 10.5% các test passing thuộc loại này, với một số gói cao tới 100.0%. Những kết quả này đầy hứa hẹn vì chúng cho thấy TESTPILOT có thể tạo ra các test phủ các trường hợp biên, nhưng rõ ràng có một số redundancy giữa các test được tạo. Tất nhiên, chúng ta không thể đơn giản loại trừ tất cả 89.5% test còn lại mà không mất độ phủ, vì một số statement có thể được phủ bởi nhiều test không duy nhất. Khám phá các kỹ thuật tối thiểu hóa bộ test [54] để giảm kích thước bộ test được tạo là một hướng thú vị cho công việc tương lai.

4.2 RQ2 TESTPILOT vs. Nessie
Chúng tôi so sánh độ phủ của TESTPILOT với bộ tạo test JavaScript tiên tiến Nessie [11], sử dụng phương pháp feedback-directed truyền thống.9 Đối với mỗi gói, Nessie tạo 1000 test, mà chúng tôi đo độ phủ statement và branch theo cách tương tự như TESTPILOT. Sau đó chúng tôi lặp lại các phép đo này 10 lần và lấy độ phủ trung vị trên 10 lần chạy để theo một thiết lập tương tự với đánh giá của TESTPILOT. Chúng tôi sử dụng test Wilcoxon paired rank-sum để xác định xem có sự khác biệt có ý nghĩa thống kê giữa độ phủ đạt được bởi cả hai công cụ hay không.

Hai cột cuối của Bảng 2 cho thấy độ phủ statement và branch cho Nessie. Chúng tôi lưu ý rằng Nessie không thể chạy trên uneval, vì export duy nhất của module là một hàm, mà Nessie không hỗ trợ. Đối với 24 gói còn lại, Nessie đạt được 4.7%–96.0% độ phủ statement, với trung vị 51.3%. Ngược lại, như được hiển thị trong Bảng 2, độ phủ statement trung vị của TESTPILOT cao hơn nhiều ở mức 70.2%. Sự khác biệt trong độ phủ branch thậm chí còn cao hơn, với 52.8% cho TESTPILOT vs 25.6% cho Nessie. Cả hai sự khác biệt này đều có ý nghĩa thống kê (p-values 0.002 và 0.027 tương ứng) với effect size lớn, được đo bằng Cliff's delta [55], là 0.493 cho độ phủ statement và một effect size trung bình (0.431) cho độ phủ branch.10 Lưu ý rằng Nessie luôn tạo 1000 test per gói, trong khi TESTPILOT thường tạo ít test hơn nhiều, ngoại trừ trên memfs và omnitool. Cũng đáng nhấn mạnh rằng Nessie (và các kỹ thuật tạo test khác như LambdaTester [56]) báo cáo độ phủ của tất cả test được tạo, bất kể chúng pass hay fail trong khi các số độ phủ được báo cáo của chúng tôi chỉ dành cho các test passing.

Bây giờ chúng tôi đi sâu vào kết quả ở cấp gói. Đối với mỗi gói, Bảng 2 làm nổi bật độ phủ cao hơn từ hai kỹ thuật bằng chữ đậm. TESTPILOT vượt trội hơn Nessie trên 17 trong số 24 gói (glob, fs-extra, bluebird, q, rsvp, memfs, js-sdsl, quill-delta, complex.js, pull-stream, simple-statistics, plural, dirty, geo-point, image-downloader, core, omnitool), tăng độ phủ từ 3.6%–74.5%, với trung vị tăng 30.0%. Đối với 7 gói còn lại (graceful-fs, jsonfile, node-dir, zip-a-folder, countries-and-timezones, crawler-url-parser, gitlab-js), TESTPILOT đạt được độ phủ thấp hơn Nessie. Đối với những gói này, nó giảm độ phủ từ 0.5%–53.2%, với trung vị giảm 3.6%. Chúng tôi cũng lưu ý rằng Nessie không đạt được bất kỳ độ phủ branch nào trên 3 dự án (dirty, geo-point, core), trong khi độ phủ statement cho những dự án này là khác không. Khi kiểm tra sâu hơn, và sau khi tham khảo ý kiến các tác giả Nessie, chúng tôi thấy rằng Nessie không thể tạo ra các test instantiate class, có nghĩa là độ phủ statement chỉ vừa vượt quá loading coverage cho các gói với API dựa trên class, trong khi độ phủ branch là zero.

Ngoài sự khác biệt trong độ phủ đạt được bởi Nessie và TESTPILOT, các test được tạo bởi Nessie có xu hướng trông khá khác so với những test được tạo bởi TESTPILOT, bắt nguồn từ phương pháp ngẫu nhiên của Nessie trong tạo test. Để minh họa điều này, Hình 6 cho thấy một ví dụ về test được tạo bởi Nessie thực hiện hàm getCountry của countries-and-timezones. Như có thể thấy trong hình, test sử dụng các tên biến dài như ret_val_manuelmhtr_countries_and_timezones_1 cản trở khả năng đọc. Hơn nữa, test gọi getCountry trên dòng 13–17 với một object literal gắn giá trị ngẫu nhiên với một số thuộc tính được đặt tên ngẫu nhiên, điều này không phản ánh việc sử dụng dự định của API. Hơn nữa, các test được tạo bởi Nessie không chứa bất kỳ assertion nào. Ngược lại, các test được tạo bởi TESTPILOT cho cùng gói (xem Hình 3) thường sử dụng tên biến tương tự như những cái được chọn bởi các lập trình viên, gọi API với các giá trị hợp lý, và thường chứa assertion.

4.3 RQ3: Assertion Không tầm thường
Chúng tôi định nghĩa một assertion không tầm thường là một assertion phụ thuộc vào ít nhất một hàm từ gói cần test. Để xác định assertion không tầm thường, trước tiên chúng tôi sử dụng CodeQL [57] để tính toán program slice ngược từ mỗi assertion trong các test được tạo. Chúng tôi xem xét các assertion có backwards slice chứa import của gói cần test là assertion không tầm thường. Sau đó chúng tôi báo cáo các test được tạo chứa ít nhất một assertion không tầm thường.

Bảng 3 cho thấy số lượng test với assertion không tầm thường (test không tầm thường tóm lại) và tỷ lệ của chúng w.r.t tất cả test được tạo từ Bảng 2. Bảng cũng cho thấy số lượng và tỷ lệ của những test này pass, cùng với độ phủ statement mà chúng đạt được.

Chúng tôi quan sát rằng chỉ có một gói, image-downloader nơi TESTPILOT chỉ tạo ra test tầm thường. Mặc dù các test được tạo cho image-downloader có bao gồm các lời gọi đến API của nó, tất cả đều thiếu statement assert. Trên các gói còn lại, trung vị 9.1%–94.6% các test được tạo bởi TESTPILOT per gói là không tầm thường. Trung vị 61.4% các test được tạo cho một gói cho trước là không tầm thường. Khi so sánh với tất cả test được tạo, chúng ta cũng có thể thấy rằng chỉ một tỷ lệ thấp hơn một chút của test không tầm thường pass (trung vị 48.0% cho test passing tổng thể từ Bảng 2 vs. 43.7% cho test passing không tầm thường từ Bảng 3). Cả hai kết quả này cho thấy TESTPILOT thường tạo ra các test với assertion thực hiện chức năng từ gói đích.

Độ phủ đạt được bởi các test không tầm thường cũng hỗ trợ phát hiện này. Cụ thể, khi so sánh độ phủ statement cho tất cả test được tạo trong Bảng 2 với độ phủ cho test không tầm thường trong Bảng 3, chúng tôi thấy rằng sự khác biệt dao động từ 0.0%–84.0%, với chênh lệch trung vị chỉ 7.5%. Điều này có nghĩa là độ phủ đạt được cho hầu hết các gói chủ yếu đến từ việc thực hiện chức năng API được test bởi các oracle được tạo. Tuy nhiên chúng tôi lưu ý rằng có 4 gói (jsonfile, node-dir, zip-a-folder, image-downloader) nơi test không tầm thường đạt được 0% độ phủ statement, gây ra sự khác biệt lớn hơn. Ngoài image-downloader được thảo luận ở trên, ba gói còn lại không có bất kỳ test không tầm thường passing nào. Vì chúng tôi tính độ phủ chỉ cho test passing, điều này dẫn đến 0% độ phủ statement cho test không tầm thường.

4.4 RQ4: Đặc điểm của Test Bị lỗi
Hình 7 cho thấy số lượng test bị lỗi cho mỗi gói, cùng với phân tích lý do đằng sau việc thất bại. Lỗi assertion xảy ra khi giá trị mong đợi trong assertion không khớp với giá trị thực tế từ việc thực thi mã. Lỗi file-system bao gồm các lỗi như tệp hoặc thư mục không được tìm thấy, mà chúng tôi xác định bằng cách kiểm tra các mã lỗi liên quan đến file-system [58] trong stack trace lỗi. Lỗi correctness bao gồm tất cả lỗi kiểu, lỗi cú pháp, lỗi tham chiếu, gọi done không đúng, và lỗi đệ quy vô hạn/call stack. Lỗi timeout xảy ra khi test vượt quá thời gian chạy tối đa mà chúng tôi cho phép chúng (2s/test). Cuối cùng, chúng tôi nhóm tất cả các lỗi cụ thể ứng dụng khác mà chúng tôi quan sát dưới Other.

Chúng tôi thấy rằng lý do thất bại phổ biến nhất là timeout với trung vị 22.7% test bị lỗi, theo sau là lỗi correctness (đặc biệt là lỗi kiểu) với trung vị 20.0% test bị lỗi. Phần lớn timeout là do thiếu lời gọi done, khiến Mocha tiếp tục chờ đợi lời gọi. Chúng tôi lưu ý rằng trung bình, RetryWithError refiner có thể sửa 15.4% những lỗi timeout như vậy, với mô hình thường chỉ đơn giản thêm lời gọi done11.

Chúng tôi thấy rằng trung vị 19.2% thất bại là lỗi assertion, cho thấy trong một số trường hợp gpt3.5-turbo không thể tìm ra giá trị mong đợi đúng cho test oracle. Điều này đặc biệt đúng khi gói cần test không được sử dụng rộng rãi và không có thông tin nào chúng tôi cung cấp cho mô hình có thể giúp nó tìm ra các giá trị đúng. Ví dụ, trong một trong các test cho geo-point, TESTPILOT có thể sử dụng tọa độ trong đoạn ví dụ được cung cấp để xây dựng đúng hai tọa độ địa lý làm đầu vào cho hàm calculateDistance, tính khoảng cách giữa hai tọa độ. Tuy nhiên, TESTPILOT tạo không đúng 131.4158102876726 làm giá trị mong đợi cho khoảng cách giữa hai điểm này, trong khi giá trị mong đợi đúng là 130584.05017990958; điều này khiến test bị lỗi với lỗi assertion. Chúng tôi lưu ý rằng trong trường hợp cụ thể này, khi TESTPILOT re-prompt mô hình với test bị lỗi và thông báo lỗi, sau đó nó có thể tạo ra test passing với oracle được sửa. Trung bình trên các gói, chúng tôi thấy rằng RetryWithError refiner có thể sửa 11.1% lỗi assertion.

Cuối cùng, chúng tôi lưu ý rằng lỗi file-system là domain specific. Các test được tạo cho các gói trong lĩnh vực file system, như fs-extra hoặc memfs, có tỷ lệ cao test bị lỗi do những lỗi như vậy. Điều này không đáng ngạc nhiên cho rằng những test này có thể dựa vào các tệp có thể không tồn tại hoặc yêu cầu chứa nội dung cụ thể. Các gói trong các lĩnh vực khác không gặp vấn đề này.

Nhìn chung, chúng tôi thấy rằng re-prompt mô hình với thông báo lỗi của test bị lỗi (bất kể lý do thất bại) cho phép TESTPILOT tạo ra test passing tiếp theo trong 15.6% các trường hợp.

4.5 RQ5: Hiệu ứng của Prompt Refiner
Kết quả của chúng tôi cho đến nay bao gồm test được tạo với tất cả bốn prompt refiner được thảo luận trong Phần 2. Trong RQ này, chúng tôi điều tra hiệu ứng của mỗi refiner này đối với chất lượng của các test được tạo. Cụ thể, chúng tôi tiến hành nghiên cứu ablation trong đó chúng tôi vô hiệu hóa một refiner tại một thời điểm. Vô hiệu hóa một refiner có nghĩa là chúng tôi không còn tạo prompt bao gồm thông tin mà nó cung cấp. Ví dụ, vô hiệu hóa DocCommentIncluder có nghĩa là không có prompt nào chúng tôi tạo sẽ chứa comment tài liệu. Sau đó chúng tôi so sánh tỷ lệ test passing, độ phủ đạt được, cũng như độ phủ bởi test không tầm thường (độ phủ không tầm thường).

Hình 8 cho thấy kết quả của chúng tôi. Trục x cho thấy các metric chúng tôi so sánh trên các cấu hình khác nhau được hiển thị trong legend. Trục y cho thấy giá trị cho mỗi metric (tất cả phần trăm). Mỗi điểm dữ liệu trong boxplot đại diện cho kết quả của metric cụ thể cho một gói cho trước, sử dụng cấu hình refiner tương ứng. Đường đen ở giữa mỗi box đại diện cho giá trị trung vị cho mỗi metric trên tất cả các gói. Cấu hình full là cấu hình chúng tôi trình bày cho đến nay (tức là tất cả refiner được bật). Các cấu hình khác cho thấy kết quả của việc loại trừ chỉ một trong các refiner. Ví dụ, red box plot cho thấy kết quả khi vô hiệu hóa SnippetIncluder (tức là Without Example Snippets). Cấu hình base prompt chứa chỉ chữ ký hàm và test scaffolding (tức là vô hiệu hóa tất cả refiner). Lưu ý, tuy nhiên, rằng chỉ 8 trong số các gói trong đánh giá của chúng tôi chứa comment tài liệu. Không có ý nghĩa gì khi so sánh hiệu ứng của việc vô hiệu hóa DocCommentIncluder trên các gói không chứa doc comment ngay từ đầu. Do đó, trong khi các phân phối được hiển thị trong tất cả boxplot đại diện cho 25 gói, cấu hình Without Doc Comments chứa dữ liệu cho chỉ 8 gói.

Nhìn chung, chúng ta có thể thấy rằng cấu hình full vượt trội hơn tất cả các cấu hình khác, trên tất cả ba metric, ngụ ý rằng tất cả thông tin prompt mà chúng tôi bao gồm đóng góp vào việc tạo ra các test hiệu quả hơn. Chúng tôi thấy rằng không có một gói nào mà việc vô hiệu hóa refiner dẫn đến kết quả tốt hơn trên bất kỳ metric nào. Ngoại trừ 4 gói mà việc vô hiệu hóa một trong các refiner không ảnh hưởng đến kết quả (SnippetIncluder trên crawler-url-parser và dirty; và RetryWithError trên gitlab-js và zip-a-folder), việc vô hiệu hóa refiner luôn dẫn đến giá trị thấp hơn trong ít nhất một metric.

Đóng góp của các refiner đặc biệt đáng chú ý đối với tỷ lệ test passing nơi việc vô hiệu hóa bất kỳ refiner nào (ví dụ: FnBodyIncluder hoặc SnippetIncluder) dẫn đến sụt giảm lớn tỷ lệ test passing. Điều này cho thấy các refiner hiệu quả trong việc hướng dẫn mô hình tạo ra nhiều test passing hơn, ngay cả khi điều này không nhất thiết dẫn đến độ phủ bổ sung. Chúng tôi thấy rằng trên tất cả gói, cấu hình full luôn dẫn đến tỷ lệ test passing cao hơn cho một API cho trước, trong khi duy trì độ phủ cao.

Để hiểu xem sự khác biệt giữa các phân phối chúng tôi quan sát trong Hình 8 có ý nghĩa thống kê hay không, chúng tôi so sánh kết quả của mỗi cặp cấu hình cho tất cả ba metric sử dụng test Wilcoxon matched pairs signed rank. Lưu ý rằng khi so sánh với DocCommentIncluder, chúng tôi so sánh phân phối cho chỉ 8 gói chứa doc comment.

Chúng tôi thấy sự khác biệt có ý nghĩa thống kê giữa cấu hình full và mỗi cấu hình vô hiệu hóa bất kỳ refiner nào cũng như giữa cấu hình base và mỗi cấu hình khác. So với cấu hình full, effect size lớn nhất chúng tôi quan sát cho việc vô hiệu hóa refiner là trên test passing khi FnBodyIncluder hoặc DocCommentIncluder bị vô hiệu hóa (Cliff's delta 0.582 và 0.531 tương ứng).

Ngoài sự khác biệt với cấu hình full và base, chúng tôi thấy không có sự khác biệt có ý nghĩa thống kê giữa các cặp cấu hình khác ngoại trừ các trường hợp sau: Chúng tôi thấy rằng cho cả test passing và coverage, có sự khác biệt có ý nghĩa thống kê giữa cấu hình vô hiệu hóa FnBodyIncluder và cấu hình vô hiệu hóa RetryWithError (effect size trung bình và không đáng kể, tương ứng). Đối với test passing, chúng tôi cũng thấy sự khác biệt có ý nghĩa thống kê giữa việc vô hiệu hóa FnBodyIncluder và vô hiệu hóa mỗi SnippetIncluder và DocCommentIncluder (effect size nhỏ và trung bình, tương ứng). Tuy nhiên, chúng tôi lưu ý rằng kích thước mẫu 8 quá nhỏ để rút ra bất kỳ kết luận hợp lệ nào cho DocCommentIncluder. Đặc biệt thú vị khi thấy rằng không có sự khác biệt có ý nghĩa thống kê giữa việc vô hiệu hóa SnippetIncluder và vô hiệu hóa bất kỳ refiner nào khác. Điều này cho thấy việc thiếu đoạn ví dụ không nhất thiết ảnh hưởng đến các metric nhiều hơn so với việc thiếu bất kỳ thông tin nào được cung cấp bởi các refiner khác. Vì Hình 8 cho thấy rằng chúng tôi vẫn có được độ phủ trung vị cao ngay cả khi vô hiệu hóa SnippetIncluder, điều này cho thấy sự hiện diện của đoạn ví dụ không cần thiết để tạo ra các bộ test hiệu quả với độ phủ cao, và TESTPILOT có thể áp dụng ngay cả trong các trường hợp không có ví dụ tài liệu nào.

Cuối cùng, chúng tôi lưu ý rằng mặc dù kết quả tổng thể trên một gói cho trước cho thấy các refiner luôn cải thiện, hoặc ít nhất duy trì, độ phủ và tỷ lệ test passing, điều này không có nghĩa là refiner luôn cải thiện kết quả cho một hàm API riêng lẻ. Chúng tôi đã quan sát các tình huống mà việc thêm thông tin như triển khai hàm vào prompt không bao gồm nó làm nhầm lẫn mô hình, dẫn đến việc tạo ra test bị lỗi. Hình 9 cho thấy một ví dụ cho gói complex.js: cho prompt cơ sở ở bên trái, gpt3.5-turbo có thể tạo ra test passing (rất đơn giản) cho phương thức valueOf của hằng số ZERO được export bởi gói; việc thêm nội dung hàm tạo ra prompt ở bên phải, có vẻ làm nhầm lẫn mô hình, dẫn đến việc tạo ra test bị lỗi. Trên tất cả các gói, 5.367 prompt được tạo bằng cách áp dụng một trong các refiner, và chỉ trong 394 trường hợp (7,3%) prompt được tinh chỉnh kém hiệu quả hơn prompt gốc theo nghĩa là test passing được tạo từ prompt gốc, nhưng không từ prompt được tinh chỉnh.

4.6 RQ6: Ghi nhớ
Vì gpt3.5-turbo được đào tạo trên mã GitHub, một số test hiện có được bao gồm trong benchmark của chúng tôi có thể đã là một phần của tập đào tạo của nó. Điều này làm nảy sinh mối lo ngại rằng TESTPILOT có thể đang ghi nhớ các test hiện có, thay vì tạo ra những test mới, hạn chế tính hữu ích của nó cho các gói mà nó không được đào tạo. Để điều tra các hiệu ứng tiềm năng của việc ghi nhớ, chúng tôi đo độ tương tự giữa mỗi test được tạo và các test hiện có trong benchmark (số lượng test hiện có được hiển thị trong Bảng 1). Gần đây, Lemieux et al. [27] báo cáo rằng các kỹ thuật plagiarism hoặc clone detection [59] mã không hiệu quả trong việc xác định việc ghi nhớ mã LLM. Thay vào đó, họ thấy rằng việc đo độ tương tự thông qua edit distance [60] tạo ra kết quả có ý nghĩa hơn. Họ định nghĩa maximum similarity như một metric đo độ tương tự chuẩn hóa cao nhất giữa một test được tạo cho trước và tất cả test hiện có như sau: max tp∈TP 1−dist(t∗,tp) max(len(t∗),len(tp)), nơi TP là tập các hàm test hiện có trong gói, t∗ là test được tạo cho trước, và dist là edit distance giữa test được tạo và test hiện có. Chúng tôi theo cùng phương pháp để tính maximum similarity cho mỗi test được tạo, sử dụng gói npm Levenstein [61] để tính dist.

Hình 10 cho thấy tỷ lệ phần trăm tích lũy của các test case được tạo cho mỗi dự án nơi maximum similarity nhỏ hơn giá trị trên trục x. Chúng tôi cũng cho thấy tỷ lệ phần trăm tích lũy này cho tất cả test case được tạo trên tất cả các dự án. Chúng tôi thấy rằng 6,2% test case được tạo bởi TESTPILOT có maximum similarity ≤0,3% với test hiện có, 60,0% có ≤0,4 similarity, 92,8% có ≤0,5, 99,6% có ≤0,6 trong khi 100,0% test case được tạo có ≤0,7. Điều này có nghĩa là TESTPILOT không bao giờ tạo ra các bản sao chính xác của test hiện có. Ngược lại, trong khi 90% test Python được tạo bởi Lemieux et al. [27] có ≤0,4 similarity, 2% test case của họ là bản sao chính xác. Điều đó nói rằng, cho sự khác biệt giữa framework testing trong Python và JavaScript (ví dụ: Mocha yêu cầu nhiều mã boilerplate hơn pytest), các số tương tự không thể được so sánh trực tiếp giữa hai ngôn ngữ.

Để minh họa thêm các số tương tự kết quả, Hình 11 cho thấy một ví dụ về test case từ bluebird với 0,62 similarity với test case hiện có. Mặc dù edit distance ở đây thấp, dẫn đến similarity cao, chúng ta có thể thấy rằng các test có sự khác biệt về ngữ nghĩa. Ví dụ, test được tạo chỉ đơn giản kiểm tra rằng exception được ném là type error, trong khi test hiện có kiểm tra các giá trị nhất định trong trace. Do đó, 7,2% test case chúng tôi tạo với >0,5 similarity không gây ra mối lo ngại rằng TESTPILOT đang tạo ra các test case được ghi nhớ. Cuối cùng, chúng tôi mong đợi các test được tạo cho các dự án được lưu trữ trên GitLab có similarity thấp hơn với test hiện có vì, theo như chúng tôi biết, tập đào tạo cho các mô hình của OpenAI chỉ bao gồm các dự án từ GitHub, vì vậy mô hình ít có khả năng thấy các test hiện có trong quá trình đào tạo. Kết quả của chúng tôi thực sự cho thấy rằng ba trong số năm dự án có maximum similarity ≤0,4, với hai dự án còn lại có maximum similarity 0,5. Điều này cho chúng tôi tin tưởng rằng metric similarity mà chúng tôi sử dụng cung cấp kết quả có ý nghĩa.

--- TRANG 6 ---
4.7 RQ7: Hiệu ứng của các LLM khác nhau
Bảng 4 cho thấy so sánh độ phủ statement của các test được tạo bởi TESTPILOT sử dụng ba LLM. Đối với mỗi dự án, chúng tôi cho thấy số lượng test được tạo, số lượng (%) test passing, và độ phủ statement đạt được bởi những test passing này. Mặc dù độ phủ riêng lẻ per gói khác nhau, chúng ta có thể thấy rằng độ phủ của test được tạo bởi mô hình code-cushman-002 có thể so sánh với những test được tạo bởi gpt3.5-turbo, với cái sau có độ phủ statement và branch trung vị cao hơn một chút trên các gói. Test Wilcoxon matched-pairs signed-rank cho thấy không có sự khác biệt có ý nghĩa thống kê giữa gpt3.5-turbo và code-cushman-002 cho cả hai loại độ phủ. Mặt khác, chúng tôi thấy sự khác biệt có ý nghĩa thống kê giữa StarCoder và mỗi mô hình OpenAI (p-value <0,05) cho cả hai loại độ phủ. Như được hiển thị trong Bảng 4, StarCoder đạt được độ phủ statement trung vị thấp hơn (54,0%) và độ phủ branch (37,5%) so với cả hai mô hình khác. Cliff's delta [55] cho thấy effect size lớn và trung bình cho độ phủ statement và branch, tương ứng, giữa gpt3.5-turbo và StarCoder và effect size trung bình và nhỏ cho độ phủ statement và branch, tương ứng giữa code-cushman-002 và StarCoder.

Tuy nhiên, chúng tôi lưu ý rằng độ phủ statement trung vị và độ phủ branch của StarCoder đều cao hơn Nessie (statement: 54,0% vs. 51,3% và branch: 37,5% vs 25,6%). Mặc dù độ phủ cao hơn này không có ý nghĩa thống kê, kết quả cho thấy rằng ngay cả LLM được đào tạo với dataset có thể nhỏ hơn và/hoặc quy trình đào tạo khác so với các mô hình của OpenAI cũng ngang bằng (hoặc thậm chí đôi khi cao hơn) các kỹ thuật tạo test truyền thống tiên tiến, như Nessie [11]. Hơn nữa, trong RQ2, chúng tôi cho thấy rằng việc sử dụng gpt3.5-turbo với TESTPILOT dẫn đến các bộ test có độ phủ cao hơn, với sự khác biệt có ý nghĩa thống kê so với Nessie. Nhìn chung, những kết quả này nhấn mạnh triển vọng của các kỹ thuật tạo test dựa trên LLM trong việc tạo ra các bộ test có độ phủ cao.

Cuối cùng, chúng tôi lưu ý rằng thời gian trung vị để TESTPILOT tạo test cho một hàm cho trước sử dụng gpt3.5-turbo là 15s, và thời gian trung vị để tạo một bộ test hoàn chỉnh cho một gói cho trước là 6m 55s.12 Phần lớn thời gian này được dành cho việc truy vấn mô hình, vì vậy việc chọn LLM tạo ra sự khác biệt lớn. Ví dụ, thời gian trung vị để TESTPILOT tạo test cho một hàm cho trước sử dụng StarCoder và code-cushman-002 là 24s và 11s, tương ứng, và 10m 48s và 4m 53s, tương ứng, cho một bộ test hoàn chỉnh. Tất cả những số hiệu suất này cho thấy rằng việc sử dụng TESTPILOT trong môi trường online (ví dụ: trong IDE) để tạo test cho các hàm riêng lẻ, hoặc trong môi trường offline (ví dụ: trong quá trình code review) để tạo các bộ test hoàn chỉnh cho API là khả thi.

5 CÁC MƯỜI ĐE DỌA TÍNH HỢP LỆ
Tính hợp lệ nội bộ: Việc trích xuất đoạn ví dụ từ tài liệu dựa vào việc matching textual tên của hàm. Cho hai hàm có cùng tên nhưng đường dẫn truy cập khác nhau, chúng tôi không thể phân biệt hàm nào đang được sử dụng trong đoạn ví dụ. Theo đó, chúng tôi match đoạn này với cả hai hàm. Mặc dù điều này có thể dẫn đến sự không chính xác, thật không may không có alternative chính xác cho việc matching này. Bất kỳ heuristic nào cũng có thể khiến chúng tôi bỏ lỡ hoàn toàn các đoạn, điều này có thể tệ hơn vì đoạn ví dụ giúp tăng tỷ lệ test passing như được hiển thị trong Hình 8. Độ phủ tổng thể cao và tỷ lệ test passing cho thấy kỹ thuật matching của chúng tôi không phải là yếu tố hạn chế trong thực tế.

Tính hợp lệ cấu trúc: Chúng tôi sử dụng khái niệm assertion không tầm thường như proxy cho chất lượng oracle trong các test được tạo. Khi xác định assertion không tầm thường, chúng tôi tìm kiếm bất kỳ việc sử dụng nào của gói cần test trong backwards slice của assertion. Việc sử dụng như vậy có thể khác với hàm dự định cần test. Tuy nhiên, cho bản chất động của JavaScript, việc xác định chính xác việc sử dụng một hàm cho trước, như được trích xuất bởi API explorer, và sự xuất hiện của nó trong backwards slice là khó khăn. Mặc dù phương pháp của chúng tôi không cho phép chúng tôi xác định chính xác độ phủ không tầm thường cho một hàm cho trước, điều này không ảnh hưởng đến độ phủ không tầm thường mà chúng tôi báo cáo cho API hoàn chỉnh của mỗi gói. Lưu ý rằng khi tính độ phủ không tầm thường, chúng tôi đo độ phủ đầy đủ của test chứa ít nhất một assertion không tầm thường. Có thể có các lời gọi khác trong những test không tầm thường đó đóng góp vào độ phủ nhưng không đóng góp vào assertion. Việc đo assertion/checked coverage như được định nghĩa bởi Schuler và Zeller [62] là một alternative có thể, nhưng điều này khó triển khai chính xác cho JavaScript trong thực tế.

Định nghĩa assertion không tầm thường của chúng tôi đơn giản, đặt ra tiêu chuẩn thấp cho tính không tầm thường. Bất kỳ assertion nào được phân loại là tầm thường bởi tiêu chí của chúng tôi thực sự không có ý nghĩa, nhưng điều ngược lại không nhất thiết đúng. Theo đó, phép đo độ phủ không tầm thường của chúng tôi là lower bound trên độ phủ không tầm thường thực.

Mặc dù các ví dụ chúng tôi cho thấy trong bài báo gợi ý rằng các test được tạo bởi TESTPILOT sử dụng tên biến tương tự như những cái được chọn bởi lập trình viên, chúng tôi không đánh giá chính thức khả năng đọc của những test này. Trong tương lai, sẽ thú vị khi tiến hành nghiên cứu người dùng để đánh giá khả năng đọc của test được tạo bởi các kỹ thuật khác nhau.

Tính hợp lệ ngoại vi: Mặc dù quy mô đánh giá của chúng tôi vượt xa đánh giá của các phương pháp tạo test trước đây [11], [25], kết quả của chúng tôi vẫn dựa trên 25 gói npm và có thể không khái quát hóa cho các cơ sở mã JavaScript khác. Đặc biệt, hiệu suất của TESTPILOT có thể không khái quát hóa cho mã độc quyền không bao giờ được thấy trong tập đào tạo của LLM. Chúng tôi cố gắng giảm thiểu hiệu ứng này theo nhiều cách: (1) chúng tôi đánh giá trên các gói ít phổ biến có khả năng có ảnh hưởng ít hơn đến đào tạo của mô hình, (2) chúng tôi đánh giá trên 5 kho GitLab không được bao gồm trong đào tạo của mô hình, và (3) chúng tôi đo độ tương tự của các test được tạo với các test hiện có. Kết quả của chúng tôi cho thấy TESTPILOT hoạt động tốt cho cả gói phổ biến và không phổ biến và 92,8% test case có độ tương tự ≤50% với test hiện có, không có bản sao chính xác. Nhìn chung, điều này làm chúng tôi an tâm rằng TESTPILOT không tạo ra mã "được ghi nhớ".

Cuối cùng, chúng tôi lưu ý rằng mặc dù kỹ thuật của chúng tôi về mặt khái niệm là language-agnostic, triển khai hiện tại của TESTPILOT nhắm đến JavaScript, và do đó chúng tôi không thể khái quát hóa kết quả của mình cho các ngôn ngữ khác.

6 CÔNG VIỆC LIÊN QUAN
TESTPILOT cung cấp một alternative cho (và có thể bổ sung) các kỹ thuật truyền thống cho tạo test tự động, bao gồm tạo test ngẫu nhiên được hướng dẫn bởi phản hồi [7]-[11], kỹ thuật dựa trên tìm kiếm và tiến hóa [16], [17], [63], [64], và thực thi symbolic động [12]-[15]. Phần này xem xét các kỹ thuật neural cho tạo test, và các kỹ thuật tạo test trước đây cho JavaScript.

6.1 Kỹ thuật Neural
Các kỹ thuật neural đang được áp dụng nhanh chóng để giải quyết các vấn đề Software Engineering khác nhau, với kết quả đầy hứa hẹn trong nhiều lĩnh vực bao gồm code completion [34]-[38], program repair [39]-[41], và bug-finding [42], [43]. Pradel và Chandra [65] khảo sát tình trạng hiện tại của nghệ thuật trong lĩnh vực nghiên cứu mới nổi này. Chúng tôi biết về một số nỗ lực nghiên cứu gần đây trong đó LLM được sử dụng cho tạo test [23]-[29]. Có hai sự khác biệt chính giữa công việc của chúng tôi và những nỗ lực này: (i) mục tiêu và loại test được tạo và (ii) nhu cầu về một số hình thức fine-tuning hoặc dữ liệu bổ sung. Chúng tôi thảo luận chi tiết dưới đây.

Mục tiêu khác biệt: TICODER [24] và CODET [23] sử dụng Codex để tạo triển khai và test case từ mô tả vấn đề được thể hiện bằng ngôn ngữ tự nhiên. TICODER dựa vào vòng lặp test-driven user-intent formalization (TDUIF) trong đó người dùng và mô hình tương tác để tạo ra cả triển khai khớp với ý định của người dùng và một tập test case để validate tính đúng đắn của nó. CODET, mặt khác, tạo ra cả tập candidate implementation và một số test case dựa trên cùng prompt, chạy các test được tạo trên candidate implementation, và chọn giải pháp tốt nhất dựa trên kết quả test. Không giống TESTPILOT, không có nỗ lực nào trong số này giải quyết vấn đề tự động tạo unit test cho mã hiện có.

Cho các đặc tính của LLM trong việc tạo ra mã trông tự nhiên, đã có một số nỗ lực khám phá việc sử dụng LLM để giúp [27] hoặc bổ sung [28] các kỹ thuật tạo test truyền thống. Gần đây nhất, Lemieux et al. [27] khám phá việc sử dụng test được tạo bởi Codex như một cách để mở khóa quy trình tìm kiếm của việc tạo test sử dụng kỹ thuật dựa trên tìm kiếm [64], thường thất bại khi test ngẫu nhiên được tạo ban đầu có đầu vào vô nghĩa không thể được mutate hiệu quả. Kết quả của họ cho thấy rằng, trên hầu hết 27 dự án Python mục tiêu của họ, kỹ thuật được đề xuất của họ, CODAMOSA, vượt trội hơn kỹ thuật dựa trên tìm kiếm baseline, triển khai MOSA [64] của Pynguin, cũng như chỉ sử dụng Codex. Tuy nhiên, prompt Codex của họ chỉ bao gồm triển khai hàm và chỉ dẫn tạo test. Vì mục tiêu chính của họ là khám phá liệu test được tạo bởi Codex có thể cải thiện quy trình tìm kiếm hay không, họ không khám phá có hệ thống hiệu ứng của các thành phần prompt khác nhau. Thực tế, họ phỏng đoán rằng prompt engineering thêm có thể cải thiện kết quả, tạo động lực cho nhu cầu công việc của chúng tôi khám phá có hệ thống các thành phần prompt khác nhau. Ngoài ra, test được tạo của họ ở định dạng MOSA [64], mà các tác giả thừa nhận có thể mất khả năng đọc, và không chứa assertion. Hầu hết test của chúng tôi chứa assertion, và chúng tôi nghiên cứu thêm về chất lượng assertion chúng tôi tạo cũng như lý do thất bại test.

Tương tự, cho rằng thường khó cho các kỹ thuật tạo test truyền thống tạo ra assertion (hữu ích) [21], [22], ATLAS [28] sử dụng LLM để tạo statement assert cho test Java cho trước (không có assertion). Họ định vị kỹ thuật của mình như bổ sung cho các kỹ thuật truyền thống [8], [17]. Với cùng mục tiêu, Mastrapaolo et al. [29], [45] và Tufano et al. [46] thực hiện công việc tiếp theo sử dụng transfer learning, trong khi Yu et al. [66] sử dụng kỹ thuật information retrieval để cải thiện thêm statement assert được tạo bởi Atlas. TOGA [67] sử dụng kỹ thuật tương tự nhưng kết hợp thêm exceptional oracle classifier để quyết định xem một phương thức cho trước có yêu cầu assertion để test hành vi exceptional hay không. Sau đó nó base việc tạo assertion trên taxonomy oracle được định nghĩa trước được tạo bằng cách phân tích thủ công các test Java hiện có và sử dụng cơ chế ranking dựa trên neural để rank candidate với oracle cao hơn. Ngược lại với những nỗ lực này, mục tiêu của chúng tôi là tạo ra phương thức test hoàn chỉnh mà không đưa cho mô hình bất kỳ nội dung nào của phương thức test (ngoài mã boilerplate được yêu cầu bởi Mocha), có nghĩa là mô hình cần tạo ra cả mã setup test (ví dụ: khởi tạo đối tượng và populate chúng) cũng như assertion. Mặc dù TOGA có thể được tích hợp với EvoSuite [16] để tạo công cụ tạo test end-to-end, công việc gần đây [68] chỉ ra một số khiếm khuyết của phương pháp đánh giá, đặt nghi ngờ về tính hợp lệ của kết quả được báo cáo.

Đầu vào/Đào tạo khác biệt: Bareiß et al. [25] đánh giá hiệu suất của Codex trên ba tác vụ tạo mã, bao gồm tạo test. Giống chúng tôi, họ dựa vào việc nhúng thông tin contextual vào prompt để hướng dẫn LLM, mặc dù dữ liệu cụ thể họ nhúng khác: trong khi TESTPILOT chỉ bao gồm chữ ký, định nghĩa, tài liệu, và ví dụ sử dụng trong prompt, Bareiß et al. theo đuổi phương pháp học few-shot nơi, ngoài định nghĩa của hàm cần test, họ bao gồm ví dụ về hàm khác từ cùng cơ sở mã và test liên quan để cho mô hình gợi ý về test nên như thế nào, cũng như danh sách chữ ký hàm helper liên quan có thể hữu ích cho tạo test. Đối với danh sách hạn chế 18 phương thức Java, họ thấy rằng phương pháp này so sánh thuận lợi với tạo test feedback-directed [8]. Đây là kết quả đầy hứa hẹn, nhưng việc tìm test ví dụ phù hợp để sử dụng trong học few-shot có thể khó khăn, đặc biệt vì đánh giá của họ cho thấy độ phủ tốt phụ thuộc quan trọng vào các ví dụ có liên quan chặt chẽ đến hàm cần test.

Tufano et al. [26] trình bày AthenaTest, một phương pháp cho tạo test tự động dựa trên mô hình transformer BART [44]. Đối với test case cho trước, họ dựa vào heuristic để xác định class và phương thức "focal" cần test. Những test case được mapped này sau đó được sử dụng để fine-tune mô hình cho tác vụ tạo ra unit test bằng cách đại diện tác vụ này như tác vụ dịch map focal method (cùng với focal class, constructor, và các phương thức và trường public khác trong class đó) thành test case. Trong thí nghiệm trên 5 dự án từ Defects4J [69], AthenaTest tạo ra 158K test case, đạt được độ phủ test tương tự như EvoSuite [16], một công cụ tạo test dựa trên tìm kiếm phổ biến, và phủ 43% tất cả focal method. Sự khác biệt đáng kể giữa công việc của họ và chúng tôi là phương pháp của họ yêu cầu đào tạo mô hình trên tập lớn test case trong khi TESTPILOT sử dụng LLM có sẵn. Thực tế, ngoài sự khác biệt mục tiêu với ATLAS [28] và công việc của Mastrapaolo et al. [29], [45] ở trên, cả hai nỗ lực này cũng yêu cầu dataset test method (với assertion) và focal method tương ứng của chúng, dù để sử dụng trong đào tạo chính [28] hay trong fine tuning trong transfer learning [29], [45], [46].

Thật không may, những sự khác biệt trên về mục tiêu hoặc trong dữ liệu yêu cầu cho đào tạo mô hình làm cho việc so sánh thực nghiệm trực tiếp với TESTPILOT vô nghĩa hoặc không thể. Ngoài ra, không có nỗ lực nào trong số này hỗ trợ JavaScript hoặc cung cấp dataset JavaScript có thể được sử dụng để so sánh. Thực tế, một trong những động lực chính của chúng tôi cho việc khám phá prompt engineering cho LLM có sẵn là tránh nhu cầu thu thập test example cho học few-shot [25] hoặc cặp test method/focal method yêu cầu cho đào tạo [28] hoặc fine tuning bổ sung [29], [45], [46].

Kỹ thuật khác: Stallenberg et al. [70] trình bày kỹ thuật tạo test cho JavaScript dựa trên suy luận kiểu unsupervised bao gồm ba giai đoạn. Đầu tiên, phân tích tĩnh được thực hiện để suy ra mối quan hệ giữa các phần tử chương trình như biến và biểu thức. Sau đó, suy luận kiểu probabilistic được áp dụng cho những mối quan hệ này để xây dựng mô hình. Cuối cùng, họ cho thấy cách kỹ thuật dựa trên tìm kiếm có thể tận dụng thông tin chứa trong những mô hình như vậy bằng cách đề xuất hai chiến lược để tham khảo những mô hình này trong vòng lặp chính của DynaMOSA [64].

Gần đây, El Haji [71] trình bày nghiên cứu thực nghiệm khám phá hiệu quả của GitHub Copilot trong việc tạo test. Trong nghiên cứu này, test được chọn từ các bộ test hiện có liên quan đến 7 dự án Python mã nguồn mở. Sau khi loại bỏ nội dung của mỗi hàm test, Copilot được yêu cầu hoàn thành triển khai để test kết quả có thể được thực thi và so sánh với test gốc. Hai biến thể của phương pháp này được khám phá, viz., "with context" nơi các test khác trong bộ được bảo tồn và "without context" nơi test khác bị loại bỏ. El Haji cũng khám phá tác động của việc thêm (thủ công) comment bao gồm mô tả hành vi dự định và ví dụ sử dụng. Kết quả từ nghiên cứu cho thấy 45,28% test được tạo đang passing trong tình huống "with context" (phần còn lại failing, không hợp lệ về cú pháp, hoặc empty) vs chỉ 7,55% test được tạo passing trong tình huống "without context", và việc thêm ví dụ sử dụng và comment thường hữu ích. Có một số sự khác biệt đáng kể giữa phương pháp của chúng tôi và công việc của El Haji: chúng tôi khám phá kỹ thuật hoàn toàn tự động mà không có bước thủ công nào, chúng tôi báo cáo về đánh giá thực nghiệm rộng rãi hơn đáng kể, chúng tôi trình bày kỹ thuật thích ứng trong đó prompt được tinh chỉnh để đáp ứng với hành vi thực thi của test được thực thi trước đó, chúng tôi nhắm đến ngôn ngữ lập trình khác (JavaScript thay vì Python), và TestPilot tương tác trực tiếp với LLM thay vì dựa vào Copilot, một programming assistant dựa trên LLM.

6.2 Kỹ thuật Tạo Test cho JavaScript
Cơ chế của TESTPILOT cho việc tinh chỉnh prompt dựa trên phản hồi thực thi được lấy cảm hứng từ cơ chế được sử dụng bởi các kỹ thuật tạo test ngẫu nhiên feedback-directed [7]-[11], nơi test mới được tạo bằng cách mở rộng test passing được tạo trước đó. Như được báo cáo trong Phần 4.2, TESTPILOT đạt được độ phủ statement và branch cao hơn đáng kể so với Nessie [11], đại diện cho tiên tiến trong tạo test ngẫu nhiên feedback-directed cho JavaScript.

Một số dự án trước đây đã xem xét tạo test cho JavaScript (xem [72] cho khảo sát). Saxena et al. [73] trình bày Kudzu, một công cụ nhằm tìm lỗ hổng injection trong ứng dụng JavaScript client-side bằng cách khám phá input space của ứng dụng. Họ phân biệt input space của ứng dụng thành event space, liên quan đến thứ tự mà event handler thực thi (ví dụ: kết quả của việc click button), và value space liên quan đến việc chọn giá trị được truyền cho hàm hoặc được nhập vào text field. Kudzu sử dụng thực thi symbolic động để khám phá value space một cách có hệ thống, nhưng nó dựa vào chiến lược khám phá ngẫu nhiên để khám phá event space. Artemis [74] là framework cho tạo test tự động lặp đi lặp lại tạo test cho ứng dụng JavaScript client-side bao gồm chuỗi sự kiện, sử dụng chiến lược dựa trên heuristic xem xét các vị trí được đọc và viết bởi mỗi event handler để tập trung vào việc tạo test liên quan đến event handler tương tác với nhau. Li et al. [75] mở rộng Artemis với thực thi symbolic động để cải thiện khả năng khám phá value space của nó, và Tanida et al. [76] cải thiện thêm công việc này bằng cách bổ sung test input được tạo với invariant do người dùng cung cấp. Fard et al. [77] trình bày ConFix, một công cụ sử dụng kết hợp phân tích động và thực thi symbolic để tự động tạo instance của Document Object Model (DOM) có thể phục vụ như test fixture trong unit test cho mã JavaScript client-side. Marchetto và Tonella [78] trình bày kỹ thuật tạo test dựa trên tìm kiếm xây dựng test bao gồm chuỗi sự kiện dựa vào việc trích xuất tự động finite state machine đại diện cho trạng thái của ứng dụng. Không có công cụ nào trong số này tạo test chứa assertion.

Một số công cụ tạo test cho JavaScript có khả năng tạo test chứa assertion. JSART [79] là công cụ tạo regression test chứa assertion phản ánh likely invariant được tạo sử dụng biến thể của Daikon dynamic invariant generator [80]. Vì Daikon tạo assertion có khả năng hold, một bước bổ sung cần thiết trong đó assertion không hợp lệ được loại bỏ khỏi test được tạo. Mirshokraie et al. [81], [82] trình bày phương pháp trong đó test được tạo cho ứng dụng JavaScript client-side bao gồm chuỗi sự kiện. Sau đó, trong bước bổ sung, unit test cấp hàm được dẫn xuất bằng cách instrument thực thi chương trình để monitor trạng thái của tham số, biến global, và DOM khi entry và exit hàm để có được giá trị mà hàm sẽ được gọi. Assertion được thêm tự động vào test được tạo bằng cách: (i) mutate DOM và mã của ứng dụng cần test, (ii) thực thi test được tạo để xác định trạng thái ứng dụng bị tác động như thế nào bởi mutation, và (iii) thêm assertion vào test phản ánh hành vi trước mutation. Testilizer [83] là công cụ tạo test nhằm tăng cường bộ test được viết bằng tay hiện có. Để làm điều này, Testilizer instrument mã để quan sát cách test hiện có truy cập DOM, và thực thi chúng để có được State-Flow Graph trong đó node phản ánh trạng thái DOM động và edge phản ánh chuyển đổi event-driven giữa những trạng thái này. Đường dẫn alternative được khám phá bằng cách khám phá sự kiện chưa được khám phá trước đó trong mỗi trạng thái. Testilizer thêm assertion vào test được tạo được sao chép nguyên văn từ test hiện có, bằng cách adapt cấu trúc của assertion hiện có cho trạng thái mới được khám phá, hoặc bằng cách suy ra assertion tương tự sử dụng kỹ thuật machine learning.

Những kỹ thuật này chia sẻ hạn chế rằng chúng yêu cầu toàn bộ ứng dụng cần test phải có thể thực thi, hạn chế tính ứng dụng của chúng. Hơn nữa, một số kỹ thuật được thảo luận ở trên yêu cầu thực thi lại test (để suy ra assertion sử dụng mutation testing [81], [82], hoặc để lọc ra assertion không hợp lệ [79]), điều này tăng thêm chi phí của chúng. Ngược lại, TESTPILOT chỉ yêu cầu các hàm của hàm API cần test có sẵn và có thể thực thi, và nó thực thi mỗi test mà nó tạo chỉ một lần.

7 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Chúng tôi đã trình bày TESTPILOT, một phương pháp cho tạo unit-test thích ứng sử dụng mô hình ngôn ngữ lớn. Không giống công việc trước đây trong lĩnh vực này, TESTPILOT không yêu cầu fine-tuning cũng không yêu cầu parallel corpus của hàm và test. Thay vào đó, chúng tôi nhúng thông tin contextual về hàm cần test vào prompt, cụ thể là chữ ký của nó, comment tài liệu đính kèm (nếu có), bất kỳ ví dụ sử dụng nào từ tài liệu dự án, và mã nguồn của hàm. Hơn nữa, nếu test được tạo bị lỗi, chúng tôi tạo thích ứng prompt mới nhúng test này và thông báo thất bại để hướng dẫn mô hình tạo ra test problematic đã được sửa. Chúng tôi đã triển khai phương pháp của mình cho JavaScript trên LLM có sẵn, và cho thấy nó đạt được độ phủ statement tiên tiến trên 25 gói npm. Đánh giá thêm cho thấy đa số test được tạo chứa assertion không tầm thường, và tất cả phần thông tin được bao gồm trong prompt đóng góp vào chất lượng của test được tạo. Thí nghiệm với ba LLM (gpt3.5-turbo, code-cushman-002, và StarCoder) chứng minh rằng tạo test dựa trên LLM đã vượt trội hơn phương pháp tạo test trước đây tiên tiến như Nessie trên các metric chính. Chúng tôi phỏng đoán rằng việc sử dụng LLM tiên tiến hơn sẽ cải thiện thêm kết quả, mặc dù chúng tôi miễn cưỡng suy đoán bao nhiêu.

Trong công việc tương lai, chúng tôi có kế hoạch điều tra thêm về chất lượng của test được tạo bởi TESTPILOT. Mặc dù trong bài báo này chúng tôi tập trung vào test passing và loại trừ hoàn toàn test bị lỗi khỏi xem xét, chúng tôi đã thấy ví dụ về test bị lỗi "gần đúng" và có thể thú vị cho nhà phát triển như điểm khởi đầu cho tinh chỉnh thêm. Tuy nhiên, việc làm điều này phụ thuộc vào việc có chiến lược tốt để phân biệt test bị lỗi hữu ích từ những test vô dụng. Thí nghiệm của chúng tôi đã chứng minh rằng lỗi timeout, lỗi assertion, và lỗi correctness là yếu tố chính khiến test bị lỗi. Trong công việc tương lai, chúng tôi có kế hoạch áp dụng phân tích chương trình tĩnh và động cho test bị lỗi để xác định tại sao lỗi timeout và lỗi assertion xảy ra và test bị lỗi có thể được sửa đổi như thế nào để làm cho chúng pass.

Nghiên cứu thêm cần thiết để xác định yếu tố nào ngăn cản việc tạo assertion không tầm thường. Anecdotally, chúng tôi đã quan sát rằng sự có sẵn của ví dụ sử dụng thường hữu ích. Chúng tôi hình dung rằng số lượng assertion hữu ích có thể được cải thiện bằng cách có được ví dụ sử dụng theo cách khác, ví dụ: bằng cách tương tác với người dùng, hoặc bằng cách trích xuất ví dụ sử dụng từ client của ứng dụng cần test.

Một lĩnh vực thí nghiệm plodal khác có thể là thay đổi nhiệt độ lấy mẫu của LLM. Trong công việc này, chúng tôi luôn lấy mẫu ở nhiệt độ zero, có lợi thế cung cấp kết quả stable, nhưng cũng có nghĩa là mô hình ít có khả năng cung cấp completion xác suất thấp hơn có thể dẫn đến test thú vị hơn.

Một lĩnh vực công việc tương lai khác là phát triển kỹ thuật hybrid kết hợp kỹ thuật tạo test feedback-directed hiện có với kỹ thuật dựa trên LLM như TESTPILOT. Ví dụ, người ta có thể sử dụng kỹ thuật dựa trên LLM để tạo tập test ban đầu và sử dụng test mà nó tạo như điểm khởi đầu cho extension bởi kỹ thuật feedback-directed như Nessie, do đó cho phép nó khám phá các trường hợp biên khó khám phá chỉ sử dụng giá trị ngẫu nhiên.

Về nguyên tắc, phương pháp của chúng tôi có thể được adapt cho bất kỳ ngôn ngữ lập trình nào. Practically speaking, điều này sẽ liên quan đến việc adapt prompt để sử dụng cú pháp của ngôn ngữ đang xem xét, và sử dụng testing framework cho ngôn ngữ đó. Ngoài ra, việc khai thác tài liệu và ví dụ sử dụng cần được adapt để khớp với định dạng tài liệu được sử dụng cho ngôn ngữ đó. LLM mà chúng tôi sử dụng không có đào tạo cụ thể ngôn ngữ và có thể được sử dụng để tạo test cho ngôn ngữ khác, mặc dù hiệu quả của phương pháp sẽ phụ thuộc vào lượng mã được viết bằng ngôn ngữ đó được bao gồm trong tập đào tạo của LLM. Một câu hỏi cụ thể thú vị để khám phá là phương pháp như TESTPILOT sẽ hoạt động như thế nào trên ngôn ngữ typed tĩnh.

LỜI CẢM ƠN
Nghiên cứu được báo cáo trong bài báo này được tiến hành trong khi S. Nadi và F. Tip là khách thăm sabbatical và A. Eghbali là thực tập sinh tại GitHub. Các tác giả biết ơn nhóm GitHub Next cho nhiều cuộc thảo luận sâu sắc và hữu ích về TestPilot. F. Tip được hỗ trợ một phần bởi các grant National Science Foundation CCF-1907727 và CCF-2307742. Nghiên cứu của S. Nadi được hỗ trợ bởi Canada Research Chairs Program và Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN-2017-04289.

THAM KHẢO
[Danh sách tham khảo được giữ nguyên như trong bản gốc]
