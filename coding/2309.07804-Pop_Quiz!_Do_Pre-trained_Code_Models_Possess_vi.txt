# Pop Quiz! Các mô hình mã được huấn luyện trước có sở hữu kiến thức về tên API đúng không?

Terry Yue Zhuo
Monash University,
CSIRO's Data61
Australia
terry.zhuo@monash.edu

Xiaoning Du
Monash University
Australia
xiaoning.du@monash.edu

Zhenchang Xing
CSIRO's Data61,
Australian National University
Australia
zhenchang.xing@data61.csiro.au

Jiamou Sun
CSIRO's Data61
Australia
frank.sun@data61.csiro.au

Haowei Quan
Monash University
Australia
haowei.quan@monash.edu

Li Li
Beihang University
China
lilicoding@ieee.org

Liming Zhu
CSIRO's Data61,
University of New South Wales
Australia
liming.zhu@data61.csiro.au

Tóm tắt — Những đột phá gần đây trong các mô hình mã được huấn luyện trước, như CodeBERT và Codex, đã cho thấy hiệu suất vượt trội của chúng trong các tác vụ downstream khác nhau. Tính đúng đắn và không mơ hồ của việc sử dụng API giữa các mô hình mã này rất quan trọng để đạt được các chức năng chương trình mong muốn, đòi hỏi chúng phải học các tên đầy đủ của API khác nhau cả về mặt cấu trúc và ngữ nghĩa. Các nghiên cứu gần đây tiết lộ rằng ngay cả các mô hình mã được huấn luyện trước tiên tiến nhất cũng gặp khó khăn trong việc gợi ý các API đúng trong quá trình tạo mã. Tuy nhiên, các lý do cho hiệu suất sử dụng API kém như vậy hầu như không được điều tra. Để giải quyết thách thức này, chúng tôi đề xuất sử dụng knowledge probing như một phương tiện diễn giải các mô hình mã, sử dụng các bài kiểm tra kiểu cloze để đo lường kiến thức được lưu trữ trong các mô hình. Nghiên cứu toàn diện của chúng tôi xem xét khả năng hiểu tên đầy đủ của API của mô hình mã từ hai góc độ khác nhau: gọi API và nhập API. Cụ thể, chúng tôi tiết lộ rằng các mô hình mã hiện tại gặp khó khăn trong việc hiểu tên API, với các chiến lược tiền huấn luyện ảnh hưởng đáng kể đến chất lượng học tên API. Chúng tôi chứng minh rằng ngữ cảnh ngôn ngữ tự nhiên có thể hỗ trợ các mô hình mã trong việc định vị tên API Python và tổng quát hóa kiến thức tên API Python cho dữ liệu chưa thấy. Các phát hiện của chúng tôi cung cấp những hiểu biết sâu sắc về các hạn chế và khả năng của các mô hình mã được huấn luyện trước hiện tại, và gợi ý rằng việc kết hợp cấu trúc API vào quá trình tiền huấn luyện có thể cải thiện việc sử dụng API tự động và biểu diễn mã. Công trình này mang lại ý nghĩa quan trọng cho việc thúc đẩy các thực hành trí tuệ mã và định hướng cho các nghiên cứu tương lai. Tất cả kết quả thí nghiệm, dữ liệu và mã nguồn được sử dụng trong công trình này có sẵn tại https://doi.org/10.5281/zenodo.7902072.

I. GIỚI THIỆU

Những tiến bộ gần đây trong trí tuệ mã đã kết hợp các kỹ thuật tiền huấn luyện, nơi các mô hình được huấn luyện trước trên các kho ngữ liệu mã nguồn không gán nhãn quy mô lớn để học biểu diễn và ngữ nghĩa của mã. Các mô hình mã được huấn luyện trước, như CodeBERT [1] và GraphCodeBERT [2], có thể được tinh chỉnh cho các tác vụ mã downstream khác nhau, như hoàn thành mã và dịch thuật [3]–[5].

Mặc dù có sự cải thiện, vẫn còn một khoảng cách hiệu suất đáng kể giữa hiệu suất của chúng và của các nhà phát triển con người khi nói đến việc sử dụng API một cách chính xác. Ví dụ, một số nghiên cứu đã chứng minh rằng ngay cả các mô hình mã được huấn luyện trước tiên tiến nhất, như Codex [6], StarCoder [7] và GPT-4 [8], cũng gặp khó khăn trong việc gợi ý các API đúng trong quá trình tạo mã [9], [10]. Tuy nhiên, ít nghiên cứu nào đã điều tra các lý do đằng sau hiệu suất sử dụng API kém của các mô hình này.

Về cơ bản, việc sử dụng API đúng phụ thuộc vào hai loại kiến thức: cách gọi một API và API nào cần gọi, với việc trước đây là bước cơ bản hướng tới việc sau đó. Để gọi một API, người ta phải có kiến thức về ngữ pháp mã của việc nhập thư viện và soạn tên API đúng dựa trên các câu lệnh nhập và gọi. Tuy nhiên, các mô hình mã không được hướng dẫn rõ ràng bởi ngữ pháp mã. Mặc dù chúng ta thường giả định rằng các mô hình có thể học cách sử dụng API hiệu quả bằng cách quan sát một số lượng lớn ví dụ, giả định này đã được xác thực kém. Do đó, chúng tôi đặt ra câu hỏi sau: Các mô hình mã được huấn luyện trước có sở hữu kiến thức về tên API đúng không?

Tên đầy đủ của một API không chỉ bao gồm tên hàm mà còn bao gồm tên của gói, module và/hoặc lớp mà nó thuộc về. Để dễ trình bày, chúng tôi sẽ sử dụng thuật ngữ "module" để chỉ các thực thể này dưới đây. Các thư viện thường tổ chức API thành các module lồng nhau để giúp các nhà phát triển hiểu các tính năng có sẵn khi sử dụng API. API thường được tổ chức thành các module lồng nhau trong các thư viện để hỗ trợ các nhà phát triển hiểu các tính năng có sẵn khi sử dụng API. Các module này tạo thành một cấu trúc phân cấp, với module cấp cao hơn đóng vai trò là cha của các module cấp thấp hơn trực tiếp, và các API hoạt động như các nút lá trong cấu trúc. Các tên API đầy đủ được thu được bằng cách duyệt qua phân cấp từ gốc đến lá, với tất cả các tên được kết nối bằng dấu chấm. Tùy thuộc vào cấp độ module được nhập và liệu nó có được liên kết với một bí danh hay không, tên API được sử dụng để gọi phải được điều chỉnh cho phù hợp. Quy ước này có thể được hiểu dễ dàng bởi các nhà phát triển con người, nhưng các mô hình mã có thể thấy khó khăn để học.

Ngoài ra, các tên đầy đủ của API truyền tải thông tin về modularization mã và thiết kế namespace API. Nếu các mô hình có thể học một biểu diễn tốt của tên API, chúng có thể hữu ích cho việc tự động hóa thiết kế namespace API bằng cách cung cấp các tùy chọn thiết kế chính xác và có liên quan hơn. Ví dụ, xem xét thư viện Python <numpy>, được sử dụng cho các phép toán trên mảng và ma trận. Như được hiển thị trong Hình 1, <numpy> bao gồm một số module, bao gồm <linalg> (viết tắt của linear algebra) cho các API liên quan đến tính toán trong đại số tuyến tính, như <multi_dot> (tích vô hướng), <cholesky> (phân tích Cholesky), và <qr> (phân tích QR), cũng như <ma> (viết tắt của masked array) cho các API liên quan đến các phép toán trên mảng che dấu. Thiết kế namespace như vậy có thể cung cấp những hiểu biết có giá trị khi thiết kế các thư viện có liên quan hoặc các thư viện tương tự cho các ngôn ngữ lập trình khác.

Để hiểu mức độ tốt của các mô hình mã được huấn luyện trước trong việc hiểu tên API, việc phân tích và diễn giải các cơ chế nội bộ của các mô hình mã được huấn luyện trước là rất quan trọng. Tuy nhiên, các mô hình neural sâu thường phức tạp và mờ đục [11], khiến việc hiểu đầy đủ các cơ chế nội bộ của chúng trở nên khó khăn. Trong cộng đồng xử lý ngôn ngữ tự nhiên, kỹ thuật chính để kiểm tra những gì các mô hình ngôn ngữ được huấn luyện trước biết là probing, nơi một probe là một bộ phân loại đơn giản nhận các biểu diễn ngữ cảnh của mô hình làm đầu vào và dự đoán một thuộc tính quan tâm [12]. Gần đây, các nhà nghiên cứu đã cố gắng phân tích cú pháp mã được bắt bởi các mô hình mã thông qua phân tích attention và grammatical structure probing [13]–[15]. Tuy nhiên, có một nỗ lực không đáng kể trong việc đánh giá kiến thức API [16].

Trong công trình này, chúng tôi khám phá khả năng diễn giải của các mô hình mã và giới thiệu một tác vụ knowledge probing quy mô lớn nhắm mục tiêu cụ thể vào các tên đầy đủ của API. Chúng tôi thiết kế một framework đánh giá tự động và hiệu quả, INK, để probe sự hiểu biết của các mô hình mã về tên API với các câu đố kiểu cloze. Để tạo ra các câu đố, đầu tiên chúng tôi trích xuất các tên đầy đủ của API từ các lời gọi API trong các kho ngữ liệu mã nguồn quy mô lớn thường được sử dụng để huấn luyện các mô hình mã. Lưu ý rằng, thay vì trực tiếp trích xuất tên API từ các thư viện, chúng tôi tính đến các hạn chế của các mô hình mã trong quá trình huấn luyện của chúng. Chúng tôi tránh kiểm tra các mô hình về kiến thức mà chúng chưa bao giờ được tiếp xúc. Hơn nữa, chúng tôi rút ra các câu lệnh nhập và câu lệnh gọi API dựa trên tên API và che đi các token khác nhau, với chỉ một token được che đi tại một thời điểm, ở mỗi cấp độ module của các câu lệnh này. Ví dụ, khi gọi module <multi_dot> như được hiển thị trong Hình 1, chúng tôi xem xét từ hai khía cạnh, gọi API (<numpy.linalg.multi_dot>) và nhập API (<from numpy.linalg import multi_dot>). Bằng cách che token lin bằng "[MASK]", chúng tôi có hai câu đố <numpy.[MASK]alg.multi_dot> và <from numpy.[MASK]alg import multi_dot>. Các mô hình mã sau đó được mong đợi dự đoán token bị che đi cho một câu lệnh nhập hoặc gọi API với mask được áp dụng.

Để hiểu rõ hơn về khả năng và hạn chế của các mô hình mã được huấn luyện trước trong việc học tên API, chúng tôi đưa ra để điều tra một số câu hỏi nghiên cứu (RQ) để đánh giá hiệu suất của chúng và hướng dẫn các cải tiến trong tương lai trong lĩnh vực này:
• RQ1: Các mô hình mã hiểu các lời gọi API và nhập API tốt như thế nào? Các mô hình mã được đánh giá về dự đoán các câu lệnh nhập hoặc gọi API bị che đi. Các phát hiện có thể giúp xác định các khu vực mà các mô hình mã có thể gặp khó khăn và hướng dẫn cải tiến trong việc học tên API của chúng bằng cách hiểu câu hỏi này.
• RQ2: Các mô hình mã có hiểu các bí danh nhập API không? RQ này tiếp tục đánh giá sự hiểu biết của các mô hình mã về các bí danh nhập API. Các câu đố được thiết kế dựa trên một câu lệnh nhập định nghĩa bí danh theo sau bởi một lời gọi API dựa trên module được nhập. Các token được che đi có chọn lọc cho các câu lệnh.
• RQ3: Ngữ cảnh ngôn ngữ tự nhiên có ảnh hưởng đến kết quả probing kiến thức tên API không? Chúng tôi điều tra liệu hiệu suất tổng thể của các mô hình có thể được cải thiện bằng cách kết hợp các truy vấn ngôn ngữ tự nhiên hay không. Các phát hiện có thể hướng dẫn việc phát triển các kỹ thuật tận dụng thông tin ngữ cảnh để nâng cao việc học tên API của các mô hình mã.
• RQ4: Các mô hình mã có thể ghi nhớ và tổng quát hóa trên tên API tốt như thế nào? Chúng tôi chia API thành hai nhóm dựa trên việc chúng có được nhìn thấy trong giai đoạn huấn luyện hay không. Các kết quả chỉ ra liệu các mô hình mã có sở hữu khả năng tổng quát hóa mạnh mẽ và kỹ năng ghi nhớ phù hợp hay không. Khả năng của các mô hình áp dụng kiến thức đã học cho các API chưa thấy sẽ hữu ích cho thiết kế namespace API.

Để đánh giá, chúng tôi xây dựng benchmark đầu tiên về kiến thức tên API Python, PyINK, và phân tích 10 mô hình mã được huấn luyện trước, bao gồm các biến thể của CodeBERT [1], GraphCodeBERT [2], PLBART [17]. Công trình của chúng tôi bổ sung cho việc phát triển các kỹ thuật tiên tiến hơn để mô hình hóa biểu diễn API trong mã, từ đó nâng cao hiệu quả và độ chính xác của các mô hình mã. Ngoài ra, những hiểu biết thu được từ công trình này có thể mở đường cho sự hiểu biết sâu sắc hơn về cách tiền huấn luyện tác động đến hiệu suất của các mô hình mã, tạo điều kiện cho các quyết định thiết kế có thông tin hơn trong lĩnh vực này. Tóm lại, các đóng góp chính của chúng tôi bao gồm:
• Một framework đánh giá kiểu cloze INK, để probe và benchmark kiến thức về tên API trong các mô hình mã được huấn luyện trước.
• Một triển khai của INK, làm giảm rào cản cho việc thiết kế các kỹ thuật probing về kiến thức tên API.
• Một tập dữ liệu đánh giá dựa trên INK, PyINK, chứa các câu đố kiểu cloze tên API đa dạng.
• Một nghiên cứu toàn diện về việc hiểu kiến thức tên API Python của các mô hình mã được huấn luyện trước thông qua PyINK.

II. BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN

Phần này cung cấp một tổng quan toàn diện về bối cảnh và công trình liên quan tạo nên nền tảng cho nghiên cứu của chúng tôi. Đầu tiên, chúng tôi giới thiệu ba họ mô hình mã được huấn luyện trước nổi bật, cụ thể là CodeBERT [1], GraphCodeBERT [2], và PLBART [17], đã được áp dụng rộng rãi trong các nghiên cứu gần đây. Bên cạnh đó, chúng tôi trình bày một bài đánh giá về knowledge probing trong các mô hình ngôn ngữ, đã nổi lên như một lĩnh vực nghiên cứu quan trọng để nâng cao khả năng diễn giải và hiểu biết của các mô hình như vậy. Cuối cùng, chúng tôi thảo luận về cách các công trình hiện có về deep API learning khác với nghiên cứu của chúng tôi. Thông qua các thảo luận này, chúng tôi thiết lập khung lý thuyết và ngữ cảnh cần thiết để đánh giá đầy đủ tính mới và ý nghĩa của phương pháp được đề xuất.

A. Các mô hình mã được huấn luyện trước

Các mô hình ngôn ngữ được huấn luyện trước, như BERT [18], thường được sử dụng để chuyển giao kiến thức trong các tác vụ xử lý ngôn ngữ tự nhiên downstream khác nhau [19]. Các mô hình này được huấn luyện trên các kho ngữ liệu NL rộng lớn và được tinh chỉnh trên các tập dữ liệu có gán nhãn nhỏ cho các tác vụ khác nhau. Chúng nắm bắt thông tin ngôn ngữ ngữ cảnh và loại bỏ sự cần thiết cho các mô hình cụ thể cho tác vụ. Tương tự, các mô hình mã được huấn luyện trước, như CodeBERT, GraphCodeBERT, và PLBART, đã được phát triển để tận dụng "tính tự nhiên" của phần mềm [20]. Các mô hình này xuất sắc trong các tác vụ downstream liên quan đến mã khác nhau, như hoàn thành mã, tóm tắt từ mã sang văn bản, và dịch từ mã sang mã.

1) CodeBERT: CodeBERT là một mô hình mã được huấn luyện trước đa ngôn ngữ tinh vi được xây dựng trên kiến trúc BERT. Nó xuất sắc trong việc hiểu các kết nối ngữ nghĩa giữa ngôn ngữ tự nhiên (NL) và ngôn ngữ lập trình (PL) thông qua mô hình hóa ngôn ngữ có che đi và phát hiện token thay thế. CodeBERT đạt được kết quả tiên tiến trong các tác vụ tìm kiếm mã NL và tạo tài liệu mã, làm cho nó trở thành một công cụ có giá trị cho việc viết và đánh giá học thuật.

Được huấn luyện trước trên tập dữ liệu CSNet [21], CodeBERT bao gồm một phạm vi đa dạng của các thể hiện NL và PL, bao gồm các đoạn mã, bình luận, và mã đơn ngôn ngữ. Hiệu suất ấn tượng của nó đạt được mà không dựa vào các chỉ báo rõ ràng để phân biệt PL. Bằng cách tinh chỉnh các tham số của nó, CodeBERT liên tục thể hiện sự thành thạo đặc biệt, củng cố vị trí của nó như một lựa chọn hàng đầu cho các nhà viết và đánh giá học thuật.

2) GraphCodeBERT: GraphCodeBERT là một mô hình mã được huấn luyện trước cách mạng hóa việc hiểu mã bằng cách xem xét cấu trúc vốn có của mã. Không giống như các mô hình khác dựa vào cây cú pháp trừu tượng (AST) phức tạp, GraphCodeBERT tận dụng luồng dữ liệu trong quá trình tiền huấn luyện để nắm bắt cấu trúc cấp độ ngữ nghĩa và mã hóa các mối quan hệ biến. Bằng cách tập trung vào khía cạnh "giá trị đến từ đâu", GraphCodeBERT đạt được biểu diễn mã hiệu quả và hiệu quả hơn [2].

Được huấn luyện trước trên cùng tập dữ liệu CSNet như CodeBERT, GraphCodeBERT sử dụng ba mục tiêu riêng biệt trong quá trình tiền huấn luyện. Nó liên quan đến mô hình hóa ngôn ngữ có che đi, dự đoán cạnh luồng dữ liệu, và căn chỉnh biến giữa mã nguồn và luồng dữ liệu. Phương pháp toàn diện này cho phép GraphCodeBERT xuất sắc trong bốn tác vụ downstream chính: tìm kiếm văn bản-mã, phát hiện bản sao, dịch mã, và tinh chỉnh mã, vượt trội hơn CodeBERT trong tất cả các lĩnh vực này. Hiệu suất vượt trội của mô hình xác nhận hiệu quả của nó và thiết lập GraphCodeBERT như một giải pháp tiên tiến trong lĩnh vực phân tích và hiểu mã.

3) PLBART: PLBART là một mô hình mã hai chiều và tự động hồi quy thể hiện sự thành thạo đặc biệt trong việc thực hiện một loạt rộng các tác vụ tóm tắt, tạo và dịch mã. Thiết kế của PLBART được lấy cảm hứng từ BART [22], một mô hình ngôn ngữ dựa trên tiền huấn luyện chuỗi-sang-chuỗi tự động mã hóa khử nhiễu. Với thiết lập tương tự, PLBART được huấn luyện trước bằng cách sử dụng ba mục tiêu tiền huấn luyện khử nhiễu, cụ thể là, che token, xóa token, và điền token. Hai chiến lược đầu liên quan đến việc lấy mẫu ngẫu nhiên các token và thay thế chúng bằng một token mask hoặc xóa chúng khỏi chuỗi đầu vào. Ngược lại, trong điền token, một phân phối Poisson (λ= 3.5) được sử dụng để vẽ độ dài của các đoạn văn bản được lấy mẫu và thay thế bằng một token mask duy nhất. Trong mỗi trường hợp, 35% của các token được che đi.

Trái ngược với CodeBERT và GraphCodeBERT, PLBART được huấn luyện trước trên một bộ sưu tập lớn các hàm Java và Python, cũng như các mô tả NL có nguồn gốc từ GitHub và StackOverflow. Dựa trên các đánh giá, PLBART đã chứng minh khả năng tổng quát hóa đáng chú ý và hiệu suất vượt trội khi áp dụng cho một số tác vụ downstream đa ngôn ngữ.

B. Knowledge Probing

Việc đánh giá các biểu diễn nội bộ và kiến thức của các mô hình ngôn ngữ là một quá trình cơ bản và quan trọng, liên quan đến kỹ thuật knowledge probing [23]. Phương pháp này bao gồm việc trình bày một tập hợp câu hỏi hoặc câu lệnh cho mô hình để đánh giá sự hiểu biết của nó về các khái niệm hoặc mối quan hệ cụ thể. Các đầu vào thường được trình bày như các câu cloze với các khái niệm hoặc mối quan hệ cụ thể được che đi như các prompt rời rạc để kiểm tra hiệu suất của mô hình. Chúng tôi chính thức hóa phương pháp knowledge probing bằng cách xem xét câu cloze đầu vào, được ký hiệu là S, nơi "Alan Turing was born in [MASK]" là một ví dụ.

Một cách chính thức, chúng tôi định nghĩa phương pháp knowledge probing như sau,

f(S) = 1/|S| Σ(k=1 to |S|) log(P(tk)|S;θ), tk ∈ V(θ) (1)

nơi θ biểu diễn các tham số mô hình, V(θ) ký hiệu từ vựng được học bởi mô hình ngôn ngữ, và tk là một token bên trong từ vựng V(θ) của mô hình. Khả năng ngữ cảnh hóa f(S) biểu diễn khả năng thay thế [MASK] bằng token tk theo dự đoán của mô hình. Dự đoán cuối cùng tương ứng với token tk mà tối đa hóa f(S).

Knowledge probing là một kỹ thuật thiết yếu để xác định các khu vực mà mô hình cần cải thiện, hiểu cách mô hình xử lý và biểu diễn thông tin, và khám phá kiến thức cơ bản. Hơn nữa, knowledge probing cho phép phát triển các mô hình ngôn ngữ mạnh mẽ và đáng tin cậy hơn phản ánh sự hiểu biết của con người về ngôn ngữ và thế giới.

Factual probing là một ứng dụng sớm của các phương pháp prompting trong xử lý ngôn ngữ tự nhiên, với mục tiêu định lượng kiến thức thực tế được mã hóa trong các mô hình ngôn ngữ được huấn luyện trước. Tác vụ này liên quan đến việc chuyển đổi đầu vào thành một prompt cloze, được tạo thủ công hoặc tự động phát hiện, để truy xuất kiến thức. Các tập dữ liệu liên quan như LAMA [24] và X-FACTR [25] đã được thiết lập để đánh giá các mô hình trong việc truy xuất thực tế. Các nhà nghiên cứu đã khám phá tìm kiếm template rời rạc [24], [26], [27] và học template liên tục, cũng như học ensemble prompt [12], [28], để cải thiện hiệu quả của factual probing. Tập hợp nghiên cứu hiện có chứng minh rằng các mô hình ngôn ngữ được huấn luyện trước chứa kiến thức thực tế đáng kể, có thể được truy cập hiệu quả bằng cách sử dụng các phương pháp prompting khác nhau.

C. Generation-based API Recommendation

API recommendation là một tác vụ đầy thách thức liên quan đến việc cung cấp một API cụ thể dựa trên một truy vấn NL. Nghiên cứu trước đây đã tập trung vào hai phương pháp: (1) Recommend dựa trên thứ hạng và (2) Recommend dựa trên tạo sinh. Recommend API dựa trên thứ hạng sử dụng một đồ thị kiến thức hoặc cơ sở kiến thức để tìm kiếm các API phù hợp nhất dựa trên ý nghĩa ngữ nghĩa của truy vấn NL [29]–[31]. Vì các phương pháp này không yêu cầu học, chúng tôi thảo luận về một hướng công trình khác nơi các phương pháp dựa trên deep-learning được sử dụng cho recommend API để tạo ra các chuỗi API sử dụng các truy vấn NL, gần với phạm vi công trình của chúng tôi.

DeepAPI [32] là người đầu tiên giải quyết tác vụ này bằng cách hình thức hóa nó như một vấn đề dịch máy và sử dụng một mô hình học có giám sát end-to-end. Sau đó, các nhà nghiên cứu đã điều tra hiệu quả của việc tinh chỉnh các mô hình mã được huấn luyện trước để thực hiện tác vụ học API [33], [34]. Mặc dù các nghiên cứu này chứng minh rằng các mô hình mã đạt được hiệu suất tốt hơn trong học API, chúng gặp phải bốn nhược điểm chính: (1) Các mô hình mã chỉ được tinh chỉnh với một vài API và không thể được tổng quát hóa để tạo ra API trong thực tế. (2) Tinh chỉnh các mô hình mã để tổng hợp các chuỗi API thiếu khả năng diễn giải để nắm bắt kiến thức API trong quá trình tiền huấn luyện. (3) Tác vụ học API dựa trên các đầu vào NL chỉ xác định sự hiểu biết ngữ nghĩa của việc sử dụng chuỗi API. (4) Đánh giá học API chỉ dựa trên điểm BLEU [35], đo lường sự tương tự giữa các chuỗi API được tổng hợp và tham chiếu và không phản ánh tính đúng đắn của tổng hợp.

III. INK: MỘT FRAMEWORK ĐÁNH GIÁ KIẾN THỨC TÊN API

A. Động lực

Nghiên cứu trước đây trong xử lý ngôn ngữ tự nhiên đã sử dụng các câu cloze để dự đoán token như một phương tiện diễn giải kiến thức được mã hóa bởi các mô hình ngôn ngữ được huấn luyện trước. Dựa trên công trình này, chúng tôi kiểm tra việc probing kiến thức tên API trong CodeBERT-MLM - một biến thể của CodeBERT được huấn luyện trước chỉ trên mô hình hóa ngôn ngữ mask - với các câu kiểu cloze phục vụ như câu đố, như được mô tả trong Hình 2. Chúng tôi sử dụng <tensorflow.compat.v2.boolean_mask> làm ví dụ và chuyển đổi nó thành một câu đố kiểu cloze, như được hiển thị trong Hình 2. Trong nghiên cứu này, chúng tôi định nghĩa các cấp độ module API như mỗi cấp độ phân cấp trong tên đầy đủ, được phân tách bằng dấu chấm. Có bốn cấp độ module trong câu lệnh gọi API: (1) <tensorflow> đại diện cho cấp độ module hàng đầu, (2) <compat> như cấp độ module thứ hai, (3) <v2> như cấp độ module thứ ba, và (4) <boolean_mask> như cấp độ gọi cuối cùng. Đối với mỗi cấp độ, chúng tôi yêu cầu CodeBERT-MLM điền vào chỗ trống thông qua dự đoán token đầu tiên, như được xác định bởi tokenizer của nó. Như kết quả của chúng tôi chứng minh, CodeBERT-MLM dự đoán chính xác token bị che đi trong lần thử đầu tiên, ngoại trừ cấp độ thứ ba của <v2>. Chúng tôi tranh luận rằng các mô hình mã, như CodeBERT-MLM, có thể học tên API trong quá trình tiền huấn luyện chức năng.

Cho rằng <tensorflow.compat.v2.boolean_mask> có thể được tái cấu trúc thành dạng của một câu lệnh nhập API, chúng tôi sau đó điều tra mức độ tốt của CodeBERT-MLM trong việc hiểu các câu lệnh nhập API. Như được chứng minh trong Hình 2, chúng tôi chuyển đổi câu lệnh nhập API thành bốn template cloze bằng cách che token đầu tiên của mỗi cấp độ module, tương tự như những template của lời gọi API. Từ kết quả được minh họa trong Hình 2, chúng tôi phát hiện rằng dự đoán một số module API trong lần bắn đầu tiên là thách thức đối với CodeBERT-MLM, không giống như trường hợp probing lời gọi API. Hành vi này gợi ý rằng các mô hình mã sở hữu các mức độ kiến thức khác nhau trong các câu lệnh nhập API và gọi API.

Sau khi xác định một số mẫu tiềm năng từ nghiên cứu knowledge probing sơ bộ của chúng tôi, cung cấp manh mối cho kiến thức tên API, chúng tôi thấy cần thiết phải khám phá thêm hiện tượng này thông qua phân tích định lượng và đánh giá có hệ thống. Được thúc đẩy bởi các quan sát nêu trên, bài báo này điều tra liệu các mô hình mã được huấn luyện trước có học tên API và mức độ nào chúng lưu trữ kiến thức tên API, bằng cách tiến hành knowledge probing như câu đố. Cụ thể, chúng tôi phân tích hai góc độ của tên API, tức là, gọi API và nhập API, trong phạm vi của các mô hình mã.

B. Chuyển đổi API-to-Pop-Quiz

Chúng tôi xem xét ba loại chuyển đổi chính để đánh giá kiến thức tên API: gọi API, nhập API và bí danh nhập API. Như đã đề cập trong Phần III-A, các câu đố kiểu cloze được cấu trúc dựa trên mỗi cấp độ gọi được phân tách bởi các dấu phân cách ".", "from" và "import". Để benchmark các mô hình một cách công bằng, chúng tôi xây dựng các câu đố bằng cách thống nhất toàn bộ từ vựng của mỗi mô hình. Đối với tất cả các đánh giá, chúng tôi tuân theo công trình trước đây về knowledge probing trong các mô hình ngôn ngữ [24] và chọn đánh giá dự đoán của việc che một token duy nhất trong câu đố. Chúng tôi cung cấp thiết kế chi tiết của mỗi quá trình như sau.

1) Thiết kế đánh giá trên gọi API:
Chúng tôi xử lý mỗi lời gọi API như một mẫu modular trên cơ sở của mỗi cấp độ module. Để chính thức hóa việc xây dựng câu đố trên các lời gọi API, một ví dụ về lời gọi API <A.B.C> và một mô hình mã M được đưa ra để chứng minh quy trình làm việc. Mô hình M đầu tiên tokenize API như sau,

M(<A.B.C>) → {t¹ᴬ, t²ᴬ, ..., tᴬₙₐ}, t_dot, {t¹ᴮ, t²ᴮ, ..., tᴮₙᵦ}, t_dot, {t¹ᶜ, t²ᶜ, ..., tᶜₙᶜ}

nơi mỗi t đại diện cho token được tạo ra bởi mô hình M, và N đại diện cho độ dài của các token trong mỗi cấp độ. Đối với mỗi cấp độ, các token được nhóm bởi {...}. Khi chuyển đổi API được tokenize thành câu đố, chúng tôi che một token cụ thể bằng cách thay thế t bằng "[MASK]" trong mỗi cấp độ. Để hình dung đầu vào câu đố, chúng tôi che token cuối cùng trong cấp độ module thứ hai của <A.B.C> như sau:

<A.B.C> → <A.B'[MASK].C> → <A.B'.C>

nơi B' là sự kết nối của {t¹ᴮ, ..., tᴮₙᵦ₋₁}. Chúng tôi nhắc mô hình M điền vào chỗ trống của <A.B'.C> thông qua dự đoán mask.

2) Thiết kế đánh giá trên nhập API:
Chúng tôi khám phá thiết kế đánh giá trên câu lệnh nhập API của "from...import...". Tương tự, chúng tôi xem xét ví dụ của <from A.B import C>. Sử dụng mô hình M để tokenize việc nhập API, chúng tôi có thể thiết kế các token sau:

M(<from A.B import C>) → t_from, {t¹ᴬ, t²ᴬ, ..., tᴬₙₐ}, t_dot, {t¹ᴮ, t²ᴮ, ..., tᴮₙᵦ}, t_import, {t¹ᶜ, t²ᶜ, ..., tᶜₙᶜ}

Chúng tôi hình dung một ví dụ về câu đố nhập API, nơi token đầu tiên trong cấp độ dưới cùng của <from A.B import C> được che:

<from A.B import C> → <from A.B import [MASK]C'> → <from A.B import C'>

nơi C' là sự kết nối của {t²ᶜ, ..., tᶜₙᶜ}. Chúng tôi probe mô hình M để điền vào chỗ trống của <from A.B import C'> thông qua dự đoán mask.

3) Thiết kế đánh giá trên bí danh nhập API:
Chúng tôi lưu ý rằng các bí danh nhập được hỗ trợ trong một số ngôn ngữ lập trình, như Python. Ví dụ, "import...as..." và "from...import...as..." là cú pháp bí danh nhập điển hình. Do đó, chúng tôi tiếp tục kiểm tra sự hiểu biết của mô hình mã được huấn luyện trước về các bí danh của các lời gọi API sau khi nhập các gói và thư viện. Chúng tôi minh họa lựa chọn thiết kế thông qua ví dụ của <import A as K \n K.B.C>, nơi <K> là bí danh và K.B.C là câu lệnh gọi API. Sau khi được tokenize bởi mô hình M, ví dụ được chính thức hóa như sau:

M(<import A as K \n K.B.C>) → t_import, {t¹ᴬ, t²ᴬ, ..., tᴬₙₐ}, t_as, {t¹ᴷ, t²ᴷ, ..., tᴷₙₐ}, t_newline, {t¹ᴷ, t²ᴷ, ..., tᴷₙₐ}, t_dot, {t¹ᴮ, t²ᴮ, ..., tᴮₙᵦ}, t_dot, {t¹ᶜ, t²ᶜ, ..., tᶜₙᶜ}

Chúng tôi sau đó chuyển đổi ví dụ thành câu đố bí danh API sau với token cuối cùng bị che của <C>:

<import A as K \n K.B.C> → <import A as K \n K.B.C'[MASK]> → <import A as K \n K.B.C'>

nơi C' là sự kết nối của {t¹ᶜ, ..., tᶜₙᶜ₋₁}. Chúng tôi probe mô hình M để điền vào chỗ trống của <import A as K \n K.B.C'> thông qua dự đoán mask. Lưu ý rằng chúng tôi chỉ che các token trong phần của lời gọi API sau tên bí danh.

4) Đánh giá các mô hình mã: Phương pháp từ vựng thống nhất:
Các mô hình mã có thể tokenize cùng một lời gọi API hoặc câu lệnh nhập khác nhau do sự khác biệt trong các từ vựng tương ứng của chúng. Ví dụ, một lời gọi API như <A.B.C> có thể được tokenize như một token duy nhất bởi một mô hình Mₐ với từ vựng Vₐ, trong khi một mô hình Mᵦ khác với từ vựng Vᵦ có thể tokenize nó thành nhiều token. Để đảm bảo so sánh công bằng, chúng tôi tạo ra các mô hình trên một từ vựng thống nhất là giao của các từ vựng của tất cả các mô hình mã được xem xét. Chúng tôi định nghĩa từ vựng thống nhất như tập hợp các token được trình bày trong từ vựng của tất cả các mô hình mã được xem xét.

Để đánh giá hiệu suất của các mô hình mã trên từ vựng thống nhất này, chúng tôi phân loại đánh giá của mỗi cấp độ tên thành hai loại: (1) che module một phần và (2) che module đầy đủ. Đối với loại trước, chúng tôi tokenize các câu lệnh gọi API và nhập thành nhiều token và chỉ che một trong các token. Ngược lại, khi các mô hình mã phân đoạn toàn bộ cấp độ module như một token duy nhất, chúng tôi ký hiệu nó như che module đầy đủ.

5) Benchmark các mô hình mã thông qua Knowledge Probing:
Chúng tôi sử dụng kỹ thuật knowledge probing để đánh giá và assess hiệu quả hiệu suất của các mô hình mã. Mục tiêu chính của phương pháp này là đi sâu vào sự hiểu biết của các mô hình mã về tên API và khả năng thành thạo của chúng trong việc tạo ra các token chính xác và phù hợp ngữ cảnh trong những tên đó. Để đạt được điều này, chúng tôi trình bày các câu đố kiểu cloze nơi một số token được cố ý che đi, thúc đẩy mô hình mã dự đoán các token cho các vị trí bị che. Để đánh giá độ chính xác của các dự đoán này, chúng tôi so sánh chúng với các token ground-truth trong tên API. Quá trình benchmark này cho phép chúng tôi xác định tính đúng đắn của các dự đoán.

IV. THIẾT LẬP THÍ NGHIỆM

Chúng tôi giới thiệu các câu hỏi nghiên cứu, thiết lập thí nghiệm cơ bản về các tập dữ liệu và mô hình, và các chỉ số đánh giá được sử dụng trong suốt quá trình đánh giá. Các câu hỏi nghiên cứu chúng tôi mong muốn trả lời bao gồm:

RQ1: Các mô hình mã hiểu các lời gọi API và nhập API tốt như thế nào?
RQ2: Các mô hình mã có hiểu các bí danh nhập API không?
RQ3: Ngữ cảnh ngôn ngữ tự nhiên có ảnh hưởng đến kết quả probing kiến thức tên API không?
RQ4: Các mô hình mã có thể ghi nhớ và tổng quát hóa trên tên API tốt như thế nào?

A. PyINK: Đánh giá về kiến thức tên API Python

[THIS IS TABLE: Bảng I showing overview of pop quizzes in PyINK with columns for First Token, Last Token, Full, Total across API Call, API Import, and API Import Alias rows]

Chúng tôi áp dụng kho ngữ liệu tiền huấn luyện được sử dụng rộng rãi, CSNet [21], một bộ sưu tập các tập dữ liệu và benchmark cho truy xuất mã ngữ nghĩa, chứa các hàm và các bình luận tương ứng của sáu ngôn ngữ lập trình được trích xuất từ các kho lưu trữ GitHub, để đánh giá kiến thức tên API của các mô hình mã. Chúng tôi tập trung vào tập Python, chứa 457,461 hàm. Để trích xuất các API Python từ CSNet, chúng tôi sử dụng framework INK được đề xuất, tận dụng phân tích tĩnh trên toàn bộ tệp để trích xuất việc sử dụng API, theo phương pháp được mô tả trong [36]. Để đạt được điều này, chúng tôi clone tất cả các kho lưu trữ tương ứng với các hàm trong CSNet và phân tích việc sử dụng API trong mỗi hàm. Quá trình này dẫn đến một benchmark mới, được ký hiệu là PyINK, được thiết kế để đánh giá kiến thức tên của các API Python. PyINK chứa 597,141 câu đố chính để đánh giá, và chúng tôi chỉ xem xét che module đầy đủ, hoặc che một phần của token đầu tiên và cuối cùng của mỗi cấp độ module để duy trì tính nhất quán.

Phương pháp của chúng tôi thành công trích xuất 79,754 API duy nhất từ 8,294 thư viện Python trong 13,519 kho lưu trữ, chỉ ra việc sử dụng đa dạng của các API Python. Hơn nữa, để đảm bảo so sánh công bằng, chúng tôi thống nhất các từ vựng trong các mô hình mã được xem xét, dẫn đến 289,585, 289,585, và 17,971 mẫu cho các câu lệnh gọi API, nhập, và nhập với bí danh, tương ứng, trong benchmark.

B. Các mô hình mã

Chúng tôi tiến hành nghiên cứu rộng rãi trên mười mô hình được chọn từ các biến thể của CodeBERT, GraphCodeBERT và PLBART. Chúng tôi cũng bao gồm GPT-3.5-turbo cho đánh giá, mặc dù nó không được huấn luyện cụ thể với dự đoán mask. Chúng tôi hướng dẫn GPT-3.5-turbo dự đoán top-20 token bị che cho mỗi thử nghiệm của 1,000 mẫu được chọn ngẫu nhiên trên các lời gọi API và nhập, tương ứng. Để hướng dẫn GPT-3.5-turbo thực hiện đúng tác vụ dự đoán, chúng tôi nhắc nó với hướng dẫn "Dự đoán top-20 câu trả lời của phần <mask> của tên đầy đủ API Python sau, chú ý đến các ký tự được kết nối ngay sau <mask>. Lưu ý số lượng ký tự bị che ít nhất là một. In mỗi trong 20 câu trả lời trên một dòng với chỉ số.".

[THIS IS TABLE: Bảng II showing overview of selected code models with columns for Model, pre-trained Dataset, Objective, #Param, Fine-tuned]

C. Chỉ số đánh giá

Chúng tôi trình bày một phương pháp đánh giá dựa trên các chỉ số dựa trên thứ hạng trong bối cảnh dự đoán tên API. Phương pháp của chúng tôi liên quan đến việc tính toán kết quả mỗi mẫu thử nghiệm và trung bình qua các câu đố, sử dụng chỉ số precision trung bình tại k (P@k). Cụ thể, P@k được tính là 1 nếu đối tượng mục tiêu được xếp hạng trong top k kết quả, và 0 ngược lại.

V. KẾT QUẢ

A. RQ1: Các mô hình mã hiểu các lời gọi API và nhập API tốt như thế nào?

[THIS IS TABLE: Bảng III showing P@k scores on selected code models, focusing on API calls and API imports, with detailed performance metrics]

Đánh giá của chúng tôi assess khả năng của các mô hình mã được huấn luyện trước để mã hóa kiến thức về tên API Python cho cả lời gọi API và nhập. Chúng tôi tính toán điểm P@k cho mỗi chiến lược che và trình bày kết quả trong Bảng III. Ngoài ra, chúng tôi cung cấp một vài ví dụ trong Bảng IV. Đầu tiên, chúng tôi đã quan sát rằng hiệu suất tương đối của các mô hình mã khác nhau vẫn nhất quán khi chúng tôi thay đổi giá trị k trong chỉ số P@k. Thứ hai, khi k tăng, sự cải thiện hiệu suất cho mỗi mô hình trở nên ít đáng kể hơn. Các quan sát này cung cấp bằng chứng mạnh mẽ để hỗ trợ hiệu quả của benchmark PyINK. Khi so sánh các biến thể mô hình, phân tích của chúng tôi tiết lộ rằng CodeBERT-MLM-Python và GraphCodeBERT-MLM chứng minh hiệu suất vượt trội trên các lời gọi API và nhập so với các mô hình khác. Tuy nhiên, precision tổng thể 30% của chúng được đo bằng P@1 không đạt được sự hoàn hảo, chỉ ra thiếu kiến thức về tên API. Mặc dù chúng tôi mong đợi GPT-3.5-turbo có sự hiểu biết tốt hơn về tên API, nó cho thấy rằng mô hình hoạt động hơi tệ hơn CodeBERT-MLM. Ngoài ra, so sánh của chúng tôi cho thấy các biến thể PLBART hoạt động tệ hơn nhiều trong việc hiểu kiến thức tên API Python so với các mô hình giống BERT, có thể được giải thích bởi các mục tiêu tiền huấn luyện của PLBART. PLBART-Large liên tục vượt trội hơn các biến thể khác, chỉ ra rằng kích thước mô hình có thể là một yếu tố quan trọng trong lượng kiến thức tên API được lưu trữ. Tuy nhiên, phát hiện này nên được diễn giải dưới ánh sáng của luật scaling của các mô hình ngôn ngữ mixed-modal [38], gợi ý rằng các mô hình lớn hơn có khả năng đạt được hiệu suất tốt hơn trên các tác vụ downstream, như tạo mã. Cuối cùng, chúng tôi thấy rằng dữ liệu được huấn luyện trước có thể ảnh hưởng đến sự hiểu biết về tên API ở một mức độ nào đó, như được thể hiện bởi khoảng cách hiệu suất giữa PLBART-Base và PLBART-CSNet. Kết quả của chúng tôi chỉ ra rằng tinh chỉnh trên các tác vụ tạo mã có thể cải thiện hiệu suất của các mô hình được huấn luyện trước, trong khi các tác vụ tạo văn bản có thể tác động tiêu cực đến chúng.

Phát hiện của RQ1: Mặc dù CodeBERT-MLM-Python và GraphCodeBERT-MLM cho thấy hiệu suất vượt trội trong kiến thức tên lời gọi API và nhập API giữa các mô hình mã, có một margin đáng kể cho sự cải thiện.

B. RQ2: Các mô hình mã có hiểu các bí danh nhập API không?

Để assess các mô hình mã về kiến thức của các bí danh nhập API, chúng tôi ghép đôi 17,971 câu đố bí danh nhập API với các ví dụ đối kháng được thiết kế để kiểm tra độ mạnh mẽ của mô hình. Để xây dựng tập đối kháng, chúng tôi chọn ngẫu nhiên 10 bí danh riêng biệt được sử dụng trong các module khác và thay thế các bí danh gốc trong các câu đố bằng các bí danh mới này. Ví dụ, "import numpy as np \n np.load(" sẽ được chuyển đổi thành "import numpy as pmd \n pmd.load(" thông qua việc thay thế "np". Cuối cùng, chúng tôi thu thập 179,710 câu đố đối kháng.

[THIS IS TABLE: Bảng V showing comparison of P@K scores on API import alias quizzes among selected code models]

Chúng tôi báo cáo kết quả P@K của mười mô hình trong Bảng V và minh họa các ví dụ trong Bảng VI. Dựa trên so sánh, CodeBERT-MLM-Python liên tục vượt trội hơn GraphCodeBERT-MLM, đạt được điểm P@1 cao hơn lên đến 24.17% cho các kịch bản Alias so với 18.91%. Các biến thể CodeBERT cũng cho thấy hiệu suất tổng thể tốt hơn với điểm P@50 từ 59.88% đến 68.37%, trong khi GraphCodeBERT-MLM từ 59.09% đến 62.17%. Phát hiện ban đầu của chúng tôi gợi ý rằng các mô hình mã có sự hiểu biết yếu hơn về các bí danh API so với các lời gọi API và nhập, như được hiển thị trong Bảng III. Điều này chỉ ra rằng các mô hình mã hiện tại mã hóa ít kiến thức về bí danh nhập. Dựa trên hiệu suất của mô hình GPT-3.5-turbo trên 1,000 câu đố được lấy mẫu ngẫu nhiên, chúng tôi có thể suy ra rằng nó có khả năng lớn hơn để hiểu các bí danh nhập API. Khi so sánh kết quả của các câu đố bí danh nhập API gốc với những câu đố của các bí danh đối kháng, chúng tôi thấy chỉ có sự khác biệt nhỏ, chỉ ra rằng các mô hình mã này có độ mạnh mẽ cao trong việc hiểu các bí danh nhập API. Chúng tôi tiếp tục phân tích phân phối của các bí danh nhập API và thấy rằng một API được ghép đôi với 1.16 bí danh trung bình, và 8% API có nhiều hơn 1 bí danh. Chúng tôi giả thuyết rằng các mô hình mã này có thể học các mẫu tổng hợp của các API này thông qua các bí danh khác nhau, và do đó quản lý để tổng quát hóa cho các bí danh nhập đối kháng.

Phát hiện của RQ 2: Mặc dù các mô hình mã cho thấy độ mạnh mẽ trong việc hiểu các bí danh nhập API, việc mã hóa kiến thức API của chúng bị giới hạn ở thông tin một phần.

C. RQ3: Ngữ cảnh ngôn ngữ tự nhiên có ảnh hưởng đến kết quả probing kiến thức tên API không?

Nghiên cứu của chúng tôi kiểm tra tác động của ngữ cảnh ngôn ngữ tự nhiên (NL) đến khả năng hiểu tên API của mô hình mã. Để xây dựng một tập dữ liệu về ngữ cảnh NL liên quan đến API, chúng tôi sử dụng các truy vấn NL được thiết kế cho việc sử dụng chuỗi API Python trong công trình của [33]. Ví dụ, <os.path.isfile> được ghép đôi với "file directory check". Chúng tôi chọn các truy vấn chứa chuỗi API có chứa API PyINK và chuyển đổi chúng thành các câu đố API. Vì một số truy vấn NL dài và không khả thi cho các mô hình mã xử lý, chúng tôi chỉ chọn 10 truy vấn ngắn nhất trong số các truy vấn NL thỏa mãn này, và lọc ra các trường hợp nơi độ dài không quá 512 token được xác định bởi tokenizer CodeBERT. Cuối cùng, chúng tôi thu thập 56,645 câu đố cho các lời gọi API, và 56,644 cho nhập API. Chúng tôi tính toán trung bình P@k trên mỗi câu đố tên API với nhóm truy vấn NL được nối trước. Chúng tôi so sánh chúng với kết quả tổng thể mà không thêm ngữ cảnh NL ở đầu trong Hình 4.

Kết quả của chúng tôi cho thấy rằng việc kết hợp các truy vấn ngôn ngữ tự nhiên dẫn đến sự cải thiện 2% trong việc probing kiến thức tên lời gọi API, mặc dù chúng tôi tranh luận rằng điều này có thể do thiết kế không tập trung vào API của chúng, vì chúng ban đầu được tạo ra cho các chuỗi API thay vì các lời gọi API riêng lẻ [33]. Ngoài ra, hiệu suất tương đối giữa các mô hình mã không thay đổi trước và sau khi thêm ngữ cảnh NL, gợi ý PyINK mạnh mẽ cho đánh giá kiến thức tên API. Chúng tôi dự đoán rằng việc kết hợp ngữ cảnh ngôn ngữ tự nhiên tập trung vào API sẽ mang lại những lợi ích đáng kể hơn.

Phát hiện của RQ3: Mặc dù việc thêm ngữ cảnh NL có liên quan có thể hỗ trợ các mô hình mã trong việc định vị kiến thức tên API chính xác hơn, nó không ảnh hưởng đến hiệu suất tương đối giữa các mô hình mã về mặt kiến thức tên API.

D. RQ4: Các mô hình mã có thể ghi nhớ và tổng quát hóa trên tên API tốt như thế nào?

Chúng tôi đánh giá liệu các mô hình mã có thể hiện sự hiểu biết sâu sắc hơn về tên của các API đã thấy trong quá trình tiền huấn luyện so với những API chưa thấy bằng cách tiến hành thí nghiệm trên CodeBERT-MLM, GraphCodeBERT-MLM, và PLBART-CSNet, được huấn luyện trước trên tập huấn luyện của CSNet. Để tạo ra phiên bản PyINK-Mem của chúng tôi, chúng tôi lấy tất cả API xuất hiện trong quá trình huấn luyện như phần chia đã thấy và các API còn lại như phần chia chưa thấy. Chúng tôi lọc ra tất cả API thuộc về các thư viện đã thấy trong phần chia chưa thấy. Phần chia đã thấy PyINK-Mem chứa 281,945 câu đố cho các lời gọi API và 281,945 câu đố cho nhập API. Phần chia chưa thấy có 7,640 câu đố lời gọi API và 7,640 câu đố nhập API. Chúng tôi lưu ý rằng các mô hình được chọn không ghi nhớ bất kỳ cấu trúc nào của các kho lưu trữ mã nguồn mở, do mục tiêu tiền huấn luyện cấp độ hàm.

[THIS IS TABLE: Bảng VII showing comparison of P@k scores on seen and unseen API name quizzes among selected code models]

Trong Bảng VII, chúng tôi đo hiệu suất mô hình thông qua P@k lên đến P@50. Kiểm tra của chúng tôi về kết quả trên các câu đố lời gọi API gợi ý có sự khác biệt nhỏ giữa các tập đã thấy và chưa thấy, chỉ ra khả năng tổng quát hóa mạnh mẽ của các mô hình mã này cho các API mới. Trong số ba mô hình chúng tôi đánh giá, CodeBERT-MLM chứng minh hiệu suất mạnh mẽ nhất, trong khi GraphCodeBERT-MLM chứng minh khả năng lớn hơn để ghi nhớ tên API trong quá trình tiền huấn luyện. Đáng ngạc nhiên, chúng tôi thấy rằng có 1,288 và 5,468 token ground-truth riêng biệt trong các phần chia đã thấy và chưa thấy cho các lời gọi API, tương ứng, và 1,257 token (97.59% của phần chia chưa thấy) được trùng lặp. Điều này chỉ ra rằng các thiết kế namespace API chia sẻ những điểm chung bất ngờ.

Phát hiện của RQ4: Các mô hình mã chứng minh khả năng tổng quát hóa ấn tượng trong việc dự đoán tên của các hàm lập trình cho các lĩnh vực mới và ghi nhớ hợp lý các API từ dữ liệu huấn luyện.

VI. THẢO LUẬN

Phần này chi tiết các ý nghĩa và tầm quan trọng của các thí nghiệm của chúng tôi, và thảo luận các mối đe dọa đến tính hợp lệ của chúng.

A. Ý nghĩa của đánh giá trên PyINK

Nghiên cứu của chúng tôi đánh giá kiến thức tên API trong các mô hình mã trên benchmark PyINK, bao gồm một phạm vi rộng của các API Python. Phân tích của chúng tôi tiết lộ rằng mặc dù các API này chia sẻ các mẫu thiết kế tương tự liên quan đến đặt tên đầy đủ, chúng cũng thể hiện các đặc điểm đa dạng. Quan sát này nhấn mạnh những thách thức liên quan đến việc mô hình hóa và hiểu thông tin tên liên quan đến các API này. Các thí nghiệm của chúng tôi trên PyINK cung cấp hiểu biết định tính về kiến thức tên API trong các mô hình mã được huấn luyện trước. Tuy nhiên, khó để suy ra lượng định lượng cụ thể, cho rằng các mô hình được chọn dựa trên các kiến trúc khác nhau. Để giải quyết điều này, chúng tôi đề xuất so sánh các mô hình từ cùng một họ sẽ hợp lý hơn.

Dựa trên đánh giá chi tiết của PyINK, chúng tôi thấy rằng các mô hình mã có thể lưu trữ một số kiến thức tên API trong khi học từ mã nguồn, nhưng kiến thức thường không đủ. Mặc dù các mô hình này đạt được precision hợp lý trong top-50 dự đoán, chúng có thể gặp khó khăn để trả lời các câu đố chính xác trong vài lần thử đầu tiên. Một phát hiện nhất quán qua các thí nghiệm khác nhau là các mô hình mã có thể không xử lý các lời gọi API và nhập tương tự, mặc dù hai cái này được rút ra từ cùng tên đầy đủ API.

Chúng tôi cũng chứng minh thực nghiệm rằng khả năng của mô hình mã để nắm bắt kiến thức tên API có thể bị ảnh hưởng bởi một số yếu tố, bao gồm các chiến lược tiền huấn luyện, tham số mô hình, dữ liệu tiền huấn luyện, và tinh chỉnh. Kết quả của chúng tôi từ RQ3 chỉ ra rằng các truy vấn ngôn ngữ tự nhiên có thể hỗ trợ các mô hình mã trong việc định vị thông tin tên API cụ thể ở một mức độ nào đó, nhấn mạnh tầm quan trọng của ngữ cảnh trong việc xây dựng phương pháp. Hơn nữa, chúng tôi nhấn mạnh rằng kiến thức tên API khác biệt với hầu hết kiến thức trong ngôn ngữ tự nhiên, nơi các mô hình mã có thể tổng quát hóa cho các tên API chưa thấy. So sánh, các mô hình ngôn ngữ liên tục hoạt động kém hơn khi tổng quát hóa cho các lĩnh vực mới [39], [40].

B. Điểm mạnh của framework đánh giá INK

Framework INK có hai điểm mạnh chính. Đầu tiên, nó có thể chẩn đoán sự hiểu biết của các mô hình mã về tên API. Mặc dù chúng tôi thấy rằng các prompt rời rạc có thể không đủ để hướng dẫn các mô hình như GPT-3.5 hoàn thành các câu đố, chúng tôi khuyến nghị sử dụng các ví dụ few-shot trong prompt để cải thiện hiệu suất. Đối với các mô hình mã tạo sinh công khai như CodeGen [41] và SantaCoder [42] có thể không thể thực hiện các tác vụ hiểu tên API trực tiếp, chúng tôi có thể sử dụng các chiến lược prompt tuning như P-tuning [43] để đánh giá hiệu quả hiệu suất của chúng.

Thứ hai, câu đố kiểu cloze của INK về hiểu tên API có thể là một tác vụ tiền huấn luyện và tinh chỉnh có giá trị cho các mô hình mã. Bằng cách học dự đoán tên API, các mô hình mã có thể phát triển biểu diễn tốt hơn của API và có thể được thích ứng dễ dàng hơn cho các tác vụ downstream khác nhau liên quan đến API, như tóm tắt mã về gọi API và gọi API trong quá trình hoàn thành mã.

C. Tầm quan trọng của việc diễn giải kiến thức tên API

Việc thu thập kiến thức về tên API là thiết yếu cho phát triển phần mềm, vì API đóng vai trò quan trọng trong việc tự động hóa các chức năng khác nhau. Tuy nhiên, các mô hình mã được sử dụng trong các benchmark hiện tại không xem xét đầy đủ việc sử dụng API khi đánh giá hiệu suất. Nghiên cứu của chúng tôi cho thấy rằng các mô hình mã thường gặp khó khăn để bảo tồn kiến thức tên API thiết yếu, dẫn đến nhu cầu cho các paradigm huấn luyện tăng cường kiến thức. Được lấy cảm hứng từ những tiến bộ gần đây trong các mô hình được huấn luyện trước tăng cường kiến thức trong xử lý ngôn ngữ tự nhiên và thị giác máy tính, chúng tôi đề xuất khám phá các phương pháp tích hợp đồ thị kiến thức API vào các mô hình mã. Học từ nghiên cứu về đồ thị kiến thức API [44]–[46] có triển vọng lớn cho việc thúc đẩy lĩnh vực này. Bằng cách kết hợp các phương pháp này vào các mô hình mã, chúng ta có thể cải thiện độ chính xác và hiệu quả của phát triển phần mềm tự động.

D. Các mối đe dọa đến tính hợp lệ

a) Lựa chọn thiết kế câu đố kiểu Cloze: Thiết kế câu đố được sử dụng liên quan đến việc dự đoán tên một phần hoặc đầy đủ của mỗi cấp độ module API. Có thể nhắc các mô hình mã dự đoán trên các token ở giữa khi một cấp độ API liên quan đến nhiều token. Tuy nhiên, việc che token đầu tiên và cuối cùng của mỗi cấp độ module là một phương pháp phù hợp để đánh giá mức độ kiến thức tên API được giữ lại bởi các mô hình mã, xác thực lựa chọn câu đố của chúng tôi. Mặc dù một số có thể chỉ trích các câu đố của chúng tôi vì nhắm mục tiêu dự đoán token đơn trong slot bị thiếu thay vì nhiều token, lựa chọn thiết kế này phù hợp với các công trình trước đây trong việc probing các mô hình ngôn ngữ [24]. Việc giới thiệu dự đoán multi-token có thể dẫn đến sự biến động nhiều hơn trong các chiến lược giải mã giữa các mô hình mã và khiến việc hoàn thiện chuỗi được kết hợp bởi tất cả các token được dự đoán trong quá trình đánh giá trở nên khó khăn. Mặc dù chúng tôi thừa nhận mối đe dọa tiềm năng này đến tính hợp lệ, chúng tôi để lại nó cho công trình tương lai.

b) Lựa chọn dữ liệu đánh giá: Lựa chọn dữ liệu đánh giá có thể ảnh hưởng đến kết quả thí nghiệm của việc chưng cất INK. Mặc dù chúng tôi sử dụng một kho ngữ liệu được sử dụng rộng rãi, CSNet, bao phủ một số lượng đáng kể API Python, quan trọng là thừa nhận rằng có các tài nguyên bổ sung, như Stack Overflow, có thể chứa nhiều API Python hơn. Hơn nữa, CSNet được đề xuất vào năm 2019, và các API mới có thể đã được phát triển kể từ đó. Chúng tôi tranh luận rằng đánh giá PyINK của chúng tôi sử dụng CSNet có ý nghĩa thống kê, nhưng chúng tôi cũng thừa nhận các hạn chế của kho ngữ liệu này. Hơn nữa, các mô hình mã có thể thể hiện hành vi khác nhau khi được đánh giá với các API trong các ngôn ngữ lập trình khác như Java và C. Để giải quyết mối đe dọa này đến tính hợp lệ, chúng ta có thể tăng cường tính đầy đủ của đánh giá bằng cách kết hợp nhiều ngôn ngữ lập trình hơn mà các mô hình mã này được huấn luyện. Bằng cách đánh giá các mô hình mã trên một phạm vi rộng hơn của các ngôn ngữ lập trình, chúng ta có thể đảm bảo tốt hơn độ mạnh mẽ và khả năng tổng quát hóa của chúng cho các tác vụ lập trình thực tế.

VII. KẾT LUẬN

Trong bài báo này, chúng tôi đã khám phá khả năng diễn giải của các mô hình mã cho mã nguồn (CodeBERT, GraphCodeBERT và PLBART). Chúng tôi tiến hành một phân tích kiến thức tên API toàn diện dựa trên một benchmark quy mô lớn, PyINK, từ bốn khía cạnh sau, nhằm mục đích đưa ra một diễn giải về các mô hình mã. Đầu tiên, chúng tôi xác định kiến thức tên API được lưu trữ bởi các mô hình mã từ hai góc độ, gọi API và nhập API. Thứ hai, chúng tôi điều tra liệu các mô hình mã có thể hiểu mạnh mẽ các bí danh nhập API hay không. Thứ ba, chúng tôi xem xét lại các thiết lập trong deep API learning và đánh giá liệu việc cung cấp ngữ cảnh ngôn ngữ tự nhiên bổ sung có thể giúp mô hình mã truy xuất kiến thức tên API chính xác hơn hay không. Thứ tư, chúng tôi kiểm tra việc ghi nhớ và tổng quát hóa của các mô hình mã trên tên API. Phân tích trong bài báo này đã tiết lộ một số phát hiện thú vị có thể truyền cảm hứng cho các nghiên cứu tương lai về học biểu diễn mã và diễn giải kiến thức được mã hóa bởi các mô hình mã.

TÀI LIỆU THAM KHẢO

[1] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., "Codebert: A pre-trained model for programming and natural languages," in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536–1547.

[2] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. LIU, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, "Graphcode{bert}: Pre-training code representations with data flow," in International Conference on Learning Representations, 2021. [Online]. Available: https://openreview.net/forum?id=jLoC4ez43PZ

[3] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, "A survey of machine learning for big code and naturalness," ACM Computing Surveys (CSUR), vol. 51, no. 4, pp. 1–37, 2018.

[4] F. F. Xu, B. Vasilescu, and G. Neubig, "In-ide code generation from natural language: Promise and challenges," ACM Trans. Softw. Eng. Methodol., vol. 31, no. 2, mar 2022. [Online]. Available: https://doi.org/10.1145/3487569

[5] C. Niu, C. Li, B. Luo, and V. Ng, "Deep learning meets software engineering: A survey on pre-trained models of source code."

[6] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021.

[7] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim et al., "Starcoder: may the source be with you!" arXiv preprint arXiv:2305.06161, 2023.

[8] OpenAI, "Gpt-4 technical report," 2023.

[9] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W.-t. Yih, D. Fried, S. Wang, and T. Yu, "Ds-1000: A natural and reliable benchmark for data science code generation," arXiv preprint arXiv:2211.11501, 2022.

[10] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y. Ma, G. Liang, Y. Li, T. Xie, and Q. Wang, "Codereval: A benchmark of pragmatic code generation with generative pre-trained models," arXiv preprint arXiv:2302.00288, 2023.

[11] W. Samek, T. Wiegand, and K.-R. Müller, "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models," arXiv preprint arXiv:1708.08296, 2017.

[12] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, "How can we know what language models know?" Transactions of the Association for Computational Linguistics, vol. 8, pp. 423–438, 2020.

[13] Y. Wan, W. Zhao, H. Zhang, Y. Sui, G. Xu, and H. Jin, "What do they capture? a structural analysis of pre-trained language models for source code," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 2377–2388.

[14] J. A. Hernández López, M. Weyssow, J. S. Cuadrado, and H. Sahraoui, "Ast-probe: Recovering abstract syntax trees from hidden representations of pre-trained language models," in 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 1–11.

[15] S. Troshin and N. Chirkova, "Probing pretrained models of source code," arXiv preprint arXiv:2202.08975, 2022.

[16] Q. Huang, Z. Yuan, Z. Xing, X. Xu, L. Zhu, and Q. Lu, "Prompt-tuned code language model as a neural knowledge base for type inference in statically-typed partial code," in 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 1–13.

[17] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, "Unified pre-training for program understanding and generation," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 2655–2668.

[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.

[19] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," Science China Technological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.

[20] A. Hindle, E. T. Barr, M. Gabel, Z. Su, and P. Devanbu, "On the naturalness of software," Communications of the ACM, vol. 59, no. 5, pp. 122–131, 2016.

[21] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt, "Codesearchnet challenge: Evaluating the state of semantic code search," arXiv preprint arXiv:1909.09436, 2019.

[22] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7871–7880.

[23] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," ACM Computing Surveys, vol. 55, no. 9, pp. 1–35, 2023.

[24] F. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller, "Language models as knowledge bases?" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 2463–2473.

[25] Z. Jiang, A. Anastasopoulos, J. Araki, H. Ding, and G. Neubig, "X-factr: Multilingual factual knowledge retrieval from pretrained language models," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 5943–5959.

[26] F. Petroni, P. Lewis, A. Piktus, T. Rocktäschel, Y. Wu, A. H. Miller, and S. Riedel, "How context affects language models' factual predictions," in Automated Knowledge Base Construction, 2020. [Online]. Available: https://openreview.net/forum?id=025X0zPfn

[27] E. Perez, D. Kiela, and K. Cho, "True few-shot learning with language models," Advances in neural information processing systems, vol. 34, pp. 11 054–11 070, 2021.

[28] G. Qin and J. Eisner, "Learning how to ask: Querying lms with mixtures of soft prompts," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 5203–5212.

[29] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, "Api method recommendation without worrying about the task-api knowledge gap," in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, 2018, pp. 293–304.

[30] M. M. Rahman, C. K. Roy, and D. Lo, "Rack: Automatic api recommendation using crowdsourced knowledge," in 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER), vol. 1. IEEE, 2016, pp. 349–359.

[31] F. Thung, S. Wang, D. Lo, and J. Lawall, "Automatic recommendation of api methods from feature requests," in 2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2013, pp. 290–300.

[32] X. Gu, H. Zhang, D. Zhang, and S. Kim, "Deep api learning," in Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering, 2016, pp. 631–642.

[33] J. Martin and J. L. Guo, "Deep api learning revisited," in Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension, 2022, pp. 321–330.

[34] M. A. Hadi, I. N. B. Yusuf, F. Thung, K. G. Luong, J. Lingxiao, F. H. Fard, and D. Lo, "On the effectiveness of pretrained models for api learning," in Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension, 2022, pp. 309–320.

[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.

[36] J. Wang, L. Li, and A. Zeller, "Restoring execution environments of jupyter notebooks," in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1622–1633.

[37] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., "Transformers: State-of-the-art natural language processing," in Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020, pp. 38–45.

[38] A. Aghajanyan, L. Yu, A. Conneau, W.-N. Hsu, K. Hambardzumyan, S. Zhang, S. Roller, N. Goyal, O. Levy, and L. Zettlemoyer, "Scaling laws for generative mixed-modal language models," arXiv preprint arXiv:2301.03728, 2023.

[39] M. Tänzer, S. Ruder, and M. Rei, "Memorisation versus generalisation in pre-trained language models," arXiv preprint arXiv:2105.00828, 2021.

[40] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang, "Quantifying memorization across neural language models," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=TatRHT_1cK

[41] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, "Codegen: An open large language model for code with multi-turn program synthesis," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=iaYcJKpY2B_

[42] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey et al., "Santacoder: don't reach for the stars!" arXiv preprint arXiv:2301.03988, 2023.

[43] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2022, pp. 61–68.

[44] H. Li, S. Li, J. Sun, Z. Xing, X. Peng, M. Liu, and X. Zhao, "Improving api caveats accessibility by mining api caveats knowledge graph," in 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2018, pp. 183–193.

[45] X. Ren, X. Ye, Z. Xing, X. Xia, X. Xu, L. Zhu, and J. Sun, "Api-misuse detection driven by fine-grained api-constraint knowledge graph," in Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 2020, pp. 461–472.

[46] X. Wang, X. Liu, J. Liu, X. Chen, and H. Wu, "A novel knowledge graph embedding based api recommendation method for mashup development," World Wide Web, vol. 24, no. 3, pp. 869–894, 2021.
