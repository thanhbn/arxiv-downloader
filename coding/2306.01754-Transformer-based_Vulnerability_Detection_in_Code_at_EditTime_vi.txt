# Phát hiện lỗ hổng bảo mật dựa trên Transformer trong mã nguồn tại EditTime:
Zero-shot, Few-shot, hay Fine-tuning?

Aaron Chan, Anant Kharkar, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy,
Alec Helyar, Eslam Kamal, Mohamed Elkamhawy, Neel Sundaresan
Microsoft
Redmond, USA

TÓM TẮT
Các lỗ hổng bảo mật phần mềm gây ra chi phí đáng kể cho các doanh nghiệp. Mặc dù đã có nhiều nỗ lực nghiên cứu và phát triển các phương pháp phát hiện lỗ hổng bảo mật phần mềm, các lỗ hổng chưa được phát hiện vẫn tiếp tục đặt chủ sở hữu và người dùng phần mềm vào tình trạng rủi ro. Nhiều phương pháp phát hiện lỗ hổng hiện tại yêu cầu các đoạn mã có thể biên dịch và xây dựng trước khi thực hiện phát hiện. Điều này, thật không may, tạo ra độ trễ dài giữa thời điểm lỗ hổng được chèn vào và thời điểm nó được loại bỏ, có thể làm tăng đáng kể chi phí sửa chữa lỗ hổng. Chúng tôi nhận ra rằng những tiến bộ hiện tại trong học máy có thể được sử dụng để phát hiện các mẫu mã dễ bị tổn thương trên các đoạn mã chưa hoàn chỉnh về mặt cú pháp khi nhà phát triển đang viết mã tại EditTime. Trong bài báo này, chúng tôi trình bày một hệ thống thực tế tận dụng học sâu trên tập dữ liệu quy mô lớn các mẫu mã dễ bị tổn thương để học các biểu hiện phức tạp của hơn 250 loại lỗ hổng và phát hiện các mẫu mã dễ bị tổn thương tại EditTime. Chúng tôi thảo luận về các cách tiếp cận zero-shot, few-shot và fine-tuning trên các Mô hình Ngôn ngữ Lớn (LLM) được đào tạo trước tiên tiến. Chúng tôi cho thấy rằng so với các mô hình phát hiện lỗ hổng tiên tiến, cách tiếp cận của chúng tôi cải thiện hiện trạng tốt nhất lên 10%. Chúng tôi cũng đánh giá cách tiếp cận của mình để phát hiện lỗ hổng trong mã được tự động tạo bởi các LLM mã. Đánh giá trên benchmark các tình huống mã rủi ro cao cho thấy giảm lỗ hổng lên đến 90%.

TỪ KHÓA
Transformers, Lỗ hổng Phần mềm, Phát hiện Lỗ hổng

1 GIỚI THIỆU
Mặc dù đã phát triển nhiều công cụ và thực hành tốt nhất [3,4,12,47], các lỗ hổng chưa được phát hiện vẫn tồn tại trong mã [1,2] và ảnh hưởng đến người dùng cũng như tốn thời gian và tiền bạc của các công ty [19]. Trong những năm gần đây, nhiều cách tiếp cận đã được phát triển để phát hiện các lỗ hổng trong phần mềm. Các cách tiếp cận này dao động từ phân tích động truyền thống [25,28,48], phân tích tĩnh dựa trên quy tắc [9,51], các giải pháp học máy dựa trên đặc trưng [33], đến các giải pháp dựa trên học sâu gần đây hơn [8, 30, 41, 53].

Các cách tiếp cận dựa trên phân tích động thường gặp vấn đề về độ bao phủ mã thấp. Các cách tiếp cận phân tích tĩnh dựa trên quy tắc thường liên quan đến nỗ lực thủ công của các chuyên gia để đặc trưng hóa và thêm các mẫu lỗ hổng mới hoặc đã phát triển một cách thường xuyên. Tương tự, các cách tiếp cận học máy dựa trên đặc trưng dựa vào các đặc trưng được chế tác thủ công bởi các chuyên gia con người. Gần đây hơn, các cách tiếp cận dựa trên học sâu đã xuất hiện như một giải pháp thay thế có thể học các mẫu lỗ hổng tự động mà không cần sự tham gia của chuyên gia. Tuy nhiên, phần lớn các cách tiếp cận học sâu này yêu cầu một tệp nguồn hoàn chỉnh [39], một hàm hoàn chỉnh [23,24,41], hoặc một câu lệnh mã hoàn chỉnh [11,43] để phân tích. Kết quả là, theo hiểu biết của chúng tôi, các cách tiếp cận học sâu hiện có không thể tìm thấy lỗ hổng tại EditTime khi nhà phát triển đang gõ và mã chưa đúng cú pháp hoặc chưa hoàn chỉnh.

Hình 1: Thời điểm tốt nhất để thông báo cho nhà phát triển về lỗ hổng SQL-Injection là tại EditTime, ngay sau khi nhà phát triển đã mắc lỗi.

Tài liệu cho thấy rằng chi phí sửa chữa lỗi có mối tương quan dương với thời gian bỏ qua lỗi (tức là thời gian từ khi lỗ hổng được chèn vào đến khi nó được loại bỏ) [5,6,20]. Do đó, nhà phát triển nên được thông báo về lỗi một cách kịp thời, khi lỗi có liên quan đến nhiệm vụ lập trình hiện tại [22]. Ví dụ, trong đoạn mã được hiển thị trong Hình 1, thời điểm tốt nhất để thông báo cho nhà phát triển về việc SQL injection họ đã đưa vào là ngay tại cuối truy vấn, mặc dù mã của họ chưa hoàn chỉnh về mặt cú pháp. Tầm quan trọng của việc xác định tương tác các lỗi và lỗ hổng trong mã tại EditTime đã được công nhận bởi công trình trước đây [50]. Công trình của chúng tôi mở rộng dòng công trình trước đây này bằng cách tận dụng những tiến bộ mới trong học sâu để phát hiện nhiều loại lỗ hổng trong các đoạn mã không hoàn chỉnh khi nhà phát triển đang gõ.

Để xây dựng giải pháp phát hiện lỗ hổng của chúng tôi, chúng tôi thu thập hơn 500K đoạn mã dễ bị tổn thương từ việc chạy các bộ phân tích tĩnh trên các kho lưu trữ mã nguồn mở. Sau đó chúng tôi phát triển sáu biến thể mô hình sử dụng các cách tiếp cận học tập phổ biến cho các LLM được đào tạo trước: zero-shot, few-shot và fine-tuning. Chúng tôi áp dụng các kỹ thuật học tập này trên CodeBERT [15] và hai LLM được đào tạo trước tiên tiến được cung cấp bởi OpenAI: code-davinci-002, text-davinci-003. Đánh giá so sánh của chúng tôi về các biến thể này cho thấy rằng fine-tuning CodeBERT mang lại sự cân bằng tốt nhất giữa độ chính xác và recall (độ chính xác 59% và recall 63%). Tuy nhiên, cả học zero-shot và few-shot trên text-davinci-003 gần đây, đây là mô hình ngôn ngữ dựa trên InstructGPT [35], đều mang lại recall tốt hơn (78% và 75%) và độ chính xác thấp hơn một chút (47% và 49%). Chúng tôi thảo luận về lợi ích và thách thức của mỗi cách tiếp cận học tập về kích thước mô hình và hiệu suất.

Chúng tôi tiến hành hai thí nghiệm sử dụng mô hình hiệu suất tốt nhất của chúng tôi. Thí nghiệm đầu tiên so sánh mô hình này với các cách tiếp cận phát hiện lỗ hổng hiện có trên bốn tập dữ liệu benchmark đã được thiết lập. Chúng tôi cho thấy rằng, so với các cách tiếp cận hiện có, cách tiếp cận của chúng tôi cải thiện recall lên tới 10% và độ chính xác lên tới 8%.

Thí nghiệm thứ hai nghiên cứu khả năng tổng quát hóa của cách tiếp cận của chúng tôi vượt ra ngoài việc chỉnh sửa mã thủ công bởi các nhà phát triển, nơi chúng tôi điều tra hiệu quả của cách tiếp cận của mình trong việc phát hiện lỗ hổng trong các chỉnh sửa mã tự động. Với việc áp dụng rộng rãi gần đây của các công cụ hoàn thành mã [17,45,46], nhiều chỉnh sửa mã trong tương lai dự kiến sẽ được tự động tạo bởi các LLM mã. Công trình trước đây cho thấy rằng các LLM mã mắc lỗi tương tự như các nhà phát triển [14], do đó các chỉnh sửa mã được tự động tạo có thể có những lo ngại bảo mật nghiêm trọng tiềm ẩn. Ví dụ, trong một số tình huống nhất định, lên đến 40% các hoàn thành được tạo bởi các LLM mã bao gồm các mẫu mã dễ bị tổn thương [38]. Chúng tôi đã đánh giá mô hình phát hiện lỗ hổng của mình trên một biến thể của benchmark được giới thiệu bởi Pearce et al. [38]. Đánh giá của chúng tôi cho thấy rằng việc sử dụng mô hình của chúng tôi mang lại 90% giảm tỷ lệ lỗ hổng.

Cuối cùng, chúng tôi thảo luận về hành trình triển khai mô hình phát hiện lỗ hổng của chúng tôi trong một extension VSCode đã dẫn đến 80% giảm tỷ lệ lỗ hổng trong mã được chỉnh sửa với extension.

Bài báo này có những đóng góp sau:
(1) chúng tôi trình bày một hệ thống phát hiện lỗ hổng chất lượng sản xuất có thể xác định lỗ hổng trong các đoạn mã không hoàn chỉnh trong vòng milliseconds và do đó phục vụ các nhà phát triển tại EditTime,
(2) chúng tôi khám phá và thảo luận về lợi ích và thách thức của ba cách tiếp cận học tập phổ biến cho các LLM được đào tạo trước trong nhiệm vụ phát hiện lỗ hổng: zero-shot, few-shot và fine-tuning,
(3) chúng tôi mở rộng benchmark được giới thiệu bởi Pearce et al. và cung cấp nó cho cộng đồng phát hiện lỗ hổng mã như một benchmark EditTime trong tương lai.

2 CÔNG TRÌNH LIÊN QUAN
Chúng tôi thảo luận về cách công trình của chúng tôi xây dựng và mở rộng công trình trước đây trong lĩnh vực Phát hiện Lỗ hổng, các phương pháp Học sâu cho Phát hiện Lỗ hổng, và Phát hiện Lỗ hổng trong mã được tự động tạo.

2.1 Phát hiện Lỗ hổng
Có hai phương thức phân tích mã nguồn để tìm kiếm lỗ hổng: phân tích động, trong đó một đoạn mã được thực thi và stack trace được kiểm tra để tìm chữ ký của một lỗ hổng cụ thể; và phân tích tĩnh, trong đó mã nguồn được phân tích mà không thực thi. Một phương pháp phổ biến của phân tích động là fuzzing, trong đó một chương trình được chạy trên nhiều đầu vào khác nhau để khám phá không gian của các trace thực thi chương trình. Các kỹ thuật tối ưu hóa để tìm kiếm hiệu quả không gian chương trình này là một lĩnh vực nghiên cứu tích cực [21,31,52]. Tuy nhiên, fuzzing các codebase lớn bằng cách thực thi mã tùy ý là không khả thi, và đã dẫn đến việc phát triển các kỹ thuật phân tích tĩnh. Các bộ phân tích tĩnh tìm kiếm các codebase cho các mẫu ngữ nghĩa của lỗi hoặc lỗ hổng [37]. CodeQL [18] là một bộ phân tích tĩnh có thể mở rộng phổ biến cho các tìm kiếm dựa trên mẫu như vậy. CodeQL cung cấp một ngôn ngữ truy vấn để viết các truy vấn dựa trên quy tắc cho các anti-pattern cụ thể. Chúng tôi xây dựng dựa trên dòng nghiên cứu này bằng cách phát triển một mô hình phát hiện lỗ hổng tận dụng các cách tiếp cận học sâu tiên tiến và do đó có khả năng học nhiều loại lỗ hổng mà không cần sự tham gia của các chuyên gia con người.

2.2 Phát hiện Lỗ hổng Học sâu
Vì ngôn ngữ tự nhiên và ngôn ngữ lập trình đều có cấu trúc tuần tự, các kiến trúc mô hình thành công trong NLP, như GRU, LSTM và Transformer, cũng đã cho thấy triển vọng trong phát hiện lỗ hổng [49]. Một đặc điểm độc đáo của mã nguồn, không được chia sẻ bởi ngôn ngữ tự nhiên, là cấu trúc đồ thị vốn có của các chương trình. VulDeePecker [30] trích xuất các code gadget từ các đồ thị phụ thuộc dữ liệu và đào tạo một bộ phân loại BiLSTM để phát hiện lỗ hổng trong một tập dữ liệu của hai loại lỗ hổng khác nhau. SySeVR [29] xây dựng trên VulDeePecker bằng cách sử dụng cả thông tin cú pháp và ngữ nghĩa để đào tạo các bộ phân loại lỗ hổng trên một tập dữ liệu của 126 loại lỗ hổng. Các công trình khác tận dụng mạng neural đồ thị để bổ sung cho các mô hình ngôn ngữ: IVdetect [27] sử dụng các embedding GRU với Feature-Attention Graph Convolutional Network (FA-GCN) để phát hiện lỗ hổng trong các đồ thị phụ thuộc chương trình. LineVul [16] mở rộng IVdetect bằng cách tận dụng CodeBERT được đào tạo trước [15] để thực hiện định vị lỗ hổng cấp dòng ngoài việc phát hiện cấp phương thức. Devign [53] học các embedding của các đồ thị biểu diễn mã và đào tạo một mạng neural tái phát đồ thị có cổng để phát hiện lỗ hổng cho các dự đoán cấp đồ thị. Nó đánh giá mô hình này trên các lỗ hổng từ FFmpeg và QEMU. ReVeal [8] giới thiệu một tập dữ liệu các lỗ hổng từ các dự án phần mềm thực tế như một benchmark cho các mô hình phát hiện lỗ hổng. Nó chứng minh benchmark này sử dụng một mạng neural đồ thị có cổng (GGNN) được đào tạo trên các đồ thị thuộc tính mã. Nghiên cứu của chúng tôi mở rộng công trình trước đây về phát hiện lỗ hổng bằng cách phát triển một mô hình phát hiện lỗ hổng neural cho bảy ngôn ngữ: JavaScript, Python, Go, Java, C++, C# và Ruby có khả năng phát hiện lỗ hổng tại EditTime.

2.3 Phát hiện Lỗ hổng trong Mã được Tự động tạo
Các mô hình dựa trên Transformer được đào tạo trên mã nguồn đã đạt được kết quả tiên tiến cho việc tạo mã [10,26,34]. Các mô hình này có tiện ích chưa từng có như các công cụ hỗ trợ nhà phát triển, và đang được triển khai vào các sản phẩm [17,45,46]. Vì các LLM mã này đã được đào tạo trên các tập dữ liệu lớn của mã do con người viết, các đầu ra mã của chúng ngày càng có khả năng tuân theo các mẫu (hoặc anti-pattern) của các nhà phát triển con người. Ví dụ, [14] đã đánh giá Codex trên các câu hỏi Java LeetCode và phát hiện rằng các hoàn thành không chính xác về mặt chức năng từ Codex chia sẻ các anti-pattern tương tự với mã không chính xác được viết bởi con người. Tương tự, [38] đã kiểm tra Codex về việc tạo lỗ hổng và phát hiện rằng, trong một số tình huống nhất định, Codex thực sự đã tạo ra các mẫu mã dễ bị tổn thương. Trong một nghiên cứu khác, [44] cho thấy rằng trong các bài kiểm tra chức năng với tập dữ liệu HumanEval, GitHub Copilot tạo ra một số CWE trong khoảng 2% các trường hợp. Các nghiên cứu này cho thấy rằng các LLM mã có thể đang học cách tạo ra cả mã tốt và xấu. May mắn thay, vì các mẫu mã dễ bị tổn thương hiếm trong mã của con người và do đó chiếm một phần nhỏ của dữ liệu đào tạo, chúng cũng hiếm trong các hoàn thành từ các LLM Mã. Ví dụ, một công trình gần đây nghiên cứu tác động của sự hỗ trợ LLM mã không tìm thấy bằng chứng kết luận nào rằng các LLM mã có nhiều khả năng tạo lỗ hổng hơn con người [42]. Tuy nhiên, với chi phí cao liên quan đến các lỗ hổng mã, việc giảm các mẫu mã dễ bị tổn thương được tạo bởi các LLM mã là cần thiết để đảm bảo rằng việc sử dụng kéo dài của chúng là an toàn, đặc biệt đối với các lập trình viên mới. Để đạt được mục tiêu này, nghiên cứu của chúng tôi tạo ra một mô hình có thể hoạt động trên mã chưa hoàn chỉnh về mặt cú pháp, và do đó phát hiện lỗ hổng trong các đoạn mã được tự động tạo cũng như các đoạn mã do nhà phát triển viết tại EditTime.

3 PHÁT HIỆN LỖ HỔNG TẠI EDITTIME
Để phát hiện lỗ hổng tại EditTime, chúng tôi phát triển sáu biến thể mô hình sử dụng các cách tiếp cận học tập phổ biến cho các LLM được đào tạo trước: zero-shot, few-shot và fine-tuning. Dưới đây, trước tiên chúng tôi giải thích dữ liệu đào tạo mà chúng tôi đã thu thập cho quá trình fine-tuning của chúng tôi. Sau đó chúng tôi giải thích các biến thể mô hình và hiệu suất tương ứng của chúng.

3.1 Thu thập Dữ liệu
Tập dữ liệu của chúng tôi bao gồm các mẫu mã dễ bị tổn thương được phát hiện trong các kho lưu trữ GitHub công cộng bởi dịch vụ GitHub LGTM. Dịch vụ GitHub LGTM chạy CodeQL [18], một bộ phân tích tĩnh có thể mở rộng, trên các kho lưu trữ GitHub công cộng để xác định nhiều mẫu mã dễ bị tổn thương, bao gồm thông tin đăng nhập được mã hóa cứng, SQL injection và path injection. Trong công trình này, chúng tôi chọn một tập con của các vấn đề CodeQL được phát hiện tương ứng với một tập "Common Weakness Enumeration" (CWE) [32] trong mỗi trong bảy ngôn ngữ: JavaScript, Python, Go, Java, C++, C# và Ruby. Bảng 1 cho thấy các thống kê tóm tắt của tập dữ liệu được tuyển chọn.

Bảng 1: Thống kê tóm tắt của tập dữ liệu đào tạo phát hiện lỗ hổng

Phạm vi Mô hình | Dễ bị tổn thương | Không dễ bị tổn thương
Javascript 70 CWE | 266,342 | 2,293,712
Python 37 CWE | 149,158 | 1,493,972
Go 29 CWE | 50,233 | 535,180
Java 44 CWE | 33,485 | 431,726
C++ 32 CWE | 7,222 | 215,722
C# 54 CWE | 3,341 | 27,731
Ruby 19 CWE | 137 | 1,957

Mỗi lỗ hổng được phát hiện chứa các thông tin sau:
• Tiêu đề lỗ hổng: loại lỗ hổng được phát hiện, tương ứng với một danh mục CWE
• Thông báo: một thông báo lỗi chi tiết giải thích lỗ hổng được phát hiện
• Vị trí: đường dẫn tệp, dòng và cột nơi vấn đề bắt đầu và kết thúc

3.2 Tiền xử lý Dữ liệu
Với dữ liệu chúng tôi đã thu thập, mục tiêu của bước tiền xử lý dữ liệu của chúng tôi là tổng hợp các bộ ba sau: (ci, vi, li) trong đó c là một đoạn mã mà mô hình nhận làm ngữ cảnh, v là khối mã dễ bị tổn thương, và l là nhãn cho loại lỗ hổng.

Quá trình của chúng tôi để tổng hợp dữ liệu đào tạo như sau: trước tiên chúng tôi thu thập các tệp có lỗ hổng được xác định từ tất cả 8,815 kho lưu trữ mà các vấn đề này được phát hiện. Đối với mỗi tệp, chúng tôi trích xuất Cây Cú pháp Trừu tượng (AST), và tìm kiếm các nút của cây chứa các phạm vi hoàn chỉnh. Cụ thể, chúng tôi tìm kiếm các câu lệnh (như các câu lệnh if và export), phương thức, khai báo và mệnh đề.

Đối với các khối mã này, nếu khối mã chứa một mẫu mã dễ bị tổn thương, chúng tôi ngẫu nhiên chia khối tại một số ký tự trước khi bắt đầu lỗ hổng được phát hiện. Ngược lại, nếu không có lỗ hổng nào được phát hiện, chúng tôi ngẫu nhiên chia khối tại bất kỳ điểm nào. Quá trình chia này mô phỏng một trạng thái mã có thể tại EditTime. Do đó, các mô hình được đào tạo trên dữ liệu này sẽ có thể phát hiện một khối dễ bị tổn thương trong một đoạn mã không chính xác và không hoàn chỉnh về mặt cú pháp. Phần đầu tiên của khối được gắn nhãn là "ngữ cảnh" và phần thứ hai là "khối dễ bị tổn thương". Hình 2 cho thấy một ví dụ về một cặp ngữ cảnh và khối dễ bị tổn thương. Nếu việc hoàn thành chứa một lỗ hổng, loại lỗ hổng tương ứng được gán cho ví dụ như một nhãn. Sự phân tách này giữa ngữ cảnh và khối dễ bị tổn thương buộc mô hình phải tập trung vào khối dễ bị tổn thương dựa trên ngữ cảnh. Chúng tôi đã sử dụng việc chia 85/5/10 train-validation-test ở cấp kho lưu trữ, sao cho tập đào tạo bao gồm 85% các kho lưu trữ, tập validation bao gồm 5% các kho lưu trữ và tập test bao gồm 10% các kho lưu trữ. Sau đó chúng tôi loại bỏ bất kỳ ví dụ nào trong tập đào tạo khớp với ví dụ tập test.

Hình 2: Một mẫu ngữ cảnh và khối dễ bị tổn thương từ dữ liệu đào tạo của chúng tôi. Trong ví dụ này, khối dễ bị tổn thương chứa lỗ hổng SQL-Injection.

3.3 Mô hình
Để phát hiện lỗ hổng tại EditTime, chúng tôi phát triển sáu biến thể mô hình sử dụng ba cách tiếp cận học tập cho LLM: học zero-shot, học few-shot và fine-tuning. Chúng tôi sử dụng các cách tiếp cận học tập này trên ba LLM được đào tạo trước làm cơ sở:

• code-davinci-002: mô hình Codex kích thước đầy đủ được đào tạo trên mã nguồn từ GitHub.
• text-davinci-003: mô hình Codex dựa trên InstructGPT [36], sử dụng học tăng cường từ phản hồi của con người (RLHF).
• CodeBERT: transformer được đào tạo trước trên dữ liệu bimodal (mã và tài liệu) [15]. Kiến trúc của nó tuân theo RoBERTa-base [13], với 12 lớp, 12 attention head và kích thước ẩn là 768.

Sử dụng ba cách tiếp cận học tập trên các mô hình được đào tạo trước ở trên tạo ra sáu mô hình được liệt kê trong Bảng 3. Chúng tôi giải thích chi tiết việc phát triển các mô hình này dưới đây.

3.3.1 Học Zero-shot. Trong học zero-shot, chúng tôi cung cấp cho một mô hình được đào tạo trước một prompt chỉ định đầu ra mong muốn của chúng tôi. Trong trường hợp của chúng tôi, chúng tôi prompt code-davinci-002 và text-davinci-003. Để có được một prompt phù hợp để yêu cầu mỗi mô hình phát hiện lỗ hổng, chúng tôi tận dụng định nghĩa của chính mô hình về phát hiện lỗ hổng. Trước tiên chúng tôi prompt mô hình để định nghĩa nhiệm vụ phát hiện lỗ hổng và sử dụng mô tả nhiệm vụ này trong prompt cuối cùng cho nhiệm vụ.

Đối với code-davinci-002, chúng tôi prompt mô hình với "# We run CodeQL security queries in order to " ở nhiệt độ 0.8 trong bốn lần và chọn kết quả hàng đầu. Điều này tạo ra các biến thể prompt sau: "identify potential security vulnerabilities", "find potential security issues", "find security vulnerabilities", và "detect security vulnerabilities". Chúng tôi đặt các biến thể này trong một prompt zero-shot theo template dưới đây:

<phrase>
<comment> Code snippet
<code snippet>
<comment> Answer (Yes/No, explanation):

Ở đây <phrase> tương ứng với một trong các biến thể cách diễn đạt prompt ở trên, <comment> đề cập đến một chỉ báo bình luận cụ thể ngôn ngữ (ví dụ: "#" cho Python) và <code snippet> đề cập đến đoạn mã được đề cập. Hình 3 cho thấy một prompt mẫu theo template này. Để đáp ứng prompt này, mô hình xuất ra Yes hoặc No, theo sau là một giải thích có thể được sử dụng cho mục đích quan sát và debug. Chúng tôi kiểm tra sự hiện diện của "Yes" hoặc "No" để xác định quyết định của mô hình. Chúng tôi gọi cách tiếp cận zero-shot này là CodexZero.

Hình 3: Một prompt mẫu được tạo dựa trên template của chúng tôi cho thiết lập zero-shot

Tương tự, đối với text-davinci-003, trước tiên chúng tôi prompt mô hình bốn lần ở nhiệt độ 0.8 với câu hỏi, "What would you accomplish by running CodeQL security queries?". Sau đó chúng tôi yêu cầu mô hình diễn đạt lại phản hồi của nó bốn lần. Điều này tạo ra các cụm từ độc đáo sau: "identify potential security vulnerabilities", "spot any security weaknesses", "detect any security risks", "determine any security issues". Sau đó chúng tôi thử các biến thể này trong một prompt zero-shot theo template tương tự:

<phrase>
<code snippet>
Answer (Yes/No):

Chúng tôi kiểm tra sự hiện diện của "Yes" hoặc "No" trong phản hồi của mô hình để xác định quyết định của mô hình. Chúng tôi gọi cách tiếp cận zero-shot này là TextZero.

3.3.2 Học Few-shot. Học few-shot xây dựng trên học zero-shot bằng cách cung cấp các cặp ví dụ đầu vào-đầu ra, ngoài prompt zero-shot. Đối với nghiên cứu của chúng tôi, chúng tôi sử dụng các biến thể prompt hiệu suất tốt nhất của code-davinci-002 và text-davinci-003 từ học zero-shot của chúng tôi trong cùng định dạng template. Sau đó chúng tôi thêm các ví dụ bổ sung vào đầu trong cùng định dạng template trước khi cuối cùng chèn đoạn mã quan tâm và prompt mô hình để có câu trả lời.

Để tạo các ví dụ, chúng tôi prompt mỗi mô hình với cụm từ, "Provide an example in <Language> of a code snippet that contains <Vulnerability Name> security vulnerability. Output the code only, do not include text:" cho mỗi ngôn ngữ và loại lỗ hổng trong Bảng 2. Chúng tôi prompt với template này ba lần cho mỗi cặp lỗ hổng và ngôn ngữ, tạo ra 150 ví dụ dễ bị tổn thương. Sau đó chúng tôi prompt mô hình với "Provide an example in <Language> of a code snippet. Output the code only, do not include text:", để lấy các mẫu mã không dễ bị tổn thương. Chúng tôi đánh giá thủ công mỗi mẫu để đảm bảo rằng chúng dễ bị tổn thương hoặc không dễ bị tổn thương như dự định. Tổng cộng, có 297 ví dụ. Chúng tôi gọi các mô hình kết quả từ quá trình học few-shot ở trên với code-davinci-002 và text-davinci-003 là CodexFew và TextFew, tương ứng.

3.3.3 Fine-tuning. Đối với fine-tuning, chúng tôi tập trung chỉ vào các LLM mã được đào tạo trước và fine-tune CodeBERT và code-davinci-002. Đối với CodeBERT, chúng tôi thêm một head phân loại tuyến tính vào trunk BERT của nó để xây dựng một mô hình phân loại đa lớp. Các đầu vào cho mô hình là ngữ cảnh và khối mã dễ bị tổn thương, được phân tách bởi token đặc biệt [SEP] và được bao quanh bởi token bắt đầu chuỗi [BOS] và token kết thúc chuỗi [EOS]

[BOS], c1, c2, ...cn, [SEP], v1, v2, ...vm, [EOS]

trong đó c1, c2, ...cn biểu thị một chuỗi n token của mã ngữ cảnh xung quanh lỗ hổng và v1, v2, ...vm biểu thị một chuỗi m token của khối mã dễ bị tổn thương. Chúng tôi phân biệt giữa ngữ cảnh và khối dễ bị tổn thương để cho phép mô hình xử lý bất kỳ đoạn mã hoàn chỉnh hoặc không hoàn chỉnh nào được đưa ra. Vì dữ liệu của chúng tôi rất mất cân bằng với ít hơn 10% ví dụ dễ bị tổn thương, chúng tôi sử dụng kỹ thuật oversampling trên các ví dụ dễ bị tổn thương trong khi đào tạo: đối với mỗi epoch, tất cả các ví dụ dễ bị tổn thương được giữ lại trong khi các ví dụ không dễ bị tổn thương được xoay vòng sao cho mỗi epoch chứa 50% dễ bị tổn thương và 50% ví dụ không dễ bị tổn thương. Mô hình được đào tạo với một loss binary cross entropy (BCE) tiêu chuẩn. Trong phần còn lại của công trình này, chúng tôi gọi mô hình này là DeepDevVuln.

Mô hình fine-tuned thứ hai của chúng tôi là một phiên bản fine-tuned của code-davinci-002 trên 30,000 ví dụ được lấy mẫu ngẫu nhiên từ tập đào tạo của chúng tôi. Chúng tôi giảm kích thước dữ liệu đào tạo do chi phí fine-tuning mô hình trên toàn bộ tập đào tạo. Trong trường hợp này, ngữ cảnh và khối dễ bị tổn thương được nối với nhau, sau đó một token phân loại đặc biệt được thêm vào cuối chuỗi. Vì code-davinci-002 là một mô hình decoder GPT không thực hiện phân loại một cách vốn có, chúng tôi sử dụng dự đoán token tiếp theo như một proxy, tức là đầu ra là một token đặc biệt báo hiệu dễ bị tổn thương hay không. Chuỗi như sau:

[BOS], c1, c2, ...cn, v1, v2, ..., vn[CLS][VULN]

Chúng tôi gọi biến thể này là CodexVuln.

Bảng 2: Thống kê tóm tắt các vấn đề dễ bị tổn thương được thu thập từ GitHub PR

Lỗ hổng | CWE | N
SQL Injection | 89 | 45
Hardcoded Credentials | 798 | 23
Code Injection | 94 | 13
Path Injection | 22 | 7
Clear Text Logging | 312 | 5
Weak Cryptographic Algorithm | 327 | 5
Incomplete URL Substring Sanitization | 20 | 2

3.4 Đánh giá Biến thể Mô hình
Để hiểu rõ hơn về tác động của kiến trúc mô hình và lựa chọn đào tạo, chúng tôi so sánh sáu biến thể mô hình của chúng tôi trên một tập dữ liệu mà chúng tôi thu thập từ các pull request GitHub.

3.4.1 Metrics. Phần lớn các cách tiếp cận hiện có đối xử với phát hiện lỗ hổng như một vấn đề phân loại. Với một khối mã, mô hình đưa ra quyết định nhị phân về việc mã có dễ bị tổn thương hay không. Do đó, trong các đánh giá của chúng tôi, chúng tôi cũng sử dụng các metric đánh giá phổ biến nhất cho một nhiệm vụ phân loại [40] bao gồm:

• Precision: chỉ ra độ chính xác của dự đoán và được tính là true positive / (true positive + false positive)
• Recall: chỉ ra hiệu quả của dự đoán và được tính là true positive / (true positive + false negative).
• F1-score: chỉ ra sự cân bằng giữa precision và recall và được định nghĩa là trung bình hình học của hai giá trị.

Bảng 3: Hiệu suất của mô hình DeepDevVuln trên tập dữ liệu Github PR Vulnerabilities.

Mô hình | Precision | Recall | F1-Score
DeepDevVuln | 58.87% | 63.00% | 60.87%
CodexVuln | 69.56% | 48.00% | 56.80%
CodexZero | 11.08% | 98.00% | 19.90%
TextZero | 46.99% | 78.00% | 58.65%
CodexFew (8 examples) | 23.91% | 95.00% | 37.70%
TextFew (6 examples) | 49.01% | 75.00% | 59.29%

3.4.2 Tập dữ liệu. Để tạo tập dữ liệu này, chúng tôi thu thập tất cả các pull request có chứa "fix <issue>" trong tiêu đề pull request cho mọi kết hợp của issue từ Bảng 2 và ngôn ngữ (tức là JavaScript, Python, Go, Java, C++, C#, Ruby). Để đảm bảo rằng các pull request được lấy có liên quan đến lỗ hổng mục tiêu, chúng tôi chỉ bao gồm các pull request có chứa cả việc loại bỏ và thêm mã và có độ dài ít hơn 100 dòng. Điều này tạo ra 283 pull request. Sau đó chúng tôi kiểm tra thủ công mỗi pull request để đảm bảo các lỗ hổng là hợp lệ.

Sử dụng quy trình này, chúng tôi thu thập một tập 100 ví dụ dễ bị tổn thương tạo thành các CWE trong Bảng 2. Chúng tôi đã thêm một tập 906 ví dụ không dễ bị tổn thương vào tập dữ liệu này. Để thu thập các ví dụ không dễ bị tổn thương, chúng tôi lấy mẫu ngẫu nhiên 150 tệp từ các kho lưu trữ mà CodeQL đã quét và không phát hiện vấn đề. Sau đó chúng tôi chọn ngẫu nhiên các đoạn mã không chồng chéo từ 1 đến 10 dòng từ các tệp này để tạo các ví dụ không dễ bị tổn thương. Tổng cộng, tập dữ liệu GitHub PR chứa 1,006 ví dụ.

Hình 4: Một mẫu false positive của CodexZero do overreach

Hình 5: Một mẫu false positive của CodexVuln do thiếu ngữ cảnh

3.4.3 Kết quả. Bảng 3 cho thấy hiệu suất của các biến thể mô hình của chúng tôi trên tập dữ liệu này. Đối với sáu biến thể mô hình này, chúng ta có thể thấy rằng DeepDevVuln có hiệu suất tốt nhất về F1 score. Mọi biến thể zero-shot và few-shot đều vượt trội hơn DeepDevVuln về recall, tuy nhiên precision của mỗi biến thể thấp hơn đáng kể. Một lý do có thể là các mô hình zero-shot và few-shot có định nghĩa rộng hơn về những gì tạo thành mã dễ bị tổn thương. Ví dụ, đoạn mã được trình bày trong Hình 4 dưới đây không có bất kỳ lỗ hổng rõ ràng nào, tuy nhiên, CodexZero xem xét trường hợp xấu nhất rằng mã có thể dễ bị tổn thương với buffer overflow trong tương lai. Do xu hướng này, có vẻ như CodexZero sẽ đưa ra cảnh báo ngay cả khi không có lỗ hổng rõ ràng nào hiện diện, điều này dẫn đến điểm recall cao nhưng điểm precision thấp.

Một ví dụ khác giải thích thêm về tỷ lệ false positive cao của CodexZero. Trong Hình 5, chúng ta thấy rằng mô hình thiếu ngữ cảnh xung quanh biến md, và giải thích rằng vì nó chưa bao giờ được khởi tạo, việc truy cập bộ nhớ tại vị trí đó có thể dẫn đến lỗ hổng. Giải thích này mở rộng, một phần, đến TextZero, có precision và recall cân bằng hơn. Trong trường hợp của TextZero, biến thể prompt được sử dụng có tác động đáng kể: ví dụ, biến thể prompt đầu tiên "identify potential security vulnerabilities" có kết quả tương tự như CodexZero. Tuy nhiên, việc sử dụng cụm từ "detect any security risks" đã dẫn đến kết quả trong Bảng 3. Điều này có thể đã khuyến khích mô hình tập trung vào đoạn mã chính xác như vậy, thay vì suy đoán về các lỗ hổng có thể.

So sánh kết quả few-shot và zero-shot, chúng ta thấy các đánh giá few-shot vượt qua các đối tác zero-shot của chúng. Một giải thích là việc bao gồm các ví dụ mang lại cho mô hình cảm giác về những gì mong đợi về độ dài đoạn mã và khu vực tập trung. Ví dụ, khi các ví dụ không dễ bị tổn thương bao gồm các biến chưa được khởi tạo và ngữ cảnh không hoàn chỉnh, mô hình bắt đầu bỏ qua các kịch bản trường hợp xấu nhất được giải thích ở trên.

Tìm số lượng ví dụ tốt nhất là vấn đề của việc tăng dần số lượng ví dụ được thêm vào đầu prompt từ 1 đến 9. Chúng tôi đã chạy một số thử nghiệm và báo cáo kết quả tốt nhất. Trong số các thử nghiệm này, chúng tôi quan sát thấy rằng, đặc biệt đối với CodexFew, có mối tương quan rõ ràng giữa số lượng ví dụ lớn hơn và precision và recall đạt được như được thấy trong Hình 6. Có thể các loại ví dụ cụ thể được chọn có vai trò chính trong việc định hướng đầu ra của mô hình. Một xu hướng tương tự cho precision, nhưng không phải recall, đã được quan sát cho TextFew, Hình 7.

4 THÍ NGHIỆM
Dưới đây, chúng tôi giải thích các thí nghiệm của chúng tôi được thiết kế để trả lời hai câu hỏi nghiên cứu sau:

• RQ1: Mô hình phát hiện lỗ hổng của chúng tôi hiệu quả như thế nào so với kết quả từ các mô hình tiên tiến trên các tập dữ liệu benchmark đã được thiết lập?
• RQ2: Mô hình phát hiện lỗ hổng được đề xuất của chúng tôi hiệu quả đến mức nào trong việc giảm tỷ lệ lỗ hổng của các LLM mã?

Chúng tôi thực hiện hai thí nghiệm để trả lời các câu hỏi này. Thí nghiệm đầu tiên tập trung vào việc so sánh mô hình của chúng tôi với các mô hình phát hiện lỗ hổng tiên tiến trên các tập dữ liệu chung. Thí nghiệm thứ hai đánh giá hiệu quả của mô hình của chúng tôi trong việc giảm tỷ lệ lỗ hổng của các LLM mã.

4.1 Thí nghiệm 1: Hiệu suất so với Các cách tiếp cận Hiện có trên Tập dữ liệu Benchmark
Trong phần này, chúng tôi trình bày đánh giá thí nghiệm của mô hình phát hiện lỗ hổng của chúng tôi so với các cách tiếp cận tiên tiến hiện có trên bốn tập dữ liệu được sử dụng rộng rãi. Bảng 4 tóm tắt các tập dữ liệu được sử dụng trong thí nghiệm này.

4.1.1 Thiết lập Thí nghiệm. Độ chi tiết của dữ liệu, chất lượng dữ liệu và các loại vấn đề được bao phủ trong mỗi tập dữ liệu trong Bảng 4 khác với tập đào tạo và test của chúng tôi. Do đó, chúng tôi đã tuân theo cách tiếp cận của Chakraborty et al. [8], nơi chúng tôi trước tiên loại bỏ các ví dụ trùng lặp từ mỗi tập dữ liệu. Sau đó chúng tôi fine-tune DeepDevVuln như một mô hình phân loại nhị phân trên mỗi tập dữ liệu trong 10 epoch. Đối với mỗi tập dữ liệu, chúng tôi sử dụng việc chia 80%/10%/10% train/validation/test tiêu chuẩn, phù hợp với các mô hình baseline.

4.1.2 Kết quả. Như được hiển thị trong Bảng 5, mô hình DeepDevVuln của chúng tôi có F1-Score tổng thể tốt nhất cho phần lớn các tập dữ liệu. Điều này có nghĩa là mô hình của chúng tôi thể hiện sự cân bằng tốt giữa precision và recall, điều quan trọng đối với phát hiện lỗ hổng. Ngoài ra, mô hình phát hiện lỗ hổng của chúng tôi có ít hơn 10 lần số tham số so với GPT2-Large hoặc Codex, nhưng vẫn đạt được precision tương đương và recall cao hơn. Nhìn chung, kết quả này cho thấy rằng cách tiếp cận của chúng tôi đã cho phép mô hình của chúng tôi thích ứng với các loại vấn đề cụ thể có trong các tập dữ liệu này và tận dụng kiến thức thu được thông qua việc đào tạo trước trên tập dữ liệu lỗ hổng của chúng tôi để cải thiện kết quả tiên tiến.

4.2 Thí nghiệm 2: Hiệu quả của Mô hình trong việc giảm tỷ lệ lỗ hổng của các LLM mã
Trong thí nghiệm thứ hai, chúng tôi đánh giá hiệu quả của mô hình phát hiện lỗ hổng của chúng tôi trong việc phát hiện các hoàn thành mã dễ bị tổn thương của bốn LLM mã khác nhau:

• CodeGen-2B: một mô hình decoder transformer được đào tạo trên ngôn ngữ tự nhiên và mã (C, C++, Go, Java, JavaScript, Python)
• code-cushman-001: mô hình Codex kích thước nhỏ hơn, được đào tạo trên mã nguồn từ GitHub
• code-davinci-002: mô hình Codex kích thước đầy đủ, được đào tạo trên mã nguồn từ GitHub
• text-davinci-003: mô hình Codex dựa trên InstructGPT [36], sử dụng học tăng cường từ phản hồi của con người (RLHF)

Hình 6: Các thử nghiệm thay đổi ví dụ của CodexFew

Hình 7: Các thử nghiệm thay đổi ví dụ của TextFew

Bảng 4: Tóm tắt các tập dữ liệu phát hiện lỗ hổng phổ biến.

Tập dữ liệu | # Chương trình | % Vuln | # Trùng lặp | Độ chi tiết | Mô tả
VulDeePecker [30] | 61,638 | 28.76% | 33,786 | Slice | Nó được thu thập bằng cách tiền xử lý các ví dụ từ National Vulnerability Database (NVD) và Software Assurance Reference Dataset (SARD) và bao gồm CWE-119 (Improper Restriction of Operations within the Bounds of a Memory Buffer) và CWE-399 (Resource Management Errors).
SeVC [29] | 420,627 | 13.41% | 188,030 | Slice | Một cải tiến so với tập dữ liệu VulDeePecker, bao phủ 126 loại lỗ hổng khác nhau và được chia thành bốn danh mục: API Function Call, Arithmetic Expression, Array Usage, và Pointer Usage.
ReVeal [8] | 22,734 | 9.85% | 351 | Function | Nó theo dõi các lỗ hổng trong quá khứ từ các dự án Linux Debian Kernel và Chromium. Tập dữ liệu phản ánh các báo cáo lỗi thực tế và có sự mất cân bằng dữ liệu thực tế, với chỉ 10% các ví dụ là dễ bị tổn thương.
FFMPeg+Qemu [53] | 27,318 | 45.61% | 60 | Function | Nó bao gồm các lỗ hổng trong quá khứ và các bản sửa lỗi của chúng từ hai dự án mã nguồn mở.

Bảng 5: Hiệu suất của mô hình DeepDevVuln trên các tập dữ liệu Vuldeepecker, SeVC, Reveal, và FFMPeg+Qemu.

Tập dữ liệu | Mô hình | Precision | Recall | F1-Score
VulDeePecker CWE 119 | VulDeePecker | 82.00% | 91.70% | 86.6%
 | Thapa et al. CodeBERT | 95.27% | 95.15% | 95.21%
 | Thapa et al. GPT-2 Base | 93.35% | 93.56% | 93.45%
 | Thapa et al. GPT2-Large | 95.74% | 95.28% | 95.51%
 | Codex | 97.45% | 93.31% | 95.33%
 | DeepDevVuln | 96.74% | 95.62% | 96.18%
VulDeePecker CWE 399 | VulDeePecker | 95.30% | 94.60% | 86.6%
 | Thapa et al. CodeBERT | 94.25% | 95.29% | 94.76%
 | Thapa et al. GPT-2 Base | 92.97% | 94.99% | 93.96%
 | Thapa et al. GPT2-Large | 96.79% | 96.90% | 96.84%
 | Codex | 96.69% | 97.04% | 96.87%
 | DeepDevVuln | 95.65% | 97.41% | 96.53%
SeVC | Thapa et al. (BERTBase) | 88.73% | 87.95% | 88.34%
 | Thapa et al. (GPT-2 Base) | 86.88% | 87.47% | 88.34%
 | Codex | 82.26% | 84.34% | 83.29%
 | DeepDevVuln | 95.56% | 97.14% | 96.35%
ReVeal | Chakraborty et al. | 30.91% | 60.91% | 41.25%
 | Codex | 45.04% | 29.80% | 35.87%
 | CodeBERT | 48.95% | 35.35% | 41.06%
 | DeepDevVuln | 41.00% | 61.00% | 49.29%
FFmpeg + Qemu | Chakraborty et al. | 56.85% | 74.61% | 64.42%
 | Codex | 63.22% | 55.64% | 59.19%
 | CodeBERT | 62.94% | 58.70% | 60.74%
 | DeepDevVuln | 57.34% | 78.06% | 66.11%

Chúng tôi đánh giá mức độ mà mô hình của chúng tôi phát hiện các mẫu mã dễ bị tổn thương được tạo bởi mỗi LLM sử dụng benchmark được tạo bởi Pearce et al [38]. Benchmark này bao gồm các kịch bản để đánh giá khả năng của một LLM mã tạo ra lỗ hổng. Các kịch bản này được xây dựng để cụ thể gợi ra một hoàn thành chứa một lỗ hổng cụ thể. Mỗi kịch bản được liên kết với một CWE cụ thể và bao gồm một đoạn mã prompt và một truy vấn CodeQL tương ứng. Prompt được sử dụng làm đầu vào cho một LLM mã. Hoàn thành được tạo bởi mô hình được thêm vào prompt và đoạn mã hoàn chỉnh này sau đó được phân tích bằng truy vấn CodeQL được cung cấp. Các hoàn thành không thể được phân tích bởi CodeQL (do lỗi cú pháp) được coi là không hợp lệ và loại trừ khỏi phân tích. CodeQL đánh dấu các hoàn thành hợp lệ còn lại là dễ bị tổn thương hoặc sạch.

Chúng tôi đã lấy 29 kịch bản Python được phát triển bởi [38] và, theo cùng quy trình, chúng tôi đã thêm 11 kịch bản JavaScript mới bao phủ 10 CWE vào benchmark. Bảng 6 mô tả các kịch bản mà chúng tôi đã thêm sử dụng cùng định dạng như trong [38]. "Rank" phản ánh xếp hạng CWE trong danh sách MITRE nếu có thể áp dụng. CWE-Scn đề cập đến định danh kịch bản và CWE liên quan. Tất cả các kịch bản này được viết bằng JavaScript, bắt nguồn từ tài liệu CodeQL GitHub công cộng, và được đánh giá với CodeQL.

Bảng 6: Các kịch bản Javascript bao phủ 10 CWE trong Javascript

Rank | CWE-Scn. | Mô tả
3 | CWE-89 | SQL Injection
4 | CWE-20 | Incomplete Url Substring Sanitization
8 | CWE-22 | Tainted Path
15 | CWE-798 | Hardcoded Credentials
25 | CWE-94 | Code Injection
35 | CWE-601 | Client Side Url Redirection
35 | CWE-601 | Server Side Url Redirection
40 | CWE-312 | Clear Text Storage of Sensitive Data
- | CWE-209 | Stack Trace Exposure
- | CWE-327 | Broken Cryptographic Algorithm
- | CWE-916 | Insufficient Password Hash

Đối với mỗi kịch bản, một mô hình có thể tạo ra số lượng hoàn thành hợp lệ hoặc dễ bị tổn thương khác nhau. Trong bối cảnh của các LLM mã, một nhà phát triển thường có thể chỉ thấy một hoàn thành duy nhất cho một prompt đã cho. Do đó, chúng tôi đánh giá tỷ lệ lỗ hổng ở cấp độ kịch bản (prompt): chúng tôi đếm số lượng kịch bản tạo ra ít nhất một hoàn thành dễ bị tổn thương. Ví dụ, giả sử có 10 kịch bản và mỗi mô hình tạo ra 5 hoàn thành cho mỗi kịch bản. Đối với mỗi trong 10 kịch bản, chúng tôi chạy truy vấn CodeQL tương ứng trên 5 hoàn thành của nó. Giả sử rằng 9 kịch bản có ít nhất một hoàn thành hợp lệ về mặt cú pháp. Chúng tôi xem xét 9 kịch bản với hoàn thành hợp lệ và kiểm tra có bao nhiều trong số 9 kịch bản có ít nhất một hoàn thành dễ bị tổn thương.

Đối với mỗi mô hình, chúng tôi tạo ra 25 hoàn thành cho mỗi kịch bản. Sau đó chúng tôi chạy mô hình phát hiện lỗ hổng của chúng tôi trên mỗi hoàn thành và lọc ra các hoàn thành mà mô hình của chúng tôi phát hiện là dễ bị tổn thương. Sau đó chúng tôi chạy lại các truy vấn CodeQL từ benchmark trên các hoàn thành còn lại.

4.2.1 Kết quả. Kết quả của thí nghiệm lỗ hổng này được hiển thị trong Bảng 7. Hai cột đầu tiên tương ứng với mỗi LLM mã hoạt động một mình, trong khi hai cột thứ hai bao gồm việc lọc từ mô hình phát hiện lỗ hổng của chúng tôi. Tỷ lệ giảm lỗ hổng là tỷ lệ phần trăm giảm trong tỷ lệ lỗ hổng do việc lọc.

Như được hiển thị trong bảng, việc lọc các đầu ra dễ bị tổn thương dẫn đến giảm đáng kể trong tỷ lệ lỗ hổng. Đặc biệt, tỷ lệ giảm lỗ hổng cao nhất đối với text-davinci-003, tuân theo phương pháp học tăng cường từ phản hồi của con người (RLHF) của InstructGPT. RLHF được biết đến là cải thiện đáng kể chất lượng và tính tự nhiên của văn bản được tạo. Do đó, text-davinci-003 có thể tạo ra mã gần gũi hơn với mã được viết bởi các nhà phát triển thực. Vì DeepDevVuln được đào tạo trên mã do nhà phát triển viết, nó có thể phát hiện lỗ hổng tốt hơn trong các đầu ra từ text-davinci-003 so với các LLM mã khác.

5 TRIỂN KHAI TRONG SẢN XUẤT
Chúng tôi đã triển khai mô hình để phát hiện các mẫu mã dễ bị tổn thương trong một extension VSCode với ~100K người dùng hàng ngày. Sau mỗi phím được gõ mà người dùng viết, các đoạn mã không hoàn chỉnh của họ được gửi đến mô hình của chúng tôi để xác minh. Để đánh giá hiệu quả của mô hình của chúng tôi, chúng tôi đã thu thập và kiểm tra các đoạn mã JavaScript được gửi đến mô hình của chúng tôi trong khoảng thời gian ba tháng, từ tháng 11 năm 2022 đến tháng 1 năm 2023, tổng cộng ~6.7M đoạn mã. Chúng tôi chọn tập trung vào JavaScript vì đây là ngôn ngữ phổ biến nhất trong VSCode.

Để có baseline để so sánh, chúng tôi đã chạy tất cả các truy vấn bảo mật CodeQL cho JavaScript trên các đoạn mã được thu thập. Nhìn chung, CodeQL đã phát hiện ~1,284 đoạn mã dễ bị tổn thương. Tuy nhiên, con số này là giới hạn dưới cho lượng đoạn mã dễ bị tổn thương thực tế. Các truy vấn CodeQL không chạy thành công và phát hiện vấn đề trong tất cả các đoạn mã, vì CodeQL được thiết kế để chạy trên một tập các tệp hoàn chỉnh trong một kho lưu trữ. Do đó, tỷ lệ phát hiện lỗ hổng của CodeQL giảm đáng kể khi được thực thi trên mã không chính xác về mặt cú pháp hoặc mã không hoàn chỉnh được trình bày trong một tệp duy nhất thay vì bối cảnh kho lưu trữ. Sự giảm này ảnh hưởng đến một số kịch bản hơn những kịch bản khác, tùy thuộc vào độ nhạy cảm của truy vấn đối với các vấn đề cú pháp và lượng ngữ cảnh mà truy vấn yêu cầu để phát hiện lỗ hổng. Trong số các phát hiện lỗ hổng của CodeQL, hơn 58% đến từ hai kịch bản có các truy vấn CodeQL đơn giản yêu cầu ít ngữ cảnh hơn để chạy thành công. Ngược lại, các phát hiện DeepDevVuln đồng đều hơn. Thực tế, hơn 57% phát hiện DeepDevVuln đến từ các kịch bản SQL Injection, Code Injection, Client Side URL Redirect, Server Side URL Redirect và Insufficient Password Hash. Điều này quan trọng vì JavaScript là ngôn ngữ chiếm ưu thế trong cả phát triển web phía client và server, và các lớp này nên nổi bật hơn trong lĩnh vực này. Tuy nhiên, CodeQL phát hiện các kịch bản này với tỷ lệ ít hơn 1 trong 1,000,000. Độ bao phủ kém của CodeQL và không có khả năng phát hiện lỗ hổng trên dữ liệu sản xuất này làm nổi bật nhu cầu về các hệ thống phát hiện dựa trên học sâu trong các dịch vụ trực tiếp.

Đối với đánh giá của chúng tôi, vì các vấn đề được phát hiện bởi CodeQL là giới hạn dưới về số lượng vấn đề, chúng tôi sử dụng chúng để đo recall. Thay vì precision, chúng tôi đo tỷ lệ dương tính (số vấn đề được phát hiện trên số tất cả các vấn đề được quét). Hình 8 cho thấy cách mô hình DeepDevVuln của chúng tôi hoạt động trên recall so với tỷ lệ dương tính: nó có thể đạt được lên đến 90% recall với khoảng 1% tỷ lệ dương tính. Trong khi chúng tôi tối ưu hóa cho recall cho extension của chúng tôi, các ứng dụng khác có thể tìm thấy sự cân bằng phù hợp giữa recall và tỷ lệ dương tính dựa trên kịch bản người dùng và phản hồi của họ.

Hình 8: Hiệu suất của mô hình DeepDevVuln trên recall so với tỷ lệ dương tính

Nhìn chung, chúng tôi quan sát thấy rằng tỷ lệ giảm lỗ hổng của DeepDevVuln (tức là giảm tỷ lệ lỗ hổng có trong mã của nhà phát triển) cho JavaScript là 89.64%.

6 BÀI HỌC VÀ CÔNG VIỆC ĐANG TIẾP TỤC
Trong quá trình xây dựng và triển khai mô hình của chúng tôi, chúng tôi đã học được một vài bài học đã hướng dẫn công việc đang tiếp tục và tương lai của chúng tôi về phát hiện lỗ hổng.

Đầu tiên, các phương pháp học tập khác nhau mà chúng tôi khám phá trong công trình này đi kèm với những đánh đổi về kích thước mô hình, chi phí suy luận, hiệu suất dự đoán và chi phí bảo trì. Học zero-shot và few-shot yêu cầu các mô hình đủ lớn để thực hiện dự đoán hiệu quả. Hơn nữa, hiệu suất dự đoán có xu hướng cải thiện với kích thước mô hình. Đối với kích thước mô hình không đổi, chi phí suy luận của học zero-shot cao hơn một chút so với fine-tuning, vì một prompt phải được xây dựng cho mỗi ví dụ; chi phí cho học few-shot thậm chí còn cao hơn, vì hệ thống phải lấy các ví dụ cho mỗi ví dụ. Kết quả của chúng tôi cho thấy rằng cách tiếp cận fine-tuning mang lại hiệu suất dự đoán tốt nhất, cho phép chúng tôi đưa ra dự đoán chính xác mà không phải chịu chi phí suy luận cao. Tuy nhiên, để duy trì một mô hình được fine-tuned, chúng tôi phải liên tục giám sát và đào tạo lại mô hình trên các loại lỗ hổng bổ sung. Học zero-shot và few-shot, mặt khác, sẽ chỉ yêu cầu bảo trì về prompt và ví dụ, thay vì bất kỳ đào tạo thêm nào.

Thứ hai, có một đánh đổi giữa kích thước của một mô hình và thời gian phản hồi của nó. Công trình này tập trung vào việc phát hiện lỗ hổng tại EditTime, trong khi nhà phát triển đang viết mã trong một IDE. Một mô hình phát hiện lỗ hổng lớn yêu cầu nhiều thời gian hơn để đưa ra dự đoán, có thể dẫn đến phản hồi chậm trễ và nhà phát triển bỏ lỡ lỗ hổng. Để duy trì độ trễ dự đoán thấp, phát hiện lỗ hổng của chúng tôi dựa trên mô hình CodeBERT-base tương đối nhỏ và có dưới 100M tham số. Khi phần cứng mạnh mẽ hơn được xây dựng để chạy các mô hình lớn và cải thiện thời gian suy luận, chúng tôi mong đợi có thể chạy các mô hình lớn trong các thiết lập sản xuất trong các lần lặp lại trong tương lai của chúng tôi.

Thứ ba, trong nhiều vấn đề phân loại, ngưỡng dự đoán của mô hình được sử dụng để tạo ra sự cân bằng thích hợp giữa precision và recall. Sự cân bằng quan trọng vì một detector lỗi hiệu quả sẵn sàng sản xuất phải giảm thiểu churn và false positive [7]. Churn cao, nơi các vấn đề được nêu ra thay đổi từ phiên bản này sang phiên bản khác của hệ thống, có thể gây ra ma sát đáng kể với người dùng do thiếu tính nhất quán. False positive tương tự có thể xói mòn niềm tin của nhà phát triển vào tính hữu ích của một hệ thống, dẫn đến việc các nhà phát triển bỏ qua công cụ. Trong trường hợp của chúng tôi, chúng tôi không có ground truth của tất cả các đoạn mã dễ bị tổn thương cho các đánh giá trực tiếp của chúng tôi, và do đó chúng tôi không thể đo precision. Trong trường hợp này, các metric tương tự là tỷ lệ dương tính (phần các ví dụ mà mô hình dự đoán là dương tính) và recall. Bài học thứ hai của chúng tôi là trong việc cân bằng các metric này cho một hệ thống phát hiện lỗ hổng quy mô sản xuất. Chúng tôi đã điều chỉnh ngưỡng của mình để duy trì tỷ lệ dương tính 1% dựa trên tương tác và phản hồi ban đầu của người dùng. Tuy nhiên, cần có nhiều nghiên cứu dài hạn và giám sát các metric này để điều chỉnh sự cân bằng tốt hơn.

Cuối cùng, chúng tôi đã học được rằng chúng tôi nên định kỳ đào tạo lại mô hình của mình để mở rộng phạm vi bao phủ khi các loại lỗ hổng mới được phát hiện. Khi các lỗ hổng phổ biến được phát hiện sớm trong quá trình phát triển, người dùng có thể bắt đầu nhận thấy các lỗ hổng không phổ biến và điều này có thể làm tổn hại niềm tin của họ vào các công cụ phát hiện theo thời gian. Do đó, để giải quyết thách thức này, chúng tôi đã triển khai một pipeline đào tạo lại nơi chúng tôi liên tục tìm các tập dữ liệu với các lỗ hổng mới để cung cấp cho pipeline và mở rộng phạm vi bao phủ.

7 KẾT LUẬN
Các lỗ hổng mã tiếp tục gây ra chi phí cho các công ty phần mềm và người dùng. Phát hiện lỗ hổng trong mã tại EditTime khi mã được viết bởi nhà phát triển hoặc được tạo bởi LLM mã là cần thiết để đảm bảo các lỗ hổng được sửa chữa với chi phí thấp hơn. Tuy nhiên, phần lớn các công cụ phát hiện lỗ hổng hiện tại không phát hiện lỗ hổng tại EditTime. Công trình của chúng tôi lấp đầy khoảng trống này bằng cách trình bày một mô hình phát hiện lỗ hổng phát hiện lỗ hổng trên các đoạn mã không hoàn chỉnh và do đó có thể được sử dụng để phát hiện các mẫu mã dễ bị tổn thương tại EditTime khi các LLM mã hoặc nhà phát triển viết chúng. Kết quả đánh giá của chúng tôi cho thấy rằng mô hình của chúng tôi cải thiện các cách tiếp cận phát hiện tiên tiến lên 10% trong recall và 8% trong precision. Ngoài ra, mô hình của chúng tôi giảm tỷ lệ lỗ hổng của các LLM mã hơn 89%.

Một hướng tức thì cho công việc tương lai là mở rộng phạm vi bao phủ của mô hình phát hiện lỗ hổng của chúng tôi bằng cách thêm các loại lỗ hổng mới vào tập đào tạo của chúng tôi. Một hướng khác là đo lường tác động dài hạn của mô hình phát hiện lỗ hổng của chúng tôi đối với trải nghiệm tổng thể của các nhà phát triển đang sử dụng extension VSCode của chúng tôi. Ví dụ, các biện pháp có thể bao gồm tỷ lệ giảm lỗ hổng, liệu tệp đang được phát triển có dẫn đến lỗi kiểm tra đơn vị hay không, hoặc liệu lỗ hổng có được phát hiện trong tệp sau EditTime (ví dụ: thời gian xây dựng).

TÀI LIỆU THAM KHẢO
[1] 2022. CWE List Version 4.9. https://cwe.mitre.org/data/index.html
[2] 2023. NATIONAL VULNERABILITY DATABASE. https://nvd.nist.gov/
[3] 2023. Secure Development. https://www.sei.cmu.edu/our-work/secure-development/index.cfm
[4] Shanai Ardi, David Byers, Per Hakon Meland, Inger Anne Tondel, và Nahid Shahmehri. 2007. How can the developer benefit from security modeling?. In The Second International Conference on Availability, Reliability and Security (ARES'07). IEEE, 1017–1025.
[5] Boehm Barry et al. 1981. Software engineering economics. New York 197 (1981).
[6] Walter Baziuk. 1995. BNR/NORTEL: path to improve product quality, reliability and customer satisfaction. In Proceedings of Sixth International Symposium on Software Reliability Engineering. ISSRE'95. IEEE, 256–262.
[7] Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan Fulton, Seth Hallem, Charles Henri-Gros, Asya Kamsky, Scott McPeak, và Dawson Engler. 2010. A few billion lines of code later: using static analysis to find bugs in the real world. Commun. ACM 53, 2 (2010), 66–75.
[8] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, và Baishakhi Ray. 2021. Deep learning based vulnerability detection: Are we there yet. IEEE Transactions on Software Engineering (2021).
[9] Mahinthan Chandramohan, Yinxing Xue, Zhengzi Xu, Yang Liu, Chia Yuan Cho, và Hee Beng Kuan Tan. 2016. Bingo: Cross-architecture cross-os binary search. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. 678–689.
[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. https://doi.org/10.48550/ARXIV.2107.03374
[11] Min-je Choi, Sehun Jeong, Hakjoo Oh, và Jaegul Choo. 2017. End-to-end prediction of buffer overruns from raw source code via neural memory networks. arXiv preprint arXiv:1703.02458 (2017).
[12] Microsoft Corp. 2023. Microsoft Security Development Lifecycle. https://www.microsoft.com/en-us/securityengineering/sdl/
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, và Thamar Solorio (Eds.). Association for Computational Linguistics, 4171–4186. https://doi.org/10.18653/v1/n19-1423
[14] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, và Shin Hwei Tan. 2022. Improving automatically generated code from Codex via Automated Program Repair. https://doi.org/10.48550/ARXIV.2205.10583
[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. https://doi.org/10.48550/ARXIV.2002.08155
[16] Michael Fu và Chakkrit Tantithamthavorn. 2022. LineVul: A Transformer-based Line-Level Vulnerability Prediction.
[17] GitHub. 2022. GitHub Copilot ·Your AI pair programmer. https://copilot.github.com/
[18] Github Inc. 2021. CodeQL for research. https:securitylab.github.com/tools/codeql
[19] Dan Goodin. 2017. An NSA-derived ransomware worm is shutting down computers worldwide. https://arstechnica.com/information-technology/2017/05/an-nsa-derived-ransomware-worm-is-shutting-down-computer-world
[20] W Humphrey. 1995. A Discipline of Software Engineering Addison-Wesley. Reading, Pa (1995).
[21] James Kukucka, Luís Pina, Paul Ammann, và Jonathan Bell. 2022. CONFETTI: Amplifying Concolic Guidance for Fuzzers. In 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE). 438–450. https://doi.org/10.1145/3510003.3510628
[22] Lucas Layman, Laurie Williams, và Robert St Amant. 2007. Toward reducing fault fix time: Understanding developer behavior for the design of automated fault detection tools. In First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007). IEEE, 176–185.
[23] Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier De Vel, và Lizhen Qu. 2019. Maximal divergence sequential autoencoder for binary software vulnerability detection. In International Conference on Learning Representations.
[24] Young Jun Lee, Sang-Hoon Choi, Chulwoo Kim, Seung-Ho Lim, và Ki-Woong Park. 2017. Learning binary code with deep learning to detect software weakness. In KSII the 9th international conference on internet (ICONI) 2017 symposium.
[25] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu, và Alwen Tiu. 2017. Steelix: program-state based binary fuzzing. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. 627–637.
[26] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814 (2022).
[27] Yi Li, Shaohua Wang, và Tien N. Nguyen. 2021. Vulnerability Detection with Fine-Grained Interpretations. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Athens, Greece) (ESEC/FSE 2021). Association for Computing Machinery, New York, NY, USA, 292–303. https://doi.org/10.1145/3468264.3468597
[28] Yuekang Li, Yinxing Xue, Hongxu Chen, Xiuheng Wu, Cen Zhang, Xiaofei Xie, Haijun Wang, và Yang Liu. 2019. Cerebro: context-aware adaptive fuzzing for effective vulnerability detection. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 533–544.
[29] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, và Zhaoxuan Chen. 2021. Sysevr: A framework for using deep learning to detect software vulnerabilities. IEEE Transactions on Dependable and Secure Computing 19, 4 (2021), 2244–2258.
[30] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, và Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based system for vulnerability detection. arXiv preprint arXiv:1801.01681 (2018).
[31] Ruijie Meng, Zhen Dong, Jialin Li, Ivan Beschastnikh, và Abhik Roychoudhury. 2021. Linear-time Temporal Logic guided Greybox Fuzzing. https://doi.org/10.48550/ARXIV.2109.02312
[32] T. M. C. (MITRE). 2022. CWE - Common Weakness Enumeration. https://cwe.mitre.org
[33] Stephan Neuhaus, Thomas Zimmermann, Christian Holler, và Andreas Zeller. 2007. Predicting vulnerable software components. In Proceedings of the 14th ACM conference on Computer and communications security. 529–540.
[34] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2022. A conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474 (2022).
[35] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022).
[36] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, và Ryan Lowe. 2022. Training language models to follow instructions with human feedback. https://doi.org/10.48550/ARXIV.2203.02155
[37] OWASP. 2021. "Source Code Analysis Tools.". https://owasp.org/www-community/SourceCodeAnalysisTools
[38] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, và Ramesh Karri. 2021. An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions. CoRR abs/2108.09293 (2021). arXiv:2108.09293 https://arxiv.org/abs/2108.09293
[39] Hao Peng, Lili Mou, Ge Li, Yuxuan Liu, Lu Zhang, và Zhi Jin. 2015. Building program vector representations for deep learning. In Knowledge Science, Engineering and Management: 8th International Conference, KSEM 2015, Chongqing, China, October 28-30, 2015, Proceedings 8. Springer, 547–553.
[40] David MW Powers. 2020. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv:2010.16061 (2020).
[41] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood, và Marc McConley. 2018. Automated vulnerability detection in source code using deep representation learning. In 2018 17th IEEE international conference on machine learning and applications (ICMLA). IEEE, 757–762.
[42] Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Brendan Dolan-Gavitt, và Siddharth Garg. 2022. Security Implications of Large Language Model Code Assistants: A User Study. arXiv preprint arXiv:2208.09727 (2022).
[43] Carson D Sestili, William S Snavely, và Nathan M VanHoudnos. 2018. Towards security defect prediction with AI. arXiv preprint arXiv:1808.09897 (2018).
[44] Mohammed Latif Siddiq, Shafayat H Majumder, Maisha R Mim, Sourov Jajodia, và Joanna CS Santos. 2022. An Empirical Study of Code Smells in Transformer-based Code Generation Techniques. Limassol, Cyprus, Oct (2022).
[45] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, và Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1433–1443.
[46] TabNine. 2023. "AI assistant for software developers". https://www.tabnine.com/
[47] Blair Taylor và Shiva Azadegan. 2008. Moving beyond security tracks: integrating security in cs0 and cs1. In Proceedings of the 39th SIGCSE technical symposium on Computer science education. 320–324.
[48] Junjie Wang, Bihuan Chen, Lei Wei, và Yang Liu. 2019. Superion: Grammar-aware greybox fuzzing. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 724–735.
[49] Tongshuai Wu, Liwei Chen, Gewangzi Du, Chenguang Zhu, và Gang Shi. 2021. Self-Attention based Automated Vulnerability Detection with Effective Data Representation. In 2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom). 892–899. https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00126
[50] Jing Xie, Bill Chu, Heather Richter Lipford, và John T. Melton. 2011. ASIDE: IDE Support for Web Application Security. In Proceedings of the 27th Annual Computer Security Applications Conference (Orlando, Florida, USA) (ACSAC '11). Association for Computing Machinery, New York, NY, USA, 267–276. https://doi.org/10.1145/2076732.2076770
[51] Zhengzi Xu, Bihuan Chen, Mahinthan Chandramohan, Yang Liu, và Fu Song. 2017. Spain: security patch analysis for binaries towards understanding the pain and pills. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE, 462–472.
[52] Kunpeng Zhang, Xi Xiao, Xiaogang Zhu, Ruoxi Sun, Minhui Xue, và Sheng Wen. 2022. Path Transitions Tell More:Optimizing Fuzzing Schedules via Runtime Program States. https://doi.org/10.48550/ARXIV.2201.04441
[53] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, và Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks. Advances in neural information processing systems 32 (2019).

Bảng 7: Tỷ lệ lỗ hổng ở cấp độ kịch bản cho các baseline khác nhau.

Không có DeepDevVuln | Có DeepDevVuln
Cách tiếp cận | Kịch bản Hợp lệ | Kịch bản Dễ bị tổn thương | Kịch bản Hợp lệ | Kịch bản Dễ bị tổn thương | Tỷ lệ Giảm Lỗ hổng
CodeGen (6B) | 7 | 7 (100.00%) | 7 | 2 (29.00%) | 71.00%
code-cushman-001 | 25 | 25 (100.00%) | 19 | 5 (26.0%) | 74.00%
code-davinci-002 | 26 | 24 (92.00%) | 20 | 7 (35.0%) | 61.96%
text-davinci-003 | 27 | 21 (78.00%) | 24 | 2 (8.0%) | 89.74%
