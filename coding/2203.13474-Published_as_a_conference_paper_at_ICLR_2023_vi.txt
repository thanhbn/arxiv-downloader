# 2203.13474.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2203.13474.pdf
# Kích thước tệp: 747283 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
CODEGEN: MỘT MÔ HÌNH NGÔN NGỮ LỚN MỞ CHO
MÃ CODE VỚI TỔNG HỢP CHƯƠNG TRÌNH ĐA LƯỢT
Erik Nijkamp, Bo Pang, Hiroaki Hayashi,
Lifu Tu ,Huan Wang ,Yingbo Zhou ,Silvio Savarese ,Caiming Xiong
Salesforce Research
TÓM TẮT
Tổng hợp chương trình cố gắng tạo ra một chương trình máy tính như một giải pháp cho một
đặc tả vấn đề nhất định, được biểu thị bằng các ví dụ đầu vào-đầu ra hoặc mô tả ngôn ngữ tự nhiên. Sự phổ biến của các mô hình ngôn ngữ lớn thúc đẩy hiện trạng
cho tổng hợp chương trình, mặc dù nguồn lực đào tạo và dữ liệu hạn chế cản trở việc truy cập mở
vào các mô hình như vậy. Để dân chủ hóa điều này, chúng tôi đào tạo và phát hành một họ các mô hình ngôn ngữ lớn
lên đến 16.1B tham số, được gọi là CODEGEN, trên dữ liệu ngôn ngữ tự nhiên
và ngôn ngữ lập trình, và mã nguồn mở thư viện đào tạo JAX FORMER .
Chúng tôi cho thấy tính hữu ích của mô hình đã đào tạo bằng cách chứng minh rằng nó có thể cạnh tranh với
hiện trạng trước đó trên việc tạo mã Python zero-shot trên HumanEval.
Chúng tôi tiếp tục điều tra mô hình đa bước cho tổng hợp chương trình, nơi một chương trình đơn
được phân tích thành nhiều prompts chỉ định các vấn đề con. Để kết thúc điều này,
chúng tôi xây dựng một bộ test chuẩn mở, Multi-Turn Programming Benchmark (MTPB),
bao gồm 115 bộ vấn đề đa dạng được phân tích thành các prompts đa lượt.
Phân tích của chúng tôi trên MTPB cho thấy rằng cùng một ý định được cung cấp cho CODEGEN theo kiểu đa
lượt cải thiện đáng kể tổng hợp chương trình so với việc cung cấp như một lượt đơn. Chúng tôi làm cho thư viện đào tạo JAX FORMER và các checkpoint mô hình có sẵn
như đóng góp mã nguồn mở: https://github.com/salesforce/CodeGen .

1 GIỚI THIỆU
Tạo một chương trình thường liên quan đến việc con người nhập mã bằng tay. Mục tiêu của tổng hợp chương trình
là tự động hóa quá trình mã hóa, và tạo ra một chương trình máy tính thỏa mãn ý định được chỉ định của người dùng. Một số người đã gọi nó là chén thánh của khoa học máy tính (Manna & Waldinger, 1971;
Gulwani et al., 2017). Tổng hợp chương trình thành công sẽ không chỉ cải thiện năng suất của
các lập trình viên có kinh nghiệm mà còn làm cho lập trình có thể tiếp cận được với một khán giả rộng hơn.

Hai thách thức chính phát sinh khi cố gắng đạt được tổng hợp chương trình: (1) tính không thể theo dõi của
không gian tìm kiếm, và (2) khó khăn trong việc chỉ định đúng ý định của người dùng. Để duy trì một không gian tìm kiếm biểu cảm
, người ta cần một không gian tìm kiếm lớn, điều này đặt ra thách thức trong tìm kiếm hiệu quả. Công việc trước đây
(Joshi et al., 2002; Panchekha et al., 2015; Cheung et al., 2013) tận dụng ngôn ngữ miền cụ thể
để hạn chế không gian tìm kiếm; tuy nhiên, điều này hạn chế khả năng áp dụng của các chương trình được tổng hợp. Ngược lại, trong khi có thể áp dụng rộng rãi, các ngôn ngữ lập trình đa mục đích ( ví dụ: C, Python)
giới thiệu một không gian tìm kiếm thậm chí còn lớn hơn cho các chương trình có thể. Để điều hướng qua không gian chương trình khổng lồ
, chúng tôi hình thành nhiệm vụ như mô hình hóa ngôn ngữ, học một phân phối có điều kiện của
token tiếp theo dựa trên các token trước đó và tận dụng transformers (Vaswani et al., 2017) và pre-training tự giám sát quy mô lớn. Cách tiếp cận này đã thấy thành công trên các phương thức (Devlin et al., 2019;
Lewis et al., 2020; Dosovitskiy et al., 2021). Tương tự, các công việc trước đây đã phát triển các mô hình ngôn ngữ được pre-train
cho hiểu ngôn ngữ lập trình (Kanade et al., 2020; Feng et al., 2020).

Để thực hiện tổng hợp chương trình thành công, người dùng phải sử dụng một số phương tiện để truyền đạt ý định của họ
đến các mô hình như một biểu thức logic (chỉ định một mối quan hệ logic giữa đầu vào

Đóng góp bằng nhau.
Liên hệ với: Erik Nijkamp (erik.nijkamp@salesforce.com), Bo Pang (b.pang@salesforce.com),
Hiroaki Hayashi (hiroakihayashi@salesforce.com), Yingbo Zhou (yingbo.zhou@salesforce.com), Caiming
Xiong (cxiong@salesforce.com).
1arXiv:2203.13474v5 [cs.LG] 27 Feb 2023

--- TRANG 2 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
và đầu ra của một chương trình), pseudo-code, ví dụ đầu vào-đầu ra, hoặc các đặc tả được diễn đạt bằng lời trong
ngôn ngữ tự nhiên. Một mặt, một đặc tả chính thức hoàn chỉnh thích thú các đặc tả chính xác
của ý định người dùng nhưng có thể yêu cầu kiến thức chuyên môn miền và nỗ lực từ người dùng để dịch ý định thành một
dạng như vậy. Mặt khác, đặc tả chỉ dựa trên ví dụ đầu vào-đầu ra ít tốn kém hơn nhưng
có thể đặc tả dưới mức ý định, dẫn đến các giải pháp không chính xác. Công việc trước đây đã hưởng lợi từ
các phương pháp khác nhau và kết hợp của chúng như đầu vào cho các mô hình tổng hợp chương trình, bao gồm pseudo-
code (Kulal et al., 2019), một phần của chương trình và tài liệu của nó (Chen et al., 2021), hoặc đoạn văn ngôn ngữ tự nhiên
với ví dụ đầu vào-đầu ra (Hendrycks et al., 2021). Tuy nhiên, chúng tôi lập luận rằng một
dạng ý định thực sự thân thiện với người dùng là văn bản ngôn ngữ tự nhiên.

Để vượt qua những thách thức này, chúng tôi đề xuất một cách tiếp cận tổng hợp chương trình đa lượt, nơi người dùng
giao tiếp với hệ thống tổng hợp bằng cách dần dần cung cấp đặc tả bằng ngôn ngữ tự nhiên
trong khi nhận được phản hồi từ hệ thống dưới dạng các chương trình con được tổng hợp, sao cho người dùng
cùng với hệ thống hoàn thành chương trình trong nhiều bước. Hai cân nhắc sau
thúc đẩy cách tiếp cận này.

Đầu tiên, chúng tôi suy đoán rằng việc phân tích một đặc tả có khả năng dài và phức tạp thành nhiều bước
sẽ làm dễ dàng việc hiểu của mô hình và do đó tăng cường tổng hợp chương trình. Trong cách tiếp cận đa lượt
, một mô hình có thể tập trung vào đặc tả liên quan đến một chương trình con và tránh việc theo dõi một cách khó khăn
sự phụ thuộc phức tạp giữa các chương trình con. Điều này có hiệu quả giảm không gian tìm kiếm
bên cạnh sự thuận tiện của việc chỉ định ý định người dùng. Thật vậy, suy đoán của chúng tôi được xác nhận trong các thí nghiệm của chúng tôi
với các chương trình được tổng hợp chất lượng cao hơn thông qua cách tiếp cận đa lượt.

Thứ hai, mã thể hiện một mẫu yếu của ngôn ngữ tự nhiên và lập trình xen kẽ, có thể
khai thác được. Mẫu như vậy được hình thành bởi các lập trình viên giải thích chức năng của một chương trình
với các comments. Với mục tiêu mô hình hóa ngôn ngữ, chúng tôi giả thuyết rằng mẫu xen kẽ
cung cấp một tín hiệu giám sát cho mô hình tạo ra các chương trình dựa trên mô tả ngôn ngữ tự nhiên
qua nhiều lượt. Tín hiệu này rất ồn ào hoặc yếu, bởi vì chỉ một tập con dữ liệu sẽ thể hiện
mẫu như vậy, các comments có thể không chính xác hoặc không thông tin, và một số trong số chúng thậm chí có thể được đặt
ở một vị trí không liên quan. Tuy nhiên, việc tăng quy mô mô hình và kích thước dữ liệu có thể vượt qua sự giám sát yếu như vậy
, cho phép mô hình phát triển khả năng tổng hợp chương trình đa lượt. Điều này cho phép ý định người dùng
được biểu thị trong nhiều lượt, nghĩa là, ý định có thể được phân giải và thực hiện từng phần một
trong khi mỗi lượt có thể dễ dàng được biểu thị bằng ngôn ngữ tự nhiên.

Trong công việc này, chúng tôi phát triển một bộ test chuẩn lập trình đa lượt để đo lường khả năng của các mô hình cho
tổng hợp chương trình đa lượt. Để giải quyết một vấn đề trong bộ test chuẩn, một mô hình cần tổng hợp
một chương trình trong nhiều bước với một người dùng chỉ định ý định trong mỗi lượt bằng ngôn ngữ tự nhiên.
Vui lòng tham khảo Hình 1 cho một ví dụ nơi mô hình tổng hợp một chương trình để trích xuất tên người dùng
của một địa chỉ email. Hiệu suất trên bộ test chuẩn được đo bằng tỷ lệ vượt qua trên các trường hợp test do chuyên gia viết
. Theo hiểu biết tốt nhất của chúng tôi, đây là bộ test chuẩn tổng hợp chương trình đa lượt đầu tiên
, cho phép phân tích định lượng của tổng hợp chương trình đa lượt. Với sự xuất hiện của khả năng tổng hợp chương trình đa lượt
trong các mô hình ngôn ngữ lớn có lợi cho việc giải quyết vấn đề, chúng tôi tin rằng bộ test chuẩn này
sẽ thúc đẩy nghiên cứu tương lai trong tổng hợp chương trình.

Đóng góp của chúng tôi Công việc của chúng tôi chia sẻ ý tưởng cơ bản về việc áp dụng các mô hình ngôn ngữ cho tổng hợp chương trình
với các nỗ lực gần đây và đồng thời (Chen et al., 2021; Austin et al., 2021; Li et al., 2022)
với đặc tả ý định người dùng đơn lượt. Ngoài ra, chúng tôi đóng góp đối với bốn khía cạnh:
• Chúng tôi nghiên cứu tổng hợp chương trình đa lượt nổi lên trong các mô hình tự hồi quy dưới quy luật tỷ lệ.
• Chúng tôi tận dụng khả năng này để giới thiệu một mô hình tổng hợp chương trình đa lượt.
• Chúng tôi điều tra các tính chất của nó một cách định lượng với một bộ test chuẩn lập trình đa lượt mới.1
• Chúng tôi sẽ mã nguồn mở các checkpoint mô hình2 và thư viện đào tạo tùy chỉnh: JAX FORMER .3

Đối với tổng hợp chương trình, không có mô hình quy mô lớn nào cạnh tranh với Codex có sẵn như mã nguồn mở.
Điều này cản trở tiến bộ, cho rằng các nguồn lực tính toán đắt đỏ cần thiết để đào tạo những mô hình này chỉ
có thể tiếp cận được với một số lượng hạn chế các tổ chức. Đóng góp mã nguồn mở của chúng tôi cho phép một loạt rộng
các nhà nghiên cứu nghiên cứu và tiến bộ những mô hình này, có thể tạo điều kiện thuận lợi rất lớn cho tiến bộ nghiên cứu.

1Bộ test chuẩn: https://github.com/salesforce/CodeGen/tree/main/benchmark
2Checkpoints: https://github.com/salesforce/CodeGen
3Đào tạo: https://github.com/salesforce/jaxformer
2

--- TRANG 3 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
2 ĐÀO TẠO MÔ HÌNH

Để đánh giá sự xuất hiện của khả năng lập trình đa lượt dưới quy luật tỷ lệ, chúng tôi áp dụng các mô hình ngôn ngữ tự hồi quy dựa trên transformer tiêu chuẩn, thay đổi (1) số lượng tham số mô hình
(350M, 2.7B, 6.1B, 16.1B) và (2) số lượng token của ngôn ngữ lập trình trong các corpus đào tạo
. Để mở rộng đào tạo, một thư viện tùy chỉnh JAX FORMER cho phần cứng TPU-v4 đã được phát triển
và sẽ được phát hành như mã nguồn mở, bao gồm cả trọng số mô hình đã đào tạo.

2.1 BỘ DỮ LIỆU

Họ các mô hình CODEGEN được đào tạo tuần tự trên ba bộ dữ liệu: THEPILE, BIGQUERY,
và BIGPYTHON.

Bộ dữ liệu ngôn ngữ tự nhiên THEPILE là một corpus văn bản tiếng Anh 825.18GiB được thu thập bởi Gao et al.
(2020) cho mô hình hóa ngôn ngữ (giấy phép MIT). Bộ dữ liệu được xây dựng từ 22 tập con chất lượng cao đa dạng
, một trong số đó là dữ liệu ngôn ngữ lập trình được thu thập từ các repository GitHub với >100
stars chiếm 7.6% của bộ dữ liệu. Vì phần lớn THEPILE là văn bản tiếng Anh, các mô hình kết quả
được gọi là các mô hình CODEGEN ngôn ngữ tự nhiên (CODEGEN-NL).

Bộ dữ liệu đa ngôn ngữ BIGQUERY là một tập con của bộ dữ liệu BigQuery có sẵn công khai của Google
, bao gồm mã (dưới giấy phép mã nguồn mở) trong nhiều ngôn ngữ lập trình. Cho đào tạo đa ngôn ngữ
, 6 ngôn ngữ lập trình sau được chọn: C, C++, Go, Java, JavaScript,
và Python. Vì vậy, chúng tôi gọi các mô hình được đào tạo trên BIGQUERY là các mô hình CODEGEN đa ngôn ngữ
(CODEGEN-MULTI).

Bộ dữ liệu đơn ngôn ngữ BIGPYTHON chứa một lượng lớn dữ liệu trong ngôn ngữ lập trình,
Python. Chúng tôi đã biên dịch thông tin công khai, không cá nhân từ GitHub bao gồm mã Python có giấy phép cho phép
vào tháng 10 năm 2021. Do đó, chúng tôi gọi các mô hình được đào tạo trên BIGPYTHON là
các mô hình CODEGEN đơn ngôn ngữ (CODEGEN-MONO).

Tiền xử lý tuân theo: (1) lọc, (2) khử trùng lặp, (3) tokenization, (4) xáo trộn, và
(5) nối. Đối với chi tiết về THEPILE, chúng tôi tham khảo Gao et al. (2020). Đối với BIGQUERY và
BIGPYTHON, chúng tôi tham khảo Phụ lục A. Bảng 5 tóm tắt thống kê của các corpus đào tạo.

2.2 MÔ HÌNH

Các mô hình CODEGEN ở dạng transformer tự hồi quy với mục tiêu học mô hình hóa ngôn ngữ dự đoán token tiếp theo
được đào tạo trên một corpus ngôn ngữ tự nhiên và dữ liệu ngôn ngữ lập trình được tuyển chọn từ GitHub. Các mô hình được đào tạo trong các kích thước khác nhau với 350M, 2.7B, 6.1B,
và 16.1B tham số. Ba cấu hình đầu cho phép so sánh trực tiếp với các mô hình ngôn ngữ lớn mã nguồn mở
được đào tạo trên corpus văn bản, GPT-NEO (350M, 2.7B) (Black et al., 2021) và
GPT-J (6B) (Wang & Komatsuzaki, 2021). Xem Bảng 6 trong Phụ lục A cho đặc tả mô hình.

Các mô hình CODEGEN được đào tạo theo bản chất tuần tự trên các bộ dữ liệu. CODEGEN-NL được đào tạo đầu tiên
trên THEPILE. CODEGEN-MULTI được khởi tạo từ CODEGEN-NL và đào tạo trên BIGQUERY.
Cuối cùng CODEGEN-MONO được khởi tạo từ CODEGEN-MULTI và đào tạo trên BIGPYTHON.

Sự xuất hiện của tổng hợp chương trình có điều kiện trên mô tả bằng ngôn ngữ tự nhiên có thể bắt nguồn từ
kích thước của các mô hình và dữ liệu, mục tiêu đào tạo, và bản chất của chính dữ liệu đào tạo. Điều này được gọi là
xuất hiện vì chúng tôi không đào tạo mô hình một cách rõ ràng trên các cặp comment-code. Các hiện tượng tương tự được
quan sát trong một loạt rộng các nhiệm vụ ngôn ngữ tự nhiên nơi một mô hình ngôn ngữ không giám sát quy mô lớn
có thể giải quyết các nhiệm vụ chưa thấy theo kiểu zero-shot (Brown et al., 2020). Hiện tượng xuất hiện hoặc
khái quát hóa zero-shot đáng ngạc nhiên thường được quy cho quy mô lớn của mô hình và dữ liệu.

Trong khi trọng tâm của chúng tôi không phải là tiết lộ cơ chế cơ bản về tại sao khả năng tổng hợp chương trình
xuất hiện từ mô hình hóa ngôn ngữ đơn giản, chúng tôi cố gắng cung cấp một lời giải thích dựa trên bản chất
của cách tiếp cận mô hình hóa của chúng tôi và dữ liệu đào tạo. Dữ liệu bao gồm mã thông thường từ
GitHub (không có lựa chọn thủ công), mà một số dữ liệu thể hiện một mẫu ngôn ngữ tự nhiên và lập trình xen kẽ
, mà chúng tôi tin rằng cung cấp một tín hiệu giám sát ồn ào cho khả năng tổng hợp chương trình
do mục tiêu đào tạo dự đoán token tiếp theo. Tuy nhiên, chúng tôi nhấn mạnh rằng
mẫu dữ liệu như vậy rất ồn ào và yếu, bởi vì chỉ một tập con dữ liệu thể hiện mẫu như vậy, ví dụ:
các comments có thể không chính xác hoặc không thông tin, và một số trong số chúng thậm chí có thể được đặt ở một vị trí không liên quan
3

--- TRANG 4 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
Mô hình pass@k [%]
k=1 k=10 k=100
GPT-NEO 350M 0.85 2.55 5.95
GPT-NEO 2.7B 6.41 11.27 21.37
GPT-J 6B 11.62 15.74 27.74
CODEX 300M 13.17 20.37 36.27
CODEX 2.5B 21.36 35.42 59.50
CODEX 12B 28.81 46.81 72.31
code-cushman-001 33.5 54.3 77.4
code-davinci-001 39.0 60.6 84.1
code-davinci-002 47.0 74.9 92.1
CODEGEN-NL 350M 2.12 4.10 7.38
CODEGEN-NL 2.7B 6.70 14.15 22.84
CODEGEN-NL 6.1B 10.43 18.36 29.85
CODEGEN-NL 16.1B 14.24 23.46 38.33
CODEGEN-MULTI 350M 6.67 10.61 16.84
CODEGEN-MULTI 2.7B 14.51 24.67 38.56
CODEGEN-MULTI 6.1B 18.16 28.71 44.85
CODEGEN-MULTI 16.1B 18.32 32.07 50.80
CODEGEN-MONO 350M 12.76 23.11 35.19
CODEGEN-MONO 2.7B 23.70 36.64 57.01
CODEGEN-MONO 6.1B 26.13 42.29 65.82
CODEGEN-MONO 16.1B 29.28 49.86 75.00

Bảng 1: Kết quả đánh giá trên bộ test chuẩn HumanEval. Mỗi pass@k (trong đó k∈{1;10;100})
cho mỗi mô hình được tính với ba nhiệt độ sampling (t∈{0.2;0.6;0.8}) và cao nhất
trong ba nhiệt độ được hiển thị, điều này tuân theo quy trình đánh giá trong Chen et al. (2021).
Kết quả cho mô hình được đánh dấu với † là từ Chen et al. (2022).

vị trí. Do đó, chúng tôi tin rằng hai yếu tố chính đóng góp vào khả năng tổng hợp chương trình: 1)
quy mô lớn của kích thước mô hình và kích thước dữ liệu và 2) tín hiệu ồn ào trong dữ liệu đào tạo.

Việc mở rộng của những LLM như vậy đòi hỏi song song dữ liệu và mô hình. Để giải quyết những yêu cầu này, một
thư viện đào tạo JAXFORMER (https://github.com/salesforce/jaxformer) đã được phát triển cho
đào tạo hiệu quả trên phần cứng TPU-v4 của Google. Chúng tôi tham khảo Phụ lục A cho các chi tiết về
triển khai kỹ thuật và các scheme sharding. Bảng 6 tóm tắt các siêu tham số.

3 ĐÁNH GIÁ ĐƠN LƯỢT

Chúng tôi đầu tiên đánh giá CODEGEN của chúng tôi sử dụng một bộ test chuẩn tổng hợp chương trình hiện có: HumanEval (giấy phép MIT) (Chen et al., 2021). HumanEval chứa 164 vấn đề lập trình Python được viết bằng tay.
Mỗi vấn đề cung cấp một prompt với mô tả của hàm sẽ được tạo ra, chữ ký hàm,
và các trường hợp test ví dụ dưới dạng assertions. Mô hình cần hoàn thành một hàm dựa trên
prompt sao cho nó có thể vượt qua tất cả các trường hợp test được cung cấp, do đó đo lường hiệu suất bằng tính đúng đắn chức năng
. Vì một ý định người dùng được chỉ định trong một prompt đơn và được cung cấp cho mô hình một lần, chúng tôi
coi đánh giá trên HumanEval như một đánh giá đơn lượt, để phân biệt nó với đánh giá đa lượt
mà chúng tôi giới thiệu trong phần tiếp theo. Tuân theo Chen et al. (2021), chúng tôi sử dụng nucleus
sampling (Holtzman et al., 2020) với top-p trong đó p=0.95.

3.1 HIỆU SUẤT HUMANEVAL TỶ LỆ THUẬN VỚI HÀMKÍCH THƯỚC MÔ HÌNH VÀ KÍCH THƯỚC DỮ LIỆU

Chúng tôi so sánh các mô hình của chúng tôi với các mô hình Codex (Chen et al., 2021), thể hiện hiệu suất hiện đại
trên HumanEval. Hơn nữa, các mô hình của chúng tôi được so sánh với các mô hình ngôn ngữ lớn mã nguồn mở
, GPT-NEO (Black et al., 2021) và GPT-J (Wang & Komatsuzaki, 2021). Những mô hình này
được đào tạo trên THEPILE (Gao et al., 2020), và do đó tương tự như các mô hình CODEGEN-NL của chúng tôi
, về mặt dữ liệu đào tạo và kích thước mô hình. Tất cả các mô hình được đánh giá với nhiệt độ t∈{0.2;0.6;0.8}, và
chúng tôi tính pass@k trong đó k∈{1;10;100} cho mỗi mô hình. Để so sánh trực tiếp với các kết quả
của Chen et al. (2021), chúng tôi chọn nhiệt độ mang lại pass@k có hiệu suất tốt nhất cho mỗi
4

--- TRANG 5 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
CODEGEN-MONO 350M 2.7B 6.1B 16.1B
Pass 3.78±0.23 3.66±0.14 3.35±0.13 3.12±0.11
Non-Pass 5.18±0.19 4.37±0.18 3.88±0.13 3.40±0.11

Bảng 2: Perplexity prompt trung bình ± (sai số chuẩn) của các mô hình CODEGEN-MONO trên các vấn đề pass và
non-pass.

k. Kết quả của các mô hình của chúng tôi và các baseline được tóm tắt trong Bảng 1. Các mô hình CODEGEN-NL của chúng tôi
(350M, 2.7B, 6.1B) vượt trội hoặc có hiệu suất ngang bằng với các mô hình GPT-NEO và GPT-J tương ứng.

Đào tạo thêm CODEGEN-NL trên dữ liệu ngôn ngữ lập trình đa ngôn ngữ (BIGQUERY) dẫn đến
CODEGEN-MULTI. Các mô hình CODEGEN đa ngôn ngữ vượt trội các mô hình được đào tạo trên THEPILE
(GPT-NEO, GPT-J, CODEGEN-NL) với một khoảng cách lớn. Sau đó chúng tôi tinh chỉnh CODEGEN-MULTI trên một
bộ dữ liệu chỉ có Python (BIGPYTHON), kết quả là CODEGEN-MONO. Khả năng tổng hợp chương trình
được cải thiện đáng kể. Do đó, khả năng tổng hợp chương trình Python tăng cường khi lượng dữ liệu đào tạo Python
tăng. Đối với hầu hết tất cả các mô hình, như mong đợi, tăng kích thước của mô hình cải thiện hiệu suất tổng thể.

Các mô hình CODEGEN đơn ngôn ngữ Python của chúng tôi có hiệu suất cạnh tranh hoặc cải thiện, so sánh
với các mô hình hiện đại hiện tại, Codex. CODEGEN-MONO 2.7B kém hiệu suất CODEX 2.5B
khi k=100 nhưng vượt trội nó khi k∈{1;10}. Trong khi nó chỉ bằng một nửa kích thước, CODEGEN-
MONO 6.1B của chúng tôi thể hiện điểm pass@k tiếp cận những của Codex có hiệu suất tốt nhất, CODEX
12B. Mô hình lớn nhất của chúng tôi CODEGEN-MONO 16.1B có tính cạnh tranh hoặc vượt trội nó tùy thuộc vào k.

3.2 HIỂU Ý ĐỊNH NGƯỜI DÙNG TỐT HỌN DẪN ĐẾN CÁC CHƯƠNG TRÌNH ĐƯỢC TỔNG HỢP TỐT HỌN

Thành công của một hệ thống tổng hợp chương trình phụ thuộc rất nhiều vào mức độ hiểu biết ý định người dùng của nó.
Khi hệ thống dựa trên một mô hình ngôn ngữ, perplexity của các prompt vấn đề cung cấp một proxy
cho sự hiểu biết của hệ thống về đặc tả ý định người dùng. Một perplexity thấp của một đặc tả ý định
dưới một mô hình cho biết rằng đặc tả ý định này tương thích với kiến thức được học bởi
mô hình từ dữ liệu đào tạo. Chúng tôi điều tra liệu sự hiểu biết prompt tốt hơn, với perplexity prompt thấp hơn
như một proxy, có dẫn đến các chương trình chính xác về mặt chức năng hơn.

Chúng tôi phân chia tất cả các vấn đề thành những vấn đề pass và non-pass. Một vấn đề pass là một vấn đề mà ít nhất một
mẫu từ 200 mẫu vượt qua tất cả các trường hợp test, trong khi đối với một vấn đề non-pass không có mẫu nào trong 200 mẫu
vượt qua tất cả các trường hợp test. Chúng tôi tính perplexity trung bình của các prompt vấn đề của các vấn đề pass
và của các vấn đề non-pass, dựa trên các mẫu từ các mô hình CODEGEN-MONO. Kết quả được
hiển thị trong Bảng 2 (xem Phụ lục F cho kết quả trên CODEGEN-NL và CODEGEN-MULTI). Các
prompt của các vấn đề pass có perplexity thấp hơn so với các vấn đề non-pass. Phát hiện này
ngụ ý rằng tổng hợp chương trình có nhiều khả năng thành công hơn khi đặc tả ý định người dùng được
hiểu tốt hơn bởi mô hình. Thật vậy, một số dữ liệu đào tạo chứa các chuỗi xen kẽ của các comments ngôn ngữ tự nhiên
và chương trình, nơi các comments mô tả chức năng của chương trình sau đó. Do đó chúng tôi suy đoán rằng các đặc tả ý định người dùng
tương tự như mẫu như vậy sẽ được hiểu tốt hơn bởi mô hình, và do đó dẫn đến tổng hợp chương trình tốt hơn
. Được truyền cảm hứng bởi mẫu này, chúng tôi đề xuất chỉ định ý định người dùng trong nhiều lượt sao cho mô hình tập trung vào một vấn đề từng phần tại một
thời điểm, điều này sẽ làm cho việc hiểu ý định người dùng bởi mô hình dễ dàng hơn.

4 ĐÁNH GIÁ ĐA LƯỢT

Trong phần này, chúng tôi đề xuất và nghiên cứu một mô hình tổng hợp chương trình đa bước nơi tổng hợp chương trình
được phân giải thành nhiều bước và hệ thống tổng hợp một chương trình con trong mỗi bước. Để
kiểm tra mô hình như vậy, chúng tôi đầu tiên phát triển một Multi-Turn Programming Benchmark (MTPB). MTPB
bao gồm 115 vấn đề được viết bởi các chuyên gia, mỗi vấn đề bao gồm một mô tả đa bước bằng
ngôn ngữ tự nhiên (prompt). Để giải quyết một vấn đề, một mô hình cần tổng hợp các chương trình con đúng chức năng
(1) tuân theo mô tả ở bước hiện tại và (2) xem xét các mô tả và
chương trình con được tổng hợp ở các bước trước đó (ví dụ: backreference chính xác của các hàm và/hoặc biến
được định nghĩa trong các bước trước đó). Một ví dụ minh họa được hiển thị trong Hình 1.
5

--- TRANG 6 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
Mẫu
Nối Lượt 1 Lượt 2 Lượt 3 Lượt 4 Lượt 5
"abc xyz" Thực thi # Import re và định nghĩa một biểu thức chính quy khớp với một ... import re email_regex = re.compile( "([a-zA-Z0-9_\-\.]+)@([a-zA-Z0-9_\-\.]+)\.([a-zA-Z]{2,5})") # Tìm kiếm một địa chỉ email trong "... abc.xyz@example.com ..." và ... address = email_regex.search("... abc.xyz@example.com ...") # Loại bỏ chuỗi con bắt đầu từ ký hiệu @ từ "address". address = address.group(0) address = address[:address.find("@")] # Thay thế các ký hiệu không phải chữ cái bằng khoảng trắng trong "address". address = re.sub("[^a-zA-Z]+", " ", address) # In ra "address". print(address) Tạo ra

Đầu ra Thực tế Đánh giá 2
3
"abc xyz" Diễn ngôn

Đầu ra Mong đợi Đầu vào Người # Import re và định nghĩa một biểu thức chính quy khớp với một địa chỉ email. import re email_regex = re.compile("[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+") Mô hình

Mô hình address = email_regex.search("... abc.xyz@example.com ...") Tìm kiếm một địa chỉ email trong "{input}" và lưu trữ kết quả khớp đầu tiên vào một biến "address". Người Loại bỏ chuỗi con bắt đầu từ ký hiệu @ từ "address". Người Thay thế các ký hiệu không phải chữ cái bằng khoảng trắng trong "address". Người address = address.group(0) address = address[:address.find("@")] Mô hình

Mô hình address = re.sub("[^a-zA-Z]+", " ", address) In ra "address". Người Mô hình print(address) 1 "... abc.xyz@example.com ..."

✓

Hình 1: Một ví dụ minh họa cho Multi-Turn Programming Benchmark, thực hiện nhiệm vụ trích xuất tên người dùng
của một địa chỉ email. 1 Mỗi vấn đề bao gồm các prompt pi và unit tests,
nơi một số prompt bao gồm các template (ví dụ: {input}) được điền với đầu vào trường hợp test trước khi nó được
đưa cho mô hình. Trong ví dụ được hiển thị, đầu vào là một chuỗi chứa abc.xyz@example.com,
thay thế {input} trong p2, và đầu ra mong đợi là abc xyz. 2 Mô hình của chúng tôi điều kiện trên
nối của các prompt và phản hồi được tạo ra xen kẽ trong quá khứ. 3 Các phản hồi được tạo ra từ
mỗi lượt được nối và thực thi, nơi đầu ra được so sánh với câu trả lời.

4.1 XÂY DỰNG BỘ TEST CHUẨN

Chúng tôi (4 tác giả) bắt đầu bằng việc định nghĩa 4 một tập hợp 115 vấn đề yêu cầu một loạt kiến thức lập trình đa dạng
, bao gồm toán học, các thao tác mảng, xử lý chuỗi, thuật toán, khoa học dữ liệu, và
các vấn đề yêu cầu kiến thức khác, sao cho số lượng vấn đề trong mỗi danh mục có cân bằng đại khái. 5 Đối với mỗi vấn đề, chúng tôi xây dựng một bộ ba
bao gồm các prompt đa lượt P, đầu vào trường hợp test
I, và đầu ra trường hợp test O. Các prompt đa lượt P được thiết kế tuân theo hai ràng buộc:
(1) vấn đề được phân giải thành 3 hoặc nhiều lượt, (2) một lượt đơn không thể được quy cho việc giải quyết
vấn đề. Ví dụ, triển khai một mô hình hồi quy tuyến tính có thể được diễn đạt như "Thực hiện hồi quy tuyến tính trên x và y". Vì nhiệm vụ chính được biểu thị đầy đủ trong prompt này, việc hiểu
prompt này là đủ để thực hiện nhiệm vụ. Chúng tôi tránh những trường hợp như vậy thông qua kiểm tra thủ công và phân phối
giải quyết vấn đề qua các lượt. Cùng với các prompt, chúng tôi giao nhiệm vụ cho tác giả vấn đề chuẩn bị 5 bộ
đầu vào trường hợp test I và đầu ra O để đánh giá đầu ra mô hình với tính đúng đắn chức năng. Để giảm
việc thưởng sai cho các giải pháp dương tính giả đưa ra các chương trình vô nghĩa nhưng vượt qua các test, chúng tôi
kiểm tra và sửa đổi những trường hợp như vậy để đảm bảo chất lượng test.

Không giống như HumanEval mà các mô hình được mong đợi hoàn thành một hàm được định nghĩa một phần, các vấn đề MTPB
chỉ cung cấp các prompt, do đó các mô hình phải tạo ra giải pháp từ đầu. 6
Trong khi việc tạo ra dạng tự do có thể cho phép nhiều giải pháp tiềm năng hơn, việc thiếu một điểm vào
để cung cấp đầu vào trường hợp test làm cho nó thách thức để test mã được tạo ra trên các trường hợp test đa dạng. Để
vượt qua thách thức này, thay vào đó chúng tôi nhúng đầu vào trường hợp test trong các prompt. Cụ thể, các prompt
được viết với chuỗi định dạng của Python 7 nơi các giá trị đầu vào được thay thế cho tên biến
khi một trường hợp test cụ thể được áp dụng cho vấn đề. Ví dụ, một prompt, "Định nghĩa một chuỗi tên 's'

4 Việc viết vấn đề được thực hiện theo định dạng sách đóng, tức là chúng tôi không được phép tham khảo với các
nguồn tài nguyên trực tuyến trong khi viết các vấn đề.
5 Xem Phụ lục D cho danh sách hoàn chỉnh.
6 Để hướng dẫn sampling trong Python, chúng tôi tiền tố prompt với: # Import libraries.\n import numpy as np.
7 https://docs.python.org/3/reference/lexical_analysis.html#f-strings
6

--- TRANG 7 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
Dữ liệu Mô hình Tỷ lệ Pass [%]
350M 2.7B 6.1B 16.1B -
THEPILE GPT-NEO & GPT-J 0.79 8.17 18.86 - -
THEPILE CODEGEN-NL 0.23 15.31 19.37 30.33 -
BIGQUERY CODEGEN-MULTI 4.09 20.82 25.51 26.27 -
BIGPYTHON CODEGEN-MONO 16.98 38.72 43.52 47.34 -
- code-cushman-001 - - - - 56.77
- code-davinci-001 - - - - 55.28
- code-davinci-002 - - - - 59.86

Bảng 3: Kết quả đánh giá trên Multi-Turn Programming Benchmark. Hiệu suất tổng hợp chương trình đa lượt
thay đổi như một hàm của kích thước mô hình (cột) và kích thước dữ liệu code (hàng).

Prompt PPL ± Tỷ lệ Pass [%]
350M 2.7B 6.1B 16.1B 350M 2.7B 6.1B 16.1B
Đơn lượt 13.92±1.89 11.67±1.46 10.58±1.20 10.25±0.99 5.75 25.43 28.48 38.74
Đa lượt 10.09±0.62 8.90±0.52 8.18±0.43 8.05±0.43 16.98 38.72 43.52 47.34

Bảng 4: So sánh giữa đặc tả đa lượt và đơn lượt được nối về perplexity (PPL)
và hiệu suất tổng hợp chương trình (được đo bằng tỷ lệ pass) dưới các mô hình CODEGEN-MONO.

với giá trị {var}.", cùng với một đầu vào trường hợp test var = 'Hello' sẽ được định dạng thành "Định nghĩa một chuỗi tên 's' với giá trị 'Hello'." Cũng xem 1 trong Hình 1 cho một ví dụ.

4.2 MÔI TRƯỜNG THỰC THI VÀ ĐÁNH GIÁ GIẢI PHÁP

Để thực thi, lịch sử của các cặp prompt và completion được tạo ra được nối thành một
chương trình tự chứa (xem 3 trong Hình 1 cho một ví dụ). Chương trình sau đó được thực thi trong một
môi trường Python cô lập tuân theo bộ test chuẩn HumanEval đơn lượt (Chen et al., 2021).
Tuy nhiên, các vấn đề trong HumanEval được xây dựng theo cách mà một chữ ký hàm đã biết
được hoàn thành, do đó việc gọi mã được tạo ra dưới một tập hợp các unit test chức năng là tầm thường. Trong trường hợp đa lượt của chúng tôi
, không có điểm vào (hoặc giá trị trả về) như vậy được đảm bảo được tạo ra. Để
vượt qua vấn đề thiếu chữ ký trả về (hoặc giá trị), prompt cuối cùng của các vấn đề đa lượt trong MTPB
luôn được chỉ định để in ra trạng thái kết quả lên terminal. Sau đó, môi trường thực thi bộ test chuẩn nạp chồng
hàm print(args) của Python và lưu trữ args trên một stack. Nếu mã được lấy mẫu
cho prompt cuối cùng của một vấn đề không bao gồm câu lệnh print(), đây là một quy ước hợp lệ
để in lên terminal trong Python hoặc cụ thể là Jupyter notebooks, thì AST của
mã được tạo ra sẽ bị biến đổi để chèn một lời gọi print(). Cuối cùng, một kiểm tra tương đương thoải mái về kiểu
(ví dụ: chuyển đổi ngầm định giữa lists và tuples) của args với đầu ra vàng được định nghĩa trước
của vấn đề được thực hiện để xác định thất bại hoặc thành công test.

4.3 KHẢ NĂNG LẬP TRÌNH ĐA BƯỚC TỶ LỆ THUẬN VỚI KÍCH THƯỚC MÔ HÌNH VÀ KÍCH THƯỚC DỮ LIỆU

Trong phân tích này, chúng tôi điều tra cách kích thước mô hình và kích thước dữ liệu ảnh hưởng đến khả năng tổng hợp chương trình
trong một mô hình đa lượt. Trong MTPB, mỗi vấn đề có 5 trường hợp test và chúng tôi lấy mẫu 40 mẫu
cho mỗi trường hợp test với mỗi mô hình, dựa trên đó tỷ lệ pass được tính cho mỗi vấn đề.
Kết quả đánh giá MTPB (tỷ lệ pass trung bình) cho các mô hình CODEGEN của chúng tôi, baselines, và các mô hình OpenAI
Codex 8 được hiển thị trong Bảng 3. Rõ ràng, hiệu suất trên MTPB cải thiện như một hàm
của kích thước mô hình và kích thước dữ liệu. Điều này gợi ý rằng khả năng tổng hợp chương trình đa bước tỷ lệ
như một hàm của kích thước mô hình và kích thước dữ liệu. Các mô hình chỉ đơn giản được đào tạo với một mục tiêu mô hình hóa ngôn ngữ tự hồi quy
. Trong khi mô hình và dữ liệu mở rộng, khả năng tổng hợp chương trình đa lượt xuất hiện, nghĩa là,
khả năng tổng hợp các chương trình theo kiểu đa lượt.

8 Truy cập vào ngày 10 tháng 11 năm 2022.
7

--- TRANG 8 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
350M 2.7B 6.1B 16.1B
0
5
10
15
20
25

14.19 14.63
0.19
0.25

22.06
19.67

22.53
9.06
2.99

8.51

1.51
9.35

Số Tham Số Mô Hình Chênh lệch Tỷ lệ Pass Dễ
Trung bình
Khó

Hình 2: Chênh lệch tỷ lệ pass trung bình của các vấn đề trong công thức đơn lượt và đa lượt qua
các mức độ khó của vấn đề. Sự cải thiện là đáng kể cho hầu hết các kích thước mô hình và mức độ khó,
ngoại trừ các vấn đề dễ với các mô hình lớn hơn.

4.4 HIỂU ĐẶC TẢ NGƯỜI DÙNG TỐT HỎN VỚI PHÂN TÍCH ĐA LƯỢT

Chúng tôi giả thuyết rằng phân tích đa lượt tăng cường sự hiểu biết của mô hình về đặc tả ý định người dùng
, điều này lần lượt dẫn đến khả năng tổng hợp chương trình cao hơn. Để kiểm tra giả thuyết này,
chúng tôi hình thành một đối tác đơn lượt của các đặc tả đa lượt bằng cách nối mỗi đặc tả
thành một lượt đơn. Như đã thảo luận trong Phần 3.2, chúng tôi áp dụng perplexity prompt như một proxy cho
sự hiểu biết ý định người dùng. Do đó, chúng tôi so sánh perplexity của các prompt đa lượt và của các
prompt đơn lượt được nối dưới bốn mô hình CODEGEN-MONO.

Perplexity trung bình (xem Phụ lục E cho các chi tiết tính toán) trên tất cả các vấn đề trong MTPB
được hiển thị trong bảng bên trái của Bảng 4. Đối với tất cả các mô hình, đặc tả đơn lượt có perplexity trung bình cao hơn
so với đặc tả đa lượt. Điều này ngụ ý rằng các đặc tả người dùng đa lượt
có thể được hiểu tốt hơn bởi các mô hình. Chúng tôi nhận thấy rằng perplexity trung bình cho cả đặc tả ý định đa lượt và
đơn lượt dưới các mô hình lớn hơn thấp hơn một chút so với dưới các mô hình nhỏ hơn,
cho thấy rằng các mô hình lớn hơn hiểu ý định người dùng tốt hơn so với các mô hình nhỏ hơn.

Chúng tôi so sánh tỷ lệ pass tổng hợp chương trình với các prompt đa lượt với của các prompt đơn lượt được nối
. Kết quả được hiển thị trong bảng bên phải của Bảng 4. Các đặc tả đa lượt
dẫn đến gần hoặc hơn 10 điểm phần trăm so với các đặc tả đơn lượt cho tất cả các kích thước mô hình
. Cùng với phân tích perplexity ở trên, có vẻ như việc phân tích một đặc tả người dùng thành
nhiều bước và tận dụng khả năng xuất hiện của các mô hình ngôn ngữ lớn cho phép chúng tiêu hóa
đặc tả dễ dàng hơn và tổng hợp các chương trình thành công hơn.

Hơn nữa, chúng tôi phân loại các vấn đề theo mức độ khó dựa trên tỷ lệ pass trung bình của chúng ("khó"
với ít hơn 30%, "dễ" với lớn hơn 70%), và kiểm tra hiệu ứng tương tác giữa mức độ khó
và kích thước mô hình trên sự cải thiện bằng phân tích đa lượt. Xem kết quả trong Hình 2.
Trên hầu hết tất cả các kích thước mô hình và mức độ khó, các prompt đa lượt dẫn đến cải thiện đáng kể
so với các prompt đơn lượt và hầu hết các cải thiện gần hoặc cao hơn 10 điểm phần trăm.
Thú vị, các mô hình lớn hơn (6.1B và 16.1B) không thay đổi với phân tích đa lượt cho các vấn đề dễ
(xem hai thanh ngắn, 0.19% và 0.25%, trong Hình 2). Điều này ngụ ý rằng khi các
vấn đề có thể được hiểu dễ dàng bởi mô hình (do hiệu ứng kết hợp của tính dễ dàng của các
vấn đề và khả năng cao của các mô hình lớn hơn), không cần thiết hoặc có lợi để phân tích các
đặc tả. Điều này thực tế phù hợp với giả định động lực của chúng tôi rằng việc phân tích các đặc tả phức tạp
sẽ làm dễ dàng việc hiểu vấn đề và cải thiện tổng hợp chương trình.

4.5 VÍ DỤ ĐỊNH TÍNH

Để hiểu thêm về sự khác biệt trong hành vi mô hình qua các kích thước mô hình, chúng tôi kiểm tra các trường hợp nơi
các mô hình lớn có hiệu suất tương phản với các mô hình nhỏ hơn. Chúng tôi cụ thể chọn các vấn đề
mà CODEGEN-MONO 16.1B và CODEGEN-MONO 2.7B cho thấy một sự khác biệt đáng kể trong
hiệu suất. Trên các vấn đề nơi CODEGEN-MONO 16.1B có hiệu suất kém đáng kể so sánh
với CODEGEN-MONO 2.7B, chúng tôi quan sát rằng mô hình lớn hơn trở nên không linh hoạt do việc lấy
prompt theo nghĩa đen. Ví dụ, khởi tạo một số luôn dẫn đến một integer, mặc dù
prompt yêu cầu ép kiểu thành một string (Hình 3), hoặc từ khóa "return" trong một prompt kích hoạt một định nghĩa hàm
trong khi ý định là tạo ra trực tiếp một chương trình có thể thực thi (Hình 4). Tuy nhiên nói chung, các mô hình quy mô lớn hơn
vượt qua các lỗi do hiểu sai prompt bởi các mô hình nhỏ hơn,
bao gồm việc gán nhiều biến cùng một lúc (Hình 5) hoặc hiểu khái niệm về
so sánh bất kỳ (Hình 6).
8

--- TRANG 9 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
5 CÔNG VIỆC LIÊN QUAN

Tổng hợp Chương trình Trong khi tổng hợp chương trình có một lịch sử dài, hai thách thức cố hữu vẫn
chưa được giải quyết: (1) tính không thể theo dõi của không gian chương trình và (2) khó khăn trong việc biểu thị chính xác ý định người dùng (Manna & Waldinger, 1971; Gulwani et al., 2017). Một khối lượng lớn nghiên cứu trước đây đã cố gắng
giải quyết (1) bằng cách khám phá các phương pháp như kỹ thuật tìm kiếm ngẫu nhiên (Parisotto et al., 2017; Schkufza
et al., 2013) và tìm kiếm top-down suy diễn (Gulwani, 2011; Polozov & Gulwani, 2015). Tuy nhiên,
khả năng mở rộng của những cách tiếp cận này vẫn còn hạn chế. Ý định người dùng có thể được biểu thị với các phương pháp khác nhau:
đặc tả logic chính thức, ví dụ đầu vào-đầu ra, và mô tả ngôn ngữ tự nhiên. Các đặc tả hoàn chỉnh và chính thức
yêu cầu quá nhiều nỗ lực, trong khi các đặc tả không chính thức như ví dụ đầu vào-đầu ra
thường đặc tả dưới mức các vấn đề (Gulwani, 2011). Phân phối có điều kiện được học tốt và khả năng hiểu ngôn ngữ
nhờ vào mô hình và dữ liệu quy mô lớn cho phép các giải pháp hiệu quả cho
hai thách thức này. Một số công việc điều tra việc chuyển đổi các ý định đàm thoại thành các biểu diễn có thể lập trình
, như SQL (Yu et al., 2019a;b) hoặc đồ thị dataflow (Andreas et al., 2020). Bộ test chuẩn được đề xuất của chúng tôi
yêu cầu việc tạo ra Python, tổng quát và phức tạp hơn.

Mô hình Ngôn ngữ Lớn Transformers nắm bắt sự phụ thuộc giữa các phần tử chuỗi thông qua
cơ chế attention (Bahdanau et al., 2014) và có khả năng mở rộng cao. Nó đã được áp dụng thành công cho
xử lý ngôn ngữ tự nhiên (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), thị giác máy tính
(Dosovitskiy et al., 2021), và nhiều lĩnh vực khác (Oord et al., 2018; Jumper et al., 2021). Các công việc trước đây
, như CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al.,
2020), và CodeT5 (Wang et al., 2021), đã áp dụng transformers hướng tới hiểu code nhưng
những cái này chủ yếu tập trung vào truy xuất code, phân loại, và sửa chữa chương trình. Một số nỗ lực gần đây và đồng thời
khám phá việc sử dụng các mô hình ngôn ngữ lớn cho tổng hợp chương trình (Chen et al., 2021; Austin et al.,
2021; Li et al., 2022; Fried et al., 2022) và hiệu quả của nó (Vaithilingam et al., 2022). Trong khi họ
tập trung vào việc tạo ra code trong một lượt đơn, chúng tôi đề xuất phân tích các đặc tả thành nhiều lượt
và chứng minh rằng nó rất hiệu quả để cải thiện chất lượng tổng hợp. Đáng chú ý rằng
Austin et al. (2021) đã khám phá việc tinh chỉnh code trong nhiều lần lặp, nhưng về bản chất nó là một cách tiếp cận đơn lượt
vì một chương trình hoàn chỉnh được sản xuất trong mỗi lượt đơn. Việc prompting các mô hình ngôn ngữ được pre-train
với thông tin trung gian để cải thiện hiệu suất nhiệm vụ đã thu hút sự quan tâm (Nye et al.,
2021; Wei et al., 2022). MTPB được đề xuất của chúng tôi cũng cho phép mô hình tận dụng các lượt quá khứ như ngữ cảnh.

Bộ test chuẩn cho Tổng hợp Chương trình Để đánh giá định lượng các mô hình tổng hợp chương trình, một số
bộ test chuẩn đã được đề xuất với các dạng đầu vào khác nhau. Một dạng đầu vào phổ biến bao gồm code trước đó
trong cùng dòng (Raychev et al., 2016), pseudo-code (Kulal et al., 2019), một docstring và
chữ ký hàm (Chen et al., 2021), hoặc mô tả vấn đề (Hendrycks et al., 2021). Trong hầu hết
những trường hợp đó, chỉ thông tin đầu vào trực tiếp liên quan được đưa cho mô hình. Ngược lại, một vài công việc trước đây
khởi tạo các bộ test chuẩn đo lường khả năng tạo ra các chương trình dựa trên ngữ cảnh chương trình xung quanh
ngoài chương trình mục tiêu, như các biến và phương thức khác (Iyer et al., 2018) hoặc
các "cell" xen kẽ của code trước đó và các khối văn bản (Agashe et al., 2019), trong khi trọng tâm chính
là tạo ra chính chương trình mục tiêu. Chúng tôi đề xuất một bộ test chuẩn mới yêu cầu một việc tạo ra tiến bộ
các chương trình con thông qua các prompt đa lượt.

6 KẾT LUẬN

Chúng tôi nghiên cứu tổng hợp chương trình với các mô hình ngôn ngữ nhân quả lớn được đào tạo trên các corpus lớn của dữ liệu code
. Khả năng hiểu ngữ cảnh dài và tạo ra các phản hồi mạch lạc xuất hiện từ
mô hình hóa ngôn ngữ đơn giản khi kích thước mô hình và kích thước dữ liệu mở rộng. Tận dụng khả năng này và
quan sát rằng hiểu ý định người dùng tốt hơn dẫn đến tổng hợp chương trình tốt hơn, chúng tôi đề xuất một
cách tiếp cận tổng hợp chương trình đa bước trong đó tổng hợp chương trình được đạt được thông qua một đặc tả đa lượt
và tạo ra code. Hơn nữa, chúng tôi phát triển Multi-Turn Programming Benchmark
(MTPB) để điều tra khả năng của các mô hình của chúng tôi về tổng hợp các chương trình trong mô hình đa bước như vậy.
Các thí nghiệm của chúng tôi cho thấy rằng khả năng tổng hợp chương trình đa bước tỷ lệ như một hàm của
kích thước mô hình và kích thước dữ liệu. Các đặc tả ý định, được chỉ định trong nhiều bước, được tiêu hóa
dễ dàng hơn bởi các mô hình và dẫn đến tổng hợp chương trình chính xác hơn. Chúng tôi mã nguồn mở code đào tạo
và các checkpoint mô hình để tạo điều kiện thuận lợi cho nghiên cứu tương lai và các ứng dụng thực tế trong lĩnh vực này.
9

--- TRANG 10 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
TÁC ĐỘNG RỘNG HỌN VÀ CÂN NHẮC ĐẠO ĐỨC

Tất cả các biến thể của CODEGEN đều được pre-train đầu tiên trên the Pile, bao gồm một phần nhỏ
ngôn ngữ tục tĩu. Tập trung vào dữ liệu GitHub phù hợp nhất với trường hợp sử dụng mong đợi của chúng tôi về tổng hợp chương trình
, Gao et al. (2020) báo cáo rằng 0.1% dữ liệu chứa ngôn ngữ tục tĩu, và có
thành kiến cảm tính chống lại giới tính và một số nhóm tôn giáo nhất định. Vì vậy, trong khi chúng tôi không quan sát trong các mẫu của mình,
CODEGEN cũng có thể tạo ra nội dung như vậy. Ngoài các rủi ro trên đầu ra ngôn ngữ tự nhiên
(ví dụ: docstrings), các chương trình được tạo ra có thể bao gồm các lỗ hổng và mối quan tâm an toàn, không được
khắc phục trong công việc này. Các mô hình không nên được sử dụng trong các ứng dụng cho đến khi được xử lý cho những rủi ro này.

TÀI LIỆU THAM KHẢO

Rajas Agashe, Srinivasan Iyer, và Luke Zettlemoyer. Juice: A large scale distantly supervised
dataset for open domain context-based code generation. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 5436–5446, 2019.

Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim,
Jordan DeLoach, Leah Dorner, Jason Eisner, et al. Task-oriented dialogue as dataflow synthesis.
Transactions of the Association for Computational Linguistics, 8:556–571, 2020.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.

Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/
10.5281/zenodo.5297715. Nếu bạn sử dụng phần mềm này, vui lòng trích dẫn nó sử dụng các metadata này.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, và Weizhu Chen.
Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Alvin Cheung, Armando Solar-Lezama, và Samuel Madden. Optimizing database-backed applica-
tions with query synthesis. ACM SIGPLAN Notices, 48(6):3–14, 2013.

Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, và Neel Sundaresan. Pymt5:
multi-mode translation of natural language and python code with transformers. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.
9052–9065, 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N19-1423.
10

--- TRANG 11 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
In ICLR, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou,
Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and
natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020,
pp. 1536–1547, 2020.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,
Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. Incoder: A generative model for code infilling
and synthesis. arXiv preprint arXiv:2204.05999, 2022.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027, 2020.

Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. ACM
Sigplan Notices, 46(1):317–330, 2011.

Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and
Trends® in Programming Languages, 4(1-2):1–119, 2017.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin
Burns, Samir Puranik, Horace He, Dawn Song, và Jacob Steinhardt. Measuring coding challenge
competence with APPS. In Thirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=
sD9GOzH3i5.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text
degeneration. In ICLR, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. Mapping language to code in
programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, pp. 1643–1652, Brussels, Belgium, October-November 2018. Association
for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https://aclanthology.org/
D18-1192.

Rajeev Joshi, Greg Nelson, và Keith Randall. Denali: A goal-directed superoptimizer. ACM
SIGPLAN Notices, 37(5):304–314, 2002.

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, và Kensen Shi. Learning and evaluating
contextual embedding of source code. In International Conference on Machine Learning, pp.
5110–5121. PMLR, 2020.

Diederik P. Kingma và Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
2015. URL http://arxiv.org/abs/1412.6980.

Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, và Percy S
Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing
Systems, 32, 2019.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, và Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020.
11

--- TRANG 12 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien
de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal,
Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,
Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. Competition-level code generation with
alphacode, Feb 2022.

Zohar Manna và Richard J Waldinger. Toward automatic program synthesis. Communications of
the ACM, 14(3):151–165, 1971.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,
2021.

Aaron van den Oord, Yazhe Li, và Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.

Pavel Panchekha, Alex Sanchez-Stern, James R Wilcox, và Zachary Tatlock. Automatically
improving accuracy for floating point expressions. ACM SIGPLAN Notices, 50(6):1–11, 2015.

Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, và Pushmeet
Kohli. Neuro-symbolic program synthesis. In ICLR (Poster), 2017. URL https://openreview.
net/forum?id=rJ0JwFcex.

Razvan Pascanu, Tomas Mikolov, và Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310–1318. PMLR, 2013.

Oleksandr Polozov và Sumit Gulwani. Flashmeta: A framework for inductive program synthe-
sis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented
Programming, Systems, Languages, and Applications, pp. 107–126, 2015.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 21:1–67, 2020.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory optimizations
toward training trillion parameter models. In SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020.

Veselin Raychev, Pavol Bielik, và Martin Vechev. Probabilistic model for code with decision trees.
ACM SIGPLAN Notices, 51(10):731–747, 2016.

Eric Schkufza, Rahul Sharma, và Alex Aiken. Stochastic superoptimization. ACM SIGARCH
Computer Architecture News, 41(1):305–316, 2013.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, và Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053, 2019.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, và Yunfeng Liu. Roformer: Enhanced transformer with
rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

Priyan Vaithilingam, Tianyi Zhang, và Elena L Glassman. Expectation vs. experience: Evaluating
the usability of code generation tools powered by large language models. In CHI Conference on
Human Factors in Computing Systems Extended Abstracts, pp. 1–7, 2022.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Ben Wang và Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
12

--- TRANG 13 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Yue Wang, Weishi Wang, Shafiq Joty, và Steven C.H. Hoi. Codet5: Identifier-aware unified pre-
trained encoder-decoder models for code understanding and generation. In Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 2021.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.

Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze
Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri,
Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard
Socher, Walter Lasecki, và Dragomir Radev. CoSQL: A conversational text-to-SQL challenge
towards cross-domain natural language interfaces to databases. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1962–1979, Hong Kong,
China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1204.
URL https://aclanthology.org/D19-1204.

Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,
Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent
Zhang, Caiming Xiong, Richard Socher, và Dragomir Radev. SParC: Cross-domain semantic
parsing in context. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 4511–4523, Florence, Italy, July 2019b. Association for Computational Linguistics.
doi: 10.18653/v1/P19-1443. URL https://aclanthology.org/P19-1443.
13

--- TRANG 14 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
A ĐÀO TẠO MÔ HÌNH

Để đánh giá sự xuất hiện của khả năng tổng hợp chương trình đa lượt dưới quy luật tỷ lệ, chúng tôi
áp dụng các mô hình ngôn ngữ tự hồi quy dựa trên transformer tiêu chuẩn, thay đổi (1) số lượng tham số mô hình
(350M, 2.7B, 6.1B, 16.1B) và (2) số lượng token của ngôn ngữ lập trình trong các
corpus đào tạo. Để mở rộng các mô hình, một thư viện tùy chỉnh JAXFORMER cho đào tạo các mô hình ngôn ngữ lớn
trên phần cứng TPU-v4 đã được phát triển và sẽ được phát hành như mã nguồn mở, bao gồm các trọng số mô hình đã đào tạo.

A.1 BỘ DỮ LIỆU

Bộ dữ liệu Ngôn ngữ Kích thước Thô Kích thước Cuối Tokens Cuối
THEPILE Ngôn ngữ Tự nhiên 825.18GiB 1159.04GiB 354.7B
Code 95.16GiB 95.16GiB 31.6B
BIGQUERY C 1772.1GiB 48.9GiB 19.7B
C++ 205.5GiB 69.9GiB 25.5B
Go 256.4GiB 21.4GiB 9.6B
Java 335.1GiB 120.3GiB 35.4B
JavaScript 1282.3GiB 24.7GiB 9.7B
Python 196.8GiB 55.9GiB 19.3B
BIGPYTHON Python 5558.1GiB 217.3GiB 71.7B

Bảng 5: Thống kê gần đúng cho các corpus đào tạo dọc theo các bước tiền xử lý.

Đối với mỗi bộ dữ liệu, tiền xử lý chia sẻ các bước sau: (1) lọc, (2) khử trùng lặp, (3) tokenization, (4) xáo trộn, và
(5) nối. Đối với chi tiết về THEPILE, chúng tôi tham khảo Gao et al. (2020). Đối với BIGQUERY và BIGPYTHON, trong (1)
các tệp được lọc bằng phần mở rộng tệp, và các tệp có độ dài dòng trung bình <100 ký tự, độ dài dòng tối đa
1,000, và >90% ký tự là chữ số thập phân hoặc thập lục phân được loại bỏ. Đối với (2), các bản sao chính xác dựa trên
hash SHA-256 của chúng được loại bỏ, chiếm một phần đáng kể của dữ liệu thô do các fork và bản sao
của repository. Đối với (3), từ vựng BPE của GPT-2 được mở rộng bằng các token đặc biệt đại diện
cho các token lặp lại của tabs và khoảng trắng. Trong cài đặt đa ngôn ngữ của BIGQUERY, một tiền tố được
thêm vào để chỉ ra tên của ngôn ngữ lập trình. Đối với (4), mỗi năm dữ liệu được xáo trộn ngẫu nhiên. Đối với (5), các chuỗi được nối để
điền độ dài ngữ cảnh 2,048 token với một token đặc biệt như một dấu phân cách. Bảng 5 tóm tắt thống kê của các corpus đào tạo.

Các mô hình CODEGEN-NL được khởi tạo ngẫu nhiên và đào tạo trên THEPILE. Các mô hình CODEGEN-MULTI
được khởi tạo từ CODEGEN-NL và sau đó đào tạo trên BIGQUERY. Các mô hình CODEGEN-MONO
được khởi tạo từ CODEGEN-MULTI và sau đó đào tạo trên BIGPYTHON.

A.2 MÔ HÌNH

Các mô hình của chúng tôi là các transformer tự hồi quy với mục tiêu học mô hình hóa ngôn ngữ dự đoán token tiếp theo thông thường
. Họ các mô hình CODEGEN được đào tạo trong các kích thước khác nhau với 350M, 2.7B,
6.1B, và 16.1B tham số. Ba cấu hình đầu cho phép so sánh trực tiếp với các mô hình ngôn ngữ lớn mã nguồn mở
được đào tạo trên corpus văn bản, GPT-NEO (350M, 2.7B) (Black et al., 2021) và GPT-J (6B) (Wang & Komatsuzaki, 2021). Xem Bảng 6 trong Phụ lục A cho đặc tả mô hình.

Kiến trúc tuân theo một decoder transformer tiêu chuẩn với masking nhân quả từ trái sang phải. Đối với mã hóa vị trí
, chúng tôi áp dụng rotary position embedding (Su et al., 2021). Đối với forward pass, chúng tôi
thực thi các mạch self-attention và feed-forward song song để cải thiện overhead giao tiếp
tuân theo Wang & Komatsuzaki (2021), nghĩa là, x_t+1 = x_t + mlp(ln(x_t + attn(ln(x_t)))) được thay đổi
thành x_t+1 = x_t + attn(ln(x_t)) + mlp(ln(x_t)) mà tính toán của self-attention, attn(), và
feed-forward, mlp(), với layer-norm, ln(), là đồng thời. Kiến trúc và lựa chọn siêu tham số
được tối ưu hóa cụ thể cho bố cục phần cứng của TPU-v4.
14

--- TRANG 15 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Mô hình Bộ dữ liệu Siêu tham số 350M 2.7B 6.1B 16.1B
CODEGEN Số lớp 20 32 33 34
Số đầu 16 32 16 24
Kích thước mỗi đầu 64 80 256 256
Độ dài ngữ cảnh 2,048 2,048 2,048 2,048
Kích thước batch 500k 1M 2M 2M
Weight decay 0.1 0.1 0.1 0.1
CODEGEN-NL THEPILE Tỷ lệ học 3.0e-4 1.6e-4 1.2e-4 0.9e-4
Bước warm-up 3k 3k 3k 3k
Bước Warm-up / Tổng 350k 350k 350k 350k
CODEGEN-MULTI BIGQUERY Tỷ lệ học 1.8e-4 0.8e-4 0.4e-4 0.5e-4
Bước warm-up 3k 3k 3k 3k
Tổng bước 150k 150k 150k 150k
CODEGEN-MONO BIGPYTHON Tỷ lệ học 1.8e-4 0.8e-4 0.4e-4 0.5e-4
Bước warm-up 3k 3k 3k 3k
Tổng bước 150k 150k 150k 150k

Bảng 6: Siêu tham số cho đặc tả mô hình và tối ưu hóa cho họ các mô hình CODEGEN.

A.3 ĐÀO TẠO

Việc mở rộng các mô hình ngôn ngữ lớn đòi hỏi song song dữ liệu và mô hình. Phần cứng TPU-v4 của Google
với kết nối mesh toroidal tốc độ cao tự nhiên cho phép song song hiệu quả. Để hiệu quả
sử dụng phần cứng, việc đào tạo các mô hình được triển khai trong JAX (Bradbury et al., 2018). Để
đánh giá song song trong JAX, toán tử pjit() 9 được áp dụng. Toán tử cho phép một mô hình được gọi là
single-program, multiple-data (SPMD) code, tham chiếu đến một kỹ thuật song song nơi cùng một
tính toán được chạy trên dữ liệu đầu vào khác nhau song song trên các thiết bị khác nhau. 10 Cụ thể, pjit() là
API được tiếp xúc cho XLA SPMD partitioner trong JAX, cho phép một hàm nhất định được đánh giá song song
với ngữ nghĩa tương đương trên một mesh logic của tính toán.

Thư viện JAXFORMER của chúng tôi tuyển dụng một node coordinator được chỉ định để điều phối cluster của TPU-
VMs 11 với một giao thức TCP/IP tùy chỉnh. Đối với song song dữ liệu, coordinator phân chia một batch và
phân phối các phân vùng cho các TPU-VM riêng lẻ. Đối với song song mô hình, hai scheme cho
sharding của các tham số mô hình được hỗ trợ: (1) Intra-TPU-VM, nơi các tham số được shard
trên các core MXU 12 bên trong một board TPU-v4 vật lý và được nhân bản trên các board tuân theo Shoeybi
et al. (2019); Wang & Komatsuzaki (2021); (2) Inter-TPU-VM, nơi các tham số được shard trên
các board TPU-v4 và các activation được nhân bản tuân theo Rajbhandari et al. (2020).

Cả scheme sharding intra-TPU-VM và inter-TPU-VM đều được triển khai dựa trên đặc tả mesh logic pjit() cụ thể của chúng tôi
(r,p,c) với r bản sao của các tham số, p phân vùng của các
tham số, và c core logic mỗi board trên n_b board TPU với mỗi board n_c core logic sao cho
d·p = n_b và r·p·c = n_b·n_c.

Scheme intra-TPU-VM được áp dụng cho các mô hình có kích thước nhỏ hơn hoặc bằng 6B tham số, tổng số
tham số mô hình và optimizer phù hợp với bộ nhớ HBM kết hợp của một board TPU-v4 đơn
. Ví dụ, một slice TPU-v4-512 với n_b = 64 và n_c = 4 sẽ được cấu hình
như (r,p,c) = (64,1,4). Nghĩa là, các tham số đang được nhân bản trên r = 64 board với
p = 1 tổng phân vùng inter-board và song song intra-board trên c = 4 chip logic. Trong cấu hình này
, gradient trung bình được tích lũy trên các board thông qua with_sharding_constraint(),
hiệu quả mô phỏng hành vi của toán tử xmap() 13.

9 https://jax.readthedocs.io/en/latest/_modules/jax/experimental/pjit.html
10 https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html
11 https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms
12 Cụ thể, 4 chip TPU-v4 (tức là 8 vật lý tương đương 4 core logic hoặc ảo MXU).
13 https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.xmap.html
15

--- TRANG 16 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Scheme inter-TPU-VM được áp dụng cho các mô hình vượt quá kích thước 6B tham số mà
các tham số mô hình và optimizer phải được shard trên các board TPU-v4. Ví dụ, một
slice TPU-v4-512 với n_b = 64 và n_c = 4 sẽ được cấu hình như (r,p,c) = (1,64,4). Đối với
các slice lớn hơn như TPU-v4-1024 với n_b = 128, người ta có thể giới thiệu redundancy trong parameter
sharding, ví dụ, (r,p,c) = (2,64,4). Trong cấu hình này, các activation được nhân bản trên các board
thông qua with_sharding_constraint(). Hơn nữa, (r,p,c) cho phép tương thích ngược cho
chuyển đổi bố cục phần cứng logic từ TPU-v3 với c = 8 sang TPU-v4 với c = 4 bằng cách điều chỉnh p
mà không cần re-sharding.

Đối với tối ưu hóa, Bảng 6 tóm tắt các siêu tham số. Chúng tôi áp dụng optimizer Adam (Kingma & Ba,
2015) với (β₁,β₂,ε) = (0.9,0.999,1e-08) và clipping norm gradient toàn cục (Pascanu
et al., 2013) của 1.0. Hàm tỷ lệ học theo thời gian tuân theo GPT-3 (Brown et al., 2020)
với các bước warm-up và cosine annealing. Tóm lại, chúng tôi chủ yếu áp dụng các cấu hình tham chiếu GPT-3
với các biến thể nhỏ tính đến các tối ưu hóa TPU. Chúng tôi không có khả năng tính toán
để tối ưu hóa những siêu tham số này thêm.

B BỘ ƯỚC TÍNH PASS@k

Chúng tôi sử dụng bộ ước tính không thiên vị được đề xuất trong Chen et al. (2021) để tính pass@k. Đối với mỗi nhiệm vụ,
n ≥ k mẫu được lấy mẫu. Cụ thể, chúng tôi sử dụng n = 200 và k ≤ 100. Giả sử c là số
mẫu chính xác, trong số n mẫu, vượt qua tất cả các unit test. Sau đó bộ ước tính không thiên vị được
định nghĩa như sau:

pass@k = E_Problems [1 - (n-c choose k)/(n choose k)]     (1)

Việc tính toán trực tiếp bộ ước tính này không ổn định về mặt số học. Chúng tôi sử dụng triển khai numpy
ổn định về mặt số học được giới thiệu bởi Chen et al. (2021).

C KIỂM TRA TƯƠNG ĐƯƠNG THOẢI MÁI KIỂU CHO ĐÁNH GIÁ MTPB

Chúng tôi thực hiện việc thoải mái kiểu sau trước khi đánh giá sự tương đương giữa đầu ra mô hình
và các đầu ra mong đợi.

• Chuyển đổi các mảng numpy thành các danh sách được gõ tương ứng của các kiểu tiêu chuẩn (ví dụ: np.int32 sẽ
được ép thành int).
• Các series pandas được chuyển đổi và so sánh ở định dạng mảng numpy.
• Đối với phần còn lại, đầu ra mô hình được ép thành kiểu của đầu ra tiêu chuẩn vàng.
• Các số thực được so sánh với ε = 1e-6 như ngưỡng dung sai.
16

--- TRANG 17 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
D DANH SÁCH CÁC VẤN ĐỀ MTPB

Tên Vấn đề Mô tả Vấn đề Danh mục
Sandwich string Nối một chuỗi vào giữa một chuỗi khác. string
Normalize integer list Chuẩn hóa một danh sách các số nguyên dương và in các phần trăm đã định dạng. math
Convert time Chuyển đổi đơn vị thời gian. math
Squared Fibonacci In các số Fibonacci bình phương. math
Compare counts So sánh số lượng các số dương và âm trong một danh sách cho trước. array
Pandas mean Xây dựng và tính trung bình của một pandas DataFrame. D.S.
Fizz buzz Giải quyết vấn đề fizz buzz. Algo.
Bi-grams In các bi-gram của một câu. string
Top note In tên với điểm cao nhất từ một từ điển. dict
Hex to binary Chuyển đổi hex sang binary và đảo ngược. math
Invert dict Phát hiện một nghịch đảo của một từ điển cho trước. dict
Class definition Tạo một lớp POJO. class
Longest number In số dài nhất. math
Linear regression Fit mô hình hồi quy tuyến tính với hàm được chỉ định và sk-learn. D.S.
Encrypt and decrypt Xoay bảng chữ cái để mã hóa, sau đó đảo ngược thao tác. Algo.
Dedup custom objects Triển khai một lớp với __hash__ và lấy số lượng các đối tượng duy nhất. class
Drunken python Chuyển đổi giữa integer và string mà không sử dụng các hàm built-in. string
Morse code Mã hóa một chuỗi thành morse code dựa trên quy tắc chuyển đổi của nó. Algo.
Two-sum Triển khai vấn đề two-sum trên một cặp đầu vào cho trước. Algo.
k-means Triển khai và chạy k-means trên các điểm được lấy mẫu. D.S.
Even odd sum In tổng các số chẵn và lẻ trong một danh sách. math
Shift zeros Di chuyển tất cả các số không trong một danh sách sang phải. array
Bootstrap 95% CI Tính khoảng tin cậy bootstrap 95% của một mảng. D.S.
Sum even digits Tổng các chữ số chẵn giữa hai số. math
Min-max diff Tính sự khác biệt giữa các số max và min trong một danh sách. array
Distinct chars In các ký tự duy nhất được sắp xếp, không phân biệt hoa thường của một chuỗi. string
Longer string So sánh và in chuỗi dài hơn dựa trên hai chuỗi. string
Sum float digits Tổng các số trước và sau dấu thập phân của một số thực. math
Count vowels Đếm số lượng nguyên âm trong một chuỗi. string
Factorial Tính giai thừa của n. math
Max edge triangle Tìm phạm vi tối đa của cạnh thứ ba của tam giác. math
Factorial & remainder Tính giai thừa và phần dư khi chia. math
Sum polygon angles Tổng các góc trong một đa giác. math
Sum string numbers Cộng hai số được biểu diễn trong chuỗi. string
Min-max sum Tổng phạm vi từ minimum đến maximum của một danh sách. array
Vowel overlap Tìm số lượng nguyên âm chồng chéo của hai từ. string
Sum negative Tính tổng các số âm trong một danh sách. math
Load dataset Tải từ một tệp và in thống kê. D.S.
Char length list Trả về một danh sách độ dài ký tự không phải dấu câu từ các từ. string
Hex to RGB Chuyển đổi một chuỗi sáu chữ số thập lục phân thành danh sách các giá trị RGB. math
Majority vote Kiểm tra xem một phần tử nhất định có phải là đa số của một danh sách cho trước. array
Week later In ngày đã định dạng của một tuần sau dựa trên một ngày. string
Sorted word weights Kiểm tra xem danh sách trọng số từ (tổng các giá trị ASCII) có được sắp xếp. math
Create Palindrome Tổng các cặp chữ số liền kề cho đến khi số đó là palindrome. string
Simulate Backspace Áp dụng các ký tự backspace trong một chuỗi và in chuỗi đã chỉnh sửa. string
Data manipulation Thao tác một pandas DataFrame và chia thành tập train và test. D.S.
Sum non-overlap Tổng các số nguyên trong phạm vi (min, max) không xuất hiện trong một danh sách. array
Detect digits Tìm xem một chuỗi có chứa chữ số. array
Cascading functions Gọi tuần tự các đối tượng hàm trong một danh sách cho trước. math
Pluralize duplicates Số nhiều các từ trùng lặp trong một danh sách. dict
Highest altitude Dựa trên độ cao tương đối, tìm độ cao cao nhất array
Truncate words Cắt bớt một câu sao cho nó chứa k từ array
Single element Tìm các phần tử xuất hiện một lần trong một mảng array
Remove elements Loại bỏ tất cả các lần xuất hiện của một phần tử trong một mảng array
Check array sum Kiểm tra xem tổng của một mảng có bằng một giá trị cho trước array

Bảng 7: Các vấn đề trong MTPB, hiển thị vấn đề 1 đến 55. D.S. và Algo. đề cập đến khoa học dữ liệu và
thuật toán.
17

--- TRANG 18 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Tên Vấn đề Mô tả Vấn đề Danh mục
Merge sorted lists Hợp nhất hai danh sách đã sắp xếp thành một Algo.
Maximum subarray Tìm mảng con liền kề max và trả về tổng Algo.
Max square root integer Tìm số nguyên lớn nhất nhưng nhỏ hơn căn bậc hai Algo.
Longest word Tìm từ dài nhất trong một danh sách từ Algo.
Sum unique elements Tổng tất cả các số duy nhất trong một danh sách Algo.
Diagonal sum Tính tổng đường chéo của một ma trận D.S.
Matrix condition number Kiểm tra số điều kiện của ma trận có nhỏ hơn một ngưỡng D.S.
Matrix multiplication sum Tính tổng nhân ma trận của hai ma trận D.S.
Matrix determinant So sánh hai định thức ma trận D.S.
Log-sum-exp Tính log của tổng đầu vào mũ D.S.
K nearest points Tìm k điểm gần nhất đến gốc tọa độ array
Longest common prefix Tìm tiền tố chung dài nhất của hai chuỗi Algo.
Duplicate elements Tìm các bản sao trong một danh sách array
First unique character Tìm ký tự không lặp lại đầu tiên trong một chuỗi Algo.
Uncommon words Tìm các từ không phổ biến trong hai câu Algo.
Average words length Tính độ dài từ trung bình của một câu Algo.
Compare char freq So sánh tần suất ký tự trong hai chuỗi string
Reverse string Đảo ngược một chuỗi string
Square Sum diff Sự khác biệt giữa bình phương của tổng và tổng của bình phương math
Cosine sim Tính độ tương tự cosine giữa hai vector math
Vector distance So sánh khoảng cách vector đến gốc tọa độ math
Smallest standard dev. Tìm độ lệch chuẩn nhỏ hơn dựa trên hai danh sách D.S.
Smallest means Tìm trung bình nhỏ hơn dựa trên hai danh sách D.S.
Coefficient of variation Tính hệ số biến thiên dựa trên một danh sách D.S.
L1 norm Tính norm L1 dựa trên một danh sách D.S.
Z-statistic Tính z-statistic dựa trên một danh sách D.S.
Move negatives Di chuyển tất cả các phần tử âm trong một danh sách đến cuối array
Remove alphabets Loại bỏ các ký tự chữ cái trong một chuỗi string
Largest norm Tìm norm lớn nhất trong các điểm n-chiều D.S.
F1 score Dựa trên hai mảng (pred, gold), tính điểm F1 D.S.
Add Space Thêm khoảng trắng trước các chữ cái viết hoa string
Remove outlier Loại bỏ các điểm dữ liệu ở đuôi (2sigma) của phân phối chuẩn D.S.
Convert to categorical Chuyển đổi các giá trị thành biến phân loại D.S.
Group by key Nhóm các mục trong một mảng sử dụng một hàm được cung cấp array
Max stock profit Dựa trên một mảng "giá", tìm lợi nhuận max array
Sum positions Tổng tất cả các chỉ số vị trí nơi một giá trị xuất hiện array
Find missing num Tìm một số bị thiếu dựa trên một danh sách và một số max array
Common num in matrix Các số chung giữa các hàng trong một ma trận array
Sum Collatz Lấy tổng của chuỗi Collatz bắt đầu từ số cho trước Algo.
Cup swap Đặt tên vị trí của một "quả bóng" sau khi hoán đổi cốc Algo.
Reverse digits Đảo ngược các chữ số trong một số với một stack Algo.
Calculate arrows Tính mũi tên trái và phải Algo.
Check interval num Kiểm tra xem khoảng (max-min) có được bao gồm trong một danh sách Algo.
Length encoding Mã hóa một chuỗi bằng cách chuyển đổi các ký tự lặp lại với số lượng string
Convert email Sử dụng regex để khớp địa chỉ email và loại bỏ các ký tự đặc biệt string
Second largest In ra phần tử lớn thứ hai trong một mảng array
Largest prefix sum Trả về tổng tiền tố lớn nhất trong một mảng array
Closest element to zero Tìm phần tử gần nhất với 0 và in khoảng cách array
Consecutive unique char Tìm độ dài max của mảng con liền kề với các ký tự duy nhất string
Highest frequency char Lấy tần suất của ký tự xuất hiện nhiều nhất string
Longest palindrome Tìm độ dài của chuỗi con palindrome dài nhất string
Count primes Tính các số nguyên tố trong một phạm vi Algo.
Rotate array Xoay một mảng sang phải k bước Algo.
Partition equal sets Kiểm tra xem một mảng có thể được chia thành hai tập hợp có tổng bằng nhau Algo.
Square root integer Tính phần nguyên của căn bậc hai math
Plus 1 Trả về các chữ số sau khi một số nguyên được cộng 1 math
Check square sum Kiểm tra xem một số nguyên có phải là tổng của hai số bình phương math
Compare standard dev. Xác định xem độ lệch chuẩn có nhỏ hơn 1 D.S.
Matrix size Tính tổng số hàng và cột D.S.
Diff mean and median Tính sự khác biệt giữa trung bình và trung vị cho một mảng D.S.

Bảng 8: Các vấn đề trong MTPB, hiển thị vấn đề 56 đến 115. D.S. và Algo. đề cập đến khoa học dữ liệu
và thuật toán.
18

--- TRANG 19 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
E TÍNH TOÁN PERPLEXITY CHO PROMPTS ĐƠN VÀ ĐA LƯỢT

Giả sử {p_i}_{i=1}^n là tập hợp các prompt cho một vấn đề cho trước, và {s_i}_{i=1}^n là n chương trình con được tổng hợp
bởi một mô hình P. Giả sử c_{i-1} = [p_1; s_1; ...; p_{i-1}; s_{i-1}] trong đó [;] chỉ ra nối,
xác suất có điều kiện của p_i là Prob_i = P(p_i|c_{i-1}), và sau đó perplexity cho các
prompt đa lượt được tính như

PPL_Multiturn = exp(-1/m ∑_{i=1}^n log Prob_i),                    (2)

trong đó m là tổng số token của tất cả các prompt {p_i}_{i=1}^n. Giả sử c = [p_1; s_1; ...; p_n; s_n], thì
xác suất của nó là Prob = P(c), và perplexity cho các prompt đơn lượt được tính như

PPL_Singleturn = exp(-1/m log Prob).                               (3)

F SO SÁNH PERPLEXITY CHO CODEGEN-NL VÀ CODEGEN-MULTI

CODEGEN-NL 350M 2.7B 6.1B
Pass 4.53 3.25 2.78
Non-Pass 4.96 3.87 3.65

Bảng 9: Perplexity prompt trung bình ± của các mô hình CODEGEN-NL trên các vấn đề pass và non-pass.

CODEGEN-MULTI 350M 2.7B 6.1B
Pass 4.78 3.82 3.82
Non-Pass 5.64 4.85 4.80

Bảng 10: Perplexity prompt trung bình ± của các mô hình CODEGEN-MULTI trên các vấn đề pass và non-pass.

G KẾT QUẢ BỘ TEST CHUẨN BỔ SUNG

Mô hình pass@1 pass@10 pass@100
CODEGEN-NL 350M 0.96 6.37 19.91
CODEGEN-NL 2.7B 5.34 24.63 48.95
CODEGEN-NL 6.1B 8.15 31.21 55.27
CODEGEN-NL 16.1B 10.92 38.43 62.76
CODEGEN-MULTI 350M 7.46 24.18 46.37
CODEGEN-MULTI 2.7B 18.06 45.80 65.34
CODEGEN-MULTI 6.1B 18.35 47.27 67.92
CODEGEN-MULTI 16.1B 20.94 51.61 70.02
CODEGEN-MONO 350M 14.59 41.49 63.00
CODEGEN-MONO 2.7B 27.31 59.19 74.24
CODEGEN-MONO 6.1B 32.48 64.20 76.81
CODEGEN-MONO 16.1B 35.28 67.32 80.09
INCODER 6B 21.30 46.50 66.20
code-cushman-001 45.90 66.90 79.90
code-davinci-001 51.80 72.80 84.10
code-davinci-002 58.10 76.70 84.50

Bảng 11: Tỷ lệ pass trên Mostly Basic Python Problems (MBPP).

Chúng tôi cũng đánh giá các mô hình của chúng tôi trên Mostly Basic Python Problems (MBPP) (Austin et al., 2021). Kết quả
được hiển thị trong Bảng 11. Tuân theo Chen et al. (2022), chúng tôi lấy mẫu các chương trình từ
19

--- TRANG 20 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
MBPP được làm sạch cho tất cả các mô hình của chúng tôi, với n = 100 và nhiệt độ = 0.8. Bốn hàng cuối
là từ bài báo được đề cập trước đó. Nói chung chúng tôi quan sát xu hướng nhất quán của việc cải thiện
hiệu suất qua các phiên bản khác nhau (NL, Multi, Mono), với CODEGEN-MONO 16.1B lớn nhất của chúng tôi
tiếp cận kết quả từ code-cushman-001. Trong khi chúng tôi không biết liệu có mô hình nào của OpenAI
là "Codex 12B" được báo cáo trong Chen et al. (2021), chúng tôi tin rằng mô hình của chúng tôi đạt được kết quả hợp lý
trên MBPP cũng vậy. Chúng tôi cũng lưu ý rằng CODEGEN-MONO 6.1B của chúng tôi vượt trội đáng kể so với
INCODER 6B.

H CÁC MẪU ĐƯỢC TẠO RA

H.1 CÁC TRƯỜNG HỢP MÀ CODEGEN-MONO 16.1B CÓ HIỆU SUẤT KÉMN

# Import libraries.
import numpy as np
# Khởi tạo một biến "stack" với một
danh sách rỗng, và "num" với 100 như một
chuỗi.
stack = []
num = "100"
# Đối với mỗi ký tự trong "num", nối ký tự
vào "stack".
for i in range(len(num)):
    stack.append(num[i])
# Gán một chuỗi rỗng cho một biến "
result", và nối các ký tự
được pop từ phần tử cuối của "stack"
vào "result" cho đến khi "stack" rỗng.
result = ""
while len(stack) != 0:
    result += stack.pop()
# Ép "result" thành integer và in ra.
print(int(result))

# Import libraries.
import numpy as np
# Khởi tạo một biến "stack" với một
danh sách rỗng, và "num" với 100 như một
chuỗi.
stack = []
num = 100
# Đối với mỗi ký tự trong "num", nối ký tự
vào "stack".
for i in num:
    stack.append(i)
# Gán một chuỗi rỗng cho một biến "
result", và nối các ký tự
được pop từ phần tử cuối của "stack"
vào "result" cho đến khi "stack" rỗng.
result = ""
while len(stack) > 0:
    result += stack.pop()
# Ép "result" thành integer và in ra.
print(int(result))

CODEGEN-MONO 2.7B CODEGEN-MONO 16.1B

Hình 3: Các mẫu được tạo ra cho vấn đề "Reverse digits". Trong khi đơn giản, chúng tôi thường quan sát
rằng CODEGEN-MONO 16.1B giả định kiểu khi gán một số cho một biến. Ở đây bên phải, mặc dù
được chỉ định rõ ràng để khởi tạo một số như một chuỗi, mô hình lớn hơn thất bại trong việc làm
như vậy. Điều này gây ra lỗi trong lượt tiếp theo khi code cố gắng lặp qua num.
20

--- TRANG 21 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

# Import libraries.
import numpy as np
# Gán ma trận "[[3, 2], [2, 3]]" cho một
biến tên "my_matrix".
my_matrix = [[3, 2], [2, 3]]
# Gán số "1" cho một biến tên
"t".
t = 1
# Tính số điều kiện của my_matrix
và lưu trữ như result.
result = np.linalg.cond(my_matrix)
# Kiểm tra xem result có nhỏ hơn t
. Nếu có, trả về "True", ngược lại
trả về "False".
if result < t:
    print("True")
else:
    print("False")

# Import libraries.
import numpy as np
# Gán ma trận "[[3, 2], [2, 3]]" cho một
biến tên "my_matrix".
my_matrix = [[3, 2], [2, 3]]
# Gán số "1" cho một biến tên
"t".
t = 1
# Tính số điều kiện của my_matrix
và lưu trữ như result.
condition_number = np.linalg.cond(my_matrix)
# Kiểm tra xem result có nhỏ hơn t
. Nếu có, trả về "True", ngược lại
trả về "False".
def check_condition_number(condition_number,
t):
    if condition_number < t:
        return True
    else:
        return False

CODEGEN-MONO 2.7B CODEGEN-MONO 16.1B

Hình 4: Các mẫu được tạo ra cho vấn đề "Matrix condition number". Cả hai mô hình tạo ra các chương trình
trong cùng phạm vi cho đến lượt cuối, nơi CODEGEN-MONO 16.1B tạo ra dưới dạng định nghĩa hàm
. Đáng chú ý, mô hình lớn hơn tạo ra từ khóa return xuất hiện trong prompt, nhưng nó
chỉ hợp lệ trong một hàm.
21

--- TRANG 22 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023
H.2 CÁC TRƯỜNG HỢP MÀ CODEGEN-MONO 16.1B VƯỢT TRỘI

# Import libraries.
import numpy as np
# Định nghĩa một biến "pos" với "B", "swap"
với ['AC', 'CA'].
pos = ['B','swap']
# Viết một hàm "move" nhận hai
chuỗi x và y như đầu vào, và thay thế
bất kỳ xuất hiện nào của x trong y bằng một chuỗi
rỗng, sau đó trả về y.
def move(x,y):
    return y.replace(x,'')
# Đối với mỗi phần tử trong "swap", nếu nó chứa
"pos", gọi "move" trên "pos" và phần tử
hiện tại và lưu trữ kết quả vào
"pos".
def swap(swap):
    for i in range(len(swap)):
        if swap[i] == 'pos':
            swap[i] = move(pos[0],pos[1])
    return swap
# In ra "pos".
print(pos)

# Import libraries.
import numpy as np
# Định nghĩa một biến "pos" với "B", "swap"
với ['AC', 'CA'].
pos = "B"
swap = ['AC', 'CA']
# Viết một hàm "move" nhận hai
chuỗi x và y như đầu vào, và thay thế
bất kỳ xuất hiện nào của x trong y bằng một chuỗi
rỗng, sau đó trả về y.
def move(x, y):
    return y.replace(x, "")
# Đối với mỗi phần tử trong "swap", nếu nó chứa
"pos", gọi "move" trên "pos" và phần tử
hiện tại và lưu trữ kết quả vào
"pos".
for i in swap:
    if pos in i:
        pos = move(pos, i)
# In ra "pos".
print(pos)

CODEGEN-MONO 2.7B CODEGEN-MONO 16.1B

Hình 5: Các mẫu được tạo ra cho vấn đề "Cup swap". Ở đây, CODEGEN-MONO 2.7B nhầm lẫn
xử lý "swap" như một phần của giá trị được gán, trong khi CODEGEN-MONO 16.1B phân tích ý định
chính xác và gán hai biến song song.
22

--- TRANG 23 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

# Import libraries.
import numpy as np
# Khởi tạo biến tên lst1 với một
danh sách ['abc', 'abc10'].
lst1 = ['abc', 'abc10']
# Tạo một hàm gọi là num_in_str() để
kiểm tra xem một chuỗi có chứa một số
.
def num_in_str(string):
    for i in string:
        if i.isnumeric():
            return True
        else:
            return False
# Gọi hàm num_in_str() để tìm
các chuỗi trong lst1 có số và
gán chúng vào một danh sách tên lst2
lst2 = []
for i in lst1:
    if num_in_str(i):
        lst2.append(i)
# In ra lst2
print(lst2)

# Import libraries.
import numpy as np
# Khởi tạo biến tên lst1 với một
danh sách ['abc', 'abc10'].
lst1 = ['abc', 'abc10']
# Tạo một hàm gọi là num_in_str() để
kiểm tra xem một chuỗi có chứa một số
.
def num_in_str(s):
    return any(i.isdigit() for i in s)
# Gọi hàm num_in_str() để tìm
các chuỗi trong lst1 có số và
gán chúng vào một danh sách tên lst2
lst2 = [i for i in lst1 if num_in_str(i)]
# In ra lst2
print(lst2)

CODEGEN-MONO 2.7B CODEGEN-MONO 16.1B

Hình 6: Các mẫu được tạo ra cho vấn đề "Detect digits". CODEGEN-MONO 2.7B thất bại trong việc triển khai
num_in_str, dẫn đến việc kiểm tra chỉ ký tự đầu tiên. Ngược lại, CODEGEN-MONO 16.1B
thành công sử dụng hàm any để quét tất cả các ký tự trong chuỗi cho trước.

I PHÂN TÍCH BỔ SUNG TRÊN MTPB

Chúng tôi đã tiến hành các phân tích bổ sung để minh họa mối quan hệ giữa độ dài chương trình được tạo ra và tỷ lệ pass
và hiển thị kết quả trong Hình 7, Hình 8, và Hình 9. Mối quan hệ giữa độ dài chương trình được tạo ra
và độ dài prompt được hiển thị trong Hình 10.
23

--- TRANG 24 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

Hình 7: Độ dài Tối đa của Completion so với Tỷ lệ Pass.

Hình 8: Độ dài Tối đa của Completion so với Tỷ lệ Pass.

Hình 9: Độ dài Tối đa của Completion so với Tỷ lệ Pass.
24

--- TRANG 25 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2023

150
200
250
Mối quan hệ số lượng Token giữa
prompts và chương trình được tạo ra từ CodeGen-16B-Mono
y=x
Turn
0 20 40 60 80 100
Số token trong prompt
0
20
40
60
80
100
120
140
Số token trong chương trình được tạo ra

Hình 10: Độ dài Prompt so với Độ dài Chương trình Được tạo ra.
25
