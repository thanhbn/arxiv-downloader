Dự đoán Mức độ Nghiêm trọng của Lỗi ở Cấp độ Phương thức sử dụng Các Chỉ số Mã nguồn và LLMs

Tóm tắt—Trong vài thập kỷ qua, những nỗ lực nghiên cứu đáng kể đã được dành cho việc dự đoán lỗi phần mềm. Tuy nhiên, hầu hết các công trình hiện có trong lĩnh vực này đều coi tất cả các lỗi như nhau, điều này không đúng trong thực tế. Điều quan trọng đối với một phương pháp dự đoán khiếm khuyết là ước tính mức độ nghiêm trọng của các lỗi được xác định để những lỗi có mức độ nghiêm trọng cao hơn được chú ý ngay lập tức. Trong nghiên cứu này, chúng tôi điều tra các chỉ số mã nguồn, biểu diễn mã nguồn sử dụng các mô hình ngôn ngữ lớn (LLMs), và sự kết hợp của chúng trong việc dự đoán nhãn mức độ nghiêm trọng lỗi của hai bộ dữ liệu nổi bật. Chúng tôi tận dụng một số chỉ số nguồn ở mức độ chi tiết phương thức để huấn luyện tám mô hình học máy khác nhau. Kết quả của chúng tôi cho thấy các mô hình Decision Tree và Random Forest vượt trội hơn các mô hình khác về một số chỉ số đánh giá của chúng tôi. Sau đó chúng tôi sử dụng LLM CodeBERT đã được huấn luyện trước để nghiên cứu hiệu quả của các biểu diễn mã nguồn trong việc dự đoán mức độ nghiêm trọng lỗi. Việc tinh chỉnh CodeBERT cải thiện đáng kể kết quả dự đoán mức độ nghiêm trọng lỗi trong khoảng 29%-140% cho một số chỉ số đánh giá, so với mô hình dự đoán cổ điển tốt nhất trên chỉ số mã nguồn. Cuối cùng, chúng tôi tích hợp các chỉ số mã nguồn vào CodeBERT như một đầu vào bổ sung, sử dụng hai kiến trúc được đề xuất của chúng tôi, cả hai đều nâng cao hiệu quả của mô hình CodeBERT.

Bảo trì phần mềm bao gồm xử lý lỗi là một trong những phần thách thức nhất của chu kỳ phát triển phần mềm. Các bước khác nhau trong xử lý lỗi như phát hiện lỗi, định vị lỗi và sửa lỗi đòi hỏi các tài nguyên đáng kể (ví dụ: đội ngũ kỹ thuật, thời gian, v.v.), và hầu hết các bước này đang được thực hiện thủ công hoặc bán tự động trong thực tế. Do đó, cả các nhà nghiên cứu và các nhà thực hành đều đang cố gắng tự động hóa những nhiệm vụ tẻ nhạt này từ các góc độ ứng dụng khác nhau như dự đoán khiếm khuyết, định vị lỗi, tạo thử nghiệm và sửa chữa chương trình.

Mặc dù đã có nhiều nghiên cứu về xử lý lỗi sử dụng các kỹ thuật khác nhau như dựa trên tìm kiếm, dựa trên mẫu và các kỹ thuật dựa trên ML, hầu hết chúng đều ngầm giả định rằng tất cả các lỗi có cùng tầm quan trọng, điều này không đúng trong thực tế. Mức độ nghiêm trọng của lỗi là một trong những đặc điểm của lỗi được gán thủ công bởi QA/nhà phát triển trong các giai đoạn báo cáo lỗi hoặc phân loại. Mức độ nghiêm trọng của lỗi chỉ ra cường độ tác động của lỗi đến hoạt động của hệ thống, vì vậy nó hướng dẫn các đội kỹ thuật điều chỉnh ưu tiên của họ về việc các lỗi nên được sửa nhanh như thế nào.

Ước tính mức độ nghiêm trọng của một lỗi nhất định, một cách tự động và có hệ thống (không thiên vị theo ý kiến của người báo cáo lỗi), có thể khá hữu ích, không chỉ cho các đội kỹ thuật (bao gồm nhà phát triển và đội QA) mà còn như một đầu vào quan trọng cho các công cụ kỹ thuật phần mềm tự động khác, chẳng hạn như sửa chữa chương trình, để ưu tiên quy trình của họ, cho phù hợp.

Động lực: Theo hiểu biết tốt nhất của chúng tôi, tài liệu hiện tại trong lĩnh vực dự đoán mức độ nghiêm trọng lỗi chủ yếu bị giới hạn trong các nghiên cứu dựa trên mô tả báo cáo lỗi sử dụng các kỹ thuật NLP (Xử lý Ngôn ngữ Tự nhiên) khác nhau. Những kỹ thuật này có một số hạn chế, như sau: (a) cho rằng các mức độ nghiêm trọng được dự đoán chỉ được ước tính dựa trên các mức độ nghiêm trọng được báo cáo cho một lỗi mới, chúng có thể thiên vị theo ý kiến chủ quan của người báo cáo (không phải lúc nào cũng là chuyên gia); (b) Chúng bỏ qua bối cảnh (mã nguồn cơ bản). Nghĩa là, một báo cáo lỗi tương tự cho mã nguồn khác nhau có thể có mức độ nghiêm trọng khác nhau; (c) Những cách tiếp cận này chỉ giới hạn ở các dự án có báo cáo lỗi phong phú và có thông tin mà không áp dụng được trong nhiều trường hợp; và (d) Không phải tất cả các lỗi được phát hiện đều có báo cáo lỗi được viết cho chúng, ngay cả trong các dự án có lịch sử lỗi phong phú. Nhiều lần, một lỗi mới có thể được phát hiện bởi một trường hợp kiểm tra và không có báo cáo được gán cho nó để được sử dụng cho dự đoán mức độ nghiêm trọng. Báo cáo sẽ được thêm vào cuối cùng sau khi lỗi được sửa hoặc được gán để sửa nhưng mức độ nghiêm trọng cần thiết trước đó.

Những thách thức trên thúc đẩy chúng tôi đề xuất một kỹ thuật để dự đoán mức độ nghiêm trọng lỗi mà không yêu cầu báo cáo lỗi. Điều này thực tế là một vấn đề khó giải quyết hơn so với các lựa chọn thay thế dựa trên báo cáo lỗi, vì (a) các công cụ điển hình trong tay để dự đoán (như transformers và LLMs) phù hợp hơn khi làm việc với ngôn ngữ tự nhiên so với mã nguồn và (b) khi một người kiểm tra/nhà phát triển/hoặc thậm chí người dùng viết báo cáo lỗi, họ đã cung cấp rất nhiều thông tin bổ sung về lỗi bao gồm thông tin có thể trực tiếp hoặc gián tiếp ngụ ý mức độ nghiêm trọng. Do đó, một cách tiếp cận không giả định sự tồn tại của báo cáo lỗi đang giải quyết một vấn đề khó khăn hơn.

Cũng có một số công trình hạn chế về việc sử dụng các chỉ số mã nguồn để dự đoán mức độ nghiêm trọng lỗi mà chỉ dựa trên các chỉ số mã nguồn cấp lớp. Tuy nhiên, các nhà nghiên cứu/nhà phát triển thấy mức độ chi tiết cấp lớp/mô-đun quá thô cho việc sử dụng thực tế (kết quả cấp lớp, ví dụ: lớp nào có lỗi rủi ro hơn, thường đã được biết trước cho các nhà phát triển). Ngoài ra, các kỹ thuật ước tính của những kỹ thuật này không phải là nghệ thuật tiên tiến ngày nay (kỹ thuật ít nhất từ 10 năm trước).

Để khắc phục những hạn chế đã đề cập và tiến bộ nghệ thuật tiên tiến, trong bài báo này, chúng tôi cung cấp một cách tiếp cận tự động tận dụng tiềm năng của 1) các chỉ số mã nguồn cấp phương thức, 2) biểu diễn mã nguồn (nhúng mã) được cung cấp bởi các mô hình ngôn ngữ lớn đã được huấn luyện trước (LLMs) cho mã, và 3) tích hợp biểu diễn mã nguồn và các chỉ số mã nguồn để dự đoán các giá trị mức độ nghiêm trọng lỗi. Gói sao chép của chúng tôi được cung cấp trong kho lưu trữ trực tuyến.

Chúng tôi tận dụng học biểu diễn mã nguồn (còn được gọi là học đặc trưng) vì chúng đã cho thấy cải thiện đáng kể so với các mô hình cổ điển bằng cách tự động khám phá các đặc trưng/biểu diễn của các mẫu để thực hiện các nhiệm vụ xuôi dòng được yêu cầu. Trong các nghiên cứu gần đây, LLMs cho mã như CodeBERT (một mô hình ngôn ngữ lớn dựa trên transformers) đã cho thấy kết quả đầy hứa hẹn trên một số nhiệm vụ liên quan đến phần mềm bao gồm phát hiện lỗi, sửa chữa chương trình và phát hiện bản sao.

Đóng góp và Tính mới: Trong nghiên cứu này, chúng tôi tận dụng 19 dự án mã nguồn mở chứa 3.342 phương thức có lỗi. Chúng tôi đã huấn luyện tám mô hình học máy cổ điển (KNN, SVM, Naive Bayes, Decision Tree, Random Forest, Ada Boost, XGBoost, và MLP) trên các chỉ số mã nguồn được trích xuất và đánh giá kết quả trên một số chỉ số hiệu suất. Kết quả cho thấy Random Forest có hiệu suất tốt nhất. Kết quả thử nghiệm của chúng tôi trên mô hình CodeBERT, sử dụng biểu diễn mã nguồn, cho thấy cải thiện đáng kể (29%-140% trên các chỉ số khác nhau) so với các phương pháp trước đó. Cuối cùng, chúng tôi đề xuất tích hợp các chỉ số mã nguồn và biểu diễn mã nguồn bằng cách cung cấp hai kiến trúc, trong đó các chỉ số mã nguồn được cung cấp cho mô hình như (a) một đầu vào bổ sung dạng văn bản, và (b) một vector số. Cách tiếp cận này cho thấy sự cải thiện (2%-10% trên các chỉ số khác nhau) so với hiệu quả của mô hình CodeBERT.

Tính mới chính của công trình này là về việc sử dụng LLM để dự đoán mức độ nghiêm trọng lỗi mà không cần báo cáo lỗi, đi kèm với các mẫu được đề xuất của chúng tôi về cách cung cấp mã nguồn và các chỉ số cấp phương thức cùng nhau cho LLM để có được kết quả tốt nhất.

Tóm lại, những đóng góp chính của nghiên cứu này bao gồm:
• Điều tra tám mô hình học máy cổ điển được huấn luyện trên 10 chỉ số mã nguồn cấp phương thức để dự đoán mức độ nghiêm trọng lỗi của hai bộ dữ liệu phổ biến, làm đường cơ sở.
• Khám phá một LLM cho mã (CodeBERT) trong việc dự đoán nhãn mức độ nghiêm trọng lỗi chỉ sử dụng mã nguồn phương thức có lỗi.
• Đề xuất một cách tiếp cận mới (hai kiến trúc khác nhau) để tích hợp các chỉ số mã nguồn với biểu diễn mã nguồn được cung cấp bởi CodeBERT.

Các chỉ số mã nguồn đã được sử dụng cho các ứng dụng liên quan đến phần mềm khác nhau như phát hiện mùi mã, ước tính nỗ lực bảo trì và phát hiện khiếm khuyết. Các mức độ chi tiết khác nhau như cấp gói/lớp, cấp phương thức, và cấp dòng đã được áp dụng trong nghiên cứu trước đây. Các mức độ chi tiết cao (gói/lớp) thực tế ít hữu ích hơn cho các nhà phát triển vì nó đòi hỏi nỗ lực đáng kể để định vị lỗi ở các thành phần gói/lớp. Ngoài ra, mức độ chi tiết cấp dòng có thể bị ảnh hưởng bởi quá nhiều dương tính giả, vì nhiều dòng có thể tương tự chỉ do tình cờ. Do đó, mức độ chi tiết cấp phương thức đã trở thành tiêu điểm mới cho cộng đồng, đặc biệt là cho các mô hình dự đoán lỗi, và một số nghiên cứu cho thấy kết quả tích cực và khuyến khích.

Do vấn đề đã đề cập, trong nghiên cứu này, chúng tôi sử dụng mức độ chi tiết cấp phương thức được mô tả trong phần này. Những chỉ số mã này có các ưu điểm khác nhau (ví dụ: nhanh để tính toán) và nhược điểm (ví dụ: yêu cầu trình phân tích cú pháp dành riêng cho ngôn ngữ). Chúng tôi đảm bảo có hầu hết các chỉ số đã được chứng minh hiệu quả trong việc dự đoán tính có lỗi cấp phương thức, trong các nghiên cứu trước đây. Những chỉ số này được định nghĩa như sau:

Số dòng mã (LC) còn được gọi là Kích thước là chỉ số mã phổ biến nhất, dễ đo lường nhất và hiệu quả nhất để ước tính bảo trì phần mềm. Trong nghiên cứu này, chúng tôi tính LC như các dòng mã nguồn không có chú thích và dòng trống, tương tự như để ngăn chặn các hiệu ứng định dạng mã và chú thích, nằm ngoài phạm vi của nghiên cứu này.

McCabe (MA) còn được gọi là độ phức tạp cyclomatic, là một chỉ số rất phổ biến khác chỉ ra số lượng đường dẫn độc lập, và do đó là độ phức tạp logic của một chương trình. Một cách trực quan, các thành phần có giá trị McCabe cao dễ có lỗi hơn.

McClure (ML) được đề xuất như một cải thiện so với McCabe. Không giống như McCabe, McClure xem xét số lượng biến điều khiển và số lượng so sánh trong một vị từ, điều này không được hỗ trợ bởi McCabe.

McCabe và McClure không xem xét độ sâu lồng nhau. Độ sâu Khối Lồng nhau (NBD) đã được nghiên cứu cùng với chúng để giảm thiểu vấn đề này.

Vì các thước đo độ phức tạp giống McCabe yêu cầu một trình phân tích cú pháp dành riêng cho ngôn ngữ (để tìm các vị từ), Hindle et al. đề xuất Thụt lề Proxy (PI) như một proxy cho các chỉ số độ phức tạp giống McCabe.

FanOut (FO) tính tổng số phương thức được gọi bởi một phương thức nhất định. Điều này cung cấp một ước tính về sự ghép nối, tức là sự phụ thuộc của một phương thức cụ thể vào các phương thức khác.

Khả năng đọc (R) kết hợp các đặc trưng mã khác nhau để tính toán một giá trị duy nhất để ước tính khả năng đọc mã. Chúng tôi sử dụng chỉ số khả năng đọc được đề xuất bởi Buse et al. tạo ra điểm khả năng đọc cho một phương thức nhất định. Điểm khả năng đọc dao động từ 0 đến 1 để chỉ định mã ít có thể đọc nhất đến mã có thể đọc nhất, tương ứng.

Các Chỉ số Halstead chứa bảy thước đo dựa trên số lượng toán tử và toán hạng trong một thành phần. Vì tất cả các chỉ số Halstead đều có tương quan cao với nhau, trong nghiên cứu này, chúng tôi chỉ xem xét hai trong số chúng: Độ khó (D) và Nỗ lực (E) sử dụng các chỉ số Halstead khác trong công thức của chúng, như sau:

E=D∗V D =n1/2∗N2/n2
N=N1 +N2 n=n1 +n2
V(HalsteadVolume) =N∗log2(n)

Trong đó n1 và n2 là số lượng toán tử và toán hạng riêng biệt, và N1 và N2 là tổng số toán tử và toán hạng, tương ứng.

Chỉ số Khả năng Bảo trì (MI) đã được giới thiệu bởi Omran và Hagemeister trong đó các tác giả xác định các chỉ số để đo lường khả năng bảo trì của một hệ thống phần mềm và kết hợp những chỉ số đó thành một giá trị duy nhất. MI có thể được tính như:

MI= 171 −5.2×ln(HalsteadVolume)−0.23×(McCabe)−16.2×ln(LC)

Trong đó HalsteadVolume và LC đã được định nghĩa trước đó trong phần này.

CodeBERT là một mô hình ngôn ngữ lớn (LLM) đã được huấn luyện trước hỗ trợ cả ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL). Nó theo kiến trúc của các mô hình BERT và RoBERTa và sử dụng một transformer đa tầng hai chiều. CodeBERT có cùng kiến trúc mô hình như RoBERTa-base với 125M tham số. Nó đã được huấn luyện trên 6.4M cặp NL-PL của 6 ngôn ngữ lập trình như Python, Java, JavaScript, PHP, Ruby, và Go từ bộ dữ liệu CodeSearchNet. Mô hình này đã được huấn luyện liên quan đến hai mục tiêu, mô hình hóa ngôn ngữ che mặt (MLM) và phát hiện token thay thế (RTD). Định dạng đầu vào theo sự nối của hai đoạn (phần đầu chứa ngôn ngữ tự nhiên, và phần thứ hai chứa mã) với một token phân tách: [CLS], w1, w2, ..., wn, [SEP], c1, c2, ..., cm, [EOS]. Đầu ra bao gồm biểu diễn vector ngữ cảnh của cả đoạn NL và PL, và biểu diễn [CLS] có thể được sử dụng như biểu diễn chuỗi tổng hợp cho các nhiệm vụ khác nhau như xếp hạng hoặc phân loại.

CodeBERT đã được sử dụng cho các nhiệm vụ kỹ thuật phần mềm khác nhau như nơi các tác giả sử dụng mô hình CodeBERT để tạo ra các bản sửa cho các lỗi Java đơn giản với độ chính xác trong khoảng 19-72%. Pan et al. tận dụng CodeBERT để phát hiện lỗi trong các cài đặt cross-version và cross-project và họ cũng điều tra các hiệu ứng của các mẫu dự đoán khác nhau. Zhang et al. sử dụng CodeBERT để nâng cao hiệu suất của nhiệm vụ tạo tiêu đề câu hỏi bằng cách sử dụng thông tin bi-modal tồn tại trong nội dung câu hỏi. Họ đề xuất rằng phương pháp này vượt trội hơn các mô hình trước đây dưới các cài đặt khác nhau trong khoảng 4%-14%.

Chúng tôi sử dụng Defects4J và Bugs.jar bộ dữ liệu chứa các lỗi thực từ các dự án Java mã nguồn mở phổ biến khác nhau. Defects4J được chọn do tính đa dạng của nó bằng cách có các dự án trong các lĩnh vực khác nhau và cũng do tính phổ biến của nó trong các lĩnh vực nghiên cứu kỹ thuật phần mềm tự động khác nhau như Định vị Lỗi, Sửa chữa Chương trình và Tạo Thử nghiệm. Bugs.jar là một bộ dữ liệu phổ biến khác chứa các dự án mã nguồn mở lớn đã được sử dụng rộng rãi trong Sửa chữa Chương trình.

Phiên bản mới nhất của Defects4J (2.0.0) chứa 835 lỗi từ 17 dự án Java mã nguồn mở. Sau khi trích xuất các lỗi có nhãn mức độ nghiêm trọng, chúng tôi kết luận với 510 lỗi từ các dự án: Chart, Cli, Closure, Codec, Collections, Compress, Csv, JxPath, Lang, Math, và Time. Bộ dữ liệu Bugs.jar chứa 1.158 lỗi từ tám dự án Java mã nguồn mở lớn và phổ biến: Accumulo, Camel, Commons-math, Flink, Jackrabbit-oak, Logging-log4j2, Maven, và Wicket.

Nghiên cứu của chúng tôi yêu cầu các phương thức có lỗi cùng với nhãn mức độ nghiêm trọng của chúng. Do đó, chúng tôi thu thập các nhãn mức độ nghiêm trọng của mỗi lỗi từ các hệ thống quản lý vấn đề tương ứng. Bộ dữ liệu defects4J chứa các lỗi từ Jira, Google Code Archive, và Source Forge, nhưng bộ dữ liệu Bugs.jar chỉ chứa các lỗi từ Jira. Để trích xuất các nhãn mức độ nghiêm trọng, chúng tôi trích xuất id vấn đề của một lỗi nhất định (mỗi lỗi trong cả hai bộ dữ liệu có một id vấn đề duy nhất), và sau đó lấy các nhãn mức độ nghiêm trọng của nó từ các hệ thống quản lý vấn đề tương ứng. Để trích xuất các nhãn mức độ nghiêm trọng từ Jira, chúng tôi sử dụng "Thư viện Python Jira", đối với Source Forge và Google Code Archive, chúng tôi phân tích trang web chứa vấn đề và trích xuất các nhãn mức độ nghiêm trọng. Mã liên quan đến thu thập bộ dữ liệu có sẵn trong gói sao chép được cung cấp.

Chúng tôi thống nhất tất cả các nhãn mức độ nghiêm trọng phân loại và số thành định dạng số từ 0 đến 3 (0 quan trọng nhất, 3 ít quan trọng nhất) để sử dụng chúng như nhãn mục tiêu trong mục đích huấn luyện của chúng tôi theo Bảng I. Chúng tôi cũng hợp nhất hai bộ dữ liệu và sau đó xáo trộn các mẫu để có số lượng mẫu tương đối lớn hơn (tốt hơn cho việc huấn luyện mô hình của chúng tôi) và cũng đạt được một mô hình có khả năng xử lý các lỗi đa dạng từ các dự án khác nhau và các lĩnh vực khác nhau thay vì có một mô hình được huấn luyện cụ thể cho mỗi bộ dữ liệu/dự án.

Mỗi dự án có lỗi trong các bộ dữ liệu đã đề cập thể hiện một lỗi được sửa trong phiên bản sửa tương ứng của nó. Vì tiêu điểm nghiên cứu của chúng tôi là mức độ chi tiết cấp phương thức, chúng tôi xem xét tất cả các phương thức bị ảnh hưởng trong quá trình vá sửa lỗi như phương thức có lỗi, tương tự như các nghiên cứu trước đây (ví dụ:). Cuối cùng, sau khi hợp nhất hai bộ dữ liệu này và loại bỏ các thể hiện trùng lặp (cùng các phương thức có lỗi xuất hiện trong một số dự án/bộ dữ liệu), chúng tôi đạt được 3.342 phương thức có lỗi. Đối với quá trình huấn luyện, chúng tôi chia bộ dữ liệu của mình một cách ngẫu nhiên thành ba phần train, validation, và test khác nhau sử dụng tỷ lệ 70%, 15%, và 15% tương ứng. Lưu ý rằng trong trường hợp của chúng tôi, việc chia ngẫu nhiên không được coi là rò rỉ dữ liệu, vì các mức độ nghiêm trọng lỗi không tương quan theo thời gian (biết một nhãn mức độ nghiêm trọng trong tương lai sẽ không đưa ra gợi ý bổ sung để dự đoán một nhãn mức độ nghiêm trọng hiện tại so với biết một nhãn mức độ nghiêm trọng trong quá khứ). Tuy nhiên, trong thực tế, người ta chỉ có thể sử dụng dữ liệu quá khứ để huấn luyện (chúng tôi không thể triển khai các thử nghiệm theo cách này vì không phải tất cả các mục dữ liệu của chúng tôi đều có dấu thời gian). Thống kê bộ dữ liệu của chúng tôi bao gồm số lượng thể hiện trong mỗi dự án, các phần train, validation và test liên quan đến các nhãn mức độ nghiêm trọng được cung cấp trong Bảng II.

Chúng tôi thấy rằng nhiều phương thức có lỗi trong bộ dữ liệu chứa các chú thích mã. Vì việc nghiên cứu tác động của chú thích mã đối với chủ đề nghiên cứu của chúng tôi nằm ngoài phạm vi của nghiên cứu này, chúng tôi loại bỏ bất kỳ chú thích nào (như JavaDoc hoặc định dạng nội tuyến) tồn tại trong tất cả các mẫu của chúng tôi.

Hơn nữa, sau khi tính toán tất cả các chỉ số mã nguồn, để loại bỏ vấn đề chia tỷ lệ tiềm ẩn của dữ liệu số, chúng tôi sử dụng RobustScaler. Chúng tôi đã thực hiện bước tiền xử lý này vì các giá trị chỉ số mã nguồn được tính toán của chúng tôi không có cùng phạm vi, vì vậy chúng tôi chuẩn hóa chúng bằng cách sử dụng thuật toán RobustScaler, thuật toán này chuẩn hóa dữ liệu đầu vào và cũng mạnh mẽ chống lại các ngoại lệ. Thuật toán này tuân theo một thuật toán tương tự như quy mô MinMax, nhưng nó sử dụng phạm vi interquartile thay vì min-max.

Vì bộ dữ liệu của chúng tôi không cân bằng (số lượng lỗi trong mỗi danh mục mức độ nghiêm trọng không bằng nhau), giống như nhiều bộ dữ liệu thực, chúng tôi không thể chỉ dựa vào chỉ số độ chính xác. Do đó, chúng tôi đánh giá thử nghiệm của mình bằng cách sử dụng một số chỉ số mạnh mẽ chống lại bộ dữ liệu không cân bằng như sau:

Các chỉ số đầu tiên để báo cáo là Precision và Recall. Vì chúng tôi có các bộ phân loại đa lớp (các lớp mục tiêu của chúng tôi là 0,1,2,3 được cung cấp trong Bảng I vì vậy các chỉ số đánh giá được cung cấp như precision, recall, và ROC-AUC được tính toán bằng cách sử dụng phiên bản "weighted" thay vì "binary" mặc định. Cách tiếp cận này tính toán các chỉ số cho mỗi nhãn và sau đó tìm trung bình của chúng được cân nhặc bởi support (số lượng thể hiện đúng cho mỗi nhãn) có tính đến sự mất cân bằng nhãn.

F1-Weighted: F-measure hoặc F-score là một chỉ số đánh giá phổ biến trong các nhiệm vụ phân loại xem xét cả precision và recall. F1 là dạng phổ biến nhất của F-measure, lấy trung bình điều hòa của precision và recall. Tuy nhiên, chúng tôi không thể sử dụng điểm F1 trực tiếp cho các vấn đề đa lớp, và vì bộ dữ liệu của chúng tôi cũng không cân bằng, chỉ số F1-weighted được sử dụng. Nó được tính toán bằng cách lấy trung bình của tất cả điểm F1 mỗi lớp và xem xét số lượng mẫu của mỗi lớp.

Biểu đồ Đường cong ROC: Đặc tính Hoạt động của Người nhận (ROC) là một đường cong vẽ Tỷ lệ Dương tính Đúng (TPR) so với Tỷ lệ Dương tính Sai (FPR) ở các giá trị ngưỡng khác nhau. FPR được định nghĩa là FP chia cho FP+TN.

Sử dụng biểu đồ này, một mô hình hoàn hảo được biểu diễn bởi một đường từ góc dưới bên trái đến góc trên bên trái và sau đó qua góc trên bên phải của biểu đồ. Một mô hình có kỹ năng được biểu diễn bởi một đường cong cong lên phía góc trên bên trái của biểu đồ. Tuy nhiên, một bộ phân loại ngẫu nhiên (không có kỹ năng) được biểu diễn bởi một đường chéo từ góc dưới bên trái đến góc trên bên phải của biểu đồ.

AUC-ROC: Diện tích Dưới Đường cong (AUC) là thước đo khả năng của một bộ phân loại để phân biệt giữa các lớp mục tiêu. Bộ phân loại hoàn hảo có điểm AUC là 1 có nghĩa là nó phân biệt tốt giữa các lớp dương và âm trong khi bộ phân loại có điểm AUC là 0.5 có nghĩa là nó hoạt động như một bộ phân loại ngẫu nhiên. Điểm AUC là 0 có nghĩa là các dự đoán của bộ phân loại sai 100%.

Biểu đồ Đường cong Precision-Recall: Đường cong precision-Recall cho thấy sự đánh đổi giữa precision và recalls cho các ngưỡng khác nhau. Điểm ở góc trên bên phải cho thấy một bộ phân loại hoàn hảo trong khi một bộ phân loại không có kỹ năng (ngẫu nhiên) sẽ là một đường ngang trên biểu đồ với precision tỷ lệ thuận với số lượng ví dụ dương.

MCC: Hệ số Tương quan Matthews (MCC) đánh giá hiệu suất của một bộ phân loại bằng cách tận dụng tất cả các giá trị có sẵn trong ma trận nhầm lẫn làm cho nó hữu ích cho các bộ dữ liệu không cân bằng. Nó được tính toán như dưới đây:

TP∗TN−FP∗FN / √((TP+FP)∗(TP+FN)∗(TN+FP)∗(TN+FN))

Giá trị MCC nằm giữa -1 và +1 trong đó +1 đại diện cho một dự đoán hoàn hảo, 0 đại diện cho một dự đoán ngẫu nhiên (không có kỹ năng), và -1 đại diện cho một dự đoán nghịch đảo.

Lưu ý rằng tất cả các chỉ số đánh giá bao gồm các phiên bản weighted đều được triển khai bằng thư viện chỉ số chuẩn của Scikit-learn, nơi công thức chính xác cho mỗi chỉ số cũng có thể được tìm thấy trong tài liệu của nó.

Mục tiêu: Trong RQ này, chúng tôi nhằm mục đích nghiên cứu các chỉ số mã nguồn được chọn của chúng tôi, đã cho thấy kết quả đáng kể trong các nhiệm vụ phát hiện khiếm khuyết trong các nghiên cứu trước đây, và khả năng của chúng trong việc dự đoán mức độ nghiêm trọng lỗi sử dụng một số mô hình học máy. Theo hiểu biết tốt nhất của chúng tôi, không có nghiên cứu trước đây nào về việc dự đoán nhãn mức độ nghiêm trọng lỗi trên các bộ dữ liệu Defects4J và Bugs.jar (bộ dữ liệu tương đối nhỏ của các dự án thực tế mã nguồn mở) sử dụng các chỉ số mã nguồn cấp phương thức. Động lực của chúng tôi đằng sau RQ này là đánh giá khả năng của các mô hình học máy trên tập hợp các dự án đa dạng sử dụng mức độ chi tiết cấp phương thức (vì nó hiệu quả hơn cho các nhà thực hành để xử lý lỗi ở cấp độ này so với mức độ chi tiết cấp lớp/gói và dự đoán khiếm khuyết cấp dòng chưa đủ trưởng thành).

Thiết kế: Chúng tôi huấn luyện tám mô hình học máy như KNN, SVM, naive Bayes, Decision Tree, RandomForest, XgBoost, AdaBoost, và MLP bằng cách sử dụng các chỉ số mã nguồn của chúng tôi được mô tả trong phần II-A. Bằng cách chọn tám mô hình này, chúng tôi bao phủ một tập hợp đa dạng các thuật toán phân loại đã cho thấy kết quả đầy hứa hẹn trong nghiên cứu/thực hành cho các nhiệm vụ phân loại khác nhau. Vì việc điều chỉnh siêu tham số nằm ngoài phạm vi của nghiên cứu này, chúng tôi giữ các siêu tham số mặc định (có sẵn trong triển khai thư viện Scikit-learn) cho tất cả các mô hình này.

Lưu ý rằng chúng tôi coi RQ1 như đường cơ sở so sánh của chúng tôi cho hai RQ tiếp theo. Mặc dù áp dụng các mô hình hoàn toàn giống nhau trên các bộ dữ liệu hoàn toàn giống nhau, không được thực hiện trước đây và là đóng góp của chúng tôi, nhưng những phương pháp cổ điển như vậy đã được sử dụng rộng rãi trong dự đoán khiếm khuyết và trong các trường hợp hạn chế về dự đoán mức độ nghiêm trọng lỗi ở cấp lớp, trong quá khứ. Do đó, việc coi chúng như đường cơ sở so sánh của chúng tôi là công bằng.

Kết quả: Bảng III cho thấy kết quả của các mô hình được nghiên cứu của chúng tôi liên quan đến các chỉ số đánh giá khác nhau (do không gian hạn chế trong bài báo này, chúng tôi cung cấp ma trận nhầm lẫn, ROC, và các đường cong Precision-Recall của tám mô hình này trong kho lưu trữ mã được cung cấp của chúng tôi). Trong nghiên cứu này, chúng tôi chỉ báo cáo F1 cho mỗi lớp (F1 Perclass) cho thấy hiệu quả của mô hình cho mỗi lớp mục tiêu (các nhóm mức độ nghiêm trọng khác nhau) riêng biệt, nhưng cung cấp các phiên bản "weighted" của Precision, Recall, AUC, và MCC vì cách tiếp cận "weighted" xem xét tất cả các lớp mục tiêu (các nhóm mức độ nghiêm trọng khác nhau) và cung cấp ước tính tốt hơn về hiệu quả của mô hình trong việc dự đoán các nhóm mức độ nghiêm trọng khác nhau. Vì tất cả các mô hình có giá trị AUC >0.5, chúng tôi có thể nói rằng chúng hoạt động tốt hơn một bộ phân loại ngẫu nhiên. Tuy nhiên, như được hiển thị trong bảng (được đánh dấu bằng kiểu in đậm), mô hình Random Forest có hiệu quả tốt nhất trong việc dự đoán nhãn mức độ nghiêm trọng lỗi bằng cách có kết quả tốt hơn sử dụng tất cả các chỉ số đánh giá được cung cấp. Ngược lại, mô hình SVM có một trong những hiệu suất yếu nhất trong việc dự đoán các giá trị mức độ nghiêm trọng, dựa trên bất kỳ chỉ số nào. Theo giá trị MCC của nó, mô hình này hoạt động như bộ phân loại không có kỹ năng (ngẫu nhiên) (ít nhất với các tham số mặc định của thư viện Scikit-learn). Chỉ số F1 per-class cho thấy mô hình SVM chỉ có thể dự đoán đúng "class 1 (Major, High) severity" và bỏ lỡ các nhóm mức độ nghiêm trọng khác. Hơn nữa, giá trị precision thấp của nó (0.34) cho thấy rằng nó trả về rất nhiều dương tính giả. Một lý do có thể đằng sau kết quả tương đối kém của các mô hình SVM và Naive Bayes là nhu cầu vốn có của chúng về việc có một tập hợp tính năng mạnh mẽ và đủ mẫu. Vì kích thước bộ dữ liệu của chúng tôi tương đối nhỏ để huấn luyện một mô hình từ đầu, vì vậy việc sao chép nghiên cứu này sử dụng các bộ dữ liệu/tính năng khác có thể cải thiện kết quả của các mô hình này.

Như đã đề cập, các nghiên cứu trước đây đã tận dụng một số chỉ số mã nguồn hướng đối tượng khác với các chỉ số mã nguồn cấp phương thức của chúng tôi. Tuy nhiên, chúng tôi không thể đơn giản so sánh kết quả RQ1 của chúng tôi với những công trình đã xuất bản này vì (1) chúng không ở cấp phương thức, đây là cấp độ thực tế hơn từ góc độ của các nhà thực hành, và (2) chúng đang dự đoán trên các mức độ nghiêm trọng khác nhau (thường chỉ là mức độ "high" và "low").

Tóm tắt RQ1: Trong số các mô hình cổ điển, Random Forest (MCC=0.21) và SVM (MCC=0) mô hình có hiệu suất tốt nhất và yếu nhất, tương ứng.

Mục tiêu: Mục đích của RQ này là nghiên cứu hiệu quả của biểu diễn mã được cung cấp bởi một mô hình ngôn ngữ lớn đã được huấn luyện trước tên là CodeBERT trong việc dự đoán mức độ nghiêm trọng lỗi. Các động lực đằng sau RQ này là: 1) những mô hình này có khả năng xuất sắc trong phạm vi rộng các nhiệm vụ liên quan đến phần mềm bao gồm phát hiện lỗi, phát hiện bản sao và sửa chữa chương trình được thể hiện thành công trong các nghiên cứu trước đây, 2) vì những mô hình này đã được huấn luyện trên hàng triệu mẫu, nó cho phép chúng tôi tinh chỉnh chúng cho các nhiệm vụ xuôi dòng khác nhau (ví dụ: các nhiệm vụ dự đoán lỗi của chúng tôi) với các bộ dữ liệu tương đối nhỏ (như bộ dữ liệu của chúng tôi) so với việc huấn luyện mô hình từ đầu đòi hỏi các bộ dữ liệu lớn, 3) những mô hình này tận dụng biểu diễn của dữ liệu đầu vào, vì vậy không cần trích xuất các tính năng từ dữ liệu đầu vào (như các chỉ số mã được trích xuất trong RQ1), đây là một nhiệm vụ khó trong dữ liệu và ứng dụng phức tạp, và cuối cùng, 4) vì những mô hình này đã được huấn luyện trên một số ngôn ngữ lập trình, nghiên cứu của chúng tôi có thể dễ dàng áp dụng cho các ngôn ngữ lập trình khác (được hỗ trợ bởi CodeBERT) mà không có bất kỳ thay đổi nào.

Hơn nữa, CodeBERT hỗ trợ cả đầu vào ngôn ngữ lập trình và ngôn ngữ tự nhiên, vì vậy các đầu vào bổ sung như mô tả lỗi có thể được cung cấp như đầu vào bổ sung cho nhiệm vụ được nghiên cứu của chúng tôi có thể cải thiện kết quả hơn nữa, điều này có thể được nghiên cứu như một công việc tương lai tiềm năng.

Thiết kế: Chúng tôi chọn mô hình "codebert-base" từ Hugging Face được khởi tạo với "roberta-base" và được huấn luyện trên dữ liệu bi-modal (tài liệu & mã) với các mục tiêu MLM (mô hình hóa ngôn ngữ che mặt) và RTD (phát hiện token thay thế). Vì chúng tôi chỉ sử dụng mã nguồn như một đầu vào, chúng tôi không tận dụng phần NL (Ngôn ngữ Tự nhiên) của đầu vào mô hình này.

Vì mô hình CodeBERT là một encoder, chúng tôi thêm một lớp RobertaClassificationHead, để làm cho nó áp dụng được cho nhiệm vụ phân loại của chúng tôi. Chúng tôi thêm lớp này vào CodeBERT để tuân theo kiến trúc có tên RobertaForSequenceClassification, được cung cấp bởi Hugging Face như một mô hình phân loại. Chúng tôi sử dụng lớp Softmax trong kiến trúc này vì chúng tôi có một vấn đề phân loại đa lớp và cũng CrossEntropyLoss được sử dụng để tính toán loss trong quá trình huấn luyện.

Để tinh chỉnh mô hình CodeBERT, trước tiên chúng tôi đóng băng các lớp của nó để ngăn chặn việc thay đổi trọng số của các lớp hiện có một cách mạnh mẽ và chỉ huấn luyện lớp phân loại được thêm của chúng tôi bằng một số lượng lớn epoch (40 epochs). Trong quá trình huấn luyện, chúng tôi sử dụng kỹ thuật early stopping với patience = 3 để ngăn chặn vấn đề over-fitting bằng cách xem xét giá trị loss của mô hình sử dụng tập validation. Trong bước tiếp theo, chúng tôi unfreeze các lớp của mô hình CodeBERT và tinh chỉnh toàn bộ kiến trúc bằng cách sử dụng một learning rate nhỏ hơn và ít epoch hơn để điều chỉnh trọng số của tất cả các lớp. Trong bước này, chúng tôi sử dụng năm epoch, như được đề xuất trong nghiên cứu trước đây để sử dụng một số lượng nhỏ epoch để ngăn chặn vấn đề over-fitting và không thay đổi trọng số mô hình đã được huấn luyện trước, một cách mạnh mẽ.

Trong toàn bộ quá trình huấn luyện, chúng tôi sử dụng phần train để huấn luyện mô hình và phần validation để đánh giá hiệu quả của mô hình, và chúng tôi cũng lưu checkpoint của mô hình với điểm F1-weighted tốt nhất. Cuối cùng, chúng tôi sử dụng phần test (không bao giờ được nhìn thấy trong quá trình huấn luyện) cho đánh giá cuối cùng của mô hình.

Vì kích thước khối 512 là giá trị tối đa được hỗ trợ bởi CodeBERT, chúng tôi pad đầu vào của mình với padding token bất cứ khi nào nó ngắn hơn 512 tokens, và bất cứ khi nào độ dài mẫu lớn hơn 512, chúng tôi cắt ngắn nó bằng cách giữ 512 tokens đầu tiên.

Kết quả: Các đường cong ROC và Precision-Recall của mô hình CodeBERT được cung cấp trong Hình 1. Những biểu đồ này cho thấy rằng mô hình CodeBERT dự đoán nhãn mức độ nghiêm trọng "Class 2 (Medium)" rất tốt vì các đường cong ROC và Precision-Recall của lớp này rất gần với đường cong bộ phân loại hoàn hảo. Kết quả này có vẻ thú vị vì, mặc dù số lượng thể hiện "Class2 (Medium)" thấp trong bộ dữ liệu của chúng tôi (489 trong phần train), mô hình đã dự đoán đúng hầu hết các thể hiện lớp này (42 trong số 48) trong phần test.

Bảng IV cho thấy kết quả của mô hình này liên quan đến các chỉ số đánh giá của chúng tôi. Những kết quả này cho thấy hiệu quả đầy hứa hẹn của CodeBERT trong việc dự đoán mức độ nghiêm trọng lỗi do các giá trị tương đối cao của tất cả các chỉ số đánh giá. Bằng cách so sánh kết quả này với kết quả của tám mô hình khác nhau được cung cấp trong RQ1 (Bảng III), chúng tôi có thể nói rằng mô hình CodeBERT có hiệu quả tốt hơn đáng kể vì nó tăng các giá trị F1-Weighted, AUC, và MCC lần lượt là 0.16 (29%), 0.22 (33%), và 0.30 (140%) so với mô hình Random Forest (mô hình tốt nhất trong số tất cả các mô hình RQ1).

Hơn nữa, chúng tôi đã cung cấp ma trận nhầm lẫn của mô hình CodeBERT trong Hình 2 nơi các nhãn thực tế và các nhãn được dự đoán được hiển thị cho mỗi lớp. Hình này cho thấy rằng CodeBERT đã gán nhãn sai 18 lỗi với mức độ nghiêm trọng "class 0"; nó dự đoán 15 trong số chúng là mức độ nghiêm trọng "class 1", có nghĩa là nó phát hiện các lỗi nghiêm trọng như các lỗi mức độ nghiêm trọng chính, nhưng nó gán mức độ nghiêm trọng thấp hơn (medium, low) chỉ cho ba lỗi. Đối với các lỗi có nhãn "class 1", nó dự đoán mức độ nghiêm trọng đúng trong 87% trường hợp. Nó cũng phân loại tất cả các lỗi với nhãn "class 2" đúng, ngoại trừ sáu lỗi nơi nó gán các giá trị mức độ nghiêm trọng cao hơn. Các lỗi với nhãn "class 3" được dự đoán có các giá trị mức độ nghiêm trọng cao hơn trong nhiều trường hợp. Những phân loại sai này có thể tăng thời gian điều tra của đội ngũ nhà phát triển/QA trong thực tế, nhưng nó không tăng rủi ro của việc sử dụng kỹ thuật này trái ngược với các tình huống mà mô hình gán các nhãn mức độ nghiêm trọng thấp hơn cho các lỗi thực tế (ví dụ: coi các lỗi nghiêm trọng như các lỗi mức độ nghiêm trọng thấp). Nó cũng vẫn hoạt động tốt hơn phương pháp cổ điển tốt nhất (Random Forest) ngay cả về mặt phân loại sai cho lớp này.

Kết quả của chúng tôi cho thấy rằng biểu diễn mã có khả năng cao hơn trong việc dự đoán mức độ nghiêm trọng lỗi của các bộ dữ liệu được nghiên cứu so với việc sử dụng các chỉ số mã nguồn được chọn của chúng tôi, một mình. Lý do tiềm năng đầu tiên là sức mạnh của ngữ nghĩa đã học đằng sau các embedding mã nguồn và biểu diễn mã so với các chỉ số mã nguồn được trích xuất có thể không có tương quan cao với mức độ nghiêm trọng lỗi, bất kể mô hình dự đoán. Lý do tiềm năng thứ hai là, trong các trường hợp mà kích thước bộ dữ liệu tương đối nhỏ, việc huấn luyện một mô hình từ đầu có thể không dẫn đến kết quả tốt, nhưng việc sử dụng các mô hình đã được huấn luyện trước và tinh chỉnh cho nhiệm vụ xuôi dòng được yêu cầu hoạt động tốt vì nó đã được huấn luyện trên các bộ dữ liệu lớn trong giai đoạn pre-training.

Tóm tắt RQ2: CodeBERT vượt trội hơn Random Forest (mô hình tốt nhất của RQ1) lần lượt là 29%, 33%, và 140% cho F1-Weighted, AUC, và MCC.

Mục tiêu: Trong RQ này, chúng tôi nghiên cứu tác động của việc tích hợp các chỉ số mã nguồn với các mô hình CodeBERT. Vì chúng tôi thấy rằng các chỉ số mã nguồn một mình không phải là chỉ số tốt của mức độ nghiêm trọng lỗi trong các bộ dữ liệu của chúng tôi, động lực đằng sau RQ này là tìm khả năng của chúng như một đầu vào bổ sung (vì chúng thể hiện các tính năng quan trọng liên quan đến mã nguồn như độ phức tạp, khả năng đọc, khả năng bảo trì, v.v.) cho mô hình CodeBERT để cải thiện hiệu quả của mô hình.

Thiết kế: Vì CodeBERT đã được huấn luyện trên mã nguồn, nó có thể hiểu đơn giản đầu vào mã nguồn trong khi các chỉ số mã nguồn được tính toán của chúng tôi là một số giá trị số (ví dụ: 1, 2) vì vậy thách thức của chúng tôi là tìm cách tích hợp những giá trị số này với đầu vào mã nguồn của chúng tôi theo cách mà CodeBERT có thể hiểu và tận dụng những dữ liệu này. Chúng tôi cung cấp hai kiến trúc khác nhau có tên "ConcatInline" và "ConcatCLS".

Đối với kiến trúc ConcatInline, chúng tôi chuyển đổi các tính năng chỉ số mã nguồn của chúng tôi sang định dạng văn bản và cung cấp nó cùng với đầu vào mã nguồn cho mô hình. Có nhiều cách tiềm năng để thêm các tính năng số/phân loại vào mô hình CodeBERT. Ý tưởng của chúng tôi dựa trên nghiên cứu trước đây trong NLP nơi các tính năng số được thêm vào BERT cho phân loại văn bản ngắn. Vì mô hình CodeBERT đã được huấn luyện trên dữ liệu văn bản và cũng xem xét bối cảnh xung quanh mỗi từ trong các giai đoạn huấn luyện/suy luận, chúng tôi cung cấp bối cảnh cho mỗi tính năng mã nguồn để làm cho toàn bộ đầu vào phong phú và có thông tin. Theo cách này, thay vì cung cấp trực tiếp các giá trị chỉ số mã thô, chúng tôi cung cấp một đoạn văn đơn giản chứa mô tả (dựa trên văn bản mẫu) cho mỗi chỉ số mã và cũng các giá trị chỉ số mã số. Chúng tôi không chuyển đổi các giá trị của các chỉ số mã sang định dạng chữ cái (ví dụ: sử dụng "one" thay vì "1") vì định dạng chữ cái tăng độ dài văn bản đầu vào một cách mạnh mẽ và mô hình CodeBERT có hạn chế về kích thước đầu vào. Một ví dụ về cách tiếp cận này được hiển thị trong Bảng V. Lưu ý rằng việc sử dụng các mẫu khác cho bối cảnh có thể thay đổi kết quả, và phân tích độ nhạy phù hợp với mẫu bối cảnh là cho công việc tương lai của chúng tôi.

Sau khi tạo đoạn văn mô tả các chỉ số mã và giá trị của chúng, chúng tôi đặt nó trong phần đầu tiên của đầu vào mô hình CodeBERT (trước token SEP) và sau đó cung cấp mã nguồn chứa phương thức có lỗi trong phần thứ hai của đầu vào mô hình (sau token SEP). Cấu trúc đầu vào cuối cùng cho CodeBERT trong kiến trúc này là: [CLS], CreatedParagraph, [SEP], SourceCode, [EOS]

Vì mô hình CodeBERT có thể hỗ trợ đầu vào lên đến 512 tokens, trong trường hợp đầu vào dài, chúng tôi giữ toàn bộ phần NL (ngôn ngữ tự nhiên chứa đoạn văn được tạo của các chỉ số mã) và cắt ngắn phần PL (phần ngôn ngữ lập trình chứa mã nguồn phương thức có lỗi) từ cuối để giảm toàn bộ kích thước đầu vào xuống 512 tokens. Ngược lại, nếu độ dài của đầu vào cuối cùng nhỏ hơn 512, chúng tôi pad đầu vào đến cuối với các Pad tokens.

Đối với kiến trúc ConcatCLS, chúng tôi tích hợp các vector chỉ số mã nguồn số với token CLS đầu ra của mô hình CodeBERT được hiển thị trong Hình 3. Nói cách khác, trong kiến trúc này, chúng tôi nối đầu ra của mô hình CodeBERT (token CLS thể hiện biểu diễn mã nguồn) và các vector số của chúng tôi (đại diện cho các chỉ số mã nguồn) và cung cấp vector kết quả cho một lớp dense. Có kiến trúc này, trong giai đoạn back-propagation của quá trình tinh chỉnh, trọng số mô hình CodeBERT không chỉ được cập nhật theo các biểu diễn mã nguồn mà chúng cũng được cập nhật dựa trên đóng góp của các chỉ số mã nguồn.

Kết quả: Các đường cong ROC và Precision-Recall của các thử nghiệm của chúng tôi sử dụng các kiến trúc ConcatInline và ConcatCLS được hiển thị trong Hình 4. Ngoài ra, kết quả liên quan đến các chỉ số đánh giá khác được cung cấp trong Bảng VI. Kết quả của chúng tôi cho thấy rằng việc tích hợp các chỉ số mã nguồn trong định dạng văn bản (ConcatInline) tăng hiệu quả trong việc dự đoán các nhãn mức độ nghiêm trọng khác nhau của mô hình CodeBERT lần lượt là 0.03, 0.01, và 0.04 cho các chỉ số F1-Weighted, AUC, và MCC. Ngoài ra, các đường cong ROC được cung cấp của kiến trúc ConcatInline cho thấy rằng việc thêm các chỉ số mã nguồn tăng khả năng của mô hình CodeBERT để dự đoán nhãn mức độ nghiêm trọng "class 0 (Critical, Blocker)" lên 0.02 so với kết quả RQ2. Sự cải thiện này cho thấy tác động quyết định của các chỉ số mã nguồn của chúng tôi trong việc dự đoán các giá trị mức độ nghiêm trọng lỗi, đặc biệt là cho các lỗi mức độ nghiêm trọng cao.

Kiến trúc ConcatCLS đã tăng hiệu quả của mô hình CodeBERT lần lượt là 0.04 và 0.05 cho các chỉ số F1-Weighted và MCC, nhưng giá trị AUC không được cải thiện. Ngoài ra, từ giá trị chỉ số F1-Perclass, chúng tôi thấy rằng kết quả được cải thiện trong tất cả các lớp. Hình 5 cho thấy ma trận nhầm lẫn của cả hai kiến trúc ConcatInline và ConcatCLS. Bằng cách so sánh những hình này với ma trận nhầm lẫn của mô hình CodeBERT (Hình IV), chúng tôi thấy rằng mô hình ConcatInline hoạt động tốt hơn mô hình CodeBERT cho tất cả các lớp ngoại trừ nhãn "class 0" nơi số lượng phân loại đúng giảm một. Mô hình ConcatCLS đã cải thiện số lượng phân loại đúng trong tất cả các lớp (lần lượt là 1, 1, 5, 7, cho lớp 0, đến 3) so với mô hình CodeBERT.

Kết quả thú vị từ các ma trận nhầm lẫn là kiến trúc ConcatCLS đã cải thiện hiệu quả dự đoán cho "class 3 (Low, Trivial, Minor)" vì nó đã giảm số lượng phân loại sai của lớp này thành "class 2 (Medium)" (vấn đề này đã được thảo luận trong RQ2) từ 70 xuống 64 (8.5%).

Để so sánh các kiến trúc ConcatCLS và ConcatInline, chúng tôi xem xét ma trận nhầm lẫn và các giá trị chỉ số đánh giá. Chúng tôi xem xét ma trận nhầm lẫn vì nó cung cấp thông tin tốt hơn về mỗi nhãn riêng biệt và cũng các chỉ số đánh giá khác như MCC, F1-perclass, và F1-weighted là các chỉ số tốt cho các nhiệm vụ phân loại, đặc biệt là giá trị MCC hoạt động tốt trên các bộ dữ liệu không cân bằng, như bộ dữ liệu của chúng tôi. Dựa trên các hình và bảng được cung cấp cho thấy kết quả, chúng tôi có thể nói rằng kiến trúc ConcatCLS vượt trội hơn mô hình ConcatInline. Cuối cùng, chúng tôi kết luận rằng việc tích hợp các chỉ số mã nguồn như một đầu vào bổ sung trong các kiến trúc ConcatInline và ConcatCLS cải thiện hiệu quả của mô hình CodeBERT.

Tóm tắt RQ3: Việc tích hợp các chỉ số mã với biểu diễn mã, trong định dạng vector (ConcatCLS) cải thiện hiệu quả của CodeBERT (MCC) lên 5% và giảm tỷ lệ phân loại sai 8.5%.

Để đối phó với tính ngẫu nhiên tiềm ẩn của kết quả, trước tiên chúng tôi loại bỏ các mẫu trùng lặp tiềm ẩn từ bộ dữ liệu của chúng tôi, và sau đó xáo trộn và chia nó thành ba phần train, validation, và test sử dụng tỷ lệ 70%, 15%, 15%. Chúng tôi không thực hiện kỹ thuật cross-validation để giữ nghiên cứu của chúng tôi phù hợp với nghiên cứu đã nghiên cứu trước đây như CodeXGLUE và các thử nghiệm CodeBERT được cung cấp. Tuy nhiên, việc chạy các kỹ thuật cross-validation như K-Fold có thể ảnh hưởng đến kết quả thu được của chúng tôi có thể được điều tra trong công việc tương lai.

Một trong những mối đe dọa tính hợp lệ nội bộ đối với nghiên cứu này là cách chúng tôi phải đối phó với những hạn chế của các công cụ như độ dài ngữ cảnh trong LLMs. Kích thước 512 token của CodeBert đã được xử lý bằng cách cắt ngắn các mẫu lớn thành 512 tokens đầu tiên. Điều này có thể tác động đến kết quả được báo cáo. Công việc tương lai có thể thử các LLM với kích thước ngữ cảnh lớn hơn để hiểu tác động của hạn chế này. Một vấn đề khác sẽ là quá trình tích hợp các tính năng số vào mô hình CodeBERT có thể ảnh hưởng đến kết quả của chúng tôi, vì vậy để không thay đổi cấu trúc mô hình CodeBERT cơ sở một cách mạnh mẽ, chúng tôi đã không thêm nhiều lớp phức tạp để làm cho việc so sánh công bằng. Tuy nhiên, có thể tận dụng các tính năng số và tích hợp chúng với mô hình CodeBERT theo cách phức tạp hơn có thể cải thiện kết quả. Chúng tôi cũng trích xuất các chỉ số mã nguồn nổi tiếng sử dụng các thư viện chuẩn, để giảm bất kỳ yếu tố gây nhiễu nào trong những phép đo này.

Để giảm tác động của các mối đe dọa tính hợp lệ cấu trúc, chúng tôi đã sử dụng bảy chỉ số đánh giá. Phân phối lớp lệch vốn có trong các bộ dữ liệu không cân bằng có thể dẫn đến các mô hình thể hiện sự thiên vị dự đoán đối với các lớp đa số, do đó việc đánh giá những mô hình này là một nhiệm vụ thách thức. Vì vậy, chúng tôi sử dụng định dạng "Weighted" của các chỉ số đánh giá của chúng tôi để giảm các hiệu ứng thiên vị. Những chỉ số này được tính toán bằng cách lấy trung bình của tất cả điểm số mỗi lớp và cũng xem xét số lượng mẫu của mỗi lớp. Nói cách khác, đầu ra được tính toán đã tính đến đóng góp của mỗi lớp bằng cách xem xét số lượng thể hiện trong lớp đã cho đó. Chúng tôi cũng báo cáo tất cả dữ liệu thô như ma trận nhầm lẫn.

Mặc dù chúng tôi đã thực hiện một thử nghiệm lớn với một số dự án trong hai bộ dữ liệu riêng biệt, tất cả các dự án của chúng tôi đều từ lĩnh vực mã nguồn mở và công việc tương lai cần thiết để sao chép nghiên cứu trên các ứng dụng thương mại. Ngoài ra, mặc dù tại thời điểm thiết kế nghiên cứu này CodeBERT là LLM nghệ thuật tiên tiến cho mã, cộng đồng đã đi một chặng đường dài cho đến bây giờ bằng cách giới thiệu các mô hình tiên tiến hơn và lớn hơn như GPT-4. Các công việc tương lai cần thiết để so sánh kết quả khi các LLM tiên tiến hơn được thuê. Tuy nhiên, chúng tôi chỉ mong đợi kết quả tốt hơn hoặc ít nhất là bằng nhau khi sử dụng các LLM tiên tiến hơn.

Các cách tiếp cận dựa trên báo cáo lỗi tận dụng mô tả báo cáo lỗi và các kỹ thuật ngôn ngữ tự nhiên để dự đoán mức độ nghiêm trọng lỗi. Tian et al. sử dụng bộ phân loại nearest neighbors và đo lường sự tương tự giữa các báo cáo lỗi trước đây và các báo cáo lỗi được đưa ra để dự đoán nhãn mức độ nghiêm trọng lỗi được đưa ra. Các thử nghiệm rộng rãi của họ trên hàng nghìn báo cáo lỗi cho thấy kết quả đầy hứa hẹn bằng cách có lên đến 76.3% cho chỉ số F-measure. Phân tích cảm xúc là một kỹ thuật khác được tận dụng bởi Ramay et al nơi các tác giả sử dụng một mô hình học sâu để phân tích các báo cáo lỗi được đưa ra để dự đoán mức độ nghiêm trọng lỗi. Các mô hình được đề xuất của họ có F-measures lên đến 88.01%. Lamkanfi et al đề xuất dự đoán mức độ nghiêm trọng lỗi sử dụng kỹ thuật Topic Modeling. Đánh giá của họ về 30.000 báo cáo lỗi được trích xuất từ các dự án Eclipse, Mozilla, và Netbeans cho thấy hiệu quả của cách tiếp cận của họ bằng cách báo cáo F-measure từ 65% đến 75% trên các báo cáo Bugzilla. Tan et al tận dụng các cặp hỏi-đáp có sẵn trên trang web Stack Overflow và kết hợp chúng với các báo cáo lỗi liên quan để tạo ra một phiên bản nâng cao của các báo cáo lỗi. Sự kết hợp này làm cho báo cáo lỗi phong phú dẫn đến cải thiện khoảng 23% của chỉ số F-measure trung bình.

Công trình gần đây nhất về dự đoán mức độ nghiêm trọng dựa trên báo cáo lỗi, thực tế, sử dụng các mô hình Transformer tương tự như CodeBERT của chúng tôi. Tuy nhiên, nghiên cứu của họ khác với công việc của chúng tôi vì những lý do sau: a) họ vẫn chỉ dựa trên báo cáo lỗi và không phải mã nguồn, và (b) họ có phân loại nhị phân trên "nhãn ưu tiên" là một trường khác với "mức độ nghiêm trọng" trong các hệ thống theo dõi vấn đề. Mặc dù có thể có tương quan, nhưng mức độ nghiêm trọng thường vốn có của lỗi (do đó ở một mức độ có thể được ước tính chỉ bằng mã) nhưng "ưu tiên" được gán một phần dựa trên lịch trình của dự án. Nói cách khác, người ta có thể gắn thẻ một lỗi không nghiêm trọng là ưu tiên cao vì ví dụ: theo lịch trình của nhà phát triển, tốt hơn là được sửa bây giờ hơn là tháng tới, do đó có ưu tiên cao hơn.

Như đã giải thích trong phần giới thiệu, không giống như tất cả các bài báo trong danh mục này, chúng tôi không làm việc trên các báo cáo vấn đề mà chỉ trên mã nguồn. Vì giả định rằng tất cả các lỗi trong mã đều có báo cáo vấn đề được gán cho chúng là sai và có nhiều lỗi được phát hiện bằng cách chạy các trường hợp kiểm tra không có báo cáo lỗi (hoặc ít nhất là chưa có). Vì lý do tương tự, chúng tôi không so sánh những nghiên cứu như vậy với cách tiếp cận của chúng tôi.

Các cách tiếp cận dựa trên chỉ số mã nguồn trích xuất các chỉ số mã từ mã có lỗi và dự đoán các nhãn mức độ nghiêm trọng dựa trên chúng. Các chỉ số thiết kế hướng đối tượng cấp lớp được sử dụng để dự đoán các lỗi Mức độ nghiêm trọng cao và thấp trên bộ dữ liệu NASA nơi các tác giả thấy những chỉ số thiết kế này có thể dự đoán các lỗi mức độ nghiêm trọng thấp trong các lớp dễ có lỗi tốt hơn các lỗi mức độ nghiêm trọng cao. Trong một nghiên cứu khác, các chỉ số cấp lớp hướng đối tượng khác nhau được sử dụng để huấn luyện các kỹ thuật hồi quy logistic và mạng thần kinh để dự đoán mức độ nghiêm trọng lỗi của ba phiên bản Mozilla Firefox. Các tác giả nói rằng mạng thần kinh được đề xuất của họ dự đoán các lỗi mức độ nghiêm trọng cao và trung bình chính xác hơn các lỗi mức độ nghiêm trọng thấp. Một số chỉ số cấp lớp hướng đối tượng được sử dụng để điều tra khuynh hướng lỗi lớp trong sự phát triển sau phát hành của hệ thống bằng cách sử dụng ba bản phát hành của dự án Eclipse.

Không giống như hầu hết công việc trước đây, chúng tôi tập trung vào dự đoán mức độ nghiêm trọng lỗi "cấp phương thức" mà trong thế giới thực thực tế hơn nhiều vì nó có thể được tính toán hiệu quả và cũng mức độ chi tiết cấp lớp/mô-đun quá thô cho các nhà thực hành để hành động. Chúng tôi cũng mang biểu diễn mã như một cách tiếp cận nghệ thuật tiên tiến để tối đa hóa sức mạnh dự đoán từ mã và chỉ số.

Trong bài báo này, chúng tôi đề xuất một cách tiếp cận để sử dụng LLM cho mã để dự đoán mức độ nghiêm trọng của một lỗi chỉ sử dụng phương thức có lỗi làm đầu vào. Nó tích hợp các chỉ số mã nguồn cấp phương thức được trích xuất và mã của phương thức có lỗi làm đầu vào và sử dụng CodeBERT để dự đoán các nhãn mức độ nghiêm trọng đa lớp. Thông qua một nghiên cứu thực nghiệm quy mô lớn trên 3.342 phương thức có lỗi từ 19 dự án của các dự án Java mã nguồn mở, chúng tôi cho thấy rằng việc sử dụng codeBERT chỉ trên mã nguồn có thể cải thiện dự đoán bằng cách sử dụng các phương pháp cổ điển trên các chỉ số mã lên đến 140%, cho các chỉ số đánh giá khác nhau. Ngoài ra, chúng tôi cho thấy rằng việc cung cấp các chỉ số mã nguồn được thu thập như một đầu vào bổ sung cho CodeBERT có thể cải thiện thêm kết quả. Thú vị là, nó cũng giảm tỷ lệ dương tính giả lên đến 8.5% cho các mức độ nghiêm trọng khác nhau, điều này cho thấy rằng các mô hình đã được huấn luyện trước phải được hướng dẫn (ví dụ: bằng cách đưa thông tin/ngữ cảnh bổ sung và tinh chỉnh) cho một nhiệm vụ cụ thể để cung cấp kết quả tốt nhất. Các hướng tương lai tiềm năng của nghiên cứu này là thêm các chú thích mã có sẵn và cấu trúc vốn có của mã (ví dụ: luồng dữ liệu, luồng điều khiển, v.v.) như một đầu vào bổ sung vào mô hình CodeBERT (hoặc các LLM gần đây hơn như GPT-4) có thể nâng cao kết quả.
