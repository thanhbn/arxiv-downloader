# 2212.10017.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2212.10017.pdf
# File size: 1454270 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and
Semantics Capacities
WEI MA, Nanyang Technological University, Singapore
SHANGQING LIU∗,Nanyang Technological University, Singapore
MENGJIE ZHAO, Ludwig Maximilian University of Munich, Germany
XIAOFEI XIE, Singapore Management University, Singapore
WENHAN WANG, University of Alberta, Canada
QIANG HU, The University of Tokyo, Japan
JIE ZHANG, Noah’s Ark Lab, Huawei, China
YANG LIU, Nanyang Technological University, Singapore
Significant progress has been made in code intelligence through code models that embed knowledge about
programming languages. Past research has examined how well these models grasp code syntax, yet their
understanding of code semantics still needs to be explored. Moreover, current analyses typically link the
number of edges in an abstract syntax tree (AST) with syntax distance. They also frequently necessitate
reducing the high-dimensional space of deep learning models to a lower dimension, potentially leading to
inaccuracies. We extensively analyze seven code models to investigate how code models represent code
syntax and semantics. This includes four prominent code pre-trained models (CodeBERT, GraphCodeBERT,
CodeT5, and UnixCoder) and three large language models (StarCoder, CodeLlama, and CodeT5+). We have
developed four probing tasks to evaluate the models’ abilities to learn code syntax and semantics. These tasks
focus on reconstructing code syntax and semantic structures—such as Abstract Syntax Trees (AST), Control
Flow Graphs (CFG), Control Dependency Graphs (CDG), and Data Dependency Graphs (DDG)—within the
models’ representation spaces. These structures are fundamental to understanding code. Additionally, we
explore the role of syntax tokens in each token representation and the extended dependencies among code
tokens. Furthermore, we examine the distribution of attention weights concerning code semantic structures.
Through detailed analysis, our results emphasize the strengths and weaknesses of various code models in
mastering code syntax and semantics. The findings reveal that these models are proficient in grasping code
syntax, effectively capturing the relationships and roles of syntax tokens. However, their ability to encode code
semantics shows more variability. CodeT5 and CodeBERT excel at capturing control and data dependencies,
whereas UnixCoder performs less effectively. We also find that large language models (LLMs) do not generally
outperform pre-trained models significantly. Interestingly, the shallower layers of LLMs demonstrate better
performance compared to their deeper layers. Our analysis of attention weights indicates that different
attention heads are specialized for distinct roles in encoding code semantics. Our research underscores the
necessity for further improvements in code models to enhance their ability to learn code semantics effectively.
∗Corresponding author.
Authors’ addresses: Wei Ma, Nanyang Technological University, Singapore, ma_wei@ntu.edu.sg; Shangqing Liu, Nanyang
Technological University, Singapore, liu.shangqing@ntu.edu.sg; Mengjie Zhao, Ludwig Maximilian University of Munich,
Munich, Germany, mzhaolmu@gmail.com; Xiaofei Xie, Singapore Management University, Singapore, xfxie@smu.edu.sg;
Wenhan Wang, University of Alberta, Edmonton, Canada, wenhan12@ualberta.ca; Qiang Hu, The University of Tokyo,
Tokyo, Japan, qianghu0515@gmail.com; Jie Zhang, Noah’s Ark Lab, Huawei, Xi’An, China, clark.zhang@huawei.com; Yang
Liu, Nanyang Technological University, Singapore, yangliu@ntu.edu.sg.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM 0004-5411/2018/8-ART111
https://doi.org/XXXXXXX.XXXXXXX
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.arXiv:2212.10017v3  [cs.SE]  17 Apr 2024

--- PAGE 2 ---
111:2 W. Ma, S. Liu and M. Zhao et al.
This study enriches our understanding of the capabilities of code models in analyzing syntax and semantics.
Our findings offer valuable insights for future code model enhancements, helping optimize their application
across a range of code-related tasks.
ACM Reference Format:
Wei Ma, Shangqing Liu, Mengjie Zhao, Xiaofei Xie, Wenhan Wang, Qiang Hu, Jie Zhang, and Yang Liu. 2018.
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities. J. ACM 37, 4, Article 111
(August 2018), 29 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Many code models [ 2,4,13,17,29,31,41,42,63,71] have been proposed to greatly advance the
development of code intelligence. Many software engineering approaches are based on fine-tuning
these pre-trained models, such as code clone detection, vulnerability detection and code completion.
Recently, large language models (LLM) have proven to have emergent abilities [ 72] that pre-trained
models do not possess. This technology makes it possible to generate software automatically, such
as MetaGPT [ 19]. Despite the fact that these models have been proven to be efficient on various
code-related tasks, a fundamental issue still remains unresolved for these code models about how
they understand code. Significantly, recent works [ 14,28,75] indicate that code models cannot give
reasonable results if token replacement or insertion tricks are used. Regarding the code models, we
should consider deeply their ability to learn the basic characteristics of the code, “ What kind of
code knowledge can these code models learn? ”. A program consists of syntax features (e.g., AST)
and semantic information (e.g., data dependency); as a result, this issue can be further decomposed
into “ Can code models capture program syntax well? ” and “ What kind of program semantics can
code models learn? ”. Prior studies have begun to explore the questions raised, especially the first
sub-question. However, a deeper understanding of the knowledge gained by code models remains
elusive. While research by Wan et al . [69] and Hernández López et al . [18] shows that code pre-
trained models can grasp program syntax, their analysis does not extend to program semantics. It
is also important to note that these studies have two assumptions:1) the number of links between
nodes is related to code syntax; 2) a linear relationship exists between the representation of code in
high-dimensional and low-dimensional spaces. However, these assumptions have some limitations:
1) the number of links is not necessary to be related to code syntax. 2) the smaller distance in the
raw code text does not mean the syntax closeness in the representation space. Furthermore, Troshin
and Chirkova [66] examine both code syntax and semantics through various tasks. However, this
work follows a similar approach to the earlier studies when analyzing syntax. It also falls short
of thoroughly investigating the various semantics inherent in programming. More importantly,
there are no studies on the transformer decoder, which is the architecture of most LLMs, such as
the StarCoder [34] and Llama [58] families.
These questions are significant for us to review the applications of code models in software
engineering. If the code models can understand the code almost perfectly, the outputs of the code
models can be trusted, and automated software generation is possible. To address the aforemen-
tioned challenges, in this paper, we comprehensively investigate four widely adopted code pre-
trained models: CodeBERT [ 13] (Encoder-only), GraphCodeBERT [ 17] (Encoder-only), CodeT5 [ 71]
(Encoder-decoder) and UnixCoder [ 16] (UniLM-style). We also include three large language models
(LLMs), StarCoder [34], CodeLlama [58] and CodeT5+ [70].
Four probing tasks are employed to analyze the capabilities of the models in learning code
syntax and semantics. Specifically, we utilize two syntax probing tasks, namely syntax node pair
prediction andtoken syntax tagging prediction . Both tasks aim to manipulate Abstract Syntax Trees
(AST) to assess the capabilities of pretrained models in learning code syntax since AST carries
all the syntax information of the code. Syntax node pair prediction aims to determine whether
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 3 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:3
vector representations of two syntax-close spans exhibit syntactic similarity while token syntax
tagging prediction aims to identify whether the vector representation captures the syntax rule of
the individual token. Syntax node pair prediction is to recover the AST structure from the vector
representation, and token syntax tagging prediction is to assign the syntax role to each code token
in the representation space. Both of the syntax information are significant for code models to
understand the code syntax at the global and local levels. The intention behind the two syntax
tasks is that the code-token vector representation should keep the syntax properties that exist in
the code: the syntax relationship between code tokens, and the syntax property of each code token.
In addition to syntax analysis, we further design two semantic probing tasks, namely semantic
relation prediction andsemantic propagation prediction . Both tasks are designed to investigate the
extent to which code models can effectively learn various aspects of code semantics. Semantic
relation prediction is to recover the significant code semantic structures in the representation space,
including Data Dependency Graph (DDG), Control Dependency Graph (CDG) and Control Flow
Graph (CFG). These semantic structures can represent code execution and states inside. Semantic
propagation prediction is to see if we can observe the long dependency relationship in the vector
space because a variable can be declared in the first statement but is used in the end. Lastly, we
performed the statistical analysis for the attention weights to comprehensively understand the role
and attention distribution in learning the semantics of the code.
Our syntax analysis shows that 1). code models effectively capture syntactic relationships between
token pairs and this property is easier to observe in the shallow hidden layers, indicating a strong
understanding of code syntax at a structural level; 2). code models are proficient at identifying
the syntax roles of individual tokens and this property is easier to observe in the deep hidden
layers. However, code pre-trained models show superior performance than LLMs for syntax tagging,
indicating the syntax characteristics are more difficult to observe in the code representation from
LLMs. The ease of being observed is not directly related to the performance of the model on
downstream tasks, as it depends on many factors, such as data quality and fine-tuning methods.
However, it can indicate the difficulty of how to build and tune a good model for the downstream
task based on LLM. In practice, people have tried and even found that the performance of traditional
small models is better than that of large models in some cases [8, 27].
Our semantic analysis demonstrates varying effectiveness in the ability of code models to predict
semantic relationships. The current models provide some nuanced understanding of code semantics,
such as data dependencies, but also highlight areas for improvement, particularly in capturing
complex semantic structures. Compared with the performance of syntactic tasks, the performance
of the code models in semantics is relatively low. For CodeT5+, the performance difference between
the encoder and decoder is very large. We think this may be caused by the different working
mechanisms of the encoder and decoder. The encoder focuses more on the nesting of global
information. The decoder is more focused on generating the next token based on the previous text.
This difference is especially obvious when analyzing the attention mechanism. When we conducted
attention analysis based on control dependencies, we found that for the decoder architecture, the
weight contribution of tokens with control dependencies is smaller than the weight contribution
of non-control dependencies. We believe this is because the encoder focuses on encoding global
information, but the decoder focuses more on the previous tokens and other non-control-dependent
information, resulting in different weight distributions.
Through extensive analysis, our work indicates that code models still need to be improved to
learn code syntax and semantics, making these properties more observable, so that reducing the
difficulty of building a downstream task model. The improvement should focus on how to integrate
code syntax and semantics into code models. This may need novel training strategies that can
integrate the whole code structure instead of flattening these structures. Although the model ability
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 4 ---
111:4 W. Ma, S. Liu and M. Zhao et al.
is scaled with its size [ 30,81], it is challenging to release its ability which needs a non-trivial extra
work to make the features more observable in the representation space for the downstream tasks.
In summary, our work has the following contributions:
•We propose the syntax and semantic probing tasks to analyze the code model’s ability to understand
syntax and semantics by directly recovering the code syntax and semantic structures (AST, CFG,
CDG and DDG) from the code representation. We study the distribution of the attention weights
related to the code semantics.
•We reveal that the syntax relationship is more observable in the shallow hidden layers, while the
token-role syntax is more observable in the deep layers. Code models have a superior performance for
the syntax representation than the semantic representation. Meanwhile, code models have inferior
performance for CFG compared with the other semantic structures, which requires code models to be
enhanced to represent CFG semantics. Different hidden layers show the different observable levels
for different types of code syntax and semantics.
•We first include large language models and show that their performance on probing tasks does not
have a massive advantage over pre-trained models, considering their huge amount of parameters.
This reflects that the code syntax and semantics are hidden and not obvious in the representation of
LLMs.
We hope these insights can inspire researchers to train more powerful code models. When
utilizing these models, it is essential to design the workflows guided by an understanding of these
models, taking into consideration whether the integration of additional code features is warranted.
Although numerous factors, including data quality, the complexity of downstream task models, and
feature integration methods influence the model performance in downstream tasks, our work can
offer some direction for their models and data design. Models devoid of semantic comprehension
require consideration of semantic information enhancement via feature integration when addressing
downstream tasks. Our work highlights that it is not mandatory to use the output of the final layer
- intermediate layers can potentially express syntax and semantics just as adequately. All code and
data can be found at the repository1.
2 MOTIVATION
The capabilities of pre-trained code models have been extensively discussed in previous studies
[18,66,69]. However, their analysis is based on the assumption that closer syntax between two
tokens in the code would result in smaller syntax distance, measured by the number of edges
between nodes in the AST [ 69], and correspondingly, a smaller distance in vector representation,
such as the Euclidean distance encoded by the code model. In other words, the assumption states that
“closer syntax" leads to “smaller edge distance in AST" which in turn leads to "smaller representation
distance." However, first, “smaller edge distance in AST” is not necessary to mean “close syntax”.
Second, code vector representation space is a high-dimension space. For example, CodeBERT
forms a 768-dimensional space but node distance in AST is a low dimensional space. Therefore,
the traditional distance metrics used in these works will not work well due to the curse of high
dimensionality [ 1,52]. First, the distance mapping from high-dimension space to low-dimension
space may not be accurate i.e., the smaller distance in the AST may not definitely represent the
smaller euclidean distance in the high-dimension space. Second, a small edge distance between
two nodes in an AST does not guarantee a close syntax relationship. This bias can make their
conclusions and approaches not generalized and can hinder our understanding of how code models
1https://github.com/Marvinmw/probing_analysis_tosem.git
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 5 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:5
Function DefifreturnreturncompareModule>baabdef max (a ,b)#return maximum valuedefmax(a,b){if ( a > b ): returnaelse:returnb}AST
Fig. 1. A simple code snippet with its AST.
85.4565.78
Fig. 2. Euclidean distance of token representations.
encode syntax. As well, it wrongly estimated the code syntax similarity and can lead to missing
analysis of the syntax structure for code models. The conclusion may not be useful for improving
model performance. We provide two examples for better illustration.
Figure 1 is the first example and provides a visualization of a function parsed into an AST. We
can see that the node distance between the variable “a” and “b” from the if-condition (marked in
green squares) is 2hops while the node distance between this variable “a” (green square) and the
variable “a” (orange square) from the return statement is 4hops. Hence, we can find that in the
low-dimension space, the variable “a” from the if-condition is close to the variable “b” and they are
syntax closer than the variable “a” from the return statement. However, the conclusion is opposite
in the high-dimension space. We encode this function by CodeBERT and calculate the euclidean
distance between the token vector representations for these variables. As shown in Figure 2, we
can see that the distance from the variable “a” (green square) to the variable “b” (green square)
from if-condition is 85.45 while the distance from it to the variable “a” (orange square) from the
return statement is 65.78 . Hence, through this example, we find that the distances in the low space
and high space are not consistent, and they may not be positively related.
For the other example, Figure 3 demonstrates that the function argument variable "b" has 4 hops
to "return" in the last statement of this function, and it shares the same distance as the function
argument variable "c". However, "b" should be closer in syntax to "c" since they are function
arguments.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 6 ---
111:6 W. Ma, S. Liu and M. Zhao et al.
Furthermore, these works mainly focus on exploring the capabilities of code pre-trained models
in learning code syntax. In-depth discussions about the learned code semantics (e.g., control/data
dependency analysis) are missing. Troshin and Chirkova [66] uses some tasks related to semantics
while they are not only related to one single code property that is usually required by the probing
analysis, the diversity of program semantics is also ignored and the deep analysis is lacked. Discus-
sion of code semantics is an essential part and cannot be overlooked as it forms the foundation for
many code-related tasks [9, 36, 38, 39, 80]. These works also lack the analysis of LLMs.
To overcome the problems above, we need to explore better approaches without the assumption
that the number of edges between nodes is related to code syntax. Code is a very special data
structure. First of all, it has a strict syntax definition; all code must comply with these syntax rules.
All names used in the code only need to follow specific naming rules. However, for the convenience
of human reading, humans usually annotate the code by names for easy reading and understanding.
These syntax rules can be expressed in a structured way by an abstract syntax tree (AST). Based
on these defined syntax rules, the code also expresses deeper semantic information. The code
has certain functionality, is executable, and implements specific logic. This semantic information
includes control flow, data flow, and dependencies. Almost all problems related to the code require
this information to solve. The control flow describes the specific logic of the code, expressing
the functionality implemented by the code. Data flow and dependencies express the semantic
association between different parts of the code. They are very important for code quality, and
defect vulnerability detection and repair. We can see that AST, Control Flow Graph (CFG), Control
Dependency Graph (CDG), and Data Dependency Graph (DDG) are structured expressions of code
syntax and semantics. Code tokens have different syntax and semantic relationships or properties
based on these structures. These relationships and properties are significant for understanding
code and should be represented in the learned vector space of code models. The ability of a code
model is to map the characteristics of the code to vector space. For a good code model, its feature
space should keep all the characteristics of the code as much as possible. The probing tasks we
design are to reconstruct these data structure relationships in the vector feature space.
Therefore, for syntax analysis, we try to reconstruct AST (syntax node pair prediction) and predict
the syntax label of the code tokens (token syntax tagging). AST carries all syntax information of
the code. Syntax node pair prediction can reflect how code models learn the syntax structure by
reconstructing AST in the representation space. Token syntax tagging is to see if the syntax role
of each token is encoded in the representation, which is a micro syntax property. Both of them
are significant for code models to learn code syntax. For semantic analysis, we reconstruct code
semantic structure (semantic propagation prediction) that includes a control dependency graph
(CDG), data dependency graph (DDG) and control flow graph (CFG). We also include the control
and data dependency with long distances (semantic propagation prediction). The three semantics
structures, CDG, DDG and CFG, are core concepts in program analysis, optimization, toolbars,
and various other software engineering tasks. The long dependency is one special characteristic
of the program. One can declare a variable at the beginning but use it after hundreds of lines.
Understanding them is significant for code models. Details can be found in Section 3.
3 METHODOLOGY
In this section, we introduce our analysis of the probing approaches for code syntax and semantics.
3.1 Preliminary Knowledge
Early software engineering researchers used feature engineering based on expert empirical knowl-
edge to extract code features as input to machine learning algorithms. Typically, experts design
feature extraction rules based on the specific tasks to be solved, such as the number of loops and
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 7 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:7
code complexity. These rule-based features are highly interpretable but limited by the experience
of experts and cannot cover the code syntax and semantic structure. In recent years, with the rise
of Transformer and the emergence of CodeBERT, software engineering researchers can extract
code features based on the encoding ability of pre-trained models, greatly reducing their reliance
on expert experience and knowledge. The transformer with the encoder-decoder architecture was
proposed in 2017 [ 68]. The encoder and the decoder are stacked by multiple encoder and decoder
layers, respectively. Both contain complex attention-calculation and allocation mechanisms. The
difference between the encoder layer and the decoder layer is that the encoder layer considers the
context of the model; the decoder layer only considers the output context of the model. The code
pre-trained models will use massive code data that is collected from the Internet such as GitHub
and Stack Overflow. Code pre-trained models can be grouped into three groups regarding their
network architecture. The first group uses a transformer encoder like CodeBERT. The second group
uses a transformer decoder like CodeGPT. The third group uses the transformer encoder-decoder
like CodeT5.
Before pre-training, we need to train a tokenizer based on code text to learn how to segment
text code, such as the algorithm Byte-Pair Encoding tokenization (BPE)2. When using Transformer
for feature learning, the original text code will be tokenized and we will get a sequence of code
tokens. Then, we converted it into the corresponding index in the embedding layer. This completes
the conversion from the raw code to the vector. The model is then optimized based on the training
paradigm implemented. Compared with traditional pre-train models, LLM is also a transformer
architecture but has a huge number of parameters and a huge amount of training data. LLM is
aligned with human preference. Since tuning the whole parameters of LLM is very expensive,
we usually use LoRA [ 22], fewshot learning and prompt tuning to solve the downstream tasks.
Some research shows that it has emergence and generalization capabilities [ 72]. Researchers
in software engineering have found that large models have good code generation and analysis
capabilities [10, 11, 76].
Feature extraction methods based on code models have a premise: the structural features of the
code are included in the feature space of the code model. The current mainstream code model train-
ing paradigms are based on masked language modelling (MLM), causal language modelling (CLM)
and other variants of the previous two. To put it simply, MLM means randomly masking code
tokens and then recovering them. The difference is that CLM predicts the next token based on
the above. These pretraining methods are not adapted for learning code features. In almost all the
cases, we covert the code into the text format required by them. The analysis and understanding of
the feature space of the code models is a challenge.
3.2 Probing Analysis Model
Probing analysis is a research method used for understanding and evaluating the knowledge and
information encoding in language models. This analytical method reveals a model’s mastery over
specific types of linguistic information during its learning process by designing and applying a
series of probing tasks. Probing tasks are usually simple, targeted tasks that are specifically designed
to test a model’s understanding of a particular linguistic attribute, such as grammatical structure,
word meaning, and sentence relations. In our study, for all designed probing tasks, we employ the
edge probing classifier [ 64] as depicted in Figure 4. Consistent with previous works [ 6,55,64,67] in
the probing literature, we keep the parameters of code pre-trained models fixed, which means they
will not be updated during training. For a given input code 𝑥, we use a tokenizer to tokenize 𝑥into
a token sequence with two special starting and ending tokens as denoted in the bottom of Figure 4.
2https://huggingface.co/learn/nlp-course/en/chapter6/5
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 8 ---
111:8 W. Ma, S. Liu and M. Zhao et al.
comvaraintc+bassignc>b=mulvaraintb*bifassignadd=returnareturnafuncargsintf(int b, int c){int a = b + c;if ( b > c ):{int a = b * b;return a;} return a;}ASTargbintargcint
Fig. 3. Syntax Pair Node Prediction.
𝐶𝐿𝑆𝑥!𝑥"𝑥"#!𝑥$𝑥$#!𝑥%𝐸𝑁𝐷………………𝐶𝐿𝑆𝑟!𝑟"𝑟"#!𝑟$𝑟$#!𝑟%𝐸𝑁𝐷………………𝑠!𝑠&ModelInput tokensContextual RepresentationAttention PoolingMLPSpan RepresentationProbing Classifier
Fig. 4. Analysis model.
Initially, we can obtain the contextual representation 𝑟𝑖of each hidden layer for each token 𝑥𝑖
from the code models. Subsequently, we extract the specific code token spans that are associated
with the graph or tree node we are interested in, and they are denoted as 𝑠1and𝑠2in Figure 4. One
code token span represents one piece of code from one node in AST or the dependency graph.
The length of one token span is various and can contain different numbers of tokens. Then, these
token spans are then passed through an attention pool, which maps them to a fixed-size vector.
The attention pool is an attention layer that can automatically assign different weights to each
token representation in the token span, and then aggregate them. Finally, the resulting vector
is forwarded to the probing classifier, which is implemented as a multi-layer perceptron (MLP)
classifier for classification. The MLP classifier is trainable and is denoted as the symbol 𝐶. The
fixed-size feature vectors of two token spans served as the input for 𝐶, and the classifier determined
whether the two token spans had a syntax or semantic relationship. We employed token spans
because code pieces that are related in terms of syntax or semantics are tokenized into two lists of
tokens with different lengths, respectively. These token lists are referred to as token spans, which
are then converted to a fixed-size vector by the attention pool.
3.3 Syntax Probing
To address potential inaccuracies resulting from transforming a high-dimensional space to a lower-
dimensional space, as well as the possible lack of relationship between distance in the AST and
syntax distance as indicated in the motivation, we introduce two syntax probing tasks: syntax-
node-pair prediction and token-syntax tagging. These tasks directly predict the attributes of the
nodes in the AST. The two tasks complement each other. The first task aims to predict the syntactic-
connectivity relationship between code tokens, while the second task focuses on examining the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 9 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:9
syntactic role of individual code tokens. Code tokens are the basic code words after the code is
tokenized and each node contains few code tokens. Through the first task, we can reconstruct AST
given the code token representation from the code models. If the code token representations do
not contain information about the AST structure, it is not possible to reconstruct the AST. Through
the second task, we can find if the code token representations contain the corresponding syntax
rules assigned by the programming language rules.
3.3.1 Syntax Node Pair Prediction. Given a source code, we can parse it to obtain AST and further
split this AST into different subtrees. Each subtree is one syntax expression from the original
code and we name it as one syntax unit. Because each syntax unit represents one complete syntax
expression from the code, hence the nodes in a unit are syntax-close. This task is designed to
predict any pair of nodes in AST belonging to a subtree. The AST is generated by the syntax parser
and contains all the syntax information of the code. For code models, they should retain such
a syntax structure in the vector space. If they cannot, it suggests that the model is not capable
of effectively encoding code syntax. Via this task, we aim to verify whether the code model can
capture and understand this syntax structure from the source code, and subsequently reconstruct
the syntax information in the vector space it represents. In particular, we present an example for
better illustration. As shown in Figure 3, there is a function whose corresponding AST is presented
on the right hand. For this AST, we split it into different units marked with different colors. For
example, the node of “=” in the red unit, should be syntax close to its left node in this unit i.e., the
node of “int” and “a”. Hence, they are labeled with 1as the positive samples. For the nodes of “int”
and “a” in another unit (marked with green), since they belong to another unit, they are labeled
with 0as the negative samples. Formally, this task can be formulated as follows:
𝐶(𝑛0,𝑛1)=(
1𝑛0∈𝑇𝑖∩𝑛1∈𝑇𝑖
0𝑛0∈𝑇𝑖∩𝑛1∈𝑇𝑗
where𝑇𝑖and𝑇𝑗(𝑖≠𝑗) are two different syntax units in AST and 𝑛0and𝑛1are the nodes in the unit.
We train a binary probing classifier 𝐶to learn whether any pair of nodes is syntax-close based on
the node representations computed from the token representation of code models by the attention
pool (Figure 4).
3.3.2 Token Syntax Tagging. The first task checks the syntactic similarity between different AST
nodes. Except for the syntax similarity, all tokens are assigned a syntax role by the programming
language rules. To check if code models can learn individual token syntax roles, we propose a
multi-class classification task, namely token syntax tagging, and this task requires more better
and fine-grained code representation to tag the syntax role of each code token. This is designed to
challenge and evaluate the depth of understanding of programming languages for code models.
A code model that can successfully perform token syntax tagging can be more useful in complex
coding applications, such as code generation.
As each AST node has its node type, a direct idea is to use the node type for tagging. However,
these syntax labels are highly abstractive, for example, the variable name is labeled with “identifier”
by the syntax rule, but it has different syntax meanings in the context. If it is in the function
declaration, it is actually an argument for this function; similarly, if it is in the class invocation, it is
a class attribute. Hence, we need to design concrete labels for these abstractive tokens. We initially
take into account the syntax type of the leaf nodes in the AST. Subsequently, we refer to the types
of their parent nodes to construct the concrete labels. Specifically, we design 36 tagging labels for
Java and 33 labels for C/C++. We filter the labels with low frequency ( 𝑓𝑟𝑒<200) for Java250 and
POJ-104. The labels we used are depicted in Table 1.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 10 ---
111:10 W. Ma, S. Liu and M. Zhao et al.
Table 1. The labels of Token Syntax Tagging for Java250 and POJ-104.
Java250
modifiers local_variable_declaration variable_declarator formal_parameters
array_type dimensions formal_parameter block
object_creation_expression argument_list field_access integral_type
method_invocation while_statement parenthesized_expression if_statement
expression_statement break_statement update_expression assignment_expression
identifier for_statement binary_expression return_statement
array_creation_expression dimensions_expr array_access ERROR
unary_expression throw_statement enhanced_for_statement ternary_expression
cast_expression generic_type type_arguments array_initializer
POJ-104
declaration array_declarator function_definition parameter_list
parameter_declaration compound_statement for_statement assignment_expression
binary_expression update_expression subscript_expression expression_statement
if_statement parenthesized_expression return_statement call_expression
argument_list string_literal pointer_expression init_declarator
function_declarator cast_expression type_descriptor break_statement
comma_expression initializer_list char_literal pointer_declarator
continue_statement while_statement field_expression sizeof_expression
case_statement
If-StatPEBEBEBEPEBFAFAMIALBEBEALESif(a<c){Systemoutprint(a*x);}BEB
Fig. 5. Token Syntax Tagging.
Table 2. The tagging labels for the tokens used in Figure 5.
Label Description
PE Parenthesized expression
BE Binary expression
B Block
FA Field access
MI Method invocation
AL Argument list
ES Expression statement
We present an example with the corresponding syntax label in Figure 5. The detailed information
of labels in this example is shown in Table 2. In particular, we can find that the tokens “(” and
“)” have different syntax labels in different contexts. The parenthesis “( )” is labeled with “PE” in
the if-condition, while the parenthesis “( )” from the method invocation is labeled with “AL”. We
design this task to explore whether these models can learn the code syntax properly from the
programming grammar.
3.4 Semantic Probing
The previous works mainly focused on analyzing the capacity of code pre-trained models in learning
code syntax. However, the analysis of code semantics is also essential. This group evaluates the
ability of the code model to understand the meaning and behaviour of code snippets. In this section,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 11 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:11
we propose two probing tasks to analyze the semantics of the learned code. The first is semantic
relation prediction for dependency graphs (CDG, DDG and CFG). Based on the prediction, we can
reconstruct the structure of dependency graphs. Dependency graphs depict complex relationships
like control and data dependencies, which are crucial for understanding how different parts of a
program interact. The second task is to predict the semantic propagation. Understanding control
and data flow is a more advanced aspect of semantic analysis, going beyond static analysis to how
the program would behave when run. This task assesses the ability of code models to understand
the dynamic nature of code execution.
3.4.1 Semantic Relation Prediction. Dependency graphs of a program can well represent code
semantics [ 20]. Similar to the task of syntax node pair prediction in Section 3.3.1, we also extract the
control dependency graph (CDG), data dependency graph (DDG) and control flow graph (CFG) to
predict whether the code models can learn code semantics. In the constructed Control Dependency
Graph (CDG), Data Dependency Graph (DDG), and Control Flow Graph (CFG), each node has an
attribute with a piece of code that can be tokenized into multiple tokens. We refer to these code
tokens derived from the same node as the token span . We unify these tasks into a meta task namely
semantic relation prediction. Formally, this task can be formulated as follows:
𝐶(𝑠0,𝑠1)=(
1∃𝑒∈𝐺, 𝑠 0∈𝑁𝑖∩𝑠1∈𝑁𝑗∩{𝑁𝑖𝑒− →𝑁𝑗}
0∀𝑒∈𝐺, 𝑠 0∈𝑁𝑖∩𝑠1∈𝑁𝑗∩{𝑁𝑖̸𝑒− →𝑁𝑗}
where𝑁𝑖,𝑁𝑗is the𝑖-th node and 𝑗-th node in the constructed graph 𝐺∈{𝐶𝐷𝐺,𝐷𝐷𝐺,𝐶𝐹𝐺}and
each node contains one token span from the original code. 𝑠0and𝑠1denote the corresponding
token spans of the original code in nodes 𝑁𝑖and𝑁𝑗after tokenization.𝑒− →means that there is an
edge between two nodes.
Figure 6a, Figure 6b and Figure 6c demonstrate examples for the three semantic relationships.
Figure 6a shows one control dependency example. The node 𝑁4is control dependent on the node
𝑁3. Based on this fact, we label that the token span from 𝑁3is control dependent on the token span
in𝑁4. Figure 6b illustrates one data dependency example. The node 𝑁4is data dependent on the
node𝑁1. Figure 6c shows one example of a control flow graph. We have some execution order facts
like that the node 𝑁2is executed after 𝑁1immediately. The token span of 𝑁2has a control-flow
relationship with the token span of 𝑁1.
3.4.2 Semantic Propagation Prediction. Data flow information is propagated in the dependency
graph. Tow nodes with the long distance may be data dependent implicitly. It is a fact that any
modification in the dependency graph potentially affects the program output. The implicit depen-
dency flow propagation is one import program semantics. The semantic propagation task (alias
inGraph ) is defined by that
𝐶(𝑠0,𝑠1)=(
1𝑠0∈𝑁𝑖∩𝑠1∈𝑁𝑗∩𝑁𝑖∈𝐺∩𝑁𝑗∈𝐺
0𝑠0∈𝑁𝑖∩𝑠1∈𝑁𝑗∩𝑁𝑖∈𝐺∩𝑁𝑗∉𝐺
where G∈{𝐶𝐷𝐺,𝐷𝐷𝐺},𝑠0and𝑠1are the token spans in the nodes of 𝐺. Figure 6d shows one
example. The shadow box highlights the statements with the control dependency relationship. We
expect the probing classifier can recognize that the “printf” statement is not in the dependency
control graph.
3.5 Attention Analysis
We conducted an analysis of the attention weights in each self-attention head, based on the program
semantic relationships 𝐺∈{𝐶𝐹𝐺,𝐷𝐷𝐺,𝐶𝐷𝐺}from Section 3.4.1. For a given node token span
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 12 ---
111:12 W. Ma, S. Liu and M. Zhao et al.
intsum ( inta ,intb ){a = a-1;  printf("addition");if(a>10)returna+b;returnb;}12340
5
(a) control dependency.
intsum ( inta ,intb ){a = a-1;  printf("addition");if(a>10)returna+b;returnb;}12340
5 (b) data dependency.
1254end3printf("addition"); if(a>10)returna+b;returnb;starta=a-1;
(c) control flow.
intsum ( inta ,intb ){a = a-1;  printf("addition");if(a>10)returna+b;returnb;}12340
5 (d) inGraph.
Fig. 6. Examples about semantic probing.
𝑠𝑖where𝑖is the𝑖-th node and a semantic relationship type 𝐺, we grouped the remaining input
tokens into two sets: 𝑅0and𝑅1.𝑅1which consists of all tokens that have the semantic relationship
with𝑠𝑖while the remaining tokens constitute the set 𝑅0. We then divided the attention weights of
one attention head related to 𝑠𝑖into two sets, denoted as 𝑊0and𝑊1, based on𝑅0and𝑅1. We check
the difference of the attention distributions between the two sets. Because we want to know which
set makes more contribution to the code representation, we compare the distribution centrality of
the two attention sets. If the sum of the attention weights in 𝑊1from a particular attention head is
greater than the sum of the attention weights in 𝑊0, it indicates that this attention head makes
more contribution to learning code semantics and we denote it as the semantic attention head.
To determine the statistical significance, we applied the paired t-test with a large sample size for
each semantic type 𝐺. We formulated the null hypothesis 𝐻0:𝜇𝑑=0, where𝜇𝑑represents the true
mean difference between 𝑊1and𝑊0. The alternative hypothesis 𝐻1:𝜇𝑑>0suggests that there is
a positive mean difference ( 𝑊1is greater than 𝑊0). By conducting this test, we can identify if the
attention head makes more contribution to learning code semantics or not. This head-level analysis
provides a more detailed understanding of self-attention. In total, we analyzed more than 10,000
semantic inputs for the 4 pre-trained code models and randomly selected 100 semantic inputs for
the three LLMs.
4 EVALUATION SETUP
In this section, we introduce the evaluation setup, including data pre-processing, evaluation models,
and evaluation metrics.
4.1 Dataset and Pre-processing
We employ two datasets, namely Java250 [ 56] and POJ-104 [ 53] for our study. The quality of the
data has a significant impact on the probing analysis. We follow the pre-processing steps outlined
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 13 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:13
Raw codeRefactorRefactored CodeAST ParserStatic AnalysisExtract ASTProgram GraphsSyntax DataSemantic DataToken Alignment
Fig. 7. The workflow of data pre-processing.
in Figure 7 to generate high-quality syntax and semantic probing data. The code is refactored
using the google-java-format tool3and the clang-refactor tool4. Refactoring the code ensures its
readability and facilitates token alignment between the graphs and the model input. To ensure the
accuracy of the results, we utilize the Joern static analysis framework and the AST parser5to extract
the program graphs and AST. In the case of program graphs, we merge redundant nodes if the code
of one node is a subset of its neighbors. We then extract the syntax and semantic relationships
among the code tokens and construct the probing datasets.
4.2 Experimental Models
We chose 7 popular code models with diverse architectures (transformer encoder, transformer de-
coder, and transformer encoder-decoder) that are widely used in software engineering. The 7 models
were built by reputable companies or organizations: Microsoft, Meta, Salesforce, and Huggingface.
All of them have a large number of downloads on Huggingface, and their corresponding papers
each have more than 150 citations. First, we conduct experiments using four pre-trained models,
CodeBERT (CB) [ 13], GraphCodeBERT (GCB) [ 17], UnixCoder (UC) [ 16] and CodeT5 (CT) [ 71].
CodeBERT and GraphCodeBERT utilize the Transformer encoder, while GraphCodeBERT incor-
porates additional data flow information. UnixCoder utilizes mask attention matrices with prefix
adapters to support encoder, decoder, and encoder-decoder learning. CodeT5 employs the encoder-
decoder Transformer architecture. For CodeT5, we consider its encoder, which can be compared
with other pre-trained models in the same architecture. These models have 12 Transformer encoder
layers (denoted as Layer 1-Layer 12) plus one embedding layer (denoted as Layer 0). We apply our
probing tasks to each encoder layer and the embedding layer. This helps us to understand the role
of each layer. Second, we also study 3 large language models (LLMs), CodeLlama-7b [ 58] (CL),
StarCoder [ 34] (SC) and CodeT5+ [ 70] (CodeT5p-770m) (CT5P+) using the Java dataset. CodeLlama-
7b has 32 hidden layers and StarCoder has 40 hidden layers. Both of them use the transformer
decoder. CodeT5p-770m has 24 hidden encoder layers and 24 hidden decoder layers. It is an open
code large language model based on the encoder-decoder structure for the CodeT5+ family [ 70].
All experiments are repeated 3 times with different random seeds.
4.3 Evaluation Metrics
We utilize Matthew’s correlation coefficient (MCC) [ 50] as our evaluation metric. MCC is one
reliable alternative to F1-score for binary classification [ 5,74], and it considers the whole confusion
3https://github.com/google/google-java-format.git
4https://clang.llvm.org/docs/ClangTools.html
5https://joern.io/,https://tree-sitter.github.io/tree-sitter/
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 14 ---
111:14 W. Ma, S. Liu and M. Zhao et al.
matrix. MCC can be computed from the confusion matrix,
𝑀𝐶𝐶=𝑇𝑃×𝑇𝑁−𝐹𝑃×𝐹𝑁√︁
(𝑇𝑃+𝐹𝑃)·(𝑇𝑃+𝐹𝑁)·(𝑇𝑁+𝐹𝑃)·(𝑇𝑁+𝐹𝑁)
, where TP is true positive, TN is true negative, FP is false positive and FN is false negative. For
token syntax tagging, it is a multiple classifications and we also show its macro F1 score,
Macro F1 =1
𝑁𝑁∑︁
𝑖=12×Precision 𝑖×Recall𝑖
Precision 𝑖+Recall𝑖
, where N is the number of classes and i is the 𝑖𝑡ℎclass. We selected the macro F1 score as our
evaluation metric due to its inherent property of assigning equal significance to every class within
our dataset. As all experiments are repeated by 3 times, we use the mean values for evaluation.
5 EXPERIMENTAL RESULTS
In this section, we present the results of analyzing code syntax and semantics.
5.1 Syntax Analysis
5.1.1 Syntax Pair Node Prediction. Figure 8a and Figure 8b present the results of the probing
classifiers for syntax pair node prediction on Java250 and POJ-104 respectively, for pre-trained code
models, CodeBert (CB), GraphCodeBert (GCB), UnixCoder (UC) and CodeT5 (CT5). These results
are derived from the hidden representations provided by the code models. The X-axis denotes the
layer index, while the Y-axis signifies the performance. 𝐿𝑎𝑦𝑒𝑟 0corresponds to the embedding layer,
while𝐿𝑎𝑦𝑒𝑟 1-𝐿𝑎𝑦𝑒𝑟 12represent the Transformer layers. We can find that overall these models are
able to accurately depict the syntactic relations among tokens. For example, the MCC gets over 70%
in different layers across different models. All pre-trained models achieve their best performance
between the shallow 𝐿𝑎𝑦𝑒𝑟 2and𝐿𝑎𝑦𝑒𝑟 4. With the depth of the layers, the performance decreases
gradually. To be more specific, GraphCodeBERT (GCB) exhibits slightly better performance than
the other models on Java250, as evidenced in Figure 8a. However, when it turns to POJ-104, the
phenomenon is more complex. CodeT5 demonstrates superior performance compared with other
models from 𝐿𝑎𝑦𝑒𝑟 4to𝐿𝑎𝑦𝑒𝑟 8, however, when the layer reaches 12, GraphCodeBERT achieves the
best performance. Despite UnixCoder and CodeT5 employing more intricate pre-training strategies,
leveraging larger datasets, and having a greater number of trainable parameters, their relative
advantage is minimal. Figure 8c shows MCC scores for CodeLlama, StarCoder and CodeT5+. An
interesting observation is that the three large language models achieve the best performance in
the shallow layers 𝐿𝑎𝑦𝑒𝑟 4and𝐿𝑎𝑦𝑒𝑟 5, which is similar to the code pre-trained models. But for
StarCoder and CodeLlama, the performance increased a lot in the last layer which is different from
others. We can conclude that the syntax relationship between code tokens is easier to observe in
the representation spaces of the shallow hidden layers. The decrease in performance of deeper
representations on this task is an interesting phenomenon. Although deep hidden layers are
supposed to contain information from shallower layers, their representation space is more complex,
encoding more other code attributes like the token-syntax role, so the syntax-token relationship
may be less salient.
5.1.2 Token Syntax Tagging. Figure 9a and Figure 9b present the MCC results of the probing
classifiers for the individual token-syntax role (Token Syntax Tagging) on Java250 and POJ-104
respectively. Figure 9c and Figure 9d demonstrates the F1 scores. Our observations indicate that
CodeT5 outperforms the other models in the middle layers, specifically from 𝐿𝑎𝑦𝑒𝑟 4to𝐿𝑎𝑦𝑒𝑟 11.
CodeBERT exhibits marginally better performance than GraphCodeBERT in these middle layers,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 15 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:15
while GraphCodeBERT surpasses the other models at the final layer, i.e., 𝐿𝑎𝑦𝑒𝑟 12. Interestingly,
a performance drop is observed for all models except UnixCoder transitioning from 𝐿𝑎𝑦𝑒𝑟 11to
𝐿𝑎𝑦𝑒𝑟 12. CodeT5 achieves peak performance approximately around 𝐿𝑎𝑦𝑒𝑟 10. Conversely, UnixCoder
lags behind other models, particularly after 𝐿𝑎𝑦𝑒𝑟 7. Except for UnixCoder, the performance of other
pre-trained models shows an increasing trend with fluctuations from 𝐿𝑎𝑦𝑒𝑟 0to𝐿𝑎𝑦𝑒𝑟 11. Figure 10a
and Figure 10b shows the F1 score and MCC score for LLMs on Java250 dataset. If we look at all the
figures in this task, we find a general trend that the performance improves as the depth of layers
increases, and the latter is stable with some drops or ups. Interestingly, at the intersection between
the encoder and decoder, the model performance dropped sharply but quickly improved again as
the number of layers increased. This phenomenon has also been observed in other experiments.
One reasonable explanation is that the encoder and the decoder work with different mechanisms.
The encoder encodes information into an abstract representation while the decoder tries to recover
the input from the abstract representation. Due to the working difference, the properties in the
representation from the interaction layers are challenging to observe when migrating from the
encoder to the decoder. As aforementioned, token syntax tagging is a non-trivial task to label the
code token with a syntax role, and it is a more difficult task than the previous one. The token syntax
role property is more obviously represented in the deeper than the shallow layers.
Regarding the syntax-relative relationship, these pre-trained code models demonstrate compara-
ble proficiency. However, CodeT5 (we consider its encoder) distinguishes itself by more effectively
representing the individual syntax-token role within context. Conversely, UnixCoder exhibits a
lower degree of syntactic understanding compared to the other models. We speculate this for the
following two reasons: 1) CodeBERT, GraphCodeBERT, and CodeT5-encoder all employ masked
language modeling (MLM) or similar ways to learn bi-directional token features while UniXCoder
incorporates different types of pre-training tasks. Although it also used MLM, the other tasks
such as unidirectional language modeling (ULM) and DeNoiSing (DNS) may have a detrimental
impact on probing tasks; 2) UnixCoder integrates the encoder and the decoder by sharing weights
while the representations from the decoder have a worse ability in information extraction than
the encoder representation [ 66]. Sharing weights between the encoder and decoder may harm the
ability of the model to extract information. The three studied LLMs show some differences from
the four pre-trained models. LLMs do not have obvious advantages over the pre-trained models,
although StarCoder has 6144 hidden dimensions, CodeLlama has 4096 hidden dimensions, and
CodeT5p-770m has 1046 hidden dimensions. Their dimensions are much higher than the pre-trained
code models (768). All of them have a huge number of parameters. One possible reason is that
the representation space dimension is too large and covers too much information, making the
embedded syntax features difficult to observe.
In summary, from Section 5.1.1 and Section 5.1.2, it is evident that the four code pre-trained
models and 3 large language models can encode the code syntax, both in terms of syntax relationship
between tokens (Syntax Pair Node prediction) and the individual token-syntax role (Token Syntax
Tagging). Different syntax features have different degrees of difficulty in being observed at shallow
and deep hidden layers.
5.2 Semantic Analysis
5.2.1 Semantic Relation Prediction. Figure 11 presents the probing performance of the four pre-
trained code models, CodeBERT (CB), GraphCodeBERT (GCB), UnixCoder (UC), and CodeT5 (CT5)
on the semantic relation prediction task. We conducted a comparison of the probing performance
across three different program semantics: control dependency (CDG, Figure 11a and Figure 11b),
control flow information (CFG, Figure 11c and Figure 11d), and data dependency (DDG, Figure 11e
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 16 ---
111:16 W. Ma, S. Liu and M. Zhao et al.
0123456789101112
Layer708090 MCC, %
CB
GCBUC
CT5
(a) Java250-AST.
0123456789101112
Layer808590 MCC, %
CB
GCBUC
CT5 (b) POJ-104-AST.
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer7075808590 MCC, %
CL
SCCT5+
(c) Java250-AST of LLM.
Fig. 8. Performance (MCC) about Java250 and POJ-104 for Syntax Pair Node prediction (AST).
0123456789101112
Layer6080 MCC, %
CB
GCBUC
CT5
(a) Java250-Tagging (MCC).
0123456789101112
Layer6080MCC, %
CB
GCBUC
CT5 (b) POJ-104-Tagging (MCC).
0123456789101112
Layer204060 F1, %
CB
GCB
UC
CT5
(c) Java250-Tagging (F1).
0123456789101112
Layer204060 F1, %
CB
GCB
UC
CT5 (d) POJ-104-Tagging (F1).
Fig. 9. Performance about Java250 and POJ-104 for Token Syntax Tagging.
and Figure 11f). Firstly, it is evident that CodeT5 demonstrates the highest proficiency in understand-
ing program semantics, i.e., CDG and DDG, especially in the last few layers, and GraphCodeBERT
proves to be a little better than CodeT5 in terms of CFG. Secondly, CodeBERT is also capable of
encoding program semantics, despite not utilizing data flow information during pre-training. For
example, the MCC reaches over 60% in CDG and DDG. It indicates that the pre-training task Masked
Language Modeling (MLM) is able to aid the model in learning code semantics. UnixCoder performs
less effectively in representing code semantics compared to the other three models, particularly
after𝐿𝑎𝑦𝑒𝑟 7.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 17 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:17
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer1020 F1, %
CL
SCCT5+
(a) Java250-Tagging LLM (F1).
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer15202530 MCC, %
CL
SCCT5+ (b) Java250-Tagging LLM (MCC).
Fig. 10. Performance about Java250 for Syntax Pair Node prediction (AST) and Token Syntax Tagging.
Figure 12 demonstrates the probing performance of three LLMs, CodeLlama, StarCoder and
CodeT5+, on the three code semantics understanding tasks, CDG, CFG and DDG. Compared with
the code pre-trained model, the representation from the shallow layer of LLM makes it easier to
observe the three semantic information of the code. When we compared StarCoder with CodeLlama
and CodeT5+, we found that the performance of StarCoder dropped significantly for the deep
layers. Since the current LLMs have emergency ability, we can think LLMs have learned the three
code semantics while they are not easily observed. Furthermore, how to induce the code semantic
representation of LLM to be more easily observed in prompting LLM, thereby improving the quality
of model answers, is a topic worthy of study in the future.
When comparing the performance of the code models across CDG, CFG, and DDG, it is clear that
the code models exhibit the lowest MCCs on CFG, higher MCCs on DDG, and the highest MCCs
on CDG, respectively. It indicates that the code models struggle to capture control flow semantics
(CFG) as effectively as the other two semantic types. CFG is an approximation execution trace of
the program. LLMs are shown to lack the ability to handle the tasks related to code execution [ 46].
Model trainers need to consider how to improve the understanding of code dynamic behaviour of
LLMs.
5.2.2 Semantic Propagation (inGraph). Figure 13 illustrates the probing results of four pre-trained
models on the inGraph task. It is evident that GraphCodeBERT outperforms the other models in
terms of semantic propagation for CDG and DDG on the Java250 dataset. CodeT5 is better on
POJ-104 about CDG. However, both GraphCodeBERT and CodeT5 exhibit similar performance
on POJ-104 about DDG. This suggests that the data propagation employed by GraphCodeBERT
proves beneficial in capturing the semantic propagation within the code. Furthermore, we observe
that CodeBERT can encode code semantics, albeit with lower performance compared to CodeT5
and GraphCodeBERT. However, UnixCoder performs worse than the other models, especially
after𝐿𝑎𝑦𝑒𝑟 7. When comparing the performance of the models between CDG and DDG, we can
find that models exhibit higher MCCs on DDG than CDG, which means that data dependency
propagation is encoded better than control dependency propagation by these 4 pre-trained code
models. For the LLMs, we observe that the shallow layers have better performance than the deep
layers. For CodeT5+, this phenomenon is even more apparent and has a sharp drop between the
encoder and the decoder ( 𝐿𝑎𝑦𝑒𝑟 24and𝐿𝑎𝑦𝑒𝑟 25). One possible explanation is that the ability of code
representations for the decoder to express long dependencies is diminished because the decoder can
only see the previous token information, and the future tokens are masked. If we see Figure 12 for
the short dependency (constructing the semantic graphs), we can find that there is a performance
drop between the encoder and the decoder ( 𝐿𝑎𝑦𝑒𝑟 24and𝐿𝑎𝑦𝑒𝑟 25) of CodeT5+ and the drops gets
more evident for the long dependency.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 18 ---
111:18 W. Ma, S. Liu and M. Zhao et al.
0123456789101112
Layer7580 MCC, %
CB
GCBUC
CT5
(a) Java250-CDG.
0123456789101112
Layer7580 MCC, %
CB
GCBUC
CT5 (b) POJ-104-CDG.
0123456789101112
Layer354045 MCC, %
CB
GCBUC
CT5
(c) Java250-CFG.
0123456789101112
Layer253035 MCC, %
CB
GCBUC
CT5 (d) POJ-104-CFG.
0123456789101112
Layer646668 MCC, %
CB
GCBUC
CT5
(e) Java250-DDG.
0123456789101112
Layer6062 MCC, %
CB
GCBUC
CT5 (f) POJ-104-DDG.
Fig. 11. Performance (MCC) about Java250 and POJ-104 for Semantic Relation.
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer758085 MCC, %
CL
SCCT5+
(a) Java250-CDG.
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer304050 MCC, %
CL
SCCT5+ (b) Java250-CFG.
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer55606570 MCC, %
CL
SCCT5+
(c) Java250-DDG.
Fig. 12. LLM Performance (MCC) about Java250 for Semantic Relation.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 19 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:19
0123456789101112
Layer8590 MCC, %
CB
GCBUC
CT5
(a) Java250-CDG.
0123456789101112
Layer889092 MCC, %
CB
GCBUC
CT5 (b) POJ-104-CDG.
0123456789101112
Layer859095 MCC, %
CB
GCBUC
CT5
(c) Java250-DDG.
0123456789101112
Layer87.590.092.595.0 MCC, %
CB
GCBUC
CT5 (d) POJ-104-DDG.
Fig. 13. Performance (MCC) about Java250 and POJ-104 for inGraph.
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer82.585.087.590.0 MCC, %
CL
SCCT5+
(a) Java250-CDG.
0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748
Layer90929496 MCC, %
CL
SCCT5+ (b) Java250-DDG.
Fig. 14. LLM Performance (MCC) about Java250 and POJ-104 for inGraph.
Table 3. Number of Semantic Attention Heads on Java250 / POJ-104.
CodeBERT GraphCodeBERT UnixCoder CodeT5
CDG 65 / 93 71 / 86 50 / 44 77 / 91
CFG 129 / 141 136 / 139 99 / 82 130 / 128
DDG 133 / 115 135 / 124 23 / 58 111 / 120
To summarize the findings from Section 5.2.1 and Section 5.2.2, all four pre-trained code models
possess the capability to learn code semantics. However, their abilities to encode semantics vary
across different types of semantics. For the LLM group, the representation from the shallow
layers makes more easier to observe the code semantics. The ability to encode the code execution
information should be enhanced for LLMs.
5.3 Attention Analysis
Based on the analyses mentioned above, we have observed that pre-trained code models and large
language models (LLM) demonstrate good or middle-level proficiency in learning code syntax
and semantics. To gain further insights into how these models encode semantics, we conducted
an investigation into the roles of self-attention heads in learning code semantics, utilizing a
dataset of over 10,000 semantic inputs for pre-trained code models and randomly sampling 100
semantic inputs for LLMs. Every one of the pre-trained models has 144 attention heads in total,
respectively. StarCoder has 1920 attention heads, CodeLlama has 960 attention heads and CodeT5+
Decoder/Encoder has 384 attention heads.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 20 ---
111:20 W. Ma, S. Liu and M. Zhao et al.
Table 4. Number of Semantic Attention Heads of LLMs on Java250 / POJ-104.
StarCoder CodeLlama CodeT5+ Decoder CodeT5+ Encoder
CDG 0/0 0/0 0/0 199/252
CFG 1372/1844 423/830 371/340 315/319
DDG 1253/1892 100/977 318/146 247/208
In Table 3 and Table 4, we present the number of attention heads that significantly learn code
semantics in terms of CDG, CFG, and DDG (statistically significant at a p-value threshold of 0.01) for
the pre-trained models and LLMs about the Java250 and POJ-104 datasets, respectively. In Table 3,
it is evident that UnixCoder has the fewest number of attention heads dedicated to semantics. This
is consistent with the previous conclusion that UnixCoder is not better than others in terms of
encoding code syntax and semantics. In Table 4, we can find that attention heads that have the
control dependency relationship do not assign more weights than the attention heads that have no
control dependency relationship for the StarCoder, CodeLlama and CodeT5+ decoder who use the
transformer decoder architecture. However, for CodeT5+ encoder, there exist 199/252 attention
heads that can assign more weights for the control dependency relationship. We think the reason is
that different model architectures may affect the distribution of attention mechanisms. Encoder and
decoder in the transformer have different functions. Encoder is more inclined to understand the
overall structure and context of the input, while the decoder focuses on generating or predicting the
next token based on this understanding. Therefore, for control-dependent statements, the encoder
may need to pay more attention to these relationships to understand the logical structure of the
entire code, while the decoder may pay more attention to local information or other semantic
information for generation, resulting in attention weights of different distributions.
We further investigate the overlap of semantic attention heads that assign more attention weights
to semantic tokens between the Java250 and POJ-104 datasets for these four models. The results
reveal that one model shares semantic attention heads across different programming languages.
To examine the disparities in semantic attention heads between the Java250 and POJ-104 datasets
across four employed models, we present the ratios of the overlapping semantic attention heads
for each model in both datasets (Table 5 and Table 6). The rows Java250 and POJ-104 are computed
by the following two equations respectively,
𝑟𝑗𝑎𝑣𝑎 250=|𝑆𝑗𝑎𝑣𝑎 250∩𝑆𝑃𝑂𝐽|
|𝑆𝑗𝑎𝑣𝑎 250|
𝑟𝑝𝑜𝑗=|𝑆𝑗𝑎𝑣𝑎 250∩𝑆𝑃𝑂𝐽|
|𝑆𝑃𝑂𝐽|
, where𝑆𝑗𝑎𝑣𝑎 250and𝑆𝑃𝑂𝐽are the set of semantic attention heads for Java250 and POJ-104 datasets.
Surprisingly, despite the distinct programming languages represented in the two datasets, a sub-
stantial overlap is observed. Although the overlapping ratio 𝑟𝑝𝑜𝑗for DDG in CodeLlama is 8.29%, it
is caused that 𝑆𝑃𝑂𝐽is too large and actually majority of attention heads from Java250 are included
in𝑆𝑃𝑂𝐽(81.00%). This finding suggests that certain semantic attention patterns are shared among
code models regardless of the programming language being used.
6 RELATED WORK
6.1 Code Models
Pre-trained models have been used to support many tasks due to their excellent generalization
ability in natural language processing tasks. Recently, researchers pre-train transformers [ 54]
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 21 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:21
Table 5. Overlapping of Semantic Attention Heads on Java250 and POJ-104, Percent %.
CodeBERT GraphCodeBERT UnixCoder CodeT5
CDG CFG DDG CDG CFG DDG CDG CFG DDG CDG CFG DDG
𝑟𝐽𝑎𝑣𝑎 250 96.92 100.0 85.71 94.37 97.06 91.11 84.62 97.87 95.50 84.42 96.92 93.69
𝑟𝑝𝑜𝑗 67.74 91.49 99.13 77.91 94.96 99.19 51.16 70.23 76.81 71.43 98.44 86.67
Table 6. Overlapping of LLM Semantic Attention Heads on Java250 and POJ-104, Percent %.
StarCoder CodeLlama CodeT5+ Decoder CodeT5+ Encoder
CDG CFG DDG CDG CFG DDG CDG CFG DDG CDG CFG DDG
𝑟𝐽𝑎𝑣𝑎 250 - 99.05 98.24 - 95.98 81.00 - 90.56 40.25 98.49 95.87 74.09
𝑟𝑝𝑜𝑗 - 73.69 65.06 - 48.92 8.29 - 98.82 87.67 77.78 94.67 87.98
using code data to solve programming tasks. According to the pre-training strategies and model
architectures, we can group the pre-trained models into three (3) types: auto-encoding models,
auto-regressive models, and sequence-to-sequence (Seq2Seq) models. Auto-encoding models utilize
Transformer encoders and are pre-trained with objectives such as Masked Language Modelling
(MLM). MLM masks some tokens in the code sequence and expects the model to predict the masked
tokens using bidirectional context information, which in fact enables the model to use future tokens
to predict current mask tokens. CodeBERT [ 13] is pre-trained on CodeSearchNet dataset [ 26].
GraphCodeBERT [ 17] includes one additional input type, data flow sequence, compared with
CodeBERT. CodeBERT and GraphCodeBERT use the encoder of Transformer. Auto-regressive
models use Causal Language Modelling (CLM) or its variants to pre-train the transformers in
a left-to-right manner. CodeGPT [ 42] uses this pre-training strategy and keeps the transformer
decoder. Seq2Seq models, e.g., CodeT5 [ 71], use both an encoder and a decoder in the Transformer.
CommitBART [40] uses BART [32] architecture to pre-train a model for GitHub commits.
Recently, ChatGPT and other large language models have gained significant attention. As a result,
several LLMs specifically designed for coding have emerged. StarCoder [ 34] with 15B parameters
trained using 1 trillion tokens is released. CodeLlama [ 58] is tuned based on Llama2 [ 65] using the
code dataset. WizardCoder [ 43] uses the code evolutionary instruction to tune the mode weights.
All of them are based on the transformer decoder. Differently from them, CodeT5+ [ 70] uses the
encoder-decoder architecture like CodeT5 [ 71]. LLMs show powerful ability in program repair,
code generation and summarization. Hou et al . [21] and Zhang et al . [78] comprehensively reviews
how large the applications of language models in software engineering.
These code models are widely used to solve various software engineering tasks [ 21,48,62,78],
such as defect detection, code summarization, vulnerability repair, and bug localization. Before
the emergence of Language Model Learners (LLMs), these deep learning models were used in two
ways in software engineering [ 35,73]: 1) Adding a task model on top of these code models and
fine-tuning the weights of the entire model. 2) These models are used as feature extractors, and
other algorithms are applied after extracting features. Compared to the first method, the second
does not require fine-tuning the weights of the model. After the emergence of LLMs, learning
methods based on in-context learning [ 7] are gradually accepted and used. These models are usually
not used alone but as a step in the workflow. Moreover, researchers will use domain knowledge to
determine how to use the code model for specific scenarios. Despite the many methods for various
software engineering tasks based on code models available today and researchers trying to explain
each step of their methods as much as possible, the use of the code model itself as a black box might
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 22 ---
111:22 W. Ma, S. Liu and M. Zhao et al.
discount these explanations to some extent. Our work aims to help software engineers understand
how models understand code syntax and semantics as much as possible.
6.2 Probing Analysis for Code Models
The impressive performance of pre-trained models stimulates loads of work trying to interpret
and understand these newly invented large-scale black box models. These analysis works can help
users understand and apply pre-trained models. Probing [6,57,79] is one of the most prominent
techniques widely leveraged for interpretability. Probing analysis aims at diagnosing which types
of regularities are encoded in a representation extracted from data. The basis of probing is that if a
simple classifier, e.g., a linear classifier, built upon the representations can solve a task sufficiently
well, then the representations should contain informative features about the task already.
Recent works strive to analyze code pre-trained models via probing. Wan et al . [69] evaluated if
pre-trained models learn programming language syntax, and they measured the number of edges
among node tokens at AST, and tried to learn this distance in the vector space. This approach cannot
recover the AST structure, given the distances among all nodes. Although the number of edges
between nodes can reflect the syntax information to some degree, it still has some problems. First, it
cannot reconstruct AST structures in the vector space, which means that it partially checks the code
syntax. Second, two tokens with similar syntax have a small number of edges, but the small number
of edges does not imply syntax closeness as shown in the motivation section. Hernández López
et al. [18] analyzed pre-trained models in a global-level AST by projecting AST into a subspace.
This work converts AST to a middle-format binary tree and then learns a subspace utilizing the
syntax distance [ 60] that is designed for natural language. However, the syntax distance for natural
language may not be suitable for code data since Allamanis et al . [3] list the difference between
the code and the natural language, including the differences in syntax tree between the natural
language and the code. In contrast, our approach is concise and efficient, and we directly recover
AST structures from the vector space. In addition, we conduct the semantic analysis for the code.
Troshin and Chirkova [66] developed a group of probing tasks to check if pre-trained models
learn the code syntax structure and data flow information. First, this work does not consider the
whole structure of AST and considers partially code syntax. Second, this work does not consider
control-flow and control-dependency semantics. Shen et al . [59] extract the syntax sub-structure
and predict their syntax relation. However, they lack semantic analysis and only include four syntax
types: assignment, function calling, if statements, and loop structures. The latest work [ 45] observes
syntax and semantic relationships and also studies how LLMs understand code behaviour based on
the generated outputs via in context learning for code analysis. Our work uses the inner states of
code models and comprehensively studies how code models encode code syntax and semantics by
targeting the whole structure of AST, control and data dependency graphs, and the control-flow
graph.
6.3 Deep Learning Testing
Deep learning models are treated as a system. Although we can know the weights of the model and
its internal calculation method, we do not know its intrinsic logic. Hu et al . [23] comprehensively
study deep learning testing. Testing such a blackbox system is quite challenging. Early software
engineering researchers mainly focused on detecting defects in deep learning systems [ 47]. For
example, DeepMutation [ 44] and DeepGini [ 12] are all used to find defects in deep learning systems.
These defects are usually defined as inputs that cause the system to produce incorrect outputs.
Once the system defects are discovered, researchers try to repair these discovered defects. Also,
in situations where multiple models are used for the same task, researchers have proposed ways
to select high-quality models with few defects [ 24,51,61]. The models being tested are usually
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 23 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:23
models that solve specific tasks in a specific scenario. Besides, some research focuses on testing
the model performance in unknown scenarios. For example, ATC (Leveraging Unlabeled Data to
Predict Out-of-Distribution Performance) [ 15], Aries [ 25], and OODRobustBench [ 33] all study the
performance of models on out-of-distribution data. It is important to emphasize that our work is
not to detect defects in the model; we aim to explain the understanding of the code syntax and
semantics for code models. Probing analysis and testing are different. Probing analysis focuses on
understanding the internal mechanisms and knowledge representation of models while testing
code models aims to evaluate the actual performance of models and identify potential defects. Both
are indispensable steps in the development and evaluation process of code models, but they focus
on different aspects.
7 CONCLUSION AND DISCUSSION
7.1 Conclusion
In this work, we aim to gain a deeper understanding of how code models handle and comprehend
complex code syntax and semantic structures. Specifically, we investigate whether these code
models can accurately capture the syntax trees (AST), control dependencies (CDG), control flow
(CFG), and data dependencies (DDG) in code by reconstructing these syntax and semantic structures
in vector space. These are all fundamental aspects of program understanding. We design a series of
probing tasks to evaluate the ability of code models to handle code syntax and semantics. We explore
four popular pre-trained code models: CodeBERT, GraphCodeBERT, UnixCoder, and CodeT5, and
introduce three large-scale language models (LLMs): CodeLlama, StarCoder, and CodeT5+, to assess
their performance in understanding code syntax and semantics. Additionally, we indirectly observe
how the models handle dependencies in code through attention analysis. From our experiments,
we obtain several interesting findings. Pre-trained code models and LLMs are able to express
code syntax quite well and capture code semantics to some extent, especially in handling data
dependencies and control dependencies. By comparing the performance of different models, we
observe that specific models have different advantages in probing tasks, indicating the varied effects
of model strategies and training datasets on the understanding of the models in code syntax and
semantics. Though LLMs demonstrate the ability to learn code syntax and semantics, in-depth
performance analysis reveals differences in the presentation of this understanding in the different
hidden layers, suggesting the possibility of optimizing pre-training strategies to improve deep
semantic processing. When we employ the SOTA code models like CodeLlama to solve software
engineering tasks, we still need to carefully design downstream-task models because the code
features extracted by large models are hidden deeply and are not easily observed.
7.2 Discussion
Our research provides valuable insights into understanding and improving the capabilities of code
models and raises challenging questions and opportunities for future research directions. Based
on our findings, we believe that future research should focus on several key areas: 1). Further
optimizing the architecture and training strategies of code models to enhance their ability to
understand complex code semantics. This could involve exploring new neural network architectures,
training objectives, and data augmentation techniques. One possible solution is to use the graph
transformer [ 37,77] to directly learn the syntax and semantic structures of the code. Ma et al . [49]
have demonstrated that the graph neural network can perform as well as the transformer but with
fewer parameters. The graph transformer combines the advantages of the graph neural network
and the transformer: encoding graph structure and learning from a large corpus. 2). Exploring
new probing tasks and evaluation methods to further unveil the inner workings of code models,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 24 ---
111:24 W. Ma, S. Liu and M. Zhao et al.
especially the ability of decoder-based models to understand code. This could involve developing
new metrics for measuring code understanding, as well as designing adversarial examples to test the
robustness of code models. 3). Investigating the relationship between the robustness of code models
and their ability to understand code semantics. This could involve studying how different types
of noise and adversarial attacks affect the performance of code models and developing methods
to improve their robustness while maintaining their ability to understand code. Looking ahead,
code models are increasingly being used for software development. Our work has highlighted that
code models still have significant shortcomings in understanding code semantics. However, code
semantics is closely related to code security. Therefore, it is crucial to perform security checks on
code generated by code models.
8 THREATS TO VALIDITY
Firstly, the results of the detection analysis are influenced by the random seed and the dataset
used. There is a certain level of randomness that may affect the performance of the final detection
classifier. To mitigate the impact of randomness, we conducted multiple experiments and performed
statistical analyses of the results.
Secondly, we used static analysis tools to build a high-quality dataset for analysis. Although this
approach provides valuable information, it also introduces certain biases. Static analysis tools may
not fully capture subtle variations in dynamic semantics and may be influenced by the syntax
rules of specific programming languages. To reduce this bias, we choose the tools that are used
by many researchers to avoid mistakes in the data. In the process of cross-task comparisons, we
extracted syntax and semantic tasks from the same data source to minimize the bias in cross-task
comparisons.
Third, different performance metrics may exhibit different biases. To evaluate the models more
accurately, we used the Matthews correlation coefficient (MCC) for binary classification and the F1
score for multi-class classification. In fact, we evaluated the models using both MCC and F1 for all
tasks, and the conclusions drawn from both metrics were consistent.
Fourth, although the probing analysis can reveal the learning capability of code models in terms
of syntax and semantics, it does not guarantee good downstream task performance in practical
applications. Code models may perform well in probing tasks but poorly in actual applications,
especially tasks requiring complex reasoning or multimodal interactions. The performance of
downstream task models is related not only to the expressive power of the base model but also to
the quality of the downstream task dataset and the robustness of the model.
Fifth, probing analysis is to observe if the representation contains one specific property that exists
in the input. However, the poor performance of the probing analysis does not mean that the
representation does not contain such information. The representation space is a high-dimension
space and mixes all learned things together. Some information is more observable and is easy to
probe. Our probing approach can only be used to answer if this property is more observable or less.
It is limited to answer ‘yes’ or ‘no’.
Last but not least, many code models have been proposed. To ensure that our evaluation approach
and conclusion are generalized, we consider three different architectures of code models: encoder,
decoder and encoder-decoder. We include the traditional pre-trained models and large language
models from large companies or organizations, and all of these models are widely used as the based
models to solve the downstream tasks. When we design our approach, we do not consider any
specific model and programming language, and we only consider the syntax and semantics of code.
AST represents all syntax information of code. CFG represents the logic/action of the code. DDG
and CDG represent the semantic relationship between different code parts. Almost all SE tasks
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 25 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:25
related to the code need this information. However, different models have different abilities, and our
conclusion may not be suitable for one model with very different training strategies and datasets.
ACKNOWLEDGMENTS
This research is supported by the National Research Foundation, Singapore, and the Cyber Security
Agency under its National Cybersecurity R&D Programme (NCRP25-P04-TAICeN), the National
Research Foundation, Singapore, and DSO National Laboratories under the AI Singapore Programme
(AISG Award No: AISG2-GC-2023-008), and NRF Investigatorship NRF-NRFI06-2020-0001. Any
opinions, findings and conclusions or recommendations expressed in this material are those of
the author(s) and do not reflect the views of National Research Foundation, Singapore and Cyber
Security Agency of Singapore.
REFERENCES
[1]Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. 2001. On the Surprising Behavior of Distance
Metrics in High Dimensional Spaces. In Proceedings of the 8th International Conference on Database Theory (ICDT ’01) .
Springer-Verlag, Berlin, Heidelberg, 420–434.
[2]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program
understanding and generation. arXiv preprint arXiv:2103.06333 (2021).
[3]Miltos Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018. A Survey of Machine Learning for Big
Code and Naturalness. Comput. Surveys 51, 4 (July 2018), 81. https://www.microsoft.com/en-us/research/publication/a-
survey-of-machine-learning-for-big-code-and-naturalness/
[4]Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello, Alessandro Morari,
Jim Laredo, Veronika Thost, Yufan Zhuang, et al .2020. Exploring software naturalness through neural language
models. arXiv preprint arXiv:2006.12641 (2020).
[5]Davide Chicco and Giuseppe Jurman. 2020. The advantages of the Matthews correlation coefficient (MCC) over F1
score and accuracy in binary classification evaluation. BMC genomics 21, 1 (2020), 1–13.
[6]Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can
cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational
Linguistics, Melbourne, Australia, 2126–2136. https://doi.org/10.18653/v1/P18-1198
[7]Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.
A survey for in-context learning. arXiv preprint arXiv:2301.00234 (2022).
[8]Shihan Dou, Junjie Shan, Haoxiang Jia, Wenhao Deng, Zhiheng Xi, Wei He, Yueming Wu, Tao Gui, Yang Liu, and
Xuanjing Huang. 2023. Towards understanding the capability of large language models on code clone detection: a
survey. arXiv preprint arXiv:2308.01191 (2023).
[9]Dawn Drain, Chen Wu, Alexey Svyatkovskiy, and Neel Sundaresan. 2021. Generating bug-fixes using pretrained
transformers. In Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming . 1–8.
[10] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,
and Yiling Lou. 2023. Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation.
arXiv preprint arXiv:2308.01861 (2023).
[11] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M Zhang. 2023.
Large language models for software engineering: Survey and open problems. arXiv preprint arXiv:2310.03533 (2023).
[12] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu Chen. 2020. DeepGini: prioritizing
massive tests to enhance the robustness of deep neural networks. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis (Virtual Event, USA) (ISSTA 2020) . Association for Computing Machinery,
New York, NY, USA, 177–188. https://doi.org/10.1145/3395363.3397357
[13] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,
Daxin Jiang, et al .2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint
arXiv:2002.08155 (2020).
[14] Fengjuan Gao, Yu Wang, and Ke Wang. 2023. Discrete Adversarial Attack to Models of Code. Proc. ACM Program.
Lang. 7, PLDI, Article 113 (jun 2023), 24 pages. https://doi.org/10.1145/3591227
[15] Saurabh Garg, Sivaraman Balakrishnan, Zachary C Lipton, Behnam Neyshabur, and Hanie Sedghi. 2022. Leveraging
unlabeled data to predict out-of-distribution performance. arXiv preprint arXiv:2201.04234 (2022).
[16] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal
Pre-training for Code Representation. arXiv preprint arXiv:2203.03850 (2022).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 26 ---
111:26 W. Ma, S. Liu and M. Zhao et al.
[17] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy,
Shengyu Fu, et al .2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366
(2020).
[18] José Antonio Hernández López, Martin Weyssow, Jesús Sánchez Cuadrado, and Houari Sahraoui. 2023. AST-Probe:
Recovering abstract syntax trees from hidden representations of pre-trained language models. In Proceedings of the
37th IEEE/ACM International Conference on Automated Software Engineering (<conf-loc>, <city>Rochester</city>,
<state>MI</state>, <country>USA</country>, </conf-loc>) (ASE ’22) . Association for Computing Machinery, New
York, NY, USA, Article 11, 11 pages. https://doi.org/10.1145/3551349.3556900
[19] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang,
Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
2023. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. arXiv:2308.00352 [cs.AI]
[20] Susan Horwitz and Thomas Reps. 1992. The Use of Program Dependence Graphs in Software Engineering. In
Proceedings of the 14th International Conference on Software Engineering (Melbourne, Australia) (ICSE ’92) . Association
for Computing Machinery, New York, NY, USA, 392–411. https://doi.org/10.1145/143062.143156
[21] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang.
2023. Large language models for software engineering: A systematic literature review. arXiv preprint arXiv:2308.10620
(2023).
[22] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations .
https://openreview.net/forum?id=nZeVKeeFYf9
[23] Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Lei Ma, Mike Papadakis, and Yves Le Traon. 2024. Test Optimization
in DNN Testing: A Survey. ACM Trans. Softw. Eng. Methodol. (jan 2024). https://doi.org/10.1145/3643678 Just Accepted.
[24] Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Mike Papadakis, and Yves Le Traon. 2023. LaF: Labeling-free
Model Selection for Automated Deep Neural Network Reusing. ACM Trans. Softw. Eng. Methodol. 33, 1, Article 25 (nov
2023), 28 pages. https://doi.org/10.1145/3611666
[25] Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Lei Ma, and Yves Le Traon. 2023. Aries: Efficient
Testing of Deep Neural Networks via Labeling-Free Accuracy Estimation. In Proceedings of the 45th International
Conference on Software Engineering (Melbourne, Victoria, Australia) (ICSE ’23) . IEEE Press, 1776–1787. https://doi.org/
10.1109/ICSE48619.2023.00152
[26] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet
challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).
[27] Mehdi Iraqi. [n. d.]. Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster
Tweets Analysis with Lora . https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-
Mistral#comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-
analysis-with-lora
[28] Akshita Jha and Chandan K. Reddy. 2023. CodeAttack: code-based adversarial attacks for pre-trained programming
language models. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth
Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances
in Artificial Intelligence (AAAI’23/IAAI’23/EAAI’23) . AAAI Press, Article 1670, 9 pages. https://doi.org/10.1609/aaai.
v37i12.26739
[29] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2019. Pre-trained contextual embedding of
source code. (2019).
[30] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,
Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
[31] Rafael-Michael Karampatsis and Charles Sutton. 2020. Scelmo: Source code embeddings from language models. arXiv
preprint arXiv:2004.13214 (2020).
[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,
and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation,
translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).
[33] Lin Li, Yifei Wang, Chawin Sitawarin, and Michael Spratling. 2023. OODRobustBench: benchmarking and analyzing
adversarial robustness under distribution shift. arXiv preprint arXiv:2310.12793 (2023).
[34] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,
Christopher Akiki, Jia Li, Jenny Chim, et al .2023. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161
(2023).
[35] Xiaochen Li, He Jiang, Zhilei Ren, Ge Li, and Jingxuan Zhang. 2018. Deep learning in software engineering. arXiv
preprint arXiv:1805.04825 (2018).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 27 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:27
[36] Xueyang Li, Shangqing Liu, Ruitao Feng, Guozhu Meng, Xiaofei Xie, Kai Chen, and Yang Liu. 2022. TransRepair:
Context-aware Program Repair for Compilation Errors. arXiv preprint arXiv:2210.03986 (2022).
[37] Yuan Li, Xiaodan Liang, Zhiting Hu, Yinbo Chen, and Eric P. Xing. 2019. Graph Transformer. https://openreview.net/
forum?id=HJei-2RcK7
[38] Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020. Retrieval-augmented generation for code
summarization via hybrid gnn. arXiv preprint arXiv:2006.05405 (2020).
[39] Shangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu, and Yang Liu. 2020. ATOM: Commit message generation based
on abstract syntax tree and hybrid ranking. IEEE Transactions on Software Engineering (2020).
[40] Shangqing Liu, Yanzhou Li, and Yang Liu. 2022. CommitBART: A Large Pre-trained Model for GitHub Commits. arXiv
preprint arXiv:2208.08100 (2022).
[41] Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, and Yang Liu. 2023. ContraBERT: Enhancing Code Pre-trained
Models via Contrastive Learning. arXiv preprint arXiv:2301.09072 (2023).
[42] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain,
Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and
generation. arXiv preprint arXiv:2102.04664 (2021).
[43] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin,
and Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. arXiv preprint
arXiv:2306.08568 (2023).
[44] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al .
2018. Deepmutation: Mutation testing of deep learning systems. In 2018 IEEE 29th international symposium on software
reliability engineering (ISSRE) . IEEE, 100–111.
[45] Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, and Yang Liu.
2024. LMs: Understanding Code Syntax and Semantics for Code Analysis. arXiv:2305.12138 [cs.SE]
[46] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023. The Scope of
ChatGPT in Software Engineering: A Thorough Investigation. arXiv preprint arXiv:2305.12138 (2023).
[47] Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon. 2021. Test Selection for Deep Learning
Systems. ACM Trans. Softw. Eng. Methodol. 30, 2, Article 13 (jan 2021), 22 pages. https://doi.org/10.1145/3417330
[48] Wei Ma, Daoyuan Wu, Yuqiang Sun, Tianwen Wang, Shangqing Liu, Jian Zhang, Yue Xue, and Yang Liu.
2024. Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications.
arXiv:2403.16073 [cs.SE]
[49] Wei Ma, Mengjie Zhao, Ezekiel Soremekun, Qiang Hu, Jie M. Zhang, Mike Papadakis, Maxime Cordy, Xiaofei Xie,
and Yves Le Traon. 2022. GraphCode2Vec: generic code embedding via lexical and program dependence analyses. In
Proceedings of the 19th International Conference on Mining Software Repositories (Pittsburgh, Pennsylvania) (MSR ’22) .
Association for Computing Machinery, New York, NY, USA, 524–536. https://doi.org/10.1145/3524842.3528456
[50] B.W. Matthews. 1975. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica
et Biophysica Acta (BBA) - Protein Structure 405, 2 (1975), 442–451. https://doi.org/10.1016/0005-2795(75)90109-9
[51] Linghan Meng, Yanhui Li, Lin Chen, Zhi Wang, Di Wu, Yuming Zhou, and Baowen Xu. 2021. Measuring discrimination
to boost comparative testing for multiple deep learning models. In 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE) . IEEE, 385–396.
[52] Evgeny M Mirkes, Jeza Allohibi, and Alexander Gorban. 2020. Fractional norms and quasinorms do not help to
overcome the curse of dimensionality. Entropy 22, 10 (2020), 1105.
[53] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for
programming language processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence . 1287–1293.
[54] Changan Niu, Chuanyi Li, Vincent Ng, Dongxiao Chen, Jidong Ge, and Bin Luo. 2023. An empirical comparison of
pre-trained models of source code. arXiv preprint arXiv:2302.04026 (2023).
[55] Gustavo Penha and Claudia Hauff. 2020. What does BERT know about books, movies and music? Probing BERT for
Conversational Recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event,
Brazil) (RecSys ’20) . Association for Computing Machinery, New York, NY, USA, 388–397. https://doi.org/10.1145/
3383313.3412249
[56] Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen,
Mihir Choudhury, Lindsey Decker, et al .2021. CodeNet: A large-scale AI for code dataset for learning a diversity of
coding tasks. arXiv preprint arXiv:2105.12655 (2021).
[57] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTology: What We Know About How BERT
Works. Transactions of the Association for Computational Linguistics 8 (2020), 842–866. https://doi.org/10.1162/tacl_a_
00349
[58] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal
Remez, Jérémy Rapin, et al .2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 28 ---
111:28 W. Ma, S. Liu and M. Zhao et al.
(2023).
[59] Da Shen, Xinyun Chen, Chenguang Wang, Koushik Sen, and Dawn Song. 2022. Benchmarking Language Models for
Code Syntax Understanding. In Findings of the Association for Computational Linguistics: EMNLP 2022 , Yoav Goldberg,
Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,
3071–3093. https://doi.org/10.18653/v1/2022.findings-emnlp.224
[60] Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, and Yoshua Bengio. 2018. Straight
to the Tree: Constituency Parsing with Neural Syntactic Distance. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) , Iryna Gurevych and Yusuke Miyao (Eds.). Association
for Computational Linguistics, Melbourne, Australia, 1171–1180. https://doi.org/10.18653/v1/P18-1108
[61] Xiaoxiao Sun, Yunzhong Hou, Weijian Deng, Hongdong Li, and Liang Zheng. 2021. Ranking Models in Unlabeled
New Environments. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) . 11741–11751. https:
//doi.org/10.1109/ICCV48922.2021.01155
[62] Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Miaolei Shi, and Yang Liu. 2024. LLM4Vuln: A
Unified Evaluation Framework for Decoupling and Enhancing LLMs’ Vulnerability Reasoning. arXiv:2401.16185 [cs.CR]
[63] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation
using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering . 1433–1443.
[64] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme,
Samuel R Bowman, Dipanjan Das, et al .2019. What do you learn from context? probing for sentence structure in
contextualized word representations. arXiv preprint arXiv:1905.06316 (2019).
[65] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya
Batra, Prajjwal Bhargava, Shruti Bhosale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[66] Sergey Troshin and Nadezhda Chirkova. 2022. Probing Pretrained Models of Source Code. arXiv preprint
arXiv:2202.08975 (2022).
[67] Betty van Aken, Benjamin Winter, Alexander Löser, and Felix A. Gers. 2019. How Does BERT Answer Questions?
A Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on
Information and Knowledge Management (Beijing, China) (CIKM ’19) . Association for Computing Machinery, New
York, NY, USA, 1823–1832. https://doi.org/10.1145/3357384.3358028
[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[69] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What Do They Capture? A Structural
Analysis of Pre-Trained Language Models for Source Code. In Proceedings of the 44th International Conference on
Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . Association for Computing Machinery, New York, NY, USA,
2377–2388. https://doi.org/10.1145/3510003.3510050
[70] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open
code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).
[71] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-
decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
[72] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,
Denny Zhou, Donald Metzler, et al .2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682
(2022).
[73] Yanming Yang, Xin Xia, David Lo, and John Grundy. 2022. A Survey on Deep Learning for Software Engineering. ACM
Comput. Surv. 54, 10s, Article 206 (sep 2022), 73 pages. https://doi.org/10.1145/3505243
[74] Jingxiu Yao and Martin Shepperd. 2020. Assessing Software Defection Prediction Performance: Why Using the
Matthews Correlation Coefficient Matters. In Proceedings of the 24th International Conference on Evaluation and
Assessment in Software Engineering (Trondheim, Norway) (EASE ’20) . Association for Computing Machinery, New
York, NY, USA, 120–129. https://doi.org/10.1145/3383219.3383232
[75] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of code. Proc. ACM Program. Lang. 4,
OOPSLA, Article 162 (nov 2020), 30 pages. https://doi.org/10.1145/3428230
[76] Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, and Xin Peng. 2023. No More Manual
Tests? Evaluating and Improving ChatGPT for Unit Test Generation. arXiv preprint arXiv:2305.04207 (2023).
[77] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. 2019. Graph transformer networks.
Advances in neural information processing systems 32 (2019).
[78] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. A survey
on language models for code. arXiv preprint arXiv:2311.07989 (2023).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

--- PAGE 29 ---
Unveiling Code Pre-Trained Models: Investigating Syntax and Semantics Capacities 111:29
[79] Mengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh, and Hinrich Schütze. 2020. Quantifying the Contextualization
of Word Representations with Semantic Class Probing. In Findings of the Association for Computational Linguistics:
EMNLP 2020 . Association for Computational Linguistics, Online, 1219–1234. https://doi.org/10.18653/v1/2020.findings-
emnlp.109
[80] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. Devign: Effective vulnerability identification
by learning comprehensive program semantics via graph neural networks. In Advances in Neural Information Processing
Systems . 10197–10207.
[81] Barret Zoph, Colin Raffel, Dale Schuurmans, Dani Yogatama, Denny Zhou, Don Metzler, Ed H. Chi, Jason Wei, Jeff
Dean, Liam B. Fedus, Maarten Paul Bosma, Oriol Vinyals, Percy Liang, Sebastian Borgeaud, Tatsunori B. Hashimoto,
and Yi Tay. 2022. Emergent abilities of large language models. TMLR (2022).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
