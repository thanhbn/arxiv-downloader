# 2307.15370.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2307.15370.pdf
# Kích thước file: 5507716 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Điểm nổi bật
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn
Daoguang Zan,Bei Chen,Yongshun Gong,Junzhi Cao,Fengji Zhang,Bingchao Wu,Bei Guan,Yilong Yin,Yongji Wang
•Chúng tôi giới thiệu một kịch bản mới tập trung vào tạo mã hướng thư viện riêng tư và đề xuất một khung công tác sáng tạo
mô phỏng quy trình của con người khi sử dụng thư viện riêng tư. Khung công tác này bao gồm hai mô-đun chính: APIFinder và
APICoder.
•Để đánh giá công bằng kịch bản này, chúng tôi xây dựng thủ công bốn bộ đánh giá, cụ thể là TorchDataEval, TorchDataComplex-
Eval, MonkeyEval, và BeatNumEval, mỗi bộ có một loạt các trường hợp kiểm tra phong phú.
•Các thí nghiệm toàn diện của chúng tôi thể hiện hiệu suất vượt trội của khung công tác được đề xuất, chứng minh tính hiệu quả và hiệu suất trong việc xử lý các API riêng tư.arXiv:2307.15370v1 [cs.SE] 28 Jul 2023

--- TRANG 2 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn
Daoguang Zana,1, Bei Chenb,∗, Yongshun Gongc,∗, Junzhi Caod, Fengji Zhange, Bingchao Wua,
Bei Guana,∗, Yilong Yincand Yongji Wanga
aViện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
bMicrosoft, Bắc Kinh, Trung Quốc
cTrường Phần mềm, Đại học Shandong, Jinan, Trung Quốc
dĐại học New York, New York, Hoa Kỳ
eTrường Khoa học Máy tính, Đại học Wuhan, Wuhan, Trung Quốc
THÔNG TIN BÀI BÁO
Từ khóa:
Tạo mã
Mô hình ngôn ngữ lớn
Thư viện riêng tư
Tài liệu API
Tạo dựa trên truy xuấtTÓM TẮT
Các mô hình ngôn ngữ lớn (LLM), như Codex và GPT-4, gần đây đã thể hiện khả năng tạo mã đáng chú ý, tạo ra sự tăng trưởng đáng kể trong hiệu quả lập trình. Bài báo này sẽ đào sâu vào việc sử dụng LLM để tạo mã trong các thư viện riêng tư, vì chúng được sử dụng rộng rãi trong lập trình hàng ngày. Mặc dù có khả năng đáng chú ý, việc tạo ra các API riêng tư như vậy đặt ra một bài toán khó khăn cho LLM, vì chúng thiếu hụt về bản chất việc tiếp xúc với các thư viện riêng tư này trong quá trình tiền huấn luyện. Để giải quyết thách thức này, chúng tôi đề xuất một khung công tác mới mô phỏng quy trình của lập trình viên khi viết mã riêng tư. Khung công tác này bao gồm hai mô-đun: APIFinder trước tiên truy xuất các API có thể hữu ích từ tài liệu API; và APICoder sau đó tận dụng các API được truy xuất này để tạo mã riêng tư. Cụ thể, APIFinder sử dụng các kỹ thuật truy xuất vector và cho phép người dùng tham gia vào quá trình truy xuất. Đối với APICoder, nó có thể trực tiếp sử dụng các mô hình tạo mã có sẵn. Để tiếp tục phát triển khả năng rõ ràng trong việc gọi API từ các prompt, chúng tôi tiếp tục tiền huấn luyện một phiên bản được tăng cường của APICoder, có tên là CODEGENAPI. Mục tiêu của chúng tôi là huấn luyện hai mô-đun trên trên các thư viện công cộng rộng lớn, cho phép khái quát hóa cho các thư viện riêng tư. Đồng thời, chúng tôi tạo ra bốn bộ đánh giá thư viện riêng tư, bao gồm TorchDataEval, TorchDataComplexEval, MonkeyEval, và BeatNumEval, và tỉ mỉ chế tạo thủ công các trường hợp kiểm tra cho mỗi bộ đánh giá để hỗ trợ đánh giá toàn diện. Nhiều thí nghiệm trên bốn bộ đánh giá liên tục khẳng định tính hiệu quả của phương pháp của chúng tôi. Hơn nữa, phân tích sâu hơn cũng được thực hiện để thu thập thêm các hiểu biết.

1. Giới thiệu
Việc sử dụng các thư viện mã của bên thứ ba có thể nâng cao đáng kể khả năng tái sử dụng mã, giảm khó khăn phát triển và tăng tốc độ phát triển trong phát triển phần mềm [3, 6, 20]. Các nhà phát triển có thể đẩy nhanh quá trình phát triển của họ bằng cách tích hợp các thư viện của bên thứ ba, đồng thời tránh công việc dư thừa của việc tái tạo bánh xe. Xem xét nhiều lợi ích của việc sử dụng thư viện mã của bên thứ ba, việc các công ty phát triển và duy trì một thư viện mã nhằm mục đích kiểm soát mã và quản lý phiên bản tốt hơn là điều phổ biến. Tuy nhiên, trái ngược với các thư viện mã công cộng, nhiều thư viện mã được duy trì bởi các công ty là riêng tư và chỉ được sử dụng độc quyền cho mục đích nội bộ, với mục tiêu tăng cường các biện pháp bảo mật để bảo vệ tài sản trí tuệ và bí mật thương mại [39]. Do đó, chúng ta có thể thấy rằng việc tận dụng các thư viện riêng tư để tạo mã là một kịch bản phổ biến và có ý nghĩa [55]. Với sự tiến bộ nhanh chóng của mô hình ngôn ngữ lớn (LLM), công nghệ này đã được áp dụng trong lĩnh vực tạo mã [13, 61, 64] thông qua việc huấn luyện trên các kho mã rộng lớn, như Codex [14], AlphaCode [35], CODEGEN [41]. Thật không may, mặc dù những mô hình ngôn ngữ này sở hữu khả năng tạo mã cực kỳ ấn tượng trong các kịch bản chung, chúng bất lực khi xử lý các thư viện riêng tư
∗Tác giả liên hệ
daoguang@iscas.ac.cn (D. Zan); beichen@microsoft.com (B. Chen);
ysgong@sdu.edu.cn (Y. Gong); guanbei@iscas.ac.cn (B. Guan)
ORCID(s):vì chúng không có kiến thức trước về thông tin như vậy. Hình 1 đưa ra một ví dụ cụ thể để minh họa điều này. Khi tạo mã cho thư viện công cộng pandas, Codex, một mô hình ngôn ngữ lớn 12B, thành công gọi các lệnh gọi API của thư viện và tạo ra mã hiệu quả. Khi chúng tôi đóng gói pandas.DataFrame.isin thành monkey.KnowledgeFrame.iscontain để sử dụng nội bộ; tuy nhiên, Codex không thể gọi đúng API và tạo mã cho thư viện riêng tư bằng cách tham khảo tài liệu API, như một lập trình viên sẽ làm. Ví dụ này càng nhấn mạnh những thách thức của LLM khi đối mặt với tạo mã hướng thư viện riêng tư.

Trong bài báo này, đề xuất của chúng tôi tìm cách giải quyết những thách thức này bằng cách giới thiệu một khung công tác giải phóng các mô hình ngôn ngữ lớn với năng lực tạo ra mã gọi các thư viện riêng tư. Vì các thư viện riêng tư thường có các tài liệu tham khảo toàn diện như tài liệu API, ý tưởng cốt lõi của chúng tôi liên quan đến việc mô phỏng quy trình mà lập trình viên sử dụng tài liệu API để phát triển mã cho các thư viện riêng tư. Quá trình tìm kiếm thông tin trong tài liệu API để hiểu cách sử dụng API trước khi viết mã, thường được gọi là tra cứu tài liệu API [50], là một thực hành thiết yếu trong phát triển phần mềm và lập trình. Tương tự, khung công tác của chúng tôi bao gồm hai mô-đun: APIFinder truy xuất các API tiềm năng từ tài liệu API dựa trên yêu cầu của người dùng, và APICoder tận dụng các API được truy xuất để tạo ra mã. Liên quan đến APIFinder, chúng tôi sử dụng một bộ truy xuất dày đặc và kết hợp một giao diện thân thiện với người dùng

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 1 của 18

--- TRANG 3 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

import pandas as pd
df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]})
values = [1, 2]
# liệu mỗi phần tử trong `df` có chứa trong values không.
Codex 12B prompt mã công cộng (pandas)

import monkey as mk
kf = mk.KnowledgeFrame({'a': [2, 4], 'b': [2, 0]})
values = [1, 2]
# liệu mỗi phần tử trong `kf` có chứa trong values không.
prompt mã riêng tư (monkey)

df.isin(values) ✓ pandas.DataFrame.isin

Codex 12B
kf.isin(values)
kf.iscontain(values) ✓ monkey.KnowledgeFrame.iscontain
pandas.DataFrame.isin

Tôi đã thấy pandas, vì vậy tôi có thể làm được.

Tôi chưa bao giờ thấy monkey, nhưng tôi có thể gọi API của pandas vì mã trông rất giống với đoạn mã từ pandas.

Tôi chưa bao giờ thấy monkey, nhưng tôi có thể tham khảo tài liệu API của monkey để làm điều đó.

Hình 1: Một ví dụ về tạo mã trên thư viện công cộng (pandas) và thư viện riêng tư (monkey) bởi Codex 12B và một lập trình viên.

giao diện cung cấp sự tham gia tùy chọn của người dùng trong quá trình truy xuất API. Liên quan đến APICoder, các mô hình ngôn ngữ có sẵn để tạo mã như CODEGEN có thể được sử dụng để viết mã riêng tư trực tiếp. Thậm chí còn thú vị hơn, chúng tôi phát hiện ra rằng việc tiếp tục tiền huấn luyện các mô hình hiện có bằng các chiến lược mới được đề xuất của chúng tôi có thể cải thiện đáng kể khả năng gọi API thư viện riêng tư của chúng. Vì chúng tôi không thể truy cập kho dữ liệu của các thư viện riêng tư, chúng tôi huấn luyện cả APIFinder và APICoder trên nhiều thư viện công cộng được thu thập với nguyện vọng cho phép khái quát hóa cho các kịch bản thư viện riêng tư.

Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên đề xuất kịch bản tạo mã hướng thư viện riêng tư. Do thiếu các bộ đánh giá có sẵn để đánh giá kịch bản này, chúng tôi tạo ra bốn bộ đánh giá: TorchDataEval, TorchDataComplexEval, MonkeyEval, và BeatNumEval. Cả TorchDataEval và TorchDataComplexEval đều bao gồm 50 bài toán lập trình cho các thư viện riêng tư, với bộ sau có các bài toán thách thức và thực tế hơn. MonkeyEval và BeatNumEval, mỗi bộ chứa 101 bài toán lập trình, là các thích ứng của PandasEval và NumpyEval [63], tương ứng.

Bài báo này phục vụ như một phiên bản mở rộng của [62] bằng cách nâng cao các khía cạnh khác nhau của nghiên cứu ban đầu. Cụ thể, các khía cạnh này có thể được tóm tắt như sau: (1) Chúng tôi điều tra tạo mã hướng thư viện riêng tư trên các mức độ khó khăn khác nhau và tiếp tục chế tạo một bộ đánh giá thư viện riêng tư thách thức và thực tế hơn có tên TorchDataComplexEval. (2) Chúng tôi khám phá tác động của tất cả các thành phần của tài liệu API, như ví dụ API và tham số, đối với hiệu quả của LLM. (3) Chúng tôi thực hiện các thí nghiệm rộng rãi trên 17 mô hình tạo mã phổ biến, và tiếp tục huấn luyện CODEGENAPI sử dụng ba prompt trên các kích thước mô hình khác nhau (350M, 2B, 6B). (4) Chúng tôi thực hiện phân tích và đánh giá toàn diện và nghiêm ngặt hơn, mang lại nhiều hiểu biết thú vị và có giá trị. Nhìn chung, đóng góp của chúng tôi được liệt kê như sau:

• Chúng tôi giới thiệu một kịch bản được gọi là tạo mã hướng thư viện riêng tư và đề xuất một khung công tác đơn giản nhưng sáng tạo mô phỏng cách tiếp cận của con người để sử dụng các thư viện riêng tư.

• Chúng tôi tỉ mỉ phát triển bốn bộ đánh giá thư viện riêng tư, mỗi bộ được trang bị một bộ các trường hợp kiểm tra toàn diện.

• Các thí nghiệm kỹ lưỡng làm nổi bật hiệu suất đáng chú ý của khung công tác của chúng tôi, nhấn mạnh tính khả thi trong việc gọi các API riêng tư.

2. Các công trình liên quan

2.1. Các mô hình ngôn ngữ lớn cho tạo mã

Các mô hình tiên phong như PLBART [1], PyMT5 [17], và GPT-C [54] sở hữu số lượng tham số khiêm tốn và thể hiện khả năng hạn chế trong tạo mã zero-shot. Ngược lại, mặc dù các mô hình quy mô lớn như GPT-Neo [8] và GPT-J [56] tự hào có hàng tỷ tham số, hiệu quả của chúng trong nhiệm vụ tạo mã vẫn bị hạn chế bởi sự khan hiếm mã trong các tập dữ liệu huấn luyện của chúng. Gần đây, một loạt các LLM tiên tiến đã được giới thiệu cho việc tạo mã, bao gồm nhưng không giới hạn ở Codex [14], PaLM-Coder [15], PanGu-Coder [16, 51], AlphaCode [35], GPT-4 [42], và ChatGPT1. Những mô hình này bao gồm số lượng tham số rộng lớn và được huấn luyện trên dữ liệu giàu mã cao cấp. Mặc dù chúng thể hiện hiệu suất xuất sắc trong các nhiệm vụ tạo mã, phần lớn chúng đều không thể truy cập được. Hiện tại, nhiều mô hình công khai xuất sắc đã xuất hiện, như CodeParrot [27], CodeGen [41], và PolyCoder [60]. Những mô hình này đóng vai trò quan trọng trong tiến bộ của LLM cho việc tạo mã. Trong khi tất cả các mô hình trên chỉ hỗ trợ tạo mã từ trái sang phải, các mô hình như SantaCoder [2], FIM [7], InCoder [24], StarCoder [34], và MIM [40] cũng hỗ trợ chèn đoạn mã tại các vị trí tùy ý, bao gồm các vị trí trung gian. Bên cạnh các file Python, JuPyT5 [11] lựa chọn các file Jupyter Notebook để huấn luyện và đạt được kết quả đáng chú ý. Các mô hình gần đây, như ERNIE-Code [10], BLOOM [49], và CodeGeeX [66], cũng đã xem xét một kịch bản mới của nhiều ngôn ngữ lập trình hoặc ngôn ngữ tự nhiên. Trong nghiên cứu này, chúng tôi sẽ đào sâu vào cách trang bị cho các mô hình đã nêu trên khả năng sử dụng các thư viện riêng tư.

Trong kỷ nguyên của LLM, các bộ đánh giá tạo mã được chế tạo thủ công đóng vai trò quan trọng trong việc đảm bảo rằng các bài toán lập trình chưa được tiếp xúc trong quá trình tiền huấn luyện. Do đó, HumanEval [14] và MBPP [5] đã được đề xuất

1https://chat.openai.com

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 2 của 18

--- TRANG 4 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

import
def KBS_Example():
    """
    """
    
Ngữ cảnh 𝒙
Mã đích 𝒚

Hình 2: Một ví dụ Python đơn giản về tạo mã. ■ đại diện cho thư viện mã như pandas; ■ đại diện cho một số đoạn mã, hàm, hoặc lớp; ■ đại diện cho mô tả ngôn ngữ tự nhiên của bài toán lập trình; ■ đại diện cho mã đích giải quyết bài toán lập trình trong ■, và có thể gọi API từ ■ và ■.

và được phổ biến rộng rãi. Hai bộ đánh giá trên là phiên bản tiếng Anh và Python. Sau đó, một số bộ đánh giá đã mở rộng chúng sang nhiều ngôn ngữ, như MBXP [4], MultiPL [9], và HumanEval-X [66]. Hơn nữa, các bộ đánh giá được phát hành gần đây ngày càng được điều chỉnh cho một số kịch bản thực tế. Ví dụ, APPS [26] và CodeContests [35] phục vụ như các bộ đánh giá để đánh giá khả năng thi lập trình; DSP [12] và DS-1000 [33] là các bộ đánh giá hướng khoa học dữ liệu; MTPB [41] được giới thiệu cho việc tạo mã đối thoại nhiều lượt. SecurityEval [53] được đề xuất để đánh giá bảo mật mã; MathQA-Python [5] và GSM8K-Python [18] được hướng đến đánh giá thành thạo toán học; PandasEval [29, 63] và NumpyEval [63] kiểm tra việc tạo mã trong các thư viện công cộng, trong khi một số bộ đánh giá [22, 52, 65] tập trung vào mức kho lưu trữ hoặc cross-file. Trong bài báo này, chúng tôi đề xuất một kịch bản thư viện riêng tư cho việc tạo mã và tỉ mỉ chế tạo bốn bộ đánh giá, có tên TorchDataEval, TorchDataComplexEval, MonkeyEval, và BeatNumEval.

2.2. Tạo dựa trên truy xuất

Tạo dựa trên truy xuất [23, 48, 59] đã thu hút sự phổ biến đáng kể trong các lĩnh vực xử lý ngôn ngữ tự nhiên và truy xuất thông tin, nhờ khả năng thành thạo trong việc tận dụng kiến thức có sẵn từ bên ngoài. Ví dụ, Fusion-in-Decoder [28], DPR [31], và RocketQA [45] tận dụng kỹ thuật này cho việc trả lời câu hỏi miền mở bằng cách truy xuất các đoạn văn liên quan và thể hiện kết quả đáng chú ý. Trong lĩnh vực tạo mã, cũng tồn tại một số nỗ lực sử dụng các kỹ thuật truy xuất, như DeepAPI [25], ReACC [37], REDCODER [44], RepoCoder [65], và DocCoder [67]. Trong bài báo này, mục tiêu của chúng tôi là truy xuất các API riêng tư thực tế từ tài liệu API.

3. Sơ bộ

3.1. Định nghĩa nhiệm vụ

Cho một ngữ cảnh mã 𝐱, mục tiêu của việc tạo mã nhằm tạo ra mã đích 𝐲 giải quyết bài toán lập trình trong 𝐱. Hình 2 trình bày một ví dụ về ngữ cảnh mã và mã đích. Chi tiết, ngữ cảnh mã bao gồm mô tả ngôn ngữ tự nhiên của các bài toán lập trình dưới dạng bình luận mã ■, cũng như các câu lệnh import ■,

pandas.DataFrame.head
DataFrame.head(n: int=5) -> NDFrame

Tên API + chữ ký

Trả về n hàng đầu tiên.

Đầu vào:             n: int, mặc định 5
Số lượng hàng để chọn.

Trả về:           cùng loại với caller
N hàng đầu tiên của đối tượng caller.

API liên quan
DataFrame.tail Trả về n hàng cuối cùng.

Ví dụ API
>>> df = pd.DataFrame(['a', 'b', 'c'])
>>> df.head(2)
   0
0  a
1  b

Mô tả API
Tham số

Hình 3: Một thể hiện API của tài liệu API. Nó thể hiện các thành phần chính của mỗi API trong tài liệu API: tên API, chữ ký, mô tả, tham số, API liên quan, và ví dụ API.

các lớp hoặc hàm do người dùng định nghĩa ■, header hàm, và v.v. Toàn bộ quá trình tạo mã có thể được hình thức hóa là 𝐲 = ℳ(𝐱), trong đó ℳ là mô hình tạo mã. Trong bài báo này, chúng tôi tập trung vào tạo mã hướng thư viện riêng tư. Một thư viện riêng tư, so với thư viện công cộng, được đặc trưng bởi việc không công khai, tức là thư viện mã không được nhìn thấy bởi ℳ trong quá trình tiền huấn luyện.

3.2. Tài liệu API

Tài liệu API, còn được gọi là tham khảo API, là một mô tả toàn diện về giao diện lập trình mà một thư viện phần mềm cung cấp cho các nhà phát triển. Tài liệu API phục vụ như một tài nguyên kỹ thuật quan trọng cho các nhà phát triển để tạo mã hướng thư viện riêng tư, cho phép họ hiểu cách sử dụng thư viện và tận dụng tối đa các khả năng của nó. Xem xét tầm quan trọng của tài liệu API, chúng tôi muốn cung cấp một phân tích chi tiết về các thành phần của nó như được minh họa trong Hình 3:

• Tên API: Đây là một nhãn mô tả được sử dụng để xác định API và truyền đạt chức năng của nó;

• Chữ ký API: Điều này phác thảo các tham số đầu vào và đầu ra của API, bao gồm các loại dữ liệu và định dạng mong đợi;

• Mô tả API: Điều này cung cấp một giải thích toàn diện về chức năng và mục đích của API;

• Tham số API: Những điều này định nghĩa các tham số đầu vào và trả về, bao gồm thông tin như loại dữ liệu, giá trị đối số mặc định, và mô tả các tham số;

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 3 của 18

--- TRANG 5 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Tài liệu API

API được truy xuất

APIFinder

APICoder

Ngữ cảnh
Mã đích

𝐱
𝐲

ℳF

ℳC

𝒜

Hình 4: Sơ đồ khung công tác của chúng tôi: APIFinder trước tiên truy xuất các API có thể hữu ích từ tài liệu API, và sau đó APICoder tạo mã đích dựa trên các API truy xuất.

• API liên quan: Đây là các API có liên quan chặt chẽ với API hiện tại, theo chức năng hoặc mục đích, và cung cấp các khả năng tương tự hoặc bổ sung.

• Ví dụ API: Những ví dụ này cung cấp một ví dụ hoạt động về cách gọi API trong thực tế.

Trong bài báo này, chúng tôi định nghĩa API Basic là sự kết hợp của Tên API, Chữ ký API, và Mô tả API.

4. Khung công tác

Lấy cảm hứng từ cách lập trình viên tận dụng tài liệu API để giải quyết các kịch bản thư viện riêng tư, khung công tác của chúng tôi được thiết kế. Lập trình viên, như một thực hành phổ biến, trước tiên tìm và định vị các API phù hợp trong tài liệu API dựa trên các bài toán lập trình của họ, sau đó học cách gọi các API này để giải quyết chúng một cách chính xác. Tương tự, như được mô tả trong Hình 4, khung công tác của chúng tôi cũng bao gồm hai mô-đun: APIFinder ℳF, tìm các API khả thi 𝒜, và APICoder ℳC, gọi các API này một cách thích hợp.

Quy trình có thể được biểu diễn là:
𝒜 = ℳF(𝐱); 𝐲 = ℳC(𝒜; 𝐱), (1)

trong đó 𝐱 và 𝐲 biểu thị ngữ cảnh mã và mã đích, 𝒜 ∋ 𝐚 đề cập đến thông tin của tất cả các API được truy xuất. Trong nghiên cứu này, một trong những đóng góp quan trọng là điều tra xem các thành phần nào trong tài liệu API có lợi cho các mô hình tạo mã hiện tại và cách sử dụng chúng tốt hơn. Do đó, thông tin của một API 𝐚 đề cập đến một hoặc nhiều thành phần trong Hình 3. Về bản chất, khung công tác được đề xuất của chúng tôi giải quyết các kịch bản thư viện riêng tư bằng cách chia việc tạo mã ban đầu một lần thành các mô-đun truy xuất và tạo, điều này có hiệu quả trao quyền cho các mô hình tạo mã để xử lý các thư viện riêng tư.

5. Phương pháp

Khung công tác được đề xuất của chúng tôi đã trình bày một ý tưởng đơn giản nhưng mới mẻ để trao quyền cho các mô hình ngôn ngữ tạo ra các đoạn mã riêng tư. Trong phần sau, chúng tôi đào sâu vào việc triển khai thực tế của khung công tác, bao gồm việc chuẩn bị kho dữ liệu và các chi tiết thiết kế tinh tế hơn của cả APIFinder và APICoder.

5.1. Chuẩn bị kho dữ liệu

Để huấn luyện một mô hình neural cho các kịch bản thư viện riêng tư, một cách tiếp cận trực quan là thu thập một lượng lớn kho dữ liệu thư viện riêng tư để huấn luyện. Rõ ràng, việc thu thập kho dữ liệu thư viện riêng tư là không khả thi. Do đó, chúng tôi chỉ có thể huấn luyện mô hình của mình trên một kho dữ liệu lớn các thư viện công cộng với hy vọng khái quát hóa chúng cho các thư viện riêng tư.

Trong nghiên cứu này, chúng tôi xác định 31 thư viện mã Python phổ biến nhất, như Pandas, NumPy, và PyTorch, dựa trên sự phổ biến của chúng trong Stack Overflow2. Tên của các thư viện này và chi tiết số lượng API tương ứng có thể được tìm thấy trong Bảng 1. Đối với các thư viện này, chúng tôi thu thập tài liệu API của chúng một cách riêng lẻ. Hơn nữa, chúng tôi phân tách mỗi API trong tài liệu API thành các thành phần con như tên API, chữ ký, mô tả, và ví dụ. Ngoài tài liệu API của các thư viện này, chúng tôi cũng cần một bộ sưu tập lớn các đoạn mã gọi các thư viện này. Do đó, chúng tôi ban đầu thu thập một kho mã 330GB từ GitHub, chứa 60,62 triệu file Python. Sau một loạt các chiến lược tiền xử lý dữ liệu, bao gồm chọn lọc các file Python liên quan đến 31 thư viện mã, khử trùng lặp, và định dạng, cuối cùng chúng tôi có được khoảng 25GB file Python bao gồm tổng cộng 4,54 triệu file, được gọi là 𝒟.

5.2. APIFinder

Cho một mô tả bài toán lập trình, APIFinder chịu trách nhiệm khớp và tìm các API có thể sử dụng được từ tài liệu API của thư viện riêng tư.

Huấn luyện. APIFinder có thể được huấn luyện bằng các kỹ thuật truy xuất dày đặc [23, 45, 48, 59]. Các kỹ thuật truy xuất dày đặc chủ yếu bao gồm hai cách tiếp cận chính: single-encoder và dual-encoder. Khi nhận được một mô tả bài toán lập trình, các mô hình single-encoder yêu cầu tính toán lại các embedding của mỗi API của tài liệu API, trong khi dual-encoder tránh điều này bằng cách tính toán trước các API này ngoại tuyến. Xem xét tính thực tế và hiệu quả trong quá trình suy luận, chúng tôi do đó chọn huấn luyện APIFinder bằng dual-encoder. Để huấn luyện một mô hình như vậy, một bộ sưu tập các mô tả bài toán lập trình và các API tương ứng của chúng là cần thiết. Do đó, chúng tôi thiết kế một chiến lược tỉ mỉ để trích xuất các cặp dữ liệu này từ kho GitHub được thu thập 𝒟. Ban đầu, chúng tôi tách mỗi file 𝐩 trong 𝒟 thành 𝐾 khối mã (𝐩1, 𝐩2, ⋯, 𝐩𝐾), trong đó một khối mã là một đoạn mã hoàn chỉnh và không bị gián đoạn như một phương thức hoặc lớp. Chức năng này có thể được triển khai tự động thông qua một số công cụ pip, bao gồm autopep8, docformatter, và redbaron. Như thể hiện trong Hình 5, đối với mỗi khối mã 𝐩𝑖, chúng tôi trích xuất chú thích và tên API của nó. Đối với mỗi tên API, chúng tôi định vị chữ ký và mô tả API tương ứng bằng cách tìm kiếm tài liệu API mà chúng tôi đã thu thập. Lưu ý rằng một tên API có thể khớp với nhiều API. Ví dụ, tên API head trong pandas có thể tương ứng với cả DataFrame.head và Series.head. Dựa trên quan sát thực nghiệm của chúng tôi, chúng tôi kết luận rằng những API có cùng tên API này tương tự hoặc thậm chí giống hệt nhau trừ

2https://stackoverflow.com/tags?tab=popular

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 4 của 18

--- TRANG 6 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Bảng 1
31 thư viện công cộng và số lượng API tương ứng của chúng.

Pandas NumPy sklearn PyTorch TensorFlow Django selenium Matplotlib Flask SciPy Seaborn ansible
7,094 12,085 53,166 124,902 32,116 24,375 4,842 439,913 31,867 153,359 161,477 40,839

NLTK BeatifulSoup pygame PIL jieba Gensim spaCy transformers fairseq SQLAlchemy Scrapy requests
206,816 22,519 70,396 127,212 26,620 37,331 239,945 652,913 158,721 54,765 3,537 39,333

AllenNLP datasets tokenizers MXNet imageio pytest MetPy
276,088 136,843 195 142,070 175,878 1,047 27,429

import pandas as pd

def get_head_and_duplicated(df, n=5):
    """
    Trả về n hàng đầu tiên và loại bỏ các
    hàng trùng lặp.
    """
    return df.head(n).drop_duplicates()

Trả về n hàng đầu tiên và loại bỏ các 
hàng trùng lặp.

[1].head(n:int=5): Trả về n hàng đầu tiên.
[2].drop_duplicates(subset=None,*): Trả về
DataFrame với các hàng trùng lặp đã được loại bỏ.

BERT API Documentation
BERT API
Embedding

Comment
Embedding

Cross
Entropy

APIFinder

[1] hoặc [2] Khối mã 𝒑𝒊

Hình 5: Tổng quan về việc chuẩn bị kho dữ liệu huấn luyện cho APIFinder.

đường dẫn API. Do đó, khi gặp một tên API tương ứng với nhiều API, chúng tôi chọn ngẫu nhiên một cái. Đối với mỗi khối mã 𝐩𝑖, chúng tôi đã thu được nhiều cặp dữ liệu về mô tả bài toán lập trình 𝐝 và thông tin API 𝐚, trong đó mỗi 𝐚 bao gồm tên API, chữ ký, và mô tả của nó3. Chúng tôi coi 𝐝 và 𝐚 được trích xuất từ một khối mã là một mẫu tích cực. Đối với những mẫu tiêu cực, chúng tôi chọn ngẫu nhiên 𝑛 API không liên quan đến 𝐝, với 𝑛 được đặt là 8 trong các thí nghiệm của chúng tôi. Tổng cộng, chúng tôi trích xuất 40,37 triệu thể hiện (𝐝, 𝐚, â𝐚1, â𝐚2, …, â𝐚𝑛) để huấn luyện APIFinder của chúng tôi. Theo RocketQA [45], chúng tôi mã hóa 𝐝 và 𝐚 thành embedding 768 chiều riêng biệt bằng hai mô hình BERT-base [21] có tên 𝐸𝐝 và 𝐸𝐚. Sau đó chúng tôi tính toán độ tương tự giữa hai embedding này bằng tích vô hướng và tối ưu hóa nó với cross-entropy loss.

Suy luận. Sau khi huấn luyện thành công APIFinder của chúng tôi trên một loạt rộng các thư viện công cộng, chúng tôi có thể tận dụng nó để truy xuất các API riêng tư từ tài liệu API. Trong các kịch bản thư viện riêng tư thực tế của chúng tôi, tài liệu API thường được coi là một tài nguyên ngoại tuyến ổn định không thay đổi trừ khi có cập nhật phiên bản. Do đó, 𝐸𝐚 có thể mã hóa trước tất cả các API trong tài liệu API ngoại tuyến và lập chỉ mục chúng trong FAISS [30] để truy xuất hiệu quả. Khi nhận được một mô tả bài toán lập trình mới, chúng tôi chỉ cần tính toán embedding của nó bằng 𝐸𝐝, và sau đó truy xuất (các) API gần nhất từ các API được mã hóa trước ngoại tuyến theo độ gần không gian.

Sự tham gia tùy chọn của người dùng với APIFinder. Những API được truy xuất bởi APIFinder có thể được đưa trực tiếp vào APICoder để tạo mã gọi các thư viện riêng tư. Để cung cấp cho APICoder các API chính xác hơn, chúng tôi thiết kế một pipeline cho phép người dùng tham gia với APIFinder. Cụ thể, như thể hiện trong Hình 6, chúng tôi cung cấp cho người dùng một giao diện tương tác

3Trong tất cả các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng câu đầu tiên của mô tả API, vì nó đủ tóm tắt nội dung.

from torchdata.datapipes.iter import IterableWrapper
source_dp = IterableWrapper(range(5))
# Ngẫu nhiên hóa và nhân bản source datapipe hai lần.
dp1, dp2 = source_dp.

Bài toán lập trình

Bạn muốn sử dụng API nào trong?

concat: Nối nhiều Map DataPipes.
shuffle: Xáo trộn Map DataPipe đầu vào thông qua các chỉ số của nó.
fork: Tạo nhiều thể hiện của cùng một DataPipe.
demux: Tách DataPipe đầu vào thành nhiều cái.
batch: Tạo mini-batches dữ liệu.

Không có cái nào ở trên.
Không chắc chắn.

Hình 6: Giao diện tương tác người dùng với APIFinder: người dùng có thể chọn một hoặc nhiều API từ top 5 gợi ý của APIFinder để APICoder sử dụng.

diện cho phép họ chọn một hoặc nhiều từ top 5 API được truy xuất bởi APIFinder. Chúng tôi cũng cung cấp "Không có cái nào ở trên" và "Không chắc chắn" như các lựa chọn cho người dùng nếu họ thấy kết quả truy xuất APIFinder thiếu một câu trả lời dứt khoát hoặc không chắc chắn. Nếu "Không có cái nào ở trên" được chọn, APICoder sẽ prompt không có API. Nếu "Không chắc chắn" được chọn, hai API được truy xuất đầu tiên sẽ được prompt cho APICoder theo mặc định. Trong giao diện được thiết kế của chúng tôi, chúng tôi chỉ trình bày tên API và câu đầu tiên của mô tả của nó, vì thông tin quá mức có thể dẫn đến mệt mỏi người đọc và tác động tiêu cực.

5.3. APICoder

APICoder tận dụng các API được truy xuất bởi APIFinder để viết mã riêng tư. Trước khi gọi các API này, lập trình viên cần phải học chúng. Tương tự, các mô hình ngôn ngữ lớn cũng cần kết hợp các API này vào mã

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 5 của 18

--- TRANG 7 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

def get_head(df, n=5):
    """
    Trả về n hàng đầu tiên
    """
    return df.head(n)

head(n:int=5):
Trả về n hàng đầu tiên.

Tài liệu API

Ví dụ API:
>>> df = pd.DataFrame(['a', 'b', 'c'])
>>> df.head(2)
   0
0  a
1  b

# Hãy sử dụng các API sau để giải quyết nhiệm vụ:
# 
# 
# Nhìn chung, bạn có thể sử dụng các API trên để giải quyết nhiệm vụ.

# Hãy sử dụng các API sau để giải quyết nhiệm vụ:
# 
# 
# Nhìn chung, bạn có thể sử dụng các API trên để giải quyết nhiệm vụ.
# Ngoài ra, hãy tham khảo các ví dụ để giải quyết nhiệm vụ:
# 
# 
# Nhìn chung, bạn có thể tham khảo các ví dụ trên.

Khối mã:

Prompt#1

Prompt#3

Noise API

# Hãy tham khảo các ví dụ để giải quyết nhiệm vụ:
# 
# 
# Nhìn chung, bạn có thể tham khảo các ví dụ trên.

Prompt#2

Mô hình cơ sở

Mô hình cơ sở

Mô hình cơ sở

CodeGenAPI

Tiếp tục
tiền huấn luyện

Noise API

Trích xuất API

Lấy chi tiết của chúng

API Basic

Xáo trộn các API này

Hình 7: Tổng quan về việc chuẩn bị kho dữ liệu huấn luyện cho CodeGenAPI. Để đơn giản hóa hình ảnh, chúng tôi sử dụng màu sắc thay vì văn bản ở bên trái để minh họa cách thiết kế prompt ở bên phải. Chúng tôi sử dụng ■ và ■ để đại diện cho noise API, với định dạng của chúng nhất quán là ■ và ■, tương ứng.

ngữ cảnh trước khi gọi thông qua một prompt được thiết kế tốt.

APICoder có sẵn. Tạo dựa trên truy xuất tạo điều kiện cho khả năng tạo mạnh mẽ của các mẫu mới trong LLM thông qua việc cung cấp hướng dẫn giàu ngữ cảnh. Do đó, về mặt kỹ thuật, các mô hình tạo mã hiện có, như Codex [14], InCoder [24], và CODEGEN [41], có thể được sử dụng trực tiếp làm APICoder của chúng tôi bằng cách cung cấp thông tin API riêng tư phong phú.

Huấn luyện một APICoder tiên tiến. Các mô hình tạo mã có sẵn có thể tận dụng tạo dựa trên truy xuất để gọi các API riêng tư, nhưng vẫn có tiềm năng cải thiện đáng kể vì chúng chưa được huấn luyện rõ ràng về cách gọi API. Do đó, để tiếp tục nâng cao hiệu suất tạo mã của các thư viện riêng tư, chúng tôi đề xuất một ý tưởng thú vị để tiếp tục huấn luyện một APICoder tiên tiến. Để biến ý tưởng này thành hiện thực, chúng tôi cần một kho dữ liệu huấn luyện lớn bao gồm các khối mã cùng với thông tin API tương ứng của chúng. Tuy nhiên, kho GitHub hiện có thường thiếu thông tin API trước mỗi khối mã. Do đó, chúng tôi thêm thông tin API liên quan trước mỗi khối mã để tiếp tục tiền huấn luyện các mô hình có sẵn. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng CODEGEN [41] làm mô hình cơ sở vì nó được sử dụng rộng rãi trong các mô hình công khai có sẵn. Chúng tôi đặt tên mô hình được huấn luyện là CODEGENAPI. Đối với kho dữ liệu huấn luyện, trước tiên chúng tôi phân đoạn mỗi file Python 𝐩 ∈ 𝒟 thành 𝐾 khối mã (𝐩1, 𝐩2, ⋯, 𝐩𝐾), như đã làm trong APIFinder. Sau đó, chúng tôi trích xuất các API liên quan 𝒜𝑖 cho mỗi khối mã 𝐩𝑖. Cuối cùng, chúng tôi nối các API được trích xuất vào phía trước của mỗi khối mã để có được kho dữ liệu (𝒜1, 𝐩1, 𝒜2, 𝐩2, ⋯, 𝒜𝐾, 𝐩𝐾). Như đã nêu trong Phần 3.2, tài liệu API bao gồm các thành phần khác nhau như API Basics, Examples, và Parameters. Việc đặt tất cả các thành phần của mỗi API ở phía trước khối mã là không thực tế do độ dài quá mức. Do đó, như thể hiện trong Hình 7, chúng tôi thiết kế ba prompt để huấn luyện CODEGENAPI riêng biệt: API Basic, API Examples, và cả hai kết hợp. Hơn nữa, để cải thiện độ mạnh mẽ của CODEGENAPI, chúng tôi kết hợp noise API với xác suất 5% và xáo trộn các API này trong 𝒜, vì các API được trả về bởi APIFinder có thể chứa các API không chính xác và không có thứ tự. Trong quá trình huấn luyện CODEGENAPI, chúng tôi sử dụng một chiến lược lấy mẫu lại. Chiến lược này ưu tiên các file Python chất lượng cao trong việc lấy mẫu và giảm việc chọn lựa các file chất lượng thấp. Cụ thể, trọng số lấy mẫu lại 𝑤 của mỗi file Python có thể được định nghĩa là:

𝑤 = 𝑤api × 𝑤star × 𝑤ut,
𝑤api = 5.0 − log(𝑀api/𝑁api).clip(0, 5) × 0.2,
𝑤star = 1.0 + log(𝑁star + 1).clip(0, 5) × 0.2,
𝑤ut = (0.5 + (1 − 𝑅ut)).clip(0, 1), (2)

trong đó .clip(𝑥, 𝑦) hạn chế giá trị trong phạm vi [𝑥, 𝑦], 𝑁api biểu thị số lượng tên API trong file này, 𝑀api đại diện cho số lượng API trong trường hợp một tên API duy nhất khớp với nhiều API, 𝑁star biểu thị số lượng sao cho repository, và 𝑅ut là tỷ lệ hàm unit test, được tính bằng phép chia các hàm unit test cho tổng số hàm.

Suy luận. Cả APICoder có sẵn và APICoder tiên tiến của chúng tôi đều là các mô hình ngôn ngữ tạo dựa trên Transformer. Trong quá trình suy luận, APICoder của chúng tôi dự đoán phân phối xác suất của token tiếp theo dựa trên ngữ cảnh mã đã cho cho đến khi gặp một trong các dấu hiệu dừng được định nghĩa trước sau:

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 6 của 18

--- TRANG 8 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

"\nclass", "\ndef", "\nprint", "\n#", hoặc "\nif". Sau khi dự đoán phân phối xác suất, chúng tôi cần giải mã các xác suất này. Hiện tại, các mô hình ngôn ngữ tạo cung cấp nhiều kỹ thuật giải mã khác nhau, như giải mã nhiệt độ, giải mã tham lam, và giải mã nucleus. Trong các kịch bản tạo mã, giải mã nhiệt độ được sử dụng rộng rãi [14, 35] vì nó cho phép cân bằng phù hợp giữa tính đa dạng và chất lượng trong mã được tạo, trong khi các kỹ thuật giải mã khác không thể. Do đó, tất cả các thí nghiệm của chúng tôi sử dụng giải mã nhiệt độ để suy luận.

6. Xây dựng bộ đánh giá

Trong phát triển phần mềm, việc sử dụng các thư viện riêng tư là một thực hành phổ biến. Tuy nhiên, cho đến nay, chưa có bộ đánh giá nào cho các thư viện riêng tư được chế tạo. Một lý do chính cho điều này là việc chế tạo một bộ đánh giá toàn diện cho các thư viện riêng tư đặt ra nhiều thách thức đáng kể. Cụ thể, những thách thức này được thể hiện trong các khía cạnh sau: (1) đảm bảo rằng LLM chưa thấy thư viện trong quá trình huấn luyện, (2) cung cấp mã triển khai chất lượng cao và tài liệu API, (3) cung cấp các bài toán lập trình với độ khó khác nhau, và (4) chú thích các trường hợp kiểm tra toàn diện cho mỗi bài toán lập trình. Để giải quyết các thách thức trên, chúng tôi phát hành bốn bộ đánh giá cho các thư viện riêng tư, cụ thể là TorchDataEval, TorchDataComplexEval, MonkeyEval, và BeatNumEval.

TorchDataEval. Sau khi khảo sát nhiều thư viện mã, chúng tôi chọn TorchData4, được phát hành vào tháng 5 năm 2022, làm thư viện riêng tư của chúng tôi vì mã triển khai chất lượng cao và tài liệu API toàn diện. Quan trọng hơn, tất cả các mô hình trong thí nghiệm của chúng tôi, bao gồm Codex và CODEGEN, đều được tiền huấn luyện trên kho GitHub trước ngày phát hành nêu trên, đảm bảo rằng LLM không có sự tiếp xúc trước với thư viện. Trước khi chế tạo các bài toán lập trình, chúng tôi nghiên cứu kỹ lưỡng tài liệu API và thực hành từng API để đảm bảo thành thạo với thư viện. Sau đó, chúng tôi chế tạo thủ công 50 bài toán lập trình và cung cấp các trường hợp kiểm tra toàn diện cho mỗi bài. Ngoài ra, chúng tôi mời hai tình nguyện viên có hơn 3 năm kinh nghiệm lập trình Python để xem xét chúng nhằm đảm bảo tính đúng đắn và chính xác. Thực tế, mối lo ngại của chúng tôi là độ phức tạp của các thách thức lập trình có thể vượt quá khả năng của LLM. Do đó, chúng tôi kiểm soát mức độ khó của các bài toán lập trình này ở mức tương đối đơn giản. Cụ thể, các bài toán lập trình bao gồm 1 hoặc 2 API chiếm khoảng 90%.

TorchDataComplexEval. Đáng ngạc nhiên, các thí nghiệm của chúng tôi tiết lộ rằng LLM có thể hoạt động tương đối tốt trên TorchDataEval, cho thấy khả năng của chúng trong việc tạo một số đoạn mã riêng tư đơn giản. Do đó, một câu hỏi nghiên cứu nảy sinh: liệu LLM có thể đối phó với các thư viện riêng tư phức tạp hơn không? Do đó, chúng tôi phát triển một bộ đánh giá phức tạp hơn dựa trên TorchData, bao gồm 50 bài toán lập trình, có tên TorchDataComplexEval. Không giống như TorchDataEval, tạo ra các bài toán lập trình thủ công bằng cách tham khảo các ví dụ trong tài liệu API, TorchDataComplexEval trực tiếp thích ứng các dự án thực tế trên GitHub. Cụ thể, chúng tôi cẩn thận chuyển đổi các dự án thực tế thành các bài toán có thể thực thi và được chú thích tốt, cũng được trang bị các trường hợp kiểm tra toàn diện. Do tính phức tạp của các dự án này, các bài toán lập trình của TorchDataComplexEval thường liên quan đến một lượng lớn API, thường vượt quá 10. TorchData là một thư viện để xây dựng các pipeline dữ liệu có thể xử lý các modality dữ liệu đa dạng. Xem xét sự thiên vị tiềm năng giữa các modality này, TorchDataComplexEval bao gồm các modality phổ biến như văn bản, hình ảnh, và âm thanh. Chúng tôi cũng mời hai tình nguyện viên xem xét các bài toán lập trình này, như đã làm trong việc chế tạo TorchDataEval.

MonkeyEval & BeatNumEval. Như đã thảo luận ở trên, một thư viện riêng tư đủ tiêu chuẩn có các tiêu chí nghiêm ngặt. Vì vậy, việc khám phá một thư viện riêng tư phù hợp như TorchData tỏ ra cực kỳ thách thức. Do đó, ngoài TorchDataEval và TorchDataComplexEval, chúng tôi tạo ra hai bộ đánh giá thư viện giả riêng tư, có tên MonkeyEval và BeatNumEval, bằng cách thích ứng hai bộ công cộng có sẵn [63], có tên PandasEval và NumpyEval, mỗi bộ có 101 bài toán lập trình. Chúng tôi diễn giải thủ công các từ khóa trong PandasEval và NumpyEval. Như được mô tả trong Hình 1, chúng tôi diễn giải "pandas" thành "monkey", "dataframe" thành "knowledgeframe", và "isin" thành "iscontain". Chi tiết thêm về việc diễn giải được cung cấp trong Phụ lục A. Ngoài các từ khóa, chúng tôi cũng diễn giải kỹ lưỡng tài liệu API của chúng để đảm bảo rằng LLM chưa thấy chúng.

7. Thí nghiệm

Trong phần này, chúng tôi sẽ trình bày một đánh giá kỹ lưỡng về phương pháp được đề xuất của chúng tôi thông qua một loạt các thí nghiệm được thiết kế cẩn thận. Cụ thể, chúng tôi sẽ ban đầu phác thảo thiết lập thí nghiệm, sau đó đào sâu vào việc trình bày toàn diện các phát hiện của chúng tôi.

7.1. Thiết lập thí nghiệm

7.1.1. Baseline

Đóng góp của chúng tôi có thể được xem từ hai góc độ: APIFinder và APICoder. Đối với APIFinder, chúng tôi đề xuất một mô hình tạo tăng cường truy xuất cho việc tạo mã trong các thư viện riêng tư. Do đó, các baseline là tất cả các mô hình tạo mã với thiết lập No API, trong khi chúng tôi đề xuất các thiết lập Oracle, Top 𝐾, và Human. Từ góc độ APICoder, chúng tôi đề xuất một ý tưởng mới để xây dựng một mô hình tiên tiến bằng cách tiếp tục tiền huấn luyện mô hình cơ bản. Vì vậy, các mô hình cơ bản là baseline cho các phiên bản tiên tiến của chúng. Nhìn chung, chúng tôi thực hiện so sánh toàn diện giữa 17 mô hình tạo mã phổ biến, như CODEGEN [41], GPT-CC [19], InCoder [24], CodeGPT [38], CodeT5 [57], CodeParrot [27], SantaCoder [2], PyCodeGPT [63], PolyCoder [60], và code-davinci-002 của OpenAI [14]. Để ngắn gọn, chúng tôi viết tắt mô hình code-davinci-002 là Codex trong các phần sau.

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 7 của 18

--- TRANG 9 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Bảng 2
Pass@𝑘(%) của 17 mô hình tạo mã trên bốn bộ đánh giá thư viện riêng tư. No API, Oracle, TopN, và Human biểu thị sử dụng không có API, API hoàn hảo, API được truy xuất từ APIFinder, và API được chọn từ Top5 bởi con người như prompt bổ sung. B, E, và BE biểu thị sử dụng thông tin cơ bản của API (tên API, chữ ký, mô tả), ví dụ, và cả hai như prompt bổ sung. Các giá trị màu đỏ/xanh lá cây biểu thị cải thiện/giảm sút so với thiết lập No API.

APICoder APIFinder TorchDataEval TorchDataComplexEval MonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 pass@1 pass@10

CodeGPT 124M Top2/B 0.48 2.83 0.00 0.00 0.18 1.02 0.12 0.96
GPT-CC 125M Top2/B 0.00 0.00 0.00 0.00 0.04 0.34 0.12 0.92
GPT-CC 1.3B Top2/B 0.00 0.00 0.20 1.34 0.00 0.00 0.10 0.66
CodeParrot 110M Top2/B 1.96 9.32 0.08 0.76 0.55 1.84 2.69 10.56
CodeParrot 1.5B Top2/B 4.00 12.25 0.00 0.00 1.58 3.97 2.48 6.07
PyCodeGPT 110M Top2/B 4.36 16.66 2.12 6.19 2.26 9.10 5.80 19.17
CodeT5 770M Top2/B 16.62 33.47 3.60 5.95 4.16 14.88 9.60 24.23
PolyCoder 160M Top2/B 3.38 9.43 0.00 0.00 0.83 4.16 2.90 12.25
PolyCoder 400M Top2/B 3.60 12.57 0.00 0.00 0.99 3.70 4.70 16.34
PolyCoder 2.7B Top2/B 5.60 14.28 0.40 2.68 1.68 6.42 5.05 15.69
InCoder 1.3B Top2/B 6.00 15.59 0.80 3.62 3.56 13.93 5.35 16.66
InCoder 6.7B Top2/B 7.00 22.63 2.80 4.00 5.25 18.49 8.42 27.90
SantaCoder 1.1B Top2/B 19.20 32.62 2.40 4.00 4.16 12.91 11.68 22.77

CodeGen 350M No API 4.40 17.76 1.12 5.79 2.38 9.77 6.24 24.58
Oracle/B 7.22 2.82 28.53 10.77 1.36 0.24 6.52 0.73 3.29 0.91 11.38 1.61 11.19 4.95 28.92 4.34
Oracle/E 13.12 8.72 30.84 13.08 1.92 0.80 6.71 0.92 3.41 1.03 15.41 5.64 6.53 0.29 23.18 -0.78
Oracle/BE 12.06 7.66 31.66 13.90 1.76 0.64 8.62 2.83 2.65 0.27 11.61 1.84 7.52 1.28 24.81 0.23
Top1/B 6.04 1.64 21.73 3.97 1.28 0.16 6.21 0.42 3.37 0.99 11.02 1.25 8.87 2.63 24.28 -0.30
Top2/B 5.72 1.32 20.36 2.60 1.36 0.24 7.20 1.41 3.76 1.38 12.39 2.62 9.31 3.07 24.33 -0.25
Top2/E 6.66 2.26 16.39 -1.37 1.12 0.00 6.75 0.96 2.42 0.04 10.84 1.07 5.50 -0.74 16.37 -8.21
Top2/BE 5.44 1.04 18.10 0.34 1.44 0.32 7.66 1.87 1.99 -0.39 9.22 -0.55 4.28 -1.96 14.73 -9.85
Top3/B 6.28 1.88 22.06 4.30 1.20 0.08 7.49 1.70 3.84 1.46 11.61 1.84 9.07 2.83 25.82 1.24
Top5/B 7.04 2.64 25.01 7.25 1.12 0.00 6.63 0.84 4.08 1.70 12.74 2.97 8.93 2.69 26.26 1.68
Human/B 6.62 2.22 26.03 8.27 1.44 0.32 6.81 1.02 3.35 0.97 11.78 2.01 11.39 5.15 30.12 5.54

CodeGen 2B No API 8.80 20.92 5.60 13.24 4.65 12.47 8.71 30.21
Oracle/B 18.80 10.00 42.93 22.01 5.60 0.00 13.59 0.35 7.23 2.58 17.25 4.78 16.73 8.02 37.79 7.58
Oracle/E 23.40 14.60 41.96 21.04 7.00 1.40 18.49 5.25 5.74 1.09 14.08 1.61 10.69 1.98 26.46 -3.75
Oracle/BE 24.00 15.20 43.55 22.63 6.40 0.80 18.59 5.35 5.74 1.09 11.49 -0.98 9.50 0.79 30.82 0.61
Top1/B 10.20 1.40 27.98 7.06 5.60 0.00 13.61 0.37 6.93 2.28 17.87 5.40 13.07 4.36 31.58 1.37
Top2/B 13.00 4.20 35.04 14.12 5.80 0.21 3.78 0.54 8.42 3.77 20.12 7.65 12.97 4.26 30.82 0.61
Top2/E 17.00 8.20 30.74 9.82 5.80 0.21 3.23 -0.01 4.55 -0.10 12.92 0.45 7.52 -1.19 18.03 -12.18
Top2/BE 16.00 7.20 31.17 10.25 6.80 1.21 7.21 3.97 4.95 0.30 12.49 0.02 3.96 -4.75 10.22 -19.99
Top3/B 12.60 3.80 36.19 15.27 4.40 -1.21 2.61 -0.63 8.12 3.47 18.58 6.11 11.58 2.87 30.29 0.08
Top5/B 10.60 1.80 33.95 13.03 5.20 -0.41 3.60 -0.36 9.41 4.76 20.23 7.76 11.78 3.07 32.16 1.95
Human/B 12.20 3.40 29.51 8.59 5.88 0.28 13.60 0.36 7.13 2.48 17.01 4.54 16.24 7.53 35.77 5.56

CodeGen 6B No API 9.28 27.63 6.40 13.68 5.26 14.59 12.58 33.80
Oracle/B 24.72 15.44 47.32 19.69 7.24 0.84 13.90 0.22 8.53 3.27 20.10 5.51 17.82 5.24 40.17 6.37
Oracle/E 24.04 14.76 44.71 17.08 6.80 0.40 19.51 5.83 6.24 0.98 18.10 3.51 13.38 0.80 33.84 0.04
Oracle/BE 25.00 15.72 46.99 19.36 8.34 1.94 20.41 6.73 6.99 1.73 16.45 1.86 13.38 0.80 29.21 -4.59
Top1/B 19.34 10.06 32.53 4.90 6.96 0.56 12.55 -1.13 7.45 2.19 19.62 5.03 15.84 3.26 36.30 2.50
Top2/B 19.36 10.08 38.15 10.52 6.48 0.08 13.98 0.30 9.04 3.78 23.51 8.92 14.16 1.58 34.90 1.10
Top2/E 22.72 13.44 36.78 9.15 7.04 0.64 15.67 1.99 7.52 2.26 15.76 1.17 9.21 -3.37 24.75 -9.05
Top2/BE 21.60 12.32 34.04 6.41 7.90 1.50 20.40 6.72 7.04 1.78 16.94 2.35 8.19 -4.39 19.54 -14.26
Top3/B 22.10 12.82 36.81 9.18 5.76 -0.64 11.45 -2.23 8.79 3.53 22.93 8.34 13.55 0.97 34.64 0.84
Top5/B 18.20 8.92 32.27 4.64 6.49 0.09 13.96 0.28 10.56 5.30 24.16 9.57 14.09 1.51 35.11 1.31
Human/B 19.32 10.04 36.40 8.77 7.35 0.95 14.63 0.95 9.52 4.26 21.52 6.93 18.31 5.73 39.44 5.64

Codex No API 8.08 24.47 6.12 12.58 3.40 10.84 20.18 60.28
Oracle/B 44.10 36.02 73.07 48.60 9.02 2.90 19.31 6.73 15.93 12.53 45.60 34.76 31.54 11.36 68.65 8.37
Oracle/E 38.18 30.10 68.10 43.63 12.62 6.50 29.77 17.19 16.16 12.76 45.00 34.16 27.80 7.62 66.24 5.96
Oracle/BE 44.80 36.72 67.57 43.10 15.80 9.68 29.66 17.08 19.16 15.76 53.96 43.12 32.84 12.66 71.50 11.22
Top1/B 14.82 6.74 38.07 13.60 7.34 1.22 16.42 3.84 13.73 10.33 39.22 28.38 24.88 4.70 58.66 -1.62
Top2/B 19.06 10.98 49.70 25.23 6.78 0.66 15.60 3.02 15.96 12.56 41.76 30.92 27.09 6.91 60.76 0.48
Top2/E 23.94 15.86 46.26 21.79 11.12 5.00 21.17 8.59 14.98 11.58 41.41 30.57 20.97 0.79 57.75 -2.53
Top2/BE 24.82 16.74 51.43 26.96 13.32 7.20 21.64 9.06 15.69 12.29 42.40 31.56 20.26 0.08 54.96 -5.32
Top3/B 19.32 11.24 49.55 25.08 7.62 1.50 18.75 6.17 13.27 9.87 41.19 30.35 26.96 6.78 62.63 2.35
Top5/B 20.36 12.28 52.71 28.24 7.18 1.06 16.33 3.75 17.00 13.60 45.94 35.10 26.81 6.63 63.52 3.24
Human/B 15.24 7.16 40.68 16.21 6.98 0.86 18.57 5.99 14.97 11.57 39.88 29.04 29.19 9.01 64.25 3.97

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 8 của 18

--- TRANG 10 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

7.1.2. Các chỉ số đánh giá
Chúng tôi sử dụng pass@𝑘 làm chỉ số đánh giá của chúng tôi phù hợp với Codex [14]. Đối với mỗi bài toán, chúng tôi lấy mẫu 𝑛 = 100 đoạn mã ứng viên từ LLM. Sau đó, chúng tôi đếm số lượng 𝑐 đoạn mã chính xác bằng cách chạy trên các trường hợp kiểm tra. Pass@𝑘 có thể được hình thức hóa là:

pass@k = {
1 nếu 𝑛 − 𝑐 < 𝑘
1 − ∏ⁿᵢ₌ₙ₋ₓ₊₁(1 − 𝑘/𝑖) nếu không, (3)

trong đó 𝑘 ∈ {1, 10, 100} trong nghiên cứu của chúng tôi. Ngoài pass@𝑘, các chỉ số khác cũng tồn tại, như ROUGE [36], BLEU [43], và CodeBLEU [47]. Chúng tôi chọn pass@𝑘 làm chỉ số đánh giá thay vì các chỉ số khác vì nó có thể cung cấp một đánh giá hoàn toàn chính xác về độ chính xác của mã bằng cách thực thi các trường hợp kiểm tra, trong khi các chỉ số khác thì không.

7.1.3. Chi tiết triển khai
Chúng tôi sử dụng Dense5, một bộ công cụ để huấn luyện bộ truy xuất dày đặc, để triển khai APIFinder. Liên quan đến chi tiết huấn luyện của APIFinder, tỷ lệ mẫu tích cực và tiêu cực được đặt là 1:8, với batch size là 10 trên mỗi card GPU, learning rate là 1e-5 và tối ưu hóa thông qua thuật toán Adam [32]. Thời gian huấn luyện của chúng tôi là khoảng 74 giờ trên một cụm 8 GPU NVIDIA V100 (32GB), với tổng cộng 100K bước.

Đối với CODEGENAPI, chúng tôi huấn luyện tổng cộng 9 phiên bản với CODEGEN 350M, 2B, và 6B, mỗi phiên bản huấn luyện ba prompt được mô tả trong Hình 7. Các phiên bản này hoàn toàn tuân theo các siêu tham số gốc của CODEGEN để tiếp tục tiền huấn luyện. Chúng tôi sử dụng DeepSpeed [46] để huấn luyện CODEGENAPI với độ chính xác FP16 và sử dụng một cụm 32 GPU NVIDIA A100 (48GB). Trong quá trình suy luận của tất cả các mô hình APICoder, số lượng mẫu được đặt là 100, nhiệt độ là 0.8, độ dài tối đa của các chuỗi mới là 300, và top-p là 0.95.

7.1.4. Cấu hình thí nghiệm
API nào nên được đưa vào APICoder cho một bài toán lập trình? Trong thí nghiệm của chúng tôi, chúng tôi cung cấp bốn thiết lập sau:

• No API: không có API nào được đưa vào APICoder.
• Oracle: các API ground-truth được đưa vào APICoder.
• Top𝐾: 𝐾 API đầu tiên được truy xuất bởi APIFinder được đưa vào APICoder, trong đó 𝐾 ∈ {1, 2, 3, 5}.
• Human: APIFinder cung cấp top 5 API để người dùng lựa chọn (Hình 6), và các API được người dùng chọn được đưa vào APICoder.

Các thành phần nào trong tài liệu API nên được đặt trong prompt? Thực tế, chúng tôi phát hiện rằng một số thành phần trong tài liệu API, bao gồm tham số API và API liên quan, làm giảm đáng kể hiệu suất của LLM. Do đó, chúng tôi chỉ xem xét ba loại prompt sau:

5https://github.com/luyug/Dense

Bảng 3
Pass@𝑘(%) của các thành phần khác nhau của tài liệu API sử dụng CodeGen 350M trong thiết lập "Oracle".

API Doc. TorchDataEval TorchDataComplexEval
pass@1 pass@10 pass@1 pass@10

Basic 7.22 28.53 1.36 6.52
Examples 13.12 30.84 1.92 6.71
Parameters 3.95 14.13 0.85 3.22
Related APIs 4.15 14.48 0.86 4.46

• API Basic Only (B): prompt chỉ bao gồm API basic gồm tên API, chữ ký, và mô tả.
• API Examples Only (E): prompt chỉ bao gồm ví dụ API.
• API Basic and Examples (BE): prompt bao gồm cả API basic và ví dụ.

7.2. Kết quả chính

Trong phần này, chúng tôi tập trung vào bốn câu hỏi nghiên cứu thiết yếu (RQ) để đánh giá tính hiệu quả của phương pháp được đề xuất của chúng tôi trong kịch bản thư viện riêng tư.

RQ1: "Liệu các LLM có sẵn có tiềm năng gọi các API riêng tư không?" Điều này nhằm xác minh tính khả thi của LLM trong việc giải quyết kịch bản thư viện riêng tư. Cụ thể, chúng tôi cung cấp cho các LLM này các API riêng tư ground truth (oracle) và đánh giá khả năng gọi chúng một cách chính xác. Bảng 2 tiết lộ một cải thiện đáng kể khi cung cấp API oracle so với không có API, cho thấy tiềm năng của LLM để gọi các API riêng tư. Thú vị, ngay cả mô hình khổng lồ, codex, cũng hoạt động kém trong việc gọi các API riêng tư mà không prompt bất kỳ API nào, càng nhấn mạnh sự cần thiết của phương pháp của chúng tôi. Thú vị, khi kích thước tham số tăng (CODEGEN 350M < CODEGEN 2B < CODEGEN 6B < Codex), lợi ích từ việc prompt API tăng tương ứng. Ví dụ, Codex mang lại 48.60% pass@10 gain trong TorchDataEval ở thiết lập "Oracle/B", trong khi CODEGEN 350M chỉ đạt được 10.77% tăng.

RQ2: "Các thành phần nào trong tài liệu API hữu ích hơn cho LLM?" Như đã thể hiện trước đó, LLM có tiềm năng gọi các API riêng tư thông qua việc prompt thông tin API trong tài liệu API. Vậy, thông tin nào trong tài liệu API là quan trọng để tối đa hóa hiệu suất của LLM? Chúng tôi prompt riêng biệt từng thành phần được đề cập trong Phần 3.2 cho LLM. Kết quả trong Bảng 3 chứng minh rằng API basic và ví dụ có lợi hơn so với tham số API và API liên quan. Điều này hợp lý vì hai thành phần đầu trực tiếp cung cấp định nghĩa hoặc cách gọi cho API, trong khi hai thành phần sau thì không. Do đó, trong bài báo này, chúng tôi chủ yếu tập trung vào API basic và ví dụ.

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 9 của 18

--- TRANG 11 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Bảng 4
Pass@𝑘(%) của CodeGenAPI được đề xuất của chúng tôi trên bốn bộ đánh giá thư viện riêng tư. #1, #2, và #3 biểu thị CodeGenAPI được huấn luyện với ba prompt trong Hình 7. Các giá trị màu đỏ và xanh lá cây cho thấy cải thiện và suy giảm của CodeGenAPI so với CodeGen dưới cùng thiết lập, tương ứng.

APICoder P. APIFinder TorchDataEval TorchDataComplexEval MonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 pass@1 pass@10

CodeGenAPI
350M #1 Oracle/B 16.54 9.32 38.95 10.42 2.16 0.80 8.10 1.58 5.81 2.52 15.27 3.89 12.53 1.34 31.97 3.05
Top1/B 11.53 5.49 27.76 6.03 2.02 0.74 7.57 1.36 6.25 2.88 15.05 4.03 10.33 1.46 27.63 3.35
Top2/B 9.09 3.37 24.92 4.56 2.18 0.82 8.42 1.22 6.21 2.45 17.01 4.62 10.35 1.04 26.89 2.56
Top3/B 9.49 3.21 25.31 3.25 1.99 0.79 8.61 1.12 6.20 2.36 15.07 3.46 10.32 1.25 28.95 3.13
Top5/B 11.03 3.99 28.95 3.94 1.54 0.42 7.46 0.83 5.58 1.50 15.48 2.74 9.96 1.03 29.30 3.04
Human/B 14.97 8.35 35.86 9.83 2.58 1.14 8.75 1.94 6.30 2.95 16.45 4.67 12.92 1.53 33.56 3.44

#2 Oracle/E 26.64 13.52 47.16 16.32 3.17 1.25 8.66 1.95 6.55 3.14 20.56 5.15 6.58 0.05 24.14 -0.96
Top2/E 11.80 5.14 23.65 7.26 2.11 0.99 8.27 1.52 4.97 2.55 15.53 4.69 5.43 -0.07 16.62 0.25

#3 Oracle/BE 23.51 11.45 45.56 13.90 3.00 1.24 10.67 2.05 5.55 2.90 16.46 4.85 7.87 0.35 26.70 1.89
Top2/BE 10.28 4.84 23.26 5.16 2.50 1.06 9.62 1.96 3.03 1.04 12.33 3.11 4.42 0.14 16.31 1.58

CodeGenAPI
2B #1 Oracle/B 33.32 14.52 59.18 16.25 8.34 2.74 17.84 4.25 13.52 6.29 25.60 8.35 20.47 3.74 45.14 7.35
Top1/B 20.30 10.10 39.53 11.55 8.26 2.66 17.65 4.04 14.67 7.74 27.29 9.42 16.60 3.53 38.46 6.88
Top2/B 20.45 7.45 44.14 9.10 8.60 2.80 17.57 3.79 13.67 5.25 28.34 8.22 15.94 2.97 36.85 6.03
Top3/B 19.79 7.19 45.61 9.42 7.16 2.76 16.24 3.63 14.89 6.77 27.68 9.10 14.63 3.05 36.78 6.49
Top5/B 18.75 8.15 44.52 10.57 7.14 1.94 16.73 3.13 14.92 5.51 27.65 7.42 13.74 1.96 37.12 4.96
Human/B 24.37 12.17 45.07 15.56 8.73 2.85 18.22 4.62 14.17 7.04 25.71 8.70 20.00 3.76 42.05 6.28

#2 Oracle/E 39.54 16.14 61.05 19.09 11.02 4.02 23.48 4.99 13.36 7.62 23.29 9.21 12.14 1.45 27.50 1.04
Top2/E 26.24 9.24 43.19 12.45 9.71 3.91 17.26 4.03 11.19 6.64 20.49 7.57 7.37 -0.15 17.96 -0.07

#3 Oracle/BE 38.36 14.36 59.82 16.27 10.74 4.34 22.86 4.27 12.59 6.85 17.62 6.13 9.45 -0.05 33.23 2.41
Top2/BE 23.15 7.15 42.29 11.12 10.26 3.46 20.16 2.95 10.18 5.23 17.03 4.54 6.41 2.45 17.12 6.90

CodeGenAPI
6B #1 Oracle/B 45.23 20.51 71.41 24.09 13.26 6.02 23.54 9.64 17.57 9.04 32.67 12.57 25.16 7.34 53.04 12.87
Top1/B 34.69 15.35 49.67 17.14 13.42 6.46 21.85 9.30 18.97 11.52 34.48 14.86 21.93 6.09 45.75 9.45
Top2/B 33.98 14.62 53.92 15.77 11.74 5.26 22.23 8.25 18.50 9.46 37.44 13.93 20.02 5.86 44.49 9.59
Top3/B 35.35 13.25 51.34 14.53 12.01 6.25 18.78 7.33 19.36 10.57 35.74 12.81 19.57 6.02 43.15 8.51
Top5/B 32.47 14.27 47.02 14.75 12.75 6.26 19.73 5.77 20.07 9.51 35.76 11.60 19.21 5.12 42.67 7.56
Human/B 35.66 16.34 52.03 15.63 14.29 6.94 23.36 8.73 20.91 11.39 35.24 13.72 25.31 7.00 49.90 10.46

#2 Oracle/E 46.60 22.56 69.85 25.14 15.77 8.97 26.62 7.11 16.27 10.03 29.56 11.46 19.14 5.76 38.09 4.25
Top2/E 37.86 15.14 53.45 16.67 14.50 7.46 23.62 7.95 16.98 9.46 25.75 9.99 9.78 0.57 26.21 1.46

#3 Oracle/BE 45.46 20.46 66.54 19.55 16.76 8.42 28.41 8.00 17.56 10.57 29.56 13.11 13.38 0.00 30.73 1.52
Top2/BE 36.85 15.25 52.37 18.33 14.77 6.87 27.62 7.22 15.50 8.46 27.89 10.95 11.75 3.56 27.20 7.66

RQ3: "APIFinder có thể truy xuất hiệu quả các API hữu ích không?" Prompt LLM với API oracle có thể mở khóa tiềm năng gọi các API riêng tư. Tuy nhiên, việc cung cấp API oracle là không thực tế. Do đó, liệu APIFinder của chúng tôi có thể truy xuất các API hữu ích không? Bảng 2 cho thấy tất cả các mô hình với Top𝐊 API được truy xuất bởi APIFinder hoạt động tốt hơn so với thiết lập No API. Quan sát này chứng minh rằng APIFinder có khả năng truy xuất các API hữu ích. Hơn nữa, APIFinder với sự tham gia của con người thường thể hiện hiệu suất vượt trội bằng cách chọn thủ công các API có thể hữu ích. Đáng ngạc nhiên, thiết lập Top𝐊 hoặc Human đôi khi có thể vượt trội hơn thiết lập Oracle. Điều này có thể xuất phát từ các API nhiễu trong quá trình huấn luyện APICoder.

RQ4: "APICoder có thể gọi hiệu quả các API riêng tư không?" Bảng 2 cho thấy gần như tất cả các mô hình hiện có như CODEGEN thể hiện hiệu suất vượt trội trên bốn bộ đánh giá thư viện riêng tư với việc prompt API so với thiết lập No API. Quan sát như vậy chứng minh khả năng của APICoder có sẵn để gọi các API riêng tư. Mặc dù các mô hình này đã đạt được những tiến bộ đáng kể, vẫn còn chỗ để cải thiện hơn nữa, như được chỉ ra bởi các giá trị tương đối thấp. Để theo đuổi hiệu suất phi thường hơn, chúng tôi phát triển một mô hình tiên tiến hơn có tên CODEGENAPI thông qua việc tiếp tục tiền huấn luyện CODEGEN. Cụ thể, chúng tôi huấn luyện tổng cộng chín phiên bản, ba cho mỗi mô hình CODEGEN (350M, 2B, và 6B), sử dụng ba prompt như chi tiết trong Hình 7. Bảng 4 trình bày hiệu suất của CODEGENAPI trên bốn bộ đánh giá. Chúng tôi quan sát thấy CODEGENAPI liên tục vượt trội hơn CODEGEN. Điều này cho thấy rằng CODEGENAPI đã tăng cường khả năng gọi các API riêng tư thông qua việc huấn luyện trên 31 thư viện công cộng được thu thập của chúng tôi. Lưu ý rằng BeatNumEval cho thấy lợi ích tương đối hạn chế từ phương pháp của chúng tôi so với các bộ đánh giá khác. Sau phân tích toàn diện, chúng tôi phát hiện một số bài toán trong BeatNumEval không yêu cầu gọi API, như 'x[:,None]+y*8', trong khi phương pháp của chúng tôi chỉ hỗ trợ các lệnh gọi API được gọi rõ ràng, khiến nó không hiệu quả. Nhìn chung, một loạt các thí nghiệm rộng lớn tiết lộ rằng APICoder của chúng tôi sở hữu khả năng gọi các API riêng tư.

7.3. Nghiên cứu sâu

Trong phần này, chúng tôi sẽ đào sâu vào phân tích toàn diện về phương pháp được đề xuất của chúng tôi thông qua một loạt rộng các thí nghiệm, với mục tiêu cung cấp cho người đọc những hiểu biết có giá trị.

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 10 của 18

--- TRANG 12 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Top1 Top2 Top3 Top4 Top5
30
40
50
60
70
80
90
Tỷ lệ recall (%)

TorchDataEval
TorchDataComplexEval
MonkeyEval
BeatNumEval
PandasEval
NumpyEval

(a)

Human Top1 Top2 Top3
0
10
20
30
40
50
60
Độ chính xác của API (%)

TorchDataEval
TorchDataComplexEval
MonkeyEval
BeatNumEval

(b)

Hình 8: (a) Tỷ lệ recall (%) và (b) độ chính xác (%) của APIFinder trên các bộ đánh giá thư viện riêng tư dưới các thiết lập khác nhau.

7.3.1. Chất lượng của các API được truy xuất

Việc cung cấp các API chất lượng cao làm prompt cho APICoder là quan trọng để tạo ra các đoạn mã riêng tư; do đó chúng tôi muốn phân tích chất lượng của các API được truy xuất bởi APIFinder hoặc được chọn thủ công bởi người dùng. Chúng tôi phân tích tỷ lệ recall của APIFinder trên sáu bộ đánh giá, và kết quả được hiển thị trong Hình 8a. Chúng tôi quan sát thấy tỷ lệ recall của Top 5 vượt quá 50% trên tất cả các bộ đánh giá. Do đó, việc cung cấp top 5 API cho người dùng trong quá trình chọn lựa API là hợp lý (Hình 6). Đặc biệt, TorchDataEval, MonkeyEval, và BeatNumEval thể hiện tỷ lệ recall Top 5 gần 90%. Điều này chủ yếu do tài liệu API của chúng chứa số lượng API tương đối nhỏ và các bài toán lập trình của chúng tương đối đơn giản. Như thể hiện trong Hình 8b, chúng tôi so sánh độ chính xác của các API được chọn bởi người dùng với các API được truy xuất bởi APIFinder. Ở đây, chúng tôi định nghĩa độ chính xác là 1 nếu tất cả API trong bài toán lập trình được truy xuất, và 0 nếu không. Kết quả chứng minh rằng sự tham gia của người dùng với APIFinder có thể dẫn đến tác động tích cực đáng chú ý đến độ chính xác. Đồng thời, chúng tôi đã nhận thấy độ chính xác thấp hơn đáng kể của TorchDataComplexEval so với các bộ đánh giá khác, ngay cả với sự tham gia của con người, nơi nó chỉ đạt được 5% độ chính xác. Độ chính xác thấp như vậy làm nổi bật thách thức mà nó đặt ra.

Bảng 5
So sánh hiệu suất của APIFinder sử dụng dual-encoder và single-encoder trên hai bộ đánh giá thư viện riêng tư với Codex trong thiết lập "Top2/B".

APIFinder TorchDataEval TorchDataComplexEval
pass@1 pass@10 pass@1 pass@10

Dual-encoder 19.06 49.70 6.78 15.60
Single-encoder 19.73 50.63 6.46 15.98

7.3.2. Single-encoder vs. Dual-encoder

APIFinder sử dụng dual-encoder theo mặc định. Về mặt kỹ thuật, single-encoder cũng có thể được sử dụng trong APIFinder. Do đó, chúng tôi so sánh hiệu suất của single-encoder và dual-

2 4 6 8
CodeGen 350M

1 2 3 5 10 20
K trong TopK

10
15
20
25
Codex (Code 002)

TorchDataEval
TorchDataComplexEval
MonkeyEval
BeatNumEval

Hình 9: Pass@1(%) của CodeGen 350M và Codex trong các 𝐾 ∈ 1,2,3,5,10,20 khác nhau trong Top𝐾 với việc prompt API basic (thiết lập Top𝐾/B).

encoder trên TorchDataEval và TorchDataComplexEval (Bảng 5). Chúng tôi quan sát thấy single-encoder thể hiện một chút lợi thế so với dual-encoder về hiệu suất. Tuy nhiên, xem xét khoảng cách hiệu suất nhỏ và tốc độ suy luận, cuối cùng chúng tôi chọn dual-encoder làm mặc định như được nêu trong Phần 5.2.

7.3.3. 𝐾 khác nhau trong Top𝐾

Khi giá trị của 𝐾 trong Top𝐾 tăng, không chỉ tỷ lệ recall cải thiện, mà nhiễu được đưa vào APICoder cũng tăng, và ngược lại. Do đó, chúng tôi muốn xác định giá trị 𝐾 phù hợp nhất. Chi tiết, chúng tôi so sánh các thay đổi pass@1 của hai mô hình với kích thước khác nhau, dưới các giá trị 𝐾 khác nhau, trên bốn bộ đánh giá thư viện riêng tư. Như được mô tả trong Hình 9, thú vị khi làm nổi bật

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 11 của 18

--- TRANG 13 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Bảng 6
Nghiên cứu loại bỏ của CodeGenAPI 350M được huấn luyện với "prompt#1" trong thiết lập "Human/B". Tỷ lệ nhiễu mặc định là 5%.

APICoder TorchDataEval TorchDataComplexEval MonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 pass@1 pass@10

CodeGenAPI 350M 14.97 35.86 2.58 8.75 6.30 16.45 12.92 33.56
- với tỷ lệ nhiễu 0% 12.14 33.52 2.20 4.24 5.12 16.18 12.89 31.44
- với tỷ lệ nhiễu 10% 14.05 34.13 2.53 6.57 5.06 15.76 12.21 33.36
- với tỷ lệ nhiễu 20% 12.46 32.74 2.25 5.61 5.17 14.36 11.00 31.32
- w/o resampling 12.27 31.57 2.36 7.14 5.69 15.67 11.26 32.90

sự khác biệt tương phản trong độ nhạy cảm của hai mô hình đối với 𝐾. Khi 𝐾 vượt quá 5, hiệu suất của mô hình nhỏ CODEGEN 350M giảm sút trên tất cả các bộ đánh giá, trong khi mô hình lớn Codex cho thấy sự ổn định mà không có suy giảm. Quan sát như vậy ngụ ý rằng mô hình lớn thể hiện độ mạnh mẽ vượt trội đối với nhiễu quá mức so với mô hình nhỏ. Nhìn chung, việc lựa chọn 𝐾 phù hợp nên tính đến các yếu tố như tham số mô hình, hiệu suất mô hình, và đặc điểm nội tại của bộ đánh giá.

7.3.4. Kích thước mô hình khác nhau

Đây là một khái niệm được biết đến rộng rãi rằng khả năng nổi lên xuất hiện khi các tham số mô hình đủ [58]. Vì vậy, chúng tôi muốn điều tra ảnh hưởng của độ lớn tham số trong APICoder đối với hiệu suất trong các thư viện riêng tư. Cụ thể, chúng tôi so sánh pass@1 và pass@10 trên TorchDataEval và TorchDataComplexEval sử dụng 10 mô hình với kích thước khác nhau. Kết quả được mô tả trong Hình 10. Chúng ta có thể thấy rằng các mô hình lớn hơn thường dẫn đến hiệu suất cải thiện trong các thư viện riêng tư. Thật không may, hiệu suất vẫn còn ở mức thấp, ngay cả với số lượng tham số đáng kể. Ví dụ, mô hình khổng lồ, Codex (code-davinci-002), chỉ đạt được 15.60% pass@10 trên TorchDataComplexEval. Hiện tượng này cũng xác nhận thách thức đáng gờm của việc tạo mã hướng thư viện riêng tư. Hơn nữa, chúng tôi phân tích mối tương quan hiệu suất của 11 mô hình tạo mã trên cả HumanEval [14] và các thư viện riêng tư trong Hình 11, trong đó HumanEval hiện tại là bộ đánh giá phổ biến nhất để đánh giá khả năng tạo mã. Chúng tôi quan sát thấy các mô hình hoạt động tốt trên HumanEval cũng nổi bật trong các bộ đánh giá thư viện riêng tư. Do đó, chúng tôi tin rằng kịch bản thư viện riêng tư được đề xuất của chúng tôi sẽ được hưởng lợi từ tiến bộ nhanh chóng trong các kỹ thuật tạo mã chung.

7.3.5. Tỷ lệ nhiễu

Một tỷ lệ nhiễu được chọn cẩn thận là quan trọng để CODEGENAPI xử lý một loạt đa dạng các API. Nếu tỷ lệ nhiễu quá cao, nó sẽ làm xáo trộn phân phối ban đầu; ngược lại, nó sẽ mất khả năng xử lý các API nhiễu nếu quá thấp. Do đó chúng tôi muốn khám phá tác động của tỷ lệ nhiễu đối với APICoder. Tỷ lệ nhiễu mặc định cho CODEGENAPI là 5%, và chúng tôi cũng thí nghiệm với 0%, 10%, và 20% trong Bảng 6. Kết quả cho thấy 5% là lựa chọn tối ưu, với quá ít hoặc quá nhiều API nhiễu gây ra giảm hiệu suất.

0 2 5 8 -
0
5
10
15
20
TorchDataEval pass@1

0 2 5 8 -
0
10
20
30
40
50
TorchDataEval pass@10

0 2 5 8 -
Số lượng tham số (Tỷ)
0
2
4
6
TorchDataComplexEval pass@1

0 2 5 8 -
Số lượng tham số (Tỷ)
0
5
10
15
TorchDataComplexEval pass@10

CodeGPT
GPT-CC
CodeParrot
PyCodeGPT
CodeT5
PolyCoder
InCoder
SantaCoder
CodeGen
Codex

Hình 10: Kích thước tham số vs. pass@𝑘: phân tích so sánh của 10 mô hình phổ biến trên TorchDataEval và TorchDataComplexEval trong thiết lập "Top 2/B".

7.3.6. Chiến lược re-sampling

Trong quá trình tiếp tục tiền huấn luyện CODEGENAPI, chúng tôi sử dụng một chiến lược re-sampling. Như đã đề cập trong Phần 5.3, ý tưởng cốt lõi của chiến lược này là làm cho các file Python chất lượng cao dễ huấn luyện hơn và ngược lại. Để xác thực tính hiệu quả của chiến lược này, chúng tôi bỏ qua nó trong quá trình huấn luyện CODEGENAPI 350M, như thể hiện trong Bảng 6. Kết quả cho thấy một sự giảm sút liên tục trong hiệu suất, chứng minh tính hợp lệ của chiến lược re-sampling.

7.3.7. Độ khó khác nhau

APICoder có khả năng giải quyết các bài toán lập trình trong các thư viện riêng tư. Chúng tôi tò mò về mức độ khó khăn nào mà APICoder có thể giải quyết trong các bài toán lập trình thư viện riêng tư. Do đó, chúng tôi muốn đánh giá hiệu suất của APICoder với các bài toán có độ khó khác nhau thông qua đánh giá độ chính xác dưới số lượng API khác nhau. Cụ thể, chúng tôi tính toán độ chính xác của bốn mô hình trên số lượng API khác nhau trên một tập hợp kết hợp của bốn bộ đánh giá được phát hành của chúng tôi. Kết quả được minh họa trong Hình 12. Phát hiện của chúng tôi là

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 12 của 18

--- TRANG 14 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

0 20 40
0
5
10
15
20
TorchDataEval pass@1

0 20 40
0
10
20
30
40
50
TorchDataEval pass@10

0 20 40
HumanEval pass@1 (%)
0
2
4
6
TorchDataComplexEval pass@1

0 20 40
HumanEval pass@1 (%)
0
5
10
15
TorchDataComplexEval pass@10

CodeGPT 124M
GPT-CC 1.3B
CodeParrot 1.5B
PyCodeGPT 110M
CodeT5
PolyCoder
InCoder 6.7B
CodeGen 350M
CodeGen 2B
CodeGen 6B
Codex

Hình 11: Phân tích hiệu suất của 11 mô hình tạo mã trên HumanEval [14] và TorchDataEval (TorchDataComplexEval) trong thiết lập "Top 2/B".

0 5 10 15 18
Số lượng API
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Độ chính xác

CodeGen 350M
CodeGen 2B
CodeGenAPI 2B
Codex

Hình 12: Số lượng API vs. độ chính xác trên tất cả các bộ đánh giá thư viện riêng tư trong thiết lập "Oracle/B".

các mô hình lớn hơn sở hữu khả năng nâng cao để giải quyết các bài toán phức tạp. Ví dụ, Codex thậm chí giải quyết những bài toán lập trình bao gồm 8 API. Đồng thời, chúng tôi cũng quan sát thấy CODEGENAPI liên tục vượt trội hơn CODEGEN trên các số lượng API khác nhau. Ví dụ, CODEGENAPI 2B có khả năng giải quyết những bài toán lập trình bao gồm 5 API, trong khi CODEGEN 2B thì không. Quan sát như vậy chứng minh việc tiếp tục tiền huấn luyện CODEGENAPI của chúng tôi trên các thư viện công cộng thực sự nâng cao khả năng gọi các API riêng tư của nó.

7.3.8. Loại lỗi

Khi được cung cấp API Oracle, APICoder có thể giải quyết một số bài toán thư viện riêng tư, nhưng một phần đáng kể vẫn chưa được giải quyết. Chúng tôi tò mò về những lý do đằng sau những bài toán chưa được giải quyết này, liệu chúng có xuất phát từ việc thiếu lệnh gọi API (được gắn nhãn là Invalid) hay việc sử dụng API không chính xác (được gắn nhãn là Incorrect). Do đó, chúng tôi so sánh tỷ lệ passed, invalid, và incorrect của bốn mô hình sử dụng TorchDataEval trong Hình 13. Phát hiện của chúng tôi là các mô hình với hiệu suất vượt trội thể hiện tỷ lệ passed và incorrect cao hơn và tỷ lệ invalid thấp hơn. Điều này làm nổi bật rằng, một mặt, các mô hình hiệu suất vượt trội tạo ra mã ứng viên dễ dàng vượt qua các trường hợp kiểm tra hơn. Mặt khác, phần lớn lỗi trong những mô hình vượt trội này là do việc sử dụng API không chính xác, trong khi các mô hình kém thậm chí không thể gọi API. Phát hiện khác của chúng tôi là CODEGENAPI 2B vượt trội hơn CODEGEN 2B trong cả tỷ lệ passed và incorrect trong khi duy trì tỷ lệ invalid thấp hơn. Điều này cũng gián tiếp phản ánh rằng việc tiếp tục tiền huấn luyện từ CODEGEN đến CODEGENAPI thực sự nâng cao khả năng gọi các API riêng tư.

Passed Invalid Incorrect
0
10
20
30
40
50
60
70
Tỷ lệ phần trăm (%)

CodeGen 350M
CodeGen 2B
CodeGenAPI 2B
Codex

Hình 13: Phân tích so sánh tỷ lệ phần trăm passed, invalid, và incorrect của việc sử dụng API cho bốn mô hình trong TorchDataEval với thiết lập 'Oracle/B'. 'Invalid' đề cập đến khi mô hình không gọi API được prompt, trong khi 'Incorrect' biểu thị các trường hợp API được gọi, nhưng không được sử dụng đúng cách.

7.3.9. Thư viện công cộng

Về mặt kỹ thuật, phương pháp được đề xuất của chúng tôi cũng có thể được áp dụng cho các kịch bản công cộng. Như vậy, Bảng 7 thể hiện hiệu suất của ba mô hình trên PandasEval và NumpyEval. Kết quả cho thấy một sự giảm sút trong hiệu suất khi prompt các mô hình có sẵn với các API công cộng, như CODEGEN 2B và Codex. Một lý do có thể là việc prompt các API công cộng đã được nhìn thấy trước đó có thể làm xáo trộn dự đoán xác suất của mô hình. Ngoài ra, sự giảm sút rõ rệt hơn khi kích thước mô hình nhỏ hơn. Ví dụ, CODEGEN 2B thấy một sự giảm 35% pass@10 trên NumpyEval trong thiết lập "Top 2/BE". Đáng ngạc nhiên, CODEGENAPI 2B mang lại một lợi ích hiệu suất. Ví dụ, CODEGENAPI 2B trải qua khoảng 6% tăng pass@1 trên NumpyEval trong thiết lập "Oracle/B". Điều này một lần nữa làm nổi bật rằng CODEGENAPI của chúng tôi sở hữu khả năng gọi các API được prompt một cách hiệu quả.

8. Thảo luận và hạn chế

Trong phần này, chúng tôi sẽ đào sâu vào một số thảo luận kích thích tư duy về những hạn chế của bài báo chúng tôi. (1) Như đã đề cập trong Phần 6, việc tạo ra một bộ đánh giá thư viện riêng tư thực sự đặt ra một thách thức đáng kể. Do đó, ngoài TorchDataEval và TorchDataComplexEval, chúng tôi cũng tạo ra hai bộ đánh giá thư viện giả riêng tư từ các bộ công cộng. Mặc dù nỗ lực tốt nhất của chúng tôi để sửa đổi các thư viện công cộng thành riêng tư bằng cách diễn giải các từ khóa và tài liệu API, vẫn tồn tại rủi ro tiềm ẩn đối với tính hợp lệ và công bằng của đánh giá thư viện riêng tư. Vì vậy, việc thu thập thêm các thư viện riêng tư thực tế và các nhiệm vụ lập trình tương ứng trong công việc tương lai là một nỗ lực đáng giá. (2) Như đã nêu trong Phần 7.2, phương pháp được đề xuất của chúng tôi mang lại hiệu suất vượt trội trên những mô hình với tham số lớn hơn. Tương tự, nếu mô hình có ít tham số hơn, lợi ích mang lại bởi phương pháp của chúng tôi có thể bị hạn chế hoặc thậm chí không hiệu quả. Do đó, phương pháp của chúng tôi tương đối nhạy cảm với khả năng của chính mô hình cơ sở. (3) Là những người khám phá sớm trong lĩnh vực tạo mã hướng thư viện riêng tư, các thư viện riêng tư được xây dựng của chúng tôi thường có số lượng API tương đối khiêm tốn (khoảng 200). Trong trường hợp này, APIFinder có thể truy xuất một số API hữu ích. Tuy nhiên, khi số lượng API tăng, thách thức mà APIFinder đối mặt có thể được khuếch đại. Ngay cả trong các bộ đánh giá của chúng tôi với số lượng API tương đối ít hơn, hiệu suất của APIFinder vẫn tụt hậu đáng kể so với thiết lập "Oracle" (Bảng 2). Điều này tiết lộ không gian rộng lớn để cải thiện trong APIFinder. (4) Việc cung cấp ví dụ API cho APICoder có thể tạo ra một thiên vị nhỏ trong việc đánh giá TorchDataEval, vì việc xây dựng TorchDataEval cũng tham khảo các ví dụ API trong tài liệu API. (5) Không thể tránh khỏi, bài báo của chúng tôi đòi hỏi một lượng lớn tài nguyên tính toán. Do đó, chúng tôi sẽ công khai phát hành các file được tạo bởi LLM để thúc đẩy nghiên cứu thêm. (6) Phương pháp của chúng tôi chỉ tập trung vào Python. Khi ngoại suy sang các ngôn ngữ lập trình khác, một số mối đe dọa tiềm ẩn có thể tồn tại do sự khác biệt tinh tế giữa chúng. (7) Một số mô hình tạo mã mạnh mẽ, như PaLM-Coder [15], PanGu-Coder [16], và AlphaCode [35], không được công khai, điều này ngăn cản chúng tôi đưa chúng vào các thí nghiệm của mình. Để giải quyết điều này, chúng tôi đã nỗ lực hết sức để chạy tất cả các mô hình có thể truy cập được liệt kê trong Bảng 2, nhằm có được kết quả đáng tin cậy nhất có thể. (8) Về mặt thực tế, một ý tưởng thú vị là chuyển đổi phương pháp của chúng tôi thành một trợ lý lập trình để hỗ trợ các nhà phát triển sử dụng tốt hơn các thư viện riêng tư, vì các thư viện riêng tư là một hiện tượng phổ biến trong các kịch bản lập trình thường ngày. Xem xét các mối quan tâm về quyền riêng tư và bảo mật tiềm ẩn, điều này vẫn là một chủ đề cho nghiên cứu tương lai.

9. Kết luận

Trong bài báo này, chúng tôi đề xuất một kịch bản mới cho việc tạo mã tập trung vào các thư viện riêng tư. Để giải quyết kịch bản này, chúng tôi thiết kế một khung công tác bằng cách mô phỏng quá trình con người sử dụng các thư viện riêng tư, bao gồm hai mô-đun: APIFinder và APICoder. APIFinder trước tiên truy xuất các API liên quan từ tài liệu API, và APICoder sau đó sử dụng các API này để giải quyết các bài toán lập trình. Ngoài ra, chúng tôi chế tạo bốn bộ đánh giá thư viện riêng tư, bao gồm TorchDataEval, TorchDataComplexEval, MonkeyEval, và BeatNumEval. Cuối cùng, chúng tôi thực hiện các thí nghiệm rộng rãi trên bốn bộ đánh giá, thể hiện những điểm mạnh và hạn chế của phương pháp của chúng tôi, có thể cung cấp một số hiểu biết có ý nghĩa cho công việc tương lai. Tiến lên phía trước, mục tiêu của chúng tôi là đóng gói phương pháp của chúng tôi như một công cụ hỗ trợ được thiết kế để tạo điều kiện cho quá trình lập trình cho các lập trình viên. Mục tiêu này đặt ra một số câu hỏi nghiên cứu thách thức nhưng đầy hứa hẹn. Ví dụ, làm thế nào để đảm bảo quyền riêng tư và bảo mật khi sử dụng LLM? Làm thế nào để giải quyết kịch bản sử dụng hỗn hợp các thư viện công cộng và riêng tư? Làm thế nào chúng ta có thể đo lường và tăng cường niềm tin mà các nhà phát triển đặt vào mã được tạo? Làm thế nào để thiết kế một giao diện người dùng hiệu quả cho phép các lập trình viên tương tác mượt mà với công cụ? Và ngoài những điều này, còn có vô số câu hỏi thú vị khác trong lĩnh vực này đang chờ khám phá.

A. Chuyển đổi từ khóa từ thư viện công cộng sang riêng tư

Chúng tôi chuyển đổi thủ công các thư viện công cộng thành riêng tư bằng cách diễn giải tất cả các từ khóa liên quan trong Phần 6. Bảng 8 liệt kê tất cả các từ khóa trước và sau khi chuyển đổi PandasEval (NumpyEval) sang MonkeyEval (BeatNumEval).

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 13 của 18

--- TRANG 15 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Bảng 7
Pass@𝑘(%) của CodeGen 2B, CodeGenAPI 2B và Codex trên hai bộ đánh giá thư viện công cộng. #1, #2, và #3 đại diện cho CodeGenAPI được huấn luyện với ba prompt trong Hình 7. Các giá trị màu đỏ và xanh lá cây đại diện cho cải thiện và suy giảm so với thiết lập No API.

APICoder P. APIFinder PandasEval NumpyEval
pass@𝑘
k=1 k=10 k=1 k=10

CodeGen
2B - No API 37 63 38 66
Oracle/B -5 +4 -2 -3 -1 +9
Oracle/E -8 -4 -9 -14 -17
Oracle/BE -12 -13 -11 -19
Top1/B -9 -8 -3 -13
Top2/B -10 -10 -5 -10
Top2/E -14 -2 -16 -27
Top2/BE -18 -18 -15 -35
Top3/B -9 -6 -3 -9
Top5/B +7 +3 -1 -11

CodeGenAPI
2B #1 Oracle/B +4 +8 +6 +3
Top1/B +1 +2 +2 0
Top2/B +2 +2 +3 +2
Top3/B +1 +1 +3 +1
Top5/B 0 +1 +2 +2

#2 Oracle/E +5 +6 +6 +15
Top2/E 0 +1 -4 -15

#3 Oracle/BE +4 +6 +7 +4
Top2/BE -2 +1 -3 -12

Codex
(Code 002) - No API 54 83 62 91
Oracle/B -2 -3 -1 -4
Oracle/E -3 -1 -1 -3
Oracle/BE -4 -3 -2 -3
Top1/B -7 -7 -6 -6
Top2/B -6 -1 -5 -5
Top2/E -11 -1 -9 -6
Top2/BE -9 -8 -13 -29
Top3/B -6 0 -10 -6
Top5/B -9 0 -9 -6

tioned trong Phần 7.2, phương pháp được đề xuất của chúng tôi mang lại hiệu suất vượt trội trên những mô hình với tham số lớn hơn. Tương tự, nếu mô hình có ít tham số hơn, lợi ích mang lại bởi phương pháp của chúng tôi có thể bị hạn chế hoặc thậm chí không hiệu quả. Do đó, phương pháp của chúng tôi tương đối nhạy cảm với khả năng của chính mô hình cơ sở. (3) Là những người khám phá sớm trong lĩnh vực tạo mã hướng thư viện riêng tư, các thư viện riêng tư được xây dựng của chúng tôi thường có số lượng API tương đối khiêm tốn (khoảng 200). Trong trường hợp này, APIFinder có thể truy xuất một số API hữu ích. Tuy nhiên, khi số lượng API tăng, thách thức mà APIFinder đối mặt có thể được khuếch đại. Ngay cả trong các bộ đánh giá của chúng tôi với số lượng API tương đối ít hơn, hiệu suất của APIFinder vẫn tụt hậu đáng kể so với thiết lập "Oracle" (Bảng 2). Điều này tiết lộ không gian rộng lớn để cải thiện trong APIFinder. (4) Việc cung cấp ví dụ API cho APICoder có thể tạo ra một thiên vị nhỏ trong việc đánh giá TorchDataEval, vì việc xây dựng TorchDataEval cũng tham khảo các ví dụ API trong tài liệu API. (5) Không thể tránh khỏi, bài báo của chúng tôi đòi hỏi một lượng lớn tài nguyên tính toán. Do đó, chúng tôi sẽ công khai phát hành các file được tạo bởi LLM để thúc đẩy nghiên cứu thêm. (6) Phương pháp của chúng tôi chỉ tập trung vào Python. Khi ngoại suy sang các ngôn ngữ lập trình khác, một số mối đe dọa tiềm ẩn có thể tồn tại do sự khác biệt tinh tế giữa chúng. (7) Một số mô hình tạo mã mạnh mẽ, như PaLM-Coder [15], PanGu-Coder [16], và AlphaCode [35], không được công khai, điều này ngăn cản chúng tôi đưa chúng vào các thí nghiệm của mình. Để giải quyết điều này, chúng tôi đã nỗ lực hết sức để chạy tất cả các mô hình có thể truy cập được liệt kê trong Bảng 2, nhằm có được kết quả đáng tin cậy nhất có thể. (8) Về mặt thực tế, một ý tưởng thú vị là chuyển đổi phương pháp của chúng tôi thành một trợ lý lập trình để hỗ trợ các nhà phát triển sử dụng tốt hơn các thư viện riêng tư, vì các thư viện riêng tư là một hiện tượng phổ biến trong các kịch bản lập trình thường ngày. Xem xét các mối quan tâm về quyền riêng tư và bảo mật tiềm ẩn, điều này vẫn là một chủ đề cho nghiên cứu tương lai.

9. Kết luận

Trong bài báo này, chúng tôi đề xuất một kịch bản mới cho việc tạo mã tập trung vào các thư viện riêng tư. Để giải quyết kịch bản này, chúng tôi thiết kế một khung công tác bằng cách mô phỏng quá trình con người sử dụng các thư viện riêng tư, bao gồm hai mô-đun: APIFinder và APICoder. APIFinder trước tiên truy xuất các API liên quan từ tài liệu API, và APICoder sau đó sử dụng các API này để giải quyết các bài toán lập trình. Ngoài ra, chúng tôi chế tạo bốn bộ đánh giá thư viện riêng tư, bao gồm TorchDataEval, TorchDataComplexEval, MonkeyEval, và BeatNumEval. Cuối cùng, chúng tôi thực hiện các thí nghiệm rộng rãi trên bốn bộ đánh giá, thể hiện những điểm mạnh và hạn chế của phương pháp của chúng tôi, có thể cung cấp một số hiểu biết có ý nghĩa cho công việc tương lai. Tiến lên phía trước, mục tiêu của chúng tôi là đóng gói phương pháp của chúng tôi như một công cụ hỗ trợ được thiết kế để tạo điều kiện cho quá trình lập trình cho các lập trình viên. Mục tiêu này đặt ra một số câu hỏi nghiên cứu thách thức nhưng đầy hứa hẹn. Ví dụ, làm thế nào để đảm bảo quyền riêng tư và bảo mật khi sử dụng LLM? Làm thế nào để giải quyết kịch bản sử dụng hỗn hợp các thư viện công cộng và riêng tư? Làm thế nào chúng ta có thể đo lường và tăng cường niềm tin mà các nhà phát triển đặt vào mã được tạo? Làm thế nào để thiết kế một giao diện người dùng hiệu quả cho phép các lập trình viên tương tác mượt mà với công cụ? Và ngoài những điều này, còn có vô số câu hỏi thú vị khác trong lĩnh vực này đang chờ khám phá.

A. Chuyển đổi từ khóa từ thư viện công cộng sang riêng tư

Chúng tôi chuyển đổi thủ công các thư viện công cộng thành riêng tư bằng cách diễn giải tất cả các từ khóa liên quan trong Phần 6. Bảng 8 liệt kê tất cả các từ khóa trước và sau khi chuyển đổi PandasEval (NumpyEval) sang MonkeyEval (BeatNumEval).

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 14 của 18

--- TRANG 16 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Bảng 8
Chuyển đổi từ khóa từ PandasEval (NumpyEval) sang MonkeyEval (BeatNumEval).

PandasEval - MonkeyEval
df Pandas pandas len tolist isin sort_index
kf Monkey monkey length convert_list incontain sorting_index
isnull apply to_numeric dropna append tail value_counts
ifnull employ to_num sipna adding last_tail counts_value_num
innull astype select_dtypes iterrows min max drop_duplicates
isnone totype choose_dtypes traversal get_min get_max remove_duplicates
pd shift merge copy rename_axis reset_index sample
mk shifting unioner clone renaming_axis reseting_index sample_by_num
concat to_dict cumsum last to_string applymap duplicated
concating convert_dict cumulative_sum final_item convert_string conduct_map duplicated_values
isna format div mean ceil assign DataFrame
ifna formating division average ceiling allocate KnowledgeFrame
drop Series ravel any fillna all to_pydatetime
sip Collections flat_underlying whatever fillnone total_all convert_pydatetime
reindex head sort_values rename sum unique to_datetime
reindexing header_num sort_the_values renaming total_sum distinctive convert_datetime
map std intersection groupby nlargest replace dataframe
mapping standard interst grouper nbiggest replacing knowledgeframe
get series round
getting collections value_round

NumpyEval - BeatNumEval
np Numpy array unique ndarray transpose reshape
bn Beatnum numset uniq ndnumset switching_places change_shape_to
real numpy vstack sum imag in1d flatten
reality beatnum vertical_stack total_count imaginary intersection1dim convert_into_one_dim
isnan all fromstring inv mean where compressed
ifnan total come_from_str inverse average filter_condition remove_masked_data
add max histogram to_numpy filled stack cumsum
add_concat get_max hist_operation to_beatnum masked_fill pile_operation cumulative_sum
insert arange ravel std argmax argmin full
stick arr_range asview standard_op get_argmax get_argmin_value full_value_func
slice squeeze hstack asarray repeat bincount unravel_index
piece sqz horizontal_stack asnumset duplicate binoccurrence convert_index_or_arr
diff concatenate any column_stack norm delete logical_and
difference connect any_condition stack_col normlizattion remove_operation logic_and_element_wise
append split ones vectorize fill_diagonal argpartition setxor1d
apd sep_split create_ones vectorisation pad_diagonal perform_partition seting_exclusive_or_one_dim
array_split abs astype searchsorted min fromarrays
split_array absolute convert_type find_sorted get_min come_from_arrays

Tuyên bố đóng góp tác giả CRediT

Daoguang Zan: Ý tưởng, Thu thập dữ liệu, Phân tích hình thức, Điều tra, Phương pháp, Tài nguyên, Viết - bản thảo gốc. Bei Chen: Ý tưởng, Phân tích hình thức, Phương pháp, Giám sát, Viết - bản thảo gốc. Yongshun Gong: Ý tưởng, Phương pháp, Xác thực. Junzhi Cao: Ý tưởng, Phương pháp, Xác thực. Fengji Zhang: Thu thập dữ liệu, Phân tích hình thức, Điều tra. Bingchao Wu: Tài nguyên, Trực quan hóa. Bei Guan: Phương pháp, Xác thực. Yilong Yin: Ý tưởng, Phương pháp, Giám sát, Quản lý dự án. Yongji Wang: Ý tưởng, Phương pháp, Giám sát, Quản lý dự án.

Tài liệu tham khảo

[1] Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2021. Unified pre-training for program understanding and generation, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2655–2668.

[2] Allal, L.B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C.M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., Umapathi, L.K., Anderson, C.J., Zi, Y., Poirier, J., Schoelkopf, H., Troshin, S.M., Abulkhanov, D., Romero, M., Lappert, M.F., Toni, F.D., del R'io, B.G., Liu, Q., Bose, S., Bhattacharyya, U., Zhuo, T.Y., Yu, I., Villegas, P., Zocca, M., Mangrulkar, S., Lansky, D., Nguyen, H., Contractor, D., Villa, L., Li, J., Bahdanau, D., Jernite, Y., Hughes, S.C., Fried, D., Guha, A., de Vries, H., von Werra, L., 2023. SantaCoder: don't reach for the stars! ArXiv abs/2301.03988.

[3] Alrubaye, H., Mkaouer, M.W., Khokhlov, I., Reznik, L., Ouni, A., Mcgoff, J., 2020. Learning to recommend third-party library migration opportunities at the api level. Applied Soft Computing 90, 106140.

[4] Athiwaratkun, B., Gouda, S.K., Wang, Z., Li, X., Tian, Y., Tan, M., Ahmad, W.U., Wang, S., Sun, Q., Shang, M., Gonugondla, S.K., Ding, H., Kumar, V., Fulton, N., Farahani, A., Jain, S., Giaquinto, R., Qian, H., Ramanathan, M.K., Nallapati, R., Ray, B., Bhatia, P., Sengupta, S., Roth, D., Xiang, B., 2022. Multi-lingual evaluation of code generation models. ArXiv abs/2210.14868.

[5] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C.J., Terry, M., Le, Q.V., Sutton, C., 2021. Program synthesis with large language models. ArXiv abs/2108.07732.

[6] Bauer, V., Heinemann, L., Deissenboeck, F., 2012. A structured approach to assess third-party library usage, in: 2012 28th IEEE International Conference on Software Maintenance (ICSM), IEEE. pp. 483–492.

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 15 của 18

--- TRANG 17 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

[7] Bavarian, M., Jun, H., Tezak, N.A., Schulman, J., McLeavey, C., Tworek, J., Chen, M., 2022. Efficient training of language models to fill in the middle. ArXiv abs/2207.14255.

[8] Black, S., Gao, L., Wang, P., Leahy, C., Biderman, S., 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. URL: https://doi.org/10.5281/zenodo.5297715 , doi: 10.5281/zenodo.5297715 .

[9] Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S.D., Phipps-Costin, L., Pinckney, D., Yee, M.H., Zi, Y., Anderson, C.J., Feldman, M.Q., Guha, A., Greenberg, M., Jangda, A., 2022. A scalable and extensible approach to benchmarking nl2code for 18 programming languages. ArXiv abs/2208.08227.

[10] Chai, Y., Wang, S., Pang, C., Sun, Y., Tian, H., Wu, H., 2022. ERNIE-Code: Beyond english-centric cross-lingual pretraining for programming languages. arXiv preprint arXiv:2212.06742 .

[11] Chandel, S., Clement, C.B., Serrato, G., Sundaresan, N., 2022a. Training and evaluating a jupyter notebook data science assistant. ArXiv abs/2201.12901.

[12] Chandel, S., Clement, C.B., Serrato, G., Sundaresan, N., 2022b. Training and evaluating a jupyter notebook data science assistant. arXiv preprint arXiv:2201.12901 .

[13] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.G., Chen, W., 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 .

[14] Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.P., Cummings, D.W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Babuschkin, I., Balaji, S.A., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M.M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W., 2021. Evaluating large language models trained on code. ArXiv abs/2107.03374.

[15] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.M., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.C., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., García, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Díaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.S., Eck, D., Dean, J., Petrov, S., Fiedel, N., 2022. PaLM: Scaling language modeling with pathways. ArXiv abs/2204.02311.

[16] Christopoulou, F., Lampouras, G., Gritta, M., Zhang, G., Guo, Y., Li, Z.Y., Zhang, Q., Xiao, M., Shen, B., Li, L., Yu, H., yuYan, L., Zhou, P., Wang, X., Ma, Y., Iacobacci, I., Wang, Y., Liang, G., Wei, J., Jiang, X., Wang, Q., Liu, Q., 2022. PanGu-Coder: Program synthesis with function-level language modeling. ArXiv abs/2207.11280.

[17] Clement, C.B., Drain, D., Timcheck, J., Svyatkovskiy, A., Sundaresan, N., 2020. PyMT5: Multi-mode translation of natural language and python code with transformers, in: Conference on Empirical Methods in Natural Language Processing, pp. 9052–9065.

[18] Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., Schulman, J., 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 .

[19] CodedotAl, 2021. GPT Code Clippy: The Open Source version of GitHub Copilot. https://github.com/CodedotAl/gpt-code-clippy .

[20] Derr, E., Bugiel, S., Fahl, S., Acar, Y., Backes, M., 2017. Keep me updated: An empirical study of third-party library updatability on android, in: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 2187–2200.

[21] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of deep bidirectional transformers for language understanding, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186.

[22] Ding, Y., Wang, Z., Ahmad, W.U., Ramanathan, M.K., Nallapati, R., Bhatia, P., Roth, D., Xiang, B., 2022. CoCoMIC: Code completion by jointly modeling in-file and cross-file context. arXiv preprint arXiv:2212.10007 .

[23] Formal, T., Lassance, C., Piwowarski, B., Clinchant, S., 2022. From distillation to hard negative sampling: Making sparse neural ir models more effective. arXiv preprint arXiv:2205.04733 .

[24] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., Lewis, M., 2023. InCoder: A generative model for code infilling and synthesis, in: The Eleventh International Conference on Learning Representations.

[25] Gu, X., Zhang, H., Zhang, D., Kim, S., 2016. Deep api learning, in: ACM SIGSOFT International Symposium on Foundations of Software Engineering, pp. 631–642.

[26] Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D.X., Steinhardt, J., 2021. Measuring coding challenge competence with apps, in: Neural Information Processing Systems.

[27] Huggingface, 2021. Training CodeParrot from Scratch. https://huggingface.co/blog/codeparrot .

[28] Izacard, G., Grave, E., 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282 .

[29] Jain, N., Vaidyanath, S., Iyer, A.S., Natarajan, N., Parthasarathy, S., Rajamani, S.K., Sharma, R., 2021. Jigsaw: Large language models meet program synthesis. 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE) , 1219–1231.

[30] Johnson, J., Douze, M., Jégou, H., 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 535–547.

[31] Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih, W.t., 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 .

[32] Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .

[33] Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, S., Fried, D., yi Wang, S., Yu, T., 2022. DS-1000: A natural and reliable benchmark for data science code generation. ArXiv abs/2211.11501.

[34] Li, R., Allal, L.B., Zi, Y., Muennighoff, N., et al., 2023. StarCoder: May the source be with you! .

[35] Li, Y., Choi, D.H., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Tom, Eccles, Keeling, J., Gimeno, F., Lago, A.D., Hubert, T., Choy, P., de, C., d'Autume, M., Babuschkin, I., Chen, X., Huang, P.S., Welbl, J., Gowal, S., Alexey, Cherepanov, Molloy, J., Mankowitz, D.J., Robson, E.S., Kohli, P., de, N., Freitas, Kavukcuoglu, K., Vinyals, O., 2022. Competition-level code generation with alphacode. Science 378, 1092 – 1097.

[36] Lin, C.Y., 2004. ROUGE: A package for automatic evaluation of summaries, in: Text summarization branches out, pp. 74–81.

[37] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S.w., Svyatkovskiy, A., 2022. ReACC: A retrieval-augmented code completion framework. arXiv preprint arXiv:2203.07722 .

[38] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., et al., 2021. CodeXGLUE: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 .

[39] Molnar, D., Wagner, D., 2004. Privacy and security in library rfid: Issues, practices, and architectures, in: Proceedings of the 11th ACM conference on Computer and communications security, pp. 210–219.

[40] Nguyen, A., Karampatziakis, N., Chen, W., 2023. Meet in the middle: A new pre-training paradigm. arXiv:2303.07295 .

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 16 của 18

--- TRANG 18 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

[41] Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., Xiong, C., 2023. CodeGen: An open large language model for code with multi-turn program synthesis, in: The Eleventh International Conference on Learning Representations.

[42] OpenAI, 2023. Gpt-4 technical report. arXiv:2303.08774 .

[43] Papineni, K., Roukos, S., Ward, T., Zhu, W.J., 2002. BLEU: a method for automatic evaluation of machine translation, in: Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311–318.

[44] Parvez, M.R., Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2021. Retrieval augmented code generation and summarization, in: Findings of EMNLP, pp. 2719–2734.

[45] Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W.X., Dong, D., Wu, H., Wang, H., 2020. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191 .

[46] Rajbhandari, S., Rasley, J., Ruwase, O., He, Y., 2019. Zero: Memory optimization towards training A trillion parameter models. CoRR abs/1910.02054. URL: http://arxiv.org/abs/1910.02054 , arXiv:1910.02054 .

[47] Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., Ma, S., 2020. CodeBLEU: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 .

[48] Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., Zaharia, M., 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488 .

[49] Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A.S., Yvon, F., Gallé, M., et al., 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 .

[50] Scheller, T., Kühn, E., 2015. Automated measurement of api usability: The api concepts framework. Information and Software Technology 61, 145–162.

[51] Shen, B., Zhang, J., Chen, T., Zan, D., Geng, B., Fu, A., Zeng, M., Yu, A., Ji, J., Zhao, J., Guo, Y., Wang, Q., 2023. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv:2307.14936 .

[52] Shrivastava, D., Larochelle, H., Tarlow, D., 2022. Repository-level prompt generation for large language models of code, in: ICML 2022 Workshop on Knowledge Retrieval and Language Models.

[53] Siddiq, M.L., msiddiq, 2022. SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques. Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security .

[54] Svyatkovskiy, A., Deng, S.K., Fu, S., Sundaresan, N., 2020. IntelliCode compose: code generation using transformer. Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering .

[55] Tan, G., Croft, J., 2008. An empirical security study of the native code in the jdk., in: Usenix Security Symposium, pp. 365–378.

[56] Wang, B., Komatsuzaki, A., 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax .

[57] Wang, Y., Wang, W., Joty, S., Hoi, S.C., 2021. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696–8708.

[58] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al., 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 .

[59] Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P., Ahmed, J., Overwijk, A., 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 .

[60] Xu, F.F., Alon, U., Neubig, G., Hellendoorn, V.J., 2022. A systematic evaluation of large language models of code. Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming .

[61] Yang, Z., Chen, S., Gao, C., Li, Z., Li, G., Lv, R., 2023. Deep learning based code generation methods: A literature review. arXiv preprint arXiv:2303.01056 .

[62] Zan, D., Chen, B., Lin, Z., Guan, B., Wang, Y., Lou, J.G., 2022a. When language model meets private library. EMNLP Findings .

[63] Zan, D., Chen, B., Yang, D., Lin, Z., Kim, M., Guan, B., Wang, Y., Chen, W., Lou, J.G., 2022b. CERT: Continual pre-training on sketches for library-oriented code generation, in: The 2022 International Joint Conference on Artificial Intelligence.

[64] Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Wang, Y., Lou, J.G., 2022c. When neural model meets nl2code: A survey. arXiv preprint arXiv:2212.09420 .

[65] Zhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou, J.G., Chen, W., 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv:2303.12570 .

[66] Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z.Y., Shen, L., Wang, A., Li, Y., Su, T., Yang, Z., Tang, J., 2023. CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x. ArXiv abs/2303.17568.

[67] Zhou, S., Alon, U., Xu, F.F., JIang, Z., Neubig, G., 2023. DocCoder: Generating code by retrieving and reading docs, in: The Eleventh International Conference on Learning Representations.

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 17 của 18

--- TRANG 19 ---
Tạo mã hướng thư viện riêng tư với các mô hình ngôn ngữ lớn

Daoguang Zan hiện đang theo đuổi bằng Tiến sĩ tại Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc. Lĩnh vực nghiên cứu chính của anh bao gồm xử lý ngôn ngữ tự nhiên và kỹ thuật phần mềm, đặc biệt trong tạo mã và mô hình ngôn ngữ lớn. Trong những lĩnh vực này, anh đã xuất bản khoảng 10 bài báo trong các kỷ yếu hội nghị hàng đầu, bao gồm ACL, IJCAI, EMNLP, ICLR, PAKDD, v.v.

Bei Chen hiện là một nhà nghiên cứu cấp cao tại Microsoft. Cô nhận bằng Tiến sĩ từ Khoa Khoa học và Công nghệ Máy tính tại Đại học Tsinghua, Bắc Kinh, Trung Quốc, vào năm 2017. Cô chủ yếu làm việc về xử lý ngôn ngữ tự nhiên, bao gồm phân tích cú pháp ngữ nghĩa, hệ thống đối thoại, mô hình ngôn ngữ được tiền huấn luyện, và các ứng dụng của chúng trong trí tuệ mã. Cô đã xuất bản trên 30 bài báo trong các hội nghị hàng đầu, bao gồm ICLR, NeurIPS, ACL, EMNLP, KDD, AAAI, IJCAI, v.v.

Yongshun Gong là Phó Giáo sư tại Trường Phần mềm, Đại học Shandong, Trung Quốc. Anh nhận bằng Tiến sĩ từ Đại học Công nghệ Sydney vào năm 2021. Lĩnh vực nghiên cứu chính của anh bao gồm khoa học dữ liệu và học máy, đặc biệt là các lĩnh vực sau: mô hình thích ứng; khai thác dữ liệu không gian-thời gian; dự đoán giao thông; hệ thống gợi ý và khai thác mẫu tuần tự. Anh đã xuất bản trên 40 bài báo trong các tạp chí hàng đầu và kỷ yếu hội nghị có trọng tài, bao gồm IEEE T-PAMI, IEEE T-KDE, IEEE T-NNLS, IEEE T-CYB, IEEE T-MM, NeurIPS, CVPR, KDD, CIKM, AAAI, IJCAI, v.v.

Junzhi Cao nhận bằng Tiến sĩ về Vật lý Thiên văn và Học sâu từ Đại học New York vào năm 2021. Anh chủ yếu nghiên cứu Hệ thống Đối thoại trong Xử lý Ngôn ngữ Tự nhiên, Mô hình ngôn ngữ lớn, và Vũ trụ học trong thống kê. Anh đã xuất bản khoảng 10 bài báo trong các tạp chí hàng đầu và kỷ yếu hội nghị có trọng tài, bao gồm Nature Communications, Monthly Notices of the Royal Astronomical Society, Journal of Applied Physics, v.v.

Fengji Zhang hiện đang theo đuổi bằng thạc sĩ tại Trường Khoa học Máy tính, Đại học Wuhan, Trung Quốc. Anh cũng nhận bằng Cử nhân từ Trường Khoa học Máy tính, Đại học Wuhan vào năm 2020. Lĩnh vực nghiên cứu hiện tại của anh bao gồm kỹ thuật phần mềm thông minh và xử lý ngôn ngữ tự nhiên. Anh đã xuất bản một số bài báo trong các tạp chí hàng đầu và kỷ yếu hội nghị có trọng tài, bao gồm IST, JSS, ICLR, và ACL.

Bingchao Wu hiện đang theo đuổi bằng Tiến sĩ tại Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc. Anh nhận bằng Cử nhân về Bảo mật Thông tin từ Đại học Hunan, Changsha, Trung Quốc vào năm 2017. Lĩnh vực nghiên cứu của anh bao gồm học sâu, hệ thống gợi ý, và nhận dạng thực thể có tên. Anh đã xuất bản một số bài báo trong các tạp chí hàng đầu và kỷ yếu hội nghị có trọng tài, bao gồm IEEE ICME, ACL, ISWC, v.v.

Bei Guan hiện là Phó Giáo sư Nghiên cứu tại Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc. Anh nhận bằng Cử nhân từ Đại học Tianjin vào năm 2007. Anh nhận bằng Tiến sĩ từ Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc vào năm 2015. Anh nhận vị trí nghiên cứu sau tiến sĩ tại Viện Nghiên cứu Máy tính Qatar, Đại học Hamad Bin Khalifa (QCRI, HBKU) và hoàn thành vào năm 2018. Lĩnh vực nghiên cứu chính của anh bao gồm phân tích dữ liệu lớn trong chăm sóc sức khỏe và an ninh mạng, kỹ thuật hệ điều hành, kỹ thuật ảo hóa, điện toán đám mây, và bảo mật hệ thống. Anh đã xuất bản hơn 20 bài báo kỹ thuật trong các tạp chí và kỷ yếu có trọng tài, bao gồm IEEE Transactions, ACM Transactions, IJCAI, PAKDD, EMNLP, IJCNN, ACL, v.v.

Yilong Yin là Giám đốc Nhóm Học máy và Ứng dụng và là Giáo sư Ưu tú tại Đại học Shandong, Jinan, Trung Quốc. Ông nhận bằng Tiến sĩ từ Đại học Jilin, Changchun, Trung Quốc, vào năm 2000. Từ năm 2000 đến 2002, ông là Nghiên cứu viên Sau tiến sĩ tại Khoa Khoa học và Kỹ thuật Điện tử, Đại học Nanjing, Nanjing, Trung Quốc. Lĩnh vực nghiên cứu của ông bao gồm học máy, khai thác dữ liệu, y học tính toán, và sinh trhọc định lượng. Ông đã xuất bản trên 100 bài báo trong các tạp chí hàng đầu và kỷ yếu hội nghị có trọng tài, bao gồm TKDE, TIP, TMM, ICML, IJCAI, v.v.

Yongji Wang là Nghiên cứu viên Ưu tú tại Viện Hàn lâm Khoa học Trung Quốc và là giáo viên hướng dẫn Tiến sĩ. Ông nhận bằng Tiến sĩ từ Đại học Edinburgh, Vương quốc Anh. Lĩnh vực nghiên cứu của ông bao gồm trí tuệ nhân tạo, phân tích dữ liệu lớn, và khai thác dữ liệu. Ông đã đạt được nhiều kết quả nghiên cứu có ảnh hưởng quốc tế, tham gia hơn 20 dự án nghiên cứu khoa học, và xuất bản sáu chuyên khảo. Ông đã viết hơn 200 bài báo chất lượng cao trong các tạp chí và hội nghị học thuật uy tín trong và ngoài nước, bao gồm IEEE Transactions, ACM Transactions, ACL, IJCAI, EMNLP, v.v.

Daoguang Zan et al.: Bản thảo được gửi đến Elsevier Trang 18 của 18
