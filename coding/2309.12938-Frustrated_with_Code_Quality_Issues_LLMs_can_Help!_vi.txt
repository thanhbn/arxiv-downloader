# 2309.12938.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2309.12938.pdf
# Kích thước tệp: 1214308 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thất vọng với các vấn đề chất lượng mã? LLM có thể giúp!
NALIN WADHWA, Microsoft Research, Ấn Độ
JUI PRADHAN, Microsoft Research, Ấn Độ
ATHARV SONWANE, Microsoft Research, Ấn Độ
SURYA PRAKASH SAHU, Microsoft Research, Ấn Độ
NAGARAJAN NATARAJAN, Microsoft Research, Ấn Độ
ATHARV SONWANE, Microsoft Research, Ấn Độ
ADITYA KANADE, Microsoft Research, Ấn Độ
SURESH PARTHASARATHY, Microsoft Research, Ấn Độ
SRIRAM RAJAMANI, Microsoft Research, Ấn Độ

Khi các dự án phần mềm phát triển, chất lượng mã trở nên vô cùng quan trọng vì nó ảnh hưởng đến độ tin cậy, khả năng bảo trì và tính bảo mật của phần mềm. Vì lý do này, các công cụ phân tích tĩnh được sử dụng trong quy trình làm việc của nhà phát triển để gắn cờ các vấn đề chất lượng mã. Tuy nhiên, các nhà phát triển cần phải bỏ ra thêm nỗ lực để sửa đổi mã của họ nhằm cải thiện chất lượng mã dựa trên kết quả của công cụ. Trong công trình này, chúng tôi nghiên cứu việc sử dụng các mô hình ngôn ngữ lớn (LLM) có khả năng tuân theo hướng dẫn để hỗ trợ các nhà phát triển trong việc sửa đổi mã để giải quyết các vấn đề chất lượng mã.

Chúng tôi trình bày một công cụ, CORE (viết tắt của COde REvisions), được thiết kế bằng cách sử dụng một cặp LLM được tổ chức thành một bộ đôi gồm một người đề xuất và một người xếp hạng. Các nhà cung cấp công cụ phân tích tĩnh khuyến nghị các cách để giảm thiểu cảnh báo của công cụ và các nhà phát triển làm theo chúng để sửa đổi mã của họ. LLM đề xuất của CORE nhận cùng một bộ khuyến nghị và áp dụng chúng để tạo ra các bản sửa đổi mã ứng viên. Các ứng viên vượt qua kiểm tra chất lượng tĩnh được giữ lại. Tuy nhiên, LLM có thể đưa ra những thay đổi chức năng tinh tế, không mong muốn mà có thể không được phát hiện bởi phân tích tĩnh. LLM xếp hạng đánh giá các thay đổi được thực hiện bởi người đề xuất bằng cách sử dụng một tiêu chí mà tuân thủ chặt chẽ các tiêu chuẩn chấp nhận mà một nhà phát triển sẽ áp dụng. CORE sử dụng điểm số được gán bởi LLM xếp hạng để xếp hạng các bản sửa đổi ứng viên trước khi trình bày chúng cho nhà phát triển.

Chúng tôi tiến hành nhiều thí nghiệm khác nhau trên hai điểm chuẩn công khai để cho thấy khả năng của CORE: 1) tạo ra các bản sửa đổi mã có thể chấp nhận được bởi cả công cụ phân tích tĩnh và người đánh giá con người (cái sau được đánh giá bằng nghiên cứu người dùng trên một tập con của điểm chuẩn Python), 2) giảm nỗ lực đánh giá của con người bằng cách phát hiện và loại bỏ các bản sửa đổi có thay đổi không mong muốn, 3) dễ dàng hoạt động trên nhiều ngôn ngữ (Python và Java), công cụ phân tích tĩnh (CodeQL và SonarQube) và kiểm tra chất lượng (lần lượt là 52 và 10 kiểm tra), và 4) đạt được tỷ lệ sửa lỗi tương đương với một công cụ sửa chữa chương trình tự động dựa trên quy tắc nhưng với nỗ lực kỹ thuật nhỏ hơn nhiều (trên điểm chuẩn Java). CORE có thể sửa đổi 59,2% tệp Python (trên 52 kiểm tra chất lượng) để chúng vượt qua sự kiểm tra của cả công cụ và người đánh giá con người. LLM xếp hạng đã giảm kết quả dương tính giả 25,8% trong những trường hợp này. CORE tạo ra các bản sửa đổi vượt qua công cụ phân tích tĩnh trong 76,8% tệp Java (trên 10 kiểm tra chất lượng) so với 78,3% của một công cụ sửa chữa chương trình chuyên biệt, với nỗ lực kỹ thuật ít hơn đáng kể.

Các khái niệm CCS: •Phần mềm và kỹ thuật của nó →Công cụ bảo trì phần mềm; Lập trình tự động.

Từ khóa và cụm từ bổ sung: Chất lượng mã, phân tích tĩnh, sửa đổi mã, LLM

1 GIỚI THIỆU

Khi các dự án phần mềm phát triển, việc đánh giá độ tin cậy, khả năng bảo trì và tính bảo mật của phần mềm trở nên vô cùng quan trọng. Chất lượng mã đóng vai trò lớn trong việc đảm bảo các mục tiêu này [20,22]. Nhiều công cụ phân tích tĩnh như CodeQL [11], Coverity [2], FindBugs [4], PMD [7] và SonarQube [8] được sử dụng trong quy trình làm việc của nhà phát triển để gắn cờ các vấn đề chất lượng mã. Tuy nhiên, các nhà phát triển cần phải bỏ ra thêm nỗ lực để sửa đổi mã của họ nhằm cải thiện chất lượng mã dựa trên kết quả của công cụ [51, 55].

Nhận ra giá trị của các công cụ phân tích tĩnh trong việc cải thiện chất lượng mã, nhiều cách tiếp cận [12, 14,19,25,26,32,33,37,46,50] sử dụng chúng để phát hiện và định vị các vi phạm kiểm tra tĩnh. Để sửa chữa các vi phạm, chúng sử dụng các biến đổi chương trình tượng trưng được thiết kế thủ công [19,25,46,50], khai thác các mẫu tượng trưng từ dữ liệu cam kết [12,14,32,33,37] hoặc học chúng từ dữ liệu được tạo tổng hợp [26]. Khả năng tạo mã hoặc sửa lỗi của các cách tiếp cận tượng trưng này bị giới hạn bởi không gian các mẫu được hỗ trợ. Các cách tiếp cận dựa trên học tập [28,49,54,56] cố gắng vượt qua giới hạn này bằng cách huấn luyện các mô hình thần kinh để ánh xạ các chương trình có lỗi thành các phiên bản đã được sửa. Tuy nhiên, tương tự như các cách tiếp cận khai thác mẫu, chúng yêu cầu dữ liệu sửa lỗi để huấn luyện và điều này giới hạn các loại lỗi mà chúng có thể sửa. Việc thiết lập các hệ thống này và hỗ trợ một ngôn ngữ lập trình khác, một kiểm tra chất lượng mới hoặc một công cụ phân tích tĩnh khác phát sinh chi phí kỹ thuật đáng kể. Những yếu tố này ngăn cản việc áp dụng rộng rãi các công cụ sửa chữa chương trình tự động (APR) này.

Các nhà cung cấp công cụ phân tích tĩnh khuyến nghị các cách để giảm thiểu cảnh báo của công cụ. Các nhà phát triển có thể làm theo chúng để sửa đổi mã của họ một cách thủ công khi cảnh báo được đưa ra. Hình 1 minh họa hai kiểm tra chất lượng (a) và (b) từ hai công cụ: CodeQL áp dụng cho mã Python¹ và SonarQube áp dụng cho mã Java². Ở trên cùng là các đoạn mã có vấn đề chất lượng. Các khuyến nghị sửa lỗi bằng ngôn ngữ tự nhiên cho các kiểm tra chất lượng được hiển thị ở giữa và mã đã được sửa đổi có thể được thu được sau khi làm theo các khuyến nghị sửa lỗi một cách thủ công được hiển thị ở dưới cùng.

Các công cụ APR cố gắng học ánh xạ giữa các ảnh chụp ban đầu và đã sửa đổi của mã. Để tránh các giới hạn của các công cụ APR được nêu ở trên, chúng tôi đề xuất thay vào đó sử dụng trực tiếp hướng dẫn bằng ngôn ngữ tự nhiên rõ ràng và ngắn gọn (khuyến nghị sửa lỗi) được cung cấp bởi các nhà cung cấp công cụ. Sự xuất hiện của các mô hình ngôn ngữ lớn (LLM) (ví dụ, [15–17,40,48]) mang lại cơ hội để làm cho điều này trở thành khả thi. LLM là các mạng thần kinh lớn nắm bắt các phân phối tạo sinh của ngôn ngữ tự nhiên và mã nguồn. Các mô hình này được huấn luyện trên dữ liệu rất lớn theo cách không giám sát. Việc điều chỉnh hướng dẫn [41] tăng cường tiện ích của chúng bằng cách tinh chỉnh các LLM cơ sở để hiểu và tuân theo các hướng dẫn bằng ngôn ngữ tự nhiên. Như chúng tôi chỉ ra trong bài báo này, có thể hướng dẫn các LLM tiên tiến để sửa đổi một đoạn mã trực tiếp bằng cách sử dụng hướng dẫn bằng ngôn ngữ tự nhiên. Các mô hình này có thể lấy mẫu nhiều loại mã có điều kiện trên hướng dẫn, mà không cần bất kỳ huấn luyện hoặc tinh chỉnh bổ sung nào. Không giống như các biến đổi chương trình tượng trưng và các mô hình thần kinh được huấn luyện trên các tập dữ liệu cụ thể, chúng không bị giới hạn bởi không gian các mẫu hoặc dữ liệu sửa lỗi được sử dụng để huấn luyện. Điều này loại bỏ nhu cầu phải bỏ ra nỗ lực cần thiết trong việc thiết kế các hệ thống biến đổi tượng trưng hoặc huấn luyện các mô hình thần kinh chuyên biệt, và tiết kiệm nỗ lực kỹ thuật.

Chúng tôi cố gắng hiện thực hóa lời hứa của LLM để giải quyết các vấn đề chất lượng mã được gắn cờ bởi phân tích tĩnh trong một công cụ có tên CORE (viết tắt của COde REvision). CORE được thiết kế bằng cách sử dụng một cặp LLM được tổ chức thành một bộ đôi gồm một người đề xuất và một người xếp hạng. LLM đề xuất của CORE nhận một khuyến nghị sửa lỗi và áp dụng nó vào một tệp mã nguồn cho trước để tạo ra các bản sửa đổi mã ứng viên. Các ứng viên vượt qua kiểm tra chất lượng tĩnh được giữ lại. Tuy nhiên, LLM có thể đưa ra những thay đổi chức năng tinh tế, không mong muốn mà có thể không được phát hiện bởi phân tích tĩnh. LLM xếp hạng đánh giá các thay đổi được thực hiện bởi người đề xuất bằng cách sử dụng một tiêu chí mà tuân thủ chặt chẽ các tiêu chuẩn chấp nhận mà một nhà phát triển sẽ áp dụng. CORE sử dụng điểm số được gán bởi LLM xếp hạng để xếp hạng các bản sửa đổi ứng viên trước khi trình bày chúng cho nhà phát triển.

Chúng tôi đánh giá CORE trên hai điểm chuẩn công khai: CodeQueries [45] và Sorald [46]. CodeQueries là một điểm chuẩn các tệp Python có vấn đề chất lượng được gắn cờ bởi một trong 52 kiểm tra tĩnh phổ biến được áp dụng bởi công cụ CodeQL. Sorald bao gồm các kho lưu trữ Java có vấn đề chất lượng được gắn cờ bởi một trong 10 kiểm tra tĩnh phổ biến được áp dụng bởi công cụ SonarQube. Cả hai tập dữ liệu đều chứa mã từ các kho lưu trữ GitHub công khai và đại diện cho các vấn đề chất lượng thực tế.

Chúng tôi tiến hành nhiều thí nghiệm khác nhau trên các điểm chuẩn này để cho thấy khả năng của CORE: 1) tạo ra các bản sửa đổi mã có thể chấp nhận được bởi cả công cụ phân tích tĩnh và người đánh giá con người (cái sau được đánh giá bằng nghiên cứu người dùng trên một tập con của điểm chuẩn Python), 2) giảm nỗ lực đánh giá của con người bằng cách phát hiện và loại bỏ các bản sửa đổi có thay đổi không mong muốn, 3) dễ dàng hoạt động trên nhiều ngôn ngữ (Python và Java), công cụ phân tích tĩnh (CodeQL và SonarQube) và kiểm tra chất lượng (lần lượt là 52 và 10 kiểm tra), và 4) đạt được tỷ lệ sửa lỗi tương đương với một công cụ sửa chữa chương trình tự động dựa trên quy tắc, Solard, nhưng với nỗ lực kỹ thuật nhỏ hơn nhiều (trên điểm chuẩn Java).

Chúng tôi thu được kết quả đầy hứa hẹn chứng minh cho tiện ích thực tế của CORE bằng cách sử dụng GPT-3.5-Turbo [41] làm LLM đề xuất và GPT-4 [40] làm LLM xếp hạng. CORE có thể sửa đổi 59,2% tệp Python (trên 52 kiểm tra chất lượng) để chúng vượt qua sự kiểm tra của cả công cụ và người đánh giá con người. LLM xếp hạng đã giảm tỷ lệ dương tính giả 25,8% trong những trường hợp này. CORE tạo ra các bản sửa đổi vượt qua công cụ phân tích tĩnh trong 76,8% tệp Java (trên 10 kiểm tra chất lượng) so với 78,3% của công cụ sửa chữa chương trình chuyên biệt Solard [46], nhưng với nỗ lực kỹ thuật ít hơn đáng kể. Các tác giả của Solard tuyên bố rằng "Thiết kế và triển khai của SORALD đã đại diện cho hơn 2 năm làm việc toàn thời gian." [46], trong khi chúng tôi có thể áp dụng CORE trên điểm chuẩn Solard với vài ngày nỗ lực của một vài tác giả.

Có sự quan tâm ngày càng tăng trong việc sử dụng LLM trong sửa chữa chương trình. Nhiều kỹ thuật hiện có [21,35, 52,53] nhằm mục đích sửa lỗi được đặc trưng bởi các test case thất bại. So với đó, chúng tôi tập trung vào việc sửa các vấn đề chất lượng được phát hiện tĩnh và không có test đơn vị đi kèm để xác thực. Các kỹ thuật sửa chữa lỗi được phát hiện tĩnh [29,30,42] hoặc nhắm vào các lỗi cú pháp hoặc ngữ nghĩa đơn giản [30], tinh chỉnh một LLM trên các prompt được thiết kế đặc biệt [29] hoặc sử dụng các mô hình kém mạnh hơn và prompting [42]. Chúng tôi nhắm vào một loạt rộng các vấn đề chất lượng mã bằng cách sử dụng một LLM được điều chỉnh hướng dẫn hỗ trợ prompting mạnh mẽ mà không cần bất kỳ tinh chỉnh nào.

Chúng tôi đóng góp những điều sau đây trong bài báo này:
•Chúng tôi xác định một cơ hội mới nổi của việc sử dụng LLM tuân theo hướng dẫn để hỗ trợ các nhà phát triển trong việc giải quyết các vấn đề chất lượng mã.

--- TRANG 2 ---
Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Atharv Sonwane, Aditya Kanade, Suresh Parthasarathy, và Sriram Rajamani

Hình 1. Ví dụ về kiểm tra chất lượng, khuyến nghị sửa lỗi, và mã trước và sau khi tuân theo khuyến nghị sửa lỗi cho các công cụ CodeQL và SonarQube cho ngôn ngữ Python và Java tương ứng.

--- TRANG 3 ---
Thất vọng với các vấn đề chất lượng mã? LLM có thể giúp!

•Chúng tôi trình bày một hệ thống, CORE, để đánh giá cơ hội này. Chúng tôi thiết kế một giao thức nhiều bước trong đó một LLM đề xuất các bản sửa đổi mã, được lọc bằng cách áp dụng phân tích tĩnh và được xếp hạng thêm bằng cách sử dụng một LLM xếp hạng, trước khi chúng được trình bày cho nhà phát triển.

•Chúng tôi tiến hành thí nghiệm rộng rãi để đánh giá khả năng chấp nhận của các bản sửa đổi được tạo ra bởi CORE, khả năng kiểm soát kết quả dương tính giả, khả năng tổng quát hóa cho các ngôn ngữ, công cụ và kiểm tra khác nhau, và hiệu suất của nó so với một công cụ sửa chữa chương trình chuyên biệt. Kết quả của chúng tôi cho thấy rằng CORE là một bước đầy hứa hẹn trong việc đưa LLM đến giúp đỡ các nhà phát triển trong việc giải quyết các vấn đề chất lượng mã. Chúng tôi cũng xác định các cơ hội để cải thiện thêm.

•Chúng tôi sẽ phát hành mã và dữ liệu của chúng tôi tại https://aka.ms/CORE_MSRI.

2 TỔNG QUAN

Mục tiêu của chúng tôi là (1) tự động hóa các bản sửa đổi chất lượng mã trong quy trình kỹ thuật phần mềm, thường bao gồm các kho lưu trữ mã quy mô lớn và các kiểm tra kiểm soát chất lượng mã khác nhau; (2) bằng cách lấy các công cụ phân tích tĩnh và tài liệu về kiểm tra chất lượng (hướng dẫn bằng ngôn ngữ tự nhiên) làm đầu vào; (3) với sự can thiệp tối thiểu của nhà phát triển. Phần này đưa ra tổng quan về kiến trúc của CORE bằng cách sử dụng một ví dụ kịch bản giải quyết vấn đề chất lượng mã.

Xem xét đoạn mã Python được hiển thị trong Mã 1 dưới đây. Lớp PersistentDict kế thừa từ lớp dict, thêm các thuộc tính _filename và _transact của riêng nó. Nó không ghi đè phương thức __eq__. Đây là một ví dụ về chất lượng mã kém có thể gây ra lỗi: khi hai đối tượng của lớp PersistentDict được so sánh, các thuộc tính của lớp con bị bỏ qua. Mã ví dụ này được gắn cờ bởi công cụ CodeQL với cảnh báo Eq-Not-Overridden [3].

Quy trình kỹ thuật phần mềm bao gồm các kiểm tra chất lượng như vậy về khả năng đọc, khả năng bảo trì, bảo mật, v.v. Đi kèm với các kiểm tra này là các hướng dẫn (hướng dẫn bằng ngôn ngữ tự nhiên) để sửa các vấn đề trong các tệp nguồn được gắn cờ bởi các công cụ phân tích tĩnh. Đối với kiểm tra Eq-Not-Overridden, trang hướng dẫn CodeQL trên web³ nêu "Một lớp định nghĩa các thuộc tính không có trong các lớp cha của nó có thể cần ghi đè phương thức __eq__() (__ne__() cũng nên được định nghĩa)". Các nhà phát triển công cụ hoặc đội ngũ đảm bảo chất lượng (QA), bảo mật và tuân thủ trong các tổ chức viết các kiểm tra chất lượng tĩnh và tài liệu (khuyến nghị sửa lỗi), và chủ sở hữu kho lưu trữ (đội phát triển) chịu trách nhiệm sửa các vấn đề chất lượng dựa trên các hướng dẫn được cung cấp.

class PersistentDict(dict):
"""Một lớp lưu trữ một dict vào tệp. Lớp này hoạt động như một dict và thêm chức năng mới để lưu dict vào tệp khi ghi."""
def __init__(self, filename, load=True):
self._filename = os.path.abspath(filename)
if load: self._load()
self._transact = False

@property
def filename(self):
'Đường dẫn tệp để ghi'
return self._filename

Mã 1. Ví dụ mã Python có vấn đề Eq-Not-Overridden.

Dưới đây, chúng tôi trình bày tổng quan về pipeline CORE được cấu hình với các đầu vào từ nhà cung cấp công cụ cùng với các tệp mã nguồn được gắn cờ bởi các công cụ, và tạo ra các bản sửa đổi tự động của các tệp nguồn để giải quyết các vấn đề.

1. Cấu hình pipeline CORE: CORE, được hiển thị trong Hình 2, là một pipeline chung cho các bản sửa đổi mã. Để cấu hình CORE xử lý vấn đề Eq-Not-Overridden, nhà cung cấp công cụ cung cấp hai loại thông tin cho CORE: (1) công cụ phân tích tĩnh (ví dụ, CodeQL) và chính kiểm tra đó (ví dụ, tệp .ql), (2) mô tả về vấn đề chất lượng mã và hướng dẫn để sửa vấn đề bằng ngôn ngữ tự nhiên. Trong đánh giá của chúng tôi, chúng tôi thu được mô tả về các vấn đề chất lượng và hướng dẫn để sửa chúng từ tài liệu trực tuyến. Khi CORE được cấu hình với hai loại thông tin này, nó có thể tự động xử lý các báo cáo phân tích tĩnh tương ứng với vấn đề này, cùng với các tệp chứa mã cần được sửa đổi để khắc phục vấn đề, và đề xuất các bản sửa đổi ứng viên cho mã.

Cấu hình thủ công được đề cập ở trên cho một vấn đề chất lượng mã là một bước một lần, ngoại tuyến, phục vụ để tạo ra các bản sửa đổi (một cách tự động, trực tuyến) cho vấn đề phát sinh trong các kho lưu trữ khác nhau, từ đó tự động hóa nhiệm vụ lặp đi lặp lại của việc giải quyết các vấn đề chất lượng mã.

Chúng tôi mô tả các thành phần của pipeline CORE tiếp theo.

2. Xây dựng prompt: Thành phần này nhận báo cáo phân tích tĩnh, tệp nguồn được gắn cờ, và tài liệu cho vấn đề (xem ở trên), và xây dựng một "prompt" để truy vấn mô hình ngôn ngữ lớn (LLM). Một prompt mã hóa hướng dẫn bằng ngôn ngữ tự nhiên để giải quyết một nhiệm vụ cụ thể và tùy chọn thông tin bổ sung mà mô hình có thể sử dụng để thực hiện nhiệm vụ như gợi ý (ví dụ, các dòng quan trọng trong tệp), ràng buộc (ví dụ, "không sửa đổi các phần mã không liên quan đến vấn đề"), và minh họa (ví dụ, một đoạn mã ví dụ với __eq__ không được ghi đè và phiên bản đã sửa đổi của nó). Thông thường, mỗi LLM có giới hạn về kích thước prompt (còn được gọi là kích thước ngữ cảnh) mà nó hỗ trợ về số lượng token. Trong CORE, prompt bao gồm mô tả về vấn đề chất lượng, đề xuất sửa lỗi, và (các) dòng mã mà báo cáo phân tích tĩnh quy cho vấn đề. Ngoài ra, CORE sử dụng thông tin liên quan khác có thể hữu ích để sửa vấn đề, chẳng hạn như lấy các khối mã liên quan được rút ra từ báo cáo phân tích tĩnh. Chi tiết của việc xây dựng prompt được trình bày trong Phần 3.

3. Tạo bản sửa đổi ứng viên bằng LLM Đề xuất: LLM đề xuất nhận đầu vào là prompt được xây dựng bằng ngôn ngữ tự nhiên cùng với mã được gắn cờ cho vấn đề chất lượng và xuất ra các bản sửa đổi mã tiềm năng. Chúng tôi sử dụng GPT-3.5-Turbo, một LLM tiên tiến cho việc tạo mã, trong các thí nghiệm của chúng tôi. GPT-3.5-Turbo hỗ trợ kích thước prompt lớn (lên đến 4000 token). Điều này cho phép chúng tôi nhập toàn bộ tệp mã nguồn cho nhiều trường hợp. Đối với các tệp rất lớn, chúng tôi đưa khối ngữ cảnh lớn nhất có thể chấp nhận được (ví dụ, toàn bộ phương thức hoặc lớp xung quanh các dòng quan trọng) theo kích thước prompt (chi tiết trong Phần 3). Mã đầu ra (một khối hoặc toàn bộ tệp, tùy trường hợp) sau đó được vá lại vào tệp gốc. Chúng tôi lấy mẫu 10 bản sửa đổi ứng viên cho mỗi tệp đầu vào.

4. Lọc bản sửa đổi với công cụ được cấu hình: Chúng tôi chạy kiểm tra phân tích tĩnh (mà CORE được cấu hình với ở 1) trên các bản sửa đổi ứng viên và lọc ra những bản mà vấn đề chất lượng mã vẫn tồn tại (tức là, phát hiện vi phạm khác không). Trong quá trình này, chúng tôi loại bỏ trùng lặp các đề xuất sửa đổi mã giống nhau cũng như đảm bảo tính hợp lệ cú pháp của các bản sửa đổi mã, và từ chối những bản có lỗi cú pháp.

5. Xếp hạng các ứng viên được chấp nhận bằng LLM Xếp hạng: Công cụ phân tích tĩnh có thể chấp nhận các bản sửa đổi không được chấp nhận bởi các nhà phát triển, chẳng hạn như đưa ra các thay đổi không mong muốn trong mã (ví dụ, một bản sửa đổi ghi đè phương thức __eq__ đúng cách, nhưng thay đổi việc triển khai __hash__ một cách không cần thiết). Khả năng chỉnh sửa và tạo sinh ấn tượng của các LLM tiên tiến cũng có nghĩa là chúng có thể thực hiện các thay đổi (thường là tinh tế, nhưng thay đổi ngữ nghĩa) đối với mã hiện có ngay cả khi chúng được hướng dẫn rõ ràng không làm như vậy. Hơn nữa, nếu chất lượng của kiểm tra tĩnh (thường là một số hình thức khớp mẫu) kém, thì việc đảm bảo các bản sửa đổi không chính xác không được hiển thị cho nhà phát triển, người cuối cùng sẽ chấp nhận hoặc từ chối chúng, càng trở nên quan trọng hơn. Để giảm gánh nặng cho các nhà phát triển và cải thiện tỷ lệ chấp nhận của các bản sửa lỗi được hiển thị, chúng tôi sử dụng một LLM khác (GPT-4) để chấm điểm các bản sửa đổi ứng viên theo thứ tự khả năng chấp nhận của chúng. Để hướng dẫn LLM làm như vậy, chúng tôi dựa vào cùng tiêu chí, tức là, hướng dẫn để giải quyết vấn đề được cung cấp ở 1, để xây dựng prompt cho LLM. Cụ thể, chúng tôi đưa bản sửa đổi ứng viên đã vượt qua công cụ phân tích tĩnh (diff với mã nguồn gốc) cùng với tiêu chí bằng ngôn ngữ tự nhiên, làm đầu vào cho LLM Xếp hạng và yêu cầu nó gán một điểm thứ tự trong một khoảng (từ chấp nhận mạnh đến từ chối mạnh). Chúng tôi sử dụng điểm này để xếp hạng các ứng viên tiềm năng cho một tệp. Chi tiết của chiến lược xếp hạng được thảo luận trong Phần 3.

Đầu ra mẫu: Đoạn mã được hiển thị dưới đây là một đầu ra mẫu được tạo ra bởi CORE cho Mã 1. Do thiếu không gian, thay vì hiển thị toàn bộ mã, chúng tôi hiển thị các đầu ra ở định dạng diff tiêu chuẩn so với Mã 1.

@@ -128,0 +130,4 @@ class PersistentDict(dict):
+ def __eq__(self, other):
+ if isinstance(other, PersistentDict):
+ return dict.__eq__(self, other) and self._filename == other._filename and self._transact == other._transact
+ return False

Mã 2. Một bản sửa đổi chính xác của Mã 1 được tạo ra bởi CORE.

Đoạn mã trên là một cách chính xác để sửa đổi Mã 1. Nó gọi phương thức __eq__ của lớp cha và so sánh tất cả các biến thành viên của lớp con. Hơn nữa, nó không thực hiện bất kỳ thay đổi không cần thiết nào đối với mã.

--- TRANG 4 ---
Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Atharv Sonwane, Aditya Kanade, Suresh Parthasarathy, và Sriram Rajamani

Hình 2. Pipeline CORE: Các vấn đề chất lượng mã (kiểm tra tĩnh) trên các kho lưu trữ phần mềm được ghi lại bởi nhà cung cấp công cụ. CORE được tích hợp trong pipeline xây dựng repo cũng chạy bộ kiểm tra phân tích tĩnh. Các tệp nguồn được gắn cờ và tài liệu được đưa vào làm đầu vào cho hệ thống CORE để tự động tạo ra các bản sửa đổi tệp nguồn giải quyết các vấn đề chất lượng. Các bản sửa đổi ứng viên vượt qua kiểm tra tĩnh được đánh giá và xếp hạng thêm bởi LLM xếp hạng để ngăn chặn việc hiển thị các bản sửa lỗi giả cho nhà phát triển.

--- TRANG 5 ---
Thất vọng với các vấn đề chất lượng mã? LLM có thể giúp!

3 THIẾT KẾ

Thiết kế và điều chỉnh prompt để truy vấn LLM là một lĩnh vực nghiên cứu mới đang phát triển mạnh [34]. Trong phần này, chúng tôi mô tả chi tiết các chiến lược xây dựng prompt, được hướng dẫn bởi các báo cáo phân tích tĩnh. Các lời gọi LLM trong pipeline của chúng tôi là để tạo các bản sửa đổi ứng viên, và để chấm điểm và xếp hạng các ứng viên.

3.1 LLM Đề xuất: Prompting LLM để tạo bản sửa đổi mã

Để tạo bản sửa đổi mã cho một vấn đề chất lượng mã cho trước và một tệp nguồn đầu vào, chúng tôi thiết kế một mẫu prompt kết hợp các loại thông tin khác nhau, với hướng dẫn bằng ngôn ngữ tự nhiên chi tiết, cần thiết để thực hiện nhiệm vụ sửa đổi. Prompt của chúng tôi tuân theo cấu trúc chung được hiển thị dưới đây, với các thành phần cố định (p1 và p2, theo cấu hình được thực hiện ở 1 được thảo luận trong Phần 2) cũng như các thành phần cụ thể cho từng trường hợp (p3, p4, và p5) được thu thập động:

Mẫu Prompt Đề xuất
p1 Mô tả về vấn đề chất lượng (tức là, kiểm tra tĩnh).
p2 Tiêu chí để giải quyết vấn đề chất lượng.
p3 (Tùy chọn) Các khối mã liên quan để thực hiện sửa đổi.
p4 Tệp nguồn đầu vào (đầy đủ, hoặc được định vị vào khối chứa vấn đề).
p5 Vị trí và thông báo cảnh báo được đưa ra bởi kiểm tra tĩnh.

Các thành phần cố định của prompt bao gồm tên của kiểm tra chất lượng, mô tả, và các cách được khuyến nghị để giải quyết vấn đề. Chúng được cung cấp tại thời điểm cấu hình pipeline CORE. Trong đánh giá thực nghiệm của chúng tôi ở Phần 5, chúng tôi chỉ đơn giản sử dụng nội dung từ các trang web tài liệu [1,8]. Khi có thông tin thiếu hoặc không đầy đủ về cách sửa các vấn đề, chúng tôi cung cấp thông tin. Prompt được khởi tạo cho ví dụ chạy Mã 1 của chúng tôi được đưa ra trong Hình 3. Văn bản in nghiêng là mẫu, văn bản màu xanh teal tương ứng với các thành phần cố định thu được từ các nhà cung cấp công cụ, và văn bản màu đỏ gạch tương ứng với thông tin cụ thể cho từng trường hợp được truy xuất từ báo cáo phân tích tĩnh (CodeQL cho ví dụ này).

Xử lý nhiều vi phạm trong tệp đầu vào: Công cụ phân tích tĩnh cung cấp cho chúng tôi các vị trí (dòng quan trọng), và trong một số trường hợp cả thông báo cảnh báo liên quan, nơi vấn đề được gắn cờ trong tệp nguồn đầu vào. Có thể có nhiều vị trí trong một tệp nguồn duy nhất nơi vi phạm kiểm tra được gắn cờ. Nếu tệp nguồn đủ nhỏ (để vừa với kích thước ngữ cảnh của LLM), chúng tôi đưa toàn bộ tệp nguồn (trong p4) cũng như tất cả các vị trí và thông báo cảnh báo được gắn cờ (trong p5) trong một prompt duy nhất. Nếu không, chúng tôi sử dụng quy trình sau để khởi tạo (các) prompt.

Gọi V={v1, v2, ..., vn} biểu thị các vị trí được gắn cờ trong tệp đầu vào. Chúng tôi bắt đầu với v1 và lấy khối bao phủ lớn nhất của nó, ví dụ, lớp hoặc phương thức chứa v1, vừa với kích thước ngữ cảnh có sẵn. Lưu ý rằng khối này cũng có thể bao gồm nhiều vị trí vi phạm vi ngoài v1. Chúng tôi khởi tạo một prompt với khối này làm đầu vào (trong p4), và tập con các vi phạm được gắn cờ (trong p5). Chúng tôi loại bỏ tập con vị trí này khỏi V và lặp lại. Do đó, LLM đề xuất được gọi với có thể nhiều prompt được khởi tạo cho một tệp đầu vào duy nhất, và khối mã được tạo cho mỗi prompt được vá lại vào tệp nguồn gốc một cách thích hợp.

Các khối mã liên quan: Trong quá trình phân tích tĩnh, các công cụ như CodeQL xác định và kiểm tra các khối mã liên quan để xác định sự hiện diện/vắng mặt của vi phạm thuộc tính. Chúng tôi ghi lại thông tin này trong khi chạy phân tích tĩnh và cung cấp nó như tín hiệu bổ sung trong prompt của chúng tôi ở p3. Ví dụ, đối với kiểm tra CodeQL "không khớp chữ ký trong phương thức ghi đè", việc khai báo của phương thức bị ghi đè từ lớp cha là một khối liên quan vì kiểm tra tĩnh xác định sự không khớp giữa các phương thức bị ghi đè và ghi đè bằng cách kiểm tra chữ ký của chúng. Mặt khác, trong ví dụ của Mã 1, thông báo lỗi CodeQL đã cung cấp đủ thông tin. Như được hiển thị trong Hình 3, p5 cung cấp chẩn đoán CodeQL rằng các thuộc tính _filename và _transact được thêm trong lớp con và không được bao phủ bởi phương thức __eq__ của lớp cha. Do đó, bản sửa lỗi có thể được xây dựng từ mã cục bộ với thông tin chẩn đoán này, và không cần bất kỳ phần nào khác của tệp nguồn. Đối với các kiểm tra như vậy, chúng tôi không cung cấp p3.

3.2 LLM Xếp hạng: Prompting LLM để chấm điểm bản sửa đổi ứng viên

Như chúng tôi đã nêu trong Phần 2, các công cụ phân tích tĩnh có thể chấp nhận các bản sửa đổi không được chấp nhận, ví dụ, đưa ra các thay đổi không mong muốn hoặc thay đổi tính đúng đắn chức năng của mã nguồn. Trong ví dụ chạy của Mã 1, chúng ta thấy hai loại bản sửa đổi có khả năng bị từ chối bởi các nhà phát triển: (1) các bản sửa đổi ghi đè phương thức __eq__ đúng cách, nhưng thay đổi chức năng của mã ở nơi khác, chẳng hạn như thay đổi việc triển khai __hash__, và (2) các bản sửa đổi không hoàn toàn giải quyết vấn đề chất lượng, nhưng vẫn bỏ qua kiểm tra CodeQL - ví dụ, tất cả các thành viên lớp con được liệt kê rõ ràng trong kiểm tra bằng nhau mà không gọi super().__eq__ cho các thành viên cha. Chúng tôi không muốn hiển thị các ứng viên giả như vậy cho nhà phát triển.

Để làm điều này, chúng tôi sử dụng một thể hiện khác của LLM để hoạt động như một người xếp hạng chấm điểm các bản sửa đổi ứng viên vượt qua công cụ, tức là, đầu ra của giai đoạn 4 trong Hình 2, trong pipeline CORE. Chúng tôi sử dụng chiến lược prompting tương tự như được sử dụng để tạo chính các bản sửa đổi trong phần trước để truy vấn LLM xếp hạng. Mẫu prompt để chấm điểm ứng viên được đưa ra ở trên (một số phần được bỏ qua để tiết kiệm không gian). Lưu ý rằng prompt khá chung chung, và đặc biệt, không phụ thuộc vào loại kiểm tra chất lượng mã hoặc công cụ phân tích tĩnh.

Ngoài thông tin về kiểm tra chất lượng và khuyến nghị sửa lỗi, prompt cung cấp mô tả về tiêu chí chấm điểm yêu cầu LLM xếp hạng các bản sửa đổi (từ từ chối mạnh đến chấp nhận mạnh) dựa trên mức độ gần gũi của chúng với thay đổi dự định trong khi tránh các thay đổi không liên quan. Đôi khi LLM áp dụng các quy ước hoặc phong cách mã hóa mà nó gặp thường xuyên trong quá trình huấn luyện và viết lại mã để tuân thủ những quy ước đó. Chúng tôi hướng dẫn LLM bỏ qua những thay đổi như vậy (được chỉ ra bởi "Ngoại lệ Được phép" trong prompt) và cung cấp một số tình huống ví dụ. Tương tự, chúng tôi giải thích thêm về loại bản sửa đổi nào nên bị từ chối trong phần cuối của prompt, trước khi cung cấp "Diff" để chấm điểm.

--- TRANG 6 ---
Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Atharv Sonwane, Aditya Kanade, Suresh Parthasarathy, và Sriram Rajamani

Prompt Đề xuất (đầu ra của giai đoạn "Xây dựng Prompt" trong Hình 2)
p1 Chúng tôi đang sửa mã đã được gắn cờ cho cảnh báo CodeQL có tiêu đề "`__eq__` not overridden when adding attributes" có mô tả sau:
Một lớp định nghĩa các thuộc tính không có trong các lớp cha của nó có thể cần ghi đè phương thức __eq__() (__ne__() cũng nên được định nghĩa).
Thêm các thuộc tính bổ sung mà không ghi đè __eq__() có nghĩa là các thuộc tính bổ sung sẽ không được tính đến trong các kiểm tra bằng nhau.

p2 Cách được khuyến nghị để sửa mã được gắn cờ cho cảnh báo này là:
Ghi đè phương thức __eq__ để cũng kiểm tra sự bằng nhau của các thuộc tính được thêm bằng cách gọi eq trên lớp cơ sở và kiểm tra sự bằng nhau của các thuộc tính được thêm, hoặc triển khai một phương thức eq mới kiểm tra sự bằng nhau trên cả thuộc tính self và kế thừa.

p4 Sửa đổi mã Lỗi dưới đây để sửa (các) cảnh báo CodeQL. Xuất toàn bộ khối mã với các thay đổi thích hợp. Không xóa bất kỳ phần nào của mã không liên quan đến bản sửa lỗi mong muốn.
Mã Lỗi:
class PersistentDict (dict) :
···

p5 (Các) cảnh báo CodeQL cho mã lỗi trên:
Lớp 'PersistentDict' không ghi đè "__eq__", nhưng thêm các thuộc tính mới "_filename" và "_transact".
Các dòng sau có khả năng quan trọng:
1. class PersistentDict (dict) :
Mã Đã sửa:

Hình 3. Prompt được cung cấp cho LLM Đề xuất để sửa đổi Mã 1. Ví dụ này không yêu cầu các khối mã liên quan bổ sung làm ngữ cảnh và do đó, thành phần prompt tương ứng p3 không có mặt.

--- TRANG 7 ---
Thất vọng với các vấn đề chất lượng mã? LLM có thể giúp!

Prompt Xếp hạng
Bạn là một nhà phát triển chuyên gia. Bạn đang xác minh mã được tạo ra bởi LLM để sửa cảnh báo có tiêu đề "'__eq__' not overridden when adding attributes" có mô tả sau:
Một lớp định nghĩa các thuộc tính không có trong các lớp cha của nó có thể cần ghi đè phương thức __eq__() (__ne__() cũng nên được định nghĩa). ...

Các cách được khuyến nghị để sửa mã được gắn cờ cho cảnh báo này là:
Ghi đè phương thức __eq__ để cũng kiểm tra sự bằng nhau của các thuộc tính được thêm bằng cách gọi eq trên lớp cơ sở và kiểm tra sự bằng nhau của các thuộc tính được thêm, hoặc ...

Nhiệm vụ của bạn là đánh giá chất lượng của patch được tạo ra và đánh giá nó theo các tiêu chí đánh giá sau:
Điểm 0, nếu patch có các thay đổi không liên quan và không cần thiết để sửa cảnh báo (Từ chối Mạnh).
Điểm 1, nếu patch có một vài bản sửa lỗi đúng, nhưng vẫn sửa đổi đoạn mã gốc một cách không cần thiết (Từ chối Yếu).
Điểm 2, nếu patch có hầu hết các bản sửa lỗi đúng nhưng vẫn chưa lý tưởng (Chấp nhận Yếu).
Điểm 3, nếu patch chỉ thực hiện các chỉnh sửa sửa cảnh báo với tác động ít nhất đến bất kỳ phân đoạn không liên quan nào của đoạn mã gốc (Chấp nhận Mạnh).

Nếu bạn tìm thấy các bổ sung hoặc xóa các đoạn mã không liên quan đến các bản sửa lỗi mong muốn (nghĩ về ảo giác LLM), nó có thể được phân loại là điểm 0 (Từ chối Mạnh). Điều đó nói rằng, bạn có thể đưa ra ngoại lệ trong những trường hợp rất cụ thể mà bạn chắc chắn rằng các bổ sung hoặc xóa không thay đổi tính đúng đắn chức năng của mã, như được nêu tiếp theo.

Ngoại lệ Được phép:
Các thay đổi mã (không liên quan) sau đây trong tệp diff có thể được coi là ổn và không cần cản trở việc gắn nhãn một thay đổi mã đúng khác như chấp nhận (điểm 2 hoặc 3). Danh sách này không đầy đủ, nhưng bạn nên hiểu ý tưởng
(a) xóa bình luận là ổn,
(b) viết lại a = a + 1 thành a += 1 là ổn, mặc dù nó có thể không liên quan gì đến cảnh báo quan tâm,
(c) thực hiện các thay đổi cụ thể cho phiên bản là ổn, chẳng hạn thay đổi print ("hello") thành print "hello".

Các thay đổi mã (không liên quan) sau đây trong tệp diff KHÔNG được coi là ổn, và bạn nên gắn nhãn tệp diff là từ chối (điểm 0 hoặc 1) ngay cả khi nó đúng cho truy vấn.
Danh sách này không đầy đủ, nhưng bạn nên hiểu ý tưởng
(a) xóa hoặc thêm một câu lệnh print,
(b) tối ưu hóa một phép tính,
(c) thay đổi tên biến hoặc đưa ra lỗi chính tả.

Chỉ xuất lý do và điểm cho patch dưới đây. Không xuất bất cứ thứ gì khác.
Diff:⟨diff⟩
Lý do:

--- TRANG 8 ---
Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Atharv Sonwane, Aditya Kanade, Suresh Parthasarathy, và Sriram Rajamani

4 THIẾT LẬP THỰC NGHIỆM

Tập dữ liệu: (1) Chúng tôi sử dụng một tập con của tập dữ liệu CodeQueries [45]⁴ trong các thí nghiệm của chúng tôi. Nó chứa các tệp Python có vấn đề chất lượng được gắn cờ bởi một bộ 52 truy vấn CodeQL (tức là, kiểm tra tĩnh). 52 truy vấn CodeQL được lấy từ bộ Python CodeQL tiêu chuẩn; chúng phân tích các khía cạnh khác nhau của mã như bảo mật, tính đúng đắn, khả năng bảo trì và khả năng đọc. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng phần test split của tập dữ liệu CodeQueries. Do quyền truy cập LLM bị giới hạn, chúng tôi có thể thí nghiệm với một tập con bao gồm 765 tệp trên 52 truy vấn. Chúng tôi ký hiệu tập dữ liệu này là CQPy. Hơn nữa, chúng tôi lấy mẫu 10 tệp mỗi truy vấn từ CQPy để tiến hành nghiên cứu người dùng về các bản sửa đổi được tạo ra bởi CORE. Chúng tôi gọi tập con này là CQPyUS. (2) Chúng tôi sử dụng một tập con của tập dữ liệu Sorald [46]⁵; tập con này bao gồm một bộ sưu tập 151 kho lưu trữ java từ Github (trong số 161 kho lưu trữ trong tập dữ liệu đầy đủ của họ) với tổng cộng 483 tệp, và bao gồm tất cả 10 kiểm tra SonarQube được nghiên cứu trong [46]. Chúng tôi gọi tập này là SQJava.

Cấu hình mô hình: Chúng tôi tiến hành các thí nghiệm của mình bằng cách sử dụng mô hình GPT-3.5-Turbo làm người đề xuất. Chúng tôi thu được 10 phản hồi cho mỗi tệp nguồn đầu vào bằng cách sử dụng API suy luận OpenAI. Theo công trình gần đây [10], để khuyến khích sự đa dạng trong các phản hồi được lấy mẫu, chúng tôi sử dụng kết hợp các cài đặt nhiệt độ cho mô hình (kiểm soát tính ngẫu nhiên trong các phản hồi được tạo ra): 1 phản hồi với nhiệt độ = 0 (giải mã tham lam), 6 phản hồi với nhiệt độ 0,75, và 3 phản hồi với nhiệt độ 1,0. Chúng tôi sử dụng GPT-4 làm người xếp hạng (các điều tra sớm của chúng tôi cho thấy rằng GPT-4 tốt hơn đáng kể về khả năng lý luận với diff mã so với GPT-3.5-Turbo) và thu được một phản hồi duy nhất (điểm) cho mỗi bản sửa đổi ứng viên, với nhiệt độ 0.

Các chỉ số đánh giá: Đối với mỗi tập dữ liệu, chúng tôi báo cáo số lượng tệp được gắn cờ và số lượng tổng cộng các vấn đề được gắn cờ trên các tệp. Chúng tôi đo lường có bao nhiêu tệp có ít nhất một bản sửa đổi vượt qua kiểm tra tĩnh và có bao nhiêu vấn đề còn lại trong các tệp không có bản sửa đổi như vậy sau khi các giai đoạn LLM Đề xuất và LLM Xếp hạng của CORE được áp dụng. Trong nghiên cứu người dùng, chúng tôi đo lường có bao nhiêu tệp có ít nhất một bản sửa đổi được chấp nhận bởi người đánh giá con người và báo cáo có bao nhiêu bản sửa đổi được chấp nhận và từ chối trên các tệp bởi người đánh giá. Chúng tôi gọi số lượng bản sửa đổi được tạo ra bởi pipeline CORE nhưng bị từ chối bởi người đánh giá con người là kết quả dương tính giả.

5 ĐÁNH GIÁ

Mục tiêu của chúng tôi là đánh giá toàn diện pipeline CORE từ đầu đến cuối trên các nhiệm vụ sửa đổi mã cải thiện chất lượng khác nhau và trả lời các câu hỏi sau:

RQ1: Pipeline CORE từ đầu đến cuối hiệu quả như thế nào trong việc giảm thiểu các vấn đề chất lượng mã và vượt qua sự kiểm tra của LLM Xếp hạng trên điểm chuẩn Python CQPy?

RQ2: Có bao nhiêu bản sửa đổi được tạo ra bởi CORE cũng được chấp nhận bởi người đánh giá con người trên điểm chuẩn Python CQPyUS?

RQ3: Pipeline CORE dễ dàng tổng quát hóa đến một ngôn ngữ lập trình khác (Java) và một công cụ phân tích tĩnh (SonarQube) như thế nào?

RQ4: CORE so sánh như thế nào với một kỹ thuật sửa chữa chương trình tự động tiên tiến (Sorald) để giảm thiểu cảnh báo phân tích tĩnh?

5.1 RQ1: Pipeline CORE từ đầu đến cuối hiệu quả như thế nào trong việc giảm thiểu các vấn đề chất lượng mã và vượt qua sự kiểm tra của LLM Xếp hạng trên điểm chuẩn Python CQPy?

Chúng tôi bắt đầu bằng cách xem xét hiệu suất tổng thể của pipeline CORE về (1) sửa các vấn đề chất lượng mã như được xác định bởi công cụ phân tích tĩnh mà CORE được cấu hình, và (2) sự chấp nhận như được xác định bởi LLM Xếp hạng bằng cách sử dụng tiêu chí đánh giá chi tiết để đánh giá các bản sửa đổi mã (như được mô tả trong Phần 3.2).

Kết quả đánh giá tổng thể của pipeline CORE từ đầu đến cuối (trên các tập dữ liệu và chỉ số được giới thiệu trong Phần 4) được trình bày trong Bảng 1. Đối với RQ1, chúng tôi sẽ tập trung vào hàng đầu tiên, tương ứng với pipeline CORE được cấu hình với CodeQL làm công cụ phân tích tĩnh, và 52 kiểm tra chất lượng là một phần của tập dữ liệu Python CQPy.

Khối cột đầu tiên hiển thị thống kê tập dữ liệu. Có 1993 vấn đề chất lượng (tức là, vi phạm kiểm tra tĩnh) được gắn cờ trong 765 tệp của tập dữ liệu CQPy, với mỗi tệp có ít nhất một vấn đề được gắn cờ, đến cuối giai đoạn 1 trong Hình 2. Trong khối cột thứ hai, chúng tôi hiển thị hiệu quả của LLM Đề xuất, sau khi các bản sửa đổi ứng viên được đề xuất (10 bản sửa đổi cho mỗi tệp được gắn cờ) được lọc bởi công cụ (tức là, CodeQL cho hàng đầu tiên) đến cuối giai đoạn 4. Đầu tiên, chúng tôi quan sát thấy 81,57% các tệp được gắn cờ được sửa hoàn toàn như được xác thực bởi công cụ phân tích tĩnh, tức là, chúng có ít nhất một bản sửa đổi hoàn toàn vượt qua bộ kiểm tra tĩnh với không có vấn đề nào được gắn cờ. Thứ hai, chúng tôi quan sát thấy rằng, số lượng vấn đề còn lại trung bình cho mỗi tệp đã sửa đổi, đến cuối giai đoạn 4 của pipeline CORE, là 0,41 so với hơn 2,6 vấn đề trung bình cho mỗi tệp nguồn ở đầu pipeline. Điều này đặc biệt đáng chú ý vì LLM Đề xuất có thể thực hiện sửa đổi chỉ với hướng dẫn bằng ngôn ngữ tự nhiên, mà không cần cung cấp rõ ràng bất kỳ ví dụ huấn luyện nào dưới dạng ⟨mã trước, mã sau⟩ thường cần thiết cho các công cụ sửa chữa chương trình tự động.

Khả năng tuân theo hướng dẫn của LLM Đề xuất để thực hiện sửa đổi mã, mặc dù ấn tượng, cũng có thể tạo ra các bản sửa lỗi giả vượt qua kiểm tra tĩnh. Trong giai đoạn cuối của pipeline CORE, LLM Xếp hạng sử dụng tiêu chí đánh giá chi tiết (trong prompt được xây dựng cẩn thận được trình bày trong Phần 3.2) để từ chối các bản sửa lỗi giả như vậy và chấp nhận các bản sửa đổi có khả năng cũng được chấp nhận bởi các nhà phát triển. Từ khối cột cuối của Bảng 1, chúng ta thấy rằng 583 trong số 624 tệp được xếp hạng cao, tức là, chấp nhận mạnh hoặc yếu, bởi LLM Xếp hạng đến cuối giai đoạn 5. Cụ thể, LLM Xếp hạng (mạnh hoặc yếu) từ chối mọi bản sửa đổi (có thể giả) cho 41 tệp mặc dù chúng được chấp nhận bởi công cụ ở giai đoạn 4. Trong RQ tiếp theo, chúng tôi phân tích mức độ tương quan giữa sự chấp nhận và từ chối của LLM Xếp hạng với người đánh giá con người, trên một tập con của tập dữ liệu CQPy.

5.2 RQ2: Có bao nhiêu bản sửa đổi được tạo ra bởi CORE cũng được chấp nhận bởi người đánh giá con người trên điểm chuẩn Python CQPyUS?

Trong RQ này, chúng tôi điều tra tính đúng đắn của các bản sửa đổi được tạo ra bởi CORE, và đặc biệt là hiệu quả của LLM Xếp hạng, bằng cách tiến hành nghiên cứu người dùng. Chúng tôi sử dụng một tập con của CQPy, gọi là CQPyUS, với một mẫu 10 tệp cho mỗi kiểm tra chất lượng, đã tạo ra 2397 bản sửa đổi ứng viên (từ giai đoạn 4) để được kiểm tra thủ công. Kết quả pipeline CORE cho tập dữ liệu này được trình bày trong hàng thứ hai của Bảng 1, nơi xu hướng gần giống với CQPy trong hàng đầu tiên.

Đối với mỗi trong số 453 tệp trong CQPyUS ra khỏi giai đoạn 4 (như thấy từ hàng 2, Bảng 1), chúng tôi yêu cầu một người đánh giá con người gắn nhãn tất cả các bản sửa đổi cho tệp là chấp nhận hoặc từ chối. Chúng tôi cung cấp cùng tiêu chí mà chúng tôi đưa ra làm prompt cho LLM Xếp hạng (được trình bày trong Phần 3.2) cho người đánh giá để đánh giá tính đúng đắn của các bản sửa đổi - thay đổi duy nhất là chúng tôi yêu cầu người đánh giá đưa ra quyết định chấp nhận/từ chối nhị phân hơn là một điểm số phân cấp mà chúng tôi khai thác từ LLM Xếp hạng. Nhóm người dùng của chúng tôi bao gồm 15 nhà phát triển Python (cấp độ trung cấp, với 1-3 năm kinh nghiệm kỹ thuật phần mềm). Mỗi bản sửa đổi chỉ được gắn nhãn bởi một người dùng, và mỗi người dùng chịu trách nhiệm gắn nhãn các bản sửa đổi của 2 đến 4 (được chọn ngẫu nhiên) truy vấn từ tập dữ liệu. Không ai trong số các tác giả của bài báo này là một phần của nhóm người dùng.

Kết quả của nghiên cứu người dùng CQPyUS được trình bày trong Bảng 2. Hàng đầu tiên của Bảng hiển thị các chỉ số (cơ sở) cho nghiên cứu người dùng mà chúng tôi tiến hành - tất cả các đầu ra của giai đoạn 4 được đánh giá bởi người dùng, và 70,64% các tệp được đánh giá có ít nhất một bản sửa đổi mà một người đánh giá con người chấp nhận. Tuy nhiên, từ cột cuối, chúng ta thấy rằng tỷ lệ chấp nhận cao này đi kèm với chi phí cao là 1321 kết quả dương tính giả, tức là, 55,11% các bản sửa đổi mà CodeQL chấp nhận bị từ chối bởi người dùng. Sự đánh đổi giữa tỷ lệ chấp nhận và kết quả dương tính giả của pipeline này có thể quan trọng trong thực tế.

Trong phần tiếp theo, chúng tôi chỉ ra rằng LLM Xếp hạng giúp đạt được sự đánh đổi tốt hơn đáng kể.

Được trang bị các nhãn chấp nhận/từ chối được người dùng đưa ra cho tất cả các bản sửa đổi được CodeQL chấp nhận của tập dữ liệu CQPyUS, chúng tôi hỏi: LLM Xếp hạng có thể giúp phân biệt các bản sửa đổi đúng với các bản sai, điều này sẽ giúp giảm thiểu gánh nặng đánh giá của các nhà phát triển không? Chúng tôi trả lời câu hỏi này một cách khẳng định trong các hàng tiếp theo của Bảng 2. Từ hàng thứ hai, chúng ta thấy rằng nếu chúng ta chỉ hiển thị các ứng viên được chấp nhận mạnh bởi LLM Xếp hạng, tỷ lệ từ chối giảm xuống 47,55%. Về mặt tuyệt đối, số lượng từ chối giảm từ 1321 xuống 835, gần 25% giảm. Đồng thời, 72,68% các tệp được kiểm tra có ít nhất một bản sửa đổi được chấp nhận bởi người đánh giá. Hơn nữa, từ hàng cuối, chúng ta thấy rằng nếu chúng ta chỉ xem xét các tệp mà không có bản sửa đổi nào được (mạnh hoặc yếu) chấp nhận bởi LLM Xếp hạng, người dùng cũng từ chối hơn 82% các bản sửa đổi đó; điều này cho thấy rằng việc bỏ các bản từ chối tin cậy thấp bởi LLM Xếp hạng thực sự có thể giúp giảm đáng kể gánh nặng đánh giá của các nhà phát triển trong thực tế.

Pipeline CORE, với bộ đôi LLM Đề xuất-Xếp hạng, để giải quyết các vấn đề chất lượng mã là hiệu quả và có thể được triển khai trong quy trình kỹ thuật phần mềm thực tế. Chỉ dựa vào các công cụ (tượng trưng) để lọc bản sửa đổi là có vấn đề - LLM Xếp hạng giúp giảm số lượng kết quả dương tính giả rất nhiều, đồng thời cũng đảm bảo rằng các bản sửa đổi có thể chấp nhận được bảo tồn tính đúng đắn chức năng được hiển thị cho các nhà phát triển.

5.3 RQ3: Pipeline CORE dễ dàng tổng quát hóa đến một ngôn ngữ lập trình khác (Java) và một công cụ phân tích tĩnh (SonarQube) như thế nào?

CORE có thể xử lý các ngôn ngữ lập trình và công cụ phân tích tĩnh khác nhau ngay lập tức. Để chứng minh điều này, trong RQ này, chúng tôi cấu hình CORE với một công cụ phân tích tĩnh được sử dụng rộng rãi khác là SonarQube, và 10 kiểm tra tĩnh từ tập dữ liệu SQJava (được giới thiệu trong Phần 4). Cấu hình này rất đơn giản; chúng tôi mất ít hơn một tuần để hoàn thành điều này. Thực tế, các dòng mã cần thay đổi trong triển khai CORE của chúng tôi (bằng Python) cho cấu hình này ít hơn 100. Cụ thể, chúng tôi không phải điều chỉnh hoặc tinh chỉnh các prompt của LLM Đề xuất và Xếp hạng trong pipeline của chúng tôi để phù hợp với công cụ mới hoặc ngôn ngữ lập trình. Các tác giả của tập dữ liệu Sorald đã cung cấp các mô tả rõ ràng và khuyến nghị sửa lỗi cho 10 kiểm tra, mà chúng tôi dễ dàng sử dụng để khởi tạo các prompt LLM của chúng tôi. Hơn nữa, SonarQube cung cấp định vị cho các vi phạm kiểm tra (số dòng trong tệp nguồn) cần thiết để trích xuất các khối mã như được thảo luận trong Phần 3.1.

Chúng tôi báo cáo kết quả trên tập dữ liệu SQJava bao gồm các kho lưu trữ Java thực tế trong hàng cuối của Bảng 1. Có 999 vấn đề chất lượng được gắn cờ trong 483 tệp của tập dữ liệu, với mỗi tệp có ít nhất một vấn đề được gắn cờ, đến cuối giai đoạn 1 của CORE. Như trong trường hợp của các tập dữ liệu khác (hàng đầu tiên và thứ hai), chúng tôi thấy rằng hơn 82% tệp có ít nhất một bản sửa đổi ứng viên hoàn toàn vượt qua kiểm tra SonarQube liên quan. Hơn nữa, số lượng vấn đề còn lại trung bình đến cuối giai đoạn 4 là khoảng 0,56 mỗi tệp, so với hơn 2 vấn đề mỗi tệp trung bình để bắt đầu. Từ cột cuối, chúng ta thấy rằng LLM Xếp hạng từ chối tất cả các bản sửa đổi (có thể giả) vượt qua kiểm tra SonarQube cho 26 tệp, và (mạnh hoặc yếu) chấp nhận ít nhất một bản sửa đổi cho 371 tệp, hơn 76% tổng số tệp.

5.4 RQ4: CORE so sánh như thế nào với một kỹ thuật sửa chữa chương trình tự động tiên tiến (Solard) để giảm thiểu cảnh báo phân tích tĩnh?

Chúng tôi so sánh CORE với công cụ sửa chữa chương trình tự động tiên tiến Sorald [46] để sửa các vấn đề kiểm tra tĩnh trong mã. Sorald là một cách tiếp cận dựa trên quy tắc tận dụng "mẫu lập trình meta", về cơ bản là các biến đổi AST-to-AST, có thể được áp dụng trên các vi phạm được phát hiện trong mã. Cụ thể, đối với mỗi vị trí vi phạm trong mã, Sorald áp dụng một mẫu lập trình meta vào phần tử AST tương ứng để sửa nó. Họ triển khai thủ công một mẫu lập trình meta cho mỗi kiểm tra tĩnh, dựa trên khuyến nghị sửa lỗi cho kiểm tra, mà chúng tôi sử dụng trực tiếp dưới dạng hướng dẫn bằng ngôn ngữ tự nhiên trong pipeline CORE. Mặc dù công cụ sửa chữa của họ có thể mở rộng cho các ngôn ngữ và công cụ phân tích tĩnh khác, triển khai có sẵn công khai của họ⁶ là cho Java và SonarQube. Vì vậy, đối với RQ này, chúng tôi tập trung vào tập dữ liệu SQJava.

Kết quả so sánh trên tập dữ liệu Java được trình bày trong Bảng 3. Trong số 483 tệp trong tập dữ liệu, CORE, tức là, đầu ra của giai đoạn 5, xem xét các bản sửa đổi được (mạnh/yếu) chấp nhận bởi LLM Xếp hạng, sửa 371 tệp hoàn toàn, với tỷ lệ 76,8%. Điều này tương đương với công cụ Sorald được chế tạo thủ công sửa 378 tệp. Mặt khác, số lượng vấn đề còn lại đến cuối pipeline CORE là 270 (khoảng 27%), ít hơn đáng kể so với 371 (hơn 37%) đối với công cụ Sorald.

6 CÁC MỐI ĐE DỌA TÍNH HỢP LỆ

Một mối đe dọa có thể đối với tính hợp lệ là mã đầu vào trong tập dữ liệu của chúng tôi có thể đã được LLM nhìn thấy trong quá trình huấn luyện. LLM không có khả năng đã nhìn thấy các prompt được xây dựng bởi chúng tôi được ghép nối với các bản sửa đổi mã mong đợi trong quá trình huấn luyện. Do đó, kết quả của chúng tôi có thể được quy cho khả năng tuân theo hướng dẫn của LLM, kiến thức của chúng về ngôn ngữ lập trình và các chi tiết thông tin mà chúng tôi cung cấp trong các prompt của chúng tôi. Bằng cách dựa trên các thí nghiệm của chúng tôi trên hàng trăm vấn đề được gắn cờ bởi 52 kiểm tra tĩnh đa dạng cho Python và 10 kiểm tra tĩnh đa dạng cho Java từ hai công cụ phân tích tĩnh khác nhau, chúng tôi tránh khả năng thiên vị kết quả của chúng tôi đối với một tập dữ liệu nhỏ, các vấn đề chất lượng mã nhất định, hoặc một công cụ hoặc ngôn ngữ lập trình duy nhất. Chúng tôi tuân theo thiết lập thí nghiệm chính xác như CodeQueries và Solard để tránh bất kỳ vấn đề không khớp ngôn ngữ hoặc phiên bản công cụ nào.

Mã được tạo ra bởi LLM có thể vượt qua các kiểm tra tĩnh trước đó thất bại nhưng thay đổi ngữ nghĩa mã, ví dụ, bằng cách xóa hoàn toàn mã. Để giảm thiểu vấn đề này, chúng tôi thực hiện đánh giá con người để xác minh tính đúng đắn của các bản sửa đổi, mặc dù trên một tập con của tập dữ liệu Python của chúng tôi, nhưng đảm bảo bao phủ đầy đủ về các kiểm tra tĩnh. Việc gắn nhãn thủ công này có thể có nhiễu. Tất cả các nhãn được xem xét độc lập bởi một trong các tác giả để tránh các trường hợp như vậy.

Chúng tôi thấy các trường hợp mà người dùng không chắc chắn tại sao công cụ gắn cờ vi phạm trong mã nguồn ngay từ đầu, hoặc liệu bản sửa lỗi trong bản sửa đổi có tác dụng phụ không mong muốn hay không. Cũng có một vài trường hợp khó khăn để xác minh thủ công tính đúng đắn của các bản sửa đổi. Ví dụ, xem xét kiểm tra tĩnh "import * may pollute namespace" cho các tệp Python⁷. Các bản sửa đổi đúng sẽ thay thế * bằng các mô-đun liên quan. Tuy nhiên, việc xác minh xem tất cả các import cần thiết có được liệt kê đầy đủ hay không có thể khó khăn, đặc biệt đối với các tệp nguồn lớn. Việc xem xét nhiều ứng viên sửa đổi cho một số tệp hữu ích cho người dùng trong vấn đề này - bất cứ khi nào hai ứng viên cho một tệp có tập con import được liệt kê không chồng lấp, người dùng cố gắng lý luận về sự khác biệt và có thể giải quyết tính không đầy đủ của một hoặc cả hai bản sửa đổi. Để tránh các trường hợp làm tăng hoặc thiên vị kết quả đánh giá của chúng tôi, chúng tôi hướng dẫn người dùng từ chối các bản sửa đổi mà họ không chắc chắn, như trong một số ví dụ được đề cập ở trên, nghiêng về phía an toàn hơn.

7 CÔNG TRÌNH LIÊN QUAN

Sửa chữa chương trình tự động là một chủ đề nghiên cứu tích cực và nhiều công cụ đã được xây dựng trong những năm qua. Ở đây, chúng tôi thảo luận về công trình liên quan gần nhất và giới thiệu người đọc đến các khảo sát tuyệt vời [23,24,38] để có phạm vi bao phủ rộng hơn.

7.1 Sửa chữa vi phạm kiểm tra tĩnh

Trong số các cách tiếp cận nhắm vào lỗi phân tích tĩnh, [19,25,46,50] sử dụng các biến đổi chương trình tượng trưng được thiết kế thủ công để sửa các lớp thuộc tính cụ thể như an toàn heap [50], lỗ hổng bảo mật [25], kiểm tra chất lượng tĩnh [46] hoặc data race [19]. Các cách tiếp cận khác [12,14,32, 33,37,44] khai thác các mẫu tượng trưng từ dữ liệu cam kết để học chiến lược sửa chữa hoặc học chúng từ dữ liệu được tạo tổng hợp [26]. Ví dụ, SpongeBugs [37] sử dụng SonarQube [8] để tìm lỗi và dữ liệu cam kết để tạo tập dữ liệu ghép nối. Tương tự, Avatar [32,33] và Phoenix [14] sử dụng FindBugs [4] và dữ liệu cam kết. Revisar [44] khai thác chỉnh sửa từ dữ liệu cam kết cho PMD [7]. GetAFix [12] sử dụng Infer [5] và Error Prone [9], và khai thác các mẫu chỉnh sửa cây chung từ dữ liệu cam kết bằng cách sử dụng anti unification.

Các kỹ thuật sửa chữa này chỉ có thể tổng hợp những bản sửa lỗi được bao phủ bởi các mẫu tượng trưng của chúng. Một cách tiếp cận thay thế dựa trên học tập [28,49,54,56] là huấn luyện các mô hình thần kinh để ánh xạ các chương trình có lỗi thành các phiên bản đã được sửa. Các mô hình thần kinh học để biến đổi mã trực tiếp. Tuy nhiên, phạm vi của chúng được xác định bởi sự đa dạng của các ví dụ sửa lỗi có trong dữ liệu huấn luyện và chúng không tổng quát hóa cho các lớp lỗi mới không được thấy trong quá trình huấn luyện.

Tất cả các cách tiếp cận này đòi hỏi nỗ lực tuyển dụng dữ liệu và học tập ngoại tuyến rộng rãi, và yêu cầu thiết kế lại khi nhắm vào các loại lỗi khác nhau. Ngược lại, hướng công việc mà chúng tôi theo đuổi, sử dụng LLM, không yêu cầu bất kỳ nỗ lực tuyển dụng dữ liệu hoặc học tập nào. Vì LLM đã được huấn luyện trước với một kho mã lớn và các tài liệu khác, chúng có thể được tùy chỉnh dễ dàng để sửa đổi mã để sửa bất kỳ loại lỗi nào được phát hiện bởi phân tích tĩnh, chỉ bằng cách viết các prompt phù hợp.

7.2 LLM cho sửa chữa chương trình

Lợi thế được đề cập ở trên của việc sử dụng LLM đã thúc đẩy các nhà nghiên cứu khác sử dụng chúng cho sửa chữa chương trình. Xia, Wei và Zhang [52] sử dụng LLM với các prompt few-shot để tạo ra các bản sửa lỗi ứng viên trên mã có lỗi từ các điểm chuẩn Defects-4J, QuixBug và ManyBugs và sử dụng các giá trị entropy (log âm của xác suất của mỗi token được tạo ra) để xếp hạng các bản sửa lỗi ứng viên. Công trình dựa vào sự tồn tại của một bộ test để xác thực một bản sửa lỗi ứng viên. Trong một công trình gần đây hơn, Xia và Zhang [53] sử dụng một cách tiếp cận hội thoại, nơi một bộ test là yêu cầu, và các thông báo lỗi từ các test thất bại được sử dụng theo phong cách hội thoại với LLM để tinh chỉnh bản sửa lỗi ứng viên thành một bản vượt qua bộ test, và trình bày kết quả trên các điểm chuẩn QuixBug. Một hướng công việc thú vị khác là sửa lỗi trong mã được tạo ra bởi một LLM bằng cách sử dụng các kỹ thuật sửa chữa chương trình truyền thống hoặc một LLM khác [21, 27, 35].

Các cách tiếp cận này nhằm mục đích sửa các lỗi được xác định bởi các test case thất bại. So với đó, công trình của chúng tôi giải quyết một vấn đề liên quan nhưng khác biệt, một vấn đề sửa các lỗi được gắn cờ bởi các công cụ phân tích tĩnh như CodeQL và SonarQube.

Kỹ thuật prompting: RING [30] sửa các lỗi cú pháp và ngữ nghĩa đơn giản trên nhiều ngôn ngữ bằng cách sử dụng LLM và prompting few-shot tăng cường truy xuất. Độ phức tạp của các lỗi và bản sửa lỗi cần thiết trong trường hợp của chúng tôi cao hơn. InferFix [29] nhắm vào các vi phạm được gắn cờ bởi bộ phân tích tĩnh Infer [5,6] cho ba loại lỗi. Tuy nhiên, nó xây dựng các prompt được tăng cường với chú thích loại lỗi và các cặp lỗi-sửa tương tự, và tinh chỉnh mô hình Codex trên các prompt này. Chúng tôi sử dụng LLM dựa trên hướng dẫn trong thiết lập zero-shot (tức là, không cần ví dụ ⟨mã trước, mã sau⟩) mà không tinh chỉnh. Pearce et al. [42] sửa các lỗ hổng bảo mật bằng cách sử dụng LLM tự hồi quy được prompted với mã một phần trong đó các dòng có lỗi được comment và LLM được prompted để tạo ra một phiên bản "đã sửa" của những dòng đó. Chúng tôi sử dụng một lớp LLM được điều chỉnh hướng dẫn mạnh mẽ hơn có lợi từ các hướng dẫn chi tiết cung cấp ngữ cảnh bổ sung cần thiết để tạo ra mã đã sửa đúng cách. Các prompt của chúng tôi bao gồm mô tả về vấn đề chất lượng, các giải pháp được đề xuất, gợi ý định vị và ràng buộc. Do tính chất tự hồi quy, các thế hệ trong [42] chỉ được điều kiện hóa trên tiền tố của mã có lỗi, trong khi chúng tôi chuyển mã có lỗi trong prompt và do đó, việc tạo mã có thể chú ý đến ngữ cảnh mã hai chiều, cả trước và sau các dòng có lỗi trong mã đầu vào.

Xác thực bản sửa lỗi: Các cách tiếp cận trên dựa vào phân tích tĩnh để xác thực các bản sửa lỗi và tính khả dụng của unit test để phát hiện hồi quy. Chúng tôi sử dụng kết hợp hai oracle: (1) bộ phân tích tĩnh, và (2) LLM xếp hạng thứ hai, để đảm bảo rằng bản sửa lỗi không vượt qua phân tích tĩnh một cách vô nghĩa, để tự động tạo ra các bản sửa lỗi có thể chấp nhận được vừa đúng chức năng vừa vượt qua phân tích tĩnh.

7.3 LLM như người xác minh

Vấn đề của các bản sửa lỗi có vẻ hợp lý nhưng không chính xác là đã được biết đến [36,43]. CORE có thể tạo ra mã vượt qua kiểm tra tĩnh (một bản sửa lỗi có vẻ hợp lý) nhưng thay đổi ngữ nghĩa của mã đầu vào theo những cách không mong muốn. Nhà phát triển có thể xem xét các bản sửa đổi mã được xác thực tĩnh để lọc ra các trường hợp như vậy. Unit test cũng có thể giúp bắt các trường hợp như vậy, nhưng chúng không phải lúc nào cũng có sẵn hoặc bản thân chúng có thể không đầy đủ. LLM đã được chứng minh là hiệu quả trong việc đánh giá và giám sát chất lượng đầu ra từ các LLM khác [13,31], do đó giúp giảm nỗ lực cần thiết cho việc đánh giá con người. Việc sử dụng LLM, đặc biệt là GPT-4, để đánh giá các thế hệ mã đã được thử nghiệm gần đây. Olausson et al. [39] cũng có thiết lập LLM kép, nơi họ sử dụng phản hồi từ GPT-4 dưới dạng phê bình để sửa đổi prompt của LLM đề xuất cho các nhiệm vụ tạo mã. Zhuo [57] xây dựng một prompt chi tiết cho GPT-3.5-Turbo để thực hiện hai khía cạnh đánh giá của các thế hệ mã, cụ thể là, tính hữu ích của mã và tính đúng đắn chức năng dựa trên đánh giá. Được truyền cảm hứng từ những phát hiện này, để giảm gánh nặng cho nhà phát triển, chúng tôi sử dụng một thể hiện thứ hai của LLM (GPT-4) như một người xếp hạng để chấm điểm các ứng viên được tạo ra bởi LLM đề xuất dựa trên (1) tính đúng đắn của việc giải quyết vấn đề, và (2) bảo tồn tính đúng đắn chức năng của mã gốc. Các tập dữ liệu tạo mã được nghiên cứu trong [57] chủ yếu bao gồm các đoạn mã nhỏ, không giống như thiết lập của chúng tôi nơi chúng tôi sử dụng các tệp mã nguồn thực tế lớn. Chúng tôi làm việc với diff mã trong prompt LLM Xếp hạng, và trong các điều tra của chúng tôi, GPT-4 tốt hơn đáng kể về khả năng lý luận với diff mã so với GPT-3.5-Turbo mà Zhuo [57] sử dụng.

8 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chất lượng mã là một mối quan tâm dai dẳng trong kỹ thuật phần mềm. Mặc dù đã có nhiều tiến bộ trong việc phát hiện các vấn đề này một cách tĩnh, việc sửa chữa chúng tự động vẫn là thách thức do sự đa dạng của các vấn đề chất lượng mã xuất hiện trong mã thực tế. Đề xuất của chúng tôi trong công trình này là sử dụng sức mạnh của các mô hình ngôn ngữ lớn, đặc biệt là những mô hình vượt ra ngoài việc hoàn thành mã và có thể tuân theo hướng dẫn bằng ngôn ngữ tự nhiên, để hỗ trợ các nhà phát triển trong việc sửa đổi và cải thiện mã của họ. Thông qua đánh giá toàn diện trên hai điểm chuẩn công khai trong Python và Java sử dụng 52 và 10 kiểm tra tĩnh từ hai công cụ khác nhau, chúng tôi cho thấy lời hứa của cách tiếp cận này khi kết hợp với các prompt được chế tạo cẩn thận. Chúng tôi tiếp tục chỉ ra rằng bằng cách sử dụng một thể hiện LLM làm người xếp hạng, đánh giá khả năng chấp nhận của các bản sửa đổi mã được đề xuất, chúng tôi có thể hiệu quả bắt các bản sửa lỗi có vẻ hợp lý nhưng không chính xác và giảm gánh nặng cho nhà phát triển.

Mục tiêu của chúng tôi cho tương lai là mở rộng phạm vi của công cụ CORE bằng cách xây dựng thêm các thành phần trong pipeline để không chỉ hỗ trợ thêm các công cụ và kiểm tra mà còn cải thiện chất lượng và tính đúng đắn của các bản sửa lỗi được tạo ra. Chúng tôi tin rằng cải thiện liên tục dựa trên phản hồi là chìa khóa để làm cho công việc này trở thành chủ đạo. Để làm điều này, chúng tôi dự định dựa vào các kỹ thuật phân tích tĩnh và động truyền thống để tạo phản hồi tự động và sử dụng những tiến bộ gần đây trong việc tinh chỉnh các mô hình bằng cách sử dụng các kỹ thuật dựa trên học tăng cường và phản hồi con người [18,41,47].

TÀI LIỆU THAM KHẢO

[1] [n. d.]. Trang web CodeQL. https://codeql.github.com/. Truy cập: 15 tháng 9, 2023.
[2] [n. d.]. Coverity Static Analysis. https://www.synopsys.com/software-integrity/security-testing/static-analysis-sast.html. Truy cập: 15 tháng 9, 2023.
[3] [n. d.]. __eq__ not overridden when adding attributes. https://codeql.github.com/codeql-query-help/python/py-missing-equals/. Truy cập: 15 tháng 9, 2023.
[4] [n. d.]. Dự án FindBugs. https://spotbugs.github.io/. Truy cập: 15 tháng 9, 2023.
[5] [n. d.]. Bộ phân tích tĩnh Infer. https://fbinfer.com/. Truy cập: 15 tháng 9, 2023.
[6] [n. d.]. Bộ phân tích tĩnh InferSharp. https://github.com/microsoft/infersharp. Truy cập: 15 tháng 9, 2023.
[7] [n. d.]. PMD: Một bộ phân tích mã tĩnh đa ngôn ngữ có thể mở rộng. https://pmd.github.io/. Truy cập: 15 tháng 9, 2023.
[8] [n. d.]. SonarQube. https://docs.sonarqube.org/latest/. Truy cập: 15 tháng 9, 2023.
[9] Edward Aftandilian, Raluca Sauciuc, Siddharth Priya, và Sundaresan Krishnan. 2012. Xây dựng các công cụ phân tích chương trình hữu ích bằng cách sử dụng một trình biên dịch java có thể mở rộng. Trong 2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation. IEEE, 14–23.
[10] Lakshya A Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K Lahiri, và Sriram K Rajamani. 2023. Hướng dẫn các Mô hình Ngôn ngữ của Mã với Ngữ cảnh Toàn cục bằng cách sử dụng Monitors. arXiv preprint arXiv:2306.10763 (2023).
[11] Pavel Avgustinov, Oege de Moor, Michael Peyton Jones, và Max Schäfer. 2016. QL: Object-oriented Queries on Relational Data. Trong 30th European Conference on Object-Oriented Programming. Schloss Dagstuhl - Leibniz-Zentrum für Informatik.
[12] Johannes Bader, Andrew Scott, Michael Pradel, và Satish Chandra. 2019. Getafix: Học để Sửa Lỗi Tự động. Proc. ACM Program. Lang. 3, OOPSLA, Bài viết 159 (oct 2019), 27 trang. https://doi.org/10.1145/3360585
[13] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional AI: Harmlessness from AI Feedback. arXiv preprint arXiv:2212.08073 (2022).
[14] Rohan Bavishi, Hiroaki Yoshida, và Mukul R. Prasad. 2019. Phoenix: Tổng hợp Sửa chữa Tự động Dựa trên Dữ liệu cho Vi phạm Phân tích Tĩnh. Trong Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Tallinn, Estonia) (ESEC/FSE 2019). Association for Computing Machinery, New York, NY, USA, 613–624. https://doi.org/10.1145/3338906.3338952
[15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Các mô hình ngôn ngữ là người học few-shot. Advances in neural information processing systems 33 (2020), 1877–1901.
[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên mã. arXiv preprint arXiv:2107.03374 (2021).
[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Mở rộng mô hình ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311 (2022).
[18] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, và Dario Amodei. 2017. Học tăng cường sâu từ sở thích con người. Advances in neural information processing systems 30 (2017).
[19] Andreea Costea, Abhishek Tiwari, Sigmund Chianasta, Abhik Roychoudhury, và Ilya Sergey. 2023. Hippodrome: Sửa chữa data race bằng cách sử dụng tóm tắt phân tích tĩnh. ACM Transactions on Software Engineering and Methodology 32, 2 (2023), 1–33.
