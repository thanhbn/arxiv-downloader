# Làm nền tảng cho việc tạo mã Data Science với các đặc tả đầu vào-đầu ra

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) gần đây đã thể hiện khả năng đáng chú ý trong việc tạo ra mã từ các lời nhắc ngôn ngữ tự nhiên (NL). Tuy nhiên, trong thế giới thực, NL thường quá mơ hồ để nắm bắt ý định thực sự đằng sau các vấn đề lập trình, đòi hỏi thêm các đặc tả đầu vào-đầu ra (I/O) bổ sung. Thật không may, các LLM có thể gặp khó khăn trong việc điều chỉnh đầu ra của chúng với cả lời nhắc NL và đặc tả I/O. Trong bài báo này, chúng tôi đưa ra một cách để giảm thiểu vấn đề này trong bối cảnh lập trình khoa học dữ liệu, nơi các tác vụ yêu cầu đặc tả I/O rõ ràng để có sự rõ ràng. Cụ thể, chúng tôi đề xuất GIFT4CODE, một phương pháp mới cho việc tinh chỉnh hướng dẫn của các LLM đối với các đặc tả I/O. Phương pháp của chúng tôi tận dụng dữ liệu tổng hợp được tạo ra bởi chính LLM và sử dụng phản hồi dựa trên thực thi như một tín hiệu học tập quan trọng. Phản hồi này, dưới dạng đặc tả I/O chương trình, được cung cấp cho LLM để hỗ trợ tinh chỉnh hướng dẫn. Chúng tôi đánh giá phương pháp của mình trên hai benchmark khoa học dữ liệu đầy thử thách, ARCADE và DS-1000. Kết quả cho thấy sự cải thiện đáng kể trong khả năng của LLM tạo ra mã không chỉ có thể thực thi được mà còn phù hợp chính xác với các đặc tả của người dùng, cải thiện đáng kể chất lượng tạo mã cho các tác vụ khoa học dữ liệu phức tạp.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) gần đây đã cho thấy tiềm năng to lớn trong việc tạo ra mã từ các lời nhắc ngôn ngữ tự nhiên (Chen et al., 2021a; Austin et al., 2021; Li et al., 2023; 2022; Nijkamp et al., 2022; Fried et al., 2022; Li et al., 2023). Đặc biệt, các LLM được huấn luyện trên mã đã được chứng minh là xuất sắc trong việc giải quyết các vấn đề lập trình kiểu phỏng vấn, được đại diện bởi các benchmark như HumanEval (Chen et al., 2021a). Trong những tác vụ này, "ý định" ngôn ngữ tự nhiên (NL) (ví dụ x trong Hình 1) có thể định nghĩa một cách ngắn gọn vấn đề lập trình hiện tại. Mục tiêu của LLM là tạo ra đầu ra phù hợp với ý định này.

Tuy nhiên, trong thế giới thực, không phải tất cả các tác vụ lập trình đều có thể được mô tả dễ dàng bằng các ý định NL gọn gàng. Trong số này, lập trình khoa học dữ liệu nổi bật như một lĩnh vực quan trọng, nơi các nhà khoa học dữ liệu dành thời gian đáng kể cho việc xử lý dữ liệu và phân tích dữ liệu khám phá (EDA). Trong khi các vấn đề trong các benchmark như HumanEval liên quan đến các kiểu Python cơ bản có thể được kiểm tra dễ dàng, các tác vụ khoa học dữ liệu thường liên quan đến các hoạt động phức tạp trên các cấu trúc dữ liệu như Pandas DataFrames hoặc PyTorch Tensors. Sự mơ hồ vốn có trong việc thể hiện các tác vụ này thông qua các ý định NL (ví dụ văn bản màu xanh lá cây trong Hình 1) thường dẫn đến sự không phù hợp giữa yêu cầu của nhà phát triển và mã được tạo ra (Yin et al., 2022).

Các đặc tả đầu vào-đầu ra (I/O) (ví dụ văn bản màu đỏ trong Hình 1), từ các ví dụ I/O cụ thể đến các tóm tắt NL cấp cao, là một cách tự nhiên để giảm sự mơ hồ của các ý định NL (Gulwani et al., 2015; Balog et al., 2016; Jain et al., 2022). Trước khi xuất hiện các LLM, các đặc tả đã được phục vụ như mô tả vấn đề thiết yếu trong tổng hợp chương trình (Gulwani, 2016; Devlin et al., 2017; Shi et al., 2020). Các hệ thống tổng hợp thế giới thực như FlashFill là minh chứng cho việc áp dụng và hiệu quả của các đặc tả I/O (Gulwani, 2011; Gulwani et al., 2012). Trong công trình này, chúng tôi xem xét vấn đề tạo mã dựa trên LLM trong khoa học dữ liệu khi lời nhắc được cung cấp cho LLM bao gồm cả ý định NL và đặc tả I/O bổ sung.

Thật không may, các LLM mã thường không thể tuân theo các ý định với các ràng buộc ngữ nghĩa bổ sung như đặc tả I/O ngay từ đầu, dẫn đến các giải pháp hợp lý nhưng không thể thỏa mãn các đặc tả được cung cấp (ví dụ y′, Hình 1). Điều này có thể gây ra gánh nặng không cần thiết cho các nhà phát triển, những người sau đó phải sửa mã được tạo ra (Bird et al., 2023). Sự không phù hợp như vậy giữa ý định của người dùng và dự đoán của mô hình chủ yếu xuất phát từ việc thiếu dữ liệu huấn luyện được định dạng với các đặc tả như vậy.

Tinh chỉnh hướng dẫn đã nổi lên như một chiến lược hiệu quả để giải quyết vấn đề không phù hợp (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022). Các phương pháp cổ điển cho điều chỉnh hướng dẫn thường yêu cầu một lượng lớn dữ liệu có nhãn song song của các ý định NL và phản hồi mô hình vàng. Quá trình thu thập dữ liệu như vậy là tốn nhiều lao động và thời gian. Các nghiên cứu gần đây đã gợi ý rằng việc tạo ra dữ liệu tổng hợp tuân theo hướng dẫn bằng cách sử dụng chính LLM là một phương pháp đầy hứa hẹn để cải thiện sự phù hợp, với thành công thực nghiệm trên các tác vụ tạo văn bản ngôn ngữ tự nhiên (Wang et al., 2022a; Honovich et al., 2022a; Taori et al., 2023; Peng et al., 2023, vv).

Trong bài báo này, chúng tôi xây dựng dựa trên thành công gần đây của điều chỉnh hướng dẫn sử dụng dữ liệu tổng hợp và tinh chỉnh các LLM mã để tuân theo các ý định NL với các đặc tả I/O bổ sung. Không giống như các phương pháp hiện có, nhận thức quan trọng của chúng tôi là tận dụng việc thực thi chương trình để tạo dữ liệu tổng hợp. Thứ nhất, trái ngược với các tác vụ tạo văn bản mở khác nơi việc đánh giá chất lượng của phản hồi mục tiêu là thách thức, chất lượng của dữ liệu tạo mã tổng hợp có thể được cải thiện dễ dàng bằng cách sử dụng các heuristic như khả năng thực thi mã (Yin et al., 2022). Hơn nữa, từ các trạng thái thực thi chương trình, người ta có thể suy ra các đặc tả I/O chính xác và phù hợp có thể được bao gồm trong các ý định để giám sát một mô hình tuân theo những ràng buộc ngữ nghĩa bổ sung đó (Hình 1, Phải). Nói cách khác, khi được tinh chỉnh trên dữ liệu tổng hợp như vậy, một mô hình học cách làm nền tảng cho các mô tả tác vụ NL đến các trạng thái thực thi chương trình được biểu thị như đặc tả I/O (Berant et al., 2013).

Chúng tôi áp dụng phương pháp tinh chỉnh hướng dẫn được làm nền tảng cho mã (GIFT4CODE) của chúng tôi vào hai ứng dụng tạo mã từ ngôn ngữ tự nhiên đầy thử thách: tổng hợp các chương trình pandas phức tạp trong các notebook tính toán (ARCADE, Yin et al. (2022)) và trả lời các câu hỏi khoa học dữ liệu trên Stack Overflow (DS-1000, Lai et al. (2022)). Trước tiên, chúng tôi chứng minh giá trị của việc tận dụng thông tin thực thi chương trình bằng cách cho thấy rằng các LLM mã mạnh vẫn có thể được cải thiện sau khi tinh chỉnh trên dữ liệu tổng hợp của chúng tôi mà không bao gồm bất kỳ đặc tả I/O nào. Sau đó, để tiếp tục điều chỉnh dự đoán mô hình với các loại đặc tả I/O do người dùng cung cấp khác nhau, chúng tôi suy ra những đặc tả đó ở các mức độ trừu tượng khác nhau từ kết quả thực thi mã. Điều này từ các ví dụ đầu vào/đầu ra cụ thể đến các tóm tắt NL của các biến mục tiêu (đặc tả trong Hình 1). Bằng cách tinh chỉnh trên dữ liệu song song của các ý định với đặc tả I/O và các giải pháp mã mục tiêu của chúng, mô hình tốt hơn trong việc tạo ra mã có nhiều khả năng thực thi đến kết quả mong muốn. Mặc dù các thí nghiệm của chúng tôi tập trung vào lĩnh vực khoa học dữ liệu, điều quan trọng cần lưu ý là phương pháp luận cơ bản của GIFT4CODE là tổng quát và có thể thích ứng. Nó có thể được áp dụng cho các lĩnh vực khác nhau yêu cầu đặc tả cho mô tả tác vụ chính xác.

## 2 Công thức hóa vấn đề

**Tạo mã từ ngôn ngữ tự nhiên** Tạo mã xem xét tác vụ dịch ý định ngôn ngữ tự nhiên của nhà phát triển x thành một chương trình có thể thực thi máy y (ví dụ Hình 1, Trái). Một ý định thường chứa một mô tả tác vụ ngắn gọn và có khả năng mơ hồ. Đối với các tác vụ có đầu ra phức tạp, ý định cũng có thể bao gồm các đặc tả I/O bổ sung như các làm rõ thêm. Các tác vụ tạo mã thường được ngữ cảnh hóa, có nghĩa là một ý định được liên kết với các ngữ cảnh lập trình nhất định c (Iyer et al., 2018), chẳng hạn như mã mà nhà phát triển đã viết trong IDE trước ý định (ví dụ df= pd.read_csv("flights.csv") cho x, không được hiển thị trong Hình 1). Trực quan, một mô hình cần tận dụng cả ý định và ngữ cảnh lập trình (ví dụ biến df) để tạo ra một giải pháp mã phù hợp.

**Tinh chỉnh hướng dẫn có giám sát cho các LLM mã** Tinh chỉnh hướng dẫn có giám sát nhằm cải thiện các LLM mã bằng cách tinh chỉnh chúng trên dữ liệu song song của các ý định và giải pháp mã mục tiêu. Trong bài báo này, chúng tôi xem xét việc tự động tổng hợp dữ liệu song song như vậy bằng cách nhắc các LLM sử dụng các minh chứng few-shot (các phương pháp khác được thảo luận trong §5). Cụ thể, tập dữ liệu tổng hợp bao gồm các ví dụ {⟨c,x,y⟩} của các ý định x với ngữ cảnh lập trình c và các giải pháp mã được tạo ra y của chúng. Câu hỏi then chốt sau đó trở thành cách tạo ra một {⟨c,x,y⟩} tổng hợp chất lượng cao để hỗ trợ quá trình tinh chỉnh hướng dẫn.

## 3 GIFT4CODE: Học cách tuân theo ý định với đặc tả I/O

Trong phần này, chúng tôi trình bày chi tiết về GIFT4CODE, phương pháp được đề xuất của chúng tôi để tinh chỉnh các LLM mã để tuân theo tốt hơn các ý định ngôn ngữ tự nhiên của nhà phát triển cùng với các đặc tả I/O, sử dụng dữ liệu song song tổng hợp. Hình 1 (Phải) minh họa tổng quan về GIFT4CODE. Trước tiên chúng tôi tổng hợp một bộ sưu tập các ý định với các giải pháp mã thông qua few-shot prompting (§3.1), sau đó thực thi mã được dự đoán bởi mô hình để suy ra các đặc tả I/O từ kết quả thực thi (§3.2). Cuối cùng, chúng tôi tinh chỉnh LLM mã để dự đoán các giải pháp mã được đưa ra các ý định được nội tuyến với các đặc tả I/O (§3.3).

### 3.1 Tạo ra các ý định tổng hợp và giải pháp mã

**Ngữ cảnh lập trình** Chúng tôi khởi tạo một trạng thái chương trình được đưa ra một số ngữ cảnh lập trình và tạo ra một loạt các vấn đề NL-to-code được ngữ cảnh hóa cho ngữ cảnh đó. Như một ví dụ, các vấn đề tổng hợp trong Hình 1 (Phải) có thể có mã ngữ cảnh df = pd.read_csv("world_statistics.csv"), khởi tạo biến DataFrame df, được sử dụng tiếp theo trong các ví dụ tổng hợp được tạo ra. Thực tế là các vấn đề của chúng tôi được ngữ cảnh hóa làm cho phương pháp của chúng tôi khác biệt với các phương pháp tinh chỉnh hướng dẫn hiện có cho các mô hình tạo văn bản (Wang et al., 2022a; Honovich et al., 2022a), nơi các ví dụ tổng hợp không phụ thuộc vào bất kỳ ngữ cảnh cụ thể nào. Trong trường hợp của chúng tôi, chúng tôi khai thác những ngữ cảnh lập trình đó từ các kho mã thế giới thực, chẳng hạn như các tập dữ liệu dạng bảng (ví dụ .csv) được sử dụng trong các notebook khoa học dữ liệu trên Github (§4).

**Tạo ra các ý định NL ban đầu** Được đưa ra một ngữ cảnh lập trình c, chúng tôi few-shot prompt một LLM để tạo ra một chuỗi các ý định ngôn ngữ tự nhiên {xi} (ví dụ x1,x2 trong Hình 1 (Phải)). Một vấn đề xi xuất hiện sau trong chuỗi có thể phụ thuộc vào những vấn đề trước đó {x<i} (Nijkamp et al., 2022; Yin et al., 2022). Để tạo ra các ý định NL, chúng tôi sử dụng một LLM "tổng quát" PALM2 (Google, 2023), thay vì LLM mã mà chúng tôi muốn cải thiện, vì việc dự đoán các ý định có điều kiện trên một số ngữ cảnh tương tự như các tác vụ tạo văn bản khác, có thể được xử lý tốt hơn bởi một LM được huấn luyện trên dữ liệu văn bản mục đích chung (Zelikman et al., 2022). LLM PALM2 đạt được kết quả cạnh tranh với GPT-4 trên nhiều tác vụ lý luận NL khác nhau. Về mặt thực nghiệm, chúng tôi quan sát thấy rằng các vấn đề được tạo ra bởi LLM này bao quát một loạt rộng các tác vụ liên quan đến ngữ cảnh lập trình đã cho. Độc giả có thể tham khảo Phụ lục B để xem các ví dụ. Lưu ý rằng những ý định được dự đoán bởi mô hình đó chưa đi kèm với các đặc tả I/O.

**Dự đoán các giải pháp mã** Sau khi tạo ra một ý định x, chúng tôi sau đó nhắc LLM mã để có được một giải pháp mã y cho x (ví dụ y1 trong Hình 1 (Phải)). Cụ thể, một lời nhắc cho LLM là sự nối tiếp của ngữ cảnh lập trình c và ý định x, với các minh chứng few-shot bổ sung của {⟨c′,x′,y′⟩}. Vì nhiều ý định NL có thể mơ hồ và có thể tồn tại nhiều giải pháp thay thế (ví dụ mà không có đặc tả I/O bổ sung, ý định màu xanh lá cây trong Hình 1 (Trái) có thể được trả lời bằng các bảng với bố cục khác nhau; xem thêm trong (Yin et al., 2022)), do đó chúng tôi rút ra nhiều giải pháp mã ứng viên {y} cho mỗi ý định. Trực quan, {y} có thể có nhiều giải pháp thay thế cho x, mỗi giải pháp dẫn đến các kết quả thực thi khác nhau. Điều này trang bị cho mô hình khả năng dự đoán mã cho cùng một tác vụ nhưng với các đặc tả I/O do người dùng cung cấp khác nhau.

**Cải thiện chất lượng dữ liệu tổng hợp** Chất lượng của dữ liệu tổng hợp là một vấn đề cơ bản của việc tăng cường dữ liệu trong điều chỉnh hướng dẫn (Wang et al., 2022a; Honovich et al., 2022a), và các phương pháp hiện có trong tạo văn bản thường dựa vào các heuristic đơn giản và ồn ào (ví dụ từ chối các ví dụ có đầu vào khác nhau nhưng đầu ra giống nhau). Như được động cơ trong §1, đối với tạo mã NL-to-code, chúng ta có thể cải thiện đáng tin cậy chất lượng của các giải pháp mã ứng viên bằng cách tận dụng các thuộc tính chương trình vốn có, chẳng hạn như lọc ra bất kỳ mã nào không thể thực thi được đưa ra ngữ cảnh lập trình được cung cấp.

### 3.2 Thực thi mã & suy luận các đặc tả I/O

Được đưa ra tập hợp các vấn đề tổng hợp {⟨x,{y}⟩} được tạo ra bởi few-shot prompting, chúng tôi thực thi mã cho mỗi vấn đề (bước 2, Hình 1 (Phải)) và suy ra các đặc tả I/O từ kết quả thực thi như các ràng buộc ngữ nghĩa bổ sung được bao gồm trong các ý định (bước 3, Hình 1 (Phải)).

Cụ thể, đối với mỗi giải pháp ứng viên y của một ý định, trước tiên chúng tôi thực thi ngữ cảnh lập trình gốc c của nó, tiếp theo là thực thi y. Chúng tôi theo dõi việc thực thi để thu thập tập hợp các biến đầu vào và đầu ra trong y, được ký hiệu là {v}, được sử dụng để suy ra các đặc tả I/O (chi tiết bên dưới). Thực thi mã với các ngữ cảnh lập trình tùy ý được thu thập từ thế giới thực rất phức tạp do các vấn đề như phụ thuộc thư viện. Tuy nhiên, việc sử dụng dữ liệu tổng hợp làm giảm bớt nhu cầu thiết lập môi trường phức tạp.

Được đưa ra tập hợp các biến đầu vào và đầu ra được trích xuất từ kết quả thực thi, chúng tôi công thức hóa một đặc tả I/O, được ký hiệu là z, phục vụ như thông tin bổ sung để tăng cường ý định của nhà phát triển, từ đó cung cấp mô tả vấn đề toàn diện hơn. Mức độ chi tiết và phong cách của các đặc tả I/O này có thể thay đổi dựa trên độ phức tạp của vấn đề và sở thích của nhà phát triển. Trong công trình này, chúng tôi điều tra ba loại đặc tả I/O khác biệt, mỗi loại được đặc trưng bởi phong cách ngôn ngữ và mức độ trừu tượng riêng của nó, như được minh họa trong Bảng 1.

Đầu tiên, như một baseline đơn giản, chúng tôi sử dụng loại biến (TypeDesc, Bảng 1) như đặc tả I/O. Tiếp theo, chúng tôi kết hợp các giá trị cụ thể của các biến đầu vào/đầu ra vào đặc tả, mà chúng tôi gọi là I/O Examples. Điều này gợi nhớ đến tổng hợp chương trình cổ điển sử dụng các ví dụ I/O (Gulwani et al., 2012; Alur et al., 2013; Balog et al., 2016). Tuy nhiên, trong kịch bản của chúng tôi, những ví dụ I/O này được sử dụng kết hợp với các ý định ngôn ngữ tự nhiên (NL) để định nghĩa vấn đề, phù hợp với (Jain et al., 2022). Cho rằng phần lớn các vấn đề trong tập dữ liệu tổng hợp của chúng tôi liên quan đến các đối tượng Python phức tạp như pandas DataFrames, chúng tôi đơn giản hóa đặc tả I/O để chỉ bao gồm các trạng thái biến một phần (ví dụ bằng cách loại trừ một số hàng và cột trong các DataFrames lớn). Vui lòng tham khảo Phụ lục C để biết chi tiết.

Trong nỗ lực tạo ra một loạt đặc tả I/O tự nhiên hơn gần giống với phong cách của các đặc tả trong các ý định NL của nhà phát triển, chúng tôi sử dụng một LLM để tóm tắt các giá trị của các biến đầu vào/đầu ra {v} thành một mô tả ngôn ngữ tự nhiên ngắn gọn z (I/O Summary). Trực quan, tóm tắt NL I/O bao gồm thông tin nổi bật trong các biến có thể làm rõ tốt nhất ý định gốc (ví dụ tập hợp con các cột trong một DataFrame có liên quan nhất để giải quyết một vấn đề, như trong Bảng 1, Dưới cùng).

Cụ thể, chúng tôi few-shot prompt "tổng quát" PALM2 để tạo ra z, sử dụng thông tin từ ngữ cảnh lập trình c, ý định x, giải pháp mã y, cũng như các biến I/O {v}, tức là z∼PLLM(· |c,x,y,{v}). Sau đó chúng tôi cập nhật ý định x bằng cách nối z vào nó. Các exemplars few-shot được sử dụng để prompting bao quát các ví dụ tóm tắt I/O cho các loại đối tượng Python khác nhau, chẳng hạn như các loại container lồng nhau (ví dụ các dict lồng nhau), cùng với các đối tượng phức tạp hơn như Pandas DataFrames và PyTorch hoặc Tensorflow Tensors. Xem Phụ lục C để biết thêm chi tiết.

### 3.3 Tinh chỉnh các LLM mã để tuân theo ý định với đặc tả I/O

Phương pháp của chúng tôi, GIFT4CODE, nhằm tinh chỉnh các LLM mã để tạo ra mã tuân thủ chặt chẽ với các ý định mong muốn được bổ sung bởi các đặc tả I/O. Trong dữ liệu huấn luyện tổng hợp của chúng tôi, mỗi ví dụ ⟨c,x,y⟩ bao gồm một ngữ cảnh lập trình c, một ý định x được tăng cường với các đặc tả I/O, và giải pháp mã tương ứng y. Trong quá trình tinh chỉnh, LLM mã học cách tạo ra mã không chỉ thỏa mãn các ý định được cung cấp mà còn tôn trọng các ràng buộc I/O được chỉ định, trong khi tận dụng bất kỳ thông tin liên quan nào trong các ngữ cảnh lập trình. Nói cách khác, chúng tôi tối ưu hóa PLLM(y|c,x). Đáng chú ý là LLM mã trải qua tối ưu hóa này khác với LLM "tổng quát" PALM2 được sử dụng để tạo ra các ý định NL và đặc tả I/O z.

## 4 Thí nghiệm

Câu hỏi nghiên cứu cốt lõi được khám phá trong phần này là liệu GIFT4CODE có tăng cường khả năng của LLM trong việc tuân theo các ý định NL của nhà phát triển với các đặc tả I/O phức tạp hay không. Trong khi các benchmark tạo mã phổ biến như HumanEval (Chen et al., 2021a) và MBPP (Austin et al., 2021) có các tác vụ thuật toán đơn giản (ví dụ sắp xếp) sử dụng các kiểu dữ liệu Python cơ bản (ví dụ danh sách), do đó cho phép sử dụng các ví dụ I/O cụ thể như đặc tả, chúng thiếu các đặc tả I/O đa dạng và phức tạp mà chúng tôi muốn khám phá. Đối với các tác vụ mở hơn trong lĩnh vực khoa học dữ liệu, kiểu dữ liệu đầu ra phức tạp và đa dạng hơn (ví dụ pandas DataFrames, pytorch Tensors). Do đó, chúng tôi áp dụng phương pháp của mình vào hai ứng dụng tạo mã khoa học dữ liệu khác nhau.

ARCADE (Yin et al., 2022) là một benchmark của việc tạo mã từ ngôn ngữ tự nhiên trong các notebook khoa học dữ liệu tương tác. Mỗi notebook đánh giá bao gồm một loạt các vấn đề NL-to-code liên quan đến nhau trong xử lý dữ liệu (ví dụ "Chuẩn hóa min-max các cột số") và phân tích dữ liệu khám phá (ví dụ các ý định trong Hình 1) sử dụng thư viện pandas. ARCADE có các ý định NL ngắn gọn để phản ánh phong cách của các truy vấn tạm thời từ nhà phát triển khi nhắc LLM để hoàn thành mã. Hơn 50% các vấn đề của tập dữ liệu được chỉ định dưới mức, có nghĩa là các đặc tả I/O bổ sung có thể cung cấp làm rõ thêm. Để xây dựng ngữ cảnh lập trình cho việc tạo dữ liệu huấn luyện tổng hợp, chúng tôi đã cào 7.500 tệp CSV được sử dụng trong các notebook Jupyter công khai. Mỗi ngữ cảnh chứa một câu lệnh import DataFrame, ví dụ df = pd.read_csv( ·), tiếp theo là một mô tả NL về DataFrame để giúp LLM hiểu nội dung của nó. Chúng tôi tạo ra 6 ý định cho mỗi ngữ cảnh lập trình và lấy mẫu 5 giải pháp mã ứng viên cho mỗi ý định. Khoảng 60% các mẫu mã có thể thực thi được. Sau khi lọc dựa trên khả năng thực thi và tính đa dạng API (§3.1), chúng tôi có được khoảng 20K ví dụ tổng hợp để tinh chỉnh hướng dẫn.

Dữ liệu tổng hợp của chúng tôi chỉ bao gồm các cặp câu hỏi và mẫu mã thiếu ngữ cảnh phong phú. Để tránh hồi quy trong hiểu ngữ cảnh trong quá trình tinh chỉnh hướng dẫn, chúng tôi tạo ra một tập dữ liệu hỗn hợp kết hợp dữ liệu tổng hợp và dữ liệu Python được sử dụng để tinh chỉnh LLM mã. Lưu ý rằng dữ liệu Python này không chứa bất kỳ tín hiệu dựa trên thực thi hoặc đặc tả I/O nào. Sau khoảng 1.500 bước điều chỉnh hướng dẫn với kích thước batch là 64, mô hình đạt được hiệu suất tối ưu. Quá trình này tiêu tốn khoảng 1,5 epoch của tập dữ liệu tổng hợp của chúng tôi.

DS-1000 (Lai et al., 2022) là một benchmark của các vấn đề khoa học dữ liệu có nguồn gốc từ Stack Overflow (SO). So với ARCADE, các vấn đề trong DS-1000 có nhiều loại I/O đa dạng hơn, chẳng hạn như numpy/scipy Arrays và pytorch/tensorflow Tensors, làm cho nó đặc biệt hấp dẫn để đánh giá phương pháp điều chỉnh hướng dẫn của chúng tôi nhằm tạo ra mã tuân theo các đặc tả I/O. Tuy nhiên, trái ngược với ARCADE có các ý định NL ngắn gọn, DS-1000 tuân theo phong cách điển hình của các mô tả vấn đề chi tiết được tìm thấy trong các bài đăng SO. Những mô tả phức tạp này thường bao gồm thông tin bổ sung như nền tảng tác vụ và mô tả các nỗ lực không thành công với độ dài trung bình 140 từ. Cho rằng các ý định phức tạp như vậy có thể không phản ánh phong cách của các lời nhắc của nhà phát triển cho các LLM mã, chúng tôi không tập trung vào việc tạo ra các ý định với phong cách tương tự. Thay vào đó, chúng tôi giữ lại 500 vấn đề trong DS-1000 và sử dụng các ý định được chú thích của chúng như dữ liệu huấn luyện, trong khi đánh giá trên các vấn đề còn lại.

### 4.1 Thiết lập

**LLM mã cơ sở** Chúng tôi sử dụng một mô hình ngôn ngữ mã chỉ decoder dựa trên PALM với 62B tham số (Chowdhery et al., 2022). Mô hình trước tiên được pre-trained trên một bộ sưu tập 1,3T token của tài liệu web và dữ liệu mã github, và sau đó được tinh chỉnh trên một tập hợp riêng biệt 64B token mã Python cùng với 10B token từ các notebook Python Jupyter. Quá trình tinh chỉnh, bao gồm cả quy trình GIFT4CODE, có thể được thực hiện thông qua API tinh chỉnh trên Google cloud Vertex AI. Đáng chú ý, mô hình cơ sở này có hiệu suất mạnh trong các tác vụ lập trình khoa học dữ liệu, vượt trội hơn mô hình StarCoder-15B Python trên ARCADE, như được chứng minh trong Bảng 2.

**Phương pháp học** Chúng tôi đánh giá hiệu suất của cả mô hình baseline và được điều chỉnh hướng dẫn trên một loạt các định dạng dữ liệu, như được hiển thị trong Bảng 2. Đối với mỗi loại đặc tả I/O, chúng tôi tăng cường các ý định và exemplars few-shot với các đặc tả của loại tương ứng. Tương tự, tại thời điểm kiểm tra, chúng tôi tăng cường các ý định với cùng loại đặc tả I/O. Đối với few-shot prompting, chúng tôi tạo ra thủ công các exemplars cho tất cả các loại đặc tả. Những exemplars này được đặt trước lời nhắc khi truy vấn LLM để tạo mã.

**Mô phỏng đặc tả I/O nhiễu tại thời điểm kiểm tra** Tại thời điểm kiểm tra, việc tạo ra I/O Summary đã trải qua một sửa đổi nhỏ từ quy trình được mô tả chi tiết trong §3.2. Chúng tôi loại bỏ các trạng thái biến đầu vào/đầu ra cụ thể {v} để tạo ra các tóm tắt I/O nhiễu, mô phỏng các kịch bản nơi người dùng có thể đưa ra các đặc tả I/O nhiễu (Devlin et al., 2017). Chúng tôi minh họa một ví dụ trong Phụ lục C nơi LLM tạo ra một đặc tả không hoàn hảo. Trong khi LLM "tổng quát" (PALM2) sử dụng giải pháp mã để tạo ra các tóm tắt I/O nhiễu, chúng tôi nhận xét rằng LLM mã, mà chúng tôi muốn đánh giá, không có quyền truy cập vào giải pháp ground truth. Nói cách khác, LLM "tổng quát" chỉ đơn thuần hoạt động như một "data labeler" để tạo ra các tóm tắt I/O trong lời nhắc để xây dựng tập dữ liệu đánh giá. Đây cũng là một thực hành phổ biến trong tổng hợp chương trình để suy ra các đặc tả từ các giải pháp ground truth, sau đó phục vụ như đầu vào duy nhất cho mô hình trong quá trình đánh giá (Balog et al., 2016).

**Metrics** Việc phát triển unit tests cho các tác vụ lập trình khoa học dữ liệu thường không đơn giản. Ví dụ, việc tạo ra một test cho một tác vụ xử lý dữ liệu như "Những thành phố đông dân nhất trong mỗi quốc gia là gì" (như được hiển thị trong Hình 1) là không khả thi. Trong tập dữ liệu ARCADE, mỗi câu hỏi đi kèm với một chương trình được chú thích. Sau đó việc đánh giá sử dụng một tập hợp các heuristic để ước tính liệu đầu ra thực thi của một chương trình được dự đoán và tham chiếu được chú thích có tương đương về mặt chức năng hay không, bằng cách so sánh đầu ra của chúng. Trên tập dữ liệu DS-1000, việc đánh giá dựa vào các tests được cung cấp.

Chúng tôi áp dụng các metrics pass@k như được định nghĩa trong Chen et al. (2021a); Austin et al. (2021), được tính như tỷ lệ các vấn đề có ít nhất một mẫu đúng được đưa ra k mẫu. Theo Yin et al. (2022), chúng tôi rút ra 50 mẫu để tính pass@5 và pass@20 để giảm phương sai trong ARCADE. Tương tự như Lai et al. (2022), chúng tôi rút ra 40 mẫu để tính pass@1 trên DS-1000. Phù hợp với cài đặt của các công trình gốc, nhiệt độ lấy mẫu được đặt là 0,8 cho ARCADE và 0,2 cho DS-1000 tương ứng.

### 4.2 Kết quả chính

Bảng 2 trình bày các kết quả pass@k trên ARCADE và DS-1000. Chúng tôi đánh giá cả few-shot prompting và tinh chỉnh với dữ liệu tổng hợp. Cụ thể, đối với ARCADE chúng tôi đánh giá trên hai phiên bản của tập dữ liệu. Đầu tiên, chúng tôi xem xét phiên bản gốc nơi một ý định được đặt trước bởi các ô notebook trước đó như ngữ cảnh lập trình của nó (Full Context), cũng như một ablation No Context để mô phỏng kịch bản nơi người dùng truy vấn một LLM mã sử dụng một ý định mà không có bất kỳ ngữ cảnh nào. Cài đặt không có ngữ cảnh này đầy thử thách hơn, nơi hiệu suất zero-shot của LLM mã cơ sở gần như giảm một nửa. Các sai số chuẩn trong tất cả các ô của bảng đều nhỏ hơn 0,5%, và được loại trừ cho sự rõ ràng trong trình bày.

Trong các thí nghiệm few-shot prompting của chúng tôi, chúng tôi quan sát thấy rằng pass@k thường cải thiện với các đặc tả I/O chi tiết hơn. Thú vị, trên ARCADE, các cải thiện từ prompting sử dụng đặc tả I/O so với baseline nơi không có đặc tả I/O nào được sử dụng (no spec), đáng chú ý hơn trong kịch bản không có ngữ cảnh đầy thử thách hơn (ví dụ 12.457→23.75 so với 27.757→37.11 cho +I/O Summary). Xu hướng này gợi ý rằng các đặc tả bổ sung có thể cung cấp các làm rõ có giá trị hơn khi thiếu ngữ cảnh lập trình đầy đủ.

Tiếp theo, chúng tôi tinh chỉnh LLM mã cơ sở sử dụng dữ liệu song song tổng hợp của chúng tôi với các loại đặc tả I/O khác nhau. Ngay cả khi không sử dụng bất kỳ đặc tả I/O nào trong dữ liệu tổng hợp, GIFT4CODE đã đăng ký các cải thiện đáng kể trên ARCADE (pass@20: 37.477→46.94). Các giải pháp mã được dự đoán bởi mô hình được lọc bằng các heuristic khả năng thực thi, giúp cải thiện chất lượng của dữ liệu tổng hợp, và một mô hình được tinh chỉnh trên dữ liệu như vậy có thể nói chung tốt hơn trong việc tuân theo các ý định của người dùng, ngay cả khi không có đặc tả I/O. Hơn nữa, bằng cách tinh chỉnh mô hình để tuân theo các ý định với các đặc tả I/O bổ sung, chúng tôi quan sát thấy kết quả tốt hơn đáng kể. Với tóm tắt I/O (+I/O Summary), GIFT4CODE cải thiện pass@20 từ 46.75 lên 55.47. Chúng tôi cũng nhận xét rằng tinh chỉnh hướng dẫn sử dụng tóm tắt I/O ngôn ngữ tự nhiên mang lại kết quả tốt nhất trên cả hai tập dữ liệu. Trực quan, những tóm tắt I/O đó có thể mã hóa thông tin nổi bật trong các biến đầu vào và đầu ra mục tiêu thông qua các mô tả ngôn ngữ tự nhiên, có thể làm cho việc mô hình nắm bắt các mẫu trong dữ liệu dễ dàng hơn.

### 4.3 Phân tích định tính

Để có cái nhìn sâu sắc hơn về hành vi của các mô hình khác nhau, chúng tôi trình bày một phân tích định tính về mô hình baseline, biến thể few-shot prompting của nó với đặc tả LLM (I/O Summary), và phương pháp GIFT4CODE được đề xuất của chúng tôi. Chúng tôi minh họa phân tích này bằng cách sử dụng hai ví dụ từ tập dữ liệu ARCADE, như được hiển thị trong Hình 2.

Trong ví dụ đầu tiên (Hình 2, Trái), LLM mã cơ sở (ô 4) không thể nhóm cột "League" như được yêu cầu trong ý định của người dùng. Lưu ý rằng giải pháp được cung cấp vẫn có thể thực thi được nên nó không thể được lọc bởi các heuristic khả năng thực thi. Biến thể few-shot prompting với tóm tắt I/O (ô 5) cũng thất bại ở đây. Nó gặp khó khăn trong việc sử dụng đúng các đặc tả này mặc dù đã chọn đúng các cột nổi bật, dẫn đến một đầu ra không đáp ứng yêu cầu của người dùng. Ngược lại, đầu ra từ GIFT4CODE (ô 6) thành công tạo ra một giải pháp tính tổng của hai cột nổi bật sau đó sắp xếp kết quả, hiệu quả sử dụng các đặc tả được cung cấp và tuân thủ ý định của người dùng.

Ví dụ thứ hai (Hình 2, Phải) tiếp tục nhấn mạnh những ưu điểm của GIFT4CODE. LLM mã baseline (ô 4) cố gắng gọi một cột không tồn tại (Turbo_Type), dẫn đến KeyError. Điều này đại diện cho một chế độ thất bại mà mô hình cố gắng tuân thủ ý định của người dùng nhưng tạo ra một giải pháp không thể thực thi được tham chiếu đến các biến đầu vào không chính xác do thiếu đặc tả I/O. Biến thể few-shot prompting (ô 5) trình bày một chế độ thất bại thú vị khác. Trong khi mô hình đang cố gắng tuân theo đặc tả I/O bổ sung (có lẽ do các minh chứng few-shot) bằng cách tham chiếu đến cột Engine volume trong đặc tả, nó không thể tạo ra một đoạn mã đúng về mặt cú pháp (df.Engine volume). Điều quan trọng cần lưu ý là đây là một chế độ thất bại phổ biến của mô hình few-shot prompting, như chúng tôi giải thích trong Hình 3 sau đó. Một lần nữa, GIFT4CODE vượt trội hơn các cài đặt khác, tạo ra các giải pháp trả lời câu hỏi ngôn ngữ tự nhiên trong khi tuân theo các đặc tả.

### 4.4 Tỷ lệ thực thi so với Pass@k

Chúng tôi đào sâu hơn vào các kết quả thực nghiệm để kiểm tra mối quan hệ giữa khả năng thực thi và chất lượng của mã được tạo ra. Đáng ngạc nhiên, chúng tôi quan sát thấy rằng một mô hình có tỷ lệ thực thi cao hơn không nhất thiết tạo ra mã tốt hơn. Hình 3 vẽ tần suất của các loại lỗi phổ biến cùng với tỷ lệ thực thi mã của các dự đoán của các mô hình khác nhau. LLM mã baseline, mặc dù tạo ra một lượng lớn mã có thể thực thi được (⋆ cao hơn), thường tạo ra các giải pháp không chính xác (không liên quan), dẫn đến tỷ lệ thực thi cao nhưng độ chính xác pass@k thấp (Bảng 2). Điều này gợi ý rằng khả năng của mô hình tạo ra mã có thể thực thi được không nhất thiết chỉ ra khả năng của nó trong việc tạo ra mã đúng về mặt ngữ nghĩa phù hợp với ý định của người dùng.

Cái nhìn sâu sắc này được chứng minh thêm khi chúng tôi tinh chỉnh mô hình trên dữ liệu tổng hợp mà không có đặc tả I/O, được gọi là SyntheticFT trong Hình 3. Tỷ lệ thực thi của mô hình giảm trong kịch bản này bởi vì nó cố gắng phù hợp tốt hơn với ý định của người dùng, dẫn đến tỷ lệ lỗi hiểu schema cao hơn (ví dụ tham chiếu đến các cột không tồn tại như trong ô 4, Ví dụ 2, Hình 2).

Việc kết hợp các đặc tả I/O sử dụng few-shot prompting dẫn đến một quan sát thú vị khác. Chúng tôi quan sát thấy một sự giảm trong các lỗi hiểu schema, chỉ ra rằng mô hình thực sự được hưởng lợi từ các đặc tả bổ sung. Tuy nhiên, cho rằng sự đa dạng rộng lớn của các đặc tả I/O Python, không thể bao quát tất cả các biến thể trong các exemplars few-shot. Do đó, mô hình gặp khó khăn trong việc tận dụng đáng tin cậy các đặc tả, dẫn đến một sự gia tăng trong các lỗi cú pháp khi tham chiếu đến các phần tử schema có giá trị chuỗi tùy ý (ví dụ ô 5, Ví dụ 2, Hình 2). GIFT4CODE hiệu quả giảm thiểu các lỗi cú pháp này, dẫn đến tỷ lệ thực thi cao hơn trong khi đạt được độ chính xác pass@k tốt nhất.

## 5 Công trình liên quan

**Tạo mã được hướng dẫn bởi thực thi** Một lĩnh vực nghiên cứu chủ yếu tập trung vào việc sử dụng thực thi như các ví dụ I/O, hỗ trợ tổng hợp các chương trình phù hợp với hành vi dự định. Gulwani (2016) liên quan đến việc tổng hợp các chương trình dự định trong một ngôn ngữ đặc thù miền (DSL) cơ bản từ các đặc tả dựa trên ví dụ. Phương pháp này đã được khám phá và thích ứng thêm với các ứng dụng khác nhau trong các nghiên cứu tiếp theo (Devlin et al., 2017; Chen et al., 2018; Bunel et al., 2018). Một hướng nghiên cứu khác (Chen et al., 2021b; Wang et al., 2018; Ellis et al., 2019) tận dụng các kết quả thực thi trung gian để hướng dẫn việc tìm kiếm các chương trình. Gần đây hơn, đã có những nỗ lực sử dụng kết quả thực thi chương trình để xác minh và chọn các mẫu mã được dự đoán bởi các LLM, hoặc trong quá trình giải mã tự hồi quy để cắt tỉa không gian tìm kiếm (Zhang et al., 2023), hoặc bằng few-shot prompting (Chen et al., 2023) và sắp xếp lại hậu hoc (Shi et al., 2022; Ni et al., 2023a). CodeT (Chen et al., 2022) được đề xuất để tạo ra các tests sau đó thực hiện các thỏa thuận thực thi kép để lọc các giải pháp được tạo ra trong thời gian suy luận.

**Tinh chỉnh hướng dẫn** Tinh chỉnh hướng dẫn là một phương pháp được áp dụng rộng rãi để giải quyết vấn đề không phù hợp trong nội dung được tạo ra bởi LLM. Các LLM như FLAN (Wei et al., 2021), xuất sắc trong việc hiểu và thực hiện các hướng dẫn từ lời nhắc, được huấn luyện trên dữ liệu huấn luyện có nhãn. Học tăng cường với phản hồi của con người (RLHF) nhằm giảm thiểu lượng nỗ lực gắn nhãn bằng cách sử dụng phần thưởng dựa trên mô hình (Ouyang et al., 2022). Các công trình khác cũng xác nhận hiệu quả của việc sử dụng dữ liệu hướng dẫn trong giai đoạn tinh chỉnh (Mishra et al., 2021; Sanh et al., 2021; Chung et al., 2022; Wang et al., 2022b). Để giảm chi phí gắn nhãn, một số công trình gần đây đã khám phá khả năng tạo ra hướng dẫn tự động (Ye et al., 2022; Zhou et al., 2022; Honovich et al., 2022b). STAR (Zelikman et al., 2022) bootstrap khả năng lý luận của mô hình bằng cách sử dụng các rationales tự tạo. Công trình của chúng tôi khác với hướng này bằng cách xem xét các đặc tả dựa trên thực thi. Ngoài ra, các công trình gần đây đã cố gắng chưng cất dữ liệu tuân theo hướng dẫn từ các LLM có khả năng hơn đã được điều chỉnh hướng dẫn (Honovich et al., 2022a; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023). Ngược lại, GIFT4CODE tạo ra dữ liệu tổng hợp từ các LLM vanilla chưa trải qua điều chỉnh hướng dẫn.

**Dữ liệu tổng hợp từ LLM** Bên cạnh việc tạo ra dữ liệu để tuân theo hướng dẫn, một số nghiên cứu gần đây cũng đã khai thác các LLM mục đích chung để tạo ra dữ liệu tổng hợp thực tế trong các lĩnh vực nơi dữ liệu có nhãn hạn chế, chẳng hạn như hiểu ngôn ngữ và nghiên cứu lâm sàng (Rosenbaum et al., 2022a; Tang et al., 2023; Borisov et al., 2022; Liu et al., 2022; Rosenbaum et al., 2022b; Josifoski et al., 2023). Để cải thiện chất lượng của dữ liệu tổng hợp được trích xuất từ các LLM, các phương pháp như vậy thường áp dụng một quy trình lấy mẫu từ chối và lọc dự đoán dựa trên các heuristic đặc thù miền như tính nhất quán logic (Bhagavatula et al., 2022; Yin et al., 2022). GIFT4CODE có tinh thần của hướng này ở chỗ nó tận dụng phản hồi thực thi chương trình để lọc dự đoán mã (Xu et al., 2020).

## 6 Kết luận

Chúng tôi đã trình bày GIFT4CODE, một framework cho tinh chỉnh hướng dẫn các mô hình ngôn ngữ lớn của mã trong đó việc huấn luyện được hướng dẫn bởi các đặc tả dựa trên thực thi. Về mặt thực nghiệm, chúng tôi đã chứng minh cách phương pháp của chúng tôi tăng cường chất lượng của mã được tạo ra bằng cách tuân theo các đặc tả do người dùng cung cấp, cải thiện đáng kể độ chính xác trên hai benchmark khoa học dữ liệu đầy thử thách, ARCADE và DS-1000.
