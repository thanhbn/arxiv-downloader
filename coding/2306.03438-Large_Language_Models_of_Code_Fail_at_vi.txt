# Các Mô hình Ngôn ngữ Lớn dành cho Mã nguồn Thất bại trong việc Hoàn thành Mã nguồn có Tiềm năng Lỗi

Tuan Dinh1∗†Jinman Zhao2∗Samson Tan2Renato Negrinho2
Leonard Lausen2Sheng Zha2George Karypis2
1University of Wisconsin–Madison2Amazon Web Services
tuan.dinh@wisc.edu
{jinmaz,samson,renatoni,lausen,zhasheng,gkarypis}@amazon.com

## Tóm tắt

Các mô hình ngôn ngữ lớn dành cho mã nguồn (Code-LLMs) gần đây đã mang lại những tiến bộ to lớn cho việc hoàn thành mã nguồn, một tính năng cơ bản của hỗ trợ lập trình và trí tuệ mã nguồn. Tuy nhiên, hầu hết các nghiên cứu hiện tại đều bỏ qua khả năng tồn tại lỗi trong ngữ cảnh mã nguồn để sinh ra, điều này là không thể tránh khỏi trong phát triển phần mềm. Do đó, chúng tôi giới thiệu và nghiên cứu bài toán hoàn thành mã nguồn có lỗi, được lấy cảm hứng từ tình huống thực tế của gợi ý mã nguồn thời gian thực nơi ngữ cảnh mã nguồn chứa các lỗi tiềm ẩn – các anti-pattern có thể trở thành lỗi trong chương trình đã hoàn thành. Để nghiên cứu bài toán một cách có hệ thống, chúng tôi giới thiệu hai bộ dữ liệu: một với các lỗi tổng hợp được tạo ra từ các thay đổi toán tử thay đổi ngữ nghĩa (buggy-HumanEval) và một với các lỗi thực tế được tạo ra từ các bài nộp của người dùng cho các bài toán lập trình (buggy-FixEval). Chúng tôi phát hiện rằng sự tồn tại của các lỗi tiềm ẩn làm giảm đáng kể hiệu suất sinh ra của các Code-LLMs có hiệu suất cao. Ví dụ, tỷ lệ vượt qua của CODEGEN-2B-MONO trên các test case của buggy-HumanEval giảm hơn 50% khi có một lỗi tiềm ẩn trong ngữ cảnh. Cuối cùng, chúng tôi điều tra một số phương pháp hậu xử lý để giảm thiểu tác động bất lợi của các lỗi tiềm ẩn và phát hiện rằng vẫn còn một khoảng cách đáng kể trong hiệu suất sau giảm thiểu.

## 1 Giới thiệu

Gợi ý mã nguồn cho một ngữ cảnh đã cho là một tính năng được sử dụng thường xuyên trong các môi trường phát triển tích hợp (IDE) hiện đại, mang lại lợi ích về năng suất cho quá trình viết mã. Bài toán này được nghiên cứu rộng rãi như hoàn thành mã nguồn trong tài liệu, với các kỹ thuật và mô hình từ mô hình hóa xác suất hoặc chuỗi, kết hợp cấu trúc mã nguồn như kiến thức tiên nghiệm, đến việc áp dụng mạng neural sâu và các kỹ thuật pre-training để học biểu diễn cho mã nguồn.

Gần đây, các mô hình ngôn ngữ Transformer lớn dành cho mã nguồn (Code-LLMs) đã trở thành một paradigm đầy hứa hẹn cho hoàn thành mã nguồn, đạt được hiệu suất tốt nhất (SotA) trong các tác vụ học mã nguồn khác nhau bao gồm hoàn thành và sinh mã nguồn.

Tuy nhiên, các nghiên cứu hiện tại về Code-LLMs thường giả định không có lỗi, mặc dù lỗi xảy ra thường xuyên và tốn kém trong phát triển phần mềm: trung bình, 70 lỗi được tạo ra trên 1000 dòng mã; và việc sửa lỗi tốn 50% thời gian phát triển. Xem xét một tình huống lập trình thực tế. Một nhà phát triển muốn sử dụng tính năng gợi ý mã nguồn trong IDE khi viết mã. Với xác suất cao, ngữ cảnh mã nguồn thời gian thực của họ, như đầu vào cho bộ dự đoán mã nguồn, chứa lỗi chính tả hoặc các triển khai ít được tinh chỉnh, có khả năng có lỗi. Vì tính lỗi là một thuộc tính của chương trình hoàn chỉnh, không được định nghĩa rõ ràng cách phát hiện lỗi và sửa chữa mã nguồn có lỗi trong ngữ cảnh mã nguồn ngắn và chưa hoàn chỉnh này, khiến việc áp dụng các công cụ phát hiện lỗi hoặc sửa mã nguồn hiện có trở nên không tối ưu hoặc không khả thi. Cũng đáng chú ý rằng khoảng cách giữa hiệu suất in vitro và in vivo của các mô hình hoàn thành mã nguồn vẫn còn lớn. Do đó, một câu hỏi tự nhiên nảy sinh: Liệu các Code-LLMs hiện tại có thể cung cấp gợi ý mã nguồn tốt với bản chất chưa được tinh chỉnh của mã nguồn nháp?

Để trả lời câu hỏi này, chúng tôi giới thiệu và nghiên cứu bài toán hoàn thành mã nguồn có các lỗi tiềm ẩn trong ngữ cảnh mã nguồn, được gọi là hoàn thành mã nguồn có lỗi (bCC). Trong nghiên cứu này, chúng tôi tập trung vào việc sử dụng Code-LLMs để sinh ra các triển khai chức năng từ một ngữ cảnh mã nguồn, trong đó ngữ cảnh mã nguồn bao gồm một đặc tả bài toán và một đoạn mã nguồn một phần. Một lỗi tiềm ẩn trong một đoạn mã nguồn một phần là một khoảng mã nguồn có thể trở thành một lỗi với một số hoàn thành nào đó, tức là nó làm thất bại việc hoàn thành và đồng thời có thể được thay đổi để làm cho việc hoàn thành hoạt động. Lưu ý rằng một lỗi tiềm ẩn không phải là một lỗi per se mà không có hoàn thành. Hình 1 thể hiện minh họa về tác vụ của chúng tôi với mô tả bài toán ở trên và mã nguồn một phần ở giữa. Bên trái là ngữ cảnh mã nguồn tham chiếu và hoàn thành đúng từ Code-LLM được chọn. Bên phải, lỗi tiềm ẩn được đánh dấu (-=) làm cho việc hoàn thành tham chiếu không đúng. Code-LLM phản ứng với lỗi tiềm ẩn này bằng cách sinh ra một hoàn thành khác (dưới bên phải). Tuy nhiên, mã nguồn đã hoàn thành vẫn không đúng về mặt chức năng.

Để tiến hành nghiên cứu định lượng về bCC, chúng tôi xây dựng hai bộ dữ liệu. Đầu tiên, bộ dữ liệu buggy-HumanEval chứa các bài toán lập trình kiểu phỏng vấn từ bộ dữ liệu HumanEval, với các cặp mã nguồn một phần có lỗi/tham chiếu được sinh ra bằng cách đưa ra các thay đổi toán tử thay đổi ngữ nghĩa vào các giải pháp tham chiếu. Bộ dữ liệu này cung cấp một thiết lập được kiểm soát tốt để đánh giá hành vi của mô hình khi có lỗi tiềm ẩn. Thứ hai, bộ dữ liệu buggy-FixEval, dựa trên FixEval, chứa các bài nộp của người dùng cho các bài toán thi lập trình. Các cặp có lỗi/tham chiếu được xây dựng từ các bài nộp bị từ chối và được chấp nhận bởi cùng một người dùng cho một bài toán nhất định. Bộ dữ liệu này giúp đánh giá hiệu suất của mô hình trên một phân phối thực tế của các lỗi tiềm ẩn. Các benchmark của chúng tôi được liên kết tốt với các benchmark hiện có cho Code-LLMs.

Thông qua các nghiên cứu thực nghiệm của chúng tôi, chúng tôi phát hiện rằng sự tồn tại của các lỗi tiềm ẩn làm giảm mạnh hiệu suất hoàn thành mã nguồn của các Code-LLMs có hiệu suất cao, với tỷ lệ vượt qua test case giảm xuống dưới 5% trên cả hai bộ dữ liệu cho tất cả các biến thể mô hình được kiểm tra. Ví dụ, trên buggy-HumanEval, tỷ lệ vượt qua test case của các hoàn thành CODEGEN-2B-MONO giảm từ 54.9% (mã nguồn một phần tham chiếu) xuống 3.1% (mã nguồn một phần chứa lỗi tiềm ẩn), tệ hơn điểm số khi không cung cấp mã nguồn một phần nào (9.3%). Kết quả của chúng tôi chứng minh rằng Code-LLMs rất dễ bị ảnh hưởng bởi các lỗi tiềm ẩn.

Hơn nữa, chúng tôi thử nghiệm một số phương pháp hậu xử lý để tăng cường Code-LLMs để xử lý tốt hơn các lỗi tiềm ẩn, cụ thể là loại bỏ-rồi-hoàn thành, hoàn thành-rồi-viết lại, và viết lại-rồi-hoàn thành. Hai phương pháp sau tăng cường Code-LLMs với một bộ sửa mã nguồn bên ngoài như thành phần viết lại. Đánh giá của chúng tôi cho thấy các phương pháp đã thử nghiệm cải thiện hiệu suất hoàn thành mã nguồn có lỗi của tất cả Code-LLMs được kiểm tra. Tuy nhiên, khoảng cách hiệu suất vẫn còn lớn giữa các phương pháp này và việc hoàn thành với mã nguồn một phần tham chiếu. Chúng tôi cung cấp các nghiên cứu tình huống và phân tích sâu hơn, ví dụ, tác động của vị trí lỗi tiềm ẩn hoặc các trường hợp thành công của hoàn thành ngây thơ để hiểu rõ hơn về hành vi của các mô hình được kiểm tra trong hoàn thành mã nguồn có lỗi.

**Phạm vi nghiên cứu và đóng góp.** Công trình này nhằm khám phá và hiểu hành vi của Code-LLMs trong thiết lập hoàn thành mã nguồn có lỗi. Chúng tôi (i) định nghĩa tác vụ hoàn thành mã nguồn có lỗi mới, (ii) giới thiệu hai bộ dữ liệu benchmark đại diện, (iii) chứng minh sự bất lực của Code-LLMs trong việc xử lý các lỗi tiềm ẩn, và (iv) đánh giá một số phương pháp baseline để cải thiện Code-LLMs trong hoàn thành mã nguồn có lỗi.

## 2 Hoàn thành Mã nguồn có Lỗi

Trong công trình này, chúng tôi xem xét thiết lập hoàn thành mã nguồn với đầu vào bao gồm (1) một đặc tả h chỉ định chức năng mong muốn của chương trình và (2) một ngữ cảnh mã nguồn s, là một số mã nguồn chưa hoàn thành cần được hoàn thành. Ở đây, h được đưa ra dưới dạng docstring hoặc một câu phát biểu bài toán bằng tiếng Anh, và s được đưa ra dưới dạng một vài dòng mã nguồn là phần đầu của một chương trình, mà chúng tôi gọi là mã nguồn một phần hoặc prefix (mã nguồn). Trong thiết lập thông thường của hoàn thành mã nguồn, mục tiêu là gợi ý một hoàn thành c sao cho t := s::c là một chương trình thỏa mãn h, trong đó "::" biểu thị nối mã nguồn. Thiết lập hoàn thành mã nguồn có lỗi của chúng tôi mở rộng thiết lập thông thường với một xem xét thách thức và thực tế: s có thể chứa các lỗi tiềm ẩn.

Xem xét một tình huống thực tế nơi một lập trình viên làm việc trên mã nguồn chưa hoàn thành và sử dụng tính năng tự động hoàn thành mã nguồn để có gợi ý. Lưu ý rằng quá trình lập trình thường gặp các sai lầm lập trình hoặc đưa ra các cách sử dụng mã nguồn không nhất quán, không nhất thiết là "không đúng" per se. Tuy nhiên, có một khả năng hợp lý rằng những sai lầm hoặc sự không nhất quán như vậy gây ra hành vi không mong muốn và không được mong đợi trong chương trình cuối cùng. Nói cách khác, chúng trở thành lỗi. Dựa trên trực giác này, chúng tôi định nghĩa các lỗi tiềm ẩn như sau:

**Định nghĩa 2.1 (lỗi tiềm ẩn).** Xem xét một đặc tả h và một prefix mã nguồn tham chiếu s mà cho đó tồn tại một hoàn thành c sao cho t := s::c thỏa mãn h. Một lỗi tiềm ẩn được thể hiện như một chỉnh sửa nhỏ e trên s sao cho t' := s'::c không thỏa mãn h, trong đó s' là kết quả của việc áp dụng e lên s.

Chúng tôi lưu ý rằng các lỗi tiềm ẩn không phải là lỗi per se và chỉ được định nghĩa liên quan đến một số prefix mã nguồn tham chiếu s và hoàn thành c. Prefix mã nguồn s' chứa các lỗi tiềm ẩn được gọi là prefix "có khả năng có lỗi" hoặc đơn giản là prefix "có lỗi" (liên quan đến s) trong toàn bộ bài báo. Trong hầu hết các trường hợp, chúng tôi bỏ qua phần "liên quan đến" để ngắn gọn. Thuật ngữ kết quả "prefix có lỗi" đề cập đến định nghĩa ở đây và không có nghĩa là giả định rằng bản thân prefix có lỗi. Nghiên cứu của chúng tôi tập trung vào các lỗi tiềm ẩn liên quan đến lỗi ngữ nghĩa, tức là các chỉnh sửa không đưa ra lỗi cú pháp, vì lỗi ngữ nghĩa thường thách thức và thú vị hơn lỗi cú pháp.

**Nhận xét 2.2.** Trực giác, chúng tôi giả định rằng prefix tham chiếu s có tương quan cao với các triển khai được ưa thích (thực tế) của tác vụ và rằng bất kỳ sự sai lệch nào từ s đều có khả năng ít được ưa thích hơn. Chúng tôi sử dụng định nghĩa hoạt động của lỗi tiềm ẩn trong 2.1 vì tính đơn giản và khả năng xác minh của nó, cũng như vì nó cho phép khám phá ban đầu về tình huống bCC chưa được điều tra trước đây. Chúng tôi lưu ý rằng theo định nghĩa này, một prefix tham chiếu s bản thân nó có thể, trong một số trường hợp, mặc dù ít có khả năng hơn, là "có lỗi" liên quan đến một số tham chiếu khác. Tuy nhiên, trường hợp như vậy ít được quan tâm trong nghiên cứu này, vì chúng tôi chủ yếu quan tâm đến việc tìm một chương trình chức năng hoàn chỉnh từ s hoặc s'.

**Định nghĩa 2.3 (hoàn thành mã nguồn có lỗi, bCC).** Cho một đặc tả h và một prefix mã nguồn s' chứa các lỗi tiềm ẩn, hoàn thành mã nguồn có lỗi là tác vụ sinh ra một chương trình hoàn chỉnh t thỏa mãn h.

Chúng tôi cố ý nới lỏng ràng buộc rằng t nên chứa s' như một prefix, vì theo định nghĩa, và cũng như người ta có thể thấy từ ví dụ trong Hình 1, việc cố gắng vào prefix có lỗi có thể làm cho việc gợi ý một giải pháp thỏa mãn trở nên khó khăn, nếu không muốn nói là không thể. Như chúng tôi khám phá trong Phần 4.3, việc cho phép các mô hình gợi ý sửa chữa cho prefix mã nguồn có lỗi tăng đáng kể khả năng một chương trình được sinh ra vượt qua các test. Tuy nhiên, vẫn thường có thể tiếp tục một prefix có lỗi thành một chương trình giải pháp hợp lệ (xem ví dụ trong Hình 12).

Một cách nhìn khác về thiết lập này là prefix mã nguồn có lỗi cung cấp một điều kiện tiên quyết nhiễu và có thể có sai sót để sinh ra một giải pháp cho bài toán lập trình. Nó thể hiện một nỗ lực thiện chí từ người dùng để hướng dẫn công cụ về các giải pháp mã nguồn dự định của họ. Một mô hình hoạt động tốt nên coi điều này như một gợi ý và, trong trường hợp tệ nhất, loại bỏ nó để hiệu suất sinh ra không tệ hơn khi không có prefix mã nguồn nào được đưa ra. Tuy nhiên, điều này không đúng với các mô hình được đánh giá của chúng tôi (Phần 4.2).

**Nhận xét 2.4.** Công thức bCC của chúng tôi không phải là một sự kết hợp đơn giản của sửa chữa mã nguồn và hoàn thành mã nguồn. Vì tính lỗi là một thuộc tính của các chương trình hoàn chỉnh, việc sửa chữa mã nguồn một phần là một bài toán không được định nghĩa rõ ràng, do đó làm cho sửa chữa-rồi-hoàn thành cũng không được định nghĩa rõ ràng. Do đó, tác vụ bCC của chúng tôi được xem tốt hơn như một mở rộng của hoàn thành mã nguồn với thách thức bổ sung rằng việc sinh ra các tiếp tục đúng ngữ nghĩa từ mã nguồn một phần đã cho mà không có sự sai lệch có thể khó khăn hoặc thậm chí không khả thi. Thách thức này đòi hỏi các mô hình phải nhận thức được sự tồn tại của các lỗi tiềm ẩn để có gợi ý tốt hơn.

## 3 Benchmarks

Phần này giới thiệu các bộ dữ liệu mới của chúng tôi với các phương pháp baseline được đề xuất và metrics đánh giá.

### 3.1 Bộ dữ liệu cho bCC

Dựa trên định nghĩa tác vụ của chúng tôi, mỗi instance trong một benchmark bCC nên chứa một mô tả bài toán chỉ định yêu cầu, một đoạn prefix mã nguồn có lỗi cần được hoàn thành, và một tập các test case để đánh giá tính đúng đắn của mã nguồn hoàn thành. Tùy chọn, một instance cũng có một prefix mã nguồn tham chiếu tương ứng và một giải pháp hợp lệ được hoàn thành từ mã nguồn tham chiếu. Theo hiểu biết của chúng tôi, không có bộ dữ liệu hiện tại nào đáp ứng tất cả các desiderata. Các bộ dữ liệu quy mô lớn được sử dụng thường xuyên cho hoàn thành mã nguồn, ví dụ Py150 và Java Corpus không đi kèm với test case. Các bộ dữ liệu quy mô nhỏ được tuyển chọn thủ công gần đây cho sinh mã nguồn, ví dụ HumanEval, MBPP, APPs, đi kèm với test case và giải pháp tham chiếu nhưng không có giải pháp có lỗi hoặc thất bại. Cũng không có cách được định nghĩa trước để trích xuất mã nguồn một phần từ các giải pháp tham chiếu. Điều này cũng đúng với các bộ dữ liệu sửa chữa chương trình: các bộ dữ liệu phổ biến hoặc thiếu test case, ví dụ [23,24,25,26] và/hoặc có kích thước nhỏ, ví dụ [27,28,29].

Chúng tôi giới thiệu hai bộ dữ liệu để đánh giá bCC trong Python, cả hai đều có tất cả các thành phần mong muốn đã đề cập. Chúng tôi đảm bảo rằng 1) tất cả các đoạn mã nguồn một phần chứa các lỗi tiềm ẩn thỏa mãn định nghĩa của chúng tôi, tức là mã nguồn hoàn thành tương ứng của chúng không đúng, được chứng nhận bằng các test case thất bại; và 2) tất cả các lỗi tiềm ẩn đều có bản chất ngữ nghĩa, tức là chúng không gây ra lỗi cú pháp trong mã nguồn hoàn thành tương ứng của chúng.

#### 3.1.1 Buggy-HumanEval

Chúng tôi đầu tiên giới thiệu buggy-HumanEval để cung cấp một thiết lập được kiểm soát để đánh giá bCC khi có một lỗi ngữ nghĩa duy nhất. Buggy-HumanEval chứa 1896 instance bCC được xây dựng từ một tập con của các bài toán HumanEval. Bộ dữ liệu HumanEval là một bộ dữ liệu phổ biến của các bài toán lập trình nhập môn được viết thủ công được thiết kế để đánh giá khả năng sinh mã nguồn của Code-LLMs.

Các lỗi tiềm ẩn được đưa ra như các thay đổi toán tử thay đổi ngữ nghĩa đối với các giải pháp tham chiếu. Chúng tôi tìm kiếm các toán tử nhị phân có thể áp dụng trong các giải pháp tham chiếu và thay đổi chúng thành các đối nghịch ngữ nghĩa của chúng, ví dụ + thành -. Để đảm bảo rằng việc chỉnh sửa đưa ra một lỗi, chúng tôi thực thi chương trình đã thay đổi và chỉ giữ những cái thất bại một số test. Sau đó chúng tôi chỉ định một dòng sau toán tử đã thay đổi để chia giải pháp thành một prefix mã nguồn và suffix. Chúng tôi giữ prefix có lỗi như một phần của đầu vào cho bCC. Chúng tôi chia giải pháp chưa thay đổi tại cùng dòng để có một prefix tham chiếu tương ứng. Trung bình, các bài toán được chọn cho buggy-HumanEval có các giải pháp dài hơn những cái không được chọn trong HumanEval, với 8.2 dòng so với 2.9 dòng mã nguồn, tương ứng. Phụ lục A.1 cung cấp chi tiết về bộ dữ liệu của chúng tôi.

#### 3.1.2 Buggy-FixEval

Để đánh giá bCC với các lỗi thực tế hơn, chúng tôi giới thiệu buggy-FixEval với các instance bCC được xây dựng từ CodeNet và FixEval. FixEval là một benchmark sửa chữa chương trình dựa trên các bài nộp của người dùng cho các trang web lập trình thi đấu. Mỗi mẫu dữ liệu trong FixEval có thể được xem như một cặp chương trình được nộp từ cùng một người dùng, với bài nộp được chấp nhận được coi là tham chiếu hoặc đã sửa và bài nộp bị từ chối trước đó được coi là có lỗi.

Để tạo buggy-FixEval, chúng tôi khớp và ghép đôi mỗi bài toán với câu phát biểu bài toán của nó được tìm thấy trong CodeNet và bỏ qua các bài toán không có khớp. Với mỗi cặp bài nộp, chúng tôi xác định các lỗi tiềm ẩn như sự khác biệt giữa các bài nộp bị từ chối và được chấp nhận. Chúng tôi đảm bảo rằng bài nộp bị từ chối không chứa lỗi cú pháp và thất bại ít nhất một test case. Chúng tôi chia các giải pháp thành hai nửa và coi prefix từ giải pháp bị từ chối như chứa các lỗi tiềm ẩn và prefix từ giải pháp được chấp nhận như một tham chiếu. Để đảm bảo sự khác biệt giữa prefix có lỗi và prefix tham chiếu liên quan đến các lỗi tiềm ẩn, chúng tôi áp đặt một giới hạn về khoảng cách chỉnh sửa cấp ký tự giữa chúng, bỏ qua comment và khoảng trắng. Giới hạn dưới đảm bảo rằng sự khác biệt không phải là comment hoặc khoảng trắng. Giới hạn trên (20, được chọn bằng kiểm tra thủ công) giảm khả năng sự khác biệt liên quan đến việc định dạng lại hoặc triển khai lại. Sau đó chúng tôi kiểm tra thủ công tất cả các cặp còn lại và loại trừ các trường hợp không mong muốn như prefix tham chiếu bản thân nó đã là một giải pháp đúng hoặc hai prefix tương đương ngữ nghĩa. Cuối cùng, chúng tôi thực thi sự nối của prefix có lỗi và hoàn thành tham chiếu và đảm bảo rằng nó thất bại ít nhất một test case. Thêm chi tiết về việc tạo buggy-FixEval có thể được tìm thấy trong Phụ lục A.2.

### 3.2 Phương pháp Baseline cho bCC

Ngoài việc sinh ra ngây thơ từ ngữ cảnh mã nguồn, chúng tôi nghiên cứu một số phương pháp để tăng cường Code-LLMs để sử dụng mã nguồn một phần tốt hơn trong khi giảm thiểu tác động của các lỗi tiềm ẩn. Giả định một thành phần phát hiện lỗi được trang bị, chúng tôi tập trung vào các cách tiếp cận thiết kế đơn giản và mô-đun không đòi hỏi huấn luyện bổ sung và linh hoạt để tiếp tục kết hợp các Code-LLMs mới được phát triển.

**Loại bỏ mã nguồn một phần, rồi hoàn thành (removal-then-completion).** Chúng tôi bỏ qua tác động tiêu cực của mã nguồn một phần có lỗi bằng cách loại bỏ toàn bộ fragment mã nguồn nội dung từ đầu vào mô hình. Cụ thể, cho buggy-HumanEval, đầu vào cho Code-LLMs sau khi loại bỏ bao gồm câu phát biểu bài toán và header hàm (và ví dụ). Cho buggy-FixEval, chúng tôi chỉ giữ câu phát biểu và fragment mã nguồn được sử dụng để đọc dữ liệu đầu vào. Trực giác, vì removal-then-completion đảm bảo rằng đầu vào cho Code-LLMs không chứa lỗi nào, chúng tôi mong đợi phương pháp này hoạt động tốt hơn hoàn thành ngây thơ. Tuy nhiên, nhược điểm của removal-then-completion là việc hy sinh tất cả thông tin có thể hữu ích mang lại bởi mã nguồn một phần.

**Hoàn thành trước, rồi viết lại chương trình (completion-then-rewriting).** Cách tiếp cận này cố gắng sửa mã nguồn có lỗi bằng cách sử dụng các mô hình sửa chữa mã nguồn đã được huấn luyện trước, ví dụ dịch neural một chương trình có lỗi thành một chương trình hợp lệ. Vì các mô hình sửa chữa mã nguồn, hoặc các bộ sửa mã nguồn, thường được huấn luyện để sửa các chương trình hoàn chỉnh, chúng tôi đầu tiên hoàn thành mã nguồn bằng hoàn thành ngây thơ, sau đó áp dụng các bộ sửa mã nguồn trên các chương trình có khả năng có lỗi. Chúng tôi sử dụng RealiT, một mô hình sửa chữa chương trình SotA như bộ sửa mã nguồn. RealiT được thiết kế cho các biến bị sử dụng sai, literal sai, toán tử nhị phân và unary sai, đặc biệt phù hợp với các lỗi được đưa ra trong buggy-HumanEval.

**Viết lại mã nguồn một phần, rồi hoàn thành (rewriting-then-completion).** Cách tiếp cận này cố gắng giải quyết các lỗi tiềm ẩn trong mã nguồn một phần trước khi hoàn thành. Để làm như vậy, chúng tôi đầu tiên định vị các dòng mã nguồn chứa lỗi tiềm ẩn, sau đó viết lại các dòng này. Để phát hiện các lỗi tiềm ẩn, chúng tôi đề xuất một biện pháp dựa trên likelihood để xác định dòng có khả năng chứa lỗi tiềm ẩn nhất. Cụ thể, chúng tôi tính điểm cho mỗi dòng mã nguồn với quy trình sau. Đầu tiên, cho mỗi token, chúng tôi định nghĩa điểm lỗi của nó là sự khác biệt trong likelihood giữa token với likelihood cao nhất (tức là token argmax) và token quan sát được. Điểm lỗi cho mỗi dòng sau đó được tính bằng cách lấy maximum hoặc average của các điểm lỗi khác không của các token của nó. Dòng với điểm cao nhất có khả năng chứa các lỗi tiềm ẩn nhất. Chúng tôi sử dụng INCODER-6B như mô hình ngôn ngữ infilling cho việc viết lại mã nguồn. Chúng tôi cung cấp giải thích chi tiết và minh họa ví dụ về các biện pháp dựa trên likelihood trong Phụ lục B để hiểu rõ hơn phương pháp rewriting-then-completion của chúng tôi.

### 3.3 Metrics Đánh giá

Chúng tôi đo chức năng của một chương trình hoàn thành bằng cách thực thi nó đối với các test case được cung cấp. Theo các công trình sinh mã nguồn gần đây [ví dụ 10,34,12,33], chúng tôi đo pass@k (↑) cho mỗi instance bCC như pass@k := 1 - (n-c choose k)/(n choose k) trong đó n hoàn thành được lấy mẫu từ một mô hình, và c trong số chúng vượt qua tất cả test. Chúng tôi chọn n = 100 và k = 1,10,100. Metric này ước tính xác suất rằng bất kỳ k mẫu nào từ mô hình vượt qua các test. Vì nhiều instance bCC có thể được tạo ra từ cùng một bài toán lập trình, chúng tôi đầu tiên tính trung bình pass@k trong mỗi bài toán, sau đó tính trung bình trên tất cả các bài toán (macro-average) để tránh sự thống trị của một số bài toán.

Mặc dù việc vượt qua tất cả test case không đảm bảo 100% tính đúng đắn của một chương trình, metric này cung cấp một proxy thực tế và hiệu quả cho tính đúng đắn chức năng. Lưu ý rằng chúng tôi không sử dụng các metric dựa trên khớp, ví dụ exact match hoặc CodeBLEU vì chúng không phản ánh đúng chức năng của mã nguồn được sinh ra [22, 10, 18], và không có hoàn thành tham chiếu nào có sẵn cho các prefix có lỗi.

## 4 Thí nghiệm

Chúng tôi thiết kế hai tập thí nghiệm để điều tra (1) các Code-LLMs hiện tại thích ứng với bCC như thế nào (Sec. 4.2) và (2) liệu chúng ta có thể có một sửa chữa đơn giản với Code-LLMs cho bCC (Sec. 4.3). Chúng tôi cung cấp các nghiên cứu ablation về các lỗi tiềm ẩn (Sec. 4.5) và về việc kết hợp hoàn thành dựa trên lỗi với prefix tham chiếu (Sec.C.4). Sec. 4.6 trình bày các nghiên cứu tình huống về hành vi thú vị của Code-LLMs dưới bCC, với kết quả bổ sung trong Phụ lục C.

### 4.1 Thiết lập Thí nghiệm

**Code-LLMs.** Chúng tôi đánh giá hai Code-LLMs phổ biến và mã nguồn mở cho hoàn thành mã nguồn. CODEGEN là một họ LLMs được huấn luyện trên cả corpus ngôn ngữ tự nhiên và lập trình với hiệu suất cao trong sinh mã nguồn trên HumanEval. Chúng tôi sử dụng các checkpoint mô hình được phát hành: CODEGEN-350M-MONO, CODEGEN-2B-MONO, và CODEGEN-16B-MONO. Các mô hình INCODER được huấn luyện với một mục tiêu causal masking, cho phép chúng điền các khối mã nguồn được điều kiện hóa trên các ngữ cảnh trái và phải tùy ý. Chúng tôi sử dụng các checkpoint mô hình được phát hành INCODER-1B và INCODER-6B, mỗi cái với 1B và 6B tham số. Chúng tôi chọn CODEGEN và INCODER vì chúng có sẵn công khai với hiệu suất cao của sinh mã nguồn.

**Sinh hoàn thành.** *Định dạng đầu vào.* Cho buggy-HumanEval, một mô hình được mong đợi hoàn thành một hàm chưa hoàn thành. Theo benchmark HumanEval, chúng tôi đặt đầu vào của các mô hình như mã nguồn một phần dẫn đến vị trí hoàn thành, với mô tả bài toán được nhúng như docstring của hàm chưa hoàn thành. Cho buggy-FixEval, tác vụ hoàn thành là hoàn thành một chương trình chưa hoàn thành với đầu vào là mô tả bài toán như một docstring cấp file (được trích dẫn trong dấu ngoặc ba), theo sau bởi mã nguồn một phần dẫn đến vị trí hoàn thành. *Chi tiết sinh và lấy mẫu.* Theo các thiết lập hoạt động tốt nhất được báo cáo trong các công trình tương ứng, chúng tôi sử dụng temperature sampling với temperature = 0.6 cho CODEGEN và top-p sampling với p = 0.95 và temperature = 0.2 cho INCODER. Dựa trên các giải pháp tham chiếu và hiệu quả tính toán, chúng tôi đặt giới hạn độ dài tối đa cho đầu ra từ 200 đến 600 token, thay đổi với các tập bài toán. Chúng tôi quan sát các xu hướng hiệu suất tương tự giữa các mô hình được kiểm tra qua các thiết lập độ dài khác nhau. Chúng tôi hậu xử lý chuỗi đầu ra theo cùng quy trình được sử dụng trong các bản phát hành mã nguồn của chúng.

**Mô hình sửa chữa mã nguồn và viết lại mã nguồn.** Cho removal-then-completion, chúng tôi sử dụng mô hình mới nhất của RealiT như bộ sửa mã nguồn, được huấn luyện và kiểm tra trên các lỗi nhân tạo và thực tế. Cho mô hình viết lại mã nguồn, chúng tôi sử dụng mô hình INCODER-6B do khả năng infilling mã nguồn của nó và áp dụng các thiết lập tương tự được sử dụng cho infilling được báo cáo trong bài báo của mô hình.

### 4.2 Các Code-LLMs Hiện tại Hoạt động như thế nào trên Ngữ cảnh Mã nguồn có Lỗi?

Chúng tôi đánh giá các Code-LLMs mới nhất cho hoàn thành mã nguồn có lỗi trên buggy-HumanEval và buggy-FixEval, được hiển thị trong Hình 2. Cụ thể, hiệu suất được đo bằng pass@1, và chúng tôi sử dụng bốn mô hình: CODEGEN-350M-MONO, CODEGEN-2B-MONO, INCODER-1B, và INCODER-6B. Đầu tiên, so sánh điểm số giữa mã nguồn một phần tham chiếu và có lỗi cho mỗi mô hình, chúng ta thấy rằng sự tồn tại của các lỗi tiềm ẩn có hại cho các mô hình hoàn thành, với pass@1 giảm từ 41.1–54.9% xuống 0.5–3.1% trên buggy-HumanEval, và từ 24.1–37.8% xuống 1.2–4.3% trên buggy-FixEval. Hơn nữa, pass@1 trên mã nguồn một phần có lỗi được thống trị phổ biến bởi những cái không có mã nguồn một phần: 3.9–9.3% trên buggy-HumanEval, 2.9–8.6% trên buggy-FixEval. Bảng 7 (Phụ lục C) cho thấy các phát hiện tương tự với một mô hình rất lớn (CODEGEN-16B-MONO). Những kết quả này chỉ ra rằng (i) các Code-LLMs được kiểm tra thất bại một cách mạnh mẽ tại bCC được thể hiện bởi các bộ dữ liệu của chúng tôi, và (ii) sự tồn tại của các lỗi tiềm ẩn phá hủy lợi ích mang lại bởi mã nguồn một phần.

**Tại sao Code-LLMs thất bại tại bCC?** Chúng tôi kiểm tra thủ công các mẫu từ mô hình CODEGEN-2B-MONO hoạt động tốt nhất và tìm thấy hai chế độ thất bại phổ biến nhất trong các mẫu thất bại. Đầu tiên, mô hình thất bại trong việc phản ứng với các lỗi tiềm ẩn (tức là các hoàn thành thường xuyên vẫn giống nhau), như được hiển thị trong Hình 8. Chế độ này xảy ra trong 90% instance và 93% bài toán với ít nhất một instance thất bại. Chúng tôi đoán rằng mô hình không nhạy cảm với và do đó bỏ qua các thay đổi mã nguồn nhỏ và/hoặc chọn mặc định về các pattern phổ biến trong dữ liệu huấn luyện. Thứ hai, mô hình thất bại trong việc bỏ qua các lỗi tiềm ẩn, có thể vì các pattern như vậy hiếm trong mã nguồn chất lượng cao. Nói cách khác, mô hình có thể đã nhận ra các lỗi tiềm ẩn và thay đổi đáng kể phân phối đầu ra nhưng vẫn thất bại. Hình 7 minh họa một ví dụ về trường hợp này. Chúng tôi cung cấp thêm chi tiết trong Phụ lục D.2.

### 4.3 Các Phương pháp Hoàn thành Baseline Hiệu quả như thế nào Chống lại Lỗi Tiềm ẩn?

Chúng tôi đánh giá các phương pháp hoàn thành được giới thiệu trong Phần 3.2 sử dụng cùng thiết lập bCC, được hiển thị trong Bảng 1. Chúng tôi bao gồm kết quả hoàn thành mã nguồn một phần tham chiếu để xem các lỗi tiềm ẩn ảnh hưởng đến Code-LLMs như thế nào. Hình 6 trong Phụ lục C cung cấp kết quả đầy đủ và giải thích cho k = 1,10,100.

**Tác động của các phương pháp hoàn thành được đề xuất.** Cả ba phương pháp được đề xuất đều vượt trội hơn baseline hoàn thành mã nguồn có lỗi ngây thơ. Trên buggy-HumanEval, chúng tôi quan sát xu hướng chung của completion-then-rewriting vượt trội hơn rewriting-then-completion, vượt trội hơn removal-then-completion. Lưu ý rằng những điểm số này của removal-then-completion thường thấp hơn điểm số được báo cáo của phương pháp tương tự có thể vì buggy-HumanEval được tạo ra từ một tập con tương đối thách thức hơn của HumanEval (xem Phần 3.1.1). Trên buggy-FixEval, chúng tôi quan sát removal-then-completion vượt trội hơn rewriting-then-completion, vượt trội hơn completion-then-rewriting. Khoảng cách hiệu suất tăng khi kích thước của mô hình hoàn thành tăng. Xu hướng so sánh tương tự được quan sát cho pass@k với k = 10,100. Tuy nhiên, khoảng cách hiệu suất vẫn đáng kể giữa phương pháp tốt nhất và hoàn thành từ mã nguồn tham chiếu cho tất cả thiết lập.

**Tác động của năng lực Code-LLMs.** Với mỗi loại mô hình hoàn thành mã nguồn, phiên bản lớn hơn hoạt động tốt hơn phiên bản nhỏ hơn sử dụng cùng phương pháp. Ví dụ, CODEGEN-2B-MONO vượt trội hơn CODEGEN-350M-MONO cho tất cả thiết lập trong hai bộ dữ liệu. So sánh với CODEGEN, các mô hình INCODER, nói chung, đạt pass@1 tốt hơn nhưng pass@10 và pass@100 tệ hơn. Điều này gợi ý rằng các mô hình CODEGEN sinh ra các hoàn thành đa dạng hơn, trong khi các mô hình INCODER sinh ra các hoàn thành chính xác hơn. Hơn nữa, INCODER nhạy cảm hơn với mã nguồn một phần có lỗi hơn các mô hình CODEGEN, được chứng minh bởi điểm số thấp hơn từ bCC ngây thơ.

**Lỗi tổng hợp so với lỗi thực.** Trong hai bộ dữ liệu bCC, chúng tôi quan sát rằng hiệu suất tổng thể của các phương pháp giảm thiểu tốt hơn trên buggy-HumanEval hơn buggy-FixEval. Điều này chỉ ra độ khó của các lỗi tiềm ẩn thực tế trong buggy-FixEval: Có thể có nhiều lỗi; lỗi có thể được trộn với các thay đổi không sửa lỗi; và lỗi tinh tế hơn các thay đổi toán tử đơn. Hơn nữa, trong khi đạt hiệu suất tốt nhất trong hầu hết trường hợp, completion-then-rewriting chỉ cho thấy sự khác biệt cận biên từ các phương pháp khác khi sử dụng các mô hình lớn hơn trên buggy-FixEval.

**Kết luận:** Các phương pháp baseline của chúng tôi cải thiện việc hoàn thành cho tất cả Code-LLMs được đánh giá. Tuy nhiên, khoảng cách hiệu suất còn lại đối với việc hoàn thành với mã nguồn một phần tham chiếu vẫn lớn.

### 4.4 Điều gì xảy ra nếu Mã nguồn Một phần Không có Lỗi Tiềm ẩn?

Như được hiển thị trong Bảng 2, các phương pháp giảm thiểu cho bCC có thể làm hại việc hoàn thành từ ngữ cảnh mã nguồn tham chiếu (removal-then-completion giống nhau cho cả hai thiết lập, do đó không được liệt kê.) Điều này gợi ý rằng một hoàn thành mã nguồn chung nên xem xét cả hai trường hợp khi các lỗi tiềm ẩn có thể và có thể không tồn tại.

Với các baseline của chúng tôi, chúng ta có thể sử dụng cách tiếp cận thresholding để phát hiện các lỗi tiềm ẩn. Ví dụ, trong rewriting-then-completion, một token chỉ được coi là một lỗi tiềm ẩn nếu khoảng cách likelihood của nó đến token argmax vượt quá một threshold (giữa 0 và 1). Bảng 3 so sánh điểm số pass@1 của rewriting-then-completion thay đổi threshold trên buggy-FixEval. Chúng ta có thể thấy rằng threshold 0.9 có thể giúp đạt được một sự cân bằng tương đối tốt cho hai trường hợp. Một cách tiếp cận tương tự có thể được áp dụng cho completion-then-rewriting, vì RealiT cung cấp xác suất của một token là một lỗi.

**Nhận xét 4.1 (Mất cân bằng trong phân phối thực).** Chúng tôi lưu ý rằng phân phối của lỗi mã nguồn tự nhiên thường mất cân bằng vì lỗi thực tế xảy ra không thường xuyên. Tuy nhiên, các lỗi tiềm ẩn có thể xảy ra thường xuyên hơn vì thiết lập mục tiêu của chúng tôi là một tình huống mã nguồn đang-trong-quá-trình thay vì mã nguồn chất lượng cao của các dự án mã nguồn mở phổ biến trong các nghiên cứu trước. Tuy nhiên, để so sánh các phương pháp, chúng tôi đánh giá dữ liệu cân bằng hơn để tránh tác động thống trị của hiệu suất trên mã nguồn tham chiếu.

### 4.5 Phân tích Tác động từ Vị trí Lỗi và Chia tách

Để hiểu vị trí lỗi và kích thước ngữ cảnh ảnh hưởng đến việc hoàn thành như thế nào, chúng tôi tổng hợp kết quả theo vị trí của các lỗi tiềm ẩn và vị trí của các chia tách mã nguồn một phần trong buggy-HumanEval. Các vị trí được chuẩn hóa như (số dòng lỗi tiềm ẩn)/(số dòng) và (số dòng chia tách)/(số dòng), trong đó số dòng lỗi tiềm ẩn, số dòng chia tách, và số dòng là số dòng bắt đầu từ header hàm đến dòng chứa lỗi tiềm ẩn, đến cuối mã nguồn một phần, và đến cuối giải pháp canonical.

Hình 3 trình bày heatmap của điểm số pass@1 (được tính trung bình trên các instance bCC rơi vào mỗi ô) được đánh giá trên CODEGEN-2B-MONO với hoàn thành ngây thơ trên HumanEval tham chiếu, và hoàn thành ngây thơ, completion-then-rewriting, và rewriting-then-completion (max) trên buggy-HumanEval.

Đầu tiên, mô hình hoạt động tốt hơn với mã nguồn một phần dài hơn trong trường hợp tham chiếu (Hình 3a). Trong khi hoàn thành ngây thơ hoạt động kém tổng thể với các lỗi tiềm ẩn (Hình 3b), nó hoạt động tương đối tốt khi các lỗi tiềm ẩn xuất hiện trên hoặc gần dòng cuối của mã nguồn một phần (dọc theo đường chéo). Thú vị hơn, rewriting-then-completion đạt điểm số cao hơn khi các lỗi tiềm ẩn xuất hiện muộn hơn (Hình 3d), trong khi completion-then-rewriting hoạt động tốt hơn với các prefix mã nguồn dài hơn (Hình 3c).

Chúng tôi nghi ngờ rằng một prefix dài hơn làm cho các mô hình hoàn thành ít có khả năng sai lệch từ hoàn thành tham chiếu. Do đó, như đầu vào cho mô hình viết lại tiếp theo, mã nguồn hoàn thành giống hơn các loại đầu vào mà mô hình viết lại được huấn luyện, làm cho việc sửa chữa có khả năng thành công hơn.

### 4.6 Nghiên cứu Tình huống

**Các lỗi tiềm ẩn có luôn có hại không?** Như được thảo luận trong Phần 2, các lỗi tiềm ẩn không đảm bảo mã nguồn hoàn thành sẽ có lỗi. Trong khi chúng tôi quan sát sự suy giảm hiệu suất của Code-LLMs dưới sự tồn tại của các lỗi tiềm ẩn, chúng tôi tìm thấy một số trường hợp thú vị nơi các mô hình này vẫn sinh ra mã nguồn đúng.

Ví dụ, Hình 12 trong Phụ lục cho thấy rằng đối với lỗi tiềm ẩn == (được đánh dấu) được sửa đổi từ !=, mô hình hoàn thành cập nhật luồng thuật toán ban đầu của nó với lệnh continue và hoàn thành với mã nguồn đúng. Đây là một ví dụ rằng một số trường hợp bCC có thể phục hồi được và Code-LLMs có thể thích ứng với chúng.

**Khi nào Code-LLMs thành công tại bCC?** Đối với các trường hợp thành công của hoàn thành ngây thơ tại bCC, chúng tôi quan sát rằng hoặc mô hình (i) bỏ qua trạng thái không đúng và sinh ra hoàn thành đúng hoặc (ii) tính đến lỗi tiềm ẩn để sinh ra một hoàn thành thích ứng với nó. Hình 9 trong Phụ lục cho thấy một ví dụ khi Code-LLMs bỏ qua câu lệnh if-else để bỏ qua lỗi tiềm ẩn.

Các nghiên cứu tình huống sâu hơn và thảo luận chi tiết hơn có trong Phụ lục D.

## 5 Công trình Liên quan

**Hoàn thành mã nguồn.** Hoàn thành mã nguồn cung cấp gợi ý mã nguồn dựa trên một ngữ cảnh đã cho. Phạm vi hoàn thành từ token hoặc dòng tiếp theo, tên phương thức và lớp, đến toàn bộ hàm hoặc chương trình. Các công trình đầu xem mã nguồn như chuỗi token và áp dụng các mô hình ngôn ngữ thống kê cho bài toán, cùng với các nỗ lực khác trong việc xây dựng các mô hình xác suất của mã nguồn. Các công trình sau đó áp dụng mạng neural sâu và kỹ thuật pre-training cho mô hình hóa và hoàn thành mã nguồn tốt hơn. Công trình của chúng tôi xem xét thiết lập hoàn thành mã nguồn ở cấp hàm và chương trình và tập trung vào việc sử dụng các mô hình ngôn ngữ lớn cho hoàn thành. Ngoài mô hình hóa chuỗi, các công trình gần đây xem xét việc tích hợp kiến thức tiên nghiệm mã nguồn thông qua cây cú pháp trừu tượng, loại token mã nguồn, cấu trúc đồ thị, ngữ cảnh phân cấp, hoặc sinh sketch, hoặc thậm chí mở rộng thông tin của tác vụ vượt ra ngoài các file đầu vào đã cho. Chúng tôi tập trung vào việc chỉ sử dụng mô tả tác vụ và mã nguồn một phần như prompt đầu vào cho mô hình, cho phép sử dụng nhiều Code-LLMs hơn và các bộ dữ liệu có sẵn công khai.

**Sửa chữa chương trình tự động.** Nghiên cứu về chương trình tự động giảm bớt nỗ lực to lớn của nhà phát triển trong việc tìm và sửa lỗi lập trình. Gần đây, Code-LLMs đã được thích ứng cho sửa chữa chương trình bằng cách dịch các chương trình có lỗi thành các đối tác tham chiếu của chúng. Trong số đó, chúng tôi sử dụng RealiT như mô hình sửa chữa của chúng tôi trong phương pháp completion-then-rewriting vì chúng đạt kết quả SotA và sử dụng các đột biến đơn giản tương tự trong quá trình huấn luyện. Mặc dù có sự tương đồng, sửa chữa mã nguồn thường nhắm vào sửa lỗi từ các chương trình hoàn chỉnh trong khi chúng tôi nghiên cứu các lỗi tiềm ẩn từ mã nguồn một phần. Để làm phong phú lượng dữ liệu cho sửa chữa chương trình, các phương pháp đã được đề xuất để tổng hợp lỗi nhân tạo thông qua đột biến mã nguồn hoặc học tạo lỗi. Tương tự, chúng tôi sử dụng đột biến mã nguồn để tạo lỗi nhân tạo.

**Liên quan đến ví dụ đối kháng.** Ví dụ đối kháng là các instance nơi các nhiễu loạn nhỏ đối với đầu vào dẫn mô hình thay đổi thành dự đoán sai. Chúng đã được nghiên cứu rộng rãi trong thị giác máy tính và xử lý ngôn ngữ tự nhiên. Các công trình gần đây gợi ý các tình huống tương tự cho các mô hình học mã nguồn, nơi các biến đổi mã nguồn nhỏ, bảo tồn ngữ nghĩa dẫn đến suy giảm hiệu suất. Hoàn thành mã nguồn có lỗi có thể được xem như một bài toán dual với ví dụ đối kháng, nơi chúng ta mong đợi mô hình điều chỉnh dự đoán của nó khi có các thay đổi thay đổi ngữ nghĩa nhỏ trong đầu vào. Trong trường hợp của chúng tôi, tính nhạy cảm không phải là vấn đề, nhưng tính không nhạy cảm là vấn đề.

**Benchmark cho hoàn thành mã nguồn có lỗi.** Nhiều benchmark đã được nghiên cứu cho hoàn thành mã nguồn và sửa chữa chương trình. Đối với hoàn thành mã nguồn, CodeXGLUE, CodeNet, và HumanEval được sử dụng rộng rãi. CodeXGLUE chứa corpus các chương trình Java và Python cho hoàn thành nhưng chỉ hỗ trợ đánh giá dựa trên khớp. CodeNet thu thập các bài toán lập trình từ các trang judge trực tuyến, với cả giải pháp và test case. HumanEval có thể được coi là một bộ dữ liệu hoàn thành hàm Python, với ngữ cảnh là câu phát biểu bài toán và header hàm. Chúng tôi tạo ra các bộ dữ liệu của chúng tôi từ các bộ dữ liệu HumanEval và CodeNet. Đối với sửa chữa chương trình neural, nhiều bộ dữ liệu đòi hỏi đánh giá dựa trên khớp hoặc tập trung vào lỗi compiler, khác với thiết lập của chúng tôi. Trong khi IntroClass, QuixBugs, Defects4J, hoặc Refactory cung cấp các test suite cho đánh giá, test của chúng không phản ánh lỗi thực tế hoặc thiếu hỗ trợ ngữ cảnh để sử dụng với Code-LLMs. FixEval được đề xuất gần đây như một bộ dữ liệu sửa chữa chương trình nhận thức ngữ cảnh mới để giảm thiểu những hạn chế này, với nhiều bài toán được tạo ra từ các chương trình được nộp thực. Tuy nhiên, vì FixEval không cung cấp câu phát biểu bài toán và chỉ tập trung vào sửa chữa chương trình, chúng tôi tạo ra một benchmark mới sử dụng FixEval và nguồn bài toán của nó – CodeNet.

## 6 Thảo luận và Kết luận

**Hạn chế.** Các phương pháp baseline của chúng tôi được phát triển cho bCC có thể làm giảm hiệu suất hoàn thành trên ngữ cảnh mã nguồn tham chiếu, như được hiển thị trong Phần 4.4, gợi ý nhu cầu cân bằng các thiết lập có lỗi và tham chiếu trong các giải pháp cho bCC. Hơn nữa, trong khi buggy-FixEval được liên kết với các chương trình thi lập trình thực tế, không rõ buggy-FixEval phù hợp đến mức nào với thiết lập phát triển phần mềm chung nơi việc có được một test suite và đánh giá đúng đắn thách thức hơn.

**Tác động và ứng dụng.** Vì công trình của chúng tôi tập trung vào mã nguồn đang-trong-quá-trình ít được tinh chỉnh và dễ lỗi hơn, ngữ cảnh mã nguồn nên được xem như một gợi ý về ý định người dùng thay vì một triển khai "vàng" chất lượng cao. Do đó tự nhiên theo sau rằng một đối tác lập trình hoặc một công cụ thông minh nên gợi ý một thay đổi cho mã nguồn nháp thay vì tiếp tục mù quáng nếu họ tin rằng một phần nhất định của nháp hiện tại không được dự định. Từ góc độ trải nghiệm người dùng, một IDE có thể hiển thị gợi ý thay đổi mã nguồn cho người dùng về mã nguồn hiện tại của họ nếu các phần như vậy được xác định. Chức năng tương tự đã tồn tại cho các loại gợi ý thay đổi mã nguồn khác, ví dụ sửa chính tả và import thiếu.

**Kết luận.** Chúng tôi giới thiệu và định nghĩa bài toán hoàn thành mã nguồn có lỗi, được lấy cảm hứng từ tình huống lập trình thực tế nơi người ta hoàn thành một chương trình lập trình với câu phát biểu bài toán và một mã nguồn một phần có các lỗi tiềm ẩn. Chúng tôi xây dựng hai bộ dữ liệu mới, buggy-HumanEval và buggy-FixEval, như các benchmark tác vụ và phát hiện rằng sự tồn tại của các lỗi tiềm ẩn làm giảm đáng kể hiệu suất hoàn thành của tất cả các mô hình ngôn ngữ lớn của mã nguồn được đánh giá. Điều tra sâu hơn của chúng tôi về các phương pháp hoàn thành cho Code-LLMs trong việc xử lý các lỗi tiềm ẩn cho thấy rằng hoàn thành với các lỗi tiềm ẩn vẫn thách thức mặc dù tăng cường các mô hình với các mô hình sửa chữa chương trình bên ngoài. Chúng tôi cung cấp các nghiên cứu ablation và tình huống mở rộng để hiểu và phân tích sâu hơn về thiết lập hoàn thành mã nguồn có lỗi. Chúng tôi hy vọng nghiên cứu mới và có hệ thống của chúng tôi sẽ mở đường cho các công trình tương lai trong việc hiểu và cải thiện khả năng sử dụng của Code-LLMs dưới các thiết lập phát triển phần mềm thực tế.
