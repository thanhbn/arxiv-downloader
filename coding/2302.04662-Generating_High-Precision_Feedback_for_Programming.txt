# 2302.04662.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2302.04662.pdf
# File size: 770959 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Generating High-Precision Feedback for Programming
Syntax Errors using Large Language Models
Tung Phung1
MPI-SWS
mphung@mpi-sws.orgJosé Cambronero2
Microsoft
jcambronero@microsoft.comSumit Gulwani2
Microsoft
sumitg@microsoft.com
Tobias Kohn2
TU Wien
tobias.kohn@tuwien.ac.atRupak Majumdar2
MPI-SWS
rupak@mpi-sws.org
Adish Singla2
MPI-SWS
adishs@mpi-sws.orgGustavo Soares2
Microsoft
gsoares@microsoft.com
ABSTRACT
Large language models (LLMs), such as Codex, hold great
promise in enhancing programming education by automat-
ically generating feedback for students. We investigate us-
ing LLMs to generate feedback for xing syntax errors in
Python programs, a key scenario in introductory program-
ming. More concretely, given a student's buggy program,
our goal is to generate feedback comprising a xed pro-
gram along with a natural language explanation describ-
ing the errors/xes, inspired by how a human tutor would
give feedback. While using LLMs is promising, the critical
challenge is to ensure high precision in the generated feed-
back, which is imperative before deploying such technology
in classrooms. The main research question we study is: Can
we develop LLMs-based feedback generation techniques with
a tunable precision parameter, giving educators quality con-
trol over the feedback that students receive? To this end, we
introduce PyFiXV , our technique to generate high-precision
feedback powered by Codex. The key idea behind PyFiXV
is to use a novel run-time validation mechanism to decide
whether the generated feedback is suitable for sharing with
the student; notably, this validation mechanism also pro-
vides a precision knob to educators. We perform an exten-
sive evaluation using two real-world datasets of Python pro-
grams with syntax errors and show the ecacy of PyFiXV
in generating high-precision feedback.
Keywords
Programming education, Python programs, syntax errors,
feedback generation, large language models
∗1: Corresponding author.
2: Listed in alphabetical order.1. INTRODUCTION
Large language models (LLMs) trained on text and code
have the potential to power next-generation AI-driven edu-
cational technologies and drastically improve the landscape
of computing education. One of such popular LLMs is Ope-
nAI's Codex [1], a variant of the 175 billion parameter model
GPT-3 [2], trained by ne-tuning GPT-3 on code from over
50 million GitHub repositories. A recent study ranked Codex
in the top quartile w.r.t. students in a large introductory
programming course [3]. Subsequently, recent works have
shown promising results in using Codex on various program-
ming education scenarios, including generating new program-
ming assignments [4], providing code explanations [5], and
enhancing programming-error-messages [6].
We investigate the use of LLMs to generate feedback for
programming syntax errors, a key scenario in introductory
programming education. Even though such errors typically
require small xes and are easily explainable by human tu-
tors, they can pose a major hurdle in learning for novice stu-
dents [7]. Moreover, the programming-error-messages pro-
vided by the default programming environment are often
cryptic and unable to provide explicable feedback to stu-
dents [8{10]. Ideally, a human tutor would help a novice
student by providing detailed feedback describing the er-
rors and required xes to the buggy program; however, it is
extremely tedious/challenging to provide feedback at scale
given the growing enrollments in introductory programming
courses [11, 12]. To this end, our goal is to automate the
feedback generation process using LLMs-based techniques.
More concretely, given a student's buggy program, we want
to generate feedback comprising a xed program and a natu-
ral language explanation describing the errors/xes, inspired
by how a human tutor would give feedback. While models
like Codex, trained on both text and code, are naturally
suitable for this, the critical challenge is to ensure high pre-
cision in the generated feedback. High precision is imper-
ative in building educators' trust before deploying such an
AI-driven technology in classrooms. A recent work inves-
tigated enhancing the default programming-error-messages
using Codex [6]; one of the takeaways, quoted from their pa-
per, is \The key implications of this work are that program-arXiv:2302.04662v2  [cs.PL]  28 Apr 2023

--- PAGE 2 ---
1print "x"
2a = input ("xxx")
3print "Xx x"a
4b = input ("xxx xxx?")
5print " xxxxxx " a + b
(a) Student's buggy program1print "x"
2a = input ("xxx")
3print "Xx x"+a
4b = input ("xxx xxx?")
5print " xxxxxx " +a + b
(b) Generated xed programThe student forgets to concate-
nate the string literal with the
variable. We can x the error by
adding a plus sign between the
string literal and the variable.
(c) Generated explanation
 (d) Validation
Figure 1: Illustrative example showcasing PyFiXV on a buggy Python 2 program from TigerJython [9]. (a)Buggy program
(with string literals anonymized as sequences of `x' in our dataset). (b, c) Fixed program and explanation generated by
PyFiXV .(d)PyFiXV 's validation mechanism successfully accepted the generated feedback as suitable for sharing.
1for i in range (int ( input ()):
2 l1 =[]
3 for i in range (int ( input ())):
4 if i ==0:
5 if i ==0:
6 l1. appned (n)
7 elif (i==(n -1) ):
8 l1. append (1)
9 else :
10 l1. append (i)
11 print (l1)
(a) Student's buggy program1for i in range (int ( input ())):
2 l1 =[]
3 for i in range (int ( input ())):
4 if i ==0:
5 if i ==0:
6 l1. append (n)
7 elif (i==(n -1) ):
8 l1. append (1)
9 else :
10 l1. append (i)
11 print (l1)
(b) Generated xed programIn line 1, a closing parenthesis
is missing when calling function
`range'. We should add a closing
parenthesis at the end of line 1,
before the colon, to x this er-
ror. In line 6, the function `ap-
pend' is misspelled. We should
replace `appned' with `append'
to x this error.
(c) Generated explanation
 (d) Validation
Figure 2: Similar to Figure 1, this example showcases PyFiXV on a buggy Python 3 program from Codeforces [13]. While
the misspelling of `append' as `appned' is not a syntax error (raising an AttributeError at runtime instead), PyFiXV typically
xes such mistakes as well, showing the generalization value in LLMs-powered technique.
1import math
2
3def binary (num):
4 x=int ( math . log2 (num))+1
5 for i in range (x):
6 num =( num ^^(1 < <i))
7 print ( num )
8for i in range (int , input ()):
9 n=int ( input ())
10 binary (n)
(a) Student's buggy program1import math
2
3def binary (num):
4 x=int ( math . log2 (num))+1
5 for i in range (x):
6 num =( num ^ (1<<i))
7 print ( num )
8for i in range (int ( input ())):
9 n=int ( input ())
10 binary (n)
(b) Generated xed programIn line 6, the operator `^^' is
not a valid operator in Python 3.
We should replace it with `^' to
x this error. In line 8, a closing
parenthesis is missing when call-
ing function `range'. We should
add a closing parenthesis at the
end of line 8, before the colon,
to x this error.
(c) Generated explanation
 (d) Validation
Figure 3: Similar to Figure 2, this example showcases PyFiXV on a buggy Python 3 program from Codeforces [13]. PyFiXV 's
validation mechanism successfully rejected the generated feedback (we marked text in (c)to highlight issues with explanation).
ming error message explanations and suggested xes gener-
ated by LLMs are not yet ready for production use in in-
troductory programming classes..." . Our initial experiments
(Section 4) also highlight issues in generating high-precision
feedback. To this end, the main research question is:
Can we develop LLMs-based feedback generation techniques
with a tunable precision parameter, giving educators quality
control over the feedback that students receive?
1.1 Our Approach and Contributions
In this paper, we develop PyFiXV , our technique to gen-
erate high-precision feedback powered by Codex. Given a
student's buggy program as input, PyFiXV decomposes the
overall process into (i) feedback generation (i.e., a xed pro-
gram and a natural language explanation for errors/xes);
and (ii) feedback validation (i.e., deciding whether the gen-
erated feedback is suitable for sharing with the student).
One of the key ideas in PyFiXV is to use a run-time feed-
back validation mechanism that decides whether the gener-
ated feedback is of good quality. This validation mechanism
uses Codex as a simulated student model { the intuition is
that a good quality explanation, when provided as Codex'sprompt instruction, should increase Codex's success in con-
verting the buggy program to the xed program. Notably,
this validation also provides a tuneable precision knob to
educators to control the precision and coverage trade-o.
The illustrative examples in Figures 1, 2, and 3 showcase
PyFiXV on three dierent student's buggy programs. Our
main contributions are:
(I) We formalize the problem of generating high-precision
feedback for programming syntax errors using LLMs,
where feedback comprises a xed program and a nat-
ural language explanation. (Section 2)
(II) We develop a novel technique, PyFiXV , that gener-
ates feedback using Codex and has a run-time feedback
validation mechanism to decide whether the generated
feedback is suitable for sharing. (Section 3)
(III) We perform extensive evaluations using two real-world
datasets of Python programs with syntax errors and
showcase the ecacy of PyFiXV . We publicly release
the implementation of PyFiXV . (Section 4)1
1Github: https://github.com/machine-teaching-group/
edm2023_PyFiXV

--- PAGE 3 ---
1.2 Related Work
Feedback generation for programming errors. There has been
extensive work on feedback generation for syntactic/seman-
tic programming errors [14{18]; however, these works have
focused on xing/repairing buggy programs without pro-
viding explanations. The work in [11] proposed a tech-
nique to generate explanations; however, it requires pre-
specied rules that map errors to explanations. Another line
of work, complementary to ours, has explored crowdsourcing
approaches to obtain explanations provided by other stu-
dents/tutors [19, 20]. There has also been extensive work
on improving the programming-error-messages by designing
customized environments [9, 10]. As discussed earlier, a re-
cent study used Codex to enhance these error messages [6];
however, our work is dierent as we focus on generating
high-precision feedback with a tuneable precision knob.
Validation of generated content. In recent work, [21] devel-
oped a technique to validate LLMs' output in the context
of program synthesis. While similar in spirit, their valida-
tion mechanism is dierent and operates by asking LLMs
to generate predicates for testing the synthesized programs.
Another possible approach is to use back-translation mod-
els to validate the generated content [22, 23]; however, such
a back-translation model (that generates buggy programs
from explanations) is not readily available for our setting.
Another approach, complementary to ours, is to use human-
in-the-loop for validating low condence outputs [24].
2. PROBLEM SETUP
Next, we introduce denitions and formalize our objective.
2.1 Preliminaries
Student's buggy program. Consider a student working on
a programming assignment who has written a buggy pro-
gram with syntax errors, such as shown in Figures 1a, 2a,
and 3a. Formally, these syntax errors are dened by the
underlying parser of the programming language [14]; we will
use the Python programming language in our evaluation.
Henceforth, we denote such a buggy program as Pb, which
is provided as an input to feedback generation techniques.
Feedback style. GivenPb, we seek to generate feedback com-
prising a xed program along with a natural language expla-
nation describing the errors and xes. This feedback style is
inspired by how a human tutor would give feedback to novice
students in introductory programming education [5, 9]. We
denote a generated xed program as Pf, a generated expla-
nation asX, and generated feedback as a tuple ( Pf;X).
Feedback quality. We assess the quality of generated feed-
back (Pf;X) w.r.t.Pbalong the following binary attributes:
(i)Pfis syntactically correct and is obtained by making a
small number of edits to x Pb; (ii)Xis complete, i.e., con-
tains information about all errors and required xes; (iii)
Xis correct, i.e., the provided information correctly ex-
plains errors and required xes; (iv) Xis comprehensible,
i.e., easy to understand, presented in a readable format, and
doesn't contain redundant information. These attributes are
inspired by evaluation rubrics used in literature [6, 25{27].
In our evaluation, feedback quality is evaluated via ratings
by experts along these four attributes. We measure feedback
quality as binary by assigning the value of 1 (good quality)
Generating fixed programGeneratingexplanationValidatingfeedback𝒫"𝒫#(𝒫#,𝒳)		𝔻*+,-(𝒫#,𝒳)
Complete
Edit
EditFigure 4: Illustration of three dierent compoments/stages
inPyFiXV 's feedback generation process; see Section 3.
if it satises allthe four quality attributes and otherwise 0
(bad quality).2
2.2 Performance Metrics and Objective
Performance metrics. Next, we describe the overall perfor-
mance metrics used to evaluate a feedback generation tech-
nique. For a buggy program Pbas input, we seek to design
techniques that generate feedback ( Pf;X) and also decide
whether the generated feedback is suitable for sharing with
the student. We measure the performance of a technique
using two metrics: (i) Coverage measuring the percentage
number of times the feedback is generated and provided to
the student ; (ii) Precision measuring the percentage num-
ber of times the provided feedback is of good quality w.r.t.
the binary feedback quality criterion introduced above. In
our experiments, we will compute these metrics on a dataset
Dtest=fPbgcomprising a set of students' buggy programs.3
Objective. Our goal is to design feedback generation tech-
niques with high precision, which is imperative before de-
ploying such techniques in classrooms. In particular, we
want to develop techniques with a tuneable precision pa-
rameter that could provide a knob to educators to control
the precision and coverage trade-o.
3. OUR TECHNIQUE P YFIXV
In this section, we present PyFiXV , our technique to gen-
erate high-precision feedback using LLMs. PyFiXV uses
OpenAPI's Codex as LLMs [1] { Codex has shown com-
petitive performance on a variety of programming bench-
marks [1, 3, 17, 18], and is particularly suitable for PyFiXV
as we seek to generate both xed programs and natural lan-
guage explanations. More specically, PyFiXV uses two
access points of Codex provided by OpenAI through public
APIs: Codex-Edit [28] and Codex-Complete [29]. As illus-
trated in Figure 4, PyFiXV has the following three com-
ponents/stages: (1) generating a xed program Pfby edit-
ingPbusing Codex-Edit; (2) generating natural language
explanationXusing Codex-Complete; (3) validating feed-
back (Pf;X) using Codex-Edit to decide whether the gen-
erated feedback is suitable for sharing. The overall pipeline
ofPyFiXV is modular and we will evaluate the utility of
dierent components in Section 4. Next, we provide details
for each of these stages.
2We note that the four attributes are independent. In partic-
ular, the attribute\complete"captures whether the explana-
tion contains information about all errors/xes (even though
the information could be wrong), and the attribute \correct"
captures the correctness of the provided information.
3When a technique cannot generate feedback for an input
programPb(e.g., the technique is unable to nd a xed pro-
gram), then we use a natural convention that no feedback is
provided to the student|this convention lowers the cover-
age metric but doesn't directly aect the precision metric.

--- PAGE 4 ---
Input Program for Codex-Edit
1for i in range (int ( input ()):
2 l1 =[]
3 for i in range (int ( input ())):
4 if i ==0:
5 if i ==0:
6 l1. appned (n)
7 elif (i==(n -1)):
8 l1. append (1)
9 else :
10 l1. append (i)
11 print (l1)
Instructions for Codex-Edit
Fix the syntax errors in this Python 3
code
(a) Stage-1 prompt for generating PfPrompt for Codex-Complete
1# Python 3
2# Give feedback for the syntax
error fixes below :
3
4...... < few - shot example 1 >......
5...... < few - shot example 2 >......
6...... < few - shot example 3 >......
7
8# [ BUGGY PYTHON 3]
9for i in range ( int ( input ()):
10 l1 =[]
11 for i in range (int ( input ())):
12 if i ==0:
13 if i ==0:
14 l1. appned (n)
15 elif (i==(n -1) ):
16 l1. append (1)
17 else :
18 l1. append (i)
19 print (l1)
20
21
22# [ FIX ]
231c1
24< for i in range ( int ( input ()):
25 ---
26> for i in range ( int ( input ())):
276c6
28< l1. appned (n)
29 ---
30> l1. append (n)
31
32
33# [ FEEDBACK ] The syntax error in
this Python 3 code is:
(b) Stage-2 prompt for generating XInput Program for Codex-Edit
1for i in range ( int ( input ()):
2 l1 =[]
3 for i in range (int ( input ())):
4 if i ==0:
5 if i ==0:
6 l1. appned (n)
7 elif (i==(n -1) ):
8 l1. append (1)
9 else :
10 l1. append (i)
11 print (l1)
Instructions for Codex-Edit
The syntax error in this Python 3
code is: In line 1, a closing paren-
thesis is missing when calling func-
tion `range'. We should add a clos-
ing parenthesis at the end of line 1,
before the colon, to x this error. In
line 6, the function `append' is mis-
spelled. We should replace `appned'
with `append' to x this error.
(c) Stage-3 prompt for validating ( Pf;X)
Figure 5: Illustration of prompts used by dierent stages of PyFiXV for buggy Python 3 program in Figure 2. In particular,
the \Instructions for Codex-Edit" in (c)is obtained by concatenating line33 of (b)and the generated Xshown in Figure 2c.
3.1 Stage-1: Generating Fixed Program
Given a student's buggy program Pbas input, PyFiXV 's
Stage-1 generates a xed program Pf. We use Codex-Edit
for xing/repairing the buggy program in this stage since it
has shown to be competitive in program repair benchmarks
in recent works [30]. Figure 5a shows a sample prompt used
byPyFiXV to query Codex-Edit for the buggy Python 3
program in Figure 2a. The process of generating Pfis de-
termined by two hyperparameters: (i) t12[0:0;1:0] is the
temperature value specied when querying Codex-Edit and
controls stochasticity/diversity in generated programs; (ii)
n1controls the number of queries made to Codex-Edit.
More concretely, PyFiXV begins by making n1queries to
Codex-Edit with temperature t1. Then, out of n1gener-
ated programs, PyFiXV selectsPfas the program that is
syntactically correct and has the smallest edit-distance to
Pb. Here, edit-distance between two programs is measured
by rst tokenizing programs using Pygments library [31]
and then computing Levenshtein edit-distance over token
strings.4If Stage-1 is unable to generate a xed program,
the process stops without generating any feedback; see Foot-
note 3. In our experiments, we set ( t1= 0:5;n1= 10) and
obtained a high success rate of generating a xed program
Pfwith a small number of edits w.r.t. Pb.
3.2 Stage-2: Generating Explanation
GivenPbandPfas inputs, PyFiXV 's Stage-2 generates a
natural language explanation Xdescribing errors/xes. We
use Codex-Complete in this stage as it is naturally suited
to generate text by completing a prompt [1, 5, 6]. A cru-
4Note that buggy programs are not parseable to Abstract
Syntax Tree (AST) representations and string-based dis-
tance is commonly used in such settings (e.g., see [17]).cial ingredient of Stage-2 is the annotated dataset Dshotused
to select few-shot examples when querying Codex-Complete
(see Figure 4). Figure 5b shows a sample prompt used by
PyFiXV to query Codex-Complete for the scenario in Fig-
ure 2. In Figure 5b, line4{line6 indicate three few-shot ex-
amples (not shown for conciseness), line9{line19 provides
Pb, line23{line30 provides Pfin the form of line-di w.r.t.
Pb, and line33 is the instruction to be completed by Codex-
Complete. Given a prompt, the process of generating X
is determined by two hyperparameters: (i) a temperature
valuet2(= 0) and (ii) the number of queries n2(= 1). Next,
we discuss the role of Dshotin selecting few-shots examples.
When querying Codex-Complete, we use three few-shot ex-
amples selected from Dshot, an annotated dataset of exam-
ples comprising buggy programs and desired feedback ob-
tained by expert annotations (see Section 4.2). These anno-
tated examples essentially provide a context to LLMs and
have shown to play an important role in optimizing the gen-
erated output (e.g., see [1, 2, 17, 18, 32]). In our case, Dshot
provides contextualized training data, capturing the format
of how experts/tutors give explanations. Given PbandPf,
we use two main criteria to select few-shot examples. The
primary criterion is to pick examples where the error type
of buggy program in the example is same as that of Pb|
the underlying parser/compiler provides error types (e.g.,
`InvalidSyntax', `UnexpectedIndent'). The secondary crite-
rion (used to break ties in the selection process) is based on
the edit-distance between thediof buggy/xed program
in the example and diofPb/Pf. In Section 4, we conduct
ablations to showcase the importance of selecting few-shots.
3.3 Stage-3: Validating Feedback
GivenPband (Pf;X) as inputs, PyFiXV 's Stage-3 vali-
dates the feedback quality and makes a binary decision of

--- PAGE 5 ---
Technique TigerJython Codeforces
Precision Coverage Precision Coverage
PyFi-PEM 05:0 (1:0) 92:5 (1:6) 35:0 (2:4) 98:8 (0:8)
PyFiX shot: None 00:9 (0:5) 92:5 (1:6) 03:0 (0:4) 98:8 (0:8)
PyFiX shot: Rand 21:6 (1:7) 92:5 (1:6) 48:5 (2:6) 98:8 (0:8)
PyFiX shot: Sel 38:9 (3:5) 92:5 (1:6) 55:2 (3:9) 98:8 (0:8)
PyFi jjXshot: Sel15:8 (1:8) 92:5 (1:6) 15:6 (2:8) 98:8 (0:8)
PyFiX-Rule P70 48:6 (4:4) 30:8 (12:5)61:6 (9:0) 38:3 (10:5)
PyFiXV P70 76:0 (4:0)31:2 (4:0) 72:4 (6:2)64:2 (6:3)
PyFiX-Opt PVP7076:1 (0:4) 47:1 (3:4) 72:8 (0:1) 75:0 (5:7)
(a) Results for dierent techniques, reported as mean (stderr)
0.4 0.6 0.8 1.0
Precision0.20.40.60.81.0Coverage
TigerJython: P YFIX-O PT
TigerJython: P YFIXV
TigerJython: P YFIX-R ULE (b) TigerJython trade-o curve
0.50.60.70.80.91.0
Precision0.20.40.60.81.0Coverage
Codeforces: P YFIX-O PT
Codeforces: P YFIXV
Codeforces: P YFIX-R ULE (c) Codeforces trade-o curve
Figure 6: Experimental results on two real-world datasets of Python programs, namely TigerJython [9] and Codeforces [13].
\accept" (feedback is suitable for sharing) or \reject" (feed-
back is discarded). PyFiXV uses a novel run-time feedback
validation mechanism using Codex-Edit to decide whether
the feedback (Pf;X) w.r.t.Pbis of good quality. Here,
Codex-Edit is used in the ipped role of a simulated student
model { the intuition is that a good quality explanation X,
when provided in Codex-Edit's prompt instruction, should
increase Codex-Edit's success in converting PbtoPf. Fig-
ure 5c shows a sample prompt used by PyFiXV to query
Codex-Edit for the scenario in Figure 2|see the caption on
how \Instructions for Codex-Edit" in Figure 5c is obtained.5
The validation mechanism has three hyperparameters: (i)
t32[0:0;1:0] is the temperature value specied when query-
ing Codex-Edit; (ii) n3controls the number of queries made
to Codex-Edit; (iii) h32[1;n3] is the threshold used for
acceptance decision. More concretely, PyFiXV begins by
makingn3queries to Codex-Edit with temperature t3. Then,
out ofn3generated programs, PyFiXV counts the num-
ber of programs that don't have syntax errors and have an
exact-match withPf. Here, exact-match is checked by con-
verting programs to their Abstract Syntax Tree (AST)-based
normalized representations.6Finally, the validation mecha-
nism accepts the feedback if the number of exact matches
is at leasth3. These hyperparameters ( t3;n3;h3) also pro-
vide a precision knob and are selected to obtain the desired
precision level, as discussed next.
3.4 Precision and Coverage Trade-Off
PyFiXV 's validation mechanism provides a precision knob
to control the precision and coverage trade-o (see perfor-
mance metrics in Section 2.2). Let P be the desired precision
level we want to achieve for PyFiXV . The idea is to choose
Stage-3 hyperparameters ( t3;n3;h3) that achieve P precision
level. For this purpose, we use a calibration dataset Dcalfor
5In our initial experiments, we tried using alternative signals
for validation, such as (a) Codex-Complete's probabilities
associated with generated X; (b) automatic scoring of X
w.r.t. explanations in few-shots using BLEU score [33]; (c)
ltering based on X's length. Section 4 reports results for (c)
as it had the highest performance among these alternatives.
6We check for AST-based exact match instead of checking
for Levenshtein edit-distance over token strings being 0 (see
Section 3.1). AST-based exact match is more relaxed than
edit-distance being 0 { AST-based representation ignores
certain dierences between codes, e.g., based on extra spaces
and comments. We used the AST-based exact match in the
validation mechanism as it is more robust to such dierences.picking the hyperparameters. More concretely, in our ex-
periments, PyFiXV rst computes performance metrics on
Dcalfor the following range of values: (i) t32f0:3;0:5;0:8g;
(ii)n32f10g; (iii)h32f1;2;:::; 10g. Then, it chooses
(t3;n3;h3) that has at least P precision level and maximizes
coverage; when achieving the desired P is not possible, then
the next lower possible precision is considered. The cho-
sen values of hyperparameters are then used in PyFiXV 's
Stage-3 validation mechanism. We refer to PyFiXV Pxas
the version of PyFiXV calibrated with P x.
4. EXPERIMENTAL EVALUATION
We perform evaluations using two real-world Python pro-
gramming datasets, namely TigerJython [9] and Codeforces
[13]. We picked Python because of its growing popularity as
an introductory programming language; notably, PyFiXV
can be used with other languages by appropriately changing
the prompts and tokenizers used. We use OpenAI's public
APIs for Codex-Edit [28] ( model=code-davinci-edit-001 ) and
Codex-Complete [29] ( model=code-davinci-002 ). We begin
by describing dierent techniques used in the evaluation.
4.1 Baselines and Variants of P YFIXV
Default programming-error-messages without validation. As
our rst baseline, PyFi-PEM usesPyFiXV 's Stage-1 to
generatePfand uses programming-error-messages provided
by the programming environment as X.PyFi-PEM uses er-
ror messages provided by Python 2 :7 environment for Tiger-
Jython and Python 3 :12 environment for Codeforces. This
baseline is without validation (i.e., the generated feedback
is always accepted).
Variants of PyFiXV without validation. PyFiX shot: Selis a
variant of PyFiXV without the validation mechanism (i.e.,
only uses Stage-1 and Stage-2). PyFiX shot: Rand is a vari-
ant of PyFiX shot: Selwhere few-shot examples in Stage-2 are
picked randomly from Dshot.PyFiX shot: None is a variant of
PyFiX shot: Selthat doesn't use few-shot examples in Stage-2.
PyFi jjXshot: Selis a variant of PyFiX shot: Selthat runs Stage-
1 and Stage-2 in parallel; hence, Stage-2's prompt doesn't
make use ofPf. All these variants are without validation
(i.e., the generated feedback is always accepted).
Techniques with alternative validation mechanisms. We con-
sider two variants of PyFiXV , namely PyFiX-Rule and
PyFiX-Opt , that use dierent validation mechanisms (i.e.,
replace PyFiXV 's Stage-3 with an alternative validation).

--- PAGE 6 ---
1import time
2
3def cd( seconds ):
4 while seconds > 0:
5 rem_m = seconds // 60
6 rem_s = seconds % 60
7 remaining_time = str ( rem_m ) + "
minutes " + str ( rem_s ) + " seconds "
8 print ( remaining_time , end ='\r')
9 seconds = seconds - 1
10 time . sleep (1)
11
12 print (" Xxxxx xxxxX ")
13
14cd (60)
(a) Student's buggy program Pb1from __future__ import print_function
2import time
3
4def cd( seconds ):
5 while seconds > 0:
6 rem_m = seconds // 60
7 rem_s = seconds % 60
8 remaining_time = str ( rem_m ) + "
minutes " + str ( rem_s ) + " seconds "
9 print ( remaining_time , end ='\r')
10 seconds = seconds - 1
11 time . sleep (1)
12
13 print (" Xxxxx xxxxX ")
14
15cd (60)
(b) Generated PfThe student
wants to use the
print function
from Python 3.
To do so, the
correct way is to
add the line `from
future import
print function' at
the beginning of
the code.
(c) Generated X
 (d) Validation
Figure 7: Similar to Figure 1, this illustrative example showcases PyFiXV on a buggy Python 2 program from TigerJython [9].
1name = input (" Xxx xx?")
2num = input ("Xxx xx xxxxxxx ?")
3print (" Xxxx " + name + " Xxx ")*,num
(a) Student's buggy program Pb1name = input ("Xxx xx?")
2num = input ("Xxx xx xxxxxxx ?")
3print (" Xxxx " + name + " Xxx ")*num
(b) Generated PfThe student forgets to enclose
a string literal with quotes.
We can x the error by enclos-
ing the string literal in line 3
with a pair of double quotes.
(c) Generated X
 (d) Validation
Figure 8: Similar to Figure 3, this example showcases PyFiXV on a buggy Python 2 program from TigerJython [9]. PyFiXV 's
validation mechanism successfully rejected the generated feedback (we marked text in (c)to highlight issues with explanation).
PyFiX-Rule validates (Pf;X) based onX's length, as noted
in Footnote 5. Given a hyperparameter hr, (Pf;X) is ac-
cepted if the number of tokens in Xis at mosthr, where to-
kenization is done by splitting on whitespaces/punctuations.
PyFiX-Rule 'shris picked from the set f30;40;50;:::; 200g
based on the desired precision level P, by following the cal-
ibration process in Section 3.4. PyFiX-Opt uses an oracle
validation that has access to expert's ratings for the gener-
ated feedback (Pf;X). Then, for a desired P, PyFiX-Opt
performs optimal validation and highlights the maximum
coverage achievable on Dtestfor the generated feedback.
4.2 Datasets and Evaluation Procedure
Datasets and annotations for few-shot examples. As our rst
dataset, namely TigerJython, we have 240 distinct Python 2
programs written by students in TigerJython's educational
programming environment [9]. We obtained a private and
anonymized version of the dataset used in [34], with string
literals in programs replaced with sequences of `x' (e.g., see
Figure 1). As our second dataset, namely Codeforces, we
curated 240 distinct Python 3 programs from the Code-
forces website using their public APIs [13], inspired by simi-
lar works that curate Codeforces dataset [35, 36]. Programs
in both datasets have syntax errors and have token length
at most 500 (see Section 3.1 about program tokenization).
For the Codeforces dataset, we only include programs sub-
mitted to contests held from July 2021 onwards (after the
cut-o date for Codex's training data [1]). Since a part of
these datasets will be used for few-shot examples (as Dshotin
PyFiXV 's Stage-2), we asked experts to annotate these 480
programs with feedback (i.e., a xed program along with an
explanation). Three experts, with extensive experience in
Python programming and tutoring, provided annotations.
Evaluation procedure and feedback ratings. Given a dataset
Dwith 240 buggy programs, we can evaluate a technique by
splitting Das follows: (a) Dtest(25%) for reporting preci-
sion and coverage performance metrics; (b) Dshot(50%) for
few-shot examples; (c) Dcal(25%) for calibrating validationmechanism. To report overall performance for techniques,
we perform a cross-validation procedure with four evalua-
tion rounds while ensuring that Dtestacross four rounds are
non-overlapping. We then report aggregated results across
these rounds as average mean (stderr). As discussed in Sec-
tions 2.1 and 2.2, evaluating these performance metrics re-
quires feedback ratings by experts to assess the quality of the
feedback generated by each technique.7For example, evalu-
ating metrics on TigerJython dataset for PyFiXV requires
480 feedback ratings (4 60 for Dtestand 460 for Dcal). To
begin, we did a smaller scale investigation to establish the
rating criteria, where two experts rated 100 generated feed-
back instances; we obtained Cohen's kappa reliability value
0:72 indicating substantial agreement between experts [37].
Afterward, one expert (with experience in tutoring Python
programming classes) did these feedback ratings for the eval-
uation results.8
4.3 Results
Comparison of dierent techniques. Figure 6a provides a
comparison of dierent techniques on two datasets. All tech-
niques here use PyFiXV 's Stage-1 to obtain Pf. The cov-
erage numbers of 92 :5 and 98:8 reported in Figure 6a corre-
spond to the success rate of obtaining Pfon these datasets
(the average edit-distance between PbandPfis about 10:4
and 7:5 tokens on these datasets, respectively). For our
baseline PyFi-PEM , we see a big jump in precision from 5 :0
for TigerJython (Python 2) to 35 :0 for Codeforces (Python
7We note that precision and coverage performance metrics
for dierent techniques are reported for the end-to-end pro-
cess associated with each technique, and not just for the
validation mechanism. Also, even if a technique doesn't use
any validation mechanism, the coverage could be less than
100:0 as discussed in Footnote 3.
8We note that the experts were blinded to the condition
(technique) associated with each feedback instance when
providing ratings. Moreover, these generated feedback in-
stances were given to experts in randomized order across
conditions instead of grouping them per condition.

--- PAGE 7 ---
3), owing to enhanced error messages in recent Python ver-
sions [38{40]. Results for PyFiXV P70in comparison with
results for PyFiX shot: Sel,PyFiX shot: Rand,PyFiX shot: None,
andPyFi jjXshot: Selshowcase the utility of dierent compo-
nents used in PyFiXV 's pipeline. Comparing PyFiXV P70
withPyFiX-Rule P70shows that PyFiXV 's validation sub-
stantially outperforms PyFiX-Rule 's validation.9Lastly,
results for PyFiX-Opt PVP70are obtained by setting the
desired precision level for PyFiX-Opt to match that of
PyFiXV P70onDtest{ the coverage numbers (47 :1 for
TigerJython and 75 :0 for Codeforces) indicate the maxi-
mum possible achievable coverage. Notably, PyFiXV P70
achieves a competitive coverage of 64 :2 on Codeforces.10
Precision and coverage trade-o curves. The curves in Fig-
ures 6b and 6c are obtained by picking dierent desired pre-
cision levels P and then computing precision/coverage val-
ues on Dtestw.r.t. P. The curves for PyFiX-Opt show the
maximum possible coverage achievable on Dtestfor dierent
precision levels P using our generated feedback. To obtain
these curves for PyFiXV andPyFiX-Rule , we did calibra-
tion directly on Dtestinstead of Dcal(i.e., doing ideal calibra-
tion for their validation mechanisms when comparing with
PyFiX-Opt 's curves). These curves highlight the precision
and coverage trade-o oered by PyFiXV in comparison to
a simple rule-based validation and the oracle validation.
Qualitative analysis. We have provided several illustrative
examples to demonstrate our technique PyFiXV . Figures 1,
2, and 7 show examples where PyFiXV 's Stage-1 and Stage-
2 generate good quality feedback and Stage-3 successfully
accepts the feedback. Figures 3 and 8 show examples where
PyFiXV 's Stage-1 and Stage-2 generate bad quality feed-
back and Stage-3 successfully rejects the feedback. Figure 7
highlights that PyFiXV can make non-trivial xes in the
buggy program and correctly explain them in a comprehen-
sible way. Figure 3 shows an example where the overall feed-
back is bad quality and successfully rejected, though parts of
the generated explanation are correct; this could potentially
be useful for tutors in a human-in-the-loop approach.
5. CONCLUDING DISCUSSIONS
We investigated using LLMs to generate feedback for xing
programming syntax errors. In particular, we considered
feedback in the form of a xed program along with a nat-
ural language explanation. We focussed on the challenge
of generating high-precision feedback, which is crucial be-
fore deploying such technology in classrooms. Our proposed
technique, PyFiXV , ensures high precision through a novel
run-time validation mechanism and also provides a precision
knob to educators. We performed an extensive evaluation to
9When comparing PyFiXV P70with these techniques in
Figure 6a, the results are signicantly dierent w.r.t. 2
tests [41] (p0:0001); here, we use contingency tables with
two rows (techniques) and four columns (240 data points
mapped to four possible precision/coverage outcomes).
10Techniques PyFiX shot: Sel,PyFiX-Rule ,PyFiXV P70,
andPyFiX-Opt PVP70dier only in terms of validation
mechanisms. We can compare the validation mechanisms
used in these techniques based on F1-score. The F1-scores
of these four techniques are as follows: 0 :56, 0:39, 0:70, and
0:86 for TigerJython, respectively; 0 :71, 0:47, 0:77, and 0:84
for Codeforces, respectively.showcase the ecacy of PyFiXV on two real-world Python
programming datasets. There are several interesting direc-
tions for future work, including (a) improving PyFiXV 's
components to obtain better precision/coverage trade-o,
e.g., by adapting our technique to use recent LLMs such
as ChatGPT [42] and GPT-4 [43] instead of Codex; (b)
extending PyFiXV beyond syntax errors to provide feed-
back for programs with semantic errors or partial programs;
(c) incorporating additional signals in PyFiXV 's validation
mechanism; (d) conducting real-world studies in classrooms.
6. ACKNOWLEDGMENTS
Funded/Co-funded by the European Union (ERC, TOPS,
101039090). Views and opinions expressed are however those
of the author(s) only and do not necessarily reect those
of the European Union or the European Research Council.
Neither the European Union nor the granting authority can
be held responsible for them.
References
[1] Mark Chen and et al. Evaluating Large Language Mod-
els Trained on Code. CoRR , abs/2107.03374, 2021.
[2] Tom B. Brown and et al. Language Models are Few-
Shot Learners. In NeurIPS , 2020.
[3] James Finnie-Ansley, Paul Denny, Brett A. Becker, An-
drew Luxton-Reilly, and James Prather. The Robots
Are Coming: Exploring the Implications of OpenAI
Codex on Introductory Programming. In ACE , 2022.
[4] Sami Sarsa, Paul Denny, Arto Hellas, and Juho
Leinonen. Automatic Generation of Programming Ex-
ercises and Code Explanations Using Large Language
Models. In ICER , 2022.
[5] Stephen MacNeil, Andrew Tran, Arto Hellas, Joanne
Kim, Sami Sarsa, Paul Denny, Seth Bernstein, and
Juho Leinonen. Experiences from Using Code Explana-
tions Generated by Large Language Models in a Web
Software Development E-Book. In SIGCSE , 2023.
[6] Juho Leinonen, Arto Hellas, Sami Sarsa, Brent N.
Reeves, Paul Denny, James Prather, and Brett A.
Becker. Using Large Language Models to Enhance Pro-
gramming Error Messages. In SIGCSE , 2023.
[7] James Prather, Raymond Pettit, Kayla Holcomb Mc-
Murry, Alani L. Peters, John Homer, Nevan Simone,
and Maxine S. Cohen. On Novices' Interaction with
Compiler Error Messages: A Human Factors Approach.
InICER , 2017.
[8] Brett A. Becker. An Eective Approach to Enhancing
Compiler Error Messages. In SIGCSE , 2016.
[9] Tobias Kohn and Bill Z. Manaris. Tell Me What's
Wrong: A Python IDE with Error Messages. In
SIGCSE , 2020.
[10] Brett A. Becker. What Does Saying That `Program-
ming is Hard' Really Say, and About Whom? Commu-
nications of ACM , 64(8):27{29, 2021.
[11] Rishabh Singh, Sumit Gulwani, and Armando Solar-
Lezama. Automated Feedback Generation for Intro-
ductory Programming Assignments. In PLDI , 2013.

--- PAGE 8 ---
[12] Samim Mirhosseini, Austin Z. Henley, and Chris
Parnin. What is Your Biggest Pain Point? An In-
vestigation of CS Instructor Obstacles, Workarounds,
and Desires. In SIGCSE , 2023.
[13] Mikhail Mirzayanov. Codeforces. https://
codeforces.com/ .
[14] Sumit Gulwani, Ivan Radicek, and Florian Zuleger. Au-
tomated Clustering and Program Repair for Introduc-
tory Programming Assignments. In PLDI , 2018.
[15] Sahil Bhatia, Pushmeet Kohli, and Rishabh Singh.
Neuro-Symbolic Program Corrector for Introductory
Programming Assignments. In ICSE , 2018.
[16] Rahul Gupta, Aditya Kanade, and Shirish K. Shevade.
Deep Reinforcement Learning for Syntactic Error Re-
pair in Student Programs. In AAAI , 2019.
[17] Jialu Zhang, Jos e Cambronero, Sumit Gulwani, Vu Le,
Ruzica Piskac, Gustavo Soares, and Gust Verbruggen.
Repairing Bugs in Python Assignments Using Large
Language Models. CoRR , abs/2209.14876, 2022.
[18] Harshit Joshi, Jos e Pablo Cambronero S anchez, Sumit
Gulwani, Vu Le, Ivan Radicek, and Gust Verbruggen.
Repair is Nearly Generation: Multilingual Program Re-
pair with LLMs. In AAAI , 2023.
[19] Bj orn Hartmann, Daniel MacDougall, Joel Brandt, and
Scott R. Klemmer. What Would Other Programmers
Do: Suggesting Solutions to Error Messages. In CHI,
2010.
[20] Andrew Head, Elena L. Glassman, Gustavo Soares, Ryo
Suzuki, Lucas Figueredo, Loris D'Antoni, and Bj orn
Hartmann. Writing Reusable Code Feedback at Scale
with Mixed-Initiative Program Synthesis. In Learning
@ Scale , 2017.
[21] Darren Key, Wen-Ding Li, and Kevin Ellis. I Speak,
You Verify: Toward Trustworthy Neural Program Syn-
thesis. CoRR , abs/2210.00848, 2022.
[22] Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. Understanding Back-Translation at Scale. In
EMNLP , 2018.
[23] Yewen Pu, Kevin Ellis, Marta Kryven, Josh Tenen-
baum, and Armando Solar-Lezama. Program Synthesis
with Pragmatic Communication. In NeurIPS , 2020.
[24] Hiroaki Funayama, Tasuku Sato, Yuichiroh Matsub-
ayashi, Tomoya Mizumoto, Jun Suzuki, and Kentaro
Inui. Balancing Cost and Quality: An Exploration of
Human-in-the-Loop Frameworks for Automated Short
Answer Scoring. In AIED , 2022.
[25] Rui Zhi, Samiha Marwan, Yihuan Dong, Nicholas Ly-
tle, Thomas W. Price, and Tiany Barnes. Toward
Data-Driven Example Feedback for Novice Program-
ming. In EDM , 2019.
[26] Ahana Ghosh, Sebastian Tschiatschek, Sam Devlin,
and Adish Singla. Adaptive Scaolding in Block-
Based Programming via Synthesizing New Tasks as
Pop Quizzes. In AIED , 2022.[27] Ana s Tack and Chris Piech. The AI Teacher Test:
Measuring the Pedagogical Ability of Blender and
GPT-3 in Educational Dialogues. In EDM , 2023.
[28] OpenAI. Codex-Edit. https://beta.
openai.com/playground?mode=edit&model=
code-davinci-edit-001 , .
[29] OpenAI. Codex-Ccomplete. https://beta.
openai.com/playground?mode=complete&model=
code-davinci-002 , .
[30] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, and
Shin Hwei Tan. Automated Repair of Programs from
Large Language Models. In ICSE , 2022.
[31] Georg Brandl, Matth aus Chajdas, and Jean Abou-
Samra. Pygments. https://pygments.org/ .
[32] Rohan Bavishi, Harshit Joshi, Jos e Cambronero, Anna
Fariha, Sumit Gulwani, Vu Le, Ivan Radicek, and
Ashish Tiwari. Neurosymbolic Repair for Low-Code
Formula Languages. Proceedings ACM Programming
Languages , 6(OOPSLA2), 2022.
[33] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: A Method for Automatic Evaluation
of Machine Translation. In ACL, 2002.
[34] Tobias Kohn. The Error Behind The Message: Finding
the Cause of Error Messages in Python. In SIGCSE ,
2019.
[35] Ethan Caballero and Ilya Sutskever. Description2Code
Dataset. https://github.com/ethancaballero/
description2code , 2016.
[36] Yujia Li and et al. Competition-Level Code Generation
with AlphaCode. 2022.
[37] Matthijs J Warrens. Five Ways to Look at Cohen's
Kappa. Journal of Psychology & Psychotherapy , 5(4):
1, 2015.
[38] The Python Software Foundation. What's New In
Python 3.10. https://docs.python.org/3/whatsnew/
3.10.html , .
[39] The Python Software Foundation. What's New In
Python 3.11. https://docs.python.org/3/whatsnew/
3.11.html , .
[40] The Python Software Foundation. What's New
In Python 3.12. https://docs.python.org/3.12/
whatsnew/3.12.html , .
[41] William G Cochran. The 2 Test of Goodness of Fit.
The Annals of Mathematical Statistics , 1952.
[42] OpenAI. ChatGPT. https://openai.com/blog/
chatgpt , 2023.
[43] OpenAI. GPT-4 Technical Report. CoRR ,
abs/2303.08774, 2023.
