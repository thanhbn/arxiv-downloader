# 2303.18116.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2303.18116.pdf
# Kích thước tệp: 715868 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2303.18116v1 [cs.CL] 31 Mar 2023Lập Trình Cặp với Các Mô Hình Ngôn Ngữ Lớn
cho Lấy Mẫu và Ước Lượng Copulas
Jan Górecki
Khoa Tin học và Toán học, Đại học Silesian tại Opava,
Univerzitní náměstí 1934/3, 733 40 Karviná, Cộng hòa Séc
gorecki@opf.slu.cz
Tóm tắt
Mà không viết một dòng mã nào bởi con người, một ứng dụng mô phỏng Monte Carlo mẫu cho mô hình hóa phụ thuộc ngẫu nhiên với copulas được phát triển sử dụng một mô hình ngôn ngữ lớn (LLM) tiên tiến được tinh chỉnh cho hội thoại. Điều này bao gồm tương tác với ChatGPT bằng ngôn ngữ tự nhiên và sử dụng hình thức toán học, mà dưới sự giám sát cẩn thận của một chuyên gia con người, đã dẫn đến việc tạo ra mã hoạt động trong MATLAB, Python và R cho lấy mẫu từ một mô hình copula đã cho, đánh giá mật độ của mô hình, thực hiện ước lượng hợp lý tối đa, tối ưu hóa mã cho tính toán song song cho CPUs cũng như GPUs, và trực quan hóa các kết quả tính toán. Trái ngược với các nghiên cứu mới nổi khác đánh giá độ chính xác của LLMs như ChatGPT trên các nhiệm vụ từ một lĩnh vực được chọn, công việc này thay vào đó điều tra các cách để đạt được một giải pháp thành công của một nhiệm vụ thống kê tiêu chuẩn trong sự hợp tác của một chuyên gia con người và trí tuệ nhân tạo (AI). Đặc biệt, thông qua kỹ thuật prompt cẩn thận, chúng tôi tách biệt các giải pháp thành công được tạo ra bởi ChatGPT khỏi những giải pháp không thành công, dẫn đến một danh sách toàn diện các ưu và nhược điểm liên quan. Được chứng minh rằng nếu các cạm bẫy điển hình được tránh, chúng ta có thể hưởng lợi đáng kể từ việc hợp tác với một đối tác AI. Ví dụ, chúng tôi cho thấy rằng nếu ChatGPT không thể cung cấp một giải pháp đúng do thiếu hoặc kiến thức không chính xác, chuyên gia con người có thể cung cấp cho nó kiến thức đúng đắn, ví dụ, dưới dạng các định lý và công thức toán học, và làm cho nó áp dụng kiến thức đã có để cung cấp một giải pháp đúng đắn. Khả năng như vậy trình bày một cơ hội hấp dẫn để đạt được một giải pháp được lập trình ngay cả cho những người dùng có kiến thức khá hạn chế về các kỹ thuật lập trình.
Từ khóa: hợp tác con người-AI, các vấn đề không thể giải được về mặt phân tích, kỹ thuật prompt, ngôn ngữ tự nhiên
Phân loại MSC: 65C60, 68N19, 68T50

1 Giới thiệu
Tiến bộ gần đây trong việc giải quyết các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) sử dụng các mô hình ngôn ngữ lớn (LLMs) đã dẫn đến các mô hình có chất lượng tạo văn bản và hiểu ngữ cảnh chưa từng thấy trước đây. Các mô hình này, chẳng hạn như BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) và GPT-3 (Brown et al.,

--- TRANG 2 ---
2020), có khả năng thực hiện một loạt các nhiệm vụ NLP, bao gồm phân loại văn bản, trả lời câu hỏi, tóm tắt văn bản, và nhiều hơn nữa. Với hơn 100 triệu người dùng đăng ký trong hai tháng sau khi phát hành để thử nghiệm công khai thông qua một cổng web¹, ChatGPT² là LLM hiện tại cộng hưởng nhiều nhất trong cộng đồng trí tuệ nhân tạo (AI). AI hội thoại này được tinh chỉnh từ chuỗi GPT-3.5 với học tăng cường từ phản hồi của con người (Christiano et al., 2017; Stiennon et al., 2020), sử dụng gần như cùng các phương pháp như InstructGPT (Ouyang et al., 2022), nhưng với những khác biệt nhỏ trong thiết lập thu thập dữ liệu. Vào tháng 3 năm 2023, nhà phát triển của ChatGPT đã phát hành một phiên bản mới của GPT-3.5, GPT-4 (OpenAI, 2023). Tại thời điểm viết bài báo này, GPT-4 không có sẵn miễn phí, vì vậy kết quả của chúng tôi không bao gồm các đầu ra của nó. Tuy nhiên, vì một báo cáo kỹ thuật cho một số thuộc tính của mô hình có sẵn, chúng tôi thêm thông tin liên quan khi thích hợp.

Một kết quả cụ thể của việc tinh chỉnh ChatGPT là nó có thể tạo ra mã tương ứng trong nhiều ngôn ngữ lập trình được đưa ra một mô tả nhiệm vụ bằng ngôn ngữ tự nhiên. Điều này có thể được khai thác trong lập trình cặp (Williams, 2001) với ChatGPT, mà sau đó mang lại một số lợi ích, bao gồm:

•Tăng năng suất: ChatGPT có thể giúp tự động hóa các nhiệm vụ lập trình lặp đi lặp lại và tốn thời gian, giải phóng thời gian cho các nhà phát triển tập trung vào giải quyết vấn đề cấp cao hơn và công việc sáng tạo. Trung bình tiết kiệm thời gian 55% đã được báo cáo cho nhiệm vụ viết một máy chủ HTTP trong JavaScript trong nghiên cứu được thực hiện bởi nhóm GitHub Next³ cho GitHub Copilot⁴. Cái sau là một công cụ gợi ý mã khác tạo ra các đoạn mã dựa trên mô tả ngôn ngữ tự nhiên, được cung cấp bởi một LLM tương tự như ChatGPT, Codex (Chen et al., 2021).

•Cải thiện chất lượng mã: Lập trình cặp với ChatGPT có thể giúp xác định lỗi và bugs trong mã trước khi chúng trở thành vấn đề lớn hơn. ChatGPT cũng có thể đề xuất cải thiện cho kiến trúc và thiết kế mã.

•Chia sẻ kiến thức: ChatGPT có thể giúp các nhà phát triển ít kinh nghiệm học hỏi từ các thành viên nhóm có kinh nghiệm hơn bằng cách cung cấp đề xuất và hướng dẫn.

•Tài liệu mã tốt hơn: ChatGPT có thể giúp tạo ra tài liệu mã chi tiết và chính xác hơn bằng cách tạo ra các bình luận và chú thích dựa trên mã.

•Khả năng tiếp cận: ChatGPT có thể làm cho lập trình dễ tiếp cận hơn với những người có thể không có nền tảng lập trình, cho phép họ hợp tác với các nhà phát triển và đóng góp vào các dự án một cách có ý nghĩa. Ví dụ, đã phát triển một lý thuyết mới đòi hỏi tính toán, có thể hấp dẫn và hiệu quả về thời gian cho các nhà nghiên cứu sử dụng các công cụ như

¹https://www.demandsage.com/chatgpt-statistics/
²https://openai.com/blog/chatgpt/
³https://github.blog/2022-09-07-...
⁴https://github.com/features/copilot

--- TRANG 3 ---
ChatGPT để triển khai giải pháp mà không cần phải liên quan đến nhân lực thường đắt đỏ trong kỹ thuật phần mềm.

Hiện tại, có vẻ như có một số nghiên cứu đánh giá độ chính xác của LLMs như ChatGPT dựa trên một tập hợp các nhiệm vụ từ một lĩnh vực cụ thể. Ví dụ, nhiều khía cạnh của kỹ năng toán học của ChatGPT được đánh giá trong Frieder et al. (2023), với quan sát chính là nó chưa sẵn sàng để cung cấp các bằng chứng hoặc tính toán chất lượng cao một cách nhất quán. Trong Katz et al. (2023), một phiên bản sơ bộ của GPT-4 được đánh giá thực nghiệm so với các thế hệ GPT trước đó trên toàn bộ Kỳ thi Luật sư Thống nhất (UBE)⁵, và được báo cáo rằng GPT-4 vượt trội hơn đáng kể cả những người làm bài kiểm tra và các mô hình trước đó, cho thấy sự gia tăng 26% so với mô hình dựa trên GPT-3.5 và đánh bại con người trong năm trong số bảy lĩnh vực chủ đề. Trong Bang et al. (2023), một đánh giá toàn diện về ChatGPT sử dụng 21 bộ dữ liệu bao gồm 8 nhiệm vụ NLP khác nhau như tóm tắt, phân tích tình cảm và trả lời câu hỏi được trình bày. Các tác giả thấy rằng, một mặt, ChatGPT vượt trội hơn LLMs với cái gọi là học zero-shot (Brown et al., 2020) trên hầu hết các nhiệm vụ và thậm chí vượt trội hơn các mô hình được tinh chỉnh trên một số nhiệm vụ. Mặt khác, họ kết luận rằng ChatGPT gặp phải các vấn đề ảo giác như các LLMs khác và nó tạo ra nhiều ảo giác bên ngoài hơn từ bộ nhớ tham số của nó vì nó không có quyền truy cập vào một cơ sở kiến thức bên ngoài. Thú vị là, các tác giả quan sát trong một số nhiệm vụ rằng khả năng tương tác với ChatGPT cho phép hợp tác con người với LLM cơ bản để cải thiện hiệu suất của nó.

Quan sát sau đây là trọng tâm chính của công việc này. Thay vì đánh giá độ chính xác của LLMs, chúng tôi điều tra các cách để hưởng lợi từ lập trình cặp với một đối tác AI để đạt được một giải pháp thành công của một nhiệm vụ đòi hỏi tính toán chuyên sâu. Mặc dù có nhiều thành tích ấn tượng gần đây của các LLMs tiên tiến, việc đạt được một mã chức năng còn xa mới đơn giản; một trong nhiều nỗ lực không thành công được báo cáo tại freeCodeCamp⁶. Quan trọng là, các nỗ lực thành công cũng đang xuất hiện. Trong Maddigan và Susnjak (2023), các tác giả báo cáo rằng LLMs cùng với các prompts được đề xuất có thể cung cấp một cách tiếp cận đáng tin cậy để tạo ra các trực quan hóa từ các truy vấn ngôn ngữ tự nhiên, ngay cả khi các truy vấn được chỉ định sai và không đầy đủ một cách cao. Tuy nhiên, trong nhiều lĩnh vực, bao gồm các giải pháp chuyên sâu về tính toán của các vấn đề thống kê không thể giải được về mặt phân tích, một nghiên cứu chứng minh lợi ích từ lập trình cặp với một đối tác AI đang thiếu.

Công việc này lấp đầy khoảng trống này và xem xét các ứng dụng liên quan đến copulas (Nelsen, 2006; Joe, 2014) như các mô hình cho sự phụ thuộc ngẫu nhiên giữa các biến ngẫu nhiên. Các ứng dụng này được biết đến với tính không thể giải quyết về mặt phân tích của chúng, do đó, phương pháp Monte Carlo (MC) được sử dụng rộng rãi nhất để tính toán các đại lượng quan tâm liên quan. Vì phương pháp MC thường liên quan đến nỗ lực tính toán lớn, việc tiến hành một nghiên cứu MC đòi hỏi một người phải triển khai tất cả các khái niệm cơ bản. Chúng tôi chứng minh cách làm cho ChatGPT tạo ra một triển khai hoạt động cho một ứng dụng như vậy bằng cách tương tác với nó bằng ngôn ngữ tự nhiên và sử dụng hình thức toán học. Để minh họa đầy đủ khả năng mã hóa của ChatGPT, vai trò con người được đẩy đến cực độ, và tất cả các nhiệm vụ đã đề cập được triển khai mà không có một dòng mã nào được viết bởi con người hoặc điều chỉnh mã được tạo ra theo bất kỳ cách nào. Điều quan trọng cần nhấn mạnh là ngay cả khi ứng dụng đang xem xét liên quan đến một lĩnh vực cụ thể của xác suất và thống kê, các quan sát của chúng tôi áp dụng trong phạm vi rộng hơn vì các nhiệm vụ chúng tôi xem xét (lấy mẫu từ một mô hình (copula) đã cho, đánh giá mật độ của mô hình, thực hiện ước lượng hợp lý tối đa, tối ưu hóa mã cho tính toán song song và trực quan hóa các kết quả tính toán) thường xuất hiện trong nhiều ứng dụng thống kê. Ngoài ra, chúng tôi không chỉ trình bày một cách để đạt được một giải pháp thành công cho một nhiệm vụ đã cho. Hầu hết các giải pháp thành công được bổ sung với các ví dụ chứng minh những điều chỉnh nào của prompts của chúng tôi cho ChatGPT biến các giải pháp không thành công thành các giải pháp thành công. Điều này dẫn đến một danh sách toàn diện các ưu và nhược điểm liên quan, cho thấy rằng nếu các cạm bẫy điển hình được tránh, chúng ta có thể hưởng lợi đáng kể từ sự hợp tác với LLMs như ChatGPT. Đặc biệt, chúng tôi chứng minh rằng nếu ChatGPT không thể cung cấp một giải pháp đúng do hạn chế trong kiến thức của nó, có thể cung cấp cho nó kiến thức cần thiết, và làm cho ChatGPT áp dụng kiến thức này để cung cấp một giải pháp đúng. Có tất cả các nhiệm vụ con của nhiệm vụ chính được mã hóa thành công trong một ngôn ngữ lập trình cụ thể, chúng tôi cũng chứng minh cách khai thác đầy đủ một số khả năng ấn tượng của ChatGPT. Ví dụ, bằng một prompt cấp cao đơn giản như "Bây giờ hãy mã hóa nó trong Python.", ChatGPT chuyển đổi đúng mã từ ngôn ngữ lập trình này sang ngôn ngữ lập trình khác trong vài giây. Ngoài ra, nếu một lỗi trong mã được tạo ra bởi ChatGPT gặp phải trong quá trình thực thi, được chứng minh rằng ChatGPT không chỉ có thể xác định lỗi, mà thậm chí ngay lập tức tạo ra một phiên bản đã được sửa sau khi thông báo lỗi được sao chép-dán vào giao diện web của ChatGPT.

Bài báo được tổ chức như sau. Phần 2 trình bày các nhiệm vụ chúng tôi xem xét và thiết lập cách chúng tôi tương tác với ChatGPT. Phần 3 trình bày sự phát triển của nhiệm vụ thông qua lập trình cặp với ChatGPT. Phần 4 tóm tắt các ưu và nhược điểm quan sát được trong quá trình phát triển nhiệm vụ, bao gồm một cuộc thảo luận về cách giảm thiểu cái sau, và Phần 5 kết luận.

2 Phương pháp luận
2.1 Nhiệm vụ

Cho (xij) ∈ Rn×d là một mẫu có kích thước n từ một vector ngẫu nhiên (X1,...,Xd) ∼ F, trong đó F là một hàm phân phối chung với các biên đơn biến liên tục F1,...,Fd và copula (Sklar, 1959) C được cho ngầm ẩn bởi F(x1,...,xd) = C(F1(x1),...,Fd(xd)) cho x1,...,xd ∈ R. Một công thức rõ ràng cho C là C(u1,...,ud) = F(F1^(-1)(u1),...,Fd^(-1)(ud)), u1,...,ud ∈ [0,1]. Một ứng dụng điển hình liên quan đến phương pháp MC và copulas giả định rằng C không xác định nhưng thuộc về một họ tham số của các mô hình copula {Cθ : θ ∈ Θ}, trong đó Θ là một tập con mở của Rp cho một số nguyên p ≥ 1. Các bước sau đó được xem xét:

1. Ước lượng tham số thực nhưng không xác định θ0 ∈ Θ của Cθ0 = C, ví dụ, sử dụng

--- TRANG 5 ---
ước lượng hợp lý tối đa giả (ML)
θ̂ = argmax θ∈Θ Σi=1^n log cθ(û i1,...,û id), (1)
trong đó cθ là mật độ của Cθ, θ ∈ Θ, û ij = F̂j(xij) và F̂j là một ước lượng của Fj cho i ∈ {1,...,n}, j ∈ {1,....d}. Đối với một số họ copula, ví dụ, đối với các họ Archimedean, việc đánh giá cθ cho d lớn đã là một thử thách; xem Hofert et al. (2013). Đối với các cấu trúc cặp-copula, thử thách chính nằm ở việc tính toán (1), xem Hobæk Haff (2013) hoặc Schellhase và Spanhel (2018), thường được thực hiện bằng các phương pháp số như gradient descent.

2. Tạo ra một mẫu (vij) ∈ [0,1]N×d từ Cθ̂, thường với N ≫ n. Đối với một số họ copula phổ biến, nhiệm vụ này cũng là thử thách, và liên quan đến các kỹ thuật khác nhau để lấy mẫu hiệu quả từ Cθ̂; xem, ví dụ, Hofert (2010); Hofert et al. (2018) cho các kỹ thuật lấy mẫu liên quan đến copulas Archimedean và Archimax và các phần mở rộng phân cấp của chúng.

3. Tính toán một mẫu từ một phân phối không thể giải quyết về mặt phân tích, ví dụ, từ phân phối của X̄ = 1/d Σj=1^d Xj. So với hai điểm trước đó, đây là một nhiệm vụ tầm thường, chúng ta chỉ cần đánh giá x̄i = 1/d Σj=1^d F̂j^(-1)(vij), i ∈ {1,...,N}.

4. Tính toán đại lượng mong muốn dựa trên x̄1,...,x̄N. Ví dụ, nếu X1,...,Xd đại diện cho các thay đổi yếu tố rủi ro, một phân vị của hàm phân phối của X̄ đại diện cho Value-at-Risk VaRα, thường được sử dụng trong quản lý rủi ro định lượng; xem McNeil et al. (2015). Xấp xỉ VaR(X1+···+Xd) cũng tầm thường vì nó chỉ liên quan đến việc tính toán các thống kê thứ tự x̄(1) ≤ ··· ≤ x̄(N) và sau đó chọn ra x̄(⌈αN⌉), trong đó α ∈ [0,1] là một mức độ tin cậy mong muốn. Trong cùng lĩnh vực, đại lượng được biết đến như expected shortfall liên quan đến việc tính toán trung bình của các giá trị x̄i lớn hơn VaRα, vì vậy một lần nữa là một nhiệm vụ tính toán tầm thường.

Để thấy rõ rằng mã được tạo ra bởi ChatGPT thực sự hoạt động như mong đợi mà không cần một lập trình viên có kinh nghiệm, chúng tôi lệch một chút khỏi dàn ý trên, trong khi vẫn giữ các nhiệm vụ không tầm thường, tức là, lấy mẫu và ước lượng. Do đó chúng tôi prompt ChatGPT để tạo ra mã thực hiện như sau:

1. Tạo ra một mẫu từ Cθ0, trong đó θ0 ∈ R.

2. Dựa trên mẫu này, tính toán ước lượng ML θ̂ của tham số thực θ0 sử dụng (1).

Sau đó, chúng tôi lặp lại hai bước này cho một số giá trị của θ0, ví dụ, được phân bố tuyến tính trên một khoảng thuận tiện nào đó của R. Nếu đồ thị của các cặp (θ0, θ̂) gần với biểu đồ đồng nhất (θ−θ), thì người ta có bằng chứng mạnh mẽ về một quy trình lấy mẫu và ước lượng thống kê đúng đắn. Cuối cùng, để cho phép mở rộng quy mô, chúng tôi yêu cầu ChatGPT tối ưu hóa mã được tạo ra cho tính toán song song trên CPUs cũng như trên GPUs.

--- TRANG 6 ---
2.2 Giao thức giao tiếp

Khi tương tác với ChatGPT, chúng tôi sử dụng cổng web được cung cấp bởi nhóm phát triển của nó⁷. Ngoài ra, chúng tôi thiết lập và tuân theo giao thức giao tiếp này:

1. Chúng tôi prompt ChatGPT để tạo ra mã giải quyết một nhiệm vụ được chọn bằng ngôn ngữ tự nhiên và sử dụng hình thức toán học, nghĩa là, chúng tôi chỉ định nhiệm vụ bằng văn bản thuần túy và không sử dụng bất kỳ ngôn ngữ hình thức cụ thể nào. Đối với các công thức, chúng tôi sử dụng văn bản thuần túy như psi(t) = (1 + t)^(-1/theta).

2. Nếu giải pháp được tạo ra bởi ChatGPT sai, nghĩa là, không giải quyết nhiệm vụ đã cho, chúng tôi thông báo vấn đề cho ChatGPT, và yêu cầu nó cung cấp cho chúng tôi một giải pháp đã được sửa.

3. Nếu giải pháp đã được sửa này vẫn sai, chúng tôi cung cấp cho ChatGPT kiến thức cần thiết để hoàn thành nhiệm vụ thành công, ví dụ, chúng tôi cung cấp cho nó các định lý và công thức bằng văn bản thuần túy. Để có ví dụ, xem prompt thứ ba trong Phần 3.4.

Theo cách này, chúng tôi mô phỏng một sự tương tác giữa hai con người, ví dụ, một khách hàng gửi qua email một nhiệm vụ cho một kỹ sư phần mềm, và chúng tôi đóng vai trò khách hàng và ChatGPT vai trò kỹ sư phần mềm. Vì điển hình là khách hàng không nhận thức được tất cả các chi tiết cần thiết để giải quyết nhiệm vụ tại đầu của sự tương tác, một giao thức giao tiếp như vậy có thể được quan sát thường xuyên trong thực tế. Khách hàng bắt đầu bằng cách cung cấp các tính năng (chủ quan) quan trọng nhất của vấn đề để giảm thiểu nỗ lực ban đầu của cô ấy/anh ấy, và sau đó, nếu cần thiết, cô ấy/anh ấy thêm chi tiết để có được một giải pháp chính xác hơn. Quan trọng là, giao thức giao tiếp này đã dẫn đến việc hoàn thành thành công các nhiệm vụ đã đề cập, được báo cáo trong Phần 3.

Liên quan đến việc truyền ChatGPT kiến thức cần thiết, điều quan trọng cần nhận ra là ChatGPT không có bất kỳ bộ nhớ nào để nhớ cuộc hội thoại trước đó với một người dùng. Thay vào đó, thủ thuật để ChatGPT có vẻ như nhớ các cuộc hội thoại trước đó là cung cấp cho nó toàn bộ lịch sử hội thoại như một prompt duy nhất. Điều này có nghĩa là khi một người dùng gửi tin nhắn, lịch sử hội thoại trước đó được thêm vào prompt và sau đó được cung cấp cho ChatGPT. Kỹ thuật kỹ thuật prompt này được sử dụng rộng rãi trong các hệ thống AI hội thoại để cải thiện khả năng của mô hình để tạo ra các phản hồi nhất quán và phù hợp về mặt ngữ cảnh. Tuy nhiên, nó chỉ là một thủ thuật được sử dụng để tạo ra ảo giác về bộ nhớ trong ChatGPT.

Nếu cuộc hội thoại trước đó quá dài (lớn hơn 4096 tokens⁸, trong đó một token khoảng 3/4 của một từ tiếng Anh⁹), nó có thể không vừa hoàn toàn trong cửa sổ ngữ cảnh mà ChatGPT sử dụng để tạo ra phản hồi. Trong những trường hợp như vậy, mô hình có thể chỉ có quyền truy cập vào một cái nhìn một phần của lịch sử hội thoại, điều này có thể dẫn đến mô hình có vẻ như nó đã quên một số phần của cuộc hội thoại. Để giảm thiểu vấn đề này, các nhà thiết kế AI hội thoại thường sử dụng các kỹ thuật như cắt ngắn hoặc tóm tắt lịch sử hội thoại để đảm bảo rằng nó vừa trong cửa sổ ngữ cảnh. Cách chúng tôi giải quyết vấn đề này trong nhiệm vụ ví dụ của chúng tôi là giới thiệu lại các phần mà chúng tôi đã đề cập đến ChatGPT. Ví dụ, khi chuyển đổi mã từ Python (Phụ lục A) sang R (Phụ lục B), chúng tôi đầu tiên sao chép-dán mã Python vào giao diện web của ChatGPT và sau đó yêu cầu nó chuyển đổi nó sang R. Mà không có hạn chế kỹ thuật này trong tâm trí, không có khả năng nhận được một câu trả lời/giải pháp đúng nếu chúng tôi đề cập đến phần chuyển đổi không vừa trong cửa sổ ngữ cảnh. Cuối cùng, lưu ý rằng theo báo cáo kỹ thuật của nó, GPT-4 sử dụng cửa sổ ngữ cảnh lớn hơn 8 lần so với ChatGPT, vì vậy nó có thể chứa khoảng 25.000 từ. Điều này cho thấy rằng hạn chế được áp đặt bởi độ dài cửa sổ ngữ cảnh sẽ trở nên ít quan tâm hơn.

2.3 Họ copula

Để làm cho nhiệm vụ ví dụ được chỉ định trong Phần 2.1 cụ thể, chúng tôi chọn họ tham số {Cθ : θ ∈ Θ} là họ phổ biến của các copulas Clayton (Clayton, 1978), được cho bởi

Cθ(u1,u2) = (max(u1^(-θ) + u2^(-θ) - 1, 0))^(-1/θ), (2)

trong đó u1, u2 ∈ [0,1] và -1 ≤ θ < ∞, θ ≠ 0. Họ copulas này được sử dụng trong nhiều ứng dụng khác nhau. Để đề cập đến một số ứng dụng gần đây, ví dụ, Huang et al. (2022) sử dụng nó để phân tích mối tương quan giữa chuỗi dư và một chuỗi tốc độ gió của một mạng thần kinh bộ nhớ ngắn hạn dài. Đặc biệt, việc ước lượng hợp lý tối đa của tham số copula được sử dụng, tức là, quy trình mà ChatGPT triển khai ở đây trong Phần 3.3. Trong nghiên cứu mô phỏng ở Michimae và Emura (2022), nơi các mô hình rủi ro cạnh tranh dựa trên copula cho các thời gian thất bại tiềm ẩn được đề xuất, các tác giả sử dụng lấy mẫu từ copulas Clayton, tức là, quy trình mà ChatGPT triển khai ở đây trong Phần 3.4.

Để đơn giản, cũng như thực tế là các mô hình với θ < 0, nghĩa là, những mô hình với sự phụ thuộc tiêu cực, hiếm khi được sử dụng trong thực tế, chúng tôi hạn chế đến θ ∈ (0,∞), điều này cho phép một người viết lại (2) thành

Cθ(u1,u2) = (u1^(-θ) + u2^(-θ) - 1)^(-1/θ). (3)

Lý do kỹ thuật để chọn họ này là dạng phân tích đơn giản của nó, điều này làm cho việc theo dõi tất cả các công thức chúng tôi yêu cầu và nhận được từ ChatGPT dễ dàng hơn cho người đọc, ví dụ, hàm mật độ xác suất (PDF). Một lý do khác là kiến thức tương đối hạn chế của ChatGPT về họ này. Ngược lại, ví dụ, đối với họ phổ biến nhất của copulas Gaussian, ChatGPT không thể tạo ra một thuật toán lấy mẫu mà không được cung cấp một số lý thuyết cần thiết. Cái sau mô phỏng một tình huống thực tế khi ChatGPT đối mặt với một lý thuyết/khái niệm mới, ví dụ, một cái được phát triển gần đây bởi người dùng. Tuy nhiên, chúng tôi muốn khuyến khích người đọc thử nghiệm với bất kỳ họ quan tâm nào hoặc thậm chí với một nhiệm vụ khác với ví dụ của chúng tôi.

⁷chat.openai.com
⁸https://platform.openai.com/docs/models/gpt-3-5
⁹Tổng số tokens trong một đoạn văn bản có thể được đo chính xác bởi https://platform.openai.com/tokenizer

--- TRANG 8 ---
3 Lập trình cặp với ChatGPT

Lập trình cặp (Williams, 2001) là một kỹ thuật phát triển phần mềm trong đó hai lập trình viên làm việc cùng nhau tại một máy trạm. Một người, người lái xe, viết mã trong khi người kia, người điều hướng xem xét từng dòng mã khi nó được gõ vào và xem xét hướng "chiến lược" của công việc. Trong những gì tiếp theo, chúng tôi, phần con người của cặp, đặt mình vào vai trò của người điều hướng, đưa ra các nhiệm vụ cụ thể cho người lái xe, ChatGPT, đại diện cho phần AI của cặp.

Chúng tôi bắt đầu prompt ChatGPT với các nhiệm vụ đòi hỏi ít dòng mã hơn để không làm choáng ngợp người đọc với số lượng kết quả, trong khi vẫn chứng minh càng nhiều khả năng của ChatGPT càng tốt. Sau đó, chúng tôi dần dần chuyển sang các nhiệm vụ lớn hơn cho đến khi chúng tôi có tất cả chúng được triển khai. Các prompts của chúng tôi là ở sans serif. Các phản hồi của ChatGPT là verbatim, nếu phản hồi là mã, hoặc ở in nghiêng, nếu không. Trong lần xuất hiện đầu tiên của nó, chúng tôi chỉ ra điều này một cách rõ ràng. Lưu ý rằng vì ChatGPT là một LLM khá lỗi mồm, chúng tôi chủ yếu giới hạn độ dài của các phản hồi của nó, nếu không chúng tôi sẽ bị choáng ngợp với các câu trả lời có chi tiết không cần thiết. Vì cùng lý do, khi yêu cầu ChatGPT về mã, chúng tôi chủ yếu bỏ qua các bình luận của nó về mã được tạo ra. Cũng lưu ý rằng khi bình luận về các phản hồi của ChatGPT, chúng tôi nói về nó như thể nó là con người, ví dụ, nó "hiểu", "biết" hoặc "nhận thức" về điều gì đó, điều này nên được người đọc diễn giải theo nghĩa là ChatGPT đã tạo ra một phản hồi (thường rất tốt) bắt chước một phản ứng con người tương ứng. Vì các phản hồi của ChatGPT theo mặc định là không xác định, tức là, đưa cho nó cùng một prompt một lần nữa, phản hồi có thể hơi khác. Để tính đến tính năng này, chúng tôi tạo lại phản hồi cho mỗi prompt của chúng tôi ba lần, và nếu những phản hồi này khác nhau về mặt thực tế với nhau, chúng tôi chỉ ra điều đó một cách phù hợp. Cuối cùng, lưu ý rằng toàn bộ sự tương tác được tiến hành trong một cửa sổ chat. Một khi chúng tôi quan sát thấy rằng ChatGPT bắt đầu quên ngữ cảnh trước đó do những lý do được mô tả trong Phần 2.2, chúng tôi giới thiệu lại nó như chúng tôi mô tả trong cùng phần.

Trong phần còn lại của phần này, chúng tôi đầu tiên điều tra kiến thức của ChatGPT về chủ đề đang xem xét, và sau đó chúng tôi prompt nó để tạo ra mã cho việc đánh giá mật độ của copula Clayton, cho ước lượng ML của tham số của nó, cho lấy mẫu từ copula, cho tạo ra một trực quan hóa của phương pháp Monte Carlo ví dụ, và cho việc tối ưu hóa mã cho các tính toán song song.

3.1 Khởi động

(Prompt:) Copula bivariate Clayton là gì, tại sao nó thú vị, dạng phân tích của nó là gì và nó có phải là một copula Archimedean không? Trả lời những câu hỏi này trong tối đa 100 từ.

--- TRANG 9 ---
(Phản hồi:) Copula Clayton là một hàm copula bivariate được sử dụng rộng rãi trong thống kê và tài chính để mô hình hóa sự phụ thuộc giữa các biến. Nó thú vị vì nó có một dạng đơn giản và có thể nắm bắt sự phụ thuộc tích cực giữa các biến trong khi cho phép các đuôi bất đối xứng. Dạng phân tích của copula Clayton là C(u, v; theta) = (max(u^(-theta) + v^(-theta) - 1, 0))^(-1/theta), trong đó theta là tham số copula Clayton. Copula Clayton là một copula Archimedean, có nghĩa là nó có thể được biểu diễn như hàm tạo của một lớp phân phối đối xứng cụ thể.

Chúng ta thấy rằng ChatGPT có thể tiết kiệm thời gian của chúng ta bằng cách tóm tắt nhanh chóng và ngắn gọn các sự kiện cơ bản về chủ đề quan tâm của chúng ta. Chúng ta cũng có thể giới hạn kích thước của câu trả lời, điều này được thỏa mãn trong câu trả lời dài 91 từ này. Thông tin về sự phụ thuộc tích cực có lẽ xuất phát từ những gì chúng ta đã nói trước đây: các mô hình phụ thuộc tiêu cực hiếm khi được sử dụng trong thực tế, điều này có lẽ được phản ánh trong dữ liệu huấn luyện của ChatGPT. Tuy nhiên, một số chi tiết của câu trả lời có thể được thảo luận. Ở dòng 3 và 4, "biến ngẫu nhiên" thay vì chỉ "biến" sẽ chính xác hơn. Từ câu cuối cùng, có thể suy ra rằng một copula Archimedean có thể được biểu diễn như hàm tạo cho một số phân phối đối xứng. Điều này ít nhất là gây nhầm lẫn vì copulas Archimedean thực chất là một lớp copulas cụ thể thừa nhận một dạng hàm số nhất định dựa trên các hàm tạo cái gọi là. Cuối cùng, các phân phối đối xứng có ý nghĩa chính xác của chúng: một phân phối như vậy không thay đổi khi, trong trường hợp liên tục, hàm mật độ xác suất của nó được phản chiếu xung quanh một đường thẳng đứng tại một giá trị nào đó của biến ngẫu nhiên được biểu diễn bởi phân phối. Trong khi copulas Archimedean sở hữu một loại đối xứng xuất phát từ tính khả hoán của chúng, chúng không thuộc về các phân phối đối xứng.

Để điều tra giới hạn của kiến thức của ChatGPT, hãy cho chúng tôi prompt nó với hai câu hỏi tiếp theo. Theo phản hồi trước đó, chúng ta có thể suy đoán rằng nó có kiến thức hạn chế về các mô hình Clayton với sự phụ thuộc tiêu cực.

Nếu tham số copula Clayton là âm, ví dụ, theta = -1/2, copula Clayton tương ứng có phải là một mô hình cho sự phụ thuộc tích cực giữa các biến không? Trả lời bằng một từ.

Không.

Chỉnh sửa: Có.

--- TRANG 10 ---
Có.

Chúng tôi prompt ChatGPT trả lời cùng một câu hỏi ba lần và nhận được các câu trả lời mâu thuẫn. Câu trả lời đầu tiên là đúng, tuy nhiên, sau khi hỏi lại, ChatGPT đã thay đổi quyết định. Trước khi bình luận về điều đó, hãy thử một lần nữa, với một khái niệm phức tạp hơn một chút.

Nếu tham số copula Clayton là dương, ví dụ, theta = 1, copula sống sót của copula Clayton tương ứng có phải là một mô hình với sự phụ thuộc đuôi dưới hay với sự phụ thuộc đuôi trên?

Khi tham số copula Clayton là dương, tức là, theta > 0, copula sống sót của copula Clayton tương ứng là một mô hình với sự phụ thuộc đuôi dưới.

Nếu (U1,U2) ∼ C, thì copula sống sót của C là phân phối của (1 - U1, 1 - U2), và do đó các thuộc tính của đuôi dưới của C là các thuộc tính của đuôi trên của copula sống sót. Do đó, chúng ta nhận được một câu trả lời không chính xác. Sau khi hỏi lại, chúng ta nhận được phản hồi này.

Đối với copula Clayton, nếu tham số theta là dương, copula sống sót sẽ có sự phụ thuộc đuôi trên. Điều này là vì copula Clayton thể hiện sự phụ thuộc đuôi dưới tích cực, có nghĩa là khi một biến có giá trị rất thấp, biến kia có xu hướng có giá trị thấp cũng vậy. Điều này ngụ ý rằng khi một biến có thời gian sống sót rất cao, biến kia cũng có xu hướng có thời gian sống sót cao, dẫn đến sự phụ thuộc đuôi trên trong copula sống sót.

Một lần nữa, chúng ta nhận được các câu trả lời mâu thuẫn. Dựa trên quan sát này, người đọc có thể đặt ra câu hỏi sau.

Bạn có đồng ý với tuyên bố sau đây không? ChatGPT là một mô hình ngôn ngữ lớn, và do đó, nếu chúng ta đơn giản hóa nó một cách mạnh mẽ, về cơ bản nó suy luận về từ nào sẽ xuất hiện tiếp theo. Tuy nhiên, nó không suy luận về các khái niệm cơ bản.

Là một mô hình ngôn ngữ AI, ChatGPT thực sự được thiết kế để đưa ra dự đoán về từ tiếp theo dựa trên ngữ cảnh và các mẫu trong văn bản đầu vào. Tuy nhiên, nó cũng có khả năng nắm bắt và suy luận các khái niệm cơ bản từ văn bản, ở một mức độ nhất định. Mặc dù đúng là ChatGPT không có khả năng hiểu các khái niệm theo cách giống như con người, nó vẫn có thể nhận ra các mẫu và mối quan hệ giữa các từ và cụm từ, và sử dụng thông tin này để đưa ra suy luận và tạo ra các phản hồi phản ánh một mức độ hiểu biết nhất định về các khái niệm cơ bản.

--- TRANG 11 ---
Phản hồi này xác nhận những gì chúng ta đã thấy cho đến nay, do đó, bất kỳ người dùng nào cũng nên tính đến những hạn chế này với sự nghiêm túc tột độ và cực kỳ cẩn thận khi yêu cầu ChatGPT về một số lập luận. Các ví dụ trên cũng minh họa rõ rằng phiên bản hiện tại của ChatGPT chắc chắn không phải là một công cụ phù hợp cho lập luận, điều này cũng được quan sát bởi Frieder et al. (2023) và Bang et al. (2023). Tuy nhiên, điều này không có nghĩa là nó không thể phục vụ như một đối tác AI hữu ích cho lập trình cặp.

3.2 Mật độ

Có thể dễ dàng chỉ ra rằng mật độ cθ của Cθ là

cθ(u,v) = ∂²Cθ(u,v)/∂u∂v = (θ+1)(uv)^(-(θ+1))(u^(-θ) + v^(-θ) - 1)^(-1/θ-2) (4)

cho θ > 0. Trước khi chúng ta yêu cầu ChatGPT tạo ra mã đánh giá mật độ copula Clayton, sẽ tốt nếu hỏi về một công thức thuần túy.

Dạng phân tích của hàm mật độ xác suất của copula bivariate Clayton là gì? Xuất ra công thức sử dụng LaTeX.

cθ(u,v) = ∂²Cθ(u,v)/∂u∂v = (θ+1)/θ (u^(-θ-1)v^(-θ-1)(u^(-θ) + v^(-θ) - 1)^(-θ+2)/(θ+1))

Trong số nhiều phản hồi khác nhau về mặt thực tế, chúng tôi trình bày cái này để minh họa việc tin tưởng rằng ChatGPT biết hoặc có thể suy ra công thức đúng có thể nguy hiểm như thế nào. Ngay cả khi nó trông khá giống với (4), đây không phải là mật độ của Cθ.

Công thức này không chính xác. Dạng phân tích của hàm mật độ xác suất của copula bivariate Clayton là gì? Xuất ra công thức sử dụng LaTeX.

c(u,v) = [(u^(-θ) + v^(-θ) - 1)^(-1/θ-2)] · (u^(-θ)v^(-θ))^(-1/θ-1)

--- TRANG 12 ---
Một công thức khác khá giống với (4), nhưng cái này cũng không chính xác. Như đã đề cập trước đây, ChatGPT không phải là một lựa chọn tốt khi nói đến lập luận. Vì vậy, để có được công thức đúng, một số công cụ tượng trưng chắc chắn được ưu tiên. Tuy nhiên, lưu ý rằng các plugin ChatGPT¹⁰ gần đây đã được công bố, và đặc biệt Code interpreter, đây là một mô hình ChatGPT thử nghiệm có thể sử dụng Python, xử lý tải lên và tải xuống, cho phép các tính toán tượng trưng. Ngay cả khi chưa có sẵn miễn phí, điều này cũng có thể là một cách có thể để giảm thiểu vấn đề, trực tiếp trong môi trường ChatGPT.

Theo giao thức giao tiếp của chúng tôi, hãy cung cấp cho ChatGPT công thức đúng, và yêu cầu một hàm tương ứng trong ba ngôn ngữ lập trình: 1) MATLAB¹¹, đại diện cho một phần mềm độc quyền, 2) Python¹², một phần mềm mã nguồn mở phổ biến trong cộng đồng AI, và R¹³, một phần mềm mã nguồn mở phổ biến trong cộng đồng thống kê. Lưu ý rằng trong các trường hợp khi đầu ra quá rộng, chúng tôi điều chỉnh nó để vừa trên trang; nếu không thì chúng tôi không điều chỉnh nó theo bất kỳ cách nào khác.

Đây là công thức đúng: c(u,v) = (1 + theta) * (u * v)^(-theta-1) * (u^(-theta) + v^(-theta) - 1)^(-2-1/theta). Mã hóa nó trong MATLAB như ClaytonCopulaDensity(u, v, theta).

Đây là mã MATLAB cho hàm mật độ xác suất của copula Clayton với tham số theta được đánh giá tại (u, v):

function dens = ClaytonCopulaDensity(u, v, theta)
    dens = (1 + theta) * (u * v)^(-theta - 1) * ...
           (u^(-theta) + v^(-theta) - 1)^(-2 - 1/theta);
end

Bạn có thể gọi hàm này với các giá trị mong muốn của u, v, và theta để đánh giá mật độ của copula Clayton tại những giá trị đó.

Sau khi cung cấp cho ChatGPT công thức đúng, nó ngay lập tức tạo ra một mã chức năng. Lưu ý rằng chúng tôi đã sử dụng một dạng hội thoại khá tự nhiên và thoải mái, ví dụ, như trong một email.

Mã ClaytonCopulaDensity trong Python.

def ClaytonCopulaDensity(u, v, theta):
    density = ((1 + theta) * (u * v)**(-theta - 1) *
              (u**(-theta) + v**(-theta) - 1)**(-2 - 1/theta))
    return density

Và trong R.

ClaytonCopulaDensity <- function(u, v, theta) {
    density <- ((1 + theta) * (u * v)^(-theta - 1) *
               (u^(-theta) + v^(-theta) - 1)^(-2 - 1/theta))
    return(density)
}

Vì ChatGPT tính đến cuộc hội thoại trước đó, chúng ta có thể đủ khả năng để cực kỳ ngắn gọn với các prompts của chúng ta và vẫn nhận được các giải pháp đúng. Trong những gì tiếp theo, chúng tôi chỉ yêu cầu mã trong MATLAB để tiết kiệm không gian. Tuy nhiên, mã tương đương trong Python và R được hiển thị trong các phụ lục, nơi tất cả các hàm có thể dễ dàng được xác định bằng tên của chúng.

3.3 Ước lượng

Có hàm ClaytonCopulaDensity(u, v, theta) này, bạn có thể viết một mã MATLAB cho ước lượng hợp lý tối đa của tham số theta không?

function thetaMLE = ClaytonCopulaMLE(u, v)
    loglik = @(theta) sum(log(ClaytonCopulaDensity(u, v, theta)));
    thetaMLE = fminsearch(@(theta) -loglik(theta), 1);
end

Trái ngược với những khó khăn của chúng tôi với PDF, chúng tôi ngay lập tức nhận được một giải pháp đúng. Điều này có thể do thực tế là các đoạn mã tính toán các ước lượng ML xuất hiện thường xuyên hơn trong dữ liệu huấn luyện của ChatGPT. Mẫu này (nhiệm vụ càng tổng quát, chúng ta càng thường xuyên nhận được một giải pháp hoạt động trong lần thử đầu tiên) cũng được quan sát trong các ví dụ khác trong suốt công việc này.

3.4 Lấy mẫu

Trong MATLAB, mã hóa một hàm ClaytonSample(theta, n) tạo ra một mẫu có kích thước n từ copula bivariate Clayton với tham số theta.

¹⁰https://openai.com/blog/chatgpt-plugins
¹¹Chúng tôi đã sử dụng phiên bản R2020a.
¹²Phiên bản 3.9.
¹³Phiên bản 4.2.2.

--- TRANG 14 ---
function u = ClaytonSample(theta, n)
    u1 = rand(n, 1);
    u2 = rand(n, 1);
    v1 = (u1.^(-theta)+u2.^(-theta)-1).^(-1/theta);
    v2 = (u1.^(-theta)+u2.^(-theta)-1).^(-1/theta);
    u = [v1, v2];
end

Chúng ta thấy rằng u1 và u2 được rút từ phân phối đồng nhất chuẩn, thường được sử dụng trong các thuật toán lấy mẫu như vậy. Tuy nhiên, sau đó chúng chỉ được sử dụng làm đối số của (3), dẫn đến hai mẫu một chiều giống hệt nhau từ phân phối của C(U1,U2), trong đó U1 và U2 là hai biến ngẫu nhiên độc lập với phân phối đồng nhất chuẩn. Lưu ý nếu vector ngẫu nhiên (U1,U2) được phân phối theo C, thì hàm phân phối của C(U1,U2) sẽ là phân phối được biết đến như hàm Kendall; xem Joe (2014, trang 419–422). Vì vậy chúng ta đang chứng kiến một hỗn hợp của các phương pháp liên quan đến mô hình hóa copula, tuy nhiên, không dẫn đến một giải pháp đúng. Một mẫu được tạo ra với mã này cho θ = 2 được hiển thị ở phía bên trái của Hình 1, và rõ ràng không phải là một mẫu từ copula Clayton Cθ với tham số θ = 2.

[Hình 1: Một mẫu từ vector ngẫu nhiên (C(U1,U2),C(U1,U2)) (trái). Một mẫu từ một thử nghiệm lấy mẫu không chính xác khác của ChatGPT (giữa). Một mẫu được tạo ra bởi quy trình lấy mẫu đúng dựa trên lý thuyết được cung cấp cho ChatGPT (phải). Mẫu được lấy bằng cách gọi ClaytonSample(2, 1000).]

Trong ClaytonSample(theta, n), v1 và v2 là hai vector giống hệt nhau. Do đó, u = [v1, v2] không thể là một mẫu từ copula bivariate Clayton với tham số theta. Bạn có thể mã hóa một phiên bản đúng của ClaytonSample(theta, n) không?

function u = ClaytonSample(theta, n)

--- TRANG 15 ---
    w = rand(n, 1);
    v = rand(n, 1);
    for i = 1:n
        v(i) = (w(i)^(-theta) * (1 - v(i))^ ...
               (-theta/(1+theta)))^(-1/theta);
    end
    u = [w, v]

Trong lần thử thứ hai, ChatGPT cũng thất bại trong việc tạo ra một giải pháp đúng; xem mẫu được tạo ra bởi mã này ở giữa Hình 1. Ở đây, lập luận đằng sau thậm chí còn ít rõ ràng hơn. Hai ví dụ tầm thường này một lần nữa minh họa rằng chúng ta phải cực kỳ cẩn thận về các kết quả được tạo ra bởi ChatGPT. Mặt khác, điều này cho chúng ta cơ hội để chỉ ra rằng ChatGPT có thể học các khái niệm mới một cách tương tác bằng cách cung cấp cho nó lý thuyết cần thiết như chúng ta sẽ chứng minh tiếp theo.

Một phương pháp tiêu chuẩn để lấy mẫu hiệu quả từ các copulas Archimedean được giới thiệu trong Marshall và Olkin (1988). Hãy cung cấp cho ChatGPT lý thuyết liên quan và yêu cầu nó một giải pháp đúng.

Cho psi(t) = (1+ t)^(-1/theta) là một hàm với tham số theta trong (0, infty). Cho E1 và E2 là hai mẫu độc lập từ phân phối mũ chuẩn, và V là một mẫu độc lập từ phân phối Gamma với tham số hình dạng 1/theta và tham số tỷ lệ 1. Thì (psi(E1/V), psi(E2/V)) là một mẫu từ copula bivariate Clayton với tham số theta. Mã hóa một hàm MATLAB ClaytonSample(theta, n), tạo ra n mẫu từ copula bivariate Clayton với tham số theta sử dụng lý thuyết được cung cấp.

function u = ClaytonSample(theta, n)
    % Generate two samples from exponential distribution
    E = exprnd(1, 2, n);
    % Generate a sample from gamma distribution
    V = gamrnd(1/theta, 1, 1, n);
    % Define the function psi(t)
    psi = @(t) (1 + t).^(-1/theta);
    % Apply the transformation
    u = [psi(E(1,:) ./ V); psi(E(2,:) ./ V)]';
end

Ngoài việc tạo ra một giải pháp đúng, điều này tạo ra mẫu ở phía bên phải của Hình 1, đây là một minh chứng rõ ràng về khả năng của ChatGPT để học trong quá trình tương tác với người dùng (có ý nghĩ rằng khả năng này chỉ là một ảo tưởng khá thuyết phục được kích hoạt bởi kỹ thuật kỹ thuật prompt được áp dụng cho ChatGPT, như đã thảo luận trong Phần 2.2). Trái ngược với ví dụ trước đó, nơi nó "chỉ" dịch công thức được cung cấp cho PDF thành một mã trông tương tự hơn hoặc ít hơn, ví dụ này cho thấy rằng ChatGPT có thể hiểu thậm chí các khái niệm tương đối phức tạp.

--- TRANG 16 ---
Khả năng như vậy làm cho ChatGPT trở thành một công cụ khả thi cũng trong các trường hợp khi nó đối mặt với các khái niệm không xác định. Điều này là cần thiết, ví dụ, trong các trường hợp khi một lý thuyết mới được phát triển bởi một nhà nghiên cứu sắp được mã hóa. Ngoài ra, lưu ý rằng ChatGPT tiết kiệm thời gian của chúng ta bằng cách ánh xạ các khái niệm của chúng ta, ví dụ, phân phối mũ chuẩn và gamma, đến các hàm hiện có trong ngôn ngữ lập trình đang xem xét. Đặc biệt, mà không yêu cầu một cách rõ ràng, ChatGPT đã tránh một vòng lặp lặp lại từ 1 đến n trong đó một mẫu từ copula sẽ được tạo ra, thường chậm, nhưng tạo ra trực tiếp n mẫu của E1 và E2 từ phân phối mũ chuẩn (exprnd(1, 2, n), trong đó 1 biểu thị tham số của phân phối mũ). Do đó chúng ta có thể tránh có lẽ phần nhàm chán nhất của việc mã hóa khi chúng ta đang duyệt tài liệu cho một hàm cụ thể trong các thư viện có sẵn.

Cuối cùng, hãy thực hiện một kiểm tra đơn giản của tất cả các hàm được tạo ra trước đó.

Tạo ra một script vẽ một mẫu có kích thước 1000 từ copula Clayton với theta = 2 và sau đó tính toán một ước lượng theta hat của theta sử dụng ước lượng ML.

U = ClaytonSample(2, 1000);
theta_hat = ClaytonCopulaMLE(U(:,1), U(:,2));

Ở trong ngữ cảnh của cuộc hội thoại trước đó, ChatGPT đã khai thác hàm ClaytonCopulaMLE được tạo ra trong Phần 3.3. Sau khi thực thi mã kiểm tra đơn giản, chúng ta nhận được lỗi sau.

Error using *
Incorrect dimensions for matrix multiplication. Check that
the number of columns in the first matrix matches the
number of rows in the second matrix. To
perform elementwise multiplication, use '.*'.
Error in ClaytonCopulaDensity (line 2)
    dens = (1 + theta) * (u * v)^(-theta - 1) * ...
Error in ClaytonCopulaMLE>@(theta)sum(log(...
    ClaytonCopulaDensity(u,v,theta))) (line 2)
    loglik = @(theta) sum(log(ClaytonCopulaDensity(u, v, theta)));
Error in ClaytonCopulaMLE>@(theta)-loglik(theta) (line 3)
    thetaMLE = fminsearch(@(theta) -loglik(theta), 1);
Error in fminsearch (line 200)
    fv(:,1) = funfcn(x,varargin{:});
Error in ClaytonCopulaMLE (line 3)
    thetaMLE = fminsearch(@(theta) -loglik(theta), 1);

--- TRANG 17 ---
Chúng tôi sao chép-dán thông báo lỗi vào ChatGPT, ví dụ, đưa cho nó prompt

Chúng ta nhận được thông báo lỗi này: ***

trong đó *** là thông báo lỗi, và nhận được phản hồi sau.

Thông báo lỗi cho thấy rằng mã đang cố gắng thực hiện phép nhân ma trận với toán tử *, nhưng các chiều của ma trận không khớp. Điều này có thể xảy ra trong dòng dens = (1 + theta) * (u * v)^(-theta - 1) * ....

Để khắc phục điều này, chúng ta cần sử dụng phép nhân theo từng phần tử với toán tử .* thay vì phép nhân ma trận với *. Đây là phiên bản đã được sửa của hàm:

function dens = ClaytonCopulaDensity(u, v, theta)
    dens = (1 + theta) * (u .* v).^(-theta - 1) .* ...
           (u.^(-theta) + v.^(-theta) - 1).^(-2 - 1/theta);
end

Lưu ý việc sử dụng .* thay vì * và .^ thay vì ^ để chỉ ra các phép toán theo từng phần tử.

ChatGPT không chỉ phát hiện ra điều gì sai, mà còn cung cấp một giải pháp đã được sửa cùng với một giải thích về vấn đề. Cho rằng những sai lầm tầm thường này xảy ra khá thường xuyên trong thực tế hàng ngày, sự giúp đỡ như vậy chỉ ra vấn đề hoặc thậm chí giải quyết nó có thể tiết kiệm thời gian đáng kể. Sau khi thực thi kiểm tra đơn giản với phiên bản đã được sửa của ClaytonCopulaDensity, chúng ta nhận được theta_hat = 2.12. Cho đến nay vẫn tốt.

3.5 Trực quan hóa

--- TRANG 18 ---
Viết mã MATLAB tính toán và tạo ra một đồ thị của 20 cặp (theta, thetahat), trong đó theta từ (0.1, 3) và theta hat là ước lượng ML của theta (được tính toán sử dụng ClaytonCopulaMLE(u, v)) dựa trên một mẫu có kích thước 1000 từ copula bivariate Clayton với tham số theta (được tạo ra sử dụng U = ClaytonSample(theta, n))

n = 1000;
thetas = linspace(0.1, 3, 20);
theta_hats = zeros(size(thetas));
for i = 1:length(thetas)
    U = ClaytonSample(thetas(i), n);
    theta_hats(i) = ClaytonCopulaMLE(U(:,1), U(:,2));
end
plot(thetas, theta_hats, 'o');
hold on;
plot([0, 3], [0, 3], '--');
xlim([0, 3]);
ylim([0, 3]);
xlabel('\theta');
ylabel('\theta_{hat}');
title(['Maximum likelihood estimates for ' ...
       'Clayton copula parameter']);
legend('Estimates', 'True values');

Đồ thị được tạo ra bởi phản hồi được mô tả ở phía bên trái của Hình 2. Vì các cặp (θ,θ̂) gần với đồng nhất, nó cung cấp bằng chứng rằng tất cả mã được tạo ra trước đó hoạt động đúng cách. Chúng tôi muốn nhấn mạnh rằng ngay cả khi ChatGPT không được hướng dẫn để:

1. Các tham số θ được phân bố tuyến tính trong khoảng mong muốn. Đây là một lựa chọn điển hình cho nhiều trực quan hóa.

2. Nó hiển thị đồng nhất hoàn hảo bằng một đường thẳng, cũng thường được xem xét trong một điểm chuẩn lý tưởng. Điều này rõ ràng chứng minh rằng ChatGPT ít nhất một phần hiểu các khái niệm cơ bản, tức là, chúng ta đang ước lượng giá trị thực của một tham số nào đó.

3. Các vấn đề tầm thường tốn thời gian điển hình như giới hạn, nhãn, tiêu đề và chú thích cũng được hiển thị.

Tóm lại, đây là loại nhiệm vụ mà người dùng có thể hưởng lợi đáng kể từ sự hợp tác với các công cụ như ChatGPT.

--- TRANG 19 ---
[Hình 2: Các đồ thị được tạo ra bởi mã được tạo ra bởi ChatGPT trong MATLAB (trên-trái), Python (trên-phải) và R (dưới).]

Tuy nhiên, chúng tôi muốn lưu ý rằng hai lần lặp lại của prompt cuối cùng của chúng tôi đã được thực hiện trước khi chúng tôi nhận được cái được trình bày. Trong lần đầu tiên, chúng tôi bỏ qua nội dung của hai dấu ngoặc đơn với tên hàm. Trong trường hợp đó, đầu ra của ClaytonSample không khớp với chiều của đầu vào của ClaytonCopulaMLE. Trong lần lặp thứ hai, chúng tôi thêm những dấu ngoặc đơn đó vào prompt, nhưng không có "U =", và nhận được một lỗi rất tương tự. Chúng tôi sao chép-dán thông báo lỗi vào ChatGPT, nhưng lần này nó không thành công trong việc cung cấp một phiên bản đúng. Cuối cùng, chúng tôi thêm "U =" đó trong prompt với trực giác rằng ChatGPT không nhận thức được các chiều đúng của đầu ra, và theo cách này chúng tôi nhận được một giải pháp hoạt động. Điểm chính từ ví dụ này là việc có được một giải pháp đúng là một quá trình lặp đòi hỏi kỹ thuật prompt cẩn thận. Kết nối với sự hợp tác con người-con người là rõ ràng: cho đến khi kỹ sư phần mềm không hiểu những gì nhà nghiên cứu chính xác muốn, cô ấy/anh ấy có lẽ sẽ cung cấp các giải pháp không thỏa mãn.

--- TRANG 20 ---
Mã chức năng cuối cùng từ MATLAB trong Python.

Phản hồi được hiển thị trong Phụ lục A và đồ thị tương ứng được mô tả ở phía bên phải của Hình 2. Khi một dự án được di chuyển từ ngôn ngữ lập trình này sang ngôn ngữ lập trình khác, khả năng này của ChatGPT có thể hạn chế các nỗ lực/chi phí liên quan một cách mạnh mẽ.

Và trong R.

Trong trường hợp này, việc chuyển đổi sang R không thành công vì chúng ta một lần nữa gặp vấn đề với việc khớp chiều của đầu vào của ClaytonCopulaMLE và đầu ra của ClaytonSample. Chúng ta cũng không thành công với việc sao chép-dán thông báo lỗi vào ChatGPT. Rõ ràng, chúng ta có thể chính xác hơn bằng cách chỉ định những chiều này. Tuy nhiên, chúng ta cố ý không chỉ định rõ nhiệm vụ để minh họa điều gì tách biệt các giải pháp thành công khỏi các giải pháp không thành công.

Để giải quyết nhiệm vụ, cuối cùng chúng tôi đã sử dụng một phương pháp "brute-force" khá bao gồm việc sao chép-dán toàn bộ mã trong Python vào ChatGPT và yêu cầu nó về cùng mã trên R. Phản hồi được hiển thị trong Phụ lục B và đồ thị tương ứng được mô tả ở phía dưới của Hình 2. Một cách khác mà chúng tôi có được một giải pháp hoạt động là cung cấp cho ChatGPT thông tin rõ ràng về các chiều của đầu vào và đầu ra, tức là, sử dụng một phương pháp tương tự như trước đây bằng cách thêm "U =". Một mặt, phương pháp này tao nhã hơn phương pháp brute-force, mặt khác, phương pháp brute-force cho phép chúng ta có được một giải pháp hoạt động trong ít thời gian hơn. Lựa chọn giữa chúng do đó phụ thuộc vào các ưu tiên của người dùng.

Về cơ bản bây giờ chúng ta có một triển khai của nhiệm vụ ví dụ của chúng ta trong mọi ngôn ngữ được hỗ trợ bởi ChatGPT. Lưu ý rằng một triển khai lấy mẫu từ copula Clayton trong Python đã xuất hiện tương đối gần đây như một nỗ lực nghiêm túc được trình bày trong Boulin (2022). Ở đây, chúng ta có được một triển khai tương đương hoàn toàn được tạo ra bởi một AI. Rõ ràng, với các khả năng của ChatGPT, việc đạt được một giải pháp trong các ngôn ngữ lập trình khác nhau sẽ trở nên ít có giá trị hơn nhiều.

3.6 Song song hóa

Lưu ý rằng vì các mã cho các nhiệm vụ sau đây trong Python và R khá dài, chúng tôi chỉ trình bày các phiên bản MATLAB.

3.6.1 CPUs

Trong MATLAB, tối ưu hóa mã để tính toán 20 cặp (theta, thetahat) đó cho tính toán song song trên CPUs. Tạo ra một minh chứng cho thấy thời gian để thực hiện cùng công việc giảm như thế nào khi số lượng workers tăng.

--- TRANG 21 ---
% Define the range of theta values
thetas = linspace(0.1, 3, 20);
% Define the number of samples
n = 1000;
% Define the range of workers to use
numWorkers = 1:4;
% Initialize an array to store the execution times
times = zeros(size(numWorkers));
for i = 1:length(numWorkers)
    % Start a timer
    tic
    % Run the loop in parallel using parfor
    parfor (j = 1:length(thetas), numWorkers(i))
        % Generate a sample from the Clayton copula with
        % parameter theta
        U = ClaytonSample(thetas(j), n);
        % Estimate the parameter theta using maximum likelihood
        ClaytonCopulaMLE(U(:,1), U(:,2));
    end
    % Stop the timer and record the execution time
    times(i) = toc;
end
% Plot the execution times
plot(numWorkers, times, 'o-')
xlabel('Number of workers')
ylabel('Execution time (s)')

Chúng tôi trực tiếp nhận được một giải pháp hoạt động, tạo ra đồ thị được hiển thị ở phía bên trái của Hình 3. Hãy nhấn mạnh một số điểm:

•Ở trong ngữ cảnh của cuộc hội thoại, chúng ta có thể đủ khả năng để cực kỳ ngắn gọn và chỉ sử dụng một từ ("đó" trong câu đầu tiên) để chỉ vào chức năng chúng ta yêu cầu; tự nhiên và tiết kiệm thời gian.

•Chúng ta chỉ prompt ChatGPT với một yêu cầu cấp cao rất cao, Tạo ra một minh chứng, và nhận được chính xác những gì chúng ta muốn. Ban đầu, chúng ta đã sử dụng, nói, cách suy nghĩ truyền thống, bao gồm việc rất cụ thể khi thiết kế một nhiệm vụ. Ví dụ, "tạo ra một mảng cho các phép đo thời gian,

--- TRANG 22 ---
[Hình 3: Hai đồ thị chứng minh sự cải thiện trong thời gian chạy khi thêm workers mới, được tạo ra bởi mã được tạo ra bởi ChatGPT. Đầu ra của mã khi được thực thi trên máy cục bộ được hiển thị ở phía bên trái. Ở phía bên phải, cùng khi được thực thi trên máy chủ có sẵn với 32 CPUs. Đối với cái sau, chúng tôi đã điều chỉnh mã với numWorkers = 1:32 và n = 10000.]

đo thời gian cho cái này và cái kia, lưu trữ nó trong mảng và sau đó vẽ đồ thị theo cách mà...". Vì các nhiệm vụ như vậy có lẽ xuất hiện nhiều lần trong dữ liệu huấn luyện của ChatGPT, việc chỉ định quá mức là không cần thiết, điều này, một lần nữa, có thể tiết kiệm rất nhiều thời gian.

•Lưu ý 4 workers trong numWorkers = 1:4. Đó là một số CPUs phổ biến của một PC văn phòng trung bình. Con số này do đó không chỉ là một dự đoán ngẫu nhiên thuần túy, mà thực chất là một con số xuất hiện nhiều nhất trong dữ liệu huấn luyện của ChatGPT.

Lưu ý rằng ngoài PC văn phòng của chúng tôi, chúng tôi đã thực thi mã trên một máy chủ có sẵn với 32 CPUs, tạo ra đồ thị ở phía bên phải của Hình 3. Vì chỉ có 20 giá trị theta (thetas) được xem xét, chúng ta quan sát rằng sự cải thiện về thời gian không tiếp tục từ worker thứ 21 trở đi.

3.6.2 GPUs

Trong MATLAB, tối ưu hóa hàm ClaytonSample cho tính toán song song trên GPUs. Gọi hàm được tối ưu hóa là ClaytonSampleGPUs. Sau đó tạo ra một script đo thời gian chạy của ClaytonSample(2, 1000000) và của ClaytonSampleGPUs(2, 1000000).

Chúng tôi nhận được hai đoạn mã như phản hồi. Đầu tiên, ChatGPT cung cấp một tối ưu hóa của ClaytonSample cho GPUs.

--- TRANG 23 ---
function [U, V] = ClaytonSampleGPUs(theta, n)
    % Generate random variables on the GPU
    E1 = gpuArray(single(exprnd(1, n, 1)));
    E2 = gpuArray(single(exprnd(1, n, 1)));
    V = gpuArray(single(gamrnd(1/theta, 1, n, 1)));
    psi = @(t) (1+t).^(-1/theta);
    U = gather(psi(E1./V));
    V = gather(psi(E2./V));
end

Sau đó, một mã cho minh chứng của nó tiếp theo.

% Define parameters
n = 1000000;
theta = 2;
% Time CPU version
tic
data_cpu = ClaytonSample(theta, n);
time_cpu = toc;
% Time GPU version
tic
data_gpu = ClaytonSampleGPUs(theta, n);
time_gpu = toc;
% Display results
disp(['CPU time: ' num2str(time_cpu) ' seconds']);
disp(['GPU time: ' num2str(time_gpu) ' seconds']);

Sau khi thực thi cái sau, đầu ra là:

CPU time: 0.17493 seconds
GPU time: 0.093107 seconds

Như có thể quan sát, việc tối ưu hóa ClaytonSample, ClaytonSampleGPUs, dựa trên việc thêm gpuArray, đảm bảo rằng việc lấy mẫu từ phân phối mũ chuẩn và gamma cũng như các tính toán còn lại được thực hiện trực tiếp trên các GPUs có sẵn. Các đầu ra U và V sau đó được thu thập từ GPUs về máy khách bằng gather. Như đầu ra của phần minh chứng của mã cho thấy, điều này đã giảm khoảng một nửa thời gian chạy không được tối ưu hóa.

Ngoài một triển khai hiệu quả hơn, ChatGPT cũng tiết kiệm thời gian của chúng ta bằng cách miễn cho chúng ta khỏi việc kiểm tra xem từng hàm liên quan có được hỗ trợ trên GPUs hay không. Chúng ta cũng không nên quên khía cạnh giáo dục của vấn đề. Vì các kỹ thuật tối ưu hóa này khá tiên tiến, một người dùng thiếu kinh nghiệm thực sự học hỏi từ những đầu ra này (có ý nghĩ rằng chúng có thể không phải lúc nào cũng đúng). Ví dụ, mà không đề cập rõ ràng trong prompt của chúng tôi, các giá trị của theta và n được lưu trữ trong các biến riêng biệt trước khi chúng được sử dụng trong ClaytonSample và ClaytonSampleGPUs. Điều này thuộc về các kỹ thuật mã hóa đúng đắn.

--- TRANG 24 ---
4 Tóm tắt và thảo luận

Trong quá trình phát triển mã hoạt động giải quyết nhiệm vụ ví dụ của chúng tôi, chúng tôi đã quan sát một danh sách đáng kể các lợi thế mà chúng ta có thể hưởng lợi trong khi lập trình cặp với ChatGPT. Đặc biệt:

1. ChatGPT có thể tiết kiệm thời gian bằng cách tóm tắt nhanh chóng và ngắn gọn các sự kiện cơ bản về chủ đề quan tâm của chúng ta, ví dụ, các công thức hoặc ví dụ ứng dụng, như được minh họa trong Phần 3.1.

2. Nếu ChatGPT không thể cung cấp một giải pháp đúng do thiếu hoặc kiến thức không chính xác, chúng ta có thể cung cấp cho nó kiến thức đúng đắn, và làm cho nó sử dụng kiến thức đó để cung cấp một giải pháp đúng. Trong Phần 3.2, phương pháp này dẫn ChatGPT tạo ra một hàm đánh giá PDF của mô hình copula trong ba ngôn ngữ lập trình khác nhau. Trong Phần 3.4, một mã hoạt động để lấy mẫu từ mô hình copula được tạo ra một khi ChatGPT được cung cấp lý thuyết không tầm thường liên quan. Đặc biệt ví dụ sau đó cho thấy rằng ChatGPT có thể hiểu thậm chí các khái niệm tương đối phức tạp, và rõ ràng chứng minh rằng nó có thể được áp dụng trong các trường hợp khi nó đối mặt với các khái niệm không xác định.

3. ChatGPT tiết kiệm thời gian bằng cách ánh xạ các khái niệm đơn giản, ví dụ, lấy mẫu từ phân phối mũ chuẩn và gamma, đến mã hiện có (thư viện, APIs, hoặc hàm) có sẵn cho một ngôn ngữ lập trình đã cho, như được minh họa trong Phần 3.4.

4. Nhiệm vụ càng phổ biến để giải quyết, ChatGPT càng thành công trong việc tạo ra một giải pháp đúng. Điều này được minh họa, ví dụ, trong Phần 3.3, nơi chúng tôi ngay lập tức thu được mã triển khai ước lượng hợp lý tối đa bằng một prompt đơn giản như viết mã cho ước lượng hợp lý tối đa của tham số đó. Một ví dụ khác là việc chuyển đổi giải pháp MATLAB sang Python trong Phần 3.5, hoặc việc tối ưu hóa mã hiện có cho tính toán song song trên CPUs và GPUs trong Phần 3.6.

5. ChatGPT có thể giúp trong các trường hợp khi một lỗi được ném ra sau khi thực thi mã được tạo ra. Trong Phần 3.4, chúng ta đã thấy rằng nó không chỉ phát hiện ra điều gì sai, mà còn cung cấp một giải pháp đã được sửa. Ngoài việc tiết kiệm thời gian cần thiết để tìm kiếm và sửa lỗi, điều này có thể quan trọng đặc biệt cho các lập trình viên ít kinh nghiệm, những người có thể thấy lỗi quá phức tạp và cuối cùng từ bỏ. ChatGPT đã giúp chúng tôi với khoảng 1/3 các lỗi chúng tôi gặp phải. Ngay cả khi không hoàn hảo, điều này đáng kể tốt hơn so với không có sự giúp đỡ gì cả.

6. ChatGPT có thể giúp tạo ra các trực quan hóa. Trong Phần 3.5, nó tạo ra một trực quan hóa cho thấy rằng tất cả mã được tạo ra trước đó là đúng. Ngay cả khi chúng ta không yêu cầu nó, trực quan hóa bao gồm tất cả các vấn đề tầm thường điển hình như nhãn, điểm chuẩn, giới hạn, chú thích, v.v.

7. ChatGPT ít nhất một phần hiểu các khái niệm cơ bản của những gì chúng ta đang làm. Mà không yêu cầu nó làm như vậy, nó đã thêm vào trực quan hóa đồ thị của đồng nhất (xem Phần 3.5), cho thấy rằng nó nhận thức rằng chúng ta đang cố gắng ước lượng giá trị thực của một tham số nào đó.

8. ChatGPT có thể chuyển đổi mã từ ngôn ngữ lập trình này sang ngôn ngữ lập trình khác cũng với các prompts cấp cao như Mã hóa nó trong Python và Và trong R, được chứng minh trong Phần 3.5. Cùng phần cũng cho thấy rằng nếu việc chuyển đổi thất bại (điều đã xảy ra với việc chuyển đổi sang R), có thể sử dụng một giải pháp "brute-force" nhanh chóng cũng hoàn thành nhiệm vụ.

9. ChatGPT có thể tối ưu hóa mã đã được tạo ra, ví dụ, cho các tính toán song song. Bằng cách prompt tối ưu hóa cái đó cho tính toán song song trên CPUs, chúng tôi ngay lập tức nhận được phiên bản được tối ưu hóa của quy trình mẫu-ước lượng được phát triển trong Phần 3.5; xem Phần 3.6.1. Cùng phần cũng cho thấy rằng một prompt cấp cao như Tạo ra một minh chứng của việc tối ưu hóa này có thể dẫn đến mã cho thấy tác động của việc tối ưu hóa, một lần nữa bao gồm các vấn đề tầm thường thường nhàm chán nhưng cần thiết như nhãn, v.v. Tương tự, một tối ưu hóa như vậy cùng với một minh chứng đơn giản cũng được tạo ra cho các tính toán trên GPUs; xem Phần 3.6.2.

10. ChatGPT tuân theo các kỹ thuật mã hóa đúng đắn, vì vậy người dùng có thể thực sự học chúng. Chúng tôi quan sát rằng mã được tạo ra được bình luận đúng cách, được thụt lề, được mô-đun hóa, tránh trùng lặp mã, v.v.

11. ChatGPT giúp người dùng làm quen với mã được tạo ra nhanh hơn. Khi cung cấp mã, ChatGPT thường bao quanh nó bằng thông tin tiếp theo giải thích các tính năng chính của nó. Để tiết kiệm không gian, chúng tôi chủ yếu cắt bỏ điều này, tuy nhiên, một ví dụ có thể được tìm thấy, ví dụ, trong Phần 3.4 liên quan đến thông báo lỗi được ném ra bởi mã kiểm tra đơn giản.

Chúng ta cũng đã thấy rằng lập trình cặp với ChatGPT mang lại một số nhược điểm, cần được xem xét cẩn thận. Hãy tóm tắt chúng và thảo luận về khả năng giảm thiểu chúng:

1. ChatGPT trong phiên bản hiện tại của nó (đầu 2023) kém trong lập luận; xem Phần 3.1. Trên hai ví dụ, chúng tôi đã chứng minh cách nó phản hồi với các câu trả lời mâu thuẫn cho cùng một câu hỏi. Chúng tôi đặc biệt nhấn mạnh trường hợp khi nó đầu tiên trả lời có và sau đó không cho cùng một câu hỏi. Ngoài ra, chúng tôi đã chứng minh việc này có thể nguy hiểm như thế nào trong lập luận định lượng, nơi nó tạo ra các công thức không chính xác trông rất giống với các công thức đúng; xem việc suy dẫn PDF trong Phần 3.2. Để giảm thiểu vấn đề này, nhiều nỗ lực hiện có thể được quan sát. Một trong những ví dụ hứa hẹn nhất theo hướng lập luận định lượng là Minerva (Lewkowycz et al., 2022), một LLM dựa trên các mô hình ngôn ngữ tổng quát PaLM (Chowdhery et al., 2022) với lên đến 540 tỷ tham số.

--- TRANG 25 ---
Mô hình này, được phát hành vào tháng 6 năm 2022, đã thu hút sự chú ý bằng cách đạt 50% trên các câu hỏi trong tập dữ liệu MATH, đây là một cải thiện đáng kể của hiệu suất tốt nhất trước đây trên các tập dữ liệu đánh giá STEM; xem Bảng 3 trong đó. Trong các công việc khác, các tác giả phát triển các mô hình được tinh chỉnh để hiểu các công thức toán học (Peng et al., 2021), hoặc sử dụng mạng thần kinh sâu trong các nhiệm vụ toán học như tích phân tượng trưng hoặc giải phương trình vi phân (Lample và Charton, 2019). Một cách khác để giảm thiểu vấn đề có thể là cố gắng khai thác tối đa các LLMs hiện tại bằng cách điều chỉnh cẩn thận prompt để có được các câu trả lời đáng tin cậy hơn. Kỹ thuật ngày càng phổ biến này, được gọi là kỹ thuật prompt, liên quan đến các kỹ thuật đặc biệt để cải thiện độ tin cậy khi mô hình thất bại trong một nhiệm vụ¹⁴, và có thể cải thiện đáng kể giải pháp, ví dụ, đối với các vấn đề toán đơn giản, chỉ bằng cách thêm "Hãy suy nghĩ từng bước." vào cuối prompt. Lưu ý rằng chúng tôi đã thử kỹ thuật này trong ví dụ xem xét sự phụ thuộc đuôi của copula Clayton sống sót trong Phần 3.1, tuy nhiên, không thành công, có lẽ vì các khái niệm cơ bản vượt quá toán học đơn giản.

2. Nếu ChatGPT thiếu kiến thức cần thiết hoặc có kiến thức không chính xác, nó có thể tạo ra một giải pháp không chính xác mà không có bất kỳ chỉ báo nào cho người dùng. Như được minh họa trong Phần 3.4, sau khi yêu cầu nó về mã để lấy mẫu từ một mô hình copula Clayton, ChatGPT đầu tiên tạo ra hai thủ tục, trông giống các thuật toán lấy mẫu đúng đắn, nhưng hoàn toàn không chính xác. Do tính mờ ám của các LLMs tiên tiến hiện tại chứa hàng chục hoặc thậm chí hàng trăm tỷ tham số, tính đúng đắn của giải pháp khó có thể được đảm bảo trong tất cả các trường hợp. Mặc dù có thể có các nỗ lực để phát triển các LLMs có thể giải thích được hơn, không chắc là các thách thức cơ bản liên quan đến độ phức tạp của ngôn ngữ và lượng dữ liệu khổng lồ cần thiết cho huấn luyện sẽ được vượt qua hoàn toàn. Do đó, điều cần thiết là một chuyên gia con người trong lĩnh vực luôn xác minh đầu ra được tạo ra bởi mô hình.

3. Cụ thể, ChatGPT có xu hướng ít thành công hơn trong việc tạo ra các giải pháp chính xác cho các nhiệm vụ ít phổ biến hơn. Điều này có nghĩa là điều ngược lại của lợi thế 4. cũng áp dụng. Trong Phần 3.2, điều này được chứng minh thông qua hàm mật độ xác suất (PDF) của mô hình copula. Trong Phần 3.4, thông qua thuật toán lấy mẫu. Để giải quyết những vấn đề này, chúng tôi đã cung cấp lý thuyết cần thiết cho ChatGPT, dẫn đến một giải pháp đúng, xem cùng hai phần.

4. ChatGPT không có bất kỳ bộ nhớ nào. Nếu cuộc hội thoại quá dài, và do đó không vừa trong cửa sổ ngữ cảnh của ChatGPT, có vẻ như mô hình đã quên một số phần của cuộc hội thoại. Điều này, cùng với các cách giảm thiểu vấn đề này, đã được thảo luận trong Phần 2.2.

Ngoài ChatGPT, có một số mô hình ngôn ngữ khác có khả năng tạo ra các giải pháp mã từ đầu vào ngôn ngữ tự nhiên. Một ví dụ đáng chú ý là AlphaCode (Li et al., 2022), đạt được trung bình xếp hạng 54,3% hàng đầu trong các cuộc thi với hơn 5.000 người tham gia trên các cuộc thi lập trình gần đây trên nền tảng Codeforces. Gần đây, AlphaCode đã được công khai¹⁵, bao gồm các giải pháp ví dụ từ cuộc thi đã đề cập. Một ví dụ khác là OpenAI Codex¹⁶, đã được đề cập trong phần giới thiệu. Trái ngược với ChatGPT, các mô hình này đã được phát triển đặc biệt cho việc tạo mã. Một mặt, do đó có thể là người ta có thể tạo ra các giải pháp tốt hơn so với những giải pháp được tạo ra với ChatGPT. Do đó sẽ thú vị cho nghiên cứu tương lai để so sánh, ví dụ, sự thành công của các mô hình này trong việc giải quyết các nhiệm vụ được xem xét trong công việc này.

Mặt khác, ChatGPT có thể thuận tiện hơn cho nhiều người dùng so với các mô hình này vì nó cho phép tương tác trong quá trình mã hóa. Không giống như AlphaCode và OpenAI Codex, tạo ra các đoạn mã dựa trên đầu vào ngôn ngữ tự nhiên mà không có tương tác tiếp theo nào, ChatGPT cho phép người dùng cung cấp phản hồi và điều chỉnh mã được tạo ra trong thời gian thực. Tương tác này có thể có lợi vì một số lý do. Đầu tiên, nó cho phép người dùng làm rõ ý định của họ và đảm bảo rằng mã được tạo ra phù hợp với mục tiêu của họ. Ví dụ, như chúng ta đã thấy trong Phần 3.4 xem xét việc lấy mẫu từ một mô hình copula Clayton, nếu một người dùng yêu cầu một chức năng cụ thể và mã được tạo ra không hoàn toàn khớp với những gì họ có trong tâm trí, người dùng có thể cung cấp phản hồi cho ChatGPT để điều chỉnh mã cho phù hợp. Thứ hai, tương tác với ChatGPT có thể giúp người dùng học hỏi thêm về lập trình và cải thiện kỹ năng mã hóa của họ. Bằng cách tham gia vào một cuộc đối thoại với ChatGPT, người dùng có thể có được cái nhìn sâu sắc về logic và cấu trúc của mã họ đang tạo ra, và học cách cải thiện mã của họ trong tương lai. Ví dụ, trong Phần 3.6.2, chúng ta có thể thực sự học cách chuyển đổi mã hiện có cho tính toán song song trên GPUs. Cuối cùng, tương tác với ChatGPT có thể giúp người dùng khắc phục lỗi và gỡ lỗi mã của họ hiệu quả hơn. Như chúng ta đã thấy trong Phần 3.4, ChatGPT có thể nhận ra các lỗi lập trình phổ biến, và cung cấp phản hồi giúp người dùng xác định và sửa lỗi trong mã của họ. Những lý do này, cùng với thực tế là ChatGPT có thể được truy cập thuận tiện thông qua một cổng web, đã dẫn chúng tôi chọn ChatGPT làm đối tác AI lập trình cặp của chúng tôi.

5 Kết luận

Trong một sự hợp tác con người-AI, chúng tôi đã phát triển mã hoạt động triển khai lấy mẫu từ một mô hình copula, ước lượng tham số của nó, trực quan hóa cho thấy rằng hai nhiệm vụ cuối cùng hoạt động đúng cách, và song song hóa mã cho CPUs cũng như GPUs. Để minh họa khả năng mã hóa của phần AI, được đại diện bởi ChatGPT, tất cả các nhiệm vụ đã đề cập được triển khai mà không có một dòng mã nào được viết bởi con người. Ngoài việc trình bày cách đạt được một giải pháp thành công cho một nhiệm vụ đã cho, chúng tôi cũng đã chỉ ra các ví dụ bổ sung chứng minh những sửa đổi nào của prompts của chúng tôi cho ChatGPT đã biến các giải pháp thất bại thành các giải pháp thành công. Điều này dẫn đến một danh sách toàn diện các ưu và nhược điểm liên quan, cho thấy rằng nếu các cạm bẫy điển hình có thể được tránh, chúng ta có thể hưởng lợi đáng kể từ sự hợp tác với một đối tác AI như ChatGPT.

Lời cảm ơn

Tác giả cảm ơn Quỹ Khoa học Séc (GAČR) về sự hỗ trợ tài chính cho công việc này thông qua grant 21-03085S. Tác giả cũng cảm ơn Martin Holešna và Marius Hofert về những bình luận và khuyến nghị mang tính xây dựng chắc chắn đã giúp cải thiện tính dễ đọc và chất lượng của bài báo.

Phụ lục A Giải pháp trong Python

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import fmin

def ClaytonSample(theta, n):
    E1 = np.random.exponential(size=n)
    E2 = np.random.exponential(size=n)
    V = np.random.gamma(1/theta, 1, size=n)
    psi = lambda t: (1+t)**(-1/theta)
    U = psi(E1/V)
    V = psi(E2/V)
    return np.column_stack((U, V))

def ClaytonCopulaDensity(u, v, theta):
    return ((1+theta)*(u*v)**(-theta-1)*
            (u**(-theta)+v**(-theta)-1)**(-2-1/theta))

def ClaytonCopulaMLE(data):
    n = data.shape[0]
    u, v = data[:, 0], data[:, 1]
    loglik = lambda theta: np.sum(np.log(
        ClaytonCopulaDensity(u, v, theta)))
    thetaMLE = fmin(lambda theta: -loglik(theta), 1,
                    disp=False)
    return thetaMLE[0]

thetas = np.linspace(0.1, 3, 20)

--- TRANG 29 ---
theta_hats = []
for theta in thetas:
    data = ClaytonSample(theta, 1000)
    theta_hat = ClaytonCopulaMLE(data)
    theta_hats.append(theta_hat)

plt.plot(thetas, theta_hats, 'o')
plt.plot(thetas, thetas, '--')
plt.xlabel('True theta')
plt.ylabel('Estimated theta')
plt.show()

Một ví dụ về mã dư thừa là n = data.shape[0]. Như có thể quan sát, biến n không có ích gì trong ClaytonCopulaMLE.

Phụ lục B Giải pháp trong R

ClaytonSample <- function(theta, n) {
    E1 <- rexp(n)
    E2 <- rexp(n)
    V <- rgamma(n, 1/theta, 1)
    psi <- function(t) (1+t)^(-1/theta)
    U <- psi(E1/V)
    V <- psi(E2/V)
    return(cbind(U, V))
}

ClaytonCopulaDensity <- function(u, v, theta) {
    return((1+theta)*(u*v)^(-theta-1)*
           (u^(-theta)+v^(-theta)-1)^(-2-1/theta))
}

ClaytonCopulaMLE <- function(data) {
    u <- data[,1]
    v <- data[,2]
    loglik <- function(theta) -sum(log(
        ClaytonCopulaDensity(u, v, theta)))
    thetaMLE <- optimize(loglik, c(0.1, 3), maximum=FALSE)$minimum
    return(thetaMLE)
}

thetas <- seq(0.1, 3, length.out=20)

--- TRANG 30 ---
theta_hats <- c()
for (theta in thetas) {
    data <- ClaytonSample(theta, 1000)
    theta_hat <- ClaytonCopulaMLE(data)
    theta_hats <- c(theta_hats, theta_hat)
}

plot(thetas, theta_hats, pch=19, col="blue", ylim=c(0,3),
     xlab="True theta", ylab="Estimated theta")
abline(a=0, b=1, lty=2)

Thú vị là, ngay cả khi mã này là một chuyển đổi trực tiếp của mã từ Phụ lục A, mã dư thừa từ phiên bản Python của ClaytonCopulaMLE không có mặt. Điều này gợi ý về khả năng của ChatGPT chỉ giữ lại mã có liên quan.

Tài liệu tham khảo

Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al. (2023). A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.

Boulin, A. (2022). Sample from copula: a coppy module. arXiv preprint arXiv:2203.17177.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30.

Clayton, D. G. (1978). A model for association in bivariate life tables and its application in epidemiological studies of familial tendency in chronic disease incidence. Biometrika, 65:141–151.

--- TRANG 31 ---
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Frieder, S., Pinchetti, L., Griffiths, R.-R., Salvatori, T., Lukasiewicz, T., Petersen, P. C., Chevalier, A., and Berner, J. (2023). Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867.

Hobæk Haff, I. (2013). Parameter estimation for pair-copula constructions.

Hofert, M. (2010). Sampling nested Archimedean copulas with applications to CDO pricing.

Hofert, M., Huser, R., and Prasad, A. (2018). Hierarchical Archimax copulas. Journal of Multivariate Analysis, 167:195–211.

Hofert, M., Mächler, M., and McNeil, A. J. (2013). Archimedean copulas in high dimensions: Estimators and numerical challenges motivated by financial applications. Journal de la Société Française de Statistique, 154(1):25–63.

Huang, Y., Zhang, B., Pang, H., Wang, B., Lee, K. Y., Xie, J., and Jin, Y. (2022). Spatio-temporal wind speed prediction based on clayton copula function with deep learning fusion. Renewable Energy, 192:526–536.

Joe, H. (2014). Dependence Modeling with Copulas. CRC Press.

Katz, D. M., Bommarito, M. J., Gao, S., and Arredondo, P. (2023). GPT-4 passes the bar exam. Available at SSRN 4389233.

Lample, G. and Charton, F. (2019). Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412.

Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. (2022). Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858.

Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. (2022). Competition-level code generation with alphacode. Science, 378(6624):1092–1097.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Maddigan, P. and Susnjak, T. (2023). Chat2vis: Generating data visualisations via natural language using chatgpt, codex and gpt-3 large language models. arXiv preprint arXiv:2302.02094.

Marshall, A. W. and Olkin, I. (1988). Families of multivariate distributions. Journal of the American Statistical Association, 83(403):834–841.

--- TRANG 32 ---
McNeil, A., Frey, R., and Embrechts, P. (2015). Quantitative risk management: Concepts, techniques and tools. Princeton university press.

Michimae, H. and Emura, T. (2022). Likelihood inference for copula models based on left-truncated and competing risks data from field studies. Mathematics, 10(13):2163.

Nelsen, R. B. (2006). An Introduction to Copulas. Springer-Verlag, 2nd edition.

OpenAI (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.

Peng, S., Yuan, K., Gao, L., and Tang, Z. (2021). Mathbert: A pre-trained model for mathematical formula understanding. arXiv preprint arXiv:2105.00377.

Schellhase, C. and Spanhel, F. (2018). Estimating non-simplified vine copulas using penalized splines. Statistics and Computing, 28:387–409.

Sklar, A. (1959). Fonctions de répartition à n dimensions et leurs marges. Publications de l'Institut Statistique de l'Université de Paris, 8:229–231.

Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.F. (2020). Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021.

Williams, L. (2001). Integrating pair programming into a software development process. In Proceedings 14th Conference on Software Engineering Education and Training. 'In search of a software engineering profession' (Cat. No. PR01059), pages 27–36. IEEE.

¹⁴https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md
¹⁵https://github.com/deepmind/code_contests
¹⁶https://openai.com/blog/openai-codex/

32
