# 2307.15370.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2307.15370.pdf
# File size: 5507716 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Highlights
Private-Library-Oriented Code Generation with Large Language Models
Daoguang Zan,Bei Chen,Yongshun Gong,Junzhi Cao,Fengji Zhang,Bingchao Wu,Bei Guan,Yilong Yin,Yongji Wang
‚Ä¢Weintroduceanewscenariofocusingonprivate-library-orientedcodegenerationandproposeaninnovativeframework
miming the human process of using private libraries. This framework consists of two main modules: APIFinder and
APICoder.
‚Ä¢Tofairlyevaluatethisscenario,wemanuallyconstructfourbenchmarks,namelyTorchDataEval,TorchDataComplex-
Eval, MonkeyEval, and BeatNumEval, each featuring an extensive array of test cases.
‚Ä¢Our comprehensive experiments exhibit the superior performance of our proposed framework, showcasing its effec-
tiveness and efficiency in handling private APIs.arXiv:2307.15370v1  [cs.SE]  28 Jul 2023

--- PAGE 2 ---
Private-Library-Oriented Code Generation with Large Language
Models
Daoguang Zana,1, Bei Chenb,‚àó, Yongshun Gongc,‚àó, Junzhi Caod, Fengji Zhange, Bingchao Wua,
Bei Guana,‚àó, Yilong Yincand Yongji Wanga
aInstitute of Software, Chinese Academy of Sciences, Beijing, China
bMicrosoft, Beijing, China
cSchool of Software, Shandong University, Jinan, China
dNew York University, New York, USA
eSchool of Computer Science, Wuhan University, Wuhan, China
ARTICLE INFO
Keywords :
Code generation
Large language model
Private library
API documentation
Retrieval-based generationABSTRACT
Largelanguagemodels(LLMs),suchasCodexandGPT-4,haverecentlyshowcasedtheirremarkable
codegenerationabilities,facilitatingasignificantboostincodingefficiency. Thispaperwilldelveinto
utilizingLLMsforcodegenerationinprivatelibraries,astheyarewidelyemployedineverydaypro-
gramming. Despite their remarkable capabilities, generating such private APIs poses a formidable
conundrum for LLMs, as they inherently lack exposure to these private libraries during pre-training.
To address this challenge, we propose a novel framework that emulates the process of programmers
writing private code. This framework comprises two modules: APIFinder first retrieves potentially
useful APIs from API documentation; and APICoder then leverages these retrieved APIs to generate
privatecode. Specifically,APIFinderemploysvectorretrievaltechniquesandallowsuserinvolvement
intheretrievalprocess. ForAPICoder,itcandirectlyutilizeoff-the-shelfcodegenerationmodels. To
further cultivate explicit proficiency in invoking APIs from prompts, we continuously pre-train a re-
inforced version of APICoder, named C ODEGENAPI. Our goal is to train the above two modules
on vast public libraries, enabling generalization to private ones. Meanwhile, we create four private
library benchmarks, including TorchDataEval, TorchDataComplexEval, MonkeyEval, and BeatNu-
mEval, and meticulously handcraft test cases for each benchmark to support comprehensive evalu-
ations. Numerous experiments on the four benchmarks consistently affirm the effectiveness of our
approach. Furthermore, deeper analysis is also conducted to glean additional insights.
1. Introduction
Theuseofthird-partycodelibrariescangreatlyenhance
codereusability,reducedevelopmentdifficulty,andacceler-
ate development speed in software development [3, 6, 20].
Developers can expedite their development process by in-
tegrating third-party libraries, while avoiding the redundant
work of reinventing the wheel. Considering the many ad-
vantages of using third-party code libraries, it is common
forcompaniestodevelopandmaintainacodelibraryaimed
atbettercodecontrolandversionmanagement. However,in
contrast to public code libraries, many code libraries main-
tainedbycompaniesareprivateandexclusivelyusedforin-
ternal purposes, with the goal of enhancing security mea-
sures for the protection of intellectual property and trade
secrets [39]. Hence, we can see that leveraging private li-
braries for code generation is a prevalent and meaningful
scenario[55]. Withtheswiftadvancementoflargelanguage
model(LLM),thistechnologyhasbeenappliedincodegen-
eration field [13, 61, 64] via training on extensive code cor-
pora,suchasCodex[14],AlphaCode[35],C ODEGEN[41].
Unfortunately,despitetheselanguagemodelspossessingin-
crediblyimpressivecodegenerationabilitiesingeneralsce-
narios, they are powerless when handling private libraries
‚àóCorresponding author
daoguang@iscas.ac.cn (D. Zan); beichen@microsoft.com (B. Chen);
ysgong@sdu.edu.cn (Y. Gong); guanbei@iscas.ac.cn (B. Guan)
ORCID(s):as they have no prior knowledge of such information. Fig-
ure 1 gives a concrete example to illustrate it. When gen-
erating code for the public library pandas, Codex, a 12B
largelanguagemodel,successfullyinvokesthelibrary‚ÄôsAPI
callsandproduceseffectivecode. Whenweencapsulatethe
pandas.DataFrame.isin into monkey.KnowledgeFrame.iscontain
forinternalusage;however,Codexisunabletocorrectlyin-
voketheAPIandgeneratecodefortheprivatelibrarybyref-
erencing API documentation, as a programmer would. This
examplefurtherunderscoresthechallengesofLLMsinfac-
ing private-library-oriented code generation.
In this paper, our proposal seeks to tackle these chal-
lenges by introducing a framework that unleashes large lan-
guagemodelswiththeaptitudetoproducecodethatinvokes
private libraries. As private libraries often feature compre-
hensivereferencematerialssuchasAPIdocumentation,our
coreideainvolvesemulatingtheprocessbywhichprogram-
mers use API documentation to develop code for private li-
braries. The process of searching information in API doc-
umentation to understand how to use an API before writing
code,commonlyreferredtoasAPIdocumentationlookup[50],
is an essential practice in software development and pro-
gramming. Likewise, our framework consists of two mod-
ules: APIFinderretrievespotentialAPIsfromtheAPIdocu-
mentationbasedonuserrequirements,andAPICoderlever-
agestheretrievedAPIstoproducecode. RegardingAPIFinder,
we employ a dense retriever and incorporate a user-friendly
Daoguang Zan et al.: Preprint submitted to Elsevier Page 1 of 18

--- PAGE 3 ---
Private-Library-Oriented Code Generation with Large Language Models
import pandas aspd
df= pd.DataFrame ({'num_legs' : [2, 4], 'num_wings' : [2, 0]})
values = [1, 2]
# whether each element in the `df` is contained in values.
Codex 12Bpublic code prompt (pandas)
import monkey asmk
kf= mk.KnowledgeFrame ({'a': [2, 4], 'b': [2, 0]})
values = [1, 2]
# whether each element in the `kf` is contained in values.private code prompt (monkey)df.isin(values )‚úìpandas. DataFrame .isin
Codex 12B
kf.isin(values )
kf.iscontain (values )
 monkey. KnowledgeFrame .iscontain
pandas. DataFrame .isin
‚úìI have seen pandas, so I can do it.
Ihave never seen monkey, butIcan invoke
the API ofpandas asthe code looks very
similar tothecode snippet from pandas .
Ihave never seen monkey, butIcan refer to
theAPIdocumentation ofmonkey todoit.
Figure 1: An example of code generation on public library
(pandas) and private library (monkey) by Codex 12B and a
programmer.
interface that provides optional user engagement in the API
retrieval process. Regarding APICoder, off-the-shelf lan-
guagemodelsforcodegenerationlikeC ODEGENcanbeuti-
lized to write private code directly. Even more thrillingly,
wediscoverthatcontinuouslypre-trainingtheexistingmod-
elsusingourproposednovelstrategiescansignificantlyim-
prove their capability of calling private library APIs. Since
we are unable to access the corpus of private libraries, we
train both APIFinder and APICoder on many crawled pub-
liclibrarieswiththeaspirationofenablinggeneralizationto
private library scenarios.
Tothebestofourknowledge,wearethefirsttopropose
the private-library-oriented code generation scenario. Due
to the lack of available benchmarks for evaluating this sce-
nario, we create four benchmarks: TorchDataEval, Torch-
DataComplexEval, MonkeyEval, and BeatNumEval. Both
TorchDataEval and TorchDataComplexEval consist of 50
programming problems for private libraries, with the latter
featuring more challenging and practical problems. Mon-
keyEval and BeatNumEval, each containing 101program-
mingproblems,areadaptationsofPandasEvalandNumpyE-
val [63], respectively.
This paper serves as an extended edition of [62] by en-
hancing various aspects of the original work. Specifically,
these aspects can be summarized as follows: (1) We inves-
tigate private-library-oriented code generation across vary-
ing levels of difficulty and further craft a more challenging
andreal-worldprivatelibrarybenchmarknamedTorchData-
ComplexEval. (2)Weexploretheimpactofallcomponents
of API documentation, such as API examples and param-eters, on the effectiveness of LLMs. (3) We conduct exten-
siveexperimentson 17popularcodegenerationmodels,and
further train C ODEGENAPI using three prompts across var-
ious model sizes ( 350M,2B,6B). (4) We perform a more
comprehensive and rigorous analysis and evaluation, yield-
ing numerous intriguing and valuable insights. Overall, our
contributions are listed as follows:
‚Ä¢Weintroduceascenariocalledprivate-library-oriented
code generation and propose a simple yet innovative
framework that emulates the human approach to uti-
lizing private libraries.
‚Ä¢We meticulously develop four private library bench-
marks, each equipped with a comprehensive suite of
test cases.
‚Ä¢Thorough experiments highlight the remarkable per-
formance of our framework, emphasizing its feasibil-
ity in invoking private APIs.
2. Related Work
2.1. Large Language Models for Code Generation
Pioneering models like PLBART [1], PyMT5 [17], and
GPT-C [54] possess modest parameter counts and exhibit
limitedproficiencyinzero-shotcodegeneration. Incontrast,
althoughlarge-scalemodelssuchasGPT-Neo[8]andGPT-
J[56]boastbillionsofparameters,theireffectivenessinthe
code generation task remains constrained by the scarcity of
code within their training datasets. Lately, a variety of ad-
vancedLLMshavebeenintroducedforcodegeneration,in-
cluding but not limited to Codex [14], PaLM-Coder [15],
PanGu-Coder [16, 51], AlphaCode [35], GPT-4 [42], and
ChatGPT1. These models encompass extensive parameter
countsandaretrainedonpremiumcode-richdata. Although
theyexhibitoutstandingperformanceincodegenerationtasks,
the majority of them are inaccessible. Currently, numerous
outstandingpubliclyavailablemodelshaveemerged,suchas
CodeParrot[27],CodeGen[41],andPolyCoder[60]. These
models play a vital role in the progress of LLMs for code
generation. Whilealloftheabovemodelsonlysupportcode
generationfromlefttoright,modelssuchasSantaCoder[2],
FIM [7], InCoder [24], StarCoder [34], and MIM [40] also
supportinsertingcodesnippetatarbitrarypositions,includ-
ingintermediatelocations. BesidesPythonfiles,JuPyT5[11]
optsforJupyterNotebookfilesfortrainingandachievesnote-
worthy results. Recent models, such as ERNIE-Code [10],
BLOOM [49], and CodeGeeX [66], have also considered a
newscenarioofmultipleprogrammingornaturallanguages.
In this study, we will delve into how to equip the aforemen-
tionedmodelswiththecapabilitytoutilizeprivatelibraries.
IntheeraofLLMs,hand-craftedcodegenerationbench-
marks play a pivotal role in ensuring that the programming
problems have not been exposed during pre-training. Con-
sequently, HumanEval [14] and MBPP [5] were proposed
1https://chat.openai.com
Daoguang Zan et al.: Preprint submitted to Elsevier Page 2 of 18

--- PAGE 4 ---
Private-Library-Oriented Code Generation with Large Language Models
import
def KBS_Example ():
"""
"""Contextùíô
Target Code ùíö
Figure 2: One simple Python example of code generation. ‚ñ†
represents code library likes pandas; ‚ñ†represents some code
snippets, functions, or classes; ‚ñ†represents a naturallanguage
description of programming problem; ‚ñ†represents target code
that solves the programming problem in ‚ñ†, and may call APIs
from‚ñ†and‚ñ†.
and gained widespread popularity. The above two bench-
marks are English and Python versions. Subsequently, sev-
eral benchmarks extended them to multiple languages, such
as MBXP [4], MultiPL [9], and HumanEval-X [66]. Fur-
thermore,recentlyreleasedbenchmarksareincreasinglytai-
loredtosomereal-worldscenarios. Forexample,APPs[26]
and CodeContests [35] served as benchmarks for assessing
programming contest abilities; DSP [12] and DS-1000 [33]
are benchmarks oriented towards data science; MTPB [41]
wasintroducedformulti-turndialoguecodegeneration. Se-
curityEval [53] was proposed for evaluating code security;
MathQA-Python[5]andGSM8K-Python[18]aregearedto-
wardsevaluatingmathematicalproficiency;PandasEval[29,
63] and NumpyEval [63] test code generation in public li-
braries,whilesomebenchmarks[22,52,65]concentrateon
therepositoryorcross-filelevel. Inthispaper,weproposea
privatelibraryscenarioforcodegenerationandmeticulously
craft four benchmarks, named TorchDataEval, TorchData-
ComplexEval, MonkeyEval, and BeatNumEval.
2.2. Retrieval-based Generation
Retrieval-basedgeneration[23,48,59]hasgarneredsig-
nificantpopularityinthefieldsofnaturallanguageprocess-
ing and information retrieval, owing to its adeptness at cap-
italizingonexternallypre-existingknowledge. Forinstance,
Fusion-in-Decoder[28],DPR[31],andRocketQA[45]lever-
age this technique for open-domain question answering by
retrieving relevant passages and exhibit remarkable results.
In the field of code generation, there also exist some ef-
forts utilizing retrieval techniques, such as DeepAPI [25],
ReACC[37],REDCODER[44],RepoCoder[65],andDoc-
Coder [67]. In this paper, our goal is to retrieve real-world
private APIs from API documentation.
3. Preliminaries
3.1. Task Definition
Givenacodecontext ùê±,thegoalofcodegenerationaims
togeneratetargetcode ùê≤thatsolvestheprogrammingprob-
lem in ùê±. Figure 2 presents an example of code context
and target code. In detail, code context includes the nat-
ural language description of programming problems in the
formofcodecomments ‚ñ†,aswellasimportstatements ‚ñ†,
pandas.DataFrame.head
DataFrame. head (n: int=5) ->NDFrameTAPI name +signature
Return the first nrows.
Inputs:             n:int, default 5
Number of rows to select.
Returns:           same type as caller
The first nrows of the caller object.
Related APIs
DataFrame.tail Returns the last nrows.
API Examples
>>> df= pd.DataFrame (['a', 'b', 'c'])
>>> df.head (2)
0
0      a
1      bAPI description
ParametersFigure 3: AnAPIinstanceofAPIdocumentation. Itshowcases
the primary components of each API in API documentation:
API name, signature, description, parameters, related APIs,
and API examples.
user-defined classes or functions ‚ñ†, function headers, and
etc. The entire code generation process can be formalized
asùê≤=Óàπ(ùê±), where Óàπis code generation model. In this
paper,wefocusonprivate-library-orientedcodegeneration.
Aprivatelibrary,comparedtoapublicone,ischaracterized
by being non-public, i.e., the code library is unseen by Óàπ
during pre-training.
3.2. API documentation
API documentation, also known as API reference, is a
comprehensivedescriptionoftheprogramminginterfacethat
a software library provides to developers. API documenta-
tion serves as a crucial technical resource to developers for
private-library-oriented code generation, enabling them to
understandhowtousethelibraryandtakefulladvantageof
its capabilities. Considering the significance of API docu-
mentation, we would like to provide a detailed breakdown
of its components as illustrated in Figure 3:
‚Ä¢APIName : Thisisadescriptivelabelusedtoidentify
the API and convey its functionality;
‚Ä¢APISignature : Thisoutlinestheinputandoutputpa-
rametersoftheAPI,includingdatatypesandexpected
format;
‚Ä¢APIDescription : Thisoffersacomprehensiveexpla-
nation of the API‚Äôs functionality and purpose;
‚Ä¢API Parameters : These define the input and return
parameters, encompassing information such as data
type,defaultargumentvalues,anddescriptionsofthe
parameters;
Daoguang Zan et al.: Preprint submitted to Elsevier Page 3 of 18

--- PAGE 5 ---
Private-Library-Oriented Code Generation with Large Language Models
API
Documentation
Retrieved APIsAPIFinder
APICoder
Context
Target Codeùê±
ùê≤ùìúùêÖ
ùìúùêÇùìê
Figure 4: Schematic diagram of our framework: APIFinder
first retrieves potentially useful APIs from API documentation,
and then APICoder generates the target code based on the
retrieval APIs.
‚Ä¢RelatedAPIs : TheseareAPIsthatarecloselyrelated
to the current API, either in terms of functionality or
purpose,andoffersimilarorcomplementarycapabil-
ities.
‚Ä¢APIExamples : Theseprovide aworking exampleof
how to invoke the API in practice.
In this paper, we define API Basic as the amalgamation
of API Name, API Signature, and API Description.
4. Framework
DrawinginspirationfromhowprogrammersleverageAPI
documentationtotackleprivatelibraryscenarios,ourframe-
work is devised. Programmers, as a common practice, first
find and locate suitable APIs in API documentation based
on their programming problems, then learn to invoke these
APIs to resolve them correctly. Analogously, as depicted
in Figure 4, our framework also consists of two modules:
APIFinder ÓàπF,whichfindsfeasibleAPIs Óà≠,andAPICoder
ÓàπC, which invokes these APIs in an appropriate manner.
The procedure can be expressed as:
Óà≠=ÓàπF(ùê±);ùê≤=ÓàπC(Óà≠;ùê±), (1)
where ùê±andùê≤denote code context and target code, Óà≠‚àãùêö
referstoinformationofallretrievedAPIs. Inthisstudy,one
of the critical contributions is to investigate which compo-
nents in API documentation are beneficial for current code
generation models and how to utilize them better. There-
fore,theinformationofanAPI ùêöreferstooneormorecom-
ponents in Figure 3. In essence, our proposed framework
solvesprivatelibraryscenariosbydividingtheoriginalone-
time code generation into retrieval and generation modules,
whicheffectivelyempowerscodegenerationmodelstohan-
dle private libraries.
5. Methodology
Ourproposedframeworkhaspresentedasimpleyetnovel
idea for empowering language models to generate private
code snippets. In the following, we delve into the practical
implementationoftheframework,includingthepreparation
of the corpus and the finer design details of both APIFinder
and APICoder.5.1. Corpus Preparation
Inordertotrainaneuralmodelforprivatelibraryscenar-
ios,anintuitiveapproachistocollectaconsiderablebodyof
privatelibrarycorporafortraining. Evidently,obtainingpri-
vate library corpora is not feasible. Therefore, we can only
trainourmodelonalargecorpusofpubliclibrariesinhopes
of generalizing them to private ones.
In this study, we determine the 31most popular Python
code libraries, such as Pandas, NumPy, and PyTorch, based
on their popularity in Stack Overflow2. The names of these
libraries and their corresponding API count details can be
foundinTable1. Fortheselibraries,wecrawltheirAPIdoc-
umentation individually. Furthermore, we decompose each
APIinAPIdocumentationintosub-componentssuchasAPI
name, signature, description, and examples. In addition to
the API documentation of these libraries, we also require
a substantial collection of code snippets invoking these li-
braries. Thus, we initially gathered a 330GB code corpus
fromGitHub,containing 60.62millionPythonfiles. Aftera
series of data pre-processing strategies, including selecting
Python files related to the 31code libraries, de-duplication,
andformatting,weultimatelyacquiredapproximately 25GB
of Python files comprising a total of 4.54million files, re-
ferred to as Óàº.
5.2. APIFinder
Givenaprogrammingproblemdescription,APIFinderis
responsibleformatchingandfindingpotentiallyusableAPIs
from private library‚Äôs API documentation.
Training. APIFinder can be trained using dense retrieval
techniques[23,45,48,59]. Denseretrievaltechniquesmainly
consistoftwoprimaryapproaches: single-encoderanddual-
encoder. Upon receiving a programming problem descrip-
tion, single-encoder models require re-computing the em-
beddings of each API of API documentation, while dual-
encoderavoidsthisbypre-computingtheseAPIsoffline. Con-
sideringpracticalityandefficiencyduringinference,wethus
choose to train APIFinder using the dual-encoder one. To
trainsuchamodel,acollectionofprogrammingproblemde-
scriptions and their corresponding APIs is essential. Thus,
we devise a meticulous strategy to extract these data pairs
from our collected GitHub corpus Óàº. Initially, we separate
each file ùê©inÓàºintoùêæcode blocks (ùê©1,ùê©2,‚ãØ,ùê©ùêæ), where
a code block is a well-formed and uninterrupted code snip-
pet such as a method or class. This functionality can be au-
tomatically implemented via some pip tools, including au-
topep8, docformatter, and redbaron. As shown in Figure 5,
for each code block ùê©ùëñ, we extract its annotation and API
names. ForeachAPIname,welocateitscorrespondingAPI
signature and description by searching the API documenta-
tionwecrawled. NotethatanAPInamemaymatchmultiple
APIs. For instance, the API name headin pandas can cor-
respond to both DataFrame.head and Series.head . Upon our
empiricalobservation,weconcludethattheseAPIswiththe
same API name are similar or even identical except for the
2https://stackoverflow.com/tags?tab=popular
Daoguang Zan et al.: Preprint submitted to Elsevier Page 4 of 18

--- PAGE 6 ---
Private-Library-Oriented Code Generation with Large Language Models
Table 1
The 31public libraries and their corresponding API counts.
Pandas NumPy sklearn PyTorch TensorFlow Django selenium Matplotlib Flask SciPy Seaborn ansible
7,094 12,085 53,166 124,902 32,116 24,375 4,842 439,913 31,867 153,359 161,477 40,839
NLTK BeatifulSoup pygame PIL jieba Gensim spaCy transformers fairseq SQLAlchemy Scrapy requests
206,816 22,519 70,396 127,212 26,620 37,331 239,945 652,913 158,721 54,765 3,537 39,333
AllenNLP datasets tokenizers MXNet imageio pytest MetPy
276,088 136,843 195 142,070 175,878 1,047 27,429
import pandas aspd
defget_head_and_duplicated (df, n=5):
"""
Return the first n rows and drop the 
duplicated rows.
"""
return df.head( n).drop_duplicates()Return the first n rows and drop the 
duplicated rows.[1].head (n:int=5):Return thefirstnrows .
[2].drop_duplicates (subset=None ,*):Return
DataFrame with duplicate rows removed .BERTAPI Documentation
BERTAPI
Embedding
Comment
EmbeddingCross
Entropy
APIFinder
[1] or [2] Code Block ùíëùíä
Figure 5: An overview of the preparation of training corpus for APIFinder.
API path. Therefore, when encountering an API name cor-
responding to multiple APIs, we randomly pink one. For
each code block ùê©ùëñ, we have obtained multiple data pairs
of programming problem description ùêùand API informa-
tion ùêö, where each ùêöincludes its API name, signature, and
description3. We regard the extracted ùêùandùêöfrom a code
block as a positive sample. For the negative ones, we ran-
domly select ùëõAPIs irrelevant to ùêù, with ùëõset to 8in our
experiments. Altogether,weextract 40.37millioninstances
(ùêù,ùêö,ÃÇùêö1,ÃÇùêö2,‚Ä¶,ÃÇùêöùëõ)totrainourAPIFinder. FollowingRock-
etQA[45],weencode ùêùandùêöinto768-dimensionalembed-
ding separately using two BERT-base models [21] named
ùê∏ùêùandùê∏ùêö. We then compute the similarity between these
two embeddings using the dot product and optimize it with
cross-entropy loss.
Inference. After successfully training our APIFinder on a
vast array of public libraries, we can leverage it to retrieve
private APIs from API documentation. In our practical pri-
vate library scenarios, the API documentation is typically
considered a stable offline resource that remains unchanged
except for version updates. Therefore, ùê∏ùêöcan pre-encode
all APIs in the API documentation offline and index them
in FAISS [30] for efficient retrieval. Upon receiving a new
programming problem description, we merely compute its
embedding using ùê∏ùêù, and subsequently retrieve the nearest
API(s) from the offline pre-encoded APIs according to spa-
tial proximity.
User‚Äôs Optional Engagement with APIFinder. These
APIs retrieved by APIFinder can be directly input to API-
Coder for generating code that invokes private libraries. To
provide APICoder with more accurate APIs, we design a
pipelinethatallowsuserengagementwithAPIFinder. Specif-
ically,asshowninFigure6,wefurnishuserswithaninterac-
3In all our experiments, we solely use the first sentence of the API
description, as it sufficiently summarizes the content.
from torchdata .datapipes .iterimport IterableWrapper
source_dp = IterableWrapper (range (5))
# Randomize and clone the source datapipe two times.
dp1, dp2 = source_dp .
Programming Problem
Which APIs would youliketousein ?
concat :Concatenate multiple Map DataPipes.
shuffle : Shuffle the input MapDataPipe via its indices.
fork: Creates multiple instances of the same DataPipe.
demux : Splits the input DataPipe into multiple ones.
batch : Create mini -batches of data.
None of the above.
Not sure.
Figure 6: User interaction interface with APIFinder: users can
choose one or more APIs from APIFinder‚Äôs top 5recommen-
dations for APICoder to use.
tiveinterfacethataffordsthemtoselectoneormorefromthe
top5APIs retrieved by APIFinder. We also provide "None
of the above" and "Not sure" as options for the user if they
find the APIFinder retrieval results lacking a definitive an-
swer or uncertain. If "None of the above" is selected, API-
CoderwillpromptnoAPIs. If"Notsure"ischosen,thefirst
tworetrievedAPIswillbepromptedtoAPICoderbydefault.
Inourdesignedinterface,wepresentonlytheAPInameand
thefirstsentenceofitsdescription,asexcessiveinformation
may result in reader fatigue and a negative impact.
5.3. APICoder
APICoder leverages the APIs retrieved by APIFinder to
write private code. Prior to invoking these APIs, program-
mers are required to learn them. Similarly, large language
models also require incorporating these APIs into the code
Daoguang Zan et al.: Preprint submitted to Elsevier Page 5 of 18

--- PAGE 7 ---
Private-Library-Oriented Code Generation with Large Language Models
defget_head (df, n=5):
"""
Return the first n rows
"""
return df.head( n)
head (n:int=5):
Return thefirstnrows .API Documentation
API Examples:
>>> df= pd.DataFrame (['a', 'b', 'c'])
>>> df.head (2)
0
0      a
1      b# Please use the following APIs to solve the task:
# 
# 
# Overall, you can use the above APIs to solve the task.
# Please use the following APIs to solve the task:
# 
# 
# Overall, you can use the above APIs to solve the task.
# Also, p lease refer to the example sto solve the task:
# 
# 
# Overall, you can refer the above examples.Code block:Prompt#1
Prompt#3Noise API
# Please refer to the example sto solve the task:
# 
# 
# Overall, you can refer the above examples.Prompt#2Base Model
Base Model
Base ModelCodeGenAPI
Continual
Pre-trainingNoise API Extract APIs
Get their detailsAPI BasicShuffle these APIs
Figure 7: An overview of the preparation of training corpus for CodeGenAPI . For visual simplicity, we use colors instead of
text of the left to illustrate how to design prompts of the right. We utilize ‚ñ†and‚ñ†to represent the noise API, with their format
being consistently ‚ñ†and‚ñ†, respectively.
context before invocation via a well-designed prompt.
Off-the-shelfAPICoder. Retrieval-basedgenerationfacil-
itates the powerful generative capability of new samples in
LLMsviatheprovisionofcontext-richguidance. Therefore,
technicallyspeaking,existingcodegenerationmodels,such
as Codex [14], InCoder [24], and C ODEGEN[41], can be
useddirectlyasourAPICoderbyprovidingrichprivateAPI
information.
TraininganAdvancedAPICoder. Off-the-shelfcodegen-
erationmodelscanleverageretrieval-basedgenerationtoin-
voke private APIs, yet there is significant potential for im-
provementastheyhavenotbeenexplicitlytrainedonhowto
invoke APIs. Thus, to further enhance the code generation
performance of private libraries, we propose an intriguing
idea to continually train an advanced APICoder. To bring
this idea to fruition, we require a substantial training cor-
pus comprising code blocks along with their corresponding
API information. However, the existing GitHub corpus of-
ten lacks API information prior to each code block. Con-
sequently, we add relevant API information preceding each
code block to continually pre-train off-the-shelf models. In
our experiments, we use C ODEGEN[41] as our base model
as it is widely used among publicly available models. We
name the trained model as C ODEGENAPI. For the training
corpus, we first segment each Python file ùê©‚ààÓàºintoùêæ
code blocks (ùê©1,ùê©2,‚ãØ,ùê©ùêæ), as done in APIFinder. Then,
weextractrelevantAPIs Óà≠ùëñforeachcodeblock ùê©ùëñ. Finally,
we concatenate the extracted APIs to the front of each code
block to obtain the corpus (Óà≠1,ùê©1,Óà≠2,ùê©2,‚ãØ,Óà≠ùêæ,ùê©ùêæ). As
stated in Section 3.2, API documentation includes variouscomponentssuchasAPIBasics,Examples,andParameters.
PlacingallcomponentsofeachAPIinfrontofthecodeblock
is impractical due to excessive length. Thus, as shown in
Figure 7, we design three prompts to train C ODEGENAPI
separately: API Basic, API Examples, and both combined.
Moreover, to improve the robustness of C ODEGENAPI, we
incorporate noise APIs with a probability of 5% and shuf-
fletheseAPIsin Óà≠,astheAPIsreturnedbyAPIFindermay
contain incorrect APIs and are unordered. During the train-
ing of C ODEGENAPI, we employ a re-sampling strategy.
This strategy prioritizes high-quality Python files in sam-
pling and reduces the selection of low-quality files. Specif-
ically, the re-sampling weight ùë§of each Python file can be
defined as:
ùë§=ùë§api√óùë§star√óùë§ut,
ùë§api= 5.0 ‚àí log(ùëÄapi
ùëÅapi).clip(0 ,5) √ó 0 .2,
ùë§star= 1.0 + log( ùëÅstar+ 1).clip(0 ,5) √ó 0 .2,
ùë§ut= (0.5 + (1 ‚àí ùëÖut)).clip(0 ,1),(2)
where .clip(ùë•, ùë¶)restricts the value to a range [ùë•, ùë¶],ùëÅapi
denotes the count of API names in this file, ùëÄapirepresents
thecountofAPIsincaseswhereasingleAPInamematches
multiple APIs, ùëÅstarsignifies the number of stars for the
repository, and ùëÖutis the unit test function rate, calculated
as the division of unit test functions by the total functions.
Inference. Both off-the-shelf and our advanced APICoder
are Transformer-based generative language models. During
inference,ourAPICoderpredictstheprobabilitydistribution
of the next token based on the given code context until en-
Daoguang Zan et al.: Preprint submitted to Elsevier Page 6 of 18

--- PAGE 8 ---
Private-Library-Oriented Code Generation with Large Language Models
countering one of the following pre-defined stop markers:
‚Äú\nclass‚Äù, ‚Äú\ndef‚Äù, ‚Äú\nprint‚Äù, ‚Äú\n#‚Äù, or ‚Äú \nif‚Äù. After pre-
dictingtheprobabilitydistribution,werequirethedecoding
of these probabilities. Currently, generative language mod-
els offer a variety of decoding techniques, such as tempera-
ture decoding, greedy decoding, and nucleus decoding. In
code generation scenarios, temperature decoding is widely
adopted[14,35]becauseitallowsforasuitablebalancebe-
tween diversity and quality in generated code, while other
decodingtechniquescannot. Consequently,allofourexper-
iments use temperature decoding for inference.
6. Benchmark Construction
In software development, the use of private libraries is
a prevalent practice. However, as of yet, no benchmarks
for private libraries have been crafted. A primary reason
for this is that crafting a comprehensive benchmark for pri-
vate libraries poses many considerable challenges. Specif-
ically, these challenges are manifested in the following as-
pects: (1)ensuringthatLLMshavenotseenthelibrarydur-
ingtraining,(2)providinghigh-qualityimplementationcode
andAPIdocumentation,(3)offeringprogrammingproblems
ofvaryingdifficulties,and(4)annotatingcomprehensivetest
cases for each programming problem. To tackle the above
challenges,wereleasefourbenchmarksforprivatelibraries,
namelyTorchDataEval,TorchDataComplexEval,MonkeyE-
val, and BeatNumEval.
TorchDataEval. Aftersurveyingnumerouscodelibraries,
we chose TorchData4, released in May 2022, as our private
libraryforitshigh-qualityimplementationcodeandcompre-
hensive API documentation. More importantly, all models
inourexperiments,includingCodexandC ODEGEN,arepre-
trained on the GitHub corpus prior to the aforementioned
release date, ensuring that the LLMs have no prior expo-
sure to the library. Before crafting programming problems,
wethoroughlystudiedtheAPIdocumentationandhands-on
eachAPItoguaranteeourproficiencywiththelibrary. Sub-
sequently, we manually crafted 50programming problems
and provided comprehensive test cases for each. Addition-
ally, we invited two volunteers with over 3years of Python
codingexperiencetoreviewthemtoensuretheirsoundness
and correctness. In fact, our worry is that the complexity of
the programming challenges may surpass the capability of
the LLM. Therefore, we control the difficulty level of these
programming problems to be relatively simple. Concretely,
programming problems that include 1or2APIs make up
approximately 90%.
TorchDataComplexEval. Astoundingly, our experiments
revealthatLLMscanperformrelativelywellonTorchDataE-
val, indicating their capability to generate some simple pri-
vate code snippets. One research question thus arises: can
LLMscopewithmorecomplexprivatelibraries? Therefore,
we developed a more complex benchmark base on Torch-
4https://pytorch.org/data/beta/index.htmlData,consistingof 50programmingproblems,namedTorch-
DataComplexEval. Unlike TorchDataEval, which manually
creates programming problems by referencing examples in
APIdocumentation,TorchDataComplexEvaldirectlyadapts
practicalprojectsonGitHub. Specifically,wecarefullycon-
vert real-world projects into executable and well-annotated,
also equipped with comprehensive test cases. Due to the
intricacy of these projects, TorchDataComplexEval‚Äôs pro-
grammingproblemstypicallyinvolveahighvolumeofAPIs,
oftenexceeding 10. TorchDataisalibraryforbuildingdata
pipelines that can handle diverse data modalities. Consid-
ering the potential bias among these modalities, TorchDat-
aComplexEvalencompassescommononesliketext,vision,
and audio. We also invited two volunteers to review these
programmingproblems,asdoneincraftingTorchDataEval.
MonkeyEval&BeatNumEval. Asdiscussedabove,aqual-
ified private library possesses rigorous criteria. So, discov-
ering a suitable private library like TorchData proves to be
exceptionally challenging. Hence, besides TorchDataEval
andTorchDataComplexEval,wecreatedtwopseudo-private
librarybenchmarks,namedMonkeyEvalandBeatNumEval,
by adapting two off-the-shelf public ones [63], named Pan-
dasEvalandNumpyEval,eachwith 101programmingprob-
lems. WemanuallyparaphrasedthekeywordsinPandasEval
and NumpyEval. As depicted in Figure 1, we paraphrased
‚Äúpandas‚Äù to ‚Äú monkey‚Äù, ‚Äúdataframe ‚Äù to ‚Äú knowledgeframe ‚Äù, and
‚Äúisin‚Äù to ‚Äú iscontain ‚Äù. Further details on the paraphrasing
areprovidedinAppendixA.Besidesthekeywords,wealso
thoroughlyrephrasedtheirAPIdocumentationtoensurethat
LLMs have not seen them.
7. Experiments
In this section, we will showcase a thorough evaluation
of our proposed methodology through a sequence of care-
fully designed experiments. Specifically, we will initially
outline the experimental setup, subsequently delving into a
comprehensive presentation of our findings.
7.1. Experimental Setup
7.1.1. Baselines
Ourcontributionscanbeviewedfromtwoperspectives:
APIFinder and APICoder. With regards to APIFinder, we
proposearetrieval-augmentedgenerativemodelforcodegen-
eration in private libraries. Therefore, the baselines are all
code generation models with No API setting, while we pro-
poseOracle,Top ùêæ,andHumansettings. Fromtheperspec-
tive of APICoder, we propose a novel idea to build an ad-
vanced model by continually pre-training the vanilla one.
So, vanilla models are the baselines for their advanced ver-
sions. Overall, we do comprehensive comparisons among
17popularcodegenerationmodels,suchasC ODEGEN[41],
GPT-CC [19], InCoder [24], CodeGPT [38], CodeT5 [57],
CodeParrot [27], SantaCoder [2], PyCodeGPT [63], Poly-
Coder [60], and OpenAI‚Äôs code-davinci-002 [14]. For the
sake of brevity, we abbreviate the code-davinci-002 model
as Codex in the following sections.
Daoguang Zan et al.: Preprint submitted to Elsevier Page 7 of 18

--- PAGE 9 ---
Private-Library-Oriented Code Generation with Large Language Models
Table 2
Pass@ ùëò(%) of 17code generation models on four private library benchmarks. No API, Oracle, TopN, and Human denote
using no API, perfect APIs, APIs retrieved from APIFinder, and APIs selected from Top5 by humans as extra prompt. B,
E, and BE denote using API‚Äôs basic information (API name, signature, description), examples, and both as extra prompt.
Values in red/green represent improvement/decline relative to the No API setting.
APICoder APIFinderTorchDataEvalTorchData
ComplexEvalMonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
CodeGPT 124M Top2/B 0.48 2.83 0.00 0.00 0.18 1.02 0.12 0.96
GPT-CC 125M Top2/B 0.00 0.00 0.00 0.00 0.04 0.34 0.12 0.92
GPT-CC 1.3B Top2/B 0.00 0.00 0.20 1.34 0.00 0.00 0.10 0.66
CodeParrot 110M Top2/B 1.96 9.32 0.08 0.76 0.55 1.84 2.69 10.56
CodeParrot 1.5B Top2/B 4.00 12.25 0.00 0.00 1.58 3.97 2.48 6.07
PyCodeGPT 110M Top2/B 4.36 16.66 2.12 6.19 2.26 9.10 5.80 19.17
CodeT5 770M Top2/B 16.62 33.47 3.60 5.95 4.16 14.88 9.60 24.23
PolyCoder 160M Top2/B 3.38 9.43 0.00 0.00 0.83 4.16 2.90 12.25
PolyCoder 400M Top2/B 3.60 12.57 0.00 0.00 0.99 3.70 4.70 16.34
PolyCoder 2.7B Top2/B 5.60 14.28 0.40 2.68 1.68 6.42 5.05 15.69
InCoder 1.3B Top2/B 6.00 15.59 0.80 3.62 3.56 13.93 5.35 16.66
InCoder 6.7B Top2/B 7.00 22.63 2.80 4.00 5.25 18.49 8.42 27.90
SantaCoder 1.1B Top2/B 19.20 32.62 2.40 4.00 4.16 12.91 11.68 22.77
CodeGen 350MNo API 4.40 17.76 1.12 5.79 2.38 9.77 6.24 24.58
Oracle/B 7.222.8228.5310.771.360.246.520.733.290.9111.381.6111.194.9528.924.34
Oracle/E 13.128.7230.8413.081.920.806.710.923.411.0315.415.646.530.2923.180.78
Oracle/BE 12.067.6631.6613.901.760.648.622.832.650.2711.611.847.521.2824.810.23
Top1/B 6.041.6421.733.971.280.166.210.423.370.9911.021.258.872.6324.280.30
Top2/B 5.721.3220.362.601.360.247.201.413.761.3812.392.629.313.0724.330.25
Top2/E 6.662.2616.391.371.120.006.750.962.420.0410.841.075.500.7416.378.21
Top2/BE 5.441.0418.100.341.440.327.661.871.990.399.220.554.281.9614.739.85
Top3/B 6.281.8822.064.301.200.087.491.703.841.4611.611.849.072.8325.821.24
Top5/B 7.042.6425.017.251.120.006.630.844.081.7012.742.978.932.6926.261.68
Human/B 6.622.2226.038.271.440.326.811.023.350.9711.782.0111.395.1530.125.54
CodeGen 2BNo API 8.80 20.92 5.60 13.24 4.65 12.47 8.71 30.21
Oracle/B 18.8010.0042.9322.015.600.0013.590.357.232.5817.254.7816.738.0237.797.58
Oracle/E 23.4014.6041.9621.047.001.4018.495.255.741.0914.081.6110.691.9826.463.75
Oracle/BE 24.0015.2043.5522.636.400.8018.595.355.741.0911.490.989.500.7930.820.61
Top1/B 10.201.4027.987.065.600.0013.610.376.932.2817.875.4013.074.3631.581.37
Top2/B 13.004.2035.0414.125.800.213.780.548.423.7720.127.6512.974.2630.820.61
Top2/E 17.008.2030.749.825.800.213.230.014.550.1012.920.457.521.1918.0312.18
Top2/BE 16.007.2031.1710.256.801.217.213.974.950.3012.490.023.964.7510.2219.99
Top3/B 12.603.8036.1915.274.401.212.610.638.123.4718.586.1111.582.8730.290.08
Top5/B 10.601.8033.9513.035.200.413.600.369.414.7620.237.7611.783.0732.161.95
Human/B 12.203.4029.518.595.880.2813.600.367.132.4817.014.5416.247.5335.775.56
CodeGen 6BNo API 9.28 27.63 6.40 13.68 5.26 14.59 12.58 33.80
Oracle/B 24.7215.4447.3219.697.240.8413.900.228.533.2720.105.5117.825.2440.176.37
Oracle/E 24.0414.7644.7117.086.800.4019.515.836.240.9818.103.5113.380.8033.840.04
Oracle/BE 25.0015.7246.9919.368.341.9420.416.736.991.7316.451.8613.380.8029.214.59
Top1/B 19.3410.0632.534.906.960.5612.551.137.452.1919.625.0315.843.2636.302.50
Top2/B 19.3610.0838.1510.526.480.0813.980.309.043.7823.518.9214.161.5834.901.10
Top2/E 22.7213.4436.789.157.040.6415.671.997.522.2615.761.179.213.3724.759.05
Top2/BE 21.6012.3234.046.417.901.5020.406.727.041.7816.942.358.194.3919.5414.26
Top3/B 22.1012.8236.819.185.760.6411.452.238.793.5322.938.3413.550.9734.640.84
Top5/B 18.208.9232.274.646.490.0913.960.2810.565.324.169.5714.091.5135.111.31
Human/B 19.3210.0436.408.777.350.9514.630.959.524.2621.526.9318.315.7339.445.64
CodexNo API 8.08 24.47 6.12 12.58 3.40 10.84 20.18 60.28
Oracle/B 44.1036.0273.0748.609.022.9019.316.7315.9312.5345.6034.7631.5411.3668.658.37
Oracle/E 38.1830.1068.1043.6312.626.5029.7717.1916.1612.7645.0034.1627.807.6266.245.96
Oracle/BE 44.8036.7267.5743.1015.809.6829.6617.0819.1615.7653.9643.1232.8412.6671.5011.22
Top1/B 14.826.7438.0713.607.341.2216.423.8413.7310.3339.2228.3824.884.7058.661.62
Top2/B 19.0610.9849.7025.236.780.6615.603.0215.9612.5641.7630.9227.096.9160.760.48
Top2/E 23.9415.8646.2621.7911.125.0021.178.5914.9811.5841.4130.5720.970.7957.752.53
Top2/BE 24.8216.7451.4326.9613.327.2021.649.0615.6912.2942.4031.5620.260.0854.965.32
Top3/B 19.3211.2449.5525.087.621.5018.756.1713.279.8741.1930.3526.966.7862.632.35
Top5/B 20.3612.2852.7128.247.181.0616.333.7517.0013.6045.9435.1026.816.6363.523.24
Human/B 15.247.1640.6816.216.980.8618.575.9914.9711.5739.8829.0429.199.0164.253.97
Daoguang Zan et al.: Preprint submitted to Elsevier Page 8 of 18

--- PAGE 10 ---
Private-Library-Oriented Code Generation with Large Language Models
7.1.2. Evaluation Metrics
Weadoptpass @ùëòasourevaluationmetricinaccordance
with Codex [14]. For each problem, we sample ùëõ= 100
candidate code snippets from the LLM. Then, we count the
number ùëêof correct ones by running on test cases. Pass @ùëò
can be formalized as:
pass@k ={
1 ifùëõ‚àíùëê < ùëò
1 ‚àí‚àèùëõ
ùëñ=ùëõ‚àíùëê+1(1 ‚àíùëò
ùëñ)otherwise,(3)
where ùëò‚àà {1,10,100}inourstudy. Besidespass @ùëò,other
metrics also exist, such as ROUGE [36], BLEU [43], and
CodeBLEU[47]. Wechosepass @ùëòasourevaluationmetric
insteadofothersbecauseitcanprovideacompletelyprecise
evaluation of code accuracy by executing test cases, while
others do not.
7.1.3. Implementation Details
We use Dense5, a toolkit for training dense retriever, to
implement the APIFinder. Regarding the training details of
APIFinder,theratioofpositivetonegativesamplesissetto
1:8, with a batch size of 10per GPU card, a learning rate
of1e-5and optimization via the Adam algorithm [32]. Our
training duration was approximately 74hours on an 8GPU
NVIDIA V 100(32GB) cluster, with a total of 100K steps.
For CODEGENAPI, we totally train 9versions with C ODE-
GEN350M,2B,and 6B,eachtrainingthethreepromptsde-
pictedinFigure7. Theseversionsexactlyfollowtheoriginal
hyperparameters of C ODEGENfor continuous pre-training.
We use DeepSpeed [46] to train C ODEGENAPI with FP 16
precision and employ an 32GPU NVIDIA A100 ( 48GB)
cluster. During the inference of all APICoder models, the
number of samples is set to 100, the temperature to 0.8, the
maximum length of new sequences to 300, and the top-p to
0.95.
7.1.4. Experimental Configurations
WhichAPIsshouldbeinputtoAPICoderforaprogram-
mingproblem? Inourexperiment,weprovidethefollowing
four settings:
‚Ä¢No API: no API is input to APICoder.
‚Ä¢Oracle: theground-truthAPIsareinputtoAPICoder.
‚Ä¢Topùêæ: The first ùêæAPIs retrieved by APIFinder are
input to APICoder, where ùêæ‚àà {1,2,3,5}.
‚Ä¢Human: APIFinder offers the top 5APIs for user se-
lection (Figure 6), and the APIs chosen by a user are
input to APICoder.
Which components in the API documentation should be
placedintheprompt? Infact,wefindthatsomecomponents
inAPIdocumentation,includingAPIparametersandrelated
APIs,significantlydevastatetheperformanceofLLMs. We
thus only consider the following three types of prompts:
5https://github.com/luyug/DenseTable 3
Pass@ ùëò(%) of various components of API documentation us-
ingCodeGen 350M in the ‚ÄúOracle‚Äù setting.
API Doc.TorchDataEvalTorchData
ComplexEval
pass@1 pass@10 pass@1 pass@10
Basic 7.22 28.53 1.36 6.52
Examples 13.12 30.84 1.92 6.71
Parameters 3.95 14.13 0.85 3.22
Related APIs 4.15 14.48 0.86 4.46
‚Ä¢APIBasicOnly(B):thepromptonlyincludesAPIba-
sicconsistingofAPIname,signature,anddescription.
‚Ä¢APIExamplesOnly(E):thepromptonlyincludesAPI
examples.
‚Ä¢API Basic and Examples (BE): the prompt includes
both API basic and examples.
7.2. Main Results
Inthissection,wefocusonfouressentialresearchques-
tions(RQs)toevaluatetheeffectivenessofourproposedap-
proach in the private library scenario.
RQ1: ‚ÄúWhether off-the-shelf LLMs possess the poten-
tialtoinvokeprivateAPIs?‚Äù Thisaimstoverifythefea-
sibility of LLMs in addressing the private library scenario.
Specifically, we furnish these LLMs with the ground truth
(oracle) private APIs and assess their capability to invoke
them correctly. Table 2 reveals a substantial improvement
when providing oracle APIs compared to no APIs, indicat-
ing the potential of LLMs for invoking private APIs. In-
triguingly, even the giant model, codex, performs poorly in
invoking private APIs without prompting any APIs, further
emphasizing the necessity of our approach. Excitingly, as
the parameter size increase (C ODEGEN350M < CODEGEN
2B<CODEGEN6B<Codex),thebenefitsfromprompting
APIs grow commensurately. For instance, Codex yields a
48.60% pass@10 gain in TorchDataEval in the ‚ÄúOracle/B‚Äù
setting, while C ODEGEN350M merely achieves a 10.77%
increase.
RQ2: ‚ÄúWhich components in API documentation are
moreusefulforLLMs?‚Äù Aspreviouslyshown,LLMspos-
sessthepotentialtoinvoketheprivateAPIsviapromptAPI
information in API documentation. So, which information
in API documentation is crucial for maximizing the perfor-
mance of LLMs? We separately prompt each component
mentioned in Section 3.2 to LLM. The results in Table 3
demonstrate that API basic and examples are more benefi-
cial compared to API parameters and related APIs. This is
reasonable as the former two components directly provide
definitions or invocations for the API, while the latter two
do not. Therefore, in this paper, we primarily focus on API
basic and examples.
Daoguang Zan et al.: Preprint submitted to Elsevier Page 9 of 18

--- PAGE 11 ---
Private-Library-Oriented Code Generation with Large Language Models
Table 4
Pass@ ùëò(%) of our proposed CodeGenAPI on four private library benchmarks. #1, #2, and #3 denote CodeGenAPI
trainedwiththethreepromptsinFigure7. Redandgreenvaluesindicateimprovementsanddeteriorationsof CodeGenAPI
compared to CodeGen under the same settings, respectively.
APICoder P. APIFinderTorchDataEvalTorchData
ComplexEvalMonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
CodeGenAPI
350M#1Oracle/B 16.549.3238.9510.422.160.808.101.585.812.5215.273.8912.531.3431.973.05
Top1/B 11.535.4927.766.032.020.747.571.366.252.8815.054.0310.331.4627.633.35
Top2/B 9.093.3724.924.562.180.828.421.226.212.4517.014.6210.351.0426.892.56
Top3/B 9.493.2125.313.251.990.798.611.126.202.3615.073.4610.321.2528.953.13
Top5/B 11.033.9928.953.941.540.427.460.835.581.5015.482.749.961.0329.303.04
Human/B 14.978.3535.869.832.581.148.751.946.302.9516.454.6712.921.5333.563.44
#2Oracle/E 26.6413.5247.1616.323.171.258.661.956.553.1420.565.156.580.0524.140.96
Top2/E 11.805.1423.657.262.110.998.271.524.972.5515.534.695.43-0.0716.620.25
#3Oracle/BE 23.5111.4545.5613.903.001.2410.672.055.552.9016.464.857.870.3526.701.89
Top2/BE 10.284.8423.265.162.501.069.621.963.031.0412.333.114.420.1416.311.58
CodeGenAPI
2B#1Oracle/B 33.3214.5259.1816.258.342.7417.844.2513.526.2925.608.3520.473.7445.147.35
Top1/B 20.3010.1039.5311.558.262.6617.654.0414.677.7427.299.4216.603.5338.466.88
Top2/B 20.457.4544.149.108.602.8017.573.7913.675.2528.348.2215.942.9736.856.03
Top3/B 19.797.1945.619.427.162.7616.243.6314.896.7727.689.1014.633.0536.786.49
Top5/B 18.758.1544.5210.577.141.9416.733.1314.925.5127.657.4213.741.9637.124.96
Human/B 24.3712.1745.0715.568.732.8518.224.6214.177.0425.718.7020.003.7642.056.28
#2Oracle/E 39.5416.1461.0519.0911.024.0223.484.9913.367.6223.299.2112.141.4527.501.04
Top2/E 26.249.2443.1912.459.713.9117.264.0311.196.6420.497.577.37-0.1517.96-0.07
#3Oracle/BE 38.3614.3659.8216.2710.744.3422.864.2712.596.8517.626.139.45-0.0533.232.41
Top2/BE 23.157.1542.2911.1210.263.4620.162.9510.185.2317.034.546.412.4517.126.90
CodeGenAPI
6B#1Oracle/B 45.2320.5171.4124.0913.266.0223.549.6417.579.0432.6712.5725.167.3453.0412.87
Top1/B 34.6915.3549.6717.1413.426.4621.859.3018.9711.5234.4814.8621.936.0945.759.45
Top2/B 33.9814.6253.9215.7711.745.2622.238.2518.509.4637.4413.9320.025.8644.499.59
Top3/B 35.3513.2551.3414.5312.016.2518.787.3319.3610.5735.7412.8119.576.0243.158.51
Top5/B 32.4714.2747.0214.7512.756.2619.735.7720.079.5135.7611.6019.215.1242.677.56
Human/B 35.6616.3452.0315.6314.296.9423.368.7320.9111.3935.2413.7225.317.0049.9010.46
#2Oracle/E 46.6022.5669.8525.1415.778.9726.627.1116.2710.0329.5611.4619.145.7638.094.25
Top2/E 37.8615.1453.4516.6714.507.4623.627.9516.989.4625.759.999.780.5726.211.46
#3Oracle/BE 45.4620.4666.5419.5516.768.4228.418.0017.5610.5729.5613.1113.380.0030.731.52
Top2/BE 36.8515.2552.3718.3314.776.8727.627.2215.508.4627.8910.9511.753.5627.207.66
RQ3: ‚ÄúCanAPIFindereffectivelyretrieveusefulAPIs?‚Äù
Prompting LLMs with oracle APIs can unlock the poten-
tialtoinvokeprivateAPIs. However,providingoracleAPIs
is not practical. Therefore, whether our APIFinder can re-
trieve useful APIs? Table 2 indicates that all models with
TopùêæAPIsretrievedbyAPIFinderperformbetterthanthose
with No API setting. This observation demonstrates that
APIFinder is capable of retrieving useful APIs. Moreover,
APIFinder with human involvement generally exhibits su-
periorperformancebymanuallyselectingpotentiallyuseful
APIs. Surprisingly, the Top ùêæor Human setting may occa-
sionally outperformthe Oracle setting. Thismay stemfrom
the noisy APIs during the training of APICoder.
RQ4: ‚ÄúCanAPICodereffectivelyinvokeprivateAPIs?‚Äù
Table 2 shows that almost existing models like C ODEGEN
exhibit superior performance on four private library bench-
marks with prompting APIs compared to the No API set-
ting. Such observation demonstrates the capability of off-
the-shelf APICoder to invoke private APIs. Although these
modelshaveachievedsignificantadvancements,thereisstill
roomforfurtherimprovement,asindicatedbytherelatively
low values. In pursuit of more extraordinary performance,we develop a more advanced model named C ODEGENAPI
viacontinuouspre-trainingC ODEGEN. Specifically,wetrain
atotalofnineversions,threeforeachoftheC ODEGENmod-
els (350M,2B, and 6B), using three prompts as detailed in
Figure 7. Table 4 presents the performance of C ODEGE-
NAPI on four benchmarks. We observe that C ODEGENAPI
consistentlyoutperformsC ODEGEN. ThisindicatesthatC ODE-
GENAPI has strengthened its capability to invoke private
APIs via training on our crawled 31public libraries. Note
that BeatNumEval shows relatively limited gains from our
approach compared to other benchmarks. After a compre-
hensiveanalysis,wefindsomeproblemsinBeatNumEvaldo
notrequireinvokingAPIs,suchas‚Äò x[:,None]+y*8 ‚Äô,whileour
approach solely support explicitly invoked API calls, ren-
dering it ineffective. Overall, a vast array of experiments
revealthatourAPICoderpossessestheabilitytoinvokepri-
vate APIs.
7.3. In-Depth Study
Inthissection,wewilldelveintoacomprehensiveanal-
ysis of our proposed methodology through a wide array of
experiments,withtheaimofprovidingreaderswithvaluable
insights.
Daoguang Zan et al.: Preprint submitted to Elsevier Page 10 of 18

--- PAGE 12 ---
Private-Library-Oriented Code Generation with Large Language Models
T op1 T op2 T op3 T op4 T op530405060708090Recall Rate (%)
T orchDataEval
T orchDataComplexEval
MonkeyEval
BeatNumEval
PandasEval
NumpyEval
(a)
Human T op1 T op2 T op30102030405060Accuracy of APIs (%)T orchDataEval
T orchDataComplexEval
MonkeyEval
BeatNumEval (b)
Figure 8: (a) The recall rate (%) and (b) accuracy (%) of APIFinder on private library benchmarks under various settings.
7.3.1. Quality of Retrieved APIs
Providing high-quality APIs as prompts to APICoder is
crucial for generating private code snippets; we thus would
like to analyze the quality of APIs retrieved by APIFinder
or manually selected by users. We analyze the recall rate of
APIFinder on six benchmarks, and the results are displayed
in Figure 8a. We observe that the recall rate of the Top 5
surpasses 50% on all benchmarks. Therefore, it is reason-
abletoprovidethetop 5APIstousersduringtheAPIselec-
tion process (Figure 6). Particularly, TorchDataEval, Mon-
keyEval, and BeatNumEval exhibit Top 5recall rates close
to90%. This is primarily due to their API documentation
containing a relatively small number of APIs and their pro-
gramming problems being relatively simple. As shown in
Figure8b,wecomparetheaccuracyoftheAPIsselectedby
userswiththeAPIsretrievedbyAPIFinder. Here,wedefine
accuracyas 1ifallAPIsintheprogrammingproblemarere-
trieved,and 0otherwise. Theresultsdemonstratethatuser‚Äôs
engagement with APIFinder can lead to a notably positive
impact on accuracy. Meanwhile, we have noticed a signifi-
cantlyloweraccuracyofTorchDataComplexEvalcompared
tootherbenchmarks,evenwithhumaninvolvement,whereit
achievesamere 5%accuracy. Suchlowaccuracyhighlights
the challenge it poses.
Table 5
Performance comparison of APIFinder using dual-encoder and
single-encoder on two private library benchmarks with Codex
in the ‚ÄúTop2/B‚Äù setting.
APIFinderTorchDataEvalTorchData
ComplexEval
pass@1 pass@10 pass@1 pass@10
Dual-encoder 19.06 49.70 6.78 15.60
Single-encoder 19.73 50.63 6.46 15.98
7.3.2. Single-encoder vs. Dual-encoder
APIFinder uses dual-encoder by default. Technically,
single-encoder can also be employed in APIFinder. There-
fore,wecomparetheperformanceofsingle-encoderanddual-
2468CodeGen 350M
1235 10 20
K in T opK10152025Codex (Code 002)
T orchDataEval
T orchDataComplexEval
MonkeyEval
BeatNumEvalFigure 9: Pass@1(%) of CodeGen 350M and Codex in
various ùêæ‚àà 1,2,3,5,10,20in Top ùêæwith prompting API basic
(Topùêæ/B setting).
encoderonTorchDataEvalandTorchDataComplexEval(Ta-
ble 5). We observe that the single-encoder exhibits a slight
advantage over the dual-encoder in performance. However,
considering the minor performance gap and the inference
speed, we ultimately opt for the dual-encoder as the default
as outlined in Section 5.2.
7.3.3. Different ùêæin Top ùêæ
As the value of ùêæin Top ùêæincreases, not only does the
recall rate improve, but the noise introduced to APICoder
also escalates, and vice versa. Therefore, we would like to
determine the most suitable value of ùêæ. In detail, we com-
parethepass @1changesoftwomodelswithdifferentsizes,
under varying values of ùêæ, on four private library bench-
marks. As depicted in Figure 9, it is intriguing to highlight
Daoguang Zan et al.: Preprint submitted to Elsevier Page 11 of 18

--- PAGE 13 ---
Private-Library-Oriented Code Generation with Large Language Models
Table 6
Ablation study of CodeGenAPI 350M trained with ‚Äúprompt#1‚Äù in the ‚ÄúHuman/B‚Äù setting. The default noise rate is 5%.
APICoderTorchDataEvalTorchData
ComplexEvalMonkeyEval BeatNumEval
pass@1 pass@10 pass@1 pass@10 pass@1 pass@10 pass@1 pass@10
CodeGenAPI 350M 14.97 35.86 2.58 8.75 6.30 16.45 12.92 33.56
- w/ noise rate 0% 12.14 33.52 2.20 4.24 5.12 16.18 12.89 31.44
- w/ noise rate 10% 14.05 34.13 2.53 6.57 5.06 15.76 12.21 33.36
- w/ noise rate 20% 12.46 32.74 2.25 5.61 5.17 14.36 11.00 31.32
- w/o resampling 12.27 31.57 2.36 7.14 5.69 15.67 11.26 32.90
a contrasting disparity in the sensitivities of the two mod-
els to ùêæ. When ùêæexceeds 5, the performance of the small
modelC ODEGEN350Mdeterioratesacrossallbenchmarks,
whilethelargemodelCodexshowsstabilitywithnodecline.
Suchanobservationimpliesthatthelargemodelexhibitssu-
perior robustness to excessive noise compared to the small
one. Overall,thechoiceofasuitable ùêæshouldtakeintoac-
count factors such as the model parameters, the model per-
formance,andtheintrinsiccharacteristicsofthebenchmark.
7.3.4. Different Model Sizes
It is a well-known notion that emergent ability emerges
when model parameters are sufficient [58]. So, we would
like to investigate the effect of the magnitude of parameters
inAPICoderontheperformanceinprivatelibraries. Specif-
ically, we compare pass @1and pass @10on TorchDataE-
valandTorchDataComplexEvalusing 10modelsofvarying
sizes. TheresultsaredepictedinFigure10. Wecanfindthat
larger models generally result in improved performance in
privatelibraries. Unfortunately,performancestillremainsat
alowlevel,evenwithaconsiderablenumberofparameters.
Forexample,thegiganticmodel,Codex(code-davinci-002),
achieveonlya 15.60%pass @10onTorchDataComplexEval.
This phenomenon also confirms the formidable challenge
of private-library-oriented code generation. Furthermore,
we analyze the performance correlation of 11code gener-
ation models on both HumanEval [14] and private libraries
in Figure 11, where HumanEval is currently the most pop-
ular benchmark for evaluating code generation capabilities.
We observe that models that perform well on HumanEval
also stand out in the private library benchmarks. Hence, we
believe that our proposed private library scenario stands to
benefit from the rapid progress in general code generation
techniques.
7.3.5. Noise Rate
A carefully chosen noise rate is crucial for C ODEGE-
NAPI to handle a diverse range of APIs. If the noise rate
is excessively high, it will disrupt the original distribution;
conversely,itwilllosetheabilitytoaddressnoiseAPIsiftoo
low. WethusaimtoexploretheimpactofnoiserateonAPI-
Coder. The default noise rate for C ODEGENAPI is 5%, and
wealsoexperimentwith 0%,10%,and 20%inTable6. The
results indicate that 5% is the optimal choice, with too little
or excessive noise APIs causing a decrease in performance.
0 2 5 8 -05101520T orchDataEval pass@1
0 2 5 8 -01020304050T orchDataEval pass@10
0 2 5 8 -
Number of Parameters (Billion)0246T orchDataComplexEval pass@1
 0 2 5 8 -
Number of Parameters (Billion)051015T orchDataComplexEval pass@10
CodeGPT
GPT-CC
CodeParrotPyCodeGPT
CodeT5
PolyCoderInCoder
SantaCoderCodeGen
CodexFigure 10: Parameter size vs. pass @ùëò: a comparative analysis
of10popular models on TorchDataEval and TorchDataCom-
plexEval in the ‚ÄúTop 2/B‚Äù setting.
7.3.6. Re-sampling strategy
During the continuous pre-training of C ODEGENAPI,
we employ a re-sampling strategy. As mentioned in Sec-
tion5.3,thecoreideaofthisstrategyistomakehigh-quality
Python files more readily trainable and vice versa. To vali-
date the effectiveness of this strategy, we omit it during the
training of C ODEGENAPI350M, as shown in Table 6. The
resultsshowasustaineddeclineinperformance,demonstrat-
ing the validity of the re-sampling strategy.
7.3.7. Different Difficulty
APICoderhasthecapabilitytotackleprogrammingprob-
lems in private libraries. We are curious about what level
ofdifficultyAPICodercansolveinprivatelibraryprogram-
ming problems. Hence, we aim to access APICoder‚Äôs per-
formance with problems of varying difficulty through accu-
racy evaluations under varying API counts. To be specific,
wecalculatetheaccuracyoffourmodelsacrossvaryingAPI
counts on a combined set of our released four benchmarks.
The results are illustrated in Figure 12. Our finding is that
Daoguang Zan et al.: Preprint submitted to Elsevier Page 12 of 18

--- PAGE 14 ---
Private-Library-Oriented Code Generation with Large Language Models
0 20 4005101520T orchDataEval pass@1
0 20 4001020304050T orchDataEval pass@10
0 20 40
HumanEval pass@1 (%)0246T orchDataComplexEval pass@1 0 20 40
HumanEval pass@1 (%)051015T orchDataComplexEval pass@10CodeGPT 124M
GPT-CC 1.3B
CodeParrot 1.5B
PyCodeGPT 110MCodeT5
PolyCoder
InCoder 6.7B
CodeGen 350MCodeGen 2B
CodeGen 6B
Codex
Figure 11: Performance analysis of 11code generation models
on HumanEval [14] and TorchDataEval (TorchDataComplex-
Eval) in the ‚ÄúTop 2/B‚Äù setting.
0 5 10 15 18
API number0.000.050.100.150.200.250.300.35Accuracy
CodeGen 350M
CodeGen 2B
CodeGenAPI 2B
Codex
Figure 12: API number vs. accuracy on all private library
benchmarks in the ‚ÄúOracle/B‚Äù setting.
largermodelspossessaheightenedabilitytosolvecomplex
problems. Forinstance,Codexevenresolvestheseprogram-
mingproblemsthatinclude 8APIs. Meanwhile,wealsoob-
serve that C ODEGENAPI consistently outperforms C ODE-
GENacross varying API counts. For example, C ODEGE-
NAPI 2B is capable of resolving these programming prob-
lems that include 5APIs, while C ODEGEN2B is not. Such
anobservationdemonstratesourC ODEGENAPIcontinuously
pre-training on public libraries does enhance its ability to
call private APIs.
7.3.8. Error Type
When provided with Oracle APIs, APICoder can solve
some private library problems, but a substantial portion re-
mains unresolved. We are intrigued by the reasons behind
Passed Invalid Incorrect010203040506070Percentage (%)CodeGen 350M
CodeGen 2B
CodeGenAPI 2B
CodexFigure 13: Comparative analysis of passed, invalid, and incor-
rect API usage proportions for four models in TorchDataEval
with ‚ÄòOracle/B‚Äô setting. ‚ÄòInvalid‚Äô refers to when the model
does not invoke the prompted API, while ‚ÄòIncorrect‚Äô denotes
instances where the API is invoked, but not used properly.
theseunresolvedproblems,whethertheystemfromthelack
of API calls (labeled as Invalid) or incorrect API usage (la-
beled as Incorrect). Thus, we compare the passed, invalid,
and incorrect rates of four models using TorchDataEval in
Figure 13. Our finding is that the models with superior per-
formanceexhibithigherpassedandincorrectratesandlower
invalidrates. Thishighlightsthat,ontheonehand,superior-
performingmodelsgeneratecandidatecodemorereadilypass-
ing test cases. On the other hand, the majority of errors
in these superior models are due to incorrect API usage,
whereas subpar models fail to even invoke APIs. Our an-
other finding is that C ODEGENAPI2B outperforms C ODE-
GEN2B in both the passed and the incorrect rates while
maintaining lower invalid rates. This also obliquely reflects
that the continuous pre-training from C ODEGENto CODE-
GENAPIindeedenhancestheabilitytoinvokeprivateAPIs.
7.3.9. Public Library
Technically, our proposed approach can be applied to
publicscenariosaswell. Assuch,Table7showcasestheper-
formance of three models on PandasEval and NumpyEval.
Theresultsindicateadeclineinperformancewhenprompt-
ing off-the-shelf models with public APIs, such as C ODE-
GEN2BandCodex. Apossiblereasonisthatpromptingthe
previouslyseenpublicAPIsmaydisrupttheprobabilitypre-
diction of the model. Also, the decline is more pronounced
whenthemodelsizeissmaller. Forexample,C ODEGEN2B
seea 35%pass @10decreaseonNumpyEvalinthe‚ÄúTop 2/BE‚Äù
setting. Surprisingly,C ODEGENAPI2Bbringsaperformance
gain. Forinstance,C ODEGENAPI2Bexperiencesaroughly
6%increaseinpass@1onNumpyEvalinthe‚ÄúOracle/B‚Äùset-
ting. ThisonceagainhighlightsthatourC ODEGENAPIpos-
sesses the ability to invoke the prompted APIs effectively.
8. Discussion and Limitations
Inthissection,wewilldelveintosomethought-provoking
discussions on the limitations of our paper. (1) As men-
Daoguang Zan et al.: Preprint submitted to Elsevier Page 13 of 18

--- PAGE 15 ---
Private-Library-Oriented Code Generation with Large Language Models
Table 7
Pass@ùëò(%) of CodeGen 2B,CodeGenAPI 2B and Codex
on two public library benchmarks. #1, #2, and #3 represent
CodeGenAPI trained with the three prompts in Figure 7.
The values of red and green represent improvements and de-
teriorations compared to the No API setting.
APICoder P. APIFinderPandasEval NumpyEval
pass@ ùëò
k=1 k=10 k=1 k=10
CodeGen
2B-No API 37 63 38 66
Oracle/B 325567362615
Oracle/E 29849143174917
Oracle/BE 2512501327114719
Top1/B 2895583355313
Top2/B 27105493265610
Top2/E 2314451822163927
Top2/BE 1918372615233135
Top3/B 289567335579
Top5/B 3075763175511
CodeGenAPI
2B#1Oracle/B 414685446693
Top1/B 381652402660
Top2/B 392652413682
Top3/B 381641413671
Top5/B 370641402682
#2Oracle/E 425663446715
Top2/E 370621344615
#3Oracle/BE 414663457704
Top2/BE 352612353588
Codex
(Code 002)-No API 54 83 62 91
Oracle/B 522803611874
Oracle/E 513812611883
Oracle/BE 504803602883
Top1/B 477776566856
Top2/B 486812575865
Top2/E 4811821539856
Top2/BE 45147854913829
Top3/B 4688035210856
Top5/B 459803539856
tioned in Section 6, creating a truly private library bench-
mark poses a significant challenge. As a result, in addi-
tiontoTorchDataEvalandTorchDataComplexEval,wealso
derive two pseudo private library benchmarks from public
ones. Despiteourbesteffortstomodifypubliclibrariesinto
private ones by paraphrasing keywords and API documen-
tation, there remains a potential risk to the validity and fair-
ness of the private library evaluation. So, it is a worthwhile
endeavortogathermorereal-worldprivatelibrariesandcor-
responding programming tasks in future work. (2) As out-
lined in Section 7.2, our proposed approach yields superior
performance on these models with larger parameters. Like-
wise,ifthemodelhasfewerparameters,thebenefitsyielded
by our approach may be limited or even ineffective. As a
result, our approach is relatively sensitive to the capabili-
ties of the base model itself. (3) As early explorers in the
field of private-library-oriented code generation, our con-
structed private libraries typically feature a relatively mod-
est API count (about 200). In this case, APIFinder can re-
trieve some useful APIs. However, as the API count grows,
the challenge faced by APIFinder could become amplified.
EveninourbenchmarkswithrelativelyfewerAPIs,theper-
formanceofAPIFinderlagsignificantlybehindthe‚ÄúOracle‚Äùsetting (Table 2). This reveals the ample room for improve-
ment in APIFinder. (4) Providing API examples to API-
CodermayintroduceaminorbiasinevaluatingTorchDataE-
val,astheconstructionofTorchDataEvalalsoreferstoAPI
examples in API documentation. (5) Unavoidably, our pa-
perentailsaheavyconsumptionofcomputationalresources.
Therefore,wewillpubliclyreleasetheLLMs-generatedfiles
to foster further research. (6) Our approach focuses solely
on Python. When extrapolated to other programming lan-
guages,somepotentialthreatsmayexistduetosubtlediffer-
ences between them. (7) Several powerful code generation
models, such as PaLM-Coder [15], PanGu-Coder [16], and
AlphaCode [35], are not publicly available, which prevents
us from including them in our experiments. In light of this,
wehavemadeeveryefforttorunallaccessiblemodelslisted
inTable2,aimingtoobtainthemosttrustworthyresultspos-
sible. (8)Practicallyspeaking,oneintriguingideaistocon-
vertourapproachintoaprogrammingassistanttoaiddevel-
opersinbetterusingprivatelibraries,asprivatelibrariesare
a common occurrence in routine coding scenarios. Consid-
eringpotentialprivacyandsecurityconcerns,thisremainsa
topic for future research.
9. Conclusion
Inthispaper,weproposeanovelscenarioforcodegener-
ation focused on private libraries. To address this scenario,
we design a framework by simulating the human use of pri-
vate libraries, which consists of two modules: APIFinder
andAPICoder. APIFinderfirst retrievesrelevantAPIsfrom
API documentation, and APICoder then utilizes these APIs
tosolveprogrammingproblems. Additionally,wecraftfour
privatelibrarybenchmarks,includingTorchDataEval,Torch-
DataComplexEval,MonkeyEval,andBeatNumEval. Lastly,
wecarryoutextensiveexperimentsonthefourbenchmarks,
showcasing the strengths and limitations of our approach,
which could provide some meaningful insights for future
work. Moving forward, our goal is to encapsulate our ap-
proach as an auxiliary tool designed to facilitate the coding
process for programmers. This aim presents several chal-
lengingyetpromisingresearchquestions. Forinstance,how
toensureprivacyandsecuritywhenemployingLLMs? How
to tackle the scenario of mixed usage of public and private
libraries? How can we measure and increase the trust that
developers place in the generated code? How to design an
effective user interface that allows programmers to interact
smoothly with the tool? And beyond these, there are count-
less other intriguing questions in this field that await explo-
ration.
A. Keyword Conversion from Public to
Private Libraries
Wemanuallyconvertthepubliclibrariesintoprivateones
by paraphrasing all relevant keywords in Section 6. Table 8
listsallthekeywordsbeforeandafterconvertingPandasEval
(NumpyEval) to MonkeyEval (BeatNumEval).
Daoguang Zan et al.: Preprint submitted to Elsevier Page 14 of 18

--- PAGE 16 ---
Private-Library-Oriented Code Generation with Large Language Models
Table 8
Keywords conversion from PandasEval (NumpyEval) to MonkeyEval (BeatNumEval).
PandasEval - MonkeyEval
df Pandas pandas len tolist isin sort_index
kf Monkey monkey length convert_list incontain sorting_index
isnull apply to_numeric dropna append tail value_counts
ifnull employ to_num sipna adding last_tail counts_value_num
innull astype select_dtypes iterrows min max drop_duplicates
isnone totype choose_dtypes traversal get_min get_max remove_duplicates
pd shift merge copy rename_axis reset_index sample
mk shifting unioner clone renaming_axis reseting_index sample_by_num
concat to_dict cumsum last to_string applymap duplicated
concating convert_dict cumulative_sum final_item convert_string conduct_map duplicated_values
isna format div mean ceil assign DataFrame
ifna formating division average ceiling allocate KnowledgeFrame
drop Series ravel any fillna all to_pydatetime
sip Collections flat_underlying whatever fillnone total_all convert_pydatetime
reindex head sort_values rename sum unique to_datetime
reindexing header_num sort_the_values renaming total_sum distinctive convert_datetime
map std intersection groupby nlargest replace dataframe
mapping standard interst grouper nbiggest replacing knowledgeframe
get series round
getting collections value_round
NumpyEval - BeatNumEval
np Numpy array unique ndarray transpose reshape
bn Beatnum numset uniq ndnumset switching_places change_shape_to
real numpy vstack sum imag in1d flatten
reality beatnum vertical_stack total_count imaginary intersection1dim convert_into_one_dim
isnan all fromstring inv mean where compressed
ifnan total come_from_str inverse average filter_condition remove_masked_data
add max histogram to_numpy filled stack cumsum
add_concat get_max hist_operation to_beatnum masked_fill pile_operation cumulative_sum
insert arange ravel std argmax argmin full
stick arr_range asview standard_op get_argmax get_argmin_value full_value_func
slice squeeze hstack asarray repeat bincount unravel_index
piece sqz horizontal_stack asnumset duplicate binoccurrence convert_index_or_arr
diff concatenate any column_stack norm delete logical_and
difference connect any_condition stack_col normlizattion remove_operation logic_and_element_wise
append split ones vectorize fill_diagonal argpartition setxor1d
apd sep_split create_ones vectorisation pad_diagonal perform_partition seting_exclusive_or_one_dim
array_split abs astype searchsorted min fromarrays
split_array absolute convert_type find_sorted get_min come_from_arrays
CRediT authorship contribution statement
DaoguangZan: Conceptualization,Datacuration,For-
mal analysis, Investigation, Methodology, Resources, Writ-
ing - original draft. Bei Chen: Conceptualization, Formal
analysis,Methodology,Supervision,Writing-originaldraft.
YongshunGong: Conceptualization,Methodology,Valida-
tion.Junzhi Cao: Conceptualization, Methodology, Vali-
dation.Fengji Zhang: Data curation, Formal analysis, In-
vestigation. Bingchao Wu: Resources, Visualization. Bei
Guan:Methodology, Validation. Yilong Yin: Conceptual-
ization, Methodology, Supervision, Project administration.
Yongji Wang: Conceptualization, Methodology, Supervi-
sion, Project administration.
References
[1] Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2021. Unified
pre-training for program understanding and generation, in: Proceed-
ings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Tech-
nologies, pp. 2655‚Äì2668.[2] Allal,L.B.,Li,R.,Kocetkov,D.,Mou,C.,Akiki,C.,Ferrandis,C.M.,
Muennighoff, N., Mishra, M., Gu, A., Dey, M., Umapathi, L.K., An-
derson, C.J., Zi, Y., Poirier, J., Schoelkopf, H., Troshin, S.M., Ab-
ulkhanov, D., Romero, M., Lappert, M.F., Toni, F.D., del R‚Äôio, B.G.,
Liu, Q., Bose, S., Bhattacharyya, U., Zhuo, T.Y., Yu, I., Villegas, P.,
Zocca, M., Mangrulkar, S., Lansky, D., Nguyen, H., Contractor, D.,
Villa, L., Li, J., Bahdanau, D., Jernite, Y., Hughes, S.C., Fried, D.,
Guha,A.,deVries,H.,vonWerra,L.,2023. SantaCoder: don‚Äôtreach
for the stars! ArXiv abs/2301.03988.
[3] Alrubaye, H., Mkaouer, M.W., Khokhlov, I., Reznik, L., Ouni, A.,
Mcgoff, J., 2020. Learning to recommend third-party library mi-
gration opportunities at the api level. Applied Soft Computing 90,
106140.
[4] Athiwaratkun, B., Gouda, S.K., Wang, Z., Li, X., Tian, Y., Tan, M.,
Ahmad, W.U., Wang, S., Sun, Q., Shang, M., Gonugondla, S.K.,
Ding, H., Kumar, V., Fulton, N., Farahani, A., Jain, S., Giaquinto,
R., Qian, H., Ramanathan, M.K., Nallapati, R., Ray, B., Bhatia, P.,
Sengupta, S., Roth, D., Xiang, B., 2022. Multi-lingual evaluation of
code generation models. ArXiv abs/2210.14868.
[5] Austin,J.,Odena,A.,Nye,M.,Bosma,M.,Michalewski,H.,Dohan,
D.,Jiang,E.,Cai,C.J.,Terry,M.,Le,Q.V.,Sutton,C.,2021.Program
synthesis with large language models. ArXiv abs/2108.07732.
[6] Bauer, V., Heinemann, L., Deissenboeck, F., 2012. A structured ap-
proach to assess third-party library usage, in: 2012 28th IEEE In-
Daoguang Zan et al.: Preprint submitted to Elsevier Page 15 of 18

--- PAGE 17 ---
Private-Library-Oriented Code Generation with Large Language Models
ternational Conference on Software Maintenance (ICSM), IEEE. pp.
483‚Äì492.
[7] Bavarian, M., Jun, H., Tezak, N.A., Schulman, J., McLeavey, C.,
Tworek, J., Chen, M., 2022. Efficient training of language models
to fill in the middle. ArXiv abs/2207.14255.
[8] Black, S., Gao, L., Wang, P., Leahy, C., Biderman, S., 2021. GPT-
Neo: Large Scale Autoregressive Language Modeling with Mesh-
Tensorflow. URL: https://doi.org/10.5281/zenodo.5297715 , doi: 10.
5281/zenodo.5297715 .
[9] Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S.D., Phipps-Costin,
L., Pinckney, D., Yee, M.H., Zi, Y., Anderson, C.J., Feldman, M.Q.,
Guha,A.,Greenberg,M.,Jangda,A.,2022. Ascalableandextensible
approach to benchmarking nl2code for 18 programming languages.
ArXiv abs/2208.08227.
[10] Chai,Y.,Wang,S.,Pang,C.,Sun,Y.,Tian,H.,Wu,H.,2022.ERNIE-
Code: Beyond english-centric cross-lingual pretraining for program-
ming languages. arXiv preprint arXiv:2212.06742 .
[11] Chandel, S., Clement, C.B., Serrato, G., Sundaresan, N., 2022a.
Training and evaluating a jupyter notebook data science assistant.
ArXiv abs/2201.12901.
[12] Chandel, S., Clement, C.B., Serrato, G., Sundaresan, N., 2022b.
Training and evaluating a jupyter notebook data science assistant.
arXiv preprint arXiv:2201.12901 .
[13] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.G., Chen,
W.,2022.Codet: Codegenerationwithgeneratedtests.arXivpreprint
arXiv:2207.10397 .
[14] Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Ed-
wards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R.,
Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan,
B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian,
M., Winter, C., Tillet, P., Such, F.P., Cummings, D.W., Plappert, M.,
Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A.,
Babuschkin, I., Balaji, S.A., Jain, S., Carr, A., Leike, J., Achiam,
J., Misra, V., Morikawa, E., Radford, A., Knight, M.M., Brundage,
M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D.,
McCandlish, S., Sutskever, I., Zaremba, W., 2021. Evaluating large
language models trained on code. ArXiv abs/2107.03374.
[15] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G.,
Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S.,
Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes,
P.,Tay,Y.,Shazeer,N.M.,Prabhakaran,V.,Reif,E.,Du,N.,Hutchin-
son, B.C., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,
Yin,P.,Duke,T.,Levskaya,A.,Ghemawat,S.,Dev,S.,Michalewski,
H., Garc√≠a, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ip-
polito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R.,
Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pel-
lat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
K., Zhou, Z., Wang, X., Saeta, B., D√≠az, M., Firat, O., Catasta, M.,
Wei, J., Meier-Hellstern, K.S., Eck, D., Dean, J., Petrov, S., Fiedel,
N., 2022. PaLM: Scaling language modeling with pathways. ArXiv
abs/2204.02311.
[16] Christopoulou,F.,Lampouras,G.,Gritta,M.,Zhang,G.,Guo,Y.,Li,
Z.Y.,Zhang,Q.,Xiao,M.,Shen,B.,Li,L.,Yu,H.,yuYan,L.,Zhou,
P.,Wang,X.,Ma,Y.,Iacobacci,I.,Wang,Y.,Liang,G.,Wei,J.,Jiang,
X., Wang, Q., Liu, Q., 2022. PanGu-Coder: Program synthesis with
function-level language modeling. ArXiv abs/2207.11280.
[17] Clement, C.B., Drain, D., Timcheck, J., Svyatkovskiy, A., Sundare-
san, N., 2020. PyMT5: Multi-mode translation of natural language
and python code with transformers, in: Conference on Empirical
Methods in Natural Language Processing, pp. 9052‚Äì9065.
[18] Cobbe,K.,Kosaraju,V.,Bavarian,M.,Hilton,J.,Nakano,R.,Hesse,
C., Schulman, J., 2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
[19] CodedotAl, 2021. GPT Code Clippy: The Open Source version of
GitHub Copilot. https://github.com/CodedotAl/gpt-code-clippy .
[20] Derr, E., Bugiel, S., Fahl, S., Acar, Y., Backes, M., 2017. Keep me
updated: An empirical study of third-party library updatability on
android, in: Proceedings of the 2017 ACM SIGSAC Conference onComputer and Communications Security, pp. 2187‚Äì2200.
[21] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-
training of deep bidirectional transformers for language understand-
ing, in: Proceedings of the 2019 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Hu-
manLanguageTechnologies,Volume1(LongandShortPapers),pp.
4171‚Äì4186.
[22] Ding, Y., Wang, Z., Ahmad, W.U., Ramanathan, M.K., Nallapati,
R., Bhatia, P., Roth, D., Xiang, B., 2022. CoCoMIC: Code comple-
tion by jointly modeling in-file and cross-file context. arXiv preprint
arXiv:2212.10007 .
[23] Formal, T., Lassance, C., Piwowarski, B., Clinchant, S., 2022. From
distillationtohardnegativesampling: Makingsparseneuralirmodels
more effective. arXiv preprint arXiv:2205.04733 .
[24] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F.,
Zhong, R., Yih, S., Zettlemoyer, L., Lewis, M., 2023. InCoder: A
generative model for code infilling and synthesis, in: The Eleventh
International Conference on Learning Representations.
[25] Gu, X., Zhang, H., Zhang, D., Kim, S., 2016. Deep api learning, in:
ACM SIGSOFT International Symposium on Foundations of Soft-
ware Engineering, pp. 631‚Äì642.
[26] Hendrycks,D.,Basart,S.,Kadavath,S.,Mazeika,M.,Arora,A.,Guo,
E., Burns, C., Puranik, S., He, H., Song, D.X., Steinhardt, J., 2021.
Measuringcodingchallengecompetencewithapps,in: NeuralInfor-
mation Processing Systems.
[27] Huggingface, 2021. Training CodeParrot from Scratch. https:
//huggingface.co/blog/codeparrot .
[28] Izacard, G., Grave, E., 2020. Leveraging passage retrieval with gen-
erative models for open domain question answering. arXiv preprint
arXiv:2007.01282 .
[29] Jain, N., Vaidyanath, S., Iyer, A.S., Natarajan, N., Parthasarathy, S.,
Rajamani, S.K., Sharma, R., 2021. Jigsaw: Large language models
meetprogramsynthesis. 2022IEEE/ACM44thInternationalConfer-
ence on Software Engineering (ICSE) , 1219‚Äì1231.
[30] Johnson, J., Douze, M., J√©gou, H., 2019. Billion-scale similarity
search with gpus. IEEE Transactions on Big Data 7, 535‚Äì547.
[31] Karpukhin, V., Oƒüuz, B., Min, S., Lewis, P., Wu, L., Edunov, S.,
Chen, D., Yih, W.t., 2020. Dense passage retrieval for open-domain
question answering. arXiv preprint arXiv:2004.04906 .
[32] Kingma,D.P.,Ba,J.,2014. Adam: Amethodforstochasticoptimiza-
tion. arXiv preprint arXiv:1412.6980 .
[33] Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L.,
Yih, S., Fried, D., yi Wang, S., Yu, T., 2022. DS-1000: A natu-
ral and reliable benchmark for data science code generation. ArXiv
abs/2211.11501.
[34] Li, R., Allal, L.B., Zi, Y., Muennighoff, N., et al., 2023. StarCoder:
May the source be with you! .
[35] Li, Y., Choi, D.H., Chung, J., Kushman, N., Schrittwieser, J.,
Leblond, R., Tom, Eccles, Keeling, J., Gimeno, F., Lago, A.D.,
Hubert, T., Choy, P., de, C., d‚ÄôAutume, M., Babuschkin, I., Chen,
X., Huang, P.S., Welbl, J., Gowal, S., Alexey, Cherepanov, Mol-
loy, J., Mankowitz, D.J., Robson, E.S., Kohli, P., de, N., Freitas,
Kavukcuoglu, K., Vinyals, O., 2022. Competition-level code gen-
eration with alphacode. Science 378, 1092 ‚Äì 1097.
[36] Lin, C.Y., 2004. ROUGE: A package for automatic evaluation of
summaries, in: Text summarization branches out, pp. 74‚Äì81.
[37] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S.w., Svyatkovskiy, A.,
2022. ReACC: A retrieval-augmented code completion framework.
arXiv preprint arXiv:2203.07722 .
[38] Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco,
A., Clement, C., Drain, D., Jiang, D., Tang, D., et al., 2021.
CodeXGLUE: A machine learning benchmark dataset for code un-
derstanding and generation. arXiv preprint arXiv:2102.04664 .
[39] Molnar, D., Wagner, D., 2004. Privacy and security in library rfid:
Issues,practices,andarchitectures,in: Proceedingsofthe11thACM
conferenceonComputerandcommunicationssecurity,pp.210‚Äì219.
[40] Nguyen,A.,Karampatziakis,N.,Chen,W.,2023. Meetinthemiddle:
A new pre-training paradigm. arXiv:2303.07295 .
Daoguang Zan et al.: Preprint submitted to Elsevier Page 16 of 18

--- PAGE 18 ---
Private-Library-Oriented Code Generation with Large Language Models
[41] Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y.,
Savarese, S., Xiong, C., 2023. CodeGen: An open large language
model for code with multi-turn program synthesis, in: The Eleventh
International Conference on Learning Representations.
[42] OpenAI, 2023. Gpt-4 technical report. arXiv:2303.08774 .
[43] Papineni,K.,Roukos,S.,Ward,T.,Zhu,W.J.,2002. BLEU:amethod
for automatic evaluation of machine translation, in: Proceedings of
the 40th annual meeting of the Association for Computational Lin-
guistics, pp. 311‚Äì318.
[44] Parvez, M.R., Ahmad, W., Chakraborty, S., Ray, B., Chang, K.W.,
2021. Retrieval augmented code generation and summarization, in:
Findings of EMNLP, pp. 2719‚Äì2734.
[45] Qu,Y.,Ding,Y.,Liu,J.,Liu,K.,Ren,R.,Zhao,W.X.,Dong,D.,Wu,
H., Wang, H., 2020. RocketQA: An optimized training approach to
dense passage retrieval for open-domain question answering. arXiv
preprint arXiv:2010.08191 .
[46] Rajbhandari, S., Rasley, J., Ruwase, O., He, Y., 2019. Zero:
Memory optimization towards training A trillion parameter mod-
els. CoRR abs/1910.02054. URL: http://arxiv.org/abs/1910.02054 ,
arXiv:1910.02054 .
[47] Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N.,
Zhou, M., Blanco, A., Ma, S., 2020. CodeBLEU: a method for auto-
maticevaluationofcodesynthesis. arXivpreprintarXiv:2009.10297
.
[48] Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., Zaharia, M.,
2021. Colbertv2: Effective and efficient retrievalvia lightweight late
interaction. arXiv preprint arXiv:2112.01488 .
[49] Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Iliƒá, S., Hesslow,
D., Castagn√©, R., Luccioni, A.S., Yvon, F., Gall√©, M., et al.,
2022. BLOOM: A 176b-parameter open-access multilingual lan-
guage model. arXiv preprint arXiv:2211.05100 .
[50] Scheller, T., K√ºhn, E., 2015. Automated measurement of api usabil-
ity: Theapiconceptsframework. InformationandSoftwareTechnol-
ogy 61, 145‚Äì162.
[51] Shen, B., Zhang, J., Chen, T., Zan, D., Geng, B., Fu, A., Zeng,
M., Yu, A., Ji, J., Zhao, J., Guo, Y., Wang, Q., 2023. Pangu-
coder2: Boosting large language models for code with ranking feed-
back. arXiv:2307.14936 .
[52] Shrivastava, D., Larochelle, H., Tarlow, D., 2022. Repository-level
promptgenerationforlargelanguagemodelsofcode,in: ICML2022
Workshop on Knowledge Retrieval and Language Models.
[53] Siddiq, M.L., msiddiq, 2022. SecurityEval dataset: mining vulner-
ability examples to evaluate machine learning-based code generation
techniques. Proceedingsofthe1stInternationalWorkshoponMining
Software Repositories Applications for Privacy and Security .
[54] Svyatkovskiy, A., Deng, S.K., Fu, S., Sundaresan, N., 2020. Intel-
liCode compose: code generation using transformer. Proceedings of
the28thACMJointMeetingonEuropeanSoftwareEngineeringCon-
ferenceandSymposiumontheFoundationsofSoftwareEngineering
.
[55] Tan, G., Croft, J., 2008. An empirical security study of the native
code in the jdk., in: Usenix Security Symposium, pp. 365‚Äì378.
[56] Wang, B., Komatsuzaki, A., 2021. GPT-J-6B: A 6 Billion Parame-
terAutoregressiveLanguageModel. https://github.com/kingoflolz/
mesh-transformer-jax .
[57] Wang, Y., Wang, W., Joty, S., Hoi, S.C., 2021. CodeT5: Identifier-
aware unified pre-trained encoder-decoder models for code under-
standing and generation, in: Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing, pp. 8696‚Äì8708.
[58] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al.,
2022. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682 .
[59] Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.,
Ahmed, J., Overwijk, A., 2020. Approximate nearest neighbor neg-
ative contrastive learning for dense text retrieval. arXiv preprint
arXiv:2007.00808 .
[60] Xu,F.F.,Alon,U.,Neubig,G.,Hellendoorn,V.J.,2022. Asystematicevaluation of large language models of code. Proceedings of the 6th
ACMSIGPLANInternationalSymposiumonMachineProgramming
.
[61] Yang,Z.,Chen,S.,Gao,C.,Li,Z.,Li,G.,Lv,R.,2023.Deeplearning
based code generation methods: A literature review. arXiv preprint
arXiv:2303.01056 .
[62] Zan, D., Chen, B., Lin, Z., Guan, B., Wang, Y., Lou, J.G., 2022a.
When language model meets private library. EMNLP Findings .
[63] Zan, D., Chen, B., Yang, D., Lin, Z., Kim, M., Guan, B., Wang,
Y., Chen, W., Lou, J.G., 2022b. CERT: Continual pre-training on
sketches for library-oriented code generation, in: The 2022 Interna-
tional Joint Conference on Artificial Intelligence.
[64] Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Wang, Y.,
Lou,J.G.,2022c.Whenneuralmodelmeetsnl2code: Asurvey.arXiv
preprint arXiv:2212.09420 .
[65] Zhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou,
J.G., Chen, W., 2023. Repocoder: Repository-level code completion
through iterative retrieval and generation. arXiv:2303.12570 .
[66] Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang,
Z.Y., Shen, L., Wang, A., Li, Y., Su, T., Yang, Z., Tang, J., 2023.
CodeGeeX: A pre-trained model for code generation with multilin-
gual evaluations on humaneval-x. ArXiv abs/2303.17568.
[67] Zhou, S., Alon, U., Xu, F.F., JIang, Z., Neubig, G., 2023. DocCoder:
Generatingcodebyretrievingandreadingdocs,in: TheEleventhIn-
ternational Conference on Learning Representations.
Daoguang Zan et al.: Preprint submitted to Elsevier Page 17 of 18

--- PAGE 19 ---
Private-Library-Oriented Code Generation with Large Language Models
Daoguang Zan is currently pursuing the Ph.D.
degree with the Institute of Software, Chinese
Academy of Sciences, Beijing, China. His prin-
cipal research interest includes natural language
processingandsoftwareengineering,especiallyin
codegenerationandlargelanguagemodel. Inthese
areas, he has published around 10 papers in top-
tier conference proceedings, including ACL, IJ-
CAI, EMNLP, ICLR, PAKDD, etc.
Bei Chen is currently a senior researcher at Mi-
crosoft. She received her Ph.D. degree from the
Department of Computer Science and Technol-
ogy at Tsinghua University, Beijing, China, in
2017. She is mainly working on natural language
processing, including semantic parsing, dialogue
systems, pre-trained language models, and their
applications in code intelligence. She has pub-
lished above 30 papers in top conferences, includ-
ing ICLR, NeurIPS, ACL, EMNLP, KDD, AAAI,
IJCAI, etc.
Yongshun Gong is an Associate Professor at
School of Software, Shandong University, China.
He received his Ph.D. degree from University of
Technology Sydney in 2021. His principal re-
searchinterestcoversthedatascienceandmachine
learning, in particular, the following areas: adap-
tivemodel;spatiotemporaldatamining;trafficpre-
diction; recommender system and sequential pat-
tern mining. He has published above 40 papers in
top journals and refereed conference proceedings,
including the IEEE T-PAMI, IEEE T-KDE, IEEE
T-NNLS, IEEE T-CYB, IEEE T-MM, NeurIPS,
CVPR, KDD, CIKM, AAAI, IJCAI, etc.
Junzhi Cao received his PhD degree in Astro-
physics and Deep Learning from New York Uni-
versity in 2021. He mainly studies Dialogue Sys-
tem in Natural Language Processing, Large lan-
guage models, and Cosmology in statistics. He
haspublishedaround10papersintopjournalsand
refereedconferenceproceedings,includingNature
Communications, Monthly Notices of the Royal
AstronomicalSociety,JournalofAppliedPhysics,
etc.
FengjiZhang iscurrentlypursuingthemasterde-
greewiththeSchoolofComputerScience,Wuhan
University, China. He also received the B.S. de-
greefromtheSchoolofComputerScience,Wuhan
University in 2020. His current research interests
include intelligent software engineering and natu-
ral language processing. He has published some
papersintopjournalsandrefereedconferencepro-
ceedings, including IST, JSS, ICLR, and ACL.
Bingchao Wu is currently pursuing his Ph.D. de-
greeattheInstituteofSoftware,ChineseAcademy
ofSciences,Beijing,China. HereceivedhisBach-
elor‚Äôs degree in Information Security from Hunan
University,Changsha,Chinain2017. Hisresearch
interests include deep learning, recommendation
systems, and named entity recognition. He has
published several papers in top journals and ref-
ereed conference proceedings, including the IEEE
ICME, ACL, ISWC, etc.
BeiGuan iscurrentlyaResearchAssociateProfes-
sor at Institute of Software, Chinese Academy of
Sciences. He received the B.S. degree from Tian-
jin University in 2007. He got the Ph.D. degree
from Institute of Software, Chinese Academy of
Sciences in 2015. He got the postdoctoral posi-
tionatQatarComputingResearchInstitute,Hamad
Bin Khalifa University (QCRI,HBKU) and fin-
ished that in 2018. His primary research interests
include big data analytics in healthcare and cyber
security, operating system techniques, virtualiza-
tion techniques, cloud computing, and system se-
curity. He has published over 20 technical arti-
cles in refereed journals and proceedings, includ-
ingIEEETransactions,ACMTransactions,IJCAI,
PAKDD, EMNLP, IJCNN, ACL, etc.
Yilong Yin is the Director of the Machine Learn-
ing and Applications Group and a Distinguished
ProfessorwithShandongUniversity,Jinan,China.
HereceivedthePh.D.degreefromJilinUniversity,
Changchun, China, in 2000. From 2000 to 2002,
hewasaPostdoctoralFellowwiththeDepartment
of Electronic Science and Engineering, Nanjing
University, Nanjing, China. His research interests
include machine learning, data mining, computa-
tionalmedicine, andbiometrics. Hehaspublished
above100papersintopjournalsandrefereedcon-
ferenceproceedings,includingTKDE,TIP,TMM,
ICML, IJCAI, etc.
Yongji Wang is a Distinguished Research Fel-
low with the Chinese Academy of Sciences and
a Ph.D. advisor. He received his Ph.D. degree
from the University of Edinburgh, United King-
dom. His research interests include artificial in-
telligence, big data analysis, and data mining. He
has achieved numerous internationally influential
research results, participated in over 20 scientific
research projects, and published six monographs.
Hehasauthoredmorethan200high-qualitypapers
inprestigiousdomesticandinternationalacademic
journalsandconferences,includingIEEETransac-
tions, ACM Transactions, ACL, IJCAI, EMNLP,
etc.
Daoguang Zan et al.: Preprint submitted to Elsevier Page 18 of 18
