--- TRANG 26 ---
Tài liệu tham khảo
Ahmed, Toufique và Premkumar Devanbu (2022a). Few-shot training LLMs for project-specific code-summarization. 
arXiv:2207.04237[cs.SE]. url: https://arxiv.org/abs/2207.04237.
— (Tháng 5 2022b). "Multilingual training for software engineering". Trong: Proceedings of the 44th International 
Conference on Software Engineering. ACM. doi:10.1145/3510003.3510049. url: https://doi.org/10.1145%2F3510003.3510049.

Bubeck, Sébastien et al. (2023). "Sparks of Artificial General Intelligence: Early experiments with GPT-4". 
url: https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/.

Chen, Fuxiang et al. (2022). On the Transferability of Pre-trained Language Models for Low-Resource Programming 
Languages. arXiv:2204.09653[cs.PL]. url: https://arxiv.org/abs/2204.09653.

Chen, Mark et al. (2021). Evaluating Large Language Models Trained on Code. arXiv: 2107.03374 [cs.LG].

Cottrell, Allin (2017). "Hansl: A DSL for Econometrics". Trong: Proceedings of the 2Nd International Workshop 
on Real World Domain Specific Languages. RWDSL17. Austin, TX, USA: ACM, 1:1–1:10. isbn: 978-1-4503-4845-4. 
doi:10.1145/3039895.3039896. url: http://doi.acm.org/10.1145/3039895.3039896.

Gong, Zi et al. (19 tháng 12, 2022). "MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code 
Completion". Trong: arXiv: 2212.09666v1 [cs.CL].

Husain, Hamel et al. (2020). CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. arXiv:1909.09436 [cs.LG].

Korinek, Anton (Tháng 2 2023). Language Models and Cognitive Automation for Economic Research. Working Paper 30957. 
National Bureau of Economic Research. doi:10.3386/w30957. url: http://www.nber.org/papers/w30957.

Lucchetti, Jack và Sven Schreiber (30 tháng 7, 2022). The SVAR addon for gretl. Phiên bản 1.97. 
url: https://sourceforge.net/projects/gretl/files/addons/doc/SVAR.pdf.

Lucchetti, Jack và Francesco Valentini (19 tháng 1, 2023). Time-Varying OLS/IV estimators in gretl: the ketvals package. 
Phiên bản 1.0. url: https://gretl.sourceforge.net/current_fnfiles/unzipped/ketvals.pdf.

Manna, Zohar và Richard J. Waldinger (1971). "Toward Automatic Program Synthesis". Trong: Commun. ACM 14.3, tr. 151–165. 
issn: 0001-0782. doi:10.1145/362566.362568. url: https://doi.org/10.1145/362566.362568.

Mastel, Pierrette Mahoro et al. (2023). "NATURAL LANGUAGE UNDERSTANDING FOR AFRICAN LANGUAGES". Trong: 4th Workshop 
on African Natural Language Processing. url: https://openreview.net/forum?id=gWuvdFMqHM.

Mens, T. và T. Tourwe (2004). "A survey of software refactoring". Trong: IEEE Transactions on Software Engineering 
30.2, tr. 126–139. doi:10.1109/TSE.2004.1265817.

Radford, Alec et al. (2019). "Language models are unsupervised multitask learners". Trong: OpenAI Blog 1.8, tr. 9.

Roziere, Baptiste et al. (2020). "Unsupervised translation of programming languages". Trong: Advances in Neural 
Information Processing Systems 33.

Sun, Weisong et al. (2023). Automatic Code Summarization via ChatGPT: How Far Are We? arXiv:2305.12865 [cs.SE]. 
url: https://arxiv.org/abs/2305.12865.

Tarassow, Artur (2019). "Practical Empirical Research Using gretl and hansl". Trong: Australian Economic Review 
52.2, tr. 255–271. doi:10.1111/1467-8462.12324. url: https://ideas.repec.org/a/bla/ausecr/v52y2019i2p255-271.html.

Tian, Haoye et al. (2023). Is ChatGPT the Ultimate Programming Assistant – How far is it? arXiv:2304.11938 [cs.SE]. 
url: https://arxiv.org/pdf/2304.11938.

Vaswani, Ashish et al. (2017). "Attention is all you need". Trong: Advances in Neural Information Processing Systems, 
tr. 5998–6008.

Wang, Ben và Aran Komatsuzaki (Tháng 5 2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. 
https://github.com/kingoflolz/mesh-transformer-jax.

--- TRANG 27 ---
Wolfram, Stephen (14 tháng 2, 2023). What Is ChatGPT Doing ... and Why Does It Work? 
url: https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work (truy cập ngày 20/06/2023).

Xu, Frank F. et al. (2022). "A Systematic Evaluation of Large Language Models of Code". Trong: Proceedings of the 
6th ACM SIGPLAN International Symposium on Machine Programming. MAPS 2022. San Diego, CA, USA: Association for 
Computing Machinery, tr. 1–10. isbn: 9781450392730. doi:10.1145/3520312.3534862. url: https://doi.org/10.1145/3520312.3534862.

Yuan, Zhiqiang et al. (2023). No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation. 
arXiv:2305.04207[cs.SE]. url: https://arxiv.org/pdf/2305.04207.pdf.

Zügner, Daniel et al. (2021). "Language-Agnostic Representation Learning of Source Code from Structure and Context". 
Trong: International Conference on Learning Representations. eprint: https://openreview.net/forum?id=Xh5eMZVONGF. 
url: https://openreview.net/forum?id=Xh5eMZVONGF.
27
