# 2410.01215.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2410.01215.pdf
# Kích thước file: 2135106 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Preprint
TỪ MÃ ĐẾN TÍNH ĐÚNG ĐẮN: ĐÓNG KHOẢNG CÁCH CUỐI CÙNG
CỦA SINH MÃ BẰNG VIỆC GỠ LỖI PHÂN CẤP
Yuling Shi1Songsong Wang2Chengcheng Wan3Xiaodong Gu1∗
1Đại học Giao thông Thượng Hải2UC Davis3Đại học Sư phạm Đông Trung Quốc
{yuling.shi,xiaodong.gu }@sjtu.edu.cn
ssswang@formerstudents.ucdavis.edu, ccwan@sei.ecnu.edu.cn
TÓM TẮT
Mặc dù các mô hình ngôn ngữ lớn đã có những bước tiến đáng kể trong việc sinh mã,
tỷ lệ vượt qua của mã được sinh ra bị hạn chế bởi các lỗi tinh vi, thường cần sự can
thiệp của con người để vượt qua các bài kiểm tra, đặc biệt đối với các vấn đề phức tạp.
Các hệ thống gỡ lỗi dựa trên LLM hiện tại xem các chương trình được sinh ra như các
đơn vị nguyên khối, không thể giải quyết lỗi ở nhiều mức độ chi tiết khác nhau, từ
lỗi cú pháp cấp thấp đến khuyết tật thuật toán cấp cao. Trong bài báo này, chúng tôi
giới thiệu Multi-Granularity Debugger (MGDebugger), một trình gỡ lỗi mã phân cấp
bằng cách cô lập, xác định và giải quyết lỗi ở nhiều mức độ chi tiết khác nhau. MGDebugger
phân tách mã có vấn đề thành cấu trúc cây phân cấp của các hàm con, với mỗi cấp
đại diện cho một mức độ chi tiết lỗi cụ thể. Trong quá trình gỡ lỗi, nó phân tích từng
hàm con và lặp đi lặp lại giải quyết lỗi theo cách từ dưới lên. Để kiểm tra hiệu quả
từng hàm con, chúng tôi đề xuất một trình thực thi Python mô phỏng LLM, theo dõi
việc thực thi mã và trạng thái các biến quan trọng để xác định lỗi một cách chính xác.
Các thí nghiệm mở rộng chứng minh rằng MGDebugger vượt trội hơn các hệ thống
gỡ lỗi hiện có, đạt được cải thiện 18.9% về độ chính xác so với sinh mã gốc trong
HumanEval và tỷ lệ sửa chữa thành công 97.6% trong HumanEval-Fix. Hơn nữa,
MGDebugger sửa chữa hiệu quả các lỗi trong các danh mục và mức độ khó khác nhau,
chứng minh tính mạnh mẽ và hiệu quả của nó.1
1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) như GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023),
và DeepSeek-Coder (Zhu et al., 2024) đã có những tiến bộ đáng kể trong các tác vụ mã hóa hỗ trợ
AI (Chen et al., 2021; Lu et al., 2021; Li et al., 2022). Được huấn luyện trên các kho dữ liệu văn
bản và mã rộng lớn, LLM có thể hiểu và sinh các đoạn mã cho nhiều tác vụ lập trình khác nhau, từ
các cấu trúc dữ liệu đơn giản đến các vấn đề thuật toán phức tạp (Li et al., 2022). Các mô hình này
đã chứng minh được khả năng thành thạo trong các tác vụ như hoàn thiện mã, phát hiện lỗi, và thậm
chí giải quyết các thách thức lập trình cạnh tranh.

Mặc dù mã được sinh ra bởi các mô hình lớn nói chung đáp ứng các yêu cầu, nó thường chứa các
lỗi nghiêm trọng cần sự can thiệp của con người để vượt qua các bài kiểm tra (Liu et al., 2023b;
Dou et al., 2024). Điều này đã dần dẫn đến một mô hình phát triển mới: các mô hình lớn sinh mã,
trong khi con người sửa chữa nó. Do đó, "khoảng cách cuối cùng", cũng như bước quan trọng nhất,
của việc sinh mã là làm thế nào để sửa chữa hiệu quả mã được sinh ra bởi các mô hình lớn.

Nhiều nỗ lực đã được thực hiện để gỡ lỗi mã được sinh ra bởi LLM. Cách phổ biến nhất là tái sử
dụng bộ sinh LLM để gỡ lỗi mã được sinh ra với phản hồi từ việc thực thi test case (Chen et al.,
2023b; Zhong et al., 2024; Hu et al., 2024). Mặc dù các phương pháp này tăng tỷ lệ vượt qua, chúng
xem chương trình có lỗi như một tập hợp tổng thể các câu lệnh (Chen et al., 2023b; Shinn et al.,
2023; Zhong et al., 2024; Ding et al., 2024) bất kể các loại và mức độ thất bại khác nhau.
∗Tác giả liên hệ
1Mã và dữ liệu có sẵn tại https://github.com/YerbaPage/MGDebugger
1arXiv:2410.01215v2  [cs.CL]  5 Oct 2024

--- TRANG 2 ---
Preprint
Thất bại của các test case phát sinh từ các yếu tố ở nhiều cấp độ khác nhau, từ lỗi cú pháp cấp thấp
đến khuyết tật thuật toán cấp cao. Cách xử lý tổng thể bỏ qua cấu trúc nội bộ của mã và hạn chế
hiệu quả của các hệ thống gỡ lỗi, đặc biệt khi xử lý các chương trình phức tạp cần gỡ lỗi trên nhiều
module khác nhau (Zeller, 2009; Tian et al., 2024).

Trong bài báo này, chúng tôi giới thiệu Multi-granularity Debugger (MGDebugger), một phương
pháp gỡ lỗi mới cho mã được sinh ra bởi LLM. Thay vì xem toàn bộ hàm như các đơn vị đơn lẻ,
MGDebugger sử dụng chiến lược phân cấp, từ dưới lên để gỡ lỗi mã một cách có hệ thống. Nó bắt
đầu bằng việc phân tách mã thành cấu trúc cây của các hàm con, cho phép cô lập các đơn vị ngữ
nghĩa để gỡ lỗi độc lập. Mỗi hàm con được gỡ lỗi tuần tự, bắt đầu với những hàm chi tiết nhất và
làm việc hướng lên các thành phần cấp cao hơn cho đến khi toàn bộ mã được sửa chữa. Để kiểm tra
và gỡ lỗi hiệu quả từng hàm con, MGDebugger sinh test case được rút ra từ các test case công khai
của hàm chính. Sau đó, nó sử dụng một trình mô phỏng thực thi dựa trên LLM để theo dõi các thay
đổi trong các biến quan trọng, tạo điều kiện cho việc xác định lỗi chính xác và linh hoạt dựa trên
các test case thất bại. Thông qua việc gỡ lỗi ở nhiều mức độ chi tiết từ dưới lên theo cách đệ quy,
MGDebugger có thể phát hiện và sửa chữa các lỗi mà các phương pháp gỡ lỗi tổng thể truyền
thống có thể bỏ qua.

Các thí nghiệm mở rộng với ba mô hình trên ba benchmark chứng minh rằng MGDebugger vượt
trội đáng kể so với các phương pháp gỡ lỗi hiện có, nâng độ chính xác từ 75.6% lên 94.5% trên
HumanEval (Chen et al., 2021) và đạt được tỷ lệ sửa chữa thành công đáng chú ý 97.6% trên HumanEvalFix (Muennighoff et al., 2023). Các nghiên cứu ablation xác nhận vai trò quan trọng của
chiến lược gỡ lỗi phân cấp. Chúng tôi cũng đánh giá hiệu quả của MGDebugger trong xử lý các
loại lỗi đa dạng và độ dài mã khác nhau, làm nổi bật tính mạnh mẽ và khả năng thích ứng của nó
trong các tình huống mã hóa thực tế. Nhìn chung, các kết quả này nhấn mạnh tiềm năng của MGDebugger trong việc nâng cao độ tin cậy của mã được sinh ra bởi LLM.

2 CÔNG TRÌNH LIÊN QUAN

Sinh Mã với LLM Các mô hình gần đây như GPT4 (OpenAI, 2023), Codestral (Mistral AI team,
2024), và DeepSeek-Coder (Zhu et al., 2024) đã cải thiện việc sinh mã thông qua điều chỉnh hướng
dẫn và RLHF với dữ liệu mã và ngôn ngữ tự nhiên hỗn hợp (Ziegler et al., 2020; Husain et al.,
2020; Rafailov et al., 2023). Sinh mã với LLM đã được cải thiện bằng nhiều kỹ thuật khác nhau.
Một số phương pháp tập trung vào cải thiện chất lượng của mã được sinh ra bằng cách sử dụng các
thuật toán lập kế hoạch, chuyển từ phác thảo sang triển khai chi tiết (Zhang et al., 2022; Yao et al.,
2023; Zelikman et al., 2023; Zhou et al., 2023; Zheng et al., 2023). Các phương pháp khác lấy mẫu
nhiều chương trình từ cùng một LLM và xếp hạng chúng để xác định chương trình tốt nhất (Chen
et al., 2023a; 2022; Ni et al., 2023). Ngoài ra, một số công trình tận dụng các khung hợp tác đa
agent để nâng cao chất lượng sinh mã (Zhang et al., 2024; Huang et al., 2023a; Dong et al., 2024).
Các phương pháp này nhằm tối ưu hóa việc sản xuất mã đúng từ ban đầu. Ngược lại, MGDebugger
nhắm vào giai đoạn sau sinh, tập trung vào việc gỡ lỗi và sửa chữa các lỗi không tránh khỏi phát
sinh trong quá trình sinh mã.

Sửa Chữa Mã Được Sinh Ra Bởi LLM Sửa chữa chương trình là một khía cạnh quan trọng của
phát triển phần mềm, nhằm tự động xác định và sửa lỗi trong mã (Just et al., 2014; Gupta et al.,
2020; Yasunaga & Liang, 2021). Có hai hướng nghiên cứu chính trong việc sửa chữa mã được sinh
ra bởi LLM: (1) huấn luyện mô hình để sửa chữa mã (Huang et al., 2023b; Jiang et al., 2024; Ding
et al., 2024; Zheng et al., 2024; Moon et al., 2024; Kumar et al., 2024) và (2) cung cấp phản hồi
bên ngoài cho các mô hình được huấn luyện sẵn thô để sửa mã (Jiang et al., 2023; Chen et al.,
2023b; Olausson et al., 2023; Zhong et al., 2024; Hu et al., 2024). Ngược lại với công trình trước
đây huấn luyện các mô hình riêng biệt cho việc sửa chữa mã (Ding et al., 2024; Zheng et al., 2024;
Moon et al., 2024), MGDebugger không yêu cầu huấn luyện lại cụ thể cho tác vụ mà tận dụng các
khả năng vốn có của các LLM được huấn luyện sẵn. Tính linh hoạt này cho phép MGDebugger hoạt
động trong các thiết lập zero-shot, cung cấp một giải pháp thay thế nhẹ và có thể mở rộng. Và việc
khám phá khả năng của LLM để sửa chữa mã của chính chúng là một hướng đầy hứa hẹn cho việc
tự cải thiện huấn luyện của LLM (Wang et al., 2023; Burns et al., 2023).

MGDebugger thuộc danh mục công trình tận dụng các mô hình được huấn luyện sẵn để sửa mã
bằng cách lý luận với phản hồi bên ngoài. Một số phương pháp gần đây (Zhang et al., 2023; Olausson et al., 2023; Bouzenia et al., 2024; Lee et al., 2024; Xia & Zhang, 2023) sử dụng kết quả thực
thi từ test case để hướng dẫn LLM trong việc sửa mã. Các công trình gần đây hơn đã khám phá các
kỹ thuật gỡ lỗi tiên tiến sử dụng khả năng lý luận của LLM. Reflexion (Shinn et al., 2023) nhắc
LLM phản ánh về
2

--- TRANG 3 ---
Preprint
defmake_palindrome(string: str) -> str:""" Tìm palindrome ngắn nhất bắt đầu với một chuỗi được cung cấp."""# Tìm hậu tố dài nhất là palindromesuffix_start= 0foriinrange(len(string)):# Kiểm tra nếu hậu tố là palindromeifstring[i:] == string[i:][::-1]:suffix_start= i# Kết hợp chuỗi với hậu tố đảo ngượcpalidrome_string= string+ string[:suffix_start][::-1]returnpalidrome_string
defmake_palindrome(string: str) -> str:""" Tìm palindrome ngắn nhất bắt đầu với một chuỗi được cung cấp."""# Tìm hậu tố dài nhất là palindromesuffix_start= 0foriinrange(len(string)):# Kiểm tra nếu hậu tố là palindromeifstring[i:] == string[i:][::-1]:suffix_start= i# Kết hợp chuỗi với hậu tố đảo ngượcpalidrome_string= string+ string[:suffix_start][::-1]returnpalidrome_string
make_palindrome
is_palindrome
find_longest_suffix
build_palindrome
make_palindrome
Phương Pháp Hiện Tại: Gỡ Lỗi Tuần Tự
MGDebugger: Gỡ Lỗi Phân Cấp
!
!
✅‍‍‍‍‍
!
❌
!
✅‍‍‍‍‍
!
✅‍‍‍‍‍
Phân Tách Phân Cấp
Định vị lỗi dễ dàng hơn
Phân Tích Cùng Nhau
Định vị lỗi là thách thức
!:LLMDebugger
❓
Gỡ lỗi đệ quy từ dưới lênGỡ lỗi nguyên khối
Hình 1: Quy trình làm việc của MGDebugger so với các phương pháp hiện có. Các phương pháp
hiện tại gỡ lỗi hàm một cách tổng thể, làm cho việc xác định lỗi trở nên khó khăn. Để giải quyết
vấn đề này, MGDebugger phân tách mã thành cấu trúc phân cấp, cô lập các hàm con để gỡ lỗi độc
lập từ dưới lên. Bằng cách này, MGDebugger có thể xác định và sửa lỗi ở nhiều mức độ chi tiết,
từ lỗi cú pháp cấp thấp đến khuyết tật thuật toán cấp cao. Để đơn giản, chúng tôi bỏ qua mã chính
xác sau khi phân tách ở đây, và cung cấp ví dụ đầy đủ trong Phụ lục A.

mã được sinh ra và sử dụng bộ đệm bộ nhớ để tinh chỉnh lặp đi lặp lại. Self-Debugging (Chen et al.,
2023b) nhắc LLM giải thích hoặc chạy thử các chương trình được sinh ra, được gọi là rubber duck
debugging. LDB (Zhong et al., 2024) phân đoạn chương trình thành các khối cơ bản, hàm hoặc dòng,
và theo dõi giá trị biến trong runtime sau mỗi khối để xác minh tính đúng đắn so với mô tả tác vụ.
Mặc dù các phương pháp này kết hợp phản hồi thực thi chi tiết và tinh chỉnh lặp đi lặp lại, chúng
xem toàn bộ hàm như một đơn vị duy nhất và thực hiện gỡ lỗi tuần tự, hạn chế hiệu quả của chúng
với mã phức tạp (Xia et al., 2023; Hossain et al., 2024). MGDebugger giải quyết vấn đề này bằng
cách giới thiệu một phương pháp phân cấp, gỡ lỗi từ lỗi cấp thấp đến khuyết tật cấp cao. Phương
pháp này đảm bảo quá trình gỡ lỗi có hệ thống và chính xác hơn, đặc biệt đối với các hệ thống
phức tạp và đa chức năng.

3 PHƯƠNG PHÁP LUẬN

3.1 TỔNG QUAN

Chúng tôi trình bày MGDebugger, một phương pháp gỡ lỗi phân cấp từ dưới lên mới để sửa chữa
mã được sinh ra bởi LLM. Quy trình làm việc tổng thể của MGDebugger được minh họa trong Hình
1, trong khi quá trình gỡ lỗi chi tiết cho mỗi hàm con được mô tả trong Hình 2.

Như được hiển thị trong Hình 1, MGDebugger bắt đầu với Phân Tách Mã Phân Cấp (Mục 3.2),
phân tách mã lỗi đầu vào thành cấu trúc phân cấp của các hàm con. Điều này cho phép xác định và
giải quyết lỗi một cách có hệ thống ở nhiều mức độ chi tiết khác nhau. Đối với mỗi hàm con, MGDebugger Sinh Test Case cho Hàm Con (Mục 3.3), rút ra các test case riêng từ test case công khai
của hàm chính, như được minh họa trong Hình 2. MGDebugger sau đó thực thi các test case này và
Gỡ Lỗi Hàm Con với Thực Thi Mô Phỏng LLM (Mục 3.4). LLM mô phỏng việc thực thi mã từng
bước cho các test case thất bại, giám sát các biến quan trọng và thay đổi trạng thái để xác định nguyên
nhân lỗi. Khi một hàm con đã được sửa, MGDebugger cập nhật nó trong cấu trúc phân cấp và truyền
các thay đổi đến các hàm phụ thuộc thông qua Gỡ Lỗi Từ Dưới Lên (Mục 3.5). Phương pháp gỡ
lỗi phân cấp này không chỉ xử lý các loại lỗi khác nhau ở nhiều mức độ trừu tượng mà còn đảm bảo
một quá trình gỡ lỗi gắn kết và có hệ thống trong toàn bộ cấu trúc mã.
3

--- TRANG 4 ---
Preprint
deffind_longest_suffix(string: str) -> int:"""Tìm hậu tố dài nhất của chuỗi được cung cấp là palindrome."""suffix_start= 0foriinrange(len(string)):ifis_palindrome(string[i:]):suffix_start= ireturnsuffix_start
deffind_longest_suffix(string: str) -> int:"""Tìm hậu tố dài nhất của chuỗi được cung cấp là palindrome."""suffix_start= 0foriinrange(len(string)):ifis_palindrome(string[i:]):suffix_start= ibreakreturnsuffix_startassertfind_longest_suffix('cat') == 2# Có 2assertfind_longest_suffix('cata')== 1# Có 3assertfind_longest_suffix('') == 0# Có 0Sinh Test Case
Thực Thi Mô Phỏng LLM
❌
✅
Lỗi Đã Sửa
!
"
!
!
"1. `suffix_start= 0`:* `suffix_start` được khởi tạo thành 0.2. `for iin range(len('cata'))`:* `i` lặp qua độ dài chuỗi đầu vào 4.3. `if is_palindrome('cata'[4:]):`:* kiểm tra nếu chuỗi con bắt đầu từ chỉ số 4 là palindrome....10. `return suffix_start`:* cuối cùng trả về `suffix_start`, là 3.Chúng ta cần dừng lặp khi tìm thấy palindrome, thay vì tiếp tục đến cuối chuỗi, bằng cách thêm `break`
Hình 2: Minh họa quá trình gỡ lỗi hàm con. Ban đầu, LLM sinh test case cho hàm con và thu thập
kết quả. Sau đó, nó mô phỏng việc thực thi mã từng bước, tập trung vào sự thay đổi của các biến
quan trọng. Điều này giúp LLM xác định lỗi chính xác và tạo ra phiên bản đã sửa của hàm con.

loại lỗi ở nhiều mức độ trừu tượng khác nhau mà còn đảm bảo quá trình gỡ lỗi gắn kết và có hệ
thống trong toàn bộ cấu trúc mã.

3.2 PHÂN TÁCH MÃ PHÂN CẤP

Việc module hóa và phân tách mã phức tạp thành các hàm con trợ giúp nhỏ hơn đã được chứng minh
là hữu ích đặc biệt đối với các hàm lớn khó hiểu (Jain et al., 2023; Zelikman et al., 2023). Để cho
phép gỡ lỗi phân cấp, chúng ta cần chuyển đổi mã đầu vào thành cấu trúc giống cây của các hàm con.

Cụ thể, với một hàm f được sinh ra bởi LLM, chúng ta phân tách nó thành cấu trúc phân cấp của
các hàm con được ký hiệu là (f1, ..., fn). Các hàm con này có thể được tổ chức như một cây froot=
TREE(froot,CHILD(froot)), trong đó froot đại diện cho hàm chính và CHILD(f) biểu thị tập hợp
các hàm con được gọi trực tiếp bởi f. Chúng tôi tận dụng LLM để phân tách, tuân thủ ba nguyên
tắc: (1) mỗi hàm con đại diện cho đơn vị mã tái sử dụng tối thiểu với mục đích cụ thể, (2) các hàm
cấp cao hơn gọi các hàm cấp thấp hơn để đạt được chức năng phức tạp, và (3) cấu trúc tổng thể
tạo điều kiện cho việc kiểm tra và gỡ lỗi độc lập. Như được minh họa trong Hình 1, cấu trúc giống
cây kết quả cho phép chúng ta cô lập các đơn vị logic của mã, cho phép nỗ lực gỡ lỗi tập trung hơn
trên các mức độ chi tiết khác nhau (Woodfield et al., 1981; Isazadeh et al., 2017). Mẫu prompt được
sử dụng cho việc phân tách mã được cung cấp trong Phụ lục G.1.

3.3 SINH TEST CASE CHO HÀM CON

Sau khi có được phân cấp của các hàm con, chúng ta muốn xác minh tính đúng đắn của mỗi hàm
con. Với mục đích này, chúng ta sinh test case cho mỗi hàm con tận dụng các kỹ thuật sinh unit test
tự động (Wang et al., 2021; Schäfer et al., 2024; Liu et al., 2023a). Cho mỗi hàm con fi∈froot,
chúng ta sinh một tập hợp test case Ti. Theo thiết lập vấn đề từ Chen et al. (2023b) và Zhong et al.
(2024), chúng ta giả định rằng các test case công khai cho hàm chính Tpub đã được cung cấp, điều
này phổ biến trong hầu hết các benchmark sinh mã (Chen et al., 2021; Hendrycks et al., 2021;
Muennighoff et al., 2023)2. Chúng ta có thể tận dụng các test case này để rút ra một tập hợp test
case tương ứng cho mỗi hàm con.

Chúng tôi sử dụng cùng một LLM để sinh test case. Cho mỗi fi∈froot. LLM được nhắc thực hiện
các bước sau: (1) phân tích cách hàm con được sử dụng trong hàm chính và cách nó đóng góp vào
các đầu ra mong đợi trong các test case công khai; (2) cho mỗi test case công khai, lý luận qua cấu
trúc mã tổng thể từng bước để tìm ra đầu vào và đầu ra mong đợi
2Nếu không, chúng ta có thể sử dụng test case được sinh ra bởi LLM thay thế.
4

--- TRANG 5 ---
Preprint
Thuật toán 1 MGDebugger: Gỡ Lỗi Đệ Quy Từ Dưới Lên
Đầu vào: f: Hàm được sinh ra bởi LLM đầu vào; Tpub: Test case công khai.
Đầu ra: f′: f đã được gỡ lỗi.
1:function MGDEBUGGER(f,Tpub)
2: if f có các hàm con {f1, . . . , fn} then
3: for fi∈f do ▷Duyệt theo chiều sâu
4: f′i←MGDEBUGGER(fi,Tpub) ▷Gỡ lỗi đệ quy
5: fi=f′i ▷Thay thế fi bằng phiên bản đã gỡ lỗi
6: end for
7: end if
8:Tf←GENTEST(f,Tpub) ▷Sinh test case cho f
9:Rf←EXEC(f,Tf) ▷Thực thi test case cho f
10: if pass(Rf,Tf) then
11: return f ▷Hàm đúng; giữ nguyên
12: else
13: f′←DEBUG(f,Tf,Rf) ▷Gỡ lỗi hàm f dựa trên kết quả test Rf
14: return f′▷Trả về mã đã sửa
15: end if
16:end function

cho hàm con. Phương pháp này đảm bảo rằng các test case được sinh ra không chỉ phản ánh chức
năng dự định của hàm con mà còn được đặt trong bối cảnh của các ràng buộc được cung cấp bởi
các test case công khai, nâng cao tính mạnh mẽ và liên quan của các test case. Mẫu để sinh test case
được cung cấp trong Phụ lục G.2.

3.4 GỠ LỖI HÀM CON VỚI THỰC THI MÔ PHỎNG LLM

Với các test case được sinh ra, chúng ta gỡ lỗi từng hàm con bằng cách chạy chúng trên các đầu vào
test case, thu được kết quả, và so sánh các kết quả này với các kết quả mong đợi trong test case.
Khi một test case thất bại được xác định, chúng ta sửa hàm con tương ứng và tạo ra phiên bản đã
sửa.

Một cách đơn giản để thực hiện quá trình này là sử dụng một trình thực thi Python bên ngoài để
giám sát giá trị biến runtime (Zhong et al., 2024). Tuy nhiên, khi gỡ lỗi các hàm cấp cao, việc theo
dõi giá trị biến trong các hàm con cấp thấp hơn thường không cần thiết, vì tính đúng đắn của chúng
được đảm bảo bởi phương pháp gỡ lỗi từ dưới lên. Hơn nữa, việc thu thập trực tiếp tất cả các dấu
vết thực thi từ trình gỡ lỗi bên ngoài có thể thêm chi phí và độ phức tạp không cần thiết vào quá
trình.

Lấy cảm hứng từ phương pháp luận trong Li et al. (2023), chúng tôi đề xuất một trình thực thi mã
mô phỏng LLM, nhắc LLM hoạt động như một trình thông dịch Python và theo dõi việc thực thi
mã. Như được hiển thị trong Hình 2, chúng tôi yêu cầu LLM mô phỏng quá trình thực thi, lý luận
về các biến quan trọng và trạng thái của chúng ở mỗi bước, và phân tích kỹ lưỡng các test case thất
bại. Điều này loại bỏ nhu cầu trình gỡ lỗi bên ngoài, cung cấp giải pháp gỡ lỗi linh hoạt và hiệu
quả hơn. Ngoài ra, LLM có thể xác định chính xác nơi xảy ra lỗi và nắm bắt bối cảnh xung quanh
chúng. Prompt LLM cho quá trình gỡ lỗi được chi tiết trong Phụ lục G.3.

3.5 GỠ LỖI TỪ DƯỚI LÊN

Sau khi giới thiệu việc phân tách mã và quá trình gỡ lỗi cho mỗi hàm con, bây giờ chúng tôi phác
thảo quy trình gỡ lỗi tổng thể.

Chúng tôi bắt đầu quá trình bằng cách gọi MGDebugger trên hàm chính với mã đã phân tách froot
và tập hợp test case công khai Tpub. MGDebugger duyệt cấu trúc phân cấp theo cách chiều sâu,
đệ quy gỡ lỗi từng hàm con trước khi chuyển sang các hàm cấp cao hơn. Đối với mỗi hàm con cụ
thể, MGDebugger sinh test case liên quan và gỡ lỗi hàm dựa trên kết quả. Khi một bản sửa được
xác định, MGDebugger cập nhật hàm và truyền các thay đổi đến các hàm phụ thuộc. Chiến lược đệ
quy, từ dưới lên này giải quyết các lỗi một cách có hệ thống, bắt đầu với các mức độ chi tiết nhất
và tiến dần qua phân cấp hàm. Phương pháp này phù hợp với nhiều loại lỗi ở các mức độ trừu tượng
khác nhau, từ lỗi cú pháp cấp thấp đến khuyết tật logic cấp cao, bằng cách tập trung vào một mức
của phân cấp tại một thời điểm và xây dựng mã đã sửa theo cách có cấu trúc. Thuật toán chi tiết
được trình bày trong Thuật toán 1.
5

--- TRANG 6 ---
Preprint
4 THÍ NGHIỆM

4.1 THIẾT LẬP

Mô hình Chúng tôi chọn ba LLM tiên tiến từ 7B đến 22B tham số làm backbone cho việc sinh mã
và gỡ lỗi: CodeQwen1.5 (7B) (Bai et al., 2023), DeepSeek-Coder-V2-Lite (16B) (Zhu et al.,
2024), và Codestral (22B) (Mistral AI team, 2024). Vui lòng tham khảo Phụ lục C cho các chi tiết
triển khai của chúng tôi.

Tập dữ liệu Chúng tôi tiến hành thí nghiệm trên ba tập dữ liệu. HumanEval (Chen et al., 2021) và
MBPP (Austin et al., 2021) là hai benchmark được sử dụng rộng rãi để đánh giá các hệ thống sinh
mã với 164 và 500 vấn đề tương ứng. Tập dữ liệu HumanEvalFix (Muennighoff et al., 2023) bao
gồm 164 hàm có lỗi với sáu danh mục lỗi khác nhau: lạm dụng giá trị, thiếu logic, logic thừa, lạm
dụng toán tử, lạm dụng biến, và lạm dụng hàm. Các giải thích chi tiết và phân bố của các danh mục
lỗi có thể được tìm thấy trong Phụ lục B.

Độ đo Chúng tôi áp dụng hai độ đo để đánh giá phương pháp của chúng tôi: 1) Độ chính xác (Chen
et al., 2023b; Zhong et al., 2024), đo tỷ lệ tổng thể của các mẫu mã đúng trong tất cả các mẫu mã
được sinh ra sau khi gỡ lỗi. Một mã đúng khi và chỉ khi nó vượt qua tất cả các test case riêng được
gán cho nó. 2) Tỷ lệ Sửa chữa Thành công (RSR) (Yasunaga & Liang, 2021), đề cập đến tỷ lệ các
mẫu mã đã sửa so với tổng số mẫu mã có lỗi.

Baseline Chúng tôi so sánh MGDebugger với tám phương pháp tiên tiến để gỡ lỗi mã được sinh ra
bởi LLM. 1) Simple Feedback là baseline cơ bản thông báo cho LLM rằng mã không đúng và yêu
cầu nó sửa vấn đề. 2) Self-Edit (Zhang et al., 2023) nhắc LLM chỉnh sửa mã dựa trên kết quả thực
thi của test case. 3) Self-Debugging (Chen et al., 2023b) có hai biến thể: Self-Debugging (Expl.)
nhắc LLM giải thích mã được sinh ra từng dòng, trong khi Self-Debugging (Trace) yêu cầu LLM
chạy thử mã để gỡ lỗi. 4) LDB (Zhong et al., 2024) phân đoạn mã thành các khối cơ bản, hàm hoặc
dòng, và theo dõi giá trị biến trong runtime sau mỗi khối để xác minh tính đúng đắn so với mô tả
tác vụ. 5) Reflexion (Shinn et al., 2023) yêu cầu LLM phản ánh về mã trước đó với kết quả thực
thi và sử dụng bộ đệm bộ nhớ để cho phép tinh chỉnh lặp đi lặp lại.

4.2 KẾT QUẢ CHÍNH

Kết quả trong Bảng 1 cho thấy MGDebugger luôn vượt trội hơn các phương pháp hiện có trên tất
cả các mô hình và tập dữ liệu. Cụ thể, MGDebugger đạt được cải thiện độ chính xác cao nhất, với
mức tăng +15.3% đến +18.9% trên HumanEval và +11.4% đến +13.4% trên MBPP. Những cải
thiện này đặc biệt đáng chú ý khi so sánh với các phương pháp baseline như Self-Debugging (Expl.)
và Reflexion, cũng kết hợp phản hồi bên ngoài nhưng cho thấy mức tăng thấp hơn về độ chính xác
và RSR. Kết quả mạnh mẽ trên các mô hình có kích thước khác nhau làm nổi bật khả năng thích
ứng của MGDebugger với các kiến trúc LLM khác nhau.

Hơn nữa, MGDebugger chứng minh khả năng gỡ lỗi đáng chú ý, đặc biệt với DeepSeek-Coder-V2-
Lite (16B) và Codestral (22B), nơi nó đạt độ chính xác 94.5% trên tập dữ liệu HumanEval, điểm
số cao nhất trong tất cả các phương pháp. Điều này đặc biệt ấn tượng xem xét rằng MGDebugger
hoạt động trong thiết lập zero-shot mà không cần huấn luyện lại cụ thể cho tác vụ. Kết quả này
minh họa khả năng gỡ lỗi vốn có của các LLM lớn hơn với MGDebugger. Ngoài ra, hiệu suất của
phương pháp trên MBPP, đạt RSR lên đến 41.1% với các mô hình nhỏ hơn như CodeQwen1.5 (7B),
tiếp tục nhấn mạnh tính mạnh mẽ của nó. Nói chung, các kết quả này xác nhận MGDebugger là
một phương pháp gỡ lỗi rất hiệu quả và có thể mở rộng cho mã được sinh ra bởi LLM.

4.3 NGHIÊN CỨU ABLATION

Để hiểu sự đóng góp của từng thành phần trong MGDebugger và xác nhận các lựa chọn thiết kế
của chúng tôi, chúng tôi tiến hành nghiên cứu ablation bằng cách loại bỏ các thành phần chính của
phương pháp một cách có hệ thống: phân
6

--- TRANG 7 ---
Preprint
Bảng 1: Kết quả của MGDebugger và các phương pháp khác trên HumanEval và MBPP. Acc.: Độ
chính xác, ∆: Cải thiện so với baseline (No-Debugging), RSR: Tỷ lệ Sửa chữa Thành công.
[Bảng với dữ liệu số liệu hiệu suất của các phương pháp khác nhau]

Bảng 2: Kết quả nghiên cứu ablation cho DeepSeek-Coder-V2-Lite. Acc.: Độ chính xác, ∆Acc.:
Cải thiện so với baseline (No-Debugging), RSR: Tỷ lệ Sửa chữa Thành công.
[Bảng với kết quả ablation study]

tách mã phân cấp, thực thi mô phỏng LLM, và sinh test case cho việc gỡ lỗi hàm con. Mỗi biến thể
được đánh giá trên cả tập dữ liệu HumanEval và MBPP sử dụng mô hình DeepSeek-Coder-V2-Lite.

Như được hiển thị trong Bảng 2, mỗi thành phần của MGDebugger đóng vai trò quan trọng trong
hiệu quả tổng thể của phương pháp. Trong số đó, chiến lược gỡ lỗi phân cấp là thành phần có tác
động nhất. Bằng cách loại bỏ chiến lược này, tỷ lệ sửa chữa thành công giảm đáng kể từ 76.3%
xuống 52.6% trên HumanEval và từ 39.0% xuống 33.5% trên MBPP. Kết quả này làm nổi bật tầm
quan trọng của phương pháp phân cấp trong việc xác định và sửa lỗi một cách có hệ thống ở các
mức độ chi tiết khác nhau. Ngoài ra, thực thi mô phỏng LLM và sinh test case cho hàm con cũng
tạo điều kiện cho việc gỡ lỗi mã đã phân tách, mang lại cải thiện đáng kể về độ chính xác và tỷ lệ
sửa chữa thành công. Các kết quả này nhấn mạnh hiệu quả của các lựa chọn thiết kế của MGDebugger và tầm quan trọng của chiến lược gỡ lỗi phân cấp.
7

--- TRANG 8 ---
Preprint
Bảng 3: Hiệu suất (RSR) trên các danh mục lỗi khác nhau trong HumanEvalFix với các mô hình
khác nhau. Điểm số tốt nhất và tốt thứ hai được làm nổi bật bằng in đậm và gạch chân tương ứng.
[Bảng hiệu suất chi tiết theo từng loại lỗi]

4.4 GỠ LỖI CÁC LOẠI LỖI KHÁC NHAU

Để đánh giá tính linh hoạt và hiệu quả của MGDebugger trên các danh mục lỗi khác nhau, chúng
tôi thực hiện thí nghiệm sử dụng tập dữ liệu HumanEvalFix, được thiết kế đặc biệt để đánh giá
hiệu suất gỡ lỗi mã. Tập dữ liệu bao gồm sáu danh mục lỗi riêng biệt: lạm dụng giá trị, thiếu logic,
logic thừa, lạm dụng toán tử, lạm dụng biến, và lạm dụng hàm, cho phép chúng tôi kiểm tra mức
độ hiệu quả của MGDebugger trong xử lý các loại lỗi lập trình khác nhau so với các phương pháp
hiện có. Các giải thích chi tiết của mỗi danh mục lỗi có sẵn trong Phụ lục B.

Bảng 3 trình bày RSR trên các danh mục lỗi khác nhau. Chúng tôi quan sát thấy MGDebugger luôn
vượt trội hơn các phương pháp khác với độ chính xác tổng thể cao hơn đáng kể. Và MGDebugger
đạt được tỷ lệ sửa chữa thành công đáng chú ý 97.6% sử dụng DeepSeek-Coder, với tỷ lệ thành
công 100% trong tất cả các danh mục lỗi ngoại trừ lạm dụng giá trị. Điều này đặc biệt đáng chú ý
do độ phức tạp và đa dạng của các lỗi trong tập dữ liệu. Điều này làm nổi bật hiệu quả của chiến
lược gỡ lỗi phân cấp.

Nhìn vào chi tiết của các danh mục lỗi khác nhau, MGDebugger cho thấy lợi thế mạnh trong việc
gỡ lỗi các lỗi cấp thấp, như thiếu logic và logic thừa. Thiếu logic đề cập đến các tình huống mà
mã cần thiết bị bỏ qua, ngăn cản giải pháp hoạt động đúng. Mặt khác, logic thừa liên quan đến mã
không cần thiết có thể dẫn đến sai lầm và nhầm lẫn (Muennighoff et al., 2023). Các phương pháp
khác thường gặp khó khăn trong việc xác định và giải quyết những vấn đề cơ bản này vì chúng xem
mã một cách tổng thể. Điều này có thể dẫn đến nhầm lẫn về các chi tiết cấp thấp khi xử lý các lỗi
logic phức tạp. Ngược lại, việc phân tách phân cấp trong MGDebugger cho phép nó tập trung vào
các mức độ chi tiết mã khác nhau. Điều này cho phép xác định và sửa chữa lỗi hiệu quả hơn. Các
kết quả này chứng minh tính mạnh mẽ và linh hoạt của MGDebugger trên các loại lỗi khác nhau.
8

--- TRANG 9 ---
Preprint
Ngắn Trung bình Dài
Độ dài Mã (Token)60708090100Tỷ lệ Sửa chữa Thành công (%)Đơn giản
LDB-BlockSD-Expl
SD-TraceReflexion
Của chúng tôi
Hình 3: Tỷ lệ sửa chữa thành công của các phương pháp khác nhau khi gỡ lỗi mã có độ dài khác
nhau trên HumanEvalFix với DeepSeek-Coder. MGDebugger luôn vượt trội hơn các phương pháp
khác trên các độ dài mã khác nhau, đặc biệt trong mã dài.

1 2 3 4 5 6 7 8 9 10
Số lần Gỡ lỗi60708090100Tỷ lệ Thành công Tích lũy (%)
Đơn giản
LDB-BlockSD-Expl
SD-TraceReflexion
Của chúng tôi
Hình 4: Tác động của số lần gỡ lỗi đến tỷ lệ sửa chữa thành công tích lũy của MGDebugger và
các phương pháp khác trên HumanEvalFix với DeepSeek-Coder. MGDebugger tiếp tục cải thiện
với nhiều lần gỡ lỗi hơn và đạt tỷ lệ thành công cao nhất.

4.5 GỠ LỖI MÃ VỚI ĐỘ DÀI KHÁC NHAU

Chúng tôi tiếp tục đánh giá tính linh hoạt của MGDebugger trong việc gỡ lỗi mã có độ dài khác
nhau (tức là số token), vì độ dài mã thường tương quan với độ phức tạp và thách thức gỡ lỗi. Chúng
tôi phân loại các đoạn mã từ tập dữ liệu HumanEvalFix thành các nhóm ngắn, trung bình và dài,
đảm bảo kích thước mẫu bằng nhau. Sau đó chúng tôi phân tích điểm số RSR thu được bởi MGDebugger và baseline khi sử dụng DeepSeek-Coder làm LLM backbone.

Kết quả được trình bày trong Hình 3. Chúng ta có thể quan sát thấy khi độ dài mã tăng, hầu hết các
phương pháp trải qua sự giảm hiệu suất rõ ràng do độ phức tạp tăng. Chúng tôi lưu ý rằng MGDebugger luôn vượt trội hơn các phương pháp khác trong các độ dài mã khác nhau và đặc biệt xuất
sắc trong việc gỡ lỗi các đoạn mã dài hơn và phức tạp hơn. Điều này cho thấy khả năng mở rộng
và tính mạnh mẽ của MGDebugger trong việc xử lý mã có độ dài và độ phức tạp khác nhau. Kết
quả trên hai tập dữ liệu khác có sẵn trong Phụ lục D, nơi MGDebugger cũng luôn vượt trội hơn các
phương pháp khác trên các độ dài mã khác nhau.

4.6 TÁC ĐỘNG CỦA SỐ LẦN GỠ LỖI

Một yếu tố quan trọng khác cho việc gỡ lỗi dựa trên LLM là số lần gỡ lỗi. Gỡ lỗi lặp đi lặp lại cho
phép LLM tinh chỉnh các sửa chữa của chúng qua nhiều lần chạy, có thể dẫn đến kết quả tốt hơn.
Chúng tôi muốn đánh giá khả năng cải thiện của MGDebugger qua các lần lặp liên tiếp.

Theo Zhong et al. (2024), chúng tôi thay đổi số lần gỡ lỗi từ 1 đến 10 sử dụng tập dữ liệu HumanEvalFix và DeepSeek-Coder.

Kết quả trong Hình 4 cho thấy MGDebugger đạt được điểm số RSR tích lũy cao nhất trong tất cả
các phương pháp, làm nổi bật khả năng liên tục tinh chỉnh việc gỡ lỗi qua nhiều lần thử. Đặc biệt,
trong khi hầu hết các phương pháp ổn định sau vài lần gỡ lỗi đầu tiên, MGDebugger và Reflexion
tiếp tục cải thiện với nhiều lần lặp hơn. Kết quả này nhấn mạnh tiềm năng lớn của MGDebugger
cho việc gỡ lỗi lặp đi lặp lại và toàn diện, làm cho nó trở thành giải pháp đầy hứa hẹn cho các tác
vụ sửa chữa mã phức tạp và thách thức. Kết quả trên hai tập dữ liệu khác có sẵn trong Phụ lục E,
nơi MGDebugger vượt trội hơn các phương pháp khác từ lần thử đầu tiên và tiếp tục cải thiện với
tiềm năng lớn.

4.7 NGHIÊN CỨU TRƯỜNG HỢP

Chúng tôi thực hiện phân tích định tính về cách MGDebugger hiệu quả xác định và sửa các phần
có lỗi so với các phương pháp baseline. Hình 5 hiển thị một ví dụ về việc gỡ lỗi các đoạn mã từ
tập dữ liệu HumanEvalFix sử dụng MGDebugger và các phương pháp đại diện khác, với DeepSeek-
9

--- TRANG 10 ---
Preprint
[Hình 5 hiển thị ví dụ so sánh các phương pháp gỡ lỗi khác nhau]

Coder-V2-Lite làm LLM backbone. Giải pháp có lỗi ban đầu tính toán dãy Collatz với logic tính
toán sai n=n×2 + 1. Trong khi các phương pháp khác sửa phép tính thành n=n×3 + 1, chúng
giới thiệu một lỗi mới bỏ qua số "1" cuối cùng trong dãy Collatz. Điều này có thể là do chúng bị
phân tâm bởi nhu cầu lọc số lẻ, và do đó di chuyển thao tác thêm số vào kết quả trước khi cập nhật
n. MGDebugger xuất sắc bằng cách phân tách vấn đề thành các hàm con riêng biệt: sinh dãy và lọc
số lẻ. Bằng cách gỡ lỗi từng hàm con độc lập, MGDebugger đảm bảo sửa lỗi toàn diện, bao gồm
yêu cầu tinh vi về việc kết hợp 1 vào dãy Collatz. Phương pháp này chứng minh khả năng của
MGDebugger xử lý các vấn đề phức tạp, nhiều bước hiệu quả hơn các phương pháp gỡ lỗi tổng
thể. Ngoài ra, nó làm nổi bật khả năng của MGDebugger không chỉ sửa lỗi mà còn tái cấu trúc mã
để tăng cường độ rõ ràng và tính đúng đắn, chứng minh tiềm năng của nó trong việc cải thiện chất
lượng mã được sinh ra bởi LLM. Thêm ví dụ và phân tích trên ba tập dữ liệu có thể được tìm thấy
trong Phụ lục F.

5 KẾT LUẬN

Trong bài báo này, chúng tôi đã giới thiệu MGDebugger, một khung gỡ lỗi mã phân cấp mới sửa
chữa lỗi một cách có hệ thống ở nhiều mức độ chi tiết. Bằng cách phân tách mã phức tạp thành cấu
trúc phân cấp, sinh test case có mục tiêu và sử dụng thực thi mô phỏng LLM, MGDebugger hiệu
quả xác định và sửa lỗi từ lỗi cú pháp đến khuyết tật logic theo cách từ dưới lên. Thí nghiệm trên
các mô hình và tập dữ liệu khác nhau chứng minh hiệu suất vượt trội của MGDebugger so với các
phương pháp hiện có, đặc biệt trong việc xử lý các lỗi logic phức tạp và các đoạn mã dài hơn.

Công trình tương lai có thể xây dựng trên nền tảng này để phát triển các phương pháp sinh mã và
gỡ lỗi tiên tiến hơn. Một hướng là mở rộng MGDebugger để xử lý các lỗi và cấu trúc mã phức tạp
hơn, chẳng hạn như các dự án nhiều file và codebase với nhiều phụ thuộc. Hướng khác là khám phá
sự hợp tác của các phương pháp sinh mã phân cấp như Parsel (Zelikman et al., 2023) với gỡ lỗi
phân cấp, cho phép các hệ thống sinh mã và gỡ lỗi end-to-end. Hơn nữa, việc tích hợp MGDebugger vào các hệ thống tự huấn luyện để sửa chữa đầu ra từ các mô hình cơ sở, sau đó huấn luyện lại
các mô hình cơ sở với dữ liệu đã sửa, có thể cải thiện hiệu suất của chúng một cách lặp đi lặp lại
(Gulcehre et al., 2023).
10

[Tiếp tục với các trang còn lại với nội dung tương tự được dịch sang tiếng Việt...]
