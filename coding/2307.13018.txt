# 2307.13018.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2307.13018.pdf
# File size: 350797 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2307.13018v1  [cs.CL]  24 Jul 2023The potential of LLMs for coding with low-resource and
domain-speciﬁc programming languages∗
Artur Tarassow
atecon@posteo.de
July 26, 2023
Abstract
This paper presents a study on the feasibility of using large language models (LLM)
for coding with low-resource and domain-speciﬁc programmi ng languages that typically
lack the amount of data required for eﬀective LLM processing techniques. This study
focuses on the econometric scripting language named hansl o f the open-source software
gretl and employs a proprietary LLM based on GPT-3.5. Our ﬁnd ings suggest that
LLMs can be a useful tool for writing, understanding, improv ing, and documenting
gretl code, which includes generating descriptive docstri ngs for functions and providing
precise explanations for abstract and poorly documented ec onometric code. While the
LLM showcased promoting docstring-to-code translation ca pability, we also identify
some limitations, such as its inability to improve certain s ections of code and to write
accurate unit tests. This study is a step towards leveraging the power of LLMs to
facilitate software development in low-resource programm ing languages and ultimately
to lower barriers to entry for their adoption.
Key Words : large language models; coding; programming; low-resourc e language; domain-
speciﬁc; gretl; hansl.
∗I am grateful for the helpful comments of Steﬀen Remus, Allin Cottrel, Riccardo (Jack) Luchhetti and
Sven Schreiber, and for the discussion of participants at th e 8th gretl conference in Gdańsk (June 2023).
Any errors or omissions are the sole responsibility of the au thors.
1

--- PAGE 2 ---
1 Introduction
Large language models (LLMs) have emerged in computer scien ce and are revolutionizing
the interplay of human (natural) language and computation. The most well-known ones are
GPT-3.5 (a base LLM) and ChatGPT1(an instruction-tuned LLM), both of which have
gained signiﬁcant attention for their impressive language generation capabilities.2
Automatic code generation is a long-standing challenge in c omputer science (Manna and
Waldinger, 1971). Large corpora comprising code and the training of huge lan guage models
have fuelled research in this direction. The potential bene ﬁts of using LLMs for programming
comprise assistance with code creation, documentation, ex planation, and description. For
instance, LLMs may assist with naming variables and functio ns, help with code refactoring.
Previous studies have investigated the eﬀectiveness of LLM s as a programming assistant
for various tasks typically faced by programmers. However, most of the research has fo-
cused on high-resource general-purpose programming langu ages, such as Python, C++, or
JavaScript (M. Chen et al., 2021; Wang and Komatsuzaki, 2021; Tian et al., 2023). There
has been only limited research on low-resource programming languages, which often lack
large amounts of data, such as source code or references, mak ing them challenging for LLMs
to process compared to high-resource languages. Recent exc eptions are Zügner et al. ( 2021),
Ahmed and Devanbu ( 2022b ), and F. Chen et al. ( 2022) who studied the performance of
LLMs for coding with low-resource but general-purpose prog ramming languages.
Domain-speciﬁc programming languages also exist and oﬀer h igh levels of abstraction.
Due this they can be more eﬃcient than general-purpose langu ages for speciﬁc domain-
related tasks, such as ﬁnancial modelling, econometrics, o r scientiﬁc computing. However,
current research on LLMs as programming assistants has focu sed only on a limited subset
of programming languages, leaving the vast majority unders tudied. The aim of this paper
is to investigate how LLMs may assist with the use of a low-res ource anddomain-speciﬁc
programming language. By exploring the potential of LLMs in t his context, this paper aims
to contribute to ﬁlling the gap in research on such languages .
For this study, we choose scripting language named hansl. Ha nsl (a recursive acronym:
“hansl’s a neat scripting language”) is the scripting langu age of gretl3, an open source econo-
metrics and statistics package written in C and licensed und er the GNU GPL (Cottrell,
2017).4It can compete with the leading paid econometrics packages o f Stata and Eviews,
as well as the top open-source statistical software project , R.5Gretl is mainly used by
economists and econometricians when doing applied work as w ell as simulations.6
While Python and R code are highly prevalent, the amount of gr etl code available
is relatively limited.7This means that the data available for training LLMs to perfo rm
speciﬁc tasks using gretl code is far less comprehensive tha n that available for Python or
R code. As a result, existing LLMs may not be as eﬀective when a pplied to gretl code
1https://chat.openai.com/
2You may think that "revolutionizing" may be exaggerated. Ho wever, researchers from Microsoft Re-
search argue that the recent GPT-4 model "[...] could reason ably be viewed as an early (yet still incomplete)
version of an artiﬁcial general intelligence (AGI) system" (Bubeck et al., 2023).
3gretl.sourceforge.net
4For a practical introduction to gretl, see e.g. Tarassow ( 2019).
5https://www.r-project.org/
6Note, in order to avoid confusion, we use gretl as a synonym fo r hansl throughout the text.
7We did a search on github.com to identify ﬁles relevant to gretl and hansl. The search quer y used was
as follows: ("gretl" OR "hansl") AND (path:*.inp OR path:*.gfn) . This query searched for ﬁles that
contained either the term "hansl" or "gretl" within ﬁles wit h either the extension inp(gretl script ﬁle) or
gfn(gretl package ﬁle). The search results showed that there we re 1500 such ﬁles available on GitHub. It
is notable that the number of ﬁles available for R and Python a re multiple factors higher in comparison:
Searching for language:R revealed a total of 5.1 million ﬁles while searching for Pyth on on GitHub revealed
ten times as many ﬁles. The search was conducted on the 2023-0 7-18.
2

--- PAGE 3 ---
tasks used in applied research. Despite these limitations, we demonstrate that LLMs can
still be applied successfully to certain coding tasks using gretl. Speciﬁcally, in the ﬁeld of
(applied) econometrics, LLMs can help users understand use r-written functions involving
linear algebra and statistics. They can also provide sugges tions for coding solutions based
on natural language prompts. Overall, LLMs have the potenti al to save time, minimize or
avoid coding errors, and increase productivity for both pro grammers, econometricians, and
other knowledge workers. This indicates that publicly exis ting models are able to generalize
well to low-resource programming languages (Zügner et al., 2021).
We conduct a series of tasks applied coding economists or eco nometricians may face
such as code documentation, code explanation and descripti on (with special focus on the
econometrics involved), variable and function naming, cod e creation as well as refactoring
of code. We also present examples on how gretl code can be writ ten by the LLM based on a
docstring only. We show that the LLM also writes gretl code fo r computing the Fibonacci
sequence or the root mean squared error including only minor syntactical errors. In a
last experiment, we simulate a simple coursework exercise f or an introductory econometric
class and show how the LLM writes the gretl script according t o the class instructions to
some extent. For this, we use a proprietary instruction-bas ed LLM provided for free (after
registration) by www.you.com . The model is a generative pre-trained transformer using th e
ChatGPT framework which has around 175 billion parameters a nd is based on GPT-3.5.
It should be said that we were unable to use a "vanilla" pre-tr ained model. The publicly
usable model relies on human-in-the-loop (HITL) feedback t o improve its performance.8
Since the beginning of June 2023, the author has actively use d the model and provided
feedback on gretl code-related prompts, allowing the model ’s performance to continually
improve over time. However, this introduces a potential cha llenge in assessing the model’s
true capabilities, as it becomes diﬃcult to determine wheth er the model can generate work-
ing gretl code on its own, or if it relies solely on the HITL fee dback to do so. While this
issue is not the focus of our paper, we recognize the potentia l impact it may have on the
results.
In the next section, we provide a brief background on LLMs and summarize the recent
literature on both the use of LLMs for programming generally as well as on low-resource
programming languages speciﬁcally. Afterwards, we presen t the experiments of applying
the LLM to gretl code and discuss the results. The paper ends w ith a conclusion.
2 Background
2.1 Large Language Models
Modern LLMs rely on deep learning algorithms and large datas ets to process and understand
human language and generate text. Nowadays, they use the tra nsformer architecture, which
were introduced by Google in 2017, as the backbone of their mo dels (Vaswani et al., 2017).
The transformer architecture is a neural network that is cap able of processing long sequences
of input data in parallel, resulting in more eﬃcient and eﬀec tive training.
The key innovation of the transformer is the self-attention mechanism, which allows
the model to identify which parts of the input are relevant fo r each output. This enables
the model to identify and learn more complex relationships b etween the input and output,
resulting in better performance on a wide range of natural la nguage processing tasks. For
details see also Radford et al. ( 2019) and Wolfram ( 2023). Current architectures can be
8HITL is a machine learning method that involves incorporati ng human feedback at various stages of
training in order to enhance the accuracy and overall model p erformance. This is particularly important in
the context of natural language processing, where the intri cacies of language can be diﬃcult for models to
interpret without additional guidance.
3

--- PAGE 4 ---
autoregressive models that have a memory of the last Nword-pieces (potentially solely
parts of a word). Their pre-training objective is to predict the next token and the next
sentence.
LLMs are used for many tasks such as machine translation, sen timent analysis, text
completion, summarization and extraction etc.
2.2 Large language models used for programming
Automatic code generation is a longstanding challenge (Man na and Waldinger, 1971). Large
corpus comprising code (Husain et al., 2020) and the training of huge language models such
as GPT-J (Wang and Komatsuzaki, 2021) have fuelled research in this direction.9In the
following, we provide a brief overview of recent research on the use and eﬀectiveness of
LLMs for programming.
M. Chen et al. ( 2021) introduce Codex, a GPT language model ﬁne-tuned on publicl y
available code from GitHub, and evaluate its Python code-wr iting capabilities. The study
reveals that Codex signiﬁcantly outperforms GPT-3 and GPT- J on a new evaluation set
designed to measure functional correctness for synthesizi ng programs from docstrings.10
The company GitHub published its product Copilot which reli es on OpenAI’s Codex
model.11Copilot is a programming assistant for various general-pur pose programming lan-
guages widely used in industry. The model is trained on billi ons lines of code to turn natural
language prompts into coding suggestions.
Xu et al. ( 2022) ﬁnd that existing open-source models perform similar to cl osed-source
models such as Codex in some programming languages, althoug h targeted mainly for natural
language modelling. The authors publish a LLM named PolyCod er which is the ﬁrst public
model trained exclusively on code from multiple programmin g languages. Also, the authors
claim that training on natural language text and code jointl y can beneﬁt code modelling.
Recently, Tian et al. ( 2023) studied the potential of ChatGPT as a programming assis-
tant on code generation, detecting and ﬁxing bugs as well as c ode summarization. They
compare the model against two benchmark approaches using Py thon as the programming
language being evaluated. One benchmark is using LeetCode w hich is an online platform
oﬀering a diverse range of programming problems for softwar e engineering interviews, where
new problems are constantly posted and updated. The second b enchmark is named Refac-
tory which is a semantic-based assignments repair tool writ ten in Python and released in
2019. ChatGPT dominates the benchmarks on code writing and p erforms equally well to
Refactory on program repair. On code summarization their re sults show that ChatGPT
may have problems in explaining the intention of code though .
Yuan et al. ( 2023) evaluate the performance of ChatGPT for writing automatic unit tests.
ChatGPT’s generated tests were analyzed to evaluate its cor rectness, suﬃciency, readability,
and usability. The results showed that while the tests still suﬀer from correctness issues,
passing tests generated by ChatGPT resemble manually-writ ten tests in terms of coverage,
readability, and developer preference, suggesting that ge nerating unit tests with ChatGPT
could be promising.
Bubeck et al. ( 2023) evaluate GPT-4 for coding challenges. They ﬁnd that GPT-4 d oes
much better on the HumanEval, a docstring-to-code dataset c onsisting coding problems,
9GPT-J is a large-scale language model that has been trained o n a massive amount of text data and is
general in nature, meaning it can perform a wide range of task s such as text generation, translation and
summarization. While GPT-J is powerful enough to generate c ode, it is not speciﬁcally tailored for this
task like the well-known "code-davinci" models are.
10A docstring is a string literal speciﬁed in the source code th at is used to document a speciﬁc segment of
code such as a function. It is used to provide information on w hat a particular piece of code does, its input
parameters, expected return values, and any relevant usage example
11https://github.com/features/copilot/
4

--- PAGE 5 ---
compared to state-of-the-art LLMs trained speciﬁcally on c ode. The authors also compare
leading LLMs on the LeetCode dataset and state that "GPT-4 si gniﬁcantly outperforms the
other models, and is comparable to human performance..." (Bu beck et al., 2023, ch. 3.1.1).
They even go further and ﬁnd that GPT-4 can be used to create co mplex data visualization,
write a 3D game in HTML with JavaScript, using a very high-lev el speciﬁcation, customize
an deep-learning optimizer module involving operations su ch as applying SVD and linear
algebra just by giving human language description. Further more, GPT-4 reverse-engineers
assembly code, reasons about code execution of C code and run s a complex Python method
by simulating the steps instead of running the program actua lly.
Korinek ( 2023) is a rare exception studying the value-added of LLMs for eco nomic
research including coding. Apart from writing Python code f or computing and plotting the
Fibonacci sequence, he shares his experience that Open AI’s model name text-davinci-003
is not yet able to write code for simulating basic economic pr oblems like optimal consumption
smoothing. According to Korinek, current LLMs can also be us ed to translate short pieces
of Python into Matlab code or debug Python functions. For mor e complex code human
assistance is still required, though.
2.3 Low-resource programming language and coding
Low-resource programming languages (LRPLs) are character ized by a low number of records
in their datasets, which means that there is little informat ion, or even none, comprising
source code or references. This lack of information makes it challenging for language models
to process LRPL data. Although models can be trained to achie ve high accuracy for tasks
such as intent classiﬁcation and sequence labelling, these tasks require large amounts of
labelled data in the target language.
However, due to their scarcity of resources, LRPLs are less s tudied. This limitation
is similar to that experienced in natural language processi ng of low-resource languages, as
elucidated by Mastel et al. ( 2023). The authors study the limited availability of natural
language understanding resources for African languages, m ost of which are considered low
resource languages.
Despite the importance of LRPLs in software development, li ttle research exists on how
pre-trained LLMs perform on them. Exceptions are Zügner et a l. (2021) who have demon-
strated that training a model with data from multiple progra mming languages enhances its
performance on individual languages. This ﬁnding is especi ally true for LRPLs and sug-
gests that multilingual LLMs may be able to generalize well a cross programming languages.
Recent research by Ahmed and Devanbu ( 2022b ) also supports this view, particularly in
the areas of code summarization, code retrieval, and functi on naming.
On the other hand, F. Chen et al. ( 2022) focus on the low-resource (general-purpose)
programming language Ruby. The authors ﬁne-tune both monol ingual and multilingual
pre-trained language models for this language, respective ly, and evaluate their performance
in code search and code summarization. Their results indica te that multilingual models
perform less well than monolingual ones.
In their recent work, Gong et al. ( 2022) have introduced the MultiCoder LLM, which
aims to improve code completion performance on LRPLs. The au thors achieve this by
adding new layers to a pre-existing LLM in order to enhance it s performance, demonstrating
that MultiCoder outperforms their monolingual baselines.
For our study, it is relevant to examine the generalization a bility of popular LLMs such
as GPT-3.5 to the programming language used in the open-sour ce software gretl. While
existing research focuses on low-resource but general programming languages, we put our
focus on a domain-speciﬁc LRPL.
5

--- PAGE 6 ---
3 Tasks
This section explores several programming tasks that are re levant to applied economists
and econometricians. Firstly, we discuss how to improve the documentation of gretl code
by writing docstrings based on the code itself. In the follow ing section, we demonstrate how
the LLM can be used to translate docstrings into executable c ode. Moving on, we show
how the LLM can help summarize or describe code, and suggest s elf-explanatory variable
and function names for better readability and maintainabil ity of the code. We then further
explore how the LLM can aid in refactoring gretl code and writ ing unit tests. Finally, we
provide a brief example of how the LLM can be utilized in trans lating real-world student
exercises from human language into gretl code.12
3.1 Create docstrings from code bodies
In the software industry, the lack of high-quality code docu mentation is a common challenge.
Docstrings are commonly used in code to address this issue. D ocstrings provide clear and
concise explanations of the code’s functionality, simplif ying collaboration among developers.
Docstrings also help ensure consistency of documentation a cross diﬀerent code bases and
modules. Additionally, API documentation can be generated automatically from docstrings.
Moreover, docstrings can be used to create unit tests, which can improve code quality over
time. However, creating and updating docstrings is labor-i ntensive, which has prompted
research into automatic code summarization (Sun et al., 2023). This paper investigates
whether it is feasible to generate docstrings from gretl cod e bodies following an exercise by
M. Chen et al. ( 2021).
We examine the creation of a docstring for the gretl function add_prefix_to_any in
Listing 1. The function accepts an array of strings as input and adds a p reﬁx followed by
each array item, returning the updated array. In case the inp ut array targets is empty,
the function throws an error. To avoid confusion, we have enc losed the relevant code in a
block of triple backticks.
Listing 1: Prompt to write a docstring for add_prefix_to_any()
Write a docstring for a gretl function. The gretl function is provided in triple backtics:
‘‘‘
function strings add_prefix_to_any (const strings target s, const string prefix)
strings ret = array(nelem(targets))
errorif(nelem(ret) == 0, "Array is empty.")
if nelem(prefix) == 0
return targets
endif
loop foreach i targets
string ret[i] = sprintf("%s%s", prefix, "$i")
endloop
return ret
end function
‘‘‘
The resulting docstring for the add_prefix_to_any function is provided in Listing 2.
The description accurately conveys the purpose of the funct ion, while the input parameter
12All exercises were conducted in June 2023. Since mid of July 2 023,www.you.com has switched from
GPT-3.5 to GPT-4.0.
6

--- PAGE 7 ---
descriptions and types are precise. Moreover, the docstrin g thoroughly explains the func-
tion’s output. Remarkably, the LLM recognizes the error cas e, even though gretl does not
support exceptions (speciﬁcally, of type "ValueError").
Listing 2: Answer on writing a docstring for add_prefix_to_any()
Add a prefix to each element in an array of strings.
:param targets: The array of strings to modify.
:type targets: array of string
:param prefix: The prefix to add to each element in the array.
:type prefix: string
:return: A new array of strings with the prefix added to each e lement.
:rtype: array of string
:raises: ValueError if the input array is empty.
The ﬁrst example may appear simple, and it is not surprising t hat the LLM can ac-
curately generate a docstring. However, it is important to n ote that (1) the LLM is most
likely not explicitly trained on gretl’s scripting languag e, and (2) the datatype strings is
not present in other popular programming languages.
The next example involves a much more complex function calle dkernel_loop , taken
from the "ketvals" package (v0.91) by Lucchetti and Valenti ni (2023). The function name
and the parameter names passed are not very meaningful, as th ey consist mostly of single
letters (e.g. yorZ), and there are no comments. Additionally, the function cal ls another
private function called kernelmat , which the LLM has not seen before. In order to write
an accurate and precise docstring, deep domain knowledge is required, as there are math-
ematical operations (involving linear algebra) and a loop b lock. The prompt for writing a
docstring can be found in Listing 3.
Listing 3: Prompt to write a docstring for kernel_loop()
Write a docstring for a gretl function. The gretl function is provided in triple backtics:
‘‘‘
function matrix kernel_loop (const matrix y,
const matrix X,
const matrix Z,
scalar h,
scalar type[0:4:1],
matrix optparm[null])
scalar q = cols(y)
scalar k = cols(X)
scalar T = rows(y)
matrix ret = mshape(NA,T,q*k)
W = kernelmat(T, type, h, optparm)
ini = T + 1
fin = 2*T
loop t = 1 .. T
matrix PX = W[ini:fin] .* Z
matrix num = PX’y
matrix den = PX’X
ret[t,] = vec(den\num)’
ini--
fin--
endloop
return ret
end function
‘‘‘
7

--- PAGE 8 ---
The output generated by the LLM for the kernel_loop function is displayed in Listing
4. The docstring correctly identiﬁes the function as executi ng a kernel regression. Surpris-
ingly, despite the non-meaningful and single-letter namin g of the parameters, the LLM still
manages to accurately describe them. For example, the LLM re fers to the hparameter as
the kernel bandwidth parameter, which is correct.
There is only one error (coloured in red) in the LLM’s output: for thetypeparameter,
the LLM incorrectly states that it is an array of integers, wh en in fact it is a scalar value.
Although an array of integers is not supported by gretl, the s yntaxscalar type[0:4:1]
used in the function signature indicates the minimum, maxim um, and default values, rather
than an array of integers.
Interestingly, the LLM identiﬁes an error in the function ev en though no explicit error is
raised in the code body. This is because the LLM recognizes th at a linear algebra operation
such as matrix multiplication can throw an error if the dimen sions of the matrices do not
match.
Listing 4: Answer on writing a docstring for kernel_loop() . Syntactical error coloured in
red.
Perform a kernel regression using a loop and return the param eter estimates.
:param y: The dependent variable matrix.
:type y: matrix
:param X: The independent variable matrix.
:type X: matrix
:param Z: The kernel weight matrix.
:type Z: matrix
:param h: The kernel bandwidth parameter.
:type h: scalar
:param type: An array of integer values representing the ker nel type.
:type type: array of int
:param optparm: Optional parameters used to calculate the k ernel.
:type optparm: matrix or None
:return: A matrix containing the parameter estimates for ke rnel regression.
:rtype: matrix
:raises ValueError: If the input arrays are not correctly fo rmatted.
3.2 Create executable code from docstrings
Docstrings provide a clear and concise description of the co de’s functionality and purpose.
By translating these descriptions into actual code, program mers can more easily understand
the intended behavior of the program and ensure that they are implementing the desired
functionality. Additionally, this process can help improv e the accessibility of code to non-
technical stakeholders or users who may not have a programmi ng background but want to
understand the software’s functionality. Thus, translati ng docstrings into programming code
is relevant, necessary, and contributes signiﬁcantly to th e quality and usability of software
applications.
M. Chen et al. ( 2021) have recently found evidence that current-state LLMs are a ble to
write executable Python code based on information containe d in docstrings only. Bubeck
et al. ( 2023) evaluate the ability of the GPT-4 model to translate docstr ings to code and
show that this LLM is comparable to human levels.
In this exercise, we want the LLM to write executable gretl co de based on a docstring.
As an initial example, we make use of the docstring the LLM pro vided in Subsection 3.1
for function add_prefix_to_any() . We start by opening a new session to clear the LLM’s
8

--- PAGE 9 ---
memory. Listing 5presents the prompt for creating a gretl function.
Listing 5: Prompt to write a gretl function based on a docstri ng
Write a gretl function based on the following docstring. The docstring is in triple
backtics.
‘‘‘
Add a prefix to each element in an array of strings.
:param targets: The array of strings to modify.
:type targets: array of string
:param prefix: The prefix to add to each element in the array.
:type prefix: string
:return: A new array of strings with the prefix added to each e lement.
:rtype: array of string
:raises: ValueError if the input array is empty.
‘‘‘
Listing 6presents the proposed function and a detailed description. While the LLM
provides a code skeleton that serves as a good starting point , the code itself contains several
syntax errors. For instance, the function signature lacks a return type, and the datatype
array_string is not recognized by gretl - strings should be used instead. Furthermore,
array_length() is not a built-in function in gretl, as the LLM notes. Additio nally, the
initialization of the string array new_array is incorrect since the array’s dimension is not
set. As a result, accessing a speciﬁc entry in the object (suc h asnew_array[i] within the
loop-block) is not possible. Lastly, the string concatenat ion will not work.
Despite these issues, the proposed function serves as a usef ul skeleton for developers
who want to structure a function that accomplishes the task, given the docstring written
in plain language. The LLM provides a highly detailed descri ption of the individual steps,
which can be particularly beneﬁcial for ﬁxing the bugs.
Listing 6: Code created for prompt in Listing 5. Syntactical error coloured in red.
function <RETURN TYPE IS MISSING> array_add_prefix( array_string targets, string prefix)
ifarray_length (targets) == 0
errorValueError: The input array cannot be empty."
endif
array_string new_array
loop i=1.. array_length (targets)
new_array[i] = prefix $ targets[i]
endloop
return new_array
end function
This function takes two arguments:
targets: an array of strings to modify
prefix: the prefix to add to each element in the array
It first checks if the input array is empty, and if so, throws a ValueError.
Then, it initializes a new array new_array to store the modif ied strings, and loops
through each element in targets. For each element, it concat enates the prefix with the
element using the $ operator, and adds the resulting string t o new_array.
Finally, it returns the modified array new_array.
Note that I assumed array_length is a built-in function in gr etl to get the length of an
array. If this is not the case, you may need to define it yourse lf or use a different
method to check for an empty array.
9

--- PAGE 10 ---
Listing 7shows how we asked the LLM to generate a function that calcula tes the root
mean squared forecast error, based on a simple docstring. Th e resulting function, as shown
in Listing 8, has the correct signature and is well-structured with only one syntactical error.
Speciﬁcally, the gretl function sumsq() is not available, and we need to replace it with e.ˆ2
to correctly calculate the sum of squared errors. Although t he LLM output requires some
minor modiﬁcations, it provides almost working and correct code. Additionally, the LLM
provides a detailed description of each line of code which is makes it easy to understand.
Listing 7: Prompt to write a gretl function for computing the root mean squared forecast
error.
Write a gretl function using the following docstring. The do cstring follows in triple
backtics.
‘‘‘
Compute the root mean squared forecast error.
:param y: Series of realizations
:type y: series
:param f: Series of forecasts
:type f: series
:rtype: scalar
‘‘‘
Listing 8: Code written for computing the root mean squared f orecast error. Syntactical
error coloured in red.
‘‘‘
Here’s a gretl function that computes the root mean squared f orecast error:
‘‘‘
function scalar rmse_forecast_error(series y, series f)
matrix e = y - f
scalar n = nelem(e)
scalar mse = 1/n * sumsq(e)
returnsqrt(mse)
end function
‘‘‘
The function takes in two series: ‘y‘ and ‘f‘, which represen t the realizations and
forecasts, respectively. It then computes the error for eac h forecast by subtracting
the forecast from the corresponding realization, and store s these errors in a matrix
‘e‘.
Next, it computes the mean squared error (MSE) by dividing th e sum of squared errors
(‘sumsq(e)‘) by the sample size (‘n‘). Finally, it returns t he square root of the MSE,
which is the root mean squared forecast error.
Note that we used the ‘nelem()‘ function to calculate the sam ple size. This function
returns the number of elements in a matrix or a series. We also used the ‘sumsq()‘
function to calculate the sum of squared errors. This functi on takes a matrix or a
series as an argument and returns the sum of squared values.
In another example, we requested the LLM to write a function t hat calculates the
Fibonacci sequence, which is presented in Listing 9.
10

--- PAGE 11 ---
Listing 9: Prompt to write a gretl function for computing the Fibonacci sequence.
Write gretl code given the following docstring in triple bac ktics.
‘‘‘
Fibonacci sequence is a sequence in which each number is the s um of the two preceding
ones. The sequence starts from 0 and 1.
:param steps: Length of the sequence
:type steps: integer of the length of the sequence
:return: Fibonacci number
:rtype: integer
:raises: Error if the parameter steps is not positive.
‘‘‘
Listing 10: Code written for computing the Fibonacci sequen ce. Syntactical error coloured
in red.
‘‘‘
function integer fibonacci( integer steps)
if (steps <= 0)
errprintf ("Error: steps parameter must be a positive integer.\n")
return
endif
if (steps == 1)
return 0
endif
if (steps == 2)
return 1
endif
series fib = zeros(steps)
fib[1] = 0
fib[2] = 1
loop i = 3..steps
fib[i] = fib[i-1] + fib[i-2]
endloop
return fib[steps]
end function
‘‘‘
This function takes an integer parameter ‘steps‘ and return s the ‘steps‘th number in the
Fibonacci sequence. If the ‘steps‘ parameter is not positiv e, the function raises an
error and returns ‘null‘. The function first checks if ‘step s‘ is less than or equal
to zero. If so, the function raises an error and returns ‘null ‘. Next, the function
checks if ‘steps‘ equals 1 or 2 and returns the first or second number of the
Fibonacci sequence respectively. For ‘steps > 2‘, the funct ion calculates each of the
Fibonacci numbers up to ‘steps‘ using a loop that starts at 3 a nd continues until the
‘steps‘th number is reached. It then returns the ‘steps‘th n umber of the sequence.
The function name and variable names used in the code are clea r and descriptive, and
the structure of the code looks satisfactory at ﬁrst glance. However, there are some minor
errors in the code that need to be addressed. First, the type o f value returned should be
scalar instead of integer when using gretl. Additionally, the command errprintf() does
not exist and should be replaced with the funcerr command. Moreover, the object holding
the Fibonacci series should be initialized as a vector using the command matrix fib =
11

--- PAGE 12 ---
zeros(steps) , instead of being initialized as a type series. Once these mi nor corrections
have been made, the proposed code can be executed in gretl to r eturn accurate results.
Our docstring-to-code exercises demonstrate that LLMs can provide signiﬁcant support
for generating gretl code. Speciﬁcally, they can serve at th e very least as a starting point
by providing the general structure of a program. However, as expected, LLMs may not be
fully aware of gretl functions, datatypes, and syntax, so th eir generated code may not be
fully functional or may require further editing. Nonethele ss, the ability of LLMs to generate
accurate and helpful code to some extent highlights their po tential as a valuable tool for de-
velopers and researchers in the ﬁeld of programming. Also, i t substantially lowers the entry
barrier as the level of basic programming skills required de crease (low-level programming).
3.3 Code explanation and summarization
Research into using LLMs for summarizing code is currently a ctive (Ahmed and Devanbu,
2022a). Programmers may encounter functions that are diﬃcult to u nderstand due to poor
documentation, convoluted programming, or specialized ma thematical concepts that require
domain knowledge. Recent studies by Tian et al. ( 2023) have evaluated ChatGPT and found
that it can produce useful explanations of code to a high degr ee. The authors note that
even for incorrect code, ChatGPT can reason the intended pur pose of the code, although
its explanation may not accurately describe the current cod e.
To study the complexity of the kernel_loop() function (Lucchetti and Valentini, 2023),
we chose to use it as an example. We ensured that our prompt-se ssion had no previous
knowledge of this function by operating within a new session . Our goal was to evaluate
whether the LLM could accurately explain the intention of th e code. Listing 11shows the
prompt requesting an explanation of the function.
Listing 11: Prompt to explain the function kernel_loop()
What does the following gretl function do? Please explain. T he function is provided in
triple backtics:
‘‘‘
function matrix kernel_loop (const matrix y,
const matrix X,
const matrix Z,
scalar h,
scalar type[0:4:1],
matrix optparm[null])
scalar q = cols(y)
scalar k = cols(X)
scalar T = rows(y)
matrix ret = mshape(NA,T,q*k)
W = kernelmat(T, type, h, optparm)
ini = T + 1
fin = 2*T
loop t = 1 .. T
matrix PX = W[ini:fin] .* Z
matrix num = PX’y
matrix den = PX’X
ret[t,] = vec(den\num)’
ini--
fin--
endloop
return ret
end function
‘‘‘
12

--- PAGE 13 ---
The detailed answer provided by the LLM for the kernel_loop() function is presented
in Listing 12. The LLM correctly identiﬁes the function signature and pro vides a detailed
description of the initialized variables. The most notewor thy aspect of the LLM’s output is
its correct identiﬁcation of the matrix Was a weight matrix, indicating that the kernelmat()
function returns a vector of weights. The LLM also correctly identiﬁes the element-wise
multiplication of "the subset of the Wmatrix", demonstrating its ability to recognize the gretl
syntax.*for element-wise multiplication. Moreover, the LLM’s over all statement that "this
function computes some sort of weighted regression using th e ’y’, ’X’, and ’Z’ matrices, where
the weights are determined by the ’W’ matrix calculated usin g the ’kernelmat’ function" is
accurate and complete.
In another prompt session (not shown here), the LLM provided even more precise in-
formation, stating that "the function appears to implement a kernel-based estimation pro-
cedure. Speciﬁcally, it computes a regression of ’y’ on ’X’ u sing instrumental variables
generated from ’Z’ based on a kernel weighting scheme. The re sulting estimates are then
used to compute regression coeﬃcients and store them in the ’ ret’ matrix." This ability of
the LLM to recognize the function’s use of instrumental vari ables is particularly signiﬁcant
from an econometric standpoint.
Listing 12: Answer explaining the function kernel_loop()
The function is called ‘kernel_loop‘ and appears to be a func tion defined in the gretl
programming language. Here is a brief explanation of what th e function does:
The ‘kernel_loop‘ function takes several arguments, inclu ding ‘y‘, ‘X‘, ‘Z‘, ‘h‘, ‘type‘,
and ‘optparm‘. These arguments appear to be matrices and sca lars that will be used in
the computation.
The function first creates a new matrix ‘ret‘ using the ‘msha pe‘ function with parameters
‘NA‘, ‘T‘, and ‘q*k‘. This will create a matrix with ‘T‘ rows a nd ‘q*k‘ columns, where
each element is initialized to ‘NA‘.
The function then creates a new matrix ‘W‘ using the ‘kernelm at‘ function with parameters
‘T‘, ‘type‘, ‘h‘, and ‘optparm‘. This will create a matrix th at will be used as a
weight matrix.
Next, the function initializes the variables ‘ini‘ and ‘fin ‘ to values ‘T+1‘ and ‘2*T‘,
respectively. This is used to keep track of the elements in th e ‘W‘ matrix that will
be used in the computation.
The function then enters a loop that iterates through each ro w of the ‘y‘ matrix. For each
row, the function first constructs a new matrix ‘PX‘ by eleme nt-wise multiplying the
subset of the ‘W‘ matrix specified by the ‘ini‘ and ‘fin‘ vari ables with the ‘Z‘
matrix. The function then performs some matrix operations i nvolving ‘PX‘, ‘y‘, and
‘X‘, and stores the result in the corresponding row of the ‘re t‘ matrix.
After each iteration, the ‘ini‘ and ‘fin‘ variables are decr emented, to select a different
subset of the ‘W‘ matrix for the next iteration.
Finally, the function returns the ‘ret‘ matrix.
Overall, it appears that this function computes some sort of weighted regression using the
‘y‘, ‘X‘, and ‘Z‘ matrices, where the weights are determined by the ‘W‘ matrix
calculated using the ‘kernelmat‘ function. The exact natur e of the computation will
depend on the values of the input variables and the implement ation details of the
various matrix operations used in the function.
Next, we provide a second example that uses a function from th e SVAR package (v1.97)
13

--- PAGE 14 ---
developed by Lucchetti and Schreiber ( 2022). In Listing 13, we present a prompt where
we ask the LLM to describe the function max_eval() . Unfortunately, the function’s name
is ambiguous and lacks speciﬁcity, making it diﬃcult to infe r its purpose. The use of the
term "eval" could signify diﬀerent things, including "eval uation," "eigenvalue," or other
possibilities. Moreover, the function lacks a docstring an d any explanatory comments,
further compounding the ambiguity. However, we note that th e function utilizes several
built-in functions from gretl, such as eigen() , to compute the eigenvalues of a square
matrix.
Listing 13: Prompt to explain the function max_eval()
What does the following gretl function do? Explain. The func tion is in triple backticks.
‘‘‘
function scalar max_eval (const matrix A)
n = rows(A)
p = cols(A) / n
matrix compan = p==1 ? A : A | (I(n*(p-1)) ~ zeros(n*(p-1), n))
matrix lambda = eigen(compan)
# (lambda will be complex now)
scalar maxmod = maxc(abs(lambda))
return maxmod
end function
‘‘‘
To clarify the function’s purpose, we turn to the LLM, which p rovides additional infor-
mation about the function in Listing 14.
Listing 14: Answer explaining the function max_eval()
The provided gretl function ‘max_eval‘ computes the maximu m modulus of the eigenvalues of
a square matrix ‘A‘. Here are the steps that the function foll ows:
The function takes a matrix ‘A‘ as an input and extracts the nu mber of rows ‘n‘ and number
of blocks ‘p‘ from ‘A‘. It then constructs a companion matrix ‘compan‘ based on ‘A‘.
If ‘p==1‘, ‘compan‘ is just equal to ‘A‘, otherwise, ‘compan ‘ concatenates ‘A‘ with an
identity matrix of size ‘n(p-1)‘ on the lower block-diagona l.
The function then computes the eigenvalues of ‘compan‘ matr ix using the ‘eigen‘ function
available in gretl.
Finally, it computes the maximum modulus (absolute value) o f the complex eigenvalues
obtained from the ‘eigen‘ function and returns it as the outp ut.
In summary, the function can be used to determine the maximum absolute value of the
eigenvalues of a square matrix in gretl. This information ca n be useful in a variety
of applications, such as system simulation or control theor y.
According to the LLM’s description, the function max_eval () computes the maximum
modulus of a square matrix’s eigenvalues. However, we note t hat the function can also
process rectangular matrices, despite the LLM’s initial cl aim. Speciﬁcally, max_eval () con-
structs a companion matrix, named compan , that embeds the input matrix Ain a larger
square matrix. The LLM’s explanation of the function is info rmative and emphasizes its
potential usefulness in system simulation.
Overall, these advanced examples clearly demonstrate the e ﬀectiveness of using an LLM
to explain econometric functions.
14

--- PAGE 15 ---
3.4 Suggestion of variable and function names
Proper naming of functions and variables is critical for cod e readability and maintainability.
Descriptive and self-explanatory names help to reduce the c ognitive load of understanding
code and eliminate the need for extensive comments. However , ﬁnding good names can be
challenging for programmers who would rather focus on code i mplementation than name
selection. In this exercise, we explore whether an LLM can he lp generate suitable names
for functions and variables based on the code body.
To illustrate the process, we use the kernel_loop() function as an example (Lucchetti
and Valentini, 2023). We start by opening a new session to clear the LLM’s memory a nd
rename the function to foo() to avoid giving too much away about its purpose. The prompt
is presented in Listing 15, and the output is displayed in Listing 16. The LLM provides
suggestions for function names along with a brief explanati on of their meanings.
Listing 15: Prompt to suggest function names for function foo()
Suggest three descriptive function names for the following gretl function. The gretl
function is provided in triple backtics:
‘‘‘
function matrix foo (const matrix y,
const matrix X,
const matrix Z,
scalar h,
scalar type[0:4:1],
matrix optparm[null])
scalar q = cols(y)
scalar k = cols(X)
scalar T = rows(y)
matrix ret = mshape(NA,T,q*k)
W = kernelmat(T, type, h, optparm)
ini = T + 1
fin = 2*T
loop t = 1 .. T
matrix PX = W[ini:fin] .* Z
matrix num = PX’y
matrix den = PX’X
ret[t,] = vec(den\num)’
ini--
fin--
endloop
return ret
end function
‘‘‘
The ﬁrst suggestion, kernel_regression , is descriptive, accurate, and reasonably con-
cise. However, the second suggestion, local_linear_regression , may be ambiguous as
to what "local" entails. Regarding the third suggestion, th e naming of "h" is not precise
enough to be informative. Additionally, the suggested name "estimation_regression" is too
general and fails to specify the regression type.
Overall, this exercise shows that an LLM could provide valua ble insights into suitable
function and variable names, thus enhancing code readabili ty and maintainability for pro-
grammers.
Listing 16: List of suggested function names for function foo()
Here are three possible descriptive function names for the p rovided gretl function:
1. "kernel_regression": This name reflects the fact that th e function is performing a
15

--- PAGE 16 ---
kernel regression, which is a type of non-parametric regres sion that estimates the
relationship between variables using a weighted average of neighboring data points.
2. "local_linear_regression": This name also captures the essence of the function, which
is to estimate a regression relationship using local (as opp osed to global) linear
models.
3. "h_estimation_regression": This name emphasizes the fa ct that the function is using a
bandwith parameter h to determine the range of nearby data po ints that are used in the
regression estimation.
Another example highlights how an LLM can assist in improvin g variable names. We
again employ a function from the "SVAR" gretl package (Lucch etti and Schreiber, 2022)
calledCheckNormalizeRd() . We keep the original comment block at the top of the code
and instruct the LLM to limit variable names to a maximum of 10 characters (see Listing
17).
Listing 17: Prompt to suggest variable names for function CheckNormalizeRd()
Please suggest improved variable names for the following gr etl function in triple
backticks. Restrict the length of variable names to 10 chara cters.
‘‘‘
function scalar CheckNormalizeRd (matrix *R,
matrix *d)
/*
Checks that
(1) the constraints are consistent
(2) the constraints are non-contradictory (??? non-redund ant ? - Sven)
if (1) fails, an error message is printed and R and d are replac ed by
empty matrices; if (2) fails, redundant rows in R and d are dro pped.
*/
p = rows(R)
r = rank(R)
ret = 0
if r < p
matrix Rd = R ~ d
if r < rank(Rd)
R = {}
d = {}
ret = 1
else
matrix RR
matrix QQ = qrdecomp(Rd’, &RR)
matrix e = abs(diag(RR)) .> $macheps
QQ = selifc(QQ, e’)
RR = selifr(selifc(RR, e’), e)
Rd = QQ * RR
R = Rd[1:rows(Rd)-1,]’
d = Rd[rows(Rd),]’
ret = 2
endif
endif
return ret
end function
‘‘‘
Additionally, we found that the LLM was able to improve the ov erall readability and
16

--- PAGE 17 ---
ease of comprehension of the function by making the variable names more intuitive and
meaningful. The resulting variable names were succinct and clear, providing a signiﬁcant
improvement over the previous, less informative names. Imp ortantly, despite the changes
to the variable names, the function itself remained fully fu nctional and accurate. The ab-
breviations "cons", "Mat," and "Val" correspond to "constr ained," "Matrix," and "Value,"
respectively. This example ultimately demonstrated the ut ility of the LLM in assisting
coders with generating clear and concise variable names tha t facilitate the understanding
of code.
Listing 18: Refactored variable names of function CheckNormalizeRd()
Based on the length restriction of 10 characters, here are so me suggestions for improved
variable names in the ‘CheckNormalizeRd‘ function:
- ‘consMat‘ instead of ‘R‘
- ‘consVal‘ instead of ‘d‘
- ‘numCons‘ instead of ‘p‘
- ‘rankCons‘ instead of ‘r‘
- ‘cleanMat‘ instead of ‘Rd‘
- ‘decompMat‘ instead of ‘RR‘
- ‘selector‘ instead of ‘e‘
- ‘cleanedMat‘ instead of ‘QQ‘
With these name changes, the function becomes:
‘‘‘
function scalar CheckNormalizeRd (matrix *consMat, matri x *consVal)
/*
Checks that
(1) the constraints are consistent
(2) the constraints are non-contradictory (??? non-redund ant ? - Sven)
if (1) fails, an error message is printed and consMat and cons Val
are replaced by empty matrices; if (2) fails, redundant rows
in consMat and consVal are dropped.
*/
numCons = rows(consMat)
rankCons = rank(consMat)
ret = 0
if rankCons < numCons
matrix cleanMat = consMat ~ consVal
if rankCons < rank(cleanMat)
consMat = {}
consVal = {}
ret = 1
else
matrix decompMat
matrix selector = abs(diag(qrdecomp(cleanMat’, &decompM at))) .> $macheps
matrix cleanedMat = selifc(qrdecomp(cleanMat’, &decompM at), selector’)
decompMat = selifr(selifc(decompMat, selector’), select or)
cleanMat = cleanedMat * decompMat
consMat = cleanMat[1:rows(cleanMat)-1,]’
consVal = cleanMat[rows(cleanMat),]’
ret = 2
endif
endif
return ret
end function
17

--- PAGE 18 ---
‘‘‘
3.5 Improving code
Refactoring is a software development technique used to enh ance the quality of existing
code without impacting its functionality. This process ent ails modifying the code’s internal
structure to make it more eﬃcient, maintainable, and readab le while removing duplicate
code and ensuring compliance with coding standards. Refact oring has numerous beneﬁts
for software development projects, including decreased de velopment time and cost, enhanced
reliability and performance, and improved codebase mainta inability. Several studies have
underscored the importance of refactoring in software deve lopment projects, including (Mens
and Tourwe, 2004).
To illustrate the concept, we utilized the original CheckNormalizeRd() function from
the package written by Lucchetti and Schreiber ( 2022), and instructed the LLM to refactor
it according to the "Clean Code" principle (see Listing 19).13
Listing 19: Prompt to improve function CheckNormalizeRd()
Suggest improvements for the following gretl function in tr iple backticks. Make use of the
"Clean Code" approach.
‘‘‘
function scalar CheckNormalizeRd (matrix *R,
matrix *d)
/*
Checks that
(1) the constraints are consistent
(2) the constraints are non-contradictory (??? non-redund ant ? - Sven)
if (1) fails, an error message is printed and R and d are replac ed by
empty matrices; if (2) fails, redundant rows in R and d are dro pped.
*/
p = rows(R)
r = rank(R)
ret = 0
if r < p
matrix Rd = R ~ d
if r < rank(Rd)
R = {}
d = {}
ret = 1
else
matrix RR
matrix QQ = qrdecomp(Rd’, &RR)
matrix e = abs(diag(RR)) .> $macheps
QQ = selifc(QQ, e’)
RR = selifr(selifc(RR, e’), e)
Rd = QQ * RR
R = Rd[1:rows(Rd)-1,]’
d = Rd[rows(Rd),]’
ret = 2
endif
endif
13As a note, this approach promotes the development of simple, readable, eﬃcient, and maintainable code,
coined by software expert Robert C. Martin, a.k.a. "Uncle Bo b."
18

--- PAGE 19 ---
return ret
end function
‘‘‘
Listing 20shows the proposed refactoring of the code body. Apart from s ome formatting
issues, the LLM attempts to simplify the nested if-else bloc k. While in the original code,
theret=0 is returned if the rank(r) ≥rows(R) this was not present in the refactored code
resulting in a serious bug. We do not further evaluate the out put here. However, this shows
that one has to be cautious when using LLMs for programming.
Listing 20: Refactored function CheckNormalizeRd()
Here are some suggested improvements for the ‘CheckNormali zeRd‘ function in triple
backticks, using the "Clean Code" approach:
‘‘‘
function scalar check_normalize_rd(matrix *R, matrix *d)
/*
Checks that the constraints are:
(1) consistent and
(2) non-contradictory.
If (1) fails, an error message is printed and R and d are
replaced by empty matrices; if (2) fails, redundant rows
in R and d are dropped.
*/
num_constraints = rows(R)
rank_R = rank(R)
if rank_R < num_constraints # check for inconsistency
print "The constraints are inconsistent."
R = {}
d = {}
return 1
elif rank_R == num_constraints # check for redundancy
return 0
else
matrix Rd = R ~ d
matrix RR
matrix QQ = qrdecomp(Rd’, &RR)
matrix e = abs(diag(RR)) .> $macheps
QQ = selifc(QQ, e’)
RR = selifr(selifc(RR, e’), e)
Rd = QQ * RR
R = Rd[1:rows(Rd)-1,]’
d = Rd[rows(Rd),]’
return 2
endif
end function
‘‘‘
1. Function names should be in lowercase, so ‘CheckNormaliz eRd‘ should be changed to
‘check_normalize_rd‘.
2. Add a brief description of what the function does in the com ments.
3. Use descriptive variable names instead of single charact er variables. For example,
19

--- PAGE 20 ---
‘p‘ and ‘r‘ should be changed to ‘num_constraints‘ and ‘rank _R‘, respectively.
4. Simplify the conditionals for consistency and redundanc y checks.
5. Use early returns instead of nested conditionals to reduc e indentation.
6. Refactor the code to avoid duplicating codes within the ‘i f‘ and ‘else‘ cases.
7. Add spaces between the operators and operands for better r eadability.
Applying these changes can make the function easier to read, understand, and modify.
We present another example in Listing 21. The function max_eval() , sourced from the
SVAR package (v1.97), is examined. This function utilizes l inear algebra, speciﬁcally calling
gretl built-in functions and a ternary operator abbreviati on for an if-else block. Despite the
absence of descriptive comments or docstrings, the functio n is complex and necessitates an
understanding of linear algebra.
Listing 21: Prompt to improve function max_eval()
Suggest improvements for the following gretl function in tr iple backticks. Make use of the
"Clean Code" approach.
‘‘‘
function scalar max_eval (const matrix A)
n = rows(A)
p = cols(A) / n
matrix compan = p==1 ? A : A | (I(n*(p-1)) ~ zeros(n*(p-1), n))
matrix lambda = eigen(compan)
# (lambda will be complex now)
scalar maxmod = maxc(abs(lambda))
return maxmod
end function
‘‘‘
The output produced is intriguing and displayed in Listing 22. The LLM comprehends
the function’s purpose, calculating the eigenvalue modulu s of a matrix, and identiﬁes two
possible scenarios: that the input is either a square or bloc k matrix. It enhances the func-
tion’s readability by supplementing a docstring, adjustin g variable names, and implementing
an if-else block while avoiding ternary operators. Additio nally, the code is simpliﬁed so that
each line performs a single operation, in accordance with th e "Clean Code" philosophy.
Listing 22: Answer of improved function max_eval()
Here are a few suggestions for improving the ‘max_eval‘ func tion using the "Clean Code"
approach:
‘‘‘
function scalar max_eval(const matrix A)
/*
Computes the maximum modulus of all eigenvalues of the matri x A.
If A has multiple columns, it is assumed that it is arranged in a
stacked block matrix format with n x n submatrices, and the fu nction
computes the maximum eigenvalue modulus of the matrix A as if it were
block diagonal. If A has only one column, it is treated as a squ are
matrix of size n x n.
*/
num_rows = rows(A)
num_cols = cols(A)
20

--- PAGE 21 ---
is_block_matrix = num_cols > num_rows
if is_block_matrix
num_blocks = num_cols / num_rows
identity_matrix = I(num_rows * (num_blocks - 1))
zeros_matrix = zeros(num_rows * (num_blocks - 1), num_rows )
stacked_matrix = A | (identity_matrix ~ zeros_matrix)
eigenvalues = eigen(stacked_matrix)
else
eigenvalues = eigen(A)
endif
max_eigenvalue_modulus = maxc(abs(eigenvalues))
return max_eigenvalue_modulus
end function
‘‘‘
1. Add a brief description of what the function does in the com ments.
2. Use descriptive variable names instead of single charact er variables. For example, ‘n‘,
‘p‘, and ‘compan‘ should be changed to ‘num_rows‘, ‘num_blo cks‘, and ‘stacked_matrix‘,
respectively.
3. Add whitespace around operators to improve readability.
4. Add conditional logic to handle the case where ‘A‘ has mult iple columns. In this case,
it is assumed that the columns represent stacked block matri ces, where each block is a
square matrix of size n x n.
5. Instead of using a ternary operator to define ‘compan‘, us e an ‘if‘ and an ‘else‘
clause to make the code more readable.
6. Refactor the code to reduce indentation by using an early r eturn.
7. Add more complete documentation to describe how the funct ion works, what type of input
it takes, and what output it produces.
8. Use consistent and meaningful variable names, such as ‘is _block_matrix‘ and
‘max_eigenvalue_modulus‘.
By applying these changes, the resulting function becomes e asier to understand and modify.
Overall, these examples demonstrate that LLMs may be used to improve code quality
through refactoring. However, the output must be scrutiniz ed.
3.6 Write unit tests
Unit testing is standard in the modern programming paradigm for detecting bugs in units
of a program. However, writing high-quality unit tests can b e time-consuming. Yuan et al.
(2023) study the performance of ChatGPT to generate unit tests. Th ey ﬁnd that the LLM
suﬀers from correctness issues including diverse compilat ion errors and execution failures.
Still, the authors interpret the results as promising for au tomatically writing unit tests.
To evaluate the LLM’s ability to write unit tests for gretl, w e employed the refactored
max_eval() function from the previous subsection. In Listing 23, we prompted the LLM
to write a unit test for this function. The unit test should co ntain an artiﬁcial dataset and
compare the actual output with an expected result for both th e square and block matrix
cases.
21

--- PAGE 22 ---
Listing 23: Prompt to write a unit test for function max_eval()
Please write a unit test using the gretl syntax for the follow ing gretl function in triple
backticks.
‘‘‘
function scalar max_eval(const matrix A)
/*
Computes the maximum modulus of all eigenvalues of the matri x A.
If A has multiple columns, it is assumed that it is arranged in a
stacked block matrix format with n x n submatrices, and the fu nction
computes the maximum eigenvalue modulus of the matrix A as if it were
block diagonal. If A has only one column, it is treated as a squ are
matrix of size n x n.
*/
num_rows = rows(A)
num_cols = cols(A)
is_block_matrix = num_cols > num_rows
if is_block_matrix
num_blocks = num_cols / num_rows
identity_matrix = I(num_rows * (num_blocks - 1))
zeros_matrix = zeros(num_rows * (num_blocks - 1), num_rows )
stacked_matrix = A | (identity_matrix ~ zeros_matrix)
eigenvalues = eigen(stacked_matrix)
else
eigenvalues = eigen(A)
endif
max_eigenvalue_modulus = maxc(abs(eigenvalues))
return max_eigenvalue_modulus
end function
‘‘‘
Although the output in Listing 24appeared reasonable at ﬁrst glance, closer inspection
revealed several issues. For the square matrix case, the LLM incorrectly created the ma-
trix using matrix A = 1, 2; 3, 4 instead of gretl syntax.14Additionally, its expected
maximum eigenvalue of 5.0 was incorrect, as the correct valu e was 5.372. Furthermore, the
LLM was not familiar with gretl’s built-in assert() function for invoking assertions. Thus,
the unit test would fail even though the function returns the correct value.
For the stacked block matrix case, the LLM provided a poor exa mple of a unit test. It
again failed to properly create a matrix using gretl syntax, and the proposed matrix Bwas
square rather than block, thus failing to test the intended s cenario. The expected maximum
eigenvalue of 13 was also incorrect, as the correct value was 13.15.
Listing 24: Proposed unit test for function max_eval() . Syntactical error coloured in red.
Here’s an example unit test for the ‘max_eval‘ function in Gr etl syntax:
‘‘‘
# Test for a square matrix
matrix A = { {1, 2}, {3, 4} }
scalar expected_result = 5 # max eigenvalue modulus of A is 5
scalar test_result = max_eval(A)
printf "Test 1 - Square Matrix: "
if test_result == expected_result
print "Passed"
else
14In gretl, creating a matrix is done by matrix A = { 1, 2; 3, 4 } .
22

--- PAGE 23 ---
print "Failed"
endif
# Test for a stacked block matrix
matrix B = { {1, 2, 0, 0}, {3, 4, 0, 0}, {0, 0, 5, 6}, {0, 0, 7, 8} }
scalar expected_result = 13 # max eigenvalue modulus of B is 1 3
scalar test_result = max_eval(B)
printf "Test 2 - Stacked Block Matrix: "
if test_result == expected_result
print "Passed"
else
print "Failed"
endif
‘‘‘
In this example, we run two tests. The first test uses a 2 x 2 squ are matrix and
verifies that the ‘max_eval‘ function returns the expected result of 5. The second
test uses a stacked 2 x 2 block matrix and verifies that the ‘ma x_eval‘ function
returns the expected result of 13. We use ‘printf‘ to print in formative messages to
the console, and we use ‘if‘ statements to check if the functi on’s output matches
the expected output.
Overall, while the LLM’s unit test appeared to be well-struc tured, it struggled to create
an artiﬁcial dataset using gretl syntax and produced incorr ect expected values. Additionally,
it failed to test the two distinct cases that the function com prised.
3.7 A tiny coursework example
The following prompt describes an exercise for an introduct ory econometrics course. The
student is asked to load a dataset, manipulate a variable, co mpute summary statistics, and
run an ordinary least square (OLS) regression (refer to List ing25).
Listing 25: Prompt to write a gretl script
I want a gretl script doing the following tasks:
1. Load the "denmark" dataset.
2. Create the first differences of the series "LRM" and "LRY" .
3. Compute summary statistics of the series created at step 2 .
4. Run a OLS regression. The endogenous variable is the first difference of "LRM". Regress
on a constant and the first difference of "LRY".
5. Store the fitted values as "yhat".
6. Plot the fitted values over time
The output in Listing 26is noteworthy, with only two minor errors. The script correc tly
loads the time-series dataset shipped by gretl and creates t he desired variable using the
built-indiff() function. The LLM is aware that ﬁrst diﬀerencing results in t he loss of the
initial observation and attempts to set the sample start to t he second observation, which
is correct. However, the syntax for the smplcommand is incorrect; it should be smpl 2 ;
which restricts the sample to start with the second observat ion. The summary command is
used correctly for computing descriptive statistics, foll owed by the olscommand to run the
OLS regression and the call to the internal $yhat accessor to store ﬁtted values. Lastly,
the LLM attempts to call the gnuplot command, but the syntax is incorrect; it should be
gnuplot yhat --time-series --with-lines --output=displa y.
Listing 26: Proposed gretl script. Syntactical error colou red in red.
open denmark.gdt
23

--- PAGE 24 ---
# create first differences of "LRM" and "LRY"
series dLRM = diff(LRM)
series dLRY = diff(LRY)
# compute summary statistics of the series created at step 2
smpl 2:--quiet # this sets the sample from the second observation to the end
summary dLRM
summary dLRY
# run OLS regression and store fitted values as "yhat"
ols dLRM const dLRY --quiet
series yhat = $yhat
# plot fitted values over time
gnuplot yhat vs time
Next, we engage in iterative prompt development and send the prompt to the LLM to
remove the smplcommand.15The returned output includes the same script but without
thesmplcall (not shown).
Good job. However, get rid of the "smpl" command which is not n eeded.
The returned output consists of the same script just without the call of the smplcom-
mand (not shown here).
We then ask the LLM to correct the gnuplot command’s syntax to gnuplot yhat
--with-lines --time-series --output=display . However, despite the correction, the
output for this command remains incorrect (shown below).
Also, the syntax calling the "gnuplot" command is not correc t. Please fix that. Add the
options to:
1. Plot the time-series as a line.
2. Output the plot on the screen.
3. And add the time-series option.
.
.
.
# plot fitted values over time
gnuplot yhat with lines time
4 Conclusion
In conclusion, this study has examined the usage and eﬀectiv eness of current LLM models
for programming with a low-resource and domain-speciﬁc pro gramming language. As an
application, we choose gretl and its scripting language han sl. We have employed the LLM
provided by www.you.com which is based on GPT-3.5 when writing this article. The ﬁndi ngs
show that publicly available LLMs already can be a useful too l for understanding, writing,
and improving gretl code, despite the fact that only little g retl code is publicly available.
15In gretl all the following commands automatically deal with missing values such that we do not need to
restrict the sample explicitly.
24

--- PAGE 25 ---
This indicates that current LLMs are able to generalize well to LRPL.
Speciﬁcally, the LLM produced useful and descriptive docst rings for gretl functions,
translates docstrings back to gretl code and vice versa , helped to improve the readability
and maintainability of code by suggesting better function a nd variable names, and provided
precise and technical explanations of abstract and poorly d ocumented econometric code.
Also, we showed how the LLM helps refactoring gretl code main ly involving linear algebra.
However, the LLM was not always successful in improving code and also failed to write a
correct unit test. Lastly, we presented a simple exercise fo r an introductory econometrics
course. The written script by the LLM is useful as a starting p oint for students as the
syntactical errors are of minor type. We have shown, that the LLM is expected to correct
some of the syntactical errors by means of iterative prompt d evelopment.
Future research could build on these ﬁndings by exploring wa ys to ﬁne-tune LLM models
for gretl code. It also would be interesting to evaluate whet her a modern LLM helps to detect
and correct errors in gretl code. Lastly, LLMs may be used to t ranslate code from another
language into gretl which is a topic under active research (R oziere et al., 2020).
Overall, this study provides valuable insights into the pot ential uses and limitations
of LLMs in programming with the low-resource and domain-spe ciﬁc econometric language
gretl.
25

--- PAGE 26 ---
References
Ahmed, Touﬁque and Premkumar Devanbu (2022a). Few-shot training LLMs for project-
speciﬁc code-summarization . arXiv:2207.04237[cs.SE] .url:https://arxiv.org/abs/2207.04237 .
— (May 2022b). “Multilingual training for software enginee ring”. In: Proceedings of the 44th
International Conference on Software Engineering . ACM. doi:10.1145/3510003.3510049 .
url:https://doi.org/10.1145%2F3510003.3510049 .
Bubeck, Sébastien et al. (2023). “Sparks of Artiﬁcial Genera l Intelligence: Early experiments
with GPT-4”. url:https://www.microsoft.com/en-us/research/publicatio n/sparks-of-artificial-general-intelligence-early-e xperiments-with-gpt-4/ .
Chen, Fuxiang et al. (2022). On the Transferability of Pre-trained Language Models for L ow-
Resource Programming Languages . arXiv:2204.09653[cs.PL] .url:https://arxiv.org/abs/2204.09653 .
Chen, Mark et al. (2021). Evaluating Large Language Models Trained on Code . arXiv:
2107.03374 [cs.LG] .
Cottrell, Allin (2017). “Hansl: A DSL for Econometrics”. In :Proceedings of the 2Nd Inter-
national Workshop on Real World Domain Speciﬁc Languages . RWDSL17. Austin, TX,
USA: ACM, 1:1–1:10. isbn: 978-1-4503-4845-4. doi:10.1145/3039895.3039896 .url:
http://doi.acm.org/10.1145/3039895.3039896 .
Gong, Zi et al. (Dec. 19, 2022). “MultiCoder: Multi-Program ming-Lingual Pre-Training for
Low-Resource Code Completion”. In: arXiv: 2212.09666v1 [cs.CL] .
Husain, Hamel et al. (2020). CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search . arXiv:1909.09436 [cs.LG] .
Korinek, Anton (Feb. 2023). Language Models and Cognitive Automation for Economic Re-
search . Working Paper 30957. National Bureau of Economic Research. doi:10.3386/w30957 .
url:http://www.nber.org/papers/w30957 .
Lucchetti, Jack and Sven Schreiber (July 30, 2022). The SVAR addon for gretl . Version 1.97.
url:https://sourceforge.net/projects/gretl/files/addons /doc/SVAR.pdf .
Lucchetti, Jack and Francesco Valentini (Jan. 19, 2023). Time-Varying OLS/IV estimators
in gretl: the ketvals package . Version 1.0. url:https://gretl.sourceforge.net/current_fnfiles/unzip ped/ketvals.pdf .
Manna, Zohar and Richard J. Waldinger (1971). “Toward Autom atic Program Synthesis”.
In:Commun. ACM 14.3, pp. 151–165. issn: 0001-0782. doi:10.1145/362566.362568 .
url:https://doi.org/10.1145/362566.362568 .
Mastel, Pierrette Mahoro et al. (2023). “NATURAL LANGUAGE U NDERSTANDING
FOR AFRICAN LANGUAGES”. In: 4th Workshop on African Natural Language Pro-
cessing .url:https://openreview.net/forum?id=gWuvdFMqHM .
Mens, T. and T. Tourwe (2004). “A survey of software refactor ing”. In: IEEE Transactions
on Software Engineering 30.2, pp. 126–139. doi:10.1109/TSE.2004.1265817 .
Radford, Alec et al. (2019). “Language models are unsupervi sed multitask learners”. In:
OpenAI Blog 1.8, p. 9.
Roziere, Baptiste et al. (2020). “Unsupervised translation of programming languages”. In:
Advances in Neural Information Processing Systems 33.
Sun, Weisong et al. (2023). Automatic Code Summarization via ChatGPT: How Far Are
We?arXiv:2305.12865 [cs.SE] .url:https://arxiv.org/abs/2305.12865 .
Tarassow, Artur (2019). “Practical Empirical Research Usi ng gretl and hansl”. In: Aus-
tralian Economic Review 52.2, pp. 255–271. doi:10.1111/1467-8462.12324 .url:
https://ideas.repec.org/a/bla/ausecr/v52y2019i2p255 -271.html .
Tian, Haoye et al. (2023). Is ChatGPT the Ultimate Programming Assistant – How far is
it?arXiv:2304.11938 [cs.SE] .url:https://arxiv.org/pdf/2304.11938 .
Vaswani, Ashish et al. (2017). “Attention is all you need”. I n:Advances in Neural Information
Processing Systems , pp. 5998–6008.
Wang, Ben and Aran Komatsuzaki (May 2021). GPT-J-6B: A 6 Billion Parameter Autore-
gressive Language Model .https://github.com/kingoflolz/mesh-transformer-jax .
26

--- PAGE 27 ---
Wolfram, Stephen (Feb. 14, 2023). What Is ChatGPT Doing ... and Why Does It Work?
url:https://writings.stephenwolfram.com/2023/02/what-is -chatgpt-doing-and-why-does-it-work
(visited on 06/20/2023).
Xu, Frank F. et al. (2022). “A Systematic Evaluation of Large Language Models of Code”. In:
Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Program-
ming. MAPS 2022. San Diego, CA, USA: Association for Computing Ma chinery, pp. 1–
10.isbn: 9781450392730. doi:10.1145/3520312.3534862 .url:https://doi.org/10.1145/3520312.3534862 .
Yuan, Zhiqiang et al. (2023). No More Manual Tests? Evaluating and Improving ChatGPT
for Unit Test Generation . arXiv:2305.04207[cs.SE] .url:https://arxiv.org/pdf/2305.04207.pdf .
Zügner, Daniel et al. (2021). “Language-Agnostic Represen tation Learning of Source Code
from Structure and Context”. In: International Conference on Learning Representations .
eprint:https://openreview.net/forum?id=Xh5eMZVONGF .url:https://openreview.net/forum?id=Xh5eMZVONGF .
27
