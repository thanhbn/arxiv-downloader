# 2212.10481.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2212.10481.pdf
# Kích thước tệp: 4476479 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Đánh giá Dựa trên Thực thi cho Sinh mã Open-Domain
Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig|
Language Technologies Institute, Carnegie Mellon University
|Inspired Cognition
{zhiruow,shuyanzh,dfried,gneubig}@cs.cmu.edu
Tóm tắt
Để mở rộng phạm vi của các truy vấn mã hóa đến
các thiết lập thực tế hơn, chúng tôi đề xuất ODEX,
bộ dữ liệu sinh mã Python từ ngôn ngữ tự nhiên (NL)
Open-Domain EXecution-based đầu tiên. ODEX có 945 cặp NL-Mã
trải rộng 79 thư viện đa dạng, cùng với 1,707
test case do con người viết để thực thi. Các
cặp NL-Mã của chúng tôi được thu thập từ các diễn đàn StackOverflow để khuyến khích các truy vấn mã hóa tự nhiên và thực tế. Hơn nữa, ODEX hỗ trợ
bốn ngôn ngữ tự nhiên làm ý định, bằng tiếng Anh,
Tây Ban Nha, Nhật Bản và Nga. ODEX tiết lộ những khác biệt hành vi thú vị giữa các mô hình ngôn ngữ mã (LM) hiệu suất cao nhất.
Trong khi CODEX đạt kết quả tổng thể tốt hơn,
CODEGEN cải thiện hiệu quả thông qua mở rộng quy mô –
CODEGEN 6.1B hoạt động tương đương với
CODEX 12B. Cả hai mô hình đều cho thấy khoảng cách đáng kể giữa miền mở và đóng, nhưng
khoảng cách CODEGEN có xu hướng giảm theo kích thước mô hình trong khi khoảng cách CODEX tăng. Chúng tôi phát hành
ODEX để tạo điều kiện nghiên cứu về các vấn đề miền mở cho cộng đồng sinh mã.¹

1 Giới thiệu
Các đánh giá của hệ thống sinh mã NL-to-code,
đặc biệt cho các ngôn ngữ lập trình đa năng
như Python, đã đặt sự nhấn mạnh ngày càng tăng lên các phương pháp thực thi mã để xác minh
kết quả. Cách tiếp cận chủ đạo để tạo
các bộ test như vậy là viết thủ công các test case
cho các giải pháp mã chuẩn (Chen et al., 2021;
Austin et al., 2021; Lai et al., 2022; Huang et al.,
2022). Tính đúng đắn của các dự đoán mô hình
sau đó được đánh giá bằng cách xem liệu mã được sinh có vượt qua
các test case hay không (Chen et al., 2021). So với
các metric không thực thi như khớp văn bản với
các giải pháp tham chiếu, các phương pháp dựa trên thực thi đánh giá chặt chẽ hơn tính đúng đắn chức năng của mã
(Hendrycks et al., 2021; Chen et al., 2021).
¹https://github.com/zorazrw/odex

Tuy nhiên, hầu hết các tài nguyên có hỗ trợ thực thi
chỉ áp dụng cho mã miền đóng, chỉ
sử dụng các hàm tích hợp của Python (Chen et al., 2021;
Hendrycks et al., 2021; Austin et al., 2021; Li et al.,
2022; Haluptzok et al., 2022) hoặc các thư viện cụ thể trong
miền khoa học dữ liệu (Lai et al., 2022; Huang et al.,
2022). Sự tập trung này vào các vấn đề miền đóng khác biệt đáng kể so với việc sử dụng chương trình miền mở tự nhiên bao gồm một phạm vi đa dạng các thư viện
và chức năng (Yin et al., 2018; Agashe et al.,
2019; Wang et al., 2022). Để cho phép đánh giá
dựa trên thực thi cho các truy vấn mã hóa sử dụng thư viện,
chúng tôi trình bày ODEX, một bộ dữ liệu
Open-Domain EXecution-based (§2). Chúng tôi xây dựng ODEX bằng cách tạo
1,707 test case cho 945 cặp NL-Mã từ
bộ dữ liệu CoNaLa (Yin et al., 2018) và MCoNaLa (Wang
et al., 2022), cả hai đều bắt nguồn từ StackOverflow² với các truy vấn mã hóa thực tế rộng rãi.

Chúng tôi phân tích và nổi bật ba khía cạnh của ODEX
(§3). Đầu tiên, ODEX có phạm vi miền rộng lớn với
79 thư viện, với 53:4% các vấn đề sử dụng ít nhất một thư viện. Thứ hai, ODEX chứa
các truy vấn bằng bốn ngôn ngữ khác nhau, với 439, 90,
164, và 252 mẫu bằng tiếng Anh, Tây Ban Nha, Nhật Bản,
và Nga, như được hiển thị trong Hình 1. Thứ ba, ODEX
giải quyết ba thách thức độc đáo trong thực thi mã miền mở: các lần chạy không thể tái tạo (Hình 1 a),
đầu ra ngẫu nhiên (Hình 1 b), và kiểm tra tương đương chuyên biệt (Hình 2).

Chúng tôi đánh giá hai họ LLM mã tiên tiến nhất,
CODEX và CODEGEN, trên ODEX (§5). Nghiên cứu của chúng tôi cho thấy kích thước mô hình lớn hơn và dữ liệu huấn luyện tăng cường cải thiện độ chính xác thực thi. Trong khi đó, chúng tôi quan sát khả năng đa ngôn ngữ thỏa đáng, mặc dù không có mô hình nào được thiết kế đặc biệt cho việc sử dụng đa ngôn ngữ. Tuy nhiên, chúng tôi thấy rằng các mô hình đối mặt với những thách thức lớn hơn nhưng đa dạng với
các truy vấn miền mở so với các truy vấn miền đóng (§5). Cụ thể, CODEX đạt được
²https://stackoverflow.com

arXiv:2212.10481v2 [cs.SE] 19 May 2023

--- TRANG 2 ---
Mô hình LM mã
< / >
a
b
c
da
b
c
d

Hình 1: Các ví dụ trong bộ dữ liệu ODEX. Các đầu vào ở bên trái được định dạng theo hàm với (1) các biểu thức import thư viện; (2) chữ ký hàm khai báo tên hàm và các đối số đầu vào; và (3) ý định ngôn ngữ tự nhiên như một phần của docstring (bản dịch tiếng Anh không được bao gồm trong các đầu vào không phải tiếng Anh thực tế trong quá trình suy luận).
Các hộp màu xám chỉ ra những nơi cho các giải pháp mã. Như được hiển thị ở bên phải, một LM mã điền vào các hộp màu xám với
các giải pháp mã, sau đó được thực thi trên các unit test bên dưới. Đáng chú ý, việc viết unit test cho các truy vấn miền mở thường khó khăn hơn: a yêu cầu thực thi mô phỏng do khó khăn trong việc tái tạo; b được
xác minh thông qua tương đương gần đúng. Các công trình trước đây tập trung nhiều hơn vào các assertion cơ bản, như trong c và d.

kết quả tổng thể cao hơn, trong khi CODEGEN trình bày hiệu quả tham số tốt hơn và hiệu suất miền mở-đóng cân bằng hơn khi kích thước mô hình tăng lên. Bằng cách so sánh metric dựa trên thực thi với một loạt
metric không thực thi (§6), chúng tôi tiếp tục xác nhận
lợi thế của thực thi trong việc cho phép các giải pháp thay thế, nhưng cũng cho thấy tiềm năng của các metric từ vựng để xác định các sửa lỗi đơn giản.

ODEX cùng nhau tạo điều kiện cho việc sinh mã miền mở thực tế
và đánh giá dựa trên thực thi.
Nó phục vụ như một bộ dữ liệu benchmark toàn diện cho
các hệ thống NL-to-code, hỗ trợ các bối cảnh NL đa dạng, sử dụng thư viện, và các phương pháp đánh giá. Bằng cách giải quyết các thách thức độc đáo của tạo test
và thực thi, chúng tôi hy vọng đặt nền móng cho
việc đánh giá mã miền mở thông qua thực thi.

2 Bộ dữ liệu ODEX

Trong phần này, chúng tôi mô tả quy trình bốn bước
xây dựng bộ dữ liệu ODEX. Đầu tiên chúng tôi thu thập
tài nguyên của các truy vấn mã hóa miền mở tự nhiên
(§2.1). Tiếp theo, chúng tôi thiết lập tiêu chuẩn annotation và thủ tục tạo test case (§2.2). Sau đó chúng tôi mô tả quá trình thuê và làm việc của người annotate (§2.3). Cuối cùng, chúng tôi tiến hành kiểm tra để đảm bảo
chất lượng dữ liệu (§2.4).

2.1 Thu thập Tài nguyên

Chúng tôi lấy hai bộ dữ liệu NL-to-code, CoNaLa (Yin
et al., 2018) và MCoNaLa (Wang et al., 2022), làm
nguồn cho ODEX. Chúng tôi gọi chúng cùng nhau là
(M)CoNaLa. Các cặp NL-Mã của chúng được thu thập
từ StackOverflow, chứa các truy vấn mã hóa phong phú mà (1) tự nhiên phản ánh việc sử dụng chương trình thực tế, và (2) bao gồm các miền đa dạng được đo bằng các thư viện được sử dụng. Những đặc tính này phù hợp
tốt với trọng tâm chính của chúng tôi về các truy vấn miền mở.
(M)CoNaLa tiếp tục kiểm tra và làm rõ ý định NL của nó bằng cách sử dụng các người chú thích con người để đảm bảo chất lượng dữ liệu.

2.2 Tiêu chuẩn và Thủ tục Annotation

Với mỗi cặp NL-Mã nguồn, nhiệm vụ annotation chính
của chúng tôi là viết các test case để kiểm tra tính đúng đắn thực thi mã, như được minh họa bằng bốn bước
trong Hình 2. Một test case đủ điều kiện nên xác minh
chức năng chính của giải pháp mã chuẩn.
Trong trường hợp mà người annotate không hiểu
ngôn ngữ của ý định, chúng tôi sử dụng các công cụ dịch như
Google Translate API.³

Bước 1: Gói các Đoạn mã vào Hàm
Các giải pháp mã trong (M)CoNaLa thường là những đoạn mã ngắn

³https://translate.google.com

--- TRANG 3 ---
Tính tổng trên tất cả các hàng của mảng numpy 2D `a`
NL
Mã
Bước 1
gói mã
Bước 2
import thư viện
Bước 3
viết test case
Bước 4
thực thi

Hình 2: Một ví dụ annotation bao gồm bốn bước.

pet (ví dụ: x = np.zeros(5)) để đảm bảo khớp chính xác hơn với ý định NL, nhưng để có thể thực thi
chúng thường cần bối cảnh bổ sung như gán biến. Do đó chúng tôi gói mã vào các hàm
độc lập bằng cách chỉ định các đối số đầu vào và đầu ra làm bối cảnh. Ví dụ, Bước 1 trong Hình 2 xác định biến a làm đối số đầu vào.

Bước 2: Chỉ định Điều kiện Tiên quyết Thư viện Do
phạm vi miền mở của (M)CoNaLa, một số
đoạn mã yêu cầu import thư viện bổ sung để thực thi đúng. Theo đó, bước thứ hai của chúng tôi là
chỉ định các thư viện điều kiện tiên quyết cho các giải pháp mã.

Bước 3: Annotation Test Case Tiếp theo, chúng tôi viết
các test case chứa ba phần: (1) đầu vào: truyền
giá trị cho các đối số đầu vào, (2) đầu ra: nêu các đầu ra thực thi mong đợi, và (3) assertion: kiểm tra xem kết quả thực thi có khớp với đầu ra mong đợi không.

Tuy nhiên, việc tạo test case cho mã miền mở
đối mặt với ba thách thức. Đầu tiên, việc thực thi an toàn và có thể tái tạo có thể khó đạt được. Như
trong Hình 1 a, việc gửi yêu cầu HTTP khi đánh giá mẫu này là không thực tế. Thay vào đó, chúng tôi
sử dụng mock để mô phỏng đầu ra (mã trạng thái phản hồi thành công 200). Thứ hai, một số mã bao gồm tính ngẫu nhiên (ví dụ: random.randint(3,5)) và không có
giá trị xác định. Thay vào đó chúng tôi thực hiện các assertion giới hạn, ví dụ: kiểm tra tất cả các phần tử là số nguyên trong phạm vi [3,5]. Thứ ba, các kiểm tra tương đương tiêu chuẩn bằng == có thể không hợp lệ,
vì các đối tượng cụ thể của thư viện thường yêu cầu kiểm tra bằng nhau chuyên biệt. Ví dụ, kiểm tra tương đương của hai mảng NumPy a và b sử dụng
np.array_equal(a,b), trong khi a == b sẽ gây ra
lỗi thực thi.

Bước 4: Tự Xác minh Trong bước cuối cùng, chúng tôi thực hiện tự xác minh để đảm bảo hiệu quả chất lượng annotation. Chúng tôi thực thi giải pháp mã chuẩn
trên mỗi test case mới được tạo. Trừ khi test case cho phép giải pháp vượt qua thành công,
nó không nên được coi là annotation hợp lệ.

2.3 Thuê Người annotate và Hoàn thành Nhiệm vụ

Vì dữ liệu của chúng tôi liên quan đến các chức năng đa dạng từ
nhiều thư viện, nhiệm vụ annotation của chúng tôi có tiêu chuẩn tương đối cao cho người annotate. Một người annotate
đủ điều kiện nên thành thạo Python và các thư viện phổ biến, và viết các test case có thể hoạt động.

Chúng tôi chọn thuê các sinh viên đại học có
nền tảng khoa học máy tính mạnh về Python. Trong số 20 ứng viên ứng tuyển, đầu tiên chúng tôi
tiến hành sàng lọc hồ sơ để lọc các ứng viên
có kinh nghiệm lập trình đủ. Tiếp theo, chúng tôi
cho mỗi ứng viên một bài kiểm tra annotation với năm
cặp NL-Mã được chọn ngẫu nhiên. Vì bài kiểm tra
phản ánh quá trình annotation chính thức, chúng tôi cung cấp
hướng dẫn rõ ràng về từng bước (như trong §2.2) và
script mã để tự xác minh. Các ứng viên được
yêu cầu hoàn thành bài kiểm tra trong ba ngày lịch.
Dựa trên hiệu suất bài kiểm tra của họ, chúng tôi thuê bốn ứng viên để chính thức tham gia công việc này.

2.4 Kiểm tra Chất lượng

Chúng tôi nỗ lực rất nhiều để đảm bảo chất lượng dữ liệu
trong suốt quá trình annotation. Để hỗ trợ người annotate viết các test case có thể hoạt động hiệu quả và chính xác hơn, chúng tôi yêu cầu họ thực thi
mỗi test case được viết bằng mã xác minh
mà chúng tôi cung cấp, và báo cáo rõ ràng xem giải pháp mã chuẩn có thể vượt qua thành công tất cả
các test case được annotation mà họ tạo ra không.

Sau annotation, các tác giả thực hiện xác minh sau để kiểm tra xem mỗi test case có đọc hợp lý và thực thi đúng không. Trong các vòng kiểm tra chất lượng tự động cuối cùng của chúng tôi, chúng tôi xác nhận rằng tỷ lệ vượt qua cho tất cả các giải pháp mã chuẩn trên các test case được annotation của chúng là 100%.

Chúng tôi thu thập tổng cộng 945 mẫu với NL bằng
bốn ngôn ngữ, bao gồm 439 mẫu bằng tiếng Anh,
90 bằng tiếng Tây Ban Nha, 164 bằng tiếng Nhật, và 252 bằng tiếng Nga.

3 Phân tích Bộ dữ liệu

Chúng tôi phân tích ODEX từ ba khía cạnh: đa dạng miền (§3.1), độ phức tạp mẫu (§3.2), và hỗ trợ thực thi (§3.3).

3.1 Đa dạng

Một đặc tính độc đáo của ODEX là phạm vi miền rộng lớn. Chúng tôi phân loại các mã bao gồm việc sử dụng thư viện

--- TRANG 4 ---
(cả tích hợp và của bên thứ ba) là thuộc miền
mở và những mã không có gì là thuộc miền đóng. Các thư viện khác nhau thường phục vụ các chức năng cụ thể và có khả năng độc đáo. Ví dụ, thư viện
datetime được thiết kế để xử lý các thao tác ngày/giờ, trong khi các thư viện khác tập trung vào nhiều
lĩnh vực khác như phân tích dữ liệu hoặc yêu cầu web.
Do đó, trong công việc này, chúng tôi xem sự đa dạng trong
thư viện như một đại diện của các miền riêng biệt.

[THIS IS TABLE: Shows number of open- and closed-domain examples and libraries by language]
Ngôn ngữ | # Thư viện Duy nhất | Kích thước
         |                   | Mở | Đóng | Tổng
en       | 45               | 230| 209  | 439
es       | 20               | 48 | 42   | 90
ja       | 44               | 113| 51   | 164
ru       | 35               | 114| 138  | 252
Tổng     | 79               | 505| 440  | 945

Bảng 1: Số lượng ví dụ miền mở và đóng, và số thư viện liên quan trong mỗi ngôn ngữ.

Bảng 1 báo cáo thống kê miền và Hình 3
hiển thị phân phối thư viện. ODEX bao gồm một
tập hợp đa dạng 79 thư viện, khác nhau theo ngôn ngữ.
Hầu hết các mẫu, 53:4%, sử dụng ít nhất một thư viện.

[THIS IS FIGURE: Bar chart showing ODEX library distribution with libraries like pandas, numpy, re, os, etc.]

So sánh với Các Bộ dữ liệu Hiện có Chúng tôi so sánh
ODEX với tám bộ dữ liệu sinh mã khác
hỗ trợ thực thi test case: HumanEval (Chen
et al., 2021), MBPP (Austin et al., 2021), APPS
(Hendrycks et al., 2021), MTPB (Nijkamp et al.,
2022), P3 (Haluptzok et al., 2022), DSP (Chandel
et al., 2022), DS-1000 (Lai et al., 2022), và Exe-
DS (Huang et al., 2022).

[THIS IS FIGURE: Multiple bar charts showing library distribution of eight other datasets]

Từ phân phối của chúng trong Hình 4, sáu trong số
tám bộ dữ liệu tập trung vào miền đóng và hầu hết
các ví dụ sử dụng không thư viện nào. Những ví dụ như vậy khác biệt
so với các chương trình thực tế, thường sử dụng API của
các thư viện khác nhau. DS-1000 và Exe-DS có
một số vấn đề miền mở, nhưng việc sử dụng thư viện của chúng
đồng nhất hơn với trọng tâm đặc biệt vào
miền khoa học dữ liệu. Hơn nữa, DS-1000 hạn chế chỉ với mã sử dụng thư viện nhưng chỉ có bảy
thư viện. Ngược lại, ODEX "nhiều màu sắc" hơn; nó
bao gồm nhiều thư viện miền mở hơn đáng kể, cũng như
các truy vấn thường xuyên trong miền đóng.

So sánh với Phân phối Tự nhiên Để cung cấp tham chiếu về phân phối miền tự nhiên, chúng tôi
ước lượng việc sử dụng thực tế bằng cách đếm các tệp Python GitHub
sử dụng mỗi thư viện. Như được hiển thị trong Hình 5, ODEX trình bày sự phù hợp tốt hơn với
tình huống thực tế liên quan đến các miền mở –
nó có nhiều miền đa dạng hơn và bảo tồn
mô hình đuôi dài trong các tình huống thực tế.

Danh sách đầy đủ các thư viện và tần suất của chúng
về ODEX, tám bộ dữ liệu so sánh, và
thiết lập tự nhiên ước lượng có trong §A.1.

[THIS IS FIGURE: Two similar bar charts showing approximated natural distribution based on GitHub Python files]

3.2 Độ phức tạp

Để đo độ phức tạp bộ dữ liệu, đầu tiên chúng tôi tính toán
độ dài của ý định NL và đoạn mã. Chúng tôi
tokenize ý định NL với tokenizer spaCy⁴
trong các ngôn ngữ tương ứng; chúng tôi theo Yin và Neubig
(2018) để tokenize mã. Đối với mã, chúng tôi cũng phân tích cây AST bằng thư viện ast⁵ chuẩn của Python,
và đếm số biến đầu vào và đầu ra
để định lượng độ phức tạp của bối cảnh thực thi.

[THIS IS TABLE: Shows complexity metrics by language including NL length, code length, AST depth, and input/output variables]

Trong Bảng 2, chúng tôi thấy rằng mã trong tập tiếng Tây Ban Nha
dài trung bình hơn các ngôn ngữ khác. Đối với cả hai

⁴https://spacy.io/
⁵https://docs.python.org/3/library/ast.html

--- TRANG 5 ---
[THIS IS TABLE: Comparison table of ODEX with other NL-to-code generation datasets, showing dataset names, samples, domain, executable status, avg test cases, data source, and NL languages]

bên đầu vào và đầu ra, mã trong tập tiếng Anh
có ít biến hơn, gợi ý môi trường thực thi có thể đơn giản hơn, có thể bắt nguồn từ
sự đơn giản tương đối của các truy vấn SO được hỏi bằng tiếng Anh.

3.3 Hỗ trợ Thực thi

Chúng tôi so sánh một cách có hệ thống các bộ dữ liệu sinh mã
liên quan đến thực thi hoặc mã miền mở trong Bảng 3. ODEX là bộ dữ liệu đầu tiên hỗ trợ đánh giá dựa trên thực thi cho mã miền mở. Mặc dù ODEX không có số lượng test case lớn nhất, chúng tôi thảo luận trong §7 về cách các test case này vẫn có thể đo đáng tin cậy tính đúng đắn của mã.

4 Thiết lập Thí nghiệm

Các LLM mã đã đạt kết quả mạnh trên nhiều
nhiệm vụ sinh mã, tuy nhiên khả năng miền mở của chúng
được nghiên cứu ít do các thiết lập miền hạn chế của các bộ dữ liệu trước đây. Để kiểm tra khả năng mô hình trong miền mở, chúng tôi đánh giá hai
họ mô hình hiệu suất cao nhất, CODEX và CODEGEN, trên ODEX. Chúng tôi thực hiện đánh giá sử dụng thiết lập prompting, không fine-tune mô hình nào.

Chúng tôi giới thiệu các mô hình baseline, thiết lập
prompt, và đưa ra các metric để đánh giá.

Họ CODEX Tại thời điểm công việc này,
CODEX có ba mô hình có sẵn công khai. CODE-
CUSHMAN-001 (C1) là mô hình CODEX 12B trong
Chen et al. (2021). CODE-DAVINCI-001/002 (D1,
D2) là hai mô hình GPT-3 175B.⁶

Họ CODEGEN Các mô hình CODEGEN (Nijkamp
et al., 2022) là các mô hình tự hồi quy được huấn luyện trên sự kết hợp của corpora NL và mã,
khác nhau về kích thước mô hình (350M, 2.7B, 6.1B, 16.1B)

⁶https://beta.openai.com/docs/
model-index-for-researchers

và dữ liệu huấn luyện. Các mô hình được huấn luyện tuần tự trên
THEPILE (Gao et al., 2020), BIGQUERY,⁷ và
bộ dữ liệu BIGPYTHON được ký hiệu là NL, MULTI,
và MONO. Mô hình mạnh nhất CODEGEN-16.1B-
MONO, hoạt động tương tự như CODE-CUSHMAN-
001 trên các bộ dữ liệu HumanEval và MTPB.

Thiết kế Prompt Để so sánh công bằng, chúng tôi sử dụng
cùng một prompt cho cả hai họ mô hình. Mặc dù
prompting với các ví dụ trong bối cảnh few-shot có thể
cải thiện, các thí nghiệm của chúng tôi không phải lúc nào cũng thấy điều này
hữu ích cho cả hai mô hình. Do đó, chúng tôi báo cáo kết quả zero-shot làm baseline và để kết quả few-shot cho §7. Tạo prompt zero-shot chỉ yêu cầu
nội dung từ mẫu test. Theo Chen
et al. (2021), chúng tôi xây dựng prompt bằng cách nối
bối cảnh hàm và docstring. Docstring
bao gồm ý định NL và unit test tùy chọn (so
sánh trong §7). Hình 6 hiển thị một ví dụ prompt.

[THIS IS FIGURE: Shows a zero-shot prompt example with function context and docstring containing one test case]

Metric Đánh giá Chúng tôi theo Chen et al.
(2021) và đo độ chính xác thực thi sử dụng
metric pass@k, bằng cách tính toán phần của
các vấn đề có ít nhất một dự đoán đúng
trong k mẫu. Chúng tôi cũng so sánh nó với một loạt
metric không thực thi sau này trong §5.

Chi tiết Triển khai Chúng tôi theo Chen et al.
(2021) và sử dụng nucleus sampling (Holtzman et al.,

⁷https://cloud.google.com/bigquery

--- TRANG 6 ---
[THIS IS TABLE: Execution accuracy table showing CODEX and CODEGEN-MONO models performance across different languages (en, es, ja, ru) with pass@k metrics from 1-10]

2019) với top-p được đặt ở 0.95 và temperature đặt ở
0.8. Chúng tôi đặt đầu ra tối đa 512 token.

5 Kết quả Thí nghiệm

Đầu tiên chúng tôi trình bày hiệu suất tổng thể của hai
họ mô hình trên ODEX (§5.1). Tiếp theo, với những thách thức độc đáo của mã miền mở, chúng tôi nghiên cứu
sự khác biệt giữa các vấn đề miền mở và đóng (§5.2), và trong các miền riêng lẻ (§5.3).

5.1 Hiệu suất Baseline

Kết quả CODEX Như trong Bảng 4, phù hợp với các
công trình hiện có và trực giác của chúng tôi, các mô hình DAVINCI 175B lớn hơn vượt trội hơn mô hình CUSHMAN 12B nhỏ hơn, và phiên bản 002 cải thiện so với 001.
Xu hướng này đúng cho tất cả ngôn ngữ và tất cả kích thước
sampling. Hơi bất ngờ, tất cả mô hình đạt kết quả tốt trên các vấn đề không phải tiếng Anh, mặc dù
CODEX không được thiết kế cho việc sử dụng đa ngôn ngữ. Độ chính xác cao này trên các vấn đề không phải tiếng Anh gợi ý
tiềm năng đa ngôn ngữ của các mô hình CODEX.

Kết quả CODEGEN Chúng tôi báo cáo kết quả của các mô hình MONO
trong Bảng 4 với hiệu suất vượt trội của chúng
so với các biến thể NL và MULTI (Nijkamp et al., 2022).
Tỷ lệ vượt qua tăng khi CODEGEN tăng từ
350M lên 2.7B, và tiếp tục tăng trong các ngôn ngữ không phải tiếng Anh khi mở rộng thêm lên 6.1B.
CODEGEN thể hiện khả năng đa ngôn ngữ, vì kết quả của nó trên các tập con không phải tiếng Anh gần với kết quả trên
tiếng Anh, và tăng liên tục trong quá trình mở rộng.

Mặc dù CODEX và CODEGEN có hiệu suất tương đương trên các bộ dữ liệu hiện có như HumanEval, ODEX hiệu quả tiết lộ hiệu quả của
CODEGEN trên các truy vấn mã hóa miền mở ngay cả
với ít tham số hơn nhiều, tức là CODEGEN 6.1B
cho pass@1 tương tự với mô hình CODEX DAVINCI-
001 176B trên một số ngôn ngữ. Kết quả chi tiết hơn (pass@k ở 1...k...10) cho cả hai mô hình có trong §B.

5.2 Miền Mở so với Miền Đóng

Kết quả CODEX Hình 7 (trái) hiển thị pass@1
trên miền mở (OD) và miền đóng (CD).
Tất cả mô hình CODEX điểm thấp hơn nhiều trong OD so với
CD. Những khoảng cách lớn như vậy tồn tại trên tất cả ngôn ngữ,
dao động từ 4.34 trong tiếng Tây Ban Nha đến 38.57 trong tiếng Nhật.
Nâng cấp mô hình (C1→D1→D2) không phải lúc nào cũng
giảm khoảng cách. Khoảng cách giảm nhẹ trong tiếng Tây Ban Nha,
nhưng tăng liên tục trong tiếng Anh và tiếng Nhật.
Trong khi D2 hoạt động tốt nhất, nó cũng thể hiện
khoảng cách nghiêm trọng nhất. Những phát hiện này gợi ý rằng các thực hành phổ biến để cải thiện LLM có thể không giải quyết
sự phức tạp vốn có trong các vấn đề sinh mã miền mở. Do đó việc sử dụng các chiến lược tiên tiến hơn là bắt buộc.

Kết quả CODEGEN Như được hiển thị trong Hình 7 (phải),
CODEGEN cũng có khoảng cách đáng kể giữa miền mở
và đóng, tuy nhiên, nhỏ hơn khoảng cách CODEX
trên tất cả ngôn ngữ, trung bình 6.0%
điểm. Khi kích thước mô hình tăng từ 2.7B lên 6.1B,
khoảng cách giảm khoảng 6.3 điểm trong tiếng Anh và
1.7 điểm trong tiếng Tây Ban Nha. Điều này trái ngược với CODEX,
khi mở rộng lên DAVINCI-002, những khoảng cách này
tiếp tục tăng trung bình 4.9 điểm, chỉ ra rằng việc mở rộng CODEGEN bắt kịp hiệu quả hơn
về hiệu suất miền mở.

5.3 Phương sai Miền

Bây giờ chúng tôi đi sâu hơn vào kết quả trong các
miền riêng lẻ. Chúng tôi tập trung vào mô hình CODE-DAVINCI-002
vì nó có hiệu suất tốt nhất trên tất cả mô hình. Trong Hình 8, chúng tôi vẽ độ chính xác theo
tần suất miền, như được ước lượng trong §3.1.

Độ chính xác thực thi không thấp trên tất cả miền mở. Ví dụ, CODE-DAVINCI-002 đạt
50% pass@1 cho một số thư viện phổ biến như
random và math. Nhưng tần suất miền cao

--- TRANG 7 ---
[THIS IS FIGURE: Two graphs showing CODEX (left) and CODEGEN (right) pass@1 performance on open- and closed-domain problems across languages]

Hình 7: CODEX (trái) và CODEGEN (phải) pass@1 trên các vấn đề miền mở và đóng trong mỗi ngôn ngữ.

[THIS IS FIGURE: Scatter plot showing CODEX pass@1 for domains of varied frequencies, with different colored dots representing frequency rankings]

Hình 8: CODEX pass@1 cho các miền có tần suất khác nhau. Các miền được tô màu khác nhau dựa trên thứ hạng tần suất của chúng: 10 miền thường xuyên nhất màu đỏ, 10 miền ít thường xuyên nhất màu xanh lam, và các miền khác ở giữa màu vàng.

không đảm bảo thành thạo mô hình. Ví dụ,
trên các thư viện có chức năng phức tạp như
matplotlib và tensorflow, pass@1 có thể xuống dưới 10%. Xem §C để biết thêm kết quả theo miền.

6 So sánh với Metric Không Thực thi

Trong phần này, chúng tôi nghiên cứu sự phù hợp giữa
đánh giá dựa trên thực thi và năm metric không thực thi, xác định lợi thế cho cả hai loại.

Xếp hạng Mô hình Sử dụng Metric Khác nhau Chúng tôi
đánh giá mô hình sử dụng năm metric không thực thi sử dụng khớp từ vựng, cú pháp và ngữ nghĩa:
BLEU (Papineni et al., 2002), ROUGE (Lin,
2004), METEOR (Banerjee và Lavie, 2005),
ChrF (Popović, 2015), và CodeBLEU (Ren et al.,
2020). Tham khảo §D.1 để biết thêm mô tả.

[THIS IS FIGURE: Bar chart showing CODEX models evaluated on six different metrics]

Hình 9: Các mô hình CODEX được đánh giá trên sáu metric.

Chúng tôi phân tích sử dụng CODEX, với hiệu suất tốt hơn của nó. Như được hiển thị trong Hình 9, xếp hạng mô hình bằng metric không thực thi không tương quan chính xác với xếp hạng của chúng bằng độ chính xác thực thi.
Ngay cả khi xếp hạng phù hợp, sự khác biệt của chúng phần lớn không tỷ lệ. So sánh các metric,
ChrF và METEOR có phương sai giữa các mô hình nhỏ hơn, trong khi BLEU và ROUGE thay đổi nhiều hơn và tương quan tốt hơn với tỷ lệ vượt qua. Đáng chú ý, CodeBLEU thấp trong hầu hết các thiết lập và có thể không phù hợp để đánh giá mã theo kiểu đoạn.

Tương quan Metric Tiếp theo chúng tôi đánh giá xem
metric không thực thi có thể được sử dụng để phân biệt giữa các mẫu vượt qua và thất bại hay không. Chúng tôi lấy
BLEU làm ví dụ vì nó hiển thị mô hình xếp hạng tương tự với thực thi. Hình 10 hiển thị phương sai không đáng kể trong điểm BLEU của các nhóm vượt qua và thất bại. Bốn metric khác thể hiện mô hình tương tự, như có thể tìm thấy trong §D.3.

[THIS IS FIGURE: Four bar charts showing BLEU scores on passed and failed samples across different languages (EN, ES, JA, RU)]

Hình 10: Điểm BLEU trên các mẫu vượt qua và thất bại.

7 Điều gì Ảnh hưởng đến Hiệu suất Mô hình?

Ngoài sự khác biệt trong cấu hình mô hình, chúng tôi
nghiên cứu ba yếu tố có thể ảnh hưởng đến hiệu suất.

Số lượng Ví dụ Trong Bối cảnh Các mô hình
có thể được hưởng lợi từ các cặp NL-Mã ví dụ. Do đó chúng tôi khám phá thiết lập few-shot bằng cách thêm tiền tố
N ∈ {1; 2; 3} cặp đầu vào-đầu ra trong prompt.

--- TRANG 8 ---
Trong Hình 11 (trái), đối với CUSHMAN-001 và
DAVINCI-001, các ví dụ few-shot mang lại cải thiện rõ ràng so với thiết lập zero-shot; nhưng đối với
DAVINCI-002 mạnh nhất, nó mang lại lợi ích tối thiểu trong
tiếng Anh. Xem kết quả tương tự trong các ngôn ngữ khác trong
§E.1.

[THIS IS FIGURE: Two graphs showing performance metrics. Left graph shows CODEX pass@1 with 0/1/2/3-shot prompts. Right graph shows DAVINCI-002 pass@1 when adding zero, one, or all test cases in prompts.]

Hình 11: Trái: CODEX pass@1 (trên tập tiếng Anh) sử dụng
prompt 0/1/2/3-shot. Phải: DAVINCI-002 pass@1 khi thêm không, một, hoặc tất cả test case trong prompt.

Số lượng Test Case trong Docstring Bao gồm test case trong đầu vào thêm gợi ý thực thi về
chức năng mong đợi của giải pháp, và do đó
có thể cải thiện độ chính xác thực thi. Chúng tôi kiểm tra giả thuyết này bằng cách thử nghiệm với prompt có
số lượng test case khác nhau. Ngoài thiết lập mặc định với không test, chúng tôi so sánh việc thêm một
test case ngẫu nhiên và tất cả test case được annotation.

Hình 11 (phải) cho thấy rằng chèn ít nhất một
test case mẫu cải thiện đáng kể độ chính xác thực thi, nhưng thêm nhiều case hơn có ít lợi ích. Điều này có thể ngụ ý sự đủ của
một test case để hiển thị chức năng chính.

Số lượng Test Case Đánh giá Kết quả thực thi
có thể đáng tin cậy hơn nếu sử dụng nhiều test case hơn để đánh giá. Tuy nhiên, có sự đánh đổi
giữa hiệu quả đánh giá và hiệu suất annotation,
do chi phí cao của nỗ lực con người. Để
nghiên cứu sự đánh đổi này, chúng tôi quan sát cách kết quả thay đổi
theo số lượng test. So với việc sử dụng tất cả case theo mặc định, chúng tôi cũng thử sử dụng một
case được chọn ngẫu nhiên. Để đơn giản, chúng tôi không
bao gồm bất kỳ test case nào trong prompt.

Như được hiển thị trong Hình 12, đánh giá trên một test
ngẫu nhiên phần lớn bảo tồn độ chính xác của việc sử dụng tất cả
test, chỉ ra rằng một case đủ để kiểm tra
chức năng chính cho hầu hết các truy vấn. Kiểm tra §E để
phân tích về các yếu tố khác như đặt tên hàm.

[THIS IS FIGURE: Bar chart showing pass@1 when executing one or all test cases across different languages]

Hình 12: pass@1 khi thực thi một hoặc tất cả test
case.

8 Công việc Liên quan

Sinh mã Miền Mở Các chương trình thường sử dụng API từ các thư viện Python khác nhau. Một số
bộ dữ liệu bảo tồn phạm vi tự nhiên từ các
Jupyter Notebook tương tác (Agashe et al., 2019) hoặc các bài đăng StackOverflow (Yin et al., 2018; Wang et al., 2022),
nhưng đối mặt với thách thức trong việc cho phép thực thi (Lai et al.,
2022; Chandel et al., 2022). Bộ dữ liệu ODEX của chúng tôi
giải quyết thực thi cho mã miền mở.

Truy vấn Mã hóa so với Thách thức Lập trình
Một số công trình bắt nguồn từ các trang web cuộc thi mã hóa (Hendrycks et al., 2021; Li et al., 2022), nhưng
GitHub Jupyter Notebook (Agashe et al., 2019;
Huang et al., 2022) và StackOverflow (SO) (Yin
et al., 2018; Wang et al., 2022; Lai et al., 2022) cung cấp các truy vấn mã hóa tự nhiên và thực tế hơn. Chúng tôi
bảo tồn tính tự nhiên này và kết hợp các thiết lập NL khác nhau để hỗ trợ lập trình viên trên toàn thế giới.

Đánh giá Dựa trên Thực thi Đánh giá bằng thực thi đã được sử dụng lâu cho SQL (Zhong et al.,
2017) hoặc các dạng logic (Dong và Lapata, 2016).
Nhiều bộ dữ liệu đã bắt đầu hỗ trợ thực thi Python
thông qua test case, tuy nhiên tập trung vào các hàm tích hợp (Chen et al., 2021; Austin et al., 2021;
Hendrycks et al., 2021) hoặc các miền cụ thể (Lai
et al., 2022; Huang et al., 2022). Ngược lại, các test case của chúng tôi bao gồm các thư viện đa dạng trong miền mở.

9 Kết luận

Chúng tôi trình bày ODEX, một bộ dữ liệu sinh mã miền mở
hỗ trợ đánh giá dựa trên thực thi thông qua các test case do con người viết. ODEX không chỉ
hỗ trợ đánh giá dựa trên thực thi của mã sử dụng test
case, mà còn mở rộng nhiệm vụ đến miền mở,
bao gồm 79 thư viện Python đa dạng và bốn ngôn ngữ tự nhiên (tiếng Anh, Tây Ban Nha, Nhật Bản, và Nga). So sánh hai mô hình sinh mã tiên tiến nhất, CODEX và CODEGEN, bộ dữ liệu của chúng tôi
hiệu quả tiết lộ hành vi khác nhau của chúng giữa
các miền chương trình và bối cảnh ngôn ngữ. ODEX
phục vụ như một benchmark NL-to-code toàn diện
với phạm vi miền mở, các truy vấn ngôn ngữ tự nhiên đa dạng, và hỗ trợ đa metric. Khi

--- TRANG 9 ---
đưa thực thi mã đến các tình huống miền mở,
các khám phá của chúng tôi cũng tiết lộ những thách thức mới nổi trong
tạo test và thực thi đáng tin cậy, mà chúng tôi hy vọng
bộ dữ liệu của chúng tôi sẽ cho phép công việc tương lai giải quyết.

Lời cảm ơn

Chúng tôi muốn cảm ơn tất cả các người annotate vì
công việc chăm chỉ của họ. Chúng tôi cảm ơn Uri Alon, Frank F. Xu, Tao
Yu vì phản hồi hữu ích của họ về công việc này.

Hạn chế

ODEX nhằm phục vụ như một testbed toàn diện,
bằng cách cho phép đánh giá dựa trên thực thi của mã
trong miền mở, với các đầu vào ý định linh hoạt trong
bốn ngôn ngữ tự nhiên. Tuy nhiên, chúng ta nên
duy trì nhận thức liên tục về bảo mật thực thi,
hỗ trợ đa ngôn ngữ, và độ tin cậy đánh giá.

Đầu tiên, hỗ trợ thực thi trong ODEX cho phép
đánh giá chặt chẽ hơn các phương pháp không thực thi khác. Tuy nhiên, do độ phức tạp tăng của các mã miền mở, cần nhiều kiểm tra hơn cho
bảo mật thực thi, cho cả giải pháp mã
hoặc test case. Chúng ta nên luôn cảnh giác với việc che giấu mã độc hại (Wallace et al., 2021) hoặc
tạo mã có lỗ hổng bảo mật (Verdi
et al., 2020; Pearce et al., 2021).

Thứ hai, ngoài đầu vào tiếng Anh, ODEX
cũng khuyến khích ý định được chỉ định trong ba ngôn ngữ khác. Tuy nhiên, phạm vi ngôn ngữ của nó bị giới hạn bởi
các diễn đàn có sẵn trong StackOverflow. Chúng tôi hy vọng
sáng kiến của chúng tôi có thể nổi bật bản chất đa ngôn ngữ
của các nhà phát triển chương trình, khuyến khích sự xuất hiện
của các tài nguyên dữ liệu tương tự trong các ngôn ngữ khác, và
liên tục thúc đẩy hỗ trợ lập trình AI
trong các ngôn ngữ trên toàn thế giới.

Thứ ba, ODEX bao gồm các truy vấn mã
trong miền mở, nó phù hợp hơn cho các tình huống ít đòi hỏi tài nguyên như đánh giá downstream hoặc học few-shot. Mặc dù ODEX
lớn hơn nhiều bộ dữ liệu trước đây có test case do con người viết, nó vẫn bị hạn chế do
nỗ lực con người chuyên sâu được yêu cầu bởi quá trình curation.
Về điều này, chúng tôi khuyến khích độc giả tiến hành
kiểm tra ý nghĩa (Dror et al., 2018) và báo cáo
cải thiện mô hình đáng kể hơn.

Tuyên bố Đạo đức

Công việc của chúng tôi đã nhận được sự chấp thuận IRB và được cấp phép
theo Giấy phép Quốc tế Creative Commons Attribution-ShareAlike
(CC BY-SA) 4.0. Bộ dữ liệu ODEX
kết quả được xây dựng để phục vụ như một benchmark cho sinh mã miền mở, để tiếp tục tạo điều kiện cho các tiến bộ công nghệ trong hỗ trợ lập trình AI, đồng thời hỗ trợ nhiều ngôn ngữ
để khuyến khích khả năng tiếp cận phổ quát của nó.

Chúng tôi cố gắng đảm bảo chất lượng dữ liệu cao và
tối ưu hóa hiệu quả annotation. Chúng tôi xây dựng bộ dữ liệu ODEX
với các tài nguyên StackOverflow tự nhiên và thực tế và thuê các người annotate có
thành thạo lập trình đủ điều kiện. Chúng tôi cung cấp cho các người annotate của chúng tôi
hướng dẫn được ghi chép rõ ràng, giao diện annotation linh hoạt (Google Sheets, Jupyter Notebook),
và công cụ tự xác minh. Chúng tôi (tác giả) tiến hành
annotation thí điểm để xác nhận tính rõ ràng của tiêu chuẩn annotation và tính khả thi của nhiệm vụ annotation. Chúng tôi
tiến hành kiểm tra posthoc về kết quả annotation, cả thủ công và tự động, để có
chất lượng dữ liệu được đảm bảo (tỷ lệ vượt qua 100%).

Chúng tôi tôn trọng đóng góp và quyền riêng tư của các
người annotate của chúng tôi. Chúng tôi cung cấp mức thù lao cạnh tranh cho
công việc annotation của họ và đối xử công bằng với từng người trong số họ. Tất cả người annotate đều có quyền rút lui
bất cứ lúc nào. Chúng tôi đảm bảo rằng tất cả thông tin cá nhân của họ được xóa trước khi phát hành công khai.

Chúng tôi tiến hành phân tích có hệ thống từ nhiều
góc độ trong bài báo, trong nỗ lực thúc đẩy
nhận thức công chúng về việc tạo và đánh giá chương trình trong miền mở, vừa khuyến khích
nhiều tiến bộ hơn theo hướng này, vừa nâng cao nhiều
mối quan tâm hơn về tính mạnh mẽ và bảo mật của những
vấn đề mã hóa độc đáo như vậy.

--- TRANG 10 ---
Tài liệu Tham khảo

Rajas Agashe, Srinivasan Iyer, và Luke Zettlemoyer.
2019. Juice: Một bộ dữ liệu được giám sát từ xa quy mô lớn
cho sinh mã dựa trên bối cảnh miền mở. Trong Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang
5436–5446.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
2021. Tổng hợp chương trình với các mô hình ngôn ngữ lớn.
arXiv preprint arXiv:2108.07732.

Satanjeev Banerjee và Alon Lavie. 2005. METEOR:
Một metric tự động cho đánh giá MT với
tương quan cải thiện với phán đoán của con người. Trong Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, trang 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.

Shubham Chandel, Colin B Clement, Guillermo Serrato, và Neel Sundaresan. 2022. Huấn luyện và
đánh giá trợ lý khoa học dữ liệu jupyter notebook.
arXiv preprint arXiv:2201.12901.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,
Zeqi Lin, Jian-Guang Lou, và Weizhu Chen. 2022.
Codet: Sinh mã với các test được tạo. arXiv
preprint arXiv:2207.10397.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên mã. arXiv preprint
arXiv:2107.03374.

Naihao Deng, Shuaichen Chang, Peng Shi, Tao Yu, và
Rui Zhang. 2021. Prefix-to-sql: Sinh text-to-sql
từ các câu hỏi người dùng không hoàn chỉnh. arXiv preprint
arXiv:2109.13066.

Li Dong và Mirella Lapata. 2016. Ngôn ngữ thành
dạng logic với attention neural. Trong Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang
33–43.

Rotem Dror, Gili Baumer, Segev Shlomov, và Roi Reichart. 2018. Hướng dẫn của người đi nhờ xe để kiểm tra ý nghĩa thống kê trong xử lý ngôn ngữ tự nhiên. Trong
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), trang 1383–1392.

Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov,
và Timofey Bryksin. 2022. Ngoài bleu: chúng ta nên
đánh giá chất lượng của các mô hình sinh mã như thế nào?
arXiv preprint arXiv:2208.03133.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: Một bộ dữ liệu 800gb văn bản đa dạng cho
mô hình hóa ngôn ngữ. arXiv preprint arXiv:2101.00027.

Patrick Haluptzok, Matthew Bowers, và Adam Tauman Kalai. 2022. Các mô hình ngôn ngữ có thể tự dạy
mình lập trình tốt hơn. arXiv preprint
arXiv:2207.14502.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,
Samir Puranik, Horace He, Dawn Song, et al. 2021.
Đo năng lực thách thức mã hóa với apps.
arXiv preprint arXiv:2105.09938.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và
Yejin Choi. 2019. Trường hợp kỳ lạ của sự thoái hóa văn bản neural. arXiv preprint arXiv:1904.09751.

Junjie Huang, Chenglong Wang, Jipeng Zhang,
Cong Yan, Haotian Cui, Jeevana Priya Inala,
Colin Clement, Nan Duan, và Jianfeng Gao.
2022. Đánh giá dựa trên thực thi cho các mô hình sinh mã khoa học dữ liệu. arXiv preprint
arXiv:2211.09374.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và
Luke Zettlemoyer. 2018. Ánh xạ ngôn ngữ thành mã
trong bối cảnh lập trình. Trong Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, trang 1643–1652.

Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang,
Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau
Yih, Daniel Fried, Sida Wang, và Tao Yu. 2022.
Ds-1000: Một benchmark tự nhiên và đáng tin cậy cho
sinh mã khoa học dữ liệu. arXiv preprint
arXiv:2211.11501.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
et al. 2022. Sinh mã cấp độ cuộc thi với
alphacode. arXiv preprint arXiv:2203.07814.

Chin-Yew Lin. 2004. ROUGE: Một gói cho đánh giá tự động các tóm tắt. Trong Text Summarization Branches Out, trang 74–81, Barcelona, Spain.
Association for Computational Linguistics.

Chin-Yew Lin và Franz Josef Och. 2004. Đánh giá tự động chất lượng dịch máy sử dụng
thống kê chuỗi con chung dài nhất và skip-bigram.
Trong Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics
(ACL-04), trang 605–612.

Stephan Lukasczyk và Gordon Fraser. 2022. Pynguin:
Sinh unit test tự động cho python. arXiv
preprint arXiv:2202.05218.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu
Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
và Caiming Xiong. 2022. Một mô hình hội thoại

--- TRANG 11 ---
cho tổng hợp chương trình. arXiv preprint
arXiv:2203.13474.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-
Jing Zhu. 2002. Bleu: một phương pháp cho đánh giá tự động
dịch máy. Trong Proceedings of the
40th annual meeting of the Association for Computational Linguistics, trang 311–318.

Hammond Pearce, Baleegh Ahmad, Benjamin Tan,
Brendan Dolan-Gavitt, và Ramesh Karri. 2021.
Một đánh giá an ninh mạng thực nghiệm về
đóng góp mã của github copilot. arXiv preprint
arXiv:2108.09293.

Maja Popović. 2015. chrF: character n-gram F-score
cho đánh giá MT tự động. Trong Proceedings of the
Tenth Workshop on Statistical Machine Translation,
trang 392–395, Lisbon, Portugal. Association for
Computational Linguistics.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie
Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, và Shuai Ma. 2020. Codebleu: một
phương pháp cho đánh giá tự động tổng hợp mã.
arXiv preprint arXiv:2009.10297.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy,
Shao Kun Deng, và Neel Sundaresan. 2020. Sinh unit
test case với transformer và bối cảnh tiêu điểm.
arXiv preprint arXiv:2009.05617.

Morteza Verdi, Ashkan Sami, Jafar Akhondali, Foutse
Khomh, Gias Uddin, và Alireza Karami Motlagh.
2020. Một nghiên cứu thực nghiệm về các lỗ hổng c++
trong các ví dụ mã crowdsourced. IEEE Transactions
on Software Engineering.

Eric Wallace, Tony Zhao, Shi Feng, và Sameer Singh.
2021. Các cuộc tấn công đầu độc dữ liệu che giấu trên các mô hình nlp. Trong Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 139–150.

Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F
Xu, và Graham Neubig. 2022. Mconala: Một benchmark cho sinh mã từ nhiều ngôn ngữ tự nhiên. arXiv preprint arXiv:2203.08388.

Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan
Vasilescu, và Graham Neubig. 2018. Học để
khai thác các cặp mã và ngôn ngữ tự nhiên được căn chỉnh từ
stack overflow. Trong 2018 IEEE/ACM 15th international conference on mining software repositories
(MSR), trang 476–486. IEEE.

Pengcheng Yin và Graham Neubig. 2018. Tranx: Một
parser cú pháp trừu tượng neural dựa trên transition cho
phân tích ngữ nghĩa và sinh mã. Trong Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, trang 7–12.

Victor Zhong, Caiming Xiong, và Richard Socher.
2017. Seq2sql: Sinh truy vấn có cấu trúc
từ ngôn ngữ tự nhiên sử dụng học tăng cường.
arXiv preprint arXiv:1709.00103.

--- TRANG 12 ---
A Bộ dữ liệu ODEX

A.1 Thống kê Phân phối Thư viện

Ngoài các minh họa trong § 3.1, chúng tôi liệt kê
thống kê chi tiết của các thư viện trong ODEX, tám
bộ dữ liệu so sánh, và phân phối tự nhiên ước lượng.

Thống kê Miền ODEX Bảng 5 liệt kê số lượng và tỷ lệ phần trăm xuất hiện cho mỗi thư viện
trong bộ dữ liệu ODEX.

[THIS IS TABLE: ODEX library distribution showing library names, counts, and frequencies in two columns]

Thống kê Miền của Các Bộ dữ liệu So sánh Bảng 6 liệt kê tần suất thư viện của tám bộ dữ liệu so sánh được đề cập trong § 3: HumanEval, MBPP,
APPS, MTPB, P3, DSP, DS-1000, và Exe-DS.

[Multiple tables showing library statistics for different datasets including HumanEval, MBPP, APPS, MTPB, P3, DSP, DS-1000, and Exe-DS]

--- TRANG 13 ---
Phân phối Miền Tự nhiên Ước lượng
Để ước lượng phân phối tự nhiên của các thư viện
trong miền mở, chúng tôi đếm số lượng tệp Python
trên GitHub import thư viện quan tâm.
Theo cú pháp tìm kiếm GitHub,⁸ chúng tôi sử dụng
truy vấn import ${library_name} để tìm kiếm các tệp
import một thư viện nhất định, và sử dụng NOT import
để đếm các tệp không sử dụng thư viện nào. Tần suất của chúng
được hiển thị trong Bảng 7.

[THIS IS TABLE: Approximated Natural Distribution showing library counts for various Python libraries like os, sys, numpy, etc.]

A.2 Chi tiết Annotation Thêm

Cùng với cặp NL-Code, chúng tôi cũng cung cấp ID
của bài đăng StackOverflow nguồn, sử dụng ID này người annotate có thể truy ngược lại trang web bài đăng gốc

⁸https://docs.github.com/en/search-github/
searching-on-github/searching-code

và hiểu rõ hơn về câu hỏi. Nếu
bất kỳ lỗi hoặc thiếu sót nào được phát hiện trong
NL hoặc mã đã cho, chúng tôi yêu cầu người annotate sửa
nó bằng cách thực hiện thay đổi tối thiểu có thể.

Phù hợp với cách lập trình viên import một
thư viện, chúng tôi yêu cầu các biểu thức được viết
ở ba dạng: (1) import ${LIBRARY}, (2)
import ${LIBRARY} as ${ABBR}, hoặc (3)
from ${LIBRARY} import ${FUNCTION}, trong đó
${LIBRARY} cũng có thể là các lớp con như
matplotlib.pyplot.

Chúng tôi khuyến khích người annotate sử dụng ngôn ngữ
giống hệt với ý định NL đã cho khi tạo
test case, đặc biệt nếu mã liên quan đến các thao tác liên quan đến chuỗi (ví dụ: viết biểu thức chính quy
bằng tiếng Nhật). Chúng tôi khuyến khích người annotate viết
nhiều test case hợp lý và đa dạng hơn, bằng cách thay đổi
giá trị hoặc loại biến.

Vui lòng tìm hướng dẫn đầy đủ⁹ và ví dụ¹⁰
cho annotation trong kho lưu trữ mã của chúng tôi.

B Kết quả Baseline

Theo kết quả baseline trong § 5.1, chúng tôi cung cấp kết quả đánh giá chi tiết hơn, về tỷ lệ vượt qua thực thi dao động từ dự đoán top-1 đến top-10 của mô hình. Bảng 8 và Bảng 9 hiển thị độ chính xác thực thi zero-shot của các mô hình CODEX và CODEGEN, tương ứng.

[THIS IS TABLE: Two tables showing CODEX and CODEGEN zero-shot performance across different languages and pass rates]

⁹https://anonymous.4open.science/r/odex/data/
instruction.md
¹⁰https://anonymous.4open.science/r/odex/data/sample_
annotation.ipynb

--- TRANG 14 ---
C Kết quả Thực thi Theo Miền

Chúng tôi liệt kê kết quả chi tiết cho các thí nghiệm trong §5.

C.1 Miền Mở so với Miền Đóng

Bảng 10 và Bảng 11 hiển thị độ chính xác thực thi
cho CODEX và CODEGEN trên các vấn đề miền mở và
miền đóng, tương ứng.

[THIS IS TABLE: CODEX pass rate in open and closed domains - detailed table with multiple rows showing performance metrics for different languages and configurations]

[THIS IS TABLE: CODEGEN pass rate in various domains - detailed table showing performance across different model sizes and languages]

C.2 Độ chính xác Thực thi Theo Miền

Như được giới thiệu trong § 5.3, chúng tôi lấy CODE-DAVINCI-
002, và báo cáo độ chính xác thực thi của nó trên mỗi
miền trong Bảng 12.

C.3 Phân tích Lỗi Định tính

Để cung cấp giải thích trực quan hơn về
sự khác biệt miền đã đề cập, chúng tôi tiến hành phân tích lỗi trên 60 ví dụ được chọn ngẫu nhiên từ
bộ dữ liệu ODEX (15 cho mỗi ngôn ngữ). Bằng cách kiểm tra các mô hình lỗi từ những ví dụ này, chúng tôi
nhằm trả lời: các loại lỗi phổ biến trên
các vấn đề miền mở và đóng là gì? Sự khác biệt chính giữa chúng là gì?

Tương tự như phần trước, chúng tôi lấy
CODE-DAVINCI-002 vì nó điểm tốt nhất và
thể hiện khoảng cách miền rõ ràng, có thể đưa ra
sự khác biệt trực quan hơn giữa các miền.

Lỗi Miền Đóng Trong số 60 mẫu ngẫu nhiên
chúng tôi phân tích, 31 là các vấn đề miền đóng,
và CODEX dự đoán các giải pháp mã sai cho
22 trong số chúng. Chúng tôi xác định bốn loại lỗi chính
từ những mẫu này: (1) 11 trường hợp (50:0%) sử dụng
các hàm tích hợp Python không đúng, chủ yếu về
thao tác chuỗi và tính toán số; (2)
7 trường hợp (31:8%) thất bại ở các hàm phức tạp, thường
yêu cầu triển khai đa bước; (3) 4
trường hợp (18:2%) nhận được dự đoán trống, có thể
vì chúng liên quan đến các chủ đề không quen thuộc với

--- TRANG 15 ---
[THIS IS TABLE: CODE-DAVINCI-001 execution accuracy on each domain subset inside ODEX, showing Library, Count, and Pass@1 columns]

mô hình; (4) 2 trường hợp (9:1%) import thư viện bổ sung hoặc thêm triển khai dư thừa.

Lưu ý rằng số lượng trường hợp lỗi trong bốn
danh mục này không cộng lại thành 22. Vì chúng tôi phân tích
tất cả các dự đoán lỗi trong top-10
dự đoán của mô hình, một trường hợp có thể trình bày nhiều loại lỗi trong các dự đoán khác nhau của nó.

Lỗi Miền Mở Trong 29 vấn đề khác
thuộc miền mở, 26 trong số chúng có
dự đoán sai. Lỗi trong miền mở
thể hiện đa dạng hơn so với miền đóng.
Lỗi chính bao gồm 16 trường hợp (61:5%) là
thất bại trong việc sử dụng các thư viện điều kiện tiên quyết, hoặc thiếu
một phần của chúng khi có nhiều thư viện liên quan.
Loại chính tiếp theo là sử dụng hàm không đúng,
xảy ra trong 9 trường hợp (34:6%). Tương tự như
lỗi miền đóng, 5 trường hợp (19:2%) có
lỗi sử dụng hàm đúng, 4 trường hợp (15:4%)
vật lộn với triển khai đa bước phức tạp,
và 3 trường hợp (11:5%) đối mặt với dự đoán trống.

Các vấn đề OD và CD chia sẻ một số danh mục lỗi
như sử dụng sai hàm và thao tác phức tạp. Tuy nhiên, các vấn đề miền mở giới thiệu thách thức bổ sung: lựa chọn và sử dụng đúng
các thư viện và hàm trong thực tế.

D Metric Đánh giá

Chúng tôi mô tả từng metric không thực thi
(§ D.1) như được giới thiệu trong § 6, báo cáo hiệu suất mô hình
với mỗi metric (§ D.2), và trực quan hóa
tương quan của chúng với độ chính xác thực thi (§ D.3).

D.1 Mô tả Metric

BLEU BLEU (Papineni et al., 2002) là một
metric dựa trên từ vựng, tính toán sự chồng lấp n-
gram giữa dự đoán văn bản và (nhiều) tham chiếu. Hầu hết quy trình tính toán mặc định
tính toán lên đến 4-gram và áp dụng hàm làm mịn được giới thiệu trong Lin và Och (2004).

ROUGE ROUGE (Lin, 2004) là một metric
dựa trên từ vựng khác hướng recall hơn. Nó
ban đầu được thiết kế để đo tóm tắt văn bản, chủ yếu bằng cách đếm số lượng
đơn vị chồng lấp (n-gram, chuỗi từ, và
cặp từ) giữa dự đoán và tham chiếu.
Trong số nhiều biến thể được đề xuất (ROUGE-N,
ROUGE-L, ROUGE-W, và ROUGE-S), chúng tôi sử dụng
ROUGE-L phổ biến nhất trong các thí nghiệm của chúng tôi.

METEOR METEOR (Banerjee và Lavie,
2005) là một metric dựa trên unigram ban đầu dành cho dịch máy. Nó xây dựng trên khái niệm unigram tổng quát bằng cách liên quan đến độ chính xác unigram, recall unigram, và các biện pháp thứ tự từ.

ChrF ChrF (Popović, 2015) nhắm đến khớp từ vựng
ở cấp độ ký tự, bằng cách tính toán F-score n-gram cấp độ ký tự giữa dự đoán và tham chiếu. ChrF cũng ban đầu được đề xuất cho
nhiệm vụ dịch máy, nhưng sau đó được áp dụng cho một số
công việc đánh giá mã (Evtikhiev et al., 2022).

CodeBLEU CodeBLEU (Ren et al., 2020) được
thiết kế đặc biệt cho đánh giá mã, bằng cách xem xét chung
khớp dạng bề mặt, tương tự cú pháp, và luồng dữ liệu ngữ nghĩa.

D.2 Đánh giá với Metric Không Thực thi

Bảng 13 và Bảng 14 hiển thị điểm của CODEX
và CODEGEN sử dụng metric không thực thi.

--- TRANG 16 ---
[THIS IS TABLE: CODEX results on non-execution metrics]
Mô hình | NL | Metric
       |    | BLEU | ROUGE | METEOR | ChrF | CodeBLEU
C1     | en | 31.27| 52.79 | 55.43  |43.07 | 3.18
       | es | 13.69| 38.29 | 40.86  |21.17 | 3.96
       | ja | 18.57| 46.67 | 48.76  |34.89 | 3.63
       | ru | 14.42| 41.49 | 45.53  |34.63 | 2.70
D1     | en | 30.94| 53.88 | 56.01  |43.60 | 3.27
       | es | 20.40| 43.93 | 46.71  |29.36 | 3.27
       | ja | 19.98| 48.23 | 51.46  |38.41 | 3.40
       | ru | 16.97| 44.71 | 47.11  |35.54 | 2.74
D2     | en | 38.75| 56.05 | 55.39  |44.40 | 3.77
       | es | 18.47| 44.98 | 43.52  |27.11 | 5.78
       | ja | 27.10| 52.04 | 50.17  |40.02 | 3.58
       | ru | 25.00| 50.04 | 50.51  |38.60 | 3.75

[THIS IS TABLE: CODEGEN results on non-execution metrics]
Mô hình | NL | Metric
       |    | BLEU | ROUGE | METEOR | ChrF | CodeBLEU
350M   | en | 12.04| 50.94 | 50.46  |30.12 | 4.90
       | es | 9.07 | 37.90 | 37.76  |20.90 | 5.47
       | ja | 9.43 | 44.21 | 41.29  |26.16 | 6.05
       | ru | 13.35| 44.77 | 44.27  |32.40 | 3.86
2.7B   | en | 18.22| 54.82 | 54.32  |34.98 | 5.30
       | es | 13.05| 39.79 | 40.93  |22.61 | 6.67
       | ja | 14.72| 52.46 | 51.22  |31.28 | 5.42
       | ru | 23.27| 50.82 | 49.98  |37.75 | 4.31
6.1B   | en | 12.41| 52.82 | 54.03  |31.38 | 4.51
       | es | 11.69| 33.26 | 34.47  |19.04 | 4.57
       | ja | 19.14| 51.31 | 52.07  |34.78 | 5.68
       | ru | 23.66| 49.09 | 49.48  |37.44 | 3.72

D.3 Trực quan hóa Tương quan Metric

Theo thảo luận trong § 6, chúng tôi trực quan hóa
các metric không thực thi giữa các mẫu
vượt qua và thất bại trong thời gian thực thi.
Tất cả thí nghiệm sử dụng dự đoán CODE-DAVINCI-002
để đánh giá. Hình 13, Hình 14, Hình 15, Hình 16 minh họa biểu đồ giữa
các mẫu vượt qua/thất bại sử dụng metric ROUGE, METEOR,
ChrF, và CodeBLEU, tương ứng.

[THIS IS FIGURE: Four sets of histograms showing ROUGE, METEOR, ChrF, and CodeBLEU scores on passed and failed samples across EN, ES, JA, RU languages]

D.4 Tại sao Thực thi Tốt hơn?

Để đưa ra lý do trực quan hơn cho lợi thế
của thực thi, chúng tôi chọn ngẫu nhiên 15 trường hợp từ
mỗi tập con ngôn ngữ và xác định hai lợi ích chính: nó chấp nhận các giải pháp thay thế và cho phép
kết quả thực thi làm đầu ra.

Triển khai Mã Thay thế Có lẽ
lợi thế lớn nhất của thực thi là nó chỉ yêu cầu kết quả thực thi đúng, không có hạn chế
về các phương pháp thay thế, như trong Hình 17.

Tạo Trực tiếp Kết quả Thực thi Một
danh mục thú vị khác là tạo trực tiếp kết quả thực thi mã thay vì các bước triển khai. Điều này thường xảy ra với các truy vấn mã hóa đơn giản như thao tác chuỗi cơ bản, nơi
dự đoán kết quả có thể tốn của mô hình nỗ lực tương tự để có được các giải pháp lập trình.

Trong Hình 18, thay vì chương trình giải mã chuỗi, mô hình trực tiếp xuất chuỗi kết quả
"JLK". Mặc dù điều này hơi bất ngờ trong
nhiệm vụ NL-to-Code, thực thi hiệu quả xử lý
những trường hợp như vậy và sẽ đánh giá chúng là đúng.

--- TRANG 17 ---
[THIS IS FIGURE: Two code examples shown side by side - Figure 17 showing an alternative correct prediction with low BLEU score, and Figure 18 showing example output with correct execution result but achieving low BLEU]

D.5 Lợi ích Tiềm năng của Metric Dựa trên Từ vựng

Các metric dựa trên từ vựng, mặc dù tương đối không hiệu quả cho tính đúng đắn chức năng, vẫn có thể
hữu ích cho việc debug và diễn giải. Chúng
hiệu quả trong các lỗi nhỏ của hai loại: (1) một lỗi sử dụng hàm duy nhất và (2) sự khác biệt nhẹ trong các chuỗi phức tạp. Sự khớp từ vựng cao trong những trường hợp như vậy chỉ ra ít nỗ lực hơn để sửa (Deng et al., 2021).

Sử dụng Sai Hàm Một số dự đoán mã đúng ngoại trừ một chỗ duy nhất nơi một hàm sai được sử dụng, hoặc một đối số bị đặt sai chỗ.

Ví dụ, trong Hình 19, mã import thư viện và sao chép tất cả chuỗi đúng. Nhưng nó sử dụng
hàm sai match thay vì findall đúng. Mặc dù thực thi thất bại, mã tương tự với giải pháp. Với dấu hiệu của điểm BLEU cao 92:5, chúng ta có thể dễ dàng phát hiện những tương tự như vậy và sửa chúng bằng các chỉnh sửa đơn giản.

Khác biệt Chuỗi Một lỗi thường xuyên khác liên quan đến việc sao chép chuỗi, nơi mã gọi các hàm đúng nhưng sao chép chuỗi khác nhau.

Ví dụ trong Hình 20 có điểm BLEU 100:0,
nhưng chuỗi bên trong thực sự thiếu một khoảng trắng duy nhất, mà quá trình tokenization BLEU sẽ bỏ qua. Mã như vậy cũng giống với giải pháp và
có thể được sửa dễ dàng ngay cả bằng các phương pháp dựa trên quy tắc.

[THIS IS FIGURE: Two more code examples - Figure 19 showing model prediction using wrong function with high BLEU score, and Figure 20 showing model prediction with slight string variation but perfect BLEU score]

E Nghiên cứu Ablation

Phần này cung cấp các bảng kết quả theo
mỗi phần nghiên cứu ablation trong § 7.

E.1 Chiến lược Prompting

E.1.1 Prompting Few-shot

Bảng 15, Bảng 16, Bảng 17 hiển thị sự thay đổi trong
độ chính xác thực thi đối với các ví dụ trong
học trong bối cảnh, trên ba biến thể CODEX

[THIS IS TABLE: Three tables showing CODE-CUSHMAN-001, CODE-DAVINCI-001, and CODE-DAVINCI-002 few-shot results with pass rates across different languages and shot configurations]

--- TRANG 18 ---
[THIS IS FIGURE: Example showing model prediction with slight string copying variation but scores 100.0 in BLEU]

Hình 20: Ví dụ về dự đoán mô hình khác nhau nhẹ trong việc sao chép chuỗi, nhưng điểm 100:0 trong BLEU.

[THIS IS TABLE: Performance results table showing different test configurations and pass rates for various languages]

E.1.2 Số lượng Test Case Đầu vào

Bảng 18 hiển thị ảnh hưởng đến độ chính xác thực thi,
của việc thêm một hoặc nhiều test case vào prompt. Thí nghiệm sử dụng CODE-DAVINCI-002 làm ví dụ.

E.1.3 Tiền xử lý: Khoảng trắng Dư thừa

Trong khi quá trình xây dựng đầu vào có thể giới thiệu khoảng trắng ở đầu và cuối của
chuỗi văn bản, chúng tôi thấy mô hình CODEGEN
nhạy cảm bất ngờ với khoảng trắng dư thừa. Như được hiển thị trong
Bảng 19, việc loại bỏ khoảng trắng khỏi đầu vào prompt
tăng tỷ lệ vượt qua của tất cả các mô hình CODEGEN kích thước bởi hơn 20 phần trăm.

[THIS IS TABLE: CODEGEN results comparison with and without trailing whitespaces]

Chúng tôi đưa ra giả thuyết rằng lợi ích mang lại bởi việc loại bỏ khoảng trắng là
sự phù hợp phân phối tốt hơn với dữ liệu huấn luyện CODEGEN. Vì CODEGEN có thể được
tiền huấn luyện trên các chuỗi văn bản đã loại bỏ khoảng trắng, các đầu vào không có khoảng trắng có thể phù hợp hơn
với chúng, do đó dẫn đến hiệu suất thời gian test tốt hơn. Trong khi đó, lưu ý rằng các quá trình tokenization cho văn bản (ngôn ngữ tự nhiên) và
mã (ngôn ngữ lập trình) khác nhau trong các token kiểu khoảng trắng như \n hoặc \t. Những token này sẽ
được loại bỏ bởi tokenizer văn bản theo mặc định, trong khi được
bảo tồn bởi tokenizer mã vì chúng ngụ ý thông tin cấu trúc trong các đoạn mã.

E.2 Số lượng Test Case Đánh giá

Bảng 20 hiển thị hiệu ứng khi sử dụng số lượng test case khác nhau cho đánh giá dựa trên thực thi.

E.2.1 Số lượng Test Case Đánh giá

[THIS IS TABLE: Results showing different numbers of test cases for execution-based evaluation]

E.3 Ngữ nghĩa của Tên Hàm

Vì mã được gói vào các hàm để cho phép
thực thi, cách đặt tên hàm có thể ảnh hưởng đến
dự đoán mô hình. Theo mặc định, chúng tôi đặt tên hàm bằng cách sử dụng ID bài đăng (ví dụ: f_3844801), biểu thị ít ngữ nghĩa của các truy vấn. Vì vậy chúng tôi thử
hai phương pháp khác: (1) một chuỗi hằng số function;
và (2) các cụm từ tóm tắt từ ý định NL, ví dụ:
find_max_value.

Để làm (2), chúng tôi tiến hành trích xuất cụm từ heuristic. Đầu tiên chúng tôi cắt ý định NL thành các từ bằng
khoảng trắng, sau đó loại bỏ các từ dừng ('in', 'of',
'a', 'to', 'and', 'for', 'with', 'that') và dấu câu vô nghĩa, cuối cùng, nối M = 4
từ đầu tiên với '_'. Ví dụ, với ý định "decode a hex string '4a4b4c' to UTF-8", tên hàm kết quả sẽ là "decode_a_hex_string".

Tuy nhiên, đối với các ngôn ngữ không tách từ
bằng khoảng trắng, cách tiếp cận này có thể tạo ra các chuỗi ít ý nghĩa hơn, do đó góp phần vào hiệu suất kém hơn như được hiển thị dưới đây.

--- TRANG 19 ---
Để so sánh công bằng với kết quả trước đây, chúng tôi không
thêm test case trong prompt.

[THIS IS FIGURE: Bar chart showing pass@1 performance using different function names across languages EN, ES, JA, RU]

Hình 21: pass@1 sử dụng các tên hàm khác nhau.

Từ Hình 21 và Bảng 21, việc sử dụng tên hàm có ý nghĩa ngữ nghĩa hơn hầu như không cải thiện so với thiết lập mặc định. Trực quan, việc tóm tắt tên từ ý định không thêm ngữ nghĩa bổ sung,
nhưng có thể gây ra mất thông tin ở bước curation,
cả hai đều góp phần vào sự sụt giảm hiệu suất.

[THIS IS TABLE: CODE-DAVINCI-002 results showing pass rates for different function naming conventions across languages]

F Công việc Liên quan

Sinh mã Miền Mở Mã được viết
trong các ngôn ngữ lập trình đa năng thường
sử dụng các lớp hoặc hàm từ các thư viện bên ngoài.
Một số bộ dữ liệu cho sinh mã bảo tồn
bản chất miền mở này. Bộ dữ liệu CONCODE (Iyer et al.,
2018) đã kiểm tra sinh các phương thức lớp Java. Các công trình sau đó nhắm đến sinh Python với bối cảnh tương tác của Jupyter Notebook (Agashe
et al., 2019) hoặc ý định ngôn ngữ tự nhiên từ các bài đăng StackOverflow (Yin et al., 2018; Wang et al.,
2022). Mặc dù phạm vi tự nhiên của chúng, việc cho phép
thực thi mã miền mở đã đối mặt với những thách thức lớn với sự đa dạng và phức tạp của nó (Lai et al.,
2022; Chandel et al., 2022). Để giải quyết vấn đề này,
ODEX của chúng tôi cung cấp test case làm bối cảnh thực thi mã cho đánh giá.

Đánh giá Mã thông qua Thực thi Đánh giá dựa trên thực thi đã được áp dụng lâu cho
các ngôn ngữ lập trình cụ thể miền như truy vấn SQL (Zhong et al., 2017) hoặc các dạng logic (Dong và Lapata, 2016). Mô hình dựa trên thực thi này chưa được giới thiệu đến các ngôn ngữ đa năng cho đến gần đây bởi bộ dữ liệu HumanEval (Chen et al., 2021), nơi các test case do con người viết được cung cấp cho thực thi mã. Nhiều công trình sau đó theo cách tiếp cận này, nhưng tập trung nhiều hơn vào các thiết lập miền đóng (Austin et al.,
2021; Hendrycks et al., 2021) hoặc các thư viện cụ thể quan tâm (Lai et al., 2022; Huang et al., 2022). Hướng đến môi trường thực thi rộng hơn, chúng tôi cung cấp
test case có thể thực thi cho nhiều đến 79 thư viện.

Truy vấn Mã hóa so với Thách thức Lập trình
Các chương trình từ các nguồn khác nhau được tổ chức cho các mục đích khác nhau. Các trang web cuộc thi mã hóa như LeetCode¹¹ và Codeforces¹² đã được
sử dụng để xây dựng nhiều benchmark sinh mã (Hendrycks et al., 2021; Li et al., 2022).
Tuy nhiên, chúng ngẫu nhiên phù hợp với cách con người
lập trình trong các tình huống thực tế. Để xây dựng bộ dữ liệu
với việc sử dụng mã tự nhiên và thực tế, nhiều
công trình sử dụng GitHub Jupyter Notebook (Agashe
et al., 2019; Huang et al., 2022) và các diễn đàn StackOverflow (Yin et al., 2018; Wang et al., 2022; Lai
et al., 2022) làm nguồn mã xuất hiện tự nhiên.
Chúng tôi duy trì tính tự nhiên như vậy bằng cách sử dụng các bài đăng StackOverflow, nhưng duy nhất từ các diễn đàn bằng nhiều ngôn ngữ khác nhau để cũng hỗ trợ lập trình viên trên toàn thế giới.

Tạo Test Case Trong khi hầu hết benchmark sử dụng
test case Python được chú thích bởi lập trình viên con người (Chen et al., 2021; Nijkamp et al., 2022;
Lai et al., 2022), các bộ dữ liệu kiểu thách thức áp dụng
cách tiếp cận trực tiếp hơn bằng cách crawl từ
web (Hendrycks et al., 2021; Li et al., 2022). Một
luồng công việc khác cố gắng tạo test
case tự động dựa trên ngữ pháp Python (Lukasczyk và Fraser, 2022), nhưng phần lớn
bị hạn chế đối với các hàm Python cơ bản. Một số đề xuất
tận dụng sức mạnh của LM neural (Tufano et al.,
2020; Li et al., 2022), thậm chí xem xét chung sinh giải pháp và test case (Chen et al., 2022).
Tuy nhiên, chất lượng và đa dạng của test case không được đảm bảo mạnh mẽ. Do đó chúng tôi sử dụng các test case do con người viết chất lượng cao cho đánh giá ODEX.

¹¹https://leetcode.com/
¹²https://codeforces.com/
