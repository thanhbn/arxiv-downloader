# PECC: Trích xuất Vấn đề và Thử thách Lập trình
Patrick Haller, Jonas Golde, Alan Akbik
Humboldt-Universit ¨at zu Berlin
{patrick.haller.1, jonas.golde, alan.akbik }@hu-berlin.de

## Tóm tắt
Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng đặc biệt của chúng trong nhiều tác vụ khác nhau, như tạo mã, giải quyết vấn đề và suy luận. Các bộ đánh giá hiện tại đánh giá các tác vụ một cách riêng lẻ, tuy nhiên mức độ mà LLM có thể hiểu các tác vụ dạng văn xuôi, xác định các vấn đề cơ bản, và sau đó tạo ra các giải pháp mã thích hợp vẫn chưa được khám phá. Để giải quyết khoảng trống này, chúng tôi giới thiệu PECC, một bộ đánh giá mới được tạo ra từ các thử thách Advent Of Code (AoC) và Project Euler, bao gồm 2396 vấn đề. Không giống như các bộ đánh giá thông thường, PECC yêu cầu LLM diễn giải các vấn đề nhúng trong câu chuyện, trích xuất yêu cầu, và tạo mã có thể thực thi. Một đặc điểm chính của tập dữ liệu của chúng tôi là sự phức tạp được thêm vào bởi việc nhắc nhở bằng ngôn ngữ tự nhiên trong các đánh giá dựa trên trò chuyện, phản ánh sự mơ hồ của hướng dẫn trong thế giới thực. Kết quả cho thấy hiệu suất mô hình khác nhau giữa các vấn đề có câu chuyện và trung tính, với những thử thách cụ thể trong tập con toán học Euler với GPT-3.5-Turbo vượt qua 50% thử thách AoC và chỉ 8% trên các vấn đề Euler. Bằng cách thăm dò giới hạn khả năng của LLM, bộ đánh giá của chúng tôi cung cấp một khung để theo dõi và đánh giá tiến bộ tiếp theo của LLM như một công cụ giải quyết vấn đề phổ quát.

**Từ khóa:** tập dữ liệu đánh giá, khả năng lập trình và toán học, trích xuất vấn đề

## 1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng đáng chú ý trong các tác vụ tạo sinh đa dạng, bao gồm văn bản và hơn thế nữa. Do đó, chúng đã trở thành công cụ đáng tin cậy cho việc tạo mã, giảm rào cản cho các kỹ sư cấp độ đầu vào và hỗ trợ các lập trình viên có kinh nghiệm. Các nhà nghiên cứu đã tạo ra các bộ đánh giá dựa trên hướng dẫn để đánh giá tiến bộ của khả năng tạo mã trong các mô hình ngôn ngữ mới (Hendrycks et al., 2021a,b; Lai et al., 2022). Các bộ đánh giá này nhắc một LLM cho trước tạo mã có thể thực thi cho các vấn đề được chỉ định. Mặc dù các bộ đánh giá này tách biệt khỏi các tiến bộ LLM khác, như cải thiện khả năng đọc hiểu hoặc trừu tượng hóa vấn đề (Wang et al., 2019; Reddy et al., 2019; Liu et al., 2020), việc tích hợp tất cả các kỹ năng này có thể biến LLM thành công cụ giải quyết vấn đề phổ quát. Do đó, mức độ LLM có thể kết hợp khả năng của chúng để (1) hiểu các vấn đề dạng văn xuôi, (2) xác định yêu cầu giải pháp, và (3) dịch chúng thành mã có thể thực thi vẫn chủ yếu chưa được khám phá.

Hình minh họa trong Hình 1 mô tả một quy trình có hệ thống của việc tạo mã và xác nhận để kiểm tra các khả năng được đặt tên theo cách tự động. Bước đầu tiên (A) liên quan đến việc hướng dẫn mô hình ngôn ngữ, thiết lập nền tảng ngữ cảnh, và xác định yêu cầu tác vụ. Tiếp theo, mô hình nhận một tuyên bố vấn đề (B) mà nó cần tạo ra một giải pháp lập trình. Bước tiếp theo liên quan đến việc mô hình tạo mã Python có thể thực thi (D) dựa trên tác vụ và dữ liệu đầu vào được cung cấp (C). Việc thực thi

[Hình 1: Biểu diễn sơ đồ của quy trình tạo mã và xác nhận.]

mã được tạo ra sẽ cho ra một kết quả (E), sau đó được so sánh với kết quả mong đợi (F). Các mũi tên hướng chỉ ra dòng chảy từ hướng dẫn đến xác nhận kết quả. Việc nhận ra quy trình có hệ thống này nhấn mạnh nhu cầu về các bộ đánh giá có thể thực sự kiểm tra những bước phức tạp này, đặc biệt khi xem xét các ứng dụng thế giới thực nơi hiểu biết về câu chuyện là then chốt.

Trong bài báo này, chúng tôi giới thiệu PECC, một bộ đánh giá mở rộng tập trung vào việc tạo mã từ các mô tả vấn đề nhúng trong câu chuyện. Không giống như các bộ đánh giá trước đây đánh giá việc tạo mã bằng cách sử dụng các hướng dẫn cụ thể, tập dữ liệu của chúng tôi yêu cầu các mô hình hiểu, trích xuất yêu cầu, và tạo ra mã thiết yếu để giải quyết vấn đề. Cách tiếp cận này đòi hỏi các chương trình chính xác về mặt cú pháp và đòi hỏi kỹ năng đọc hiểu để rút ra giải pháp mong muốn.

Tập dữ liệu của chúng tôi bao gồm 2,396 vấn đề trải rộng các mức độ khó khác nhau. Chúng tôi sử dụng các vấn đề từ các thử thách Advent Of Code¹ hàng năm và nền tảng trực tuyến Project Euler² làm tập dữ liệu nguồn. Các thử thách được trình bày theo phong cách văn xuôi hoặc định dạng trung tính. Chúng tôi biến đổi mỗi tập dữ liệu thành phong cách viết đối lập, cho phép chúng tôi đánh giá sự trừu tượng hóa vấn đề qua các công thức khác nhau, cụ thể là công thức trung tính so với câu chuyện.

Nếu một mô hình hoạt động tốt trên PECC, điều đó biểu thị khả năng thành thạo của các mô hình trong việc hiểu chính xác các đặc tả ngôn ngữ tự nhiên và tạo mã đúng. Thành tựu này dựa trên khả năng của mô hình trong việc sử dụng các cấu trúc dữ liệu và thuật toán thích hợp để giải quyết vấn đề được đóng gói. Ngoài ra, tập dữ liệu cho phép chúng tôi đánh giá kỹ năng giải quyết vấn đề tuần tự của các mô hình trong các tình huống lập trình thế giới thực. Đánh giá này xảy ra khi các giải pháp cho một vấn đề là điều kiện tiên quyết để giải quyết vấn đề tiếp theo, như với phần chia Advent Of Code (AoC). Trong AoC, mỗi ngày bao gồm hai thử thách, trong đó thử thách thứ hai chỉ có thể được giải quyết bằng cách sử dụng

¹https://adventofcode.com/
²https://projecteuler.net/

giải pháp được rút ra từ vấn đề đầu tiên.

Chúng tôi tóm tắt đóng góp của bài báo này như sau:

1. Chúng tôi giới thiệu quy trình xây dựng PECC, một bộ đánh giá mới được thiết kế để đánh giá LLM trong các thử thách lập trình dạng văn xuôi.

2. Chúng tôi đánh giá các mô hình ngôn ngữ tiên tiến và cho thấy rằng mặc dù chúng hoạt động tốt trên các tác vụ đơn giản, hiệu suất của chúng giảm đáng kể khi độ phức tạp của tác vụ tăng lên.

3. Chúng tôi phân tích kỹ lưỡng hiệu suất, lỗi chương trình, và các sơ đồ nhắc nhở khác nhau trên tất cả các loại vấn đề và mức độ khó trong PECC.

4. Chúng tôi sẽ cung cấp tập dữ liệu và khung đánh giá cho cộng đồng nghiên cứu tại https://github.com/HallerPatrick/pecc. Pipeline hỗ trợ nhiều nhà cung cấp LLM nổi bật và lưu trữ cục bộ để suy luận.

## 2. Tập dữ liệu PECC

Tập dữ liệu PECC tận dụng hai nguồn tài nguyên nổi bật: Các thử thách Advent of Code (AoC) và Project Euler.

### 2.1. Xây dựng Tập dữ liệu

**Advent of Code.** AoC, một sự kiện trực tuyến hàng năm, triển khai các thử thách lập trình hàng ngày trong suốt tháng Mười hai, mỗi thử thách được nhúng trong một câu chuyện lễ hội. Nó trình bày các vấn đề phức tạp, cho phép các lập trình viên tinh tế kỹ năng giải quyết vấn đề của họ trong một bối cảnh vui tươi, được thúc đẩy bởi câu chuyện.

**Project Euler.** Mặt khác, Project Euler, một nền tảng trực tuyến, tuyển chọn các vấn đề toán học và tính toán đòi hỏi sự kết hợp giữa hiểu biết toán học và kỹ năng lập trình để giải quyết. Nó thúc đẩy học tập và giải quyết vấn đề trong một môi trường được thúc đẩy bởi cộng đồng, có các mức độ khó tăng dần. Các mức độ khó tăng theo bước 5 từ 0 đến 100.

Cách trình bày vấn đề và độ dài lời trong AoC và Project Euler đứng trong sự tương phản rõ rệt với nhau. Trong khi các vấn đề AoC dựa trên câu chuyện, thường được bao bọc trong một câu chuyện với các mô tả vấn đề dài, Project Euler cung cấp những mô tả chính xác, ngắn gọn. Sự khác biệt này phản ánh các cách tiếp cận khác nhau của hai nền tảng trong việc thu hút đối tượng của họ và làm sáng tỏ các vấn đề. Những độ dài lời của vấn đề này cung cấp một nền tảng phong phú để công thức hóa tập dữ liệu PECC.

### 2.2. Mở rộng Phong cách Vấn đề AoC và Euler

Chúng tôi mở rộng các tập dữ liệu bổ sung để khám phá cách các công thức vấn đề khác nhau - phong cách câu chuyện hoặc phong cách trung tính - ảnh hưởng đến khả năng giải quyết vấn đề. Từ các mô tả ngắn gọn của Project Euler, chúng tôi tạo ra các vấn đề với một khúc quanh câu chuyện, thêm một bối cảnh được thúc đẩy bởi câu chuyện. Ngược lại, chúng tôi biến đổi các vấn đề AoC chi tiết thành các thử thách có tông màu trung tính để giống với sự trực tiếp của Project Euler. Những mở rộng này tạo ra các bài trình bày vấn đề đa dạng để kiểm tra khả năng thích ứng của mô hình ngôn ngữ với các bối cảnh vấn đề khác nhau.

Các tập dữ liệu gốc bao gồm 392 vấn đề từ AoC và 806 vấn đề từ Project Euler, tổng cộng 1,198 vấn đề. Sau khi tạo các điểm dữ liệu bổ sung, tập dữ liệu bao gồm 2,352 vấn đề, mỗi vấn đề với giải pháp tương ứng, thường là một số nguyên, và trong trường hợp của AoC, đầu vào thực tế cần thiết để giải quyết các thử thách. Hình 2 minh họa các vấn đề bổ sung và các giải pháp được tạo ra của chúng. Chúng tôi ký hiệu các nguồn gốc là aoc và euler cho các thử thách Advent of Code và Project Euler, tương ứng. Các bổ sung được tạo ra là aoc-concise và euler-stories.

Một giả định phổ biến có thể là để một mô hình ngôn ngữ tạo ra một mô tả bổ sung của một vấn đề cho trước, nó phải có khả năng giải quyết vấn đề đó. Tuy nhiên, các kiểm tra chất lượng của chúng tôi cho thấy rằng khả năng mô tả hoặc diễn đạt lại một vấn đề không nhất thiết ngụ ý một sự hiểu biết sâu sắc hoặc khả năng giải quyết nó.

### 2.3. Nhắc nhở Đơn và Đa lượt

Tập dữ liệu của chúng tôi sử dụng hai công thức nhắc nhở khác biệt. Chúng tôi sử dụng định dạng (1) dựa trên hướng dẫn cho tập con Euler để hướng dẫn các mô hình tạo các chương trình Python có thể thực thi và trả về câu trả lời qua đầu ra tiêu chuẩn. Ngược lại, chúng tôi sử dụng định dạng (2) dựa trên trò chuyện cho tập con AoC do cấu trúc vấn đề hai phần của nó. Chúng tôi hướng dẫn các mô hình tạo mã Python có thể thực thi trực tiếp, với việc bổ sung tải đầu vào từ một tệp "input.txt", mà các mô hình cũng cần tạo mã liên quan, đảm bảo sự tham gia toàn diện với tình huống giải quyết vấn đề. Chúng tôi lặp lại quy trình này cho phần thứ hai của thử thách AoC, bao gồm lịch sử cuộc trò chuyện trước đó. Mỗi thử thách AoC bắt đầu với một phần đầu tiên tự chứa và có thể sử dụng định dạng dựa trên hướng dẫn bằng cách bao gồm tệp "input.txt". Phù hợp với các bộ đánh giá hiện có, chúng tôi sử dụng định dạng zero-shot cho PECC, nhưng tất cả các vấn đề đều không có ví dụ few-shot.

## 3. Thí nghiệm

### 3.1. Thiết lập Thí nghiệm

Chúng tôi đánh giá các mô hình ngôn ngữ khác nhau trên PECC để điều tra mức độ mà các mô hình này có thể khai thác khả năng vốn có của chúng để giải quyết các tác vụ lập trình phức tạp xem xét các công thức vấn đề khác nhau. Chúng tôi chọn GPT-3.5-turbo-16k (gpt-3.5-turbo) (OpenAI, 2023) từ OpenAI và PALM 2 (chat-bison) của VertexAI (Anil et al., 2023), Codey (codechat-bison) và Claude 3 Haiku (claude-haiku) (Anthropic, 2024), vì những mô hình này là một trong những mô hình có khả năng nhất về xử lý ngôn ngữ tự nhiên và các tác vụ tạo mã. Ngoài ra, chúng tôi đã thử nghiệm một số mô hình ngôn ngữ mã nguồn mở khác nhau, được tinh chỉnh hướng dẫn và trò chuyện: (Luo et al., 2023; Jiang et al., 2023, 2024; AI@Meta, 2024; Abdin et al., 2024) Phần sau của nghiên cứu tập trung vào việc so sánh hiệu suất của các mô hình này, điều này rất cần thiết để phân biệt điểm mạnh và điểm yếu tương đối của chúng trong bối cảnh tạo mã.

Pipeline đánh giá của chúng tôi dựa trên thư viện Langchain (Chase, 2022), giúp tạo điều kiện cho các định dạng trò chuyện và dựa trên hướng dẫn để tương tác với các mô hình. Mã nguồn được tạo ra được thực thi trong một môi trường Python cô lập, đảm bảo một quy trình thực thi an toàn và được kiểm soát.

### 3.2. Phân tích Định tính Chế độ Lỗi

Trong quá trình thực thi mã, chúng tôi trích xuất và phân loại các lỗi gặp phải thành năm loại khác biệt:

1. **Lỗi Cú pháp**: Phát sinh khi LLM tạo ra không có hoặc chỉ có một phần mã nguồn Python, hoặc trình thông dịch Python trả về SyntaxError.

2. **Lỗi Thời gian chạy**: Xảy ra khi một chương trình, mặc dù đúng về mặt cú pháp, dừng lại do lỗi trong quá trình thực thi, như IndexErrors, KeyError, và NameError.

3. **Lỗi Hết thời gian**: Được kích hoạt khi thời gian chạy của chương trình vượt quá ngưỡng đặt là 60 giây.

4. **Đầu ra Sai**: Được chỉ định cho khi một chương trình thoát thành công nhưng cho ra đầu ra khác với kết quả mong đợi.

5. **Phần 1 Thất bại**: Xảy ra khi Phần 1 của các thử thách AoC không vượt qua.

Phân loại lỗi có cấu trúc này hỗ trợ phân tích chi tiết hiệu suất của các mô hình, xác định chính xác các khu vực mạnh và các con đường tiềm năng để cải thiện.

### 3.3. Chỉ số

**Pass@k.** Chỉ số Pass@k đánh giá tổng hợp mã của mô hình bằng cách kiểm tra xem ít nhất một trong 'k' đoạn mã được tạo ra hàng đầu có vượt qua các bài kiểm tra đơn vị được xác định trước không. Được sử dụng trong các khung như CodeEval, nó đánh giá mức độ các mô hình dịch các nhắc nhở ngôn ngữ tự nhiên thành mã chính xác, có thể thực thi. Nó đo lường định lượng khả năng của mô hình để tạo mã hợp lệ về mặt chức năng. Trong đánh giá của chúng tôi, mã hợp lệ về mặt chức năng được định nghĩa là mã có thể thực thi và trả về kết quả mong đợi qua đầu ra tiêu chuẩn. Việc áp dụng Pass@k cho phép hiểu biết sắc thái về hiệu suất mô hình, đặc biệt trong các tình huống nơi nhiều giải pháp mã có thể hợp lý.

**Pass@k + Difficulty.** Giới thiệu một thuật ngữ khó bổ sung cho Pass@k, được gọi là Pass@k-Difficulty, cho phép đánh giá phân biệt khả năng của mô hình để giải quyết các vấn đề có độ khó khác nhau, cung cấp một sự hiểu biết chi tiết về điểm mạnh và điểm yếu của mô hình. Trọng số khó được xác định dựa trên tỷ lệ phần trăm của những người tham gia giải quyết thành công một vấn đề cho trước, cung cấp một sự phản ánh thế giới thực về độ phức tạp của vấn đề. Pass@k-Difficulty được thực hiện bằng cách giới thiệu một yếu tố trọng số dựa trên độ khó vấn đề vào công thức chỉ số Pass@k hiện có tính đến độ khó vấn đề, với trọng số cao hơn được trao cho các vấn đề phức tạp hơn, do đó trình bày một sự phản ánh chính xác hơn về hiệu suất của mô hình trong các tình huống thách thức. Chúng tôi đã thu thập thống kê người dùng về dân số cơ sở và số lượng người tham gia đã giải quyết mỗi vấn đề từ trang web tương ứng của họ³

³Thống kê cho AoC có thể truy cập tại https://adventofcode.com/2022/stats và Euler Project tại https://projecteuler.net/problem_analysis

### 3.4. Công thức Nhắc nhở

Hơn nữa, chúng tôi so sánh hiệu suất của các mô hình bằng cách sử dụng các công thức nhắc nhở đa dạng. Trong khi tạo và thực thi mã Python để giải quyết vấn đề trình bày một thử thách đáng gờm, thí nghiệm này tập trung vào việc xác định liệu LLM có thể giải quyết hiệu quả những thử thách này bằng cách tận dụng kiến thức thế giới vốn có của chúng, được định nghĩa là năng khiếu nội tại của chúng để giải quyết các vấn đề logic hoặc toán học.

Hơn nữa, chúng tôi giới thiệu một lớp phức tạp bổ sung bằng cách buộc các mô hình phải sử dụng một quy trình chuỗi suy nghĩ có hệ thống, yêu cầu chúng biện minh và làm sáng tỏ câu trả lời của họ một cách toàn diện.

Tóm tắt, chúng tôi đã phân chia thí nghiệm này thành hai bối cảnh khác biệt: thứ nhất, nơi LLM phản hồi các thử thách chỉ dựa vào kiến thức thế giới bẩm sinh của chúng, được gọi là trả lời; và thứ hai, nơi LLM được giao nhiệm vụ biện minh giải pháp của chúng thông qua một quy trình chuỗi suy nghĩ, được ký hiệu là trả lời + chuỗi suy nghĩ.

## 4. Kết quả

### 4.1. Đánh giá Đầu ra

Chúng tôi hiển thị kết quả chính của đánh giá trong Bảng 2. Bảng được trình bày mô tả một đánh giá so sánh của một số mô hình ngôn ngữ trên các tập con khác biệt của tập dữ liệu PECC, quan sát hiệu suất tại k=1 và k=3 cho các tập con vấn đề tương ứng. Các mô hình được đánh giá bao gồm các mô hình độc quyền từ OpenAI và VertexAI, cùng với các mô hình hướng dẫn và trò chuyện tinh chỉnh mã nguồn mở.

**Lấy mẫu đa lần tăng khả năng của các giải pháp đúng.** Có thể nhận ra rằng việc sử dụng cách tiếp cận lấy mẫu đa lần với k=3 nói chung cải thiện điểm Pass@k so với khi k=1, đó là lấy mẫu đơn. Sự cải thiện này đặc biệt đáng chú ý trong tập con Advent of Code (AoC) cho các mô hình gpt-3.5-turbo và claude-haiku. Sự tăng điểm từ k=1 đến k=3 cho thấy rằng việc cung cấp cho các mô hình nhiều lần thử để giải quyết một vấn đề tăng cường khả năng tạo ra một giải pháp đúng. Tuy nhiên, lấy mẫu đa lần không phải lúc nào cũng đảm bảo kết quả tốt hơn. Nếu một mô hình không thể giải quyết một vấn đề cho trước một cách vốn có, như quan sát được với mô hình mistral-instruct, thì nhiều lần thử sẽ không nhất thiết dẫn đến điểm số tốt hơn. Hiệu quả của lấy mẫu đa lần phụ thuộc vào khả năng cơ bản của mô hình trong việc giải quyết nhiệm vụ.

**Câu chuyện có thể hỗ trợ hoặc cản trở các mô hình.** Hiệu suất khác nhau trên các tập dữ liệu AoC và Euler tiết lộ rằng câu chuyện có thể là một con dao hai lưỡi. Trong đánh giá, tập con AoC được thúc đẩy bởi câu chuyện tỏ ra phù hợp hơn cho các mô hình so với đối tác được công thức hóa một cách trung tính. Những câu chuyện trong AoC cung cấp bối cảnh quan trọng, tăng cường khả năng của mô hình để giải mã và giải quyết vấn đề. Ngược lại, việc giới thiệu câu chuyện dẫn đến sự sụt giảm hiệu suất cho tập con Project Euler (Euler), bao gồm các vấn đề toán học chính xác. Sự giảm này cho thấy rằng việc thêm câu chuyện vào Euler giới thiệu sự mơ hồ, làm phức tạp các vấn đề và giảm hiệu quả của mô hình. Sự khác biệt rõ rệt này trong hiệu suất trên hai tập dữ liệu nhấn mạnh một thiên vị đào tạo tiềm năng trong các mô hình: chúng có thể xuất sắc trong các môi trường giàu câu chuyện như AoC nhưng gặp khó khăn với các thử thách được thúc đẩy bởi độ chính xác vốn có của Euler.

**LLM nói chung không đạt yêu cầu trên các thử thách phức tạp.** Các đánh giá nhấn mạnh những thử thách vốn có trong các vấn đề lập trình phức tạp, đặc biệt rõ ràng trong tập con Euler. gpt-3.5-turbo và claude-haiku nổi lên như những người thực hiện tương đối mạnh trong số đó, tuy nhiên kết quả cũng làm nổi bật một không gian đáng kể để cải thiện trên tất cả các mô hình. Đánh giá gợi ý một cách tinh tế về những giá trị triển vọng của việc tinh chỉnh hoặc sử dụng các mô hình chuyên biệt trong tổng hợp mã để tăng cường hiệu suất trong những nhiệm vụ như vậy.

**Ảnh hưởng Tiềm năng của Contamination Tiền đào tạo.** Với sự phổ biến và nổi bật của các vấn đề AoC, có một rủi ro có thể hình dung về contamination tiền đào tạo cho các mô hình ngôn ngữ. Có thể nhiều mô hình này, trong giai đoạn đào tạo của chúng, đã được tiếp xúc với vô số giải pháp cho các vấn đề AoC có sẵn công khai trong Python. Sự tiếp xúc này đặt ra câu hỏi về tính xác thực trong phản hồi của mô hình: Mô hình có thực sự cố gắng giải quyết vấn đề dựa trên giá trị của nó, hay nó chỉ đơn giản là nhớ lại một giải pháp đã gặp trước đó? Việc xác định mức độ của ảnh hưởng này là thách thức. Chúng tôi không thể phân biệt một cách rõ ràng liệu một mô hình tạo ra một giải pháp dựa trên khả năng giải quyết vấn đề của nó hay dựa vào sự hồi tưởng về các giải pháp đã thấy trước đó. Tiềm năng này cho contamination nhấn mạnh sự phức tạp của việc đánh giá các mô hình ngôn ngữ trên các nhiệm vụ nơi dữ liệu đào tạo có thể chồng chéo với các bộ đánh giá.

### 4.2. Phân tích Lỗi

Bảng 3 trình bày phân tích lỗi tỉ mỉ trên tất cả các mô hình được đánh giá và tập con tập dữ liệu, phân loại lỗi thành Runtime Error, Wrong Output, Timeout, và Syntax Error. Rõ ràng trên tất cả các mô hình rằng Syntax Errors xảy ra ít thường xuyên hơn, cho thấy sự tuân thủ hợp lý với các quy tắc cú pháp của Python. Tuy nhiên, các mô hình như gpt-3.5-turbo thể hiện Runtime và Wrong Output errors đáng kể, đặc biệt trong các tập dữ liệu tập trung toán học như Euler và Euler Stories, cho thấy những thách thức trong việc giải quyết logic hoặc toán học các vấn đề mặc dù tính chính xác cú pháp. Một mô hình lỗi nhất quán được quan sát trên các mô hình như Chat-Bison, CodeChat-Bison, và Wizard, nơi Runtime Errors là chủ đạo, cho thấy những cuộc đấu tranh với các khía cạnh logic hoặc thuật toán của các vấn đề. Đáng chú ý, mô hình Wizard cho thấy một sự xuất hiện tương đối cao hơn của Syntax Errors trong các tập dữ liệu Euler và Euler Stories, gợi ý về những thách thức trong việc dịch các yếu tố toán học hoặc câu chuyện thành mã đúng.

### 4.3. Lập trình so với Kiến thức Thế giới

Trong thí nghiệm ablation này, chúng tôi so sánh hiệu suất của việc giải quyết các vấn đề Euler bằng mã và so sánh nó với các câu trả lời được thu thập trực tiếp thông qua các nhắc nhở. Kết quả được hiển thị trong Hình 3.

**Lập trình khó hơn trả lời.** Chúng tôi ban đầu nhận thấy rằng điểm số lập trình thấp hơn so với khi LLM tạo ra các câu trả lời. Trong các tình huống đơn giản nhất, hiệu suất lập trình giảm 20pp so với cài đặt trả lời. Phát hiện này cho thấy rằng GPT có thể hiệu quả phụ thuộc vào khả năng lý luận của nó. Ngoài ra, chúng tôi quan sát thấy rằng các nhiệm vụ lập trình hiếm khi được giải quyết khi mức độ khó đạt 30.

**Chain-of-Thought cải thiện hiệu suất.** Chúng tôi phát hiện ra rằng việc nhắc nhở GPT cung cấp lý do cho các giải pháp của nó tăng cường hiệu suất trên tất cả các mức độ khó. Trong cài đặt vấn đề đơn giản nhất, gpt-3.5-turbo giải quyết >80% vấn đề, và kỹ thuật này cũng cho phép nó giải quyết một số vấn đề thách thức hơn. So với trả lời mà không có chain-of-thought, việc sử dụng chain-of-thought dẫn đến một sự cải thiện đáng chú ý là +13.8 pp. trên mức độ khó 10, chẳng hạn.

### 4.4. Độ khó Vấn đề

Hình 4 hiển thị sự phát triển độ chính xác tổng thể các vấn đề cho các tập con AoC và Euler. Các vấn đề Euler được sắp xếp theo độ khó, trong khi các vấn đề AoC được sắp xếp theo năm và ngày. Đường biểu đồ màu xám mô tả trọng số khó trên mỗi vấn đề. Biểu đồ chứa độ chính xác tiến bộ qua Pass@3 và Pass@3-Difficulty. Trong đánh giá so sánh của gpt-3.5-turbo và codechat-bison, những khác biệt rõ ràng được quan sát trên các tập con Euler và AoC của tập dữ liệu.

Tập con Euler hiển thị một tiến trình có thể dự đoán hơn, với độ khó vấn đề tăng từ trái sang phải. Sự gia tăng có hệ thống này trong độ khó phù hợp với các xu hướng độ chính xác giảm của cả hai mô hình.

Các thử thách AoC cho thấy một mô hình tái diễn của các mức độ khó cho mỗi năm. Theo lịch sử, các vấn đề AoC có xu hướng bắt đầu dễ hơn ở đầu năm và trở nên thách thức hơn về cuối năm. Bản chất định kỳ của độ khó này được ghi lại trong nhiều đỉnh quan sát trong biểu đồ màu xám. Những đỉnh này đại diện cho những thách thức đáng chú ý khó hơn so với các vấn đề trước đó của chúng. Một phân phối phi tuyến như vậy có thể dẫn đến hiệu suất đa dạng từ các mô hình, nơi chúng có thể xuất sắc trong một số phân đoạn và đối mặt với những thách thức ở những phân đoạn khác, ngay cả trong một khoảng thời gian ngắn của các vấn đề.

Xem xét những sắc thái này, rõ ràng rằng một chỉ số đánh giá phù hợp cho tất cả, như độ chính xác thô, có thể không nắm bắt được những phức tạp của hiệu suất của một mô hình, đặc biệt trong các tập dữ liệu như AoC với độ khó không đồng nhất. Bằng cách kết hợp độ khó vấn đề như một yếu tố trọng số trong đánh giá, chúng ta có thể tính đến những khác biệt này và có được một cái nhìn cân bằng hơn về khả năng của một mô hình. Cách tiếp cận này cung cấp một đánh giá đại diện phản ánh các tình huống thế giới thực và nhấn mạnh tầm quan trọng của khả năng thích ứng và sức chịu đựng của mô hình trong việc giải quyết những thách thức có độ phức tạp khác nhau.

## 5. Công trình Liên quan

Một số bộ đánh giá đánh giá khả năng lập trình của các mô hình ngôn ngữ lớn tồn tại. Lu et al. (2021) phân loại những đánh giá này thành bốn nhóm chính: nhiệm vụ mã-to-mã (như dịch giữa các ngôn ngữ lập trình hoặc hoàn thành dòng lệnh), nhiệm vụ văn bản-to-văn bản (bao gồm dịch tài liệu), nhiệm vụ mã-to-văn bản (như tạo tài liệu cho một hàm), và nhiệm vụ văn bản-to-mã (liên quan đến hướng dẫn để tạo một hàm để giải quyết vấn đề).

Phần lớn các bộ đánh giá nổi tiếng chủ yếu tập trung vào các nhiệm vụ mã-to-mã hoặc văn bản-to-mã. Các bộ đánh giá như HumanEval (Chen et al., 2021), CoNaLa (Yin et al., 2018), MMLU (Hendrycks et al., 2021b), APPS (Hendrycks et al., 2021a), MBPP, MathQA (Austin et al., 2021), hoặc HumanEvalPack (Muennighoff et al., 2023) đạt được điều này bằng cách trình bày một tập hợp đa dạng các thách thức lập trình cơ bản. Ví dụ, HumanEval đánh giá tính chính xác chức năng của các chương trình được tạo ra từ docstrings và bao gồm 164 trường hợp, trong khi APPS bao gồm cả các vấn đề một dòng đơn giản và các thách thức thuật toán phức tạp. Đáng chú ý, những bộ đánh giá này thiếu các cài đặt đa lượt thực tế nơi một agent tham gia với hệ thống tạo sinh.

Mặt khác, các bộ đánh giá toán học khác nhau như GSM8K (Cobbe et al., 2021), và Rainbow (Lourie et al., 2021) tồn tại. Tuy nhiên, phạm vi toàn diện của các độ khó, từ cơ bản đến các mức kỹ năng chuyên nghiệp cao, làm cho Euler Project khác biệt. Độ rộng này cho phép một sự hiểu biết sắc thái về các nhiệm vụ cụ thể và các mức độ khó tương ứng của chúng mà các Large Language Models (LLM) hiện tại có thể xử lý hiệu quả.

Một loạt các bộ đánh giá tồn tại để đo lường khả năng đọc hiểu của các mô hình ngôn ngữ, bao gồm SQuAD (Rajpurkar et al., 2016), CoQA (Reddy et al., 2019), LogiQA (Liu et al., 2020) hoặc GLUE (Wang et al., 2019). Tuy nhiên, những bộ đánh giá này khác với phạm vi của tập dữ liệu của chúng tôi. Trọng tâm của chúng tôi tập trung vào việc giải quyết các nhiệm vụ phức tạp liên quan đến tạo mã, một kỹ năng có thể đạt được chỉ với một mức độ đọc hiểu cụ thể. Thông qua tập dữ liệu của chúng tôi, chúng tôi đóng góp những hiểu biết có giá trị vào việc tối ưu hóa các nhắc nhở mô hình ngôn ngữ. Cụ thể, chúng tôi so sánh hiệu suất mô hình giữa các vấn đề dựa trên câu chuyện và toán học thuần túy, tăng cường sự hiểu biết của chúng ta về việc tham gia hiệu quả các mô hình ngôn ngữ trong các bối cảnh thách thức.

## 6. Kết luận

Chúng tôi đã giới thiệu tập dữ liệu PECC như một công cụ để đánh giá khả năng tạo mã của các mô hình ngôn ngữ lớn trên một phổ của các độ phức tạp vấn đề, trải rộng cả bối cảnh câu chuyện và trung tính. Các đánh giá, bao gồm một tập hợp đa dạng các mô hình từ độc quyền đến mã nguồn mở và nội bộ, liên tục làm nổi bật những thách thức vốn có mà các mô hình đối mặt khi điều hướng những phức tạp của các nhiệm vụ lập trình, đặc biệt những nhiệm vụ được nhúng trong câu chuyện.

Một quan sát đáng kể từ nghiên cứu của chúng tôi liên quan đến công thức đánh giá. Hành động nhắc nhở bằng ngôn ngữ tự nhiên, kết hợp với cách tiếp cận đánh giá dựa trên trò chuyện, giới thiệu các phức tạp. Những phức tạp này vừa là kỹ thuật vừa là khái niệm, vì chúng phản ánh những thách thức thế giới thực nơi hướng dẫn và truy vấn có thể mơ hồ hoặc đa diện. Điều này nhấn mạnh nhu cầu về các mô hình có thể điều hướng một cách khéo léo những sắc thái của ngôn ngữ con người trong khi duy trì độ chính xác tính toán.

Phân tích so sánh của chúng tôi giữa các tập con vấn đề khác nhau nhấn mạnh tác động khác biệt của công thức vấn đề câu chuyện so với trung tính trên hiệu suất mô hình làm nổi bật tiềm năng để khám phá thêm trong lĩnh vực này. Đặc biệt, tập con Euler nổi bật, tiết lộ những khu vực đáng kể nơi các mô hình có thể cải thiện khả năng tạo mã khi đối mặt với các công thức vấn đề dựa trên toán học thách thức.

Quan trọng, phương pháp của chúng tôi đã được bắt nguồn trong một cách tiếp cận nhắc nhở zero-shot thiếu vòng lặp phản hồi. Chuyển đổi sang các phương pháp tinh vi hơn, như các cách tiếp cận chain-of-thought (Wei et al., 2023; Le et al., 2023) hoặc các reasoning-agents tích hợp công cụ (Gou et al., 2023), hứa hẹn để tăng cường đáng kể hiệu suất mô hình.

Phân tích của chúng tôi phát hiện ra một khoảng cách hiệu suất đáng chú ý giữa các mô hình thương mại và mã nguồn mở. Tập dữ liệu được giới thiệu tỏ ra có giá trị để đánh giá các mô hình trên các nhiệm vụ lập trình và lý luận, nhằm mục đích thúc đẩy đánh giá của LLM trong các thách thức toán học và lập trình phức tạp. Công trình này nhằm mục đích tăng cường nghiên cứu tương lai, mở rộng khả năng LLM trong các lĩnh vực thách thức.

**Xuất bản Tập dữ liệu.** PECC sẽ được xuất bản trên Github tại https://github.com/HallerPatrick/pecc và HuggingFace tại https://huggingface.co/datasets/PatrickHaller/pecc. Tập dữ liệu sẽ chứa mỗi vấn đề, các tệp đầu vào tương ứng, và các giải pháp mong đợi. Việc xuất bản sẽ không bao gồm các tập dữ liệu AoC gốc do hạn chế cấp phép. Thay vào đó, chúng tôi cung cấp một script tải để tải xuống các điểm dữ liệu liên quan.

## 7. Lời cảm ơn

Chúng tôi cảm ơn tất cả những người đánh giá vì những bình luận có giá trị của họ. Alan Akbik và Patrick Haller được hỗ trợ bởi Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) dưới khoản tài trợ Emmy Noether "Eidetic Representations of Natural Language" (số dự án 448414230). Alan Akbik hơn nữa được hỗ trợ dưới Germany's Excellence Strategy "Science of Intelligence" (EXC 2002/1, số dự án 390523135). Chúng tôi cảm ơn tất cả những người đánh giá vì những bình luận có giá trị của họ. Jonas Golde được hỗ trợ bởi German Federal Ministry of Economic Affairs and Climate Action (BMWK) như một phần của dự án ENA (KK5148001LB0).

## 8. Tài liệu tham khảo

[Tài liệu tham khảo đầy đủ được giữ nguyên như trong bản gốc]
