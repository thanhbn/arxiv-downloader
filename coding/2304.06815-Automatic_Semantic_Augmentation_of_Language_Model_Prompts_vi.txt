NgÃ´n ngá»¯ | ThÃ nh pháº§n Lá»i nháº¯c | BLEU-4
Java | Táº¤T Cáº¢ | 25.41
| -Repo. | 23.50
| -Id | 25.27
| -DFG | 24.86
Python | Táº¤T Cáº¢ | 24.26
| -Repo. | 22.80
| -Id | 23.93
| -DFG | 23.31

Báº£ng 7: NghiÃªn cá»©u ablation.

VÃ­ dá»¥ 1
```
def round(input_a, name: nil)
  check_allowed_types(input_a, TensorStream::Ops::FLOATING_POINT_TYPES)
  _op(:round, input_a, name: name)
end
```

Nháº­n xÃ©t VÃ ng & Ä‘áº§u ra mÃ´ hÃ¬nh | BLEU
VÃ ng | LÃ m trÃ²n cÃ¡c giÃ¡ trá»‹ cá»§a má»™t tensor Ä‘áº¿n sá»‘ nguyÃªn gáº§n nháº¥t theo tá»«ng pháº§n tá»­ | NA
BM25 | LÃ m trÃ²n má»™t tensor Ä‘áº¿n sá»‘ nguyÃªn gáº§n nháº¥t | 39
Ağ‘†ğ´ğ‘ƒ | LÃ m trÃ²n cÃ¡c giÃ¡ trá»‹ cá»§a má»™t tensor Ä‘áº¿n sá»‘ nguyÃªn gáº§n nháº¥t, theo tá»«ng pháº§n tá»­. | 74

VÃ­ dá»¥ 2
```java
public static void main(final String[] args) {
    loadPropertiesFiles(args);
    final ShutdownSignalBarrier barrier = new ShutdownSignalBarrier();
    final MediaDriver.Context ctx = new MediaDriver.Context();
    ctx.terminationHook(barrier::signal);
    try (MediaDriver ignore = MediaDriver.launch(ctx))
    {
        barrier.await();
        System.out.println("Shutdown Driver...");
    }
}
```

Nháº­n xÃ©t VÃ ng & Ä‘áº§u ra mÃ´ hÃ¬nh | BLEU
VÃ ng | Khá»Ÿi Ä‘á»™ng Media Driver nhÆ° má»™t tiáº¿n trÃ¬nh Ä‘á»™c láº­p. | NA
BM25 | PhÆ°Æ¡ng thá»©c chÃ­nh khá»Ÿi Ä‘á»™ng CLR Bridge tá»« Java. | 10
Ağ‘†ğ´ğ‘ƒ | PhÆ°Æ¡ng thá»©c chÃ­nh Ä‘á»ƒ cháº¡y Media Driver nhÆ° má»™t tiáº¿n trÃ¬nh Ä‘á»™c láº­p. | 33

Báº£ng 8: CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c chá»n, minh há»a tÃ­nh hiá»‡u quáº£ cá»§a viá»‡c tÄƒng cÆ°á»ng Ağ‘†ğ´ğ‘ƒ.

Liá»‡u MÃ´ HÃ¬nh CÃ³ Ghi Nhá»› ÄÆ°á»ng Dáº«n KhÃ´ng? Trong sá»‘ ba sá»± kiá»‡n ngá»¯ nghÄ©a mÃ  Ağ‘†ğ´ğ‘ƒ thÃªm vÃ o lá»i nháº¯c, thÃ´ng tin repo. tÃ¡c Ä‘á»™ng Ä‘áº¿n hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh nhiá»u nháº¥t. Äiá»u nÃ y cÃ³ thá»ƒ do thá»±c táº¿ lÃ  Code-Davinci-002 Ä‘Ã£ ghi nhá»› cÃ¡c Ä‘Æ°á»ng dáº«n tá»‡p cá»¥ thá»ƒ trong dá»¯ liá»‡u cá»§a chÃºng tÃ´i trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n; khi chÃºng tÃ´i cung cáº¥p Ä‘Æ°á»ng dáº«n Ä‘áº¿n hÃ m, cÃ³ láº½ mÃ´ hÃ¬nh chá»‰ nhá»› láº¡i thÃ´ng tin Ä‘Ã£ ghi nhá»›? Äá»ƒ Ä‘iá»u tra cÃ¢u há»i nÃ y, chÃºng tÃ´i thay Ä‘á»•i biá»ƒu diá»…n Ä‘Æ°á»ng dáº«n: chÃºng tÃ´i láº¥y tÃªn kho lÆ°u trá»¯ vÃ  Ä‘Æ°á»ng dáº«n, chia cÃ¡c token táº¡i "/", vÃ  trÃ¬nh bÃ y cho mÃ´ hÃ¬nh má»™t danh sÃ¡ch cÃ¡c token. Ã tÆ°á»Ÿng chÃ­nh Ä‘áº±ng sau phÆ°Æ¡ng phÃ¡p nÃ y lÃ  lÃ m khuáº¿ch tÃ¡n biá»ƒu diá»…n gá»‘c, vÃ  trÃ¬nh bÃ y cho mÃ´ hÃ¬nh thá»© gÃ¬ Ä‘Ã³ khÃ´ng gáº·p pháº£i trong quÃ¡ trÃ¬nh tiá»n huáº¥n luyá»‡n. Náº¿u mÃ´ hÃ¬nh khÃ´ng ghi nhá»› theo nghÄ©a Ä‘en, hiá»‡u suáº¥t cá»§a nÃ³ khÃ´ng nÃªn bá»‹ tÃ¡c Ä‘á»™ng. ChÃºng tÃ´i quan sÃ¡t ráº±ng sá»± khÃ¡c biá»‡t giá»¯a cáº£ hai phiÃªn báº£n ráº¥t nhá». Äá»‘i vá»›i Java, chÃºng tÃ´i Ä‘Æ°á»£c 0.24 BLEU nhÆ°ng, Ä‘á»‘i vá»›i Python, chÃºng tÃ´i máº¥t 0.04 vá»›i cÃ¡c Ä‘Æ°á»ng dáº«n Ä‘Æ°á»£c token hÃ³a. Äiá»u nÃ y cho tháº¥y rá»§i ro tháº¥p hÆ¡n ráº±ng mÃ´ hÃ¬nh Ä‘Ã£ ghi nhá»› Ä‘Æ°á»ng dáº«n Ä‘áº¿n hÃ m.

Liá»‡u Tháº» Äá»‹nh Danh CÃ³ Cáº§n Thiáº¿t KhÃ´ng? Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i gÃ¡n vai trÃ² cho cÃ¡c Ä‘á»‹nh danh vÃ  gáº¯n tháº» chÃºng nhÆ° TÃªn HÃ m, Tham Sá»‘, Äá»‹nh Danh v.v. trong lá»i nháº¯c (Xem HÃ¬nh 2). Liá»‡u viá»‡c gáº¯n tháº» rÃµ rÃ ng nÃ y thá»±c sá»± giÃºp hiá»‡u suáº¥t khÃ´ng? Äá»ƒ Ä‘iá»u tra cÃ¢u há»i nÃ y, chÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh khi Ä‘Æ°á»£c cung cáº¥p danh sÃ¡ch Ä‘á»‹nh danh "khÃ´ng cÃ³ tháº»" thuáº§n tÃºy. ChÃºng tÃ´i quan sÃ¡t ráº±ng cÃ¡c Ä‘á»‹nh danh Ä‘Æ°á»£c gáº¯n tháº» dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t hÆ¡n cho cáº£ Java vÃ  Python so vá»›i danh sÃ¡ch Ä‘á»‹nh danh khÃ´ng cÃ³ tháº» Ä‘Æ¡n giáº£n. Chá»‰ sá»‘ hiá»‡u suáº¥t BLEU cá»§a chÃºng tÃ´i tÄƒng 0.41 vÃ  1.22 cho Java vÃ  Python, tÆ°Æ¡ng á»©ng, cho tháº¥y ráº±ng thÃ´ng tin ngá»¯ nghÄ©a rÃµ rÃ ng thá»±c sá»± Ä‘Ã³ng gÃ³p vÃ o hiá»‡u suáº¥t mÃ´ hÃ¬nh tá»‘t hÆ¡n.

CÃ¡i GÃ¬ Tá»‘t HÆ¡n: Nhiá»u Shots HÆ¡n hay ASAP? Máº·c dÃ¹ cÃ³ hÃ ng tá»· tham sá»‘, LLM cÃ³ kÃ­ch thÆ°á»›c lá»i nháº¯c háº¡n cháº¿. VÃ­ dá»¥, code-davinci-002 vÃ  gpt-3.5-turbo há»— trá»£ cho phÃ©p Ä‘á»™ dÃ i lá»i nháº¯c chá»‰ 4k token. Viá»‡c tÄƒng cÆ°á»ng Ağ‘†ğ´ğ‘ƒ thá»±c sá»± tiÃªu thá»¥ má»™t pháº§n ngÃ¢n sÃ¡ch Ä‘á»™ dÃ i lá»i nháº¯c cÃ³ sáºµn! Do Ä‘Ã³ chÃºng ta cÃ³ hai lá»±a chá»n thiáº¿t káº¿: 1) sá»­ dá»¥ng Ã­t hÆ¡n, cÃ¡c máº«u Ä‘Æ°á»£c TÄƒng cÆ°á»ng Ağ‘†ğ´ğ‘ƒ trong lá»i nháº¯c hoáº·c 2) sá»­ dá»¥ng nhiá»u máº«u few-shot hÆ¡n mÃ  khÃ´ng cÃ³ tÄƒng cÆ°á»ng. Äá»ƒ Ä‘iá»u tra Ä‘iá»u nÃ y, chÃºng tÃ´i cÅ©ng thá»­ sá»­ dá»¥ng 4 vÃ  5 shots (thay vÃ¬ 3) cho Java vÃ  Python vá»›i mÃ´ hÃ¬nh code-davinci-002. Tuy nhiÃªn, Báº£ng 9 cho tháº¥y ráº±ng shots cao hÆ¡n sá»­ dá»¥ng BM25 khÃ´ng nháº¥t thiáº¿t dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t hÆ¡n. Vá»›i shots cao hÆ¡n, cÃ³ kháº£ nÄƒng giá»›i thiá»‡u cÃ¡c máº«u khÃ´ng liÃªn quan, cÃ³ thá»ƒ lÃ m tá»•n háº¡i mÃ´ hÃ¬nh thay vÃ¬ giÃºp Ä‘á»¡ nÃ³.

NgÃ´n ngá»¯ | Lá»i nháº¯c ÄÆ°á»£c TÄƒng cÆ°á»ng | Vanilla BM25
| #shots | BLEU-4 | #shots | BLEU-4
Java | 3 | 25.41 | 3 | 22.87
| | | 4 | 23.13
| | | 5 | 23.20
Python | 3 | 24.26 | 3 | 21.78
| | | 4 | 21.89
| | | 5 | 21.74

Báº£ng 9: So sÃ¡nh vá»›i Vanilla BM25 shots cao hÆ¡n.

Chá»‰ Ä‘á»‘i vá»›i Java chÃºng tÃ´i quan sÃ¡t hiá»‡u suáº¥t tá»‘t hÆ¡n vá»›i cáº£ 4 vÃ  5 shots so vá»›i mÃ´ hÃ¬nh baseline cá»§a chÃºng tÃ´i. Tuy nhiÃªn, ká»¹ thuáº­t Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i vá»›i chá»‰ 3-shots váº«n vÆ°á»£t trá»™i hÆ¡n viá»‡c sá»­ dá»¥ng BM25 vá»›i 5 shots. ÄÃ¡ng chÃº Ã½ ráº±ng cá»­a sá»• bá»‘i cáº£nh cá»§a mÃ´ hÃ¬nh Ä‘ang tÄƒng tá»«ng ngÃ y, vÃ  mÃ´ hÃ¬nh GPT-4 sáº¯p tá»›i sáº½ cho phÃ©p chÃºng ta cÃ³ tá»›i 32K token6. Do Ä‘Ã³, giá»›i háº¡n Ä‘á»™ dÃ i cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  váº¥n Ä‘á» trong tÆ°Æ¡ng lai gáº§n. Tuy nhiÃªn, nghiÃªn cá»©u cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng TÄƒng CÆ°á»ng Ngá»¯ NghÄ©a Tá»± Äá»™ng váº«n sáº½ lÃ  má»™t cÃ¡ch cÃ³ lá»£i Ä‘á»ƒ sá»­ dá»¥ng ngÃ¢n sÃ¡ch Ä‘á»™ dÃ i lá»i nháº¯c cÃ³ sáºµn; hÆ¡n ná»¯a, viá»‡c xÃ¢y dá»±ng cÃ¡c lá»i nháº¯c giÃ u tÃ­n hiá»‡u, thÃ´ng tin hÆ¡n sáº½ cÃ³ lá»£i báº¥t ká»ƒ Ä‘á»™ dÃ i.

CÃ¡i GÃ¬ Má»›i trong Äáº§u Ra cá»§a Ağ‘†ğ´ğ‘ƒ? ChÃºng tÃ´i thÃªm má»™t phÃ¢n tÃ­ch pro forma cá»§a má»™t vÃ i vÃ­ dá»¥ Ä‘Æ°á»£c chá»n thá»§ cÃ´ng, Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c nghi thá»©c cá»™ng Ä‘á»“ng yÃªu cáº§u Ä‘Ã¡nh giÃ¡ ngang hÃ ng; tuy nhiÃªn, nhá»¯ng phÃ¢n tÃ­ch nÃ y ráº¥t giai thoáº¡i vÃ  pháº£i Ä‘Æ°á»£c diá»…n giáº£i má»™t cÃ¡ch tháº­n trá»ng. ChÃºng tÃ´i kiá»ƒm tra thá»§ cÃ´ng má»™t sá»‘ máº«u Ä‘á»ƒ tháº£o luáº­n káº¿t quáº£ cá»§a chÃºng tÃ´i chi tiáº¿t hÆ¡n; cá»¥ thá»ƒ, Ä‘á»ƒ tráº£ lá»i ba cÃ¢u há»i: Ä‘á»ƒ chá»‰ rÃµ 1) cÃ¡c loáº¡i thÃ´ng tin má»›i mÃ  Ağ‘†ğ´ğ‘ƒ trÃ¬nh bÃ y cho LLM vÃ  2) cÃ¡ch tÃ³m táº¯t cá»§a Ağ‘†ğ´ğ‘ƒ khÃ¡c vá»›i nhá»¯ng tÃ³m táº¯t Ä‘Æ°á»£c táº¡o bá»Ÿi cÃ¡c ká»¹ thuáº­t hiá»‡n cÃ³, vÃ  3) Ä‘á»ƒ phÃ¢n tÃ­ch cÃ¡c lá»—i mÃ  Ağ‘†ğ´ğ‘ƒ giá»›i thiá»‡u. Báº£ng 11 trÃ¬nh bÃ y má»™t sá»‘ máº«u nÆ¡i, Ä‘á»‘i vá»›i ba máº«u Ä‘áº§u tiÃªn, Ağ‘†ğ´ğ‘ƒ hoáº¡t Ä‘á»™ng ráº¥t tá»‘t so vá»›i cÃ¡c baseline dá»±a trÃªn truy xuáº¥t cá»§a chÃºng tÃ´i, vÃ  Ä‘á»‘i vá»›i ba máº«u thá»© hai, baseline hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n Ağ‘†ğ´ğ‘ƒ. Trong khi chÃºng tÃ´i tháº£o luáº­n phÃ¡t hiá»‡n cá»§a mÃ¬nh trong bá»‘i cáº£nh cÃ¡c máº«u Ä‘Æ°á»£c cung cáº¥p, quan sÃ¡t cá»§a chÃºng tÃ´i khÃ¡i quÃ¡t hÃ³a cho cÃ¡c máº«u khÃ¡c.

CÃ¡c loáº¡i thÃ´ng tin má»›i mÃ  Ağ‘†ğ´ğ‘ƒ trÃ¬nh bÃ y cho LLM: NhÆ° Ä‘Ã£ tháº£o luáº­n trong bÃ i bÃ¡o, Ä‘Ã³ng gÃ³p chÃ­nh cá»§a chÃºng tÃ´i bao gá»“m viá»‡c tÄƒng cÆ°á»ng cÃ¡c máº«u Ä‘Ã£ truy xuáº¥t (Ä‘Æ°á»£c truy xuáº¥t báº±ng BM25, theo Nashid vÃ  cá»™ng sá»± [48]) vá»›i cÃ¡c sá»± kiá»‡n ngá»¯ nghÄ©a, dáº«n Ä‘áº¿n hiá»‡u suáº¥t cáº£i thiá»‡n so vá»›i phÆ°Æ¡ng phÃ¡p truy xuáº¥t cÆ¡ báº£n. ChÃºng tÃ´i thÃªm cÃ¡c sá»± kiá»‡n ngá»¯ nghÄ©a liÃªn quan Ä‘áº¿n chi tiáº¿t kho lÆ°u trá»¯, Ä‘á»‹nh danh, vÃ  Ä‘á»“ thá»‹ luá»“ng dá»¯ liá»‡u cho cáº£ máº«u Ä‘Ã£ truy xuáº¥t vÃ  mÃ£ Ä‘áº§u vÃ o. NhÆ° dá»± Ä‘oÃ¡n, cÃ¡c sá»± kiá»‡n ngá»¯ nghÄ©a Ä‘Æ°á»£c thÃªm vÃ o chuyá»ƒn thÃ nh, vÃ  tÄƒng cÆ°á»ng, Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh.

Trong máº«u Ä‘áº§u tiÃªn, phÆ°Æ¡ng phÃ¡p chá»‰ truy xuáº¥t baseline hoÃ n toÃ n khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c thuáº­t ngá»¯ "gradient". Tuy nhiÃªn, báº±ng cÃ¡ch káº¿t há»£p

6https://platform.openai.com/docs/models/gpt-4

--- TRANG 10 ---
ICSE '24, 14-20 thÃ¡ng 4, 2024, Lisbon, Bá»“ ÄÃ o Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, vÃ  Earl T. Barr

NgÃ´n ngá»¯ | BLEU-DC | | | | ROUGE-L | | | | METEOR | | | |
| BM25 | ASAP | Lá»£i Ã­ch (%) | p-value | BM25 | ASAP | Lá»£i Ã­ch (%) | p-value | BM25 | ASAP | Lá»£i Ã­ch (%) | p-value
Java | 14.09 | 15.94 | +13.13% | <0.01 | 36.85 | 38.41 | +4.23% | <0.01 | 35.66 | 36.10 | +1.23% | 0.32
Python | 12.63 | 14.49 | +14.73% | <0.01 | 35.32 | 37.74 | +6.85% | <0.01 | 33.05 | 35.63 | +7.81% | <0.01
Ruby | 9.16 | 11.01 | +20.2% | <0.01 | 28.19 | 30.55 | +8.37% | <0.01 | 27.65 | 29.20 | +5.61% | 0.03
JavaScript | 14.89 | 16.71 | +12.22% | <0.01 | 32.28 | 33.88 | +4.96% | <0.01 | 32.08 | 33.02 | +2.93% | 0.15
Go | 17.10 | 18.57 | +8.60% | <0.01 | 41.04 | 42.43 | +3.39% | <0.01 | 36.78 | 37.26 | +1.31% | 0.27
PHP | 16.97 | 20.63 | +21.57% | <0.01 | 40.48 | 44.90 | +10.92% | <0.01 | 40.14 | 43.35 | +8.00% | <0.01
Tá»•ng thá»ƒ | 14.14 | 16.23 | +14.78% | <0.01 | 35.69 | 37.99 | +6.44% | <0.01 | 34.23 | 35.76 | +4.47% | <0.01

Báº£ng 10: TÃ­nh hiá»‡u quáº£ cá»§a ASAP trong cÃ¡c chá»‰ sá»‘ tÃ³m táº¯t mÃ£ phá»• biáº¿n. p-values Ä‘Æ°á»£c tÃ­nh toÃ¡n Ã¡p dá»¥ng kiá»ƒm Ä‘á»‹nh cáº·p má»™t phÃ­a Wilcoxon signed-rank vÃ  Ä‘Æ°á»£c Ä‘iá»u chá»‰nh B-H.

Thay Ä‘á»•i (BLEU-4) | Tham chiáº¿u | BM25 | Ağ‘†ğ´ğ‘ƒ
Ağ‘†ğ´ğ‘ƒ (+47) | Táº¡o má»™t giÃ¡ trá»‹ nhiá»…u-gradient tá»« tá»a Ä‘á»™ cá»§a má»™t giÃ¡ trá»‹ Ä‘áº§u vÃ o ba chiá»u vÃ  tá»a Ä‘á»™ sá»‘ nguyÃªn cá»§a má»™t giÃ¡ trá»‹ ba chiá»u gáº§n Ä‘Ã³. | TÃ­nh toÃ¡n má»™t giÃ¡ trá»‹ táº¡i Ä‘iá»ƒm (x, y, z) trong hÃ m nhiá»…u Perlin 3D. | Táº¡o má»™t giÃ¡ trá»‹ nhiá»…u-gradient-nháº¥t quÃ¡n tá»« tá»a Ä‘á»™ cá»§a má»™t giÃ¡ trá»‹ Ä‘áº§u vÃ o ba chiá»u.
Ağ‘†ğ´ğ‘ƒ (+26) | Thay tháº¿ type báº±ng mapped type cho Ä‘Æ°á»ng dáº«n hiá»‡n táº¡i. | Tráº£ vá» mapped type cho Ä‘Æ°á»ng dáº«n hiá»‡n táº¡i, hoáº·c target type náº¿u khÃ´ng cÃ³ mapping nÃ o tá»“n táº¡i. | Thay tháº¿ target type báº±ng mapped type náº¿u cÃ³.
Ağ‘†ğ´ğ‘ƒ (+41) | Chuyá»ƒn Ä‘á»•i má»™t dataRootExt thÃ nh má»™t dataRoot | Chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‘i tÆ°á»£ng DataRoot. | Chuyá»ƒn Ä‘á»•i má»™t DataRootExt thÃ nh má»™t DataRoot.
Ağ‘†ğ´ğ‘ƒ (-36) | TÃ­nh toÃ¡n convex hull cá»§a táº­p há»£p cÃ¡c Ä‘iá»ƒm. | TÃ­nh toÃ¡n convex hull cá»§a táº­p há»£p cÃ¡c Ä‘iá»ƒm | TÃ­nh toÃ¡n convex hull cá»§a táº­p há»£p cÃ¡c Ä‘iá»ƒm báº±ng thuáº­t toÃ¡n chuá»—i Ä‘Æ¡n Ä‘iá»‡u cá»§a Andrew
Ağ‘†ğ´ğ‘ƒ (-35) | Getter cho Uninstall Actions. | Tráº£ vá» uninstall actions. | Truy xuáº¥t uninstall actions cho Ä‘Æ¡n vá»‹ triá»ƒn khai nÃ y.
Ağ‘†ğ´ğ‘ƒ (-67) | Láº¥y má»™t cá»™t cá»§a ma tráº­n nÃ y. | Láº¥y má»™t cá»™t cá»§a ma tráº­n nÃ y. | Tráº£ vá» cá»™t Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh cá»§a ma tráº­n nÃ y nhÆ° má»™t vector cá»™t.

Báº£ng 11: VÃ­ dá»¥ Hiá»ƒn thá»‹ Äiá»ƒm Máº¡nh vÃ  Äiá»ƒm Yáº¿u cá»§a Ağ‘†ğ´ğ‘ƒ.

cÃ¡c sá»± kiá»‡n ngá»¯ nghÄ©a, mÃ´ hÃ¬nh thÃ nh cÃ´ng khÃ´i phá»¥c thuáº­t ngá»¯ vÃ¬ nÃ³ thÆ°á»ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong cáº£ Ä‘á»‹nh danh vÃ  tÃªn kho lÆ°u trá»¯, áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh. Trong vÃ­ dá»¥ thá»© hai, nÆ¡i má»¥c tiÃªu lÃ  thay tháº¿ thay vÃ¬ Ä‘Æ¡n giáº£n tráº£ vá», baseline khÃ´ng táº¡o ra Ä‘Æ°á»£c thuáº­t ngá»¯ "replace", máº·c dÃ¹ cÃ³ chá»‰ bÃ¡o rÃµ rÃ ng trong tÃªn hÃ m ("replaceWithMappedTypeForPath"). Luá»“ng dá»¯ liá»‡u giá»¯a cÃ¡c Ä‘á»‹nh danh, Ä‘Æ°á»£c cung cáº¥p trong cÃ¡c sá»± kiá»‡n ngá»¯ nghÄ©a, cÃ³ thá»ƒ Ä‘Ã£ giÃºp mÃ´ hÃ¬nh nháº­n ra cÃ¡c thao tÃ¡c thay tháº¿.

CÃ¡ch tÃ³m táº¯t cá»§a Ağ‘†ğ´ğ‘ƒ khÃ¡c vá»›i nhá»¯ng tÃ³m táº¯t Ä‘Æ°á»£c táº¡o bá»Ÿi cÃ¡c ká»¹ thuáº­t hiá»‡n cÃ³: Theo tháº£o luáº­n trÃªn, chÃºng tÃ´i quan sÃ¡t ráº±ng Ağ‘†ğ´ğ‘ƒ Ä‘ang táº¡o ra thÃ´ng tin cá»¥ thá»ƒ hÆ¡n:
(1) NÃ³ xÃ¡c Ä‘á»‹nh "gradient" trong máº«u 1.
(2) NÃ³ Ä‘á» xuáº¥t thay Ä‘á»•i "return" thÃ nh "replace" trong má»™t máº«u khÃ¡c (máº«u 2).
(3) NÃ³ khuyáº¿n nghá»‹ thay Ä‘á»•i "dataroot" thÃ nh "datarootext" trong má»™t máº«u khÃ¡c (máº«u 3).

Nhá»¯ng khÃ¡c biá»‡t nÃ y Ä‘Æ°á»£c quan sÃ¡t qua nhiá»u máº«u khi so sÃ¡nh baseline cá»§a chÃºng tÃ´i vá»›i Ağ‘†ğ´ğ‘ƒ. PhÆ°Æ¡ng phÃ¡p Ağ‘†ğ´ğ‘ƒ liÃªn tá»¥c táº¡o ra thÃ´ng tin cá»¥ thá»ƒ hÆ¡n so vá»›i baseline.

PhÃ¢n tÃ­ch cÃ¡c lá»—i mÃ  Ağ‘†ğ´ğ‘ƒ giá»›i thiá»‡u: CÃ¡c vÃ­ dá»¥ Ä‘Æ°á»£c kiá»ƒm tra cho tháº¥y ráº±ng Ağ‘†ğ´ğ‘ƒ cÃ³ thá»ƒ trá»Ÿ nÃªn quÃ¡ cá»¥ thá»ƒ, vÃ  do Ä‘Ã³ khÃ´ng khá»›p vá»›i tÃ³m táº¯t Ä‘Æ°á»£c viáº¿t bá»Ÿi nhÃ  phÃ¡t triá»ƒn. Ağ‘†ğ´ğ‘ƒ trá»Ÿ nÃªn quÃ¡ cá»¥ thá»ƒ trong ba vÃ­ dá»¥ cuá»‘i cÃ¹ng vá»›i "thuáº­t toÃ¡n chuá»—i Ä‘Æ¡n Ä‘iá»‡u cá»§a Andrew" vÃ  "Ä‘Æ¡n vá»‹ triá»ƒn khai", "vector cá»™t". Máº·c dÃ¹ nhá»¯ng thuáº­t ngá»¯ nÃ y khÃ´ng nháº¥t thiáº¿t sai, BLEU-4 giáº£m, vÃ¬ tÃ³m táº¯t Ä‘Æ°á»£c viáº¿t bá»Ÿi nhÃ  phÃ¡t triá»ƒn tá»•ng quÃ¡t hÆ¡n.

ChÃºng tÃ´i cÅ©ng quan sÃ¡t Ä‘á»‹nh lÆ°á»£ng ráº±ng Ağ‘†ğ´ğ‘ƒ táº¡o ra thay Ä‘á»•i tÃ­ch cá»±c trong 44% máº«u. Tuy nhiÃªn, hiá»‡u suáº¥t cÅ©ng giáº£m cho 30% máº«u, vÃ  váº«n giá»¯ nguyÃªn trÃªn pháº§n cÃ²n láº¡i. So vá»›i baseline cá»§a chÃºng tÃ´i (há»c few-shot vá»›i máº«u Ä‘Æ°á»£c truy xuáº¥t BM25), Ağ‘†ğ´ğ‘ƒ Ä‘Ã²i há»i nhiá»u token hÆ¡n. Chi phÃ­ token bá»• sung, má»—i truy váº¥n (cáº£ vá» máº·t chi phÃ­ tiá»n tá»‡ vÃ  overhead hiá»‡u suáº¥t) khÃ¡ khiÃªm tá»‘n. Máº·t khÃ¡c, chÃºng tÃ´i quan sÃ¡t sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ 12% tá»•ng thá»ƒ vá»›i Ağ‘†ğ´ğ‘ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh Codex.

6 CÃ”NG VIá»†C LIÃŠN QUAN

6.1 TÃ³m Táº¯t MÃ£
CÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u Ä‘Ã£ thÃºc Ä‘áº©y hiá»‡n tráº¡ng trong cÃ¡c nhiá»‡m vá»¥ SE nhÆ° tÃ³m táº¯t mÃ£. MÃ´ hÃ¬nh LSTM cho tÃ³m táº¯t mÃ£ láº§n Ä‘áº§u tiÃªn Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Iyer vÃ  cá»™ng sá»± [33]. CÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn transformer tiá»n huáº¥n luyá»‡n [62] nhÆ° CodeBERT [21], PLBART [2], vÃ  CodeT5 [64] Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trÃªn bá»™ dá»¯ liá»‡u tÃ³m táº¯t mÃ£ CodeXGLUE [47], dáº«n Ä‘áº¿n nhá»¯ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ. Tuy nhiÃªn, cÃ³ má»™t lÆ°u Ã½ khi sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tiá»n huáº¥n luyá»‡n: máº·c dÃ¹ nhá»¯ng mÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng tá»‘t, fine-tuning rá»™ng rÃ£i lÃ  cáº§n thiáº¿t, cÃ³ thá»ƒ Ä‘Ã²i há»i nhiá»u dá»¯ liá»‡u & tá»‘n thá»i gian. NgoÃ i ra, cÃ¡c mÃ´ hÃ¬nh riÃªng biá»‡t pháº£i Ä‘Æ°á»£c huáº¥n luyá»‡n cho cÃ¡c ngÃ´n ngá»¯ khÃ¡c nhau, lÃ m tÄƒng chi phÃ­ huáº¥n luyá»‡n. Äá»ƒ giáº£m sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh cáº§n thiáº¿t, fine-tuning Ä‘a ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t, Ä‘á»ƒ duy trÃ¬ hoáº·c cáº£i thiá»‡n hiá»‡u suáº¥t trong khi giáº£m sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh xuá»‘ng má»™t [4]. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y khÃ´ng giáº£m thá»i gian huáº¥n luyá»‡n hoáº·c nhu cáº§u vá» dá»¯ liá»‡u cÃ³ nhÃ£n.

LLM, hoáº·c cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, lá»›n hÆ¡n nhiá»u so vá»›i nhá»¯ng mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n nÃ y, vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u lá»›n hÆ¡n nhiá»u vá»›i má»™t má»¥c tiÃªu huáº¥n luyá»‡n Ä‘Æ¡n giáº£n â€” dá»± Ä‘oÃ¡n token tiáº¿p theo tá»± há»“i quy [12]. Nhá»¯ng mÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng Ä‘Ã¡ng ngáº¡c nhiÃªn tá»‘t trÃªn cÃ¡c nhiá»‡m vá»¥, tháº­m chÃ­ khÃ´ng cáº§n fine-tuning. Chá»‰ cáº§n nháº¯c mÃ´ hÃ¬nh vá»›i nhá»¯ng cÃ¢u há»i khÃ¡c nhau, trong khi cung cáº¥p má»™t vÃ i máº«u váº¥n Ä‘á»-giáº£i phÃ¡p, lÃ  Ä‘á»§. Há»c few-shot Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng cho tÃ³m táº¯t mÃ£, vÃ  Ä‘Ã£ Ä‘Æ°á»£c tÃ¬m tháº¥y lÃ  cÃ³ lá»£i [3].

6.2 CÃ¡c Bá»™ Dá»¯ Liá»‡u KhÃ¡c
CÃ³ má»™t sá»‘ bá»™ dá»¯ liá»‡u cÃ³ sáºµn cho tÃ³m táº¯t mÃ£, ngoÃ i CodeXGLUE [47]. TL-CodeSum [30] lÃ  má»™t bá»™ dá»¯ liá»‡u tÆ°Æ¡ng Ä‘á»‘i nhá» hÆ¡n, vá»›i khoáº£ng 87K máº«u, nhÆ°ng nÃ³ bao gá»“m cÃ¡c báº£n sao, cÃ³ thá»ƒ dáº«n Ä‘áº¿n Æ°á»›c tÃ­nh hiá»‡u suáº¥t cao cÃ³ thá»ƒ khÃ´ng khÃ¡i quÃ¡t hÃ³a. Funcom [41] lÃ  má»™t bá»™ dá»¯ liá»‡u chuyÃªn dá»¥ng vá»›i 2.1 triá»‡u hÃ m Java, nhÆ°ng chá»©a cÃ¡c báº£n sao. ChÃºng tÃ´i chá»n CodeXGLUE (cÃ³ nguá»“n gá»‘c tá»« CodeSearchNet) vÃ¬ nÃ³ lÃ  má»™t bá»™ dá»¯ liá»‡u Ä‘a dáº¡ng, Ä‘a ngÃ´n ngá»¯ Ä‘Æ°a ra thÃ¡ch thá»©c cho cÃ¡c mÃ´ hÃ¬nh. Tháº­m chÃ­ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n tá»‘t nhÆ° CodeBERT cÅ©ng gáº·p khÃ³ khÄƒn trÃªn benchmark nÃ y; hiá»‡u suáº¥t cá»§a nÃ³ Ä‘áº·c biá»‡t kÃ©m trÃªn cÃ¡c ngÃ´n ngá»¯ cÃ³ Ã­t máº«u huáº¥n luyá»‡n hÆ¡n.

ÄÃ£ cÃ³ ráº¥t nhiá»u cÃ´ng viá»‡c vá» tÃ³m táº¯t mÃ£, tá»« khá»›p máº«u Ä‘áº¿n há»c few-shot. Nhá»¯ng mÃ´ hÃ¬nh nÃ y sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n vÃ  nguá»“n thÃ´ng tin khÃ¡c nhau Ä‘á»ƒ hoáº¡t Ä‘á»™ng tá»‘t trong tÃ³m táº¯t mÃ£. So sÃ¡nh hoáº·c tháº£o luáº­n táº¥t cáº£ nhá»¯ng mÃ´ hÃ¬nh nÃ y náº±m ngoÃ i pháº¡m vi cá»§a cÃ´ng viá»‡c nÃ y. Tuy nhiÃªn, chÃºng tÃ´i lÆ°u Ã½ ráº±ng con sá»‘ cá»§a chÃºng tÃ´i Ä‘áº¡i diá»‡n cho má»™t Ä‘iá»ƒm cao má»›i trÃªn benchmark CodeXGlue Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho tÃ³m táº¯t mÃ£ vÃ  hoÃ n thÃ nh mÃ£; chÃºng tÃ´i giá»›i thiá»‡u ngÆ°á»i Ä‘á»c Ä‘áº¿n https://microsoft.github.io/CodeXGLUE/ Ä‘á»ƒ xem nhanh báº£ng xáº¿p háº¡ng. Máº«u cá»§a chÃºng tÃ´i nhá» hÆ¡n (N=1000), nhÆ°ng cÃ¡c Æ°á»›c tÃ­nh, vÃ  Æ°á»›c tÃ­nh cáº£i thiá»‡n, máº¡nh máº½ vá» máº·t thá»‘ng kÃª (Xem tháº£o luáº­n kÃ­ch thÆ°á»›c máº«u trong Pháº§n 7).

6.3 LLM trong Ká»¹ thuáº­t Pháº§n má»m
Máº·c dÃ¹ LLM chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho tÃ³m táº¯t mÃ£, chÃºng Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho táº¡o mÃ£ [14, 49, 67] vÃ  sá»­a chÆ°Æ¡ng trÃ¬nh [5,18,35,36]. CÃ¡c mÃ´ hÃ¬nh nhÆ° Codex nháº±m giáº£m gÃ¡nh náº·ng cho cÃ¡c nhÃ  phÃ¡t triá»ƒn báº±ng cÃ¡ch tá»± Ä‘á»™ng táº¡o mÃ£ hoáº·c hoÃ n thÃ nh dÃ²ng. Má»™t sá»‘ mÃ´ hÃ¬nh nhÆ° Polycoder [67] vÃ  Codegen [49] hoáº¡t Ä‘á»™ng khÃ¡ tá»‘t, vÃ  do há»c few-shot hoáº·c prompting cá»§a chÃºng, chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho má»™t táº­p há»£p rá»™ng cÃ¡c váº¥n Ä‘á». Tuy nhiÃªn, mÃ´ hÃ¬nh Code-davinci-002 thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ vÃ  cho phÃ©p chÃºng tÃ´i vá»«a cÃ¡c lá»i nháº¯c Ä‘Æ°á»£c tÄƒng cÆ°á»ng cá»§a mÃ¬nh vÃ o má»™t cá»­a sá»• lá»›n hÆ¡n.

Jain vÃ  cá»™ng sá»± Ä‘Ã£ Ä‘á» xuáº¥t bá»• sung hoáº¡t Ä‘á»™ng LLM vá»›i cÃ¡c bÆ°á»›c xá»­ lÃ½ tiáº¿p theo dá»±a trÃªn ká»¹ thuáº­t phÃ¢n tÃ­ch vÃ  tá»•ng há»£p chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t trong táº¡o Ä‘oáº¡n chÆ°Æ¡ng trÃ¬nh [34]. BareiÃŸ vÃ  cá»™ng sá»± Ä‘Ã£ chá»‰ ra tÃ­nh hiá»‡u quáº£ cá»§a há»c few-shot trong Ä‘á»™t biáº¿n mÃ£, táº¡o oracle kiá»ƒm thá»­ tá»« tÃ i liá»‡u ngÃ´n ngá»¯ tá»± nhiÃªn, vÃ  cÃ¡c nhiá»‡m vá»¥ táº¡o trÆ°á»ng há»£p kiá»ƒm thá»­ [10]. CODAMOSA [42], má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn LLM, tiáº¿n hÃ nh kiá»ƒm thá»­ pháº§n má»m dá»±a trÃªn tÃ¬m kiáº¿m cho Ä‘áº¿n khi cÃ¡c cáº£i thiá»‡n coverage cá»§a nÃ³ ngÆ°ng trá»‡, sau Ä‘Ã³ yÃªu cáº§u LLM cung cáº¥p cÃ¡c trÆ°á»ng há»£p kiá»ƒm thá»­ vÃ­ dá»¥ cho cÃ¡c hÃ m khÃ´ng Ä‘Æ°á»£c bao phá»§. Báº±ng cÃ¡ch sá»­ dá»¥ng nhá»¯ng vÃ­ dá»¥ nÃ y, CODAMOSA giÃºp chuyá»ƒn hÆ°á»›ng kiá»ƒm thá»­ pháº§n má»m dá»±a trÃªn tÃ¬m kiáº¿m Ä‘áº¿n cÃ¡c khu vá»±c há»¯u Ã­ch hÆ¡n cá»§a khÃ´ng gian tÃ¬m kiáº¿m. Jiang vÃ  cá»™ng sá»± Ä‘Ã¡nh giÃ¡ tÃ­nh hiá»‡u quáº£ cá»§a LLM cho váº¥n Ä‘á» sá»­a chÆ°Æ¡ng trÃ¬nh [35].

Truy xuáº¥t vÃ  thÃªm má»™t táº­p há»£p cÃ¡c máº«u huáº¥n luyá»‡n Ä‘Ã£ Ä‘Æ°á»£c tÃ¬m tháº¥y lÃ  cÃ³ lá»£i cho nhiá»u nhiá»‡m vá»¥ phÃ¢n tÃ­ch ngá»¯ nghÄ©a trong NLP, tháº­m chÃ­ khÃ´ng sá»­ dá»¥ng LLM [68]. Má»™t háº¡n cháº¿ cá»§a phÆ°Æ¡ng phÃ¡p nÃ y lÃ  hiá»‡u suáº¥t cÃ³ thá»ƒ bá»‹ rÃ ng buá»™c bá»Ÿi sá»± sáºµn cÃ³ cá»§a cÃ¡c vÃ­ dá»¥ tÆ°Æ¡ng tá»±. Nashid vÃ  cá»™ng sá»± Ä‘Ã£ sá»­ dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng tá»± vÃ  Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cáº£i thiá»‡n trong sá»­a mÃ£ vÃ  táº¡o xÃ¡c nháº­n vá»›i sá»± há»— trá»£ cá»§a LLM [48]. Tuy nhiÃªn, khÃ´ng cÃ³ cÃ´ng viá»‡c nÃ o á»Ÿ trÃªn Ä‘Ã£ cá»‘ gáº¯ng tá»± Ä‘á»™ng tÄƒng cÆ°á»ng ngá»¯ nghÄ©a lá»i nháº¯c. LÆ°u Ã½ ráº±ng váº«n cÃ²n quÃ¡ sá»›m Ä‘á»ƒ bÃ¬nh luáº­n vá» kháº£ nÄƒng Ä‘áº§y Ä‘á»§ cá»§a nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nÃ y. CÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho Ä‘áº¿n nay cho tháº¥y ráº±ng viá»‡c tÄƒng cÆ°á»ng cÃ¡c máº«u trong lá»i nháº¯c vá»›i cÃ¡c gá»£i Ã½ ngá»¯ nghÄ©a giÃºp Ã­ch cho cÃ¡c nhiá»‡m vá»¥ tÃ³m táº¯t mÃ£ vÃ  hoÃ n thÃ nh mÃ£; viá»‡c Ä‘Ã¡nh giÃ¡ giÃ¡ trá»‹ cá»§a Ağ‘†ğ´ğ‘ƒ trong cÃ¡c nhiá»‡m vá»¥ khÃ¡c Ä‘Æ°á»£c Ä‘á»ƒ láº¡i cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

7 Má»I ÄE Dá»ŒA & Háº N CHáº¾
Má»™t má»‘i quan tÃ¢m lá»›n khi lÃ m viá»‡c vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n lÃ  kháº£ nÄƒng phÆ¡i bÃ y dá»¯ liá»‡u kiá»ƒm tra trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. ÄÃ¡ng tiáº¿c, ngÆ°á»i ta khÃ´ng thá»ƒ kiá»ƒm tra trá»±c tiáº¿p Ä‘iá»u nÃ y vÃ¬ bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n khÃ´ng thá»ƒ truy cáº­p. Hiá»‡u suáº¥t tháº¥p hÆ¡n cá»§a mÃ´ hÃ¬nh vá»›i few-shotting ngáº«u nhiÃªn cho tháº¥y ráº±ng ghi nhá»› cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  má»™t yáº¿u tá»‘ chÃ­nh. Khi chÃºng tÃ´i káº¿t há»£p thÃ´ng tin liÃªn quan, hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh cáº£i thiá»‡n vá»›i lÆ°á»£ng vÃ  cháº¥t lÆ°á»£ng thÃ´ng tin. Náº¿u mÃ´ hÃ¬nh Ä‘Ã£ ghi nhá»› cÃ¡c tÃ³m táº¯t, nÃ³ cÃ³ thá»ƒ Ä‘Ã£ ghi Ä‘iá»ƒm cao hÆ¡n nhiá»u, tháº­m chÃ­ khÃ´ng cÃ³ lá»£i Ã­ch cá»§a cÃ¡c máº«u liÃªn quan vÃ  tÄƒng cÆ°á»ng ngá»¯ nghÄ©a.

PhÃ¢n TÃ­ch KÃ­ch ThÆ°á»›c Máº«u: ChÃºng tÃ´i sá»­ dá»¥ng trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n quan sÃ¡t Ä‘á»ƒ tÃ­nh toÃ¡n (sá»­ dá»¥ng G*power [19,20]) kÃ­ch thÆ°á»›c máº«u cáº§n thiáº¿t, sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng: ğ›¼ lÃ  0.01 (p-value mong muá»‘n) vÃ  ğ›½ lÃ  0.20 (tá»©c lÃ , 20% cÆ¡ há»™i KHÃ”NG khÃ¡m phÃ¡ hiá»‡u á»©ng, náº¿u cÃ³ má»™t hiá»‡u á»©ng tá»“n táº¡i). Äá»‘i vá»›i cÃ¡c kiá»ƒm Ä‘á»‹nh mÃ  chÃºng tÃ´i sá»­ dá»¥ng (kiá»ƒm Ä‘á»‹nh Wilcoxon Signed-rank), chÃºng tÃ´i tháº¥y ráº±ng kÃ­ch thÆ°á»›c máº«u cáº§n thiáº¿t luÃ´n dÆ°á»›i kÃ­ch thÆ°á»›c máº«u mÃ  chÃºng tÃ´i sá»­ dá»¥ng cho cÃ¡c nghiÃªn cá»©u chÃ­nh cá»§a mÃ¬nh, tá»©c lÃ , 1000.

NghiÃªn Cá»©u NgÆ°á»i DÃ¹ng: ChÃºng tÃ´i khÃ´ng tiáº¿n hÃ nh nghiÃªn cá»©u ngÆ°á»i dÃ¹ng cho Ağ‘†ğ´ğ‘ƒ. Do Ä‘Ã³, nhá»¯ng cáº£i thiá»‡n trong cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c trÃ¬nh bÃ y á»Ÿ Ä‘Ã¢y cÃ³ thá»ƒ khÃ´ng nháº¥t thiáº¿t chuyá»ƒn thÃ nh hiá»‡u suáº¥t nhÃ  phÃ¡t triá»ƒn cáº£i thiá»‡n. KhÃ­a cáº¡nh nÃ y Ä‘Æ°á»£c Ä‘á»ƒ láº¡i cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.

Cuá»‘i cÃ¹ng: fine-tuning cÃ¡c LM lá»›n Ä‘á»ƒ sá»­ dá»¥ng cÃ¡c sá»± kiá»‡n ngá»¯ nghÄ©a cÃ³ nguá»“n gá»‘c cÃ³ thá»ƒ cáº£i thiá»‡n so vá»›i phÆ°Æ¡ng phÃ¡p prompting Ä‘Æ°á»£c tÄƒng cÆ°á»ng cá»§a chÃºng tÃ´i, nhÆ°ng sáº½ tá»‘n kÃ©m. ChÃºng tÃ´i sáº½ Ä‘á»ƒ viá»‡c xem xÃ©t nÃ³ cho nghiÃªn cá»©u tÆ°Æ¡ng lai.

8 Káº¾T LUáº¬N
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘Ã£ khÃ¡m phÃ¡ Ã½ tÆ°á»Ÿng TÄƒng CÆ°á»ng Ngá»¯ NghÄ©a Tá»± Äá»™ng cho CÃ¡c Lá»i Nháº¯c, theo Ä‘Ã³ chÃºng tÃ´i Ä‘á» xuáº¥t tÄƒng cÆ°á»ng cÃ¡c máº«u few-shot trong lá»i nháº¯c LLM, vá»›i cÃ¡c sá»± kiá»‡n Ä‘Æ°á»£c gáº¯n tháº» tá»± Ä‘á»™ng cÃ³ nguá»“n gá»‘c báº±ng phÃ¢n tÃ­ch ngá»¯ nghÄ©a. Äiá»u nÃ y dá»±a trÃªn trá»±c giÃ¡c ráº±ng cÃ¡c nhÃ  phÃ¡t triá»ƒn con ngÆ°á»i thÆ°á»ng quÃ©t mÃ£ Ä‘á»ƒ ngáº§m trÃ­ch xuáº¥t nhá»¯ng sá»± kiá»‡n nhÆ° váº­y trong quÃ¡ trÃ¬nh hiá»ƒu mÃ£ dáº«n Ä‘áº¿n viáº¿t má»™t tÃ³m táº¯t tá»‘t. Máº·c dÃ¹ cÃ³ thá»ƒ hÃ¬nh dung ráº±ng LLM cÃ³ thá»ƒ ngáº§m suy ra nhá»¯ng sá»± kiá»‡n nhÆ° váº­y cho chÃ­nh chÃºng, chÃºng tÃ´i Ä‘oÃ¡n ráº±ng viá»‡c thÃªm nhá»¯ng sá»± kiá»‡n nÃ y theo phong cÃ¡ch Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng vÃ o cÃ¡c máº«u vÃ  má»¥c tiÃªu, trong lá»i nháº¯c, sáº½ giÃºp LLM tá»• chá»©c "chuá»—i suy nghÄ©" cá»§a nÃ³ khi nÃ³ tÃ¬m cÃ¡ch xÃ¢y dá»±ng má»™t tÃ³m táº¯t. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ã½ tÆ°á»Ÿng nÃ y trÃªn má»™t bá»™ dá»¯ liá»‡u CodeSearchNet Ä‘áº§y thÃ¡ch thá»©c, Ä‘Æ°á»£c khá»­ trÃ¹ng, Ä‘Æ°á»£c tuyá»ƒn chá»n ká»¹ lÆ°á»¡ng, trÃªn hai nhiá»‡m vá»¥: tÃ³m táº¯t mÃ£ vÃ  hoÃ n thÃ nh mÃ£. PhÃ¡t hiá»‡n cá»§a chÃºng tÃ´i chá»‰ ra ráº±ng TÄƒng CÆ°á»ng Ngá»¯ NghÄ©a Tá»± Äá»™ng cho CÃ¡c Lá»i Nháº¯c thÆ°á»ng há»¯u Ã­ch. Æ¯á»›c tÃ­nh cá»§a chÃºng tÃ´i cho tháº¥y nÃ³ giÃºp vÆ°á»£t qua hiá»‡n tráº¡ng.

Lá»i Cáº£m Æ n: ChÃºng tÃ´i muá»‘n cáº£m Æ¡n Quá»¹ Khoa há»c Quá»‘c gia dÆ°á»›i Grant NSF CCF (SHF-MEDIUM) Sá»‘ 2107592 vÃ  CÆ¡ quan Dá»± Ã¡n NghiÃªn cá»©u TiÃªn tiáº¿n Intelligence (IARPA) dÆ°á»›i há»£p Ä‘á»“ng W911NF20C0038 cho sá»± há»— trá»£ má»™t pháº§n cá»§a cÃ´ng viá»‡c nÃ y. Káº¿t luáº­n cá»§a chÃºng tÃ´i khÃ´ng nháº¥t thiáº¿t pháº£n Ã¡nh vá»‹ trÃ­ hoáº·c chÃ­nh sÃ¡ch cá»§a cÃ¡c nhÃ  tÃ i trá»£ vÃ  khÃ´ng nÃªn suy ra sá»± cháº¥p thuáº­n chÃ­nh thá»©c nÃ o.

TÃ€I LIá»†U THAM KHáº¢O
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, vÃ  Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4998â€“5007.

--- TRANG 12 ---
ICSE '24, 14-20 thÃ¡ng 4, 2024, Lisbon, Bá»“ ÄÃ o Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, vÃ  Earl T. Barr

[2] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, vÃ  Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2655â€“2668.

[3] Toufique Ahmed vÃ  Premkumar Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization. Trong 37th IEEE/ACM International Conference on Automated Software Engineering. 1â€“5.

[4] Toufique Ahmed vÃ  Premkumar Devanbu. 2022. Multilingual training for software engineering. Trong Proceedings of the 44th International Conference on Software Engineering. 1443â€“1455.

[5] Toufique Ahmed vÃ  Premkumar Devanbu. 2023. Better patching using LLM prompting, via Self-Consistency. Trong 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1742â€“1746.

[6] Toufique Ahmed, Supriyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao Zhang, vÃ  Saravan Rajmohan. 2023. Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models. ICSE (2023).

[7] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. Trong Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. 143â€“153.

[8] Uri Alon, Shaked Brody, Omer Levy, vÃ  Eran Yahav. 2018. code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv:1808.01400 (2018).

[9] Satanjeev Banerjee vÃ  Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Trong Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65â€“72.

[10] Patrick BareiÃŸ, Beatriz Souza, Marcelo d'Amorim, vÃ  Michael Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335 (2022).

[11] Lionel C Briand. 2003. Software documentation: how much is enough?. Trong Seventh European Conference on Software Maintenance and Reengineering, 2003. Proceedings. IEEE, 13â€“15.

[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, vÃ  cÃ¡c cá»™ng sá»±. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877â€“1901.

[13] Boxing Chen vÃ  Colin Cherry. 2014. A systematic comparison of smoothing techniques for sentence-level BLEU. Trong Proceedings of the ninth workshop on statistical machine translation. 362â€“367.

[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, vÃ  cÃ¡c cá»™ng sá»±. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, vÃ  Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[16] Samanta Dey, Venkatesh Vinayakarao, Monika Gupta, vÃ  Sampath Dechu. 2022. Evaluating commit message generation: to BLEU or not to BLEU?. Trong Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. 31â€“35.

[17] Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, vÃ  Jeffrey C Carver. 2013. Evaluating source code summarization techniques: Replication and expansion. Trong 2013 21st International Conference on Program Comprehension (ICPC). IEEE, 13â€“22.

[18] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, vÃ  Shin Hwei Tan. 2022. Automated Repair of Programs from Large Language Models. ICSE.

[19] Franz Faul, Edgar Erdfelder, Axel Buchner, vÃ  Albert-Georg Lang. 2009. Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses. Behavior research methods 41, 4 (2009), 1149â€“1160.

[20] Franz Faul, Edgar Erdfelder, Albert-Georg Lang, vÃ  Axel Buchner. 2007. G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods 39, 2 (2007), 175â€“191.

[21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, vÃ  cÃ¡c cá»™ng sá»±. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 1536â€“1547.

[22] Andrew Forward vÃ  Timothy C Lethbridge. 2002. The relevance of software documentation, tools and technologies: a survey. Trong Proceedings of the 2002 ACM symposium on Document engineering. 26â€“33.

[23] Jianfeng Gao vÃ  Xiaodong He. 2013. Training MRF-based phrase translation models using gradient ascent. Trong Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 450â€“459.

[24] David Gros, Hariharan Sezhiyan, Prem Devanbu, vÃ  Zhou Yu. 2020. Code to Comment ?Translation?: Data, Metrics, Baselining & Evaluation. Trong 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 746â€“757.

[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, vÃ  cÃ¡c cá»™ng sá»±. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. Trong International Conference on Learning Representations.

[26] Sonia Haiduc, Jairo Aponte, vÃ  Andrian Marcus. 2010. Supporting program comprehension with source code summarization. Trong Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 2. 223â€“226.

[27] Sonia Haiduc, Jairo Aponte, Laura Moreno, vÃ  Andrian Marcus. 2010. On the use of automated text summarization techniques for summarizing source code. Trong 2010 17th Working Conference on Reverse Engineering. IEEE, 35â€“44.

[28] Sakib Haque, Zachary Eberhart, Aakash Bansal, vÃ  Collin McMillan. 2022. Semantic similarity metrics for evaluating source code summarization. Trong Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. 36â€“47.

[29] Xing Hu, Ge Li, Xin Xia, David Lo, vÃ  Zhi Jin. 2018. Deep code comment generation. Trong Proceedings of the 26th conference on program comprehension. 200â€“210.

[30] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, vÃ  Zhi Jin. 2018. Summarizing source code with transferred API knowledge. Trong Proceedings of the 27th International Joint Conference on Artificial Intelligence. 2269â€“2275.

[31] Jie Huang vÃ  Kevin Chen-Chuan Chang. 2022. Towards Reasoning in Large Language Models: A Survey. arXiv preprint arXiv:2212.10403 (2022).

[32] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, vÃ  Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[33] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, vÃ  Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. Trong Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2073â€“2083.

[34] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, vÃ  Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. Trong Proceedings, 44th ICSE. 1219â€“1231.

[35] Nan Jiang, Kevin Liu, Thibaud Lutellier, vÃ  Lin Tan. 2023. Impact of Code Language Models on Automated Program Repair. ICSE (2023).

[36] Harshit Joshi, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, vÃ  Gust Verbruggen. 2022. Repair is nearly generation: Multilingual program repair with llms. arXiv preprint arXiv:2208.11640 (2022).

[37] Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, vÃ  Mohamed Coulibali. 2020. NUBIA: NeUral Based Interchangeability Assessor for Text Generation. arXiv:2004.14667 [cs.CL]

[38] Sungmin Kang, Juyeon Yoon, vÃ  Shin Yoo. 2023. Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction. ICSE (2023).

[39] Jan KocoÅ„, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika SzydÅ‚o, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, vÃ  cÃ¡c cá»™ng sá»±. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.

[40] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, vÃ  Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).

[41] Alexander LeClair, Siyuan Jiang, vÃ  Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. Trong 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 795â€“806.

[42] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, vÃ  Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models. Trong 45th International Conference on Software Engineering, ser. ICSE.

[43] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Trong Text summarization branches out. 74â€“81.

[44] Chin-Yew Lin vÃ  Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. Trong COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. 501â€“507.

[45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, vÃ  Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).

[46] Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny, Hitesh Sajnani, vÃ  Jan Vitek. 2017. DÃ©jÃ Vu: a map of code duplicates on GitHub. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1â€“28.

[47] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, vÃ  cÃ¡c cá»™ng sá»±. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).

[48] Noor Nashid, Mifta Sintaha, vÃ  Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. Trong Proceedings, 45th ICSE.

[49] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, vÃ  Caiming Xiong. 2022. Codegen: An open large language

--- TRANG 13 ---
TÄƒng CÆ°á»ng Ngá»¯ NghÄ©a Tá»± Äá»™ng cho CÃ¡c Lá»i Nháº¯c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
(Ä‘á»ƒ TÃ³m Táº¯t MÃ£) ICSE '24, 14-20 thÃ¡ng 4, 2024, Lisbon, Bá»“ ÄÃ o Nha

model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022).

[50] Kishore Papineni, Salim Roukos, Todd Ward, vÃ  Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311â€“318.

[51] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, vÃ  Huajun Chen. 2022. Reasoning with Language Model Prompting: A Survey. arXiv preprint arXiv:2212.09597 (2022).

[52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, vÃ  cÃ¡c cá»™ng sá»±. 2018. Improving language understanding by generative pre-training. (2018).

[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, vÃ  cÃ¡c cá»™ng sá»±. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

[54] Juan Ramos vÃ  cÃ¡c cá»™ng sá»±. 2003. Using tf-idf to determine word relevance in document queries. Trong Proceedings of the first instructional conference on machine learning, Vol. 242. Citeseer, 29â€“48.

[55] Stephen Robertson, Hugo Zaragoza, vÃ  cÃ¡c cá»™ng sá»±. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends Â®in Information Retrieval 3, 4 (2009), 333â€“389.

[56] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, vÃ  Sidney D'Mello. 2014. Improving automated source code summarization via an eye-tracking study of programmers. Trong Proceedings of the 36th international conference on Software engineering. 390â€“401.

[57] Devjeet Roy, Sarah Fakhoury, vÃ  Venera Arnaoudova. 2021. Reassessing automatic evaluation metrics for code summarization tasks. Trong Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1105â€“1116.

[58] Thibault Sellam, Dipanjan Das, vÃ  Ankur P Parikh. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696 (2020).

[59] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, vÃ  Hongbin Sun. 2023. On the evaluation of neural code summarization. Trong Proceedings of the 44th International Conference on Software Engineering. 1597â€“1608.

[60] Disha Shrivastava, Hugo Larochelle, vÃ  Daniel Tarlow. 2022. Repository-level prompt generation for large language models of code. arXiv preprint arXiv:2206.12839 (2022).

[61] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, vÃ  K Vijay-Shanker. 2010. Towards automatically generating summary comments for java methods. Trong Proceedings of the IEEE/ACM international conference on Automated software engineering. 43â€“52.

[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, vÃ  Illia Polosukhin. 2017. Attention is all you need. Trong Advances in neural information processing systems. 5998â€“6008.

[63] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, vÃ  Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).

[64] Yue Wang, Weishi Wang, Shafiq Joty, vÃ  Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696â€“8708.

[65] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, vÃ  Zhi Jin. 2019. Code generation as a dual task of code summarization. Advances in neural information processing systems 32 (2019).

[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, vÃ  Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).

[67] Frank F Xu, Uri Alon, Graham Neubig, vÃ  Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. Trong Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1â€“10.

[68] Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, vÃ  Fei Sha. 2022. Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing. arXiv preprint arXiv:2209.14899 (2022).

[69] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, vÃ  Xudong Liu. 2020. Retrieval-based neural source code summarization. Trong Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1385â€“1397.

[70] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, vÃ  Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).
