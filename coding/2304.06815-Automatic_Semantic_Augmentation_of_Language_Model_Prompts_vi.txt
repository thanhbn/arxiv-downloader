Ngôn ngữ | Thành phần Lời nhắc | BLEU-4
Java | TẤT CẢ | 25.41
| -Repo. | 23.50
| -Id | 25.27
| -DFG | 24.86
Python | TẤT CẢ | 24.26
| -Repo. | 22.80
| -Id | 23.93
| -DFG | 23.31

Bảng 7: Nghiên cứu ablation.

Ví dụ 1
```
def round(input_a, name: nil)
  check_allowed_types(input_a, TensorStream::Ops::FLOATING_POINT_TYPES)
  _op(:round, input_a, name: name)
end
```

Nhận xét Vàng & đầu ra mô hình | BLEU
Vàng | Làm tròn các giá trị của một tensor đến số nguyên gần nhất theo từng phần tử | NA
BM25 | Làm tròn một tensor đến số nguyên gần nhất | 39
A𝑆𝐴𝑃 | Làm tròn các giá trị của một tensor đến số nguyên gần nhất, theo từng phần tử. | 74

Ví dụ 2
```java
public static void main(final String[] args) {
    loadPropertiesFiles(args);
    final ShutdownSignalBarrier barrier = new ShutdownSignalBarrier();
    final MediaDriver.Context ctx = new MediaDriver.Context();
    ctx.terminationHook(barrier::signal);
    try (MediaDriver ignore = MediaDriver.launch(ctx))
    {
        barrier.await();
        System.out.println("Shutdown Driver...");
    }
}
```

Nhận xét Vàng & đầu ra mô hình | BLEU
Vàng | Khởi động Media Driver như một tiến trình độc lập. | NA
BM25 | Phương thức chính khởi động CLR Bridge từ Java. | 10
A𝑆𝐴𝑃 | Phương thức chính để chạy Media Driver như một tiến trình độc lập. | 33

Bảng 8: Các ví dụ được chọn, minh họa tính hiệu quả của việc tăng cường A𝑆𝐴𝑃.

Liệu Mô Hình Có Ghi Nhớ Đường Dẫn Không? Trong số ba sự kiện ngữ nghĩa mà A𝑆𝐴𝑃 thêm vào lời nhắc, thông tin repo. tác động đến hiệu suất của mô hình nhiều nhất. Điều này có thể do thực tế là Code-Davinci-002 đã ghi nhớ các đường dẫn tệp cụ thể trong dữ liệu của chúng tôi trong quá trình tiền huấn luyện; khi chúng tôi cung cấp đường dẫn đến hàm, có lẽ mô hình chỉ nhớ lại thông tin đã ghi nhớ? Để điều tra câu hỏi này, chúng tôi thay đổi biểu diễn đường dẫn: chúng tôi lấy tên kho lưu trữ và đường dẫn, chia các token tại "/", và trình bày cho mô hình một danh sách các token. Ý tưởng chính đằng sau phương pháp này là làm khuếch tán biểu diễn gốc, và trình bày cho mô hình thứ gì đó không gặp phải trong quá trình tiền huấn luyện. Nếu mô hình không ghi nhớ theo nghĩa đen, hiệu suất của nó không nên bị tác động. Chúng tôi quan sát rằng sự khác biệt giữa cả hai phiên bản rất nhỏ. Đối với Java, chúng tôi được 0.24 BLEU nhưng, đối với Python, chúng tôi mất 0.04 với các đường dẫn được token hóa. Điều này cho thấy rủi ro thấp hơn rằng mô hình đã ghi nhớ đường dẫn đến hàm.

Liệu Thẻ Định Danh Có Cần Thiết Không? Trong bài báo này, chúng tôi gán vai trò cho các định danh và gắn thẻ chúng như Tên Hàm, Tham Số, Định Danh v.v. trong lời nhắc (Xem Hình 2). Liệu việc gắn thẻ rõ ràng này thực sự giúp hiệu suất không? Để điều tra câu hỏi này, chúng tôi so sánh hiệu suất của mô hình khi được cung cấp danh sách định danh "không có thẻ" thuần túy. Chúng tôi quan sát rằng các định danh được gắn thẻ dẫn đến hiệu suất tốt hơn cho cả Java và Python so với danh sách định danh không có thẻ đơn giản. Chỉ số hiệu suất BLEU của chúng tôi tăng 0.41 và 1.22 cho Java và Python, tương ứng, cho thấy rằng thông tin ngữ nghĩa rõ ràng thực sự đóng góp vào hiệu suất mô hình tốt hơn.

Cái Gì Tốt Hơn: Nhiều Shots Hơn hay ASAP? Mặc dù có hàng tỷ tham số, LLM có kích thước lời nhắc hạn chế. Ví dụ, code-davinci-002 và gpt-3.5-turbo hỗ trợ cho phép độ dài lời nhắc chỉ 4k token. Việc tăng cường A𝑆𝐴𝑃 thực sự tiêu thụ một phần ngân sách độ dài lời nhắc có sẵn! Do đó chúng ta có hai lựa chọn thiết kế: 1) sử dụng ít hơn, các mẫu được Tăng cường A𝑆𝐴𝑃 trong lời nhắc hoặc 2) sử dụng nhiều mẫu few-shot hơn mà không có tăng cường. Để điều tra điều này, chúng tôi cũng thử sử dụng 4 và 5 shots (thay vì 3) cho Java và Python với mô hình code-davinci-002. Tuy nhiên, Bảng 9 cho thấy rằng shots cao hơn sử dụng BM25 không nhất thiết dẫn đến hiệu suất tốt hơn. Với shots cao hơn, có khả năng giới thiệu các mẫu không liên quan, có thể làm tổn hại mô hình thay vì giúp đỡ nó.

Ngôn ngữ | Lời nhắc Được Tăng cường | Vanilla BM25
| #shots | BLEU-4 | #shots | BLEU-4
Java | 3 | 25.41 | 3 | 22.87
| | | 4 | 23.13
| | | 5 | 23.20
Python | 3 | 24.26 | 3 | 21.78
| | | 4 | 21.89
| | | 5 | 21.74

Bảng 9: So sánh với Vanilla BM25 shots cao hơn.

Chỉ đối với Java chúng tôi quan sát hiệu suất tốt hơn với cả 4 và 5 shots so với mô hình baseline của chúng tôi. Tuy nhiên, kỹ thuật đề xuất của chúng tôi với chỉ 3-shots vẫn vượt trội hơn việc sử dụng BM25 với 5 shots. Đáng chú ý rằng cửa sổ bối cảnh của mô hình đang tăng từng ngày, và mô hình GPT-4 sắp tới sẽ cho phép chúng ta có tới 32K token6. Do đó, giới hạn độ dài có thể không phải là vấn đề trong tương lai gần. Tuy nhiên, nghiên cứu của chúng tôi cho thấy rằng Tăng Cường Ngữ Nghĩa Tự Động vẫn sẽ là một cách có lợi để sử dụng ngân sách độ dài lời nhắc có sẵn; hơn nữa, việc xây dựng các lời nhắc giàu tín hiệu, thông tin hơn sẽ có lợi bất kể độ dài.

Cái Gì Mới trong Đầu Ra của A𝑆𝐴𝑃? Chúng tôi thêm một phân tích pro forma của một vài ví dụ được chọn thủ công, để phù hợp với các nghi thức cộng đồng yêu cầu đánh giá ngang hàng; tuy nhiên, những phân tích này rất giai thoại và phải được diễn giải một cách thận trọng. Chúng tôi kiểm tra thủ công một số mẫu để thảo luận kết quả của chúng tôi chi tiết hơn; cụ thể, để trả lời ba câu hỏi: để chỉ rõ 1) các loại thông tin mới mà A𝑆𝐴𝑃 trình bày cho LLM và 2) cách tóm tắt của A𝑆𝐴𝑃 khác với những tóm tắt được tạo bởi các kỹ thuật hiện có, và 3) để phân tích các lỗi mà A𝑆𝐴𝑃 giới thiệu. Bảng 11 trình bày một số mẫu nơi, đối với ba mẫu đầu tiên, A𝑆𝐴𝑃 hoạt động rất tốt so với các baseline dựa trên truy xuất của chúng tôi, và đối với ba mẫu thứ hai, baseline hoạt động tốt hơn A𝑆𝐴𝑃. Trong khi chúng tôi thảo luận phát hiện của mình trong bối cảnh các mẫu được cung cấp, quan sát của chúng tôi khái quát hóa cho các mẫu khác.

Các loại thông tin mới mà A𝑆𝐴𝑃 trình bày cho LLM: Như đã thảo luận trong bài báo, đóng góp chính của chúng tôi bao gồm việc tăng cường các mẫu đã truy xuất (được truy xuất bằng BM25, theo Nashid và cộng sự [48]) với các sự kiện ngữ nghĩa, dẫn đến hiệu suất cải thiện so với phương pháp truy xuất cơ bản. Chúng tôi thêm các sự kiện ngữ nghĩa liên quan đến chi tiết kho lưu trữ, định danh, và đồ thị luồng dữ liệu cho cả mẫu đã truy xuất và mã đầu vào. Như dự đoán, các sự kiện ngữ nghĩa được thêm vào chuyển thành, và tăng cường, đầu ra của mô hình.

Trong mẫu đầu tiên, phương pháp chỉ truy xuất baseline hoàn toàn không nắm bắt được thuật ngữ "gradient". Tuy nhiên, bằng cách kết hợp

6https://platform.openai.com/docs/models/gpt-4

--- TRANG 10 ---
ICSE '24, 14-20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

Ngôn ngữ | BLEU-DC | | | | ROUGE-L | | | | METEOR | | | |
| BM25 | ASAP | Lợi ích (%) | p-value | BM25 | ASAP | Lợi ích (%) | p-value | BM25 | ASAP | Lợi ích (%) | p-value
Java | 14.09 | 15.94 | +13.13% | <0.01 | 36.85 | 38.41 | +4.23% | <0.01 | 35.66 | 36.10 | +1.23% | 0.32
Python | 12.63 | 14.49 | +14.73% | <0.01 | 35.32 | 37.74 | +6.85% | <0.01 | 33.05 | 35.63 | +7.81% | <0.01
Ruby | 9.16 | 11.01 | +20.2% | <0.01 | 28.19 | 30.55 | +8.37% | <0.01 | 27.65 | 29.20 | +5.61% | 0.03
JavaScript | 14.89 | 16.71 | +12.22% | <0.01 | 32.28 | 33.88 | +4.96% | <0.01 | 32.08 | 33.02 | +2.93% | 0.15
Go | 17.10 | 18.57 | +8.60% | <0.01 | 41.04 | 42.43 | +3.39% | <0.01 | 36.78 | 37.26 | +1.31% | 0.27
PHP | 16.97 | 20.63 | +21.57% | <0.01 | 40.48 | 44.90 | +10.92% | <0.01 | 40.14 | 43.35 | +8.00% | <0.01
Tổng thể | 14.14 | 16.23 | +14.78% | <0.01 | 35.69 | 37.99 | +6.44% | <0.01 | 34.23 | 35.76 | +4.47% | <0.01

Bảng 10: Tính hiệu quả của ASAP trong các chỉ số tóm tắt mã phổ biến. p-values được tính toán áp dụng kiểm định cặp một phía Wilcoxon signed-rank và được điều chỉnh B-H.

Thay đổi (BLEU-4) | Tham chiếu | BM25 | A𝑆𝐴𝑃
A𝑆𝐴𝑃 (+47) | Tạo một giá trị nhiễu-gradient từ tọa độ của một giá trị đầu vào ba chiều và tọa độ số nguyên của một giá trị ba chiều gần đó. | Tính toán một giá trị tại điểm (x, y, z) trong hàm nhiễu Perlin 3D. | Tạo một giá trị nhiễu-gradient-nhất quán từ tọa độ của một giá trị đầu vào ba chiều.
A𝑆𝐴𝑃 (+26) | Thay thế type bằng mapped type cho đường dẫn hiện tại. | Trả về mapped type cho đường dẫn hiện tại, hoặc target type nếu không có mapping nào tồn tại. | Thay thế target type bằng mapped type nếu có.
A𝑆𝐴𝑃 (+41) | Chuyển đổi một dataRootExt thành một dataRoot | Chuyển đổi thành đối tượng DataRoot. | Chuyển đổi một DataRootExt thành một DataRoot.
A𝑆𝐴𝑃 (-36) | Tính toán convex hull của tập hợp các điểm. | Tính toán convex hull của tập hợp các điểm | Tính toán convex hull của tập hợp các điểm bằng thuật toán chuỗi đơn điệu của Andrew
A𝑆𝐴𝑃 (-35) | Getter cho Uninstall Actions. | Trả về uninstall actions. | Truy xuất uninstall actions cho đơn vị triển khai này.
A𝑆𝐴𝑃 (-67) | Lấy một cột của ma trận này. | Lấy một cột của ma trận này. | Trả về cột được chỉ định của ma trận này như một vector cột.

Bảng 11: Ví dụ Hiển thị Điểm Mạnh và Điểm Yếu của A𝑆𝐴𝑃.

các sự kiện ngữ nghĩa, mô hình thành công khôi phục thuật ngữ vì nó thường được tìm thấy trong cả định danh và tên kho lưu trữ, ảnh hưởng đến đầu ra của mô hình. Trong ví dụ thứ hai, nơi mục tiêu là thay thế thay vì đơn giản trả về, baseline không tạo ra được thuật ngữ "replace", mặc dù có chỉ báo rõ ràng trong tên hàm ("replaceWithMappedTypeForPath"). Luồng dữ liệu giữa các định danh, được cung cấp trong các sự kiện ngữ nghĩa, có thể đã giúp mô hình nhận ra các thao tác thay thế.

Cách tóm tắt của A𝑆𝐴𝑃 khác với những tóm tắt được tạo bởi các kỹ thuật hiện có: Theo thảo luận trên, chúng tôi quan sát rằng A𝑆𝐴𝑃 đang tạo ra thông tin cụ thể hơn:
(1) Nó xác định "gradient" trong mẫu 1.
(2) Nó đề xuất thay đổi "return" thành "replace" trong một mẫu khác (mẫu 2).
(3) Nó khuyến nghị thay đổi "dataroot" thành "datarootext" trong một mẫu khác (mẫu 3).

Những khác biệt này được quan sát qua nhiều mẫu khi so sánh baseline của chúng tôi với A𝑆𝐴𝑃. Phương pháp A𝑆𝐴𝑃 liên tục tạo ra thông tin cụ thể hơn so với baseline.

Phân tích các lỗi mà A𝑆𝐴𝑃 giới thiệu: Các ví dụ được kiểm tra cho thấy rằng A𝑆𝐴𝑃 có thể trở nên quá cụ thể, và do đó không khớp với tóm tắt được viết bởi nhà phát triển. A𝑆𝐴𝑃 trở nên quá cụ thể trong ba ví dụ cuối cùng với "thuật toán chuỗi đơn điệu của Andrew" và "đơn vị triển khai", "vector cột". Mặc dù những thuật ngữ này không nhất thiết sai, BLEU-4 giảm, vì tóm tắt được viết bởi nhà phát triển tổng quát hơn.

Chúng tôi cũng quan sát định lượng rằng A𝑆𝐴𝑃 tạo ra thay đổi tích cực trong 44% mẫu. Tuy nhiên, hiệu suất cũng giảm cho 30% mẫu, và vẫn giữ nguyên trên phần còn lại. So với baseline của chúng tôi (học few-shot với mẫu được truy xuất BM25), A𝑆𝐴𝑃 đòi hỏi nhiều token hơn. Chi phí token bổ sung, mỗi truy vấn (cả về mặt chi phí tiền tệ và overhead hiệu suất) khá khiêm tốn. Mặt khác, chúng tôi quan sát sự cải thiện đáng kể 12% tổng thể với A𝑆𝐴𝑃 sử dụng mô hình Codex.

6 CÔNG VIỆC LIÊN QUAN

6.1 Tóm Tắt Mã
Các mô hình học sâu đã thúc đẩy hiện trạng trong các nhiệm vụ SE như tóm tắt mã. Mô hình LSTM cho tóm tắt mã lần đầu tiên được giới thiệu bởi Iyer và cộng sự [33]. Các mô hình dựa trên transformer tiền huấn luyện [62] như CodeBERT [21], PLBART [2], và CodeT5 [64] đã được sử dụng rộng rãi trên bộ dữ liệu tóm tắt mã CodeXGLUE [47], dẫn đến những cải thiện đáng kể. Tuy nhiên, có một lưu ý khi sử dụng các mô hình ngôn ngữ tiền huấn luyện: mặc dù những mô hình này hoạt động tốt, fine-tuning rộng rãi là cần thiết, có thể đòi hỏi nhiều dữ liệu & tốn thời gian. Ngoài ra, các mô hình riêng biệt phải được huấn luyện cho các ngôn ngữ khác nhau, làm tăng chi phí huấn luyện. Để giảm số lượng mô hình cần thiết, fine-tuning đa ngôn ngữ đã được đề xuất, để duy trì hoặc cải thiện hiệu suất trong khi giảm số lượng mô hình xuống một [4]. Tuy nhiên, phương pháp này không giảm thời gian huấn luyện hoặc nhu cầu về dữ liệu có nhãn.

LLM, hoặc các mô hình ngôn ngữ lớn, lớn hơn nhiều so với những mô hình tiền huấn luyện này, và được huấn luyện trên các bộ dữ liệu lớn hơn nhiều với một mục tiêu huấn luyện đơn giản — dự đoán token tiếp theo tự hồi quy [12]. Những mô hình này hoạt động đáng ngạc nhiên tốt trên các nhiệm vụ, thậm chí không cần fine-tuning. Chỉ cần nhắc mô hình với những câu hỏi khác nhau, trong khi cung cấp một vài mẫu vấn đề-giải pháp, là đủ. Học few-shot đã được áp dụng cho tóm tắt mã, và đã được tìm thấy là có lợi [3].

6.2 Các Bộ Dữ Liệu Khác
Có một số bộ dữ liệu có sẵn cho tóm tắt mã, ngoài CodeXGLUE [47]. TL-CodeSum [30] là một bộ dữ liệu tương đối nhỏ hơn, với khoảng 87K mẫu, nhưng nó bao gồm các bản sao, có thể dẫn đến ước tính hiệu suất cao có thể không khái quát hóa. Funcom [41] là một bộ dữ liệu chuyên dụng với 2.1 triệu hàm Java, nhưng chứa các bản sao. Chúng tôi chọn CodeXGLUE (có nguồn gốc từ CodeSearchNet) vì nó là một bộ dữ liệu đa dạng, đa ngôn ngữ đưa ra thách thức cho các mô hình. Thậm chí các mô hình được huấn luyện tốt như CodeBERT cũng gặp khó khăn trên benchmark này; hiệu suất của nó đặc biệt kém trên các ngôn ngữ có ít mẫu huấn luyện hơn.

Đã có rất nhiều công việc về tóm tắt mã, từ khớp mẫu đến học few-shot. Những mô hình này sử dụng các biểu diễn và nguồn thông tin khác nhau để hoạt động tốt trong tóm tắt mã. So sánh hoặc thảo luận tất cả những mô hình này nằm ngoài phạm vi của công việc này. Tuy nhiên, chúng tôi lưu ý rằng con số của chúng tôi đại diện cho một điểm cao mới trên benchmark CodeXGlue được sử dụng rộng rãi cho tóm tắt mã và hoàn thành mã; chúng tôi giới thiệu người đọc đến https://microsoft.github.io/CodeXGLUE/ để xem nhanh bảng xếp hạng. Mẫu của chúng tôi nhỏ hơn (N=1000), nhưng các ước tính, và ước tính cải thiện, mạnh mẽ về mặt thống kê (Xem thảo luận kích thước mẫu trong Phần 7).

6.3 LLM trong Kỹ thuật Phần mềm
Mặc dù LLM chưa được sử dụng rộng rãi cho tóm tắt mã, chúng được sử dụng rộng rãi cho tạo mã [14, 49, 67] và sửa chương trình [5,18,35,36]. Các mô hình như Codex nhằm giảm gánh nặng cho các nhà phát triển bằng cách tự động tạo mã hoặc hoàn thành dòng. Một số mô hình như Polycoder [67] và Codegen [49] hoạt động khá tốt, và do học few-shot hoặc prompting của chúng, chúng có thể được áp dụng cho một tập hợp rộng các vấn đề. Tuy nhiên, mô hình Code-davinci-002 thường hoạt động tốt hơn những mô hình đó và cho phép chúng tôi vừa các lời nhắc được tăng cường của mình vào một cửa sổ lớn hơn.

Jain và cộng sự đã đề xuất bổ sung hoạt động LLM với các bước xử lý tiếp theo dựa trên kỹ thuật phân tích và tổng hợp chương trình để cải thiện hiệu suất trong tạo đoạn chương trình [34]. Bareiß và cộng sự đã chỉ ra tính hiệu quả của học few-shot trong đột biến mã, tạo oracle kiểm thử từ tài liệu ngôn ngữ tự nhiên, và các nhiệm vụ tạo trường hợp kiểm thử [10]. CODAMOSA [42], một phương pháp dựa trên LLM, tiến hành kiểm thử phần mềm dựa trên tìm kiếm cho đến khi các cải thiện coverage của nó ngưng trệ, sau đó yêu cầu LLM cung cấp các trường hợp kiểm thử ví dụ cho các hàm không được bao phủ. Bằng cách sử dụng những ví dụ này, CODAMOSA giúp chuyển hướng kiểm thử phần mềm dựa trên tìm kiếm đến các khu vực hữu ích hơn của không gian tìm kiếm. Jiang và cộng sự đánh giá tính hiệu quả của LLM cho vấn đề sửa chương trình [35].

Truy xuất và thêm một tập hợp các mẫu huấn luyện đã được tìm thấy là có lợi cho nhiều nhiệm vụ phân tích ngữ nghĩa trong NLP, thậm chí không sử dụng LLM [68]. Một hạn chế của phương pháp này là hiệu suất có thể bị ràng buộc bởi sự sẵn có của các ví dụ tương tự. Nashid và cộng sự đã sử dụng một phương pháp tương tự và đạt được hiệu suất cải thiện trong sửa mã và tạo xác nhận với sự hỗ trợ của LLM [48]. Tuy nhiên, không có công việc nào ở trên đã cố gắng tự động tăng cường ngữ nghĩa lời nhắc. Lưu ý rằng vẫn còn quá sớm để bình luận về khả năng đầy đủ của những mô hình ngôn ngữ lớn này. Các phát hiện của chúng tôi cho đến nay cho thấy rằng việc tăng cường các mẫu trong lời nhắc với các gợi ý ngữ nghĩa giúp ích cho các nhiệm vụ tóm tắt mã và hoàn thành mã; việc đánh giá giá trị của A𝑆𝐴𝑃 trong các nhiệm vụ khác được để lại cho công việc tương lai.

7 MỐI ĐE DỌA & HẠN CHẾ
Một mối quan tâm lớn khi làm việc với các mô hình ngôn ngữ lớn là khả năng phơi bày dữ liệu kiểm tra trong quá trình huấn luyện. Đáng tiếc, người ta không thể kiểm tra trực tiếp điều này vì bộ dữ liệu huấn luyện không thể truy cập. Hiệu suất thấp hơn của mô hình với few-shotting ngẫu nhiên cho thấy rằng ghi nhớ có thể không phải là một yếu tố chính. Khi chúng tôi kết hợp thông tin liên quan, hiệu suất của mô hình cải thiện với lượng và chất lượng thông tin. Nếu mô hình đã ghi nhớ các tóm tắt, nó có thể đã ghi điểm cao hơn nhiều, thậm chí không có lợi ích của các mẫu liên quan và tăng cường ngữ nghĩa.

Phân Tích Kích Thước Mẫu: Chúng tôi sử dụng trung bình và độ lệch chuẩn quan sát để tính toán (sử dụng G*power [19,20]) kích thước mẫu cần thiết, sử dụng các giá trị thường được sử dụng: 𝛼 là 0.01 (p-value mong muốn) và 𝛽 là 0.20 (tức là, 20% cơ hội KHÔNG khám phá hiệu ứng, nếu có một hiệu ứng tồn tại). Đối với các kiểm định mà chúng tôi sử dụng (kiểm định Wilcoxon Signed-rank), chúng tôi thấy rằng kích thước mẫu cần thiết luôn dưới kích thước mẫu mà chúng tôi sử dụng cho các nghiên cứu chính của mình, tức là, 1000.

Nghiên Cứu Người Dùng: Chúng tôi không tiến hành nghiên cứu người dùng cho A𝑆𝐴𝑃. Do đó, những cải thiện trong các chỉ số được trình bày ở đây có thể không nhất thiết chuyển thành hiệu suất nhà phát triển cải thiện. Khía cạnh này được để lại cho công việc tương lai.

Cuối cùng: fine-tuning các LM lớn để sử dụng các sự kiện ngữ nghĩa có nguồn gốc có thể cải thiện so với phương pháp prompting được tăng cường của chúng tôi, nhưng sẽ tốn kém. Chúng tôi sẽ để việc xem xét nó cho nghiên cứu tương lai.

8 KẾT LUẬN
Trong bài báo này, chúng tôi đã khám phá ý tưởng Tăng Cường Ngữ Nghĩa Tự Động cho Các Lời Nhắc, theo đó chúng tôi đề xuất tăng cường các mẫu few-shot trong lời nhắc LLM, với các sự kiện được gắn thẻ tự động có nguồn gốc bằng phân tích ngữ nghĩa. Điều này dựa trên trực giác rằng các nhà phát triển con người thường quét mã để ngầm trích xuất những sự kiện như vậy trong quá trình hiểu mã dẫn đến viết một tóm tắt tốt. Mặc dù có thể hình dung rằng LLM có thể ngầm suy ra những sự kiện như vậy cho chính chúng, chúng tôi đoán rằng việc thêm những sự kiện này theo phong cách được định dạng vào các mẫu và mục tiêu, trong lời nhắc, sẽ giúp LLM tổ chức "chuỗi suy nghĩ" của nó khi nó tìm cách xây dựng một tóm tắt. Chúng tôi đánh giá ý tưởng này trên một bộ dữ liệu CodeSearchNet đầy thách thức, được khử trùng, được tuyển chọn kỹ lưỡng, trên hai nhiệm vụ: tóm tắt mã và hoàn thành mã. Phát hiện của chúng tôi chỉ ra rằng Tăng Cường Ngữ Nghĩa Tự Động cho Các Lời Nhắc thường hữu ích. Ước tính của chúng tôi cho thấy nó giúp vượt qua hiện trạng.

Lời Cảm Ơn: Chúng tôi muốn cảm ơn Quỹ Khoa học Quốc gia dưới Grant NSF CCF (SHF-MEDIUM) Số 2107592 và Cơ quan Dự án Nghiên cứu Tiên tiến Intelligence (IARPA) dưới hợp đồng W911NF20C0038 cho sự hỗ trợ một phần của công việc này. Kết luận của chúng tôi không nhất thiết phản ánh vị trí hoặc chính sách của các nhà tài trợ và không nên suy ra sự chấp thuận chính thức nào.

TÀI LIỆU THAM KHẢO
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4998–5007.

--- TRANG 12 ---
ICSE '24, 14-20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

[2] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2655–2668.

[3] Toufique Ahmed và Premkumar Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization. Trong 37th IEEE/ACM International Conference on Automated Software Engineering. 1–5.

[4] Toufique Ahmed và Premkumar Devanbu. 2022. Multilingual training for software engineering. Trong Proceedings of the 44th International Conference on Software Engineering. 1443–1455.

[5] Toufique Ahmed và Premkumar Devanbu. 2023. Better patching using LLM prompting, via Self-Consistency. Trong 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1742–1746.

[6] Toufique Ahmed, Supriyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao Zhang, và Saravan Rajmohan. 2023. Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models. ICSE (2023).

[7] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. Trong Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. 143–153.

[8] Uri Alon, Shaked Brody, Omer Levy, và Eran Yahav. 2018. code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv:1808.01400 (2018).

[9] Satanjeev Banerjee và Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Trong Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65–72.

[10] Patrick Bareiß, Beatriz Souza, Marcelo d'Amorim, và Michael Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335 (2022).

[11] Lionel C Briand. 2003. Software documentation: how much is enough?. Trong Seventh European Conference on Software Maintenance and Reengineering, 2003. Proceedings. IEEE, 13–15.

[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và các cộng sự. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.

[13] Boxing Chen và Colin Cherry. 2014. A systematic comparison of smoothing techniques for sentence-level BLEU. Trong Proceedings of the ninth workshop on statistical machine translation. 362–367.

[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và các cộng sự. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[16] Samanta Dey, Venkatesh Vinayakarao, Monika Gupta, và Sampath Dechu. 2022. Evaluating commit message generation: to BLEU or not to BLEU?. Trong Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. 31–35.

[17] Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, và Jeffrey C Carver. 2013. Evaluating source code summarization techniques: Replication and expansion. Trong 2013 21st International Conference on Program Comprehension (ICPC). IEEE, 13–22.

[18] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, và Shin Hwei Tan. 2022. Automated Repair of Programs from Large Language Models. ICSE.

[19] Franz Faul, Edgar Erdfelder, Axel Buchner, và Albert-Georg Lang. 2009. Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses. Behavior research methods 41, 4 (2009), 1149–1160.

[20] Franz Faul, Edgar Erdfelder, Albert-Georg Lang, và Axel Buchner. 2007. G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods 39, 2 (2007), 175–191.

[21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và các cộng sự. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 1536–1547.

[22] Andrew Forward và Timothy C Lethbridge. 2002. The relevance of software documentation, tools and technologies: a survey. Trong Proceedings of the 2002 ACM symposium on Document engineering. 26–33.

[23] Jianfeng Gao và Xiaodong He. 2013. Training MRF-based phrase translation models using gradient ascent. Trong Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 450–459.

[24] David Gros, Hariharan Sezhiyan, Prem Devanbu, và Zhou Yu. 2020. Code to Comment ?Translation?: Data, Metrics, Baselining & Evaluation. Trong 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 746–757.

[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, và các cộng sự. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. Trong International Conference on Learning Representations.

[26] Sonia Haiduc, Jairo Aponte, và Andrian Marcus. 2010. Supporting program comprehension with source code summarization. Trong Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 2. 223–226.

[27] Sonia Haiduc, Jairo Aponte, Laura Moreno, và Andrian Marcus. 2010. On the use of automated text summarization techniques for summarizing source code. Trong 2010 17th Working Conference on Reverse Engineering. IEEE, 35–44.

[28] Sakib Haque, Zachary Eberhart, Aakash Bansal, và Collin McMillan. 2022. Semantic similarity metrics for evaluating source code summarization. Trong Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. 36–47.

[29] Xing Hu, Ge Li, Xin Xia, David Lo, và Zhi Jin. 2018. Deep code comment generation. Trong Proceedings of the 26th conference on program comprehension. 200–210.

[30] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, và Zhi Jin. 2018. Summarizing source code with transferred API knowledge. Trong Proceedings of the 27th International Joint Conference on Artificial Intelligence. 2269–2275.

[31] Jie Huang và Kevin Chen-Chuan Chang. 2022. Towards Reasoning in Large Language Models: A Survey. arXiv preprint arXiv:2212.10403 (2022).

[32] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[33] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. Trong Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2073–2083.

[34] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, và Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. Trong Proceedings, 44th ICSE. 1219–1231.

[35] Nan Jiang, Kevin Liu, Thibaud Lutellier, và Lin Tan. 2023. Impact of Code Language Models on Automated Program Repair. ICSE (2023).

[36] Harshit Joshi, José Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, và Gust Verbruggen. 2022. Repair is nearly generation: Multilingual program repair with llms. arXiv preprint arXiv:2208.11640 (2022).

[37] Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, và Mohamed Coulibali. 2020. NUBIA: NeUral Based Interchangeability Assessor for Text Generation. arXiv:2004.14667 [cs.CL]

[38] Sungmin Kang, Juyeon Yoon, và Shin Yoo. 2023. Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction. ICSE (2023).

[39] Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, và các cộng sự. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.

[40] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).

[41] Alexander LeClair, Siyuan Jiang, và Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. Trong 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 795–806.

[42] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, và Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models. Trong 45th International Conference on Software Engineering, ser. ICSE.

[43] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Trong Text summarization branches out. 74–81.

[44] Chin-Yew Lin và Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. Trong COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. 501–507.

[45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).

[46] Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny, Hitesh Sajnani, và Jan Vitek. 2017. DéjàVu: a map of code duplicates on GitHub. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1–28.

[47] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, và các cộng sự. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).

[48] Noor Nashid, Mifta Sintaha, và Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. Trong Proceedings, 45th ICSE.

[49] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2022. Codegen: An open large language

--- TRANG 13 ---
Tăng Cường Ngữ Nghĩa Tự Động cho Các Lời Nhắc Mô Hình Ngôn Ngữ
(để Tóm Tắt Mã) ICSE '24, 14-20 tháng 4, 2024, Lisbon, Bồ Đào Nha

model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022).

[50] Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318.

[51] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, và Huajun Chen. 2022. Reasoning with Language Model Prompting: A Survey. arXiv preprint arXiv:2212.09597 (2022).

[52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, và các cộng sự. 2018. Improving language understanding by generative pre-training. (2018).

[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, và các cộng sự. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

[54] Juan Ramos và các cộng sự. 2003. Using tf-idf to determine word relevance in document queries. Trong Proceedings of the first instructional conference on machine learning, Vol. 242. Citeseer, 29–48.

[55] Stephen Robertson, Hugo Zaragoza, và các cộng sự. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends ®in Information Retrieval 3, 4 (2009), 333–389.

[56] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, và Sidney D'Mello. 2014. Improving automated source code summarization via an eye-tracking study of programmers. Trong Proceedings of the 36th international conference on Software engineering. 390–401.

[57] Devjeet Roy, Sarah Fakhoury, và Venera Arnaoudova. 2021. Reassessing automatic evaluation metrics for code summarization tasks. Trong Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1105–1116.

[58] Thibault Sellam, Dipanjan Das, và Ankur P Parikh. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696 (2020).

[59] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, và Hongbin Sun. 2023. On the evaluation of neural code summarization. Trong Proceedings of the 44th International Conference on Software Engineering. 1597–1608.

[60] Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. 2022. Repository-level prompt generation for large language models of code. arXiv preprint arXiv:2206.12839 (2022).

[61] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, và K Vijay-Shanker. 2010. Towards automatically generating summary comments for java methods. Trong Proceedings of the IEEE/ACM international conference on Automated software engineering. 43–52.

[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in neural information processing systems. 5998–6008.

[63] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, và Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).

[64] Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696–8708.

[65] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, và Zhi Jin. 2019. Code generation as a dual task of code summarization. Advances in neural information processing systems 32 (2019).

[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).

[67] Frank F Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. Trong Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1–10.

[68] Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, và Fei Sha. 2022. Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing. arXiv preprint arXiv:2209.14899 (2022).

[69] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, và Xudong Liu. 2020. Retrieval-based neural source code summarization. Trong Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1385–1397.

[70] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).
