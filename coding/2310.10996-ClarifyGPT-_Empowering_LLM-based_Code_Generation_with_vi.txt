# ClarifyGPT: Trao quyền cho việc sinh code dựa trên LLM thông qua làm rõ ý định

FANGWEN MU∗, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
LIN SHI∗, Đại học Beihang, Trung Quốc
SONG WANG, Đại học York, Canada
ZHUOHAO YU, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
BINQUAN ZHANG, Đại học Beihang, Trung Quốc
CHENXUE WANG, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
SHICHAO LIU, Phòng thí nghiệm đổi mới IDE Phần mềm, Viện Phần mềm Trung tâm Huawei, Trung Quốc
QING WANG, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc

Các Mô hình Ngôn ngữ Lớn (LLM), chẳng hạn như ChatGPT, đã thể hiện khả năng ấn tượng trong việc tự động sinh code từ các yêu cầu ngôn ngữ tự nhiên được cung cấp. Tuy nhiên, trong thực tế, không thể tránh khỏi việc các yêu cầu được người dùng viết có thể mơ hồ hoặc không đầy đủ. Các LLM hiện tại sẽ trực tiếp sinh chương trình theo những yêu cầu không rõ ràng đó bất kể việc làm rõ tương tác, điều này có thể sẽ lệch khỏi ý định gốc của người dùng. Để thu hẹp khoảng cách đó, chúng tôi giới thiệu một framework mới tên là ClarifyGPT, nhằm tăng cường việc sinh code bằng cách trao quyền cho LLM khả năng xác định các yêu cầu mơ hồ và đặt các câu hỏi làm rõ có mục tiêu. Cụ thể, ClarifyGPT đầu tiên phát hiện xem một yêu cầu đã cho có mơ hồ hay không bằng cách thực hiện kiểm tra tính nhất quán của code. Nếu nó mơ hồ, ClarifyGPT nhắc LLM sinh các câu hỏi làm rõ có mục tiêu. Sau khi nhận được phản hồi câu hỏi, ClarifyGPT tinh chỉnh yêu cầu mơ hồ và đưa nó vào cùng LLM để sinh giải pháp code cuối cùng. Để đánh giá ClarifyGPT của chúng tôi, trước tiên chúng tôi tiến hành đánh giá con người liên quan đến mười người tham gia sử dụng ClarifyGPT để sinh code trên hai benchmark công khai: MBPP-sanitized và MBPP-ET. Kết quả cho thấy ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 từ 70.96% lên 80.80% trên MBPP-sanitized. Hơn nữa, để thực hiện đánh giá tự động quy mô lớn của ClarifyGPT trên các LLM và benchmark khác nhau mà không yêu cầu sự tham gia của người dùng, chúng tôi giới thiệu một phương pháp mô phỏng độ chính xác cao để mô phỏng phản hồi của người dùng. Kết quả đánh giá tự động cũng chứng minh rằng ClarifyGPT có thể tăng cường đáng kể hiệu suất sinh code so với các baseline. Cụ thể, ClarifyGPT cải thiện hiệu suất trung bình của GPT-4 và ChatGPT trên bốn benchmark từ 68.02% lên 75.75% và từ 58.55% lên 67.22%, tương ứng. Chúng tôi tin rằng ClarifyGPT có thể hỗ trợ hiệu quả việc ứng dụng thực tế của LLM trong môi trường phát triển thế giới thực.

∗Cả hai tác giả đều đóng góp như nhau cho nghiên cứu này

Địa chỉ tác giả: Fangwen Mu, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, fangwen2020@iscas.ac.cn; Lin Shi, Đại học Beihang, Bắc Kinh, Trung Quốc, shilin@buaa.edu.cn; Song Wang, Đại học York, Toronto, Canada, wangsong@yorku.ca; Zhuohao Yu, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, yuzhuohao23@mails.ucas.edu.cn; Binquan Zhang, Đại học Beihang, Bắc Kinh, Trung Quốc, binquan@buaa.edu.cn; ChenXue Wang, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, chenxuew02@gmail.com; Shichao Liu, Phòng thí nghiệm đổi mới IDE Phần mềm, Viện Phần mềm Trung tâm Huawei, Bắc Kinh, Trung Quốc, liushichao2@huawei.com; Qing Wang, Viện Phần mềm, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, wq@iscas.ac.cn.

Quyền thực hiện bản sao kỹ thuật số hoặc bản cứng toàn bộ hoặc một phần công trình này cho sử dụng cá nhân hoặc lớp học được cấp miễn phí với điều kiện các bản sao không được thực hiện hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao mang thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của những người khác ngoài ACM phải được tôn trọng. Tóm tắt với tín dụng được phép. Để sao chép khác, hoặc tái xuất bản, đăng trên máy chủ hoặc phân phối lại danh sách, yêu cầu quyền cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.

©2023 Association for Computing Machinery.
XXXX-XXXX/2023/10-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Ngày xuất bản: Tháng 10 năm 2023.arXiv:2310.10996v1 [cs.SE] 17 Oct 2023

Định dạng Tham khảo ACM:
Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, ChenXue Wang, Shichao Liu, và Qing Wang. 2023. ClarifyGPT: Trao quyền cho việc sinh code dựa trên LLM thông qua làm rõ ý định. 1, 1 (Tháng 10 năm 2023), 21 trang. https://doi.org/10.1145/nnnnnnn.nnnnnnn

## 1 GIỚI THIỆU

Sinh code nhằm tạo ra một đoạn code thỏa mãn ý định của người dùng được thể hiện trong một yêu cầu ngôn ngữ tự nhiên. Nhiệm vụ này, cung cấp tiềm năng tiết kiệm chi phí, tăng tốc các hoạt động lập trình, và tạo thuận lợi cho phát triển phần mềm, do đó đã thu hút sự chú ý trên nhiều lĩnh vực khác nhau, ví dụ: xử lý ngôn ngữ tự nhiên, trí tuệ nhân tạo, và kỹ thuật phần mềm. Các nỗ lực gần đây giải quyết nhiệm vụ này bằng cách tận dụng Mô hình Ngôn ngữ Lớn (LLM) với hàng tỷ tham số, chẳng hạn như ChatGPT[34] và CodeGen[33]. Các LLM nhận các yêu cầu ngôn ngữ tự nhiên (tức là prompts) làm đầu vào và xuất ra các đoạn code tương ứng, đạt được tiến bộ đáng kể trong sinh code.

Tuy nhiên, trong thực tế, do sự đa dạng về kinh nghiệm và quan điểm của người dùng, không thể tránh khỏi việc các yêu cầu được người dùng viết có thể mơ hồ hoặc không đầy đủ. Ví dụ, yêu cầu "Viết một hàm để sắp xếp một danh sách các phần tử" không chỉ định xem người dùng có ý định danh sách được sắp xếp theo thứ tự tăng dần hay giảm dần. Các LLM hiện tại không xử lý những yêu cầu mơ hồ như vậy: chúng hiếm khi yêu cầu người dùng làm rõ những yêu cầu này mà thay vào đó trực tiếp sinh chương trình có thể lệch khỏi nhu cầu của người dùng [21]. Các phương pháp sinh code dựa trên LLM hiện tại thiếu cơ chế làm rõ các yêu cầu không rõ ràng [20,21], tức là chúng trực tiếp sinh chương trình theo những yêu cầu không rõ ràng đó bất kể việc làm rõ tương tác. Ngược lại, khi các nhà phát triển con người gặp phải yêu cầu mơ hồ, họ thường tìm kiếm thông tin bổ sung bằng cách tương tác đặt câu hỏi làm rõ cho người dùng. Đối với ví dụ trên, một câu hỏi làm rõ đơn giản như "Việc sắp xếp có nên theo thứ tự tăng dần hay giảm dần?" có thể giúp làm rõ yêu cầu.

Dựa trên quan sát này, chúng tôi cho rằng việc trao quyền cho LLM khả năng tự động đặt câu hỏi làm rõ cho các yêu cầu mơ hồ là cần thiết để cải thiện chất lượng và hiệu quả của sinh code. Tuy nhiên, việc trao quyền cho LLM khả năng này khá thách thức do các rào cản sau. (1) Khi nào nên Đặt Câu hỏi Làm rõ? Trong môi trường phát triển thực tế, tồn tại nhiều yêu cầu, bao gồm cả những yêu cầu mơ hồ và không mơ hồ. Việc không tập trung vào chỉ đặt câu hỏi cho các yêu cầu mơ hồ có thể dẫn đến tương tác không cần thiết giữa LLM và người dùng về các yêu cầu được định nghĩa rõ ràng. Những tương tác không cần thiết này, đến lượt mình, có thể làm giảm hiệu quả và gây tổn hại đến trải nghiệm người dùng. (2) Những Câu hỏi Làm rõ nào Nên được Đặt? Chất lượng của các câu hỏi làm rõ cũng ảnh hưởng đến hiệu quả và hiệu suất của sinh code. Các câu hỏi chính xác và có mục tiêu giúp người dùng thể hiện ý định của họ một cách rõ ràng, đảm bảo rằng các phản hồi thu được có liên quan trực tiếp đến các sự mơ hồ có trong yêu cầu. Các câu hỏi mơ hồ hoặc rộng tăng rủi ro thu được phản hồi ngoài chủ đề hoặc không liên quan, có khả năng cản trở LLM khỏi việc hiểu ý định của người dùng.

Trong bài báo này, chúng tôi đề xuất một framework mới được gọi là ClarifyGPT nhằm tăng cường sinh code dựa trên LLM thông qua làm rõ yêu cầu. Thứ nhất, chúng tôi sử dụng kiểm tra tính nhất quán code hai bước để quyết định khi nào đặt câu hỏi làm rõ. Chúng tôi được thúc đẩy bởi quan sát rằng việc đưa một yêu cầu rõ ràng vào LLM thường dẫn đến việc sinh các đoạn code đa dạng hoạt động nhất quán, tức là với cùng đầu vào kiểm tra, những đoạn code khác nhau đó có khả năng trả về cùng đầu ra. Trong khi đưa một yêu cầu không rõ ràng, LLM có khả năng sinh các đoạn code đa dạng hoạt động khác nhau. Cụ thể, trong bước đầu tiên, ClarifyGPT nhằm sinh nhiều đầu vào kiểm tra chất lượng cao cho một yêu cầu đã cho thông qua đột biến có nhận thức kiểu. Trong bước thứ hai, ClarifyGPT đưa yêu cầu đã cho vào LLM để lấy mẫu n giải pháp code và kiểm tra xem chúng có tạo ra đầu ra giống hệt nhau khi được kiểm tra với đầu vào được sinh hay không. Nếu các đầu ra không giống hệt nhau, ClarifyGPT xác định rằng yêu cầu cần làm rõ thêm; và ngược lại. Thứ hai, chúng tôi sử dụng prompting dựa trên lý luận để sinh câu hỏi làm rõ. Ban đầu, ClarifyGPT hướng dẫn LLM phân tích các yếu tố góp phần vào sự mơ hồ của yêu cầu đã cho bằng cách so sánh các giải pháp code với các chức năng khác nhau. Sau đó, nó xây dựng các câu hỏi làm rõ có mục tiêu dựa trên kết quả của phân tích này. Bằng cách so sánh những triển khai code khác nhau này, các điểm mơ hồ tiềm năng trong yêu cầu có thể được xác định dễ dàng. Sau khi phát hiện các điểm mơ hồ trong yêu cầu, LLM có thể sinh câu hỏi làm rõ có mục tiêu cho chúng. Cuối cùng, ClarifyGPT tinh chỉnh yêu cầu ban đầu dựa trên các câu hỏi được sinh và phản hồi của chúng và sinh giải pháp code cuối cùng.

Để đánh giá hiệu quả của ClarifyGPT, trước tiên chúng tôi tích hợp GPT-4 [35] vào ClarifyGPT và tuyển mười người tham gia để đánh giá hiệu suất của nó trên hai benchmark công khai (MBPP-sanitized [4], và MBPP-ET [10]). Kết quả đánh giá con người cho thấy ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 trên MBPP-sanitized từ 70.96% lên 80.8%, cải thiện hiệu suất (Pass@1) của ChatGPT trên MBPP-ET từ 51.52% lên 60.19%. Bên cạnh đó, do yêu cầu sự tham gia của người tham gia con người, việc đánh giá ClarifyGPT có thể rất tốn kém và khó tái tạo. Để thực hiện đánh giá tự động của ClarifyGPT trên các LLM và benchmark khác nhau mà không yêu cầu sự tham gia của người dùng, chúng tôi giới thiệu một phương pháp mô phỏng độ chính xác cao để mô phỏng phản hồi của người dùng. Sau đó, chúng tôi tiến hành thí nghiệm toàn diện trên bốn benchmark (HumanEval [8], HumanEval-ET [10], MBPP-sanitized, và MBPP-ET) sử dụng hai LLM hiện đại (tức là GPT-4 và ChatGPT). Kết quả chứng minh rằng, so với GPT-4 mặc định, ClarifyGPT đạt được cải thiện trung bình 11.52% trên bốn benchmark; so với ChatGPT mặc định, ClarifyGPT đạt được cải thiện trung bình 15.07% trên bốn benchmark. Các đóng góp chính của chúng tôi được nêu như sau:

• Framework: Chúng tôi đề xuất một framework mới, tên là ClarifyGPT, cho phép LLM phát hiện các yêu cầu mơ hồ và xây dựng các câu hỏi làm rõ có mục tiêu. ClarifyGPT tinh chỉnh các yêu cầu mơ hồ dựa trên các câu trả lời cho câu hỏi làm rõ và tiếp tục sinh giải pháp code.

• Mô phỏng Người dùng: Chúng tôi giới thiệu một phương pháp mô phỏng người dùng để tạo ra các câu trả lời mô phỏng độ chính xác cao cho các câu hỏi làm rõ, điều này tạo thuận lợi cho đánh giá tự động của ClarifyGPT trên các LLM và benchmark khác nhau, loại bỏ sự cần thiết tham gia trực tiếp của người dùng.

• Đánh giá: Chúng tôi tiến hành thí nghiệm rộng rãi trên bốn benchmark được sử dụng rộng rãi để cho thấy rằng, ClarifyGPT đạt được cải thiện đáng kể trên các model và benchmark khác nhau. Một đánh giá con người tiếp tục xác nhận tiềm năng đáng kể của việc ứng dụng ClarifyGPT trong thực tế.

• Dữ liệu: dataset có thể truy cập công khai và mã nguồn [1] để tạo thuận lợi cho việc sao chép nghiên cứu của chúng tôi và ứng dụng của nó trong các bối cảnh rộng rãi.

Trong phần còn lại của bài báo này, Mục 2 giới thiệu nền tảng và công trình liên quan. Mục 3 trình bày chi tiết framework ClarifyGPT được đề xuất của chúng tôi. Mục 4 trình bày thiết lập thí nghiệm. Mục 5 minh họa kết quả và phân tích. Mục 6 thảo luận về lợi ích và hạn chế của ClarifyGPT và các mối đe dọa đối với tính hợp lệ. Cuối cùng, Mục 7 tóm tắt công trình này.

## 2 NỀN TẢNG VÀ CÔNG TRÌNH LIÊN QUAN

### 2.1 Sinh Code dựa trên LLM

Sinh code là một chủ đề nghiên cứu nóng cho cộng đồng kỹ thuật phần mềm và trí tuệ nhân tạo. Gần đây, nhiều LLM đã được đề xuất để sinh code. Một loại model là các model encoder-decoder, ví dụ: PLBART [2], CodeT5 [45], và AlphaCode [27], thường mã hóa một văn bản đầu vào thành một embedding ngữ cảnh và giải mã embedding thành một giải pháp code. Loại model khác là các model chỉ decoder được huấn luyện với mục tiêu dự đoán token tiếp theo và sinh code từ trái sang phải. Các model GPT series [5,8], PolyCoder [48], và InCoder [13] là ví dụ của các model như vậy. Trong số chúng, ChatGPT [34] và GPT-4 [35] là các LLM hiện đại được phát triển bởi OpenAI. Chúng đã thể hiện khả năng hiểu và lý luận được cải thiện, thành thạo trong việc hiểu ngữ cảnh được cung cấp, và khả năng sinh văn bản chất lượng cao.

Vì việc huấn luyện hoặc fine-tuning các LLM này rất tốn kém, cũng đã có nhiều nghiên cứu tập trung vào việc tăng cường hiệu suất của LLM trong sinh code với việc fine-tuning tối thiểu hoặc không có. Prompt Learning là một trong những kỹ thuật quan trọng nhất để đạt được mục tiêu này [11,28,31,40,47]. Chain-of-Thought (CoT) [47] là một kỹ thuật prompt engineering mới, có thể khiến LLM tạo ra các bước lý luận trung gian dẫn đến câu trả lời cuối cùng. Nó đã cho thấy hiệu suất ấn tượng trong các nhiệm vụ lý luận phức tạp (ví dụ: lý luận số học và ký hiệu) [19,47], và do đó đã được áp dụng vào sinh code [16,26]. Lấy cảm hứng từ CoT, Li et al. [26] đề xuất một phương pháp prompting mới, tên là Structured CoT (SCoT). Khác với CoT, SCoT giới thiệu rõ ràng các cấu trúc code và dạy LLM sinh các bước lý luận trung gian với cấu trúc chương trình. Jiang et al. [16] đề xuất một phương pháp tự lập kế hoạch có thể hướng dẫn LLM hiểu kế hoạch code với các minh họa few-shot và viết kế hoạch code tương ứng cho yêu cầu đã cho.

Các nghiên cứu nêu trên tập trung vào việc tận dụng và tăng cường khả năng lý luận của LLM, tức là nhắc LLM sinh các bước lý luận trung gian để tăng cường hiệu suất sinh code. Tuy nhiên, chúng vẫn không đủ để giải quyết các yêu cầu mơ hồ được con người cung cấp, vì ý định người dùng không rõ ràng có thể khiến LLM tạo ra các bước lý luận sai, từ đó mang lại kết quả không chính xác. ClarifyGPT của chúng tôi nhận ra tầm quan trọng của việc làm rõ các yêu cầu mơ hồ và đề xuất một framework mới cho phép LLM tự động phát hiện các yêu cầu mơ hồ và đặt câu hỏi làm rõ có mục tiêu. Bằng cách làm rõ yêu cầu của người dùng, ClarifyGPT có thể sinh giải pháp code đáp ứng ý định của người dùng. GPT-Engineer [36] là một repository Github mã nguồn mở gần đây. Nó sử dụng các hướng dẫn được thiết kế thủ công để nhắc LLM đặt câu hỏi làm rõ cho các yêu cầu đầu vào của người dùng, và sau đó sinh đoạn code dựa trên phản hồi của người dùng. Tuy nhiên, GPT-Engineer đặt câu hỏi làm rõ cho cả yêu cầu mơ hồ và không mơ hồ, điều này có hại cho trải nghiệm người dùng và có thể dẫn đến giải pháp code sai¹. Trong khi ClarifyGPT có thể phát hiện yêu cầu mơ hồ bằng cách kiểm tra xem các đầu ra kiểm tra của các giải pháp code được lấy mẫu có giống hệt nhau hay không. Hơn nữa, ClarifyGPT sử dụng kỹ thuật prompting để hướng dẫn LLM trước tiên phân tích các yếu tố góp phần vào sự mơ hồ của yêu cầu và sau đó xây dựng câu hỏi có mục tiêu.

### 2.2 Sinh Câu hỏi Làm rõ

Nhiệm vụ sinh câu hỏi làm rõ cho các truy vấn hoặc đối thoại mơ hồ đã nhận được nhiều sự chú ý trong các lĩnh vực truy xuất thông tin và hệ thống đối thoại [9,18,20,30,37,41]. Về mặt truy xuất thông tin, nhiều nghiên cứu đã chỉ ra rằng câu hỏi làm rõ có thể giúp giải quyết các truy vấn mơ hồ và cải thiện trải nghiệm người dùng. Ví dụ, Wang và Li [43] thấy rằng các truy vấn tìm kiếm thường ngắn và ý định người dùng cơ bản thường mơ hồ. Họ đề xuất một model sinh câu hỏi làm rõ được hướng dẫn bởi mẫu hiệu quả, sử dụng Transformer để chọn một mẫu câu hỏi từ danh sách các ứng cử viên mẫu và điền vào slot câu hỏi từ từ vựng slot. Eberhart và McMillan [12] đề xuất một phương pháp mới để đặt câu hỏi làm rõ cho việc tinh chỉnh truy vấn, sử dụng thuật toán trích xuất nhiệm vụ để xác định các khía cạnh truy vấn và tuân theo quy trình dựa trên quy tắc để sinh câu hỏi. Về mặt lĩnh vực hệ thống đối thoại, cả phương pháp dựa trên quy tắc và dựa trên học đều đã được đề xuất. Dhole [9] đề xuất một phương pháp mới sinh câu hỏi phân biệt bằng cách tận dụng hệ thống dựa trên quy tắc đơn giản, nhằm tìm kiếm làm rõ từ người dùng, từ đó giảm tính robot của cuộc trò chuyện và làm cho tương tác trở nên tự nhiên đáng kể. Rao et al. [37] mô tả một phương pháp sinh câu hỏi làm rõ, sử dụng model seq2seq để sinh câu hỏi khi cho một ngữ cảnh và sử dụng một model seq2seq khác để sinh câu trả lời khi cho ngữ cảnh và câu hỏi.

Trong sinh code, việc xử lý các yêu cầu người dùng mơ hồ đã nhận được ít sự chú ý cho đến nay. Theo hiểu biết tốt nhất của chúng tôi, Li et al. [25] là bài báo nghiên cứu duy nhất giải quyết việc giải quyết yêu cầu mơ hồ cho sinh code. Công trình này nhằm làm rõ các yêu cầu mơ hồ thiếu các hoạt động chính, ví dụ: lời gọi API. Nó đầu tiên thu thập một dataset có tên Code ClarQA chứa các yêu cầu ngôn ngữ tự nhiên, code, câu hỏi làm rõ, và câu trả lời. Sau đó, nó đề xuất một pipeline sinh code có thể chọn câu hỏi làm rõ liên quan và câu trả lời của chúng từ dataset cho một yêu cầu đã cho để sinh giải pháp code. Tuy nhiên, phạm vi ứng dụng cho công trình này bị hạn chế. Thứ nhất, nó chủ yếu tập trung vào việc làm rõ sự mơ hồ ở cấp hoạt động, để lại các hình thức mơ hồ khác, chẳng hạn như sự mơ hồ ngữ nghĩa trong yêu cầu ngôn ngữ tự nhiên, ít được giải quyết hiệu quả hơn. Hơn nữa, nó phụ thuộc nhiều vào dataset được xây dựng, truy xuất câu hỏi liên quan cho các yêu cầu mơ hồ. Nếu dataset thiếu các yêu cầu tương tự, hiệu suất của phương pháp có thể bị ảnh hưởng. Khác với công trình này, ClarifyGPT không bị giới hạn trong một loại làm rõ yêu cầu mơ hồ cụ thể. Và ClarifyGPT có thể sinh câu hỏi chính xác và có mục tiêu cho các yêu cầu khác nhau bằng cách tận dụng khả năng hiểu mạnh mẽ của LLM.

## 3 PHƯƠNG PHÁP

Trong phần này, chúng tôi giới thiệu ClarifyGPT, một framework sinh code cho LLM. Hình 1 minh họa tổng quan về ClarifyGPT, bao gồm bốn giai đoạn chính: (1) Sinh Đầu vào Kiểm tra (Mục 3.1), nhằm sinh đầu vào kiểm tra chất lượng cao cho một yêu cầu đã cho bằng cách sử dụng kỹ thuật prompting và đột biến heuristic; (2) Kiểm tra Tính nhất quán Code (Mục 3.2), để tận dụng các đầu vào kiểm tra được sinh để tiến hành đánh giá tính nhất quán, và sau đó xác định các yêu cầu mơ hồ; (3) Sinh câu hỏi dựa trên lý luận (Mục 3.3), tập trung vào việc sinh câu hỏi làm rõ có mục tiêu cho các yêu cầu mơ hồ được xác định bằng cách nhắc LLM tham gia vào lý luận trung gian; (4) Sinh Code Cải tiến (Mục 3.4), kết hợp các câu hỏi làm rõ và phản hồi của chúng để tinh chỉnh yêu cầu ban đầu và sinh giải pháp code cuối cùng dựa trên prompt được tinh chỉnh. Dưới đây, chúng tôi cung cấp chi tiết cho từng giai đoạn trong ClarifyGPT.

### 3.1 Sinh Đầu vào Kiểm tra

Trong bước này, ClarifyGPT nhằm tạo ra đầu vào kiểm tra chất lượng cao để phân biệt hiệu quả giữa các giải pháp code với các chức năng khác nhau. Có nhiều nghiên cứu đã cố gắng sử dụng LLM để sinh test case đơn vị [24,38,42] và đã thể hiện hiệu suất ấn tượng. Theo công trình trước [29], ClarifyGPT tận dụng LLM làm trình sinh đầu vào kiểm tra và sinh đầu vào kiểm tra bằng cách áp dụng phương pháp hai bước (tức là khởi tạo đầu vào seed và đột biến có nhận thức kiểu). Cụ thể, ClarifyGPT bắt đầu bằng việc thiết kế prompt để hướng dẫn LLM tạo một tập hợp đầu vào seed. Sau đó, nó thực hiện đột biến có nhận thức kiểu để sinh ra số lượng lớn đầu vào mới. Những hiểu biết của chúng tôi là: (1) một mặt, vì LLM sở hữu khả năng hiểu và lý luận mạnh mẽ, việc sử dụng chúng làm trình sinh đầu vào kiểm tra có thể tạo ra đầu vào chất lượng cao vẫn hợp lệ ngay cả dưới các ràng buộc ngữ nghĩa. Lấy ví dụ được hiển thị trong Hình 4, vấn đề trong HumanEval yêu cầu chuỗi đầu vào phải chứa dấu ngoặc đơn cân bằng. Các trình sinh đầu vào truyền thống thường gặp thách thức trong việc đảm bảo tuân thủ các ràng buộc ngữ nghĩa như vậy. (2) mặt khác, LLM không phù hợp cho việc sinh kiểm tra tự động với số lượng lớn do tốc độ và chi phí không mong muốn khi truy vấn các model lớn như vậy [29]. Do đó, chúng tôi sử dụng phương pháp dựa trên đột biến heuristic để tăng tốc việc sinh ra nhiều test case, đảm bảo cả tính ổn định và độ tin cậy.

#### 3.1.1 Khởi tạo Đầu vào Seed

ClarifyGPT bắt đầu với việc thiết kế prompt để khởi tạo đầu vào seed. Như được hiển thị trong Hình 3 (a), prompt bao gồm ba phần: (1) một hướng dẫn, được thiết kế để khiến LLM sinh đầu vào kiểm tra phức tạp, khó, và trường hợp góc; (2) các minh họa được tạo thủ công few-shot, bao gồm yêu cầu người dùng và đầu vào kiểm tra ground-truth, có thể hỗ trợ LLM hiểu rõ hơn nhiệm vụ được mô tả trong hướng dẫn; (3) một truy vấn, mà LLM sinh kiểm tra đầu vào dựa trên nó. Cụ thể, trước tiên chúng tôi hoàn thiện prompt với hướng dẫn, minh họa, và yêu cầu đã cho. Sau đó, ClarifyGPT sử dụng prompt để truy vấn LLM nhằm sinh đầu vào seed. Cuối cùng, chúng tôi thu thập những đầu vào seed được sinh này để khởi tạo một pool seed sẽ được sử dụng cho đột biến.

#### 3.1.2 Đột biến Đầu vào Có nhận thức Kiểu

Sau khi khởi tạo pool seed, ClarifyGPT sử dụng chiến lược đột biến đầu vào có nhận thức kiểu [29] để sinh đầu vào kiểm tra chất lượng cao hơn. Cụ thể, phương pháp của chúng tôi tuân theo quy trình fuzzing dựa trên đột biến tiêu chuẩn [49,50]: (1) Tại mỗi vòng lặp, một đầu vào được chọn ngẫu nhiên từ pool seed. (2) Đối với đầu vào được chọn, chúng tôi kiểm tra các kiểu dữ liệu của nó và thực hiện một hoạt động đột biến duy nhất phù hợp với kiểu của nó để tạo test case mới. Các đột biến cơ bản được sử dụng cho các kiểu đầu vào khác nhau được minh họa trong Hình 2. Đối với các kiểu dữ liệu đơn giản, chẳng hạn như int và float, một hoạt động đột biến đơn giản tăng hoặc giảm giá trị của nó đi 1. Đối với các kiểu hợp chất, chúng tôi đột biến các phần tử dựa trên kiểu bên trong của chúng. (3) Sau khi hoàn thành một vòng đột biến, chúng tôi thêm các đầu vào mới được sinh vào pool seed và lặp lại quy trình nêu trên cho đến khi chúng tôi đạt được số lượng đầu vào được sinh mong muốn.

### 3.2 Kiểm tra Tính nhất quán Code

Một yêu cầu người dùng rõ ràng nên dễ hiểu và không để lại chỗ cho diễn giải, trong khi một yêu cầu người dùng mơ hồ có thể dẫn đến các bên liên quan diễn giải nó theo những cách khác nhau. Được lấy cảm hứng từ điều này, chúng tôi đưa ra giả định rằng, đối với một yêu cầu đã cho, nếu LLM sinh ra nhiều giải pháp code với các chức năng khác nhau, nó báo hiệu rằng yêu cầu có thể dẫn đến LLM diễn giải nó theo những cách khác nhau. Do đó, yêu cầu như vậy cần làm rõ và tinh chỉnh thêm. Dựa trên giả định này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả để xác định các yêu cầu mơ hồ. Đầu tiên, chúng tôi đưa một yêu cầu đã cho vào LLM để lấy mẫu n giải pháp code. Sau đó, những giải pháp code này được thực thi với đầu vào kiểm tra được sinh trong bước trước. Chúng tôi thu được đầu ra kiểm tra của những chương trình này và so sánh đầu ra kiểm tra để kiểm tra xem chúng có giống hệt nhau hay không. Nếu các đầu ra giống hệt nhau, ClarifyGPT coi những giải pháp code này đang diễn giải yêu cầu theo cùng một cách, do đó xác định yêu cầu là không mơ hồ. Trong trường hợp này, một trong những code được lấy mẫu sẽ được xuất ra làm giải pháp code cuối cùng. Tuy nhiên, nếu các đầu ra không giống hệt nhau, ClarifyGPT tin rằng LLM có những hiểu biết khác nhau về yêu cầu này khi nó tạo ra giải pháp code và xác định yêu cầu là mơ hồ. Đối với những yêu cầu mơ hồ này, như được hiển thị trong Hình 1, chúng tôi thực hiện clustering code để chia những giải pháp code này thành nhiều nhóm dựa trên đầu ra kiểm tra của chúng. Tiếp theo, ClarifyGPT chọn ngẫu nhiên một giải pháp code từ mỗi nhóm và đưa những giải pháp code không nhất quán này vào thành phần tiếp theo để tổng hợp prompt được sử dụng để đặt câu hỏi.

### 3.3 Sinh Câu hỏi Dựa trên Lý luận

Các câu hỏi làm rõ có mục tiêu tạo thuận lợi cho người dùng trong việc diễn đạt ý định của họ một cách rõ ràng, đảm bảo rằng những phản hồi thu được có liên quan trực tiếp đến những phần không rõ ràng trong yêu cầu. Các câu hỏi mơ hồ hoặc rộng tăng rủi ro nhận được phản hồi ngoài chủ đề hoặc không liên quan, có thể làm tổn hại hiệu suất của sinh code. Do đó, khi xác định các yêu cầu mơ hồ, việc trao quyền cho LLM khả năng đặt ra câu hỏi chính xác và có mục tiêu trở nên thiết yếu. Để đạt được mục tiêu này, chúng tôi thiết kế prompt dựa trên lý luận nhằm hướng dẫn LLM ban đầu xem xét kỹ lưỡng các yếu tố góp phần vào sự mơ hồ của yêu cầu và sau đó xây dựng câu hỏi có mục tiêu dựa trên phân tích. Prompt được thiết kế được mô tả trong Hình 3 (b). Nó bao gồm ba phần: (1) một hướng dẫn, mô tả nhiệm vụ (tức là sinh câu hỏi làm rõ) chúng tôi muốn LLM giải quyết; (2) các bộ ba <yêu cầu, giải pháp không nhất quán, câu hỏi làm rõ> few-shot làm minh họa, giúp LLM hiểu và giải quyết nhiệm vụ; (3) một truy vấn, chứa yêu cầu người dùng và giải pháp code của nó, được đưa vào LLM để sinh câu hỏi.

Cụ thể, ClarifyGPT xây dựng prompt để hướng dẫn LLM phân tích các yếu tố góp phần vào yêu cầu không rõ ràng bằng cách hiểu các chức năng của những giải pháp code không nhất quán này và so sánh sự khác biệt của chúng. Động lực là, trong phát triển phần mềm, các giải pháp code đại diện cho việc triển khai cụ thể của yêu cầu. Nếu một yêu cầu mơ hồ, các nhà phát triển khác nhau có thể có những diễn giải khác nhau và do đó viết code khác nhau. Một số trong những giải pháp code không nhất quán này là không chính xác hoặc không phù hợp với ý định ban đầu. Bằng cách so sánh những triển khai code khác nhau này, các điểm mơ hồ tiềm năng trong yêu cầu có thể được xác định dễ dàng. Sau khi phát hiện các điểm mơ hồ trong yêu cầu, LLM tiếp tục sinh câu hỏi làm rõ có mục tiêu dựa trên kết quả phát hiện.

Prompting được đề xuất của chúng tôi chia sẻ ý tưởng tương tự với prompting Chain of Thought (CoT) [47], khiến LLM sinh các bước lý luận trung gian (phân tích các yếu tố góp phần vào sự mơ hồ) trước, và sau đó tạo ra kết quả cuối cùng (câu hỏi làm rõ có mục tiêu) dựa trên những bước lý luận trung gian này. Theo cách này, ClarifyGPT khuyến khích LLM thực hiện "lập kế hoạch xa" [6], cho phép chúng tận dụng tốt hơn khả năng lý luận và hiểu của mình để nâng cao chất lượng của các câu hỏi được sinh.

### 3.4 Sinh Code Cải tiến

Khi phản hồi của người dùng được ghi lại, ClarifyGPT kết hợp chúng với các câu hỏi được sinh để tinh chỉnh yêu cầu ban đầu thành một yêu cầu rõ ràng. Cụ thể, như được hiển thị trong truy vấn trong Hình 3 (d), chúng tôi ghép đôi mỗi câu hỏi và câu trả lời tương ứng của nó để tạo ra một làm rõ, sau đó được thêm vào cuối docstring để tạo thành yêu cầu được tinh chỉnh. Bằng cách tinh chỉnh yêu cầu mơ hồ theo cách này, chúng tôi có thể bảo tồn tính toàn vẹn cấu trúc của docstring trong yêu cầu ban đầu trong khi tăng cường nó với thông tin làm rõ bổ sung. Tiếp theo, chúng tôi sử dụng yêu cầu được tinh chỉnh để xây dựng prompt hướng dẫn LLM sinh giải pháp code cuối cùng. Prompt được xây dựng cũng bao gồm ba phần, tức là một hướng dẫn, một số minh họa, và một truy vấn, như được mô tả trong Hình 3 (d).

## 4 THIẾT KẾ THÍ NGHIỆM

Để đánh giá hiệu quả của ClarifyGPT, chúng tôi tiến hành thí nghiệm toàn diện. Trong phần này, chúng tôi minh họa thiết kế thí nghiệm của mình, bao gồm câu hỏi nghiên cứu, model, benchmark, metric, baseline, và chi tiết triển khai.

### 4.1 Câu hỏi Nghiên cứu

Chúng tôi giải quyết ba câu hỏi nghiên cứu sau để đánh giá hiệu suất của ClarifyGPT.

RQ1: ClarifyGPT hoạt động như thế nào khi nhận phản hồi người dùng thực so với các phương pháp baseline? Trong các kịch bản thế giới thực, ClarifyGPT của chúng tôi hỗ trợ người dùng viết code bằng cách tương tác với họ, tức là đặt câu hỏi làm rõ và nhận phản hồi của người dùng. Do đó, trong RQ này, chúng tôi khám phá xem ClarifyGPT với con người trong vòng lặp có thể đạt được hiệu suất cao hơn so với các baseline sinh code hiện có hay không. Vì việc đánh giá sinh code tương tác với người tham gia con người tốn kém, chúng tôi chỉ chọn GPT-4 làm model cơ sở và thuê mười người tham gia (bao gồm các nhà nghiên cứu học thuật và nhà phát triển ngành) để trả lời thủ công các câu hỏi làm rõ được sinh bởi ClarifyGPT. Chúng tôi so sánh ClarifyGPT với ba baseline trên hai benchmark (tức là MBPP-sanitized và MBPP-ET).

RQ2: ClarifyGPT hoạt động như thế nào khi nhận phản hồi người dùng mô phỏng so với các phương pháp baseline hiện đại? RQ này thực hiện đánh giá tự động quy mô lớn của ClarifyGPT trên các LLM và benchmark khác nhau mà không yêu cầu sự tham gia của người dùng, nhằm xác minh thêm xem ClarifyGPT có thể đạt được hiệu suất cao hơn so với các baseline sinh code hiện có hay không. Trước tiên chúng tôi đề xuất phương pháp mô phỏng người dùng tận dụng LLM để mô phỏng phản hồi của người dùng. Sau đó, chúng tôi áp dụng ba baseline và ClarifyGPT cho hai LLM đại diện (tức là GPT-4 và ChatGPT), và đánh giá hiệu suất của chúng trên bốn benchmark được sử dụng rộng rãi (tức là HumanEval, MBPP-sanitized, HumanEval-ET, và MBPP-ET).

RQ3: Số lượng minh họa trong prompt ảnh hưởng như thế nào đến hiệu suất của ClarifyGPT? Kỹ thuật prompting có thể nhạy cảm với số lượng minh họa [14,32]. Trong câu hỏi nghiên cứu này, chúng tôi đo hiệu suất của ClarifyGPT với số lượng minh họa khác nhau để điều tra tính mạnh mẽ của prompt của ClarifyGPT.

### 4.2 LLM được Nghiên cứu

Có nhiều LLM có sẵn để sinh code. Tuy nhiên, bối cảnh cụ thể của công trình này đòi hỏi LLM phải sở hữu một mức độ năng lực giao tiếp nhất định, tức là khả năng hiểu hướng dẫn của con người và xây dựng câu hỏi làm rõ. Do đó, LLM không có instruction tuning (ví dụ: InCoder [13] và CodeGen [33]) không phù hợp làm model cơ sở được áp dụng cho framework ClarifyGPT. Trong công trình này, chúng tôi chọn hai chat-LLM đại diện (tức là ChatGPT và GPT4) làm model cơ sở để đánh giá framework ClarifyGPT.

• ChatGPT [34] là một trong những model chat mạnh nhất được OpenAI trao quyền. Nó được huấn luyện bằng phương pháp mới gọi là Reinforcement Learning from Human Feedback (RLHF), tích hợp một cách liền mạch reinforcement learning và phản hồi con người. Cụ thể, ChatGPT đầu tiên được huấn luyện với lượng lớn văn bản ngôn ngữ tự nhiên và file code. Sau đó, nó được fine-tuned thông qua reinforcement learning, cho phép nó hiểu và thực thi hướng dẫn của con người một cách thành thạo. Trong thí nghiệm của chúng tôi, chúng tôi sử dụng API của OpenAI để truy cập model ChatGPT, tức là gpt-3.5-turbo.

• GPT-4 [35] là LLM tiên tiến nhất của OpenAI, có thể chấp nhận đầu vào hình ảnh và văn bản, phát ra đầu ra văn bản. Nó cũng được huấn luyện với reinforcement learning và học cách tuân theo hướng dẫn của con người. GPT-4 đã thể hiện khả năng hiểu ngôn ngữ được cải thiện, cho phép nó hiểu các ngữ cảnh phức tạp và tinh tế, làm cho nó rất hiệu quả trong nhiều nhiệm vụ downstream, bao gồm tóm tắt văn bản, dịch thuật và sinh code [6]. Trong thí nghiệm của chúng tôi, chúng tôi sử dụng API của OpenAI để truy cập model GPT-4, tức là gpt-4-turbo.

### 4.3 Benchmark

Theo công trình trước [7,11,17,26], chúng tôi tiến hành thí nghiệm trên bốn benchmark sinh code công khai: HumanEval [8], MBPP-sanitized [4], cùng với các phiên bản test case mở rộng của chúng (tức là HumanEval-ET và MBPP-ET [10]). Thống kê của những benchmark này được hiển thị trong Bảng 1.

• HumanEval [8] là dataset giải quyết vấn đề được viết tay được chế tác sau ngày cắt của dataset huấn luyện Codex, bao gồm 164 vấn đề lập trình Python. Các vấn đề lập trình trong HumanEval liên quan đến hiểu ngôn ngữ, thuật toán và toán học. Mỗi vấn đề bao gồm chữ ký hàm, yêu cầu ngôn ngữ tự nhiên và một số unit test. Một vấn đề được coi là được giải quyết bởi code-LLM khi tất cả unit test được vượt qua.

• MBPP-sanitized [4] là tập con được xác minh thủ công của dataset MBPP (Mostly Basic Programming Problems), chứa 427 vấn đề lập trình Python được crowdsource, liên quan đến thao tác số, chức năng thư viện tiêu chuẩn, và hơn thế nữa. Mỗi vấn đề chứa chữ ký hàm, yêu cầu người dùng và ba test case.

• HumanEval-ET và MBPP-ET [10] là hai phiên bản mở rộng của benchmark HumanEval và MBPP với trung bình 100+ test case bổ sung cho mỗi vấn đề. Để cải thiện độ tin cậy của đánh giá code được sinh, họ thu thập nhiều test case cạnh không được bao gồm trong benchmark gốc.

### 4.4 Metric Đánh giá

Chúng tôi đánh giá độ chính xác của code được sinh bằng metric Pass@k [22]. Metric này phục vụ như một ước lượng khả năng sinh dưới ngân sách cụ thể, được sử dụng rộng rãi trong các nghiên cứu liên quan đến LLM trước [7,23,51]. Đối với mỗi vấn đề trong benchmark, chúng tôi sinh k giải pháp code, và nếu bất kỳ giải pháp code nào trong k giải pháp code vượt qua tất cả kiểm tra, vấn đề này được coi là được giải quyết. Trong các kịch bản phát triển thế giới thực, việc sinh k code sẽ đặt gánh nặng lên nhà phát triển, tức là họ cần đọc và hiểu k code khác nhau và chọn một làm code mục tiêu. Do đó, trong bài báo này, k được đặt thành 1, thỏa mãn hầu hết các kịch bản mà nhà phát triển chỉ xem xét code được sinh duy nhất [11,17]. Để tránh phương sai cao và tính ngẫu nhiên, chúng tôi chạy mỗi phương pháp ba lần và báo cáo kết quả trung bình làm kết quả cuối cùng.

### 4.5 Baseline So sánh

• Default LLM: lấy các yêu cầu ban đầu trực tiếp từ benchmark làm đầu vào để nhắc LLM sinh code.

• CoT (Chain-of-Thought) [47]: sinh một loạt bước lý luận cho mỗi yêu cầu bằng cách sử dụng prompt CoT và sau đó sinh code tương ứng. Để đảm bảo sự công bằng trong so sánh, baseline CoT có cùng số lượng minh họa (tức là ba minh họa) và seed minh họa.

• GPT-Engineer²: là repository Github mã nguồn mở gần đây. Nó sử dụng hướng dẫn được thiết kế thủ công để khiến LLM đặt câu hỏi làm rõ cho yêu cầu đầu vào của người dùng và sau đó sinh đoạn code dựa trên phản hồi của người dùng.

### 4.6 Chi tiết Triển khai

Chi tiết triển khai xây dựng prompt và cấu hình model trong ClarifyGPT như sau.

Xây dựng Prompt. Vì bốn benchmark không có tập huấn luyện, theo công trình trước [44,47], chúng tôi chọn ba vấn đề đầu tiên từ mỗi benchmark và trích xuất yêu cầu người dùng từ những vấn đề này làm seed minh họa. Tiếp theo, chúng tôi tạo thủ công các minh họa riêng biệt cho các prompt khác nhau, như được minh họa trong Hình 3. Cần lưu ý rằng lý do chúng tôi chỉ tạo ba minh họa cho mỗi prompt là do giới hạn độ dài đầu vào của LLM.

Cấu hình Model. Chúng tôi coi hai LLM được sử dụng trong thí nghiệm là trình sinh hộp đen và chỉ đặt một vài tham số giao diện mà chúng cung cấp mà không truy cập các tham số bên trong. Đối với tất cả LLM, chúng tôi đặt top p thành 0.95, frequency_penalty thành 0. Max_tokens đại diện cho số lượng token tối đa được sinh, được đặt thành 800 cho prompt đặt câu hỏi làm rõ và 300 cho các prompt khác. Cụ thể, chúng tôi đặt temperature thành 0, ngoại trừ khi lấy mẫu giải pháp code, temperature được đặt thành 0.8. Chúng tôi theo Chen et al. [8] để cắt bớt nội dung được sinh trong HumanEval và MBPP bằng năm chuỗi dừng: "\nclass", "\ndef", "\n#", "\nif", và "\nprint".

## 5 KẾT QUẢ VÀ PHÂN TÍCH

### 5.1 RQ1: ClarifyGPT hoạt động như thế nào khi nhận phản hồi người dùng thực so với các phương pháp baseline?

Thiết lập. Trong RQ này, chúng tôi khám phá ClarifyGPT hoạt động như thế nào trong các kịch bản thế giới thực, tức là ClarifyGPT có thể đạt được hiệu suất cao hơn so với các baseline sinh code hiện có khi nhận phản hồi người dùng thực hay không. Cụ thể, chúng tôi áp dụng ClarifyGPT cho model GPT-4. Vì benchmark MBPP-ET chia sẻ cùng yêu cầu người dùng với MBPP-sanitized, chúng tôi chỉ áp dụng ClarifyGPT cho các phiên bản gốc của benchmark (tức là MBPP-sanitized) và báo cáo hiệu suất của ClarifyGPT trên hai benchmark này bằng cách sử dụng unit test tương ứng của chúng. ClarifyGPT đầu tiên lấy yêu cầu người dùng của mỗi vấn đề trong benchmark làm đầu vào và xác định chúng là mơ hồ hoặc không mơ hồ. Sau đó, nó sinh câu hỏi làm rõ cho các yêu cầu mơ hồ (như được hiển thị trong Hình 1). Tổng cộng, chúng tôi thu được 140 vấn đề với yêu cầu mơ hồ từ benchmark MBPP-sanitized. Số lượng câu hỏi làm rõ trung bình cho mỗi vấn đề là 2.85. Chúng tôi chế tác ba bảng câu hỏi giống hệt nhau cho mỗi vấn đề, đảm bảo rằng mỗi vấn đề sẽ được đánh giá bởi ba người tham gia khác nhau. Mỗi bảng câu hỏi bao gồm ba yếu tố: (1) yêu cầu (mơ hồ) của vấn đề, mô tả ý định của vấn đề; (2) các unit test case chứa ví dụ đầu vào-đầu ra mong đợi, hỗ trợ người tham gia hiểu ý định của vấn đề; (3) các câu hỏi làm rõ được sinh, mà người tham gia được yêu cầu trả lời.

Chúng tôi tuyển mười người tham gia, bao gồm ba sinh viên tiến sĩ, hai sinh viên thạc sĩ, hai nhà nghiên cứu cấp cao và ba nhà phát triển ngành. Không ai trong số họ là đồng tác giả của bài báo này. Tất cả người tham gia có ít nhất ba năm kinh nghiệm phát triển Python, với sáu người trong số họ có hơn năm năm kinh nghiệm. Người tham gia ban đầu được cung cấp mô tả nhiệm vụ và bảng câu hỏi ví dụ chứa câu trả lời câu hỏi phù hợp. Sau khi hoàn thành bài tập đào tạo, chúng tôi giao 42 vấn đề cho mỗi người tham gia và yêu cầu họ trả lời các câu hỏi làm rõ dựa trên thông tin được cung cấp trong bảng câu hỏi. Mỗi vấn đề sẽ được giải quyết bởi ba người tham gia.

Chúng tôi thu thập các câu trả lời được cung cấp bởi người tham gia và đưa chúng vào ClarifyGPT để sinh giải pháp code cuối cùng. Như đã đề cập trước đó, chúng tôi đánh giá tính đúng đắn của code được sinh trên hai benchmark bằng cách sử dụng unit test case. Vì câu hỏi làm rõ của mỗi vấn đề được trả lời bởi ba người tham gia, chúng tôi báo cáo kết quả Pass@1 trung bình.

Kết quả. Kết quả so sánh giữa hiệu suất của ClarifyGPT và các baseline khác được mô tả trong Bảng 2. Các giá trị màu đỏ là cải thiện tương đối của ClarifyGPT so với baseline Default.

Chúng ta có thể thấy rằng ClarifyGPT (Human Feedback) đạt được hiệu suất cao nhất trên tất cả bốn benchmark. So với Default, ClarifyGPT (Human Feedback) thể hiện hiệu suất vượt trội về metric Pass@1, đạt được sự tăng 13.87% trên MBPP-sanitized và 16.83% trên MBPP-ET. Hơn nữa, khi so sánh với baseline hoạt động tốt nhất (tức là CoT hoặc GPT-Engineer), ClarifyGPT (Human Feedback) cũng cải thiện hiệu suất Pass@1 thêm 9.53% trên MBPP-sanitized và 9.52% trên MBPP-ET. Điều này chủ yếu vì ClarifyGPT có thể thành thạo xác định các yêu cầu mơ hồ và đưa ra câu hỏi làm rõ có mục tiêu. Người dùng dễ dàng làm rõ ý định của họ bằng cách trả lời những câu hỏi này, do đó tạo thuận lợi cho việc sinh code chính xác hơn bởi LLM. Nó chỉ ra rằng ClarifyGPT, như một framework sinh code tương tác, có thể hỗ trợ các nhà phát triển viết code trong bối cảnh phát triển thế giới thực.

Trả lời RQ1: Trong đánh giá con người, ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 trên MBPP-sanitized từ 70.96% lên 80.8%; nâng cao hiệu suất của nó trên MBPP-ET từ 51.52% lên 60.19%. Cải thiện tương đối là 15.35% trung bình, vượt trội hơn các baseline.

### 5.2 RQ2: ClarifyGPT hoạt động như thế nào khi nhận phản hồi người dùng mô phỏng so với các phương pháp baseline hiện đại?

Thiết lập. Do sự tham gia của người tham gia con người, việc đánh giá framework sinh code tương tác ClarifyGPT rất tốn kém và khó tái tạo. Một giải pháp tương đối đơn giản là tiến hành đánh giá ngoại tuyến [3]. Tuy nhiên, nó giới hạn hệ thống chỉ chọn câu hỏi làm rõ từ một tập hợp câu hỏi được định nghĩa trước hoặc được gắn nhãn, điều này không chuyển giao tốt sang môi trường phát triển thực tế. Trong RQ này, chúng tôi áp dụng phương pháp User Simulation for Evaluation [15,39] để tạo thuận lợi cho đánh giá tự động của ClarifyGPT trên các LLM và benchmark khác nhau, loại bỏ sự cần thiết tham gia trực tiếp của người dùng.

Khía cạnh quan trọng nhất của việc mô phỏng phản hồi người dùng là đảm bảo rằng phản hồi người dùng được tạo ra gần giống với phản hồi thực mà người dùng sẽ cung cấp trong cùng môi trường. Mô phỏng độ chính xác thấp có thể dẫn đến ClarifyGPT nhận phản hồi khó gặp trong thực tế, từ đó mang lại kết quả sai lệch và ảnh hưởng đến đánh giá hiệu suất của ClarifyGPT. Do đó, chúng tôi đề xuất phương pháp mô phỏng người dùng độ chính xác cao tận dụng LLM để sinh phản hồi người dùng bằng cách cung cấp cho LLM câu hỏi làm rõ và test case ground-truth. Hiểu biết chính của chúng tôi là test case ground-truth chứa ví dụ đầu vào-đầu ra mong đợi, phản ánh chức năng mong muốn mà người dùng tìm kiếm. Việc trao cho LLM kiến thức trước này tạo thuận lợi cho việc hiểu ý định người dùng và cho phép sinh phản hồi người dùng mô phỏng độ chính xác cao. Để hướng dẫn LLM giải quyết nhiệm vụ này, chúng tôi thiết kế prompt (như được hiển thị trong Hình 3), cũng bao gồm ba phần: (1) một hướng dẫn, mô tả nhiệm vụ (tức là mô phỏng phản hồi người dùng) chúng tôi muốn LLM giải quyết; (2) các bộ bốn <yêu cầu, kiểm tra ground-truth, câu hỏi làm rõ, câu trả lời> few-shot làm minh họa, giúp LLM hiểu và giải quyết nhiệm vụ; (3) một truy vấn, chứa yêu cầu người dùng và kiểm tra ground-truth của nó, được đưa vào LLM để sinh phản hồi mô phỏng.

Chúng tôi áp dụng ba baseline (Mục 4.5) và ClarifyGPT của chúng tôi cho hai SOTA LLM (Mục 4.2). Chúng tôi đánh giá chúng trên bốn benchmark (Mục 4.3) và so sánh hiệu suất của chúng bằng cách tính metric Pass@1 (Mục 4.4). Để so sánh công bằng, tất cả baseline đều áp dụng cùng thiết lập thí nghiệm như ClarifyGPT của chúng tôi.

Kết quả. Bảng 3 trình bày kết quả so sánh giữa hiệu suất của ClarifyGPT (Simulated Feedback) và các baseline khác về sinh code. Các giá trị màu đỏ là cải thiện tương đối của ClarifyGPT (Simulated Feedback) so với baseline Default.

Nhìn chung, ClarifyGPT (Simulated Feedback) có thể cải thiện đáng kể hiệu suất sinh code, đạt được lợi ích trên các LLM và dataset khác nhau. Đối với model GPT-4, so với baseline Default, ClarifyGPT (Simulated Feedback) thể hiện cải thiện đáng chú ý trong hiệu suất Pass@1, đạt được sự tăng 11.34% trên dataset HumanEval, 10.35% trên HumanEval-ET, 10.89% trên MBPP-sanitized, và 13.49% trên MBPP-ET. Đối với model ChatGPT, khi so sánh với baseline Default, ClarifyGPT (Simulated Feedback) cải thiện hiệu suất Pass@1 lần lượt 15.10%, 13.12%, 12.98%, và 19.07% trên bốn benchmark. Kết quả chứng minh rằng ClarifyGPT, trao quyền cho LLM tự động sinh câu hỏi làm rõ và tinh chỉnh yêu cầu người dùng dựa trên phản hồi người dùng, tạo thuận lợi cho người dùng làm rõ ý định của họ, từ đó tăng cường hiệu suất sinh code bằng cách nắm bắt ý định người dùng.

Chúng tôi cũng lưu ý rằng, so với baseline liên quan nhất (tức là GPT-Engineer), thể hiện hiệu suất vượt trội về metric Pass@1, đạt được cải thiện trung bình 11.45%, 8.65%, 6.95%, và 8.56% trên bốn benchmark. Chúng tôi quy cải thiện này cho các kỹ thuật mới của chúng tôi, tức là xác định yêu cầu mơ hồ và sinh câu hỏi làm rõ. Việc đặt câu hỏi làm rõ cho mọi yêu cầu người dùng dẫn đến tương tác LLM-Người không cần thiết về các yêu cầu không mơ hồ, điều này đặt gánh nặng bổ sung lên người dùng và làm tổn hại hiệu suất sinh code khi tạo ra câu hỏi ngoài chủ đề. Trong khi ClarifyGPT có thể xác định hiệu quả các yêu cầu mơ hồ mà không cần bất kỳ huấn luyện có giám sát nào bằng cách tiến hành kiểm tra tính nhất quán code. Các đoạn code không nhất quán được lấy làm đầu vào để giúp ClarifyGPT xây dựng câu hỏi có mục tiêu hướng dẫn người dùng làm rõ sự mơ hồ.

Bên cạnh đó, chúng tôi quan sát thấy rằng hiệu suất của ClarifyGPT (Human Feedback) cao hơn một chút so với ClarifyGPT (Simulated Feedback). Điều này cho thấy phương pháp mô phỏng người dùng của chúng tôi có thể sinh phản hồi người dùng không đáp ứng ý định của người dùng. Tuy nhiên, cả hai phương pháp đều có thể cải thiện đáng kể hiệu suất sinh code và đạt được lợi ích nhất quán trên các LLM và benchmark khác nhau, chứng minh độ tin cậy của kết quả đánh giá phương pháp mô phỏng của chúng tôi.

Trả lời RQ2: ClarifyGPT (Simulated Feedback) cải thiện hiệu suất trung bình (Pass@1) của GPT-4 trên bốn benchmark từ 68.02% lên 75.75%, cải thiện hiệu suất trung bình của ChatGPT trên bốn benchmark từ 58.55% lên 67.22%. Cải thiện tương đối của chúng lần lượt là 11.52% và 15.07%, và cải thiện trung bình là 13.27%.

### 5.3 RQ3: Số lượng minh họa trong prompt ảnh hưởng như thế nào đến hiệu suất của ClarifyGPT?

Thiết lập. Trong RQ này, chúng tôi điều tra xem việc tăng hoặc giảm số lượng minh họa có ảnh hưởng đến hiệu suất của ClarifyGPT (Simulated Feedback) trong nhiệm vụ sinh code hay không. Cụ thể, do giới hạn độ dài đầu vào của LLM, chúng tôi thay đổi số lượng minh họa trong prompt từ không đến ba. Sau đó, chúng tôi áp dụng hai LLM cho ClarifyGPT và các biến thể của nó, và đánh giá hiệu suất của chúng trên bốn benchmark. Chúng tôi chạy những phương pháp này ba lần và báo cáo kết quả Pass@1 trung bình làm báo cáo cuối cùng.

Kết quả. Bảng 4 trình bày so sánh hiệu suất giữa ClarifyGPT và các biến thể của nó. Nhìn chung, ClarifyGPT thể hiện tính mạnh mẽ đối với số lượng minh họa trong prompt. Khi thay đổi số lượng minh họa từ không đến ba, ClarifyGPT luôn vượt trội hơn baseline Default trên hai LLM và bốn benchmark.

Chúng ta có thể quan sát thấy rằng, như mong đợi, hiệu suất của ClarifyGPT tăng theo số lượng minh họa. Cụ thể, khi số lượng minh họa trong prompt được tăng từ không đến ba, liên quan đến ChatGPT, ClarifyGPT đạt được sự tăng hiệu suất trung bình từ 59.77% lên 67.22% trên bốn benchmark. Đối với model GPT-4, hiệu suất trung bình của ClarifyGPT tăng từ 68.59% lên 75.75%. Điều này chủ yếu vì nhiều minh họa hơn có thể cung cấp đa dạng tình huống và thông tin cho LLM, cho phép chúng hiểu rõ hơn ngữ cảnh của vấn đề và giải pháp cần thiết. Hơn nữa, LLM có thể học để tổng quát hóa tốt hơn thông qua minh họa, tức là suy ra giải pháp cho tình huống mới từ minh họa đã biết. Điều này cho phép LLM thích ứng tốt hơn với các đầu vào và yêu cầu khác nhau.

Chúng tôi cũng thấy rằng hiệu suất của ClarifyGPT trong thiết lập zero-shot thể hiện cải thiện nhỏ so với baseline Default, trong khi hiệu suất của nó trong thiết lập one-shot được tăng cường đáng kể so với baseline Default. Chúng tôi quy sự khác biệt này cho thực tế rằng trong thiết lập zero-shot, ClarifyGPT được mong đợi sinh phản hồi có ý nghĩa mà không có bất kỳ minh họa nào, điều này có thể đặc biệt thách thức đối với các nhiệm vụ phức tạp (ví dụ: yêu cầu LLM sinh câu hỏi làm rõ có mục tiêu). Hơn nữa, prompting zero-shot chỉ dựa vào kiến thức được huấn luyện trước của LLM và cách diễn đạt của prompt đã cho, có thể không cung cấp đủ hướng dẫn hoặc ràng buộc cho LLM để tạo ra phản hồi chính xác hoặc có liên quan ngữ cảnh. Ngược lại, hiệu suất của ClarifyGPT với thiết lập one-shot cao hơn đáng kể so với thiết lập zero-shot và gần với hiệu suất của ClarifyGPT với thiết lập three-shot. Điều này chỉ ra rằng ClarifyGPT có hiệu suất tổng quát hóa mạnh khi chỉ cung cấp một minh họa. Chúng tôi tin rằng trong các kịch bản sử dụng thực tế, việc sử dụng ClarifyGPT trong thiết lập one-shot có thể phục vụ như một sự đánh đổi giữa hiệu quả và hiệu suất.

Trả lời RQ3: Nhìn chung, ClarifyGPT thể hiện tính mạnh mẽ đối với số lượng minh họa trong prompt. Khi thay đổi số lượng minh họa từ không đến ba, ClarifyGPT luôn vượt trội hơn baseline Default trên hai LLM và bốn benchmark.

## 6 THẢO LUẬN

### 6.1 Nghiên cứu Trường hợp

Để đánh giá thêm hiệu quả của phương pháp chúng tôi, chúng tôi tiến hành phân tích định tính. Như được hiển thị trong Hình 4, chúng tôi chọn hai ví dụ đại diện từ hai benchmark sinh code phổ biến (tức là HumanEval và MBPP). Mỗi yêu cầu đầu vào bao gồm chữ ký hàm và mô tả NL. Chúng tôi lấy ChatGPT [34] làm model cơ sở và sử dụng hai baseline (tức là Default và GPT-Engineer) và ClarifyGPT để sinh giải pháp code cho mỗi yêu cầu đầu vào.

Đối với ví dụ đầu tiên lấy từ MBPP-sa, mô tả "viết một hàm để sắp xếp danh sách các phần tử" không chỉ định xem hàm này có nên được sắp xếp theo thứ tự tăng dần hay giảm dần. ChatGPT mặc định trực tiếp sinh giải pháp code sắp xếp danh sách đã cho theo thứ tự giảm dần, không vượt qua test case ground-truth. GPT-Engineer đặt năm câu hỏi làm rõ và sinh giải pháp code chính xác dựa trên phản hồi người dùng. Tuy nhiên, một số trong những câu hỏi đó không mang thông tin và có thể được trả lời bằng thông tin trong yêu cầu đã cho. Ví dụ, câu trả lời cho câu hỏi thứ ba "Có bất kỳ ràng buộc cụ thể nào cho thuật toán sắp xếp?" có thể được suy ra bằng tên hàm comb_sort được đề cập trong chữ ký hàm. Câu hỏi thứ năm "Có ngôn ngữ lập trình ưa thích nào không?" cũng có vẻ tầm thường, vì chúng ta có thể dễ dàng biết rằng hàm nên được triển khai trong Python dựa trên cú pháp của chữ ký hàm. Việc trả lời những câu hỏi này không thể thu được thông tin bổ sung; thay vào đó, nó sinh ra đối thoại thừa có hại cho trải nghiệm người dùng. Hơn nữa, nó dẫn đến sự tăng số lượng token cho cả đầu vào và đầu ra của LLM, do đó làm tăng chi phí hoạt động. Ngược lại, ClarifyGPT có thể xác định các điểm mơ hồ trong yêu cầu bằng cách so sánh các triển khai code khác nhau, từ đó đặt câu hỏi làm rõ có mục tiêu. Kết quả là, ClarifyGPT chỉ đặt một câu hỏi "Việc sắp xếp có nên theo thứ tự tăng dần hay giảm dần?" và sinh giải pháp code chính xác.

Đối với ví dụ thứ hai, yêu cầu người dùng được định nghĩa rõ ràng. ChatGPT mặc định sinh giải pháp chính xác, trong khi GPT-Engineer tạo ra giải pháp code không chính xác. Sự khác biệt này chủ yếu phát sinh từ việc GPT-Engineer không thể xác định xem một yêu cầu có mơ hồ hay không. Do đó, ngay cả đối với yêu cầu không mơ hồ này, GPT-Engineer vẫn đặt ba câu hỏi, hóa ra không mang thông tin. Hơn nữa, những câu hỏi này góp phần làm cho prompt được tinh chỉnh quá dài, có thể gây nhầm lẫn cho LLM. Ngược lại, ClarifyGPT của chúng tôi có thể xác định xem một yêu cầu có cần làm rõ hay không bằng cách tiến hành kiểm tra tính nhất quán code. Vì vậy chúng ta có thể thấy ClarifyGPT không đưa ra bất kỳ câu hỏi nào cho yêu cầu này mà thay vào đó trực tiếp tạo ra giải pháp chính xác.

### 6.2 Lợi ích và Hạn chế

Trong phần này, chúng tôi thảo luận về một số lợi ích và hạn chế tiềm năng của ClarifyGPT của chúng tôi.

Lợi ích. (1) Ngược lại với các phương pháp sinh code dựa trên LLM hiện tại [7,23,27] tận dụng kỹ thuật hậu xử lý để lấy mẫu một pool ứng cử viên code đáng kể và sau đó chọn một, ClarifyGPT nhằm trực tiếp làm rõ yêu cầu đầu vào bằng cách đặt câu hỏi làm rõ. Do đó, framework của chúng tôi góp phần tăng cường khả năng diễn giải trong code được sinh bởi LLM. Bằng cách làm rõ chi tiết cụ thể trong yêu cầu hoặc thêm kiến thức bổ sung vào chúng, người dùng có thể dễ dàng nhận biết những thay đổi tương ứng trong code kết quả. Điều này góp phần cung cấp hướng dẫn cho người dùng về cách xây dựng yêu cầu để cải thiện sinh code, từ đó tạo thuận lợi cho hiểu biết rõ ràng hơn về code được sinh. (2) ClarifyGPT của chúng tôi cải thiện kỹ năng tương tác của LLM bằng cách trao quyền cho chúng khả năng tự động đặt câu hỏi làm rõ cho các yêu cầu mơ hồ. Theo cách này, nó phục vụ để tạo thuận lợi cho người dùng xác định sự mơ hồ trong yêu cầu và cung cấp hướng dẫn làm rõ ý định của họ mà không yêu cầu người dùng ban đầu sinh code và sau đó đọc và phân tích code để tinh chỉnh yêu cầu. Do đó, ClarifyGPT tăng cường trải nghiệm người dùng và hiệu quả sản xuất.

Hạn chế. (1) Lý tưởng nhất, framework của chúng tôi có thể áp dụng cho tất cả LLM. Tuy nhiên, ClarifyGPT đòi hỏi LLM phải sở hữu một mức độ năng lực giao tiếp nhất định, tức là khả năng hiểu hướng dẫn của con người và xây dựng câu hỏi làm rõ. Do đó, LLM có thể áp dụng cho framework của chúng tôi bị hạn chế, tức là LLM không có instruction tuning (ví dụ: InCoder [13] và CodeGen [33]) không phù hợp làm model cơ sở được áp dụng cho framework ClarifyGPT. (2) Do sử dụng kiểm tra tính nhất quán code để xác định xem một yêu cầu có cần làm rõ hay không, ClarifyGPT được yêu cầu sinh đầu vào kiểm tra cho yêu cầu và so sánh đầu ra kiểm tra của các giải pháp được lấy mẫu. Do đó, ClarifyGPT không phù hợp để sinh code với đầu vào phức tạp (ví dụ: hình ảnh hoặc file). Ngoài ra, đối với một số code không trả về giá trị đầu ra (ví dụ: chương trình deep learning), việc sử dụng ClarifyGPT cũng có thể chịu một số hạn chế.

### 6.3 Mối đe dọa đối với Tính hợp lệ

Mối đe dọa đầu tiên đối với tính hợp lệ là khả năng rò rỉ dữ liệu. Vì những LLM này được huấn luyện trên các repository code mã nguồn mở, có thể một số benchmark công khai đã được bao gồm trong dữ liệu huấn luyện của chúng. Điều này có thể làm thiên lệch đánh giá của chúng tôi về phương pháp được đề xuất, vì một số đầu ra model có thể bị ảnh hưởng bởi việc tiếp xúc trước với những benchmark này. Để giảm thiểu mối đe dọa này, chúng tôi cẩn thận chọn HumanEval [8], MBPP-sanitized [4], và các phiên bản mở rộng tương ứng của chúng để đánh giá. HumanEval là dataset giải quyết vấn đề được chế tác thủ công, được OpenAI giới thiệu để đánh giá hiệu suất của Codex. MBPP-sanitized, mặt khác, là tập con được xác minh thủ công của dataset MBPP, bao gồm 427 vấn đề Python đã trải qua xác minh crowdsource. Những dataset này đã trải qua xem xét thủ công tỉ mỉ và đã được sử dụng rộng rãi trong các nghiên cứu trước [7, 23, 46].

Mối đe dọa thứ hai đối với tính hợp lệ là mô phỏng người dùng để đánh giá. Do sự tham gia của người tham gia con người, việc đánh giá ClarifyGPT, một framework sinh code tương tác, rất tốn kém và khó tái tạo. Do đó, chúng tôi đề xuất phương pháp mô phỏng người dùng để tạo thuận lợi cho đánh giá tự động của ClarifyGPT trên các LLM và benchmark khác nhau. Tuy nhiên, mô phỏng độ chính xác thấp có thể dẫn đến ClarifyGPT nhận phản hồi khó gặp trong thực tế, từ đó mang lại kết quả sai lệch và ảnh hưởng đến đánh giá hiệu suất của ClarifyGPT. Để giảm thiểu mối đe dọa này, chúng tôi thiết kế prompt đặc biệt để cung cấp cho LLM câu hỏi làm rõ và test case ground-truth. Bằng cách trao cho LLM kiến thức trước này, ClarifyGPT tạo thuận lợi cho việc hiểu ý định người dùng của LLM và cho phép sinh phản hồi người dùng mô phỏng độ chính xác cao. Kết quả cho thấy hiệu suất của ClarifyGPT (Simulated Feedback) rất gần với ClarifyGPT (Human Feedback), chứng minh rằng phương pháp mô phỏng được đề xuất của chúng tôi có thể phục vụ như một proxy tốt cho đánh giá tự động của ClarifyGPT, loại bỏ sự cần thiết tham gia trực tiếp của người dùng.

Mối đe dọa thứ ba liên quan đến khả năng tổng quát hóa của kết quả thí nghiệm của chúng tôi. Để giải quyết vấn đề này, một mặt, chúng tôi đã cẩn thận chọn hai chat LLM đại diện (ChatGPT và GPT-4) làm model cơ sở và bốn dataset phổ biến làm đối tượng đánh giá. Chúng tôi áp dụng hai LLM cho ClarifyGPT của chúng tôi và đánh giá hiệu suất của chúng trên bốn dataset này. Mặt khác, xem xét tính nhạy cảm vốn có của LLM đối với prompt, chúng tôi chạy baseline và ClarifyGPT ba lần để giúp giảm thiểu phương sai cao và tính ngẫu nhiên. Chúng tôi báo cáo kết quả trung bình làm kết quả cuối cùng. Kết quả cho thấy ClarifyGPT của chúng tôi có thể cải thiện đáng kể hiệu suất của tất cả LLM, đạt được lợi ích nhất quán trên các dataset khác nhau. Do đó, chúng tôi tin rằng ClarifyGPT có khả năng tổng quát hóa tốt, và có thể hoạt động hiệu quả trong nhiều bối cảnh liên quan.

## 7 KẾT LUẬN

Trong bài báo này, được thúc đẩy bởi quan sát rằng các nhà phát triển con người thường đặt câu hỏi làm rõ khi họ đối mặt với yêu cầu mơ hồ, chúng tôi cho rằng việc trao quyền cho LLM khả năng tự động làm rõ yêu cầu mơ hồ có thể cải thiện sinh code. Để đạt được mục tiêu này, chúng tôi đề xuất ClarifyGPT, một framework sinh code cho phép LLM xác định các yêu cầu mơ hồ và sinh câu hỏi làm rõ có mục tiêu. Cụ thể, ClarifyGPT bao gồm bốn giai đoạn chính, tức là sinh đầu vào kiểm tra, kiểm tra tính nhất quán code, sinh câu hỏi dựa trên lý luận, và sinh code cải tiến. Đối với một yêu cầu đã cho, ClarifyGPT đầu tiên sinh đầu vào kiểm tra chất lượng cao bằng cách sử dụng kỹ thuật prompting và đột biến heuristic. Sau đó, nó sử dụng đầu vào kiểm tra được sinh để tiến hành đánh giá tính nhất quán và xác định các yêu cầu mơ hồ. Tiếp theo, ClarifyGPT xây dựng câu hỏi làm rõ có mục tiêu cho các yêu cầu mơ hồ được xác định bằng cách nhắc LLM tham gia vào lý luận trung gian. Cuối cùng, nó kết hợp các câu hỏi làm rõ và phản hồi của chúng để tinh chỉnh yêu cầu ban đầu và sinh giải pháp code cuối cùng dựa trên prompt được tinh chỉnh. Trong phần đánh giá, trước tiên chúng tôi áp dụng GPT-4 cho ClarifyGPT và tuyển mười người tham gia để đánh giá hiệu suất của nó trên hai benchmark công khai. Kết quả đánh giá con người cho thấy ClarifyGPT đạt được cải thiện tương đối lên đến 16.83% trong Pass@1 so với baseline Default. Ngoài ra, để tự động hóa đánh giá ClarifyGPT, chúng tôi giới thiệu phương pháp mô phỏng độ chính xác cao để mô phỏng phản hồi người dùng. Chúng tôi tiến hành thí nghiệm toàn diện trên bốn benchmark (tức là HumanEval, HumanEval-ET, MBPP-sanitized, và MBPP-ET) sử dụng hai LLM (tức là GPT-4 và ChatGPT). Kết quả rộng rãi minh họa rằng ClarifyGPT cải thiện hiệu suất trung bình của GPT-4 trên bốn benchmark từ 68.02% lên 75.75%, và cải thiện hiệu suất trung bình của ChatGPT trên bốn benchmark từ 58.55% lên 67.22%. Do đó, chúng tôi tin rằng ClarifyGPT có thể tạo thuận lợi đáng kể cho việc ứng dụng thực tế của LLM trong môi trường phát triển thế giới thực.
