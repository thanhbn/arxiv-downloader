# 2309.07804.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2309.07804.pdf
# File size: 678093 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pop Quiz! Do Pre-trained Code Models Possess
Knowledge of Correct API Names?
Terry Yue Zhuo
Monash University,
CSIRO’s Data61
Australia
terry.zhuo@monash.eduXiaoning Du
Monash University
Australia
xiaoning.du@monash.eduZhenchang Xing
CSIRO’s Data61,
Australian National University
Australia
zhenchang.xing@data61.csiro.auJiamou Sun
CSIRO’s Data61
Australia
frank.sun@data61.csiro.au
Haowei Quan
Monash University
Australia
haowei.quan@monash.eduLi Li
Beihang University
China
lilicoding@ieee.orgLiming Zhu
CSIRO’s Data61,
University of New South Wales
Australia
liming.zhu@data61.csiro.au
Abstract —Recent breakthroughs in pre-trained code models,
such as CodeBERT and Codex, have shown their superior
performance in various downstream tasks. The correctness and
unambiguity of API usage among these code models are crucial
for achieving desirable program functionalities, requiring them
to learn various API fully qualified names structurally and
semantically. Recent studies reveal that even state-of-the-art pre-
trained code models struggle with suggesting the correct APIs
during code generation. However, the reasons for such poor
API usage performance are barely investigated. To address this
challenge, we propose using knowledge probing as a means
of interpreting code models, which uses cloze-style tests to
measure the knowledge stored in models. Our comprehensive
study examines a code model’s capability of understanding
API fully qualified names from two different perspectives: API
call and API import. Specifically, we reveal that current code
models struggle with understanding API names, with pre-training
strategies significantly affecting the quality of API name learning.
We demonstrate that natural language context can assist code
models in locating Python API names and generalize Python API
name knowledge to unseen data. Our findings provide insights
into the limitations and capabilities of current pre-trained code
models, and suggest that incorporating API structure into the
pre-training process can improve automated API usage and code
representations. This work provides significance for advancing
code intelligence practices and direction for future studies. All
experiment results, data and source code used in this work are
available at https://doi.org/10.5281/zenodo.7902072.
I. I NTRODUCTION
Recent advances in code intelligence have incorporated pre-
training techniques, where models are pre-trained on large-
scale unlabeled source code corpora to learn the code’s rep-
resentation and semantics. The pre-trained code models, such
as CodeBERT [1] and GraphCodeBERT [2], can be fine-tuned
for various downstream code tasks, such as code completion
and translation [3]–[5].
Despite the improvement, there is still a significant perfor-
mance gap between their performance and that of human de-
velopers when it comes to using APIs correctly. For instance,
several studies have demonstrated that even state-of-the-artpre-trained code models, such as Codex [6], StarCoder [7]
and GPT-4 [8], struggle with suggesting the correct APIs
during code generation [9], [10]. However, few studies have
investigated the reasons behind these models’ poor API usage
performance.
Basically, correct API usage depends on two types of
knowledge: how to invoke an API and which API to invoke,
with the former being a fundamental step towards the latter. To
invoke an API, one must have knowledge of the code grammar
of importing libraries and composing the correct API name
based on the import and call statements. However, code models
are not explicitly instructed by code grammar. Although we
often assume that the models can learn to use APIs effectively
by observing a large number of examples, this assumption
has been poorly validated. Therefore, we pose the following
question: Do Pre-trained Code Models Possess Knowledge
of Correct API Names?
The fully qualified name of an API comprises not only the
function name but also the name of the package, module,
and/or class to which it belongs. To ease the presentation,
we will use the term “module” to refer to these entities
below. Libraries usually organize APIs into nested modules
to help developers understand the available features when
using the APIs. APIs are typically organized into nested
modules within libraries to aid developers in understanding
the available features when using the APIs. These modules
form a hierarchical structure, with the higher-level module
serving as the parent of its direct lower-level modules, and
the APIs acting as the leaf nodes in the structure. The fully
qualified API names are obtained by traversing the hierarchy
from the root to the leaf, with all names connected by dots.
Depending on the imported module’s level and whether it is
associated with an alias, the API name used for invocation
should be adjusted accordingly. This convention can be easily
understood by human developers, but code models may find
it challenging to learn.
In addition, the fully qualified names of APIs convey infor-arXiv:2309.07804v1  [cs.SE]  14 Sep 2023

--- PAGE 2 ---
numpy
all any ma linalg
getmask array make_mask multi_dot cholesky qr...
...Fig. 1: An example of module design of <numpy> library.
mation about code modularization and API namespace design.
If models can learn a good representation of API names,
they could be useful for automating API namespace design
by providing more precise and relevant design options. For
example, consider the Python library <numpy> , which is used
for mathematical operations on arrays and matrices. As shown
in Figure 1, <numpy> includes several modules, including
<linalg> (short for linear algebra) for APIs related to
computations in linear algebra, such as <multi_dot> (dot
product), <cholesky> (Cholesky decomposition), and <qr>
(QR factorization), as well as <ma> (short for masked array)
for APIs related to operations on masked arrays. Such names-
pace design can provide valuable insights when designing
relevant libraries or similar libraries for other programming
languages.
To understand how well pre-trained code models understand
API names, it is essential to analyze and interpret the internal
mechanisms of pre-trained code models. However, deep neural
models are generally complex and opaque [11], making it hard
to fully understand their internal mechanisms. In the natural
language processing community, the mainstream technique to
examine what pre-trained language models know is probing,
where a probe is a simple classifier that takes the model’s
contextual representations as input and predicts a property of
interest [12]. Recently, researchers have attempted to analyze
the code syntax captured by code models via attention analysis
and grammatical structure probing [13]–[15]. However, there
is an insubstantial effort on assessing API knowledge [16].
In this work, we explore the interpretability of code models
and introduce a large-scale knowledge probing task that
specifically targets fully qualified names of APIs. We design
an automated and effective evaluation framework, INK, to
probe code models’ understanding of API names with cloze-
style pop quizzes. To generate the quizzes, we first extract API
full qualified names from API calls in large-scale source code
corpora that are commonly used for training code models.
Note that, instead of directly extracting API names from
libraries, we take into account the limitations of code models
during their training. We refrain from testing the models on
the knowledge they have never been exposed to. Further, we
derive import statements and API call statements based on
the API names and mask out different tokens, with only one
token masked out at a time, at each module level of these
statements. For instance, when invoking <multi_dot>
module as shown in Figure 1, we consider from two aspects,
API call ( <numpy.linalg.multi_dot> ) and API import(<from numpy.linalg import multi_dot> ). By
masking token lin by “[MASK] ”, we get two quizzes
of<numpy.[MASK]alg.multi_dot> and <from
numpy.[MASK]alg import multi_dot> . The code
models are then expected to predict the masked token given
an import or API call statement with the mask applied.
To better understand the capabilities and limitations of
pre-trained code models in learning API names, we set out
to investigate several research questions (RQs) to assess their
performance and guide future improvements in this domain:
•RQ1: How well do code models understand API calls and
imports? The code models are assessed on their prediction
of masked import or API call statements. The findings
can help identify areas where code models may struggle
and guide improvements in their API name learning by
understanding this question.
•RQ2: Do code models understand API import aliases?
This RQ further assesses code models’ understanding of API
import aliases. The quizzes are designed based on an alias-
defining import statement followed by an API call based on
the imported module. Tokens are selectively masked out for
the statements.
•RQ3: Will natural language context affect the API name
knowledge probing results? We investigate whether the
overall performance of the models can be improved by
incorporating natural language queries. The findings can
guide the development of techniques that leverage contextual
information to enhance code models’ API name learning.
•RQ4: How well can code models memorize and gen-
eralize on API names? We divide APIs into two groups
based on whether they are seen during the training phase.
The outcomes indicate if the code models possess robust
generalization capabilities and appropriate memorization
skills. Models’ ability to apply learned knowledge to unseen
APIs would be helpful for API namespace design.
For the evaluation, we construct the first benchmark on
Python API name knowledge, PyINK , and analyze 10 pre-
trained code models, including the variants of CodeBERT [1],
GraphCodeBERT [2], PLBART [17]. Our work is complemen-
tary to developing more advanced techniques for modeling
API representations in code, thereby enhancing the efficiency
and accuracy of code models. Additionally, the insights gained
from this work can pave the way for a deeper understanding
of how pre-training impacts the performance of code models,
facilitating more informed design decisions in this domain. In
summary, our main contributions include:
•A cloze-style evaluation framework INK, to probe and
benchmark the knowledge of API names in pre-trained
code models.
•An implementation of INK, which lowers the bar for
designing probing techniques on API name knowledge.
•An evaluation dataset based on INK,PyINK , containing
diverse API name cloze-style pop quizzes.
•A comprehensive study on understanding the Python API
name knowledge of pre-trained code models via PyINK .

--- PAGE 3 ---
II. B ACKGROUND AND RELATED WORK
This section provides a comprehensive overview of the
background and related work that form the foundation of
our research. Firstly, we introduce three prominent families
of pre-trained code models, namely CodeBERT [1], Graph-
CodeBERT [2], and PLBART [17], which have been widely
adopted in recent studies. Besides, we present a review on
knowledge probing in language models, which has emerged
as a crucial research area for enhancing the interpretability
and understanding of such models. Finally, we discuss how
existing works on deep API learning are different from our
study. Through these discussions, we establish the contextual
and theoretical framework necessary to fully appreciate the
novelty and significance of our proposed approach.
A. Pre-trained Code Models
Pre-trained language models, such as BERT [18], are com-
monly utilized for knowledge transfer in various downstream
natural language processing tasks [19]. These models are
trained on extensive NL corpora and fine-tuned on small
labeled datasets for different tasks. They capture contextual
linguistic information and eliminate the necessity for task-
specific models. Similarly, pre-trained code models, such as
CodeBERT, GraphCodeBERT, and PLBART, have been de-
veloped to leverage the “naturalness” of software [20]. These
models excel in various code-related downstream tasks, such
as code completion, code-to-text summarization, and code-to-
code translation.
1) CodeBERT: CodeBERT is a sophisticated multilingual
pre-trained code model built on the BERT architecture. It
excels in understanding the semantic connections between nat-
ural language (NL) and programming language (PL) through
masked language modeling and replaced token detection.
CodeBERT achieves state-of-the-art results in NL code search
and code documentation generation tasks, making it a valuable
tool for academic writing and review.
Pre-trained on the CSNet dataset [21], CodeBERT encom-
passes a diverse range of NL and PL instances, including
code snippets, comments, and unimodal code. Its impressive
performance is achieved without relying on explicit indicators
to differentiate PLs. By fine-tuning its parameters, CodeBERT
consistently demonstrates exceptional proficiency, reinforcing
its position as a top choice for academic writers and reviewers.
2) GraphCodeBERT: GraphCodeBERT is a pre-trained
code model that revolutionizes code understanding by con-
sidering the inherent structure of code. Unlike other models
that rely on complex abstract syntax trees (AST), Graph-
CodeBERT leverages data flow during pre-training to capture
the semantic-level structure and encode variable relationships.
By focusing on the "where-the-value-comes-from" aspect,
GraphCodeBERT achieves more efficient and effective code
representation [2].
Pre-trained on the same CSNet dataset as CodeBERT,
GraphCodeBERT employs three distinct objectives during pre-
training. It involves masked language modeling, data flow
edge prediction, and variable alignment across source codeand data flow. This comprehensive approach enables Graph-
CodeBERT to excel in four key downstream tasks: text-code
search, clone detection, code translation, and code refinement,
outperforming CodeBERT in all of these areas. The model’s
superior performance validates its efficacy and establishes
GraphCodeBERT as a cutting-edge solution in the field of
code analysis and understanding.
3) PLBART: PLBART is a bidirectional and autoregressive
code model that exhibits exceptional proficiency in perform-
ing a wide range of code summarization, generation, and
translation tasks. The design of PLBART was inspired by
BART [22], which is a language model based on denois-
ing autoencoding sequence-to-sequence pre-training. With a
similar setup, PLBART is pre-trained using three denoising
pre-training objectives, namely, token masking, token deletion,
and token infilling. The first two strategies involve randomly
sampling tokens and replacing them with a mask token or
deleting them from the input sequence. Conversely, in token
infilling, a Poisson distribution ( λ= 3.5) is used to draw the
lengths of text spans that are sampled and replaced with a
single mask token. In each instance, 35% of the tokens are
masked.
In contrast to CodeBERT and GraphCodeBERT, PLBART is
pre-trained on a vast collection of Java and Python functions,
as well as NL descriptions sourced from GitHub and Stack-
Overflow. Based on evaluations, PLBART has demonstrated
remarkable generalization capabilities and superior perfor-
mance when applied to several multilingual downstream tasks.
B. Knowledge Probing
The evaluation of internal representations and knowledge
of language models is a fundamental and critical process,
which involves the technique of knowledge probing [23]. This
approach entails presenting a set of questions or statements
to the model to assess its understanding of specific concepts
or relationships. The inputs are typically presented as cloze
sentences with particular concepts or relationships masked
as discrete prompts to test the model’s performance. We
formalize the knowledge probing approach by considering the
input cloze sentence, denoted as S, where “Alan Turing was
born in [ MASK ]” is an example.
Formally, we define the knowledge probing approach as
follows,
f(S) =1
|S||S|X
k=1log(P(tk)|S;θ), tk∈ V(θ) (1)
where θrepresents the model parameters, V(θ)denotes the
vocabulary learned by the language model, and tkis a token
inside the model’s vocabulary V(θ). The contextualized likeli-
hood f(S)represents the possibility of replacing [ MASK ] with
the token tkas per the model’s prediction. The final prediction
corresponds to the token tkthat maximizes f(S).
Knowledge probing is an essential technique for identifying
areas where the model requires improvement, understanding
how the model processes and represents information, and

--- PAGE 4 ---
_?_ensorflow .compat.v2.boolean_mask
tensorflow .compat.v2.boolean_mask✅
tensorflow .compat. _?_2.boolean_mask
tensorflow .compat. py2.boolean_mask❌
tensorflow .compat. v2.boolean_mask
✅
tensorflow .compat. v2._?_olean_mask
tensorflow .compat.v2. boolean_mask✅
tensorflow .comp at.v2.boolean_mask✅
tensorflow ._?_at.v2.boolean_mask<tensorflow>
from _?_ensorflow .compat.v2
import boolean_mask
from tensensorflow .compat.v2
import boolean_mask
<v2>
<compat> <boolean_mask>from Tensorflow .compat.v2
import boolean_mask
from tensorflow .compat.v2
import boolean_mask
from tensorflow ._?_at.v2
import boolean_mask
from tensensorflow .compat.v2
import boolean_mask✅
from tensorflow .compat.v2
import  _?_olean_mask
from t ensorflow .compat. mask 2
import boolean_mask
from tensorflow .compat. v2
import boolean_mask
from tensorflow .compat.v2
import _?_olean_mask
from tensensorflow .compat.v2
import boolean_mask✅
✅
✅
❌
❌
❌<tensorflow>
<compat> <boolean_mask><v2>
INK
INKINK
INKINK
INKINK
INKAPI Call
API ImportFig. 2: API fully qualified names such as <tensorflow.compat.v2.boolean_mask> can be formalized into the cloze-
style tests from two perspectives, API calls and API imports. The example shows the top- kpredictions of CodeBERT-MLM
model for each test. Note that the dialogues are for illustration only.
exploring the underlying knowledge. Furthermore, knowledge
probing enables the development of more robust and reliable
language models that reflect human understanding of language
and the world.
Factual probing is an early application of prompting meth-
ods in natural language processing, with the goal of quanti-
fying the factual knowledge encoded in pre-trained language
models. This task involves transforming the input into a cloze
prompt, either manually crafted or automatically discovered, to
retrieve knowledge. Relevant datasets such as LAMA [24] and
X-FACTR [25] have been established for evaluating models
in fact retrieval. Researchers have explored discrete template
search [24], [26], [27] and continuous template learning, as
well as prompt ensemble learning [12], [28], to improve the
effectiveness of factual probing. The existing body of research
demonstrates that pre-trained language models contain sub-
stantial factual knowledge, which can be efficiently accessed
using various prompting methods.
C. Generation-based API Recommendation
API recommendation is a challenging task that involves
providing a specific API based on an NL query. Previous
research has focused on two approaches: (1) Rank-based
recommendation and (2) Generation-based recommendation.
Rank-based API recommendation utilizes a knowledge graph
or knowledge base to search for the most suitable APIs based
on the semantic meaning of the NL query [29]–[31]. As these
methods do not require learning, we discuss another line of
work where deep-learning-based approaches are used for API
recommendation to generate API sequences using NL queries,
close to the scope of our work.
DeepAPI [32] was the first to tackle this task by formulating
it as a machine translation problem and employing an end-
to-end supervised learning model. Subsequently, researchers
have investigated the effectiveness of fine-tuning pre-trained
code models to perform the API learning task [33], [34].
While these studies demonstrate that code models achievebetter performance on API learning, they suffer from four
main drawbacks: (1) Code models are only fine-tuned with a
few APIs and can not be generalized to generate APIs in the
wild. (2) Fine-tuning code models to synthesize API sequences
lacks interpretability for capturing API knowledge during pre-
training. (3) The API learning task is based on NL inputs that
only determine the semantic understanding of API sequence
usage. (4) Evaluation of API learning is solely based on BLEU
score [35], which measures the similarity between synthesized
API sequences and references and fails to reflect synthesis
correctness.
III.INK: AN EVALUATION FRAMEWORK OF API NAME
KNOWLEDGE
A. Motivation
Previous research in natural language processing has utilized
cloze sentences for token prediction as a means of interpret-
ing the knowledge encoded by pre-trained language models.
Building upon this work, we examine probing API name
knowledge in CodeBERT-MLM - a variant of CodeBERT pre-
trained solely on mask language modeling - with cloze-style
sentences serving as pop quizzes, as depicted in Figure 2. We
use<tensorflow.compat.v2.boolean
_mask> as an example and transform it into a cloze-style
pop quiz, as shown in Figure 2. In this study, we define
the API module levels as each hierarchical level within the
fully qualified name, separated by a period. There are four
module levels in the API call statement: (1) <tensorflow>
representing the top module level, (2) <compat> as the
second module level, (3) <v2> as the third module level,
and (4) <boolean_mask> as the last call level. For each
level, we request CodeBERT-MLM to fill in the blank via first
token prediction, as determined by its tokenizer. As our results
demonstrate, CodeBERT-MLM correctly predicts the masked
token on the first attempt, except for the third level of <v2> .
We contend that code models, such as CodeBERT-MLM, can
learn API names during function pre-training.

--- PAGE 5 ---
Step 2:
UnificationStep 3: API
pop quizzesRQ1:
API Call & API Import
Source Code
CopursUnified API Fully
Qualified NamesCode ModelStep 4:
Benchmarking
 Model V ocabularies
Step 1: API
Extraction
API Fully
Qualified NamesRQ2:
API Import Alias
RQ3:
Probing W ith NL  Context
RQ4:
Memorization &
Generalization
Fig. 3: An overview of the INK framework.
Given that <tensorflow.compat.v2.boolean
_mask> can be reconstituted to the form of an API import
statement, we then investigate how well CodeBERT-MLM
understands the API import statements. As demonstrated in
Figure 2, we transform the API import statement into four
cloze templates by masking the first token of each module
level, similar to the ones of the API call. From the results
illustrated in Figure 2, we discover that predicting some API
modules at the first shot is challenging for CodeBERT-MLM,
unlike the case of API call probing. This behavior suggests
that code models possess varying degrees of knowledge in
API import and API call statements.
Having identified several potential patterns from our pre-
liminary knowledge probing study, which provides clues to
API name knowledge, we find it necessary to further explore
this phenomenon through quantitative analysis and systematic
evaluation. Motivated by the aforementioned observations, this
paper investigates whether pre-trained code models learn the
API names and to what extent they store API name knowledge,
by conducting knowledge probing as the pop quiz. Specifically,
we analyze two perspectives of API names, i.e., API call and
API import, within the purview of code models.
B. API-to-Pop-Quiz Conversion
We consider three main types of transformation of evaluat-
ing API name knowledge: API calls ,API imports andAPI
import aliases . As aforementioned in Section III-A, cloze-
style pop quizzes are structured based on each call level split
by the delimiters of “.”, “from” and “import”. To benchmark
models fairly, we construct pop quizzes by unifying the entire
vocabulary of each model. For all the evaluations, we follow
previous work of knowledge probing in language models [24]
and choose to evaluate the prediction of a single token masking
in the pop quiz. We provide the detailed design of each process
as follows.
1)Evaluation Design on API Call :
We treat each API call as a modular pattern on the basis of
each module level. To formalize the pop quiz construction
on API calls, an example of API call <A.B.C> and a code
model Mare given to demonstrate the workflow. The modelMfirstly tokenizes the API as follows,
M(<A.B.C> )→
{tA
1, tA
2, . . . , tA
Na}, tdot,{tB
1, tB
2, . . . , tB
Nb}, tdot,
{tC
1, tC
2, . . . , tC
Nc}
where each trepresents the token produced by the model M,
andNrepresents the length of tokens in each level. For each
level, the tokens are grouped by {. . .}. When converting the
tokenized API to the pop quiz, we mask a specific token by
replacing twith a “ [MASK] ” in each level. To visualize the
pop quiz input, we mask the last token in the second module
level of <A.B.C> as follows:
<A.B.C> →<A.B’[MASK].C> →<A.B’ .C>
where B′is the concatenation of {tB
1, . . . , tB
Nb−1}. We prompt
the model Mto fill in the blank of <A.B’ .C> via mask
prediction.
2)Evaluation Design on API Import :
We explore the evaluation design on the API import statement
of “from . . .import . . . ”. Similarly, we consider the ex-
ample of <from A.B import C> . Using the model Mto
tokenize the API import, we can devise the following tokens:
M(<from A.B import C> )→
tfrom,{tA
1, tA
2, . . . , tA
Na}, tdot,
{tB
1, tB
2, . . . , tB
Nb}, timport,{tC
1, tC
2, . . . , tC
Nc}
We visualize an example of API import quiz, where the first
token in the bottom level of <from A.B import C> is
masked:
<from A.B import C>
→<from A.B import [MASK]C’>
→<from A.B import C’>
where C′is the concatenation of {tC
2, . . . , tC
Nc}. We probe
the model Mto fill in the blank of <from A.B import
C’> via mask prediction.

--- PAGE 6 ---
3)Evaluation Design on API Import Alias :
We note that import aliases are supported in some pro-
gramming languages, such as Python. For example, “ import
. . . as . . . ” and “ from . . .import . . . as . . . ” are the typical
import alias syntax. Therefore, we further examine pre-trained
code model’s understanding of the aliases of API calls after
packages and libraries imports. We illustrate the design choice
via the example of <import A as K \n K.B.C> , where
<K> is the alias and K.B.C is the API call statement. After
being tokenized by the model M, the example is formalized
as follows:
M(<import A as K \n K.B.C> )→
timport,{tA
1, tA
2, . . . , tA
Na}, tas,{tK
1, tK
2, . . . , tK
Na}, tnewline,
{tK
1, tK
2, . . . , tK
Na}, tdot,
{tB
1, tB
2, . . . , tB
Nb}, tdot,{tC
1, tC
2, . . . , tC
Nc}
We then convert the example to the following API alias pop
quiz with the masked last token of <C>:
<import A as K \n K.B.C>
→<import A as K \n K.B.C’[MASK]>
→<import A as K \n K.B.C’ >
where C′is the concatenation of {tC
1, . . . , tC
Nc−1}. We probe
the model Mto fill in the blank of <import A as K \n
K.B.C’ >via mask prediction. Note that we only mask the
tokens in the part of the API call after the alias name.
4)Evaluation of Code Models: A Unified Vocabulary
Approach :
Code models may tokenize the same API call or import
statement differently due to variations in their respective
vocabularies. For example, an API call such as <A.B.C>
may be tokenized as a single token by one model MAwith
vocabulary VA, while another model MBwith vocabulary
VBmay tokenize it into multiple tokens. To ensure a fair
comparison, we generate models over a unified vocabulary
that is the intersection of the vocabularies of all considered
code models. We define the unified vocabulary as the set of
tokens that are presented in the vocabularies of all considered
code models.
To evaluate the performance of code models on this unified
vocabulary, we categorize the evaluation of each name level
into two types: (1) partial module masking and (2) full module
masking . For the former, we tokenize the API call and import
statements into multiple tokens and mask only one of the
tokens. In contrast, when code models segment the entire
module level as a single token, we denote it as the full module
masking.
5)Benchmarking Code Models via Knowledge Probing :
We utilize the knowledge probing technique to effectively
evaluate and assess the performance of code models. The
primary objective of this approach is to delve into the code
models’ comprehension of API names and their proficiency ingenerating precise and contextually fitting tokens within those
names. To achieve this, we present cloze-style quizzes where
certain tokens are intentionally masked, prompting the code
model to predict the tokens for the masked positions. To gauge
the accuracy of these predictions, we compare them against
the ground-truth tokens in the API names. This benchmarking
process allows us to determine the correctness of predictions.
IV. E XPERIMENT SETUP
We introduce the research questions, the basic experimental
setup about the datasets and models, and the evaluation metrics
used throughout the evaluation. The research questions we aim
to answer include:
RQ1: How well do code models understand API calls and
imports?
RQ2: Do code models understand API import aliases?
RQ3: Will natural language context affect the API name
knowledge probing results?
RQ4: How well can code models memorize and generalize
on API names?
A.PyINK : Evaluation on Python API Name Knowledge
First Token Last Token Full Total
API Call 108,863 111,373 69,349 289,585
API Import 108,863 111,373 69,349 289,585
API Import Alias 7,385 7,122 3,464 17,971
TABLE I: Overview of the number of pop quizzes in PyINK .
We adopt the widely-used pre-training corpus, CSNet [21],
which is a collection of datasets and benchmarks for semantic
code retrieval, containing functions and their correspond-
ing comments of six programming languages extracted from
GitHub repositories, to evaluate the API name knowledge of
code models. We focus on the Python set, which contains
457,461 functions. To extract Python APIs from CSNet, we
use our proposed INK framework, which leverages static
analysis on the entire file to extract API usages, following
the method described in [36]. To achieve this, we clone
all repositories corresponding to the functions in CSNet and
analyze the API usage in each function. This process results
in a new benchmark, denoted as PyINK , which is designed to
evaluate the name knowledge of Python APIs. PyINK contains
597,141 main pop quizzes for evaluation, and we only consider
fullmodule masking, or partial masking of the first andlast
tokens of each module level to maintain consistency.
Our approach successfully extracts 79,754 unique APIs
from 8,294 Python libraries in 13,519 repositories, indicating
the diverse usage of Python APIs. Moreover, to ensure a fair
comparison, we unify the vocabularies in the considered code
models, resulting in 289,585, 289,585, and 17,971 samples
for API call, import, and import with aliases statements,
respectively, in the benchmark.
B. Code Models
We conduct extensive studies on ten selected models from
the variants of CodeBERT, GraphCodeBERT and PLBART.

--- PAGE 7 ---
We also include GPT-3.5-turbo for the assessment, though it
is not specifically trained with mask prediction. We instruct
GPT-3.5-turbo to predict top-20 masked tokens for each test
of 1,000 randomly selected samples on API calls and imports,
respectively. To guide GPT-3.5-turbo to correctly perform the
prediction task, we prompt it with the instruction of “Predict
top-20 answers of <mask> part of the following Python API
fully qualified name, pay attention to the connected characters
right after <mask> . Note the number of masked characters is
at least one. Print each of 20 answers per line with index.”.
The description of each model is summarized in Table II.
Model pre-trained Dataset Objective #Param Fine-tuned
CodeBERTMLM CSNet MLM 125M ✗
MLM-Python CSNet+CodeParrot* MLM 125M ✗
GraphCodeBERT MLM CSNet MLM* 125M ✗
PLBARTBase N/A DAE 140M ✗
Large N/A DAE 406M ✗
CSNet CSNet DAE 140M ✗
Sum N/A DAE 140M ✓
Gen N/A DAE 140M ✓
MT N/A DAE 140M ✓
GPT-3.5 turbo N/A N/A 154B ✓
TABLE II: The overview of selected code models for eval-
uation. Note that only the train split of CSNet is used dur-
ing pre-training. MLM: Masked Language Modeling. DAE:
Denoising Auto-Encoding. CodeBERT-MLM-Python uses the
Python split of CodeParrot [37] for continuous pre-training.
PLBART-Sum and PLBART-Gen are the PLBART-Base model
fine-tuned on CSNet Python code summarization and code
generation tasks, respectively. PLBART-MT adopts multitask
learning and is fine-tuned on both tasks that the previous two
models use.
C. Evaluation Metric
We present an evaluation methodology based on rank-based
metrics in the context of API name prediction. Our approach
involves computing results per test sample and means across
pop quizzes, utilizing the mean precision at k ( P@k) metric.
Specifically, P@kis computed as 1 if the target object is
ranked among the top k results, and 0 otherwise.
V. R ESULTS
A. RQ1: How well do code models understand API calls and
imports?
P@1↑P@5↑P@10 ↑P@20 ↑P@30 ↑P@40 ↑P@50 ↑API CallCodeBERTMLM 23.90 42.34 50.28 58.17 62.75 65.87 68.28
MLM-Python 29.35 47.51 55.67 63.61 68.06 71.06 73.33
GraphCodeBERT MLM 25.89 43.30 50.74 58.28 62.73 65.87 68.26
PLBARTBase 1.65 6.58 10.43 15.71 19.40 22.44 24.99
Large 2.29 8.63 13.22 18.62 22.19 25.02 27.40
CSNet 2.57 10.78 15.57 21.49 25.55 28.76 31.41
Sum 2.11 9.65 14.46 19.98 23.53 26.21 28.46
Gen 4.31 14.51 20.69 27.21 31.27 34.23 36.59
MT 4.79 15.80 22.71 30.27 34.83 38.10 40.67
GPT-3.5 turbo* 16.00 34.10 39.80 44.20 - - -API ImportCodeBERTMLM 26.33 43.87 51.19 58.63 62.89 65.85 68.16
MLM-Python 29.67 49.26 56.82 64.18 68.34 71.23 73.39
GraphCodeBERT MLM 30.76 48.24 55.35 62.50 66.68 69.55 71.73
PLBARTBase 2.64 12.96 20.12 28.21 33.22 36.90 39.83
Large 5.66 18.03 25.72 34.20 39.18 42.69 45.44
CSNet 3.44 14.90 21.95 30.31 35.79 39.86 43.11
Sum 2.88 12.26 18.90 26.09 30.44 33.61 36.18
Gen 3.83 15.25 23.59 32.52 37.72 41.38 44.25
MT 4.92 19.16 28.02 36.65 41.65 45.33 48.16
GPT-3.5 turbo* 20.30 37.80 45.20 50.50 - - -
TABLE III: P@kscores on selected code models, focusing
API calls and API imports.Our evaluation assesses the capability of pre-trained code
models to encode knowledge of Python API names for both
API calls and imports. We computed P@kscores for each
masking strategy and present the results in Table III. In
addition, we provide a few examples in Table IV. Firstly, we
have observed that the relative performances of different code
models remain consistent when we vary the value of kin
theP@kmetric. Secondly, as kincreases, the improvement
in performance for each model becomes less significant.
These observations provide strong evidence to support the
effectiveness of the PyINK benchmark. When comparing the
model variants, our analysis reveals that CodeBERT-MLM-
Python and GraphCodeBERT-MLM demonstrate superior per-
formance on API calls and imports compared to other models.
However, their overall precision of 30% measured by P@1
falls short of perfection, indicating a lack of knowledge about
API names. While we expected GPT-3.5-turbo to have a better
understanding of API names, it shows that the model performs
slightly worse than CodeBERT-MLM. Additionally, our com-
parison shows that PLBART variants perform much worse on
understanding Python API name knowledge than BERT-like
models, which can be explained by the pre-training objectives
of PLBART. PLBART-Large consistently outperforms other
variants, indicating that model size may be an important factor
in the amount of stored API name knowledge. However,
this finding should be interpreted in light of the scaling
law of mixed-modal language models [38], which suggests
that larger models are likely to achieve better performance
on downstream tasks, such as code generation. Finally, we
find that pre-trained data can influence the understanding of
API names to some extent, as shown by the performance
gap between PLBART-Base and PLBART-CSNet. Our results
indicate that fine-tuning on code generation tasks can improve
the performance of pre-trained models, while text generation
tasks may negatively impact them.
Findings of RQ1: Although CodeBERT-MLM-Python
and GraphCodeBERT-MLM show superior perfor-
mance on API call and import name knowledge among
code models, there is a significant margin for the
improvement.
B. RQ2: Do code models understand API import aliases?
To assess the code models on the knowledge of API
import aliases, we pair the 17,971 API import alias quizzes
with the adversarial examples designed to test the model’s
robustness. To construct the adversarial set, we randomly
selected 10 distinct aliases that are used in other modules
and replaced the original aliases in the quizzes with these
new aliases. For example, “ import numpy as np \n
np.load (” will be transformed to “ import numpy as
pmd \n pmd.load (” via the replacement of “ np”. In the
end, we collected 179,710 adversarial quizzes.
We report P@Kresults of ten models in Table V
and illustrate examples in Table VI. Based on the com-

--- PAGE 8 ---
Test Answer CodeBERT-MLM-Python GraphCodeBERT-MLMAPI Callstsci.tools.fileutil.buildFITS ( Name file [0.10], File [0.06], Image [0.04], image [0.03], Data [0.02] )\ [0.42], ): [0.08], () [0.06], ); [0.05], ) [0.04]
win32 .QueryServiceStatusEx( service api [0.59], gui [0.25], service [0.04], security [0.02], com [0.01] service [0.33], Service [0.09], net [0.08], api [0.06], API [0.05]
demand .bdew.ElecSlp( lib 2 [0.12], lib [0.09], 1 [0.07], 3 [0.03], py [0.02] com [0.47], org [0.17], google [0.16], com [0.03], org [0.02]
cltk.prosody.latin.string_ .move_consonant_right( utils left [0.02], table [0.02], translation [0.02], char [0.02], right [0.02] utils [0.28], list [0.16], selection [0.10], table [0.08], util [0.04]
fuzz.rand.randint( gram \n [0.40], # [0.34], py [0.09], ## [0.02], . [0.01] # [0.26], ˆ [0.05], open [0.05], ! [0.03], test [0.03]API Importfrom stsci.tools.fileutil import buildFITS Name \n [0.94], \n\n [0.03], File [0.00], file [0.00], _ [0.00] File [0.38], file [0.23], Filename [0.02], Tools [0.02], Files [0.01]
from win32 import QueryServiceStatusEx service api [0.43], service [0.07], query [0.07], security [0.06], db [0.04] service [0.35], file [0.22], net [0.19], db [0.02], security [0.02]
from demand .bdew import ElecSlp lib er [0.03], 1 [0.03], y [0.02], en [0.02], 2 [0.02] igo [0.05], y [0.02], com [0.02], en [0.02], ache [0.02]
from cltk.prosody.latin.string_ import move_consonant_right utils move [0.14], right [0.08], tools [0.04], translation [0.03], left [0.03] utils [0.56], selection [0.10], util [0.06], list [0.03], table [0.02]
from fuzz.rand import randint gram py [0.29], sk [0.06], fuzzy [0.05], core [0.03], ham [0.03] ham [0.43], exp [0.12], math [0.05], http [0.04], easy [0.03]
TABLE IV: Examples of prediction for CodeBERT-MLM-Python and GraphCodeBERT-MLM on API calls and imports. The
last two columns reports the top five tokens generated together with the associated probabilities (in square brackets).
P@1↑P@5↑P@10 ↑P@20 ↑P@30 ↑P@40 ↑P@50 ↑API Call With AliasCodeBERTMLMAlias 15.83 32.54 40.40 48.46 53.50 56.93 59.88
Adv. Alias 14.10 29.84 37.66 45.89 50.86 54.37 57.16
MLM-PythonAlias 24.17 42.30 50.19 58.02 62.68 66.05 68.37
Adv. Alias:::20.35:::37.30::::45.33::::53.42::::58.22::::61.72::::64.37
GraphCodeBERT MLMAlias 18.91 36.85 43.99 51.66 56.18 59.58 62.17
Adv. Alias 16.89 33.63 40.88 48.48 53.12 56.44 59.09
PLBARTBaseAlias 0.84 4.20 8.05 14.56 19.34 23.46 26.84
Adv. Alias 0.38 2.62 5.60 10.81 14.99 18.45 21.47
LargeAlias 3.36 11.64 18.34 26.15 30.88 34.71 37.58
Adv. Alias 1.49 7.19 12.70 19.63 23.92 27.08 29.69
CSNetAlias 1.75 7.86 12.37 18.59 22.93 26.87 30.56
Adv. Alias 1.68 6.63 10.39 15.48 19.30 22.45 25.21
SumAlias 1.21 4.93 8.44 14.20 18.14 21.50 24.21
Adv. Alias 1.02 3.75 6.36 10.43 13.74 16.55 18.95
GenAlias 3.43 12.87 19.29 26.85 31.66 35.05 37.61
Adv. Alias 2.13 8.54 13.55 19.79 23.89 27.08 29.67
MTAlias 3.56 11.68 18.61 26.42 31.35 34.80 37.82
Adv. Alias 2.39 8.08 12.98 19.72 24.47 28.03 30.94
GPT-3.5 turbo*Alias 26.80 45.20 49.80 54.90 - - -
Adv. Alias 24.30 42.60 47.20 51.45 - - -
TABLE V: Comparison of P@kscores on API import alias
quizzes among selected code models.
parison, CodeBERT-MLM-Python consistently outperforms
GraphCodeBERT-MLM, achieving higher P@1 scores of
up to 24.17% for Alias scenarios compared to 18.91%.
CodeBERT variants also show better overall performance
with P@50 scores ranging from 59.88% to 68.37%, while
GraphCodeBERT-MLM ranges from 59.09% to 62.17%. Our
initial finding suggests that code models have a weaker
understanding of API aliases compared to API calls and
imports, as shown in Table III. This indicates that current
code models encode little knowledge of import aliases. Based
on the performance of the GPT-3.5-turbo model on 1,000
randomly sampled quizzes, we can infer that it has a greater
capability to understand API import aliases. When comparing
the results of the original API import alias quizzes with those
of the adversarial aliases, we found only minor discrepancies,
indicating that these code models have strong robustness in
understanding API import aliases. We further analyze the
distribution of the API import aliases and find that an API
is paired with 1.16 aliases on average, and 8% of APIs have
more than 1 alias. We hypothesize that these code models
are able to learn the compositional patterns of these APIs via
different aliases, and thus manage to generalize to adversarial
import aliases.
Findings of RQ 2: Although code models show
robustness in understanding API import aliases, their
encoding of API knowledge is limited to partial infor-
mation.C. RQ3: Will natural language context affect the API name
knowledge probing results?
Our study examines the impact of natural language (NL)
context on a code model’s ability to comprehend API names.
To build a dataset of API-related NL context, we utilize the
NL queries designed for Python API sequence usages in the
work of [33]. For example, <os.path.isfile> is paired
with “file directory check”. We selected queries containing
API sequences containing the PyINK API and transformed
them into API pop quizzes. As some of the NL queries are
long and infeasible for code models to process, we only choose
the 10 shortest ones among these satisfied NL queries, and
filter out the cases where the length is no more than 512
tokens determined by the CodeBERT tokenizer. At the end,
we collected 56,645 pop quizzes for API calls, and 56,644 for
API imports. We compute the average P@kon each API name
pop quiz with the group of NL queries concatenated ahead.
We compare them with the overall results without adding NL
context at the beginning in Figure 4.
Our results show that incorporating natural language queries
led to a 2% improvement in probing API call name knowledge,
although we contend that this may be due to their non-
API-focused design, as they were initially created for API
sequences rather than individual API calls [33]. In addition, the
relative performances among code models do not change be-
fore and after adding NL context, suggesting PyINK is robust
for API name evaluation. We anticipate that incorporating API-
focused natural language context will yield more substantial
gains.
Findings of RQ3: While adding relevant NL context
can aid code models in locating more precise API
name knowledge, it does not affect the relative per-
formances among code models in terms of API name
knowledge.
D. RQ4: How well can code models memorize and generalize
on API names?
We evaluate whether code models demonstrate a deeper un-
derstanding of the names of seen APIs during pre-training than
theunseen ones by conducting experiments on CodeBERT-
MLM, GraphCodeBERT-MLM, and PLBART-CSNet, which
were pre-trained on the train set of CSNet. To create our

--- PAGE 9 ---
Test Answer CodeBERT-MLM-Python GraphCodeBERT-MLMAPI Aliasimport memote.suite.cli.callbacks as callbacksgit check [0.19], load [0.16], get [0.07], register [0.07], update [0.04] on [0.22], set [0.09], register [0.05], command [0.03], server [0.03]callbacks.<mask>_installed(
import weka.flow.conversion as conversionPass pass [0.50], Pass [0.17], run [0.05], Go [0.02], Run [0.01] pass [0.66], Pass [0.15], cycle [0.08], write [0.04], walk [0.01]conversion.<mask>Through(
import memote.support.helpers as helpersfind test [0.29], get [0.10], load [0.08], check [0.07], create [0.05] start [0.27], allow [0.14], stop [0.08], try [0.04], begin [0.04]helpers.<mask>_converting_reactions(
import pandas as pdStore Store [0.35], 5 [0.30], S [0.11], Parser [0.04], 4 [0.04] 5 [0.34], 4 [0.15], 3 [0.07], 0 [0.06], s [0.05]pd.HDF<mask>(
import numpy as npar ar [0.39], g [0.05], n [0.04], cal [0.04], b [0.03] ar [0.94], eu [0.03], ar [0.02], g [0.00], e [0.00]np.<mask>ccos(API Adv. Aliasimport memote.suite.cli.callbacks as attselgit check [0.18], load [0.12], get [0.10], is [0.04], on [0.03] on [0.27], set [0.07], callback [0.06], app [0.03], not [0.03]attsel.<mask>_installed(
import weka.flow.conversion as ipPass pass [0.51], run [0.08], Pass [0.06], connect [0.02], import [0.02] pass [0.69], Pass [0.20], cycle [0.06], write [0.01], run [0.01]ip.<mask>Through(
import memote.support.helpers as hermitefind test [0.27], get [0.13], check [0.09], load [0.05], create [0.04] start [0.41], stop [0.15], try [0.04], begin [0.03], allow [0.03]hermite.<mask>_converting_reactions(
import pandas as ITStore 5 [0.61], S [0.13], Store [0.11], 4 [0.03], Frame [0.01] 5 [0.27], 3 [0.12], s [0.11], 0 [0.10], 4 [0.07]IT.HDF<mask>(
import numpy as simplejsonar py [0.09], simple [0.04], g [0.04], cal [0.03], fun [0.03] ar [0.28], eu [0.21], g [0.09], e [0.08], ce [0.06]simplejson.<mask>ccos(
TABLE VI: Examples of prediction for CodeBERT-MLM-Python and GraphCodeBERT-MLM on API import aliases and their
adversarial samples. The last two columns reports the top five tokens generated together with the associated probabilities (in
square brackets).
020406080100API CallPrecision@1
020406080100Precision@5
020406080100Precision@10
020406080100Precision@20
CodeBERT-MLM
CodeBERT-MLM-Python GraphCodeBERT-MLMPLBART-Base PLBART-Large PLBART-CSNet PLBART-Sum PLBART-Gen PLBART-MT020406080100API Import
CodeBERT-MLM
CodeBERT-MLM-Python GraphCodeBERT-MLMPLBART-Base PLBART-Large PLBART-CSNet PLBART-Sum PLBART-Gen PLBART-MT020406080100
CodeBERT-MLM
CodeBERT-MLM-Python GraphCodeBERT-MLMPLBART-Base PLBART-Large PLBART-CSNet PLBART-Sum PLBART-Gen PLBART-MT020406080100
CodeBERT-MLM
CodeBERT-MLM-Python GraphCodeBERT-MLMPLBART-Base PLBART-Large PLBART-CSNet PLBART-Sum PLBART-Gen PLBART-MT020406080100
w/o NL w/ NL (mean)
Fig. 4: Comparisons of P@1,P@5,P@10 andP@20 scores of selected code models. w/o NL : No natural language context
is provided along with pop quizzes. w/ NL : Mean performances when natural language context is provided along with pop
quizzes.
P@1↑P@5↑P@10 ↑P@20 ↑P@30 ↑P@40 ↑P@50 ↑API CallCodeBERTSeen 23.96 42.42 50.38 58.26 62.84 65.96 68.39
Unseen 21.54:::39.42::::46.62::::54.92::::59.60::::62.63::::64.35
GraphCodeBERTSeen 26.00 43.45 50.91 58.45 62.89 66.02 68.42
Unseen:::21.86 37.42 44.44 51.89 56.97 60.26 62.36
PLBARTSeen 2.83 11.24 16.09 22.08 26.20 29.45 32.10
Unseen 2.00 8.31 12.80 18.38 21.94 24.52 27.32API ImportCodeBERTSeen 26.49 44.01 51.34 58.80 63.06 66.03 68.35
Unseen 20.46:::38.83::::45.82 52.58 56.49 58.94 61.08
GraphCodeBERTSeen 30.99 48.51 55.61 62.74 66.92 69.79 71.96
Unseen:::22.37 38.41 45.81::::53.69::::57.83::::60.78::::63.16
PLBARTSeen 3.64 15.29 22.39 30.80 36.29 40.37 43.63
Unseen 2.17 9.96 16.42 23.97 29.08 32.59 35.65
TABLE VII: Comparison of P@kscores on seen andunseen
API name quizzes among selected code models.
PyINK-Mem version, we take all APIs appearing during
training as the seen split and the remaining APIs as the unseen
split. We filter out all the APIs belonging to the seen libraries
in the unseen split. The PyINK-Mem seen split contained
281,945 quizzes for API calls and 281,945 quizzes for API
imports. The unseen split had 7,640 API call quizzes and 7,640
API import quizzes. We note that the selected models do not
memorize any structures of the open-source repositories, due
to the function-level pre-training objective.
In Table VII, we measure the model performance via P@k
up to P@50. Our inspection of the results on API call popquizzes suggests there are slight differences between seen
and unseen sets, indicating the strong generalization ability
of these code models to new APIs. Among the three models
we evaluated, CodeBERT-MLM demonstrates the most robust
performance, while GraphCodeBERT-MLM demonstrates a
greater ability to memorize API names during pre-training.
Surprisingly, we found that there were 1,288 and 5,468 distinct
ground-truth tokens in the seen and unseen splits for API
calls, respectively, and 1,257 tokens (97.59% of the unseen
split) were overlapped. This indicates that the API namespace
designs share unexpected commonalities.
Findings of RQ4: Code models demonstrate impres-
sive generalization abilities in predicting the names of
programming functions for new domains and reason-
able memorizations of APIs from the training data.
VI. D ISCUSSION
This section details the implications and importance of our
experiments, and discusses threats to their validity.

--- PAGE 10 ---
A. The implication of the evaluation on PyINK
Our study evaluates API name knowledge in code models on
thePyINK benchmark, encompassing a wide range of Python
APIs. Our analysis reveals that while these APIs share similar
design patterns regarding fully qualified naming, they also
exhibit diverse characteristics. This observation underscores
the challenges associated with modeling and comprehending
the name information pertaining to these APIs. Our exper-
iments on PyINK provide qualitative insight into the API
name knowledge in the pre-trained code models. However,
it is difficult to infer the specific quantitative amount, given
the models are selected based on different architectures. To
address this, we suggest comparing models from the same
family would be more reasonable.
Based on a detailed evaluation of PyINK , we find that code
models can store some API name knowledge while learning
from source code, but the knowledge is often insufficient.
Although these models achieve reasonable precision within the
top-50 predictions, they may struggle to answer pop quizzes
correctly within the first few attempts. One consistent finding
across various experiments is that code models may not treat
API calls and imports similarly, even though these two are
derived from the same API fully qualified name.
We also empirically demonstrate that the code model’s
ability to capture API name knowledge can be influenced
by several factors, including pre-training strategies, model
parameters, pre-training data, and fine-tuning. Our results
from RQ3 indicate that natural language queries can assist
code models in locating specific API name information to
some extent, underscoring the importance of context in con-
structing the approach. Furthermore, we emphasize that API
name knowledge is distinct from most knowledge in natural
language, where code models can generalize to unseen API
names. In comparison, language models consistently perform
inferior when generalizing to new domains [39], [40].
B. Strengths of INK evaluation framework
TheINK framework has two key strengths. Firstly, it can
diagnose code models’ understanding of API names. While
we found that discrete prompts may not be sufficient for
instructing models like GPT-3.5 to complete pop quizzes, we
recommend using few-shot examples in prompts to improve
performance. For publicly generative code models such as
CodeGen [41] and SantaCoder [42] that may not be able
to perform API name understanding tasks directly, we can
use prompt tuning strategies like P-tuning [43] to effectively
evaluate their performance.
Secondly, INK’s cloze-style pop quiz on API name under-
standing can be a valuable pre-training and fine-tuning task for
code models. By learning to predict API names, code models
can develop better representations of APIs and can be more
easily adapted to various downstream tasks involving APIs,
such as code summarization on API invocation and invoking
APIs during code completion.C. The importance of interpreting API name knowledge
Acquiring knowledge of API names is essential for software
development, as APIs play a critical role in automating various
functionalities. However, code models used in current bench-
marks do not adequately consider API usage when assessing
performance. Our research shows that code models often
struggle to preserve essential API name knowledge, leading
to the need for knowledge-enhanced training paradigms. In-
spired by recent advancements in knowledge-enhanced pre-
trained models in natural language processing and computer
vision, we propose exploring approaches that integrate API
knowledge graphs into code models. Learning from research
on API knowledge graphs [44]–[46] holds great promise for
advancing the field. By incorporating these approaches into
code models, we can improve the accuracy and effectiveness
of automated software development.
D. Threats to Validity
a) Choice of Cloze-style Pop Quiz Design: The em-
ployed pop quiz design involves predicting the partial or full
name of each API module level. It is possible to prompt
code models to predict on the tokens in between when an
API level involves multiple tokens. However, masking the first
and last tokens of each module level is a suitable approach
for assessing the extent of API name knowledge retained by
code models, validating our pop quiz choice. While some
may criticize our pop quizzes for targeting single token
prediction in the missing slot instead of multiple tokens, this
design choice aligns with prior works in probing language
models [24]. Introducing multi-token prediction could lead to
more variability in decoding strategies among code models
and make it difficult to finalize the sequence combined by all
predicted tokens during evaluation. Although we acknowledge
this potential threat to validity, we leave it to future work.
b) Choice of Evaluation Data: The choice of evaluation
data may affect experimental results of INK distillation. While
we used a widely-used corpus, CSNet, which covers a substan-
tial number of Python APIs, it is important to acknowledge
that there are additional resources, such as Stack Overflow1,
that may contain more Python APIs. Moreover, CSNet was
proposed in 2019, and new APIs may have been developed
since then. We contend that our evaluation of PyINK using
CSNet is statistically significant, but we also acknowledge
the limitations of this corpus. Furthermore, code models may
exhibit different behaviors when evaluated with APIs in other
programming languages such as Java and C. To address this
threat to validity, we can enhance the completeness of our
evaluation by incorporating more programming languages on
which these code models are trained. By evaluating the code
models on a wider range of programming languages, we can
better ensure their robustness and generalizability to real-world
programming tasks.
1https://stackoverflow.com/

--- PAGE 11 ---
VII. C ONCLUSION
In this paper, we have explored the interpretability of code
models for source code (CodeBERT, GraphCodeBERT and
PLBART). We conduct a thorough API name knowledge
analysis based on a large-scale benchmark, PyINK , from the
following four aspects, aiming to give an interpretation of
code models. Firstly, we determine the API name knowledge
stored by code models from two perspectives, API call and
API import. Secondly, we investigate whether code models can
robustly understand API import aliases. Thirdly, we revisit the
settings in deep API learning and assess if providing additional
natural language context can help code model retrieve more
precise API name knowledge. Fourthly, we examine the mem-
orization and generalization of code models on API names.
The analysis in this paper has revealed several interesting
findings that can inspire future studies on code representation
learning and interpretation of knowledge encoded by code
models.
REFERENCES
[1] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al. , “Codebert: A pre-trained model for programming
and natural languages,” in Findings of the Association for Computational
Linguistics: EMNLP 2020 , 2020, pp. 1536–1547.
[2] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. LIU, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. Clement, D. Drain,
N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, “Graphcode{bert}:
Pre-training code representations with data flow,” in International
Conference on Learning Representations , 2021. [Online]. Available:
https://openreview.net/forum?id=jLoC4ez43PZ
[3] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, “A survey
of machine learning for big code and naturalness,” ACM Computing
Surveys (CSUR) , vol. 51, no. 4, pp. 1–37, 2018.
[4] F. F. Xu, B. Vasilescu, and G. Neubig, “In-ide code generation
from natural language: Promise and challenges,” ACM Trans. Softw.
Eng. Methodol. , vol. 31, no. 2, mar 2022. [Online]. Available:
https://doi.org/10.1145/3487569
[5] C. Niu, C. Li, B. Luo, and V . Ng, “Deep learning meets software
engineering: A survey on pre-trained models of source code.”
[6] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , “Evaluating large
language models trained on code,” arXiv preprint arXiv:2107.03374 ,
2021.
[7] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,
M. Marone, C. Akiki, J. Li, J. Chim et al. , “Starcoder: may the source
be with you!” arXiv preprint arXiv:2305.06161 , 2023.
[8] OpenAI, “Gpt-4 technical report,” 2023.
[9] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W.-
t. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and
reliable benchmark for data science code generation,” arXiv preprint
arXiv:2211.11501 , 2022.
[10] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y . Ma, G. Liang,
Y . Li, T. Xie, and Q. Wang, “Codereval: A benchmark of pragmatic
code generation with generative pre-trained models,” arXiv preprint
arXiv:2302.00288 , 2023.
[11] W. Samek, T. Wiegand, and K.-R. Müller, “Explainable artificial in-
telligence: Understanding, visualizing and interpreting deep learning
models,” arXiv preprint arXiv:1708.08296 , 2017.
[12] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know
what language models know?” Transactions of the Association for
Computational Linguistics , vol. 8, pp. 423–438, 2020.
[13] Y . Wan, W. Zhao, H. Zhang, Y . Sui, G. Xu, and H. Jin, “What do they
capture? a structural analysis of pre-trained language models for source
code,” in Proceedings of the 44th International Conference on Software
Engineering , 2022, pp. 2377–2388.[14] J. A. Hernández López, M. Weyssow, J. S. Cuadrado, and H. Sahraoui,
“Ast-probe: Recovering abstract syntax trees from hidden representa-
tions of pre-trained language models,” in 37th IEEE/ACM International
Conference on Automated Software Engineering , 2022, pp. 1–11.
[15] S. Troshin and N. Chirkova, “Probing pretrained models of source code,”
arXiv preprint arXiv:2202.08975 , 2022.
[16] Q. Huang, Z. Yuan, Z. Xing, X. Xu, L. Zhu, and Q. Lu, “Prompt-tuned
code language model as a neural knowledge base for type inference in
statically-typed partial code,” in 37th IEEE/ACM International Confer-
ence on Automated Software Engineering , 2022, pp. 1–13.
[17] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified pre-
training for program understanding and generation,” in Proceedings of
the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , 2021,
pp. 2655–2668.
[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” in
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , 2019, pp. 4171–4186.
[19] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained
models for natural language processing: A survey,” Science China
Technological Sciences , vol. 63, no. 10, pp. 1872–1897, 2020.
[20] A. Hindle, E. T. Barr, M. Gabel, Z. Su, and P. Devanbu, “On the
naturalness of software,” Communications of the ACM , vol. 59, no. 5,
pp. 122–131, 2016.
[21] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code search,”
arXiv preprint arXiv:1909.09436 , 2019.
[22] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,
V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehen-
sion,” in Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , 2020, pp. 7871–7880.
[23] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-
train, prompt, and predict: A systematic survey of prompting methods
in natural language processing,” ACM Computing Surveys , vol. 55, no. 9,
pp. 1–35, 2023.
[24] F. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y . Wu,
and A. Miller, “Language models as knowledge bases?” in Proceedings
of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) , 2019, pp. 2463–2473.
[25] Z. Jiang, A. Anastasopoulos, J. Araki, H. Ding, and G. Neubig, “X-
factr: Multilingual factual knowledge retrieval from pretrained language
models,” in Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , 2020, pp. 5943–5959.
[26] F. Petroni, P. Lewis, A. Piktus, T. Rocktäschel, Y . Wu, A. H. Miller, and
S. Riedel, “How context affects language models’ factual predictions,”
inAutomated Knowledge Base Construction , 2020. [Online]. Available:
https://openreview.net/forum?id=025X0zPfn
[27] E. Perez, D. Kiela, and K. Cho, “True few-shot learning with language
models,” Advances in neural information processing systems , vol. 34,
pp. 11 054–11 070, 2021.
[28] G. Qin and J. Eisner, “Learning how to ask: Querying lms with mixtures
of soft prompts,” in Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies , 2021, pp. 5203–5212.
[29] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, “Api method
recommendation without worrying about the task-api knowledge gap,”
inProceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering , 2018, pp. 293–304.
[30] M. M. Rahman, C. K. Roy, and D. Lo, “Rack: Automatic api recom-
mendation using crowdsourced knowledge,” in 2016 IEEE 23rd Interna-
tional Conference on Software Analysis, Evolution, and Reengineering
(SANER) , vol. 1. IEEE, 2016, pp. 349–359.
[31] F. Thung, S. Wang, D. Lo, and J. Lawall, “Automatic recommendation
of api methods from feature requests,” in 2013 28th IEEE/ACM Inter-
national Conference on Automated Software Engineering (ASE) . IEEE,
2013, pp. 290–300.
[32] X. Gu, H. Zhang, D. Zhang, and S. Kim, “Deep api learning,” in
Proceedings of the 2016 24th ACM SIGSOFT international symposium
on foundations of software engineering , 2016, pp. 631–642.

--- PAGE 12 ---
[33] J. Martin and J. L. Guo, “Deep api learning revisited,” in Proceedings
of the 30th IEEE/ACM International Conference on Program Compre-
hension , 2022, pp. 321–330.
[34] M. A. Hadi, I. N. B. Yusuf, F. Thung, K. G. Luong, J. Lingxiao,
F. H. Fard, and D. Lo, “On the effectiveness of pretrained models
for api learning,” in Proceedings of the 30th IEEE/ACM International
Conference on Program Comprehension , 2022, pp. 309–320.
[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311–318.
[36] J. Wang, L. Li, and A. Zeller, “Restoring execution environments of
jupyter notebooks,” in 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE) . IEEE, 2021, pp. 1622–1633.
[37] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,
P. Cistac, T. Rault, R. Louf, M. Funtowicz et al. , “Transformers: State-
of-the-art natural language processing,” in Proceedings of the 2020
conference on empirical methods in natural language processing: system
demonstrations , 2020, pp. 38–45.
[38] A. Aghajanyan, L. Yu, A. Conneau, W.-N. Hsu, K. Hambardzumyan,
S. Zhang, S. Roller, N. Goyal, O. Levy, and L. Zettlemoyer, “Scaling
laws for generative mixed-modal language models,” arXiv preprint
arXiv:2301.03728 , 2023.
[39] M. Tänzer, S. Ruder, and M. Rei, “Memorisation versus generalisation in
pre-trained language models,” arXiv preprint arXiv:2105.00828 , 2021.
[40] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang,
“Quantifying memorization across neural language models,” in The
Eleventh International Conference on Learning Representations , 2023.
[Online]. Available: https://openreview.net/forum?id=TatRHT_1cK
[41] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou,
S. Savarese, and C. Xiong, “Codegen: An open large language
model for code with multi-turn program synthesis,” in The Eleventh
International Conference on Learning Representations , 2023. [Online].
Available: https://openreview.net/forum?id=iaYcJKpY2B_
[42] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis,
N. Muennighoff, M. Mishra, A. Gu, M. Dey et al. , “Santacoder: don’t
reach for the stars!” arXiv preprint arXiv:2301.03988 , 2023.
[43] X. Liu, K. Ji, Y . Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, “P-
tuning: Prompt tuning can be comparable to fine-tuning across scales and
tasks,” in Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) , 2022, pp. 61–68.
[44] H. Li, S. Li, J. Sun, Z. Xing, X. Peng, M. Liu, and X. Zhao, “Improving
api caveats accessibility by mining api caveats knowledge graph,” in
2018 IEEE International Conference on Software Maintenance and
Evolution (ICSME) . IEEE, 2018, pp. 183–193.
[45] X. Ren, X. Ye, Z. Xing, X. Xia, X. Xu, L. Zhu, and J. Sun, “Api-
misuse detection driven by fine-grained api-constraint knowledge graph,”
inProceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering , 2020, pp. 461–472.
[46] X. Wang, X. Liu, J. Liu, X. Chen, and H. Wu, “A novel knowledge
graph embedding based api recommendation method for mashup devel-
opment,” World Wide Web , vol. 24, no. 3, pp. 869–894, 2021.
