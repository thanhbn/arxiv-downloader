# 2305.16430.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/coding/2305.16430.pdf
# File size: 1168222 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Too Few Bug Reports? Exploring Data Augmentation for
Improved Changeset-based Bug Localization
Agnieszka Ciborowska
Virginia Commonwealth University
Richmond, Virginia, USA
imranm3@vcu.eduKostadin Damevski
Virginia Commonwealth University
Richmond, Virginia, USA
kdamevski@vcu.edu
ABSTRACT
Modern Deep Learning (DL) architectures based on transformers
(e.g., BERT, RoBERTa) are exhibiting performance improvements
across a number of natural language tasks. While such DL models
have shown tremendous potential for use in software engineer-
ing applications, they are often hampered by insufficient training
data. Particularly constrained are applications that require project-
specific data, such as bug localization, which aims at recommending
code to fix a newly submitted bug report. Deep learning models
for bug localization require a substantial training set of fixed bug
reports, which are at a limited quantity even in popular and actively
developed software projects. In this paper, we examine the effect
of using synthetic training data on transformer-based DL models
that perform a more complex variant of bug localization, which
has the goal of retrieving bug-inducing changesets for each bug
report. To generate high-quality synthetic data, we propose novel
data augmentation operators that act on different constituent com-
ponents of bug reports. We also describe a data balancing strategy
that aims to create a corpus of augmented bug reports that better
reflects the entire source code base, because existing bug reports
used as training data usually reference a small part of the code
base. Data balancing helps the model perform better for newly-
reported bug reports that reference previously unobserved code.
Our evaluation results indicate that both data augmentation and
balancing are effective, improving retrieval performance across all
three BERT-based models we studied.
ACM Reference Format:
Agnieszka Ciborowska and Kostadin Damevski. 2023. Too Few Bug Reports?
Exploring Data Augmentation for Improved Changeset-based Bug Local-
ization. In Proceedings of ACM Conference (Conference’17). ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
The emergence of novel Deep Learning (DL) architectures, such as
transformers, has fueled outstanding improvements across multiple
tasks in Natural Language Processing (NLP), and encouraged their
application to various problems in the software engineering domain.
Software engineering researchers have studied the potential of DL
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
©2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnnin the context of problems such as code search [ 13,14,21,29], defect
prediction [ 17,28,37,55,63], and bug localization [ 6,19,20,26,62].
However, the fundamental weakness of DL approaches is that they
require large amount of labelled data to train the model. At the
same time, maintaining the quality of the labelled data is crucial
to achieve the best performance. While manual labelling is typi-
cally a preferred approach to ensure high data quality, it is a slow
and time-consuming process [ 51], often intractable considering the
amount of data required to train a DL model. On the other hand,
automated mining for labels is far more likely to meet the demand
for data quantity, however at the cost of introducing noise in the
form of both false positives and false negatives [ 8,53]. Hence, col-
lecting large amount of good quality labelled data can pose a signif-
icant challenge for many important software engineering problems
and tasks, in particular those that require single project data (i.e.,
within project) [ 50]. A recent approach to address this problem is
to use transfer learning, i.e., pre-training a model with unsuper-
vised learning on a large, general corpus, followed by fine-tuning
via supervised learning towards the downstream task. However,
this strategy still requires a non-trivial dataset for fine-tuning and,
as observed by Gururangan et al., it leads to suboptimal perfor-
mance compared to when a model is pre-trained and fine tuned on
in-domain data [16].
One of the software engineering tasks that benefits from a DL-
based approach is bug localization, which aims to identify relevant
code entities (e.g., classes, methods or changesets) for a given bug
report describing a software failure. Over the years, researchers
have proposed multiple approaches for bug localization based on
the Vector Space Model (VSM) [ 43,56,58] and probabilistic models
(e.g., Latent Dirichlet Allocation) [ 7,33], while also recognizing
that the key drawback of those techniques is their limited ability to
deal with the semantic gap between source code causing the bug
and the description given in the bug report [ 1,64]. To address that,
recent efforts have been diverted towards DL techniques, includ-
ing RNN, LSTM and, finally, transformer-based models [ 6,29]. As
noted by Guo et al. [ 14], the availability of training data is one of
the key factors limiting DL performance. In the case of bug local-
ization, the training data consists of pairs of bug reports and their
introducing (or inducing) changesets, which are difficult to obtain
at scale for a couple of key reasons. First, matching a bug report
to bug-introducing changesets is challenging as developers rarely
mark culprit code changes explicitly [ 30], while approaches that
find the bug-inducing changesets automatically, based on the SZZ
algorithm [ 46], are prone to introducing noise [ 31,42]. Second, the
number of positive samples is bounded by the number of fixed bug
reports, which are limited even for large and actively maintained
projects. Relatively smaller software projects with, e.g., dozens ofarXiv:2305.16430v2  [cs.SE]  1 Jun 2023

--- PAGE 2 ---
Conference’17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski
fixed bug reports, would be very difficult to use. In the end, the
main question remains open: how to leverage DL techniques for
bug localization, given the paucity of project-specific data.
In the NLP domain, this question has been answered with some
success by Data Augmentation (DA) techniques, which, in general,
can be described as strategies to artificially increase the number
and diversity of training samples based on the currently available
data [ 11]. DA aims to create high quality synthetic data by apply-
ing transformations to the available data, while maintaining label
invariance. As a result, the size of the original dataset increases,
which in turn enables training a DL model for low resource domains
and tasks.
Encouraged by recent advances of DA in the NLP domain, in
this work, we aim to explore data augmentation for bug reports
with the goal of producing a large number of high quality, realistic,
synthetic bug reports, which can be subsequently used to increase
the size of the training set for a bug localization DL model. To this
end, we propose two sets of DA operators that independently target
natural language text and code-related data (e.g., code tokens, stack
traces and code snippets) in each bug report. More specifically, nat-
ural language text is augmented using token- and paragraph-level
transformations (e.g., synonym inserts), while the code-related data
is augmented with code tokens from its respective bug-inducing
changesets in order to strengthen the connection between a bug
report and different portions of its introducing changeset. At the
same time, by leveraging the augmented bug reports we plan to
achieve another important goal, balancing the augmented dataset
toward parts of the source code underrepresented in the original
training set. This addresses the common occurrence in software
projects that existing bug reports reference only a specific part of
the code base, while other parts have few or no bug reports, leading
to the bug localization model overly focusing only on a part of
the code base. In this paper, we investigate the following Research
Questions (RQs):
RQ1: (a) Can Data Augmentation improve the retrieval performance
of DL-based bug localization? (b) How does Data Augmentation
impact the performance of different DL-based bug localization ap-
proaches?
To understand whether Data Augmentation is a relevant strategy
in DL-based bug localization, we identify three recent transformer-
based models to perform this task. We evaluate the performance
of these bug localization approaches, with and without DA, using
a standard bug localization dataset and metrics commonly used
to measure information retrieval performance. As augmentation
necessarily introduces significantly higher data quantity, we add
baselines to the evaluation that aim to differentiate the quantity vs.
the quality of the augmented dataset. The results indicate that (1)
the proposed data augmentation strategy improves retrieval accu-
racy by between 39% and 82%, (2) augmenting the dataset is more
beneficial than increasing the size of the training dataset by repeti-
tion; and (3) balancing the training dataset results in improvement
in retrieval performance, but the magnitude of the improvement
depends on the architecture of the DL model.
RQ2: Which of the proposed DA operators contribute the most to
retrieval performance?The Data Augmentation approach in RQ1 relies on augmentation
operators that perform specific types of transformations (e.g., in-
sert, remove). In RQ2, we aim to understand the impact of these
augmentation operators on the retrieval performance during bug
localization. To answer RQ2, we perform ablation studies, train-
ing each DL model with augmented datasets created using all but
one augmentation operator type. The results indicate that most of
the operators contribute to the final performance, while certain
operators are more consistent across different DL models.
2 DATA AUGMENTATION FOR BUG
LOCALIZATION
With the increasing complexity of DL-based methods for bug lo-
calization [ 6,15,20,26], the problem of data scarcity comes to the
forefront. More specifically, while the more advanced models have
the potential to bridge the lexical gap between a bug report and
source code [ 1,64], in order to fulfill that promise, they require
large amount of bug reports to learn the semantics of the project
and subsequently associate it with bug-inducing changesets. Insuf-
ficient amount of training examples may lead to model overfitting,
memorizing high-frequency patterns or structures instead of gener-
alizing the knowledge [ 45]. DA can help to address the data scarcity
problem in bug localization by focusing on the following goals.
1. Increasing the number of bug reports. Training a DL model
for bug localization requires a substantial dataset consisting of
bug reports and bug-inducing changesets. The main challenge of
constructing such dataset is that it is project-specific. Most software
projects typically have few bug reports with a clear indication of
the changesets that caused them [30]. Moreover, the total number
of bug reports in a project is an upper bound on the number of
positive training instances that are available. Note that while we
can create numerous negative instances (i.e., a bug report and a
non-bug-inducing changeset), the benefit to the DL model is limited
as the bug report remains the same in each instance. Moreover, out
of all the reported bugs, some are closed with Won’t fix orNot a Bug
status [ 24,59], hence they do not have corresponding changesets
and cannot be used for training. To empirically verify the scale
of data scarcity problem in bug localization data, we examined
Bench4BL [ 27], a large bug localization dataset. Bench4BL includes
10K bug reports and their fixes coming from 51 popular and actively
developed open source software projects, which equals to roughly
200 bug reports per project. Considering that the projects in the
Bench4BL dataset are typically large and well-established (e.g., long
running Apache Software Foundation projects like Camel and Hive),
200 bug reports is a discouragingly low number when it comes to
ability to train an effective DL model.
2. Maintaining label invariance of bug reports. In NLP, data
augmentation is primarily evaluated on classification tasks, such as
sentiment analysis or topic classification, in which rarely a single
word can be representative of the overall result (i.e., a sentiment or
a topic). Data in software engineering is a mix of natural language
and code-related segments. In case of bug localization, this mix
typically affects bug reports, which often contain not only natu-
ral language description but also mentions of relevant program
elements, stack traces or code snippets [ 1]. Applying off-the-shelf
data augmentation transformations to bug localization data may

--- PAGE 3 ---
Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization Conference’17, July 2017, Washington, DC, USA
Table 1: Four examples of textual data augmentation with
varying validity [57].
Bug report summary Valid
Original Async connector does not timeout with HTTP
NIO context.–
Random Swap 1 Async connector does timeout notwith HTTP
NIO context.✓
Random Swap 2 Async context does not timeout with HTTP
NIOconnector .✗
Synonym Replacement 1 Async connector does not timeout with HTTP
NIOsession .✓
Synonym Replacement 2 TCP connector does not timeout with HTTP
NIO context.✗
cause more harm than good as it does not differentiate between
NL and code, which both bring useful information, but in different
forms and quantities. Table 1 shows examples of textual augmen-
tation performed on the summary of bug report #55996 from the
Tomcat project using two augmentation operators proposed by Wei
et al. [ 57]. Random Swap exchanges two randomly selected words,
while Synonym Replacement substitutes a randomly selected word
with its synonym. To find synonyms, we use BERTOverflow [ 49],
a BERT model pre-trained on the StackOverflow corpus. Given
the randomness of data augmentation operations, we see different
versions of augmented bug report summary. While Random Swap 1
swaps two words without affecting the semantics, Random Swap 2
exchanges words that can indicate the relevant code component, if
a project contains AsyncContext andAsyncConnector classes. Sim-
ilarly, in the case of Synonym Replacement 1 changing context to
session affects the semantics less than replacing Async with TCP
which are different concepts. This toy-example shows how easily
off-the-shelf data augmentation can introduce noise that affects the
original label, especially when handling data that contains key soft-
ware engineering-related phrases. Hence augmentation of software
engineering data in general, and bug reports in particular, requires
additional steps to ensure the invariance of the newly generated
data points.
3. Diversifying the training data. The goal of data diversification
in DA is to ensure that augmented data introduces ”new quality”
to a training set, such as previously unobserved motifs, patterns
or expressions, leading a DL model to learn the meaning behind
the data instead of memorizing certain forms [ 34,45]. In the case
of bug localization, the training dataset depicts how natural lan-
guage describing a bug connects to source code concepts in the
bug-inducing changeset. Commonly, the natural language in bug re-
ports consists of Observed Behavior (OB), Expected Behavior (EB),
or Steps to Reproduce (S2R) [ 3]. Given that OB, EB and S2R have
been recognized by developers as useful information when fixing
a bug [ 1], augmentation for bug localization data should focus on
introducing diversity into those through, e.g., paraphrasing their
sentences. The second important component of diversification of
a bug localization training set are the connections between bug
reports and source code. While it is true that bugs are not evenly
distributed in the source code base, the over-representation of one
source code component (e.g., class, package) in the training set, may
lead to the model blaming that particular component for every bug.To account for that, while augmenting training set for bug localiza-
tion, additional steps can be taken to mitigate that risk, through,
e.g., creating more augmented bug reports for those source code
components that occur less often in the training set. In summary,
the diversification of training data should focus on: (1) modifying
the natural language content of a bug report, and (2) diversifying
how a bug report connects to the source code.
3 APPROACH
To create augmented bug reports that introduce diversity and pre-
serve invariance (i.e., the augmented bug report still matches the
same changeset as the original bug report), we propose a set of
custom DA operators. Bug reports describe software failure using
various types of information, such as natural language, code snip-
pets or stack traces, which may have different impact on matching
a bug report to its inducing changeset, hence we decided to sep-
arately augment natural language and code-related information
to ensure invariance of the newly created data points and avoid
introducing noise. Figure 1 illustrates the workflow of our data
augmentation process which starts with data extraction and pre-
processing, followed by augmentation with the proposed operators,
and construction of augmented bug reports combining the newly
generated data.
3.1 Data preprocessing
As a first step, we use infozilla [39], a tool that extracts stack traces
and code snippets from unstructured bug report content, leaving
the remaining text broadly categorized as natural language. The
infozilla produces minimal error as experiments have shown it
to have 97%+ precision, 95%+ recall and 97%+ accuracy. To bring
out further structure from the natural language data, we extract
Observed Behavior (OB), Expected Behavior (EB), and Steps to
Reproduce (S2R) using the BEE tool [ 48], which has shown to
be highly effective at this task (94%+ accuracy, 87%+ recall, 70%+
precision).
Stack traces are a valuable source of localization hints, however,
due to their length they tend to introduce noise through multiple
mentions of classes not necessarily related to a particular bug re-
port [ 6,40]. To mitigate the noise in stack traces, we reduce their
size by selecting the lines that are most likely to contain relevant
information. For instance, for Java stack traces this leads to three
groups: 1) top lines, which include the exception name and where
the exception originated; 2) middle lines, which occur after the
Java standard library traces and are most likely last lines of the
application code closest to the bug; and 3) bottom lines, which
can be useful for exceptions thrown from threads. Sampling from
these three groups creates a generic recipe that shortens the stack
trace, captures different software designs, and preserves important
information. Hence, for each stack trace, we decided to keep top
1 line, first 3 lines that refer to the application code, and bottom 1
line. Heuristic approaches such as this one have been reported to
perform reasonably well even on unstructured runtime data (e.g.,
raw crash logs with multiple stack traces, possibly from different
programming languages) [38].
For preprocessing code snippets, we decided to filter out punctu-
ation for two reasons. First, in a recently published study, Paltenghi

--- PAGE 4 ---
Conference’17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski
Figure 1: A visualization of our augmentation pipeline for a single bug report.
et al. [ 36] compared the reasoning of developers and neural mod-
els, and observed that the models pay more attention to syntactic
tokens (e.g., dots, periods, brackets), while developers focus more
on strings or keywords. Given that developers perform better, DL
models should mimic developers and put less attention to syntactic
tokens. The second reason for filtering punctuation is pragmatic –
reducing the number of tokens to prevent exceeding the input limit
size of the DL models. Following preprocessing, each bug report is
represented as a collection of OB, EB, S2R, stack traces, and code
snippets.
3.2 Natural language DA operators
This group of operators is applied to OB, EB, and S2R due to their
primarily natural language content. We propose to use two types
of operators: token-level and paragraph-level. Inspired by a simple
yet effective technique called Easy Data Augmentation (EDA) [ 57],
we propose to use 4 token-level operators.
•Dictionary Replace - randomly selects a word from a pre-
defined in-domain dictionary and replaces the word with its
substitute.
•Dictionary Insert - works similarly to Dictionary Replace,
however instead of replacing the word, this operator inserts
the substitute at a random position in the text.
•Random Swap - randomly selects two words and swaps
them.
•Random Delete - removes a randomly selected word.
To build the in-domain dictionary for augmenting OB, EB and S2R,
we use keywords from language patterns devised by Chaparro
et al. [ 4]. The patterns specify combinations of different parts of
speech with certain keywords that have to occur to classify a sen-
tence or a paragraph as OB, EB or S2R. For instance, one of the
most popular OB patterns is NEG_VERB defined as: (subject/noun
phrase) ([adjective/adverb]) [negative verb] ([complement]) , where
the negative verbs are defined as: affect, break, block, close, etc. The
in-domain dictionary contains all keywords identified by Chapparo
et al. and maps each keyword to its substitutes, e.g., affect→{break,
block, close, ...} . Domain knowledge guided operators have been
recently shown to lead to better performance compared to more
advanced but general approaches (e.g., embeddings) [25].
As a paragraph-level operator, we use Backtranslation to trans-
late paragraphs of OB, EB or S2R from English to German and back
to English [ 32]. Backtranslation is a popular data augmentation
operation that allows to paraphrase the original text.Finally, let us describe how those operators are applied together
to generate augmented data. For each bug report and for each
OB, EB, S2R, we apply all token level operators 𝑛times, where
𝑛=𝜆∗#𝑡𝑜𝑘𝑒𝑛𝑠 . The operators are applied in the following or-
der: replace, insert, swap and delete. The value of 𝜆is set to 0.1
for insert, replace, and swap operations, and 0.05for delete opera-
tion as these parameters have been empirically shown to produce
best results [ 66]. Next, the Backtranslation operator is applied to
paraphrase the modified text. Given the randomness of the aug-
mentation operators, the quality of the augmented sample may
vary. While, in general, natural language text typically retains its
semantics when minor noise is introduced, bug reports are more
structured type of data with certain keywords (e.g., code names),
whose removal may have severe consequences for mapping the
bug report to the source code. Hence, as a final step, we employ
quality control that consists of two steps. First, we check if OB,
EB and/or S2R can be still identified in the augmented paragraph
using the BEE tool. For instance, if the original paragraph contained
OB and EB, then the augmented version must contain OB and EB
be to considered a valid paragraph. We also disallow changing the
pattern (e.g., from OB to EB). Second, we ensure that no code tokens
are lost during the augmentation by comparing the number of code
tokens between the augmented and the original paragraph.
3.3 Code-related DA operators
In the context of this paper, code-related data refers to stack traces,
code snippets, and code tokens present in natural language text.
To augment code-related data, we propose 3 code token operators
that are more strict versions of the natural language operators to
minimize the risk of distorting the context.
•Code Token Replace - randomly selects a code token and
replaces it with its substitute.
•Code Token Insert - randomly selects a code token, and
insert a substitute of that code token at a random position
that is at most 3 positions away from the selected code token.
•Code Token Swap - swaps two randomly selected code
tokens, such that (1) for stack traces code tokens can be
swapped only between consecutive stack lines; (2) for code
snippets, a swap operation must be performed within the
surrounding 3 tokens to minimize the potential distortion of
the bug report’s semantics. Changing this to a larger limit
would result in more diverse samples being created, how-
ever, with a higher chance of disturbing the bug report’s
semantics..

--- PAGE 5 ---
Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization Conference’17, July 2017, Washington, DC, USA
Figure 2: An example of augmented bug report for Tomcat #55171. Token-level modifications are marked with grey color.
We decided against including a code token deletion operator as
removing code tokens is more likely to disturb the invariance of
augmented samples.
To find substitutes for a code token, first, for each bug report we
build a dictionary of code names using class and method names
that occur in its corresponding bug-inducing changesets. Next, we
use the Levenshtein distance to measure the distance between the
selected code token and all other tokens in the dictionary. Leven-
shtein distance is a string similarity metric that quantifies similarity
through the number of edits required to convert one string into
another for strings of arbitrary length. We empirically observed
that the most similar code tokens are often of the same form, and
hence, may introduce very limited diversity into the augmented
samples. For instance, consider the code token word with the top-3
closest tokens is_word ,set_word ,get_word , and the top-20 token
check_word_missing_letter . To allow for more diverse augmentation,
based on empirical observations for one of the evaluation projects,
we opted for a less conservative top- 𝑘selection, with 𝑘set to 20.
Hence, a code token substitute is selected randomly from the 20
code tokens that have the lowest Levenshtein distance from the
given code token.
3.4 Building augmented bug reports
After augmentation, each bug report is decomposed into a collec-
tion of the original and augmented samples, i.e., natural language
data (OB, EB, S2R), and code-related data (stack traces and code
snippets). The remaining question is how to build a synthetic bug
report out of all the available samples. Recent work in neural ma-
chine translation has shown that concatenating augmented samples
introduces structural diversity that prevents a DL model from learn-
ing to focus only on one part of the input, thus leading to a strong
improvement in the model’s performance [ 34,61]. We propose to
use a similar approach to build augmented bug reports. More specif-
ically, first we recreate the original structure of a bug report by
concatenating augmented samples. Next, samples are reordered
and at most 1 sample can be dropped to achieve further structural
diversity. While dropping parts of bug reports may seem coun-
terintuitive, DA strategies that remove tokens or sentences has
been observed to have a positive impact on large pre-trained DL
models [ 5,44]. Figure 2 shows bug report #55171 from the Tomcatproject and its augmented version. Each part of the bug report has
been augmented separately using all of its respective DA operators.
In the case of natural language, we see semantically correct inser-
tions and replacements (e.g., blocked→dead), while the paragraph
rephrasing performed by Backtranslation is less precise yet still
conveys the main message (e.g., the second OB in the figure). Code
augmentation for this bug report includes augmenting stack trace
and code tokens with code names from the bug inducing change-
set. While such an approach inherently limits the possibility to
add noise, it also introduces information about code components
that are related to the bug report. This, in turn allows the model
to learn these relations and utilize them during inference. Finally,
When constructing the augmented bug report, the second OB and
the stack trace have been swapped, while the third OB has been
dropped, creating the final augmented version of bug report #55171.
3.5 Ensuring a balanced augmented dataset
To increase the size of the bug localization training set with data
augmentation, our approach is to focus on augmenting bug reports,
increasing the number of pairs of bug reports and bug-inducing
hunks. Recent studies show that using hunks, a set of consecu-
tive line modifications that capture changes in one area of the
file, produces improved retrieval results compared to using entire
changesets [ 6,58]. Hence, in this work, we build bug localization
training set using pairs of bug reports and hunks extracted from
bug-inducing changesets.
Bugs affect different parts of source code base with varying
frequency [ 2]. In other words, parts of the source code (i.e., specific
files or classes) are related to multiple bug reports and therefore
their hunks can also be overrepresented in the original dataset.
For instance, given a bug report with 𝑛introducing hunks, data
augmentation by a factor of 10 creates 10 new bug reports for each
hunk, which leads to 10𝑛new training samples. However, there
is one major drawback to this DA approach. This data imbalance,
created by the uneven distribution of bug reports and hunks in the
training set, can be exasperated by DA, with strong downstream
effects on the DL model and its prediction.
To provide further evidence, we empirically checked the dataset
published by Wen et al. [ 58], which we use in our study. Figure 3
shows the distribution of bug reports (Fig. 3a) and class occurrences

--- PAGE 6 ---
Conference’17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski
0 20 40 60 80
Bug report0100020003000Number of training examplesOriginal data Augmented data Balanced data
0 20 40 60 800100200300
(a) Number of times a bug report occurs in the training set.
0 20 40 60 80 100
Class0100020003000Number of training examples0 20 40 60 80 1000100200300400
(b) Number of times a class occurs in the training set.
Figure 3: Distribution of bug reports and classes showing
data imbalance in bug localization training set.
(Fig. 3b) for the Tomcat project. The distributions shows how many
samples in the training dataset refer to a specific bug report (or a
class), where a bug report (or a class) has as many occurrences as the
number of hunks. We show these distributions for three different
choices of training datasets: the original unaugmented dataset, a
10x augmented dataset, and an artificially balanced dataset. Within
the plots, there are zoomed-in versions to increase readability at
the smaller scale. In the plot for the original training set (i.e., the
blue line), we observe that 11 out of 97 bug reports cover over 50%
(1432 out of 2812) of the training samples. In other words, 50% of
the samples in the training set refer to 11 bug reports, since these
bug reports have multiple introducing hunks, which translates to
multiple entries in the training set. At the same time, 39 bug reports
occur less than 10 times. Similarly, out of the unique 110 classes that
introduced a bug, the top 10 classes with most frequently occurring
hunks cover 34.5% (2586 out of 7478) of all training data. This
imbalance in the training data can have two potential consequences
for supervised training of a DL model. First, the model is more likely
to learn the structure and semantics of bug reports that have a large
number of bug inducing hunks, while neglecting less frequent bug
reports. Secondly, classes that occur the most in the training set
are more likely to be selected as bug-inducing by the trained model
since they were often seen during training as bug-inducing. The
issue of data imbalance has also been recognized in defect prediction
datasets [65].
How augmentation exacerbates the problem of uneven data
distribution can be observed in Figure 3, where the orange dotted
line depicts the data distributions in a dataset that was augmented
by a factor of 10. The majority bug reports and classes become
even more dominant in the augmented dataset, making the data
imbalance problem more severe than in the original dataset. Tomitigate this problem, we propose a data balancing strategy that
deliberately chooses samples to augment in order to smooth out the
distributions of bug reports with respect to the source code. There
are two main concerns that a data balancing strategy has to consider:
(1) increasing the number of training samples for infrequent bug
reports, and (2) ensuring that the number of samples with a given
class does not dominate the dataset. To illustrate the need for these
strategies, consider a bug report 𝐵1with 20 hunks from different
classes, and a bug report 𝐵2with one hunk from class 𝐶. If the
balancing strategy is focused only on the distribution of bug reports,
then it creates 20 augmented samples for 𝐵2, every time using the
hunk from class 𝐶, hence𝐶is likely to be overrepresented in the
training set. To address this, we introduce two augmentation factors
𝛼and𝜔. While𝛼influences the number of times each bug report is
augmented, 𝜔restricts how many times each class can be repeated
in the augmented dataset.
Algorithm 1: Data balancing with augmented bug reports
Input :𝐷𝑡𝑟𝑎𝑖𝑛 – training dataset;
𝛼– augmentation factor;
𝜔– balancing factor
Output:𝐷𝑏𝑙– balanced training dataset
1𝑚𝑎𝑥 𝑏𝑟←𝛼×max. # of bug reports in 𝐷𝑡𝑟𝑎𝑖𝑛
2𝑚𝑎𝑥 𝑐𝑙←𝜔×max. # of classes in 𝐷𝑡𝑟𝑎𝑖𝑛
3𝐷𝑏𝑙←𝐷𝑡𝑟𝑎𝑖𝑛
4for𝑏𝑟,𝐻𝑏𝑟in𝐷𝑡𝑟𝑎𝑖𝑛 do
5 while (count of𝑏𝑟in𝐷𝑏𝑎𝑙)<𝑚𝑎𝑥 𝑏𝑟do
6𝑏𝑟𝑎←augment𝑏𝑟
7ℎ𝑎←select hunkℎ𝑎from𝐻𝑏𝑟, where
8 (count of class for ℎ𝑎in𝐷𝑏𝑎𝑙)<𝑚𝑎𝑥 𝑐𝑙
9 Add𝑏𝑟𝑎,ℎ𝑎to𝐷𝑏𝑙
10 end while
11end for
12return𝐷𝑏𝑙
The sequence of steps for the proposed data balancing augmen-
tation strategy is presented in Algorithm 1. In lines 1-2, we compute
a limit for bug reports 𝑚𝑎𝑥𝑏𝑟and classes𝑚𝑎𝑥𝑐𝑙based on factors 𝛼
and𝜔and the maximum number of times a unique bug report and
class is present in the original training dataset. Line 3 copies the
existing data instances into the balanced dataset 𝐷𝑏𝑙. For each bug
report that occurs below the 𝑚𝑎𝑥𝑏𝑟limit, the algorithm augments
the bug report (line 6), and selects a bug-inducing hunk from a class
that occurs less than 𝑚𝑎𝑥𝑐𝑙times in𝐷𝑏𝑙(lines 7-8), creating a new
training sample. The algorithm continues to add new samples for a
bug report until (1) the 𝑚𝑎𝑥𝑏𝑟limit is reached, or (2) bug-inducing
hunks from all the classes have reached 𝑚𝑎𝑥𝑐𝑙. The result of this
balancing strategy is depicted in the green line in Figure 3, using
values of𝛼=0.7and𝜔=1.0. Compared to the augmented dataset,
the data distribution of the balanced dataset is obviously smoother,
with a much more even representation of the source code.
4 EXPERIMENTAL EVALUATION
4.1 Dataset and metrics
To evaluate, we require a dataset that contains bug reports and
bug-inducing changesets. We use a dataset published by Wen et
al. [58] that contains manually-validated data from 6 open source
software projects: AspectJ, JDT, PDE, SWT, Tomcat, and ZXing.
Given that infozilla requires new lines to extract code snippets and

--- PAGE 7 ---
Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization Conference’17, July 2017, Washington, DC, USA
Table 2: Evaluation datasets.
Training Testing
Project # bugs𝐷𝑜𝑟𝑖 # bugs # hunks
AspectJ 100 2212 100 23446
SWT 45 9982 45 69833
Tomcat 96 5624 97 72134
PDE 30 3856 30 100373
JDT 47 18230 47 150630
stack traces, and new lines were removed from all bug reports in
Wen et al. ’s dataset, we located and re-scraped the bug reports (with
new lines) from Bugzilla for all projects. For ZXing, the bug reports
in the GitHub issue tracker did not match those collected by Wen
et al., likely because the project was moved, and therefore ZXing
was excluded from the evaluation set. To create a training set for
each project, we ordered the bug reports by opening dates and
selected the first half for training, while the remaining bug reports
constitute the test set. Each positive training sample corresponds
to a pair of a bug report and one of its inducing hunks (extracted
from the inducing changeset). Each bug report includes the bug
summary and description, while each hunk contains a log message
and source code changes. The dataset of Wen et al. was constructed
using SZZ [ 46], which identifies a changeset as bug-inducing if
it shares anyfile modifications with bug fixing changeset. While
a bug-inducing changeset may include modifications of multiple
files, only a few of those may be relevant to a bug (as indicated by
bug fixing changeset). Hence, to ensure the quality of the training
samples, we only include bug inducing hunks that refer to classes
that also occurs in the bug fixing commit. For each positive sample,
we create a negative sample by randomly selecting a hunk from
a class which does not belong to the inducing changeset. After
completing this step, for each project we obtain our baseline dataset,
𝐷𝑜𝑟𝑖. The descriptive statistics of training and testing datasets used
in this study are shown in Table 2. Note that the last column, #
hunks , denotes the number of allhunks that are examined by the
model during retrieval.
To evaluate the retrieval performance of the DL models trained
on different datasets, we use the following metrics.
Mean Reciprocal Rank: MRR measures the retrieval accuracy
using the reciprocal ranks of first relevant changeset in the ranking
averaged across all bug reports. The higher the value of MRR is,
the closer the bug-inducing changeset is to the top of the ranking.
𝑀𝑅𝑅 =1
|𝐵||𝐵|∑︁
𝑖=11
1𝑠𝑡𝑅𝑎𝑛𝑘𝐵𝑖.
Mean Average Precision: MAP quantifies the ability of a model
to retrieve all relevant changesets for a given bug report. MAP is
calculated as the mean of Average Precision scores across all bug
reports, where an Average Precision for a bug report is based on
ranks of all relevant changesets in the ranking. The higher the
values of MAP, the more relevant changesets is located in the topof the ranking.
𝑀𝐴𝑃 =1
|𝐵||𝐵|∑︁
𝑖=11
𝐴𝑣𝑔𝑃𝐵𝑖.
Precision@K: P@K measures how many of the top- 𝐾changesets
in the ranking are relevant to a bug report. The higher the value of
P@K, the more relevant changesets can be found in top- 𝐾positions.
𝑃@𝑛=1
|𝐵||𝐵|∑︁
𝑖=1|𝑅𝑒𝑙𝐵𝑖|
𝐾.
4.2 DL models
To evaluate the impact of the proposed data augmentation and
balancing strategies on the retrieval performance, we train and
evaluate three BERT-based [10] code retrieval architectures.
TBERT-Single [9,29,35] is the most straightforward approach for
information retrieval with BERT. The model concatenates a bug
report and a hunk, and processes it through BERT and a pooling
layer to obtain a fused vector representation, which is subsequently
passed to the classification head to obtain a relevancy score. While
this model typically provides high retrieval accuracy, it also incurs
significant retrieval delay, since a bug report needs to be compared
with allhunks available in a project.
TBERT-Siamese [29,41] processes a bug report and a hunk se-
quentially through BERT and a pooling layer, creating two features
vectors, that are subsequently concatenated and passed to the clas-
sification layer to produce the relevancy score. The key difference
between TBERT-Single and TBERT-Siamese is in the opportunity
to perform offline encoding of feature vectors for hunks, hence
reducing the retrieval delay.
FBL-BERT [6,22] is a recently proposed BERT-based architecture
that enables rapid retrieval across large collection of documents (i.e.,
hunks). Unlike TBERTs, which flattens the embedding matrix to a
vector to make a prediction, FBL-BERT leverages the full embedding
matrix and calculates relevancy score between a bug report and
a hunk as a sum of maximum vector similarities between word
embeddings of the bug report and hunk. This, in turn, allows to use
efficient vector similarity search algorithms to find the most similar
hunks and only re-rank those with FBL-BERT, hence significantly
reducing the retrieval time per bug report. Given that FBL-BERT
leverages fine-grained token-to-token embeddings matching, the
model is more likely to better utilize relevant keywords if they
occur in the bug report.
While all the models are based on BERT, their architectures differ
in a few aspects. TBERT-Single concatenates a bug report and a
changeset, processing them jointly through BERT, followed by a
classification layer. TBERT-Siamese and FBL-BERT first use BERT to
encode a bug report and a changeset separately, which results in two
embedding matrices. The main difference between TBERT-Siamese
and FBL-BERT is how they handle these matrices. TBERT-Siamese
aggregates each matrix into a vector using a pooling operation,
and, next, compares the embedding vector of a bug report with a
changeset embedding vector. On the other hand, FBL-BERT uses
both matrices to compute the relevancy score taking into account
the embedding of each word.

--- PAGE 8 ---
Conference’17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski
Table 3: Dataset characteristics for RQ1.
AspectJ SWT Tomcat PDE JDT
Not augmented datasets
𝐷𝑜𝑟𝑖 2.2k 9.9k 5.6k 3.9k 18.2k
𝐷𝑟𝑒𝑝 22.1k 99.8k 56.2k 38.6k 182.3k
Augmented datasets
𝐷𝑎𝑢𝑔 24.3k 109.8k 61.9k 42.4k 200.5k
𝐷𝑏𝑙1 22.4k 66.9k 33.8k 25.1k 112.9k
𝐷𝑏𝑙2 29.8k 90.5k 46.8k 30.7k 142.3k
𝐷𝑏𝑙3 31.5k 95.1k 49.0k 32.5k 150.5k
𝐷𝑏𝑙4 44.2k 130.9k 65.6k 46.7k 216.4k
4.3 Evaluation setup
We performed the experiments on a server with Dual 12-core
3.2GHz Intel Xeon and 1 NVIDIA Tesla V100 with 32GB RAM
memory running CUDA v.11.4. The models are implemented with
PyTorch v.1.7.1, HuggingFace library v.4.3.2, and Faiss v.1.6.5 with
GPU support. We opted for using BERTOverflow [ 49] as our pre-
trained base BERT model, since, similarly to our data, StackOverflow
data is also a mixture of code and natural language. All models are
fine tuned for 4 epochs, using a batch size of 16 and Adam (ab-
breviated from adaptive moment estimation) optimizer [ 23] with
learning rate set to 3e-6 [ 10]. Based on the average number of to-
kens in bug reports and hunks in our dataset, we set the input size
limit to 256 and 512 tokens for bug reports and hunks respectively.
All input documents are padded or truncated with respect to their
input size limit.
5 RESULTS
5.1 RQ1: (a) Can Data Augmentation improve
the retrieval performance of DL-based bug
localization? (b) How does Data
Augmentation impact the performance of
different DL-based bug localization
approaches?
Setup. To evaluate the impact of DA on DL-based models, we com-
pare the retrieval accuracy when training on the original, unaug-
mented dataset, 𝐷𝑜𝑟𝑖, to training with augmented and balanced data.
More specifically, for each project we construct the five augmented
datasets shown in Table 3. 𝐷𝑎𝑢𝑔is an augmented, but unbalanced,
dataset that contains 10 additional samples for each pair of bug
report and hunk, while 𝐷𝑏𝑙𝑖,𝑖=1,2,3,4, are balanced datasets
with different choices for ( 𝛼,𝜔) = {(0.7,1.0);(0.85,2.0);(1.0,2.0);
(1.3,2.0)} respectively. The rationale for these specific values of 𝛼
and𝜔is to explore 𝛼(in ranges that do not generate more data than
we can manage computationally), while selecting values for 𝜔that
do not constrain 𝛼’s effect. Given that augmentation increases the
number of positive samples, the number of negative samples grows
proportionally as well (i.e., for each positive sample, we randomly
create one negative sample). To ensure that the difference in perfor-
mance is in fact the result of DA, and not the diversity introduced
by a new negative samples, we created an additional baseline, 𝐷𝑟𝑒𝑝,that repeats positive samples without augmentation 10 times, and,
correspondingly, also adds 10 new negative samples. In effect, the
only difference between 𝐷𝑟𝑒𝑝and𝐷𝑎𝑢𝑔is the fact that 𝐷𝑎𝑢𝑔uses
augmented bug reports while 𝐷𝑟𝑒𝑝repeats the positive samples.
All models are evaluated on the same (unaugmented) test set. Since
TBERT-Single requires considerably more time than the other mod-
els (e.g., TBERT-Single takes more than 24h to run on the JDT
project), we only evaluate it on one of the balanced datasets - 𝐷𝑏𝑙1-
as it exhibits the best performance for TBERT-Siamese, which uses
a relatively similar DL architecture to TBERT-Single.
To evaluate the statistical significance of the difference in perfor-
mance when training DL models with and without data augmen-
tation, we use the Student’s paired t test to compute 𝑝-values be-
tween performance metrics of 𝐷𝑜𝑟𝑖and all other datasets (i.e., 𝐷𝑟𝑒𝑝,
𝐷𝑎𝑢𝑔,𝐷𝑏𝑙∗) [47,52]. The test assumes the performance values to
be normally distributed. We consider 𝑝<0.05to be statistically
significant.
Results. Table 4 shows the retrieval performance of FBL-BERT,
TBERT-Siamese and TBERT-Single trained on four dataset: 𝐷𝑜𝑟𝑖,
𝐷𝑟𝑒𝑝,𝐷𝑎𝑢𝑔and𝐷𝑏𝑙∗, where𝐷𝑏𝑙∗denotes the average best per-
forming balanced dataset for the given model. The top part of the
table shows results across bug reports from all projects, followed
by per project results. In general, we observe that the models im-
prove across all the metrics compared to 𝐷𝑜𝑟𝑖, with the lowest
improvement noted for 𝐷𝑟𝑒𝑝, followed by 𝐷𝑎𝑢𝑔, and with the high-
est improvement recorded for 𝐷𝑏𝑙∗.
Depending on the model the scale of the improvement varies.
While the MRR score for FBL-BERT increases from 0.264 for 𝐷𝑜𝑟𝑖to
0.367 for𝐷𝑏𝑙∗, about half of the improvement can be attributed to
the dataset size as indicated by the results for 𝐷𝑟𝑒𝑝with the MRR
score of 0.307. Moreover, we also observe that 𝐷𝑎𝑢𝑔improves the
score from 0.307 for 𝐷𝑟𝑒𝑝to 0.353, indicating that using an aug-
mented dataset makes a difference not only through data quantity.
The improvement between 𝐷𝑎𝑢𝑔and𝐷𝑏𝑙∗is marginal and equal to
0.014, indicating that even the best balancing configuration has a
small effect on FBL-BERT in general.
Training with a balanced dataset has a bigger impact on TBERT-
Single and TBERT-Siamese with an improvement of 0.035 and 0.092
in MRR scores respectively when compared to 𝐷𝑎𝑢𝑔. Moreover,
data balancing is the key contributor to the improvement in TBERT-
Siamese for which statistically significant difference is observed
only for𝐷𝑏𝑙∗. In the case of TBERT-Single, training the model with
both𝐷𝑎𝑢𝑔and𝐷𝑏𝑙∗leads to significant improvement across all the
metrics.
RQ1 (a): Data augmentation improves the DL-based bug
localization results across all models. Using data balancing
with augmentation can further improve performance.
Table 5 shows the retrieval accuracy for FBL-BERT and TBERT-
Siamese when the models are trained on different balanced datasets,
with the values of 𝛼,𝜔and the dataset size provided on the right
side of the table. In the case of FBL-BERT, 𝐷𝑏𝑙2and𝐷𝑏𝑙3provide on
average the best performance, improving MRR and MAP by 16.8%
and 14.8% compared to 𝐷𝑏𝑙1and𝐷𝑏𝑙4. However, as noted before, the
improvement over the imbalanced dataset 𝐷𝑎𝑢𝑔is marginal. In case

--- PAGE 9 ---
Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization Conference’17, July 2017, Washington, DC, USA
Table 4: Bug localization performance across all evaluation projects and for different training datasets: 𝐷𝑜𝑟𝑖– original dataset;
𝐷𝑟𝑒𝑝– dataset with 10x repeated instances; 𝐷𝑎𝑢𝑔– augmented dataset; 𝐷𝑏𝑙1−4– augmented and balanced datasets. Statistically
significant ( 𝑝<0.05) differences are marked with ↑and↓, where↑indicates significant improvement in performance compared
to𝐷𝑜𝑟𝑖, and↓indicates significant decrease compared to 𝐷𝑜𝑟𝑖.
MRR MAP P@1 P@3 P@5 MRR MAP P@1 P@3 P@5 MRR MAP P@1 P@3 P@5
FBL-BERT TBERT-Siamese TBERT-Single
𝐷𝑜𝑟𝑖 0.264 0.109 0.163 0.153 0.145 0.180 0.062 0.144 0.076 0.069 0.273 0.120 0.162 0.145 0.149
𝐷𝑟𝑒𝑝 0.307↑0.129↑0.213↑0.179 0.176 0.201 0.086 0.110 0.093 0.093 0.271 0.140↑0.152 0.136↑0.176
𝐷𝑎𝑢𝑔 0.353↑0.146↑0.247↑0.202↑0.197↑0.236 0.103 0.157 0.124 0.119↑0.333↑0.144↑0.217↑0.188↑0.194↑
𝐷𝑏𝑙∗ 0.367↑0.147↑0.267↑0.198↑0.206↑0.328↑0.107↑0.247↑0.150↑0.146↑0.368↑0.149↑0.269↑0.192↑0.182↑
Table 5: Bug localization performance with different aug-
mented and balanced training datasets.
MRR MAP P@1 P@3 P@5 𝛼 𝜔 #𝐷
Dataset FBL-BERT
𝐷𝑏𝑙1 0.314 0.128 0.210 0.183 0.177 0.70 1.0 260k
𝐷𝑏𝑙2 0.367 0.147 0.267 0.198 0.206 0.85 2.0 340k
𝐷𝑏𝑙3 0.357 0.155 0.260 0.204 0.215 1.00 2.0 360k
𝐷𝑏𝑙4 0.315 0.142 0.217 0.176 0.179 1.30 2.0 500k
TBERT-Siamese
𝐷𝑏𝑙1 0.328 0.107 0.247 0.150 0.146 0.70 1.0 260k
𝐷𝑏𝑙2 0.220 0.080 0.140 0.116 0.111 0.85 2.0 340k
𝐷𝑏𝑙3 0.215 0.081 0.130 0.105 0.117 1.00 2.0 360k
𝐷𝑏𝑙4 0.182 0.068 0.107 0.091 0.086 1.30 2.0 500k
of TBERT-Siamese, the smallest balanced dataset, 𝐷𝑏𝑙1, produces
the highest MRR score of 0.328 which outperforms other balanced
dataset by at least 49%.
The difference in the models’ performance with different datasets
can be attributed to (1) the overall difference in the models’ archi-
tectures affecting models demand for training data; and (2) the size
of each project measured as the number of hunks (see Table 2). To
better understand if and when model may require more data, in
Figure 4 we show MRR scores across all evaluation projects ordered
by their size, i.e., the number of hunks in a project.
RQ1 (b): The results indicate that different model architec-
tures may have different needs in terms of training dataset
size to achieve their optimal performance. Some models ben-
efit from more augmented samples, especially for larger
projects.
5.2 RQ2: Which of the proposed DA operators
contribute the most to retrieval
performance?
Setup. To better understand the influence of the proposed data
augmentation operators on the downstream model effectiveness,
we perform ablation studies on training datasets created using all
but one augmentation operator type. To this end, we create 5 types
AspectJ SWT Tomcat PDE JDTDori
Dbl1
Dbl2
Dbl3
Dbl40.122 0.473 0.318 0.243 0.288
0.158 0.502 0.402 0.298 0.272
0.206 0.595 0.452 0.401 0.319
0.185 0.529 0.424 0.446 0.343
0.18 0.396 0.401 0.308 0.3660.10.20.30.40.50.6
MRR
(a) FBL-BERT
AspectJ SWT Tomcat PDE JDTDori
Dbl1
Dbl2
Dbl3
Dbl40.193 0.535 0.083 0.073 0.094
0.203 0.572 0.475 0.171 0.224
0.147 0.274 0.371 0.091 0.135
0.167 0.315 0.281 0.105 0.118
0.165 0.195 0.268 0.086 0.0950.10.20.30.40.50.6
MRR
 (b) TBERT-Siamese
Figure 4: MRR scores for evaluation projects trained on dif-
ferent balanced datasets.
of augmented training datasets: No Backtranslation, No Insert, No
Delete, No Replace and No Swap operator. Note that we consider,
e.g., both Code Token Swap and Random Swap as operators of Swap
type. To balance the datasets, we use 𝛼and𝜔values from RQ1 that
resulted in the best performance for the models, i.e., for FBL-BERT
𝛼=0.85,𝜔=2.0, while for TBERT models 𝛼=0.7, and𝜔=1.0.
Results. Figure 5 shows the MRR scores for datasets augmented
with 4 out of 5 operator types as well as MRR scores of 𝐷𝑜𝑟𝑖and
𝐷𝑎𝑢𝑔as horizontal lines for reference. We note that most of the
operators contribute towards the final performance, with an ex-
ception of Swap operator for FBL-BERT. The lack of impact for
Swap operator can be attributed to the model architecture. Given
that FBL-BERT leverages all of the tokens in a bug report sepa-
rately, swapping the token positions does not preclude them from
being matched. On the other hand, excluding Random Insert affects
FBL-BERT the most, indicating that inserted tokens are valuable
to the model and improve its effectiveness when matching token
embeddings. The Delete operator is the most prominent contribu-
tor to the performance of both TBERT models. When the Delete
operator is disallowed during augmentation, the MRR score of aug-
mented datasets drops by 0.054 and 0.055 for TBERT-Single and
TBERT-Siamese respectively, indicating that the variance caused
by removing tokens randomly has a positive impact. The Delete
operator, when applied in relative moderation, seems to add to the
robustness of the models, i.e., the models create additional links
between terms and concepts in the bug reports and changesets. The
value of this process is also supported by recent work in adversarial

--- PAGE 10 ---
Conference’17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski
D o r i D o r iD o r iD a u gD a u gD a u g
Figure 5: MRR scores when trained with augmented data
using different DA operators.
training of large language models, like BERT, in order to improve
their robustness against malicious attacks [18].
RQ2: All DA operators contribute to the performance im-
provement with varying degree, with the exception of Swap
for FBL-BERT. The Delete operator consistently improves
performance in all three models.
5.3 Threats to validity
There are several validity threads of our findings. A threat to inter-
nal validity of the study are the parameter choices for DL-based
bug localization models, particularly in the context of (1) training
procedure; (2) BERT-base selection; and (3) parameters inherent
to each model. To mitigate that threat, during training we follow
recommendations of BERT authors [ 10], while for each model we
use parameters identified as optimal by the previous studies [ 6,29].
While in our study, we use BERTOverflow as our BERT base model,
other choices exist (e.g., CodeBERT [ 12]), and more models are
underway, hence we leave the evaluation of different BERT base
models in the context of bug localization to future work.
Another internal threat is in our choices of augmentation opera-
tors, their parameters (e.g., 𝜆), and how they are applied together
(e.g., stacking operators). This threat is mitigated by following best
known practices from the NLP augmentation literature that focus
on token-level operators and in-domain tasks [ 25,57]. While we
explore some parameter choices for data balancing in the paper (e.g.,
𝛼and𝜔), there are also additional parameters related to bug report
building process as well as other possible augmentation operators
that may provide more improvement.
The external tools we leveraged to build our augmentation pipeline,
e.g., infozilla, BEE tool, can introduce noise that propagates to our
reported results. However, these are state-of-the-art tools that have
been thoroughly evaluated so their error rate should be limited.
Furthermore, the randomness of the augmentation operators
may pose a threat to the internal validity. To mitigate that, we ensure
to set an initial value on the system’s pseudo-random number
generator when building an augmented dataset as well as when
training a DL model.
A threat to the external validity is that we evaluated the data aug-
mentation technique only for bug localization on a limited number
of bugs collected from a selection of open source Java projects. This
threat is mitigated by the fact that the dataset has been used in sev-
eral prior bug localization studies [ 43,58,60]. Another mitigatingfactor is that the projects reflect a variety of purposes, development
styles and histories.
Limitations in the chosen evaluation metrics pose a threat to
conclusion validity as they may not directly measure user satisfac-
tion with the retrieved change hunks [ 54]. The threat is mitigated
by the fact that the selected metrics are well-known and widely
accepted as best available to measure and compare the performance
of IR techniques.
6 CONCLUSION AND FUTURE WORK
DL models toward bug localization excel at bridging the lexical gap
between natural language describing a bug report and program-
ming language that defines the source code. However, training an
effective DL model requires large amount of project-specific la-
belled data (i.e., pairs of bug reports and bug-inducing changesets),
that is typically difficult to obtain in sufficient quantity for a single
project. To relax the requirement on data quantity, and enable using
DL model when training data is scarce, this work proposes to use
data augmentation (DA) to create new, realistically looking bug
reports that can be used to significantly increase the size of the
training set. To augment bug reports, we propose DA operators
that independently augment the natural language and code-related
content of a bug report. To build a new training dataset using aug-
mented bug reports, we propose a data balancing strategy that
selectively augments bug reports to add more training samples for
underrepresented parts of the source code.
The results indicate that the proposed data augmentation im-
proves retrieval accuracy across all studied DL models increasing
MRR score by 39% to 82% compared to the original, unaugmented
dataset. Moreover, when augmented datasets are compared against
training sets expanded by data repetition, we observe that they im-
prove MRR scores by 20% to 36%. All of the proposed DA operators
contribute to the final performance, with token deletion bringing
most consistent impact for different DL models.
This is one of the first papers to introduce data augmentation for
software engineering. We believe data augmentation as a technique
has potential for SE because the datasets are not as large as in main-
stream ML. In addition, data augmentation is not a one-size-fits-all
technique and its optimal application requires custom operators, so
the paper contributes in designing the data augmentation operators
for bug reports and applying them using a data balancing strategy.
Despite this, the proposed approach requires more experiments
to strengthen our observations and recommendations. As our fu-
ture work, we plan to (1) extend our evaluation datasets with new
software projects written in Java, Python and Javascript; (2) con-
duct experiments with more heavily augmented data, i.e., by using
DA operators on a larger number of tokens; (3) experiment with
different deletion operators (e.g., removing irrelevant code tokens)
given the good performance of Random Delete for natural language;
and (4) experiment with different configurations for the bug report
builder.
7 DATA AVAILABILITY
A replication package that includes all relevant code and scripts is
available at https://anonymous.4open.science/r/fbl-bert-987B.

--- PAGE 11 ---
Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization Conference’17, July 2017, Washington, DC, USA
REFERENCES
[1]Nicolas Bettenburg, Sascha Just, Adrian Schröter, Cathrin Weiss, Rahul Premraj,
and Thomas Zimmermann. 2008. What Makes a Good Bug Report?. In Proceedings
of the 16th ACM SIGSOFT International Symposium on Foundations of Software
Engineering (SIGSOFT ’08/FSE-16) .
[2]Gemma Catolino, Fabio Palomba, Andy Zaidman, and Filomena Ferrucci. 2019.
Not all bugs are the same: Understanding, characterizing, and classifying bug
types. Journal of Systems and Software (2019).
[3]O. Chaparro, J. M. Florez, and A. Marcus. 2017. Using Observed Behavior to
Reformulate Queries during Text Retrieval-based Bug Localization. In 2017 IEEE
International Conference on Software Maintenance and Evolution (ICSME) . 376–
387.
[4]Oscar Chaparro, Jing Lu, Fiorella Zampetti, Laura Moreno, Massimiliano Di Penta,
Andrian Marcus, Gabriele Bavota, and Vincent Ng. 2017. Detecting Missing
Information in Bug Descriptions. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering (ESEC/FSE 2017) .
[5]Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi Yang. 2021. Hiddencut: Simple
data augmentation for natural language understanding with better generalizabil-
ity. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) . 4380–4390.
[6]Agnieszka Ciborowska and Kostadin Damevski. 2022. Fast Changeset-Based
Bug Localization with BERT. In Proceedings of the 44th International Conference
on Software Engineering (ICSE ’22) . Association for Computing Machinery, New
York, NY, USA, 946–957.
[7]C. S. Corley, K. Damevski, and N. A. Kraft. 2018. Changeset-Based Topic Modeling
of Software Repositories. IEEE Transactions on Software Engineering (2018).
[8]Daniel Alencar da Costa, Shane McIntosh, Weiyi Shang, Uirá Kulesza, Roberta
Coelho, and Ahmed E. Hassan. 2017. A Framework for Evaluating the Results of
the SZZ Approach for Identifying Bug-Introducing Changes. IEEE Transactions
on Software Engineering (2017).
[9]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with
Contextual Neural Language Modeling. In Proceedings of the 42nd International
ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR’19) .
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies .
[11] Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,
Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation
approaches for nlp. arXiv preprint arXiv:2105.03075 (2021).
[12] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of the
Association for Computational Linguistics: EMNLP 2020 .
[13] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep Code Search. In
2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE) .
933–944.
[14] Jin Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically Enhanced
Software Traceability Using Deep Learning Techniques. In Proceedings of the 39th
International Conference on Software Engineering (ICSE ’17) .
[15] Rahul Gupta, Aditya Kanade, and Shirish Shevade. 2019. Neural Attribution for
Semantic Bug-Localization in Student Programs. In Advances in Neural Informa-
tion Processing Systems , H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc,
E. Fox, and R. Garnett (Eds.), Vol. 32.
[16] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,
Doug Downey, and Noah A Smith. 2020. Don’t stop pretraining: adapt language
models to domains and tasks. arXiv preprint arXiv:2004.10964 (2020).
[17] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu
Ubayashi. 2019. DeepJIT: An End-to-End Deep Learning Framework for Just-
in-Time Defect Prediction. In 2019 IEEE/ACM 16th International Conference on
Mining Software Repositories (MSR) . 34–45.
[18] Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and Cho-
Jui Hsieh. 2019. On the Robustness of Self-Attentive Models. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics . Association
for Computational Linguistics, Florence, Italy, 1520–1529. https://doi.org/10.
18653/v1/P19-1147
[19] Xuan Huo, Ming Li, and Zhi-Hua Zhou. 2016. Learning Unified Features from Nat-
ural and Programming Languages for Locating Buggy Source Code. In Proceedings
of the 25th International Joint Conference on Artificial Intelligence (IJCAI’16) .
[20] X. Huo, F. Thung, M. Li, D. Lo, and S. Shi. 2019. Deep Transfer Bug Localization.
IEEE Transactions on Software Engineering (2019), 1–1. https://doi.org/10.1109/
TSE.2019.2920771
[21] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).[22] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage
Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd
International ACM SIGIR conference on research and development in Information
Retrieval (SIGIR ’20) .
[23] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-
mization. https://arxiv.org/abs/1412.6980
[24] Pavneet Singh Kochhar, Yuan Tian, and David Lo. 2014. Potential Biases in Bug
Localization: Do They Matter?. In Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering (Vasteras, Sweden) (ASE ’14) .
803–814.
[25] Venelin Kovatchev, Phillip Smith, Mark Lee, and Rory Devine. 2021. Can vectors
read minds better than experts? Comparing data augmentation strategies for
the automated scoring of children’s mindreading ability. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long
Papers) .
[26] An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2017.
Bug Localization with Combination of Deep Learning and Information Retrieval.
InProceedings of the 25th International Conference on Program Comprehension
(ICPC ’17) . IEEE Press. https://doi.org/10.1109/ICPC.2017.24
[27] Jaekwon Lee, Dongsun Kim, Tegawendé F. Bissyandé, Woosung Jung, and Yves
Le Traon. 2018. Bench4BL: Reproducibility Study on the Performance of IR-
based Bug Localization. In Proceedings of the 27th ACM SIGSOFT International
Symposium on Software Testing and Analysis (Amsterdam, Netherlands) (ISSTA
2018) . 61–72.
[28] Jian Li, Pinjia He, Jieming Zhu, and Michael R. Lyu. 2017. Software Defect Pre-
diction via Convolutional Neural Network. In 2017 IEEE International Conference
on Software Quality, Reliability and Security (QRS) . 318–328.
[29] Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2021.
Traceability transformed: Generating more accurate links with pre-trained BERT
models. In 2021 IEEE/ACM 43rd International Conference on Software Engineering
(ICSE) . IEEE, 324–335.
[30] Vijayaraghavan Murali, Lee Gross, Rebecca Qian, and Satish Chandra. 2020.
Industry-scale IR-based Bug Localization: A Perspective from Facebook. In Pro-
ceedings of the 42nd International Conference on Software Engineering (ICSE ’20) .
[31] E. C. Neto, D. A. da Costa, and U. Kulesza. 2018. The impact of refactoring
changes on the SZZ algorithm: An empirical study. In IEEE 25th International
Conference on Software Analysis, Evolution and Reengineering (SANER) (SANER
2018) .
[32] Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov.
2019. Facebook FAIR’s WMT19 News Translation Task Submission. In Proceedings
of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers,
Day 1) .
[33] A. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V. Nguyen, and T. N. Nguyen. 2011.
A Topic-based Approach for Narrowing the Search Space of Buggy Files from
a Bug Report. In Proceedings of the 26th IEEE/ACM International Conference on
Automated Software Engineering (ASE 2011) . 263–272. https://doi.org/10.1109/
ASE.2011.6100062
[34] Toan Q Nguyen, Kenton Murray, and David Chiang. 2021. Data Augmentation by
Concatenation for Low-Resource Translation: A Mystery and a Solution. arXiv
preprint arXiv:2105.01691 (2021).
[35] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.
https://doi.org/10.48550/ARXIV.1901.04085
[36] Matteo Paltenghi and Michael Pradel. 2021. Thinking Like a Developer? Compar-
ing the Attention of Humans with Neural Models of Code. In 2021 36th IEEE/ACM
International Conference on Automated Software Engineering (ASE) . IEEE, 867–879.
[37] Chanathip Pornprasit and Chakkrit Tantithamthavorn. 2022. DeepLineDP: To-
wards a Deep Learning Approach for Line-Level Defect Prediction. IEEE Trans-
actions on Software Engineering (2022).
[38] Michael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik
Meijer, and Satish Chandra. 2020. Scaffle: bug localization on millions of files.
InProceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis .
[39] Rahul Premraj, Thomas Zimmermann, Sunghun Kim, and Nicolas Bettenburg.
2008. Extracting structural information from bug reports. In Proceedings of the
2008 international workshop on Mining software repositories - MSR 2008 .
[40] Mohammad Masudur Rahman and Chanchal K Roy. 2018. Improving IR-based
bug localization with context-aware query reformulation. In Proceedings of the
2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering . ACM, 621–632.
[41] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. arXiv:1908.10084 [cs.CL]
[42] Giovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele
Bavota, Michele Lanza, and Rocco Oliveto. 2021. Evaluating SZZ Implementations
Through a Developer-informed Oracle. arXiv:2102.03300 [cs.SE]
[43] Ripon K. Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E. Perry. 2013. Im-
proving Bug Localization Using Structured Information Retrieval. In Proceedings
of the 28th IEEE/ACM International Conference on Automated Software Engineering

--- PAGE 12 ---
Conference’17, July 2017, Washington, DC, USA Agnieszka Ciborowska and Kostadin Damevski
(Silicon Valley, CA, USA) (ASE’13) . 345–355.
[44] Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. 2020.
A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language
Understanding and Generation. arXiv preprint arXiv:2009.13818 (2020).
[45] Connor Shorten, Taghi M Khoshgoftaar, and Borko Furht. 2021. Text data aug-
mentation for deep learning. Journal of big Data 8, 1 (2021), 1–34.
[46] Jacek Sliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When Do
Changes Induce Fixes?. In Proceedings of the 2005 International Workshop on
Mining Software Repositories (MSR ’05) .
[47] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of
Statistical Significance Tests for Information Retrieval Evaluation. In Proceedings
of the Sixteenth ACM Conference on Conference on Information and Knowledge
Management (CIKM ’07) . 623–632.
[48] Yang Song and Oscar Chaparro. 2020. BEE: A Tool for Structuring and Ana-
lyzing Bug Reports. In Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering .
[49] Jeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and
Named Entity Recognition in StackOverflow. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics .
[50] Huy Tu and Tim Menzies. 2021. FRUGAL: Unlocking SSL for Software Analytics.
arXiv preprint arXiv:2108.09847 (2021).
[51] Huy Tu, Zhe Yu, and Tim Menzies. 2022. Better Data Labelling With EMBLEM
(and how that Impacts Defect Prediction). IEEE Transactions on Software Engi-
neering 48, 1 (2022), 278–294.
[52] Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical Significance
Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and
Type III Errors. In Proceedings of the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval (Paris, France) (SIGIR’19) .
[53] Bogdan Vasilescu, Yue Yu, Huaimin Wang, Premkumar Devanbu, and Vladimir
Filkov. 2015. Quality and Productivity Outcomes Relating to Continuous Integra-
tion in GitHub. In Proceedings of the 2015 10th Joint Meeting on Foundations of
Software Engineering (ESEC/FSE 2015) .
[54] Qianqian Wang, Chris Parnin, and Alessandro Orso. 2015. Evaluating the Use-
fulness of IR-Based Fault Localization Techniques. In Proceedings of the 2015
International Symposium on Software Testing and Analysis (ISSTA 2015) (Balti-
more, MD, USA). 1–11.
[55] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020. Deep Semantic
Feature Learning for Software Defect Prediction. IEEE Transactions on Software
Engineering 46, 12 (2020).
[56] Shaowei Wang and David Lo. 2014. Version History, Similar Report, and Structure:
Putting Them Together for Improved Bug Localization. In Proceedings of the 22Nd
International Conference on Program Comprehension (Hyderabad, India) (ICPC
2014) . 53–63.
[57] Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques for
Boosting Performance on Text Classification Tasks. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .
[58] Ming Wen, Rongxin Wu, and Shing-Chi Cheung. 2016. Locus: Locating Bugs from
Software Changes. In Proceedings of the 31st IEEE/ACM International Conference
on Automated Software Engineering (Singapore, Singapore) (ASE 2016) . 262–273.
[59] Ratnadira Widyasari, Stefanus Agus Haryono, Ferdian Thung, Jieke Shi, Con-
stance Tan, Fiona Wee, Jack Phan, and David Lo. 2022. On the Influence of
Biases in Bug Localization: Evaluation and Benchmark. In Proceedings of the 29th
IEEE International Conference on Software Analysis, Evolution and Reengineering
(SANER), RENE Track. (SANER 2022) .
[60] Chu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, and Hong
Mei. 2014. Boosting Bug-Report-Oriented Fault Localization with Segmentation
and Stack-Trace Analysis. In Proceedings of the 2014 IEEE International Conference
on Software Maintenance and Evolution (ICSME ’14) . 181–190.
[61] Xueqing Wu, Yingce Xia, Jinhua Zhu, Lijun Wu, Shufang Xie, Yang Fan, and
Tao Qin. 2021. mixSeq: A Simple Data Augmentation Methodfor Neural Ma-
chine Translation. In Proceedings of the 18th International Conference on Spoken
Language Translation (IWSLT 2021) .
[62] Yan Xiao, Jacky Keung, Kwabena E. Bennin, and Qing Mi. 2019. Improving bug
localization with word embedding and enhanced convolutional neural networks.
Information and Software Technology 105 (2019).
[63] Xinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep Learning
for Just-in-Time Defect Prediction. In 2015 IEEE International Conference on
Software Quality, Reliability and Security . 17–26. https://doi.org/10.1109/QRS.
2015.14
[64] Xin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to Rank Relevant Files for
Bug Reports Using Domain Knowledge. In Proceedings of the 22Nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering (Hong Kong,
China) (FSE 2014) . 689–699.
[65] Rahul Yedida and Tim Menzies. 2021. On the value of oversampling for deep
learning in software defect prediction. IEEE Transactions on Software Engineering
(2021).[66] W. Zou, D. Lo, Z. Chen, X. Xia, Y. Feng, and B. Xu. 2020. How Practitioners
Perceive Automated Bug Report Management Techniques. IEEE Transactions on
Software Engineering 46, 8 (2020).
