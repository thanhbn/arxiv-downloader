KHẢ NĂNG KIỂM THỬ CHƯƠNG TRÌNH CỦA CÁC MÔ HÌNH NGÔN NGỮ LỚN DÀNH CHO CODE

Weimin Xiong1,2, Yiwen Guo3∗, Hao Chen4
1Phòng thí nghiệm Dữ liệu lớn An ninh Tencent, 2Đại học Bắc Kinh, 3Nhà nghiên cứu độc lập, 4Đại học UC Davis
weiminxiong@tencent.com, guoyiwen89@gmail.com, chen@ucdavis.edu

TÓM TẮT
Sự phát triển gần đây của các mô hình ngôn ngữ lớn (LLM) cho code như CodeX và CodeT5+ thể hiện tiềm năng to lớn trong việc đạt được trí tuệ code. Khả năng tổng hợp code hoàn thành một chương trình để thực hiện một nhiệm vụ được định nghĩa trước đã được kiểm tra và xác minh một cách chuyên sâu trên các bộ dữ liệu benchmark bao gồm HumanEval và MBPP. Tuy nhiên, việc đánh giá các LLM này từ nhiều góc độ hơn (không chỉ tổng hợp chương trình) cũng được mong đợi, xét đến phạm vi ứng dụng rộng lớn của chúng trong kỹ thuật phần mềm. Trong bài báo này, chúng tôi khám phá khả năng của LLM trong việc kiểm thử các chương trình/code. Bằng cách thực hiện phân tích kỹ lưỡng các LLM gần đây dành cho code trong kiểm thử chương trình, chúng tôi cho thấy một loạt các tính chất thú vị của các mô hình này và chứng minh cách cải thiện khả năng kiểm thử chương trình của LLM. Theo các nghiên cứu gần đây sử dụng các test case được tạo ra để nâng cao tổng hợp chương trình, chúng tôi tiếp tục tận dụng các phát hiện của mình để cải thiện chất lượng của các chương trình được tổng hợp và cho thấy tỷ lệ pass code cao hơn +11.77% và +4.22% trên HumanEval+ so với baseline GPT-3.5-turbo và state-of-the-art gần đây, tương ứng. Code của chúng tôi có sẵn tại https://github.com/WeiminXiong/TestingLLM.

1 GIỚI THIỆU

Cộng đồng đã chứng kiến sự gia tăng mạnh mẽ trong việc phát triển các mô hình ngôn ngữ lớn (LLM), vốn đã đạt được khả năng đáng kinh ngạc trong việc hiểu và tạo ra không chỉ văn bản mà còn cả code. Các LLM cho code (CodeX (Chen et al., 2021), StarCoder (Li et al., 2023b), CodeT5+ (Wang et al., 2023b), v.v.) đã được áp dụng rộng rãi cho nhiều ứng dụng khác nhau để đạt được trí tuệ code. Tuy nhiên, việc đánh giá hiện tại của các LLM này chủ yếu tập trung vào hoàn thành/tổng hợp chương trình, mặc dù các mô hình cũng có thể được sử dụng trong các ứng dụng khác. Khi lĩnh vực tiếp tục phát triển, việc đánh giá các mô hình này từ nhiều góc độ hơn được mong đợi, điều này có thể tạo điều kiện cho việc hiểu sâu hơn về các LLM.

Là cốt lõi của việc phân tích hành vi của code, khả năng tạo ra các test case phù hợp là điều rất mong muốn đối với kỹ thuật phần mềm. Mặc dù sự phát triển sơ khai của việc sử dụng các mô hình sâu trong kiểm thử đã được thể hiện (Tufano et al., 2020; 2022), với sự tiến bộ đáng kể trong LLM, vẫn chưa rõ khả năng như vậy của AI đã tiến được đến đâu khi được trang bị những mô hình mạnh mẽ này. Trong bài báo này, chúng tôi, lần đầu tiên, phân tích khả năng của các LLM gần đây trong việc kiểm thử các chương trình/code. Các phân tích của chúng tôi được thực hiện dựa trên 164 bài toán từ HumanEval+ (Chen et al., 2021) và 427 bài toán được làm sạch từ MBPP (Austin et al., 2021). Chúng tôi xem xét 4 cài đặt tạo test-case (tức là, self-generated, all-generated, oracle, và placeholder trong Hình 1) và kiểm tra một tập hợp 11 LLM cạnh tranh dành cho code (bao gồm 4 LLM có khoảng 1 tỷ tham số và 7 LLM lớn hơn đáng kể). Chúng tôi đã tiến hành nhiều thí nghiệm khác nhau, từ đó nhiều thông điệp rút ra thú vị được đưa ra.

Một số bài báo rất gần đây (Shi et al., 2022; Li et al., 2023a; Chen et al., 2023) đã cho thấy rằng việc sử dụng phù hợp ngay cả các test case được tạo ra có thể cải thiện chất lượng tổng hợp chương trình, theo tinh thần rằng các chương trình được tổng hợp có thể vượt qua một số lượng lớn test case có nhiều khả năng đúng hơn. Tuy nhiên, chất lượng của các test case được tạo ra ảnh hưởng lớn đến hiệu suất của các phương pháp như vậy. Do thiếu đánh giá hệ thống về khả năng kiểm thử của LLM cho code, vẫn chưa rõ làm thế nào để tạo ra các test case có khả năng hữu ích hơn cho tổng hợp chương trình và, một cách rộng lớn hơn, trí tuệ code. Các nghiên cứu trong bài báo này nhằm làm sáng tỏ điều này. Chúng tôi sẽ chứng minh rằng, hiệu suất tổng hợp chương trình được cải thiện đáng kể có thể đạt được bằng cách sử dụng các thông điệp rút ra trong các nghiên cứu của chúng tôi. Cụ thể, trên GPT-3.5-turbo, chúng tôi có thể đạt được tỷ lệ pass code cao hơn +11.77% trên HumanEval+, so với baseline GPT-3.5-turbo. Khi so sánh với một state-of-the-art rất gần đây gọi là CodeT, giải pháp của chúng tôi đạt được tỷ lệ pass code cao hơn +4.22%.

2 METRICS ĐÁNH GIÁ

Để làm cho việc đánh giá đáng tin cậy và toàn diện hơn, điều quan trọng là phải thiết kế một số metrics phù hợp, như BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), và tỷ lệ pass (Chen et al., 2021) để đánh giá dịch máy, tóm tắt văn bản, và tổng hợp chương trình, tương ứng. Trong phần này, chúng tôi chỉ định hai metrics đánh giá chính để đánh giá khả năng kiểm thử chương trình của LLM, từ góc độ tính đúng đắn và đa dạng.

Tỷ lệ pass Trong kỹ thuật phần mềm, chúng ta mong đợi các test case đại diện cho một số chức năng "ground-truth" mong muốn của chương trình/code được kiểm thử. Trong thực tế, chức năng "ground-truth" như vậy có thể được mô tả trong các comment header của một hàm (tức là, docstring của hàm) và được kiểm thử bằng cách sử dụng implementation oracle, như trong HumanEval (Chen et al., 2021) và MBPP Austin et al. (2021). Chương trình/code oracle nên có thể vượt qua kiểm thử, nếu một test case được tạo ra là đúng. Do đó, chúng tôi tận dụng tỷ lệ pass như một thước đo để đánh giá tính đúng đắn của các test case được tạo ra. Để so sánh công bằng, chúng tôi hướng dẫn mỗi mô hình tạo ra ba test case trong prompt, và, khi một mô hình tạo ra nhiều hơn ba test case, chúng tôi chọn ba test case đầu tiên để đánh giá. Giả sử có tổng cộng M bài toán lập trình trong một bộ dữ liệu thí nghiệm và, cho mỗi bài toán, chúng ta có N implementation chương trình/code để tạo ra test case. Mỗi mô hình chỉ có một cơ hội để tạo ra các test case này cho mỗi chương trình/code. Sau đó, chúng tôi tính tỷ lệ pass như sau:

P=1/(M*N) * ∑(i=1 to M)∑(j=1 to N) pij/nij, (1)

trong đó nij là số test case trong Qij bao gồm không quá ba test case được tạo ra cho implementation chương trình/code thứ j của bài toán thứ i bởi LLM được đánh giá cùng lúc, tức là, Qij={(xijk, yijk)}k, và pij là số test case (trong Qij) không fail oracle.

Tỷ lệ pass được định nghĩa trong Eq. (1) đo lường tính đúng đắn của các test case được tạo ra. Tuy nhiên, như có thể thấy trong Hình 1, mô hình có thể tạo ra các test case trùng lặp ít hữu ích hơn, mặc dù chúng đúng. Để tránh bias đánh giá như vậy, chúng tôi tiếp tục ủng hộ khử trùng lặp trong tập hợp các test case được coi là đúng, dẫn đến tính toán tỷ lệ pass đã khử trùng lặp được định nghĩa là P′=1/(M*N)∑∑p′ij/n′ij, trong đó chúng tôi sử dụng ′ để biểu thị số lượng test case duy nhất.

Tỷ lệ coverage Ngoài các tỷ lệ pass ở trên, chúng tôi tiếp tục xem xét tỷ lệ coverage như một metric chi tiết hơn để đánh giá tính đa dạng của các test case được tạo ra. Theo định nghĩa của nó, tỷ lệ coverage tính toán mức độ code được thực thi, cho một test case. Vì, đối với mỗi chương trình/code, chúng tôi giữ không quá ba test case cùng lúc, chúng tôi tính toán bao nhiêu phần trăm của cấu trúc điều khiển được bao phủ cho các test case này. Tương tự như Eq. (1), chúng tôi đánh giá hiệu suất kiểm thử tất cả các chương trình/code trên tất cả M×N lần tạo ra, tức là, chúng tôi tính toán

C=1/(M*N) * ∑(i=1 to M)∑(j=1 to N) cij. (2)

Chúng tôi sử dụng thư viện pytest để đánh giá branch coverage cho tất cả ba test case cho mỗi code và tổng hợp kết quả cho tất cả chương trình/code và tất cả các bài toán. Rõ ràng, C cao hơn cho thấy khả năng kiểm thử tốt hơn của một LLM, vì chúng ta mong đợi tất cả các phần của chương trình/code được thực thi để tìm ra tất cả các bug tiềm ẩn, với tập hợp test case được tạo ra bởi LLM này.

3 CÁC MÔ HÌNH NGÔN NGỮ LỚN DÀNH CHO CODE

Trong phần này, chúng tôi nêu các mô hình được đánh giá. Chúng tôi áp dụng một số mô hình "nhỏ" có số tham số khoảng 1B (cụ thể hơn, từ 770M đến 1.3B trong lựa chọn của chúng tôi) và một số mô hình lớn hơn đạt được hiệu suất state-of-the-art trong nhiệm vụ tổng hợp chương trình.

Đối với các mô hình nhỏ, chúng tôi sử dụng InCoder (1.3B) (Fried et al., 2023), CodeGen2 (1B) (Nijkamp et al., 2023a), CodeT5+ (770M) (Wang et al., 2023b), và SantaCoder (1.1B) (Allal et al., 2023). InCoder là một mô hình tạo sinh thống nhất có thể thực hiện tổng hợp chương trình/code cũng như chỉnh sửa code, và nó kết hợp các điểm mạnh của causal language modeling và masked language modeling. Mô hình CodeGen2 được huấn luyện trên một tập con đã khử trùng lặp của bộ dữ liệu Stack v1.1 (Kocetkov et al., 2023), và việc huấn luyện của nó được định dạng với một hỗn hợp các mục tiêu cho causal language modeling và span corruption. CodeT5+ là một mô hình encoder-decoder được huấn luyện trên một số nhiệm vụ tiền huấn luyện bao gồm span denoising và hai biến thể của causal language modeling. SantaCoder được huấn luyện trên code Python, Java, và JavaScript trong bộ dữ liệu Stack. Tỷ lệ pass (Chen et al., 2021) của các chương trình được tạo ra bởi các mô hình này được so sánh trong Bảng 1. Khi đánh giá tỷ lệ pass (chương trình), chúng tôi để mô hình tạo ra 200 implementation code cho mỗi bài toán, và chúng tôi đặt temperature thành 0.2, 0.6, và 0.8 để tính toán pass@1, pass@10, và pass@100, tương ứng.

Đối với các mô hình lớn hơn đạt được hiệu suất tổng hợp chương trình state-of-the-art, chúng tôi sử dụng CodeGen2 (16B) (Nijkamp et al., 2023a), CodeGen-Multi (16B) Nijkamp et al. (2023b), CodeGen-Mono (16B) Nijkamp et al. (2023b), StarCoder (15B) (Li et al., 2023b), WizardCoder (15B) (Luo et al., 2023), CodeGeeX2 (6B) (Zheng et al., 2023), và GPT-3.5-turbo. CodeGen-Multi và CodeGen-Mono là hai mô hình lớn từ phiên bản đầu tiên của CodeGen. CodeGen-Multi đầu tiên được huấn luyện trên bộ dữ liệu pile (Gao et al., 2020) và sau đó được huấn luyện trên một tập con của bộ dữ liệu BigQuery có sẵn công khai chứa code được viết bằng C, C++, Go, Java, JavaScript, và Python. Dựa trên mô hình CodeGen-Multi 16B, CodeGen-Mono (16B) được thu được bằng cách tinh chỉnh thêm trên một tập hợp code Python được thu thập từ GitHub. Cho một mô hình cơ sở được tiền huấn luyện trên 1 nghìn tỷ token từ bộ dữ liệu Stack, mô hình StarCoder 15B được thu được bằng cách huấn luyện nó trên 35B token code Python. WizardCoder tiếp tục tăng cường StarCoder với instruction tuning, theo một chiến lược instruction evolution tương tự như trong WizardLM (Xu et al., 2023). CodeGeeX2, thế hệ thứ hai của một mô hình tạo sinh đa ngôn ngữ cho code, được thực hiện dựa trên kiến trúc ChatGLM2 và được huấn luyện trên nhiều dữ liệu code hơn. GPT-3.5-turbo là một LLM thương mại rất có khả năng được phát triển bởi OpenAI và chúng tôi đã truy cập nó vào tháng 8 năm 2023. Đối với các LLM lớn này, chúng tôi đã kiểm tra pass@1 của tất cả các mô hình ngoại trừ GPT-3.5-turbo (kết quả của nó có thể được thu thập trực tiếp từ bài báo của Liu et al. (2023a)). Bằng cách sắp xếp pass@1 của chúng từ cao đến thấp, chúng được xếp hạng như sau: GPT-3.5-turbo (61.7%), WizardCoder (46.23%, 15B), CodeGeeX2 (29.97%, 6B), StarCoder (27.9%, 15B), CodeGen-Mono (26.15%, 16B), CodeGen2 (19.33%, 16B), CodeGen-Multi (15.35%, 16B). Xếp hạng trên bộ dữ liệu MBPP tương tự.

Mô hình | Kích thước | Pass@1 | Pass@10 | Pass@100
InCoder | 1.3B | 6.99%/14.06% | 14.20%/34.98% | 23.76%/55.34%
CodeGen2 | 1B | 9.19%/17.50% | 16.06%/36.86% | 25.90%/59.32%
CodeT5+ | 770M | 12.95%/28.02% | 25.09%/47.69% | 37.56%/65.26%
SantaCoder | 1.1B | 15.21%/29.42% | 26.01%/51.30% | 43.80%/69.10%

Bảng 1: Hiệu suất tổng hợp chương trình của các LLM nhỏ (có số tham số khoảng 1 tỷ) được đánh giá trên HumanEval+ / MBPP (đã làm sạch).

4 CODE CẦN ĐƯỢC KIỂM THỬ

Để đánh giá khả năng kiểm thử của LLM, chúng ta cần một oracle để biểu thị chức năng ground-truth của code được kiểm thử. May mắn thay, các bộ dữ liệu hiện tại để đánh giá hiệu suất tổng hợp chương trình thường cung cấp các oracle như vậy (xem HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021)). Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng một phiên bản sửa đổi của HumanEval gọi là HumanEval+ (Liu et al., 2023a), cùng với MBPP (phiên bản đã làm sạch). Các bộ dữ liệu này được thiết lập để đánh giá hiệu suất lập trình Python cơ bản của LLM, và chúng chứa 164 và 427 bài toán, tương ứng.

4.1 IMPLEMENTATION CODE KHÔNG HOÀN HẢO

Để mô phỏng các tình huống thực tế nơi code được kiểm thử thường có lỗi, chúng tôi đầu tiên áp dụng các chương trình/code được tổng hợp như là các chương trình/code cần được kiểm thử, xem xét rằng việc tổng hợp ngay cả các LLM state-of-the-art vẫn chưa hoàn hảo. Chúng tôi đánh giá hiệu suất của mỗi LLM trong việc kiểm thử code được tạo ra bởi chính nó (được ký hiệu là "Self-generated") và code trong một tập hợp bao gồm kết quả hoàn thành chương trình của một số LLM khác nhau (được ký hiệu là "All-generated"). Tức là, các LLM được so sánh lấy các implementation code khác nhau khi tạo ra test case cho mỗi bài toán lập trình trong cài đặt self-generated. Trong khi đó, trong cài đặt all-generated, các implementation chương trình/code giống nhau được đưa cho các LLM khác nhau để tạo ra test case để so sánh.

Trong thực tế, chúng tôi áp dụng InCoder (1.3B), CodeGen2 (1B), CodeT5+ (770M), và SantaCoder (1.1B) để xây dựng tập hợp chương trình/code all-generated, trong khi, trong cài đặt self-generated, mỗi LLM đầu tiên tổng hợp code và hoàn thành một chương trình để đáp ứng yêu cầu của mỗi bài toán lập trình, và sau đó LLM tạo ra test case với các chương trình/code được tổng hợp trong prompt của nó. Temperature cho tất cả LLM được đặt thống nhất là 0.2 để tổng hợp các chương trình/code trong cả hai cài đặt. Chúng tôi thu được 100 hoàn thành chương trình/code cho mỗi bài toán và chúng tôi nhắc mỗi LLM tạo ra 3 test case cho mỗi implementation chương trình/code trong cài đặt self-generated, và chúng tôi lấy mẫu 100 implementation từ kết quả tổng hợp của InCoder (1.3B), CodeGen2 (1B), CodeT5+ (770M), và SantaCoder (1.1B) để tạo thành tập hợp code all-generated, tức là, chúng tôi có N = 100 cho các cài đặt này.

Chúng tôi tuân theo cùng một cách tạo ra code như được giới thiệu trong các bài báo của những LLM này. Đối với mô hình không có instruction tuning, như InCoder và CodeT5+, chúng tôi tổng hợp chương trình/code sử dụng prompt mặc định được đưa ra bởi mỗi bài toán lập trình trong bộ dữ liệu kiểm thử, trong khi, đối với các mô hình đã áp dụng instruction tuning, ví dụ, WizardCoder, chúng tôi sử dụng prompt được khuyến nghị trong các bài báo của họ.

4.2 IMPLEMENTATION CODE TỐI ƯU

Như một tham chiếu, chúng tôi cũng báo cáo hiệu suất tạo ra các test case chính xác và đa dạng khi code được viết hoàn toàn chính xác, điều này được đạt được bằng cách áp dụng oracle như là các chương trình/code cần được kiểm thử (và cài đặt như vậy được ký hiệu là "Oracle"). Vì Liu et al. (2023a) đã báo cáo rằng một số code oracle trong bộ dữ liệu HumanEval có thể không chính xác, chúng tôi áp dụng tập hợp oracle được sửa đổi trong HumanEval+ trong cài đặt này. Chúng tôi tiếp tục sử dụng các implementation code oracle đã sửa đổi thay vì những implementation ban đầu trong việc đánh giá tỷ lệ pass (tức là, P′) của các test case được tạo ra. Xem xét rằng các bộ dữ liệu công khai thường chỉ cung cấp một implementation oracle cho mỗi bài toán, và để giữ cho tính không chắc chắn của kết quả đánh giá nhất quán, chúng tôi sao chép implementation oracle 100 lần và chúng tôi nhắc tạo ra 3 test case cho mỗi bản sao này. Nó có thể được coi là để N = 100, giống như trong các cài đặt trước đó trong Phần 4.1.

4.3 KHÔNG CÓ IMPLEMENTATION

Trong một số tình huống nhất định, chúng ta yêu cầu các test case trước khi hàm/chương trình được thực hiện đầy đủ, do đó chúng tôi cũng đánh giá trong một cài đặt nơi phần thân chính của một hàm/chương trình được kiểm thử chỉ là một placeholder, như được mô tả trong Hình 1(b). Tình huống này thường xảy ra khi code chính chưa được thực hiện cho một hàm/chương trình hoặc kỹ sư kiểm thử không muốn đưa bias implementation vào LLM khi tạo ra test case cho một hàm/chương trình. Chúng tôi ký hiệu cài đặt như vậy là "Placeholder" trong bài báo này. Chúng tôi cũng để N = 100, như trong cài đặt oracle.

5 TẠO RA TEST CASE

Trong phần này, chúng tôi giới thiệu cách các test case có thể được tạo ra, khi implementation của một hàm/chương trình được đưa ra như được mô tả trong Phần 4. Trong bài báo này, một test case mong muốn là một cặp input và output mong đợi của nó cho hàm/chương trình được định nghĩa trong ngữ cảnh. Như một ví dụ, Hình 1 chứng minh một số test case cho bài toán lập trình kiểm tra xem hai từ có thỏa mãn một mẫu xoay cụ thể hay không. Để tạo ra test case, chúng tôi sử dụng các LLM được giới thiệu trong Phần 3.

Chúng tôi đã viết các prompt bổ sung để hướng dẫn LLM tạo ra ba test case cho mỗi code được đưa ra bao gồm các docstring mô tả mục đích của hàm này, như được mô tả trong Hình 1. Hướng dẫn của chúng tôi yêu cầu LLM (1) "check the correctness of this function with three test" và (2) bắt đầu viết test code với một câu lệnh "assert" và hàm được kiểm thử, điều này chỉ định định dạng của các test case như các cặp input-output có thể được phân tích. Ví dụ, cho ví dụ trong Hình 1, prompt bổ sung nên là "# Check the correctness of this function with three test cases \n assert cycpattern check".

Sau đó chúng tôi nối prompt bổ sung với code và đưa việc nối vào mỗi LLM, để trích xuất test case từ output của mô hình. LLM sẽ cố gắng hoàn thành input đã cho bằng cách tạo ra một hoặc nhiều câu lệnh "assert", và chúng tôi chia kết quả tạo ra thành các chuỗi con, với "assert" như là dấu phân cách. Mỗi chuỗi con sau đó được coi là một câu lệnh kiểm thử, và chúng tôi chỉ lấy ba câu lệnh đầu tiên nếu có nhiều hơn ba câu lệnh, như đã được giới thiệu trong Phần 2. Việc chia như vậy có thể được coi là một thao tác hậu xử lý hiệu quả cải thiện đáng kể chất lượng test code được tạo ra, xem xét rằng một số đoạn code vô nghĩa có thể được tạo ra trong output của các LLM. Khi sử dụng HumanEval+ và MBPP, chúng tôi cố gắng loại bỏ các test case trong docstring của hàm, nếu có, chỉ để thoát khỏi các gợi ý rộng từ docstring (Chen et al., 2023). Temperature để tạo ra test case được giữ là 0.2.

Sau khi thu được, các test case được tạo ra sau đó được biên dịch, và được đánh giá về tính đúng đắn và tính đa dạng để báo cáo tỷ lệ pass P′ và tỷ lệ coverage C. Khi tính toán, cho mỗi bài toán và mỗi tập hợp hoàn thành được tạo ra, chúng tôi tạo một thư mục tạm thời.

6 KẾT QUẢ CHÍNH CHO TẠO RA TEST CASE

Kết quả thí nghiệm của các LLM nhỏ và lớn trên HumanEval+ có thể được tìm thấy trong Bảng 2 và Bảng 3, tương ứng. Bảng 4 cho thấy kết quả trên MBPP. Có một số điểm rút ra từ các bảng này.

• Thứ nhất, các test case được tạo ra bởi LLM có thể cho thấy tỷ lệ pass tốt, và tỷ lệ pass này thậm chí còn cao hơn tỷ lệ pass code trên HumanEval+, điều này đúng cho cả LLM lớn và nhỏ. Kết quả như vậy phù hợp với trực giác từ công việc trước đó loại bỏ code không thể vượt qua các kiểm thử được tạo ra để cải thiện chất lượng tổng hợp chương trình.

Mô hình | Kích thước | Oracle | Self-generated | All-generated | Placeholder
InCoder | 1.3B | 21.31% (61.43%) | 23.37% (59.36%) | 22.72% (61.10%) | 25.19% (62.75%)
CodeGen2 | 1B | 31.63% (71.55%) | 30.62% (69.38%) | 30.93% (69.70%) | 30.69% (69.00%)
CodeT5+ | 770M | 35.43% (71.45%) | 32.34% (70.45%) | 31.49% (69.75%) | 32.67% (70.67%)
SantaCoder | 1.1B | 30.97% (71.46%) | 30.43% (70.81%) | 30.13% (70.55%) | 30.78% (71.24%)

Bảng 2: Tỷ lệ pass (và tỷ lệ coverage) của các test case được tạo ra trên HumanEval+ trong các cài đặt khác nhau cho LLM với khoảng 1 tỷ tham số.

Mô hình | Kích thước | Oracle | Self-generated | All-generated | Placeholder
CodeGen-Multi | 16B | 43.88% (67.91%) | 41.85% (69.30%) | 40.38% (66.97%) | 39.74% (68.28%)
CodeGen2 | 16B | 46.34% (73.07%) | 45.44% (73.17%) | 42.00% (72.45%) | 42.69% (72.86%)
CodeGen-Mono | 16B | 49.03% (74.82%) | 45.73% (73.74%) | 43.91% (73.66%) | 44.92% (73.63%)
StarCoder | 15B | 55.07% (76.02%) | 52.52% (72.45%) | 48.20% (72.30%) | 50.58% (74.52%)
CodeGeeX2 | 6B | 57.03% (74.42%) | 53.16% (73.55%) | 49.28% (70.32%) | 51.78% (73.08%)
WizardCoder | 15B | 53.89% (77.87%) | 55.47% (76.07%) | 48.02% (75.27%) | 49.89% (75.12%)
GPT-3.5-turbo | - | 71.03% (77.85%) | 72.45% (77.24%) | 59.24% (74.99%) | 66.28% (74.03%)

Bảng 3: Tỷ lệ pass (và tỷ lệ coverage) của các test case được tạo ra trên HumanEval+ trong các cài đặt khác nhau cho LLM có tham số rõ ràng nhiều hơn 1 tỷ.

• Thứ hai, tính đúng đắn của các test case được tạo ra có tương quan thuận với khả năng tạo ra code của LLM (xem Hình 2, trong đó mỗi dấu chéo đỏ đại diện cho hiệu suất của một mô hình), có nghĩa là một LLM cho thấy hiệu suất tổng hợp chương trình state-of-the-art có khả năng cũng là LLM state-of-the-art cho kiểm thử chương trình. Như được hiển thị trong Bảng 2 và 3, GPT-3.5-turbo, tổng hợp chương trình/code với tính đúng đắn cao nhất, cung cấp test case với tỷ lệ pass cao nhất (71.03%) trên HumanEval+. Đối với một LLM, càng chính xác nó có khả năng tổng hợp chương trình/code trên một bộ dữ liệu, khả năng kiểm thử mạnh mẽ hơn có thể sẽ được thể hiện trên cùng bộ dữ liệu đó. Cũng tồn tại một vài ngoại lệ, ví dụ, SantaCoder (1.1B) vượt trội hơn CodeT5+ (770M) và CodeGen2 (1B) trong việc tạo ra code, nhưng nó cho thấy hiệu suất kém hơn trong kiểm thử chương trình trên HumanEval+. Bằng cách kiểm tra kỹ các test case được tạo ra bởi SantaCoder trên HumanEval+, chúng tôi phát hiện rằng nó có xu hướng tạo ra các test case phức tạp và dài hơn CodeT5+ cho một số bài toán trên HumanEval+, vốn thường mong muốn hơn trong kiểm thử chương trình. Đây cũng là lý do tại sao các test case SantaCoder cho thấy tỷ lệ coverage cao hơn trong Bảng 2. Cụ thể, trong Bài toán 131 trong HumanEval+, nơi chương trình được yêu cầu trả về tích của tất cả các chữ số với vị trí lẻ trong một số nguyên dương n (đó là input), test input được cung cấp bởi CodeT5+ có xu hướng nhỏ cho bài toán này, ví dụ, n = 2, trong khi các test case SantaCoder có xu hướng có nhiều chữ số hơn (ví dụ, n = 12358), điều này hữu ích trong việc phát hiện các bug ẩn. Tuy nhiên, tạo ra các test case dài hơn và phức tạp hơn đầy thử thách hơn, và tính đúng đắn có thể thấp hơn.

• Thứ ba, như có thể thấy trong Bảng 3 và 4, tạo ra test case sử dụng các LLM lớn với code tự tạo ra của chúng (trong prompt) thường dẫn đến mức độ đúng đắn cao hơn, so với kết quả placeholder. Quan sát này thực sự không bất ngờ, xem xét rằng tạo ra code trước và test case sau đó giống với chain-of-thought prompting (Wei et al., 2022) (nếu áp dụng placeholder được coi là plain prompting), điều này có lợi cho suy luận. Hơn nữa, hiệu suất self-generated của một LLM đôi khi thậm chí vượt trội hơn hiệu suất kiểm thử của nó với một oracle, và chúng tôi quy cho điều này: 1) tính ngẫu nhiên trong phong cách của các oracle vốn ít số lượng và/hoặc 2) ít sự chuyển đổi phân phối giữa code tự tạo ra trong prompt và code huấn luyện, đối với một số LLM mạnh mẽ.

• Thứ tư, chỉ với một vài ngoại lệ, các test case thu được bằng cách sử dụng code oracle thể hiện code coverage hơi cao hơn, trong khi tỷ lệ coverage đạt được trong các cài đặt khác (tức là, cài đặt self-generated, all-generated, và placeholder) thường hơi thấp hơn.

Bốn thông điệp rút ra ở trên đều có thể được suy ra từ Bảng 2, 3, và 4. Ngoài tất cả những kết quả này, chúng tôi tiến hành thêm các thí nghiệm để đạt được những thông điệp rút ra sau.

• Thứ năm, bằng cách phân tích mối quan hệ giữa chất lượng code trong prompt và tính đúng đắn của kiểm thử, chúng tôi phát hiện rằng implementation code đúng trong prompt thường dẫn đến chất lượng tạo ra test code cao hơn so với trường hợp khi một số code không chính xác được đưa ra. Chúng tôi tiến hành một thí nghiệm nơi chúng tôi đầu tiên chọn các bài toán lập trình trong HumanEval+, nơi tỷ lệ pass code của một LLM không phải là 0% hoặc 100%. Sau đó chúng tôi tách các chương trình/code tự tạo ra của mô hình thành hai nhóm, với một nhóm chỉ chứa chương trình/code được coi là đúng và nhóm khác chỉ chứa chương trình/code không chính xác. Trong Bảng 5, chúng tôi so sánh hiệu suất sử dụng hai loại code này trong prompt, để tạo ra test case sử dụng cùng một LLM. Rõ ràng, chất lượng test case thu được với chương trình/code đúng cao hơn rõ rệt. Chúng tôi tiếp tục đánh giá hiệu suất kiểm thử tổng thể của LLM chỉ với chương trình/code tự tạo ra đúng, nếu có, trong prompt của chúng. Không giống như trong Bảng 5 nơi chúng tôi không lấy các bài toán có thể được giải quyết 100% hoặc 0%, chúng tôi lấy tất cả các bài toán đã cho trong đánh giá này, ngoại trừ, cho mỗi bài toán, chúng tôi loại bỏ tất cả chương trình/code tự tạo ra không chính xác nếu tồn tại ít nhất một implementation đúng được tổng hợp bởi LLM được đánh giá. Bằng cách này, chúng tôi có thể quan sát khả năng kiểm thử chương trình được cải thiện đáng kể trên HumanEval+ (tức là, 74.95% cho GPT-3.5-turbo, 56.87% cho WizardCoder, 54.33% cho CodeGeeX2, và 53.24% cho StarCoder), so với kết quả self-generated ban đầu trong Bảng 3. Tương tự trên MBPP. Nhớ lại rằng, trong thông điệp rút ra thứ ba của chúng tôi, chúng tôi đã đề cập rằng các test case thu được với chương trình/code tự tạo ra đôi khi thậm chí vượt trội hơn những test case được tạo ra trong cài đặt oracle trên HumanEval+, có thể một phần do ít sự chuyển đổi phân phối giữa chương trình/code được tổng hợp và code huấn luyện, các kết quả trên xác nhận thêm rằng, nếu chúng ta có thể cải thiện tính đúng đắn của chương trình/code được tổng hợp trong khi giữ phong cách phù hợp, thì khả năng kiểm thử mạnh mẽ hơn có thể được đạt được thêm.

• Thứ sáu, bằng cách tiến hành một thí nghiệm bổ sung, chúng tôi tiếp tục so sánh chất lượng test case được thu thập từ các vị trí khác nhau trong kết quả tạo ra. Cho mỗi tập hợp ba test case được tạo ra, chúng tôi phân tích mối quan hệ giữa tính đúng đắn của chúng và thứ tự khi chúng được tạo ra. Kết quả được minh họa trong Hình 3. Như có thể thấy trong hình, test case đầu tiên được tạo ra thường cho thấy tính đúng đắn tốt nhất và những test case được tạo ra sau đó không chính xác hơn. Điều này có thể do thực tế rằng mô hình có xu hướng đầu tiên tạo ra nội dung với mức độ tin cậy cao (cũng có nhiều khả năng đúng hơn).

Mô hình | Kích thước | Oracle | Self-generated | All-generated | Placeholder
InCoder | 1.3B | 21.56% (46.81%) | 17.98% (46.11%) | 19.53% (46.45%) | 22.58% (46.72%)
CodeGen2 | 1B | 25.61% (54.26%) | 21.85% (53.09%) | 23.15% (50.43%) | 22.81% (52.11%)
CodeT5+ | 770M | 29.02% (56.86%) | 24.44% (52.31%) | 24.84% (53.20%) | 25.59% (55.81%)
SantaCoder | 1.1B | 32.37% (55.68%) | 26.40% (52.38%) | 26.20% (52.83%) | 26.53% (53.86%)
CodeGen-Multi | 16B | 41.32% (60.63%) | 35.96% (59.03%) | 34.17% (58.09%) | 34.84% (58.92%)
CodeGen2 | 16B | 45.30% (62.15%) | 38.67% (60.16%) | 36.77% (58.59%) | 37.27% (59.16%)
CodeGen-Mono | 16B | 50.24% (64.39%) | 43.94% (62.94%) | 39.55% (61.99%) | 42.41% (62.31%)
StarCoder | 15B | 54.84% (65.10%) | 46.77% (63.60%) | 42.80% (61.95%) | 45.35% (62.66%)
CodeGeeX2 | 6B | 52.45% (64.64%) | 44.52% (63.72%) | 41.72% (60.48%) | 43.86% (63.51%)
WizardCoder | 15B | 57.85% (66.68%) | 46.56% (64.86%) | 41.62% (60.72%) | 47.45% (64.54%)
GPT-3.5-turbo | - | 74.30% (66.19%) | 66.14% (65.30%) | 49.56% (62.95%) | 63.34% (64.72%)

Bảng 4: Tỷ lệ pass (và tỷ lệ coverage) của các test case được tạo ra trên MBPP.

7 CẢI THIỆN TỔNG HỢP CHƯƠNG TRÌNH SỬ DỤNG CÁC TEST CASE ĐƯỢC TẠO RA

Các test case chất lượng cao không chỉ được mong muốn trong phân tích chương trình, mà còn hữu ích cho tổng hợp chương trình. Các phương pháp trước đây đã thành công sử dụng các test case được tạo ra để cải thiện hiệu suất của LLM trong việc tổng hợp chương trình/code. Ví dụ, Li et al. (2023a) đã thiết kế một prompt đặc biệt bao gồm các test case như một bước sơ bộ, nếu chúng có sẵn, để tạo ra chương trình/code. Shi et al. (2022) đã giới thiệu một cơ chế Bayes risk decoding, thực thi code được tạo ra từ một tập hợp ứng viên trên một số lượng nhỏ test input và chọn bằng cách tổng hợp qua các implementation chia sẻ cùng output khi được đưa các test input này. Nó sử dụng tính nhất quán giữa output của code được thực hiện đúng. Tiến thêm một bước, Chen et al. (2023) đã đề xuất CodeT, tận dụng LLM để thu được test case đầu tiên và kiểm thử tất cả chương trình/code được tổng hợp với các test case này bằng cách thực hiện một thỏa thuận thực thi kép, xem xét cả thỏa thuận giữa output thực thi và test output và tính nhất quán giữa output của các implementation chương trình đúng, để đạt được state-of-the-art. Chúng tôi khuyến khích độc giả quan tâm đọc bài báo gốc.

Trong phần trước, chúng tôi đã thu được kết quả về nhiều tính chất thú vị của hiệu suất kiểm thử chương trình của LLM cho code. Trong phần này, chúng tôi muốn hướng dẫn độc giả suy nghĩ xem có thể sử dụng những kết quả này để cải thiện hiệu suất tổng hợp chương trình hay không, xem xét rằng các test case (được tạo thủ công và đưa ra hoặc được tạo tự động đặc biệt) được sử dụng rộng rãi và thành công trong tổng hợp chương trình. Chúng tôi sẽ chứng minh rằng, bằng cách sử dụng các thông điệp rút ra trong Phần 6, hiệu suất tổng hợp chương trình của các phương pháp trước đây có thể được cải thiện đáng kể. Lấy CodeT làm ví dụ về state-of-the-art trước đây, phương pháp sử dụng placeholder để tạo ra test case và coi tất cả test case đều đúng như nhau như một prior. Tuy nhiên, như được thảo luận trong thông điệp rút ra thứ ba của chúng tôi, sử dụng code tự tạo ra giúp đạt được khả năng mạnh mẽ hơn trong việc tạo ra test case đúng. Hơn nữa, nếu nhiều test case được cung cấp trong một lần chạy tạo ra cho một LLM, tính đúng đắn của test case giảm theo thứ tự tạo ra của chúng, như được hiển thị trong điểm thứ năm của chúng tôi. Do đó, để đạt được hiệu suất tổng hợp chương trình vượt trội, chúng tôi giới thiệu hai sửa đổi đơn giản cho nó: 1) chúng tôi sử dụng cài đặt "self-generated" thay vì cài đặt "placeholder" để tạo ra test case, có nghĩa là chúng tôi tổng hợp chương trình trước và sau đó tạo ra test case cho mỗi chương trình trong prompt, 2) chúng tôi gán các trọng số khác nhau cho các test case được tạo ra dựa trên thứ tự của chúng trong mỗi kết quả tạo ra.

Chúng tôi kiểm tra tính hiệu quả của việc sử dụng 1) prompt bao gồm code tự tạo ra (SG) và 2) các test case có trọng số dựa trên thứ hạng (RW), trong việc cải thiện hiệu suất tổng hợp chương trình trên HumanEval+. Chi tiết triển khai của chúng tôi được giới thiệu như sau. Theo Chen et al. (2023), chúng tôi sử dụng temperature là 0.8 để tạo ra code và test case tự tạo ra. Mỗi test case được gán trọng số p^(i-1) với i là thứ tự của nó trong output mô hình, và chúng tôi để p = 0.8.

Bảng 6 cho thấy kết quả. Chúng tôi so sánh CodeT với CodeT+SG, CodeT+RW, và CodeT+SG+RW. Đối với CodeT, chúng tôi tuân theo triển khai chính thức của họ và tạo ra 100×5 test case cho mỗi bài toán. Để so sánh công bằng, chúng tôi đảm bảo rằng các giải pháp của chúng tôi với SG và/hoặc RW tạo ra cùng số lượng implementation chương trình và test case như CodeT. Do đó, cho mỗi bài toán trong HumanEval+, chúng tôi tổng hợp một chương trình cùng với 5 test case của nó cho 100 lần khi SG và/hoặc RW được kết hợp, tức là, chúng tôi có i ∈ {1,2,3,4,5}. Có thể thấy từ bảng rằng cả SG và RW đều cải thiện hiệu suất tổng hợp chương trình đáng kể trên hầu hết các LLM, ngoại trừ Incoder, CodeGen2-1B, CodeT5+, và SantaCoder mà các test case được tạo ra trong cài đặt placeholder cho thấy tính đúng đắn tương tự hoặc thậm chí cao hơn so với cài đặt self-generated và SG thất bại với chúng. Đối với một số LLM, SG mạnh mẽ hơn, trong khi, trên các mô hình khác bao gồm SantaCoder và StarCoder, RW mạnh mẽ hơn. Bằng cách kết hợp SG và RW, hiệu suất tổng hợp chương trình của hầu hết các LLM mạnh mẽ trong Bảng 6 cải thiện, so với chỉ sử dụng một trong hai. Trên GPT-3.5-turbo và WizardCoder, là hai mô hình tốt nhất trong việc tổng hợp chương trình trên HumanEval+, chúng tôi đạt được mức tăng hiệu suất +4.22% và +3.04% cho CodeT, tương ứng, với SG & RW.

Mô hình | Kích thước | với code đúng | với code không đúng | #Bài toán
InCoder | 1.3B | 28.55% | 27.39% | 27
CodeGen2 | 1B | 27.25% | 25.74% | 11
CodeT5+ | 770M | 40.19% | 36.78% | 27
SantaCoder | 1.1B | 37.45% | 34.08% | 24
CodeGen-Multi | 16B | 55.49% | 50.06% | 32
CodeGen2 | 16B | 43.56% | 39.31% | 29
CodeGen-Mono | 16B | 45.18% | 42.86% | 56
StarCoder | 15B | 58.16% | 57.08% | 68
CodeGeeX2 | 6B | 52.84% | 48.63% | 51
WizardCoder | 15B | 48.02% | 45.12% | 54
GPT-3.5-turbo | - | 75.39% | 68.52% | 126

Bảng 5: Với code đúng (tự tạo ra), các LLM cho thấy khả năng mạnh hơn trong việc tạo ra test case đúng trên HumanEval+ (được đánh giá chỉ trên những bài toán không thể được giải quyết 0% hoặc 100%), so với trường hợp khi code tự tạo ra không chính xác được đưa ra trong prompt. Vì hầu hết các LLM không thể tạo ra bất kỳ code đúng nào cho nhiều bài toán khó trong khi chúng thường tạo ra code không chính xác ngay cả cho các bài toán dễ, số bài toán được kiểm tra trong thí nghiệm này tăng theo sức mạnh của LLM được kiểm tra, như được hiển thị trong cột ngoài cùng bên phải.

Mô hình | Kích thước | Baseline | CodeT | + SG | + RW | + SG & RW
InCoder | 1.3B | 6.99% | 9.85% | 9.45% | 10.26% | 9.98%
CodeGen2 | 1B | 9.19% | 15.15% | 14.89% | 15.67% | 15.35%
CodeT5+ | 770M | 12.95% | 16.57% | 16.28% | 17.19% | 16.98%
SantaCoder | 1.1B | 15.21% | 18.43% | 18.17% | 18.75% | 18.63%
CodeGen-Multi | 16B | 15.35% | 24.50% | 25.71% | 25.72% | 26.95%
CodeGen2 | 16B | 19.33% | 27.56% | 28.51% | 28.43% | 29.63%
CodeGen-Mono | 16B | 26.15% | 35.63% | 36.69% | 36.63% | 37.95%
StarCoder | 15B | 27.90% | 40.46% | 41.21% | 42.12% | 43.15%
CodeGeeX2 | 6B | 29.97% | 44.16% | 45.23% | 44.92% | 46.32%
WizardCoder | 15B | 46.23% | 58.41% | 60.13% | 59.60% | 61.45%
GPT-3.5-turbo | - | 61.70% | 69.25% | 72.45% | 70.75% | 73.47%

Bảng 6: Hiệu suất tổng hợp chương trình (Pass@1) của các LLM có thể được cải thiện đáng kể bằng cách sử dụng các thông điệp rút ra của chúng tôi trong Phần 6. Thí nghiệm được thực hiện trên HumanEval+.

Chúng tôi tin rằng tồn tại những cách khác để sử dụng các thông điệp rút ra của chúng tôi để cải thiện hiệu suất tổng hợp chương trình, và chúng tôi muốn khuyến khích các nghiên cứu tương lai khám phá thêm theo hướng này.

8 NGHIÊN CỨU LIÊN QUAN

Tạo ra test case thông qua phân tích chương trình. Tạo ra các test case hợp lý để phân tích chương trình là một vấn đề lâu dài trong cộng đồng kỹ thuật phần mềm. Các kỹ thuật phân tích chương trình khác nhau, ví dụ, fuzzing, đã được phát triển để đạt được mục tiêu này. AFL++ (Fioraldi et al., 2020) là công cụ phổ biến nhất kết hợp nhiều kỹ thuật trong danh mục này. Một điểm yếu chính của các kỹ thuật này là khả năng hiểu được của các test case được tạo ra.

Tạo ra test case thông qua deep learning. Sự phát minh ra transformer (Vaswani et al., 2017) và tiền huấn luyện tự giám sát (Devlin et al., 2018; Lewis et al., 2019; Raffel et al., 2020; Radford et al., 2018) đã mang lại một bước đột phá cho xử lý ngôn ngữ lập trình. Sau khi được huấn luyện theo cách tự giám sát trên một corpus code lớn và đa dạng, các LLM đã thể hiện khả năng đáng kể trong việc hiểu và tổng hợp chương trình. Chúng tôi cũng đã chứng kiến sự thích ứng của các LLM được tiền huấn luyện (ví dụ, ChatGPT) với fuzz testing (Xia et al., 2023) gần đây. Tuy nhiên, vẫn thiếu và đòi hỏi các phân tích sâu sắc và so sánh chuyên sâu của các LLM khác nhau trong kiểm thử chương trình. Đặc biệt, các LLM mạnh mẽ xuất hiện liên tục. Ví dụ, WizardCoder gần đây (Luo et al., 2023) thể hiện sự vượt trội rõ ràng về tổng hợp chương trình so với các LLM mã nguồn mở khác. Trong nghiên cứu của chúng tôi, chúng tôi tập trung vào các phân tích và so sánh của các LLM trong việc viết test code và tạo ra test case.

Đánh giá Mô hình Ngôn ngữ Lớn. Gần đây, các mô hình ngôn ngữ lớn (LLM) đã kích thích sự quan tâm đáng kể trong cả học thuật và công nghiệp. Để đánh giá khả năng của các mô hình ngôn ngữ lớn, nhiều nỗ lực đã được dành ra từ các góc độ độ chính xác xử lý ngôn ngữ tự nhiên/lập trình, tính mạnh mẽ, đạo đức, bias, và độ tin cậy, v.v. Ví dụ, PromptBench (Zhu et al., 2023) chứng minh rằng các LLM hiện tại nhạy cảm với các prompt đối nghịch, và kỹ thuật prompt cẩn thận là cần thiết để đạt được hiệu suất tốt với chúng. Một ví dụ khác, DecodingTrust (Wang et al., 2023a), cung cấp một khám phá đa mặt về độ tin cậy của các mô hình GPT, đặc biệt là GPT-3.5 và GPT-4. Đánh giá mở rộng vượt ra ngoài các mối quan tâm về độ tin cậy điển hình để bao gồm một số khía cạnh quan trọng mới. Agentbench (Liu et al., 2023b) đánh giá LLM như các agent trên các nhiệm vụ đầy thử thách trong môi trường tương tác. Kết quả thí nghiệm của họ cho thấy rằng, trong khi các LLM thương mại hàng đầu thể hiện khả năng mạnh mẽ trong việc hoạt động như các agent trong môi trường phức tạp, có một sự khác biệt đáng kể về hiệu suất giữa chúng và các đối thủ cạnh tranh mã nguồn mở.

9 KẾT LUẬN

Trong bài báo này, chúng tôi đã thực hiện phân tích kỹ lưỡng các LLM gần đây (chủ yếu là LLM cho code) trong việc kiểm thử chương trình/code. Thông qua các thí nghiệm toàn diện với 11 LLM trên các bộ dữ liệu benchmark lập trình bao gồm HumanEval+ và MBPP (phiên bản đã làm sạch), chúng tôi đã khám phá ra một loạt các đặc điểm thú vị của các LLM này cho kiểm thử chương trình/code. Chúng tôi đã minh họa cách các khả năng kiểm thử chương trình của những LLM này có thể được nâng cao trong việc so sánh kết quả thực nghiệm chuyên sâu trong bốn cài đặt khác nhau. Dựa trên các phát hiện của chúng tôi, chúng tôi cũng có khả năng cải thiện hiệu suất của các LLM state-of-the-art trong việc tổng hợp chương trình/code với test case có chất lượng cao hơn. Như một nghiên cứu sơ bộ, chúng tôi tin rằng bài báo của chúng tôi có thể cung cấp những hiểu biết nghiên cứu mới và khơi dậy những ý tưởng mới trong tổng hợp chương trình/code, tạo ra test-case, và hiểu biết về LLM, và chúng tôi mong đợi sự khám phá tương lai theo hướng này trong công việc tương lai.

TÀI LIỆU THAM KHẢO

Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ktrw68Cmu9c.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Andrea Fioraldi, Dominik Maier, Heiko Eißfeldt, and Marc Heuse. {AFL++}: Combining incremental steps of fuzzing research. In 14th USENIX Workshop on Offensive Technologies (WOOT 20), 2020.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=hQwb-lbM6EL.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harm de Vries. The stack: 3 TB of permissively licensed source code. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=pxpbTdUEpD.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.

Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. Towards enhancing in-context learning for code generation. arXiv preprint arXiv:2303.17780, 2023a.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004.

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023a.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023b.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.

Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309, 2023a.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis, 2023b.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311–318, 2002.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural language to code translation with execution. arXiv preprint arXiv:2204.11454, 2022.

Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617, 2020.

Michele Tufano, Shao Kun Deng, Neel Sundaresan, and Alexey Svyatkovskiy. Methods2test: A dataset of focal methods mapped to test cases. In Proceedings of the 19th International Conference on Mining Software Repositories, pp. 299–303, 2022.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models, 2023a.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. Universal fuzzing via large language models. arXiv preprint arXiv:2308.04748, 2023.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568, 2023.

Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, and Xing Xie. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts, 2023.

A PHỤ LỤC

A.1 PHÂN TÍCH THÊM VỀ KẾT QUẢ THỰC NGHIỆM

Trong phần này, chúng tôi cung cấp phân tích thêm về kết quả thực nghiệm trong Phần 6.

Liên quan đến tình huống mà chất lượng test case được tạo ra bởi SantaCoder thấp hơn so với được tạo ra bởi CodeT5+ trên bộ dữ liệu HumanEval+, chúng tôi đã giải thích rằng điều này có thể do SantaCoder có xu hướng tạo ra các test case dài hơn và phức tạp hơn. Ở đây chúng tôi tiếp tục chứng minh rằng SantaCoder có khả năng tạo ra output chính xác hơn khi được đưa ra cùng testing input như của CodeT5+. Để cho thấy điều này, chúng tôi đầu tiên trích xuất phần input của các test case (bao gồm các testing input được ghép cặp với output tương ứng của chúng) được tạo ra bởi CodeT5+ trong cài đặt oracle. Sau đó chúng tôi để SantaCoder tạo ra testing output cho các input này, và đánh giá độ chính xác của các test case như vậy. Kết quả cho thấy rằng, cho các testing input đã có, SantaCoder và CodeT5+ đạt được độ chính xác 41.67% và 40.34%, tương ứng, cho thấy rằng SantaCoder thực sự mạnh hơn, nếu cùng testing input được đưa ra và nó không có cơ hội tạo ra các testing input phức tạp hơn.
