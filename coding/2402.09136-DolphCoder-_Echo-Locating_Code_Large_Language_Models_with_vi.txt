# 2402.09136.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/coding/2402.09136.pdf
# Kích thước tệp: 1090795 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DolphCoder: Định vị Mã nguồn bằng Mô hình Ngôn ngữ Lớn thông qua
Điều chỉnh Hướng dẫn Đa dạng và Đa mục tiêu
Yejie Wang1*, Keqing He2*, Guanting Dong1, Pei Wang1, Weihao Zeng1, Muxi Diao1
Yutao Mou1,Mengdi Zhang2,Jingang Wang2,Xunliang Cai2,Weiran Xu1†
1Đại học Bưu điện và Viễn thông Bắc Kinh, Bắc Kinh, Trung Quốc
2Meituan, Bắc Kinh, Trung Quốc
{wangyejie,dongguanting,wangpei,zengwh,dmx,myt,xuweiran}@bupt.edu.cn
{hekeqing,zhangmengdi02,wangjingang02,caixunliang}@meituan.com
Tóm tắt
Các Mô hình Ngôn ngữ Lớn cho Mã nguồn (Code LLMs)
đã thể hiện hiệu suất vượt trội trong các tác vụ
liên quan đến mã nguồn. Một số phương pháp điều
chỉnh hướng dẫn đã được đề xuất để nâng cao
hiệu suất tạo mã của các Code LLMs đã được
tiền huấn luyện. Trong bài báo này, chúng tôi giới
thiệu một mô hình hướng dẫn đa dạng (DolphCoder)
với khả năng tự đánh giá cho việc tạo mã. Nó học
các mục tiêu hướng dẫn đa dạng và kết hợp một
mục tiêu đánh giá mã để nâng cao khả năng tạo
mã của nó. Mô hình của chúng tôi đạt được hiệu
suất vượt trội trên các benchmark HumanEval và
MBPP, thể hiện những hiểu biết mới cho công việc
điều chỉnh hướng dẫn mã trong tương lai. Những
phát hiện chính của chúng tôi là: (1) Bổ sung thêm
các phản hồi đa dạng với các đường dẫn lý luận
khác biệt làm tăng khả năng mã của LLMs. (2) Cải
thiện khả năng đánh giá tính đúng đắn của các giải
pháp mã cũng nâng cao khả năng tạo ra chúng.

1 Giới thiệu
Các mô hình tiền huấn luyện mã đã đạt được tiến
bộ đáng chú ý trong kỷ nguyên của các mô hình
ngôn ngữ lớn (LLMs), chẳng hạn như Codex (Chen
et al., 2021), AlphaCode (Li et al., 2022), và PaLM-
Coder (Chowdhery et al., 2022b). Các tác vụ liên
quan đến mã cũng là những yếu tố chính trong việc
đánh giá khả năng của LLMs.
Rất nhiều code LLMs đã được đề xuất, bao gồm
các mô hình nguồn đóng (Chen et al., 2021; Li
et al., 2022; OpenAI, 2023) và các mô hình nguồn
mở (Li et al., 2023; Rozière et al., 2023). Chúng
thực hiện việc tiền huấn luyện tốn kém bằng cách
sử dụng lượng lớn dữ liệu mã và hiển thị hiệu suất
ấn tượng.

Trái ngược với những code LLMs tiền huấn luyện
này, một paradigm nhẹ khác để nâng cao khả năng
mã là điều chỉnh hướng dẫn sử dụng dữ liệu liên
quan đến mã chất lượng cao tương đối nhỏ. Ví dụ,
Code Alpaca (Chaudhary, 2023) sử dụng phương
pháp tự hướng dẫn tương tự như Alpaca (Taori
et al., 2023) để tạo các hướng dẫn mã thông qua
ChatGPT1 của OpenAI. Hơn nữa, WizardCoder (Luo
et al., 2023) giới thiệu phương pháp Evol-Instruct
phức tạp hơn (Xu et al., 2023a) để phát triển dữ
liệu hướng dẫn hiện có nhằm tạo ra các bộ dữ liệu
phức tạp và đa dạng hơn. Thay vào đó, OctoPack
(Muennighoff et al., 2023) và Magicoder (Wei et al.,
2023) xây dựng các hướng dẫn mã bằng cách khai
thác corpus mã hiện có. Tất cả các phương pháp
này đều nâng cao hiệu suất của các Code LLMs
nguồn mở.

Tuy nhiên, các phương pháp này có hai điểm yếu:
(1) Chúng chỉ lấy câu trả lời vàng duy nhất nhưng
bỏ qua tính đa dạng của câu trả lời trong việc tạo
mã. Chúng tôi thấy rằng việc bổ sung thêm các phản
hồi đa dạng sử dụng các prompt hệ thống khác nhau
làm tăng khả năng mã của LLMs. (2) Các mô hình
hiện tại tạo ra các đoạn mã hợp lý về mặt ngữ pháp
và logic nhưng không thể xác định các lỗi tinh vi,
chẳng hạn như các trường hợp biên và định dạng
đầu vào/đầu ra sai. Không có gì đảm bảo rằng việc
lấy mẫu nhiệt độ sẽ luôn tạo ra câu trả lời chính xác
theo thời gian. Chúng tôi cho rằng LLMs có khả
năng tạo ra các giải pháp đúng nhưng gặp khó khăn
trong việc phân biệt đúng sai. Cải thiện khả năng
đánh giá tính đúng đắn của mã cũng nâng cao khả
năng tạo ra nó.

Được truyền cảm hứng từ hai hiểu biết này, chúng
tôi giới thiệu một mô hình hướng dẫn đa dạng
(DolphCoder) với tự đánh giá cho việc tạo mã.
Cụ thể, chúng tôi sử dụng Code Llama-python làm
mô hình cơ sở và thu được dữ liệu hướng dẫn phát
triển theo WizardCoder. Sau đó được thúc đẩy bởi
rejection sampling (Touvron et al., 2023b) và ORCA
(Mukherjee et al., 2023), chúng tôi sử dụng các
prompt hệ thống khác nhau để tạo ra các câu trả
lời đa dạng thông qua ChatGPT. Sau khi loại bỏ
dữ liệu chất lượng thấp và tương tự bằng các quy
tắc heuristic (Luo et al., 2023; Di et al., 2023a),
chúng tôi thực hiện điều chỉnh có giám sát

1https://openai.com/blog/ChatGPT arXiv:2402.09136v1 [cs.CL] 14 Feb 2024

--- TRANG 2 ---
Phản hồi 2
Sắp xếp nổi bọt...Hướng dẫn
Sắp xếp một danh sách trong 
O(nlogn).GPT
3.5Phản hồi 1
Sắp xếp nhanh...
Phản hồi 2
Sắp xếp trộn...
Phản hồi N
Sắp xếp heap......Phản hồi 1
Sắp xếp nhanh...
Phản hồi N
Sắp xếp heap......Đánh giá 1
 Đáp ứng yêu cầu 
của bạn...
 O(n log n)...
Đánh giá 2
Câu trả lời 
sai... 
O(n^2)...
Đánh giá N
Giải pháp 
tốt...  
O(nlogn)...MẫuGPT4
 Đánh giá
...
(a) Điều chỉnh Hướng dẫn Đa dạng (b) Điều chỉnh Hướng dẫn Đa mục tiêuCác prompt 
hệ thống 
khác nhauDolphCoder

Hình 1: Kiến trúc tổng thể của phương pháp điều chỉnh hướng dẫn đa dạng với tự đánh giá cho việc tạo mã được đề xuất,
DolphCoder. Giai đoạn (a) biểu thị Điều chỉnh Hướng dẫn Đa dạng (DIT) và Giai đoạn (b) biểu thị Điều chỉnh Hướng dẫn
Đa mục tiêu (MOT) cho tự đánh giá.

trên dữ liệu hướng dẫn còn lại. Hơn nữa, chúng tôi
khám phá liệu việc cải thiện khả năng đánh giá mã
có giúp tạo ra nó hay không. Chúng tôi đề xuất một
framework học đa tác vụ tự đánh giá bằng cách
thêm một mục tiêu đánh giá mã vào tác vụ điều
chỉnh hướng dẫn truyền thống. Chúng tôi thấy rằng
việc huấn luyện mô hình cho cả việc tạo mã và đánh
giá mã đều có lợi cho khả năng mã.

Những đóng góp chính của chúng tôi được tóm tắt
như sau:

1.Chúng tôi giới thiệu một mô hình hướng dẫn đa
dạng (DolphCoder) với tự đánh giá cho việc tạo
mã. Nó học các mục tiêu hướng dẫn đa dạng và
kết hợp một mục tiêu đánh giá mã để nâng cao
khả năng tạo mã của nó.

2.DolphCoder vượt trội hơn các code LLMs nguồn
mở mạnh mẽ với biên độ lớn, bao gồm
CODELLAMA-INSTRUCT, OctoCoder, và
WizardCoder.

2 Phương pháp

Trong phần này, chúng tôi trình bày chi tiết phương
pháp luận của DolphCoder. Như được hiển thị trong
Hình 1, DolphCoder có hai giai đoạn huấn luyện:
(1) Đầu tiên là Điều chỉnh Hướng dẫn Đa dạng (DIT)
với nhiều câu trả lời chuỗi suy nghĩ cho cùng một
hướng dẫn. (2) Thứ hai là Điều chỉnh Đa mục tiêu
(MOT) kết hợp tác vụ tạo mã và tác vụ đánh giá mã
cả hai đều dưới dạng tác vụ tạo ngôn ngữ tự nhiên.

2.1 Điều chỉnh Hướng dẫn Đa dạng

Chúng tôi theo kỹ thuật Evol-Instruct (Xu et al.,
2023a; Luo et al., 2023) để xây dựng corpus huấn
luyện của chúng tôi. Dựa trên bộ dữ liệu Code
Alpaca2, chúng tôi phát triển lặp đi lặp lại các bài
toán lập trình trong bộ dữ liệu này thông qua phát
triển sâu để có được các hướng dẫn mới.3 Đối với
mỗi hướng dẫn, sau đó chúng tôi sử dụng các prompt
hệ thống khác nhau để truy vấn ChatGPT và thu
được các mục tiêu đa dạng. Những prompt hệ thống
này nhằm mục đích bổ sung các hướng dẫn người
dùng và mô tả tác vụ để cung cấp thêm các giải pháp
mã với các đường dẫn lý luận đa dạng. Chúng tôi
hiển thị các prompt hệ thống trong Hình 2.

Chúng tôi thấy rằng các câu trả lời đa dạng hơn có
thể tăng khả năng mã của LLMs (xem Phần 4.2) và
lập luận rằng các phong cách giải pháp mã khác
nhau cung cấp thêm tín hiệu giám sát cho mô hình.
Tương tự như Luo et al. (2023); Di et al. (2023a),
chúng tôi cũng sử dụng một số quy tắc heuristic
để loại bỏ dữ liệu chất lượng thấp và tương tự.
Cuối cùng, chúng tôi có được một bộ dữ liệu hướng
dẫn mã đa dạng với kích thước 510k và sử dụng
Code LLama làm LLM nền tảng để tinh chỉnh.

2.2 Điều chỉnh Hướng dẫn Đa mục tiêu

Chúng tôi phát hiện rằng các mô hình hướng dẫn
hiện tại tạo ra cả các giải pháp mã đúng và sai khi
được lấy mẫu ngẫu nhiên. Chúng tôi lập luận rằng
LLMs có thể tạo ra các giải pháp đúng nhưng gặp
khó khăn trong việc phân biệt đúng sai. Do đó,
chúng tôi khám phá liệu việc cải thiện khả năng
đánh giá mã có giúp tạo ra nó hay không.

Chúng tôi lấy mẫu 5k hướng dẫn từ bộ dữ liệu
trên và sử dụng mô hình trong giai đoạn đầu tiên
để tạo ra 100 câu trả lời bằng cách lấy mẫu nhiệt
độ. Tiếp theo,

2https://huggingface.co/datasets/sahil2801/CodeAlpaca-
20k
3Vì các bộ dữ liệu hoặc mã gốc không được phát hành, chúng
tôi tái tạo quy trình phát triển theo WizardLM (Xu et al., 2023a)
và sửa đổi các prompt phát triển theo bài báo WizardCoder gốc.

--- TRANG 3 ---
Các Prompt Hệ thống
1. [TRỐNG]
2. Bạn là một trợ lý mã biết nhiều ngôn ngữ 
lập trình và cách dịch giữa chúng. Được 
giao một tác vụ, bạn giải thích bằng các 
bước đơn giản tác vụ đang yêu cầu gì, 
bất kỳ hướng dẫn nào nó cung cấp, và 
cách sử dụng những hướng dẫn đó để 
tìm giải pháp.
3. Bạn là một trợ lý AI. Bạn sẽ được giao 
một tác vụ. Bạn phải tạo ra một câu trả 
lời chi tiết và dài.
4. Bạn là một trợ lý AI, biết mọi ngôn ngữ 
và cách dịch từ ngôn ngữ này sang ngôn 
ngữ khác. Được giao một tác vụ, bạn giải 
thích bằng các bước đơn giản tác vụ đang 
yêu cầu gì, bất kỳ hướng dẫn nào mà nó 
cung cấp.
5. Bạn giải quyết tác vụ và cho thấy cách 
bạn đã sử dụng các hướng dẫn để giải 
quyết tác vụ.
6. Bạn là một giáo viên. Được giao một tác 
vụ, bạn giải thích bằng các bước đơn giản 
tác vụ đang yêu cầu gì, bất kỳ hướng dẫn 
nào nó cung cấp và cách sử dụng những 
hướng dẫn đó để tìm câu trả lời.

Hình 2: Chúng tôi sử dụng những prompt hệ thống này để tạo ra các phản hồi đa dạng hơn trong đó [TRỐNG] có nghĩa là không có prompt hệ thống.

chúng tôi sử dụng GPT-44 để xác minh tính đúng
đắn của các câu trả lời đã tạo được khử trùng lặp
về mặt ngữ pháp, logic và hiệu quả. Trong các thí
nghiệm sơ bộ của chúng tôi, Chúng tôi gặp khó
khăn trong việc đánh giá tính đúng đắn của mã
bằng ChatGPT hoặc các LLMs hiện có khác. Chúng
tôi cũng xem xét việc sử dụng tín hiệu biên dịch
từ trình thực thi mã nhưng thấy rằng hầu hết mã
được tạo ra đều đúng về mặt ngữ pháp. Các công
trình trước đây (Liu et al., 2023a; Shen et al.,
2023a) sử dụng các bài kiểm tra đơn vị hiện có
trong tập huấn luyện, điều này không áp dụng được
cho tình huống của chúng tôi. Chúng tôi để lại
nhiều phương pháp đánh giá hơn cho công việc
tương lai. Cuối cùng, chúng tôi thu được bộ dữ
liệu đánh giá mã với kích thước 370k.5 Chúng tôi
công thức hóa tác vụ đánh giá mã dưới dạng tác
vụ tạo ngôn ngữ tự nhiên như

4https://openai.com/GPT-4
5Chúng tôi loại bỏ các yêu cầu GPT-4 thất bại và các phản
hồi không có kết quả <passed> hoặc <not passed>.

Prompt Đánh giá
Bạn là một mô hình đánh giá mã. Nhiệm vụ 
của bạn là phân tích và kiểm tra đoạn mã 
đầu vào. Điều này bao gồm việc đánh giá 
liệu nó có thể chạy trơn tru, liệu nó có tạo 
ra kết quả đúng hay không, và liệu có các 
vấn đề như timeout. Bạn nên xuất kết quả 
của mã với <passed> hoặc <not passed>. 
Ngoài ra, bạn cũng nên cung cấp giải thích 
cho kết quả đánh giá.

Hình 3: Chúng tôi sử dụng prompt đánh giá để truy vấn GPT-4 nhằm đánh giá tính đúng đắn của các giải pháp mã được tạo ra bởi mô hình của chúng tôi.

được hiển thị trong Hình 1. Chúng tôi hy vọng dạng
huấn luyện tương tự của nó có thể cung cấp nhiều
tín hiệu giám sát có ý nghĩa cho mô hình tạo mã
gốc. Trong các thí nghiệm của chúng tôi, chúng tôi
thấy khó khăn trong việc cân bằng hai tác vụ tạo
mã và đánh giá mã vì mô hình luôn có xu hướng
overfitting mục tiêu tạo mã. Do đó, chúng tôi sử
dụng một paradigm huấn luyện nhiều bước trong
đó chúng tôi đầu tiên tinh chỉnh mô hình trên như
một đánh giá viên và sau đó như một trình tạo. Cụ
thể, chúng tôi tinh chỉnh mô hình DIT bằng bộ dữ
liệu đánh giá mã trong 1 epoch và sau đó tiếp tục
huấn luyện trên dữ liệu hướng dẫn đa dạng trong
100 bước. Chúng tôi thấy rằng huấn luyện thêm
các bước sẽ không mang lại cải thiện thêm. Kết
quả thí nghiệm chứng minh rằng huấn luyện nhiều
bước ổn định nâng cao khả năng tạo mã của nó
(được hiển thị trong Hình 6).

3 Thí nghiệm

3.1 Benchmarks

Trong bài báo này, chúng tôi tập trung vào hai trong
số những benchmark được sử dụng rộng rãi nhất
trong lĩnh vực tạo mã.

•HumanEval (base)6 và HumanEval+
(plus). HumanEval (base) là một benchmark
được sử dụng rộng rãi được đề xuất bởi OpenAI
cho tổng hợp mã. Nó bao gồm 164 bài toán lập
trình được tạo thủ công, với trung bình 9.6 test
cases được phân bổ cho mỗi bài để kiểm tra
tính đúng đắn. Hơn nữa, Liu et al. (2023b) thấy
rằng các test cases trong benchmark có thể
không đủ và đề xuất HumanEval+ (plus) được
hỗ trợ bởi framework EvalPlus để có được
80 × test cases.

6https://github.com/openai/human-eval

--- TRANG 4 ---
•MBPP (base) (Austin et al., 2021) và
MBPP+ (plus). MBPP (base) cũng là một
benchmark tổng hợp mã cung cấp một tập
hợp 500 bài toán lập trình Python được
crowdsource bao gồm các khái niệm cơ bản
về lập trình, chức năng thư viện tiêu chuẩn,
v.v. Mỗi bài toán bao gồm mô tả tác vụ, giải
pháp mã và 3 test cases tự động. EvalPlus
cũng cung cấp benchmark MBPP+ (plus)
mở rộng bằng 35x test cases.

Chúng tôi thực hiện khử trùng lặp matching n-gram
giữa các bộ dữ liệu huấn luyện và benchmarks để
ngăn chặn rò rỉ dữ liệu. Đối với tất cả các thí nghiệm,
chúng tôi sử dụng giải mã tham lam và báo cáo
metric pass@1. Template prompt suy luận được
hiển thị trong Hình 4 theo WizardCoder. Để giữ
so sánh công bằng, chúng tôi sử dụng cùng một
framework EvalPlus7 để tính toán metrics.

3.2 Baselines

Trong bài báo này, chúng tôi phân loại các mô hình
baseline thành hai loại sau. (1) Mô hình nguồn
đóng: Chúng tôi đã kết hợp cụ thể GPT-3.5 và
GPT-4 của OpenAI, được phát triển riêng tư bởi
các công ty công nghệ hàng đầu, thể hiện trình
độ tiên tiến hiện tại trong thành thạo LLM. (2) Mô
hình nguồn mở: Một số LLMs nguồn mở đã được
cung cấp cho cộng đồng AI, mặc dù hiệu suất của
chúng thường kém hơn các mô hình nguồn đóng
rất nhiều. Như một phần nghiên cứu của chúng
tôi, chúng tôi kết hợp một số lượng đáng kể các
mô hình nguồn mở này làm baselines, bao gồm
CodeGen (Nijkamp et al., 2023), CodeT5+ (Wang
et al., 2021), StarCoder, CODELLAMA, OctoCoder
và dòng WizardCoder.

3.3 Chi tiết Triển khai

Tạo dữ liệu Đối với dữ liệu Hướng dẫn Đa dạng,
chúng tôi thiết kế sáu prompt hệ thống được hiển
thị như Hình 2, bao gồm một prompt trống, và
prompt GPT-3.5-turbo để tạo ra các phản hồi đa
dạng hơn. Được truyền cảm hứng từ CodeFuse (Di
et al., 2023b), chúng tôi sử dụng các quy tắc heuristic
để lọc ra dữ liệu chất lượng thấp và khử trùng lặp
dữ liệu dựa trên tập kiểm tra. Các quy tắc cụ thể
như sau:

•Lọc dữ liệu chất lượng thấp
1.Lọc dữ liệu có độ dài hướng dẫn ít hơn
10 từ hoặc lớn hơn 1000 từ;

7https://evalplus.github.io/leaderboard.html

Template Prompt
Dưới đây là một hướng dẫn mô tả một tác vụ,
được ghép nối với một đầu vào cung cấp thêm
ngữ cảnh. Viết một phản hồi hoàn thành một
cách thích hợp yêu cầu.

###Hướng dẫn: Tạo một script Python cho
bài toán này: {Câu hỏi}

###Phản hồi:

Hình 4: Template prompt suy luận khi kiểm tra trên HumanEval và MBPP.

2.Lọc dữ liệu có độ dài đầu ra ít hơn
80 từ;
3.Lọc ra dữ liệu với định dạng markdown
không hợp lệ, chẳng hạn: các khối mã
không được đóng;
4. Lọc dữ liệu có hơn 2048 tokens;

•Lọc dữ liệu tương tự với bộ dữ liệu kiểm tra
1.Lọc dữ liệu chứa bất kỳ tên hàm nào
từ bộ dữ liệu kiểm tra.
2.Sử dụng NLTK để loại bỏ các từ dừng và
dấu câu từ docstring của HumanEval,
thu được các từ cốt lõi như "sort array
prime", v.v. Lọc dữ liệu chứa hơn 40%
từ cốt lõi từ bộ dữ liệu kiểm tra.

Đối với dữ liệu Hướng dẫn Đa mục tiêu, chúng tôi
trích xuất 5000 hướng dẫn từ dữ liệu Hướng dẫn
Đa dạng và sau đó tạo ra 100 phản hồi cho mỗi
hướng dẫn với nhiệt độ 0.5 và top-p 0.95 với mô
hình chúng tôi có được sau điều chỉnh hướng dẫn
đa dạng. Để xác định tính đúng đắn của mỗi giải
pháp mã, chúng tôi prompt GPT-4 để đánh giá liệu
giải pháp mã này có thể đáp ứng yêu cầu hướng
dẫn hay không. Và sau đó chúng sẽ được phân loại
thành 'passed' hoặc 'not passed'. Prompt chúng
tôi sử dụng để đánh giá như Hình 3.

Huấn luyện Trong nghiên cứu của chúng tôi, chúng
tôi sử dụng CODELLAMA-PYTHON-7B và
CODELLAMA-PYTHON-13B làm mô hình nền
tảng. Chúng tôi huấn luyện DIT trong 3 epochs
và MOT trong 1 epoch dữ liệu đánh giá mã và
100 bước dữ liệu tạo mã. Trong giai đoạn huấn
luyện, chúng tôi thiết lập kích thước batch toàn
cục là 512 và độ dài chuỗi là 2048, với tốc độ học
được khởi tạo ở 5e-6 và tỷ lệ warmup là

--- TRANG 5 ---
Mô hình Kích thước HumanEval MBPP
Base Plus Base Plus
GPT-3.5 (Nov 2023) - 72.6 65.9 81.7 69.4
GPT-4 (Nov 2023) - 85.4 81.7 83.0 70.7
CODELLAMA-PYTHON 34B 51.8 42.7 67.2 52.9
WizardCoder-CL 34B 73.2 64.6 73.2 59.9
CodeT5+ 16B 31.7 26.2 54.6 44.4
CodeGen-Mono 16B 32.9 27.4 52.6 43.6
StarCoder 15B 34.1 29.3 55.1 46.1
CODELLAMA-PYTHON 13B 42.7 36.6 61.2 50.9
CODELLAMA-INSTRUCT 13B 42.7 - 49.4 -
OctoCoder 15B 46.2 - - -
WizardCoder 13B 60.4* 54.3* 65.2* 53.1*
DolphCoder(của chúng tôi) 13B 67.7 57.9 67.2 54.1
StarCoder 7B 24.4 20.7 33.1 28.8
CodeT5+ 6B 29.3 23.8 51.9 40.9
CodeGen-Mono 6B 29.3 25.6 49.9 42.1
CODELLAMA-PYTHON 7B 37.8 34.1 57.6 45.4
CODELLAMA-INSTRUCT 7B 34.8 - 44.4 -
WizardCoder 7B 48.2 40.9 56.6 47.1
DolphCoder(của chúng tôi) 7B 62.8 54.9 64.9 52.6

Bảng 1: Kết quả Pass@1 của các code LLMs khác nhau cho HumanEval và MBPP. Base có nghĩa là benchmark gốc và Plus biểu thị benchmark mở rộng. * biểu thị các kết quả được tái tạo thông qua các script EvalPlus và các kết quả baseline khác được trích dẫn từ bảng xếp hạng chính thức. Kết quả tốt nhất trong mỗi cột được in đậm.

15%. Sau khi warmup, tốc độ học giảm theo lịch
trình cosine. Chúng tôi sử dụng optimizer Adam,
với các tham số β1 và β2 được đặt ở 0.9 và 0.95
tương ứng. Và để đảm bảo tính ổn định huấn luyện,
chúng tôi kết hợp gradient clipping với giá trị được
đặt ở 1.0, một kỹ thuật được thiết kế để ngăn chặn
việc leo thang gradient quá mức có thể dẫn đến
bất ổn định số hoặc phân kỳ mô hình.

3.4 Kết quả Chính

Các kết quả chính của phương pháp được đề xuất
so với các baselines được minh họa trong Bảng 1.
Chúng tôi tiến hành các so sánh sau: (1) Nói chung,
Phương pháp được đề xuất của chúng tôi vượt trội
đáng kể so với tất cả các phương pháp trước đây
trong các kích thước tham số mô hình khác nhau.
(2) So với mô hình cơ sở của chúng tôi (CODELLAMA),
DolphCoder cho thấy cải thiện đáng kể trong cả
HumanEval(+) và MBPP(+). Chi tiết, DolphCoder-7b
có sự gia tăng 25 điểm phần trăm trên HumanEval
và sự gia tăng 7.3 điểm phần trăm trên MBPP. (3)
Khi chúng tôi so sánh mô hình DolphCoder với mô
hình baseline gần đây WizardCoder-7b, được xây
dựng trên cùng một mô hình cơ sở (CODELLAMA),
DolphCoder vẫn nhất quán vượt trội trên tất cả
benchmarks và kích thước mô hình. (4) DolphCoder-7b
thậm chí vượt trội hơn CODELLAMA-PYTHON 34b,
chứng minh hiệu quả của việc sử dụng LLMs kích
thước nhỏ. Những kết quả này chỉ ra tính tổng quát
và độ mạnh mẽ của phương pháp chúng tôi.

4 Phân tích

4.1 Nghiên cứu Ablation

Để điều tra các đặc điểm của các thành phần chính
trong DolphCoder, chúng tôi tiến hành các thí nghiệm
ablation trong Bảng 2. Từ kết quả, chúng tôi có
những quan sát sau: (1) Cả DIT và MOT đều đóng
góp vào cải thiện hiệu suất. Cụ thể, trên mô hình
cơ sở CODELLAMA-PYTHON-7b

--- TRANG 6 ---
Mô hình Kích thước HumanEval MBPP
Base Plus Base Plus
Evol Instruct 7B 50.0 44.5 59.4 50.1
+DIT 7B 57.9 51.2 64.2 52.1
+MOT 7B 55.5 46.3 64.2 52.6
+ALL(DolphCoder) 7B 62.8 54.9 64.9 52.6
Evol Instruct 13B 64.0 55.5 65.7 51.6
+DIT 13B 66.5 57.3 65.2 52.1
+MOT 13B 65.2 56.7 66.7 53.9
+ALL(DolphCoder) 13B 67.7 57.9 67.2 54.1

Bảng 2: Nghiên cứu ablation của DolphCoder.

cơ sở, DIT mang lại cải thiện 7.9 pp trên HumanEval
và 4.8 pp trên MBPP, so với phương pháp hướng
dẫn phát triển baseline. MOT dẫn đến cải thiện 5.5
pp và 4.8 pp tương ứng trên HumanEval và MBPP.
Hơn nữa, chúng tôi quan sát thấy rằng dữ liệu huấn
luyện MOT được xây dựng bởi GPT-4 chứa khoảng
20% nhiễu lỗi, điều này hạn chế nghiêm trọng hiệu
suất giới hạn trên của phương pháp MOT trong
công trình này. Một cuộc thảo luận chi tiết hơn có
thể được tìm thấy trong Phụ lục B. (2) Sự kết hợp
của DIT và MOT mang lại lợi ích thêm. DolphCoder-7b
thể hiện sự nâng cao 12.8 pp trên HumanEval và
5.5 pp trên MBPP, chứng minh mối quan hệ tương
đối trực giao giữa hai phương pháp này. Những
kết quả này chứng minh hiệu quả của các phương
pháp được đề xuất.

4.2 Hiệu ứng của Điều chỉnh Hướng dẫn Đa dạng

Để khám phá thêm nguồn gốc của sự cải thiện mô
hình được mang lại bởi DIT, chúng tôi kiểm tra hiệu
suất của mô hình dưới các tỷ lệ lấy mẫu khác nhau
được hiển thị trong Bảng 3. Tỷ lệ lấy mẫu đại diện
cho số lượng prompt hệ thống. Khi tỷ lệ lấy mẫu
tăng, hiệu suất của mô hình trên tất cả các chỉ số
dần dần tăng, điều này chứng minh rằng việc sử
dụng các phản hồi đa dạng hơn có thể nâng cao
hiệu suất của mô hình.

Ngoài ra, chúng tôi trích xuất các đoạn mã từ các
phản hồi và kiểm tra xem việc triển khai các mã
này có khác nhau hay không. Để dễ thống kê, chúng
tôi chỉ tập trung vào mã liên quan đến Python. Cụ
thể, chúng tôi sử dụng AST để phân tích từng đoạn
mã và tính toán độ tương tự giữa các mã, sau đó
loại bỏ trùng lặp. Chúng tôi đếm số lượng trung
bình các giải pháp mã duy nhất còn lại cho mỗi
tác vụ mã như một chỉ số về tính đa dạng mã.
Bảng 3 cho thấy rằng khi tỷ lệ lấy mẫu tăng, số
lượng các giải pháp mã khác nhau tương ứng với
cùng một hướng dẫn mã

Tỷ lệ Kích thước Đa dạng HumanEval
Base Plus
1 7B 1.0 52.4 45.1
3 7B 2.1 57.9 50.6
6 7B 2.7 57.9 51.2
1 13B 1.0 63.4 53.7
3 13B 2.1 65.2 58.5
6 13B 2.7 66.5 57.3

Bảng 3: Hiệu ứng của các tỷ lệ lấy mẫu khác nhau của DIT. Đa dạng mã cho thấy số lượng trung bình các giải pháp mã khác nhau cho một hướng dẫn mã, và chúng tôi sử dụng phân tích cú pháp để phân tích các giải pháp mã khác nhau. Tất cả chỉ số đều tham chiếu đến pass@1.

cũng dần dần tăng và hiệu suất của mô hình trên
HumanEval cũng cải thiện. Hơn nữa, chúng tôi quan
sát thấy rằng sự gia tăng trong tính đa dạng mã
không phải là tuyến tính. Cụ thể, khi chúng tôi tăng
tỷ lệ lấy mẫu từ 3 lên 6, chúng tôi chỉ quan sát thấy
những lợi ích hiệu suất biên tế mà chúng tôi lập
luận rằng dữ liệu DIT có thể chứa sự dư thừa không
cần thiết. Các công trình đồng thời (Lu et al., 2023;
Liu et al., 2023c) khám phá các phương pháp nén
dựa trên đa dạng phức tạp hơn cho điều chỉnh
hướng dẫn tổng quát. Chúng tôi để lại điều này
cho công việc tương lai.

4.3 Hiệu ứng của Điều chỉnh Hướng dẫn Đa mục tiêu

Để khám phá cách khả năng đánh giá mã và khả
năng tạo mã có lợi cho nhau, chúng tôi đánh giá
chúng trong giai đoạn huấn luyện MOT. Cụ thể,

Mô hình Tham lam Pass@1 Pass@10
DolphCoder 62.8 59.9 71.3
-w/o MOT 57.9 56.6 69.5
Cải thiện 4.9 3.3 1.8

Bảng 4: Pass@k với các phương pháp giải mã khác nhau và các giá trị k khác nhau, trong đó Tham lam sử dụng lấy mẫu tham lam, pass@1 và pass@10 lấy mẫu ở nhiệt độ=0.2. Tất cả chỉ số được kiểm tra trên HumanEval dựa trên DolphCoder-7b.

--- TRANG 7 ---
chúng tôi đánh giá khả năng đánh giá mã của mô
hình bằng cách lấy mẫu 40 câu trả lời cho mỗi trong
số 164 câu hỏi kiểm tra trong HumanEval và phân
loại tính đúng đắn của chúng bằng cách sử dụng
các bài kiểm tra đơn vị vàng được cung cấp. Kết
quả thí nghiệm được hiển thị trong Hình 5.

Từ kết quả, chúng tôi quan sát thấy rằng tồn tại
một mối liên hệ mạnh mẽ giữa khả năng đánh giá
và tạo của mô hình. Khi mô hình tiếp tục huấn luyện
trên tác vụ đánh giá, khả năng đánh giá của nó
tiếp tục cải thiện và dần dần ổn định. Tuy nhiên,
quá trình này làm suy giảm khả năng tạo mã của
mô hình, có thể do quên thảm khốc gây ra bởi
huấn luyện nhiều bước. Sau khi mô hình trải qua
quá trình huấn luyện trình tạo, khả năng tạo mã
của mô hình được khôi phục và metric pass@1
vượt qua giới hạn hiệu suất mô hình DIT. Trong
khi đó, khả năng đánh giá của nó giảm đáng kể.
Chúng tôi cho rằng sự cải thiện trong khả năng
tạo mã được đạt được thông qua việc chuyển đổi
khả năng đánh giá, nhưng hai khả năng này khó
có thể cùng tồn tại.

Để xem xét kỹ hơn tác động của MOT, chúng tôi
so sánh các metrics của pass@1 và pass@10 để
xác định liệu những cải thiện có xuất phát từ sự
ưa thích cao hơn của mô hình đối với phản hồi
đúng hay không. Từ Bảng 4, chúng tôi thấy rằng
DolphCoder vượt trội hơn Mô hình DIT trên tất cả
metrics. Sự cải thiện của MOT trên chỉ số giải mã
tham lam pass@1 là lớn nhất, với mức tăng 4.9%.
So với pass@1, sự cải thiện trên pass@10 giảm
đáng kể xuống chỉ 1.8%. Điều này ngụ ý rằng tác
động của MOT không tập trung vào việc nâng cao
khả năng tạo ra các giải pháp mạnh mẽ của mô
hình. Thay vào đó, nó chủ yếu tăng cường khả
năng phân biệt phản hồi của mô hình, dẫn đến sự
ưa thích cao hơn cho câu trả lời đúng.

Mô hình Tham lam Pass@1 Pass@10
DolphCoder 62.8 59.9 71.3
-w/o MOT 57.9 56.6 69.5
Cải thiện 4.9 3.3 1.8

Mô hình Ban đầubước 40bước 120 bước 200 bước 280 bước 360 bước 440 bước 520 bước 600 bước 680Mô hình 
Cuối cùng4550556065
45.047.550.052.555.057.560.062.5
ACC cho đánh giá
Pass@1 cho tạo

Hình 5: Xu hướng của khả năng đánh giá mã và khả năng tạo mã trong giai đoạn MOT trong đó bước 200 đề cập đến bước huấn luyện trong bước đầu tiên của MOT. Mô hình ban đầu có nghĩa là mô hình DIT và Cuối cùng có nghĩa là DolphCoder. Pass@1 đề cập đến pass@1 trên HumanEval.

Huấn luyện Suy luận HumanEval
Base Plus
✕ ✕ 57.9 50.6
✕ ✓ 55.5 48.2
✓ ✕ 66.5 57.3
✓ ✓ 61.0 53.7

Bảng 5: Hiệu ứng của prompt hệ thống trong quá trình huấn luyện và suy luận. Xem xét hiệu quả, chúng tôi chỉ tiến hành thí nghiệm trên mô hình DIT dựa trên CODELLAMA-13B-PYTHON và sử dụng metric pass@1 trên HumanEval với giải mã tham lam. Đối với suy luận với prompt hệ thống, chúng tôi ngẫu nhiên chọn một prompt hệ thống trong huấn luyện cho mỗi truy vấn kiểm tra.

4.4 Phân tích các Thiết kế Chính

Ablation Prompt Hệ thống Chúng tôi thu được dữ
liệu huấn luyện đa dạng hơn bằng cách biến đổi
prompt hệ thống. Để khám phá liệu prompt hệ
thống có nên được sử dụng trong quá trình huấn
luyện và suy luận hay không, chúng tôi tiến hành
xác minh thí nghiệm. Bảng 5 cho thấy hiệu ứng
của các prompt hệ thống trong quá trình huấn
luyện và suy luận. Từ kết quả, chúng tôi quan sát
thấy rằng, so với huấn luyện với prompt hệ thống,
huấn luyện không có prompt hệ thống dẫn đến sự
suy giảm đáng kể. Điều này ngụ ý rằng việc gán
các hướng dẫn mã hoàn toàn giống nhau cho các
câu trả lời khác nhau không phải là một phương
pháp huấn luyện tốt. Khi so sánh liệu có sử dụng
prompt hệ thống trong quá trình suy luận hay không,
chúng tôi thấy rằng việc xóa prompt hệ thống dẫn
đến sự cải thiện đáng kể so với việc sử dụng prompt
hệ thống. Sự cải thiện này có thể xuất phát từ tự
do của mô hình trong việc xác định

Base Plus404550556065pass@1DIT
MOT-Mix
MOT-Multi-Step

Hình 6: Hiệu ứng của các thiết lập khác nhau của học đa tác vụ dựa trên DolphCoder-7B. Mix biểu thị việc cộng trực tiếp loss tạo và đánh giá lại với nhau và Multi-Step có nghĩa là huấn luyện tuần tự.

--- TRANG 8 ---
đường dẫn suy luận phù hợp nhất.

Ablation Phương pháp Huấn luyện Đa tác vụ Chúng
tôi cũng khám phá các phương pháp huấn luyện
đa tác vụ khác nhau như được hiển thị trong Hình
6. Kết quả cho thấy cả hai biến thể MOT đều vượt
trội đáng kể so với baseline DIT và thiết lập huấn
luyện nhiều bước có hiệu suất vượt trội so với
thiết lập huấn luyện trộn. Chúng tôi cũng thấy rằng
huấn luyện nhiều bước có quá trình huấn luyện ổn
định hơn để cân bằng hai tác vụ tạo mã và đánh
giá mã.

5 Công trình Liên quan

5.1 Điều chỉnh Hướng dẫn

Các mô hình ngôn ngữ lớn (LLMs) trải qua giai
đoạn Điều chỉnh hướng dẫn (IFT), điều này nâng
cao khả năng hoàn thành các tác vụ và tuân theo
hướng dẫn của con người. Thuật ngữ IFT được sử
dụng rộng rãi ở đây để bao gồm một loạt các ứng
dụng tinh chỉnh chuỗi-tới-chuỗi. T5 (Raffel et al.,
2023) là một trong những mô hình đầu tiên khám
phá phương pháp này, huấn luyện trên nhiều tác
vụ văn bản-tới-văn bản có giám sát. Các nghiên
cứu gần đây đã đi sâu vào việc tinh chỉnh đa tác
vụ dựa trên hướng dẫn của các LLMs tiền huấn
luyện, nhằm cải thiện khả năng bẩm sinh của chúng
để thực hiện các tác vụ NLP downstream một cách
hiệu quả, chẳng hạn như FLAN (Wei et al., 2022),
T0 (Sanh et al., 2022), và UnifiedQA (Khashabi
et al., 2020), mở rộng hơn nữa phạm vi tác vụ để
cải thiện khả năng tổng quát hóa tổng thể của LMs.

Theo những thành tựu đáng chú ý của các LLMs
độc quyền, đặc biệt là ChatGPT, đã có sự tập trung
ngày càng tăng vào việc sử dụng Điều chỉnh Hướng
dẫn (IFT) để căn chỉnh tốt hơn LLMs với ý định
của con người, như được nêu bật trong nghiên cứu
bởi (Brown et al., 2020; Ouyang et al., 2022). Một
phát hiện chung trong những nghiên cứu này là
việc tinh chỉnh LMs trên dữ liệu đa dạng hơn có
thể cải thiện đáng kể hiệu suất mô hình. Taori et
al. (2023) chọn một cách tiếp cận khác, áp dụng
phương pháp tự hướng dẫn, sử dụng ChatGPT để
tạo ra dữ liệu huấn luyện rộng hơn. Chiang et al.
(2023) huấn luyện mô hình của họ bằng cách sử
dụng các cuộc trò chuyện người dùng chia sẻ được
thu thập từ ShareGPT.com. Xu et al. (2023b) giới
thiệu phương pháp Evol-Instruct, bao gồm việc
phát triển dữ liệu hướng dẫn hiện có để tạo ra các
bộ dữ liệu phức tạp và đa dạng hơn.

5.2 Mô hình Ngôn ngữ Lớn cho Mã

Các mô hình ngôn ngữ lớn thường được tiền huấn
luyện trên hàng nghìn tỷ tokens theo quy luật tỷ lệ
(Hoffmann et al., 2022; Kaplan et al., 2020), điều
này thể hiện những thành tựu đáng chú ý trên một
phổ rộng các tác vụ và lượng dữ liệu văn bản như
vậy thường là một hỗn hợp đa dạng với một phần
không nhỏ mã (Zhang et al., 2023). Tiên phong bởi
Codex, các nhà nghiên cứu cũng đã thấy rằng việc
tiền huấn luyện liên tục trên mã có lợi đáng kể cho
hiệu suất của các mô hình ngôn ngữ trên mã. Ví
dụ, Chowdhery et al. (2022a) hậu huấn luyện PaLM
trên 7.8B tokens mã bổ sung để có được PaLM-Coder
và Rozière et al. (2023) huấn luyện LLaMA 2 (Touvron
et al., 2023a) trên hơn 500B tokens mã để có được
Code LLaMA, điều này đưa các mô hình nguồn mở
lên một tầm cao mới. Đối với tinh chỉnh có giám
sát, nhiều công trình sử dụng các mô hình giáo
viên lớn hơn, có khả năng hơn để tổng hợp dữ liệu
hướng dẫn nhằm tinh chỉnh các mô hình ngôn ngữ
nhỏ (Mitra et al., 2023; Luo et al., 2023; Di et al.,
2023b). Thay vào đó, Muennighoff et al. (2023);
Wei et al. (2023) xây dựng các hướng dẫn mã bằng
cách khai thác corpus mã hiện có, điều này trực
giao với phương pháp của chúng tôi. Vì các tín hiệu
có giám sát mã dễ dàng được thu thập bằng cách
biên dịch và chạy chúng, học tăng cường trở thành
một nhánh quan trọng khác. Nhiều công trình đã
cố gắng sử dụng học tăng cường với thông tin phản
hồi được cung cấp bởi biên dịch và các nguồn khác.
PanGu-Coder2 (Shen et al., 2023b) giới thiệu một
ranking loss dựa trên các bài kiểm tra đơn vị, điều
này giúp căn chỉnh sâu với mô hình mã có khả
năng. Tuy nhiên, các phương pháp này dựa vào
các bài kiểm tra đơn vị được chú thích bởi con
người, điều này hạn chế ứng dụng trong tình huống
thực tế.

6 Kết luận và Công việc Tương lai

Trong bài báo này, chúng tôi điều tra hai phương
pháp tinh chỉnh để cải thiện hiệu suất của LLMs
trong việc tạo mã. Chúng tôi đầu tiên giới thiệu
một chiến lược bổ sung phản hồi bằng cách sử
dụng các prompt hệ thống ChatGPT khác nhau để
tăng tính đa dạng của các giải pháp mã. Chúng tôi
thấy rằng các đường dẫn lý luận chuỗi suy nghĩ
khác nhau cải thiện hiệu suất. Sau đó, Chúng tôi
áp dụng một phương pháp huấn luyện nhiều bước
kết hợp các mục tiêu tạo mã truyền thống và đánh
giá mã. Chúng tôi thấy rằng việc cải thiện khả năng
đánh giá tính đúng đắn của mã cũng nâng cao khả
năng tạo ra nó. Đối với công việc tương lai, chúng
tôi nhằm khám phá hiệu ứng của các phương pháp
trên các mô hình nền tảng lớn hơn và các cơ chế
tinh chỉnh hiệu quả tham số. Chúng tôi cũng dự
định tăng độ chính xác của tín hiệu đánh giá thông
qua các cách khác như các bài kiểm tra đơn vị tự
động và các phương pháp học tăng cường.

--- TRANG 9 ---
7 Hạn chế

Hạn chế của chúng tôi có ba mặt: (1) Chúng tôi chỉ
khám phá phương pháp trên các mô hình cơ sở
7B/13B do chi phí tính toán. Cần tiến hành thêm
thí nghiệm trên các mô hình lớn hơn và các mô
hình mã khác để xác nhận kết luận của chúng tôi.
(2) Chúng tôi chỉ sử dụng GPT-4 để đánh giá chất
lượng của các giải pháp mã được tạo ra. Hiệu suất
của GPT-4 vẫn còn kém và hạn chế hiệu suất của
phương pháp được đề xuất. Các mô hình đánh giá
chính xác và nguồn mở hơn nên được khám phá
trong công việc tương lai. (3) Vẫn còn chỗ để tối
ưu hóa trong dữ liệu huấn luyện của chúng tôi. Ví
dụ, chúng tôi thấy rằng việc liên tục tăng số lượng
prompt hệ thống chỉ mang lại lợi ích hiệu suất biên
tế. Các phương pháp nén dựa trên đa dạng (Lu et
al., 2023; Liu et al., 2023c) có thể có giá trị khi số
lượng prompt hệ thống lớn.

8 Tác động Rộng hơn

Tương tự như các LLMs khác, DolphCoder của chúng
tôi cũng có thể tạo ra thông tin không đạo đức, có
hại hoặc gây hiểu lầm, điều này không được xem
xét trong công trình của chúng tôi. Nghiên cứu
tương lai để giải quyết các tác động đạo đức và xã
hội là cần thiết. DolphCoder cũng dễ bị ảo giác
trong các trường hợp sử dụng tạo không có căn
cứ do kích thước nhỏ hơn của nó. Mô hình này chỉ
được thiết kế cho các thiết lập nghiên cứu, và việc
kiểm tra của nó chỉ được thực hiện trong những
môi trường như vậy. Nó không nên được sử dụng
trong các ứng dụng downstream, vì cần có phân
tích bổ sung để đánh giá tác hại hoặc thiên vị tiềm
ẩn trong ứng dụng được đề xuất.

Tài liệu tham khảo

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, và
Charles Sutton. 2021. Program synthesis with large
language models. ArXiv, abs/2108.07732.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, và Dario Amodei.
2020. Language models are few-shot learners.

Sahil Chaudhary. 2023. Code alpaca: An instruction-
following llama model for code generation. https:
//github.com/sahil280114/codealpaca.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-Voss, William H. Guss, Alex Nichol, Igor
Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew M. Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, và Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv,
abs/2107.03374.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, và Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt
quality.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
và Noah Fiedel. 2022a. Palm: Scaling language
modeling with pathways.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha
Tsvyashchenko, Joshua Maynez, Abhishek Rao,
Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C.
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier García,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,

--- TRANG 10 ---
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav
Petrov, và Noah Fiedel. 2022b. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res.,
24:240:1–240:113.

Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting
Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei
Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong,
Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng
Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Mingquan Shen,
Guangpei Wang, Huan Wang, Zhi Yu Wang, Zhaogui
Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang,
Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu,
và Xianying Zhu. 2023a. Codefuse-13b: A pretrained multi-lingual code large language model.
ArXiv, abs/2310.06266.

Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting
Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei
Chen, Liang Chen, et al. 2023b. Codefuse-13b: A
pretrained multi-lingual code large language model.
arXiv preprint arXiv:2310.06266.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv
preprint arXiv:2203.15556.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, và Dario Amodei. 2020.
Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, và Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. Trong Findings of the Association for Computational Linguistics:
EMNLP 2020, trang 1896–1907, Online. Association
for Computational Linguistics.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim,
Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,
Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,
Nicolas Gontier, Nicholas Meade, Armel Zebaze,
Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo
Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
Patel, Dmitry Abulkhanov, Marco Zocca, Manan
Dey, Zhihan Zhang, Nourhan Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni,

Paulo Villegas, Maxim Kunakov, Fedor Zhdanov,
Manuel Romero, Tony Lee, Nadav Timor, Jennifer
Ding, Claire Schlesinger, Hailey Schoelkopf, Jana
Ebert, Tri Dao, Mayank Mishra, Alexander Gu,
Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,
Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean M. Hughes, Thomas Wolf,
Arjun Guha, Leandro von Werra, và Harm de Vries.
2023. Starcoder: may the source be with you! ArXiv,
abs/2305.06161.

Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal
Lago, Thomas Hubert, Peter Choy, Cyprien de,
Masson d'Autume, Igor Babuschkin, Xinyun Chen,
Po-Sen Huang, Johannes Welbl, Sven Gowal,
Alexey, Cherepanov, James Molloy, Daniel Jaymin
Mankowitz, Esme Sutherland Robson, Pushmeet
Kohli, Nando de, Freitas, Koray Kavukcuoglu, và
Oriol Vinyals. 2022. Competition-level code generation with alphacode. Science, 378:1092 – 1097.

Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao
Han, Wei Yang, và Deheng Ye. 2023a. Rltf: Reinforcement learning from unit test feedback. ArXiv,
abs/2307.04349.

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, và Lingming Zhang. 2023b. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint
arXiv:2305.01210.

Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, và
Junxian He. 2023c. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.

Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, và Jingren
Zhou. 2023. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, và Daxin Jiang. 2023. Wizardcoder:
Empowering code large language models with evolinstruct. ArXiv, abs/2306.08568.

Arindam Mitra, Luciano Del Corro, Shweti Mahajan,
Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi
Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, và Ahmed Awadallah. 2023.
Orca 2: Teaching small language models how to reason.

Niklas Muennighoff, Qian Liu, Qi Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo,
Swayam Singh, Xiangru Tang, Leandro von Werra,
và S. Longpre. 2023. Octopack: Instruction tuning
code large language models. ArXiv, abs/2308.07124.

--- TRANG 11 ---
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, và Ahmed Hassan Awadallah. 2023. Orca: Progressive learning
from complex explanation traces of gpt-4. ArXiv,
abs/2306.02707.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, và Caiming
Xiong. 2023. Codegen: An open large language
model for code with multi-turn program synthesis.

OpenAI. 2023. Gpt-4 technical report. ArXiv,
abs/2303.08774.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, và Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, và Peter J. Liu. 2023. Exploring the limits
of transfer learning with a unified text-to-text transformer.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P
Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal
Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
Thomas Scialom, và Gabriel Synnaeve. 2023. Code
llama: Open foundation models for code. ArXiv,
abs/2308.12950.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
Thomas Scialom, và Gabriel Synnaeve. 2023. Code
llama: Open foundation models for code.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,
Han Wang, Matteo Manica, Sheng Shen, Zheng Xin
Yong, Harshit Pandey, Rachel Bawden, Thomas
Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,
Andrea Santilli, Thibault Fevry, Jason Alan Fries,
Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao,
Thomas Wolf, và Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization.

Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,
Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan
Ji, Jingyang Zhao, Yuenan Guo, và Qianxiang
Wang. 2023a. Pangu-coder2: Boosting large language models for code with ranking feedback. ArXiv,
abs/2307.14936.

Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,
Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan
Ji, Jingyang Zhao, Yuenan Guo, và Qianxiang Wang.
2023b. Pangu-coder2: Boosting large language models for code with ranking feedback.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
và Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas
Scialom. 2023a. Llama 2: Open foundation and finetuned chat models.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V.
Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin
Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và

--- TRANG 12 ---
Thomas Scialom. 2023b. Llama 2: Open foundation
and fine-tuned chat models. ArXiv, abs/2307.09288.

Yue Wang, Weishi Wang, Shafiq Joty, và Steven C. H.
Hoi. 2021. Codet5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. 2022. Finetuned
language models are zero-shot learners.

Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, và
Lingming Zhang. 2023. Magicoder: Source code is
all you need. ArXiv, abs/2312.02120.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin
Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions. ArXiv,
abs/2304.12244.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin
Jiang. 2023b. Wizardlm: Empowering large language models to follow complex instructions.

Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao,
Zi Gong, Hang Yu, Jianguo Li, và Rui Wang. 2023.
Unifying the perspectives of nlp and software engineering: A survey on language models for code.

A Nghiên cứu Trường hợp

Bảng 6 trình bày các ví dụ so sánh của DolphCoder
và WizardCoder. Trong trường hợp này, các mô hình
được yêu cầu xác định liệu một mảng có thể trở
thành một chuỗi không giảm sau khi áp dụng phép
toán dịch chuyển phải vòng tròn hay không. Từ
trường hợp này, chúng ta có thể quan sát thấy rằng
WizardCoder-13b tạo ra một lỗi logic rõ ràng, nơi
nó chỉ kiểm tra xem chuỗi có không giảm hay không
mà không thực hiện bất kỳ phép dịch chuyển phải
nào. Trái lại, DolphCoder mô phỏng chính xác phép
toán dịch chuyển phải vòng tròn và xác định đúng
điều kiện kết thúc. Hơn nữa, đáng chú ý là DolphCoder
xem xét các trường hợp đầu vào biên mạnh mẽ
hơn, điều này có thể được quy cho việc huấn luyện
trên tác vụ đánh giá mã và bộ dữ liệu huấn luyện
đa dạng hơn.

B Hiệu ứng của Khả năng Đánh giá Mã của GPT-4

Trong nghiên cứu của chúng tôi, chúng tôi sử dụng
GPT-4 để tạo một bộ dữ liệu huấn luyện cho đánh
giá mã. Tuy nhiên, có một mối quan tâm về việc liệu
GPT-4 có khả năng tạo ra một nhãn vàng hoàn hảo
hay không. Để có được câu trả lời, chúng tôi kiểm
tra nó trên HumanEval. Cụ thể, chúng tôi sử dụng
mô hình DIT để tạo ra nhiều câu trả lời ứng viên
cho mỗi câu hỏi. Sau đó, chúng tôi tận dụng GPT-4
để đánh giá từng câu trả lời ứng viên này. Và sau
đó chúng tôi chọn các giải pháp mã mà GPT-4 xác
định là đúng. Vì các test cases có sẵn trong HumanEval,
chúng tôi có thể có được độ chính xác nhị phân
<passed> hoặc <not passed> của GPT-4 chỉ là 79.4%,
điều này chỉ ra rằng dữ liệu huấn luyện chúng tôi
xây dựng thông qua GPT-4 có thể chứa khoảng 20%
nhiễu. Nhiễu này hạn chế hiệu suất giới hạn trên
của phương pháp MOT trong công trình này. Ngoài
ra, chúng tôi báo cáo kết quả pass@1 trung bình
trước và sau khi lọc bởi GPT-4, như được hiển thị
trong Bảng 7. Chúng ta có thể quan sát thấy rằng
pass@1 trước khi lọc là 56.6% được tạo ra bởi mô
hình DIT trực tiếp ở nhiệt độ 0.2. Sau đó chúng
tôi tính toán pass@1 sau khi lọc vàng là 72.6% nơi
chúng tôi sử dụng các bài kiểm tra đơn vị vàng để
lọc các giải pháp sai. Chúng tôi coi tỷ lệ 72.6% này
là giới hạn trên lý thuyết của GPT-4. Chúng tôi cũng
báo cáo pass@1 của MOT và GPT-4 Filtering. Chúng
tôi tóm tắt các hiểu biết chính như sau: (1) GPT-4
không thể thực hiện đánh giá mã hoàn hảo (62.7%
so với 72.6%). (2) MOT của chúng tôi có thể đạt
được hiệu suất hơi kém hơn so với GPT-4 Filtering
(59.9% so với 62.7%). (3) Khả năng đánh giá GPT-4
hạn chế giới hạn trên của MOT. Cách cải thiện đánh
giá mã tự động là thiết yếu cho công việc tương lai.

--- TRANG 13 ---
Hướng dẫnhef move_one_ball(arr):
"""
Chúng ta có một mảng 'arr' gồm N số nguyên arr[1],
arr[2], ..., arr[N]. Các số trong mảng sẽ được sắp xếp ngẫu nhiên.
Nhiệm vụ của bạn là xác định liệu có thể có được một mảng được sắp xếp theo
thứ tự không giảm bằng cách thực hiện phép toán sau trên
mảng đã cho hay không: Bạn được phép thực hiện phép dịch chuyển phải bất kỳ
số lần nào. Một phép dịch chuyển phải có nghĩa là dịch chuyển tất cả các phần tử
của mảng một vị trí theo hướng phải. Phần tử cuối cùng
của mảng sẽ được di chuyển đến vị trí bắt đầu trong mảng tức là
chỉ số thứ 0. Nếu có thể có được mảng đã sắp xếp bằng cách thực hiện
phép toán trên thì trả về True ngược lại trả về False. Nếu mảng đã cho
trống thì trả về True. Lưu ý: Danh sách đã cho được đảm bảo có
các phần tử duy nhất...
"""
WizardCoder-
13bDưới đây là script Python cho bài toán đã cho:
def move_one_ball(arr):
    if not arr:
        return True
    for i in range(len(arr)):
        if arr[i] > arr[i+1]:
            return False
    return True

Hàm nhận một mảng 'arr' làm đầu vào và trả về 'True' nếu
có thể có được một mảng đã sắp xếp theo thứ tự không giảm bằng cách
thực hiện các phép dịch chuyển phải trên mảng đã cho. Ngược lại,
nó trả về 'False'. Hàm đầu tiên kiểm tra xem mảng đã cho có
trống hay không. Nếu nó trống, nó trả về 'True' vì có thể có được
một mảng đã sắp xếp theo thứ tự không giảm bằng cách không thực hiện
phép dịch chuyển phải nào. Tiếp theo, hàm lặp qua mảng
và kiểm tra xem phần tử hiện tại có lớn hơn phần tử tiếp theo
hay không. Nếu có, thì không thể có được một mảng đã sắp xếp
theo thứ tự không giảm bằng cách thực hiện bất kỳ số lần
phép dịch chuyển phải nào. Do đó, hàm trả về 'False'. Nếu
hàm chưa trả về 'False' đến thời điểm này, có nghĩa là
mảng đã được sắp xếp theo thứ tự không giảm. Do đó,
nó trả về 'True'.

DolphCoder-
13bDưới đây là script Python cho bài toán đã cho:
def move_one_ball(arr):
    if len(arr) == 0:
        return True
    if len(arr) == 1:
        return True
    if arr == sorted(arr):
        return True
    for i in range(len(arr)):
        arr.append(arr.pop(0))
        if arr == sorted(arr):
            return True
    return False

Giải thích:
1. Đầu tiên, chúng ta kiểm tra xem mảng đã cho có trống hoặc chỉ có một
phần tử hay không. Nếu có, chúng ta trả về True vì nó đã được sắp xếp.
2. Tiếp theo, chúng ta kiểm tra xem mảng đã cho đã được sắp xếp chưa. Nếu có,
chúng ta trả về True.
3. Nếu mảng chưa được sắp xếp, chúng ta thực hiện một phép dịch chuyển phải
trên mảng bằng cách thêm phần tử đầu tiên vào cuối mảng
và loại bỏ phần tử đầu tiên khỏi đầu.
4. Chúng ta lặp lại bước 3 cho đến khi mảng được sắp xếp hoặc chúng ta đã
thực hiện tất cả các phép dịch chuyển phải có thể.
5. Nếu mảng được sắp xếp sau khi thực hiện tất cả các phép dịch chuyển phải
có thể, chúng ta trả về True. Ngược lại, chúng ta trả về False.

Bảng 6: Ví dụ về DolphCoder so với WizardCoder.

--- TRANG 14 ---
Phương pháp Pass@1
DIT 56.6
+MOT 59.9
+GPT-4 Filtering 62.7
+Golden Filtering 72.6

Bảng 7: Hiệu ứng của khả năng đánh giá mã của GPT-4. Chúng tôi lấy mẫu 10 câu trả lời với nhiệt độ=0.2 và báo cáo metric pass@1 trung bình. +MOT biểu thị mô hình MOT dựa trên mô hình DIT. GPT-4 hoặc golden filtering có nghĩa là chúng tôi sử dụng GPT-4 hoặc các test cases đơn vị vàng để lọc ra những câu trả lời sai từ mô hình DIT, và sau đó chúng tôi báo cáo pass@1 trung bình trong số các câu trả lời còn lại.
