# 2310.04564.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/activation/2310.04564.pdf
# Kích thước tệp: 2087280 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ReLU Trở Lại:
Khai Thác Tính Thưa Thớt Kích Hoạt Trong Các Mô Hình Ngôn Ngữ Lớn
Iman Mirzadeh†Keivan Alizadeh Sachin Mehta Carlo C Del Mundo
Oncel Tuzel Golnoosh Samei Mohammad Rastegari Mehrdad Farajtabar†
Apple

TÓM TẮT
Các Mô Hình Ngôn Ngữ Lớn (LLM) với hàng tỷ tham số đã biến đổi mạnh mẽ các ứng dụng AI. Tuy nhiên, nhu cầu tính toán cao trong quá trình suy luận đã tạo ra những thách thức đáng kể cho việc triển khai trên các thiết bị có tài nguyên hạn chế. Mặc dù xu hướng gần đây ưa chuộng các hàm kích hoạt thay thế như GELU hoặc SiLU, được biết đến với việc tăng tính toán, nghiên cứu này ủng hộ mạnh mẽ việc khôi phục lại kích hoạt ReLU trong LLM. Chúng tôi chứng minh rằng việc sử dụng hàm kích hoạt ReLU có tác động không đáng kể đến sự hội tụ và hiệu suất trong khi giảm đáng kể tính toán và truyền tải trọng số. Việc giảm này đặc biệt có giá trị trong bước suy luận bị giới hạn bộ nhớ, nơi hiệu quả là tối quan trọng. Khám phá các mẫu thưa thớt trong LLM dựa trên ReLU, chúng tôi tiết lộ việc tái sử dụng các neuron đã kích hoạt để tạo ra token mới và tận dụng những hiểu biết này, chúng tôi đề xuất các chiến lược thực tế để giảm đáng kể tính toán suy luận LLM lên đến ba lần, sử dụng kích hoạt ReLU với sự đánh đổi hiệu suất tối thiểu.

1 Giới thiệu
Sự phấn khích rộng rãi xung quanh Các Mô Hình Ngôn Ngữ Lớn (LLM) đã tạo ra sự quan tâm đáng kể trong việc tận dụng AI trong các lĩnh vực đa dạng [5, 9, 6]. Tuy nhiên, việc hiện thực hóa tiềm năng của LLM bị thách thức bởi các yêu cầu tính toán và bộ nhớ đáng kể trong quá trình suy luận [60, 40, 3]. Để nâng cao hiệu quả suy luận¹, nhiều kỹ thuật khác nhau đã được khám phá, bao gồm lượng tử hóa [12, 50], giải mã suy đoán [41], cắt tỉa [53, 71], và thưa thớt hóa trọng số [20, 15]. Trong số các kỹ thuật này, việc đạt được tính thưa thớt kích hoạt mang lại lợi thế hấp dẫn bằng cách cung cấp sự cân bằng thuận lợi giữa độ chính xác và tăng tốc, đặc biệt trên phần cứng hiện đại như GPU [51].

Đáng chú ý, việc sử dụng hàm kích hoạt Rectified Linear Unit (ReLU) [22] trong mạng neural được công nhận vì gây ra kích hoạt thưa thớt và đã được áp dụng trong nhiều công trình trước đây [27, 44, 48, 69]. Để tái khẳng định tính chất này, chúng tôi sử dụng mô hình OPT [80], sử dụng ReLU, và đo tính thưa thớt của kích hoạt trong Mạng Nguồn Cấp Dữ Liệu Tiến (FFN) giữa các lớp kết nối đầy đủ. Như được minh họa trong Hình 1a, tất cả các lớp đều thể hiện tính thưa thớt vượt quá 90%. Trung bình, trên tất cả các lớp, tính thưa thớt kích hoạt này dẫn đến tiết kiệm truyền tải trọng số đáng kể (I/O) giữa GPU và CPU, tác động đến 95% hàng của trọng số lớp phép chiếu xuống (Hình 1b). Việc giảm này trực tiếp chuyển thành tiết kiệm tính toán, vì đối với các hàng này, kết quả của phép nhân ma trận sẽ bằng không. Hơn nữa, không giống như tính thưa thớt không có cấu trúc (ví dụ, cắt tỉa trọng số), loại thưa thớt này thân thiện với phần cứng hơn do làm không các khối mở rộng và có cấu trúc hơn, như hàng hoặc cột [36, 51]. Đối với các mô hình OPT, tính thưa thớt này giảm tính toán cần thiết cho suy luận từ 6.6G FLOPS (Phép Toán Dấu Phẩy Động Mỗi Giây) xuống 4.5G FLOPS mỗi token, dẫn đến tiết kiệm tính toán 32% (Hình 1c).

†Tác giả liên hệ: {imirzadeh,farajtabar }@apple.com
¹Trong công trình này, chúng tôi sử dụng FLOPS như một đại diện cho hiệu quả suy luận. Trong Phụ lục B, chúng tôi chứng minh rằng đối với LLM có tính thưa thớt kích hoạt, FLOPS có thể phục vụ như một xấp xỉ tốt của hiệu quả thế giới thực do cấu trúc vốn có trong tính thưa thớt kích hoạt (ví dụ, bỏ qua toàn bộ hàng tương ứng với kích hoạt bằng không).arXiv:2310.04564v1  [cs.LG]  6 Oct 2023

--- TRANG 2 ---
BẢNTHẢO

0 5 10 15 20 25 30
Lớp020406080100Tính Thưa Thớt Kích Hoạt
OPT 6.7B (ReLU)
Llama 7B (SiLU)
Falcon 7B (GELU)
(a) Tính thưa thớt của các mô hình khác nhau

Chiếu LênChiếu XuốngChuẩn hóaChuẩn hóa
Chiếu XuốngKích HoạtChiếu LênChú ÝReLU
0
000 (b) Tính thưa thớt cho Hiệu quả

4.14.54.8 6.6
GFLOPS Mỗi Token59.566.068.0Độ Chính Xác 0-shot Trung BìnhOPT 6.7B (ReLU)OPT 6.7B (GELU)OPT 6.7B (SiLU)Falcon 7B (GELU)
Falcon 7B (ReLU) Falcon 7B (SiLU)Llama 7B (SiLU)Llama 7B (ReLU)Llama 7B (GELU) (c) Độ chính xác so với Tính toán

Hình 1: (a)Tính Thưa Thớt Kích Hoạt của các mô hình được huấn luyện trước khác nhau: OPT dựa trên ReLU cho thấy tính thưa thớt cao hơn đáng kể.
(b)Các mục đã được làm không sau ReLU tiết kiệm tính toán trong các khối bán cấu trúc lớn (ví dụ, hàng). (c)So sánh hiệu quả suy luận và hiệu suất của các mô hình khác nhau với các hàm kích hoạt khác nhau sau tinh chỉnh: Việc lựa chọn hàm kích hoạt không tác động đáng kể đến độ chính xác, vì bất kỳ GELU, SiLU, hoặc ReLU nào cũng có thể được sử dụng trên cả ba mô hình và đạt được cùng mức độ chính xác như hàm kích hoạt gốc. Tuy nhiên, việc sử dụng ReLU có thể cung cấp lợi ích bổ sung là dẫn đến tính thưa thớt kích hoạt và suy luận nhanh hơn.

Tuy nhiên, một xu hướng gần đây đã xuất hiện, ưa chuộng các biến thể của ReLU mượt mà hơn nhưng phức tạp hơn [28, 64].
Những phương án thay thế này đã trở nên phổ biến do khả năng hội tụ nhanh hơn một chút và độ chính xác cuối cùng được cải thiện [66].
Ví dụ, PaLM [9] và các mô hình Llama [73] áp dụng SiLU² [28, 17, 64], trong khi các mô hình MPT [56] và Falcon [2]
sử dụng GELU [28]. Tuy nhiên, như được chứng minh trong Hình 1c, khi chúng tôi tinh chỉnh một số LLM được huấn luyện trước với các hàm kích hoạt khác nhau, hiệu suất của chúng không thay đổi đáng kể (trong một mô hình cụ thể), trong khi các mô hình ReLU yêu cầu ít tính toán hơn nhiều.

Trong bài báo này, chúng tôi đánh giá lại việc sử dụng ReLU cho LLM. Chúng tôi được thúc đẩy bởi sự cân nhắc thực dụng rằng, trong nhiều ứng dụng thế giới thực và nền tảng tính toán có khả năng hỗ trợ phép nhân vector-ma trận thưa thớt, hiệu quả tính toán trong quá trình suy luận vượt trội hơn chi phí tính toán một lần phát sinh trong quá trình huấn luyện. Chúng tôi đóng góp những điều sau:

• Chúng tôi chứng minh rằng khi được huấn luyện từ đầu, không có sự khác biệt đáng kể về hiệu suất giữa các hàm kích hoạt khác nhau. Tuy nhiên, về yêu cầu tính toán trong quá trình suy luận, kích hoạt ReLU chứng minh nhẹ hơn đáng kể (Mục 3).

• Xem xét rằng nhiều LLM hiện đại (ví dụ, Llama và Falcon) đã được huấn luyện với kích hoạt không phải ReLU, và việc huấn luyện chúng từ đầu không hiệu quả về chi phí, chúng tôi nghiên cứu việc tinh chỉnh các mô hình này với kích hoạt ReLU. Chúng tôi cho thấy rằng các mô hình nhanh chóng lấy lại hiệu suất ban đầu trên nhiều tác vụ lý luận và đọc hiểu khác nhau (Mục 4.1). Hơn nữa, chúng tôi cho thấy rằng bằng cách tận dụng tính thưa thớt kích hoạt của các lớp ReLU và chèn thêm các lớp ReLU sau các lớp chuẩn hóa, chúng tôi có thể giảm thêm FLOPS suy luận lên đến ba lần (Mục 4.2).

• Ngoài lợi ích tính toán của chúng, chúng tôi trình bày hai ứng dụng đầy hứa hẹn của tính thưa thớt kích hoạt có thể truyền cảm hứng cho công trình tương lai. Thứ nhất, chúng tôi chứng minh rằng LLM với kích hoạt ReLU tái sử dụng một phần đáng kể các neuron đã được kích hoạt trong quá trình tạo token, một hiện tượng chúng tôi gọi là tính thưa thớt tổng hợp (Mục 5.1). Khả năng tái sử dụng này dẫn đến tăng tốc suy luận cho giải mã suy đoán (Mục 5.2). Thêm vào đó, chúng tôi cho thấy rằng việc nghiên cứu các kích hoạt trước của LLM được huấn luyện trước có thể hướng dẫn việc lựa chọn các hàm kích hoạt không thông thường (ví dụ, ReLU dịch chuyển), đạt được lên đến 90% tính thưa thớt trong khi duy trì hiệu suất tương tự như kích hoạt ReLU (Mục 5.3).

Nhìn chung, chúng tôi tin rằng công trình của chúng tôi đại diện cho một bước quan trọng hướng tới việc tận dụng tiềm năng của các hàm kích hoạt thưa thớt cho suy luận nhanh hơn và hiệu quả hơn trong các mô hình ngôn ngữ lớn.

²Để chính xác hơn, các mô hình được đề cập sử dụng hàm kích hoạt SwiGLU, nhưng trong công trình này, chúng tôi tập trung vào mô-đun cổng sử dụng hàm SiLU (Swish).

--- TRANG 3 ---
BẢNTHẢO

2 Các Công Trình Liên Quan

Hàm Kích Hoạt trong Transformer. Kiến trúc Transformer gốc [74] được đề xuất với hàm kích hoạt ReLU [22], theo xu hướng phổ biến của ReLU vào thời điểm đó. Sau đó, một số nghiên cứu nhằm cải thiện hàm kích hoạt ReLU bằng cách tăng tính mượt mà [28] và/hoặc bao gồm các cơ chế cổng có tham số, như GELU, SiLU, GLU, và SwiGLU [11, 64]. Các nghiên cứu trước đây đã chứng minh lợi ích của những phương án thay thế này cho ReLU đối với transformer [66, 57], nhưng ở quy mô nhỏ (ví dụ, họ huấn luyện các mô hình lên đến vài trăm triệu tham số với tối đa 35B token, trong khi trong công trình này, chúng tôi huấn luyện các mô hình 1B tham số trên hơn 100B token). Tuy nhiên, chúng tôi tin rằng tác động của các hàm kích hoạt lên hiệu suất là biên tế, theo các quy luật mở rộng [37, 31], nói rằng các thay đổi kiến trúc không tác động đáng kể đến hiệu suất.

Tính Thưa Thớt Kích Hoạt. Nghiên cứu hiện tại cho thấy tăng tính thưa thớt giảm thời gian suy luận và huấn luyện [44, 25, 70, 81, 47, 51]. Ví dụ, Jaszczur et al. [36] sử dụng ReLU và thêm một bộ điều khiển để cả thúc đẩy và dự đoán tính thưa thớt, trong khi các công trình khác chỉ sử dụng các mô-đun dự đoán để dự đoán mặt nạ kích hoạt [51]. Chúng tôi lưu ý rằng các công trình được đề cập giả định mô hình được huấn luyện trước đã sử dụng kích hoạt ReLU thưa thớt, và do đó, chỉ huấn luyện một mô-đun riêng để dự đoán tính thưa thớt có thể đủ. Tuy nhiên, chúng tôi lưu ý rằng hầu hết LLM được huấn luyện trước ngày nay không sử dụng ReLU, và chúng tôi nhằm khắc phục khoảng cách này. Hơn nữa, những công trình này chỉ tập trung vào một kiến trúc transformer duy nhất trong khi chúng tôi tập trung vào nhiều kiến trúc khác nhau để phát hiện của chúng tôi có thể thực tế. Cuối cùng, chúng tôi cho thấy rằng không cần huấn luyện một mô-đun dự đoán riêng biệt làm phức tạp đồ thị tính toán, và việc sử dụng các lớp ReLU hiệu quả có thể đủ.

Giải Mã Suy Đoán và Tính Thưa Thớt. Giải mã suy đoán chống lại độ trễ dưới ràng buộc bộ nhớ bằng cách sử dụng một mô hình nhỏ hơn để dự đoán token và một mô hình lớn hơn để xác minh [46, 41]. Nghiên cứu tích hợp nó với tính thưa thớt, chúng tôi phát hiện tính thưa thớt kích hoạt thể hiện một mẫu thời gian, nâng cao giải mã suy đoán. Chúng tôi cung cấp hướng dẫn cho việc lựa chọn tham số khi kết hợp tính thưa thớt.

Chúng tôi trì hoãn các dòng công trình liên quan khác trực giao với công trình của chúng tôi, như các kỹ thuật nén mô hình, phương pháp chú ý thưa thớt, và Hỗn Hợp Chuyên Gia (MoE) đến Phụ lục A.

3 Hàm Kích Hoạt Có Tác Động Đến Hiệu Suất Không?

Mục này đầu tiên tổng quan về thiết lập thí nghiệm của chúng tôi, bao gồm các mô hình, dữ liệu và đánh giá. Sau đó, bằng cách huấn luyện nhiều mô hình từ đầu với các hàm kích hoạt khác nhau, chúng tôi chứng minh rằng việc thay đổi hàm kích hoạt tác động tối thiểu đến hiệu suất. Tuy nhiên, tác động lên hiệu quả suy luận là đáng kể.

3.1 Thiết lập thí nghiệm

Mô hình. Chúng tôi sử dụng các mô hình được huấn luyện trước mã nguồn mở như OPT [80], Llama (v1) [73], và Falcon [2] vì chúng sử dụng các kiến trúc và thiết lập huấn luyện trước khác nhau (ví dụ, cấu trúc attention/FFN/chuẩn hóa, hàm kích hoạt), cho phép nghiên cứu của chúng tôi bao phủ một phạm vi rộng hơn các mô hình.

Bộ dữ liệu. Chúng tôi sử dụng bộ dữ liệu RefinedWeb [59], cho việc huấn luyện trước trong Mục 3.2 và tinh chỉnh các mô hình được huấn luyện trước trong Mục 4. Chúng tôi chọn RefinedWeb vì nó là một tập con chất lượng cao của Common Crawl, thường được sử dụng trong giai đoạn huấn luyện trước của LLM, bao gồm Llama, Falcon, và OPT. Chúng tôi cũng sử dụng phần validation của WikiText [54] để đo tính thưa thớt và ghi lại phân phối kích hoạt trước của nhiều mô hình được huấn luyện trước khác nhau. Tuy nhiên, kết luận của chúng tôi áp dụng cho các bộ dữ liệu khác mà chúng tôi đã thử nghiệm.

Huấn luyện và Tinh chỉnh. Để tinh chỉnh các mô hình được huấn luyện trước, chúng tôi theo công thức huấn luyện trước gốc, ngoại trừ chúng tôi sử dụng tốc độ học cố định 1.5e-5 cho các mô hình Llama 7B, Falcon 7B, và OPT 6.7B. Thêm vào đó, chúng tôi sử dụng bộ tối ưu AdamW [52] cho việc tinh chỉnh của chúng tôi với ZeRO stage 1 [62], nơi chúng tôi chia sẻ các trạng thái tối ưu trên các GPU khác nhau. Để huấn luyện trước các mô hình OPT 1.3B từ đầu trong Mục 3.2, chúng tôi theo công thức huấn luyện OPT.

Đánh giá. Để đánh giá hiệu suất, chúng tôi sử dụng các tác vụ few-shot từ Language Model Evaluation Harness [23]. Chúng tôi chọn những tác vụ này sao cho chúng có thể đo nhiều khả năng của các mô hình (ví dụ, đọc hiểu, lý luận, v.v.), và chúng tôi nhằm nhất quán với các công trình khác trong tài liệu để làm cho việc so sánh dễ dàng hơn.
Nhất quán với các mục khác, chúng tôi so sánh tính thưa thớt kích hoạt như một thước đo hiệu quả.
Chi tiết thêm về mối quan hệ giữa tính thưa thớt kích hoạt, FLOPS, và hiệu quả suy luận được thảo luận trong Phụ lục B.

--- TRANG 4 ---
BẢNTHẢO

−4−2 0 2 4
x0.00.20.40.60.81.0σ(βx)β= 1.0(SiLU)
β= 1.7(GELU)
β= 8
β→∞ (ReLU)
(a)

−10−9−8−7−6−5
x0.0000.0020.0040.0060.0080.010σ(βx)β= 1.0(SiLU)
β= 1.7(GELU)
β= 8
β→∞ (ReLU) (b)

0 20 40 60 80 100
Tỷ Token Đã Huấn Luyện020406080100Tính Thưa Thớt Kích Hoạt Trung BìnhΒ= 1.0(SiLU)
β= 1.7(GELU)
β= 8
β→∞ (ReLU) (c)

0 20 40 60 80 100
Tỷ Token Đã Huấn Luyện253035404550Độ Chính Xác ARC-EasyKích hoạt:xσ(βx)
β= 1.0(SiLU)
β= 1.7(GeLU)
β= 8
β→∞ (ReLU)
(d)

0 20 40 60 80 100
Tỷ Token Đã Huấn Luyện262830323436Độ Chính Xác HellaSwagKích hoạt:xσ(βx)
β= 1.0(SiLU)
β= 1.7(GeLU)
β= 8
β→∞ (ReLU) (e)

0 20 40 60 80 100
Tỷ Token Đã Huấn Luyện010203040Độ Chính Xác LAMBADAKích hoạt:xσ(βx)
β= 1.0(SiLU)
β= 1.7(GeLU)
β= 8
β→∞ (ReLU) (f)

Hình 2: (trên) (a) Hình dạng của các hàm cổng khác nhau trên [-5, 5]; (b) Tiếp tục (a) nơi SiLU tương đối lớn hơn so với các hàm khác; (c) Tính thưa thớt của FFN với các kích hoạt khác nhau: tăng β tăng tính thưa thớt. (dưới) khi được huấn luyện từ đầu, các mô hình OPT 1.3 B sử dụng các hàm kích hoạt khác nhau đạt được hiệu suất tương tự.

3.2 Huấn luyện từ đầu: hiệu suất và tính thưa thớt

Trong khi tài liệu trước đây gợi ý rằng các biến thể không phải ReLU có thể cải thiện hiệu suất của transformer [66, 57], chúng tôi lập luận rằng tác động này là biên tế nhất. Để hỗ trợ tuyên bố của chúng tôi, chúng tôi huấn luyện mô hình OPT 1.3B từ đầu trên một trăm tỷ token của bộ dữ liệu RefinedWeb với các hàm kích hoạt khác nhau, bao gồm ReLU, SiLU, và GELU. Tất cả các hàm kích hoạt này có thể được xem như f(x) = x·σ(βx), nơi β điều khiển phần cổng (ngưỡng cắt mượt) của hàm kích hoạt (xem Hình 2a). Với β = 1, chúng ta sẽ có SiLU(x·σ(x)), và β = 1.7 là một xấp xỉ tốt của GELU. Cuối cùng, khi β → ∞, hàm kích hoạt trở nên gần với ReLU hơn. Để khám phá thêm phổ từ ReLU đến SiLU chúng tôi thêm một hàm khác với β = 8.

Như được hiển thị trong hàng dưới của Hình 2, hiệu suất của các mô hình rất tương tự khi sử dụng các hàm kích hoạt khác nhau. Điều này nhất quán với tài liệu về quy luật mở rộng ([37, 31]), gợi ý rằng hiệu suất của các mô hình đủ lớn được huấn luyện trên dữ liệu đủ lớn phụ thuộc rất nhiều vào tính toán và dữ liệu, không phải chi tiết kiến trúc.

Trong khi mức hiệu suất của các kích hoạt khác nhau tương tự, mức tính thưa thớt kích hoạt của chúng khác nhau. Ở đây, chúng tôi định nghĩa tính thưa thớt như mức tính thưa thớt trung bình trên tất cả các lớp cho mỗi mô hình. Như được hiển thị trong Hình 2c, khi chúng ta chuyển từ SiLU sang ReLU (tăng β), tính thưa thớt cũng tăng. Điều này do các ngưỡng cổng khác nhau, vì ReLU loại bỏ đáng kể nhiều giá trị hơn so với GELU và SiLU (xem Hình 2b). Trong Phụ lục D, chúng tôi minh họa sự tiến hóa của phân phối kích hoạt trước trong suốt quá trình huấn luyện.

Nhìn chung, kết quả hỗ trợ tuyên bố ban đầu của chúng tôi: các kích hoạt không phải ReLU dẫn đến lợi ích hiệu suất không đáng kể (nếu có) nhưng mất mát đáng kể về tính thưa thớt và hiệu quả. Trong khi, đôi khi, hiệu suất của GeLU hoặc SiLU có thể cao hơn một chút, ReLU có thể theo kịp với việc huấn luyện dài hơn một chút. Chúng tôi thừa nhận rằng để bù đắp cho khoảng cách nhỏ về hiệu suất, chúng ta cần trả chi phí một lần cho việc huấn luyện dài hơn. Tuy nhiên, đổi lại, chúng ta nhận được tính thưa thớt cao hơn đáng kể.

4 Relufication

Trong khi ở mục trước, chúng ta đã thấy rằng hiệu suất không phụ thuộc vào hàm kích hoạt, chúng tôi lưu ý rằng hầu hết các LLM được huấn luyện trước có sẵn được huấn luyện với các hàm kích hoạt khác ReLU. Do đó, để kết hợp lợi ích tính toán của kích hoạt ReLU tại thời điểm suy luận, chúng tôi thực hiện nhiều phẫu thuật kiến trúc khác nhau và nghiên cứu hậu quả của những thay đổi như vậy.

--- TRANG 5 ---
BẢNTHẢO

Chú ÝReLUChiếu LênChiếu Xuống
Chuẩn hóaChuẩn hóaReLU
Lớp OPTRL eLUChú ÝChiếu LênChiếu Xuống
Chuẩn hóaReLU
Lớp FalconReLUChèn ReLUThay thế bằng ReLUGiữ ReLU

Chú ÝReLUChiếu LênChiếu Xuống
Chuẩn hóaChuẩn hóaReLU
Lớp LlamaChiếu Cổnggiai đoạn 2giai đoạn 1

Hình 3: Phẫu thuật kiến trúc cho relufication. Trong giai đoạn 1 chúng tôi giữ các ReLU hiện có (trong trường hợp OPT) hoặc thay thế hàm kích hoạt giữa phép chiếu lên và phép chiếu xuống từ GELU (Falcon) và SiLU (Llama) thành ReLU. Trong giai đoạn 2, chúng tôi chèn ReLU mới sau các lớp chuẩn hóa.

0 100 200
Token0.02.55.07.510.0Tính Thưa Thớt Kích Hoạt (%)18162432
Lớp
(a) Falcon 7B (GELU)

0 100 200
Token708090100Tính Thưa Thớt Kích Hoạt (%)18162432
Lớp
 (b) Falcon 7B (ReLU)

0 100 200
Token0.000.100.250.500.751.00Tính Thưa Thớt Kích Hoạt (%)18162432
Lớp
 (c) Llama 7B (SiLU)

0 50 100 150 200
Token3040506070Tính Thưa Thớt Kích Hoạt (%)18162432
Lớp
 (d) Llama 7B (ReLU)

Hình 4: Tính thưa thớt kích hoạt của các mô hình Falcon và Llama cải thiện đáng kể sau relufication.

Chúng tôi trình bày các phát hiện của chúng tôi về việc kết hợp kích hoạt ReLU vào các LLM được huấn luyện trước, một quá trình chúng tôi gọi là relufication. Cụ thể hơn, chúng tôi cho thấy rằng việc thay thế các hàm kích hoạt của LLM được huấn luyện trước bằng ReLU là có thể, và hiệu suất có thể được khôi phục rất nhanh trong quá trình tinh chỉnh. Hơn nữa, chúng tôi cho thấy rằng chúng ta có thể khai thác các kích hoạt ReLU thưa thớt, và bằng cách chèn thêm các lớp ReLU sau các lớp chuẩn hóa, chúng ta có thể cải thiện hiệu quả suy luận, như FLOPS chỉ ra. Cuối cùng, chúng tôi cho thấy những sửa đổi này, dễ triển khai, dẫn đến các mô hình nhẹ hơn tại thời điểm suy luận trong khi duy trì hiệu suất tương đương với các mô hình được huấn luyện trước gốc.

4.1 Giai đoạn 1: thay thế các kích hoạt không phải ReLU

Quá trình relufication cho các kiến trúc được huấn luyện trước khác nhau được hiển thị trong Hình 3. Quá trình này có thể được thực hiện theo nhiều giai đoạn, như chúng tôi mô tả ở đây. Giai đoạn đầu tiên và trực quan hơn thay thế các kích hoạt không phải ReLU bằng ReLU trong lớp FFN. Đối với các mô hình Falcon và Llama, điều này có nghĩa là thay thế GELU và SiLU, tương ứng. Chúng tôi lưu ý rằng vì các mô hình OPT đã sử dụng kích hoạt ReLU, chúng tôi giữ nguyên những kích hoạt đó. Sau khi tinh chỉnh trên 30 tỷ token của RefinedWeb, Hình 4 cho thấy rằng các mô hình đã sửa đổi có tính thưa thớt đáng kể hơn trong kích hoạt của chúng.

Ngoài việc cải thiện mạnh mẽ tính thưa thớt kích hoạt, chúng ta có thể đưa ra một số quan sát đáng chú ý. Đầu tiên, trong khi hình dạng của kích hoạt trước phụ thuộc vào động lực huấn luyện trước và kiến trúc, trong Hình 5, chúng tôi cho thấy rằng nó không thay đổi đáng kể trong giai đoạn tinh chỉnh tương đối ngắn. Kết quả là, chúng ta có thể dự đoán tính thưa thớt kích hoạt trước khi tinh chỉnh, biết rằng nó sẽ không thay đổi đáng kể. Sau đó trong Mục 5.3 chúng tôi xây dựng trên quan sát này và đề xuất dịch chuyển các giá trị kích hoạt trước trước khi áp dụng ReLU và tăng thêm tính thưa thớt kích hoạt. Tính ổn định của phân phối kích hoạt trước có thể gợi ý rằng hành vi của mạng không thay đổi trong khi tạo ra các biểu diễn thưa thớt. Thật vậy, chúng tôi cho thấy rằng sau khi thay thế hàm kích hoạt bằng ReLU, các mô hình được tinh chỉnh nhanh chóng khôi phục hiệu suất của chúng trong Hình 6. Chúng tôi tin rằng việc tối ưu hóa quá trình này hơn nữa (ví dụ, sử dụng dữ liệu tinh chỉnh tốt hơn) là một hướng nghiên cứu tiếp theo thú vị.

--- TRANG 6 ---
BẢNTHẢO

−15−10−5 0 5 10 15
Giá Trị Kích Hoạt Trước020406080100Tỷ Lệ (%)GELU(x)/negationslash= 0
18162432
Lớp
(a) Falcon 7B (GELU)

−15−10−5 0 5 10 15
Giá Trị Kích Hoạt Trước020406080100Tỷ Lệ (%)ReLU(x)/negationslash= 0
18162432
Lớp
 (b) Falcon 7B (ReLU)

−15−10−5 0 5 10 15
Giá Trị Kích Hoạt Trước020406080100Tỷ Lệ (%)SiLU(x)/negationslash= 0
18162432
Lớp
 (c) Llama 7B (SiLU)

−15−10−5 0 5 10 15
Giá Trị Kích Hoạt Trước020406080100Tỷ Lệ (%)ReLU(x)/negationslash= 0
18162432
Lớp
 (d) Llama 7B (ReLU)

Hình 5: Phân phối kích hoạt trước của các mô hình được huấn luyện trước cho Falcon và Llama không thay đổi đáng kể trong giai đoạn tinh chỉnh ngắn của relufication. Đường nét đứt hiển thị điểm cắt trước đó đầu ra gần như bằng không.

Bảng 1: So sánh hiệu suất zero-shot trên một số tác vụ: Sau relufication, tính thưa thớt kích hoạt của các mô hình tăng đáng kể, do đó tăng hiệu quả được đo bằng FLOPS. Trong mỗi nhóm, mức hiệu suất tương đương.

Mô hình (giai đoạn)Tính Thưa Thớt Đầu Vào (%) FLOPS
(G)Độ Chính Xác Zero-Shot (%)
QKV DownProj UpProj Avg Arc-E Arc-C Hellaswag BoolQ PIQA LAMBADA TriviaQA WinoGrande SciQ

OPT 1.3B 0 96 0 1.3 50.7 57.3 22.9 41.3 57.0 71.8 56.0 6.1 58.9 84.6
OPT 2.7B (s2) 50 96 35 1.1 53.1 60.3 26.8 44.9 55.4 73.9 57.6 12.4 59.6 86.7
OPT 2.7B 0 96 0 1.8 54.5 63.3 29.2 45.8 57.6 74.2 61.4 12.3 60.8 85.9
OPT 6.7B (s2) 50 97 40 2.8 58.6 66.5 32.2 49.1 63.0 76.4 63.3 23.8 63.1 90.3
OPT 6.7B 0 97 0 4.5 59.8 68.0 32.4 50.2 68.4 75.8 67.2 20.9 65.3 90.2
Falcon 7B (s2) 56 95 56 2.2 64.8 73.6 38.6 55.3 68.4 78.9 67.6 40.4 67.1 93.4
Falcon 7B (s1) 0 94 0 4.1 65.2 72.2 39.1 55.4 70.6 78.4 69.2 40.5 67.5 93.1
Falcon 7B 0 1 0 6.6 66.8 74.6 40.2 57.7 73.5 79.4 74.5 40.4 67.2 94.0
Llama 7B (s2) 51 65 67 2.9 66.4 73.8 39.6 54.8 69.9 77.9 70.7 48.5 68.6 93.8
Llama 7B (s1) 0 62 0 4.8 67.1 75.2 40.1 55.2 73.4 77.7 71.5 49.6 67.1 94.2
Llama 7B 0 0 0 6.6 68.4 75.5 42.1 69.9 74.8 78.7 73.1 49.9 69.8 95.4

4.2 Giai đoạn 2: Thúc đẩy tính thưa thớt hơn

0 5 10 15 20 25 30
Tỷ Token Được Tinh Chỉnh3040506070Độ Chính Xác 0-shot Trung Bình
Llama (ReLU)
Llama (SiLU)
Falcon (ReLU)
Falcon (GELU)

Hình 6: Tiến trình của độ chính xác zero-shot trong quá trình tinh chỉnh: Mô hình nhanh chóng khôi phục hầu hết hiệu suất bị mất do phẫu thuật kiến trúc.

Trong giai đoạn trước, chúng tôi thay thế các kích hoạt không phải ReLU để có được tính thưa thớt hơn. Điều này dẫn đến đầu vào của lớp phép chiếu xuống thưa thớt, khoảng 30% tổng tính toán. Tuy nhiên, có những phép nhân ma trận-vector khác trong lớp giải mã của transformer bên cạnh phép chiếu xuống. Ví dụ, trước phép chiếu lên và phép chiếu cổng của lớp FFN, và phép chiếu QKV trong lớp chú ý (xem Hình 3). Cùng nhau, các phép nhân ma trận-vector được đề cập tiêu thụ khoảng 55% tổng tính toán.

Để đạt được mục tiêu này, chúng tôi sử dụng thực tế rằng trong các lớp transformer hiện đại, đầu vào cho cả lớp chú ý và FFN đều đến từ một lớp chuẩn hóa, ví dụ, LayerNorm [4] hoặc RMSNorm [78]. Những lớp này có thể được xem như một dạng MLP cụ thể, nơi, thay vì áp dụng các tham số học tùy ý, chúng học để chia tỷ lệ đầu vào. Do đó, chúng tôi áp dụng ReLU để có được kích hoạt thưa thớt sau các lớp chuẩn hóa mà chúng tôi gọi là giai đoạn thứ hai của relufication trong Hình 3.

Bảng 1 cho thấy rằng các giai đoạn khác nhau của quá trình relufication không làm giảm đáng kể độ chính xác zero-shot trong khi sử dụng ít tính toán hơn đáng kể. Tính thưa thớt được chia thành ba loại: phép chiếu lên, xuống, và QKV. Đáng chú ý, đầu vào cho QKV ít thưa thớt hơn so với phép chiếu FFN, điều này mở ra một hướng nghiên cứu thú vị cho tương lai. Chúng tôi lưu ý rằng khoảng cách nhỏ về hiệu suất giữa các mô hình gốc và relufied có thể một phần do quá trình tinh chỉnh chứ không nhất thiết là hàm kích hoạt. Việc tinh chỉnh của chúng tôi chỉ được áp dụng cho 30B và 50B token cho giai đoạn 1 và 2, tương ứng. Đặt vào quan điểm và so sánh với 1T token của Llama, ví dụ, điều này tương đương với 3-5% thời lượng huấn luyện gốc. Như được thảo luận trong Mục 3.2, theo tính chất mở rộng của LLM, khoảng cách sẽ được thu hẹp thêm bằng các bước tinh chỉnh bổ sung.

--- TRANG 7 ---
BẢNTHẢO

Bảng 2: Độ chính xác MMLU five-shot. Các mô hình được tinh chỉnh với các hàm kích hoạt khác nhau có hiệu suất tương tự. * Biểu thị chúng tôi thay thế hàm SiLU trong hàm kích hoạt SwiGLU của Llama bằng ReLU.

Mô hình Kích hoạt FLOPS(%) Avg Humanities STEM Social Sciences Other
Falcon 7B SiLU 100 26.4 24.8 27.4 27.2 26.2
Falcon 7B GELU 100 27.7 28.1 26.0 28.0 29.4
Falcon 7B ReLU 62 27.9 26.0 26.5 31.8 27.9
Llama 7B SiLU* 100 35.1 37.9 30.2 37 37.1
Llama 7B GELU 100 35.9 38.4 29.4 37.6 39.5
Llama 7B ReLU 72 34.7 34.8 31.2 36.3 37.8

Chúng tôi cũng đánh giá khả năng học theo ngữ cảnh của các mô hình relufied với benchmark Massive Multitask Language Understanding (MMLU) [29] trong Bảng 2. Kết quả của chúng tôi cho thấy rằng khi chúng tôi tăng cường các LLM gốc với các kích hoạt khác nhau và tinh chỉnh, hiệu suất few-shot cũng không thay đổi đáng kể. Hơn nữa, Mục E trong phụ lục cho thấy rằng một mô hình lớn hơn nhưng relufied hoạt động tốt hơn một mô hình nhỏ hơn gốc có cùng FLOPS.

Nhìn chung, kết quả khẳng định rằng quy trình relufication được đề xuất có thể giảm FLOPS suy luận ở nhiều giai đoạn và tỷ lệ khác nhau trong khi duy trì hiệu suất ngang bằng trên nhiều tác vụ khác nhau.

5 Ứng dụng

Trong mục này, chúng tôi thảo luận các hướng đầy hứa hẹn được thúc đẩy bởi cuộc điều tra của chúng tôi trong Mục 4. Đầu tiên, chúng tôi giới thiệu tính thưa thớt tổng hợp, cho thấy rằng các mạng ReLU tái sử dụng các neuron đã được kích hoạt trước đó khi tạo token. Do đó, chúng ta có thể tận dụng điều này để tăng tốc độ tạo. Tiếp theo, chúng tôi liên hệ tính thưa thớt tổng hợp với giải mã suy đoán để cải thiện thêm thời gian suy luận của giải mã suy đoán. Cuối cùng, chúng tôi thảo luận ngắn gọn một hướng đầy hứa hẹn của việc sử dụng hàm kích hoạt ReLU dịch chuyển để cải thiện tính thưa thớt hơn nữa.

5.1 Tính Thưa Thớt Tổng Hợp: tái sử dụng các neuron đã được kích hoạt trước đó

Một hệ quả của việc chỉ sử dụng một tập con nhỏ neuron cho mỗi token là nếu những neuron này được chia sẻ ở một mức độ nào đó, mô hình vẫn không sử dụng tất cả neuron cho đến khi nhiều token được xử lý. Chúng tôi gọi điều này là tính thưa thớt tổng hợp, mà chúng tôi định nghĩa là tỷ lệ neuron chưa được sử dụng đến khi xử lý token thứ t đầu tiên. Lưu ý rằng chỉ số này sẽ luôn không tăng. Một cách trực quan, nó đo khả năng chưa sử dụng của các neuron feed-forward để xử lý một prompt cụ thể.

Ở đây trong Hình 7a chúng tôi cho thấy rằng đối với mô hình OPT-6.7B, trung bình, khoảng 50% tất cả neuron sẽ không được sử dụng trên 150 token đầu tiên của các prompt đến từ bộ dữ liệu WikiText. Kết quả thực nghiệm của chúng tôi áp dụng cho các mô hình ReLU khác và các bộ dữ liệu khác. Thêm vào đó, trong Hình 7b, chúng tôi cho thấy rằng mẫu này khác xa với việc kích hoạt ngẫu nhiên các neuron trong quá trình tạo token với tỷ lệ bằng tỷ lệ trung bình sử dụng kích hoạt mỗi token. Gọi si là tính thưa thớt kích hoạt của lớp i được tính trung bình trên tất cả token. Khi đó, xác suất của một kích hoạt không được sử dụng trong việc tạo t token đầu tiên trong việc lựa chọn ngẫu nhiên đồng nhất là s^t_i. Hình 7b hiển thị đại lượng này cho hai lớp i = 8,24 cho 256 token đầu tiên trong đường nét đứt. Nó cũng hiển thị số lượng thực (quan sát được) của các kích hoạt được sử dụng trong đường liền. Thực tế rằng tính thưa thớt tổng hợp ngẫu nhiên (được gọi là tính thưa thớt ngẫu nhiên) thấp hơn tính thưa thớt tổng hợp quan sát được (chúng tôi gọi nó là tính thưa thớt tổng hợp) cho thấy một mẫu rõ ràng của việc tái sử dụng kích hoạt.

Chúng ta có thể hưởng lợi từ các kích hoạt chồng chéo bằng cách sử dụng các trọng số đã được tải trước đó từ lớp phép chiếu xuống cho các token sắp tới. Để kiểm tra điều này, chúng tôi bắt đầu với việc đọc 128 token. Đối với 128 token tiếp theo, chúng tôi không thường xuyên tránh tải trọng số mới cho mỗi γ token. Sử dụng γ = 16 làm ví dụ, các token 129-145 được tạo theo cách thông thường. Tuy nhiên, đối với các token 146-161, chúng tôi giữ lại trọng số hiện có mà không giới thiệu bất kỳ trọng số mới nào. Mẫu này tiếp tục, với mỗi tập γ token tiếp theo xen kẽ giữa việc tạo thông thường và tái sử dụng trọng số. Trong Hình 7c, chúng tôi quan sát chỉ có sự gia tăng nhẹ trong perplexity khi sử dụng xấp xỉ này để giải quyết bản chất tốn bộ nhớ và I/O của suy luận LLM. Hình này tương phản perplexity thu được từ việc tái sử dụng kích hoạt và lựa chọn ngẫu nhiên. Chiến lược tái sử dụng phù hợp tốt với baseline, trong khi lựa chọn ngẫu nhiên tăng perplexity đáng kể, làm nổi bật hiệu quả của việc tái sử dụng các kích hoạt đã được tải cho các token tiếp theo.

5.2 Tính thưa thớt kích hoạt và giải mã suy đoán

Như được nhighlighted trong Mục 5.1, việc tái sử dụng kích hoạt xảy ra cho nhiều token liên tiếp. Khi nhiều token liên tiếp được xử lý cùng nhau, chúng ta có thể tiết kiệm I/O (tức là, chuyển trọng số đến GPU/CPU như được thảo luận trong Phụ lục B)

--- TRANG 8 ---
BẢNTHẢO

0 50 100 150 200 250
Token0102030405060708090100Tính Thưa Thớt Tổng Hợp (%) trung bình
18162432
Lớp
(a)

0 50 100 150 200 250
Token020406080100Tính Thưa Thớt Tổng Hợp (%)Lớp
8, thực tế
8, ngẫu nhiên24, thực tế
24, ngẫu nhiên (b)

248 16 32 64
γ1050100200350Perplexity Wikitext
15.116.7 18.824.7Tái Sử Dụng Tổng Hợp
Tái Sử Dụng Ngẫu Nhiên
Không Tái Sử Dụng (c)

248 16 32 64
γ1.01.11.21.31.41.5Tăng Tốc
Tính Thưa Thớt Tổng Hợp
Tính Thưa Thớt Ngẫu Nhiên
Không Tính Thưa Thớt (d)

Hình 7: (a)Tính thưa thớt tổng hợp của các lớp khác nhau và trung bình của chúng. (b)Tính thưa thớt tổng hợp trong quá trình tạo token và so sánh với tính thưa thớt ngẫu nhiên. (c)Perplexity, dựa trên số lượng token mà trọng số đã tải từ các token trước được tái sử dụng. Đường nét đứt đại diện cho không tái sử dụng, đường liền màu xanh cho thấy trường hợp với việc tái sử dụng kích hoạt theo tính thưa thớt tổng hợp, và đường màu cam mô tả perplexity khi kích hoạt được tái sử dụng theo tính thưa thớt ngẫu nhiên. (d)Tăng tốc suy luận của giải mã suy đoán với tính thưa thớt tổng hợp và với tính thưa thớt ngẫu nhiên. Tăng tốc bằng 1.0 là phiên bản chuẩn của giải mã suy đoán.

liên quan đến các kích hoạt không được sử dụng trong bất kỳ kích hoạt nào trong số chúng. Nếu việc tái sử dụng không xảy ra, và tính thưa thớt của tất cả token hoàn toàn ngẫu nhiên, tính thưa thớt tổng hợp sẽ thu nhỏ theo cấp số nhân và nhanh chóng biến mất. Giải mã suy đoán [46] là một kỹ thuật liên quan sử dụng một mô hình nhỏ hơn Mq để đề xuất γ token và một mô hình lớn hơn Mp để xác minh những token đó và chọn những token phù hợp. Nó cải thiện thời gian chạy của mô hình bằng cách tránh chạy Mp tuần tự.

Để cải thiện giải mã suy đoán, tính thưa thớt tổng hợp có thể cắt giảm phần của mô hình cần được chạy. Thay vì chạy mô hình đầy đủ, chỉ những phần không thưa thớt cần được đánh giá, điều này sẽ giảm độ trễ I/O và tính toán. Giả sử tính thưa thớt tổng hợp trung bình của Mp cho γ token là s̄agg(γ), và chi phí chạy Mq so với Mp là c. Khi đó tăng tốc độ trễ dự kiến khi chuyển từ giải mã suy đoán chuẩn sang giải mã suy đoán thưa thớt là cγ+1 / cγ+(1−s̄agg(γ)).

Hình 7d so sánh giải mã suy đoán thưa thớt với phiên bản chuẩn cho mô hình OPT 6.7B. Như một nghiên cứu điển hình, với γ = 16, phiên bản thưa thớt có tăng tốc 1.27x so với giải mã suy đoán chuẩn. Nếu tính thưa thớt tổng hợp là ngẫu nhiên trên các token khác nhau, tăng tốc sẽ chỉ là 1.20x. Lưu ý rằng ngay cả tính thưa thớt ngẫu nhiên cũng sẽ dẫn đến tăng tốc so với giải mã suy đoán chuẩn. Điều này cho thấy thêm giá trị của relufication. Tuy nhiên, tăng tốc do tính thưa thớt ngẫu nhiên sẽ giảm nhanh so với tính thưa thớt tổng hợp khi chúng ta đi với γ lớn hơn. Ví dụ, với γ = 64 tăng tốc gần như không đáng kể, trong khi tăng tốc cho tính thưa thớt tổng hợp là khoảng 1.14x. Thảo luận thêm và chi tiết được hoãn đến Phụ lục C, nơi chúng tôi so sánh giải mã suy đoán thưa thớt, giải mã suy đoán chuẩn, và giải mã tự hồi quy và thảo luận γ tối ưu trong trường hợp giải mã suy đoán thưa thớt.

5.3 Hàm kích hoạt ReLU dịch chuyển

Công trình của chúng tôi trong mục này được thúc đẩy bởi quan sát từ Mục 4, nơi, so sánh Hình 4d với Hình 4b tiết lộ rằng Llama relufied có tính thưa thớt ít hơn nhiều (65%) so với mô hình Falcon relufied (95%). Thêm vào đó, chúng tôi xây dựng trên hai phát hiện trước đây của chúng tôi. Thứ nhất, phân phối kích hoạt trước của Llama relufied (Hình 5c) bao gồm một khối lượng đáng kể sau giá trị cắt tại không. Thứ hai, hình dạng của phân phối kích hoạt trước không thay đổi trước và sau quá trình relufication (Hình 5c và Hình 5d).

Do đó, chúng ta có thể dịch chuyển phân phối kích hoạt trước sang trái để đặt nhiều thể tích hơn trước điểm cắt tại 0. Để đạt được mục tiêu này, đối với đầu vào kích hoạt trước x, thay vì áp dụng ReLU(x), chúng tôi sử dụng ReLU(x−b) nơi b ∈ R là một vô hướng hằng số. Chúng tôi đề xuất đặt giá trị b dựa trên phân phối kích hoạt trước. Ví dụ, dựa trên phân phối trong Hình 5d, đặt b = 1 và do đó sử dụng ReLU(x−1) như hàm kích hoạt của chúng tôi sẽ dẫn đến việc loại bỏ 95% kích hoạt trước và làm cho nó thưa thớt đáng kể hơn. Một lợi ích khác của phương pháp này là tính đơn giản, vì điều này không yêu cầu thay đổi hàm mất mát hoặc chế độ huấn luyện.

Hình 8a cho thấy rằng hàm kích hoạt ReLU dịch chuyển có độ chính xác ngang bằng với hàm kích hoạt ReLU. Hơn nữa, tương tự như quan sát của chúng tôi trong Mục 4, kích hoạt ReLU dịch chuyển nhanh chóng khôi phục hiệu suất bị mất do thay đổi mạnh mẽ của hàm kích hoạt, trong khi nó cũng duy trì mức tính thưa thớt kích hoạt rất cao trong giai đoạn tinh chỉnh. Khoảng cách giữa ReLU dịch chuyển và ReLU rộng hơn trong các giai đoạn đầu của huấn luyện, và nó thu hẹp khi nhìn thấy nhiều token hơn.

Một cuộc điều tra sâu hơn về các biến thể ReLU có thể thúc đẩy tính thưa thớt mà không hy sinh hiệu suất là một hướng nghiên cứu tương lai hấp dẫn. Hơn nữa, sẽ thú vị khi nghiên cứu tác động của ReLU dịch chuyển cho giai đoạn 2 của quá trình relufication của chúng tôi nơi mức tính thưa thớt thường không cao lắm.

--- TRANG 9 ---
BẢNTHẢO

012 5 10 15
Tỷ Token Được Tinh Chỉnh3040506070Độ Chính Xác 0-shot Trung Bình
Kích Hoạt Llama
SiLU
ReLU
ReLU Dịch Chuyển
(a)

012 5 10 15
Tỷ Token Được Tinh Chỉnh02040608095100Tính Thưa Thớt Kích Hoạt Trung Bình
Kích Hoạt Llama
ReLU Dịch Chuyển
ReLU
SiLU (b)

Hình 8: Tác động của ReLU dịch chuyển trên mô hình Llama. (a)Hiệu suất gần như giống với ReLU gốc. (b)ReLU dịch chuyển (tức là, ReLU(x−1)) thưa thớt hơn nhiều so với ReLU gốc.

6 Kết luận

Trong nghiên cứu này, chúng tôi đã tiến hành một cuộc điều tra quy mô lớn về các hàm kích hoạt, và chúng tôi đã cho thấy rằng việc lựa chọn hàm kích hoạt trong quá trình huấn luyện trước và tinh chỉnh không có tác động đáng kể đến hiệu suất trong khi việc sử dụng ReLU có thể cung cấp lợi ích bổ sung là dẫn đến tính thưa thớt kích hoạt và suy luận hiệu quả hơn. Để thu hẹp khoảng cách giữa các mô hình được huấn luyện trước hiện có và công trình của chúng tôi, chúng tôi đã relufied một số mô hình để kết hợp hàm kích hoạt ReLU vào kiến trúc của những mô hình đã được huấn luyện trước này. Chúng tôi đã cho thấy rằng trên một số tác vụ zero-shot và few-shot, các LLM dựa trên ReLU hoạt động tương tự như các mô hình không phải ReLU của chúng với tính toán giảm đáng kể. Thêm vào đó, sau khi quan sát các mẫu thưa thớt trong LLM ReLU, chúng tôi đã khám phá một vài hướng đầy hứa hẹn để cải thiện tốc độ tạo token thông qua tính thưa thớt tổng hợp và đạt được hiệu quả lớn hơn bằng cách sử dụng các hàm kích hoạt dựa trên ReLU như ReLU dịch chuyển.

Chúng tôi tin rằng công trình của chúng tôi là một trong số ít các nghiên cứu điều tra thay đổi trong các thành phần kiến trúc của LLM ở quy mô lớn. Chúng tôi hy vọng những phát hiện của chúng tôi thúc đẩy cộng đồng tiếp tục điều tra các lợi thế của tính thưa thớt kích hoạt có cấu trúc tốt, cuối cùng nâng cao hiệu quả của những mô hình này.

Lời cảm ơn

Các tác giả muốn cảm ơn Fartash Faghri, Minsik Cho, Thomas Merth, và Mohammad Samragh cho những thảo luận và phản hồi vô giá của họ về dự án này.

--- TRANG 10 ---
BẢNTHẢO

Tài liệu tham khảo
[1] Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, và Olivier Bachem. Gkd: Generalized knowledge distillation for auto-regressive sequence models. CoRR, 2023.
[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Maitha Alhammadi, Mazzotta Daniele, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, và Guilherme Penedo. The falcon series of language models: Towards open frontier models. 2023.
[3] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. Trong SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, trang 1–15. IEEE, 2022.
[4] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E. Hinton. Layer normalization, 2016.
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[7] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, và Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. CoRR, abs/2307.13304, 2023. doi: 10.48550/arXiv.2307.13304.
[8] Tianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, và Furu Wei. Task-specific expert pruning for sparse mixture-of-experts. ArXiv, abs/2206.00277, 2022.
[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[10] Djork-Arné Clevert, Thomas Unterthiner, và Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). Trong Yoshua Bengio và Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.
[11] Yann N. Dauphin, Angela Fan, Michael Auli, và David Grangier. Language modeling with gated convolutional networks. Trong Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, trang 933–941. JMLR.org, 2017.
[12] Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. CoRR, 2023.
[14] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, và Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless LLM weight compression. CoRR, abs/2306.03078, 2023. doi: 10.48550/arXiv.2306.03078.
[15] Gaochen Dong và Wei Chen. Blockwise compression of transformer-based models without retraining. arXiv preprint arXiv:2304.01483, 2023.
[16] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, và Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts. Trong Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, và Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 của Proceedings of Machine Learning Research, trang 5547–5569. PMLR, 2022.
[17] Stefan Elfwing, Eiji Uchibe, và Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3–11, 2018.
[18] William Fedus, Jeff Dean, và Barret Zoph. A review of sparse expert models in deep learning. ArXiv, abs/2209.01667, 2022.

--- TRANG 11 ---
BẢNTHẢO

[19] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1–120:39, 2022.
[20] Elias Frantar và Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. Trong International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 của Proceedings of Machine Learning Research, trang 10323–10337. PMLR, 2023. URL https://proceedings.mlr.press/v202/frantar23a.html.
[21] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. GPTQ: accurate post-training quantization for generative pre-trained transformers. CoRR, abs/2210.17323, 2022. doi: 10.48550/arXiv.2210.17323.
[22] Kunihiko Fukushima. Visual feature extraction by a multilayered network of analog threshold elements. IEEE Trans. Syst. Sci. Cybern., 5:322–333, 1969.
[23] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.
[24] Yuxian Gu, Li Dong, Furu Wei, và Minlie Huang. Knowledge distillation of large language models. CoRR, 2023.
[25] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, và William J. Dally. Retrospective: Eie: Efficient inference engine on sparse and compressed neural network, 2023.
[26] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, và Ed H. Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Trong Neural Information Processing Systems, 2021.
[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.
[28] Dan Hendrycks và Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.
[29] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Measuring massive multitask language understanding. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.
[30] Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. CoRR, 2015.
[31] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, và Laurent Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550/arXiv.2203.15556.
[32] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, và Roman Novak. Infinite attention: NNGP and NTK for deep attention networks. Trong Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 của Proceedings of Machine Learning Research, trang 4376–4386. PMLR, 2020.
[33] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, và Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Trong Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, trang 8003–8017. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/2023.findings-acl.507.
[34] Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, và Minsoo Rhu. Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference. arXiv preprint arXiv:2308.12066, 2023.
[35] Ajay Jaiswal, Shiwei Liu, Tianlong Chen, và Zhangyang Wang. The emergence of essential sparsity in large pre-trained models: The weights that matter. CoRR, 2023.
[36] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, và Jonni Kanerva. Sparse is enough in scaling transformers. Trong Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=-b5OSCydOMe.
[37] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.

--- TRANG 12 ---
BẢNTHẢO

[38] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, và Dongsoo Lee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. CoRR, abs/2305.14152, 2023. doi: 10.48550/arXiv.2305.14152. URL https://doi.org/10.48550/arXiv.2305.14152.
[39] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, và Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. CoRR, abs/2306.07629, 2023. doi: 10.48550/arXiv.2306.07629.
[40] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference. Trong Architecture and System Support for Transformer Models (ASSYST@ ISCA 2023), 2023.
[41] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, John Canny, Jitendra Malik, Michael W. Mahoney, Amir Gholami, và Kurt Keutzer. Speculative decoding with big little decoder, 2023.
[42] Günter Klambauer, Thomas Unterthiner, Andreas Mayr, và Sepp Hochreiter. Self-normalizing neural networks. Trong Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, và Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, trang 971–980, 2017.
[43] Rui Kong, Yuanchun Li, Qingtian Feng, Weijun Wang, Linghe Kong, và Yunxin Liu. Serving moe models on resource-constrained edge devices via dynamic expert swapping, 2023.
[44] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, và Dan Alistarh. Inducing and exploiting activation sparsity for fast inference on deep neural networks. Trong International Conference on Machine Learning, trang 5533–5543. PMLR, 2020.
[45] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, và Eunhyeok Park. OWQ: lessons learned from activation outliers for weight quantization in large language models. CoRR, abs/2306.02272, 2023. doi: 10.48550/arXiv.2306.02272.
[46] Yaniv Leviathan, Matan Kalman, và Yossi Matias. Fast inference from transformers via speculative decoding. Trong Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, và Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 của Proceedings of Machine Learning Research, trang 19274–19286. PMLR, 2023.
[47] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. Large models are parsimonious learners: Activation sparsity in trained transformers. arXiv preprint arXiv:2210.06313, 2022.
[48] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye, Felix Chern, Felix X. Yu, Ruiqi Guo, và Sanjiv Kumar. The lazy neuron phenomenon: On emergence of activation sparsity in transformers. Trong The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[49] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, và Song Han. AWQ: activation-aware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978, 2023. doi: 10.48550/arXiv.2306.00978.
[50] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, và Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. CoRR, 2023.
[51] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. Trong International Conference on Machine Learning, trang 22137–22176. PMLR, 2023.
[52] Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. Trong 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
[53] Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large language models. arXiv preprint arXiv:2305.11627, 2023.
[54] Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture models. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.

--- TRANG 13 ---
BẢNTHẢO

[55] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, và Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. Trong Proceedings of the AAAI conference on artificial intelligence, volume 34, trang 5191–5198, 2020.
[56] NLP Team MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b.
[57] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Févry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, và Colin Raffel. Do transformer modifications transfer across implementations and applications? Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, trang 5758–5773. Association for Computational Linguistics, 2021.
[58] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, và Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. CoRR, 2023.
[59] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. CoRR, abs/2306.01116, 2023. doi: 10.48550/arXiv.2306.01116.
[60] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, và Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.
[61] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, và Neil Houlsby. From sparse to soft mixtures of experts. arXiv preprint arXiv:2308.00951, 2023.
[62] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: memory optimizations toward training trillion parameter models. Trong Christine Cuicchi, Irene Qualters, và William T. Kramer, editors, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, trang 20. IEEE/ACM, 2020.
[63] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, và Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale. Trong Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, và Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 của Proceedings of Machine Learning Research, trang 18332–18346. PMLR, 2022.
[64] Prajit Ramachandran, Barret Zoph, và Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.
[65] Michael Santacroce, Zixin Wen, Yelong Shen, và Yuanzhi Li. What matters in the structured pruning of generative language models? CoRR, 2023.
[66] Noam Shazeer. Glu variants improve transformer, 2020.
[67] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
[68] Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, và Jiang Bian. A study on relu and softmax in transformer. CoRR, 2023.
[69] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, và Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single GPU. Trong Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, và Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 của Proceedings of Machine Learning Research, trang 31094–31116. PMLR, 2023.
[70] Zhao Song, Lichen Zhang, và Ruizhe Zhang. Training multi-layer over-parametrized neural network in sub-quadratic time, 2021.
[71] Mingjie Sun, Zhuang Liu, Anna Bair, và J. Zico Kolter. A simple and effective pruning approach for large language models. CoRR, 2023.

--- TRANG 14 ---
BẢNTHẢO

[72] NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, và Jeff Wang. No language left behind: Scaling human-centered machine translation, 2022.
[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.
[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, và Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, trang 5998–6008, 2017.
[75] Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, và Simon Kornblith. Replacing softmax with relu in vision transformers, 2023.
[76] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, và Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. Trong Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, và Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 của Proceedings of Machine Learning Research, trang 38087–38099. PMLR, 2023.
[77] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, và Mengwei Xu. Edgemoe: Fast on-device inference of moe-based large language models. arXiv preprint arXiv:2308.14352, 2023.
[78] Biao Zhang và Rico Sennrich. Root mean square layer normalization. Trong Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, và Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, trang 12360–12371, 2019.
[79] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, và Bohan Zhuang. Pruning meets low-rank parameter-efficient fine-tuning. CoRR, 2023.
[80] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068, 2022.
[81] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, và Jie Zhou. Moefication: Transformer feed-forward layers are mixtures of experts. Trong Smaranda Muresan, Preslav Nakov, và Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, trang 877–890. Association for Computational Linguistics, 2022.
[82] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, và Weiping Wang. A survey on model compression for large language models. CoRR, 2023.
[83] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, và William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022.

--- TRANG 15 ---
BẢNTHẢO

Phụ lục

A Các Công Trình Liên Quan Mở Rộng

Hàm Kích Hoạt. ReLU, được giới thiệu bởi [22], vẫn là hàm kích hoạt chủ đạo cho mạng neural sâu và được sử dụng đáng chú ý trong công trình transformer gốc [74]. SwiGLU [66] đã được chứng minh là nâng cao hiệu suất khi thay thế ReLU trong các lớp feedforward và là một tính năng trong các mô hình như Llama [73]. Narang et al. [57] đã tiến hành so sánh rộng rãi nhiều hàm kích hoạt khác nhau, như GeLU, SiLU [28, 17], ELU [10], SeLU [42], và các biến thể GLU [11], xác định một số lợi thế so với ReLU. Bài báo và kết quả của chúng tôi khác với họ bằng cách huấn luyện các mô hình và dữ liệu quy mô tỷ trái ngược với quy mô nhỏ hơn của họ. Hơn nữa, kết quả của chúng tôi chỉ ra rằng việc huấn luyện mở rộng có thể giảm khoảng cách hiệu suất giữa ReLU và những hàm khác này, cũng dẫn đến tiết kiệm chi phí tính toán.

Tính Thưa Thớt Kích Hoạt. Một tập hợp nghiên cứu trước đây [44, 25, 70] đã chứng minh rằng tăng tính thưa thớt có thể dẫn đến giảm cả thời gian suy luận và huấn luyện. Dejavu [51] và [47] quan sát tính thưa thớt rõ rệt trong kích hoạt khi sử dụng hàm ReLU trong các lớp feedforward. Những nghiên cứu này đề xuất rằng việc dự đoán tính thưa thớt này có thể tăng cường thêm tốc độ suy luận. Tương tự, [36] sử dụng kích hoạt ReLU và giới thiệu một bộ điều khiển để tích cực thúc đẩy tính thưa thớt. Đáng chú ý, những nghiên cứu này chủ yếu tập trung vào các mạng sử dụng kích hoạt ReLU, bỏ qua những mạng có hàm kích hoạt thay thế. Ngược lại, phương pháp của chúng tôi sửa đổi mạng bằng cách thay thế các hàm kích hoạt khác bằng ReLU. Sau đó chúng tôi tinh chỉnh những mạng này để đạt được tính thưa thớt kích hoạt trong lớp MLP sau ReLUfication. Chúng tôi minh họa thêm rằng việc chèn ReLU trước các lớp QKV và Feedforward có thể giảm đáng kể FLOPS, mặc dù với chi phí nhỏ về độ chính xác. Không giống như các nghiên cứu được đề cập ở trên, chúng tôi không sử dụng bộ dự đoán tính thưa thớt để giảm thiểu FLOPS.

ReLU trong Cơ Chế Chú Ý. Ngoài hàm kích hoạt trong các MLP của mô hình ngôn ngữ lớn, một kích hoạt softmax thường được sử dụng trong mô-đun chú ý. Các nghiên cứu trước đây đã chỉ ra rằng có thể thay thế softmax này bằng ReLU mà không làm tổn hại độ chính xác [75, 68, 32]. Hướng nghiên cứu này khác biệt với phương pháp Relufication của chúng tôi, tập trung cụ thể vào các kích hoạt đi trước phép nhân trọng số.

Nén mô hình cho suy luận hiệu quả Lượng tử hóa, cắt tỉa và chưng cất là ba kỹ thuật chính để nén mạng neural [82]. Lượng tử hóa đã được sử dụng để giảm kích thước mô hình và suy luận nhanh hơn [13, 50, 58, 12, 49, 45, 14, 39, 7, 76]. Mô hình lượng tử hóa chiếm ít không gian hơn giảm độ trễ bộ nhớ [21, 38]. Reluification trực giao với lượng tử hóa và giảm lượng bộ nhớ cần được tải và có thể giảm thêm độ trễ bộ nhớ. Chưng cất [33, 30, 24, 55, 1] là một kỹ thuật khác để huấn luyện các mô hình nhỏ hơn. Điều này trực giao với việc sử dụng kích hoạt ReLU vì bất kỳ kích hoạt nào cũng có thể được sử dụng trong các phương pháp chưng cất. Thưa thớt hóa hoặc cắt tỉa trọng số của mạng neural [20, 35, 79, 71, 65, 53] có thể giảm tính toán và thời gian suy luận. Tính thưa thớt trọng số thường không có cấu trúc và khó triển khai cho phần cứng, nhưng tính thưa thớt được gây ra bởi ReLU có thể dễ dàng được triển khai như một phép nhân ma trận của các hàng khác không. Các mô hình thưa thớt trọng số có thể được kết hợp với relufication của chúng tôi để giảm thêm tính toán.

Hỗn Hợp Chuyên Gia. Các LLM Hỗn Hợp Chuyên Gia (MoE) thường chia nhỏ lớp feed-forward thành nhiều chuyên gia. Một bộ định tuyến sau đó được sử dụng để chọn lọc và thưa thớt kích hoạt những chuyên gia này [67, 19, 72]. Tương tự như công trình của chúng tôi, MoE là một dạng tính thưa thớt kích hoạt nhưng ở dạng nhóm và có thể được xem như một tập con của kích hoạt thưa thớt. Các nghiên cứu tiếp theo đã tinh chỉnh thêm các phương pháp suy luận và huấn luyện cho các mô hình MoE [61, 34, 77, 16, 43, 63, 83, 8, 26]. MoE cũng có thể được kết hợp với Relufication, có tính thưa thớt bên trong FFN của mỗi chuyên gia.

Một dòng công trình khác là MoEfication của các mạng có kích hoạt thưa thớt bằng cách chia nhỏ neuron [81]. Relufication cũng có thể giúp MoEfication áp dụng cho một phạm vi rộng hơn các mạng bằng cách tăng tính thưa thớt của FFN. Để đánh giá sâu hơn về các mô hình hỗn hợp chuyên gia, chúng tôi giới thiệu độc giả đến [18].

Giải Mã Suy Đoán và Tính Thưa Thớt. Giải mã suy đoán là một phương pháp nhằm cải thiện độ trễ mô hình khi đối mặt với các ràng buộc băng thông bộ nhớ [46, 41]. Nó bao gồm việc sử dụng một mô hình nhỏ hơn để dự đoán các token tiếp theo, với một mô hình lớn hơn sau đó xác minh nhiều token trong một phép toán duy nhất. Trong công trình này, chúng tôi kiểm tra các tác động trực tiếp của việc kết hợp tính thưa thớt vào giải mã suy đoán. Chúng tôi cho thấy rằng việc thêm tính thưa thớt có thể dẫn đến cải thiện hiệu suất trong giải mã suy đoán. Thêm vào đó, chúng tôi cung cấp hướng dẫn về việc lựa chọn tham số cho giải mã suy đoán khi tính thưa thớt được giới thiệu.

B Thảo luận về Tính Thưa Thớt Kích Hoạt và Hiệu Quả Suy Luận

Động lực chính đằng sau công trình của chúng tôi là nâng cao hiệu quả, và chúng tôi tin rằng việc cung cấp định nghĩa chính xác của thuật ngữ này là cần thiết. Trong suốt văn bản chính, chúng tôi chủ yếu sử dụng FLOPS như thước đo hiệu quả của chúng tôi. Trong mục này, chúng tôi lập luận tại sao, trong sự hiện diện của tính thưa thớt kích hoạt, FLOPS có thể phục vụ như một đại diện phù hợp để đo các chỉ số hiệu quả khác nhau.

Đầu tiên, quan trọng là phải được nhắc nhở về hai yếu tố chính góp phần vào hiệu quả của Các Mô Hình Ngôn Ngữ Lớn (LLM): (1) tổng lượng tính toán và (2) truyền input/output (IO)—tức là, chuyển tham số từ RAM đến CPU/GPU để tính toán. Đáng chú ý, đối với các mô hình lớn ngày nay, yếu tố (2) hoạt động như nút thắt cổ chai chính trong giai đoạn suy luận. Chúng tôi giới thiệu độc giả đến phân tích chi tiết của Liu et al. [51]. Cuối cùng, đối với một thiết bị đích cụ thể và giả định một triển khai hiệu quả, chúng tôi tin rằng thước đo thực tế nhất của hiệu quả là độ trễ (ví dụ, thời gian trung bình để tạo một token). Tuy nhiên, mỗi thiết bị sở hữu các tính chất độc đáo riêng, đòi hỏi một thước đo đại diện phổ quát hơn.

T*T*=
(a) phép nhân vector thưa thớt, ma trận dày đặc: bằng cách bỏ qua các hàng, chúng ta giảm cả việc truyền trọng số (tức là, tải những hàng này để tính toán) và tính toán (tức là, kết quả sẽ bằng không).

10 20 30 40 50 60 70 80 90 100
Tính Thưa Thớt Kích Hoạt (%)255075100125150175200Độ Trễ (ms)
Phương pháp
Thực tế
Xấp Xỉ FLOPS(b) So sánh FLOPS với độ trễ thực tế cho mô hình OPT (FFN).

Hình 9: Đối với LLM có kích hoạt thưa thớt, FLOPS là một xấp xỉ tốt của độ trễ thực tế.

Chúng tôi lập luận rằng cách chúng tôi tính FLOPS trong bài báo của chúng tôi và bị ảnh hưởng mạnh mẽ bởi tính thưa thớt kích hoạt có thể xấp xỉ hợp lý hiệu quả. Đây là lý do của chúng tôi:

•Giảm Tính Toán: Như được hiển thị trong Hình 9a, với tính thưa thớt kích hoạt, chúng ta có phép nhân vector thưa thớt-ma trận dày đặc tại suy luận, trong khi điều này sẽ là phép nhân ma trận thưa thớt-ma trận dày đặc trong quá trình huấn luyện. Quan trọng là lưu ý rằng đây là tính thưa thớt bán cấu trúc (không giống như cắt tỉa trọng số theo độ lớn), và chúng ta có thể tận dụng thực tế rằng chúng ta đang truyền trọng số trong các khối lớn (tức là, hàng). Các bộ tăng tốc hiện đại đã hỗ trợ các phép toán thưa thớt³ và chúng ta có thể xây dựng trên những công cụ hiện có này.

•Giảm Truyền IO: Trong quá trình suy luận, trọng số cần được chuyển đến bộ nhớ đệm thiết bị để tính toán (ví dụ, từ RAM đến bộ nhớ đệm CPU hoặc GPU VRAM đến bộ nhớ đệm GPU). Bước này tạo thành nút thắt cổ chai chính trong quá trình tạo token. Ví dụ, khoảng 99.3% tổng độ trễ được quy cho IO, như được chỉ ra bởi Liu et al. [51]. Tuy nhiên, bằng cách lưu trữ ma trận theo thứ tự hàng-major, chúng ta có thể bỏ qua việc tải các hàng không cần thiết vì đầu ra sẽ bằng không.

Nhìn chung, như được mô tả trong Hình 9b dựa trên các tính toán của Liu et al. [51], chúng tôi chứng minh rằng đối với mô hình OPT trên nút NVIDIA A100, việc đếm FLOPS cung cấp một xấp xỉ hợp lý và có tương quan cao với thời gian cần thiết để tạo token, đặc biệt, đối với LLM với tính thưa thớt kích hoạt.

C Tính Thưa Thớt Kích Hoạt và Giải Mã Suy Đoán

Giải mã suy đoán [46] là một kỹ thuật sử dụng một mô hình nhỏ hơn Mq để đề xuất γ token và một mô hình lớn hơn Mp để xác minh những token đó và chọn những token phù hợp. Kỹ thuật này cải thiện thời gian chạy của mô hình bằng cách tránh chạy Mp tuần tự cho tất cả token. Để cải thiện thêm suy luận giải mã suy đoán, chúng ta có thể tận dụng tính thưa thớt như sau.

Mô hình độ trễ. Chúng tôi giả định một mô hình khái niệm đơn giản cho độ trễ trong giải mã suy đoán. Theo Deja Vu [51] độ trễ có thể được chia thành độ trễ tính toán và I/O. Độ trễ tính toán không đáng kể so với I/O. Cũng lưu ý rằng Giải mã suy đoán được dành cho các ràng buộc mà băng thông bộ nhớ là nút thắt cổ chai. Do đó chúng tôi chỉ so sánh độ trễ I/O giữa các mô hình thưa thớt và không thưa thớt. Nếu tính thưa thớt tổng hợp trung bình của Mp cho γ token là s̄agg(γ), và thời gian chạy của Mp là T, chúng tôi xấp xỉ độ trễ chạy Mp cho γ token liên tiếp bằng (1−s̄agg(γ))T. Như được thảo luận trong mục trước, đây là một xấp xỉ tốt của độ trễ thực tế.

Cải thiện độ trễ lý thuyết. Giả sử mô hình nhỏ hơn Mq hoạt động nhanh hơn c lần so với mô hình cồng kềnh Mp. Theo văn bản chính, việc chấp nhận token được giả định tuân theo hành vi độc lập và phân phối đồng nhất (i.i.d.). Ký hiệu α là xác suất dự kiến của một token được tạo bởi Mq được phê duyệt bởi Mp. Các định lý sau đây đúng:

Định lý 1. Yếu tố cải thiện dự kiến về độ trễ cho giải mã suy đoán với tính thưa thớt, so với giải mã suy đoán chuẩn là cγ+1 / cγ+(1−s̄agg(γ)).

Chứng minh. Lượng thời gian cần thiết để chạy mô hình thưa thớt hóa được định lượng là Tcγ + (1−s̄agg(γ))T. Đó là thời gian chạy một mô hình nhỏ hơn cộng với phần không thưa thớt của mô hình lớn hơn. Thời gian chạy của giải mã suy đoán không có tính thưa thớt là Tcγ+T. Số lượng token được tạo trong cả hai trường hợp là như nhau. Do đó tăng tốc tương đối của việc sử dụng tính thưa thớt được cho bởi: Tcγ+T / Tcγ+(1−s̄agg(γ))T.

Định lý 2. Yếu tố cải thiện dự kiến về độ trễ, khi kết hợp tính thưa thớt với giải mã suy đoán chống lại giải mã bình thường (tự hồi quy) chỉ sử dụng Mp, là 1−αγ+1 / (cγ+(1−s̄agg(γ)))(1−α).

Chứng minh. Tương tự như định lý trên Tcγ + (1−s̄agg(γ))T cho thời gian cần thiết cho giải mã suy đoán thưa thớt. Theo bài báo gốc, giải mã suy đoán chuẩn mang lại trung bình 1−αγ+1 / 1−α token được tạo ra mỗi lần chạy [46]. Do đó, thời gian chạy dự đoán khi tạo token với tính thưa thớt trong giải mã suy đoán trở thành (cγ+(1−s̄agg(γ)))(1−α) / 1−αγ+1 T. Cho thời gian chạy để tạo ra một token duy nhất qua phương pháp tự hồi quy là T, nghịch đảo của phân số này cho kết quả mong muốn.

γ tối ưu. γ tối ưu cho giải mã suy đoán có thể được tìm thấy bằng cách tối ưu hóa phương trình yếu tố tăng tốc trong Định lý 2. Khi tính thưa thớt không có mặt, phương trình có thể được giải số, nhưng đối với các mạng relufied, tính thưa thớt tổng hợp cho các γ khác nhau sẽ ảnh hưởng đến kết quả cuối cùng. Chúng tôi đã tìm thấy γ tối ưu dựa trên s̄agg(γ) cho mô hình OPT 6.7B và trình bày kết quả trong hình 10a. γ được chọn cho giải mã suy đoán thưa thớt nhỏ hơn so với giải mã suy đoán chuẩn vì γ cao hơn sẽ dẫn đến ít tính thưa thớt hơn. Khoảng cách trong γ luôn ít hơn 20%. Cũng trong hình 10b, có thể thấy cho trường hợp cụ thể của α = 0.8, c = 0.02, giải mã suy đoán thưa thớt có yếu tố tăng tốc cao nhất so với tự hồi quy tại γ = 10 s so với điểm tối ưu của phiên bản chuẩn xảy ra cho γ = 12. Giải mã suy đoán thưa thớt tại γ = 12 tốt hơn giải mã suy đoán chuẩn tại γ = 12, trong khi giải mã suy đoán thưa thớt tại γ = 10 đánh bại cả hai. Một quan sát khác từ 10b là cho trường hợp tính thưa thớt hoàn toàn ngẫu nhiên, lợi ích của giải mã suy đoán thưa thớt sẽ giảm so với giải mã suy đoán chuẩn ở γ cao hơn. Ngược lại, lợi ích của tính thưa thớt tổng hợp sẽ kéo dài cho các giá trị γ lớn hơn.

0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90
α510152025γ Tối Ưuc=0.01 Suy Đoán Thưa Thớt
c=0.01 Suy Đoán Chuẩn
c=0.05 Suy Đoán Thưa Thớt
c=0.05 Suy Đoán Chuẩn
(a)

0 10 20 30 40 50
γ1.01.52.02.53.03.54.04.55.0Tăng Tốc
α= 0.8
Tính Thưa Thớt Tổng Hợp
Tính Thưa Thớt Ngẫu Nhiên
Giải Mã Suy Đoán Chuẩn (b)

Hình 10: (a) γ tối ưu cho giải mã suy đoán thưa thớt (b) tăng tốc của giải mã suy đoán thưa thớt và giải mã suy đoán chuẩn so với giải mã tự hồi quy khi α = 0.8 và c = 0.02

--- TRANG 17 ---
BẢNTHẢO

D Phân phối Kích hoạt Trước của các mô hình OPT được huấn luyện từ đầu

Phân phối của các đầu vào kích hoạt trước được nghi ngờ là yếu tố chính quyết định lượng tính thưa thớt. Như chúng ta đã thấy trong Mục 4.1, phân phối kích hoạt trước cho Llama và Falcon khác nhau rất nhiều. Người ta có thể tự hỏi rằng nếu chúng ta kiểm soát dữ liệu huấn luyện và thuật toán tối ưu, liệu hình dạng của phân phối có khác nhau không? Để đạt được mục tiêu này, chúng tôi huấn luyện các mô hình OPT 1.3B từ đầu sử dụng bốn biến thể của hàm kích hoạt và mô tả phân phối kích hoạt trước dọc theo quá trình huấn luyện trong Hình 11. Chúng bắt đầu từ cùng một hình nhưng dần dần phân kỳ. Từ SiLU đến ReLU (tăng β), phân phối kích hoạt trước trở nên tập trung hơn xung quanh 0 và sẽ gần như đơn modal.

Các cuộc điều tra sâu hơn về động lực của kích hoạt trước và sau và mối quan hệ của chúng với hiệu quả và độ chính xác được để lại như một hướng nghiên cứu tương lai hấp dẫn.

−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β→∞ (ReLU)khởi tạo
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β→∞ (ReLU)25B
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β→∞ (ReLU)50B
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β→∞ (ReLU)100B
181624
Lớp
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 8
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 8
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 8
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 8
181624
Lớp
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β≈1.7(GELU)
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β≈1.7(GELU)
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β≈1.7(GELU)
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β≈1.7(GELU)
181624
Lớp
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 1 (SiLU)
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 1 (SiLU)
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 1 (SiLU)
−10 0 10
Giá Trị Kích Hoạt Trước01020304050Tỷ Lệ (%)β= 1 (SiLU)
181624
Lớp

Hình 11: Phân phối kích hoạt trước của nhiều mô hình OPT 1.3B với tất cả bốn loại kích hoạt được huấn luyện từ đầu tại nhiều số lượng token đã thấy khác nhau trong quá trình huấn luyện.

E Mô hình lớn thưa thớt hóa có tốt hơn mô hình nhỏ dày đặc không?

Khi triển khai các mô hình hiệu quả, người ta có thể tự nhiên sử dụng một mô hình kích thước nhỏ hơn (dày đặc) gốc. Lập luận sẽ là hiệu suất của mô hình lớn hơn relufied có thể đã bằng hoặc ít hơn mô hình nhỏ hơn dày đặc. Để nghiên cứu câu hỏi trên, chúng tôi đã vẽ biểu đồ hiệu suất so với hiệu quả của các mô hình OPT gốc và relufied trong Hình 12. Lấy mô hình OPT 6.7B relufied làm ví dụ, nó hoạt động ở 2.8 GFLPOPs mỗi token. Nội suy đường màu xanh (có thể được xem như một biểu đồ mở rộng của mô hình OPT), một mô hình dày đặc với FLOPS tương đương giảm hơn 2% trong hiệu suất zero-shot.

Tương tự, so với mô hình OPT 2.7B relufied, mô hình dày đặc tương đương (về FLOPS) hoạt động thấp hơn gần 2%. Thật vậy, thực tế rằng các mô hình relufied nằm cao hơn biểu đồ mở rộng của các mô hình OPT gốc, cho thấy hiệu quả của các quy trình relufication như một phương pháp để có được các mô hình tốt hơn nhưng hiệu quả hơn. Như một lợi ích phụ, nó làm cho phổ hiệu quả của các LLM có sẵn liên tục hơn. Ví dụ, xem xét sự kết hợp của phần cứng và trường hợp sử dụng chỉ cho phép triển khai LLM với thấp hơn 3 GFLOPS trong quá trình suy luận. Đi với các mô hình được huấn luyện trước chuẩn, lựa chọn duy nhất có sẵn là OPT 2.7B với gần 1 GFLOPS, vì 6.7B không đáp ứng ràng buộc phần cứng. Trong tình huống này, mô hình relufied của chúng tôi không chỉ nằm trong ngân sách suy luận hạn chế mà còn rất gần với mô hình lớn nhất có sẵn tiếp theo về độ chính xác. Một hướng nghiên cứu tương lai thú vị và kịp thời là tìm các phương pháp, mà, cho một LLM (hoặc một họ LLM), có thể tạo ra mô hình hoạt động tốt nhất phù hợp với ngân sách tính toán suy luận được chỉ định.

1.0 1.8 2.8 4.5
GFLOPS Mỗi Token50.554.059.0Độ Chính Xác 0-shot Trung Bình
OPT 1.3BOPT 2.7B
OPT 2.7B (giai đoạn-1)OPT 6.7B
OPT 6.7B (giai đoạn-2)

Hình 12: Hiệu suất của các mô hình lớn thưa thớt so với các mô hình nhỏ dày đặc: Các mô hình lớn relufied (sao đỏ) nằm trên đường cong mở rộng của các mô hình dày đặc gốc (hình tròn xanh và đường nét đứt).
