# 2404.17651.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/activation/2404.17651.pdf
# Kích thước tệp: 587686 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024
HARD ASH: ĐỘ THƯA VÀ BỘ TỐI ỨU PHÙ HỢP
TẠO NƯỚC HỌC LIÊN TỤC
Santtu Keskinen
Không liên kết
santtu.keskinen@gmail.com
TÓM TẮT
Trong học tăng cường lớp, mạng nơ-ron thường gặp phải vấn đề quên thảm khốc.
Chúng tôi chỉ ra rằng một MLP có hàm kích hoạt thưa và một bộ tối ưu tốc độ
học thích ứng có thể cạnh tranh với các kỹ thuật chính quy hóa đã được thiết lập
trong tác vụ Split-MNIST. Chúng tôi nêu bật hiệu quả của hàm kích hoạt Adaptive
SwisH (ASH) trong bối cảnh này và giới thiệu một biến thể mới, Hard Adaptive
SwisH (Hard ASH) để tăng cường khả năng giữ lại học tập.

1 GIỚI THIỆU
Học liên tục đặt ra một thách thức độc đáo cho mạng nơ-ron nhân tạo, đặc biệt trong
cài đặt tăng cường lớp (Hsu et al., 2019), nơi một mạng duy nhất phải nhớ các lớp
cũ đã rời khỏi tập huấn luyện. Trong bài báo này, tôi khám phá một cách tiếp cận
bị bỏ qua không yêu cầu bất kỳ kỹ thuật nào được phát triển đặc biệt cho học liên tục.
Đối với chính quy hóa, tôi chỉ sử dụng các bộ tối ưu được điều chỉnh cẩn thận với
tốc độ học thích ứng như Adagrad (Duchi et al., 2011). Cách tiếp cận này không
khai thác cấu trúc tác vụ theo bất kỳ cách nào. Điều này trái ngược với hầu hết các
phương pháp học liên tục chính quy hóa yêu cầu ranh giới tác vụ rõ ràng như EWC
(Kirkpatrick et al., 2017) và MAS (Aljundi et al., 2018) hoặc ranh giới tác vụ ngầm
như Online EWC (Schwarz et al., 2018).

Có lẽ gần nhất với phương pháp của tôi là Elephant MLP (Lan & Mahmood, 2023)
và SDMLP (Bricken et al., 2023), nhưng các kết quả ở đây vượt trội hơn cả hai
trong Split-MNIST với một phương pháp đơn giản hơn có thể nói.

Các biểu diễn thưa đã được chỉ ra là hiệu quả trong việc giảm quên trong mạng
nơ-ron (Srivastava et al., 2013; Shen et al., 2021; Ahmad & Scheinkman, 2019;
Lan & Mahmood, 2023). Tôi tiếp tục mô hình này và chỉ ra rằng việc kết hợp độ
thưa với bộ tối ưu tốc độ học thích ứng là đủ để tạo ra một người học liên tục
đơn giản về mặt khái niệm nhưng hiệu quả đáng ngạc nhiên.

Để làm cho các biểu diễn lớp ẩn MLP của tôi thưa, tôi đã sử dụng một hàm kích
hoạt làm cho phần lớn các kích hoạt bằng không. Top-K (còn được gọi là k-WTA)
là hàm kích hoạt thưa đơn giản nhất về mặt khái niệm và việc sử dụng nó trong
mạng nơ-ron có từ ít nhất Makhzani & Frey (2014). Tôi chỉ ra rằng Top-K hoạt
động tốt trong thiết lập của tôi, nhưng tôi có được độ chính xác tốt hơn với kích
hoạt Hard Adaptive Swish (Hard ASH) mới của tôi.

2 ASH VÀ HARD ASH
Hàm kích hoạt Adaptive SwisH (ASH) (Lee et al., 2022), đã giới thiệu một cách
mới để kiểm soát lượng độ thưa của các kích hoạt, rẻ hơn để tính toán so với hàm
Top-K. Đây là nghiên cứu đầu tiên sử dụng ASH cho học liên tục. Công thức cho
ASH mà tôi sử dụng là:

ASH (xi) =xi·S(α·(xi−µX−zk·σX)),X= [x1, x2, . . . , x n]

S là hàm sigmoid, µX và σX là trung bình và độ lệch chuẩn của vector X, α là một
siêu tham số kiểm soát độ dốc của sigmoid và zk là một siêu tham số kiểm soát
lượng độ thưa. Giá trị zk cao hơn tương ứng với độ thưa nhiều hơn trong các kích hoạt.

2.1 HARD ASH
Lan & Mahmood (2023) đã lý thuyết hóa rằng các hàm kích hoạt nên phù hợp hơn
cho học liên tục nếu gradient của chúng khá thưa, tức là hàm kích hoạt nên phẳng
ở hầu hết các vị trí. Để giảm dòng gradient, tôi đã thay thế hàm sigmoid bằng một
hard sigmoid (Courbariaux et al., 2016) và cắt term xi đầu tiên thành các giá trị
giữa 0 và 2. Xem phụ lục A.2 cho công thức Hard ASH chính xác.

1arXiv:2404.17651v1  [cs.LG]  26 Apr 2024

--- TRANG 2 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

Hình 1: Độ chính xác xác thực tổng thể và theo từng tác vụ của một lần chạy đơn
lẻ của mỗi phương pháp. Các đường thẳng đứng biểu thị các điểm trong quá trình
huấn luyện nơi tác vụ thay đổi. Bộ tối ưu là Adagrad khi không được chỉ định.
Các phương pháp tốt nhất từ từ mất độ chính xác trên các tác vụ cũ, nhưng gặp
khó khăn để học tác vụ cuối cùng. ReLU quên các tác vụ cũ ngay cả với bộ tối
ưu tốt như Adagrad. Trong khi đó Hard ASH giữ được một số hiệu suất tác vụ
cũ ngay cả với SGD đơn giản. Sự biến thiên giữa các lần chạy đủ nhỏ để gần như
không nhìn thấy.

3 THỰC NGHIỆM
Tôi đã chạy một thực nghiệm trên tập dữ liệu Split-MNIST 5 tác vụ trong cài đặt
tăng cường lớp, cài đặt khó nhất của Split-MNIST, nơi một mạng duy nhất phải
học tất cả các tác vụ mà không có đầu vào ID tác vụ (Hsu et al., 2019). Để tạo ra
5 tác vụ, các chữ số MNIST được chia thành 5 tập của 2 lớp mỗi tập và các tác vụ
được huấn luyện lần lượt mà không có replay của các đầu vào trước đó. Kiến trúc
mạng là một MLP đơn giản với một lớp ẩn gồm 1000 nơ-ron, như trong Bricken
et al. (2023). Tôi huấn luyện mỗi mạng chỉ trong 1 epoch để tiết kiệm tính toán.

Mục tiêu của thực nghiệm là kiểm tra hiệu quả của các hàm kích hoạt thưa khác
nhau với các bộ tối ưu tiêu chuẩn được điều chỉnh cho học liên tục. Đối với mỗi
cặp (hàm kích hoạt, bộ tối ưu) được kiểm tra, tôi đã chạy một quét siêu tham số
để tìm độ chính xác trung bình cuối cùng tốt nhất. Trước khi chạy quét, tôi đã
ước tính thủ công các siêu tham số tốt nhất, được liệt kê trong A.6. Được hiển thị
trong bảng 1 là kết quả tốt nhất cho các hàm kích hoạt chính của nghiên cứu.
Để có bảng kết quả đầy đủ, xem A.5.

Tất cả các hàm kích hoạt thưa đều hoạt động tốt hơn tất cả các hàm không thưa.
Hard ASH là tốt nhất trong hầu hết mọi thiết lập bộ tối ưu, tiếp theo là Top-K
và ASH. Adagrad hoạt động tốt nhất trong số các bộ tối ưu được kiểm tra, tiếp
theo là RMSprop (Tieleman & Hinton, 2012) và Adam Kingma & Ba (2014).
Tôi cũng đã chạy SGD và SGDM để so sánh, nhưng cả hai đều có độ chính xác
thấp hơn các bộ tối ưu với tốc độ học thích ứng, theo từng tham số.

Bảng 1: Các hàm kích hoạt với hiệu suất tốt nhất trên các bộ tối ưu được kiểm tra.
Trung bình của 5 lần chạy và 95% C.I. Kết quả cơ sở EWC (Kirkpatrick et al., 2017),
FlyModel (Shen et al., 2021) và SDMLP+EWC từ Bricken et al. (2023) và cũng
sử dụng một MLP với một lớp ẩn 1000 nơ-ron đơn lẻ.

Kích hoạt / Phương pháp Epochs Độ chính xác trung bình Bộ tối ưu tốt nhất
ASH 1 76.4% (±1.4%) Adagrad
Hard ASH 1 78.3% (±1.4%) Adagrad
Top-K 1 76.0% (±1.6%) Adagrad
ReLU 1 49.2% (±7.9%) Adam
EWC 500 61% SGD
SDMLP 500 69% SGD
SDMLP+EWC 500 83% SGDM
FlyModel 1 77% Học tập quy tắc liên kết

4 KẾT LUẬN
Nghiên cứu này thách thức các cách tiếp cận thông thường đối với học liên tục bằng
cách chứng minh rằng chúng ta có thể có được kết quả khá tốt trong Split-MNIST
ngay cả khi không có bất kỳ thuật toán học liên tục hoặc thông tin liên quan đến
tác vụ nào. Hard ASH đã hoạt động rất tốt trong các thực nghiệm của tôi. Tôi đề
xuất thử nó như một thay thế tính toán nhanh hơn cho Top-K, có thể cũng tăng
độ chính xác.

Việc chọn bộ tối ưu tốt nhất là một vấn đề khó với lượng lựa chọn phải chọn từ
đó và lượng điều chỉnh siêu tham số cần thiết cho hiệu suất tốt. Chỉ riêng Schmidt
et al. (2021) đã liệt kê hơn 100 thuật toán đã biết, mỗi thuật toán có thể được
ghép nối tùy chọn với các lịch trình tốc độ học khác nhau. Hiện tại, tôi khuyến
nghị Adagrad như một mặc định tương đối dễ điều chỉnh cho học liên tục.
Phụ lục A.8 và A.9 cung cấp thêm hiểu biết về hiệu suất bộ tối ưu.

2

--- TRANG 3 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

5 TUYÊN BỐ URM
Tác giả thừa nhận rằng tại tác giả của công trình này đáp ứng tiêu chí URM của
ICLR 2024 Tiny Papers Track.

TÀI LIỆU THAM KHẢO
Subutai Ahmad and Luiz Scheinkman. How can we be so dense? the benefits of using highly sparse
representations, 2019.

Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget, 2018.

Jordan T. Ash and Ryan P. Adams. On warm-starting neural network training, 2020.

Trenton Bricken, Xander Davies, Deepak Singh, Dmitry Krotov, and Gabriel Kreiman. Sparse
distributed memory is a continual learner, 2023.

Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations, 2016.

Shibhansh Dohare, Richard S. Sutton, and A. Rupam Mahmood. Continual backprop: Stochastic
gradient descent with persistent randomness, 2022.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research , 12(7), 2011.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification, 2015.

Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learning
scenarios: A categorization and case for strong baselines, 2019.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
CoRR , abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID:
6628106 .

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences , 114(13):
3521–3526, March 2017. ISSN 1091-6490. doi: 10.1073/pnas.1611835114. URL http:
//dx.doi.org/10.1073/pnas.1611835114 .

Qingfeng Lan and A. Rupam Mahmood. Elephant neural networks: Born to be a continual learner,
2023.

Kyungsu Lee, Jaeseung Yang, Haeyun Lee, and Jae Youn Hwang. Stochastic adaptive activation
function, 2022.

Alireza Makhzani and Brendan Frey. k-sparse autoencoders, 2014.

Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, and Alex Graves. A
practical sparse approximation for real time recurrent learning, 2020.

Martial Mermillod, Aur ´elia Bugaiska, and Patrick BONIN. The stability-plasticity dilemma: inves-
tigating the continuum from catastrophic forgetting to age-limited learning effects. Frontiers
in Psychology , 4, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00504. URL https:
//www.frontiersin.org/articles/10.3389/fpsyg.2013.00504 .

Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks, 2016.

3

--- TRANG 4 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

Robin M. Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley -
benchmarking deep learning optimizers, 2021.

Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-
work for continual learning, 2018.

Yang Shen, Sanjoy Dasgupta, and Saket Navlakha. Algorithmic insights on continual learning from
fruit flies, 2021.

Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and J ¨urgen Schmid-
huber. Compete to compute. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems , volume 26. Cur-
ran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/
paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf .

Tijmen Tieleman and Geoffrey Hinton.. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude, 2012.

Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-take-
all, 2019.

A PHỤ LỤC

A.1 KHẢ NĂNG TÁI TẠO
Mã để tái tạo các thực nghiệm trong bài báo này có sẵn công khai tại:
https://github.com/LesserScholar/hard-ash

Đã được cẩn thận để đảm bảo kết quả có thể tái tạo, bao gồm việc sử dụng nhất
quán các khóa JAX và xáo trộn dữ liệu xác định.

A.2 CÔNG THỨC HARD ASH
Công thức tôi sử dụng cho Hard ASH là:

HardSigmoid (x) =clip(x+ 3,0,6)
6

HardASH (xi) =clip(xi,0, xmax)·HardSigmoid (α·(xi−µX−zk·σX))

Trong đó xmax là một siêu tham số mà tôi luôn đặt thành 2. Công thức cho hard
sigmoid là công thức được sử dụng trong JAX (jax.nn.hard sigmoid).

Cùng nhau, clip, hard sigmoid và giá trị đủ cao cho α làm cho hầu hết các kích
hoạt bão hòa (ở 0 hoặc 2) và giảm lượng gradient chảy. Một cách trực quan,
điều này có nghĩa là đối với mỗi ví dụ huấn luyện, tôi chỉ cập nhật các trọng số
đến và đi cho các kích hoạt nơi mạng không chắc chắn liệu xi cụ thể đó có nên
bật hay tắt.

A.3 KHỞI TẠO MẠNG
Trong tất cả các thử nghiệm của tôi, tôi đã sử dụng khởi tạo Kaiming tiêu chuẩn
(He et al., 2015) cho cả hai lớp của MLP. Có khả năng có những cách hiệu quả
hơn để khởi tạo mạng Hard ASH tính đến độ thưa trong các kích hoạt, nhưng
những khám phá đó không được bao gồm trong nghiên cứu này.

A.4 CHUẨN HÓA TRỌNG SỐ
Tôi đã sử dụng chuẩn hóa trọng số (Salimans & Kingma, 2016) với g cố định
là 1, chỉ trên lớp đầu tiên của MLP. Trong thử nghiệm sơ bộ, chuẩn hóa trọng
số trên lớp đầu tiên nhất quán làm tăng hiệu suất của nhiều phương pháp 1 đến
2 điểm phần trăm. Chuẩn hóa trọng số trên lớp thứ hai, trong thử nghiệm sơ bộ,
hoặc là trung tính hoặc hơi âm.

4

--- TRANG 5 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

A.5 KẾT QUẢ ĐẦY ĐỦ
Bảng 2 và 3 cho thấy tất cả các phương pháp và cặp phương pháp đã cạnh tranh
với nhau như thế nào.

Đáng chú ý là tất cả các hàm kích hoạt thưa đều giữ được một phần khá tốt
hiệu suất của chúng ngay cả với SGD cơ bản.

Trong kết quả đầy đủ, tôi đã thử hai phiên bản khác nhau của Top-K, subtract
và mask. Sự khác biệt giữa hai phương pháp là trong Top-K subtract, giá trị cao
thứ k được trừ khỏi các kích hoạt trước khi masking. Trong văn bản chính chỉ
Top-K subtract được sử dụng, vì nó hoạt động tốt hơn. Bricken et al. (2023)
cũng thấy Top-K subtract hoạt động tốt hơn Top-K mask.

Tôi cũng đã thử nghiệm LWTA (Srivastava et al., 2013; Xiao et al., 2019) đã
được đề xuất như một thay thế tính toán nhanh hơn cho Top-K, nhưng thấy hiệu
suất của nó tệ hơn Top-K subtract hoặc các hàm dựa trên ASH.

Bảng 2: Kết quả tốt nhất cho mỗi bộ tối ưu. Trung bình của 5 lần chạy và 95% C.I.

Bộ tối ưu Độ chính xác trung bình Hàm kích hoạt tốt nhất
Adagrad 78.3% (±1.4%) Hard ASH
RMSprop 77.7% (±1.8%) Hard ASH
Adam 71.6% (±1.6%) Hard ASH
SGDM 66.3% (±3.4%) Top-K Subtract
SGD 52.9% (±5.3%) Hard ASH

Bảng 3: Kết quả đầy đủ cho tất cả các hàm kích hoạt được kiểm tra với tất cả
các bộ tối ưu được kiểm tra. Trung bình của 5 lần chạy và 95% C.I. Các kết hợp
hoạt động kém được chấm dứt sớm để tiết kiệm tính toán và do đó có giới hạn
lỗi lớn hơn.

Kích hoạt Adagrad RMSprop Adam SGDM SGD
ASH 76.4 ± 1.4 75.7 ± 1.2 69.5 ± 2.0 65.0 ± 0.4 52.4 ± 8.6
Hard ASH 78.3 ± 1.4 77.7 ± 1.8 71.6 ± 1.6 65.1 ± 1.6 52.9 ± 7.6
Top-K subtract 76.0 ± 1.6 75.0 ± 2.9 71.5 ± 1.4 66.3 ± 3.5 51.5 ± 10.4
Top-K mask 65.0 ± 4.6 69.7 ± 0.8 67.9 ± 2.4 62.9 ± 3.9 44.1 ± 15.3
LWTA 67.2 ± 2.6 67.1 ± 2.0 64.9 ± 2.2 61.3 ± 4.1 39.9 ± 14.4
ReLU 43.8 ± 10.7 39.7 ± 11.7 49.2 ± 9.7 35.7 ± 2.2 19.8 ± 4.0
SwisH 46.2 ± 9.7 41.4 ± 10.1 49.2 ± 9.7 51.9 ± 2.8 26.5 ± 8.3
Sigmoid 52.4 ± 10.3 44.2 ± 9.7 35.3 ± 8.2 20.8 ± 5.0 15.6 ± 1.0
Hard Sigmoid 56.5 ± 1.2 48.1 ± 11.6 32.0 ± 9.2 19.4 ± 1.4 14.7 ± 0.8

A.6 SIÊU THAM SỐ
Bảng 4 và 5 liệt kê các siêu tham số cho mỗi phương pháp được sử dụng trong nghiên cứu.

Các tham số bộ tối ưu được điều chỉnh mạnh mẽ hướng tới hiệu suất tốt hơn
trong Split-MNIST. Đối với các bộ tối ưu momentum, các giá trị momentum được
đặt cao hơn bình thường. Đối với RMSprop, decay được đặt rất cao. Đối với Adagrad,
giá trị điền ban đầu được đặt thấp bất thường làm cho hành vi của nó tương tự
RMSprop ở đầu quá trình huấn luyện, tức là tốc độ học rất cao ở đầu huấn luyện.

Đối với Top-K, các giá trị k tốt nhất là 64 và 96, tương ứng với độ thưa giữa
94% và 90%. Đối với LWTA, lượng nhóm tốt nhất là 25 và 50, tương ứng 97.5%
và 95% thưa. Lee et al. (2022) cho thấy cách tính mật độ ASH dựa trên giá trị
zk. Tôi thấy rằng giá trị zk 2.0-2.5 mang lại kết quả tốt nhất, tương ứng khoảng
97-99% độ thưa.

5

--- TRANG 6 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

Bảng 4: Siêu tham số chung
Siêu tham số Giá trị
Kích thước batch 64
Chuẩn hóa trọng số lớp đầu True
Chuẩn hóa trọng số lớp thứ hai False
Kích thước ẩn 1000
Cắt gradient 0.01

Bảng 5: Siêu tham số đặc biệt cho phương pháp
Phương pháp Siêu tham số Giá trị
Kích hoạt
Ash α 3.0, 4.0
Zk 2.2, 2.3, 2.4
Hard ASH xmax 2.0
Top-K k 32, 64, 96, 128, 256
LWTA nhóm 25, 50, 100
Bộ tối ưu
RMSprop decay 0.998, 0.999, 0.9991, 0.9992, 0.9993
tốc độ học 4e-6, 5e-6, 5.5e-6, 6e-6, 8e-6
Adam β1 0.9, 0.95, 0.98, 0.99
β2 0.999, 0.9995
tốc độ học 8e-6, 1e-5, 1.5e-5
Adagrad tốc độ học 1e-4, 2e-4, 3e-4
giá trị ban đầu 1e-6
SGD tốc độ học 3e-4, 4e-4, 5e-4
SGDM momentum 0.99, 0.992, 0.994, 0.996
tốc độ học 8e-6, 1e-5, 1.5e-5

A.7 HIỆU SUẤT KHÔNG CÓ PHÂN CHIA TÁC VỤ
Các siêu tham số được tối ưu hóa học liên tục được sử dụng trong quét thí nghiệm
chính đã được điều chỉnh chỉ cho tác vụ Split-MNIST. Việc sử dụng tốc độ học
thích ứng cao và các tham số momentum như vậy là rất bất thường. Để đánh giá
mất bao nhiêu hiệu suất khi sử dụng các cài đặt này trong MNIST điển hình mà
không có phân chia tác vụ, tôi đã chạy qua cùng một quét các hàm kích hoạt và
bộ tối ưu nhưng huấn luyện tất cả các lớp đồng thời, tức là với dữ liệu i.i.d.

Bảng 6 cho thấy kết quả khi được huấn luyện trên toàn bộ MNIST cùng một lúc
(tức là với tập dữ liệu i.i.d.). Quét đầu tiên với cùng các siêu tham số được sử
dụng trong thí nghiệm chính, được điều chỉnh cho học liên tục, và thứ hai với
các siêu tham số bộ tối ưu được điều chỉnh cho dữ liệu i.i.d. Trong trường hợp
thứ hai, momentum thấp hơn nhiều và tốc độ học cao hơn, cho phép mô hình
đạt được độ chính xác tốt hơn nhiều trong 1 epoch.

Bảng 6: So sánh kịch bản học liên tục phân chia tác vụ, i.i.d. với bộ tối ưu học
liên tục và i.i.d với bộ tối ưu thông thường. Trung bình của 5 lần chạy và 95% C.I.

Phương pháp Epochs Độ chính xác trung bình
Hard ASH /w phân chia tác vụ và bộ tối ưu Split-MNIST 1 78.3% (±1.4%)
ReLU tập dữ liệu i.i.d. và bộ tối ưu Split-MNIST 1 91.3% (±2.5%)
ReLU tập dữ liệu i.i.d. và bộ tối ưu bình thường 1 96.9% (±3.3%)

6

--- TRANG 7 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

A.8 ADAM VÀ HIỆU CHỈNH BIAS
Trong văn bản chính, tôi tập trung vào đánh giá các bộ tối ưu hiện có và được
nghiên cứu kỹ mà không có sửa đổi. Một trong những kết quả đáng ngạc nhiên
nhất là Adam hoạt động tệ hơn bao nhiêu khi so sánh với các thuật toán RMSprop
và Adagrad rất tương tự. Sau khi kiểm tra, tôi thấy rằng tôi có thể tăng hiệu suất
Adam gần đến mức RMSprop chỉ bằng cách loại bỏ hiệu chỉnh bias khỏi thuật toán.

Hiệu chỉnh bias trong Adam điều chỉnh các ước tính moment thứ nhất và thứ hai
để tính đến việc khởi tạo chúng ở không. Điều này làm cho các ước tính moment
chính xác hơn (tức là không thiên vị) trong các bước tối ưu đầu tiên (Kingma & Ba, 2014).
Không giống như Adam, các triển khai tiêu chuẩn của RMSprop không sử dụng
hiệu chỉnh bias. RMSprop và Adam thiên vị hoạt động tốt hơn đáng kể so với
Adam tiêu chuẩn cho thấy chúng ta rằng tốc độ học ban đầu rất cao được đưa ra
bởi việc lấy trung bình theo cấp số nhân thiên vị là quan trọng đối với độ chính
xác cuối cùng của mạng.

Bảng 7: Kiểm tra hiệu suất Adam với hiệu chỉnh bias được loại bỏ. Trung bình
của 5 lần chạy và 95% C.I.

Bộ tối ưu Độ chính xác trung bình Hàm kích hoạt
Adam với hiệu chỉnh bias 71.6% (±1.5%) Hard ASH
Adam không có hiệu chỉnh bias 76.7% (±1.5%) Hard ASH

A.9 LỊCH TRÌNH TỐC ĐỘ HỌC
Trong thí nghiệm của tôi, các phương pháp tốc độ học thích ứng đã hoạt động
rất tốt, nhưng tôi không kiểm tra lịch trình tốc độ học. Các thí nghiệm học liên
tục thường được thực hiện với tốc độ học không đổi, để duy trì tính dẻo (để biết
thêm về tính dẻo xem A.10). Nhưng sự thành công của Adagrad, RMSprop và
Adam thiên vị cho thấy rằng tốc độ học ban đầu cao là quan trọng. Do đó tôi
cũng đã kiểm tra SGD với lịch trình suy giảm theo cấp số nhân và thấy nó khá
hiệu quả. Tôi có được độ chính xác 72.4% (±3.2%), kết hợp Hard ASH với tốc
độ học suy giảm theo cấp số nhân với suy giảm 0.7 mỗi 200 bước, với tốc độ
học bắt đầu 3.35e−3. Các tham số cho tốc độ học suy giảm theo cấp số nhân khá
nhạy cảm và khó điều chỉnh hơn nhiều so với cái gì đó như Adagrad với tốc độ
học không đổi.

A.10 THÍ NGHIỆM TÍNH DẺO TRÊN PERMUTED MNIST
Mối quan tâm chính trong học liên tục là cái gọi là tiến thoái lưỡng nan ổn định-tính
dẻo Mermillod et al. (2013), làm nổi bật sự căng thẳng giữa việc nhớ các tác vụ
trong quá khứ và học để thực hiện trên các tác vụ mới. Như một ví dụ cực đoan,
sẽ dễ dàng xây dựng một hệ thống học hoàn hảo nhớ các tác vụ đầu và sau đó
ngừng học, chỉ bằng cách đặt tốc độ học gần 0 sau một lượng bước huấn luyện
nhất định. Nhưng điều này sẽ trái với tinh thần của học liên tục vì mạng sẽ hoàn
toàn ngừng hấp thụ kiến thức mới. Vì vậy mục tiêu cuối cùng của học liên tục
là xây dựng một hệ thống học nơi hiệu suất trên các tác vụ cũ ổn định và nó
tiếp tục học các tác vụ mới một cách dễ dàng.

Để đánh giá tác động mà độ thưa có đối với tính dẻo, tôi đã chạy một thí nghiệm
nhỏ khác trên tập dữ liệu permuted MNIST (Kirkpatrick et al., 2017). Trong
permuted MNIST, mỗi tác vụ tiếp theo trong một chuỗi tác vụ được tạo ra bằng
cách áp dụng một hoán vị cố định cho các pixel của hình ảnh MNIST gốc, dẫn
đến các tác vụ khác nhau nhưng tương tự về mặt cấu trúc. Vì bạn có thể tiếp tục
áp dụng các hoán vị mới theo ý muốn, bạn có thể tiếp tục huấn luyện trên các
tác vụ mới gần như mãi mãi.

Để kiểm tra hiệu quả của độ thưa đối với tính dẻo, tôi đã chạy thí nghiệm này
trên Hard ASH với các siêu tham số khác nhau. Giả thuyết được kiểm tra là việc
thay đổi lượng độ thưa (sửa đổi zk) hoặc độ thưa gradient (sửa đổi α, đường
cong alpha dốc hơn dẫn đến độ thưa nhiều hơn trong gradient) sẽ có tác động
đến tính dẻo của mạng.

Thí nghiệm được chạy trong 100 epoch, chuyển sang một tác vụ mới sau mỗi
epoch với tổng cộng 100 tác vụ. Adagrad được chọn làm bộ tối ưu vì nó là người
thực hiện tốt nhất tổng thể trong nghiên cứu chính. Kiến trúc mạng giống như
trong thí nghiệm chính, một MLP với một lớp ẩn 1000 nơ-ron đơn lẻ.

7

--- TRANG 8 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

Hình 2: Độ chính xác xác thực tác vụ mới nhất và tác vụ đầu tiên khi thay đổi zk

Hình 2 và 3 cho thấy độ chính xác trên tác vụ mới nhất ngay sau khi hoàn thành
huấn luyện (tính dẻo) và hiệu suất trên tác vụ đầu tiên trong toàn bộ quá trình
huấn luyện (ổn định).

Trong hình 2, tôi cho thấy tác động của việc thay đổi zk, tức là lượng độ thưa
trong biểu diễn lớp ẩn. Biểu đồ độ chính xác tác vụ mới nhất cho thấy rằng việc
thay đổi lượng độ thưa có tác động không đáng kể đến tính dẻo. Khi zk<3, Hard
ASH MLP có tính dẻo gần như giống MLP ReLU cơ sở. Khi zk≥3, hiệu suất đã
tệ hơn trong epoch đầu tiên, nhưng nó dường như không giúp ích nhiều cho tính dẻo.

Trong biểu đồ độ chính xác tác vụ đầu tiên, chúng ta thấy rằng khả năng giữ lại
độ chính xác của mạng trên tác vụ đầu tiên tăng lên với lượng độ thưa. Khi zk<1,
mạng hoạt động tương tự như ReLU cơ sở, nhưng khi zk>1, hiệu suất trên tác vụ
đầu tiên chỉ giảm nhẹ trong quá trình huấn luyện 99 tác vụ tiếp theo.

Trong hình 3, tôi cho thấy tác động của việc thay đổi α hoặc độ dốc của độ dốc
kích hoạt, tức là lượng độ thưa trong gradient. Đối với thí nghiệm này, zk được
giữ không đổi ở 1.5. Biểu đồ độ chính xác tác vụ mới nhất cho thấy phần lớn
giống như hình 2. Tính dẻo giống nhau bất kể cài đặt và gần như khớp với ReLU
cơ sở. Tính ổn định, hoặc khả năng giữ lại độ chính xác trên tác vụ đầu tiên,
tăng lên với α. Khi α trở nên rất cao (ví dụ 8, như được hiển thị trong biểu đồ)
có sự suy giảm hiệu suất đáng chú ý ngay từ đầu quá trình huấn luyện, nhưng
không có lợi ích đáng kể cho tính dẻo.

Do đó kết luận của tôi là độ thưa biểu diễn một mình không đủ để giải quyết
tính dẻo trong học liên tục và cần thêm cái gì đó. Thay vào đó tôi có thể nói
rằng độ thưa giúp ích cho tính ổn định mà không có hình phạt đáng kể cho tính
dẻo. Đối với một người học liên tục hoàn chỉnh hơn, vượt qua cả hai mặt của
tiến thoái lưỡng nan ổn định-tính dẻo, chúng ta có thể thử kết hợp các biểu diễn
thưa với một kỹ thuật tăng tính dẻo, như Continual backprop (Dohare et al., 2022),
Shrink and perturb (Ash & Adams, 2020) hoặc pruning dựa trên độ lớn tiến triển
với mở rộng mô hình như trong Menick et al. (2020).

8

--- TRANG 9 ---
Được xuất bản như một Bài báo Nhỏ tại ICLR 2024

Hình 3: Độ chính xác xác thực tác vụ mới nhất và tác vụ đầu tiên khi thay đổi α

9
