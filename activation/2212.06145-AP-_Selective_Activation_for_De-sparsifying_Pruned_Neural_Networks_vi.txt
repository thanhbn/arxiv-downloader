# 2212.06145.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/activation/2212.06145.pdf
# Kích thước tệp: 1478037 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
AP: Kích Hoạt Có Chọn Lọc để Khử Thưa Thớt Mạng Nơ-ron Đã Cắt Tỉa
Shiyu Liu, Rohan Ghosh, Dylan Tan, Mehul Motani
Khoa Kỹ Thuật Điện và Máy Tính
Trường Kỹ Thuật, Đại Học Quốc Gia Singapore
shiyu liu@u.nus.edu, rghosh92@gmail.com, eledtht@nus.edu.sg, motani@nus.edu.sg
Tóm tắt
Đơn vị kích hoạt tuyến tính chỉnh lưu (ReLU) là một hàm kích hoạt rất thành công trong mạng nơ-ron vì nó cho phép mạng dễ dàng có được biểu diễn thưa thớt, điều này làm giảm hiện tượng quá khớp trong các mạng có quá nhiều tham số. Tuy nhiên, trong việc cắt tỉa mạng, chúng tôi thấy rằng tính thưa thớt được tạo ra bởi ReLU, mà chúng tôi định lượng bằng một thuật ngữ gọi là tỷ lệ nơ-ron chết động (DNR), không có lợi cho mạng đã cắt tỉa. Thú vị là, mạng càng được cắt tỉa nhiều, DNR động càng trở nên nhỏ hơn trong quá trình tối ưu hóa. Điều này thúc đẩy chúng tôi đề xuất một phương pháp để giảm rõ ràng DNR động cho mạng đã cắt tỉa, tức là, khử thưa thớt mạng. Chúng tôi gọi phương pháp của mình là Kích Hoạt-trong-khi-Cắt Tỉa (AP). Chúng tôi lưu ý rằng AP không hoạt động như một phương pháp độc lập, vì nó không đánh giá tầm quan trọng của trọng số. Thay vào đó, nó hoạt động song song với các phương pháp cắt tỉa hiện có và nhằm cải thiện hiệu suất của chúng bằng kích hoạt có chọn lọc các nút để giảm DNR động. Chúng tôi tiến hành các thí nghiệm mở rộng sử dụng các mạng phổ biến (ví dụ: ResNet, VGG) thông qua hai phương pháp cắt tỉa cổ điển và ba phương pháp cắt tỉa tiên tiến. Kết quả thí nghiệm trên các bộ dữ liệu công khai (ví dụ: CIFAR-10/100) cho thấy AP hoạt động tốt với các phương pháp cắt tỉa hiện có và cải thiện hiệu suất 3% - 4%. Đối với các bộ dữ liệu quy mô lớn hơn (ví dụ: ImageNet) và các mạng tiên tiến (ví dụ: vision transformer), chúng tôi quan sát được sự cải thiện 2% - 3% với AP so với không có. Cuối cùng, chúng tôi tiến hành một nghiên cứu loại bỏ để kiểm tra hiệu quả của các thành phần tạo nên AP.

1 Giới thiệu
Đơn vị kích hoạt tuyến tính chỉnh lưu (ReLU) (Glorot, Bordes, và Bengio 2011), sigma(x)=max{x, 0}, là hàm kích hoạt được sử dụng rộng rãi nhất trong mạng nơ-ron (ví dụ: ResNet (He et al. 2016), Transformer (Vaswani et al. 2017)). Sự thành công của ReLU chủ yếu do thực tế là các mạng hiện có có xu hướng có quá nhiều tham số và ReLU có thể dễ dàng điều chỉnh các mạng có quá nhiều tham số bằng cách tạo ra tính thưa thớt (tức là, đầu ra sau kích hoạt bằng không) (Glorot, Bordes, và Bengio 2011), dẫn đến kết quả hứa hẹn trong nhiều tác vụ thị giác máy tính (ví dụ: phân loại hình ảnh (Simonyan và Zisserman 2014; He et al. 2016), phát hiện đối tượng (Dai et al. 2021; Joseph et al. 2021)).

Trong bài báo này, chúng tôi nghiên cứu ràng buộc thưa thớt của ReLU trong bối cảnh cắt tỉa mạng (tức là, một phương pháp nén loại bỏ trọng số khỏi mạng). Cụ thể, chúng tôi đặt câu hỏi về tính hữu ích của ràng buộc thưa thớt của ReLU, khi mạng không còn có quá nhiều tham số trong quá trình cắt tỉa lặp. Trong phần sau, chúng tôi tóm tắt quy trình làm việc của nghiên cứu cùng với các đóng góp của chúng tôi.

1. Động lực và Nghiên cứu Lý thuyết. Trong Phần 3.1, chúng tôi giới thiệu một thuật ngữ gọi là Tỷ lệ Nơ-ron Chết Động (DNR), định lượng tính thưa thớt được tạo ra bởi các nơ-ron ReLU không bị cắt tỉa hoàn toàn trong quá trình cắt tỉa lặp. Thông qua các thí nghiệm nghiêm ngặt trên các mạng phổ biến (ví dụ: ResNet (He et al. 2016)), chúng tôi thấy rằng mạng càng được cắt tỉa nhiều, DNR động càng trở nên nhỏ hơn trong quá trình tối ưu hóa. Điều này cho thấy tính thưa thớt được tạo ra bởi ReLU không có lợi cho các mạng đã cắt tỉa. Các nghiên cứu lý thuyết sâu hơn cũng tiết lộ tầm quan trọng của việc giảm DNR động cho các mạng đã cắt tỉa từ góc độ thông tin nút cổ chai (IB) (Tishby và Zaslavsky 2015) (xem Phần 3.2).

2. Một Phương pháp để Khử Thưa Thớt Mạng Đã Cắt Tỉa. Trong Phần 3.3, chúng tôi đề xuất một phương pháp gọi là Kích Hoạt-trong-khi-Cắt Tỉa (AP) nhằm giảm rõ ràng DNR động. Chúng tôi lưu ý rằng AP không hoạt động như một phương pháp độc lập, vì nó không đánh giá tầm quan trọng của trọng số. Thay vào đó, nó hoạt động song song với các phương pháp cắt tỉa hiện có và nhằm cải thiện hiệu suất của chúng bằng cách giảm DNR động. AP được đề xuất có hai biến thể: (i) AP-Lite cải thiện nhẹ hiệu suất của các phương pháp hiện có, nhưng không tăng độ phức tạp thuật toán, và (ii) AP-Pro giới thiệu một bước đào tạo lại bổ sung cho các phương pháp hiện có trong mỗi chu kỳ cắt tỉa, nhưng cải thiện đáng kể hiệu suất của các phương pháp hiện có.

3. Thí nghiệm. Trong Phần 4, chúng tôi tiến hành thí nghiệm trên CIFAR-10/100 (Krizhevsky et al. 2009) với một số mạng phổ biến (ví dụ: ResNet, VGG) sử dụng hai phương pháp cắt tỉa cổ điển và ba phương pháp cắt tỉa tiên tiến (SOTA). Kết quả cho thấy AP hoạt động tốt với các phương pháp cắt tỉa hiện có và cải thiện hiệu suất của chúng 3% - 4%. Đối với bộ dữ liệu quy mô lớn hơn (ví dụ: ImageNet (Deng et al. 2009)) và các mạng SOTA (ví dụ: vision transformer (Dosovitskiy et al. 2020)), chúng tôi quan sát được sự cải thiện 2% - 3% với AP so với không có.

4. Nghiên cứu Loại bỏ. Trong Phần 4.3, chúng tôi thực hiện một nghiên cứu loại bỏ để điều tra và chứng minh hiệu quả của một số thành phần chính tạo nên AP được đề xuất.

--- TRANG 2 ---
2 Kiến thức nền tảng
Cắt tỉa mạng là một phương pháp được sử dụng để giảm kích thước của mạng nơ-ron, với công trình đầu tiên (LeCun et al. 1998) có từ năm 1990. Về phong cách cắt tỉa, tất cả các phương pháp hiện có có thể được chia thành hai lớp: (i) Cắt Tỉa Một Lần và (ii) Cắt Tỉa Lặp. Giả sử chúng ta dự định cắt tỉa Q% tham số của một mạng đã được đào tạo, một chu kỳ cắt tỉa điển hình bao gồm ba bước cơ bản:

1. Cắt tỉa eta% của các tham số hiện có dựa trên các số liệu đã cho.
2. Đóng băng các trọng số đã cắt tỉa thành không.
3. Đào tạo lại mạng đã cắt tỉa để khôi phục hiệu suất.

Trong Cắt Tỉa Một Lần, eta được đặt thành Q và các tham số được cắt tỉa trong một chu kỳ cắt tỉa. Trong khi đối với Cắt Tỉa Lặp, một phần nhỏ hơn nhiều của các tham số (tức là, eta << Q) được cắt tỉa mỗi chu kỳ cắt tỉa. Quá trình cắt tỉa được lặp lại nhiều lần cho đến khi Q% tham số được cắt tỉa. Về hiệu suất, Cắt Tỉa Lặp thường dẫn đến hiệu suất tốt hơn so với Cắt Tỉa Một Lần (Han et al. 2015; Frankle và Carbin 2019; Li et al. 2017). Cho đến nay, các công trình hiện có nhằm cải thiện hiệu suất cắt tỉa bằng cách khám phá các số liệu cắt tỉa mới hoặc các phương pháp đào tạo lại mới.

Số liệu Cắt tỉa. Độ lớn trọng số là số liệu xấp xỉ phổ biến nhất được sử dụng để xác định các kết nối ít hữu ích hơn; trực giác là các trọng số có độ lớn nhỏ hơn có ảnh hưởng nhỏ hơn trong đầu ra, và do đó ít có khả năng tác động đến kết quả mô hình nếu bị cắt tỉa (He et al. 2020; Li et al. 2020a,b). Nhiều công trình đã nghiên cứu việc sử dụng độ lớn trọng số làm số liệu cắt tỉa, tức là (Han et al. 2015; Frankle và Carbin 2019). Gần đây hơn, (Lee et al. 2020) đã giới thiệu cắt tỉa dựa trên độ lớn thích ứng theo lớp (LAMP) và cố gắng cắt tỉa trọng số dựa trên phiên bản được tỷ lệ của độ lớn. (Park et al. 2020) đề xuất một phương pháp gọi là Cắt Tỉa Nhìn Xa (LAP), đánh giá tầm quan trọng của trọng số dựa trên tác động của cắt tỉa đến các lớp lân cận. Một số liệu phổ biến khác được sử dụng để cắt tỉa là thông qua gradient; trực giác là các trọng số có gradient nhỏ hơn ít có tác động hơn trong việc tối ưu hóa hàm mất mát. Ví dụ là (LeCun et al. 1998; Theis et al. 2018), trong đó (LeCun et al. 1998) đề xuất sử dụng đạo hàm bậc hai của hàm mất mát đối với các tham số (tức là, Ma trận Hessian) làm số liệu cắt tỉa và (Theis et al. 2018) sử dụng thông tin Fisher để xấp xỉ Ma trận Hessian. Một công trình gần đây (Blalock et al. 2020) đã xem xét nhiều phương pháp cắt tỉa và đề xuất hai phương pháp cắt tỉa cổ điển để đánh giá hiệu suất:

1. Độ Lớn Toàn cục: Cắt tỉa các trọng số có giá trị tuyệt đối thấp nhất ở bất kỳ đâu trong mạng.
2. Gradient Toàn cục: Cắt tỉa các trọng số có giá trị tuyệt đối thấp nhất của (trọng số × gradient) ở bất kỳ đâu trong mạng.

Phương pháp Đào tạo lại. Một yếu tố khác ảnh hưởng đáng kể đến hiệu suất cắt tỉa là phương pháp đào tạo lại. Theo (Han et al. 2015), Han et al. đã đào tạo mạng chưa cắt tỉa với một lịch tỷ lệ học tập và đào tạo lại mạng đã cắt tỉa sử dụng tỷ lệ học tập không đổi (tức là, thường là tỷ lệ học tập cuối cùng của lịch tỷ lệ học tập). Một công trình gần đây (Renda, Frankle, và Carbin 2019) đề xuất việc tua lại tỷ lệ học tập sử dụng cùng lịch tỷ lệ học tập để đào tạo lại mạng đã cắt tỉa, dẫn đến hiệu suất cắt tỉa tốt hơn. Gần đây hơn, (Liu, Tan, và Motani 2021) đã cố gắng tối ưu hóa lựa chọn tỷ lệ học tập (LR) trong quá trình đào tạo lại và đề xuất một lịch LR gọi là S-Cyc. Họ cho thấy S-Cyc có thể hoạt động tốt với các phương pháp cắt tỉa khác nhau, cải thiện thêm hiệu suất hiện có. Đáng chú ý nhất, (Frankle và Carbin 2019) phát hiện rằng việc đặt lại các trọng số chưa cắt tỉa về giá trị ban đầu của chúng (được gọi là tua lại trọng số) sau mỗi chu kỳ cắt tỉa có thể dẫn đến hiệu suất thậm chí cao hơn mô hình ban đầu. Một số công trình tiếp theo (Zhou et al. 2019; Renda, Frankle, và Carbin 2019; Malach et al. 2020) đã nghiên cứu hiện tượng này chính xác hơn và áp dụng phương pháp này trong các lĩnh vực khác (ví dụ: học chuyển giao (Mehta 2019), học tăng cường và xử lý ngôn ngữ tự nhiên (Yu et al. 2020)).

Các Công trình Khác. Ngoài các công trình được đề cập ở trên, một số công trình khác cũng chia sẻ một số hiểu biết sâu sắc hơn về cắt tỉa mạng (Liu et al. 2019; Zhu và Gupta 2018; Liu, Simonyan, và Yang 2019; Wang et al. 2020). Ví dụ, (Liu, Simonyan, và Yang 2019) chứng minh rằng đào tạo từ đầu trên kiến trúc thưa thớt phù hợp mang lại kết quả tốt hơn so với cắt tỉa từ các mô hình đã được đào tạo trước. Tương tự, (Wang et al. 2020) gợi ý rằng mạng được đào tạo đầy đủ có thể giảm không gian tìm kiếm cho cấu trúc đã cắt tỉa. Gần đây hơn, (Luo và Wu 2020) đã giải quyết vấn đề cắt tỉa các kết nối dư với dữ liệu hạn chế và (Ye et al. 2020) đã chứng minh lý thuyết sự tồn tại của các mạng con nhỏ với mất mát thấp hơn mạng chưa cắt tỉa.

3 Kích Hoạt-trong-khi-Cắt Tỉa
Trong Phần 3.1, chúng tôi trước tiên tiến hành thí nghiệm để đánh giá DNR trong quá trình cắt tỉa lặp. Tiếp theo, trong Phần 3.2, chúng tôi liên kết các kết quả thí nghiệm với các nghiên cứu lý thuyết và thúc đẩy Kích Hoạt-trong-khi-Cắt Tỉa (AP). Trong Phần 3.3, chúng tôi giới thiệu ý tưởng của AP và trình bày thuật toán của nó. Cuối cùng, trong Phần 3.4, chúng tôi minh họa cách AP có thể cải thiện hiệu suất của các phương pháp cắt tỉa hiện có.

3.1 Thí nghiệm về DNR
Chúng tôi nghiên cứu trạng thái của hàm ReLU trong quá trình cắt tỉa lặp và giới thiệu một thuật ngữ gọi là Tỷ lệ Nơ-ron Chết (DNR), đó là tỷ lệ phần trăm của các nơ-ron ReLU chết (tức là, một nơ-ron có đầu ra sau ReLU bằng không) trong mạng được tính trung bình trên tất cả các mẫu đào tạo khi mạng hội tụ. Về mặt toán học, DNR có thể được viết là

DNR = 1/n * Σ(i=1 to n) (số nơ-ron ReLU chết / tất cả nơ-ron trong mạng chưa cắt tỉa), (1)

trong đó n là số mẫu đào tạo. Chúng tôi phân loại một nơ-ron chết thành nơ-ron chết động hoặc nơ-ron chết tĩnh. Nơ-ron chết động là một nơ-ron chết trong đó không phải tất cả trọng số đã bị cắt tỉa. Do đó, nó không có khả năng chết vĩnh viễn và trạng thái của nó phụ thuộc vào đầu vào của nó. Ví dụ, một nơ-ron có thể chết cho mẫu X, nhưng nó có thể hoạt động (tức là, đầu ra sau ReLU > 0) cho mẫu Y. DNR được đóng góp bởi các nơ-ron chết động được gọi là DNR động. Nơ-ron chết tĩnh là một nơ-ron chết trong đó tất cả các trọng số liên quan đã bị cắt tỉa. DNR được đóng góp bởi các nơ-ron chết tĩnh được gọi là DNR tĩnh.

--- TRANG 3 ---
Hình 1: Tỷ lệ Nơ-ron Chết (DNR) động và tĩnh khi cắt tỉa lặp ResNet-20 trên CIFAR-10 sử dụng Độ Lớn Toàn cục. Trái: DNR động và tĩnh khi mạng hội tụ; Phải: DNR động trong quá trình tối ưu hóa.

DNR là một thuật ngữ mà chúng tôi giới thiệu để định lượng tính thưa thớt được tạo ra bởi ReLU. Nhiều số liệu thưa thớt tương tự đã được đề xuất trong văn hệ (Hurley và Rickard 2009). Ví dụ, Chỉ số Gini (Goswami, Murthy, và Das 2016) được tính từ đường cong Lorenz (tức là, vẽ biểu đồ tỷ lệ phần trăm tích lũy của tổng số lượng) có thể được sử dụng để đánh giá tính thưa thớt của đồ thị mạng. Một số liệu phổ biến khác sẽ là thước đo Hoyer (Hoyer 2004) là tỷ số giữa chuẩn L1 và L2, cũng có thể được sử dụng để đánh giá tính thưa thớt của mạng. Số liệu gần nhất với DNR là tính thưa thớt tham số (Goodfellow, Bengio, và Courville 2016) tính tỷ lệ phần trăm của các tham số có độ lớn bằng không trong tất cả các tham số. Chúng tôi lưu ý rằng cả tính thưa thớt tham số và DNR đều sẽ đóng góp vào biểu diễn thưa thớt, và trong bài báo này, chúng tôi sử dụng DNR để định lượng tính thưa thớt được tạo ra bởi ReLU.

Thiết lập Thí nghiệm và Quan sát. Với định nghĩa của DNR, DNR tĩnh và động, chúng tôi tiến hành thí nghiệm cắt tỉa sử dụng ResNet-20 trên bộ dữ liệu CIFAR-10 với mục đích kiểm tra lợi ích (hoặc thiếu lợi ích) của tính thưa thớt của ReLU đối với các mạng đã cắt tỉa. Chúng tôi cắt tỉa lặp ResNet-20 với tỷ lệ cắt tỉa 20 (tức là, 20% trọng số hiện có được cắt tỉa) sử dụng Độ Lớn Toàn cục (tức là, cắt tỉa các trọng số có độ lớn nhỏ nhất ở bất kỳ đâu trong mạng). Chúng tôi tham khảo triển khai tiêu chuẩn được báo cáo trong (Renda, Frankle, và Carbin 2019; Frankle và Carbin 2019) (tức là, bộ tối ưu SGD (Ruder 2016), 100 epoch đào tạo và kích thước batch 128, khởi động tỷ lệ học tập đến 0.03 và giảm theo hệ số 10 tại epoch 55 và 70) và tính DNR tĩnh và DNR động trong khi mạng được cắt tỉa lặp. Kết quả thí nghiệm được hiển thị trong Hình 1, nơi chúng tôi có hai quan sát.

1. Như được hiển thị trong Hình 1 (trái), giá trị của DNR (tức là, tổng của DNR tĩnh và động) tăng khi mạng được cắt tỉa lặp. Như mong đợi, DNR tĩnh tăng khi nhiều trọng số được cắt tỉa trong quá trình cắt tỉa lặp.

2. Đáng ngạc nhiên, DNR động có xu hướng giảm khi mạng được cắt tỉa lặp (xem Hình 1 (trái)), cho thấy rằng các mạng đã cắt tỉa không ưa tính thưa thớt của ReLU. Trong Hình 1 (phải), chúng tôi cho thấy rằng đối với các mạng đã cắt tỉa với lambda khác nhau (tức là, tỷ lệ phần trăm trọng số còn lại), chúng có DNR động tương tự trước khi đào tạo, nhưng mạng đã cắt tỉa với lambda nhỏ hơn có xu hướng có DNR động nhỏ hơn trong và sau khi tối ưu hóa.

Phân tích Kết quả. Một lý do có thể cho sự giảm DNR động có thể do thực tế là một khi nơ-ron chết, gradient của nó trở thành không, có nghĩa là nó ngừng học và làm giảm khả năng học của mạng (Lu et al. 2019; Arnekvist et al. 2020). Điều này có thể có lợi vì các mạng hiện có có xu hướng có quá nhiều tham số và DNR động có thể giúp giảm sự xuất hiện của quá khớp. Tuy nhiên, đối với các mạng đã cắt tỉa có khả năng học bị giảm nặng, DNR động có thể có hại vì một ReLU chết luôn xuất ra cùng một giá trị (không như nó xảy ra) cho bất kỳ đầu vào không dương nào, có nghĩa là nó không đóng vai trò trong việc phân biệt giữa các đầu vào. Do đó, trong quá trình đào tạo lại, mạng đã cắt tỉa cố gắng khôi phục hiệu suất của nó bằng cách giảm DNR động để thông tin được trích xuất có thể được truyền đến các lớp tiếp theo. Xu hướng hiệu suất tương tự có thể được quan sát sử dụng VGG-19 với Gradient Toàn cục (xem Hình 3 trong Phụ lục). Tiếp theo, chúng tôi trình bày một nghiên cứu lý thuyết về DNR và cho thấy tính liên quan của nó đến khả năng phân biệt của mạng.

3.2 Liên quan đến IB và Độ phức tạp
Ở đây, chúng tôi trình bày một kết quả lý thuyết và những hiểu biết tiếp theo làm nổi bật tính liên quan của DNR động của một lớp cụ thể của mạng đã cắt tỉa, đến Nguyên lý Nút cổ chai Thông tin (IB) được đề xuất trong (Tishby và Zaslavsky 2015). Trong thiết lập IB, luồng tính toán thường được ký hiệu bằng đồ thị X→T→Y, trong đó X đại diện cho đầu vào, T đại diện cho biểu diễn được trích xuất, và Y đại diện cho đầu ra của mạng. Nguyên lý IB nói rằng mục tiêu của việc đào tạo mạng phải là giảm thiểu thông tin tương hỗ (Cover và Thomas 2006) giữa X và T (ký hiệu là I(X;T)) trong khi giữ I(Y;T) lớn. Các đặc trưng được nén quá mức (I(X;T) thấp) sẽ không giữ lại đủ thông tin để dự đoán nhãn, trong khi các đặc trưng được nén dưới mức (I(X;T) cao) ngụ ý rằng nhiều thông tin không liên quan đến nhãn được giữ lại trong T có thể ảnh hưởng xấu đến tổng quát hóa. Tiếp theo, chúng tôi cung cấp một vài định nghĩa.

Định nghĩa 1. DNR động Cụ thể theo Lớp (DDNR(T)): Chúng tôi được cho một bộ dữ liệu S={X1, ..., Xm}, trong đó Xi∼P cho tất cả i và P là phân phối tạo dữ liệu. Chúng tôi ký hiệu DNR động chỉ của các nơ-ron tại một lớp nhất định trong mạng, được biểu diễn bằng vector T, bằng DDNR(T). DDNR(T) được tính trên toàn bộ phân phối đầu vào trong P.

--- TRANG 4 ---
Định nghĩa 2. DNR tĩnh Cụ thể theo Lớp (SDNR(T)): Nó được định nghĩa theo cách tương tự như DDNR(T), thay vì tính DNR động, chúng tôi tính DNR tĩnh của T.

Với điều này, chúng tôi bây giờ phác thảo kết quả lý thuyết đầu tiên của chúng tôi làm nổi bật tính liên quan của DDNR(T) và SDNR(T) đến I(X;T), như sau.

Định lý 1. Chúng tôi được cho luồng tính toán X→T→Y, trong đó T đại diện cho các đặc trưng tại một độ sâu tùy ý trong mạng, được biểu diễn với độ chính xác hữu hạn (ví dụ: float32 hoặc float64). Chúng tôi chỉ xem xét tập con của các cấu hình mạng mà (a) các kích hoạt trong T nhỏ hơn ngưỡng tau và (b) xác suất kích hoạt bằng không của mỗi nơ-ron trong T được giới hạn trên bởi một số pS<1. Gọi dim(T) đại diện cho chiều của T, tức là, số nơ-ron tại độ sâu đó. Khi đó chúng ta có,

(2) I(X;T) <= C × dim(T) × (1-SDNR(T)-DDNR(T)) / (1-1/C * log(1-SDNR(T)/DDNR(T))),

cho một hằng số hữu hạn C chỉ phụ thuộc vào kiến trúc mạng, tau và pS.

Hệ quả sau giải quyết các phụ thuộc của Định lý 1. Chứng minh của Định lý 1 và Hệ quả 1 được cung cấp trong Phụ lục.

Hệ quả 1. Cận trên cho I(X;T) trong Định lý 1 giảm để đáp ứng với sự tăng của DDNR(T) và SDNR(T).

Nhận xét 1. (Liên quan đến Độ phức tạp) Chúng ta thấy rằng (Shamir, Sabato, và Tishby 2010) lưu ý cách số liệu I(X;T) đại diện cho độ phức tạp hiệu quả của mạng. Như Định lý 3 trong (Shamir, Sabato, và Tishby 2010) cho thấy, I(X;T) nắm bắt các phụ thuộc giữa X và T và tương quan trực tiếp với khả năng khớp hàm của mạng. Kết hợp với các quan sát từ Định lý 1 và Hệ quả 1, đối với một cấu hình mạng đã cắt tỉa cố định (tức là, SDNR(T) cố định), DDNR(T) lớn hơn có khả năng sẽ giảm độ phức tạp hiệu quả của mạng, làm suy yếu khả năng học của mạng nơ-ron.

Nhận xét 2. (Động lực cho AP) Định lý 1 cũng cho thấy rằng một mạng đã cắt tỉa, sở hữu SDNR(T) lớn, dẫn đến rủi ro nén thông tin quá mức cao hơn (I(X;T) thấp). Để giải quyết vấn đề này, chúng ta có thể giảm DNR động (từ Hệ quả 1) để cận trên của I(X;T) có thể được tăng lên, giảm thiểu vấn đề nén quá mức cho mạng đã cắt tỉa. Điều này phù hợp với động lực ban đầu của chúng tôi rằng tính thưa thớt được tạo ra bởi ReLU không có lợi cho mạng đã cắt tỉa và giảm DNR động giúp cải thiện khả năng học của mạng đã cắt tỉa.

3.3 Thuật toán của Kích Hoạt-trong-khi-Cắt Tỉa
Các kết quả thí nghiệm và lý thuyết ở trên cho thấy rằng, để bảo tồn tốt hơn khả năng học của các mạng đã cắt tỉa, một DNR động nhỏ hơn được ưa thích. Điều này thúc đẩy chúng tôi đề xuất Kích Hoạt-trong-khi-Cắt Tỉa (AP) nhằm giảm rõ ràng DNR động.

Chúng tôi lưu ý rằng AP được đề xuất không hoạt động một mình, vì nó không đánh giá tầm quan trọng của trọng số. Thay vào đó, nó phục vụ như một bộ tăng cường cho các phương pháp cắt tỉa hiện có và giúp cải thiện hiệu suất cắt tỉa của chúng bằng cách giảm DNR động (xem Hình 2). Giả sử phương pháp cắt tỉa X loại bỏ p% trọng số trong mỗi chu kỳ cắt tỉa (xem phần trên trong Hình 2). Sau khi sử dụng AP, tỷ lệ cắt tỉa tổng thể vẫn không thay đổi là p%, nhưng (p-q)% trọng số được cắt tỉa theo phương pháp cắt tỉa X với mục đích cắt tỉa các trọng số ít quan trọng hơn, trong khi q% trọng số được cắt tỉa theo AP (xem phần dưới trong Hình 2) với mục đích giảm DNR động. Xem xét một mạng f(theta) với hàm kích hoạt ReLU. Hai bước chính để giảm DNR động được tóm tắt như sau.

(1) Định vị Nơ-ron ReLU Chết. Xem xét một nơ-ron trong lớp ẩn với hàm kích hoạt ReLU, nhận n đầu vào {X1W1, ..., XnWn|Xi∈R là đầu vào và Wi∈R là trọng số liên quan}. Gọi j là đầu ra tiền kích hoạt của nơ-ron (tức là, j=Σ(i=1 to n)XiWi) và J là đầu ra hậu kích hoạt của nơ-ron (J=ReLU(j)). Gọi L là hàm mất mát và giả sử nơ-ron chết (J=0), khi đó gradient của các trọng số liên quan của nó (ví dụ: W1) đối với hàm mất mát sẽ là ∂L/∂W1 = ∂L/∂J · ∂J/∂j · ∂j/∂W1 = 0 vì ∂J/∂j = 0. Nếu một nơ-ron thường chết trong quá trình đào tạo, chuyển động trọng số của các trọng số liên quan của nó có khả năng nhỏ hơn so với các nơ-ron khác. Do đó, chúng tôi tính sự khác biệt giữa trọng số tại khởi tạo (theta0) và các trọng số khi mạng hội tụ (theta*), tức là, |theta*-theta0| và sử dụng nó như một heuristic để định vị các nơ-ron ReLU chết.

(2) Kích Hoạt Nơ-ron ReLU Chết. Giả sử chúng ta đã định vị một nơ-ron chết trong lớp ẩn với n đầu vào {X1W1, ..., XnWn|Xi∈R là đầu vào và Wi∈R là trọng số liên quan}. Chúng tôi lưu ý rằng Xi không âm vì Xi thường là đầu ra hậu kích hoạt từ lớp trước (tức là, đầu ra của ReLU không âm). Do đó, một cách đơn giản để kích hoạt nơ-ron chết là cắt tỉa các trọng số có giá trị âm. Bằng cách cắt tỉa các trọng số âm như vậy, chúng ta có thể tăng giá trị của đầu ra tiền kích hoạt, có thể biến đầu ra tiền kích hoạt thành dương để giảm DNR động.

3.4 Cách AP Cải thiện Các Phương pháp Hiện có
Bây giờ chúng tôi tóm tắt cách AP có thể cải thiện các phương pháp cắt tỉa hiện có trong Thuật toán 2, trong đó phần trên là thuật toán của một phương pháp cắt tỉa lặp tiêu chuẩn gọi là phương pháp cắt tỉa X và phần dưới là thuật toán của phương pháp X với AP. AP được đề xuất có hai biến thể: AP-Pro và AP-Lite. Chúng tôi lưu ý rằng cả AP-Pro và AP-Lite đều chứa cùng ba bước, được tóm tắt như sau.

1. Cắt tỉa. Cho một mạng tại sự hội tụ với một tập hợp các nơ-ron ReLU chết động, N1={n1, n2, n3, ...}. Bước cắt tỉa của AP nhằm kích hoạt các nơ-ron ReLU chết động này (tức là, giảm DNR động) để bảo tồn khả năng học của mạng đã cắt tỉa (xem số liệu cắt tỉa của AP trong thuật toán 1).

2. Tua lại Trọng số. Đặt lại các trọng số chưa cắt tỉa về giá trị của chúng tại khởi tạo. Chúng tôi lưu ý rằng các khởi tạo trọng số khác nhau có thể dẫn đến các tập hợp N khác nhau. Trong bước trước, AP nhằm giảm DNR động cho N1 mục tiêu và tua lại trọng số cố gắng ngăn chặn

--- TRANG 5 ---
Phương pháp Cắt tỉa X
p% p% p% p% Tỷ lệ phần trăm
Trọng số được Cắt tỉa
bởi Phương pháp X
Chu kỳ Cắt tỉa 1 2 3 4 ······

Phương pháp Cắt tỉa X với Kích Hoạt-trong-khi-Cắt Tỉa (AP)
p-q% q% p-q% q% p-q% q% p-q% q% Tỷ lệ phần trăm
Trọng số được Cắt tỉa
bởi AP
Chu kỳ Cắt tỉa 1 2 3 4 ······

Hình 2: Minh họa cách AP hoạt động song song với các phương pháp cắt tỉa hiện có (ví dụ: phương pháp X) trong mỗi chu kỳ cắt tỉa.

Thuật toán 1: Số liệu Cắt tỉa của AP được Đề xuất
Yêu cầu: (i) Mạng f với trọng số chưa cắt tỉa theta0 tại khởi tạo, f(theta0); (ii) Mạng f với trọng số chưa cắt tỉa theta* tại hội tụ, f(theta*); (iii) Tỷ lệ Cắt tỉa của AP, q;
1: Định vị Nơ-ron Chết: Sắp xếp |theta*-theta0| theo thứ tự tăng dần.
2: Kích Hoạt Nơ-ron Chết: Theo thứ tự tăng dần của |theta*-theta0|, cắt tỉa q% đầu tiên của các trọng số có độ lớn âm.

N1 mục tiêu thay đổi quá nhiều. Vì các trọng số của các nơ-ron ReLU trong N1 đã được cắt tỉa bởi AP, những nơ-ron này có thể trở nên hoạt động trong quá trình đào tạo lại. Hiệu quả của tua lại trọng số được đánh giá thông qua một nghiên cứu loại bỏ.

3. Đào tạo lại mạng đã cắt tỉa để khôi phục hiệu suất.

Sự khác biệt chính giữa AP-Lite và AP-Pro là AP-Lite áp dụng ba bước này chỉ một lần ở cuối cắt tỉa (tức là, khi tất cả chu kỳ cắt tỉa kết thúc). Nó nhằm cải thiện nhẹ hiệu suất, nhưng không tăng đáng kể độ phức tạp thuật toán. Đối với AP-Pro, nó áp dụng ba bước trên trong mỗi chu kỳ cắt tỉa, làm tăng độ phức tạp thuật toán (chủ yếu do bước đào tạo lại), nhưng nhằm cải thiện đáng kể hiệu suất, có thể được ưa thích trong các tác vụ định hướng hiệu suất.

4 Đánh giá Hiệu suất

4.1 Thiết lập Thí nghiệm
(1) Chi tiết Thí nghiệm. Để chứng minh rằng AP có thể hoạt động tốt với các phương pháp cắt tỉa khác nhau, chúng tôi liệt kê ngắn hai phương pháp cắt tỉa cổ điển và ba phương pháp cắt tỉa tiên tiến. Chi tiết cho mỗi thí nghiệm được tóm tắt như sau.

1. Cắt tỉa ResNet-20 trên bộ dữ liệu CIFAR-10 sử dụng Độ Lớn Toàn cục có và không có AP.
2. Cắt tỉa VGG-19 trên bộ dữ liệu CIFAR-10 sử dụng Gradient Toàn cục có và không có AP.
3. Cắt tỉa DenseNet-40 (Huang et al. 2017) trên CIFAR-100 sử dụng Cắt tỉa dựa trên Độ lớn Thích ứng theo Lớp (LAMP) (Lee et al. 2020) có và không có AP.
4. Cắt tỉa MobileNetV2 (Sandler et al. 2018) trên bộ dữ liệu CIFAR-100 sử dụng Cắt tỉa Nhìn Xa (LAP) (Park et al. 2020) có và không có AP.
5. Cắt tỉa ResNet-50 (He et al. 2016) trên ImageNet (tức là, ImageNet-1000) sử dụng Cắt tỉa Độ lớn Lặp (IMP) (Frankle và Carbin 2019) có và không có AP.
6. Cắt tỉa Vision Transformer (ViT-B-16) trên CIFAR-10 sử dụng IMP có và không có AP.

Thuật toán 2: Phương pháp Cắt tỉa X có và không có AP
Yêu cầu: (i) Mạng, f(theta); (ii) Tỷ lệ Cắt tỉa của Phương pháp X, p; (iii) Tỷ lệ Cắt tỉa của AP, q; (iv) Cờ Pro = {0: AP-Lite, 1: AP-Pro}; (v) Chu kỳ Cắt tỉa, n;
-------- Phương pháp Cắt tỉa X ---------
1: for i=1 to n do
2: Khởi tạo ngẫu nhiên các trọng số chưa cắt tỉa, theta←theta0.
3: Đào tạo mạng đến hội tụ, đạt được các tham số theta*.
4: Cắt tỉa p% của theta* theo phương pháp cắt tỉa X.
5: end for
6: Đào tạo lại: Đào tạo lại mạng để khôi phục hiệu suất của nó.
---- Phương pháp Cắt tỉa X với AP được Đề xuất ----
7: for i=1 to n do
8: Khởi tạo ngẫu nhiên các trọng số chưa cắt tỉa, theta←theta0.
9: Đào tạo mạng đến hội tụ, đạt được các tham số theta*.
10: Cắt tỉa (p-q)% của theta* theo phương pháp cắt tỉa X.
11: if ProFlag then #Thực thi AP-Pro
12: (i) Cắt tỉa: Cắt tỉa q% của tham số theta* theo số liệu của AP (xem chi tiết trong Thuật toán 1).
13: (ii) Tua lại Trọng số: Đặt lại các tham số còn lại về giá trị của chúng trong theta0.
14: (iii) Đào tạo lại: Đào tạo lại mạng đã cắt tỉa để khôi phục hiệu suất của nó.
15: end if
16: end for
17: if NOT ProFlag then #Thực thi AP-Lite
18: (i) Cắt tỉa: Cắt tỉa q% của các tham số theta* theo số liệu của AP (xem chi tiết trong Thuật toán 1).
19: (ii) Tua lại Trọng số: Đặt lại các tham số còn lại về giá trị của chúng trong theta0.
20: (iii) Đào tạo lại: Đào tạo lại mạng đã cắt tỉa để khôi phục hiệu suất của nó.
21: end if

Chúng tôi đào tạo mạng sử dụng SGD với momentum = 0.9 và weight decay 1e-4 (giống như (Renda, Frankle, và Carbin 2019; Frankle và Carbin 2019)). Đối với phương pháp cắt tỉa chuẩn, chúng tôi cắt tỉa mạng với tỷ lệ cắt tỉa p=20 (tức là, 20% trọng số hiện có được cắt tỉa) trong 1 chu kỳ cắt tỉa. Sau khi sử dụng AP, tỷ lệ cắt tỉa tổng thể vẫn không thay đổi là 20%, nhưng 2% trọng số hiện có được cắt tỉa dựa trên AP, trong khi 18% trọng số hiện có khác được cắt tỉa dựa trên phương pháp cắt tỉa chuẩn để so sánh (xem Thuật toán 2). Chúng tôi lặp lại 25 chu kỳ cắt tỉa trong 1 lần chạy và sử dụng độ chính xác kiểm tra top-1 dừng sớm (tức là, độ chính xác kiểm tra tương ứng khi tiêu chí dừng sớm cho lỗi xác thực được đáp ứng) để đánh giá hiệu suất. Thí nghiệm

--- TRANG 6 ---
Độ chính xác Kiểm tra Top-1 Gốc: 91.7% (lambda = 100%)
lambda 32.8% 26.2% 13.4% 5.72%
Độ Lớn Toàn cục 90.3±0.4 89.8±0.6 88.2±0.7 81.2±1.1
AP-Lite 90.4±0.7 90.2±0.8 88.7±0.7 82.4±1.4
AP-Pro 90.7±0.6 90.4±0.4 89.3±0.8 84.1±1.1

Bảng 1: Hiệu suất (độ chính xác kiểm tra top-1 ± độ lệch chuẩn) của cắt tỉa ResNet-20 trên CIFAR-10 sử dụng Độ Lớn Toàn cục (Độ Lớn Toàn cục) có và không có AP.

Độ chính xác Kiểm tra Top-1 Gốc: 92.2% (lambda = 100%)
lambda 32.8% 26.2% 13.4% 5.72%
Gradient Toàn cục 90.2±0.5 89.8±0.8 89.2±0.8 76.9±1.1
AP-Lite 90.5±0.8 90.3±0.7 89.7±0.9 78.4±1.4
AP-Pro 90.8±0.6 90.7±0.9 90.4±0.8 79.2±1.3

Bảng 2: Hiệu suất (độ chính xác kiểm tra top-1 ± độ lệch chuẩn) của cắt tỉa VGG-19 trên CIFAR-10 sử dụng Gradient Toàn cục (Gradient Toàn cục) có và không có AP.

kết quả được tính trung bình trên 5 lần chạy và độ lệch chuẩn tương ứng được tóm tắt trong Bảng 1 - 6, trong đó lambda là tỷ lệ phần trăm trọng số còn lại.

(2) Lựa chọn và Điều chỉnh Siêu tham số. Để đảm bảo so sánh công bằng với các kết quả trước đó, chúng tôi sử dụng các triển khai tiêu chuẩn (tức là, siêu tham số mạng và lịch tỷ lệ học tập) được báo cáo trong văn hệ. Cụ thể, các triển khai cho Bảng 1 - 6 là từ (Frankle và Carbin 2019), (Zhao et al. 2019), (Chin et al. 2020), (Renda, Frankle, và Carbin 2019) và (Dosovitskiy et al. 2020). Chi tiết triển khai có thể được tìm thấy trong Phần B.2 của Phụ lục. Ngoài ra, chúng tôi cũng điều chỉnh siêu tham số sử dụng bộ dữ liệu xác thực thông qua tìm kiếm lưới. Một số siêu tham số được điều chỉnh như sau. (i) Kích thước batch đào tạo được điều chỉnh từ {64, 128, ...., 1024}. (ii) Tỷ lệ học tập được điều chỉnh từ 1e-3 đến 1e-1 thông qua kích thước bước 2e-3. (iii) Số epoch đào tạo được điều chỉnh từ 80 đến 500 với kích thước bước 20. Hiệu suất xác thực sử dụng các tham số đã điều chỉnh của chúng tôi gần với hiệu suất sử dụng các triển khai tiêu chuẩn. Do đó, chúng tôi sử dụng các triển khai tiêu chuẩn được báo cáo trong văn hệ để tái tạo kết quả chuẩn.

(3) Tái tạo Kết quả Chuẩn. Bằng cách sử dụng các triển khai được báo cáo trong văn hệ, chúng tôi đã tái tạo chính xác các kết quả chuẩn. Ví dụ, các kết quả chuẩn trong Bảng 1 - 6 của chúng tôi có thể so sánh với Hình 11 và Hình 9 của (Blalock et al. 2020), Bảng 4 trong (Liu et al. 2019), Hình 3 trong (Chin et al. 2020), Hình 10 trong (Frankle et al. 2020), Bảng 5 trong (Dosovitskiy et al. 2020), tương ứng.

(4) Mã nguồn & Thiết bị: Chúng tôi sử dụng thiết bị Tesla V100 cho các thí nghiệm của mình và mã nguồn (bao gồm seed ngẫu nhiên) sẽ được phát hành ở giai đoạn camera-ready.

4.2 So sánh Hiệu suất
(1) Hiệu suất sử dụng Các Phương pháp Cắt tỉa SOTA và Cổ điển. Trong Bảng 3 và 4, chúng tôi cho thấy AP có thể hoạt động tốt với các phương pháp cắt tỉa SOTA (ví dụ: LAMP, LAP). Trong Bảng 3, chúng tôi cho thấy hiệu suất của AP sử dụng LAMP thông qua

Độ chính xác Kiểm tra Top-1 Gốc: 74.6% (lambda = 100%)
lambda 32.8% 26.2% 13.4% 5.72%
LAMP 71.5±0.7 69.6±0.8 65.8±0.9 61.2±1.4
AP-Lite 71.9±0.8 70.3±0.7 66.6±0.7 62.2±1.2
AP-Pro 72.2±0.7 71.1±0.7 68.8±0.9 63.5±1.5

Bảng 3: Hiệu suất (độ chính xác kiểm tra top-1 ± độ lệch chuẩn) của cắt tỉa DenseNet-40 trên CIFAR-100 sử dụng Cắt tỉa Độ lớn Thích ứng theo Lớp (LAMP) có/không có AP.

Độ chính xác Kiểm tra Top-1 Gốc: 73.7% (lambda = 100%)
lambda 32.8% 26.2% 13.4% 5.72%
LAP 72.1±0.8 70.5±0.9 67.3±0.8 64.8±1.5
AP-Lite 72.5±0.9 70.9±0.8 68.2±1.2 66.2±1.5
AP-Pro 72.8±0.7 71.4±0.8 69.1±0.8 67.4±1.1

Bảng 4: Hiệu suất (độ chính xác kiểm tra top-1 ± độ lệch chuẩn) của cắt tỉa MobileNetV2 trên CIFAR-100 sử dụng Cắt tỉa Nhìn Xa (LAP) có và không có AP.

DenseNet-40 trên CIFAR-100. Chúng tôi quan sát rằng AP-Lite cải thiện hiệu suất của LAMP 1.2% tại lambda=13.4% và sự cải thiện tăng lên 1.6% tại lambda=5.7%. Lưu ý rằng AP-Lite không tăng độ phức tạp thuật toán của các phương pháp hiện có. Đối với AP-Pro, nó gây ra sự cải thiện lớn hơn 4.6% và 3.8% tại lambda=13.4% và lambda=5.7%, tương ứng. Xu hướng hiệu suất tương tự có thể được quan sát trong Bảng 4, nơi chúng tôi cho thấy hiệu suất của AP sử dụng LAP thông qua MobileNetV2 trên CIFAR-100. Hơn nữa, các cải thiện hiệu suất tương tự có thể được quan sát sử dụng các phương pháp cắt tỉa cổ điển (Độ Lớn Toàn cục/Gradient) thông qua ResNet-20/VGG-19 trên CIFAR-10 (xem Bảng 1 - 2).

(2) Hiệu suất trên ImageNet. Trong Bảng 5, chúng tôi cho thấy hiệu suất của AP sử dụng Cắt tỉa Độ lớn Lặp (IMP, tức là, phương pháp cắt tỉa giả thuyết vé số may mắn) thông qua ResNet-50 trên ImageNet (tức là, phiên bản ILSVRC) chứa hơn 1.2 triệu hình ảnh từ 1000 lớp khác nhau. Chúng tôi quan sát rằng AP-Lite cải thiện hiệu suất của IMP 1.5% tại lambda=5.7%. Đối với AP-Pro, nó cải thiện hiệu suất của IMP 2.8% tại lambda=5.7%.

(3) Hiệu suất trên Mạng SOTA (Vision Transformer). Một số công trình gần đây (Liu et al. 2021; Yuan et al. 2021; Chen, Fan, và Panda 2021) đã chứng minh rằng các mạng dựa trên transformer có xu hướng cung cấp hiệu suất xuất sắc trong các tác vụ thị giác máy tính (ví dụ: phân loại). Bây giờ chúng tôi kiểm tra hiệu suất của AP sử dụng Vision Transformer (tức là, ViT-B16 với độ phân giải 384). Chúng tôi lưu ý rằng ViT-B16 sử dụng Đơn vị Tuyến tính Lỗi Gaussian (GELU, GELU(x) = x Φ(x), trong đó Φ(x) là hàm phân phối tích lũy Gaussian tiêu chuẩn) làm hàm kích hoạt. Tương tự như ReLU chặn đầu ra tiền kích hoạt âm, GELU điều chỉnh mạnh đầu ra tiền kích hoạt âm bằng cách nhân với một giá trị cực nhỏ của Φ(x), cho thấy rằng AP có thể hữu ích với việc cắt tỉa các mô hình dựa trên GELU.

Chúng tôi lặp lại cùng thiết lập thí nghiệm như trên và đánh giá hiệu suất của AP sử dụng ViT-B16 trong Bảng 6. Chúng tôi

--- TRANG 7 ---
Độ chính xác Kiểm tra Top-1 Gốc: 77.0% (lambda = 100%)
lambda 32.8% 26.2% 13.4% 5.72%
IMP 76.8±0.2 76.4±0.3 75.2±0.4 71.5±0.4
AP-Lite 77.2±0.3 76.9±0.4 76.1±0.3 72.6±0.5
AP-Pro 77.5±0.4 77.2±0.3 76.8±0.6 73.5±0.4

Bảng 5: Hiệu suất (độ chính xác xác thực top-1 ± độ lệch chuẩn) của cắt tỉa ResNet-50 trên ImageNet sử dụng Cắt tỉa Độ lớn Lặp (IMP) có và không có AP.

Độ chính xác Kiểm tra Top-1 Gốc: 98.0% (lambda = 100%)
lambda 32.8% 26.2% 13.4% 5.72%
IMP 97.3±0.6 96.8±0.7 88.1±0.9 82.1±0.9
AP-Lite 98.0±0.4 97.3±0.7 89.9±0.6 83.6±0.8
AP-Pro 98.2±0.6 97.6±0.5 91.1±0.8 84.8±1.0

Bảng 6: Hiệu suất (độ chính xác kiểm tra top-1 ± độ lệch chuẩn) của cắt tỉa Vision Transformer (ViT-B-16) trên CIFAR-10 sử dụng IMP có và không có AP.

quan sát rằng AP-Lite giúp cải thiện hiệu suất của IMP 1.8% tại lambda=5.7%. Đối với AP-Pro, nó cải thiện hiệu suất của IMP 3.3% tại lambda=5.7%.

4.3 Nghiên cứu Loại bỏ
Bây giờ chúng tôi tiến hành một nghiên cứu loại bỏ để đánh giá hiệu quả của các thành phần trong AP. Cụ thể, chúng tôi loại bỏ một thành phần tại một thời điểm trong AP và quan sát tác động đến hiệu suất cắt tỉa. Chúng tôi xây dựng một số biến thể của AP như sau.

1. AP-Lite-NO-WR: Sử dụng AP-Lite không có bước tua lại trọng số (tức là, loại bỏ bước (ii) từ AP-Lite trong Thuật toán 2). Điều này nhằm đánh giá hiệu quả của tua lại trọng số đối với hiệu suất cắt tỉa.

2. AP-Lite-SOLO: Chỉ sử dụng AP-Lite không có phương pháp cắt tỉa chuẩn (tức là, trong mỗi chu kỳ cắt tỉa, cắt tỉa trọng số chỉ dựa trên AP). Điều này nhằm đánh giá xem số liệu cắt tỉa của AP có thể được sử dụng để đánh giá tầm quan trọng của trọng số hay không.

Trong Bảng 7, chúng tôi tiến hành thí nghiệm Cắt tỉa ResNet-20 trên bộ dữ liệu CIFAR-10 sử dụng Độ Lớn Toàn cục. Dựa trên cấu hình này, chúng tôi so sánh hiệu suất của AP-Lite-NO-WR, AP-Lite-SOLO với AP-Lite để chứng minh hiệu quả của các thành phần trong AP. Chúng tôi lưu ý rằng, giống như trên, chúng tôi sử dụng triển khai được báo cáo trong văn hệ. Cụ thể, các siêu tham số và lịch tỷ lệ học tập là từ (Frankle và Carbin 2019).

Hiệu quả của Tua lại Trọng số. Trong Bảng 7, chúng tôi so sánh hiệu suất của AP-Lite-NO-WR với AP-Lite trong khi sự khác biệt chính là AP-Lite sử dụng tua lại trọng số (xem Thuật toán 2) và AP-Lite-NO-WR thì không. Chúng tôi thấy rằng hiệu suất của AP-Lite tại lambda=51.2% cao hơn AP-Lite-NO-WR 2.4%. Điều này cho thấy vai trò quan trọng của tua lại trọng số trong việc cải thiện hiệu suất. Xu hướng hiệu suất tương tự có thể được quan sát cho các giá trị lambda khác.

Khi AP Hoạt động Một mình. Số liệu cắt tỉa của AP (xem thuật toán 1) nhằm giảm DNR động bằng cắt tỉa. Chúng tôi

lambda 51.2% 40.9% 32.8%
AP-Lite 89.6±0.5 88.9±0.6 88.5±0.8
AP-Lite-SOLO 87.1±0.7 86.3±0.9 85.2±1.1
AP-Lite-NO-WR 87.5±0.5 86.8±0.8 85.9±0.9

Bảng 7: Nghiên cứu Loại bỏ: So sánh Hiệu suất (độ chính xác kiểm tra top-1 ± độ lệch chuẩn) giữa AP-Lite và AP-SOLO, AP-Lite-NO-WR trên cắt tỉa ResNet-20 trên CIFAR-10 thông qua Độ Lớn Toàn cục.

so sánh hiệu suất của AP-Lite-SOLO với AP-Lite để đánh giá xem số liệu cắt tỉa của AP có thể được sử dụng một mình, không cần hoạt động với các phương pháp cắt tỉa khác hay không. Trong Bảng 7, chúng tôi quan sát rằng AP-Lite-SOLO hoạt động kém hơn nhiều so với AP-Lite. Ví dụ, tại lambda=51.2%, hiệu suất của AP-Lite-SOLO là 87.1, thấp hơn AP-Lite 2.8%. Xu hướng hiệu suất tương tự có thể được quan sát trong Bảng 14 (xem Phụ lục), nơi chúng tôi cắt tỉa VGG-19 sử dụng CIFAR-10. Điều này cho thấy rằng số liệu cắt tỉa của AP không phù hợp để đánh giá tầm quan trọng của trọng số. Hiệu quả của số liệu AP đối với việc giảm DNR động và tỷ lệ cắt tỉa q của nó đối với hiệu suất cắt tỉa được thảo luận trong Phần Suy ngẫm dưới đây.

5 Suy ngẫm
AP được đề xuất nhằm cải thiện các phương pháp cắt tỉa hiện có bằng cách giảm DNR động. Các thí nghiệm mở rộng trên các mạng phổ biến/SOTA và các bộ dữ liệu quy mô lớn chứng minh rằng AP hoạt động tốt với các phương pháp cắt tỉa khác nhau, cải thiện đáng kể hiệu suất 3% - 4%. Bây giờ chúng tôi kết luận bài báo bằng cách trình bày một số điểm liên quan.

(1) Tỷ lệ Cắt tỉa của AP, q. AP loại bỏ q% tham số còn lại trong mỗi chu kỳ cắt tỉa, để giảm DNR động. Giá trị của q thường nhỏ hơn nhiều so với tỷ lệ cắt tỉa của phương pháp cắt tỉa mà nó hoạt động cùng. Điều chỉnh giá trị của q là một sự đánh đổi giữa việc cắt tỉa các trọng số ít quan trọng hơn và giảm DNR động. q lớn cho thấy ưu tiên giảm DNR động, trong khi q nhỏ có nghĩa là ưu tiên loại bỏ các trọng số ít quan trọng hơn. Chúng tôi tiến hành thí nghiệm để đánh giá hiệu quả của q đối với hiệu suất và kết quả cho thấy rằng giá trị q nhỏ hơn có thể dẫn đến hiệu suất tốt (xem Phụ lục để biết thêm chi tiết).

(2) DNR Động với AP. Chúng tôi cũng kiểm tra hiệu quả của AP trong việc giảm DNR động. Kết quả thí nghiệm trên ResNet-20/VGG-19 cho thấy rằng AP hoạt động như mong đợi và giảm đáng kể DNR động. Chúng tôi giới thiệu độc giả quan tâm đến Phụ lục để biết thêm chi tiết.

(3) Giảm DNR Tĩnh. Định lý 1 cho thấy rằng, ngoài DNR động, việc giảm DNR tĩnh cũng có thể cải thiện cận trên của I(X;T). Trên thực tế, việc giảm DNR tĩnh đã được tích hợp trực tiếp hoặc gián tiếp vào các phương pháp cắt tỉa hiện có. Ví dụ, LAMP (tức là, một phương pháp cắt tỉa SOTA được sử dụng trong đánh giá hiệu suất, xem Bảng 3) tính đến số lượng trọng số chưa cắt tỉa của các nơ-ron/lớp và tránh cắt tỉa trọng số từ các nơ-ron/bộ lọc có ít trọng số chưa cắt tỉa hơn. Điều này ngăn các nơ-ron trở thành chết tĩnh. Khác với các phương pháp hiện có, AP là phương pháp đầu tiên nhắm vào DNR động. Do đó, như một phương pháp hoạt động song song với các phương pháp cắt tỉa hiện có, AP cải thiện các phương pháp cắt tỉa hiện có bằng cách lấp đầy khoảng trống trong việc giảm DNR động, dẫn đến hiệu suất cắt tỉa tốt hơn nhiều.

--- TRANG 8 ---
Tài liệu tham khảo
Arnekvist, I.; Carvalho, J. F.; Kragic, D.; và Stork, J. A. 2020. Hiệu ứng của Chuẩn hóa Mục tiêu và Momentum trên ReLU Chết. CoRR, abs/2005.06195.

Blalock, D.; et al. 2020. Tình trạng Cắt tỉa Mạng Nơ-ron là gì? Trong Kỷ yếu Hệ thống Học Máy (MLSys).

Chen, C.-F. R.; Fan, Q.; và Panda, R. 2021. Crossvit: Cross-attention multi-scale vision transformer cho phân loại hình ảnh. Trong Kỷ yếu Hội nghị Quốc tế IEEE/CVF về Thị giác Máy tính, 357-366.

Chin, T.-W.; Ding, R.; Zhang, C.; và Marculescu, D. 2020. Hướng tới nén mô hình hiệu quả thông qua xếp hạng toàn cục đã học. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), 1518-1528.

Cover, T. M.; và Thomas, J. A. 2006. Các Yếu tố của Lý thuyết Thông tin, ấn bản lần 2. John Wiley & Sons.

Dai, X.; Chen, Y.; Xiao, B.; Chen, D.; Liu, M.; Yuan, L.; và Zhang, L. 2021. Dynamic head: Thống nhất các đầu phát hiện đối tượng với attention. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, 7373-7382.

Deng, J.; et al. 2009. Imagenet: Cơ sở dữ liệu hình ảnh phân cấp quy mô lớn. Trong Hội nghị IEEE 2009 về thị giác máy tính và nhận dạng mẫu, 248-255. IEEE.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh ở quy mô. arXiv preprint arXiv:2010.11929.

Frankle, J.; và Carbin, M. 2019. Giả thuyết Vé Số May Mắn: Tìm Mạng Nơ-ron Thưa thớt, Có thể Đào tạo. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Frankle, J.; Dziugaite, G. K.; Roy, D. M.; và Carbin, M. 2019. Ổn định giả thuyết vé số may mắn. arXiv preprint arXiv:1903.01611.

Frankle, J.; et al. 2020. Kết nối Chế độ Tuyến tính và Giả thuyết Vé Số May Mắn. Trong Kỷ yếu Hội nghị Quốc tế về Học Máy (ICML), 3259-3269.

Glorot, X.; Bordes, A.; và Bengio, Y. 2011. Mạng nơ-ron chỉnh lưu thưa thớt sâu. Trong Kỷ yếu hội nghị quốc tế lần thứ mười bốn về trí tuệ nhân tạo và thống kê, 315-323. JMLR Workshop và Conference Proceedings.

Goodfellow, I.; Bengio, Y.; và Courville, A. 2016. Học sâu. MIT press.

Goswami, S.; Murthy, C. A.; và Das, A. K. 2016. Thước đo Thưa thớt của Đồ thị Mạng: Chỉ số Gini. arXiv:1612.07074.

Han, S.; et al. 2015. Học cả trọng số và kết nối cho mạng nơ-ron hiệu quả. Trong Kỷ yếu Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron (Neurips), 1135-1143.

He, K.; et al. 2016. Học dư sâu cho nhận dạng hình ảnh. Trong Kỷ yếu Hội nghị IEEE về Thị giác Máy tính và Nhận dạng Mẫu, 770-778.

He, Y.; Ding, Y.; Liu, P.; Zhu, L.; Zhang, H.; và Yang, Y. 2020. Học tiêu chí cắt tỉa bộ lọc cho tăng tốc mạng nơ-ron tích chập sâu. Trong Kỷ yếu hội nghị IEEE/CVF về thị giác máy tính và nhận dạng mẫu, 2009-2018.

Hoyer, P. O. 2004. Phân tích nhân tử ma trận không âm với ràng buộc thưa thớt. Tạp chí Nghiên cứu Học Máy, 5(9).

Huang, G.; et al. 2017. Mạng tích chập kết nối dày đặc. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, 4700-4708.

Hurley, N. P.; và Rickard, S. T. 2009. So sánh Các Thước đo Thưa thớt. arXiv:0811.4706.

Joseph, K.; Khan, S.; Khan, F. S.; và Balasubramanian, V. N. 2021. Hướng tới phát hiện đối tượng thế giới mở. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, 5830-5840.

Krizhevsky, A.; et al. 2009. Học nhiều lớp đặc trưng từ hình ảnh nhỏ.

LeCun, Y.; et al. 1998. Học dựa trên gradient áp dụng cho nhận dạng tài liệu. Kỷ yếu IEEE, 86(11): 2278-2324.

Lee, J.; Park, S.; Mo, S.; Ahn, S.; và Shin, J. 2020. Thưa thớt Thích ứng theo Lớp cho Cắt tỉa dựa trên Độ lớn. Trong Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Li, B.; Wu, B.; Su, J.; và Wang, G. 2020a. Eagleeye: Đánh giá mạng con nhanh cho cắt tỉa mạng nơ-ron hiệu quả. Trong Hội nghị châu Âu về thị giác máy tính, 639-654. Springer.

Li, H.; et al. 2017. Cắt tỉa bộ lọc cho convnets hiệu quả. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Li, Y.; Wu, W.; Liu, Z.; Zhang, C.; Zhang, X.; Yao, H.; và Yin, B. 2020b. Cổng Phụ thuộc Trọng số cho Cắt tỉa Mạng Nơ-ron Có thể Vi phân. Trong Hội nghị Châu Âu về Thị giác Máy tính, 23-37. Springer.

Liu, H.; Simonyan, K.; và Yang, Y. 2019. DARTS: Tìm kiếm kiến trúc có thể vi phân. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Liu, S.; Tan, C. M. J.; và Motani, M. 2021. S-Cyc: Lịch Tỷ lệ Học tập cho Cắt tỉa Lặp của Mạng dựa trên ReLU. CoRR, abs/2110.08764.

Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; và Guo, B. 2021. Swin transformer: Vision transformer phân cấp sử dụng cửa sổ dịch chuyển. Trong Kỷ yếu Hội nghị Quốc tế IEEE/CVF về Thị giác Máy tính, 10012-10022.

--- TRANG 9 ---
Liu, Z.; et al. 2019. Suy nghĩ lại về giá trị của cắt tỉa mạng. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Lu, L.; Shin, Y.; Su, Y.; và Karniadakis, G. E. 2019. ReLU chết và khởi tạo: Lý thuyết và ví dụ số. arXiv preprint arXiv:1903.06733.

Luo, J.-H.; và Wu, J. 2020. Cắt tỉa Mạng Nơ-ron với Kết nối Dư và Dữ liệu Hạn chế. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), 1458-1467.

Malach, E.; et al. 2020. Chứng minh giả thuyết vé số may mắn: Cắt tỉa là tất cả những gì bạn cần. Trong Kỷ yếu Hội nghị Quốc tế về Học Máy (ICML), 6682-6691.

Mehta, R. 2019. Học Chuyển giao Thưa thớt thông qua Vé Số May Mắn Chiến thắng. Trong Kỷ yếu Hội thảo Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron về Học Kỹ năng Có thể Chuyển giao.

Park, S.; Lee, J.; Mo, S.; và Shin, J. 2020. Lookahead: Một lựa chọn thay thế có tầm nhìn xa của cắt tỉa dựa trên độ lớn.

Renda, A.; Frankle, J.; và Carbin, M. 2019. So sánh tua lại và tinh chỉnh trong cắt tỉa mạng nơ-ron. Trong Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Ruder, S. 2016. Tổng quan về các thuật toán tối ưu hóa gradient descent. arXiv preprint arXiv:1609.04747.

Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; và Chen, L.-C. 2018. Mobilenetv2: Residual đảo ngược và nút cổ chai tuyến tính. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu, 4510-4520.

Shamir, O.; Sabato, S.; và Tishby, N. 2010. Học và tổng quát hóa với nút cổ chai thông tin. Khoa học Máy tính Lý thuyết, 411(29): 2696-2711.

Simonyan, K.; và Zisserman, A. 2014. Mạng tích chập rất sâu cho nhận dạng hình ảnh quy mô lớn. arXiv preprint arXiv:1409.1556.

Theis, L.; et al. 2018. Dự đoán ánh mắt nhanh hơn với mạng dày đặc và cắt tỉa fisher. arXiv preprint arXiv:1801.05787.

Tishby, N.; và Zaslavsky, N. 2015. Học Sâu và Nguyên lý Nút cổ chai Thông tin. arXiv:1503.02406.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; và Polosukhin, I. 2017. Attention là tất cả những gì bạn cần. Trong Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 5998-6008.

Wang, Y.; et al. 2020. Cắt tỉa từ đầu. Trong Kỷ yếu Hội nghị AAAI về Trí tuệ Nhân tạo, tập 34, 12273-12280.

Ye, M.; et al. 2020. Các mạng con tốt chắc chắn tồn tại: Cắt tỉa thông qua lựa chọn tiến tham lam. Trong Kỷ yếu Hội nghị Quốc tế về Học Máy (ICML), 10820-10830. PMLR.

Yu, H. n.; et al. 2020. Chơi xổ số với phần thưởng và nhiều ngôn ngữ: vé số may mắn trong RL và NLP. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

Yuan, L.; Hou, Q.; Jiang, Z.; Feng, J.; và Yan, S. 2021. Volo: Vision outlooker cho nhận dạng thị giác. arXiv preprint arXiv:2106.13112.

Zhao, C.; et al. 2019. Cắt tỉa mạng nơ-ron tích chập biến phân. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), 2780-2789.

Zhou, H.; et al. 2019. Tháo rời Vé Số May Mắn: Số không, Dấu hiệu và Supermask. Trong Kỷ yếu Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron (Neurips).

Zhu, M.; và Gupta, S. 2018. Cắt tỉa, hay không cắt tỉa: khám phá hiệu quả của cắt tỉa cho nén mô hình. Trong Kỷ yếu Hội nghị Quốc tế về Biểu diễn Học tập (ICLR).

--- TRANG 10 ---
A Chứng minh các Kết quả Lý thuyết
Trong phần này, chúng tôi cung cấp các chứng minh cho các kết quả lý thuyết (Định lý 1 và Hệ quả 1) của bài báo chính.

A.1 Chứng minh Định lý 1
Định lý 1. Chúng tôi được cho luồng tính toán X→T→Y, trong đó T đại diện cho các đặc trưng tại một độ sâu tùy ý trong mạng, được biểu diễn với độ chính xác hữu hạn (ví dụ: float32 hoặc float64). Chúng tôi chỉ xem xét tập con của các cấu hình mạng mà (a) các kích hoạt trong T nhỏ hơn ngưỡng tau và (b) xác suất kích hoạt bằng không của mỗi nơ-ron trong T được giới hạn trên bởi một số pS<1. Gọi dim(T) đại diện cho chiều của T, tức là, số nơ-ron tại độ sâu đó. Khi đó chúng ta có,

I(X;T) <= C × dim(T) × (1-SDNR(T)-DDNR(T)) / (1-1/C * log(1-SDNR(T)/DDNR(T))), (3)

cho một hằng số hữu hạn C chỉ phụ thuộc vào kiến trúc mạng, tau và pS.

Chứng minh. Đầu tiên, lưu ý rằng do độ chính xác hữu hạn T là một biến rời rạc, và do đó I(X;T) = H(T), vì T=f(X) là một hàm xác định của X, trong đó f ký hiệu hàm trong mạng ánh xạ X thành T. Tiếp theo, chúng ta chỉ xem xét các nút trong T không chết tĩnh; tức là chúng không tạo thành một phần của SDNR(T). Gọi chúng là các nút hoạt động. Lưu ý rằng sẽ có k=dim(T)×(1-SDNR(T)) nút hoạt động trong trường hợp này.

Đối với k nút này, gọi p1, p2, ..., pk ký hiệu xác suất mỗi nút sẽ có giá trị bằng không, khi X được rút vô hạn trên toàn bộ phân phối P. Cũng gọi D'DNR(T) = DDNR(T)/(1-SDNR(T)) là tỷ lệ DNR động được điều chỉnh cardinality của mạng đã cắt tỉa. Lưu ý rằng E[pi] = D'DNR(T). Gọi các k nút này được biểu diễn bởi T1, T2, .., Tk cho những gì tiếp theo. Lưu ý rằng giống như T, mỗi Ti sẽ có giá trị rời rạc. Do đó chúng ta có thể viết

H(T) <= Σ H(Ti) (4)

Vì tất cả các kích hoạt nhỏ hơn tau, nếu độ chính xác của biểu diễn là alpha, chúng ta sẽ có tối đa N=tau/alpha số kết quả có thể cho mỗi Ti. Gọi phi_i^0, phi_i^1, ..., phi_i^(N-1) đại diện cho xác suất của Ti là mỗi kết quả rời rạc có thể. Chúng ta có Σj phi_i^j = 1. Lưu ý rằng phi_i^0 = pi.

Bây giờ, chúng ta có thể viết

H(Ti) = pi log(1/pi) + (1-pi) Σ(j=1 to N-1) (phi_i^j/(1-pi)) log(1/phi_i^j) (5)

Ở đây, chúng ta xem xét đại lượng Σ(j=1 to N-1) (phi_i^j/(1-pi)) log(1/phi_i^j). Gọi C là giá trị tối đa có thể mà đại lượng này có thể có, trên tất cả các cấu hình trọng số mạng tuân thủ các ràng buộc được cung cấp trong Định lý. Lưu ý rằng C sẽ chỉ phụ thuộc vào kiến trúc mạng và các tham số tau và pS. Bây giờ chúng tôi sẽ chứng minh rằng C hữu hạn và cung cấp một cận trên cho nó.

Cho pi = phi_i^0, chúng ta có Σ(j>=1) phi_i^j = 1-pi. Do đó, dưới ràng buộc tổng không đổi này, đại lượng Σ(j=1 to N-1) (phi_i^j/(1-pi)) log(1/phi_i^j) sẽ chỉ được tối đa hóa khi phi_i^1 = phi_i^2 = ... = phi_i^(N-1) = (1-pi)/(N-1). Do đó, chúng ta có

C <= Σ(j=1 to N-1) (1/(N-1)) log((N-1)/(1-pi)) (6)
= log((N-1)/(1-pi)) <= log((N-1)/(1-pS)) (7)

Điều này cho thấy C hữu hạn và phụ thuộc vào kiến trúc mạng, pS và tau (ảnh hưởng đến N). Cuối cùng, chúng ta có

Σ(i=1 to k) H(Ti) <= Σ(i=1 to k) [pi log(1/pi) + (1-pi)C] (8)
= Σ(i=1 to k) pi log(1/pi) + k × C × (1-D'DNR(T)) (9)
<= k × D'DNR(T) log(1/D'DNR(T)) + k × C × (1-D'DNR(T)) (10)
= C × dim(T) (1-SDNR(T)) [1-D'DNR(T)/(1-1/C log(1/D'DNR(T)))] (11)

trong đó bước cuối cùng tuân theo định nghĩa của k. Thay thế D'DNR(T) = DDNR(T)/(1-SDNR(T)) cho kết quả dự định.

A.2 Chứng minh Hệ quả 1
Hệ quả 1. Cận trên cho I(X;T) trong Định lý 1 giảm để đáp ứng với sự tăng của cả DDNR(T) và SDNR(T).

Chứng minh. Rõ ràng là việc tăng SDNR(T) chỉ có thể làm giảm cận trên. Gọi

Z = C × dim(T) (1-SDNR(T)) [1-D'DNR(T)/(1-1/C log(1/D'DNR(T)))] (12)

Để đơn giản ký hiệu, gọi beta = C × dim(T) × (1-SDNR(T)). Để điều tra cách cận trên của I(X;T) (ký hiệu là Z) thay đổi với DDNR(T), chúng ta đầu tiên tính đạo hàm của Z đối với D'DNR(T) cho biểu thức sau.

dZ/d(D'DNR(T)) = beta [-1+1/C-1/C log(1/D'DNR(T))-D'DNR(T) × 1/C × 1/D'DNR(T)] (13)
= beta [-1-1/C+1/C log(1/D'DNR(T))] (14)

Để dZ/d(D'DNR(T)) nhỏ hơn hoặc bằng 0, chúng ta phải có

C >= log(1/D'DNR(T)) - 1 (15)

Trong những gì tiếp theo, chúng tôi sẽ cho thấy rằng C >= log(1/D'DNR(T)). Vui lòng tham khảo chứng minh của Định lý 1 cho các định nghĩa.

Đối với mỗi đầu ra nút được biểu diễn trong T1, T2, .., Tk, chúng ta thêm một bias âm -1/alpha và đảo dấu của tất cả các trọng số trỏ vào mỗi nút này. Chúng tôi lưu ý rằng việc thực hiện thay đổi này sẽ cho Σ(j=1 to N-1) phi_i^j = pi. Khi đó chúng ta sẽ có

Σ(j=1 to N-1) (phi_i^j/pi) log(1/phi_i^j) >= log(1/pi) (16)

trong đó cận dưới đạt được bằng cách chỉ điền xác suất pi còn lại vào một bin duy nhất. Bây giờ, vì C đại diện cho giá trị tối đa có thể mà đại lượng được giới hạn trong (16) có thể có, chúng ta tự nhiên có C >= log(1/pi) cũng vậy, cho tất cả i. Vì E[pi] = D'DNR(T), sau đó cũng tuân theo rằng C >= log(1/D'DNR(T)). Điều này chứng minh kết quả dự định của chúng tôi và cho dZ/d(D'DNR(T)) <= 0.

Do đó, cho rằng D'DNR(T) = DDNR(T)/(1-SDNR(T)), một giá trị lớn hơn của cả DDNR và SDNR sẽ dẫn đến một giá trị lớn hơn của D'DNR(T), làm giảm Z (tức là, cận trên của I(X;Z)) và tăng nguy cơ nén quá mức.

--- TRANG 11 ---
[Phần còn lại của tài liệu tiếp tục với các kết quả thí nghiệm bổ sung trong Phụ lục...]

--- TRANG 12 ---
B Kết quả Thí nghiệm Bổ sung
Trong Phụ lục, chúng tôi hiển thị một số kết quả thí nghiệm bổ sung. Cụ thể,

1. Trong Phần B.1, chúng tôi lặp lại thí nghiệm DNR sử dụng VGG-19 trên bộ dữ liệu CIFAR-10 với phương pháp cắt tỉa Gradient Toàn cục.

2. Trong Phần B.2, chúng tôi trình bày chi tiết triển khai được sử dụng trong Phần Đánh giá Hiệu suất (tức là, Phần 4) và chứng minh hiệu suất của AP cho nhiều giá trị lambda hơn.

3. Trong Phần B.3, chúng tôi hiển thị kết quả nghiên cứu loại bỏ sử dụng VGG-19 trên CIFAR-10 với phương pháp cắt tỉa Gradient Toàn cục.

4. Trong Phần B.4, chúng tôi đánh giá hiệu quả của tỷ lệ cắt tỉa, q, đối với hiệu suất cắt tỉa sử dụng ResNet-20 và VGG-19 trên bộ dữ liệu CIFAR-10.

5. Trong Phần B.5, chúng tôi đánh giá hiệu quả của AP đối với việc giảm DNR động trên hiệu suất cắt tỉa sử dụng ResNet-20 và VGG-19 trên bộ dữ liệu CIFAR-10.

[Phần còn lại của tài liệu tiếp tục với các chi tiết và bảng kết quả bổ sung...]
