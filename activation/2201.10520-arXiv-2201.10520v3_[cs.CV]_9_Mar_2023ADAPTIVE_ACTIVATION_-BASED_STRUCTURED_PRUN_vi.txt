# 2201.10520.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/activation/2201.10520.pdf
# Kích thước tệp: 749043 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2201.10520v3 [cs.CV] 9 Mar 2023 CẮT TỈA CẤU TRÚC DỰA TRÊN KÍCH HOẠT THÍCH ỨNG

Kaiqi Zhao1, Animesh Jain2, Ming Zhao1
1Đại học bang Arizona 2Facebook
kzhao27@asu.edu, anijain@umich.edu, mingzhao@asu.edu

TÓM TẮT
Cắt tỉa là một phương pháp đầy hứa hẹn để nén các mô hình học sâu phức tạp nhằm triển khai chúng trên các thiết bị biên có tài nguyên hạn chế. Tuy nhiên, nhiều giải pháp cắt tỉa hiện tại dựa trên cắt tỉa không có cấu trúc, tạo ra các mô hình không thể chạy hiệu quả trên phần cứng thông thường, và yêu cầu người dùng phải khám phá và điều chỉnh quá trình cắt tỉa thủ công, điều này tốn thời gian và thường dẫn đến kết quả không tối ưu. Để giải quyết những hạn chế này, bài báo này trình bày một phương pháp cắt tỉa có cấu trúc thích ứng dựa trên kích hoạt để tự động và hiệu quả tạo ra các mô hình nhỏ, chính xác và hiệu quả về mặt phần cứng đáp ứng yêu cầu của người dùng. Đầu tiên, nó đề xuất cắt tỉa có cấu trúc lặp đi lặp lại bằng cách sử dụng bản đồ đặc trưng chú ý dựa trên kích hoạt để xác định và cắt tỉa hiệu quả các bộ lọc không quan trọng. Sau đó, nó đề xuất các chính sách cắt tỉa thích ứng để tự động đáp ứng các mục tiêu cắt tỉa của các nhiệm vụ quan trọng về độ chính xác, hạn chế về bộ nhớ và nhạy cảm về độ trễ. Một đánh giá toàn diện cho thấy phương pháp được đề xuất có thể vượt trội đáng kể so với các công trình cắt tỉa có cấu trúc hiện đại nhất trên các bộ dữ liệu CIFAR-10 và ImageNet. Ví dụ, trên ResNet-56 với CIFAR-10, mà không có bất kỳ sụt giảm độ chính xác nào, phương pháp của chúng tôi đạt được việc giảm tham số lớn nhất (79.11%), vượt trội hơn các công trình liên quan từ 22.81% đến 66.07%, và việc giảm FLOPs lớn nhất (70.13%), vượt trội hơn các công trình liên quan từ 14.13% đến 26.53%.

1 GIỚI THIỆU
Các mạng neural sâu (DNN) có yêu cầu tính toán và bộ nhớ đáng kể. Khi học sâu trở nên phổ biến và chuyển hướng về các thiết bị biên, việc triển khai DNN trở nên khó khăn hơn do sự không khớp giữa các DNN đòi hỏi tài nguyên cao và các thiết bị biên có tài nguyên hạn chế. Cắt tỉa DNN là một phương pháp đầy hứa hẹn (Li et al. (2016); Han et al. (2015); Molchanov et al. (2016); Theis et al. (2018); Renda et al. (2020)), xác định các tham số (hoặc phần tử trọng số) không đóng góp đáng kể vào độ chính xác và cắt tỉa chúng khỏi mạng. Gần đây, các công trình dựa trên Giả thuyết Vé số trúng thưởng (LTH) đã đạt được thành công lớn trong việc tạo ra các mô hình nhỏ hơn và chính xác hơn thông qua cắt tỉa lặp đi lặp lại với quay lại (Frankle & Carbin (2018)). Tuy nhiên, LTH chỉ được chứng minh hoạt động thành công với cắt tỉa không có cấu trúc, điều này không may dẫn đến các mô hình có độ thưa thớt thấp và khó tăng tốc trên phần cứng thông thường như CPU và GPU (ví dụ, Hill et al. (2017) cho thấy việc áp dụng trực tiếp NVIDIA cuSPARSE trên các mô hình cắt tỉa không có cấu trúc có thể dẫn đến chậm 60 lần trên GPU so với các kernel dày đặc.) Hơn nữa, hầu hết các phương pháp cắt tỉa yêu cầu người dùng khám phá và điều chỉnh nhiều siêu tham số, ví dụ, với cắt tỉa lặp đi lặp lại dựa trên LTH, người dùng cần xác định bao nhiêu tham số cần cắt tỉa trong mỗi vòng. Điều chỉnh quá trình cắt tỉa tốn thời gian và thường dẫn đến kết quả không tối ưu.

Chúng tôi đề xuất cắt tỉa có cấu trúc lặp đi lặp lại thích ứng dựa trên kích hoạt để tìm các mô hình "vé trúng thưởng" vừa hiệu quả về mặt phần cứng vừa tự động đáp ứng các yêu cầu về độ chính xác, kích thước và tốc độ của mô hình. Đầu tiên, chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc dựa trên kích hoạt để xác định và loại bỏ các bộ lọc không quan trọng trong một quá trình cắt tỉa lặp đi lặp lại dựa trên LTH (với quay lại). Cụ thể, chúng tôi định nghĩa thích hợp một hàm ánh xạ chú ý nhận đầu vào là bản đồ đặc trưng kích hoạt 2D của một bộ lọc, và đưa ra giá trị 1D được sử dụng để chỉ ra tầm quan trọng của bộ lọc. Phương pháp này hiệu quả hơn so với cắt tỉa bộ lọc dựa trên giá trị trọng số bởi vì các giá trị chú ý dựa trên kích hoạt không chỉ nắm bắt các đặc trưng của đầu vào mà còn chứa thông tin của các lớp tích chập hoạt động như các bộ phát hiện đặc trưng cho các nhiệm vụ dự đoán. Sau đó, chúng tôi tích hợp phương pháp dựa trên chú ý này vào khung công tác cắt tỉa lặp đi lặp lại dựa trên LTH để cắt tỉa các bộ lọc trong mỗi vòng và tìm vé trúng thưởng nhỏ, chính xác và hiệu quả về mặt phần cứng.

--- TRANG 2 ---
Thứ hai, chúng tôi đề xuất cắt tỉa thích ứng tự động tối ưu hóa quá trình cắt tỉa theo các mục tiêu khác nhau của người dùng. Đối với các tình huống nhạy cảm về độ trễ như trợ lý ảo tương tác, chúng tôi đề xuất cắt tỉa đảm bảo FLOPs để đạt được độ chính xác tốt nhất với lượng FLOPs tính toán tối đa; Đối với môi trường hạn chế bộ nhớ như hệ thống nhúng, chúng tôi đề xuất cắt tỉa đảm bảo kích thước mô hình để đạt được độ chính xác tốt nhất với lượng dấu chân bộ nhớ tối đa; Đối với các ứng dụng quan trọng về độ chính xác như những ứng dụng trên xe tự lái, chúng tôi đề xuất cắt tỉa đảm bảo độ chính xác để tạo ra mô hình hiệu quả tài nguyên nhất với mức độ mất mát độ chính xác có thể chấp nhận. Nhắm đến các mục tiêu khác nhau, phương pháp của chúng tôi điều khiển thích ứng tính tích cực của cắt tỉa bằng cách điều chỉnh ngưỡng toàn cục được sử dụng để cắt tỉa các bộ lọc. Hơn nữa, nó xem xét sự khác biệt trong đóng góp của mỗi lớp đối với kích thước mô hình và độ phức tạp tính toán và sử dụng ngưỡng theo lớp, được tính bằng cách chia số tham số hoặc FLOPs còn lại của mỗi lớp cho số tham số hoặc FLOPs còn lại của toàn bộ mô hình, để cắt tỉa mỗi lớp với mức độ tích cực khác nhau.

Kết quả của chúng tôi vượt trội đáng kể so với các công trình liên quan trong tất cả các trường hợp nhắm đến mất mát độ chính xác, giảm tham số và giảm FLOPs. Ví dụ, trên ResNet-56 với bộ dữ liệu CIFAR-10, mà không có sụt giảm độ chính xác, phương pháp của chúng tôi đạt được việc giảm tham số lớn nhất (79.11%), vượt trội hơn các công trình liên quan từ 22.81% đến 66.07%, và việc giảm FLOPs lớn nhất (70.13%), vượt trội hơn các công trình liên quan từ 14.13% đến 26.53%. Ngoài ra, phương pháp của chúng tôi cho phép một mô hình đã cắt tỉa đạt độ chính xác cao hơn 0.6% hoặc 1.08% so với mô hình gốc nhưng chỉ với 30% hoặc 50% tham số của mô hình gốc. Trên ResNet-50 trên ImageNet, với cùng mức độ giảm tham số và FLOPs, phương pháp của chúng tôi đạt được mất mát độ chính xác nhỏ nhất, thấp hơn các công trình liên quan từ 0.08% đến 3.21%; và với cùng mức độ mất mát độ chính xác, phương pháp của chúng tôi giảm đáng kể nhiều tham số hơn (6.45% đến 29.61% cao hơn so với các công trình liên quan) và nhiều FLOPs hơn (0.82% đến 17.2% cao hơn so với các công trình liên quan).

2 BỐI CẢNH VÀ CÁC CÔNG TRÌNH LIÊN QUAN

Cắt tỉa không có cấu trúc so với có cấu trúc. Cắt tỉa không có cấu trúc (LeCun et al. (1990); Han et al. (2015); Molchanov et al. (2017)) là một phương pháp tinh vi cắt tỉa các phần tử không quan trọng riêng lẻ trong tensor trọng số. Nó có ít tác động đến độ chính xác mô hình, so với cắt tỉa có cấu trúc, nhưng các mô hình cắt tỉa không có cấu trúc khó tăng tốc trên phần cứng thông thường. Cắt tỉa có cấu trúc là một phương pháp thô cắt tỉa toàn bộ các vùng thường xuyên của tensor trọng số theo một số phương pháp heuristic dựa trên quy tắc, như L1-norm (Li et al. (2016)), tỷ lệ phần trăm trung bình của số không (Molchanov et al. (2016)), và các thông tin khác xem xét mối quan hệ giữa các lớp lân cận (Theis et al. (2018); Lee et al. (2018)). So với cắt tỉa không có cấu trúc, việc cắt tỉa một mô hình mà không gây mất mát độ chính xác bằng cắt tỉa có cấu trúc khó khăn hơn, bởi vì bằng cách loại bỏ toàn bộ các vùng, nó có thể loại bỏ các phần tử trọng số quan trọng đối với độ chính xác cuối cùng (Li et al. (2016)). Tuy nhiên, các mô hình cắt tỉa có cấu trúc có thể được ánh xạ dễ dàng vào phần cứng đa mục đích và được tăng tốc trực tiếp với phần cứng và thư viện có sẵn (He et al. (2018b)).

Cắt tỉa một lần so với lặp đi lặp lại. Cắt tỉa một lần cắt tỉa một mô hình đã được huấn luyện trước và sau đó huấn luyện lại một lần, trong khi cắt tỉa lặp đi lặp lại cắt tỉa và huấn luyện lại mô hình trong nhiều vòng. Cả hai kỹ thuật đều có thể chọn kỹ thuật cắt tỉa có cấu trúc hoặc không có cấu trúc. Gần đây, các công trình dựa trên Giả thuyết Vé số trúng thưởng (LTH) đã đạt được thành công lớn trong việc tạo ra các mô hình nhỏ hơn và chính xác hơn thông qua cắt tỉa lặp đi lặp lại với quay lại (Frankle & Carbin (2018)). LTH đề xuất rằng một mạng khởi tạo ngẫu nhiên dày đặc có một mạng con, được gọi là vé trúng thưởng, có thể đạt được độ chính xác tương đương với mạng gốc. Ở đầu mỗi vòng cắt tỉa, nó quay lại các trọng số và/hoặc tốc độ học của mạng con về một epoch sớm nào đó của giai đoạn huấn luyện của mô hình gốc để giảm khoảng cách giữa mạng con và mô hình gốc và tăng cơ hội tìm ra vé trúng thưởng. Tuy nhiên, hầu hết các công trình dựa trên LTH chỉ xem xét cắt tỉa không có cấu trúc, ví dụ, Cắt tỉa Độ lớn Lặp đi lặp lại (IMP) (Frankle & Carbin (2018); Frankle et al. (2019)), như đã thảo luận ở trên, là không hiệu quả về mặt phần cứng.

Việc thiết kế một phương pháp cắt tỉa lặp đi lặp lại với cắt tỉa có cấu trúc là không đơn giản. Để hiểu tình trạng của cắt tỉa có cấu trúc lặp đi lặp lại, chúng tôi đã thử nghiệm với đối tác cắt tỉa có cấu trúc của IMP - cắt tỉa có cấu trúc L1-norm (ILP) (Li et al. (2016)) loại bỏ toàn bộ các bộ lọc tùy thuộc vào giá trị L1-norm của chúng. Chúng tôi quan sát thấy rằng ILP không thể cắt tỉa hiệu quả một mô hình trong khi duy trì độ chính xác của nó,

--- TRANG 3 ---
Thuật toán 1 Thuật toán Cắt tỉa Có cấu trúc Lặp đi lặp lại Thích ứng
1: [Khởi tạo] Khởi tạo mạng f(x;M₀⊙W₀⁰) với mặt nạ ban đầu M₀ ∈ {0,1}^|W₀⁰| và Ngưỡng T(0)
2: [Lưu trọng số] Huấn luyện mạng trong k epochs, tạo ra mạng f(x;M₀⊙W₀ᵏ), và lưu trọng số W₀ᵏ
3: [Huấn luyện đến hội tụ] Huấn luyện mạng trong T-k epochs để hội tụ, tạo ra mạng f(x;M₀⊙W₀ᵀ)
4: for vòng cắt tỉa r (r≥1) do
5: [Cắt tỉa] Cắt tỉa các bộ lọc từ W_{r-1}^T sử dụng T(r), tạo ra mặt nạ Mᵣ, và mạng f(x;Mᵣ⊙W_{r-1}^T)
6: [Quay lại Trọng số] Đặt lại các bộ lọc còn lại về W₀ᵏ tại epoch k, tạo ra mạng f(x;Mᵣ⊙W₀ᵏ)
7: [Quay lại Tốc độ học] Đặt lại lịch trình tốc độ học về trạng thái từ epoch k
8: [Huấn luyện lại] Huấn luyện lại các bộ lọc chưa cắt tỉa trong T-k epoch để hội tụ, tạo ra mạng f(x;Mᵣ⊙Wᵣᵀ)
9: [Đánh giá] Đánh giá mạng đã huấn luyện lại f(x;Mᵣ⊙Wᵣᵀ) theo mục tiêu.
10: [Đặt lại Trọng số] Nếu mục tiêu không đạt được, đặt lại trọng số về một vòng trước đó
11: [Điều chỉnh Ngưỡng] Tính toán Ngưỡng T(r+1) cho vòng cắt tỉa tiếp theo
12: end for

[Hình ảnh mô tả]: (a) Đầu ra kích hoạt của 16 bộ lọc của một lớp conv2d, và (b) tensor đầu vào và đầu ra của một lớp tích chập. Trong (a), phần bên trái hiển thị hình ảnh gốc và hình ảnh sau tăng cường dữ liệu; và phần bên phải trực quan hóa các đầu ra kích hoạt của mỗi bộ lọc.

ví dụ, ILP có thể cắt tỉa ResNet-50 tối đa chỉ 11.5% tham số khi mất mát độ chính xác tối đa được giới hạn ở 1% trên ImageNet. Do đó, việc áp dụng trực tiếp cắt tỉa lặp đi lặp lại với các phương pháp cắt tỉa có cấu trúc dựa trên độ lớn trọng số hiện có không tạo ra các mô hình cắt tỉa chính xác. Trong bài báo này, chúng tôi nghiên cứu cách tạo ra các mô hình nhỏ, chính xác và hiệu quả về mặt phần cứng dựa trên cắt tỉa có cấu trúc lặp đi lặp lại với quay lại.

Cắt tỉa Tự động. Để cắt tỉa trở nên hữu ích trong thực tế, điều quan trọng là tự động đáp ứng các mục tiêu cắt tỉa cho các ứng dụng và thiết bị ML đa dạng. Hầu hết các phương pháp cắt tỉa yêu cầu người dùng khám phá và điều chỉnh nhiều siêu tham số, ví dụ, với cắt tỉa lặp đi lặp lại dựa trên LTH, người dùng cần xác định bao nhiêu tham số cần cắt tỉa trong mỗi vòng. Điều chỉnh quá trình cắt tỉa tốn thời gian và thường dẫn đến kết quả không tối ưu. Do đó, chúng tôi nghiên cứu cách tự động thích ứng quá trình cắt tỉa để đáp ứng các mục tiêu cắt tỉa mà không cần can thiệp của người dùng.

Một số công trình (Zoph et al. (2018); Cai et al.; Ashok et al. (2017)) sử dụng các thuật toán học tăng cường để tìm các kiến trúc hiệu quả về mặt tính toán. Các công trình này thuộc lĩnh vực Tìm kiếm Kiến trúc Neural (NAS), trong đó gần đây nhất là AutoML cho Nén Mô hình (AMC) (He et al. (2018b)). AMC cho phép mô hình đạt đến tăng tốc mục tiêu bằng cách giới hạn không gian hành động (tỷ lệ thưa thớt cho mỗi lớp), và nó tìm ra giới hạn nén không gây mất mát độ chính xác bằng cách điều chỉnh hàm phần thưởng. Nhưng phương pháp này phải khám phá trên một không gian tìm kiếm lớn của tất cả thưa thớt theo lớp có sẵn, điều này tốn thời gian khi mạng neural lớn và bộ dữ liệu phức tạp.

3 PHƯƠNG PHÁP LUẬN

Thuật toán 1 minh họa dòng chảy tổng thể của phương pháp cắt tỉa có cấu trúc thích ứng dựa trên kích hoạt được đề xuất. Để biểu diễn cắt tỉa trọng số, chúng tôi sử dụng mặt nạ Mᵣ ∈ {0,1}^d cho mỗi tensor trọng số Wᵣᵗ ∈ ℝ^d, trong đó r là số vòng cắt tỉa và t là epoch huấn luyện. Do đó, mạng đã cắt tỉa ở cuối epoch huấn luyện T được biểu diễn bởi tích theo phần tử Mᵣ⊙Wᵣᵀ. Ba bước đầu tiên là huấn luyện mô hình gốc đến hoàn thành, trong khi lưu trọng số tại Epoch k. Các bước 4-10 đại diện cho một vòng cắt tỉa. Bước 4 cắt tỉa mô hình (thảo luận trong Phần 3.1). Bước 5 (tùy chọn) và 6 thực hiện quay lại. Bước 7 huấn luyện lại mô hình đã cắt tỉa trong T-k epochs còn lại. Bước 8 đánh giá mô hình đã cắt tỉa theo mục tiêu cắt tỉa. Nếu mục tiêu không đạt được, Bước 9 đặt lại trọng số về một vòng trước đó. Bước 10 tính toán ngưỡng cắt tỉa cho vòng cắt tỉa tiếp theo theo chính sách cắt tỉa thích ứng (thảo luận trong Phần 3.2).

3.1 CẮT TỈAM BỘ LỌC DỰA TRÊN KÍCH HOẠT

Để thực hiện cắt tỉa có cấu trúc lặp đi lặp lại, người ta có thể bắt đầu với các phương pháp cắt tỉa có cấu trúc hiện đại nhất và áp dụng nó một cách lặp đi lặp lại. Phương pháp cắt tỉa có cấu trúc được sử dụng rộng rãi - cắt tỉa có cấu trúc dựa trên L1-norm Renda et al. (2020) loại bỏ các bộ lọc có giá trị L1-norm thấp nhất, trong khi phương pháp gần đây nhất - cắt tỉa có cấu trúc dựa trên Phân cực Zhuang et al. (2020) cải thiện nó bằng cách sử dụng một bộ điều chỉnh trên các yếu tố tỷ lệ của bộ lọc và cắt tỉa các bộ lọc có yếu tố tỷ lệ thấp hơn ngưỡng. Cả hai phương pháp này đều giả định rằng các giá trị trọng số của một bộ lọc có thể được sử dụng làm chỉ báo về tầm quan trọng của bộ lọc đó, giống như cách LTH sử dụng giá trị trọng số trong cắt tỉa không có cấu trúc. Tuy nhiên, chúng tôi quan sát thấy rằng các phương pháp cắt tỉa có cấu trúc dựa trên trọng số không thể tạo ra các mô hình cắt tỉa chính xác. Ví dụ, để cắt tỉa ResNet-56 trên CIFAR-10 mà không có mất mát trong độ chính xác top-1, cắt tỉa có cấu trúc dựa trên L1-norm chỉ có thể đạt được tối đa chỉ 1.15× nén mô hình, và cắt tỉa có cấu trúc dựa trên Phân cực-norm chỉ có thể đạt được tối đa chỉ 1.89× tăng tốc suy luận. Lý do là, một số bộ lọc, mặc dù giá trị trọng số của chúng nhỏ, vẫn có thể tạo ra các giá trị kích hoạt khác không hữu ích quan trọng cho việc học đặc trưng trong quá trình lan truyền ngược. Tức là, các bộ lọc có giá trị nhỏ có thể có kích hoạt lớn.

Chúng tôi đề xuất rằng các giá trị kích hoạt của bộ lọc hiệu quả hơn trong việc tìm các bộ lọc không quan trọng để cắt tỉa. Các kích hoạt như ReLu cho phép các phép toán phi tuyến tính, và cho phép các lớp tích chập hoạt động như các bộ phát hiện đặc trưng. Nếu một giá trị kích hoạt nhỏ, thì bộ phát hiện đặc trưng tương ứng của nó không quan trọng cho các nhiệm vụ dự đoán. Vì vậy các giá trị kích hoạt, tức là, các tensor đầu ra trung gian sau kích hoạt phi tuyến tính, không chỉ phát hiện các đặc trưng của tập dữ liệu huấn luyện, mà còn chứa thông tin của các lớp tích chập hoạt động như các bộ phát hiện đặc trưng cho các nhiệm vụ dự đoán. Chúng tôi trình bày một động lực trực quan trong Hình 1(a). Hình này hiển thị đầu ra kích hoạt của 16 bộ lọc của một lớp tích chập trên một hình ảnh đầu vào. Hình ảnh đầu tiên bên trái là hình ảnh gốc, và hình ảnh thứ hai là các đặc trưng đầu vào sau tăng cường dữ liệu. Chúng tôi quan sát thấy một số bộ lọc trích xuất các đặc trưng hình ảnh với các mẫu kích hoạt cao, ví dụ, bộ lọc thứ 6 và 12. Để so sánh, các đầu ra kích hoạt của một số bộ lọc gần bằng không, như bộ lọc thứ 2, 14, và 16. Do đó, từ kiểm tra trực quan, việc loại bỏ các bộ lọc có mẫu kích hoạt yếu có khả năng có tác động thấp đến độ chính xác cuối cùng của mô hình đã cắt tỉa.

Có một kết nối tự nhiên giữa phương pháp cắt tỉa dựa trên kích hoạt của chúng tôi và các công trình truyền tải kiến thức dựa trên chú ý liên quan (Zagoruyko & Komodakis (2016)). Chú ý là một hàm ánh xạ được sử dụng để tính toán thống kê của mỗi phần tử trong các kích hoạt trên chiều kênh. Bằng cách giảm thiểu sự khác biệt của các bản đồ chú ý dựa trên kích hoạt từ các lớp trung gian giữa mô hình giáo viên và mô hình học sinh, truyền tải chú ý cho phép học sinh bắt chước hành vi của giáo viên. Phương pháp cắt tỉa được đề xuất của chúng tôi xây dựng dựa trên cùng lý thuyết rằng chú ý dựa trên kích hoạt là một chỉ báo tốt của các bộ lọc về khả năng nắm bắt đặc trưng của chúng, và nó giải quyết các thách thức mới trong việc sử dụng kích hoạt để hướng dẫn cắt tỉa có cấu trúc tự động.

Trong phần sau, chúng tôi mô tả cách cắt tỉa các bộ lọc dựa trên bản đồ đặc trưng kích hoạt của nó trong mỗi vòng của quá trình cắt tỉa có cấu trúc lặp đi lặp lại. Hình 1(b) hiển thị các đầu vào và đầu ra của một lớp tích chập 2D (được gọi là conv2d), theo sau bởi một lớp kích hoạt. Đối với lớp conv2d thứ i, cho Xᵢ ∈ ℝⁿⁱ⁻¹ˣʰⁱ⁻¹ˣʷⁱ⁻¹ biểu thị các đặc trưng đầu vào, và Fⱼⁱ ∈ ℝⁿⁱ⁻¹ˣᵏⁱˣᵏⁱ là bộ lọc thứ j, trong đó hᵢ₋₁ và wᵢ₋₁ lần lượt là chiều cao và chiều rộng của các đặc trưng đầu vào, nᵢ₋₁ là số kênh đầu vào, nᵢ là số kênh đầu ra, và kᵢ là kích thước kernel của bộ lọc. Kích hoạt của bộ lọc thứ j Fⱼⁱ sau ánh xạ ReLu do đó được ký hiệu bởi Aⱼⁱ ∈ ℝʰⁱˣʷⁱ.

Hàm ánh xạ chú ý nhận đầu vào là kích hoạt 2D Aⱼⁱ ∈ ℝʰⁱˣʷⁱ của bộ lọc Fⱼⁱ, và đưa ra giá trị 1D sẽ được sử dụng làm chỉ báo về tầm quan trọng của các bộ lọc. Chúng tôi xem xét ba dạng của bản đồ chú ý dựa trên kích hoạt, trong đó p≥1 và aᵢₖ,ₗ biểu thị mọi phần tử của Aⱼⁱ: 1) Trung bình của các giá trị chú ý được nâng lên lũy thừa p: Fₘₑₐₙ(Aⱼⁱ) = 1/(hᵢ×wᵢ) Σₖ₌₁ʰⁱ Σₗ₌₁ʷⁱ |aᵢₖ,ₗ|ᵖ; 2) Max của các giá

--- TRANG 4 ---
Thuật toán 2 Cắt tỉa Thích ứng Đảm bảo Độ chính xác
1: Input: Mục tiêu Mất mát Độ chính xác AccLossTarget
2: Output: Một mô hình cắt tỉa nhỏ với độ chính xác có thể chấp nhận
3: Initialize: T = 0.0, lambda = 0.005.
4: for vòng cắt tỉa r (r≥1) do
5: Cắt tỉa mô hình sử dụng T(r) (Tham khảo Thuật toán 3)
6: Huấn luyện mô hình đã cắt tỉa, đánh giá độ chính xác Acc(r) của nó
7: Tính toán mất mát độ chính xác AccLoss(r): AccLoss(r) = Acc(0) - Acc(r)
8: if AccLoss(r) < AccLossTarget then
9: if các thay đổi về kích thước mô hình nằm trong 0.1% cho nhiều vòng then
10: Terminate
11: else
12: lambda(r+1) = lambda(r)
13: T(r+1) = T(r) + lambda(r+1)
14: end if
15: else
16: Tìm vòng chấp nhận được cuối cùng k
17: if k đã được sử dụng để quay lại nhiều lần then
18: Đánh dấu k là không chấp nhận được
19: Đi đến Bước 15
20: else
21: Quay lại trọng số mô hình về vòng k
22: lambda(r+1) = lambda(r)/2.0^(N+1) (N là số lần quay lại về vòng k)
23: T(r+1) = T(k) + lambda(r+1)
24: end if
25: end if
26: end for

trị chú ý được nâng lên lũy thừa p: Fₘₐₓ(Aⱼⁱ) = max_{l=1,...,hᵢ×wᵢ}|aᵢₖ,ₗ|ᵖ; và 3) Tổng của các giá trị chú ý được nâng lên lũy thừa p: Fₛᵤₘ(Aⱼⁱ) = Σₖ₌₁ʰⁱ Σₗ₌₁ʷⁱ |aᵢₖ,ₗ|ᵖ. Chúng tôi chọn Fₘₑₐₙ(Aⱼⁱ) với p bằng 1 làm chỉ báo để xác định và cắt tỉa các bộ lọc không quan trọng, và phương pháp của chúng tôi loại bỏ các bộ lọc có giá trị chú ý thấp hơn ngưỡng cắt tỉa. Xem Phần 5 để nghiên cứu ablation về những lựa chọn này.

3.2 CẮT TỈAA LẶP ĐI LẶP LẠI THÍCH ỨNG

Phương pháp cắt tỉa của chúng tôi là tự động và hiệu quả tạo ra một mô hình đã cắt tỉa đáp ứng các mục tiêu khác nhau của người dùng. Cắt tỉa tự động có nghĩa là người dùng không phải tìm ra cách cấu hình quá trình cắt tỉa. Cắt tỉa hiệu quả có nghĩa là quá trình cắt tỉa nên tạo ra mô hình mong muốn của người dùng càng nhanh càng tốt. Các mục tiêu cắt tỉa của người dùng có thể thay đổi tùy thuộc vào các tình huống sử dụng: 1) Các nhiệm vụ quan trọng về độ chính xác, như những nhiệm vụ được sử dụng bởi xe tự lái có yêu cầu độ chính xác nghiêm ngặt, quan trọng cho an toàn, nhưng không có giới hạn nghiêm ngặt về việc sử dụng tính toán và lưu trữ; 2) Các nhiệm vụ hạn chế bộ nhớ, như những nhiệm vụ được triển khai trên vi điều khiển có bộ nhớ rất hạn chế để lưu trữ các mô hình nhưng không có yêu cầu độ chính xác nghiêm ngặt; và 3) Các nhiệm vụ nhạy cảm về độ trễ, như những nhiệm vụ được sử dụng bởi trợ lý ảo nơi phản hồi kịp thời là mong muốn nhưng độ chính xác không phải là ràng buộc cứng.

Để đạt được cắt tỉa có cấu trúc tự động và hiệu quả, chúng tôi đề xuất ba chính sách cắt tỉa thích ứng để cung cấp 1) Cắt tỉa Thích ứng Đảm bảo Độ chính xác tạo ra mô hình hiệu quả tài nguyên nhất với mất mát độ chính xác có thể chấp nhận; 2) Cắt tỉa Thích ứng Hạn chế Bộ nhớ tạo ra mô hình chính xác nhất trong một dấu chân bộ nhớ nhất định; và 3) Cắt tỉa Thích ứng Hạn chế FLOPs tạo ra mô hình chính xác nhất trong một cường độ tính toán nhất định. Cụ thể, phương pháp cắt tỉa thích ứng được đề xuất của chúng tôi tự động điều chỉnh ngưỡng toàn cục (T), được sử dụng trong thuật toán cắt tỉa có cấu trúc lặp đi lặp lại của chúng tôi (Thuật toán 1) để nhanh chóng tìm ra mô hình đáp ứng mục tiêu cắt tỉa. Các mục tiêu khác (ví dụ, giới hạn tiêu thụ năng lượng của mô hình) cũng có thể được hỗ trợ bằng cách đơn giản cắm các chỉ số liên quan vào thuật toán cắt tỉa thích ứng của chúng tôi. Chúng tôi lấy Cắt tỉa Thích ứng Đảm bảo Độ chính xác, được mô tả trong Thuật toán 2, làm ví dụ để hiển thị quy trình cắt tỉa thích ứng. Các thuật toán Cắt tỉa Thích ứng Hạn chế Bộ nhớ và Hạn chế FLOPs được bao gồm trong Phụ lục A.3 và A.4, tương ứng.

--- TRANG 5 ---
Thuật toán 3 Điều chỉnh Ngưỡng Theo Lớp
1: for lớp i trong một vòng cắt tỉa r do
2: Tính toán số tham số của các bộ lọc chưa cắt tỉa của lớp hiện tại và toàn bộ mô hình, tương ứng: Nᵢ(r) = n(i-1)(r) × kᵢ(r) × kᵢ(r) × nᵢ(r), NTotal(r) = Σᵢ₌₁ᴹ n(i-1)(r) × kᵢ(r) × kᵢ(r) × nᵢ(r), trong đó M là số lớp tích chập, và cho các ký hiệu khác tham khảo Phần 3.1.
3: Tính toán ngưỡng Tᵢ(r) của lớp hiện tại: Tᵢ(r) = T(r) × Nᵢ(r)/NTotal(r).
4: Đối với mỗi bộ lọc Fⱼⁱ trong Fⁱ, tính toán chú ý của nó: Fmean(Aⱼⁱ) = 1/(hᵢ×wᵢ) Σₖ₌₁ʰⁱ Σₗ₌₁ʷⁱ |aᵢₖ,ₗ|ᵖ.
5: Cắt tỉa bộ lọc nếu chú ý của nó không lớn hơn ngưỡng Tᵢ(r), tức là, đặt Mⱼⁱ = 0 nếu Fmean(Aⱼⁱ) ≤ Tᵢ(r); Ngược lại, đặt Mⱼⁱ = 1
6: end for

Trong Thuật toán 2, mục tiêu là đảm bảo mất mát độ chính xác trong khi giảm thiểu kích thước mô hình (thuật toán để giảm thiểu FLOPs mô hình tương tự). Trong thuật toán, T kiểm soát tính tích cực của cắt tỉa, và lambda xác định mức tăng của T tại mỗi vòng cắt tỉa. Cắt tỉa bắt đầu một cách thận trọng, với T được khởi tạo thành 0, để chỉ những bộ lọc hoàn toàn vô dụng không thể nắm bắt bất kỳ đặc trưng nào được cắt tỉa. Sau mỗi vòng cắt tỉa, nếu mất mát độ chính xác mô hình thấp hơn mất mát độ chính xác mục tiêu, nó được coi là "có thể chấp nhận", và thuật toán tăng tính tích cực của cắt tỉa bằng cách tăng T bằng lambda. Khi cắt tỉa trở nên ngày càng tích cực, độ chính xác cuối cùng giảm xuống dưới mục tiêu tại một vòng nhất định được coi là "không thể chấp nhận". Khi điều này xảy ra, thuật toán của chúng tôi quay lại trọng số mô hình và ngưỡng cắt tỉa về vòng có thể chấp nhận cuối cùng nơi mất mát độ chính xác nằm trong mục tiêu, và khởi động lại cắt tỉa từ đó nhưng thận trọng hơn - nó tăng ngưỡng chậm hơn bằng cách cắt giá trị lambda làm đôi. Nếu điều này vẫn không dẫn đến một vòng có thể chấp nhận, thuật toán cắt lambda làm đôi một lần nữa và khởi động lại. Nếu sau nhiều lần thử, mất mát độ chính xác vẫn không thể chấp nhận, thuật toán quay lại xa hơn và khởi động lại từ một vòng trước đó. Lý do đằng sau thuật toán thích ứng này là tính tích cực của cắt tỉa nên tăng tốc khi mô hình còn xa mục tiêu cắt tỉa, và giảm tốc khi gần mục tiêu. Cuối cùng, các thay đổi của kích thước mô hình hội tụ (khi các thay đổi về số tham số nằm trong 0.1% cho nhiều vòng) và thuật toán kết thúc.

3.3 ĐIỀU CHỈNH NGƯỠNG THEO LỚP

Trong khi thích ứng ngưỡng cắt tỉa toàn cục bằng các chính sách đã thảo luận ở trên, phương pháp cắt tỉa của chúng tôi còn xem xét sự khác biệt trong đóng góp của mỗi lớp đối với kích thước và độ phức tạp mô hình và sử dụng các ngưỡng cụ thể theo lớp được phân biệt để cắt tỉa các lớp. Như được minh họa trong Hình 1(b), về mặt đóng góp vào kích thước mô hình, số tham số của lớp i có thể được ước tính là Nᵢ = n(i-1) × kᵢ × kᵢ × nᵢ; về mặt đóng góp vào độ phức tạp tính toán, số FLOPs của lớp i có thể được ước tính là 2 × hᵢ × wᵢ × Nᵢ. Một lớp đóng góp nhiều hơn vào kích thước hoặc FLOPs của mô hình có nhiều khả năng có các bộ lọc dư thừa có thể được cắt tỉa mà không ảnh hưởng đến độ chính xác của mô hình. Do đó, để cắt tỉa hiệu quả một mô hình trong khi duy trì độ chính xác của nó, chúng ta cần đối xử khác nhau với mỗi lớp tại mỗi vòng của quá trình cắt tỉa lặp đi lặp lại dựa trên đóng góp hiện tại của nó đối với kích thước và độ phức tạp mô hình. Cụ thể, phương pháp cắt tỉa thích ứng của chúng tôi tính toán một trọng số cho mỗi lớp dựa trên đóng góp của nó và sau đó sử dụng trọng số này để điều chỉnh ngưỡng toàn cục hiện tại và suy ra một ngưỡng cục bộ cho lớp. Nếu mục tiêu là giảm kích thước mô hình, trọng số được tính toán là số tham số của mỗi lớp chia cho tổng số tham số của mô hình; nếu mục tiêu là giảm độ phức tạp tính toán mô hình, trọng số được tính toán là FLOPs của mỗi lớp chia cho tổng FLOPs của mô hình. Các ngưỡng cụ thể theo lớp này sau đó được sử dụng để cắt tỉa các lớp trong vòng cắt tỉa hiện tại.

Giả định mục tiêu là giảm kích thước mô hình, quy trình trong một vòng cắt tỉa r cho lớp conv2d thứ i được hiển thị trong Thuật toán 3 (thuật toán để giảm FLOPs mô hình tương tự). Để cắt tỉa, chúng tôi giới thiệu mặt nạ Mⱼⁱ ∈ {0,1}^(n(i-1)×kᵢ×kᵢ) cho mỗi bộ lọc Fⱼⁱ. Trọng số hiệu quả của lớp tích chập sau cắt tỉa là tích theo phần tử của bộ lọc và mặt nạ - Fⱼⁱ ⊙ Mⱼⁱ. Đối với một bộ lọc, tất cả giá trị của mặt nạ này đều là 0 hoặc 1, do đó hoặc giữ lại hoặc cắt tỉa toàn bộ bộ lọc.

--- TRANG 6 ---
Bảng 1: So sánh phương pháp được đề xuất với các phương pháp cắt tỉa có cấu trúc hiện đại nhất trên bộ dữ liệu CIFAR-10. Độ chính xác Top-1 cơ sở của ResNet-56 và ResNet-50 trên CIFAR-10 lần lượt là 92.84%, 91.83% (5 lần chạy). Các số của các công trình liên quan được trích dẫn từ bài báo của họ.

[Bảng với nhiều cột hiển thị các kết quả so sánh chi tiết giữa phương pháp được đề xuất và các phương pháp khác trên các mô hình khác nhau, bao gồm ResNet-56, ResNet-50, VGG-16, VGG-19, và MobileNetV2 với các mục tiêu khác nhau về độ chính xác, tham số và FLOPs]

4 ĐÁNH GIÁ

4.1 KẾT QUẢ CIFAR-10

Bảng 1 liệt kê các kết quả từ CIFAR-10 trên ResNet-56, ResNet-50, VGG-16, VGG-19, và MobileNetV2. Chúng tôi so sánh phương pháp của chúng tôi với các công trình hiện đại nhất, bao gồm SCOP (Tang et al. (2020)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), PPR (Zhuang et al. (2020)), RFR (He et al. (2017)), GAL (Lin et al. (2019)), DCP (Zhuang et al. (2018)), GBN (You et al. (2019)), CP (He et al. (2017)), SFP (He et al. (2018a)), FPGM (He et al. (2019)), DeepHoyer (DH) (Yang et al. (2019)), AMC (He et al. (2018b)), PFS (Wang et al. (2020)), SCP (Kang & Han (2020)), VCNNP (Zhao et al. (2019)), Hinge (Li et al. (2020)), GDP (Guo et al. (2021)), Eigendamage (Wang et al. (2019)), NN Slimming (Liu et al. (2017)), MDP (Guo et al. (2020)), và DMC (Gao et al. (2020)). Lưu ý rằng một sụt giảm độ chính xác âm có nghĩa là mô hình đã cắt tỉa đạt được

--- TRANG 7 ---
Bảng 2: So sánh phương pháp được đề xuất với các phương pháp cắt tỉa có cấu trúc hiện đại nhất trên ResNet-50 với bộ dữ liệu ImageNet, và VGG-19 trên Tiny-ImageNet. Kết quả của ILP là từ việc triển khai của chúng tôi, và tất cả các số khác của các công trình liên quan được trích dẫn từ bài báo của họ.

[Bảng hiển thị kết quả so sánh trên ImageNet và Tiny-ImageNet với các mục tiêu khác nhau]

độ chính xác tốt hơn so với mô hình cơ sở chưa cắt tỉa. "Ours-1" và "Ours-2" ký hiệu các trường hợp mà mục tiêu cắt tỉa được đặt để giảm thiểu kích thước mô hình và FLOPs, tương ứng, trong khi đáp ứng mục tiêu độ chính xác.

Trong tất cả các trường hợp nhắm đến độ chính xác, kích thước mô hình và cường độ tính toán, phương pháp của chúng tôi vượt trội đáng kể so với các công trình liên quan gần đây. Ví dụ, trên ResNet-56, mà không có sụt giảm độ chính xác, phương pháp của chúng tôi đạt được việc giảm tham số lớn nhất (79.11%), vượt trội hơn các công trình liên quan từ 22.81% đến 66.07%, và việc giảm FLOPs lớn nhất (70.13%), vượt trội hơn các công trình liên quan từ 14.13% đến 26.53%. Với 70% giảm tham số, phương pháp của chúng tôi đạt được mất mát độ chính xác nhỏ nhất (-0.6%), vượt trội hơn các công trình liên quan từ 0.59% đến 2.98%. Ngoài ra, với 75% giảm FLOPs, phương pháp của chúng tôi đạt được mất mát độ chính xác nhỏ nhất (0.3%), vượt trội hơn các công trình liên quan từ 0.87% đến 2.24%. Lưu ý rằng phương pháp được đề xuất cũng tạo ra một mô hình cắt tỉa đạt độ chính xác cao hơn 0.6% hoặc 1.08% so với mô hình gốc nhưng với chỉ 30% hoặc 50%, tương ứng, số tham số của mô hình gốc. Những mô hình nhỏ và chính xác như vậy hữu ích cho nhiều ứng dụng thực tế.

Trên VGG-16, mà không có sụt giảm độ chính xác, phương pháp của chúng tôi đạt được việc giảm tham số lớn nhất (72.85%), vượt trội hơn các công trình liên quan từ 6.62% đến 33.78%; Với 70% giảm tham số, mất mát độ chính xác đạt được bởi phương pháp của chúng tôi thấp hơn GDP là 0.19%. Trên VGG-19, với không có mất mát độ chính xác, phương pháp của chúng tôi đạt được việc giảm tham số lớn nhất (85.99%) vượt trội hơn các công trình liên quan từ 5.92% đến 7.81%, và việc giảm FLOPs lớn nhất (56.22%) vượt trội hơn các công trình liên quan từ 13.57% đến 19.09%; Với 85% giảm FLOPs, mất mát độ chính xác đạt được bởi phương pháp của chúng tôi thấp hơn EigenDamage là 0.07%.

Trên MobileNet, mà không có sụt giảm độ chính xác, phương pháp của chúng tôi đạt được việc giảm tham số lớn nhất (60.16%), vượt trội hơn GDP và MDP lần lượt là 13.94% và 31.45%; Với 40% giảm tham số, mất mát độ chính xác đạt được bởi phương pháp của chúng tôi thấp hơn SCOP và DMC lần lượt là 1.04% và 0.54%.

Các kết quả xác nhận rằng cắt tỉa dựa trên kích hoạt hiệu quả hơn so với cắt tỉa dựa trên độ lớn trọng số trong việc tìm các bộ lọc không quan trọng, như đã thảo luận trong Phần 3.1. Ví dụ, trên ResNet-56, với 1% mất mát độ chính xác, lượng giảm tham số đạt được bởi phương pháp của chúng tôi cao hơn so với ILP (Renda et al. (2020)) là 34.51%; và với 75% giảm FLOPs, độ chính xác đạt được bởi phương pháp của chúng tôi cao hơn PPR (Zhuang et al. (2020)) là 0.87%.

Các kết quả xác nhận rằng phương pháp của chúng tôi có thể đạt được kết quả tốt hơn so với các phương pháp cắt tỉa tự động hiện đại nhất, AMC (He et al. (2018b)). Ví dụ, khi nhắm đến khoảng 55% giảm FLOPs trên ResNet-56, phương pháp của chúng tôi đạt được độ chính xác cao hơn AMC là 1.53%; và khi nhắm đến khoảng 60% giảm tham số trên ResNet-50, phương pháp của chúng tôi đạt được độ chính xác cao hơn AMC là 0.84%. Cắt tỉa thích ứng dựa trên kích hoạt của chúng tôi có thể trực tiếp nhắm đến các bộ lọc không quan trọng trong mô hình, trong khi AMC phải sử dụng học tăng cường để khám phá toàn bộ mạng.

--- TRANG 8 ---
Bảng 3: Kết quả của cắt tỉa một lần với các loại hàm ánh xạ chú ý khác nhau và L1 Norm cho VGG-16 trên bộ dữ liệu CIFAR-10.

Phương pháp | Acc. ↓(%)
Attention Mean (p=1) | 0.38
Attention Mean (p=2) | 0.42
Attention Mean (p=4) | 0.47
Attention Sum (p=1) | 0.63
Attention Max (p=1) | 0.51
L1 Norm | 0.48

Bảng 4: Kết quả của chiến lược cắt tỉa thích ứng và các chiến lược cắt tỉa lặp đi lặp lại không thích ứng cho VGG-16 trên bộ dữ liệu CIFAR-10.

Phương pháp | Acc. ↓(%) | Params. ↓(%)
IAP | 0.9 | 69.83
ILP | 1.1 | 69.83
Ours | -0.29 | 70.54

4.2 KẾT QUẢ IMAGENET

Bảng 2 hiển thị kết quả của ResNet-50 trên bộ dữ liệu ImageNet và VGG-19 trên bộ dữ liệu Tiny-ImageNet, so sánh phương pháp của chúng tôi với PFB (Liebenwein et al. (2019)), HRank (Lin et al. (2020)), ILP(Renda et al. (2020)), Sparse Structure Selection (SSS) (Huang & Wang (2018)), NN Slimming (Liu et al. (2017)), và Eigendamage (Wang et al. (2019)). "Ours-1" và "Ours-2" ký hiệu các trường hợp mà mục tiêu cắt tỉa được đặt để giảm thiểu số tham số và FLOPs, tương ứng, trong khi đáp ứng mục tiêu độ chính xác. Trên ImageNet, với cùng mức độ mất mát độ chính xác, phương pháp của chúng tôi giảm đáng kể nhiều tham số hơn (6.45% đến 29.61% cao hơn so với các công trình liên quan) và nhiều FLOPs hơn (0.82% đến 17.2% cao hơn so với các công trình liên quan). Với cùng mức độ giảm tham số hoặc FLOPs, phương pháp của chúng tôi đạt được mất mát độ chính xác nhỏ nhất, thấp hơn các công trình liên quan từ 0.08% đến 3.21%. Trên Tiny-ImageNet, với cùng mức độ giảm tham số, phương pháp của chúng tôi đạt được mất mát độ chính xác thấp hơn đáng kể so với NN Slimming là 11%; Với cùng mức độ mất mát độ chính xác, phương pháp của chúng tôi đạt được giảm tham số cao hơn (72.58% vs 61.87%), và giảm FLOPs cao hơn (78.97% vs 66.21%) so với EigenDamage.

5 NGHIÊN CỨU ABLATION

Tác động của các hàm ánh xạ chú ý. Bảng 3 hiển thị mất mát độ chính xác Top-1 của VGG-16 được cắt tỉa bởi cắt tỉa một lần với các loại hàm ánh xạ chú ý khác nhau trên CIFAR-10. Việc giảm tham số là 57.73%, và độ chính xác của mô hình gốc là 93.63% (5 lần thử). Đầu tiên, chúng tôi phân tích ba dạng của bản đồ chú ý dựa trên kích hoạt với p bằng 1 (thảo luận trong Phần 3.1), được ghi chú là Attention Mean (p=1), Attention Sum (p=1), Attention Max (p=1). Attention Mean dẫn đến mất mát độ chính xác thấp nhất, thấp hơn Attention Sum và Attention Max lần lượt là 0.25% và 0.13%. Thứ hai, chúng tôi phân tích Attention Mean với các giá trị khác nhau của p (p=1,2,4). Khi p được đặt thành 1, nó dẫn đến mất mát độ chính xác thấp nhất (0.38% vs 0.42% và 0.47%). Ngoài ra, tất cả các dạng của Attention Mean đều vượt trội hơn L1-Norm, điều này xác nhận rằng hàm ánh xạ chú ý dựa trên kích hoạt được đề xuất hiệu quả hơn so với các phương pháp dựa trên độ lớn trọng số để đánh giá tầm quan trọng của các bộ lọc.

Tác động của cắt tỉa thích ứng. Chúng tôi so sánh chiến lược cắt tỉa thích ứng được đề xuất với các chiến lược cắt tỉa lặp đi lặp lại không thích ứng cắt tỉa một tỷ lệ phần trăm cố định của các bộ lọc trong mỗi vòng cắt tỉa. Một cơ sở cắt tỉa một tỷ lệ phần trăm cố định của các bộ lọc có giá trị trọng số thấp nhất, được ghi chú là Iterative L1-Norm Pruning (ILP); cái khác cắt tỉa một tỷ lệ phần trăm cố định của các bộ lọc sử dụng hàm ánh xạ chú ý được đề xuất với p bằng 1, được ghi chú là Iterative Attention Pruning (IAP). Bảng 4 so sánh mất mát độ chính xác Top-1 của mô hình VGG-16 với độ thưa thớt 70% được cắt tỉa bởi IAP, ILP, và chiến lược cắt tỉa thích ứng được đề xuất trên CIFAR-10. Đối với IAP và ILP, tỷ lệ cắt tỉa được đặt thành 5%, tức là, 5% bộ lọc được cắt tỉa trong mỗi vòng cắt tỉa. Với 70% tham số được giảm, chiến lược cắt tỉa thích ứng được đề xuất dẫn đến độ chính xác cao hơn IAP và ILP lần lượt là 1.19% và 1.39%.

6 KẾT LUẬN

Bài báo này đề xuất một phương pháp cắt tỉa có cấu trúc lặp đi lặp lại thích ứng dựa trên kích hoạt để tự động và hiệu quả tạo ra các mô hình nhỏ, chính xác và hiệu quả về mặt phần cứng có thể đáp ứng các yêu cầu đa dạng của người dùng. Chúng tôi cho thấy rằng giá trị chú ý dựa trên kích hoạt là một chỉ báo chính xác hơn để xác định các bộ lọc không quan trọng để cắt tỉa so với giá trị độ lớn trọng số thường được sử dụng. Chúng tôi cũng cung cấp một cách hiệu quả để thực hiện cắt tỉa có cấu trúc trong một quá trình cắt tỉa lặp đi lặp lại, tận dụng lý thuyết LTH để tìm các mạng con nhỏ và chính xác đồng thời hiệu quả về mặt phần cứng. Cuối cùng, chúng tôi lập luận rằng cắt tỉa tự động là cần thiết để phương pháp này trở nên hữu ích trong thực tế, và đề xuất một phương pháp cắt tỉa thích ứng có thể tự động đáp ứng các mục tiêu do người dùng chỉ định về độ chính xác mô hình, kích thước và tốc độ suy luận nhưng không cần can thiệp của người dùng. Kết quả của chúng tôi xác nhận rằng phương pháp được đề xuất vượt trội hơn các phương pháp cắt tỉa có cấu trúc hiện có với một biên độ lớn.

TÀI LIỆU THAM KHẢO

[Danh sách các tài liệu tham khảo tiếp tục với định dạng giống như trong bản gốc]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

A PHỤ LỤC

A.1 CHI TIẾT TRIỂN KHAI

Chúng tôi đã triển khai phương pháp được đề xuất của chúng tôi trên PyTorch phiên bản 1.6.0. Đối với các mô hình ResNet trên CIFAR-10, tốc độ học được đặt ban đầu là 0.1 và giảm với tỷ lệ 0.1 tại epoch 91 và 136. Weight decay được đặt thành 0.0002. Đối với ResNet-50 trên ImageNet, tốc độ học tăng lên 0.256 trong cơ chế warmup trong 5 epoch đầu, và giảm với hệ số 0.1 tại epoch 30, 60, 80 (Renda et al. (2020); Frankle et al. (2019)). Bộ tối ưu hóa Nesterov SGD được sử dụng với momentum 0.9 cho tất cả các mô hình. Tăng cường dữ liệu đơn giản (cắt ngẫu nhiên và lật ngang ngẫu nhiên) được sử dụng cho tất cả hình ảnh huấn luyện. Quay lại tốc độ học được sử dụng cho cắt tỉa lặp đi lặp lại, quay lại epoch khoảng 60% thời gian huấn luyện tổng (xem Phụ lục A.5 để phân tích tác động của epoch quay lại đối với độ chính xác của các mô hình đã cắt tỉa). Đối với cắt tỉa thích ứng, T và lambda được khởi tạo lần lượt là 0.0 và 0.005.

A.2 TÁC ĐỘNG CỦA CHÚY Ý

Hình 2 hiển thị chú ý của mỗi bộ lọc của lớp tích chập đầu tiên của ResNet-50 trên ImageNet với các giá trị khác nhau của p (p=1, 2, 4). Cài đặt mà p bằng 1 có xu hướng tốt nhất, vì nó thúc đẩy hiệu quả của cắt tỉa bằng cách cho phép khoảng cách giữa giá trị trung bình của các bộ lọc hữu ích và vô dụng trở nên lớn.

A.3 CẮT TỈAM THÍCH ỨNG HẠN CHẾ BỘ NHỚ

Thuật toán Cắt tỉa Thích ứng Hạn chế Bộ nhớ được hiển thị trong Thuật toán 4.

A.4 CẮT TỈAM THÍCH ỨNG HẠN CHẾ FLOPS

Thuật toán Cắt tỉa Thích ứng Hạn chế FLOPs được hiển thị trong Thuật toán 5.

A.5 TÁC ĐỘNG CỦA EPOCH QUAY LẠI

Để hiểu cách quay lại ảnh hưởng đến độ chính xác của các mô hình đã cắt tỉa, chúng tôi phân tích tính ổn định đối với cắt tỉa như được đề xuất bởi Frankle et al. (2019), được định nghĩa là khoảng cách L2 giữa các trọng số mặt nạ của mạng đã cắt tỉa và mạng gốc ở cuối huấn luyện. Chúng tôi xác nhận các quan sát được đề xuất bởi Frankle et al. (2019) rằng đối với các mạng sâu, quay lại về các giai đoạn rất sớm là không tối ưu vì mạng chưa học được nhiều đến thời điểm đó; và quay lại về các giai đoạn huấn luyện rất muộn cũng không tối ưu vì không có đủ thời gian để huấn luyện lại. Cụ thể, Hình 3(a) hiển thị độ chính xác test Top-1 của ResNet-50 đã cắt tỉa với 83.74% tham số còn lại khi tốc độ học được quay lại về các epoch khác nhau, và Hình 3(b) hiển thị các giá trị ổn định tại các epoch quay lại tương ứng. Chúng tôi quan sát thấy có một vùng, 65 đến 80 epoch, nơi độ chính xác kết quả cao. Chúng tôi thấy rằng khoảng cách L2 tuân theo mẫu này một cách chặt chẽ, hiển thị khoảng cách lớn cho các epoch huấn luyện sớm và khoảng cách nhỏ cho các epoch huấn luyện muộn. Các phát hiện của chúng tôi cho thấy quay lại về 75%-90% thời gian huấn luyện dẫn đến độ chính xác tốt.

A.6 TÁC ĐỘNG CỦA CẮT TỈAM LẶP ĐI LẶP LẠI

Để minh họa hiệu quả của cắt tỉa thích ứng được đề xuất của chúng tôi, trong Hình 4, chúng tôi hiển thị cách mất mát độ chính xác và giảm tham số thay đổi qua các vòng cắt tỉa khi ngưỡng cắt tỉa thích ứng theo Thuật toán 2, trong thí nghiệm cắt tỉa ResNet-56 trên CIFAR-10, với mục tiêu 1% mất mát độ chính xác. Từ Vòng 1 đến Vòng 24, mất mát độ chính xác của mô hình đã cắt tỉa thấp hơn mất mát độ chính xác mục tiêu (1%), vì vậy thuật toán tăng tính tích cực cắt tỉa dần dần bằng cách tăng ngưỡng. Tại Vòng 25, mất mát độ chính xác vượt quá mục tiêu, vì vậy thuật toán quay lại trọng số mô hình và ngưỡng cắt tỉa về Vòng 24, và khởi động lại cắt tỉa từ đó thận trọng hơn. Quá trình trên lặp lại cho đến sau Vòng 39, kích thước mô hình hội tụ, và thuật toán kết thúc với tổng cộng giảm 88.23% tham số.
