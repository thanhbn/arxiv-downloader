# Học Hàm Kích Hoạt cho Mạng Nơ-ron Thưa

Mohammad Loni∗1Aditya Mohan∗2Mehdi Asadi3Marius Lindauer2
1Khoa Khoa học Máy tính và Kỹ thuật Phần mềm, Đại học Mälardalen, Thụy Điển
2Viện Trí tuệ Nhân tạo, Đại học Leibniz Hannover, Đức
3Khoa Kỹ thuật Điện, Đại học Tarbiat Modares, Tehran, Iran

## Tóm tắt
Mạng Nơ-ron Thưa (SNN) có khả năng thể hiện hiệu suất tương tự như các mạng dày đặc tương đương trong khi tiết kiệm đáng kể năng lượng và bộ nhớ khi suy luận. Tuy nhiên, sự sụt giảm độ chính xác do SNN gây ra, đặc biệt ở tỷ lệ cắt tỉa cao, có thể là vấn đề trong các điều kiện triển khai quan trọng. Trong khi các nghiên cứu gần đây giảm thiểu vấn đề này thông qua các kỹ thuật cắt tỉa tinh vi, chúng tôi chuyển tập trung sang một yếu tố bị bỏ qua: siêu tham số và hàm kích hoạt. Phân tích của chúng tôi đã cho thấy sự sụt giảm độ chính xác có thể được quy cho thêm (i) Sử dụng ReLU làm lựa chọn mặc định cho hàm kích hoạt một cách đồng nhất, và (ii) Tinh chỉnh SNN với cùng siêu tham số như các mạng dày đặc tương đương. Do đó, chúng tôi tập trung vào việc học một cách thức mới để điều chỉnh hàm kích hoạt cho mạng thưa và kết hợp chúng với một chế độ tối ưu hóa siêu tham số (HPO) riêng biệt cho mạng thưa. Bằng cách tiến hành thí nghiệm trên các mô hình DNN phổ biến (LeNet-5, VGG-16, ResNet-18, và EfficientNet-B0) được huấn luyện trên bộ dữ liệu MNIST, CIFAR-10, và ImageNet-16, chúng tôi cho thấy sự kết hợp mới của hai phương pháp này, được gọi là Tìm kiếm Hàm Kích hoạt Thưa, viết tắt: SAFS, dẫn đến cải thiện độ chính xác tuyệt đối lên đến 15,53%, 8,88%, và 6,33% cho LeNet-5, VGG-16, và ResNet-18 so với các giao thức huấn luyện mặc định, đặc biệt ở tỷ lệ cắt tỉa cao.

## 1 Giới thiệu

Mạng Nơ-ron Sâu, mặc dù đã thể hiện hiệu suất mạnh mẽ trên nhiều tác vụ khác nhau, nhưng lại tốn kém về mặt tính toán để huấn luyện và triển khai. Khi kết hợp với các mối quan tâm về quyền riêng tư, hiệu quả năng lượng, và thiếu kết nối ổn định, điều này đã dẫn đến sự quan tâm ngày càng tăng trong việc triển khai DNN trên các thiết bị có hạn chế tài nguyên như vi điều khiển và FPGA (Chen và Ran, 2019).

Các nghiên cứu gần đây đã cố gắng giải quyết vấn đề này bằng cách giảm dung lượng bộ nhớ khổng lồ và tiêu thụ điện năng của DNN. Chúng bao gồm lượng tử hóa (Zhou et al., 2017), chưng cất tri thức (Hinton et al., 2015), phân tách hạng thấp (Jaderberg et al., 2014), và thưa hóa mạng sử dụng cắt tỉa không có cấu trúc (còn gọi là Mạng Nơ-ron Thưa) (Han et al., 2015).

Trong số này, Mạng Nơ-ron Thưa (SNN) đã cho thấy lợi ích đáng kể thông qua khả năng loại bỏ các trọng số dư thừa (Hoefler et al., 2021). Tuy nhiên, chúng gặp phải sự sụt giảm độ chính xác, đặc biệt ở tỷ lệ cắt tỉa cao; ví dụ, Mousavi et al. (2022) báo cáo giảm ≈54% độ chính xác top-1 cho MobileNet-v2 (Sandler et al., 2018) được huấn luyện trên ImageNet so với không cắt tỉa.

Mặc dù phần lớn lỗi cho sự sụt giảm độ chính xác này thuộc về bản thân quá trình thưa hóa, chúng tôi đã xác định hai yếu tố liên quan chưa được khám phá đầy đủ có thể tác động thêm đến nó: (i) Các hàm kích hoạt của các mạng thưa tương đương không bao giờ được tối ưu hóa, với Đơn vị Tuyến tính Chỉnh lưu (ReLU) (Nair và Hinton, 2010) là lựa chọn mặc định. (ii) Các siêu tham số huấn luyện của mạng nơ-ron thưa thường được giữ nguyên như các mạng dày đặc tương đương.

Một bước tự nhiên, do đó, là hiểu cách các hàm kích hoạt tác động đến quá trình học cho SNN. Trước đây, Jaiswal et al. (2022) và Tessera et al. (2021) đã chứng minh rằng ReLU làm giảm khả năng huấn luyện của SNN vì những thay đổi đột ngột trong gradient quanh zero dẫn đến việc chặn luồng gradient. Ngoài ra, Apicella et al. (2021) đã cho thấy rằng một hàm kích hoạt phổ quát không thể ngăn chặn các vấn đề học tập điển hình như gradient biến mất.

Trong khi lĩnh vực Máy học Tự động (AutoML) (Hutter et al., 2019) trước đây đã khám phá việc tối ưu hóa hàm kích hoạt của DNN dày đặc (Ramachandran et al., 2018; Loni et al., 2020; Bingham et al., 2020), hầu hết các phương pháp này đòi hỏi một lượng lớn tài nguyên tính toán (lên đến 2000 giờ GPU (Bingham et al., 2020)), dẫn đến thiếu quan tâm trong việc tối ưu hóa hàm kích hoạt cho các vấn đề học sâu khác nhau. Mặt khác, các nỗ lực cải thiện độ chính xác của SNN sử dụng tìm kiếm kiến trúc thưa (Fedorov et al., 2019; Mousavi et al., 2022) hoặc chế độ huấn luyện thưa (Srinivas et al., 2017). Theo hiểu biết của chúng tôi, không có phương pháp hiệu quả nào để tối ưu hóa hàm kích hoạt trong huấn luyện SNN.

**Đóng góp của Bài báo:** (i) Chúng tôi phân tích tác động của hàm kích hoạt và siêu tham số huấn luyện đến hiệu suất của kiến trúc CNN thưa. (ii) Chúng tôi đề xuất một phương pháp AutoML mới, được gọi là SAFS, để điều chỉnh hàm kích hoạt và siêu tham số huấn luyện của mạng nơ-ron thưa để khác biệt với các giao thức huấn luyện của các mạng dày đặc tương đương. (iii) Chúng tôi chứng minh những cải thiện hiệu suất đáng kể khi áp dụng SAFS với cắt tỉa độ lớn không có cấu trúc cho LeNet-5 trên bộ dữ liệu MNIST (LeCun et al., 1998), mạng VGG-16 và ResNet-18 được huấn luyện trên bộ dữ liệu CIFAR-10 (Krizhevsky et al., 2014), và mạng ResNet-18 và EfficientNet-B0 được huấn luyện trên bộ dữ liệu ImageNet-16 (Chrabaszcz et al., 2017), khi so sánh với các giao thức huấn luyện mặc định, đặc biệt ở mức độ thưa cao.

## 2 Nghiên cứu Liên quan

Theo hiểu biết tốt nhất của chúng tôi, SAFS là khung tự động đầu tiên điều chỉnh hàm kích hoạt của mạng nơ-ron thưa sử dụng phương pháp tối ưu hóa đa giai đoạn. Nghiên cứu của chúng tôi cũng làm sáng tỏ thực tế rằng việc điều chỉnh siêu tham số đóng vai trò quan trọng trong độ chính xác của mạng nơ-ron thưa. Cải thiện độ chính xác của mạng nơ-ron thưa đã được nghiên cứu rộng rãi trong quá khứ. Các nghiên cứu trước đây chủ yếu được phân loại thành (i) đề xuất các tiêu chí khác nhau để chọn trọng số không đáng kể, (ii) cắt tỉa khi khởi tạo hoặc huấn luyện, và (iii) tối ưu hóa các khía cạnh khác của mạng thưa ngoài tiêu chí cắt tỉa. Trong phần này, chúng tôi thảo luận về các phương pháp này và so sánh chúng với SAFS, và xem xét ngắn gọn nghiên cứu tiên tiến về tối ưu hóa hàm kích hoạt của mạng dày đặc.

### 2.1 Tối ưu hóa Mạng Nơ-ron Thưa

**Cắt tỉa Trọng số Không đáng kể.** Một số nghiên cứu đã đề xuất cắt tỉa các tham số trọng số dưới một ngưỡng cố định, bất kể mục tiêu huấn luyện (Han et al., 2015; Li et al., 2016; Zhou et al., 2019). Gần đây, Azarian et al. (2020) và Kusupati et al. (2020) đề xuất ngưỡng có thể huấn luyện theo lớp để xác định giá trị tối ưu cho từng lớp.

**Cắt tỉa khi Khởi tạo hoặc Huấn luyện.** Các phương pháp này nhằm bắt đầu thưa thay vì huấn luyện trước một mạng dày đặc rồi sau đó cắt tỉa. Để xác định trọng số nào nên duy trì hoạt động khi khởi tạo, chúng sử dụng các tiêu chí như sử dụng độ nhạy kết nối (Lee et al., 2018) và bảo tồn tính nổi bật synap (Tanaka et al., 2020). Mặt khác, Mostafa và Wang (2019); Mocanu et al. (2018); Evci et al. (2020) đề xuất tận dụng thông tin thu thập được trong quá trình huấn luyện để cập nhật động mẫu thưa của kernel.

**Tối ưu hóa Mạng Thưa Đa dạng.** Evci et al. (2019) nghiên cứu địa hình mất mát của mạng nơ-ron thưa và Frankle et al. (2020) giải quyết cách nó bị tác động bởi nhiễu của Gradient Descent Ngẫu nhiên (SGD). Cuối cùng, Lee et al. (2020) nghiên cứu ảnh hưởng của khởi tạo trọng số đến hiệu suất của mạng thưa. Trong khi công trình của chúng tôi cũng nhằm cải thiện hiệu suất của mạng thưa và cho phép chúng đạt được hiệu suất tương tự như các mạng dày đặc tương đương, chúng tôi thay vào đó tập trung vào tác động của việc tối ưu hóa hàm kích hoạt và siêu tham số của mạng nơ-ron thưa trong một thiết lập HPO kết hợp.

### 2.2 Tìm kiếm Hàm Kích hoạt

Việc chọn hàm kích hoạt không phù hợp dẫn đến mất thông tin trong quá trình lan truyền tiến và các vấn đề gradient biến mất và/hoặc bùng nổ trong quá trình lan truyền ngược (Hayou et al., 2019). Để tìm hàm kích hoạt tối ưu, một số nghiên cứu đã tự động điều chỉnh hàm kích hoạt cho DNN dày đặc, dựa trên tính toán tiến hóa (Bingham et al., 2020; Basirat và Roth, 2021; Nazari et al., 2019), học tăng cường (Ramachandran et al., 2018), hoặc gradient descent để thiết kế hàm tham số (Tavakoli et al., 2021; Zamora et al., 2022).

Mặc dù thành công của các phương pháp này, việc điều chỉnh tự động hàm kích hoạt cho mạng dày đặc không đáng tin cậy cho bối cảnh thưa vì không gian tìm kiếm cho hàm kích hoạt của mạng dày đặc không tối ưu cho mạng thưa (Dubowski, 2020). Cùng các thao tác thành công trong mạng dày đặc có thể làm giảm drastically luồng gradient mạng trong mạng thưa (Tessera et al., 2021). Ngoài ra, các phương pháp hiện tại gặp phải chi phí tìm kiếm đáng kể; ví dụ, Bingham et al. (2020) yêu cầu 1000 giờ GPU mỗi lần chạy trên NVIDIA® GTX 1080Ti. Jin et al. (2016) cho thấy sự vượt trội của SReLU so với ReLU khi huấn luyện mạng thưa vì nó cải thiện luồng gradient của mạng. Tuy nhiên, SReLU yêu cầu học bốn tham số bổ sung cho mỗi nơ-ron. Trong trường hợp triển khai mạng với hàng triệu đơn vị ẩn, điều này có thể dễ dàng dẫn đến chi phí tính toán và bộ nhớ đáng kể tại thời điểm suy luận. SAFS, mặt khác, thống nhất tìm kiếm cục bộ ở mức meta với gradient descent để tạo ra chiến lược tối ưu hóa hai tầng và đạt được hiệu suất vượt trội với hội tụ tìm kiếm nhanh hơn so với nghệ thuật tiên tiến.

## 3 Kiến thức Cơ bản

Trong phần này, chúng tôi phát triển ký hiệu cho các phần sau bằng cách giới thiệu chính thức hai vấn đề mà chúng tôi giải quyết: Thưa hóa Mạng và Tối ưu hóa Siêu tham số.

### 3.1 Thưa hóa Mạng

Thưa hóa mạng là một kỹ thuật hiệu quả để cải thiện hiệu suất của DNN cho các ứng dụng có tài nguyên tính toán hạn chế. Zhan và Cao (2019) báo cáo rằng thưa hóa mạng có thể hỗ trợ tiết kiệm thời gian suy luận ResNet-18 được huấn luyện trên ImageNet trên thiết bị di động lên đến 29,5×. Thưa hóa mạng thường bao gồm ba giai đoạn:

1. **Huấn luyện trước**: Huấn luyện một mô hình lớn, được tham số hóa quá mức. Cho một metric mất mát L_train và tham số mạng θ, điều này có thể được hình thức hóa như nhiệm vụ tìm tham số θ*_pre tối thiểu hóa L_train trên dữ liệu huấn luyện D_train:

θ*_pre ∈ argmin_{θ∈Θ} L_train(θ; D_train)     (1)

2. **Cắt tỉa**: Sau khi huấn luyện mô hình dày đặc, bước tiếp theo là loại bỏ các tensor trọng số quan trọng thấp của mạng đã huấn luyện trước. Điều này có thể được thực hiện theo lớp, theo kênh, và toàn mạng. Các cơ chế thông thường đơn giản đặt một phần trăm nhất định của trọng số (tỷ lệ cắt tỉa) về zero, hoặc học một mặt nạ Boolean m* trên vector trọng số. Cả hai khái niệm này có thể được nắm bắt một cách tổng quát tương tự như công thức huấn luyện dày đặc nhưng với một metric mất mát riêng biệt L_prune. Mục tiêu ở đây là thu được một mặt nạ cắt tỉa m*, trong đó ⊙ đại diện cho thao tác masking và N đại diện cho kích thước của mặt nạ:

m* ∈ argmin_{m∈{0,1}^N} L_prune(θ*_pre ⊙ m; D_train)
s.t. ||m*||_0 ≤ ε     (2)

trong đó ε là ngưỡng về số lượng tối thiểu trọng số được masking.

3. **Tinh chỉnh**: Bước cuối cùng là huấn luyện lại mạng đã cắt tỉa để lấy lại độ chính xác ban đầu sử dụng mất mát tinh chỉnh L_fine, có thể giống với mất mát huấn luyện hoặc một loại khác:

θ*_fine ∈ argmin_{θ∈Θ} L_fine(θ; θ*_pre ⊙ m*, D_train)     (3)

Đối với giai đoạn cắt tỉa, SAFS sử dụng phương pháp cắt tỉa độ lớn phổ biến (Han et al., 2015) bằng cách loại bỏ một phần trăm nhất định của trọng số có độ lớn thấp hơn. So với các phương pháp cắt tỉa có cấu trúc (Liu et al., 2018), phương pháp cắt tỉa độ lớn cung cấp tính linh hoạt cao hơn và tỷ lệ nén tốt hơn |θ*_fine|/|θ*_pre| × 100. Quan trọng là, SAFS độc lập với thuật toán cắt tỉa; do đó, nó có thể tối ưu hóa bất kỳ mạng thưa nào.

### 3.2 Tối ưu hóa Siêu tham số (HPO)

Chúng tôi ký hiệu không gian siêu tham số của mô hình là Λ từ đó chúng tôi lấy mẫu một cấu hình siêu tham số λ = (λ₁,...,λ_d) để được điều chỉnh bởi một số phương pháp HPO. Chúng tôi giả định c: λ → R là một hàm chi phí hộp đen ánh xạ cấu hình được chọn λ đến một metric hiệu suất, như lỗi mô hình. Mục tiêu của HPO sau đó có thể được tóm tắt như nhiệm vụ tìm cấu hình tối ưu λ* tối thiểu hóa c. Cho các tham số tinh chỉnh θ*_fine thu được trong Phương trình (3), chúng tôi định nghĩa chi phí như tối thiểu hóa mất mát L_hp trên bộ dữ liệu validation D_val như một vấn đề tối ưu hóa hai cấp:

λ* ∈ argmin_{λ∈Λ} c(λ) = argmin_{λ∈Λ} L_hp(θ*_fine(λ); D_val)     (4)

s.t.
θ*_fine(λ) ∈ argmin_{θ∈Θ} L_fine(θ; θ*_pre ⊙ m*, D_train, λ)

Chúng tôi lưu ý rằng về nguyên tắc HPO cũng có thể được áp dụng cho việc huấn luyện mô hình gốc (Phương trình (1)), nhưng chúng tôi giả định rằng mô hình gốc được cho và chúng tôi chỉ quan tâm đến thưa hóa.

## 4 Tìm kiếm Hàm Kích hoạt cho Mạng Thưa

Mục tiêu của SAFS là tìm cấu hình siêu tham số tối ưu cho mạng đã cắt tỉa với trọng tâm là hàm kích hoạt. Cho thiết lập HPO được mô tả trong Phần 3.2, bây giờ chúng tôi giải thích cách hình thức hóa vấn đề tìm kiếm hàm kích hoạt và những gì cần thiết để giải quyết nó.

### 4.1 Mô hình hóa Hàm Kích hoạt

Sử dụng các kỹ thuật tối ưu hóa đòi hỏi tạo ra một không gian tìm kiếm chứa các hàm kích hoạt ứng viên có triển vọng. Không gian tìm kiếm bị hạn chế quá mức có thể không chứa hàm kích hoạt mới (tính biểu đạt) trong khi tìm kiếm trong không gian tìm kiếm quá lớn có thể khó khăn (kích thước) (Ramachandran et al., 2018). Do đó, việc tạo ra sự cân bằng giữa tính biểu đạt và kích thước của không gian tìm kiếm là một thách thức quan trọng trong việc thiết kế không gian tìm kiếm.

Để giải quyết vấn đề này, chúng tôi mô hình hóa hàm kích hoạt tham số như một sự kết hợp của một toán tử đơn ngôi f và hai yếu tố tỷ lệ có thể học α, β. Do đó, cho một đầu vào x và đầu ra y, hàm kích hoạt có thể được hình thức hóa là y = αf(βx), có thể được biểu diễn thay thế như một đồ thị tính toán được hiển thị trong Hình 3a.

Hình 1 minh họa một ví dụ về việc điều chỉnh các tham số có thể học α và β của hàm kích hoạt Swish. Chúng ta có thể thấy một cách trực quan rằng việc sửa đổi các tham số có thể học được đề xuất cho một toán tử đơn ngôi mẫu cung cấp cho mạng thưa tính linh hoạt bổ sung để tinh chỉnh hàm kích hoạt (Godfrey, 2019; Bingham và Miikkulainen, 2022). Các ví dụ về hàm kích hoạt mà chúng tôi xem xét trong công trình này đã được liệt kê trong Phụ lục E.

Đối với mạng thưa, biểu diễn này cho phép triển khai hiệu quả cũng như tham số hóa hiệu quả. Như chúng tôi giải thích thêm trong Phần 4.2, bằng cách coi điều này như một quá trình tối ưu hóa hai giai đoạn, trong đó việc tìm kiếm f là một vấn đề tối ưu hóa rời rạc và việc tìm kiếm α, β được xen kẽ với tinh chỉnh, chúng tôi có thể làm cho quá trình tìm kiếm hiệu quả trong khi nắm bắt bản chất của việc tỷ lệ đầu vào-đầu ra và các biến đổi chức năng phổ biến với hàm kích hoạt.

Lưu ý rằng SAFS thuộc về danh mục hàm kích hoạt thích ứng do giới thiệu các tham số có thể huấn luyện (Dubey et al., 2022). Các tham số này cho phép hàm kích hoạt điều chỉnh mô hình một cách mượt mà với độ phức tạp của bộ dữ liệu (Zamora et al., 2022). Trái ngược với các hàm kích hoạt thích ứng phổ biến như PReLU và Swish, SAFS tự động hóa việc điều chỉnh hàm kích hoạt trên một họ đa dạng các hàm kích hoạt cho từng lớp của mạng với các siêu tham số được tối ưu hóa.

### 4.2 Quy trình Tối ưu hóa

SAFS thực hiện tối ưu hóa theo lớp tức là chúng tôi dự định tìm hàm kích hoạt cho từng lớp. Cho chỉ số lớp i = 1,...,L của mạng có độ sâu L, một thuật toán tối ưu hóa cần có thể chọn một toán tử đơn ngôi f*ᵢ và tìm các yếu tố tỷ lệ phù hợp (α*ᵢ, β*ᵢ). Chúng tôi hình thức hóa chúng như hai hàm mục tiêu độc lập, được giải quyết trong một quy trình tối ưu hóa hai giai đoạn kết hợp tối ưu hóa rời rạc và ngẫu nhiên. Hình 2 cho thấy tổng quan về pipeline SAFS.

**Giai đoạn 1: Tìm kiếm Toán tử Đơn ngôi.** Giai đoạn đầu tiên là tìm các toán tử đơn ngôi sau khi mạng đã được cắt tỉa. Quan trọng là, bước tinh chỉnh chỉ xảy ra sau khi tối ưu hóa này cho hàm kích hoạt đã hoàn thành. Chúng tôi mô hình hóa nhiệm vụ tìm toán tử đơn ngôi tối ưu cho từng lớp như một vấn đề tối ưu hóa rời rạc. Cho một tập hàm được định nghĩa trước F = {f₁, f₂,..., fₙ}, chúng tôi định nghĩa một không gian F của các chuỗi toán tử có thể ψ = ⟨fᵢ|fᵢ ∈ F⟩ᵢ∈{1,...,L} ∈ F có kích thước L. Nhiệm vụ của chúng tôi là tìm một chuỗi ψ sau giai đoạn cắt tỉa (Mục 2). Vì các tham số mạng được huấn luyện trước θ*_pre và mặt nạ cắt tỉa m* đã được khám phá, chúng tôi giữ chúng cố định và sử dụng chúng như một điểm khởi tạo cho tối ưu hóa hàm kích hoạt. Nhiệm vụ được hình thức hóa như tìm các toán tử tối ưu cho các tham số mạng, như thể hiện trong Phương trình (5). Trong bước này, các tham số α và β được đặt thành 1 để tập trung vào lớp hàm trước.

ψ* ∈ argmin_{ψ∈F} L_train(θ*_pre ⊙ m*, ψ; D_train)     (5)

Cho tính chất rời rạc của Phương trình (5), chúng tôi sử dụng Late Acceptance Hill Climbing (LAHC) (Burke và Bykov, 2017) để giải quyết nó một cách lặp đi lặp lại (Vui lòng tham khảo Phụ lục A để so sánh với các thuật toán tìm kiếm khác). LAHC là một thuật toán Hill Climbing sử dụng một bản ghi lịch sử - Độ dài Lịch sử - của các giá trị mục tiêu của các giải pháp đã gặp trước đó để quyết định có chấp nhận một giải pháp mới hay không. Nó cung cấp cho chúng tôi hai lợi ích: (i) Là một phương pháp tìm kiếm bán-cục bộ, LAHC hoạt động trên không gian rời rạc và nhanh chóng tìm kiếm không gian để tìm toán tử đơn ngôi. (ii) LAHC mở rộng thuật toán hill-climbing vanilla (Selman và Gomes, 2006) bằng cách cho phép các giải pháp tồi tệ hơn với hy vọng tìm ra giải pháp tốt hơn trong tương lai.

Chúng tôi biểu diễn không gian thiết kế của LAHC bằng một nhiễm sắc thể là một danh sách các hàm kích hoạt tương ứng với từng lớp của mạng. Hình 3b cho thấy một ví dụ về một giải pháp trong không gian thiết kế. Lợi ích của biểu diễn này là tính linh hoạt và đơn giản của nó. Để tạo ra một ứng viên tìm kiếm mới (thao tác đột biến), trước tiên chúng tôi hoán đổi hai gen được chọn ngẫu nhiên từ nhiễm sắc thể, và sau đó, chúng tôi thay đổi ngẫu nhiên một gen từ nhiễm sắc thể bằng một ứng viên mới từ danh sách.

Phụ lục E liệt kê các toán tử đơn ngôi được xem xét trong nghiên cứu này. Để tránh bất ổn trong quá trình huấn luyện, chúng tôi bỏ qua các toán tử tuần hoàn (ví dụ: cos(x)) và các toán tử chứa đường tiệm cận ngang (y = 0) hoặc dọc (x = 0) (ví dụ: y = 1/x).

Quá trình chọn toán tử để tạo thành nhiễm sắc thể được lặp lại trong một số vòng lặp được xác định trước (tham khảo Phụ lục E để biết cấu hình của LAHC). Cho rằng chúng tôi chỉ có hai đột biến cho mỗi vòng lặp tìm kiếm, toàn bộ nhiễm sắc thể không bị ảnh hưởng đáng kể. Dựa trên các lần chạy thử, chúng tôi xác định ngân sách 20 vòng lặp tìm kiếm để cung cấp cải thiện tốt cùng với việc giảm chi phí tìm kiếm. Mỗi vòng lặp bao gồm việc huấn luyện mạng bằng các hàm kích hoạt được chọn và đo lường mất mát huấn luyện L_train như một metric fitness cần được tối thiểu hóa.

Một nhược điểm của quá trình này là nhu cầu huấn luyện lại mạng cho mỗi vòng lặp tìm kiếm, có thể tốn nhiều thời gian và tài nguyên tính toán. Chúng tôi khắc phục vấn đề này bằng cách tận dụng ước tính độ trung thực thấp hơn của hiệu suất cuối cùng. Cho rằng hiệu suất mạng không thay đổi sau một số epoch nhất định, chúng tôi tận dụng công trình của Loni et al. (2020) và chỉ huấn luyện mạng đến một điểm nhất định sau đó hiệu suất nên duy trì ổn định.

**Giai đoạn 2: Yếu tố Tỷ lệ và HPO** Cho một chuỗi toán tử tối ưu đã học ψ, bước tiếp theo là tìm một chuỗi ψ' = ⟨(αᵢ, βᵢ)|αᵢ, βᵢ ∈ R⟩ᵢ∈{1,...,L} đại diện cho các yếu tố tỷ lệ cho từng lớp. Chúng tôi thực hiện quá trình này cùng với giai đoạn tinh chỉnh (Phương trình (3)) và HPO để khám phá các tham số tinh chỉnh θ*_fine và siêu tham số λ* như thể hiện trong Phương trình (6).

λ* ∈ argmin_{λ∈Λ} c(λ; D_val)
s.t. ψ'*, θ*_fine(λ) ∈ argmin_{θ∈Θ,ψ'∈R^(2,L)} L_fine(θ|θ*_pre ⊙ m*), ψ'; ψ, D_train     (6)

Do tính chất liên tục của giai đoạn này, chúng tôi sử dụng Stochastic Gradient Descent (SGD) để giải quyết Phương trình (6), và sử dụng độ chính xác validation như một metric fitness cho cấu hình siêu tham số.

Coi các yếu tố tỷ lệ như các tham số có thể học cho phép chúng tôi học chúng trong giai đoạn tinh chỉnh. Do đó, tối ưu hóa bên trong trong bước này hầu như không có chi phí bổ sung. Chi phí bổ sung duy nhất là của HPO, mà chúng tôi chứng minh trong các thí nghiệm là quan trọng và đáng giá vì các siêu tham số từ việc huấn luyện mô hình gốc có thể không tối ưu cho tinh chỉnh.

## 5 Thí nghiệm

Chúng tôi phân loại các thí nghiệm dựa trên các câu hỏi nghiên cứu mà công trình này nhằm trả lời. Phần 5.1 giới thiệu thiết lập thí nghiệm. Phần 5.2 thúc đẩy vấn đề điều chỉnh hàm kích hoạt cho SNN. Phần 5.3 giới thiệu nhu cầu HPO với điều chỉnh kích hoạt cho SNN. Trong Phần 5.4, chúng tôi so sánh SAFS với các baseline khác nhau. Phụ lục D cung cấp sự đánh đổi cải thiện độ chính xác vs. tỷ lệ nén để so sánh SAFS với các phương pháp nén mạng tiên tiến. Trong Phần 5.5 chúng tôi so sánh hiệu suất của SAFS cho các tỷ lệ cắt tỉa khác nhau. Trong Phần 5.6 chúng tôi cung cấp những hiểu biết về các hàm kích hoạt được SAFS học. Cuối cùng, chúng tôi ablate SAFS trong Phần 5.7 để xác định tác động của các lựa chọn thiết kế khác nhau.

### 5.1 Thiết lập Thí nghiệm

**Bộ dữ liệu.** Để đánh giá SAFS, chúng tôi sử dụng các bộ dữ liệu phân loại công khai MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2014) và ImageNet-16 (Chrabaszcz et al., 2017). Lưu ý rằng ImageNet-16 bao gồm tất cả hình ảnh của bộ dữ liệu ImageNet gốc, được thay đổi kích thước thành 16 × 16 pixel. Tất cả các thí nghiệm HPO được tiến hành bằng SMAC3 (Lindauer et al., 2022). Phụ lục E trình bày phần còn lại của thiết lập thí nghiệm.

### 5.2 Tác động của việc Điều chỉnh Hàm Kích hoạt đến Độ chính xác của SNN

Để xác thực giả định rằng hàm kích hoạt thực sự tác động đến độ chính xác, chúng tôi điều tra xem các hàm kích hoạt hiện được sử dụng cho mạng dày đặc (Evci et al., 2022) có còn đáng tin cậy trong bối cảnh thưa hay không. Hình 4a cho thấy tác động của năm hàm kích hoạt khác nhau đến độ chính xác của kiến trúc thưa với các tỷ lệ cắt tỉa khác nhau. Để đo lường hiệu suất trong giai đoạn tìm kiếm, chúng tôi sử dụng phương pháp validation ba fold. Tuy nhiên, chúng tôi báo cáo độ chính xác test của SAFS để so sánh kết quả của chúng tôi với các baseline khác.

Kết luận của chúng tôi từ thí nghiệm này có thể được tóm tắt như sau: (i) ReLU không hoạt động tốt nhất trong tất cả các tình huống. Chúng ta thấy rằng SRS, Swish, Tanh, Symlog, FLAU, và PReLU vượt trội hơn ReLU ở mức độ thưa cao hơn. Do đó, quyết định sử dụng ReLU một cách đồng nhất có thể hạn chế mức tăng tiềm năng về độ chính xác. (ii) Khi chúng ta tăng tỷ lệ cắt tỉa lên 99% (mạng cực kỳ thưa), mặc dù có sự sụt giảm chung về độ chính xác, sự khác biệt trong độ chính xác của mạng thưa và dày đặc thay đổi rất nhiều tùy thuộc vào hàm kích hoạt. Do đó, việc lựa chọn hàm kích hoạt cho mạng có độ thưa cao trở thành một tham số quan trọng. Chúng tôi cần đề cập rằng mặc dù SAFS thành công trong việc cung cấp độ chính xác cao hơn, nó cần 47 giờ GPU tổng cộng để học hàm kích hoạt và HP tối ưu. Mặt khác, tinh chỉnh một mạng nơ-ron thưa mất ≈3.9 giờ GPU.

### 5.3 Khó khăn trong Huấn luyện Mạng Nơ-ron Thưa

Hiện tại, hầu hết các thuật toán để huấn luyện DNN thưa sử dụng các cấu hình được tùy chỉnh cho các mạng dày đặc tương đương, ví dụ: bắt đầu từ một bộ lập lịch học cố định. Để xác thực nhu cầu tối ưu hóa các siêu tham số huấn luyện của mạng thưa, chúng tôi sử dụng các cấu hình dày đặc như một baseline so với các siêu tham số được học bởi một phương pháp HPO. Hình 4b cho thấy các đường cong tinh chỉnh VGG-16 thưa hóa với tỷ lệ cắt tỉa 99% được huấn luyện trên CIFAR-10. Việc huấn luyện đã được thực hiện với các siêu tham số của mạng dày đặc (Xanh), và các siêu tham số huấn luyện được tối ưu hóa bằng SMAC3 (Cam).

Chúng tôi tối ưu hóa tỷ lệ học, bộ lập lịch tỷ lệ học, và các siêu tham số optimizer với phạm vi được chỉ định trong Phụ lục E (Bảng 4). Loại và phạm vi của các siêu tham số được chọn dựa trên các phạm vi được khuyến nghị từ tài liệu học sâu (Simonyan và Zisserman, 2014; Subramanian et al., 2022), tài liệu SMAC3 (Lindauer et al., 2022), và từ các thư viện mã nguồn mở khác nhau được sử dụng để triển khai VGG-16. Để ngăn chặn overfitting trên dữ liệu test, chúng tôi tối ưu hóa các siêu tham số trên dữ liệu validation và kiểm tra hiệu suất cuối cùng trên dữ liệu test. Hiệu suất kém (giảm 7.17% độ chính xác) của chiến lược học SNN sử dụng tham số dày đặc thúc đẩy nhu cầu cho một chế độ HPO riêng biệt nhận thức về thưa.

### 5.4 So sánh với Baseline Cắt tỉa Độ lớn

Bảng 1 cho thấy kết quả tối ưu hóa hàm kích hoạt VGG-16 thưa được huấn luyện trên CIFAR-10 bằng SAFS với tỷ lệ cắt tỉa 99%. Trung bình của ba lần chạy đã được báo cáo. Kết quả cho thấy SAFS cung cấp cải thiện độ chính xác tuyệt đối 8.88% cho VGG-16 và 6.33% cho ResNet-18 được huấn luyện trên CIFAR-10 khi so sánh với baseline cắt tỉa độ lớn vanilla. SAFS cũng mang lại cải thiện độ chính xác Top-1 tuyệt đối 1.8% cho ResNet-18 và 1.54% cho EfficientNet-B0 được huấn luyện trên ImageNet-16 khi so sánh với baseline cắt tỉa độ lớn vanilla.

SReLU (Jin et al., 2016) là một hàm kích hoạt tuyến tính từng phần được hình thành bởi bốn tham số có thể học. Mocanu et al. (2018); Curci et al. (2021); Tessera et al. (2021) đã cho thấy SReLU hoạt động xuất sắc cho mạng nơ-ron thưa do cải thiện luồng gradient của mạng. Kết quả cho thấy SAFS cung cấp độ chính xác cao hơn 15.99% và 19.17% so với huấn luyện VGG-16 và ResNet-18 với hàm kích hoạt SReLU trên CIFAR-10. Thêm vào đó, SAFS cung cấp độ chính xác tốt hơn 0.88% và 1.28% so với huấn luyện ResNet-18 và EfficientNet-B0 với hàm kích hoạt SReLU trên bộ dữ liệu ImageNet-16. Cuối cùng, Phụ lục B cho thấy SAFS cải thiện đáng kể luồng gradient của mạng nơ-ron thưa, điều này liên quan đến hàm kích hoạt được tối ưu hóa và giao thức huấn luyện hiệu quả.

### 5.5 Đánh giá SAFS với Các Tỷ lệ Cắt tỉa Khác nhau

Hình 4a so sánh hiệu suất của VGG-16 được tinh chỉnh bởi SAFS và giao thức huấn luyện mặc định trên CIFAR-10 qua ba tỷ lệ cắt tỉa khác nhau bao gồm 90%, 95%, và 99%. Kết quả cho thấy SAFS cực kỳ hiệu quả bằng cách đạt được độ chính xác cao hơn 1.65%, 7.45%, và 8.88% so với VGG-16 với hàm kích hoạt ReLU được tinh chỉnh với giao thức huấn luyện mặc định ở tỷ lệ cắt tỉa 90%, 95%, và 99%. Thêm vào đó, SAFS tốt hơn các hàm kích hoạt được thiết kế cho mạng dày đặc, đặc biệt cho mạng với tỷ lệ cắt tỉa 99%.

### 5.6 Hiểu biết về Tìm kiếm Hàm Kích hoạt

Hình 5 trình bày mô hình thống trị của mỗi toán tử đơn ngôi trong giai đoạn học đầu tiên (α = β = 1) cho bộ dữ liệu CIFAR-10. Kết quả là trung bình của ba lần chạy với các seed ngẫu nhiên khác nhau. Đơn vị của thanh màu là số lần thấy một hàm kích hoạt cụ thể qua tất cả các vòng lặp tìm kiếm cho giai đoạn học đầu tiên. Theo kết quả, rõ ràng rằng (i) Symexp và ELU là các hàm kích hoạt không thuận lợi, (ii) Symlog và Acon là các hàm kích hoạt thống trị trong khi được sử dụng ở các lớp đầu, và (iii) Nhìn chung Swish và HardSwish tốt, nhưng chúng chủ yếu xuất hiện ở các lớp giữa.

### 5.7 Nghiên cứu Ablation

Chúng tôi nghiên cứu ảnh hưởng của từng giai đoạn tối ưu hóa riêng lẻ của SAFS đến hiệu suất của VGG-16 và ResNet-18 thưa được huấn luyện trên CIFAR-10 trong Bảng 2. Kết quả cho thấy mỗi đóng góp riêng lẻ cung cấp độ chính xác cao hơn cho cả VGG-16 và ResNet-18. Tuy nhiên, hiệu suất tối đa đạt được bởi SAFS (+15.53%, +8.88%, +6.33%, và +1.54% cho LeNet-5, VGG-16, ResNet-18, và EfficientNet-B0), nơi chúng tôi đầu tiên học toán tử đơn ngôi chính xác nhất cho từng lớp và sau đó tinh chỉnh các yếu tố tỷ lệ với các siêu tham số được tối ưu hóa.

## 6 Kết luận

Trong bài báo này, chúng tôi đã nghiên cứu tác động của hàm kích hoạt đến việc huấn luyện mạng nơ-ron thưa và sử dụng điều này để học các hàm kích hoạt mới. Để kết thúc, chúng tôi đã chứng minh rằng sự sụt giảm độ chính xác phát sinh từ việc huấn luyện SNN đồng nhất với ReLU cho tất cả các đơn vị có thể được giảm thiểu một phần bằng tìm kiếm theo lớp cho hàm kích hoạt. Chúng tôi đề xuất một pipeline tối ưu hóa hai giai đoạn mới kết hợp tối ưu hóa rời rạc và ngẫu nhiên để chọn một chuỗi hàm kích hoạt cho từng lớp của SNN, cùng với việc khám phá các siêu tham số tối ưu cho tinh chỉnh. Phương pháp SAFS của chúng tôi cung cấp cải thiện đáng kể bằng cách đạt được độ chính xác cao hơn lên đến 8.88% và 6.33% cho VGG-16 và ResNet-18 trên CIFAR-10 so với các giao thức huấn luyện mặc định, đặc biệt ở tỷ lệ cắt tỉa cao. Quan trọng là, vì SAFS độc lập với thuật toán cắt tỉa, nó có thể tối ưu hóa bất kỳ mạng thưa nào.

## 7 Hạn chế và Tác động Rộng lớn

**Tác động Rộng lớn.** Các tác giả đã xác định rằng công trình này sẽ không có tác động tiêu cực nào đến xã hội hoặc môi trường, vì công trình này không giải quyết bất kỳ ứng dụng cụ thể nào.

**Nghiên cứu Tương lai và Hạn chế.** Mạng Nơ-ron Thưa (SNN) cho phép triển khai các mô hình lớn trên các thiết bị có hạn chế tài nguyên bằng cách tiết kiệm chi phí tính toán và tiêu thụ bộ nhớ. Ngoài ra, điều này trở nên quan trọng trong việc giảm dấu chân carbon và sử dụng tài nguyên của DNN tại thời điểm suy luận. Chúng tôi tin rằng điều này mở ra những hướng nghiên cứu mới về các phương pháp có thể cải thiện độ chính xác của SNN. Chúng tôi hy vọng rằng công trình của chúng tôi thúc đẩy các kỹ sư sử dụng SNN nhiều hơn trước đây trong các sản phẩm thực tế vì SAFS cung cấp cho SNN hiệu suất tương tự như các mô hình dày đặc tương đương. Một số hướng trực tiếp để mở rộng công trình của chúng tôi là (i) tận dụng ý tưởng về bộ dự đoán độ chính xác (Li et al., 2023) để đẩy nhanh quy trình tìm kiếm. (ii) SNN gần đây đã cho thấy triển vọng trong ứng dụng cho các kỹ thuật giải quyết vấn đề ra quyết định tuần tự như Học Tăng cường (Vischer et al., 2022; Graesser et al., 2022). Chúng tôi tin rằng việc kết hợp SAFS vào các tình huống như vậy có thể giúp ích cho khả năng triển khai của các pipeline như vậy.

SAFS đã được đánh giá trên các bộ dữ liệu đa dạng, bao gồm MNIST, CIFAR-10, và ImageNet-16, và các kiến trúc mạng khác nhau như LeNet-5, VGG-16, ResNet-18, và EfficientNet-B0. Trong khi kết quả hiện tại chứng minh khả năng ứng dụng chung của phương pháp chúng tôi và dấu hiệu của khả năng mở rộng, chúng tôi tin rằng các thí nghiệm thêm trên các bộ dữ liệu lớn hơn và các mạng có khả năng mở rộng hơn sẽ là một hướng thú vị cho nghiên cứu tương lai.

## Lời cảm ơn

Aditya Mohan và Marius Lindauer được hỗ trợ bởi Bộ Môi trường, Bảo tồn Thiên nhiên, An toàn Hạt nhân và Bảo vệ Người tiêu dùng Liên bang Đức (dự án GreenAutoML4FAS số 67KI32007A). Mohammad Loni được hỗ trợ bởi dự án HiPEAC, một chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu theo thỏa thuận tài trợ số 871174.
