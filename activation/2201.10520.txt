# 2201.10520.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/activation/2201.10520.pdf
# File size: 749043 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2201.10520v3  [cs.CV]  9 Mar 2023ADAPTIVE ACTIVATION -BASED STRUCTURED PRUN-
ING
Kaiqi Zhao1, Animesh Jain2, Ming Zhao1
1Arizona State University2Facebook
kzhao27@asu.edu, anijain@umich.edu, mingzhao@asu.edu
ABSTRACT
Pruning is a promising approach to compress complex deep lea rning models in
order to deploy them on resource-constrained edge devices. However, many ex-
isting pruning solutions are based on unstructured pruning , which yield models
that cannot efﬁciently run on commodity hardware, and requi re users to manu-
ally explore and tune the pruning process, which is time cons uming and often
leads to sub-optimal results. To address these limitations , this paper presents an
adaptive, activation-based, structured pruning approach to automatically and ef-
ﬁciently generate small, accurate, and hardware-efﬁcient models that meet user
requirements. First, it proposes iterative structured pru ning using activation-based
attention feature maps to effectively identify and prune un important ﬁlters. Then,
it proposes adaptive pruning policies for automatically me eting the pruning ob-
jectives of accuracy-critical, memory-constrained, and l atency-sensitive tasks. A
comprehensive evaluation shows that the proposed method ca n substantially out-
perform the state-of-the-art structured pruning works on C IFAR-10 and ImageNet
datasets. For example, on ResNet-56 with CIFAR-10, without any accuracy drop,
our method achieves the largest parameter reduction (79.11 %), outperforming the
related works by 22.81% to 66.07%, and the largest FLOPs redu ction (70.13%),
outperforming the related works by 14.13% to 26.53%.
1 I NTRODUCTION
Deep neural networks (DNNs) have substantial compute and me mory requirements. As deep learn-
ing becomes pervasive and moves towards edge devices, DNN de ployment becomes harder be-
cause of the mistmatch between resource-hungry DNNs and res ource-constrained edge devices.
DNN pruning is a promising approach (Li et al. (2016); Han et a l. (2015); Molchanov et al. (2016);
Theis et al. (2018); Renda et al. (2020)), which identiﬁes th e parameters (or weight elements) that
do not contribute signiﬁcantly to the accuracy and prunes th em from the network. Recently, works
based on the Lottery Ticket Hypothesis (LTH) have achieved g reat successes in creating smaller and
more accurate models through iterative pruning with rewind ing (Frankle & Carbin (2018)). How-
ever, LTH has only been shown to work successfully with unstr uctured pruning which, unfortunately
leads to models with low sparsity and difﬁcult to accelerate on commodity hardware such as CPUs
and GPUs (e.g., Hill et al. (2017) shows directly applying NV DIA cuSPARSE on unstructured
pruned models can lead to 60 ˆslowdown on GPU compared to dense kernels.) Moreover, most
pruning methods require users to explore and adjust multipl e hyper-parameters, e.g., with LTH-
based iterative pruning, users need to determine how many pa rameters to prune in each round.
Tuning the pruning process is time consuming and often leads to sub-optimal results.
We propose activation-based ,adaptive ,iterative structured pruning to ﬁnd the “winning ticket”
models that are at the same time hardware efﬁcient and to auto matically meet the users’ model
accuracy, size, and speed requirements. First, we propose a n activation-based structured pruning
method to identify and remove unimportant ﬁlters in an LTH-b ased iterative pruning (with rewind-
ing) process. Speciﬁcally, we properly deﬁne an attention m apping function that takes a 2D acti-
vation feature maps of a ﬁlter as input, and outputs a 1D value used to indicate the importance of
the ﬁlter. This approach is more effective than weight-valu e based ﬁlter pruning because activation-
based attention values not only capture the features of inpu ts but also contain the information of
1

--- PAGE 2 ---
convolution layers that act as feature detectors for predic tion tasks. We then integrate this attention-
based method into the LTH-based iterative pruning framewor k to prune the ﬁlters in each round and
ﬁnd the winning ticket that is small, accurate, and hardware -efﬁcient.
Second, we propose adaptive pruning that automatically opt imizes the pruning process according
to different user objectives. For latency-sensitive scena rios like interactive virtual assistants, we
propose FLOPs-guaranteed pruning to achieve the best accur acy given the maximum amount of
compute FLOPs; For memory-limited environments like embed ded systems, we propose model-
size-guaranteed pruning to achieve the best accuracy given the maximum amount of memory foot-
print; For accuracy-critical applications such as those on self-driving cars, we propose accuracy-
guaranteed pruning to create the most resource-efﬁcient mo del given the acceptable accuracy loss.
Aiming for different targets, our method adaptively contro ls the pruning aggressiveness by adjust-
ing the global threshold used to prune ﬁlters. Moreover, it c onsiders the difference in each layer’s
contributions to the model’s size and computational comple xity and uses a per-layer threshold, cal-
culated by dividing each layer’s remaining parameters or FL OPs by the entire model’s remaining
parameters or FLOPs, to prune each layer with differentiate d level of aggressiveness.
Our results outperform the related works signiﬁcantly in al l cases targeting accuracy loss, parame-
ters reduction, and FLOPs reduction. For example, on ResNet -56 with CIFAR-10 dataset, without
accuracy drop, our method achieves the largest parameter re duction (79.11%), outperforming the
related works by 22.81% to 66.07%, and the largest FLOPs redu ction (70.13%), outperforming the
related works by 14.13% to 26.53%. In addition, our method en ables a pruned model the reach
0.6% or 1.08% higher accuracy than the original model but wit h only 30% or 50%of the original’s
parameters. On ResNet-50 on ImageNet, for the same level of p arameters and FLOPs reduction,
our method achieves the smallest accuracy loss, lower than t he related works by 0.08% to 3.21%;
and for the same level of accuracy loss, our method reduces si gniﬁcantly more parameters (6.45%
to 29.61% higher than related works) and more FLOPs (0.82% to 17.2% higher than related works).
2 B ACKGROUND AND RELATED WORKS
Unstructured vs. Structured Pruning. Unstructured pruning (LeCun et al. (1990); Han et al.
(2015); Molchanov et al. (2017)) is a ﬁne-grained approach t hat prunes individual unimportant ele-
ments in weight tensors. It has less impact to model accuracy , compared to structured pruning, but
unstructured pruned models are hard to accelerate on commod ity hardware. Structured pruning is a
coarse-grained approach that prunes entire regular region s of weight tensors according to some rule-
based heuristics, such as L1-norm (Li et al. (2016)), averag e percentage of zero (Molchanov et al.
(2016)), and other information considering the relationsh ip between neighboring layers (Theis et al.
(2018); Lee et al. (2018)). Compared to unstructured prunin g, it is more difﬁcult to prune a model
without causing accuracy loss using structured pruning, be cause by removing entire regions, it might
remove weight elements that are important to the ﬁnal accura cy (Li et al. (2016)). However, struc-
tured pruned models can be mapped easily to general-purpose hardware and accelerated directly
with off-the-shelf hardware and libraries (He et al. (2018b )).
One Shot vs. Iterative Pruning. One-shot pruning prunes a pre-trained model and then retrai ns it
once, whereas iterative pruning prunes and retrains the mod el in multiple rounds. Both techniques
can choose either structured or unstructured pruning techn iques. Recently, works based on the Lot-
tery Ticket Hypothesis (LTH) have achieved great successes in creating smaller and more accurate
models through iterative pruning with rewinding (Frankle & Carbin (2018)). LTH posits that a dense
randomly initialized network has a sub-network, termed as a winning ticket , which can achieve an
accuracy comparable to the original network. At the beginni ng of each pruning round, it rewinds
the weights and/or learning rate of the sub-network to some e arly epoch of the training phase of
the original model to reduce the distance between the sub-ne twork and original model and increase
the change of ﬁnding the winning ticket. However, most of LTH -based works considered only un-
structured pruning, e.g., Iterative Magnitude Pruning (IM P) (Frankle & Carbin (2018); Frankle et al.
(2019)), which, as discussed above, is hardware-inefﬁcien t.
It is non-trivial to design an iterative pruning method with structured pruning. To understand the state
of iterative structured pruning, we experimented with IMP’ s structured pruning counterpart—L1-
norm structured pruning (ILP) (Li et al. (2016)) which remov es entire ﬁlters depending on their L1-
norm value. We observed that ILP cannot effectively prune a m odel while maintaining its accuracy,
2

--- PAGE 3 ---
Algorithm 1 Adaptive Iterative Structured Pruning Algorithm
1: [Initialize] Initialize a network fpx;M0dW0
0qwith initial mask M0“ t0,1u|W0
0|and Threshold Tr0s
2: [Save weights] Train the network for kepochs, yielding network fpx;M0dW0
kq, and save weights W0
k
3: [Train to converge] Train the network for T´kepochs to converge, producing network fpx;M0dW0
Tq
4:forpruning round r(rě1)do
5: [Prune] Prune ﬁlters from Wr´1
TusingTrrs, producing a mask Mr, and a network fpx;MrdWr´1
Tq
6: [Rewind Weights] Reset the remaining ﬁlters to W0
kat epochk, producing network fpx;MrdWr´1
kq
7: [Rewind Learning Rate] Reset the learning rate schedule t o its state from epoch k
8: [Retrain] Retrain the unpruned ﬁlters for T´kepoch to converge, yielding network fpx;MrdWr
Tq
9: [Evaluate] Evaluate the retrained network fpx;MrdWr
Tqaccording to the target.
10: [Reset Weights] If the target is not met, reset the weight s to an earlier round
11: [Adapt Threshold] Calculate Threshold Trr`1sfor the next pruning round
12:end for
Original
 Input filter_1 filter_2 filter_4 filter_3
filter_5 filter_6 filter_8 filter_7
filter_9 filter_10 filter_12 filter_11
filter_13 filter_14 filter_16 filter_15
(a) Filter activation outputsInputs
niFiltersOutputsReLuConv2d
F0iki
kini-1
F1iki
kini-1
F iki
kini-1Xiwi-1
hi-1ni-1
Aiwi
hini
(b) Convolution layer tensors
Figure 1: Illustration of the (a) activation outputs of 16 ﬁl ters of a conv2d layer, and (b) input and
output tensors of a convolution layer. In (a), the left porti on shows the original image and the image
after data augmentation; and the right portion visualizes t he activation outputs of each ﬁlter.
e.g., ILP can prune ResNet-50 by at most 11.5% of parameters w hen the maximum accuracy loss
is limited to 1% on ImageNet. Therefore, directly applying i terative pruning with existing weight-
magnitude based structured pruning methods does not produc e accurate pruned models. In this
paper, we study how to produce small ,accurate , and hardware-efﬁcient models based on iterative
structured pruning with rewinding .
Automatic Pruning. For pruning to be useful in practice, it is important to autom atically meet the
pruning objectives for diverse ML applications and devices . Most pruning methods require users
to explore and adjust multiple hyper-parameters, e.g., wit h LTH-based iterative pruning, users need
to determine how many parameters to prune in each round. Tuni ng the pruning process is time
consuming and often leads to sub-optimal results. Therefor e, we study how to automatically adapt
the pruning process in order to meet the pruning objectives without user intervention .
Some works (Zoph et al. (2018); Cai et al.; Ashok et al. (2017) ) use reinforcement-learning algo-
rithms to ﬁnd computationally efﬁcient architectures. The se works are in the area of Neural Archi-
tecture Search (NAS), among which the most recent is AutoML f or Model Compression (AMC)
(He et al. (2018b)). AMC enables the model to arrive at the tar get speedup by limiting the action
space (the sparsity ratio for each layer), and it ﬁnds out the limit of compression that offers no loss
of accuracy by tweaking the reward function. But this method has to explore over a large search
space of all available layer-wise sparsity, which is time co nsuming when neural networks are large
and datasets are complicated.
3 M ETHODOLOGY
Algorithm 1 illustrates the overall ﬂow of the proposed adap tive activation-based structured pruning.
To represent pruning of weights, we use a mask Mrǫt0,1udfor each weight tensor Wr
tǫRd, where
ris the pruning round number and tis the training epoch. Therefore, the pruned network at the e nd
3

--- PAGE 4 ---
of training epoch Tis represented by the element-wise product MrdWr
T. The ﬁrst three steps are
to train the original model to completion, while saving the w eights at Epoch k. Steps 4–10 represent
a pruning round. Step 4 prunes the model (discussed in Sectio n 3.1). Step 5 (optional) and 6 perform
rewinding. Step 7 retrains the pruned model for the remainin gT´kepochs. Step 8 evaluates the
pruned model according to the pruning target. If the target i s not met, Step 9 resets the weights to
an earlier round. Step 10 calculates the pruning threshold f or the next pruning round following the
adaptive pruning policy (discussed in Section 3.2).
3.1 A CTIVATION -BASED FILTER PRUNING
To realize iterative structured pruning, one can start with the state-of-the-art structured pruning meth-
ods and apply it iteratively. The widely used structured pru ning method—L1-norm based structured
pruning Renda et al. (2020) removes ﬁlters with the lowest L1 -norm values, whereas the most recent
method—Polarization-based structured pruning Zhuang et a l. (2020) improves it by using a regular-
izer on scaling factors of ﬁlters and pruning ﬁlters whose sc aling factors are below than a threshold.
These two methods both assume that the weight values of a ﬁlter can be used as an indicator about the
importance of that ﬁlter, much like how LTH uses weight value s in unstructured pruning. However,
we observe that weight-based structured pruning methods ca nnot produce accurate pruned mod-
els. For example, to prune ResNet-56 on CIFAR-10 with no loss in top-1 accuracy, L1-norm based
structured pruning can achieve at most only 1.15 ˆmodel compression, and Polarization-norm based
structured pruning can achieve at most only 1.89 ˆinference speedup. The reason is that, some ﬁl-
ters, even though their weight values are small, can still pr oduce useful non-zero activation values
that are important for learning features during backpropag ation. That is, ﬁlters with small values
may have large activations.
We propose that the activation values of ﬁlters are more effective in ﬁnding unimportant ﬁlters to
prune. Activations like ReLu enable non-linear operations , and enable convolutional layers to act
as feature detectors. If an activation value is small, then i ts corresponding feature detector is not
important for prediction tasks. So activation values, i.e. , the intermediate output tensors after the
non-linear activation, not only detect features of trainin g dataset, but also contain the information of
convolution layers that act as feature detectors for predic tion tasks. We present a visual motivation
in Figure 1(a). The ﬁgure shows the activation output of 16 ﬁl ters of a convolution layer on one input
image. The ﬁrst image on the left is the original image, and th e second image is the input features
after data augmentation. We observe that some ﬁlters extrac t image features with high activation
patterns, e.g., the 6th and12th ﬁlters. In comparison, the activation outputs of some ﬁlt ers are close
to zero, such as the 2nd,14th, and16th. Therefore, from visual inspection, removing ﬁlters wit h
weak activation patterns is likely to have low impact on the ﬁ nal accuracy of the pruned model.
There is a natural connection between our activation-based pruning approach and the related
attention-based knowledge transfer works (Zagoruyko & Kom odakis (2016)). Attention is a map-
ping function used to calculate the statistics of each eleme nt in the activations over the channel
dimension. By minimizing the difference of the activation- based attention maps from intermediate
layers between a teacher model and a student model, attentio n transfer enables the student to imi-
tate the behaviour of the teacher. Our proposed pruning meth od builds upon the same theory that
activation-based attention is a good indicator of ﬁlters re garding their ability to capture features, and
it addresses the new challenges in using activations to guid e automatic structured pruning.
In the following, we describe how to prune ﬁlters based on its activation feature maps in each
round of the iterative structured pruning process. Figure 1 (b) shows the inputs and outputs of a
2D convolution layer (referred to as conv2d), followed by an activation layer. For the ith conv2d
layer, let XiǫRni´1ˆhi´1ˆwi´1denote the input features, and Fi
jǫRni´1ˆkiˆkibe thejth ﬁlter,
wherehi´1andwi´1are the height and width of the input features, respectively ,ni´1is the number
of input channels, niis the number of output channels, and kiis the kernel size of the ﬁlter. The
activation of the jth ﬁlterFi
jafter ReLu mapping is therefore denoted by Ai
jǫRhiˆwi.
The attention mapping function takes a 2D activation Ai
jǫRhiˆwiof ﬁlterFi
jas input, and outputs
a 1D value which will be used as an indicator of the importance of ﬁlters. We consider three forms
of activation-based attention maps, where pě1andai
k,ldenotes every element of Ai
j: 1) Mean of
attention values raised to the power of p:Fmean pAi
jq “1
hiˆwiřhi
k“1řwi
l“1ˇˇˇai
k,lˇˇˇp
; 2) Max of atten-
4

--- PAGE 5 ---
Algorithm 2 Accuracy-guaranteed Adaptive Pruning
1:Input: Target Accuracy Loss AccLossTarget
2:Output: A small pruned model with an acceptable accuracy
3: Initialize: T“0.0,λ“0.005.
4:forpruning round r(rě1)do
5: Prune the model using Trrs(Refer to Algorithm 3)
6: Train the pruned model, evaluate its accuracy Accrrs
7: Calculate the accuracy loss AccLoss rrs:AccLoss rrs “Accr0s ´Accrrs
8: ifAccLoss rrs ăAccLossTarget then
9: ifthe changes of model size are within 0.1% for several rounds then
10: Terminate
11: else
12: λrr`1s “λrrs
13: Trr`1s “Trrs `λrr`1s
14: end if
15: else
16: Find the last acceptable round k
17: ifkhas been used to roll back for several times then
18: Mark kas unacceptable
19: Go to Step 15
20: else
21: Roll back model weights to round k
22: λrr`1s “λrrs{2.0pN`1q(Nis the number of times for rolling back to round k)
23: Trr`1s “Trks `λrr`1s
24: end if
25: end if
26:end for
tion values raised to the power of p:FmaxpAi
jq “maxl“1,hiˆwiˇˇˇai
k,lˇˇˇp
; and 3) Sum of attention
values raised to the power of p:FsumpAi
jq “řhi
k“1řwi
l“1ˇˇˇai
k,lˇˇˇp
. We choose Fmean pAi
jqwithp
equals to 1as the indicator to identify and prune unimportant ﬁlters, a nd our method removes the
ﬁlters whose attention value is lower than the pruning thres hold. See Section 5 for an ablation study
on these choices.
3.2 A DAPTIVE ITERATIVE PRUNING
Our approach to pruning is to automatically and efﬁciently g enerate a pruned model that meets the
users’ different objectives. Automatic pruning means that users do not have to ﬁgure out how to
conﬁgure the pruning process. Efﬁcient pruning means that t he pruning process should produce
the user-desired model as quickly as possible. Users’ pruni ng objectives can vary depending on
the usage scenarios: 1) Accuracy-critical tasks, like thos e used by self-driving cars have stringent
accuracy requirement, which is critical for safety, but do n ot have strict limits on their computing
and storage usages; 2) Memory-constrained tasks, like thos e deployed on microcontrollers have
very limited available memory to store the models but do not h ave strict accuracy requirement; and
3) Latency-sensitive tasks, like those employed by virtual assistants where timely responses are
desirable but accuracy is not a hard constraint.
In order to achieve automatic and efﬁcient structured pruni ng, we propose three adaptive pruning
policies to provide 1) Accuracy-guaranteed Adaptive Pruni ng which produces the most resource-
efﬁcient model with the acceptable accuracy loss; 2) Memory -constrained Adaptive Pruning which
generates the most accurate model within a given memory foot print; and 3) FLOPs-constrained
Adaptive Pruning which creates the most accurate model with in a given computational intensity.
Speciﬁcally, our proposed adaptive pruning method automat ically adjusts the global threshold ( T),
used in our iterative structured pruning algorithm (Algori thm 1) to quickly ﬁnd the model that meets
the pruning objective. Other objectives (e.g., limiting a m odel’s energy consumption) can also be
supported by simply plugging in the relevant metrics into ou r adaptive pruning algorithm. We take
Accuracy-guaranteed Adaptive Pruning, described in Algor ithm 2, as an example to show the pro-
cedure of adaptive pruning. Memory-constrained and FLOPs- constrained Adaptive Pruning algo-
rithms are included in Appendix A.3 and A.4, respectively.
5

--- PAGE 6 ---
Algorithm 3 Layer-aware Threshold Adjustment
1:forthe layeriin a pruning round rdo
2: Calculate the number of parameters of unpruned ﬁlters of t he current layer and the whole model, respec-
tively:Nirrs “npi´1qrrsˆkirrsˆkirrsˆnirrs,NTotalrrs “ři“M
i“1npi´1qrrsˆkirrsˆkirrsˆnirrs,
whereMis the number of convolution layers, and for other notations refer to Section 3.1.
3: Calculate the threshold Tirrsof the current layer: Tirrs “Trrs ˆNirrs
NTotal rrs.
4: For each ﬁlter Fi
jinFi, calculate its attention: Fmean pAi
jq “1
hiˆwiřhi
k“1řwi
l“1ˇˇai
k,lˇˇp.
5: Prune the ﬁlter if its attention is not larger than thresho ldTirrs, i.e., setMi
j“0ifFmean pAi
jq ďTirrs;
Otherwise, set Mi
j“1
6:end for
In Algorithm 2, the objective is to guarantee the accuracy lo ss while minimizing the model size
(the algorithm for minimizing the model FLOPs is similar). I n the algorithm, Tcontrols the ag-
gressiveness of pruning, and λdetermines the increment of Tat each pruning round. Pruning starts
conservatively, with Tinitialized to 0, so that only completely useless ﬁlters that cannot capture
any features are pruned. After each round of pruning, if the m odel accuracy loss is below than the
target accuracy loss, it is considered “acceptable”, and th e algorithm increases the aggressiveness
of pruning by incrementing Tbyλ. As pruning becomes increasingly aggressive, the accuracy
eventually drops below the target accuracy at a certain roun d which is considered “unacceptable”.
When this happens, our algorithm rolls back the model weight s and pruning threshold to the last
acceptable round where the accuracy loss is within the targe t, and restarts the pruning from there but
more conservatively—it increases the threshold more slowl y by cutting the λvalue by half. If this
still does not lead to an acceptable round, the algorithm cut sλby half again and restarts again. If
after several trials, the accuracy loss is still not accepta ble, the algorithm rolls back even further and
restarts from an earlier round. The rationale behind this ad aptive algorithm is that the aggressiveness
of pruning should accelerate when the model is far from the pr uning target, and decelerate when it
is close to the target. Eventually, the changes of model size converges (when the changes of the
number of parameters is within 0.1% for several rounds) and t he algorithm terminates.
3.3 L AYER -AWARE THRESHOLD ADJUSTMENT
While adapting the global pruning threshold using the above discussed policies, our pruning method
further considers the difference in each layer’s contribut ion to model size and complexity and use
differentiated layer-speciﬁc thresholds to prune the laye rs. As illustrated in Figure 1(b), in terms
of the contribution to model size, the number of parameters o f layerican be estimated as Ni“
npi´1qˆkiˆkiˆni; in terms of the contribution to computational complexity, the number of
FLOPs of layer ican be estimated as 2ˆhiˆwiˆNi. A layer that contributes more to the
model’s size or FLOPs is more likely to have redundant ﬁlters that can be pruned without affecting
the model’s accuracy. Therefore, to effectively prune a mod el while maintaining its accuracy, we
need to treat each layer differently at each round of the iter ative pruning process based on its current
contributions to the model size and complexity. Speciﬁcall y, our adaptive pruning method calculates
a weight for each layer based on its contribution and then use s this weight to adjust the current global
threshold and derive a local threshold for the layer. If the g oal is to reduce model size, the weight is
calculated as each layer’s number of parameters divided by t he model’s total number of parameters;
if the goal is to reduce model computational complexity, the weight is calculated as each layer’s
FLOPs divided by the model’s total FLOPs. These layer-speci ﬁc thresholds are then used to prune
the layers in the current pruning round.
Assuming the goal is to reduce model size, the procedure in a p runing round rfor theith conv2d
layer is shown in Algorithm 3 (the algorithm for reducing mod el FLOPs is similar). For pruning, we
introduce a mask Mi
jǫt0,1uni´1ˆkiˆki
for each ﬁlter Fi
j. The effective weights of the convolution
layer after pruning is the element-wise product of the ﬁlter and mask— Fi
jdMi
j. For a ﬁlter, all
values of this mask is either 0 or 1, thus either keeping or pru ning the entire ﬁlter.
6

--- PAGE 7 ---
Table 1: Comparing the proposed method with state-of-the-a rt structured pruning methods on the
CIFAR-10 dataset. The baseline Top-1 accuracy of ResNet-56 and ResNet-50 on CIFAR-10 is
92.84%, 91.83% (5 runs), respectively. The numbers of the re lated works are cited from their papers.
Model Target Target Level Method Acc. ↓(%) Params. ↓(%) FLOPs. ↓(%)
ResNet-56Acc.↓(%)0%SCOP 0.06 56.30 56.00
HRank 0.09 42.40 50.00
PFF 0.03 42.60 43.60
ILP 0.00 13.04 -
PPR -0.03 - 47.00
EagleEye -1.4 - 50.41
Ours-1 -0.33 79.11 -
Ours-2 -0.08 - 70.13
1%RFR 1.00 50.00 -
ILP 1.00 41.18 -
GAL 0.52 44.80 48.50
Ours-1 0.86 88.23 -
Ours-2 0.77 - 81.19
Params. ↓(%)70%DCP -0.01 70.30 47.10
GBN 0.03 66.70 70.30
HRank 2.38 68.10 74.10
GDP -0.35 65.64 -
Ours -0.6 71.57 -
50%CP 1.00 - 50.00
SFP 1.33 50.60 52.60
FPGM 0.10 50.60 52.60
GDP -0.07 53.35 -
Ours -1.08 53.3 -
FLOPs. ↓(%)75%HRank 2.38 68.10 74.10
DH 2.54 - 71.00
PPR 1.17 - 71.00
Ours 0.3 - 71.44
55%CP 1 - 50.00
SFP 1.33 50.60 52.60
FPGM 0.10 50.60 52.60
AMC 0.90 - 50.00
Ours -0.63 - 52.92
ResNet-50 Params. ↓(%) 60%AMC -0.02 60.00 -
Ours -0.86 64.81 -
VGG-16Acc.↓(%) 0%PFS -0.19 50 -
SCP 0.06 66.23 -
VCNNP 0.07 60.9 -
Hinge 0.43 39.07 -
HRank 0.53 53.6 -
Ours -0.16 72.85 61.17
Params. ↓(%) 70%GDP -0.1 69.45 -
Ours -0.29 70.54 41.72
VGG-19Acc.↓(%) 0%Eigendamage 0.19 78.18 37.13
NN Slimming 1.33 80.07 42.65
Ours -0.26 85.99 56.22
FLOPs. ↓(%) 85%Eigendamage 1.88 - 86.51
Ours 1.81 - 89.02
MobileNetV2Acc.↓(%) 0%GDP -0.26 46.22 -
MDP -0.12 28.71 -
Ours -0.54 60.16 35.93
Params. ↓(%) 40%SCOP 0.24 40.30 -
DMC -0.26 40 -
Ours -0.8 43.77 78.65
4 E VALUATION
4.1 CIFAR-10 R ESULTS
Table 1 lists the results from CIFAR-10 on ResNet-56, ResNet -50, VGG-16, VGG-19, and Mo-
bileNetV2. We compare our method with the state-of-the-art works, including SCOP (Tang et al.
(2020)), HRank (Lin et al. (2020)), ILP (Renda et al. (2020)), PPR (Zhuang et al. (2020)), RFR
(He et al. (2017)), GAL (Lin et al. (2019)), DCP (Zhuang et al. (2018)), GBN (You et al. (2019)),
CP (He et al. (2017)), SFP (He et al. (2018a)), FPGM (He et al. (2019)), DeepHoyer ( DH)
(Yang et al. (2019)), AMC (He et al. (2018b)), PFS (Wang et al. (2020)), SCP (Kang & Han
(2020)), VCNNP (Zhao et al. (2019)), Hinge (Li et al. (2020)), GDP (Guo et al. (2021)), Eigen-
damage (Wang et al. (2019)), NN Slimming (Liu et al. (2017)), MDP (Guo et al. (2020)), and
DMC (Gao et al. (2020)). Note that a negative accuracy drop means that the pruned model achieves
7

--- PAGE 8 ---
Table 2: Comparing the proposed method with state-of-the-a rt structured pruning methods on
ResNet-50 with ImageNet dataset, and VGG-19 on Tiny-ImageN et. The result of ILP is from our
implementation, and all the other numbers of the related wor ks are cited from their papers.
Model Targets Targets Level Method Acc. ↓(%) Params. ↓(%) FLOPs ↓(%)
ResNet-50Acc.↓(%)0%PFP-A 0.22 18.10 10.80
Ours-1 -0.12 24.55 -
Ours-2 0.18 - 11.62
1%SSS-41 0.68 0.78 15.06
ILP 1.00 11.5 -
Ours-1 0.64 30.39 -
Ours-2 0.83 - 32.26
Params. ↓(%)40%Hrank 1.17 36.70 43.70
SSS-26 4.30 38.82 43.04
Ours 1.09 37.17 -
30%PFP-B 0.92 30.10 44.00
Ours 0.48 29.42 -
FLOPs. ↓(%) 30%SSS-32 1.94 27.06 31.08
Ours 0.57 - 31.88
VGG-19Acc.↓(%) 3%EigenDamage 3.36 61.87 66.21
Ours 2.61 72.58 78.97
Params. ↓(%) 60%NN Slimmin 10.66 60.14 -
Ours -0.34 60.21 -
better accuracy than its unpruned baseline model. “Ours-1” and “Ours-2” denote the cases where the
pruning objective is set to minimize model size and FLOPs, re spectively, while meeting the accuracy
target.
In all cases targeting accuracy, model size, and compute int ensity, our method signiﬁcantly outper-
forms the recent related works. For example, on ResNet-56, w ithout accuracy drop, our method
achieves the largest parameter reduction (79.11%), outper forming the related works by 22.81% to
66.07%, and the largest FLOPs reduction (70.13%), outperfo rming the related works by 14.13% to
26.53%. With 70% of parameter reduction, our method achieve s the smallest accuracy loss (-0.6%),
outperforming the related works by 0.59% to 2.98%. Also, wit h 75% of FLOPs reduction, our
method achieves the smallest accuracy loss (0.3%), outperf orming the related works by 0.87% to
2.24%. Note that the proposed method also produces a pruned m odel that reaches 0.6% or 1.08%
higher accuracy than the original model but with only 30% or 5 0%, respectively, of the original’s
parameters. Such small and accurate models are useful for ma ny real-world applications.
On VGG-16, without accuracy drop, our method achieves the la rgest parameter reduction (72.85%),
outperforming the related works by 6.62% to 33.78%; With 70% of parameter reduction, the accu-
racy loss achieved by our method is lower than GDP by 0.19%. On VGG-19, with no accuracy loss,
our method achieves the largest parameter reduction (85.99 %) which outperforms the related works
by 5.92% to 7.81%, and the largest FLOPs reduction (56.22%) w hich outperforms the related works
by 13.57% to 19.09%; With 85% of FLOPs reduction, the accurac y loss achieved by our method is
lower than EigenDamage by 0.07%.
On MobileNet, without accuracy drop, our method achieves th e largest parameter reduction
(60.16%), outperforming GDP and MDP by 13.94% and 31.45%, re spectively; With 40% of pa-
rameter reduction, the accuracy loss achieved by our method is lower than SCOP and DMC by
1.04% and 0.54%, respectively.
The results validate that activation-based pruning is more effective than weight magnitude based
pruning in ﬁnding unimportant ﬁlters, as discussed in Secti on 3.1. For example, on ResNet-56, with
1% of accuracy loss, the amount of parameter reduction achie ved by our method is higher than that
of ILP (Renda et al. (2020)) by 34.51%; and with 75% FLOPs redu ction, the accuracy achieved by
our method is higher than PPR (Zhuang et al. (2020)) by 0.87%.
The results conﬁrm that our method can achieve better result s that the state-of-the-art automatic
pruning methods, AMC (He et al. (2018b)). For example, when t argeting at about 55% of FLOPs
reduction on ResNet-56, our method achieves 1.53% higher ac curacy than AMC; and when tar-
geting at about 60% of parameters reduction on ResNet-50, ou r method achieves 0.84% higher
accuracy than AMC. Our activation-based adaptive pruning c an directly target unimportant ﬁlters in
the model, whereas AMC has to use reinforcement learning to e xplore the whole network.
8

--- PAGE 9 ---
Table 3: Results of one-shot pruning with differ-
ent types of attention mapping functions and L1
Norm for VGG-16 on CIFAR-10 dataset.
Method Acc. ↓(%)
Attention Mean (p=1) 0.38
Attention Mean (p=2) 0.42
Attention Mean (p=4) 0.47
Attention Sum (p=1) 0.63
Attention Max (p=1) 0.51
L1 Norm 0.48Table 4: Results of the adaptive pruning strategy
and non-adaptive iterative pruning strategies for
VGG-16 on CIFAR-10 dataset.
Method Acc. ↓(%) Params. ↓(%)
IAP 0.9 69.83
ILP 1.1 69.83
Ours -0.29 70.54
4.2 I MAGE NETRESULTS
Table 2 shows the results of of ResNet-50 on ImageNet dataset and VGG-19 on Tiny-ImageNet
dataset, comparing our method with PFB (Liebenwein et al. (2019)), HRank (Lin et al. (2020)),
ILP(Renda et al. (2020)), Sparse Structure Selection ( SSS) (Huang & Wang (2018)), NN Slimming
(Liu et al. (2017)), and Eigendamage (Wang et al. (2019)). “Ours-1” and “Ours-2” denote the cases
where the pruning objective is set to minimize the number of p arameters and FLOPs, respectively,
while meeting the accuracy target. On ImageNet, for the same level of accuracy loss, our method
reduces signiﬁcantly more parameters (6.45% to 29.61% high er than the related works) and more
FLOPs (0.82% to 17.2% higher than the related works). For the same level of parameters or FLOPs
reduction, our method achieves the smallest accuracy loss, lower than the related works by 0.08%
to 3.21%. On Tiny-ImageNet, for the same level of parameters reduction, our method achieves
signiﬁcantly lower accuracy loss than NN Slimming by 11%; Fo r the same level of accuracy loss,
our method achieves higher parameter reduction (72.58% vs 6 1.87%), and higher FLOPs reduction
(78.97% vs 66.21%) than EigenDamage.
5 A BLATION STUDY
The effect of attention mapping functions. Table 3 shows the Top-1 accuracy loss of the VGG-16
pruned by one-shot pruning with different types of attentio n mapping functions on CIFAR-10. The
parameter reduction is 57.73%, and the accuracy of the origi nal model is 93.63% (5 trails). First, we
analyze three forms of activation-based attention maps wit hpequal to1(discussed in Section 3.1),
noted as Attention Mean ( p“1), Attention Sum ( p“1), Attention Max ( p“1). Attention Mean
leads to the lowest accuracy loss, lower than Attention Sum a nd Attention Max by 0.25% and 0.13%,
respectively. Second, we analyze Attention Mean with diffe rence values of p(p“1,2,4). Whenp
is set to1, it leads to the lowest accuracy loss (0.38% vs 0.42% and 0.47 %). Also, all the forms of
Attention Mean outperforms L1-Norm, which validates that t he proposed activation-based attention
mapping function is more efﬁcient than weight magnitude bas ed methods to evaluate the importance
of ﬁlters.
The effect of adaptive pruning. We compare the proposed adaptive pruning strategy with non-
adaptive iterative pruning strategies that prune a ﬁxed per centage of ﬁlters in each pruning round.
One baseline prunes a ﬁxed percentage of ﬁlters with lowest w eight values, noted as Iterative L1-
Norm Pruning (ILP); the other prunes a fxied percentage of ﬁl ters using proposed attention mapping
function with pequal to1, noted as Iterative Attention Pruning (IAP). Table 4 compar es the Top-1
accuracy loss of the VGG-16 model with sparsity of 70% pruned by IAP, ILP, and the proposed
adaptive pruning strategy on CIFAR-10. For IAP and ILP, the p runing rate is set to 5%, that is, 5%
of ﬁlters are pruned in each pruning round. With 70% reduced p arameters, the proposed adaptive
pruning strategy leads to a higher accuracy than IAP and ILP b y 1.19% and 1.39%, respectively.
6 C ONCLUSIONS
This paper proposes an adaptive, activation-based, iterat ive structured pruning approach to automat-
ically and efﬁciently generate small, accurate, and hardwa re-efﬁcient models that can meet diverse
user requirements. We show that activation-based attentio n value is a more precise indicator to
identify unimportant ﬁlters to prune than the commonly used weight magnitude value. We also of-
fer an effective way to perform structured pruning in an iter ative pruning process, leveraging the
LTH theory to ﬁnd small and accurate sub-networks that are at the same time hardware efﬁcient.
Finally, we argue that automatic pruning is essential for th is method to be useful in practice, and
proposes an adaptive pruning method that can automatically meet user-speciﬁed objectives in terms
9

--- PAGE 10 ---
of model accuracy, size, and inference speed but without use r intervention. Our results conﬁrm that
the proposed method outperforms existing structured pruni ng approaches with a large margin.
REFERENCES
Anubhav Ashok, Nicholas Rhinehart, Fares Beainy, and Kris M Kitani. N2n learning: Net-
work to network compression via policy gradient reinforcem ent learning. arXiv preprint
arXiv:1709.06030 , 2017.
H Cai, T Chen, W Zhang, Y Yu, and J Wang. Reinforcement learnin g for architecture search by
network transformation. corr abs/1707.04873 (2017).
Jonathan Frankle and Michael Carbin. The lottery ticket hyp othesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy , and Michael Carbin. Stabilizing the
lottery ticket hypothesis. arXiv preprint arXiv:1903.01611 , 2019.
Shangqian Gao, Feihu Huang, Jian Pei, and Heng Huang. Discre te model compression with resource
constraint for deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 1899–1908, 2020.
Jinyang Guo, Wanli Ouyang, and Dong Xu. Multi-dimensional p runing: A uniﬁed framework for
model compression. In Proceedings of the IEEE/CVF Conference on Computer Vision a nd Pattern
Recognition , pp. 1508–1517, 2020.
Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, an d Ji Liu. Gdp: Stabilized neural
network pruning via gates with differentiable polarizatio n. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 5239–5250, 2021.
Song Han, Huizi Mao, and William J Dally. Deep compression: C ompressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. S oft ﬁlter pruning for accelerating
deep convolutional neural networks. arXiv preprint arXiv:1808.06866 , 2018a.
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter p runing via geometric median for
deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 4340–4349, 2019.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for ac celerating very deep neural net-
works. In Proceedings of the IEEE international conference on comput er vision , pp. 1389–1397,
2017.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Son g Han. Amc: Automl for model
compression and acceleration on mobile devices. In Proceedings of the European Conference on
Computer Vision (ECCV) , pp. 784–800, 2018b.
P. Hill, A. Jain, M. Hill, B. Zamirai, C. Hsu, M. A. Laurenzano , S. Mahlke, L. Tang, and J. Mars.
Deftnn: Addressing bottlenecks for dnn execution on gpus vi a synapse vector elimination and
near-compute data ﬁssion. In 2017 50th Annual IEEE/ACM International Symposium on Mi-
croarchitecture (MICRO) , pp. 786–799, 2017.
Zehao Huang and Naiyan Wang. Data-driven sparse structure s election for deep neural networks. In
Proceedings of the European conference on computer vision ( ECCV) , pp. 304–320, 2018.
Minsoo Kang and Bohyung Han. Operation-aware soft channel p runing using differentiable masks.
InInternational Conference on Machine Learning , pp. 5122–5131. PMLR, 2020.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain dam age. In Advances in neural
information processing systems , pp. 598–605, 1990.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. S nip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
10

--- PAGE 11 ---
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Pe ter Graf. Pruning ﬁlters for
efﬁcient convnets. ICLR , 2016.
Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, and Radu Timofte. Group sparsity: The
hinge between ﬁlter pruning and decomposition for network c ompression. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recogni tion, pp. 8018–8027, 2020.
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable ﬁlter
pruning for efﬁcient neural networks. arXiv preprint arXiv:1911.07412 , 2019.
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Z hang, Yonghong Tian, and Ling
Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 1529–1538, 2020.
Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liu juan Cao, Qixiang Ye, Feiyue
Huang, and David Doermann. Towards optimal structured cnn p runing via generative adver-
sarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision a nd Pattern
Recognition , pp. 2790–2799, 2019.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Y an, and Changshui Zhang. Learn-
ing efﬁcient convolutional networks through network slimm ing. In Proceedings of the IEEE
international conference on computer vision , pp. 2736–2744, 2017.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Vari ational dropout sparsiﬁes deep neural
networks. In International Conference on Machine Learning , pp. 2498–2507. PMLR, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efﬁcient inference. arXiv preprint arXiv:1611.06440 , 2016.
Alex Renda, Jonathan Frankle, and Michael Carbin. Comparin g rewinding and ﬁne-tuning in neural
network pruning. arXiv preprint arXiv:2003.02389 , 2020.
Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop:
Scientiﬁc control for reliable neural network pruning. arXiv preprint arXiv:2010.10732 , 2020.
Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc H usz´ ar. Faster gaze prediction with
dense networks and ﬁsher pruning. arXiv preprint arXiv:1801.05787 , 2018.
Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning
in the kronecker-factored eigenbasis. In International Conference on Machine Learning , pp.
6566–6575. PMLR, 2019.
Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Z hang, and Xiaolin Hu. Pruning
from scratch. In Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce, volume 34, pp.
12273–12280, 2020.
H Yang, W Wen, and H Deephoyer Li. Learning sparser neural net work with differentiable scale-
invariant sparsity measures. arXiv preprint arXiv:1908.09979 , 2019.
Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang. Gat e decorator: Global
ﬁlter pruning method for accelerating deep convolutional n eural networks. arXiv preprint
arXiv:1909.08174 , 2019.
Sergey Zagoruyko and Nikos Komodakis. Paying more attentio n to attention: Improving the perfor-
mance of convolutional neural networks via attention trans fer.arXiv preprint arXiv:1612.03928 ,
2016.
Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and Qi Tian. Variational
convolutional neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 2780–2789, 2019.
Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Sh uang, and Xiang Li. Neuron-level
structured pruning using polarization regularizer. Advances in Neural Information Processing
Systems , 33, 2020.
11

--- PAGE 12 ---
Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou
Huang, and Jinhui Zhu. Discrimination-aware channel pruni ng for deep neural networks. arXiv
preprint arXiv:1810.11809 , 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le . Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 8697–8710, 2018.
A A PPENDIX
A.1 I MPLEMENTATION DETAILS
We implemented our proposed method on PyTorch version 1.6.0 . For ResNet models on CIFAR-
10, the learning rate is set to 0.1 initially and decays with a rate of 0.1 at the epoch 91 and 136.
Weight decay is set to 0.0002. For ResNet-50 on ImageNet, the learning rate increases to 0.256 in
a warmup mechanism during the ﬁrst 5 epochs, and decays with a factor of 0.1 at epochs 30, 60,
80 (Renda et al. (2020); Frankle et al. (2019)). Nesterov SGD optimizer is used with a momentum
of 0.9 for all models. The simple data augmentation (random c rop and random horizontal ﬂip) is
used for all training images. Learning rate rewinding is use d for iterative pruning, which rewinds
to the epoch at roughly 60% of the total training duration (se e Appendix A.5 for an analysis of the
effect of rewinding epochs on the accuracy of the pruned mode ls). For adaptive pruning, Tandλ
are initialized to 0.0 and 0.005, respectively.
A.2 T HEEFFECT OF ATTENTIONS
Figure 2 shows the attention of each ﬁlter of the ﬁrst convolu tion layer of ResNet-50 on ImageNet
with different values of p(p=1, 2, 4). The setting where pis equal to 1 tends to be best, since iit
promotes the effectiveness of the pruning by enabling the ga p between the mean values of the useful
ﬁlters and useless ﬁlters to be large.
(a) p=1
 (b) p=2
 (c) p=4
Figure 2: Attentions of each ﬁlter of the ﬁrst convolution la yer of ResNet-50 on ImageNet with
different values of p(p“1,2,4).
A.3 M EMORY -CONSTRAINED ADAPTIVE PRUNING
The Memory-constrained Adaptive Pruning Algorithm is show n in Algorithm 4.
A.4 FLOP S-CONSTRAINED ADAPTIVE PRUNING
The FLOPs-constrained Adaptive Pruning Algorithm is shown in Algorithm 5.
A.5 T HEEFFECT OF REWINDING EPOCH
In order to understand how the rewinding impacts accuracy of the pruned models, we analyze sta-
bility to pruning as proposed by Frankle et al. (2019), which is deﬁned as the L2 distance between
the masked weights of the pruned network and the original net work at the end of training. We val-
idate the observations proposed by Frankle et al. (2019) tha t for deep networks, rewinding to very
early stages is sub-optimal as the network has not learned co nsiderably by then; and rewinding to
12

--- PAGE 13 ---
Algorithm 4 Memory-constrained Adaptive Pruning
1:Input: Target Parameters Reduction ParamTarget
2:Output: A small pruned model with an acceptable model size
3: Initialize: T“0.0,λ“0.005.
4:forpruning round r(rě1)do
5: Prune the model using Trrs(Refer to Algorithm 3)
6: Train the pruned model, calculate its remaining number of parameters Param rrs
7: Calculate the parameters reduction: ParamRed rrs:ParamRed rrs “Param r0s ´Param rrs
8: ifParamRed rrs ăParamTarget then
9: ifthe changes of model size are within 0.1% for several rounds then
10: Terminate
11: else
12: λrr`1s “λrrs
13: Trr`1s “Trrs `λrr`1s
14: end if
15: else
16: Find the last acceptable round k
17: ifkhas been used to roll back for several times then
18: Mark kas unacceptable
19: Go to Step 15
20: else
21: Roll back model weights to round k
22: λrr`1s “λrrs{2.0pN`1q(Nis the number of times for rolling back to round k)
23: Trr`1s “Trks `λrr`1s
24: end if
25: end if
26:end for
Algorithm 5 FLOPs-constrained Adaptive Pruning
1:Input: Target FLOPs Reduction FLOPsTarget
2:Output: A small pruned model with an acceptable FLOPs
3: Initialize: T“0.0,λ“0.005.
4:forpruning round r(rě1)do
5: Prune the model using Trrs(Refer to Algorithm 3)
6: Train the pruned model, calculate its remaining FLOPs FLOPs rrs
7: Calculate the parameters reduction: FLOPsRed rrs:FLOPsRed rrs “FLOPs r0s ´FLOPs rrs
8: ifFLOPsRed rrs ăFLOPsTarget then
9: ifthe changes of FLOPs are within 0.1% for several rounds then
10: Terminate
11: else
12: λrr`1s “λrrs
13: Trr`1s “Trrs `λrr`1s
14: end if
15: else
16: Find the last acceptable round k
17: ifkhas been used to roll back for several times then
18: Mark kas unacceptable
19: Go to Step 15
20: else
21: Roll back model weights to round k
22: λrr`1s “λrrs{2.0pN`1q(Nis the number of times for rolling back to round k)
23: Trr`1s “Trks `λrr`1s
24: end if
25: end if
26:end for
13

--- PAGE 14 ---
very late training stages is also sub-optimal because there is not enough time to retrain. Speciﬁcally,
Figure 3(a) shows the Top-1 test accuracy of the pruned ResNe t-50 with 83.74% remaining param-
eters when learning rate is rewound to different epochs, and Figure 3(b) shows the stability values
at the corresponding rewinding epochs. We observe that ther e is a region, 65 to 80 epochs, where
the resulting accuracy is high. We ﬁnd that L2 distance close ly follows this pattern, showing large
distance for early training epochs and small distance for la ter training epochs. Our ﬁndings show
that rewinding to 75%-90% of training time leads to good accu racy.
25556570808588
Rewinding  Epoch69707172737475T op-1 T est Accuracy
IAP
(a) Top-1 Test Accuracy25 55 65 70 80 85 88
Rewinding Epoch0.02.55.07.510.012.515.017.520.0L2 Distance
IAP
(b) Stability to Pruning
Figure 3: The effect of the rewinding epoch (x-axis) on (a) To p-1 test accuracy, and (b) pruning
stability, for pruned ResNet-50, when 83.74% of parameters are remaining.
A.6 T HEEFFECT OF ITERATIVE PRUNING
0 5 10 15 20 25 30 35
R ound−101Acc. Loss
0 5 10 15 20 25 30 35
R ound04080P aram. R eduction
Figure 4: Illustration of the changes
of the accuracy loss and parameter re-
duction over the pruning rounds as the
pruning threshold adapts when pruning
ResNet-56 with CIFAR-10, given the
target of 1% Top-1 accuracy loss.To illustrate the effectiveness of our proposed adaptive
pruning, in Figure 4, we show how the accuracy loss and
parameter reduction change over the pruning rounds as
the pruning threshold adapts according to Algorithm 2,
in the experiment of pruning ResNet-56 on CIFAR-10,
given the target of 1% accuracy loss. From Round 1 to
Round 24, the accuracy loss of the pruned model is lower
than the target accuracy loss (1%), so the algorithm in-
creases the pruning aggressiveness gradually by increas-
ing the threshold. At Round 25, the accuracy loss exceeds
the target, so the algorithm rolls back the model weights
and the pruning threshold to Round 24, and restart the
pruning from there more conservatively. The above pro-
cess repeats until after Round 39, the model size con-
verges, and the algorithm terminates at reducing totally
88.23% of parameters.
14
