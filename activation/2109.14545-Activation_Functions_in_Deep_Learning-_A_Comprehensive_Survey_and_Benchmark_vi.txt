# 2109.14545.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/activation/2109.14545.pdf
# Kích thước tệp: 856567 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Hàm Kích Hoạt trong Deep Learning: Một Khảo Sát Toàn Diện và Đánh Giá
Shiv Ram Dubey1, Satish Kumar Singh1, Bidyut Baran Chaudhuri2
1Phòng thí nghiệm Thị giác Máy tính và Sinh trắc học, Viện Công nghệ Thông tin Ấn Độ, Allahabad, Ấn Độ.
2Đại học Techno India, Kolkata, Ấn Độ và Viện Thống kê Ấn Độ, Kolkata, Ấn Độ.
srdubey@iiita.ac.in, sk.singh@iiita.ac.in, bidyutbaranchaudhuri@gmail.com
Bài báo này đã được chấp nhận tại Neurocomputing. Bản quyền sẽ được chuyển giao cho Elsevier.

Tóm tắt
Mạng thần kinh đã có sự phát triển mạnh mẽ trong những năm gần đây để giải quyết nhiều vấn đề. Các loại mạng thần kinh khác nhau đã được giới thiệu để xử lý các loại vấn đề khác nhau. Tuy nhiên, mục tiêu chính của bất kỳ mạng thần kinh nào là chuyển đổi dữ liệu đầu vào không thể phân tách tuyến tính thành các đặc trưng trừu tượng có thể phân tách tuyến tính hơn bằng cách sử dụng một hệ thống phân cấp các lớp. Những lớp này là sự kết hợp của các hàm tuyến tính và phi tuyến tính. Các lớp phi tuyến tính phổ biến và thông dụng nhất là các hàm kích hoạt (AFs), chẳng hạn như Logistic Sigmoid, Tanh, ReLU, ELU, Swish và Mish. Trong bài báo này, một tổng quan toàn diện và khảo sát được trình bày cho AFs trong mạng thần kinh cho deep learning. Các lớp khác nhau của AFs như dựa trên Logistic Sigmoid và Tanh, dựa trên ReLU, dựa trên ELU, và dựa trên học tập được bao quát. Một số đặc tính của AFs như phạm vi đầu ra, tính đơn điệu, và tính mượt mà cũng được chỉ ra. Một so sánh hiệu suất cũng được thực hiện giữa 18 AFs tiên tiến với các mạng khác nhau trên các loại dữ liệu khác nhau. Những hiểu biết về AFs được trình bày để có lợi cho các nhà nghiên cứu trong việc thực hiện nghiên cứu thêm và các nhà thực hành trong việc lựa chọn giữa các lựa chọn khác nhau. Mã nguồn được sử dụng cho so sánh thực nghiệm được phát hành tại: https://github.com/shivram1987/ActivationFunctions .

1. Giới thiệu
Trong những năm gần đây, deep learning đã cho thấy sự tăng trưởng mạnh mẽ để giải quyết các vấn đề thách thức như phát hiện đối tượng [1], phân đoạn ngữ nghĩa [2], nhận dạng lại người [3], truy xuất hình ảnh [4], phát hiện bất thường [5], chẩn đoán bệnh da [6], và nhiều hơn nữa. Các loại mạng thần kinh khác nhau đã được định nghĩa trong deep learning để học các đặc trưng trừu tượng từ dữ liệu, như Multilayer Perceptron (MLP) [7], Convolutional Neural Networks (CNN) [8], Recurrent Neural Networks (RNN) [9], và Generative Adversarial Networks (GAN) [10]. Những khía cạnh quan trọng của mạng thần kinh bao gồm khởi tạo trọng số [11], hàm mất mát [12], các lớp khác nhau [13], overfitting [14], và tối ưu hóa [15].

Các hàm kích hoạt (AFs) đóng vai trò rất quan trọng trong mạng thần kinh [16] bằng cách học các đặc trưng trừu tượng thông qua các phép biến đổi phi tuyến tính. Một số tính chất phổ biến của AFs như sau: a) nó nên thêm độ cong phi tuyến tính vào cảnh quan tối ưu hóa để cải thiện sự hội tụ huấn luyện của mạng; b) nó không nên làm tăng độ phức tạp tính toán của mô hình một cách quá mức; c) nó không nên cản trở dòng gradient trong quá trình huấn luyện; d) nó nên duy trì phân phối của dữ liệu để tạo điều kiện thuận lợi cho việc huấn luyện mạng tốt hơn. Một số AFs đã được khám phá trong những năm gần đây cho deep learning để đạt được các tính chất đã đề cập ở trên. Cuộc khảo sát này được dành riêng cho các phát triển trong lĩnh vực AFs trong mạng thần kinh. Những hiểu biết về các AFs khác nhau được trình bày cùng với lý luận để có lợi cho cộng đồng deep learning. Các đóng góp chính của cuộc khảo sát này được nêu như sau:

1. Cuộc khảo sát này cung cấp một phân loại chi tiết cho một loạt rộng các AFs. Nó cũng bao gồm các AFs một cách rất toàn diện, bao gồm Logistic Sigmoid/Tanh, Rectified Unit, Exponential Unit, và Adaptive AFs.

2. Cuộc khảo sát này làm giàu cho người đọc với các AFs tiên tiến với phân tích từ các góc độ khác nhau. Nó bao quát cụ thể về sự tiến bộ trong AFs cho deep learning.

3. Cuộc khảo sát này cũng tóm tắt các AFs với những điểm nổi bật ngắn gọn và các thảo luận quan trọng để mô tả tính phù hợp của nó cho các loại dữ liệu khác nhau (Tham khảo Bảng 6).

4. Cuộc khảo sát này được so sánh với cuộc khảo sát hiện có và phân tích hiệu suất để cho thấy tầm quan trọng của nó (Tham khảo Bảng 7).

5. Bài báo này cũng trình bày các so sánh hiệu suất trên 4 bộ dữ liệu chuẩn của các phương thức khác nhau sử dụng 18 AFs tiên tiến với các loại mạng khác nhau (Tham khảo Bảng 8, 9 và 11).

Sự tiến hóa của AFs được minh họa trong Phần 2. Sự tiến bộ trong Logistic Sigmoid và Tanh, rectified, exponential, adaptive và miscellaneous AFs được tóm tắt trong Phần 3, 4, 5, 6, và 7, tương ứng. Một số khía cạnh của AFs được thảo luận trong Phần 8. Một phân tích hiệu suất toàn diện được thực hiện trong Phần 9. Một tóm tắt với kết luận và khuyến nghị được cung cấp trong Phần 10.

arXiv:2109.14545v3  [cs.LG]  28 Jun 2022

--- TRANG 2 ---
Hình 1: Minh họa các AF Tuyến tính, Logistic Sigmoid và Tanh.

2. Sự tiến hóa của các Hàm Kích hoạt
Một hàm tuyến tính có thể được coi là một AF đơn giản xuất ra cx cho đầu vào x với c là một hằng số. AF tuyến tính được minh họa trong Hình 1 với c=1, tức là, hàm đồng nhất. Lưu ý rằng AF tuyến tính không thêm tính phi tuyến tính vào mạng. Tuy nhiên, tính phi tuyến tính cần được đưa vào mạng thần kinh. Nếu không, một mạng thần kinh tạo ra đầu ra như một hàm tuyến tính của đầu vào mặc dù có nhiều lớp. Hơn nữa, trong thực tế, dữ liệu thường không thể phân tách tuyến tính; do đó, các lớp phi tuyến tính giúp chiếu dữ liệu theo cách phi tuyến tính trong không gian đặc trưng có thể được sử dụng với các hàm mục tiêu khác nhau. Phần này cung cấp một tổng quan về sự tiến hóa của AFs cho deep learning. Một phân loại được trình bày trong Hình 2 theo các tính chất khác nhau và các loại đặc trưng.

Hàm Kích hoạt Dựa trên Logistic Sigmoid/Tanh Unit: Để đưa tính phi tuyến tính vào mạng thần kinh, các AFs Logistic Sigmoid và Tanh đã được sử dụng trong những ngày đầu. Việc kích hoạt của các nơ-ron sinh học là động lực để sử dụng các AFs Logistic Sigmoid và Tanh với các nơ-ron nhân tạo. AF Logistic Sigmoid là một hàm phi tuyến tính rất phổ biến và truyền thống. Nó được cho như sau:

Logistic Sigmoid (x) = 1 / (1 + e^(-x))  (1)

AF này nén đầu ra giữa [0,1] như được thể hiện trong Hình 1. Đầu ra của hàm Logistic Sigmoid bị bão hòa đối với đầu vào cao hơn và thấp hơn, điều này dẫn đến vấn đề gradient biến mất. Vấn đề gradient biến mất mô tả một tình huống trong đó gradient của hàm mục tiêu w.r.t. một tham số trở nên rất gần với zero và dẫn đến hầu như không có cập nhật nào trong các tham số trong quá trình huấn luyện mạng sử dụng kỹ thuật stochastic gradient descent. Do đó, việc huấn luyện gần như bị dừng lại dưới tình huống gradient biến mất. Hơn nữa, đầu ra không tuân theo tính chất zero-centric dẫn đến sự hội tụ kém. Hàm Tanh cũng đã được sử dụng như AF trong mạng thần kinh. Nó tương tự như hàm Logistic Sigmoid trong khi thể hiện tính chất zero centric như được mô tả trong Hình 1.

Hàm Kích hoạt (AFs)

Sigmoid/ Tanh Unit Based AFs | Tính chất của AFs | Ứng dụng khác nhau | Loại Đặc trưng
Rectified Unit Based AFs | Exponential Unit Based AFs | Adaptive Unit Based AFs | Miscellaneous AFs
Parametric vs Non-parametric | Monotonic vs Non-monotonic | Smooth vs Non-smooth | Bounded vs Un-bounded
Ứng dụng dựa trên Hình ảnh | Ứng dụng dựa trên Văn bản | Ứng dụng liên quan đến Trò chơi | Ứng dụng Chuỗi thời gian | Ứng dụng dựa trên Tín hiệu | Phạm vi Đầu ra của AFs

Hình 2: Phân loại các hàm kích hoạt.

Hàm Tanh được viết như sau:
Tanh (x) = (e^x - e^(-x)) / (e^x + e^(-x))  (2)

Hàm Tanh cũng nén các đầu vào, nhưng trong [-1; 1]. Các nhược điểm của hàm Logistic Sigmoid như gradient biến mất và độ phức tạp tính toán cũng tồn tại với hàm Tanh. Các AFs Logistic Sigmoid và Tanh chủ yếu gặp phải vấn đề gradient biến mất. Một số cải tiến đã được đề xuất dựa trên các AFs Logistic Sigmoid và Tanh được mô tả chi tiết trong Phần 3.

Hàm Kích hoạt Dựa trên Rectified Linear Unit: Đầu ra bão hòa và độ phức tạp tăng lên là những hạn chế chính của các AFs dựa trên Logistic Sigmoid và Tanh đã đề cập ở trên. Rectified Linear Unit (ReLU) [17] đã trở thành AF tiên tiến do tính đơn giản và hiệu suất cải thiện. ReLU cũng được sử dụng trong mô hình AlexNet [8]. Các biến thể khác nhau của ReLU đã được nghiên cứu bằng cách giải quyết các nhược điểm của nó, chẳng hạn như không sử dụng các giá trị âm, tính phi tuyến tính hạn chế và đầu ra không giới hạn, như được chi tiết trong Phần 4.

Hàm Kích hoạt Dựa trên Exponential Unit: Vấn đề chính mà các AFs dựa trên Logistic Sigmoid và Tanh gặp phải là với đầu ra bão hòa cho đầu vào dương và âm lớn. Tương tự, vấn đề chính với các AFs dựa trên ReLU là với việc không sử dụng đầy đủ các giá trị âm dẫn đến gradient biến mất. Để đối phó với những hạn chế này, các AFs dựa trên hàm exponential đã được sử dụng trong văn liệu. AF dựa trên Exponential Linear Unit (ELU) [27] sử dụng các giá trị âm với sự trợ giúp của hàm exponential. Một số AFs đã được giới thiệu trong văn liệu như các biến thể ELU được trình bày chi tiết trong Phần 5.

Hàm Kích hoạt Học tập/Thích ứng: Hầu hết các AFs dựa trên Sigmoid, Tanh, ReLU, và ELU được thiết kế thủ công có thể không thể khai thác được độ phức tạp của dữ liệu. Các AFs thích ứng dựa trên học tập là xu hướng gần đây. Lớp AFs này chứa các tham số có thể học được, ví dụ như Adaptive Piecewise Linear (APL) [28] và Swish [29] AFs chứa hai và một tham số có thể học được, tương ứng. Gần đây, một số AFs dựa trên học tập đã được đề xuất như được minh họa trong Phần 6.

--- TRANG 3 ---
Bảng 1: Ưu điểm và nhược điểm của các AF chính.

AFs | Gradients giảm dần | Phi tuyến tính hạn chế | Khó khăn tối ưu hóa | Thiếu khả năng thích ứng | Không hiệu quả tính toán
Sigmoid | Có | Không | Có | Có | Có
Tanh | Có | Không | Một phần | Có | Có
ReLU | Một phần | Có | Một phần | Có | Không
ELU | Không | Một phần | Không | Có | Một phần
APL | Không | Một phần | Không | Không | Không
Swish | Không | Một phần | Không | Không | Một phần

Bảng 2: Tóm tắt các hàm kích hoạt dựa trên Logistic Sigmoid và Tanh.

Tên AF | Tham số | Đơn điệu | Mượt | Có giới hạn
Logistic Sigmoid | Không | Có | Có | Có
Tanh | Không | Có | Có | Có
Scaled Tanh (sTanh), 1998 [18] | Có | Có | Có | Có
Rectified Hyperbolic Secant (ReSech), 2016 [19] | Không | Không | Có | Có
Scaled Sigmoid (sSigmoid), 2016 [20] | Không | Có | Có | Có
Penalized Tanh (pTanh), 2016 [20] | Không | Có | Không | Có
Hexpo, 2017 [21] | Không | Có | Có | Có
Improved Sigmoid (ISigmoid), 2018 [22] | Không | Có | Có | Không
Sigmoid-Weighted Linear Units (SiLU), 2018 [23] | Không | Không | Có | Cho đầu vào âm
Linearly Scaled Hyperbolic Tangent (LiSHT), 2019 [24] | Không | Không | Có | Không
Elliott, 2019 [25] | Không | Có | Có | Có
Soft-Root-Sign (SRS), 2020 [26] | Có | Không | Có | Có

Hàm Kích hoạt Tổng hợp: Trong những năm gần đây, nhiều AFs khác cũng đã được nghiên cứu như được trình bày trong Phần 7. Những kích hoạt này bao gồm các đơn vị Softplus, hàm xác suất, hàm đa thức, và hàm nhân.

Bảng 1 làm nổi bật ưu điểm và nhược điểm của các AF chính về mặt gradients giảm dần, phi tuyến tính hạn chế, khó khăn tối ưu hóa, không hiệu quả tính toán và thiếu khả năng thích ứng. Có thể nhận thấy rằng hàm Tanh không hiệu quả về mặt tính toán vì nó liên quan đến việc tính toán exponential nhiều lần [30]. Tuy nhiên, trong việc triển khai, nó có thể được tính toán bằng cách sử dụng exponential đơn với sự trợ giúp của hàm Sigmoid. Những hạn chế này trong các AF hiện có đã là những yếu tố thúc đẩy cho việc phát triển các AF gần đây như được khảo sát trong các phần tiếp theo của bài báo này.

3. AFs Dựa trên Logistic Sigmoid và Tanh

Các AF truyền thống như Logistic Sigmoid và Tanh đã được sử dụng rất rộng rãi trong những ngày đầu của mạng thần kinh. Tuy nhiên, những AF này đã cho thấy trở ngại trong việc huấn luyện các mạng sâu do đầu ra bão hòa của chúng. Một số nỗ lực cũng đã được thực hiện để cải thiện những AF này cho các mạng khác nhau. Bảng 2 trình bày so sánh các AF dựa trên Logistic Sigmoid và Tanh về mặt tính chất của chúng bao gồm tham số, đơn điệu, mượt và có giới hạn.

Để giải quyết vấn đề phạm vi đầu ra hạn chế và gradient bằng không của Tanh, một Hyperbolic Tangent có tỷ lệ (sTanh) được sử dụng trong [18] được định nghĩa là:

sTanh (x) = A*Tanh (B*x)  (3)

với phạm vi đầu ra trong [-A; A]. Một Hàm Sigmoid Tham số (PSF) được đề xuất như một hàm liên tục, khả vi, và có giới hạn như sau:

PSF (x) = 1 / (1 + e^(-x))^m  (4)

trong đó m là một siêu tham số [31]. Dòng gradient được cải thiện cho giá trị m cao hơn. Tổng của log-sigmoid dịch chuyển cũng được khám phá như một AF [32] giữ lại tính đối xứng trong các đặc trưng được tạo ra. AF Rectified Hyperbolic Secant (ReSech) có thể khả vi, đối xứng, và có giới hạn [19] được cho như sau:

ReSech (x) = x*Sech (x)  (5)

với phạm vi đầu ra trong [-1; 1]. Tuy nhiên, nó thể hiện vấn đề gradient biến mất do hành vi bão hòa cho cả đầu vào dương lớn và đầu vào âm lớn. Việc huấn luyện các mạng sâu trở nên khó khăn do độ dốc đồng nhất của các AF Logistic Sigmoid và Tanh gần gốc [20]. Để giảm thiểu hạn chế này, Scaled Sigmoid (sSigmoid) được định nghĩa là:

sSigmoid (x) = (4*Sigmoid (x) - 2)  (6)

với phạm vi đầu ra trong [-2; 2] và Penalized Tanh (pTanh) được định nghĩa là:

pTanh (x) = { Tanh (x); nếu x ≥ 0
            { a*Tanh (x); nếu x < 0  (7)

với phạm vi đầu ra trong [-a; 1] trong đó a ∈ (0; 1). Tuy nhiên, các AF sSigmoid và pTanh cũng gặp phải vấn đề gradient biến mất. Được nhận thấy rằng AF pTanh hoạt động tốt hơn cho các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) [33].

Một AF có nhiễu được định nghĩa để khắc phục vấn đề gradient biến mất [48]. Do nhiễu được thêm vào, các gradient có thể chảy dễ dàng ngay cả trong chế độ bão hòa.

--- TRANG 4 ---
Bảng 3: Tóm tắt các hàm kích hoạt dựa trên Rectified Linear Unit.

Tên | Tham số | Đơn điệu | Mượt | Có giới hạn
Rectified Linear Unit (ReLU), 2010 [17] | Không | Có | Không | Cho đầu vào âm
Leaky ReLU (LReLU), 2013 [34] | Không | Có | Không | Không
Parametric ReLU (PReLU), 2015 [35] | Có | Có | Không | Không
Randomized ReLU (RReLU), 2015 [35] | Không | Có | Không | Không
Concatenated ReLU (CReLU), 2016 [36] | Không | Có | Không | Cho đầu vào âm
Bounded ReLU (BReLU), 2016 [37] | Không | Có | Không | Có
Parametric Tanh Linear Unit (PTELU), 2017 [38] | Có | Có | Có | Cho đầu vào âm
Flexible ReLU (FReLU), 2018 [39] | Có | Có | Không | Cho đầu vào âm
Elastic ReLU (EReLU), 2018 [40] | Không | Có | Không | Cho đầu vào âm
Randomly Translational ReLU (RTReLU), 2018 [41] | Không | Có | Không | Cho đầu vào âm
Dual ReLU (DualReLU), 2018 [42] | Không | Có | Không | Không
Paired ReLU (PairedReLU), 2018 [43] | Có | Có | Không | Không
Average Biased ReLU (ABReLU), 2018 [44] | Không | Có | Không | Cho đầu vào âm
Natural-Logarithm (NLReLU), 2019 [45] | Không | Có | Không | Cho đầu vào âm
Multi-bin Trainable Linear Units (MTLU), 2019 [46] | Có | Không | Không | Không
Lipschitz ReLU (L-ReLU), 2020 [47] | Có | Phụ thuộc vào α và β | Phụ thuộc vào α và β | Phụ thuộc vào α và β

Vấn đề gradient biến mất được giảm thiểu bởi hàm Hexpo [21] tương tự như Tanh với gradient có tỷ lệ. Nó được cho như sau:

Hexpo (x) = { a*(e^(x/b) - 1); nếu x ≥ 0
            { c*(e^(x/d) - 1); nếu x < 0  (8)

trong phạm vi đầu ra của [-c; a]. Đầu ra của hàm sigmoid được nhân với đầu vào của nó trong AF sigmoid-weighted linear unit (SiLU) [23] như sau:

SiLU (x) = x*Sigmoid (x)  (9)

trong phạm vi đầu ra của (-0.5; ∞). Đồng thời, một AF improved logistic Sigmoid (ISigmoid) [22] được đề xuất để giải quyết vấn đề gradient biến mất của Sigmoid với sự trợ giúp của một kết hợp từng phần của các hàm sigmoidal và tuyến tính. Nó được định nghĩa là:

ISigmoid (x) = { (x - a) + Sigmoid (a); nếu x ≥ a
               { Sigmoid (x); nếu -a < x < a
               { (x + a) + Sigmoid (-a); nếu x ≤ -a  (10)

trong phạm vi đầu ra của (-∞; ∞). AF Linearly scaled hyperbolic tangent (LiSHT) chia tỷ lệ Tanh theo cách tuyến tính để khắc phục vấn đề gradient biến mất [24]. LiSHT có thể được định nghĩa là:

LiSHT (x) = x*Tanh (x)  (11)

trong phạm vi đầu ra của [0; ∞). Hàm LiSHT đối xứng, nhưng có nhược điểm là bao gồm các đầu ra không giới hạn và chỉ không âm. AF Elliott [25] tương tự như hàm Sigmoid về mặt biểu đồ đặc tính và được định nghĩa là:

Elliott (x) = 0.5*x / (1 + |x|) + 0.5  (12)

trong phạm vi đầu ra của [0; 1]. AF Soft-Root-Sign (SRS) [26] được định nghĩa là:

SRS (x) = x / (α*|x| + e^(-β*x))  (13)

trong phạm vi đầu ra của [-α/e^β; ∞] trong đó α và β là các tham số có thể học được. Việc sử dụng các tham số bổ sung làm tăng độ phức tạp của hàm SRS. Hầu hết các biến thể của các AF Sigmoid/Tanh đã cố gắng khắc phục vấn đề gradient biến mất. Tuy nhiên, vấn đề này vẫn còn tồn tại trong hầu hết các AF này.

4. Hàm Kích hoạt Rectified

Một tóm tắt về các AF rectified được minh họa trong Bảng 3. Rectified Linear Unit (ReLU) là một hàm đơn giản, là hàm đồng nhất cho đầu vào dương và bằng không cho đầu vào âm và được cho như sau:

ReLU (x) = max(0; x) = { x; nếu x ≥ 0
                       { 0; ngược lại  (14)

Do đó, phạm vi của ReLU là [0; ∞). Gradient cho đầu vào dương và âm lần lượt là một và không. Hàm ReLU giải quyết vấn đề độ phức tạp tính toán của các hàm Logistic Sigmoid và Tanh. Nhược điểm của ReLU là vấn đề gradient biến mất đối với các đầu vào âm. Mặc dù có vấn đề gradient biến mất, AF ReLU đã được sử dụng rất rộng rãi với các mô hình deep learning. Các tiến bộ trong các AF dựa trên ReLU được thảo luận trong phần còn lại của phần này.

4.1. Về việc Không sử dụng Giá trị Âm của ReLU

Gradient biến mất là vấn đề chính với AF ReLU được gây ra do việc không sử dụng các giá trị âm. Leaky Rectified Linear Unit (LReLU) là phần mở rộng của ReLU bằng cách sử dụng các giá trị âm [34]. LReLU được định nghĩa là:

LReLU (x) = { x; nếu x ≥ 0
            { 0.01*x; nếu x < 0  (15)

trong phạm vi đầu ra của (-∞; ∞). LReLU đã được sử dụng trong nhiều ứng dụng với hiệu suất hứa hẹn. Một vấn đề chính liên quan đến LReLU là việc tìm ra độ dốc phù hợp trong hàm tuyến tính cho các đầu vào âm. Các độ dốc khác nhau có thể phù hợp cho các vấn đề khác nhau và các mạng khác nhau.

--- TRANG 5 ---
Do đó, nó được mở rộng thành Parametric ReLU (PReLU) bằng cách xem xét độ dốc cho đầu vào âm như một tham số có thể huấn luyện [35]. PReLU được cho như sau:

PReLU (x) = { x; nếu x ≥ 0
            { p*x; nếu x < 0  (16)

trong phạm vi đầu ra của (-∞; ∞) trong đó p là tham số có thể huấn luyện. Tuy nhiên, nó có thể dẫn đến overfitting dễ dàng, đây là nhược điểm của PReLU. Lớp Maxout, tính toán giá trị tối đa của nhiều đơn vị tuyến tính, cũng được sử dụng như AF [49]. Cả ReLU và Leaky ReLU đều có thể được xem như các trường hợp đặc biệt của Maxout.

Randomized ReLU (RReLU) xem xét độ dốc của LReLU một cách ngẫu nhiên trong quá trình huấn luyện được lấy mẫu từ một phân phối đồng nhất U(l; u) [50]. RReLU được định nghĩa là:

RReLU (x) = { x; nếu x ≥ 0
            { R*x; nếu x < 0  (17)

trong phạm vi đầu ra của (-∞; ∞) trong đó R ~ U(l; u), l < u và l, u ∈ [0; 1). Nó sử dụng một giá trị xác định x = (l + u)/2 trong thời gian kiểm tra.

ReLU không thể sử dụng thông tin hữu ích tiềm năng từ các giá trị âm. Trong hầu hết các mạng, bản đồ đặc trưng được cung cấp như đầu vào cho AF dày đặc gần zero. Do đó, một nhiễu nhỏ trong điểm rectification có thể dẫn đến khó khăn trong việc huấn luyện. Concatenated ReLU (CReLU) [36] nối đầu ra của ReLU trên đầu vào gốc và đầu vào bị phủ định. CReLU có thể được cho như sau:

CReLU (x) = [ReLU (x); ReLU (-x)]  (18)

trong phạm vi đầu ra của [0; ∞). CReLU được suy ra từ thực tế rằng các kernel lớp thấp hơn trong các mô hình CNN tạo thành các cặp với các pha đối lập. Việc dịch chuyển bản đồ đặc trưng với nhiều bias cũng được thực hiện trước lớp ReLU [51]. Tuy nhiên, nó làm tăng độ phức tạp của mô hình vì cần nhiều ReLU hơn. Parametric Tan Hyperbolic Linear Unit (P-TELU) cũng được sử dụng như một AF [38]. P-TELU được định nghĩa là:

P-TELU (x) = { x; nếu x ≥ 0
             { α*Tanh (β*x); nếu x < 0  (19)

trong phạm vi đầu ra của [-∞; ∞) trong đó {α; β} ≥ 0 là các tham số có thể học được.

Flexible ReLU (FReLU) [39] nắm bắt các giá trị âm với một điểm rectified được xem xét như có thể huấn luyện trong Shifted ReLU [39]. FReLU được cho như sau:

FReLU (x) = ReLU (x) + b  (20)

trong phạm vi đầu ra của [b; ∞). Một sắp xếp tương tự cũng được tuân theo bởi Random Translation ReLU (RTReLU) [41] bằng cách sử dụng một offset, được lấy mẫu từ phân phối Gaussian, được cho như sau:

RTReLU (x) = { x + a; nếu x + a > 0
             { 0; nếu x + a ≤ 0  (21)

trong phạm vi đầu ra của [0; ∞) trong đó a là một số ngẫu nhiên. Tại thời gian kiểm tra, offset được đặt thành zero. Average Biased ReLU (AB-ReLU) [44] phụ thuộc vào dữ liệu cũng được nghiên cứu để giải quyết các giá trị âm bằng cách dịch chuyển ngang dựa trên trung bình của các đặc trưng. ABReLU có thể được viết là:

ABReLU (x) = { x - μ; nếu x - μ ≥ 0
             { 0; nếu x - μ < 0  (22)

có phạm vi đầu ra trong [0; ∞) trong đó μ được tính toán như trung bình của bản đồ kích hoạt đầu vào cho hàm kích hoạt. Ngưỡng phụ thuộc vào batch cho ReLU được sử dụng bởi Dynamic ReLU (D-ReLU) [60]. Dual ReLU (DualReLU) [42] là một AF hai chiều cho mạng thần kinh hồi quy. DualReLU được cho như sau:

DualReLU (a; b) = max(0; a) - max(0; -b)  (23)

trong phạm vi đầu ra của (-∞; ∞) trong đó a và b là các đầu vào trong các chiều khác nhau. Tương tự như CReLU, AF PairedReLU được sử dụng cho siêu phân giải hình ảnh [43]. PairedReLU được cho như sau:

PairedReLU (x) = [max(-s*x; 0); max(s*p*x - p; 0)]  (24)

trong phạm vi đầu ra của (-∞; ∞). Tuy nhiên, độ phức tạp tính toán của PairedReLU tăng so với CReLU. Trong một nỗ lực khác, AF V-shaped ReLU (vReLU) [61] được định nghĩa là:

vReLU (x) = { x; nếu x ≥ 0
            { -x; nếu x < 0  (25)

có phạm vi đầu ra trong [0; ∞]. Hàm kích hoạt vReLU gặp phải vấn đề đầu ra không đối xứng. AF SignReLU sử dụng các giá trị âm bằng cách sử dụng hàm Softsign [62]. Phần dương của SignReLU giống như ReLU.

Displaced ReLU (DisReLU) [63] được thiết kế như một tổng quát hóa của Shifted ReLU [39]. DisReLU dịch chuyển điểm rectification để xem xét các giá trị âm, được cho như sau:

DisReLU (x) = { x; nếu x ≥ λ
              { λ; nếu x < λ  (26)

có phạm vi đầu ra trong [λ; ∞]. Bendable Linear Unit (BLU) AF được nghiên cứu như sau:

BLU (x) = (√(x² + 1) - 1) + αx  (27)

trong đó -1 ≤ α ≤ 1 là một tham số có thể học được để thích ứng hình dạng giữa hàm đồng nhất và hàm rectifier [64]. Lipschitz ReLU (L-ReLU) AF sử dụng các hàm tuyến tính từng phần để mô hình hóa mức độ hiện diện và mức độ vắng mặt của các đặc trưng [47]. L-ReLU được định nghĩa là:

L-ReLU (x) = { max(α(x); 0); nếu x ≥ 0
             { min(β(x); 0); nếu x < 0  (28)

trong đó α và β là các hàm phi tuyến tính. Hơn nữa, phạm vi của L-ReLU cũng phụ thuộc vào các giá trị của các hàm α và β.

--- TRANG 6 ---
Bảng 4: Tóm tắt các hàm kích hoạt dựa trên Exponential Linear Unit.

Tên | Tham số | Đơn điệu | Mượt | Có giới hạn
Exponential Linear Unit (ELU), 2016 [27] | Có | Có | Có | Cho đầu vào âm
Scaled ELU (SELU), 2017 [52] | Có | Có | Có | Cho đầu vào âm
Continuously Differentiable ELU (CELU), 2017 [53] | Có | Có | Không | Cho đầu vào âm
Parametric ELU (PELU), 2017 [54] | Có | Có | Không | Cho đầu vào âm
Multiple PELU (MPELU), 2018 [55] | Có | Có | Không | Cho đầu vào âm
Fast ELU (FELU), 2019 [56] | Có | Có | Không | Cho đầu vào âm
Parametric Rectified Exponential Unit (PREU), 2019 [57] | Có | Không | Có | Cho đầu vào âm
Elastic ELU (EELU), 2020 [58] | Có | Có | Không | Cho đầu vào âm
Parametric Deformable ELU (PDELU), 2020 [59] | Có | Có | Có | Cho đầu vào âm

4.2. Về Tính Phi tuyến Hạn chế của ReLU

S-shaped ReLU (SReLU) tăng tính phi tuyến trong ReLU bằng cách kết hợp ba hàm tuyến tính với bốn tham số có thể học được [65]. Trên một hướng tương tự, Multi-bin Trainable Linear Unit (MTLU) [46] xem xét nhiều bin để tăng khả năng phi tuyến. MTLU có thể được viết là:

MTLU (x) = { a₀x + b₀; nếu x ≤ c₀
           { aₖx + bₖ; nếu cₖ₋₁ < x ≤ cₖ
           { ...
           { aₖx + bₖ; nếu cₖ₋₁ < x  (29)

có phạm vi đầu ra trong (-∞; ∞). Số lượng bin và phạm vi của bin là các siêu tham số, trong khi hàm tuyến tính của một bin có thể huấn luyện được (tức là, a₀, ..., aₖ, b₀, ..., bₖ là các tham số có thể học được). Bản chất không khả vi tại nhiều điểm là nhược điểm của MTLU. Elastic ReLU (EReLU) xem xét một độ dốc được rút ngẫu nhiên từ một phân phối đồng nhất trong quá trình huấn luyện cho các đầu vào dương để kiểm soát lượng phi tuyến [40]. EReLU được định nghĩa là:

EReLU (x) = max(R*x; 0)  (30)

trong phạm vi đầu ra của [0; ∞) trong đó R là một số ngẫu nhiên. Tại thời gian kiểm tra, EReLU trở thành hàm đồng nhất cho các đầu vào dương. Hàm Linearized Sigmoidal Activation (LiSHA) xem xét ba hàm tuyến tính để tăng các đặc tính phi tuyến [66]. Nó cũng được mở rộng thành AF sigmoidal tuyến tính thích ứng bằng cách học độ dốc của các hàm tuyến tính trên và dưới. ReLU được kết hợp với Tanh như Rectified Linear Tanh (ReLTanh) [67] để tăng tính phi tuyến của ReLU và khắc phục vấn đề gradient biến mất của Tanh. Tuy nhiên, ReLTanh không giới hạn trong cả hướng dương và âm. Natural-Logarithm ReLU (NLReLU) sửa đổi đầu ra của ReLU cho các đầu vào dương bằng cách sử dụng hàm logarithm để tăng mức độ phi tuyến [45]. NLReLU được định nghĩa là:

NLReLU (x) = ln(max(0; x) + 1.0)  (31)

có phạm vi đầu ra trong [0; ∞) trong đó λ là một hằng số. NLReLU không ảnh hưởng đến chế độ âm, do đó gặp phải vấn đề gradient biến mất. Khái niệm của Leaky ReLU (LReLU) được cải thiện thêm thành Dynamic ReLU [68] bằng cách xem xét một siêu tham số bổ sung dựa trên sai số bình phương trung bình (MSE). Do đó, nó có thể kiểm soát độ dốc của Dynamic ReLU trong mỗi epoch dựa trên sự hội tụ. Piecewise Linear Unit (PLU) [69] được định nghĩa là:

PLU (x) = max((αx + c) - c; min((βx - c) + c; x))  (32)

có phạm vi đầu ra trong [-γ; +γ], trong đó α, β và c là các hằng số. Về cơ bản, hàm kích hoạt PLU bao gồm ba hàm tuyến tính theo từng phần, nhưng liên tục. Do đó, nó tránh được sự bão hòa và dẫn đến một lượng tốt của dòng gradient thông qua hàm kích hoạt trong quá trình lan truyền ngược để giải quyết các vấn đề gradient biến mất của ReLU và Tanh. Tuy nhiên, kích hoạt PLU không giới hạn trong cả hướng dương và âm.

4.3. Về Đầu ra Không giới hạn của ReLU

Các đầu ra không giới hạn của ReLU và nhiều biến thể của nó có thể dẫn đến sự không ổn định trong huấn luyện. Hơn nữa, AF có giới hạn là cần thiết cho các ứng dụng hệ thống nhúng dựa trên phần cứng chuyên dụng. ReLU được mở rộng thành Bounded ReLU (BReLU) [37] được định nghĩa là:

BReLU (x) = min(max(0; x); A)  (33)

có phạm vi đầu ra trong [0; A]. Tính ổn định huấn luyện được cải thiện trong BReLU do hai rectification (tức là, tại 0 và A). ReLU là một lựa chọn phổ biến trong thực tế trong deep learning. Các AF dựa trên ReLU thường hiệu quả. Các nhược điểm chính của ReLU, chẳng hạn như gradient giảm dần cho đầu vào âm, phi tuyến tính hạn chế và không giới hạn, được cải thiện trong các AF khác nhau. Tuy nhiên, các biến thể ReLU không thể giải quyết tất cả các vấn đề của ReLU.

5. Hàm Kích hoạt Exponential

Các AF exponential giải quyết vấn đề gradient giảm dần của ReLU. Bảng 4 liệt kê các tính chất của các AF exponential. Exponential Linear Unit (ELU) [27] được cho như sau:

ELU (x) = { x; nếu x > 0
          { α*(e^x - 1); nếu x ≤ 0  (34)

có phạm vi đầu ra trong [-α; ∞) trong đó α là một tham số có thể học được. Hàm ELU thể hiện tất cả các lợi ích của hàm ReLU. ELU có thể khả vi, bão hòa cho các đầu vào âm lớn và giảm bias shift. Chế độ bão hòa âm của ELU thêm một số tính mạnh mẽ đối với nhiễu so với Leaky ReLU và Parametric ReLU.

--- TRANG 7 ---
ELU được mở rộng thành Scaled ELU (SELU) [52] bằng cách sử dụng một siêu tham số scaling để làm cho độ dốc lớn hơn một cho đầu vào dương. SELU có thể được định nghĩa là:

SELU (x) = { λx; nếu x > 0
           { λα*(e^x - 1); nếu x ≤ 0  (35)

có phạm vi đầu ra trong [-λα; ∞) trong đó λ là một siêu tham số. Về cơ bản, SELU tạo ra tự chuẩn hóa để tự động hội tụ về zero mean và unit variance. Parametric ELU (PELU) [54] thay đổi điểm bão hòa và exponential decay và cũng điều chỉnh độ dốc của hàm tuyến tính cho các đầu vào dương để có tính khả vi. AF PELU có thể được viết là:

PELU (x) = { (a/b)*x; nếu x ≥ 0
           { a*(e^(x/b) - 1); nếu x < 0  (36)

có phạm vi đầu ra [-a; ∞), trong đó a và b là các tham số có thể huấn luyện. Parametric ELU cũng được khám phá trong Continuously differentiable ELU (CELU) [53] cho các đầu vào âm. CELU được cho như sau:

CELU (x) = { x; nếu x ≥ 0
           { α*(e^(x/α) - 1); nếu x < 0  (37)

có phạm vi đầu ra trong [-α; ∞) trong đó α là một tham số có thể học được. PELU cũng được mở rộng thành multiple PELU (MPELU) [55] bằng cách sử dụng hai tham số có thể học được để biểu diễn MPELU như rectified, exponential hoặc kết hợp. MPELU có thể được biểu diễn là:

MPELU (x) = { αx; nếu x > 0
            { βc*(e^(cx) - 1); nếu x ≤ 0  (38)

có phạm vi đầu ra trong [-βc; ∞), trong đó β, c và α là các tham số có thể huấn luyện.

Một AF soft exponential nội suy giữa các hàm exponential, tuyến tính và logarithmic sử dụng tham số có thể huấn luyện [70]. Shifted ELU (ShELU) AF cũng được khám phá như một hàm tối ưu cục bộ [71]. Parametric Rectified Exponential Unit (PREU) [57] được thiết kế như sau:

PREU (x) = { αx; nếu x > 0
           { α*β*x*e^x; nếu x ≤ 0  (39)

có phạm vi đầu ra trong [-∞; ∞), trong đó α và β là các tham số có thể huấn luyện. PREU sử dụng thông tin âm gần zero một cách hiệu quả. Hiệu quả của ELU được cải thiện trong Fast ELU (FELU) AF [56] với sự trợ giúp của các bit dịch chuyển đơn giản và các phép toán đại số số nguyên. FELU được định nghĩa là:

FELU (x) = { x; nếu x > 0
           { α*(e^(x/ln(2)) - 1); nếu x ≤ 0  (40)

có phạm vi đầu ra trong [-α; ∞) với α là một tham số có thể học được. Gần đây, các tính chất của ELU và ReLU đã được sử dụng để thiết kế Elastic ELU (EELU) AF [58]. EELU được định nghĩa là:

EELU (x) = { kx; nếu x > 0
           { α*(e^x - 1); nếu x ≤ 0  (41)

có phạm vi đầu ra trong [-α; ∞) trong đó α và k là các tham số có thể huấn luyện. EELU bảo tồn một gradient nhỏ khác không cho đầu vào âm và thể hiện độ dốc đàn hồi cho đầu vào dương. Parametric Deformable ELU (PDELU) AF cố gắng dịch chuyển giá trị trung bình của đầu ra gần hơn với zero bằng cách sử dụng hình dạng bản đồ linh hoạt [59]. PDELU được định nghĩa là:

PDELU (x) = { x; nếu x > 0
            { α*([1 + (1 - t)*x]^(1/(1-t)) - 1); nếu x ≤ 0  (42)

có phạm vi đầu ra trong [-∞; ∞) trong đó α là một tham số có thể học được. ReLU-Memristor-like AF (RMAF) [72] sử dụng hai siêu tham số để có hình dạng giống ReLU cho đầu vào dương và để đưa ra tầm quan trọng hơn cho các giá trị âm gần zero. Exponential Linear Sigmoid SquasHing (ELiSH) được định nghĩa trong [73] như sau:

ELiSH (x) = { x/(1 + e^(-x)); nếu x ≥ 0
            { (e^x - 1)/(1 + e^(-x)); nếu x < 0  (43)

Hơn nữa, nó cũng được mở rộng thành HardELiSH là phép nhân của HardSigmoid và Linear trong phần dương và HardSigmoid và ELU trong phần âm. Ở đây, HardSigmoid được định nghĩa là:

HardELiSH (x) = max(0; min(1; (x + 1)/2))  (44)

Các AF dựa trên ELU khai thác các đầu vào âm mà không thỏa hiệp với tính phi tuyến. Một số biến thể ELU cũng sửa đổi hàm cho đầu vào dương để làm cho nó có giới hạn.

6. Hàm Kích hoạt Học tập/Thích ứng

Hầu hết các AF đã đề cập ở trên không thích ứng và có thể không thể điều chỉnh dựa trên độ phức tạp của tập dữ liệu. Vấn đề này được giải quyết bằng cách sử dụng các AF học tập/thích ứng như được tóm tắt trong Bảng 5. Một số AF đã đề cập trước đó cũng thích ứng, chẳng hạn như PReLU [57], SReLU [65], PTELU [38], MTLU [46], PELU [54], MPELU [55], PREU [57], EELU [58], PDELU [59], SRS [26], v.v.

Adaptive Piecewise Linear (APL) được định nghĩa như một tổng của các hàm hình bản lề [28]. Nó được cho như sau:

APL(x) = max(0; x) + Σ(s=1 to S) a_s * max(0; b_s - x)  (45)

trong đó a và b là các tham số có thể huấn luyện và S là một siêu tham số đại diện cho số lượng bản lề. Phạm vi đầu ra của APL là [0; ∞). Do các tham số có thể huấn luyện, các nơ-ron khác nhau có thể học các AF khác nhau.

--- TRANG 8 ---
Bảng 5: Tóm tắt các hàm kích hoạt thích ứng và dựa trên học tập.

Tên | Tham số | Đơn điệu | Mượt | Có giới hạn
Adaptive Piecewise Linear Unit (APL), 2015 [28] | Có | Không | Không | Không
Spline AF (SAF), 2016 [74] | Có | Có | Có | Không
Bi-Modal Derivative Adaptive Activation (BDAA), 2017 [75] | Có | Có | Có | Có
Adaptive AF (AAF), 2018 [76] | Có | Có | Không | Không
Swish, 2018 [29] | Có | Không | Có | Không
ESwish, 2018 [77] | Có | Không | Có | Không
Trainable AF (TAF), 2018 [78] | Có | Không | Có | Không
Self-Learnable AF (SLAF), 2019 [79] | Có | Không | Có | Không
Mexican ReLU (MeLU), 2019 [80] | Có | Không | Không | Không

Ramachandran et al. [29] đã thực hiện một tìm kiếm tự động, dẫn đến AF Swish. Nó được định nghĩa là:

Swish (x) = x * Sigmoid (βx)  (46)

trong đó β là một tham số có thể học được. Phạm vi đầu ra của Swish là (-∞; ∞). Dựa trên giá trị học được của β, hình dạng của AF Swish được điều chỉnh giữa các hàm tuyến tính và ReLU. Các giá trị β nhỏ hơn và cao hơn dẫn về phía các hàm tuyến tính và ReLU, tương ứng. Do đó, nó có thể kiểm soát lượng phi tuyến dựa trên độ phức tạp của tập dữ liệu và mạng. Swish cũng được mở rộng thành E-Swish bằng cách nhân Swish với một tham số có thể học được để kiểm soát độ dốc theo hướng dương [77]. E-Swish được định nghĩa là:

E-Swish (x) = β * x * Sigmoid (x)  (47)

có phạm vi đầu ra trong (-∞; ∞) và β là tham số có thể huấn luyện. Flatten-T Swish xem xét hàm zero cho đầu vào âm tương tự như ReLU [81]. Adaptive Richard's Curve weighted Activation (ARiA) cũng được thúc đẩy từ Swish và thay thế hàm sigmoidal bằng Richard's Curve [82]. AF ARiA sử dụng năm siêu tham số để kiểm soát hình dạng của phi tuyến.

Các AF cơ bản được kết hợp với các trọng số có thể học được trong các AF thích ứng [76]. Adaptive AF (AAF) được thiết kế trên PReLU [35] và PELU [54] được cho như sau:

AAF (x) = σ(wx) * PReLU (x) + (1 - σ(wx)) * PELU (x)  (48)

có phạm vi đầu ra trong [0; ∞], trong đó σ là hàm sigmoidal và w là một tham số có thể học được. Trong thực tế, AAF tốn kém vì có nhiều AF tham gia. Trong [83], AF cho mỗi nơ-ron được chọn từ một thư viện AF. Trong [84], các kết hợp khác nhau của hàm đồng nhất, ReLU, và Tanh được học tự động. Trong một nỗ lực khác, Adaptive Blending Unit (ABU) được định nghĩa để cho phép các mạng học các AF ưa thích của chúng [85]. ABU kết hợp một tập hợp các AF với các trọng số có thể huấn luyện. Hàm Lookup Table Unit (LuTU) [86] sử dụng một mặt nạ cosine một chu kỳ dựa trên làm mượt và nội suy tuyến tính sử dụng một tập hợp các điểm neo. Activation ensembles được sử dụng tại mỗi lớp trong [87] với sự đóng góp của mỗi AF được kiểm soát bởi các trọng số có thể huấn luyện. Tương tự, Self-Learnable AF (SLAF) tính toán tổng của các hàm khác nhau trong một ensemble với các hệ số đã học [79]. SLAF có thể được biểu diễn là:

SLAF (x) = Σ(i=0 to N-1) a_i * x^i  (49)

trong phạm vi đầu ra của (-∞; ∞), trong đó a_i là tham số có thể huấn luyện. Mexican ReLU (MeLU) AF được đề xuất trong [80] bằng cách sử dụng một hàm kiểu "Mexican hat" và được cho như sau:

MeLU (x) = PReLU (x) + Σ(j=1 to k) c_j * max(|x - a_j| - ε_j; 0)  (50)

trong phạm vi đầu ra của (-∞; ∞), trong đó c_j là tham số có thể huấn luyện và ε_j & a_j là các số thực.

Một nội suy spline bậc ba cũng được sử dụng để học AF từ dữ liệu [74] được cho như sau:

SAF (x) = ψ(s; q)  (51)

có phạm vi đầu ra trong (-∞; ∞) trong đó ψ(·) được tham số hóa bởi một vector q có bản chất bậc ba. Fourier series basis expansion được sử dụng để học các AF không tham số (NPF) [88]. Hyperactivations sử dụng một hypernetwork trên đầu của một activation network, được sử dụng để khám phá không gian tìm kiếm AF [89]. Một mạng thần kinh nông được sử dụng trong activation network để tạo ra đầu ra cho mỗi đầu vào, trong khi một mạng thần kinh được sử dụng trong hypernetwork để tạo ra trọng số cho một mạng khác. Hàm bi-modal derivative adaptive activation (BDAA) sử dụng hàm sigmoidal derivative twin maxima [75] bằng cách kiểm soát vị trí của maxima với một tham số thích ứng. BDAA được cho như sau:

BDAA (x) = (1/2) * [1/(1 + e^(-x)) - 1/(1 + e^(-x + a))]  (52)

trong phạm vi đầu ra của [0; 1] trong đó a là một tham số có thể học được. Các tác giả đã khai thác các đạo hàm Bi-modal trên bốn AF. Linear regression được sử dụng trong [78] để huấn luyện AF cho mỗi nơ-ron dẫn đến các AF khác nhau cho các nơ-ron khác nhau. TAF được định nghĩa là:

TAF (x) = √((x - a)² + b²)  (53)

trong phạm vi đầu ra của [b; ∞), trong đó a và b là các tham số có thể huấn luyện. Gần đây, một tham số có thể huấn luyện đã được sử dụng trong các AF không thích ứng khác nhau như Sigmoid, Tanh, và ReLU để làm cho nó thích ứng [90].

Các AF thích ứng và có thể huấn luyện là xu hướng gần đây để điều chỉnh tính phi tuyến dựa trên độ phức tạp dữ liệu và mạng. Tuy nhiên, gánh nặng tối thiểu được tăng lên về mặt số lượng tham số tăng lên. Mặc dù độ phức tạp của các AF có thể điều chỉnh tăng tương đối w.r.t. các AF không thể điều chỉnh, nó là

--- TRANG 9 ---
không đáng kể w.r.t. tất cả các tham số của toàn bộ mạng trong thực tế. Điều tương tự cũng được quan sát thông qua thực nghiệm như được báo cáo trong Bảng 10 về mặt thời gian huấn luyện.

7. Hàm Kích hoạt Tổng hợp

Phần này bao gồm các nỗ lực khác trong AFs như Softplus, Probabilistic, Polynomial, Subnetwork và Kernel.

7.1. Hàm Kích hoạt Softplus

Hàm softplus [91] được đề xuất vào năm 2001 như log(e^x + 1) và chủ yếu được sử dụng trong các ứng dụng thống kê. Sau bước đột phá của deep learning, hàm softmax được sử dụng như AF [92]. Hàm Softmax tạo ra đầu ra tương đương phân phối xác suất categorical. AF dựa trên đơn vị Softplus cũng được sử dụng trong mạng thần kinh sâu [93]. Bản chất mượt mà của Softplus tạo điều kiện thuận lợi cho tính khả vi. AF noisy softplus [94] phù hợp cho spiking neural networks (SNNs). Softplus Linear Unit (SLU) cũng được đề xuất bằng cách xem xét softplus với rectified unit [95]. AF SLU được định nghĩa là:

SLU (x) = { αx; nếu x ≥ 0
          { βlog(e^x + 1) - γ; nếu x < 0  (54)

trong đó α, β, và γ là các tham số có thể huấn luyện với α kiểm soát độ dốc theo hướng dương, β kiểm soát các điểm bão hòa theo hướng âm và γ kiểm soát offset theo hướng âm w.r.t. trục ngang. Rectified Softplus (ReSP) AF giới thiệu rectification cho đầu vào dương trong kích hoạt Softplus [96]. Để làm cho hàm softplus tuân theo zero mean, một dịch chuyển và scaling của các đầu ra được thực hiện trong [97]. Rand Softplus (RSP) AF mô hình hóa tính stochasticity-adaptability của các nơ-ron sinh học như sau:

RSP(x) = (1 - λ)*max(0; x) + λ*log(1 + e^x)  (55)

trong đó λ là một siêu tham số ngẫu nhiên [98]. Nó cải thiện khả năng của mạng đối với nhiễu. Hàm softplus cũng được sử dụng với hàm Tanh trong hàm kích hoạt Mish [99], được cho như sau:

Mish (x) = x * Tanh (Softplus (x))  (56)

Mish là một AF không đơn điệu và mượt mà. Nó gần đây đã được sử dụng bởi mô hình YOLOv4 cho phát hiện đối tượng [100]. Tuy nhiên, độ phức tạp tăng lên trong Mish do nhiều hàm có thể là một hạn chế cho các mạng sâu.

7.2. Hàm Kích hoạt Xác suất

Cho đến nay, các AF ngẫu nhiên chưa được khám phá nhiều do các quá trình lấy mẫu đắt đỏ. Một số AF tồn tại trong danh mục này như Randomized ReLU (RReLU) [50], Elastic ReLU (EReLU) [40], Randomly Translational ReLU (RTReLU) [41] và Gaussian Error Linear Unit (GELU) [101]. GELU [101] xem xét phi tuyến như biến đổi được điều khiển bởi regularization ngẫu nhiên và được định nghĩa là:

GELU (x) = x * P(X ≤ x)  (57)

trong đó P là xác suất. Độ phức tạp của GELU tăng lên do sử dụng bản chất xác suất. GELU cũng được mở rộng thành Symmetrical Gaussian Error Linear Unit (SGELU) [102] để tăng cường khả năng hội tụ hai chiều của nó. Doubly truncated Gaussian distributions [103] là một họ của các phi tuyến có thể tạo ra các AF khác nhau như Sigmoid, Tanh và ReLU bằng cách đặt các điểm cắt ngắn thích hợp. Probabilistic AF (ProbAct) giới thiệu variance thích ứng và có thể huấn luyện trong đầu ra của ReLU [104]. Nó dẫn đến tổng quát hóa của các mô hình. Tuy nhiên, tất cả các nhược điểm khác của ReLU cũng tồn tại với ProbAct.

7.3. Hàm Kích hoạt Đa thức

Smooth Adaptive AF (SAAF) được định nghĩa như hàm đa thức từng phần [105]. Hai hàm lũy thừa đối xứng với phần tuyến tính của ReLU được kết hợp trong [106] để cải thiện hiệu suất của ReLU. AF dựa trên xấp xỉ đa thức từng phần cũng được học từ dữ liệu [107]. Kích hoạt này dẫn đến các mô hình nhẹ phù hợp cho FPGAs và microcontrollers. AF cũng được coi như phân phối tích lũy