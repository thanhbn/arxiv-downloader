# 2212.06145.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/activation/2212.06145.pdf
# File size: 1478037 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AP: Selective Activation for De-sparsifying Pruned Neural Networks
Shiyu Liu, Rohan Ghosh, Dylan Tan, Mehul Motani
Department of Electrical and Computer Engineering
School of Engineering, National University of Singapore
shiyu liu@u.nus.edu, rghosh92@gmail.com, eledtht@nus.edu.sg, motani@nus.edu.sg
Abstract
The rectified linear unit (ReLU) is a highly successful activa-
tion function in neural networks as it allows networks to eas-
ily obtain sparse representations, which reduces overfitting in
overparameterized networks. However, in network pruning,
we find that the sparsity introduced by ReLU, which we quan-
tify by a term called dynamic dead neuron rate (DNR), is not
beneficial for the pruned network. Interestingly, the more the
network is pruned, the smaller the dynamic DNR becomes
during optimization. This motivates us to propose a method
to explicitly reduce the dynamic DNR for the pruned net-
work, i.e., de-sparsify the network. We refer to our method
as Activating-while-Pruning (AP). We note that AP does not
function as a stand-alone method, as it does not evaluate
the importance of weights. Instead, it works in tandem with
existing pruning methods and aims to improve their perfor-
mance by selective activation of nodes to reduce the dynamic
DNR. We conduct extensive experiments using popular net-
works (e.g., ResNet, VGG) via two classical and three state-
of-the-art pruning methods. The experimental results on pub-
lic datasets (e.g., CIFAR-10/100) suggest that AP works well
with existing pruning methods and improves the performance
by 3% - 4%. For larger scale datasets (e.g., ImageNet) and
state-of-the-art networks (e.g., vision transformer), we ob-
serve an improvement of 2% - 3% with AP as opposed to
without. Lastly, we conduct an ablation study to examine the
effectiveness of the components comprising AP.
1 Introduction
The rectified linear unit (ReLU) (Glorot, Bordes, and Bengio
2011), σ(x)=max{x, 0}, is the most widely used activation
function in neural networks (e.g., ResNet (He et al. 2016),
Transformer (Vaswani et al. 2017)). The success of ReLU is
mainly due to fact that existing networks tend to be overpa-
rameterized and ReLU can easily regularize overparameter-
ized networks by introducing sparsity (i.e., post-activation
output is zero) (Glorot, Bordes, and Bengio 2011), leading
to promising results in many computer vision tasks (e.g., im-
age classification (Simonyan and Zisserman 2014; He et al.
2016), object detection (Dai et al. 2021; Joseph et al. 2021)).
In this paper, we study the ReLU’s sparsity constraint in
the context of network pruning (i.e., a method of compres-
sion that removes weights from the network). Specifically,
we question the utility of ReLU’s sparsity constraint, whenthe network is no longer overparameterized during iterative
pruning. In the following, we summarize the workflow of
our study together with our contributions.
1.Motivation and Theoretical Study. In Section 3.1, we
introduce a term called dynamic Dead Neuron Rate
(DNR), which quantifies the sparsity introduced by
ReLU neurons that are not completely pruned during it-
erative pruning. Through rigorous experiments on pop-
ular networks (e.g., ResNet (He et al. 2016)), we find
that the more the network is pruned, the smaller the dy-
namic DNR becomes during optimization. This suggests
that the sparsity introduced by ReLU is not beneficial
for pruned networks. Further theoretical investigations
also reveal the importance of reducing dynamic DNR
for pruned networks from an information bottleneck (IB)
(Tishby and Zaslavsky 2015) perspective (see Sec. 3.2).
2.A Method for De-sparsifying Pruned Networks. In
Section 3.3, we propose a method called Activating-
while-Pruning (AP) which aims to explicitly reduce dy-
namic DNR. We note that AP does not function as a
stand-alone method, as it does not evaluate the impor-
tance of weights. Instead, it works in tandem with exist-
ing pruning methods and aims to improve their perfor-
mance by reducing dynamic DNR. The proposed AP has
two variants: (i) AP-Lite which slightly improves the per-
formance of existing methods, but without increasing the
algorithm complexity, and (ii) AP-Pro which introduces
an addition retraining step to the existing methods in ev-
ery pruning cycle, but significantly improves the perfor-
mance of existing methods.
3.Experiments. In Section 4, we conduct experiments on
CIFAR-10/100 (Krizhevsky et al. 2009) with several
popular networks (e.g., ResNet, VGG) using two clas-
sical and three state-of-the-art (SOTA) pruning methods.
The results demonstrate that AP works well with existing
pruning methods and improve their performance by 3%
- 4%. For the larger scale dataset (e.g., ImageNet (Deng
et al. 2009)) and SOTA networks (e.g., vision transformer
(Dosovitskiy et al. 2020)), we observe an improvement of
2% - 3% with AP as opposed to without.
4.Ablation Study. In Section 4.3, we carry out an ablation
study to investigate and demonstrate the effectiveness of
several key components that make up the proposed AP.arXiv:2212.06145v1  [cs.LG]  9 Dec 2022

--- PAGE 2 ---
2 Background
Network pruning is a method used to reduce the size of the
neural network, with its first work (LeCun et al. 1998) dating
back to 1990. In terms of the pruning style, all existing meth-
ods can be divided into two classes: (i) One-Shot Pruning
and (ii) Iterative Pruning . Assuming that we plan to prune
Q%of the parameters of a trained network, a typical prun-
ing cycle consists of three basic steps:
1. Prune η%of existing parameters based on given metrics.
2. Freeze pruned weights as zero.
3. Retrain the pruned network to recover the performance.
In One-Shot Pruning, ηis set to Qand the parameters are
pruned in one pruning cycle. While for Iterative Pruning,
a much smaller portion of parameters (i.e., η << Q ) are
pruned per pruning cycle. The pruning process is repeated
multiple times until Q%of parameters are pruned. As for
performance, Iterative Pruning often results in better per-
formance compared to One-Shot Pruning (Han et al. 2015;
Frankle and Carbin 2019; Li et al. 2017). So far, existing
works aim to improve the pruning performance by explor-
ing either new pruning metrics or new retraining methods.
Pruning Metrics. Weight magnitude is the most popu-
lar approximation metric used to determine less useful con-
nections; the intuition being that smaller magnitude weights
have a smaller effect in the output, and hence are less likely
to have an impact on the model outcome if pruned (He
et al. 2020; Li et al. 2020a,b). Many works have investi-
gated the use of weight magnitude as the pruning metric, i.e.
(Han et al. 2015; Frankle and Carbin 2019). More recently,
(Lee et al. 2020) introduced layer-adaptive magnitude-based
pruning (LAMP) and attempts to prune weights based on a
scaled version of the magnitude. (Park et al. 2020) proposed
a method called Lookahead Pruning (LAP), which evaluates
the importance of weights based on the impact of pruning on
neighbor layers. Another popular metric used for pruning is
via the gradient; the intuition being that weights with smaller
gradients are less impactful in optimizing the loss function.
Examples are (LeCun et al. 1998; Theis et al. 2018), where
(LeCun et al. 1998) proposed using the second derivative of
the loss function with respect to the parameters (i.e., the Hes-
sian Matrix) as a pruning metric and (Theis et al. 2018) used
Fisher information to approximate the Hessian Matrix. A re-
cent work (Blalock et al. 2020) reviewed numerous pruning
methods and suggested two classical pruning methods for
performance evaluation:
1.Global Magnitude : Pruning weights with the lowest ab-
solute value anywhere in the network.
2.Global Gradient : Pruning weights with the lowest abso-
lute value of (weight ×gradient) anywhere in the network.
Retraining Methods. Another factor that significantly
affects the pruning performance is the retraining method.
According to (Han et al. 2015), Han et al. trained the un-
pruned network with a learning rate schedule and retrained
the pruned network using a constant learning rate (i.e., of-
ten the final learning rate of the learning rate schedule). A
recent work (Renda, Frankle, and Carbin 2019) proposed
learning rate rewinding which used the same learning rate
schedule to retrain the pruned network, leading to a betterpruning performance. More recently, (Liu, Tan, and Motani
2021) attempted to optimize the choice of learning rate (LR)
during retraining and proposed a LR schedule called S-Cyc.
They showed that S-Cyc could work well with various prun-
ing methods, further improving the existing performance.
Most notably, (Frankle and Carbin 2019) found that reset-
ting the unpruned weights to their original values (known
asweight rewinding ) after each pruning cycle could lead
to even higher performance than the original model. Some
follow-on works (Zhou et al. 2019; Renda, Frankle, and
Carbin 2019; Malach et al. 2020) investigated this phe-
nomenon more precisely and applied this method in other
fields (e.g., transfer learning (Mehta 2019), reinforcement
learning and natural language processing (Yu et al. 2020)).
Other Works. In addition to works mentioned above,
several other works also share some deeper insights on net-
work pruning (Liu et al. 2019; Zhu and Gupta 2018; Liu,
Simonyan, and Yang 2019; Wang et al. 2020). For example,
(Liu, Simonyan, and Yang 2019) demonstrated that train-
ing from scratch on the right sparse architecture yields bet-
ter results than pruning from pre-trained models. Similarly,
(Wang et al. 2020) suggested that the fully-trained network
could reduce the search space for the pruned structure. More
recently, (Luo and Wu 2020) addressed the issue of pruning
residual connections with limited data and (Ye et al. 2020)
theoretically proved the existence of small subnetworks with
lower loss than the unpruned network.
3 Activating-while-Pruning
In Section 3.1, we first conduct experiments to evaluate the
DNR during iterative pruning. Next, in Section 3.2, we link
the experimental results to theoretical studies and motivate
Activating-while-Pruning (AP). In Section 3.3, we introduce
the idea of AP and present its algorithm. Lastly, in Section
3.4, we illustrate how AP can improve on the performance
of existing pruning methods.
3.1 Experiments on DNR
We study the state of the ReLU function during iterative
pruning and introduce a term called Dead Neuron Rate
(DNR), which is the percentage of dead ReLU neurons (i.e.,
a neuron with a post-ReLU output of zero) in the network
averaged over all training samples when the network con-
verges. Mathematically, the DNR can be written as
DNR =1
nPn
i=1# of dead ReLU neurons
all neurons in the unpruned network,(1)
where nis the number of training samples. We classify a
dead neuron as either dynamically dead or statically dead.
The dynamically dead neuron is a dead neuron in which
not all of the weights have been pruned. Hence, it is not
likely to be permanently dead and its state depends on its
input. As an example, a neuron can be dead for a sample X,
but it could be active (i.e., post-ReLU output >0) for a sam-
ple Y . The DNR contributed by dynamically dead neurons is
referred to as dynamic DNR . The statically dead neuron
is a dead neuron in which all associated weights have been
pruned. The DNR contributed by statically dead neurons is
referred to as static DNR .

--- PAGE 3 ---
Figure 1: Dynamic and static Dead Neuron Rate (DNR) when iterative pruning ResNet-20 on CIFAR-10 using Global Magni-
tude. Left: dynamic and static DNR when the network converges; Right: dynamic DNR during optimization.
DNR is a term that we introduce to quantify the sparsity
introduced by ReLU. Many similar sparsity metrics have
been proposed in the literature (Hurley and Rickard 2009).
As an example, the Gini Index (Goswami, Murthy, and Das
2016) computed from Lorenz curve (i.e., plot the cumula-
tive percentages of total quantities) can be used to evaluate
the sparsity of network graphs. Another popular metric will
be Hoyer measure (Hoyer 2004) which is the ratio between
L1 and L2 norms, can also be used to evaluate the sparsity of
networks. The closest metric to DNR is parameter sparsity
(Goodfellow, Bengio, and Courville 2016) which computes
the percentage of zero-magnitude parameters among all pa-
rameters. We note that both parameter sparsity and DNR will
contribute to sparse representations, and in this paper, we use
DNR to quantify the sparsity introduced by ReLU.
Experiment Setup and Observations. Given the defini-
tion of DNR, static and dynamic DNR, we conduct pruning
experiments using ResNet-20 on the CIFAR-10 dataset with
the aim of examining the benefit (or lack thereof) of ReLU’s
sparsity for pruned networks. We iteratively prune ResNet-
20 with a pruning rate of 20 (i.e., 20% of existing weights
are pruned) using the Global Magnitude (i.e., prune weights
with the smallest magnitude anywhere in the network). We
refer to the standard implementation reported in (Renda,
Frankle, and Carbin 2019; Frankle and Carbin 2019) (i.e.,
SGD optimizer (Ruder 2016), 100 training epochs and batch
size of 128, learning rate warmup to 0.03 and drop by a
factor of 10 at 55 and 70 epochs) and compute the static
DNR and dynamic DNR while the network is iteratively
pruned. The experimental results are shown in Fig. 1, where
we make two observations.
1. As shown in Fig. 1 (left), the value of DNR (i.e., sum of
static and dynamic DNR) increases as the network is it-
eratively pruned. As expected, static DNR grows as more
weights are pruned during iterative pruning.
2. Surprisingly, dynamic DNR tends to decrease as the net-
work is iteratively pruned (see Fig. 1 (left)), suggesting
that pruned networks do not favor the sparsity of ReLU.
In Fig. 1 (right), we show that for pruned networks with
different λ(i.e., percent of remaining weights), they have
similar dynamic DNR before training, but the pruned net-
work with smaller λtends to have a smaller dynamic
DNR during and after optimization.Result Analysis. One possible reason for the decrease in
dynamic DNR could be due to the fact that once the neu-
ron is dead, its gradient becomes zero, meaning that it stops
learning and degrades the learning ability of the network (Lu
et al. 2019; Arnekvist et al. 2020). This could be beneficial
as existing networks tend to be overparameterized and dy-
namic DNR may help to reduce the occurrence of overfit-
ting. However, for pruned networks whose learning ability
are heavily degraded, the dynamic DNR could be harmful
as a dead ReLU always outputs the same value (zero as it
happens) for any given non-positive input, meaning that it
takes no role in discriminating between inputs. Therefore,
during retraining, the pruned network attempts to restore its
performance by reducing its dynamic DNR so that the ex-
tracted information can be passed to the subsequent layers.
Similar performance trends can be observed using VGG-19
with Global Gradient (see Fig. 3 in the Appendix ). Next, we
present a theoretical study of DNR and show its relevance
to the network’s ability to discriminate.
3.2 Relevance to IB and Complexity
Here, we present a theoretical result and subsequent insights
that highlight the relevance of the dynamic DNR of a cer-
tain layer of the pruned network, to the Information Bot-
tleneck (IB) Principle proposed in (Tishby and Zaslavsky
2015). In the IB setting, the computational flow is usually
denoted by the graph X− →T− →Y, where Xrepresents the
input, Trepresents the extracted representation, and Yrep-
resents the network’s output. The IB principle states that the
goal of training networks must be to minimize the mutual
information (Cover and Thomas 2006) between XandT
(denoted as I(X;T)) while keeping I(Y;T)large. Overly
compressed features (low I(X;T)) will not retain enough
information to predict the labels, whereas under-compressed
features (high I(X;T)) imply that more label-irrelevant in-
formation is retained in Twhich can adversely affect gener-
alization. Next, we provide a few definitions.
Definition 1. Layer-Specific dynamic DNR ( DDNR(T)):
We are given a dataset S={X1, ..., X m}, where Xi∼P∀i
andPis the data generating distribution. We denote the dy-
namic DNR of only the neurons at a certain layer within the
network, represented by the vector T, byDDNR(T).DDNR(T)
is computed over the entire distribution of input in P.

--- PAGE 4 ---
Definition 2. Layer-Specific static DNR ( SDNR(T)):It is
defined in the same manner as DDNR(T), instead of comput-
ing the dynamic DNR, we compute the static DNR of T.
With this, we now outline our first theoretical result
which highlights the relevance of DDNR(T)andSDNR(T)to
I(X;T), as follows.
Theorem 1. We are given the computational flow X− →
T− →Y, where Trepresents the features at some arbi-
trary depth within a network, represented with finite preci-
sion (e.g. float32 or float64). We only consider the subset of
network configurations for which (a) the activations in Tare
less than a threshold τand (b) the zero-activation probability
of each neuron in Tis upper bounded by some pS<1. Let
dim(T)represent the dimensionality of T, i.e., the number
of neurons at that depth. We then have,
(2)I(X;T)≤C×dim(T)×
1−SDNR(T)
−DDNR(T)
1−1
Clog1−SDNR(T)
DDNR(T)
,
for a finite constant Cthat only depends on the network
architecture, τandpS.
The following corollary addresses the dependencies of
Theorem 1. The proof of Theorem 1 and Corollary 1 are
provided in the Appendix.
Corollary 1. The upper bound for I(X;T)in Theorem 1
decreases in response to increase of DDNR(T)andSDNR(T).
Remark 1. (Relevance to Complexity) We see that
(Shamir, Sabato, and Tishby 2010) notes how the metric
I(X;T)represents the effective complexity of the network.
As Theorem 3 in (Shamir, Sabato, and Tishby 2010) shows,
I(X;T)captures the dependencies between XandTand
directly correlates with the network’s function fitting ability.
Coupled with the observations from Theorem 1 and Corol-
lary 1, for a fixed pruned network configuration (i.e., fixed
SDNR(T)), greater DDNR(T)will likely reduce the effective
complexity of the network, undermining the learning ability
of the neural network.
Remark 2. (Motivation for AP) Theorem 1 also shows
that a pruned network, which possesses large SDNR(T), leads
to a higher risk of over-compression of information (low
I(X;T)). To address this issue, we can reduce the dynamic
DNR (from Corollary 1) so that the upper bound of I(X;T)
can be increased, mitigating the issue of over-compression
for a pruned network. This agrees with our initial motiva-
tion that the sparsity introduced by ReLU is not beneficial
for the pruned network and reducing dynamic DNR helps to
improve the learning ability of the pruned network.
3.3 Algorithm of Activating-while-Pruning
The experimental and theoretical results above suggest that,
in order to better preserve the learning ability of pruned net-
works, a smaller dynamic DNR is preferred. This motivates
us to propose Activating-while-Pruning (AP) which aims to
explicitly reduce dynamic DNR.
We note that the proposed AP does not work alone, as
it does not evaluate the importance of weights. Instead, itserves as a booster to existing pruning methods and help
to improve their pruning performance by reducing dynamic
DNR (see Fig. 2). Assume that the pruning method X re-
moves p% of weights in every pruning cycle (see the upper
part in Fig. 2). After using AP, the overall pruning rate re-
mains unchanged as p%, but (p−q)% of weights are pruned
according to the pruning method X with the aim of prun-
ing less important weights, while q% of weights are pruned
according to AP (see the lower part in Fig. 2) with the aim
of reducing dynamic DNR. Consider a network f(θ)with
ReLU activation function. Two key steps to reducing dy-
namic DNR are summarized as follows.
(1) Locate Dead ReLU Neurons. Consider a neuron in
the hidden layer with ReLU activation function, taking nin-
puts{X1W1, ..., X nWn|Xi∈Ris the input and Wi∈R
is the associated weight }. Let jbe the pre-activated output
of the neuron (i.e., j=Pn
i=1XiWi) andJbe the post-
activated output of the neuron ( J=ReLU (j)). LetLbe
the loss function and assume the neuron is dead ( J= 0),
then the gradient of its associated weights (e.g., W1) with re-
spected to the loss function will be∂L
∂W1=∂L
∂J·∂J
∂j·∂j
∂W1= 0
as∂J
∂j= 0 . If a neuron is often dead during training,
the weight movement of its associated weights is likely
to be smaller than other neurons. Therefore, we compute
the difference between weights at initialization ( θ0) and the
weights when the network convergences ( θ∗), i.e.,|θ∗−θ0|
and use it as a heuristic to locate dead ReLU neurons.
(2) Activate Dead ReLU Neurons. Assume we have
located a dead neuron in the hidden layer with ninputs
{X1W1, ..., X nWn|Xi∈Ris the input and Wi∈Ris
the associated weight }. We note that Xiis non-negative as
Xiis usually the post-activated output from the previous
layer (i.e., the output of ReLU is non-negative). Therefore, a
straightforward way to activate the dead neuron is to prune
the weights with the negative value. By pruning such nega-
tive weights, we can increase the value of the pre-activation
output, which may turn the the pre-activation output into
positive so as to reduce dynamic DNR.
3.4 How AP Improves Existing Methods
We now summarize how AP can improve existing pruning
methods in Algorithm 2, where the upper part is the algo-
rithm of a standard iterative pruning method called pruning
method X and the lower part is the algorithm of method X
with AP. The proposed AP has two variants: AP-Pro and
AP-Lite . We note that both AP-Pro and AP-Lite contain the
same three steps, summarized as follows.
1.Pruning. Given a network at convergence with a set of
dynamically dead ReLU neurons, N1={n1, n2, n3, ...}.
The pruning step of AP aims to activate these dynami-
cally dead ReLU neurons (i.e., reduce dynamic DNR) so
as to preserve the learning ability of the pruned network
(see the pruning metric of AP in algorithm 1).
2.Weight Rewinding. Resetting unpruned weights to their
values at the initialization. We note that different weight
initializations could lead to different sets of N. In the
previous step, AP aims to reduce dynamic DNR for the
targetN1and weight rewinding attempts to prevent the

--- PAGE 5 ---
Pruning Method X
p% p% p% p%Percent of
Weights Pruned
by Method X
Pruning Cycles1 2 3 4······
Pruning Method X with Activating-while-Pruning (AP)
p-q% q% p-q% q% p-q% q% p-q% q%Percent of
Weights Pruned
by AP
Pruning Cycles1 2 3 4······
Figure 2: Illustration of how AP works in tandem with existing pruning methods (e.g., method X) in each pruning cycle.
Algorithm 1: The Pruning Metric of the Proposed AP
Require: (i) Network fwith unpruned weights θ0at initializa-
tion,f(θ0); (ii) Network fwith unpruned weights θ∗at con-
vergence, f(θ∗); (iii) Pruning Rate of AP, q;
1:Locate Dead Neurons : Sort|θ∗-θ0|in an ascending order.
2:Activate Dead Neurons : In the ascending order of |θ∗-θ0|,
prune first q%of weights with the negative magnitude.
targetN1from changing too much. Since the weights of
ReLU neurons in N1have been pruned by AP, these neu-
rons could become active during retraining. The effect of
weight rewinding is evaluated via an ablation study.
3.Retraining the pruned network to recover performance.
The key difference between AP-Lite and AP-Pro is that
AP-Lite applies these three steps only once at the end of
pruning (i.e., when all pruning cycles ends). It aims to
slightly improve the performance, but does not substantially
increase the algorithm complexity. For AP-Pro, it applies the
three steps above in every pruning cycle, which increases the
algorithm complexity (mainly due to the retraining step), but
aims to significantly improve the performance, which could
be preferred in performance oriented tasks.
4 Performance Evaluation
4.1 Experiment Setup
(1) Experiment Details. To demonstrate that AP can work
well with different pruning methods, we shortlist two clas-
sical and three state-of-the-art pruning methods. The details
for each experiment are summarized as follows.
1. Pruning ResNet-20 on the CIFAR-10 dataset using
Global Magnitude with and without AP.
2. Pruning VGG-19 on the CIFAR-10 dataset using Global
Gradient with and without AP.
3. Pruning DenseNet-40 (Huang et al. 2017) on CIFAR-
100 using Layer-Adaptive Magnitude-based Pruning
(LAMP) (Lee et al. 2020) with and without AP.
4. Pruning MobileNetV2 (Sandler et al. 2018) on the
CIFAR-100 dataset using Lookahead Pruning (LAP)
(Park et al. 2020) with and without AP.
5. Pruning ResNet-50 (He et al. 2016) on the ImageNet
(i.e., ImageNet-1000) using Iterative Magnitude Pruning
(IMP) (Frankle and Carbin 2019) with and without AP.
6. Pruning Vision Transformer (ViT-B-16) on CIFAR-10
using IMP with and without AP.Algorithm 2: The Pruning Method X with and without AP
Require: (i) Network, f(θ); (ii) Pruning Rate of Method X, p;
(iii) Pruning Rate of AP, q; (iv) Pro Flag = {0: AP-Lite, 1: AP-
Pro}; (v) Pruning Cycles, n;
———————— Pruning Method X ————————–
1:fori= 1tondo
2: Randomly initialize unpruned weights, θ←θ0.
3: Train the network to convergence, arriving at parameters θ∗.
4: Prune p% ofθ∗according to the pruning method X.
5:end for
6: Retrain: Retrain the network to recover its performance.
———— Pruning Method X with Proposed AP ————
7:fori= 1tondo
8: Randomly initialize unpruned weights, θ←θ0.
9: Train the network to convergence, arriving at parameters θ∗.
10: Prune ( p-q) % of θ∗according to the pruning method X.
11: ifProFlag then #Execution of AP-Pro
12: (i) Pruning: Prune q% of parameter θ∗according to the
metric of AP (see details in Algorithm 1).
13: (ii) Weight Rewinding: Reset the remaining parameters
to their values in θ0.
14: (iii) Retrain: Retrain the pruned network to recover its
performance.
15: end if
16:end for
17:ifNOT Pro Flag then #Execution of AP-Lite
18: (i) Pruning: Prune q% of the parameters θ∗according to
the metric of AP (see details in Algorithm 1).
19: (ii) Weight Rewinding: Reset the remaining parameters to
their values in θ0.
20: (iii) Retrain: Retrain the pruned network to recover its per-
formance.
21:end if
We train the network using SGD with momentum = 0.9
and a weight decay of 1 e-4 (same as (Renda, Frankle, and
Carbin 2019; Frankle and Carbin 2019)). For the benchmark
pruning method, we prune the network with a pruning rate
p= 20 (i.e., 20% of existing weights are pruned) in 1 prun-
ing cycle. After using AP, the overall pruning rate remains
unchanged as 20%, but 2% of existing weights are pruned
based on AP, while the other 18% of existing weights are
pruned based on the benchmark pruning method to be com-
pared with (see Algorithm 2). We repeat 25 pruning cycles
in 1 run and use the early-stop top-1 test accuracy (i.e., the
corresponding test accuracy when early stopping criteria for
validation error is met) to evaluate the performance. The ex-

--- PAGE 6 ---
Original Top-1 Test Accuracy: 91.7% ( λ= 100%)
λ 32.8% 26.2% 13.4% 5.72%
Glob Mag 90.3±0.4 89.8±0.6 88.2±0.7 81.2±1.1
AP-Lite 90.4±0.7 90.2±0.8 88.7±0.7 82.4±1.4
AP-Pro 90.7±0.6 90.4±0.4 89.3±0.8 84.1±1.1
Table 1: Performance (top-1 test accuracy ±standard de-
viation) of pruning ResNet-20 on CIFAR-10 using Global
Magnitude (Glob Mag) with and without AP.
Original Top-1 Test Accuracy: 92.2% ( λ= 100%)
λ 32.8% 26.2% 13.4% 5.72%
Glob Grad 90.2±0.5 89.8±0.8 89.2±0.8 76.9±1.1
AP-Lite 90.5±0.8 90.3±0.7 89.7±0.9 78.4±1.4
AP-Pro 90.8±0.6 90.7±0.9 90.4±0.8 79.2±1.3
Table 2: Performance (top-1 test accuracy ±standard devi-
ation) of pruning VGG-19 on CIFAR-10 using Global Gra-
dient (Glob Grad) with and without AP.
perimental results averaged over 5 runs and the correspond-
ing standard deviation are summarized in Tables 1 - 6, where
λis the percentage of weights remaining.
(2) Hyper-parameter Selection and Tuning. To ensure
fair comparison against prior results, we utilize standard
implementations (i.e., network hyper-parameters and learn-
ing rate schedules) reported in the literature. Specifically,
the implementations for Tables 1 - 6 are from (Frankle and
Carbin 2019), (Zhao et al. 2019), (Chin et al. 2020), (Renda,
Frankle, and Carbin 2019) and (Dosovitskiy et al. 2020).
The implementation details can be found in Section B.2 of
theAppendix . In addition, we also tune hyper-parameters
using the validation dataset via grid search. Some hyper-
parameters are tuned as follows. (i) The training batch size
is tuned from {64, 128, ...., 1024 }. (ii) The learning rate is
tuned from 1e-3 to 1e-1 via a stepsize of 2e-3. (iii) The num-
ber training epochs is tuned from 80 to 500 with a stepsize
of 20. The validation performance using our tuned param-
eters are close to that of using standard implementations.
Therefore, we use standard implementations reported in the
literature to reproduce benchmark results.
(3) Reproducing Benchmark Results. By using the im-
plementations reported in the literature, we have correctly
reproduced the benchmark results. For example, the bench-
mark results in our Tables 1 - 6 are comparable to Fig.11 and
Fig.9 of (Blalock et al. 2020), Table.4 in (Liu et al. 2019),
Fig.3 in (Chin et al. 2020), Fig. 10 in (Frankle et al. 2020),
Table 5 in (Dosovitskiy et al. 2020), respectively.
(4) Source Code & Devices: We use Tesla V100 devices
for our experiments and the source code (including random
seeds) will be released at the camera-ready stage.
4.2 Performance Comparison
(1) Performance using SOTA and Classical Pruning
Methods. In Tables 3 and 4, we show that AP can work
well with SOTA pruning methods (e.g., LAMP, LAP). In
Table 3, we show the performance of AP using LAMP viaOriginal Top-1 Test Accuracy: 74.6% ( λ= 100%)
λ 32.8% 26.2% 13.4% 5.72%
LAMP 71.5±0.7 69.6±0.8 65.8±0.9 61.2±1.4
AP-Lite 71.9±0.8 70.3±0.7 66.6±0.7 62.2±1.2
AP-Pro 72.2±0.7 71.1±0.7 68.8±0.9 63.5±1.5
Table 3: Performance (top-1 test accuracy ±standard devi-
ation) of pruning DenseNet-40 on CIFAR-100 using Layer-
Adaptive Magnitude Pruning (LAMP) with/without AP.
Original Top-1 Test Accuracy: 73.7% ( λ= 100%)
λ 32.8% 26.2% 13.4% 5.72%
LAP 72.1±0.8 70.5±0.9 67.3±0.8 64.8±1.5
AP-Lite 72.5±0.9 70.9±0.8 68.2±1.2 66.2±1.5
AP-Pro 72.8±0.7 71.4±0.8 69.1±0.8 67.4±1.1
Table 4: Performance (top-1 test accuracy ±standard devia-
tion) of pruning MobileNetV2 on CIFAR-100 using Looka-
head Pruning (LAP) with and without AP.
DenseNet-40 on CIFAR-100. We observe that AP-Lite im-
proves the performance of LAMP by 1.2% at λ= 13.4%
and the improvement increases to 1.6% at λ= 5.7%. Note
that AP-Lite does not increase the algorithm complexity of
existing methods. For AP-Pro, it causes a larger improve-
ment of 4.6% and 3.8% at λ= 13.4% and λ= 5.7%, re-
spectively. Similar performance trends can be observed in
Table 4, where we show the performance of AP using LAP
via MobileNetV2 on CIFAR-100. Furthermore, similar per-
formance improvements can be observed using classical
pruning methods (Global Magnitude/Gradient) via ResNet-
20/VGG-19 on CIFAR-10 as well (see Tables 1 - 2).
(2) Performance on ImageNet. In Table 5, we show
the performance of AP using Iterative Magnitude Pruning
(IMP, i.e., the lottery ticket hypothesis pruning method)
via ResNet-50 on ImageNet (i.e., the ILSVRC version)
which contains over 1.2 million images from 1000 different
classes. We observe that AP-Lite improves the performance
of IMP by 1.5% at λ= 5.7%. For AP-Pro, it improves the
performance of IMP by 2.8% at λ= 5.7%.
(3) Performance on SOTA networks (Vision Trans-
former). Several recent works (Liu et al. 2021; Yuan et al.
2021; Chen, Fan, and Panda 2021) demonstrated that trans-
former based networks tend to provide excellent perfor-
mance in computer vision tasks (e.g., classification). We
now examine the performance of AP using Vision Trans-
former (i.e., ViT-B16 with a resolution of 384). We note
that the ViT-B16 uses Gaussian Error Linear Units (GELU,
GELU(x) = x Φ(x), where Φ(x)is the standard Gaussian
cumulative distribution function) as the activation func-
tion. Similar to ReLU which blocks the negative pre-
activation output, GELU heavily regularizes the negative
pre-activation output by multiplying an extremely small
value of Φ(x), suggesting that AP could be helpful with
pruning GELU based models as well.
We repeat the same experiment setup as above and eval-
uate the performance of AP using ViT-B16 in Table 6. We

--- PAGE 7 ---
Original Top-1 Test Accuracy: 77.0% ( λ= 100%)
λ 32.8% 26.2% 13.4% 5.72%
IMP 76.8±0.2 76.4±0.3 75.2±0.4 71.5±0.4
AP-Lite 77.2±0.3 76.9±0.4 76.1±0.3 72.6±0.5
AP-Pro 77.5±0.4 77.2±0.3 76.8±0.6 73.5±0.4
Table 5: Performance (top-1 validation accuracy ±standard
deviation) of pruning ResNet-50 on ImageNet using Itera-
tive Magnitude Pruning (IMP) with and without AP.
Original Top-1 Test Accuracy: 98.0% ( λ= 100%)
λ 32.8% 26.2% 13.4% 5.72%
IMP 97.3±0.6 96.8±0.7 88.1±0.9 82.1±0.9
AP-Lite 98.0±0.4 97.3±0.7 89.9±0.6 83.6±0.8
AP-Pro 98.2±0.6 97.6±0.5 91.1±0.8 84.8±1.0
Table 6: Performance (top-1 test accuracy ±standard devia-
tion) of pruning Vision Transformer (ViT-B-16) on CIFAR-
10 using IMP with and without AP.
observe that AP-Lite helps to improve the performance of
IMP by 1.8% at λ= 5.7%. For AP-Pro, it improves the
performance of IMP by 3.3% at λ= 5.7%.
4.3 Ablation Study
We now conduct an ablation study to evaluate the effective-
ness of components in AP. Specifically, we remove one com-
ponent at a time in AP and observe the impact on the pruning
performance. We construct several variants of AP as follows.
1.AP-Lite-NO-WR : Using AP-Lite without the weight
rewinding step (i.e., remove step (ii) from AP-Lite in
Algorithm 2). This aims to evaluate the effect of weight
rewinding on the pruning performance.
2.AP-Lite-SOLO : Using only AP-Lite without the bench-
mark pruning method (i.e., in every pruning cycle, prun-
ing weights only based on AP). This aims to evaluate if
the pruning metric of AP can be used to evaluate the im-
portance of weights.
In Table 7, we conduct experiments of Pruning ResNet-20
on the CIFAR-10 dataset using Global Magnitude. Based on
this configuration, we compare the performance of AP-Lite-
NO-WR, AP-Lite-SOLO to AP-Lite so as to demonstrate
the effectiveness of components in AP. We note that, same
as above, we utilize the implementation reported in the lit-
erature. Specifically, the hyper-parameters and the learning
rate schedule are from (Frankle and Carbin 2019).
Effect of Weight Rewinding. In Table 7, we compare
the performance of AP-Lite-NO-WR to AP-Lite while the
key difference is that AP-Lite utilizes weight rewinding (see
Algorithm 2) and AP-Lite-NO-WR does not. We find that
the performance of AP-Lite at λ= 51.2% is 2.4% higher
than AP-Lite-NO-WR. It suggests the crucial role of weight
rewinding in improving the performance. Similar perfor-
mance trends can be observed for other values of λ.
When AP Works Solely. The pruning metric of AP (see
algorithm 1) aims to reduce dynamic DNR by pruning. Weλ 51.2% 40.9% 32.8%
AP-Lite 89.6±0.5 88.9±0.6 88.5±0.8
AP-Lite-SOLO 87.1±0.7 86.3±0.9 85.2±1.1
AP-Lite-NO-WR 87.5±0.5 86.8±0.8 85.9±0.9
Table 7: Ablation Study: Performance Comparison (top-
1 test accuracy ±standard deviation) between AP-Lite
and AP-SOLO, AP-Lite-NO-WR on pruning ResNet-20 on
CIFAR-10 via Global Magnitude.
compare the performance of AP-Lite-SOLO to AP-Lite to
evaluate if the pruning metric of AP can be used solely,
without working with other pruning methods. In Table 7,
we observe that AP-Lite-SOLO performs much worse than
AP-Lite. For example, at λ= 51 .2%, the performance of
AP-Lite-SOLO is 87.1, which is 2.8% lower than AP-Lite.
Similar performance trends can be observed in Table 14 (see
Appendix ), where we prune VGG-19 using CIFAR-10. It
suggests that the pruning metric of AP is not suitable to eval-
uate the importance of weights. The effect of AP’s metric on
reducing dynamic DNR and its pruning rate qon pruning
performance are discussed in the Reflections below.
5 Reflections
The proposed AP aims to improve existing pruning meth-
ods by reducing dynamic DNR. The extensive experiments
on popular/SOTA networks and large-scale datasets demon-
strate that AP works well with various pruning methods, sig-
nificantly improving the performance by 3% - 4%. We now
conclude the paper by presenting some relevant points.
(1) Pruning Rate of AP, q.AP removes q% of remain-
ing parameters in every pruning cycle, so as to reduce dy-
namic DNR. The value of qis usually much smaller than
the pruning rate of the pruning method it works with. Ad-
justing the value of qis a trade-off between pruning less im-
portant weights and reducing dynamic DNR. A large qindi-
cates preferential reduction of dynamic DNR, while a small
qmeans preferential removal of less important weights. We
conduct experiments to evaluate the effect of qon the per-
formance and results suggest that a smaller value of qcould
lead to good performance (see Appendix for more details).
(2) Dynamic DNR with AP. We also examine the effect
of AP in reducing dynamic DNR. The experimental results
on ResNet-20/VGG-19 suggest that AP works as expected
and significantly reduce the dynamic DNR. We refer inter-
ested reader to Appendix for more details.
(3) Reducing Static DNR. The Theorem 1 shows that,
in addition to dynamic DNR, reducing static DNR also can
improve the upper bound of I(X;T). In fact, reducing static
DNR has been incorporated directly or indirectly into the
existing pruning methods. As an example, LAMP (i.e., one
SOTA pruning method used in performance evaluation, see
Table 3) takes the number of unpruned weights of neuron-
s/layers into account and avoids pruning weights from neu-
rons/filters with less number of unpruned weights. This pre-
vents neurons from being statically dead. Differ from exist-
ing methods, AP is the first method targeting the dynamic
DNR. Hence, as a method that works in tandem with exist-

--- PAGE 8 ---
ing pruning methods, AP improves existing pruning meth-
ods by filling in the gap in reducing dynamic DNR, leading
to much better pruning performance.
References
Arnekvist, I.; Carvalho, J. F.; Kragic, D.; and Stork, J. A.
2020. The effect of Target Normalization and Momentum
on Dying ReLU. CoRR , abs/2005.06195.
Blalock, D.; et al. 2020. What is the State of Neural Net-
work Pruning? In Proceedings of the Machine Learning and
Systems (MLSys) .
Chen, C.-F. R.; Fan, Q.; and Panda, R. 2021. Crossvit:
Cross-attention multi-scale vision transformer for image
classification. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 357–366.
Chin, T.-W.; Ding, R.; Zhang, C.; and Marculescu, D. 2020.
Towards efficient model compression via learned global
ranking. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 1518–
1528.
Cover, T. M.; and Thomas, J. A. 2006. Elements of Informa-
tion Theory, 2nd edition . John Wiley & Sons.
Dai, X.; Chen, Y .; Xiao, B.; Chen, D.; Liu, M.; Yuan, L.;
and Zhang, L. 2021. Dynamic head: Unifying object detec-
tion heads with attentions. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
7373–7382.
Deng, J.; et al. 2009. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vi-
sion and pattern recognition , 248–255. IEEE.
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 .
Frankle, J.; and Carbin, M. 2019. The Lottery Ticket Hy-
pothesis: Finding Sparse, Trainable Neural Networks. In
Proceedings of the International Conference on Learning
Representations (ICLR) .
Frankle, J.; Dziugaite, G. K.; Roy, D. M.; and Carbin, M.
2019. Stabilizing the lottery ticket hypothesis. arXiv
preprint arXiv:1903.01611 .
Frankle, J.; et al. 2020. Linear Mode Connectivity and the
Lottery Ticket Hypothesis. In Proceedings of the Inter-
national Conference on Machine Learning (ICML) , 3259–
3269.
Glorot, X.; Bordes, A.; and Bengio, Y . 2011. Deep sparse
rectifier neural networks. In Proceedings of the fourteenth
international conference on artificial intelligence and statis-
tics, 315–323. JMLR Workshop and Conference Proceed-
ings.
Goodfellow, I.; Bengio, Y .; and Courville, A. 2016. Deep
learning . MIT press.
Goswami, S.; Murthy, C. A.; and Das, A. K. 2016.
Sparsity Measure of a Network Graph: Gini Index.
arXiv:1612.07074.Han, S.; et al. 2015. Learning both weights and connections
for efficient neural network. In Proceedings of the Advances
in Neural Information Processing Systems (Neurips) , 1135–
1143.
He, K.; et al. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 770–778.
He, Y .; Ding, Y .; Liu, P.; Zhu, L.; Zhang, H.; and Yang,
Y . 2020. Learning filter pruning criteria for deep convo-
lutional neural networks acceleration. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , 2009–2018.
Hoyer, P. O. 2004. Non-negative matrix factorization with
sparseness constraints. Journal of Machine Learning Re-
search , 5(9).
Huang, G.; et al. 2017. Densely connected convolutional
networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 4700–4708.
Hurley, N. P.; and Rickard, S. T. 2009. Comparing Measures
of Sparsity. arXiv:0811.4706.
Joseph, K.; Khan, S.; Khan, F. S.; and Balasubramanian,
V . N. 2021. Towards open world object detection. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 5830–5840.
Krizhevsky, A.; et al. 2009. Learning multiple layers of fea-
tures from tiny images.
LeCun, Y .; et al. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE , 86(11):
2278–2324.
Lee, J.; Park, S.; Mo, S.; Ahn, S.; and Shin, J. 2020. Layer-
adaptive Sparsity for the Magnitude-based Pruning. In Inter-
national Conference on Learning Representations (ICLR) .
Li, B.; Wu, B.; Su, J.; and Wang, G. 2020a. Eagleeye:
Fast sub-net evaluation for efficient neural network prun-
ing. In European conference on computer vision , 639–654.
Springer.
Li, H.; et al. 2017. Pruning filters for efficient convnets.
InProceedings of the International Conference on Learning
Representations (ICLR) .
Li, Y .; Wu, W.; Liu, Z.; Zhang, C.; Zhang, X.; Yao, H.; and
Yin, B. 2020b. Weight-Dependent Gates for Differentiable
Neural Network Pruning. In European Conference on Com-
puter Vision , 23–37. Springer.
Liu, H.; Simonyan, K.; and Yang, Y . 2019. DARTS: Differ-
entiable architecture search. In Proceedings of the Interna-
tional Conference on Learning Representations (ICLR) .
Liu, S.; Tan, C. M. J.; and Motani, M. 2021. S-Cyc: A Learn-
ing Rate Schedule for Iterative Pruning of ReLU-based Net-
works. CoRR , abs/2110.08764.
Liu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,
S.; and Guo, B. 2021. Swin transformer: Hierarchical vi-
sion transformer using shifted windows. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, 10012–10022.

--- PAGE 9 ---
Liu, Z.; et al. 2019. Rethinking the value of network pruning.
InProceedings of the International Conference on Learning
Representations (ICLR) .
Lu, L.; Shin, Y .; Su, Y .; and Karniadakis, G. E. 2019. Dy-
ing relu and initialization: Theory and numerical examples.
arXiv preprint arXiv:1903.06733 .
Luo, J.-H.; and Wu, J. 2020. Neural Network Pruning with
Residual-Connections and Limited-Data. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 1458–1467.
Malach, E.; et al. 2020. Proving the lottery ticket hypothesis:
Pruning is all you need. In Proceedings of the International
Conference on Machine Learning (ICML) , 6682–6691.
Mehta, R. 2019. Sparse Transfer Learning via Winning Lot-
tery Tickets. In Proceedings of the Advances in Neural In-
formation Processing Systems Workshop on Learning Trans-
ferable Skills .
Park, S.; Lee, J.; Mo, S.; and Shin, J. 2020. Lookahead: A
far-sighted alternative of magnitude-based pruning.
Renda, A.; Frankle, J.; and Carbin, M. 2019. Compar-
ing rewinding and fine-tuning in neural network pruning.
InInternational Conference on Learning Representations
(ICLR) .
Ruder, S. 2016. An overview of gradient descent optimiza-
tion algorithms. arXiv preprint arXiv:1609.04747 .
Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and
Chen, L.-C. 2018. Mobilenetv2: Inverted residuals and lin-
ear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , 4510–4520.
Shamir, O.; Sabato, S.; and Tishby, N. 2010. Learning and
generalization with the information bottleneck. Theoretical
Computer Science , 411(29): 2696–2711.
Simonyan, K.; and Zisserman, A. 2014. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 .Theis, L.; et al. 2018. Faster gaze prediction with dense net-
works and fisher pruning. arXiv preprint arXiv:1801.05787 .
Tishby, N.; and Zaslavsky, N. 2015. Deep Learning and the
Information Bottleneck Principle. arXiv:1503.02406.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. In Advances in neural information
processing systems , 5998–6008.
Wang, Y .; et al. 2020. Pruning from scratch. In Proceed-
ings of the AAAI Conference on Artificial Intelligence , vol-
ume 34, 12273–12280.
Ye, M.; et al. 2020. Good subnetworks provably exist:
Pruning via greedy forward selection. In Proceedings of
the International Conference on Machine Learning (ICML) ,
10820–10830. PMLR.
Yu, H. n.; et al. 2020. Playing the lottery with rewards and
multiple languages: lottery tickets in RL and NLP. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR) .
Yuan, L.; Hou, Q.; Jiang, Z.; Feng, J.; and Yan, S. 2021.
V olo: Vision outlooker for visual recognition. arXiv preprint
arXiv:2106.13112 .
Zhao, C.; et al. 2019. Variational convolutional neural net-
work pruning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2780–2789.
Zhou, H.; et al. 2019. Deconstructing Lottery Tickets: Zeros,
Signs, and the Supermask. In Proceedings of the Advances
in Neural Information Processing Systems (Neurips) .
Zhu, M.; and Gupta, S. 2018. To prune, or not to prune:
exploring the efficacy of pruning for model compression.
InProceedings of the International Conference on Learning
Representations (ICLR) .

--- PAGE 10 ---
A Proofs of Theoretical Results
In this section, we provide the proofs for theoretical results (Theorem 1 and Corollary 1) of the main paper.
A.1 Proof of Theorem 1
Theorem 1. We are given the computational flow X− →T− →Y, where Trepresents the features at some arbitrary depth within
a network, represented with finite precision (e.g. float32 or float64). We only consider the subset of network configurations for
which (a) the activations in Tare less than a threshold τand (b) the zero-activation probability of each neuron in Tis upper
bounded by some pS<1. Letdim(T)represent the dimensionality of T, i.e., the number of neurons at that depth. We then
have,
I(X;T)≤C×dim(T)×
1−SDNR(T)−DDNR(T)
1−1
Clog1−SDNR(T)
DDNR(T)
, (3)
for a finite constant Cthat only depends on the network architecture, τandpS.
Proof. First, note that due to finite precision Tis a discrete variable, and thus I(X;T) =H(T), asT=f(X)is a deterministic
function of X, where fdenotes the function within the network that maps XtoT. Next, let us only consider the nodes in T
which are not statically dead; i.e. they do not form a part of the SDNR(T). Let us denote them as active nodes. Note that there
will be k=dim(T)×(1−SDNR(T))active nodes in this case.
For these knodes, let p1, p2, ..., p kdenote the probability that each node will be zero-valued, when Xis drawn infinitely
over the entire distribution P. Let us also denote D′
DNR(T) =DDNR(T)
1−SDNR(T)as the cardinality adjusted dynamic DNR rate of the
pruned network. Note that E[pi] =D′
DNR(T). Let us represent these knodes by T1, T2, .., T kfor what follows. Note that like T,
eachTiwill be discrete valued. We can thus write
H(T)≤X
H(Ti) (4)
As all activations are less than τ, if the precision of representation is α, we will have a maximum N=τ
αnumber of possible
outcomes for each Ti. Letϕi
0, ϕi
1, ..., ϕi
N−1thus represent the probabilities of Tibeing each possible discrete outcome. We have
thatP
jϕi
j= 1. Note that ϕi
0=pi.
Now, we can write
H(Ti) =pilog1
pi+ (1−pi)X
N−1≥j≥1ϕi
j
(1−pi)log1
ϕi
j(5)
Here let us consider the quantityP
N−1≥j≥1ϕi
j
(1−pi)log1
ϕi
j. Let Cbe the maximum possible value this quantity can take,
across all network weight configurations that obey the constraints provided in the Theorem. Note that Cwill only depend on
the network architecture, and the parameters τandpS. We will now demonstrate that Cis finite, and provide an upper bound
for the same.
Given pi=ϕi
0, we have thatP
j≥1ϕi
j= 1−pi. Thus, under this constant summation constraint, the quantity
P
N−1≥j≥1ϕi
j
(1−pi)log1
ϕi
jwill only be maximized when ϕi
1=ϕi
2=...ϕi
N−1=1−pi
N−1. Thus, we have
C≤X
N−1≥j≥11
N−1logN−1
1−pi(6)
= logN−1
1−pi≤logN−1
1−pS(7)
This shows that Cis finite, and depends on the network architecture, pSandτ(which affects N). Lastly, we have
kX
i=1H(Ti)≤kX
i=1
pilog1
pi+ (1−pi)C
(8)
=kX
i=1pilog1
pi+k×C×(1−D′
DNR(T)) (9)
≤k×D′
DNR(T) log1
D′
DNR(T)+k×C×(1−D′
DNR(T)) (10)
=C×dim(T) (1−SDNR(T))
1−D′
DNR(T)
1−1
Clog1
D′
DNR(T)
, (11)

--- PAGE 11 ---
where the last step follows from the definition of k. Replacing D′
DNR(T) =DDNR(T)
1−SDNR(T)yields the intended result.
A.2 Proof of Corollary 1
Corollary 1. The upper bound for I(X;T)in Theorem 1 decreases in response to increase of both DDNR(T)andSDNR(T).
Proof. It is trivial to see that increasing SDNR(T)can only decrease the upper bound. Let us denote
Z=C×dim(T) (1−SDNR(T))
1−D′
DNR(T)
1−1
Clog1
D′
DNR(T)
. (12)
For simplicity of notation, let β=C×dim(T)×(1−SDNR(T)). For investigating how the upper bound of I(X;T)(denoted
asZ) changes with DDNR(T), we first compute the derivative of Zw.r.tD′
DNR(T)which yields the following expression.
dZ
d(D′
DNR(T))=β
−
1−1
Clog1
D′
DNR(T)
−D′
DNR(T)1
C×D′
DNR(T)
(13)
=β
−1−1
C+1
Clog1
D′
DNR(T)
(14)
FordZ
d(D′
DNR(T))to be less than or equal to 0, we must have
C≥log1
D′
DNR(T)−1. (15)
In what follows, we will show that C≥log1
D′
DNR(T)itself. Please refer to the proof of Theorem 1 for the definitions.
For each node output represented in T1, T2, .., T k, we add a negative bias of −1
α, and flip the sign of all the weights pointing
to each of these nodes. We note that performing this change would yield thatPN−1
j=1ϕi
j=pi. We will then have that
X
N−1≥j≥1ϕi
j
(pi)log1
ϕi
j≥log1
pi, (16)
where the lower bound is achieved by only filling the remaining probability pionto a single bin. Now, as Crepresents the
maximum possible value that the quantity bounded in (16) can take, we naturally have that C≥log1
pias well, for all i. As
E[pi] =D′
DNR(T), it also then follows that C≥log1
D′
DNR(T). This proves our intended result, and yields thatdZ
d(D′
DNR(T))≤0.
Therefore, given that D′
DNR(T) =DDNR(T)
1−SDNR(T), a larger value of both DDNRandSDNRwill lead to a larger value of D′
DNR(T),
decreasing Z(i.e., the upper bound of I(X;Z)) and increasing the risk of over-compression .

--- PAGE 12 ---
B Supplementary Experimental Results
In the Appendix, we show some additional experimental results. Specifically,
1. In Section B.1, we repeat the DNR experiment using VGG-19 on the CIFAR-10 dataset with the Global Gradient pruning
method.
2. In Section B.2, we present the implementation details used in the Section of Performance Evaluation (i.e., Section 4) and
demonstrate the performance of AP for more values of λ.
3. In Section B.3, we show the ablation study results using VGG-19 on CIFAR-10 with the Global Gradient pruning method.
4. In Section B.4, we evaluate the effect of pruning rate, q, on the pruning performance using ResNet-20 and VGG-19 on the
CIFAR-10 dataset.
5. In Section B.5, we evaluate the effect of AP on reducing the dynamic DNR on the pruning performance using ResNet-20
and VGG-19 on the CIFAR-10 dataset.
B.1 The dynamic DNR experiment
In this section, we repeat the DNR experiment in the Section of Activating-while-Pruning (i.e., Section 3) using VGG-19 on
CIFAR-10 with global gradient pruning method. The hyper-parameters and the LR schedule used are from (Frankle and Carbin
2019). As shown in Fig. 3, we observe the performance trend largely mirrors those reported in Fig. 1. The dynamic DNR tends
to decrease as the network is iteratively pruned (shown in Fig. 3 (left)), and during optimization, the network aims to reduce the
dynamic DNR so as to preserve the learning ability of the pruned network.
Figure 3: Dynamic and static Dead Neuron Rate (DNR) when iterative pruning VGG-19 on CIFAR-10 using Global Gradient.
Left: dynamic and static DNR when the network converges; Right: dynamic DNR during optimization.
B.2 Implementation Details and Performance of AP for More Values of λ
In this section, we present the implementation details used in the Section of Performance Evaluation (i.e., Section 4) and
demonstrate the performance of AP for more values of λ.
(1) Implementation Details. We use standard implementations reported in the literature. Specifically, the implementation
for Tables 8 - 9 is from (Frankle and Carbin 2019). The implementation for Table 10 - 13 are from (Zhao et al. 2019), (Chin
et al. 2020), (Renda, Frankle, and Carbin 2019) and (Dosovitskiy et al. 2020), respectively. The implementation details can also
be found on the top row of each table (from Table 8 to Table 13). Furthermore, for the IMP method examined in this work, we
rewind the unpruned weights to their values during training (e.g., epoch 6), in order to obtain a more stable subnetwork (Frankle
et al. 2019). To better work with IMP, the weight rewinding step in the proposed AP also rewinds the unpruned weights to their
values during training (i.e., the same epoch as IMP).
(2) More Performance of AP. In Tables 8- 13, we also show performance of AP for more values of λ. We observe that for
other values of λ, AP-Lite and AP-Pro also help to achieve higher performance. For example, in Table 9, AP-Lite improves the
performance of the Global Gradient (at λ= 8.59%) from 84.5% to 86.1% while AP-Pro improves the performance to 87.0%.

--- PAGE 13 ---
(i) Params: 270K; (ii) Train Steps: 100 Epochs; (iii) Batch size: 128;
(iv) LR Schedule: warmup to 0.03 at 55 epochs, 10X drop at 55, 70 epochs.
Percent of Weights Remaining Global Magnitude AP-Lite AP-Pro
λ= 100.0% 91.7±0.2 91.7±0.2 91.7±0.5
λ= 64.0% 91.5±0.3 91.7±0.2 91.8±0.3
λ= 40.9% 90.8±0.5 91.0±0.6 91.4±0.4
λ= 32.8% 90.3±0.4 90.4±0.7 90.7±0.6
λ= 26.2% 89.8±0.6 90.2±0.8 90.4±0.7
λ= 13.4% 88.2±0.7 88.7±0.7 89.3±0.8
λ= 8.59% 85.9±0.9 86.8±0.9 87.3±0.8
λ= 5.72% 81.2±1.1 82.4±0.8 84.1±1.1
Table 8: Performance (top-1 test accuracy ±standard deviation) of pruning ResNet-20 on CIFAR-10 using Global Magnitude
with and without AP. The hyper-parameters and the LR schedule are from (Frankle and Carbin 2019).
(i) Params: 139M; (ii) Train Steps: 160 epochs; (iii) Batch size: 64;
(iv) LR Schedule: warmup to 0.1 at 15 epochs, 10X drop at 85, 125 epochs.
Percent of Weights Remaining Global Gradient AP-Lite AP-Pro
λ= 100.0% 92.2±0.3 92.2±0.3 92.2±0.3
λ= 64.0% 91.3±0.2 91.5±0.3 91.9±0.3
λ= 40.9% 90.6±0.4 90.8±0.5 91.1±0.7
λ= 32.8% 90.2±0.5 90.5±0.8 90.8±0.6
λ= 26.2% 89.8±0.8 90.3±0.7 90.7±0.9
λ= 13.4% 89.2±0.8 89.7±0.9 90.4±0.8
λ= 8.59% 84.5±0.9 86.1±1.0 87.0±0.7
λ= 5.72% 76.9±1.1 78.4±1.4 79.2±1.3
Table 9: Performance (top-1 test accuracy ±standard deviation) of pruning VGG-19 on CIFAR-10 using Global Gradient with
and without AP. The hyper-parameters and the LR schedule are from (Frankle and Carbin 2019).
(i) Params: 1.1M; (ii) Train Steps: 300 epochs; (iii) Batch size: 256;
(iv) LR Schedule: warmup to 0.1 at 150 epochs, 10X drop at 150, 240 epochs.
Percent of Weights Remaining LAMP AP-Lite AP-Pro
λ= 100.0% 74.6±0.5 74.6±0.5 74.6±0.5
λ= 64.0% 73.4±0.6 73.7±0.5 74.2±0.6
λ= 32.8% 71.5±0.7 71.9±0.8 72.2±0.7
λ= 26.2% 69.6±0.8 70.3±0.7 71.1±0.7
λ= 13.4% 65.8±0.9 66.6±0.7 68.8±0.9
λ= 5.72% 61.2±1.4 62.2±1.2 63.5±1.5
Table 10: Performance (top-1 test accuracy ±standard deviation) of pruning DenseNet-40 on CIFAR-100 using Layer-Adaptive
Magnitude Pruning (LAMP) (Lee et al. 2020) with/without AP. The hyper-parameters and the LR schedule are from (Zhao et al.
2019).

--- PAGE 14 ---
(i) Params: 2.36M; (ii) Train Steps: 200 epochs; (iii) Batch size: 128;
(iv) LR Schedule: warmup to 0.1 at 60 epochs, 10X drop at 60, 120, 160 epochs.
Percent of Weights Remaining LAP AP-Lite AP-Pro
λ= 100.0% 73.7±0.4 73.7±0.4 73.7±0.4
λ= 64.0% 72.5±0.4 72.7±0.3 72.9±0.5
λ= 32.8% 72.1±0.8 72.5±0.9 72.8±0.7
λ= 26.2% 70.5±0.9 70.9±0.8 71.4±0.8
λ= 13.4% 67.3±0.8 68.2±1.2 69.1±0.8
λ= 5.72% 64.8±1.5 66.2±1.5 67.4±1.1
Table 11: Performance (top-1 test accuracy ±standard deviation) of pruning MobileNetV2 on CIFAR-100 using Lookahead
Pruning (LAP) (Park et al. 2020) with/without AP. The hyper-parameters and the LR schedule are from (Chin et al. 2020).
(i) Params: 25.5M; (ii) Train Steps: 90 epochs; (iii) Batch size: 1024;
(iv) LR Schedule: warmup to 0.4 at 5 epochs, 10X drop at 30, 60, 80 epochs.
Percent of Weights Remaining IMP AP-Lite AP-Pro
λ= 100.0% 77.0±0.1 77.0±0.1 77.0±0.1
λ= 64.0% 77.2±0.2 77.5±0.1 77.7±0.1
λ= 32.8% 76.8±0.2 77.2±0.3 77.5±0.4
λ= 26.2% 76.4±0.3 76.9±0.4 77.2±0.3
λ= 13.4% 75.2±0.4 76.1±0.3 76.8±0.6
λ= 8.59% 73.8±0.5 75.2±0.7 75.9±0.5
λ= 5.72% 71.5±0.4 72.6±0.5 73.5±0.4
Table 12: Performance (top-1 test accuracy ±standard deviation) of pruning ResNet-50 on ImageNet using Iterative Magnitude
Pruning (IMP) with and without AP (Frankle and Carbin 2019). The hyper-parameters and the LR schedule are from (Renda,
Frankle, and Carbin 2019).
(i) Params: 86M; (ii) Train Steps: 50 epochs; (iii) Batch size: 1024;
(iv) Optimizer: Adam; (v) LR Schedule: cosine decay from 1e-4.
Percent of Weights Remaining IMP AP-Lite AP-Pro
λ= 100.0% 98.0±0.3 98.0±0.3 98.0±0.3
λ= 64.0% 98.4±0.3 98.5±0.2 98.7±0.3
λ= 32.8% 97.3±0.6 98.0±0.4 98.2±0.6
λ= 26.2% 96.8±0.7 97.3±0.7 97.6±0.5
λ= 13.4% 88.1±0.9 89.9±0.6 91.1±0.8
λ= 8.59% 84.4±0.8 85.5±0.8 87.4±0.7
λ= 5.72% 82.1±0.9 83.6±0.8 84.8±1.0
Table 13: Performance (top-1 test accuracy ±standard deviation) of pruning Vision Transformer (Dosovitskiy et al. 2020) (ViT-
B-16) on CIFAR-10 using IMP with and without AP (Frankle and Carbin 2019). The hyper-parameters and the LR schedule
are from (Dosovitskiy et al. 2020).

--- PAGE 15 ---
B.3 Ablation Study Results using VGG-19
In this section, we repeat the ablation study experiment in the Section of Ablation Study (i.e., Section 4.3) using VGG-19 on
CIFAR-10 with the Global Gradient method. We note that the hyper-parameters and the LR schedule used are from (Frankle
and Carbin 2019).
As show in Table 14, we observe similar performance trends as Table 7. Specifically, the performance of AP-Lite-NO-WR
is much lower as compared to AP-Lite, suggesting the effectiveness of weight rewinding (WR). Furthermore, when AP works
solely, the performance (i.e., AP-Lite-SOLO) tends to become much worse than AP-Lite. This agrees with our argument that
AP does not work solely as it does not evaluate the importance of weights. Instead, the goal of AP is to work in tandem with
existing pruning methods and aims to improve their performance by reducing the dynamic DNR.
λ 64% 51.2% 40.9% 32.8%
AP-Lite 91.7±0.3 90.6 ±0.5 89.8 ±0.9 89.5 ±0.8
AP-Lite-SOLO 90.5±0.7 89.8 ±0.6 88.3 ±1.1 87.2 ±1.3
AP-Lite-NO-WR 90.8±0.4 90.1 ±0.3 88.5 ±0.9 87.7 ±1.0
Table 14: Ablation Study: Performance Comparison (top-1 test accuracy ±standard deviation) between AP-Lite and AP-
SOLO, AP-Lite-NO-WR on pruning VGG-19 using the CIFAR-10 dataset via Global Gradient. The hyper-parameters and the
LR schedule are from (Frankle and Carbin 2019).
B.4 Pruning Rate of AP, q
In this subsection, we repeat the experiments of pruning ResNet-20 on CIFAR-10 using Global Magnitude and AP-Lite. We
note that the overall pruning rate is fixed as 20% and the pruning rate of AP increases from 1% to 5%. Correspondingly, the
pruning rate of Global Magnitude decreases from 19% to 15%. The experimental results are summarized in Table 15. We
observe that as we increase the pruning rate of AP from 2%, the performance tends to decrease. Similar performance trends can
be observed using VGG-19 on CIFAR-10 as well (see Table 16). As an example, in Table 16, the top-1 test accuracy is reduced
from 88.5 to 87.1 for λ= 26.2%when the value of qincreases from 2% to 5%. It suggests that the primary goal of pruning
still should be pruning less important weights.
The theoretical determination of the optimal value of qis clearly worth deeper thought. Alternatively, qcan be thought of as
a hyper-parameter and tuned via the validation dataset and let q= 2 could be a good choice as it provides promising results in
various experiments.
AP Pruning Rate, q 1% 2% 3% 5%
AP-Lite ( λ= 64.0%) 89.8±0.1 90.0±0.2 89.5±0.4 89.2 ±0.6
AP-Lite ( λ= 40.9%) 88.2±0.4 88.9±0.6 88.5±0.7 87.3 ±0.5
AP-Lite ( λ= 26.2%) 87.1±0.5 87.9±0.8 86.7±0.8 86.3 ±0.9
Table 15: Performance (top-1 test accuracy ±standard deviation) of AP-Lite when iterative pruning ResNet-20 on CIFAR-10
with different pruning rate.
AP Pruning Rate, q 1% 2% 3% 5%
AP-Lite ( λ= 64.0%) 91.1±0.2 91.7±0.3 90.2±0.5 89.8 ±0.6
AP-Lite ( λ= 40.9%) 88.7±0.5 89.8±0.9 89.3±1.1 88.0 ±0.9
AP-Lite ( λ= 26.2%) 87.9±0.8 88.5±0.7 88.1±1.3 87.1 ±1.3
Table 16: Performance (top-1 test accuracy ±standard deviation) of AP-Lite when iterative pruning VGG-19 on CIFAR-10
with different pruning rate.

--- PAGE 16 ---
B.5 Effect of AP on Dynamic DNR
In this section, we repeat the same experiments in the Section of Performance Evaluation (i.e., Section 4) and compare the
dynamic DNR with and without using AP-Lite in Tables 18 & 17.
In Table 18, we observe that dynamic DNR is reduced from 9.8% to 9.1% at λ= 80% after applying AP-Lite with a pruning
rate of 2%. As λdecreases, AP-Lite also works well and reduces dynamic DNR from 5.1% to 4.4% at λ= 20.9%. Similar
performance trends can also be observed in Table 17. This suggests that AP works as expected and explains why AP is able to
improve the pruning performance of existing pruning methods.
λ 80% 51.2% 40.9% 32.8% 26.2% 20.9%
Global Magnitude 7.7% 7.4% 7.0% 6.7% 6.3% 6.1%
AP-Lite (2%) 7.3% 7.1% 6.8% 6.3% 5.9% 5.5%
Table 17: The dynamic DNR when iteratively pruning ResNet-20 on CIFAR-10 using Global Magnitude and AP-Lite (with a
pruning rate of 2%).
λ 80% 51.2% 40.9% 32.8% 26.2% 20.9%
Global Gradient 9.8% 9.2% 7.6% 7.0% 5.8% 5.1%
AP-Lite (2%) 9.1% 8.6% 7.0% 6.1% 4.9% 4.4%
Table 18: The dynamic DNR when iteratively pruning VGG-19 on CIFAR-10 using Global Gradient and AP-Lite (with a
pruning rate of 2%).
