# Head-to-Tail: Các Mô Hình Ngôn Ngữ Lớn (LLMs) Có Hiểu Biết Đến Đâu?
# Tức Là, Liệu LLMs Có Thể Thay Thế Đồ Thị Tri Thức?

Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, Xin Luna Dong
Meta Reality Labs
{sunkaicn,ethanxu,hwzha,yuei,lunadong}@meta.com

## Tóm tắt
Kể từ sự thịnh vượng gần đây của các Mô Hình Ngôn Ngữ Lớn (LLMs), đã có những cuộc thảo luận xen kẽ về cách giảm thiểu các ảo tưởng từ phản hồi của LLM, cách tăng tính sự thật của LLMs, và liệu Đồ Thị Tri Thức (KGs), lưu trữ tri thức thế giới dưới dạng ký hiệu, có bị thay thế bằng LLMs hay không. Trong bài báo này, chúng tôi cố gắng trả lời những câu hỏi này từ một góc độ mới: Các LLMs có hiểu biết đến đâu?

Để trả lời câu hỏi này, chúng tôi đã xây dựng Head-to-Tail, một bộ đánh giá bao gồm 18K cặp câu hỏi-trả lời (QA) liên quan đến các sự thật về head, torso, và tail theo độ phổ biến. Chúng tôi thiết kế một phương pháp đánh giá tự động và một tập hợp các chỉ số gần đúng với kiến thức mà một LLM tự tin nội hóa.

Thông qua đánh giá toàn diện 16 LLMs công khai, chúng tôi cho thấy rằng các LLMs hiện tại vẫn còn xa mới hoàn hảo về khả năng nắm bắt kiến thức sự thật, đặc biệt đối với các sự thật của các thực thể torso-to-tail.

## 1 Giới thiệu
Các mô hình ngôn ngữ lớn được huấn luyện trước (LLMs), như ChatGPT1, GPT-4 (OpenAI, 2023), và Llama 2 (Touvron et al., 2023b), đã thể hiện khả năng ấn tượng trong việc nội hóa kiến thức và phản hồi các câu hỏi thông thường (Ouyang et al., 2022; OpenAI, 2023). Tuy nhiên, những mô hình này thường thiếu kiến thức về các chi tiết tinh tế, cụ thể cho từng lĩnh vực và dễ bị ảo tưởng (Bang et al., 2023), nhấn mạnh những thách thức đáng kể trong việc tăng tính sự thật của LLMs và giảm thiểu ảo tưởng từ phản hồi LLM.

Ngược lại, sự nổi lên của LLMs đã làm dấy lên các cuộc tranh luận về việc liệu Đồ Thị Tri Thức (KGs), lưu trữ kiến thức sự thật thế giới thực dưới dạng bộ ba (chủ thể, vị từ, đối tượng), có bị thay thế bằng LLMs hay không. Bài báo này cố gắng trả lời những câu hỏi này từ một góc độ mới: Các LLMs có hiểu biết đến đâu?

[Hình 1: Độ chính xác trả lời câu hỏi của GPT-4 giảm theo thứ tự head, torso, và tail entities trên bộ đánh giá Head-to-Tail, và chỉ đạt 31% trung bình.]

Việc tìm câu trả lời cho những câu hỏi này không dễ dàng. Đầu tiên, khó có thể "truy vấn" trực tiếp kiến thức được nhúng trong một LLM—ảo tưởng có thể do thiếu kiến thức nhưng cũng có thể do trục trặc của mô hình sinh ngay cả khi kiến thức đã được tham số hóa trong mô hình. Chúng tôi ước lượng lượng kiến thức trong một LLM bằng độ chính xác của nó trong việc trả lời các câu hỏi có dạng đơn giản, như "cầu thủ bóng rổ Michael Jordan sinh ở đâu?"; ngoài ra, chúng tôi yêu cầu LLM tạo ra câu trả lời ngắn gọn và thừa nhận "không chắc chắn" khi độ tin cậy của nó thấp. Chúng tôi chọn proxy này vì chúng tôi nhận thấy LLMs thường rất giỏi trong việc hiểu các câu hỏi có dạng đơn giản và tạo ra câu trả lời nhất quán khi tái tạo câu trả lời, đặc biệt nếu được yêu cầu ngắn gọn (Phần 3.5).

Thứ hai, không có bộ đánh giá sẵn sàng sử dụng nào thể hiện tốt phân phối sở thích của người dùng (các nhật ký truy vấn cho các LLMs hoặc công cụ tìm kiếm chính không được công khai) hoặc thể hiện tốt phân phối đồng đều của kiến thức thế giới (ngay cả các đồ thị tri thức lớn nhất cũng thừa nhận sự thưa thớt của kiến thức, đặc biệt đối với các sự thật không phổ biến). Để giải quyết thách thức này, chúng tôi xây dựng một bộ đánh giá gồm 18K cặp QA bao phủ nhiều lĩnh vực khác nhau và nhiều mối quan hệ khác nhau trong các lĩnh vực này. Chúng tôi chia các thực thể và mối quan hệ thành head, torso, và tail theo độ phổ biến của chúng (chi tiết trong Phần 2) và lấy mẫu ngẫu nhiên từ mỗi nhóm; do đó, chúng tôi gọi bộ đánh giá của chúng tôi là Head-to-Tail. Bộ đánh giá này giúp chúng tôi đạt được cái nhìn toàn diện về mức độ hiểu biết của LLMs đối với từng nhóm.

Thông qua bộ đánh giá Head-to-Tail và phương pháp thực nghiệm, chúng tôi trả lời ba câu hỏi nghiên cứu (RQs) sau:

RQ1: LLMs có đáng tin cậy đến đâu trong việc trả lời các câu hỏi sự thật? (Phần 3.2)

RQ2: LLMs có hoạt động tốt như nhau trên các sự thật head, torso, và tail không? (Phần 3.3)

RQ3: Các phương pháp thông thường cải thiện LLMs, như tăng kích thước mô hình và điều chỉnh hướng dẫn, có giúp LLMs hiểu biết hơn không? (Phần 3.4)

Như được thể hiện trong Hình 1, phân tích của chúng tôi cho thấy sự suy giảm nhất quán trong hiệu suất của LLMs, theo thứ tự head, torso, và tail entities, xác nhận giả thuyết của chúng tôi rằng LLMs chứa nhiều kiến thức head hơn nơi dữ liệu huấn luyện dồi dào. Đáng ngạc nhiên, ngay cả đối với top-0.5% thực thể phổ biến trong các lĩnh vực phổ biến như Movie, các LLMs được đánh giá, tốt nhất, chỉ cung cấp câu trả lời chính xác cho ~60% câu hỏi trong bộ đánh giá. Các phương pháp thông thường tăng cường LLMs không nhất thiết làm cho chúng hiểu biết hơn, làm nổi bật nhu cầu về các cách tiếp cận hiệu quả hơn để tăng tính sự thật của LLMs.

Những đóng góp chính của chúng tôi như sau:

(i) Chúng tôi giới thiệu Head-to-Tail, bộ đánh giá đầu tiên tập trung vào việc đánh giá toàn diện hiệu quả của LLMs trong việc kết hợp kiến thức sự thật bao gồm các phần head, torso, và tail của đồ thị tri thức (Phần 2.1). Head-to-Tail sẽ có sẵn tại https://github.com/facebookresearch/head-to-tail.

(ii) Chúng tôi trình bày một phương pháp đánh giá kèm theo các chỉ số được thiết kế để đánh giá tính sự thật của LLMs. Các chỉ số của chúng tôi cho phép chúng tôi phân biệt ảo tưởng và câu trả lời thiếu, và phương pháp đánh giá của chúng tôi, dù hoàn toàn tự động, được chứng minh là đáng tin cậy và mạnh mẽ (Phần 2.2-2.3).

(iii) Chúng tôi tiến hành đánh giá toàn diện và định lượng tính sự thật của 16 LLMs liên quan đến các sự thật head, torso, và tail để trả lời các câu hỏi nghiên cứu (RQ1–RQ3) (Phần 3). Dựa trên những phát hiện này, chúng tôi hình dung tương lai của đồ thị tri thức và phác thảo một bối cảnh nghiên cứu nhằm cải thiện độ tin cậy sự thật tổng thể của LLMs (Phần 4).

## 2 Bộ đánh giá Head-to-Tail
Chúng tôi bây giờ mô tả bộ đánh giá Head-to-Tail, các chỉ số và phương pháp đánh giá của chúng tôi.

### 2.1 Tạo cặp QA
**Lĩnh vực và nguồn dữ liệu.** Để bao phủ một phạm vi rộng kiến thức, chúng tôi sử dụng đồ thị tri thức DBpedia (Auer et al., 2007), nơi kiến thức có nguồn gốc từ Wikipedia (Denoyer và Gallinari, 2006). Chúng tôi sử dụng phiên bản đã được làm sạch của ảnh chụp tiếng Anh từ ngày 1 tháng 12 năm 2022.2

Để hiểu rõ hơn hiệu suất LLM trên các lĩnh vực cụ thể, chúng tôi cũng chọn ba lĩnh vực có dữ liệu công khai dễ tiếp cận.

• **Movie**: Chúng tôi sử dụng ảnh chụp IMDb3 từ ngày 21 tháng 5 năm 2023.
• **Book**: Chúng tôi sử dụng dữ liệu Goodreads được thu thập vào năm 2017 được phát hành bởi Wan và McAuley (2018).
• **Academics**: Chúng tôi sử dụng ảnh chụp MAG (Sinha et al., 2015) từ ngày 13 tháng 9 năm 2021 và DBLP4 từ ngày 10 tháng 5 năm 2023.

**Thực thể.** Một đóng góp quan trọng của bộ đánh giá Head-to-Tail là việc chia nhóm head, torso và tail entities, được quyết định bởi độ phổ biến của các thực thể (chúng tôi cũng sẽ thảo luận về cách độ phổ biến của các vị từ ảnh hưởng đến kết quả trong Phần 3.3). Chúng tôi sử dụng hai cách để ước lượng độ phổ biến: traffic và density. Khi có thông tin traffic, như lượt xem và lượt bình chọn, chúng tôi tiện lợi sử dụng traffic để đo độ phổ biến; nếu không, chúng tôi sử dụng density như một proxy, chẳng hạn như số lượng sự thật hoặc tác phẩm được tác giả về thực thể. Chúng tôi thường quan sát mối tương quan giữa density và traffic (ví dụ, người nào càng phổ biến, chúng ta càng biết nhiều về họ), nhưng như chúng ta sẽ thấy sớm từ thống kê bộ đánh giá (Bảng 1), chúng vẫn có thể dẫn đến các phân phối head, torso, và tail hơi khác nhau. Chúng tôi đưa ra chi tiết về cách chúng tôi quyết định độ phổ biến của các loại thực thể khác nhau từ mỗi nguồn dữ liệu trong Phụ lục A.2.

[Bảng 1: Số lượng (%) của head, torso, và tail entities. Phân phối tuân theo luật lũy thừa.]

Chúng tôi chia nhóm head, torso, và tail entities thành ba bước. Đầu tiên, chúng tôi sắp xếp các thực thể theo độ phổ biến của chúng, được đo như trên. Thứ hai, đối với mỗi thực thể, chúng tôi tính điểm phổ biến tích lũy lên đến thực thể top-1 trong danh sách đã sắp xếp. Thứ ba, chúng tôi chia nhóm các thực thể sao cho head entities bao gồm các thực thể có điểm phổ biến tích lũy lên đến 1/3 của tất cả thực thể, torso entities bao gồm các thực thể có điểm tích lũy từ 1/3 đến 2/3, và tail entities từ 2/3 đến 1. (Xem Phụ lục A.7 để biết ví dụ.) Chúng tôi xác định việc phân vùng riêng biệt cho các loại thực thể khác nhau cho mỗi lĩnh vực.

Để làm cho điểm phổ biến công bằng, chúng tôi lọc ra các thực thể có khả năng quá mới để có đủ dữ liệu thống kê cho việc đo độ phổ biến. Đối với IMDb, MAG, DBLP, và Goodreads, chúng tôi chỉ giữ lại các thực thể theo năm 2020, 2020, 2020, và 2015, tương ứng. Các năm cắt đều trước thời gian cắt của dữ liệu huấn luyện LLM, do đó bộ đánh giá tránh các câu hỏi đòi hỏi kiến thức gần đây. Chúng tôi không thực hiện lọc tương tự cho DBpedia vì thuộc tính năm không có sẵn hoặc không áp dụng cho hầu hết các thực thể, và nghiên cứu thử nghiệm của chúng tôi cho thấy rất ít (nếu có) câu hỏi được tạo từ DBpedia đòi hỏi kiến thức sau năm 2020.

Bảng 1 tóm tắt phân phối của head, torso, và tail entities. Phân phối tuân theo luật lũy thừa, trong đó tỷ lệ phần trăm rất nhỏ của các thực thể rơi vào nhóm head và torso, và phần lớn các thực thể rơi vào nhóm tail; ví dụ, hơn 99.9% phim rơi vào nhóm tail, theo số lượng bình chọn IMDb. Chúng tôi cũng quan sát rằng hiện tượng này rõ ràng hơn khi chúng tôi đo bằng traffic hơn là density; đối với cái sau, các nhóm torso thường lớn hơn (~15% thực thể), và tail nhỏ hơn một chút (~82%).

**Câu hỏi.** Chúng tôi tạo câu hỏi bằng cách tiếp cận dựa trên mẫu, trong đó mỗi câu hỏi được tạo yêu cầu một thuộc tính của một thực thể. Chúng tôi lọc ra các loại thuộc tính sau: (i) không cụ thể (ví dụ, seeAlso trong DBpedia), (ii) động (ví dụ, lastLaunchRocket trong DBpedia), (iii) cụ thể cho nguồn dữ liệu (ví dụ, averageRating trong IMDb), và (iv) không phải văn bản (ví dụ, picture trong DBpedia). Chúng tôi thảo luận tiêu chí lọc nghiêm ngặt hơn trong Phụ lục A.4. Đối với mỗi lĩnh vực cụ thể (Movie, Book, Academics), chúng tôi thiết kế thủ công mẫu câu hỏi cho mỗi thuộc tính. DBpedia chứa một tập hợp lớn các thuộc tính, vì vậy trước tiên chúng tôi sử dụng ChatGPT để soạn thảo các mẫu (sử dụng Prompt 1 trong Phụ lục A.1), sau đó đọc lại chúng thủ công và thực hiện các chỉnh sửa cần thiết. Mỗi mẫu câu hỏi tương ứng với một vị từ riêng biệt.

Câu trả lời cho mỗi câu hỏi là đối tượng của bộ ba liên quan; khi có nhiều câu trả lời (ví dụ, một cuốn sách có thể có nhiều tác giả), chúng tôi bao gồm tất cả trong câu trả lời. Khi cần thiết, chúng tôi bao gồm thông tin bổ sung cho một thực thể để tránh sự mơ hồ tiềm ẩn (ví dụ, chúng tôi bao gồm năm xuất bản cho một cuốn sách để phân biệt các cuốn sách có tên rất giống nhau).

Chúng tôi tạo số lượng câu hỏi bằng nhau cho các thực thể head, torso, và tail được lấy mẫu ngẫu nhiên bằng cách sử dụng mỗi mẫu. Đối với mỗi lĩnh vực cụ thể, chúng tôi tạo ~1K câu hỏi cho mỗi nhóm head, torso, và tail. Vì DBPedia chứa nhiều lĩnh vực và loại mối quan hệ hơn, chúng tôi tạo ~3K câu hỏi cho mỗi nhóm. Bảng 2 tóm tắt thống kê tổng thể của Head-to-Tail về số lượng câu hỏi và mẫu.

[Bảng 2: Thống kê tổng thể của Head-to-Tail.]

### 2.2 Chỉ số
**Chỉ số.** Chúng tôi nhận thấy rằng thường thì LLMs đủ thông minh để thừa nhận rằng nó không có đủ thông tin để trả lời một câu hỏi. Do đó, chúng tôi sử dụng ba chỉ số: độ chính xác (A), tỷ lệ ảo tưởng (H), và tỷ lệ thiếu (M), đo tỷ lệ phần trăm câu hỏi mà một LLM đưa ra câu trả lời đúng, đưa ra câu trả lời sai hoặc một phần không chính xác, hoặc thừa nhận không thể trả lời, tương ứng; theo định nghĩa, A+H+M= 100%.

Việc quyết định thủ công tính đúng đắn của câu trả lời có thể cồng kềnh. Chúng tôi tiếp theo mô tả một vài cách khác nhau để tự động quyết định xem một câu trả lời có đúng hay không.

**Dựa trên LLM.** Chúng tôi yêu cầu ChatGPT kiểm tra xem một câu trả lời có đúng hay không dựa trên câu hỏi và sự thật cơ bản (Prompt 2 trong Phụ lục A.1). Do đó, độ chính xác ALM được định nghĩa là tỷ lệ phần trăm câu trả lời mà ChatGPT đánh giá là đúng; tỷ lệ ảo tưởng HLM được định nghĩa là tỷ lệ phần trăm thời gian khi (i) một câu trả lời cố gắng không thiếu, và (ii) ChatGPT đánh giá câu trả lời là không chính xác (tức là, HLM= 100% −ALM−M).

Để hiểu độ tin cậy của các chỉ số dựa trên LLM, chúng tôi lấy mẫu ngẫu nhiên 840 câu trả lời từ các LLMs được đánh giá và kiểm tra thủ công xem đánh giá của con người có đồng ý với các chỉ số dựa trên LLM hay không. Mức độ đồng ý là 98%, điều mà chúng tôi coi là đáng tin cậy. Do đó, chúng tôi sử dụng ALM và HLM như các chỉ số chính trong nghiên cứu này.

**Dựa trên quy tắc.** Ngoài ra, chúng tôi áp dụng các chỉ số phổ biến, bao gồm khớp chính xác (EM), token F1 (F1), và ROUGE-L (RL) (Lin, 2004; Rajpurkar et al., 2016); nói cách khác, chúng tôi sử dụng các phương pháp dựa trên quy tắc để đánh giá tính đúng đắn của một câu trả lời. Cụ thể, AEM được tính là tỷ lệ phần trăm câu trả lời khớp chính xác với sự thật cơ bản; AF1 được tính là trung bình hài hòa của độ chính xác và khả năng nhớ lại khi so sánh tokens trong các câu trả lời trả về và trong các câu trả lời sự thật cơ bản; ARL được tính là trung bình dãy con chung dài nhất được chuẩn hóa (LCS) giữa các câu trả lời trả về và sự thật cơ bản. Đối với các loại câu trả lời phổ biến, chúng tôi bổ sung mở rộng tập hợp câu trả lời sự thật cơ bản với các biến thể của chúng bằng cách sử dụng các quy tắc thủ công (ví dụ, "W Shakespeare" là một biến thể của "William Shakespeare"); khi một câu hỏi cho trước có nhiều câu trả lời sự thật cơ bản mở rộng, chúng tôi lấy điểm tối đa.

Tương ứng, chúng tôi đo tỷ lệ ảo tưởng bằng HEM(= 100% −AEM−M), HF1(= 100% −AF1−M), và HRL(= 100% −ARL−M). Như chúng tôi sẽ thể hiện sau trong Phần 3.5, chúng tôi quan sát mối tương quan cao giữa các chỉ số dựa trên quy tắc và dựa trên LLM.

### 2.3 Phương pháp đánh giá
Chúng tôi nhắc LLM như được thể hiện trong Prompt 3 trong Phụ lục A.1. Đầu tiên, chúng tôi yêu cầu LLMs đưa ra câu trả lời ngắn gọn nhất có thể. Thứ hai, chúng tôi nhắc LLMs phản hồi "không chắc chắn" khi LLM không tự tin vào câu trả lời. Chúng tôi áp dụng học few-shot và bao gồm trong prompt hai ví dụ không có trong Head-to-Tail: một là câu hỏi đơn giản, có thể trả lời được với câu trả lời tương ứng làm phản hồi; cái khác là câu hỏi không thể trả lời được với "không chắc chắn" làm phản hồi.

Với prompt này, các chỉ số dựa trên quy tắc có nhiều khả năng phản ánh tính đúng đắn sự thật của các câu trả lời, và chúng tôi có thể đơn giản tính tỷ lệ thiếu (tức là, M) bằng cách đếm các câu trả lời "không chắc chắn" hoặc trống. Chúng tôi quan sát rằng việc yêu cầu rõ ràng "không chắc chắn" như một câu trả lời có thể giảm đáng kể tỷ lệ ảo tưởng (Phần 3.5).

Tóm lại, ba cài đặt sau trong bộ đánh giá và phương pháp đánh giá giúp chúng tôi ước lượng tốt nhất sự tồn tại của kiến thức (tự tin) trong các LLMs: (i) tập trung vào các câu hỏi đơn giản ở dạng dễ hiểu, (ii) yêu cầu câu trả lời ngắn gọn để dễ đánh giá, và (iii) gợi ý cho LLMs trả lời "không chắc chắn" để ngăn chặn ảo tưởng không cần thiết.

## 3 Phân tích thực nghiệm

### 3.1 Mô hình và cấu hình
Chúng tôi đánh giá các LLMs đại diện, hiện đại có nhiều kích thước và kiến trúc khác nhau, bao gồm ChatGPT, GPT-4 (OpenAI, 2023), LLaMA (7B, 13B, 33B, 65B) (Touvron et al., 2023a), Llama 2 (70B) (Touvron et al., 2023b), Vicuna (7B, 13B) (Chiang et al., 2023), Flan-T5 (3B, 11B) (Chung et al., 2022), RWKV (7B) (Peng et al., 2023b), Falcon (7B, 40B), và Falcon-Instruct (7B, 40B) (Almazrouei et al., 2023). Chúng tôi sử dụng các cài đặt xác định nhất (tức là, temperature=0 hoặc top_k=1) cho tất cả các mô hình. Chúng tôi trình bày thêm chi tiết trong Phụ lục A.3.

[Bảng 3: Độ chính xác tổng thể tốt nhất chỉ ~31% trên Head-to-Tail. Tất cả các số đều là phần trăm (%).]

Bảng 14 trong Phụ lục A.8 đưa ra kết quả chi tiết của tất cả LLMs. Chúng tôi lưu ý rằng mục tiêu của chúng tôi KHÔNG phải là so sánh các mô hình LLM khác nhau; thay vào đó, bằng cách kiểm tra các chỉ số của các LLMs khác nhau, chúng tôi đảm bảo báo cáo các mẫu chung giữa các LLMs đại diện. Chúng tôi cũng lưu ý rằng khó có thể đánh giá một cách đầy đủ mọi mô hình gần đây trong lĩnh vực phát triển nhanh này; chúng tôi tiến hành đánh giá đến GPT-4 (OpenAI, 2023) và Llama 2 (Touvron et al., 2023b), và các thảo luận chi tiết có thể dựa trên các mô hình hơi cũ hơn, nơi chúng tôi quan sát các mẫu tương tự.

### 3.2 RQ1: LLMs có đáng tin cậy đến đâu trong việc trả lời các câu hỏi sự thật?
Chúng tôi trình bày trong Bảng 3 hiệu suất tổng thể của GPT-4, ChatGPT, Llama 2-70B, và LLaMA-33B, thực hiện tốt nhất trong hầu hết các chỉ số trên Head-to-Tail giữa tất cả LLMs được giới thiệu trong Phần 3.1. Độ chính xác tổng thể tốt nhất được đạt bởi GPT-4 ở mức 31%.

Thú vị là, đối với các câu hỏi không được trả lời đúng, các LLMs khác nhau cho thấy các mẫu khác nhau: GPT-4 và ChatGPT đưa ra câu trả lời không chắc chắn hoặc trống cho phần lớn trong số chúng, và tỷ lệ ảo tưởng là <20% (vẫn không thể bỏ qua); LLaMA-33B chủ yếu cung cấp các câu trả lời ảo tưởng, dẫn đến tỷ lệ ảo tưởng cao (~80%); Llama 2-70B nằm ở giữa. Chúng tôi nghi ngờ việc tinh chỉnh và học tăng cường của các mô hình này có thể giải thích các mẫu khác nhau khi mô hình không chắc chắn về câu trả lời. Hình 1 cho thấy các ví dụ về câu trả lời phản thực tế được đưa ra bởi GPT-4.

Cuối cùng, đối với tất cả các mô hình, hiệu suất tổng thể thay đổi đáng kể giữa các lĩnh vực cụ thể khác nhau. Tất cả các mô hình hoạt động tốt nhất trong lĩnh vực Movie và tồi tệ nhất trong lĩnh vực Academics, có thể do độ phổ biến tương đối thấp của lĩnh vực Academics, như chúng tôi sẽ thảo luận sớm.

[Bảng 4: Tính sự thật của LLMs, được đo bằng ALM(%), giảm theo thứ tự head, torso, và tail entities từ Head-to-Tail.]

[Bảng 5: Độ chính xác trên top-10% câu hỏi phổ biến trong nhóm head chỉ tốt hơn một chút so với tổng thể head entities.]

### 3.3 RQ2: LLMs có hoạt động tốt như nhau trên các sự thật head, torso, và tail không?
Độ chính xác QA tổng thể của GPT-4 và Llama 2-70B (ALM) giảm theo thứ tự head, torso, và tail entities, như được thể hiện trong Hình 1 và Bảng 4. Chúng tôi quan sát cùng mẫu cho các LLMs khác. Điều này xác minh giả thuyết của chúng tôi rằng vì chúng ta thiếu dữ liệu huấn luyện cho các thực thể long-tail, nên khó khăn cho LLMs để có được kiến thức cho các thực thể như vậy.

Đáng ngạc nhiên, độ chính xác QA vẫn thấp ngay cả đối với head entities (ví dụ, GPT-4 đạt ALM là 48% trong lĩnh vực mở). Chúng tôi tiếp tục giữ lại top-10% câu hỏi phổ biến từ nhóm head. Như được thể hiện trong Bảng 5, GPT-4 và Llama 2-70B có độ chính xác cao hơn một chút (trong 6 điểm phần trăm) và tỷ lệ ảo tưởng thấp hơn cho các thực thể siêu phổ biến này, nhưng độ chính xác vẫn thất vọng thấp (46% cho GPT-4 và 19% cho Llama 2-70B), và tỷ lệ thiếu đáng chú ý. Chúng tôi có thảo luận thêm trong Phụ lục A.6.

Độ chính xác QA trên tail entities thấp hơn đáng kể trong hầu hết các lĩnh vực. Đáng chú ý, Academics trực quan là một lĩnh vực long-tail, và chúng tôi quan sát ~10% độ chính xác tổng thể và độ chính xác rất thấp (16% cho GPT-4 và 13% cho Llama 2-70B) ngay cả đối với head entities trong lĩnh vực này.

Cuối cùng, tỷ lệ ảo tưởng giảm từ head đến torso đến tail cho GPT-4, nhưng tăng cho Llama 2-70B. Chúng tôi đưa ra giả thuyết rằng có ít nhất một yếu tố khác ảnh hưởng đến tỷ lệ ảo tưởng—đánh giá nội bộ về độ tin cậy. Khi một LLM "biết" những gì không biết đối với nó, nó có khả năng giảm độ tin cậy khi trả lời các câu hỏi liên quan và tạo ra ít ảo tưởng hơn.

**Vị từ head-to-tail.** Chúng tôi điều tra xem hiệu suất có vẫn tương quan với thứ tự head-to-tail liên quan đến độ phổ biến của vị từ thay vì thực thể hay không. Chúng tôi sắp xếp các vị từ từ DBpedia theo độ phổ biến (được đo bằng số lượng bộ ba quan hệ với vị từ) và phân vùng các vị từ đã sắp xếp thành head, torso, và tail theo cách tương tự. Sau đó chúng tôi phân vùng lại các câu hỏi lĩnh vực mở thành các nhóm vị từ head, torso, và tail, mỗi nhóm chứa 72, 450, và 8,610 câu hỏi, tương ứng. Vì số lượng câu hỏi trong nhóm head thấp, chúng tôi hợp nhất các nhóm head và torso.

[Bảng 6: So sánh tính sự thật của LLMs về head, torso, và tail predicates trong ALM(%) và HLM(%) sử dụng các thể hiện lĩnh vực mở từ Head-to-Tail.]

Bảng 6 so sánh hiệu suất trên head & torso so với tail. Chúng tôi không quan sát mối tương quan nhất quán giữa các LLMs khác nhau giữa hiệu suất và thứ tự vị từ head-to-tail, và sự khác biệt về độ chính xác không cao lắm. Điều này không quá ngạc nhiên vì hai lý do. Đầu tiên, ngữ nghĩa của mỗi vị từ chủ yếu nhất quán với ngữ nghĩa của tên vị từ, có thể được LLMs hiểu rõ. Thứ hai, khi các sự thật có mặt cho các vị từ tail, chúng thường về head entities, và thông tin sự thật cho head entities có khả năng dồi dào hơn trong dữ liệu huấn luyện.

### 3.4 RQ3: Các phương pháp thông thường cải thiện LLMs có tăng tính sự thật không?
Bảng 7 so sánh LLMs với các kích thước khác nhau và có hoặc không có điều chỉnh hướng dẫn. Đầu tiên, chúng tôi quan sát rằng việc tăng kích thước mô hình không tự động chuyển thành khả năng nắm bắt kiến thức sự thật tốt hơn. Ví dụ, LLaMA-33B khiêm tốn vượt trội LLaMA-65B trên các tập con head, torso, và tail (+0.4% trong ALM và −1.9% trong HLM trung bình) trong khi chúng chia sẻ cùng bộ dữ liệu huấn luyện và siêu tham số. Điều này cung cấp bằng chứng bổ sung cho giả thuyết của chúng tôi rằng một khi mô hình đủ lớn, sự dồi dào của dữ liệu huấn luyện đóng vai trò quan trọng hơn trong tính sự thật của LLMs.

Thứ hai, so với LLaMA và Falcon, các đối tác được điều chỉnh hướng dẫn (tức là, Vicuna và Falcon-Instruct) có độ chính xác thấp hơn, vì chúng học được cách bảo thủ hơn trong việc cung cấp câu trả lời sự thật và do đó tạo ra "không chắc chắn" thường xuyên hơn (ví dụ, Vicuna-13B cao hơn 26.9% trong M so với LLaMA-13B). Mặc dù vậy, chúng vẫn có tỷ lệ ảo tưởng cao.

[Bảng 7: So sánh các LLMs khác nhau với các kích thước khác nhau. Tất cả các số đều là phần trăm (%).]

### 3.5 Tính mạnh mẽ của phương pháp đánh giá của chúng tôi
Cuối cùng, chúng tôi đánh giá tính mạnh mẽ của phương pháp đánh giá của chúng tôi.

**Mối tương quan giữa các chỉ số dựa trên quy tắc và LLM.** Đối với mỗi kết hợp của độ phổ biến (head, torso, tail) và lĩnh vực (movie, book, academics, open), chúng tôi tính hệ số tương quan hạng Spearman và Pearson giữa các chỉ số dựa trên quy tắc và LLM trên tất cả LLMs. Chúng tôi báo cáo kết quả tổng hợp (tối thiểu, trung bình) trong Bảng 8. Điểm tương quan gợi ý rằng ALM (tương ứng HLM) tương quan mạnh với AEM, AF1, và ARL (tương ứng HEM, HF1, và HRL), chỉ ra rằng các chỉ số dựa trên quy tắc là các lựa chọn thay thế tốt cho đánh giá chi phí thấp hơn hoặc nhanh hơn.

[Bảng 8: Hệ số tương quan hạng Spearman tối thiểu và trung bình (ρ) và hệ số tương quan Pearson (r) cho thấy tương quan cao giữa các chỉ số LM- và dựa trên quy tắc.]

**Hiệu ứng của ngắn gọn và "không chắc chắn".** Chúng tôi lấy mẫu ngẫu nhiên 1.2K câu hỏi và kiểm tra tính ổn định của câu trả lời nếu chúng tôi gọi ChatGPT để tái tạo câu trả lời. Khi không yêu cầu câu trả lời ngắn gọn hoặc "không chắc chắn", đối với 18% câu hỏi, ChatGPT tái tạo các câu trả lời khác nhau. Thêm yêu cầu cho câu trả lời ngắn gọn (Prompt 6 trong Phụ lục A.1) giảm tỷ lệ xuống 4%, và yêu cầu thêm câu trả lời "không chắc chắn" với các ví dụ few-shot (Prompt 3) giảm tỷ lệ xuống 1%. Ngoài ra, theo đánh giá thủ công trên 150 câu hỏi được lấy mẫu ngẫu nhiên, việc loại bỏ "không chắc chắn" như một lựa chọn tăng tỷ lệ ảo tưởng của ChatGPT lên 13 điểm phần trăm.

**Tính mạnh mẽ của prompts.** Chúng tôi khám phá hai prompts khác. So với prompt gốc tiến hành học few-shot (Phần 3.1), được ký hiệu là Few-shot, prompt Zero-shot không cung cấp ví dụ và do đó là học zero-shot (Prompt 4 trong Phụ lục A.1), và prompt In-domain có ví dụ có thể trả lời được hoán đổi cho ví dụ trong lĩnh vực được tạo bởi cùng mẫu câu hỏi như câu hỏi mục tiêu (Prompt 5 trong Phụ lục A.1).

[Bảng 9: Hiệu suất của ChatGPT với các prompts khác nhau trên Head-to-Tail. Tất cả các số đều là phần trăm (%).]

Như được thể hiện trong Bảng 9, Few-shot và Zero-shot cho thấy kết quả rất tương tự, nhưng sự khác biệt hiệu suất đáng chú ý giữa Few-shot và In-domain. Đặc biệt, các ví dụ trong lĩnh vực giúp có được nhiều câu trả lời đúng hơn (+8.9%, +7.9%, +5.9% trong ALM cho head, torso, tail) nhưng với chi phí của nhiều ảo tưởng hơn (+7.5%, +8.2%, +9.7% trong HLM cho head, torso, tail). Chúng tôi nghi ngờ rằng các ví dụ trong lĩnh vực tăng cường độ tin cậy của ChatGPT trong việc trả lời một câu hỏi, vì vậy nó trả lời câu hỏi ngay cả khi độ tin cậy thực sự không cao, gây ra cả độ chính xác cao hơn và tỷ lệ ảo tưởng cao hơn.

Mặc dù có biến động, mẫu prompt gốc của chúng tôi (Few-shot) có vẻ tốt hơn trong việc ước lượng tính sự thật (tự tin) của LLMs với độ chính xác QA, và hiệu suất tương đối giữa head, torso, và tail vẫn ổn định trên các prompts khác nhau.

## 4 Thảo luận

### 4.1 Tương lai của đồ thị tri thức
Phân tích thực nghiệm chỉ ra rằng mặc dù LLMs đã kết hợp kiến thức sự thật trong các tham số của chúng, lượng kiến thức được mã hóa này vẫn hạn chế. Kiến thức về các thực thể long-tail đã thưa thớt trong KGs và thậm chí còn thiếu hụt hơn trong LLMs.

Tuy nhiên, LLMs đã cách mạng hóa cách mọi người tìm kiếm thông tin và kêu gọi xem xét lại biểu diễn tốt nhất của kiến thức sự thật. Chúng tôi gọi thế hệ KGs sắp tới là Dual Neural KGs: kiến thức có thể tồn tại một cách rõ ràng như các bộ ba (tương tự như KGs) và ngầm định như các embeddings (như trong LLMs); dạng ký hiệu phục vụ cho sự hiểu biết và giải thích của con người, trong khi dạng neural có lợi cho sự hiểu biết của máy và các cuộc trò chuyện liền mạch. Một phần kiến thức có thể tồn tại ở cả hai định dạng hoặc ở định dạng phù hợp hơn. Sự pha trộn hài hòa của hai dạng, tận dụng các đổi mới LLM mới nhất, là một lĩnh vực nghiên cứu thú vị như chúng tôi sẽ trình bày tiếp theo.

**Kiến thức Head.** Điều này liên quan đến các thực thể phổ biến nơi dữ liệu huấn luyện dồi dào. Lý tưởng nhất, LLMs có thể được dạy kiến thức như vậy để truy xuất hiệu quả, có nghĩa là kiến thức head sẽ tồn tại ở cả hai dạng. Hiện tại, LLMs vẫn có độ chính xác QA trung bình cho các thực thể phổ biến (xem Bảng 5), vì vậy một lĩnh vực nghiên cứu quan trọng là truyền kiến thức head vào LLMs thông qua huấn luyện mô hình hoặc tinh chỉnh. Công việc sớm trong hướng này bao gồm knowledge infusion (Liu et al., 2021; Wang et al., 2021; Zhen et al., 2022).

**Kiến thức Torso-to-tail và gần đây.** Điều này liên quan đến các thực thể không phổ biến và kiến thức mới nổi, nơi dữ liệu huấn luyện thường thưa thớt hoặc vắng mặt. Loại kiến thức này có thể được thể hiện tốt nhất dưới dạng bộ ba. Phục vụ kiến thức như vậy đòi hỏi quyết định hiệu quả khi nào kiến thức bên ngoài là cần thiết, truy xuất hiệu quả kiến thức liên quan, và tích hợp liền mạch nó vào các câu trả lời. Các nỗ lực sớm trong hướng này liên quan đến LLMs được tăng cường kiến thức (Asai et al., 2023; Nakano et al., 2022; Shi et al., 2023; Borgeaud et al., 2022).

### 4.2 Hạn chế và mở rộng
**Phân loại.** Công việc của chúng tôi không thảo luận về hiệu quả của LLMs trong việc nắm bắt phân loại hoặc cấu trúc phân cấp kiểu, có thể là một mở rộng của nghiên cứu này. Cụ thể, chúng tôi đưa ra giả thuyết rằng LLMs có thể kết hợp hiệu quả các mối quan hệ kiểu (ví dụ, hypernyms và synonyms), ngay cả đối với các kiểu con có độ chi tiết cao. Do đó, có thể không còn đáng để xây dựng thủ công một cấu trúc phân cấp rất sâu và phức tạp trong tương lai.

**Tính mạnh mẽ đối với công thức câu hỏi.** Bài báo này chủ yếu nhằm đánh giá một LLM "biết" một sự thật với độ tin cậy cao đến mức nào; do đó chúng tôi kiểm tra nhiều cách khác nhau để công thức hóa các câu hỏi sự thật và chọn dạng ít mơ hồ nhất cho nghiên cứu này. Tuy nhiên, cách tiếp cận này không đánh giá tính mạnh mẽ của mô hình đối với paraphrasing hoặc xem xét các cách đa dạng mà mô hình có thể được truy vấn, chẳng hạn như entailment hoặc prompts dạng cloze. Thí nghiệm bổ sung của chúng tôi trong Phụ lục A.5 gợi ý rằng việc thay đổi dạng của câu hỏi không ảnh hưởng đáng kể đến kết quả đánh giá. Một đánh giá tính mạnh mẽ kỹ lưỡng hơn nằm ngoài phạm vi của bài báo này và để lại cho nghiên cứu tương lai.

## 5 Công trình liên quan
**Bộ đánh giá.** Hầu hết các công trình nghiên cứu tính sự thật của LLMs sử dụng các bộ đánh giá QA hiện có như WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), LC-QuAD (Trivedi et al., 2017; Dubey et al., 2019), QALD-9 (Usbeck et al., 2018), Natural Questions (Kwiatkowski et al., 2019), và EntityQuestions (Sciavolino et al., 2021). Một hướng công việc gần đây đã xây dựng các bộ đánh giá QA mới để đánh giá tính sự thật của LLMs, đặc biệt cho kiến thức long-tail (Mallen et al., 2023; Kim et al., 2023).

So với các bộ đánh giá này, Head-to-Tail là đầu tiên đánh giá cụ thể mức độ LLMs kết hợp thông tin sự thật head, torso, và tail.

**Đánh giá LLM.** Những năm gần đây đã chứng kiến sự phát triển mạnh mẽ của nghiên cứu về đánh giá tính sự thật của LLMs (Roberts et al., 2020; Petroni et al., 2021; Shuster et al., 2021; Mielke et al., 2022; Tan et al., 2023; Hu et al., 2023; Peng et al., 2023a; Omar et al., 2023; Kandpal et al., 2023; Mallen et al., 2023; Chen et al., 2023). Hầu hết các công trình này tập trung vào một nguồn kiến thức duy nhất, như Freebase hoặc Wikipedia, và chúng chưa thực hiện đánh giá một cách có hệ thống một cách rõ ràng liên quan đến head/torso/tail entities hoặc attributes. Một công trình gần với chúng tôi là Omar et al. (2023), đánh giá ChatGPT sử dụng các sự thật được thu thập từ các nguồn kiến thức đa dạng; tuy nhiên, đánh giá của họ được thực hiện thủ công chỉ trên 450 thể hiện QA.

Có ba công trình cũng cho thấy mối tương quan giữa độ chính xác QA của các mô hình ngôn ngữ và độ phổ biến của sự thật (Mallen et al., 2023; Kandpal et al., 2023; Kim et al., 2023). Công việc của chúng tôi, được tiến hành song song, tập trung vào một góc độ khác—các LLMs có hiểu biết đến đâu? Cho mục đích này, chúng tôi thiết kế có hệ thống phương pháp thực nghiệm, bao gồm định nghĩa của head, torso, và tail entities, thiết kế các chỉ số, và phương pháp đánh giá. Bộ đánh giá của chúng tôi toàn diện trong việc chứa các nguồn kiến thức khác nhau, các lĩnh vực khác nhau, và các mối quan hệ phong phú. So với ba công trình này, chúng tôi đưa ra câu trả lời định lượng hơn cho các câu hỏi nghiên cứu RQ1–RQ3.

## 6 Kết luận
Chúng tôi giới thiệu Head-to-Tail, bộ đánh giá đầu tiên được thiết kế để đánh giá khả năng của LLMs nội hóa các sự thật head, torso, và tail. Cùng với bộ dữ liệu, chúng tôi trình bày một phương pháp đánh giá mới với các chỉ số thích hợp để tự động đánh giá tính sự thật của LLMs. Đánh giá của chúng tôi cho thấy rằng ngay cả các LLMs tiên tiến nhất cũng có những hạn chế đáng chú ý trong việc thể hiện kiến thức sự thật, đặc biệt đối với các thực thể torso và tail. Theo đó, chúng tôi đề xuất các lĩnh vực nghiên cứu mới để pha trộn liền mạch kiến thức ở dạng ký hiệu và dạng neural.

## Lời cảm ơn
Chúng tôi muốn cảm ơn các reviewer ARR ẩn danh và meta reviewer vì phản hồi xây dựng và sâu sắc của họ.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc do chứa tên riêng và thuật ngữ chuyên môn]

## A Phụ lục

### A.1 Danh sách Prompts
[Các prompts được dịch sang tiếng Việt nhưng vẫn giữ cấu trúc và ý nghĩa gốc]

### A.2 Đo độ phổ biến trong phân vùng head-to-tail
[Nội dung được dịch chi tiết các phương pháp đo độ phổ biến]

### A.3 Chi tiết triển khai
[Chi tiết kỹ thuật được dịch]

### A.4 Tác động của các câu hỏi ít xuất hiện tự nhiên
[Phân tích được dịch với các bảng số liệu]

### A.5 Đặt câu hỏi ở các dạng khác nhau
[Thí nghiệm bổ sung được dịch]

### A.6 Thảo luận thêm về tỷ lệ thiếu
[Phân tích chi tiết về missing rate]

### A.7 Ví dụ về chia nhóm thực thể
[Ví dụ minh họa được dịch]

### A.8 Kết quả bổ sung
[Bảng 14 với kết quả chi tiết được dịch đầy đủ]
