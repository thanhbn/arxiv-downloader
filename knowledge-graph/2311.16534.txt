# 2311.16534.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2311.16534.pdf
# File size: 5505099 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1
Graph Prompt Learning: A Comprehensive
Survey and Beyond
Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, Jia Li
Abstract ‚ÄîArtificial General Intelligence (AGI) has revolutionized numerous fields, yet its integration with graph data, a cornerstone in
our interconnected world, remains nascent. This paper presents a pioneering survey on the emerging domain of graph prompts in AGI,
addressing key challenges and opportunities in harnessing graph data for AGI applications. Despite substantial advancements in AGI
across natural language processing and computer vision, the application to graph data is relatively underexplored. This survey critically
evaluates the current landscape of AGI in handling graph data, highlighting the distinct challenges in cross-modality, cross-domain,
and cross-task applications specific to graphs. Our work is the first to propose a unified framework for understanding graph prompt
learning, offering clarity on prompt tokens, token structures, and insertion patterns in the graph domain. We delve into the intrinsic
properties of graph prompts, exploring their flexibility, expressiveness, and interplay with existing graph models. A comprehensive
taxonomy categorizes over 100 works in this field, aligning them with pre-training tasks across node-level, edge-level, and graph-level
objectives. Additionally, we present, ProG, a Python library, and an accompanying website, to support and advance research in graph
prompting. The survey culminates in a discussion of current challenges and future directions, offering a roadmap for research in
graph prompting within AGI. Through this comprehensive analysis, we aim to catalyze further exploration and practical applications
of AGI in graph data, underlining its potential to reshape AGI fields and beyond. ProG and the website can be accessed by https:
//github.com/WxxShirley/Awesome-Graph-Prompt, and https://github.com/sheldonresearch/ProG, respectively.
Index Terms ‚Äîgraph prompt, graph pre-training, graph learning, artificial general intelligence.
‚ú¶
1 I NTRODUCTION
In an era marked by the rapid evolution of Artificial General
Intelligence (AGI), there emerged many fantastic applications
with AGI techniques such as ChatGPT in Natural Language
Processing (NLP) and Midjourney in Computer Vision (CV).
AGI has greatly improved our lives, making our work more
efficient and freeing us from repetitive tasks to focus on more
creative endeavors. However, when it comes to working
with graph data, AGI applications are still in their early
stages compared with the huge success in NLP [ 9,2,50]
and CV areas [ 91,114]. In our increasingly interconnected
world, understanding and extracting valuable insights from
graphs is crucial. This places AGI applied to graph data
at the forefront of both academic and industrial interest
[48,120,108], with the potential to redefine fields like drug
design [68, 64] and battery development [90], etc.
However, realizing this vision is never easy. Figure 1
illustrates this landscape for recent research in Artificial
General Intelligence, from which we can see that there are
at least three fundamental problems in technique: How to
make the model general for different modalities, different
domains, and different tasks? Within the NLP and CV
areas, there have been many commercial models that can
‚Ä¢Xiangguo Sun, Hong Cheng: Department of Systems Engineering and
Engineering Management, and Shun Hing Institute of Advanced Engi-
neering, The Chinese University of Hong Kong, Hong Kong SAR.
{xgsun, hcheng }@se.cuhk.edu.hk
‚Ä¢Jiawen Zhang, Jia Li: Hong Kong University of Science and Technology
(Guangzhou), China.
jzhang302@connect.hkust-gz.edu.cn,jialee@ust.hk
‚Ä¢Xixi Wu, Yun Xiong: Shanghai Key Laboratory of Data Science, School of
Computer Science, Fudan University, China.
21210240043@m.fudan.edu.cn, yunx@fudan.edu.cn
PaperTitle:¬∑¬∑¬∑Abstract:¬∑¬∑¬∑Keywords:¬∑¬∑
cite(a) Cross-modalitiesImageTextGraph(b)  Cross-domains
Biology
KnowledgeSociety
(c)  Cross-tasks
Graph-levelMoleculeinhibit HIV?Edge-levelDid Jobsfound Apple?Node-levelIs this accountmalicious?Fig. 1: The Most Frequently Problems towards Artificial
General Intelligence. (a) Cross-modalities, (b) Cross-domains,
(c) Cross-tasks.
understand and translate information across these modalities
[9,114,2]. For example, models like BERT [ 9] and GPT-
3 [2] have demonstrated the ability to perform tasks that
involve both textual and visual information. However, in the
context of graph data, the harmonization of information from
multiple modalities remains largely uncharted territory [ 44].
For the cross-domain issue, transfer learning has proven
effective, enabling models to apply knowledge learned
from images and text in one domain to another. However,
transferring knowledge between different graph domains
is very tough because the semantic spaces are not aligned
[125] and the structural patterns are also not similar [ 122],
making graph domain adaptation remains a very frontier
and not well-solved AGI problem. Currently, most graph
research on transfer learning focuses on the third problem,
how to leverage the pre-trained graph knowledge in the
same graph domain to perform different graph tasks (likearXiv:2311.16534v1  [cs.AI]  28 Nov 2023

--- PAGE 2 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2
node classification, link prediction, graph classification, etc)
[78,52,80,12,124,31,74,17]. However, compared with
the huge success in NLP and CV , task transferring within
the same graph domain is still primitive with far fewer
instances of successful industrial applications. While the
AGI research boasts notable achievements in many linear
data like images, text [ 67,9,2], and videos [ 91,114], the
fundamental problems within the realm of graph data
remain underexplored. Besides the above three foundation
problems, Artificial General Intelligence has also encountered
many social disputes. For example, training large foundation
models consumes exorbitant amounts of energy and may
yield unintended counterfactual outcomes [ 51,71]. These
concerns have led to a growing consensus within the AI
community on the efficient extraction of useful knowledge
preserved by these large models, minimizing the need
for repetitive fine-tuning across various downstream tasks
[16,40]. This consensus not only promises to alleviate the
environmental impact but also offers a practical solution to
the challenge of model efficiency and adaptability in an era
of AGI.
At the core of recent AGI technology, prompt learning
has presented huge potential to solve the above problems
and demonstrated remarkable success in NLP and CV
applications [ 65,86,50]. Prompt learning is the art of
designing informative prompts to manipulate input data
for the pre-trained foundation models. Figure 2 shows
an example of a textual-format prompt applied to a pre-
trained language model to directly perform downstream
inference tasks. By reformulating downstream tasks into pre-
training tasks, this approach avoids the need for extensive
model tuning and efficiently extracts preserved knowledge
[2,35]. Since its powerful capabilities in data manipulation,
task reformulation, and extraction of significant insights,
prompting is very promising to address the aforementioned
cross-modalities, cross-domains, and cross-task challenges in
one way. Compared with large models, the prompt is usually
very lightweight and can efficiently extract useful knowledge
by reducing the extensive computational resources caused
by repeat tuning of these large models [ 40,73]. Intuitively,
text and images can be perceived as specific instances of a
more general graph data structure. For instance, a sentence
can be treated as a graph path, with words as nodes, and
an image can be viewed as a grid graph, where each pixel
serves as a graph node. This insight encourages us to explore
the transference of successful prompting techniques from
text to the graph area for similar concerns.
Recently, some researchers have started to introduce
prompt learning to graph data [ 78,52,80,12,124,55,20,
4,18]. However, some further studies have found that the
graph prompt is very different from their counterparts in the
NLP area [ 80]. First, the design of graph prompts proves to be
a far more intricate endeavor compared to the formulation of
language prompts. Classic language prompts often comprise
predefined phrases or learnable vectors appended to input
text [ 2,16]. Here, the primary focus lies in the content of the
language prompt. However, we actually do not know what a
graph prompt looks like. A graph prompt not only contains
the prompt ‚Äùcontent‚Äù but also includes the undefined task
of determining how to structure these prompt tokens and
seamlessly integrate them into the original graph. Second,
Pre-trained Large Language ModelHelp me answer a multiple choiceQuestion: Greenhouses are great for plants likeA. Pizza   B. Lollipops   C. French beans
ü§ñPrompt
The correct answer is C. French beans.AnswerFig. 2: An example of the language prompt. A textual prompt
is designed to let the pre-trained large language model
perform the multi-choice question-answering task.
the harmonization of downstream graph problems with the
pre-training task is more difficult than language tasks [ 52,80].
For example, a typical pre-training approach for a language
model is to predict a masked word by the model [ 9]. Then
many downstream tasks such as question answering, and
sentiment classification can be easily reformulated as word-
level tasks [ 50]. Unlike NLP , where pre-training tasks often
share a substantial task sub-space, graph tasks span node-
level [ 19], edge-level [ 117], and graph-level objectives [ 76,79],
making pre-training pretexts less adaptable. Third, compared
with prompts in NLP which are usually some understandable
phrases, graph prompts are usually less intuitive to non-
specialists. The fundamental nature and role that graph
prompts play within the graph model remain somewhat
elusive without comprehensive theoretical analysis. There is
also a lack of clear-cut evaluation criteria for the quality
of designed graph prompts. In addition, there are still
many unclear questions for us to further understand graph
prompting. For example, how effective are these graph
prompts? What is their efficiency in terms of parameter
complexity and training burden? How powerful and flexible
do these prompts manipulate the original graph data? In light
of these intricacies questions, there is a pressing need for
delving deeper into the potential of graph prompts in AGI,
thereby paving the way for a more profound understanding
of this evolving frontier within the broader data science
landscape.
While there have been recent endeavors to explore graph
prompting, a consistent framework or clear route remains
unavailable. These efforts vary significantly in terms of
perspective, methodology, and target tasks, which present
a fragmented landscape of graph prompting and pose a
considerable obstacle to the systematical advancement of
this research area. There arises an urgent need to provide
a panoramic view, analysis, and synthesis of the latest
advances in this realm with a unified framework. In light of
this situation, we offer this survey to present how existing
work on graph prompts tries to solve the three foundation
problems towards AGI as previously mentioned. Beyond
that, we also wish to push forward the research area by
answering the following detailed research questions (RQs):
‚Ä¢RQ1: How to understand existing work with a
unified framework since they are very different? Our

--- PAGE 3 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 3
main focus is to understand the various methods used
in the field of graph prompting. We want to bring
together all these different approaches and ideas to
create a single, cohesive framework. This framework
will help us thoroughly grasp the existing research
and provide a strong foundation for future research
in this area.
‚Ä¢RQ2: Why Prompt? What‚Äôs the Nature of Graph
Prompt? In this part of our study, we aim to un-
derstand why prompts are important. We want to
uncover the fundamental aspects of graph prompts.
What exactly do graph prompts do in graph prob-
lems? How do they fit into the complex graphs,
and how do they help us achieve the broader goal
of creating AI systems that can handle graph data
effectively? These questions highlight the significant
role that graph prompts play in shaping the future of
AI when dealing with graph information.
‚Ä¢RQ3: How to Design Graph Prompts? Designing
good graph prompts is a complex task. In this part,
we explore the technical details of designing graph
prompts: what do they look like, how do they align
downstream tasks and the pre-train task, and how
they are learned? These important questions focus on
the skill of making prompts that work well with the
complexities of graph data, helping researchers make
better prompts.
‚Ä¢RQ4: How to Deploy Graph Prompts in Real-world
Applications? At the moment, there isn‚Äôt an easy-to-
use toolkit for creating graph prompts. The potential
applications that graph prompts can be deployed are
under-exploration. This research question focuses on
making graph prompts practical for use in real-world
situations with an easily extensible programming
package.
‚Ä¢RQ5: What Are the Current Challenges and Fu-
ture Directions in Graph Prompting? This question
guides us to look at the challenges we‚Äôre dealing
with today and the way forward. By tackling these
important questions, we hope to provide a roadmap
for ongoing graph-prompting research.
To answer the first research question (RQ1), we propose
a unified framework to analyze graph prompt learning work.
Our framework casts the concept of a graph prompt into
prompt tokens, token structures, and inserting patterns. This
higher-level perspective offers clarity and comprehensive-
ness, providing readers with a structured understanding of
this burgeoning field. To the best of our knowledge, our
survey marks the first of its kind to bring together these
multifaceted aspects of graph prompting within a single
unified framework.
To answer the second research question (RQ2), we explore
the correlations between prompts and existing graph models
through the lenses of flexibility and expressiveness and then
present a fresh and insightful perspective to uncover the na-
ture of graph prompts. Unlike most prompt learning surveys
in NLP areas [ 50] that treat prompting as a trick of filling
the gap between the pre-training tasks and downstream
tasks, we reveal that graph prompts and graph models are
interconnected on a deeper level. This novel perspectiveoffers invaluable insights into why prompt learning holds
promise in the graph area and what distinguishes it from
traditional fine-tuning methods [ 30]. To our knowledge, this
is the first endeavor to offer such an illuminating perspective
on graph prompting.
To answer the third research question (RQ3), we introduce
a comprehensive taxonomy that includes more than 100
related works. Our taxonomy dissects these works, catego-
rizing them according to node-level, edge-level, and graph-
level tasks, thereby aligning them with the broader context
of the pre-training task. This will empower our readers
with a clearer comprehension of the mechanisms underlying
prompts within the whole ‚Äùpre-training and prompting‚Äù
workflow.
To answer the fourth research question (RQ4), we de-
veloped ProG (prompt graph)1, a unified Python library
to support graph prompting. Additionally, we established
a website2that serves as a repository for the latest graph
prompt research. This platform curates a comprehensive col-
lection of research papers, benchmark datasets, and readily
accessible code implementations. By providing this accessible
ecosystem, we aim to empower researchers and practitioners
to advance this burgeoning field more effectively.
Beyond these, our survey goes a step further with an in-
troduction of potential applications, a thoughtful analysis of
the current challenges, and a discussion of future directions,
thus providing a comprehensive roadmap for the evolution
of this vibrant and evolving field (RQ5). Our contributions
are summarised as follows:
‚Ä¢Enabling Comprehensive Analysis. We propose a
unified framework for analyzing graph prompt learn-
ing work, providing a comprehensive view of prompt
tokens, token structures, and inserting patterns.
‚Ä¢Novel Perspectives on Prompt-Model Interplay. We
offer fresh insights into the nature of graph prompts.
Unlike traditional work that simply treats prompts as
a trick of filling the gap between downstream tasks
and the pre-train task, we explore the flexibility and
expressiveness issues of graph models and pioneer a
more thorough perspective into the interplay between
prompts and existing graph models.
‚Ä¢A Systematic Taxonomy of Graph Prompting. We
systematically explore over a hundred recent works
in the domain of graph prompting. This taxonomy
not only organizes these contributions but also fur-
nishes readers with a comprehensive understanding
of prompt mechanisms within the overarching ‚Äùpre-
training and prompting‚Äù workflow.
‚Ä¢Empowering the Graph Prompting Ecosystem. We
developed ProG, a Python library supporting graph
prompting, and a comprehensive website for collect-
ing the latest graph prompt research.
‚Ä¢Charting a Path Forward. A detailed exploration of
current challenges and future directions in the field.
Roadmap. The rest of this survey is organized as follows:
we present our survey methodology in section 2, followed
by preliminaries in section 3, the introduction of pre-training
1. https://github.com/sheldonresearch/ProG
2. https://github.com/WxxShirley/Awesome-Graph-Prompt

--- PAGE 4 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 4
methods in section 4, prompting methods for graph models
in section 5. We discuss potential applications of graph
prompt in section 7 and present our developed library ProG
in section 8. In section 9, we summarize our survey with
current challenges and future directions. Section 10 concludes
the survey and presents the contribution declaration of the
authors.
2 S URVEY METHODOLOGY
2.1 Research Objectives
This survey will introduce the art of prompting from a
big picture of artificial general intelligence (AGI). We first
present three fundamental problems towards AGI in Table
1. Recently, prompt learning has been demonstrated as a
promising solution to these problems in many linear data
such as text [ 2,16], images [ 91], etc. However, whether the
prompt technique can still solve these problems in the graph
area, is not clearly discussed. Through this survey, we wish
to figure out how the graph prompt potentially helps graph
models to be more general across various tasks and domains,
and how it generalizes the foundation models to interact
with other modalities (e.g. text, image, etc). Beyond the
above common problems of AGI in NLP , CV , and graph
areas, graph prompting is usually very different from its
counterparts in NLP and CV areas, leading to many detailed
questions as shown in Table 1.
2.2 Taxonomy
The taxonomy of this survey is presented in Figure 3, which
is intricately designed to categorize graph prompts based on
their specific applications and functionalities, providing a
structured approach to understanding their role in AGI.
(1) Pre-training Strategies for Graph Prompt.
Since prompting techniques mostly seek to reformulate
downstream tasks to the pre-training tasks, they are highly
customized for detailed pre-training approaches, thus we
briefly discuss the representative pre-training work in the
graph area before we formally introduce graph prompt
content. We split existing pre-training approaches into node-
level, edge-level, graph-level, and multi-task pre-training
strategies. Next, we present how different prompting ideas
reformulate various downstream tasks to the corresponding
pre-training tasks in the graph area. By revisiting existing
pre-training literature, we will be more clear about the role
of graph prompts in the whole ‚Äùpre-training and prompting‚Äù
framework.
(2) Prompting Methods in Graph Areas.
Aiming at the three foundation problems mentioned in
the research objectives (P1-P3 in Table 1), we analyze graph
prompting from three aspects: i. prompt design for graph
tasks (Section 5); ii. multi-modal prompting with graphs
(Section 6); and iii. graph domain adaptation with prompting
techniques (Section 6.2). Within each aspect, we present a
detailed discussion related to the five specific problems in
the graph prompt area (Q1-Q5 in Table 1).
i. Prompt Design for Graph Tasks. In this section, we
propose a unified framework to analyze existing works on
graph prompt design. Our framework treats existing graph
prompts with three key components: prompt tokens , whichTABLE 1: Research Objectives
Foundamental
Problems
towards AGIP1: How to Make the Model General for Different
Modalities? (Section 6.1)
P2: How to Make the Model General for Different
Domains? (Section 6.2)
P3: How to Make the Model General for Different
Tasks? (Section 5)
Detailed
Questions of
Graph PromptQ1: How to Understand Existing Work with a
Unified Framework? (Section 5.1)
Q2: What‚Äôs the Nature of Graph Prompt?
(Section 3.5, Section 5.4)
Q3: How to Design Graph Prompts? (Section 5)
Q4: How to Deploy Graph Prompts in Real-world
Applications? (Section 7, Section 8)
Q5: What Are the Current Challenges and Future
Directions? (Section 9.1)
preserve prompt content as vectors; token structures , which
indicate how multiple tokens are organized; and inserting
patterns , which define how to combine graph prompt with
the original graphs. Beyond that, we also carefully analyze
how these works design the prompt answering function,
which means how they get results for the downstream tasks
from their prompts. We also summarize three representative
methods to learn appropriate prompts, including meta-
learning techniques, task-specific tuning, and tuning in line
with pretext. In the end, we further discuss these works in
Section 5.4 to see their intrinsic connections with pros and
cons.
ii. Multi-modal Prompting with Graphs. In this section, we
briefly present how graph prompts work in a text-attributed
graph, which can be seen as the fusion of text and graph
modalities. With the progress of large language models
(LLMs), the fusion of text and graph data has become easier
and has aroused a lot of work on this topic. Since the topic of
integrating LLMs with graphs has been well summarized in
[46], we won‚Äôt present too much in this survey. Instead, we
only briefly discuss some representative works in this area
that focus on the prompt area.
iii. Graph Domain Adaptation through Prompting Techniques.
In this section, we introduce related work from two branches.
The first branch presents works solving semantic alignment
across different graph domains, and the second branch
presents structural alignment.
2.3 Literature Overview
In this survey, we carefully studied more than 100 high-
quality papers published within the past 5 years from
reputable conferences and journals including but not limited
to NeurIPS, SIGKDD, The Web Conference, ICLR, CIKM,
ICML, IJCAI, EMNLP , SIGIR, ACL, AAAI, WSDM, TKDE,
etc. Most of these venues are ranked as CCF A3or CORA
A*4. Besides these works, we also introduce several latest
works in arXiv so that our survey can catch up with the
frontier and latest progress in this area. A more detailed pie
chart (Figure 4a) presents the distribution of collected papers
3. https://www.ccf.org.cn/Academic Evaluation/By category
4. https://www.core.edu.au/conference-portal

--- PAGE 5 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 5TaxonomyPre-training Strategies for
Graph Prompt (Section 4)Node-level Strategies
(Section 4.1)Cheng et al. [6], Zhu et al. [123], Jin et al. [37], Peng et al. [62]
Hamilton et al. [21], Hou et al. [27], Wang et al. [95, 89], Park et al. [61]
Hou et al. [28], Wang et al. [95], Jiang et al. [33]
Edge-level Strategies
(Section 4.2)Jin et al. [38], Long et al. [54], Tan et al. [82], Li et al. [41], Pan et al. [58]
Hasanzadeh et al. [23], Kim and Oh [39]
Graph-level Strategies
(Section 4.3)Xie et al. [104], Rong et al. [68], Velickovic et al. [88], Sun et al. [76]
Sun et al. [77], Subramonian [75], You et al. [111], Suresh et al. [81]
Thakoor et al. [84], Qiu et al. [66]
Multi-task Pre-training
(Section 4.4)Hu et al. [30, 31], Zhang et al. [115], Fang et al. [13]
Prompt Design for
Graph Tasks (Section 5)Prompt Token, Structure, and
Inserting Pattern (Section 5.1)Prompt as TokensZhu et al. [125], Fang et al. [11, 12]
Gong et al. [18], Tan et al. [83], Liu et al. [52]
Zhu et al. [124], Ma et al. [55], Sun et al. [78]
Chen et al. [4], Shirkavand and Huang [74]
Prompt as Graphs Sun et al. [80], Huang et al. [32], Ge et al. [17]
Aligning Tasks by Answering
Function (Section 5.2)Handling Different Level TasksSun et al. [80], Huang et al. [32]
Liu et al. [52], Gong et al. [18]
Learnable Answering FunctionSun et al. [80], Fang et al. [11]
Fang et al. [12], Tan et al. [83]
Hand-crafted Answering FunctionSun et al. [80, 78]
Ma et al. [55], Ge et al. [17]
Huang et al. [32], Liu et al. [52]
Prompt Tuning (Section 5.3)Meta-Learning Technique Sun et al. [80]
Task-specific Tuning Fang et al. [11]
Tuning in Line with PretextLiu et al. [52], Sun et al. [78]
Tan et al. [83], Ma et al. [55]
Ge et al. [17], Gong et al. [18]
Chen et al. [4]
Connection, Pros, and Cons
(Section 5.4)Fang et al. [11, 12], Sun et al. [80], Ma et al. [55], Liu et al. [52]
Sun et al. [78], Huang et al. [32], Chen et al. [4], Tan et al. [83]
Multi-Modal Prompting
with Graphs (Section 6.1)Prompt in Text-Attributed Graphs Wen and Fang [97, 98], Zhao et al. [121], Li and Hooi [45]
Large Language Models in Graph
Data ProcessingChen et al. [5], Fatemi et al. [14], Jin et al. [36], Wang et al. [96]
Multi-modal Fusion with Graph
and PromptingEdwards et al. [10], Li et al. [44], Liu et al. [49]
Graph Domain Adaptation
with Prompting (Section 6.2)Semantic AlignmentSun et al. [80], Zhu et al. [125], Zhang et al. [119], Yi et al. [110]
Liu et al. [47]
Structural AlignmentShirkavand and Huang [74], Cao et al. [3], Zhao et al. [122]
Guo et al. [20]
Fig. 3: Taxonomy of this Survey with Representative Works.
over these venues. Furthermore, we conducted an analysis
of the topics covered by these references. In Figure 4b, we
present the top 15 keywords that appeared in the titles of
these papers. Notably, these keywords align closely with the
focus of our survey, which is centered around graph domains
and prompt learning.
Connection to Existing Work:
Our survey stands out from existing surveys in several
notable ways. For example, Liu et al. [48] primarily focuses
on graph foundation models (GFMs). Their survey does not
specifically target graph prompts, and only a few papers in
this area are briefly discussed. Li et al. [46] systematically
analyze recent works that integrate graphs and LLMs, which
is a detailed analysis of a small portion (Section 6.1) in
our survey. We go beyond their scope by exploring various
aspects of graph prompts in a more extensive manner. Mean-
while, surveys [ 105,113] focus primarily on the pre-training
stages, without involving the crucial aspect of graph promptlearning. While a prior survey on graph prompt learning by
Wu et al. [102] exists, our survey surpasses it in several
key aspects. Firstly, we provide a more comprehensive
analysis of related works . Their survey was published in
May 2023 when there were only a few graph prompt works
available [ 78,124,11]. In contrast, our survey encompasses
a broader scope, including all relevant works in the field.
Secondly, we offer a systematic analysis of existing works
within a uniform framework, facilitating comprehension
and comparison between different approaches. Thirdly, our
survey provides deep insights into the relationship between
graph pre-training and prompts , shedding light on the
interplay between these critical elements. Lastly, we not
only present empirical insights but also include engineering
works aimed at deploying graph prompts in real-world
applications, ensuring the practical applicability of our
survey.

--- PAGE 6 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 6
Others, 5%ICCV, 1%ACM Computing Surveys, 1%Nature, 1%NAACL, 1%TKDE, 2%WSDM, 2%AAAI, 3%ACL, 3%SIGIR, 3%EMNLP, 3%IJCAI, 3%ICML, 3%CIKM, 3%ICLR, 6%The Web Conference, 6%KDD, 10%NeurIPS, 10%arXiv preprint, 31%
(a) Venue distribution of collected papers.
 (b) Top 15 Keywords appeared in titles of collected papers
Fig. 4: Statistics of the collected papers
3 P RELIMINARIES
Graph representation learning has been a topic of extensive
research over the past few decades. This journey, illustrated
in Figure 5, has witnessed the evolution from shallow
embedding methods to supervised graph neural networks,
transitioning from the fine-tuning paradigm to the emerging
prompting paradigm. In this section, we will provide an
overview of the fundamental notations employed in this
survey, delve into the historical developments of graph
representation learning, explore the pre-training and fine-
tuning paradigm, and trace the evolution of prompt-based
learning. Most importantly, we will present a novel perspec-
tive focusing on flexibility and expressiveness, shedding
light on why prompts offer a promising solution to address
the limitations of existing graph representation learning
methods.
3.1 Notations
Let a graph instance denoted as G={V,E}, where V=
{v1, v2, . . . , v N}represents the node set containing Nnodes.
The edge set E ‚àà V √ó V describes the connection between
nodes. Each node viis associated with a feature vector
represented as xi‚ààRD. To characterize the connectivity
within the graph, we employ the adjacency matrix denoted
asA‚àà {0,1}N√óN, where the entry Aij= 1 if and only if the
edge (vi, vj)‚àà E.
3.2 Graph Representation Learning
The last decades have witnessed a notable surge in the
development of graph representation learning techniques.
These approaches can be broadly categorized into two main
branches: shallow embedding methods and deep graph
neural networks (GNNs). The shallow embedding approach
is centered on mapping nodes into lower-dimensional, learn-
able embeddings, enhancing their applicability in various
downstream tasks, as exemplified by node2vec [ 19] and
DeepWalk [ 63]. On the other hand, the deep GNNs maintain
the input node features as constants and optimize deep
graph model parameters for specific tasks, leading to more
expressive representation capabilities, as seen in methodssuch as Graph Convolution Networks (GCN) [ 56] and
GraphSAGE [21].
Shallow embedding approaches make input node features
learnable parameters, aiming at encoding nodes in a manner
that retains the original network‚Äôs similarity structure. Ac-
cording to node similarity definition, these methods can be
categorized as factorization-based [ 57,116] and random walk
approaches [ 19,63,94,93]. Despite the flexibility that shallow
embedding methods offer for various downstream tasks, they
are constrained by their inability to generate embeddings for
nodes not encountered during training. Additionally, these
approaches lack the capability to incorporate node features.
Therefore, more ‚Äúdeeper‚Äù methods, specifically those based
on graph neural networks, have been developed to address
these limitations.
Most deep GNNs follow a message-passing schema
and use a more complex encoder, resulting in powerful
expressiveness in graph representation. The representative
neural network structure is convolutional graph neural
networks (ConvGNNs), which comprise spectral [ 8,26] and
spatial methods [ 56,21,87,72,107]. While these methods
have exhibited remarkable capabilities in various graph-
based applications, their reliance on task-specific supervision
imposes constraints on their adaptability and generalizability,
particularly when dealing with tasks that have limited
labeled data.
In summary, shallow embedding methods offer flexi-
bility, preserving network structure and node content for
straightforward graph analytic tasks. However, they lack
expressiveness and the ability to encapsulate additional node
features. Conversely, GNNs provide more expressive graph
representations but require task-specific training data, limit-
ing their transferability. Hence, it calls for a graph learning
mechanism that combines expressiveness and flexibility. This
need led to the development of the pre-training and fine-
tuning paradigm.
3.3 Pre-training and Fine-tuning
To address the challenges of limited labeled data and
generalization issues in GNNs, the pre-training and fine-
tuning paradigm, thriving in the natural language processing

--- PAGE 7 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 7
Graph Neural Networks
√ºExpressivenessTask-specific SupervisionSpecific Downstream Task
ShallowNode Embedding (Nodeswithfreeparameters)¬∑¬∑¬∑Downstream Tasks
Node-levelEdge-levelGraph-level√ºFlexibility
Pre-trained Graph Model       Pre-training Graph Model
√ºExpressiveness
Pretext Task
√ºFlexibility across tasks/domains ?
Prompt Tuning for Downstream Tasks¬∑¬∑¬∑+
+(a) ShallowNode Embedding Methods(b) Deep Graph Neural Networks
(c) Comparison between fine-tune and promptFine-tuning for Specific TasksPre-trained Graph Model       
Node-level Prediction¬∑¬∑¬∑Pre-trained Graph Model       
Edge-level Prediction
e.g.Node Classification
Node-level PredictionGraph-level Prediction
tuned
frozenprompte.g.DeepWalk
Fig. 5: Our perspective of prompt upon flexibility and
expressiveness. (a) Shallow node embedding methods offer
flexibility across different downstream tasks but sacrifice
expressiveness. (b) GNNs provide expressiveness but require
task-specific supervision and constant node features, limiting
their flexibility. (c) Prompts enable a balanced approach,
achieving both flexibility and expressiveness.
(NLP) community, has gained widespread adoption in
graph representation learning. These approaches involve
pre-training models on large-scale graph data, with or
without labels, followed by fine-tuning model parameters for
diverse downstream tasks. This two-step process improves
model initialization, yielding broader optima and enhanced
generalization compared to training from scratch. Commonly
employed pre-training schemes include Graph AutoEncoders
(GAEs) [ 89], Masked Components Modeling (MCM) [ 30,68],
Graph Contrastive Learning (GCL) [ 88,76], etc. In Section 4,
we will delve into a detailed discussion of the pre-training
and fine-tuning method, offering a comprehensive picture.
3.4 A Brief History of Prompt Learning
Due to the growing number of model parameters, the con-
ventional pre-training and fine-tuning process is evolving
into a new approach termed pre-training, prompting, and
predicting [50]. In this paradigm, instead of manually adapt-
ing the pre-trained model for specific downstream tasks,
these tasks are reformulated to resemble those addressed
during the pre-training phase, aided by prompts. Prompts in
NLP take various shapes, including cloze prompts , which
complete textual strings like those used in masked language
models, and prefix prompts [ 40,43], where the input textprecedes the answer slot, as employed by autoregressive
language models. Some studies involve manually designed
templates based on human insights [ 2,70,71], while others
explore automated template learning. This includes searching
for templates in a discrete space [ 35,25,73,16] or conducting
prompting directly in the embedding space [ 43,40,86,65].
Such a paradigm enables a single pre-trained model to
address a multitude of downstream tasks in an unsupervised
manner, which has been widely demonstrated by large lan-
guage models. In light of this, the application of prompting
techniques in the context of graph-based tasks is currently
an area of active exploration.
3.5 Why Prompt? A New Perspective upon Flexibility
and Expressiveness.
Why is prompt learning promising for the graph domain?
An existing perspective that appears in most related work
is that prompt can reformulate downstream tasks to the
pre-training task, which might fill the gap between them.
This perspective is good but still not profound enough to
see the intrinsic difference from traditional fine-tuning. For
example, in a similar perspective, pre-training and fine-
tuning can be treated as using fine-tuning to reformulate
the pre-training task to the downstream task. It seems that
these two technique routines can both address the same
problem. Why the first choice is better than the second one?
In this section, we propose a new perspective, from this
view, we can further see the difference between prompting
and fine-tuning. As discussed in previous sections, existing
graph representation learning methods fail to achieve a satis-
factory trade-off between expressiveness and flexibility. Shal-
low graph embedding approaches offer flexibility as they can
be applied to a wide range of downstream tasks. However,
they sacrifice expressiveness due to limited parameterization
and the inability to incorporate original node features. Take
the DeepWalk model as an example, shallow graph methods
usually treat node representations as free parameters, which
is very flexible because each node can learn its individual
representations independently. However, for the reason of
gradient, they can not rely on more complicated networks
later, which might lose some expressiveness. Actually, there
are many more advanced works with deep graph layers
using the node representations from DeepWalk as their input
features as these node representations are very general in
various tasks. On the other hand, GNN-based methods treat
node embedding as constant features and seek to find a
powerful network for mapping node features to a specific
task, which is very expressive. However, the learned feature
transform pattern is applied to all the nodes, which means the
model can not treat each node embedding as free parameters,
and can not achieve as flexible results as the previous ones.
When we have multiple tasks, we usually need to train
different versions of the same GNN model, which is not as
flexible as the previous one.
With the above analysis, we can find that traditional fine-
tuning actually seeks to further improve the expressiveness
of a new task with the pre-trained graph model and can
not take care of node flexibility. Unlike fine-tuning, a graph
prompt usually has several tokens with free parameters,
which is very similar to shallow graph methods. In the

--- PAGE 8 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 8
Graph Pre-trainingTask-specific Fine-tuningTask-agnostic Prompting
Node-level
Contrastive MethodMasked Feature Regression ???Edge-levelGraph-levelÔºüAuxiliary Property Prediction 
Predictive Method??????
Fig. 6: Graph pre-training methods.
meantime, each node in the original graph has constant
features for GNN models. By inserting the prompt graph
to the original graph, the combined graph has both nodes
with constant features and tokens with free parameters. The
token parameters can be efficiently tuned, preserving node
flexibility. The combined graph is sent to a frozen pre-trained
GNN model to leverage the powerful expressiveness of deep
graph models.
In this paper, we argue that the prompting mechanism
offers a promising solution to address the limitations of
existing graph representation learning methods, effectively
balancing flexibility and expressiveness. Pre-trained GNNs
inherently possess knowledge of both structural and seman-
tic aspects, enabling the desired level of expressiveness. By
introducing prompts, we can seamlessly apply powerful pre-
trained models to diverse downstream tasks across various
domains in an efficient manner. This is achieved by aligning
the format of downstream tasks with that of pre-trained
tasks, thus leveraging the full potential of pre-trained models
even with minimal supervision signals. While the fine-tuning
mechanism can also facilitate domain or task adaptation of
pre-trained graph models, it often necessitates a considerable
amount of labeled information and requires exhaustive re-
training of the pre-trained model. In comparison, the prompt
mechanism offers a higher degree of flexibility and efficiency.
4 P RE-TRAINING GNN S FOR GRAPH PROMPTING
Graph pre-training is a pivotal step of the pre-training,
prompting, and predicting paradigm in graph representation
learning. This approach leverages readily available infor-
mation to encode the inherent graph structure, providing a
robust foundation for generalization across diverse down-
stream tasks. By integrating these pre-training methods into
the comprehensive workflow, we offer an exploration of their
interplay with the subsequent prompting and predicting
phases, shedding light on the strengths and limitations of
this holistic approach. This unique perspective distinguishesour survey, framing graph pre-training as an integral part of
the broader graph-prompting learning process. To better illus-
trate the motivation behind the prompting paradigm, we will
now delve into four distinct pre-training strategies within
the traditional pre-training and fine-tuning framework.
4.1 Node-level Strategies
Node-level pre-training strategies empower the acquisition
of valuable local structure representations that can be
transferred to downstream tasks. As shown in Figure 6,
these strategies encompass both contrastive and predictive
learning methods. In contrastive learning, self-supervised
signals typically result from perturbations in the original
graph structure or attributes, with the goal of maximizing
Mutual Information (MI) between the original and self-
supervised views. Noteworthy node-level contrastive meth-
ods include those presented in [ 6,123,37,62,33,95]. On
the other hand, predictive models focus on reconstructing
perturbed data using information from the unperturbed
data, as demonstrated in [ 21,27,89,61,92,28]. However, its
emphasis on partially semantic topology patterns restricts its
ability to capture higher-order information.
4.2 Edge-level Strategies
To enhance performance in tasks such as link prediction, di-
verse edge-level pre-training strategies have been developed.
These strategies excel at capturing node interactions and have
undergone extensive exploration. One approach involves
discriminating the presence of edges between pairs of nodes,
which can be regarded as contrastive methods [ 38,54].
Another approach focuses on reconstructing masked edges
by recovering the adjacency matrix [ 82,41,58,23,39]. Al-
though this pre-training strategy performs admirably in tasks
closely related to predicting node relations, it concentrates
solely on structural aspects, neglecting the portrayal of node
properties, and may encounter challenges when applied to
graph-level downstream tasks.
4.3 Graph-level Strategies
The necessity to improve graph-level representations for
subgraph-related downstream tasks has prompted the explo-
ration of various graph-level pre-training strategies. Similar
to node- and edge-level strategies, these approaches can be
broadly categorized into two main groups: graph reconstruc-
tion methods, involving the masking of graph components
and their subsequent recovery [ 104,68], and contrastive
methods focused on maximizing mutual information. These
contrastive methods target either local patches of node
features and global graph features [ 88,76,24,77,75], or
positive and negative pairs of graphs [ 111,81,84,66]. While
these approaches effectively encode global information and
generate valuable graph-level representations, a significant
challenge lies in transferring knowledge from a specific
pretext task to downstream tasks with substantial gaps,
potentially resulting in negative transfer [ 69]. This can limit
the applicability and reliability of pre-trained models and
potentially yield less favorable outcomes, even worse than
learning from scratch.

--- PAGE 9 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 9
4.4 Multi-task Pre-training
Multi-task pre-training accommodates multiple optimization
objectives, addressing a broad spectrum of graph-related
aspects to enhance generalization while mitigating negative
transfer issues. These objectives may encompass various
combinations, such as concurrent training of node attribution
reconstruction and structural recovery [ 30,31,115,13]. For
example, Hu et al. [ 30] pre-trained a GNN at both the
node and graph levels, enabling the GNN to learn valuable
local and global representations simultaneously. Furthermore,
some works have employed contrastive learning at different
levels [ 34,106], or joint optimization of both contrastive loss
and graph reconstruction error [ 42]. However, it is crucial to
recognize that multi-task pre-training approaches may face
optimization challenges, potentially resulting in suboptimal
performance across tasks. As a result, optimizing model per-
formance for each task while mitigating the negative transfer
problem remains a significant but unresolved concern.
4.5 Further Discussion
Fortunately, the prompting and predicting paradigm offers a
robust solution to the challenges mentioned above. This ap-
proach can fully exploit model performance and seamlessly
integrate with advanced GNN architectures. Instead of adapt-
ing pre-trained GNNs to downstream tasks through objective
engineering, this paradigm reformulates downstream tasks
into those solved during the pre-training phase using a graph
prompt. This innovative strategy effectively bridges the gap
between pretext and downstream tasks while sidestepping
suboptimal performance pitfalls. Furthermore, in compar-
ison to traditional fine-tuning approaches, the prompting
paradigm showcases remarkable flexibility, enabling it to
excel in scenarios demanding few-shot or even zero-shot
learning, where adapting to new contexts with limited or no
labeled data is paramount. In the current landscape marked
by surging model volumes and an ever-expanding array of
downstream tasks, the ascent of the prompting paradigm
represents an irresistible and transformative trend.
5 P ROMPT DESIGN FOR GRAPH TASKS
In this section, we propose a unified view for the graph
prompt. As shown in Figure 7, the graph prompt should con-
tain at least three components: prompt tokens with prompt
vector; token structures preserving inner correlations of these
tokens; and inserting patterns indicating how to integrate the
original graph with prompts. Beyond these details, we are
particularly interested in the following questions: Question 1:
How do these works design the graph prompt? Question 2:
How do these works reformulate downstream tasks to the
pre-training tasks? Question 3: How do these works learn
an effective prompt? and Question 4: What are the inner
connections of these works, their advantages and limitations?
With these questions, we summarize the most representative
works published recently and present them in Table 2.
5.1 Prompt Token, Structure, and Inserting Pattern
...........Question...1:......How...do.......They.........Design...A........Graph...........Prompt?
A. Prompt as Tokens. The simplest graph prompt can
be treated as some additional features added to the original
PromptGraphOriginal GraphFourKindsofInsertingPatternsPromptedGraph
ByCrossLinksByFeatureAddingPromptedGraph
PromptedGraph
ByConcatenatingByMultiplicationPromptedGraph
TokenfeaturePromptTokenTokenStructure
NodefeatureFig. 7: Prompt Tokens, Structures, and Inserting Patterns.
graph features [ 125]. For example, given a graph feature
matrixX={x1,¬∑¬∑¬∑, xN} ‚ààRN√ódwhere xi‚ààR1√ódis the
feature of i-th node and dis the dimension of the feature
space. Fang et al. [11] and Shirkavand and Huang [74] treat
the basic prompt as a learnable vector p‚ààR1√ód, which can
be added to all node features and make the manipulated
feature matrix be X‚àó={x1+p,¬∑¬∑¬∑, xN+p}. In this
way, we can use the reformulated features to replace the
original features and process the graph with pre-trained
graph models. A later work [ 12] further extends one prompt
token to multiple tokens and makes the performance better.
PGCL [ 18] design a prompt vector for semantic view and
another prompt vector for contextual view, then they add
these prompt vectors to the graph-level representations by
element-wise multiplication. A similar prompt design is also
adopted in VNT [ 83]. The difference is that their inserting
pattern does not add the prompt token to the original
graph feature but concatenates the prompt token with the
original node set and tries to integrate them by the self-
attention function in a graph transformer. In GraphPrompt
[52], the prompt token is similar to the format defined
by Fang et al. [11]. The difference thing is that previous
work designed the prompt tokens in the initial feature
space, while this method assumes the prompt in the hidden
layer of the graph model. Usually, the hidden size will be
smaller than the original feature, making their prompt token
shorter than the previous one. Another different thing is
that the graph prompt here is used to assist graph pooling
operation (a.k.a Readout ). For example, given the node set
V={v1,¬∑¬∑¬∑, v|V|}, the embedding of node vishv‚ààR1√ód,
a prompt token pt‚ààR1√ódspecified to task tis inserted
to the graph nodes by the element-wise multiplication ( ‚äó):
st=Readout ({pt‚äóhv:v‚àà V} ). Similarly, SGL-PT [ 124]
creates a prompt token to connect to all nodes in the graph.
The prompt token preserves a global perception of the graph
and can assist their global branch of the pre-training tasks,
which can be also treated as a pooling strategy. Aiming at
aligning node classification and link prediction, GPPT [ 78]
defines graph prompts as additional tokens that contain task
tokens and structure tokens. Here the task token refers to
the description of the downstream node label to be classified
and the structure token is the representation of the subgraph
surrounding the target node. By this means, predicting node

--- PAGE 10 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 10
TABLE 2: Summary of existing representative works on graph prompt. S: Subgraph. V(S): Node set within subgraph S.œÄ:
Pre-trained parameters. œï: Task head parameters. Œ∏: Prompt parameters. Àús: Filled prompt.
pre-training task prompt design downstream tasks answering function
Paper node edge graph prompt components inserting pattern prompt tuning node edge graph Preset Learnable
GPPT
(KDD 2022 [78])‚úó ‚úì ‚úóstructure token:
sv‚ààRd
task token:
cy‚ààRdsvi‚ÜêfŒ∏(vi)
Àúsy,vi‚Üê[cy,svi]Cross Entropy ‚úì ‚úó ‚úó ‚úì ‚úó
GPF
(arXiv [11])‚úì ‚úì ‚úì prompt feature p‚ààRdÀúsi‚Üêxi+pmax p,œïP
(yi,Àúsi)
pœÄ,œï(yi|Àúsi)‚úó ‚úó ‚úì ‚úó ‚úì
All in One
(KDD 2023 [80])‚úó ‚úó ‚úìprompt token:
P={p1, ...,p|P|}
token structure:
{(pi,pj)|pi,pj‚àà P}wik‚ÜêœÉ(pk¬∑xT
i)
ifœÉ(pk¬∑xT
i)> Œ¥ else0
Àúsi‚Üêxi+P|P|
k=1wikpkMeta-Learning ‚úì ‚úì ‚úì ‚úì ‚úì
GraphPrompt
(WWW 2023[52])‚úó ‚úì ‚úóprompt token:
pt‚ààRd, t‚àà T
structure token: s‚ààRd
task token: cy‚ààRdÀúst
i‚ÜêReadout ({pt‚äôfœÄ(v)|
v‚ààV(Si)})
cy‚ÜêMean ({Àúst
j|yj=y})min pt‚àíP
(yi,Si)ln
exp( sim(Àúst
i,cyi)/œÑ)
P
y‚ààYexp( sim(Àúst
i,cy)/œÑ)‚úì ‚úó ‚úì ‚úì ‚úó
PGCL
(arXiv [18])‚úó ‚úó ‚úìsemantic token: ps‚ààRd
contextual token: pc‚ààRdzps
x=zs
x‚äôps
zpc
x=zc
x‚äôpcmin‚àíP
(v,a,b )‚ààTlog
exp(sim(zp
v,zp
a)/œÑ)P
u‚àà{x,y}exp(sim(zp
v,zp
u)/œÑ)‚úì ‚úì ‚úì ‚úì ‚úó
PRODIGY
(NeurIPS 2023 [32])‚úó ‚úì ‚úódata graph:
GD‚àº ‚äïk
i=1N(Vi,G)‚ü©
task graph: GTÀúsi‚ÜêfT
œÄT(GT|fD
œÄD(GD)) Fixed ‚úì ‚úì ‚úì ‚úì ‚úó
SGL-PT
(arXiv [124])‚úì ‚úó ‚úìprompt token:
one vector for each graphconnect to all nodes in the graphcontrastive loss and
reconstruction loss‚úì ‚úó ‚úó ‚úì ‚úó
GPF-Plus
(NeurIPS 2023 [12])‚úì ‚úì ‚úìprompt features
p1,¬∑¬∑¬∑,pk‚ààRdÀúsi‚Üêxi+œÉ(p1,
¬∑¬∑¬∑,pk)max p,œïP
(yi,Àúsi)
pœÄ,œï(yi|Àúsi)‚úó ‚úó ‚úì ‚úó ‚úì
DeepGPT
(arXiv [74])‚úì ‚úì ‚úìprompt token:
p‚ààRd
prefix token:
P‚ààR|P|√ódÀúxi‚Üêxi+p
Àúsi‚ÜêfP,œÄ(G,Àúxi)min p,P,œïP
(yi,vi)
L(pœï(Àúsi), yi)‚úó ‚úó ‚úì ‚úó ‚úì
ULTRA-DP
(arXiv [4])‚úó ‚úì ‚úóprompt token:
pi=ptask+wposppos
i,
ppos
idenotes vi‚Äôs
positional embeddingcreate a virtual node
vp
ifor target node vi,
G‚Ä≤‚Üê(V ‚à™ { vp
i},
E ‚à™ { (vp
i, vi)},X‚à™ {pi})Multi-task-Learning ‚úì ‚úó ‚úó ‚úó ‚úì
HetGPT
(arXiv [55])‚úó ‚úì ‚úóprompt token:
F={fA
i}K
i=1
forA‚àà A
task token: cy‚ààRdÀúsA
i‚ÜêxA
i+PK
k=1wikfA
k,
zi=fœÄ(G,ÀúsA
i)min C,F‚àíP
(yi,vi)log
exp(sim(zi,cyi)/œÑ)P
y‚ààYexp(sim(zi,cy)/œÑ)‚úì ‚úó ‚úó ‚úì ‚úó
SAP
(arXiv [17])‚úì ‚úó ‚úótask token:
P={cy}y‚ààY
structure token:
W={(vi, cj)}vi‚ààV,cj‚ààPG‚Ä≤‚Üê(V ‚à™ P ,E ‚à™ W )
Z(1)=MLP œÄ‚Ä≤(X)
Z(2)=GNN œÄ‚Ä≤‚Ä≤([X,P],
[A,W])min W‚àíP
(yi,vi)log
exp(sim(z(1)
i,z(2)
yi)/œÑ)
P
y‚ààYexp(sim(z(1)
i,z(2)
y)/œÑ)‚úì ‚úó ‚úì ‚úì ‚úó
VNT
(KDD 2023 [83])‚úì ‚úì ‚úóP= [p1;. . .;pP],
pp‚ààRF
E1‚à•Z1
=L1 
E0‚à•P
‚ààR(V+P)√óFCross Entropy ‚úì ‚úó ‚úó ‚úó ‚úì
v‚Äôs label can be reformulated to predict a potential link
between node v‚Äôs structure token and the label task token.
Aiming at the node classification task, ULTRA-DP [ 4] creates
a prompt token for each target node, where the token feature
is the weighted sum of position embedding of the target node
and a task embedding of the pre-training task. HetGPT [ 55]
design a prompt with node tokens and class tokens, which
are organized in a similar way to GPPT. The difference is that
they also add a type-specific feature token to make graph
prompts sensitive to different node types, by which they can
extend existing graph prompts to heterogeneous graphs.
B. Prompt as Graphs. The graph prompt in All in One
[80] is an additional subgraph that can be learned by efficient
tuning. The prompt tokens are some additional nodes that
have the same size of node representation as the original
nodes. They assume the prompt tokens should be in the
same semantic space as the original node features so that
we can easily manipulate node features with these tokens.
The token structures include two parts. The first part is the
inner links among different tokens, and the second part is
the cross-links between the prompt graph and the original
graph. These links can be pre-calculated by the dot product
between one token to another token (inner links) or one
token to another original node (cross links). The inserting
pattern is to add the prompt graph to the original graphby the cross-links and then treat the combined graph as
a new graph, and send it to the pre-trained graph model
to get the graph-level representation. The prompt graph
in PRODIGY [ 32] includes data graphs and a task graph.
The data graphs can be treated as subgraphs surrounding
the target nodes (for node classification task), node pairs
(for edge classification task), or just denoted as the graph
classification instance. Here the prompt tokens and prompt
structures are just the same as in the original graph. The
task graph contains data tokens and label tokens where
each data token connects to one data graph and is further
connected by label tokens. Unlike previous works that aim
at reformulating downstream tasks to the pre-training task
by prompting the downstream data, PRODIGY leverages the
prompt graph to unify all the upstream and downstream
tasks. Their pre-training strategy is a set of tasks including
neighboring matching and label matching, which can be
reformulated as predicting the similarity between the data
token and the label token in the prompt graph. SAP [ 17]
also connects several prompt tokens (each token corresponds
to one class) to the original graph by cross-links defined in
All in One. The difference is that their prompting task is a
node-level contrastive task, in which they use MLP to encode
the node features as the first view and they use a GNN to
encode the prompted graph as the second view, which is

--- PAGE 11 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 11
consistent with their pre-training task.
5.2 Aligning Tasks by Answering Function
...........Question...2:.......How....do......they..............reformulate................downstream.......tasks...to.....the
.........pretext?
A. Handling Different Level Tasks. All in One [ 80] pre-
trains the graph model via graph-level contrastive learning.
The pre-training task aims to learn a robust graph encoder
over different graph views generated by the perturbations of
the original graph. To reformulate various graph tasks to this
graph-level pretext, they first unify node-level, edge-level,
and graph-level tasks to the graph-level task by induced sub-
graphs, which are also introduced in PRODIGY [ 32,52,18].
Then they claim that the graph prompt added to the original
graph is in nature the simulation of any graph operations
such as node feature masking, node or edge perturbations,
subgraph removing, etc. With this opinion, we just need
to add an appropriate graph prompt to downstream graph
datasets then it will be promising to further reformulate the
downstream task to the pretext.
B. Learnable Answering Function. To output the results
of downstream tasks, Sun et al. [80] design two types of
answering functions. The first one is a learnable task head
(such as an MLP mapping function) that can be easily tuned
with very limited data. It takes the graph-level representation
generated by the pre-trained graph model and then outputs
the downstream result. For example, if the downstream
is a three-class node classification, we can simply use a
dense layer with three hidden units to connect the graph
representation, which is generated by the pre-trained model
on the combined graph with node included graph and a
graph prompt. In this case, both the graph prompt and the
task head are tunable, so we can adjust them alternately.
Similar learnable answering functions are also adopted in
other works like [ 11,12,83]. The good point is that they are
very easy to align two tasks, however, it also increases the
tuning workflow.
C. Hand-crafted Answering Function. To further reduce
the tuning burden, All in One [ 80] also proposes a second
answering function, which is hand-crafted without any
trainable task head. For example, for a node classification
task, we can set up Kunique sub-prompts, each aligning
with a different node type, where Krepresents the total
number of node categories. If the pre-training involves a
task like GraphCL [ 111], which aims to maximize similarity
at the graph level between pairs of graph views, then the
target node can be classified with label ‚Ñì,(‚Ñì= 1,2,¬∑¬∑¬∑, K)
if the ‚Ñì-th graph most closely resembles the original node-
inclusive graph. Similarly, GPPT [ 78] and HetGPT [ 55] use
link prediction as their pre-training task and reformulate
downstream node classification by unifying it as the same
task template. For example, by treating the node label as an
additional token, we can use the pre-trained model to directly
output the possibility of an edge between the label token
and the target node. The pre-training strategy in SAP [ 17]
is to compare node representations from two graph views,
the first of which is generated by node feature encoding and
the second of which is encoded by a graph model. To this
end, their prompt tokens denote class information and they
compare node representation with each class token to findthe class with the largest similarity as the inference results.
By designing a unified task template, PRODIGY [ 32] uses a
hand-crafted graph prompt to describe all node, edge, and
graph classification tasks by predicting the link between
data tokens and label tokens. Liu et al. [52] extend the link
prediction task as graph pair similarity and treat it as their
pre-training task, to align the downstream node classification
and graph classification task, they design a unified answering
template making the downstream side aligned with the pre-
training side. Specifically, given a triplet of induced graphs
(g1, g2, g3)where g1andg2have the same label, g1andg3
have different labels. In particular, when the target task is
node classification, the induced graph refers to the contextual
subgraph of the target node. The unified answering template
is defined as sim (g1, g2)>sim(g1, g3).
5.3 Prompt Tuning
...........Question...3:......How...do.......They.......Learn...A........Graph...........Prompt?.
A. Meta-Learning Technique. To learn appropriate
prompts, Sun et al. [80] leverage meta-learning techniques
(such as MAML[ 15] model) to obtain a robust starting
point for the prompt parameters. Since the support set
and query set include various graph tasks (such as node
classification, link prediction, graph classification, etc), the
learned graph prompt is expected to be more general on
various downstream tasks.
B. Task-specific T uning. Besides All in One [ 80], which
aims to learn a general prompt on various downstream tasks,
there are also some works that target specific downstream
tasks such as graph classification. In this case, the prompt
tuning can be more task-directed. For example, GPF [ 11]
aims at a graph classification task, so it just needs to tune
the prompt token pand the task head œïby maximizing
the likelihood of predicted correct graph labels given the
prompted graph representation Àúsifrom the pre-trained graph
model œÄ. In this case, the task head tuning and the prompt
tuning share the same objectives, which can be formulated
by:max p,œïP
(yi,Àúsi)pœÄ,œï(yi|Àúsi).
C. T uning in Line with Pretext. Intuitively, prompting
aims at reformulating downstream tasks to the pre-training
task. Therefore, it would be more natural if the prompt
tuning shares the same objective with the pre-training task.
As suggested in GraphPrompt [ 52], the authors use a similar
loss function to learn prompts. Similarly, GPPT [ 78] and VNT
[83] adopt the same loss function (Cross-Entropy) as their
link prediction and node classification tasks, respectively.
HetGPT [ 55] and SAP [ 17] use a node-level contrastive loss
to learn their prompt tokens because their pre-training task
is also conducted by the same contrastive task (node pair
comparison). PGCL [ 18] introduces graph-level loss to align
with the pre-training task. ULTRA-DP [ 4] develop two pre-
training tasks including edge prediction and neighboring
prediction, each of which corresponds to one task embedding.
In the pre-training phase, they randomly select a task and
then integrate specific task-related embeddings into the
prompt tokens. These learnable task embeddings are then
trained with the graph model.
5.4 Further Discussion
...........Question...4:.......What....are.......Their................Connections,......Pros.....and........Cons?

--- PAGE 12 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 12
The good point of GPF [ 11] is that they propose a very
simple prompt that can be easily used in various pre-training
tasks and downstream tasks. However, a single prompt
token added to all nodes is very limited in expressiveness
and generalization. A potential solution is to learn an
independent prompt token for each node, which means one
node corresponds to one prompt token, but this will cause
low efficiency in parameters. To this end, we can train K-
independent basis vectors and use them to compound each
node token (GPF-Plus [ 12]). This improvement makes their
work have more similar insights with All in One [80].
HetGPT [ 55] extends prompt tokens to type-specific for-
mat, which can deal with graph prompting in heterogeneous
data. However, they can only deal with node classification
tasks. To this end, GraphPrompt [ 52] reformulates link
prediction to graph pair similarity task. It is worth noticing
that the role of their prompt token is very similar to the
project vector in the graph attention network. There are also
some attention-based graph-pooling methods, which share
the same motivation. The difference is that the authors claim
the graph-pooling component in the pre-training stage might
not fit other downstream tasks, thus needing additional
prompts to redirect the graph-pooling component in the
graph model.
GPPT [ 78] represents a specific instance within the
broader framework of All in One [ 80]. For instance, if we
minimize the prompt graph to isolated tokens that correlate
with node classes and substitute the resulting graphs with
a complete graph, the All in One prompt structure can
be simplified into the GPPT format. This allows for the
utilization of edge-level pretexts in node classification tasks
within the GPPT framework. The shortcoming of GPPT
might be that it is restricted to binary edge prediction pretexts
and is solely effective for node classification in downstream
tasks. In comparison, frameworks like GraphPrompt and All
in One are designed to accommodate a wider array of graph-
related tasks, extending beyond just node-level classification.
The good point is that when adapting models for different
tasks, GraphPrompt, GPF, and GPF-Plus often require the
tuning of an extra task-specific module. In contrast, All in
One, and GPPT utilize task templates that focus more on the
manipulation of input data and are less dependent on the
specifics of downstream tasks.
Intuitively, the data graphs, one of the components in
PRODIGY [ 32], are very similar to the induced graph in
All in One and GraphPrompt. The pre-training task in
PRODIGY can be seen as predicting a link between the
data token and the label token, which shares a similar insight
with GPPT. The good thing is that their prompts have no
trainable parameters, which means they do not need to
tune the prompt and are more efficient in the in-context
learning area. PRODIGY does not need any tuning work
and can be used in knowledge transferring between different
datasets. However, a non-tunable prompt is usually not
flexible enough, which might also limit the generalization
of the pre-trained model when the downstream tasks to be
transferred are located in a different domain from the pre-
training one. In contrast, ULTRA-DP [ 4] tune prompt both in
the pre-training stage and the downstream tasks. It first put
the prompt tuning work in the pre-training stage to obtain
the task embeddings, which are one of the main componentsin their prompt. Then they use these task embeddings to
initialize a downstream prompt. Intuitively, their prompts
are not used to reformulate downstream tasks to the pretext.
Instead, these prompt tokens are used to select suitable pre-
training tasks from a task pool to fit the downstream task. It
is still an interesting question of how to achieve the optimal
balance given efficiency, generalization, and the flexibility of
prompt.
Compared with other works that usually define clear
inserting patterns, VNT [ 83] concatenates prompt tokens
with the original node set and then puts all of them into
the graph transformer. Actually, the graph transformer will
leverage a self-attention function to further calculate the
correlations among them, which can also be treated as a
variant of inserting patterns defined in All in One. The good
thing is that we do not need to design a threshold to tailor the
connection but the shortcoming is that it can only use a graph
transformer as its backbone and can not applied to more
message-passing-based graph models. In addition, there are
also some more advanced variants of graph transformers
requiring additional position embedding as one of their input.
However, the prompt tokens in VNT have no clear inserting
links to the original graph, which might not make it easy to
apply existing position encoding approaches for these graph
transformer variants.
6 G RAPH PROMPTING IN MULTI-MODAL AND
MULTI-DOMAIN AREAS
6.1 Multi-Modal Prompting with Graphs
The fusion of images, sound, and text has been widely
studied and achieved remarkable success. However, most of
these modalities are described by linear data structure. In
our real-world life, there are more kinds of data in non-linear
structures like graphs. How to connect these linear modalities
(e.g. text, images, sound, etc) to the non-linear modalities (e.g.
graphs) has become a very attractive research topic because
it is a bigger move towards artificial general intelligence.
Unfortunately, reaching this vision is very tough. Currently,
we only see some hard progress in the fusion of text and
graphs, especially in the text-attributed graphs. With the help
of recent large language models, the fusion of text and graph
has achieved even more notable performance. Since there
have already been some informative surveys on this topic,
we next briefly discuss some representative works that are
closely related to........prompt.............techniques . We suggest readers refer
to the mentioned literature [ 46,48] to require more detailed
information further.
A. Prompt in Text-Attributed Graphs. Wen and Fang
[97] ventured into enhancing text classification in scenarios
with limited data resources by the proposed model, Graph-
Grounded Pre-training and Prompting (G2P2). Their work
identifies the issue of insufficient labeled data in supervised
learning and proposes a solution leveraging the inherent
network structure of text data, such as hyperlinks or citation
networks. G2P2 utilizes text-graph interaction strategies
with contrastive learning during the pre-training phase,
followed by an inventive prompting technique during the
classification phase, demonstrating notable effectiveness in
zero- and few-shot text classification tasks. Zhao et al. [121]
focused on molecule property prediction, a field grappling

--- PAGE 13 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 13
with the scarcity of labeled data. Their study introduces
an integrated graph-text model to enhance prompt-based
molecule task learning in a zero-shot context. This model
employs generalized position embedding and decouples
encoding of the graph from task prompt, enhancing its
generalization capability across novel tasks. Li and Hooi
[45] explored node classification within the framework of
multi-modal data (text and graph), particularly focusing on
limited-label scenarios. Unlike traditional works that usually
feed pre-computed text features into graph neural networks,
they incorporate raw texts and graph topology by a hand-
crafted language prompt template into the model design.
B. Large Language Models in Graph Data Processing.
Chen et al. [5]explored the potential of Large Language
Models (LLMs) in graph node classification tasks. They inves-
tigated two pipelines: LLMs-as-Enhancers, which enhances
node text attributes using LLMs followed by predictions via
Graph Neural Networks (GNNs), and LLMs-as-Predictors,
which directly employs LLMs as standalone predictors. Their
empirical evaluations revealed that deep sentence embed-
ding models and text-level augmentation through LLMs
effectively enhance node attributes, while LLMs also show
promise as standalone predictors, albeit with concerns about
accuracy and test data leakage. Fatemi et al. [14] conducted a
comprehensive study on encoding graph-structured data for
consumption by LLMs. Their findings include the influence
of the graph encoding method, the nature of the graph task,
and the structure of the graph on LLM performance. They
demonstrated that simple prompts are most effective for
basic graph tasks and that graph encoding functions sig-
nificantly impact LLM reasoning. Their experimental setup
introduced modifications to the graph encoding function,
revealing improvements in performance and demonstrating
the effect of model capacity on graph reasoning ability. Jin
et al. [36] introduced an innovative approach to pre-train
language models on networks rich in text. Their framework,
named PATTON, focuses on integrating the intricacies of
textual attributes with the underlying network structure.
They developed two novel pretraining strategies: one concen-
trating on the context within networks for masked language
modeling, and the other on predicting masked nodes, thus
capturing the interplay between text and network structure.
The effectiveness of PATTON was validated through various
experiments, showcasing its superior performance over
traditional text/graph pretraining methods in diverse tasks
such as document classification, retrieval, and link prediction
in different domain datasets. This approach signifies a
shift in pretraining methodologies, emphasizing the synergy
between textual data and network context. Wang et al. [96]
proposed a Knowledge Graph Prompting (KGP) method
to enhance LLMs for multi-document question answering
(MD-QA). They created a knowledge graph over multiple
documents, with nodes representing passages or document
structures and edges denoting semantic/lexical similarity.
The LM-guided graph traverser in KGP navigates the graph
to gather supporting passages, aiding LLMs in MD-QA. Their
experiments indicated that the construction of KGs and the
design of the LM-guided graph traverser significantly impact
MD-QA performance.
C. Multi-modal Fusion with Graph and Prompting. The
integration of multi-modal data using graph and promptingtechniques has seen remarkable progress in recent years.
For example, Edwards et al. [10] propose SynerGPT in the
field of drug synergy prediction. This model leverages a
transformer-based approach, uniquely combining in-context
learning with genetic algorithms to predict drug synergies.
In the area of vision-language models, Li et al. [44] develop
GraphAdapter, a prompt-based strategy that utilizes an
adapter-style tuning mechanism, bringing together textual
and visual modalities through a dual knowledge graph. Liu
et al. [49] extend work multi-modal fusion into molecular
science with their proposed GIT-Mol. A large language model
integrates graph, image, and textual data with the help
of prompt, offering substantial improvements in various
tasks like molecule generation and property prediction.
Although much effort has been made in the past few years,
the academic is still trying hard to find better solutions
to integrate text and graphs via text-attributed graphs or
knowledge graphs. There is still a very large imagination in
the fusion of more kinds of modalities.
6.2 Graph Domain Adaptation with Prompting
The field of graph domain adaptation has seen significant
advancements, particularly with the integration of prompting
techniques. However, graph domain adaptation is still not
a well-solved problem because there exist at least two
challenges: The first one is how to align semantic spaces
from different domains. The second one is how to identify
structural differences.
A. Semantic Alignment. In particular, All in One [ 80]
extends the ‚Äúpre-training and fine-tuning‚Äù workflow with
multi-task prompting for GNNs, unifying prompt formats,
and introducing meta-learning for prompt optimization. To
make the graph model adaptive to different graph domains,
they first reveal that the graph prompt in nature can be
seen as graph operation and then they use graph prompt to
manipulate different domain graph datasets. GraphControl
[125] introduces a unique deployment module inspired
by ControlNet, effectively integrating downstream-specific
information as conditional inputs to enhance the adaptability
of pre-trained models to target data. This approach aligns
input space across various graphs and incorporates unique
characteristics of the target data. Zhang et al. [119] presents
a pre-training model for knowledge graph transfer learning.
This model uses a general prompt-tuning mechanism, treat-
ing task data as a triple prompt, enabling flexible interactions
between task KGs and task data. Yi et al. [110] combines
personalized graph prompts with contrastive learning for
efficient and effective cross-domain recommendation, par-
ticularly in cold-start scenarios. A representative work is
proposed by Liu et al. [47], in which they describe graph
nodes from different domains by language and then use
LLM to get a textual embedding. However, this work needs
the semantic name of each feature while sometimes graph
features are usually latent vectors without clear semantic
meaning.
B. Structural Alignment. Cao et al. [3]analyze the
feasibility between different graph datasets and found
that the structural gap holds the upper bound of various
graph model transferability. GraphGLOW [ 122] addresses
the limitations of existing models that operate under a closed-
world assumption, where the testing graph is identical to

--- PAGE 14 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 14
the training graph. This approach often leads to prohibitive
computational costs and overfitting risks due to the need
for training a structure learning model from scratch for each
graph dataset. To this end, it coordinates a single graph-
shared structure learner with multiple graph-specific GNNs
to capture generalizable patterns of optimal message-passing
topology across datasets. The structure learner, once trained,
can produce adaptive structures for unseen target graphs
without fine-tuning, thereby significantly reducing training
time and computational resources. AAGOD by Guo et al.
[20] proposes a data-centric framework for OOD detection in
GNNs. They use a parameterized amplifier matrix, which is
treated as a prompt, to superimpose on the adjacency matrix
of input graphs. Inspired by prompt tuning, Shirkavand
and Huang [74] propose DeepGPT for graph transformer
models. They add prompt tokens to the input graph and each
transformer layer. By updating the prompt tokens, they can
efficiently tune a graph model on different graph datasets.
7 P OTENTIAL APPLICATIONS
With the widespread utilization of networks as a data model-
ing structure for representing diverse relational information
across social, natural, and academic domains, the graph
prompt mechanism exhibits substantial potential for a wide
range of real-world applications. In this section, we explore
the potential applications of graph prompting in online social
networks, recommender systems, knowledge management,
and biology.
Online Social Networks. Online social platforms consist
of users who can be represented as nodes, and their social
connections form online social networks (OSNs). Previous
research has investigated the potential of prompting mecha-
nisms in identifying fake news within OSNs to prevent mali-
cious attacks [ 101]. Specifically, they employ textual prompts
applied to pre-trained language models (PLMs) to distill
general semantic information. By combining this semantic
signal with the dynamics of information propagation within
social networks, improved classification performance can
be achieved. While the use of tailored textual prompts for
PLMs has been studied, the application of graph prompting
mechanisms within social networks is still under-explored.
In the future, it is promising to directly apply prompt tuning
techniques to social networks, utilizing few-shot labels for
tasks such as fake news detection or anomaly detection
[99,20], where the labeling process is laborious and requires
domain expertise. By incorporating prompts directly within
social networks, this approach can address the scarcity of
labeled data and enhance the security and trustworthiness
of online social networks.
Recommender Systems. E-commerce platforms provide
a valuable opportunity to leverage recommender systems
for enhancing online services. While prompt tuning in rec-
ommender systems has received limited research attention,
it holds significant potential [ 110,109,103,22]. In [ 110],
the graph prompt tuning technique is applied to cross-
domain recommendation scenarios to address the challenges
of domain adaptation. Specifically, when applying a pre-
trained recommendation model to the target domain, extra
prompt nodes are introduced to achieve both efficient and
effective domain recommendation. Meanwhile, Yang et al.[109] propose personalized user prompts to bridge the gap
between contrastive pretext [ 100,112] to downstream recom-
mendation task. They design different kinds of personalized
prompts, in combination with pre-trained user embeddings
to facilitate dynamic user representations, leading to more
accurate and personalized recommending results. In the fu-
ture, further exploration into the integration of graph prompt
tuning within recommender systems can be conducted to
enhance recommendation performance, personalization, and
adaptability across different domains.
Knowledge Management. There are two branches of
research focused on performing prompting on knowledge
graph (KG) for improved knowledge management. The first
branch involves direct prompting on knowledge graphs
using a pre-trained KG model to facilitate knowledge transfer,
enabling better generalization across different KG data and
tasks [ 119]. The second branch explores the combination of
grounded knowledge from KGs with the cognitive abilities
of LLMs [ 96,85,67,60,118,59] to improve performance
in downstream tasks. In the first research line, Zhang et al.
[119] proposed a structure pre-training and prompt tuning
approach to realize knowledge transfer. They designed
specific pre-training objectives to obtain a powerful KG
model. Subsequently, a general prompt tuning technique
was employed to facilitate knowledge transfer between task-
specific KGs and data. In the second research line, prompt
tuning techniques are adopted to combine the grounded
knowledge inherent in KGs with LLMs for enhancing
downstream tasks. For example, in [ 85], a novel graph
neural prompting method was introduced to adapt KGs for
LLMs by distilling valuable knowledge from KGs in a time-
and parameter-efficient manner. Future research can further
explore this trend by implementing more efficient graph
prompting methods to fully distill beneficial knowledge from
KGs to assist LLMs on diverse downstream tasks.
Biology. Molecules can be represented naturally as
graphs, where atoms serve as nodes and chemical bonds
act as edges [ 68]. Such graph modeling provides a basis
for applying graph representation learning methods to
perform tasks such as molecular property prediction, thereby
benefiting scientific research and discovery [ 68,53,121,10].
Previous research on graph representation learning in the
molecular domain followed a task-specific approach. It
involved training individual models tailored to specific
molecule datasets [ 68,76], lacking the generalization ability
within the domain. Though there exist works that explored
the utilization of LLMs (or LMs) as a universal tool for
understanding molecules [ 64,47], these approaches primarily
rely on regular texts to describe molecules, overlooking the
inner graph structures within molecules [ 120]. Meanwhile,
some recent works have focused on investigating the co-
modeling of graphs and languages to preserve both structural
dependencies and achieve generalization abilities across
tasks and datasets, even under few-shot or zero-shot settings
[49,121,10]. For instance, in [ 49], the authors employed
LoRA [ 29] to efficiently adapt to downstream tasks. Follow-
ing this research direction, we believe that graph prompting
techniques can also be adopted to achieve more efficient task
adaptation within the molecular domain.

--- PAGE 15 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 15
Pre-processingFeature EngineeringDataPromptingEvaluationData LoaderModel BackboneGCNGATTask LevelPrompting MethodGraph TransformerNode-levelEdge-levelGraph-levelAll in OneGPPTGPFGPF_plusGraphPrompt‚Ä¶ ‚Ä¶Component MaskingSamplerUtilsLossGraphSAGE‚Ä¶ ‚Ä¶ConfigurationConf Files Pre-trained ModelDemoMultiple TaskMeta-learningComprehensive MetricsBatch EvaluatorDynamic Dispatcher
Fig. 8: The architecture of ProG .
8PR OG: A U NIFIED LIBRARY FOR GRAPH PROMPT -
ING
An indispensable component for fortifying the graph prompt-
ing ecosystem is a well-crafted tool. Despite the plethora of
tools proposed for generalized graph learning, a notable
absence persists in the realm of libraries dedicated to graph
prompt functionalities. Addressing this gap, we are introduc-
ingProG (Prompt Graph), an open-source, unified library
meticulously designed to cater to the specific needs of graph
prompting. This initiative promises to significantly enhance
the landscape of graph-based applications by providing a
versatile and comprehensive resource for researchers and
practitioners alike.
ProG is a PyTorch-based library designed to facilitate
single or multi-task prompting for pre-trained GNNs. The
architecture is illustrated in Figure 8. It seamlessly integrates
several widely used datasets in the graph prompt evaluation,
including Cora, CiteSeer, Reddit, Amazon, and Pubmed etc.
The tool is equipped with essential evaluation metrics such as
Accuracy, F1 Score, and AUC score, commonly employed in
various graph prompt-related tasks. Notably, ProG incorpo-
rates state-of-the-art methods like All in One [ 80], GPPT [ 78],
GPF [ 11], and GPF-Plus [ 12], and it continues integrating
more graph prompt models. In summary, ProG offers the
following key features:
‚Ä¢Quick Initiation. All models are implemented within
a consistent environment, accompanied by detailed
demos, ensuring a swift initiation for newcomers.
‚Ä¢Fully Modular. ProG adopts a modular structure,
empowering users to customize and construct models
as needed.
‚Ä¢Easy Extendable. ProG is designed for seamless
extension to encompass additional methods and a
broader spectrum of downstream tasks, adapting to
evolving research needs.
‚Ä¢Standardized Evaluation. ProG establishes a uniform
set of evaluation processes, promoting equitable
performance comparisons across models.
For additional information and access to the library,
please visit the website of our library5. Additionally, we have
curated a GitHub repository6, serving as a centralized re-
5. https://github.com/sheldonresearch/ProG
6. https://github.com/WxxShirley/Awesome-Graph-Promptsource for the latest advancements in graph prompt learning.
This repository includes a list of research papers, benchmark
datasets, and available codes, fostering an environment
conducive to ongoing research in this dynamic field. Regular
real-time updates ensure that the repository remains current
with emerging papers and associated codes.
9 C HALLENGES AND FUTURE DIRECTIONS
9.1 Current Challenges
Graph prompt learning has made significant research
progress, but it still encounters several challenges. In this
subsection, we will discuss current challenges in detail.
Inherent Limitation within Graph Models Prompts,
originated from the NLP field [ 2,40,50], serve as a means
to unlock the potential of pre-trained language models
for adapting to downstream tasks. This in-context learning
ability emerges when the scale of model parameters reaches
a certain level. For instance, widely known LLMs typically
possess billions of parameters. With such a powerful pre-
trained model, a simple textual prompt can distill specific
knowledge for downstream tasks. However, when it comes
to prompting on graph tasks, the conditions become more
intractable since pre-trained graph models have significantly
fewer parameters, making it challenging to harness their full
downstream adaptation potential.
Intuitive Evaluation of Graph Prompt Learning In the
NLP field, prompts are typically in a discrete textual format
[2,67], allowing for intuitive understanding, comparison,
and explanation. However, existing graph prompts are
represented as learnable tokens [ 78,12,11,83,55,17] or
augmented graphs [ 80,32]. Such format poses challenges in
intuitively understanding and interpreting graph prompts,
as they lack a readable design. As a result, the effectiveness of
prompts can only be evaluated based on downstream tasks,
limiting efficient and comprehensive performance compari-
son among different kinds of graph prompts. Therefore, the
development of a more intuitive graph prompt design with
a readable format remains an open problem.
More Downstream Applications Currently, graph
prompt learning is primarily applied to node or graph
classification tasks on open-source benchmarks [ 21,107].
Although potential applications have been discussed in
Section 7, the real-world utilization of graph prompt learning
remains limited. A notable example is its use in fraud
detection within real-world transaction networks, addressing
the issue of label scarcity [ 99]. However, compared to the
widespread application of prompting techniques in the
NLP domain [ 1,67,118], the potential of graph prompt
learning in diverse real-world applications requires further
exploration. Overcoming the main challenges of obtaining
powerful domain-specific pre-trained graph models and
designing suitable prompts for specific application scenarios
that exhibit unique characteristics remains crucial.
Transferable Prompt Designs Existing studies on graph
prompt learning [ 52,78,18,12,11,17] typically focus on
pre-training and prompt tuning using the same dataset,
which limits the exploration of more transferable designs
and empirical evaluation. Although PRODIGY [ 32] explores
transferability within the same domain and All in One [ 80]
provides empirical results regarding transferability across

--- PAGE 16 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 16
tasks and domains, the investigation of prompt learning
across diverse domains and tasks remains limited. Achieving
transferability across diverse tasks and domains requires
aligning the task space [ 80], semantic features [ 125], and
structural patterns [ 122], which necessitates further theoret-
ical work to provide insights guiding the development of
transferable prompt designs.
9.2 Future Directions
With the above analysis on graph prompt, we summarize
future directions as follows:
Learning Knowledge from Large Graph Models (LGMs)
like LLMs. Currently, graph models are typically tailored
to specific domains or tasks, limiting generalization abilities
across broader domains or tasks [ 48,125]. Therefore, we are
expecting the realization of large graph models (LGMs) as
a universal tool to be intelligent enough to handle graph
tasks across domains [ 47,14]. A significant step towards this
direction has been taken by Liu et al. [47], who proposed a
powerful model (OFA) capable of addressing classification
tasks for graphs coming from various domains. However,
their approach still relies on LLMs as a general template
to distill specific domain knowledge, which we believe
can be replaced with suitable graph prompting techniques.
Just as textual prompt has been widely used to adapt
LLM for diverse applications [ 67,96,2], graph prompting
techniques are promising to distill LGMs‚Äô knowledge specific
for concrete downstream tasks. Applying the graph prompts
to LGMs has the potential to revolutionize the field of deep
graph learning, leveraging LGMs as a universal tool to tackle
different tasks on graphs from diverse domains.
Transferable Learning. As we have discussed in Section
9.1, current graph prompt learning methods [ 80,78,32,52,
83,17,74] are primarily limited to intra-dataset/domain
settings, lacking the ability to transfer knowledge across
different tasks or domains. While there have been efforts in
graph transfer learning [ 125] to realize domain adaptation, re-
search specifically focused on transferable graph prompting
techniques remains limited. To enable the transfer ability, the
prompts should be designed to realize the alignment between
different task spaces [ 80], reformulating different tasks on
graphs into a uniform template. Besides, both structural
[122] and semantics alignment [ 125] should be realized
via suitable graph prompts to enable domain adaptation.
It is promising to implement transferable graph prompts,
realizing knowledge transfer across domains to extend the
generalization and applicability of graph models.
More Theoretical Foundation. Despite the great success
of graph prompt learning on various downstream tasks,
they mostly draw on the successful experience of prompt
tuning on NLP domain [ 51,2,16,86,65,73]. In other words,
most existing graph prompt tuning methods are designed
with intuition, and their performance gains are evaluated
by empirical experiments. The lack of sufficient theoretical
foundations behind the design has led to both performance
bottlenecks and poor explainability. Therefore, we believe
that building a solid theoretical foundation for graph prompt
learning from a graph theory perspective and minimizing
the gap between the theoretical foundation and empirical
design is also a promising future direction.More Explainable and Understandable Design. While
existing graph prompt learning methods have demonstrated
impressive results on various downstream tasks [ 20,99,109],
we still lack a clear understanding of what exactly is being
learned from the prompts. The black-box learning mode of
prompt vectors raises questions about their interpretability
and whether we can establish meaningful correspondences
between the input data and the prompted graph [ 80]. These
issues are crucial for understanding and interpreting prompts
but are currently missing in most graph prompt research.
To work towards trustworthy graph prompt learning, it is
promising to explore the self-interpretability [ 7] to enable
intuitive explanations of graph prompts. By gaining insights
into the learned prompt vectors and structures, we can
enhance our understanding of the underlying mechanisms
and improve the interpretability of graph prompts. This, in
turn, can lead to more effective utilization of prompts for
security- or privacy-related downstream tasks.
10 C ONCLUSION
In this survey, we explore the promising intersection between
Artificial General Intelligence and graph data by graph
prompt. Our unified framework has unveiled a structured
understanding of graph prompts, dissecting them into tokens,
token structures, and inserting patterns. This framework is a
novel contribution, providing clarity and comprehensiveness
for researchers and practitioners. By exploring the interplay
between graph prompts and models, we‚Äôve revealed fresh
insights into the essence of graph prompts, highlighting
their pivotal role in reshaping AI for graph data. With the
development of ProG, a Python library, and a dedicated
website, we‚Äôve expanded the graph prompting ecosystem,
enhancing collaboration and access to research, benchmark
datasets, and code implementations. Our survey outlines a
roadmap for the future. The challenges and future directions
we‚Äôve discussed serve as a beacon for the evolving field of
graph prompting. With the above work, we hope our survey
can push forward a new era of insights and applications in
AGI family.
ACKNOWLEDGMENTS
The work was supported by grants from the Research Grant
Council of the Hong Kong Special Administrative Region,
China (Project No. CUHK 14217622), and CUHK Direct
Grant No. 4055159......The......first..........author,.....Dr...............Xiangguo......Sun,....in
.............particular,........wants....to........thank....his..........parents.....for......their.......kind...........support
.........during....his........tough..........period.
Author Contributions:
‚Ä¢Xiangguo Sun: wrote section 1, section 2, section 3.5
(with Xixi), the main content of section 5, section 6,
did proofreading of the whole paper, and proposed
most of the original insights.
‚Ä¢Jiawen Zhang: wrote most of section 3 (section 3.1-
3.4), section 4, section 5(collect, analyze papers and
supplement insightful content), section 8, and were
in charge of library development.
‚Ä¢Xixi Wu: wrote section 3.5, section 5 (collect, analyze
papers and supplement insightful content), section 7,
most of section 9 (challenges and future directions),

--- PAGE 17 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 17
collected and analyzed the references, and drew most
of the figures.
‚Ä¢Hong Cheng, Yun Xiong, and Jia Li: were in charge
of proofreading, and discussion, and contributed with
many insightful opinions.
REFERENCES
[1] A. M. Bran and P . Schwaller, ‚ÄúTransformers and Large
Language Models for Chemistry and Drug Discovery,‚Äù
arXiv preprint , 2023.
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,
P . Dhariwal et al. , ‚ÄúLanguage Models Are Few-Shot
Learners,‚Äù in NeurIPS , vol. 33, 2020, pp. 1877‚Äì1901.
[3] Y. Cao, J. Xu, C. Yang, J. Wang, Y. Zhang, C. Wang,
L. Chen, and Y. Yang, ‚ÄúWhen to pre-train graph
neural networks? An answer from data generation
perspective!‚Äù in KDD , 2023.
[4] M. Chen, Z. Liu, C. Liu, J. Li, Q. Mao, and J. Sun,
‚ÄúULTRA-DP: Unifying Graph Pre-training with Multi-
task Graph Dual Prompt,‚Äù arXiv preprint , 2023.
[5] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei,
S. Wang, D. Yin, W. Fan, H. Liu, and J. Tang, ‚ÄúExploring
the Potential of Large Language Models (LLMs) in
Learning on Graphs,‚Äù arXiv preprint , 2023.
[6] J. Cheng, M. Li, J. Li, and F. Tsung, ‚ÄúWiener graph de-
convolutional network improves graph self-supervised
learning,‚Äù in AAAI , 2023, pp. 7131‚Äì7139.
[7] E. Dai and S. Wang, ‚ÄúTowards Self-Explainable Graph
Neural Network,‚Äù in CIKM , 2021, pp. 302‚Äì311.
[8] M. Defferrard, X. Bresson, and P . Vandergheynst,
‚ÄúConvolutional Neural Networks on Graphs with Fast
Localized Spectral Filtering,‚Äù in NeurIPS , vol. 29, 2016.
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
‚ÄúBERT: Pre-training of Deep Bidirectional Transformers
for Language Understanding,‚Äù arXiv preprint , 2019.
[10] C. Edwards, A. Naik, T. Khot, M. Burke, H. Ji, and
T. Hope, ‚ÄúSynerGPT: In-Context Learning for Personal-
ized Drug Synergy Prediction and Drug Design,‚Äù arXiv
preprint , 2023.
[11] T. Fang, Y. Zhang, Y. Yang, and C. Wang, ‚ÄúPrompt
tuning for graph neural networks,‚Äù arXiv preprint , 2022.
[12] T. Fang, Y. Zhang, Y. Yang, C. Wang, and L. Chen,
‚ÄúUniversal Prompt Tuning for Graph Neural Networks,‚Äù
inNeurIPS , 2023.
[13] X. Fang, L. Liu, J. Lei, D. He, S. Zhang, J. Zhou, F. Wang,
H. Wu, and H. Wang, ‚ÄúGeometry-enhanced molecu-
lar representation learning for property prediction,‚Äù
Nature Machine Intelligence , vol. 4, no. 2, pp. 127‚Äì134,
2022.
[14] B. Fatemi, J. Halcrow, and B. Perozzi, ‚ÄúTalk like a
graph: Encoding graphs for large language models,‚Äù
arXiv preprint , 2023.
[15] C. Finn, P . Abbeel, and S. Levine, ‚ÄúModel-Agnostic
Meta-Learning for Fast Adaptation of Deep Networks,‚Äù
inICML , 2017.
[16] T. Gao, A. Fisch, and D. Chen, ‚ÄúMaking Pre-Trained
Language Models Better Few-Shot Learners,‚Äù in ACL ,
2021, pp. 3816‚Äì3830.[17] Q. Ge, Z. Zhao, Y. Liu, A. Cheng, X. Li, S. Wang,
and D. Yin, ‚ÄúEnhancing Graph Neural Networks with
Structure-Based Prompt,‚Äù arXiv preprint , 2023.
[18] C. Gong, X. Li, J. Yu, C. Yao, J. Tan, C. Yu, and D. Yin,
‚ÄúPrompt Tuning for Multi-View Graph Contrastive
Learning,‚Äù arXiv preprint , 2023.
[19] A. Grover and J. Leskovec, ‚Äúnode2vec: Scalable feature
learning for networks,‚Äù in KDD , 2016, pp. 855‚Äì864.
[20] Y. Guo, C. Yang, Y. Chen, J. Liu, C. Shi, and J. Du,
‚ÄúA Data-Centric Framework to Endow Graph Neural
Networks with Out-of-Distribution Detection Ability,‚Äù
inKDD , 2023, pp. 638‚Äì648.
[21] W. Hamilton, Z. Ying, and J. Leskovec, ‚ÄúInductive
Representation Learning on Large Graphs,‚Äù in NeurIPS ,
vol. 30, 2017.
[22] B. Hao, C. Yang, L. Guo, J. Yu, and H. Yin, ‚ÄúMotif-
Based Prompt Learning for Universal Cross-Domain
Recommendation,‚Äù in WSDM , 2024.
[23] A. Hasanzadeh, E. Hajiramezanali, K. Narayanan,
N. Duffield, M. Zhou, and X. Qian, ‚ÄúSemi-Implicit
Graph Variational Auto-Encoders,‚Äù in NeurIPS , vol. 32,
2019.
[24] K. Hassani and A. H. K. Ahmadi, ‚ÄúContrastive Multi-
View Representation Learning on Graphs,‚Äù in ICML ,
vol. 119, 2020, pp. 4116‚Äì4126.
[25] A. Haviv, J. Berant, and A. Globerson, ‚ÄúBERTese:
Learning to Speak to BERT,‚Äù in EACL , 2021, pp. 3618‚Äì
3623.
[26] M. He, Z. Wei, z. Huang, and H. Xu, ‚ÄúBernNet:
Learning Arbitrary Graph Spectral Filters via Bernstein
Approximation,‚Äù in NeurIPS , vol. 34, 2021, pp. 14 239‚Äì
14 251.
[27] Z. Hou, X. Liu, Y. Cen, Y. Dong, H. Yang, C. Wang, and
J. Tang, ‚ÄúGraphMAE: Self-Supervised Masked Graph
Autoencoders,‚Äù in KDD , 2022, pp. 594‚Äì604.
[28] Z. Hou, Y. He, Y. Cen, X. Liu, Y. Dong, E. Kharlamov,
and J. Tang, ‚ÄúGraphMAE2: A Decoding-Enhanced
Masked Self-Supervised Graph Learner,‚Äù in The Web
Conference , 2023, pp. 737‚Äì746.
[29] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang,
L. Wang, and W. Chen, ‚ÄúLoRA: Low-Rank Adaptation
of Large Language Models,‚Äù arXiv preprint , 2021.
[30] W. Hu, B. Liu, J. Gomes, M. Zitnik, P . Liang, V . S.
Pande, and J. Leskovec, ‚ÄúStrategies for Pre-training
Graph Neural Networks,‚Äù in ICLR , 2020.
[31] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun,
‚ÄúGPT-GNN: Generative Pre-Training of Graph Neural
Networks,‚Äù in KDD , 2020, pp. 1857‚Äì1867.
[32] Q. Huang, H. Ren, P . Chen, G. Kr Àázmanc, D. Zeng,
P . Liang, and J. Leskovec, ‚ÄúPRODIGY: Enabling In-
context Learning Over Graphs,‚Äù in NeurIPS , 2023.
[33] X. Jiang, T. Jia, Y. Fang, C. Shi, Z. Lin, and H. Wang,
‚ÄúPre-training on Large-Scale Heterogeneous Graph,‚Äù in
KDD , 2021, pp. 756‚Äì766.
[34] X. Jiang, Y. Lu, Y. Fang, and C. Shi, ‚ÄúContrastive
Pre-Training of GNNs on Heterogeneous Graphs,‚Äù in
CIKM , 2021, pp. 803‚Äì812.
[35] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, ‚ÄúHow
Can We Know What Language Models Know?‚Äù TACL ,
vol. 8, pp. 423‚Äì438, 2020.
[36] B. Jin, W. Zhang, Y. Zhang, Y. Meng, X. Zhang, Q. Zhu,

--- PAGE 18 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 18
and J. Han, ‚ÄúPatton: Language Model Pretraining on
Text-Rich Networks,‚Äù in ACL , 2023.
[37] M. Jin, Y. Zheng, Y.-F. Li, C. Gong, C. Zhou, and S. Pan,
‚ÄúMulti-Scale Contrastive Siamese Networks for Self-
Supervised Graph Representation Learning,‚Äù in IJCAI ,
2021, pp. 1477‚Äì1483.
[38] W. Jin, T. Derr, Y. Wang, Y. Ma, Z. Liu, and J. Tang,
‚ÄúNode Similarity Preserving Graph Convolutional Net-
works,‚Äù in WSDM , 2021, pp. 148‚Äì156.
[39] D. Kim and A. Oh, ‚ÄúHow to Find Your Friendly
Neighborhood: Graph Attention Design with Self-
Supervision,‚Äù in ICLR , 2021.
[40] B. Lester, R. Al-Rfou, and N. Constant, ‚ÄúThe Power
of Scale for Parameter-Efficient Prompt Tuning,‚Äù in
EMNLP , 2021, pp. 3045‚Äì3059.
[41] J. Li, R. Wu, W. Sun, L. Chen, S. Tian, L. Zhu, C. Meng,
Z. Zheng, and W. Wang, ‚ÄúWhat‚Äôs Behind the Mask:
Understanding Masked Graph Modeling for Graph
Autoencoders,‚Äù in KDD , 2023, pp. 1268‚Äì1279.
[42] S. Li, X. Han, and J. Bai, ‚ÄúAdapterGNN: Efficient
Delta Tuning Improves Generalization Ability in Graph
Neural Networks,‚Äù arXiv preprint , 2023.
[43] X. L. Li and P . Liang, ‚ÄúPrefix-Tuning: Optimizing
Continuous Prompts for Generation,‚Äù in ACL-IJCNLP ,
2021, pp. 4582‚Äì4597.
[44] X. Li, D. Lian, Z. Lu, J. Bai, Z. Chen, and X. Wang,
‚ÄúGraphAdapter: Tuning Vision-Language Models With
Dual Knowledge Graph,‚Äù in NeurIPS , 2023.
[45] Y. Li and B. Hooi, ‚ÄúPrompt-Based Zero-and Few-Shot
Node Classification: A Multimodal Approach,‚Äù arXiv
preprint , 2023.
[46] Y. Li, Z. Li, P . Wang, J. Li, X. Sun, H. Cheng, and J. X.
Yu, ‚ÄúA survey of graph meets large language model:
Progress and future directions,‚Äù arXiv preprint , 2023.
[47] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and
M. Zhang, ‚ÄúOne for All: Towards Training One Graph
Model for All Classification Tasks,‚Äù arXiv preprint , 2023.
[48] J. Liu, C. Yang, Z. Lu, J. Chen, Y. Li, M. Zhang, T. Bai,
Y. Fang, L. Sun, P . S. Yu, and C. Shi, ‚ÄúTowards Graph
Foundation Models: A Survey and Beyond,‚Äù arXiv
preprint , 2023.
[49] P . Liu, Y. Ren, and Z. Ren, ‚ÄúGIT-Mol: A Multi-modal
Large Language Model for Molecular Science with
Graph, Image, and Text,‚Äù arXiv preprint , 2023.
[50] P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neu-
big, ‚ÄúPre-train, Prompt, and Predict: A Systematic
Survey of Prompting Methods in Natural Language
Processing,‚Äù ACM Computing Surveys , vol. 55, no. 9, pp.
195:1‚Äì195:35, 2023.
[51] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang,
‚ÄúP-Tuning: Prompt Tuning Can Be Comparable to Fine-
Tuning Across Scales and Tasks,‚Äù in ACL , vol. 2, 2022,
pp. 61‚Äì68.
[52] Z. Liu, X. Yu, Y. Fang, and X. Zhang, ‚ÄúGraphprompt:
Unifying Pre-Training and Downstream Tasks for
Graph Neural Networks,‚Äù in The Web Conference , 2023,
pp. 417‚Äì428.
[53] Z. Liu, S. Li, Y. Luo, H. Fei, Y. Cao, K. Kawaguchi,
X. Wang, and T.-S. Chua, ‚ÄúMolCA: Molecular Graph-
Language Modeling with Cross-Modal Projector and
Uni-Modal Adapter,‚Äù in EMNLP , 2023.[54] Y. Long, M. Wu, Y. Liu, Y. Fang, C. K. Kwoh, J. Chen,
J. Luo, and X. Li, ‚ÄúPre-training graph neural networks
for link prediction in biomedical networks,‚Äù Bioinfor-
matics , vol. 38, no. 8, pp. 2254‚Äì2262, 2022.
[55] Y. Ma, N. Yan, J. Li, M. Mortazavi, and N. V . Chawla,
‚ÄúHetGPT: Harnessing the Power of Prompt Tuning in
Pre-Trained Heterogeneous Graph Neural Networks,‚Äù
arXiv preprint , 2023.
[56] M. Niepert, M. Ahmed, and K. Kutzkov, ‚ÄúLearning
Convolutional Neural Networks for Graphs,‚Äù in ICML ,
2016, pp. 2014‚Äì2023.
[57] M. Ou, P . Cui, J. Pei, Z. Zhang, and W. Zhu, ‚ÄúAsym-
metric Transitivity Preserving Graph Embedding,‚Äù in
KDD , 2016, pp. 1105‚Äì1114.
[58] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang,
‚ÄúAdversarially Regularized Graph Autoencoder for
Graph Embedding,‚Äù in IJCAI , 2018, pp. 2609‚Äì2615.
[59] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu,
‚ÄúUnifying Large Language Models and Knowledge
Graphs: A Roadmap,‚Äù arXiv preprint , 2023.
[60] J. Park, A. Patel, O. Z. Khan, H. J. Kim, and J.-K. Kim,
‚ÄúGraph-Guided Reasoning for Multi-Hop Question
Answering in Large Language Models,‚Äù arXiv preprint ,
2023.
[61] J. Park, M. Lee, H. J. Chang, K. Lee, and J. Y. Choi,
‚ÄúSymmetric Graph Convolutional Autoencoder for
Unsupervised Graph Representation Learning,‚Äù in
ICCV , 2019, pp. 6519‚Äì6528.
[62] Z. Peng, W. Huang, M. Luo, Q. Zheng, Y. Rong, T. Xu,
and J. Huang, ‚ÄúGraph Representation Learning via
Graphical Mutual Information Maximization,‚Äù in The
Web Conference , 2020, pp. 259‚Äì270.
[63] B. Perozzi, R. Al-Rfou, and S. Skiena, ‚ÄúDeepwalk:
Online learning of social representations,‚Äù in KDD ,
2014, pp. 701‚Äì710.
[64] C. Qian, H. Tang, Z. Yang, H. Liang, and Y. Liu, ‚ÄúCan
Large Language Models Empower Molecular Property
Prediction?‚Äù arXiv preprint , 2023.
[65] G. Qin and J. Eisner, ‚ÄúLearning How to Ask: Querying
LMs with Mixtures of Soft Prompts,‚Äù in NAACL-HLT ,
2021, pp. 5203‚Äì5212.
[66] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding,
K. Wang, and J. Tang, ‚ÄúGCC: Graph Contrastive
Coding for Graph Neural Network Pre-Training,‚Äù in
KDD , 2020, pp. 1150‚Äì1160.
[67] J. Robinson, C. M. Rytting, and D. Wingate, ‚ÄúLever-
aging Large Language Models for Multiple Choice
Question Answering,‚Äù arXiv preprint , 2023.
[68] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. WEI, W. Huang,
and J. Huang, ‚ÄúSelf-Supervised Graph Transformer on
Large-Scale Molecular Data,‚Äù in NeurIPS , vol. 33, 2020,
pp. 12 559‚Äì12 571.
[69] M. T. Rosenstein, Z. Marx, L. P . Kaelbling, and T. G.
Dietterich, ‚ÄúTo transfer or not to transfer,‚Äù in NeurIPS ,
vol. 898, 2005.
[70] T. Schick and H. Sch ¬®utze, ‚ÄúFew-Shot Text Generation
with Natural Language Instructions,‚Äù in EMNLP , 2021,
pp. 390‚Äì402.
[71] ‚Äî‚Äî, ‚ÄúIt‚Äôs Not Just Size That Matters: Small Language
Models Are Also Few-Shot Learners,‚Äù in NAACL-HLT ,
2021, pp. 2339‚Äì2352.

--- PAGE 19 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 19
[72] Y. Shi, Z. Huang, S. Feng, H. Zhong, W. Wang, and
Y. Sun, ‚ÄúMasked Label Prediction: Unified Message
Passing Model for Semi-Supervised Classification,‚Äù in
IJCAI , vol. 2, 2021, pp. 1548‚Äì1554.
[73] T. Shin, Y. Razeghi, R. L. L. IV , E. Wallace, and S. Singh,
‚ÄúAutoprompt: Eliciting Knowledge from Language
Models with Automatically Generated Prompts,‚Äù in
EMNLP , 2020, pp. 4222‚Äì4235.
[74] R. Shirkavand and H. Huang, ‚ÄúDeep Prompt Tuning
for Graph Transformers,‚Äù arXiv preprint , 2023.
[75] A. Subramonian, ‚ÄúMOTIF-Driven Contrastive Learn-
ing of Graph Representations,‚Äù in AAAI , vol. 35, 2021,
pp. 15 980‚Äì15 981.
[76] F.-Y. Sun, J. Hoffmann, V . Verma, and J. Tang, ‚ÄúIn-
foGraph: Unsupervised and Semi-supervised Graph-
Level Representation Learning via Mutual Information
Maximization,‚Äù in ICLR , 2020.
[77] M. Sun, J. Xing, H. Wang, B. Chen, and J. Zhou, ‚ÄúMoCL:
Data-driven Molecular Fingerprint via Knowledge-
aware Contrastive Learning from Molecular Graph,‚Äù
inKDD , 2021, pp. 3585‚Äì3594.
[78] M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang, ‚ÄúGPPT:
Graph Pre-Training and Prompt Tuning to Generalize
Graph Neural Networks,‚Äù in KDD , 2022, pp. 1717‚Äì
1727.
[79] X. Sun, H. Yin, B. Liu, H. Chen, J. Cao, Y. Shao,
and N. Q. Viet Hung, ‚ÄúHeterogeneous Hypergraph
Embedding for Graph Classification,‚Äù in WSDM , 2021,
pp. 725‚Äì733.
[80] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, ‚ÄúAll in One:
Multi-Task Prompting for Graph Neural Networks,‚Äù in
KDD , 2023, pp. 2120‚Äì2131.
[81] S. Suresh, P . Li, C. Hao, and J. Neville, ‚ÄúAdversarial
Graph Augmentation to Improve Graph Contrastive
Learning,‚Äù in NeurIPS , vol. 34, 2021, pp. 15 920‚Äì15 933.
[82] Q. Tan, N. Liu, X. Huang, S.-H. Choi, L. Li, R. Chen, and
X. Hu, ‚ÄúS2GAE: Self-Supervised Graph Autoencoders
are Generalizable Learners with Graph Masking,‚Äù in
WSDM , 2023, pp. 787‚Äì795.
[83] Z. Tan, R. Guo, K. Ding, and H. Liu, ‚ÄúVirtual Node
Tuning for Few-shot Node Classification,‚Äù in KDD ,
2023, pp. 2177‚Äì2188.
[84] S. Thakoor, C. Tallec, M. G. Azar, R. Munos,
P . Veli Àáckovi ¬¥c, and M. Valko, ‚ÄúBootstrapped representa-
tion learning on graphs,‚Äù in ICLR , 2021.
[85] Y. Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang,
N. V . Chawla, and P . Xu, ‚ÄúGraph Neural Prompting
with Large Language Models,‚Äù arXiv preprint , 2023.
[86] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. A.
Eslami, O. Vinyals, and F. Hill, ‚ÄúMultimodal Few-Shot
Learning with Frozen Language Models,‚Äù in NeurIPS ,
vol. 34, 2021, pp. 200‚Äì212.
[87] P . Velickovic, G. Cucurull, A. Casanova, A. Romero,
P . Li `o, and Y. Bengio, ‚ÄúGraph Attention Networks,‚Äù in
ICLR , 2018.
[88] P . Velickovic, W. Fedus, W. L. Hamilton, P . Li `o, Y. Ben-
gio, and R. D. Hjelm, ‚ÄúDeep Graph Infomax,‚Äù in ICLR ,
2019.
[89] C. Wang, S. Pan, G. Long, X. Zhu, and J. Jiang,
‚ÄúMGAE: Marginalized Graph Autoencoder for Graph
Clustering,‚Äù in CIKM , 2017, pp. 889‚Äì898.[90] H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu et al. ,
‚ÄúScientific discovery in the age of artificial intelligence,‚Äù
Nature , vol. 620, no. 7972, pp. 47‚Äì60, 2023.
[91] J. Wang, D. Chen, C. Luo, X. Dai, L. Yuan, Z. Wu, and
Y.-G. Jiang, ‚ÄúChatVideo: A Tracklet-centric Multimodal
and Versatile Video Understanding System,‚Äù arXiv
preprint , 2023.
[92] P . Wang, K. Agarwal, C. Ham, S. Choudhury, and
C. K. Reddy, ‚ÄúSelf-Supervised Learning of Contextual
Embeddings for Link Prediction in Heterogeneous
Networks,‚Äù in The Web Conference , 2021, pp. 2946‚Äì2957.
[93] R. Wang, Y. Li, S. Lin, W. Wu, H. Xie, Y. Xu, and J. C.
Lui, ‚ÄúCommon neighbors matter: fast random walk
sampling with common neighbor awareness,‚Äù TKDE ,
vol. 35, no. 5, pp. 4570‚Äì4584, 2022.
[94] X. Wang, Y. Zhang, and C. Shi, ‚ÄúHyperbolic hetero-
geneous information network embedding,‚Äù in AAAI ,
vol. 33, 2019, pp. 5337‚Äì5344.
[95] X. Wang, N. Liu, H. Han, and C. Shi, ‚ÄúSelf-supervised
Heterogeneous Graph Neural Network with Co-
contrastive Learning,‚Äù in KDD , 2021, pp. 1726‚Äì1736.
[96] Y. Wang, N. Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi
Zhang, and Tyler Derr, ‚ÄúKnowledge Graph Prompt-
ing for Multi-Document Question Answering,‚Äù arXiv
preprint , 2023.
[97] Z. Wen and Y. Fang, ‚ÄúAugmenting Low-Resource Text
Classification with Graph-Grounded Pre-Training and
Prompting,‚Äù in SIGIR , 2023, pp. 506‚Äì516.
[98] ‚Äî‚Äî, ‚ÄúPrompt Tuning on Graph-augmented Low-
resource Text Classification,‚Äù arXiv preprint , 2023.
[99] Z. Wen, Y. Fang, Y. Liu, Y. Guo, and S. Hao, ‚ÄúVoucher
Abuse Detection with Prompt-based Fine-tuning on
Graph Neural Networks,‚Äù arXiv preprint , 2023.
[100] J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and
X. Xie, ‚ÄúSelf-supervised Graph Learning for Recom-
mendation,‚Äù in SIGIR , 2021.
[101] J. Wu, S. Li, A. Deng, M. Xiong, and H. Bryan, ‚ÄúPrompt-
and-Align: Prompt-Based Social Alignment for Few-
Shot Fake News Detection,‚Äù in CIKM , 2023.
[102] X. Wu, K. Zhou, M. Sun, X. Wang, and N. Liu, ‚ÄúA
Survey of Graph Prompting Methods: Techniques,
Applications, and Challenges,‚Äù arXiv preprint , 2023.
[103] Y. Wu, R. Xie, Y. Zhu, F. Zhuang, X. Zhang, L. Lin, and
Q. He, ‚ÄúPersonalized Prompt for Sequential Recom-
mendation,‚Äù arXiv preprint , 2023.
[104] Y. Xie, Z. Xu, and S. Ji, ‚ÄúSelf-Supervised Representation
Learning via Latent Graph Prediction,‚Äù in ICML , 2022,
pp. 24 460‚Äì24 477.
[105] Y. Xie, Z. Xu, J. Zhang, Z. Wang, and S. Ji, ‚ÄúSelf-
Supervised Learning of Graph Neural Networks: A
Unified Review,‚Äù TP AML , vol. 45, no. 2, pp. 2412‚Äì2429,
2023.
[106] D. Xu, W. Cheng, D. Luo, H. Chen, and X. Zhang, ‚ÄúIn-
foGCL: Information-Aware Graph Contrastive Learn-
ing,‚Äù in NeurIPS , vol. 34, 2021, pp. 30 414‚Äì30 425.
[107] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, ‚ÄúHow
Powerful are Graph Neural Networks?‚Äù in ICLR , 2018.
[108] C. Yang, D. Bo, J. Liu, Y. Peng, B. Chen, H. Dai et al. ,
‚ÄúData-centric graph learning: A survey,‚Äù arXiv preprint ,
2023.
[109] H. Yang, X. Zhao, Y. Li, H. Chen, and G. Xu, ‚ÄúAn

--- PAGE 20 ---
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 20
Empirical Study Towards Prompt-Tuning for Graph
Contrastive Pre-Training in Recommendations,‚Äù in
NeurIPS , 2023.
[110] Z. Yi, I. Ounis, and C. Macdonald, ‚ÄúContrastive Graph
Prompt-tuning for Cross-domain Recommendation,‚Äù
TOIS , 2023.
[111] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen,
‚ÄúGraph contrastive learning with augmentations,‚Äù in
NeurIPS , 2020, pp. 5812‚Äì5823.
[112] J. Yu, H. Yin, X. Xia, T. Chen, L. Cui, and N. Q. V . Hung,
‚ÄúAre Graph Augmentations Necessary? Simple Graph
Contrastive Learning for Recommendation,‚Äù in SIGIR ,
2022.
[113] J. Yu, H. Yin, X. Xia, T. Chen, J. Li, and Z. Huang,
‚ÄúSelf-Supervised Learning for Recommender Systems:
A Survey,‚Äù TKDE , pp. 1‚Äì20, 2023.
[114] H. Zhang, X. Li, and L. Bing, ‚ÄúVideo-LLaMA: An
Instruction-tuned Audio-Visual Language Model for
Video Understanding,‚Äù arXiv preprint , 2023.
[115] J. Zhang, H. Zhang, C. Xia, and L. Sun, ‚ÄúGraph-
Bert: Only Attention is Needed for Learning Graph
Representations,‚Äù arXiv preprint , 2020.
[116] J. Zhang, Y. Dong, Y. Wang, J. Tang, and M. Ding,
‚ÄúProNE: Fast and Scalable Network Representation
Learning,‚Äù in IJCAI , 2019, pp. 4278‚Äì4284.
[117] M. Zhang and Y. Chen, ‚ÄúLink Prediction Based on
Graph Neural Networks,‚Äù in NeurIPS , 2018.
[118] T. Zhang, F. Ladhak, E. Durmus, P . Liang, K. McKeown,
and T. B. Hashimoto, ‚ÄúBenchmarking Large Language
Models for News Summarization,‚Äù arXiv preprint , 2023.
[119] W. Zhang, Y. Zhu, M. Chen, Y. Geng, Y. Huang, Y. Xu,
W. Song, and H. Chen, ‚ÄúStructure Pretraining and
Prompt Tuning for Knowledge Graph Transfer,‚Äù in The
Web Conference , 2023, pp. 2581‚Äì2590.
[120] Z. Zhang, H. Li, Z. Zhang, Y. Qin, X. Wang, and W. Zhu,
‚ÄúLarge graph models: A perspective,‚Äù arXiv preprint ,
2023.
[121] H. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z.-H. Deng,
L. Kong, and Q. Liu, ‚ÄúGIMLET: A Unified Graph-
Text Model for Instruction-Based Molecule Zero-Shot
Learning,‚Äù arXiv preprint , 2023.
[122] W. Zhao, Q. Wu, C. Yang, and J. Yan, ‚ÄúGraphGLOW:
Universal and Generalizable Structure Learning for
Graph Neural Networks,‚Äù in KDD , 2023, pp. 3525‚Äì
3536.
[123] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang, ‚ÄúGraph
Contrastive Learning with Adaptive Augmentation,‚Äù
inThe Web Conference , 2021, pp. 2069‚Äì2080.
[124] Y. Zhu, J. Guo, and S. Tang, ‚ÄúSGL-PT: A Strong Graph
Learner with Graph Prompt Tuning,‚Äù arXiv preprint ,
2023.
[125] Y. Zhu, Y. Wang, H. Shi, Z. Zhang, and S. Tang, ‚ÄúGraph-
Control: Adding Conditional Control to Universal
Graph Pre-trained Models for Graph Domain Transfer
Learning,‚Äù arXiv preprint , 2023.
Xiangguo Sun is a postdoctoral research fellow
at the Chinese University of Hong Kong. He was
recognized as the ‚ÄùSocial Computing Rising Star‚Äù
in 2023 from CAAI. He studied at Zhejiang Lab as
a visiting researcher in 2022. In the same year, he
received his Ph.D. from Southeast University and
won the Distinguished Ph.D. Dissertation Award.
During his Ph.D. study, he worked as a research
intern at Microsoft Research Asia (MSRA) from
Sep 2021 to Jan 2022 and won the ‚ÄùAward of
Excellence‚Äù. He studied as a joint Ph.D. student
at The University of Queensland hosted by ARC Future Fellow Prof.
Hongzhi Yin from Sep 2019 to Sep 2021. His research interests include
social computing and network learning. He was the winner of the Best
Research Paper Award at KDD‚Äô23, which is the first time for Mainland and
Hong Kong of China. He has published 11 CORE A*, 9 CCF A, and 13
SCI (including 6 IEEE Trans), some of which appear in SIGKDD, VLDB,
The Web Conference (WWW), TKDE, TOIS, WSDM, TNNLS, CIKM, etc.
Jiawen Zhang is a Ph.D. student in Data Science
and Analytics at HKUST (Guangzhou) under the
supervision of Prof. Jia Li. She received her
M.Eng. degree in Computer Technology from the
Chinese Academy of Sciences. She worked as a
research intern at the Machine Learning Group in
Microsoft Research Asia. Her research interests
include data mining and time series modeling.
Xixi Wu is a final-year master student at School
of Computer Science, Fudan University. She
obtained her B.S. in Computer Science from
Fudan University in 2021. Her research interests
include Deep Graph Learning, Data Mining, and
Recommender Systems. She has published mul-
tiple first-authored papers at KDD/WWW/CIKM.
Hong Cheng is a Professor in the Department
of Systems Engineering and Engineering Man-
agement, Chinese University of Hong Kong. She
received the Ph.D. degree from the University
of Illinois at Urbana-Champaign in 2008. Her
research interests include data mining, database
systems, and machine learning. She received
research paper awards at ICDE‚Äô07, SIGKDD‚Äô06,
and SIGKDD‚Äô05, and the certificate of recogni-
tion for the 2009 SIGKDD Doctoral Dissertation
Award. She received the 2010 Vice-Chancellor‚Äôs
Exemplary Teaching Award at the Chinese University of Hong Kong.
Yun Xiong is a Professor in Shanghai Key Lab-
oratory of Data Science, School of Computer
Science, Fudan University. She obtained her
Ph.D. degree from Fudan University in 2008.
Her research interests include big data mining
and data science. Her research achievements
include the publication of more than 50 papers
in internationally top journals and conferences in
the field of Data Mining, including TKDE, KDD,
WWW, AAAI, etc.
Jia Li is an assistant professor in HKUST
(Guangzhou). He received Ph.D. degree at The
Chinese University of Hong Kong in 2021. Be-
fore that, he worked as a full-time data mining
engineer at Tencent from 2014 to 2017, and
research intern at Google AI (Mountain View)
in 2020. His research interests include graph
learning and data mining. Some of his work has
been published in TPAMI, ICML, NeurIPS, WWW,
KDD, etc.
