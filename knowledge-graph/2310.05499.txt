# 2310.05499.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2310.05499.pdf
# File size: 1082070 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Integrating Graphs with Large Language
Models: Methods and Prospects
Shirui Pan, Griffith University, Gold Coast, QLD 4215, Australia
Yizhen Zheng & Yixin Liu, Monash University, Melbourne, VIC 3800, Australia
Abstract—Large language models (LLMs) such as GPT -4 have emerged as
frontrunners, showcasing unparalleled prowess in diverse applications, including
answering queries, code generation, and more. Parallelly, graph-structured data,
an intrinsic data type, is pervasive in real-world scenarios. Merging the
capabilities of LLMs with graph-structured data has been a topic of keen interest.
This paper bifurcates such integrations into two predominant categories. The first
leverages LLMs for graph learning, where LLMs can not only augment existing
graph algorithms but also stand as prediction models for various graph tasks.
Conversely, the second category underscores the pivotal role of graphs in
advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting
graphs in either reasoning or collaboration. Integrating with such structures can
significantly boost the performance of LLMs in various complicated tasks. We also
discuss and propose open questions for integrating LLMs with graph-structured
data for the future direction of the field.
Large language models (LLMs) have rapidly
taken centre stage due to their remarkable ca-
pabilities. They have demonstrated prowess in
various tasks, including but not limited to, translation,
question-answering, and code generation. Their adapt-
ability and efficiency in processing and understanding
vast amounts of data position them as revolutionary
tools in the age of information. Concurrently, graphs
are a natural representation of the world, and their
ability to capture complex relationships between en-
tities makes them a powerful tool for modelling real-
world scenarios. For instance, structures reminiscent
of graphs can be observed in nature and on the
internet. Given the individual significance of both LLMs
and graph structures, the exploration into how they
can be synergistically combined has emerged as a hot
topic in the AI community.
In the paper, we delineate two primary paradigms
for the synergy between LLMs and graphs. The first
paradigm, “LLMs enhance graph learning” involves
harnessing the capabilities of LLMs to handle various
graph-related tasks. This includes predicting graph
properties, such as the degrees and connectivity
of nodes, as well as more intricate challenges like
node and graph classification. Here, LLMs can ei-
ther supplement graph algorithms or serve as main
XXXX-XXX © 2023 IEEE
Digital Object Identifier 10.1109/XXX.0000.0000000predictive/generative models. Conversely, the second
paradigm, “Graphs advance LLMs ability”, capitalises
on the inherent structure of graphs to enhance the
reasoning capabilities of LLMs or help LLMs to collab-
orate, facilitating them in handling multifaceted tasks.
By leveraging graph structures, the efficacy of LLMs
in complex problem-solving can be significantly aug-
mented.
Why integrating graphs and LLMs?
Using LLMs to address graph tasks enjoys two pri-
mary advantages. First, unlike the often opaque graph
deep learning techniques, LLMs approach graph-
related challenges primarily through reasoning, pro-
viding clearer insight into the basis for their predic-
tions. This transparency offers a more interpretable
method for understanding complex graph analyses.
Secondly, LLMs possess an extensive repository of
prior knowledge spanning diverse domains. Traditional
graph learning models, constrained by limited train-
ing data, struggle to comprehensively assimilate this
wealth of knowledge. Consequently, harnessing LLMs
for graph data processing presents an opportunity to
leverage their scalability and the expansive reservoir
of prior knowledge. Such knowledge can be especially
valuable for graph machine learning in domains such
as finance and biology.
Using graphs to enhance LLMs is also a promising
learning paradigm. In specific, graphs can substantially
Oct Published by the IEEE Computer Society Publication Name 1arXiv:2310.05499v1  [cs.AI]  9 Oct 2023

--- PAGE 2 ---
THEME/FEATURE/DEPARTMENT
LLM
Text Enhanced
AttributeGraph Learning 
Models
(a) LLMs Augmenting Graph Algorithms
(c) LLMs Constructing Graphs
LLM
Graphs(b) LLMs Predicting Graph Tasks
LLM
Graphs
AnalysePrediction
Direct    is predicted as A
Predict with 
Explanation   's 1-hop neighbors 
is ......   Thus, it is 
more likely A.
Text 
GenerateTask
 output
LLM
Reasoning(d1) Overall pipeline of LLM reasoning
(d2) Input-output  
No 
reasoning(d3) Chain of 
Thoughts (D/M)
Step 
1
Step 
2
Step 
3
(d4) Tree of Thoughts Prompting (U/M)
4
Step 
1
Step 
12
2
2
2
3
3
3
3
4
4
4
4
5
(d5) Graph of Thoughts Prompting (U/E)
Upon Tree of Thoughts...
Planner
Worker
Node 
HeterogeneityAggregation & 
Combination 
Interaction 
between pathsInteraction 
between graphs
(e) LLMs Multi-agent Systems
LLM
Product 
Manager
LLM
ProgrammerLLM
Project 
Manager
LLM
System 
Architect
LLM
QA Engineer
LLMs
GraphsLLMs Enhance
Graph LearningGraphs Enhance 
LLM AbilityEncode
FIGURE 1. The overall framework of the mutual enhancement between LLMs and Graphs. (a)-(c): three pathways for LLMs
to enhance graph learning. (d)-(e): techniques for graph structures enhancing LLM reasoning. Brackets after technique names
indicate graph types. D, U, M and E represent directed, undirected, homogeneous and heterogeneous graphs, respectively.
amplify the capacity of LLMs in both logical reasoning
and collaboration within multi-agent systems1,2,7,9. For
instance, using a straightforward prompt like "Let’s
think step by step", commonly referred to as the chain-
of-thoughts, has been proven to markedly enhance the
LLM’s proficiency in resolving mathematical problems4.
It’s noteworthy that such enhancements are observed
even with the use of a chain, which represents one
of the simplest graph structures. This gives rise to
the anticipation that leveraging more intricate graph
structures could usher in even more profound im-
provements. From a broader viewpoint, in multi-agent
systems, graphs model inter-agent relationships, facil-
itating efficient information flow and collaboration.
LLMs Enhance Graph Learning
One pivotal approach to integrating LLMs and graphs
involves leveraging LLMs to bolster graph learning.
As illustrated in the left part of Figure 1, this en-
hancement can materialise through three distinct path-
ways: augmenting conventional graph algorithms with
the prowess of LLMs; directly employing LLMs for
downstream graph-related tasks; and utilizing LLMs in
the intricate construction of graph structures. In the
following sections, we dissect each of these strategies
in detail.
LLMs Augmenting Graph Algorithms
The integration of Large Language Models (LLMs)
with graph algorithms primarily seeks to harness LLMs
as attribute-enhancement mechanisms, elevating the
intrinsic attributes of graph nodes. As depicted in
Figure 1(a), LLMs process text information for nodesto produce refined attributes. These enhanced at-
tributes can potentially improve the performance of
graph learning models such as graph neural networks
(GNNs).
A direct approach is to employ LLMs as encoders
for processing node text-based attributes, with the
option to fine-tune on specific downstream tasks3.
Another technique uses a proprietary LLM, GPT -3.5, to
simultaneously produce predictions and explanations
for tasks like paper classification5. Using another open-
source LLM, they derive node embeddings by encod-
ing both the output of LLMs and the original attributes.
These embeddings are combined and then integrated
into GNNs to boost performance.
A more sophisticated approach uses an iterative
method to harmoniously integrate both GNNs and
LLMs capabilities3. They are initially trained sepa-
rately; then, via a variational EM framework, the LLM
uses text and GNN’s pseudo labels, while the GNN
utilizes LLM-encoded embeddings or node attributes
and LLMs’ pseudo labels, iteratively boosting mutual
performance.
LLMs Predicting Graph Tasks
LLMs are adept at predicting graph properties, includ-
ing attributes like node degrees and connectivity, and
can even tackle complex challenges such as node and
graph classification, as illustrated in Figure 1(b).
A straightforward application involves presenting
LLMs with zero-shot or few-shot prompts, prompting
them to either directly predict an outcome or to first
provide an analytical rationale followed by the ulti-
mate prediction3,6. Experiments reveal that while LLMs
demonstrate a foundational grasp of graph structures,
2 Integrating Graphs with Large Language Models: Methods and Prospects Oct 2023

--- PAGE 3 ---
THEME/FEATURE/DEPARTMENT
their performance lags behind that of graph neural net-
work benchmarks. They also show that performance of
LLMs is significantly affected by the prompting strategy
and the use of graph description language, which is a
textual way to describe graphs.
A more advanced method, dubbed InstructGLM,
has been put forth8. This strategy utilises a multi-
task, multi-prompt instructional tuning process to refine
LLMs prior to inference on specific tasks. During fine-
tuning, nodes are treated as new tokens—initialised
with inherent node features—to broaden the original
vocabulary of LLMs. Consequently, node embeddings
can be refined during the training phase. Employing
this refined methodology, their system outperforms
graph neural network benchmarks across three citation
networks.
LLMs Constructing Graphs
LLMs can help in building graphs for downstream
tasks as shown in Figure 1(c). For instance, some
researchers have tried using LLMs to analyse news
headlines and identify companies that might be
impacted10. In specific, a network of companies that
have correlations is constructed by LLMs automati-
cally. The generated network can be used to improve
the performance of predictions of stock market move-
ments.
Graphs Enhance LLM Ability
Leveraging graph structures can significantly boost the
reasoning and collaborative capacities of LLMs. As
shown in the right part of Figure 1, these improvements
emerge via two primary mechanisms: (1) employing
graph structures to bolster logical reasoning in LLMs,
and (2) utilizing graph structures to enhance LLM
collaboration in multi-agent systems. We delve deeper
into each of these approaches in the subsequent sec-
tions.
Graphs Improving LLMs Reasoning
Graphs are the foundational structure of human rea-
soning. Through tools like mind maps and flowcharts,
and strategies like trial and error or task decomposi-
tion, we manifest our intrinsic graph-structured thought
processes. Not surprisingly, when properly leveraged,
they can significantly elevate the reasoning capabilities
of LLMs. As illustrated in Figure 1(d1), when tasked,
LLMs follow a sequence: they process the input data,
engage in reasoning, and then produce the final re-
sults. Figure 1(d2) highlights the limitations of LLMs us-
ing “Input-output Prompting”; without reasoning, theirperformance tends to suffer, especially with complex
tasks.
Employing graph structures, from basic chains and
trees to more complex designs, can profoundly aug-
ment the reasoning capabilities of LLMs. Consider the
“chain-of-thought prompting” (COT) method, depicted
in Figure 1(d3)4. In this, LLMs harness a chain, a
type of directed acyclic graph, for structured problem-
solving. Remarkably, even this basic framework triples
LLMs’ efficacy on GSM8K, a math word problem
benchmark.
In contrast, the “Tree of Thoughts” (ToT) method,
utilising trees—an elementary undirected acyclic
graph—delves deeper into reasoning. Eeach reason-
ing phase in ToT is a node7. LLMs traverse this
tree, eliminating non-compliant nodes and returning
upwards as necessary, to deduce the solution. With
this methodology, LLMs notch up a 74% accuracy in
the “Game of 24” test, overshadowing the 4% from
COT7.
Diving into intricate graph structures propels LLMs’
capabilities even further. Improving ToT, the “Graph
of Thoughts” (GoT) paradigm has been introduced1,2,
as illustrated in Figure 1(d5). This advanced rea-
soning graph can be heterogeneous, with diverse
nodes dedicated to specific tasks. Sophisticated mech-
anisms, such as node aggregation and combination
(A&C), and dynamic interactions between paths and
graphs, are incorporated. A&C, for instance, facilitates
node subdivision for task decomposition and node
amalgamation1. Path interactions offer LLMs greater
flexibility by enabling cross-path traversals, a leap from
ToT’s isolated branch framework. Multi-graph interac-
tions can even be orchestrated for intricate tasks2.
These GoT methodologies dramatically outpace sim-
pler graph models in handling complex challenges,
indicating that more intricate graph structures could
usher in even more significant enhancements.
Graphs Building LLMs Collaboration
While the preceding section examined the capabilities
of individual LLMs, complex tasks, such as software
development, require multiple LLMs to work in tandem
within a collaborative framework, i.e., multi-agent sys-
tems, as illustrated in Figure 1(e). Graph structures can
be instrumental in this context. As depicted in the same
figure, these structures can effectively model the rela-
tionships and information flow between collaborating
LLMs.
Oct 2023 Integrating Graphs with Large Language Models: Methods and Prospects 3

--- PAGE 4 ---
THEME/FEATURE/DEPARTMENT
Open Questions and Directions
The intersection of Large Language Models (LLMs)
with graph structures holds promise, yet its current
development sparks some open questions and chal-
lenges.
LLMs Enhancing Graph Learning
Question 1. How to leverage LLMs to learn
on other types of graphs beyond Text-attributed
Graphs (TAG)? Current LLMs for graph learning pri-
marily concern TAGs. However, real-world graph data,
such as social networks and molecular graphs, often
incorporate attributes from different domains. To realise
the potential of LLMs in graph learning, it is crucial to
efficiently handle a wide variety of graphs as input to
these models.
Future Directions: Direction 1. Translate diverse data
types into textual format: For instance, a user’s profile
on a social network might list attributes like age, ad-
dress, gender, and hobbies. These can be articulated
as: "User X is a male in his 20s, residing in Melbourne,
with a passion for playing guitars." Direction 2. Lever-
aging multi-modal models for graph-text alignment:
Multi-modal LLMs have already made notable strides
in domains like audio and images. Identifying methods
to synchronise graph data with text would empower
us to tap into the capabilities of multi-modal LLMs for
graph-based learning.
Question 2. How can we help LLMs under-
stand graphs? Central to the success of LLMs in
graph learning is their ability to genuinely compre-
hend graphs. Experimental evidence suggests that
the choice of graph description language can have a
significant impact on LLM performance6.
Future Directions: Direction 1. Expanding graph
description languages: Current graph description lan-
guages offer a somewhat restricted scope. Developing
enhanced description methods would enable LLMs
to grasp and process graphs more effectively. Direc-
tion 2. Pretraining or fine-tuning LLMs on Graphs: Pre-
training or fine-tuning LLMs on various graph data con-
verted by graph description language can help LLMs
understand graphs better8.Direction 3. Foundational
graph models for graph learning: While foundational
models have made strides in areas like language,
and image, a gap remains in establishing large-scale
foundational models for graphs. Converting graphs to
textual format offers a unique opportunity: LLMs can
be trained on this data, enabling graph learning to
leverage the prior knowledge and scalability inherent
in LLMs.Graphs Enhance LLM Ability
Question 3. How to elevate more sophisticated
graph structures to enhance LLM reasoning? Cur-
rent explorations into LLM reasoning have touched
upon graph structures like chains, trees, and traditional
graphs. However, there is vast potential in delving into
more intricate graph structures, such as hypergraphs,
probabilistic graphical models, and signed graphs.
Future Directions: Expanding the types of graphs
for LLM reasoning: Diversifying the graph types used
could significantly bolster LLM reasoning.
Question 4 .How to elevate more sophisticated
graph structures to enhance multi-agent systems
(MLS)? Presently, the graph structures guiding MLS,
like that in MetaGPT9, are relatively rudimentary. While
MetaGPT employs the waterfall model in software
development—illustrated by a simple chain structure
linking different agents—contemporary software de-
velopment is far more nuanced with intricate agent
relationships and multifaceted processes.
Future Directions: Incorporating advanced graph
structures for team-based LLM workflows: Drawing
from the utility of graph structures in reasoning, adopt-
ing varied graph forms such as trees, traditional
graphs, and even more intricate structures may help.
Question 5. How to integrate graph structures
into the pipeline of LLMs? The applicability of graph
structures is not confined to reasoning and collabora-
tion. There’s a compelling case to be made for their
integration across all stages of the LLM lifecycle: from
training and fine-tuning to inference.
Future Directions: Utilising graph structures in train-
ing, fine-tuning, and inference. For example, graphs
can structure the training data, enabling more effective
learning.
REFERENCES
1. M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L.
Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H.
Niewiadomski, P . Nyczyk, et al., “Graph of thoughts:
Solving elaborate problems with large language mod-
els,” arXiv:2308.09687 , 2023.
2. B. Lei, C. Liao, C. Ding, et al., “Boosting Logical
Reasoning in Large Language Models through
a New Framework: The Graph of Thought,”
arXiv:2308.08614 , 2023.
3. Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S.
Wang, D. Yin, W. Fan, H. Liu, et al., “Exploring the
potential of large language models (LLMs) in learning
on graphs,” arXiv:2307.03393 , 2023.
4 Integrating Graphs with Large Language Models: Methods and Prospects Oct 2023

--- PAGE 5 ---
THEME/FEATURE/DEPARTMENT
4. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F . Xia,
E. Chi, Q. V. Le, D. Zhou, et al., “Chain-of-thought
prompting elicits reasoning in large language models,”
Advances in Neural Information Processing Systems ,
vol. 35, pp. 24824–24837, 2022.
5. X. He, X. Bresson, T. Laurent, B. Hooi, et al., “Ex-
planations as Features: LLM-Based Features for Text-
Attributed Graphs,” arXiv:2305.19523 , 2023.
6. J. Guo, L. Du, H. Liu, “GPT4Graph: Can Large
Language Models Understand Graph Structured
Data? An Empirical Evaluation and Benchmarking,”
arXiv:2305.15066 , 2023.
7. S. Y ao, D. Yu, J. Zhao, I. Shafran, T.L. Griffiths,Y . Cao, K. Narasimhan, “Tree of thoughts: Delib-
erate problem solving with large language models,”
arXiv:2305.10601 , 2023.
8. R. Y e, C. Zhang, R. Wang, S. Xu, Y . Zhang, et al., “Nat-
ural language is all a graph needs,” arXiv:2308.07134 ,
2023.
9. S. Hong, X. Zheng, J. Chen, Y . Cheng, C. Zhang,
Z. Wang, S. K. Y au, Z. Lin, L. Zhou, C. Ran, et al.,
“Metagpt: Meta programming for multi-agent collabo-
rative framework,” arXiv:2308.00352 , 2023.
10. Z. Chen, L. N. Zheng, C. Lu, J. Yuan, D. Zhu, et al.,
“ChatGPT Informed Graph Neural Network for Stock
Movement Prediction,” in SIGKDD 2023 Workshop on
Robust NLP for Finance , 2023.
Oct 2023 Integrating Graphs with Large Language Models: Methods and Prospects 5
