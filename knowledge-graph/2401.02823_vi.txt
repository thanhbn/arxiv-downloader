# DocGraphLM: Mô hình Ngôn ngữ Đồ thị Tài liệu cho Trích xuất Thông tin

Dongsheng Wang
JPMorgan AI Research
London, UK
dongsheng.wang@jpmchase.com

Zhiqiang Ma
JPMorgan AI Research
New York, New York, USA
zhiqiang.ma@jpmchase.com

Armineh Nourbakhsh
JPMorgan AI Research
New York, New York, USA
armineh.nourbakhsh@jpmchase.com

Kang Gu
Dartmouth College
Hanover, New Hampshire, USA
Kang.Gu.GR@dartmouth.edu

Sameena Shah
JPMorgan AI Research
New York, New York, USA
sameena.shah@jpmchase.com

## TÓM TẮT

Những tiến bộ trong Hiểu biết Tài liệu Giàu Thị giác (VrDU) đã cho phép trích xuất thông tin và trả lời câu hỏi trên các tài liệu có bố cục phức tạp. Hai loại kiến trúc đã xuất hiện—các mô hình dựa trên transformer được lấy cảm hứng từ LLMs, và Mạng Nơ-ron Đồ thị. Trong bài báo này, chúng tôi giới thiệu DocGraphLM, một khung làm việc mới kết hợp các mô hình ngôn ngữ được đào tạo trước với ngữ nghĩa đồ thị. Để đạt được điều này, chúng tôi đề xuất 1) một kiến trúc bộ mã hóa kết hợp để biểu diễn tài liệu, và 2) một phương pháp dự đoán liên kết mới để tái tạo đồ thị tài liệu. DocGraphLM dự đoán cả hướng và khoảng cách giữa các nút bằng cách sử dụng một hàm mất mát kết hợp hội tụ ưu tiên khôi phục vùng lân cận và giảm trọng số phát hiện nút xa. Các thí nghiệm của chúng tôi trên ba bộ dữ liệu SotA cho thấy sự cải thiện nhất quán trong các tác vụ IE và QA với việc áp dụng các đặc trưng đồ thị. Hơn nữa, chúng tôi báo cáo rằng việc áp dụng các đặc trưng đồ thị tăng tốc độ hội tụ trong quá trình học trong suốt quá trình đào tạo, mặc dù chỉ được xây dựng thông qua dự đoán liên kết.

**CCS CONCEPTS**
•Information systems →Document structure ;Language models;Information extraction .

**KEYWORDS**
mô hình ngôn ngữ, mạng nơ-ron đồ thị, trích xuất thông tin, hiểu biết tài liệu thị giác

**ACM Reference Format:**
Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, và Sameena Shah. 2023. DocGraphLM: Mô hình Ngôn ngữ Đồ thị Tài liệu cho Trích xuất Thông tin. Trong Kỷ yếu Hội nghị ACM SIGIR Quốc tế lần thứ 46 về Nghiên cứu và Phát triển trong Truy xuất Thông tin (SIGIR '23), ngày 23–27 tháng 7, 2023, Taipei, Đài Loan. ACM, New York, NY, USA, 5 trang.
https://doi.org/10.1145/3539618.3591975

Quyền tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này để sử dụng cá nhân hoặc lớp học được cấp miễn phí với điều kiện các bản sao không được tạo hoặc phân phối để kiếm lợi nhuận hoặc lợi thế thương mại và các bản sao mang thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của những người khác ngoài (các) tác giả phải được tôn trọng. Việc tóm tắt có ghi công được cho phép. Để sao chép theo cách khác, hoặc xuất bản lại, để đăng trên máy chủ hoặc để phân phối lại cho danh sách, yêu cầu sự cho phép cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
SIGIR '23, ngày 23–27 tháng 7, 2023, Taipei, Đài Loan
©2023 Bản quyền thuộc về chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM ISBN 978-1-4503-9408-6/23/07. . . $15.00
https://doi.org/10.1145/3539618.3591975

## 1 GIỚI THIỆU

Trích xuất thông tin từ các tài liệu giàu thị giác (VrDs), như các biểu mẫu kinh doanh, hóa đơn, và hóa đơn thanh toán ở định dạng PDF hoặc hình ảnh đã thu hút sự chú ý gần đây. Các tác vụ như nhận dạng và trích xuất trường và liên kết thực thể là rất quan trọng để số hóa VrDs và xây dựng các hệ thống truy xuất thông tin trên dữ liệu. Các tác vụ đòi hỏi lý luận phức tạp như Trả lời Câu hỏi Thị giác trên tài liệu yêu cầu mô hình hóa các tín hiệu không gian, thị giác và ngữ nghĩa trong VrDs. Do đó, Hiểu biết VrD quan tâm đến việc mô hình hóa nội dung đa phương thức trong các tài liệu hình ảnh. Nghiên cứu trước đây đã khám phá việc sử dụng mã hóa văn bản, bố cục và các đặc trưng hình ảnh trong một mô hình ngôn ngữ bố cục hoặc cài đặt đa phương thức để cải thiện các tác vụ hạ nguồn. Ví dụ, LayoutLM và các biến thể của nó [7,23,24] sử dụng thông tin hình ảnh và bố cục để tăng cường biểu diễn văn bản, từ đó cải thiện hiệu suất trên các tác vụ khác nhau. Tuy nhiên, các mô hình sử dụng cơ chế Transformer đặt ra thách thức trong việc biểu diễn ngữ nghĩa cách xa về mặt không gian, như các ô bảng xa tiêu đề của chúng hoặc nội dung qua các ngắt dòng. Dưới ánh sáng của những hạn chế này, một số nghiên cứu [25,27] đã đề xuất sử dụng mạng nơ-ron đồ thị (GNNs) để mô hình hóa mối quan hệ và cấu trúc giữa các token văn bản hoặc đoạn trong tài liệu. Mặc dù các mô hình này một mình vẫn kém hiệu suất so với các mô hình ngôn ngữ bố cục, chúng thể hiện tiềm năng của việc kết hợp thông tin có cấu trúc bổ sung để cải thiện biểu diễn tài liệu.

Được thúc đẩy bởi điều này, chúng tôi giới thiệu một khung làm việc mới có tên DocGraphLM tích hợp ngữ nghĩa đồ thị tài liệu và ngữ nghĩa được bắt nguồn từ các mô hình ngôn ngữ được đào tạo trước để cải thiện biểu diễn tài liệu. Như được mô tả trong Hình 1, đầu vào của mô hình chúng tôi là các embedding của token, vị trí và hộp giới hạn, tạo thành nền tảng của biểu diễn tài liệu. Để tái tạo đồ thị tài liệu, chúng tôi đề xuất một phương pháp dự đoán liên kết mới dự đoán hướng và khoảng cách giữa các nút bằng cách sử dụng một hàm mất mát kết hợp, cân bằng mất mát phân loại và hồi quy. Ngoài ra, mất mát khuyến khích khôi phục vùng lân cận gần trong khi giảm cấp phát hiện trên các nút xa hơn. Điều này được đạt được bằng cách chuẩn hóa khoảng cách thông qua biến đổi logarit, coi các nút được phân cách bởi khoảng cách có độ lớn cụ thể là cách đều về mặt ngữ nghĩa.

Các thí nghiệm của chúng tôi trên nhiều bộ dữ liệu bao gồm FUNSD, CORD, và DocVQA, cho thấy sự vượt trội của mô hình một cách nhất quán. Hơn nữa, việc kết hợp các đặc trưng đồ thị được tìm thấy để tăng tốc quá trình học. Chúng tôi nêu bật các đóng góp chính của công trình chúng tôi như sau:

• chúng tôi đề xuất một kiến trúc mới tích hợp mạng nơ-ron đồ thị với mô hình ngôn ngữ được đào tạo trước để tăng cường biểu diễn tài liệu;

• chúng tôi giới thiệu một phương pháp dự đoán liên kết cho tái tạo đồ thị tài liệu, và một hàm mất mát kết hợp nhấn mạnh khôi phục trên các nút lân cận gần;

• cuối cùng, các đặc trưng mạng nơ-ron đồ thị được đề xuất dẫn đến cải thiện hiệu suất nhất quán và hội tụ nhanh hơn.

## 2 CÔNG TRÌNH LIÊN QUAN

Các kiến trúc dựa trên Transformer đã được áp dụng thành công cho các tác vụ hiểu biết bố cục, vượt qua các kết quả tiên tiến (SotA) trước đây [3,13,14,16,21,22]. Các nghiên cứu như LayoutLM [23] và LayoutLMv2 [24] kết hợp các embedding văn bản với các đặc trưng thị giác sử dụng mạng đề xuất vùng, cho phép các mô hình được đào tạo trên các mục tiêu như Mô hình Ngôn ngữ Thị giác Che dấu (MVLM) và chú ý nhận biết không gian, dẫn đến hiệu suất cải thiện trên các tác vụ phức tạp như VQA và hiểu biết biểu mẫu. TILT [9] tăng cường chú ý bằng cách thêm bias để nắm bắt vị trí 2-D tương đối, đã cho thấy hiệu suất xuất sắc trên bảng xếp hạng DocVQA. StructuralLM [12] tận dụng tối đa các tương tác của các ô mà mỗi ô chia sẻ cùng hộp giới hạn.

Việc sử dụng GNNs [20] để biểu diễn tài liệu cho phép thông tin lan truyền linh hoạt hơn. Trong các mô hình VrDU dựa trên GNN, tài liệu thường được biểu diễn như đồ thị của các token và/hoặc câu, và các cạnh biểu diễn mối quan hệ không gian giữa chúng, ví dụ nắm bắt K-Láng giềng Gần nhất. Các mô hình dựa trên GNN có thể được sử dụng cho các tác vụ dựa trên tài liệu khác nhau như phân loại văn bản [25,27] hoặc trích xuất thông tin khóa [2,26]. Tuy nhiên, hiệu suất của chúng vẫn tụt hậu so với các mô hình ngôn ngữ bố cục. Điều này là do biểu diễn đồ thị một mình không đủ để nắm bắt ngữ nghĩa phong phú của một tài liệu. Trong các trường hợp mà các mô hình dựa trên GNN vượt trội đáng kể so với các mô hình ngôn ngữ bố cục, chúng thường lớn hơn và tập trung vào các tác vụ cụ thể [10]. Trong bài báo này, chúng tôi đề xuất một khung làm việc kết hợp ngữ nghĩa phong phú của các mô hình ngôn ngữ bố cục với tín hiệu cấu trúc mạnh mẽ được nắm bắt bởi các mô hình GNN. Chúng tôi thể hiện cách việc bổ sung ngữ nghĩa đồ thị có thể tăng cường hiệu suất của các mô hình ngôn ngữ bố cục trên các tác vụ IE và QA, và cải thiện hội tụ mô hình.

## 3 DOCGRAPHLM: MÔ HÌNH NGÔN NGỮ ĐỒ THỊ TÀI LIỆU

### 3.1 Biểu diễn tài liệu như đồ thị

Trong GNN, một đồ thị bao gồm các nút và cạnh. Trong bối cảnh biểu diễn tài liệu như đồ thị, các nút biểu diễn các đoạn văn bản (tức là nhóm các từ liền kề) và mối quan hệ giữa chúng được biểu diễn như các cạnh. Các đoạn văn bản từ tài liệu hình ảnh có thể được thu thập thông qua các công cụ Nhận dạng Ký tự Quang học, thường nắm bắt các token như hộp giới hạn có kích thước khác nhau.

Để tạo ra các cạnh giữa các nút, chúng tôi áp dụng một heuristic mới có tên Hướng Tầm nhìn (D-LoS), thay vì phương pháp K-láng giềng gần nhất (KNN) [19] hoặc phương pháp β-skeleton thường được sử dụng [11]. Phương pháp KNN có thể dẫn đến các hàng hoặc cột dày đặc, không liên quan được coi là láng giềng, bỏ qua thực tế rằng một số cặp khóa-giá trị trong biểu mẫu có thể là các nút xa hơn. Để giải quyết điều này, chúng tôi áp dụng phương pháp D-LoS, nơi chúng tôi chia đường chân trời 360 độ xung quanh một nút nguồn thành tám khu vực rời rạc 45 độ, và chúng tôi xác định nút gần nhất so với nút nguồn trong mỗi khu vực. Tám khu vực này định nghĩa tám hướng so với nút nguồn. Định nghĩa này được lấy cảm hứng từ tác vụ đào tạo trước được báo cáo trong StrucTexT [14] áp dụng phương pháp này để xây dựng biểu diễn đồ thị của nó.

**Biểu diễn nút.** Một nút có hai đặc trưng — ngữ nghĩa văn bản và kích thước nút. Ngữ nghĩa văn bản có thể được thu thập thông qua các embedding token (ví dụ từ các mô hình ngôn ngữ), trong khi kích thước nút được biểu diễn bởi các kích thước của nó trên tọa độ x và y, về mặt toán học M=emb([width,height]) trong đó width = x₂−x₁ và height = y₂−y₁, cho rằng (x₁,y₁) và (x₂,y₂) là tọa độ của góc trên bên trái và góc dưới bên phải của hộp giới hạn đoạn. Một cách trực quan, kích thước nút là một chỉ báo quan trọng vì nó giúp phân biệt kích thước phông chữ và có thể là vai trò ngữ nghĩa của đoạn, ví dụ, tiêu đề, chú thích, và thân. Do đó, chúng tôi ký hiệu một đầu vào nút như E_u=emb(T_u)⊕M_u, trong đó u={1,2,...,N} chỉ ra nút thứ u trong một tài liệu và T_u đại diện cho các văn bản bên trong nút u.

Chúng tôi học biểu diễn nút bằng cách tái tạo đồ thị tài liệu sử dụng GNN, được biểu diễn như h^G_u=GNN(E_u). Chi tiết về việc học h^G_u được mô tả trong Phần 3.2.

**Biểu diễn cạnh.** Để biểu diễn mối quan hệ giữa hai nút, chúng tôi sử dụng các đặc trưng cực của chúng, bao gồm khoảng cách tương đối và hướng (một trong tám khả năng). Chúng tôi tính toán khoảng cách Euclid ngắn nhất, d, giữa hai hộp giới hạn. Để giảm tác động của các nút xa có thể kém liên quan về mặt ngữ nghĩa với nút nguồn, chúng tôi áp dụng một kỹ thuật làm mịn khoảng cách với biến đổi log được ký hiệu như e_dis=log(d+1). Hướng tương đối e_dir∈{0,...,7} cho một cặp nút được thu thập từ D-LoS. Chúng tôi định nghĩa một liên kết, được ký hiệu như e_p=[e_dis,e_dir], để tái tạo đồ thị tài liệu trong phần 3.2.

### 3.2 Tái tạo đồ thị bằng dự đoán liên kết

Chúng tôi dự đoán hai thuộc tính chính của các liên kết e_p để tái tạo đồ thị và khung quá trình như một bài toán học đa tác vụ.

Đầu vào của GNN là các biểu diễn nút được mã hóa, và biểu diễn được truyền qua cơ chế truyền thông điệp trên GNN, cụ thể:

h^{G,l+1}_u := aggregate(h^{G,l}_v, ∀v∈N(u)),     (1)

trong đó l là lớp của láng giềng, N(u) ký hiệu tập hợp các láng giềng của nút u, và aggregate(·) là một hàm tổng hợp cập nhật biểu diễn nút.

Chúng tôi đào tạo GNN một cách kết hợp trên hai tác vụ — dự đoán khoảng cách và hướng giữa các nút — để học biểu diễn nút. Đối với dự đoán khoảng cách, chúng tôi định nghĩa một đầu hồi quy ŷ^e_{u,v}, tạo ra một giá trị vô hướng thông qua tích chấm của hai vector nút, và sử dụng một kích hoạt tuyến tính, như được trình bày trong Phương trình 2.

ŷ^e_{u,v} = Linear((h^G_u)^T × h^G_v)     (2)

Đối với dự đoán hướng, chúng tôi định nghĩa một đầu phân loại ŷ^d_{u,v} gán một trong tám hướng cho mỗi cạnh dựa trên tích theo từng phần tử giữa hai nút, được biểu diễn như sau:

ŷ^d_{u,v} = σ((h^G_u ⊙ h^G_v) × W)     (3)

trong đó h^G_u ⊙ h^G_v là một tích theo từng phần tử giữa hai nút và W là trọng số có thể học được cho vector tích. σ là một hàm kích hoạt phi tuyến.

Chúng tôi sử dụng mất mát MSE cho hồi quy khoảng cách và entropy chéo cho phân loại hướng, tương ứng. Sau đó, mất mát kết hợp là:

loss = Σ_{(u,v)∈batch}[(λ·loss_{MSE}(ŷ^e_{u,v}, y^e_{u,v}) + (1−λ)·loss_{CE}(ŷ^d_{u,v}, y^d_{u,v})]·(1−r_{u,v})     (4)

trong đó λ là một siêu tham số có thể điều chỉnh cân bằng trọng số của hai mất mát, và r_{u,v} là chuẩn hóa của khoảng cách e_dis, bị ràng buộc trong khoảng [0,1], để giá trị của 1−r_{u,v} giảm trọng số các đoạn xa và ưu tiên các đoạn gần.

### 3.3 Biểu diễn kết hợp

Biểu diễn nút kết hợp, h^C_u, là một sự kết hợp của biểu diễn mô hình ngôn ngữ h^L_u và biểu diễn GNN h^G_u thông qua một hàm tổng hợp f (ví dụ, nối, trung bình, hoặc tổng) được biểu diễn như h^C_u = f(h^L_u, h^G_u). Trong công trình này, chúng tôi vận hành hàm tổng hợp f với việc nối ở cấp độ token. Các biểu diễn nút được giới thiệu có thể được sử dụng như đầu vào cho các mô hình khác để hỗ trợ các tác vụ hạ nguồn, ví dụ, IE_Head(h^C_u) cho trích xuất thực thể và QA_Head(h^C_u) cho tác vụ trả lời câu hỏi thị giác.

## 4 THÍ NGHIỆM

### 4.1 Bộ dữ liệu và đường cơ sở

Chúng tôi đánh giá các mô hình của chúng tôi trên hai tác vụ trích xuất thông tin qua ba bộ dữ liệu thường được sử dụng: FUNSD [8], CORD [18], và DocVQA [17]. FUNSD và CORD tập trung vào trích xuất cấp độ thực thể, trong khi DocVQA tập trung vào việc xác định các khoảng trả lời trong tài liệu hình ảnh trong một tác vụ trả lời câu hỏi. Thống kê bộ dữ liệu được hiển thị trong Bảng 1. Vui lòng tham khảo các trích dẫn để biết thêm chi tiết.

Bảng 1: Thống kê của các bộ dữ liệu tài liệu thị giác. Sự khác biệt giữa DocVQA và DocVQA† được giới thiệu trong Phần 4.1.

| Bộ dữ liệu | Số nhãn | Số train | Số val | Số test |
|------------|---------|----------|--------|---------|
| FUNSD      | 4       | 149      | -      | 50      |
| CORD       | 30      | 800      | 100    | 100     |
| DocVQA     | -       | 39,000   | 5,000  | 5,000   |
| DocVQA†    | -       | 32,553   | 4,400  | 5,000   |

Cần lưu ý rằng các tệp OCR được cung cấp trong DocVQA¹ chứa một số lượng nhỏ các đầu ra OCR không hoàn hảo, ví dụ, lệch văn bản và thiếu văn bản, dẫn đến thất bại trong việc xác định câu trả lời. Chúng tôi chỉ có thể sử dụng 32,553 mẫu để đào tạo và 4,400 mẫu để xác thực. Chúng tôi ký hiệu bộ dữ liệu đã sửa đổi như DocVQA†. Vì lợi ích của việc đảm bảo so sánh công bằng trong các thí nghiệm của chúng tôi, chúng tôi đã duy trì việc sử dụng các đầu ra OCR từ bộ dữ liệu.

Như các đường cơ sở của chúng tôi, chúng tôi sử dụng các mô hình SotA tận dụng các đặc trưng khác nhau, bao gồm RoBERTa [15], BROS [6], DocFormer-base [1], StructuralLM [12], LayoutLM [23], LayoutLMv3 [7] và Doc2Graph [4]. RoBERTa là mô hình transformer không có bất kỳ đặc trưng bố cục hoặc hình ảnh nào, BROS và StructuralLM chỉ áp dụng thông tin bố cục, DocFormer và LayoutLMv3 sử dụng cả đặc trưng bố cục và hình ảnh, và Doc2Graph chỉ dựa vào các đặc trưng đồ thị tài liệu.

### 4.2 Thiết lập thí nghiệm

Đối với FUNSD và CORD, chúng tôi áp dụng các siêu tham số đào tạo sau: epoch = 20, tỷ lệ học = 5e-5, và kích thước batch = 6, và đào tạo mô hình của chúng tôi trên một GPU NVIDIA T4 Tensor Core duy nhất. Đối với DocVQA, chúng tôi áp dụng các siêu tham số đào tạo sau: epoch = 5, tỷ lệ học = 5e-5, và kích thước batch = 4.

Chúng tôi áp dụng GraphSage [5] như mô hình GNN của chúng tôi, vì nó đã được chứng minh hiệu quả trong các đặc trưng đồ thị tài liệu [4]. Đối với tái tạo đồ thị, chúng tôi đặt một giá trị hằng số λ=0.5 trong suốt thí nghiệm.

### 4.3 Kết quả

Hiệu suất của DocGraphLM và các mô hình khác trên bộ dữ liệu FUNSD được trình bày trong Bảng 2. Mô hình của chúng tôi đạt điểm F1 tốt nhất là 88.77, đạt được khi nó được kết hợp với mô hình LayoutLMv3-base. Mặt khác, RoBERTa-base (không tận dụng các đặc trưng bố cục) có điểm F1 thấp nhất là 65.37, nhưng kết hợp nó với DocGraphLM dẫn đến cải thiện 1.66 điểm. Vui lòng lưu ý các điểm có ⋄ được báo cáo trong các trích dẫn tương ứng. Ký hiệu tương tự áp dụng cho các bảng khác.

Bảng 2: So sánh hiệu suất mô hình trên FUNSD.

| Mô hình                         | F1    | Precision | Recall |
|--------------------------------|-------|-----------|--------|
| RoBERTa-base                   | 65.37 | 61.17     | 70.20  |
| Doc2Graph⋄[4]                  | 82.25 | -         | -      |
| StructuralLM_large⋄[12]        | 85.14 | 83.52     | 86.81  |
| LayoutLM-base⋄[23]             | 78.66 | 75.97     | 81.55  |
| LayoutLMv3-base[7]             | 88.16 | 86.70     | 87.7   |
| BROS⋄[6]                       | 83.05 | 81.16     | 85.02  |
| DocFormer-base⋄[1]             | 83.34 | 80.76     | 86.09  |
| DocGraphLM (RoBERTa-base)      | 67.03 (↑1.66) | 62.92 | 70.0 |
| DocGraphLM (LLMv3-base)        | 88.77 (↑0.61) | 87.44 | 90.15 |

Đối với bộ dữ liệu CORD, các so sánh hiệu suất được hiển thị trong Bảng 3, và hiệu suất tốt nhất được đạt bởi DocGraphLM (LayoutLMv3-base) với điểm F1 là 96.93, theo sát bởi BROS. Tương tự, mặc dù RoBERTa-base một mình đạt điểm thấp hơn nhiều, DocGraphLM (RoBERTa-base) tăng điểm F1 lên 2.26 điểm.

Bảng 3: So sánh hiệu suất mô hình CORD.

| Mô hình                         | F1    | Precision | Recall |
|--------------------------------|-------|-----------|--------|
| RoBERTa-base                   | 48.99 | 42.77     | 57.34  |
| LayoutLM-base⋄                 | 94.80 | 95.03     | 94.58  |
| LayoutLMv3-base                | 95.59 | 95.31     | 95.88  |
| BROS⋄                          | 95.36 | 95.58     | 95.14  |
| DocFormer-base⋄                | 96.33 | 96.52     | 96.14  |
| DocGraphLM (RoBERTa-base)      | 51.25 (↑2.26) | 45.45 | 58.76 |
| DocGraphLM (LayoutLMv3-base)   | 96.93 (↑1.62) | 96.86 | 97.01 |

Bảng 4 hiển thị hiệu suất mô hình trên bộ dữ liệu thử nghiệm DocVQA. Các điểm hiệu suất được thu thập bằng cách gửi đầu ra mô hình của chúng tôi đến bảng xếp hạng DocVQA², vì các câu trả lời chuẩn không được cung cấp cho công chúng. Bên cạnh điểm tổng thể, hiệu suất của mô hình trên các tác vụ tiểu mục cũng được báo cáo. DocGraphLM (với LayoutLMv3-base) vượt trội hơn các mô hình khác trong hầu hết mọi khía cạnh ngoại trừ ngữ nghĩa văn bản thuần túy, cho thấy khả năng của mô hình trong việc mô hình hóa ngữ nghĩa đa phương thức một cách hiệu quả. Bảng trình bày bằng chứng mạnh mẽ về hiệu quả của DocGraphLM trong việc cải thiện biểu diễn tài liệu, khi các mô hình ngôn ngữ bố cục được tăng cường với phương pháp của chúng tôi.

Bảng 4: So sánh hiệu suất mô hình trên bộ dữ liệu thử nghiệm DocVQA. Điểm từ bảng xếp hạng DocVQA.

| Mô hình                        | Điểm  | Form  | Table | Text  |
|-------------------------------|-------|-------|-------|-------|
| RoBERTa_base                  | 60.40 | 71.75 | 54.23 | 61.35 |
| LayoutLMv3_base               | 67.80 | 77.84 | 67.58 | 70.55 |
| DocGraphLM (LayoutLMv3-base)  | 69.84 (↑2.04) | 79.73 | 68.48 | 63.23 |

Hiệu suất vượt trội qua các bộ dữ liệu khác nhau cho thấy rằng việc sử dụng biểu diễn đồ thị được đề xuất trong DocGraphLM dẫn đến cải thiện nhất quán. Một p-value nhỏ hơn 0.05 đã được nhận khi so sánh hiệu suất của các mô hình qua các bộ dữ liệu này, cho thấy cải thiện có ý nghĩa thống kê từ mô hình của chúng tôi.

### 4.4 Tác động lên hội tụ

Chúng tôi cũng quan sát thấy rằng tốc độ hội tụ đào tạo thường nhanh hơn khi bổ sung các đặc trưng đồ thị so với LayoutLM vanilla (các mô hình base V1 và V3). Ví dụ, Hình 2 minh họa rằng điểm F1 cải thiện với tỷ lệ hội tụ nhanh hơn trong bốn epoch đầu tiên, khi thử nghiệm trên bộ dữ liệu CORD. Điều này có thể do các đặc trưng đồ thị cho phép transformer tập trung nhiều hơn vào các láng giềng gần, cuối cùng dẫn đến quá trình lan truyền thông tin hiệu quả hơn.

Hình 2: So sánh tốc độ hội tụ mô hình trên CORD. Các đường cong được tạo từ việc lấy trung bình qua mười lần thử.

## 5 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Bài báo này trình bày một khung làm việc DocGraphLM mới kết hợp ngữ nghĩa đồ thị với các mô hình ngôn ngữ được đào tạo trước để cải thiện biểu diễn tài liệu cho VrDs. Phương pháp dự đoán liên kết được đề xuất tái tạo khoảng cách và hướng giữa các nút, ngày càng giảm trọng số các liên kết xa hơn. Các thí nghiệm của chúng tôi trên nhiều tác vụ hạ nguồn trên các bộ dữ liệu khác nhau cho thấy hiệu suất được tăng cường so với đường cơ sở chỉ LM. Ngoài ra, việc giới thiệu các đặc trưng đồ thị tăng tốc quá trình học. Như một hướng tương lai, chúng tôi có kế hoạch kết hợp các kỹ thuật đào tạo trước khác nhau cho các đoạn tài liệu khác nhau. Chúng tôi cũng sẽ kiểm tra tác động của các biểu diễn liên kết khác nhau cho tái tạo đồ thị.

**Tuyên bố từ chối trách nhiệm.** Bài báo này được chuẩn bị cho mục đích thông tin bởi nhóm Nghiên cứu Trí tuệ Nhân tạo của JPMorgan Chase & Co. và các chi nhánh ("JP Morgan"), và không phải là sản phẩm của Bộ phận Nghiên cứu của JP Morgan. JP Morgan không đưa ra tuyên bố và bảo đảm gì và từ chối mọi trách nhiệm, đối với tính đầy đủ, chính xác hoặc độ tin cậy của thông tin được chứa ở đây. Tài liệu này không được dự định như nghiên cứu đầu tư hoặc tư vấn đầu tư, hoặc một khuyến nghị, đề nghị hoặc yêu cầu mua hoặc bán bất kỳ chứng khoán, công cụ tài chính, sản phẩm tài chính hoặc dịch vụ nào, hoặc được sử dụng theo bất kỳ cách nào để đánh giá các ưu điểm của việc tham gia vào bất kỳ giao dịch nào, và sẽ không cấu thành một yêu cầu dưới bất kỳ thẩm quyền nào hoặc đối với bất kỳ người nào, nếu yêu cầu đó dưới thẩm quyền đó hoặc đối với người đó sẽ là bất hợp pháp.

## TÀI LIỆU THAM KHẢO

[1] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, và R Manmatha. 2021. Docformer: End-to-end transformer cho hiểu biết tài liệu. Trong Kỷ yếu hội nghị quốc tế IEEE/CVF về thị giác máy tính. 993–1003.

[2] Brian L. Davis, Bryan S. Morse, Brian L. Price, Chris Tensmeyer, và Curtis Wigington. 2021. Visual FUDGE: Form Understanding via Dynamic Graph Editing. CoRR abs/2105.08194 (2021). arXiv:2105.08194 https://arxiv.org/abs/2105.08194

[3] Lukasz Garncarek, Rafal Powalski, Tomasz Stanislawek, Bartosz Topolski, Piotr Halama, và Filip Gralinski. 2020. LAMBERT: Layout-Aware language Modeling using BERT for information extraction. CoRR abs/2002.08087 (2020). arXiv:2002.08087 https://arxiv.org/abs/2002.08087

[4] Andrea Gemelli, Sanket Biswas, Enrico Civitelli, Josep Lladós, và Simone Marinai. 2022. Doc2Graph: a Task Agnostic Document Understanding Framework based on Graph Neural Networks. arXiv preprint arXiv:2208.11168 (2022).

[5] Will Hamilton, Zhitao Ying, và Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems 30 (2017).

[6] Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, và Sungrae Park. 2020. BROS: a pre-trained language model for understanding texts in document. (2020).

[7] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, và Furu Wei. 2022. Layoutlmv3: Pre-training for document ai with unified text and image masking. Trong Kỷ yếu Hội nghị ACM Quốc tế về Đa phương tiện lần thứ 30. 4083–4091.

[8] Guillaume Jaume, Hazim Kemal Ekenel, và Jean-Philippe Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents. Trong Hội nghị Quốc tế về Phân tích và Nhận dạng Tài liệu 2019 Workshops (ICDARW), Tập 2. IEEE, 1–6.

[9] Rafał Powalski, Lukasz Borchmann, và Dawid Jurkiewicz. 2021. Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. arXiv preprint arXiv:2102.09550 (2021).

[10] Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, và Tomas Pfister. 2022. Formnet: Structural encoding beyond sequential modeling in form document information extraction. arXiv preprint arXiv:2203.08411 (2022).

[11] Chen-Yu Lee, Chun-Liang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii, Siyang Qin, Ashok Popat, và Tomas Pfister. 2021. ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction. Trong Kỷ yếu Cuộc họp Thường niên lần thứ 59 của Hiệp hội Ngôn ngữ học Tính toán và Hội nghị Kết hợp Quốc tế lần thứ 11 về Xử lý Ngôn ngữ Tự nhiên (Tập 2: Bài báo Ngắn). Hiệp hội Ngôn ngữ học Tính toán, Trực tuyến, 314–321. https://doi.org/10.18653/v1/2021.acl-short.41

[12] Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, và Luo Si. 2021. Structurallm: Structural pre-training for form understanding. arXiv preprint arXiv:2105.11210 (2021).

[13] Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, và Hongfu Liu. 2021. Selfdoc: Self-supervised document representation learning. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu. 5652–5660.

[14] Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, và Errui Ding. 2021. Structext: Structured text understanding with multi-modal transformers. Trong Kỷ yếu Hội nghị ACM Quốc tế về Đa phương tiện lần thứ 29. 1912–1920.

[15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).

[16] Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, và Marc Najork. 2020. Representation learning for information extraction from form-like documents. Trong kỷ yếu cuộc họp thường niên lần thứ 58 của Hiệp hội Ngôn ngữ học Tính toán. 6495–6504.

[17] Minesh Mathew, Dimosthenis Karatzas, và CV Jawahar. 2021. Docvqa: A dataset for vqa on document images. Trong Kỷ yếu hội nghị mùa đông IEEE/CVF về ứng dụng thị giác máy tính. 2200–2209.

[18] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, và Hwalsuk Lee. 2019. CORD: a consolidated receipt dataset for post-OCR parsing. Trong Workshop về Document Intelligence tại NeurIPS 2019.

[19] Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, và Regina Barzilay. 2019. GraphIE: A Graph-Based Framework for Information Extraction. Trong Kỷ yếu Hội nghị 2019 của Chương Bắc Mỹ của Hiệp hội Ngôn ngữ học Tính toán: Công nghệ Ngôn ngữ Con người, Tập 1 (Bài báo Dài và Ngắn). Hiệp hội Ngôn ngữ học Tính toán, Minneapolis, Minnesota, 751–761. https://doi.org/10.18653/v1/N19-1082

[20] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, và Gabriele Monfardini. 2008. The graph neural network model. IEEE transactions on neural networks 20, 1 (2008), 61–80.

[21] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, và Furu Wei. 2021. Layoutreader: Pre-training of text and layout for reading order detection. arXiv preprint arXiv:2108.11591 (2021).

[22] Zilong Wang, Mingjie Zhan, Xuebo Liu, và Ding Liang. 2020. Docstruct: A multimodal method to extract hierarchy structure in document for general form understanding. arXiv preprint arXiv:2010.11685 (2020).

[23] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, và Ming Zhou. 2020. Layoutlm: Pre-training of text and layout for document image understanding. Trong Kỷ yếu Hội nghị ACM SIGKDD Quốc tế lần thứ 26 về Khám phá Tri thức & Khai thác Dữ liệu. 1192–1200.

[24] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. 2020. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. arXiv preprint arXiv:2012.14740 (2020).

[25] Liang Yao, Chengsheng Mao, và Yuan Luo. 2019. Graph convolutional networks for text classification. Trong Kỷ yếu hội nghị AAAI về trí tuệ nhân tạo, Tập 33. 7370–7377.

[26] Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, và Rong Xiao. 2021. PICK: processing key information extraction from documents using improved graph learning-convolutional networks. Trong Hội nghị Quốc tế về Nhận dạng Mẫu lần thứ 25 năm 2020 (ICPR). IEEE, 4363–4370.

[27] Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen, và Liang Wang. 2020. Every document owns its structure: Inductive text classification via graph neural networks. arXiv preprint arXiv:2004.13826 (2020).

---

¹ https://www.docvqa.org/
² https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1
