# 2305.05936.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2305.05936.pdf
# File size: 326438 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multi-hop Commonsense Knowledge Injection Framework for
Zero-Shot Commonsense Question Answering
Xin Guan, Biwei Cao, Qingqing Gao, Zheng Yin, Bo Liu, Jiuxin Cao
Southeast University
{xin_guan,caobiwei,qingqing_gao,z.yin,bliu,jx.cao}@seu.edu.cn
Abstract
Commonsense question answering (QA) re-
search requires machines to answer questions
based on commonsense knowledge. However,
this research requires expensive labor costs to
annotate data as the basis of research, and mod-
els that rely on ﬁne-tuning paradigms only ap-
ply to speciﬁc tasks, rather than learn a general
commonsense reasoning ability. As a more ro-
bust method, zero-shot commonsense question
answering shows a good prospect. The current
zero-shot framework tries to convert triples
in commonsense knowledge graphs (KGs)
into QA-form samples as the pre-trained data
source to incorporate commonsense knowl-
edge into the model. However, this method
ignores the multi-hop relationship in the KG,
which is also an important central problem in
commonsense reasoning. In this paper, we pro-
pose a novel multi-hop commonsense knowl-
edge injection framework. Speciﬁcally, it ex-
plores multi-hop reasoning paradigm in KGs
that conform to linguistic logic, and we further
propose two multi-hop QA generation meth-
ods based on KGs. Then, we utilize con-
trastive learning to pre-train the model with
the synthetic QA dataset to inject multi-hop
commonsense knowledge. Extensive experi-
ments on ﬁve commonsense question answer-
ing benchmarks demonstrate that our frame-
work achieves state-of-art performance.
1 Introduction
Commonsense knowledge is an essential basis for
daily human communication and is also a key re-
search area in the present natural language under-
standing system. In order to explore and improve
the understanding and reasoning ability of ma-
chines for commonsense, many question answering
(QA) benchmark datasets have been proposed, e.g.,
CommonsenseQA (Talmor et al., 2019), Abductive
NLI (Bhagavatula et al., 2020), PhysicalIQA (Bisk
Corresponding author.
Knowledge GraphQuestion and  Answer Choice
AtLocation AtLocation AtLocationAtLocationRevolving door is convenient for two direction travel, 
but it also serves as a security measure at a what?
RelateTorevolving door
department 
storeNew York mallbank
securityA.bank   B. library  C. department store
D. mall     E. New York
Figure 1: Example of multi-hop commonsense reason-
ing. Two-hops path of knowledge is required to reason
out the correct answer.
et al., 2020), etc. Meanwhile, with the develop-
ment of large-scale pre-trained language models
(Devlin et al., 2019; Liu et al., 2019; Raffel et al.,
2020), the gap between the performance of ma-
chines and humans in these datasets is gradually
closing. However, some studies (Mitra et al., 2019;
Ma et al., 2021; Kim et al., 2022) point out that
models ﬁne-tuned for speciﬁc downstream tasks
are ﬁtting individual datasets rather than learning
a general commonsense reasoning ability. There-
fore, as a more comprehensive evaluation method,
zero-shot commonsense question answering has
gradually become the focus of future research.
Zero-shot commonsense question answering is
to build a general commonsense question answer-
ing system with reasoning ability, rather than target
task-speciﬁc scenarios. Recent work (Ma et al.,
2021; Kim et al., 2022) uses multi-source com-
monsense knowledge graphs, such as ConceptNet
(Speer et al., 2017), Atomic (Sap et al., 2019a),
and WordNet (Miller, 1995), as data sources, and
uses single-hop triples in knowledge graphs to gen-
erate synthetic QA datasets for training the QAarXiv:2305.05936v1  [cs.CL]  10 May 2023

--- PAGE 2 ---
model. However, most commonsense questions
require multi-hop knowledge for correct reason-
ing. For these questions, the existing models do
not have the ability of multi-hop reasoning, so they
will choose the wrong answer. As shown in Figure
1, for this commonsense question, it is necessary to
reason the correct answer through the two-hop path
of(revolving door, at location, bank) and(bank,
related to, security) in the knowledge graph. How-
ever, previous work only generated QA samples
based on single-hop triplets. The model trained
by the dataset generated in this method does not
have the reasoning ability for this multi-hop com-
monsense question. Since there are four answer
entities directly connected with question entities in
the knowledge graph, this model even misleads the
answer selection by considering these four entities
to be the correct answer.
To address the above problem, we propose a
novel multi-hop commonsense knowledge injec-
tion framework for zero-shot commonsense ques-
tion answering, which is divided into two steps: the
synthetic multi-hop QA generation andthe knowl-
edge injection pre-training . Speciﬁcally, in the pro-
cess of synthetic multi-hop QA generation, we ex-
plore the general paradigm of multi-hop common-
sense reasoning, and further propose two multi-hop
QA generation methods with linguistic logic, com-
positive commonsense QA generation method and
conjunctive commonsense QA generation method,
which correspond to the continuous reasoning abil-
ity and multi-faceted reasoning ability of humans
respectively. We generate a synthetic multi-hop
QA dataset by these two methods. In the knowl-
edge injection pre-training process, we utilize con-
trastive learning to pre-train the model on the syn-
thetic multi-hop QA dataset to inject multi-hop
commonsense knowledge. Finally, we can get a
general commonsense question answering model
with multi-hop knowledge reasoning ability.
The main contributions of this work are summa-
rized as follows:
•To our best knowledge, the proposed multi-
hop commonsense knowledge injection frame-
work is the ﬁrst work to introduce multi-hop
commonsense knowledge to improve the per-
formance of zero-shot commonsense question
answering.
•We explore general paradigms of multi-hop
commonsense reasoning in KG, and furtherpropose two multi-hop QA generation meth-
ods with linguistic logic to generate a syn-
thetic multi-hop QA dataset. Both methods
can generate highly confusing negative sam-
ples to improve the performance of the model.
•We utilize contrastive learning to pre-train on
synthetic multi-hop QA datasets to get a com-
monsense question answering model with gen-
eral reasoning ability and multi-hop knowl-
edge.
•We conduct extensive experiments on vari-
ous commonsense question answering bench-
marks. The results demonstrate that our frame-
work achieves state-of-art performance.
2 Related Work
2.1 Zero-Shot Commonsense Question
answering
Zero-shot commonsense question answering fo-
cuses on constructing unsupervised models with-
out any label supervision. Current work can be
divided into two paradigms: (1) Designing unsu-
pervised models by leveraging the properties of pre-
trained language models. Some work utilizes the
masked language model (MLM) task in autoregres-
sive language models, such as Word2Vec, BERT
(Devlin et al., 2019), to evaluate the plausibility of
sentences composed of question and answer (Tam-
borrino et al., 2020; Trinh and Le, 2018). Other
works take generative language models, such as
GPT-2 (Radford et al., 2019) and GPT-3 (Brown
et al., 2020), to design template preﬁxes to gener-
ate commonsense knowledge or answers (Shwartz
et al., 2020; Niu et al., 2021; Liu et al., 2022; Wang
and Zhao, 2022). (2) Injecting knowledge into
pre-trained models. These works utilize external
knowledge (e.g., ConceptNet, ATOMIC, Wiki) as
data sources and design different pre-training tasks
to incorporate knowledge into the pre-trained mod-
els (Banerjee and Baral, 2020; Ma et al., 2021; Kim
et al., 2022). Since the injected knowledge enables
the model to learn relevant commonsense knowl-
edge through pre-training, this method is more com-
petitive on some commonsense question answering
tasks. Our work also focuses on this zero-shot
method.
2.2 Commonsense Question Answering with
External Knowledge
In the supervised setting, there have been many
work attempts to incorporate external knowledge

--- PAGE 3 ---
at location
locker room weeding gardenKG
gymnasium exercise trampolineused for
used for at locationtrampoline canbefound at 
gymnasiumtrampoline canbeused for 
exerciseSynthetic QA 
Q: [mask] canbefound at gymnasium and can be used for 
exercise.
A. trampoline ()    B. locker room  C. weeding garden
revolving door canbefound
at a banka bank canbeused for 
storing money Synthetic QA 
Q: revolving door canbefound at [mask] , that can 
be used for storing money.
A. a bank ()           B. a mall                 C. lobby
(a) Compositive Commonsense Knowledge (b) Conjunctive Commonsense Knowledgenatural language transformationsynthesize
natural language transformationsynthesize
revolving 
door
a mall
lobbyat location storing 
money 
shopping
have a 
drinkused for
used for
used forat location
at locationKG
a bankFigure 2: Synthetic multi-hop QA generation process. (a)is the compositive commonsense QA generation process
and(b)is the conjuctive commonsense QA generation process. Each QA generation method extracts multi-hop
knowledge containing positive and negative samples in KG, and converts them into QA-form samples according
to natural language templates.
reasoning modules into the model. These works
(Lin et al., 2019; Feng et al., 2020; Yasunaga et al.,
2021; Guan et al., 2022) extract subgraphs related
to QA concepts in the knowledge graph, and design
graph encoders, combined with text encoders for
reasoning. In the unsupervised scene, many works
directly use the knowledge graph as a data source
for pre-training. Banerjee and Baral (2020) design
the knowledge triplet learning, using any two ele-
ments to predict the remaining element. Ma et al.
(2021) construct question answering samples using
knowledge graphs, and train the model with the
masked language modeling method. (Kim et al.,
2022) design knowledge adapters to mitigate the
loss of knowledge from the interference among the
different knowledge sources. However, the above
unsupervised methods only use single-hop triples
as training samples, ignoring the rich multi-hop re-
lationships in the knowledge graph. We propose a
multi-hop knowledge injection framework to solve
this problem.
3 Multi-hop Commonsense Knowledge
Injection Framework
In this paper, we focus on the zero-shot common-
sense knowledge question answering task, where
the model or system can not access any training
or validation data for commonsense tasks. Un-
der this setting, we ﬁrst generate a synthetic QA
dataset from KG, and then pre-train the model onthis dataset to inject commonsense knowledge. Fi-
nally, we test our model on different commonsense
question answering tasks to verify the model’s com-
monsense reasoning ability.
In previous zero-shot commonsense question an-
swering frameworks (Ma et al., 2021; Kim et al.,
2022), four knowledge graphs, ConceptNet (Speer
et al., 2017), WordNet (Miller, 1995), WikiData
(Vrandecic and Krötzsch, 2014), ATOMIC (Sap
et al., 2019a), are used as data sources to synthe-
size QA. Formally, for a triplet (ehead;r;etail)in
KG, whereehead;r;etaildenote head entity, rela-
tion and tail entity respectively, previous method
convertseheadandrinto question Qthrough natu-
ral language templates, takes etailas the correct an-
swerAc, and randomly selects tail entities in other
triplets as distractors fA1;;Ang, wherenis
the number of negative samples. By above method,
one synthesized QA sample (Q;A)is generated,
whereA=fAc;A1;;Ang. After generating
the synthetic dataset, it uses the model to perform
a masked language modeling task on this dataset to
obtain commonsense knowledge.
Our work builds on previous frameworks, ad-
dressing the lack of multi-hop reasoning capabil-
ities in current zero-shot commonsense question
answering research.

--- PAGE 4 ---
3.1 Synthetic Multi-hop QA Generation
We follow the deﬁnition in the previous frame-
works and propose two methods for synthetic multi-
hop QA generation on this basis, as shown in Fig-
ure 2. During the generation process, we follow
two criteria: (1) The synthetic multi-hop QA needs
to have practical signiﬁcance, conforming to daily
dialogue scenarios and human logical thinking. (2)
The distractors should have a strong interference
ability to improve the reasoning performance of
the model. It needs to have a semantic relationship
with the question while also clearly distinguished
from the correct answer. The following are the pro-
posed generation methods based on these criteria.
Compositive Commonsense QA Generation.
Compositive commonsense QA is generated by
triples with the logical relationship in the knowl-
edge graph. This is the most common form
of multi-hop commonsense knowledge, which
shows the continuous reasoning ability of humans.
For example, when you see the word “univer-
sity”, you will think of “library”, and then think
of “books”. Our proposed generation method
is as follows, divided into two steps. (1) Gen-
erating question and answer. Formally, given
a triple (ehead
1;r1;etail
1), we ﬁnd another triple
(ehead
2;r2;etail
2), whileetail
1=ehead
2=ekey. Each
triple will be converted into a sentence through
the natural language template. Then, we integrate
these two sentences and mask the connected enti-
tiesekeyin them as the question Q. Meanwhile,
entityekeyis also the correct answer Ac.(2) Gen-
erating hard negative samples (distractors). We
generate distractors following the above criteria as
follows. Formally, for each distractor entity e3,
in order to ensure it is relevant to the question,
it needs to satisfy the condition: (ehead
1;r1;e3).
Meanwhile, to ensure that it is an error option,
it needs to satisfy the condition: a): e36=ekey,
b): for any (e3;r2;etail
3);etail
36=etail
2. We take
e3as a distractor Aiand select the distractor set
fA1;;Angin this way. Finally, we can get a
compositive commonsense QA pairs (Q;A)sam-
ple. Figure 2 (a) shows an example of this process.
Conjunctive Commonsense QA Generation.
Conjunctive commonsense QA is generated by
entities with multiple attributes in the knowledge
graph, which shows that human thinking ability is
multi-faceted. For example, when you see a “gym”,
you think of “basketball”, “football”, and othersports simultaneously. We also generate QA in
two steps. (1) Generating question and answer.
Formally, given a triple (ehead
1;r1;etail
1), we ﬁnd
another triple (ehead
1;r2;etail
2), whileetail
16=etail
2
andehead
1 =ekey. Each triple will be converted
into a sentence through the natural language tem-
plate. Then, we integrate these two sentences and
mask the connected entities ekeyin them as the
questionQ. Meanwhile, entity ekeyis also the
correct answer Ac.(2) Generating hard negative
samples (distractors). Formally, for each distrac-
tor entitye3, in order to ensure it is relevant to
the question, it needs to satisfy one of the follow-
ing conditions: a) (e3;r1;etail
1), b)(e3;r2;etail
2).
Meanwhile, to ensure that it is an error option, for
the above two conditions, only one of them can be
satisﬁed and the other can not be satisﬁed. We take
e3as a distractor Aiand select the distractor set
fA1;;Angin this way. Finally, we can get a
conjunctive commonsense QA pairs (Q;A)sample.
Figure 2 (b) shows an example of this process.
By the above method, we generate the synthetic
multi-hop QA dataset and merge it with the single-
hop QA dataset from the previous work (Ma et al.,
2021) for the subsequent pre-training task.
3.2 Knowledge Injection Pre-training
The overview of our knowledge injection pre-
training method is shown in Figure 3. We
use the synthetic multi-hop QA dataset for pre-
training to inject multi-hop commonsense knowl-
edge. Speciﬁcally, we use the masked language
model, RoBERTa (Liu et al., 2019), for pre-training.
Given a synthesized QA pairs (Q;A)sample,
whereA=fAc;A1;;Ang. We concatenate
questionQwith each answer option Aito build the
the input QA sequence T=fTP
0;TN
1;;TN
ng,
whereTP
0= [Q;Ac]is the positive QA sample and
TN
i= [Q;Ai]is the negative QA sample. For each
input sample, we mask each token one by one and
compute the sum of their MLM losses as the score
Sas following:
S(Ti) = 1
mmX
j=1logP(:::t j 1;tj+1;:::;)
(1)
wheretjis a word token in the input QA sequence
Ti,mis the length of Ti,Pis the conditional prob-
ability from masked language model parameterized
by.
After getting the scores of all samples, our pur-
pose is to make the positive sample have the lowest

--- PAGE 5 ---
Masked Language Model
[mask] canbefound atgymnasium andcanbeused forexercise .
trampoline canbefound at[mask] andcanbeused forexercise .
trampoline canbefound atgymnasium andcanbeused for[mask] .trampoline gymnasium exercisePositive MLM Score
Masked Language Model
[mask] room canbefound atgymnasium andcanbeused forexercise .
locker [mask] canbefound atgymnasium andcanbeused forexercise .
locker room can befound at[mask] and canbeused forexercise .
locker room canbefound atgymnasium andcanbeused for[mask] .locker gymnasium exerciseNegative MLM Score
roomInfoNCE Loss
Positive QA sample
Negative QA sampleFigure 3: The overview of Knowledge injection pre-training process. We concatenate question with each answer
option to build positive and negative samples. Then we mask the tokens one by one and compute the MLM loss as
the score of samples. Finally, we use InfoNCE loss from contrastive learning to optimize the model.
score and make the negative sample score higher
than that of the positive sample. Here we use the
idea of contrastive learning to optimize the model.
To this end, we invert all values and use Softmax
for normalization. In this way, the positive sample
should have a score of 1, and the negative sampless
have a score of 0. We use InfoNCE loss (van den
Oord et al., 2018), a form of a contrastive loss func-
tion, to optimize the model as following:
L= logexp 
 S(TP)=
Pn
i=0exp ( S(Ti)=)(2)
whereis a temperature hyper-parameter. The sum
is over one positive QA sample and nnegative QA
samples.
In the evaluation of the model, we construct the
input QA samples in the same way, and calculate
the score of each answer option according to Equa-
tion 1, and ﬁnally choose the correct answer with
the lowest score.
4 Experiments
In this section, We evaluate our framework on ﬁve
commonsense question answering benchmarks and
conduct ablation experiments to verify the effec-
tiveness of each part of our framework.
4.1 Experimental Settings
Our framework is under the zero-shot setting, that
is, the model cannot have access to any ofﬁcialtraining data of any benchmarks during the training
process. For the evaluation, we use the validation
set of each benchmark (some test sets are not pub-
licly available). Since the validation set is not used
for hyperparameter tuning here, it can be regarded
as the test set.
4.2 Benckmarks
We evaluate our proposed framework on ﬁve
question-answering benchmarks for commonsense
reasoning.
CommonsenseQA (CSQA) (Talmor et al.,
2019): It is a multiple choice question QA
dataset that requires conceptual commonsense
knowledge for reasoning. Questions and answers
are artiﬁcially generated according to entities in
ConceptNet and their relations.
Multiple Choice Temporal commonsense (MC-
TACO) (Zhou et al., 2019): This dataset requires
temporal commonsense comprehension, such as du-
ration, temporal ordering, typical time, frequency,
and stationarity.
Abductive NLI (a-NLI) (Bhagavatula et al.,
2020): Abductive reasoning is inference to the most
plausible explanation. Each sample is a real event,
which has two potential explanations for the given
situation.

--- PAGE 6 ---
PhysicalIQA (PIQA) (Bisk et al., 2020): This is
a dataset about physical commonsense knowledge
(e.g., a bucket can hold paint). It needs to select the
solution for the given goal among two choices.
Wino-Grande (WG) (Sakaguchi et al., 2021). It
is a dataset that is crowdsourced with a carefully
designed procedure to improve the scale and robust-
ness against the dataset-speciﬁc bias. It is formu-
lated as a ﬁll-in-a-blank task with binary options.
4.3 Knowledge Graphs
In our framework, we use four KGs: ATOMIC
(Sap et al., 2019b), ConceptNet (Speer et al., 2017),
WikiData (Vrandecic and Krötzsch, 2014) and
WordNet (Miller, 1995). Each KG has different
types of knowledge. ATOMIC focuses on social
commonsense knowledge, such as the cause or
follow-up of an event. ConceptNet contains general
conceptual commonsense knowledge, describing
the relationship between a conceptual entity and
another conceptual entity. WikiData is a general
KG collected by Wikipedia. WordNet is a lexical
database of semantic relations between words. In
the synthetic single-hop QA generation, we follow
the work of Ma et al. (2021) and use the above
four KGs for generation. In our synthetic multi-
hop QA generation, we choose ConceptNet as the
data source for generation, because ConceptNet
has an obvious multi-hop relationship compared
with other KGs.
4.4 Implementation
Our framework is implemented with PyTorch and
RoBERTa-Large (Liu et al., 2019) from the Hug-
ging face Transformers library. In our experiments,
we use max sequence length 128, warm-up propor-
tion 0.05, weight decay 0.01, adam 10.9, adam
20.98, adam epsilon 1e 6, batch size 2 per GPU,
for only 1 epoch in the synthetic multi-hop QA
dataset. The training process is conducted on 4 *
NVIDIA Tesla T4 (15G) and costs about 12 hours
in total. We run our experiments with different
random seeds.
In the synthetic multi-hop QA generation pro-
cess, we generate two negative samples for each
QA. In the knowledge injection pre-training pro-
cess, we randomly sample 95% of the synthetic
QA dataset for training while the remaining 5% are
used for validation. The temperature in InfoNCE
loss is set to 0.7.4.5 Baselines
We compare our framework with the following
baselines. Random method is to randomly take
the label as the answer and Majority method is to
take the most frequent label as the answer. We take
the pre-trained language model, RoBERTa-Large
(Liu et al., 2019) and GPT2-Large (Radford et al.,
2019) without ﬁne-tuning, for comparison.
Self-talk (Shwartz et al., 2020) combines con-
text and pre-deﬁned template preﬁxes, which are
used LM to generate clariﬁcation prompts, and then
elicit knowledge from another LM. Finally, it eval-
uates each answer candidate with the original text
and generated knowledge.
SMLM (Banerjee and Baral, 2020) is a method
based on knowledge triplet learning, which masks
any one of the triplets and use the remaining two
for inference.
RoBERTa-L(MR) (Ma et al., 2021) uses differ-
ent KG to generate the synthetic single-hop QA
dataset and uses marginal ranking loss to train the
RoBERTa model.
Zero-shot fusion (Adapter) (Kim et al., 2022)
uses different KG to generate the synthetic single-
hop QA dataset separately. It introduces an expert
adapter for each dataset for training, and ﬁnally
fuses these expert adapters for evaluation.
4.6 Main Results
Table 1 shows the zero-shot evaluation results on
ﬁve commonsense tasks. Our framework achieves
the best performance across all baseline models.
In particular, our framework greatly improves the
accuracy by ~2.8% and ~5.3% in the CSQA and
MC-TACO benchmarks respectively.
It is worth noting that though our framework has
a slight improvement on some benchmarks com-
pared to zero-shot fusion (Adapter) (Kim et al.,
2022), they introduce expert adapters for each KG
separately, which increase the number of param-
eters of the model. And they train more epochs
on each separate synthetic dataset, using more syn-
thetic data (since they didn’t ﬁlter the data). This
means their method is not friendly used in the low-
resource computing setting. RoBERTa-L (MR)
(Ma et al., 2021), as a method with the same model
architecture as ours, comparing with it can better
reﬂect the effectiveness of our framework. It can be
clearly observed that our framework is signiﬁcantly

--- PAGE 7 ---
Model KG CSQA MC-TACO a-NLI PIQA WG
Random - 20.0 - 50.0 50.0 50.0
Majority - 20.9 - 50.8 50.5 50.4
GPT2-Large - 41.4 64.5 56.5 68.9 53.2
RoBERTa-Large - 45.0 57.7 65.5 67.6 57.5
Self-talk (Shwartz et al., 2020) - 32.4 59.9 - 70.2 54.7
SMLM (Banerjee and Baral, 2020) * 38.8 - 65.3 - -
RoBERTa-L (MR) (Ma et al., 2021) AT 64.2 - - 72.1 59.2
RoBERTa-L (MR) (Ma et al., 2021) CN,WD,WN - - 70.0 72.0 59.4
RoBERTa-L (MR) (Ma et al., 2021) Whole 67.4 65.6 70.5 72.4 60.9
Zero-shot fusion (Adapter) (Kim et al., 2022) Whole 68.2 - 72.5 72.9 60.8
Multi-hop Knowledge Inject Framework Whole 71.0(0.3) 70.9(1.0) 72.5(0.3) 73.1(0.3) 61.0(0.3)
Table 1: Zero-shot evaluation results with different combinations of models and knowledge sources across ﬁve
commonsense tasks. The best results of each benchmark are in bold. AT, CN, WD and WN represent ATOMIC,
ConceptNet, WikiData and WordNet, respectively. Whole represents the combination of AT, CN, WD and WN.
SMLM (*) used different KGs for the different benchmark.
Methods CSQA
Multi-hop Knowledge Inject Framework 71.0
w/o hard negative answer 69.7
w/o compositive QA 70.2
w/o conjunctive QA 70.6
w/o multi-hop knowledge 68.4
w/o InfoNCE loss; +margin loss 70.2
Table 2: Ablation study results. We report the accuracy
on CommonsenseQA. The middle rows are ablation ex-
periments for synthetic multi-hop QA generation and
the bottom row is the ablation experiments for knowl-
edge injection pre-training.
improved compared to RoBERTa-L (MR) on mul-
tiple benchmarks. This also reﬂects that our pro-
posed multi-hop knowledge injection framework
can enable the model to learn multi-hop knowl-
edge.
4.7 Ablation Study
Synthetic Multi-hop QA Generation. To ex-
plore the impact of the proposed synthetic multi-
hop QA dataset, we conduct multiple ablation ex-
periments. 1) w/o hard negative answer: remove
hard negative samples and replace them with ordi-
nary negative samples (randomly select entities as
negative samples). 2) w/o compositive QA: remove
compositive commonsense QA generation method.
3) w/o conjunctive QA: remove conjunctive com-
monsense QA generation method. 4). w/o multi-
hop knowledge: remove all multi-hop common-
sense knowledge. Table 2 shows the ablation study
results. We ﬁnd that removing the hard negative
answers will signiﬁcantly degrade performance by
1.3%, which proves that our negative sample selec-tion method is effective. Removing the compositive
QA generation method or the conjunctive QA gen-
eration method will reduce the accuracy, demon-
strating that the QA samples generated by these
two methods have different aspects of multi-hop
knowledge. Removing all multi-hop knowledge
will signiﬁcantly degrade the performance, which
shows that our framework overcomes the shortcom-
ings of QA methods generated from single-hop
triplet data.
Knowledge Injection Pre-training. We con-
duct ablation experiments on the knowledge in-
jection pre-training method. We remove InfoNCE
loss and use margin loss for evaluation. Table 2
shows that the performance of the model drops
slightly, which also proves the effectiveness of our
framework. Meanwhile, we train the model with
different temperatures and evaluate them on the
benchmarks as shown in Figure 4. The temperature
corresponding to the best results of most datasets
is between 0.6 and 0.8, except for the WG. We
analyze that due to the small number of negative
samples we select (two negative samples), our tem-
peratureis relatively high. Since the WG dataset
involves more social commonsense knowledge and
less conceptual commonsense knowledge, it is not
sensitive to multi-hop commonsense knowledge
and negative samples.
4.8 Case Study
Table 3 shows some cases from different datasets,
where we compare predictions of our framework
with the RoBERTa (LM) (Ma et al., 2021). We ob-
serve that RoBERTa (LM) makes incorrect choices
on some options that are highly confusing. For

--- PAGE 8 ---
Dataset ExampleRoBERTa(MR)  
PredictionOur Prediction
CSQAQ: James wanted to find an old underground map from the 50s.  
Where might he look for one?
A. library  B. subway station   C. county engineer's office  D. super marketB. subway station (✗) A. library (✓)
a-NLIContext: Cosmo was a pudgy cat.
Q: Now he's fit and muscular!
A. His owner gave him a lower fat cat food.
B. The vet put Cosmo on a treadmill.B. The vet put Cosmo 
on a treadmill.
(✗)A. His owner gave him 
a lower fat cat food.
(✓)
PIQAGoal: To smooth mayo onto raw chicken, you can
A. Use a toothpick to smooth the mayo
B. Use a spoon to smooth the mayoA. Use a toothpick to 
smooth the mayo
(✗)B. Use a spoon to 
smooth the mayo
(✓)
WGThey had to eat a lot to gain the strength they had lost and be able to work, 
the ___ was too much.
A. work        B. strengthB. strength   (✗) A. work (✓)Table 3: Cases from different datasets. Compared with Roberta(MR), our framework can obtain more knowledge
and perform better on different datasets.
0.2 0.4 0.6 0.8 1.0
temperature60626466687072accuracy
anli
commonsense
mctaco
piqa
winogrande
Figure 4: Result with different temperatures across ﬁve
commonsense tasks.
example, in the CSQA dataset, for the question
“James wanted to ﬁnd an old underground map from
the 50s. Where might he look for one?” RoBERTa
(LM) only captures the relationship between “un-
derground” and “subway station”, so it chooses
the wrong answer. Our model captures the rela-
tionship between “old”, “underground map”, and
“library”, so it chooses the right answer. We also
show other examples in the table to illustrate the
effectiveness of our framework. These cases show
that our framework can overcome the lack of multi-
hop knowledge of previous models by injecting the
multi-hop knowledge generated by our synthetic
Multi-hop QA dataset.
5 Conclusion
In this paper, we propose a novel multi-hop com-
monsense knowledge injection framework for zero-
shot commonsense question answering task to ad-
dress the lack of multi-hop knowledge reasoningability in the current zero-shot framework. Spe-
cially, this framework is divided into two steps:
synthetic multi-hop QA generation and knowledge
injection pre-training. In the process of QA syn-
thesis, we explore the linguistically logical multi-
hop reasoning patterns existing in KGs, and pro-
pose two QA generation methods based on KGs.
Then, in the process of knowledge injection pre-
training, we use the synthetic multi-hop QA dataset
for pre-training with contrastive learning to inject
multi-hop knowledge. We conduct extensive exper-
iments on ﬁve commonsense question answering
benchmarks, the results show that our framework
achieves state-of-the-art performance. Ablation
experiments and case study also conﬁrm the effec-
tiveness of our knowledge injection framework.
Limitations
Although we have explored the multi-hop com-
monsense knowledge injection framework, we uti-
lize one KG, ConceptNet. Since there is no ex-
plicit multi-hop relationship for other KGs, such as
Atomic, we do not apply them. Therefore, explor-
ing the implicit multi-hop relations in other KGs
can be our future work. In addition, our synthetic
multi-hop QA generation method applies entities
and their relationships within two hops of an entity
node as the center, and does not consider the rest
of the nodes beyond two hops. For these nodes,
we can consider designing new methods to utilize
them to enrich more multi-hop knowledge.

--- PAGE 9 ---
Ethics Statement
Commonsense question answering is an impor-
tant ﬁeld in question answering. The datasets and
knowledge graphs used in our work are all public
data, and the models we use are also public models
in the ﬁeld. Since our framework uses data from
existing datasets and KGs, it may also inherit the
social biases present in these underlying datasets.
Our work is conform to to ACL Code of Ethics.
References
Pratyay Banerjee and Chitta Baral. 2020. Self-
supervised knowledge triplet learning for zero-shot
question answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2020, Online, November
16-20, 2020 , pages 151–162. Association for Com-
putational Linguistics.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen-tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020 . OpenReview.net.
Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng
Gao, and Yejin Choi. 2020. PIQA: reasoning about
physical commonsense in natural language. In The
Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of Artiﬁcial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020 , pages 7432–
7439. AAAI Press.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human LanguageTechnologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers) , pages 4171–4186. Association for Computa-
tional Linguistics.
Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng
Wang, Jun Yan, and Xiang Ren. 2020. Scalable
multi-hop relational reasoning for knowledge-aware
question answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2020, Online, Novem-
ber 16-20, 2020 , pages 1295–1309. Association for
Computational Linguistics.
Xin Guan, Biwei Cao, Qingqing Gao, Zheng Yin,
Bo Liu, and Jiuxin Cao. 2022. CORN: co-
reasoning network for commonsense question an-
swering. In Proceedings of the 29th International
Conference on Computational Linguistics, COLING
2022, Gyeongju, Republic of Korea, October 12-17,
2022 , pages 1677–1686. International Committee on
Computational Linguistics.
Yu Jin Kim, Beong-woo Kwak, Youngwook Kim,
Reinald Kim Amplayo, Seung-won Hwang, and
Jinyoung Yeo. 2022. Modularized transfer learning
with multiple knowledge graphs for zero-shot com-
monsense reasoning. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL 2022, Seattle, WA,
United States, July 10-15, 2022 , pages 2244–2257.
Association for Computational Linguistics.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xi-
ang Ren. 2019. Kagnet: Knowledge-aware graph
networks for commonsense reasoning. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing, EMNLP-IJCNLP 2019, Hong Kong, China,
November 3-7, 2019 , pages 2829–2839. Association
for Computational Linguistics.
Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-
ter West, Ronan Le Bras, Yejin Choi, and Hannaneh
Hajishirzi. 2022. Generated knowledge prompting
for commonsense reasoning. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022 , pages
3154–3169. Association for Computational Linguis-
tics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan
Bisk, Eric Nyberg, and Alessandro Oltramari. 2021.
Knowledge-driven data construction for zero-shot
evaluation in commonsense question answering. In

--- PAGE 10 ---
Thirty-Fifth AAAI Conference on Artiﬁcial Intelli-
gence, AAAI 2021, Thirty-Third Conference on In-
novative Applications of Artiﬁcial Intelligence, IAAI
2021, The Eleventh Symposium on Educational Ad-
vances in Artiﬁcial Intelligence, EAAI 2021, Vir-
tual Event, February 2-9, 2021 , pages 13507–13515.
AAAI Press.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM , 38(11):39–41.
Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal,
Swaroop Mishra, and Chitta Baral. 2019. Explor-
ing ways to incorporate additional knowledge to im-
prove natural language commonsense question an-
swering. CoRR , abs/1909.08855.
Yilin Niu, Fei Huang, Jiaming Liang, Wenkai Chen, Xi-
aoyan Zhu, and Minlie Huang. 2021. A semantic-
based method for unsupervised commonsense ques-
tion answering. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, Au-
gust 1-6, 2021 , pages 3037–3049. Association for
Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog , 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1–140:67.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2021. Winogrande: an adversar-
ial winograd schema challenge at scale. Commun.
ACM , 64(9):99–106.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A. Smith, and Yejin Choi.
2019a. ATOMIC: an atlas of machine commonsense
for if-then reasoning. In The Thirty-Third AAAI Con-
ference on Artiﬁcial Intelligence, AAAI 2019, The
Thirty-First Innovative Applications of Artiﬁcial In-
telligence Conference, IAAI 2019, The Ninth AAAI
Symposium on Educational Advances in Artiﬁcial
Intelligence, EAAI 2019, Honolulu, Hawaii, USA,
January 27 - February 1, 2019 , pages 3027–3035.
AAAI Press.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le
Bras, and Yejin Choi. 2019b. Social iqa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing, EMNLP-IJCNLP 2019, Hong
Kong, China, November 3-7, 2019 , pages 4462–
4472. Association for Computational Linguistics.Vered Shwartz, Peter West, Ronan Le Bras, Chandra
Bhagavatula, and Yejin Choi. 2020. Unsupervised
commonsense question answering with self-talk. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2020, Online, November 16-20, 2020 , pages 4615–
4629. Association for Computational Linguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. In Proceedings of the Thirty-First
AAAI Conference on Artiﬁcial Intelligence, Febru-
ary 4-9, 2017, San Francisco, California, USA ,
pages 4444–4451. AAAI Press.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. Commonsenseqa: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers) , pages 4149–4158. Association for Computa-
tional Linguistics.
Alexandre Tamborrino, Nicola Pellicanò, Baptiste Pan-
nier, Pascal V oitot, and Louise Naudin. 2020. Pre-
training is (almost) all you need: An application
to commonsense reasoning. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2020, Online, July 5-10,
2020 , pages 3878–3887. Association for Computa-
tional Linguistics.
Trieu H. Trinh and Quoc V . Le. 2018. A sim-
ple method for commonsense reasoning. CoRR ,
abs/1806.02847.
Aäron van den Oord, Yazhe Li, and Oriol Vinyals.
2018. Representation learning with contrastive pre-
dictive coding. CoRR , abs/1807.03748.
Denny Vrandecic and Markus Krötzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commun.
ACM , 57(10):78–85.
Jiawei Wang and Hai Zhao. 2022. ArT: All-round
thinker for unsupervised commonsense question an-
swering. In Proceedings of the 29th International
Conference on Computational Linguistics , pages
1490–1501, Gyeongju, Republic of Korea. Interna-
tional Committee on Computational Linguistics.
Michihiro Yasunaga, Hongyu Ren, Antoine Bosse-
lut, Percy Liang, and Jure Leskovec. 2021. QA-
GNN: reasoning with language models and knowl-
edge graphs for question answering. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2021,
Online, June 6-11, 2021 , pages 535–546. Associa-
tion for Computational Linguistics.

--- PAGE 11 ---
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Roth. 2019. "going on a vacation" takes longer
than "going for a walk": A study of temporal com-
monsense understanding. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, Novem-
ber 3-7, 2019 , pages 3361–3367. Association for
Computational Linguistics.
