# 2309.01538.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2309.01538.pdf
# File size: 841987 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ChatRule : Mining Logical Rules with Large Language Models for
Knowledge Graph Reasoning
Linhao Luo1,Jiaxin Ju2,Bo Xiong3,Yuan-Fang Li1,Gholamreza Haffari1,Shirui Pan2∗
1Monash University,2Griffith University,3University of Stuttgart
{linhao.luo, yuanfang.li, Gholamreza.Haffari }@monash.edu,
jiaxin.ju@griffithuni.edu.au, bo.xiong@ipvs.uni-stuttgart.de, s.pan@griffith.edu.au
Abstract
Logical rules are essential for uncovering the log-
ical connections between relations, which could
improve reasoning performance and provide inter-
pretable results on knowledge graphs (KGs). Al-
though there have been many efforts to mine mean-
ingful logical rules over KGs, existing methods suf-
fer from computationally intensive searches over
the rule space and a lack of scalability for large-
scale KGs. Besides, they often ignore the seman-
tics of relations which is crucial for uncovering log-
ical connections. Recently, large language mod-
els (LLMs) have shown impressive performance in
the field of natural language processing and various
applications, owing to their emergent ability and
generalizability. In this paper, we propose a novel
framework, ChatRule , unleashing the power of
large language models for mining logical rules over
knowledge graphs. Specifically, the framework is
initiated with an LLM-based rule generator, lever-
aging both the semantic and structural information
of KGs to prompt LLMs to generate logical rules.
To refine the generated rules, a rule ranking mod-
ule estimates the rule quality by incorporating facts
from existing KGs. Last, the ranked rules can be
used to conduct reasoning over KGs. ChatRule
is evaluated on four large-scale KGs, w.r.t. different
rule quality metrics and downstream tasks, showing
the effectiveness and scalability of our method1.
1 Introduction
Knowledge graphs (KGs) store enormous real-world knowl-
edge in a structural format of triples. KG reasoning, which
aims to infer new knowledge from existing facts, is a fun-
damental task in KGs and essential for many applications,
such as KG completion [Qu and Tang, 2019 ], question-
answering [Atif et al. , 2023 ], and recommendation [Wang
et al. , 2019 ]. Recently, there has been an increasing need
for interpretable KG reasoning, which can help users un-
derstand the reasoning process and improve the trustwor-
thiness in high-stake scenarios, such as medical diagnosis
∗Corresponding author.
1Code is available at: https://github.com/RManLuo/ChatRule
Knowledge Graphs (KGs)
AliceBob
CharlieAlex
MotherFather Sister
TomSonUncle
Grandmother?
Logical Rule
MiningLogical
Reasoning(Alice, Grandmother ,
Charlie)New Facts
Logical Rules
Figure 1: Illustration of mining logical rules for knowledge graphs
reasoning with LLMs.
[Liuet al. , 2021 ]and legal judgment [Zhong et al. , 2020 ].
Therefore, logical rules [Barwise, 1977 ], which are human-
readable and can be generalized to different tasks, have been
widely adopted for KG reasoning [Hou et al. , 2021; Liu et al. ,
2022 ]. For example, as shown in Figure 1, we can identify a
logical rule: GrandMother (X, Y)←Mother (X, Z)∧
Father (Z, Y)to predict missing facts for relation “Grand-
Mother”. To automatically discover meaningful rules from
KGs for reasoning, logical rule mining has gained signifi-
cant attention in the research community [Yang et al. , 2017;
Sadeghian et al. , 2019 ].
Earlier studies on logical rule mining usually find logi-
cal rules by discovering the co-occurrences of frequent pat-
terns in KG structure [Gal´arraga et al. , 2013; Chen et al. ,
2016 ]. However, they usually require the enumeration of
all possible rules on KGs and ranking them by estimated
importance [Lao and Cohen, 2010 ]. Although recent re-
search has proposed to use deep learning methods to rank the
rules. They are still limited by the exhaustive enumeration of
rules and cannot scale to large-scale KGs [Yang et al. , 2017;
Sadeghian et al. , 2019 ].
Some recent methods address this issue by sampling paths
from KGs and training models on them to capture the log-
ical connections that form rules [Quet al. , 2020; Cheng
et al. , 2022b; Cheng et al. , 2022a ]. But they usually ig-
nore contributions of relation semantics for expressing logi-
cal connections. For example, in commonsense, we know the
“mother” of a person’s “father” is his “grandmother”. Based
on this, we can define a rule like GrandMother (X, Y)←arXiv:2309.01538v3  [cs.AI]  22 Jan 2024

--- PAGE 2 ---
Mother (X, Z)∧Father (Z, Y)to express the logical con-
nection. Whereas, due to the number of relations in KGs, it
could be burdensome to ask domain-experts to annotate rules
for each relation. Therefore, it is essential to automatically
incorporate both the structure and semantics of relations to
discover logical rules in KGs.
Large language models (LLMs) such as ChatGPT2and
BARD3exhibit great ability in understanding natural lan-
guage and handling many complex tasks [Zhao et al. , 2023 ].
Trained on large-scale corpora, LLMs store a great amount
of commonsense knowledge that can be used to facilitate KG
reasoning [Panet al. , 2023 ]. At the same time, LLMs are not
designed to understand the structure of KGs, making it diffi-
cult to directly apply them to mining logical rules over KGs.
Moreover, the widely acknowledged hallucination problem
could make LLMs generate meaningless logical rules [Jiet
al., 2023 ].
To mitigate the gap between LLMs and logical rule min-
ing, we propose a novel framework called ChatRule , which
leverages both the semantic and structural information of
KGs to prompt LLMs to generate logical rules. Specifically,
we first present an LLM-based rule generator to generate
candidate rules for each relation. We sample some paths from
KGs to represent the structural information, which are then
used in a carefully designed prompt to leverage the capabil-
ities of LLMs for rule mining. To reduce the hallucination
problem, we design a logical rule ranker to evaluate the qual-
ity of generated rules and filter out meaningless rules by en-
compassing the observed facts in KGs. The quality scores
are further used in the logical reasoning stage to reduce the
impact of low-quality rules. Lastly, we feed the ranked rules
into a logical reasoning module to conduct interpretable rea-
soning over KGs. In our framework, the mined rules can be
directly used for downstream tasks without any model train-
ing. Extensive experiments on four large-scale KGs demon-
strate that ChatRule significantly outperforms state-of-the-
art methods on both knowledge graph completion and rule
quality evaluation.
The main contributions of this paper are summarized as
follows:
• We propose a framework called ChatRule that lever-
ages the advantage of LLMs for mining logical rules. To
the best of our knowledge, this is the first work that ap-
plies LLMs for logical rule mining.
• We present an end-to-end pipeline that utilizes the rea-
soning ability of LLMs and structure information of
KGs for rule generation, rule ranking, and rule-based
logical reasoning.
• We conduct extensive experiments on four datasets. Ex-
periment results show that ChatRule significantly out-
performs state-of-the-art methods.
2https://openai.com/blog/chatgpt
3https://bard.google.com/2 Related Works
2.1 Logical Rule Mining
Logical rule mining, which focuses on extracting meaning-
ful rules from KGs, has been studied for a long time. Tra-
ditional methods enumerate the candidate rules, then access
the quality of them by calculating weight scores [Lao and
Cohen, 2010; Gal ´arraga et al. , 2013 ]. With the advance-
ment of deep learning, researchers explore the idea of si-
multaneously learning logic rules and weights in a differ-
entiable manner [Yang et al. , 2017; Sadeghian et al. , 2019;
Yang and Song, 2020 ]. However, these methods still conduct
heavy optimization on the rule space, which limits their scal-
ability. Recently, researchers have proposed to sample paths
from KGs and train models on them to learn the logical con-
nections. RLvLR [Omran et al. , 2018 ]samples rules from a
subgraph and proposes an embedding-based score function to
estimate the importance of each rule. RNNLogic [Quet al. ,
2020 ]separates the rule generation and rule weighting, which
can mutually enhance each other and reduce the search space.
R5[Luet al. , 2021 ]proposes a reinforcement learning frame-
work that heuristically searches over KGs and mines under-
lying logical rules. NCRL [Cheng et al. , 2022a ]predicts the
best composition of rule bodies to discover rules. Ruleformer
[Xuet al. , 2022 ]adopts a transformer-based model to en-
code context information and generate rules for the reasoning
tasks, which is the state-of-the-art method in this area. Nev-
ertheless, existing methods do not consider the semantics of
relations and could lead to a suboptimal result.
2.2 Large Language Models
Large language models (LLMs) are revolutionizing the field
of natural language processing and artificial intelligence.
Many LLMs (e.g., ChatGPT2, Bard3, FLAN [Wei et al. ,
2021 ], and LLaMA [Touvron et al. , 2023 ]) have demon-
strated strong ability in various tasks. Recently, researchers
have also explored the possibility of applying LLMs to ad-
dress KG tasks [Panet al. , 2023; Luo et al. , 2023 ]. To bet-
ter access the potential of LLMs, researchers design some
prompts with few-shot examples [Brown et al. , 2020 ]or
chain-of-thought reasoning [Wei et al. , 2022 ]to maximize
their ability. Nevertheless, these methods are not designed
for logical rule mining, which requires the LLMs to under-
stand both the structure of KGs and semantics of relations for
generating meaningful rules.
3 Preliminary and Problem Definition
Knowledge Graphs (KGs) represent collections of facts in a
form of triples G={(e, r, e′)⊆ E ×R×E} , where e, e′∈ E
andr∈ R respectively denote the set of entities and relations.
Logical Rules are special cases of first-order logic [Barwise,
1977 ], which could facilitate interpretable reasoning on KGs
[Yang et al. , 2017 ]. Logical rules ρstate the logical implica-
tion in the following form
ρ:=rh(X, Y)←r1(X, Z 1)∧ ··· ∧ rL(ZL−1, Y),(1)
where body (ρ) := r1(X, Z 1)∧ ··· ∧ rL(ZL−1, Y)de-
notes the conjunction of a series of relations called rule body ,
rh(X, Y)denotes the rule head , and Ldenotes the length of

--- PAGE 3 ---
Rule SamplerKnowledge Graphs (KGs)
Rule Samples
Prompt: Based on sampled
rules, please generate
important logical rules for
relation .Multi Queries
Candidate RulesKGs Ranked RulesLogical ReasoningNew Facts
①
②LLM-based
Rule Generator④
Rule Ranker③Figure 2: The overall framework of ChatRule . 1) we first sample a few rule instances from the knowledge graph for a given target relation
rh. 2) we prompt the large language model (e.g., ChatGPT) to generate a set of coarse candidate rules. 3) we propose a rule ranker to
estimable the quality of the generated rules based on facts in KGs. 4) the final rules can be applied for logical reasoning and addressing
downstream tasks, such as knowledge graph completion.
rules. If the conditions on the rule body are satisfied, then the
statement on the rule head also holds.
An instance of the rule is realized by replacing the vari-
ables X, Y, Z ∗with actual entities in KGs. For example,
given a rule GrandMother (X, Y)←Mother (X, Z 1)∧
Father (Z1, Y), one rule instance δcould be
GrandMother (Alice,Charlie )←
Mother (Alice,Bob)∧Father (Bob,Charlie ),(2)
which means that if Alice is the mother of Bob and Bob is the
father of Charlie, then Alice is the grandmother of Charlie.
Problem Definition. Given a target relation rh∈ R as the
rule head, the goal of logical rule mining is to find a set of
meaningful rules Prh={ρ1,···, ρK}that capture the logi-
cal connections of other relations to express the target relation
rhin KGs.
4 Approach
In this section, we will introduce the proposed framework,
called ChatRule , for mining logical rules over KGs with
large language models. The overall framework is illustrated
in Figure 2, which contains three main components: 1) an
LLM-based rule generator that leverages both the semantics
and structural information to generate meaningful rules. 2) a
rule ranker to estimate the quality of generated rules on KGs,
and 3) a rule-based logical reasoning module to conduct rea-
soning over KGs for downstream tasks.
4.1 LLM-based Rule Generator
Conventional studies on logical rule mining usually focus
on using structural information, [Gal´arraga et al. , 2013;Cheng et al. , 2022a ], which ignores the contributions of rela-
tion semantics for expressing logical connections. To harness
the semantics understanding ability of large language models
(LLMs), we propose an LLM-based rule generator that lever-
ages both the semantics and structural information of KGs in
generating meaningful rules.
Rule Sampler
To enable LLMs to understand KG structures for rule min-
ing, we adopt a breadth-first search (BFS) sampler to sample
a few closed-paths from KGs, which can be treated as the
instances of logical rule [Omran et al. , 2018; Cheng et al. ,
2022b ]. Given a triple (e1, rh, eL), the closed-path is defined
as a sequence of relations r1,···, rLthat connects e1andeL
in KGs, i.e., e1r1− →e2r2− → ···rL− →eL. For example, given a
triple (Alice,GrandMother ,Charlie ), a closed-path pcan be
found as
p:=AliceMother− − − − → BobFather− − − − → Charlie , (3)
which closes the triple (Alice,GrandMother ,Charlie )in
KGs. By treating the triple as the rule head and closed-path
as the rule body, we can obtain the rule instance δshown in
Equation (2).
Given a target relation rh, we first select a set of seed triples
{(e, rh, e′)}from KGs, from which we conduct BFS to sam-
ple a set of closed-paths {p}with lengths less than Lto con-
stitute a set of rule instances {δ}. Following that, we substi-
tute the actual entities in the rule instances with variables to
obtain the rule samples Srh={ρ}. The rule samples con-
vey the structural information of KGs in a sequential format,
which can be fed into the large language model to facilitate
rule generation.

--- PAGE 4 ---
Logical rules define the relationship between
two entities: X and Y. Each rule is written in
the form of a logical implication, which states
that if the conditions on the right-hand side
(rule body) are satisfied, then the statement on
the left-hand side (rule head) holds true.
Now we have the following rule samples:
husband(X,Y) <-- inv_wife(X,Y)
husband(X,Y) <-- father(X,Z_1) & daughter(Z_1,Y)
...
husband(X,Y) <-- father(X,Z_1) & sister(Z_1,Z_2)
& daughter(Z_2,Y)
Based on the above rules, please generate as
many of the most important rules for the rule
head: "husband(X,Y) "Rule Generation
Prompt
husband(X,Y) <-- inv_wife(X,Y)
husband(X,Y) <-- father(X,Z_1) & daughter(Z_1, Y)
husband(X,Y) <-- nephew(X,Z_1) & aunt(Z_1,Y)
husband(X,Y) <-- husband(X,Z_1) & brother(Z_1, Y)
....Output
Figure 3: An example of the rule generation prompt and results of
LLMs for relation “ husband (X,Y)”.
LLM-based Rule Generation
Large language models (LLMs) trained on large-scale cor-
pora exhibit the ability to understand the semantics of natural
language and perform complex reasoning with commonsense
knowledge [Zhou et al. , 2020; Tan et al. , 2023 ]. To incor-
porate the structure and semantics information, we design a
meticulously crafted prompt to harness the ability of LLMs
for rule mining.
For each rule from Srhobtained by the rule sampler for a
target relation rh, we verbalize it into a natural-language sen-
tence by removing the special symbols in the relation names,
which could deteriorate the semantic understanding of LLMs.
For the inverse of an original relation (i.e., wife−1), we ver-
balize it by adding an “inv ” symbol. Then, we place the ver-
balized rule samples into the prompt template and feed them
into the LLMs (e.g., ChatGPT) to generate the rules. An ex-
ample of the rule generation prompt and results of LLMs for
relation “ husband (X,Y)” is shown in Figure 3. The detailed
prompt can be found in the Appendix .
Multi-Queries Rule Generation
Due to the large number of rule samples, they cannot all be
fed into the LLMs at once due to exceeding the context limita-
tion. Thus, we divide the rule samples into ddifferent queries.
Each query contains krule samples that are randomly se-
lected from Srh. Then we prompt LLMs with queries sep-
arately and gather the responses of LLMs to obtain a set of
candidate rules Crh={ρ}.4.2 Logical Rule Ranking
LLMs are known for having hallucination issues, which
could generate incorrect results [Jiet al. , 2023 ]. For
example, the generated rule husband (X, Y)←
husband (X, Z 1) & brother (Z1, Y), shown in
the results of Figure 3, is incorrect. Therefore, we develop
a rule ranker to detect the hallucination and estimate the
quality of generated rules based on facts in KGs.
The rule ranker aims to assign a quality score s(ρ)for each
ruleρin the candidate rule set Crh. Motivated by previous
rule mining works [Gal´arraga et al. , 2013 ], we employ four
measures, namely support, coverage, confidence, and PCA
confidence, to assess the quality of rules. A detailed intro-
duction and examples of each measure can be found in the
Appendix .
Support denotes the number of facts in KGs that satisfy the
ruleρ, which is defined as
supp(ρ) := #( e, e′) :∃(e, r 1, e2)∧,···,∧(eL−1, rL, e′) :
body (ρ)∧(e, rh, e′)∈ G,(4)
where (e1, r1, e2),···,(eL−1, rL, e′)denotes a series of
facts in KGs that satisfy rule body body (ρ)and(e, rh, e′)
denotes the fact satisfying the rule head rh.
Clearly, the rules that have zero support can be easily
pruned away from the candidate set without any further re-
finement. However, support is an absolute number that could
be higher for relations with more facts in KGs and provide
biased ranking results.
Coverage normalizes the support by the number of facts for
each relation in KGs, which is defined as
cove(ρ) :=supp(ρ)
#(e, e′) : (e, rh, e′)∈ G. (5)
The coverage quantifies the ratio of the existing facts in
KGs that are implied by the rule ρ. To further consider the
incorrect predictions of the rules, we introduce the confidence
and PCA confidence to estimate the quality of rules.
Confidence is defined as the ratio of the number of facts
that satisfy the rule ρand the number of times the rule body
body(ρ)is satisfied in KGs, which is defined as
conf(ρ) :=supp(ρ)
#(e, e′) :body (ρ)∈ G. (6)
The confidence assumes that all the facts derived from the
rule body should be contained in KGs. However, the KGs are
often incomplete in practice, which could lead to the missing
of evidence facts. Therefore, we introduce the PCA confi-
dence to select rules that could better generalize to unseen
facts.
PCA Confidence is based on the theory of partial complete-
ness assumption (PCA) [Gal´arraga et al. , 2013 ]which is de-
fined as the ratio of the number of facts that satisfy the rule ρ
and the number of times the rule body body (ρ)is satisfied in
the partial completion of KGs, which is defined as
partial (ρ)(e, e′) := ( e, r 1, e2)∧,···,∧(eL−1, rL,ˆe) :
body (ρ)∧(e, rh,ˆe),(7)
pca(ρ) :=supp(ρ)
#(e, e′) :partial (ρ)(e, e′)∈ G. (8)

--- PAGE 5 ---
The denominator of PCA confidence is not the size of the
entire set of facts derived from the rule body. Instead, it is
based on the number of facts that we know to be true along
with those that we assume to be false. Therefore, the PCA
confidence is better for estimating the quality and generaliz-
ability of rules in incomplete KGs. Experiment results in rule
quality evaluation also support this claim.
4.3 Rule-based Logical Reasoning
After logical rule ranking, we obtain a set of ranked rules
Rrh={(ρ, s(ρ))}for the target relation rh. The ranked rules
can be used for logical reasoning and addressing downstream
tasks, such as knowledge graph completion, by applying ex-
isting algorithms such as forward chaining [Salvat and Mug-
nier, 1996 ].
Given a query (e, rh,?), letAbe the set of candidate an-
swers. For each e′∈A, we can apply the rule in Prhto obtain
the score as
score(e′) =X
ρ∈RrhX
body (ρ)(e,e′)∈Gs(ρ), (9)
where body (ρ)(e, e′)denotes the path in the KGs that satis-
fies the rule body, and s(ρ)denotes the quality score of the
rule, which could be either converge, confidence, and PCA
confidence. Then, we can rank the candidate answers Abased
on the scores and select the top- Nanswers as the final re-
sults.
5 Experiment
5.1 Datasets
In the experiment, we select four widely used datasets fol-
lowing previous studies [Cheng et al. , 2022b ]: Family [Hin-
ton and others, 1986 ], UMLS [Kok and Domingos, 2007 ],
WN18RR [Dettmers et al. , 2018 ], and YAGO3-10 [Suchanek
et al. , 2007 ]. The statistics of the datasets are summarized in
Table 1.
5.2 Baselines
We compare our method against the SOTA rule mining base-
lines: AIME [Gal´arraga et al. , 2013 ], NeuralLP [Yang et al. ,
2017 ], RNNLogic [Quet al. , 2020 ], NLIL [Yang and Song,
2020 ], NCRL [Cheng et al. , 2022a ], and Ruleformer [Xu
et al. , 2022 ], on both knowledge graph completion and rule
quality evaluation tasks.
5.3 Metrics
For the knowledge graph completion task, we mask the tail
or head entity of each test triple and use the rule generated by
each method to predict it. Following previous studies [Cheng
et al. , 2022a ], we use the mean reciprocal rank (MRR) and
the Hits@ Nas the evaluation metrics and set Nto 1 and
10. For the rule quality evaluation task, we use the measures
(e.g., support, coverage, confidence, and PCA confidence)
discussed in the previous section on rule ranking.Table 1: Dataset statistics
Dataset #Triples #Relation #Entity
Family 28,356 12 3,007
UMLS 5,960 46 135
WN18RR 93,003 11 40,943
YAGO3-10 1,089,040 37 123,182
5.4 Experiment Settings
ForChatRule , we use the ChatGPT4as the LLMs for
rule generation. We select the PCA confidence as the fi-
nal rule ranking measure and set maximum rule lengths L
to 3. In the knowledge graph completion task, we follow
the same settings as previous studies [Cheng et al. , 2022b;
Cheng et al. , 2022a ]. For baselines, we use the publicized
official implantation to conduct experiments. Detailed dis-
cussions about the settings can be found in the Appendix .
5.5 Knowledge Graph Completion
Knowledge graph completion is a classical task that aims
to predict the missing facts by using rule-based logical rea-
soning. This task has been adopted by various existing rule
mining methods such as Neural-LP [Yang et al. , 2017 ], and
NCRL [Cheng et al. , 2022a ]to evaluate the quality of gen-
erated rules. We adopt rules generated by each method and
use the same rule-based logical reasoning presented in Sec-
tion 4.3 to predict the missing facts. The results are shown in
Table 2.
From the results, we can observe that ChatRule out-
performs the baselines on most datasets. Specifically, the
traditional method AIME, which only utilizes the structure
information with inductive logic programming, has already
achieved relatively good performance. However, AIME fails
in large-scale KGs (e.g., YAGO3-10) due to the increasing
number of relations and triples. Recent deep learning-based
methods (e.g., Neural-LP, RNNLogic, and NLIL) achieve
better performance by utilizing the learning ability of neural
networks as they optimize the models on rule learning tasks.
However, they suffer from the out-of-memory issue in han-
dling large KGs due to the extensive rule-searching space.
While NCRL samples close-paths to reduce the search space,
it still ignores the semantics of relations, which leads to a sub-
optimal performance. Similar to the architecture of LLMs,
Ruleformer adopts a transformer-based model to aggregate
the context information from KGs and generate rules in a
sequence-to-sequence manner, which achieves the second-
best performance. This also demonstrates the great potential
of the transformer architecture in logical rule mining. With
the help of powerful pre-trained LLMs, ChatRule can gen-
erate high-quality rules by incorporating the structure and se-
mantic information of KGs. ChatRule also sets new STOA
performance in most settings.
4We use the snapshot of ChatGPT taken from June 13th 2023
(gpt-3.5-turbo-0613) to ensure the reproducibility.

--- PAGE 6 ---
Table 2: Knowledge graph completion results. OOM denotes out-of-memory.
MethodsFamily UMLS WN18RR YAGO3-10
MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10
AMIE 0.778 0.683 0.891 0.312 0.195 0.560 0.162 0.060 0.278 0.012 0.008 0.021
Neural-LP 0.785 0.720 0.863 0.505 0.415 0.638 0.228 0.223 0.235 OOM OOM OOM
RNNLogic 0.860 0.792 0.957 0.750 0.630 0.924 0.216 0.183 0.275 OOM OOM OOM
NLIL 0.358 0.321 0.416 0.693 0.632 0.921 0.223 0.222 0.225 OOM OOM OOM
NCRL 0.826 0.725 0.963 0.728 0.576 0.938 0.316 0.272 0.397 0.234 0.181 0.334
Ruleformer 0.897 0.841 0.963 0.691 0.555 0.863 0.292 0.258 0.355 0.527 0.520 0.535
ChatRule (ChatGPT) 0.906 0.854 0.968 0.780 0.685 0.948 0.335 0.301 0.400 0.449 0.354 0.627
Table 3: Rule quality evaluation results.
MethodsFamily UMLS
Support Coverage Confidence PCA confidence Support Coverage Confidence PCA confidence
AMIE 243.90 0.11 0.17 0.30 35.52 0.10 0.13 0.21
Neural-LP 280.94 0.13 0.21 0.30 11.59 0.06 0.08 0.09
NCRL 179.96 0.09 0.12 0.16 13.25 0.03 0.04 0.06
Ruleformer 325.32 0.15 0.22 0.32 57.00 0.25 0.33 0.37
ChatRule (ChatGPT) 403.04 0.17 0.23 0.34 28.06 0.32 0.21 0.38
MethodsWN18RR YAGO3-10
Support Coverage Confidence PCA Confidence Support Coverage Confidence PCA confidence
AMIE 378.76 0.07 0.08 0.10 758.28 0.06 0.06 0.07
Neural-LP 285.24 0.05 0.06 0.09 - - - -
NCRL 400.38 0.18 0.02 0.03 12660.16 0.13 0.12 0.05
Ruleformer 325.32 0.15 0.22 0.32 495.79 0.15 0.22 0.22
ChatRule (ChatGPT) 667.28 0.12 0.09 0.16 13656.41 0.27 0.09 0.14
5.6 Rule Quality Evaluation
To further demonstrate the effectiveness of the four measures
(i.e., support, coverage, confidence, and PCA confidence)
adopted in the rule ranking, we use them to evaluate the rules
generated by each method. The results are shown in Table 3.
From the results, we can observe that ChatRule can gen-
erate rules with higher support, coverage, and confidence than
the baselines. Specifically, we can observe that the scores of
the measures are consistent with the performance in knowl-
edge graph completion. This demonstrates that the selected
measures can well quantify the quality of rules. Notably,
while NCRL achieves higher scores than Ruleformer in the
support metric of YAGO3-10 (12660.16 vs 495.79), NCRL is
still outperformed by Ruleformer in knowledge graph com-
pletion. This is because the rules generated by Ruleformer
have a better PCA confidence, which is more suitable for
evaluating rules in incomplete KGs. Similarly, a higher PCA
confidence score indicates that ChatRule can generate rules
with better generalizability instead of solely relying on sam-
pled rules from prompts. Consequently, ChatRule also
demonstrates superior performance in the task of knowledge
graph completion.
5.7 Ablation Studies
Analysis of different LLMs. We first evaluate the perfor-
mance of ChatRule with several LLMs at different sizes,
including GPT-4 [OpenAI, 2023 ], Mistral-7B-Instruct [Jiang
et al. , 2023 ], and LLaMA2-Chat-7B/13B/70B [Touvron etTable 4: Performance of knowledge graph completion using rules
generated by different LLMs on the Family dataset.
Model MRR Hits@1 Hits@10
ChatGPT 0.849 0.765 0.964
GPT-4 0.803 0.704 0.926
Mistral-7B-Instruct 0.735 0.652 0.912
Llama2-chat-7B 0.742 0.636 0.893
Llama2-chat-7B-13B 0.767 0.671 0.900
Llama2-chat-7B-70B 0.731 0.626 0.875
al., 2023 ]. The details of model versions are available in Ap-
pendix .
From the results shown in Table 4, we can observe that
ChatRule with different LLMs still achieves a comparable
performance against baselines. This demonstrates the gener-
alizability of ChatRule . An interesting finding is that the
performance of ChatRule is not always improved by us-
ing larger LLMs. For instance, ChatRule performs better
with ChatGPT compared to GPT-4. In the LLaMA2 fam-
ily, LLaMA2-Chat-7B outperforms LLaMA2-Chat-70B but
is surpassed by LLaMA2-Chat-13B. One reason for this is
that different LLMs are sensitive to the input prompt; they
may behave differently and achieve varying levels of perfor-
mance despite having the same input prompt. Thus, further
adjusting the input prompt for different LLMs may achieve
better performance. Another reason is that larger LLMs tend

--- PAGE 7 ---
Table 5: Analysis of each ranking measure.
Ranking MetricsFamily UMLS
MRR Hits@1 Hits@10 MRR Hits@1 Hits@10
None 0.756 0.630 0.940 0.413 0.327 0.605
Coverage 0.795 0.673 0.956 0.604 0.460 0.843
Confidence 0.872 0.799 0.967 0.700 0.602 0.898
PCA Confidence 0.906 0.854 0.968 0.780 0.685 0.948
5105075100
Number of rule samples per query (k)0.60.8
35101520
Number of queries (d)0.750.800.85
MRR Hits@1
Figure 4: Parameter analysis of number of rule samples per query
(k) and number of queries ( d) on the Family dataset.
to generate fewer rules compared to smaller ones. We hy-
pothesize this could be due to the heavier computation cost
of larger LLMs, which could lead to a higher probability of
early stopping. The reduced number of rules may result in
poorer coverage of ground-truth rules and suboptimal perfor-
mance.
Analysis of ranking measures. We test the effectiveness
of each measure (i.e., coverage, confidence, and PCA con-
fidence) adopted in rule ranking. The rules are all generated
by ChatGPT on the Family and UMLS datasets.
The results are shown in Table 5. From the results, we
can see that when one of the ranking measures is employed,
the performance of ChatRule is improved over when no
ranking measure (i.e., None) is employed. This demonstrates
that the ranking measure can effectively reduce the impact of
low-quality rules. The PCA confidence metric achieves the
best performance among all the ranking measures. This show
that PCA confidence enables to quantify the quality of rules in
incomplete KGs and selects rules with better generalizability,
which is also chosen as the final ranking metrics.
Analysis of hyperparameters. Last, we evaluate the per-
formance of ChatRule with different hyperparameters, i.e.,
different numbers of rule samples per query ( k) and different
numbers of queries ( d) on the Family dataset. The results are
shown in Figure 4.
From results, we can observe that the performance of
ChatRule first improves with the increase of kand slightly
drops once kreaches 50. This shows that with more rule sam-
ples in the prompt, ChatRule can generate rules with better
quality. However, too large a kvalue leads to a longer prompt.
Existing LLMs are known to suffer from understanding long
contexts [Liuet al. , 2023 ], which could lead to a suboptimal
performance. In contrast, the performance of ChatRule
continually improves with the increase of d. This demon-
strates that ChatRule can generate better rules by “seeing”
more rule samples in KGs. However, more queries also intro-
duce higher computational costs, which could limit the scal-
ability of ChatRule . Therefore, we set kto 50 and dto 10Table 6: Statistics of the mined rules and overall API cost for each
dataset.
Dataset Total Rules Avg. Rules per Relation Cost ($)
Family 1200 100 0.880
UMLS 610 29 2.623
WN18RR 451 41 0.399
YAGO 1924 50 1.409
Table 7: Representative rules mined on each dataset.
Datasets Rule Support PCA Confidence
Familyhusband ←invwife 655 0.98
father ←husband ∧mother 1260 0.88
aunt←sister ∧aunt 2152 0.79
UMLSprevents ←complicates ∧invprevents ∧causes 16 0.53
treats ←prevents ∧invtreats ∧,treats 51 0.92
diagnoses ←analyzes ∧causes 20 1.00
WN18RRhaspart←haspart∧inv haspart∧haspart 4209 0.39
also see←also see∧inv also see∧also see 1151 0.47
hypernym ←member meronym ∧inv member meronym ∧hypernym 1996 1.0
YAGO3-10hasChild ←isMarriedTo ∧hasChild 1174 0.60
isPoliticianOf ←hasChild ∧isPoliticianOf 720 0.86
playsFor ←isAffiliatedTo 229794 0.75
in the experiment.
5.8 Case Studies
We first show the statistics of the mined rule the correspond-
ing overall API cost5for each dataset in Table 6. From re-
sults, we can observe that ChatRule can mine a substantial
number of meaningful rules at a low API cost.
We present some representative logic rules mined from
different datasets in Table 7. The results show that the
rules generated by our method are both interpretable and of
high quality. For instance, “wife” is intuitively the inverse
relation of “husband”, the rule husband ←invwife
is successfully extracted by ChatRule with the seman-
tics of the relationship considered. Similarly, “playsFor”
is the synonym of “isAffiliatedTo”, which constitutes the
ruleplaysFor ←isAffiliatedTo . Additionally,
the rules also follow commonsense knowledge, exhibiting
great interpretability. For example, the rule diagnoses ←
analyzes ∧causes shows that to make a diagnosis, doc-
tors usually need to analyze the patient’s symptoms and
find the cause of the disease. Last, the generated rules
also uncover some associative logical connections. The rule
isPoliticianOf ←hasChild ∧isPoliticianOf
indicates that children usually inherit their parents’ political
position, which is supported by the support and PCA scores.
The full lists of mined rules are available in the supplemen-
tary file .
6 Conclusion
In this paper, we introduce a new approach called ChatRule
for bridging the gap in logical rule mining on KGs. In
ChatRule , we propose a rule generator based on LLMs
that incorporates both semantics and structural information to
generate meaningful rules. Additionally, a rule ranker is de-
veloped to assess the quality of the rules and eliminate incor-
rect ones. Finally, the generated rules can be directly used for
5The costs are calculated based on the API price defined by Ope-
nAI (i.e., 0.001$ and 0.002$ per 1000 input and output tokens).

--- PAGE 8 ---
knowledge graph reasoning without additional model train-
ing. Extensive experiments on several datasets demonstrate
ChatRule can generate high-quality and interpretable rules
for downstream tasks. In the future, we will explore inte-
grating advanced models to enhance LLMs understanding of
structural information and improve the performance of rule
mining.
References
[Atifet al. , 2023 ]Farah Atif, Ola El Khatib, and Djellel Di-
fallah. Beamqa: Multi-hop knowledge graph question an-
swering with sequence-to-sequence prediction and beam
search. In SIGIR , pages 781–790, 2023.
[Barwise, 1977 ]Jon Barwise. An introduction to first-order
logic. In Studies in Logic and the Foundations of Mathe-
matics , volume 90, pages 5–46. Elsevier, 1977.
[Brown et al. , 2020 ]Tom Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, et al. Language models are few-shot
learners. NeurIPS , 33:1877–1901, 2020.
[Chen et al. , 2016 ]Yang Chen, Sean Goldberg, Daisy Zhe
Wang, and Soumitra Siddharth Johri. Ontological
pathfinding. In SIGMOD , pages 835–846, 2016.
[Cheng et al. , 2022a ]Kewei Cheng, Nesreen Ahmed, and
Yizhou Sun. Neural compositional rule learning for
knowledge graph reasoning. In ICLR , 2022.
[Cheng et al. , 2022b ]Kewei Cheng, Jiahao Liu, Wei Wang,
and Yizhou Sun. Rlogic: Recursive logical rule learning
from knowledge graphs. In KDD , pages 179–189, 2022.
[Dettmers et al. , 2018 ]Tim Dettmers, Pasquale Minervini,
Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d
knowledge graph embeddings. In AAAI , volume 32, 2018.
[Gal´arraga et al. , 2013 ]Luis Antonio Gal ´arraga, Christina
Teflioudi, Katja Hose, and Fabian Suchanek. Amie: as-
sociation rule mining under incomplete evidence in onto-
logical knowledge bases. In WWW , pages 413–422, 2013.
[Hinton and others, 1986 ]Geoffrey E Hinton et al. Learning
distributed representations of concepts. In Proceedings of
the eighth annual conference of the cognitive science soci-
ety, volume 1, page 12. Amherst, MA, 1986.
[Hou et al. , 2021 ]Zhongni Hou, Xiaolong Jin, Zixuan Li,
and Long Bai. Rule-aware reinforcement learning for
knowledge graph reasoning. In ACL, pages 4687–4692,
2021.
[Jiet al. , 2023 ]Ziwei Ji, Nayeon Lee, Rita Frieske,
Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
Andrea Madotto, and Pascale Fung. Survey of halluci-
nation in natural language generation. ACM Computing
Surveys , 55(12):1–38, 2023.
[Jiang et al. , 2023 ]Albert Q. Jiang, Alexandre Sablayrolles,
Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, L ´elio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,Thibaut Lavril, Thomas Wang, Timoth ´ee Lacroix, and
William El Sayed. Mistral 7b, 2023.
[Kok and Domingos, 2007 ]Stanley Kok and Pedro Domin-
gos. Statistical predicate invention. In ICML , pages 433–
440, 2007.
[Lao and Cohen, 2010 ]Ni Lao and William W Cohen. Re-
lational retrieval using a combination of path-constrained
random walks. Machine learning , 81:53–67, 2010.
[Liuet al. , 2021 ]Yushan Liu, Marcel Hildebrandt, Mitchell
Joblin, Martin Ringsquandl, Rime Raissouni, and V olker
Tresp. Neural multi-hop reasoning with logical rules on
biomedical knowledge graphs. In ESWC , pages 375–391.
Springer, 2021.
[Liuet al. , 2022 ]Yushan Liu, Yunpu Ma, Marcel Hilde-
brandt, Mitchell Joblin, and V olker Tresp. Tlogic: Tem-
poral logical rules for explainable link forecasting on tem-
poral knowledge graphs. In AAAI , volume 36, pages 4120–
4127, 2022.
[Liuet al. , 2023 ]Nelson F Liu, Kevin Lin, John Hewitt,
Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. Lost in the middle: How language mod-
els use long contexts. arXiv preprint arXiv:2307.03172 ,
2023.
[Luet al. , 2021 ]Shengyao Lu, Bang Liu, Keith G Mills,
SHANGLING JUI, and Di Niu. R5: Rule discovery with
reinforced and recurrent relational reasoning. In ICLR ,
2021.
[Luoet al. , 2023 ]Linhao Luo, Yuan-Fang Li, Gholamreza
Haffari, and Shirui Pan. Reasoning on graphs: Faithful
and interpretable large language model reasoning. arXiv
preprint arXiv:2310.01061 , 2023.
[Omran et al. , 2018 ]Pouya Ghiasnezhad Omran, Kewen
Wang, and Zhe Wang. Scalable rule learning via learning
representation. In IJCAI , pages 2149–2155, 2018.
[OpenAI, 2023 ]OpenAI. Gpt-4 technical report, 2023.
[Panet al. , 2023 ]Shirui Pan, Linhao Luo, Yufei Wang, Chen
Chen, Jiapu Wang, and Xindong Wu. Unifying large lan-
guage models and knowledge graphs: A roadmap. arXiv
preprint arXiv:2306.08302 , 2023.
[Qu and Tang, 2019 ]Meng Qu and Jian Tang. Probabilistic
logic neural networks for reasoning. NeurIPS , 32, 2019.
[Quet al. , 2020 ]Meng Qu, Junkun Chen, Louis-Pascal
Xhonneux, Yoshua Bengio, and Jian Tang. Rnnlogic:
Learning logic rules for reasoning on knowledge graphs.
InICLR , 2020.
[Sadeghian et al. , 2019 ]Ali Sadeghian, Mohammadreza Ar-
mandpour, Patrick Ding, and Daisy Zhe Wang. Drum:
End-to-end differentiable rule mining on knowledge
graphs. NeurIPS , 32, 2019.
[Salvat and Mugnier, 1996 ]Eric Salvat and Marie-Laure
Mugnier. Sound and complete forward and backward
chainings of graph rules. In International Conference on
Conceptual Structures , pages 248–262, 1996.

--- PAGE 9 ---
[Suchanek et al. , 2007 ]Fabian M Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. Yago: a core of semantic
knowledge. In WWW , pages 697–706, 2007.
[Tanet al. , 2023 ]Yiming Tan, Dehai Min, Yu Li, Wenbo Li,
Nan Hu, Yongrui Chen, and Guilin Qi. Evaluation of chat-
gpt as a question answering system for answering complex
questions. arXiv preprint arXiv:2303.07992 , 2023.
[Touvron et al. , 2023 ]Hugo Touvron, Thibaut Lavril, Gau-
tier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-
oth´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Ham-
bro, Faisal Azhar, et al. Llama: Open and efficient founda-
tion language models. arXiv preprint arXiv:2302.13971 ,
2023.
[Wang et al. , 2019 ]Xiang Wang, Dingxian Wang, Canran
Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. Ex-
plainable reasoning over knowledge graphs for recommen-
dation. In AAAI , volume 33, pages 5329–5336, 2019.
[Weiet al. , 2021 ]Jason Wei, Maarten Bosma, Vincent Zhao,
Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. Finetuned language models
are zero-shot learners. In ICLR , 2021.
[Weiet al. , 2022 ]Jason Wei, Xuezhi Wang, Dale Schuur-
mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits
reasoning in large language models. NeurIPS , 35:24824–
24837, 2022.
[Xuet al. , 2022 ]Zezhong Xu, Peng Ye, Hui Chen, Meng
Zhao, Huajun Chen, and Wen Zhang. Ruleformer:
Context-aware rule mining over knowledge graph. In Pro-
ceedings of the 29th International Conference on Compu-
tational Linguistics , pages 2551–2560, 2022.
[Yang and Song, 2020 ]Yuan Yang and Le Song. Learn to
explain efficiently via neural logic inductive learning. In
International Conference on Learning Representations ,
2020.
[Yang et al. , 2017 ]Fan Yang, Zhilin Yang, and William W
Cohen. Differentiable learning of logical rules for knowl-
edge base reasoning. NeurIPS , 30, 2017.
[Zhao et al. , 2023 ]Wayne Xin Zhao, Kun Zhou, Junyi Li,
Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian
Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
A survey of large language models. arXiv preprint
arXiv:2303.18223 , 2023.
[Zhong et al. , 2020 ]Haoxi Zhong, Yuzhong Wang, Cunchao
Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. It-
eratively questioning and answering for interpretable legal
judgment prediction. In AAAI , volume 34, pages 1250–
1257, 2020.
[Zhou et al. , 2020 ]Xuhui Zhou, Yue Zhang, Leyang Cui,
and Dandan Huang. Evaluating commonsense in pre-
trained language models. In AAAI , volume 34, pages
9733–9740, 2020.A Ranking Measures
In this section, we provide a detailed example of the rank-
ing measures (i.e., support, coverage, confidence, and PCA
confidence). For example, given a rule
ρ:=playsFor ←isAffiliatedTo ,
and a simple KG in Table 8. The measures can be calculated
as follows.
Support denotes the number of facts in KGs that satisfy the
ruleρ, which is defined as
supp(ρ) := #( e, e′) :∃(e, r 1, e2)∧,···,∧(eL−1, rL, e′) :
body (ρ)∧(e, rh, e′)∈ G,
(10)
where (e1, r1, e2),···,(eL−1, rL, e′)denotes a series of
facts in KGs that satisfy rule body body (ρ)and(e, rh, e′)
denotes the fact satisfying the rule head rh.
Example. For KG given in Table 8, support of the rule is 1,
because (Alex, isAffiliatedTo, Club 1) and (Alex, playsFor,
Club 1) exist in KG.
Coverage normalizes the support by the number of facts for
each relation in KGs, which is defined as
cove(ρ) :=supp(ρ)
#(e, e′) : (e, rh, e′)∈ G. (11)
Example. For KG given in Table 8, coverage of the rule is
1/2, because we have 2 facts under the “ playsFor ” relation.
The coverage quantifies the ratio of the existing facts in
KGs that are implied by the rule ρ. To further consider the
incorrect predictions of the rules, we introduce the confidence
and PCA confidence to estimate the quality of rules.
Confidence is defined as the ratio of the number of facts
that satisfy the rule ρand the number of times the rule body
body(ρ)is satisfied in KGs, which is defined as
conf(ρ) :=supp(ρ)
#(e, e′) :body (ρ)∈ G. (12)
Example. For KG given in Table 8, the confidence of the
rule is 1/3, because there is one positive fact satisfy the rule,
and there are two facts (i.e., (Alex, isAffiliatedTo, Club 2)
and (Bob, isAffiliatedTo, Club 3)) are considered as negative
examples.
The confidence assumes that all the facts derived from the
rule body should be contained in KGs. However, the KGs are
often incomplete in practice, which could lead to the miss-
ing of evidence facts. Therefore, we introduce the PCA (par-
tial completeness assumption) confidence to select rules that
could better generalize to unseen facts. PCA confidence only
considers the hard negative examples, which contradict the
facts in existing KGs, and PCA confidence ignores the soft
negative examples, which we have zero knowledge of their
correctness.
PCA Confidence is based on the theory of partial complete-
ness assumption (PCA) [Gal´arraga et al. , 2013 ]which is de-
fined as the ratio of the number of facts that satisfy the rule ρ
and the number of times the rule body body (ρ)is satisfied in

--- PAGE 10 ---
Table 8: An example KG containing two relations and five facts
isAffiliatedTo playsFor
(Alex, Club 1) (Alex, Club 1)
(Alex, Club 2) (Charlie, Club 2)
(Bob, Club 3)
the partial completion of KGs, which is defined as
partial (ρ)(e, e′) := ( e, r 1, e2)∧,···,∧(eL−1, rL,ˆe) :
body (ρ)∧(e, rh,ˆe),(13)
pca(ρ) :=supp(ρ)
#(e, e′) :partial (ρ)(e, e′)∈ G. (14)
Example. For KG given in Table 8, the PCA confidence of
the rule is 1/2, because (Alex, isAffiliatedTo, Club 2) is a hard
negative example which violates the fact that (Alex, playsFor,
Club 1), and (Bob, isAffiliatedTo, Club 3) is a soft negative
example, which is removed from the denominator since we
do not know whether “Bob” plays for “Club 3” or not, based
on facts in existing KG.
B Datasets
In the experiment, we select four widely used datasets fol-
lowing previous studies [Cheng et al. , 2022b ]: Family [Hin-
ton and others, 1986 ], UMLS [Kok and Domingos, 2007 ],
WN18RR [Dettmers et al. , 2018 ], and YAGO3-10 [Suchanek
et al. , 2007 ].
• Family [Hinton and others, 1986 ]is a knowledge graph
defines the relationships of members in a family, e.g.,
“Father”, “Mother”, and “Aunt”.
• UMLS [Kok and Domingos, 2007 ]is a bio-medicine
knowledge, where entities are biomedical concepts, and
relations include treatments and diagnoses.
• WN18RR [Dettmers et al. , 2018 ]is an English vocab-
ulary knowledge graphs designed to organize words ac-
cording to their semantic relationships. Words are con-
nected by a series of relationships, including “hyper-
nym”, “derivation”, etc.
• YAGO3-10 [Suchanek et al. , 2007 ]is another large
scale knowledge graph constructed from multiple data
sources, like Wikipedia, WordNet, and GeoNames,
which contains many relations, e.g., “was born in”,
“lives in”, and “politician of”.
C Baselines
We compare our method against the SOTA rule mining base-
lines: AIME [Gal´arraga et al. , 2013 ], NeuralLP [Yang et al. ,
2017 ], RNNLogic [Quet al. , 2020 ], NLIL [Yang and Song,
2020 ], NCRL [Cheng et al. , 2022a ], and Ruleformer [Xuet
al., 2022 ]in experiments.
• AIME6[Gal´arraga et al. , 2013 ]is a conventional logi-
cal rule mining method, which discovers rules from KG
with inductive logica programming.
6https://github.com/dig-team/amieLLM Model Implementation
Mistral-7B-Instruct mistralai/Mistral-7B-Instruct-v0.1
LlaMA2-chat-7B meta-llama/Llama-2-7b-chat
LlaMA2-chat-13B meta-llama/Llama-2-13b-chat
LlaMA2 70B meta-llama/Llama-2-70b-chat
ChatGPT gpt-3.5-turbo-0613
GPT-4 gpt-4-0613
Table 9: Details of used LLMs.
• NeuralLP7[Yang et al. , 2017 ]proposes a neural logic
programming, that learns logical rules in an end-to-end
differential way.
• RNNLogic8[Quet al. , 2020 ]proposes a rule generator
and a reasoning predictor with logic rules. It develop an
EM-based algorithm for optimization and learning high-
quality rules for reasoning.
• NLIL9[Yang and Song, 2020 ]proposes a neural logic
inductive learning model that is an efficient differen-
tiable inductive logical programming framework that
learns first-order logic rules.
• NCRL10[Cheng et al. , 2022a ]infers rules by recurrently
merging compositions in the rule body, which detects the
best compositional structure of a rule body for express-
ing the rule head.
• Ruleformer11[Xuet al. , 2022 ]is a transformer-based
model that encodes the context information from KGs
to generate rules.
D Large Language Models
The LLMs used in experiments are shown in Table 9. For
open-sourced LLMs, we utilize available checkpoints from
HuggingFace12. For ChatGPT and GPT-4, we used the API
provided by OpenAI13.
E Experiment Settings
ForChatRule , we use the ChatGPT14as the LLMs for rule
generation. We select the PCA confidence as the final rule
ranking measure and set maximum rule lengths Lto 3. In
the knowledge graph completion task, we follow the same
settings as previous studies [Cheng et al. , 2022b; Cheng et
al., 2022a ]. For baselines, we use the publicized codes for
comparison.
7https://github.com/fanyangxyz/Neural-LP
8https://github.com/DeepGraphLearning/RNNLogic/tree/main
9https://github.com/gblackout/NLIL
10https://github.com/vivian1993/NCRL/commits/main
11https://github.com/zjukg/ruleformer
12https://huggingface.co/
13https://api.openai.com/v1/chat/completions
14We use the snapshot of ChatGPT taken from June 13th 2023
(gpt-3.5-turbo-0613) to ensure the reproducibility.

--- PAGE 11 ---
F Rule Generation Prompt
The prompt template used for rule generation is shown as fol-
lows.
Rule Generation Prompt
Logical rules define the relationship between two entities:
X and Y . Each rule is written in the form of a logical
implication, which states that if the conditions on the
right-hand side (rule body) are satisfied, then the statement
on the left-hand side (rule head) holds true.
Now we have the following rules:
{rule samples }
Based on the above rules, please generate as many
of the most important rules for the rule head: “ {rule
head }(X,Y)” as possible. Please only select predicates
form: {relations in KGs }. Return the rules only without
any explanations.
