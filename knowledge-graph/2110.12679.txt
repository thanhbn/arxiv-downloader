# 2110.12679.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2110.12679.pdf
# File size: 1164055 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Springer Nature 2021 L ATEX template
Improving Embedded Knowledge Graph
Multi-hop Question Answering by
Introducing Relational Chain Reasoning
Weiqiang Jin1, Biao Zhao1, Hang Yu2*, Xi Tao2, Ruiping
Yin3,4and Guizhong Liu1
1School of Information and Communications Engineering, Xi`an
Jiaotong University, Xi`an, 710049, Shaanxi, China.
2School of Computer Engineering and Science, Shanghai
University, Baoshan, 200444, Shanghai, China.
3Information Faculty of Computer School, Beijing University of
Technology, Chaoyang, 100124, Beijing, China.
4Engineering Research Center of Intelligence Perception and
Autonomous Control, Ministry of Education, 100124, Beijing,
China.
*Corresponding author(s). E-mail(s): yuhang@shu.edu.cn;
Contributing authors: weiqiangjin@stu.xjtu.edu.cn;
biaozhao@xjtu.edu.cn; 20721546@shu.edu.cn;
yinruiping@bjut.edu.cn; liugz@xjtu.edu.cn;
Abstract
Knowledge Graph Question Answering (KGQA) aims to answer user-
questions from a knowledge graph (KG) by identifying the reasoning
relations between topic entity and answer. As a complex branch task
of KGQA, multi-hop KGQA requires reasoning over the multi-hop rela-
tional chain preserved in KG to arrive at the right answer. Despite
recent successes, the existing works on answering multi-hop complex
questions still face the following challenges: i) The absence of an
explicit relational chain order reected in user-question stems from a
misunderstanding of a user's intentions. ii) Incorrectly capturing rela-
tional types on weak supervision of which dataset lacks intermediate
reasoning chain annotations due to expensive labeling cost. iii) Fail-
ing to consider implicit relations between the topic entity and the
1arXiv:2110.12679v3  [cs.CL]  13 Nov 2022

--- PAGE 2 ---
Springer Nature 2021 L ATEX template
2 Relational Chain Reasoning Improved Embedded KGQA
answer implied in structured KG because of limited neighborhoods
size constraint in subgraph retrieval-based algorithms. To address these
issues in multi-hop KGQA, we propose a novel model herein, namely
Relational Chain based Embedded KGQA (Rce-KGQA), which simul-
taneously utilizes the explicit relational chain revealed in natural lan-
guage question and the implicit relational chain stored in structured
KG. Our extensive empirical study on three open-domain benchmarks
proves that our method signicantly outperforms the state-of-the-art
counterparts like GraftNet, PullNet and EmbedKGQA. Comprehen-
sive ablation experiments also verify the eectiveness of our method
on the multi-hop KGQA task. We have made our model's source
code available at github: https://github.com/albert-jin/Rce-KGQA.
Keywords: Data Mining and Search, Question Answering, Knowledge Graph
based Multi-hop QA, Neural Semantic Parsing, Knowledge Graph Embedding
1 Introduction
Knowledge Base Question Answering (KBQA) [1] is an attractive service
mining and analytics method that has attracted extensive attention from aca-
demic and industrial circles in recent years. Given a natural language question,
the KBQA system aims to answer the correct target entities from a given
knowledge base (KB) [2]. It relies on certain capabilities including capturing
rich semantic information to understand natural language questions clearly
and seek correct answers in large scale structured knowledge databases accu-
rately. Knowledge Graph Question Answering (KGQA) [3, 4] is a popular
research branch of KBQA which uses a knowledge graph (KG) as its knowledge
source [2, 5] and uses factoid triples stored in KG to answer natural language
questions. Thanks to KG's unique data structure and its ecient querying
capability, users can benet from a more ecient acquisition of the substantial
and valuable KG knowledge, and gain excellent customer experience.
Early works [6, 7] on KGQA focus on answering a simple question, where
only a single relation between the topic entity and the answer are involved.
For example, in the question \What lms did [Martin Lawrence] act in?", as
depicted in Fig.1, there only exists a single relation `starred actors' between
the topic entity `Martin Lawrence' and the answer. The nal answer only
relies on just a single KG fact (Martin Lawrence, starred actors reverse, Black
Knight). To solve simple question tasks, most traditional methods [8, 9] create
diverse pre-dened manual templates and then utilize these templates to map
unstructured questions into structured logical forms. Unfortunately, these pre-
dened templates and hand-crafted syntactic rules are both labor-intensive
and expensive. Moreover, such approaches require crowd workers to be familiar
with linguistic and specic domain expert knowledge. Due to the dependency
on large-scale xed rules and manual templates, these methods cannot handle
complex questions which require multiple relations inferences.

--- PAGE 3 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 3
To make KGQA more applicable in realistic application scenarios,
researchers have shifted their attentions from simple questions to complex ones
. Knowledge graph multi-hop question answering is a challenging task which
aims to seek answers which is multiple hops away from the topic entity in the
knowledge graph. For example, the question \Who directed the lms which
[Martin Lawrence] acted in?" is a complex multi-hop question which requires a
relational chain (starred actors, directed by) which has multiple relationships
to arrive at the corresponding answers. This task is a relatively complex task
compared with its simple counterparts due to the multi-hop relations retrieval
procedure [5] which requires more than one KG fact in the inference.
Previous approaches [10{13] for handling complex question answering con-
structed a specialized pipeline consisting of multiple semantic parsing and
knowledge graph answer-retrieving modules to complete the KGQA task.
However, most have several drawbacks and encounter various challenges. We
summarize these challenges as follows:
Expected Reasoning Chain
Unexpected Reasoning Chain
Irrelevant Reasoning ChainBlack KnightMartin
Lawrence
ComedyBad Boys II A Thin Line between 
Love and Hate
1996 ComedyKim BassEndless LoveEve`s  Bayou
Gone Fishin
Brave HeartMeagan Good
Fig. 1 The Freebase subgraph entered on the topic entity [Martin Lawrence] of the example
questions. The red, yellow, green, green with an imaginary line, and grey circles denote
the topic, intermediate, expected, unexpected and irrelevant entity nodes, respectively. The
green, red and grey colored edges indicate the correct, incorrect and irrelevant reasoning
chains, respectively.
Unexpected Relation Type Recognition. As the rst step of KGQA,
a semantic parser of most existing methods performs with poor accuracy
in recognizing the correct relational type implied in questions, which hin-
ders downstream answering reasoning. For example, as shown in Fig.1, let
us consider questions where the topic entity and answer are connected by
a multiple-hop reasoning chain, e.g., \Who acted in the movies directed
by the director [Martin Lawrence]?". To answer this type of question, the
related two facts (Martin Lawrence, directed byreverse) and (A Tine Line,

--- PAGE 4 ---
Springer Nature 2021 L ATEX template
4 Relational Chain Reasoning Improved Embedded KGQA
starred actors reverse, Gone Fishin) help derive the answers within the neigh-
borhood of the topic entity [Martin Lawrence]. Typically, these methods are
prone to encounter incorrect relational reasoning ( AB!AC) when we mistake
the unexpected relational chain (starred actors reverse, written by) for the
expected relational chain (starred actors reverse, directed byreverse). Thus,
it is necessary to optimize relational semantics parsing for more accurate user
intention recognition.
Unexpected Relation Order Recognition. Semantic parsing-based
[14{17] methods mostly do not eectively capitalize on the correlation infor-
mation of relationship order and direction from user-question expression. They
become more susceptible to incorrect understanding when the questions are
complicated from both semantic and syntactic aspects. The accuracy rate of
parsing syntactics can be dramatically decreased by those with long-distance
dependency. More especially, tracing back to the above question example and
Fig.1, in addition to the correct chain (with green arrows), the spurious multi-
hop chain (with red arrows) from entity node [Martin Lawrence] to [Kim Bass]
can lead to incorrect reasoning results when the semantic parser module fails
to parse such semantics as, reversing ( AB!BA) or shuing ( ABC!BCA ),
the correct order of the relational chain. In short, we need an accurately cap-
ture of longer ordered-relational mappings implied in user language expressions
to reach correct answers.
Renata Remedios
Amaranta OrsulaJose Arcadio
SegundoTose Areadio
Renate RemediosBrother
Brother SiblingSiblingFather
MotherDaughterSon
Cousin
Cousin -1
Fig. 2 A structured knowledge graph related to the user-question, \Who is [Renate Reme-
dio]'s cousin?". The illustration typically introduces implicit relations between the topic
entity and the answer that hides in KG.
Implicit Relation Reasoning & Subgraph Neighborhood Con-
straint. Most mainstream KGQA methods [18, 19] cannot indirectly capture
knowledge of implicit relational chains for reasoning due to the limiting con-
straint of neighborhood size constraint. All the answers are provided by

--- PAGE 5 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 5
retrieving the extracted question subgraph. Let us consider the question \Who
is [Renate Remedio]'s cousin?". As depicted in Fig.2, the corresponding knowl-
edge graph has no direct relational chain between the topic entity [Renate
Remedio] and answer [Amaranta Orsule]. In other words, for future successive
KGQA solutions, it is important to be able to discover the implicit fac-
toid knowledge (Remedio  Cousin!Orsule) in the incomplete KG by the
explicit relational chain (Remedio  Father - Siblings - Mother !Orsule),
similar to the KG-based link-prediction task [20]. Furthermore, most existing
methods labor under the undesirable constraint of answer detection from a
pre-specied localized KG sub-graph neighborhood. For example, the state-
of-the-art (SOTA) method GraftNet [21] whose answer is restricted to being
a subset of the entities present in a localized KG sub-graph neighborhood,
only reports a recall around 0.55 on an incomplete KG where only half of the
original triples are presented.
To alleviate these limitations and challenges for the multi-hop KGQA task,
our paper introduces a novel architecture, namely Relational Chain based
Embedded KGQA, which supports the integration to learn the explicit rela-
tional mappings implied in the user's expression and the implicit knowledge
from the structured KG, similar to link prediction. Our proposed approach
counters these limitations by simultaneously using the explicit semantic rela-
tional chain described in the question and the implicit relational chain between
the structured KG nodes. We use the knowledge graph embedding and con-
struct the Answer Filtering Module to calculate the mutual relationship
between the topic entity and answer. Motivated by the previous work Embed-
KGQA [22], we show how our model leverages an end-to-end neural network
that employs the KG entity and relation embeddings to provide complex ques-
tions with answers from the KG. Since our model replaces the traditional
pipeline procedure of generating and retrieving a localized subgraph at inter-
mediate reasoning steps, it helps to decrease memory costs eciently and
obtain computational eciency. To obtain a more competitive performance
in large-scale KG, we apply an extra reasoning procedure called the Rela-
tional Chain Reasoning Module to prune the candidate entities ranked by the
Answer Filtering Module . We apply a Siamese architecture [23] based on the
long short-term memory (LSTM) [24] and transformer RoBERTa [25] to learn
the semantic similarity between the relational chain of the problem description
and the KG factoid relational chain. It also leverages the external supervised
signal of the relational chain from the training sample. By calculating the
semantic similarity between question semantics and candidate entity retrieval
chain, we can further determine the nal answer more accurately. The internal
construction details of our model are introduced in Sec. 4.
We summarize the contributions of this paper as follows:
1. We propose a novel approach namely Relational Chain based Embedded
KGQA which includes two main modules: the Answer Filtering Module and
theRelational Chain Reasoning Module . Moving away from previous studies,
our model simultaneously takes advantage of the knowledge graph embedding

--- PAGE 6 ---
Springer Nature 2021 L ATEX template
6 Relational Chain Reasoning Improved Embedded KGQA
and training with weak supervision by predicting the intermediate relational
chain implied in KG to perform the multi-hop KGQA task.
2. We introduce the Answer Filtering Module , a knowledge graph embed-
ding based end-to-end network for preliminary answer ltering. This module
can address the problem of inadequacy that is a factor in the missing links in
the incomplete knowledge graph thanks to its capability of capturing implicit
KG relationships. Furthermore, we consider all entities as candidate answers
in this step, so our model won't suer from the out-of-reach issues brought by
limited subgraph neighborhood constraint.
3. Our proposed Relational Chain Reasoning Module can help capture
the multi-hop relations surrounding the topic node to support the results
more accurately. We apply the Siamese network [23] to calculate the vec-
tor representation-based semantic similarity score between the user's question
and KG structured knowledge. To the best of our knowledge, our proposed
sub-module is the rst to consider the question relational direction and order
information by using the Siamese network.
4. Our experimental results on three widely adopted KGQA benchmarks
demonstrate our method's competitive capability compared with most SOTA
methods (average 1.2% absolute improvement across hit@1 evaluation met-
ric). Furthermore, using an extensive ablation study, we demonstrate the
superiority and eectiveness of our proposed model for the multi-hop KGQA
task.
The rest of the paper is organized as follows: We rst provide a thorough
review of the related KGQA works in Sec. 2. Next, we introduce the pre-
liminary knowledge about the KGQA task in Sec. 3. Following the internal
structure of our model, we then explicate our two features: Answer Filtering
Module in Sec. 4.2 and Relational Chain Reasoning Module in Sec. 4.3. Sec.
5 describes the experimental details on three open-domain datasets. Finally,
in Sec. 6, we conclude our contributions to this work and suggest several
promising innovations for our Relational Chain based Embedded KGQA in
the future.
2 Related Work
Our work is closely related to the Multi-hop Knowledge Graph Question
Answering, Knowledge Graph Embedding, Siamese Network, and Pretrained
Language Model.
2.1 Multi-hop KGQA
Multi-hop Knowledge BaseQuestion Answering comprises two mainstream
branches: Information Retrieval-based (IR) and Semantic Parsing-based (SP-
based). The most popular methods fall into these two categories.

--- PAGE 7 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 7
2.1.1 Semantic Parsing Methods
SP-based approaches follow a parse-then-execute procedure. These methods
[22, 26{29] can be summarized as the following steps: (1) question semantic
understanding : parsing relations and subjects involved in complex questions,
(2)logical formula construction : decode the subgraph into an executable logic
form such as high-level programming languages or structured queries such as
SPARQL, (3) KG-based positioning and querying : search from KGs using the
query language and provide query results as the nal answer. Owing to its
intermediate procedure of generating expressive logic forms, SP-based methods
are more interpretable than their IR-based methods counterparts. Nonethe-
less, for most existing SP-based methods, more relations in complex questions
indicate a larger search space of potential logic forms for parsing, which will
dramatically increase the computational cost.
Yu et al. [30] pointed out that KG relation detection is a core component
and entity linking is a key step in KGQA tasks. To improve the recognition
accuracy of both sub-tasks, they proposed the Hierarchical Residual BiLSTM
(HR-BiLSTM) to encode question descriptions and word-level and phrase-
level relationship path. The new HR-BiLSTM module calculates the similarity
scores for all the questions and textual relationships, which integrates for these
two components entity linking and relationship path identication into a single
step and enhances each other. When in inference, the model only selects the
highly-scored (relations, topic entity) pairs as correct answers from candidates.
Miller et al. [31] proposed an ideal domain-specic KGQA framework,
called Key Value-Memory networks (KV-MemNN), which has proved to be
eective to support answer reasoning over specic domain multi-source knowl-
edge like textual documents and structured KG. It performs QA by employing
a widely used long-term memory mechanism to reason on a key-value struc-
tured memory network. They dened three operations, i.e., key hashing, the
model rst fetchs all KG triples relevant to given questions and then stores
their topic entities and relationships in the key slot, tail entity in the value
slot; key addressing, the model assigns each memory unit with a normalized
relevance weight by the dot product operation as the relevance probability
between the question and each key representations in the memory; nally,
value reading, where the model reads the values of all addressed memories by
taking their weighted sums of all values and relevance weights, and use the out-
puts to represent intermediate reasoning results, which is then used to update
the question representation. To obtain the nal prediction over all candidate
answers, the model repeats the key addressing and value reading steps in the
Ranking component several times.
However, the KV-MemNN obviously presents the following challenges: 1)
It often fails to precisely update multi-relation question queries during mul-
tiple memory reading. 2) It reads the memory repeatedly since they can not
well determine accurately when to stop. 3) It focuses more on memory facts
understanding rather than the properly questions understanding, so it does

--- PAGE 8 ---
Springer Nature 2021 L ATEX template
8 Relational Chain Reasoning Improved Embedded KGQA
not perform as well as expected when applied to the scenario where its ques-
tions are complicated and associated with complex constraints, such as an
open-domain KGQA task. 4) It selects the candidate with the highest similar-
ity score as the only answer in default. So, it conducts ineciently when the
questions contain more than one answer.
To solve these challenges, Xu et al. [32] proposed an interpretable mech-
anism to enable a basic KV-MemNN model to work for complex questions,
which yielded state-of-the-art performances on three benchmarks. Enhanced
KV-MemNN introduced a novel STOP strategy into multi-hop memory read-
ing to generate a exible number of queries and introduce a new query updating
method, which considers the already-addressed keys in previous hops as well
as the value representations that avoids repeated or invalid memory readings.
For multi-constraint questions, the model considers the value representation
of each hop by accumulating all the value representations of both current and
previous hops to address each relevant constraint at dierent hops.
In addition to the above representative methods, many knowledge base
question answering approaches based on Graph Neural Network (GNN) [33]
and Graph Convolutional Network (GCN) [34] have been proposed in recent
years, approaches such as Graph Convolutional Network-based Multi-Relation
Question Answering system (QAGCN) [35] and Case-Based Reasoning SUB-
Graph model (CBR-SUBG) [36]. As the name suggests, QAGCN is a simple
but eective model that leverages attentional graph convolutional networks
that can perform multi-step reasoning during the encoding of knowledge
graphs. Able to leverage highly-ecient embedding computations, the model's
signicant advantage is that it can essentially simplify complex reasoning
mechanisms. CBR-SUBG is a semiparametric model for weakly-supervised
KGQA that retrieves similar queries and utilizes the similarities in graph struc-
ture of local subgraphs to answer a query. It contains a parametric component
comprising a graph neural network (GNN). Through experiments, it performs
competitively with state-of-the-art KGQA models on multiple benchmarks.
Due to its capacity for reasoning pattern identication, the method CBR-
SUBG can also provide interpretable paths for returned answers, which could
bring slightly better interpretability.
2.1.2 Information Retrieval Methods
IR-based approaches typically include a series of procedures as fol-
lows: question-specic graph extraction, question semantics representation,
extracted graph-based reasoning and answer candidates ranking. Given a
complex question description, these methods [37, 38] rst construct a question-
specic subgraph which includes all question-related entity nodes and relation
edges from KGs without generating an executable logic formula, This is fol-
lowed by employing a question representation module to encode user-question
tokens as low-dimensional vectors. Secondly, an extracted-graph based reason-
ing module conducts a semantic matching algorithm to aggregate the center
entity's neighborhoods' information from the question-specic subgraph. At

--- PAGE 9 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 9
the end of the reasoning, they rank all the entities' scores in the subgraph by
applying an answer-ranking module to predict the top-ranked entities as the
nal answers. Based on feature representation technology, IR-based approaches
can be divided into feature engineering-based approaches and representation
learning-based approaches.
IR-based feature engineering approaches [39] rely on manually dened
and extracted features, which are time-consuming and cannot detect the
whole question semantics. To solve these problems, representation learning IR-
based methods convert questions and related entities into distributed vector
representations in the same dimension space and treat KGQA tasks as seman-
tic matching between distributed representations of questions and candidate
answers [2].
Sun et al. [21] propose a integrated framework namely GRAFT-Net , which
is adopted an knowledge fusion strategy, where the answers are selected from
a heterogeneous question-specic subgraph constructed from the KG and tex-
tual documents based on the given questions. The subgraph contains three
factors: entity nodes, sentence nodes and a special type of edges which indi-
cates the mutual relations between entity and sentence nodes. During answer
detection, the convolution neural network GRAFT-Net spreads central entity
node feature to neighboring nodes in several iterations and determines whether
an entity node is an answer or not.
However, the automatically constructed subgraph in GRAFT-Net relies
heavily on heuristic rules and can lead to serious error cascading and bring
incorrect reasoning. Thus, soon after proposing the GRAFT-Net [21], Sun et
al. [10] presented a learned iterative process for topic-entity-centric graph con-
struction. The improved method, called Pull-Net , where the \pull" classier is
weakly supervised so that only QA pairs are used for supervision. It rst selects
seed entity nodes by GRAFT-Net and a novel classication model at each step.
Then, more and more extra valuable entities and sentences are introduced into
the current graph through several pre-dened operational iterations, with the
nal answer determined by the same procedure as GRAFT-Net [21]. Exper-
imentally, PullNet improves dramatically over prior state-of-the-art methods
[21, 38, 39] even under weakly supervised signals and incomplete KGs.
A signicant challenge in multi-hop Knowledge Base Question Answering is
the lack of supervision signals at intermediate steps. To address this challenge,
He et al. [27] propose an elaborate teacher-student framework by adapting
the generic Neural State Machine (NSM) [40] as the student network, while
the teacher network aims to learn intermediate supervision signals to improve
the student network. The extensive evaluation results with three benchmark
datasets show that their proposed model is superior to previous methods in
terms of eectiveness for the multi-hop KGQA task. Moreover, other detailed
experiments prove that their approach is more exible to extend itself to other
neural architectures or learning strategies on graphs.
Apart from these traditional subgraph-generation methods, researchers also
try to incorporate the KG embedding mechanism as extra information into

--- PAGE 10 ---
Springer Nature 2021 L ATEX template
10 Relational Chain Reasoning Improved Embedded KGQA
entity and relation representations to alleviate the incomplete KG sparsity
problems. Inspired by relationship completion and missing link prediction tasks
in the KGs, Saxena et al. [22] propose a novel framework, named EmbedKGQA ,
which leverages the pre-trained KG embeddings to enrich the learned entity
and relation representations. Extensive comparative experiments on multiple
benchmarks show that EmbedKGQA is particularly eective in performing
multi-hop KGQA over sparse KGs.
2.2 Knowledge Graph Embedding
Knowledge Graph Embedding [28] is to embed a KG's factoid triples knowl-
edge including all entities and relations into continuous and low-dimensional
embedding representation space, such that the original entities and relations
are well preserved in the vectors. Representative KG embedding models exploit
the distance-based scoring function f() which is used to measure the plausibil-
ity of a triple (topic entity, predicate, and tail entity) as the distance between
the head and tail entity such as TransE [41] and its extensions (e.g. TransH
[42]), DistMult [43] and ComplEx [44]. In short, a typical KG embedding tech-
nique generally consists of three steps: (1) representing entities and relations,
(2) dening a scoring function, and (3) learning entity and relation representa-
tions. Thanks to its ability to simplify the manipulation while preserving the
KG inherent structure, it can benet a variety of downstream tasks to take
the entire KG into consideration, such as entity alignment [45], relation pre-
diction [20] and even KGQA work [22]. The eectiveness of knowledge graph
embedding in various real-world NLP tasks [46, 47] motivates us to explore its
potential advantages in the KGQA task.
2.3 Siamese Network
The Siamese network [23] is a semantic textual similarity metric that is built on
top of a feature representation network such as CNN [48] and RNN [23]. Given
an example input pair, the Siamese network rst maps these two inputs into
sequences of the word embedding vector using large-scale pretrained embed-
dings like Glove [49], then passes these vectors through the representation
extractor's forward procedure and get the semantic vector representations,
respectively. Finally, the Siamese network applies `1norm (Manhattan dis-
tance) or`2(Euclidean distance) norm as the distance measurement function
to calculate the similarity between these two representations. Furthermore, the
long short-term memory (LSTM) is superior to the original RNNs for learn-
ing long-range dependencies because its memory cell units can capture rich
features across lengthy language token sequences. Therefore, in this work, we
employ the Siamese adaptation of the bidirectional LSTM network to learn
the semantic relational chain.

--- PAGE 11 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 11
2.4 Pretrained Language Model: RoBERTa
Bidirectional Encoder Representations from Transformers [50], or BERT, is
a revolutionary self-supervised pretraining technique that learns to predict
intentionally hidden (masked) sections of text. Crucially, the representations
learned by BERT have been shown to generalize well to downstream tasks and,
when BERT was rst released in 2018, it achieved state-of-the-art results on
many NLP benchmark datasets.
RoBERTa [25], as is proposed by Liu et al., is built on BERT's language-
masking strategy and modies key hyperparameters in BERT, and it can be
regarded as a heavily optimized version of BERT. It includes removing BERT's
next-sentence pretraining objective, and trains with much larger mini-batches
and learning rates. It was also trained on an order of magnitude more data
than BERT, for a longer amount of time. This allows RoBERTa represen-
tations to generalize even better to downstream tasks compared to BERT.
The improved model RoBERTa achieves state-of-the-art results on GLUE,
RACE and SQuAD benchmarks, without multi-task netuning for GLUE or
additional data for SQuAD.
3 Preliminaries
In this section, we formally introduce the preliminary knowledge [2, 5] on
the multi-hop KGQA task formulation and its related denitions. Before the
formulaic description, all the summarized pre-dened notations for the KGQA
task are given as follows: we denote a KG as G(";R) in which",Rrespectively
denote the entities and relation set, and we use ( h;`;t ) to represent a factoid
triple in KG. We use an uppercase and lowercase letter to denote a matrix
(e.g.W) and a vector (e.g. v). The`nnorm of a vector is denoted as kpkn.
Denition 1 (Multi-hop question) [2, 5, 22] If a natural language ques-
tion involves more than one predicate between the topic entity and answer,
then we believe the answer is multiple hops away from the topic entity in the
KG. Thus, we identify this as a multi-hop question. For example, let us consider
the multi-hop question: \When did the lm production company announce
which actor also directed the movie [Cast a Deadly Spell]?", which consists of
several predicates which correspond to the KG relational links: release year,
starred actors, directed byrespectively.
Denition 2 (Knowledge Graph Embedding) [51] The KG embedding
algorithm [28, 51] aims to map all the KG components including entity and
relation to a low-dimension and continuous vector space. Given a KG consisting
ofnentities and mrelations, we rstly initialize the values of h,`andt
randomly. Then, a scoring function f`(h;t) which we dened measures the
relation of a fact triple ( h;`;t ). Finally, the embedding algorithm utilizes a
margin-based ranking criterion to optimize the embedding distribution that
maximizes the overall plausibility of factoid triples ( h;`;t ) and to minimize
the plausibility of spurious triples ( h0;`0;t0) simultaneously.

--- PAGE 12 ---
Springer Nature 2021 L ATEX template
12 Relational Chain Reasoning Improved Embedded KGQA
Denition 3 ( Multi-hop KGQA task) [2, 5] The multi-hop question
was introduced in Denition 1. In this section, we dene a knowledge graph
(KG) asG.Gis a directed graph whose nodes represent entities and edges
represent relations, and each triple in the KG represents an atomic realistic
fact, such as (Joseph Robinette Biden, president of, USA).
Formally, given a complex natural language question in the format of a
sequence of tokens q=w1;w2;:::;wl and the available KG G, the KGQA task
rst links the topic entity wi;:::;wj to the KGG. The subject mentioned in a
question is also named as a topic entity. Then, it identies the most possible
KG relations which are related to the user's question. Using these two steps,
the goal of KGQA is to determine the factual answer with triples stored in KG,
denoted by the set Aq, to queryqfrom the candidate entities Eby leveraging
the topic entity and related relations in KG. Specically, we focus on solving
complex question answering, termed the multi-hop KGQA task, where the
answer is multiple hops away from the topic entity in a knowledge graph, which
means these questions require more than one KG triple.
Who 
directed 
movies 
written 
by 
the 
writer 
of 
[Millennium 
Actress ]?
QuestionAnswer Filtering Module
Relational Reasoning Chain 
ModuleKim BassMankiewicz
Gone Fishin 0.9830.991
0.976Score Answer
Candidate Entities
Final Answer Gone FishinKim BassMankiewiczGone Fishin
0.8250.997
0.639Score Answer
Answer Scores
Fig. 3 This gure shows our overall pipeline architecture for the multi-hop KGQA task. The
green rectangles denote our two sub-modules, the solid arrows and dashed arrows indicate
the information owing through our model and the intermediate results. The next gures [5,
6] also illustrate this by using the typical user's complex question \Who acted in the movies
directed by the director [Martin Lawrence]?"
4 Our Proposed Model
Our Relational Chain based Embedded KGQA is a two-stage pipeline model
which consists of two components: Answer Filtering Module and Relational
Chain Reasoning Module .

--- PAGE 13 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 13
4.1 Overview
As illustrated in Fig.3, given a real-world question and an available KG, the
Answer Filtering Module rst jointly leverages topic entity embedding and
question representation to score all possible candidate entities in this KG to
provide a set of pruned candidate answers for this question. However, the entity
nodes in a KG are often on a scale as large as a million, hence it could be
noisy and inaccurate when comparing the topic entity with all other entities
^t. To make the learning more ecient and accurate, we do not directly select
the top-1 scoring entity from the sorted entities as our nal answer. Instead,
we introduce the extra module Relational Chain Reasoning Module to take the
relation type and order of semantic relational chain into consideration for a
higherhit@1 accuracy result.
Before being fed into the next stage, we transform these intermediate can-
didate entities to their shortest relational chains which point to the question's
topic entity by retrieving them in KG and mapping these ordered chains to
sequences of embedding which correspond with our embedded KG. The Rela-
tional Chain Reasoning Module receives the intermediate results generated by
the last step. Then, it simultaneously utilizes the relational chain sequences
and user-question to measure the mutual similarity score through our Siamese
network. Taking the question relational chain reasoning details into consider-
ation can help increase the accuracy of answer prediction compared with the
rst stage, the Answer Filtering Module . Finally, after sorting the scored can-
didates, we choose the entity which has the highest similarity score as the nal
answer. Fig.4 formally illustrates the algorithm of how our method works and
predicts the nal answer for a given multi-hop question, where `denotes the
question semantic representation, and denotes the ComplEx scorer.
Fig. 4 The formulaic illustration of our Rce-KGQA model.

--- PAGE 14 ---
Springer Nature 2021 L ATEX template
14 Relational Chain Reasoning Improved Embedded KGQA
4.2 Answer Filtering Module
As the rst step of our model, our Answer Filtering Module aims to lter an
entity set from all KG entities as candidate answers via three steps. These three
operational steps are illustrated in Fig.5 and relate respectively to three sub-
modules: Graph Embedding Generator, Question Semantic Parser and Answer
Scorer. We introduce each sub-module followed by the system processing order.
4.2.1 Graph Embedding Generator
Traditional solutions could not handle many scenario problems such as Implicit
Relation Reasoning and Subgraph Neighborhood Constraint . Inspired by the
competitive performance of previous work EmbedKGQA [22], we observe that
the global relation knowledge and structure information preserved in KG
embedding could potentially be used to resolve these issues eciently and
improve the overall accuracy of question answering.
In this work, our used KG is also embedded in continuous low-dimensional
vector space to obtain all sparse representations for all entities and relations
that existed in KG so we can simplify computations on the KG. We apply the
Complex Embeddings (ComplEx) [44] approach to embed relations and entities
in complex vector space. Compared with traditional KG embedding methods
likeTransE [41] and its extensions, semantic matching models such as Holo-
graphic Embeddings [52], ComplEx and RESCAL [53] have shown that they
can generally yield better results. All KG embeddings are initialized randomly
from uniform distributions. Generally, the hyper-parameters about entity and
relation embedding dimension are not less than 100 and, in this paper, we set
embedding dimension at 200 which follows previous similar works.
In each training step, positive facts which present correct real-world rela-
tional triples are sampled from all factoid triples existing in KG. Negative
facts which present fake factoid relational triples are generated from negative
sampling in the negative example generation step, where it randomly replaces
the tail entity with an incorrect entity or replaces the relation with incorrect
relation.
Considering the samples count ratio of positive to negative ones, Trouillon
et al. [44] further investigated the inuence of dierent numbers of negative
samples for each positive one. Their work demonstrates that generating more
negatives usually leads to better performance, and around fty negatives per
positive example is an appropriate trade-o between reasoning accuracy and
training cost. So, our implementations also follow this prior setting.
Givenh;t2"and`2 R , this embedding approach would provide
vh;v`;vt2Cdfor each relation triple ( h0;`0;t0), and the scoring function is
dened as follows:
(h0;`0;t0) = Re (hvh;vr;vti)
= Re dX
k=1v(k)
hv(k)
rv(k)
t!
(1)

--- PAGE 15 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 15
KG Embedding Generator
Embedded Graph
[Martin 
Lawrence][Gone
Fishin ]act_indirect
Who     acted      in          the     movies  directed   by       [TE]Question Embedding ModuleQuestion Semantic Parser
Bi-LSTMAttention Unit
Context Semantic EmbeddingAnswer Scoring Module
Topic Entity [Martin Lawrence]
All Other Entities in KB
â€¦â€¦
sort
Candidate Answer EntitiesPuala PrentissKim BassMankiewicz
Gone Fishin
â€¦0.9830.991
0.976
0.957Score Answer
Question: Who acted in the movies 
directed by [Martin Lawrence]?
Answer: Gone FishinAnswer Filtering Module
Fig. 5 Structure illustration of the Answer Filtering Module.
(h0;`0;t0)>08a2A (2)
 
h0;`0;t0
<08 
h0;`0;t0
=2A (3)
where theRe() means taking the real part of a complex value , the  vt
denotes the conjugate of vt, the `0;t0is the random replaced wrong relation and
wrong tail entity of the `0;t0, and theAmeans the set including all real-world
knowledge triples.
Optimization Eq.1 aims to minimize the values for all false triples less than
0, Eq.3 and maximizes the values for all true triples greater than 0, Eq.2.
It can be easily carried out by stochastic gradient descent (SGD) or Adam
optimizer at each training iteration. Lastly, the original structure and relation
information in the KG are preserved in these learned vectors, which helps
ecient completion of the downstream procedures.
4.2.2 Question Semantic Parser
In this section, we introduce the Question Semantic Parser , which consists
of a recurrent neural network (bidirectional-LSTM) and extra self-attention
operation, to help represent the question's meanings. During the inference
procedure, the Question Semantic Parser takes a question as the input and
provides a predicted vector ^`as this question's relationship representation
between the topic and answer in KG.
As shown in Fig.5, we build this sub-module based on the hierarchical
neural network. Firstly, we encode the Llength questionftjg, forj= 1;:::L
into a sequence of word embedding vectors fvjgthrough our word embedding
layer whose parameters are learnable during the training procedure. The word

--- PAGE 16 ---
Springer Nature 2021 L ATEX template
16 Relational Chain Reasoning Improved Embedded KGQA
embedding dimension is consistent with the recurrent network's hidden dimen-
sion. Then we employ a single layer bidirectional LSTM to learn a forward
hidden state sequence  !h1;  !h2;:::;  !hL
and a backward hidden state sequence  h1;  h2;:::;  hL
. Compared to general RNN, bidirectional LSTM is a special
RNN which mainly solves the gradient disappearance and gradient explosion
challenges and captures better long-distance semantics during long sequence
modeling. Our LSTM component uses 256 as its dimension of hidden represen-
tationshtand memory cells ct. It is well known that the performance of LSTMs
depends crucially on their initialization and oers a strong starting point to
facilitate model convergence, so we initialize our bidirectional LSTM weights
with Xavier [54] initialization which is markedly superior to other initialization
methods such as Gaussian, Uniform and Kaiming initialization[55].
Taking the forward step as an example, the next state  !hjbased on last
state  !hj 1is computed via the following operations.
fj=
Wxfxj+Whf  !hj 1+bf
(4)
ij=
Wxixj+Whi  !hj 1+bi
(5)
oj=
Wxoxj+Who  !hj 1+bo
(6)
cj=fjcj 1+ijtanh
Wxcxj+Whc  !hj 1+bc
(7)
  !hj=ojtanh ( cj) (8)
The variables fj;ij;ojin the above equations are the input, forget and
output gate's activation vectors respectively, where cj 1andcjare the cell
state vectors in the j 1 time and jtime,tanh andare the hyperbolic
tangent and sigmoid functions. Eq.4 denotes the forget gate operation, which
aims to control whether to forget the hidden cell state's part information of the
last moment with a certain probability. Eq.5 denotes the input gate operation,
which is responsible for processing the current sequence input. Eq.6 denotes
the output gate operation, which determines the degree to which information
is updated and output. Eqs. 7 and 8 are the steps to update the old cell
state, which is determined jointly by the state of the previous sequence, this
sequence's current input and the activation function. After the information
ows through LSTM, we concatenate the forward  !hjand backward  hjand
obtain the combined features hj= [  !hj;  hj]. After that, the last hidden state
is considered to be the question semantic representation.
Dierent word tokens make dierent contributions to the relationship
semantic recognition. For example, words which are prepositions and articles
are more irrelevant for discovering question semantics than relational demon-
strators. Thus, after LSTM we apply the self-attention mechanism to capture

--- PAGE 17 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 17
more valuable features. The attention operation details are shown in Eq.9 and
Eq.10. Given an LSTM hidden representation, a full connect layer, activation
functiontanh and softmax operation will jointly generate the attention weight
jat rst. Then, as shown in Eq.10, the nal attention vector representation
sjwhich is the semantic representation of the language question is aggregated
by the weighted sum operation of hjandaj.
j=exp (aj)PL
i=1exp (ai)where a j= tanh 
w>
a[hj] +b
(9)
sj=X
iijhij (10)
All the weight matrices, weight vector W, and bias terms are calculated
based on the training data,i.e. LSTM gate unit weight matrix fWf;Wi;Wog
and attention weight matrix w>
a. In this way, we obtain the rich relationship
semantics implied in natural language questions for answer reasoning.
4.2.3 Answer Scorer
Like the ComplEx [44] scoring function which depicted in Eqs. 1, 2, and 3,
as shown in Eq.11, we learn an answer ranker Rank (t) for each candidate
t, namely Answer Scorer to score the (topic entity, relationship semantic)
pair against all possible KG entities t2"by maximizing the probabilities of
positive samples t2Aand minimizing the negative sample t0=2A, where the
Ameans the set including all real-world knowledge triples.
Rank (t) =
max ((h;`;t ));8t2A
min ((h;`;t0));8t0=2A(11)
Since our Question Semantic Parser is designed to t realistic relationship
features, all the pretrained KG entity embeddings are frozen during the model
convergence procedure.
Instead of simply selecting the entity with the highest score due to its low
accuracy but high recall performance, we conduct a rough ltering by selecting
top-n, where n2f5;10;15gto obtain the intermediate scored candidate enti-
ties that have a high answer recall rate. For the inference, the Answer Scoring
Module gives each candidate a plausibility score to indicate its answer con-
dence and lter out the top-n scored intermediate result which is fed into the
next step, the Relational Chain Reasoning module.
4.3 Relational Chain Reasoning Module
Our available KG often contains a large number of entities and has enor-
mous factoid triples, and it could be inaccurate when comparing all candidate
embedding representations against with each other. Specically, after training
theAnswer Filtering Module , we obtain all the scored entities for each training
sample. During the prediction result analysis, we observe that the model per-
formance on hit@5 outperforms that on hit@1 metric, which could be due to

--- PAGE 18 ---
Springer Nature 2021 L ATEX template
18 Relational Chain Reasoning Improved Embedded KGQA
the inuence of a large number of noisy entities number in the large-scale KG.
Furthermore, since the answer is given as the only ground-truth information,
a major challenge for multi-hop KGQA is that it usually lacks intermediate
reasoning supervision signals.
Who acted  in the  movies  directed  by [TE] ?Bi-LSTM
Embedded Graph
[Martin 
Lawrence][Gone
Fishin ]act_indirect
Attention Unit
Relational Chain Embeddingâ€¦â€¦
Question semantic Parser
ï¼ˆRoberta Transformer ï¼‰Full Connect
DropOut
Relational Chain SequenceSimilarity Score
Question: Who acted in the movies directed 
by [Martin Lawrence]?
Answer: Gone FishinRelational Chain Reasoning Module
Question Semantic Representation Semantic Relational Chain Representation
Full Connect
Fig. 6 Structure of the designed Relational Chain Reasoning Module.
To tackle these issues, we propose an extra component for our KGQA
work, termed the Relational Chain Reasoning Module . As the nal and impor-
tant step of our approach, it aims to improve the reasoning accuracy through
considering the reasoning chain order and its relational type under a weak
supervised situation. Its training procedure is irrelevant to the Answer Filter-
ing Module , but its training dataset is constructed from the Answer Filtering
Module 's prediction results.
Formally, we use the trained Answer Filtering Module to obtain the sorted
scored entitiesf[eit;sit]gn
i=1,f[eiv;siv]gn
i=1andf[eie;sie]gn
i=1for the training,
validating, and testing datasets, respectively. Then, these scored results are
truncated by only reserving top- n(n2f5;10;15g) candidates. Next, looking
at the rough ltered results in each experimental sample, if it belongs to the
correct answers, we construct a corresponding positive sample [ q;frgc
i=1], in
which qdenotes the question tokens and frgc
i=1is the clength shortest path
searched by the graph retrieval algorithm, as do the negative samples that are
not inside in the correct answers.
4.3.1 Siamese Network Based Similarity Scoring
As shown in Fig.6, we introduce our novel sub-module Siamese Network Based
Similarity Scoring , which is essentially a Siamese network [23]. The module's
two feature detectors aim to extract the question Eq.12 and the relational

--- PAGE 19 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 19
chain semantic representations Eq.13. They are constructed by a pretrained
transformer RoBERTa [25] to generate question vector Vqfrom question
q=fwigN
i=1and a single-layer bidirectional LSTM to generate relational
chain vectorVrfrom relational chain r=frjgM
j=1. RoBERTa is a revolution-
ary self-supervised pretraining technique that learns to predict intentionally
hidden sections of language text. Moreover, the representations learned by
RoBERTa has been shown it can generalize outcomes superior even to many
NLP downstream tasks compared to original BERT [50]. Structure details will
be described in the next successive two subsection. After information passes
through these two encoded networks, the semantic feature similarities Eq14
in the vector representation space are subsequently used to infer the semantic
similarity between the question and relational chain from the topic entity to
answer.
Vq=W>
2(drop(W>
1(RoBERTa (q)) +b1)) +b2 (12)
Vr=Attn(BiLSTM (r)) (13)
Score (Vq;Vr) = exp
 V(2)
q V(2)
r
2
(14)
Here, we explain why we use a semantic similarity score between rela-
tion chainVrand question representations Vqto determine the right answer
in detail. Taking the above question \Who acted in the movies directed by
the director [Martin Lawrence]?" as an example, we denote the natural lan-
guage tokens as q. Inspired by literature in recent years such as [2, 36], we
intuitively suppose that the implied question semantic is similar and its rep-
resentation is closer to the relational chain representation in vector space,
\directed byreverse!starred actors reverse ", which is correct in both order
and type. It is inconsistent and far away from the relational chains which have
the wrong type or order, such as \ starred actors reverse!directed byreverse "
and \ directed byreverse!written byreverse ".
As shown in Eq.14, we train the similarity scorer using the stochastic
gradient descent (SGD) backward propagation algorithm under the mean-
squared-error (MSE) loss function. Then we endow our training criterion with
the`2(Euclidean distance) norm metric to avoid the model parameter dis-
tribution being highly warped. Regarding the predicted relatedness labels to
lie in [0;1], for the positive sample we maximize the prediction value as close
as possible to 1, and the negative sample as close as possible to 0. Finally,
after sorting the scored candidates, we choose the entity which has the highest
similarity score as the nal answer.
4.3.2 Question Semantic Representation
As depicted in Fig.6 left, we use and netune a standard version of RoBERTa
to obtain the hidden state Vqlying in start token [ CLS ] for our question
encoders. Note that we reformat question qthrough replacing the topic entity's

--- PAGE 20 ---
Springer Nature 2021 L ATEX template
20 Relational Chain Reasoning Improved Embedded KGQA
mention in the question with a token \NE". And we respectively supplement
two special characters `[ CLS ]' and `[SEP ]' before and after the reformatted
question, which follows the BERT default conguration. This aforesaid opera-
tions can help our model better distinguish the topic entity and other question
words mentioned. Afterwards, we link the question's topic entity mention to
the KG node through matching with standard KG entity literal representa-
tions. We regard it as the semantic signal for answer reasoning and then adopt
two full connect layers, neuron activation function ReLU [56] and a dropout
layer to get better feature learning capability. After the above step, a vector
representation is generated by the last full-connected layer, and we use the
vector to compare space distance similarity with the output of another feature
encoder, Relational Chain Representation Module.
4.3.3 Relational Chain Representation
As depicted in Fig.6 right, we provide the Relational Chain Representation
module which consists of a single-layer bidirectional LSTM and a self-attention
layer. Given the KG relational chain embedding sequence as input, this mod-
ule learns and captures the relevant and necessary semantic information for
answering reasoning. It has a similar structure to the Question Semantic
Parser in the Answer Filtering Module , but the token embeddings used are not
initialized randomly in this case. Instead, we apply the pretrained KG relation
embedded representations existing in Sec 4.2.1 to embed our relational chain.
Formally, as depicted in Eq.13, for relational chain r=frjgM
j=1, whereM
denotes the chain length, each relationship representation rj(j= 1;2;:::;M )
is initialized with the pretrained KG relational embeddings. Next, we feed
the embedded vectors rinto a single-layer bidirectional LSTM network to
obtain a series of output states h1;h2;:::;h M, where the j-th relation hj
denotes [  !hj;  hj], a combined vector of forward LSTM state output  !hjand
backward LSTM state output  hj. Then the new question representation q=
[h1;h2;:::;hM] can be transformed through a self-attention operation, which
is similar to Answer ltering Module counterpart, shown in Eq.9, 10. Through
this forward propagation, that is similar to Question Semantic Parser , we
can obtain a question semantic vector Vqwhich has the same dimension as
relational chain vector Vrfor the following similarity computation step.
5 Experiment
In this section, we evaluate our proposed Rce-KGQA against competitive
baselines on three benchmark datasets to investigate whether our model can
outperform other methods on reasoning over the weakly supervised signal and
incomplete knowledge graph. And we also append extensive ablation experi-
ments and a case study to carefully verify and vividly demonstrate that the
necessity, superiority, and meaning about our ideas in this work. The datasets
as well as the pytorch implementation of our model are publicly available at
https://github.com/albert-jin/Rce-KGQA.

--- PAGE 21 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 21
5.1 Datasets and Evaluation Metric
Here, we rst describe the three benchmark datasets we use in this work and
then give a brief introduction of the metric hit@1 we used for model evaluation.
MetaQA [57] is a large KGQA dataset which provides an original version
Vanilla and two variations. In this paper, we use the original ones because they
are designed manually. This dataset includes up to 75w QA pairs which are
merely 2-hop questions. The questions' literal descriptions are generated by
cross-language translation, English!French!English . This relies on a large-
scale movie domain which contains 9 relationship types, 43234 entities and up
to 135k factoid triples. Here, we use the dataset edition which is generated
from Apoorv et al. [22].
WebQuestionsSP-tiny [58] dataset is a relatively small dataset with a
total of 4736 QA pairs. This QA dataset's available KG is a subset of Free-
base that contains all the facts within 2-hops of any entity mentioned in the
questions of the original WebQuestionsSP, which have more than 188w enti-
ties and 1000 relationship types. Following [22], for all topic entities labeled
in the original Freebase, He et al. [27] construct a subgraph containing other
KG entities close to them by the PageRank-Nibble algorithm (PRN) [59]. In
this way, theoretically, the pruned KG is likely to contain the correspond-
ing answer entity for close to whole questions. In this work, we use the same
train/dev/test splits as GraftNet [21].
Complex WebQuestionsSP (Complex-WebQSP)[60] is a more com-
plex multi-hop reasoning dataset and its questions require up to a 4-hops
relational path during answer reasoning. There are four types of question: con-
junction(about 45%), composition(about 45%), comparative (about 5%), and
superlative (about 5%). Our used edition of the Complex-WebQSP dataset is
obtained from He et al. [27].
Table 1 Statistics for dataset MetaQA, WebQuestionsSP-tiny and Complex
WebQuestionsSP. MetaQA contains three subsets of dierent complex question relational
chain length, 1/2/3hop MetaQA. WebQuestionsSP-tiny dataset requires up to 2-hop
reasoning from knowledge base. Meanwhile, the Complex-WebQSP dataset requires up to
4-hops of reasoning on the KG.
Datasets Train Dev Test
MetaQA 1-hop 208970 19947 9992
MetaQA 2-hop 231844 14872 14872
MetaQA 3-hop 227060 14274 14274
WebQSP-tiny 2848 250 1639
Complex-WebQSP 27639 3519 3531
Metrichit@1 is a standard assessment for measuring the ratio in all vali-
dation samples that the highest scored entity belongs to the correct answers.
In brief, if the QA system provides the user with a single entity and this entity

--- PAGE 22 ---
Springer Nature 2021 L ATEX template
22 Relational Chain Reasoning Improved Embedded KGQA
is right, we then determine that this prediction is correct. This evaluating indi-
cator is popular and publicly recognized and has been used in many recent
KGQA works [22, 26, 37].
5.2 Experiment Setting
As we know, hyperparameter choices have a signicant impact on the model's
nal performance [61{63]. Our optimal model hyperparameter conguration
is summarized as follows. All the LSTM modules we used as feature encoders
have a single layer with a hidden dimension of 256. All the dropout layers ran-
domly drop about 30% of their features for inputs during the training step but
they do not drop any features during testing. We apply Xavier initialization to
each network layer's training parameters in our model. Our applied pretrained
transformer RoBERTa is a PyTorch-implemented conguration, which uses
the BERT-base architecture [50, 64, 65], consisting of 12 layers,768-d hidden
size and 12 attention heads for ecient training and inference. Roberta-base
encoder [25, 66] contains up to about 125M parameters. The Answer Filter-
ing Module is trained for up to 200 epochs with a batch size of 128, and the
relational chain reasoning module is trained for up to 120 epochs with a batch
size of 32 on three benchmarks. For every 10 training epochs, we adopt the
early-stopping strategy by evaluating hit@1 on the test set to avoid overtting.
During model convergence, the stochastic gradient descent (SGD) optimizer
with initial learning rate lr= 1e-5 is adopted. We used dierent random seeds
to validate our best-congured model independently 5 times and report the
average validation performances of our model in the next sections.
5.3 Compared Methods
In our experiment, the state-of-the-art methods for comparison are described
as follows:
â€¢EmbedKGQA [22] is a KG embedding driving method for multi-hop
KGQA which matches the pretrained entity embeddings with question
embeddings generated from the transformer.
â€¢SRN [11] is an RL-based multi-hop question answering model which
conducts the QA task by extending the inference chains on a KG.
â€¢KVMem [31] The Key-Value Memory Network rst attempts to conduct
QA over incomplete KGs by augmenting it with text. It uses a memory
table which stores the KG facts encoded into key-value pairs to retrieve a
question-specic subgraph for reasoning.
â€¢GraftNet [21] is a question description-based semantic sub-graph driv-
ing method that uses a variational graph CNN to perform QA tasks over
question-specic subgraphs containing KG facts, entities and discourses from
textual corpora.
â€¢PullNet [10] improves GraftNet on the retrieval subgraph by introducing
the graph retrieval module which utilizes shortest path from the topic entity
to answer as the additional supervised signal.

--- PAGE 23 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 23
â€¢NSM s[27] is a series of teacher-student learning approaches implemented
as based on the Neural State Machine [40]. NSM ,NSM +p, andNSM +hare
three model variants which eectively employ the intermediate supervision
signals. Specically, NSM do not use the teacher network, NSM +puse
the teacher network with parallel reasoning, and NSM +huse the teacher
network with hybrid reasoning.
5.4 Main Results
In this section, we compare our model with the state-of-the-art baseline
methods on three benchmarks, and the following questions are answered:
Q1. How eective and accurate is the performance of our model compared
with other SOTA models?
Q2. Can our model really identify the implicit relations and the indirectly
linked answer when there is no direct relational chain from topic entity to
answer?
Q3. How many parameters does our Rce-KGQA contain, and does our
model have a certain high execution eciency?
5.4.1 Answer Reasoning on MetaQA
As illustrated in Table 2, the overall experimental results on the MetaQA test
set clearly demonstrate that our proposed KGQA architecture signicantly
outperforms state-of-the-art methods on hit@1 metric. Specically, accord-
ing to the performance of our model's result on 1-hop MetaQA (identical to
WikiMovies), compared to the other methods, Row 1~5, we observe that
our method's hit@1 accuracy achieves much higher performance up to 98.3%,
which is an increase of 1.3% compared to GraftNet [21] and PullNet [10],
an increase of 0.8% compared to EmbedKGQA [22] and an increase of 1.1%
compared to NSM +h[27].
For the evaluation of multi-relation questions which require at least two
hops of inference to nd the answers, hit@1 results on 2-hop and 3-hop
MetaQA also show better performance than most competitive state-of-the-art
baselines. Although our model test results on 2-hop did not achieve the best
score, it is nevertheless comparable to other SOTA models. It is worth noting
that the retrieval-and-reason process of PullNet , which can simultaneously
extract answers from both corpora and KGs, is good at reasoning answers over
large-scale KGs such as MetaQA. In contrast, we can see that our model Rce-
KGQA , which takes relation chain reasoning into consideration, does not drop
signicantly in performance but remains almost unchanged when the relational
chain hop increases. We think the reason may be that the baseline models only
consider the question shadow semantic representations, and inevitably intro-
duce noise and incorrect retrieving path over KG. On the other hand, since
our model focuses on relational chain order and relation type, it is less sensi-
tive to the hop size and shows robustness on complex multi-constraint queries
over KG.

--- PAGE 24 ---
Springer Nature 2021 L ATEX template
24 Relational Chain Reasoning Improved Embedded KGQA
In summary, these results have shown their eectiveness and superiority
when considering the question semantic and its relational chain into reasoning,
which largely improves the KGQA performance.
Model 1-hop MetaQA 2-hop MetaQA 3-hop MetaQA
EmbedKGQA 97.5 98.8 94.8
SRN 97.0 95.1 75.2
KVMem 96.2 82.7 48.9
GraftNet 97.0 94.8 77.7
PullNet 97.0 99.9 91.4
NSM 97.1 99.9 98.9
NSM+p 97.3 99.9 98.9
NSM+h 97.2 99.9 98.9
Our Model 98.3 99.7 97.9
Table 2 Eectiveness comparisons on three subsets of MetaQA. The rst group of results
was taken from papers on recent methods. The values are reported using hits@1. The
number in bold and underlined number denote the best and second-best methods,
respectively. This gure corresponds to Sec. 5.4.1.
However, we also consistently nd that our proposed Rce-KGQA's two-
stage pipeline mechanism could bring deviation cascade propagation between
the coarse-grained answer ltering procedure [5] and the ne-grain answer
selecting procedure [6]. As is shown in Table 2, and the comparison results from
the columns of 2/3-hop MetaQA indicated. Our approach performs poorly
when compared with other KGQA models such as PullNet andNSMs . They
all consistently achieved very high performance with the hit@1 metric of 99.9
percentage on 2-hop MetaQA and 98.9 percentage hit@1 on the 3-hop MetaQA
dataset. Correspondingly, our Rce-KGQA underperforms with about 0.2 per-
centage points behind on the 2-hop MetaQA and about 1.0 percentage points
behind on the 3-hop MetaQA dataset.
In general, although our two-stage pipeline solution outperforms many
baselines such as GraftNet and PullNet [10, 21], our designed Rce-KGQA's
separate architecture determines the fact that the quality of the nal answer
provided by Relational Chain Reasoning Module completely depends on the
quality of the candidate entities provided by Answer Filtering Module . The
feature of pipeline architecture like this is the principal reason that inevitably
causes the cascaded error propagation, which would bring down the overall per-
formance of the question answering service. We think it is crucial to enhance
our model Rce-KGQA by integrating these two separated modules [5, 6] into
one joint module, which we leave for future work.
In addition to the embedded graph vector's parameter volumes, our
two-stage modules respectively contain 46M parameters (mainly owned by
BiLSTM-Attn block) and 197M parameters (mainly owned by Roberta encoder
andBiLSTM-Attn block). Through our experiments, we observe that the total
inference time uctuates between 1.9 seconds and 2.7 seconds, in which the
Answer Filtering Module contributes about 0.6s and the Relational Chain Rea-
soning Module contributes about 1.4s. It is conceivable that if the encoder were

--- PAGE 25 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 25
switched to RoBERTa using the BERT-large architecture [25, 66], it would
mean more heavy parameters, including up to 355M parameters and addi-
tional training optimization procedures and more inference time-consuming.
Based on the experimental reproducibility and the consideration of the model
lightweight, we chose the basic conguration of the Roberta encoder.
5.4.2 Experiments on WebQSP-tiny and Complex-WebQSP
WebQuestionsSP-tiny [22, 27, 36] is a relatively small dataset for training but
relies on a large-scale KG (Freebase) whose entities' count is greater than
10 million. Table 3 presents the evaluation results on the WebQuestionsSP-
tiny validation dataset, from which we can observe that our KGQA system
still performs better that other state-of-the-art counterparts, EmbedKGQA
(has 3.8% lower hists-at-one than our model) and PullNet (has 2.3% lower
hists-at-one than our model).
Model WebQuestionsSP-tiny Complex-WebQSP
KVMem 46.7 21.1
GraftNet 66.4 32.8
EmbedKGQA 66.6 -
PullNet 68.1 45.9
NSM 68.7 47.6
NSM+p 73.9 48.3
NSM+h 74.3 48.8
Our Model 70.4 48.3
Table 3 Experiment results (% Hits@1) compared with SOTA methods on the
WebQuestionsSP-tiny and Complex WebQuestionsSP validation datasets. All QA pairs in
WebQuestionsSP-tiny are 2-hop relational questions. We copy the results for KV-Mem,
GraftNet, EmbedKGQA, PullNet and NSM from [10, 21, 22, 27, 31], respectively. The best
score is in bold and the second-best score is underlined .
Specically, the last row shows that our full model achieves accuracy of
up to 70.4% hit@1, which improves a large margin to other prior models.
A possible explanation is that the ltering model equips the extra relational
chain module with better reasoning perception, leveraging KG and question
implicit features more eciently, and emphasizing the order of relational triples
selection to help our model make a correct decision. Even in large-scale KGs
along with small training dataset situations like WebQuestionsSP-tiny , our
Rce-KGQA solution can still be robust and helpful for handling realistic QA
applications.
Complex-WebQSP [27] is a derivative KGQA dataset edition which is gen-
erated from WebQuestionsSP-tiny by extending the question entities or adding
constraints to answers. As its name indicates, most of the questions this dataset
included require up to 4-hops of relational chain reasoning from the topic entity
to the corresponding answers.
The third column of Table 3 reports the hit@1 metric statistics on the
Complex-WebQSP benchmark. Our model outperforms competitively with

--- PAGE 26 ---
Springer Nature 2021 L ATEX template
26 Relational Chain Reasoning Improved Embedded KGQA
state-of-the-art KGQA baselines on such a complex multi-hop question answer-
ing scenario. More specically, among most baselines (KVMem sNSM), our
Rce-KGQA signicantly surpasses other baselines, and respectively achieves
27.2%, 15.5%, 3.3%, and 0.7% absolute gains over these baselines (KVMem
sNSM) in terms of the overall metric hit@1. This establishes the fact that
ourRce-KGQA is better than previous approaches in terms of answering the
questions with long-distance relational dependency. The NSM +hachieves the
best performance on the two adopted benchmarks. The NSM +pand our
Rce-KGQA both gain the second best performance on the Complex-WebQSP
benchmark, proving both our model's competitive capability and the impor-
tance of the added teacher network. This is an important observation and
advancement when it comes to handling such complex question answering
tasks since our proposed approach is robust and ecient in dealing with these
complex questions in multi-relational-hop answer reasoning situations.
5.5 Answer Reasoning for implicit relationship discovery
As shown in Table 4, we verify our method's ability to discover missing implicit
relationships through comparison experiments. The KG which MetaQA uses
has no missing link during the reasoning path because the QA question pairs
are constructed upon this KG. However, to make it become a realistic setting,
we simulate an incomplete KG by randomly removing half (with probability
= 0.5) of the factoid triples from it. We call this pruned setting half and we
call the full KG setting fullin the text.
The experiments show our method's implicit relation discovering capability
substantially outperforms other state-of-the-art methods over incomplete KGs.
The amount of improvement is signicant, with an increase of 43.7% compared
toKVMem inhit@1. Furthermore, our competitive model also delivers an
average 1.7% hit@1 rate on 2-hop MetaQA half setting and performs well on
3-hop MetaQA half setting while PullNet still achieves the highest hit@1
score.
Hence many baseline methods such as GraftNet ,PullNet require con-
structed question-specic subgraphs, indicating they lack the capability to
recall the answer nodes out of their generated subgraph and cannot perform
well in real QA scenarios. Fortunately our model, which exploits the KG
link prediction properties, does not limit its capability due to this constraint.
Although those complex questions in WebQuestionsSP-tiny could be easily
covered by hand-crafted rules, as many have been, our model is not suitable
for such pre-dened rules. We think it is crucial to use more advanced reason-
ing capabilities to enhance our model Rce-KGQA correctly, a task which we
leave for future work.
5.6 Answer Filtering Result Analysis
To further examine whether our proposed enhancement to the extra module
Relational Chain Reasoning Module with advanced and obvious improvements,

--- PAGE 27 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 27
Models2-hop MetaQA 3-hop MetaQA
full half full half
KVMem 82.7 48.4 48.9 37.6
GraftNet 94.8 69.5 77.7 66.4
PullNet 99.9 90.4 91.4 85.2
EmbedKGQA 98.8 91.8 94.8 70.3
Our Model 99.7 92.1 97.9 84.7
Table 4 Experimental results about reasoning on incomplete KG ( hit@1 as a percentage).
We consider two dierent KG settings, fullandhalf.Full denotes the complete KG and
half denotes a KG subset whose 50% factoid triples are randomly removed.
we analyze the reasoning performance of rst module Answer Filtering Module
and show the answer distributions with prediction in Table 5. In Table 5, as we
can see that, if we regard the scored candidate entities provided by this module
as the nal answer, the hit@1 accuracy rate drops markedly compared to hit@5
andhit@10. This observation indicates that the model which does not consider
relational chain order and relation type achieves very poor performance and
proves the necessity and superiority of our proposed module, the Relational
Chain Reasoning Module .
Furthermore, from Table 5, we can clearly observe that almost right answers
of our used datasets are collectively distributed in the top-5 of our model's
predictions. Due to the high recall rate of our rst module, the Answer Filtering
Module , we think we can only rely on a few top-scoring candidates (such as
top-15, top-10, or even top-5) to further lter the nal answer more accurately.
So, during our model training and inference experiments, we tried several
experimental schemes and considered the number of top-scoring candidates
specically, how to select candidates to automatically generate positive or
negative samples and how to cut the top-N candidates for the sub-module
Relational Chain Reasoning Module inference. The related experimental details
are shown in Sec. 5.6.
Dataset Hit@1 rHit@5 rHit@10 r
2-hop MetaQA 0.861 0.995 0.999
3-hop MetaQA 0.858 0.984 0.997
Table 5 OurAnswer Filtering Module answer reasoning performance on three dierent
metrics Hit@1;5;10. Model performance on Hit@5 and Hit@10 accuracy highly
outperform over Hit@1 accuracy.
5.7 Candidates Filtering Strategy Analysis
Our QA solution Rce-KGQA is a complex pipeline system, and we inevitably
must choose some crucial hyper-parameters to acquire an optimal model. These
pre-dened parameters include full connected/LSTM layer number, dropout
rate, learning rate, and so on. As illustrated in Sec. 5.6 and Table 5, dierent
selection modes about top-scoring candidates during training and inference

--- PAGE 28 ---
Springer Nature 2021 L ATEX template
28 Relational Chain Reasoning Improved Embedded KGQA
have a huge impact on the nal model performances. We now further inves-
tigate what inuence would our model experience in dierent candidates
selection mode.
Firstly, from Table 5 we can observe that our rst step `answering lter-
ing' consistently achieves high recall performances on hit@5. Now we manually
choose and cut-dierence sorted candidate answers and conduct our compar-
ison experiments. Specically, from Sec. 4.3 we know that the dataset for
Relational Chain Reasoning Module training is dynamically constructed by
the last step. And the data construction follows the selection of top- Nsorted
scoring entities in which number Nsimultaneously decides the proportion of
positive/negative samples and the intermediate results selection during the
inference step. For example, in question Q, during model evaluation, we rstly
obtain the intermediate scoring candidate answers f[Ai;Si]gN
i=1. In the next
step, we should use the Relational Chain Reasoning Module to provide all l-
tered candidates with a more precise score to reach a nal answer. Selecting
how many top scoring entities to conduct the further step precisely becomes
our focus research point.
We choose four strategies which include top- f5, 10, 15, 20g; then, from the
corresponding experiments, we receive the following results which is shown in
Table 6. From experimental comparison results we clearly nd that in top-
5 selection strategy, our model achieves the highest hit@1 performances and
achieves the second highest performances in the top-10 strategy, in both 2-hop
MetaQA and 3-hop MetaQA datasets. These phenomena can come from two
aspects. First, the recall rate of the module Relational Chain Reasoning Module
is good enough for the next ne-grained screening and the positive/negative
samples proportion should be in a suitable extent. In addition, more candidates
in the model inference step could introduce more noise entities which could
aect model answer judgment and decrease the overall model performances.
Policy top-5 pick top-10 pick top-15 pick top-20 pick
2-hop MetaQA 0.997 0.994 0.989 0.985
3-hop MetaQA 0.979 0.971 0.967 0.967
Table 6 Our model nal performance statistics about impacts of the four selection
strategies: choose top-5, choose top-10, choose top-15 and choose top-20. The values
reported are hit@1. Bold and underlined fonts denote the best and the second-best
selection strategies.
5.8 Ablation Study
To better understand and gain a deep insight into our model design, we also
perform ablation experiments to investigate systematically the impact and
contributions of dierent components. RceKGQA r, RceKGQA  aand
RceKGQA bare variants of our full model, RceKGQA. Note that, for our
ablated experiments, we remove one component each time. Here we briey
introduce these variants for the ablated experiments.

--- PAGE 29 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 29
â€¢RceKGQA rremoves the Relational Chain Reasoning Module and the
highest scoring entity is provided by the Answer Filtering Module as the
nal answer.
â€¢RceKGQA aremoves all the self-attention operations from the model.
â€¢RceKGQA breplaces RoBERTa with LSTM in the question semantic
representation part of the Relational Chain Reasoning Module .
â€¢RceKGQA is the full model introduced in this paper.
In this section, the following questions are answered:
Q1. How much does the Relational Chain Reasoning Module help our
model's reasoning accuracy?
Q2. Can the attention mechanism really help increase our model's overall
performance?
Q3. Is the eectiveness of our method due to the use of RoBERTa in
theRelational Chain Reasoning Module ?
According to the result comparison between RceKGQA and its variant
RceKGQA r, we can clearly conclude that removing the Relational Chain
Reasoning Module from the proposed model has a huge impact on the results.
The performance gap between RceKGQA  rwhich is shown in Row 2and
our full model as shown in Row 1indicates that the semantic relational chain
factor plays a pivotal role in answer reasoning, which incorporates relational
chain order and relationship type to provide more accurate answers for the
terminal user.
As shown in Row 3, when our full model is compared with RceKGQA  a,
we can see an average 3.5% performance drop across the hit@1 metric if the
self-attention components are removed, demonstrating the importance of the
self-attention mechanism used in our method, as it eectively helps with the
nal answer prediction. A possible reason is that the self-attention mechanism
can distinguish the most relevant and interesting signals from noise informa-
tion, and help our model better understand the question and relational chain
semantic.
RceKGQA b, which is shown in Row 4, only loses 1.6% hit@1 accuracy
compared with our full model, which replaces RoBERTa with BiLSTM, and
demonstrates that using the transformer is not a major factor in increasing
the overall model performance.
The above-ablated statistics conrm that all three components introduced
for handling the multi-hop relation question answering contribute to the overall
model performance.
5.9 Case Study
The major novelty of our approach lies in the introduced relational chain rea-
soning network. Here, we present a case study to demonstrate its contribution
to improving the overall model architecture.
As shown in Fig. 7, given the question, \ In which years were movies
released which starred actors who appeared in the movie [Thunderbolt]? ", the

--- PAGE 30 ---
Springer Nature 2021 L ATEX template
30 Relational Chain Reasoning Improved Embedded KGQA
Model 1-hop MetaQA 2-hop MetaQA 3-hop MetaQA
RceKGQA 98.3 99.7 97.9
RceKGQA  r 85.8 86.1 84.8
RceKGQA  a 96.1 95.9 93.4
RceKGQA  b 95.9 98.2 95.6
Table 7 Ablation study statistical results for RceKGQA and its three variants ( Hit@1 by
percentage). Compared with our full model, sux  rdenotes RceKGQA whose Relational
Chain Reasoning Module is removed, sux  adenotes the RceKGQA variant whose
attention operation is dropped and sux  bdenotes the RceKGQA variant whose
question encoder RoBERTa is replaced with BiLSTM.
Thunderbolt (movie)starred actors reverseStanley Holloway (actor)
starred actorsThe Way Ahead (movie)
release year â€˜1986â€™(year)
release year
â€˜1991â€™(year)
has tags reverse
Chris O'Dowd Terms of Endearment (movie)directed by
Howard Deutchdirected by reverse
Fig. 7 Case analysis from the 3-hop MetaQA dataset. We use green, red, yellow and grey
circles to denote the topic KG nodes, correct answer, intermediate nodes and irrelevant
nodes, respectively. The orange and red coloured circles denote the actual reasoning inter-
mediate nodes and answer nodes. The colour darkness indicates the relevant degree of an
entity by a method.
right reasoning relational chain preserved in KG is Thunderbolt (movie)-
starred actors reverse!Stanley Holloway (actor)-starred actors!The
Way Ahead (movie)-release year!`1986 '(year). When ignoring the rela-
tional chain feature factor and only using the high-scored entity generated
by the Answer Filtering Module as the nal answer, the network mistak-
enly selects a wrong reasoning path Thunderbolt (movie) release year!
`1991 '(year) for the aforesaid question with a very high probability of 0.96 as
the answer. Its attention only focuses on the relationship: release year, which
ignores the repeated relations of starred actors and starred actors reverse . In
comparison, the complete model which considers the relational chain factor
and utilizes the relational chain reasoning network for ne-grained selection
can easily and correctly provide the right answer ` 1986 '(year) with a high
probability of 0.99 from KG.

--- PAGE 31 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 31
This example shows that our relational chain reasoning network indeed
provides very useful supervision signals of relational chain recognition at
intermediate steps to improve our model's overall QA performance.
6 Conclusion and Future Perspective
In this work, we introduce an elaborate KG embedding-based pipeline
approach for the multi-hop KGQA task, termed Relational Chain-based
Embedded KGQA. Novel techniques are proposed to eectively utilize QA
relational chain parsing to identify the semantics more accurately and leverage
the structure information preserved in KG embedding to reason the implicit
answer indirectly. Our comprehensive empirical results on three benchmarks
demonstrate that our method outperforms many of its state-of-the-art coun-
terparts. The experimental comparison between our approach and its ablated
variants also veries that the proposed model components contribute to the
answer reasoning result. We believe KGQA will continue to be an attractive
and promising research direction with realistic industrial and domestic scenar-
ios, such as Intelligent Recommendation, Smart Personal Assistant, Big Data
Mining Services, and Automatic Customer Services.
In the future, we plan to study the following major problems: (i) To sup-
port real-world dynamic application scenarios, the KGQA application is always
updated quickly and inevitably accumulates new and immense external knowl-
edge in real time. How can we augment our available KG's knowledge reserve
automatically and incrementally to expand our system's knowledge cover-
age? (ii) This model is trained on relatively small QA datasets under weak
supervision without external prior knowledge. How can we introduce external
knowledge such as knowledge from web pages and other open-domain KGs to
improve our question answering system's performance?
Following the universal solution patterns of the KGQA task, the method
presented in this paper assumes that \Our model will always choose an opti-
mal answer from the KG". Therefore, the method based on this assumption is
denitely not suitable for the case where the answer does not exist in the KG.
In future work, we will add research on the sub-task of \Detecting whether the
answer exists in the KG". Moreover, our proposed Rce-KGQA is essentially a
pipeline mechanism, which could bring deviation propagation and poor perfor-
mance. In future work, we will also enhance our Rce-KGQA by integrating the
separated Answer Filtering Module and Relational Chain Reasoning Module
together. Concretely, the major factor, which hinders the joint modelling, is the
multiple step-by-step answer retrieval due to the KG's traditional structured
storage pattern. Inspired by Fabio et al. [67] who prove that the huge-volume
PLMs have surprising knowledge storing capabilities, we will try to infuse the
KG knowledge into the Transformer-based PLMs, which can theoretically solve
the end-to-end fashion modelling diculty well from the root.

--- PAGE 32 ---
Springer Nature 2021 L ATEX template
32 Relational Chain Reasoning Improved Embedded KGQA
Acknowledgement
This work was partially supported by the Shanghai Yangfan Program (Project
Code: 22YF1413600), the Major Research Plan of National Natural Science
Foundation of China (Project Code: 92167102), and the Shaanxi Province
Key Industrial Chain Projects (Project Code: NO.2018ZDCXL-GY-04-03-02).
The authors would like to thank Guizhong Liu and Ruiping Yin for providing
helpful discussions and comments.
References
[1] Xiao, J., Kalia, A.K., Vukovic, M.: Juno: An intelligent chat service for
it service automation. In: Service-Oriented Computing { ICSOC 2018
Workshops, pp. 486{490. Springer, Cham (2019)
[2] Bin, F., Yunqi, Q., Chengguang, T., Yang, L., Haiyang, Y., Jian, S.: A sur-
vey on complex question answering over knowledge base: Recent advances
and challenges. CoRR (2020) arXiv:2007.13069
[3] Hao, Y., Zhang, Y., Liu, K., Zhao, J.: An end-to-end model for ques-
tion answering over knowledge base with cross-attention combining global
knowledge, pp. 221{231. Association for Computational Linguistics,
(2017)
[4] Michael, P., Luke, Z.: Simplequestions nearly solved: A new upperbound
and baseline approach. CoRR (2018)
[5] Yunshi, L., Gaole, H., Jinhao, J., Jing, J., Wayne, X., Jirong, W.: A survey
on complex knowledge base question answering: Methods, challenges and
solutions. CoRR (2021) arXiv:2105.11644
[6] Lan, Y., Wang, S., Jiang, J.: Knowledge base question answering with
a matching-aggregation model and question-specic contextual relations.
IEEE/ACM Transactions on Audio, Speech, and Language Processing
27(10), 1629{1638 (2019)
[7] Yunshi, L., Shuohang, W., Jing, J.: Knowledge base question answering
with topic units. In: IJCAI (2019)
[8] Bast, H., Haussmann, E.: More accurate question answering on free-
base. In: Proceedings of the 24th ACM International on Conference on
Information and Knowledge Management, pp. 1431{1440 (2015)
[9] Abujabal, A., Yahya, M., Riedewald, M.: Automated template genera-
tion for question answering over knowledge graphs. In: Proceedings of
the 26th International Conference on World Wide Web, pp. 1191{1200.
International Conference on World Wide Web, (2017)

--- PAGE 33 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 33
[10] Haitian, S., Tania, B.-W., William, W.C.: Pullnet: Open domain question
answering with iterative retrieval on knowledge bases and text, pp. 474{
482. EMNLP, (2019)
[11] Yunqi, Q., Yuanzhuo, Wang, X.J., Kun, Z.: Stepwise reasoning for multi-
relation question answering over knowledge graph with weak supervision,
pp. 474{482. WSDM, (2020)
[12] Dong, L., Wei, F., Zhou, M., Xu, K.: Question answering over Free-
base with multi-column convolutional neural networks, pp. 260{269.
Association for Computational Linguistics, Beijing, China (2015)
[13] Yu, H., Lu, J., Zhang, G.: An online robust support vector regression for
data streams. IEEE Transactions on Knowledge and Data Engineering,
1{1 (2020). https://doi.org/10.1109/TKDE.2020.2979967
[14] Boris, G.: Question-answering system for teaching autistic children to
reason about mental states. Technical report (2000)
[15] Gao, S., Chen, X., Ren, Z., Zhao, D., Yan, R.: Meaningful answer gen-
eration of e-commerce question-answering. ACM Trans. Inf. Syst. 39(2)
(2021). https://doi.org/10.1145/3432689
[16] Yu, H., Lu, J., Zhang, G.: Continuous support vector regression for
nonstationary streaming data. IEEE Transactions on Cybernetics, 1{14
(2020). https://doi.org/10.1109/TCYB.2020.3015266
[17] Xia, N., Yu, H., Wang, Y., Xuan, J., Luo, X.: Dafs: a domain aware
few shot generative model for event detection. Machine Learning 11(12)
(2022). https://doi.org/10.1007/s10994-022-06198-5
[18] Huang, H., Wei, X., Nie, L., Mao, X., Xu, X.-S.: From question to text:
Question-oriented feature attention for answer selection. ACM Trans. Inf.
Syst. 37(1) (2018). https://doi.org/10.1145/3233771
[19] Molino, P., Aiello, L.M., Lops, P.: Social question answering: Textual,
user, and network features for best answer prediction. ACM Trans. Inf.
Syst. 35(1) (2016). https://doi.org/10.1145/2948063
[20] Deepak, N., Jatin, C., Charu, S., Manohar, K.: Learning attention-based
embeddings for relation prediction in knowledge graphs. In: Proceedings
of the 57th Conference of the Association for Computational Linguistics,
ACL 2019, pp. 4710{4723 (2019)
[21] Haitian, S., Bhuwan, D., Manzil, Z., Kathryn, M., Ruslan, S., Cohen,
W.W.: Open domain question answering using early fusion of knowledge
bases and text (2018)

--- PAGE 34 ---
Springer Nature 2021 L ATEX template
34 Relational Chain Reasoning Improved Embedded KGQA
[22] Saxena, A., Tripathi, A., Talukdar, P.: Improving multi-hop question
answering over knowledge graphs using knowledge base embeddings, pp.
4498{4507 (2020)
[23] Mueller, J., Thyagarajan, A.: Siamese recurrent architectures for learning
sentence similarity. In: AAAI, pp. 2786{2792. Proceedings of the Thirtieth
AAAI Conference on Articial Intelligence, (2016)
[24] Gao, J., Yu, H., Zhang, S.: Joint event causality extraction using dual-
channel enhanced neural network. Knowledge-Based Systems 258, 109935
(2022). https://doi.org/10.1016/j.knosys.2022.109935
[25] Yinhan, L., Myle, O., Naman, G., Jingfei, D.: A robustly optimized bert
pretraining approach: Roberta. PMLR, (2019)
[26] LAN, Y., Jing, J.: Query graph generation for answering multi-hop
complex questions from knowledge bases (2020)
[27] He, G., Lan, Y., Jiang, J., Zhao, W.X., Wen, J.-R.: Improving multi-hop
knowledge base question answering by learning intermediate supervision
signals. In: Proceedings of the 14th ACM International Conference on
Web Search and Data Mining. WSDM '21, pp. 553{561. Association for
Computing Machinery, New York, NY, USA (2021). https://doi.org/10.
1145/3437963.3441753
[28] Xiao, H., Jingyuan, Z., Dingcheng, L., Ping, L.: Knowledge graph embed-
ding based question answering, pp. 105{113. Proceedings of the 13th ACM
International Conference on Web Search and Data Mining, (2019)
[29] Chen, Y., Subburathinam, A., Chen, C.-H., Zaki, M.J.: Personalized food
recommendation as constrained question answering over a large-scale food
knowledge graph. Proceedings of the 14th ACM International Conference
on Web Search and Data Mining (2021)
[30] Mo, Y., Wenpeng, Y., Kazi, S.H., Bowen, Z.: Improved neural relation
detection for knowledge base question answering. In: Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics,
pp. 571{581. Association for Computational Linguistics, (2017)
[31] Alexander, H.M., Adam, F., Jesse, D., Amir-Hossein, K.: Key-value
memory networks for directly reading documents, pp. 249{256. EMNLP,
(2016)
[32] Kun, X., Yuxuan, L., Yansong, F., Zhiguo, W.: Enhancing key-value
memory neural networks for knowledge based question answering. In: Pro-
ceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 2937{2947. Association for

--- PAGE 35 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 35
Computational Linguistics, (2019)
[33] Bill, Yuchen, L., Xinyue, C., Jamin, C., Xiang, R.: Kagnet: Knowledge-
aware graph networks for commonsense reasoning, pp. 2829{2839. Asso-
ciation for Computational Linguistics, (2019)
[34] Kipf, T.N., Welling, M.: Semi-supervised classication with graph convo-
lutional networks. In: 5th International Conference on Learning Repre-
sentations, pp. 486{490. ICLR, April 24-26 (2017)
[35] Wang, R., Rossetto, L., Cochez, M., Bernstein, A.: QAGCN: A graph con-
volutional network-based multi-relation question answering system. arXiv
(2022). https://doi.org/10.48550/ARXIV.2206.01818
[36] Das, R., Godbole, A., Naik, A., Tower, E., Zaheer, M., Hajishirzi, H.,
Jia, R., Mccallum, A.: Knowledge base question answering by case-
based reasoning over subgraphs. In: Chaudhuri, K., Jegelka, S., Song, L.,
Szepesvari, C., Niu, G., Sabato, S. (eds.) Proceedings of the 39th Interna-
tional Conference on Machine Learning. Proceedings of Machine Learning
Research, vol. 162, pp. 4777{4793. PMLR, (2022)
[37] Zi-Yuan, C., Chih-Hung, C., Lun-Wei, K.: Uhop: An unrestricted-hop
relation extraction framework for knowledge-based question answering.
In: NAACL (2019)
[38] Jain, S.: Question answering over knowledge-base using factual memory
networks. In: NAACL (2016)
[39] Yao, X., Van, D.: Information extraction over structured data: question
answering with freebase (2014)
[40] Drew, A.H., Christopher, D.M.: Learning by abstraction: The neural state
machine. In: NeurIPS, pp. 5901{5914 (2019)
[41] A.Bordes, N.Usunier, A.Garcla-Duran, J.Weston, O.Yakhnenko: Trans-
lating embeddings for modeling multi-relational data. In: Proc. Adv.
Neural Inf. Process, pp. 2787{2795 (2013)
[42] Wang, A., Zhang, J., Feng, J., Chen, Z.: Knowledge graph embedding by
translating on hyperplanes. In: 28th AAAI, pp. 1112{1119 (2014)
[43] Min-Chul, Y., Do-Gil, L., HaeChang, R.: Knowledge-based question
answering using the semantic embedding space. In: Expert Systems with
Applications, pp. 9086{9104 (2015)
[44] Trouillon, T., Welbl, J., Riedel, S., Gaussier, E., Bouchard, G.: Complex
embeddings for simple link prediction. In: International Conference on

--- PAGE 36 ---
Springer Nature 2021 L ATEX template
36 Relational Chain Reasoning Improved Embedded KGQA
Machine Learning, pp. 2071{2080 (2016)
[45] Sun, Z., Wang, C., Hu, W., Chen, M., Dai, J., Zhang, W., Qu, Y.:
Knowledge graph alignment network with gated multi-hop neighborhood
aggregation 34, 222{229 (2020)
[46] Tingting, J., Hao, W., Xiangfeng, L., Xie, S., Jingchao, W.: Mifas:
Multi-source heterogeneous information fusion with adaptive importance
sampling for link prediction. (2021). https://doi.org/10.1111/exsy.12888
[47] Afzal, A., Sading, M., Hussain, M., Ali, M., Lee, S., Khattak, A.:
Knowledge-based reasoning and recommendation framework for intelli-
gent decision making. In: Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, pp. 571{581. Expert Systems,
(2018). https://doi.org/10.1111/exsy.12242
[48] Chopra, S., LeCun, Y.: Learning a similarity metric discriminatively with
application to face verication. In: IEEE Computer Society Conference
Computer Vision and Pattern Recognition, pp. 539{546 (2005)
[49] Jerey, P., Richard, S., Christopher, M.: Global vectors for word repre-
sentation. In: In EMNLP, pp. 1532{1543 (2014)
[50] Ashish, V., Noam, S., Niki, P., Jakob, U., Llion, J., Gomez, A.N., Lukasz,
K., Illia, P.: Attention Is All You Need (2017)
[51] Wang, Q., Mao, Z., Wang, B., Guo, L.: Knowledge graph embedding: A
survey of approaches and applications. IEEE Transactions on Knowledge
and Data Engineering 29(12), 2724{2743 (2017)
[52] Nickel, M., Rosasco, L., Poggio, T.: Holographic embeddings of knowledge
graphs. In: in Proc. 30th AAAI Conf 2016, pp. 1955{1961 (2016)
[53] M, N., Tresp, V., Kriegel, H.-P.: A three-way model for collective learning
on multi-relational data. In: in Proc. 28th Int Conf, pp. 809{816 (2011)
[54] Glorot, Bengio, Y.: Understanding the diculty of training deep feed-
forward neural networks. In: Proceedings of the Thirteenth International
Conference on Articial Intelligence and Statistics, vol. 9, pp. 249{256.
PMLR, (2010)
[55] He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiers: Surpassing
human-level performance on ImageNet classication (2015)
[56] Xavier, G., Antoine, B., Yoshua, B.: Deep sparse rectier neural networks.
In: Proceedings of the Fourteenth International Conference on Articial
Intelligence and Statistics, vol. 15, pp. 315{323 (2011)

--- PAGE 37 ---
Springer Nature 2021 L ATEX template
Relational Chain Reasoning Improved Embedded KGQA 37
[57] Yuyu, Z., Hanjun, D., Zornitsa, Kozareva, Alexander, J, S., Le, S.: Vari-
ational reasoning for question answering with knowledge grap. In: In
Thirty-Second AAAI Conference on Articial Intelligence, pp. 2787{2795
(2018)
[58] Wentau, Y., Matthew, R., Christopher, M., Ming-Wei, C., Jina, S.: The
value of semantic parse labeling for knowledge base question answering.
In: In ACL, pp. 2787{2795 (2016)
[59] Reid, A., Fan, R.K.C., Kevin, J.L.: Local graph partitioning using
pagerank vectors. In: FOCS (2006)
[60] Alon, T., Jonathan, B.: The web as a knowledge-base for answering
complex questions. In: Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, pp. 641{651. In NAACL-HLT,
(2018)
[61] Ding, H., Huang, S., Jin, W., Shan, Y., Yu, H.: A novel cascade model for
end-to-end aspect-based social comment sentiment analysis. Electronics
11(12) (2022). https://doi.org/10.3390/electronics11121810
[62] Jin, W., Yu, H., Luo, X.: Cvt-assd: Convolutional vision-transformer
based attentive single shot multibox detector. In: 2021 IEEE 33rd Inter-
national Conference on Tools with Articial Intelligence (ICTAI), pp.
736{744 (2021). https://doi.org/10.1109/ICTAI52525.2021.00117
[63] Zhao, Z., Yu, H., Luo, X., Gao, J., Xu, X., Shengming, G.: Ia-icgcn:
Integrating prior knowledge via intra-event association and inter-event
causality for chinese causal event extraction. In: Pimenidis, E., Angelov,
P., Jayne, C., Papaleonidas, A., Aydin, M. (eds.) Articial Neural Net-
works and Machine Learning { ICANN 2022, pp. 519{531. Springer, Cham
(2022)
[64] Tao, Q., Luo, X., Wang, H., Xu, R.: Enhancing relation extraction using
syntactic indicators and sentential contexts. In: 2019 IEEE 31st Inter-
national Conference on Tools with Articial Intelligence (ICTAI), pp.
1574{1580 (2019). https://doi.org/10.1109/ICTAI.2019.00227
[65] Gu, H., Yu, H., Luo, X.: Dbgare: Across-within dual bipartite graph atten-
tion for enhancing distantly supervised relation extraction. In: Memmi,
G., Yang, B., Kong, L., Zhang, T., Qiu, M. (eds.) Knowledge Science,
Engineering and Management, pp. 400{412. Springer, Cham (2022)
[66] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier,
D., Auli, M.: fairseq: A fast, extensible toolkit for sequence modeling. In:
Proceedings of NAACL-HLT 2019: Demonstrations (2019)

--- PAGE 38 ---
Springer Nature 2021 L ATEX template
38 Relational Chain Reasoning Improved Embedded KGQA
[67] Petroni, F., Rockt aschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y.,
Miller, A.: Language models as knowledge bases? In: Proceedings of the
2019 Conference on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pp. 2463{2473. Association for Computa-
tional Linguistics, Hong Kong, China (2019). https://doi.org/10.18653/
v1/D19-1250. https://aclanthology.org/D19-1250
