# 2009.13964.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2009.13964.pdf
# File size: 3137206 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CokeBERT: Contextual Knowledge Selection and Embedding towards
Enhanced Pre-Trained Language Models
Yusheng Su1, Xu Han1, Zhengyan Zhang1, Yankai Lin2,
Peng Li2,Zhiyuan Liu1,Jie Zhou2,Maosong Sun1
1Department of Computer Science and Technology, Tsinghua University, Beijing, China
Institute for Artiﬁcial Intelligence, Tsinghua University, Beijing, China
State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China
2Pattern Recognition Center, WeChat AI, Tencent Inc.
fsuys19,hanxu17 g@mails.tsinghua.edu.cn,
Abstract
Several recent efforts have been devoted to en-
hancing pre-trained language models (PLMs)
by utilizing extra heterogeneous knowledge in
knowledge graphs (KGs), and achieved con-
sistent improvements on various knowledge-
driven NLP tasks. However, most of these
knowledge-enhanced PLMs embed static sub-
graphs of KGs (“knowledge context”), regard-
less of that the knowledge required by PLMs
may change dynamically according to speciﬁc
text (“textual context”). In this paper, we
propose a novel framework named Coke to
dynamically select contextual knowledge and
embed knowledge context according to tex-
tual context for PLMs, which can avoid the ef-
fect of redundant and ambiguous knowledge
in KGs that cannot match the input text. Our
experimental results show that Coke outper-
forms various baselines on typical knowledge-
driven NLP tasks, indicating the effective-
ness of utilizing dynamic knowledge context
for language understanding. Besides the per-
formance improvements, the dynamically se-
lected knowledge in Coke can describe the se-
mantics of text-related knowledge in a more in-
terpretable form than the conventional PLMs.
Our source code and datasets will be available
to provide more details for Coke.
1 Introduction
Pre-trained language models (PLMs) such as
BERT (Devlin et al., 2019) and RoBERTa (Liu
et al., 2019) have achieved state-of-the-art perfor-
mance on a wide range of natural language pro-
cessing (NLP) tasks. As some research (Poerner
et al., 2019) suggests that these PLMs still strug-
gle to learn factual knowledge, intensive recent
efforts (Lauscher et al., 2019; Yoav et al., 2019; Yu
et al., 2019; Wang et al., 2019; Zhang et al., 2019;
Peters et al., 2019; He et al., 2019; Liu et al., 2020)
indicates equal contribution
Steph Curry DQGKlay ThompsonOHGcWKH:DUULRUVWRWKH1%$&KDPSLRQVKLS
Steph CurryKlay Thompson
RileyDavidson CollegeWashington State UniversityWarriorsPlay forPlay forGraduates fromGraduates fromDaughter of Steph CurryKlay ThompsonTeammate
Low ImportanceHigh ImportanceTextKGFigure 1: The example of capturing knowledge context
from a KG and incorporating them for language under-
standing. Different sizes of circles express different en-
tity importance for understanding the given sentence.
have therefore been devoted to leveraging rich het-
erogeneous knowledge in knowledge graphs (KGs)
to enhance PLMs.
An ideal process for injecting factual knowledge
into PLMs is to ﬁrst identify mentioned entities1in
the input text (“textual context”), then dynamically
select sub-graphs (“knowledge context”) centered
on these mentioned entities from KGs, and ﬁnally
embed the selected knowledge context for PLMs.
Intuitively, knowledge context contributes to better
language understanding on the one hand, serving
as an effective complementarity to textual context.
For example, given two entities Steph Curry and
Klay Thompson in Figure 1, we can infer that they
play for the same basketball team, which is not
explicitly described in the given sentence. On the
other hand, not all knowledge in KGs is relevant
to textual context, e.g., the fact ( Riley ,Daughter
of,Steph Curry ) has no positive effect on under-
standing the given sentence.
We argue that it is meaningful to dynamically se-
lect appropriate knowledge context that can match
speciﬁc textual context for enhancing PLMs. How-
1Those words or phrases in the text corresponding to cer-
tain entities in KGs are often named “entity mentions”.arXiv:2009.13964v5  [cs.CL]  5 Apr 2023

--- PAGE 2 ---
ever, most knowledge context utilized in existing
knowledge-enhanced PLMs is not highly matching
textual context: (1) ERNIE (Zhang et al., 2019) just
uses entities mentioned in the text as knowledge
context and only injects the embeddings of these
entities into PLMs, ignoring informative neighbors
in KGs; (2) KnowBert (Peters et al., 2019), K-
BERT (Liu et al., 2020) and K-A DAPTER (Wang
et al., 2020) consider more information as knowl-
edge context than ERNIE (e.g, entity properties in
KGs), yet their knowledge context is still static and
cannot dynamically change according to textual
context. As we mentioned before, not all informa-
tion in static knowledge context can match textual
context, and the knowledge interfere with redun-
dant and ambiguous information may interfere un-
derstanding semantics. Hence, how to dynamically
select and embed knowledge context according to
textual context for PLMs still remains a challenge.
To alleviate the issue, we propose a novel frame-
work named Coke to dynamically select knowledge
context matching textual context and embed the
dynamic context for enhancing PLMs: (1) For dy-
namically selecting knowledge context , accord-
ing to textual context, we propose a novel semantic-
driven graph neural network (S-GNN). Given an
entity mentioned in textual context, S-GNN lever-
ages an attention mechanism to ﬁlter out irrelevant
KG information by assigning scores to neighbors
(1-hop, 2-hop, etc) and relations between entities
based on textual context. The score can weigh
how much the information in KGs matches textual
context and help Coke dynamically select an ap-
propriate sub-graph as the knowledge context of
the given entity mention. (2) For dynamically em-
bedding knowledge context , given a mentioned
entity, S-GNN computes its representation condi-
tioned on both its pre-trained entity embedding and
the information aggregated from the selected con-
textual sub-graph in a recursive way, making Coke
be aware of both global and local KG information
and grasp the text-related information. (3) By fus-
ing the embeddings of dynamic knowledge con-
textfor PLMs with speciﬁc training and adaption
strategies, Coke improves language understanding
and beneﬁts for downstream applications.
Following existing work, we conduct experi-
ments on four datasets for two typical knowledge-
driven tasks, i.e., entity typing and relation classi-
ﬁcation. The experimental results show that Coke
outperforms various baselines, indicating the ef-fectiveness of dynamically selecting and embed-
ding knowledge context for PLMs. Moreover,
some qualitative analyses also suggest that, as com-
pared with the state-of-the-art knowledge-enhanced
PLMs, our model not only achieves competitive
results but also provides a more interpretable ap-
proach to describing speciﬁc words based on their
dynamic knowledge context.
2 Related Work
Intuitively, two types of context are involved in
language understanding: (1) the semantic informa-
tion of the text ( textual context ), and (2) the factual
knowledge related to the text ( knowledge context ).
The typical PLMs focus on capturing information
from the textual context, like ELMO (Peters et al.,
2018), GPT (Radford et al., 2018), BERT (De-
vlin et al., 2019), XLNET (Yang et al., 2019), and
RoBERTa (Liu et al., 2019). In order to enable
PLMs to better understand the knowledge con-
text, intensive efforts have been devoted to inject-
ing various factual knowledge of KGs into PLMs.
ERNIE (Zhang et al., 2019) links entity mentions
in textual context to their corresponding entities in
KGs and then inject the pre-trained embeddings of
the corresponding entities into PLMs. Although
ERNIE has shown the feasibility and effectiveness
of fusing knowledge embeddings for enhancing
PLMs, it still doees not consider the informative
neighbors of entities.
To this end, various models have been proposed
to further incorporate a wider range of knowledge
information. KnowBert (Peters et al., 2019) and
KRL (He et al., 2019) employ attention mech-
anisms to learn more informative entity embed-
dings based on the entity-related sub-graphs. Nev-
ertheless, the computation of entity embeddings
is independent of textual context. K-BERT (Liu
et al., 2020) heuristically converts textual context
and entity-related sub-graphs into united input se-
quences, and leverages a Transformer (Vaswani
et al., 2017) with a specially designed attention
mechanism to encode the sequences. Unfortu-
nately, it is not trivial for the heuristic method
in K-BERT to convert the second or higher or-
der neighbors related to textual context into a se-
quence without losing graph structure information.
K-A DAPTER (Wang et al., 2020) proposes variant
frameworks to inject factual knowledge in differ-
ent domains, yet still suffers from the similar issue
like K-BERT. Although most existing knowledge-

--- PAGE 3 ---
enhanced PLMs are aware of utilizing both textual
context and knowledge context, their knowledge
context cannot change with textual context, like
ERNIE using single entities, KRL and KnowBert
embedding sub-graphs independently of textual
context, K-BERT and K-A DAPTER using ﬁxed sub-
graphs. In contrast, our proposed Coke model can
leverage dynamic sub-graphs of arbitrary size as
knowledge context according to textual context.
There are also several PLM methods for captur-
ing knowledge from only textual context. Span-
BERT (Mandar et al., 2019) and ERNIE 1.0-
Baidu (Yu et al., 2019) propose to predict
masked variable-length spans or entity mentions
to encourage PLMs to learn multi-token phrases.
WKLM (Xiong et al., 2019) is trained to distin-
guish whether an entity mention has been replaced
with the name of other entities having the same
type to learn entity types. LIBERT (Lauscher et al.,
2019) and SenseBERT (Yoav et al., 2019) extend
PLMs to predict word relations (e.g., synonym and
hyponym-hypernym) and word-supersense respec-
tively to inject lexical-semantic knowledge. More-
over, there are also efforts on continual knowledge
infusion (Yu et al., 2020; Wang et al., 2020). Al-
though these models do not use extra knowledge
context to understand factual knowledge, they are
complementary to our work and can be used to-
gether towards better PLMs.
3 Methodology
As shown in Figure 2, Coke consists of three mod-
ules:
(1)Text Encoder computes embeddings for the
input text, i.e. textual context;
(2)Dynamic Knowledge Context Encoder
ﬁrst dynamically selects knowledge context accord-
ing to textual context, and then computes contex-
tual knowledge embeddings conditioned on both
textual context and KG context;
(3)Knowledge Fusion Encoder fuses both tex-
tual context and dynamic knowledge context em-
beddings for better language understanding. In this
section, we will ﬁrst give the notations and then
present the three modules in details.
3.1 Notations
A KG is denoted by G=f(h;r;t )jh;t2E;r2
Rg, whereEandRare the set of entities and re-
lations respectively. For each fact (h;r;t )2G,
it indicates that there is a relation rbetween thehead entityhand the tail entity t. Given a token se-
quenceS=fwjgN
j=1of the length N, some tokens
in the sequence may correspond to certain entities
inE, we name these tokens “entity mentions” and
denote their mentioned entities in KGs as fejgM
j=1,
whereMis the number of mentioned entities2.
3.2 Text Encoder
Similar to existing knowldege-enhanced PLMs,
Coke leverages a L-layer bidirectional Transformer
encoder (Vaswani et al., 2017; Devlin et al., 2019)
to embed the input text (tokens) S=fwjgN
j=1and
obtain its textual context representations, which is
denoted as T-Encoder (),
fwjgN
j=1=T-Encoder (fwjgN
j=1):(1)
AsT-Encoder ()is the same as that used in
BERT, we refer the readers to the original pa-
per (Devlin et al., 2019) for more details.
3.3 Dynamic Knowledge Context Encoder
Constructing Raw Knowledge Context As
KGs are often in a large scale, we ﬁrst construct raw
knowledge context for computational efﬁciency.
Then we dynamically select and embed appropriate
knowledge context that can match the textual con-
text. Speciﬁcally, given a mentioned entity m2E
mentioned by the input text S=fwjgN
j=1, we de-
ﬁne its raw knowledge context Gmas a sub-graph
ofGcentered inm. The entities ofGmare at most
K-hops away from m. Formally, we deﬁne the 0-
hop away entity set as E0
m=fmg. Then thei-hop
away entity setEi
mcan be deﬁned recursively as
  !Ei
m=n
th2Ei 1
m^t =2Si 1
j=0Ej
m;(h;r;t )2Go
;
  Ei
m=n
ht2Ei 1
m^h =2Si 1
j=0Ej
m;(h;r;t )2Go
;
Ei
m=  !Ei
m[  Ei
m:
(2)
Intuitively, all entities in Ei
m(both head or tail en-
tities) only have relations to the entities in Ei 1
m.
Then, the raw knowledge context Gmand its entity
setEmcan be deﬁned as
Em=K[
i=0Ei
m
Gm=
(h;r;t )h2Em^t2Em;
(h;r;t )2G
:(3)
2Typically, M 6=Nas an entity may correspond to multi-
ple different tokens. In this work, we use the toolkit TAGME
to identify the mentioned entities.

--- PAGE 4 ---
Kx DK-Encoder  Px K-Encoder  Steph Curry DQGKlay ThompsonOHGcWKH:DUULRUVWRWKH1%$&KDPSLRQVKLS
2XWSXW…
…Play for
Steph CurryWarriors
Daughter of 
RileySteph Curry
Warriors
Riley
Clay
S.F
C.A
N.C
Riley
WarriorsPlay forLocatedLive inwas Born in…
…
…
…Steph CurryRileyWarriors………………Nx T-Encoder  
Selecting and Embedding Knowledge ContextConstructing Raw Knowledge Context1-hop2-hopD
Steph Curry DQGKlay ThompsonOHGcWKH:DUULRUVWRWKH1%$&KDPSLRQVKLSInput for Common NLP tasks:StephCurryandKlay[CLS]ThompsonInput for Entity Typing tasks:andKlay[CLS]Thompson[ENT]StephCurry[ENT]Input for Relation Classiﬁcation tasks:andKlay[CLS]Thompson[HD]StephCurry[HD][TL][SEP][SEP][SEP][TL][CLS][ENT][HD][TL]Common NLP tasksEntity TypingRelation Classiﬁcation………EFigure 2: (a) The upper part is the overall framework of Coke and illustrates how to generate entity representations.
(b) The lower part is the example of inserting special tokens to the input sequence for speciﬁc tasks during ﬁne-
tuning.
Selecting and Embedding Knowledge Context
To dynamically select informative features in Gm
and embed these features for PLMs, we propose
a semantic-driven graph neural network (S-GNN).
For each entity in Gm, i.e.,e2Em, we initialize
its input features for S-GNN with its embedding
pre-trained by TransE (Bordes et al., 2013) (Other
knowledge embedding models can also provide pre-
trained embeddings for S-GNN), and named the
initialized features as e0.
In order to fully transfer the structure and knowl-
edge information among entities in Gm, S-GNN
consists of several hidden layers to aggregate infor-
mation following the structure of Gm. At thei-th
layer, given an entity e2Em, S-GNN aggregates
all information from its neighbors entity nandrin
Gm,
hi
n!e=Wi[n+r;ni 1];(n;r;e )2Gm
Wi[n r;ni 1];(e;r;n )2Gm;
(4)
where ni 1is the embedding of nat thei 1layer,
nandrare the entity and relation embeddings
respectively pre-trained by TransE, Wiis a learn-
able linear matrix, and [;]denotes the horizontal
concatenation of vectors. Then the embedding of e
at thei-th layer can be computed as
ei=fi(fhi
n!egn2Ne); (5)whereNeis the neighboring set of e,fi()is the
function to aggregate information at the i-th layer
and will be introduced in detail next.
As not all information in the raw knowledge con-
textGmis useful for understanding the input text to-
kensS=fwjgN
j=1, we design a special semantic
attention mechanism as the function fiin Eq. (5)
to ﬁlter out irrelevant information and aggregate
essential information. The attention mechanism
functionfcan be formally denoted as follows,
fi(fhi
^e!eg^e2Ne) =
X
^e2Neexp(k>
^eq)P
~e2Neexp(k>
~eq)hi
^e!e;(6)
where q,knare referred to as query and key vectors
respectively.
To dynamically select information according to
textual context, the query vector qcomes from the
embedding of the input text (tokens):
q=
cWis+bbi
; (7)
where= tanh(),cWiandbbiare the learnable
linear matrix and bias vector respectively for the
query vector at the i-th layer, sis the whole seman-
tic embedding of the input text (tokens). Specially,
following BERT (Devlin et al., 2019), we place a

--- PAGE 5 ---
special token [CLS] at the beginning of the input
sequence, and sis the output embedding of [CLS]
computed by Eq. (1).
The key vector knis based on the embedding of
the relation between the entity eand its neighboring
entityn, and computed as
kn=(
fWi( r)+ebi;(e;r;n )2Gm
fWir+ebi; (n;r;e )2Gm;(8)
wherefWiandebiare the learnable linear matrix
and bias vector respectively for the key vector at
thei-th layer. Two triples with head an tail entities
switched will get the reverse key vectors.
In summary, S-GNN utilizes textual context to
adjust the weight of feature aggregation, and ﬁnally
selects and embeds knowledge related to the tex-
tual context into embbedings for PLMs. Hence,
given the mentioned entity m, the output embed-
ding ofmat the last layer of S-GNN is its ﬁnal
embedding computed by its dynamic knowledge
context. For simplicity, given the input text (tokens)
fwjgN
j=1and the mentioned entities fejgM
j=1, the
whole computation to achieve dynamic knowledge
context embeddings is denoted as,
fejgM
j=1=DK-Encoder (fejgM
j=1;fwjgN
j=1):
(9)
3.4 Knowledge Fusion Encoder
Knowledge fusion encoder aims to fuse the infor-
mation of contextual entity embedding fejgM
j=1
and the text (tokens) embedding fwjgN
j=1. We
leverage the encoder K-Encoder ()similar to
(Zhang et al., 2019) to serve the purpose,
fwo
jgN
j=1;feo
jgM
j=1=
K-Encoder (fwjgN
j=1;fejgM
j=1)(10)
We refer the readers to (Zhang et al., 2019) for
more details. Roughly speaking, K-Encoder ()
consists ofPaggregators. As shown in Figure 2,
in each aggregator, there are two multi-head self-
attentions injecting text (tokens) and contextual
knowledge embeddings respectively, and a multi-
layer perceptron (MLP) fusing two heterogeneous
features.
3.5 Training Details
Pre-Training Strategies To incorporate knowl-
edge embeddings into language understanding, we
randomly mask token-entity alignments and let themodel learn to predict all corresponding entities
for these tokens by masking their alignments. We
refer this to a denoising entity auto-encoder (dEA),
which is one of the pre-training tasks for existing
knowledge-enhanced PLMs (Zhang et al., 2019).
Besides, we choose BERT BASE(Devlin et al.,
2019), RoBERTa BASE(Liu et al., 2019), and
RoBERTa LARGE (Liu et al., 2019) as our base mod-
els. Considering that our base models are originally
pre-trained by different pre-training tasks, we have
two different training objectives for them.
For the CokeBERT
BASE, which is based on BERT BASE,
the training objective can be described as:
L=LMLM+LNSP+LdEA; (11)
where theLMLMandLNSPare loss functions for
masked language model and next sentence predic-
tion correspondingly. The denoising entity auto-
encoder (dEA) loss is LdEA.
For CokeROBERT A
BASE and CokeROBERT A
LARGE , which
are representatively based on RoBERTa BASEand
RoBERTa LARGE, their training objective can be de-
scribed as:
L=LMLM+LdEA; (12)
where the sentence prediction loss is removed.
Fine-Tuning for Downstream Tasks Coke ap-
plies the ﬁne-tuning procedure similar to BERT
and take the ﬁnal output embedding of the ﬁrst to-
ken[CLS] for various common NLP tasks. Sim-
liar to the previous knowledge-enhanced PLMs, for
knowledge-driven tasks such as entity typing and
relation classiﬁcation, we apply speciﬁc ﬁne-tuning
procedures. As shown in Figure 2, to help Coke
combine context information and entity mention
attentively, we modify the input sequence with the
mention markers. We attend the token which is in
front of the entity mention as [ENT] and then use
the ﬁnal output embedding of [ENT] for the entity
typing task. As for the relation classiﬁcation task,
we insert [HD] and[TL] tokens for head entities
and tail entities respectively, and concatenate the
[HD] representation and [TL] representation as
ﬁnal representation (Baldini Soares et al., 2019) for
the task.
4 Experiments
In the experiments, we ﬁrst introduce the training
dataset and other training details of our model. Af-
ter that, we give an empirical analysis to show the

--- PAGE 6 ---
Dataset Train Dev Test Type Rel
FIGER 2,000,000 10,000 563 113 -
Open Entity 2,000 2,000 2,000 6 -
FewRel 8,000 16,000 16,000 - 80
TACRED 68,124 22,631 15,509 - 42
Table 1: The statistics of FIGER, Open Entity, FewRel,
and TACRED datasets.
usefulness of the selected knowledge context. Then
we compare Coke with several strong baselines in
two typical knowledge-guided tasks including en-
tity typing and relation classiﬁcation. Finally, we
perform an ablation study to show the effectiveness
of our dynamic knowledge context encoder.
4.1 Training Dataset
We use English Wikipedia3as our pre-training cor-
pus and align the entity mentions to Wikidata with
widely-used entity linking tool TAGME (Ferragina
and Scaiella, 2010). There are nearly 4;500M sub-
words and 140M entities in the pre-training corpus
and we we sample 24;267;796fact triples, includ-
ing5;040;986entities in Wikidata. We conduct
our experiments on the following datasets: FIGER,
Open Entity, FewRel, and TACRED. The statistics
of these datasets are shown in Table 1. Besides, we
use knowledge embeddings of WikiData released
by (Zhang et al., 2019).
4.2 Experimental Settings
Training and Parameter Settings In experi-
ments, we choose BERT BASE(Devlin et al., 2019),
RoBERTa BASEand RoBERTa LARGE (Liu et al.,
2019) as our base models. To reduce the cost of
training from scratch, we adopt these models’ re-
leased parameters to initialize our text encoder and
the rest of parameters of Coke are all initialized
randomly.
For optimization, we set the learning rate as
510 5, the max sequence length as 256, the
batch size as 32, and the rest settings largely follow-
ing the original PLMs. For ﬁne-tuning, we use the
same parameters as pre-training except the batch
sizes and the learning rates. In all downstream
tasks, we select the batch size from f16,32,64g,
the learning rate is 210 5, the number of epochs
fromf5,6,7,8,9,10g. The following ranges of
value all perform well. Besides, to prevent Coke
from overﬁtting in FIGER, we use large batch size
3https://en.wikipedia.org/1024 . We refer more details of training and hyper-
parameter settings to our Appendix.
Baselines We split baseline models into three
groups:
BERT BASEbased models, RoBERTa BASEbased
models, and RoBERTa LARGE based models. For
the sake of fairness, all models only incorporate
factual knowledge from Wikidata. For knowledge-
enhanced PLMs like ERNIE, KnowBert, and K-
BERT, we re-implement them or use their released
code for our experiments, and report the results
which can match their results in the original papers.
As K-A DAPTER is similar to K-BERT and without
any released code, we thus directly compare with
K-BERT rather than K-A DAPTER .
4.3 Empirical Analysis for Dynamically
Selecting Knowledge Context
To demonstrate Coke is able to capture useful in-
formation from KGs, we design a qualitative and
quantitative experiments to evaluate Coke.
In the qualitative experiment, given the same
entity mentions in different context, we adopt
PLMs for selecting text-related 1-hop triples (“ 1-
hop knowledge context”) from Wikidata, which is
similar to Eq. (6) without summation. More speciﬁ-
cally, we apply the [CLS] of the input text (tokens)
computed by these PLMs to attend each neighbour-
ing triple of entity mentions.
As shown in Table 2, when given the sentence
“:::Bill Gates and Mark Zuckerberg dropped out of
Harvard:::” indicating the relation alumni between
Mark Zuckerberg and Bill Gates, our model pays
more attention to the factual knowledge of their
education. Yet when given the sentence “ Bill Gates
and Mark Zuckerberg are working together :::” in-
dicating the cooperation between Mark Zuckerberg
and Bill Gates, the factual knowledge of their enter-
prises is considered by our model. Apparently, we
can ﬁnd the importance scores of attended triples
is interpretable and can help us understand the se-
mantics more clearly.
In the quantitative experiment, we annotate the
test sets of FewRel and TACRED. Given a sam-
ple, including context and the corresponding entity
mentions, we manually annotate its 1-hop triples by
judging the relevance between context and triples.
Finally, we extract 15981 instances from FewRel
and5684 instances from TACRED. By ranking
importance scores of all triples for an entity men-
tion and setting a threshold, we can obtain positive

--- PAGE 7 ---
Text:[CLS] Both Microsoft co-founder Bill Gates and Facebook
co-founder Mark Zuckerberg dropped out of Harvard and began
building their companies right around the same time.
Factual triple : Mark Zuckerberg, Bill Gates, alumnus
Entity h: Mark Zuckerberg
Importance Entity t Relation
19% Harvard University educated at
19% Phillips Exeter Academy educated at
19% Ardsley High School educated at
10% Facebook CEO of
10% Chief executive ofﬁcer position held
6% Businessperson occupation
6% Computer scientist occupation
6% Palo Alto, California residence
3% White Plains, New York place of birth
2% Mandarin Chinese languages spoken
Entity h: Bill Gates
Importance Entity t Relation
35% Harvard University educated at
11% Microsoft CEO of
11% Chief executive ofﬁcer position held
9% American Academy of member of
Arts and Sciences
9% National Academy member of
of Engineering
6% Computer scientist occupation
6% Investor occupation
6% Businessperson occupation
4% Bill&Melinda Gates Foundation foundation of
3% United States citizenshipText:[CLS] Bill Gates and Mark Zuckerberg are working together
to fund research for COVID-19 treatments.
Factual triple : Mark Zuckerberg, Bill Gates, cooperate
Entity h: Mark Zuckerberg
Importance Entity t Relation
15% Facebook CEO of
14% Chief executive ofﬁcer position held
11% Businessperson occupation
11% Computer scientist occupation
9% Harvard University educated at
9% Phillips Exeter Academy educated at
9% Ardsley High School educated at
8% Palo Alto, California residence
7% White Plains, New York place of birth
7% Mandarin Chinese languages spoken
Entity h: Bill Gates
Importance Entity t Relation
33% Bill&Melinda Gates Foundation foundation of
10% Microsoft CEO of
9% Chief executive ofﬁcer position held
8% American Academy of member of
Arts and Sciences
8% National Academy member of
of Engineering
7% Computer scientist occupation
7% Investor occupation
7% Businessperson occupation
6% Harvard University educated at
5% United States citizenship
Table 2: The shade of color expresses the importance of triples for a given sentence.
FewRel TACRED
P R F1 P R F1
ERNIE 87.6 50.6 64.1 81.1 41.8 55.1
CokeBERT
BASE 87.9 71.5 78.9 86.1 58.4 69.6
CokeROBERT A
BASE 79.8 84.0 81.9 74.9 72.0 73.4
Table 3: The results of capturing positive triples from
the labeled triples on FewRel and TACRED (%).
triples and negative triples to calculate F1 scores
for evaluation.
To fairly demonstrate effectiveness of extracting
triples via Coke, we choose ERNIE as our base-
line model, which inherently aligns the language
embedding space and KG embedding space using
the same training data as Coke. As shown in Ta-
ble 3, the F1 scores of Coke are better than the
baseline model by 14:8%-17:8%on FewRel and
14:5%-18:3% on TACRED.
4.4 Overall Evaluation Results
In this section, we compare our models with var-
ious effective PLMs on entity typing and relation
classiﬁcation, including both vanilla PLMs andknowledge-enhanced PLMs.
Entity Typing Given an entity mention and its
corresponding sentence, entity typing requires to
classify the entity mention into its types. For this
task, we ﬁne-tune Coke on FIGER (Ling et al.,
2015) and Open Entity (Choi et al., 2018). The
training set of FIGER is labeled with distant super-
vision, and its test set is annotated by human. Open
Entity is a completely manually-annotated dataset.
We compare our model with baseline models we
mentioned in Baselines 4.2.
As shown in Table 4, Coke can achieve com-
parable F1 scores on Open Entity. On FIGER,
Coke signiﬁcantly outperform the BERT BASEand
RoBERTa BASEby3:7% and 3:5% Micro scores re-
spectively. Besides, the performance of Coke is
better than other baseline models as well. It di-
rectly demonstrates that Coke has better ability to
reduce the noisy label challenge in FIGER than the
baseline models that we mentioned above.
Moreover, we found the domain of FIGER
is similar to Wikidata, this is consistent with
the observation in the empirical analysis section,
which further highlights the importance of select-
ing knowledge context cross domains.

--- PAGE 8 ---
Task Entity Typing Relation Classiﬁcation
Dataset Open Entity FIGER FewRel TACRED
Metric P R F1 Acc. Macro Micro P R F1 P R F1
Pre-Trained Language Models
BERT BASE 76.2 71.0 73.6 52.0 75.2 71.6 85.0 85.1 84.9 67.2 64.8 66.0
RoBERTa BASE 75.3 73.2 74.2 56.3 76.9 74.2 86.3 86.3 86.3 73.0 68.7 70.8
RoBERTa LARGE 78.5 72.7 75.5 57.1 82.4 76.5 88.4 88.4 88.4 74.3 66.8 70.4
Knowledge Enhance Pre-Trained Language Models
ERNIE 78.4 72.9 75.6 57.2 76.5 73.4 88.5 88.4 88.3 69.9 66.0 67.9
K-BERT 76.7 71.5 74.0 56.5 77.1 73.8 83.1 85.9 84.3 68.1 66.1 67.1
KnowBert-Wiki 78.6 71.6 75.0 57.0 79.8 75.0 89.2 89.2 89.2 71.1 66.8 68.9
Contextual Knowledge Enhanced Pre-Trained Language Models
CokeBERT
BASE 78.0 73.3 75.6 57.9 79.7 75.3 89.4 89.4 89.4 71.0 66.9 68.9
CokeROBERT A
BASE 76.8 74.2 75.6 62.2 82.3 77.7 90.1 90.1 90.1 71.3 71.0 71.1
CokeROBERT A
LARGE 75.3 76.2 75.7 58.3 82.3 77.8 91.1 91.1 91.1 69.9 71.8 70.8
Table 4: The results of various models for Relation Classiﬁcation and Entity Typing (%).
Relation Classiﬁcation Relation classiﬁcation
aims to determine the correct relation between two
entities in a given sentence. We ﬁne-tune Coke on
two widely-used benchmark dataset FewRel (Han
et al., 2018) and TACRED (Zhang et al., 2017). We
also compare our model with baseline models we
mentioned in Baselines 4.2.
On FewRel, Coke signiﬁcantly outperforms the
BERT BASEand RoBERTa BASEby4:5% and 3:8%
F1 scores respectively as shown in Table 4. It di-
rectly demonstrates that Coke can capture the re-
lation between two entities better than ERNIE by
considering the information of higher-order neigh-
bours, especially in small dataset FewRel.
Besides, Coke models have comparable re-
sults with other baseline models on TACRED but
achieve substantially improvements on FewRel. As
we mentioned before, the domain of FewRel data
is more similar to Wikidata and therefore it gains
more beneﬁt from pre-training.
4.5 Ablation Study
In order to indicate the effect of S-GNN on the pro-
cess of dynamically selecting knowledge context,
we conduct essential ablation studies for different
modules in S-GNN.
K-Hop Sub-Graphs In this section, we explore
the effects of dynamic knowledge context encoder.
There are two main components in the dynamic
knowledge context encoder: raw knowledge con-
text construction and S-GNN. Coke applies raw
knowledge context construction to sample K-hop
(a) FewRel
 (b) FIGER
Figure 3: The results of Coke incorporating K-hop sub-
graphs (%).
sub-graphs, and then incorporates S-GNN to em-
bed informative knowledge in the raw context.
From Figure 3, we ﬁnd that Coke incorporating
the2-hop sub-graph outperforms by 0:4% to 0:6%
than incorporating the 1-hop sub-graph. It proves
that considering a wider range of knowledge can
lead to better entity embeddings.
Attention Mechanism In S-GNN, there is an es-
sential mechanism: attention. It takes responsibil-
ity for weighing how much knowledge matches
the text and help compute ﬁnal dynamic contex-
tual embeddings. To further demonstrate the effect
of the attention mechanism, we simplify it with
a mean-pooling operation to aggregate features.
From Figure 4, we can ﬁnd that the attention mech-
anism outperforms than the mean-pooling mech-
anism and ﬁxed embeddings (ERNIE), indicating
the effectiveness of our attention mechanism.
5 Conclusion and Future Work
We have proposed an effective and general frame-
work to enable PLMs to dynamically select ap-

--- PAGE 9 ---
(a) FewRel
 (b) FIGER
Figure 4: The effect of the attention mechanism and its
simpliﬁed versions (%).
propriate knowledge context with textual context,
and then insert the embedded knowledge into
PLMs. The experiments demonstrate that Coke can
achieve comparable results with the state-of-the-art
knowledge-enhanced PLMs in the entity typing and
relation classiﬁcation. Coke dynamically selects
knowledge context with textual context is more
interpretable than injecting all knowledge context
from KGs. In the empirical analysis, Coke demon-
strates the effective selection of knowledge context
as well. This direction may lead to more general
and effective language understanding. In the future,
we will continue to explore how to inject other
type of knowledge (e.g. linguistic knowledge) in
conjunction with factual knowledge to further en-
hance PLMs. And it is also an interesting direction
to explore how to continually inject emerging fac-
tual knowledge into PLMs without re-training the
whole model.
References
Livio Baldini Soares, Nicholas FitzGerald, Jeffrey
Ling, and Tom Kwiatkowski. 2019. Matching the
blanks: Distributional similarity for relation learn-
ing. In Proceedings of ACL , pages 2895–2905.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NeurIPS , pages
2787–2795.
Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle-
moyer. 2018. Ultra-ﬁne entity typing. In Proceed-
ings of ACL , pages 87–96.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proceedings of NAACL , pages 4171–4186.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-ﬂy annotation of short text fragments (by
wikipedia entities). In Proceedings of CIKM , page
1625–1628.Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan
Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel:
A large-scale supervised few-shot relation classiﬁca-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of EMNLP , pages 4803–4809.
Bin He, Di Zhou, Jinghui Xiao, Xin jiang, Qun Liu,
Nicholas Jing Yuan, and Tong Xu. 2019. Integrat-
ing graph contextualized knowledge into pre-trained
language models. arXiv .
Anne Lauscher, Ivan Vuli ´c, Edoardo Maria Ponti, Anna
Korhonen, and Goran Glava ˇs. 2019. Specializing
unsupervised pretraining models for word-level se-
mantic similarity. arXiv .
Xiao Ling, Sameer Singh, and Daniel S. Weld. 2015.
Design challenges for entity linking. In Proceedings
of ACL , page 315–328.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,
Haotang Deng, and Ping Wang. 2020. K-bert:
Enabling language representation with knowledge
graph. In Proceedings of AAAI , pages 2901–2908.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv .
Joshi Mandar, Chen Danqi, Liu Yinhan, Daniel S.
Weld, Zettlemoyer Luke, and Levy Omer. 2019.
Spanbert: Improving pre-training by representing
and predicting spans. In Proceedings of TACL ,
pages 64–77.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT , pages
2227–2237.
Matthew E. Peters, Mark Neumann, Robert L Lo-
gan, Roy Schwartz, Vidur Joshi, Sameer Singh, and
Noah A. Smith. 2019. Knowledge enhanced con-
textual word representations. In Proceedings of
EMNLP , pages 43–54.
Nina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.
2019. BERT is not a knowledge base (yet): Fac-
tual knowledge vs. name-based reasoning in unsu-
pervised QA. arXiv .
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. arXiv .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NeurIPS , pages 5998–
6008.

--- PAGE 10 ---
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-
anjing Huang, Jianshu Ji, Cuihong Cao, Daxin Jiang,
and Ming Zhou. 2020. K-adapter: Infusing knowl-
edge into pre-trained models with adapters. arXiv .
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan
Liu, Juan-Zi Li, and Jian Tang. 2019. KEPLER:
A uniﬁed model for knowledge embedding and pre-
trained language representation. arXiv .
Wenhan Xiong, Jingfei Du, William Yang Wang, and
Veselin Stoyanov. 2019. Pretrained encyclopedia:
Weakly supervised knowledge-pretrained language
model. In Proceedings of ICLR .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for
language understanding. In Proceedings of NeurIPS ,
pages 5753–5763.
Levine Yoav, Lenz Barak, Dagan Or, Padnos Dan,
Sharir Or, Shalev-Shwartz Shai, Shashua Amnon,
and Shoham Yoav. 2019. Sensebert: Driving some
sense into bert. In Proceedings of ACL , pages 4656–
4667.
Sun Yu, Wang Shuohuan, Li Yu-Kun, Feng Shikun,
Chen Xuyi, Zhang Han, Tian Xin, Zhu Danxiang,
Tian Hao, and Wu Hua. 2019. Ernie: Enhanced rep-
resentation through knowledge integration. In Pro-
ceedings of ACL , pages 1441–1451.
Sun Yu, Wang Shuohuan, Li Yukun, Feng Shikun, Tian
Hao, Wu Hua, and Wang Haifeng. 2020. Ernie2.0:
A continual pre-training framework for language un-
derstanding. In Proceedings of AAAI .
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D. Manning. 2017. Position-
aware attention and supervised data improve slot ﬁll-
ing. In Proceedings of EMNLP , pages 35–45.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,
Maosong Sun, and Qun Liu. 2019. Ernie: Enhanced
language representation with informative entities. In
Proceedings of ACL , page 1441–1451.
