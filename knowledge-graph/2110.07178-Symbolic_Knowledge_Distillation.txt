# 2110.07178.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2110.07178.pdf
# File size: 1381911 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Symbolic Knowledge Distillation:
from General Language Models to Commonsense Models
Peter Westyz*Chandra BhagavatulazJack HesselzJena D. Hwangz
Liwei JiangyzRonan Le BraszXiming LuyzSean WelleckyzYejin Choiyz*
yPaul G. Allen School of Computer Science & Engineering, University of Washington
zAllen Institute for Artiﬁcial Intelligence
Abstract
The common practice for training com-
monsense models has gone from–human–to–
corpus–to–machine : humans author common-
sense knowledge graphs in order to train com-
monsense models. In this work, we investi-
gate an alternative, from–machine–to–corpus–
to–machine : general language models author
these commonsense knowledge graphs to train
commonsense models.
Our study leads to a new framework, Sym-
bolic Knowledge Distillation . As with prior
art in Knowledge Distillation (Hinton et al.,
2015), our approach uses larger models to
teach smaller models. A key difference is that
we distill knowledge symbolically–as text–in
addition to the resulting neural model. We dis-
till only one aspect–the commonsense of a gen-
eral language model teacher, allowing the stu-
dent to be a different type of model, a common-
sense model. Altogether, we show that careful
prompt engineering and a separately trained
critic model allow us to selectively distill high-
quality causal commonsense from GPT-3, a
general language model.
Empirical results demonstrate that, for the ﬁrst
time, a human-authored commonsense knowl-
edge graph is surpassed by our automatically
distilled variant in all three criteria: quantity,
quality, and diversity. In addition, it results in a
neural commonsense model that surpasses the
teacher model’s commonsense capabilities de-
spite its 100x smaller size. We apply this to
the A TOMIC resource, and will share our new
symbolic knowledge graph and commonsense
models1.
1 Introduction
Prior works have suggested that pre-trained lan-
guage models possess limited understanding of
commonsense knowledge (Merrill et al., 2021; Tal-
mor et al., 2021; Davis and Marcus, 2017) despite
1We will share this following the anonymity period. We
have permission from OpenAI to release GPT-3 generations
GPT-3175B Parameters General Model
!
ATOMIC10X 6.5M Examples Commonsense KGCOMETdistil 1.5B Parameters Commonsense ModelSymbolic KnowledgeDistillation
!CRITIC Fine-tuned RoBERTa ﬁlters for qualityFigure 1: Symbolic knowledge distillation extracts
the commonsense from the large, general language
model GPT-3, into 2 forms: a large commonsense
knowledge graph ATOMIC10x, and a compact common-
sense model COMETDIS
TIL. The quality of this knowl-
edge can be controlled and improved by adding a critic
model, making GPT-3 a stronger teacher.
otherwise stellar performance on leaderboards. As
a result, symbolic commonsense knowledge graphs
(Speer et al., 2017; Sap et al., 2019; Hwang et al.,
2021) and corresponding neural representations
(Bosselut et al., 2019; Hwang et al., 2021; Zhang
et al., 2020b) have supplemented past models with
commonsense capabilities. This has enabled di-
verse downstream applications, including interac-
tive learning through a conversational interface
(Arabshahi et al., 2021), persona- and affect-aware
conversation models (Kearns et al., 2020), ﬁgura-
tive language understanding (Chakrabarty et al.,
2020, 2021), story telling (Ammanabrolu et al.,
2021a) and fantasy games (Ammanabrolu et al.,
2021b).
The common practice for commonsense knowl-
edge graph construction sees humans spell out
as many pieces of knowledge as possible. This
pipeline goes from–human–to–corpus–to–machine ,
with commonsense models trained from human-arXiv:2110.07178v2  [cs.CL]  28 Nov 2022

--- PAGE 2 ---
authored knowledge graphs. Yet, high-quality,
human-authored knowledge is expensive to scale,
limiting coverage; this motivates an alternative:
from–machine–to–corpus–to–machine . Prior ef-
forts toward automatic commonsense knowledge
graphs have resulted in considerably lower qual-
ity than human-written data (Hwang et al., 2021;
Zhang et al., 2020b), which in turn leads to less
reliable neural models (Hwang et al., 2021). Broad
literature consistently shows machine-authored
knowledge graphs underperform human-authored
graphs (Etzioni et al., 2011; Mitchell et al., 2015;
Bollacker et al., 2008).
In this work, we propose Symbolic knowledge
distillation , a new conceptual framework towards
high-quality automatic knowledge graphs for com-
monsense, leveraging state-of-the-art models and
novel methodology. Most prior art for automatic
knowledge graph construction extracts knowledge
from raw text (Bhakthavatsalam et al., 2020; Zhang
et al., 2020a; Zhou et al., 2020; Zhang et al., 2020b;
Li et al., 2020). In contrast, our approach is mo-
tivated by knowledge distillation (Hinton et al.,
2015) wherein a larger teacher model transfers
knowledge to a compact student model (§2.1). Our
method differs from prior knowledge distillation in
key ways: we distill a symbolic knowledge graph
(i.e., generated text) in addition to a neural model,
and we distill only a selective aspect of the teacher
model. This selectively allows the student model
to be of a different type (commonsense model),
compared to the teacher (general language model),
enriching the scope of distillation. An added ben-
eﬁt is that knowledge distilled as text is human
readable: it can be understood and evaluated.
A general language model–GPT-3 in our case–is
an imperfect commonsense teacher on its own, and
the ability to evaluate distilled knowledge is useful
in improving it. We empirically demonstrate that,
by training a separate critic model to judge sym-
bolic generation quality, a more precise teacher can
be deﬁned. Knowledge from this critical teacher
is higher quality–even exceeding human-authored
knowledge. Yet even before training a critic, our
study makes the unexpected ﬁnding that the student
model surpasses the commonsense of GPT-3, our
knowledge source.
To test symbolic knowledge distillation against
thehuman–to–corpus–to–machine paradigm, we
compare with ATOMIC20
20(Hwang et al., 2021),
which is a human-authored commonsense knowl-edge graph. We ﬁnd that ATOMIC10x, our machine-
generated corpus, exceeds the human generated
corpus in scale, accuracy, anddiversity with re-
spect to 7 commonsense inference types that we
focus on in this study. The resulting commonsense
model, COMETDIS
TIL, not only surpasses the human-
trained equivalent COMET20
20, but is also smaller,
more efﬁcient, and produces commonsense at a
higher accuracy than its own teacher–GPT-3.
Symbolic knowledge distillation offers a promis-
ing new role for general language models, as com-
monsense knowledge sources, and humans, as
small-scale evaluators to train critic models rather
than authors of commonsense knowledge. Our
work demonstrates that humans and LMs can be
effective collaborators for curating commonsense
knowledge graphs and training efﬁcient and perfor-
mant commonsense models.
2 Overview and Key Findings
Throughout our work, we describe the machine–
to–corpus–to–machine methodology of symbolic
knowledge distillation. We ﬁrst go machine–to–
corpus (§3), by decoding from GPT-3, then im-
prove our knowledge with a specialized critic
model (§4), and ﬁnally distill this knowledge
into an efﬁcient commonsense model (§5), going
corpus–to–machine . Throughout this process, we
evaluate against a human knowledge source, com-
paring our automatic knowledge graph ATOMIC10x
and commonsense model COMETDIS
TILto the human-
authored ATOMIC20
20and resulting model COMET20
20
(Hwang et al., 2021).
2.1 Symbolic Knowledge Distillation
Our proposed methodology parallels knowledge
distillation (Hinton et al., 2015), a method for com-
pressing a large or complicated teacher distribution
Ptinto a smaller/simpler student distribution Ps.
Key to knowledge distillation2is the notion of min-
imizing the cross-entropy between PtandPs:
H(Pt; Ps) = X
y2YPt(y) logPs(y) (1)
Knowledge is transferred to the student by encour-
aging it to match teacher predictions. Hinton et al.
(2015) apply this to conditional classiﬁcation: for
2In its simplest case, with temperature set to 1.0

--- PAGE 3 ---
X starts runningxEffectso, Xgets in shapeX sings a songHinderedBybut not ifX can't remember the lyricsX and Y engage in an argumentxWantso, X wantsto avoid YX is not well likedxReactso, X feelslonelyX learns to type fastxNeedX neededto have taken typing lessonsX takes care of a monkeyxAttr X is seen askindX steals his grandfather's swordxEffectso, Xis punished by his grandfatherX butts inHinderedBybut not ifX is too shy to speak upX takes up new employmentxIntentbecause X wantsto be self sufficientX waits for the storm to breakxEffectso, Xis safe from the stormFigure 2: Example automatically generated ATOMIC triples from our A TOMIC10xcommonsense knowledge
graph. Each example includes a generated event ,relation (with natural language interpretation), and generated
inference .
each training input, PtandPsare model predic-
tions over label set Y. Typically Yis a tractable set,
over which this sum can reasonably be calculated.
For distilling the knowledge of generative mod-
els, we can think of an unconditional language
model (LM e.g. GPT-3) as Pt. This makes Ythe
set of all strings, over which LMs deﬁne probability.
Unfortunately Yis an exponential set, intractable
to sum over in Eq 1. Kim and Rush (2016) address
this problem by simply taking the mode of Ptover
Y, truncating most of the teacher distribution to the
most likely sequence and discarding information.
Instead, we consider a sampling-based interpre-
tation of the same objective:
H(Pt; Ps) = E
yPt(y)[ logPs(y)] (2)
which exactly equals the cross-entropy of Eq 1, at
the limit under pure sampling from Pt.3
Yet distilling all knowledge from the teacher may
not be desirable–our work is speciﬁcally focused
on distlling commonsense knowledge from GPT-
3. The ideal teacher Ptis a commonsense expert,
but GPT-3 can approximate such a teacher, off-the-
shelf, via prompting. This ability to select informa-
tion is one explicit beneﬁt of the sampling-based
interpretation of Eq 2: while Eq 1 uses continu-
ous logits over existing data, sampling gives dis-
crete control over transferred information, by se-
lecting which samples are elicited and used. For
the general language model GPT-3, We encour-
age domain/quality with prompting, and sample
truncation (Holtzman et al., 2020). We call this
theloose teacher PL
t–knowledge is generated and
3A useful consequence of this framing is that access to the
full model distribution is not required. Our experiments (§3)
use GPT-3, for which the distribution is not available , thus
our method is applicable while knowledge distillation is not.transferred from GPT-3, but without critical assess-
ment of correctness (§3).
In fact, sampling knowledge in Eq 2 offers even
more control, as generations can be individually
interpreted and judged. Given an indicator function
A(x)for which knowledge xiscorrect , we can
deﬁne a stronger teacher model. Using a Product of
Experts (Hinton, 2002) between the loose teacher
PL
tand and the critic A(x), we deﬁne a critical
teacher :
Pt(x)/PL
t(xjp)A(x) (3)
In practice, A(x)is a textual classiﬁer learned on
human judgements, 1 for knowledge predicted to
be correct and 0 otherwise. Thus, the critic gives
control over the correctness and conﬁdence of the
knowledge that is transferred (§4).
2.2 Key Findings
Applying symbolic knowledge distillation in prac-
tice results in promising and surprising ﬁndings:
1. Learning symbolic knowledge from language
models can be framed as a symbolic extension
to knowledge distillation. In §2.1, we describe
learning commonsense as a symbolic extension to
knowledge distillation, with GPT-3 a knowledge
source. We elaborate on this process with positive
results in §3,4, and 5.
2. Symbolic knowledge distillation constructs
a high quality knowledge graph at scale. Our
method naturally yields a machine-generated com-
monsense knowledge graph, which can achieve
impressive quality (§4), beyond that of human-
authored data. An effective critic which ﬁlters
incorrect generated knowledge is key.
3. A critical teacher results in a higher quality
student. In §4, we show that making the teacher

--- PAGE 4 ---
more critical results in higher quality knowledge,
even as it reduces the scale of knowledge trans-
ferred. This demonstrates that quality matters, not
justquantity , as higher quality knowledge results in
a higher quality commonsense model in §5 despite
smaller scale data.
4. Critical teacher or not, a student can outper-
form the knowledge source. In §5, we show the
unexpected result that all student models exceed
the quality of GPT-3, the knowledge source.
5. Machines can win over humans for auto-
matic knowledge graph construction. In §4
and §5, we show that machine generated knowl-
edge and the resulting commonsense model can out-
perform their equivalents that use a human knowl-
edge source. Our symbolic knowledge exceeds hu-
mans at scale, quality, and diversity. The resulting
commonsense model achieves the most accurate
commonsense KG completions.
3 Machine-to-Corpus Verbalization
Symbolic knowledge distillation begins by going
machine–to–corpus , i.e. generating many com-
monsense facts, which results in a commonsense
knowledge graph. §2.1 frames this as sampling
to estimate the knowledge distillation objective–a
student commonsense model learns from the gen-
erations of a teacher (GPT-3).
We start with a loose teacher , transferring knowl-
edge by prompted generation with truncated sam-
pling alone–this is in contrast to the critical teacher
(§4) which explicitly judges and ﬁlters the gen-
erated samples. The loose teacher uses few-shot
prompting as in Brown et al. (2020). We use a
few-shot template:
<TASK-PROMPT>
<EX 1-INP><EX 1-OUT>
. . .
<EX N 1-INP><EX N 1-OUT>
<EX N-INP>
where <EX i-INP> /<EX i-OUT> are human-
authored, natural language ATOMIC entries,
and<TASK-PROMPT> is a description of the
problem. Given such a prompt, GPT-3 generates
themissing piece , output <EX N-OUT> for input
<EX N-INP> , following the pattern of earlier
examples (1 to N-1). We ﬁnd important aspects for
producing high-quality commonsense knowledge:
•Examples should be numbered. e.g.<EX 5-INP> might begin with "5)" to in-
dicate it is the 5th example.
•The format of <EX i-INP> and<EX i-OUT>
should linguistically imply the relationship be-
tween them. See below for examples.
•<TASK-PROMPT> can be used to give extra
speciﬁcation to complicated problems.
3.1 Data: A TOMIC
We demonstrate symbolic knowledge distillation
on the ATOMIC if-then resource (Sap et al., 2019).
This follows an event-relation-inference (triple) for-
mat. The corpus links events (e.g. X attacks Y ) to
relations, e.g. HinderedBy which describes what
might hinder an event. For a relation/event, the
goal is to generate a resulting inference, e.g. X
attacks Y HinderedBy X is restrained .
Of the 23 relations from the most recent version–
ATOMIC20
20–we limit our investigation to 7 relations
that correspond to causal commonsense knowl-
edge: xAttr (how X is perceived after event ),xRe-
act(how X reacts in response to event ),xEffect
(what X does after event ),xIntent (X’s intent in
event ),xWant (what X wants after event ),xNeed
(what X needed for event to take place) and Hin-
deredBy . We describe how verbalization is ap-
plied to ATOMIC data in 2 steps: generating under-
lying events (heads), then full examples (inference
given event).
3.2 Event Generation
Events are context-free premises in ATOMIC
involving PersonX (and sometimes a second
PersonY ) in various scenarios. These events form
heads in knowledge graph triples. We generate
events by ﬁlling in the elements of our template:
1. Event: X overcomes evil with good
2. Event: X does not learn from Y
. . .
10. Event: X looks at flowers
11.
The format is simple, as events are generated un-
conditionally . We use 100 high-quality events from
theATOMIC20
20corpus for our prompt, selected
to avoid grammatical or logical errors, and min-
imize semantic overlap. We randomly sample 10
of these seed events for each generation batch, re-
sulting in randomized prompts. We use nucleus
sampling ( p= 0:9) (Holtzman et al., 2020), and
presence/frequency penalties of 0.5 from the GPT-
3 interface. We generate 165K unique events using

--- PAGE 5 ---
the 175B-parameter Davinci model4from Brown
et al. (2020) (human-authored ATOMIC20
20contains
only 6.2K events).
3.3 Inference Generation
Generating ATOMIC inferences requires reasoning
about events and relations together. We design ver-
balization templates fo reach relation, with iterative
design and small-scale veriﬁcation by the authors5
e.g. we prompt the xNeed relation as follows:
What needs to be true for this
event to take place?
. . .
Event <i>: X goes jogging
Prerequisites: For this to
happen, X needed to wear running
shoes
. . .
Event <N>: X looks at flowers
Prerequisites: For this to
happen,
The language of this template implies the relation-
speciﬁc task, both "Prerequisites:" and beginning
with "for this to happen" suggest the xNeed re-
lation. As well, we include an xNeed-speciﬁc
<TASK-PROMPT> . We use 10 few-shot examples
for each prompt.6
For each event/relation (165K X 7) we gener-
ate 10 inferences with the Curie GPT-3 model7
and earlier hyperparameters. Removing duplicate
and degenerate (e.g. fewer than 3 characters) gen-
erations yields 6.46M ATOMIC -style data triples
(examples in Figure 2). We call this ATOMIC10x,
as it contains an order of magnitude more triples
than A TOMIC20
20for the 7 relations we study.
3.4 Evaluating a Generated Commonsense
Knowledge Graph
Machine generation enables a large scale of unique
generations at a much lower cost than human-
authored knowledge (Table 1), but what kind of
examples are produced by GPT-3, and how does
4the largest available version of GPT-3
5See Appendix D for full prompts.
6We also replace anonymous names (“X”) with sampled
generic names as this improved quality, See Appendix D. Once
generation is complete, we substitute in generic markers (“X”)
for the ﬁnal dataset.
7for the largest, Davinci, 12M generations is computation-
ally/monetarily intractable.it differ from knowledge produced by humans? In
this section, we conduct an in-depth analysis to
answer these questions.
Lexical Differences: Diversity and Uniqueness
Recent work ﬁnds that machine generations can be
repetitive and lack diversity (Welleck et al., 2020;
Holtzman et al., 2020); one way generated knowl-
edge may differ from human-authored is less cre-
ative word choice, diversity, or more repetition.
To test this, we begin with lexical diversity
(i.e. unique words used, Table 2). While there
is variation by relation, the diveristy of ATOMIC10x
actually exceeds ATOMIC20
20here, 5.2M unique
words to 1.5M. In addition, it contains signiﬁcantly
more strictly unique generated inferences (Table 2,
unique tails).
BLEU Soft Uniqueness. Exact match (above)
fails to capture the notion of similar text. Follow-
ing the intuition of self-BLEU (Zhu et al., 2018),
we deﬁne soft uniqueness to describe diversity of
generations in a corpus. An inference xis softly-
unique if:
BLEU 2(C; x)<0:5
where Cis the set of inferences for a given in-
put (in our case, event + relation), and 0.5 is an
empirical threshold. To ﬁnd soft-uniqueness of a
corpus, we iteratively remove examples until all
are softly unique, i.e. low mutual lexical over-
lap; higher diversity means more such examples
(thus a larger softly unique corpus is preferable).
Softly-unique corpus sizes are given in Table 4
(“Size (div)”). ATOMIC10xhas a smaller fraction
of softly-unique examples than ATOMIC20
20, yet it
contains many more such examples. ATOMIC10x
contains 4.38M such examples (full size 6.5M) vs.
ATOMIC20
20, which has 560K (full size 600K).
Model-based Diversity Measurement. Lexical
notions of diversity reward differences in surface
form, which may not always reﬂect diversity of
information , only format. Thus, we next study
information-theoretic measures for diversity. In-
tuitively, diverse information should be less pre-
dictable, or higher entropy. With GPT-2 XL mod-
els ﬁnetuned on ATOMIC20
20andATOMIC10x(§5)
we estimate entropy –roughly, how difﬁcult it is
for a model to capture the corpus information (Ta-
ble 3). This is 4 times higher for ATOMIC10x,
suggesting more content from a modeling per-
spective. We also estimate cross-entropy –how

--- PAGE 6 ---
Relation ATOMIC20
20ATOMIC10x
HinderedBy 77,616 1,028,092
xNeed 100,995 760,232
xWant 109,098 730,223
xIntent 54,839 965,921
xReact 62,424 1,033,123
xAttr 113,096 884,318
xEffect 90,868 1,054,391
Total Count 608,936 6,456,300
Est Total Cost ~$40,000 ~$6,000
Est Cost Per Triple ~$0.06 ~$0.001
Table 1: Number of unique triples with the given
relation,j(;relation ;)j. The estimated cost for
ATOMIC10xcomes at a fraction of a conservative esti-
mation for A TOMIC20
20crowdsourcing costs.
Unique Unique
Length Tokens (K) Tails (K)
A20
20A10xA20
20 A10xA20
20A10x
xWant 4.69 5.16 322 784 69 152
xAttr 1.42 2.73 15 21 11 8
xEffect 3.92 4.66 216 864 55 185
xIntent 4.59 5.92 136 800 30 135
xNeed 4.51 5.97 289 1378 64 231
xReact 4.03 1.77 48 5 12 2
HinderedBy 7.93 7.49 522 1775 290 874
Events 5.20 5.32 109 881 6.2 165
Table 2: Average length, total unique tokens and to-
tal unique examples (in K, i.e. 1000s) by relation type
and in events (bottom row) from A TOMIC20
20(A20
20) and
ATOMIC10x(A10x).
Entropy Cross Entropy KL Divergence
H(D1) = 1:27H(D1; D2) = 9:31DKL(D1jjD2) = 8 :04
H(D2) = 7:80H(D2; D1) = 41 :48DKL(D2jjD1) = 33 :68
Table 3: Entropy, cross-entropy, and divergence of
ATOMIC20
20(D1) and A TOMIC10x(D2).
well a model trained on one corpus describes the
other. From ATOMIC10xtoATOMIC20
20, this is 9.31,
only 2 points higher than its entropy suggesting
ATOMIC20
20is describable with information from
ATOMIC10x. In reverse, this is 41.48 suggesting
much of ATOMIC10xis not captured by ATOMIC20
20–
ATOMIC10xis surprising given only information
from A TOMIC20
20.
Human Evaluation of Quality. Perhaps most
importantly, we study the quality of knowledge
in each corpus. We conduct human evaluation with
Amazon Mechanical Turk. 3 annotators rate each
triple resulting in “accepted”, “rejected” or “noCorpus Accept Reject N/A Size Size (div)
ATOMIC20
20 86.8 11.3 1.9 0.6M 0.56M
ATOMIC10x78.5 18.7 2.8 6.5M 4.38M
88.4 9.5 2.1 5.1M 3.68M
(critic low) 91.5 6.8 1.7 4.4M 3.25M
95.3 3.8 1.0 3.0M 2.33M
(critic high) 96.4 2.7 0.8 2.5M 2.00M
+ GPT-J 72.0 27.6 0.4 - -
+ T5-11B LM 71.7 26.9 1.4 - -
Table 4: Attributes of A TOMIC10xand A TOMIC10x(row
2) including the critic model (§4, rows 3 - 6) with var-
ious ﬁltering cutoffs. Accept and Reject are by ma-
jority human vote unless any mark N/A. Size is in
unique examples9. The highest precision corpus is
ATOMIC10xwith (critic high), but multiple versions sur-
pass A TOMIC20
20. We also include alternate models
(GPT-J and T5-11B) as the loose teacher.
judgement”. We evaluate 3000 examples8from
ATOMIC10x, and 1000 from ATOMIC20
20(Table 4).
We ﬁnd Fleiss’ kappa (Fleiss, 1971) of 40.8 indicat-
ing moderate agreement (Landis and Koch, 1977),
and 90.5% accuracy agreement. We require work-
ers meet an Amazon Mechanical Turk qualiﬁcation
for annotation quality based on past commonsense
evaluations. We compensate workers $0:17per
task, which we estimate require 30 seconds. Fur-
ther details and task template are in appendix §A.
For the loose teacher , consider the top row of
ATOMIC10xin Table 4 (other rows add the critic
§4). ATOMIC10xexceeds ATOMIC20
20in scale, but
is somewhat less acceptable by human raters–by
roughly 8 percentage points. Yet, the larger scale of
ATOMIC10ximplies a signiﬁcantly higher number
of accurate examples. Increasing the proportion of
these is the main objective of the critic (§4).
How do Knowledge Sources Compare? To un-
derstand the robustness of our approach, we assess
other language models as the knowledge source
(i.e. loose teacher): GPT-J (Wang and Komat-
suzaki, 2021) and T5-11B adapted for language
modelling (Lester et al., 2021). We substitute both
for GPT-3 as in §3.2,3.3, generating a small-scale
corpus to evaluate. We conduct human evaluation
on 1000 examples as above (Table 4). Both mod-
els attain roughly 72% accuracy, 6 points below
GPT-3 (78.5). This suggests strong potential, but
higher quality from GPT-3. We explore this further
in Appendix B.
8this ensures at least 1000 after ﬁltering by the critic §4)

--- PAGE 7 ---
4 Making the Teacher More Critical
Symbolic knowledge distillation requires a strong
teacher model to maximize the quality of the gener-
ated knowledge graph and resulting student model
(§5). While the loose teacher (GPT-3 alone) re-
sults in a viable commonsense knowledge graph,
evaluation shows this isn’t a perfect commonsense
teacher. Thus, we multiply in a critic model , to ﬁl-
ter lower-quality knowledge, correcting the teacher
(§2.1). With modest supervision (a small-scale hu-
man evaluation) we train a classiﬁer to predict and
discriminate unacceptable examples. We multiply
this with the loose teacher §3, creating a critical
teacher product of experts. In practice this means
ﬁltering ATOMIC10xto create new corpora that are
higher quality, yet still larger scale than human-
authored A TOMIC20
20.
Training a knowledge critic We gather a train-
ing set of correct vs. incorrect human judgments
on a randomly-sampled set of 10K entries of
ATOMIC10x, as in §3.4 but with one annotation per
example. We take a (random) train/dev/test split of
8k/1k/1k. While this step requires human annota-
tion, humans take on the role of high-level supervi-
sors here–critiquing a small number of generations
rather than authoring the entire knowledge graph
as in previous work. Indeed, the cost/complexity
of this step is similar to a typical human evaluation,
making it far cheaper/easier than eliciting human-
authored knowledge in past work.
We train binary classiﬁers (critics) for human ac-
ceptability using RoBERTa-Large (Liu et al., 2019).
We ﬁnd pretraining on MNLI results in the best
model in terms of precision and recall, and we sug-
gest this technique for future studies. We give more
detail in Appendix C, including baselines. Our best
model vastly improves the accuracy of ATOMIC10x
(Table 4), demonstrating that a small amount of
human supervision can consistently help to correct
GPT-3’s mistakes.
Size-accuracy trade-off Using our critic to ﬁl-
ter knowledge results in a natural trade-off be-
tween size and accuracy. We test several cut-
offs for ATOMIC10x, i.e. conﬁdence at which
the critic rejects examples. We report human-
measured accuracy (Accept/Reject column Ta-
ble 4) following §3.4. We compare the loose
9Size of ATOMIC20
20is given as the number of comparable
datapoints, i.e. those with the same relations as A TOMIC10x.Random Inf Event EMAP Full
AP 79.3 81.9 86.2 87.1 94.0
Table 5: Average Precision for ablated critic models.
The critic not only ﬁlters awkward phrasings which can
be identiﬁed by either the event ( Event ) or inference
(Inf) in isolation (EMAP only identiﬁes these), but also
logical misalignments , which require modeling interac-
tions between event/inference, i.e. the full critic ( Full).
teacher (unﬁltered) to critical teachers. Discard-
ing 20% of instances that the critic judges as least
acceptable (reducing corpus size from 6.5M to
5.1M), ATOMIC10x’s accuracy rises 78.5 !88.4;
human-authored ATOMIC20
20contains 600K entries
at 86.8% accuracy. Reducing to total size to 2.5M
examples (38% of full size), we attain 96.4% accu-
racy, nearly 10 points above ATOMIC20
20while still
4X larger.
What gets ﬁltered out? We qualitatively iden-
tify two types of ﬁltered triples: 1) logical misalign-
ments , events/inferences joined in an inconsistent
manner. Recognizing these requires understand-
ing events-inference interactions, e.g., X cannot
ﬁnd his shirt as a result X is wearing a shirt ; 2)
awkward phrasings , in which events/inferences are
individually incoherent e.g. PersonX has a ﬁre in
the bath –resulting triples are invalid as the event is
implausible.
To understand what is ﬁltered, we ablate the
critic (Table 5): our full model is compared to a
random predictor, event-only model, and inference-
only model. We also compare to an EMAP (Hessel
and Lee, 2020) version, i.e. an ensemble of event
and inference-only, without interactions between
event/inference (needed for logical misalignments ).
We ﬁnd GPT-3 produces both independent
awkwardly-phrased events/inferences (ﬁltered by
X-only models) and logical misalignments. The
classiﬁer, trained on validated knowledge triples,
helps in both cases. The EMAP of our full model
(identiﬁes only awkward phrasings) achieves 87%
AP, and our full model (which additionally identi-
ﬁes logical misalignments) improves to 94% AP.
Does ﬁltering hurt diversity? One concern is
that the critic may keep only similar “safe” ex-
amples, lacking novelty. We repeat our diversity
analysis (§3.4) for critical corpora (Table 4, “Size
(div)”, higher=better). As we ﬁlter, we surprisingly
observe proportionally more diverse examples: full

--- PAGE 8 ---
CKG Completion Train Corpus
Model Acc Accept Reject N/A
GPT2-XL zero-shot - 45.1 50.3 4.6
GPT-3 - 73.3 24.1 2.6
COMET20
20 86.8 81.5 16.3 2.2
COMETDIS
TIL 78.5 78.4 19.2 2.4
+critic low 91.5 82.9 14.9 2.2
+critic high 96.4 87.5 10.2 2.3
Table 6: Model performance on knowledge base com-
pletion, measured by human judgement. Inferences are
generated on held-out events from A TOMIC20
20. Models
besides GPT-3 use GPT-2 XL architecture. C OMETDIS
TIL
with a strong critic ( +critic high) achieves the highest ac-
ceptance rate overall–87.5.
ATOMIC10xhas a diverse subset 68% of its size;
rising to 80% with the most extreme ﬁltering. One
possibility is that GPT-3 gravitates towards com-
mon sentence structures for inconsistent knowl-
edge. These would be recognizable to the critic,
and removing them would increase both quality
and diversity. This surprising result warrants fur-
ther study.
5 Corpus-to-Machine: Distillation
The ﬁnal step of symbolic knowledge distillation
trains a compact model on the generated natural
language knowledge graph. Our base model is
GPT2-XL trained on all of ATOMIC10x: we denote
this model by COMETDIS
TIL. We additionally train the
model on critical versions of ATOMIC10x–critlow
denotes training on the corpus achieving 91.5% ac-
curacy, and crithighon the 96.4% accuracy corpus.
Models are trained for 1 epoch, with default param-
eters using the Huggingface Transformers library
(Wolf et al., 2019).
5.1 Evaluating a Symbolically Distilled
Model
Evaluation follows past work (Hwang et al., 2021;
Bosselut et al., 2019; Sap et al., 2019) testing the
ability of models to do knowledge base completion,
i.e. generating inferences for test events, specif-
ically from the ATOMIC20
20test set. We use hu-
man evaluation10following Section 3.4, on 1000
inputs (event + relation), with results in Table 6. We
compare to the GPT2-XL-based COMET20
20model
trained on human-generated ATOMIC20
20, and GPT-
10We ﬁnd Fleiss’ kappa (Fleiss, 1971) of 47.1 for accep-
tance, indicating moderate agreement. (Landis and Koch,
1977), and accuracy agreement of 88.7%.3 using the same generation method as §3–in ef-
fect, comparing the student COMETDIS
TILto the loose
teacher GPT-3. We omit the critical teacher (GPT-
3 + critic), which is not assured to produce an in-
ference for each input, as the critic may reject all
tails for some inputs. We also compare to zero-shot
GPT2-XL (Radford et al., 2019) using the same
methodology (Table 6).
How does C OMETDIS
TILcompare to GPT-3? In
knowledge distillation, the student model often de-
teriorates in performance (Hinton et al., 2015; Kim
and Rush, 2016) compared to its teacher. Compar-
ing our base teacher–GPT-3–to the simplest version
ofCOMETDIS
TIL(top-row COMETDIS
TILof Table 6) sur-
prisingly shows the student surpasses GPT-3, the
model that generates its training data11. We posit
that the superior performance of COMETDIS
TILmay
have to do with mistakes of GPT-3 being ﬁltered by
verbalization and training of GPT-2, and possibly
the focus of COMETDIS
TILon one commonsense do-
main while GPT-3 covers a more general domain.
We leave further study of this effect for future work.
How does C OMETDIS
TILcompare to human knowl-
edge? While COMETDIS
TILwithout the critic is
slightly outperformed by COMET20
20in terms of ac-
curacy, this reverses with the critic. For both cutoffs
tested, COMETDIS
TILsurpasses COMET20
20, with more
ﬁltering resulting in a wider gap.
Usefulness of C OMETDIS
TIL For on-demand infer-
ence, where a single high quality inference for
some input event/relation is required, COMETDIS
TIL
is the best available model : the most performant
version surpasses COMET20
20by 5 points and GPT-3
by over 10. The critical teacher (GPT-3 + critic)
yields a more accurate corpus , but may ﬁlter all
inferences for an input, giving no output.
Limits and Future Work The success of
symbolic knowledge distillation is a ﬁrst step–
demonstrating superior performance to human au-
thoring on the commonsense relations tested here.
No aspect of our approach is speciﬁc to these rela-
tions, yet further work is needed to explore the fea-
sibility of generation for other aspects of common-
sense and knowledge, beyond these relations, to
concepts like physical or temporal commonsense.
11The slight difference in acceptability for GPT-3 from
Table 4 is likely due to variance in raters between rounds of
evaluation, and a different distribution of events–Table 4 uses
generated events while Table 6 uses events from A TOMIC20
20.

--- PAGE 9 ---
6 Related Work
Commonsense Knowledge Graphs (CKG)
CKGs provide knowledge for commonsense
reasoning. Some are manually constructed, e.g.
ATOMIC (Sap et al., 2019; Hwang et al., 2021).
ConceptNet (Speer et al., 2017) contains taxonomy
and physical commonsense, authored by humans
or compiled from such sources. Some CKGs are
automatically constructed: TransOMCS (Zhang
et al., 2020a) extracts 18.48M tuples from syntactic
parses and CausalBank (Li et al., 2020) extracts
314M cause-effect pairs by pattern-matching. In
contrast, we generate commonsense.
Extracting Knowledge from LMs Past work
uses models for automatic knowledge graph com-
pletion (Bosselut et al., 2019; Hwang et al., 2021;
Li et al., 2020). Yet, models are trained on existing
resources; ATOMIC10xis generated without these.
Other works mine factual/commonsense knowl-
edge directly from off-the-shelf LMs (Petroni et al.,
2019; Davison et al., 2019; Xiong et al., 2020), but
not resulting in the quality at scale of A TOMIC10x.
Knowledge Distillation Other works use knowl-
edge distillation (Hinton et al., 2015) for generation.
(Sanh et al., 2019) follow a label smoothing formu-
lation, while Kim and Rush (2016) follow a sim-
ilar formulation to us (§2.1), but use the mode of
the teacher distribution rather than sampling. Our
work is unique in distilling speciﬁc information
(commonsense) from a general language model.
Data Generation While manual dataset creation
is expensive and complex (Schwartz et al., 2017;
Agrawal et al., 2018; Tsuchiya, 2018; Bras et al.,
2020), crowdsourcing is the most popular method
for goal-oriented, high quality/coverage datasets.
Past automatic data mainly use extractive ap-
proaches, e.g. syntactic parsing (Zhang et al.,
2020a) or pattern matching (Li et al., 2020) from
unstructured text (Lehmann et al., 2015; Buck et al.,
2014). These scale, but are noisy and limited in
format– ATOMIC knowledge will not appear simply
in natural text. Some works explore automatic data
synthesis/expansion by ﬁnetuning LMs on existing
labeled data (Anaby-Tavor et al., 2020; Papaniko-
laou and Pierleoni, 2020; Kumar et al., 2020; Yang
et al., 2020), but are limited by data quality.7 Conclusions
We introduce symbolic knowledge distillation, a
machine–to–corpus–to–machine pipeline for com-
monsense that does not require human-authored
knowledge–instead, using machine generation.
Knowledge is transferred from a large, general
model to a compact commonsense model, through
a commonsense corpus–yielding a commonsense
knowledge graph and model. Our resulting sym-
bolic knowledge graph has greater scale, diversity,
and quality than human authoring. symbolic knowl-
edge distillation offers an alternative to human-
authored knowledge in commonsense research.
Acknowledgments
This work was funded in part by the Natural Sci-
ences and Engineering Research Council of Canada
(NSERC) (funding reference number 401233309),
DARPA MCS program through NIWC Paciﬁc
(N66001-19-2-4031), and the Allen Institute for
AI.
Ethical Considerations
One aspect of our work with the potential for ethi-
cal pitfalls is large-scale generation from pretrained
language models, in constructing ATOMIC10x. Re-
cent work (Bender et al., 2021) has highlighted the
risks of models trained on massive text resources,
as GPT-3 (Brown et al., 2020) is, which we use
for generation. Indeed, open generations from pre-
trained language models can often contain harmful,
biased, or offensive aspects. We argue here that
this risk is largely mitigated in our work, mainly
due to the narrow and constrained nature of our
generations. The goal of our work is characterising
simple and generic anonymous situations, speciﬁ-
cally in terms of commonsense causes and effects.
We ensure generations are focused on these top-
ics through careful prompting, which we found to
be quite effective at keeping these generations on-
topic. As such, the potential for harmful generation
is very low; indeed, in a manual inspection of 100
generated examples, we found none that were sig-
niﬁcant harmful, besides one that contained adult
content.
A related concern is the potential for large mod-
els and training sets to make automated oppression
or exploitation possible, for instance in surveillance
or generating fake news. As above, we argue that
the generic, commonsense nature of our data and

--- PAGE 10 ---
models makes this concern less relevant here. Our
data does not contain any information directly re-
lated to these harmful domains (e.g. social media
or fake news generation). While our data may as-
sist machines in understanding basic situations, this
is unlikely to be useful for harmful models given
the simplicity of our data and still-ﬂawed com-
monsense capabilities of even the most advanced
models.
Finally, we note that we ensure fair and gener-
ous compensation for all human evaluators we hire
through Amazon Mechanical Turk. Based on our
estimates of time required per task, we ensure that
the effective pay rate is at least $15 per hour.
References
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and
Aniruddha Kembhavi. 2018. Don’t just assume;
look and answer: Overcoming priors for visual ques-
tion answering. 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages
4971–4980.
Prithviraj Ammanabrolu, Wesley Cheung, William
Broniec, and Mark O. Riedl. 2021a. Automated sto-
rytelling via causal, commonsense plot ordering. In
AAAI .
Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li,
Arthur D. Szlam, Tim Rocktaschel, and Jason We-
ston. 2021b. How to motivate your dragon: Teach-
ing goal-driven agents to speak and act in fantasy
worlds. In NAACL .
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,
Amir Kantor, George Kour, Segev Shlomov, Naama
Tepper, and Naama Zwerdling. 2020. Do not have
enough data? deep learning to the rescue! Pro-
ceedings of the AAAI Conference on Artiﬁcial Intel-
ligence , 34:7383–7390.
Forough Arabshahi, Jennifer Lee, Antoine Bosselut,
Yejin Choi, and Tom. Mitchell. 2021. Conver-
sational multi-hop reasoning with neural common-
sense knowledge and symbolic logic rules. In
EMNLP .
Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency ,
FAccT ’21, page 610–623, New York, NY , USA. As-
sociation for Computing Machinery.
Sumithra Bhakthavatsalam, Chloe Anastasiades, and
Peter E. Clark. 2020. Genericskb: A knowledge
base of generic statements. ArXiv , abs/2005.00660.Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh,
Tim Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structur-
ing human knowledge. In SIGMOD Conference .
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-
tanya Malaviya, A. Çelikyilmaz, and Yejin Choi.
2019. Comet: Commonsense transformers for au-
tomatic knowledge graph construction. In ACL.
Ronan Le Bras, Swabha Swayamdipta, Chandra Bha-
gavatula, Rowan Zellers, Matthew E. Peters, Ashish
Sabharwal, and Yejin Choi. 2020. Adversarial ﬁlters
of dataset biases. In ICML .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers.
Christian Buck, Kenneth Heaﬁeld, and Bas Van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC , volume 2, page 4. Cite-
seer.
Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Mure-
san, and Nanyun Peng. 2020. Rˆ3: Reverse, retrieve,
and rank for sarcasm generation with commonsense
knowledge. In ACL.
Tuhin Chakrabarty, Xurui Zhang, Smaranda Muresan,
and Nanyun Peng. 2021. MERMAID: Metaphor
generation with symbolism and discriminative de-
coding. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 4250–4261, Online. Association for
Computational Linguistics.
Ernest Davis and Gary Marcus. 2017. Causal genera-
tive models are just a start. Behavioral and Brain
Sciences , 40.
Joe Davison, Joshua Feldman, and Alexander M Rush.
2019. Commonsense knowledge mining from pre-
trained models. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP) , pages 1173–1178.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, et al. 2011. Open informa-
tion extraction: The second generation. In Twenty-
Second International Joint Conference on Artiﬁcial
Intelligence .

--- PAGE 11 ---
Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin ,
76(5):378.
Jack Hessel and Lillian Lee. 2020. Does my multi-
modal model learn cross-modal interactions? it’s
harder to tell than you might think! In EMNLP .
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
InNIPS Deep Learning and Representation Learn-
ing Workshop .
Geoffrey E Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation , 14(8):1771–1800.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,
Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and
Yejin Choi. 2021. Comet-atomic 2020: On symbolic
and neural commonsense knowledge graphs. AAAI .
William R. Kearns, Neha Kaura, Myra Divina,
Cuong Viet V o, Dong Si, Teresa M. Ward, and We-
ichao Yuwen. 2020. A wizard-of-oz interface and
persona-based methodology for collecting health
counseling dialog. Extended Abstracts of the 2020
CHI Conference on Human Factors in Computing
Systems .
Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In EMNLP .
Diederik P. Kingma and Jimmy Ba. 2015. Adam:
A method for stochastic optimization. CoRR ,
abs/1412.6980.
Varun Kumar, Ashutosh Choudhary, and Eunah Cho.
2020. Data augmentation using pre-trained trans-
former models. In Proceedings of the 2nd Workshop
on Life-long Learning for Spoken Language Systems ,
pages 18–26, Suzhou, China. Association for Com-
putational Linguistics.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics , pages 159–174.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, and
C. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web, 6:167–195.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. In EMNLP .Zhongyang Li, Xiao Ding, Ting Liu, J. Edward Hu,
and Benjamin Van Durme. 2020. Guided generation
of cause and effect. In Proceedings of the Twenty-
Ninth International Joint Conference on Artiﬁcial In-
telligence, IJCAI-20 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
William Merrill, Yoav Goldberg, Roy Schwartz, and
Noah A. Smith. 2021. Provable limitations of ac-
quiring meaning from ungrounded form: What will
future language models understand? Transactions
of the Association for Computational Linguistics ,
9:1047–1060.
Tom Michael Mitchell, William W. Cohen, Estevam R.
Hruschka, Partha P. Talukdar, Bo Yang, Justin Bet-
teridge, Andrew Carlson, Bhavana Dalvi, Matt Gard-
ner, Bryan Kisiel, Jayant Krishnamurthy, N. Lao,
Kathryn Mazaitis, Thahir Mohamed, Ndapandula
Nakashole, Emmanouil Antonios Platanios, Alan
Ritter, Mehdi Samadi, Burr Settles, Richard C.
Wang, D. Wijaya, Abhinav Gupta, Xinlei Chen, Ab-
ulhair Saparov, Malcolm Greaves, and Joel Welling.
2015. Never-ending learning. Communications of
the ACM , 61:103 – 115.
Yannis Papanikolaou and A. Pierleoni. 2020. Dare:
Data augmented relation extraction with gpt-2.
ArXiv , abs/2004.13845.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
2463–2473.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. ArXiv ,
abs/1910.01108.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A Smith, and Yejin Choi. 2019.
Atomic: An atlas of machine commonsense for if-
then reasoning. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , volume 33, pages
3027–3035.
Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017. The
effect of different writing tasks on linguistic style: A

--- PAGE 12 ---
case study of the ROC story cloze task. In Proceed-
ings of the 21st Conference on Computational Natu-
ral Language Learning (CoNLL 2017) , pages 15–25,
Vancouver, Canada. Association for Computational
Linguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence , volume 31.
Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-
gavatula, Yoav Goldberg, Yejin Choi, and Jonathan
Berant. 2021. Commonsenseqa 2.0: Exposing the
limits of ai through gamiﬁcation.
Masatoshi Tsuchiya. 2018. Performance impact
caused by hidden bias of training data for recog-
nizing textual entailment. In Proceedings of the
Eleventh International Conference on Language Re-
sources and Evaluation (LREC 2018) , Miyazaki,
Japan. European Language Resources Association
(ELRA).
Ben Wang and Aran Komatsuzaki. 2021. GPT-
J-6B: A 6 Billion Parameter Autoregressive
Language Model. https://github.com/
kingoflolz/mesh-transformer-jax .
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2020. Neu-
ral text generation with unlikelihood training. In
International Conference on Learning Representa-
tions .
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122. Association for
Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-
icz, and Jamie Brew. 2019. Huggingface’s trans-
formers: State-of-the-art natural language process-
ing. ArXiv , abs/1910.03771.
Wenhan Xiong, Jingfei Du, William Yang Wang, and
Veselin Stoyanov. 2020. Pretrained encyclopedia:
Weakly supervised knowledge-pretrained language
model. In International Conference on Learning
Representations .
Yiben Yang, Chaitanya Malaviya, Jared Fernandez,
Swabha Swayamdipta, Ronan Le Bras, Ji-Ping
Wang, Chandra Bhagavatula, Yejin Choi, and Doug
Downey. 2020. Generative data augmentation for
commonsense reasoning. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020 ,
pages 1008–1025, Online. Association for Computa-
tional Linguistics.Hongming Zhang, Daniel Khashabi, Y . Song, and
D. Roth. 2020a. TransOMCS: From linguistic
graphs to commonsense knowledge. In IJCAI .
Hongming Zhang, Xin Liu, Haojie Pan, Y . Song, and
C. Leung. 2020b. Aser: A large-scale eventuality
knowledge graph. Proceedings of The Web Confer-
ence 2020 .
Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan
Roth. 2020. Temporal common sense acquisition
with minimal supervision. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 7579–7589, Online. As-
sociation for Computational Linguistics.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-
gen: A benchmarking platform for text generation
models. In The 41st International ACM SIGIR Con-
ference on Research & Development in Information
Retrieval , pages 1097–1100.

--- PAGE 13 ---
A Human Evaluation Details
We conduct human evaluations on Amazon Me-
chanical Turk using the template of Figures 4,5.
Workers are presented with ATOMIC -style triples,
replacing relations with natural language templates
(e.g. HinderedBy becomes “can be hindered by”).
3 annotators rate each triple, with options for ac-
ceptability: “always/often”, “sometimes/likely”,
“farfetched/never”, “invalid”, or “too unfamiliar to
judge”. The ﬁrst two are considered “accepted”,
the second two “rejected” and the ﬁnal is “no judge-
ment”. For reporting acceptance rates, and training
a critic model, we only distinguish between “ac-
cepted” and not “accepted”.
Workers are compensated $0.17 per task (i.e.
completing all questions in the evaluation template
Figures 4,5). We estimate an upper bound of 30s to
complete a single task, which gives an hourly rate
of $20.4. Workers are selected based on an Amazon
Mechanical Turk qualiﬁcation, speciﬁcally ﬁltering
for workers with high accuracy on past knowledge
base triple evaluations. We follow the same setup
for all evaluations, besides number of annotators.
This setup is shown to result in consistent and reli-
able annotations, with an inter-annotator agreement
given by Fleiss’ kappa (Fleiss, 1971) of 40.8 when
evaluating with 3 annotators, in §3.4.
B Using Alternate Models as Knowledge
Sources
One natural question that arises from the strong
performance of symbolic knowledge distillation is
whether other sources of knowledge (i.e. language
models) would similarly beneﬁt from this method.
In this section, we particularly measure the capacity
of other language models to serve as the “loose
teacher” which generated the base knowledge of
the resulting corpus.
We expand our study beyond GPT-3 here (the
model used in our work), to include 2 contempo-
rary large language models, GPT-J (Wang and Ko-
matsuzaki, 2021) and T5-11B (Lester et al., 2021)
ﬁnetuned for language modelling. For knowledge
generation (verbalization) we follow the same pro-
cedure as §3 along with simple adjustments to im-
prove quality. We are investigating the effect of
the critic on knowledge precision here, so we also
include ATOMIC20
20to probe the usefulness of auto-
matic ﬁltering for human-authored knowledge.
For each knowledge source, we follow the hu-
man evaluation setup in §3.4 to obtain quality an-notations of 2000 examples, with 1 annotation per
example. This follows a similar setup to §4–indeed,
we are replicating the earlier critic experiments but
at a smaller scale (2000 annotations vs. 10000)
to allow for more knowledge sources. For each
knowledge source, we randomly split into sizes of
1400/300/300 for train, dev, and test sets. We fol-
low §4 to train a critic model for each knowledge
source.
We plot different thresholds (% of corpus ﬁl-
tered) against the resulting precision (proportion
of corpus that is judged to be “valid” knowledge)
in Figure 3, and give numbers at various sizes
in Table 7. One striking aspect is that a critic
model can raise the precision of any of these knowl-
edge sources to approximately 90% while retaining
30% of the original corpus size. While this dis-
cards a signiﬁcant portion of the original generated
knowledge, it raises the exciting prospect of using
more cost-effective models at a large scale to gener-
ate strong commonsense corpora like ATOMIC10x.
GPT-J and T5-11B can both be run locally by
researchers, unlike GPT-3 which uses a pay-per-
generation API. Thus, one can imagine producing
a large and high-quality corpus like A TOMIC10xat
a lower cost by instead generating a larger volume
of knowledge from such an accessible model, and
simply ﬁltering to a greater extent.
Another interesting aspect is how the various
knowledge sources diverge. Under little to no criti-
cal ﬁltering (i.e. corpus size = 1.0), the precision
of various knowledge sources is widely spread. Be-
fore applying a critic, quality of knowledge source
is very important. Indeed, precision is ordered
by cost of generation: human ATOMIC20
20has the
highest precision while being the most expensive,
followed by GPT-3 (used here) which is pay-per-
generation, and ﬁnally the two publicly available
models. Another point of divergence is for extreme
ﬁltering (at approximately 20% of the original cor-
pus size. All knowledge sources but GPT-3 plateau
at approximately 90% accuracy, while GPT-3 rises
towards 100%. Indeed, this supports our use of
GPT-3 in this work, as a high-quality automatic
knowledge source.
C Critic Model
We train binary classiﬁers (critics) for human ac-
ceptability using RoBERTa-Large (Liu et al., 2019),
ﬁne-tuning all parameters, along with a 2-layer
MLP on the [CLF] representation. We conduct

--- PAGE 14 ---
Precision atCorpus Size (%)
Knowledge Source 100 90 80 70 60 50 40 30 20 10
ATOMIC20
20 84.0 86.3 87.9 89.0 88.3 88.7 91.7 90.0 90.0 90.0
GPT-J 71.7 76.7 81.7 83.8 86.7 88.0 88.3 87.8 93.3 90.0
T5-11B 64.7 66.7 70.8 74.8 79.4 84.7 89.2 92.2 91.7 93.3
GPT-3 curie 79.3 81.5 85.0 86.2 88.3 90.7 91.7 90.0 98.3 100.0
Table 7: Knowledge precision at various corpus sizes (from 100% to 10%) based on ﬁltering by the critic model.
Precision is calculated by human annotation of valid or invalid knowledge. We consider 4 knowledge sources, as
described in Appendix B. This corresponds to the data plotted in Figure 3.
0 20 40 60 80 100
corpus size (%)65707580859095100precision (%)ATOMIC2020
GPT-J
T5-11B
GPT-3 curie
Figure 3: Precision resulting from the critic step from
§4, with various thresholds. We include corpora gen-
erated by GPT-3 (A TOMIC10x), GPT-J, T5-11B, and
humans (A TOMIC20
20). Without ﬁltering (corpus size
= 1.0), different corpora have a variety of precisions.
As more examples are ﬁltered by the critic, precision
rises signiﬁcantly demonstrating the strong value of the
critic step.a small grid search on the validation set ﬁnding
batch size 128, dropout .1, and Adam (Kingma
and Ba, 2015) learning rate 5e-6 to be effective.
We use early stopping and decay learning rate on
validation performance plateauing, to maximize
R@80% on the validation set. We ﬁnd RoBERTa
pretrained on MNLI (Williams et al., 2018) effec-
tive, outperforming other options. As well, we
substitute randomly-sampled names in for person
designations “X”/“Y”. We include as a baseline an
unsupervised ﬁltration metric inspired by (Davison
et al., 2019): they propose a model estimate of
PMI to score mined commonsense triples. In our
case, we use Negative Log-Likelihood (NLL) and
token-mean-NLL from GPT-3 itself.
The validation precision/recall of our best per-
forming model, the baselines, and the in-optimal
hyperparameter conﬁgurations are given in Fig-
ure 6. Once ﬁxing our model, we applied it to the
test set (also in Fig 6), verifying that it generalizes
toATOMIC10xentries. Overall, our trained critic
model is more effective than the baselines in iden-
tifying high and low quality teacher generations at
all levels of precision and recall. This result demon-
strates that a small amount of human supervision
can consistently help to correct GPT-3’s mistakes.
D A TOMIC10xGeneration Prompts
We include example prompts for all generations
we do, from Table 8 to 15. Note that elements
of generation prompts are randomized for each
batch. For event generation, the few-shot examples
and order are randomly sampled from a seed set
of 100 high-quality examples from ATOMIC20
20in
each batch. For inference generation, the natural
names used for PersonX and PersonY are randomly
sampled from a small predeﬁned set of names.

--- PAGE 15 ---
Instructions (click to expand/collapse)(WARNING: This HIT may contain adult content. Worker discretion is advised.)Thanks for participating in this HIT!If the data is good, it's good. If bad, then bad. Please annotate as you see not worrying about how many of each labelyou !nd yourself assigning! If you understand the words but the Phrases or the complete assertation makes poorsense, please mark as INVALID. Thank you!You will evaluate how often assertions are true. Each assertion is comprised of 3 parts: Phrase A, Relation,Phrase BFor each assertion, determine how true it is:If you see "nothing in particular" for Phrase B, assess Phrase B in context:Sometimes certain actions can simply be responded to by doing nothing!Other times, doing nothing in particular is simply a weird or unlikely reaction to something.See examples under tricky relations tagged with nothing in particular examplePlease report any prejudiced or inappropriate language:Profane or o"ensive content (NSFW, R-rated material etc)Prejudiced assumptions or derogatory language that villainizes people.HOWEVER, please note, not all negative content is derogatory especially if Phrase B is intrinsically what PhraseA means. For example:criminals are characterized by committing crime is OK.↳ This isn't necessarily villianizing people since "criminal" means "a person who has commited a crime".homeless are characterized by being lazy is prejudiced.↳ There are many reason a person is rendered homeless. This is a gratuitous prejudice about homelessness.Material that people may !nd disturbing, o"-putting, or improperA couple NOTES:Please be forgiving of spelling or grammatical errorsIf the terms are too obscure or you don't know the truth of the fact at the top of your head, it is okay to mark is"too unfamiliar to judge". If you can answer (e.g., based on likelihood), please provide a response.Phrase A, Phrase BShort phrases. May describe objects, object properties, events, actions, etc.RelationHow A relates to B.always/oftenAlways or quite often true.sometimes/likelySometimes is true or true for some people. -or- Likely true.farfetched/neverFalse or farfetched, at best. -or- Unlikely to be true.invalidThis assertion makes no sense (i.e., "what does this even mean?!").too unfamiliar to judgeCannot make a fair evaluation. Unfamiliar with one or both of the phrase.
Tricky Relations (click to expand/collpase)Examples (click to expand/collapse)Figure 4: Page 1 of template used for human evaluation.

--- PAGE 16 ---
1) PersonX approaches PersonY's aunt, as a result, PersonX feels, awkwardHow often does the assertion hold true?
This fact is true but outdated
I would count this as an inappropriate, prejudiced or o"ensive material2) PersonX asked PersonY out on a date, can be hindered by, PersonX is still dating SarahHow often does the assertion hold true?
This fact is true but outdated
I would count this as an inappropriate, prejudiced or o"ensive material3) PersonX fails to go home, as a result, PersonX, is groundedHow often does the assertion hold true?
This fact is true but outdated
I would count this as an inappropriate, prejudiced or o"ensive material4) PersonX makes her own clothes, as a result, PersonX feels, artisticHow often does the assertion hold true?
This fact is true but outdated
I would count this as an inappropriate, prejudiced or o"ensive material5) PersonX notices PersonY's response, can be hindered by, PersonX is distracted by the musicHow often does the assertion hold true?
This fact is true but outdated
I would count this as an inappropriate, prejudiced or o"ensive materialalways/oftensometimes/likelyfarfetched/neverinvalidtoo unfamiliar to judge
always/oftensometimes/likelyfarfetched/neverinvalidtoo unfamiliar to judge
always/oftensometimes/likelyfarfetched/neverinvalidtoo unfamiliar to judge
always/oftensometimes/likelyfarfetched/neverinvalidtoo unfamiliar to judge
always/oftensometimes/likelyfarfetched/neverinvalidtoo unfamiliar to judge(Optional) Please let us know if anything was unclear, if you experienced anyissues, or if you have any other fedback for us.
You must ACCEPT the HIT before you can submit the results.Figure 5: Page 2 of template used for human evaluation.

--- PAGE 17 ---
1. Event: PersonX unwraps PersonY’s hands
2. Event: PersonX overcomes evil with good
3. Event: PersonX is fed up with the present situation
4. Event: PersonX breaks PersonX’s back
5. Event: PersonX calls no one
6. Event: PersonX never gets angry
7. Event: PersonX does not learn from PersonY
8. Event: PersonX refuses to touch PersonY’s hands
9. Event: PersonX looks at ﬂowers
10. Event: PersonX unloads an atomic bomb
11. Event:
Table 8: Prompt for head generation.
0.0 0.2 0.4 0.6 0.8 1.0
recall0.600.650.700.750.800.850.900.951.00precisionBest Model Val
Best Model Test
GPT-3 NLL
GPT-3 mean NLL
Figure 6: Precision vs. recall of our critic model on the
human labelled validation set. The best trained mod-
els are labelled, and other hyper-parameter settings are
shown as faded lines. We also include generation neg-
ative log-likelihood (nll) and token-wise mean nll as
cutoff measures–these perform much worse than the su-
pervised model.

--- PAGE 18 ---
Next, how are people seen in each situation? Examples:
Situation 1: Devin bullies Jean.
Devin is seen as dominant.
Situation 2: Jamie moves to another city.
Jamie is seen as adventurous.
Situation 3: Sydney changes Ryan’s mind.
Sydney is seen as inﬂuential.
Situation 4: Lindsay writes a story.
Lindsay is seen as creative.
Situation 5: Rowan covers Pat’s expenses.
Rowan is seen as wealthy.
Situation 6: Lee takes time off.
Lee is seen as carefree.
Situation 7: Riley advises Noel.
Riley is seen as informed.
Situation 8: Adrian bursts into tears.
Adrian is seen as depressed.
Situation 9: Hunter deals with problems.
Hunter is seen as responsible.
Situation 10: Sam follows Charlie.
Sam is seen as suspicious.
Situation 11: Alex makes Chris wait.
Alex is seen as
Table 9: Prompt for generating xAttr.

--- PAGE 19 ---
Next, what do situations make people do? Examples:
Situation 1: Devin gets a divorce.
As a result, Devin dates someone new.
Situation 2: Jamie lifts weights.
As a result, Jamie has sore muscles.
Situation 3: Sydney takes Ryan to a bar.
As a result, Sydney gets drunk.
Situation 4: Lindsay decides to hire a tutor.
As a result, Lindsay gets better grades.
Situation 5: Rowan buys Pat drinks.
As a result, Rowan is thanked by Pat.
Situation 6: Lee hears bad news.
As a result, Lee begins to cry.
Situation 7: Riley buys a chocolate bar.
As a result, Riley gets change.
Situation 8: Adrian does a lot of work.
As a result, Adrian gets mental fatigue.
Situation 9: Hunter attends a concert.
As a result, Hunter hears a new song.
Situation 10: Sam gets the job done.
As a result, Sam gets more responsibilities.
Situation 11: Alex makes Chris wait.
As a result, Alex
Table 10: Prompt for generating xEffect.

--- PAGE 20 ---
For each situation, describe the intent. Examples:
Situation 1: Devin gets the newspaper.
Devin intends to read the newspaper.
Situation 2: Jamie works all night.
Jamie intends to meet a deadline.
Situation 3: Sydney destroys Ryan.
Sydney intends to punish Ryan.
Situation 4: Lindsay clears her mind.
Lindsay intends to be ready for a new task.
Situation 5: Rowan wants to start a business.
Rowan intends to be self sufﬁcient.
Situation 6: Lee ensures Ali’s safety.
Lee intends to be helpful.
Situation 7: Riley buys lottery tickets.
Riley intends to become rich.
Situation 8: Alex makes Chris wait.
Alex intends
Table 11: Prompt for generating xIntent.

--- PAGE 21 ---
Next, we will discuss what people need for certain situations. Examples:
1. Before Devin makes many new friends, Devin has to spend time with people.
2. Before Jamie gets a date, Jamie has to ask someone out.
3. Before Sydney changes Ryan’s mind, Sydney has to think of an argument.
4. Before Lindsay gets a job offer, Lindsay has to apply.
5. Before Rowan takes a quick nap, Rowan has to lie down.
6. Before Lee tries to kiss Ali, Lee has to approach Ali.
7. Before Riley rides Noel’s skateboard, Riley has to borrow it.
8. Before Adrian eats the food, Adrian has to prepare a meal.
9. Before Hunter watches Netﬂix, Hunter has to turn on the TV .
10. Before Sam has a baby shower, Sam has to invite some friends.
11. Before Alex makes Chris wait, Alex has
Table 12: Prompt for generating xNeed.

--- PAGE 22 ---
Next, how do people feel in each situation? Examples:
Situation 1: Devin lives with Jean’s family.
Devin feels loved.
Situation 2: Jamie expects to win.
Jamie feels excited.
Situation 3: Sydney comes home late.
Sydney feels tired.
Situation 4: Lindsay sees dolphins.
Lindsay feels joyful.
Situation 5: Rowan causes Pat anxiety.
Rowan feels guilty.
Situation 6: Lee goes broke.
Lee feels embarrassed.
Situation 7: Riley has a drink.
Riley feels refreshed.
Situation 8: Adrian has a heart condition.
Adrian feels scared about their health.
Situation 9: Hunter shaves Avery’s hair.
Hunter feels helpful.
Situation 10: Sam loses all of Charlie’s money.
Sam feels horrible.
Situation 11: Alex makes Chris wait.
Alex feels
Table 13: Prompt for generating xReact.

--- PAGE 23 ---
Next, what do people want in each situation? Examples:
Situation 1: Devin mows the lawn.
Devin wants to take a shower.
Situation 2: Jamie is going to a party.
Jamie wants to take an Uber home.
Situation 3: Sydney bleeds a lot.
Sydney wants to go to the ER.
Situation 4: Lindsay works as a cashier.
Lindsay wants to ﬁnd a better job.
Situation 5: Rowan gets dirty.
Rowan wants to do a load of laundry.
Situation 6: Lee stays up all night studying.
Lee wants to rest.
Situation 7: Riley gets Noel’s autograph.
Riley wants to tell some friends.
Situation 8: Adrian sees Taylor’s point.
Adrian wants to agree with Taylor.
Situation 9: Hunter leaves Avery’s bike.
Hunter wants to keep the bike safe.
Situation 10: Sam wants a tattoo.
Sam wants to ﬁnd a tattoo design.
Situation 11: Alex makes Chris wait.
Alex wants
Table 14: Prompt for generating xWant.

--- PAGE 24 ---
Next, what can hinder each situation? Examples:
Situation 1: Devin makes a doctor’s appointment,
This is hindered if Devin can’t ﬁnd the phone to call the doctor.
Situation 2: Jamie rubs Wyatt’s forehead,
This is hindered if Jamie is afraid to touch Wyatt.
Situation 3: Sydney eats peanut butter,
This is hindered if Sydney is allergic to peanuts.
Situation 4: Lindsay looks perfect,
This is hindered if Lindsay can’t ﬁnd any makeup.
Situation 5: Rowan goes on a run,
This is hindered if Rowan injures her knees.
Situation 6: Lee takes Ali to the emergency room,
This is hindered if Ali has no health insurance to pay for medical care.
Situation 7: Riley spends time with Noel’s family,
This is hindered if Noel’s family doesn’t like spending time with Riley.
Situation 8: Adrian moves from place to place,
This is hindered if Adrian can’t afford to move.
Situation 9: Hunter protests the government,
This is hindered if Hunter is arrested.
Situation 10: Sam has a huge ﬁght,
This is hindered if Sam does not like confrontation.
Situation 11: Alex makes Chris wait,
This is hindered if
Table 15: Prompt for generating HinderedBy.
