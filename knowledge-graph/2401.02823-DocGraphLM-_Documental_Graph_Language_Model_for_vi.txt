# 2401.02823.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-graph/2401.02823.pdf
# Kích thước tệp: 694172 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
DocGraphLM: Mô hình Ngôn ngữ Đồ thị Tài liệu cho
Trích xuất Thông tin
Dongsheng Wang
JPMorgan AI Research
London, UK
dongsheng.wang@jpmchase.comZhiqiang Ma
JPMorgan AI Research
New York, New York, USA
zhiqiang.ma@jpmchase.comArmineh Nourbakhsh
JPMorgan AI Research
New York, New York, USA
armineh.nourbakhsh@jpmchase.com
Kang Gu
Dartmouth College
Hanover, New Hampshire, USA
Kang.Gu.GR@dartmouth.eduSameena Shah
JPMorgan AI Research
New York, New York, USA
sameena.shah@jpmchase.com
TÓM TẮT
Những tiến bộ trong Hiểu biết Tài liệu Phong phú Trực quan (VrDU) đã cho phép trích xuất thông tin và trả lời câu hỏi trên các tài liệu có bố cục phức tạp. Hai kiểu kiến trúc đã xuất hiện—các mô hình dựa trên transformer được lấy cảm hứng từ LLM, và Mạng Neural Đồ thị. Trong bài báo này, chúng tôi giới thiệu DocGraphLM, một khung làm việc mới kết hợp các mô hình ngôn ngữ được huấn luyện trước với ngữ nghĩa đồ thị. Để đạt được điều này, chúng tôi đề xuất 1) một kiến trúc bộ mã hóa kết hợp để biểu diễn tài liệu, và 2) một phương pháp dự đoán liên kết mới để tái tạo đồ thị tài liệu. DocGraphLM dự đoán cả hướng và khoảng cách giữa các nút bằng cách sử dụng một hàm mất mát kết hợp hội tụ ưu tiên khôi phục vùng lân cận và giảm trọng số phát hiện nút xa. Các thí nghiệm của chúng tôi trên ba bộ dữ liệu SotA cho thấy cải thiện nhất quán trên các tác vụ IE và QA với việc áp dụng các đặc trưng đồ thị. Hơn nữa, chúng tôi báo cáo rằng việc áp dụng các đặc trưng đồ thị tăng tốc độ hội tụ trong quá trình học trong quá trình huấn luyện, mặc dù chỉ được xây dựng thông qua dự đoán liên kết.
CÁC KHÁI NIỆM CCS
•Hệ thống thông tin →Cấu trúc tài liệu ;Mô hình ngôn ngữ;Trích xuất thông tin .
TỪ KHÓA
mô hình ngôn ngữ, mạng neural đồ thị, trích xuất thông tin, hiểu biết tài liệu trực quan
Định dạng Tham chiếu ACM:
Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, and Sameena
Shah. 2023. DocGraphLM: Documental Graph Language Model for Infor-
mation Extraction. In Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR
'23), July 23–27, 2023, Taipei, Taiwan. ACM, New York, NY, USA, 5 pages.
https://doi.org/10.1145/3539618.3591975
Việc tạo bản sao kỹ thuật số hoặc bằng giấy của toàn bộ hoặc một phần công trình này cho việc sử dụng cá nhân hoặc lớp học được cấp phép miễn phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của những người khác ngoài (các) tác giả phải được tôn trọng. Việc tóm tắt có ghi nguồn được cho phép. Để sao chép theo cách khác, hoặc tái xuất bản, đăng trên máy chủ hoặc phân phối lại cho danh sách, yêu cầu sự cho phép cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
SIGIR '23, July 23–27, 2023, Taipei, Taiwan
©2023 Bản quyền thuộc về chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM ISBN 978-1-4503-9408-6/23/07. . . $15.00
https://doi.org/10.1145/3539618.35919751 GIỚI THIỆU
Trích xuất thông tin từ các tài liệu phong phú trực quan (VrDs), như các biểu mẫu kinh doanh, hóa đơn, và hóa đơn thanh toán ở định dạng PDF hoặc hình ảnh đã thu hút sự chú ý gần đây. Các tác vụ như nhận dạng và trích xuất trường và liên kết thực thể là rất quan trọng để số hóa VrDs và xây dựng hệ thống truy xuất thông tin trên dữ liệu. Các tác vụ yêu cầu lý luận phức tạp như Trả lời Câu hỏi Trực quan trên tài liệu yêu cầu mô hình hóa các tín hiệu không gian, trực quan và ngữ nghĩa trong VrDs. Do đó, Hiểu biết VrD quan tâm đến việc mô hình hóa nội dung đa phương thức trong tài liệu hình ảnh. Nghiên cứu trước đây đã khám phá việc sử dụng mã hóa văn bản, bố cục và các đặc trưng hình ảnh trong một mô hình ngôn ngữ bố cục hoặc thiết lập đa phương thức để cải thiện các tác vụ hạ nguồn. Ví dụ, LayoutLM và các biến thể của nó [7,23,24] sử dụng thông tin hình ảnh và bố cục để tăng cường biểu diễn văn bản, từ đó cải thiện hiệu suất trên nhiều tác vụ khác nhau. Tuy nhiên, các mô hình sử dụng cơ chế Transformer đặt ra thách thức trong việc biểu diễn ngữ nghĩa xa về mặt không gian, như các ô bảng xa so với tiêu đề của chúng hoặc nội dung qua các dấu ngắt dòng. Dưới ánh sáng của những hạn chế này, một số nghiên cứu [25,27] đã đề xuất sử dụng mạng neural đồ thị (GNNs) để mô hình hóa mối quan hệ và cấu trúc giữa các token văn bản hoặc phân đoạn trong tài liệu. Mặc dù những mô hình này một mình vẫn kém hiệu suất so với các mô hình ngôn ngữ bố cục, chúng chứng minh tiềm năng của việc kết hợp thông tin có cấu trúc bổ sung để cải thiện biểu diễn tài liệu.

Được thúc đẩy bởi điều này, chúng tôi giới thiệu một khung làm việc mới có tên DocGraphLM tích hợp ngữ nghĩa đồ thị tài liệu và ngữ nghĩa có nguồn gốc từ các mô hình ngôn ngữ được huấn luyện trước để cải thiện biểu diễn tài liệu. Như được mô tả trong Hình 1, đầu vào cho mô hình của chúng tôi là các embedding của token, vị trí và hộp giới hạn, tạo thành nền tảng của biểu diễn tài liệu. Để tái tạo đồ thị tài liệu, chúng tôi đề xuất một phương pháp dự đoán liên kết mới dự đoán hướng và khoảng cách giữa các nút bằng cách sử dụng hàm mất mát kết hợp, cân bằng mất mát phân loại và hồi quy. Ngoài ra, mất mát khuyến khích khôi phục vùng lân cận gần trong khi giảm cấp phát hiện trên các nút xa hơn. Điều này được đạt được bằng cách chuẩn hóa khoảng cách thông qua biến đổi logarithm, coi các nút được phân tách bởi khoảng cách có độ lớn cụ thể như có khoảng cách ngữ nghĩa bằng nhau.

Các thí nghiệm của chúng tôi trên nhiều bộ dữ liệu bao gồm FUNSD, CORD, và DocVQA, cho thấy tính ưu việt của mô hình một cách nhất quán. Hơn nữa, việc kết hợp các đặc trưng đồ thị được tìm thấyarXiv:2401.02823v1  [cs.CL]  5 Jan 2024

--- TRANG 2 ---
SIGIR '23, July 23–27, 2023, Taipei, Taiwan Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, and Sameena Shah
Attention
Đa đầu
Tám hướng & 
Khoảng cách logarithmDự đoán hướng và khoảng cách 
với mất mát kết hợpLược đồ học kết hợp
GNN
Biểu diễn nút & cạnh 
tiềm ẩnToken IDs
Vị trí
Hộp giới hạnCộng 
&chuẩn hóaMô hình Ngôn ngữ (Với thông tin bố cục) Ngữ nghĩa Token
Đồ thị Tái tạoxNFeed
forwardCộng 
&chuẩn hóa
[Hướng, Khoảng cách]Trích xuất thực thể
Head
VQA
Head012
3
4
567
Hình 1: Kiến trúc mô hình của DocGraphLM.
để tăng tốc quá trình học. Chúng tôi nhấn mạnh những đóng góp chính của công trình như sau:
•chúng tôi đề xuất một kiến trúc mới tích hợp mạng neural đồ thị với mô hình ngôn ngữ được huấn luyện trước để tăng cường biểu diễn tài liệu;
•chúng tôi giới thiệu một phương pháp dự đoán liên kết đến tái tạo đồ thị tài liệu, và một hàm mất mát kết hợp nhấn mạnh khôi phục trên các nút lân cận gần;
•cuối cùng, các đặc trưng neural đồ thị được đề xuất dẫn đến cải thiện nhất quán về hiệu suất và hội tụ nhanh hơn.

2 CÔNG TRÌNH LIÊN QUAN
Các kiến trúc dựa trên Transformer đã được áp dụng thành công cho các tác vụ hiểu biết bố cục, vượt qua kết quả hiện đại (SotA) trước đây [3,13,14,16,21,22]. Các nghiên cứu như LayoutLM [23] và LayoutLMv2 [24] kết hợp embedding văn bản với các đặc trưng trực quan sử dụng mạng đề xuất vùng, cho phép các mô hình được huấn luyện trên các mục tiêu như Mô hình Ngôn ngữ Trực quan Ẩn (MVLM) và attention nhận biết không gian, dẫn đến hiệu suất cải thiện trên các tác vụ phức tạp như VQA và hiểu biết biểu mẫu. TILT [9] tăng cường attention bằng cách thêm bias để nắm bắt vị trí 2-D tương đối, điều này đã cho thấy hiệu suất xuất sắc trên bảng xếp hạng DocVQA. StructuralLM [12] tận dụng tối đa các tương tác của các ô trong đó mỗi ô chia sẻ cùng hộp giới hạn.

Việc sử dụng GNNs [20] để biểu diễn tài liệu cho phép thông tin lan truyền linh hoạt hơn. Trong các mô hình VrDU dựa trên GNN, tài liệu thường được biểu diễn dưới dạng đồ thị của token và/hoặc câu, và các cạnh biểu diễn mối quan hệ không gian giữa chúng, ví dụ nắm bắt K-Nearest Neighbours. Các mô hình dựa trên GNN có thể được sử dụng cho nhiều tác vụ dựa trên tài liệu khác nhau như phân loại văn bản [25,27] hoặc trích xuất thông tin chính [2,26]. Tuy nhiên, hiệu suất của chúng vẫn thua kém so với các mô hình ngôn ngữ bố cục. Điều này là do biểu diễn đồ thị một mình không đủ để nắm bắt ngữ nghĩa phong phú của một tài liệu. Trong các trường hợp mà các mô hình dựa trên GNN vượt trội đáng kể so với các mô hình ngôn ngữ bố cục, chúng thường lớn hơn và tập trung vào các tác vụ cụ thể [10]. Trong bài báo này, chúng tôi đề xuất một khung làm việc kết hợp ngữ nghĩa phong phú của các mô hình ngôn ngữ bố cục với tín hiệu cấu trúc mạnh mẽ được nắm bắt bởi các mô hình GNN. Chúng tôi chứng minh cách việc bổ sung ngữ nghĩa đồ thị có thể tăng cường hiệu suất của các mô hình ngôn ngữ bố cục trên các tác vụ IE và QA, và cải thiện hội tụ mô hình.

3 DOCGRAPHLM: MÔ HÌNH NGÔN NGỮ
ĐỒ THỊ TÀI LIỆU
3.1 Biểu diễn tài liệu dưới dạng đồ thị
Trong GNN, một đồ thị bao gồm các nút và cạnh. Trong bối cảnh biểu diễn tài liệu dưới dạng đồ thị, các nút biểu diễn các phân đoạn văn bản (tức là các nhóm từ liền kề) và mối quan hệ giữa chúng được biểu diễn dưới dạng cạnh. Các phân đoạn văn bản từ tài liệu hình ảnh có thể được lấy thông qua các công cụ Nhận dạng Ký tự Quang học, thường nắm bắt các token dưới dạng hộp giới hạn với nhiều kích thước khác nhau.

Để tạo ra các cạnh giữa các nút, chúng tôi áp dụng một heuristic mới có tên Direction Line-of-sight (D-LoS), thay vì phương pháp K-nearest-neighbours (KNN) [19] hoặc phương pháp β-skeleton [11] thường được sử dụng. Phương pháp KNN có thể dẫn đến các hàng hoặc cột dày đặc, không liên quan được coi là hàng xóm, bỏ qua thực tế rằng một số cặp khóa-giá trị trong biểu mẫu có thể là các nút xa nhau hơn. Để giải quyết điều này, chúng tôi áp dụng phương pháp D-LoS, trong đó chúng tôi chia đường chân trời 360 độ xung quanh một nút nguồn thành tám khu vực rời rạc 45 độ, và chúng tôi xác định nút gần nhất đối với nút nguồn trong mỗi khu vực. Tám khu vực này xác định tám hướng đối với nút nguồn. Định nghĩa này được lấy cảm hứng từ tác vụ tiền huấn luyện được báo cáo trong StrucTexT [14] áp dụng phương pháp này để xây dựng biểu diễn đồ thị của nó.

Biểu diễn nút. Một nút có hai đặc trưng — ngữ nghĩa văn bản và kích thước nút. Ngữ nghĩa văn bản có thể được lấy thông qua embedding token (ví dụ từ các mô hình ngôn ngữ), trong khi kích thước nút được biểu thị bởi các chiều của nó trên tọa độ x và y, về mặt toán học M=emb([width,height]) trong đó width =x2−x1 và height =y2−y1, cho rằng (x1,y1) và (x2,y2) là tọa độ của góc trên bên trái và góc dưới bên phải của hộp giới hạn phân đoạn. Trực giác, kích thước nút là một chỉ báo quan trọng vì nó giúp phân biệt kích thước phông chữ và có thể là vai trò ngữ nghĩa của phân đoạn, ví dụ, tiêu đề, chú thích và nội dung. Do đó, chúng tôi ký hiệu một đầu vào nút

--- TRANG 3 ---
DocGraphLM: Documental Graph Language Model for Information Extraction SIGIR '23, July 23–27, 2023, Taipei, Taiwan
như Eu=emb(Tu)⊕Mu, trong đó u={1,2,...,N} chỉ ra nút thứ u trong một tài liệu và Tu đại diện cho các văn bản bên trong nút u.
Chúng tôi học biểu diễn nút bằng cách tái tạo đồ thị tài liệu sử dụng GNN, được biểu thị là hGu=GNN(Eu). Chi tiết về việc học hGu được mô tả trong Phần 3.2.
Biểu diễn cạnh. Để biểu thị mối quan hệ giữa hai nút, chúng tôi sử dụng các đặc trưng polar của chúng, bao gồm khoảng cách tương đối và hướng (một trong tám khả năng). Chúng tôi tính khoảng cách Euclidean ngắn nhất, d, giữa hai hộp giới hạn. Để giảm tác động của các nút xa có thể ít liên quan về mặt ngữ nghĩa đến nút nguồn, chúng tôi áp dụng kỹ thuật làm mượt khoảng cách với biến đổi log được ký hiệu là edis=log(d+1). Hướng tương đối edir∈{0,..., 7} cho một cặp nút được lấy từ D-LoS. Chúng tôi định nghĩa một liên kết, được ký hiệu là ep=[edis,edir], để tái tạo đồ thị tài liệu trong phần 3.2.

3.2 Tái tạo đồ thị bằng dự đoán liên kết
Chúng tôi dự đoán hai thuộc tính chính của các liên kết ep để tái tạo đồ thị và đóng khung quá trình như một vấn đề học đa tác vụ.
Đầu vào cho GNN là các biểu diễn nút được mã hóa, và biểu diễn được truyền qua cơ chế truyền tin trên GNN, cụ thể:
hG,l+1u :=aggregate(hG,lv,∀v∈N(u)), (1)
trong đó l là lớp của các hàng xóm, N(u) ký hiệu tập hợp các hàng xóm của nút u, và aggregate(·) là một hàm tổng hợp cập nhật biểu diễn nút.

Chúng tôi huấn luyện kết hợp GNN trên hai tác vụ — dự đoán khoảng cách và hướng giữa các nút — để học biểu diễn nút. Đối với dự đoán khoảng cách, chúng tôi định nghĩa một đầu hồi quy ŷeu,v, tạo ra một giá trị vô hướng thông qua tích vô hướng của hai vector nút, và sử dụng một kích hoạt tuyến tính, như được trình bày trong Phương trình 2.
ŷeu,v=Linear((hGu)⊤×hGv) (2)

Đối với dự đoán hướng, chúng tôi định nghĩa một đầu phân loại ŷdu,v gán một trong tám hướng cho mỗi cạnh dựa trên tích theo từng phần tử giữa hai nút, được biểu thị như sau:
ŷdu,v=σ((hGu⊙hGv)×W) (3)
trong đó hGu⊙hGv là một tích theo từng phần tử giữa hai nút và W là trọng số có thể học cho vector tích. σ là một hàm kích hoạt phi tuyến.

Chúng tôi sử dụng mất mát MSE cho hồi quy khoảng cách và cross-entropy cho phân loại hướng, tương ứng. Sau đó, mất mát kết hợp là:
loss=∑(u,v)∈batch[(λ·lossMSE(ŷeu,v,yeu,v)
+(1−λ)·lossCE(ŷdu,v,ydu,v)]·( 1−ru,v)(4)
trong đó λ là một siêu tham số có thể điều chỉnh cân bằng trọng số của hai mất mát, và ru,v là chuẩn hóa của khoảng cách edis, bị ràng buộc trong khoảng [0,1], sao cho giá trị của 1−ru,v giảm trọng số các phân đoạn xa và ưu tiên các phân đoạn gần.

3.3 Biểu diễn kết hợp
Biểu diễn nút kết hợp, hCu, là một kết hợp của biểu diễn mô hình ngôn ngữ hLu và biểu diễn GNN hGu thôngBảng 1: Thống kê của các bộ dữ liệu tài liệu trực quan. Sự khác biệt giữa DocVQA và DocVQA† được giới thiệu trong Phần 4.1.
Bộ dữ liệu Số nhãn Số train Số val Số test
FUNSD 4 149 - 50
CORD 30 800 100 100
DocVQA - 39,000 5,000 5,000
DocVQA† - 32,553 4,400 5,000
qua một hàm tổng hợp f (ví dụ, nối chuỗi, trung bình, hoặc tổng) được biểu diễn là hCu=f(hLu,hGu). Trong công trình này, chúng tôi vận hành hàm tổng hợp f với nối chuỗi ở cấp độ token. Các biểu diễn nút được giới thiệu có thể được sử dụng làm đầu vào cho các mô hình khác để hỗ trợ các tác vụ hạ nguồn, ví dụ, IE_Head(hCu) cho trích xuất thực thể và QA_Head (hCu) cho tác vụ trả lời câu hỏi trực quan.

4 THỰC NGHIỆM
4.1 Bộ dữ liệu và đường cơ sở
Chúng tôi đánh giá các mô hình của mình trên hai tác vụ trích xuất thông tin trên ba bộ dữ liệu thường được sử dụng: FUNSD [8], CORD [18], và DocVQA [17]. FUNSD và CORD tập trung vào trích xuất cấp độ thực thể, trong khi DocVQA tập trung vào việc xác định các đoạn trả lời trong tài liệu hình ảnh trong một tác vụ trả lời câu hỏi. Thống kê bộ dữ liệu được hiển thị trong Bảng 1. Vui lòng tham khảo các trích dẫn để biết thêm chi tiết.

Cần lưu ý rằng các tệp OCR được cung cấp trong DocVQA1 chứa một số lượng nhỏ các đầu ra OCR không hoàn hảo, ví dụ, lỗi căn chỉnh văn bản và văn bản bị thiếu, dẫn đến thất bại trong việc xác định câu trả lời. Chúng tôi chỉ có thể sử dụng 32,553 mẫu để huấn luyện và 4,400 mẫu để xác thực. Chúng tôi ký hiệu bộ dữ liệu đã sửa đổi là DocVQA†. Vì lợi ích đảm bảo so sánh công bằng trong các thí nghiệm của chúng tôi, chúng tôi đã duy trì việc sử dụng các đầu ra OCR từ bộ dữ liệu.

Như các đường cơ sở của chúng tôi, chúng tôi sử dụng các mô hình SotA tận dụng các đặc trưng khác nhau, bao gồm RoBERTa [15], BROS [6], DocFormer-base [1], StructuralLM [12], LayoutLM [23], LayoutLMv3 [7] và Doc2Graph [4]. RoBERTa là mô hình transformer không có bất kỳ đặc trưng bố cục hoặc hình ảnh nào, BROS và StructuralLM áp dụng thông tin bố cục duy nhất, DocFormer và LayoutLMv3 sử dụng cả đặc trưng bố cục và hình ảnh, và Doc2Graph chỉ dựa vào các đặc trưng đồ thị tài liệu.

4.2 Thiết lập thực nghiệm
Đối với FUNSD và CORD, chúng tôi áp dụng các siêu tham số huấn luyện sau: epoch = 20, learning rate = 5e-5, và batch size = 6, và huấn luyện mô hình của chúng tôi trên một GPU NVIDIA T4 Tensor Core duy nhất. Đối với DocVQA, chúng tôi áp dụng các siêu tham số huấn luyện sau: epoch = 5, learning rate = 5e-5, và batch size = 4.

Chúng tôi áp dụng GraphSage [5] làm mô hình GNN của chúng tôi, vì nó đã được chứng minh hiệu quả trong các đặc trưng đồ thị tài liệu [4]. Đối với tái tạo đồ thị, chúng tôi đặt một giá trị hằng số λ=0.5 trong suốt thí nghiệm.

4.3 Kết quả
Hiệu suất của DocGraphLM và các mô hình khác trên bộ dữ liệu FUNSD được trình bày trong Bảng 2. Mô hình của chúng tôi đạt điểm F1 tốt nhất1https://www.docvqa.org/

--- TRANG 4 ---
SIGIR '23, July 23–27, 2023, Taipei, Taiwan Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, and Sameena Shah
Bảng 2: So sánh hiệu suất mô hình trên FUNSD.
Mô hình F1 Precision Recall
RoBERTa-base 65.37 61.17 70.20
Doc2Graph⋄[4] 82.25 - -
StructuralLM_large⋄[12] 85.14 83.52 86.81
LayoutLM-base⋄[23] 78.66 75.97 81.55
LayoutLMv3-base[7] 88.16 86.70 87.7
BROS⋄[6] 83.05 81.16 85.02
DocFormer-base⋄[1] 83.34 80.76 86.09
DocGraphLM (RoBERTa-base) 67.03 ( ↑1.66) 62.92 70.0
DocGraphLM (LLMv3-base) 88.77(↑0.61) 87.44 90.15
Bảng 3: So sánh hiệu suất mô hình CORD.
Mô hình F1 Precision Recall
RoBERTa-base 48.99 42.77 57.34
LayoutLM-base⋄94.80 95.03 94.58
LayoutLMv3-base 95.59 95.31 95.88
BROS⋄95.36 95.58 95.14
DocFormer-base⋄96.33 96.52 96.14
DocGraphLM (RoBERTa-base) 51.25 ( ↑2.26) 45.45 58.76
DocGraphLM (LayoutLMv3-base) 96.93 (↑1.62) 96.86 97.01
ở mức 88.77, đạt được khi nó được kết hợp với mô hình LayoutLMv3-base. Mặt khác, RoBERTa-base (không tận dụng các đặc trưng bố cục) có điểm F1 thấp nhất là 65.37, nhưng kết hợp nó với DocGraphLM dẫn đến cải thiện 1.66 điểm. Vui lòng lưu ý các điểm số với ⋄ được báo cáo trong các trích dẫn tương ứng. Ký hiệu tương tự áp dụng cho các bảng khác.

Đối với bộ dữ liệu CORD, các so sánh hiệu suất được hiển thị trong Bảng 3, và hiệu suất tốt nhất được đạt bởi DocGraphLM (LayoutLMv3-base) với điểm F1 là 96.93, theo sát là BROS. Tương tự, mặc dù RoBERTa-base một mình đạt điểm số thấp hơn nhiều, DocGraphLM (RoBERTa-base) tăng điểm F1 lên 2.26 điểm.

Bảng 4 hiển thị hiệu suất mô hình trên bộ dữ liệu test DocVQA. Các điểm hiệu suất được lấy bằng cách gửi đầu ra mô hình của chúng tôi đến bảng xếp hạng DocVQA2, vì các câu trả lời ground-truth không được cung cấp cho công chúng. Bên cạnh điểm số tổng thể, hiệu suất của mô hình trên các tác vụ danh mục phụ cũng được báo cáo. DocGraphLM (với LayoutLMv3-base) vượt trội hơn các mô hình khác trong hầu hết mọi khía cạnh trừ ngữ nghĩa văn bản thuần túy, điều này cho thấy khả năng mô hình hóa ngữ nghĩa đa phương thức hiệu quả của mô hình. Bảng trình bày bằng chứng mạnh mẽ về hiệu quả của DocGraphLM trong việc cải thiện biểu diễn tài liệu, khi các mô hình ngôn ngữ bố cục được tăng cường với phương pháp của chúng tôi.

Hiệu suất vượt trội trên nhiều bộ dữ liệu khác nhau cho thấy rằng việc sử dụng biểu diễn đồ thị được đề xuất trong DocGraphLM dẫn đến cải thiện nhất quán. Một p-value nhỏ hơn 0.05 được nhận khi so sánh hiệu suất của các mô hình trên các bộ dữ liệu này, cho thấy cải thiện có ý nghĩa thống kê từ mô hình của chúng tôi.
2https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1Bảng 4: So sánh hiệu suất mô hình trên bộ dữ liệu testing DocVQA. Điểm số từ bảng xếp hạng DocVQA.
Mô hình Score Form Table Text
RoBERTa_base 60.40 71.75 54.23 61.35
LayoutLMv3_base 67.80 77.84 67.58 70.55
DocGraphLM (LayoutLMv3-base) 69.84 (↑2.04) 79.73 68.48 63.23

4.4 Tác động đến hội tụ
Chúng tôi cũng quan sát thấy rằng tốc độ hội tụ huấn luyện thường nhanh hơn khi bổ sung các đặc trưng đồ thị so với LayoutLM thông thường (các mô hình base V1 và V3). Ví dụ, Hình 2 minh họa rằng điểm F1 cải thiện với tốc độ hội tụ nhanh hơn trong bốn epoch đầu tiên, khi thử nghiệm trên bộ dữ liệu CORD. Điều này có thể do các đặc trưng đồ thị cho phép transformer tập trung nhiều hơn vào các hàng xóm gần, cuối cùng dẫn đến quá trình lan truyền thông tin hiệu quả hơn.

Hình 2: So sánh tốc độ hội tụ mô hình trên CORD. Các đường cong được tạo ra từ trung bình của mười thử nghiệm.

5 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Bài báo này trình bày một khung làm việc DocGraphLM mới kết hợp ngữ nghĩa đồ thị với các mô hình ngôn ngữ được huấn luyện trước để cải thiện biểu diễn tài liệu cho VrDs. Phương pháp dự đoán liên kết được đề xuất tái tạo khoảng cách và hướng giữa các nút, ngày càng giảm trọng số các liên kết xa hơn. Các thí nghiệm của chúng tôi trên nhiều tác vụ hạ nguồn trên nhiều bộ dữ liệu khác nhau cho thấy hiệu suất tăng cường so với đường cơ sở chỉ-LM. Ngoài ra, việc giới thiệu các đặc trưng đồ thị tăng tốc quá trình học. Như một hướng tương lai, chúng tôi dự định kết hợp các kỹ thuật tiền huấn luyện khác nhau cho các phân đoạn tài liệu khác nhau. Chúng tôi cũng sẽ kiểm tra tác động của các biểu diễn liên kết khác nhau cho tái tạo đồ thị.

Tuyên bố từ chối trách nhiệm. Bài báo này được chuẩn bị cho mục đích thông tin bởi nhóm Nghiên cứu Trí tuệ Nhân tạo của JPMorgan Chase & Co. và các chi nhánh ("JP Morgan"), và không phải là sản phẩm của Bộ phận Nghiên cứu của JP Morgan. JP Morgan không đưa ra bất kỳ tuyên bố và bảo đảm nào và từ chối mọi trách nhiệm pháp lý, về tính đầy đủ, chính xác hoặc độ tin cậy của thông tin chứa trong đây. Tài liệu này không được dự định làm nghiên cứu đầu tư hoặc lời khuyên đầu tư, hoặc một khuyến nghị, đề nghị hoặc lời mời gọi mua hoặc bán bất kỳ chứng khoán, công cụ tài chính, sản phẩm tài chính hoặc dịch vụ nào, hoặc được sử dụng theo bất kỳ cách nào để đánh giá giá trị của việc tham gia vào bất kỳ giao dịch nào, và không được cấu thành một lời mời gọi dưới bất kỳ quyền tài phán nào hoặc đối với bất kỳ người nào, nếu lời mời gọi như vậy dưới quyền tài phán như vậy hoặc đối với người như vậy sẽ là bất hợp pháp.

--- TRANG 5 ---
DocGraphLM: Documental Graph Language Model for Information Extraction SIGIR '23, July 23–27, 2023, Taipei, Taiwan
TÀI LIỆU THAM KHẢO
[1]Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R
Manmatha. 2021. Docformer: End-to-end transformer for document understand-
ing. In Proceedings of the IEEE/CVF international conference on computer vision .
993–1003.
[2]Brian L. Davis, Bryan S. Morse, Brian L. Price, Chris Tensmeyer, and Curtis
Wigington. 2021. Visual FUDGE: Form Understanding via Dynamic Graph
Editing. CoRR abs/2105.08194 (2021). arXiv:2105.08194 https://arxiv.org/abs/
2105.08194
[3]Lukasz Garncarek, Rafal Powalski, Tomasz Stanislawek, Bartosz Topolski, Pi-
otr Halama, and Filip Gralinski. 2020. LAMBERT: Layout-Aware language
Modeling using BERT for information extraction. CoRR abs/2002.08087 (2020).
arXiv:2002.08087 https://arxiv.org/abs/2002.08087
[4]Andrea Gemelli, Sanket Biswas, Enrico Civitelli, Josep Lladós, and Simone Mari-
nai. 2022. Doc2Graph: a Task Agnostic Document Understanding Framework
based on Graph Neural Networks. arXiv preprint arXiv:2208.11168 (2022).
[5]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[6]Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and
Sungrae Park. 2020. BROS: a pre-trained language model for understanding texts
in document. (2020).
[7]Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Layoutlmv3:
Pre-training for document ai with unified text and image masking. In Proceedings
of the 30th ACM International Conference on Multimedia . 4083–4091.
[8]Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. 2019. Funsd: A
dataset for form understanding in noisy scanned documents. In 2019 International
Conference on Document Analysis and Recognition Workshops (ICDARW) , Vol. 2.
IEEE, 1–6.
[9]Rafa l Powalski, Lukasz Borchmann, and Dawid Jurkiewicz. 2021. Going Full-
TILT Boogie on Document Understanding with Text-Image-Layout Transformer.
arXiv preprint arXiv:2102.09550 (2021).
[10] Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan
Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister. 2022.
Formnet: Structural encoding beyond sequential modeling in form document
information extraction. arXiv preprint arXiv:2203.08411 (2022).
[11] Chen-Yu Lee, Chun-Liang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii, Siyang
Qin, Ashok Popat, and Tomas Pfister. 2021. ROPE: Reading Order Equivariant
Positional Encoding for Graph-based Document Information Extraction. In Pro-
ceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing (Vol-
ume 2: Short Papers) . Association for Computational Linguistics, Online, 314–321.
https://doi.org/10.18653/v1/2021.acl-short.41
[12] Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and
Luo Si. 2021. Structurallm: Structural pre-training for form understanding. arXiv
preprint arXiv:2105.11210 (2021).
[13] Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain,
Varun Manjunatha, and Hongfu Liu. 2021. Selfdoc: Self-supervised document
representation learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition . 5652–5660.[14] Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun
Yao, Junyu Han, Jingtuo Liu, and Errui Ding. 2021. Structext: Structured text
understanding with multi-modal transformers. In Proceedings of the 29th ACM
International Conference on Multimedia . 1912–1920.
[15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).
[16] Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley
Wendt, Qi Zhao, and Marc Najork. 2020. Representation learning for information
extraction from form-like documents. In proceedings of the 58th annual meeting
of the Association for Computational Linguistics . 6495–6504.
[17] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: A dataset
for vqa on document images. In Proceedings of the IEEE/CVF winter conference on
applications of computer vision . 2200–2209.
[18] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon
Seo, and Hwalsuk Lee. 2019. CORD: a consolidated receipt dataset for post-OCR
parsing. In Workshop on Document Intelligence at NeurIPS 2019 .
[19] Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, and Regina Barzilay. 2019.
GraphIE: A Graph-Based Framework for Information Extraction. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,
751–761. https://doi.org/10.18653/v1/N19-1082
[20] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. The graph neural network model. IEEE transactions on neural
networks 20, 1 (2008), 61–80.
[21] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. 2021. Lay-
outreader: Pre-training of text and layout for reading order detection. arXiv
preprint arXiv:2108.11591 (2021).
[22] Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang. 2020. Docstruct: A
multimodal method to extract hierarchy structure in document for general form
understanding. arXiv preprint arXiv:2010.11685 (2020).
[23] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020.
Layoutlm: Pre-training of text and layout for document image understanding.
InProceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining . 1192–1200.
[24] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan
Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al .2020. Layoutlmv2: Multi-
modal pre-training for visually-rich document understanding. arXiv preprint
arXiv:2012.14740 (2020).
[25] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional net-
works for text classification. In Proceedings of the AAAI conference on artificial
intelligence , Vol. 33. 7370–7377.
[26] Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao. 2021. PICK:
processing key information extraction from documents using improved graph
learning-convolutional networks. In 2020 25th International Conference on Pattern
Recognition (ICPR) . IEEE, 4363–4370.
[27] Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen, and Liang Wang.
2020. Every document owns its structure: Inductive text classification via graph
neural networks. arXiv preprint arXiv:2004.13826 (2020).
