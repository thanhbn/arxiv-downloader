IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1
Học Prompt Đồ Thị: Một Khảo Sát Toàn Diện và Xa Hơn
Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, Jia Li

Tóm tắt —Trí Tuệ Nhân Tạo Tổng Quát (AGI) đã cách mạng hóa nhiều lĩnh vực, tuy nhiên việc tích hợp với dữ liệu đồ thị, một nền tảng trong thế giới kết nối của chúng ta, vẫn còn sơ khai. Bài báo này trình bày một khảo sát tiên phong về lĩnh vực mới nổi của graph prompts trong AGI, giải quyết những thách thức và cơ hội chính trong việc khai thác dữ liệu đồ thị cho các ứng dụng AGI. Mặc dù có những tiến bộ đáng kể trong AGI qua xử lý ngôn ngữ tự nhiên và thị giác máy tính, việc ứng dụng vào dữ liệu đồ thị vẫn tương đối chưa được khám phá kỹ. Khảo sát này đánh giá một cách phê phán bối cảnh hiện tại của AGI trong xử lý dữ liệu đồ thị, làm nổi bật những thách thức riêng biệt trong các ứng dụng cross-modality, cross-domain, và cross-task cụ thể cho đồ thị. Công trình của chúng tôi là đầu tiên đề xuất một khung thống nhất để hiểu về graph prompt learning, mang lại sự rõ ràng về prompt tokens, token structures, và insertion patterns trong lĩnh vực đồ thị. Chúng tôi đi sâu vào các tính chất nội tại của graph prompts, khám phá tính linh hoạt, khả năng biểu đạt, và sự tương tác với các mô hình đồ thị hiện có. Một phân loại toàn diện categorizes hơn 100 công trình trong lĩnh vực này, sắp xếp chúng với các nhiệm vụ pre-training qua các mục tiêu node-level, edge-level, và graph-level. Ngoài ra, chúng tôi giới thiệu ProG, một thư viện Python, và một trang web đi kèm, để hỗ trợ và thúc đẩy nghiên cứu trong graph prompting. Khảo sát kết thúc với một cuộc thảo luận về các thách thức hiện tại và hướng tương lai, đưa ra lộ trình cho nghiên cứu trong graph prompting trong AGI. Thông qua phân tích toàn diện này, chúng tôi nhằm kích thích việc khám phá thêm và các ứng dụng thực tế của AGI trong dữ liệu đồ thị, nhấn mạnh tiềm năng của nó để định hình lại các lĩnh vực AGI và xa hơn nữa. ProG và trang web có thể được truy cập tại https://github.com/WxxShirley/Awesome-Graph-Prompt, và https://github.com/sheldonresearch/ProG, tương ứng.

Từ khóa chỉ mục —graph prompt, graph pre-training, graph learning, artificial general intelligence.
✦

1 GIỚI THIỆU

Trong một thời đại được đánh dấu bởi sự phát triển nhanh chóng của Trí Tuệ Nhân Tạo Tổng Quát (AGI), đã xuất hiện nhiều ứng dụng tuyệt vời với các kỹ thuật AGI như ChatGPT trong Xử Lý Ngôn Ngữ Tự Nhiên (NLP) và Midjourney trong Thị Giác Máy Tính (CV). AGI đã cải thiện đáng kể cuộc sống của chúng ta, làm cho công việc của chúng ta hiệu quả hơn và giải phóng chúng ta khỏi các nhiệm vụ lặp đi lặp lại để tập trung vào những nỗ lực sáng tạo hơn. Tuy nhiên, khi làm việc với dữ liệu đồ thị, các ứng dụng AGI vẫn đang trong giai đoạn đầu so với thành công to lớn trong các lĩnh vực NLP [9,2,50] và CV [91,114]. Trong thế giới ngày càng kết nối của chúng ta, việc hiểu và khai thác những hiểu biết có giá trị từ đồ thị là rất quan trọng. Điều này đặt AGI áp dụng cho dữ liệu đồ thị ở tiền tuyến của cả sự quan tâm học thuật và công nghiệp [48,120,108], với tiềm năng để định hình lại các lĩnh vực như thiết kế thuốc [68, 64] và phát triển pin [90], v.v.

Tuy nhiên, việc hiện thực hóa tầm nhìn này không bao giờ dễ dàng. Hình 1 minh họa bối cảnh này cho nghiên cứu gần đây trong Trí Tuệ Nhân Tạo Tổng Quát, từ đó chúng ta có thể thấy rằng có ít nhất ba vấn đề cơ bản trong kỹ thuật: Làm thế nào để tạo ra mô hình tổng quát cho các modalities khác nhau, các domains khác nhau, và các tasks khác nhau? Trong các lĩnh vực NLP và CV, đã có nhiều mô hình thương mại có thể hiểu và dịch thông tin qua các modalities này [9,114,2]. Ví dụ, các mô hình như BERT [9] và GPT-3 [2] đã chứng minh khả năng thực hiện các nhiệm vụ liên quan đến cả thông tin văn bản và thị giác. Tuy nhiên, trong bối cảnh dữ liệu đồ thị, việc hài hòa thông tin từ nhiều modalities vẫn còn là lãnh thổ chưa được khám phá [44].

Đối với vấn đề cross-domain, transfer learning đã được chứng minh là hiệu quả, cho phép các mô hình áp dụng kiến thức học được từ hình ảnh và văn bản trong một domain sang domain khác. Tuy nhiên, việc chuyển giao kiến thức giữa các graph domains khác nhau rất khó khăn vì các semantic spaces không được căn chỉnh [125] và các structural patterns cũng không tương tự [122], khiến graph domain adaptation vẫn là một vấn đề AGI rất tiên tiến và chưa được giải quyết tốt. Hiện tại, hầu hết nghiên cứu đồ thị về transfer learning tập trung vào vấn đề thứ ba, làm thế nào để tận dụng kiến thức đồ thị pre-trained trong cùng graph domain để thực hiện các graph tasks khác nhau (như node classification, link prediction, graph classification, v.v.) [78,52,80,12,124,31,74,17]. Tuy nhiên, so với thành công to lớn trong NLP và CV, việc chuyển giao task trong cùng graph domain vẫn còn sơ khai với ít trường hợp ứng dụng công nghiệp thành công hơn. Trong khi nghiên cứu AGI tự hào với những thành tựu đáng chú ý trong nhiều dữ liệu tuyến tính như hình ảnh, văn bản [67,9,2], và video [91,114], các vấn đề cơ bản trong lĩnh vực dữ liệu đồ thị vẫn chưa được khám phá kỹ. Bên cạnh ba vấn đề nền tảng trên, Trí Tuệ Nhân Tạo Tổng Quát cũng đã gặp phải nhiều tranh cãi xã hội. Ví dụ, việc đào tạo các mô hình nền tảng lớn tiêu thụ một lượng năng lượng cực kỳ lớn và có thể tạo ra những kết quả counterfactual không mong muốn [51,71]. Những lo ngại này đã dẫn đến một sự đồng thuận ngày càng tăng trong cộng đồng AI về việc khai thác hiệu quả kiến thức hữu ích được bảo tồn bởi những mô hình lớn này, giảm thiểu nhu cầu fine-tuning lặp đi lặp lại qua các downstream tasks khác nhau [16,40]. Sự đồng thuận này không chỉ hứa hẹn giảm thiểu tác động môi trường mà còn đưa ra một giải pháp thực tế cho thách thức về hiệu quả và khả năng thích ứng của mô hình trong thời đại AGI.

Ở trung tâm của công nghệ AGI gần đây, prompt learning đã thể hiện tiềm năng to lớn để giải quyết các vấn đề trên và đã chứng minh thành công đáng kể trong các ứng dụng NLP và CV [65,86,50]. Prompt learning là nghệ thuật thiết kế các prompts thông tin để thao tác dữ liệu đầu vào cho các mô hình nền tảng pre-trained. Hình 2 cho thấy một ví dụ về textual-format prompt được áp dụng cho một mô hình ngôn ngữ pre-trained để trực tiếp thực hiện các nhiệm vụ suy luận downstream. Bằng cách tái cấu trúc các downstream tasks thành pre-training tasks, cách tiếp cận này tránh được nhu cầu điều chỉnh mô hình rộng rãi và khai thác kiến thức được bảo tồn một cách hiệu quả [2,35]. Vì khả năng mạnh mẽ trong thao tác dữ liệu, tái cấu trúc task, và khai thác những hiểu biết quan trọng, prompting rất có triển vọng để giải quyết các thách thức cross-modalities, cross-domains, và cross-task đã đề cập theo một cách. So với các mô hình lớn, prompt thường rất nhẹ và có thể khai thác kiến thức hữu ích một cách hiệu quả bằng cách giảm các tài nguyên tính toán rộng rãi gây ra bởi việc điều chỉnh lặp đi lặp lại những mô hình lớn này [40,73]. Trực quan, văn bản và hình ảnh có thể được nhận thức như những trường hợp cụ thể của cấu trúc dữ liệu đồ thị tổng quát hơn. Ví dụ, một câu có thể được coi như một đường dẫn đồ thị, với các từ như các nodes, và một hình ảnh có thể được xem như một lưới đồ thị, nơi mỗi pixel phục vụ như một graph node. Hiểu biết này khuyến khích chúng ta khám phá việc chuyển giao các kỹ thuật prompting thành công từ văn bản sang lĩnh vực đồ thị cho những mối quan tâm tương tự.

Gần đây, một số nhà nghiên cứu đã bắt đầu giới thiệu prompt learning vào dữ liệu đồ thị [78,52,80,12,124,55,20,4,18]. Tuy nhiên, một số nghiên cứu sâu hơn đã phát hiện rằng graph prompt rất khác so với các đối tác của chúng trong lĩnh vực NLP [80]. Thứ nhất, việc thiết kế graph prompts chứng minh là một nỗ lực phức tạp hơn nhiều so với việc công thức hóa language prompts. Các language prompts cổ điển thường bao gồm các cụm từ được định nghĩa trước hoặc các vector có thể học được được thêm vào văn bản đầu vào [2,16]. Ở đây, trọng tâm chính nằm trong nội dung của language prompt. Tuy nhiên, chúng ta thực sự không biết graph prompt trông như thế nào. Một graph prompt không chỉ chứa "nội dung" prompt mà còn bao gồm nhiệm vụ chưa được định nghĩa để xác định cách cấu trúc những prompt tokens này và tích hợp chúng một cách liền mạch vào đồ thị gốc. Thứ hai, việc hài hòa các vấn đề đồ thị downstream với pre-training task khó khăn hơn so với language tasks [52,80]. Ví dụ, một cách tiếp cận pre-training điển hình cho một mô hình ngôn ngữ là dự đoán một từ bị che khuất bởi mô hình [9]. Sau đó nhiều downstream tasks như question answering, và sentiment classification có thể dễ dàng được tái cấu trúc như word-level tasks [50]. Không giống như NLP, nơi pre-training tasks thường chia sẻ một task sub-space đáng kể, graph tasks trải dài qua các mục tiêu node-level [19], edge-level [117], và graph-level [76,79], khiến pre-training pretexts kém thích ứng hơn. Thứ ba, so với prompts trong NLP thường là một số cụm từ có thể hiểu được, graph prompts thường ít trực quan hơn đối với những người không chuyên. Bản chất cơ bản và vai trò mà graph prompts đóng trong graph model vẫn còn khá mơ hồ mà không có phân tích lý thuyết toàn diện. Cũng thiếu các tiêu chí đánh giá rõ ràng cho chất lượng của graph prompts được thiết kế. Ngoài ra, vẫn có nhiều câu hỏi chưa rõ ràng cho chúng ta để hiểu thêm về graph prompting. Ví dụ, những graph prompts này hiệu quả như thế nào? Hiệu quả của chúng về độ phức tạp tham số và gánh nặng đào tạo ra sao? Những prompts này thao tác dữ liệu đồ thị gốc mạnh mẽ và linh hoạt như thế nào? Dưới ánh sáng của những câu hỏi phức tạp này, có một nhu cầu cấp thiết để đi sâu hơn vào tiềm năng của graph prompts trong AGI, từ đó mở đường cho sự hiểu biết sâu sắc hơn về biên giới đang phát triển này trong bối cảnh khoa học dữ liệu rộng lớn hơn.

Trong khi đã có những nỗ lực gần đây để khám phá graph prompting, một khung nhất quán hoặc lộ trình rõ ràng vẫn chưa có sẵn. Những nỗ lực này khác nhau đáng kể về quan điểm, phương pháp luận, và target tasks, điều này tạo ra một bối cảnh phân mảnh của graph prompting và gây ra một trở ngại đáng kể cho sự tiến bộ có hệ thống của lĩnh vực nghiên cứu này. Nảy sinh một nhu cầu cấp thiết để cung cấp một cái nhìn toàn cảnh, phân tích, và tổng hợp những tiến bộ mới nhất trong lĩnh vực này với một khung thống nhất. Dưới ánh sáng của tình hình này, chúng tôi đưa ra khảo sát này để trình bày cách các công trình hiện có về graph prompts cố gắng giải quyết ba vấn đề nền tảng hướng tới AGI như đã đề cập trước đó. Ngoài ra, chúng tôi cũng muốn thúc đẩy lĩnh vực nghiên cứu bằng cách trả lời các câu hỏi nghiên cứu chi tiết sau đây (RQs):

•RQ1: Làm thế nào để hiểu công trình hiện có với một khung thống nhất vì chúng rất khác nhau? Trọng tâm chính của chúng tôi là hiểu các phương pháp khác nhau được sử dụng trong lĩnh vực graph prompting. Chúng tôi muốn tập hợp tất cả những cách tiếp cận và ý tưởng khác nhau này để tạo ra một khung duy nhất, gắn kết. Khung này sẽ giúp chúng ta nắm bắt kỹ lưỡng nghiên cứu hiện có và cung cấp một nền tảng vững chắc cho nghiên cứu tương lai trong lĩnh vực này.

•RQ2: Tại sao Prompt? Bản chất của Graph Prompt là gì? Trong phần này của nghiên cứu, chúng tôi nhằm hiểu tại sao prompts quan trọng. Chúng tôi muốn khám phá những khía cạnh cơ bản của graph prompts. Graph prompts chính xác làm gì trong các vấn đề đồ thị? Chúng phù hợp như thế nào với các đồ thị phức tạp, và chúng giúp chúng ta đạt được mục tiêu rộng lớn hơn là tạo ra các hệ thống AI có thể xử lý dữ liệu đồ thị hiệu quả như thế nào? Những câu hỏi này làm nổi bật vai trò quan trọng mà graph prompts đóng trong việc định hình tương lai của AI khi xử lý thông tin đồ thị.

•RQ3: Làm thế nào để Thiết kế Graph Prompts? Thiết kế graph prompts tốt là một nhiệm vụ phức tạp. Trong phần này, chúng tôi khám phá các chi tiết kỹ thuật của việc thiết kế graph prompts: chúng trông như thế nào, chúng căn chỉnh downstream tasks và pre-train task như thế nào, và chúng được học như thế nào? Những câu hỏi quan trọng này tập trung vào kỹ năng tạo ra prompts hoạt động tốt với sự phức tạp của dữ liệu đồ thị, giúp các nhà nghiên cứu tạo ra prompts tốt hơn.

•RQ4: Làm thế nào để Triển khai Graph Prompts trong Các Ứng dụng Thực tế? Hiện tại, không có một toolkit dễ sử dụng để tạo graph prompts. Các ứng dụng tiềm năng mà graph prompts có thể được triển khai đang được khám phá dưới mức. Câu hỏi nghiên cứu này tập trung vào việc làm cho graph prompts thực tế để sử dụng trong các tình huống thực tế với một gói lập trình dễ mở rộng.

•RQ5: Những Thách thức Hiện tại và Hướng Tương lai trong Graph Prompting là gì? Câu hỏi này hướng dẫn chúng ta nhìn vào những thách thức chúng ta đang đối phó ngày nay và con đường phía trước. Bằng cách giải quyết những câu hỏi quan trọng này, chúng tôi hy vọng cung cấp một lộ trình cho nghiên cứu graph-prompting đang diễn ra.

Để trả lời câu hỏi nghiên cứu đầu tiên (RQ1), chúng tôi đề xuất một khung thống nhất để phân tích công trình graph prompt learning. Khung của chúng tôi đưa khái niệm graph prompt vào prompt tokens, token structures, và inserting patterns. Góc nhìn cấp cao hơn này mang lại sự rõ ràng và toàn diện, cung cấp cho độc giả sự hiểu biết có cấu trúc về lĩnh vực đang nổi lên này. Theo hiểu biết tốt nhất của chúng tôi, khảo sát của chúng tôi đánh dấu đầu tiên loại này để tập hợp những khía cạnh đa diện của graph prompting trong một khung thống nhất duy nhất.

Để trả lời câu hỏi nghiên cứu thứ hai (RQ2), chúng tôi khám phá các mối tương quan giữa prompts và các mô hình đồ thị hiện có thông qua lăng kính của tính linh hoạt và khả năng biểu đạt và sau đó trình bày một góc nhìn mới và sâu sắc để khám phá bản chất của graph prompts. Không giống như hầu hết các khảo sát prompt learning trong lĩnh vực NLP [50] coi prompting như một thủ thuật để lấp đầy khoảng cách giữa pre-training tasks và downstream tasks, chúng tôi tiết lộ rằng graph prompts và graph models được kết nối với nhau ở một cấp độ sâu hơn. Góc nhìn mới này mang lại những hiểu biết vô giá về lý do tại sao prompt learning có triển vọng trong lĩnh vực đồ thị và điều gì phân biệt nó với các phương pháp fine-tuning truyền thống [30]. Theo hiểu biết của chúng tôi, đây là nỗ lực đầu tiên để cung cấp một góc nhìn sáng tỏ như vậy về graph prompting.

Để trả lời câu hỏi nghiên cứu thứ ba (RQ3), chúng tôi giới thiệu một phân loại toàn diện bao gồm hơn 100 công trình liên quan. Phân loại của chúng tôi phân tích những công trình này, phân loại chúng theo các nhiệm vụ node-level, edge-level, và graph-level, từ đó căn chỉnh chúng với bối cảnh rộng lớn hơn của pre-training task. Điều này sẽ trao quyền cho độc giả với sự hiểu biết rõ ràng hơn về các cơ chế cơ bản của prompts trong toàn bộ quy trình "pre-training and prompting".

Để trả lời câu hỏi nghiên cứu thứ tư (RQ4), chúng tôi phát triển ProG (prompt graph)1, một thư viện Python thống nhất để hỗ trợ graph prompting. Ngoài ra, chúng tôi thiết lập một trang web2 phục vụ như một kho lưu trữ cho nghiên cứu graph prompt mới nhất. Nền tảng này tuyển chọn một bộ sưu tập toàn diện các bài báo nghiên cứu, datasets chuẩn, và các implementation code dễ tiếp cận. Bằng cách cung cấp hệ sinh thái dễ tiếp cận này, chúng tôi nhằm trao quyền cho các nhà nghiên cứu và thực hành viên để thúc đẩy lĩnh vực đang nổi lên này hiệu quả hơn.

Ngoài những điều này, khảo sát của chúng tôi đi xa hơn một bước với việc giới thiệu các ứng dụng tiềm năng, phân tích sâu sắc về các thách thức hiện tại, và thảo luận về các hướng tương lai, do đó cung cấp một lộ trình toàn diện cho sự phát triển của lĩnh vực sôi động và đang phát triển này (RQ5). Những đóng góp của chúng tôi được tóm tắt như sau:

•Cho phép Phân tích Toàn diện. Chúng tôi đề xuất một khung thống nhất để phân tích công trình graph prompt learning, cung cấp cái nhìn toàn diện về prompt tokens, token structures, và inserting patterns.

•Góc nhìn Mới về Tương tác Prompt-Model. Chúng tôi đưa ra những hiểu biết mới về bản chất của graph prompts. Không giống như công trình truyền thống chỉ đơn giản coi prompts như một thủ thuật để lấp đầy khoảng cách giữa downstream tasks và pre-train task, chúng tôi khám phá các vấn đề tính linh hoạt và khả năng biểu đạt của graph models và tiên phong một góc nhìn kỹ lưỡng hơn về sự tương tác giữa prompts và các mô hình đồ thị hiện có.

•Một Phân loại Có hệ thống về Graph Prompting. Chúng tôi có hệ thống khám phá hơn một trăm công trình gần đây trong lĩnh vực graph prompting. Phân loại này không chỉ tổ chức những đóng góp này mà còn trang bị cho độc giả sự hiểu biết toàn diện về các cơ chế prompt trong quy trình "pre-training and prompting" bao quát.

•Trao quyền cho Hệ sinh thái Graph Prompting. Chúng tôi phát triển ProG, một thư viện Python hỗ trợ graph prompting, và một trang web toàn diện để thu thập nghiên cứu graph prompt mới nhất.

•Vạch ra Con đường Phía trước. Một khám phá chi tiết về các thách thức hiện tại và hướng tương lai trong lĩnh vực.

Lộ trình. Phần còn lại của khảo sát này được tổ chức như sau: chúng tôi trình bày phương pháp luận khảo sát trong phần 2, tiếp theo là các điều kiện tiên quyết trong phần 3, giới thiệu các phương pháp pre-training trong phần 4, các phương pháp prompting cho graph models trong phần 5. Chúng tôi thảo luận về các ứng dụng tiềm năng của graph prompt trong phần 7 và trình bày thư viện ProG đã phát triển trong phần 8. Trong phần 9, chúng tôi tóm tắt khảo sát với các thách thức hiện tại và hướng tương lai. Phần 10 kết thúc khảo sát và trình bày tuyên bố đóng góp của các tác giả.

2 PHƯƠNG PHÁP LUẬN KHẢO SÁT

2.1 Mục tiêu Nghiên cứu

Khảo sát này sẽ giới thiệu nghệ thuật prompting từ một bức tranh lớn của trí tuệ nhân tạo tổng quát (AGI). Chúng tôi đầu tiên trình bày ba vấn đề cơ bản hướng tới AGI trong Bảng 1. Gần đây, prompt learning đã được chứng minh là một giải pháp đầy hứa hẹn cho những vấn đề này trong nhiều dữ liệu tuyến tính như văn bản [2,16], hình ảnh [91], v.v. Tuy nhiên, liệu kỹ thuật prompt có thể vẫn giải quyết những vấn đề này trong lĩnh vực đồ thị hay không, không được thảo luận rõ ràng. Thông qua khảo sát này, chúng tôi muốn tìm hiểu cách graph prompt có khả năng giúp graph models trở nên tổng quát hơn qua các tasks và domains khác nhau, và cách nó tổng quát hóa các foundation models để tương tác với các modalities khác (ví dụ: văn bản, hình ảnh, v.v.). Ngoài các vấn đề chung của AGI trong các lĩnh vực NLP, CV, và đồ thị ở trên, graph prompting thường rất khác so với các đối tác của nó trong lĩnh vực NLP và CV, dẫn đến nhiều câu hỏi chi tiết như được hiển thị trong Bảng 1.

2.2 Phân loại

Phân loại của khảo sát này được trình bày trong Hình 3, được thiết kế phức tạp để phân loại graph prompts dựa trên các ứng dụng và chức năng cụ thể của chúng, cung cấp một cách tiếp cận có cấu trúc để hiểu vai trò của chúng trong AGI.

(1) Chiến lược Pre-training cho Graph Prompt.
Vì các kỹ thuật prompting chủ yếu tìm cách tái cấu trúc downstream tasks thành pre-training tasks, chúng được tùy chỉnh cao cho các cách tiếp cận pre-training chi tiết, do đó chúng tôi thảo luận ngắn gọn về công trình pre-training đại diện trong lĩnh vực đồ thị trước khi chính thức giới thiệu nội dung graph prompt. Chúng tôi chia các cách tiếp cận pre-training hiện có thành các chiến lược pre-training node-level, edge-level, graph-level, và multi-task. Tiếp theo, chúng tôi trình bày cách các ý tưởng prompting khác nhau tái cấu trúc các downstream tasks khác nhau thành các pre-training tasks tương ứng trong lĩnh vực đồ thị. Bằng cách xem xét lại tài liệu pre-training hiện có, chúng ta sẽ rõ ràng hơn về vai trò của graph prompts trong toàn bộ khung "pre-training and prompting".

(2) Phương pháp Prompting trong Lĩnh vực Đồ thị.
Nhằm vào ba vấn đề nền tảng đã đề cập trong mục tiêu nghiên cứu (P1-P3 trong Bảng 1), chúng tôi phân tích graph prompting từ ba khía cạnh: i. thiết kế prompt cho graph tasks (Phần 5); ii. multi-modal prompting với đồ thị (Phần 6); và iii. graph domain adaptation với kỹ thuật prompting (Phần 6.2). Trong mỗi khía cạnh, chúng tôi trình bày một cuộc thảo luận chi tiết liên quan đến năm vấn đề cụ thể trong lĩnh vực graph prompt (Q1-Q5 trong Bảng 1).

i. Thiết kế Prompt cho Graph Tasks. Trong phần này, chúng tôi đề xuất một khung thống nhất để phân tích công trình hiện có về thiết kế graph prompt. Khung của chúng tôi xử lý các graph prompts hiện có với ba thành phần chính: prompt tokens, giữ nội dung prompt như vectors; token structures, chỉ ra cách nhiều tokens được tổ chức; và inserting patterns, định nghĩa cách kết hợp graph prompt với các đồ thị gốc. Ngoài ra, chúng tôi cũng phân tích cẩn thận cách những công trình này thiết kế hàm trả lời prompt, có nghĩa là cách chúng nhận kết quả cho các downstream tasks từ prompts của chúng. Chúng tôi cũng tóm tắt ba phương pháp đại diện để học các prompts thích hợp, bao gồm các kỹ thuật meta-learning, task-specific tuning, và tuning in line with pretext. Cuối cùng, chúng tôi thảo luận thêm về những công trình này trong Phần 5.4 để thấy các kết nối nội tại với ưu và nhược điểm.

ii. Multi-modal Prompting với Đồ thị. Trong phần này, chúng tôi ngắn gọn trình bày cách graph prompts hoạt động trong text-attributed graph, có thể được xem như sự fusion của text và graph modalities. Với tiến bộ của large language models (LLMs), việc fusion text và graph data đã trở nên dễ dàng hơn và đã khơi dậy nhiều công trình về chủ đề này. Vì chủ đề tích hợp LLMs với đồ thị đã được tóm tắt tốt trong [46], chúng tôi sẽ không trình bày quá nhiều trong khảo sát này. Thay vào đó, chúng tôi chỉ thảo luận ngắn gọn một số công trình đại diện trong lĩnh vực này tập trung vào lĩnh vực prompt.

iii. Graph Domain Adaptation thông qua Kỹ thuật Prompting. Trong phần này, chúng tôi giới thiệu công trình liên quan từ hai nhánh. Nhánh đầu tiên trình bày các công trình giải quyết semantic alignment qua các graph domains khác nhau, và nhánh thứ hai trình bày structural alignment.

2.3 Tổng quan Tài liệu

Trong khảo sát này, chúng tôi cẩn thận nghiên cứu hơn 100 bài báo chất lượng cao được xuất bản trong 5 năm qua từ các hội nghị và tạp chí có uy tín bao gồm nhưng không giới hạn ở NeurIPS, SIGKDD, The Web Conference, ICLR, CIKM, ICML, IJCAI, EMNLP, SIGIR, ACL, AAAI, WSDM, TKDE, v.v. Hầu hết những venues này được xếp hạng là CCF A3 hoặc CORA A*4. Bên cạnh những công trình này, chúng tôi cũng giới thiệu một số công trình mới nhất trong arXiv để khảo sát của chúng tôi có thể bắt kịp với biên giới và tiến bộ mới nhất trong lĩnh vực này. Một biểu đồ tròn chi tiết hơn (Hình 4a) trình bày phân bố của các bài báo được thu thập qua những venues này. Hơn nữa, chúng tôi thực hiện phân tích về các chủ đề được bao phủ bởi những tài liệu tham khảo này. Trong Hình 4b, chúng tôi trình bày 15 từ khóa hàng đầu xuất hiện trong tiêu đề của những bài báo này. Đáng chú ý, những từ khóa này căn chỉnh chặt chẽ với trọng tâm của khảo sát chúng tôi, tập trung xung quanh graph domains và prompt learning.

Kết nối với Công trình Hiện có:
Khảo sát của chúng tôi nổi bật so với các khảo sát hiện có trong một số cách đáng chú ý. Ví dụ, Liu et al. [48] chủ yếu tập trung vào graph foundation models (GFMs). Khảo sát của họ không nhắm cụ thể đến graph prompts, và chỉ một vài bài báo trong lĩnh vực này được thảo luận ngắn gọn. Li et al. [46] có hệ thống phân tích các công trình gần đây tích hợp đồ thị và LLMs, đây là phân tích chi tiết của một phần nhỏ (Phần 6.1) trong khảo sát của chúng tôi. Chúng tôi vượt ra ngoài phạm vi của họ bằng cách khám phá các khía cạnh khác nhau của graph prompts theo cách rộng rãi hơn. Trong khi đó, các khảo sát [105,113] tập trung chủ yếu vào các giai đoạn pre-training, mà không liên quan đến khía cạnh quan trọng của graph prompt learning. Trong khi một khảo sát trước đó về graph prompt learning của Wu et al. [102] tồn tại, khảo sát của chúng tôi vượt qua nó trong một số khía cạnh chính. Thứ nhất, chúng tôi cung cấp phân tích toàn diện hơn về các công trình liên quan. Khảo sát của họ được xuất bản vào tháng 5 năm 2023 khi chỉ có một vài công trình graph prompt có sẵn [78,124,11]. Ngược lại, khảo sát của chúng tôi bao gồm phạm vi rộng hơn, bao gồm tất cả các công trình liên quan trong lĩnh vực. Thứ hai, chúng tôi đưa ra phân tích có hệ thống về các công trình hiện có trong một khung thống nhất, tạo điều kiện hiểu và so sánh giữa các cách tiếp cận khác nhau. Thứ ba, khảo sát của chúng tôi cung cấp những hiểu biết sâu sắc về mối quan hệ giữa graph pre-training và prompts, làm sáng tỏ sự tương tác giữa những yếu tố quan trọng này. Cuối cùng, chúng tôi không chỉ trình bày những hiểu biết thực nghiệm mà còn bao gồm các công trình kỹ thuật nhằm triển khai graph prompts trong các ứng dụng thực tế, đảm bảo tính ứng dụng thực tế của khảo sát chúng tôi.

3 ĐIỀU KIỆN TIÊN QUYẾT

Graph representation learning đã là chủ đề nghiên cứu rộng rãi trong vài thập kỷ qua. Hành trình này, được minh họa trong Hình 5, đã chứng kiến sự phát triển từ các phương pháp embedding nông đến các graph neural networks có giám sát, chuyển từ paradigm fine-tuning sang paradigm prompting mới nổi. Trong phần này, chúng tôi sẽ cung cấp tổng quan về các ký hiệu cơ bản được sử dụng trong khảo sát này, đi sâu vào các phát triển lịch sử của graph representation learning, khám phá paradigm pre-training và fine-tuning, và theo dõi sự phát triển của prompt-based learning. Quan trọng nhất, chúng tôi sẽ trình bày một góc nhìn mới tập trung vào tính linh hoạt và khả năng biểu đạt, làm sáng tỏ lý do tại sao prompts cung cấp một giải pháp đầy hứa hẹn để giải quyết các hạn chế của các phương pháp graph representation learning hiện có.

3.1 Ký hiệu

Gọi một graph instance được ký hiệu là G={V,E}, trong đó V={v1, v2, . . . , vN} đại diện cho tập node chứa N nodes. Tập edge E ∈ V × V mô tả kết nối giữa các nodes. Mỗi node vi được liên kết với một feature vector được biểu diễn như xi∈RD. Để đặc trưng cho tính kết nối trong đồ thị, chúng tôi sử dụng ma trận kề được ký hiệu là A∈ {0,1}N×N, trong đó entry Aij= 1 khi và chỉ khi edge (vi, vj)∈ E.

3.2 Graph Representation Learning

Những thập kỷ gần đây đã chứng kiến sự gia tăng đáng chú ý trong việc phát triển các kỹ thuật graph representation learning. Những cách tiếp cận này có thể được phân loại rộng rãi thành hai nhánh chính: shallow embedding methods và deep graph neural networks (GNNs). Cách tiếp cận shallow embedding tập trung vào việc ánh xạ các nodes vào các embeddings có thể học được ở chiều thấp hơn, tăng cường khả năng ứng dụng của chúng trong các downstream tasks khác nhau, như được minh họa bởi node2vec [19] và DeepWalk [63]. Mặt khác, deep GNNs duy trì các node features đầu vào như hằng số và tối ưu hóa các tham số mô hình đồ thị sâu cho các tasks cụ thể, dẫn đến khả năng biểu diễn biểu đạt hơn, như thấy trong các phương pháp như Graph Convolution Networks (GCN) [56] và GraphSAGE [21].

Các cách tiếp cận shallow embedding làm cho các node features đầu vào trở thành tham số có thể học được, nhằm mã hóa các nodes theo cách giữ lại cấu trúc tương tự của mạng gốc. Theo định nghĩa tương tự node, những phương pháp này có thể được phân loại như factorization-based [57,116] và random walk approaches [19,63,94,93]. Mặc dù tính linh hoạt mà các phương pháp shallow embedding cung cấp cho các downstream tasks khác nhau, chúng bị hạn chế bởi không khả năng tạo ra embeddings cho các nodes không gặp phải trong quá trình đào tạo. Ngoài ra, những cách tiếp cận này thiếu khả năng tích hợp node features. Do đó, các phương pháp "sâu" hơn, cụ thể là những phương pháp dựa trên graph neural networks, đã được phát triển để giải quyết những hạn chế này.

Hầu hết deep GNNs theo một lược đồ message-passing và sử dụng một encoder phức tạp hơn, dẫn đến khả năng biểu đạt mạnh mẽ trong graph representation. Cấu trúc neural network đại diện là convolutional graph neural networks (ConvGNNs), bao gồm các phương pháp spectral [8,26] và spatial [56,21,87,72,107]. Trong khi những phương pháp này đã thể hiện khả năng đáng kể trong các ứng dụng dựa trên đồ thị khác nhau, sự phụ thuộc của chúng vào task-specific supervision áp đặt các ràng buộc về khả năng thích ứng và tổng quát hóa, đặc biệt khi xử lý các tasks có dữ liệu được gán nhãn hạn chế.

Tóm lại, các phương pháp shallow embedding cung cấp tính linh hoạt, bảo tồn cấu trúc mạng và nội dung node cho các nhiệm vụ phân tích đồ thị đơn giản. Tuy nhiên, chúng thiếu khả năng biểu đạt và khả năng bao gồm các node features bổ sung. Ngược lại, GNNs cung cấp các graph representations biểu đạt hơn nhưng yêu cầu dữ liệu đào tạo task-specific, hạn chế khả năng chuyển giao của chúng. Do đó, nó kêu gọi một cơ chế graph learning kết hợp khả năng biểu đạt và tính linh hoạt. Nhu cầu này dẫn đến sự phát triển của paradigm pre-training và fine-tuning.

3.3 Pre-training và Fine-tuning

Để giải quyết các thách thức về dữ liệu được gán nhãn hạn chế và các vấn đề tổng quát hóa trong GNNs, paradigm pre-training và fine-tuning, phát triển mạnh trong cộng đồng xử lý ngôn ngữ tự nhiên, đã được áp dụng rộng rãi trong graph representation learning. Những cách tiếp cận này liên quan đến việc pre-training các mô hình trên dữ liệu đồ thị quy mô lớn, có hoặc không có nhãn, tiếp theo là fine-tuning các tham số mô hình cho các downstream tasks đa dạng. Quá trình hai bước này cải thiện khởi tạo mô hình, tạo ra optima rộng hơn và tăng cường tổng quát hóa so với đào tạo từ đầu. Các lược đồ pre-training thường được sử dụng bao gồm Graph AutoEncoders (GAEs) [89], Masked Components Modeling (MCM) [30,68], Graph Contrastive Learning (GCL) [88,76], v.v. Trong Phần 4, chúng tôi sẽ đi sâu vào thảo luận chi tiết về phương pháp pre-training và fine-tuning, cung cấp một bức tranh toàn diện.

3.4 Lịch sử Ngắn gọn của Prompt Learning

Do số lượng tham số mô hình ngày càng tăng, quá trình pre-training và fine-tuning thông thường đang phát triển thành một cách tiếp cận mới được gọi là pre-training, prompting, và predicting [50]. Trong paradigm này, thay vì thích ứng mô hình pre-trained một cách thủ công cho các downstream tasks cụ thể, những tasks này được tái cấu trúc để giống với những tasks được giải quyết trong giai đoạn pre-training, được hỗ trợ bởi prompts. Prompts trong NLP có nhiều hình dạng khác nhau, bao gồm cloze prompts, hoàn thành các chuỗi văn bản như những chuỗi được sử dụng trong masked language models, và prefix prompts [40,43], nơi văn bản đầu vào đi trước answer slot, như được sử dụng bởi autoregressive language models. Một số nghiên cứu liên quan đến templates được thiết kế thủ công dựa trên hiểu biết của con người [2,70,71], trong khi những nghiên cứu khác khám phá automated template learning. Điều này bao gồm tìm kiếm templates trong không gian rời rạc [35,25,73,16] hoặc thực hiện prompting trực tiếp trong embedding space [43,40,86,65]. Paradigm như vậy cho phép một mô hình pre-trained duy nhất giải quyết vô số downstream tasks theo cách không giám sát, điều này đã được chứng minh rộng rãi bởi large language models. Dưới ánh sáng này, việc ứng dụng các kỹ thuật prompting trong bối cảnh các graph-based tasks hiện tại là một lĩnh vực khám phá tích cực.

3.5 Tại sao Prompt? Một Góc nhìn Mới về Tính linh hoạt và Khả năng biểu đạt.

Tại sao prompt learning có triển vọng cho graph domain? Một góc nhìn hiện có xuất hiện trong hầu hết công trình liên quan là prompt có thể tái cấu trúc downstream tasks thành pre-training task, có thể lấp đầy khoảng cách giữa chúng. Góc nhìn này tốt nhưng vẫn chưa đủ sâu sắc để thấy sự khác biệt nội tại so với fine-tuning truyền thống. Ví dụ, trong một góc nhìn tương tự, pre-training và fine-tuning có thể được coi như sử dụng fine-tuning để tái cấu trúc pre-training task thành downstream task. Có vẻ như hai quy trình kỹ thuật này đều có thể giải quyết cùng một vấn đề. Tại sao lựa chọn đầu tiên lại tốt hơn lựa chọn thứ hai?

Trong phần này, chúng tôi đề xuất một góc nhìn mới, từ góc nhìn này, chúng ta có thể thấy thêm sự khác biệt giữa prompting và fine-tuning. Như đã thảo luận trong các phần trước, các phương pháp graph representation learning hiện có không thể đạt được sự cân bằng thỏa đáng giữa khả năng biểu đạt và tính linh hoạt. Các cách tiếp cận shallow graph embedding cung cấp tính linh hoạt vì chúng có thể được áp dụng cho một loạt rộng các downstream tasks. Tuy nhiên, chúng hy sinh khả năng biểu đạt do parameterization hạn chế và không thể tích hợp các node features gốc. Lấy mô hình DeepWalk làm ví dụ, các phương pháp đồ thị nông thường coi node representations như tham số tự do, điều này rất linh hoạt vì mỗi node có thể học các representations riêng lẻ của nó một cách độc lập. Tuy nhiên, vì lý do gradient, chúng không thể dựa vào các mạng phức tạp hơn sau này, có thể mất một số khả năng biểu đạt. Thực tế, có nhiều công trình tiên tiến hơn với các graph layers sâu sử dụng node representations từ DeepWalk như input features của chúng vì những node representations này rất tổng quát trong các tasks khác nhau. Mặt khác, các phương pháp dựa trên GNN coi node embedding như constant features và tìm cách tìm một mạng mạnh mẽ để ánh xạ node features thành một task cụ thể, điều này rất biểu đạt. Tuy nhiên, mẫu biến đổi feature đã học được áp dụng cho tất cả các nodes, có nghĩa là mô hình không thể coi mỗi node embedding như tham số tự do, và không thể đạt được kết quả linh hoạt như những cái trước đó. Khi chúng ta có nhiều tasks, chúng ta thường cần đào tạo các phiên bản khác nhau của cùng một mô hình GNN, điều này không linh hoạt như cái trước đó.

Với phân tích trên, chúng ta có thể thấy rằng fine-tuning truyền thống thực sự tìm cách cải thiện thêm khả năng biểu đạt của một task mới với mô hình đồ thị pre-trained và không thể chăm sóc tính linh hoạt của node. Không giống như fine-tuning, một graph prompt thường có một số tokens với tham số tự do, điều này rất giống với các phương pháp đồ thị nông. Trong khi đó, mỗi node trong đồ thị gốc có constant features cho các mô hình GNN. Bằng cách chèn prompt graph vào đồ thị gốc, đồ thị kết hợp có cả nodes với constant features và tokens với tham số tự do. Các tham số token có thể được điều chỉnh một cách hiệu quả, bảo tồn tính linh hoạt của node. Đồ thị kết hợp được gửi đến một mô hình GNN pre-trained đông lạnh để tận dụng khả năng biểu đạt mạnh mẽ của các mô hình đồ thị sâu.

Trong bài báo này, chúng tôi lập luận rằng cơ chế prompting cung cấp một giải pháp đầy hứa hẹn để giải quyết các hạn chế của các phương pháp graph representation learning hiện có, cân bằng hiệu quả tính linh hoạt và khả năng biểu đạt. Pre-trained GNNs vốn dĩ sở hữu kiến thức về cả khía cạnh cấu trúc và ngữ nghĩa, cho phép mức độ khả năng biểu đạt mong muốn. Bằng cách giới thiệu prompts, chúng ta có thể áp dụng liền mạch các mô hình pre-trained mạnh mẽ cho các downstream tasks đa dạng qua các domains khác nhau theo cách hiệu quả. Điều này đạt được bằng cách căn chỉnh format của downstream tasks với format của pre-trained tasks, do đó tận dụng toàn bộ tiềm năng của các mô hình pre-trained ngay cả với các tín hiệu giám sát tối thiểu. Trong khi cơ chế fine-tuning cũng có thể tạo điều kiện cho domain hoặc task adaptation của các mô hình đồ thị pre-trained, nó thường cần một lượng thông tin được gán nhãn đáng kể và yêu cầu đào tạo lại toàn diện mô hình pre-trained. Để so sánh, cơ chế prompt cung cấp mức độ tính linh hoạt và hiệu quả cao hơn.

4 PRE-TRAINING GNNS CHO GRAPH PROMPTING

Graph pre-training là một bước quan trọng của paradigm pre-training, prompting, và predicting trong graph representation learning. Cách tiếp cận này tận dụng thông tin có sẵn dễ dàng để mã hóa cấu trúc đồ thị vốn có, cung cấp một nền tảng vững chắc cho tổng quát hóa qua các downstream tasks đa dạng. Bằng cách tích hợp những phương pháp pre-training này vào quy trình làm việc toàn diện, chúng tôi đưa ra một khám phá về sự tương tác của chúng với các giai đoạn prompting và predicting tiếp theo, làm sáng tỏ điểm mạnh và hạn chế của cách tiếp cận tổng thể này. Góc nhìn độc đáo này phân biệt khảo sát của chúng tôi, đặt graph pre-training như một phần không thể thiếu của quá trình graph-prompting learning rộng lớn hơn. Để minh họa tốt hơn động lực đằng sau paradigm prompting, chúng tôi sẽ đi sâu vào bốn chiến lược pre-training riêng biệt trong khung pre-training và fine-tuning truyền thống.

4.1 Chiến lược Node-level

Các chiến lược pre-training node-level trao quyền cho việc thu thập các local structure representations có giá trị có thể được chuyển giao đến các downstream tasks. Như được hiển thị trong Hình 6, những chiến lược này bao gồm cả phương pháp contrastive và predictive learning. Trong contrastive learning, các tín hiệu self-supervised thường xuất phát từ các perturbations trong cấu trúc đồ thị gốc hoặc thuộc tính, với mục tiêu tối đa hóa Mutual Information (MI) giữa view gốc và self-supervised. Các phương pháp contrastive node-level đáng chú ý bao gồm những phương pháp được trình bày trong [6,123,37,62,33,95]. Mặt khác, các mô hình predictive tập trung vào việc tái tạo dữ liệu bị perturbation sử dụng thông tin từ dữ liệu không bị perturbation, như được chứng minh trong [21,27,89,61,92,28]. Tuy nhiên, sự nhấn mạnh của nó vào các mẫu topology ngữ nghĩa một phần hạn chế khả năng nắm bắt thông tin bậc cao hơn.

4.2 Chiến lược Edge-level

Để tăng cường hiệu suất trong các tasks như link prediction, các chiến lược pre-training edge-level đa dạng đã được phát triển. Những chiến lược này xuất sắc trong việc nắm bắt các tương tác node và đã trải qua khám phá rộng rãi. Một cách tiếp cận liên quan đến việc phân biệt sự hiện diện của edges giữa các cặp nodes, có thể được coi như các phương pháp contrastive [38,54]. Một cách tiếp cận khác tập trung vào việc tái tạo các masked edges bằng cách khôi phục ma trận kề [82,41,58,23,39]. Mặc dù chiến lược pre-training này hoạt động đáng ngưỡng mộ trong các tasks liên quan chặt chẽ đến việc dự đoán quan hệ node, nó chỉ tập trung vào các khía cạnh cấu trúc, bỏ qua việc miêu tả các thuộc tính node, và có thể gặp thách thức khi áp dụng cho các graph-level downstream tasks.

4.3 Chiến lược Graph-level

Sự cần thiết để cải thiện graph-level representations cho các subgraph-related downstream tasks đã thúc đẩy việc khám phá các chiến lược pre-training graph-level khác nhau. Tương tự như các chiến lược node- và edge-level, những cách tiếp cận này có thể được phân loại rộng rãi thành hai nhóm chính: các phương pháp graph reconstruction, liên quan đến việc masking các thành phần đồ thị và khôi phục tiếp theo của chúng [104,68], và các phương pháp contrastive tập trung vào việc tối đa hóa mutual information. Những phương pháp contrastive này nhắm mục tiêu hoặc các local patches của node features và global graph features [88,76,24,77,75], hoặc các cặp positive và negative của đồ thị [111,81,84,66]. Trong khi những cách tiếp cận này mã hóa hiệu quả thông tin toàn cục và tạo ra các graph-level representations có giá trị, một thách thức đáng kể nằm trong việc chuyển giao kiến thức từ một pretext task cụ thể đến các downstream tasks với khoảng cách đáng kể, có khả năng dẫn đến negative transfer [69]. Điều này có thể hạn chế khả năng ứng dụng và độ tin cậy của các mô hình pre-trained và có khả năng tạo ra kết quả kém thuận lợi hơn, thậm chí tệ hơn so với học từ đầu.

4.4 Multi-task Pre-training

Multi-task pre-training chứa nhiều mục tiêu tối ưu hóa, giải quyết một phổ rộng các khía cạnh liên quan đến đồ thị để tăng cường tổng quát hóa trong khi giảm thiểu các vấn đề negative transfer. Những mục tiêu này có thể bao gồm các kết hợp khác nhau, như đào tạo đồng thời của node attribution reconstruction và structural recovery [30,31,115,13]. Ví dụ, Hu et al. [30] pre-trained một GNN ở cả node và graph levels, cho phép GNN học các representations local và global có giá trị đồng thời. Hơn nữa, một số công trình đã sử dụng contrastive learning ở các levels khác nhau [34,106], hoặc tối ưu hóa chung của cả contrastive loss và graph reconstruction error [42]. Tuy nhiên, điều quan trọng là nhận ra rằng các cách tiếp cận multi-task pre-training có thể đối mặt với thách thức tối ưu hóa, có khả năng dẫn đến hiệu suất dưới tối ưu qua các tasks. Do đó, tối ưu hóa hiệu suất mô hình cho mỗi task trong khi giảm thiểu vấn đề negative transfer vẫn là một mối quan tâm đáng kể nhưng chưa được giải quyết.

4.5 Thảo luận Thêm

May mắn thay, paradigm prompting và predicting cung cấp một giải pháp mạnh mẽ cho các thách thức đã đề cập ở trên. Cách tiếp cận này có thể khai thác đầy đủ hiệu suất mô hình và tích hợp liền mạch với các kiến trúc GNN tiên tiến. Thay vì thích ứng các GNNs pre-trained cho các downstream tasks thông qua objective engineering, paradigm này tái cấu trúc các downstream tasks thành những tasks được giải quyết trong giai đoạn pre-training sử dụng một graph prompt. Chiến lược sáng tạo này hiệu quả kết nối khoảng cách giữa pretext và downstream tasks trong khi tránh những cạm bẫy hiệu suất dưới tối ưu. Hơn nữa, so với các cách tiếp cận fine-tuning truyền thống, paradigm prompting thể hiện tính linh hoạt đáng kể, cho phép nó xuất sắc trong các tình huống đòi hỏi few-shot hoặc thậm chí zero-shot learning, nơi việc thích ứng với các bối cảnh mới với dữ liệu được gán nhãn hạn chế hoặc không có là tối quan trọng. Trong bối cảnh hiện tại được đánh dấu bởi sự gia tăng khối lượng mô hình và một mảng ngày càng mở rộng của các downstream tasks, sự lên cao của paradigm prompting đại diện cho một xu hướng không thể chống lại và biến đổi.

5 THIẾT KẾ PROMPT CHO GRAPH TASKS

Trong phần này, chúng tôi đề xuất một cái nhìn thống nhất cho graph prompt. Như được hiển thị trong Hình 7, graph prompt nên chứa ít nhất ba thành phần: prompt tokens với prompt vector; token structures bảo tồn các mối tương quan bên trong của những tokens này; và inserting patterns chỉ ra cách tích hợp đồ thị gốc với prompts. Ngoài những chi tiết này, chúng tôi đặc biệt quan tâm đến những câu hỏi sau: Câu hỏi 1: Những công trình này thiết kế graph prompt như thế nào? Câu hỏi 2: Những công trình này tái cấu trúc downstream tasks thành pre-training tasks như thế nào? Câu hỏi 3: Những công trình này học một prompt hiệu quả như thế nào? và Câu hỏi 4: Những kết nối bên trong của những công trình này, ưu điểm và hạn chế của chúng là gì? Với những câu hỏi này, chúng tôi tóm tắt các công trình đại diện nhất được xuất bản gần đây và trình bày chúng trong Bảng 2.

5.1 Prompt Token, Structure, và Inserting Pattern

Câu hỏi 1: Họ Thiết kế Một Graph Prompt Như thế nào?

A. Prompt như Tokens. Graph prompt đơn giản nhất có thể được coi như một số features bổ sung được thêm vào các graph features gốc [125]. Ví dụ, cho một ma trận graph feature X={x1,···, xN} ∈RN×d trong đó xi∈R1×d là feature của node thứ i và d là chiều của feature space. Fang et al. [11] và Shirkavand và Huang [74] coi basic prompt như một vector có thể học được p∈R1×d, có thể được thêm vào tất cả node features và làm cho ma trận feature được thao tác là X∗={x1+p,···, xN+p}. Theo cách này, chúng ta có thể sử dụng các features được tái cấu trúc để thay thế các features gốc và xử lý đồ thị với các mô hình đồ thị pre-trained. Một công trình sau này [12] mở rộng thêm một prompt token thành nhiều tokens và làm cho hiệu suất tốt hơn. PGCL [18] thiết kế một prompt vector cho semantic view và một prompt vector khác cho contextual view, sau đó họ thêm những prompt vectors này vào các graph-level representations bằng element-wise multiplication. Một thiết kế prompt tương tự cũng được áp dụng trong VNT [83]. Sự khác biệt là inserting pattern của chúng không thêm prompt token vào graph feature gốc mà concatenates prompt token với tập node gốc và cố gắng tích hợp chúng bằng hàm self-attention trong một graph transformer. Trong GraphPrompt [52], prompt token tương tự như format được định nghĩa bởi Fang et al. [11]. Điều khác biệt là các công trình trước đây thiết kế các prompt tokens trong initial feature space, trong khi phương pháp này giả định prompt trong hidden layer của graph model. Thường thì hidden size sẽ nhỏ hơn feature gốc, làm cho prompt token của chúng ngắn hơn cái trước đó. Một điều khác biệt khác là graph prompt ở đây được sử dụng để hỗ trợ graph pooling operation (a.k.a Readout). Ví dụ, cho tập node V={v1,···, v|V|}, embedding của node vi là hv∈R1×d, một prompt token pt∈R1×d được chỉ định cho task t được chèn vào graph nodes bằng element-wise multiplication (⊗): st=Readout ({pt⊗hv:v∈ V}). Tương tự, SGL-PT [124] tạo một prompt token để kết nối với tất cả nodes trong đồ thị. Prompt token bảo tồn một nhận thức toàn cục về đồ thị và có thể hỗ trợ nhánh toàn cục của các pre-training tasks, cũng có thể được coi như một pooling strategy. Nhằm căn chỉnh node classification và link prediction, GPPT [78] định nghĩa graph prompts như các tokens bổ sung chứa task tokens và structure tokens. Ở đây task token đề cập đến mô tả của downstream node label cần được phân loại và structure token là representation của subgraph xung quanh target node. Bằng cách này, việc dự đoán node label có thể được tái cấu trúc để dự đoán một liên kết tiềm năng giữa structure token của node v và label task token. Nhằm vào node classification task, ULTRA-DP [4] tạo một prompt token cho mỗi target node, nơi token feature là tổng có trọng số của position embedding của target node và một task embedding của pre-training task. HetGPT [55] thiết kế một prompt với node tokens và class tokens, được tổ chức theo cách tương tự như GPPT. Sự khác biệt là họ cũng thêm một type-specific feature token để làm cho graph prompts nhạy cảm với các node types khác nhau, bằng cách đó họ có thể mở rộng các graph prompts hiện có đến heterogeneous graphs.

B. Prompt như Graphs. Graph prompt trong All in One [80] là một subgraph bổ sung có thể được học bằng efficient tuning. Các prompt tokens là một số nodes bổ sung có cùng kích thước node representation như các nodes gốc. Họ giả định rằng các prompt tokens nên ở trong cùng semantic space như các node features gốc để chúng ta có thể dễ dàng thao tác node features với những tokens này. Các token structures bao gồm hai phần. Phần đầu tiên là các inner links giữa các tokens khác nhau, và phần thứ hai là các cross-links giữa prompt graph và đồ thị gốc. Những links này có thể được tính toán trước bằng dot product giữa một token với token khác (inner links) hoặc một token với node gốc khác (cross links). Inserting pattern là thêm prompt graph vào đồ thị gốc bằng cross-links và sau đó coi đồ thị kết hợp như một đồ thị mới, và gửi nó đến mô hình đồ thị pre-trained để lấy graph-level representation. Prompt graph trong PRODIGY [32] bao gồm data graphs và một task graph. Data graphs có thể được coi như subgraphs xung quanh các target nodes (cho node classification task), node pairs (cho edge classification task), hoặc chỉ được ký hiệu như graph classification instance. Ở đây các prompt tokens và prompt structures giống như trong đồ thị gốc. Task graph chứa data tokens và label tokens nơi mỗi data token kết nối với một data graph và được kết nối thêm bởi label tokens. Không giống như các công trình trước đây nhằm tái cấu trúc downstream tasks thành pre-training task bằng cách prompting downstream data, PRODIGY tận dụng prompt graph để thống nhất tất cả các upstream và downstream tasks. Chiến lược pre-training của họ là một tập hợp các tasks bao gồm neighboring matching và label matching, có thể được tái cấu trúc như dự đoán sự tương tự giữa data token và label token trong prompt graph. SAP [17] cũng kết nối một số prompt tokens (mỗi token tương ứng với một class) với đồ thị gốc bằng cross-links được định nghĩa trong All in One. Sự khác biệt là prompting task của họ là một node-level contrastive task, trong đó họ sử dụng MLP để mã hóa node features như view đầu tiên và họ sử dụng một GNN để mã hóa prompted graph như view thứ hai, phù hợp với pre-training task của họ.

5.2 Căn chỉnh Tasks bằng Answering Function

Câu hỏi 2: Họ tái cấu trúc downstream tasks thành pretext như thế nào?

A. Xử lý Different Level Tasks. All in One [80] pre-trains graph model thông qua graph-level contrastive learning. Pre-training task nhằm học một graph encoder mạnh mẽ qua các graph views khác nhau được tạo ra bởi các perturbations của đồ thị gốc. Để tái cấu trúc các graph tasks khác nhau thành graph-level pretext này, họ đầu tiên thống nhất các node-level, edge-level, và graph-level tasks thành graph-level task bằng induced subgraphs, cũng được giới thiệu trong PRODIGY [32,52,18]. Sau đó họ tuyên bố rằng graph prompt được thêm vào đồ thị gốc về bản chất là mô phỏng của bất kỳ graph operations nào như node feature masking, node hoặc edge perturbations, subgraph removing, v.v. Với ý kiến này, chúng ta chỉ cần thêm một graph prompt thích hợp vào downstream graph datasets thì nó sẽ có triển vọng để tái cấu trúc thêm downstream task thành pretext.

B. Learnable Answering Function. Để xuất kết quả của downstream tasks, Sun et al. [80] thiết kế hai loại answering functions. Loại đầu tiên là một learnable task head (như một hàm ánh xạ MLP) có thể dễ dàng được điều chỉnh với dữ liệu rất hạn chế. Nó lấy graph-level representation được tạo ra bởi mô hình đồ thị pre-trained và sau đó xuất kết quả downstream. Ví dụ, nếu downstream là một three-class node classification, chúng ta có thể đơn giản sử dụng một dense layer với ba hidden units để kết nối graph representation, được tạo ra bởi mô hình pre-trained trên đồ thị kết hợp với đồ thị bao gồm node và một graph prompt. Trong trường hợp này, cả graph prompt và task head đều có thể điều chỉnh được, vì vậy chúng ta có thể điều chỉnh chúng xen kẽ. Các learnable answering functions tương tự cũng được áp dụng trong các công trình khác như [11,12,83]. Điểm tốt là chúng rất dễ căn chỉnh hai tasks, tuy nhiên, nó cũng tăng quy trình tuning.

C. Hand-crafted Answering Function. Để giảm thêm gánh nặng tuning, All in One [80] cũng đề xuất một answering function thứ hai, được hand-crafted mà không có bất kỳ trainable task head nào. Ví dụ, cho một node classification task, chúng ta có thể thiết lập K sub-prompts độc đáo, mỗi cái căn chỉnh với một node type khác nhau, nơi K đại diện cho tổng số các node categories. Nếu pre-training liên quan đến một task như GraphCL [111], nhằm tối đa hóa sự tương tự ở graph level giữa các cặp graph views, thì target node có thể được phân loại với label ℓ,(ℓ= 1,2,···, K) nếu ℓ-th graph giống nhất với đồ thị gồm node gốc. Tương tự, GPPT [78] và HetGPT [55] sử dụng link prediction như pre-training task của họ và tái cấu trúc downstream node classification bằng cách thống nhất nó như cùng task template. Ví dụ, bằng cách coi node label như một token bổ sung, chúng ta có thể sử dụng mô hình pre-trained để trực tiếp xuất khả năng của một edge giữa label token và target node. Chiến lược pre-training trong SAP [17] là so sánh node representations từ hai graph views, cái đầu tiên được tạo ra bởi node feature encoding và cái thứ hai được mã hóa bởi một graph model. Để kết thúc này, các prompt tokens của họ biểu thị class information và họ so sánh node representation với mỗi class token để tìm class có sự tương tự lớn nhất như kết quả suy luận. Bằng cách thiết kế một task template thống nhất, PRODIGY [32] sử dụng một hand-crafted graph prompt để mô tả tất cả các node, edge, và graph classification tasks bằng cách dự đoán liên kết giữa data tokens và label tokens. Liu et al. [52] mở rộng link prediction task như graph pair similarity và coi nó như pre-training task của họ, để căn chỉnh downstream node classification và graph classification task, họ thiết kế một answering template thống nhất làm cho downstream side căn chỉnh với pre-training side. Cụ thể, cho một triplet của induced graphs (g1, g2, g3) nơi g1 và g2 có cùng label, g1 và g3 có labels khác nhau. Đặc biệt, khi target task là node classification, induced graph đề cập đến contextual subgraph của target node. Answering template thống nhất được định nghĩa là sim (g1, g2)>sim(g1, g3).

5.3 Prompt Tuning

Câu hỏi 3: Họ Học Một Graph Prompt Như thế nào?

A. Meta-Learning Technique. Để học các prompts thích hợp, Sun et al. [80] tận dụng các kỹ thuật meta-learning (như mô hình MAML [15]) để có được một điểm khởi đầu mạnh mẽ cho các tham số prompt. Vì support set và query set bao gồm các graph tasks khác nhau (như node classification, link prediction, graph classification, v.v.), graph prompt đã học được kỳ vọng sẽ tổng quát hơn trên các downstream tasks khác nhau.

B. Task-specific Tuning. Bên cạnh All in One [80], nhằm học một prompt tổng quát trên các downstream tasks khác nhau, cũng có một số công trình nhắm vào các downstream tasks cụ thể như graph classification. Trong trường hợp này, prompt tuning có thể hướng task hơn. Ví dụ, GPF [11] nhằm vào một graph classification task, vì vậy nó chỉ cần điều chỉnh prompt token p và task head ϕ bằng cách tối đa hóa likelihood của việc dự đoán đúng graph labels cho prompted graph representation ˜si từ mô hình đồ thị pre-trained π. Trong trường hợp này, task head tuning và prompt tuning chia sẻ cùng mục tiêu, có thể được công thức hóa bởi: max p,ϕP(yi,˜si)pπ,ϕ(yi|˜si).

C. Tuning in Line with Pretext. Trực quan, prompting nhằm tái cấu trúc downstream tasks thành pre-training task. Do đó, sẽ tự nhiên hơn nếu prompt tuning chia sẻ cùng mục tiêu với pre-training task. Như được gợi ý trong GraphPrompt [52], các tác giả sử dụng một loss function tương tự để học prompts. Tương tự, GPPT [78] và VNT [83] áp dụng cùng loss function (Cross-Entropy) như link prediction và node classification tasks tương ứng của họ. HetGPT [55] và SAP [17] sử dụng một node-level contrastive loss để học các prompt tokens của họ vì pre-training task của họ cũng được thực hiện bởi cùng contrastive task (node pair comparison). PGCL [18] giới thiệu graph-level loss để căn chỉnh với pre-training task. ULTRA-DP [4] phát triển hai pre-training tasks bao gồm edge prediction và neighboring prediction, mỗi cái tương ứng với một task embedding. Trong giai đoạn pre-training, họ ngẫu nhiên chọn một task và sau đó tích hợp các task-related embeddings cụ thể vào prompt tokens. Những learnable task embeddings này sau đó được đào tạo với graph model.

5.4 Thảo luận Thêm

Câu hỏi 4: Những Kết nối, Ưu điểm và Nhược điểm của Họ là gì?

Điểm tốt của GPF [11] là họ đề xuất một prompt rất đơn giản có thể dễ dàng được sử dụng trong các pre-training tasks và downstream tasks khác nhau. Tuy nhiên, một prompt token duy nhất được thêm vào tất cả nodes rất hạn chế về khả năng biểu đạt và tổng quát hóa. Một giải pháp tiềm năng là học một prompt token độc lập cho mỗi node, có nghĩa là một node tương ứng với một prompt token, nhưng điều này sẽ gây ra hiệu quả thấp trong tham số. Để kết thúc này, chúng ta có thể đào tạo K-independent basis vectors và sử dụng chúng để kết hợp mỗi node token (GPF-Plus [12]). Cải tiến này làm cho công trình của họ có những hiểu biết tương tự hơn với All in One [80].

HetGPT [55] mở rộng prompt tokens thành type-specific format, có thể xử lý graph prompting trong heterogeneous data. Tuy nhiên, họ chỉ có thể xử lý node classification tasks. Để kết thúc này, GraphPrompt [52] tái cấu trúc link prediction thành graph pair similarity task. Đáng chú ý rằng vai trò của prompt token của họ rất giống với project vector trong graph attention network. Cũng có một số phương pháp graph-pooling dựa trên attention, chia sẻ cùng động lực. Sự khác biệt là các tác giả tuyên bố rằng graph-pooling component trong giai đoạn pre-training có thể không phù hợp với các downstream tasks khác, do đó cần các prompts bổ sung để chuyển hướng graph-pooling component trong graph model.

GPPT [78] đại diện cho một trường hợp cụ thể trong khung rộng lớn hơn của All in One [80]. Ví dụ, nếu chúng ta giảm thiểu prompt graph thành các isolated tokens tương quan với node classes và thay thế các đồ thị kết quả bằng một complete graph, cấu trúc prompt All in One có thể được đơn giản hóa thành format GPPT. Điều này cho phép việc sử dụng edge-level pretexts trong node classification tasks trong khung GPPT. Nhược điểm của GPPT có thể là nó bị hạn chế trong binary edge prediction pretexts và chỉ hiệu quả cho node classification trong downstream tasks. Để so sánh, các khung như GraphPrompt và All in One được thiết kế để chứa một mảng rộng hơn các graph-related tasks, mở rộng ra ngoài chỉ node-level classification. Điểm tốt là khi thích ứng mô hình cho các tasks khác nhau, GraphPrompt, GPF, và GPF-Plus thường yêu cầu tuning của một module task-specific bổ sung. Ngược lại, All in One, và GPPT sử dụng task templates tập trung nhiều hơn vào thao tác input data và ít phụ thuộc vào đặc điểm của downstream tasks.

Trực quan, data graphs, một trong những thành phần trong PRODIGY [32], rất giống với induced graph trong All in One và GraphPrompt. Pre-training task trong PRODIGY có thể được xem như dự đoán một liên kết giữa data token và label token, chia sẻ hiểu biết tương tự với GPPT. Điều tốt là prompts của họ không có tham số trainable, có nghĩa là họ không cần điều chỉnh prompt và hiệu quả hơn trong lĩnh vực in-context learning. PRODIGY không cần bất kỳ công việc tuning nào và có thể được sử dụng trong knowledge transferring giữa các datasets khác nhau. Tuy nhiên, một prompt không thể điều chỉnh thường không đủ linh hoạt, cũng có thể hạn chế tổng quát hóa của mô hình pre-trained khi các downstream tasks cần được chuyển giao nằm trong một domain khác với pre-training one. Ngược lại, ULTRA-DP [4] điều chỉnh prompt cả trong giai đoạn pre-training và downstream tasks. Nó đầu tiên đặt công việc prompt tuning trong giai đoạn pre-training để có được task embeddings, là một trong những thành phần chính trong prompt của họ. Sau đó họ sử dụng những task embeddings này để khởi tạo một downstream prompt. Trực quan, prompts của họ không được sử dụng để tái cấu trúc downstream tasks thành pretext. Thay vào đó, những prompt tokens này được sử dụng để chọn các pre-training tasks phù hợp từ một task pool để phù hợp với downstream task. Vẫn là một câu hỏi thú vị về cách đạt được sự cân bằng tối ưu cho hiệu quả, tổng quát hóa, và tính linh hoạt của prompt.

So với các công trình khác thường định nghĩa các inserting patterns rõ ràng, VNT [83] concatenates prompt tokens với tập node gốc và sau đó đặt tất cả chúng vào graph transformer. Thực tế, graph transformer sẽ tận dụng một hàm self-attention để tính toán thêm các mối tương quan giữa chúng, cũng có thể được coi như một biến thể của inserting patterns được định nghĩa trong All in One. Điều tốt là chúng ta không cần thiết kế một threshold để điều chỉnh kết nối nhưng nhược điểm là nó chỉ có thể sử dụng graph transformer như backbone của nó và không thể áp dụng cho các message-passing-based graph models khác. Ngoài ra, cũng có một số biến thể tiên tiến hơn của graph transformers yêu cầu position embedding bổ sung như một trong những input của chúng. Tuy nhiên, các prompt tokens trong VNT không có liên kết chèn rõ ràng với đồ thị gốc, có thể không làm cho nó dễ dàng áp dụng các cách tiếp cận position encoding hiện có cho những biến thể graph transformer này.

6 GRAPH PROMPTING TRONG CÁC LĨNH VỰC MULTI-MODAL VÀ MULTI-DOMAIN

6.1 Multi-Modal Prompting với Đồ thị

Việc fusion hình ảnh, âm thanh, và văn bản đã được nghiên cứu rộng rãi và đạt được thành công đáng kể. Tuy nhiên, hầu hết những modalities này được mô tả bởi cấu trúc dữ liệu tuyến tính. Trong cuộc sống thực tế của chúng ta, có nhiều loại dữ liệu khác trong cấu trúc phi tuyến tính như đồ thị. Cách kết nối những modalities tuyến tính này (ví dụ: văn bản, hình ảnh, âm thanh, v.v.) với các modalities phi tuyến tính (ví dụ: đồ thị) đã trở thành một chủ đề nghiên cứu rất hấp dẫn vì nó là một bước tiến lớn hơn hướng tới trí tuệ nhân tạo tổng quát. Thật không may, việc đạt được tầm nhìn này rất khó khăn. Hiện tại, chúng ta chỉ thấy một số tiến bộ khó khăn trong việc fusion văn bản và đồ thị, đặc biệt là trong text-attributed graphs. Với sự giúp đỡ của large language models gần đây, việc fusion văn bản và đồ thị đã đạt được hiệu suất đáng chú ý hơn nữa. Vì đã có một số khảo sát thông tin về chủ đề này, chúng tôi tiếp theo thảo luận ngắn gọn một số công trình đại diện liên quan chặt chẽ đến kỹ thuật prompt. Chúng tôi gợi ý độc giả tham khảo tài liệu đã đề cập [46,48] để có thêm thông tin chi tiết.

A. Prompt trong Text-Attributed Graphs. Wen và Fang [97] mạo hiểm vào việc tăng cường text classification trong các tình huống với tài nguyên dữ liệu hạn chế bằng mô hình được đề xuất, Graph-Grounded Pre-training và Prompting (G2P2). Công trình của họ xác định vấn đề thiếu dữ liệu được gán nhãn trong supervised learning và đề xuất một giải pháp tận dụng cấu trúc mạng vốn có của text data, như hyperlinks hoặc citation networks. G2P2 sử dụng các chiến lược text-graph interaction với contrastive learning trong giai đoạn pre-training, tiếp theo là một kỹ thuật prompting sáng tạo trong giai đoạn classification, chứng minh hiệu quả đáng chú ý trong zero- và few-shot text classification tasks. Zhao et al. [121] tập trung vào molecule property prediction, một lĩnh vực grappling với sự khan hiếm của dữ liệu được gán nhãn. Nghiên cứu của họ giới thiệu một mô hình graph-text tích hợp để tăng cường prompt-based molecule task learning trong bối cảnh zero-shot. Mô hình này sử dụng generalized position embedding và decouples encoding của đồ thị từ task prompt, tăng cường khả năng tổng quát hóa của nó qua các novel tasks. Li và Hooi [45] khám phá node classification trong khung multi-modal data (text và graph), đặc biệt tập trung vào các tình huống limited-label. Không giống như các công trình truyền thống thường feed pre-computed text features vào graph neural networks, họ tích hợp raw texts và graph topology bằng một hand-crafted language prompt template vào thiết kế mô hình.

B. Large Language Models trong Graph Data Processing. Chen et al. [5] khám phá tiềm năng của Large Language Models (LLMs) trong graph node classification tasks. Họ điều tra hai pipelines: LLMs-as-Enhancers, tăng cường node text attributes sử dụng LLMs tiếp theo là dự đoán thông qua Graph Neural Networks (GNNs), và LLMs-as-Predictors, trực tiếp sử dụng LLMs như standalone predictors. Đánh giá thực nghiệm của họ tiết lộ rằng deep sentence embedding models và text-level augmentation thông qua LLMs hiệu quả tăng cường node attributes, trong khi LLMs cũng cho thấy triển vọng như standalone predictors, mặc dù có lo ngại về độ chính xác và test data leakage. Fatemi et al. [14] thực hiện một nghiên cứu toàn diện về encoding graph-structured data để tiêu thụ bởi LLMs. Phát hiện của họ bao gồm ảnh hưởng của graph encoding method, bản chất của graph task, và cấu trúc của đồ thị lên hiệu suất LLM. Họ chứng minh rằng simple prompts hiệu quả nhất cho basic graph tasks và graph encoding functions ảnh hưởng đáng kể đến LLM reasoning. Thiết lập thực nghiệm của họ giới thiệu các sửa đổi cho graph encoding function, tiết lộ cải thiện trong hiệu suất và chứng minh hiệu ứng của model capacity lên graph reasoning ability. Jin et al. [36] giới thiệu một cách tiếp cận sáng tạo để pre-train language models trên các mạng giàu văn bản. Khung của họ, được đặt tên là PATTON, tập trung vào việc tích hợp sự phức tạp của textual attributes với underlying network structure. Họ phát triển hai chiến lược pretraining mới: một tập trung vào context trong networks cho masked language modeling, và cái khác về dự đoán masked nodes, do đó nắm bắt sự tương tác giữa text và network structure. Hiệu quả của PATTON được xác nhận thông qua các thí nghiệm khác nhau, thể hiện hiệu suất vượt trội so với các phương pháp pretraining text/graph truyền thống trong các tasks đa dạng như document classification, retrieval, và link prediction trong các datasets domain khác nhau. Cách tiếp cận này biểu thị một sự thay đổi trong các phương pháp luận pretraining, nhấn mạnh sự synergy giữa textual data và network context. Wang et al. [96] đề xuất một phương pháp Knowledge Graph Prompting (KGP) để tăng cường LLMs cho multi-document question answering (MD-QA). Họ tạo ra một knowledge graph qua nhiều documents, với nodes đại diện cho passages hoặc document structures và edges biểu thị semantic/lexical similarity. LM-guided graph traverser trong KGP điều hướng đồ thị để thu thập supporting passages, hỗ trợ LLMs trong MD-QA. Các thí nghiệm của họ chỉ ra rằng việc xây dựng KGs và thiết kế của LM-guided graph traverser ảnh hưởng đáng kể đến hiệu suất MD-QA.

C. Multi-modal Fusion với Graph và Prompting. Việc tích hợp multi-modal data sử dụng graph và prompting techniques đã thấy tiến bộ đáng kể trong những năm gần đây. Ví dụ, Edwards et al. [10] đề xuất SynerGPT trong lĩnh vực drug synergy prediction. Mô hình này tận dụng một cách tiếp cận dựa trên transformer, kết hợp độc đáo in-context learning với genetic algorithms để dự đoán drug synergies. Trong lĩnh vực vision-language models, Li et al. [44] phát triển GraphAdapter, một chiến lược dựa trên prompt sử dụng một cơ chế adapter-style tuning, tập hợp textual và visual modalities thông qua một dual knowledge graph. Liu et al. [49] mở rộng công việc multi-modal fusion vào molecular science với GIT-Mol được đề xuất của họ. Một large language model tích hợp graph, image, và textual data với sự giúp đỡ của prompt, cung cấp cải thiện đáng kể trong các tasks khác nhau như molecule generation và property prediction. Mặc dù nhiều nỗ lực đã được thực hiện trong vài năm qua, academic vẫn đang cố gắng tìm kiếm các giải pháp tốt hơn để tích hợp text và graphs thông qua text-attributed graphs hoặc knowledge graphs. Vẫn có một trí tưởng tượng rất lớn trong việc fusion nhiều loại modalities hơn.

6.2 Graph Domain Adaptation với Prompting

Lĩnh vực graph domain adaptation đã thấy những tiến bộ đáng kể, đặc biệt với việc tích hợp các kỹ thuật prompting. Tuy nhiên, graph domain adaptation vẫn chưa phải là một vấn đề được giải quyết tốt vì tồn tại ít nhất hai thách thức: Thách thức đầu tiên là cách căn chỉnh semantic spaces từ các domains khác nhau. Thách thức thứ hai là cách xác định structural differences.

A. Semantic Alignment. Đặc biệt, All in One [80] mở rộng quy trình "pre-training và fine-tuning" với multi-task prompting cho GNNs, thống nhất prompt formats, và giới thiệu meta-learning cho prompt optimization. Để làm cho graph model thích ứng với các graph domains khác nhau, họ đầu tiên tiết lộ rằng graph prompt về bản chất có thể được xem như graph operation và sau đó họ sử dụng graph prompt để thao tác các datasets graph domain khác nhau. GraphControl [125] giới thiệu một deployment module độc đáo được lấy cảm hứng từ ControlNet, hiệu quả tích hợp downstream-specific information như conditional inputs để tăng cường khả năng thích ứng của các mô hình pre-trained với target data. Cách tiếp cận này căn chỉnh input space qua các đồ thị khác nhau và tích hợp các đặc điểm độc đáo của target data. Zhang et al. [119] trình bày một mô hình pre-training cho knowledge graph transfer learning. Mô hình này sử dụng một cơ chế general prompt-tuning, coi task data như một triple prompt, cho phép các tương tác linh hoạt giữa task KGs và task data. Yi et al. [110] kết hợp personalized graph prompts với contrastive learning cho cross-domain recommendation hiệu quả và hiệu quả, đặc biệt trong các tình huống cold-start. Một công trình đại diện được đề xuất bởi Liu et al. [47], trong đó họ mô tả graph nodes từ các domains khác nhau bằng ngôn ngữ và sau đó sử dụng LLM để có được textual embedding. Tuy nhiên, công trình này cần semantic name của mỗi feature trong khi đôi khi graph features thường là latent vectors mà không có ý nghĩa ngữ nghĩa rõ ràng.

B. Structural Alignment. Cao et al. [3] phân tích tính khả thi giữa các graph datasets khác nhau và phát hiện rằng structural gap giữ upper bound của transferability của các graph model khác nhau. GraphGLOW [122] giải quyết các hạn chế của các mô hình hiện có hoạt động dưới closed-world assumption, nơi testing graph giống hệt với training graph. Cách tiếp cận này thường dẫn đến chi phí tính toán cấm đoán và rủi ro overfitting do nhu cầu đào tạo một structure learning model từ đầu cho mỗi graph dataset. Để kết thúc này, nó phối hợp một single graph-shared structure learner với multiple graph-specific GNNs để nắm bắt các mẫu generalizable của optimal message-passing topology qua datasets. Structure learner, một khi được đào tạo, có thể tạo ra adaptive structures cho unseen target graphs mà không cần fine-tuning, từ đó giảm đáng kể thời gian đào tạo và tài nguyên tính toán. AAGOD bởi Guo et al. [20] đề xuất một framework data-centric cho OOD detection trong GNNs. Họ sử dụng một parameterized amplifier matrix, được coi như một prompt, để superimpose trên adjacency matrix của input graphs. Được lấy cảm hứng từ prompt tuning, Shirkavand và Huang [74] đề xuất DeepGPT cho graph transformer models. Họ thêm prompt tokens vào input graph và mỗi transformer layer. Bằng cách cập nhật prompt tokens, họ có thể điều chỉnh hiệu quả một graph model trên các graph datasets khác nhau.

7 ỨNG DỤNG TIỀM NĂNG

Với việc sử dụng rộng rãi networks như một cấu trúc modeling dữ liệu để đại diện cho thông tin quan hệ đa dạng qua các domains xã hội, tự nhiên, và học thuật, cơ chế graph prompt thể hiện tiềm năng đáng kể cho một loạt rộng các ứng dụng thực tế. Trong phần này, chúng tôi khám phá các ứng dụng tiềm năng của graph prompting trong online social networks, recommender systems, knowledge management, và biology.

Online Social Networks. Các nền tảng xã hội trực tuyến bao gồm người dùng có thể được đại diện như nodes, và các kết nối xã hội của họ tạo thành online social networks (OSNs). Nghiên cứu trước đây đã điều tra tiềm năng của các cơ chế prompting trong việc xác định fake news trong OSNs để ngăn chặn các cuộc tấn công độc hại [101]. Cụ thể, họ sử dụng textual prompts được áp dụng cho pre-trained language models (PLMs) để chưng cất thông tin ngữ nghĩa tổng quát. Bằng cách kết hợp tín hiệu ngữ nghĩa này với dynamics của information propagation trong social networks, hiệu suất classification được cải thiện có thể đạt được. Trong khi việc sử dụng textual prompts được thiết kế riêng cho PLMs đã được nghiên cứu, việc ứng dụng các cơ chế graph prompting trong social networks vẫn chưa được khám phá đủ. Trong tương lai, việc áp dụng trực tiếp các kỹ thuật prompt tuning cho social networks có triển vọng, sử dụng few-shot labels cho các tasks như fake news detection hoặc anomaly detection [99,20], nơi quá trình labeling tốn công sức và yêu cầu chuyên môn domain. Bằng cách tích hợp prompts trực tiếp trong social networks, cách tiếp cận này có thể giải quyết sự khan hiếm của dữ liệu được gán nhãn và tăng cường bảo mật và độ tin cậy của online social networks.

Recommender Systems. Các nền tảng thương mại điện tử cung cấp một cơ hội có giá trị để tận dụng recommender systems để tăng cường các dịch vụ trực tuyến. Trong khi prompt tuning trong recommender systems đã nhận được sự chú ý nghiên cứu hạn chế, nó có tiềm năng đáng kể [110,109,103,22]. Trong [110], kỹ thuật graph prompt tuning được áp dụng cho các tình huống cross-domain recommendation để giải quyết các thách thức của domain adaptation. Cụ thể, khi áp dụng một mô hình recommendation pre-trained cho target domain, các prompt nodes bổ sung được giới thiệu để đạt được cả domain recommendation hiệu quả và hiệu quả. Trong khi đó, Yang et al. [109] đề xuất personalized user prompts để kết nối khoảng cách giữa contrastive pretext [100,112] với downstream recommendation task. Họ thiết kế các loại personalized prompts khác nhau, kết hợp với pre-trained user embeddings để tạo điều kiện cho dynamic user representations, dẫn đến kết quả recommending chính xác và cá nhân hóa hơn. Trong tương lai, khám phá thêm về việc tích hợp graph prompt tuning trong recommender systems có thể được thực hiện để tăng cường hiệu suất recommendation, personalization, và khả năng thích ứng qua các domains khác nhau.

Knowledge Management. Có hai nhánh nghiên cứu tập trung vào việc thực hiện prompting trên knowledge graph (KG) để cải thiện knowledge management. Nhánh đầu tiên liên quan đến prompting trực tiếp trên knowledge graphs sử dụng một mô hình KG pre-trained để tạo điều kiện cho knowledge transfer, cho phép tổng quát hóa tốt hơn qua dữ liệu và tasks KG khác nhau [119]. Nhánh thứ hai khám phá sự kết hợp của grounded knowledge từ KGs với khả năng nhận thức của LLMs [96,85,67,60,118,59] để cải thiện hiệu suất trong downstream tasks. Trong dòng nghiên cứu đầu tiên, Zhang et al. [119] đề xuất một cách tiếp cận structure pre-training và prompt tuning để hiện thực hóa knowledge transfer. Họ thiết kế các mục tiêu pre-training cụ thể để có được một mô hình KG mạnh mẽ. Tiếp theo, một kỹ thuật general prompt tuning được sử dụng để tạo điều kiện cho knowledge transfer giữa task-specific KGs và data. Trong dòng nghiên cứu thứ hai, các kỹ thuật prompt tuning được áp dụng để kết hợp grounded knowledge vốn có trong KGs với LLMs để tăng cường downstream tasks. Ví dụ, trong [85], một phương pháp graph neural prompting mới được giới thiệu để thích ứng KGs cho LLMs bằng cách chưng cất kiến thức có giá trị từ KGs theo cách time- và parameter-efficient. Nghiên cứu tương lai có thể khám phá thêm xu hướng này bằng cách thực hiện các phương pháp graph prompting hiệu quả hơn để chưng cất đầy đủ kiến thức có lợi từ KGs để hỗ trợ LLMs trên các downstream tasks đa dạng.

Biology. Molecules có thể được đại diện tự nhiên như đồ thị, nơi atoms phục vụ như nodes và chemical bonds hoạt động như edges [68]. Modeling đồ thị như vậy cung cấp một cơ sở để áp dụng các phương pháp graph representation learning để thực hiện các tasks như molecular property prediction, từ đó có lợi cho nghiên cứu và khám phá khoa học [68,53,121,10]. Nghiên cứu trước đây về graph representation learning trong molecular domain theo một cách tiếp cận task-specific. Nó liên quan đến việc đào tạo các mô hình riêng lẻ được thiết kế riêng cho các molecule datasets cụ thể [68,76], thiếu khả năng tổng quát hóa trong domain. Mặc dù tồn tại các công trình khám phá việc sử dụng LLMs (hoặc LMs) như một công cụ universal để hiểu molecules [64,47], những cách tiếp cận này chủ yếu dựa vào regular texts để mô tả molecules, bỏ qua các inner graph structures trong molecules [120]. Trong khi đó, một số công trình gần đây đã tập trung vào việc điều tra co-modeling của graphs và languages để bảo tồn cả structural dependencies và đạt được khả năng tổng quát hóa qua tasks và datasets, thậm chí dưới few-shot hoặc zero-shot settings [49,121,10]. Ví dụ, trong [49], các tác giả sử dụng LoRA [29] để thích ứng hiệu quả với downstream tasks. Theo hướng nghiên cứu này, chúng tôi tin rằng các kỹ thuật graph prompting cũng có thể được áp dụng để đạt được task adaptation hiệu quả hơn trong molecular domain.

8 PROG: MỘT THƯ VIỆN THỐNG NHẤT CHO GRAPH PROMPTING

Một thành phần không thể thiếu để củng cố hệ sinh thái graph prompting là một công cụ được chế tác tốt. Mặc dù có vô số công cụ được đề xuất cho generalized graph learning, một sự vắng mặt đáng chú ý vẫn tồn tại trong lĩnh vực thư viện dành riêng cho graph prompt functionalities. Giải quyết khoảng trống này, chúng tôi đang giới thiệu ProG (Prompt Graph), một thư viện mã nguồn mở, thống nhất được thiết kế tỉ mỉ để phục vụ các nhu cầu cụ thể của graph prompting. Sáng kiến này hứa hẹn sẽ tăng cường đáng kể bối cảnh của các ứng dụng dựa trên đồ thị bằng cách cung cấp một tài nguyên đa năng và toàn diện cho các nhà nghiên cứu và thực hành viên.

ProG là một thư viện dựa trên PyTorch được thiết kế để tạo điều kiện cho single hoặc multi-task prompting cho pre-trained GNNs. Kiến trúc được minh họa trong Hình 8. Nó tích hợp liền mạch một số datasets được sử dụng rộng rãi trong đánh giá graph prompt, bao gồm Cora, CiteSeer, Reddit, Amazon, và Pubmed v.v. Công cụ được trang bị các metrics đánh giá cần thiết như Accuracy, F1 Score, và AUC score, thường được sử dụng trong các graph prompt-related tasks khác nhau. Đáng chú ý, ProG kết hợp các phương pháp state-of-the-art như All in One [80], GPPT [78], GPF [11], và GPF-Plus [12], và nó tiếp tục tích hợp thêm graph prompt models. Tóm lại, ProG cung cấp các tính năng chính sau:

•Quick Initiation. Tất cả mô hình được thực hiện trong một môi trường nhất quán, đi kèm với các demos chi tiết, đảm bảo sự khởi đầu nhanh chóng cho newcomers.

•Fully Modular. ProG áp dụng một cấu trúc modular, trao quyền cho người dùng để customize và xây dựng mô hình theo nhu cầu.

•Easy Extendable. ProG được thiết kế để mở rộng liền mạch để bao gồm các phương pháp bổ sung và một phổ rộng hơn của downstream tasks, thích ứng với nhu cầu nghiên cứu đang phát triển.

•Standardized Evaluation. ProG thiết lập một tập hợp uniform của các quy trình đánh giá, thúc đẩy so sánh hiệu suất công bằng qua các mô hình.

Để biết thêm thông tin và truy cập thư viện, vui lòng truy cập trang web của thư viện chúng tôi5. Ngoài ra, chúng tôi đã tuyển chọn một GitHub repository6, phục vụ như một tài nguyên tập trung cho những tiến bộ mới nhất trong graph prompt learning. Repository này bao gồm một danh sách các bài báo nghiên cứu, benchmark datasets, và codes có sẵn, nuôi dưỡng một môi trường thuận lợi cho nghiên cứu đang diễn ra trong lĩnh vực dynamic này. Các cập nhật real-time thường xuyên đảm bảo rằng repository vẫn hiện tại với các bài báo mới nổi và codes liên quan.

9 THÁCH THỨC VÀ HƯỚNG TƯƠNG LAI

9.1 Thách thức Hiện tại

Graph prompt learning đã đạt được tiến bộ nghiên cứu đáng kể, nhưng nó vẫn gặp phải một số thách thức. Trong phần phụ này, chúng tôi sẽ thảo luận chi tiết về các thách thức hiện tại.

Inherent Limitation within Graph Models Prompts, bắt nguồn từ lĩnh vực NLP [2,40,50], phục vụ như một phương tiện để mở khóa tiềm năng của các mô hình ngôn ngữ pre-trained để thích ứng với downstream tasks. Khả năng in-context learning này xuất hiện khi quy mô của model parameters đạt đến một mức độ nhất định. Ví dụ, LLMs được biết đến rộng rãi thường sở hữu hàng tỷ parameters. Với một mô hình pre-trained mạnh mẽ như vậy, một textual prompt đơn giản có thể chưng cất kiến thức cụ thể cho downstream tasks. Tuy nhiên, khi nói đến prompting trên graph tasks, các điều kiện trở nên khó giải quyết hơn vì các mô hình đồ thị pre-trained có ít parameters hơn đáng kể, khiến việc khai thác toàn bộ tiềm năng thích ứng downstream của chúng trở nên thách thức.

Intuitive Evaluation of Graph Prompt Learning Trong lĩnh vực NLP, prompts thường ở discrete textual format [2,67], cho phép hiểu, so sánh, và giải thích trực quan. Tuy nhiên, các graph prompts hiện có được đại diện như learnable tokens [78,12,11,83,55,17] hoặc augmented graphs [80,32]. Format như vậy gây ra thách thức trong việc hiểu và diễn giải graph prompts một cách trực quan, vì chúng thiếu một thiết kế có thể đọc được. Do đó, hiệu quả của prompts chỉ có thể được đánh giá dựa trên downstream tasks, hạn chế so sánh hiệu suất hiệu quả và toàn diện giữa các loại graph prompts khác nhau. Do đó, việc phát triển một thiết kế graph prompt trực quan hơn với format có thể đọc được vẫn là một vấn đề mở.

More Downstream Applications Hiện tại, graph prompt learning chủ yếu được áp dụng cho node hoặc graph classification tasks trên open-source benchmarks [21,107]. Mặc dù các ứng dụng tiềm năng đã được thảo luận trong Phần 7, việc sử dụng graph prompt learning trong thực tế vẫn hạn chế. Một ví dụ đáng chú ý là việc sử dụng nó trong fraud detection trong real-world transaction networks, giải quyết vấn đề khan hiếm label [99]. Tuy nhiên, so với việc ứng dụng rộng rãi các kỹ thuật prompting trong NLP domain [1,67,118], tiềm năng của graph prompt learning trong các ứng dụng thực tế đa dạng yêu cầu khám phá thêm. Vượt qua các thách thức chính của việc có được các mô hình đồ thị pre-trained mạnh mẽ domain-specific và thiết kế prompts phù hợp cho các tình huống ứng dụng cụ thể thể hiện đặc điểm độc đáo vẫn quan trọng.

Transferable Prompt Designs Các nghiên cứu hiện có về graph prompt learning [52,78,18,12,11,17] thường tập trung vào pre-training và prompt tuning sử dụng cùng dataset, hạn chế việc khám phá các thiết kế transferable hơn và đánh giá thực nghiệm. Mặc dù PRODIGY [32] khám phá transferability trong cùng domain và All in One [80] cung cấp kết quả thực nghiệm về transferability qua tasks và domains, việc điều tra prompt learning qua các domains và tasks đa dạng vẫn hạn chế. Đạt được transferability qua các tasks và domains đa dạng yêu cầu căn chỉnh task space [80], semantic features [125], và structural patterns [122], cần công việc lý thuyết thêm để cung cấp hiểu biết hướng dẫn việc phát triển transferable prompt designs.

9.2 Hướng Tương lai

Với phân tích trên về graph prompt, chúng tôi tóm tắt các hướng tương lai như sau:

Learning Knowledge from Large Graph Models (LGMs) like LLMs. Hiện tại, các mô hình đồ thị thường được thiết kế riêng cho các domains hoặc tasks cụ thể, hạn chế khả năng tổng quát hóa qua các domains hoặc tasks rộng lớn hơn [48,125]. Do đó, chúng tôi đang mong đợi việc hiện thực hóa large graph models (LGMs) như một công cụ universal đủ thông minh để xử lý graph tasks qua domains [47,14]. Một bước đáng kể hướng tới hướng này đã được thực hiện bởi Liu et al. [47], những người đề xuất một mô hình mạnh mẽ (OFA) có khả năng giải quyết classification tasks cho đồ thị đến từ các domains khác nhau. Tuy nhiên, cách tiếp cận của họ vẫn dựa vào LLMs như một template tổng quát để chưng cất kiến thức domain cụ thể, mà chúng tôi tin có thể được thay thế bằng các kỹ thuật graph prompting phù hợp. Giống như textual prompt đã được sử dụng rộng rãi để thích ứng LLM cho các ứng dụng đa dạng [67,96,2], các kỹ thuật graph prompting có triển vọng để chưng cất kiến thức của LGMs cụ thể cho các downstream tasks cụ thể. Áp dụng graph prompts cho LGMs có tiềm năng cách mạng hóa lĩnh vực deep graph learning, tận dụng LGMs như một công cụ universal để giải quyết các tasks khác nhau trên đồ thị từ các domains đa dạng.

Transferable Learning. Như chúng tôi đã thảo luận trong Phần 9.1, các phương pháp graph prompt learning hiện tại [80,78,32,52,83,17,74] chủ yếu bị hạn chế trong các settings intra-dataset/domain, thiếu khả năng chuyển giao kiến thức qua các tasks hoặc domains khác nhau. Trong khi đã có những nỗ lực trong graph transfer learning [125] để hiện thực hóa domain adaptation, nghiên cứu tập trung cụ thể vào các kỹ thuật transferable graph prompting vẫn hạn chế. Để cho phép transfer ability, các prompts nên được thiết kế để hiện thực hóa alignment giữa các task spaces khác nhau [80], tái cấu trúc các tasks khác nhau trên đồ thị thành một template uniform. Bên cạnh đó, cả structural [122] và semantics alignment [125] nên được hiện thực hóa thông qua các graph prompts phù hợp để cho phép domain adaptation. Việc thực hiện transferable graph prompts có triển vọng, hiện thực hóa knowledge transfer qua domains để mở rộng tổng quát hóa và khả năng ứng dụng của graph models.

More Theoretical Foundation. Mặc dù thành công to lớn của graph prompt learning trên các downstream tasks khác nhau, chúng chủ yếu dựa vào kinh nghiệm thành công của prompt tuning trên NLP domain [51,2,16,86,65,73]. Nói cách khác, hầu hết các phương pháp graph prompt tuning hiện có được thiết kế với trực giác, và cải thiện hiệu suất của chúng được đánh giá bằng các thí nghiệm thực nghiệm. Thiếu nền tảng lý thuyết đầy đủ đằng sau thiết kế đã dẫn đến cả bottlenecks hiệu suất và explainability kém. Do đó, chúng tôi tin rằng việc xây dựng một nền tảng lý thuyết vững chắc cho graph prompt learning từ góc độ graph theory và giảm thiểu khoảng cách giữa nền tảng lý thuyết và thiết kế thực nghiệm cũng là một hướng tương lai đầy hứa hẹn.

More Explainable and Understandable Design. Trong khi các phương pháp graph prompt learning hiện có đã chứng minh kết quả ấn tượng trên các downstream tasks khác nhau [20,99,109], chúng ta vẫn thiếu sự hiểu biết rõ ràng về chính xác những gì đang được học từ prompts. Chế độ học black-box của prompt vectors làm dấy lên câu hỏi về interpretability của chúng và liệu chúng ta có thể thiết lập các correspondences có ý nghĩa giữa input data và prompted graph [80]. Những vấn đề này rất quan trọng để hiểu và diễn giải prompts nhưng hiện tại đang thiếu trong hầu hết nghiên cứu graph prompt. Để hướng tới trustworthy graph prompt learning, việc khám phá self-interpretability [7] để cho phép giải thích trực quan của graph prompts có triển vọng. Bằng cách có được hiểu biết về các prompt vectors và structures đã học, chúng ta có thể tăng cường sự hiểu biết về các cơ chế cơ bản và cải thiện interpretability của graph prompts. Điều này, đến lượt nó, có thể dẫn đến việc sử dụng prompts hiệu quả hơn cho các downstream tasks liên quan đến security- hoặc privacy-.

10 KẾT LUẬN

Trong khảo sát này, chúng tôi khám phá giao điểm đầy hứa hẹn giữa Trí Tuệ Nhân Tạo Tổng Quát và dữ liệu đồ thị bằng graph prompt. Khung thống nhất của chúng tôi đã tiết lộ sự hiểu biết có cấu trúc về graph prompts, phân tích chúng thành tokens, token structures, và inserting patterns. Khung này là một đóng góp mới, cung cấp sự rõ ràng và toàn diện cho các nhà nghiên cứu và thực hành viên. Bằng cách khám phá sự tương tác giữa graph prompts và models, chúng tôi đã tiết lộ những hiểu biết mới về bản chất của graph prompts, làm nổi bật vai trò quan trọng của chúng trong việc định hình lại AI cho dữ liệu đồ thị. Với việc phát triển ProG, một thư viện Python, và một trang web chuyên dụng, chúng tôi đã mở rộng hệ sinh thái graph prompting, tăng cường hợp tác và truy cập vào nghiên cứu, benchmark datasets, và code implementations. Khảo sát của chúng tôi phác thảo một lộ trình cho tương lai. Các thách thức và hướng tương lai mà chúng tôi đã thảo luận phục vụ như một ngọn hải đăng cho lĩnh vực đang phát triển của graph prompting. Với công việc trên, chúng tôi hy vọng khảo sát của chúng tôi có thể thúc đẩy một kỷ nguyên mới của hiểu biết và ứng dụng trong gia đình AGI.

LỜI CẢM ƠN

Công trình được hỗ trợ bởi các grants từ Research Grant Council của Hong Kong Special Administrative Region, China (Project No. CUHK 14217622), và CUHK Direct Grant No. 4055159. Tác giả đầu tiên, Dr. Xiangguo Sun, đặc biệt muốn cảm ơn cha mẹ anh ấy về sự hỗ trợ tốt bụng của họ trong thời kỳ khó khăn của anh ấy.

Đóng góp Tác giả:

•Xiangguo Sun: viết phần 1, phần 2, phần 3.5 (với Xixi), nội dung chính của phần 5, phần 6, thực hiện proofreading toàn bộ bài báo, và đề xuất hầu hết các hiểu biết gốc.

•Jiawen Zhang: viết hầu hết phần 3 (phần 3.1-3.4), phần 4, phần 5 (thu thập, phân tích bài báo và bổ sung nội dung sâu sắc), phần 8, và phụ trách phát triển thư viện.

•Xixi Wu: viết phần 3.5, phần 5 (thu thập, phân tích bài báo và bổ sung nội dung sâu sắc), phần 7, hầu hết phần 9 (thách thức và hướng tương lai), thu thập và phân tích các tài liệu tham khảo, và vẽ hầu hết các hình.

•Hong Cheng, Yun Xiong, và Jia Li: phụ trách proofreading, và thảo luận, và đóng góp với nhiều ý kiến sâu sắc.

TÀI LIỆU THAM KHẢO

[1] A. M. Bran and P. Schwaller, "Transformers and Large Language Models for Chemistry and Drug Discovery," arXiv preprint, 2023.

[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal et al., "Language Models Are Few-Shot Learners," in NeurIPS, vol. 33, 2020, pp. 1877–1901.

[3] Y. Cao, J. Xu, C. Yang, J. Wang, Y. Zhang, C. Wang, L. Chen, and Y. Yang, "When to pre-train graph neural networks? An answer from data generation perspective!" in KDD, 2023.

[4] M. Chen, Z. Liu, C. Liu, J. Li, Q. Mao, and J. Sun, "ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt," arXiv preprint, 2023.

[5] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang, D. Yin, W. Fan, H. Liu, and J. Tang, "Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs," arXiv preprint, 2023.

[6] J. Cheng, M. Li, J. Li, and F. Tsung, "Wiener graph deconvolutional network improves graph self-supervised learning," in AAAI, 2023, pp. 7131–7139.

[7] E. Dai and S. Wang, "Towards Self-Explainable Graph Neural Network," in CIKM, 2021, pp. 302–311.

[8] M. Defferrard, X. Bresson, and P. Vandergheynst, "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering," in NeurIPS, vol. 29, 2016.

[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," arXiv preprint, 2019.

[10] C. Edwards, A. Naik, T. Khot, M. Burke, H. Ji, and T. Hope, "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design," arXiv preprint, 2023.

[11] T. Fang, Y. Zhang, Y. Yang, and C. Wang, "Prompt tuning for graph neural networks," arXiv preprint, 2022.

[12] T. Fang, Y. Zhang, Y. Yang, C. Wang, and L. Chen, "Universal Prompt Tuning for Graph Neural Networks," in NeurIPS, 2023.

[13] X. Fang, L. Liu, J. Lei, D. He, S. Zhang, J. Zhou, F. Wang, H. Wu, and H. Wang, "Geometry-enhanced molecular representation learning for property prediction," Nature Machine Intelligence, vol. 4, no. 2, pp. 127–134, 2022.

[14] B. Fatemi, J. Halcrow, and B. Perozzi, "Talk like a graph: Encoding graphs for large language models," arXiv preprint, 2023.

[15] C. Finn, P. Abbeel, and S. Levine, "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks," in ICML, 2017.

[16] T. Gao, A. Fisch, and D. Chen, "Making Pre-Trained Language Models Better Few-Shot Learners," in ACL, 2021, pp. 3816–3830.

[17] Q. Ge, Z. Zhao, Y. Liu, A. Cheng, X. Li, S. Wang, and D. Yin, "Enhancing Graph Neural Networks with Structure-Based Prompt," arXiv preprint, 2023.

[18] C. Gong, X. Li, J. Yu, C. Yao, J. Tan, C. Yu, and D. Yin, "Prompt Tuning for Multi-View Graph Contrastive Learning," arXiv preprint, 2023.

[19] A. Grover and J. Leskovec, "node2vec: Scalable feature learning for networks," in KDD, 2016, pp. 855–864.

[20] Y. Guo, C. Yang, Y. Chen, J. Liu, C. Shi, and J. Du, "A Data-Centric Framework to Endow Graph Neural Networks with Out-of-Distribution Detection Ability," in KDD, 2023, pp. 638–648.

[21] W. Hamilton, Z. Ying, and J. Leskovec, "Inductive Representation Learning on Large Graphs," in NeurIPS, vol. 30, 2017.

[22] B. Hao, C. Yang, L. Guo, J. Yu, and H. Yin, "Motif-Based Prompt Learning for Universal Cross-Domain Recommendation," in WSDM, 2024.

[23] A. Hasanzadeh, E. Hajiramezanali, K. Narayanan, N. Duffield, M. Zhou, and X. Qian, "Semi-Implicit Graph Variational Auto-Encoders," in NeurIPS, vol. 32, 2019.

[24] K. Hassani and A. H. K. Ahmadi, "Contrastive Multi-View Representation Learning on Graphs," in ICML, vol. 119, 2020, pp. 4116–4126.

[25] A. Haviv, J. Berant, and A. Globerson, "BERTese: Learning to Speak to BERT," in EACL, 2021, pp. 3618–3623.

[26] M. He, Z. Wei, z. Huang, and H. Xu, "BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation," in NeurIPS, vol. 34, 2021, pp. 14,239–14,251.

[27] Z. Hou, X. Liu, Y. Cen, Y. Dong, H. Yang, C. Wang, and J. Tang, "GraphMAE: Self-Supervised Masked Graph Autoencoders," in KDD, 2022, pp. 594–604.

[28] Z. Hou, Y. He, Y. Cen, X. Liu, Y. Dong, E. Kharlamov, and J. Tang, "GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner," in The Web Conference, 2023, pp. 737–746.

[29] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "LoRA: Low-Rank Adaptation of Large Language Models," arXiv preprint, 2021.

[30] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. S. Pande, and J. Leskovec, "Strategies for Pre-training Graph Neural Networks," in ICLR, 2020.

[31] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun, "GPT-GNN: Generative Pre-Training of Graph Neural Networks," in KDD, 2020, pp. 1857–1867.

[32] Q. Huang, H. Ren, P. Chen, G. Kržmanc, D. Zeng, P. Liang, and J. Leskovec, "PRODIGY: Enabling In-context Learning Over Graphs," in NeurIPS, 2023.

[33] X. Jiang, T. Jia, Y. Fang, C. Shi, Z. Lin, and H. Wang, "Pre-training on Large-Scale Heterogeneous Graph," in KDD, 2021, pp. 756–766.

[34] X. Jiang, Y. Lu, Y. Fang, and C. Shi, "Contrastive Pre-Training of GNNs on Heterogeneous Graphs," in CIKM, 2021, pp. 803–812.

[35] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, "How Can We Know What Language Models Know?" TACL, vol. 8, pp. 423–438, 2020.

[36] B. Jin, W. Zhang, Y. Zhang, Y. Meng, X. Zhang, Q. Zhu, and J. Han, "Patton: Language Model Pretraining on Text-Rich Networks," in ACL, 2023.

[37] M. Jin, Y. Zheng, Y.-F. Li, C. Gong, C. Zhou, and S. Pan, "Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning," in IJCAI, 2021, pp. 1477–1483.

[38] W. Jin, T. Derr, Y. Wang, Y. Ma, Z. Liu, and J. Tang, "Node Similarity Preserving Graph Convolutional Networks," in WSDM, 2021, pp. 148–156.

[39] D. Kim and A. Oh, "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision," in ICLR, 2021.

[40] B. Lester, R. Al-Rfou, and N. Constant, "The Power of Scale for Parameter-Efficient Prompt Tuning," in EMNLP, 2021, pp. 3045–3059.

[41] J. Li, R. Wu, W. Sun, L. Chen, S. Tian, L. Zhu, C. Meng, Z. Zheng, and W. Wang, "What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders," in KDD, 2023, pp. 1268–1279.

[42] S. Li, X. Han, and J. Bai, "AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks," arXiv preprint, 2023.

[43] X. L. Li and P. Liang, "Prefix-Tuning: Optimizing Continuous Prompts for Generation," in ACL-IJCNLP, 2021, pp. 4582–4597.

[44] X. Li, D. Lian, Z. Lu, J. Bai, Z. Chen, and X. Wang, "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph," in NeurIPS, 2023.

[45] Y. Li and B. Hooi, "Prompt-Based Zero-and Few-Shot Node Classification: A Multimodal Approach," arXiv preprint, 2023.

[46] Y. Li, Z. Li, P. Wang, J. Li, X. Sun, H. Cheng, and J. X. Yu, "A survey of graph meets large language model: Progress and future directions," arXiv preprint, 2023.

[47] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and M. Zhang, "One for All: Towards Training One Graph Model for All Classification Tasks," arXiv preprint, 2023.

[48] J. Liu, C. Yang, Z. Lu, J. Chen, Y. Li, M. Zhang, T. Bai, Y. Fang, L. Sun, P. S. Yu, and C. Shi, "Towards Graph Foundation Models: A Survey and Beyond," arXiv preprint, 2023.

[49] P. Liu, Y. Ren, and Z. Ren, "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text," arXiv preprint, 2023.

[50] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing," ACM Computing Surveys, vol. 55, no. 9, pp. 195:1–195:35, 2023.

[51] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, "P-Tuning: Prompt Tuning Can Be Comparable to Fine-Tuning Across Scales and Tasks," in ACL, vol. 2, 2022, pp. 61–68.

[52] Z. Liu, X. Yu, Y. Fang, and X. Zhang, "Graphprompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks," in The Web Conference, 2023, pp. 417–428.

[53] Z. Liu, S. Li, Y. Luo, H. Fei, Y. Cao, K. Kawaguchi, X. Wang, and T.-S. Chua, "MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter," in EMNLP, 2023.

[54] Y. Long, M. Wu, Y. Liu, Y. Fang, C. K. Kwoh, J. Chen, J. Luo, and X. Li, "Pre-training graph neural networks for link prediction in biomedical networks," Bioinformatics, vol. 38, no. 8, pp. 2254–2262, 2022.

[55] Y. Ma, N. Yan, J. Li, M. Mortazavi, and N. V. Chawla, "HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks," arXiv preprint, 2023.

[56] M. Niepert, M. Ahmed, and K. Kutzkov, "Learning Convolutional Neural Networks for Graphs," in ICML, 2016, pp. 2014–2023.

[57] M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, "Asymmetric Transitivity Preserving Graph Embedding," in KDD, 2016, pp. 1105–1114.

[58] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, "Adversarially Regularized Graph Autoencoder for Graph Embedding," in IJCAI, 2018, pp. 2609–2615.

[59] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, "Unifying Large Language Models and Knowledge Graphs: A Roadmap," arXiv preprint, 2023.

[60] J. Park, A. Patel, O. Z. Khan, H. J. Kim, and J.-K. Kim, "Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models," arXiv preprint, 2023.

[61] J. Park, M. Lee, H. J. Chang, K. Lee, and J. Y. Choi, "Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning," in ICCV, 2019, pp. 6519–6528.

[62] Z. Peng, W. Huang, M. Luo, Q. Zheng, Y. Rong, T. Xu, and J. Huang, "Graph Representation Learning via Graphical Mutual Information Maximization," in The Web Conference, 2020, pp. 259–270.

[63] B. Perozzi, R. Al-Rfou, and S. Skiena, "Deepwalk: Online learning of social representations," in KDD, 2014, pp. 701–710.

[64] C. Qian, H. Tang, Z. Yang, H. Liang, and Y. Liu, "Can Large Language Models Empower Molecular Property Prediction?" arXiv preprint, 2023.

[65] G. Qin and J. Eisner, "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts," in NAACL-HLT, 2021, pp. 5203–5212.

[66] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang, "GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training," in KDD, 2020, pp. 1150–1160.

[67] J. Robinson, C. M. Rytting, and D. Wingate, "Leveraging Large Language Models for Multiple Choice Question Answering," arXiv preprint, 2023.

[68] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. WEI, W. Huang, and J. Huang, "Self-Supervised Graph Transformer on Large-Scale Molecular Data," in NeurIPS, vol. 33, 2020, pp. 12,559–12,571.

[69] M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich, "To transfer or not to transfer," in NeurIPS, vol. 898, 2005.

[70] T. Schick and H. Schütze, "Few-Shot Text Generation with Natural Language Instructions," in EMNLP, 2021, pp. 390–402.

[71] ——, "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners," in NAACL-HLT, 2021, pp. 2339–2352.

[72] Y. Shi, Z. Huang, S. Feng, H. Zhong, W. Wang, and Y. Sun, "Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification," in IJCAI, vol. 2, 2021, pp. 1548–1554.

[73] T. Shin, Y. Razeghi, R. L. L. IV, E. Wallace, and S. Singh, "Autoprompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts," in EMNLP, 2020, pp. 4222–4235.

[74] R. Shirkavand and H. Huang, "Deep Prompt Tuning for Graph Transformers," arXiv preprint, 2023.

[75] A. Subramonian, "MOTIF-Driven Contrastive Learning of Graph Representations," in AAAI, vol. 35, 2021, pp. 15,980–15,981.

[76] F.-Y. Sun, J. Hoffmann, V. Verma, and J. Tang, "InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization," in ICLR, 2020.

[77] M. Sun, J. Xing, H. Wang, B. Chen, and J. Zhou, "MoCL: Data-driven Molecular Fingerprint via Knowledge-aware Contrastive Learning from Molecular Graph," in KDD, 2021, pp. 3585–3594.

[78] M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang, "GPPT: Graph Pre-Training and Prompt Tuning to Generalize Graph Neural Networks," in KDD, 2022, pp. 1717–1727.

[79] X. Sun, H. Yin, B. Liu, H. Chen, J. Cao, Y. Shao, and N. Q. Viet Hung, "Heterogeneous Hypergraph Embedding for Graph Classification," in WSDM, 2021, pp. 725–733.

[80] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, "All in One: Multi-Task Prompting for Graph Neural Networks," in KDD, 2023, pp. 2120–2131.

[81] S. Suresh, P. Li, C. Hao, and J. Neville, "Adversarial Graph Augmentation to Improve Graph Contrastive Learning," in NeurIPS, vol. 34, 2021, pp. 15,920–15,933.

[82] Q. Tan, N. Liu, X. Huang, S.-H. Choi, L. Li, R. Chen, and X. Hu, "S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking," in WSDM, 2023, pp. 787–795.

[83] Z. Tan, R. Guo, K. Ding, and H. Liu, "Virtual Node Tuning for Few-shot Node Classification," in KDD, 2023, pp. 2177–2188.

[84] S. Thakoor, C. Tallec, M. G. Azar, R. Munos, P. Veličković, and M. Valko, "Bootstrapped representation learning on graphs," in ICLR, 2021.

[85] Y. Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V. Chawla, and P. Xu, "Graph Neural Prompting with Large Language Models," arXiv preprint, 2023.

[86] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill, "Multimodal Few-Shot Learning with Frozen Language Models," in NeurIPS, vol. 34, 2021, pp. 200–212.

[87] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio, "Graph Attention Networks," in ICLR, 2018.

[88] P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm, "Deep Graph Infomax," in ICLR, 2019.

[89] C. Wang, S. Pan, G. Long, X. Zhu, and J. Jiang, "MGAE: Marginalized Graph Autoencoder for Graph Clustering," in CIKM, 2017, pp. 889–898.

[90] H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu et al., "Scientific discovery in the age of artificial intelligence," Nature, vol. 620, no. 7972, pp. 47–60, 2023.

[91] J. Wang, D. Chen, C. Luo, X. Dai, L. Yuan, Z. Wu, and Y.-G. Jiang, "ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System," arXiv preprint, 2023.

[92] P. Wang, K. Agarwal, C. Ham, S. Choudhury, and C. K. Reddy, "Self-Supervised Learning of Contextual Embeddings for Link Prediction in Heterogeneous Networks," in The Web Conference, 2021, pp. 2946–2957.

[93] R. Wang, Y. Li, S. Lin, W. Wu, H. Xie, Y. Xu, and J. C. Lui, "Common neighbors matter: fast random walk sampling with common neighbor awareness," TKDE, vol. 35, no. 5, pp. 4570–4584, 2022.

[94] X. Wang, Y. Zhang, and C. Shi, "Hyperbolic heterogeneous information network embedding," in AAAI, vol. 33, 2019, pp. 5337–5344.

[95] X. Wang, N. Liu, H. Han, and C. Shi, "Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning," in KDD, 2021, pp. 1726–1736.

[96] Y. Wang, N. Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr, "Knowledge Graph Prompting for Multi-Document Question Answering," arXiv preprint, 2023.

[97] Z. Wen and Y. Fang, "Augmenting Low-Resource Text Classification with Graph-Grounded Pre-Training and Prompting," in SIGIR, 2023, pp. 506–516.

[98] ——, "Prompt Tuning on Graph-augmented Low-resource Text Classification," arXiv preprint, 2023.

[99] Z. Wen, Y. Fang, Y. Liu, Y. Guo, and S. Hao, "Voucher Abuse Detection with Prompt-based Fine-tuning on Graph Neural Networks," arXiv preprint, 2023.

[100] J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and X. Xie, "Self-supervised Graph Learning for Recommendation," in SIGIR, 2021.

[101] J. Wu, S. Li, A. Deng, M. Xiong, and H. Bryan, "Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection," in CIKM, 2023.

[102] X. Wu, K. Zhou, M. Sun, X. Wang, and N. Liu, "A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges," arXiv preprint, 2023.

[103] Y. Wu, R. Xie, Y. Zhu, F. Zhuang, X. Zhang, L. Lin, and Q. He, "Personalized Prompt for Sequential Recommendation," arXiv preprint, 2023.

[104] Y. Xie, Z. Xu, and S. Ji, "Self-Supervised Representation Learning via Latent Graph Prediction," in ICML, 2022, pp. 24,460–24,477.

[105] Y. Xie, Z. Xu, J. Zhang, Z. Wang, and S. Ji, "Self-Supervised Learning of Graph Neural Networks: A Unified Review," TPAML, vol. 45, no. 2, pp. 2412–2429, 2023.

[106] D. Xu, W. Cheng, D. Luo, H. Chen, and X. Zhang, "InfoGCL: Information-Aware Graph Contrastive Learning," in NeurIPS, vol. 34, 2021, pp. 30,414–30,425.

[107] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, "How Powerful are Graph Neural Networks?" in ICLR, 2018.

[108] C. Yang, D. Bo, J. Liu, Y. Peng, B. Chen, H. Dai et al., "Data-centric graph learning: A survey," arXiv preprint, 2023.

[109] H. Yang, X. Zhao, Y. Li, H. Chen, and G. Xu, "An Empirical Study Towards Prompt-Tuning for Graph Contrastive Pre-Training in Recommendations," in NeurIPS, 2023.

[110] Z. Yi, I. Ounis, and C. Macdonald, "Contrastive Graph Prompt-tuning for Cross-domain Recommendation," TOIS, 2023.

[111] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, "Graph contrastive learning with augmentations," in NeurIPS, 2020, pp. 5812–5823.

[112] J. Yu, H. Yin, X. Xia, T. Chen, L. Cui, and N. Q. V. Hung, "Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation," in SIGIR, 2022.

[113] J. Yu, H. Yin, X. Xia, T. Chen, J. Li, and Z. Huang, "Self-Supervised Learning for Recommender Systems: A Survey," TKDE, pp. 1–20, 2023.

[114] H. Zhang, X. Li, and L. Bing, "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding," arXiv preprint, 2023.

[115] J. Zhang, H. Zhang, C. Xia, and L. Sun, "Graph-Bert: Only Attention is Needed for Learning Graph Representations," arXiv preprint, 2020.

[116] J. Zhang, Y. Dong, Y. Wang, J. Tang, and M. Ding, "ProNE: Fast and Scalable Network Representation Learning," in IJCAI, 2019, pp. 4278–4284.

[117] M. Zhang and Y. Chen, "Link Prediction Based on Graph Neural Networks," in NeurIPS, 2018.

[118] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto, "Benchmarking Large Language Models for News Summarization," arXiv preprint, 2023.

[119] W. Zhang, Y. Zhu, M. Chen, Y. Geng, Y. Huang, Y. Xu, W. Song, and H. Chen, "Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer," in The Web Conference, 2023, pp. 2581–2590.

[120] Z. Zhang, H. Li, Z. Zhang, Y. Qin, X. Wang, and W. Zhu, "Large graph models: A perspective," arXiv preprint, 2023.

[121] H. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z.-H. Deng, L. Kong, and Q. Liu, "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning," arXiv preprint, 2023.

[122] W. Zhao, Q. Wu, C. Yang, and J. Yan, "GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks," in KDD, 2023, pp. 3525–3536.

[123] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang, "Graph Contrastive Learning with Adaptive Augmentation," in The Web Conference, 2021, pp. 2069–2080.

[124] Y. Zhu, J. Guo, and S. Tang, "SGL-PT: A Strong Graph Learner with Graph Prompt Tuning," arXiv preprint, 2023.

[125] Y. Zhu, Y. Wang, H. Shi, Z. Zhang, and S. Tang, "GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning," arXiv preprint, 2023.

Xiangguo Sun là một nghiên cứu viên hậu tiến sĩ tại Đại học Trung Văn Hong Kong. Anh được công nhận là "Social Computing Rising Star" năm 2023 từ CAAI. Anh học tại Zhejiang Lab như một nhà nghiên cứu thăm viếng năm 2022. Cùng năm đó, anh nhận bằng Ph.D. từ Southeast University và giành được Distinguished Ph.D. Dissertation Award. Trong quá trình học Ph.D., anh làm việc như một research intern tại Microsoft Research Asia (MSRA) từ tháng 9 năm 2021 đến tháng 1 năm 2022 và giành được "Award of Excellence". Anh học như một sinh viên Ph.D. chung tại The University of Queensland được host bởi ARC Future Fellow Prof. Hongzhi Yin từ tháng 9 năm 2019 đến tháng 9 năm 2021. Các mối quan tâm nghiên cứu của anh bao gồm social computing và network learning. Anh là người chiến thắng Best Research Paper Award tại KDD'23, đây là lần đầu tiên cho Mainland và Hong Kong của Trung Quốc. Anh đã xuất bản 11 CORE A*, 9 CCF A, và 13 SCI (bao gồm 6 IEEE Trans), một số trong đó xuất hiện trong SIGKDD, VLDB, The Web Conference (WWW), TKDE, TOIS, WSDM, TNNLS, CIKM, v.v.

Jiawen Zhang là một sinh viên Ph.D. trong Data Science and Analytics tại HKUST (Guangzhou) dưới sự giám sát của Prof. Jia Li. Cô nhận bằng M.Eng. trong Computer Technology từ Chinese Academy of Sciences. Cô làm việc như một research intern tại Machine Learning Group trong Microsoft Research Asia. Các mối quan tâm nghiên cứu của cô bao gồm data mining và time series modeling.

Xixi Wu là một sinh viên thạc sĩ năm cuối tại School of Computer Science, Fudan University. Cô có được B.S. trong Computer Science từ Fudan University năm 2021. Các mối quan tâm nghiên cứu của cô bao gồm Deep Graph Learning, Data Mining, và Recommender Systems. Cô đã xuất bản nhiều bài báo tác giả đầu tiên tại KDD/WWW/CIKM.

Hong Cheng là một Giáo sư trong Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong. Cô nhận bằng Ph.D. từ University of Illinois at Urbana-Champaign năm 2008. Các mối quan tâm nghiên cứu của cô bao gồm data mining, database systems, và machine learning. Cô nhận research paper awards tại ICDE'07, SIGKDD'06, và SIGKDD'05, và certificate of recognition cho 2009 SIGKDD Doctoral Dissertation Award. Cô nhận 2010 Vice-Chancellor's Exemplary Teaching Award tại Chinese University of Hong Kong.

Yun Xiong là một Giáo sư trong Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University. Cô có được bằng Ph.D. từ Fudan University năm 2008. Các mối quan tâm nghiên cứu của cô bao gồm big data mining và data science. Các thành tựu nghiên cứu của cô bao gồm việc xuất bản hơn 50 bài báo trong các tạp chí và hội nghị quốc tế hàng đầu trong lĩnh vực Data Mining, bao gồm TKDE, KDD, WWW, AAAI, v.v.

Jia Li là một assistant professor tại HKUST (Guangzhou). Anh nhận bằng Ph.D. tại Chinese University of Hong Kong năm 2021. Trước đó, anh làm việc như một full-time data mining engineer tại Tencent từ 2014 đến 2017, và research intern tại Google AI (Mountain View) năm 2020. Các mối quan tâm nghiên cứu của anh bao gồm graph learning và data mining. Một số công trình của anh đã được xuất bản trong TPAMI, ICML, NeurIPS, WWW, KDD, v.v.
