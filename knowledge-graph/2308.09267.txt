# 2308.09267.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-graph/2308.09267.pdf
# File size: 716169 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GraphReason: Enhancing Reasoning Capabilities of Large Language
Models through A Graph-Based Verification Approach
Lang Cao
University of Illinois Urbana-Champaign
Department of Computer Science
langcao2@illinois.edu
Abstract
Large Language Models (LLMs) have show-
cased impressive reasoning capabilities, partic-
ularly when guided by specifically designed
prompts in complex reasoning tasks such as
math word problems. These models typically
solve tasks using a chain-of-thought approach,
which not only bolsters their reasoning abilities
but also provides valuable insights into their
problem-solving process. However, there is
still significant room for enhancing the reason-
ing abilities of LLMs. Some studies suggest
that the integration of an LLM output verifier
can boost reasoning accuracy without necessi-
tating additional model training. In this paper,
we follow these studies and introduce a novel
graph-based method to further augment the rea-
soning capabilities of LLMs. We posit that mul-
tiple solutions to a reasoning task, generated
by an LLM, can be represented as a reasoning
graph due to the logical connections between in-
termediate steps from different reasoning paths.
Therefore, we propose the Reasoning Graph
Verifier (GraphReason) to analyze and verify
the solutions generated by LLMs. By eval-
uating these graphs, models can yield more
accurate and reliable results.Our experimental
results show that our graph-based verification
method not only significantly enhances the rea-
soning abilities of LLMs but also outperforms
existing verifier methods in terms of improving
these models’ reasoning performance.
1 Introduction
Large Language Models (LLMs) have demon-
strated exceptional capabilities in a variety of hu-
man tasks (Zhao et al., 2023). Among the many
abilities LLMs possess, their reasoning capacity
is of paramount importance (Kojima et al., 2023;
Huang and Chang, 2023). This has been substanti-
ated by recent progresses (Wei et al., 2022; Zhou
et al., 2023; Lampinen et al., 2022a). Equipped
with the ability to reason, especially in a multi-
step manner, LLMs can decompose complex prob-
Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?A: Step 1: Janet sells 16 -3 -4 = <<16-3-4=9>>9 duck eggs a day.Step 2:She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.Step3:#### 18…… (More Exemplars) ……Q: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?A: Step 1:It takes 2/2=<<2/2=1>>1 bolt of white fiber.Step2:Sothe total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric.Step3:#### 3Chain-of-thought Reasoning in Math Word Problem
LLM’s Generated SolutionExemplars with Step-by-step SolutionCurrent QuestionFigure 1: An example of chain-of-thought reasoning
in a math word problem, using data from the GSM8K
dataset. Large language models learn from exemplars
that provide step-by-step solutions, subsequently gener-
ating their reasoning path for the current question.
lems into simpler tasks, thereby facilitating their
resolution. In everyday life, many complex tasks
typically require multi-step solutions. A prime ex-
ample of a reasoning task is arithmetic reasoning,
also known as solving math word problems (Zhang
et al., 2019). These math word problems represent
simplified versions of complex real-life situations.
The reasoning ability is inherent in Large Lan-
guage Models (LLMs), but it necessitates specific
methods for manifestation. To activate the robust
reasoning capability of LLMs, the use of specially
designed prompts should be considered. Numerous
methods have been proposed to tap into this poten-
tial, among which chain-of-thought reasoning (Wei
et al., 2022) and in-context learning (Lampinen
et al., 2022b) are two notable approaches. Chain-
of-thought reasoning can elucidate the reasoning
paths during the process. In-context learning fur-arXiv:2308.09267v4  [cs.AI]  21 Apr 2024

--- PAGE 2 ---
nishes LLMs with exemplary cases, thereby en-
abling them to learn from and simulate these ex-
amples for improved results. In the arithmetic rea-
soning scenario, GPT-4 can achieve an accuracy
of 92% on the GSM8K dataset using 5-shot chain-
of-thought prompts (Cobbe et al., 2021a). This
represents a level of difficulty that a bright middle
school student should be capable of handling. As
depicted in Figure 1, this illustrates a multi-step
arithmetic reasoning process in LLMs.
In addition to further training of LLMs and
prompt design, some methods have been proposed
to enhance the reasoning capabilities of LLMs
from the perspective of output verification. The
primary idea is to have LLMs generate reasoning
paths multiple times, and then design a verifier
to evaluate these paths and deliver the final re-
sults. (Wang et al., 2023) introduces the concept of
self-consistency , based on the intuition that a com-
plex reasoning problem usually allows for multiple
thought processes, all leading to a unique correct
answer. (Li et al., 2023) also proposes All Roads
Lead to Rome , which introduces a step-aware veri-
fier to analyze reasoning paths not just through the
entire path, but at every step. However, both meth-
ods treat each reasoning path as an independent
entity and do not consider the potential interrela-
tion and interaction between different reasoning
paths. Once reasoning paths are disassembled into
steps, intermediate steps from one path may bear
reasoning relations to other reasoning paths. These
methods do not perceive all LLM outputs for a
given input as a collective entity, thereby failing to
analyze the internal relations of all candidate paths
in depth.
Inspired by these observations, we propose Rea-
soningGraph Verifier ( GraphReason ) in this pa-
per. We posit that reasoning paths of one question
can form reasoning graphs, where similar interme-
diate reasoning steps can be merged into the same
node. With a graph structure, we can more effec-
tively model and capture the reasoning logic be-
tween intermediate steps from different reasoning
paths. Specifically, we first construct a reasoning
graph based on all outputs from LLMs, and then
train a verifier to learn the relationship between the
graph structure and the final answer. During the
prediction stage, we process the data in the same
way as in the training stage, and use the verifier
to evaluate each reasoning graph. We then select
the reasoning graph with the highest score, using
its answer as the final answer. To the best of ourknowledge, we are the first to approach reason-
ing logic of LLMs from a graph perspective. We
conduct extensive experiments to demonstrate the
improvements over the original LLMs, and show
that our method outperforms other verifiers.
In summary, our contributions are as follows:
•We propose a graph-based verification method,
GraphReason , aimed at significantly enhancing
the reasoning capabilities of large language mod-
els without the need for additional training of
LLMs.
•We establish an arithmetic reasoning benchmark
using three Math Word Problem datasets to illus-
trate the fundamental reasoning performance of
large language models, and to provide a fair com-
parison of the performance of various existing
verifiers.
•Our experimental results indicate that the method
proposed in this paper outperforms other en-
hancement methods. We also provide an exten-
sive analysis of the limitations and future poten-
tial of GraphReason .
2 Related Works
Reasoning of Fine-tuning Models has been ex-
tensively studied. It focuses on addressing reason-
ing tasks using a general sequence-to-sequence ap-
proach, enhanced by reasoning-aware pre-training
or fine-tuning of language models. (Cobbe et al.,
2021a) proposed training a verifier to rank solutions
sampled from fine-tuned language models. (Yoran
et al., 2022; Wang et al., 2022) suggested equip-
ping language models with reasoning abilities by
generating training examples with human-designed
templates. (Pi et al., 2022) proposed injecting rea-
soning capabilities into language models by contin-
ually pre-training on program execution data.
Several studies have focused on imbuing PLM
with reasoning ability for specific tasks, such as
arithmetic reasoning (Cobbe et al., 2021a; Miao
et al., 2020; Patel et al., 2021), commonsense
reasoning (Talmor et al., 2019), and inductive
reasoning (Sinha et al., 2019). For instance,
various strategies have been proposed to improve
language models’ performance on arithmetic
reasoning tasks, often referred to as math word
problems. (Xie and Sun, 2019) proposed a
tree-structured decoder to generate an equation
tree, while (Zhang et al., 2020) applied graph
convolutional networks to extract relationships

--- PAGE 3 ---
of quantities in math problems. (Li et al., 2022)
used contrastive learning to better learn patterns
in math word problems. However, (Valmeekam
et al., 2023; Rae et al., 2022) suggested that
reasoning, particularly multi-step reasoning, is
often a weakness in language models and other
NLP models.
Reasoning of Large Language Models has gar-
nered significant attention and demonstrated im-
mense potential. Recent advancements in LLMs
suggest that the ability for multi-step reasoning is
already embedded within these large-scale models
(Kojima et al., 2023; Huang and Chang, 2023),
such as PaLM (Chowdhery et al., 2022), GPT-
4 (OpenAI, 2023). Therefore, providing an ade-
quate prompt is sufficient to utilize this reasoning
ability. For example, the prompting method pro-
posed by (Kojima et al., 2023; Wei et al., 2022),
which is based on a chain-of-thought, could aid
LLMs in generating text with arithmetic reasoning
and common factual knowledge. Following (Wei
et al., 2022), experiments on current language mod-
els demonstrated that chain-of-thought prompting
could enhance the accuracy of solving math prob-
lems from 18% to 57%. (Lampinen et al., 2022b)
included explanations in the in-context examples
and tested the influence of explanations by evalu-
ating the score between explain-then-predict and
predict-then-explain . Moreover, (Zhou et al., 2023)
suggested a two-stage prompting strategy, least-
to-most prompting, which breaks down a complex
problem into a series of subproblems and solves
them step-by-step. (Li et al., 2023) proposed sam-
pling multiple times from diverse prompts to en-
hance the variety of responses.
In addition to designing prompts, adopting ad-
ditional strategies like verifier has contributed to
enhancing the performance of reasoning abilities
of large language models. For instance, (Wang
et al., 2023) proposes self-consistency , which in-
volves sampling different reasoning paths from the
language model, and then returning the most con-
sistent final answer via majority voting. (Li et al.,
2023) used a step-aware voting verifier to enhance
the reasoning ability of LLMs from two perspec-
tives. These methods strive to augment the rea-
soning abilities or yield superior reasoning results
without necessitating additional training of LLMs.
Our work continues this research direction, with a
specific focus on developing a novel graph-based
verifier to boost the reasoning capabilities of LLMs.3 Methodology
3.1 GraphReason Framework
Problem 1 (Reasoning to Solve Problems)
Given a set of nmath word problems
Q={Q1, Q2, ..., Q n}, where each Qiis
represented by the text description of a single
math word problem, the goal of reasoning to
solve math word problems is to generate the
answers A={A1, A2, ..., A n}for these problems.
Here, each Airepresents the generated text of the
corresponding answer. During the process of large
language models generating answers, a set of n
reasoning paths for solutions S={S1, S2, ..., S n}
is also produced. Each solution Siis represented
asSi={Q, Step 1, Step 2, ..., Step l, A}, where
eachStep idenotes the intermediate steps in the
step-by-step solutions.
We propose GraphReason to verify the solutions
generated by LLMs in order to improve the final
answer accuracy. This method is a graph-based ver-
ification technique that analyzes reasoning paths
from generated solutions from a graph perspective.
The final answer is obtained without modifying
the original LLMs, functioning much like a plugin.
As illustrated in Figure 2, there are two steps in
the training stage: Graph Construction andGraph
Classification . In the Graph Construction step, we
obtain the generated solution from LLMs with the
specific designed prompt and group them accord-
ing to their final answers. We split reasoning paths
by steps and then merge intermediate steps with
identical expression to form reasoning graphs. In
theGraph Classification step, we classify these
reasoning graphs with the additional feature of the
sum of scores from the base verifier to train the
integrated verifier model. In the prediction stage,
the candidate solutions are first generated by LLMs.
We process them in the same manner as in the train-
ing stage, then we use trained verifier to evaluate
the scores of each candidate solution. The best so-
lution, denoted by the highest score, is selected as
the final predicted answer. We will now provide a
detailed introduction to the entire process.
3.2 Prompt Design
To improve the output of Language Models (LLMs)
in providing solutions, it is essential to design ef-
fective prompts. We incorporate chain-of-thought
and in-context learning to enable LLMs to generate
step-by-step answers for math word problems. The
language models generate output ybased on the

--- PAGE 4 ---
TrainingStagePredictionStageBase Verifier1 or 0Generated Solutions with the Same Answer AGraphConstructorGNNClassifierSumConcatReasoningGraphVerifierScore1Score2ScorenFinalAnswerExemplars&Question
Large Language Models
Generated Solutions with the same Answer
Scores of Every SolutionReasoning GraphArgmaxFigure 2: The framework of GraphReason . In the training stage, GraphReason processes generated solutions from
LLMs to construct reasoning graphs, and then trains a verifier to judge them according to graph classification. In
the prediction stage, GraphReason evaluates candidate solutions to assign a score, and selects the solution with the
highest score as the final answer.
inputxusing the following equation:
p(y|C,x) =|y|Y
t=1pLM(yt|C,x,y< t),(1)
where, Crepresents the input provided to the LLMs
prior to the current math word problem’s question.
Cis a concatenation of kexemplars, denoted as:
C= [(Q1, S1, A1); (Q2, S2, A2), ...; (Qk, Sk, Ak)],
(2)
where, Qirepresents the question, Sirepresents
the intermediate steps of the solution, and Airep-
resents the answer. We set kto five in this study,
resulting in a prompt that consists of five question-
answer pairs sampled from the training split of a
math word problem dataset. Therefore, the prompt
can be denoted as:
Prompt = [C;Q], (3)
where Qrepresents the question of the current math
word problem.
Using a greedy decoding approach to sample
one output from LLMs may not be robust. It can
lead to instability and occasional errors. To address
this, (Wang et al., 2023) propose the concept of
self-consistency . This approach involves sampling
different reasoning paths from the language model
and then selecting the most consistent final answer
through majority voting. Instead of using greedy
decoding to sample only once and verify, they uti-
lize sampling decoding to sample N1times. We
also follow the idea presented by (Li et al., 2023)
in their work named All Roads Lead to Rome . This
approach involves generating N2diverse prompts
for LLMs to produce multiple outputs. By employ-
ing multiple sampling decodes on diverse prompts,
we can obtain generated solutions from different
sources. Specifically, we obtain N=N1×N2diverse reasoning paths for each question. In our
main experiments, we set N1= 10 andN2= 3.
These solutions will be further processed and veri-
fied using our designed verifier.
3.3 Reasoning Graph Construction
After generating multiple solutions for a question,
it becomes necessary to construct reasoning graphs
based on the reasoning paths taken by these solu-
tions.
As shown in Figure 3, we begin by grouping all
the generated solutions for a particular question ac-
cording to their final answer. Since these solutions
originate from the same question, their reasoning
paths will share the same starting point. Similarly,
solutions with the same final answer will have the
same endpoint, as their reasoning paths converge.
Therefore, a group of generated solutions with the
same final answer can form a reasoning graph with
a uniform start node (question node) and end node
(answer node). We define this division process as
follows:
S={SA1, SA2, ..., S An}, (4)
where Srepresents the set of generated solutions
for a question, and SAi={S1, S2, ..., S m}is the
subset of generated solutions that all have the same
final answer Ai.
For each subset of generated solutions SAi, we
construct a reasoning graph. This construction is
motivated by the understanding that each step in the
reasoning path of a generated solution does not ex-
ist in isolation from the other solutions. The steps
from one solution’s reasoning path can impact the
steps from another solution, enhancing the overall
reasoning process. We utilize the graph structure
to model and capture these relationships between
steps from different solutions. As the different rea-
soning paths can benefit each other, we construct

--- PAGE 5 ---
Exemplars&QuestionLargeLanguageModelSolution 1Generated Solutions with the Same Answer A1Construct Reasoning Graph according to Reasoning Paths
Generated Solutions with the same Answer A2
Generated Solutions with the same Answer AnQuestionS1Step1S1Step2S1StepLFinal AnswerS2Step1S2Step2S2StepLSmStep1SmStep2SmStepLGraph2QuestionNodeMergetheSameStepNodes
Graphn
Solution 2Solution mDetailed Process
Answer NodeGraph ConstructorFigure 3: The graph constructor in GraphReason . We detail the process of transforming ‘Generated Solutions with
the Same Answer A1’ to ‘Graph 1’.
QuestionS1Step1S1Step2S1Step3Final AnswerS2Step1S2Step2S2Step3If S1.Step2 equals S2.Step2Graph1Status1
QuestionS1Step1StepNodeS1Step3Final AnswerS2Step1S2Step3Graph1Status2
Figure 4: The process of reasoning graph construction.
The primary operation here is the merging of identical
intermediate steps in reasoning paths into a single graph
node.
a reasoning graph to link these paths together. As
shown in Figure 4, the primary operation here is
the merging of identical intermediate nodes in rea-
soning paths into a single graph node. We first
compare the reasoning steps from any two solution
reasoning paths. If they have the same intermediate
steps of arithmetic expression, we merge them into
the same node, and if they differ, we do not. For
reasoning math word problems here, we define rea-
soning steps as the current arithmetic expression
without other language text in the current reasoning
step for clarity. It can help us simplify construction
of reasoning graphs in the reasoning task. The de-
tailed algorithm for constructing a reasoning graph
is shown in Algorithm 1.
The generated solutions, divided by their finalanswers {SA1, SA2, ..., S An}, can be transformed
intonreasoning graphs of generated solutions
{GA1, GA2, ..., G An}.
Regarding the node features in the graph, we
select the score from the Base Verifier and the node
degree. We believe the score from the Base Verifier
encapsulates the semantic information of solutions,
and the node degree contains information about
the graph structure. The Base Verifier is trained
independently from the whole framework. It is de-
signed to judge whether a single reasoning path of
one solution is correct, which is a binary text classi-
fication task. After training, it can be used to verify
any single solution and assign a score∈(0,1)to
evaluate the likelihood of the solution being cor-
rect, where score = 0.99suggests a 99% prob-
ability of the solution being correct. We use the
score from the Base Verifier to better incorporate
solution semantic information because, according
to our experiments, it is challenging to model se-
mantic information while modeling reasoning logic
information. The score of a step is the same as its
solution score. Therefore, for one step node V, it
has many scores {scorea, scoreb, ..., scorec}from
different solutions. The feature of one node Viin
the graph is then concatenated by the selected fea-
ture, which can be represented as:
V= [scoremean
i, scoremax
i, scoremin
i,
scorenum
i, in_degree i],(5)
where V∈R5,scoremean
i is the mean of all scores
of one step Vi,scoremax
i is the maximum score,
scoremin
iis the minimum score, scorenum
i is the
number of scores, and in_degree iis the in-degree
of the step node Vi.

--- PAGE 6 ---
Algorithm 1 Reasoning graph construction algo-
rithm
Input : generated solutions SAiwhich have the
same final answers
Output : a reasoning graph GAi
1:node _num←0
2:node 2id←dict()
3:edges ←list()
4:foreachreason _path inSAido
5: foreachstep inreason _path do
6: ifstep not in node 2id.keys ()then
7: node 2id[step]←node _num
8: node _num←node _num + 1
9: end if
10: end for
11:end for
12:foreachreason _path inSAido
13: foreachstep inreason _path do
14: start _node←node 2id[last_step]
15: end_node←node 2id[step]
16: if(start _node, end _node )not in edges
then
17: edges.add ((start _node, end _node ))
18: end if
19: last_step←step
20: end for
21:end for
22:GAi←graph (node 2id, edges )
In this way, we can obtain multiple reasoning
graphs to represent all generated solutions from
LLMs for a single math word problem question.
3.4 Verifier Design
Our designed verifier GraphReason ,is used to eval-
uate the answer of a generated solutions group,
which is also represented as a reasoning graph.
This verifier has two inputs: the graph and the sum
of solution scores. We employ the Graph Isomor-
phism Network (GIN) (Xu et al., 2019) to perform
node feature propagation, thereby encoding the in-
formation from the reasoning graphs we obtained.
The node feature is propagated and aggregated as
follows:
h(k)
v=MLP(k) 
(1 +ε(k))·h(k−1)
v +X
u∈N(v)h(k−1)
u
,
(6)
where h(k)
vrepresents the state of node vafter the
kthupdate. MLP(k)refers to a multi-layer percep-
tron in the kthlayer. N(v)denotes all the neigh-
bors of node vandεis a learnable parameter. Then,we perform a sum readout to obtain the representa-
tion of the reasoning graph:
hG=X
v∈Gh(k)
v, (7)
where hG∈R5. We set kto 3, signifying the
application of three layers of GIN. Concurrently,
the sum of the scores of solutions with the same
final answer, Ai, denoted as score Ai, is represented
as follows:
score A=X
i∈SAscore i. (8)
Then a reasoning graph can then be represented as:
G= [hG, score A], (9)
where G∈R6.
The target label of the graph y∈ {0,1}indicates
whether the final answer matches the correct final
answer. We compute the loss and train the verifier
model by:
L=nX
i=1LBCE(label i, f(Gi)), (10)
where irepresents the number of solution subset
among all nsubsets after grouping solutions. The
corresponding reasoning graph for this subset is
denoted by Gi, andf()is a linear classifier.
3.5 Answer Verification
During the prediction stage, all generated solutions
are processed in the same way as in the training
stage. The trained verifier is then used to evaluate
the scores of each reasoning graph, each of which
represents a group of solutions that yield the same
final answer. The final answer associated with the
highest score is selected as our final predicted an-
swer:
ˆy=Answer [arg max
iscore i], (11)
where score idenotes the score of the reasoning
graph Gi, as determined by our verifier. Answer
represents the list of all candidate final answers.
By predicting the number of the optimal reasoning
graph, we can determine the final predicted result
of the current reasoning task.

--- PAGE 7 ---
GSM8K SV AMP ASDiv-a StrategyQA
Fine-tuning SOTA 57a57.4b75.3c73.9d
9–12 year olds 60 - - -
gpt-3.5-turbo:
Greedy Decode 72.7 78.7 93.0 65.0
Self-Consistency (V oting) 82.3 82.9 95.6 66.0
Verifier 66.9 73.1 92.8 69.3
V oting Verifier 85.4 84.8 96.9 70.7
DIVERSE (Step-aware V oting Verifier) 85.0 85.1 96.8 66.9
Reasoning Graph Verifier (Ours) 85.7 85.4 97.0 71.2
Table 1: The comparison experiment results of GraphReason , other verifiers, and other baselines. We primarily
compare GraphReason with other verifiers which are all based on the same generated solutions from gpt-3.5-turbo
4 Experiments
In this section, we conducted extensive experiments
to demonstrate the performance of GraphReason ,
along with a more in-depth analysis. Universally,
we reproduced all types of verifiers to report their
results based on the same generated solutions. Our
experiments are conducted in two settings: Arith-
metic Reasoning and Commonsense Reasoning.
We ensured a fair comparison by setting the same
random seed, using the same hardware environ-
ment, and applying similar hyperparameters. We
used accuracy as the metric to evaluate the ability
of solving math word problems, which determines
whether the final answer is correct or not.
4.1 Training Details
For LLMs sampling, we use gpt-3.5-turbo as our
base LLMs and set the temperature tto 1. All
verifiers use the same LLMs’ output. Regarding
verifier training, we fine-tune on bert-base-uncased
(Devlin et al., 2019). We employ the AdamW op-
timizer (Loshchilov and Hutter, 2019) to optimize
the model parameters during training. We apply
differential learning rates, setting the learning rate
of the final linear classifier to 4e-2, while the other
graph neural network layers are set to 4e-3. The
activation layer between them is ReLU (Agarap,
2019). The batch size in each training step is set
to 2. The batch size is small because the verifier
needs to verify multiple reasoning graphs for a sin-
gle question.
To ensure a fair comparison between the V ot-
ing Verifier, Simple Verifier, and GraphReason , we
use the same trained base verifier for all three ap-
proaches.
The details of the datasets and baselines are pro-
vided in Appendix A and Appendix B, respectively.4.2 Main Results
We present the main results in Table 1. As can
be seen from the table, GraphReason significantly
enhances the original gpt-3.5-turbo ’s reasoning
abilities across all three datasets, for instance, im-
proving accuracy by 13.0% (72.7% →85.7%) on
GSM8K. It is also evident that our method sur-
passes other verifier methods with the same output
from LLMs and achieves the state-of-the-art on all
three datasets.
Additionally, the Step-aware V oting Verifier im-
proves upon the V oting Verifier by recognizing
that not all steps in an incorrect reasoning path
are equally erroneous, and some steps may still be
useful for reasoning. We believe this hypothesis
is overly simplistic and cannot describe complex
logical relationships among steps. According to Ta-
ble 1, it leads to some metric decline, and the same
finding also observed in the original paper. Further-
more, it does not perform well in the StrategyQA
task, because there are no gold reasoning paths for
the training of this commonsense reasoning task.
In this task, the reasoning paths are generated and
pseudo, indicating a requirement for gold labels at
each step of the reasoning process. However, our
paper consistently improves upon the V oting Veri-
fier by considering complex relationship between
different reasoning paths through reasoning graphs.
We enhance the previous method, which did not
consider relations in steps between different solu-
tions, by 0.3% (85.4% →85.7%), 0.3% (85.1% →
85.4%), 0.1% (96.9% →97.0%), and 0.5% (70.7%
→71.2%) across the four datasets.
Moreover, GraphReason yields only a slight im-
provement in performance on ASDiv-a, and the
results are nearly identical. One reason for this is
that the math word problems from ASDiv-a are sim-

--- PAGE 8 ---
GSM8K ▽ SV AMP ▽ ASDiv-a ▽
Reasoning Graph Verifier (Ours) 85.7 - 85.4 - 97.0 -
w/o solution semantic from base verifier 81.2 -4.5 83.1 -2.3 94.3 -2.7
w/o solution scores sum 82.8 -2.9 83.2 -2.2 95.6 -1.4
w/o reasoning graphs 85.4 -0.3 84.8 -0.6 96.9 -0.1
Table 2: The ablation experiment results of GraphReason . Missing each component leads to a decline in the final
result.
pler compared to those in the other two datasets,
based on our observations. In most cases, these
problems do not require complex reasoning from
a graph perspective to generate a satisfactory an-
swer. It demonstrates that our method is particu-
larly well-suited for such situations. We believe
thatGraphReason can offer more substantial im-
provements in the more complex scenario.
4.3 Ablation Study
We conducted an ablation study to evaluate the im-
pact of each component on the overall performance
of our method. Table 2 presents the results of this
study, highlighting how these modules contribute to
the improvement of the base model in distinct ways.
It can be observed that the omission of any com-
ponent leads to a decline in the final result. The
solution semantics from the base verifier appear
to be most crucial to GraphReason . The current
method still relies on semantic information, which
is reasonable since reasoning steps from different
solutions require semantic information for better
reasoning. We also notice that reasoning graphs
bring a slight improvement to the entire method,
thereby proving effectiveness of graph structure.
The improvement is not substantial because we do
not model the graph structure and semantic infor-
mation simultaneously, and create a training gap
here. Another essential factor is the complexity of
graph classification, compounded by the presence
of noise and limitations in our training data.
4.4 GraphReason with Different LLMs
To evaluate the compatibility of GraphReason and
its effectiveness across various models, we addi-
tionally include gpt-4 (OpenAI, 2023) and PaLM-2
(Google, 2023) in our experiments. Given our lim-
ited computing resources, we utilize the same train-
ing data previously sampled from gpt-3.5-turbo .
For testing in the GSM8K task, we select samples
from 100 pieces of data from gpt-4 andPaLM-2gpt-3.5-turbo gpt-4 PaLM-2
Greedy Decode 72.7 87.0 53.0
V oting 82.3 94.0 71.0
Simple Verifier 66.9 89.0 36.0
V oting Verifier 85.4 97.0 77.0
DIVERSE 85.0 97.0 75.0
Ours 85.7 94.0 78.0
Table 3: The experimental results of GraphReason with
different LLMs.
respectively. We conduct the sampling 10 times
using three types of five exemplars, maintaining
the same settings as in our previous experiments.
Our method aims to enhance the original reasoning
capabilities. Therefore, we do not include small-
sized LMs, which typically exhibit weaker reason-
ing abilities.
From Table 3, it is evident that our method en-
hances the original reasoning performance of both
GPT-4 andPaLM-2 . However, there is a perfor-
mance decline in gpt-4 when compared with the
best baselines. The performance of GraphReason
is comparable to that of the voting method. We
hypothesize that this is because the reasoning pat-
terns of GPT-4 differ from those of GPT-3.5-Turbo ,
and our verifier is trained specifically on GPT-3.5-
Turbo samples in this setting.
5 Conclusion
In this paper, we propose GraphReason , a novel
and general method to enhance the reasoning abili-
ties of large language models. Our method is the
first to approach reasoning logic of large language
models from a graph perspective and verifies candi-
date reasoning paths accordingly. We demonstrate
the superiority of GraphReason through extensive
experiments.

--- PAGE 9 ---
Limitations
There are several limitations in the current research
that contribute to performance that is not as good
as expected:
•Computing Resources . Despite the impressive
performance it achieves, our framework requires
large language models like GPT3.5. Inference
with these models is more time-consuming and
costly than fine-tuning models like BERT(Devlin
et al., 2019). Some experiments, such as hyper-
parameter analysis, have already been conducted
in related previous work and are not replicated
here. Furthermore, due to limited computing re-
sources, we have not conducted experiments with
additional LLMs. We have chosen solely to use
the representative LLM, GPT3.5, to compare the
performance of the verifiers.
•Labeled CoT data .GraphReason is a complex
verifier method that builds on graph classifica-
tion, which requires more labeled data with well-
annotated chain-of-thought reasoning paths for
training. In the training of GraphReason , we use
reasoning paths from LLMs’ output which may
introduce significant noise. If the training data
included labeled reasoning graphs, the perfor-
mance would improve significantly.
•Other Reasoning Tasks . There are many types
of reasoning tasks beyond math word problems,
such as Commonsense Reasoning (Talmor et al.,
2019), Inductive Reasoning (Sinha et al., 2019),
etc. Given that graph construction is a complex
process, we have focused mainly on solving math
word problems (Arithmetic Reasoning). This fo-
cus allows for a more convenient implementation
of the merging of intermediate steps. In other
cases, identifying similar steps can be challeng-
ing. On the other hand, a math word problem
typically presents a greater variety of potential
solutions.
Nevertheless, we believe that future studies, con-
ducted by us or others, can overcome these limita-
tions and further improve upon our approach.
References
Abien Fred Agarap. 2019. Deep learning using rectified
linear units (relu). Preprint , arXiv:1803.08375.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. Preprint , arXiv:2204.02311.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021a. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021b. Training verifiers to solve math word prob-
lems. Preprint , arXiv:2110.14168.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. Preprint , arXiv:1810.04805.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did Aristo-
tle Use a Laptop? A Question Answering Bench-
mark with Implicit Reasoning Strategies. Transac-
tions of the Association for Computational Linguis-
tics (TACL) .
Google. 2023. Palm 2 technical report. Preprint ,
arXiv:2305.10403.
Jie Huang and Kevin Chen-Chuan Chang. 2023. To-
wards reasoning in large language models: A survey.
Preprint , arXiv:2212.10403.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2023. Large
language models are zero-shot reasoners. Preprint ,
arXiv:2205.11916.
Andrew Lampinen, Ishita Dasgupta, Stephanie Chan,
Kory Mathewson, Mh Tessler, Antonia Creswell,
James McClelland, Jane Wang, and Felix Hill. 2022a.
Can language models learn from explanations in con-
text? In Findings of the Association for Computa-
tional Linguistics: EMNLP 2022 , pages 537–563,

--- PAGE 10 ---
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Andrew K Lampinen, Ishita Dasgupta, Stephanie CY
Chan, Kory Matthewson, Michael Henry Tessler,
Antonia Creswell, James L McClelland, Jane X
Wang, and Felix Hill. 2022b. Can language models
learn from explanations in context? arXiv preprint
arXiv:2204.02329 .
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2023. Making
language models better reasoners with step-aware
verifier. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 5315–5333, Toronto,
Canada. Association for Computational Linguistics.
Zhongli Li, Wenxuan Zhang, Chao Yan, Qingyu Zhou,
Chao Li, Hongzhi Liu, and Yunbo Cao. 2022. Seek-
ing patterns, not just memorizing procedures: Con-
trastive learning for solving math word problems.
InFindings of the Association for Computational
Linguistics: ACL 2022 , pages 2486–2496, Dublin,
Ireland. Association for Computational Linguistics.
Ilya Loshchilov and Frank Hutter. 2019. De-
coupled weight decay regularization. Preprint ,
arXiv:1711.05101.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and developing
english math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 975–984.
OpenAI. 2023. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2080–2094, Online.
Association for Computational Linguistics.
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin,
Qiang Fu, Yan Gao, Jian-Guang Lou, and Weizhu
Chen. 2022. Reasoning like program executors. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages 761–
779, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Jonathan Uesato, John Mellor, Irina Higgins, Anto-
nia Creswell, Nat McAleese, Amy Wu, Erich Elsen,Siddhant Jayakumar, Elena Buchatskaya, David Bud-
den, Esme Sutherland, Karen Simonyan, Michela Pa-
ganini, Laurent Sifre, Lena Martens, Xiang Lorraine
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya, Domenic Donato, Angeliki Lazaridou,
Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-
tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, Yujia
Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
Aidan Clark, Diego de Las Casas, Aurelia Guy,
Chris Jones, James Bradbury, Matthew Johnson,
Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed Lockhart, Simon Osindero, Laura
Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,
Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-
ray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling
language models: Methods, analysis & insights from
training gopher. Preprint , arXiv:2112.11446.
Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle
Pineau, and William L. Hamilton. 2019. CLUTRR:
A diagnostic benchmark for inductive reasoning from
text. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
4506–4515, Hong Kong, China. Association for Com-
putational Linguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Karthik Valmeekam, Alberto Olmo, Sarath Sreedha-
ran, and Subbarao Kambhampati. 2023. Large lan-
guage models still can’t plan (a benchmark for llms
on planning and reasoning about change). Preprint ,
arXiv:2206.10498.
Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu
Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan
Duan. 2022. Logic-driven context extension and data
augmentation for logical reasoning of text. In Find-
ings of the Association for Computational Linguis-
tics: ACL 2022 , pages 1619–1629, Dublin, Ireland.
Association for Computational Linguistics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. 2023. Self-consistency improves chain
of thought reasoning in language models. Preprint ,
arXiv:2203.11171.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .

--- PAGE 11 ---
Zhipeng Xie and Shichao Sun. 2019. A goal-driven
tree-structured neural model for math word problems.
InProceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence, IJCAI-19 ,
pages 5299–5305. International Joint Conferences on
Artificial Intelligence Organization.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie
Jegelka. 2019. How powerful are graph neural net-
works? In International Conference on Learning
Representations .
Ori Yoran, Alon Talmor, and Jonathan Berant. 2022.
Turning tables: Generating examples from semi-
structured tables for endowing language models with
reasoning skills. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 6016–6031,
Dublin, Ireland. Association for Computational Lin-
guistics.
Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian
Dai, and Heng Tao Shen. 2019. The gap of semantic
parsing: A survey on automatic math word problem
solvers. Preprint , arXiv:1808.07290.
Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan
Wang, Jie Shao, and Ee-Peng Lim. 2020. Graph-to-
tree learning for solving math word problems. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 3928–
3937, Online. Association for Computational Lin-
guistics.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang
Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023. A survey of large language models. Preprint ,
arXiv:2303.18223.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi.
2023. Least-to-most prompting enables complex
reasoning in large language models. Preprint ,
arXiv:2205.10625.
A Datasets
We compared GraphReason with other methods
on three different math word problem datasets:
GSM8K (Cobbe et al., 2021a), SV AMP (Patel
et al., 2021), and ASDiv-a (Miao et al., 2020) and
one commonsense reasoning dataset: StrategyQA
(Geva et al., 2021). We selected the subset ASDiv-a
(arithmetic) from the original dataset ASDiv, which
only involves arithmetic operations.
These three arithmetic reasoning datasets are
more challenging than other math word problem
datasets, making them more suitable for testingthe reasoning capability of LLMs with a verifier.
As the GSM8K dataset is the only one providing
step-by-step solutions as chain-of-thought exem-
plars, we chose exemplars from the GSM8K train-
ing dataset and tested them on all three datasets.
Additionally, the training data for the verifier also
used the GSM8K training data. In this setting, we
could also demonstrate the transfer learning and
generalization ability of our method. The size of
the training split from GSM8k is 1000. The test
data sizes for GSM8K, SV AMP, and ASDiv-a are
1319, 1000, and 1218, respectively.
In the StrategyQA commonsense reasoning task,
we set the number of exemplars to 8 and select
pseudo-exemplars from (Li et al., 2023). Addition-
ally, we conduct five sampling iterations for each
context of LLMs. From the entire dataset, we se-
lect a subset of 1,000 instances, allocating 700 for
training and 300 for testing.
B Baselines
In our evaluation, we consider the following base-
lines:
•Greedy Decode is a simple method that uses a
greedy decoding strategy to sample once.
•Self-Consistency (Voting) (Wang et al., 2023)
samples multiple times and selects the final an-
swers based on majority voting.
•Simple Verifier (Cobbe et al., 2021b), which is
also known as the Sampling and Re-ranking strat-
egy, uses a verifier to assign scores to sampled
solutions and selects the final answer with the
highest score.
•Voting Verifier (Li et al., 2023) combines the
V oting and Verifier approaches. It assigns total
scores to answers from scores of all candidate
solutions and selects the final answer with the
highest score.
•DIVERSE (Step-aware Voting Verifier) (Li
et al., 2023), which is the state-of-the-art method,
considers the reasoning steps throughout the en-
tire reasoning path. It recognizes that not all
steps in an incorrect reasoning path are equally
wrong and that some steps may still be useful for
reasoning.
We primarily compare GraphReason with other
verifiers using the same generated solutions from
gpt-3.5-turbo . Additionally, we include some previ-
ous Fine-tuning state-of-the-art methods to reflect

--- PAGE 12 ---
the strong reasoning ability of LLMs. The previous
Fine-tuning SOTA methods are denoted as follows:
a: (Cobbe et al., 2021b), b: (Pi et al., 2022), c:
(Miao et al., 2020), d: (Chowdhery et al., 2022).
