# Bộ Sưu Tập Flan: Thiết Kế Dữ Liệu và Phương Pháp cho Điều Chỉnh Hướng Dẫn Hiệu Quả

Shayne Longpre∗ Le Hou Tu Vu Albert Webson Hyung Won Chung
Yi Tay Denny Zhou Quoc V. Le Barret Zoph Jason Wei Adam Roberts
Google Research

Tóm tắt
Chúng tôi nghiên cứu các quyết định thiết kế của các phương pháp điều chỉnh hướng dẫn công khai, và phân tích sự phát triển của các mô hình Flan 2022 (Chung et al., 2022). Thông qua các nghiên cứu loại bỏ cẩn thận trên Bộ Sưu Tập Flan các tác vụ và phương pháp điều chỉnh hướng dẫn, chúng tôi tách biệt tác động của các quyết định thiết kế cho phép Flan-T5 vượt trội hơn các nghiên cứu trước đó 3-17%+ qua các cài đặt đánh giá. Chúng tôi phát hiện kỹ thuật cân bằng tác vụ và làm giàu bị bỏ qua nhưng quan trọng cho điều chỉnh hướng dẫn hiệu quả, và đặc biệt, huấn luyện với các cài đặt nhắc nhở hỗn hợp (zero-shot, few-shot, và chain-of-thought) thực sự mang lại hiệu suất mạnh hơn (2%+) trong tất cả các cài đặt. Trong các thí nghiệm tiếp theo, chúng tôi cho thấy Flan-T5 yêu cầu ít điều chỉnh hơn để hội tụ cao hơn và nhanh hơn T5 trên các tác vụ downstream đơn lẻ—thúc đẩy các mô hình được điều chỉnh hướng dẫn như các checkpoint khởi đầu hiệu quả hơn về mặt tính toán cho các tác vụ mới. Cuối cùng, để đẩy nhanh nghiên cứu về điều chỉnh hướng dẫn, chúng tôi công khai bộ sưu tập Flan 2022 gồm dữ liệu, mẫu và phương pháp.

1 Giới thiệu
Các mô hình ngôn ngữ lớn như PaLM (Chowdhery et al., 2022), Chinchilla (Hoffmann et al., 2022), và ChatGPT cùng những mô hình khác (Brown et al., 2020; Ouyang et al., 2022) đã mở khóa các khả năng mới trong việc thực hiện các tác vụ xử lý ngôn ngữ tự nhiên (NLP) từ việc đọc các lời nhắc hướng dẫn. Nghiên cứu trước đó đã cho thấy điều chỉnh hướng dẫn—điều chỉnh mô hình ngôn ngữ trên một bộ sưu tập các tác vụ NLP được định dạng với hướng dẫn—tiếp tục nâng cao khả năng của các mô hình ngôn ngữ thực hiện một tác vụ chưa thấy từ một hướng dẫn (Wei et al., 2021; Sanh et al., 2021; Min et al., 2022).

Trong nghiên cứu này, chúng tôi đánh giá các phương pháp và kết quả của các nỗ lực tổng quát hóa hướng dẫn mã nguồn mở, so sánh các kỹ thuật và phương pháp điều chỉnh của chúng. Và đặc biệt, chúng tôi xác định và đánh giá các cải tiến phương pháp luận quan trọng trong "Bộ Sưu Tập Flan 2022", thuật ngữ chúng tôi sử dụng cho bộ sưu tập dữ liệu và phương pháp tăng cường dữ liệu và điều chỉnh hướng dẫn, lần đầu được triển khai và sử dụng trong Chung et al. (2022). Trong khi Chung et al. (2022) tập trung vào các kết quả mới nổi và hiện đại nhất của việc kết hợp Flan 2022 với PaLM 540B, nghiên cứu này tập trung vào chi tiết của các phương pháp điều chỉnh hướng dẫn, loại bỏ các yếu tố riêng lẻ, và so sánh chúng trực tiếp với nghiên cứu trước bằng cách giữ kích thước mô hình tiền huấn luyện và checkpoint nhất quán.

Bộ Sưu Tập Flan 2022 cung cấp tập hợp các tác vụ và phương pháp điều chỉnh hướng dẫn công khai rộng rãi nhất, mà chúng tôi đã biên soạn tại một nơi. Chúng tôi cũng đã bổ sung điều này với hàng trăm mẫu chất lượng cao của riêng mình, các mẫu định dạng phong phú hơn, và tăng cường dữ liệu. Chúng tôi cho thấy một mô hình được huấn luyện trên bộ sưu tập này vượt trội so với các bộ sưu tập công khai khác trên tất cả các benchmark đánh giá được thử nghiệm, bao gồm Flan 2021 gốc (Wei et al., 2021), T0++ (Sanh et al., 2021), Super-Natural Instructions (Wang et al., 2022c), và nghiên cứu đồng thời về OPT-IML (Iyer et al., 2022). Như được thể hiện trong Hình 1, điều này bao gồm cải thiện 4.2%+ và 8.5% trên các benchmark đánh giá MMLU (Hendrycks et al., 2020) và BIG-Bench Hard (Suzgun et al., 2022) tương ứng, cho các mô hình có kích thước bằng nhau.

Phân tích phương pháp Flan 2022 cho thấy các kết quả mạnh mẽ bắt nguồn từ cả tập hợp các tác vụ lớn hơn và đa dạng hơn, cũng như từ một tập hợp các kỹ thuật điều chỉnh và tăng cường dữ liệu đơn giản. Đặc biệt, huấn luyện trên một hỗn hợp các ví dụ được mẫu hóa với các lời nhắc zero-shot, few-shot, và chain-of-thought cải thiện hiệu suất trong mọi cài đặt này, cùng nhau. Ví dụ, chỉ thêm 10% lời nhắc few-shot cải thiện kết quả lời nhắc zero-shot 2%+. Ngoài ra, làm giàu tính đa dạng tác vụ bằng cách đảo ngược các cặp đầu vào-đầu ra, như được sử dụng trong (Sanh et al., 2021; Min et al., 2022), cùng với việc cân bằng các nguồn tác vụ, đều được chỉ ra là quan trọng đối với hiệu suất. Mô hình Flan-T5 kết quả hội tụ nhanh hơn và ở hiệu suất cao hơn so với các mô hình T5 trong điều chỉnh tác vụ đơn—gợi ý các mô hình được điều chỉnh hướng dẫn cung cấp checkpoint khởi đầu hiệu quả hơn về mặt tính toán cho các ứng dụng downstream, xác nhận Aribandi et al. (2021) và Liu et al. (2022b).

Chúng tôi hy vọng việc công khai các phát hiện và tài nguyên này sẽ thống nhất các tài nguyên xung quanh điều chỉnh hướng dẫn và đẩy nhanh nghiên cứu về các mô hình ngôn ngữ mục đích chung hơn. Chúng tôi tóm tắt các đóng góp cốt lõi của nghiên cứu này như sau:

•Phương pháp luận: Cho thấy huấn luyện với các lời nhắc zero- và few-shot hỗn hợp mang lại hiệu suất tốt hơn nhiều trong cả hai cài đặt (Mục 3.2).
•Phương pháp luận: Đo lường và chứng minh các kỹ thuật quan trọng cho điều chỉnh hướng dẫn hiệu quả: mở rộng Mục 3.3, làm giàu tính đa dạng tác vụ với đảo ngược đầu vào (Mục 3.4), thêm dữ liệu huấn luyện chain-of-thought, và cân bằng các nguồn dữ liệu khác nhau (Mục 3.5).
•Kết quả: Chứng minh các lựa chọn kỹ thuật này mang lại cải thiện 3-17% tác vụ Held-Out so với các bộ sưu tập điều chỉnh hướng dẫn mã nguồn mở hiện có (Hình 1).
•Kết quả: Chứng minh Flan-T5 phục vụ như một checkpoint khởi đầu mạnh hơn và hiệu quả hơn về mặt tính toán cho điều chỉnh tác vụ đơn (Mục 4).
•Mã nguồn mở bộ sưu tập tác vụ Flan 2022 mới, mẫu, và phương pháp cho nghiên cứu công khai.

2 Các Bộ Sưu Tập Điều Chỉnh Hướng Dẫn Công Khai

Các Mô Hình Ngôn Ngữ Lớn Điều chỉnh hướng dẫn đã nổi lên như một công cụ để làm cho các mô hình ngôn ngữ lớn (LLMs) và khả năng của chúng hữu ích hơn cho đối thoại tương tác và các tác vụ chức năng. Nghiên cứu trước đây (Raffel et al., 2020; Liu et al., 2019; Aghajanyan et al., 2021; Aribandi et al., 2021) đã thử nghiệm với điều chỉnh đa tác vụ quy mô lớn, để cải thiện điều chỉnh mục tiêu đơn downstream, nhưng không có lời nhắc hướng dẫn. UnifiedQA và những nghiên cứu khác (Khashabi et al., 2020; McCann et al., 2018; Keskar et al., 2019) thống nhất một phạm vi rộng các tác vụ NLP thành một định dạng trả lời câu hỏi sinh sản duy nhất, sử dụng hướng dẫn nhắc nhở cho điều chỉnh và đánh giá đa tác vụ.

Làn Sóng Đầu Tiên Từ năm 2020, một số bộ sưu tập tác vụ điều chỉnh hướng dẫn đã được phát hành liên tiếp nhanh chóng, được nêu trong Hình 2. Natural Instructions (Mishra et al., 2021), Flan 2021 (Wei et al., 2021), P3 (Public Pool of Prompts, Bach et al., 2022) tổng hợp các bộ sưu tập tác vụ NLP lớn và tạo mẫu chúng với hướng dẫn (lời nhắc zero-shot), cụ thể để điều chỉnh các mô hình tổng quát hóa sang các hướng dẫn chưa thấy. MetaICL (Min et al., 2022) cũng hợp nhất các bộ sưu tập tác vụ khác (Ye et al., 2021; Khashabi et al., 2020) để huấn luyện các mô hình học tác vụ "trong ngữ cảnh"—từ một số ví dụ đầu vào-đầu ra, được gọi là lời nhắc few-shot, nhưng trong trường hợp này không có hướng dẫn. Mỗi nghiên cứu này khẳng định lợi ích mở rộng của tính đa dạng tác vụ và mẫu, và một số báo cáo lợi ích mạnh mẽ từ việc đảo ngược các đầu vào và đầu ra trong mẫu để tạo ra các tác vụ mới ("kênh nhiễu" trong Min et al., 2022).

Làn Sóng Thứ Hai Một làn sóng thứ hai của các bộ sưu tập điều chỉnh hướng dẫn mở rộng các tài nguyên trước: kết hợp nhiều dataset và tác vụ hơn thành một tài nguyên, như Super-Natural Instructions (Wang et al., 2022c) hoặc OPT-IML (Iyer et al., 2022), thêm điều chỉnh hướng dẫn đa ngôn ngữ trong xP3 (Muennighoff et al., 2022), và lời nhắc huấn luyện Chain-of-Thought trong Flan 2022 (Chung et al., 2022). Cả Bộ Sưu Tập Flan và OPT-IML đều chứa hầu hết các tác vụ được đại diện trong các bộ sưu tập trước. Nghiên cứu của chúng tôi được định vị ở đây, hợp nhất hầu hết các bộ sưu tập này (của các bộ sưu tập) và phương pháp của chúng, như điểm khởi đầu mạnh nhất cho nghiên cứu mã nguồn mở tương lai.

Hướng Mới Nghiên cứu đồng thời và tương lai đang bắt đầu khám phá hai hướng mới: (a) mở rộng tính đa dạng tác vụ thậm chí còn tích cực hơn với việc tạo dữ liệu tổng hợp, đặc biệt trong đối thoại sáng tạo và mở (Wang et al., 2022b; Honovich et al., 2022; Ye et al., 2022; Gupta et al., 2022), và (b) cung cấp tín hiệu phản hồi của con người về phản ứng mô hình (Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022a; Nakano et al., 2021; Bai et al., 2022b). Chúng tôi coi hầu hết các hướng mới này có khả năng bổ sung cho nền tảng phương pháp điều chỉnh hướng dẫn.

Điều Chỉnh với Phản Hồi Con Người Điều chỉnh hướng dẫn về phản hồi con người đã chứng minh kết quả mạnh mẽ trên các tác vụ mở, nhưng với cái giá của hiệu suất trên một loạt rộng các tác vụ NLP truyền thống hơn (Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022a; Nakano et al., 2021). (Xem thảo luận của Ouyang et al. (2022) về "thuế căn chỉnh".) Nghiên cứu của chúng tôi tập trung cụ thể vào tổng quát hóa hướng dẫn, không có phản hồi con người, vì hai lý do. Thứ nhất, các dataset phản hồi con người ít công khai hơn nhiều so với các dataset điều chỉnh hướng dẫn (và có thể cụ thể cho mô hình). Thứ hai, bản thân nó, tổng quát hóa hướng dẫn cho thấy triển vọng lớn trong việc nâng cao các phản ứng được con người ưa thích trên các tác vụ mở, cũng như cải thiện các chỉ số NLP truyền thống (Chung et al., 2022). Mức độ tiến bộ có thể đạt được mà không có các cuộc biểu diễn hoặc xếp hạng phản ứng con người đắt đỏ vẫn là một câu hỏi mở, và một theo đuổi quan trọng để thu hẹp khoảng cách giữa nghiên cứu công khai và không công khai.

Tầm Quan Trọng của Mã Nguồn Mở Nghiên cứu hồ sơ cao ngày càng được thúc đẩy bởi dữ liệu không công khai, như trong trường hợp GPT-3 và những mô hình khác (Ouyang et al., 2022; Glaese et al., 2022). Sự không thể tiếp cận của các tài nguyên này ngăn cản khả năng của cộng đồng nghiên cứu phân tích và cải thiện các phương pháp này trong lĩnh vực công khai. Chúng tôi thu hẹp phạm vi của mình đến các bộ sưu tập dữ liệu mã nguồn mở và có thể tiếp cận, được thúc đẩy bởi mục tiêu dân chủ hóa khả năng tiếp cận nghiên cứu.

3 Thí Nghiệm Điều Chỉnh Hướng Dẫn Flan 2022

Nghiên cứu gần đây chưa hợp nhất xung quanh một tập hợp kỹ thuật thống nhất, với các tác vụ, kích thước mô hình, và định dạng đầu vào mục tiêu khác nhau đều được đại diện. Chúng tôi mã nguồn mở một bộ sưu tập mới, lần đầu được giới thiệu trong Chung et al. (2022), được ký hiệu "Flan 2022", kết hợp Flan 2021, P3++, Super-Natural Instructions, với một số dataset lập luận, đối thoại, và tổng hợp chương trình bổ sung. Chúng tôi để lại cho Chung et al. (2022) các chi tiết về tạo mẫu và bộ sưu tập; và trong nghiên cứu này chúng tôi xem xét sâu hơn các cải tiến phương pháp luận chính và so sánh bộ sưu tập trên các kích thước mô hình tương đương với các bộ sưu tập hiện có.

Trong mục này, chúng tôi đánh giá các quyết định thiết kế trong Flan và thảo luận về bốn quyết định cụ thể mang lại cải thiện mạnh mẽ cho công thức điều chỉnh hướng dẫn. Các thành phần thiết kế này, được nêu trong Mục 2, là: (I) sử dụng các mẫu zero-shot, few-shot, và Chain-of-Thought hỗn hợp trong huấn luyện (Mục 3.2), (II) mở rộng các mô hình kích thước T5 đến hơn 1800 tác vụ (Mục 3.3), (III) làm giàu tác vụ với đảo ngược đầu vào (Mục 3.4), và (IV) cân bằng các hỗn hợp tác vụ này (Mục 3.5). Trong Mục 3.1, chúng tôi bắt đầu bằng cách đo lường giá trị của từng thành phần và so sánh mô hình cuối cùng với các bộ sưu tập điều chỉnh hướng dẫn thay thế (và phương pháp của chúng).

Thiết Lập Thí Nghiệm Chúng tôi điều chỉnh trên mô hình ngôn ngữ tiền tố được điều chỉnh T5-LM (Lester et al., 2021), sử dụng kích thước XL (3B) cho tất cả các mô hình để đảm bảo tính nhất quán, trừ khi có ghi chú khác. Mặc dù các kích thước khác của Flan-T5 có sẵn, chúng tôi cảm thấy XL có kích thước phù hợp để chạy các nghiên cứu loại bỏ hệ thống quy mô lớn, trong khi đủ lớn để rút ra kết luận chung. Chúng tôi đánh giá trên (a) một bộ 8 tác vụ "Held-In" được đại diện trong bộ sưu tập tác vụ huấn luyện hơn 1800 (4 tập validation trả lời câu hỏi và 4 suy luận ngôn ngữ tự nhiên), (b) tác vụ Chain-of-Thought (CoT) (5 tập validation), và (c) các benchmark MMLU (Hendrycks et al., 2020) và BBH (Suzgun et al., 2022) như tập hợp các tác vụ "Held-Out" của chúng tôi, vì chúng không được bao gồm như một phần của điều chỉnh Flan 2022. Benchmark Hiểu Biết Ngôn Ngữ Đa Tác Vụ Khổng Lồ (MMLU) kiểm tra rộng rãi khả năng lập luận và kiến thức qua 57 tác vụ trong khoa học, khoa học xã hội, nhân văn, kinh doanh, sức khỏe, cùng các chủ đề khác. BIG-Bench Hard (BBH) bao gồm 23 tác vụ thách thức từ BIG-Bench (Srivastava et al., 2022) nơi PaLM có hiệu suất kém hơn người đánh giá. Trong các nghiên cứu loại bỏ của chúng tôi, chúng tôi cũng đánh giá BBH với đầu vào Chain-of-Thought, theo Chung et al. (2022). Các chi tiết điều chỉnh và đánh giá bổ sung được cung cấp trong Phụ lục A.

3.1 Các Nghiên Cứu Loại Bỏ

Bảng 1 tóm tắt đóng góp trung bình cho các tác vụ Held-in, Held-out, và Chain-of-thought, bằng cách loại bỏ từng phương pháp: cân bằng trọng số hỗn hợp ("- Mixture Balancing"), tác vụ Chain-of-thought ("- CoT"), cài đặt nhắc nhở hỗn hợp ("- Few Shot Templates"), và Đảo Ngược Đầu Vào ("- Input Inversion"). Flan-T5 XL tận dụng cả bốn phương pháp này cùng nhau. Chúng tôi cũng điều chỉnh T5-XL-LM trên các bộ sưu tập khác, bao gồm Flan 2021, P3++, Super-Natural Instructions để so sánh.

Mỗi thành phần được loại bỏ của Flan đóng góp cải thiện cho các chỉ số khác nhau: huấn luyện Chain-of-Thought cho đánh giá Chain-of-Thought, đảo ngược đầu vào cho đánh giá Held-Out (MMLU và BBH), huấn luyện nhắc nhở few-shot cho đánh giá few-shot, và cân bằng hỗn hợp cho tất cả các chỉ số.

So với các mô hình T5-XL được huấn luyện trên các bộ sưu tập điều chỉnh hướng dẫn thay thế (và phương pháp của chúng), Flan vượt trội trong hầu hết mọi cài đặt. Trong khi các bộ sưu tập trước được điều chỉnh cụ thể cho nhắc nhở zero-shot, Flan-T5 XL được điều chỉnh cho nhắc nhở zero- hoặc few-shot. Điều này mang lại lợi thế hiệu suất +3-10% cho hầu hết các cài đặt zero-shot, và lợi thế 8-17% cho các cài đặt few-shot. Ấn tượng nhất, Flan 2022 vượt trội so với các mô hình OPT-IML-Max lớn hơn nhiều (10x) 30B và (58x) 175B. Tiếp theo, chúng tôi cô lập từng phương pháp được loại bỏ của Flan 2022 riêng lẻ, để kiểm tra lợi ích của mỗi phương pháp.

3.2 Huấn Luyện với Cài Đặt Nhắc Nhở Hỗn Hợp

Nghiên cứu trước đây đã cho thấy một loạt các mẫu đầu vào cho mỗi tác vụ có thể cải thiện hiệu suất. Tuy nhiên, tách biệt khỏi cách diễn đạt của mẫu hướng dẫn, các LLM trước đây chủ yếu điều chỉnh với các tập mẫu nhắm đến một cài đặt nhắc nhở duy nhất: cho nhắc nhở zero-shot (Wei et al., 2021; Sanh et al., 2021; Aghajanyan et al., 2021; Aribandi et al., 2021) hoặc cho nhắc nhở few-shot (Min et al., 2022; Wang et al., 2022c).

Một quyết định thiết kế chưa được đánh giá cao trong InstructGPT (Ouyang et al., 2022) là trộn các mẫu huấn luyện cho mỗi cài đặt nhắc nhở này, thay vì nhắm đến một cài đặt duy nhất. Tuy nhiên, vì Ouyang et al. (2022) không kiểm tra lựa chọn này, chúng tôi mong đợi một sự đánh đổi hiệu suất trong điều chỉnh cho hiệu suất nhắc nhở zero-shot hoặc few-shot – đặc biệt đối với các mô hình nhỏ hơn. Thay vào đó, chúng tôi thấy huấn luyện với nhắc nhở zero- và few-shot hỗn hợp cải thiện đáng kể hiệu suất trong cả hai cài đặt – ngạc nhiên nhất, thậm chí đối với các mô hình chỉ có 3B tham số.

Hình 3 cho thấy (1) thêm chỉ 5% mẫu huấn luyện few-shot có thể cải thiện hiệu suất zero-shot một cách kịch tính, và (2) thêm 10%+ dữ liệu zero-shot cũng cải thiện hiệu suất few-shot. Cả tác vụ Held-In và Held-Out đều đạt đỉnh ở bất kỳ đâu giữa 10-90% dữ liệu few-shot, nhưng phạm vi này liên tục cao hơn huấn luyện chỉ với một cài đặt nhắc nhở.

3.3 Mở Rộng Các Mô Hình Nhỏ đến Hơn 1.8k Tác Vụ

Các nỗ lực điều chỉnh hướng dẫn công khai gần đây và đồng thời nhất, như Flan 2022, huấn luyện trên hàng nghìn tác vụ (Wang et al., 2022c; Iyer et al., 2022), nhưng hoạt động trên các thành phần tác vụ và phương pháp huấn luyện cơ bản khác nhau. Để đo lường tác động của việc mở rộng kích thước mô hình và tác vụ cho bộ sưu tập Flan 2022, chúng tôi điều chỉnh các mô hình T5-LM đã điều chỉnh (Small, Base, Large, XL, XXL) trên các tập con tác vụ được chọn ngẫu nhiên (8, 25, 50, 100, 200, 400, 800, tất cả 1873). Mỗi lần chạy điều chỉnh được đảm bảo bao gồm các tác vụ Held-In, vì vậy chúng tôi có thể ước tính cách mở rộng tác vụ tác động đến khả năng mô hình duy trì hiệu suất trên một tác vụ đã thấy.

Hình 4 chứng minh rằng cả tác vụ Held-In và Held-Out đều có vẻ như hưởng lợi từ việc thêm hàng trăm tác vụ điều chỉnh. Đánh giá tác vụ Held-in đạt đỉnh khoảng 200 tác vụ tổng cộng, và giảm hiệu suất khi thêm nhiều tác vụ hơn, mặc dù các mô hình lớn hơn đạt đỉnh muộn hơn và giảm ít hơn. Hiệu suất tác vụ Held-out tăng log-tuyến tính với số lượng tác vụ, đạt hiệu suất cao nhất với tất cả 1836 tác vụ.

Đáng ngạc nhiên, chỉ có T5-Small dường như vượt quá hiệu suất tác vụ Held-Out của nó trước 1836 tác vụ, trong khi các kích thước mô hình lớn hơn tiếp tục cải thiện. Các kết quả này gợi ý (a) thậm chí T5-Base có thể chưa cạn kiệt khả năng của nó với hàng nghìn tác vụ, và (b) các LM lớn nhất có thể hưởng lợi từ hàng nghìn tác vụ hơn cho hiệu suất tác vụ Held-In và Held-Out.

Một giả định cần thiết của phân tích này là tất cả các tác vụ được định nghĩa và đếm như nhau. Mục 3.5 chứng minh cách không phải tất cả các nguồn tác vụ đều có lợi như nhau cho huấn luyện, và hiệu suất mô hình có thể bão hòa từ quá nhiều tác vụ từ một nguồn (ví dụ Super-Natural Instructions). Chúng tôi khuyến cáo thận trọng với kết luận rằng việc mở rộng tác vụ vượt quá 1800 sẽ dẫn đến lợi ích tăng mà không cũng chú ý đến tính đa dạng và chất lượng tác vụ.

3.4 Làm Giàu Tác Vụ với Đảo Ngược Đầu Vào

Nghiên cứu điều chỉnh hướng dẫn trước đây đã làm giàu tính đa dạng tác vụ của họ bằng cách đảo ngược các cặp đầu vào-đầu ra (x,y) trong các tác vụ có giám sát—được gọi là "nhắc nhở không dành cho tác vụ gốc" trong P3 (Bach et al., 2022) hoặc "kênh nhiễu" trong MetaICL (Min et al., 2022). Ví dụ, một dataset có thể được thiết kế ban đầu để, cho một câu hỏi x, đánh giá xem một mô hình có thể trả lời y. Đảo ngược đầu vào thay vào đó cho mô hình câu trả lời y và huấn luyện nó tạo ra câu hỏi x. Đây là một phương pháp dễ dàng để làm giàu tính đa dạng tác vụ với một tập hợp nguồn dữ liệu hạn chế. Tuy nhiên, không rõ phương pháp này có còn hữu ích khi hàng trăm nguồn dữ liệu độc đáo và hàng nghìn tác vụ đã có sẵn.

Để đánh giá điều này, chúng tôi làm giàu hỗn hợp của chúng tôi với các tác vụ đầu vào đảo ngược (chi tiết và ví dụ trong Phụ lục B) và đo lường hiệu ứng. Trong Bảng 1 chúng tôi thấy điều này không có lợi cho hiệu suất Held-In, nhưng có lợi mạnh mẽ cho hiệu suất Held-Out. Những lợi ích này khuyến khích triển vọng của các kỹ thuật tăng cường dữ liệu cho điều chỉnh LLM, điều mà trước đây được cho là có lợi ích giảm dần khi các mô hình được tiền huấn luyện lâu hơn (Longpre et al., 2020).

3.5 Cân Bằng Các Nguồn Dữ Liệu

Mở rộng kích thước kiến trúc và số lượng tác vụ là hiệu quả, nhưng kết quả của chúng tôi gợi ý trọng số hỗn hợp đáng được chú ý nhiều như vậy để tối ưu hóa kết quả. Để hội tụ về một trọng số cân bằng, chúng tôi bỏ qua các tập hợp nguồn tác vụ khác nhau, từng cái một (Flan 2021, T0-SF, Super-Natural Instructions, Chain-of-Thought, Dialog, và Program Synthesis), và xếp hạng đóng góp của chúng trên benchmark MMLU.

Như được thể hiện trong Bảng 2, Flan 2021 và T0-SF là trong số các hỗn hợp có lợi nhất, theo sau là Super-Natural Instructions và Chain-of-Thought, với Dialog và Program Synthesis cuối cùng. Những phát hiện này được xác nhận bởi Iyer et al. (2022) người đã thử nghiệm rộng rãi tỷ lệ trộn dữ liệu, và cũng xác định các hỗn hợp Flan 2021, T0-SF, và T5 của họ có lợi nhất rộng rãi. Ngoài ra, họ thấy Super-Natural Instructions có lợi ích mở rộng hạn chế trên hiệu suất tác vụ Held-Out, điều mà họ liên quan đến định dạng đầu vào độc đáo và thiết kế hướng dẫn của nó. Đáng chú ý, điều chỉnh Chain-of-thought xuất hiện có lợi qua tất cả các cài đặt đánh giá của chúng tôi, đặc biệt xem xét chúng chứa ít tác vụ hơn nhiều so với Flan 2021, T0-SF hoặc Natural Instructions.

Chúng tôi sử dụng những phát hiện này để thu hẹp đáng kể không gian tìm kiếm trọng số hỗn hợp, và sử dụng trực giác thực hành của chúng tôi từ đó. Chiến lược này đơn giản nhưng hiệu quả, như được thể hiện trong Bảng 1, nhưng để lại nhiều chỗ cho nghiên cứu tương lai tinh vi hơn.

3.6 Thảo Luận

OPT-IML (Iyer et al., 2022) trình bày so sánh gần nhất với nghiên cứu này, bao gồm một bộ sưu tập tương tự các tác vụ, ví dụ và kỹ thuật. Tuy nhiên, trong khi các tác vụ được sử dụng của họ đều có nguồn gốc công khai, bộ sưu tập của họ, với mẫu, xử lý, và trộn ví dụ, không được phát hành, và do đó không thể dễ dàng so sánh. Iyer et al. (2022) báo cáo rằng Flan-T5-XL (3B) và XXL (11B) vượt trội so với OPT-IML-Max 175B trên cả MMLU và BBH. Như họ thảo luận, những khác biệt này có thể phát sinh từ bất kỳ kết hợp nào của tiền huấn luyện, kiến trúc mô hình, và điều chỉnh hướng dẫn. Kiến trúc mô hình và tiền huấn luyện trước điều chỉnh hướng dẫn có thể đóng một vai trò quan trọng (Wang et al., 2022a). Nhưng có nhiều chi tiết khác trong điều chỉnh hướng dẫn có thể khác nhau giữa Flan 2022 và OPT-IML. Các ứng cử viên có khả năng là: tạo mẫu ví dụ, cách các thủ tục nhắc nhở đầu vào hỗn hợp được sử dụng trong huấn luyện, và thành phần tác vụ.

Mỗi khác biệt này có ý nghĩa như thế nào? Trong khi OPT-IML chứa nhiều tác vụ hơn Flan 2022, chúng tôi ước tính khoảng 94% (2067/2207) cũng được sử dụng trong bộ sưu tập Flan 2022, và rất ít tác vụ trong Flan 2022 không được chứa trong một định dạng nào đó trong OPT-IML. Điều này gợi ý sự khác biệt tổng thể trong tính đa dạng tác vụ không đáng kể khi sử dụng định nghĩa "tác vụ" chung. Tỷ lệ hỗn hợp tác vụ cũng nhấn mạnh các nguồn tương tự, bao gồm Flan 2021 (46% so với 20%), PromptSource/P3 (28% so với 45%), và Super-Natural Instructions (25% so với 25%), cho Flan 2022 và OPT-IML tương ứng. Các bộ sưu tập khác của OPT-IML (Crossfit, ExMix, T5, U-SKG) không được trọng số đáng kể: 4%, 2%, 2%, 2% tương ứng.

Chúng tôi tin tạo mẫu ví dụ và các định dạng nhắc nhở hỗn hợp có thể tạo ra những khác biệt lớn nhất với điều chỉnh hướng dẫn OPT-IML. Kho mẫu của chúng tôi đã được cập nhật đáng kể từ Flan 2021, thêm tính đa dạng không chỉ trong hướng dẫn, mà còn theo các chiều kích. Ví dụ, thủ tục tạo mẫu thay đổi nơi hướng dẫn được đặt (trước hoặc sau nhắc nhở few-shot), khoảng cách và dấu phân cách giữa nhắc nhở few-shot và Chain-of-Thought, và các hoán vị định dạng của các tùy chọn câu trả lời (và mục tiêu của chúng) cho các ví dụ nhiều lựa chọn, đôi khi bao gồm và đôi khi loại trừ các tùy chọn câu trả lời trong đầu vào hoặc mẫu. Trong khi chúng tôi không có thí nghiệm chuyên dụng so sánh nhiều lần lặp phát triển, chúng tôi thấy các thủ tục này tăng cường kịch tính tính đa dạng đầu vào và cho thấy cải thiện hiệu suất lặp lại. Thủ tục tạo mẫu ví dụ của chúng tôi được mã nguồn mở để kiểm tra và nghiên cứu tương lai.

4 Điều Chỉnh Hướng Dẫn Nâng Cao Điều Chỉnh Tác Vụ Đơn

Trong các cài đặt ứng dụng, các nhà thực hành học máy triển khai các mô hình NLP được điều chỉnh (FT) cụ thể cho một tác vụ mục tiêu duy nhất, thường là nơi dữ liệu điều chỉnh đã có sẵn. Trong khi nghiên cứu trước đây đã chỉ ra lợi ích của điều chỉnh trung gian (Pruksachatkun et al., 2020; Vu et al., 2020) hoặc điều chỉnh đa tác vụ (Aghajanyan et al., 2021; Aribandi et al., 2021) cho các tác vụ downstream, điều này chưa được nghiên cứu rộng rãi cho các mô hình được điều chỉnh hướng dẫn.

Chúng tôi đánh giá điều chỉnh hướng dẫn Flan 2022 như một bước trung gian trước điều chỉnh mục tiêu đơn, để hiểu xem Flan-T5 có phục vụ như một checkpoint khởi đầu tốt hơn cho các nhà thực hành ứng dụng. Chúng tôi đánh giá ba cài đặt trong Hình 5: điều chỉnh T5 trực tiếp trên tác vụ mục tiêu như đường cơ sở thông thường (thanh xanh), sử dụng Flan-T5 mà không điều chỉnh thêm (thanh be), và điều chỉnh Flan-T5 thêm trên tác vụ mục tiêu (thanh đỏ).

Cải Thiện Pareto cho Điều Chỉnh Tác Vụ Đơn Đối với cả hai tập hợp tác vụ Held-In và Held-Out được kiểm tra, điều chỉnh Flan-T5 cung cấp cải thiện pareto so với điều chỉnh T5 trực tiếp. Trong một số trường hợp, thường là nơi dữ liệu điều chỉnh bị hạn chế cho một tác vụ, Flan-T5 mà không điều chỉnh thêm vượt trội so với T5 với điều chỉnh tác vụ.

Hội Tụ Nhanh Hơn & Lợi Ích Tính Toán Sử dụng Flan-T5 như một checkpoint khởi đầu có lợi ích bổ sung trong hiệu quả huấn luyện. Như được chứng minh trong Hình 6, Flan-T5 hội tụ nhanh hơn nhiều so với T5 trong điều chỉnh mục tiêu đơn, cũng như đạt đỉnh ở độ chính xác cao hơn. Các kết quả hội tụ này cũng gợi ý có động lực AI xanh mạnh mẽ cho cộng đồng NLP áp dụng các mô hình được điều chỉnh hướng dẫn, như Flan-T5 cho điều chỉnh tác vụ đơn, thay vì các mô hình thông thường không được điều chỉnh hướng dẫn. Trong khi điều chỉnh hướng dẫn tốn kém hơn về mặt tính toán so với điều chỉnh tác vụ đơn, nó là một chi phí một lần. Ngược lại, các mô hình tiền huấn luyện yêu cầu điều chỉnh rộng rãi trở nên tốn kém hơn khi tổng hợp qua hàng triệu bước huấn luyện bổ sung (Wu et al., 2022; Bommasani et al., 2021). Các mô hình được điều chỉnh hướng dẫn cung cấp một giải pháp hứa hẹn để giảm đáng kể lượng bước điều chỉnh qua một loạt rộng các tác vụ, nếu chúng được áp dụng như một điểm khởi đầu tiêu chuẩn mới cho điều chỉnh tác vụ đơn.

5 Nghiên Cứu Liên Quan

Các Mô Hình Ngôn Ngữ Lớn Như nền tảng của điều chỉnh hướng dẫn, thực hành tiền huấn luyện một biểu diễn ngôn ngữ mục đích chung hữu ích cho nhiều tác vụ downstream có truyền thống lâu dài ít nhất từ Mikolov et al. (2013) và Dai và Le (2015). Năm 2018, Peters et al. (2018) và Devlin et al. (2019) củng cố mô hình tiền huấn luyện một mô hình lớn trên một corpus không giám sát lớn, và lĩnh vực NLP nhanh chóng hội tụ sử dụng các mô hình này vượt trội đáng kể so với nghệ thuật trước đây của các mô hình LSTM cụ thể tác vụ không tiền huấn luyện trên tất cả các tác vụ. Tuy nhiên, cách thống trị để tiếp cận kiến thức cú pháp và ngữ nghĩa chất lượng cao được mã hóa trong các mô hình tiền huấn luyện không phải là nhắc nhở chúng với hướng dẫn, mà là huấn luyện một lớp tuyến tính cụ thể tác vụ bổ sung ánh xạ các kích hoạt mô hình thành nhãn lớp số. Một năm ngắn sau, Radford et al. (2019), Raffel et al. (2020), và Lewis et al. (2020) phổ biến khái niệm rằng các tác vụ downstream—và nhiều tác vụ—có thể được học chung bằng cách sử dụng trực tiếp đầu LM tiền huấn luyện để tạo ra câu trả lời bằng ngôn ngữ tự nhiên (so với nhãn lớp số cụ thể tác vụ), bản chất tổng quát tác vụ của các mô hình sinh sản này trở thành tiền thân của nhiều nghiên cứu học chuyển giao đa tác vụ (McCann et al., 2018; Khashabi et al., 2020; Ye et al., 2021; Vu et al., 2020), điều này lần lượt dẫn đến làn sóng đầu tiên của điều chỉnh hướng dẫn như được mô tả trong Mục 2.

Sự tiến bộ liên tục trong nghiên cứu về corpus tiền huấn luyện, kiến trúc và mục tiêu tiền huấn luyện của LM cũng có tác động lớn đến điều chỉnh hướng dẫn. Tính đến năm 2022, các Transformer nhân quả trái-sang-phải chỉ bộ giải mã thống trị thị trường các mô hình lớn hơn 100B (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022; Hoffmann et al., 2022), và tất cả các mô hình của lớp kích thước đó với tham số mô hình hoàn toàn công khai là chỉ bộ giải mã (Wang và Komatsuzaki, 2021; Le Scao et al., 2022; Zhang et al., 2022), quyết định này thường do hỗ trợ khung phần cứng và phần mềm tốt hơn. Tuy nhiên, Raffel et al. (2020), Lewis et al. (2020), và Tay et al. (2022a) đã liên tục thấy rằng mô hình hóa ngôn ngữ nhân quả trái-sang-phải là một mục tiêu dưới tối ưu, trong khi Tay et al. (2022b) và Wang et al. (2022a) đặc biệt cho thấy rằng một hỗn hợp các mục tiêu không tuần tự vượt trội nhiều cho các tác vụ downstream với nhắc nhở zero-shot và few-shot. Một yếu tố bổ sung vẫn chưa được khám phá là mối quan hệ giữa corpus tiền huấn luyện, điều chỉnh hướng dẫn, và khả năng downstream. Thông thường, các mô hình công khai đều được huấn luyện trên một trong số ít corpus công khai: C4 (Raffel et al., 2020), The Pile (Gao et al., 2020), hoặc ROOTs (Laurençon et al., 2022).

Điều Chỉnh Hướng Dẫn Trong Mục 2 chúng tôi phác thảo các phát triển chính trong điều chỉnh hướng dẫn. Các phát triển quan trọng khác bao gồm triển vọng bổ sung hoặc thay thế học trong ngữ cảnh few-shot—phương pháp thống trị hiện tại để đánh giá các mô hình tiền huấn luyện và được điều chỉnh hướng dẫn—với điều chỉnh hiệu quả tham số. Vì điều chỉnh tiêu chuẩn của các mô hình lớn hơn 100B yêu cầu một số lượng cao bộ tăng tốc với kết nối phù hợp thường quá đắt thậm chí đối với nhiều phòng thí nghiệm công nghiệp, điều chỉnh hiệu quả tham số (hay còn gọi là "điều chỉnh nhắc nhở" liên tục hoặc mềm) cho thấy chỉ cập nhật một tập con nhỏ tham số mô hình có thể đạt hiệu suất có thể so sánh như điều chỉnh hoàn toàn tất cả tham số mô hình (Lester et al., 2021; Vu et al., 2022; Hu et al., 2021; xem He et al., 2022 cho phân tích chi tiết). Đáng chú ý, Liu et al. (2022b) cho thấy rằng, do độ dài chuỗi dài của ICL few-shot và các mẫu few-shot cần được suy luận lặp lại để đánh giá mỗi ví dụ, điều chỉnh hiệu quả tham số có thể rẻ hơn về mặt tính toán và có hiệu suất cao hơn học trong ngữ cảnh. Hơn nữa, Liu et al. (2022b), Vu et al. (2022), Wei et al. (2021), và Singhal et al. (2022) cùng nhau cho thấy rằng cả điều chỉnh hiệu quả tham số tác vụ đơn và đa tác vụ có thể được kết hợp một cách hiệu quả với điều chỉnh hướng dẫn, trước hoặc sau điều chỉnh hướng dẫn mô hình đầy đủ thường xuyên. Dòng nghiên cứu này giúp các nhà nghiên cứu khác dễ dàng xây dựng dựa trên một mô hình được điều chỉnh hướng dẫn miền chung, và thu thập một hỗn hợp điều chỉnh hướng dẫn tùy chỉnh cho việc sử dụng của họ, ví dụ, với nhiều phương thức (Ahn et al., 2022; Huang et al., 2022; Xu et al., 2022) hoặc các miền đặc biệt như khoa học và y học (Lewkowycz et al., 2022; Singhal et al., 2022).

Vấn Đề Được Giải Quyết bởi Điều Chỉnh Hướng Dẫn & Kỹ Thuật Căn Chỉnh Điều chỉnh hướng dẫn là một phần của dòng nghiên cứu được thiết kế để "căn chỉnh" các mô hình ngôn ngữ với các mục tiêu hữu ích hơn và sở thích của con người. Khi không có các phương pháp như vậy, các mô hình ngôn ngữ được biết là thể hiện hành vi độc hại/có hại (Sheng et al., 2019; Liang et al., 2021; Wallace et al., 2019), tạo ra thông tin không thực tế (Maynez et al., 2020; Longpre et al., 2021; Devaraj et al., 2022), và các thách thức khác trong triển khai và đánh giá (Zellers et al., 2019; McGuffie và Newhouse, 2020; Talat et al., 2022). Phân tích, đánh giá và giảm thiểu các vấn đề này tạo ra một hướng hứa hẹn cho nghiên cứu tương lai (Gao et al., 2022; Ganguli et al., 2022). Điều chỉnh hướng dẫn đáng được điều tra nhiều hơn, vì nó đã chứng minh bản thân là một phương thuốc khuyến khích trong việc giảm các chỉ số thiên vị NLP, như được thể hiện trong Chung et al. (2022).

6 Kết Luận

Bộ sưu tập điều chỉnh hướng dẫn Flan 2022 mới thống nhất các bộ sưu tập công khai phổ biến nhất trước đây và phương pháp của chúng, đồng thời thêm các mẫu mới và cải tiến đơn giản như huấn luyện với cài đặt nhắc nhở hỗn hợp. Bộ sưu tập kết quả vượt trội so với Flan 2021, P3++, Super-Natural Instructions, và OPT-IML-Max 175B trên các tác vụ Held-In QA, NLI, và Chain-of-Thought, và Held-Out MMLU và BBH, thường với lợi thế lớn. Kết quả gợi ý bộ sưu tập mới này phục vụ như một điểm khởi đầu cạnh tranh hơn cho các nhà nghiên cứu và thực hành quan tâm đến việc tổng quát hóa sang các hướng dẫn mới, hoặc điều chỉnh trên một tác vụ mới duy nhất.

Lời Cảm Ơn

Chúng tôi muốn cảm ơn Ed H Chi, Xinyun Chen, và Colin Raffel vì lời khuyên và phản hồi của họ về bài báo.
