# Tìm kiếm bằng mã: Bộ dữ liệu SearchBySnippet và mô hình truy xuất SnippeR mới để tìm kiếm bằng đoạn mã

Ivan Sedykh1, Dmitry Abulkhanov2, Nikita Sorokin1,
Sergey Nikolenko3,4, Valentin Malykh1,4
1Phòng thí nghiệm Huawei Noah's Ark, 2Nhà nghiên cứu độc lập,
3Khoa St. Petersburg thuộc Viện Toán học Steklov,
4Viện Lập trình Hệ thống Ivannikov
valentin.malykh@phystech.edu

## Tóm tắt
Tìm kiếm mã là một nhiệm vụ quan trọng và được nghiên cứu kỹ lưỡng, nhưng nó thường có nghĩa là tìm kiếm mã bằng một truy vấn văn bản. Chúng tôi lập luận rằng việc sử dụng một đoạn mã (và có thể là một traceback lỗi) làm truy vấn khi tìm kiếm hướng dẫn sửa lỗi và mẫu mã là một trường hợp sử dụng tự nhiên không được các nghiên cứu trước đây bao phủ. Hơn nữa, các bộ dữ liệu hiện có sử dụng chú thích mã thay vì mô tả toàn văn làm văn bản, khiến chúng không phù hợp với trường hợp sử dụng này. Chúng tôi trình bày một bộ dữ liệu SearchBySnippet mới thực hiện trường hợp sử dụng tìm kiếm bằng mã dựa trên dữ liệu StackOverflow; chúng tôi chỉ ra rằng trên SearchBySnippet, các kiến trúc hiện có còn kém hơn cả đường cơ sở BM25 đơn giản ngay cả sau khi tinh chỉnh. Chúng tôi trình bày một mô hình mã hóa đơn mới SnippeR vượt trội hơn một số đường cơ sở mạnh trên SearchBySnippet với kết quả 0.451 Recall@10; chúng tôi đề xuất bộ dữ liệu SearchBySnippet và SnippeR như một chuẩn mực quan trọng mới để đánh giá tìm kiếm mã.

Từ khóa: tìm kiếm mã, truy xuất thông tin, mô hình ngôn ngữ

## 1. Giới thiệu
Lượng mã nguồn ngày càng tăng được viết mỗi ngày dẫn đến vô số vấn đề có thể xảy ra, mà hầu như chắc chắn đã được giải quyết và báo cáo trên các diễn đàn như StackOverflow. Một nhà phát triển đang gỡ lỗi một lỗi có đoạn mã liên quan và traceback lỗi được tạo ra bởi trình biên dịch hoặc trình thông dịch, và cô ấy muốn tìm hiểu lý do đằng sau lỗi và cách khắc phục. Điều này dẫn đến bối cảnh mà chúng tôi gọi là "tìm kiếm bằng đoạn mã": dựa trên một đoạn mã và/hoặc traceback lỗi, tìm các bài đăng có thể chứa giải pháp. Thật ngạc nhiên, bối cảnh này đã được xem xét rất ít trong tài liệu; ví dụ, Ponzanelli et al. (2014) xem xét nó một cách không chính thức và chỉ sử dụng một công cụ tìm kiếm thương mại. Trong công việc này, chúng tôi đề xuất một thiết lập truy xuất thông tin trong đó truy vấn là một đoạn mã và/hoặc traceback và tài liệu là các bài đăng có văn bản và có thể có các đoạn mã khác (Hình 1); bối cảnh này có thể được tự động hóa và tích hợp vào IDEs. Các nghiên cứu trước đây về tìm kiếm mã (xem Phần 2) thường khớp mã nguồn của một hàm và chú thích của nó, và tìm kiếm mã cũng xem xét các truy vấn văn bản; người ta có thể đảo ngược vấn đề nhưng các phần văn bản thường là các chú thích ngắn thay vì các bài đăng toàn văn có thể chứa giải pháp.

Trong công việc này, chúng tôi trình bày một bộ dữ liệu mới có tên SearchBySnippet nắm bắt bối cảnh vấn đề này dựa trên các bài đăng StackOverflow (trong Python). Chúng tôi đã điều chỉnh một số mô hình tìm kiếm mã tiên tiến làm đường cơ sở, bao gồm CodeBERT, GraphCodeBERT (GCB), và SynCoBERT. Thật ngạc nhiên, hiệu suất của chúng trên SearchBySnippet rất kém; ngay cả GCB được huấn luyện đặc biệt trên bộ dữ liệu CodeSearchNet cho bối cảnh này cũng thua đáng kể so với đường cơ sở BM25 đơn giản. Do đó, chúng tôi đã phát triển một mô hình SnippeR mới sử dụng bộ mã hóa dựa trên GCB cho cả truy vấn và tài liệu và kết hợp một số cải tiến để vượt trội hơn BM25 trên SearchBySnippet. Tuy nhiên, các giá trị tuyệt đối của kết quả không quá cao, và chúng tôi tin rằng bối cảnh vấn đề được thể hiện trong SearchBySnippet mở ra một hướng nghiên cứu mới có thể dẫn đến hiểu biết mã tốt hơn.

Những đóng góp chính của công việc này bao gồm:
(i) một bối cảnh vấn đề mới cho tìm kiếm mã và một bộ dữ liệu SearchBySnippet mới để huấn luyện và đánh giá trong bối cảnh này; (ii) mô hình SnippeR vượt trội hơn các đường cơ sở truy xuất thông tin mạnh và có thể phục vụ như một điểm khởi đầu cho nghiên cứu trong bối cảnh mới này. Dưới đây, Phần 2 khảo sát các công trình liên quan, Phần 3 trình bày SearchBySnippet, Phần 4 giới thiệu SnippeR và quy trình huấn luyện của nó, Phần 5 cho thấy thiết lập thực nghiệm và kết quả của chúng tôi, và Phần 6 kết luận bài báo.

## 2. Công trình liên quan
Bộ dữ liệu. Husain et al. (2019) đã trình bày CodeSearchNet (CSN), được xây dựng từ một bản dump GitHub, với các thân hàm được chia thành chính mã và mô tả. CSN chứa 2M cặp (đoạn mã, mô tả) trong 6 ngôn ngữ lập trình, bao gồm Python. Hasan et al. (2021) đã kết hợp CSN và các bộ dữ liệu khác thành một bộ dữ liệu lớn hơn (với các tập con Java và Python của CSN), thu được 4M cặp (đoạn mã, mô tả). Một bộ dữ liệu thậm chí lớn hơn đã được xây dựng sớm hơn bởi Gu et al. (2018); bộ dữ liệu CODEnn-Train dựa trên Java của họ có 18M cặp phương thức và mô tả một câu của chúng. CodeXGLUE của Lu et al. (2021) là một bộ sưu tập chuẩn mực học máy của các bộ dữ liệu cho các nhiệm vụ hiểu và tạo mã, bao gồm mã trong 10 ngôn ngữ lập trình (và một sự sửa đổi của CSN). Một bộ dữ liệu đa nhiệm vụ khác được trình bày bởi Puri et al. (2021), với 14M đoạn mã trong 5 ngôn ngữ lập trình.

Tìm kiếm mã. Các biểu diễn vector dày đặc thường được sử dụng cho truy xuất thông tin (IR): Gu et al. (2018) đã sử dụng hai RNN để biểu diễn mã và mô tả văn bản, Feng et al. (2020) đã dựa CodeBERT trên các mô hình ngôn ngữ, Gotmare et al. (2021) đã sử dụng ba mô hình dựa trên Transformer, hai bộ mã hóa và một bộ phân loại, để có được biểu diễn phân cấp của mã và văn bản. Mô hình của chúng tôi sử dụng một bộ mã hóa duy nhất để nhúng cả truy vấn và tài liệu và không có bộ phân loại riêng biệt.

Các mô hình ngôn ngữ cho mã. GraphCodeBERT (Guo et al., 2021) sử dụng các đồ thị luồng dữ liệu trong quá trình tiền huấn luyện để giải quyết các nhiệm vụ mô hình hóa ngôn ngữ có mặt nạ, dự đoán cạnh, và căn chỉnh nút. SynCoBERT (Wang et al., 2021) sử dụng học tương phản đa phương thức để đạt được biểu diễn mã tốt hơn và được tiền huấn luyện trên dự đoán định danh và dự đoán cạnh cây cú pháp trừu tượng (AST).

## 3. Bộ dữ liệu SearchBySnippet
Tiền xử lý dữ liệu. SearchBySnippet được xây dựng từ một bản dump StackOverflow công khai với các câu hỏi và câu trả lời từ 2014 đến 2021 và thông tin meta phong phú. Trong quá trình gửi, người dùng StackOverflow điền vào một số trường xuất hiện trong cấu trúc dump (cùng với các trường như "FavouriteCount"); trường "tags" cho phép dễ dàng phân loại các câu hỏi. Chúng tôi giới hạn công việc của mình với Python do tính phổ biến của nó. Để tiền xử lý, chúng tôi lấy các trường "text" và "title" chứa văn bản chính của một câu hỏi ("text" có thể có đánh dấu định dạng) và thẻ ⟨code⟩ cho mã nguồn và/hoặc đầu ra hệ thống và trích xuất văn bản từ các thẻ này. Nếu nó không trông giống như một traceback (ví dụ, không có từ khóa "Error"), chúng tôi đánh dấu nó là "code" và trích xuất trong trường "code"; nếu có, chúng tôi sử dụng trường "error". Chúng tôi cũng phân tích loại lỗi từ traceback bằng biểu thức chính quy và đặt nó vào trường "keyword". Nếu một câu hỏi chứa nhiều thẻ ⟨code⟩, chúng được phân loại độc lập. Chúng tôi thêm trường "best_answer" cho câu trả lời được người dùng chấp nhận và lưu trữ văn bản gốc trong thẻ "body". Hình 1 cho thấy một mẫu truy vấn được tiền xử lý ở bên trái và một tài liệu ở bên phải.

Bảng 1 cho thấy thống kê bộ dữ liệu. Chỉ một nửa số câu hỏi có traceback lỗi, và hơn 70% chứa mã. Thú vị là, các câu hỏi mà chúng tôi đã trích xuất cả hai trường "code" và "error" chỉ chiếm 1/3 bộ dữ liệu, trong khi những câu hỏi có "code" hoặc "error" chiếm 85%. Chỉ 35% câu hỏi có câu trả lời được chấp nhận. Bảng 2 cho thấy kích thước trung bình (theo ký hiệu) cho các trường đã trích xuất và so sánh chúng với trường "body" (tỷ lệ phần trăm chỉ đếm các câu hỏi có trường(s) hiện diện).

Tập đánh giá. Một số câu hỏi trong bản dump StackOverflow được đánh dấu là trùng lặp; thường bài đăng A là bản trùng lặp của bài đăng B nếu các quản trị viên StackOverflow đã coi câu hỏi trong bài đăng A tương đương với câu hỏi trong bài đăng B. Chúng tôi đã chọn các câu hỏi trùng lặp có chứa câu trả lời được chấp nhận (trong bài đăng B) và một đoạn mã hoặc traceback, thu được 1369 câu hỏi mà chúng tôi sử dụng để đánh giá. Chúng tôi sử dụng hợp của các trường "code" và "error" từ bài đăng A làm truy vấn và "best_answer" từ bài đăng B làm tài liệu chuẩn gốc.

So sánh với các bộ dữ liệu khác. Chúng tôi so sánh bộ dữ liệu của mình với CodeSearchNet (Husain et al., 2019) cũng dành cho tìm kiếm mã. Nó chứa các đoạn mã trong một số ngôn ngữ lập trình, bao gồm Python, dưới dạng các hàm được ghép nối với mô tả của chúng. Chúng tôi cũng xem xét NeuralCodeSearch (Li et al., 2019) như bộ dữ liệu có thiết kế tương tự nhất; chúng tôi chỉ sử dụng phần đánh giá của bộ dữ liệu này chứa các câu hỏi StackOverflow với các đoạn mã được cắt từ các câu trả lời tốt nhất.

Bảng 1 và 2 so sánh CodeSearchNet (CSN) và NeuralCodeSearch (NCS) với SearchBySnippet. CSN lớn hơn gấp đôi tổng thể, nhưng phần Python của nó nhỏ hơn SearchBySnippet gấp đôi. NCS chỉ chứa 287 câu hỏi trong phần đánh giá, ít hơn SearchBySnippet 3500 lần. Bảng 2 so sánh CSN và NCS với SearchBySnippet về kích thước trung bình của các trường khác nhau (theo ký hiệu); chúng tôi giả định rằng "func_code_string" trong CSN là tương đương gần đúng với hợp của "code" và "error" của chúng tôi, và "func_documentation_string" tương ứng với "best_answer". Đối với NCS, các trường "answer" và "question" được đảo ngược vì "best_answer" trong trường hợp của chúng tôi là trường văn bản, trong khi trong NCS nó là đoạn mã. Các phần CSN và NCS của Bảng 2 cho thấy tỷ lệ phần trăm của kích thước trung bình trường tương ứng (mã và văn bản) trong SearchBySnippet; trong khi các trường chứa mã trong CSN chỉ ngắn hơn 20%, trường văn bản ngắn hơn 3-4 lần so với trong SearchBySnippet. Chúng tôi tin rằng điều này có thể làm cho việc truy xuất trên SearchBySnippet khó khăn hơn. Trong NCS, cả hai thực thể đều ngắn hơn một bậc độ lớn, dẫn đến một nhiệm vụ truy xuất dễ dàng hơn nhiều.

## 4. Mô hình
Bối cảnh vấn đề. Trong nhiệm vụ IR của chúng tôi, truy vấn là một đoạn mã và/hoặc traceback từ các trường "code" và "error" và tài liệu là các câu trả lời từ trường "best_answer". Cho một bộ sưu tập tài liệu D và một truy vấn q, mô hình phải xếp hạng tài liệu để câu trả lời chuẩn gốc d∈D gần hơn với đầu danh sách. Theo các nghiên cứu trước về tìm kiếm mã, chúng tôi sử dụng bộ mã hóa mạng nơ-ron để có được biểu diễn vector dày đặc của truy vấn và tài liệu. Không giống như Karpukhin et al. (2020), và theo Feng et al. (2020) trong các nhiệm vụ liên quan đến mã và Sorokin et al. (2022) trong IR tổng quát, chúng tôi sử dụng cùng một bộ mã hóa E cho cả truy vấn và tài liệu. Hệ thống đầu tiên mã hóa tất cả tài liệu trong cơ sở dữ liệu thành các vector nhúng và sau đó xây dựng chỉ mục tìm kiếm. Đối với một truy vấn q, nó tính toán điểm tương đồng từng cặp giữa E(q) và các nhúng tài liệu E(d) và sắp xếp chúng với điểm tích vô hướng score(q, d) = E(q)⊤E(d). Hình 1 cho thấy cấu trúc hệ thống; chúng tôi gọi mô hình này là SnippeR (Snippe tRetrieval).

Chúng tôi khởi tạo bộ mã hóa E với GraphCodeBERT (GCB) được tiền huấn luyện (Guo et al., 2021), một mô hình dựa trên RoBERTa (Liu et al., 2019) với 125M tham số có thể huấn luyện, được tiền huấn luyện cho mã nguồn sử dụng đồ thị luồng dữ liệu cùng với biểu diễn văn bản. Trong trường hợp của chúng tôi, đầu vào không phải lúc nào cũng là mã, vì vậy chúng tôi không thể sử dụng đồ thị luồng dữ liệu, nhưng chúng tôi phát hiện ra rằng ngay cả khi không có nó, GCB vẫn vượt trội hơn các mô hình khác. Chúng tôi sử dụng đầu ra mô hình (trạng thái ẩn của lớp cuối) cho token ⟨s⟩ đầu tiên làm biểu diễn vector của văn bản (hoặc mã) đầu vào.

Quy trình huấn luyện. Bộ mã hóa được huấn luyện để tối đa hóa độ tương đồng giữa truy vấn và nhúng tài liệu phù hợp đồng thời tối thiểu hóa độ tương đồng giữa truy vấn và nhúng của các tài liệu không liên quan. Mỗi mẫu huấn luyện chứa một truy vấn q, một tài liệu liên quan (dương) d+, và n tài liệu không liên quan (âm) D−={d−j}nj=1. Như hàm mất mát đối lập, chúng tôi sử dụng logarit âm của khả năng tài liệu dương:

L(q, d+, D−) = −log(escore(q,d+)/(∑nj=1escore(q,d−j) + escore(q,d+)))

Để huấn luyện, chúng tôi sử dụng các âm tính cứng được khai thác từ lần lặp mô hình trước đó thông qua tự huấn luyện. Chúng tôi sử dụng học lặp (Qu et al., 2021; Izacard and Grave, 2020) theo dạng được hiển thị trong Hình 2: ở mỗi bước, mô hình truy xuất k tài liệu hàng đầu từ cơ sở dữ liệu cho mỗi truy vấn tập huấn luyện. Sau đó chúng tôi coi những k tài liệu hàng đầu này (ngoại trừ câu trả lời chuẩn gốc) là các ví dụ âm tính cứng cho lần lặp huấn luyện mô hình tiếp theo; trong Phần 5 chúng tôi cho thấy kết quả sau một vòng lặp như vậy (SnippeR2). Đối với mỗi truy vấn, như các âm tính khác (không cứng), chúng tôi sử dụng các tài liệu khác từ batch huấn luyện và các mẫu âm tính cứng của chúng; đây là thủ thuật âm tính trong batch (Karpukhin et al., 2020; Sorokin et al., 2022) giúp tránh lấy mẫu âm tính bổ sung.

Tiền huấn luyện. SearchBySnippet chỉ có 1369 câu hỏi với các bản trùng lặp và câu trả lời được chấp nhận trong phần đánh giá, nhưng có 148K cặp câu hỏi trùng lặp với đoạn mã hoặc traceback nhưng không có câu trả lời được chấp nhận. Chúng tôi sử dụng chúng trong tiền huấn luyện để thích ứng tốt hơn với cấu trúc và ngữ nghĩa của StackOverflow. Tiền huấn luyện chạy giống như huấn luyện với hai điểm khác biệt: (i) đối với một cặp trùng lặp A và B, chúng tôi sử dụng đoạn mã và/hoặc traceback từ A làm truy vấn và B làm tài liệu đích; (ii) chúng tôi bao gồm thân bài đăng trong văn bản vì chúng không chồng lấp với tập đánh giá.

Tiền xử lý dữ liệu và thiết lập huấn luyện. Chúng tôi nối đoạn mã c và traceback t (các trường "code" và "error") để tạo thành truy vấn q = [c, t]. Truy vấn thường dài hơn độ dài đầu vào tối đa (256 hoặc 512 token), và vì cuối traceback thường chứa thông tin quan trọng như định danh lỗi và mô tả lỗi có ý nghĩa, chúng tôi loại bỏ các token từ giữa thay vì cuối, để lại số lượng token bằng nhau ở đầu và cuối. Tài liệu văn bản được cắt bớt đến (đầu tiên) 256 hoặc 512 token. Trong SearchBySnippet, tài liệu được biểu diễn như tiêu đề câu hỏi, thân câu hỏi, và câu trả lời được chấp nhận (các trường "title", "body", và "best_answer"). Vì truy vấn được trích xuất từ thân bài đăng, chúng tôi không thể sử dụng chúng trong trường "body" trong tập huấn luyện, vì vậy thân bài đăng đã được loại bỏ khỏi biểu diễn tài liệu trong quá trình huấn luyện, để lại mô hình với các trường "title" và "best_answer" cho một tài liệu. Trong đánh giá, chúng tôi cũng sử dụng trường "body" vì bây giờ không có vấn đề về việc rò rỉ câu trả lời.

## 5. Đánh giá
Thiết lập và siêu tham số. Chúng tôi đo hiệu suất mô hình bằng Recall@k = ∑ki=1[ri = d+], trong đó d+ là tài liệu chuẩn gốc và ri là tài liệu được truy xuất ở vị trí i; k ∈ {5,10,20,50} trong các thí nghiệm của chúng tôi. Mô hình được huấn luyện trong 21 giờ trên 2 GPU NVIDIA Tesla V100 (mỗi GPU có 16GB bộ nhớ). Chúng tôi sử dụng bộ tối ưu hóa Adam (Kingma and Ba, 2014) với lịch tỷ lệ học không đổi và 3500 bước khởi động. Để ổn định việc huấn luyện, chúng tôi cắt chuẩn gradient xuống 2.0. Tỷ lệ học được đặt thành 10−5, kích thước batch là 12.

Đường cơ sở. Chúng tôi sử dụng đường cơ sở truy xuất thông tin tiêu chuẩn Okapi BM25 (Robertson et al., 1994). Bộ dữ liệu có sự dịch chuyển phân phối đáng kể giữa huấn luyện và đánh giá; vì BM25 không huấn luyện, nó không bị ảnh hưởng bởi sự dịch chuyển, đây là một yếu tố quan trọng khiến đường cơ sở này mạnh. Các đường cơ sở khác bao gồm các mô hình được tiền huấn luyện dựa trên Transformer hiện đại cho NLP và hiểu mã, được huấn luyện để tạo ra biểu diễn vector có ý nghĩa cho mã và/hoặc văn bản trong bối cảnh tìm kiếm mã (Husain et al., 2019); ví dụ, CodeBERT (Feng et al., 2020) nhằm căn chỉnh các nhúng của mã và văn bản tương ứng. Chúng tôi cũng đánh giá GraphCodeBERT (GCB) (Guo et al., 2021) và SynCoBERT (Wang et al., 2021) kết hợp biểu diễn cây cú pháp trừu tượng (AST) cho mã. AST được sử dụng trong huấn luyện nhưng không phải suy luận vì các đoạn mã ngắn từ truy vấn thường không tạo ra AST có ý nghĩa. Chúng tôi coi các mô hình này là mô hình cơ sở cho SnippeR trong các thí nghiệm sơ bộ, và GCB thắng. Chúng tôi cũng cố gắng tinh chỉnh GraphCodeBERT trên CodeSearchNet; vì dữ liệu của chúng tôi có nhiễu, chúng tôi tinh chỉnh GraphCodeBERT không có AST ("GraphCodeBERT (+CSN)"). Tất cả các mô hình trong Bảng 3 (ngoại trừ BM25) đều dựa trên RoBERTa, với 125M tham số có thể huấn luyện.

Kết quả. Kết quả đánh giá chính của chúng tôi được hiển thị trong Bảng 3. Thật ngạc nhiên, tất cả các mô hình dựa trên Transformer hoạt động rất kém ngay từ đầu và thua so với đường cơ sở BM25 cổ điển, mặc dù thực tế là chúng đã được huấn luyện để nhúng cả mã nguồn và văn bản vào một không gian nhúng duy nhất. Tinh chỉnh GCB trên CSN đã cải thiện hiệu suất đáng kể, nhưng ngay cả khi đó GCB vẫn thua kém BM25 với biên độ lớn. Chúng tôi trình bày kết quả tốt nhất cho SnippeR trong bảng; nó đã có thể vượt trội hơn BM25 và tất cả các đường cơ sở khác theo tất cả các chỉ số được xem xét. Tuy nhiên, Recall@5 kết quả chỉ vượt hơn một chút so với 30% và Recall@50 khoảng 65%, điều này để lại không gian đáng kể cho cải tiến.

## 6. Kết luận
Chúng tôi đã trình bày một trường hợp sử dụng mới cho tìm kiếm mã chưa được nghiên cứu rộng rãi trong tài liệu: tìm kiếm bằng một đoạn mã và/hoặc traceback lỗi. Chúng tôi đã trình bày một cách mới để xây dựng bộ dữ liệu cho trường hợp sử dụng này, dẫn đến bộ dữ liệu SearchBySnippet với khoảng 1M truy vấn. Chúng tôi đã đánh giá một số mô hình hiểu mã và phát hiện ra rằng trên SearchBySnippet, tất cả chúng đều thua ngay cả so với đường cơ sở BM25. Do đó, chúng tôi đã phát triển một mô hình SnippeR mới để tìm kiếm bằng đoạn mã và traceback, và đã có thể vượt trội hơn BM25 trên SearchBySnippet. Tuy nhiên, các giá trị tuyệt đối của kết quả của chúng tôi tương đối thấp, và chúng tôi hy vọng rằng bối cảnh và bộ dữ liệu mới này sẽ phục vụ như một hướng nghiên cứu mới cho hiểu mã, với SnippeR cung cấp một đường cơ sở hợp lý.
