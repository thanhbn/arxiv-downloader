# 2305.17701.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/datasets/2305.17701.pdf
# File size: 2564201 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
KOSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large
Language Model Applications
Hwaran Lee1,2,⋆Seokhee Hong3,⋆,♯Joonsuk Park1,2,4
Takyoung Kim1,♯Gunhee Kim3Jung-Woo Ha1,2
1NA VER AI Lab2NA VER Cloud3Seoul National University4University of Richmond
{hwaran.lee, jungwoo.ha}@navercorp.com park@joonsuk.org
seokhee.hong@vision.snu.ac.kr gunhee@snu.ac.kr youngerous@gmail.com
Abstract
Large language models (LLMs) learn not only
natural text generation abilities but also social
biases against different demographic groups
from real-world data. This poses a critical risk
when deploying LLM-based applications. Ex-
isting research and resources are not readily
applicable in South Korea due to the differ-
ences in language and culture, both of which
significantly affect the biases and targeted de-
mographic groups. This limitation requires lo-
calized social bias datasets to ensure the safe
and effective deployment of LLMs. To this end,
we present KOSBI, a new social bias dataset
of 34k pairs of contexts and sentences in Ko-
rean covering 72 demographic groups in 15
categories. We find that through filtering-based
moderation, social biases in generated content
can be reduced by 16.47%p on average for
HyperCLOV A (30B and 82B), and GPT-3.
1 Introduction
Large language models (LLMs) acquire impressive
text generation abilities from large-scale real-world
pre-training data (Brown et al., 2020; Kim et al.,
2021). However, LLMs also absorb toxicity, such
as social biases (Sheng et al., 2019; Wallace et al.,
2019a). This cannot be overlooked since the risk
of generating toxic content impedes the safe use
and potential commercialization of various down-
stream applications, such as AI assistants (Dinan
et al., 2022; Bai et al., 2022a). To minimize the
harm, numerous studies have tackled the detection
and mitigation of toxicity in LLMs (Blodgett et al.,
2020; Ganguli et al., 2022). Each study typically
leverages datasets capturing a specific type of toxi-
city, such as social bias (Sap et al., 2020; Nangia
⋆Authors equally contributed.
♯This work was done during their internship at NA VER
AI Lab.
Email to: {hwaran.lee, jungwoo.ha}@navercorp.com,
seokhee.hong@vision.snu.ac.kret al., 2020) or hate speech (Warner and Hirschberg,
2012; Lee et al., 2022).
These datasets are not only task-specific but also
language- and culture-specific. For instance, con-
sider hate speech made in South Korea and in the
United States. In addition to the language, the
mainly targeted demographic groups also differ—
feminists and Korean Chinese in South Korea, as
opposed to African Americans and Jewish in the
United States (Jeong et al., 2022). Also, the ex-
isting toxicity datasets in Korean mostly focus on
explicit hate speech and consider a limited number
of targeted demographic groups (Moon et al., 2020;
Yang et al., 2022; Kang et al., 2022; Lee et al.,
2022). This calls for a dataset to address social
biases against a more comprehensive set of demo-
graphic groups in South Korea so that as many
groups and people are protected.
Here we present the Korean Social Bias ( KOSBI)
dataset, a large-scale dataset of 34k pairs of con-
texts and sentences in Korean with labels mainly
capturing the presence of social biases.1It cov-
ers 72 targeted demographic groups in 15 cate-
gories,2which is much more comprehensive than
existing datasets, as shown in Table 2. The cat-
egories include not only the common ones like
gender and religion but also those especially rele-
vant to South Korea—e.g., marital status and do-
mestic area of origin, both of which consist of
demographic groups that suffer from social bi-
ases in the country more commonly than others
do. Given the difficulty of crawling from the web
sufficient data for each of the 72 demographic
groups, we leveraged HyperCLOV A (Kim et al.,
2021) to generate the data with in-context few-
1TheKOSBIdataset is released with English-translated
annotations for those who are not fluent in Korean at https:
//github.com/naver-ai/korean-safety-benchmarks
2The categories and demographic groups were selected
based on the Universal Declaration of Human Rights (UDHR)
and the National Human Rights Commission of Korea
(NHRCK).arXiv:2305.17701v2  [cs.CL]  30 May 2023

--- PAGE 2 ---
Dataset # Inst.Demographic GroupsData SourceIncludesToxicity Labels# Cat. # Groups Context?
BEEP! (Moon et al., 2020) 9,341 - - News comments ✗ Hate speech, Bias
APEACH (Yang et al., 2022) 3,770 10 - Human-written ✗ Offensive
KOLD (Jeong et al., 2022) 40,448 5 19 News, YouTube comments ✗(Title) Offensive
HateScore, Unsmile (Kang et al., 2022) 31,195 7 (mixed) News, online community comments ✗ Hate speech, Profanity
K-MHaS (Lee et al., 2022) 109,692 7 - News comments ✗ Hate speech, Profanity
KOSBI(Ours) 34,214 15 72 LM-generated ✓Biased (Stereotypes, Prejudice,
Discrimination), Other
Table 1: Comparison of Toxicity Datasets in Korean.
shot learning (Gao et al., 2021; Mishra et al.,
2022). More specifically, we generated sentences
and their respective contexts—which are also sen-
tences, grammatically—for given target demo-
graphic groups. The generated contexts and sen-
tences were then annotated by crowd workers as
safeorunsafe . Here, unsafe contexts and sentences
were further labeled as expressions of stereotypes
(cognitive bias), prejudice (emotional bias), dis-
crimination (behavioral bias), and/or other , adopt-
ing the taxonomy by Fiske (2023),3in Figure 1.
With KOSBI, we mitigate social biases in LLM-
generated content using a filtering-based modera-
tion approach, also known as rejection sampling
(Ganguli et al., 2022). To do this, we first trained
a safe sentence classifier using KOSBI. Then,
for a given context, each LLM was used to gen-
erate a pool of sentences from which the safest
sentence was chosen by the classifier. The hu-
man evaluation shows that social biases in gen-
erated content are reduced by 16.47% on average
for all three models tested— HyperCLOV A (82B),
HyperCLOV A (30B), and GPT-3.
2 Related Works
Bias Mitigation in LLM-generated Content.
LLMs are trained on real-world data, which often
contains social biases toward certain demographic
groups. This, in turn, induces biases in LLMs (Xu
et al., 2021a). To date, various resources have
been published to measure and mitigate such bi-
ases in LLMs (Sap et al., 2020; Nangia et al., 2020;
Nadeem et al., 2021). Some of them are associ-
ated with specific tasks: coreference resolution to
fight the phenomena like associating certain pro-
fessions with a particular gender (Rudinger et al.,
2018; Zhao et al., 2018), and question answering
to prevent answers stereotyped toward certain bias
categories like gender or socio-economic status (Li
et al., 2020; Parrish et al., 2022). These resources
3For labeling the context, prejudice anddiscrimination
were combined due to the limited number of instances.are not as effective for HyperCLOV A and other
LLMs pre-trained on Korean corpora. Thus, we
present a new resource in Korean, capturing the bi-
ases against prevalent demographic groups in South
Korea. Also, our dataset covers a much more com-
prehensive set of demographic groups.
Hate Speech Detection. Röttger et al. (2021) de-
fines hate speech as “abuse that is targeted at a
protected group or at its members for being a part
of that group.” Resources created to help detect
hate speech can be used to reduce hate speech
generated by LLMs, thereby reducing the harm
they can incur. Note these resources use vari-
ous names interchangeably for the most part, e.g.,
hate speech (Warner and Hirschberg, 2012), abu-
sive language (Wiegand et al., 2019), and toxic
language (Gehman et al., 2020; Hartvigsen et al.,
2022). Also, quite a few resources are for safer
dialogue (Sun et al., 2022; Xu et al., 2021b; Xenos
et al., 2021; Kim et al., 2022). Meanwhile, to re-
flect different languages and societies, researchers
have created and proposed hate speech corpora in
Chinese (Deng et al., 2022), Dutch (Demus et al.,
2022), and Arabic (Mubarak et al., 2022) Similar
to the resources capturing social biases, these re-
sources are not as useful for Korean LLMs due to
the differences in language and culture. Luckily,
several resources in Korean exist, as summarized
in Table 1. However, these resources either unspec-
ify or cover only a small subset of demographic
groups in South Korea. More importantly, they
focus on explicit profanity and otherwise offensive
expressions. Our dataset instead targets cases that
cannot be identified with specific keywords, such
as expressions of stereotypes, discrimination, and
prejudice (without explicit profanity) toward 72
demographic groups.
Safety Alignment of Language Models. Be-
yond social biases and hate speech, various cat-
egories have been proposed recently to enhance
the safety of language models, such as human val-

--- PAGE 3 ---
ues (Solaiman and Dennison, 2021; Kenton et al.,
2021), ethical judgements (Hendrycks et al., 2021;
Lourie et al., 2021), and moral norms (Forbes et al.,
2020; Emelin et al., 2021). Then, alignment learn-
ing methods through human feedback (Bai et al.,
2022a) or even by AI feedback (Bai et al., 2022b)
have been proposed. Moreover, red-teaming (Perez
et al., 2022; Ganguli et al., 2022) and adversarial
attack (Wallace et al., 2019b) approaches have also
been suggested to identify vulnerabilities in lan-
guage models in terms of safety. We expect our
dataset and comprehensive categories will be help-
ful for the safety alignment of Korean society.
3 The K OSBIDataset
This study aims to address social biases against a
comprehensive set of demographic groups in South
Korea so as to make LLMs safer for as many groups
and people as possible. (Here, we focus on so-
cial biases without explicit hate speech, as exist-
ing datasets address the latter.) To achieve this,
we wanted KOSBIto consist of context-sentence
pairs labeled as safeorunsafe for the demographic
groups mentioned in them; this way, we can train
LLMs to behave safely in the context of discussing
a demographic group, rather than simply avoid it.
3.1 Demographic Groups Compilation
With the goal of covering a comprehensive list of
demographic groups, we first compiled the list
by combining categories derived from the Uni-
versal Declaration of Human Rights (UDHR) and
the National Human Rights Commission of Korea
(NHRCK)4, which prohibit discriminatory treat-
ment based on social identity. (See Table 4 for the
list of categories.) Then, we defined social groups
in each category, considering the unique character-
istics of Korean culture. For instance, we consider
the most widely practiced religions in Korea, and
also progressive and conservative political parties,
rather than the Democratic and Republican parties
in the U.S. (See Table 8 for the list of demographic
groups.)
3.2 Raw Data Construction
Since crawling from the web sufficient context-
sentence pairs for every demographic group
would be challenging, we generated them using
4Specifically, refer to provisions related to discriminatory
acts in violation of equal rights – Article 2 Subparagraph 3 of
the National Human Rights Commission Act, and Article 3
Paragraph 1 Subparagraph 1 of the Anti-Discrimination Act.Categories # Groups
Gender identity†3
Sexual orientation†1
Age & Generation†12
Race, Ethnicity, Nationality†11
Religion†6
Disability status†1
Physical appearance†4
Political orientation†3
Socio-economic status†3
Domestic area of origin 8
Marital status 6
Pregnancy & Birth 4
Family form 5
Criminal record 2
Education, University, Major 3
Total 72
Table 2: Category and demographic groups considered
inKOSBI.†marks categories in both UDHR and
NHRCK. Entire social groups are listed in Table 8.
HyperCLOV A . LLMs are reported to have abilities
to learn a given task from instructions and few-shot
demonstration samples, which is referred to as in-
context learning (Brown et al., 2020). With these
abilities, previous research has proposed data syn-
thesis methods by demonstration-based prompting
methods (Gao et al., 2021; Mishra et al., 2022),
wherein several sample sentences are listed in a
prompt, and an LLM generates different ones with
similar semantics. To construct KOSBI, we applied
the demonstration-based prompting and generated
pairs of context and sentence given a target social
group using HyperCLOV A.
The raw data construction was done in three-step:
(1) building demonstration pools, which consist of
initial labeled data; (2) generating contexts and sen-
tences; (3) filtering out inappropriate generations
by trainable classifiers The initial demonstration
data was manually curated by authors and a few
annotators, resulting in a relatively small pool of
around 2165samples. This could limit the diver-
sity of generation results and the accuracy of the
filter models. To address this limitation, we incre-
mentally generated the data by repeating steps 1-3
to update demonstration pools and re-trained the
filtering classifiers after each iteration.
The detailed prompts can be found in Ap-
pendix C. In the context prompt, the LLM is asked
to produce “neutral contextual sentences” pertain-
5In the initial demonstration pool, we collected three safe
and three unsafe context-sentence pairs for each demographic
group. The initial demonstration samples and all labeled gen-
eration data will be published.

--- PAGE 4 ---
Figure 1: Example pairs of a context and a sentence
with labels pertaining to a given social demographic
category and group.
ing to the given social group. However, the model
often generated biased sentences due to intrinsic
bias. We labeled them as unsafe contexts. In the
sentence generation case, we separated unsafe and
safe demonstration pools and instructions for class-
conditional sentence generation.
At the context filtering step, the filter model
classified generated sentences pertaining the target
demographics, and annotators only labeled well-
conditioned outputs. In the sentence filtering step,
on the other hand, we first over-generated sentences
for each context, i.e., three sentences for each class.
We then selected the most ambiguous sentence for a
safe sentence classifier to label. The ambiguity was
measured by the estimated max variability (Liu
et al., 2022; Swayamdipta et al., 2020). Conse-
quently, by excluding obvious and easy-to-learn
samples in the dataset, this filtering process served
to ensure that the constructed dataset has an appro-
priate level of difficulty.
3.3 Annotation
The contexts and sentences were then labeled by
crowd workers according to the following guide-
lines (See Figure 1 for examples):
•Context. The role of the context is to rep-
resent a scenario in which an LLM needs to
speak about a demographic group. Each gen-
erated context is first annotated as safe if it
only contains objective information and thus
does not cause harm to the targeted demo-
graphic group, and unsafe , otherwise. If la-
beled unsafe , it is further labeled as an ex-
pression of 1) stereotypes (cognitive bias), 2)
prejudice (emotional bias), 3) discrimination
(behavioral bias), and/or 4) other , adopting
the taxonomy by Fiske (2023). Here, sub-
classes 2 and 3 are combined due to the rareContext Sentence Train Valid Test All
SafeSafe 11,630 1,427 1,382 14,439
Unsafe 8,521 1,060 1,092 10,673
Total 20,151 2,487 2,474 25,112
UnsafeSafe 2,537 320 317 3,174
Unsafe 4,589 596 617 5,802
Total 7,126 916 934 8,976
UndecidedSafe 58 45 7 6
Unsafe 68 48 11 9
Total 93 18 15 126
Total 27,370 3,421 3,423 34,214
Table 3: The number of instances for all label combina-
tions in K OSBI. (Refer to Table 7 for subclass.)
occurrences observed during a pilot study.
•Sentence. Each sentence generated for a
given context is first annotated as safe orun-
safe, depending on whether or not it harms
the targeted demographic group. If labeled
unsafe , the sentence is further labeled as an
expression of one of the bias types or other,
same as above, except subclasses 2 and 3 are
not combined this time. Note, a seemingly
safe sentence may be unsafe dependent on
its context. For instance, a sentence simply
agreeing (e.g., “Yes, that is true.”) to an un-
safe context (e.g., “ [Demographic Group] are
always lazy.”) is unsafe . In such cases, it is ad-
ditionally marked as (implicit) , and (explicit)
if the sentence is unsafe itself.
To label the filtered outputs, 200 crowd work-
ers affiliated across a wide range of social demo-
graphics were hired (Table 12). The detailed well-
being information of workers can be found in Ap-
pendix C. They evaluated the qualities of contexts
and sentences in terms of understandability and
coherences between the pairs. Data that did not
meet the criteria were excluded. They were then
asked to label them. In particular, in the case of
unsafe sentences, they were requested to find the
social groups targeted in the context-sentence pair
for explainability. The annotation guidelines are
shown in Appendix H.
In the human evaluation step, three crowd work-
ers annotated contexts and sentences, and the fi-
nal labels were decided by a majority vote. First,
in labeling contexts as safe or unsafe, the inner-
annotator agreement by Krippendorff’s αis 0.459
for binary (safe/unsafe) classes. The agreement is

--- PAGE 5 ---
Datasets Models Macro F1 (%)
BEEP! KcBERT 52.90
APEACH KcBERT 48.82
KOLD KLUE-BERT 38.15
Hatescore KcBERT 40.28
Unsmile KcBERT 48.02
Ours KLUE-BERT 69.94
Ours KcELECTRa 71.21
Table 4: Comparison of classification performance on
our test set. Fine-tuned models on the previous datasets
and ours are compared.
lower if we consider subclasses of unsafe contexts
(α= 0.359). For sentence annotations, the αis
0.256 for labeling them as safe or unsafe. This sug-
gests that determining the labels for the sentences is
harder. This is expected given that both the context
and the sentence need to be considered for labeling
a sentence, whereas contexts are self-contained.
3.4 The Resulting Dataset
KOSBIconsists of 34,214 context-sentence pairs as
summarized in Table 3. There are 25,112 (73.4%)
and 8,976 (26.2%) of safe and unsafe contexts, re-
spectively. Also, there are 17,619 (51.5%) and
16,484 (48.2%) safe and unsafe sentences. Train-
ing, validation, and test sets are randomly separated
as 80%, 10%, and 10%, respectively, considering
the balance of social group distribution.
4 Experimental Results
To improve the safety of LLMs towards social
groups, we explore a simple filtering-based mod-
eration approach. In this section, we first build
a safe sentence classification. Then we automati-
cally evaluate LLMs’ generation given a context
with the safety classifier. Finally, we sample the
safest sentence among over-generated sentence can-
didates. The efficacy of the filtering approach is
demonstrated by human evaluation.
4.1 Safe Sentence Classification
We train the safe sentence classifier by fine-
tuning KLUE-BERT (Park et al., 2021) and
KcELECTRa (Lee, 2021)6. To identify unsafe sen-
tences in context, the context and the sentence are
concatenated and then fed into the models. We
also simply augment data by using context data
and their labels, resulting in the best macro-F1 of
71.21% as shown in Table 4. The performance
implies that the proposed dataset is challenging.
6We used the latest version of the model: https://
huggingface.co/beomi/KcELECTRA-base-v2022.To validate the novelty of our dataset, we
employed classifiers trained on previous Korean
hate speech corpus: BEEP! (Moon et al., 2020),
APEACH (Yang et al., 2022), KOLD (Jeong et al.,
2022), Hatescore, and Unsmile (Kang et al., 2022)
to evaluate the test set of our KOSBI7. As a result,
in Table 4, all classifiers consistently show poor
accuracies, approximating a random guess. These
poor performances strongly suggest that our dataset
is dissimilar from existing corpora, mainly address-
ing Korean hate speech. Such difference may stem
from the fact that our dataset includes instances of
context dependent and implicit hate speech, which
contrasts with the overt and explicit instances found
in the previous corpus sourced from online plat-
form comments.
4.2 Safety Evaluation of LLMs
Using the safe sentence classifier, we measure safe
generations of LLMs. Similarly to Gehman et al.
(2020), we evaluate the models with two metrics.
First, the empirical probability of generating a safe
sentence (safe score is ≥0.5) given a context at
least once over multiple kgenerations. We measure
the probability with different sizes of k. The higher
the safety probability, the more frequently filter-
moderated outputs are safe. The second metric is
the expected average safety score of safe sentences
overk= 8generations. This means that the higher
the expected average safety, the model likely gen-
erates more safe sentences on average.
We evaluate HyperCLOV A with different model
sizes (6.9B, 13B, 30B, and 82B), and GPT-3 (175B)
8. We sample a subset of the test set to contain 30
contexts per each demographical category, i.e., a
total of 450 contexts. The LLMs generate sentences
given the contexts in a zero-shot generation setup.
The prompt used for this experiment is listed in
Appendix C.
Table 5 presents the evaluation results. First, the
empirical probability of generating safe sentences
7For a fair comparison, we employed the published BERT-
base-sized checkpoints of each model. Classifiers except for
KOLD are pretrained on KcBERT (Lee, 2020). For KOLD,
we manually fine-tuned KOLD dataset on KLUE-BERT by
following the paper’s experiment setup because there are no
publicly shared checkpoints nor train/valid/test split.
8The largest HyperCLOV A model (82B) was trained on
HyperCLOV A Corpus consisting of 300B tokens, and the re-
mains are further trained with 30B of a spoken dataset. The
version of ’text-davinci-003’ is used as the GPT-3 model. Note
also that HyperCLOV A models are not trained by instruct-
tuning or reinforcement learning from human feedback, like-
wise ’text-davinci-003’.

--- PAGE 6 ---
ModelSafety ProbabilityExp. Avg. Safety
k=1 2 4 8
GPT-3 (175B) .809 .902 .956 .969 .625 ±.083
HyperClova (6.9B) .673 .796 .796 .876 .589 ±.102
HyperClova (13B) .713 .789 .789 .862 .581 ±.096
HyperClova (30B) .711 .844 .844 .900 .588 ±.105
HyperClova (82B) .647 .813 .813 .887 .575 ±.100
Table 5: Safety evaluations of LLM’s continuations after
given contexts. Left: The empirical probability of gen-
erating safe sentence at lease once over kgenerations.
Right: Expected average safety score of safe sentences
with standard deviations over 8 generations.
Figure 2: Human evaluation on the subset of the test
set. We compared two HyperCLOV A models (82B and
30B) and the GPT-3 (175B; text-davinci-003) models,
for both with and without filtering.
increases as generation increases for all LLMs. In
other words, when the HyperCLOV A -82B gener-
ates 8 sentences per context, 88.7% of continua-
tions are safe w.r.t the classifier model. Notably, the
more over-generations, the more improved safety.
Next, for the expected average of safety score, we
could not find distinguished differences among
different sizes of HyperCLOV A . Overall, GPT-3
shows more improved safety probability and score
than HyperCLOV A by the automatic evaluations.
Furthermore, we divide the results into those
generated from a safe context and an unsafe context
in order to measure how the safety of the context
affects the model’s continuation. As can be seen
by comparing both results presented in Table 9,
models generate more unsafe sentences when an
unsafe context was given, while all models generate
99% of safe continuations when conditioned on a
safe context in k= 8settings.
4.3 Filter-based Moderation
We demonstrate the efficacy of filter-based mod-
eration of unsafe sentence generation. The filter-
ing approach samples the safest sentence among
8 generations. We conduct a human-evaluation
experiment to assess the quality and safety of gen-
eration results. The evaluation results of the three
models — GPT-3, HyperCLOV A 30B, and 82B are
compared in Figure 2 and Table 6.
With the filtering process, we find that the ra-
Figure 3: Moderation results on each category in the
augmented test set. Left: Safe response ratio from
human evaluation results. Right: Safe sentence classifi-
cation performance of the best classifier (KcELECTRa).
The vertical lines represent the averages of safe response
and accuracy for all categories. Categories are ordered
by descend of the classifier’s accuracy.
tio of unsafe generations decreases for all mod-
els by 16.47%p on average. We observe that the
filter-based moderation remarkably improves the
safety of all LLMs by reducing unsafe generation
as 16%, 15%, and 18.5% and by increasing safe
sentences as 15.6%, 15.3%, and 18.7% for GPT-
3, 82B- HyperCLOV A , and 30B- HyperCLOV A , re-
spectively. It is interesting that the ratio of the
ambiguous sentences generated by GPT-3 does not
decrease despite the filtering.
Table 6 presents qualitative results of sentences
generated by each model and the effects of the filter-
based moderation. Inconsistent with the results
in Figure 2, the filter-based moderation does not
improve the quality of generated sentences. This
means the filtering is likely to slightly sacrifice the
coherency of generation by playing the role of con-
straints as a side effect against enhancing safety.
However, overall quality scores of all LLMs are
competitive enough, and HyperCLOV A presents
better qualitative performance than GPT-3, consis-
tent with the results in Figure 2.
4.4 Social Bias Mitigation Level by Category
We analyze the moderation results by the 15 de-
mographic categories. Before getting a result, we
augmented the test set with additional annotated
data to increase the number of samples per category
and the reliability of the test results. As a result,
ouraugmented test set consists of 6,801 (context,

--- PAGE 7 ---
Quality Assessments
Grammatical
Error-Free (%)Understandability
(%)Pertaning to Target
Social Group (%)Context (%)
CoherencyOverall (%)
GPT-3 (175B) 89.8 80.2 90.0 71.6 32.0
GPT-3 (175B) + filtering 89.3 80.9 87.3 69.1 31.6
HyperCLOV A (80B) 99.1 97.1 93.6 89.6 49.3
HyperCLOV A (80B) + filtering 99.6 96.2 93.3 88.9 54.0
HyperCLOV A (30B) 99.3 98.2 95.8 93.8 61.6
HyperCLOV A (30B) + filtering 100 97.3 94.7 91.6 56.9
Table 6: Human evaluation on the subset of test set. Comparisons between unfiltered responses and filtered responses
among 8 generations from GPT-3 (175B; ‘text-davinci-003’), HyperClova (82B and 30B). Overall score denotes the
percentage of instances that are marked as passed all quality assessment questions by all evaluators.
sentence) pairs (see Table 10 for detailed statistics
for it). For experiments conducted in this section,
we sample a small subset from the augmented test
set to contains at least 48 contexts per category, re-
sulting in 1,746 contexts. All other settings follow
of them in Sec 4.3.
Figure 3 presents the human evaluation results
of filter-based moderation by each demographic
category. Each category displays a different ratio
of generated safe sentences. By comparing with
and without filter-based moderation, we can no-
tice that the efficacy of the filtering process also
varies. For example, we find the biggest increase
of safe generations ratio in Disability status cate-
gory (+64.0%) while the smallest in Marital status
(+0.85%). Within the category, the differences also
exist between models; such as in Disability sta-
tuscategory, HyperCLOV A -82B got an increase
of 33.3%p but HyperCLOV A -30B got only 4.1%p
(See Figure 6 for the results by the group for all
three models).
Since filter-based moderation utilizes a filter
model, it it natural to assume that there could ap-
pear to be a correlation between the performance
of the filter model and the moderation efficacy. To
identify any tendencies between the two, we have
also included the accuracy of the filter model in
Figure 3. We, however, couldn’t find a strong cor-
relation between them. We conjecture the reason is
the relatively small differences in accuracy across
the categories or the sampled set used here not be-
ing large enough. Further analysis is expected in
future work. Despite this, the filter-based moder-
ation approach demonstrates the effectiveness for
allsocial demographic categories. It is crucial to
scrutinize and improve the models’ safety for fair
consideration of each demographic category and
group.5 Conclusion
To alleviate unsafe social bias of LLMs, we pro-
pose a large-scale social bias dataset related to
safety addressing the Korean language and cultures,
KOSBI. Our dataset covers 72 demographic groups
in 15 categories, consisting of 34k of situation con-
text and following sentence pairs. To construct
KOSBI, we employ a human-LLM collaboration
framework, where HyperCLOV A generates con-
texts and sentences, and human annotators label
them as safe or unsafe. Extensive experiments
present our dataset as differentiated from existing
prevalent datasets on social bias and hate speech.
Moreover, the results show the filter model trained
with our dataset remarkably improves the ratio of
generating safe sentences for various LLMs such
as GPT-3 and HyperCLOV A with diverse model
sizes, which presents the efficacy of our dataset.
Limitations
The proposed KOSBIaddresses social bias based
on Korean culture with the Korean language. This
Korean-specific property might restrict the effec-
tiveness of our dataset in Korea and its similar cul-
tures. However, our dataset construction and eval-
uation protocol can contribute to a helpful guide
for other research groups on AI safety to build the
datasets for their cultures and languages.
The performance of the filter models for harm-
less sentence classification in this study is not very
competitive. We leave it as a future research topic
to make a filter classifier with higher accuracy on
our dataset because the goal of this study is not to
make a strong social bias filter itself.

--- PAGE 8 ---
Ethics Statement
We expect that our KOSBIcan considerably con-
tribute to enhancing the safe usage of LLMs’ ap-
plications by reducing risks caused by social bias.
Constructing datasets on harmfulness is likely to
cause stress on the contributors, such as human ex-
perts and crowd workers. To minimize their stress
exposure, we use HyperCLOV A to generate con-
texts and sentences and ask humans to label them.
Furthermore, our study was approved by the pub-
lic institutional review board (IRB) affiliated with
the Ministry of Health and Welfare of South Korea
(P01-202211-01-016).
Acknowledgements
The authors would like to thank all committee mem-
bers of the AI Ethics Forum for Human at NA VER,
including Meeyoung Cha, Byoungpil Kim, Eun-Ju
Lee, Yong Lim, Alice Oh, Sangchul Park, Woochul
Park, Joonha Jeon, Jonghyun Kim, Do Hyun Park,
and Eunjung Cho, for their constructive feedback
and helpful discussions. We are also grateful to
Ryumin Song, Jaehyeon Kim, and Jisun Kim at
Crowdworks, who cooperated in the data collec-
tion process, and the 200 crowdworkers who partic-
ipated in the process. In addition, the authors thank
the research members of SNU-NA VER Hyperscale
AI Center and KAIST-NA VER Hypercreative AI
Center for discussion and thank Haksoo Ko and
Yejin Choi for valuable discussion. This project is
financially supported by NA VER Cloud.
References
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Ben Mann, and Jared Kaplan. 2022a. Training a
helpful and harmless assistant with reinforcement
learning from human feedback.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, LianeLovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022b. Constitutional ai: Harmless-
ness from ai feedback.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5454–
5476, Online. Association for Computational Lin-
guistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Christoph Demus, Jonas Pitz, Mina Schütz, Nadine
Probol, Melanie Siegel, and Dirk Labudde. 2022.
Detox: A comprehensive dataset for German offen-
sive language and conversation analysis. In Proceed-
ings of the Sixth Workshop on Online Abuse and
Harms (WOAH) , pages 143–153, Seattle, Washington
(Hybrid). Association for Computational Linguistics.
Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng,
Fei Mi, Helen Meng, and Minlie Huang. 2022.
COLD: A benchmark for Chinese offensive language
detection. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 11580–11599, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
Emily Dinan, Gavin Abercrombie, A. Bergman, Shan-
non Spruit, Dirk Hovy, Y-Lan Boureau, and Verena
Rieser. 2022. SafetyKit: First aid for measuring
safety in open-domain conversational systems. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 4113–4133, Dublin, Ireland. As-
sociation for Computational Linguistics.
Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell
Forbes, and Yejin Choi. 2021. Moral stories: Situ-
ated reasoning about norms, intents, actions, and
their consequences. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 698–718, Online and Punta Cana,

--- PAGE 9 ---
Dominican Republic. Association for Computational
Linguistics.
Susan T. Fiske. 2023. Prejudice, discrimination, and
stereotyping . Noba textbook series: Psychology.
DEF Publisher. http://noba.to/jfkx7nrd .
Maxwell Forbes, Jena D. Hwang, Vered Shwartz,
Maarten Sap, and Yejin Choi. 2020. Social chem-
istry 101: Learning to reason about social and moral
norms. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 653–670, Online. Association for
Computational Linguistics.
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda
Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,
Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
Andy Jones, Sam Bowman, Anna Chen, Tom Con-
erly, Nova DasSarma, Dawn Drain, Nelson Elhage,
Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,
Tom Henighan, Danny Hernandez, Tristan Hume,
Josh Jacobson, Scott Johnston, Shauna Kravec,
Catherine Olsson, Sam Ringer, Eli Tran-Johnson,
Dario Amodei, Tom Brown, Nicholas Joseph, Sam
McCandlish, Chris Olah, Jared Kaplan, and Jack
Clark. 2022. Red teaming language models to re-
duce harms: Methods, scaling behaviors, and lessons
learned.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020. RealToxi-
cityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
3356–3369, Online. Association for Computational
Linguistics.
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
ToxiGen: A large-scale machine-generated dataset
for adversarial and implicit hate speech detection.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3309–3326, Dublin, Ireland.
Association for Computational Linguistics.
Dan Hendrycks, Collin Burns, Steven Basart, Andrew
Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
2021. Aligning AI with shared human values. In In-
ternational Conference on Learning Representations .
Younghoon Jeong, Juhyun Oh, Jongwon Lee, Jaimeen
Ahn, Jihyung Moon, Sungjoon Park, and Alice Oh.
2022. KOLD: Korean offensive language dataset.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages10818–10833, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
TaeYoung Kang, Eunrang Kwon, Junbum Lee,
Youngeun Nam, Junmo Song, and JeongKyu Suh.
2022. Korean online hate speech dataset for multil-
abel classification: How can social science improve
dataset on hate speech?
Zachary Kenton, Tom Everitt, Laura Weidinger, Ia-
son Gabriel, Vladimir Mikulik, and Geoffrey Irving.
2021. Alignment of language agents.
Boseop Kim, HyoungSeok Kim, Sang-Woo Lee,
Gichang Lee, Donghyun Kwak, Jeon Dong Hyeon,
Sunghyun Park, Sungju Kim, Seonhoon Kim, Dong-
pil Seo, Heungsub Lee, Minyoung Jeong, Sungjae
Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim,
Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon
Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh,
Sookyo In, Jinseong Park, Kyungduk Kim, Hiun
Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham,
Dongju Park, Min Young Lee, Jaewook Kang, Inho
Kang, Jung-Woo Ha, Woomyoung Park, and Nako
Sung. 2021. What changes can large-scale language
models bring? intensive study on HyperCLOV A:
Billions-scale Korean generative pretrained trans-
formers. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 3405–3424, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing
Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and
Maarten Sap. 2022. ProsocialDialog: A prosocial
backbone for conversational agents. In Proceedings
of the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 4005–4029, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Jean Lee, Taejun Lim, Heejun Lee, Bogeun Jo, Yangsok
Kim, Heegeun Yoon, and Soyeon Caren Han. 2022.
K-MHaS: A multi-label hate speech detection dataset
in Korean online news comment. In Proceedings of
the 29th International Conference on Computational
Linguistics , pages 3530–3538, Gyeongju, Republic
of Korea. International Committee on Computational
Linguistics.
Junbum Lee. 2020. Kcbert: Korean comments bert.
InProceedings of the 32nd Annual Conference on
Human and Cognitive Language Technology , pages
437–440.
Junbum Lee. 2021. Kcelectra: Korean comments elec-
tra.https://github.com/Beomi/KcELECTRA .
Tao Li, Daniel Khashabi, Tushar Khot, Ashish Sab-
harwal, and Vivek Srikumar. 2020. UNQOVERing
stereotyping biases via underspecified questions. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 3475–3489, Online.
Association for Computational Linguistics.

--- PAGE 10 ---
Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and
Yejin Choi. 2022. WANLI: Worker and AI collabora-
tion for natural language inference dataset creation.
InFindings of the Association for Computational
Linguistics: EMNLP 2022 , pages 6826–6847, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021.
Scruples: A corpus of community ethical judg-
ments on 32,000 real-life anecdotes. Proceedings
of the AAAI Conference on Artificial Intelligence ,
35(15):13470–13479.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470–3487, Dublin, Ireland.
Association for Computational Linguistics.
Jihyung Moon, Won Ik Cho, and Junbum Lee. 2020.
BEEP! Korean corpus of online news comments for
toxic speech detection. In Proceedings of the Eighth
International Workshop on Natural Language Pro-
cessing for Social Media , pages 25–31, Online. As-
sociation for Computational Linguistics.
Hamdy Mubarak, Sabit Hassan, and Shammur Absar
Chowdhury. 2022. Emojis as anchors to detect arabic
offensive language and hate speech.
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
StereoSet: Measuring stereotypical bias in pretrained
language models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 5356–5371, Online. Association for
Computational Linguistics.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 1953–1967, Online. As-
sociation for Computational Linguistics.
Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik
Cho, Ji Yoon Han, Jangwon Park, Chisung Song, Jun-
seong Kim, Youngsook Song, Taehwan Oh, Joohong
Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,
Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo
Kim, Myeonghwa Lee, Seongbo Jang, Seungwon
Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee,
Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy
Park, Lucy Park, Alice Oh, Jung-Woo Ha (NA VER
AI Lab), Kyunghyun Cho, and Kyunghyun Cho.
2021. Klue: Korean language understanding eval-
uation. In Proceedings of the Neural Information
Processing Systems Track on Datasets and Bench-
marks , volume 1. Curran.Alicia Parrish, Angelica Chen, Nikita Nangia,
Vishakh Padmakumar, Jason Phang, Jana Thompson,
Phu Mon Htut, and Samuel Bowman. 2022. BBQ:
A hand-built bias benchmark for question answering.
InFindings of the Association for Computational
Linguistics: ACL 2022 , pages 2086–2105, Dublin,
Ireland. Association for Computational Linguistics.
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoffrey Irving. 2022. Red teaming
language models with language models. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 3419–3448,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Paul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak
Waseem, Helen Margetts, and Janet Pierrehumbert.
2021. HateCheck: Functional tests for hate speech
detection models. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 41–58, Online. Association for
Computational Linguistics.
Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender bias in
coreference resolution. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers) ,
pages 8–14, New Orleans, Louisiana. Association for
Computational Linguistics.
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-
sky, Noah A. Smith, and Yejin Choi. 2020. Social
bias frames: Reasoning about social and power im-
plications of language. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 5477–5490, Online. Association
for Computational Linguistics.
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,
and Nanyun Peng. 2019. The woman worked as
a babysitter: On biases in language generation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3407–
3412, Hong Kong, China. Association for Computa-
tional Linguistics.
Irene Solaiman and Christy Dennison. 2021. Process
for adapting language models to society (palms) with
values-targeted datasets. In Advances in Neural Infor-
mation Processing Systems , volume 34, pages 5861–
5873. Curran Associates, Inc.
Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng,
Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan
Zhu, and Minlie Huang. 2022. On the safety of con-
versational models: Taxonomy, dataset, and bench-
mark. In Findings of the Association for Compu-
tational Linguistics: ACL 2022 , pages 3906–3923,

--- PAGE 11 ---
Dublin, Ireland. Association for Computational Lin-
guistics.
Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie,
Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,
and Yejin Choi. 2020. Dataset cartography: Mapping
and diagnosing datasets with training dynamics. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9275–9293, Online. Association for Computa-
tional Linguistics.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019a. Universal adversarial
triggers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 2153–2162, Hong
Kong, China. Association for Computational Linguis-
tics.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019b. Universal adversarial
triggers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 2153–2162, Hong
Kong, China. Association for Computational Linguis-
tics.
William Warner and Julia Hirschberg. 2012. Detecting
hate speech on the world wide web. In Proceedings
of the Second Workshop on Language in Social Me-
dia, pages 19–26, Montréal, Canada. Association for
Computational Linguistics.
Michael Wiegand, Josef Ruppenhofer, and Thomas
Kleinbauer. 2019. Detection of Abusive Language:
the Problem of Biased Datasets. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 602–608, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Alexandros Xenos, John Pavlopoulos, Ion Androut-
sopoulos, Lucas Dixon, Jeffrey Sorensen, and Leo
Laugier. 2021. Toxicity detection can be sensitive to
the conversational context.
Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Guru-
rangan, Maarten Sap, and Dan Klein. 2021a. Detoxi-
fying language models risks marginalizing minority
voices. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2390–2397, Online. Association for
Computational Linguistics.
Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason
Weston, and Emily Dinan. 2021b. Bot-adversarial di-
alogue for safe conversational agents. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:Human Language Technologies , pages 2950–2968,
Online. Association for Computational Linguistics.
Kichang Yang, Wonjun Jang, and Won Ik Cho. 2022.
APEACH: Attacking pejorative expressions with
analysis on crowd-generated hate speech evaluation
datasets. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2022 , pages 7076–7086,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 15–20, New
Orleans, Louisiana. Association for Computational
Linguistics.

--- PAGE 12 ---
A The K OSBIDataset
A.1 Domain and Categories of Social
Demographics
The entire social demographic categories and
groups are listed in Table 8.
A.2 Example Data
Figure 4: Example pairs of a context and a sentence
with labels pertaining to a given social demographic
category and group. Note, "그 ᆷᄉ ᅮᄌ ᅥ "is a Korean buz-
zword, roughly meaning "Silver spoon" or "Privileged
background" in English.
A.3 Details of Unsafe Label
Unsafe sub-labels # data
ContextStereotypical 4,719
Prejudice / Discrimination 407
Other 1,590
Undefined 2,260
SentenceStereotypical 8,197
Prejudice 1,085
Discrimination 655
Other 336
Undefined 6,905
Table 7: Distribution of the unsafe sub-labels of context
and sentence. Undefined represents cases where three
annotators could not decide the label through major
voting, but 2 or more annotators chose one of the unsafe
sub-labels.Category Social Group
Gender identity†Male
Female
Others
Sexual orientation†Homosexual
Age & Generation†Baby
Childern
Teenagers
Young people
Middle-aged
Old people
Baby bommeres
386 Generation
Generation X
Milennials
Generation Z
Alpha Generation
Race, Ethnicity & Nationality†South Korean
North Korean
Chinese
Japanese
American (U.S.)
Russian
Asian
African
European
Americans, Oceanians
People of color / White
Religion†Nonreligious
Protestantism
Buddhism
Catholic
Islam
Others
Disability status†Disability
Physical appearance†Face Appearance
Body Type
Sexual Appearance
Others
Political orientation†Liberal
Conservative
Others
Socio-economic status†Employment Type
Economic Condition
Occupation
Domestic area of originSeoul
Gyeonggi-do
Chungcheong-do
Gyeongsang-do
Jeolla-do
Jeju-do
Gangwon-do
Korean residents from abroad
Marital statusMarried
Single
Divorced
Remarriaed
Common-law marriage
Others
Pregnancy & BirthPregnant Person
Person Who Gave Birth
Pregnancy Before Marriage
Others
Family formFoster Family
Broken Family
Multicultural Family
Step Family
Others
Criminal recordViolent Criminal Convictions
Others
Education & University & MajorAcademic Background
Universities
Major
Table 8: Category and demographic groups considered
inKOSBI. The categories in both UDHR and NHRCK
are marked by†.

--- PAGE 13 ---
B HyperClova Generation
Hyperparameters We use the same hyperparam-
eters for generating contexts and sentences: top- p
sampling with p= 0.8, a temperature of 0.5, repeat
penalty of 5, and stop word of ”\n”. We gradually
increased top- pand temperature every time we met
duplication generations.
C Prompts of LLMs
Context Generation Prompt Among 10 demon-
stration samples, we collect 5 demonstrations from
the pool which has the same demographic category
with what we are generating and 3 demonstrations
from the pool which has the same demographic
group. Detailed prompt is below:
ᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢᄐ ᅳ ᆨ정한상황ᄋ ᅵ
ᄂ ᅡ전반ᄌ ᅥ ᆨᄋ ᅵᆫ현상을중리 ᆸᄌ ᅥ ᆨᄋ ᅳᄅ ᅩᄌ ᅡ ᆨ성ᄒ ᅡᄉ ᅦᄋ ᅭ .⇒General
Instruction
{category}분ᄅ ᅲᄋ ᅦ ᄄ ᅡᄅ ᅡ {social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡ
ᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성⇒Category and demographic group
중리 ᆸᄌ ᅥ ᆨ상황문장: {context}
###
...
###
{category}분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ {social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢ
ᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성
중리 ᆸᄌ ᅥ ᆨ상황문장:
Safe Sentence Generation Prompt Among 10
demonstration samples, we collect 5 demonstra-
tions from the pool which has the same context
label with what we are generating.
ᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢ "중리 ᆸᄌ ᅥ ᆨ상황
문장"ᄃ ᅱᄋ ᅦᄋ ᅵᄋ ᅥᄌ ᅵ는ᄌ ᅡ연ᄉ ᅳᄅ ᅥ운"ᄃ ᅡᄋ ᅳ ᆷ문장"을ᄌ ᅡ ᆨ성ᄒ ᅡ
ᄉ ᅦᄋ ᅭ . "ᄃ ᅡᄋ ᅳ ᆷ문장"ᄋ ᅳ ᆫᄒ ᅢ당ᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄃ ᅢᄒ ᅢᄀ ᅩ정관
념과편견ᄋ ᅵ없는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅵᄑ ᅭ현할ᄉ ᅮ있는안전한문장
이 ᆸᄂ ᅵᄃ ᅡ .ᄇ ᅵᄒ ᅡ와ᄎ ᅡ별ᄋ ᅴᄋ ᅴᄃ ᅩᄀ ᅡ없ᄀ ᅩ,ᄀ ᅩ ᆼ정ᄒ ᅡᄆ ᅧᄀ ᅩ정관
념ᄌ ᅥ ᆨᄋ ᅵᄌ ᅵᄋ ᅡ ᆭᄋ ᅳ ᆫᄑ ᅭ현,ᄃ ᅡ양성을존중ᄒ ᅡᄀ ᅩ ,ᄋ ᅵᆫ간ᄋ ᅴ존ᄋ ᅥ ᆷ
성과ᄌ ᅡᄋ ᅲ와ᄀ ᅵ본ᄀ ᅯᆫ을ᄇ ᅩ장합ᄂ ᅵᄃ ᅡ .⇒General Instruction
{category}분ᄅ ᅲᄋ ᅦ ᄄ ᅡᄅ ᅡ {social group}ᄋ ᅵᆫᄉ ᅡᄅ ᅡ ᆷᄋ ᅦ ᄃ ᅢ
ᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group
중리 ᆸᄌ ᅥ ᆨ상황문장: {context}
ᄃ ᅡᄋ ᅳ ᆷ문장: {sentence}
###
...
###
{category}분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ {social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢ
ᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group
중리 ᆸᄌ ᅥ ᆨ상황문장: {context}
ᄃ ᅡᄋ ᅳ ᆷ문장:
Unsafe Sentence Generation Prompt All pro-
cess is same with safe sentence generation except
the prompt below:
ᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢ "중리 ᆸᄌ ᅥ ᆨ상황문장"ᄃ ᅱᄋ ᅦᄋ ᅵᄋ ᅥᄌ ᅵ는ᄌ ᅡ연ᄉ ᅳᄅ ᅥ운"ᄃ ᅡᄋ ᅳ ᆷ문장"을ᄌ ᅡ ᆨ성ᄒ ᅡᄉ ᅦ
ᄋ ᅭ. "ᄃ ᅡᄋ ᅳ ᆷ문장"ᄋ ᅳ ᆫᄒ ᅢ당ᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄃ ᅢᄒ ᅢᄀ ᅩ정관념과
편견을ᄀ ᅡᄌ ᅵᆫᄉ ᅡᄅ ᅡ ᆷᄋ ᅵᄑ ᅭ현할ᄉ ᅮ있는ᄋ ᅱᄒ ᅥ ᆷ한문장이 ᆸᄂ ᅵᄃ ᅡ .
ᄇ ᅵᄒ ᅡ와ᄎ ᅡ별ᄋ ᅴᄋ ᅴᄃ ᅩᄀ ᅡ있ᄀ ᅩ,불ᄀ ᅩ ᆼ정ᄒ ᅡᄀ ᅩ논란을불ᄅ ᅥ이 ᆯ
ᄋ ᅳ키 ᆯᄉ ᅮ있ᄋ ᅳᄆ ᅧ ,ᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄂ ᅢᄃ ᅡ양성을ᄒ ᅢ치 ᆯᄉ ᅮ있ᄉ ᅳ ᆸ
ᄂ ᅵᄃ ᅡ . ⇒General Instruction
{category}분ᄅ ᅲᄋ ᅦ ᄄ ᅡᄅ ᅡ {social group}ᄋ ᅵᆫᄉ ᅡᄅ ᅡ ᆷᄋ ᅦ ᄃ ᅢ
ᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group
중리 ᆸᄌ ᅥ ᆨ상황문장: {context}
ᄃ ᅡᄋ ᅳ ᆷ문장: {sentence}
###
...
###
{category}분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ {social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢ
ᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group
중리 ᆸᄌ ᅥ ᆨ상황문장: {context}
ᄃ ᅡᄋ ᅳ ᆷ문장:
Zero-shot Prompt for Human-evaluation The
following prompt is used for the human-evaluation
in Section 4.3
ᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢ ”상황문장”ᄃ ᅱ
ᄋ ᅦᄋ ᅵᄋ ᅥᄌ ᅵ는ᄌ ᅡ연ᄉ ᅳᄅ ᅥ운”ᄃ ᅡᄋ ᅳ ᆷ문장”을ᄌ ᅡ ᆨ성ᄒ ᅡᄉ ᅦᄋ ᅭ .
##
{category}분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ ”{social group}”ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦ
ᄃ ᅢᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성
상황문장: {context}
ᄃ ᅡᄋ ᅳ ᆷ문장:
D Modeling Details
All the training processes described below are un-
der PyTorch-Lightning9and Huggingface10envi-
ronments. For training, the search space for hyper-
parameters is:
•learning rate : [ 1e−5,2e−5,3e−5,4e−5,
5e−5]
• batch size : [32, 48]
• gradient clipping value : [0.0, 1.0]
• epoch : 15
•early stopping : after 5 epochs without im-
provement
D.1 Context Filter Models
We use KcELECTRa (Lee, 2021) as a backbone
model for our context filter model. The demo-
graphic group and the context concatenated by
the separate token([SEP]) are fed to the model to
train the model to predict whether the demographic
group is in the context text. 3,819 and 7,569 data
points are used for training after iterations 1 and 2,
9https://www.pytorchlightning.ai/
10https://huggingface.co/

--- PAGE 14 ---
respectively (80/10/10 split). The best configura-
tion is 5e−5learning rate, 48 batch size, and 0.0
gradient clipping value for both iterations 1 and 2,
showing 83.51% and 90.75% of accuracy for each
test set, respectively.
D.2 Next Sentence Filter Models
We also use KcELECTRa as a backbone model for
our next sentence filter model. Note that the main
purpose of the next sentence filtering process is to
leverage the filter model to collect the most ambigu-
ous samples w.r.t the model. The separate token
concatenates the context and the next sentence, and
the model is trained to predict the unsafeness of
the text. 4,324 and 11,457 data points are used
for training after iterations 1 and 2, respectively
(80/10/10 split). The best hyperparameter setup
is (5e−5learning rate, 32 batch size, 0.0 gradi-
ent clipping value) and ( 2e−5learning rate, 48
batch size, 0.0 gradient clipping value) for itera-
tions 1 and 2, respectively. The accuracies of the
best models are 83.83% (iteration 1) and 69.37%
(iteration 2). Due to ambiguous data points being
augmented for iteration 2, the later model shows
lower accuracy.
D.3 Safe Sentence Classifiers
After collecting all data points, we train a safe sen-
tence classifier. In addition to the KcELECTRa
model, we use KLUE-BERT (Park et al., 2021)
and KcBERT (Lee, 2020) as candidates. As men-
tioned in Section 4.1, we augment data by using
context data. Among six configurations which con-
sist of three models and two datasets (with and
without augmentation), the best model is KcELEC-
TRa with augmentation (71.22% accuracy). The
hyperparameter setup is 1e−5learning rate, 32
batch size, and 0.0 gradient clipping value.
E Safety Evaluations of Continuations
Table 9 shows the safety generation results given
safe and unsafe contexts, respectively. As can be
seen by comparing both results, models generate
more unsafe sentences when an unsafe context is
given, while all models generate 99% of safe con-
tinuations when conditioned on a safe context in
k= 8settings.ModelSafety ProbabilityExp. Avg. Safety
k=1 2 4 8
Safe Context
GPT-3 (175B) .931 .961 .984 .993 .674 ±.083
HyperClova (6.9B) .806 .931 .977 .993 .626 ±.103
HyperClova (13B) .766 .918 .974 .990 .642 ±.108
HyperClova (30B) .809 .941 .977 .990 .647 ±.102
HyperClova (82B) .829 .918 .967 .993 .660 ±.106
Unsafe Context
GPT-3 (175B) .644 .753 .870 .918 .522 ±.082
HyperClova (6.9B) .432 .616 .740 .842 .469 ±.093
HyperClova (13B) .507 .616 .767 .849 .473 ±.099
HyperClova (30B) .363 .514 .603 .712 .443 ±.073
HyperClova (82B) .342 .493 .651 .788 .441 ±.093
Table 9: Safety evaluations of LLM’s continuations af-
ter given safe(top) and unsafe (bottom) contexts, respec-
tively. All metrics are calculated as the same manner as
in Table 5.
Context Safe Unsafe UndecidedTotal
Sentence Safe Unsafe Total S. U. T. S. U. T.
Test set 1,382 1,092 2,474 317 617 934 7 11 15 3,423
Augmented 2,681 2,268 4,949 589 1,239 1,828 11 13 24 6,801
Table 10: The number of instances for the test and
augmented test sets.
Figure 5: Human evaluation on the subset of the aug-
mented test set. For all three models, filter-based mod-
eration shows efficacy on reducing unsafe generations.
F Results and Analyses on Augmented
Test Set
As mentioned in Sec 4.4, we augmented our test set
with additional annotated data to increase the relia-
bility of test results. As a result, the augmented test
set has 6,801 data points (See Table 10). Among
them, we randomly sampled 1,746 contexts for the
human-evaluation experiments, which is the same
procedure described in Sec 4.3. As seen in Figure 5,
we can still observe that the filter-based moderation
reduces unsafe generations for all three models. Ta-
ble 11 presents qualitative results for another subset
of the test set.

--- PAGE 15 ---
Quality Assessments
Grammatical
Error-Free (%)Understandability
(%)Pertaning to Target
Social Group (%)Context (%)
CoherencyOverall (%)
GPT-3 (175B) 84.4 77.4 87.3 70.8 30.2
GPT-3 (175B) + filtering 86.4 79.4 86.5 71.1 30.1
HyperCLOV A (80B) 98.9 97.9 93.9 90.5 56.5
HyperCLOV A (80B) + filtering 99.3 97.5 92.5 88.9 56.0
HyperCLOV A (30B) 99.0 98.3 95.4 93.0 62.6
HyperCLOV A (30B) + filtering 99.1 97.9 93.6 91.8 60.0
Table 11: Human evaluation on the subset of augmented test set. Following the Table 6, comparisons between
unfiltered responses and filtered responses among 8 generations from GPT-3 (175B; ‘text-davinci-003’), HyperClova
(82B and 30B) are shown.
Figure 6: Moderation results on each category in the augmented test set. Left: Safe response ratio from human
evaluation results. Right: Safe sentence classification performance of the best classifier (KcELECTRa). The vertical
lines represent the averages of safe response and accuracy for all categories. Categories are ordered by descend of
the classifier’s accuracy.
G Social Bias Mitigation Level by
Category
Figure 6 shows all results with and without the
filter-based moderation for GPT-3 (175B), Hyper-
CLOV A (82B), and HyperCLOV A (30B). Although
the increment of safety does not strongly correlate
to the performance of the classifier, the filter-based
moderation approach demonstrates the effective-
ness for allsocial demographic categories. It is
crucial to scrutinize and improve the models’ safety
for fair consideration of each demographic category
and group.H Human Annotation
H.1 Crowd Worker Compensation
We utilized one of the representative crowdsourc-
ing platforms in South Korea. Among all appli-
cants to our project, we selected 200 crowd work-
ers. All workers have received reasonable monetary
compensation; 80 KRW per sub-single question.
All workers are expected to finish 2 ∼3 sub-single
questions in one minute, resulting in the minimum
compensation is 9,600 KRW/hour. For reference,
the minimum hourly wage in South Korea is 9260
KRW in 2023. The annotation guidelines and the
interface is depicted in Figure 7 and Figure 8.

--- PAGE 16 ---
H.2 Annotation Demographics
The detailed demographics are presented in Ta-
ble 12. Note that every single data was annotated
by two females and one male or vice versa.
Gender
Male 96 48.0%
Female 103 51.5%
Prefer not to mention 1 0.5%
Age
18-24 4 2.0%
25-34 44 22.0%
35-44 71 35.5%
45-54 55 27.5%
55-64 23 11.5%
65+ 2 1.0%
Prefer not to mention 1 0.5%
Country of Origin
South Korea 199 99.5%
China 1 0.5%
Domestic Area of Origin
Seoul 71 35.5%
Gyeongsang, Daegu, Busan 40 20.0%
Gyeonggi, Incheon 42 21.0%
Jeolla, Gwangju 19 9.5%
Chungcheong, Daejeon, Sejong 22 11.0%
Gangwon 2 1.0%
Jeju 3 1.5%
Prefer not to mention 1 0.5%
Education
College degree - Associate or Bachelor’s 147 73.5%
Graduate or Professional Degree 31 15.5%
High school, GED, etc. 21 10.5%
Prefer not to mention 1 0.5%
Sexual Orientation
Straight 187 93.5%
LGBTQ+ 1 0.5%
Prefer not to mention 12 6.0%
Disability
No 194 97.0%
Yes 1 0.5%
Prefer not to mention 5 2.5%
Total 200
Table 12: Demographics of the crowd workers.H.3 Annotation Guidelines and Interface
Figure 7: Question annotation setup. Q1:
Quality check (understandability and grammati-
cally/semantically error-free). Q2: Pertaining to Target
Social Group. Q3: Label of Context (Safe/Unsafe).

--- PAGE 17 ---
Figure 8: Response annotation setup. Q1: Quality
check (appropriateness to the "Question" and grammat-
ically/semantically error-free). Q2: Label of Sentence
(Safe/Unsafe) Q2-1: (if the sentence is ‘Unsafe’) Label
sub-labels.
