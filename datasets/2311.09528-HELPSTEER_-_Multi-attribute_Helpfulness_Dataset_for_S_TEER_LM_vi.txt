# 2311.09528.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/datasets/2311.09528.pdf
# Kích thước tệp: 543797 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
HELPSTEER: Tập dữ liệu Tính hữu ích Đa thuộc tính cho STEERLM
Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams,
Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau,
Jane Polak Scowcroft, Neel Kant, Aidan Swope, Oleksii Kuchaiev
NVIDIA
{zhilinw, yidong}@nvidia.com
Tóm tắt
Các tập dữ liệu ưu tiên tính hữu ích mã nguồn mở hiện tại không chỉ rõ điều gì khiến một số phản hồi hữu ích hơn và những phản hồi khác kém hữu ích hơn. Các mô hình được huấn luyện trên những tập dữ liệu này có thể vô tình học cách mô hình hóa các hiện tượng của tập dữ liệu (ví dụ: ưu tiên các phản hồi dài hơn nhưng không hữu ích chỉ do độ dài của chúng). Để giảm thiểu vấn đề này, chúng tôi thu thập HELPSTEER, một tập dữ liệu tính hữu ích đa thuộc tính được chú thích cho các khía cạnh khác nhau khiến phản hồi trở nên hữu ích. Cụ thể, tập dữ liệu 37k mẫu của chúng tôi có các chú thích cho tính đúng đắn, tính mạch lạc, độ phức tạp và tính dài dòng ngoài tính hữu ích tổng thể của phản hồi. Huấn luyện Llama 2 70B sử dụng tập dữ liệu HELPSTEER với kỹ thuật STEERLM tạo ra một mô hình đạt điểm 7.54 trên MT Bench, hiện đang là điểm cao nhất cho các mô hình mở không cần dữ liệu huấn luyện từ các mô hình mạnh hơn (ví dụ: GPT4). Chúng tôi phát hành tập dữ liệu này với giấy phép CC-BY-4.0 tại https://huggingface.co/datasets/nvidia/HelpSteer

1 Giới thiệu
Tính hữu ích và An toàn là hai mục tiêu chính để căn chỉnh các mô hình tổng quát theo hướng tuân thủ hướng dẫn của người dùng (Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023). Trong khi các nghiên cứu khác nhau (Ganguli et al., 2022; Ji et al., 2023; Rebedea et al., 2023) đã định nghĩa kỹ lưỡng các khía cạnh của an toàn, tiêu chí về những gì tạo nên một mô hình hữu ích vẫn còn mơ hồ. Xu hướng gần đây đã gợi ý một sự thay đổi từ việc đặt câu hỏi này hoàn toàn và dừng lại ở quan niệm rằng phản hồi của mô hình nên phù hợp với sở thích của người dùng hoặc người chú thích. Mặc dù việc sử dụng sở thích hộp đen của người dùng/người chú thích có thể hướng dẫn chúng ta về phía các phản hồi mô hình hữu ích hơn, cách tiếp cận này vừa không thỏa mãn về mặt trí tuệ vừa không hiệu quả về mặt tính toán và lượng dữ liệu cần thiết (Touvron et al., 2023; Bai et al., 2022). Việc sử dụng cách tiếp cận hộp đen cũng dẫn đến khả năng

Hình 1: HELPSTEER có thể cung cấp thông tin tính hữu ích đa thuộc tính để mô hình hóa phản hồi của con người, giảm khả năng các mô hình học các hiện tượng tập dữ liệu như ưu tiên phản hồi đơn giản do độ dài của chúng.

các mô hình liên kết phản hồi dài hơn với tính hữu ích, do đó ưu tiên độ dài hơn nội dung (Dong et al., 2023; Dubois et al., 2023; Singhal et al., 2023). Điều này đặt ra rủi ro đánh giá cao các phản hồi dài dòng nhưng không liên quan, làm nổi bật nhu cầu có sự hiểu biết tinh tế hơn về 'tính hữu ích' trong huấn luyện mô hình.

Huấn luyện các mô hình ngôn ngữ tổng quát hiệu quả hơn đòi hỏi phải làm rõ những gì con người thấy hữu ích trong phản hồi. Köpf et al. (2023) sử dụng các khía cạnh về sáng tạo và hài hước như một thước đo tính hữu ích của các mô hình tuân thủ hướng dẫn. Mặc dù những thuộc tính này có thể nâng cao tính hữu dụng của phản hồi trong các thiết lập cụ thể (ví dụ: viết truyện), chúng không góp phần vào tính hữu ích trong các tình huống khác (ví dụ: phân loại văn bản) và thậm chí có thể phản tác dụng trong môi trường kinh doanh chính thức. Những hiểu biết từ các mô hình ngôn ngữ chuyên biệt được huấn luyện cho các nhiệm vụ cụ thể cung cấp hướng dẫn có giá trị về những gì tạo nên tính hữu ích. Trong các nhiệm vụ tóm tắt, StiennonarXiv:2311.09528v1 [cs.CL] 16 Nov 2023

--- TRANG 2 ---
et al. (2020) xác định độ chính xác, phạm vi bao phủ và tính mạch lạc như những thành phần quan trọng của chất lượng tổng thể của các bản tóm tắt. Tương tự, Wu et al. (2023) nhấn mạnh tầm quan trọng của sự liên quan, tính thực tế và tính đầy đủ thông tin trong nhiệm vụ trả lời câu hỏi dài. Những phát hiện này gợi ý rằng các yếu tố góp phần vào tính hữu ích trong các mô hình ngôn ngữ có thể khác nhau đáng kể giữa các ứng dụng và bối cảnh khác nhau.

Chúng tôi đề xuất rằng tính hữu ích của phản hồi mô hình tổng quát có thể được đánh giá bởi tính đúng đắn, tính mạch lạc, độ phức tạp và tính dài dòng của chúng. Tính đúng đắn đề cập đến việc bao gồm tất cả các sự kiện có liên quan mà không có lỗi. Tính mạch lạc đề cập đến tính nhất quán và rõ ràng trong biểu đạt. Chúng tôi tin rằng cả hai khía cạnh này đều quan trọng để người dùng tin tưởng phản hồi của mô hình trong tất cả các nhiệm vụ. Độ phức tạp thể hiện chiều sâu trí tuệ của phản hồi, phản ánh liệu nội dung có cơ bản hay đòi hỏi chuyên môn sâu sắc (tức là liệu phản hồi có thể được viết bởi bất kỳ ai có năng lực ngôn ngữ cơ bản hay đòi hỏi chuyên môn sâu về lĩnh vực để tác giả), và quan trọng vì mọi người có xu hướng tin tưởng các chuyên gia hơn. Tính dài dòng đề cập đến lượng chi tiết được bao gồm trong phản hồi, được phát hiện có tương quan tích cực với sở thích của con người (Dong et al., 2023; Dubois et al., 2023), có thể vì các phản hồi dài có khả năng chứa thông tin liên quan bổ sung. Mặc dù không đầy đủ, chúng tôi tin rằng tập hợp các yếu tố này cung cấp một khung nền tảng để tuyển chọn dữ liệu nhằm nâng cao tính hữu ích của các mô hình ngôn ngữ.

Để chứng minh đóng góp của những thuộc tính này, chúng tôi:
1. Tuyển chọn một tập dữ liệu tính hữu ích với 37k cuộc trò chuyện, với mỗi phản hồi được chú thích cho tính đúng đắn, tính mạch lạc, độ phức tạp và tính dài dòng ngoài tính hữu ích tổng thể.
2. Tận dụng tập dữ liệu tính hữu ích này để căn chỉnh một mô hình có điểm MT Bench cao nhất là 7.54 trong số các mô hình không cần dữ liệu huấn luyện từ các mô hình độc quyền mạnh mẽ (ví dụ: GPT4).
3. Phát hành công khai tập dữ liệu kết quả này với giấy phép CC-BY-4.0 để cho phép cộng đồng xây dựng trên các phát hiện của chúng tôi.

2 Các Công trình Liên quan
Tập dữ liệu Ưu tiên Tính hữu ích Một cái nhìn tổng quan về các tập dữ liệu ưu tiên tính hữu ích tổng quát mã nguồn mở có thể được tìm thấy trong Bảng 1. HH-RLHF (Bai et al., 2022) là một tập dữ liệu dựa trên xếp hạng phổ biến chứa các cặp phản hồi, trong đó một cái là phản hồi được ưu tiên và cái khác là phản hồi bị từ chối. Ngoài dữ liệu xếp hạng, Open Assistant (Köpf et al., 2023) cũng chứa các thuộc tính liên quan đến tính hữu ích được gắn nhãn cho mỗi phản hồi. Cụ thể, các người chú thích được yêu cầu đánh giá mỗi phản hồi về chất lượng, sáng tạo và hài hước của họ trên thang điểm likert 5 điểm, sau đó có thể hữu ích cho việc huấn luyện các mô hình STEERLM (Dong et al., 2023).

Các công trình đương đại (Sharma et al., 2023; Cui et al., 2023) cũng đã sử dụng GPT-4 để chú thích cho các khía cạnh khác nhau góp phần vào tính hữu ích như tính trung thực và khả năng tuân thủ hướng dẫn. Với việc thiếu rõ ràng về các thiên lệch và hạn chế của GPT-4 trong việc thực hiện các chú thích như vậy (Cui et al., 2023), chúng tôi thấy khó để tin tưởng các chú thích tự động như vậy, đặc biệt khi việc xác thực con người tiếp theo của các chú thích này không được thực hiện. Một rủi ro bổ sung nằm trong Điều khoản Sử dụng GPT-4 của OpenAI¹, cụ thể là
Phần 2 (c) Hạn chế. Bạn không được ... (iii) sử dụng đầu ra từ Dịch vụ để phát triển các mô hình cạnh tranh với OpenAI.

Điều này khiến việc huấn luyện mô hình sử dụng đầu ra GPT-4 có khả năng gây tranh cãi pháp lý và đặt ra câu hỏi liệu dữ liệu được chú thích bởi GPT-4 có thực sự có thể mã nguồn mở hay không.

Ngoài ra, có một số tập dữ liệu ưu tiên tính hữu ích cho các nhiệm vụ/lĩnh vực cụ thể. Chúng bao gồm tóm tắt (Stiennon et al., 2020), trả lời câu hỏi (Nakano et al., 2022; Wu et al., 2023), giải quyết vấn đề mã (Lambert et al., 2023) cũng như các cuộc trò chuyện Reddit (Ethayarajh et al., 2022; Wang và Torres, 2022). Với tính cụ thể về nhiệm vụ/lĩnh vực của các tập dữ liệu này, chúng không có khả năng cải thiện hiệu suất trên nhiều nhiệm vụ đa dạng, đó là mục đích của HELPSTEER.

3 Tập dữ liệu
Trong phần này, chúng tôi trình bày chi tiết về phương pháp thu thập dữ liệu của HELPSTEER, nêu rõ động lực cơ bản, chiến lược lựa chọn lời nhắc, quy trình tạo phản hồi và các biện pháp được thực hiện để đảm bảo chất lượng chú thích.

¹https://openai.com/policies/terms-of-use

--- TRANG 3 ---
Tên | Thuộc tính Liên quan đến Tính hữu ích | N conv. (k) | Độ dài Trung bình tính bằng ký tự (Std.)
 | | | Lời nhắc | Phản hồi
HELPSTEER | Tính hữu ích, Tính đúng đắn, Tính mạch lạc, Độ phức tạp, Tính dài dòng | 37.1 | 2491.8 (1701.7) | 497.3 (426.7)
Open Assistant | Chất lượng, Sáng tạo, Hài hước | 59.4 | 397.5 (620.8) | 396.2 (618.8)
HH RLHF | - | 337.7 | 794.4 (706.9) | 310.7 (311.4)

Bảng 1: Tổng quan về Tập dữ liệu Mô hình hóa Ưu tiên Tính hữu ích Mã nguồn mở

3.1 Thu thập dữ liệu
Động lực Chúng tôi thu thập HELPSTEER dựa trên những hạn chế mà chúng tôi gặp phải khi sử dụng tập dữ liệu Open Assistant với kỹ thuật STEERLM (Dong et al., 2023). Mặc dù các phản hồi chủ yếu là hữu ích, một số trường hợp tiết lộ các vấn đề như thiếu chính xác về mặt sự kiện, thiếu mạch lạc, đơn giản hóa quá mức, hoặc quá dài dòng. Ngoài ra, chúng tôi quan sát thấy hiệu suất không tối ưu trên một số nhiệm vụ như Viết lại, Tóm tắt, Phân loại, Trích xuất và Trả lời Câu hỏi Đóng. Những nhiệm vụ này thường liên quan đến một văn bản tham chiếu, có thể kém hấp dẫn đối với các tình nguyện viên trong dự án Open Assistant do độ dài và độ phức tạp gia tăng của các lời nhắc. Để có được các chú thích tốt hơn cho các thuộc tính góp phần vào tính hữu ích của phản hồi cho những nhiệm vụ như vậy, chúng tôi đã sử dụng một số lượng đáng kể lời nhắc từ các danh mục này và thu thập các chú thích cho tính đúng đắn, tính mạch lạc, độ phức tạp và tính dài dòng ngoài tính hữu ích tổng thể để nâng cao tính hữu dụng của tập dữ liệu.

Thu thập Lời nhắc Đầu tiên chúng tôi thu thập 10,459 lời nhắc một lượt. Khoảng một nửa trong số này được tạo bởi Scale AI, một nhà cung cấp chú thích dữ liệu bên ngoài, trong khi phần còn lại được tạo tổng hợp bằng cách sử dụng các mẫu để đảm bảo sự đa dạng trong các nguồn lời nhắc. Ban đầu, một tập hợp lời nhắc lớn hơn được sản xuất, từ đó chúng tôi lọc bỏ khoảng 20% được coi là không thỏa đáng. Theo Ouyang et al. (2022), bộ sưu tập của chúng tôi bao gồm các lời nhắc từ các nhiệm vụ Trả lời Câu hỏi Mở, Tạo sinh và Động não, cùng với năm nhiệm vụ (Viết lại, Tóm tắt, Phân loại, Trích xuất và Trả lời Câu hỏi Đóng) có đại diện hạn chế trong tập dữ liệu Open Assistant. Chúng tôi duy trì phân phối khoảng 10% cho mỗi danh mục, ngoại trừ năm nhiệm vụ mà STEERLM cho thấy hiệu suất giảm, mà chúng tôi phân bổ 20% còn lại.

Tạo Phản hồi Để tạo phản hồi, chúng tôi đã sử dụng mô hình 43 tỷ tham số nội bộ của chúng tôi, tạo ra bốn phản hồi khác biệt cho mỗi lời nhắc. Những phản hồi này được tạo trong độ dài ngữ cảnh tối đa 4,096 token. Cấu hình của chúng tôi bao gồm thiết lập nhiệt độ 1.0 và giá trị top_p 0.80, kết hợp với hình phạt lặp lại 1.0 và thiết lập top_k 1000 dẫn đến các phản hồi đa dạng nhưng hợp lý.

Chú thích Phản hồi Mỗi phản hồi trong tập dữ liệu của chúng tôi được đánh giá dựa trên năm thuộc tính: Tính hữu ích, Tính đúng đắn, Tính mạch lạc, Độ phức tạp và Tính dài dòng. Chúng được đánh giá trên thang điểm Likert-5, từ 0 đến 4. Không giống như các chú thích cho RLHF (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023) liên quan đến so sánh với các phản hồi khác cho cùng một lời nhắc, mỗi phản hồi được đánh giá độc lập với các phản hồi khác. Chúng tôi thấy cách tiếp cận này có khả năng mở rộng hơn so với các chú thích RLHF vì mô hình so sánh của nó dẫn đến tăng trưởng bậc hai trong khối lượng công việc chú thích so với số lượng phản hồi trên mỗi lời nhắc, trong khi cách tiếp cận của chúng tôi (HELPSTEER) tăng tuyến tính.

Đối với các chú thích HELPSTEER, chúng tôi đã tham gia một nhóm nhà thầu được lựa chọn thông qua Scale AI. Những nhà thầu này được cung cấp các hướng dẫn toàn diện xác định từng thuộc tính và tiêu chí cho mỗi mức độ xếp hạng, cùng với một số ví dụ được chú thích. Những hướng dẫn và ví dụ này được chi tiết trong Phụ lục A.1.

Quy trình chú thích liên quan đến khoảng 200 người chú thích con người có trụ sở tại Hoa Kỳ. Các ứng cử viên đầu tiên trải qua các nhiệm vụ sơ bộ, bao gồm đánh giá trình độ tiếng Anh, để xác định tính đủ điều kiện làm việc trên dự án HELPSTEER. Sau đó, họ tham gia vào một khóa đào tạo giới thiệu về nhiệm vụ kết thúc bằng một bài kiểm tra bao gồm việc chú thích 35 phản hồi mẫu. Quy trình này đảm bảo không chỉ hiểu biết kỹ lưỡng về các yêu cầu nhiệm vụ mà còn cung cấp các chú thích chất lượng cao.

Sau chú thích, Scale AI đã thực hiện đảm bảo chất lượng mở rộng, với mỗi chú thích đạt tối thiểu hai đánh giá của con người ngoài các kiểm tra tự động. Sau khi nhận được các chú thích từ Scale AI, chúng tôi đã tiến hành đảm bảo chất lượng độc lập để đảm bảo rằng chất lượng của

--- TRANG 4 ---
các chú thích đạt đến kỳ vọng của chúng tôi. Sau khi lọc bỏ các chú thích không đáp ứng tiêu chí của chúng tôi ở các giai đoạn khác nhau, chúng tôi đã hoàn thiện một tập dữ liệu bao gồm 37,120 mẫu chú thích chất lượng cao.

3.2 Phân tích Dữ liệu
Thuộc tính | Pearson R | Trung bình | Std.
với tính hữu ích / chất lượng

HELPSTEER
tính hữu ích | 1 | 2.7856 | 0.9793
tính đúng đắn | 0.8525 | 2.8369 | 0.9935
tính mạch lạc | 0.6348 | 3.2991 | 0.7699
độ phức tạp | 0.2361 | 1.4423 | 0.8205
tính dài dòng | 0.2555 | 1.5331 | 0.9287

Open Assistant
chất lượng | 1 | 2.5735 | 0.9878
sáng tạo | 0.3428 | 1.5764 | 1.0618
hài hước | -0.0992 | 0.7218 | 0.8507

Bảng 2: Thống kê mô tả cho các thuộc tính liên quan đến tính hữu ích trong HELPSTEER và Open Assistant. Trong Open Assistant, thuộc tính Chất lượng giống nhất với định nghĩa của chúng tôi về tính hữu ích. Điểm số cho mỗi thuộc tính nằm giữa 0 và 4 trên thang điểm Likert-5.

Với 37.1k cuộc trò chuyện, HELPSTEER có kích thước tương đương với 59.4k cuộc trò chuyện được tìm thấy trong tập dữ liệu Open Assistant (Köpf et al., 2023) được chú thích với các thuộc tính góp phần vào tính hữu ích. Các lời nhắc được tìm thấy trong HELPSTEER có độ dài trung bình 2491.8 ký tự (std. = 1701.7) trong khi phản hồi có độ dài trung bình 497.3 ký tự (std. = 426.7). Điều này có nghĩa là các lời nhắc HELPSTEER dài hơn đáng kể so với những lời nhắc trong Open Assistant (397.5 ký tự với std. = 620.8) và điều này chủ yếu có thể được quy cho việc bao gồm các nhiệm vụ như Tóm tắt, Viết lại và Trích xuất, kết hợp các văn bản tham chiếu trong các lời nhắc.

Bảng 2 cung cấp phân tích chi tiết về phân phối của từng thuộc tính và tương quan Pearson của chúng với tính hữu ích trong HELPSTEER, và chất lượng trong Open Assistant. Trong HELPSTEER, tính đúng đắn và tính mạch lạc thể hiện tương quan tích cực mạnh với tính hữu ích (Pearson's R > 0.6) trong khi độ phức tạp và tính dài dòng có tương quan yếu (Pearson's R > 0.2). Điều này gợi ý rằng tính đúng đắn và tính mạch lạc của phản hồi mô hình có ảnh hưởng đáng kể đến tính hữu ích được cảm nhận, trong khi tính dài dòng và độ phức tạp ít then chốt hơn.

Các thuộc tính trong Open Assistant có tương quan yếu (Sáng tạo với Pearson's R = 0.34) hoặc tương quan âm nhẹ (Hài hước với Pearson's R = −0.099). Điều này phù hợp với giả thuyết của chúng tôi rằng sáng tạo và hài hước, mặc dù có thể nâng cao, không cần thiết cho tính hữu ích và đôi khi có thể làm giảm nó.

Phân phối thuộc tính của HELPSTEER chỉ ra rằng các phản hồi thường thể hiện tính mạch lạc cao (trung bình 3.30 trên 4), tính đúng đắn trung bình (trung bình 2.84), và tương đối thấp về độ phức tạp (trung bình 1.44) và tính dài dòng (trung bình 1.53) cùng nhau dẫn đến các phản hồi hữu ích ở mức trung bình (2.78). Các phản hồi trong Open Assistant tương tự về chất lượng tổng thể (2.57), nhưng chúng thấp về cả sáng tạo (1.58) và hài hước (0.72), gợi ý rằng những thuộc tính này không đủ để giải thích tính hữu ích tổng thể như được hỗ trợ bởi điểm tương quan Pearson thấp/âm của chúng (Pearson's R < 0.35) với tính hữu ích.

Một phân tích Hồi quy Bình phương Tối thiểu Thông thường, với tính hữu ích như biến phụ thuộc và bốn thuộc tính như các biến độc lập trong HELPSTEER, tiết lộ những đóng góp đáng kể của từng thuộc tính đối với tính hữu ích tổng thể (p < 0.05). Cộng gộp, những thuộc tính này chiếm 73.0% đáng kể của phương sai trong tính hữu ích, cung cấp sự hiểu biết toàn diện về các yếu tố thúc đẩy tính hữu ích trong phản hồi mô hình.

4 Thí nghiệm
Trong phần này, chúng tôi chi tiết phương pháp của chúng tôi trong việc đánh giá liệu HELPSTEER có thể hướng dẫn các mô hình hiệu quả hướng tới tính hữu ích được cải thiện, độ chính xác thực tế, tính mạch lạc và mức độ phù hợp của độ phức tạp và tính dài dòng. Chúng tôi sử dụng sự kết hợp giữa đánh giá tự động và đánh giá của con người để đo lường những khía cạnh này. Ngoài ra, chúng tôi mô tả quy trình huấn luyện mô hình STEERLM (Dong et al., 2023) sử dụng tập dữ liệu HELPSTEER. Phần này cũng bao gồm các mô hình cơ sở mà chúng tôi so sánh hiệu suất của mô hình STEERLM, cung cấp cái nhìn toàn diện về hiệu quả của nó trong căn chỉnh mô hình.

4.1 Đánh giá Tự động
Tính hữu ích Chúng tôi theo Jiang et al. (2023); Lu et al. (2023) để sử dụng MT Bench (Zheng et al., 2023) cho đánh giá tính hữu ích. MT Bench bao gồm

--- TRANG 5 ---
80 câu hỏi đa lượt, mỗi câu bao gồm một câu hỏi ban đầu và một câu hỏi tiếp theo, tổng cộng 160 lời nhắc. Những câu hỏi này xuất phát từ 8 danh mục bao gồm Viết, Nhập vai, Trích xuất, Suy luận, Toán học, Lập trình, STEM và Nhân văn/Khoa học Xã hội. Kết quả là, MT Bench có thể được sử dụng để đánh giá tính hữu ích trong nhiều thiết lập đa dạng. Đầu tiên chúng tôi tạo các phản hồi một cách tham lam với tối đa 1024 token (giá trị mặc định cho MT Bench). Các phản hồi cho những lời nhắc này được đánh giá bởi GPT-4 để cho điểm từ 1 đến 10, và chúng tôi báo cáo trung bình trên tất cả lời nhắc với điểm MT Bench cao hơn cho thấy tính hữu ích lớn hơn.

Tính đúng đắn Chúng tôi theo Ouyang et al. (2022); Bai et al. (2022); Touvron et al. (2023) trong việc sử dụng TruthfulQA (Lin et al., 2022) để đánh giá tính thực tế của các mô hình. TruthfulQA bao gồm 817 câu hỏi trên 38 danh mục (ví dụ: sức khỏe, tài chính và pháp lý). Chúng tôi sử dụng TruthfulQA MC2 như được sử dụng trong Bảng xếp hạng OpenLLM của Huggingface², đại diện cho tổng xác suất được chuẩn hóa được gán cho tập hợp một hoặc nhiều câu trả lời đúng trong số 4 đến 5 lựa chọn câu trả lời cho mỗi câu hỏi. TruthfulQA MC2 cao hơn cho thấy rằng các phản hồi chính xác hơn về mặt thực tế.

Tính mạch lạc Chúng tôi theo Laban et al. (2021) trong việc sử dụng Độ phức tạp Mô hình Ngôn ngữ Cơ sở như một thước đo tính mạch lạc văn bản. Đối với thước đo này, chúng tôi tính toán độ phức tạp của các phản hồi MT Bench sử dụng mã đánh giá từ các tác giả gốc³. Trong khi mô hình hoạt động tốt nhất không được tinh chỉnh trong Laban et al. (2021) là mô hình GPT2-medium, chúng tôi thấy nó không phù hợp cho trường hợp sử dụng của chúng tôi vì độ phức tạp hơn 100 cho nhiều phản hồi gợi ý rằng GPT2-medium bị thiếu huấn luyện. Thay vào đó, chúng tôi sử dụng độ phức tạp mô hình Llama 2 13B Foundation, mà chúng tôi chỉ ra có độ chính xác cao hơn trên Zero-Shot Shuffle Test⁴, như được sử dụng trong Laban et al. (2021). Độ phức tạp thấp hơn ngụ ý rằng các phản hồi mạch lạc hơn.

Độ phức tạp Chúng tôi theo Scarton và Specia (2018) và Scialom et al. (2021) để sử dụng Flesch-Kincaid Grade Level (FGKL) như một thước đo cho độ phức tạp văn bản (Kincaid et al., 1975). FKGL đại diện cho mức độ học vấn của Hoa Kỳ (tức là 1 đến 12 trong đó 12

²https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
³https://github.com/tushar117/Transformer-Models-for-Text-Coherence-Assessment
⁴Đối với Llama 2 13B vs. GPT2-medium - Pháp lý: 99.7 vs 98.6; Reddit 98.5 vs 88.9; kho văn bản WSJ không thể truy cập được sau paywall với tham chiếu đến Bảng 1 của Laban et al. (2021)

là năm cuối của giáo dục tiền đại học) mà văn bản hướng tới. Chúng tôi tính toán FKGL dựa trên các phản hồi MT Bench sử dụng gói Easse (Alva-Manchego et al., 2019). FKGL cao hơn có nghĩa là độ phức tạp văn bản cao hơn.

Tính dài dòng Chúng tôi sử dụng số lượng ký tự trung bình trong các phản hồi MT Bench như một thước đo cho tính dài dòng.

4.2 Đánh giá Con người
Theo Dong et al. (2023), chúng tôi tiến hành đánh giá con người để đánh giá tính hữu ích tương đối của các phản hồi mô hình để bổ sung cho đánh giá tự động.

Dữ liệu Chúng tôi chọn các lời nhắc lượt đầu tiên từ tập dữ liệu MT Bench, bao gồm 80 câu hỏi mở về các chủ đề đa dạng bao gồm toán học, khoa học, lập trình, nhập vai, suy luận, v.v.

Quy trình Chú thích Chúng tôi tuyển dụng 12 tình nguyện viên có ít nhất trình độ đại học về khoa học máy tính hoặc các lĩnh vực liên quan để đánh giá chất lượng phản hồi mô hình trong môi trường mù. Những người chú thích được trình bày lời nhắc và 3 phản hồi mô hình theo thứ tự ngẫu nhiên, và được yêu cầu xếp hạng các phản hồi theo thứ tự tính hữu ích. Để giảm mệt mỏi người chú thích, các lời nhắc được chia thành 4 tập 20 lời nhắc, và mỗi người chú thích được gán một tập. Điều này có nghĩa là các lời nhắc cho mỗi phản hồi được xếp hạng độc lập bởi 3 người chú thích (Fleiss' κ=0.383).

Thước đo Chúng tôi hiển thị tỷ lệ thắng của các mô hình so với nhau dựa trên sở thích phản hồi mô hình theo cặp từ đánh giá con người. Ngoài ra, chúng tôi tính toán điểm Elo theo Chiang et al. (2023) để minh họa tốt hơn cách các mô hình so sánh với nhau. Chúng tôi bắt đầu với điểm ban đầu 1000 và K = 32 và lặp lại quy trình 10000 lần để tính đến hiệu ứng thứ tự trong việc tính toán điểm Elo.

4.3 Mô hình cơ sở
Chúng tôi sử dụng các mô hình Llama 2 Foundation (Touvron et al., 2023) cho tất cả các thí nghiệm của chúng tôi - biến thể 70B cho mô hình ngôn ngữ chính và biến thể 13B như Mô hình Dự đoán Thuộc tính và Mô hình Thưởng trong đường cơ sở STEERLM và RLHF tương ứng. Các khám phá ban đầu của chúng tôi cho thấy lợi ích ít khi sử dụng mô hình lớn hơn (tức là mô hình 70B) và tăng lớn về yêu cầu tính toán.

--- TRANG 6 ---
4.4 STEERLM
Chúng tôi huấn luyện mô hình Llama 2 13/70B theo cách tiếp cận STEERLM (Dong et al., 2023). STEERLM là một phương pháp căn chỉnh mô hình (thay thế cho RLHF) với bốn bước chính. Đầu tiên, một Mô hình Dự đoán Thuộc tính được huấn luyện để dự đoán điểm cho nhiều thuộc tính ngữ nghĩa nắm bắt các chiều của tính hữu ích phản hồi như tính đúng đắn và tính mạch lạc. Tiếp theo, các tập dữ liệu bao gồm các cặp lời nhắc-phản hồi được chú thích với những thuộc tính này sử dụng Mô hình Dự đoán Thuộc tính. Sau đó, Tinh chỉnh Có giám sát Có điều kiện Thuộc tính (AC-SFT) được thực hiện bằng cách huấn luyện mô hình nền tảng trên các tập dữ liệu được chú thích để tạo các phản hồi có điều kiện trên các giá trị thuộc tính được chỉ định. Cuối cùng, mô hình AC-SFT có thể được cải thiện thêm bằng cách bootstrapping thêm dữ liệu huấn luyện thông qua lấy mẫu mô hình để có được các phản hồi đa dạng, chất lượng cao cho việc huấn luyện bổ sung.

Các sửa đổi Trái ngược với Dong et al. (2023), các thí nghiệm của chúng tôi chỉ sử dụng tập dữ liệu Open Assistant (OASST) cho huấn luyện AC-SFT thay vì nhiều tập dữ liệu. Chúng tôi cũng chia tỷ lệ các nhãn thuộc tính từ OASST về phạm vi 0-4 để phù hợp với các chú thích của tập dữ liệu HELPSTEER. Để huấn luyện Mô hình Dự đoán Thuộc tính, chúng tôi kết hợp các tập dữ liệu OASST và HELPSTEER để dự đoán tổng cộng 9 nhãn. Chúng tôi chọn sử dụng các nhãn Chất lượng, Hài hước, Độc tính và Sáng tạo từ tập dữ liệu OASST ngoài 5 nhãn từ dữ liệu HELPSTEER. Sau khám phá ban đầu, chúng tôi chọn loại trừ bước bootstrapping của việc lấy mẫu mô hình AC-SFT và huấn luyện lại trên các thế hệ của nó, vì điều này mang lại lợi ích tối thiểu. Cuối cùng, thay vì mô hình dự đoán thuộc tính dựa trên mô hình ngôn ngữ trong Dong et al. (2023), chúng tôi sử dụng mô hình hồi quy, mà chúng tôi thấy hoạt động tốt hơn. Chúng tôi thực hiện điều này bằng cách lấy trạng thái ẩn cuối cùng từ mô hình nền tảng Llama 2 và thêm đầu hồi quy lên trên nó cho mỗi thuộc tính. Với những sửa đổi này, chúng tôi hợp lý hóa pipeline STEERLM trong khi duy trì hiệu quả của nó trong việc kết hợp các tín hiệu ngữ nghĩa phong phú để căn chỉnh các mô hình nền tảng.

Siêu tham số Chúng tôi huấn luyện cả mô hình Dự đoán Thuộc tính và mô hình SFT Có điều kiện Thuộc tính trong 800 bước sử dụng kích thước batch toàn cục 128 (gần 2 epoch) và tốc độ học không đổi 5e-6 với bộ tối ưu hóa AdamW (Loshchilov và Hutter, 2017). Trừ khi nói khác (ví dụ: để hiển thị khả năng định hướng), chúng tôi đặt tất cả thuộc tính thành 4 tại thời điểm suy luận, ngoại trừ sáng tạo, hài hước và độc tính được đặt thành 0.

4.5 Mô hình Cơ sở
SFT Chúng tôi huấn luyện một mô hình chỉ sử dụng các lời nhắc và phản hồi Open Assistant, giống hệt với STEERLM trừ các nhãn thuộc tính mà chúng tôi sử dụng để điều kiện SFT. Chúng tôi huấn luyện mô hình trong 800 bước với kích thước batch toàn cục 128 (gần 2 epoch) và tốc độ học không đổi 5e-6 sử dụng bộ tối ưu hóa AdamW, phù hợp với huấn luyện STEERLM.

RLHF trên Tập dữ liệu Mã nguồn mở Bắt đầu từ mô hình SFT trên, chúng tôi tiến hành RLHF trên HH-RLHF (Bai et al., 2022). Chúng tôi huấn luyện mô hình thưởng trong một epoch, và chọn checkpoint có tổn thất xác thực thấp nhất. Sau đó chúng tôi tối ưu hóa mạng chính sách trên cùng dữ liệu sử dụng thuật toán PPO (Schulman et al., 2017). Theo Ouyang et al. (2022); Touvron et al. (2023), chúng tôi áp dụng kích thước batch toàn cục 512 và kích thước mini-batch 64 cho mỗi lần lặp PPO. Chúng tôi đặt ngưỡng clip PPO thành 0.2, β(hình phạt KL) thành 0.005, và nhiệt độ lấy mẫu thành 1 cho rollout. Chúng tôi sử dụng bộ tối ưu hóa AdamW và áp dụng tốc độ học không đổi 9e-7 với warmup trong 10 lần lặp đầu tiên. Chúng tôi huấn luyện mô hình trong 800 bước sử dụng đánh giá trên các lời nhắc xác thực giữ lại để lựa chọn checkpoint.

DPO trên Tập dữ liệu Mã nguồn mở Chúng tôi thực hiện Tối ưu hóa Ưu tiên Trực tiếp, một thay thế hiệu quả cho RLHF, theo phương pháp của Rafailov et al. (2023). Khởi tạo với mô hình SFT trên, chúng tôi huấn luyện hai mô hình với DPO, một sử dụng tập dữ liệu HH-RLHF, và một khác sử dụng tập dữ liệu Open Assistant. Chúng tôi huấn luyện mỗi mô hình trong 1 epoch của tập dữ liệu tương ứng, với hình phạt KL 0.2 và kích thước batch toàn cục 512. Chúng tôi sử dụng bộ tối ưu hóa AdamW với tốc độ học không đổi 9e-6, weight decay 0.1, beta (0.9, 0.98) với 10 bước warmup.

Llama 2 70B Chat là một mô hình RLHF phổ biến (Touvron et al., 2023), sử dụng cùng mô hình nền tảng như STEERLM nhưng được huấn luyện với dữ liệu nguồn đóng. Nó được huấn luyện sử dụng 27,540 mẫu SFT riêng tư và sau đó được huấn luyện với RLHF trên 1.4 triệu cặp mẫu so sánh riêng tư. Chúng tôi sử dụng điểm MT Bench từ Bảng xếp hạng Chatbot Arena⁵ và

⁵https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard

--- TRANG 7 ---
điểm Truthful MC2 từ Bảng xếp hạng Open LLM⁶. Các thước đo khác được tính toán dựa trên các phản hồi mô hình trong không gian MT Bench Huggingface⁷.

5 Kết quả
Mô hình | MTBench ↑ | TruthfulQA ↑ | PPL↓ | FGKL↑ | Chars.
STEERLM | 7.54 | 0.5613 | 2.876 | 8.658 | 1192.7
SFT | 6.29 | 0.4930 | 8.199 | 7.852 | 604.2
Llama 2 Chat | 6.86 | 0.5280 | 4.377 | 7.496 | 1350.6
RLHF w. HH-RLHF | 7.21 | 0.5042 | 3.438 | 7.418 | 831.6
DPO w. HH-RLHF | 6.94 | 0.5021 | 8.102 | 7.977 | 787.7
DPO w. OASST | 6.98 | 0.5022 | 7.028 | 7.323 | 834.9

Bảng 3: Đánh giá tự động của STEERLM so với các mô hình cơ sở được huấn luyện với dữ liệu nguồn mở và Llama 2 Chat. Cao hơn là tốt hơn cho MT Bench, TruthfulQA và FKGL, và thấp hơn là tốt hơn cho PPL.

Bảng 3 chứng minh rằng khi tận dụng HELPSTEER, STEERLM tạo ra các phản hồi hữu ích, chính xác và mạch lạc nhất so với các mô hình cơ sở có thể được huấn luyện với các tập dữ liệu nguồn mở cũng như mô hình Llama 2 70B Chat. Trên MT Bench (một thước đo cho tính hữu ích), STEERLM đạt điểm cao nhất 7.54. Các mô hình cơ sở được huấn luyện với RLHF hoặc DPO trên các tập dữ liệu nguồn mở (Open Assistant hoặc HH-RLHF) đạt tối đa 7.21. Điều này đặc biệt có ý nghĩa vì mô hình hoạt động tốt nhất (RLHF w. HH-RLHF) yêu cầu gấp 5 lần tính toán so với STEERLM (xem Phụ lục A.2), trong khi các lựa chọn thay thế yêu cầu tính toán tương tự với STEERLM tụt lại xa hơn (MT Bench ≤ 6.98).

Mô hình | Tỷ lệ Thắng (%) vs. | Điểm Elo
 | STEERLM | Llama 2 Chat | RLHF w. HH-RLHF | 
STEERLM | - | 57.5 | 62.9 | 1050
Llama 2 Chat | 42.5 | - | 49.2 | 979
RLHF w. HH-RLHF | 37.1 | 50.8 | - | 971

Bảng 4: Đánh giá Con người. Cao hơn là tốt hơn cho Tỷ lệ Thắng và Điểm Elo.

Chúng tôi cũng tiến hành đánh giá con người để bổ sung cho đánh giá tự động trong việc hiểu tính hữu ích tương đối của các phản hồi mô hình. Với các hạn chế về tài nguyên, chúng tôi chỉ có thể tiến hành đánh giá con người trên ba mô hình - STEERLM, Llama 2 Chat và RLHF w. HH-RLHF. Như được thấy

⁶https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
⁷https://huggingface.co/spaces/lmsys/mt-bench/blob/main/data/mt_bench/model_answer/Llama-2-70b-chat.jsonl

trong Bảng 4, STEERLM đạt được xếp hạng Elo cao nhất 1050 dựa trên so sánh mô hình theo cặp, với 57.5% tỷ lệ thắng so với Llama 2 Chat và 62.9% so với đường cơ sở RLHF w. HH-RLHF của chúng tôi.

Khi chúng tôi phân tích hiệu suất MT Bench theo danh mục (Hình 2), mô hình STEERLM đạt được lợi ích lớn so với Llama 2 70B Chat trong các danh mục như "Trích xuất", "Lập trình", "Toán học", "Suy luận" và "Nhập vai" mà mô hình cần tuân thủ hướng dẫn chính xác và tạo ra các câu trả lời đúng. Lợi ích lớn có thể được giải thích một phần bởi hiệu suất cao của STEERLM trên TruthfulQA (0.5613), tính mạch lạc phản hồi tốt (2.876 PPL.) và độ phức tạp lớn hơn của phản hồi (FKGL của 8.658). Đồng thời, STEERLM tạo ra các phản hồi với chi tiết đầy đủ (độ dài trung bình 1192.7 ký tự) trái ngược với tính ngắn gọn của các mô hình SFT/RLHF/DPO được huấn luyện trên dữ liệu nguồn mở và Llama2 Chat rất dài dòng. Điều này chứng minh sự thành công của việc sử dụng các thuộc tính như tính đúng đắn, tính mạch lạc, độ phức tạp và tính dài dòng để điều kiện tạo sinh.

Hình 2: STEERLM hoạt động tốt hơn các mô hình Llama2 Chat và RLHF w. HH-RLHF trong hầu hết các danh mục MT Bench.

6 Nghiên cứu Ablation
Để hiểu rõ hơn về đóng góp của mỗi thuộc tính HELPSTEER, chúng tôi tiến hành các nghiên cứu ablation trong đó chúng tôi loại trừ một hoặc nhiều thuộc tính khi huấn luyện mô hình Attribute-Condition SFT. Như được thấy trong Bảng 5, tất cả năm thuộc tính trong HELPSTEER đều góp phần cải thiện MT Bench, vì MT Bench bị giảm khi bất kỳ thuộc tính nào trong số chúng bị loại bỏ. Hơn nữa, việc loại bỏ một thuộc tính cũng làm giảm hiệu suất trên các thước đo đo lường thuộc tính đó, gợi ý rằng STEERLM có thể sử dụng hiệu quả từng thuộc tính để điều kiện tạo sinh.

--- TRANG 8 ---
Mô hình | MTBench ↑ | TruthfulQA ↑ | PPL↓ | FGKL↑ | Chars.
STEERLM | 7.54 | 0.5613 | 2.876 | 8.658 | 1192.7
- tính hữu ích | 7.17 | 0.5754 | 3.066 | 8.571 | 1085.6
- tính đúng đắn | 6.92 | 0.5474 | 3.014 | 8.991 | 1175.4
- tính mạch lạc | 7.13 | 0.5381 | 2.973 | 8.265 | 1170
- độ phức tạp | 7.12 | 0.5374 | 2.872 | 8.019 | 1143.1
- tính dài dòng | 7.07 | 0.5217 | 3.718 | 8.333 | 1021.6
- HELPSTEER | 6.9 | 0.5393 | 6.138 | 7.945 | 825.4
- OASST | 7.36 | 0.5557 | 3.244 | 8.483 | 1022.4
- HELPSTEER - OASST (=SFT) | 6.29 | 0.493 | 8.199 | 7.852 | 604.2

Bảng 5: Nghiên cứu ablation với các thước đo đánh giá tự động. Mỗi hàng đại diện cho hiệu suất của STEERLM khi các thuộc tính liên quan bị loại trừ khi huấn luyện mô hình Attribute-Condition SFT.

Trong số các thuộc tính khác nhau, các ablation cho tính hữu ích và tính đúng đắn nổi bật. Sự tăng TruthfulQA MC2 khi thuộc tính tính hữu ích bị loại bỏ (0.5613 → 0.5754) cho thấy rằng tính đúng đắn có thể được cải thiện thêm khi không tối ưu hóa rõ ràng cho tính hữu ích. Một hệ quả của ablation này là tối ưu hóa các mô hình ngôn ngữ chỉ cho tính hữu ích, như được thực hiện trong RLHF chính thống (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023), có thể dẫn đến các mô hình không tối ưu về mặt tính đúng đắn, như cũng được quan sát bởi công trình về tối ưu hóa quá mức mô hình thưởng (Lambert và Calandra, 2023).

Ablation của tính đúng đắn cho thấy tầm quan trọng của tính thực tế của mô hình đối với tính hữu ích tổng thể của nó. Khi một mô hình không được huấn luyện rõ ràng để trung thực, nó có thể làm tổn hại đáng kể đến tính hữu ích tổng thể, như được thể hiện trong sự giảm lớn trong MT Bench (7.54 → 6.92). Mặt khác, một mô hình như vậy có thể tạo ra các phản hồi có vẻ phức tạp hơn, giống như tài liệu cho độ tuổi đọc cao hơn (FKGL). Khả năng các mô hình ngôn ngữ được tối ưu hóa để tạo ra các phản hồi có vẻ phức tạp nhưng không được căn cứ tốt vào sự thật thế giới nhắc lại tầm quan trọng của việc huấn luyện các mô hình một cách rõ ràng để thực tế ngoài việc hữu ích.

Cuối cùng, việc so sánh ablation của các thuộc tính HELPSTEER với các thuộc tính Open Assistant gợi ý rằng các thuộc tính HELPSTEER đóng góp nhiều hơn vào tính hữu ích của mô hình vì sự giảm MT Bench của nó là đáng kể hơn (7.54 → 6.9 vs. 7.54 → 7.36). Sử dụng cả hai tập thuộc tính cùng nhau bổ sung cho nhau, dẫn đến mô hình hữu ích nhất (MT Bench 7.54) trong khi huấn luyện với một trong hai tập thuộc tính riêng lẻ vẫn có thể vượt trội hơn nhiều so với mô hình SFT vanilla (MT Bench 6.29).

7 Demo Khả năng Định hướng
Một lợi thế khác mà STEERLM cung cấp là khả năng tùy chỉnh phản hồi tại thời điểm suy luận, cung cấp cho người dùng cuối khả năng thích ứng với các tình huống khác nhau. Chúng tôi trình bày một phân tích về việc định hướng các thuộc tính độ phức tạp và tính dài dòng, có thể quan trọng cho các nhu cầu người dùng khác nhau. Trong môi trường giáo dục, việc trình bày thông tin với độ phức tạp thấp hơn hỗ trợ hiểu biết cho người học mới bắt đầu. Ngược lại, đối với các chuyên gia, độ phức tạp cao hơn có thể nâng cao hiểu biết nâng cao của họ. Khi chúng tôi tăng thuộc tính độ phức tạp, mức độ đọc (FKGL) của phản hồi tăng dần từ 8.489 đến 9.104. Ví dụ, khi được nhắc với một câu hỏi, Trọng lực là gì?

Một phản hồi độ phức tạp thấp trông giống như
Trọng lực là lực kéo mọi thứ về phía nhau. Đó là thứ giữ các hành tinh trong quỹ đạo quanh mặt trời, và gây ra thủy triều trong đại dương lên xuống...

trong khi một phản hồi độ phức tạp cao trông giống như,
Trọng lực là một lực tồn tại giữa bất kỳ hai vật thể nào có khối lượng hoặc năng lượng, làm chúng hút nhau. Trọng lực là kết quả của độ cong của không-thời gian do sự hiện diện của khối lượng hoặc năng lượng...

Trong môi trường kinh doanh, các bên liên quan khác nhau (ví dụ: giám đốc điều hành, quản lý và kỹ sư) có thể cần câu trả lời cho các câu hỏi tương tự ở các mức độ chi tiết khác nhau. Khi chúng tôi tăng tính dài dòng, độ dài phản hồi trung bình tăng từ 422.3 đến 1125.2 ký tự.

8 Kết luận
Chúng tôi tạo ra HELPSTEER, một tập dữ liệu ưu tiên tính hữu ích nhiều thuộc tính cho STEERLM với 37k mẫu, bao gồm các thuộc tính về tính đúng đắn, tính mạch lạc, độ phức tạp và tính dài dòng ngoài tính hữu ích tổng thể. Chúng tôi phát hành công khai tập dữ liệu này với giấy phép CC-BY-4.0. Huấn luyện mô hình Llama 2 70B trên HELPSTEER sử dụng kỹ thuật STEERLM tạo ra một mô hình đạt điểm 7.54 trên MT Bench, cao nhất trong số các mô hình được huấn luyện với các tập dữ liệu nguồn mở. Ngoài tính hữu ích, mô hình này cũng trung thực và mạch lạc hơn so với Llama 2 70B Chat và các đường cơ sở khác trong khi có thể định hướng người dùng về tính dài dòng và độ phức tạp.

--- TRANG 9 ---
Hạn chế
HELPSTEER chỉ chứa các chú thích cho các lời nhắc và phản hồi bằng tiếng Anh. Mặc dù chúng tôi không đánh giá STEERLM trên các benchmark đa ngôn ngữ, nhưng không có khả năng cải thiện hiệu suất của các mô hình trên các lời nhắc không phải tiếng Anh. Tuy nhiên, phương pháp thu thập dữ liệu của chúng tôi có thể được áp dụng để thu thập các chú thích cho một tập dữ liệu tương tự bằng các ngôn ngữ khác.

Các chú thích HELPSTEER cũng có khả năng phản ánh những gì được hiểu là hữu ích tại Hoa Kỳ vì tất cả các người chú thích đều có trụ sở tại Hoa Kỳ. Với hiểu biết rằng tính hữu ích trong các phản hồi có thể cụ thể theo văn hóa, chúng tôi không chắc chắn rằng tập dữ liệu này sẽ phản ánh ý kiến của những người có trụ sở tại các quốc gia khác. Đối với những tình huống này, chúng tôi tin rằng phương pháp thu thập dữ liệu của chúng tôi có thể được sử dụng để thu thập thêm các chú thích để nắm bắt tính hữu ích trong các nền văn hóa khác.

Tuyên bố Đạo đức
Các người chú thích cho tập dữ liệu HELPSTEER được thuê thông qua Scale AI. Scale AI áp dụng Phương pháp Anker, Tiêu chuẩn Tác động Nguồn cung GISC và Mục tiêu Phát triển Bền vững của Liên hợp quốc để cung cấp mức lương công bằng và cạnh tranh. Mức lương cụ thể được tính toán dựa trên nhiều yếu tố, bao gồm dự án cụ thể, bộ kỹ năng chuyên môn và chuyên môn yêu cầu, chi phí sinh hoạt khu vực và sau đó được liệt kê minh bạch trên nền tảng Scale AI. Scale AI cũng cung cấp nhiều kênh cho các câu hỏi và hỗ trợ, bao gồm các đội hỗ trợ 24/7, các kênh thảo luận cộng đồng với các người điều hành được đào tạo đặc biệt, và một đường dây "lên tiếng" nơi các nhà thầu có thể báo cáo mối quan tâm một cách ẩn danh. Các mối quan tâm của nhân viên có thể được gửi đến và được xem xét bởi đội hỗ trợ Remotasks, và các tranh chấp về lương được xem xét bởi các chuyên gia hỗ trợ được đào tạo trong lĩnh vực này.

Lời cảm ơn
Chúng tôi muốn cảm ơn nhiều người tại NVIDIA và Scale AI đã hỗ trợ dự án này. Cụ thể, chúng tôi muốn cảm ơn Shengyang Sun đã xem xét bản thảo trước đó của bài báo.

Tài liệu tham khảo
Fernando Alva-Manchego, Louis Martin, Carolina Scarton, và Lucia Specia. 2019. EASSE: Đánh giá đơn giản hóa câu tự động dễ dàng hơn. Trong Kỷ yếu Hội nghị 2019 về Các phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên và Hội nghị Quốc tế lần thứ 9 về Xử lý ngôn ngữ tự nhiên (EMNLP-IJCNLP): Trình diễn Hệ thống, trang 49–54, Hồng Kông, Trung Quốc. Hiệp hội Ngôn ngữ học Tính toán.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, và Jared Kaplan. 2022. Huấn luyện một trợ lý hữu ích và vô hại với học tăng cường từ phản hồi của con người.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: Một chatbot mã nguồn mở gây ấn tượng với gpt-4 với chất lượng 90%* chatgpt.

Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, và Maosong Sun. 2023. Ultrafeedback: Tăng cường các mô hình ngôn ngữ với phản hồi chất lượng cao.

Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, và Oleksii Kuchaiev. 2023. Steerlm: SFT có điều kiện thuộc tính như một lựa chọn thay thế (có thể định hướng người dùng) cho rlhf.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Alpacafarm: Một khung mô phỏng cho các phương pháp học từ phản hồi của con người.

Kawin Ethayarajh, Yejin Choi, và Swabha Swayamdipta. 2022. Hiểu độ khó của tập dữ liệu với thông tin V-usable. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 39 về Học máy, tập 162 của Kỷ yếu Nghiên cứu Học máy, trang 5988–6008. PMLR.

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, và Jack Clark. 2022. Red teaming các mô hình ngôn ngữ để giảm tác hại: Phương pháp, hành vi mở rộng và bài học kinh nghiệm.

Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou

--- TRANG 10 ---
Wang, và Yaodong Yang. 2023. Beavertails: Hướng tới căn chỉnh an toàn cải thiện của llm thông qua tập dữ liệu sở thích con người.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, và William El Sayed. 2023. Mistral 7b.

Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, và Brad S. Chissom. 1975. Sự dẫn xuất của các công thức khả năng đọc mới (chỉ số khả năng đọc tự động, đếm fog và công thức dễ đọc flesch) cho nhân viên hải quân đăng ký.

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, và Alexander Mattick. 2023. Cuộc trò chuyện openassistant – dân chủ hóa căn chỉnh mô hình ngôn ngữ lớn.

Philippe Laban, Luke Dai, Lucas Bandarkar, và Marti A. Hearst. 2021. Các mô hình transformer có thể đo lường tính mạch lạc trong văn bản: Tái xem xét bài kiểm tra xáo trộn. Trong Kỷ yếu Cuộc họp thường niên lần thứ 59 của Hiệp hội Ngôn ngữ học Tính toán và Hội nghị Quốc tế lần thứ 11 về Xử lý ngôn ngữ tự nhiên (Tập 2: Bài báo ngắn), trang 1058–1064, Trực tuyến. Hiệp hội Ngôn ngữ học Tính toán.

Nathan Lambert và Roberto Calandra. 2023. Trần căn chỉnh: Sự không phù hợp mục tiêu trong học tăng cường từ phản hồi của con người.

Nathan Lambert, Lewis Tunstall, Nazneen Rajani, và Tristan Thrush. 2023. Tập dữ liệu sở thích trao đổi ngăn xếp huggingface h4.

Stephanie Lin, Jacob Hilton, và Owain Evans. 2022. TruthfulQA: Đo lường cách các mô hình bắt chước sai lầm của con người. Trong Kỷ yếu Cuộc họp thường niên lần thứ 60 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Bài báo dài), trang 3214–3252, Dublin, Ireland. Hiệp hội Ngôn ngữ học Tính toán.

Ilya Loshchilov và Frank Hutter. 2017. Điều chỉnh trọng lượng phân tách. arXiv preprint arXiv:1711.05101.

Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, và Jingren Zhou. 2023. Instag: Gắn thẻ hướng dẫn để phân tích tinh chỉnh có giám sát của các mô hình ngôn ngữ lớn.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, và John Schulman. 2022. Webgpt: Trả lời câu hỏi hỗ trợ bởi trình duyệt với phản hồi của con người.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, và Ryan Lowe. 2022. Huấn luyện các mô hình ngôn ngữ tuân theo hướng dẫn với phản hồi của con người.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, và Chelsea Finn. 2023. Tối ưu hóa sở thích trực tiếp: Mô hình ngôn ngữ của bạn bí mật là một mô hình thưởng.

Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, và Jonathan Cohen. 2023. Nemo guardrails: Một bộ công cụ cho các ứng dụng llm có thể kiểm soát và an toàn với các rail có thể lập trình.

Carolina Scarton và Lucia Specia. 2018. Học đơn giản hóa cho đối tượng mục tiêu cụ thể. Trong Kỷ yếu Cuộc họp thường niên lần thứ 56 của Hiệp hội Ngôn ngữ học Tính toán (Tập 2: Bài báo ngắn), trang 712–718, Melbourne, Australia. Hiệp hội Ngôn ngữ học Tính toán.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. 2017. Thuật toán tối ưu hóa chính sách gần đây. arXiv preprint arXiv:1707.06347.

Thomas Scialom, Louis Martin, Jacopo Staiano, Éric Villemonte de la Clergerie, và Benoît Sagot. 2021. Tái xem xét đánh giá tự động trong đơn giản hóa câu.

Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, và Ethan Perez. 2023. Hướng tới hiểu sự nịnh nọt trong các mô hình ngôn ngữ.

Prasann Singhal, Tanya Goyal, Jiacheng Xu, và Greg Durrett. 2023. Một chặng đường dài phía trước: Điều tra tương quan độ dài trong rlhf.

Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, và Paul Christiano. 2020. Học tóm tắt từ phản hồi của con người. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 34 về Các hệ thống Xử lý thông tin Thần kinh, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,

--- TRANG 11 ---
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023. Llama 2: Mô hình nền tảng và chat được tinh chỉnh mở.

Zhilin Wang và Pablo E. Torres. 2022. Làm thế nào để hữu ích trên các diễn đàn hỗ trợ trực tuyến? Trong Kỷ yếu Hội thảo lần thứ 4 về Hiểu biết Tường thuật (WNU2022), trang 20–28, Seattle, United States. Hiệp hội Ngôn ngữ học Tính toán.

Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, và Hannaneh Hajishirzi. 2023. Phản hồi con người chi tiết mang lại phần thưởng tốt hơn cho huấn luyện mô hình ngôn ngữ.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. 2023. Đánh giá llm-as-a-judge với mt-bench và chatbot arena.

A Phụ lục
A.1 Hướng dẫn Chú thích HELPSTEER
Bối cảnh NVIDIA đang làm việc để tạo ra một Mô hình Ngôn ngữ Lớn (LLM) có thể tuân theo hướng dẫn và đưa ra các câu trả lời phù hợp. Như một phần của nỗ lực này, đánh giá chi tiết các phản hồi được đưa ra bởi các mô hình và bởi con người, trên các trục và thuộc tính khác nhau, là cơ bản để hiểu cách các cải tiến và thay đổi đang ảnh hưởng đến hiệu suất. Do đó, chúng tôi yêu cầu các thành viên tham gia đánh giá các cặp lời nhắc-phản hồi trên một số tiêu chí để giúp đội LLM đánh giá hiệu suất của nó.

Hướng dẫn Bạn sẽ được đưa ra các lời nhắc/hướng dẫn và một số lượng đầu ra biến đổi. Nhiệm vụ của bạn là đánh giá những đầu ra đó dựa trên 5 trục này, mỗi trục trên thang điểm likert 5 điểm.

1. Tính hữu ích: Mức độ hữu dụng và hữu ích của phản hồi tổng thể.
2. Tính đúng đắn: Phản hồi dựa trên sự thật, không có ảo giác, không có sai lầm. Phản hồi bao gồm tất cả mọi thứ cần thiết trong hướng dẫn.
3. Tính mạch lạc: Phản hồi tự nhất quán về mặt nội dung, phong cách viết, và không tự mâu thuẫn. Phản hồi có thể được theo dõi và hiểu một cách logic bởi con người. Phản hồi không chứa thông tin dư thừa hoặc lặp lại.
4. Độ phức tạp: Đánh giá phản hồi dọc theo phổ từ đơn giản đến phức tạp. Một phản hồi đơn giản sử dụng từ vựng và cấu trúc câu đơn giản, dễ hiểu mà trẻ em có thể hiểu. Ngược lại, một phản hồi phức tạp sử dụng ngôn ngữ tinh vi với từ vựng nâng cao mà người lớn có trình độ học vấn cao hoặc chuyên gia về chủ đề sẽ sử dụng.
5. Tính dài dòng: Một phản hồi tính dài dòng thấp là trực tiếp đến điểm mà không có từ ngữ thừa. Hướng ngược lại là dài dòng, phản hồi có nhiều từ, đưa ra câu trả lời dài dòng và/hoặc chi tiết.

Dưới đây chúng tôi đưa ra giải thích sâu hơn về loại câu trả lời nào tương ứng với mỗi mức đánh giá.

Tính hữu ích
0. Phản hồi không hữu dụng hoặc hữu ích chút nào. Phản hồi hoàn toàn bỏ lỡ bản chất của những gì người dùng muốn.
1. Phản hồi gần như không hữu ích và chủ yếu không nắm bắt được những gì người dùng đang tìm kiếm, nhưng vẫn có thể sử dụng được và hữu ích theo một cách nhỏ.
2. Phản hồi hữu ích một phần nhưng bỏ lỡ mục tiêu tổng thể của truy vấn/đầu vào của người dùng theo một cách nào đó. Phản hồi không hoàn toàn thỏa mãn những gì người dùng đang tìm kiếm.
3. Phản hồi chủ yếu hữu ích và chủ yếu phù hợp với những gì người dùng đang tìm kiếm, nhưng vẫn còn chút chỗ để cải thiện.
4. Phản hồi cực kỳ hữu ích và hoàn toàn phù hợp với tinh thần của những gì lời nhắc đang yêu cầu.

Tính đúng đắn
0. Phản hồi hoàn toàn không chính xác. Tất cả thông tin được cung cấp đều sai, giả hoặc ảo giác. Nếu lời nhắc yêu cầu trợ lý thực hiện một nhiệm vụ, nhiệm vụ hoàn toàn không được thử, hoặc nhiệm vụ sai đã được thử trong phản hồi. Phản hồi hoàn toàn không liên quan đến lời nhắc.

--- TRANG 12 ---
1. Phản hồi có một số yếu tố đúng nhưng chủ yếu sai hoặc không đầy đủ. Phản hồi có thể chứa nhiều trường hợp ảo giác, thông tin sai, thông tin gây hiểu lầm, hoặc thông tin không liên quan. Nếu lời nhắc yêu cầu trợ lý thực hiện một nhiệm vụ, nhiệm vụ đã được thử với một lượng thành công nhỏ.

2. Phản hồi chứa hỗn hợp thông tin đúng và sai. Phản hồi có thể bỏ lỡ một số chi tiết, chứa thông tin gây hiểu lầm, hoặc ảo giác nhỏ, nhưng ít nhiều phù hợp với những gì lời nhắc yêu cầu. Nếu lời nhắc yêu cầu trợ lý thực hiện một nhiệm vụ, nhiệm vụ được thử với thành công vừa phải nhưng vẫn có chỗ rõ ràng để cải thiện.

3. Phản hồi chủ yếu chính xác và đúng với một lượng nhỏ thông tin bị thiếu. Nó không chứa thông tin gây hiểu lầm hoặc ảo giác. Nếu lời nhắc yêu cầu trợ lý thực hiện một nhiệm vụ, nhiệm vụ chủ yếu được thử thành công.

4. Phản hồi hoàn toàn chính xác và chính xác với những gì được yêu cầu bởi lời nhắc mà không có chi tiết cần thiết nào bị thiếu và không có thông tin sai, gây hiểu lầm, hoặc ảo giác. Nếu lời nhắc yêu cầu trợ lý thực hiện một nhiệm vụ, nhiệm vụ được thực hiện và giải quyết hoàn toàn trong phản hồi.

Tính mạch lạc Với thuộc tính này, chúng tôi đo lường mức độ rõ ràng, thuyết phục và tự nhất quán của phản hồi mô hình. Thuộc tính này sẽ đặc biệt đa dạng cho các câu hỏi, nhiệm vụ và mục tiêu mở như viết một câu chuyện, tạo ra một cuộc đối thoại, hoặc tóm tắt nhưng cũng áp dụng cho các cặp lời nhắc/phản hồi đơn giản hơn.

0. (Hoàn toàn Không mạch lạc và/hoặc Không rõ ràng) - Phản hồi hoàn toàn không thể hiểu được và không có ý nghĩa rõ ràng hoặc thông điệp hợp lý nào có thể được hiểu từ nó.

1. (Chủ yếu Không mạch lạc và/hoặc Không rõ ràng) - Phản hồi chủ yếu khó theo dõi, với sự không nhất quán, mâu thuẫn, luồng logic khó hiểu, hoặc ngôn ngữ không rõ ràng được sử dụng xuyên suốt, nhưng có một số phần mạch lạc/rõ ràng.

2. (Hơi Không rõ ràng và/hoặc Không mạch lạc) - Phản hồi hơi không rõ ràng. Có một số sự không nhất quán hoặc mâu thuẫn, câu chạy dài, tuyên bố khó hiểu, hoặc các phần khó theo dõi của phản hồi.

3. (Chủ yếu Mạch lạc và Rõ ràng) - Phản hồi chủ yếu rõ ràng và mạch lạc, nhưng có thể có một hoặc hai chỗ mà từ ngữ khó hiểu hoặc luồng phản hồi hơi khó theo dõi. Nhìn chung, phản hồi chủ yếu có thể được theo dõi với chút chỗ để cải thiện.

4. (Hoàn toàn Mạch lạc và Rõ ràng) - Phản hồi hoàn toàn rõ ràng và tự nhất quán xuyên suốt. Không có các khẳng định hoặc tuyên bố mâu thuẫn, việc viết chảy một cách logic, và việc theo dõi dòng suy nghĩ/câu chuyện không phải là thử thách.

Độ phức tạp
0. (Cơ bản) - Phản hồi sử dụng ngôn ngữ rất dễ hiểu, rõ ràng và hoàn toàn có thể hiểu được bởi trẻ em, người lớn và bất kỳ ai có khả năng sử dụng ngôn ngữ chức năng.

1. (Đơn giản) - Phản hồi sử dụng ngôn ngữ và từ ngữ tương đối đơn giản, nhưng có thể cần một số học vấn qua tiểu học hoặc trung học cơ sở trong ngôn ngữ đó để hiểu phản hồi.

2. (Trung cấp) - Những người đã hoàn thành giáo dục trung học có thể có thể hiểu từ vựng và cấu trúc câu được sử dụng, nhưng những người ở mức cơ bản hoặc trẻ em có thể gặp khó khăn để hiểu phản hồi.

3. (Nâng cao) - Phản hồi sử dụng từ vựng và thuật ngữ khá tinh vi. Một người chuyên ngành này tại một trường cao đẳng hoặc đại học có thể đã viết nó và sẽ hiểu phản hồi. Một người lớn trung bình không làm việc hoặc học tập trong lĩnh vực này không thể viết phản hồi này.

4. (Chuyên gia) - Một chuyên gia trong lĩnh vực hoặc khu vực có thể đã viết phản hồi. Nó sử dụng từ vựng cụ thể và liên quan về mặt kỹ thuật. Nó chứa ngôn ngữ cao cấp mà một người ở mức đơn giản hoặc cơ bản có thể không hiểu chút nào. Ngôn ngữ chuyên nghiệp của luật sư, nhà khoa học, kỹ sư, hoặc bác sĩ rơi vào danh mục này.

--- TRANG 13 ---
Tính dài dòng Mục tiêu ở đây là đặt phản hồi trên một phổ từ những câu trả lời ngắn nhất, sắc bén, đến những câu trả lời dài nhất, chi tiết nhất và/hoặc dài dòng nhất trong bối cảnh những gì người dùng mong đợi như một phản hồi cho lời nhắc. Ví dụ, nếu lời nhắc yêu cầu mô hình một câu hỏi có hoặc không và mô hình đơn giản trả lời "có" thì câu trả lời là súc tích. Nhưng nếu mô hình trả lời "có", phát lại lại câu hỏi được diễn đạt như một câu trả lời, và giải thích tại sao nó đưa ra câu trả lời đó, thì câu trả lời là dài dòng. Ngay cả khi hai phản hồi có chính xác cùng độ dài, một cái có thể được đánh giá là dài dòng và cái khác là súc tích tùy thuộc vào bối cảnh nhắc nhở.

0. (Súc tích) - Phản hồi ngắn, đúng trọng tâm, và súc tích nhất có thể. Không có thông tin bổ sung nào được cung cấp ngoài những gì được yêu cầu bởi lời nhắc.

1. (Khá Ngắn) - Phản hồi ở phía ngắn hơn nhưng vẫn có thể có từ ngữ, chi tiết, và/hoặc văn bản được loại bỏ trước khi nó ở mức tối thiểu của những gì phản hồi đang cố gắng truyền đạt.

2. (Độ dài Trung bình) - Phản hồi không đặc biệt dài hoặc ngắn dựa trên những gì lời nhắc yêu cầu từ mô hình. Độ dài là đầy đủ để truyền đạt một phản hồi đầy đủ nhưng không đặc biệt dài dòng cũng không đặc biệt súc tích.

3. (Hơi Dài) - Phản hồi ở phía dài hơn nhưng vẫn có thể có nhiều hơn được thêm vào nó trước khi nó được coi là đầy đủ chi tiết hoặc lan man.

4. (Dài dòng) - Phản hồi đặc biệt dài, nhiều từ, và/hoặc mở rộng với chi tiết bổ sung dựa trên những gì lời nhắc yêu cầu từ mô hình trợ lý. Phản hồi có thể dài dòng bất kể độ dài có do lặp lại và thiếu mạch lạc hay do chi tiết phong phú và sâu sắc.

Ví dụ cho người chú thích Ngoài ra, chúng tôi đưa cho người chú thích 5 lời nhắc ví dụ, mỗi cái với 2 đến 3 phản hồi được chú thích với các nhãn thuộc tính HELPSTEER. Chúng tôi cung cấp một lời nhắc ví dụ như vậy dưới đây với các phản hồi mẫu và nhãn.

Lời nhắc Ánh sáng xanh là một bước sóng có mặt tự nhiên trong ánh sáng, bao gồm ánh sáng từ mặt trời. So với các loại ánh sáng khác, ánh sáng xanh có bước sóng ngắn. Điều này quan trọng vì các bác sĩ liên kết ánh sáng bước sóng ngắn với rủi ro lớn hơn cho tổn thương mắt. Trong khi nhiều thiết bị điện tử (bao gồm bóng đèn) phát ra ánh sáng xanh, màn hình máy tính và tivi thường phát ra nhiều ánh sáng xanh hơn các thiết bị điện tử khác. Điều này là do máy tính và tivi thường sử dụng màn hình tinh thể lỏng hoặc LCD. Những màn hình này có thể trông siêu rõ ràng và sống động, nhưng chúng cũng phát ra nhiều ánh sáng xanh hơn các đối tác không phải LCD của chúng.

Tóm tắt văn bản thành 2 câu.

Phản hồi 1 Ánh sáng xanh là một bước sóng có mặt tự nhiên trong ánh sáng, bao gồm ánh sáng từ mặt trời.

Nhãn 1
1. Tính hữu ích: 0
2. Tính đúng đắn: 1
3. Tính mạch lạc: 4
4. Độ phức tạp: 1
5. Tính dài dòng: 1

Phản hồi 2 Ánh sáng xanh là một bước sóng có mặt tự nhiên trong ánh sáng, và nó được liên kết với rủi ro lớn hơn cho tổn thương mắt. Màn hình máy tính và tivi thường phát ra nhiều ánh sáng xanh hơn các thiết bị điện tử khác vì chúng sử dụng màn hình tinh thể lỏng.

Nhãn 2
1. Tính hữu ích: 4
2. Tính đúng đắn: 4
3. Tính mạch lạc: 4
4. Độ phức tạp: 3
5. Tính dài dòng: 2

Phản hồi 3 Ánh sáng xanh là một loại ánh sáng với bước sóng ngắn được phát ra từ cả nguồn tự nhiên và nhân tạo. Tiếp xúc với ánh sáng xanh được liên kết với tổn thương mắt, đặc biệt từ việc tiếp xúc kéo dài với các nguồn nhân tạo như màn hình máy tính và tivi.

Nhãn 3
1. Tính hữu ích: 4
2. Tính đúng đắn: 4
3. Tính mạch lạc: 4
4. Độ phức tạp: 2
5. Tính dài dòng: 2

--- TRANG 14 ---
A.2 Yêu cầu tính toán để huấn luyện mô hình

Mô hình | MTBench ↑ | Tính toán (Giờ GPU) ↓
STEERLM | 7.54 | 1536
SFT | 6.29 | 1024
RLHF w. HH-RLHF | 7.21 | 7168
DPO w. HH-RLHF | 6.94 | 2048
DPO w. OASST | 6.98 | 1024

Bảng 6: Tính toán so với MT Bench, đo lường tính hữu ích của mô hình. Cao hơn là tốt hơn cho MT Bench và thấp hơn là tốt hơn cho Tính toán. Thí nghiệm được chạy trên 32 nút của 8 GPU A100-80GB SXM mỗi nút. Touvron et al. (2023) không phát hành tính toán cần thiết để thực hiện SFT và RLHF trên mô hình Llama 2 70B Chat.
