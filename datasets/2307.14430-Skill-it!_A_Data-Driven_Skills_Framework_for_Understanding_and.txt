# 2307.14430.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/datasets/2307.14430.pdf
# File size: 2009966 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Skill-it! A Data-Driven Skills Framework for Understanding and
Training Language Models
Mayee F. Chen*1Nicholas Roberts2Kush Bhatia1Jue Wang3Ce Zhang3, 4
Frederic Sala2Christopher Ré1
1Department of Computer Science, Stanford University
2Department of Computer Sciences, University of Wisconsin-Madison
3Together AI
4Department of Computer Science, University of Chicago
July 28, 2023
Abstract
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget
of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a
new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language
models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be
utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes
the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data,
we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with
less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data
sampling algorithm, SKILL -IT, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the
objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in
the continual pre-training setting, SKILL -ITobtains 36.5points higher accuracy than random sampling. On the Natural
Instructions dataset in the fine-tuning setting, SKILL -ITreduces the validation loss on the target skill by 13.6% versus
training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to
continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the
baseline approach of sampling uniformly over data sources with 3B tokens.
1 Introduction
Large language models (LMs) exhibit remarkable capabilities, including producing creative content [ 55], writing source
code [ 8], or chatting with users [ 7]. A key ingredient in enabling models to perform such tasks is the data on which the
models are trained [ 17,19,59]. A natural way to unlock particular capabilities is to improve this training data. However,
it is unclear how to select data from a large corpus for these capabilities given a fixed budget of training tokens, as data
selection methods for current state-of-the-art LMs mostly rely on heuristics for filtering and mixing together different
datasets [ 32,59]. We lack a formal framework for capturing how data influences the model’s capabilities and how to utilize
this data effectively for improving LM performance.
To develop such a framework, we take inspiration from how humans acquire knowledge. A classic idea in education
literature is the concept of skills that form a learning hierarchy [ 65]. For example, one study found that students learned
mathematical and scientific skills most quickly when these skills were presented in a particular order [ 11]. We seek to
understand the extent that similar skill-based orderings characterize LM training. Such orderings, if they exist, may provide
a better understanding of LMs as well as a mechanism for data-efficient training. For instance, to train an LM for Spanish
question generation, we wish to know if training first on related but simpler tasks, such as Spanish grammar and English
question generation, helps.
We study if the idea of skill orderings can help us build a framework that relates data to LM training and behavior. This
requires addressing two challenges revolving around the connection between skills and data. First, in order to show that
there exist sets of skills that the LM learns most efficiently in some particular order, an operational definition of LM skill and
skill ordering must be developed and validated on data . In initial experiments, we investigated if semantic groupings of data,
such as metadata attributes or embedding clusters, were sufficient to represent a skill and characterize how models learn. For
*Corresponding author: mfchen@cs.stanford.edu .
1arXiv:2307.14430v1  [cs.CL]  26 Jul 2023

--- PAGE 2 ---
Figure 1: Inspired by how humans acquire knowledge, we hypothesize that LMs best learn skills in a particular order and
that this can help improve our understanding and training of LMs. We show that these ordered skill sets exist in real data,
which enables skills to be learned with less data given that we train on their prerequisite skills. We then propose SKILL -IT,
an online data selection algorithm that learns skills quickly by exploiting their ordering.
instance, we partitioned the Alpaca dataset [ 56] by instruction type—a technique used to capture dataset diversity [ 62]—but
we found that sampling based on instruction types and random sampling resulted in similar model performance, suggesting
that not just any existing notion of data groups can characterize skills.
Second, these definitions of skills must be used to construct sampling distributions to actually improve model training.
To develop criteria for a data selection algorithm that learns skills efficiently, we identify challenges that naive selection
approaches face. The standard approach of random uniform sampling over data fails to learn skills optimally due to not
accounting for skill imbalance and ordering. Skills can be distributed unevenly in the data, with more complex skills
being rare—for instance, Spanish and question generation (QG) are 5%and4%of the Natural Instructions dataset [ 63],
respectively, but Spanish QG is only 0.2%. Random sampling also provides no mechanism for taking into account a particular
training order and dependency structure on skills. More sophisticated techniques like curriculum learning account for
sample-level ordering, but not skills or their dependencies. Our goal framework must account for these issues of imbalance
and ordering.
Skill-based framework We define a skill as a unit of behavior that a model can learn using an associated slice of data
(Definition 2.1). An ordered skill set is a collection of skills with a directed skills graph that is neither complete nor empty,
where an edge from a prerequisite skill to a skill exists if the amount of training it takes to learn the skill can be reduced if
the prerequisite skill is also learned (Definition 2.2, Figure 1 left, center). We show that ordered skill sets exist in synthetic
and real datasets using this operational definition. Interestingly, the existence of these ordered skill sets unveils that one can
learn a skill quickly not by training solely on that skill, but on a mixture of that skill and prerequisite skills. For instance,
in Figure 3 we observe that Spanish QG can be learned more efficiently when the model also learns English QG and
Spanish—we can achieve 4%lower validation loss than training on only Spanish QG over a fixed budget of overall training
steps.
Next, given an ordered skill set to train on, we use our framework to propose methods for how to select data so that the
LM learn skills faster: skill-stratified sampling and an online generalization, SKILL -IT. We address the issue of unevenly
distributed skills in datasets by proposing skill-stratified sampling, a simple approach that allows us to explicitly optimize
for learning skills by uniformly sampling relevant skills (such as a target skill and its prerequisite skills in fine-tuning).
Skill-stratified sampling uses the construction of the ordered skill set but is static, which does not incorporate the ordering
as training proceeds and results in oversampling skills that may be already learned early on in training. We address this
issue by proposing an online data selection algorithm, SKILL -IT, for selecting mixtures of training skills that allocates more
weight towards learning skills that are not yet learned or towards prerequisite influential skills (Figure 1 right). SKILL -ITis
derived from an online optimization problem over the training skills for minimizing loss on a set of evaluation skills given a
fixed budget of data and the skills graph. SKILL -ITis inspired by online mirror descent and can be adapted for continual
pre-training, fine-tuning, or out-of-domain evaluation depending on the relationship between the evaluation skill set and the
training skill set.
We evaluate SKILL -ITon synthetic and real datasets at two model scales, 125M and 1.3B parameters. For the continual
pre-training setting, we show on the LEGO synthetic [ 72] that we obtain a 35.8point improvement in accuracy over
randomly selecting training data and curriculum learning [ 3]. For the fine-tuning setting, we show that on the widely-used
Natural Instructions dataset [ 40,64], our algorithm over a mixture of skills is able to achieve up to 13.6% lower loss on
that skill than solely training on that skill, given the same overall training budget. For the out-of-domain setting when our
2

--- PAGE 3 ---
Alpaca
0.00.51.01.52.0
Pile of Law
0.00.51.01.5
Natural Instructions
0.00.20.40.60.8Figure 2: Heatmaps of adjacency matrices we compute for skill graphs for Alpaca, Pile of Law, and Natural Instructions.
Negative elements and diagonals are thresholded to 0for clarity. See Appendix C.2 for descriptions of how they were
constructed and larger versions.
training skills do not align perfectly with evaluation skills, our algorithm is able to achieve the lowest loss on 11 out of 12
evaluation skills corresponding to task categories in the Natural Instructions test tasks dataset over random and skill-stratified
sampling on the training data. We finally apply our framework to a case study on the recent RedPajama 1.2 trillion token
dataset [ 57]. We use the data mixture produced by SKILL -ITto continually pre-train a 3B parameter model. We find that
SKILL -ITachieves higher accuracy with 1B tokens than uniform sampling over data sources with 3B tokens.
2 Skills framework
First, we propose definitions of skills and ordered skill sets in order to formalize our intuition around how models learn
skills, and we demonstrate that not just any existing notion of data groups can characterize an ordered skill set in the dataset.
Then, we demonstrate the existence of ordered skill sets on synthetic and real data, which show how viewing data through a
skills-based framework can help with training and understanding model performance. Finally, we explore unsupervised skill
recovery from data, finding that embedding-based approaches do not adequately recover synthetic skills.
2.1 Definitions
We first present a definition of an individual skill. Let the input space of all possible text data be X, where x∈ X is an
individual text sample that a next-token-prediction LM f∈ F:X → X is trained on. We quantify learning via a metric
L:F × X → R, which maps from a model and evaluation data to a scalar quantity. In our setup, we use the cross-entropy
validation loss applied over next-token predictions as our metric L.
Definition 2.1 (Skill) .Askillsis a unit of behavior with associated data Xs⊆ X such that if fis trained on an dataset
Ds⊂ Xs, then fhas improved metric Lon samples belonging to Xs\Dson average.
This definition of a skill is flexible—it simply means that given a training dataset associated with the skill, a model fhas
an improved metric (e.g., decreasing validation loss) when evaluated on validation data associated with this skill. Under this
definition, a skill could be a granular task, such as Spanish question generation for a subset of Wikipedia articles, or can be
defined over a data source, such as next-token prediction of legal data from tax court rulings. However, our next definition,
the ordered skill set, has a more specific construction and provides a framework for how models learn across dependent
skills.
Definition 2.2 (Ordered skill set, skills graph) .Anordered skill set forfis a collection of skills S={s1, . . . , s k}over
which there is a directed skills graph G= (S, E)on the skill set that is neither complete or empty, where (si, sj)∈Eif the
amount of data needed to learn sjwhen uniformly sampling from Dsi∪ Dsjis no more than the amount of data needed
when sampling only from Dsj. We equate learning a skill sjtofattaining a certain value of Lor lower on average over
Xsj\Dsj.
This definition isolates complete and empty graphs as extrema that do not capture meaningful sets of skills. We discuss
the three types of skill graphs—complete, empty, intermediate—and their implications for data selection. In particular, we
discuss how several initial attempts of defining skills over datasets via semantic groupings resulted in the extrema cases (see
Appendix C.2 for full results):
•The complete graph demonstrates that all skills influence each other. A random partition is an example of a skill set
that yields a complete graph. This graph suggests that the best approach for learning any skill or set of skills is random
sampling on the dataset. This is not a setting where we can gain much with skill-based sampling. For example, using
instruction types as skills on the Alpaca dataset results in a nearly complete estimated skills graph ( 97.4%dense), and we
3

--- PAGE 4 ---
0 1000 2000 3000 4000 5000 6000
Steps0.00.20.40.60.8Validation LossModel performance on LEGO skill 3
Trained on skill 3
Trained on skills 1, 2, 3
0 1000 2000 3000 4000 5000 6000
Steps0.00.51.01.52.0Validation LossModel performance on Addition skill 1
Trained on skill 1
Trained on skills 1, 2
2.42.62.8Validation Loss
Model performance on Spanish QG
Trained on Spanish QG
Trained on [Spanish, English] x [QA, QG]
0 100 200 300 400 500 600
Steps1.52.02.5Validation Loss
Model performance on stance detection
Trained on stance detection
Trained on stance detection, text matchingFigure 3: On the LEGO synthetic, 3-digit addition, and Natural Instructions, we identify examples of ordered skill sets in
which training on a mixture of skills helps learn an individual skill faster than just training on that skill itself, given a fixed
training budget.
find that stratified sampling on these skills only improves validation loss per skill by 0.007points over random sampling
on average (Figure 2 left), suggesting that utilizing skills does not improve model performance in this case.
•The empty graph demonstrates that each skill is independent. This can occur if skills are too granular; for instance, learning
Spanish math problems is unlikely to help with English poem generation. This graph suggests that the best approach for
learning an individual skill is to train on the skill itself. We see that empty graphs exist in real data; in Figure 2 (center),
using data sources as skills on the Pile of Law [21] results in a nearly empty skills graph ( 3.9%dense).
•Graphs that are neither empty nor complete thus suggest a nontrivial order of how skill influence each other. This is the
setting in which we expect that identifying skills and exploiting their ordering will help the most . In Figure 2 right, we use
task categories, which capture broader reasoning patterns, as skills on Natural Instructions and find that the estimated
graph has intermediate density ( 42.7%dense). We show concrete examples of how skills can be learned more efficiently
on Natural Instructions in Section 2.2.
While these intuitive groupings result in ordered skill sets on some datasets (e.g., task categories on NI), this is not always
the case (e.g., instruction types on Alpaca and sources on Pile of Law). Even though these groupings capture some notion
of diversity in the dataset, our findings suggest that not just any semantic grouping induces an ordered skill set. We now
empirically demonstrate that our definition of ordered skill sets aligns with how models learn and can be exploited for more
data-efficient training.
2.2 Examples of skills and ordered skill sets
We provide examples of ordered skill sets on the LEGO synthetic dataset, an addition synthetic dataset, and subsets of the
Natural Instructions dataset. On these datasets, we find that certain skills are better learned when trained along with their
prerequisite skills rather than in isolation.
LEGO skills The LEGO synthetic, first introduced in [ 72], can evaluate a model’s ability to follow a chain of reasoning.
In this synthetic, the letters of the alphabet, A, are variables each with some binary label in {0,1}. An individual sample
consists of kclauses for some fixed kacross the dataset, each of the form a=gxwhere a, x∈ A andgis either a negation
(“not”) or assertion (“val”), e.g. we assign ato the value of x, or we assign ato the opposite label. At the end of the sentence,
we prompt the model for what the value of one of these variables is. Two samples x∈ X are given below for k= 5:
Input: b = not y, r = val 1, m = val b, q = val m, y = not r. Output: b = 1.
Input: c = val x, p = val f, x = val k, f = not c, k = val 0. Output: k = 0.
These samples each correspond to a chain of reasoning; for instance the first sample has the chain r, y, b, m, q , where
knowing q’s label requires the most reasoning steps. We define the ith skill sias the model’s ability to know the ith variable
of the chain. From our example above, the first sample belongs to Xs3and the second sample belongs to Xs1. To demonstrate
the existence of ordered skill sets, we continually pre-train the 125M parameter GPT-Neo model [ 5,13] over various
mixtures of LEGO skills with k= 5. In Figure 3 (left), we find that in 35.9% fewer training steps, training on a balanced
mixture of Xs1,Xs2, andXs3resulted in the same validation loss of 0.01as training solely on Xs3. This suggests that s1, s2
helped unlock performance on s3and that there exist edges from s1ors2tos3in the skill graph. Additional observations
are available in Appendix D.1, where we examine other edges as well as more complex reasoning chains, and the full skills
graph corresponding to the ordered skill set for LEGO with k= 5is in Figure 10.
Addition skills We consider a variant of a synthetic 5-digit addition dataset analyzed in [ 44]. We show the existence of
ordered skill sets for a simplified 3-digit addition dataset where we treat each digit prediction as a skill—the outputs, in this
case, are the integers {0,1, ...,9}. Examples are of the following form:
4

--- PAGE 5 ---
Input: A = 1 0 6 + 0 7 1 , A 0 = ? Output: 7 Input: A = 6 0 6 + 8 7 9 , A 2 = ? Output: 4
where ‘A 0’ refers to the ones digit of the output ( s1) and ‘A 2’ refers to the hundreds digit ( s3). In Figure 3 (center), we
find that in 32% fewer training steps, training on a balanced mixture of Xs1, andXs2resulted in the same validation loss of
0.01as training solely on Xs1. That is, the ones digit addition skill can be improved by simultaneously learning the tens
digit addition skill, even though the former should not require information from the latter—this is in line with observations
from prior work that models do not always learn the ones digit addition first [ 44]. The full skills graph corresponding to the
ordered skill set over 3-digit addition is in Figure 11.
Natural Instructions (NI) skills We show that ordered skill sets exist in NI [ 63] when we treat task categories as skills.
•In Figure 3 (top right), we show that ordered skill sets exist over crosslingual task categories. Training on Spanish question
generation (QG) along with equal parts of English QG, Spanish question answering (QA), and English QA results in 4.1%
lower validation loss than training only on Spanish QG. Remarkably, the former only uses 25% of the latter’s Spanish QG
data. This suggests that there are edges from Spanish QA, English QA, and English QG to Spanish QG.
•In Figure 3 (bottom right), we see that training on the task category Text Matching along with Stance Detection helps
decrease the loss on Stance Detection by 11%. This suggests that these categories, which both involve understanding the
relationship between two input texts, share an edge.
The full skills graphs corresponding to the ordered skill sets over these task categories are in Figure 13. While equating
task categories to skills may be noisy, these examples suggest that there is signal within real data that suggests that ordered
skill sets can improve data efficiency.
2.3 Skill recovery
A final component of characterizing skills is unsupervised recovery of ordered skill sets. We consider embedding-based
clustering approaches and a loss-based clustering approach for recovering LEGO skills. When clustering data using various
trained and pre-trained embeddings, we find that they were unable to achieve above 39% accuracy on LEGO. Instead, we
find that taking 10random training runs and clustering data by their lossper timestep per run recovers the skills with 61%
accuracy (Table 3). The intuition behind this method is that the validation losses on points from the same skill have similar
trajectories as models learn. We discuss this approach more in Appendix D.2.
3 Skills-based data selection
Now that we have established the existence of ordered skill sets, we discuss how to use them for data selection. We state
the data selection problem for learning across skills in Section 3.1. We discuss how to learn the skills graph that will be
exploited in our data selection methods in Section 3.2. We then introduce two sampling methods that utilize the graph, a
simple skill-stratified sampling method and the online sampling method S KILL -IT, in Section 3.3.
3.1 Problem statement
We are given an ordered training skill set Strain={strain,1, . . . , s train,k}on the training data, each with associated support
setXstrain,1, . . .Xstrain,k, and an ordered evaluation skill set Seval={seval,1, . . . , s eval,m}ofmevaluation skills on a separate
evaluation dataset. We aim to select nsamples from Strainvia a mixture of training skills, p∈∆k−1, to achieve three goals
depending on how Sevalis constructed:
•Continual pre-training : when Seval=Strain, our goal is select a mixture of training skills to learn all of them.
•Fine-tuning : when Seval⊂ S train, our goal is to select a mixture of training skills to learn an individual target skill or
subset of these skills.
•Out-of-domain : when Seval∩S train=∅, our goal is to select a mixture of training skills to learn a disjoint set of evaluation
skills we cannot train on. This can arise when we have a separate downstream validation dataset or the skills identified in
the training dataset are noisy.
Furthermore, we have a skills graph G= (Strain∪S eval, E), where E⊆ S train×S evalandA∈Rk×mis a weighted adjacency
submatrix, where Aijdescribes the strength of the edge from strain,itoseval,j. In Table 1, we summarize how the three
different settings are constructed and how Avaries across them. Next, we discuss how Acan be estimated from the data.
3.2 Skills graph learning
The skills graph is important for determining how to sample from the ordered skill set for training efficiently. We present two
approaches for learning the skills graph—brute-force and linear approximation. Algorithms are provided in Appendix B.2.
By definition 2.2, the brute-force way of identifying edges involves fixing an overall training budget of Hsteps and 1)
5

--- PAGE 6 ---
Setting Seval Skills graph
Continual pre-training Seval=Strain A∈Rk×k, edges among all Strain
Fine-tuning Seval⊂ S train A∈Rk×m, edges from all training skills to target skill subset
Out-of-domain Seval∩ S train=∅A∈Rk×m, edges from all training skills to separate evaluation skill set
Table 1: Summary of three settings—continual pre-training, fine-tuning, and out-of-domain. These settings are determined
by how Sevalis defined and result in different skills graphs used for our sampling methods.
Algorithm 1: SKILL -ITOnline Data Selection Algorithm
1:Input: Ordered training skill set Strain, ordered evaluation skill set Seval. Learning rate η,Trounds, nsamples, H
training steps per run for graph learning, model f1, window parameter w.
2:A←LEARN GRAPH (Strain,Seval, H, f 1)(Alg. 2, 3).
3:Initialize pi
1= exp( ηPm
j=1Aij)for all i∈[k], the softmax over A.
4:fort= 1, . . . , T −1do
5: Observe losses Leval,j(ft)for all seval,j∈ S eval.
6: Train model ftwithn/T samples from mixture ptoverStrain. Update model ft+1= Φ(ft, pt).
7: Setpi
t+1= exp( ηPt
τ=t−w+1Pm
j=1AijLeval,j(fτ)).
8:end for
training and evaluating the model on each siand 2) training the model on each pair of (si, sj)and evaluating on siand
sj. If the loss on sjwhen trained on both siandsjis lower, there exists an edge from sitosj. This approach has runtime
O(Hk2), which is feasible for small k. When kis large, we can approximate this approach in linear time by training on
eachsiforh < H steps and setting Aij>0if the loss on sjdecreases over hsteps for a runtime of O(hk). This linear
approach is necessary in the out-of-domain setting when SevalandStrainare disjoint, as we do not train on data associated
withSeval. In addition, both graph learning approaches can be performed on a smaller model, and the learned graph can be
used for data selection for training a larger model (Appendix D.4).
3.3 Skills graph-aware sampling
We present two approaches for sampling over the mixture of training skills according to the skills graph: skill-stratified sam-
pling, which samples uniformly over relevant training skills according to A, and SKILL -IT, which is an online generalization
that incorporates knowledge of how skills are being learned throughout training.
3.3.1 Skill-stratified sampling
A straightforward sampling approach is to discard training skills that do not benefit the evaluation skills and sample uniformly
over the set of relevant training skills, which we call skill-stratified sampling . For continual pre-training, the relevant skills
are the entire training skill set; for each strain,i∈ S train,Pr(strain,i) =1
k. This enables each skill to have sufficient training
data. For fine-tuning, the relevant skills are the target skills and prerequisite skills, which can be identified via positive entries
of the ith column of AwithSprereq={strain,i:∃seval,js.t.Aij>0}. We then set Pr(s) =1
|Sprereq∪S eval|fors∈ S prereq∪S eval.
For the out-of-domain setting, skill-stratified sampling is over the set of prerequisite skills. For each s∈ S prereq, we set
Pr(s) =1
|Sprereq|. Next, we propose our online algorithm that exploits the graph dynamically for more efficient training.
3.3.2 S KILL -ITonline data selection algorithm
Despite accounting for prerequisite skills, one shortcoming of skill-stratified sampling is that even if a skill has already
obtained sufficiently low validation loss early during training, we will continue to allocate the same weight to that skill
throughout training. Therefore, we formulate our data selection problem as an online learning problem and propose SKILL -IT,
which both prioritizes prerequisite skills and skills that are not yet learned.
We are given a budget of Trounds and ntotal samples to train on. At round t, we select a mixture pt∈∆k−1from the
k-dimensional unit simplex, and for each training skill strain,i∈ S train, we sample from Xstrain,iwith proportion pi
tfor a total
ofn
Tsamples per round. Let ftbe the model at at the start of round t. We can define ftrecursively as a function of the
previous round’s model ft−1and mixture pt−1via a dynamics function Φ :F ×∆k−1→ F ; that is, ft= Φ(ft−1, pt−1).
LetLeval,j(ft)be the validation loss of ftonseval,j. Our goal is to select p1, . . . , p Tto minimize loss per evaluation skill at
6

--- PAGE 7 ---
the end of training:
minimize
p1,...,pT∈∆k−11
mmX
j=1Leval,j(fT). (1)
This optimization problem is challenging to solve without additional assumptions. In order to make the problem
tractable, we impose an explicit dynamics rule for the each evaluation skill’s loss Leval,jin terms of the current loss and
data mixture. Assuming for simplicity that Seval⊆ S train, a simple rule would be Leval,j(ft) =Leval,j(Φ(ft−1, pt−1)) :=
Leval,j(ft−1)(1−αpj
t−1)forα∈[0,1]. That is, we expect that allocating more data to skill jshould result in the validation
loss on skill jdecreasing. However, such an expression assumes that only training on the jth skill will help learn the jth skill.
Instead, Section 2.2 suggests that there are other skills that may help with the jth skill. We propose the following dynamics:
Leval,j(ft) =Leval,j(ft−1)(1−A⊤
:,jpt−1), (2)
where A:,jis the column with weights of all skills that influence seval,j, and we absorb the scalar αintoA. The
optimization problem in (1) can thus be simplified as follows:
minimize
p1,...,pT∈∆k−11
mmX
j=1Leval,j(fT) (3)
s.tft= Φ(ft−1, pt−1)∀t= 1, . . . T
Leval,j(ft) =Leval,j(ft−1)(1−A⊤
:,jpt−1)∀j∈[m]
In Appendix B, we derive the following update rule via online mirror descent [45] for learning rate η >0:
pi
t+1=pi
texp
ηmX
j=1AijLeval,j(ft)
. (4)
In addition, when equation 4 is expanded, we have that pi
t+1=pi
1exp
ηPt
τ=1Pm
j=1AijLeval,j(fτ)
. Since this
summation over τresults in diminishing strength of updates, we change it to a moving window of size w. Our full method is
in Algorithm 1.
Intuitively, at each step we adjust the weight on skill ibased on the losses of skills that iinfluences, with the assumption
that more training data helps decrease loss. Note that when we use our algorithm with a complete graph or empty graph, we
achieve expected behavior discussed in Section 2.1. For the complete graph, our algorithm reduces to stratified sampling.
When we have a skill set with an empty graph, the update rule reduces to sampling proportional to each skill’s validation
loss.
4 Experimental results
Given an ordered skill set, we aim to validate SKILL -IT’s ability to select data for efficiently learning skills in the continual
pre-training, fine-tuning, and out-of-domain settings. We provide full tables of results in Appendix D.3.1 and results where
we learn the skills graph on the 125M model and use it for the 1.3B parameter model in Appendix D.4. Skills graphs are in
Appendix C.2, weight trajectories for SKILL -ITare in Appendix D.3.2, and ablations on the graph and online components of
SKILL -ITare in Appendix D.5.
4.1 Continual pre-training
Setup We evaluate the ability of SKILL -ITto select data for efficiently learning over all skills. We measure average
validation loss per skill after a fixed number of training steps. We construct the LEGO synthetic and addition synthetic with
k= 5and3, respectively, and an imbalanced dataset over the skills. On the Natural Instructions dataset, we use 23of the
task categories as skills.
Baselines We compare SKILL -ITagainst three baselines that do not account for skills: random sampling, curriculum
learning, and anticurriculum learning. Random sampling is a standard procedure for selecting samples given no additional
information. Curriculum learning [ 3] and anticurriculum learning [ 67] score the samples from easiest to hardest and vice
versa, respectively, and sample over an expanding set of the lowest scored samples at every epoch; we use the pre-trained
7

--- PAGE 8 ---
0 2000 4000 6000104
103
102
101
100Validation Loss (Log)LEGO Skill 1
0 2000 4000 6000102
101
100 LEGO Skill 2
0 2000 4000 6000101
LEGO Skill 3
0 2000 4000 6000
Steps102
101
Validation Loss (Log)LEGO Skill 4
0 2000 4000 6000
Steps101
LEGO Skill 5
0 2000 4000 6000
Steps102
101
Average per skill
Random
Curriculum
Anticurriculum
Skill-stratified
Skill-It
0 2000 4000 6000102
101
100Validation Loss (Log)Addition Skill 1
0 2000 4000 6000102
101
100Addition Skill 2
0 2000 4000 6000
Steps102
101
100Validation Loss (Log)Addition Skill 3
0 2000 4000 6000
Steps102
101
100Average per skill
Random
Curriculum
Anticurriculum
Skill-stratified
Skill-ItFigure 4: Performance of SKILL -ITon each skill in the continual pre-training setting (learning over all skills in the ordered
training skill set) on the LEGO synthetic (left) and addition synthetic (right).
model’s loss to rank points. We evaluate skill-stratified sampling, which uses knowledge of the skills but is not online, and
include an additional skills curriculum baseline in Appendix D.3.1
Analysis Our results are shown in Figure 4. Across our experiments we find that SKILL -IToutperforms baselines that
do not use skills as well as skill-stratified sampling. On the LEGO dataset, all three baselines that do not utilize a notion
of skills exhibit plateauing loss on four of the skills. Both skill-stratified sampling and SKILL -ITare able to significantly
reduce loss on all skills, but the former is slower. Halfway through training, SKILL -ITexhibits an accuracy improvement
between 9.9and25.9points over other approaches, reaching a final accuracy of 99.4(Figure 19). SKILL -IToutperforms
skill-stratified sampling by initially allocating more weight to prerequisite skills and eventually allocating more weight to
skills that are learned more slowly (Figure 20). On the addition synthetic with k= 3,SKILL -ITconverges to near-zero
validation loss faster than the baselines on skills 1 and 2. While the random baseline may seem competitive at first glance,
it fails to learn skill 1 (adding together the ones digits), which hurts its average loss per skill. On NI, the validation loss
from SKILL -ITis3.2%lower than from random sampling (Table 7). Our results suggest that exploiting the construction and
ordering of skills is critical to learning skills quickly.
4.2 Fine-tuning
0 2000 4000 6000
Steps0.000.250.500.75Validation LossPerformance on Skill 3
Skill 3 only
Skill-stratified
Skill-It
0 2000 4000 6000
Steps012Validation LossPerformance on Skill 1
Skill 1 only
Skill stratified
Skill-It
200 400 600
Steps2.32.42.52.6Validation Loss
Performance on Spanish QG
Spanish QG only
Skill-stratified
Skill-It
200 400 600
Steps1.21.41.61.8Validation Loss
Performance on stance detection
Stance detection only
Skill-stratified
Skill-It
Figure 5: Performance of SKILL -ITin the fine-tuning setting (learning a target skill using the ordered training skill set) on
LEGO, addition, and NI.
Setup We evaluate the ability of SKILL -ITto select data from an ordered training skill set for learning a target skill.
Mirroring Figure 3, we evaluate on LEGO target skill 3 (third in reasoning chain), on the addition synthetic’s skill 1 (ones
place digit addition), and on NI’s Spanish QG and Stance Detection.
Baselines We compare SKILL -ITagainst training on the target skill only and skill-stratified sampling over prerequisite
skills and the target skill. The skill-stratified sampling approach uses the ordered skill set to identify prerequisite skills, but
does not exploit them dynamically.
Analysis Our results are shown in Figure 5. On LEGO, SKILL -ITresults in the same validation loss of 0.01as training only
on the target skill in 38.1%fewer steps. We observe a similar trend on addition, with SKILL -ITconverging to a validation
loss of 0.01in59% fewer steps required to do so when training only on the target skill. Finally, on NI, SKILL -ITimproves
validation loss on Spanish question generation by 5.3%and Stance Detection by 13.6%over just training on the respective
8

--- PAGE 9 ---
3.063.083.10Validation Loss
Answerability Classification
2.082.102.122.14
Cause Effect Classification
3.103.123.143.16
Coreference Resolution
2.362.372.38
Data To Text
Random
Skill-stratified
Skill-It
2.322.342.362.38Validation Loss
Dialogue Act Recognition
2.402.422.44
Grammar Error Correction
2.742.762.78
Keyword Tagging
2.742.762.782.802.82
Overlap Extraction
1000 2000 3000 4000 5000
Steps2.602.62Validation Loss
Question Rewriting
1000 2000 3000 4000 5000
Steps2.482.502.522.54
Textual Entailment
1000 2000 3000 4000 5000
Steps3.023.033.043.05
Title Generation
1000 2000 3000 4000 5000
Steps1.671.681.691.70
Word AnalogyFigure 6: Performance of SKILL -ITin the out-of-domain setting for the NI test task split. SKILL -ITuses the graph between
the train and evaluation skills to produce an online mixture on the training dataset.
0 1 2 3
Number of tokens (billion)0.6380.6400.6420.644Accuracy
 Uniform
Skill-It
RedPajama source SKILL -ITmixture
ArXiv 0.1370
Books 0.0437
C4 0.4195
CommonCrawl 0.0732
GitHub 0.189
StackExchange 0.0892
Wikipedia 0.0484
Figure 7: Left: Accuracy on LM Evaluation Harness for continual pre-training of a 3B parameter model using SKILL -ITon
the RedPajama dataset. We achieve higher accuracy at 1B additional tokens than uniform at 3B tokens. Right: SKILL -IT
mixture over RedPajama sources.
target skill only. In this setting, a significant portion of the improvement over training only on the target skill comes from
identification of prerequisite skills through the learned graph in the skill-stratified sampling method. SKILL -ITis further
able to improve performance with finer-grained dynamic weighting on prerequisite skills.
4.3 Out-of-domain setting
Natural Instructions We evaluate the ability of SKILL -ITto select data from a set of training skills for learning a disjoint
set of evaluation skills that we cannot train on. We use all 59task categories in the NI train tasks split as the training skills and
the12task categories in the test tasks split as our evaluation skills. We compare SKILL -ITagainst random and skill-stratified
sampling, both of which do not exploit the relationships between training skills and evaluation skills. SKILL -ITachieves the
lowest loss on 11 out of 12 task categories over random and skill-stratified sampling (Figure 6, tables in Appendix).
RedPajama We use SKILL -ITto produce a data mixture on the RedPajama dataset. The training skills are the data sources
comprising the dataset, and the evaluation skills are several tasks from the Language Model Evaluation Harness [ 14].
SKILL -ITwithT= 1(i.e. a static, graph-based mixture) yields the mixture in Figure 7 (right). We continually pre-train
a 3B parameter model trained on one trillion tokens for three billion additional tokens using this mixture, and see that it
outperforms uniform sampling over the data sources (Figure 7 left). In particular, S KILL -ITachieves higher accuracy with
1B additional tokens than uniform with 3B additional tokens.
5 Related work
Data selection for LMs There have been several studies of large-scale data selection for LMs. Data deduplication [ 1,
22,32], in which identical or nearly identical samples are removed, is a method that enables LMs to be trained on smaller,
cleaned datasets and has been increasingly used as a pre-processing step for training data [ 4,59,71]. Other methods applied
at scale involve ensuring high quality of data by explicitly filtering out samples or comparing the training dataset with a
cleaned reference dataset [ 7,31,59]. Importance reweighting approaches have also been proposed for identifying training
data from a large corpus that best approximates a smaller target distribution [ 69], and influence functions have been used
to select a subset of training data to improve performance on downstream tasks [ 61]. These approaches can identify data
9

--- PAGE 10 ---
pertaining to a particular target distribution or filter out low quality data according to some heuristic, while our work aims to
understand how the choice of data is related to the numerous skills that LMs learn.
Recent development of LMs has shifted focus from emphasizing the scale of the model to prioritizing the training data
utilized. For example, models like Alpaca [ 56], Vicuna [ 9], and Koala [ 15] are all based on the LLaMA model combined
with instruction data generated by an existing LM. Palm 2’s technical report states that the data mixture was a critical
component of the final model [ 17], and Mosaic ML’s recent MPT model was trained on a hand-engineered mixture of the
RedPajama dataset [ 42]. However, these works lack rigorous explanation for why their training datasets were constructed in
this way.
Finally, perhaps most related to our approach is the contemporary work DoReMi [ 68], which uses group distributionally
robust optimization on a smaller LM to select data source mixtures for training a larger LM. Their approach focuses on
selecting data at the data source level for optimizing worst-case performance across the training data sources, rather than at
the more general skills level for a variety of target skill sets. Furthermore, we focus on understanding how skills are related
to each other and induce some order in how LMs learn by explicitly modeling skill graph structure, which we find to be
important for data-efficient LM training (see ablations in Appendix D.5).
Data selection methods Many data selection methods have been proposed for supervised, task-specific settings. In this
setting, the most typical objective is dataset condensation, which aims to identify a small subset of data that captures the
larger dataset’s properties with respect to the model. Some approaches include constructing coresets [ 30,47], identifying
samples that the model forgets during training [ 58]; identifying samples with the largest gradients [ 46] or gradients that
approximate the overall gradient [ 39]; clustering in embedding space and selecting points farthest from cluster centers [ 53];
and selecting samples with the highest uncertainty or entropy [ 33]. These approaches have also been shown to transfer from
smaller models to larger models [ 10]. Unlike these methods, we study how to select data for learning one or many skills at
the mixture level for LMs instead of the instance level.
Another area of interest is data selection for domain adaptation and multitask learning. For domain adaptation, there
are a wide range of methods that select data to best match the target distribution. For example, the Moore-Lewis method
matches data based on the difference in cross-entropy using a model trained on the target versus a model trained on the
source data [ 41]. Several other approaches suggest training a model to distinguish between source and target and selecting
points with high uncertainty [ 50], or selecting points based on some divergence in an embedding space [ 51]. In comparison
to these approaches, our work focuses on learning one or many skills and also finds that embedding-based heuristics do not
fully identify skills.
Data attribution Another perspective on understanding training data is data attribution, which seeks to identify what data
is responsible for particular model behaviors. Influence functions [ 28] and shapley values [ 16] are two ways to quantify
the role of individual samples. Datamodels [ 23] fit a model to predict behavior given a subset of training data, providing a
framework for understanding individual samples as well as dataset counterfactuals. Simfluence [ 20] fits a Markov process to
a set of training trajectories for finer-grained understanding of how data impacts training. We focus on understanding how
groups of data associated with skills elicit broader model capabilities, and utilize this understanding to select data for more
efficient training.
Curriculum learning Curriculum learning [ 3] proposes to show the model data in order from easy samples to hard
ones. Various criteria have been used to determine hardness, and anticurriculum as well as various pacing functions and
mixing rates have been explored [ 54]. Curriculum learning can also be performed at the group level [ 60]. More sophisticated
approaches include parametrizing each sample with a dynamic importance [ 52], and also accounting for irrelevant and noisy
data [ 38]. Our approach similarly utilizes a curriculum, but it is defined over a skills graph and does not necessarily align
with training on easiest to hardest skills.
How LMs learn Many different explanations for how LMs learn from data have been proposed. One hypothesis is that
there exist discrete, universal building blocks of LM knowledge called quanta, and power law scaling emerges from a
learning over a particular distribution of quanta in the right order [ 37]. Another is that chain of thought reasoning emerges
due to local clusters of latent variables that influence each other, which can be validated by studying the LM’s ability to do
conditional inference given intermediate variables [ 48]. Others have provided theoretical analysis of how transformers learn
topics by studying co-occurrences of words in the training data [ 34]. Empirically, how models learn is still a mystery—for
instance, models trained on code are found to perform fairly well at commensense reasoning [ 36]. Our work initiates a study
on how LMs learn various skills and how to exploit this for better data selection.
10

--- PAGE 11 ---
Task selection In multitask auxiliary learning, the goal is to train a model to perform well on a target task(s) by selecting
the most beneficial source tasks to train on. One can use feature similarity to select tasks [ 29], but we find in our synthetics
that feature similarity does not always recover skills. In Taskonomy [ 70], a hypergraph over a set of tasks is learned and
used to select tasks. The methods used to develop the taxonomy can be applied to further expand our graph learning (e.g.,
studying transitive and higher-order properties). However, their focus is on task selection in computer vision rather than data
selection for LMs to learn skills. Lastly, the contemporary work of TaskWeb [ 24] builds a graph among 22common NLP
tasks in order to determine what the best source tasks are for a target task. Their definition of an edge in the task graph is
less strict than ours (their comparison is on if training on additional data from sihelps with sj, while we fix the overall
amount of training data over both siandsj). Overall, our approach is similar in use of the skills graph, but we incorporate it
into a dynamic sampling algorithm. Furthermore, we look more broadly at skills, rather than tasks, and characterize when
we expect using the skills graph to improve model performance.
Education The notion of skill has been studied in education. Classical research on learning hierarchies [ 66] identify sets of
skills that make up subordinate capabilities for students. For instance, [ 12] identified that in order for students to solve linear
equations, there were many prerequisite skills, ranging from the simplest being symbol recognition to the most complex
being the ability to add, subtract, multiple, and divide from both sides of the equation. More recently, decision-making over
lesson sequences based on skills, e.g., what the student already knows versus what the lesson teaches, has become an area of
interest in personalized learning [49].
6 Conclusion
Given a fixed budget of data, knowing what data to train on to induce various capabilities in an LM is challenging. As
LMs continue to improve, it will become increasingly important to extract as much signal as possible from the data and to
direct that signal towards acquiring a broad variety of capabilities. In this paper, we introduce a skills-based framework for
understanding how LMs learn and for selecting training data. We hope our study invites others to build on such a notion of
skill and further explore how to align skills with data.
Acknowledgements
We thank Together Computer ( https://together.xyz/ ) for providing portions of the compute used to train models
in this paper. We thank Sabri Eyuboglu, Karan Goel, Arjun Desai, Neel Guha, Michael Zhang, Vishnu Sarrukai, Simran
Arora, Ben Spector, Brandon Yang, Gautam Machiraju, and Sang Michael Xie for their helpful feedback and discussion.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315
(Beyond Sparsity), CCF1563078 (V olume to Velocity), and 1937301 (RTML); US DEVCOM ARL under No. W911NF-21-
2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-
1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP,
Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm,
Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data
Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. FS is supported
by NSF CCF2106707 and the Wisconsin Alumni Research Foundation (WARF). The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions,
findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily
reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.
11

--- PAGE 12 ---
References
[1]Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-efficient learning
at web-scale through semantic deduplication, 2023.
[2]Yuntao Bai, Andy Jones, et al. Training a helpful and harmless assistant with reinforcement learning from human
feedback, 2022.
[3]Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the
26th annual international conference on machine learning , pages 41–48, 2009.
[4]Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah
Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.
[5]Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language
Modeling with Mesh-Tensorflow, March 2021. If you use this software, please cite it using these metadata.
[6] Rishi Bommasani, Percy Liang, et al. On the opportunities and risks of foundation models, 2021.
[7]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 , 2020.
[8] Mark Chen, Jerry Tworek, et al. Evaluating large language models trained on code, 2021.
[9]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,
Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, March 2023.
[10] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure
Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning, 2019.
[11] Robert M Gagne. The acquisition of knowledge. Psychological review , 69(4):355, 1962.
[12] Robert M Gagne and Noel E Paradise. Abilities and learning sets in knowledge acquisition. Psychological Monographs:
General and Applied , 75(14):1, 1961.
[13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,
Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint
arXiv:2101.00027 , 2020.
[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu,
Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
and Andy Zou. A framework for few-shot language model evaluation, September 2021.
[15] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A
dialogue model for academic research. Blog post, April 2023.
[16] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In International
Conference on Machine Learning , pages 2242–2251. PMLR, 2019.
[17] Google. Palm2 technical report. Technical report, 2023.
[18] Anupam Gupta. Advanced algorithms: Notes for cmu 15-850 (fall 2020), 2020.
[19] Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith.
Don’t stop pretraining: Adapt language models to domains and tasks. Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , 2020.
[20] Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the
influence of individual training examples by simulating training runs, 2023.
[21] Peter Henderson*, Mark S. Krass*, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E.
Ho. Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset, 2022.
12

--- PAGE 13 ---
[22] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac
Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario
Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. Scaling laws and interpretability of learning from
repeated data, 2022.
[23] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting
predictions from training data, 2022.
[24] Joongwon Kim, Akari Asai, Gabriel Ilharco, and Hannaneh Hajishirzi. Taskweb: Selecting better source tasks for
multi-task nlp, 2023.
[25] Hannah Rose Kirk, Yennie Jun, Filippo V olpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski,
and Yuki Asano. Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative
language models. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances
in Neural Information Processing Systems , volume 34, pages 2611–2624. Curran Associates, Inc., 2021.
[26] Nikita Kitaev, Steven Cao, and Dan Klein. Multilingual constituency parsing with self-attention and pre-training. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3499–3505, Florence,
Italy, July 2019. Association for Computational Linguistics.
[27] Nikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2676–2686, Melbourne,
Australia, July 2018. Association for Computational Linguistics.
[28] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions, 2017.
[29] Po-Nien Kung, Sheng-Siang Yin, Yi-Cheng Chen, Tse-Hsuan Yang, and Yun-Nung Chen. Efficient multi-task auxiliary
learning: Selecting auxiliary data by feature similarity. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 416–428, Online and Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics.
[30] Michael Langberg and Leonard J. Schulman. Universal approximators for integrals , pages 598–607.
[31] Hugo Laurençon, Lucile Saulnier, et al. The bigscience roots corpus: A 1.6tb composite multilingual dataset, 2023.
[32] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas
Carlini. Deduplicating training data makes language models better. Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) , 2022.
[33] David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. In Acm Sigir
Forum , volume 29, pages 13–19. ACM New York, NY , USA, 1995.
[34] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic
understanding, 2023.
[35] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and mitigating
social biases in language models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 6565–6576.
PMLR, 18–24 Jul 2021.
[36] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot
commonsense learners, 2022.
[37] Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural scaling, 2023.
[38] Sören Mindermann, Muhammed Razzak, Winnie Xu, Andreas Kirsch, Mrinank Sharma, Adrien Morisot, Aidan N.
Gomez, Sebastian Farquhar, Jan Brauner, and Yarin Gal. Prioritized training on points that are learnable, worth
learning, and not yet learned (workshop version), 2021.
[39] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine learning
models, 2019.
[40] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural
language crowdsourcing instructions. In ACL, 2022.
13

--- PAGE 14 ---
[41] Robert C. Moore and William Lewis. Intelligent selection of language model training data. In Proceedings of the
ACL 2010 Conference Short Papers , pages 220–224, Uppsala, Sweden, July 2010. Association for Computational
Linguistics.
[42] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
[43] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models.
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers) , 2021.
[44] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via
mechanistic interpretability. In The Eleventh International Conference on Learning Representations , 2023.
[45] Arkadij Semenovi ˇc Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimiza-
tion. 1983.
[46] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding important
examples early in training, 2021.
[47] Jeff M. Phillips. Coresets and sketches, 2016.
[48] Ben Prystawski and Noah D. Goodman. Why think step-by-step? reasoning emerges from the locality of experience,
2023.
[49] Siddharth Reddy, Igor Labutov, and Thorsten Joachims. Latent skill embedding for personalized lesson sequence
recommendation, 2016.
[50] Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. Data selection strategies for multi-domain sentiment analysis,
2017.
[51] Sebastian Ruder and Barbara Plank. Learning to select data for transfer learning with bayesian optimization. Proceed-
ings of the 2017 Conference on Empirical Methods in Natural Language Processing , 2017.
[52] Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste. Data parameters: A new family of parameters for learning a
differentiable curriculum. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett,
editors, Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
[53] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws:
beating power law scaling via data pruning, 2022.
[54] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey. International Journal
of Computer Vision , 130(6):1526–1565, Apr 2022.
[55] Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, and Han van der Maas. Putting gpt-3’s creativity to the
(alternative uses) test, 2022.
[56] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/
stanford_alpaca , 2023.
[57] Together. Redpajama-data: An open source recipe to reproduce llama training dataset, 2023.
[58] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J.
Gordon. An empirical study of example forgetting during deep neural network learning, 2018.
[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. Llama: Open and efficient foundation language models, 2023.
[60] Neeraj Varshney, Swaroop Mishra, and Chitta Baral. Let the model decide its curriculum for multitask learning, 2022.
[61] Xiao Wang, Weikang Zhou, Qi Zhang, Jie Zhou, Songyang Gao, Junzhe Wang, Menghan Zhang, Xiang Gao, Yunwen
Chen, and Tao Gui. Farewell to aimless large-scale pretraining: Influential subset selection for language model, 2023.
[62] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
Self-instruct: Aligning language model with self generated instructions, 2022.
14

--- PAGE 15 ---
[63] Yizhong Wang, Swaroop Mishra, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+
nlp tasks, 2022.
[64] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and
Quoc V . Le. Finetuned language models are zero-shot learners, 2021.
[65] Richard T White. Research into learning hierarchies. Review of Educational Research , 43(3):361–375, 1973.
[66] Richard T. White and Robert M. Gagné. Past and future research on learning hierarchies. Educational Psychologist ,
11(1):19–28, 1974.
[67] Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work?, 2020.
[68] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V . Le, Tengyu Ma,
and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining, 2023.
[69] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance
resampling, 2023.
[70] Amir R. Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling task transfer learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , Jun
2018.
[71] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona
Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
[72] Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers
with lego: a synthetic reasoning task, 2022.
15

--- PAGE 16 ---
A Broader Impacts and Limitations
Broader Impacts As more LMs are developed, a key criteria for their adoption and utility is if they exhibit a wide array
of useful capabilities, such as generating harmless content, summarizing essays, and being conversational with the user.
While improvements in other parts of the LM development pipeline such as training and architecture are important, many
recent advances in building LMs with a wide array of useful capabilities have come from the data itself [ 9,15,17,42,56].
Our work is fundamental in investigating how LMs learn and how to select data to learn skills more efficiently. However,
we recognize that data selection methods can always be utilized to optimize for particular skills that may be considered
malicious or negatively target or exclude specific groups [ 2]. Furthermore, pre-trained LMs have been found to have various
biases [6, 25, 35, 43].
Limitations The skills graph can either be provided (e.g., using a knowledge graph) or learned. Our work learns the skills
graph using Algorithm 2 or Algorithm 3, which requires initial training runs on pairs of skills or each skill, respectively.
This can be made more efficient by performing these training runs on a smaller model and for fewer number of steps,
but tradeoffs here have yet to be thoroughly investigated. SKILL -ITalso assumes that the ordered skill set is provided; as
discussed in sections 2.1 and 2.3, it is challenging to recover ordered skill sets simply via metadata attributes or embedding
clustering. Otherwise, the best way to sample over collections of skills that form a complete or empty graph is random or
stratified sampling with no ordering to exploit. Our loss-based clustering approach presented in section 2.3 demonstrates
that grouping by losses can provide an explanation for how skills are defined over data. An important direction for future
work is to use such a clustering approach or other unsupervised algorithms in an end-to-end pipeline for skill discovery, skill
graph learning, and data selection based on such skills.
B Additional Algorithmic Details
B.1 Derivation of S KILL -ITUpdate Rule
First, we provide the derivation of our update rule from online mirror descent using the proximal point view [ 18]. We restate
our optimization problem from (3):
minimize
p1,...,pT∈∆k−11
mmX
j=1Leval,j(fT) (5)
s.tLeval,j(ft) =Leval,j(ft−1)(1−αA⊤
:,jpt−1)∀j∈[m], t= 1, . . . , T
ft= Φ(ft−1, pt−1)∀t= 1, . . . T
Let¯Lt(p) =1
mPm
i=jLeval,j(ft+1) =1
mPm
i=jLeval,j(Φ(ft, p)); that is, pis the mixture we must choose at time tand¯Lt
is the average loss per skill of the model after it is trained on pat round t. A greedy approximation of (5)isminimize
p∈∆k−1¯Lt(p),
given the model and mixtures at previous rounds. A linear approximation of ¯Lt(p)is
¯Lt(p)≈¯Lt(pt−1) +⟨▽¯Lt−1(pt−1), p−pt−1⟩ (6)
Then, the problem of minimizing ¯Lt(p)can be approximated as
argminp∈∆k−1⟨η▽¯Lt−1(pt−1), p⟩ (7)
after we drop terms from (6)that do not depend on p. Note that the ηis a constant and does not impact the solution. The
optimal solution to this problem is selecting the pthat has the most weight on the slice with the largest gradient (e.g., a
folow-the-leader sort of algorithm). To improve stability and prevent overfitting, we introduce regularization via a Bregman
divergence Dh(p||pt−1) =h(p)−h(pt−1)−⟨▽h(pt−1), p−pt−1⟩. After dropping terms that do not contain p, our problem
is now
argminp∈∆k−1⟨η▽¯Lt−1(pt−1), p⟩+h(p)− ⟨▽h(pt−1), p⟩ (8)
Taking the gradient and setting it equal to 0gives us
η▽¯Lt−1(pt−1) +▽h(p)−▽h(pt−1) = 0 (9)
16

--- PAGE 17 ---
Algorithm 2: LEARN GRAPH (Brute-Force)
1:Input: Ordered skill set S={s1, . . . , s k}. Number of training steps H, base model f.
2:forj∈[k]do
3: Train fon samples from XsjforHsteps and denote fH,jto be the model after training.
4: Observe change in loss, δj
j=Leval,j(f)−Leval,j(fH,j).
5:end for
6:fori, j∈[k]do
7: Train fon samples from Xsi∪ XsjforHsteps and denote fH,i,j to be the model after training.
8: Observe change in loss, δi,j
j=Leval,j(f)−Leval,j(fH,i,j).
9: ifδij
j> δj
jthen
10: Draw edge si→sjand set Aij>0.
11: end if
12:end for
13:Return Adjacency matrix A∈Rk×k
Algorithm 3: LEARN GRAPH (Approximate)
1:Input: Ordered skill sets StrainandSeval. Number of training steps H, base model f.
2:fori∈[k]do
3: Train fon samples from Xstrain,iforHsteps and denote fH,ito be the model after training.
4: forj∈[m]do
5: Observe change in loss, δi
j=Leval,j(f)−Leval,j(fH,i).
6: Ifδi
j>0, draw edge strain,i→seval,jand set Aij>0.
7: end for
8:end for
9:Return Bipartite adjacency submatrix A∈Rk×m
Similar to in standard multiplicative weights, we set h(p) =P
ipilnpiand▽h(p) = [ln pi+ 1]i. Then,
lnpi= lnpi
t−1−η▽iLt−1(pt−1)
⇒pi
t+1=pi
texp(−η▽i¯Lt(pt)) (10)
where▽iis the ith element of the gradient. Now we wish to compute ▽i¯Lt(pt) =1
mPm
j=1▽i[Leval,j(ft+1)] =
1
mPm
j=1▽i[Leval,j(Φ(ft, pt))]. Recall the dynamics model for Leval:
Leval,j(ft+1) =Leval,j(ft)(1−A⊤
:,jpt), (11)
The gradient of this model with respect to each training skill siis
▽iLeval,j(ft+1) =−AijLeval,j(ft) (12)
⇒▽i¯Lt(pt) =1
mmX
j=1−AijLeval,j(ft)
Plugging this back into (10),
pi
t+1=pi
texp
ηmX
j=1AijLeval,j(ft)
, (13)
where we can absorb the1
mintoη.
B.2 Graph Learning Method
We provide algorithms for learning the graph over an ordered skill set. In Algorithm 2, we discuss the brute-force
approach for learning the adjacency matrix. This approach only works when Seval⊆ S train(e.g. pre-training and fine-tuning
17

--- PAGE 18 ---
cases), so we denote S=Strainin the algorithm box. In Algorithm 3, we discuss the linear approach for learning the
adjacency matrix. This approach works even in the out-of-domain case when SevalandStrainare disjoint.
In both approaches, the exact value of Aijcan vary, but we can typically set it proportional to δi,j
j−δj
j, the difference
between the changes in loss, in the brute-force case or δi
j, the change in loss itself, in the approximate case. The exact
constructions and methods for learning each Ain our experiments are in Appendix C.2.
C Additional Experimental Details
C.1 Datasets
We present details about each dataset used, including information on the skills and the validation dataset. A summary is
presented in Table 2.
Dataset Skill #skills Validation data
Alpaca Instruction type 38 50samples per skill
Pile of Law Legal data source 31 645samples per skill
LEGO Reasoning chain depth 5 100samples per skill
Addition Digit 3 100samples per skill
NI (pre-training) Task category 23 50samples per task
NI (Spanish QG) Task category ×language 4 100samples per task
NI (stance detection) Task category 2 50samples per task
NI (out-of-domain) Task category 59,12 400samples per task
RedPajama Data source 7 LM eval harness
Table 2: We list each dataset used as well as its corresponding skill. We include the number of skills in the training dataset,
as well as details on how the validation dataset is constructed.
•Alpaca dataset [ 56]: the Alpaca dataset consists of 52K instruction examples that were generated from text-davinci-003.
We applied the Berkeley Neural Parser [ 26,27] to each instruction, keeping 40777 samples it was able to parse successfully.
If the sample began with a question, we annotated it with the skill “question”, and otherwise we annotated it with the verb
identified from the parser. We grouped the data into a total of 38skills, such as "list", "edit", "calculate", "describe" and
"identify".
•Pile of Law [ 21]: the Pile of Law dataset consists of various sources of legal and administrative data, ranging from tax
rulings to the world’s constitutions. We evaluate on a subset of the Pile of Law validation dataset consisting of 13883
samples, where we selected max( 645, source size) samples per source. We truncated each sample to be no more than
100K characters.
•LEGO [ 72]: for the LEGO synthetic, we set k= 5and sample 192000 points across the skills. Our validation dataset
consisted of 100samples per skill.
•Addition: for the 3-digit addition synthetic, we set k= 3and sample 192000 points across the skills. We use a validation
dataset of 100samples per skill.
•Natural Instructions [ 40,63]: the Natural Instructions dataset is a large collection of tasks and their definitions in natural
language. For the pre-training setting, we used a set of 23task categories that had the largest degree (in-degree + out-
degree) in the learned skills graph, for a total of 1,232,437samples and 425tasks to select from. We evaluated on 50
samples per task.
For the fine-tuning setting with Spanish question generation, we select data over 4skills (Spanish question generation,
Spanish question answering, English question generation, English question answering) for a total of 513210 samples and
212tasks to select from. We evaluated on 100samples per task.
For the fine-tuning setting with stance detection, we select data over 2skills (stance detection, text matching) for a total of
50990 samples and 19tasks to select from. We evaluated on 50samples per task.
For the out-of-domain setting, we select data over all 59task categories for a total of 2,417,867samples and 753tasks to
select from. The test split consisted of 12task categories and 119tasks, and we evaluated on min( 400, task size) samples
per task.
•RedPajama [ 57]: the RedPajama dataset is a 1-trillion token dataset that aims to reproduce the LLaMA [ 59] training
dataset. We select over the 7data sources and evaluate using the LM evaluation harness [14].
18

--- PAGE 19 ---
addanalyzearrangecalculatecategorizechangechooseclassifycomparecomposeconstructconvertcreatedescribedesigndeterminedevelopeditevaluateexplainfindgenerategiveidentifylistmakenameoutlineprovidequestionrewriteselectsuggestsummarizetaketelltranslatewriteaddanalyzearrangecalculatecategorizechangechooseclassifycomparecomposeconstructconvertcreatedescribedesigndeterminedevelopeditevaluateexplainfindgenerategiveidentifylistmakenameoutlineprovidequestionrewriteselectsuggestsummarizetaketelltranslatewriteAlpaca
0.000.250.500.751.001.251.501.752.00Figure 8: Alpaca heatmap where i, jth entry is max(0 , δi
j)(the change in loss on sjafter training on sifor150steps).
Diagonal entries are set to 0for clearer visualization.
C.2 Graph Learning Details
We describe how the skills graph was learned on each dataset.
•Alpaca (Figure 8): we use Algorithm 3 and train for K= 150 steps per skill. Each edge i→jhas a weight of δi
j, the
difference in loss on skill jbefore and after training on i. Next, we compare the average validation loss of skill-stratified
sampling versus random sampling when we train for K= 1000 steps. We find that skill-stratified sampling only does
0.007 better than random sampling, confirming that Alpaca’s dense skills graph suggests that random sampling is the best
we can do.
•Pile of Law (Figure 9): we use Algorithm 3 and train for K= 150 steps. Each edge i→jhas a weight of δi
j, the
difference in loss on skill jbefore and after training on i.
•LEGO (Figure 10): we use both Algorithm 2 and Algorithm 3 and train for K= 6000 steps each. Each edge i→jhas a
weight of 0.5if the amount of data associated with skill jthat is needed to reach 0.01validation loss is less when training
on(i, j)than on j(edges are set to 0if0.01validation loss is not reached, even if loss is decreasing). Each edge i→jis
also set to 0.5if training on idecreases loss directly on j. We set each diagonal entry of Ato be 1.
•Addition (Figure 11): we use Algorithm 2 and train for K= 6000 steps. Each edge i→jhas a weight of 0.5if the
amount of data associated with skill jthat is needed to reach 0.01validation loss is less when training on (i, j)than on j
(edges are set to 0if0.01validation loss is not reached, even if loss is decreasing). We set each diagonal entry of Ato be 1.
•Natural Instructions (Figure 12, 13, 14): we use Algorithm 3. For the pre-training setting, we train for K= 600 steps
and assign each edge i→ja weight δi
jequal to the change in loss on jin the first 100steps for all i, j∈[k], including
diagonal entries. For the fine-tuning setting, we train for K= 600 steps and assign each edge i→ja weight δi
jequal to
the change in loss before and after training. For the out-of-domain setting, we train for K= 600 steps and assign each
edgei→ja weight δi
jequal to the change in loss before and after training in the first 100steps.
•RedPajama (Figure 15): we use Algorithm 3 and train for 1billion tokens per data source. We assign each edge i→ja
weight δi
jequal to the change in perplexity on the validation datalsoa before and after training.
C.3 Training Details
We describe the parameters used for S KILL -IT.
SKILL -ITpre-training
19

--- PAGE 20 ---
atticus_contracts
bva_opinions
canadian_decisions
cfpb_creditcard_contracts
congressional_hearings
courtlistener_docket_entry_documents
courtlistener_opinions
dol_ecab
echr
edgar
eoir
eurlex
euro_parl
federal_register
founding_docs
hhs_alj_opinions
icj-pcij
medicaid_policy_guidance
nlrb_decisions
oig
olc_memos
r_legaladvice
resource_contracts
scotus_filings
scotus_oral_arguments
sec_administrative_proceedings
tax_rulings
un_debates
us_bills
uspto_office_actions
doj_guidance_documents
ed_policy_guidance
ftc_advisory_opinionsatticus_contracts
bva_opinions
canadian_decisions
cfpb_creditcard_contracts
congressional_hearings
courtlistener_docket_entry_documents
courtlistener_opinions
dol_ecab
echr
edgar
eoir
eurlex
euro_parl
federal_register
founding_docs
hhs_alj_opinions
icj-pcij
medicaid_policy_guidance
nlrb_decisions
oig
olc_memos
r_legaladvice
resource_contracts
scotus_filings
scotus_oral_arguments
sec_administrative_proceedings
tax_rulings
un_debates
us_bills
uspto_office_actionsPile of Law
0.00.20.40.60.81.01.21.41.6Figure 9: Pile of Law heatmap where i, jth entry is max(0 , δi
j)(the change in loss on sjafter training on sifor150steps).
Diagonal entries are set to 0for clearer visualization.
20

--- PAGE 21 ---
1 2 3 4 512345LEGO
0.00.20.40.60.81.0Figure 10: LEGO heatmap with k= 5where i, jth entry is set to 0.5if the number of steps needed to reach 0.01loss on
skilljwhen training on a balanced mixture of skills iandjis less than when training on skill jonly.
1 2 31 2 3Addition
0.00.20.40.60.81.0
Figure 11: Addition heatmap with k= 3where i, jth entry is set to 0.5if the number of steps needed to reach 0.01loss on
skilljwhen training on a balanced mixture of skills iandjis less than when training on skill jonly.
21

--- PAGE 22 ---
answer_verification
code_to_text
discourse_connective_identification
entity_generation
entity_relation_classification
information_extraction
irony_detection
preposition_prediction
punctuation_error_detection
question_answering
question_generation
question_understanding
sentence_expansion
sentiment_analysis
stance_detection
summarization
text_categorization
text_matching
text_simplification
text_to_code
toxic_language_detection
word_semantics
wrong_candidate_generationanswer_verification
code_to_text
discourse_connective_identification
entity_generation
entity_relation_classification
information_extraction
irony_detection
preposition_prediction
punctuation_error_detection
question_answering
question_generation
question_understanding
sentence_expansion
sentiment_analysis
stance_detection
summarization
text_categorization
text_matching
text_simplification
text_to_code
toxic_language_detection
word_semantics
wrong_candidate_generationNatural Instructions
0.00.20.40.60.8Figure 12: Natural Instructions heatmap where i, jth entry is max(0 , δi
j)(the change in loss on sjafter training on sifor
100steps). Diagonal entries are set to 0for clearer visualization.
English QA
Spanish QA
English QG
Spanish QGEnglish QA
Spanish QA
English QG
Spanish QGSpanish question generation
0.00.20.40.6
Stance Detection
Text MatchingStance Detection
Text MatchingStance detection
0.00.51.0
Figure 13: Spanish question generation and stance detection heatmaps where i, jth entry is max(0 , δi
j)(the change in loss
onsjafter training on sifor100steps).
22

--- PAGE 23 ---
answerability_classification
cause_effect_classification
coreference_resolution
data_to_text
dialogue_act_recognition
grammar_error_correction
keyword_tagging
overlap_extraction
question_rewriting
textual_entailment
title_generation
word_analogyanswer_verification
code_to_text
coherence_classification
commonsense_classification
dialogue_generation
dialogue_state_tracking
discourse_connective_identification
discourse_relation_classification
entity_generation
entity_relation_classification
explanation
fact_verification
fill_in_the_blank
gender_classification
grammar_error_detection
information_extraction
intent_identification
irony_detection
linguistic_probing
mathematics
misc.
named_entity_recognition
negotiation_strategy_detection
number_conversion
paraphrasing
poem_generation
pos_tagging
preposition_prediction
program_execution
punctuation_error_detection
question_answering
question_decomposition
question_generation
question_understanding
sentence_composition
sentence_compression
sentence_expansion
sentence_ordering
sentence_perturbation
sentiment_analysis
spam_classification
speaker_identification
spelling_error_detection
stance_detection
stereotype_detection
story_composition
style_transfer
summarization
text_categorization
text_completion
text_matching
text_quality_evaluation
text_simplification
text_to_code
toxic_language_detection
translation
word_relation_classification
word_semantics
wrong_candidate_generationNatural Instructions
0.000.020.040.060.080.100.120.14Figure 14: Natural Instructions heatmap for out-of-domain setting where rows are for the training skills and columns are for
the evaluation skills. The i, jth entry is max(0 , δi
j)(the change in loss on sjafter training on sifor100steps).
23

--- PAGE 24 ---
arc_challenge
arc_easy
boolq
copa
hellaswag
lambada_openai
piqa
winograndearxiv
books
c4
common_crawl
github
stackexchange
wikipediaRedPajama
0.00.10.2Figure 15: RedPajama heatmap for out-of-domain setting where rows are for the training skills and columns are for the
evaluation skills. The i, jth entry is max(0 , δi
j)(the change in perplexity on sjafter training on sifor1B tokens).
• LEGO: η= 0.5, T= 6, w= 3. We train for 6000 steps.
• Addition: η= 0.1, T= 5, w= 3. We train for 6000 steps.
• Natural Instructions (pre-training): η= 0.2, T= 1. We train for 5000 steps.
For the LEGO random baseline, when we selected points at random, we used an imbalanced training dataset with
proportions 1:1:1:3:5. For the addition random baseline, we used an imbalanced dataset with randomly selected proportions:
13:14:18. For the curriculum learning baselines, the pacing function, g(i), denotes the size of the subset of the highest
scoring samples that we uniformly select from in the ith epoch. We define our pacing function as g(i) =iH
M, where His the
number of steps and Mis5epochs for LEGO and NI, and 3for addition.
SKILL -ITfine-tuning
• LEGO: η= 0.5, T= 10, w= 3. We train for 6000 steps.
• Addition: η= 0.1, T= 5, w= 3. We train for 6000 steps.
• Natural Instructions (Spanish QG): η= 0.8, T= 6, w= 3. We train for 600steps.
• Natural Instructions (stance detection): η= 0.2, T= 6, w= 3. We train for 600steps.
SKILL -ITout-of-domain
• Natural Instructions: η= 0.2, T= 10, w= 3. We train for 5000 steps.
• RedPajama: η= 100 , T= 1. We train for 3billion tokens.
All results are computed over 5random seeds.
Batch sizes of 32and64were used for the LEGO and addition synthetic on the 125M and 1.3B parameter model,
respectively. Batch sizes of 4and16were used for the Natural Instructions experiments on the 125M and 1.3B parameter
model.
For the out-of-domain Natural Instructions experiment and Alpaca graph learning experiments, a learning rate of 5e-6
with linear scheduler and 50warmup steps was used. For the Natural Instructions continual pre-training experiment on the
1.3B parameter model, a learning rate of 1e-6 was used. All other experiments used a learning rate of 5e-5. All experiments
used AdamW with betas = 0.9, 0.999, eps = 1e-8, and weight decay = 0.01. A context window of 512was used for all
experiments except LEGO and addition, which used a window of 128.
Experiments with the Addition dataset were run using an Nvidia RTX A6000. Other experiments using the GPT-Neo
125M parameter model were run on an Nvidia Tesla P100. Experiments using the GPT-Neo 1.3B parameter model were run
on an Nvidia Tesla A100.
24

--- PAGE 25 ---
0 2000 4000 6000
Steps0.40.50.60.70.80.9Validation LossModel performance on LEGO skill 4
Trained on skill 4
Trained on skills 2, 4
0 2000 4000 6000
Steps0.680.700.72Validation LossModel performance on LEGO skill 4
Trained on skill 4
Trained on skills 3, 4Figure 16: Performance on LEGO skill 4 when training on skill 4, skills 2 and 4, and skills 3 and 4. Even though skill 3
and skill 4share an edge in the LEGO synthetic’s underlying reasoning chain (i.e. a model predicting correct for the fourth
variable is one extra step beyond predicting correct for the third variable), we find that training on skills 2and4helps
improve performance on skill 4more.
0 1000 2000 3000
Steps0.00.20.40.6Validation LossModel performance on LEGO skill 3 (tree)
Trained on skill 3
Trained on skill 2
0 1000 2000 3000
Steps0.00.20.40.6Validation LossModel performance on LEGO skill 2 (tree)
Trained on skill 3
Trained on skill 2
Figure 17: Performance on LEGO skill 2and3when training on skills 2and3. The reasoning pattern is a tree rather than
a chain over k= 4variables. Skills 2and3are at the same “depth” in the graph and both depend on skill 1, so there is
positive influence between the skills despite there being no edge between 2and3in the LEGO reasoning graph.
D Additional Experimental Results
D.1 Additional examples of LEGO ordered skill sets
For the LEGO synthetic, it may appear obvious that the skills graph is equivalent to the reasoning chain over the variables.
However, in Figure 16 we see that this is not the case. Training on skills 2and4together results in lower loss on skill 4than
when trained on skill 4alone. However, training on skills 3and4together results in roughly the same loss on skill 4as when
training on skill 4alone, even though skill 3and skill 4share an edge in the LEGO synthetic’s underlying reasoning chain.
This suggests that our intuition for how skills influence each other does not always match how the model learns skills.
Next, we consider a slightly more complex reasoning pattern on the LEGO synthetic. Instead of a chain, we construct a
tree, where two variables in the LEGO synthetic are both defined in terms of the same parent variable. For example,
Input: c = val 1, y = not w, v = val c, w = not c. Output: y = 1.
In this example, k= 4and both vandware written in terms of c, and the reasoning graph has edges 1→2,1→3,
2→4. In this case, we see that training on skill 2or skill 3both improve losses on skills 2and3(Figure 17). However,
unlike the previous figures, training on skills 2and4or skills 3and4do not significantly help reduce loss on skill 4
(Figure 18). Again, these measurements demonstrate that the reasoning graph does not necessarily equal the skills graph.
D.2 Unsupervised skill recovery
We explore several clustering techniques for recovering the skills in the LEGO synthetic on the validation dataset. Our
results are shown in Table 3.
We first cluster based on the pre-trained model embeddings of the last token and the average token. We also report
25

--- PAGE 26 ---
0 1000 2000 3000
Steps0.00.20.40.60.8Validation LossModel performance on LEGO skill 4 (tree)
Trained on skill 4
Trained on skills 2, 4
0 1000 2000 3000
Steps0.00.20.40.60.8Validation LossModel performance on LEGO skill 4 (tree)
Trained on skill 4
Trained on skills 3, 4Figure 18: Performance on LEGO skill 4when training on skills 2,4and skills 3,4. We find that in both cases, the benefit
from training on additional skills is minor. For instance, training on 2and4reaches 0.01loss in 2700 steps, while training
on4only reaches it in 2100 steps.
Cluster method Accuracy
Pretrained embedding of last token 24.8±0.5
Pretrained embedding of average token 25.2±1.1
Trained model embedding of last token 38.4±0.8
Sentence-BERT embedding 23.9±0.7
Losses over multiple runs 61 .0±1.6
Table 3: Clustering-based skill recovery methods on the LEGO dataset. The validation dataset we cluster consists of 500
points with k= 5, and results are reported over 10 runs of k-means.
accuracies of clustering based on the trained model embedding’s last token, where we train the model using random sampling
for6000 steps, and clustering based on Sentence-BERT embeddings. Among these four methods, using the trained model
embeddings has the highest accuracy of 38.4points.
Next, we cluster points based on losses. In particular, we do 10runs, each for 6000 steps and with a randomly sampled
mixture of skills. For each run, we evaluate the model on the validation dataset at 120checkpoints. Then, each sample in
the validation dataset has 1200 losses associated with it, comprising a feature vector for that sample. We perform k-means
clustering on these features, which has an accuracy of 61.0points, significantly higher than the second best accuracy of 38.4.
D.3 Full results for Section 4
D.3.1 Per-skill performance
In this section, we provide tables containing the per skill break-down of our results from Section 4.
Continual Pre-training In the continual pre-training setting, we report two additional baselines that combine curriculum
learning with skills. Curriculum learning has been proposed for multitask learning [ 60], in which groups of data are ranked
by their average score and then trained in order of this ranking (with mixing of previously seen groups to avoid forgetting).
We construct two baselines, Skill-curriculum and Skill-anticurriculum, using Algorithm 1 from [ 60]. In contrast to the
random baseline which has imbalanced skills, this approach has knowledge of skills and thus uses a skill-stratified training
dataset to sample from. We set the fraction of the previous group to be frac= 0.4, as we found that setting frac= 0.0
resulted in forgetting.
We report loss per skill for the LEGO synthetic in Table 4, which corresponds to the results in Figure 4. We report
accuracy per skill in Table 5 and Figure 19. We report the loss per skill for the Addition synthetic in Table 6, which also
correspond to to the results in Figure 4. Finally, we report validation loss per task category for the Natural Instructions
continual pre-training experiment in Table 7, where we find that SKILL -IToutperforms random sampling by 3.2%on
average across skills.
26

--- PAGE 27 ---
Skill 1 Skill 2 Skill 3 Skill 4 Skill 5 Average
Random 0±0.000 0.675±0.041 0.688±0.008 0.673±0.049 0.667±0.056 0.541±0.031
Curriculum 0±0.000 0.645±0.052 0.686±0.018 0.674±0.042 0.671±0.0459 0.535±0.029
Anticurriculum 0±0.000 0.690±0.003 0.695±0.004 0.693±0.003 0.689±0.004 0.554±0.001
Skill-stratified 0±0.000 0.045±0.036 0.056±0.029 0.079±0.044 0.050±0.025 0.046±0.022
Skill-curriculum 0±0.000 0.484±0.200 0.698±0.027 0.697±0.010 0.689±0.007 0.514±0.040
Skill-anticurriculum 0.001±0.001 0.174±0.118 0.245±0.091 0.443±0.125 0.566±0.118 0.286±0.060
SKILL -IT 0±0.000 0.002±0.002 0.024±0.031 0.013±0.010 0.022±0.021 0.012±0.008
Table 4: Results on validation loss per skill for LEGO pre-training experiment, averaged over 5random seeds.
Skill 1 Skill 2 Skill 3 Skill 4 Skill 5 Average
Random 100.0±0.0 54.2±5.9 58.0±3.1 48.0±6.3 54.4±7.3 62.9±3.5
Curriculum 100.0±0.0 60.0±10.6 55.2±5.8 51.2±6.3 51.8±6.1 63.6±3.6
Anticurriculum 100.0±0.0 53.4±2.3 49.0±4.8 48.2±6.4 56.0±5.7 61.3±2.2
Skill-stratified 100.0±0.0 98.2±1.8 98.2±1.3 97.8±1.6 98.2±1.3 98.5±0.9
Skill-curriculum 100.0±0.0 75.2±30.1 52.2±3.7 51.0±4.6 54.4±3.1 66.6±7.7
Skill-anticurriculum 100.0±0.0 90.2±8.1 88.2±8.3 73.2±12.2 62.4±9.4 82.8±4.9
SKILL -IT 100.0±0.0 99.2±0.8 99.0±1.0 99.4±0.5 99.6±0.5 99.4±0.2
Table 5: Results on accuracy per skill (binary classification) for LEGO pre-training experiment, averaged over 5random
seeds.
0 2000 4000 60000.40.60.81.0AccuracyLEGO Skill 1
0 2000 4000 60000.50.60.70.80.91.0LEGO Skill 2
0 2000 4000 60000.40.50.60.70.80.91.0LEGO Skill 3
0 2000 4000 6000
Steps0.50.60.70.80.91.0AccuracyLEGO Skill 4
0 2000 4000 6000
Steps0.40.50.60.70.80.91.0LEGO Skill 5
0 2000 4000 6000
Steps0.20.40.60.81.0Average per skill
Random
Curriculum
Anticurriculum
Skill-stratified
Skill-curriculum
Skill-anticurriculum
Skill-It
Figure 19: Accuracy of SKILL -ITon each skill (binary classification) on the LEGO synthetic in the continual pre-training
setting. S KILL -ITattains higher accuracy more quickly than baselines that both do and do not utilize the notion of skills.
Skill 1 Skill 2 Skill 3 Average
Random 0.008±0.007 0.020±0.019 0.005±0.005 0.011±0.014
Curriculum 0.009±0.011 0.010±0.008 0.008±0.010 0.009±0.010
Anticurriculum 0.007±0.010 0.012±0.013 0.008±0.017 0.009±0.014
Skill-stratified 0.012±0.011 0.015±0.015 0.010±0.020 0.012±0.016
Skill-curriculum 0.016±0.013 0.019±0.013 0.010±0.003 0.015±0.010
Skill-anticurriculum 0.005±0.008 0.037±0.028 1.141±1.126 0.395±0.371
SKILL -IT 0.004±0.003 0.009±0.007 0.013±0.017 0.009±0.011
Table 6: Results on validation loss per skill for Addition pre-training experiment, averaged over 5random seeds.
27

--- PAGE 28 ---
Skill Random Curriculum Anticurriculum Skill-stratified Skill-curriculum Skill-anticurriculum SKILL -IT
Answer Verification 2.297±0.058 2.368±0.055 2.391±0.061 2.180±0.059 2.249±0.116 2.325±0.085 2.158±0.059
Code to Text 0.246±0.021 0.203±0.019 1.099±0.115 0.178±0.016 0.126±0.009 1.232±0.070 0.223±0.017
Discourse Connective Identification 2.927±0.069 3.084±0.067 2.932±0.058 2.805±0.071 2.891±0.001 2.925±0.011 2.784±0.068
Entity Generation 2.033±0.421 2.012±0.437 2.363±0.234 1.803±0.384 1.853±0.483 2.068±0.719 1.863±0.418
Entity Relation Classification 1.020±0.147 1.014±0.140 1.533±0.138 0.859±0.131 0.825±0.022 0.959±0.009 0.908±0.146
Information Extraction 2.154±0.040 2.247±0.037 2.352±0.042 2.140±0.037 2.286±0.022 2.338±0.025 2.073±0.042
Irony Detection 3.024±0.154 3.798±0.095 2.942±0.158 2.680±0.146 3.889±0.066 2.099±0.152 2.797±0.155
Preposition Prediction 0.979±0.124 0.887±0.147 1.488±0.213 0.845±0.152 0.941±0.019 1.044±0.029 0.876±0.173
Punctuation Error Detection 2.950±0.065 3.120±0.052 2.961±0.064 3.264±0.061 3.019±0.010 3.360±0.013 3.216±0.055
Question Answering 2.277±0.005 2.367±0.006 2.398±0.006 2.542±0.004 2.689±0.001 2.707±0.016 2.448±0.008
Question Generation 2.617±0.005 2.777±0.015 2.695±0.008 2.783±0.021 3.062±0.006 2.876±0.032 2.666±0.012
Question Understanding 1.965±0.051 2.199±0.059 2.060±0.033 1.958±0.051 2.385±0.022 2.100±0.054 1.895±0.043
Sentence Expansion 2.501±0.095 2.598±0.097 2.583±0.074 2.225±0.095 2.311±0.076 2.408±0.074 2.236±0.083
Sentiment Analysis 3.203±0.012 3.415±0.016 3.209±0.010 3.278±0.014 3.607±0.012 3.308±0.015 3.213±0.012
Stance Detection 1.810±0.100 1.775±0.120 2.231±0.128 1.385±0.070 1.361±0.114 1.823±0.189 1.556±0.125
Summarization 2.961±0.015 3.149±0.023 3.041±0.014 2.960±0.019 3.323±0.028 3.021±0.013 2.907±0.012
Text Categorization 2.488±0.023 2.692±0.029 2.553±0.006 2.570±0.015 3.001±0.007 2.635±0.014 2.448±0.017
Text Matching 2.177±0.059 2.232±0.055 2.316±0.048 2.152±0.061 2.324±0.004 2.304±0.035 2.093±0.054
Text Simplification 2.155±0.023 2.193±0.039 2.325±0.033 1.926±0.026 2.037±0.005 2.156±0.011 1.952±0.026
Text to Code 0.560±0.037 0.495±0.036 1.215±0.052 0.490±0.029 0.433±0.014 1.455±0.086 0.553±0.042
Toxic Language Detection 3.106±0.027 3.496±0.017 3.058±0.029 3.199±0.024 3.758±0.025 3.155±0.050 3.129±0.020
Word Semantics 2.092±0.027 2.334±0.034 2.156±0.064 1.916±0.043 1.784±0.048 2.424±0.038 1.952±0.019
Wrong Candidate Generation 2.438±0.021 2.606±0.039 2.519±0.027 2.506±0.026 2.849±0.029 2.574±0.018 2.432±0.025
Average 2.173±0.028 2.307±0.025 2.366±0.026 2.115±0.027 2.304±0.031 2.317±0.052 2.103±0.032
Table 7: Validation loss per skill for data selection in continual pre-training setting on a subset of the Natural Instructions Dataset.
28

--- PAGE 29 ---
Out-of-domain In Table 8, we provide a breakdown of validation loss per evaluation skill under random sampling on the
training data, skill-stratified sampling over prerequisite skills (e.g., the nonzero rows in Figure 14), and S KILL -IT.
Skill Random Skill-stratified SKILL -IT
Answerability Classification 3.048±0.003 3.076±0.002 3.043±0.003
Cause Effect Classification 2.068±0.004 2.101±0.005 2.067±0.006
Coreference Resolution 3.101±0.003 3.142±0.004 3.099±0.004
Data to Text 2.363±0.004 2.388±0.005 2.359±0.005
Dialogue Act Recognition 2.329±0.009 2.364±0.010 2.320±0.009
Grammar Error Correction 2.399±0.008 2.418±0.009 2.389±0.007
Keyword Tagging 2.744±0.005 2.760±0.007 2.733±0.006
Overlap Extraction 2.749±0.011 2.763±0.012 2.733±0.010
Question Rewriting 2.591±0.009 2.628±0.011 2.586±0.010
Textual Entailment 2.472±0.002 2.503±0.003 2.468±0.002
Title Generation 3.027±0.002 3.037±0.002 3.015±0.002
Word Analogy 1.665±0.016 1.682±0.015 1.668±0.016
Average 2.546±0.003 2.572±0.003 2.540±0.003
Table 8: Validation loss per skill for data selection in out-of-domain setting over Natural Instructions train task split and test
task split.
In Table 9 we provide a breakdown of the RedPajama experiment’s accuracy per evaluation skill, corresponding to the
results in Figure 7.
1 Billion Tokens 2 Billion Tokens 3 Billion Tokens
Uniform S KILL -IT Uniform S KILL -IT Uniform S KILL -IT
ARC Challenge (acc norm) 35.4 34 .6 35.3 34 .9 34.6 34 .8
ARC Easy (acc norm) 62.2 61 .2 62.4 61 .7 62.5 62 .0
BoolQ 68.9 68 .2 67.7 68 .6 67.2 68 .7
COPA 81.0 82 .0 80.0 81 .0 81.0 81 .0
HellaSwag (acc norm) 63.9 63 .7 63.8 63 .9 64.0 63 .9
LAMBADA OpenAI 64.4 67 .0 65.9 66 .7 66.8 66 .0
PIQA (acc norm) 74.8 75 .0 75.5 75 .2 75.0 75 .7
Winogrande 62.8 63 .9 63.9 63 .2 63.4 63 .1
Average accuracy 64.2 64 .4 64.3 64 .4 64.3 64 .4
Table 9: Performance of model trained on RedPajama with uniform sampling and SKILL -ITon LM evaluation harness.
Unless otherwise noted, accuracy is reported for each task.
D.3.2 Weight trajectories
We provide SKILL -IT’s weight trajectories for each result. The weight per skill across training steps for the LEGO pre-
training experiment corresponding to Figure 4 (left) is shown in Figure 20. We see that SKILL -ITinitially allocates more
weight to skill 2and less to 1,3,4,5. Since skill 1is learned quickly, the weight on skill 1immediately drops to below 0.1
at1000 steps. The weight on skills 3,4,and5increase from around 0to3000 steps, during which their respective validation
losses are higher than those of skills 1and2. Near the end of training, all losses are converging to 0, and so the weight per
skill is roughly uniform.
The weight per skill across training steps for the addition pre-training experiment corresponding to Figure 4 (right) is
shown in Figure 21. SKILL -ITallocates more weight to skill 2, which has an edge to skill 1as shown in Figure 11. It also
allocates very little weight to skill 3, which is learned faster than the other two skills. Eventually, it puts more weight on
skill1, the hardest skill, and then converges to uniform sampling as all validation losses approach 0.
The weight per skill across training steps for the LEGO fine-tuning experiment and the Spanish question generation
and stance detection experiments corresponding to Figure 5 is shown in Figure 22. Since there is only one target skill in
these experiments, the mixture of weights approaches uniform as the loss on the target skill approaches 0. It is interesting
29

--- PAGE 30 ---
0 1000 2000 3000 4000 5000 6000
Steps0.050.100.150.200.250.300.35Weight per skillSkill-It LEGO weights
Skill 1
Skill 2
Skill 3
Skill 4
Skill 5Figure 20: Weight per skill for LEGO pre-training experiment. SKILL -ITinitially allocates more weight to skill 2, but
eventually puts more weight on harder skills ( 3,4,5) before converging to uniform sampling when all losses converge
roughly to 0.
to explore how to reduce edge weights and regularization so that the mixture approaches the target skill instead, although
preliminary experiments where we decayed the edge weight and the strength of the Bregman divergence term did not appear
better. We hypothesize that since training on a uniform mixture (as in Figure 3) did strictly better than training on the target
skill and their loss curves did not intersect during the training run, it is better to allocate non-negligible weight on all skills
throughout the training run.
The weight per skill across training steps for the Natural Instructions out-of-domain experiment corresponding to Figure 6
is shown in Figure 23, where the legend is provided for the top 10task categories with the largest weights. While the initial
weights based on the skills graph roughly establishes the order of weight magnitude, the differences among the losses on
the evaluation skills increases the range of weights as training continues. As validation losses saturate, the weights also
converge to fixed values.
D.4 Experiments on 1.3B parameter model
We demonstrate that the skills graph learned on the 125M parameter model can be used for data selection with the GPT-Neo-
1.3B model. We present results in the continual pre-training setting on the LEGO synthetic and Natural Instructions.
All results are reported over 3random seeds. For the LEGO experiment, we train for 1500 steps with η= 0.5, T=
30, w= 3. For the NI experiment, we train for 5000 steps with η= 0.2, andT= 1. The skill graphs were learned using the
125M parameter model as described in section C.2.
In Figure 24, we train the 1.3B model using SKILL -ITfor the LEGO synthetic and find that it still outperforms random
and skill-stratified sampling on average. In particular, while performance across sampling methods is similar for early skills,
the discrepancy is larger for skill 5, for which SKILL -ITallocates more weight to dynamically. In Figure 25, we provide the
weight trajectories of SKILL -IT. We observe that the weight trajectories are similar to that on the 125M parameter model,
where initial weight is allocated towards skill 2. Later on, more weight is allocated towards skills 4and5, whose losses are
higher, and eventually the weight mixture converges to uniform as all losses converge to near 0.
In Table 10, we report performance of SKILL -ITwith the 1.3B model on the Natural Instructions pre-training experiment
and find that the trends from the smaller model hold— SKILL -IToutperforms random and skill-stratified sampling on
average.
D.5 Ablations
We report ablations on the skills graph and the online component of SKILL -IT. Instead of using Ain Algorithm 1, we study
the performance when the identity matrix is used instead; intuitively, this corresponds to a misspecified skills graph where
30

--- PAGE 31 ---
0 1000 2000 3000 4000 5000 6000
Steps0.150.200.250.300.350.400.450.500.55Weight per skillSkill-It addition weights
Skill 1
Skill 2
Skill 3Figure 21: Weight per skill for addition pre-training experiment. SKILL -ITinitially allocates more weight to skill 2, which
has an edge to skill 1, while allocating little weight to skill 3which is learned quickly. Eventually, SKILL -ITputs more
weight on the harder skill 1before converging to uniform sampling when all losses roughly approach 0.
0 1000 2000 3000 4000 5000 6000
Steps0.240.260.280.300.320.340.360.38Weight per skillSkill-It LEGO weights for learning skill 3
Skill 1
Skill 2
Skill 3
0 100 200 300 400 500 600
Steps0.150.200.250.300.35Weight per skillSkill-It weights for Spanish QG
English QA
Spanish QA
English QG
Spanish QG
0 100 200 300 400 500 600
Steps0.4250.4500.4750.5000.5250.5500.575Weight per skillSkill-It weights for Stance Detection
Stance Detection
Text Matching
Figure 22: Weight per skill for fine-tuning experiments. Left: LEGO; Center: Spanish question generation; Right: stance
detection.
0 1000 2000 3000 4000 5000
Steps0.000.020.040.060.080.100.120.14Weight per skillSkill-It weights for Natural Instructions
question_generation
question_answering
text_categorization
sentiment_analysis
wrong_candidate_generation
text_matching
summarization
information_extraction
question_understanding
toxic_language_detection
Figure 23: Weight per skill for Natural Instructions out-of-domain experiment. The legend shows the top 10skills with the
largest weight. While the relative order of weight magnitude does not change significantly across training, the incorporation
of loss dramatically increases the range of the weights, showing the importance of an online algorithm.
31

--- PAGE 32 ---
0 500 1000 1500105
104
103
102
101
Validation Loss (Log)LEGO Skill 1
0 500 1000 1500104
103
102
101
100LEGO Skill 2
0 500 1000 1500103
102
101
100LEGO Skill 3
0 500 1000 1500
Steps103
102
101
100Validation Loss (Log)LEGO Skill 4
0 500 1000 1500
Steps103
102
101
100LEGO Skill 5
0 500 1000 1500
Steps103
102
101
100 Average per skill
Random
Skill-stratified
Skill-ItFigure 24: Performance of SKILL -ITfor LEGO pre-training setting when skills graph is learned on a 125M parameter model
and used for data selection with a 1.3B model. SKILL -ITon average still outperforms random and skill-stratified sampling,
suggesting that findings on ordered skill sets can transfer from small models to large models.
0 250 500 750 1000 1250 1500
Steps0.00.10.20.30.40.50.6Weight per skillSkill-It LEGO weights for 1.3B param model
Skill 1
Skill 2
Skill 3
Skill 4
Skill 5
Figure 25: Weight per skill for LEGO pre-training experiment on 1.3B parameter model. The trajectories are similar to those
of the 125M parameter model in Figure 20. SKILL -ITinitially allocates more weight to skill 2, but eventually puts more
weight on skills 4and5before converging to uniform sampling when all losses converge to near 0.
32

--- PAGE 33 ---
Skill Random Skill-stratified SKILL -IT
Answer Verification 2.005±0.059 1.903±0.069 1.890±0.072
Code to Text 0.302±0.032 0.204±0.022 0.269±0.032
Discourse Connective Identification 2.529±0.046 2.372±0.054 2.393±0.056
Entity Generation 2.108±0.328 1.788±0.429 1.885±0.461
Entity Relation Classification 1.130±0.048 0.836±0.006 0.841±0.010
Information Extraction 2.032±0.013 1.992±0.006 1.933±0.013
Irony Detection 2.802±0.125 2.528±0.146 2.585±0.149
Preposition Prediction 1.095±0.040 0.686±0.041 0.774±0.029
Punctuation Error Detection 2.633±0.027 3.188±0.055 2.726±0.025
Question Answering 1.947±0.003 2.119±0.003 2.073±0.001
Question Generation 2.214±0.007 2.345±0.008 2.263±0.010
Question Understanding 1.928±0.020 1.837±0.031 1.700±0.042
Sentence Expansion 2.054±0.018 1.828±0.060 1.853±0.058
Sentiment Analysis 2.771±0.009 2.818±0.006 2.774±0.007
Stance Detection 1.814±0.151 1.500±0.117 1.628±0.149
Summarization 2.531±0.009 2.472±0.012 2.440±0.013
Text Categorization 2.289±0.016 2.341±0.021 2.231±0.022
Text Matching 1.967±0.008 1.913±0.005 1.872±0.005
Text Simplification 1.861±0.003 1.692±0.023 1.698±0.022
Text to Code 0.614±0.030 0.518±0.030 0.585±0.022
Toxic Language Detection 2.853±0.020 2.911±0.019 2.862±0.018
Word Semantics 1.999±0.023 1.870±0.039 1.902±0.024
Wrong Candidate Generation 2.187±0.028 2.192±0.023 2.140±0.020
Average 1.985±0.022 1.907±0.027 1.883±0.032
Table 10: Results when skills graph for Natural Instructions learned on 125M parameter model is used for data selection
with a 1.3B model. We see that SKILL -ITon average still outperforms random and skill-stratified sampling, even though the
edges used by S KILL -ITare not derived from the larger model.
no skill influences another skill. We refer this approach as “No graph”. Note that the opposite case of a complete graph
recovers skill-stratified sampling, which we already have as a baseline.
Second, instead of sampling over multiple rounds and weighting according to the loss of each skill, we study the effect of
setting T= 1, which only uses a softmax on Ato yield static weights on the skills. We refer to this approach as “Static”.
We omit results on Natural Instructions continual pre-training, since SKILL -ITusesT= 1and using no graph with T= 1
recovers skill-stratified sampling. Intuitively, we expect the static version of SKILL -ITto perform somewhat well unless
there is significant discrepancy among the losses (e.g. in synthetics where the loss on one skill can be close to 0while the
other is not, versus in Natural Instructions where all losses decrease consistently). For both ablations, we sweep over values
ofη= [0.1,0.2,0.5,0.8].
Figure 26 shows the comparison between SKILL -ITand no graph on the continual pre-training LEGO experiment,
and Figure 27 shows the comparison between SKILL -ITand a static approach. We see that both the graph and the online
dynamics of SKILL -ITare important for its performance. In particular, using no graph results in allocating significant weight
to harder skills early on, even though many of them have easier prerequisite skills (such as skill 3 having edges to skills
1 and 2). Using a static graph results in consistent allocation of significant weight to prerequisite skills even after their
validation losses converge to near 0, and thus the harder skills that have higher loss are not learned quickly afterwards.
We perform the same ablation on the Addition dataset—the results for this are shown in Figures 28 and Figure 29. We
find that these simple baselines, including using a static graph and no graph perform similarly to SKILL -ITon average across
all skills—while SKILL -ITperforms the best on skill 2 compared to vanilla multiplicative weights, and SKILL -ITperforms
the best on skill 1 compared to a static graph. This suggests that Addition is somewhat easier than the other datasets that we
consider, as S KILL -ITstill outperforms other baselines, as shown in Figure 4.
Figure 30 compares SKILL -IT, no graph, and static data selection for the LEGO fine-tuning experiment. No graph can be
interpreted as allocating equal weight to all training skills not equal to the target skill, and varying this weight versus the
weight on the target skill. While SKILL -ITand setting T= 1behave similarly, we see that SKILL -ITis slightly better than
using no graph. For instance, SKILL -ITobtains a validation loss of 0.05in2000 steps, compared to 2050 -2200 steps when
using no graph.
Figure 31 and 32 compare SKILL -IT, no graph, and static data selection for the Natural Instructions fine-tuning
experiments. For both Spanish QG and stance detection, SKILL -ITattains lower loss than using no graph or using T= 1
round.
Figure 33 compares SKILL -ITand static data selection for the Natural Instructions out-of-domain experiment. SKILL -IT
33

--- PAGE 34 ---
0 2000 4000 6000104
103
102
101
100Validation Loss (Log)LEGO Skill 1
0 2000 4000 6000102
101
100 LEGO Skill 2
0 2000 4000 6000101
LEGO Skill 3
0 2000 4000 6000
Steps102
101
Validation Loss (Log)LEGO Skill 4
0 2000 4000 6000
Steps101
LEGO Skill 5
0 2000 4000 6000
Steps102
101
Average per skill
No graph (0.1)
No graph (0.2)
No graph (0.5)
No graph (0.8)
Skill-ItFigure 26: Comparison of SKILL -ITversus using the identity adjacency matrix (no skills graph) with η= 0.1,0.2,0.5,0.8
on the LEGO continual pre-training experiment. The latter does not capture the relationship between skills, and we find that
SKILL -ITattains lower loss on all skills.
0 2000 4000 6000104
103
102
101
100Validation Loss (Log)LEGO Skill 1
0 2000 4000 6000102
101
LEGO Skill 2
0 2000 4000 6000101
100LEGO Skill 3
0 2000 4000 6000
Steps102
101
100Validation Loss (Log)LEGO Skill 4
0 2000 4000 6000
Steps101
LEGO Skill 5
0 2000 4000 6000
Steps102
101
Average per skill
Static (0.1)
Static (0.2)
Static (0.5)
Static (0.8)
Skill-It
Figure 27: Comparison of SKILL -ITversus using static data selection ( T= 1) with η= 0.1,0.2,0.5,0.8on the LEGO
continual pre-training experiment. While SKILL -ITeventually allocates more weights to skills 3, 4, 5, which have higher
loss, the static approach is not able to do this. We find that S KILL -ITattains lower loss on all skills.
34

--- PAGE 35 ---
0 2000 4000 6000102
101
100Validation Loss (Log)Addition Skill 1
0 2000 4000 6000102
101
100Addition Skill 2
0 2000 4000 6000
Steps102
101
100Validation Loss (Log)Addition Skill 3
0 2000 4000 6000
Steps102
101
100Average per skill
No graph (0.1)
No graph (0.2)
No graph (0.5)
No graph (0.8)
Skill-ItFigure 28: Comparison of SKILL -ITversus using the identity adjacency matrix (no skills graph) with η= 0.1,0.2,0.5,0.8
on the Addition continual pre-training experiment. The latter does not capture the relationship between skills, and we find
that S KILL -ITattains lower loss on skill 2, but attains similar performance to methods that do not use the skills graph.
0 2000 4000 6000102
101
100Validation Loss (Log)Addition Skill 1
0 2000 4000 6000102
101
100Addition Skill 2
0 2000 4000 6000
Steps102
101
100Validation Loss (Log)Addition Skill 3
0 2000 4000 6000
Steps102
101
100Average per skill
Static (0.1)
Static (0.2)
Static (0.5)
Static (0.8)
Skill-It
Figure 29: Comparison of SKILL -ITversus using static data selection ( T= 1) with η= 0.1,0.2,0.5,0.8on the Addition
continual pre-training experiment. We find that SKILL -ITattains lower loss on skill 1, but attains similar performance to the
static methods.
0 1000 2000 3000 4000 5000 6000
Steps0.00.20.40.60.8Validation LossPerformance on Skill 3
No graph (0.1)
No graph (0.2)
No graph (0.5)
No graph (0.8)
Skill-It
0 1000 2000 3000 4000 5000 6000
Steps0.00.20.40.60.8Validation LossPerformance on Skill 3
Static (0.1)
Static (0.2)
Static (0.5)
Static (0.8)
Skill-It
Figure 30: Comparison of SKILL -ITversus using no graph (left) and static data selection (right) with η= 0.1,0.2,0.5,0.8
on the LEGO fine-tuning experiment. All approaches have roughly the same loss trajectories, but SKILL -ITis slightly lower
than using no graph.
35

--- PAGE 36 ---
200 300 400 500 600
Steps2.302.352.402.452.502.552.60Validation Loss
Performance on Spanish QG
No graph (0.1)
No graph (0.2)
No graph (0.5)
No graph (0.8)
Skill-It
200 300 400 500 600
Steps2.302.352.402.452.502.552.60Validation Loss
Performance on Spanish QG
Static (0.1)
Static (0.2)
Static (0.5)
Static (0.8)
Skill-ItFigure 31: Comparison of SKILL -ITversus using no graph (left) and static data selection (right) with η= 0.1,0.2,0.5,0.8
on the Natural Instructions Spanish QG fine-tuning experiment. SKILL -ITattains lower validation loss than both no graph
and static data selection.
200 300 400 500 600
Steps1.21.31.41.51.61.71.8Validation Loss
Performance on stance detection
No graph (0.1)
No graph (0.2)
No graph (0.5)
No graph (0.8)
Skill-It
200 300 400 500 600
Steps1.21.31.41.51.61.71.8Validation Loss
Performance on stance detection
Static (0.1)
Static (0.2)
Static (0.5)
Static (0.8)
Skill-It
Figure 32: Comparison of SKILL -ITversus using no graph (left) and static data selection (right) with η= 0.1,0.2,0.5,0.8
on the Natural Instructions stance detection fine-tuning experiment. SKILL -ITattains lower validation loss than both no
graph and static data selection.
36

--- PAGE 37 ---
3.063.083.10Validation Loss
Answerability Classification
Static (0.1)
Static (0.2)
Static (0.5)
Static (0.8)
Skill-It
2.082.102.12
Cause Effect Classification
3.103.123.143.16
Coreference Resolution
2.352.362.37
Data To Text
2.322.342.36Validation Loss
Dialogue Act Recognition
2.382.402.42
Grammar Error Correction
2.742.762.78
Keyword Tagging
2.742.762.782.80
Overlap Extraction
1000 2000 3000 4000 5000
Steps2.592.602.612.62Validation Loss
Question Rewriting
1000 2000 3000 4000 5000
Steps2.482.502.52
Textual Entailment
1000 2000 3000 4000 5000
Steps3.023.033.043.05
Title Generation
1000 2000 3000 4000 5000
Steps1.671.681.691.70
Word AnalogyFigure 33: Comparison of SKILL -ITversus using static data selection with η= 0.1,0.2,0.5,0.8on the Natural Instructions
out-of-domain experiment. SKILL -ITattains the lowest validation loss on 7out of 12evaluation skills, and an average loss
of2.540compared to a range of 2.541-2.551for static data selection.
attains the lowest validation loss on 7out of 12evaluation skills. It has an average loss of 2.540compared to a range of
2.541-2.551for static data selection.
37
