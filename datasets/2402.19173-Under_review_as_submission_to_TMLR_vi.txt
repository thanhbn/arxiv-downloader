Đây là báo cáo kỹ thuật về StarCoder2, phiên bản tiếp theo của mô hình ngôn ngữ lớn cho mã nguồn, được phát triển bởi dự án BigCode - một cộng tác khoa học mở tập trung vào phát triển có trách nhiệm các mô hình ngôn ngữ lớn cho mã nguồn.

Dự án BigCode xây dựng The Stack v2 dựa trên kho lưu trữ mã nguồn của Software Heritage, bao gồm các kho lưu trữ SWH trải rộng 619 ngôn ngữ lập trình. Bên cạnh đó, họ cẩn thận chọn lọc các nguồn dữ liệu chất lượng cao khác như pull request GitHub, notebook Kaggle và tài liệu về mã nguồn. Điều này tạo ra một tập huấn luyện lớn gấp 4 lần so với tập dữ liệu StarCoder đầu tiên.

Họ huấn luyện các mô hình StarCoder2 với 3B, 7B và 15B tham số trên 3.3 đến 4.3 nghìn tỷ token và đánh giá toàn diện trên một bộ các benchmark Code LLM.

Mô hình nhỏ StarCoder2-3B vượt trội hơn các Code LLM khác có kích thước tương tự trên hầu hết các benchmark, và cũng vượt trội hơn StarCoderBase-15B. Mô hình lớn StarCoder2-15B vượt trội đáng kể so với các mô hình có kích thước tương đương. Ngoài ra, nó có hiệu suất ngang bằng hoặc vượt trội hơn CodeLlama-34B, một mô hình lớn hơn gấp đôi kích thước.

Mặc dù DeepSeekCoder-33B là mô hình có hiệu suất tốt nhất trong việc hoàn thành mã cho các ngôn ngữ có tài nguyên cao, họ phát hiện rằng StarCoder2-15B vượt trội hơn trong các benchmark lý luận toán học và mã nguồn, cũng như một số ngôn ngữ có tài nguyên thấp.

Họ cung cấp trọng số mô hình dưới giấy phép OpenRAIL và đảm bảo tính minh bạch hoàn toàn về dữ liệu huấn luyện bằng cách phát hành các định danh bền vững SWHIDs của dữ liệu mã nguồn.

Các mô hình ngôn ngữ lớn cho mã nguồn đã nhanh chóng nổi lên như những trợ lý mạnh mẽ để viết và chỉnh sửa mã. Tính đến ngày 30 tháng 1 năm 2024, GitHub CoPilot đã có hơn 1.3 triệu người đăng ký trả phí, với hơn 50,000 tổ chức lựa chọn phiên bản doanh nghiệp, ước tính tăng năng suất nhà phát triển lên đến 56% cũng như sự hài lòng của nhà phát triển.

ServiceNow gần đây tiết lộ rằng giải pháp "text-to-code" của họ, được xây dựng từ việc tinh chỉnh các mô hình StarCoderBase, dẫn đến việc tăng 52% năng suất nhà phát triển.

Dự án BigCode được thành lập vào tháng 9 năm 2022 như một cộng tác khoa học mở tập trung vào phát triển mở và có trách nhiệm các Code LLM. BigCode được điều hành bởi ServiceNow và Hugging Face theo tinh thần quản trị mở và đã tập hợp hơn 1,100 thành viên từ các viện học thuật đa dạng và phòng thí nghiệm công nghiệp.

The Stack v2 được xây dựng trên nền tảng kho lưu trữ mã nguồn rộng lớn của Software Heritage, trải rộng hơn 600 ngôn ngữ lập trình. Họ sử dụng phiên bản 2023-09-06 của tập dữ liệu SH graph làm nguồn chính, bắt đầu bằng việc trích xuất các phiên bản được thu thập gần đây nhất của tất cả kho lưu trữ GitHub và lọc chúng để chỉ giữ lại nhánh chính.

Họ phát hiện giấy phép ở cấp kho lưu trữ từ GHArchive cho tất cả kho lưu trữ có tên khớp trong tập dữ liệu SWH. Khi giấy phép cấp repo không có sẵn (96.93% kho lưu trữ), họ sử dụng ScanCode Toolkit để phát hiện giấy phép cấp tệp.

Khác với Stack v1 phát hiện ngôn ngữ lập trình theo phần mở rộng tệp, họ dựa vào một bộ phân loại ngôn ngữ, cụ thể là go-enry dựa trên thư viện linguist của GitHub để phát hiện ngôn ngữ lập trình cho mỗi tệp, phát hiện 658 ngôn ngữ duy nhất.

Họ áp dụng một tập hợp các bộ lọc cơ bản cho tập dữ liệu để loại bỏ các tệp được tạo tự động, tệp dữ liệu hoặc dữ liệu huấn luyện chất lượng thấp khác, bao gồm bộ lọc dòng dài, bộ lọc tự động tạo, bộ lọc alpha và bộ lọc dữ liệu được mã hóa.

Họ tích hợp các issue GitHub được thu thập từ GHArchive, loại bỏ pull request và xử lý chúng riêng. Họ bao gồm các đánh giá mã bằng cách thu thập các sự kiện pull request từ GHArchive và mã nguồn tương ứng từ Software Heritage.

Họ bao gồm notebook từ hai nguồn riêng biệt: notebook Jupyter được trích xuất từ kho lưu trữ Software Heritage và notebook được phát hành bởi nền tảng Kaggle. Họ chuyển đổi notebook Jupyter thành script và notebook có cấu trúc theo pipeline giống như StarCoder.

Họ thu thập tài liệu từ một số nền tảng quản lý gói bao gồm npm, PyPI, Go Packages, Packagist, Rubygems, Cargo, CocoaPods, Bower, CPAN, Clojars, Conda, Hex và Julia.

Họ tăng cường mã nguồn bằng cách ghép nối các biểu diễn trung gian để nâng cao hiểu biết của mô hình về các ngôn ngữ lập trình có tài nguyên thấp. Họ chọn LLVM làm biểu diễn trung gian do tính khả dụng rộng rãi trên GitHub.

Họ bao gồm một số tập dữ liệu nhỏ chất lượng cao cho toán học và lập trình như APPS, Code Contest, GSM8K, DeepMind Mathematics, Rosetta Code, MultiPL-T và Proofsteps.

Họ áp dụng một số bước tiền xử lý như khử trùng lặp, biên tập PII, khử nhiễm benchmark, loại bỏ malware và yêu cầu opt-out. Họ sử dụng mô hình StarPII để biên tập các thực thể PII khác nhau từ mã nguồn, pull request, issue và StackOverflow.

Với một tập huấn luyện lớn hơn nhiều có sẵn, họ quyết định điều chỉnh thành phần dữ liệu cho từng kích thước mô hình. Họ lý luận rằng các mô hình nhỏ hơn, có khả năng hạn chế, nên được tiếp xúc với một tập dữ liệu ít đa dạng hơn.

Họ đưa ra một số thay đổi kiến trúc so với StarCoderBase. Đầu tiên, họ thay thế embedding vị trí học được bằng Rotary Positional Encodings (RoPE). Thay đổi kiến trúc thứ hai là thay thế Multi-Query Attention (MQA) bằng Grouped Query Attention (GQA).

Các mô hình được huấn luyện với độ dài chuỗi 4,096 sử dụng Adam với β1 = 0.9, β2 = 0.95, ε = 10^-8 và weight decay 0.1, không dropout. Tốc độ học tập tuân theo cosine decay sau warm-up tuyến tính 1,000 lần lặp.

Họ tiếp tục tiền huấn luyện mỗi mô hình cho ngữ cảnh dài trên 200B token từ cùng corpus tiền huấn luyện, sử dụng độ dài ngữ cảnh 16,384 với sliding window 4,096.

Họ đánh giá các mô hình StarCoder2 trên nhiều benchmark từ văn học và so sánh chúng với các Code LLM mở gần đây như StableCode, Code Llama, DeepSeekCoder và StarCoder gốc.

Trên HumanEval và MBPP, StarCoder2-3B là mô hình nhỏ có hiệu suất tốt nhất trên tất cả các tập dữ liệu. StarCoder2-7B đứng thứ hai trong các mô hình trung bình. StarCoder2-15B là mô hình lớn có hiệu suất tốt nhất với biên độ đáng kể.

Trên MultiPL-E, StarCoder2-3B hoạt động tốt nhất trên 11/18 ngôn ngữ lập trình trong các mô hình nhỏ. StarCoder2-15B hoạt động tốt nhất trên 16/18 ngôn ngữ lập trình trong các mô hình lớn.

Trên DS-1000, StarCoder2-3B nhìn chung là mô hình nhỏ có hiệu suất tốt nhất. StarCoder2-7B đứng thứ hai trong các mô hình trung bình. StarCoder2-15B là mô hình lớn có hiệu suất tốt nhất.

Họ đánh giá khả năng sửa lỗi và chỉnh sửa mã của các mô hình trên HumanEvalFix và CanItEdit. Mặc dù StarCoder2 là mô hình cơ sở, nó được tiền huấn luyện trên GitHub issue và thảo luận StackOverflow sử dụng định dạng đặc biệt.

Trên GSM8K với PAL, StarCoder2-15B vượt trội đáng kể so với tất cả các mô hình lớn, bao gồm cả CodeLlama-13B và StarCoderBase-15B, thậm chí vượt trội hơn CodeLlama-34B và DeepSeekCoder-33B.

Trên CRUXEval, StarCoder2-15B là mô hình lớn có hiệu suất tốt nhất, hoạt động ngang bằng với các mô hình cực lớn. StarCoder2 hỗ trợ fill-in-the-middle (FIM), khả năng hoàn thành một đoạn mã tùy ý dựa trên văn bản trước và sau điểm chèn.

Họ đánh giá các mô hình trên hoàn thành mã cấp kho lưu trữ với RepoBench và CrossCodeEval. StarCoder2, với huấn luyện cấp kho lưu trữ, liên tục vượt trội hơn StarCoderBase trên tất cả các kích thước mô hình được đánh giá.

Trên benchmark bảo mật "Asleep at the Keyboard", StarCoder2 tạo ra số lượng chương trình hợp lệ tương đương với StarCoderBase, CodeLlama và DeepSeekCoder. Ngoại trừ StarCoder2-3B, StarCoder2-7B và StarCoder2-15B có tỷ lệ chương trình không an toàn cao nhất trong các mô hình có tham số tương tự.

Họ đo lường các thế hệ có hại bằng BOLD, WinoBias, HONEST và RealToxicityPrompts. Kết quả cho thấy các mô hình LLM mà họ xem xét tạo ra khoảng cùng lượng nội dung có hại.

Theo tiêu chuẩn do StarCoder thiết lập, họ xây dựng một bộ công cụ kiểm tra dữ liệu, quy kết và tìm kiếm khác. Họ cập nhật công cụ "Am I in the Stack" với các kho lưu trữ trong tập dữ liệu mới và triển khai Data Portrait sử dụng Bloom filter để cho phép khớp trên nội dung tệp.

Dự án này tiến hành theo tinh thần Khoa học Mở, tập trung vào phát triển và sử dụng có trách nhiệm Code LLM. Những nỗ lực đáng kể từ cộng đồng BigCode đã đi vào việc tuyển chọn cẩn thận, xác thực, khử nhiễm, loại bỏ malware, lọc giấy phép, quy trình opt-out, loại bỏ PII, cấu trúc, đóng gói, hosting, cấp phép và xuất bản Data Card cho dữ liệu được sử dụng để huấn luyện StarCoder2.

Cách tiếp cận mở của BigCode đối với cộng tác khoa học, phân phối mô hình truy cập mở và cấp phép, cũng như tính mở và tiết lộ dữ liệu huấn luyện, kiến trúc và phát triển là cần thiết để cộng đồng nghiên cứu có quyền truy cập vào các LLM mạnh mẽ, thực sự mở, giúp tăng tốc nghiên cứu tương lai.

Giấy phép BigCode Open RAIL-M chứa các hạn chế sử dụng quan trọng và đi kèm với FAQ để hướng dẫn triển khai và sử dụng có trách nhiệm mô hình bởi người dùng downstream.

BigCode là một cộng đồng thực hành với hơn 1,200 thành viên đa ngành từ hơn 60 quốc gia làm việc hướng tới phát triển có trách nhiệm các mô hình ngôn ngữ lớn cho mã nguồn.

Tính đến tháng 10 năm 19, 2023, Software Heritage đã xuất bản một tuyên bố định nghĩa việc sử dụng máy học chấp nhận được của kho lưu trữ Software Heritage. Đây là một cột mốc quan trọng mở ra cánh cửa cho việc tìm nguồn dữ liệu có trách nhiệm hơn và cấp phép dữ liệu huấn luyện AI.

Software Heritage cung cấp các định danh duy nhất SWHID, ràng buộc nội tại với các thành phần phần mềm, và không cần đăng ký trung tâm, để đảm bảo rằng một web kiến thức kiên cường có thể được xây dựng trên kho lưu trữ Software Heritage.

Mức độ mở trong quá trình phát triển LLM được kết nối với các rủi ro tiềm năng liên quan đến việc phát hành mô hình. Khi các hệ thống được phát triển theo cách hoàn toàn đóng, quyền lực có khả năng tập trung trong các tổ chức có tài nguyên cao, và nhóm phát triển nhỏ có thể không hiểu đầy đủ tác động và hậu quả dài hạn của mô hình được triển khai.

Mặt khác, phát triển hoàn toàn mở cho phép nghiên cứu cộng đồng, dân chủ hóa quyền truy cập vào các mô hình và cho phép kiểm toán trong suốt toàn bộ quá trình phát triển. Tuy nhiên, không có các biện pháp bảo vệ thích hợp, phát triển LLM mở đặt ra rủi ro lạm dụng cao hơn.

Thật khó để xác định và phân loại chính xác các loại PII khác nhau để có thể đánh giá xử lý dữ liệu cá nhân, chuyển đổi và luồng thông qua mã. Người dùng downstream được khuyến nghị triển khai thêm quét PII, lọc, làm sạch và giảm thiểu để đảm bảo tuân thủ với các trường hợp sử dụng dự định của họ.

Như với bất kỳ nghiên cứu khoa học mở nào cung cấp quyền truy cập mở vào trọng số mô hình, siêu tham số, mã xử lý dữ liệu, mã huấn luyện, dữ liệu huấn luyện và tài liệu, bất kỳ tác nhân nào cũng có thể chạy hoặc tinh chỉnh mô hình được tối ưu hóa với chi phí tính toán rất thấp.

Như đã được thiết lập trước đây trong các đánh giá của các mô hình mã hóa, Code LLM có thể tạo ra mã với cấu trúc phản ánh các định kiến về giới tính, chủng tộc, cảm xúc, giai cấp, cấu trúc tên và các đặc điểm khác.

Như đã thảo luận trong các phần trước, có nhiều dữ liệu hơn trong tập dữ liệu huấn luyện cho các ngôn ngữ lập trình phổ biến như Python và Java so với các ngôn ngữ ít phổ biến như Haskell và Fortran. Do đó, mô hình hoạt động tốt hơn trên các ngôn ngữ có tài nguyên cao như vậy, điều này có thể củng cố sở thích của các nhà phát triển hướng tới việc sử dụng các ngôn ngữ như vậy.

Code LLM phục vụ như các mô hình nền tảng mạnh mẽ có thể được tinh chỉnh để tạo ra mã chất lượng cao, tài liệu, unit test, tóm tắt văn bản, quy trình tự động hóa và nhiều hơn nữa. Kết quả từ phân tích dựa trên nhiệm vụ cho thấy các công việc có tiềm năng tự động hóa cao nhất của các nhiệm vụ bởi LLM nhấn mạnh các quy trình thường xuyên và lặp đi lặp lại và không yêu cầu mức độ giao tiếp liên cá nhân cao.

Họ giới thiệu StarCoder2, một họ LLM được thiết kế để tạo mã, cùng với The Stack v2, corpus tiền huấn luyện lớn nhất cho Code LLM được xây dựng trên nền tảng kho lưu trữ Software Heritage. The Stack v2 lớn gấp mười lần so với phiên bản tiền nhiệm, tạo ra một tập dữ liệu thô 67.5 TB.

Thông qua việc làm sạch, lọc và lấy mẫu phụ rộng rãi của mã nguồn, cùng với việc tích hợp các tập dữ liệu chất lượng cao khác liên quan đến mã, họ tạo ra một tập huấn luyện khoảng 3TB (900B+ token).

Tận dụng tập dữ liệu mới này, họ huấn luyện các mô hình StarCoder2 với 3B, 7B và 15B tham số. Các đánh giá Code LLM rộng rãi của họ, đánh giá khả năng hoàn thành, chỉnh sửa và lý luận mã, cho thấy StarCoder2-3B và StarCoder2-15B là các mô hình tiên tiến trong các lớp kích thước tương ứng của chúng.

Bằng cách không chỉ phát hành trọng số mô hình mà còn đảm bảo tính minh bạch hoàn toàn về dữ liệu huấn luyện, họ hy vọng sẽ tăng niềm tin vào các mô hình được phát triển và trao quyền cho các nhóm kỹ thuật và nhà khoa học khác xây dựng dựa trên nỗ lực của họ.
