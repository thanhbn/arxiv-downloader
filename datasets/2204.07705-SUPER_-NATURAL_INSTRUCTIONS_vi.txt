# 2204.07705.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/datasets/2204.07705.pdf
# Kích thước tệp: 1745029 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SUPER-NATURAL INSTRUCTIONS:
Tổng quát hóa thông qua Hướng dẫn Tuyên bố trên hơn 1600 Nhiệm vụ NLP

}Yizhong Wang2}Swaroop Mishra3|Pegah Alipoormolabashi4|Yeganeh Kordi5
Amirreza Mirzaei4Anjana Arunkumar3Arjun Ashok6Arut Selvan Dhanasekaran3
Atharva Naik7David Stap8Eshaan Pathak9Giannis Karamanolakis10Haizhi Gary Lai11
Ishan Purohit12Ishani Mondal13Jacob Anderson3Kirby Kuznia3Krima Doshi3Maitreya Patel3
Kuntal Kumar Pal3Mehrad Moradshahi14Mihir Parmar3Mirali Purohit15Neeraj Varshney3
Phani Rohitha Kaza3Pulkit Verma3Ravsehaj Singh Puri3Rushang Karia3Shailaja Keyur Sampat3
Savan Doshi3Siddhartha Mishra16Sujan Reddy17Sumanta Patro18Tanay Dixit19Xudong Shen20
Chitta Baral3Yejin Choi1;2Noah A. Smith1;2Hannaneh Hajishirzi1;2Daniel Khashabi21

1Allen Institute for AI2Univ. of Washington3Arizona State Univ.4Sharif Univ. of Tech.5Tehran Polytechnic6PSG College of Tech.7IIT Kharagpur
8Univ. of Amsterdam9UC Berkeley10Columbia Univ.11Factored AI12Govt. Polytechnic Rajkot13Microsoft Research14Stanford Univ.15Zycus Infotech
16Univ. of Massachusetts Amherst17National Inst. of Tech. Karnataka18TCS Research19IIT Madras20National Univ. of Singapore21Johns Hopkins Univ.

Tóm tắt

Các mô hình NLP có thể tổng quát hóa tốt đến mức nào đối với nhiều nhiệm vụ chưa được thấy khi được cung cấp hướng dẫn nhiệm vụ? Để giải quyết câu hỏi này, đầu tiên chúng tôi giới thiệu SUPER-NATURAL INSTRUCTIONS,1 một điểm chuẩn gồm 1.616 nhiệm vụ NLP đa dạng và hướng dẫn được viết bởi chuyên gia. Bộ sưu tập của chúng tôi bao gồm 76 loại nhiệm vụ riêng biệt, bao gồm nhưng không giới hạn ở phân loại, trích xuất, điền vào chỗ trống, gắn thẻ chuỗi, viết lại văn bản và sáng tác văn bản. Bộ sưu tập lớn và đa dạng các nhiệm vụ này cho phép đánh giá nghiêm ngặt khả năng tổng quát hóa xuyên nhiệm vụ dưới hướng dẫn - huấn luyện các mô hình để tuân theo hướng dẫn trên một tập con các nhiệm vụ và đánh giá chúng trên những nhiệm vụ chưa được thấy còn lại.

Hơn nữa, chúng tôi xây dựng Tk-INSTRUCT, một mô hình transformer được huấn luyện để tuân theo nhiều hướng dẫn trong ngữ cảnh (định nghĩa nhiệm vụ bằng ngôn ngữ thông thường hoặc k-shot examples). Các thí nghiệm của chúng tôi cho thấy Tk-INSTRUCT vượt trội hơn các mô hình tuân theo hướng dẫn hiện có như InstructGPT hơn 9% trên điểm chuẩn của chúng tôi mặc dù nhỏ hơn một bậc độ lớn. Chúng tôi tiếp tục phân tích khả năng tổng quát hóa như một hàm của các tham số mở rộng khác nhau, chẳng hạn như số lượng nhiệm vụ quan sát được, số lượng instances trên mỗi nhiệm vụ và kích thước mô hình. Chúng tôi hy vọng bộ dữ liệu và mô hình của mình sẽ tạo điều kiện cho tiến bộ trong tương lai hướng tới các mô hình NLP có mục đích tổng quát hơn.2

1 Giới thiệu

Cộng đồng NLP đã chứng kiến tiến bộ lớn trong việc xây dựng các mô hình để tổng quát hóa cho các nhiệm vụ chưa được thấy thông qua hướng dẫn trong ngữ cảnh (Mishra et al.,

1SUPER-NATURAL INSTRUCTIONS đại diện cho sự mở rộng siêu quy mô của NATURAL INSTRUCTIONS (Mishra et al., 2022b) đã có 61 nhiệm vụ.
2Bộ dữ liệu, mô hình và bảng xếp hạng có thể được tìm thấy tại https://instructions.apps.allenai.org.

}Đồng tác giả chính |Đồng tác giả thứ hai

•Đầu vào:"Ngữ cảnh:…'Thật tuyệt vời, tôi rất vui vì chúng ta đã đi đến điều gì đó mà cả hai đều đồng ý.' Phát ngôn: 'Tôi cũng vậy. Tôi hy vọng bạn có một chuyến cắm trại tuyệt vời.'"•Đầu ra: "Có"•Giải thích: "Người tham gia tham gia vào cuộc trò chuyện phiếm khi chúc đối thủ có một chuyến đi tuyệt vời."•Đầu vào: "Ngữ cảnh: …'Nghe hay đấy, tôi cần thức ăn nhất, vật phẩm bạn cần nhất là gì?!' Phát ngôn:'Vật phẩm của tôi cũng là thức ăn'."•Đầu ra: "Có"•Giải thích: "Phát ngôn chỉ đưa cuộc đàm phán tiến lên và không có cuộc nói chuyện phụ. Do đó, câu trả lời đúng là 'Không'." Định nghĩa"...Cho một phát ngôn và ngữ cảnh đối thoại gần đây chứa 3 phát ngôn trước (bất cứ khi nào có sẵn), đầu ra 'Có' nếu phát ngôn chứa chiến lược nói chuyện phiếm, ngược lại đầu ra 'Không'. Nói chuyện phiếm là một chiến lược đàm phán hợp tác. Nó được sử dụng để thảo luận các chủ đề ngoài cuộc đàm phán, để xây dựng mối quan hệ với đối thủ."Hướng dẫn Nhiệm vụ

•Đầu vào: "Ngữ cảnh: …'Tôi rất hào hứng được dành thời gian với mọi người từ trại!'Phát ngôn:'Thật tuyệt vời!Tôi thực sự thích được ở ngoài đây với con trai mình.Bạn có thể dành chút thức ăn không?'"•Đầu ra Dự kiến:"Có"Ví dụ Tích cực

Ví dụ Tiêu cựcCác Instance Đánh giáTk-InstructHình 1: Một ví dụ nhiệm vụ từ SUP-NATINST được chấp nhận từ Chawla et al. (2021). Một mô hình thành công được mong đợi sử dụng các hướng dẫn được cung cấp (bao gồm định nghĩa nhiệm vụ và ví dụ minh họa) để đưa ra phản hồi cho một nhóm các instances đánh giá.

2022b; Sanh et al., 2022; Wei et al., 2022) sử dụng các mô hình ngôn ngữ được đào tạo trước lớn (Raffel et al., 2020; Brown et al., 2020). Mặc dù các mô hình như InstructGPT (Ouyang et al., 2022) rất đáng chú ý, nhưng đóng góp của các lựa chọn thiết kế khác nhau cho thành công của chúng vẫn còn mờ mịt. Đặc biệt, vai trò của dữ liệu được giám sát vẫn chưa được nghiên cứu đầy đủ do dữ liệu hạn chế được phát hành bởi các thực thể doanh nghiệp đứng sau các mô hình chính. Ngoài ra, việc mở rộng và đào tạo lại các mô hình khổng lồ này gần như không thể đối với cộng đồng nghiên cứu. Giải quyết hai thách thức này arXiv:2204.07705v3 [cs.CL] 24 Oct 2022

--- TRANG 2 ---

Tài nguyên →SUP-NATINST
(công trình này)NATINST
(Mishra et al., 2022b)CROSS FIT
(Ye et al., 2021)PROMPT SOURCE
(Bach et al., 2022)FLAN
(Wei et al., 2022)INSTRUCT GPT
(Ouyang et al., 2022)

Có hướng dẫn nhiệm vụ? 3 3 7 3 3 3
Có ví dụ tiêu cực? 3 3 7 7 7 7
Có nhiệm vụ không phải tiếng Anh? 3 7 7 7 3 3
Là công khai? 3 3 3 3 3 7
Số lượng nhiệm vụ 1616 61 269 176 62 –
Số lượng hướng dẫn 1616 61 – 2052 620 14378
Số loại nhiệm vụ được chú thích 76 6 13 1312 10
Độ dài trung bình định nghĩa nhiệm vụ (từ) 56.6 134.4 – 24.8 8.2 –

Bảng 1: So sánh SUP-NATINST với một số bộ dữ liệu đáng chú ý trong lĩnh vực. Chúng tôi lấy số lượng nhiệm vụ, hướng dẫn và loại nhiệm vụ của các bộ dữ liệu khác từ bài báo gốc của chúng. "–" chỉ ra các trường không áp dụng hoặc không rõ. Tiêu chuẩn phân loại các loại nhiệm vụ khác nhau giữa các bộ dữ liệu khác nhau (xem Hình 2). *PROMPT SOURCE không cung cấp chú thích loại nhiệm vụ cho tất cả các nhiệm vụ của họ, mà chúng tôi chỉ báo cáo 13 loại nhiệm vụ được chú thích để huấn luyện T0 (Sanh et al., 2022).

[Một biểu đồ bubble lớn hiển thị các loại nhiệm vụ khác nhau được tổ chức theo bộ dữ liệu]

Hình 2: So với các bộ dữ liệu khác, SUP-NATINST bao gồm một phạm vi đa dạng hơn các loại nhiệm vụ. InstructGPT báo cáo một phân loại rất thô các loại nhiệm vụ của họ. Kích thước bong bóng đại diện cho số lượng nhiệm vụ của mỗi loại theo thang logarit.

thách thức đòi hỏi sự sẵn có của các điểm chuẩn công khai quy mô lớn về một phạm vi rộng các nhiệm vụ NLP và hướng dẫn của chúng để tạo điều kiện phát triển và đánh giá các mô hình có thể tổng quát hóa cho các nhiệm vụ chưa được thấy.

Trong bài báo này, chúng tôi xây dựng một meta-dataset (tức là, bộ dữ liệu của các bộ dữ liệu; Triantafillou et al., 2019) bao gồm nhiều nhiệm vụ NLP với hướng dẫn của chúng, và huấn luyện một mô hình có thể thực hiện một nhiệm vụ mới được đưa ra hướng dẫn, vượt trội hơn InstructGPT (sử dụng nhiều hơn 16× tham số).

Bộ dữ liệu của chúng tôi, SUPER-NATURAL INSTRUCTIONS (SUP-NATINST gọi tắt), là một điểm chuẩn lớn gồm 1.616 nhiệm vụ NLP và hướng dẫn ngôn ngữ tự nhiên của chúng. Nó mang lại một loạt các nhiệm vụ đa dạng—76 loại nhiệm vụ rộng trải rộng trên 55 ngôn ngữ khác nhau. Mỗi nhiệm vụ được ghép đôi với một hướng dẫn bao gồm định nghĩa nhiệm vụ để ánh xạ một văn bản đầu vào thành một đầu ra nhiệm vụ và một số ví dụ để minh họa đầu ra mong muốn hoặc không mong muốn (xem Hình 1 như một ví dụ nhiệm vụ). Những nhiệm vụ này và hướng dẫn của chúng được đóng góp bởi 88 chuyên gia NLP, để đáp ứng lời kêu gọi công khai của chúng tôi. Những đóng góp này được hợp nhất sau nhiều vòng đánh giá đồng nghiệp và phản hồi crowdsourced để đảm bảo chất lượng. Có dữ liệu đa dạng và quy mô lớn này cho phép chúng tôi cẩn thận chia các nhiệm vụ thành tập huấn luyện và kiểm tra và nghiên cứu có hệ thống cách các phương pháp hiện đại hoạt động trên chúng. Bảng 1 và Hình 2 làm nổi bật các thuộc tính của SUP-NATINST so với các điểm chuẩn liên quan, nhấn mạnh sự đa dạng của các nhiệm vụ và loại hướng dẫn trong điểm chuẩn của chúng tôi.

Mô hình của chúng tôi, Tk-INSTRUCT, là một mô hình sinh để biến đổi đầu vào nhiệm vụ được đưa ra hướng dẫn trong ngữ cảnh tuyên bố (định nghĩa nhiệm vụ hoặc k-shot examples). Nó được xây dựng bằng huấn luyện đa nhiệm vụ

--- TRANG 3 ---

của mô hình T5 (Raffel et al., 2020) trên tất cả hướng dẫn nhiệm vụ trong tập huấn luyện của chúng tôi, và được đánh giá trên các nhiệm vụ chưa được thấy trong tập kiểm tra. Thú vị là, một Tk-INSTRUCT 11B-parameter có thể vượt trội hơn mô hình InstructGPT 175B-parameter 9.9 điểm ROUGE-L khi được đánh giá trên 119 nhiệm vụ tiếng Anh chưa được thấy, và biến thể đa ngôn ngữ mTk-INSTRUCT vượt trội hơn InstructGPT 13.3 điểm trên 35 nhiệm vụ không phải tiếng Anh (§6.1). Theo đánh giá của con người, Tk-INSTRUCT tạo ra phản hồi ít nhất cũng tốt như ground truth cho 77% các instances kiểm tra (§6.2), xác nhận khả năng tổng quát hóa mạnh mẽ của nó đối với các nhiệm vụ chưa được thấy.

Hiệu suất thực nghiệm hấp dẫn của Tk-INSTRUCT xác nhận tầm quan trọng của các meta datasets siêu quy mô như SUP-NATINST của chúng tôi để tạo điều kiện nghiên cứu hướng tới các mô hình NLP có thể tổng quát hóa. Chúng tôi tiến hành phân tích mở rộng để hiểu các yếu tố quan trọng cho khả năng tổng quát hóa này (§7). Phân tích của chúng tôi cho thấy việc mở rộng quy mô sự đa dạng của các nhiệm vụ huấn luyện và kích thước mô hình đều quan trọng cho khả năng tổng quát hóa mạnh mẽ đối với các nhiệm vụ chưa được thấy. Cuối cùng, chúng tôi ước tính giới hạn trên hiệu suất, cho thấy vẫn còn chỗ để cải thiện thêm.

2 Công trình Liên quan

Hướng dẫn ngôn ngữ là một cách linh hoạt để định nghĩa mục tiêu, đó là lý do tại sao chúng đã được nghiên cứu trong bối cảnh của nhiều ứng dụng khác nhau, chẳng hạn như hướng dẫn trong môi trường nền tảng (Shridhar et al., 2020; Stepputtis et al., 2020; Min et al., 2022b; Weir et al., 2022) và lệnh cơ sở dữ liệu (Kim et al., 2020). Ở đây, chúng tôi tập trung vào các ứng dụng của hướng dẫn cho các nhiệm vụ NLP tổng quát.

Tài liệu gần đây đã được thúc đẩy bởi việc xây dựng các mô hình có thể tổng quát hóa trên nhiều nhiệm vụ NLP khác nhau, khi được nhắc nhở bằng một vài ví dụ (Ye and Ren, 2021; Bragg et al., 2021) hoặc định nghĩa ngôn ngữ (Efrat and Levy, 2020; Weller et al., 2020; Zhong et al., 2021; Mishra et al., 2022b,a; Parmar et al., 2022). Công trình của chúng tôi liên quan đến các điểm chuẩn hiện có trong dòng công trình này, như được miêu tả trong Bảng 1 theo các chiều khác nhau. Điểm chuẩn của chúng tôi mở rộng NATINST (Mishra et al., 2022b) với 26× nhiệm vụ hơn và sự đa dạng lớn hơn của các loại nhiệm vụ (Hình 2). Trong khi CROSSFIT (Ye et al., 2021) tập trung vào điểm chuẩn với một vài ví dụ trong ngữ cảnh, điểm chuẩn của chúng tôi cũng cung cấp hướng dẫn nhiệm vụ.

Đồng thời với công trình của chúng tôi, PROMPT SOURCE (Bach et al., 2022) là một điểm chuẩn khác của các nhiệm vụ và hướng dẫn ngôn ngữ của chúng (prompts). Một sự khác biệt quan trọng giữa điểm chuẩn này và của chúng tôi là cách diễn đạt các định nghĩa nhiệm vụ: trong khi các định nghĩa nhiệm vụ PROMPT SOURCE tương đối ngắn gọn, các định nghĩa nhiệm vụ của chúng tôi được thu thập với ý định cung cấp một định nghĩa hoàn chỉnh của mỗi nhiệm vụ và do đó dài hơn (24 token so với 56 token trung bình; Bảng 1). Gần đây hơn, BIG-BENCH (Srivastava et al., 2022) giới thiệu một bộ sưu tập 204 nhiệm vụ và cũng cung cấp mô tả nhiệm vụ ngắn và tiền tố đầu vào có thể được sử dụng để nhắc nhở LMs. Với ít sự chồng lấp với bộ sưu tập nhiệm vụ của chúng tôi, họ tập trung nhiều hơn vào việc tìm các nhiệm vụ thách thức có thể được sử dụng để kiểm tra các hành vi khác nhau của LMs hiện tại. Tuy nhiên, chúng tôi tin rằng tất cả những nỗ lực này trong việc thu thập các nhiệm vụ khác nhau cũng như hướng dẫn nhiệm vụ là bổ sung, và cộng đồng sẽ được hưởng lợi từ việc xem xét các điểm chuẩn khác nhau. Cuối cùng, mô hình InstructGPT được chấp nhận rộng rãi (Ouyang et al., 2022) được hỗ trợ một phần bởi một bộ dữ liệu lớn các prompts được thu thập thông qua tăng cường dữ liệu tổng hợp khác nhau mà, thật không may, không có sẵn công khai.

Ngoài khả năng tổng quát hóa xuyên nhiệm vụ, điểm chuẩn của chúng tôi cũng có thể được sử dụng để nghiên cứu học đa nhiệm vụ rộng hơn, đây là một mục tiêu lâu dài của AI (Caruana, 1997). Theo truyền thống, tài liệu này tập trung vào các thiết lập bao gồm đánh giá trên các nhiệm vụ được quan sát trong quá trình huấn luyện (Collobert and Weston, 2008; Hashimoto et al., 2017). Các nghiên cứu gần đây hơn cho thấy triển vọng rằng học đa nhiệm vụ quy mô lớn có thể cho phép khả năng tổng quát hóa mạnh mẽ đối với các nhiệm vụ tương tự thông qua mã hóa thống nhất (Khashabi et al., 2020; Xie et al., 2022) hoặc kết quả fine-tuning tốt hơn trên các nhiệm vụ downstream (McCann et al., 2018; Aribandi et al., 2022). Điểm chuẩn được đề xuất của chúng tôi cung cấp các nhiệm vụ đa dạng để nghiên cứu đa nhiệm vụ ở quy mô lớn.

3 SUPER-NATURAL INSTRUCTIONS

SUPER-NATURAL INSTRUCTIONS là một meta-dataset (Triantafillou et al., 2019) bao gồm nhiều nhiệm vụ NLP (xem Hình 2a) và hướng dẫn mô tả chúng bằng ngôn ngữ thông thường.

Lược đồ hướng dẫn. Tất cả hướng dẫn nhiệm vụ tuân theo cùng một lược đồ thống nhất (xem Hình 1) bao gồm các phần sau:

• ĐỊNH NGHĨA định nghĩa một nhiệm vụ nhất định bằng ngôn ngữ tự nhiên. Đây là một định nghĩa hoàn chỉnh về cách một văn bản đầu vào (ví dụ: một câu hoặc một tài liệu) được mong đợi ánh xạ thành một văn bản đầu ra.

• VÍ DỤ TÍCH CỰC là các mẫu đầu vào và

--- TRANG 4 ---

đầu ra đúng của chúng, cùng với một giải thích ngắn cho mỗi.

• VÍ DỤ TIÊU CỰC là các mẫu đầu vào và đầu ra sai/không hợp lệ của chúng, cùng với một giải thích ngắn cho mỗi.

Lược đồ trên dựa trên của Mishra et al. (2022b), mặc dù nó được đơn giản hóa. Xem Phụ lục C để so sánh.

Instances nhiệm vụ. Cho các hướng dẫn cho mỗi nhiệm vụ, một mô hình được mong đợi giải quyết các instances của nhiệm vụ đó. Chúng tôi sử dụng một định dạng thống nhất để tổ chức các instances của tất cả các nhiệm vụ của chúng tôi. Cụ thể hơn, mỗi instance bao gồm một đầu vào văn bản và một danh sách các đầu ra văn bản có thể chấp nhận. Chúng tôi giới hạn số lượng instances trong mỗi nhiệm vụ ở 6.5K để tránh sự mất cân bằng của instances giữa các nhiệm vụ.

Thu thập điểm chuẩn. Điểm chuẩn được thu thập thông qua một nỗ lực cộng đồng lớn trên GitHub.3 Các nhiệm vụ được thu thập và đóng góp bởi các chuyên gia NLP đã phản hồi lời mời công khai của chúng tôi4 hoặc sinh viên được khuyến khích đóng góp như một phần của dự án lớp của họ.5 Những người đóng góp được khuyến khích sáng tạo và tìm nguồn các nhiệm vụ từ một số tài nguyên: (a) các bộ dữ liệu NLP công khai hiện có, (b) các chú thích trung gian có sẵn trong các thí nghiệm crowdsourcing (ví dụ: paraphrasing questions hoặc đánh giá chất lượng của chúng trong quá trình crowdsourcing một bộ dữ liệu QA), hoặc (c) các nhiệm vụ tổng hợp có thể được truyền đạt cho một con người trung bình trong vài câu (ví dụ: các phép toán đại số cơ bản như so sánh số, tìm chuỗi con palindrome dài nhất, v.v.). Khi sử dụng các bộ dữ liệu hiện có hoặc chú thích crowdsourcing, những người đóng góp được khuyến khích áp dụng các hướng dẫn được sử dụng để tạo bộ dữ liệu này bất cứ khi nào có sẵn. Điều này được thực hiện để đảm bảo rằng các hướng dẫn đủ để định nghĩa các nhiệm vụ cho người đọc trung bình. Các nhiệm vụ cùng với hướng dẫn và thông tin meta khác được đóng góp dưới dạng tệp JSON thông qua các pull request GitHub, được xem xét bởi các kiểm tra tự động và đồng nghiệp. Chúng tôi có 88 người đóng góp từ các địa điểm và nền tảng đa dạng đóng góp vào repository của chúng tôi.

Kiểm soát chất lượng. Kiểm soát chất lượng của dữ liệu do cộng đồng đóng góp này được thực hiện trong nhiều giai đoạn: (1) Khi tạo một GitHub pull request của nhiệm vụ được đề xuất, nó ngay lập tức trải qua một bài kiểm tra tự động. Quá trình này xác minh rằng tệp được giới thiệu chứa các trường dự kiến và tuân thủ các thuộc tính mong muốn của chúng tôi (ví dụ: không có instances trùng lặp, các nhãn đầu ra không bị mất cân bằng nặng, v.v.) và (2) Nhiệm vụ được đề xuất sau đó được đánh giá đồng nghiệp bởi 1-2 người đóng góp chuyên gia khác để đảm bảo sự rõ ràng và đầy đủ của nội dung hướng dẫn. Quá trình đánh giá được thực hiện lặp đi lặp lại cho đến khi người đánh giá hài lòng với chất lượng của hướng dẫn được đề xuất. Cụ thể, người đánh giá được yêu cầu xác minh rằng hướng dẫn rõ ràng và đủ để một người nói ngôn ngữ trung bình giải quyết nhiệm vụ cơ bản (instances đánh giá) trong khi có ngữ pháp, trôi chảy và ngắn gọn. Trung bình, việc đánh giá mỗi GitHub pull request mất khoảng 4-6 lần lặp lại trong khoảng thời gian nhiều ngày trước khi được merge. (3) Cuối cùng, các nhiệm vụ được thêm vào được trình bày cho crowdworkers để thu thập phản hồi về chất lượng của các hướng dẫn được cung cấp, chẳng hạn như lỗi chính tả, sự rõ ràng hoặc các vấn đề khác (chi tiết trong §A). Sau đó, một trong những tác giả đã sử dụng phản hồi này để cải thiện định nghĩa nhiệm vụ của các instances. Phản hồi này chỉ được thực hiện cho các nhiệm vụ tiếng Anh, vì việc tìm crowdworkers chất lượng cao trong các ngôn ngữ khác là không đơn giản (Pavlick et al., 2014).

Sự đa dạng của các nhiệm vụ. Thu thập nhiệm vụ cho SUP-NATINST được giám sát cẩn thận để bao gồm một loạt các nhiệm vụ hiểu ngôn ngữ tự nhiên, miền và ngôn ngữ. Để hiểu rõ hơn sự đa dạng này, chúng tôi phân loại toàn diện các nhiệm vụ theo ba chiều khác nhau:

• LOẠI NHIỆM VỤ định nghĩa bản chất của ánh xạ từ đầu vào instances đến đầu ra (ví dụ: trả lời câu hỏi, phân loại, v.v.).

• NGÔN NGỮ chỉ ra (các) ngôn ngữ của các instances.

• MIỀN chỉ ra (các) miền mà văn bản của các nhiệm vụ thuộc về (ví dụ: chính trị, y học, đối thoại, v.v.).

Những biện pháp phân loại khác nhau này có thể được sử dụng để nghiên cứu các nghĩa khác nhau của khả năng tổng quát hóa. Trong các nghiên cứu thực nghiệm của chúng tôi (§5), chúng tôi nghiên cứu khả năng tổng quát hóa theo trục các loại nhiệm vụ. Chúng tôi giới thiệu độc giả đến Hình 10 trong phụ lục để biết sự phân bố của các nhiệm vụ giữa các loại nhiệm vụ, ngôn ngữ và miền khác nhau.

Thống kê. Bảng 2 hiển thị các thống kê khác nhau cho điểm chuẩn. Tổng cộng, bộ dữ liệu bao gồm 1616 nhiệm vụ và 5M instances. Trung bình, mỗi hướng dẫn được ghép đôi với 2.8 ví dụ tích cực và 2.4 ví dụ tiêu cực. Độ dài định nghĩa trung bình là 56.6 từ.

--- TRANG 5 ---

thống kê
# của nhiệm vụ 1616
# của loại nhiệm vụ 76
# của ngôn ngữ 55
# của miền 33
# của nhiệm vụ không phải tiếng Anh 576
độ dài định nghĩa trung bình (từ mỗi nhiệm vụ) 56.6
# trung bình của ví dụ tích cực (mỗi nhiệm vụ) 2.8
# trung bình của ví dụ tiêu cực (mỗi nhiệm vụ) 2.4
# trung bình của instances (mỗi nhiệm vụ) 3106.0

Bảng 2: Thống kê của SUP-NATINST.

4 Tk-INSTRUCT: Học Tuân theo Hướng dẫn ở Quy mô Lớn

Định nghĩa Khả năng Tổng quát hóa đối với Các Nhiệm vụ Chưa được Thấy. Mỗi nhiệm vụ t được định nghĩa thông qua hướng dẫn ngôn ngữ tự nhiên It, và mỗi nhiệm vụ có một tập các instances đầu vào/đầu ra (Xt; Yt). Một mô hình M được mong đợi tạo ra đầu ra y, cho đầu vào x và hướng dẫn nhiệm vụ It: M(It; x) = y; cho (x; y) ∈ (Xt; Yt). Đặc biệt, chúng tôi muốn đánh giá mô hình M trên các nhiệm vụ không được quan sát (tức là, các instances của chúng không được sử dụng để huấn luyện M). Nguồn tín hiệu duy nhất để học nhiệm vụ tại thời điểm suy luận là hướng dẫn trong ngữ cảnh It chứa một định nghĩa và ví dụ minh họa của nhiệm vụ.

Tk-INSTRUCT. Chúng tôi giới thiệu Tk-INSTRUCT, một mô hình được meta-huấn luyện trên SUP-NATINST để giải quyết các nhiệm vụ được đưa ra hướng dẫn trong ngữ cảnh của chúng. Công trình trước đây đã cho thấy hiệu quả của việc meta-huấn luyện như vậy trong việc cải thiện khả năng của mô hình để thực hiện học trong ngữ cảnh với prompts (Zhong et al., 2021; Sanh et al., 2022) hoặc ví dụ minh họa (Min et al., 2022a). Do sự đa dạng lớn của các nhiệm vụ trong SUP-NATINST, chúng tôi có thể thực hiện meta-huấn luyện đa nhiệm vụ này ở quy mô lớn hơn trước đây. Chúng tôi tiến hành các thí nghiệm và phân tích của mình dựa trên mô hình T5 (Raffel et al., 2020). Vì mỗi hướng dẫn It bao gồm nhiều yếu tố như được mô tả trong lược đồ hướng dẫn của chúng tôi (§3), chúng tôi ánh xạ các yếu tố này thành định dạng văn bản và thêm chúng trước instance đầu vào. Hình 8 trong phụ lục cho thấy cách chúng tôi mã hóa các hướng dẫn đầy đủ. Chúng tôi nghiên cứu các kết hợp khác nhau của các yếu tố hướng dẫn này trong §7.2. Theo mặc định, chúng tôi sẽ sử dụng các yếu tố hướng dẫn hiệu quả nhất (tức là, định nghĩa nhiệm vụ và hai ví dụ tích cực) trừ khi được chỉ định khác. Theo cách tương tự, chúng tôi huấn luyện biến thể đa ngôn ngữ mTk-INSTRUCT dựa trên mô hình mT5 (Xue et al., 2021).

5 Đánh giá Khả năng Tổng quát hóa Xuyên Nhiệm vụ với SUP-NATINST

Ở đây chúng tôi cung cấp công thức được khuyến nghị của chúng tôi để đánh giá khả năng tổng quát hóa thông qua SUP-NATINST.

5.1 Thiết lập Đánh giá

Một Phân chia Đánh giá của Các Nhiệm vụ Chưa được Thấy. Chúng tôi chia bộ sưu tập lớn các nhiệm vụ trong SUP-NATINST thành hai tập con: một để đánh giá và cái kia để giám sát. Đối với các nhiệm vụ đánh giá, chúng tôi cố định một bộ sưu tập được chọn thủ công gồm 12 danh mục đại diện cho 154 nhiệm vụ. Sự đa dạng lớn của các nhiệm vụ trong SUP-NATINST cho phép chúng tôi chọn một tập các nhiệm vụ đa dạng để đánh giá - chẳng hạn như những nhiệm vụ ở cấp độ từ, câu và tài liệu, bao gồm cả định dạng phân loại và sinh. Phụ lục G liệt kê các nhiệm vụ đánh giá của chúng tôi với các ví dụ cho các nhiệm vụ đại diện. Để đánh giá hiệu quả, chúng tôi lấy mẫu tối đa 100 instances cho mỗi nhiệm vụ, dẫn đến tổng cộng 15.310 instances kiểm tra. Các nhiệm vụ còn lại được sử dụng để huấn luyện mô hình.6

Chia Thành Các Track cho Nhiệm vụ Tiếng Anh và X-lingual. SUP-NATINST bao gồm các nhiệm vụ trên nhiều ngôn ngữ, cho phép đánh giá khả năng tổng quát hóa của mô hình đối với các nhiệm vụ chưa được thấy không chỉ bằng tiếng Anh mà còn trong các ngôn ngữ khác. Do đó, chúng tôi chia các nhiệm vụ đánh giá của mình thành hai track: một cho khả năng tổng quát hóa xuyên nhiệm vụ tiếng Anh (119 nhiệm vụ) và cái kia cho khả năng tổng quát hóa xuyên ngôn ngữ xuyên nhiệm vụ (35 nhiệm vụ). Theo hiểu biết tốt nhất của chúng tôi, đây là nghiên cứu đầu tiên về khả năng tổng quát hóa xuyên ngôn ngữ xuyên nhiệm vụ (tức là, khả năng tổng quát hóa đối với các nhiệm vụ chưa được thấy trong các ngôn ngữ khác nhau). Hình 11 và Hình 12 trong phụ lục chứa các nhiệm vụ đánh giá cho mỗi track.

Metrics Đánh giá. Do sự đa dạng của các nhiệm vụ của chúng tôi và bản chất sinh mở của công thức của chúng tôi,7 chúng tôi áp dụng ROUGE-L (Lin, 2004) để báo cáo kết quả hiệu suất tổng hợp. Đây là một metric overlap chuỗi mềm có thể được áp dụng cho một loạt các nhiệm vụ sinh văn bản. Chúng tôi cho thấy rằng thứ hạng từ metric này tương quan tốt với độ chính xác cho các nhiệm vụ phân loại trong Phụ lục E. Chúng tôi cũng tiến hành đánh giá con người trong §6.2.

6Để tránh rò rỉ dữ liệu, chúng tôi loại trừ các nhiệm vụ khỏi tập huấn luyện nếu chúng có nguồn từ cùng một bộ dữ liệu với bất kỳ nhiệm vụ kiểm tra nào. Điều này dẫn đến 757 nhiệm vụ huấn luyện cho track tiếng Anh và 1271 nhiệm vụ huấn luyện cho track xuyên ngôn ngữ.

7Không giống như Sanh et al. (2022) và Wei et al. (2022), người đánh giá mô hình của họ trên các nhiệm vụ phân loại thông qua xếp hạng tùy chọn (tức là, ghi điểm câu trả lời đúng cao hơn các câu trả lời ứng viên khác), chúng tôi đánh giá mô hình của mình trong một thiết lập sinh mở không có giả định cụ thể về nhiệm vụ. Chúng tôi tin rằng đây là một biện pháp thực tế hơn của khả năng tổng quát hóa đối với các nhiệm vụ chưa được thấy.

--- TRANG 6 ---

5.2 Baselines và Mô hình Hiện có

Ở đây chúng tôi thảo luận về nhiều baselines và mô hình cạnh tranh cho ứng dụng mục tiêu của chúng tôi. Xem Phụ lục D để biết chi tiết triển khai.

Heuristic baselines. Đầu tiên chúng tôi đánh giá các heuristics sau để đánh giá các phím tắt có thể trong dữ liệu. Copying Demo Output sao chép đầu ra của một ví dụ minh họa ngẫu nhiên. Vì chúng tôi cân bằng các nhãn cho các nhiệm vụ kiểm tra của mình, hiệu suất của baseline này sẽ gần bằng một phỏng đoán ngẫu nhiên hoặc một baseline đa số cho các nhiệm vụ phân loại. Copying Instance Input sao chép đầu vào instance đã cho. Chiến lược này hoạt động tốt trên các nhiệm vụ mà đầu ra mục tiêu phần lớn chồng chéo với đầu vào (ví dụ: viết lại câu hỏi, sửa lỗi ngữ pháp).

Mô hình ngôn ngữ được đào tạo trước sẵn có. Chúng tôi đánh giá các LM hiện có không được fine-tuned với dữ liệu cụ thể về hướng dẫn. Cụ thể, chúng tôi đánh giá T5 11B-parameter (Raffel et al., 2020) như một đối tác trực tiếp của Tk-INSTRUCT. Do mục tiêu đào tạo trước infilling của mô hình T5 gốc, nó không thể tiếp tục văn bản tốt. Do đó, chúng tôi đánh giá phiên bản "LM-adapted" của nó, được huấn luyện thêm với một mục tiêu mô hình hóa ngôn ngữ (Lester et al., 2021). Ngoài ra, chúng tôi đánh giá GPT-3 (Brown et al., 2020), một LM tự hồi quy 175B-parameter đã cho thấy khả năng đáng chú ý trong việc tuân theo các minh chứng được cung cấp trong prompt của nó.

Mô hình được điều chỉnh hướng dẫn. Ngoài Tk-INSTRUCT của chúng tôi (§4), chúng tôi đánh giá các mô hình hiện có được fine-tuned để tuân theo hướng dẫn ngôn ngữ. Đặc biệt, chúng tôi đánh giá InstructGPT (Ouyang et al., 2022) sử dụng học tăng cường để kết hợp sở thích của con người vào một mô hình được đào tạo trước GPT-3, và T0 (Sanh et al., 2022) fine-tunes T5 trên một bộ sưu tập prompts nhiệm vụ trong PROMPT-SOURCE (Bach et al., 2022).

Ước tính giới hạn trên. Chúng tôi ước tính một giới hạn trên về khả năng tổng quát hóa của mô hình đối với các nhiệm vụ chưa được thấy bằng cách fine-tuning một mô hình oracle trên các instances được gắn nhãn của các nhiệm vụ. Vì mô hình này quan sát các instances ẩn của các nhiệm vụ đánh giá, nó là, theo định nghĩa, một giới hạn trên ước tính cho các mô hình dựa trên tổng quát hóa của chúng tôi. Cụ thể, chúng tôi fine-tune một mô hình T5-11B trên 119 nhiệm vụ đánh giá tiếng Anh, và một mô hình mT5-13B trên 35 nhiệm vụ không phải tiếng Anh, với 1K instances huấn luyện ngẫu nhiên mỗi nhiệm vụ, mà không chồng chéo với các instances đánh giá.

Phương pháp # tham số Đánh giá! En X-lingual

Heuristic
Baselines Copying Instance Input 14.2 5.4
Copying Demo Output 28.5 50.3

Pretrained LMs T5-LM (11B) 30.2 –
GPT3 (175B) 45.0 51.3

Instruction-tuned
Models T0 (11B) 32.3 –
InstructGPT (175B) 52.1 52.8
Tk-INSTRUCT (của chúng tôi, 11B) 62.0 –
mTk-INSTRUCT (của chúng tôi, 13B) 57.1 66.1

Upper-bound (est.) Supervised Training 74.3 94.0

Bảng 3: Hiệu suất tổng thể của các phương pháp khác nhau trên các nhiệm vụ chưa được thấy trong tập kiểm tra của SUP-NATINST (§6.1). Chúng tôi báo cáo ROUGE-L ở đây như metric tổng hợp của chúng tôi. Các mô hình tận dụng hướng dẫn cho thấy khả năng tổng quát hóa mạnh mẽ hơn đối với các nhiệm vụ chưa được thấy. Đặc biệt, mô hình của chúng tôi được fine-tuned trên một tập các nhiệm vụ đa dạng vượt trội hơn InstructGPT và T0 với một biên độ lớn.

15.240.064.664.176.9
14.232.352.154.362.0
01020304050607080
Copying Instance Input T0 (11B) InstructGPT Tk-Instruct(3B) Tk-Instruct(11B) Human ROUGE-L

Hình 3: Đánh giá con người vs. ROUGE-L cho một số phương pháp (§6.2). Xu hướng của hai metrics này có mối tương quan cao với hệ số Pearson là 0.998.

6 Kết quả Thí nghiệm

6.1 Kết quả Tổng thể

Bảng 3 tóm tắt kết quả đánh giá tổng thể của chúng tôi. Chúng tôi sử dụng cùng một mã hóa đầu vào chứa các yếu tố hướng dẫn hiệu quả nhất (định nghĩa nhiệm vụ và hai ví dụ tích cực mà không có ví dụ tiêu cực và giải thích) cho tất cả các phương pháp. Để hiểu rõ hơn khả năng tổng quát hóa của mô hình đối với các nhiệm vụ khác nhau, chúng tôi cũng chia nhỏ hiệu suất theo các danh mục nhiệm vụ trong Hình 4. Chúng tôi giới thiệu độc giả đến Phụ lục H để phân tích chi tiết hơn về từng nhiệm vụ cá nhân.

Instruction-tuning cho phép khả năng tổng quát hóa mạnh mẽ hơn đối với các nhiệm vụ chưa được thấy. Nói chung, các mô hình được điều chỉnh hướng dẫn hoạt động tốt hơn so với các đối tác LM chưa được điều chỉnh của chúng (Tk-INSTRUCT vs. T5-LM, InstructGPT vs. GPT-3) và heuristic baselines. Điều này chỉ ra các mô hình thực sự học để tuân theo hướng dẫn bằng cách fine-tuning trên dữ liệu hướng dẫn, và điều này có thể tổng quát hóa cho các hướng dẫn mới cho các nhiệm vụ chưa được thấy. T0 là một ngoại lệ, chỉ tốt hơn một chút so với

--- TRANG 7 ---

0255075100ROUGE-L43
133496576Textual Entailment
50
1046627579Cause Effect Classification
25
827506686Coreference Resolution
41
232637386Dialogue Act Recognition
53
050577080Answerability Classification
157145055100Word Analogy
0255075100ROUGE-L
152419446084Overlap Extraction
111645576775Keyword Tagging
1363
346570 73Question Rewriting
8 91938 4046Title Generation
2038 36475358Data to Text
787
7486 88 88Grammar Error CorrectionSupervised Training Copying Demo. Output Copying Instance Input T0 (11B) InstructGPT (175B) Tk-Instruct (Ours, 11B)

Hình 4: Hiệu suất theo loại nhiệm vụ đánh giá. Tk-INSTRUCT hoạt động nhất quán tốt hơn các phương pháp tổng quát hóa khác trên tất cả các loại nhiệm vụ, trong khi vẫn còn một khoảng cách đáng kể so với huấn luyện có giám sát.

T5-LM. Chúng tôi nghi ngờ điều này là do phong cách prompting trong dữ liệu huấn luyện của T0 rất khác với phong cách hướng dẫn của chúng tôi.

Tk-INSTRUCT của chúng tôi vượt trội hơn InstructGPT. Các mô hình Tk-INSTRUCT và mTk-INSTRUCT của chúng tôi, được huấn luyện với nhiều nhiệm vụ, tổng quát hóa tốt nhất đối với các nhiệm vụ chưa được thấy cho cả nhiệm vụ tiếng Anh và không phải tiếng Anh trong tất cả các danh mục nhiệm vụ đánh giá. InstructGPT cũng cho thấy mức độ tổng quát hóa lớn đối với các nhiệm vụ đánh giá của chúng tôi. Tuy nhiên, chúng tôi muốn lưu ý rằng không rõ liệu dữ liệu huấn luyện của InstructGPT có chồng chéo với các nhiệm vụ đánh giá của chúng tôi hay không vì dữ liệu của họ không có sẵn.

Có một khoảng cách đáng kể để cải thiện. Mặc dù hiệu suất ấn tượng của các mô hình hiện tại, có một khoảng cách đáng kể giữa khả năng tổng quát hóa của các mô hình dựa trên hướng dẫn và phương pháp huấn luyện có giám sát, để lại nhiều chỗ để cải thiện.

6.2 Đánh giá Con người

Đối với các nhiệm vụ sinh ngôn ngữ, các metrics tự động chỉ là một xấp xỉ của đánh giá con người; chúng tôi tiến hành đánh giá con người để xác nhận những phát hiện cho đến nay. Cụ thể, chúng tôi yêu cầu crowdworkers chỉ ra liệu họ thích câu trả lời dự đoán bởi mô hình hay đầu ra ground truth cho mỗi instance với việc cho phép hòa (xem Phụ lục B để biết chi tiết). Metric đánh giá con người kết quả chỉ ra tần suất dự đoán mô hình được đánh giá ít nhất cũng tốt như nhãn ground truth của chúng tôi. Giới hạn trên lý thuyết của metric này là 100% khi mô hình được đánh giá ít nhất cũng tốt như ground truth cho tất cả các instances. Kết quả của đánh giá con người (được hiển thị trong Hình 3) phù hợp khá tốt với các metrics tự động của chúng tôi và xác nhận chất lượng được con người nhận thức của các mô hình của chúng tôi.

7 Phân tích Thêm

Chúng tôi tiến hành phân tích thêm để hiểu các yếu tố quan trọng cho các mô hình để tổng quát hóa trên các nhiệm vụ. Do chi phí tính toán, phân tích này được thực hiện trên track tiếng Anh và sử dụng checkpoint T5-3B, ngoại trừ các thí nghiệm về kích thước mô hình.

7.1 Xu hướng Mở rộng của Khả năng Tổng quát hóa

Chúng tôi nghiên cứu hiệu suất tổng quát hóa của Tk-INSTRUCT đối với ba yếu tố mở rộng: số lượng nhiệm vụ huấn luyện, số lượng instances mỗi nhiệm vụ và kích thước mô hình. Hình 5 trình bày sự thay đổi hiệu suất bằng cách mở rộng từng yếu tố.

Nhiều nhiệm vụ quan sát được cải thiện khả năng tổng quát hóa. Chúng tôi fine-tune Tk-INSTRUCT với số lượng nhiệm vụ khác nhau được lấy mẫu ngẫu nhiên từ toàn bộ tập huấn luyện (Hình 5a). Hiệu suất tổng quát hóa mô hình tăng log-tuyến tính8 khi chúng tôi tăng tập các nhiệm vụ được sử dụng để huấn luyện. Công trình trước đây (Mishra et al., 2022b; Sanh et al., 2022; Wei et al., 2022) đã có những quan sát tương tự ở quy mô nhỏ hơn nhiều, trong khi chúng tôi cho thấy rằng xu hướng này vẫn giữ nguyên ngay cả với 757 nhiệm vụ huấn luyện đa dạng.

Một số lượng lớn instances huấn luyện không giúp tổng quát hóa. Sau đó chúng tôi thay đổi số lượng instances mỗi nhiệm vụ được sử dụng để fine-tuning (Hình 5b). Trong khi wisdom thông thường trong học có giám sát là nhiều instances huấn luyện thường giúp (Banko and Brill, 2001; Sun et al., 2017; Hestness et al., 2017), trong thiết lập của chúng tôi, hiệu suất của mô hình bão hòa khi chỉ 64 instances mỗi nhiệm vụ được sử dụng để huấn luyện. Một số lượng lớn instances huấn luyện thay vào đó sẽ dẫn đến thời gian huấn luyện dài hơn và nguy cơ overfitting đối với các nhiệm vụ huấn luyện.

8Một hàm tuyến tính của sự tăng üstel của các tham số, tức là, tăng trưởng với tốc độ nhân không đổi.

--- TRANG 8 ---

43.745.648.451.252.954.3
30354045505560
660600ROUGE-L
Số lượng Nhiệm vụ Huấn luyện(a)
48.550.754.753.753.253.7
30354045505560
660600ROUGE-L
Số lượng Instances mỗi Nhiệm vụ Huấn luyện (b)
40.142.148.054.362.0
3035404550556065
404004000ROUGE-L
Số lượng Tham số Mô hình (Triệu) (c)

Hình 5: Xu hướng mở rộng hiệu suất mô hình (§7.1) như một hàm của (a) số lượng nhiệm vụ huấn luyện; (b) số lượng instances mỗi nhiệm vụ huấn luyện; (c) kích thước mô hình. Trục x theo thang logarit. Sự tăng trưởng tuyến tính của hiệu suất mô hình với sự tăng üstel trong các nhiệm vụ quan sát được và kích thước mô hình là một xu hướng đầy hứa hẹn. Rõ ràng, lợi ích hiệu suất từ nhiều instances là hạn chế.

Mã hóa Kiểm tra →
Mã hóa Huấn luyện ↓Task ID Def Pos (1)Def
+ Pos (1) Pos (2) Def
+ Pos (2) Def 
+ Pos (2) 
+ Neg (2) Def
+ Pos (2) 
+ Neg (2) 
+ ExplPos (4) Def 
+ Pos (4) Trung bình

Task ID 21.2 33.3 16.8 30.9 23.0 33.7 33.9 31.6 26.0 36.4 33.9
Def 17.3 45.0 31.1 43.8 36.4 46.4 44.2 44.3 38.0 46.0 39.9
Pos (1) 10.9 22.1 43.9 47.8 46.6 49.2 46.2 43.4 46.6 49.5 43.1
Def + Pos (1) 11.1 42.2 43.8 52.4 47.4 53.3 53.1 51.8 47.8 53.7 44.5
Pos (2) 12.7 22.4 47.1 50.2 49.3 52.3 50.6 46.7 49.8 52.4 45.0
Def + Pos (2) 12.4 42.1 44.5 52.4 49.0 54.3 53.5 52.7 50.3 54.8 46.4
Def + Pos (2) + Neg (2) 14.0 42.3 43.6 51.8 48.6 53.5 54.3 50.2 49.6 53.8 45.9
Def + Pos (2) + Neg (2) + Expl 15.4 42.0 43.8 50.7 47.6 51.9 52.5 52.6 48.6 52.2 44.3
Pos (4) 11.0 23.9 45.6 49.8 49.0 51.7 49.5 47.5 49.8 51.3 44.5
Definition + Pos (4) 11.0 42.4 44.3 51.9 48.7 53.7 53.4 50.6 50.5 53.5 46.0

Bảng 4: Hiệu suất (ROUGE-L) của các mô hình được huấn luyện và đánh giá với các mã hóa khác nhau. Các số trên đường chéo (được gạch chân) đại diện cho hiệu suất của các mô hình được huấn luyện và đánh giá với cùng mã hóa hướng dẫn. Mỗi mã hóa là một kết hợp của các yếu tố trong hướng dẫn (Hình 1). Task ID là một chuỗi ngắn bao gồm tên bộ dữ liệu và danh mục nhiệm vụ; Def đại diện cho định nghĩa nhiệm vụ; Pos (k) đại diện cho k ví dụ tích cực; Neg (k) đại diện cho k ví dụ tiêu cực; Expl đại diện cho giải thích. Những kết quả này (a) cho thấy lợi ích từ các yếu tố hướng dẫn khác nhau, và (b) chỉ ra độ tin cậy đáng ngạc nhiên của các mô hình đối với các mã hóa đầu vào khác nhau. Một mô hình được huấn luyện với định nghĩa và ví dụ tích cực (tức là, hàng cuối) vẫn robust đối với các mã hóa khác nhau.

Điều chỉnh các mô hình lớn hơn với hướng dẫn nhất quán dẫn đến lợi ích. Chúng tôi nghiên cứu tác động của việc mở rộng mô hình bằng cách khởi tạo Tk-INSTRUCT từ các kích thước khác nhau của các checkpoint T5 được đào tạo trước, bao gồm kích thước small, base, large, xl và xxl (Hình 5c). Chúng tôi thấy rằng việc tăng kích thước mô hình nhất quán mang lại cải thiện đáng kể (log-tuyến tính với kích thước tham số). Phát hiện này mâu thuẫn với tuyên bố trong Xu et al. (2022) rằng "kích thước mô hình có ít tác động đến hiệu suất với một số lượng cực lớn các nhiệm vụ." Kết hợp Hình 5(a) và Hình 5(c), ta có thể tạo ra một sự tương ứng giữa kích thước mô hình và kích thước nhiệm vụ. Ví dụ, một mô hình T5-large được huấn luyện với 757 nhiệm vụ có thể đạt được hiệu suất tương đương (48.0 ROUGE-L) với mô hình T5-3B được huấn luyện với 128 nhiệm vụ (48.4 ROUGE-L), chỉ ra rằng việc tăng sự đa dạng của các nhiệm vụ huấn luyện là một thay thế cho việc mở rộng kích thước mô hình.

7.2 Hướng dẫn với Các Yếu tố Khác nhau

Chúng tôi đánh giá hiệu suất của Tk-INSTRUCT dưới các yếu tố hướng dẫn khác nhau.

Lợi ích của các yếu tố hướng dẫn khác nhau. Như được hiển thị trong Hình 1, SUP-NATINST cung cấp nhiều yếu tố để hướng dẫn một nhiệm vụ. Chúng tôi huấn luyện nhiều mô hình với các kết hợp khác nhau của các yếu tố này. Các ô chéo của Bảng 4 cho thấy hiệu suất của các mô hình của chúng tôi khi được huấn luyện và đánh giá trên một mã hóa hướng dẫn cụ thể. Dựa trên các số chéo, việc bao gồm định nghĩa nhiệm vụ nhất quán giúp mô hình tổng quát hóa tốt hơn. Hơn nữa, việc kết hợp định nghĩa nhiệm vụ với các ví dụ minh họa tích cực mang lại cải thiện thêm. Tuy nhiên, việc thêm nhiều ví dụ minh họa hơn là không đáng kể. Các ví dụ tiêu cực giúp một chút; giải thích làm giảm hiệu suất, điều này phù hợp với các quan sát của Mishra et al. (2022b) và Lampinen et al. (2022) khi

--- TRANG 9 ---

mô hình không đủ lớn. Công trình tương lai có thể khám phá liệu các mô hình mạnh mẽ hơn có thể hưởng lợi từ các yếu tố này.

Khả năng tổng quát hóa đối với các mã hóa đầu vào khác nhau. Chúng tôi tiếp tục điều tra liệu một mô hình được huấn luyện trên một mã hóa cụ thể có thể tổng quát hóa cho các mã hóa khác. Điều này có thể được đọc từ các ô không chéo của Bảng 4. Kết quả tiêu cực ở đây là các mô hình chỉ định nghĩa không thể tổng quát hóa cho các mã hóa kiểm tra chỉ ví dụ; và tương tự, các mô hình chỉ ví dụ không thể tổng quát hóa cho các mã hóa kiểm tra chỉ định nghĩa. Tuy nhiên, các mô hình được huấn luyện trên các mã hóa chứa cả định nghĩa và ví dụ đáng ngạc nhiên robust trên các biến thể mã hóa khác nhau.

8 Kết luận

Chúng tôi xây dựng một điểm chuẩn quy mô lớn bao gồm một tập đa dạng các nhiệm vụ NLP và hướng dẫn của chúng. Điểm chuẩn này có thể phục vụ như một sân chơi phong phú để huấn luyện hoặc đánh giá các mô hình có thể tổng quát hóa cho các nhiệm vụ chưa được thấy bằng cách tuân theo hướng dẫn. Hơn nữa, chúng tôi huấn luyện Tk-INSTRUCT sử dụng dữ liệu này, và chứng minh khả năng của nó để thực hiện các nhiệm vụ chưa được thấy đến mức đáng ngạc nhiên. Chúng tôi cung cấp phân tích mở rộng để hiểu các yếu tố quan trọng cho khả năng tổng quát hóa như vậy. Chúng tôi hy vọng dữ liệu và mô hình của mình sẽ tạo điều kiện cho công trình tương lai hướng tới các mô hình có mục đích tổng quát hơn.

9 Hạn chế

Mặc dù dữ liệu được trình bày cung cấp một sự đa dạng đáng chú ý (ví dụ: các loại nhiệm vụ đa dạng), các phân phối cơ bản của nó chịu sự lệch mà nên được giải quyết trong công trình tương lai (xem Phụ lục F). Về sự đa dạng ngôn ngữ, điểm chuẩn được đề xuất thiên về tiếng Anh. Về sự đa dạng đầu ra, các nhiệm vụ được thu thập nói chung vẫn lệch về phản hồi ngắn, điều này có thể phản ánh sự phân phối của các nhiệm vụ có sẵn trong lĩnh vực. Việc đại diện thiếu của long-tail các nhiệm vụ này đặt ra một thách thức cho việc xây dựng các mô hình có mục đích tổng quát trong tương lai. Chúng tôi hy vọng công trình tương lai sẽ giải quyết những mất cân bằng phân phối như vậy. Hơn nữa, chúng tôi thấy các mở rộng tự nhiên của thiết lập tuân theo hướng dẫn ở đây trong bối cảnh của các phương thức khác như thị giác hoặc giọng nói.

Đánh giá tự động hiệu suất của mô hình là một thách thức khác, xem xét tập đa dạng các nhiệm vụ trong điểm chuẩn của chúng tôi, và nhiều trong số chúng là các nhiệm vụ sinh mở. Chúng tôi sử dụng ROUGE-L như một metric tổng hợp trong bài báo này và thấy nó như một proxy tốt cho hiệu suất tổng thể của các mô hình, phù hợp tốt với đánh giá con người. Tuy nhiên, có các nhiệm vụ cụ thể mà ROUGE-L có thể không phục vụ như một proxy hiệu quả của chất lượng (chẳng hạn như các nhiệm vụ viết lại hoặc sửa lỗi mà việc sao chép đầu vào có thể dẫn đến điểm ROUGE-L cao). Chúng tôi hy vọng những vấn đề này sẽ được giải quyết với việc phát triển các metrics đánh giá mạnh mẽ hơn cho sinh văn bản.

Về mặt sức mạnh tính toán, chúng tôi đã thử nghiệm với các mô hình mà chúng tôi có thể tiếp cận và đã công khai các mô hình kết quả. Chúng tôi cũng thừa nhận rằng có những mô hình lớn hơn mà chúng tôi không thể huấn luyện do hạn chế của ngân sách tính toán của chúng tôi.

Lời cảm ơn

Chúng tôi cảm ơn các reviewer ẩn danh, các đồng nghiệp từ AI2 và UWNLP, đặc biệt là Matthew Peters vì những cuộc trò chuyện khuyến khích đã thúc đẩy dự án này. Chúng tôi cũng cảm ơn các sinh viên đóng góp của khóa học CSE 576 "Topics in NLP" của Arizona State University và tất cả các người đóng góp khác cho repository dữ liệu của chúng tôi. Tất cả các thí nghiệm đã được chạy trên các cụm GPU Beaker của AI2 và TPUs nghiên cứu của Google. Công trình này được hỗ trợ một phần bởi các khoản tài trợ ONR MURI N00014-18-1-2670, ONR N00014-18-1-2826, và DARPA MCS N66001-19-2-4031.

Tài liệu tham khảo

[Các tài liệu tham khảo được liệt kê theo định dạng học thuật tiêu chuẩn...]

--- TRANG 10 ---

[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 11 ---

[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 12 ---

[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 13 ---

Tài liệu Bổ sung

A Crowdsourcing Phản hồi Con người

Chúng tôi sử dụng Amazon Mechanical Turk (AMT) để crowdsource phản hồi về chất lượng của các hướng dẫn được thu thập. Chúng tôi giới hạn crowdworkers của mình ở các quốc gia nói tiếng Anh chính (Mỹ, Anh, Canada và Australia), và những người đã hoàn thành hơn 1K HITs với tỷ lệ chấp thuận trên 99%.

Hình 6 hiển thị template crowdsourcing được sử dụng để thu thập phản hồi crowdworker về hướng dẫn của chúng tôi. Chúng tôi hiển thị các hướng dẫn (định nghĩa nhiệm vụ, cùng với ví dụ tích cực và tiêu cực) theo sau là các form cho phản hồi của họ. Chúng tôi cho phép crowdworkers cung cấp cho chúng tôi một biện pháp định tính về chất lượng được nhận thức của họ cũng như các hộp văn bản cho các mục cụ thể hơn (chẳng hạn như lỗi chính tả hoặc cách diễn đạt có thể được hưởng lợi từ sự rõ ràng hơn). Đối với mỗi nhiệm vụ, chúng tôi xin phản hồi của 3 crowdworkers và sau đó sử dụng phản hồi này để cải thiện định nghĩa nhiệm vụ hoặc các ví dụ cho mỗi nhiệm vụ.

B Crowdsourcing Đánh giá Con người về Chất lượng Sinh

Chúng tôi thực hiện một thí nghiệm crowdsourcing trên Amazon Mechanical Turk (AMT) để đánh giá chất lượng của các phản hồi được sinh ra bởi các mô hình. Cụ thể, chúng tôi yêu cầu crowdworkers chỉ ra liệu họ thích câu trả lời dự đoán bởi mô hình hay đầu ra ground truth cho mỗi instance. Giao diện chú thích được hiển thị trong Hình 7. Về cơ bản nó là cùng template được sử dụng cho đánh giá chất lượng của bộ dữ liệu (§A), ngoại trừ ở đây crowdworkers được hiển thị một cặp phản hồi cho mỗi instance—văn bản tham chiếu (từ điểm chuẩn của chúng tôi) và cái được sinh ra bởi mô hình—biến nhiệm vụ thành một đánh giá so sánh.

Đối với mỗi instance, chúng tôi nhận được chú thích từ một annotator về việc họ thích phản hồi nào hơn cái kia hoặc họ sẽ đánh giá chúng như nhau ("hòa"). Mô hình nhận được tín dụng 1.0 nếu worker ưu tiên dự đoán của mô hình ít nhất cũng tốt như nhãn ground truth (ngược lại, mô hình sẽ nhận được tín dụng 0.0). Điểm độ chính xác tổng thể cho mô hình được tính bằng cách tính trung bình điểm cấp instance. Để giảm chi phí, đánh giá con người của các mô hình của chúng tôi được thực hiện trên 60 nhiệm vụ được chọn ngẫu nhiên (khoảng một nửa các nhiệm vụ đánh giá của chúng tôi), và trên 10 instances ngẫu nhiên của mỗi nhiệm vụ.

Vì việc tìm crowdworkers không nói tiếng Anh là không đơn giản (Pavlick et al., 2014), đánh giá này được giới hạn ở các nhiệm vụ tiếng Anh. Do đó, vì nhiệm vụ của chúng tôi tập trung vào các nhiệm vụ tiếng Anh, chúng tôi yêu cầu workers có trụ sở tại một quốc gia có dân số chủ yếu là người bản xứ nói tiếng Anh (Mỹ, Canada, Anh và Australia) và đã hoàn thành ít nhất 5000 HITs với tỷ lệ chấp thuận assignment 99%.

Metric đánh giá con người kết quả chỉ ra tần suất dự đoán mô hình được đánh giá ngang bằng hoặc được ưa chuộng hơn so với nhãn ground truth của chúng tôi. Trong đánh giá này, giới hạn trên lý thuyết là 100% khi mô hình được đánh giá ít nhất cũng tốt như ground truth. Kết quả của đánh giá con người được hiển thị trong hàng dưới của Hình 3.

C Lược đồ Hướng dẫn

Lược đồ hướng dẫn của chúng tôi dựa trên của NATINST (Mishra et al., 2022b), nhưng chúng tôi đơn giản hóa nó để làm cho việc thu thập dữ liệu dễ dàng hơn. Trường DEFINITION của chúng tôi phục vụ như sự kết hợp của DEFINITION, THINGS TO AVOID và EMPHASIS & CAUTION của Mishra et al. (2022b). Ngoài ra, chúng tôi bỏ TITLE và PROMPT của họ vì nội dung của chúng thường được bao gồm bởi DEFINITION.

D Chi tiết Triển khai Mô hình

Thí nghiệm T5. Chúng tôi sử dụng T5 để huấn luyện Tk-INSTRUCT của chúng tôi, ước tính hiệu suất của phương pháp có giám sát và tiến hành phân tích.

Các thí nghiệm của chúng tôi fine-tune mô hình T5-11B được tiến hành dựa trên thư viện T5 của Google9 và chúng tôi sử dụng checkpoint T5.1.1.xxl10 của họ theo mặc định, được đào tạo trước chỉ trên C4.11 Những thí nghiệm này được chạy trên Google V3-256 TPUs sử dụng batch size 1.048.576 tokens (1.024 ví dụ), learning rate không đổi 1e-5 và tổng cộng 1000 bước. Mỗi lần chạy huấn luyện mất 4 giờ để hoàn thành.

Các phân tích của chúng tôi sử dụng các mô hình T5 nhỏ hơn 11B tham số được tiến hành dựa trên thư viện transformers của Huggingface và các checkpoint mô hình12 (Wolf et al., 2020) trên các máy GPU.

9https://github.com/google-research/text-to-text-transfer-transformer
10https://console.cloud.google.com/storage/browser/t5-data/pretrained_models/t5.1.1.xxl
11Chúng tôi cũng đã thử fine-tune Tk-INSTRUCT từ checkpoint T5-LM nhưng hiệu suất cuối cùng thấp hơn. Do đó, chúng tôi quyết định sử dụng checkpoint T5.1.1.xxl.
12https://huggingface.co/models?sort=downloads&search=google%2Ft5

--- TRANG 14 ---

[Hình 6: Template crowdsourcing chúng tôi sử dụng để nhận phản hồi về các nhiệm vụ được thu thập của chúng tôi.]

[Hình 7: Giao diện crowdsourcing được sử dụng để đánh giá con người về các baseline của chúng tôi (§6.2).]

Khi fine-tuning các mô hình, chúng tôi huấn luyện chúng trong hai epochs với batch size 16 và learning rate không đổi 1e-5. Độ dài đầu vào tối đa được đặt thành 1024, và độ dài đầu ra tối đa được đặt thành 128. Những thí nghiệm này được tiến hành với 8 A100 GPUs với bộ nhớ GPU 48GB mỗi cái. Chúng tôi sử dụng DeepSpeed13 cho song song hóa mô hình, với độ chính xác bfloat16 được kích hoạt để tiết kiệm bộ nhớ GPU. Mỗi lần chạy huấn luyện mất 6 giờ để hoàn thành.

Thí nghiệm GPT-3 và InstructGPT. Chúng tôi sử dụng OpenAI API14 để tiến hành các thí nghiệm GPT-3. Chúng tôi sử dụng engine "davinci" của họ cho các thí nghiệm mô hình ngôn ngữ GPT-3 và engine "text-davinci-001" của họ cho các thí nghiệm InstructGPT. Khi tạo yêu cầu, chúng tôi đặt temperature là 0, top_p là 1 và độ dài sinh tối đa là 128. Do chi phí cao, chúng tôi lấy mẫu ngẫu nhiên 20 instances từ mỗi trong 119 nhiệm vụ kiểm tra của chúng tôi để ước tính hiệu suất của GPT-3 và InstructGPT. Tất cả yêu cầu API đã được thực hiện vào ngày 30 tháng 5 năm 2022.

Mã hóa hướng dẫn với đầu vào Đối với mọi thiết lập bài toán, chúng tôi ánh xạ một hướng dẫn đã cho It và một instance đầu vào x thành một định dạng văn bản, thu được enc(It; x). Mỗi hướng dẫn It bao gồm nhiều yếu tố như được mô tả trong lược đồ hướng dẫn của chúng tôi (§3). Chúng tôi ánh xạ mỗi yếu tố của hướng dẫn thành một định dạng văn bản và thêm nó vào instance đầu vào. Hình 8 cho thấy cách chúng tôi mã hóa hướng dẫn đầy đủ. Chúng tôi nghiên cứu các kết hợp khác nhau của các yếu tố hướng dẫn này trong §7.2. Instance được mã hóa sau đó được đưa vào một mô hình encoder-decoder để dự đoán y: M: enc(It; x) → y.

Định nghĩa: {{định nghĩa}}
Ví dụ Tích cực 1
đầu vào: {{p_ex1:đầu vào}}
đầu ra: {{p_ex1:đầu ra}}
giải thích: {{p_ex1:exp}}
Ví dụ Tích cực 2
...
Ví dụ Tiêu cực 1
đầu vào: {{n_ex1:đầu vào}}
đầu ra: {{n_ex1:đầu ra}}
giải thích: {{n_ex1:exp}}
Ví dụ Tiêu cực 2
...
Bây giờ hoàn thành ví dụ sau
đầu vào: {{x:đầu vào}}
đầu ra:

Hình 8: Mã hóa hướng dẫn nhiệm vụ với đầu vào.

E Metrics Đánh giá

Chúng tôi áp dụng ROUGE-L như metric đánh giá tự động trong công trình này. Tuy nhiên, vẫn còn một câu hỏi về mức độ ROUGE-L có thể phản ánh hiệu suất của mô hình trên các nhiệm vụ khác nhau. Mặc dù chúng tôi không thể kiểm tra mối tương quan của ROUGE-L với mỗi metric cụ thể nhiệm vụ của các nhiệm vụ được bao gồm trong dữ liệu của chúng tôi, chúng tôi điều tra liệu ROUGE-L có thể được sử dụng cho các nhiệm vụ phân loại. Hình 9 vẽ điểm ROUGE-L và độ chính xác của một số mô hình trên các loại nhiệm vụ khác nhau. Những loại nhiệm vụ này thường được coi là các nhiệm vụ phân loại và có đầu ra ground truth rất ngắn. Chúng ta có thể thấy rằng đối với tất cả các loại nhiệm vụ này, xu hướng của ROUGE-L tương quan tốt với xu hướng của độ chính xác. Đối với một số loại nhiệm vụ, chúng ta thực sự thấy một số khoảng cách giữa hai metrics này. Lý do là vì có một số nhiệm vụ sinh được phân loại vào các loại này. Những kết quả này chỉ ra rằng ROUGE-L là một proxy tốt cho độ chính xác đối với các nhiệm vụ phân loại.

F Phân phối Nhiệm vụ

Như được mô tả trong §3, SUP-NATINST cung cấp chú thích để phân loại các nhiệm vụ theo ba chiều khác nhau: loại nhiệm vụ, ngôn ngữ và miền. Hình 10 hiển thị sự phân phối của các nhiệm vụ giữa ba chiều này. Thông tin meta này có thể được sử dụng để nghiên cứu khả năng tổng quát hóa của mô hình theo các nghĩa khác nhau. Mặc dù sự đa dạng của dữ liệu, chúng tôi thừa nhận sự lệch về một số nhiệm vụ và ngôn ngữ nhất định, mà chúng tôi để lại để được giải quyết bởi công trình tương lai.

G Nhiệm vụ Đánh giá

Bảng 5 liệt kê 12 danh mục nhiệm vụ được sử dụng cho đánh giá của chúng tôi và tất cả các nhiệm vụ được bao gồm trong mỗi danh mục (được giới thiệu trong §5.1). Để cung cấp cảm nhận tốt hơn về những nhiệm vụ đó trông như thế nào, chúng tôi cũng chọn một nhiệm vụ đại diện từ mỗi danh mục và liệt kê chúng trong Bảng 6–17. Do số lượng lớn các nhiệm vụ trong bộ dữ liệu của chúng tôi, chúng tôi không thể liệt kê tất cả 1.616 nhiệm vụ trong bài báo này. Chúng tôi giới thiệu độc giả đến bộ dữ liệu của chúng tôi.

H Cải thiện Hiệu suất trên mỗi Nhiệm vụ Đánh giá

Để cung cấp phân tích chi tiết hơn về Tk-INSTRUCT trên từng nhiệm vụ cá nhân, Hình 11 trình bày cải thiện từng nhiệm vụ của mô hình Tk-INSTRUCT (3B) của chúng tôi so với tốt nhất của hai heuristic baselines trên các nhiệm vụ đánh giá tiếng Anh, và Hình 12 trình bày cải thiện từng nhiệm vụ của mô hình mTk-INSTRUCT trên các nhiệm vụ đánh giá xuyên ngôn ngữ. Đối với hầu hết các nhiệm vụ đánh giá, chúng ta thấy một mức độ tổng quát hóa đáng chú ý bởi Tk-INSTRUCT.

--- TRANG 16 ---

[Các biểu đồ so sánh ROUGE-L vs Accuracy cho các loại nhiệm vụ khác nhau]

Hình 9: Rouge-L v.s. Accuracy cho các loại nhiệm vụ thường được coi là các nhiệm vụ phân loại. Xu hướng của hai metrics này có mối tương quan cao với hệ số Pearson là 0.970.

[Biểu đồ phân phối nhiệm vụ theo (a) Loại Nhiệm vụ (b) Ngôn ngữ (c) Miền]

Hình 10: Phân phối các nhiệm vụ SUP-NATINST về (a) loại nhiệm vụ (b) ngôn ngữ (c) miền của chúng. Trục y theo thang logarit.

--- TRANG 17 ---

[Bảng 5: 12 Danh mục Đánh giá với danh sách chi tiết các nhiệm vụ trong mỗi danh mục]

--- TRANG 18 đến 22 ---

[Các bảng 6-17: Ví dụ chi tiết về các nhiệm vụ từ mỗi danh mục đánh giá]

--- TRANG 23 đến 25 ---

[Hình 11 và 12: Biểu đồ hiển thị cải thiện hiệu suất của Tk-INSTRUCT trên từng nhiệm vụ so với heuristic baselines]
