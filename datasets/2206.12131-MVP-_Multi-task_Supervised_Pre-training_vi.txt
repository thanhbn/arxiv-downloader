# 2206.12131.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/datasets/2206.12131.pdf
# Kích thước tệp: 738831 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MVP: Tiền huấn luyện có giám sát đa nhiệm
cho Sinh ngôn ngữ tự nhiên
Tianyi Tang1,4, Junyi Li1,3, Wayne Xin Zhao1,4Band Ji-Rong Wen1,2,4
1Trường Trí tuệ nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
2Trường Thông tin, Đại học Nhân dân Trung Quốc
3DIRO, Đại học Montréal
4Phòng thí nghiệm chính Bắc Kinh về Phương pháp Quản lý và Phân tích Dữ liệu lớn
steventianyitang@outlook.com lijunyi@ruc.edu.cn batmanfly@gmail.com
Tóm tắt
Các mô hình ngôn ngữ tiền huấn luyện (PLM) đã đạt được thành công đáng kể trong các nhiệm vụ sinh ngôn ngữ tự nhiên (NLG). Cho đến nay, hầu hết các PLM hướng NLG được tiền huấn luyện theo cách không giám sát sử dụng kho ngữ liệu chung quy mô lớn. Trong khi đó, một số lượng ngày càng tăng các mô hình tiền huấn luyện với dữ liệu có nhãn (tức là "tiền huấn luyện có giám sát") thể hiện hiệu suất vượt trội so với các mô hình tiền huấn luyện không giám sát. Được thúc đẩy bởi thành công của tiền huấn luyện có giám sát, chúng tôi đề xuất Tiền huấn luyện có giám sát đa nhiệm (MVP) cho sinh ngôn ngữ tự nhiên. Chúng tôi thu thập kho ngữ liệu sinh ngôn ngữ tự nhiên quy mô lớn, MVPCorpus, từ 77 tập dữ liệu trên 11 nhiệm vụ NLG đa dạng. Sau đó chúng tôi thống nhất các ví dụ này thành định dạng văn bản-sang-văn bản chung để tiền huấn luyện mô hình sinh văn bản MVP theo cách có giám sát. Đối với mỗi nhiệm vụ, chúng tôi tiến hành tiền huấn luyện thêm các prompt mềm cụ thể để kích thích khả năng của mô hình thực hiện một nhiệm vụ cụ thể. Mô hình MVP của chúng tôi có thể được xem như một thực hành sử dụng instruction tuning gần đây trên các PLM tương đối nhỏ. Các thí nghiệm rộng rãi đã chứng minh tính hiệu quả và tổng quát của mô hình MVP trong một số nhiệm vụ NLG, đạt hiệu suất tiên tiến trên 13 trong số 17 tập dữ liệu, vượt trội hơn BART 9,3% và Flan-T5 5,8%.

1 Giới thiệu
Sinh ngôn ngữ tự nhiên (NLG, còn được gọi là sinh văn bản) là một khả năng quan trọng đối với trí tuệ ngôn ngữ, nhằm sinh ra các văn bản giống con người theo yêu cầu (Garbacea và Mei, 2020). Kể từ khi xuất hiện mô hình tiền huấn luyện và fine-tuning, các mô hình ngôn ngữ tiền huấn luyện (PLM) đã thống trị các phương pháp chủ lưu cho các nhiệm vụ NLG (Lewis et al., 2020; Brown et al., 2020). Với kho ngữ liệu chung quy mô lớn, đa số PLM được tiền huấn luyện theo cách không giám sát (tự giám sát) bằng cách tận dụng các mối tương quan dữ liệu nội tại làm tín hiệu giám sát. Tuy nhiên, tiền huấn luyện không giám sát có thể kết hợp nhiễu ảnh hưởng đến hiệu suất của các nhiệm vụ downstream (Feng et al., 2022), cũng dẫn đến tốc độ tiếp thu kiến thức chậm hơn (Zhang et al., 2021).

Trong khi đó, ngày càng nhiều tập dữ liệu có nhãn quy mô lớn trở nên dễ tiếp cận (Deng et al., 2009; Liu et al., 2020). Có bằng chứng ngày càng tăng rằng tiền huấn luyện với dữ liệu có nhãn có thể cải thiện thêm hiệu suất của PLM, cả trong lĩnh vực thị giác máy tính (He et al., 2016; Dosovitskiy et al., 2021) và xử lý ngôn ngữ tự nhiên (Lin et al., 2020b; Su et al., 2022). Những phát triển đầy hứa hẹn này thúc đẩy chúng tôi xem xét tiền huấn luyện các mô hình sinh văn bản với dữ liệu có nhãn, được gọi là "tiền huấn luyện có giám sát" (Feng et al., 2022). Các nghiên cứu hiện tại đã cho thấy tiền huấn luyện có giám sát có thể học rõ ràng các đặc tính cụ thể cho nhiệm vụ và giảm thiểu sự khác biệt giữa tiền huấn luyện không giám sát và fine-tuning có giám sát (Lin et al., 2020b).

Hơn nữa, hầu hết các hệ thống NLG thường được huấn luyện theo cách có giám sát, đòi hỏi tín hiệu giám sát để học chuyển đổi đầu vào-đầu ra. Ví dụ, các hệ thống đối thoại học sinh ra phản hồi phù hợp dựa trên các phát ngôn lịch sử, và các hệ thống tóm tắt văn bản học trích xuất thông tin quan trọng từ các tài liệu dài theo các bản tóm tắt được con người viết. Do đó, chúng tôi nghi ngờ rằng tiền huấn luyện có giám sát phù hợp hơn với các PLM hướng NLG về bản chất vì nó có thể cung cấp hướng dẫn liên quan đến nhiệm vụ ngay từ giai đoạn tiền huấn luyện thay vì giai đoạn fine-tuning sau này.

Được truyền cảm hứng bởi thành công gần đây của tiền huấn luyện có giám sát, chúng tôi đề xuất Tiền huấn luyện có giám sát đa nhiệm (MVP) cho sinh ngôn ngữ tự nhiên bằng cách tận dụng nhiều tập dữ liệu sinh văn bản có nhãn. Cụ thể, chúng tôi thu thập một kho ngữ liệu có nhãn quy mô lớn, MVPCorpus, bao gồm 77 tập dữ liệu trên 11 nhiệm vụ sinh văn bản. Vì nghiên cứu gần đây cho thấy quy mô rộng lớn của tiền huấn luyện đa nhiệm (Aribandi et al., 2022) là chìa khóa để tổng quát hóa cho các nhiệm vụ mới đối với các PLM lớn, chúng tôi kết hợp các tập dữ liệu có nhãn này để tiền huấn luyện đa nhiệm. Các nghiên cứu phổ biến hiện tại, như được trình bày trong Bảng 1, chủ yếu tập trung vào các nhiệm vụ NLU (Sanh et al., 2022; Aribandi et al., 2022) hoặc sử dụng tiền huấn luyện không giám sát (Lewis et al., 2020; Raffel et al., 2020), không xem xét tiền huấn luyện có giám sát trên các nhiệm vụ NLG. Để lấp đầy khoảng trống này, chúng tôi khám phá tiền huấn luyện có giám sát và học đa nhiệm để tạo ra các mô hình NLG vừa hiệu quả vừa tổng quát.

Để phát triển phương pháp của chúng tôi, chúng tôi áp dụng mô hình sequence-to-sequence dựa trên Transformer (Vaswani et al., 2017) làm xương sống. Trong huấn luyện đa nhiệm, các nhiệm vụ khác nhau có thể "trung hòa" khả năng học được thông qua các nhiệm vụ khác (He và Choi, 2021). Để giảm thiểu vấn đề tiềm ẩn này, chúng tôi đề xuất học các prompt cụ thể cho nhiệm vụ dựa trên mô hình MVP, theo cấu trúc của prefix-tuning (Li và Liang, 2021). Tiền huấn luyện cụ thể cho nhiệm vụ cho phép các prompt "lưu trữ" kiến thức chuyên biệt cho mỗi nhiệm vụ tương ứng. Việc tích hợp MVP với các prompt cụ thể cho nhiệm vụ có thể kích thích thêm khả năng của mô hình để thực hiện một số nhiệm vụ cụ thể.

Để tóm tắt, các đóng góp chính của chúng tôi tập trung vào các câu hỏi nghiên cứu sau:
• Làm thế nào để huấn luyện một PLM hướng NLG theo cách tiền huấn luyện có giám sát? Để chuẩn bị kho ngữ liệu có giám sát, chúng tôi thu thập MVPCorpus có nhãn khổng lồ, bao gồm 77 tập dữ liệu trên 11 nhiệm vụ NLG qua các miền và mục tiêu cụ thể khác nhau. Theo hiểu biết của chúng tôi, MVPCorpus là bộ sưu tập lớn nhất các tập dữ liệu NLG. Đầu tiên, chúng tôi xây dựng các nhiệm vụ NLG khác nhau dưới dạng văn bản-sang-văn bản chung sử dụng hướng dẫn nhiệm vụ để kho ngữ liệu có giám sát có thể được sử dụng theo cách thống nhất để tiền huấn luyện mô hình NLG. Nghiên cứu của chúng tôi trình bày một phương pháp đơn giản nhưng tổng quát để tiền huấn luyện mô hình NLG có khả năng hơn bằng cách tận dụng các tập dữ liệu NLG có nhãn khác nhau.

• Các mô hình NLG tiền huấn luyện có giám sát có thể vừa hiệu quả vừa tổng quát không? Các thí nghiệm rộng rãi cho thấy MVP tiền huấn luyện có giám sát vượt trội hơn đối tác tiền huấn luyện không giám sát BART trong cả thiết lập full tuning (+9,3% theo tỷ lệ) và parameter-efficient tuning (+4,3% theo tỷ lệ). Mô hình MVP của chúng tôi đạt hiệu suất tiên tiến trên 13 trong số 17 tập dữ liệu và vượt trội hơn Flan-T5 (Chung et al., 2022) 5,8%. Hiệu suất zero-shot của chúng tôi cũng vượt trội hơn T0-11B (Sanh et al., 2022) với biên độ lớn. Hơn nữa, các thí nghiệm trên các nhiệm vụ NLG và NLU chưa thấy chứng minh rằng mô hình MVP có giám sát của chúng tôi có tính tổng quát mạnh mẽ cho các nhiệm vụ chưa thấy.

Để tái tạo và tái sử dụng nghiên cứu của chúng tôi, chúng tôi phát hành bộ sưu tập MVPCorpus, tất cả các biến thể mô hình MVP, và mã tương ứng tại liên kết: https://github.com/RUCAIBox/MVP.

2 Nghiên cứu liên quan
Mô hình ngôn ngữ tiền huấn luyện. Các mô hình ngôn ngữ tiền huấn luyện đã đạt được thành công đặc biệt trong một loạt các nhiệm vụ, và đa số chúng được tiền huấn luyện theo cách không giám sát (Devlin et al., 2019; Brown et al., 2020). Ví dụ, với văn bản thuần quy mô lớn làm kho ngữ liệu tiền huấn luyện không giám sát (~570GB), GPT-3 (Brown et al., 2020) sử dụng mô hình hóa ngôn ngữ làm nhiệm vụ tiền huấn luyện, tức là dự đoán token tiếp theo có điều kiện trên các token trước đó. Trong khi đó, cộng đồng thị giác máy tính hưởng lợi rất nhiều từ tập dữ liệu có nhãn ImageNet (Deng et al., 2009). Các mô hình có ảnh hưởng, như ResNet (He et al., 2016) và ViT (Dosovitskiy et al., 2021), tận dụng ImageNet để tiền huấn luyện. Được truyền cảm hứng bởi thành công của tiền huấn luyện với dữ liệu có nhãn, các nhà nghiên cứu dịch máy khám phá tiền huấn luyện có giám sát (McCann et al., 2017; Lin et al., 2020b). Lin et al. (2020b) thử nghiệm tiền huấn luyện mô hình dịch với dữ liệu song song đa ngôn ngữ. Mặc dù sử dụng ít dữ liệu tiền huấn luyện hơn nhiều, mRASP vẫn đạt hiệu suất tốt hơn các mô hình dịch tiền huấn luyện không giám sát (Liu et al., 2020). Trong bài báo này, chúng tôi đề xuất tiền huấn luyện mô hình NLG phổ quát theo cách có giám sát với các bộ sưu tập tập dữ liệu có nhãn (~23GB).

Học đa nhiệm. Quá trình tiền huấn luyện của chúng tôi cũng liên quan đến học đa nhiệm (MTL), một phương pháp trộn nhiều nhiệm vụ thành một quá trình huấn luyện duy nhất (Collobert và Weston, 2008). Một mô hình được huấn luyện với MTL có thể hưởng lợi từ kiến thức hữu ích của các nhiệm vụ liên quan, dẫn đến hiệu suất được cải thiện (Subramanian et al., 2018). Gần đây, MT-DNN (Liu et al., 2019a) và Muppet (Aghajanyan et al., 2021) thu thập hàng chục tập dữ liệu trong quy trình đa nhiệm và đạt hiệu suất tốt hơn trong các nhiệm vụ downstream. Lược đồ pre-finetuning được đề xuất trong Muppet chia sẻ ý tưởng tương tự với nghiên cứu của chúng tôi. Aribandi et al. (2022) tiến hành kết hợp nhiệm vụ tiền huấn luyện denoising của T5 (Raffel et al., 2020) và học đa nhiệm để tiền huấn luyện mô hình mới, ExT5. MTL cũng đã đóng góp cho các tiểu lĩnh vực sinh văn bản, như hệ thống đối thoại mở (Zhang et al., 2020), hệ thống đối thoại hướng nhiệm vụ (Su et al., 2022), chuyển đổi phong cách văn bản (Bujnowski et al., 2020), và trả lời câu hỏi (Khashabi et al., 2020). Đồng thời, các nhà nghiên cứu khám phá khả năng chuyển giao của các mô hình được huấn luyện trên các tập dữ liệu đa nhiệm (Mishra et al., 2022). FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), ZeroPrompt (Xu et al., 2022), và FLAN-T5 (Chung et al., 2022) điều tra khả năng tổng quát hóa zero-shot hoặc few-shot của các mô hình ngôn ngữ lớn (LLM) (Zhao et al., 2023) được huấn luyện trên nhiều tập dữ liệu nhiệm vụ với các prompt được thiết kế tốt. So với các nghiên cứu này, chúng tôi nhằm khám phá học đa nhiệm để tạo ra các mô hình NLG vừa hiệu quả vừa tổng quát theo cách tiền huấn luyện có giám sát.

Học prompt. Học prompt là một phương pháp phát triển mạnh trong lĩnh vực NLP. Học prompt chuyển đổi văn bản fine-tuning thành định dạng tương tự như tiền huấn luyện để tận dụng kiến thức tiền huấn luyện ngầm và giảm thiểu sự khác biệt giữa tiền huấn luyện và fine-tuning (Liu et al., 2021b). GPT-2 (Radford et al., 2019) và T5 (Raffel et al., 2020) thêm các prompt nhiệm vụ do con người viết vào văn bản đầu vào. Ví dụ, T5 thêm "Summarize: " vào đầu tài liệu đầu vào cho các nhiệm vụ tóm tắt. Một số nhà nghiên cứu cũng thiết kế các prompt phức tạp cho mỗi nhiệm vụ và tập dữ liệu và điều tra tính hiệu quả và độ bền của chúng (Wei et al., 2022; Sanh et al., 2022). Để vượt qua các ràng buộc của các prompt được xây dựng thủ công, các nhà nghiên cứu phát triển các prompt liên tục (mềm) có thể được tối ưu hóa trong không gian liên tục (Lester et al., 2021; Qin và Eisner, 2021; Tang et al., 2022b). Xem xét việc khởi tạo ngẫu nhiên của các prompt mềm, Gu et al. (2022) đề xuất PPT để tiền huấn luyện các prompt liên tục sử dụng dữ liệu không nhãn. SPoT (Vu et al., 2022), UnifiedSKG (Xie et al., 2022), và PTG (Li et al., 2022a) tiến hành học các prompt trên các nhiệm vụ liên quan và chuyển giao các prompt cho các nhiệm vụ mới.

3 Mô hình MVP
Phần này giới thiệu mô hình MVP của chúng tôi: một mô hình tiền huấn luyện có giám sát đa nhiệm cho sinh ngôn ngữ tự nhiên. Tổng quan về mô hình của chúng tôi được minh họa trong Hình 1.

3.1 Thu thập dữ liệu
Chính thức, nhiệm vụ sinh ngôn ngữ tự nhiên (NLG) nhằm sinh ra một chuỗi token Y = (y1, y2, ..., yn) có điều kiện trên dữ liệu đầu vào X (ví dụ: một đoạn văn bản hoặc dữ liệu có cấu trúc) (Li et al., 2022b). Trong bài báo này, chúng tôi thu thập MVPCorpus có nhãn quy mô lớn bao gồm 77 tập dữ liệu có nhãn từ 11 nhiệm vụ NLG đại diện1, bao gồm sinh thông thường, sinh dữ liệu-sang-văn bản, hệ thống đối thoại mở, sinh paraphrase, trả lời câu hỏi, sinh câu hỏi, sinh truyện, hệ thống đối thoại hướng nhiệm vụ, đơn giản hóa văn bản, chuyển đổi phong cách văn bản, và tóm tắt văn bản. Các tập dữ liệu này đến từ nhiều miền khác nhau và có kích thước khác nhau. Một số tập dữ liệu được xây dựng thủ công tỉ mỉ và do đó tương đối nhỏ về kích thước, trong khi những tập khác được tạo ra để giám sát yếu quy mô lớn. Mô tả chi tiết của các nhiệm vụ này có thể tìm thấy trong Phụ lục A.1.

Tiếp theo, chúng tôi chuyển đổi dữ liệu đầu vào X khác nhau của mỗi nhiệm vụ thành định dạng văn bản-sang-văn bản thống nhất. Ví dụ, chúng tôi tuyến tính hóa dữ liệu có cấu trúc (ví dụ: đồ thị kiến thức hoặc bảng) bằng cách nối các bộ ba hoặc cặp khóa-giá trị sử dụng token đặc biệt "[SEP]" cho sinh dữ liệu-sang-văn bản, và chúng tôi sử dụng token đặc biệt "[X_SEP]" để phân tách câu trả lời và đoạn văn cho sinh câu hỏi. Định dạng đầu vào được chuyển đổi cho mỗi nhiệm vụ có thể tìm thấy trong Phụ lục E.

Chúng tôi chia MVPCorpus thành hai phần, được sử dụng cho tiền huấn luyện và fine-tuning (đánh giá), tương ứng. Để tiền huấn luyện có giám sát, chúng tôi sử dụng 50 tập dữ liệu từ 7 nhiệm vụ, bao gồm sinh dữ liệu-sang-văn bản, hệ thống đối thoại mở, trả lời câu hỏi, sinh câu hỏi, sinh truyện, hệ thống đối thoại hướng nhiệm vụ, và tóm tắt văn bản. Chúng tôi cũng loại bỏ các ví dụ tiền huấn luyện chồng lấp với dữ liệu đánh giá để tránh rò rỉ dữ liệu (chi tiết hơn trong Phụ lục A.2). Cuối cùng, chúng tôi có kho ngữ liệu tiền huấn luyện có giám sát 25GB chứa 32M ví dụ. Thống kê của các tập dữ liệu cho tiền huấn luyện được liệt kê trong Bảng 9.

Để đánh giá, chúng tôi sử dụng 27 tập dữ liệu còn lại, được sử dụng phổ biến hơn trong tài liệu. Trong số các tập dữ liệu này, 23 tập dữ liệu đến từ 7 nhiệm vụ được sử dụng trong tiền huấn luyện. Chúng tôi gọi chúng là nhiệm vụ đã thấy và sử dụng chúng để kiểm tra tính hiệu quả của mô hình. 4 tập dữ liệu còn lại đến từ các nhiệm vụ sinh thông thường, sinh paraphrase, đơn giản hóa, và chuyển đổi phong cách, tương ứng. Chúng tôi gọi chúng là nhiệm vụ chưa thấy và sử dụng chúng để kiểm tra tính tổng quát của mô hình.

3.2 Kiến trúc mô hình
Mô hình MVP của chúng tôi được xây dựng trên kiến trúc Transformer encoder-decoder chuẩn (Vaswani et al., 2017). So với các PLM chỉ decoder như GPT-3 (Brown et al., 2020) và prefix LM như UniLM (Dong et al., 2019), kiến trúc encoder-decoder hiệu quả hơn cho các nhiệm vụ sinh văn bản (Raffel et al., 2020). Trong giai đoạn đầu, chúng tôi tiền huấn luyện xương sống MVP sử dụng hỗn hợp các tập dữ liệu có nhãn từ bảy nhiệm vụ. Để chỉ ra mỗi nhiệm vụ, chúng tôi áp dụng hướng dẫn do con người viết cho mỗi thể hiện nhiệm vụ. Ví dụ, chúng tôi viết "Summarize: " làm prompt cho các nhiệm vụ tóm tắt. Các hướng dẫn thủ công cho mỗi nhiệm vụ được trình bày trong Phụ lục E.

Trong giai đoạn thứ hai, chúng tôi đóng băng xương sống MVP và tiền huấn luyện một tập hợp các prompt cụ thể cho nhiệm vụ (tức là các vector liên tục) để kích thích khả năng của mô hình thực hiện một số nhiệm vụ cụ thể. Cụ thể, chúng tôi tuân theo prefix-tuning (Li và Liang, 2021) để chèn các vector liên tục tại mỗi lớp Transformer và học chúng sử dụng hỗn hợp các tập dữ liệu intra-task tương ứng (tức là các tập dữ liệu dưới cùng một nhiệm vụ2). So với prompt tuning (Lester et al., 2021), chỉ thêm prompt vào lớp đầu vào, các prompt theo lớp hiệu quả và ổn định hơn (Liu et al., 2022), đặc biệt đối với các nhiệm vụ NLG. Những prompt mềm này, không được chia sẻ giữa các nhiệm vụ, mã hóa kiến thức ngữ nghĩa cụ thể cho nhiệm vụ để giảm thiểu vấn đề mờ nhạt do học đa nhiệm gây ra (He và Choi, 2021).

3.3 Chi tiết huấn luyện
Mô hình MVP của chúng tôi sử dụng Transformer với 12 lớp trong cả encoder và decoder (~406M tham số), giống như kích thước mô hình của BART LARGE (Lewis et al., 2020). Chúng tôi khởi tạo xương sống với các tham số BART để cung cấp điểm khởi đầu tốt cho các nhiệm vụ NLG theo nghiên cứu trước đây (Dong et al., 2019; Zhang et al., 2020). Chúng tôi tiền huấn luyện mô hình với batch size 8,192 và áp dụng chiến lược trộn có nhiệt độ (Raffel et al., 2020) với tỷ lệ T = 2 để giảm thiểu sự khác biệt trong các nhiệm vụ và tập dữ liệu.

Chúng tôi tuân theo prefix-tuning (Li và Liang, 2021) để tiền huấn luyện các prompt cụ thể cho nhiệm vụ bằng cách thêm các vector có thể huấn luyện vào các module multi-head attention tại mỗi lớp. Độ dài prompt được đặt là 100, và chúng tôi sử dụng hàm tái tham số hóa MLP với kích thước ẩn 800 để cải thiện tính bền vững và hiệu suất huấn luyện (Li và Liang, 2021). Do đó, mỗi prompt nhiệm vụ có khoảng 62M tham số. Sau đó, chúng tôi đóng băng mô hình MVP và huấn luyện bảy nhóm prompt cụ thể cho nhiệm vụ, mỗi nhóm tương ứng với một nhiệm vụ khác nhau.

Trong hai giai đoạn, độ dài tối đa của cả chuỗi đầu vào và đầu ra được đặt là 1,024 để hỗ trợ các ví dụ chứa nhiều token hơn. Chúng tôi tối ưu hóa mô hình với tốc độ học hằng số 3×10^-5 sử dụng cross-entropy loss sequence-to-sequence chuẩn. Chúng tôi áp dụng trình tối ưu hóa AdamW với β1 = 0.9, β2 = 0.98, ε = 1×10^-6 để cải thiện tính ổn định huấn luyện (Liu et al., 2019b). Hệ số weight decay là 0.1. Để kiểm tra, chúng tôi chọn checkpoint với hiệu suất validation cao nhất. Tất cả các thí nghiệm được thực hiện trên 32 GPU NVIDIA Tesla V100 32GB. Chúng tôi thực hiện mô hình của mình sử dụng thư viện sinh văn bản TextBox (Tang et al., 2022a).

Để tóm tắt, chúng tôi tiền huấn luyện mô hình sinh 406M MVP và bảy nhóm prompt cụ thể cho nhiệm vụ 62M. Đối với mỗi nhiệm vụ downstream, người dùng có thể sử dụng trực tiếp xương sống (~406M) hoặc kết hợp thêm MVP với các prompt cụ thể cho nhiệm vụ (~468M).

4 Kết quả thí nghiệm
Trong phần này, chúng tôi chủ yếu điều tra tính hiệu quả và tổng quát của mô hình MVP. Chúng tôi thực hiện các thí nghiệm rộng rãi trong các thiết lập khác nhau:

• Trong các tình huống full tuning, chúng tôi sử dụng 27 tập dữ liệu sinh và benchmark GLUE (Wang et al., 2019) để đánh giá. Phần 4.1 và Phụ lục C phân tích kết quả trên 23 tập dữ liệu từ 7 nhiệm vụ đã thấy. Phần 4.3 bao gồm kết quả của 4 nhiệm vụ sinh chưa thấy và 8 nhiệm vụ hiểu. Để so sánh tốt hơn với ExT5, chúng tôi thực hiện thí nghiệm trên benchmark GEM (Gehrmann et al., 2021) trong Phụ lục C.2.

• Trong học zero-shot, chúng tôi so sánh các mô hình của chúng tôi với T0 trong Phần 4.2.

• Trong các thiết lập parameter-efficient tuning, chúng tôi sử dụng cùng các tập dữ liệu như trong Phần 4.1, và kết quả có thể tìm thấy trong Phần 4.4.

• Chúng tôi thực hiện đánh giá con người trong Phần 4.5.

Đối với thiết lập full tuning (Bảng 2 và 11), chúng tôi fine-tune toàn bộ mô hình (bao gồm xương sống MVP và prompt), trong khi đối với parameter-efficient tuning (Bảng 6), chúng tôi chỉ fine-tune prompt nhưng đóng băng trọng số tham số của MVP. Chúng tôi tối ưu hóa mô hình thông qua seq2seq loss với hệ số label smoothing (Szegedy et al., 2016) là 0.1 và trình tối ưu hóa AdamW với siêu tham số mặc định. Chúng tôi quét qua batch size trong {16, 64, 256} và tốc độ học trong {5×10^-6, 1×10^-5, 3×10^-5} để tìm siêu tham số tối ưu cho mỗi nhiệm vụ đánh giá. Chúng tôi sử dụng checkpoint với hiệu suất validation tốt nhất để suy luận tập test. Trong quá trình suy luận, chúng tôi đặt kích thước beam là 5 và kích thước no-repetitive ngram là 3. Chi tiết về fine-tuning và đánh giá có thể tìm thấy trong Phụ lục B.

4.1 Hiệu suất Full Tuning
Chúng tôi thực hiện thí nghiệm trên bảy tập dữ liệu mới của bảy nhiệm vụ đã thấy để xác minh tính hiệu quả của phương pháp tiền huấn luyện hai giai đoạn. Chúng tôi thiết kế một số biến thể mô hình. Trong giai đoạn đầu, MVP sử dụng tiền huấn luyện có giám sát đa nhiệm, và chúng tôi so sánh nó với hai biến thể khác sử dụng các chiến lược huấn luyện khác nhau:

• BART LARGE (Lewis et al., 2020): BART là PLM được sử dụng rộng rãi cho sinh ngôn ngữ tự nhiên sử dụng denoising auto encoding làm mục tiêu tiền huấn luyện không giám sát.

• Flan-T5 LARGE (Chung et al., 2022): Flan-T5 là mô hình ngôn ngữ gần đây được huấn luyện theo cách có giám sát trên các nhiệm vụ NLP khác nhau, có thể là đối thủ mạnh mẽ với mô hình của chúng tôi.

• Tiền huấn luyện đơn nhiệm vụ (Single): Chúng tôi huấn luyện riêng biệt một mô hình duy nhất cho mỗi nhiệm vụ sử dụng các tập dữ liệu intra-task trong cùng thiết lập tiền huấn luyện như huấn luyện đa nhiệm vụ. Ví dụ, chúng tôi tiền huấn luyện mô hình tóm tắt sử dụng các tập dữ liệu tóm tắt (ví dụ: Newsroom, WikiHow, và MSNews). Do đó, chúng tôi có tổng cộng bảy mô hình tiền huấn luyện đơn nhiệm vụ.

Đối với giai đoạn thứ hai tích hợp các prompt tiền huấn luyện đơn nhiệm vụ (ký hiệu là MVP+S), chúng tôi so sánh nó với hai biến thể sử dụng các prompt khác nhau:

• Prompt được khởi tạo ngẫu nhiên (MVP+R): Các prompt theo lớp cho mô hình MVP được khởi tạo ngẫu nhiên mà không có tiền huấn luyện.

• Prompt tiền huấn luyện đa nhiệm vụ (MVP+M): Chúng tôi chỉ tiền huấn luyện một nhóm prompt cho tất cả các nhiệm vụ, sử dụng cùng tập dữ liệu hỗn hợp như trong tiền huấn luyện xương sống.

Bên cạnh các biến thể này, chúng tôi bao gồm thêm kết quả tốt nhất được báo cáo từ các bài báo gốc trong tài liệu để so sánh (ký hiệu là SOTA). Từ kết quả trong Bảng 2, chúng ta có thể thấy:

Đầu tiên, các mô hình tiền huấn luyện có giám sát (tức là MVP, Flan-T5, và Single) đạt hiệu suất tốt hơn mô hình tiền huấn luyện không giám sát BART, mang lại cải thiện trung bình 9,3%, 3,13%, và 4,4% (theo tỷ lệ), tương ứng. Phát hiện này xác minh tính hiệu quả của phương pháp tiền huấn luyện có giám sát của chúng tôi, cho phép mô hình tiếp thu nhiều thông tin cụ thể cho nhiệm vụ hơn. Về tiền huấn luyện đa nhiệm vụ (MVP) và đơn nhiệm vụ (Single), mô hình MVP của chúng tôi vượt trội hơn các đối tác đơn nhiệm vụ 5,0%. Kết quả này chỉ ra rằng phương pháp học đa nhiệm vụ có thể tăng cường hiệu suất đơn nhiệm vụ bằng cách học thông tin ngữ nghĩa có thể chuyển giao qua các nhiệm vụ. Đáng chú ý, mô hình MVP của chúng tôi vượt trội hơn Flan-T5 5,8%, cho thấy tầm quan trọng của việc huấn luyện trên bộ sưu tập tập dữ liệu NLG của chúng tôi, MVPCorpus.

Thứ hai, học prompt cụ thể cho nhiệm vụ có hiệu quả để giảm thiểu vấn đề "mờ nhạt" của học đa nhiệm vụ. Đối với các nhiệm vụ như sinh dữ liệu-sang-văn bản và trả lời câu hỏi, MVP với prompt đơn nhiệm vụ (MVP+S) luôn luôn vượt trội hơn hai biến thể khác (MVP+R và MVP+M). Điều này xác minh rằng các prompt cụ thể cho nhiệm vụ có thể tiếp thu kiến thức chuyên biệt cho nhiệm vụ và kích thích khả năng của mô hình MVP thực hiện các nhiệm vụ nhất định.

Cuối cùng, phương pháp tiền huấn luyện có giám sát của chúng tôi đạt được năm kết quả SOTA mới trên sinh dữ liệu-sang-văn bản, sinh câu hỏi, trả lời câu hỏi, sinh truyện, và các nhiệm vụ đối thoại mở. Chúng tôi cũng đạt hiệu suất SOTA trong sáu trong tám tập dữ liệu trong Bảng 11, cho thấy khả năng sinh văn bản mạnh mẽ của mô hình MVP. Đối với các nhiệm vụ còn lại, các mô hình SOTA kết hợp các kỹ thuật được thiết kế riêng, ví dụ: khung re-ranking (Ravaut et al., 2022) và các mục tiêu cụ thể cho nhiệm vụ khác nhau (He et al., 2022), mang lại hiệu suất tốt hơn. Ngược lại, mô hình MVP của chúng tôi có thể tạo ra kết quả cạnh tranh chỉ với kiến trúc chung và mục tiêu học thống nhất.

4.2 Hiệu suất Zero-shot
Vì chúng tôi không tiền huấn luyện MVP trên bảy tập dữ liệu thường được sử dụng, chúng tôi tiến hành thí nghiệm zero-shot để xem khả năng chuyển giao miền của các mô hình. Chúng tôi bao gồm T0-3B và T0-11B (Sanh et al., 2022) làm baseline, là các mô hình lớn được huấn luyện trên nhiều nhiệm vụ downstream khác nhau. Kết quả được liệt kê trong Bảng 3. Chúng ta có thể quan sát thấy mô hình MVP nhỏ (406M) của chúng tôi vượt trội hơn T0-3B và T0-11B trong tất cả các thước đo với biên độ lớn, ngoại trừ một số thước đo trên ROCStories và MultiWOZ. Điều này chứng minh tính hiệu quả của việc sử dụng tiền huấn luyện có giám sát trên MVPCorpus của chúng tôi.

Tuy nhiên, tất cả các nhiệm vụ cho thấy rằng các mô hình trong thiết lập zero-shot hoạt động kém hơn đáng kể so với những mô hình trong thiết lập full tuning. Điều này gợi ý rằng các chiến lược huấn luyện hiệu quả cho các nhiệm vụ NLU có thể không tạo ra kết quả thỏa mãn cho các nhiệm vụ NLG. Mặc dù mô hình của chúng tôi đã tiếp thu kiến thức về nhiệm vụ, nó vẫn gặp khó khăn để hoạt động tốt trong miền mới mà không được fine-tune. Do đó, vẫn cần thiết phải phát triển các mô hình NLG cụ thể cho các nhiệm vụ và miền nhất định. Các mô hình MVP của chúng tôi có thể là các mô hình hiệu quả để điều tra thêm.

4.3 Tính tổng quát cho các nhiệm vụ chưa thấy
Trong phần con này, chúng tôi kiểm tra mô hình MVP trên các nhiệm vụ NLG và NLU chưa thấy để xác minh tính tổng quát của nó.

Các nhiệm vụ NLG chưa thấy. Theo Deng et al. (2021), một nhiệm vụ NLG có thể được gán cho một trong ba loại sau: nén (ví dụ: tóm tắt), chuyển đổi (ví dụ: dịch), hoặc tạo tác (ví dụ: sinh truyện). Vì chúng tôi không bao gồm bất kỳ nhiệm vụ chuyển đổi nào trong quá trình tiền huấn luyện, chúng tôi đánh giá mô hình MVP bằng hai nhiệm vụ NLG chuyển đổi chưa thấy: sinh paraphrase và chuyển đổi phong cách văn bản. Chúng tôi chọn các phương pháp SOTA cho hai nhiệm vụ này, tức là AESOP (Sun et al., 2021) cho sinh paraphrase và SC & BLEU (Lai et al., 2021) cho chuyển đổi phong cách văn bản, và thay thế xương sống BART của chúng bằng mô hình MVP để so sánh. Từ kết quả trong Bảng 4, chúng ta có thể thấy mô hình của chúng tôi vượt trội hơn BART với tỷ lệ 2,3% và đạt hai kết quả SOTA mới, xác minh tính tổng quát mạnh mẽ của mô hình. Phát hiện này cho thấy mô hình MVP có khả năng hơn BART và có thể phục vụ như một xương sống chung nhưng hiệu quả.

Các nhiệm vụ NLU chưa thấy. Mặc dù MVP được thiết kế đặc biệt cho các nhiệm vụ NLG, chúng tôi cũng đánh giá hiệu suất của nó trên các nhiệm vụ NLU chưa thấy sử dụng benchmark GLUE được sử dụng rộng rãi (Wang et al., 2019). Chúng tôi so sánh mô hình của chúng tôi với BART LARGE sử dụng phương pháp phân loại sequence của nó (Lewis et al., 2020). Theo kết quả được trình bày trong Bảng 5, mô hình MVP của chúng tôi vượt trội hơn BART trong 9 trên 12 thước đo và có hiệu suất tổng thể vượt trội 0,71%. Kết quả này chỉ ra khả năng tổng quát của mô hình MVP và chứng minh thêm rằng tiền huấn luyện có giám sát không chỉ học khả năng sinh mà còn cải thiện biểu diễn ngữ nghĩa tổng thể.

4.4 Hiệu suất Parameter-Efficient Tuning
Trong thiết lập fine-tuning nhẹ, chúng tôi chỉ tune các prompt trong khi đóng băng mô hình xương sống MVP để xác minh tính hiệu quả của nó trong các tình huống hạn chế tài nguyên. Bên cạnh mô hình MVP+S, chúng tôi xem xét so sánh các phương pháp sau:

• Prefix-tuning (Li và Liang, 2021): Prefix-tuning là phương pháp tuning nhẹ dựa trên prompt phổ biến cho sinh văn bản. Chúng tôi sử dụng BART làm xương sống, ký hiệu là BART+R.

• Chỉ tuning prompt được khởi tạo ngẫu nhiên (MVP+R): Biến thể này chỉ tune các prompt được khởi tạo ngẫu nhiên của MVP+R, và nó chia sẻ ý tưởng tương tự với prefix-tuning.

• Chỉ tuning prompt tiền huấn luyện đa nhiệm vụ (MVP+M): Biến thể này chỉ tune các prompt tiền huấn luyện đa nhiệm vụ của MVP+M. Ý tưởng như vậy đã được sử dụng trong SPoT (Vu et al., 2022).

Từ kết quả thí nghiệm trong Bảng 6, chúng ta có thể thấy: hiệu suất tốt của mô hình MVP trong thiết lập nhẹ chứng minh thêm tính hiệu quả của tiền huấn luyện có giám sát. Bằng cách so sánh hai phương pháp prompting được khởi tạo ngẫu nhiên (BART+R và MVP+R), chúng ta có thể thấy MVP+R đạt hiệu suất vượt trội hơn BART+R (+2,0%) nhờ xương sống đa nhiệm vụ có giám sát. Hơn nữa, khi được khởi tạo với các prompt tiền huấn luyện, MVP+S và MVP+M đạt kết quả cải thiện so với MVP+R, phù hợp với các phát hiện của SPoT (Vu et al., 2022). Khi so sánh với MVP+M, MVP+S hoạt động tốt hơn một chút với 1,2%, chỉ ra rằng các prompt cụ thể cho nhiệm vụ hữu ích để cải thiện mô hình trong các nhiệm vụ sinh. Đáng ngạc nhiên, MVP+S nhẹ của chúng tôi thậm chí có thể vượt trội hơn BART được tuned đầy đủ trên các nhiệm vụ như sinh câu hỏi và trả lời câu hỏi, thể hiện tính hiệu quả của phương pháp tiền huấn luyện có giám sát được đề xuất.

4.5 Đánh giá con người
Xem xét rằng tồn tại khoảng cách nhất định giữa các thước đo tự động và đánh giá con người (Sai et al., 2022), chúng tôi tiến hành đánh giá con người để chứng minh tốt hơn khả năng sinh của mô hình MVP. Chúng tôi so sánh MVP với BART trên bốn nhiệm vụ, bao gồm tóm tắt văn bản, sinh dữ liệu-sang-văn bản, hệ thống đối thoại mở, và sinh truyện. Theo thực hành của van der Lee et al. (2021), chúng tôi sử dụng mẫu phân tầng gồm 100 đầu vào có tần suất từ thấp, trung bình, và cao cho mỗi nhiệm vụ. Chúng tôi mời sáu giám khảo con người đánh giá văn bản được sinh ra bởi MVP và BART. Sau đó họ cần chọn cái nào tốt hơn hoặc chọn hòa theo độ trôi chảy, tính thông tin, tính nhất quán, đặc điểm nhiệm vụ, v.v. Chi tiết đánh giá con người hơn được liệt kê trong Phụ lục D. Bảng 7 thể hiện tỷ lệ "MVP thắng", "Hòa", và "BART thắng" cho mỗi tập dữ liệu. Từ kết quả, chúng ta có thể thấy MVP có thể sinh ra văn bản tốt hơn tổng thể so với BART từ góc độ con người.

5 Thảo luận
Sự khác biệt với các phương pháp hiện tại. Theo hiểu biết của chúng tôi, các nghiên cứu tiền huấn luyện có giám sát hiện tại chủ yếu tập trung vào các nhiệm vụ NLU (Aghajanyan et al., 2021; Aribandi et al., 2022) hoặc một số lượng nhỏ nhiệm vụ NLG (Lin et al., 2020b; Su et al., 2022). Với hiệu suất vượt trội được đạt bởi các phương pháp tiền huấn luyện có giám sát, việc khám phá tiền huấn luyện có giám sát để tạo ra các mô hình NLG vừa hiệu quả vừa tổng quát là quan trọng. Nghiên cứu của chúng tôi đóng góp đáng kể trong hướng này, đạt hiệu suất SOTA với một mô hình duy nhất trên 13 trong 17 tập dữ liệu. So với đối tác mạnh mẽ, ExT5 (Aribandi et al., 2022), mô hình MVP của chúng tôi vượt trội hơn nó trong 26 trong 27 thước đo (chi tiết trong Phụ lục C.2). Để hiểu rõ hơn sự khác biệt giữa nghiên cứu của chúng tôi và các nghiên cứu tiền huấn luyện có giám sát (đa nhiệm vụ) trước đây, chúng tôi trình bày so sánh chi tiết trong Bảng 8. Như chúng ta có thể thấy, nghiên cứu của chúng tôi thực hiện nghiên cứu với số lượng nhiệm vụ NLG lớn nhất cho cả tiền huấn luyện có giám sát và fine-tuning, kết hợp các prompt cụ thể cho nhiệm vụ, và cũng phát hành tất cả các tài nguyên quan trọng để tái tạo hoặc tái sử dụng nghiên cứu của chúng tôi.

Khả năng ứng dụng. Để tạo thuận lợi cho việc ứng dụng nghiên cứu của chúng tôi, chúng tôi đã phát hành kho ngữ liệu thu thập, các mô hình tiền huấn luyện, các prompt cụ thể cho nhiệm vụ, và văn bản được sinh ra. MVPCorpus thu thập của chúng tôi là bộ sưu tập nhiệm vụ NLG lớn nhất, có thể là tài nguyên chất lượng cao cho các LLM gần đây (Zhao et al., 2023). Chúng tôi có thể sử dụng tất cả dữ liệu để tiền huấn luyện mô hình chung hoặc chọn một tập con để tiếp tục tiền huấn luyện mô hình cụ thể cho miền hoặc nhiệm vụ (Gururangan et al., 2020). MVPCorpus của chúng tôi cũng có thể được xem như benchmark đánh giá cho các nhiệm vụ NLG khác nhau. Hơn nữa, mô hình MVP có thể được sử dụng để đạt kết quả cạnh tranh trong các nhiệm vụ NLG khác nhau. Người dùng có thể fine-tune mô hình MVP hoặc tích hợp nó với các prompt cụ thể cho nhiệm vụ dựa trên dữ liệu có nhãn đủ. Đáng chú ý, mô hình MVP có thể được sử dụng trực tiếp để có được hiệu suất tốt trong học zero-shot. Ngoài ra, mô hình MVP có thể cung cấp khởi tạo tham số hiệu quả để cải thiện các phương pháp hiện tại, như được mô tả trong Phần 4.3. Cuối cùng, các prompt cụ thể cho nhiệm vụ và văn bản được sinh ra có thể được sử dụng thêm để nghiên cứu sự tương tự nhiệm vụ và tác động của chúng đối với tiền huấn luyện đa nhiệm vụ.

6 Kết luận
Trong bài báo này, chúng tôi trình bày Tiền huấn luyện có giám sát đa nhiệm vụ (MVP) cho sinh ngôn ngữ tự nhiên. Đầu tiên, chúng tôi thu thập kho ngữ liệu NLG quy mô lớn, MVPCorpus, từ 77 tập dữ liệu trên 11 nhiệm vụ NLG đa dạng. Sau khi chuyển đổi các nhiệm vụ NLG khác nhau thành định dạng văn bản-sang-văn bản thống nhất, chúng tôi đề xuất tiền huấn luyện có giám sát đa nhiệm vụ để học mô hình MVP hiệu quả và tổng quát với các prompt cụ thể cho nhiệm vụ cho các nhiệm vụ NLG. Các thí nghiệm rộng rãi đã chứng minh rằng: (1) tiền huấn luyện có giám sát có lợi cho các nhiệm vụ NLG như một giải pháp hiệu quả. Mô hình MVP của chúng tôi vượt trội hơn các đối tác mạnh mẽ BART và Flan-T5 và thậm chí đạt hiệu suất SOTA trên 13 trong 17 tập dữ liệu; (2) các mô hình tiền huấn luyện có giám sát có tính tổng quát mạnh mẽ trên các nhiệm vụ sinh chưa thấy hoặc thậm chí hiểu.

Trong nghiên cứu tương lai, chúng tôi sẽ khám phá phiên bản đa ngôn ngữ của mô hình MVP bằng cách bao gồm nhiều tập dữ liệu hơn bằng các ngôn ngữ khác. Mô hình như vậy được kỳ vọng sẽ nắm bắt các đặc tính nhiệm vụ độc lập ngôn ngữ và cải thiện các nhiệm vụ sinh trong ngôn ngữ thiểu số. Bên cạnh đó, thú vị khi nghiên cứu cách các nhiệm vụ khác nhau liên quan đến nhau trong không gian ngữ nghĩa thống nhất, có thể truyền cảm hứng cho các phương pháp kết hợp quan hệ nhiệm vụ như tiên nghiệm.

Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Grant No. 62222215, Quỹ Khoa học Tự nhiên Bắc Kinh theo Grant No. 4222027, và Chương trình Nhà khoa học trẻ xuất sắc Bắc Kinh theo Grant No. BJJWZYJH012019100020098. Xin Zhao là tác giả liên hệ.

Hạn chế
Mặc dù nỗ lực thu thập càng nhiều nhiệm vụ và tập dữ liệu sinh càng tốt, chúng tôi chỉ đánh giá chất lượng sinh và tính tổng quát của các mô hình trên một số lượng nhỏ nhiệm vụ và tập dữ liệu. Khả năng diễn giải và tính bền vững của các mô hình cần phân tích thêm. Bên cạnh đó, tồn tại tính chủ quan khi thu thập các nhiệm vụ downstream và tập dữ liệu intra-task, mặc dù chúng tôi cố gắng sử dụng các phân loại được công nhận rộng rãi từ tài liệu. Do hạn chế về sức mạnh tính toán, chúng tôi không nghiên cứu hiệu suất của phương pháp ở các quy mô mô hình khác nhau. Tính hiệu quả của tiền huấn luyện đa nhiệm vụ từ đầu, tương tự như ExT5 (Aribandi et al., 2022), cũng đáng được nghiên cứu sâu.

Tác động rộng lớn hơn
Trong bài báo này, chúng tôi tiền huấn luyện mô hình ngôn ngữ MVP sử dụng các tập dữ liệu NLG có nhãn. Theo nghiên cứu (Bender et al., 2021; Bommasani et al., 2021), các PLM có xu hướng "ghi nhớ" những gì chúng đã "thấy" trong kho ngữ liệu tiền huấn luyện. Điều này có thể dẫn đến việc tái tạo các thiên vị không mong muốn từ dữ liệu tiền huấn luyện trên các nhiệm vụ downstream. Can thiệp dữ liệu huấn luyện có thể là giải pháp để giảm thiểu vấn đề này (Lu et al., 2020). Cũng thú vị khi điều tra liệu tiền huấn luyện có giám sát có tạo ra ít thiên vị hơn so với tiền huấn luyện không giám sát hay không.

Tác động môi trường là yếu tố khác chúng tôi nên xem xét. Chúng tôi cố gắng một chiến lược tiền huấn luyện hiệu quả hơn và phát hành PLM cho nghiên cứu tương lai. Trái ngược với các PLM lớn có hàng chục tỷ tham số, như T5 (Raffel et al., 2020) và GPT-3 (Brown et al., 2020), chúng tôi chỉ tiền huấn luyện mô hình nhỏ với hàng trăm triệu tham số. Ngoài ra, chúng tôi sử dụng dữ liệu tiền huấn luyện có giám sát và khởi tạo mô hình với BART tiền huấn luyện, cả hai đều cải thiện sự hội tụ của mô hình. Cuối cùng, mô hình của chúng tôi được tiền huấn luyện khoảng 20.000 bước, trong khi BART cùng kích thước được tiền huấn luyện 500.000 bước.

Khả năng tái tạo
Để tái tạo và tái sử dụng nghiên cứu của chúng tôi, chúng tôi đã phát hành bộ sưu tập MVPCorpus, các mô hình (ví dụ: MVP, prompt cụ thể cho nhiệm vụ, và các biến thể đa nhiệm vụ), kết quả trung gian (ví dụ: văn bản được sinh ra), và mã nguồn để tiền huấn luyện và fine-tuning tại liên kết: https://github.com/RUCAIBox/MVP. Các thiết lập chi tiết của thí nghiệm được liệt kê trong Phụ lục B. Chúng tôi hy vọng rằng những tài nguyên mã nguồn mở này sẽ tạo thuận lợi cho nghiên cứu tương lai về tiền huấn luyện có giám sát và đóng góp vào sự tiến bộ của nghiên cứu NLG.

Tài liệu tham khảo
[Phần này chứa danh sách tài liệu tham khảo dài được dịch sang tiếng Việt - bao gồm tất cả các tài liệu từ trang 10-19 của bản gốc]

--- TRANG 20 ---
A Nhiệm vụ và Tập dữ liệu
A.1 Mô tả các Nhiệm vụ và Tập dữ liệu
[Phần này chứa mô tả chi tiết về các nhiệm vụ và tập dữ liệu được sử dụng trong nghiên cứu]

A.2 Rò rỉ dữ liệu
[Phần này mô tả cách xử lý vấn đề rò rỉ dữ liệu]

--- TRANG 21-35 ---
[Các trang còn lại chứa các bảng chi tiết về thống kê tập dữ liệu, kết quả thí nghiệm bổ sung, hướng dẫn đánh giá con người, và các ví dụ định tính - tất cả đều được dịch sang tiếng Việt]
