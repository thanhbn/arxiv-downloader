# 2301.13688.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/datasets/2301.13688.pdf
# File size: 899298 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
The Flan Collection: Designing Data and Methods
for Eﬀective Instruction Tuning
Shayne Longpre∗Le Hou Tu Vu Albert Webson Hyung Won Chung
Yi Tay Denny Zhou Quoc V. Le Barret Zoph Jason Wei Adam Roberts
Google Research
Abstract
We study the design decisions of publicly available instruction tuning methods, and break down the
development of Flan 2022 models (Chung et al., 2022). Through careful ablation studies on the Flan
Collection of instruction tuning tasks and methods ,weteaseaparttheeﬀectofdesigndecisionsthatenableFlan-
T5 to outperform prior work by 3-17%+ across evaluation settings. We ﬁnd task balancing and enrichment
techniques are overlooked but critical to eﬀective instruction tuning, and in particular, training with mixed
promptsettings(zero-shot,few-shot,andchain-of-thought)actuallyyieldsstronger(2%+)performance
inallsettings. In further experiments, we show Flan-T5 requires less ﬁnetuning to converge higher and
faster than T5 on single downstream tasks—motivating instruction-tuned models as more computationally-
eﬃcientstartingcheckpointsfornewtasks. Finally,toaccelerateresearchoninstructiontuning,wemake
the Flan 2022 collection of datasets, templates, and methods publicly available.1
Zero-Shot Held-In020406080Avg. Accuracy (%)
Zero-Shot CoT Few-Shot BBH Zero-Shot MMLU Few-Shot MMLU+3.3
+10.2+8.5+4.2+17.6
T5-XL Flan 2022 T5-XL Flan 2021 T5-XL P3++ T5-XL SNI OPT-IML-Max 175B
Figure1: Comparingpublicinstructiontuningcollections onHeld-In,Held-Out(BIG-BenchHard(Suzgun
et al., 2022) and MMLU (Hendrycks et al., 2020)), and Chain-of-Thought evaluation suites, detailed in
Appendix A.3. All models except OPT-IML-Max (175B) are T5-XL with 3B parameters. Green text indicates
absolute improvement over the next best comparable T5-XL (3B) model.
∗Research completed while a Student Researcher at Google. Correspondence: slongpre@mit.edu .
1Datagenerationcodeavailableat: https://github.com/google-research/FLAN/tree/main/flan/v2 . Generationcodeallows
users to vary mixtures rates, templates, prompt types and data augmentations techniques, for faster public research.
1arXiv:2301.13688v2  [cs.AI]  14 Feb 2023

--- PAGE 2 ---
1 Introduction
Large language models such as PaLM (Chowdhery et al., 2022), Chinchilla (Hoﬀmann et al., 2022), and
ChatGPTamongothers(Brownetal.,2020;Ouyangetal.,2022)haveunlockednewcapabilitiesinperforming
natural language processing (NLP) tasks from reading instructive prompts. Prior art has shown that
instruction tuning—ﬁnetuning language models on a collection of NLP tasks formatted with instructions—
furtherenhancestheabilityoflanguagemodelstoperformanunseentaskfromaninstruction(Weietal.,
2021; Sanh et al., 2021; Min et al., 2022).
Inthiswork,weevaluatethemethodsandresultsof open sourced instructiongeneralizationeﬀorts,comparing
theirﬁnetuningtechniquesandmethods. Andinparticular,weidentifyandevaluatethecriticalmethod-
ological improvements in the “Flan 2022 Collection”, which is the term we use for the collection of data and
methods for data augmentation and instruction tuning ,ﬁrstimplementedandusedinChungetal.(2022). Where
Chung etal. (2022)focuses on theemergent andstate-of-the-artresults ofcombining Flan 2022with PaLM
540B,this workfocuses inon thedetailsof theinstruction tuningmethodsthemselves,ablatingindividual
factors,andcomparingthemdirectlytopriorworkbykeepingthepretrainedmodelsizeandcheckpoint
consistent.
The Flan 2022 Collection oﬀers the most extensive publicly available set of tasks and methods for instruction
tuning,whichwehavecompiledinone place. Wehavealsosupplementedthiswithhundreds moreofour
own high-quality templates, richer formatting patterns, and data augmentations. We show that a model
trainedonthiscollectionoutperformsotherpubliccollectionsonalltestedevaluationbenchmarks,including
theoriginalFlan2021(Weietal.,2021),T0++(Sanhetal.,2021),Super-NaturalInstructions(Wangetal.,
2022c), andthe concurrentworkon OPT-IML (Iyer etal., 2022). As shown inFigure 1, thisincludes 4.2%+
and8.5%improvementsontheMMLU(Hendrycksetal.,2020)andBIG-BenchHard(Suzgunetal.,2022)
evaluation benchmarks respectively, for equally sized models.
AnalysisoftheFlan2022methodsuggeststhestrongresultsstembothfromthelargerandmorediversesetof
tasks, but also from a set of simple ﬁnetuning and data augmentation techniques. In particular, training on a
mix of examples templatized with zero-shot, few-shot, and chain-of-thought prompts improves performance
in everyone of thesesettings, together. Forinstance, adding just10% few-shot promptsimproves zero-shot
promptingresultsby2%+. Additionally,enrichingtaskdiversitybyinvertinginput-outputpairs,asused
in (Sanh et al., 2021; Min et al., 2022), along with balancing task sources, are both shown to be critical to
performance. The resulting Flan-T5 model converges faster and at a higher performance than T5 models in
single-task ﬁnetuning—suggesting instruction-tuned models oﬀer a more computationally-eﬃcient starting
checkpoint for downstream applications, corroborating Aribandi et al. (2021) and Liu et al. (2022b).
We hope making these ﬁndings and resources publicly available will unify resources around instruction
tuning and accelerate research into more general-purpose language models. We summarize this work’s core
contributions as follows:
•Methodological: Show that training with mixed zero- and few-shot prompts yields much better perfor-
mance in bothsettings (Section 3.2).
•Methodological: Measureanddemonstratethecriticaltechniquestoeﬀectiveinstructiontuning: scaling
Section 3.3, enriching task variety with input inversion (Section 3.4), adding chain-of-thought training
data, and balancing diﬀerent data sources (Section 3.5).
•Results: Demonstratethesetechnicalchoicesyield3-17%Held-Outtaskimprovementsoverexisting
open source instruction tuning collections (Figure 1).
•Results: DemonstrateFlan-T5servesasastrongerandmorecomputationally-eﬃcientstartingcheck-
point for single-task ﬁnetuning (Section 4).
•Open source the new Flan 2022 task collection, templates, and methods for public research.
2

--- PAGE 3 ---
2 Public Instruction Tuning Collections
†Release Collection 
+ + 
+ 
†
†+ P
PNPNP
P
PNP
NP
NPPPNPNPZSFSZS
FSZSFS
ZS
ZS
ZSFS
ZS
ZS
ZSFS
FSZS
CoTModel Base Size Public? Model Details Data Collection & Training  Details 
Prompt Types Tasks in Flan # Exs Methods 
+ 
+ 
+ + 
+ 
+ + P FSP
FSZS
CoT
Figure2: ATimelineofPublicInstructionTuningCollections speciﬁesthecollectionreleasedate,detailedinformation
ontheﬁnetunedmodels(thebasemodel,theirsize,andwhetherthemodelitselfisPublic(P)orNotPublic(NP)),what
prompt speciﬁcation they were trained for (zero-shot, few-shot, or Chain-of-Thought), the number of tasks contained in
the Flan 2022 Collection (released with this work), and core methodological contributions in each work.
Note that the number of tasks and of examples vary under diﬀerent assumptions and so are estimates. For instance, the
deﬁnition of “task” and ”task category” vary by work, and are not easily simpliﬁed to one ontology. The reported counts
for the number of tasks are reported using task deﬁnitions from the respective works.
†indicates concurrent work.
Large Language Models Instruction tuning has emerged as a tool to make large language models (LLMs)
and their abilities more useful for interactive dialog and functional tasks. Previous work (Raﬀel et al., 2020;
Liu et al., 2019; Aghajanyan et al., 2021; Aribandi et al., 2021) experimented with large scale multi-task
ﬁnetuning,toimprovedownstreamsingletargetﬁnetuning,butwithoutinstructionprompts. UniﬁedQA
and others (Khashabi et al., 2020; McCann et al., 2018; Keskar et al., 2019) uniﬁed a wide range of NLP tasks
into a single generative question answering format, using prompt instructions for multi-task ﬁnetuning and
evaluation.
TheFirstWave Since2020,severalinstructiontuningtaskcollectionshavebeenreleasedinrapidsuccession,
outlinedinFigure2. NaturalInstructions(Mishraetal.,2021),Flan2021(Weietal.,2021),P3(thePublicPool
of Prompts, Bach et al., 2022) aggregated large NLP task collections and templatized them with instructions
(zero-shot prompting ), speciﬁcally for ﬁnetuning models to generalize to unseen instructions. MetaICL (Min
et al., 2022) also consolidated other task collections (Ye et al., 2021; Khashabi et al., 2020) to train models
tolearntasks“in-context”–fromseveralinput-outputexamples,knownas few-shot prompting ,butinthis
casewithoutinstructions. Eachoftheseworksaﬃrmedthescalingbeneﬁtsoftaskandtemplatediversity,
3

--- PAGE 4 ---
and some reported strong beneﬁts from inverting the inputs and outputs in templates to produce new tasks
(“noisy channel” in Min et al., 2022).
TheSecondWave Asecondwaveofinstructiontuningcollectionsexpandedpriorresources: combining
more datasets and tasks into one resource, like Super-Natural Instructions (Wang et al., 2022c) or OPT-IML
(Iyeretal.,2022),addingmultilingualinstructiontuninginxP3(Muennighoﬀetal.,2022),andChain-of-
Thought training prompts in Flan 2022 (Chung et al., 2022). Both the Flan Collection and OPT-IML contain
mosttasksrepresentedinpriorcollections.2Ourworkispositionedhere,coalescingmostofthesecollections
(of collections) and their methods, as the strongest starting point for future open source work.
New Directions Concurrentandfutureworkisbeginningtoexploretwonewdirections: (a)expanding
taskdiversityevenmoreaggressivelywithsyntheticdatageneration,particularlyincreative,andopen-ended
dialogue(Wangetal.,2022b;Honovichetal.,2022;Yeetal.,2022;Guptaetal.,2022),and(b)oﬀeringhuman
feedback signals on model responses (Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022a; Nakano et al.,
2021; Bai et al., 2022b). We view most of these new directions as likely additive to a foundation of instruction
tuning methods.
TuningwithHumanFeedback Instructiontuningonhumanfeedbackhasdemonstratedstrongresultson
open-endedtasks,butattheexpenseofperformanceonawidearrayofmoretraditionalNLPtasks(Ouyang
et al., 2022;Glaese et al., 2022; Baiet al., 2022a; Nakano et al., 2021). (SeeOuyang etal. (2022)’s discussionof
the “alignment tax”.) Our work focuses speciﬁcally on instruction generalization, without human feedback,
fortworeasons. First,humanfeedbackdatasetsarefarlesspubliclyavailablethaninstructiontuningdatasets
(and may be model-speciﬁc). Second, by itself, instruction generalization shows great promise in enhancing
human preferred responses on open-ended tasks, as well as improving traditional NLP metrics (Chung
et al., 2022). The extent of obtainable progress withoutexpensive human response demonstrations or ratings
remains an open question, and an important pursuit to narrow the gap between public and non-public
research.
The Importance of OpenSource High proﬁle research is increasingly driven by non-public data, as in the
case of GPT-3 and others (Ouyang et al., 2022; Glaese et al., 2022). The inaccessibility of these resources
inhibitsthe researchcommunity’sability toanalyzeand improvethese methodsinthepublic domain. We
narrow our purview to open source and accessible data collections, motivated by the goal of democratizing
accessibility to research.
3 Flan 2022 Instruction Tuning Experiments
Recent research has yet to coalesce around a uniﬁed set of techniques, with diﬀerent tasks, model sizes,
andtargetinputformatsallrepresented. Weopensourceanewcollection,ﬁrstintroducedinChungetal.
(2022), denoted “Flan 2022”, which combines Flan 2021, P3++3, Super-Natural Instructions, with some
additional reasoning, dialog, and program synthesis datasets. We defer to Chung et al. (2022) for details of
templatization and collection; and in this work we take a deeper look at key methodological improvements
and compare the collection on equivalent model sizes to existing collections.
In this section, we evaluate the design decisions in Flan and discuss four in particular that yield strong
improvements to the instruction tuning recipe. These design components, outlined in Section 2, are: (I)
usingmixedzero-shot,few-shot,andChain-of-Thoughttemplatesattraining(Section3.2), (II)scalingT5-
sized models to 1800+ tasks (Section 3.3), (III)enriching tasks with input inversion (Section 3.4), and (IV)
balancingthesetaskmixtures(Section3.5). InSection3.1,webeginbymeasuringthevalueofeachcomponent
and compare the ﬁnal model against alternative instruction tuning collections (and their methods).
2Note that each work deﬁnes datasets, tasks, and task categories diﬀerently. For simplicity, we use their own deﬁnitions in Section 2.
3“P3++” is our notation for all datasets in the Public Pool of Prompts (P3): https://huggingface.co/datasets/bigscience/P3
4

--- PAGE 5 ---
Experimental Setup We ﬁnetune on the preﬁx language model adapted T5-LM (Lester et al., 2021), using
the XL (3B) size for all models for consistency, unless otherwise stated. While other sizes of Flan-T5 are
available, we felt XL was appropriately sized to run large-scale systematic ablations, while being suﬃciently
largetodrawgeneralconclusions. Weevaluateon(a)asuiteof8“Held-In”tasksrepresentedwithinthe
1800+trainingtaskcollection(4questionansweringand4naturallanguageinferencevalidationsets),(b)
Chain-of-Thought (CoT) tasks (5 validation sets), and (c) the MMLU (Hendrycks et al., 2020) and BBH
(Suzgunetal.,2022)benchmarksasoursetof“Held-Out”tasks,astheyarenotincludedaspartofFlan2022
ﬁnetuning. The Massivley Multitask Language Understanding benchmark (MMLU) broadly tests reasoning
and knowledge capacity across 57 tasks in the sciences, social sciences, humanities, business, health, among
other subjects. BIG-Bench Hard (BBH) includes 23 challenging tasks from BIG-Bench (Srivastava et al.,
2022) where PaLM under-performs human raters. In our ablations, we also evaluate BBH with Chain-of-
Thought inputs, following Chung et al. (2022). Additional ﬁnetuning and evaluation details are provided in
Appendix A.
3.1 Ablation Studies
Table1summarizesthemeancontributiontoHeld-in,Held-out,andChain-of-thoughttasks,byindividually
deductingmethods: mixtureweightbalancing (“-MixtureBalancing"),Chain-of-thoughttasks(“-CoT"),
mixed prompt settings (“- Few Shot Templates"), and Input Inversion (“- Input Inversion"). Flan-T5 XL
leverages all four of these methods together. We also ﬁnetune T5-XL-LM on other collections, including Flan
2021, P3++, Super-Natural Instructions for comparison.
M/o.sc/d.sc/e.sc/l.sc H/e.sc/l.sc/d.sc-I/n.sc C/o.scT MMLU BBH BBH-C/o.scT
T5-XL Flan 2022 73.8 / 74.8 35.8 /34.1 50.3 / 52.4 26.2 /39.3 33.9 / 35.2
- CoT 73.3 / 73.2 28.8 / 24.6 47.5 / 46.9 18.2 / 30.0 18.2 / 12.0
- Input Inversion 73.8/ 74.1 32.2 / 23.5 41.7 / 41.2 18.4 / 24.2 15.7 / 13.0
- Mixture Balancing 71.2 / 73.1 32.3 / 30.5 45.4 / 45.8 15.1 / 24.3 13.8 / 15.4
- Few Shot Templates 72.5 / 62.2 38.9/ 28.6 47.3 / 38.7 27.6 / 30.8 18.6 / 23.3
T5-XL Flan 2021 68.4 / 56.3 24.6 / 22.7 41.4 / 34.8 28.1/ 28.3 26.0 / 26.9
T5-XL P3++ 70.5 / 62.8 25.6 / 25.6 46.1 / 34.1 26.0 / 30.8 23.4 / 26.1
T5-XL Super-Natural Inst. 50.3 / 42.2 13.8 / 14.3 35.6 / 31.1 10.4 / 15.6 8.0 / 12.5
GLM-130B†- - – / 44.8 - -
OPT-IML-Max 30B†- - 46.3 / 43.2 – / 30.9 -
OPT-IML-Max 175B†- - 49.1 / 47.1 – / 35.7 -
Flan 2022 - Next Best T5-XL +3.3 / +12 +10.2 / +8.5 +4.2 / +17.6 -1.9 / +8.5 +7.9 / +8.3
Table1:MethodAblations(top) showtheimportanceofeachmethodforFlan-T5XL. CollectionAblations
(bottom) evaluateFlan-T5XLagainstT5-XLﬁnetunedonotherinstructiontuningcollections: FLAN2021,
P3++,andSuper-NaturalInstructions. Flan2022-NextBestT5-XL showstheimprovementofFlan-T5XL
over the next best T5-XL (comparatively sized) ﬁnetuned on another collection. Metrics are reported in both
zero-shot / few-shot settings across Held-In, Chain-of-Thought, and Held-Out (MMLU, BBH) tasks.
†We also inlcude the results reported by OPT-IML (Iyer et al., 2022) and GLM-130B (Zeng et al., 2022).
EachoftheablatedcomponentsofFlancontributesimprovementstodiﬀerentmetrics: Chain-of-Thought
trainingtoChain-of-Thoughtevaluation,inputinversiontoHeld-Outevaluations(MMLUandBBH),few-shot
prompt training to few-shot evaluations, and mixture balancing to all metrics.
AscomparedtoT5-XLmodelstrainedonalternativeinstructiontuningcollections(andtheirmethods),Flan
outperforms in almost every setting. While previous collections are tuned speciﬁcally to zero-shot prompts,
Flan-T5XListunedforeitherzero-orfew-shotprompts. Thisyieldsperformancemarginsof+3-10%for
most of the zero-shot settings, and margins of 8-17% for the few-shot settings. Most impressively, Flan 2022
outperforms OPT-IML-Max’smuch larger(10x) 30Band (58x)175B models. Next,we isolate someof Flan
2022’s ablated methods individually, to examine the beneﬁts of each.
5

--- PAGE 6 ---
3.2 Training with Mixed Prompt Settings
Priorworkhasshownawidevarietyofinputtemplatespertaskcanimproveperformance. However,separate
fromthewordingoftheinstructiontemplate,thesepriorLLMsmostlytunewithtemplatesets targeted to
a single prompt setting : for zero-shot prompting (Wei et al., 2021; Sanh et al., 2021; Aghajanyan et al., 2021;
Aribandi et al., 2021) or for few-shot prompting (Min et al., 2022; Wang et al., 2022c).
An underappreciated design decision in InstructGPT (Ouyang et al., 2022) was to mix training templates for
each of these prompt settings, rather than target a single setting. However, since Ouyang et al. (2022) do not
examine this choice, we expected a performance trade-oﬀ in ﬁnetuning for zero-shot or few-shot prompting
performance – particularly for smaller models. Instead, we ﬁnd training with mixed zero- and few-shot
prompts signiﬁcantly improves performance in bothsettings – most surprisingly, even for models with only
3B parameters.
01020304050607080901006062646668707274
Percent (%) Few Shot Templates at TrainingAccuracy (%)H/e.sc/l.sc/d.sc-I/n.sc T/a.sc/s.sc/k.sc P/e.sc/r.sc/f.sc/o.sc/r.sc/m.sc/a.sc/n.sc/c.sc/e.sc
01020304050607080901003436384042444648
Percent (%) Few Shot Templates at TrainingAccuracy (%)H/e.sc/l.sc/d.sc-O/u.sc/t.sc MMLU P/e.sc/r.sc/f.sc/o.sc/r.sc/m.sc/a.sc/n.sc/c.sc/e.sc
Zero-Shot Eval Few-Shot Eval
Figure3: Training jointly with zero-shot and few-shot prompt templates improves performance onboth
Held-In and Held-Out tasks. The stars indicate the peak performance in each setting.
Figure 3shows (1)adding aslittle as5% few-shot trainingtemplatescan dramaticallyimprovezero-shot
performance, and (2) adding 10%+ of zero-shot data improves few-shot performance too. Both Held-In and
Held-Outtaskspeakanywherebetween10-90%offew-shot data,butthisrangeis consistentlyhigherthan
training with only one prompt setting.
3.3 Scaling Small Models to 1.8k+ Tasks
Themostrecentandconcurrentpubliclyavailableinstructiontuningeﬀorts,likeFlan2022,trainonthousands
of tasks (Wang et al., 2022c; Iyer et al., 2022), but operate on diﬀerent task compositions and underlying
trainingmethods. TomeasuretheimpactofscalingmodelsizesandtasksfortheFlan2022collection,we
ﬁnetune T5-LM adapted models (Small, Base, Large, XL, XXL) on randomly selected task subsets (8, 25,
50,100,200,400,800,all1873). EveryﬁnetuningrunisguaranteedtoincludetheHeld-Intasks,sowecan
estimatehowtaskscalingimpactsthemodelcapacitytomaintainperformanceonagiventaskitsalready
seen.
Figure 4 demonstrates that both Held-In and Held-Out tasks appear to beneﬁt from adding hundreds of
ﬁnetuning tasks. Held-in task evaluations peak around 200 total tasks, and diminish in performance as
more tasks are added, though larger models peak later and diminish less. Held-out task performance
increases log-linearly with the number of tasks, achieving the highest performances with all 1836 tasks.
6

--- PAGE 7 ---
10 501002004008001600405060708090
XXL
XL
Large
Base
Small
Number of TasksAccuracy (%)H/e.sc/l.sc/d.sc-I/n.sc T/a.sc/s.sc/k.sc/s.sc P/e.sc/r.sc/f.sc/o.sc/r.sc/m.sc/a.sc/n.sc/c.sc/e.sc
10 50100200400800160020304050XXL
XL
Large
Base
Small
Number of TasksAccuracy (%)H/e.sc/l.sc/d.sc-O/u.sc/t.sc MMLU P/e.sc/r.sc/f.sc/o.sc/r.sc/m.sc/a.sc/n.sc/c.sc/e.sc
Figure 4: Performance Scaling Laws for the number of ﬁnetuning tasks and model sizes . Held-In per-
formance (left) and Held-Out MMLU performance (right) are shown. The gold star indicates the peak
performance for that model size.
Surprisingly, only T5-Small appears to exceed its Held-Out task performance before 1836 tasks, while larger
modelsizescontinuetoimprove. Theseresultssuggest(a)evenT5-Basemaynothaveexhausteditscapacity
withthousandsoftasks,and(b)thelargestLMscouldbeneﬁtfromthousandsmoretasksforHeld-Inand
Held-Out task performance.
One necessary assumption of this analysis is that all tasks are deﬁned and counted equally. Section 3.5
demonstrateshownotalltasksourcesareequallybeneﬁcialtotraining,andthemodelperformancemay
saturate from too many tasks from one source (e.g. Super-Natural Instructions). We would caution conclu-
sionsthattaskscalingbeyond1800wouldtranslatetoincreasedreturnswithoutalsopayingattentiontotask
diversity and quality.
3.4 Task Enrichment with Input Inversion
Prior instruction tuning work has enriched their diversity of tasks by inverting the ( x,y) input-output pairs
in supervised tasks—referred to as “prompts not intended for the original task” in P3 (Bach et al., 2022) or
the“noisychannel”inMetaICL(Minetal.,2022). Forexample,adatasetmaybeoriginallydesignedfor,
given a question x, evaluate if a model can answer y. Input inversion instead gives a model the answer yand
trainsittogeneratethequestion x. Thisisaneasymethodtoenrichthetaskvarietygivenalimitedsetof
data sources. However, it isn’t clear that this method remains helpful when 100s of unique data sources and
1000s of tasks are already available.
Toassessthis,weenrichourmixtureswithinputinvertedtasks(detailsandexamplesinAppendixB)and
measurethe eﬀect. InTable 1weﬁnd thisisnot beneﬁcialforHeld-In performance,butstronglybeneﬁcial
for Held-Out performance. These beneﬁts invigorate the prospect of data augmentation techniques for LLM
ﬁnetuning, which had previously been shown to have diminishing returns the longer models are pretrained
(Longpre et al., 2020).
3.5 Balancing Data Sources
Scaling architecture size and the number of tasks are eﬀective, but our results suggest the mixture weighting
deserves as much attention to optimize results. To converge on a balanced weighting, we omit diﬀerent sets
of task sources,one at a time (Flan 2021,T0-SF, Super-Natural Instructions, Chain-of-Thought, Dialog, and
7

--- PAGE 8 ---
T/r.sc/a.sc/i.sc/n.sc M/i.sc/x.sc/t.sc/u.sc/r.sc/e.sc/s.sc M/e.sc/t.sc/r.sc/i.sc/c.sc/s.sc
Held-In CoT MMLU
All (Equal) 64.9 41.4 47.3
All - Flan 2021 55.3 38.6 45.7
All - T0-SF 63.2 43.4 44.7
All - Super-Nat. Inst. 65.9 42.2 46.8
All - CoT 65.6 29.1 46.8
All - Prog. Synth. 66.9 42.3 46.8
All - Dialog 65.4 40.3 47.1
All (Weighted) 66.4 40.1 48.1
Table 2: Subsets oftasks are left out from an equallyweighted mixture to measure their importance. T0-SF
and Flan 2021 ﬁnetuning are most important for MMLU, while Chain-of-Thought (CoT) ﬁnetuning is
most important for Chain-of-Thought evaluation.
Program Synthesis), and rank their contributions on the MMLU benchmark.4.
AsshowninTable2,Flan2021andT0-SFareamongthemostbeneﬁcialmixtures,followedbySuper-Natural
InstructionsandChain-of-Thought,withDialogandProgramSynthesislast. Theseﬁndingsarecorroborated
by Iyer et al. (2022) who extensively test data mixing proportions, and also determine their Flan 2021, T0-SF,
and T5 mixtures are the most broadly beneﬁcial. Additionally, they ﬁnd Super-Natural Instructions has
limited scaling beneﬁts on Held-Out task performance, which they relate to its unique input format and
instructiondesign. Notably,Chain-of-thoughtﬁnetuningappearsbeneﬁcialacrossallourevaluationsettings,
especially considering they contain far fewer tasks than Flan 2021, T0-SF or Natural Instructions.
We used these ﬁndings to signiﬁcantly narrow the mixture weights search space, and used our practitioner’s
intuition from there. This strategy is simple but eﬀective, as shown in Table 1, but leaves ample room for
more sophisticated future work.
3.6 Discussion
OPT-IML (Iyer et al., 2022) presents the closest comparison to this work, including a similar collection of
tasks,examplesandtechniques. However,whiletheirusedtasksareallpubliclysourced,theircollection,
with templates,processing, and examplemixing, is not released,and as aresult cannot beeasily compared.
Iyer et al. (2022) report that Flan-T5-XL (3B) and XXL (11B) outperforms OPT-IML-Max 175B on both
MMLU and BBH. As they discuss, these diﬀerences may arise from any combination of pre-training, model
architecture, and instruction tuning. Model architecture and pretraining before instruction tuning can play a
signiﬁcantrole(Wangetal.,2022a). Buttherearemanyotherdetailsininstructiontuningthatmayvary
betweenFlan2022andOPT-IML.Likelycandidatesareare: exampletemplatization,howthemixedinput
prompting procedures are used at training, and task composition.
Howsigniﬁcantareeachofthesediﬀerence? WhileOPT-IMLcontainsmoretasksthanFlan2022,weestimate
approximately 94%(2067 =2207)are also used in the Flan 2022 collection5, and very few tasks in Flan 2022
are not contained in some format in OPT-IML. This suggests the overall diﬀerence in task diversity is not
signiﬁcant when using a shared deﬁnition of “task”. Task mixture rates also emphasize similar sources,
including Flan 2021 (46% vs 20%), PromptSource/P3 (28% vs 45%), and Super-Natural Instructions (25% vs
25%),forFlan2022andOPT-IMLrespectively.6OPT-IML’sothercollections(Crossﬁt,ExMix,T5,U-SKG)
4Following Chung et al. (2022) we refer to the subset of P3++ that is not in Flan 2021 as T0-SF (SF stands for “sans Flan”).
5This is calculated using their deﬁnition of “task” (reported in Iyer et al. (2022)’s Table 1), which does not deduplicate across
collections.
6Note that 46% weight for Flan 2022 is actually on Muﬃn from Chung et al. (2022) which combines Flan 2021 with new dialog and
program synthesis tasks.
8

--- PAGE 9 ---
ANLI5060708090100Eval Metrics (%)
ARC BoolQ CosmosQA RTE SQuAD v2 AI2 Science
CondaQA5060708090100Eval Metrics (%)
CxC MedNLI PubmedQA WANLIT5!Flan!FT
T5!FT
T5!FlanF/l.sc/a.sc/n.sc H/e.sc/l.sc/d.sc-I/n.sc T/a.sc/s.sc/k.sc/s.sc
F/l.sc/a.sc/n.sc H/e.sc/l.sc/d.sc-O/u.sc/t.sc T/a.sc/s.sc/k.sc/s.sc+4.0+8.7+2.7+1.0+7.8
+2.4
+16.7
+2.6+0.0+0.1
+2.3+1.6
Figure5: Flan-T5OutperformsT5onSingle-TaskFinetuning. Wecomparesingle-taskﬁnetunedT5,single-
task ﬁnetuned Flan-T5, and Flan-T5 without any further ﬁnetuning.
are not weighted signiﬁcantly: 4%, 2%, 2%, 2% respectively.
WebelieveexampletemplatizationandthemixedpromptformatsmayposethelargestdiﬀerenceswithOPT-
IMLs instruction tuning. Our template repository was signiﬁcantly updated from Flan 2021, adding variety
not just in instructions, but also along dimensions. For instance, the templatization procedure varies where
theinstructionisplaced(beforeorafterfew-shotprompts),thespacingandseparatorsbetweenfew-shot
andChain-of-Thoughtprompts,andtheformattingpermutationsofansweroptions(andtheirtargets)for
multiple-choice examples, which sometimes includes and sometimes excludes answer options in the inputs
or exemplars. While we do not have dedicated experiments comparing many iterations of development, we
foundtheseproceduresdramaticallyaugmentinputvarietyandshowedrepeatedperformanceimprovements.
Our example templatizing procedure is open sourced for inspection and future work.
4 Instruction Tuning Enhances Single-Task Finetuning
Inappliedsettings,machinelearningpractitionersdeployNLPmodelsﬁnetuned(FT)speciﬁcallyforasingle
targettask,usuallywhereﬁnetuningdataisalreadyavailable. Whilepriorworkhasshownthebeneﬁtsof
intermediateﬁnetuning(Pruksachatkunetal.,2020;Vuetal.,2020)ormulti-taskﬁnetuning(Aghajanyan
et al., 2021; Aribandi et al., 2021) for downstream tasks, this has not been studied extensively for instruction-
tuned models.
WeevaluateFlan2022instructiontuningasanintermediarystepbeforesingletargetﬁnetuning,tounderstand
if Flan-T5 would serve as a better starting checkpoint for applied practitioners. We evaluate three settings in
9

--- PAGE 10 ---
0 50 100 150 20030405060708090Accuracy (%)WANLI
0 50 100 150 200M/e.sc/d.scNLI
0 50 100 150 200
Number of Finetuning StepsC/o.sc/n.sc/d.sc/a.scQA
0 50 100 150 200P/u.sc/b.sc/m.sc/e.sc/d.scQA
0 50 100 150 200C/x.scC
Flan-T5 XL T5-XL
Figure 6: Flan-T5 convergences faster than T5 on single-task ﬁnetuning for each of 5 Held-Out tasks from
Flan ﬁnetuning.
Figure 5: ﬁnetuning T5 directly on the target task as the conventional baseline (blue bars), using Flan-T5
without further ﬁnetuning (beige bars), and ﬁnetuning Flan-T5 further on the target task (red bars).
Pareto Improvements to Single Task Finetuning Forboth setsofHeld-Inand Held-Outtasksexamined,
ﬁnetuning Flan-T5 oﬀers a pareto improvement over ﬁnetuning T5 directly. In some instances, usually where
ﬁnetuning data is limited for a task, Flan-T5 without further ﬁnetuning outperforms T5 with task ﬁnetuning.
FasterConvergence&ComputationalBeneﬁts UsingFlan-T5asastartingcheckpointhasanaddedbeneﬁt
intrainingeﬃciency. AsdemonstratedinFigure6,Flan-T5convergesmuchmorequicklythanT5during
single target ﬁnetuning, as well as peaking at higher accuracies. These convergence results also suggest
therearestronggreen-AIincentivesfortheNLPcommunitytoadoptinstruction-tunedmodels,likeFlan-
T5 for single-task ﬁnetuning, rather than conventional non-instruction-tuned models. While instruction
tuning is more computationally-expensive than single-task ﬁnetuning, it is a one-time cost. On the contrary,
pretrained models that require extensive ﬁnetuning become more costly when aggregating over many
millionsofadditionaltrainingsteps(Wuetal.,2022;Bommasanietal.,2021). Instruction-tunedmodelsoﬀer
a promising solution to signiﬁcantly reduce the amount of ﬁnetuning steps across a wide swathe of tasks, if
they are adopted as a new standard starting point for single-task ﬁnetuning.
5 Related Work
Large Language Models Asthefoundationofinstructiontuning,thepracticeofpretrainingonegeneral-
purpose language representation that is useful for multiple downstream tasks has a long tradition that goes
backat least Mikolovet al.(2013) andDaiand Le(2015). In2018, Peterset al.(2018)and Devlinet al.(2019)
cementedtheparadigmofpretrainingalargemodelonalargeunsupervisedcorpus,andtheﬁeldofNLP
quickly converged to using these models which substantially outperform the prior art of non-pretrained
task-speciﬁc LSTM models on all tasks. However, the dominate way to access that high-quality syntactic
andsemanticknowledgeencodedinpretrainedmodelswasnottopromptthemwithinstructions,butto
trainanadditionaltask-speciﬁclinearlayerthatmapsthemodelactivationsintonumericalclasslabels. A
short year later, Radford et al. (2019), Raﬀel et al. (2020), and Lewis et al. (2020) popularized the notion that
downstream tasks—and multiple tasks—can be jointly learned by directly using the pretrained LM head to
generate the answers in natural language (cf. task-speciﬁc numerical class labels), the task-general nature of
thesegenerativemodelsbecametheprecursortomanymultitasktransferlearningstudies(McCannetal.,
2018;Khashabietal.,2020;Yeetal.,2021;Vuetal.,2020),whichinturnledtotheﬁrstwaveofinstruction
tuning as described in Section 2.
The continuing advancement in research on the pretraining corpora, architectures and pretraining objectives
ofLMsalsohasalargeimpactoninstructiontuning. Asof2022,decoder-onlyleft-to-rightcausalTransformers
dominate themarket of modelslarger than100B (Brown etal., 2020;Thoppilanet al.,2022; Rae etal., 2021;
10

--- PAGE 11 ---
Chowdhery et al., 2022; Hoﬀmann et al., 2022), and all models of such size class with fully public model
parameters are decoder-only (Wang and Komatsuzaki, 2021; Le Scao et al., 2022; Zhang et al., 2022), the
decision of which are often due to better hardware and software framework support. However, Raﬀel et al.
(2020),Lewisetal.(2020),andTayetal.(2022a)haveconsistentlyfoundthatleft-to-rightcausallanguage
modelingisa suboptimalobjective, whileTayetal.(2022b) andWanget al.(2022a)particularlyshowedthat
a mixture of non-sequential objectives is much superior for downstream tasks with zero-shot and few-shot
prompting. An additional factor which remains under-explored is the relationship between pretraining
corpora,instructiontuning,anddownstreamabilities. Typically,publicmodelsarealltrainedononeofa
few public corpora: C4 (Raﬀel et al., 2020), The Pile (Gao et al., 2020), or ROOTs (Laurençon et al., 2022).
Instruction Tuning In Section 2 we outline major developments in instruction tuning. Other important
developments include the prospect of complimenting or replacing few-shot in-context learning-the currently
predominate method of evaluating pretrained and instruction-tuned models—with parameter-eﬃcient
tuning. As standard ﬁnetuning of models larger than 100B requires a high number of accelerators with
therightinterconnectsoftentooexpensiveevenformanyindustrylabs,parameter-eﬃcienttuning(a.k.a.
continuous or soft “prompt tuning”) shows that only updating a small subset of model parameters can
reach comparableperformance asfully tuning allmodel parameters(Lester etal., 2021;Vuet al.,2022; Hu
et al., 2021; see He et al., 2022 for a detailed analysis). Notably, Liu et al. (2022b) show that, due to the
long sequence length of few-shot ICL and that the few-shot exemplars need to be repeatedly inferenced for
evaluatingeveryexample,parameter-eﬃcienttuningcanbecomputationallycheaperandhigherperforming
than in-context learning. Further, Liu et al. (2022b), Vu et al. (2022), Wei et al. (2021), and Singhal et al.
(2022) collectively show that both single-task and multi-task parameter-eﬃcient tuning can be productively
combinedwithinstructiontuning,eitherbeforeorafterregularfull-modelinstructiontuning. Thislineof
workmakesiteasyforotherresearcherstobuildontopofageneral-domaininstruction-tunedmodel,and
collect a custom instruction-tuning mixture for their use, e.g., with multiple modalities (Ahn et al., 2022;
Huang et al., 2022; Xu et al., 2022) or special domains such as science and medicine (Lewkowycz et al., 2022;
Singhal et al., 2022).
ProblemsAddressedbyInstructionTuning&AlignmentTechniques Instruction tuning is partof a line
ofworkdesignedto“align”languagemodelswithmoreusefulobjectivesandhumanpreferences. Inthe
absence of such methods, language models are known to demonstrate toxic/harmful behaviour (Sheng et al.,
2019; Liang et al., 2021; Wallace et al., 2019), generate non-factual information (Maynez et al., 2020; Longpre
et al., 2021; Devaraj et al., 2022), and other challenges in deployment and evaluation (Zellers et al., 2019;
McGuﬃe and Newhouse, 2020; Talat et al., 2022). Analyzing, evaluating and mitigating these problems pose
a promisingdirection for future work (Gao etal., 2022; Ganguliet al., 2022). Instruction tuning warrants
greater investigation, as it has already demonstrated itself an encouraging remedy in reducing NLP bias
metrics, as shown in Chung et al. (2022).
6 Conclusions
ThenewFlan2022instructiontuningcollectionuniﬁesthemostpopularpriorpubliccollectionsandtheir
methods,whileaddingnewtemplatesandsimpleimprovementsliketrainingwithmixedpromptsettings.
The resulting collection outperforms Flan 2021, P3++, Super-Natural Instructions, and OPT-IML-Max 175B
onHeld-InQA,NLI,andChain-of-Thoughttasks,andHeld-OutMMLUandBBH,oftenbylargemargins.
Resultssuggestthisnewcollectionservesasamorecompetitivestartingpointforresearchersandpractitioners
interested in both generalizing to new instructions, or ﬁnetuning on a single new task.
Acknowledgements
We would like to thank Ed H Chi, Xinyun Chen, and Colin Raﬀel for their advice and feedback on the paper.
11

--- PAGE 12 ---
References
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
Muppet: Massive multi-task representations with pre-ﬁnetuning. In EMNLP, 2021. URL https://
aclanthology.org/2021.emnlp-main.468 .
MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,ChelseaFinn,
ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,AlexHerzog,DanielHo,JasmineHsu,Julian
Ibarz,BrianIchter,AlexIrpan,EricJang,RosarioJaureguiRuano,KyleJeﬀrey,SallyJesmonth,NikhilJ
Joshi,RyanJulian,DmitryKalashnikov,YuhengKuang,Kuang-HueiLee,SergeyLevine,YaoLu,Linda
Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes,
PierreSermanet,NicolasSievers,ClaytonTan,AlexanderToshev,VincentVanhoucke,FeiXia,TedXiao,
Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do As I Can, Not As I Say: Grounding Language in
Robotic Aﬀordances. arXiv e-prints , art. arXiv:2204.01691, April 2022.
VamsiAribandi,YiTay,TalSchuster,JinfengRao,HuaixiuStevenZheng,SanketVaibhavMehta,Honglei
Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. Ext5: Towards extreme multi-task scaling for transfer
learning. arXiv preprint arXiv:2111.10952 , 2021.
Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raﬀel, Nihal V. Nayak, Abheesht Sharma,
Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun,
Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya
Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, MikeTian-jian Jiang, and
Alexander Rush. PromptSource: An integrated development environment and repository for natural
language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:
System Demonstrations ,pages93–104,Dublin,Ireland,May2022.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2022.acl-demo.9. URL https://aclanthology.org/2022.acl-demo.9 .
YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,Stanislav
Fort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmlessassistantwithreinforcement
learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073 , 2022b.
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual
entailment challenge. In TAC, 2009.
RishiBommasani,DrewAHudson, EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,MichaelS
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258 , 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeﬀrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemod-
els are few-shot learners. NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Hyung Won Chung,
CharlesSutton,SebastianGehrmann,ParkerSchuh,etal. PaLM:ScalinglanguagemodelingwithPathways.
arXiv preprint arXiv:2204.02311 , 2022. URL https://arxiv.org/abs/2204.02311 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-ﬁnetunedlanguagemodels. arXiv preprint
arXiv:2210.11416 , 2022.
12

--- PAGE 13 ---
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristinaToutanova.
Boolq: Exploring the surprising diﬃculty of natural yes/no questions. arXiv preprint arXiv:1905.10044 ,
2019.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvind
Tafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge. arXiv preprint
arXiv:1803.05457 , 2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and
John Schulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
URL https://arxiv.org/abs/2110.14168 .
Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems , vol-
ume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
7137debd45ae4d0ab9aa953017286b20-Paper.pdf .
Ashwin Devaraj, William Sheﬃeld, Byron Wallace, and Junyi Jessy Li. Evaluating factuality in text simpliﬁ-
cation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pages 7331–7345, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:
10.18653/v1/2022.acl-long.506. URL https://aclanthology.org/2022.acl-long.506 .
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. NAACL, 2019. URL https://aclanthology.org/N19-1423 .
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,
Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms:
Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 , 2022.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace
He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027 , 2020.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y
Zhao,NiLao,HongraeLee,Da-ChengJuan,etal. Attributedtextgenerationviapost-hocresearchand
revision. arXiv preprint arXiv:2210.08726 , 2022.
MorGeva, DanielKhashabi,EladSegal, TusharKhot, DanRoth, andJonathanBerant. Didaristotleusea
laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association
for Computational Linguistics , 9:346–361, 2021.
Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
LauraWeidinger,MartinChadwick,PhoebeThacker,etal. Improvingalignmentofdialogueagentsvia
targeted human judgements. arXiv preprint arXiv:2209.14375 , 2022.
PrakharGupta,CathyJiao,Yi-TingYeh,ShikibMehri,MaxineEskenazi,andJeﬀreyPBigham. Improving
zeroand few-shot generalizationin dialoguethrough instructiontuning. arXiv preprint arXiv:2205.12673 ,
2022.
JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-Kirkpatrick,andGrahamNeubig. Towardsauniﬁed
viewofparameter-eﬃcienttransferlearning. In International Conference on Learning Representations ,2022.
URL https://openreview.net/forum?id=0RDcd5Axok .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
Measuring massive multitask language understanding. ICLR, 2020. URL https://openreview.net/
forum?id=d7KBjmI3GmQ .
13

--- PAGE 14 ---
JordanHoﬀmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya, TrevorCai, ElizaRutherford,
DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,TomHennigan,EricNoland,Katie
Millican,GeorgevandenDriessche,BogdanDamoc,AureliaGuy,SimonOsindero,KarenSimonyan,Erich
Elsen, JackW. Rae, OriolVinyals, andLaurent Sifre. Training compute-optimallarge language models.
arXiv preprint arXiv:2203.15556 , 2022.
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language
models with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. URL https:
//arxiv.org/abs/2106.09685 .
Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading com-
prehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 2391–2401, 2019.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
IgorMordatch,YevgenChebotar,PierreSermanet,NoahBrown,TomasJackson,LindaLuu,SergeyLevine,
KarolHausman,andBrianIchter. Innermonologue: Embodiedreasoningthroughplanningwithlanguage
models. In arXiv preprint arXiv:2207.05608 , 2022.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,
Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeﬀ Wang, Christopher
Dewan,AsliCelikyilmaz,LukeZettlemoyer,andVesStoyanov.Opt-iml: Scalinglanguagemodelinstruction
meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. URL https:
//arxiv.org/abs/2212.12017 .
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for
biomedicalresearchquestionanswering. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
pages 2567–2577, 2019. URL https://aclanthology.org/D19-1259 .
NitishShirishKeskar,BryanMcCann,CaimingXiong,andRichardSocher. Unifyingquestionanswering,
text classiﬁcation, and regression via span extraction. arXiv preprint arXiv:1904.09286 , 2019.
DanielKhashabi,SewonMin, TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark, andHannaneh
Hajishirzi. UniﬁedQA:CrossingformatboundarieswithasingleQAsystem. In Findings of the Association for
Computational Linguistics: EMNLP 2020 ,2020. URL https://aclanthology.org/2020.findings-emnlp.
171.
Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven
Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The
bigscience roots corpus: A 1.6 tb composite multilingual dataset. In Thirty-sixth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track , 2022.
TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlić,DanielHesslow,RomanCastagné,
Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access
multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-eﬃcient prompt tun-
ing. EMNLP, 2021. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.
emnlp-main.243 .
MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,Veselin
Stoyanov,andLukeZettlemoyer. BART:Denoisingsequence-to-sequencepre-trainingfornaturallanguage
generation, translation,and comprehension. In Proceedings of the 58th Annual Meeting of the Association for
14

--- PAGE 15 ---
Computational Linguistics ,pages7871–7880,Online,July2020.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703 .
Aitor Lewkowycz, AndersAndreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy
Gur-Ari,andVedantMisra. Solvingquantitativereasoningproblemswithlanguagemodels,2022. URL
https://arxiv.org/abs/2206.14858 .
Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and
mitigating social biases in language models. In ICML, 2021.
Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. Wanli: Worker and ai collaboration
for natural language inference dataset creation. arXiv preprint arXiv:2201.05955 , 2022a. URL https:
//arxiv.org/abs/2201.05955 .
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raﬀel.
Few-shot parameter-eﬃcient ﬁne-tuning is better and cheaper than in-context learning, 2022b. URL
https://arxiv.org/abs/2205.05638 .
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural
language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pages 4487–4496, 2019.
ShayneLongpre,YuWang,andChrisDuBois. Howeﬀectiveistask-agnosticdataaugmentationforpretrained
transformers? In Findings of the Association for Computational Linguistics: EMNLP 2020 ,pages4401–4411,
2020.
ShayneLongpre,KartikPerisetla,AnthonyChen,NikhilRamesh,ChrisDuBois,andSameerSingh. Entity-
based knowledge conﬂicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 7052–7063, 2021.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in
abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics , pages 1906–1919, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/
v1/2020.acl-main.173. URL https://aclanthology.org/2020.acl-main.173 .
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon:
Multitask learning as question answering. arXiv preprint arXiv:1806.08730 , 2018.
Kris McGuﬃe and Alex Newhouse. The radicalization risks of gpt-3 and advanced neural language models.
arXiv preprint arXiv:2009.06807 , 2020.
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english
mathwordproblemsolvers. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics , pages 975–984, 2020.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed represen-
tations of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems , vol-
ume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/
9aa42b31882ec039965f3c4923ce901b-Paper.pdf .
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context.
InNAACL, 2022. URL https://aclanthology.org/2022.naacl-main.201 .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via
natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021.
15

--- PAGE 16 ---
Niklas Muennighoﬀ, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through
multitask ﬁnetuning. arXiv preprint arXiv:2211.01786 , 2022.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeﬀ Wu, Long Ouyang, Christina Kim, Christopher Hesse,
ShantanuJain,VineetKosaraju,WilliamSaunders,etal. Webgpt: Browser-assistedquestion-answering
with human feedback. arXiv preprint arXiv:2112.09332 , 2021.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli:
A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages 4885–4901, 2020.
Long Ouyang, Jeﬀ Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. arXiv preprint arXiv:2203.02155 , 2022. URL https://arxiv.org/abs/2203.02155 .
ZaranaParekh,JasonBaldridge,DanielCer,AustinWaters,andYinfeiYang. Crisscrossedcaptions: Extended
intramodalandintermodalsemanticsimilarityjudgmentsforMS-COCO.In Proceedings of the 16th Conference
of the European Chapter of the Association for Computational Linguistics (EACL) , pages 2855–2870, 2021. URL
https://aclanthology.org/2021.eacl-main.249 .
ArkilPatel,SatwikBhattamishra,andNavinGoyal. Arenlpmodelsreallyabletosolvesimplemathword
problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages 2080–2094, 2021.
MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLuke
Zettlemoyer. Deep contextualized word representations. NAACL, 2018. URL https://aclanthology.
org/N18-1202 .
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang,
ClaraVania,KatharinaKann,andSamuelBowman. Intermediate-tasktransferlearningwithpretrained
language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 5231–5247, 2020.
Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
areunsupervisedmultitasklearners. OpenAI blog ,1(8):9,2019. URL https://d4mucfpksywv.cloudfront.
net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf .
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoﬀmann, Francis Song, John Aslanides,
Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Al-
bin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen
Huang,Amelia Glaese,JohannesWelbl, SumanthDathathri,Saﬀron Huang,JonathanUesato,John Mel-
lor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena
Buchatskaya,DavidBudden,EsmeSutherland,KarenSimonyan,MichelaPaganini,LaurentSifre,Lena
Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato,
Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug
Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson
d’Autume,YujiaLi,TayfunTerzi,VladimirMikulik,IgorBabuschkin,AidanClark,DiegodeLasCasas,
AureliaGuy,ChrisJones,JamesBradbury,MatthewJohnson,BlakeHechtman,LauraWeidinger,Iason
Gabriel,WilliamIsaac,EdLockhart,SimonOsindero,LauraRimell,ChrisDyer,OriolVinyals,Kareem
Ayoub, Jeﬀ Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoﬀrey Irving. Scaling
languagemodels: Methods,analysis&insightsfromtraininggopher. arXiv preprint arXiv:2112.11446 ,2021.
ColinRaﬀel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
Journal of Machine Learning Research , 21:1–67, 2020. URL https://arxiv.org/abs/1910.10683 .
16

--- PAGE 17 ---
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for
squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers) , pages 784–789, 2018.
Abhilasha Ravichander, Matt Gardner, and Ana Marasović. Condaqa: A contrastive reading comprehension
dataset for reasoning about negation. arXiv preprint arXiv:2211.00295 , 2022. URL https://arxiv.org/
abs/2211.00295 .
AdamRoberts,HyungWonChung,AnselmLevskaya,GauravMishra,JamesBradbury,DanielAndor,Sharan
Narang,BrianLester,ColinGaﬀney,AfrozMohiuddin,CurtisHawthorne,AitorLewkowycz,AlexSalcianu,
Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,
AakankshaChowdhery,JasmijnBastings,JannisBulian,XavierGarcia,JianmoNi,AndrewChen,Kathleen
Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raﬀel, Noam Shazeer,
MarvinRitter,MaartenBosma,AlexandrePassos,JeremyMaitin-Shepard,NoahFiedel,MarkOmernick,
Brennan Saeta, RyanSepassi, Alexander Spiridonov, JoshuaNewlan, and Andrea Gesmundo. Scaling up
models and data with t5xand seqio.arXiv preprint arXiv:2203.17189 , 2022. URL https://arxiv.org/
abs/2203.17189 .
AlexeyRomanovandChaitanyaShivade. Lessonsfromnaturallanguageinferenceintheclinicaldomain.
InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,pages
1586–1596, 2018. URL https://aclanthology.org/D18-1187 .
VictorSanh,AlbertWebson,ColinRaﬀel,StephenH.Bach,LintangSutawika,ZaidAlyafeai,AntoineChaﬃn,
Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task
generalization. ICLR 2022 , 2021. URL https://arxiv.org/abs/2110.08207 .
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a babysitter:
On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
pages 3407–3412, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1339. URL https://aclanthology.org/D19-1339 .
Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales,
Ajay Tanwani, HeatherCole-Lewis, StephenPfohl, PerryPayne,Martin Seneviratne,Paul Gamble,Chris
Kelly,NathanealScharli,AakankshaChowdhery,PhilipMansﬁeld,BlaiseAguerayArcas,DaleWebster,
Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar,
Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models
encode clinical knowledge, 2022. URL https://arxiv.org/abs/2212.13138 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
AdamRBrown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,etal. Beyondtheimitationgame:
Quantifyingandextrapolatingthecapabilitiesoflanguagemodels. arXiv preprint arXiv:2206.04615 ,2022.
URL https://arxiv.org/abs/2206.04615 .
MiracSuzgun,NathanScales,NathanealScharli,SebastianGehrmann,YiTay,HyungWonChung,Aakanksha
Chowdhery,QuocV.Le,EdH.Chi,DennyZHou,andJasonWei.ChallengingBIG-Benchtasksandwhether
chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022. URL https://arxiv.org/abs/
2210.09261 .
ZeerakTalat,AurélieNévéol,StellaBiderman,MirunaClinciu,MananDey,ShayneLongpre,AlexandraSasha
Luccioni10,MaraimMasoud11,MargaretMitchell10,DragomirRadev12,etal. Youreapwhatyousow:
On the challenges of bias evaluation under multilingual settings. Challenges & Perspectives in Creating Large
Language Models , page 26, 2022.
AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant.Commonsenseqa: Aquestionanswering
challengetargetingcommonsenseknowledge. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 4149–4158, 2019.
17

--- PAGE 18 ---
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil
Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131 ,
2022a. URL https://arxiv.org/abs/2205.05131 .
Yi Tay, Jason Wei, Hyung Won Chung, David R. So, Siamak Shakeri, Xavier Garcia, Vinh Q. Tran,
Hauixiu Steven Zheng, Jinfeng Rao, Denny Zhou, Donald Metzler, Neil Houlsby, Quoc V. Le, and Mostafa
Dehghani. Transcending scaling laws with 0.1% extra compute. In arxiv, 2022b.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. arXiv
preprint arXiv:2201.08239 , 2022. URL https://arxiv.org/abs/2201.08239 .
TuVu,TongWang,TsendsurenMunkhdalai,AlessandroSordoni,AdamTrischler,AndrewMattarella-Micke,
SubhransuMaji,andMohitIyyer. ExploringandpredictingtransferabilityacrossNLPtasks. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7882–7926, 2020.
URL https://aclanthology.org/2020.emnlp-main.635 .
TuVu,BrianLester,NoahConstant,RamiAl-Rfou’,andDanielCer. SPoT:Betterfrozenmodeladaptation
throughsoftprompttransfer. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (ACL) , pages 5039–5059, 2022. URL https://aclanthology.org/2022.acl-long.346 .
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers
for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
pages 2153–2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1221. URL https://aclanthology.org/D19-1221 .
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:
//github.com/kingoflolz/mesh-transformer-jax , May 2021.
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay,
and Colin Raﬀel. What language model architecture and pretraining objective work best for zero-shot
generalization? ICML, 2022a. URL https://arxiv.org/abs/2204.05832 .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. Self-instruct: Aligninglanguagemodelwithselfgeneratedinstructions,2022b. URL https:
//arxiv.org/abs/2212.10560 .
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana
Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking
generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705 , 2022c.
URL https://arxiv.org/abs/2204.07705 .
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ICLR 2022 , 2021. URL https:
//openreview.net/forum?id=gEZrGCozdqR .
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria
Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental implications, challenges
and opportunities. Proceedings of Machine Learning and Systems , 4:795–813, 2022.
Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via
instruction tuning, 2022. URL https://arxiv.org/abs/2212.10773 .
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossﬁt: A few-shot learning challenge for cross-task general-
ization in NLP. In EMNLP, 2021. URL https://arxiv.org/abs/2104.08835 .
SeonghyeonYe,DoyoungKim,JoelJang,JoongboShin,andMinjoonSeo. Guesstheinstruction! making
language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969 , 2022.
18

--- PAGE 19 ---
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin
Choi. Defending against neural fake news. Advances in neural information processing systems , 32, 2019.
AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,HanyuLai,MingDing,ZhuoyiYang,YifanXu,Wendi
Zheng,XiaoXia,etal. Glm-130b: Anopenbilingualpre-trainedmodel. arXiv preprint arXiv:2210.02414 ,
2022.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 , 2022.
19

--- PAGE 20 ---
Appendix
Table of Contents
A Experimental Details 20
A.1 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.2 Single-Task Finetuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B Input Inversion Details 21
A Experimental Details
A.1 Instruction Tuning
TheFlanCollectionexperimentsareassembledandrunusingT5X(Robertsetal.,2022). Ourinstruction
tuningfollowsthesamesetupdescribedinChungetal.(2022). Forfew-shotandfew-shotChain-of-Thought
promptsduringﬁnetuningourtemplatizingproceduregeneratesfew-shotexampleswith2,3,or5exemplars.
TheexperimentsinthisworkuseaslightlyearlierversionoftheFlan2022collectiontheonewearereleasing,
which had some minor improvements to the templates.
ThemixtureweightsusedtobalancethevarioussourcesofdatawereinformedbyexperimentsinSection
3.5, along with the resulting practitioner intuition.
A.2 Single-Task Finetuning
Forsingle-taskﬁnetuning, describedinSection4, ourmodelsareﬁnetunedfor100 ;000stepsforalltasks. We
useaconstantlearningrateof0.001,adropoutprobabilityof0.1,andabatchsizeof128length-512sequences.
We save a checkpoint every 20 steps and report test performance on the model checkpoint corresponding to
the highest validation performance. For tasks without a validation split, we hold out 1024 training examples
forvalidation. Fortaskswithoutatestsplit,weholdout1024trainingexamplesforvalidationandreport
results on the original validation set. For PubmedQA, we do not use any of the unlabeled and artiﬁcially
generatedQAinstancesassociatedwiththedataset. ForCxC,weonlyconsiderthetext-textportionofthe
dataset, following Vuet al. (2022). Fortasks with less than 1K training examples, we report average results
across 3 random seeds.
We also evaluate on certain metrics to account for label skew in some of the datasets, as shown in Table 3.
A.3 Evaluation
For Held-In evaluations we use the validation sets from 4 question answering (QA) tasks, BoolQ, ARC Easy,
ARC Challenge, and AI2’s Middle School Science Exams, and 4 natural language inference (NLI) tasks,
including ANLI R1, R2, R3, and RTE. These datasets are contained in the Flan 2022 ﬁnetuning collection and
representchallengingbenchmarks,oftenusedtoevaluateLLMsonQAandNLI.TheHeld-Inscoreisthe
mean accuracy across these 8 tasks.
20

--- PAGE 21 ---
U/s.sc/e.sc/d.sc /i.sc/n.sc
D/a.sc/t.sc/a.sc/s.sc/e.sc/t.sc M/e.sc/t.sc/r.sc/i.sc/c.sc H/e.sc/l.sc/d.sc-I/n.sc C/o.scT ST-FT H/e.sc/l.sc/d.sc-I/n.sc ST-FT H/e.sc/l.sc/d.sc-O/u.sc/t.sc C/i.sc/t.sc/a.sc/t.sc/i.sc/o.sc/n.sc
ARC E+C Acc X X (Clark et al., 2018)
ANLI R1+R2+R3 3-class F1 X X (Nie et al., 2020)
AI2 Mid. Science 4-class F1 X X AI2 Science Questions
BoolQ AUC-ROC X X (Clark et al., 2019)
RTE AUC-ROC X X (Bentivogli et al., 2009)
SQuAD V2 F1 X (Rajpurkar et al., 2018)
CosmosQA Acc X (Huang et al., 2019)
GSM8K Acc X (Cobbe et al., 2021)
StrategyQA Acc X (Geva et al., 2021)
SVAMP Acc X (Patel et al., 2021)
Asdiv Acc X (Miao et al., 2020)
CommonsenseQA Acc X (Talmor et al., 2019)
WANLI Acc X (Liu et al., 2022a)
MedNLI Acc X (Romanov and Shivade, 2018)
CondaQA Acc X (Ravichander et al., 2022)
PubmedQA F1 X (Jin et al., 2019)
CxC Spearman X (Parekh et al., 2021)
Table3:Datasets used forVarious Finetuning and Evaluation Experiments. ST-FTstandsforSingleTask
Finetuning.
FortheChain-of-Thought(CoT)evaluation,weusethemeanaccuracyacross5datasetswhichhavebeen
preparedwithpromptswhichrequeststep-by-stepexplanationsintheirtargetanswers: GSM8K,StrategyQA,
SVAMP, Asdiv, and CommonsenseQA.
FortheHeld-Outtasks,weuseMMLU’ssuiteof57exams,andBBH’ssuiteof23taskswherePaLMperformed
worse than the average human annotators. MMLU tasks were removed from the Super-Natural Instructions
part of the Flan 2022 collection at training, to ensure they were Held-Out.
B Input Inversion Details
For the input inversion experiments we note that Flan 2021, P3++, and Super-Natural Instructions already
implicitlyincludetasksthathavebeeninverted,e.g. questionansweringtoquestionorcontextgeneration.
Consequently,wechoosetoalsocreateinputinversionsfortheremainingdatasetsintheFlan2022collection,
including for the Dialog, Program Synthesis, and Chain-of-Thought tasks.
As examples: for Dialog tasks, we write template instructions asking for the previous conversational history
fromthecurrentdialogturn;forprogramsynthesisweaskforthecodingquestionwhichthecodesolves;and
forChain-of-Thoughtweincludeeverypermutationofthequery-answer-explanationtriple,whereatleast
one of the three appears as the in output. An illustration of Chain-of-Thought input inversion permutations
are shown in Figure 7.
Theseinversionsaremixedinwiththeexistingtasksatarateof30%,meaningforaDialogtask,3inverted
examples will be generated for every 10 regular examples. We choose this rate for simplicity, approximately
mirroring prior work, and leave the large space of exploration for future work.
21

--- PAGE 22 ---
Question Chain-of-Thought Answer 
Question Answer Chain-of-Thought 
Question Answer Chain-of-Thought Question Answer Chain-of-Thought 
Question Answer Chain-of-Thought 
Question Answer Chain-of-Thought Inputs T argets Figure 7: Input Inversions permutations for a Zero-Shot Chain-of-Thought example. Each is accompanied by a
corresponding instruction template that prompts the model with what the input is, and what to predict as the targets.
22
