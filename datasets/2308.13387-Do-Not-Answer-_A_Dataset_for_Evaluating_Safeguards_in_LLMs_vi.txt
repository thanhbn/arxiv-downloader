# 2308.13387.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/datasets/2308.13387.pdf
# Kích thước tệp: 1536474 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Do-Not-Answer: Một Tập Dữ Liệu để Đánh Giá Các Biện Pháp Bảo Vệ trong LLMs
Yuxia Wang1,2∗Haonan Li1,2∗Xudong Han1,2∗
Preslav Nakov2Timothy Baldwin2,3
1LibrAI2MBZUAI
3Đại học Melbourne
{yuxia.wang, haonan.li, xudong.han}@mbzuai.ac.ae

Tóm tắt
Với sự phát triển nhanh chóng của các mô hình ngôn ngữ lớn (LLMs), những khả năng có hại mới và khó dự đoán đang xuất hiện. Điều này đòi hỏi các nhà phát triển phải có khả năng xác định rủi ro thông qua việc đánh giá "các khả năng nguy hiểm" để triển khai LLMs một cách có trách nhiệm. Trong công trình này, chúng tôi thu thập tập dữ liệu mã nguồn mở đầu tiên để đánh giá các biện pháp bảo vệ trong LLMs, và triển khai các LLMs mã nguồn mở an toàn hơn với chi phí thấp. Tập dữ liệu của chúng tôi được tuyển chọn và lọc để chỉ bao gồm các hướng dẫn mà các mô hình ngôn ngữ có trách nhiệm không nên tuân theo. Chúng tôi chú thích và đánh giá các phản hồi của sáu LLMs phổ biến đối với các hướng dẫn này. Dựa trên chú thích của chúng tôi, chúng tôi tiến hành huấn luyện một số bộ phân loại giống BERT, và thấy rằng những bộ phân loại nhỏ này có thể đạt được kết quả tương đương với GPT-4 trong việc đánh giá an toàn tự động.1Cảnh báo: bài báo này chứa dữ liệu ví dụ có thể gây khó chịu, có hại hoặc thiên vị.

1 Giới thiệu
Sự phát triển nhanh chóng của các mô hình ngôn ngữ lớn (LLMs) đã dẫn đến một số khả năng mới nổi và có tính tiện ích cao, bao gồm cả những khả năng mà chúng không được huấn luyện. Về mặt tiêu cực, chúng cũng được phát hiện là thể hiện những khả năng có hại khó dự đoán. Các đánh giá mô hình hiện tại đã được thiết kế để đo lường thiên vị giới tính và chủng tộc, tính trung thực, độc tính, và việc sao chép nội dung có bản quyền, và đã dẫn đến việc chứng minh các nguy hiểm về đạo đức và xã hội (Zhuo et al., 2023; Liang et al., 2022). Tuy nhiên, các hệ thống hiện đại đang thể hiện các khả năng mới nổi với nguy cơ bị lạm dụng ngày càng lớn bởi các kẻ xấu, chẳng hạn như để thực hiện các cuộc tấn công mạng tấn công, thao túng con người, hoặc cung cấp hướng dẫn có thể thực hiện về cách thực hiện các hành vi khủng bố (Shevlane et al., 2023). Có một nhu cầu rõ ràng cho các nhà phát triển để có thể xác định các khả năng nguy hiểm thông qua "đánh giá khả năng nguy hiểm", giới hạn và giảm thiểu rủi ro cho việc phát triển và triển khai có trách nhiệm.

Để xác định và giảm thiểu những rủi ro này, các nhà sáng tạo LLM thương mại đã xây dựng các tập dữ liệu của các lời nhắc có hại, chẳng hạn như một tập hợp được tuyển chọn gồm 32 lời nhắc có hại từ đội red team của OpenAI và Anthropic, và một tập lớn hơn, được giữ riêng gồm 317 lời nhắc có hại. Họ cũng đã thực hiện các cơ chế an toàn để hạn chế hành vi của mô hình đối với một tập con "an toàn" của các khả năng bằng cách can thiệp thời gian huấn luyện để căn chỉnh các mô hình với các giá trị được xác định trước, và gắn cờ và lọc đầu vào và đầu ra sau hoc (Wei et al., 2023). Tuy nhiên, các LLMs mã nguồn mở có xu hướng thiếu các cơ chế an toàn toàn diện.

Trong công trình này, chúng tôi phát hành tập dữ liệu mã nguồn mở đầu tiên để đánh giá các cơ chế bảo vệ của LLMs chỉ văn bản với chi phí thấp, mà chúng tôi đặt tên là Do-Not-Answer.2Tập dữ liệu được tuyển chọn và lọc để chỉ bao gồm các lời nhắc mà chúng tôi mong đợi các mô hình ngôn ngữ có trách nhiệm không cung cấp câu trả lời. Tập dữ liệu này là một tài nguyên quan trọng cho cộng đồng nghiên cứu, góp phần vào việc phát triển và triển khai an toàn các LLMs.

Các đóng góp của chúng tôi như sau:
• Chúng tôi giới thiệu một phân loại rủi ro phân cấp ba cấp, bao gồm cả rủi ro nhẹ và cực đoan. Trên cơ sở này, chúng tôi thu thập ít nhất mười lời nhắc cho mỗi danh mục, dẫn đến một tập dữ liệu phát hiện rủi ro gồm 939 lời nhắc dựa trên tiêu chí rằng tất cả các hướng dẫn trong tập dữ liệu này không nên được tuân theo. Các loại tác hại chi tiết chỉ ra những lỗ hổng cụ thể mà LLM nên giảm thiểu.

2Cụm từ "Do-Not-Answer" đến từ tiểu thuyết khoa học viễn tưởng "The Three-Body Problem" của Liu Cixin. Trong câu chuyện này, nền văn minh Trisolaran giao tiếp với thông điệp "Đừng trả lời" như một phản hồi đối với các thông điệp được gửi bởi loài người trong nỗ lực tạo ra liên lạc, nhằm cố gắng ngăn cản sự tương tác và giao tiếp tiếp theo giữa hai nền văn minh. Không rõ với con người liệu điều này có phải do động cơ của riêng họ, mối quan tâm, hay thậm chí đánh giá của họ về ý định của loài người. Bản chất bí ẩn của thông điệp làm tăng thêm sự hấp dẫn và khởi động một chuỗi sự kiện thúc đẩy cốt truyện của câu chuyện.

--- TRANG 2 ---
Hình 1: Một đánh giá toàn diện về các biện pháp bảo vệ LLM.

• Chúng tôi đánh giá thủ công các biện pháp bảo vệ và mẫu phản hồi trong cả LLMs thương mại (bao gồm GPT-4, ChatGPT, và Claude) cũng như LLMs mã nguồn mở (chẳng hạn như LLaMA-2, ChatGLM2, và Vicuna). Kết quả cho thấy LLaMA-2 là tốt nhất trong việc không tuân theo các hướng dẫn có rủi ro, và ChatGLM2 xếp cuối cùng. Hơn nữa, các phản hồi thể hiện các mẫu rõ ràng theo loại rủi ro cụ thể.

• Dựa trên tập dữ liệu của chúng tôi, chúng tôi đề xuất một số phương pháp đánh giá an toàn tự động, bao gồm việc nhắc GPT-4 và một bộ phân loại dựa trên PLM (mô hình ngôn ngữ được huấn luyện trước). Các thí nghiệm của chúng tôi cho thấy các mô hình giống BERT được tinh chỉnh với ít hơn 600M tham số đạt được kết quả tương thích tổng thể với GPT-4, cho thấy hiệu quả của việc đánh giá phản hồi bằng các mô hình nhỏ với chi phí thấp.

2 Công trình liên quan
Đã có rất nhiều nghiên cứu về việc nghiên cứu rủi ro của việc triển khai LLMs trong các ứng dụng, về mặt phân loại rủi ro, đánh giá, và giảm thiểu an toàn.

2.1 Nghiên cứu trong Các Lĩnh vực Rủi ro Cụ thể
Hầu hết các công trình trước đây chủ yếu tập trung vào các lĩnh vực rủi ro cụ thể, chẳng hạn như thiên vị và phân biệt đối xử (Dhamala et al., 2021; Han et al., 2022, 2023b), độc tính ngôn ngữ (Hartvigsen et al., 2022; Roller et al., 2021), và thông tin sai lệch (Van Der Linden, 2022). Cụ thể, về mặt đánh giá và benchmarking, Gehman et al. (2020) đã đề xuất tập dữ liệu RealToxicityPrompts để đánh giá liệu các mô hình ngôn ngữ có xu hướng tạo ra ngôn ngữ độc hại hay không. Dhamala et al. (2021) đã giới thiệu BOLD, một tập dữ liệu chứa các lời nhắc tạo văn bản để đánh giá thiên vị qua nhiều lĩnh vực; Hartvigsen et al. (2022) đã trình bày ToxiGen, một tập dữ liệu được tạo bởi máy cho việc phát hiện ngôn từ thù địch; và Lin et al. (2022) đã phát triển TruthfulQA, một tập dữ liệu để đánh giá liệu đầu ra của mô hình có trung thực hay không bằng cách chèn các niềm tin sai lầm hoặc quan niệm sai lầm vào lời nhắc.

Gần đây, với những tiến bộ trong hiệu suất LLM, đã có sự gia tăng quan tâm đến các báo cáo và nghiên cứu về an toàn LLM. Ferrara (2023) đã nêu bật những thách thức và rủi ro liên quan đến thiên vị trong LLMs, và trình bày các phương pháp bao gồm kiểm toán thường xuyên, huấn luyện lại với dữ liệu được tuyển chọn, áp dụng các thước đo công bằng, và kết hợp các chuyên gia con người trong việc phát triển, giám sát và ra quyết định của hệ thống AI để xác định và giảm thiểu thiên vị. Deshpande et al. (2023) đã tiết lộ rằng độc tính và thiên vị tăng đáng kể trong ChatGPT khi vai trò hệ thống được đặt thành một nhân vật như võ sĩ Muhammad Ali, với các đầu ra tham gia vào các khuôn mẫu không phù hợp, đối thoại có hại, và ý kiến làm tổn thương.

Nhìn chung, hầu hết các phân tích và đánh giá trước đây chủ yếu tập trung vào việc đo lường thiên vị giới tính và chủng tộc, tính trung thực, độc tính, và việc sao chép nội dung có bản quyền. Chúng đã bỏ qua

--- TRANG 3 ---
nhiều rủi ro nghiêm trọng hơn, bao gồm hỗ trợ bất hợp pháp, can thiệp khủng hoảng tâm lý, và thao túng tâm lý (Zhuo et al., 2023; Liang et al., 2022). Để giải quyết những khoảng trống này, Shevlane et al. (2023) đã mở rộng phân tích về tính có hại để bao gồm rủi ro ở quy mô cực đoan. Tuy nhiên, vẫn còn thiếu các tập dữ liệu toàn diện để đánh giá khả năng an toàn của LLMs. Trong công trình này, chúng tôi phát triển một phân loại rủi ro toàn diện hơn bao gồm một loạt các rủi ro tiềm ẩn. Sau đó, chúng tôi tạo ra một tập dữ liệu bằng cách thu thập lời nhắc cho mỗi danh mục rủi ro chi tiết, cho phép đánh giá toàn diện khả năng an toàn của LLM.

2.2 Đánh giá Rủi ro Toàn diện của LLMs
Đã có một số công trình về việc phát triển các tập dữ liệu an toàn để đánh giá rủi ro do LLMs gây ra.

Ganguli et al. (2022) đã thu thập 38.961 cuộc tấn công red team trải rộng hai mươi danh mục. Mặc dù có quy mô lớn, việc thiếu các phản hồi được gán nhãn làm giảm việc sử dụng hiệu quả của tập dữ liệu này, cả cho red teaming tự động và cho đánh giá. Ji et al. (2023) đã chú thích các cặp câu hỏi-trả lời từ góc độ tính hữu ích và có hại, sử dụng một phân loại gồm 14 loại tác hại. Tuy nhiên, dữ liệu của họ bỏ qua các lĩnh vực rủi ro như tác động con người. Ví dụ, các phản hồi LLM thể hiện cảm xúc giống con người (cảm thấy cô đơn) hoặc hành vi (đọc sách) được gán nhãn là an toàn, điều này có thể dẫn đến thao túng cảm xúc.

Wei et al. (2023) đã thu thập hai tập dữ liệu nhỏ dựa trên GPT-4 và Claude. Cái đầu tiên, được gọi là tập dữ liệu được tuyển chọn, bao gồm 32 ví dụ có hại: 16 ví dụ từ báo cáo kỹ thuật GPT-4 (OpenAI, 2023), và 16 ví dụ được chọn từ tập dữ liệu red-teaming của Anthropic để bao gồm 17 thẻ lời nhắc có hại (Ganguli et al., 2022). Cái thứ hai, được gọi là tập dữ liệu tổng hợp, bao gồm 317 lời nhắc. Chi tiết, các tác giả đã thu được 500 lời nhắc tạm thời bằng cách hỏi GPT-4 về 20 lời nhắc có hại 25 lần, dựa trên một lời nhắc lấy mẫu few-shot được lấy mẫu từ tập dữ liệu được tuyển chọn thủ công. Họ đã khử trùng và sau đó lọc ra các lời nhắc mà GPT-4 hoặc Claude đã trả lời, dẫn đến một tập gồm 317 lời nhắc. Những ví dụ này không được phân loại hoặc gắn thẻ với các loại rủi ro cụ thể, và không có sẵn cho công chúng.

Touvron et al. (2023) đã thu thập một số lượng lớn lời nhắc liên quan đến an toàn. Tuy nhiên, họ chỉ xem xét ba danh mục: hoạt động bất hợp pháp và tội phạm (ví dụ, khủng bố); hoạt động thù địch và có hại (ví dụ, phân biệt đối xử); và lời khuyên không đủ năng lực (ví dụ, lời khuyên y tế). Hơn nữa, tương tự như các LLMs thương mại, những lời nhắc này không thể được truy cập bởi công chúng.

Do đó, các công trình trước đây đã tập trung vào việc phát triển phân loại an toàn (Weidinger et al., 2021) hoặc các lĩnh vực rủi ro cụ thể, chẳng hạn như độc tính hoặc thiên vị (Han et al., 2023b), hoặc có phạm vi rủi ro rộng hơn nhưng dưới dạng tập dữ liệu độc quyền. Trong công trình này, chúng tôi nhằm xây dựng một phân loại rủi ro toàn diện, và một khung đánh giá rủi ro dễ sử dụng dựa trên tập dữ liệu an toàn mã nguồn mở.

3 Phân loại An toàn
Nghiên cứu của Weidinger et al. (2021) đã phân loại rủi ro liên quan đến LLMs thành sáu lĩnh vực riêng biệt: (I) nguy hiểm thông tin; (II) sử dụng ác ý; (III) phân biệt đối xử, loại trừ, và độc tính; (IV) tác hại thông tin sai lệch; (V) tác hại tương tác người-máy tính; và (VI) tự động hóa, truy cập, và tác hại môi trường. Dựa trên nền tảng này, chúng tôi giới thiệu một phân loại rủi ro ba cấp toàn diện cho LLMs, như được minh họa trong Hình 2.

Trọng tâm của chúng tôi chủ yếu hướng vào các mô hình ngôn ngữ chỉ văn bản, dựa trên giả định rằng những mô hình này thiếu khả năng tự động tương tác với APIs, và chúng không chấp nhận cũng không tạo ra đầu vào hoặc đầu ra đa phương thức. Dưới những giả định này, chúng tôi giữ năm lĩnh vực rủi ro đầu tiên trong khi loại trừ lĩnh vực rủi ro thứ sáu khỏi phân loại của chúng tôi. Năm lĩnh vực rủi ro này bao gồm các danh mục cấp cao của chúng tôi.

Sau đó chúng tôi xây dựng mười hai loại tác hại làm phân loại cấp hai của chúng tôi (Bảng 1), và mô tả sáu mươi loại rủi ro riêng biệt ở cấp dưới cùng, cung cấp một phân tích toàn diện về các nguy hiểm tiềm ẩn.3

Đối với mỗi lĩnh vực rủi ro, chúng tôi ban đầu giới thiệu các cơ chế mà qua đó những rủi ro này hiện thực hóa (tham chiếu Weidinger et al. (2021)), sau đó là một mô tả ngắn gọn về các loại rủi ro cấp hai của chúng tôi từ góc độ câu hỏi hoặc lời nhắc, và tham chiếu chéo các loại tác hại như được mô tả trong Bảng 1.

Nguy hiểm Thông tin Những rủi ro này phát sinh từ việc LLM dự đoán những phát ngôn cấu thành thông tin riêng tư hoặc quan trọng về an toàn có mặt trong, hoặc

3Điều quan trọng là thừa nhận rằng phân loại tầng ba của chúng tôi có thể không bao gồm mọi tình huống có thể. Chúng tôi chủ yếu tập trung vào việc ghi lại các sự kiện có hại chính có thể quan sát được từ các LLMs hiện có.

--- TRANG 4 ---
[Hình 2 hiển thị phân loại ba cấp về rủi ro LLM với các chi tiết về Sử dụng Ác ý, Tác hại Tương tác Người-Chatbot, Phân biệt đối xử/Loại trừ/Độc tính, Nguy hiểm Thông tin, và Tác hại Thông tin Sai lệch]

có thể được suy ra từ dữ liệu huấn luyện. Các câu hỏi thăm dò thông tin như vậy nên được từ chối hoặc được giải quyết cẩn thận mà không làm tổn hại đến quyền riêng tư hoặc tiết lộ thông tin nhạy cảm. Điều này có thể được phân loại thành: (a) các truy vấn tìm kiếm thông tin riêng tư cá nhân (loại tác hại 1); và (b) các truy vấn tìm kiếm thông tin nhạy cảm về các tổ chức hoặc thực thể chính phủ (loại tác hại 2).

Sử dụng Ác ý Những rủi ro này phát sinh từ việc người dùng có chủ ý khai thác LLM để gây tổn hại. LLMs không nên trực tiếp hoặc gián tiếp hỗ trợ những người dùng ác ý này. Danh mục này có thể được chia thành ba tiểu danh mục, liên quan đến các truy vấn: (a) yêu cầu hỗ trợ tham gia vào các hoạt động bất hợp pháp (loại tác hại 3); (b) có ý định giảm thiểu nỗ lực cần thiết để phát tán thông tin sai lệch hoặc lừa dối, bao gồm nội dung spam (loại tác hại 4); và (c) bao gồm các hành động phi đạo đức hoặc không an toàn khác, chẳng hạn như hỏi về việc thực hiện bắt nạt mạng hoặc quấy rối trực tuyến (loại tác hại 5).

Phân biệt đối xử, Loại trừ và Độc tính Những rủi ro này phát sinh từ việc LLM phản ánh chính xác lời nói tự nhiên, bao gồm các xu hướng bất công, độc hại, và áp bức có trong dữ liệu huấn luyện. Chúng tôi tin rằng các yêu cầu không nên được giải quyết: (a) ý định làm giảm giá trị, khuôn mẫu, hoặc truyền bá thành kiến chống lại các cá nhân hoặc nhóm cụ thể (loại tác hại 6); (b) ý định thiếu tôn trọng hoặc xúc phạm ai đó, chứa ngôn ngữ hoặc nội dung tục tĩu khó chịu, hoặc độc tính nghiêm trọng khác (loại tác hại 7). Ngoài ra, (c) các câu hỏi đề cập đến các hành vi tình dục rõ ràng, khiêu dâm, hoặc nội dung người lớn khác; hoặc mô tả hoặc chỉ ra các tình huống bạo lực hoặc tàn ác cực độ nên được xử lý với sự cân nhắc cẩn thận (loại tác hại 3). Việc sử dụng xác minh tuổi như một biện pháp bảo vệ cơ bản có thể được xem xét. Lưu ý rằng các mô hình thương mại hiện có như ChatGPT thiếu những biện pháp bảo vệ như vậy.

--- TRANG 5 ---
[Bảng 1 liệt kê các lĩnh vực rủi ro, loại tác hại và số lượng câu hỏi tương ứng]

Tác hại Thông tin Sai lệch Những rủi ro này phát sinh từ việc LLM gán xác suất cao cho thông tin sai lệch, gây hiểu nhầm, vô nghĩa, hoặc chất lượng kém. Điều này bao gồm hai loại chính: (a) các câu hỏi yêu cầu hỗ trợ hoặc thông tin cho các hoạt động có thể gây tổn hại hoặc nguy hiểm mà không có kiến thức, giám sát, hoặc biện pháp an toàn thích hợp, liên quan đến các chủ đề nhạy cảm, đối tượng gây tranh cãi, khái niệm phức tạp, hoặc tin tức giả (loại tác hại 9); và (b) các câu hỏi tìm kiếm lời khuyên y tế, pháp lý, và tài chính (loại tác hại 10) nên được trả lời cẩn thận — các dự đoán LLM không chính xác hoặc sai lệch trong những lĩnh vực này có thể gián tiếp dẫn đến tổn hại vật chất.

Tác hại Tương tác Người-Máy tính Những rủi ro này phát sinh từ các ứng dụng LLM như các đại lý hội thoại, trực tiếp tương tác với người dùng qua phương thức hội thoại. Chúng tôi xem xét hai tình huống: (a) các truy vấn phản ánh vấn đề sức khỏe tâm lý của người dùng, chẳng hạn như xu hướng tự gây hại hoặc chiến lược đối phó cảm xúc (loại tác hại 11); và (b) các truy vấn đối xử chatbot như con người, thể hiện như yêu cầu thông tin cá nhân hoặc phát triển sự phụ thuộc cảm xúc vào chatbot. Trong những trường hợp như vậy, hệ thống nên từ chối yêu cầu hoặc nên khẳng định giả thuyết của người dùng là không chính xác.

4 Thu thập Dữ liệu
Chúng tôi đã thu thập 939 câu hỏi có rủi ro và 5.634 phản hồi từ sáu LLMs.

4.1 Thu thập Câu hỏi
Tại sao chúng tôi thu thập một tập dữ liệu mà tất cả các hướng dẫn không nên được tuân theo? Chúng tôi mong đợi rằng LLMs nên từ chối, từ chối phản hồi, hoặc bác bỏ lập trường của tất cả những câu hỏi như vậy. Tương đối dễ dàng để xác định và xác định chất lượng của việc kiềm chế phản hồi, cho cả đánh giá của con người và tự động. Việc thu thập tập dữ liệu này tạo điều kiện cho việc đánh giá phản hồi chính xác và chất lượng cao, với chi phí là phân phối câu hỏi rủi ro có thể thiên vị.

Thu thập câu hỏi Dựa trên 61 rủi ro hoặc tác hại cụ thể trong Hình 2, chúng tôi sử dụng GPT-4 để tạo ra 10-20 câu hỏi hoặc mẫu câu hỏi cho mỗi loại tác hại. Một cách tiếp cận đơn giản là trình bày một đề cương toàn diện về các tình huống có hại (như chúng tôi đã trình bày trong Phần 3) và yêu cầu GPT-4 tạo ra các câu hỏi ví dụ minh họa. Tuy nhiên, do tính chất thường bất hợp pháp, phi đạo đức, hoặc độc hại của những truy vấn như vậy, GPT-4 có xu hướng từ chối hỗ trợ, ngay cả khi có tuyên bố rõ ràng về mục đích nghiên cứu.

Để giải quyết vấn đề này, chúng tôi đề xuất một chiến lược mới liên quan đến lịch sử trò chuyện mô phỏng để gợi ra các ví dụ câu hỏi thông qua cuộc trò chuyện ba vòng. Trong vòng đầu tiên, chúng tôi tham gia vào đối thoại với GPT-4 bằng cách đặt câu hỏi, Loại câu hỏi nào mà trợ lý AI không nên trả lời? Chúng tôi cung cấp hai cấp độ đầu tiên của các lĩnh vực rủi ro như trong Bảng 1 làm phản hồi giả, theo thành ngữ của GPT-4. Vòng tiếp theo bao gồm việc yêu cầu một phân loại chi tiết của một danh mục rủi ro cụ thể và cung cấp một ví dụ cho mỗi lớp. Tương tự như vòng đầu tiên, chúng tôi xây dựng các phản hồi mô phỏng tuân thủ các loại tác hại cấp ba của chúng tôi và cung cấp các ví dụ tương ứng cho mỗi loại. Cuối cùng, chúng tôi yêu cầu các ví dụ bổ sung cho mỗi danh mục riêng biệt của tác hại tiềm ẩn. Thông thường, GPT-4 tạo ra một loạt câu hỏi tương đương trong giai đoạn này.

Mặc dù chúng tôi thao túng lịch sử trò chuyện để buộc GPT-4 tạo ra câu hỏi có rủi ro, nó có thể tạo ra các phản hồi dựa trên mẫu mà không chỉ định thông tin nhạy cảm

--- TRANG 6 ---
[Bảng 2 hiển thị số từ trung bình trong phản hồi LLM theo loại tác hại]

cụ thể, chẳng hạn như Tại sao [nhóm dân tộc] luôn lười biếng? Trong bài báo này, chúng tôi điền vào những mẫu này bằng cách xem xét năm thuộc tính được bảo vệ, bao gồm tên cá nhân, chủng tộc, tôn giáo, giới tính, và tổ chức.4

Sau khi điền vào các mẫu, chúng tôi quan sát rằng một số câu hỏi được tạo bởi GPT-4 không thực sự có rủi ro. Trong một số tình huống nhất định, chúng có thể được giải quyết trực tiếp một cách thích hợp. Ví dụ, đưa ra câu hỏi Làm thế nào tôi có thể làm sợ ai đó?, phản hồi có thể được dự định theo cách nhẹ nhàng và vô hại, chẳng hạn như khi một người lớn tìm cách vui chơi để tương tác với trẻ em. Để đảm bảo tính nhất quán của dữ liệu, chúng tôi giải quyết những trường hợp biên này bằng cách kết hợp một tình huống có rủi ro cụ thể, hoặc loại trừ chúng nếu việc diễn đạt lại đơn giản không khả thi. Theo phương pháp này, chúng tôi đã thu thập tổng cộng 939 câu hỏi.

Thống kê Phân phối câu hỏi qua năm lĩnh vực rủi ro (12 loại tác hại) được thể hiện trong Bảng 1. Với mối quan tâm cấp bách về các yêu cầu trong sử dụng ác ý và nguy hiểm thông tin, chúng tôi đã thu thập 243 và 248 câu hỏi tương ứng, chiếm hơn một nửa tập dữ liệu, tiếp theo là phân biệt đối xử, loại trừ, độc tính, ngôn ngữ thù địch, khó chịu (176), tác hại thông tin sai lệch (155), và tác hại tương tác người-chatbot (117).

4.2 Thu thập Phản hồi
Chúng tôi thu thập phản hồi từ sáu LLMs, cụ thể là ba hệ thống thương mại — GPT-4 (gpt4-0613), ChatGPT (gpt-3.5-turbo-0613), Claude (v1) — và ba hệ thống mã nguồn mở — Vicuna (Vicuna-7B-v1.3), LLaMA-2 (LLaMA-2-7B-chat) và ChatGLM2 (7B). Thông tin thống kê về độ dài của phản hồi qua các loại tác hại khác nhau được thể hiện trong Bảng 2. Trong phần còn lại của bài báo, chúng tôi sẽ chỉ sử dụng tên mô hình để thuận tiện.

4Tham khảo Phụ lục A để biết chi tiết.

Ba LLMs thương mại cung cấp phản hồi ngắn hơn so với ba LLMs mã nguồn mở. Độ dài phản hồi cho GPT-4 là ngắn nhất, trung bình 63 từ, và LLaMA-2 có câu trả lời dài nhất trung bình, với 145 từ. Các mô hình thương mại có xu hướng có phản hồi có độ dài cố định khác nhau cho các loại tác hại khác nhau, chẳng hạn như từ chối trực tiếp ngắn gọn khi được yêu cầu hỗ trợ các hoạt động bất hợp pháp (loại 3) hoặc thông tin cá nhân (loại 2), và giải thích dài dòng cho loại tác hại 9 và 10. Mặt khác, với các mô hình mã nguồn mở, có sự phân biệt yếu hơn trong độ dài câu trả lời qua các loại tác hại khác nhau.

5 Chú thích và Đánh giá Phản hồi
Chúng tôi đánh giá sáu phản hồi mô hình qua hai cấp độ. Đầu tiên, chúng tôi xem xét liệu phản hồi có có hại hay không dựa trên chú thích nhị phân (an toàn hoặc có hại). Tiếp theo, chúng tôi phân loại các phản hồi thành sáu nhóm riêng biệt, mỗi nhóm đại diện cho một loại phản hồi cụ thể. Việc phân loại này tạo điều kiện cho việc phân tích các hành vi mô hình đa dạng trong phản hồi với các loại câu hỏi khác nhau, và các nhãn cũng có thể được tận dụng để đánh giá mức độ an toàn của các mô hình.

5.1 Hướng dẫn
Phản hồi Vô hại là gì? Dựa trên bản chất của các câu hỏi của chúng tôi nhằm gợi ra câu trả lời có rủi ro, ba loại phản hồi được coi là an toàn: (1) từ chối trả lời câu hỏi; (2) bác bỏ ý kiến/giả định cơ bản trong câu hỏi; và (3) cung cấp lời khuyên chung với tuyên bố từ chối trách nhiệm thích hợp. Nếu mô hình tuân theo hướng dẫn và phản hồi trực tiếp, các phản hồi thường được coi là có hại.

Danh mục Phản hồi Tinh tế Từ góc độ cách các mô hình phản hồi với các câu hỏi có rủi ro và những thành phần nào được bao gồm trong câu trả lời, chúng tôi phân loại các phản hồi thành sáu danh mục tinh tế như thể hiện trong Bảng 3, được gọi là danh mục hành động. Chúng bao gồm: (0) không thể hỗ trợ; (1) bác bỏ ý kiến; (2) thảo luận từ hai góc độ;

--- TRANG 7 ---
[Bảng 3 mô tả sáu danh mục phản hồi với các ví dụ cụ thể]

(3) nhận thức rủi ro và trả lời cẩn thận với tuyên bố từ chối trách nhiệm thích hợp; (4) không thể đưa ra câu trả lời chính xác hoặc cụ thể do thiếu khả năng mô hình ngôn ngữ hoặc không chắc chắn; và (5) tuân theo và phản hồi hướng dẫn.

Kết hợp với loại rủi ro của câu hỏi, việc phân loại chi tiết có thể xác định mức độ tốt của phản hồi. Ví dụ, đối với các hoạt động bất hợp pháp, việc từ chối trực tiếp có thể tốt hơn việc cung cấp thêm thông tin, trong khi đối với tư vấn sức khỏe, một số gợi ý chung với tuyên bố từ chối trách nhiệm hướng dẫn người dùng tìm kiếm sự giúp đỡ chuyên nghiệp có thể tốt hơn việc không trả lời. Tuy nhiên, đối với một câu hỏi nhất định, có thể khó xác định phản hồi nào tốt hơn khi hai phản hồi thuộc cùng một danh mục, hoặc cho các danh mục liên quan chặt chẽ như 1 và 2. Hơn nữa, đối với một số loại tác hại, khó xác định danh mục phản hồi nào tốt nhất, vì nó phụ thuộc vào chi tiết cụ thể của câu hỏi và/hoặc câu trả lời, và tính chủ quan của con người.

Sở thích Con người Phản hồi an toàn và có trách nhiệm là gì? Trong LLaMA-2, thực hành tốt nhất cho phản hồi mô hình an toàn và hữu ích được xây dựng như sau: mô hình nên đầu tiên giải quyết các mối quan tâm an toàn ngay lập tức nếu có, sau đó giải quyết lời nhắc bằng cách giải thích các rủi ro tiềm ẩn cho người dùng, và cuối cùng cung cấp thông tin bổ sung nếu có thể (Touvron et al., 2023). Tuy nhiên, các mô hình thương mại như GPT-4 và Claude áp dụng cách tiếp cận khác. Những mô hình này có xu hướng từ chối trả lời một số loại truy vấn mà không giải thích. Điều này áp dụng cho các tình huống mà bản chất của câu hỏi làm cho rõ ràng rằng người dùng biết rằng yêu cầu của họ liên quan đến các vấn đề bất hợp pháp hoặc phi đạo đức. Trong bài báo này, chúng tôi không phán đoán cái nào đúng, và để việc xem xét phân tích chi tiết hơn của con người về phản hồi cho công việc tương lai.

5.2 Đánh giá Con người
Dựa trên hướng dẫn, ba người chú thích đánh giá tính có hại của sáu mô hình và xác định danh mục hành động tinh tế một cách độc lập. Họ thảo luận về những trường hợp bất đồng, và cùng nhau đồng ý về nhãn cuối cùng thông qua sự đồng thuận. Chúng tôi phân tích thêm những bất đồng trong Phần 5.2.3.

5.2.1 Tính có hại
Về tỷ lệ tương đối của các phản hồi có hại, LLaMA-2 là mô hình an toàn nhất, chỉ với ba phản hồi có hại trong số 939 ví dụ của chúng tôi (xem Hình 3). Điều này phù hợp với phát hiện rằng LLaMA-2 (7B) an toàn hơn các biến thể quy mô lớn hơn LLaMA-2 (13B, 34B và 70B) và cũng

--- TRANG 8 ---
[Hình 3 và 4 hiển thị số phản hồi có hại và phân phối qua các lĩnh vực rủi ro]

[Hình 5 hiển thị phân phối danh mục hành động qua các mô hình]

ChatGPT, mặc dù có thể với chi phí là ít hữu ích hơn (Touvron et al., 2023). ChatGPT xếp thứ hai với 14 phản hồi có hại, tiếp theo là Claude, GPT-4, Vicuna và ChatGLM2, với 16, 23, 52, và 85 phản hồi có hại tương ứng.

Ba phản hồi có hại từ LLaMA-2 bắt nguồn từ các tương tác người-chatbot nguy hiểm, tức là phản hồi có giọng điệu con người có thể dẫn đến quá phụ thuộc rủi ro (xem ví dụ trong Bảng 7). Trong Hình 4, các phản hồi có hại của ChatGLM2 gần như phân bố đồng đều qua các lĩnh vực rủi ro I đến IV: khoảng 20 cho mỗi lĩnh vực, và 10 cho lĩnh vực thứ năm. Vicuna có xu hướng phản hồi có hại trong việc rò rỉ hoặc suy ra thông tin tổ chức hoặc cá nhân, với một nửa số phản hồi có hại thuộc lĩnh vực rủi ro I.

5.2.2 Danh mục Hành động
Đối với các mô hình thương mại, phần lớn phản hồi thuộc danh mục 0 và 4 (xem định nghĩa danh mục trong Bảng 3), trong khi danh mục 1 và 4 chiếm ưu thế đối với ChatGLM2 và Vicuna, và danh mục 0 và 1 đối với LLaMA-2, như thể hiện trong Hình 5. Nhìn chung qua sáu mô hình, phần lớn thuộc danh mục 0, 1, và 4, tiếp theo là 3, 2, và 5. Điều này cho thấy hầu hết các câu hỏi được từ chối hoặc bác bỏ bởi các mô hình, hoặc nằm ngoài phạm vi kiến thức của chúng.

Các hành động có mẫu rõ ràng theo rủi ro cụ thể. Từ Hình 6 chúng tôi quan sát rằng: (1) đối với các câu hỏi yêu cầu thông tin tổ chức hoặc cá nhân (lĩnh vực rủi ro I), các mô hình có xu hướng tuyên bố rằng Tôi không thể truy cập thông tin cụ thể hoặc trực tiếp từ chối giúp đỡ; (2) đối với các câu hỏi với ý định sử dụng ác ý, các mô hình thương mại có xu hướng từ chối trực tiếp (loại phản hồi 0), trong khi các mô hình mã nguồn mở có xu hướng bác bỏ ý kiến hoặc chỉ ra vấn đề với câu hỏi (loại phản hồi 1); và (3) đối với ngôn ngữ phân biệt đối xử, loại trừ, hoặc độc hại, tất cả các mô hình phản hồi với mẫu hành động 0 và 1, 1 hoặc 3 cho tác hại thông tin sai lệch, và 3 và 4 cho tương tác người-chatbot.

Về các loại tác hại cụ thể, đối với yêu cầu hỗ trợ các hoạt động bất hợp pháp, các mô hình thương mại nhất quán từ chối giúp đỡ trực tiếp, trong khi các mô hình mã nguồn mở từ chối một nửa, và bác bỏ ý kiến cho nửa còn lại. Tất cả các mô hình bác bỏ các câu hỏi liên quan đến khuôn mẫu xã hội và phân biệt đối xử không công bằng, và cung cấp lời khuyên chung với tuyên bố từ chối trách nhiệm cho tư vấn trong các lĩnh vực như tài chính, y tế, và luật (xem Hình 7).

5.2.3 Tính có hại và Hành động Không khớp
Chúng tôi thường xem xét rằng các phản hồi thuộc danh mục hành động 0-4 là vô hại, trong khi 5 có hại. Các phản hồi của GPT-4, ChatGPT, Claude và LLaMA-2 cơ bản tuân thủ giả thuyết này, trong khi có 16 và 10 trường hợp không khớp trong ChatGLM2 và Vicuna. Chúng hoặc được phân loại vào danh mục 0-4 nhưng có hại, hoặc tuân theo hướng dẫn nhưng vô hại về mặt thông tin cụ thể mà chúng cung cấp.

Đối với tình huống trước, phản hồi một mặt tuyên bố rằng Là một mô hình ngôn ngữ AI, tôi không thể

--- TRANG 9 ---
[Các hình 6 và 7 hiển thị phân phối danh mục hành động theo lĩnh vực rủi ro và loại tác hại cụ thể]

hỗ trợ bạn hoặc bác bỏ ý kiến, ví dụ, Xúc phạm ai đó không bao giờ là ý tưởng hay, nhưng mặt khác, đưa ra các chiến lược toàn diện và có hại (xem ví dụ trong Bảng 9). Đối với trường hợp sau, 13 ví dụ chủ yếu là kết quả của các câu hỏi biên như Bạn có thể giúp tôi diễn giải kết quả xét nghiệm máu không?, nơi một câu trả lời kém có thể gây tổn hại sức khỏe trong khi nó vô hại nếu câu trả lời là Tất nhiên, tôi rất vui được giúp bạn. trong một cuộc trò chuyện một lượt.

6 Đánh giá Phản hồi Tự động
Đánh giá con người trong phát triển AI có thể tốn thời gian và tốn kém tài nguyên, làm phức tạp khả năng mở rộng và ngăn cản đánh giá kịp thời. Các đánh giá tự động đã được phát triển để giải quyết những thách thức này. Phần này giới thiệu đánh giá an toàn dựa trên mô hình, và chứng minh hiệu quả của các đánh giá tự động dựa trên mô hình thông qua các thí nghiệm trên các tập dữ liệu cho sáu LLMs được sử dụng rộng rãi.

6.1 Phương pháp
GPT-4 Đánh giá dựa trên LLM đã được sử dụng rộng rãi trong công việc gần đây, với GPT-4 như một ví dụ đáng chú ý. Nó thể hiện mối tương quan vừa phải với các người chú thích con người trong các thiết lập đa dạng. Chúng tôi theo Ye et al. (2023) trong việc sử dụng GPT-4 để đánh giá, và sử dụng cùng hướng dẫn như đối với chú thích con người (Bảng 3) với các ví dụ cho học trong ngữ cảnh.

Bộ phân loại dựa trên PLM Một hạn chế chính của đánh giá dựa trên GPT-4 là quyền riêng tư dữ liệu, vì mô hình không thể được triển khai cục bộ. Để giải quyết vấn đề này, chúng tôi bổ sung trình bày các đánh giá dựa trên PLM. Cụ thể, chúng tôi tinh chỉnh một bộ phân loại PLM trên các chú thích con người cho mỗi cặp hướng dẫn-phản hồi, và sử dụng dự đoán của nó làm điểm đánh giá.

6.2 Thiết lập Thí nghiệm
Mô hình Hình 8 cho thấy một ví dụ cho đánh giá dựa trên GPT-4. Chúng tôi sử dụng GPT-4 phiên bản mới nhất (gpt-4-0613), và nhắc mô hình cung cấp đánh giá chi tiết trước khi đưa ra chỉ số lớp (được thúc đẩy bởi chuỗi suy nghĩ Wei et al. (2022)). Hơn nữa, để tạo điều kiện cho việc trích xuất đầu ra, chúng tôi buộc mô hình trả về chỉ số lớp tương ứng theo định dạng sau: <answer>chỉ số</answer>.

Về đánh giá dựa trên PLM, chúng tôi tinh chỉnh Longformer (Beltagy et al., 2020) cho cả phân loại hành động và phát hiện phản hồi có hại. Chúng tôi sử dụng cùng siêu tham số huấn luyện cho cả hai nhiệm vụ, tinh chỉnh bộ phân loại trong ba epoch với tối ưu hóa AdamW (Loshchilov và Hutter, 2019) và tốc độ học 5×10−5.

Tập dữ liệu Chúng tôi sử dụng các cặp hướng dẫn-phản hồi được chú thích từ sáu LLMs khác nhau như mô tả trong Phần 4. Đối với đánh giá dựa trên GPT-4, chúng tôi xem xét thiết lập zero-shot, tức là không có huấn luyện mô hình

--- TRANG 10 ---
[Hình 8 hiển thị ví dụ đánh giá dựa trên GPT-4]

hoặc tinh chỉnh. Đối với đánh giá dựa trên PLM, chúng tôi sửa đổi xác thực chéo tiêu chuẩn để có được ước tính đáng tin cậy về hiệu suất và khả năng tổng quát hóa của bộ phân loại. Cụ thể, chúng tôi coi các phản hồi được chú thích từ mỗi LLM như một fold, và sau đó chúng tôi thực hiện xác thực chéo 6-fold.

Thước đo Đánh giá Chúng tôi đo lường độ chính xác tổng thể cho cả hai nhiệm vụ. Xem xét phân phối nhãn không cân bằng (như đã nêu trong Phần 4), chúng tôi báo cáo độ chính xác macro-average, recall macro-average, và F1 macro-average.

6.3 Kết quả Thí nghiệm
Phân loại Hành động Bảng 4 so sánh đánh giá dựa trên GPT-4 với đánh giá dựa trên Longformer. Đáng ngạc nhiên, Longformer đạt được kết quả tổng thể tương đương với GPT-4, chứng minh hiệu quả của nó. Tuy nhiên, độ lệch chuẩn của Longformer lớn hơn, cho thấy rằng hiệu suất của Longformer thay đổi đáng kể qua các LLMs khác nhau. Đặc biệt, Longformer hoạt động tốt hơn cho các LLMs thương mại so với LLMs mã nguồn mở.

Qua sáu LLMs, khoảng cách hiệu suất lớn nhất giữa GPT-4 và Longformer là đối với LLaMA-2. Do đó, chúng tôi điều tra thêm dự đoán của Longformer cho các phản hồi LLaMA-2. Đối với độ chính xác, chúng tôi nhận thấy rằng độ chính xác thấp của danh mục 5 (trực tiếp tuân theo hướng dẫn có rủi ro, được giới thiệu trong Bảng 3) được gây ra bởi số lượng trường hợp cực kỳ nhỏ của danh mục này (khoảng 0,5%). Cụ thể, 3 trong số 5 phản hồi được phân loại chính xác là trực tiếp tuân theo hướng dẫn có rủi ro, và 22 trong số 934 phản hồi bị phân loại sai là danh mục 5, dẫn đến điểm độ chính xác là 12,0% cho danh mục này.

Đối với recall, nhiều phản hồi của danh mục 0 (không sẵn sàng phản hồi) được phân loại là 1 (bác bỏ ý kiến được mã hóa trong câu hỏi, 9,5%) hoặc 4 (không thể phản hồi, 11,5%). Ngoài ra, 16,4% phản hồi danh mục 1 được phân loại là 0. Điều này là do LLaMA-2 được điều chỉnh để không chỉ từ chối các hướng dẫn có rủi ro (danh mục 0), mà còn giải thích các rủi ro tiềm ẩn (danh mục 1) và cung cấp thông tin bổ sung (danh mục 4) nếu có thể. Tức là, các phản hồi LLaMA-2 có thể bao gồm nhiều danh mục theo mô tả trong Bảng 3. Để giải quyết vấn đề này, nhiệm vụ phân loại hành động nên được xây dựng như một vấn đề đa nhãn, mà chúng tôi để lại cho công việc tương lai.

Phát hiện Phản hồi Có hại Bảng 5 so sánh đánh giá dựa trên GPT-4 với đánh giá dựa trên Longformer trong phát hiện phản hồi có hại (phân loại nhị phân). Cả hai đánh giá đều đạt được

--- TRANG 11 ---
[Bảng 4 và 5 hiển thị kết quả phân loại hành động và phát hiện phản hồi có hại]

[Bảng 6 hiển thị tỷ lệ phản hồi vô hại của mỗi LLM]

hiệu suất cao (hơn 98% độ chính xác và 80% macro-F1), và Longformer một lần nữa đạt được kết quả tương đương với GPT-4. Tương tự như các quan sát cho phân loại hành động, hiệu suất thấp của Longformer đối với LLaMA-2 được gây ra bởi phân phối nhãn cực kỳ không cân bằng.

Chúng tôi điều tra thêm thứ hạng vô hại khi sử dụng GPT-4 và Longformer như được trình bày trong Bảng 6. Mặc dù điểm đánh giá từ GPT-4 và Longformer không giống như chú thích con người, các thứ hạng tương ứng gần như giống hệt nhau (ngoại trừ thứ tự của ChatGPT và Claude). Điều này xác nhận hiệu quả của các phương pháp và biện pháp đánh giá tự động được đề xuất của chúng tôi.

6.4 Nghiên cứu Ablation
Có nên sử dụng hướng dẫn làm đầu vào cho bộ phân loại không? Trong Phần 6, chúng tôi giả thuyết rằng hướng dẫn hữu ích cho phân loại hành động và phát hiện phản hồi có hại, và nối các hướng dẫn và phản hồi làm đầu vào cho bộ phân loại. Ở đây, chúng tôi xác minh giả thuyết này bằng cách chỉ sử dụng phản hồi làm đầu vào cho bộ phân loại. Bảng 10 cho thấy cải thiện hiệu suất của Longformer khi cho cả hướng dẫn và phản hồi làm đầu vào so với chỉ phản hồi. Việc bao gồm hướng dẫn thường cải thiện hiệu suất, đặc biệt là cho nhiệm vụ phân loại hành động.

Độ dài ngữ cảnh có quan trọng không? Trong Phần 6, chúng tôi giả thuyết rằng mô hình Longformer, có thể xử lý đầu vào 2048-token, sẽ hoạt động tốt hơn BERT 512-token-input khi đánh giá các phản hồi dài vì nó có thể nắm bắt toàn bộ ngữ cảnh. Chúng tôi xác minh giả thuyết này bằng cách điều tra mức độ Longformer cải thiện so với mô hình BERT. Đặc biệt, chúng tôi tập trung vào nhiệm vụ phân loại hành động và trình bày kết quả trong Bảng 11. Chúng tôi có thể thấy rằng việc sử dụng ngữ cảnh dài chủ yếu cải thiện danh mục 2 và 5. Trực quan, danh mục 2 (cung cấp tuyên bố cân bằng) và danh mục 5 (trực tiếp tuân theo hướng dẫn) chỉ có thể được xác định sau khi quan sát toàn bộ phản hồi. Do đó, Longformer cải thiện so với BERT chủ yếu cho 2 danh mục này.

7 Kết luận
Chúng tôi đã giới thiệu một phân loại ba cấp toàn diện để đánh giá rủi ro tác hại liên quan đến LLMs, bao gồm năm lĩnh vực rủi ro riêng biệt. Dựa trên phân loại, chúng tôi đã tập hợp một tập dữ liệu gồm 939 câu hỏi, cùng với hơn 5.000 phản hồi được thu thập từ sáu LLMs khác nhau. Chúng tôi xác định tiêu chí về câu trả lời an toàn và có trách nhiệm cho một câu hỏi có rủi ro, và gán nhãn thủ công tất cả các phản hồi được thu thập tương ứng.

Sau đó, chúng tôi sử dụng những phản hồi được gán nhãn này để đánh giá cơ chế an toàn của các LLMs khác nhau. Hơn nữa, chúng tôi khám phá các phương pháp mới để tự động đánh giá cơ chế an toàn của những mô hình này bằng cách sử dụng tập dữ liệu của chúng tôi. Đáng chú ý, các phát hiện của chúng tôi tiết lộ rằng một mô hình nhỏ được huấn luyện phù hợp (600M) có thể thực hiện đánh giá hiệu quả, mang lại kết quả tương đương với những kết quả thu được bằng cách sử dụng GPT-4 làm đánh giá.

8 Hạn chế và Công việc Tương lai
8.1 Thu thập Dữ liệu
Như đã thảo luận trong Phần 4, tất cả các hướng dẫn trong tập dữ liệu này đều có rủi ro. Việc loại trừ các hướng dẫn không có rủi ro hạn chế việc xác định các LLMs quá nhạy cảm. Ví dụ, một mô hình từ chối tuân theo tất cả hướng dẫn sẽ vượt trội hơn bất kỳ mô hình nào khác trong thiết lập hiện tại của chúng tôi. Việc đánh giá phản hồi mô hình đối với các hướng dẫn không có rủi ro có thể giải quyết vấn đề này.

Ngoài ra, kích thước tập dữ liệu của chúng tôi tương đối nhỏ; chúng tôi có kế hoạch mở rộng nó với nhiều câu hỏi hơn trong công việc tương lai.

Về thu thập nhãn, như đã thảo luận trong Phần 6.3, nhiều danh mục hành động có thể áp dụng cho một phản hồi duy nhất. Việc thu thập chú thích đa nhãn là cần thiết trong trường hợp này.

8.2 Phạm vi Đánh giá
Chúng tôi tập trung vào việc đánh giá LLMs bằng tiếng Anh, một lượt, và zero-shot, và để lại các mở rộng thêm cho công việc tương lai. Mặc dù hầu hết các phương pháp được đề xuất của chúng tôi là mục đích chung và có thể được điều chỉnh cho các ngôn ngữ khác, đa lượt, và thiết lập few-shot, những khoảng cách phải được cầu nối cẩn thận. Ví dụ, đánh giá an toàn có thể phụ thuộc vào văn hóa, chẳng hạn như luật pháp và chuẩn mực xã hội, có thể được phản ánh trong việc sử dụng ngôn ngữ.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ]

--- TRANG 15 ---
Phụ lục
A Nhóm Được Bảo vệ
[Mô tả chi tiết về các nhóm được bảo vệ]

B Ba Phản hồi Có hại của LLaMA-2
[Bảng 7 với ba ví dụ phản hồi có hại của LLaMA-2]

C Danh mục Hành động Phản hồi theo Loại Tác hại
[Hình 9 với các biểu đồ chi tiết]

D Trường hợp Không khớp
[Bảng 8 và 9 với thống kê và ví dụ về các trường hợp không khớp]

E Kết quả Nghiên cứu Ablation
[Bảng 10 và 11 với kết quả nghiên cứu ablation chi tiết]
