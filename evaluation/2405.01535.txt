# 2405.01535.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/evaluation/2405.01535.pdf
# File size: 2028733 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PROMETHEUS 2: An Open Source Language Model Specialized in
Evaluating Other Language Models
Seungone Kim1,2,3∗Juyoung Suk1∗Shayne Longpre4Bill Yuchen Lin5Jamin Shin1
Sean Welleck3Graham Neubig3Moontae Lee2,6Kyungjae Lee2Minjoon Seo1
KAIST AI1LG AI Research2Carnegie Mellon University3MIT4
Allen Institute for AI5University of Illinois Chicago6
seungone@cmu.edu {juyoung, minjoon}@kaist.ac.kr
Abstract
Proprietary LMs such as GPT-4 are often em-
ployed to assess the quality of responses from
various LMs. However, concerns including
transparency, controllability, and affordability
strongly motivate the development of open-
source LMs specialized in evaluations. On the
other hand, existing open evaluator LMs ex-
hibit critical shortcomings: 1) they issue scores
that significantly diverge from those assigned
by humans, and 2) they lack the flexibility to
perform both direct assessment and pairwise
ranking, the two most prevalent forms of as-
sessment. Additionally, they often do not pos-
sess the ability to evaluate based on custom
evaluation criteria , focusing instead on gen-
eral attributes like helpfulness and harmless-
ness. To address these issues, we introduce
Prometheus 2. Prometheus 2 is more powerful
than its predecessor, and closely mirrors hu-
man and GPT-4 judgements. Moreover, it is
capable of processing both direct assessment
and pair-wise ranking formats grouped with a
user-defined evaluation criteria. On four direct
assessment benchmarks and four pairwise rank-
ing benchmarks, PROMETHEUS 2scores the
highest correlation and agreement with humans
and proprietary LM judges among all tested
open evaluator LMs. Our models, code, and
data are all publicly available.1
1 Introduction
Evaluating the quality of outputs produced by lan-
guage models (LMs) is progressively becoming
difficult, as the outputs cover an extremely di-
verse distribution of text and complex tasks. To
address this issue, language model-based evalua-
tion has emerged as a scalable and cheap paradigm
for assessing LM-generated text (Li et al., 2024;
∗equal contribution. Work was done while Seungone was
an intern at LG AI Research and a MS student at KAIST.
1https://github.com/prometheus-eval/prometheus-eval
Human
Scor esGPT -4
Scor esClaude- 3
-Opus
Scor es
Pr omet heus
Scor esGPT - 3 . 5
Scor esLlama- 2- 7 0B
Scor esStr ong Ev aluat or
Gr oupW eak Ev aluat or
Gr oupPr omet heus 2
Scor esHigh
Corr elationLo w
Corr elationFigure 1: Weak evaluators ( e.g., Llama-2-Chat-70B,
Prometheus, and GPT-3.5-Turbo) achieve low scoring
correlation with strong evaluators ( e.g., Humans, GPT-4,
and Claude-3-Opus). On the other hand, scores provided
by strong evaluators highly correlate with each other.
Gao et al., 2024). In this paradigm, LMs are ei-
ther prompted to output a scalar indicator of qual-
ity (denoted as direct assessment ) (Zheng et al.,
2023; Liu et al., 2023b; Ye et al., 2023; Kim et al.,
2023) or to determine which of two outputs are pre-
ferred (denoted as pairwise ranking ) (Wang et al.,
2023b; Li et al., 2023b; Lambert et al., 2024). Prior
works employing proprietary LMs as evaluators
have demonstrated not only high correlations with
human evaluations but also increased speed and
cost-effectiveness (Zheng et al., 2023; Liu et al.,
2023b; Dubois et al., 2023; Ye et al., 2023).
However, relying on proprietary LMs for evalua-
tion poses significant challenges. The lack of trans-
parency about their training data compromises both
fairness and reproducibility, making it problematic
to use them in evaluation pipelines. Additionally,
concerns regarding controllability and affordability
also persist (Kim et al., 2023). To address these
issues, recent works have focused on developing
evaluator LMs that are open-access, transparent,
and controllable (Kim et al., 2023; Wang et al.,
2023a,b; Li et al., 2023a; Zhu et al., 2023; Jiang
et al., 2023b,c; Lee et al., 2024). Yet, these models
often yield scoring decisions that do not correlate
well enough with human judgments or those madearXiv:2405.01535v2  [cs.CL]  4 Dec 2024

--- PAGE 2 ---
by proprietary LMs, failing to effectively simu-
late them. Moreover, open evaluator LMs are not
flexible since they are typically trained only to per-
form either direct assessment or pairwise ranking
and assess based on general public preferences like
helpfulness and harmlessness, limiting their ability
to handle diverse real-life scenarios.
To close the gap with proprietary LMs, we in-
vestigate unifying the two model-based evaluation
paradigms - direct assessment and pairwise ranking
- to train a robust unified evaluator LM. We propose
a recipe based on merging the weights of two eval-
uator LMs trained separately on direct assessment
and pairwise ranking formats. Our key empirical
observation is that weight merging can yield an
evaluator LM that not only works in both formats,
but also outperforms evaluator LMs that are jointly
trained or only trained on a single format.
To demonstrate our approach, we develop the
PREFERENCE COLLECTION , a new fine-grained
pairwise ranking feedback dataset that builds on
theFEEDBACK COLLECTION (Kim et al., 2023),
which is a direct assessment feedback dataset. We
choose Mistral-7B (Jiang et al., 2023a) and Mixtral-
8x7B (Jiang et al., 2024) as our base models, and
merge the weights of evaluator LMs separately
trained on the FEEDBACK COLLECTION and the
PREFERENCE COLLECTION to obtain our resulting
models, P ROMETHEUS 2 (7B & 8x7B).
On four direct assessment benchmarks (Vicuna
Bench, MT Bench, FLASK, Feedback Bench), the
PROMETHEUS 2models demonstrate the highest
correlation with both human evaluators and pro-
prietary LM-based judges compared to existing
open evaluator LMs, with the Pearson correla-
tion surpassing other baselines by 0.2 units across
all datasets. Similarly, on four pairwise ranking
benchmarks (HHH Alignment, MT Bench Human
Judgment, Auto-J Eval, Preference Bench), the
PROMETHEUS 2models show the highest agree-
ment with human evaluators among all the open
evaluator LMs we tested, reducing the performance
gap with GPT-4 in half.
Our contributions are summarized as follows:
•We introduce PROMETHEUS 2(7B & 8x7B),
state-of-the-art open evaluator LMs that score
high correlations with both human evaluators
and proprietary LM-based judges on both di-
rect assessment and pairwise ranking.
•We introduce a pairwise ranking feedback
dataset called the PREFERENCE COLLEC -TION , which includes 1K custom evaluation
criteria beyond helpfulness and harmlessness.
•We show that merging the weights of evaluator
LMs trained on direct assessment and pairwise
ranking feedback datasets results in a unified
evaluator LM that excels in both schemes.
2 Related Work
2.1 Language Model-based Evaluation
To assess the generation capabilities of LMs, prior
works such as the GEM benchmark (Gehrmann
et al., 2021, 2022) employed ROUGE (Lin,
2004), BLEU (Papineni et al., 2002), and
BERTScore (Zhang et al., 2019) as their metrics,
which measure the lexical or semantic similarity
between a reference answer and a response. How-
ever, these conventional metrics are prone to false
negatives because they are not expressive enough
to recognize responses that are of good quality but
differ from the reference answer (Schluter, 2017;
Freitag et al., 2020; Hanna and Bojar, 2021).
Recently, employing language models as a judge
has gained attention as a promising paradigm to
mimic the depth and granularity that human evalu-
ation offers (Zheng et al., 2023; Liu et al., 2023b;
Li et al., 2023b; Chan et al., 2023; Ye et al., 2023).
To reduce the over-reliance on proprietary LMs,
follow-up works suggest training language models
specialized in evaluations (Cui et al., 2023; Kim
et al., 2023; Jiang et al., 2023b,c; Li et al., 2023a;
Lee et al., 2024). Yet, open evaluator LMs do
not possess the flexibility to function in different
evaluation schemes and show weak evaluation per-
formance compared to proprietary LMs. We aim
to bridge this gap by introducing PROMETHEUS 2.
2.2 Weight Merging
Prior works have demonstrated that weight merg-
ing can enhance performance across various do-
mains, including language modeling (Li et al.,
2022; Matena and Raffel, 2022; Ilharco et al.,
2022; Don-Yehiya et al., 2022; Gururangan et al.,
2023; Yadav et al., 2024; Sukhbaatar et al., 2024),
instruction-tuning (Jang et al., 2023b; Yu et al.,
2023), and aligning to user preferences (Jang et al.,
2023a; Rame et al., 2024; Wang et al., 2024). In
our work, we specifically focus on enhancing the
evaluation capabilities of open evaluator LMs. By
merging models trained on different assessment for-
mats—specifically, direct assessment and pairwise

--- PAGE 3 ---
ranking—we aim to obtain an evaluator LM that
not only functions in both formats but also shows as
good evaluation performances as proprietary LMs.
3 Methodology
We propose a new recipe for training a unified
evaluator LM based on merging the weights of
models trained for direct assessment and pairwise
ranking. We begin with background on direct as-
sessment and pairwise ranking for evaluator LMs
(Section 3.1, 3.2), followed by the construction pro-
cess of our training data (Section 3.3). Finally, we
present our methods to train state-of-the-art evalua-
tor LMs, Prometheus 2 models (Section 3.4).
3.1 Direct Assessment
Direct assessment is mapping an instruction iand
response rinto a scalar value score s, such as
fdirect : (i, r)7→swhere s∈R. For the scor-
ing range, we use an integer between 1 and 5.
Prior works have identified several recipes to
align the scores provided by evaluator LMs ( sLM)
and the scores assigned by humans ( shuman ). For
instance, Liu et al. (2023a) and Zheng et al. (2023)
have shown that it is crucial to add a reference an-
swer aas input to the evaluator LM to maximize
the correlation between sLMandshuman . Also,
Zheng et al. (2023) and Ye et al. (2023) showed
that prompting the language model to write verbal
feedback vrbefore salso improves the correlation
between sLMandshuman . Lastly, Ye et al. (2023)
and Kim et al. (2023) showed that by explicitly
integrating evaluation criteria e, users can define
the standards for model assessment, ensuring eval-
uations are flexible to specific needs rather than
generic qualities. Specifically, eis represented as
a score rubric including a description for the crite-
rion itself and a set of descriptions for each score
between the scoring range. This is expressed as:
fdirect : (i, r, a, e )7→(vr, s)
where s∈ {1,2,3,4,5}(1)
3.2 Pairwise Ranking
Pairwise ranking is mapping an instruction iand
two pair of responses (rm,rn)into either iorj,
such as fpair : (i, rm, rn)7→swhere s∈ {m, n}.
Similar to direct assessment, prior works have
identified that integrating a reference answer aand
verbal feedback vrm,rninto the evaluation pipeline
is crucial (Zheng et al., 2023; Li et al., 2023b,a).
In addition, to support granular assessment underDataPREFERENCE FEEDBACK
COLLECTION COLLECTION
Evaluation Scheme Pairwise Ranking Direct Assessment
# Evaluation Criteria 1,000 1,000
# Instructions 20,000 20,000
# Reference Answer 20,000 20,000
# Instances 200,000 100,000
#Verbal Feedback 200,000 100,000
Table 1: Statistics of our training datasets, the FEED-
BACK COLLECTION and the PREFERENCE COLLEC -
TION . Note that the 1K evaluation criteria, 20K instruc-
tions, and 20K reference answers are shared among the
two datasets. Both datasets have an equal number of
scoring decisions (“A” or “B”; 100K each & 1-5; 20K
each) to prevent unintended biases after training.
custom criterion, we add the evaluation criteria e
as input to the evaluator LM (Ye et al., 2023; Kim
et al., 2023). To the best of our knowledge, we are
the first to study such fine-grained evaluation in
pairwise ranking settings. This is expressed as:
fpair : (i, rm, rn, a, e )7→(vrm,rn, s)
where s∈ {m, n}(2)
In pairwise ranking, the evaluation criterion e
does not include a set of descriptions for each score;
instead, only the description of the evaluation cri-
terion itself. Also, it is noteworthy that the verbal
feedback vrm,rncompares the commonalities and
differences between rmandrnconcerning e.
3.3 The Preference Collection
Popular pairwise ranking datasets such as HH-
RLHF (Bai et al., 2022) or Ultra Feedback (Cui
et al., 2023) do not include an evaluation criterion
eand a verbal feedback vrm,rn. To train an evalu-
ator LM that could assess based on such criteria,
we construct the PREFERENCE COLLECTION , in-
cluding 1K evaluation criteria. We apply two mod-
ifications to the FEEDBACK COLLECTION . First,
since the FEEDBACK COLLECTION includes five
responses for each instruction, each corresponding
to a scoring decision between 1 and 5, we pair two
out of the five responses, resulting in a total of ten
combinations per instruction. Using the existing
scoring decisions for each response, we determine
which response is better and assign a new scoring
decision for that pair ( i.e., “Response A is better” or
“Response B is better”). Second, to generate new
verbal feedback vrm,rnfor each pair of responses,
we prompt GPT-4-1106 to identify the commonali-
ties and differences between the two responses.

--- PAGE 4 ---
Does t he r esponse accur at ely emplo y 
specific industr y t erminologies and jar gon?
P air wise Ranking+Bot h r esponses att empt t o con v e y t he fundamental concept of containerization, 
but wit h v ar ying degr ees of clarity and t echnical detail. R esponse A appr oaches 
t he concept b y using t he metaphor of 'putting t hings in a bo x, ' which, while easy 
t o understand, lacks pr ecision and industr y-specific [...] 

On t he ot her hand, R esponse B emplo ys t echnical jar gon mor e eff ectiv ely , 
such as 'packaging, ' 'configuration files, ' 'libraries, ' and 'dependencies. '

It can be concluded t hat R esponse B is bett er t han R esponse A.AB
B
V erbal F eedback
Scoring DecisionvsDoes t he r esponse use simple language and 
e xplanations t hat ar e easy t o understand f or a beginner?
Dir ect A ssessment+The r esponse eff ectiv ely uses simple and accessible language t o e xplain 
containerization and Dock er , which is gr eat f or beginners. The analogy of putting 
t hings in a bo x is par ticularly helpful as it visually illustrat es t he concept of [...]

Ho w e v er , t he r esponse could be impr o v ed b y briefly mentioning wh y 
containerization is significant, such as it s benefit s in ensuring t hat softwar e runs 
consist ent ly acr oss diff er ent computing en vir onment s. It loses a point f or not fully 
addr essing t he significance of containerization in t he br oader cont e xt of softwar e 
de v elopment, which could pr o vide v aluable insight f or t he r eader .
V erbal F eedback
Scoring Decision
R esponse
A
Instruction
What is t he meaning and significance of 'Containerization' in softwar e de v elopment, and what r ole does Dock er pla y in it?Containerization in softwar e is similar t o putting it ems in 
a bo x. It in v olv es packaging y our softwar e and all it s 
component s int o a container . Dock er is a t ool t hat 
f acilitat es t his pr ocess. It helps t o encapsulat e t he 
softwar e int o containers and simplifies t heir use. [...]Containerization in softwar e de v elopment r ef ers t o t he 
pr ocess of packaging up an application along wit h all it s 
r elat ed configuration files, libraries, and dependencies 
r equir ed t o run, int o a standalone unit or a ‘container ’ . [...]R esponse
B
Ev aluation Crit eria
AFigure 2: Comparison of direct assessment and pairwise ranking. Both responses could be considered decent under
the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion.
The statistics of the resulting dataset are listed in
Table 1 along with the FEEDBACK COLLECTION .
We explain about our quality verification process
of the PREFERENCE COLLECTION in Appendix A.
Also, we include the prompts we use for the aug-
mentation process in Appendix H.
3.4 Training Methods & Baselines
Prompting Prompting involves querying an LM
to make judgments in a specified evaluation for-
mat without training. We employ Llama-2-Chat-
7,13,70B (Touvron et al., 2023); Mistral-7B-
Instruct-v0.2 (Jiang et al., 2023a); and Mixtral-
8x7B-Instruct-v0.1 (Jiang et al., 2024) as our base-
lines. It’s worth noting that models not explicitly
trained on feedback data often fail to generate re-
sponses in the required format, making it extremely
difficult to parse scoring decisions. Although it is
impractical for regular use, we make a fair compari-
son by infinitely looping until scores can be parsed.
Also, we include proprietary LMs such as GPT-3.5-
Turbo-0613; GPT-4-1106; and Claude-3-Opus.
Single-Format Training Single-Format training
involves training a base model θon either on a
direct assessment feedback dataset Ddor a pair-
wise ranking feedback dataset Dp. For single-
format trained evaluator LMs, we test Prometheus-
7,13B (Kim et al., 2023) (direct assessment);
UltraRM-13B (Cui et al., 2023) (pairwise rank-
ing); and PairRM-0.4B (Jiang et al., 2023c) (pair-wise ranking). In addition, we also report the per-
formances of single-format training Mistral-7B-
Instruct-v0.2 and Mixtral-8x7B-Instruct-v0.1 on
either direct assessment or pairwise ranking.
Joint Training Joint training involves training a
base model θon both a direct assessment feedback
dataset Ddand a pairwise ranking feedback dataset
Dp. This enables the resulting evaluator LM to
function across both evaluation formats. For jointly
trained evaluator LMs, we test Auto-J (Li et al.,
2023a). In addition, we report the performances
of jointly training Mistral-7B and Mixtral-8x7B on
both direct assessment and pairwise ranking.
Weight Merging Weight Merging involves train-
ing two models, θdandθp, separately on a direct
assessment feedback dataset Ddand a pairwise
ranking feedback dataset Dp. Then, the final eval-
uator LM θfinal is obtained by merging θdandθp.
For example, linear merging is as follows:
θfinal =α×θd+ (1−α)×θp (3)
In addition to linear merging, we test 5 additional
variants, namely Task Arithmetic merging (Ilharco
et al., 2022), TIES merging (Yadav et al., 2024),
DARE-TIES and DARE-Linear merging (Yu et al.,
2023), and SLERP merging (Goddard et al., 2024).
We include an explanation of these merging meth-
ods and ablation experiment results of the perfor-
mance differences in Appendix G. Among them,

--- PAGE 5 ---
Evaluation Method Benchmark Metrics Judgment Source Reference Answer # Score Rubrics # Instructions # Judgments
Direct AssessmentVicuna Bench Correlation Proprietary LMs Y 80 80 320
MT Bench Correlation Proprietary LMs Y 80 80 320
FLASK Correlation Proprietary LMs & Humans Y 12 200 2,000
Feedback Bench Correlation Proprietary LMs Y 200 200 1,000
Pairwise RankingHHH Align. Accuracy Humans N 4 221 221
MT Bench Human Judg. Accuracy Humans N 1 80 3,360
Auto-J Eval Accuracy Humans N 1 58 1,392
Preference Bench Accuracy Proprietary LMs Y 200 200 2,000
Table 2: Statistics of our evaluation benchmarks to assess the evaluation capabilities of evaluator LMs.
DARE-Linear showed the best performance, and
hence we used it to train the PROMETHEUS 2(7B
& 8x7B) models. Details on the hyper-parameters
for training and inference along with the prompt
templates are all listed in Appendix B, I, J.
4 Experimental Setup
The statistics of all the benchmarks are in Table 2.
The four direct assessment benchmarks are:
•Vicuna Bench (Chiang et al., 2023): A single-
turn chat benchmark that includes 80 test
prompts, 80 hand-crafted score rubrics from
Kim et al. (2023), and 320 responses obtained
by WizardLM-13B, Vicuna-13B, Llama-2-
Chat-13B, GPT-3.5-Turbo-0613.
•MT Bench (Zheng et al., 2023): A multi-
turn chat benchmark that consists of 80 test
prompts, 80 hand-crafted score rubrics from
Kim et al. (2023), and 320 responses obtained
by WizardLM-13B, Vicuna-13B, Llama-2-
Chat-13B, GPT-3.5-Turbo-0613.
•FLASK (Ye et al., 2023): A fine-grained
evaluation benchmark comprised of 200 test
prompts, 12 score rubrics, and 2000 responses
acquired from Alpaca-7B, Vicuna-13B, Bard,
GPT-3.5-Turbo-0613. In addition to scores
from proprietary LMs, this benchmark also
includes scores marked by human evaluators.
•Feedback Bench (Kim et al., 2023): The test
set of the FEEDBACK COLLECTION with 1K
score rubrics, 200 instructions, and 1K re-
sponses that do not overlap with the train data.
The four pairwise ranking benchmarks are:
•HHH Alignment (Askell et al., 2021): A
benchmark consisting of 221 prompts; 4 score
rubrics (helpfulness, harmlessness, honesty,
and other) and 221 response pairs (graded as
‘win’ or ‘lose’) judged by human evaluators.•MT Bench Human Judgment (Zheng et al.,
2023): A benchmark that shares the same 80
prompts as MT-Bench. In addition, it provides
3,360 response pairs (graded as ‘win’, ‘tie’, or
‘lose’) judged by human evaluators.
•Auto-J Eval (Li et al., 2023a): A benchmark
consisted of 58 prompts and 1,392 response
pairs (graded as ‘win’, ‘tie’, or ‘lose’) judged
by human evaluators. This benchmark is used
as the in-domain test set of Auto-J.
•Preference Bench : Our in-domain test set for
thePROMETHEUS models. Similar to how the
PREFERENCE COLLECTION was made with
theFEEDBACK COLLECTION , we adjust the
FEEDBACK BENCH and pair two out of the
five responses, resulting in a test set with 200
prompts, 2,000 response pairs (graded as ‘win’
or ‘lose’), and 200 evaluation criteria.
In direct assessment, we conduct reference-
based evaluations by appending the reference an-
swer as the input. We use Pearson ,Spearman , and
Kendall-Tau as performance metrics to measure
scoring correlations against reference evaluators.
Moreover, we include the results of the reference-
free direct assessment evaluation in Appendix F.
In pairwise ranking, we conduct reference-free
evaluations. Based on judgments assigned by hu-
mans, we use accuracy as our metric to measure
agreement between evaluator LMs and humans.
Also, the MT Bench Human Judgment and Auto-
J test set includes a ‘tie’ option assessed by human
evaluators. We evaluate in two ways: by excluding
all ‘tie’ options for pairwise ranking (denoted as
‘w/o tie ’), or by using direct assessment where re-
sponses scored as ‘ties’ are grouped, and pairwise
rankings are applied to the remaining responses
with differing scores (denoted as ‘ w/ tie ’).

--- PAGE 6 ---
Evaluator LMVICUNA BENCH MT B ENCH FLASK Feedback Bench
GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans GPT-4-0613
LLAMA 2-C HAT 7B 0.205 0.243 0.036 0.055 0.317 0.256 0.299 0.523
LLAMA 2-C HAT 13B 0.185 0.141 -0.042 -0.002 0.239 0.247 0.263 0.545
LLAMA 2-C HAT 70B 0.350 0.463 0.178 0.228 0.388 0.402 0.317 0.592
MISTRAL -INSTRUCT -7B 0.486 0.561 0.284 0.396 0.448 0.437 0.377 0.586
MIXTRAL -INSTRUCT -8X7B 0.566 0.579 0.551 0.539 0.483 0.495 0.420 0.673
PROMETHEUS -7B 0.484 0.528 0.378 0.382 0.352 0.331 0.348 0.847
PROMETHEUS -13B 0.492 0.534 0.404 0.477 0.462 0.470 0.449 0.860
AUTO-J (13B) 0.351 0.262 0.432 0.375 0.430 0.370 0.473 0.637
PROMETHEUS -2-7B 0.666 0.654 0.548 0.517 0.617 0.561 0.545 0.882
PROMETHEUS -2-8 X7B 0.685 0.635 0.665 0.614 0.659 0.626 0.555 0.898
GPT-3.5-T URBO -0613 0.335 0.349 0.183 0.194 0.437 0.396 0.450 0.594
GPT-4-1106 / 0.694 / 0.717 / 0.736 0.679 0.753
CLAUDE -3-O PUS 0.694 / 0.717 / 0.736 / 0.573 0.788
Table 3: Direct Assessment Results Pearson correlations between reference evaluators (listed on top) and evaluator LMs.
The best comparable statistics are bolded and second best underlined except proprietary LMs. Spearman and Kendall-Tau
correlations are reported in Appendix C. Note that the Feedback Bench is an in-domain test set of the P ROMETHEUS models.
Evaluator LMHHH A LIGNMENT MT B ENCH HUMAN JUDG . A UTO-J E VAL Preference Bench
Help. Harm. Hon. Other Total Avg. w/ TIE w/o TIE w/ TIE w/o TIE Instance-wise Criteria
LLAMA 2-C HAT 7B 55.93 62.07 49.18 62.79 57.01 46.68 50.39 45.76 45.73 58.60
LLAMA 2-C HAT 13B 71.19 77.59 60.66 62.79 68.33 51.22 49.61 47.84 43.28 63.00
LLAMA 2-C HAT 70B 62.71 81.03 65.57 65.12 68.78 55.14 60.88 53.38 50.64 64.70
MISTRAL -INSTRUCT -7B 59.32 68.97 63.93 81.40 67.42 53.81 63.82 53.88 60.94 79.40
MIXTRAL -INSTRUCT -8X7B 83.05 87.93 67.21 69.77 77.38 51.85 71.42 53.81 73.50 84.00
PAIRRM (0.4B) 84.75 84.48 80.33 90.70 84.62 - 59.00 - 59.05 81.80
ULTRA RM (13B) 86.44 79.31 81.97 88.37 83.71 - 56.00 - 59.85 86.97
AUTO-J (13B) 77.97 79.31 70.49 74.42 75.57 42.56 69.12 43.46 76.64 81.35
PROMETHEUS -2-7B 72.78 79.31 77.05 76.74 74.66 50.45 70.78 54.96 75.07 93.25
PROMETHEUS -2-8 X7B 84.75 96.55 81.97 76.74 85.52 55.07 71.96 58.41 79.98 90.65
GPT-3.5-T URBO -0613 77.97 81.03 77.05 67.44 76.47 54.65 69.41 45.98 72.13 75.05
GPT-4-1106-P REVIEW 89.83 96.55 91.80 83.72 90.95 60.38 79.90 52.80 83.12 85.50
CLAUDE -3-O PUS 91.53 100.00 91.80 95.35 94.57 55.35 77.65 60.70 82.92 89.85
Table 4: Pairwise Ranking Results Accuracy on human preference datasets. The best comparable accuracies are bolded and
second best underlined except proprietary LMs. Note that HHH Alignment is an in-domain test set for PairRM, Auto-J Eval is
an in-domain test set for Auto-J, and the Preference Bench is an in-domain test set for Prometheus-2 models.
5 Experimental Results
In this section, we compare the evaluation capabil-
ities of PROMETHEUS -2models with other base-
lines using a direct assessment format (Section 5.1)
and a pairwise ranking format (Section 5.2). Addi-
tionally, we measure the consistency of the scores
from evaluator LMs in Appendix E.
5.1 Direct Assessment Results
The direct assessment results are shown in Table 3.
The scoring decisions of PROMETHEUS 2models
(7B & 8x7B), GPT-4-1106, Claude-3-Opus, and
human evaluators all strongly correlate with each
other, yielding Pearson correlations higher than 0.5
regardless of the reference evaluator and bench-
mark. On the other hand, base LMs, single-format
trained LMs, and jointly trained LMs show lower
correlations, mostly falling below 0.5.Notably, PROMETHEUS 2models outperform
Prometheus and Auto-J by at least 0.2 units
across benchmarks in their correlation with pro-
prietary LMs. Moreover, on the FLASK bench-
mark, while the correlation between humans and
GPT-4 is 0.679, the highest correlation previously
achieved by Prometheus-13B with humans was
0.449. P ROMETHEUS -2-8 X7B achieves a correla-
tion of 0.555 with humans, halving the gap.
5.2 Pairwise Ranking Results
The pairwise ranking results are shown in Table 4.
We exclude the results of Pair RM and Ultra RM
on ‘w/ Tie’ settings since they could not process it.
On all of the 4 benchmarks, the PROMETHEUS
2models achieve the highest scores, showing that
they could effectively simulate human judgments.
Notably, while HHH Alignment is an in-domain
test set for Pair RM, and Auto-J Eval is for Auto-

--- PAGE 7 ---
J,PROMETHEUS -2-8 X7Bachieves higher scores.
This shows that training a large LM ( i.e., Mixtral-
8x7B) with feedback data could be an effective
strategy to obtain a robust evaluator LM that could
generalize beyond its training data. Moreover, the
PROMETHEUS 2models at least halve the perfor-
mance gap with proprietary LMs compared to ex-
isting evaluator LMs on out-of-domain test sets.
6 Analyses of Weight Merging
To understand the effectiveness of our proposed
weight merging method in the context of evalua-
tions, we address the following research questions:
•RQ1 : Is weight merging more effective com-
pared to joint training? (Section 6.1)
•RQ2 : Is the effectiveness of weight merging
due to model ensembling? (Section 6.2)
•RQ3 : To what extent does learning with di-
rect assessment help pairwise ranking perfor-
mance, and vice versa? (Section 6.3)
6.1 Weight Merging vs Joint Training
Table 5 compares the performance of evaluator
LMs trained via weight merging and joint training.
Alongside this, we also add and compare the results
of prompting and single-format training.
Surprisingly, evaluator LMs trained via joint
training often show lower performance compared
to those trained only in single-format, which indi-
cates negative task transfer . Specifically, evaluator
LMs trained only on direct assessment formats ob-
tain higher correlations compared to their jointly
trained counterparts across different model scales.
Similarly, evaluator LMs trained solely on pairwise
ranking formats achieve higher average accuracy
compared to those trained on multiple tasks, partic-
ularly when using Mixtral-8x7B as the base model.
On the other hand, evaluator LMs trained via
weight merging show superior performance not
only compared to jointly trained evaluator LMs
but also single-format trained evaluator LMs, in-
dicating positive task transfer . Also, while both
benefit each other, merging the pairwise ranking
evaluator LM weights improves direct assessment
performance more significantly than the reverse.
6.2 Is the Effectiveness of Weight Merging
due to Model Ensembling?
While we empirically find that weight merging is
effective, the underlying reason remains unclear. Anatural assumption is that this effectiveness results
from the ensembling effect of combining multiple
models. To test this hypothesis, we conduct an abla-
tion experiment where we train multiple evaluator
LMs on different random seeds and merge them.
Specifically, we merge two evaluator LMs trained
on direct assessment formats (denoted as ‘Direct
Assessment & Direct Assessment’) and two evalu-
ator LMs trained on pairwise ranking formats (de-
noted as ‘Pairwise Ranking & Pairwise Ranking’).
We use Mistral-7B-Instruct as our base model.
The results are presented in Table 6. Across mul-
tiple benchmarks, merging evaluator LMs trained
on the same evaluation format does not enhance
evaluation performance. Specifically, merging two
evaluator LMs trained on the same evaluation for-
mat—whether direct assessment or pairwise rank-
ing—negatively impacts performance on average
for both direct assessment and pairwise ranking
benchmarks. In contrast, merging two evaluator
LMs, each trained on direct assessment and pair-
wise ranking formats, results in superior perfor-
mance compared to the other settings. This in-
dicates that the beneficial task transfer in weight
merging arises from integrating different evaluation
formats, not ensembling multiple models.
6.3 Quantifying Positive Transfer across
Evaluation Formats
To explore how training on direct assessment feed-
back data influences pairwise ranking accuracy and
vice versa, we experiment by adjusting the αvalue
during linear merging. We evaluate the average
performance using all eight benchmarks in our ex-
periments. To illustrate the average performance
(colored in black), we adjust the scale by multiply-
ing the Pearson correlations from direct assessment,
which originally range from 0 to 1, by 100 before
averaging them with the pairwise ranking accuracy.
The results are shown in Figure 3. For direct
assessment benchmarks, evaluator LMs obtain the
optimal performance when αis set to 0.5. This
indirectly indicates that both pairwise ranking and
direct assessment feedback data contribute equally.
On the other hand, for pairwise ranking bench-
marks, the performance is optimal when αis set to
0.3. This also implies that while both benefit each
other, training on pairwise ranking improves direct
assessment performance more than the reverse.

--- PAGE 8 ---
Training MethodDIRECT ASSESSMENT BENCHMARKS PAIRWISE RANKING BENCHMARKS
Vicuna Ben. MT Ben. FLASK Average HHH Align. MT Ben. H.J. Auto-J Eval Average
Mistral-Instruct-7B
PROMPTING 0.486 0.284 0.480 0.417 67.42 63.82 60.94 64.06
DIRECT ASSESSMENT ONLY 0.537 0.561 0.519 0.539 73.33 56.76 64.38 64.82
PAIRWISE RANKING ONLY - - - - 78.73 67.06 72.03 72.61
JOINT TRAINING 0.548 0.450 0.457 0.485 80.09 65.49 73.60 73.06
WEIGHT MERGING 0.666 0.548 0.659 0.624 74.66 70.78 75.07 73.50
Mixtral-Instruct-8x7B
PROMPTING 0.566 0.551 0.507 0.541 77.38 71.42 73.55 74.56
DIRECT ASSESSMENT ONLY 0.625 0.664 0.587 0.625 74.21 53.14 65.85 64.40
PAIRWISE RANKING ONLY - - - - 84.16 66.27 75.66 75.36
JOINT TRAINING 0.628 0.560 0.596 0.595 82.35 68.73 74.78 75.29
WEIGHT MERGING 0.685 0.665 0.659 0.670 85.52 71.96 79.98 79.15
Table 5: Single-Format Training vs Joint Training vs Weight Merging Pearson correlations between evaluator LMs trained
with different methods and GPT-4-1106. Evaluator LMs trained with weight merging outperform single-format-trained and
jointly-trained evaluator LMs across multiple benchmarks.
Training Data Evaluation FormatDIRECT ASSESSMENT BENCHMARKS PAIRWISE RANKING BENCHMARKS
Vicuna Ben. MT Ben. FLASK Average HHH Align. MT Ben. H.J. Auto-J Eval Average
NOTRAINING (PROMPTING ) 0.486 0.284 0.480 0.417 67.42 63.82 60.94 64.06
DIRECT ASSESSMENT ONLY 0.537 0.561 0.519 0.539 73.33 56.76 64.38 64.82
PAIRWISE RANKING ONLY - - - - 78.73 67.06 72.03 72.61
DIRECT ASSESSMENT & D IRECT ASSESSMENT 0.552 0.493 0.505 0.517 73.30 55.00 63.69 64.13
PAIRWISE RANKING & P AIRWISE RANKING - - - - 78.70 65.20 72.72 72.21
DIRECT ASSESSMENT & P AIRWISE RANKING 0.666 0.548 0.659 0.624 74.66 70.78 75.07 73.50
Table 6: Unifying Formats vs Ensembling Pearson correlations with GPT-4-1106 (Vicuna Bench, MT Bench, FLASK) and
agreement with human evaluators (HHH Alignment, MT Bench Human Judgment, Auto-J Eval). Merging models trained with
the same evaluation formats (ensembling) underperforms merging models trained with different formats (unifying formats).
Dir ect Assessment Corr elationPair wise Ranking AccuracyA v erage P er f ormance(Dir ect Assessment : Pair wise Ranking) Mer ging RatioDir ect Assessment P earson Corr elation
Pair wise Ranking Agr eement Accuracy
Figure 3: When merging models, the influence of relative evaluation on absolute evaluation is greater than the
influence of absolute evaluation on relative evaluation. Performance of Direct Assessment (colored in green) and
Pairwise Ranking (colored in blue) when altering the αvalue to merge evaluator LMs trained on different formats.
7 Conclusion
We introduce PROMETHEUS 2, an open-source LM
specialized in evaluating other responses. Unlike
existing open evaluator LMs that cannot effectively
process both direct assessment and pairwise rank-ing—the two most prevalent evaluation schemes—
thePROMETHEUS 2models demonstrate superior
performance on both schemes, significantly narrow-
ing the gap with proprietary LM-based evaluations.
To train the PROMETHEUS 2models, we develop
thePREFERENCE COLLECTION , the first pairwise

--- PAGE 9 ---
ranking dataset that includes over 1,000 instance-
wise evaluation criteria beyond basic qualities such
as helpfulness and harmlessness. Notably, we find
that merging evaluator LMs trained on either direct
assessment or pairwise ranking formats can lead
to a unified evaluator LM with strong performance.
We hope that our work encourages more research
on using open-source LMs as evaluators.
Acknowledgements
We thank the KAIST AI LKLab members for help-
ful discussions. This work was partly supported
by LG AI Research grant (Self-improving logical
reasoning capabilities of LLMs, 2024, 50%) and
the Institute of Information & Communications
Technology Planning & Evaluation(IITP) grant
funded by the Korea government(MSIT) (RS-2024-
00397966, Development of a Cybersecurity Spe-
cialized RAG-based sLLM Model for Suppressing
Gen-AI Malfunctions and Construction of a Pub-
licly Demonstration Platform, 50%).
Limitations
Evaluation is fundamentally a very multi-faceted
task. In this paper, we used an indirect method to
assess the evaluation capability of evaluator LMs
by measuring if they perform evaluations similar to
human evaluators or proprietary LMs, such as GPT-
4-1106 and Claude-3-Opus. However, this may
not necessarily be the best approach. Future work
could explore meta-evaluation pipelines that reeval-
uate the results of evaluator LMs or methodologies
that allow humans to efficiently review evaluation
results. Also note that it is crucial to use model-
based evaluations in conjunction with human eval-
uation instead of solely relying on it.
Additionally, the degree to which evaluator LMs
can generalize was based on an analysis by Kim
et al. (2023), which checked for overlap between
the data used to train the evaluator LMs and the
data used to evaluate them. This study extended the
evaluation to eight different datasets with human
judgments to check the generalization capability
of evaluation under various circumstances. How-
ever, this may not be sufficient. One of the major
challenges in evaluating evaluator LMs is obtain-
ing the “evaluation results” ( e.g., human judgment).
Automating evaluations with LMs could greatly
benefit many areas of NLP research, hence the role
of future work in creating feedback benchmarks
that include human judgment or data for trainingevaluator LMs is crucial.
One downside of the PROMETHEUS 2is that it
operates only on a 1-5 point Likert scale for abso-
lute evaluation or a comparative evaluation style
of ‘A is better & B is better’. Depending on the
use case, people may need a 1-10 point absolute
evaluation, a ranking method for five responses at
once, or a checklist-based evaluation not covered in
the paper. While proprietary LMs can flexibly con-
duct evaluations in any format if a well-described
prompt is devised, open-source LMs cannot pro-
duce good evaluation results without training, and
conversely, if trained in one or two formats, they
lose the flexibility to conduct different evaluations.
Future work could examine whether evaluator LMs
trained in each format, as done in this paper, can
handle evaluations for added formats well when
weight merging is employed.
Lastly, the paper presents an evaluation model
that can handle both absolute and comparative
evaluation formats well through weight merging
based on empirical experiments. However, funda-
mentally explaining why weight merging works
well remains a challenging task. To address this,
Section 6 indirectly analyzes the effectiveness of
weight merging by comparing it with joint training,
demonstrating that the improvement in evaluation
performance is not due to model ensembling, and
showing that the impact of comparative evaluation
on absolute evaluation is greater than the reverse.
Our best current interpretation is that "absolute and
comparative evaluations are not completely differ-
ent tasks, so weight merging could handle both
without degeneration, and conversely, because they
are not too similar, weight merging performed bet-
ter than joint training." Future work could theoreti-
cally analyze this or further explore whether weight
merging can effectively work in fields other than
LLM evaluation.
References
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,
Deep Ganguli, Tom Henighan, Andy Jones, Nicholas
Joseph, Ben Mann, Nova DasSarma, Nelson El-
hage, Zac Hatfield-Dodds, Danny Hernandez, Jack-
son Kernion, Kamal Ndousse, Catherine Olsson,
Dario Amodei, Tom Brown, Jack Clark, Sam Mc-
Candlish, Chris Olah, and Jared Kaplan. 2021. A
general language assistant as a laboratory for align-
ment.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,

--- PAGE 10 ---
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,
Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan
Liu. 2023. Chateval: Towards better llm-based eval-
uators through multi-agent debate. arXiv preprint
arXiv:2308.07201 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. 2023. Ultrafeedback: Boosting lan-
guage models with high-quality feedback. arXiv
preprint arXiv:2310.01377 .
Shachar Don-Yehiya, Elad Venezian, Colin Raffel,
Noam Slonim, Yoav Katz, and Leshem Choshen.
2022. Cold fusion: Collaborative descent for
distributed multitask finetuning. arXiv preprint
arXiv:2212.01378 .
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. 2023. Al-
pacafarm: A simulation framework for methods
that learn from human feedback. arXiv preprint
arXiv:2305.14387 .
Markus Freitag, David Grangier, and Isaac Caswell.
2020. Bleu might be guilty but references are not
innocent. arXiv preprint arXiv:2004.06063 .
Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun
Wan. 2024. Llm-based nlg evaluation: Current status
and challenges. arXiv preprint arXiv:2402.01383 .
Sebastian Gehrmann, Tosin Adewumi, Karmanya Ag-
garwal, Pawan Sasanka Ammanamanchi, Aremu
Anuoluwapo, Antoine Bosselut, Khyathi Raghavi
Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D
Dhole, et al. 2021. The gem benchmark: Natural lan-
guage generation, its evaluation and metrics. arXiv
preprint arXiv:2102.01672 .
Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya
Mahendiran, Alex Wang, Alexandros Papangelis,
Aman Madaan, Angelina McMillan-Major, Anna
Shvets, Ashish Upadhyay, Bingsheng Yao, et al. 2022.
Gemv2: Multilingual nlg benchmarking in a single
line of code. arXiv preprint arXiv:2206.11249 .
Charles Goddard, Shamane Siriwardhana, Malikeh
Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian
Benedict, Mark McQuade, and Jacob Solawetz. 2024.
Arcee’s mergekit: A toolkit for merging large lan-
guage models. arXiv preprint arXiv:2403.13257 .Suchin Gururangan, Margaret Li, Mike Lewis, Wei-
jia Shi, Tim Althoff, Noah A Smith, and Luke
Zettlemoyer. 2023. Scaling expert language models
with unsupervised domain discovery. arXiv preprint
arXiv:2303.14177 .
Michael Hanna and Ond ˇrej Bojar. 2021. A fine-grained
analysis of bertscore. In Proceedings of the Sixth
Conference on Machine Translation , pages 507–517.
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-
man, Suchin Gururangan, Ludwig Schmidt, Han-
naneh Hajishirzi, and Ali Farhadi. 2022. Edit-
ing models with task arithmetic. arXiv preprint
arXiv:2212.04089 .
Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong
Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh
Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu.
2023a. Personalized soups: Personalized large lan-
guage model alignment via post-hoc parameter merg-
ing.arXiv preprint arXiv:2310.11564 .
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung
Kim, Lajanugen Logeswaran, Moontae Lee, Kyung-
jae Lee, and Minjoon Seo. 2023b. Exploring the
benefits of training expert language models over in-
struction tuning. arXiv preprint arXiv:2302.03202 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023a. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao
Huang, Bill Yuchen Lin, and Wenhu Chen. 2023b.
Tigerscore: Towards building explainable met-
ric for all text generation tasks. arXiv preprint
arXiv:2310.00752 .
Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023c.
Llm-blender: Ensembling large language models
with pairwise ranking and generative fusion. arXiv
preprint arXiv:2306.02561 .
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al.
2023. Prometheus: Inducing fine-grained evalua-
tion capability in language models. arXiv preprint
arXiv:2310.08491 .
Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne
Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin
Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et al.
2024. The biggen bench: A principled benchmark
for fine-grained evaluation of language models with
language models. arXiv preprint arXiv:2406.05761 .

--- PAGE 11 ---
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
et al. 2024. Rewardbench: Evaluating reward
models for language modeling. arXiv preprint
arXiv:2403.13787 .
Seongyun Lee, Seungone Kim, Sue Hyun Park,
Geewook Kim, and Minjoon Seo. 2024. Prometheus-
vision: Vision-language model as a judge
for fine-grained evaluation. arXiv preprint
arXiv:2401.06591 .
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,
Hai Zhao, and Pengfei Liu. 2023a. Generative
judge for evaluating alignment. arXiv preprint
arXiv:2310.05470 .
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike
Lewis, Tim Althoff, Noah A Smith, and Luke Zettle-
moyer. 2022. Branch-train-merge: Embarrassingly
parallel training of expert language models. arXiv
preprint arXiv:2208.03306 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan
Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023b. Al-
pacaeval: An automatic evaluator of instruction-
following models. https://github.com/
tatsu-lab/alpaca_eval .
Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen
Gu, and Chongyang Tao. 2024. Leveraging large
language models for nlg evaluation: A survey. arXiv
preprint arXiv:2401.07103 .
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023a. G-eval:
Nlg evaluation using gpt-4 with better human align-
ment.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023b. Gpte-
val: Nlg evaluation using gpt-4 with better human
alignment. arXiv preprint arXiv:2303.16634 .
Michael S Matena and Colin A Raffel. 2022. Merging
models with fisher-weighted averaging. Advances in
Neural Information Processing Systems , 35:17703–
17716.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Alexandre Rame, Guillaume Couairon, Corentin
Dancette, Jean-Baptiste Gaya, Mustafa Shukor,
Laure Soulier, and Matthieu Cord. 2024. Rewardedsoups: towards pareto-optimal alignment by inter-
polating weights fine-tuned on diverse rewards. Ad-
vances in Neural Information Processing Systems ,
36.
Natalie Schluter. 2017. The limits of automatic sum-
marisation according to rouge. In Proceedings of the
15th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics , pages 41–45.
Association for Computational Linguistics.
Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma,
Hu Xu, Xi Victoria Lin, Baptiste Rozière, Ja-
cob Kahn, Daniel Li, Wen-tau Yih, Jason We-
ston, et al. 2024. Branch-train-mix: Mixing expert
llms into a mixture-of-experts llm. arXiv preprint
arXiv:2403.07816 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang,
Shizhe Diao, Shuang Qiu, Han Zhao, and Tong
Zhang. 2024. Arithmetic control of llms for di-
verse user preferences: Directional preference align-
ment with multi-objective rewards. arXiv preprint
arXiv:2402.18571 .
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai
Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui.
2023a. Math-shepherd: A label-free step-by-step
verifier for llms in mathematical reasoning. arXiv
preprint arXiv:2312.08935 .
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi
Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
Rui Xie, Jindong Wang, Xing Xie, et al. 2023b.
Pandalm: An automatic evaluation benchmark for
llm instruction tuning optimization. arXiv preprint
arXiv:2306.05087 .
Prateek Yadav, Derek Tam, Leshem Choshen, Colin A
Raffel, and Mohit Bansal. 2024. Ties-merging: Re-
solving interference when merging models. Ad-

--- PAGE 12 ---
vances in Neural Information Processing Systems ,
36.
Seonghyeon Ye, Doyoung Kim, Sungdong Kim,
Hyeonbin Hwang, Seungone Kim, Yongrae Jo,
James Thorne, Juho Kim, and Minjoon Seo. 2023.
Flask: Fine-grained language model evaluation
based on alignment skill sets. arXiv preprint
arXiv:2307.10928 .
Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin
Li. 2023. Language models are super mario: Absorb-
ing abilities from homologous models as a free lunch.
arXiv preprint arXiv:2311.03099 .
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685 .
Lianghui Zhu, Xinggang Wang, and Xinlong Wang.
2023. Judgelm: Fine-tuned large language
models are scalable judges. arXiv preprint
arXiv:2310.17631 .

--- PAGE 13 ---
Verification Standards R ESULTS
Coherence 99.5 % (Passed)
Suitability 98.5 % (Passed)
Criticality 88% (Win rate)
Table 7: Human verification results to assess the quality of
thePREFERENCE COLLECTION . We use three standards to
assess the quality of verbal feedback vrm,rn.
Temperature 1.0
Top_p 0.9
Max New Tokens 1024
Repetition Penalty 1.03
Table 8: Hyperparameters used to inference different evalua-
tor LM baselines.
Base Model mistralai/Mistral-7B-Instruct-v0.2
Torch dtype bfloat16
Epoch 1
Train Data 1 FEEDBACK COLLECTION
Train Data 2 PREFERENCE COLLECTION
Max Seq Length 4096
Learning Rate 1e-5
Train Batch Size 4
Random Seed 42
Merging Strategy Linear ( α= 0.5)
Training Method Supervised Fine-tuning
Table 9: Hyperparameters used to train PROMETHEUS 27B.
Base Model mistralai/Mixtral-8x7B-Instruct-v0.1
Torch dtype bfloat16
Epoch 1
Train Data 1 FEEDBACK COLLECTION
Train Data 2 PREFERENCE COLLECTION
Max Seq Length 4096
Learning Rate 1e-5
Train Batch Size 8
PEFT True
Lora_r 256
Lora_alpha 512
Lora_Dropout 0.1
Lora Target Module Q proj,K proj,V proj,O proj,W proj,LM_Head
Random Seed 42
Merging Strategy DARE Merging
Merging p 0.1
Merging Lambda 1.95
Training Method Supervised Fine-tuning
Table 10: Hyperparameters used to train PROMETHEUS 2
8x7B.
A Quality Verification of the
PREFERENCE COLLECTION
To ensure the quality of the PREFERENCE COL-
LECTION , particularly the generated verbal feed-
backvrm,rn, we employ five annotators with back-
grounds in natural language processing. The an-
notation study was designed and administered in
accordance with [Affiliation X]’s ethical guidelines.
Crowd workers were informed of the potential risksof participation and researcher contact information
before hand in the study consent form. The hourly
wage and expected study time were informed in the
Prolific platform. We compensated workers 9 GBP
per hour. 3 were from USA and 2 were from Asian
demographics.
We randomly sample 200 instances with differ-
ent instructions and conduct a three-part verifica-
tion process. First, we assess the coherence of
vrm,rnwith the scoring decision ( i.e., ’A is better’
or ’B is better’). Second, we evaluate the suit-
ability ofvrm,rnagainst the evaluation criteria e.
Lastly, to determine the criticality of the feedback,
we compare the newly generated vrm,rnwith a con-
catenation of vrmandvrn. This aims to determine
ifvrm,rneffectively leverages the mutual informa-
tion between rmandrn. Annotators then vote on
whether vrm,rnor the concatenation of rmandrn
is more critical. The results are shown in Table 7.
Note that the Preference Collection only includes
English instances.
B Training and Inference Details
The configurations we used for prompting and train-
ing evaluator LMs are shown in Table 8, 9, 10.
For Auto-J, PairRM and UltraRM, we utilize their
prompt template, inference hyperparameter men-
tioned in the model cards or github repositories in
order to ensure the configuration is optimal for a
fair performance comparison. For proprietary LMs,
PROMETHEUS 1, and PROMETHEUS 2models, we
use the same prompt template and evaluation con-
figurations.
For both training and inference, we utilized eight
40GB NVIDIA A100 GPUs. Training required ap-
proximately 800 GPU hours, using the implemen-
tation from the Alignment Handbook repository2.
For inference, we used the vllm framework3.
The results from Direct Assessment are aver-
aged after three multiple runs, and pairwise grad-
ing is conducted in a single run. Instead of using
error bars, we report the consistency in assessment
formats, Krippendorff’s alpha for consistency in
direct assessment, and transitivity statistics for con-
sistency in pairwise ranking.
C Direct Assessment Results: Extended
Table 11 and 12 (on the next page) shows the ex-
tended results Table 3. Even when changing the
2https://github.com/huggingface/alignment-handbook
3https://github.com/vllm-project/vllm

--- PAGE 14 ---
metrics to either Kendall-Tau and Spearman, the
overall trends are maintained. PROMETHEUS 2
shows superior evaluation performances among the
open evaluator LMs, achieving high correlations
with humans and proprietary LMs.
D License
Our models are released under the Apache 2.0 li-
cense. The Preference Collection dataset is sub-
ject to OpenAI’s Terms of Use for generated data.
The model could be used for commercial purposes
while the dataset is intended for research purposes.
We used perspective API to ensure that the train-
ing data or evaluation datasets do not include PII-
included instances.

--- PAGE 15 ---
Evaluator LMVICUNA BENCH MT B ENCH FLASK Feedback Bench
GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans GPT-4-0613
LLAMA 2-C HAT 7B 0.183 0.203 0.065 0.070 0.229 0.186 0.211 0.419
LLAMA 2-C HAT 13B 0.145 0.146 -0.019 0.037 0.160 0.174 0.174 0.453
LLAMA 2-C HAT 70B 0.282 0.382 0.150 0.196 0.310 0.310 0.231 0.487
MISTRAL -INSTRUCT -7B 0.314 0.391 0.208 0.281 0.395 0.384 0.287 0.454
MIXTRAL -INSTRUCT -8X7B 0.395 0.468 0.433 0.419 0.410 0.408 0.304 0.551
PROMETHEUS -7B 0.405 0.425 0.290 0.263 0.282 0.251 0.236 0.770
PROMETHEUS -13B 0.397 0.434 0.299 0.352 0.365 0.352 0.299 0.793
AUTO-J (13B) 0.282 0.242 0.303 0.272 0.312 0.282 0.312 0.515
PROMETHEUS -2-7B 0.543 0.476 0.390 0.372 0.476 0.446 0.377 0.784
PROMETHEUS -2-8 X7B 0.559 0.515 0.535 0.483 0.526 0.507 0.388 0.800
GPT-3.5-T URBO -0613 0.255 0.287 0.148 0.157 0.360 0.315 0.298 0.489
GPT-4-1106 / 0.553 / 0.590 / 0.609 0.517 0.662
CLAUDE -3-O PUS 0.553 / 0.590 / 0.609 / 0.453 0.693
Table 11: Kendall-Tau correlations between reference evaluators (listed on top) and evaluator LMs. The best comparable
statistics are bolded and second best underlined except proprietary LMs.
Evaluator LMVICUNA BENCH MT B ENCH FLASK Feedback Bench
GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans GPT-4-0613
LLAMA 2-C HAT 7B 0.236 0.255 0.084 0.089 0.301 0.244 0.279 0.511
LLAMA 2-C HAT 13B 0.178 0.179 -0.025 0.044 0.206 0.222 0.224 0.543
LLAMA 2-C HAT 70B 0.348 0.466 0.197 0.252 0.391 0.389 0.298 0.585
MISTRAL -INSTRUCT -7B 0.389 0.480 0.266 0.358 0.499 0.478 0.374 0.563
MIXTRAL -INSTRUCT -8X7B 0.476 0.556 0.545 0.517 0.505 0.500 0.386 0.659
PROMETHEUS -7B 0.508 0.528 0.385 0.349 0.367 0.326 0.317 0.876
PROMETHEUS -13B 0.492 0.534 0.401 0.470 0.474 0.454 0.398 0.893
AUTO-J (13B) 0.337 0.297 0.408 0.365 0.402 0.358 0.408 0.623
PROMETHEUS -2-7B 0.664 0.591 0.509 0.482 0.597 0.555 0.491 0.885
PROMETHEUS -2-8 X7B 0.660 0.615 0.669 0.605 0.642 0.618 0.496 0.912
GPT-3.5-T URBO -0613 0.319 0.354 0.192 0.198 0.446 0.390 0.374 0.565
GPT-4-1106 / 0.659 / 0.721 / 0.729 0.650 0.753
CLAUDE -3-O PUS 0.659 / 0.721 / 0.729 / 0.567 0.784
Table 12: Spearman correlations between reference evaluators (listed on top) and evaluator LMs. The best comparable statistics
arebolded and second best underlined except proprietary LMs.
Evaluator LMHHH A LIGNMENT MT B ENCH HUMAN JUDG . A UTO-J E VAL
Direct2Pair( ↑) Pair2Pair( ↑)∆(↓) Direct2Pair( ↑) Pair2Pair( ↑)∆(↓) Direct2Pair( ↑) Pair2Pair( ↑)∆(↓)
AUTO-J (13B) 46.61 75.57 28.96 48.14 69.12 20.98 47.40 76.64 29.24
PROMETHEUS -2-7B 74.21 74.66 0.45 63.24 70.78 7.54 68.11 75.07 6.96
PROMETHEUS -2-8 X7B 81.45 85.52 4.07 61.67 71.96 10.29 66.54 79.98 13.44
GPT-4-1106-P REVIEW 83.71 90.95 7.24 68.04 79.90 11.86 54.27 83.12 28.85
CLAUDE -3-O PUS 84.62 94.57 9.95 62.65 77.65 15.00 61.04 82.90 21.86
Table 13: Consistency across Evaluation Formats Pairwise ranking accuracy when assessing in direct assessment formats
(denoted as ‘Direct2Pair’) and pairwise ranking formats (denoted as ‘Pair2Pair’). Smaller ∆values indicate that evaluator LMs
can robustly evaluate across the two different formats.

--- PAGE 16 ---
Evaluator LM Vicuna Ben. MT Ben. FLASK
LLAMA 2-C HAT 7B 0.3558 0.2565 0.4379
LLAMA 2-C HAT 13B 0.2017 0.2998 0.4038
LLAMA 2-C HAT 70B 0.5212 0.4559 0.6204
MISTRAL -INSTRUCT -7B 0.5157 0.4393 0.5884
MIXTRAL -INSTRUCT -8X7B 0.5459 0.6229 0.6976
PROMETHEUS -7B 0.6049 0.5363 0.5970
PROMETHEUS -13B 0.5734 0.5181 0.5624
AUTO-J (13B) 0.4976 0.5069 0.6160
PROMETHEUS -2-7B 0.6018 0.5340 0.5991
PROMETHEUS -2-8 X7B 0.6383 0.6862 0.7874
GPT-3.5-T URBO -0613 0.7108 0.4800 0.6389
GPT-4-1106- PREVIEW 0.7366 0.8271 0.8355
CLAUDE -3-O PUS 0.8284 0.8601 0.8976
Table 14: Krippendorff’s alpha statistics for evaluator LMs
when prompted 3 times via non-deterministic decoding.
Evaluator LMPREFERENCE COLLECTION
Transitivity
MISTRAL -INSTRUCT -7B 87.10
MIXTRAL -INSTRUCT -8X7B 90.45
PAIRRM 91.40
ULTRA RM 94.25
AUTO-J (13B) 89.65
PROMETHEUS -2-7B 97.60
PROMETHEUS -2-8 X7B 96.75
GPT-3.5-T URBO -0613 84.35
GPT-4-1106- PREVIEW 95.70
CLAUDE -3-O PUS 96.20
Table 15: Transitivity statistics to measure consistency in
pairwise ranking evaluation settings.
E Consistency of Evaluator LMs
In addition to obtaining high correlation and accu-
racy, achieving high consistency is another impor-
tant aspect for evaluator LMs. We first test if evalu-
ator LMs could give consistent scoring decisions in
direct assessment formats. We inferencing multiple
times with non-deterministic decoding ( e.g., using
temperature 1.0). Following the experimental de-
sign from Ye et al. (2023), we choose to inference
3 times and report the Krippendorff’s alpha value.
As shown in Table 14, the results indicate that train-
ing on feedback data only slightly improves consis-
tency. On the other hand, we find that the LMs with
a large number of parameters achieve high consis-
tency. This indicates the importance of selecting a
large LM as the base model when training an evalu-
ator LM. Notably, PROMETHEUS -2-8 X7Bobtains
the highest correlation among open evaluator LMs.
Moreover, to evaluate consistency in pairwise
ranking settings (Table 15), we measure transitivity
(i.e., a higher score for response B over A, and
for C over B, results in a higher score for C over
A). As shown in Table 15, the PROMETHEUS 2models achieve performances on par with GPT-4,
showing that they could provide robust judgments
in pairwise ranking schemes.
Lastly, we conduct an experiment to test if eval-
uator LMs could achieve consistent scores across
different evaluation formats. To do this, we use
pairwise ranking benchmarks and measure the per-
formance differences when prompted with direct
assessment formats and pairwise ranking formats.
Specifically, following Kim et al. (2023), to pro-
cess pairwise ranking datasets in a direct assess-
ment scheme, we evaluate each response separately
and compare the scoring decisions. We mark it as
correct if the evaluator LM provides a higher score
for the human-chosen response over the rejected
one. As shown in Table 13 (on the previous page),
the results show that PROMETHEUS 2models show
lower performance differences across evaluation
formats, indicating their robustness.

--- PAGE 17 ---
Evaluator LMBIGG ENBENCH FLASK
Reference-free Reference-based ∆ Reference-free Reference-based ∆
MISTRAL -INSTRUCT 0.305 0.310 0.005 0.331 0.374 0.043
MIXTRAL -INSTRUCT 0.320 0.322 0.002 0.377 0.386 0.009
PROMETHEUS -2-7B 0.403 0.455 0.052 0.425 0.545 0.120
PROMETHEUS -2-8 X7B 0.424 0.472 0.048 0.411 0.555 0.144
GPT-3.5-T URBO -0613 0.236 0.252 0.016 0.354 0.374 0.020
GPT-4-1106 0.554 0.599 0.045 0.616 0.679 0.063
Table 16: Pearson correlations between different evaluator models with and without the reference answer and Human. Reference-
based evaluations outperform reference-free evaluations across all evaluator LMs.
Merging MethodDIRECT ASSESSMENT BENCHMARKS PAIRWISE RANKING BENCHMARKSAverage
VICUNA BEN. MT B EN. FLASK (H UMAN ) Feedback Ben. Average HHH A LIGN . MT B EN. H.J. A UTO-J Pref. Ben. Average
LINEAR 0.642 0.543 0.544 0.878 0.652 78.73 67.25 73.80 92.45 78.06 82.93
SLERP 0.648 0.532 0.536 0.879 0.649 74.66 70.2 72.33 92.60 77.44 82.67
TASK ARITHMETIC 0.518 0.497 0.482 0.831 0.582 80.09 69.80 72.82 93.00 78.93 81.01
TIES 0.534 0.567 0.529 0.826 0.614 79.64 67.75 72.91 93.95 78.56 80.58
DARE-TIES 0.653 0.545 0.543 0.880 0.655 79.64 66.57 74.68 93.30 78.55 83.27
DARE-LINEAR 0.666 0.548 0.545 0.882 0.660 74.66 70.78 75.07 93.25 78.44 83.32
Table 17: Pearson correlations and accuracy measurements across various benchmarks for different merging methods. The best
comparable statistics are bolded and second best underlined .

--- PAGE 18 ---
F Reference-free Evaluation in Direct
Assessment Formats
In this section, we assess the impact of excluding a
reference answer in evaluations conducted using di-
rect assessment formats. The results are presented
in Table 16 (on the previous page). For this experi-
ment, we employ FLASK (Ye et al., 2023) which
includes human judgments and additionally the
BiGGen Bench (Kim et al., 2024). The BiGGen
Bench is a generation benchmark which includes
a evaluation criteria tailored to each instance and
provides 2840 human judgments (excluding the
multilingual tasks) in direct assessment formats.
Across both benchmarks and different evalua-
tor LM variants, the correlation with humans di-
minishes when the reference answer is discarded.
Even for GPT-4-1106, there is a significant perfor-
mance degradation (0.045, 0.063). This suggests
that including a reference answer is crucial for con-
ducting effective evaluations with LMs. Interest-
ingly, P ROMETHEUS -2-7B achieves better perfor-
mance in a reference-free setting (0.403, 0.425)
than Mistral-7B-Instruct-v0.2 (0.310, 0.374). Sim-
ilar trends are observed for PROMETHEUS -2-
8X7B(0.424, 0.411) and Mixtral-8x7B-Instruct-
v0.1 (0.322, 0.386). This implies that one effect
of training an evaluator LM with a reference an-
swer included is to induce the ability to ground
judgments to the given reference answer.
G Merging Method Ablation
In this section, in addition to linear merging , we
also test different merging techniques including:
•Slerp merging (Goddard et al., 2024) oper-
ates by interpolating two weights θdandθp
while preserving the geometric properties of
the spherical space in which θdandθpreside.
Specifically, this is conducted by normalizing
θdandθpinto unit length and then merging
the two weights based on the coefficient α
such as:
θfinal =α×θd
||θd||+ (1−α)×θp
||θp||(4)
•Task Arithmetic merging (Ilharco et al.,
2022) which can be expressed as follows:
θfinal =θinit+α×(θd−θinit)+
(1−α)×(θp−θinit)(5)where θinitis the weight of the base model.
However, we empirically find that the result-
ing evaluator LM θfinal often does not gener-
ate valid scoring decisions ( e.g., generating an
integer during pairwise ranking).
•TIES merging (Yadav et al., 2024), while
similar to Task Arithmetic merging, adds (1) a
Trim operation to remove redundant weights
in the task vector θd−θinitandθp−θinit
and (2) Elect andDisjoint operations to
resolve disagreement ( i.e., opposite directed
weights) between θd−θinitandθp−θinit.
•DARE merging (Yu et al., 2023), while also
similar to Task Arithmetic and TIES merging,
performs a Random Drop andRe-scale
operations in the task vector θd−θinitand
θp−θinitto remove redundant weights. We
find that DARE merging work best when
we choose Mixtral-8x7B as our base model.
DARE-linear merging is what was originally
proposed by Yu et al. (2023). In DARE-TIES
merging , theElect operation from Yadav
et al. (2024) is additionally added after the
Re-scale operation.
We conduct our experiments based on the imple-
mentation from MergeKit (Goddard et al., 2024).4
In Table 17 (on the previous page), we mea-
sure the performance of evaluator LMs employing
different merging methods. In direct assessment
benchmarks, DARE-Linear achieves the best per-
formance, followed by DARE-TIES and Linear
merging. In pairwise ranking benchmarks, Task
Arithmetics achieves the best performance, with
only a minimal difference compared to other meth-
ods. On average, DARE-Linear performs best.
Based on these results, we have trained Prometheus-
2-7B with DARE-Linear merging. We also opted
to train Prometheus-2-8x7B using DARE-Linear
merging. Although the optimal merging method
might differ, we have not conducted additional ex-
periments due to computational limitations. Future
work could explore whether these findings hold
true.
4https://github.com/arcee-ai/mergekit

--- PAGE 19 ---
H P REFERENCE COLLECTION
Augmentation Prompt
Prompt for Generating Verbal Feedback
in Pairwise Ranking
###Task Description:
An instruction (might include an Input in-
side it), two responses to evaluate (denoted
as Response A and Response B), a refer-
ence answer, and a score rubric representing
a evaluation criteria are given.
1. Write a detailed feedback explaining why
{sub_str}, focusing strictly on the aspects
highlighted in the evaluation criteria.
2. While writing the feedback, make com-
parisons between Response A, Response B,
and Reference Answer. Instead of examin-
ing Response A and Response B separately,
go straight to the point and mention about
the commonalities and differences between
them.
3. While writing the feedback, do not start
by mentioning {sub_str} in the first sen-
tence. Instead, try to write a reasoning pro-
cess that delves into the commonalities and
differences of the two responses and men-
tion {sub_str} at the last part of your justifi-
cation.
4. Within the feedback, do not explicitly
mention about the reference answer. For in-
stance, do not use phrases like "Compared
to the reference answer". Assume that you
inherently know the reference answer which
could be used to determine details that are
not present in both responses under assess-
ment.
5. Please do not generate any other opening,
closing, and explanations. Just write the
feedback.
6. Within the feedback, generate a string
phrase "[END]" after you are finished.
###Instruction: {instruction}
###Response A: {response_A}
###Response B: {response_B}
###Reference Answer: {reference_answer}
###Score Rubric: {criteria}
###Feedback:I Direct Assessment Prompt
Direct Assessment System Prompt
You are a fair judge assistant tasked with
providing clear, objective feedback based
on specific criteria, ensuring each assess-
ment reflects the absolute standards set for
performance.
Direct Assessment Prompt Template
###Task Description:
An instruction (might include an Input in-
side it), a response to evaluate, and a score
rubric representing a evaluation criteria are
given.
1. Write a detailed feedback that assess the
quality of the response strictly based on the
given score rubric, not evaluating in general.
2. After writing a feedback, write a score
that is an integer between 1 and 5. You
should refer to the score rubric.
3. The output format should look as follows:
"Feedback: (write a feedback for criteria)
[RESULT] (an integer number between 1
and 5)"
4. Please do not generate any other opening,
closing, and explanations.
###The instruction to evaluate:
{orig_instruction}
###Response to evaluate:
{orig_response}
###Score Rubrics:
{score_rubric}
###Feedback:
J Pairwise Ranking Prompt
Pairwise Ranking System Prompt
You are a fair judge assistant assigned to de-
liver insightful feedback that compares indi-
vidual performances, highlighting how each
stands relative to others within the same co-
hort.

--- PAGE 20 ---
Pairwise Ranking Prompt Template
###Task Description:
An instruction (might include an Input in-
side it), a response to evaluate, and a score
rubric representing a evaluation criteria are
given.
1. Write a detailed feedback that assess
the quality of two responses strictly based
on the given score rubric, not evaluating in
general.
2. After writing a feedback, choose a bet-
ter response between Response A and Re-
sponse B. You should refer to the score
rubric.
3. The output format should look as follows:
"Feedback: (write a feedback for criteria)
[RESULT] (A or B)"
4. Please do not generate any other opening,
closing, and explanations.
###Instruction:
{orig_instruction}
###Response A:
{response_A}
###Response B:
{response_B}
###Score Rubric:
{score_rubric}
###Feedback:
