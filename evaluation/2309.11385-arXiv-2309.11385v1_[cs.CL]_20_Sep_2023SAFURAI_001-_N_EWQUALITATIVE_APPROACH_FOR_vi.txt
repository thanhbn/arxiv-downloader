# 2309.11385.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/evaluation/2309.11385.pdf
# Kích thước file: 387740 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
arXiv:2309.11385v1 [cs.CL] 20 Sep 2023 SAFURAI 001: PHƯƠNG PHÁP ĐỊNH TÍNH MỚI ĐỂ
ĐÁNH GIÁ LLM MÃ HÓA
Cifarelli D., Boiardi L. & Puppo A.∗
Nhóm Safurai
Genoa
Corso Firenze 39/3, 16136, Italy
info@safurai.com
TÓM TẮT
Bài báo này giới thiệu Safurai-001, một Mô hình Ngôn ngữ Lớn (LLM) mới với tiềm năng đáng kể trong lĩnh vực hỗ trợ lập trình. Được thúc đẩy bởi những tiến bộ gần đây trong các LLM lập trình, Safurai-001 cạnh tranh về hiệu suất với những mô hình mới nhất như WizardCoder [Xu et al. (2023)], PanguCoder [Shen et al. (2023)] và Phi-1 [Gunasekar et al. (2023)] nhưng hướng đến việc mang lại sự tương tác "đối thoại" hơn. Bằng cách tận dụng tiến bộ trong kỹ thuật dữ liệu (các kỹ thuật biến đổi dữ liệu và kỹ thuật prompt mới nhất) và điều chỉnh chỉ dẫn, mô hình mới này hứa hẹn có thể đứng ngang hàng với các phát triển mã nguồn đóng và mở gần đây. Nhận ra nhu cầu về một thước đo đánh giá hiệu quả cho các LLM lập trình, bài báo này cũng giới thiệu MultiParameters dựa trên GPT4: một benchmark đánh giá khai thác các tham số đa dạng để đưa ra cái nhìn toàn diện về hoạt động và hiệu suất của mô hình. Đánh giá của chúng tôi cho thấy Safurai-001 có thể vượt trội hơn GPT-3.5 1,58% và WizardCoder 18,78% trong tham số Khả năng Đọc Mã và nhiều hơn nữa.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn cho mã lệnh là một trong những ứng dụng đầy hứa hẹn nhất của LLM và chúng đã thu hút rất nhiều sự quan tâm từ cả học thuật và công nghiệp vì khả năng phi thường của chúng đối với các tác vụ liên quan đến mã lệnh.

Bối cảnh các mô hình mã nguồn đóng được thống trị bởi các mô hình OpenAI: GPT-3.5 và GPT-4 [OpenAI (2023)] (thực tế là mô hình được xếp hạng tốt nhất trong bảng xếp hạng HumanEval pass@1). Trước khi phát hành Starcoder [Li et al. (2023)], thế giới mã nguồn mở đã tụt hậu xa so với các mô hình thương mại về kích thước mô hình, khả năng và hiệu suất.

Tuy nhiên, mô hình này bắt đầu thay đổi với sự ra đời của Starcoder. Nó đã được sử dụng thường xuyên như một mô hình nền tảng trong việc phát triển các mô hình khác với kết quả tuyệt vời như WizardCoder [Xu et al. (2023)] và PanguCoder [Shen et al. (2023)], giảm đáng kể khoảng cách hiệu suất giữa các LLM lập trình mã nguồn mở và đóng. Gần đây, Meta cũng đã giới thiệu một bộ 12 LLM mới có sẵn để sử dụng thương mại: phiên bản LLAMA2 [Touvron et al. (2023)]. Các nhóm từ khắp nơi trên thế giới có thể sử dụng LLAMA2 làm mô hình nền tảng mới cho LLM Lập trình, cạnh tranh với StarCoder.

Trong các ấn phẩm mới nhất trong lĩnh vực LLM Lập trình, nhiều nỗ lực đã được thực hiện liên quan đến kỹ thuật dữ liệu (Phi-1) và điều chỉnh chỉ dẫn (WizardCoder).

Chúng tôi đã cố gắng tận dụng tất cả các đổi mới mới nhất trong lĩnh vực LLM Lập trình để phát triển một mô hình hiệu suất cao phù hợp với các bản phát hành mã nguồn mở mới nhất.

Tóm lại, chúng tôi đưa ra những đóng góp sau:

--- TRANG 2 ---
• Chúng tôi giới thiệu Safurai-001, một mô hình cạnh tranh với WizardCoder về hiệu suất và cố gắng có cách tiếp cận "đối thoại" hơn.
• Chúng tôi giới thiệu một benchmark đánh giá mới cho các LLM lập trình, Benchmark Đánh giá MultiParameters dựa trên GPT4. Benchmark này bao gồm nhiều tham số quan trọng để cung cấp cái nhìn sâu sắc hơn về hiệu suất của mô hình.

2 CÔNG TRÌNH LIÊN QUAN

2.1 MÔ HÌNH NGÔN NGỮ LỚN CHO LẬP TRÌNH

Mô hình Codex ấn tượng, với 12 tỷ tham số, minh họa khả năng đáng chú ý để giải quyết khoảng 72% các thách thức lập trình Python. Thành tựu này đã mở đường cho việc phát triển các mô hình tạo mã tiên tiến khác, bao gồm AlphaCode [Li et al. (2022)], PaLM-Coder [Chowdhery et al. (2022)], và PanGu-Coder [Shen et al. (2023)]. Tuy nhiên, một nhược điểm đáng chú ý là việc thiếu sự sẵn có của mã nguồn mở cho các mô hình tiên tiến này, một khoảng trống mà sau đó đã được lấp đầy bởi việc phát hành một số biến thể mã nguồn mở như CodeParrot², PolyCoder³, PyCodeGPT⁴, SantaCoder [Allal et al. (2023)], và StarCoder [Li et al. (2023)]. Làn sóng mới của các mô hình mã nguồn mở này đã làm sống lại lĩnh vực tạo mã.

Hơn nữa, việc mở rộng tuần tự phạm vi ứng dụng của việc tạo mã phản ánh tính thực tiễn ngày càng tăng của lĩnh vực này. Ví dụ, CodeGeeX [Zheng et al. (2023)], BLOOM [Workshop (2022)] và ERNIE-Code [Chai et al. (2022)] đã được phát triển để cho phép mô hình hóa đa ngôn ngữ. JuPyT5 [Chandel et al. (2022)] đã được huấn luyện bằng cách sử dụng một kho dữ liệu mở rộng các notebook Jupyter, mục tiêu chính của nó là tăng cường quá trình lập trình tương tác. Các mô hình như DocCoder và APICoder [Zan et al. (2022)] cũng đã được xây dựng để trang bị cho các mô hình ngôn ngữ chức năng gọi API. Hơn nữa, một số mô hình, bao gồm InCoder [Fried et al. (2022)], SantaCoder, và StarCoder, hỗ trợ tạo mã tại các vị trí tùy ý.

Gần đây, một số nhóm đã sử dụng kỹ thuật điều chỉnh chỉ dẫn để khai thác kiến thức tiềm năng rộng lớn chứa trong các mô hình ngôn ngữ mở rộng. Quá trình này liên quan đến việc tinh chỉnh cẩn thận các mô hình này với các bộ dữ liệu chất lượng cao. Về mặt tạo mã, các mô hình WizardCoder (15B), PanguCoder và phi-1 (1.3B) nổi bật vì hiệu suất mẫu mực của chúng. Điều này đã đạt được thông qua việc tinh chỉnh cẩn thận với dữ liệu được tạo bởi GPT-3.5 và GPT-4 của OpenAI.

2.2 BỐI CẢNH BỘ DỮ LIỆU MÃ, ĐẠI SỐ VÀ LOGIC

Bối cảnh của các bộ dữ liệu mã, logic và đại số tràn ngập với các nguồn tài nguyên mới có thể được sử dụng để tinh chỉnh LLM Lập trình (phần lớn trong số chúng là mã nguồn mở).

Bộ dữ liệu lập trình quan trọng nhất trong lĩnh vực này là CodeAlpaca-20k⁵. Nhiều mô hình, như PanGu-Coder hoặc WizardCoder, đã cấu trúc bộ dữ liệu của chúng cũng thông qua việc thao tác Code Alpaca với các kỹ thuật tăng cường dữ liệu. Mô hình lập trình Phi-1 [Gunasekar et al. (2023)] cũng đã được huấn luyện với bộ dữ liệu ngôn ngữ-mã được lọc, đây là một tập con của The Stack⁶ (nó chứa hơn 6TB các file mã nguồn được cấp phép cho phép bao gồm 358 ngôn ngữ lập trình).

Cộng đồng mã nguồn mở cung cấp nhiều nguồn tài nguyên ở định dạng Q&A hữu ích cho việc tinh chỉnh LLM về các bộ dữ liệu cho toán học và logic. Phần lớn các bộ dữ liệu này được tạo ra bởi T5⁷, GPT-3.5, GPT-4, hoặc sự kết hợp của các mô hình này (mặc dù các chính sách của OpenAI vẫn có thể được giải thích trong bối cảnh này).

--- TRANG 3 ---

2.3 KỸ THUẬT MỚI NHẤT CHO KỸ THUẬT PROMPT

Trong phần này, chúng tôi phác thảo các phương pháp kỹ thuật prompt chính kết hợp với kỹ thuật prompt, được áp dụng vào lĩnh vực LLM lập trình:

• Chuỗi Suy nghĩ (CoT): Wei et al.[2023] báo cáo rằng các mô hình ngôn ngữ lớn có thể tạo ra khả năng suy luận khi được prompt theo cách này. Một chuỗi suy nghĩ là một chuỗi các bước suy luận ngôn ngữ tự nhiên trung gian dẫn đến kết quả cuối cùng.

• CoT và Tự nhất quán: đây là sự tiến hóa tự nhiên của kỹ thuật CoT. Nó trước tiên lấy mẫu một tập hợp đa dạng các đường suy luận thay vì chỉ lấy cách tham lam, sau đó chọn câu trả lời nhất quán nhất bằng cách loại bỏ các đường suy luận đã lấy mẫu. Tự nhất quán tận dụng trực giác rằng một vấn đề suy luận phức tạp thường cho phép nhiều cách suy nghĩ khác nhau dẫn đến câu trả lời đúng duy nhất của nó (Wang et al.[2022]).

• Cây Suy nghĩ (ToT): Yao et al.[2023] báo cáo rằng ToT cho phép LM thực hiện việc đưa ra quyết định có chủ ý bằng cách xem xét nhiều đường suy luận khác nhau và tự đánh giá các lựa chọn để quyết định hành động tiếp theo, cũng như nhìn về phía trước hoặc quay trở lại khi cần thiết để đưa ra các lựa chọn toàn cục.

• CoT Giáo viên: Ho et al.[2023] đã chứng minh rằng thông qua việc tăng cường prompt với một lời giải thích "giáo dục" được tạo ra bởi một mô hình lớn hơn, kết quả xuất sắc được đạt được trong việc tinh chỉnh các mô hình nhỏ hơn. Mukherjee et al.[2023] cũng đã sử dụng cách tiếp cận "giảng dạy" này để phát triển mô hình Orca.

• EvolInstruct: Luo et al.[2023] đã đề xuất một cách tiếp cận mới cho tăng cường dữ liệu đã đạt được kết quả quan trọng. Họ phát hiện rằng LLM có thể làm cho các chỉ dẫn đã cho phức tạp và khó khăn hơn bằng cách sử dụng các prompt cụ thể. Ngoài ra, các mô hình có thể tạo ra các chỉ dẫn hoàn toàn mới có độ phức tạp tương đương nhưng hoàn toàn khác nhau. Sử dụng khám phá này, các nhà tạo ra WizardCoder có thể tiến hóa lặp đi lặp lại một bộ dữ liệu chỉ dẫn ban đầu, cải thiện mức độ khó khăn và mở rộng sự phong phú và đa dạng của nó.

2.4 KỸ THUẬT ĐÁNH GIÁ MỚI NHẤT CHO LLM LẬP TRÌNH (HUMAN EVAL, MBPP, MULTI PL-E, HUMAN EVAL PACK)

Chương phụ này cung cấp một cái nhìn tổng quan về các benchmark hiện đang được sử dụng để đánh giá LLM Lập trình.

1. HumanEval⁸: Benchmark chuẩn chung này chứa một tập hợp 163 bài toán giới hạn trong ngôn ngữ Python. Nó đánh giá liệu mã của mô hình có thành công vượt qua tất cả các bài kiểm tra hay không và chỉ cung cấp kết quả nhị phân và định lượng. Thông thường, có 3 loại đánh giá Humaneval: pass@1, pass@10 và pass@100. Chúng khác nhau về số lượng "cơ hội" được cung cấp cho mô hình được kiểm tra để tạo ra câu trả lời đúng cho bài toán.

2. MultiPL-E⁹: Dựa trên tiền đề của HumanEval, MultiPL-E lấy benchmark này và dịch kết quả của nó sang nhiều ngôn ngữ lập trình như C++, Rust, Go, Java và nhiều hơn nữa. Với cùng cấu trúc xếp hạng như HumanEval, công cụ này cũng cung cấp một đánh giá nhị phân định lượng.

3. MBPP¹⁰: Bao gồm khoảng 1000 vấn đề lập trình được lấy từ các lập trình viên Python, benchmark này hướng đến người mới bắt đầu. Nó cung cấp mô tả các tác vụ, các giải pháp mã tương ứng, và ba trường hợp kiểm tra tự động. Trọng tâm của nó là về các nguyên tắc cơ bản của lập trình và việc áp dụng các hàm thư viện chuẩn.

4. HumanEval Pack: Phương pháp đánh giá sáng tạo này của nhóm Big Code¹¹ mang lại một góc nhìn mới mẻ cho việc đánh giá LLM Lập trình. Nó mở rộng HumanEval bằng cách tham gia vào ba giai đoạn khác nhau: Sửa, Giải thích, và Tổng hợp. Giai đoạn "Sửa" đánh giá khả năng của mô hình trong việc sửa chữa các hàm mã chứa lỗi tinh vi, giai đoạn "Giải thích" đánh giá khả năng của mô hình trong việc tạo ra các lời giải thích mã rõ ràng, trong khi giai đoạn "Tổng hợp" đánh giá mức độ hiệu quả mà mô hình tổng hợp mã được đưa ra một chỉ dẫn ngôn ngữ tự nhiên.

--- TRANG 4 ---

3 PHƯƠNG PHÁP

3.1 TỔNG QUAN BỘ DỮ LIỆU

Nhìn chung, để tạo ra Safurai-001 (bắt đầu từ StarCoder 15B [Li et al. (2023)]) chúng tôi đã sử dụng một bộ dữ liệu gồm 200.000 ví dụ Q&A.

Như chúng ta đã thấy từ các ấn phẩm của WizardCoder [Xu et al. (2023)] và Phi-1 [Gunasekar et al. (2023)], chất lượng dữ liệu là thiết yếu để tạo ra một LLM lập trình hiệu suất. Vì vậy chúng tôi đã sử dụng các kỹ thuật tăng cường dữ liệu và kỹ thuật prompt mới nhất để tạo ra các bộ dữ liệu. Hơn nữa, chúng tôi đã tham gia một số bộ dữ liệu và dữ liệu liên quan đến suy luận logic và đại số cơ bản, để tăng cường khả năng hiểu biết của StarCoder.

3.2 NGUỒN BỘ DỮ LIỆU BAN ĐẦU

Đây là các bộ dữ liệu độc quyền mà chúng tôi đã chọn cho việc huấn luyện Safurai-001:

• Bộ dữ liệu Mã Safurai (163k)
• Bộ dữ liệu Logic Q&A (22k)
• Bộ dữ liệu Toán Q&A (15k)

3.3 BIẾN ĐỔI DỮ LIỆU

Chúng tôi đã sử dụng một LLM bổ sung để tăng cường tiềm năng giáo dục hiện diện trong mô hình. Bằng cách kết hợp cả bài toán và giải pháp của nó, chúng tôi đã prompt mô hình để làm sáng tỏ quá trình suy luận dẫn đến giải pháp.

Việc thử nghiệm của chúng tôi với các kỹ thuật khác nhau đã dẫn đến việc tạo ra một bộ dữ liệu đa dạng. Sau đây là một số phương pháp chúng tôi đã khai thác để tăng cường giá trị giáo dục:

Kỹ thuật chuyển đổi được sử dụng cho các bộ dữ liệu ban đầu của chúng tôi:
• Suy luận chuỗi suy nghĩ
• Suy luận cây suy nghĩ
• Hiển thị các lỗi tiềm năng
• Tập trung vào các trường hợp biên và giải thích unit test
• Làm nổi bật các yêu cầu câu hỏi một cách khách quan hơn
• Bài học lập trình liên quan đến chủ đề
• Giảng dạy phản hồi

3.3.1 VÍ DỤ THỰC NGHIỆM PROMPT BIẾN ĐỔI DỮ LIỆU:

ToT Code Instructor
"Là một phần của bài tập cải thiện giải thích mã AI
, nhiệm vụ của bạn như sau:\n"
f"Câu hỏi: \n\n{row['instruction']}\n\n"
f"Câu trả lời Hiện tại: \n\n{row['output']}\n\n"
"Câu trả lời đã cho, mặc dù đúng về mặt kỹ thuật, không
cung cấp cái nhìn sâu sắc về quy trình suy nghĩ cơ bản.\n"
"Nhiệm vụ của bạn: lên kế hoạch từng bước toàn diện
dẫn đến câu trả lời.\n"
"Điều này nên bao gồm các giải thích bằng ngôn ngữ đơn giản và
mã tương ứng, được trình bày gọn gàng bằng markdown."
"Câu trả lời của bạn sẽ phục vụ như một sự thay thế
nhiều thông tin hơn cho câu trả lời ban đầu. Hãy phấn đấu
cho sự đơn giản và giao tiếp giống con người.\n\n"

--- TRANG 5 ---

"Nhưng có một điều đặc biệt: hình dung sự hợp tác giữa
ba chuyên gia, mỗi người thêm một mảnh vào câu đố."
"Sau khi đóng góp một bước, họ thảo luận với
nhóm trước khi tiếp tục."
"Nếu một chuyên gia xác định bước của họ không đúng, họ
rút lui khỏi nhiệm vụ."
"Bài tập kết thúc khi một câu trả lời đúng toàn diện
đã được đạt được, hoặc tất cả các chuyên gia đã
rút lui."

CoT Code Instructor
"Tôi đang huấn luyện AI viết mã và tôi cần sự giúp đỡ của bạn.\n"
f"Đây là một câu hỏi mẫu: \n\n{row['instruction']}\n\n"
f"Và đây là một câu trả lời: \n\n{row['output']}\n\n"
"Câu trả lời đã cho quá cơ bản và không giải thích các
bước được thực hiện để đạt được nó.\n"
"Bạn có thể giúp tạo ra một kế hoạch từng bước để đạt được
câu trả lời này không?\n"
"Mỗi bước nên đơn giản và dễ hiểu.\n"
"Câu trả lời của bạn nên bao gồm kế hoạch này và mã
thực tế trong markdown trong một khối."
"Câu trả lời của bạn sẽ thay thế câu trả lời tôi đã cho bạn thấy.
Nó nên nghe có vẻ con người!"
"Hãy chắc chắn không cắt đứt từ hoặc câu giữa chừng."

Teacher Code Instructor
"Hãy tưởng tượng bạn là một chuyên gia lập trình được giao nhiệm vụ
cung cấp hỗ trợ lập trình rõ ràng và chính thức."
f"Bạn được trình bày với vấn đề này: \n\n{row['instruction']}\n\n"
"Bạn có hai mục tiêu chính:"
"1) giải thích quy trình để giải quyết vấn đề từng bước.
Giải thích quy trình để giải quyết vấn đề từng bước
theo cách đối thoại, với một số điểm đánh dấu."
"2) bao gồm các ví dụ cụ thể về lỗi phổ biến cần
tránh, kèm theo các đoạn mã minh họa những lỗi này.
Gắn thẻ các đoạn mã này là 'ErrorExample'."
"Khi cung cấp giải pháp đúng, hãy đảm bảo có
bình luận trong mã để tăng cường khả năng hiểu của nó
, giải quyết các điểm quan trọng và lỗi có thể xảy ra."

CoT Logic Instructor
"Tôi có một bộ dữ liệu với các câu hỏi và phản hồi về
các vấn đề logic.\n"
f"Đây là một vấn đề logic: \n\n{row['instruction']}\n\n"
f"Đây là giải pháp được cung cấp cho vấn đề: \n\n{row['output']}\n\n"
"Giải pháp được cung cấp quá đơn giản và không giải thích
quy trình để có được nó.\n"
"Bạn có thể vui lòng cung cấp một lý luận chi tiết để có được
giải pháp không?\n"

--- TRANG 6 ---

3.4 BỘ DỮ LIỆU SAFURAI-001 ĐƯỢC SỬ DỤNG CHO FINETUNING:

• Safurai Code Instructor (16k) (các ví dụ được lọc từ Bộ dữ liệu Mã Safurai ban đầu, không được biến đổi)
• Bộ dữ liệu Logic Q&A (22k)
• Teacher Code Instructor (70k)
• Bộ dữ liệu Toán Q&A (15k)
• Teacher Code Instructor với Lỗi Tiềm năng (21k)
• ToT Code Instructor (30k)
• CoT Code Instructor (26k)

Hình 1: Phân chia Bộ dữ liệu Safurai-001

3.5 PHƯƠNG PHÁP FINE-TUNING

Quá trình huấn luyện cho mô hình StarCoder được thực hiện trên tám card đồ họa 100 A100 80GB, với toàn bộ mạng được huấn luyện trong 10 giờ. Quá trình này được thực hiện bằng cách sử dụng framework DeepSpeed ZeRO-3. Để tinh chỉnh các mô hình StarCoder, chúng tôi đã tận dụng kho lưu trữ GitHub của Big Code: https://github.com/bigcode-project/starcoder/tree/main/chat

Vì trọng tâm là điều chỉnh chỉ dẫn, dữ liệu đã được bao bọc trong định dạng đối thoại. Một ví dụ về điều này được minh họa dưới đây:

{
"messages": [
{
"content": "Có thể tưởng tượng một xã hội
mà không có luật pháp không?",
"role": "user"},
{
"content": "Rất khó để tưởng tượng một
xã hội có thể được duy trì
mà không có bất kỳ dấu hiệu nào của Luật pháp.",
"role": "assistant",
},
{

--- TRANG 7 ---

"content": "Có vẻ như bạn coi sự
vắng mặt của luật pháp bằng sự vắng mặt
của bất cứ thứ gì có thể hướng dẫn hành vi
của cá nhân.",
"role": "user",
},
{
"content": "Bạn nói đúng khi có
những yếu tố khác có thể hướng dẫn hành vi trong
xã hội và đóng vai trò trong việc hình thành
hành vi và tương tác của cá nhân với
nhau. Tuy nhiên, ngay cả trong các xã hội nơi
những yếu tố này hiện diện, luật pháp vẫn phục vụ
một vai trò quan trọng trong việc duy trì trật tự xã hội
và giải quyết xung đột.",
"role": "assistant",
}
]
}

Chúng tôi cũng đã bao bọc đối thoại với các token đặc biệt. Đây là một ví dụ về cách nó trông:

<|system|>
Dưới đây là cuộc đối thoại giữa con người và trợ lý AI
được gọi là StarChat.
<|end|>
<|user|>
Có thể tưởng tượng một xã hội mà không có luật pháp không?<|end|>
<|assistant|>
Rất khó để tưởng tượng ...<|end|>
<|user|>
Có vẻ như bạn ...<|end|>
<|assistant|>
Bạn nói đúng ...<|end|>
<|user|>
Vâng, nhưng luật pháp phức tạp ...<|end|>

Quá trình huấn luyện liên quan đến việc thiết lập một số siêu tham số. Cài đặt siêu tham số cho việc huấn luyện được chi tiết như sau:

• Kích thước batch: 512
• Tỷ lệ học: 2e-5
• Epochs: 3
• Độ dài tối đa: 2048
• Bước warmup: 30
• Bộ lập lịch tỷ lệ học (LR): cosine

3.6 ĐÁNH GIÁ

Việc làm sâu sắc hiểu biết của chúng ta về khả năng và phạm vi của các mô hình LLM là quan trọng để tinh chỉnh việc áp dụng chúng trong thế giới thực. Tuy nhiên, chúng tôi thấy rằng các phương pháp đánh giá hiện có như HumanEval¹² bị hạn chế trong khả năng cung cấp phân tích toàn diện về khả năng của các mô hình này. Điều này dẫn đến việc phát minh ra phương pháp Đánh giá Multi Parameters dựa trên GPT4, một thay thế định tính được thiết kế để cung cấp hiểu biết tinh tế hơn về hiệu suất của các LLM lập trình.

--- TRANG 8 ---

Những tiêu chí định tính mới này cho phép chúng tôi khám phá thêm nhiều trường hợp sử dụng bên ngoài kết quả pass-fail nhị phân thông thường của các phương pháp định lượng hiện có, do đó cung cấp một câu chuyện chi tiết hơn xác định các điểm mạnh (hoặc điểm yếu) độc đáo của từng mô hình. HumanEval, MBPP¹³ và MultiPL-E¹⁴. Hầu hết chúng nghiêng về đánh giá định lượng hơn là định tính, bỏ qua các khía cạnh quan trọng của khả năng của mô hình. Như vậy, chúng tôi biện minh cho sự đổi mới và cần thiết của phương pháp Đánh giá MultiParameters dựa trên GPT4 trong việc giải quyết khoảng trống này.

3.7 THỰC NGHIỆM MỚI VỀ ĐÁNH GIÁ (ĐÁNH GIÁ MULTI PARAMETERS DỰA TRÊN GPT4)

Tìm cách khám phá các khía cạnh định tính của mô hình LLM Safurai của chúng tôi, chúng tôi đã thử nghiệm với một cách tiếp cận đánh giá mới dựa trên GPT-4 [OpenAI (2023)].

3.7.1 PHÂN TÍCH DỰA TRÊN GPT4

Phương pháp này liên quan đến việc đánh giá 20 (GPT-4 HE-20) và 40 (GPT-4 HE-40) câu trả lời được lấy từ mỗi mô hình được so sánh, thu được thông qua bộ dữ liệu HumanEval. GPT-4 đã được sử dụng để xác định hiệu suất của các mô hình: Safurai, Claude¹⁵, WizardCoder [Xu et al. (2023)], ChatGPT¹⁶, và Starchat Alpha Prompted¹⁷. Chúng tôi đã kết hợp cả bài toán và năm phản hồi thành một prompt GPT-4 duy nhất cộng với các bài kiểm tra cụ thể của bài toán, được yêu cầu đánh giá từng phản hồi trên thang điểm từ 0 đến 100 - điểm số tốt nhất có thể. Hơn nữa, để làm sâu sắc hiểu biết của chúng tôi, chúng tôi đã yêu cầu GPT-4 cung cấp một mô tả ngắn gọn chi tiết lý luận đằng sau các xếp hạng của nó. Đây là prompt GPT-4: Tôi đã hỏi 4 mô hình AI khác nhau: [bài toán] Đây là câu trả lời của mô hình đầu tiên: [câu trả lời] Đây là câu trả lời của mô hình thứ hai: [câu trả lời] Đây là câu trả lời của mô hình thứ ba: [câu trả lời] Đây là câu trả lời của mô hình thứ tư: [câu trả lời] Đây là các bài kiểm tra cho giải pháp mã của bài toán: [bài kiểm tra] Vui lòng đánh giá từng câu trả lời từ 0 đến 100 (câu trả lời tốt nhất có thể). Xem xét liệu mã có giải quyết hoàn toàn bài toán hay không, liệu nó có xử lý tất cả các trường hợp biên hay không, và liệu nó có chứa tất cả các chức năng cần thiết hay không. Ngoài ra, hãy cung cấp một lời giải thích ngắn cho mỗi xếp hạng. Theo cách này, ngoài việc định lượng hiệu suất, chiến lược đánh giá của chúng tôi tiết lộ những cái nhìn sâu sắc có giá trị về điểm mạnh và điểm yếu của từng mô hình. (Bảng 2)

Bảng 2: Kết quả của GPT-4 HumanEval-20 và GPT-4 HumanEval-40

Ngày     Mô hình                   GPT-4 HumanEval-20    GPT-4 HumanEval-40
Mô hình nguồn đóng
2022 Nov GPT3.5-turbo            81,5%                 80,875%
2023 March Claude                75%                   78,7%
Mô hình nguồn mở
2023 May  Starchat-Alpha prompted 64,3%                62,4%
2023 June WizardCoder            74,4%                 74,7%
2023 June Safurai-001            85% (+3.5%)          84,875% (+4%)

Các thực nghiệm được chi tiết ở trên cung cấp một quy trình toàn diện cho đánh giá mô hình so sánh. Bằng cách đánh giá 20 (GPT-4 HE-20) và 40 (GPT-4 HE-40) phản hồi từ mỗi mô hình được so sánh bằng cách sử dụng bộ dữ liệu HumanEval, chúng tôi đã tạo ra dữ liệu định lượng có giá trị và những hiểu biết định tính cơ bản về hiệu suất mô hình.

--- TRANG 9 ---

Tuy nhiên, chúng tôi nhận ra rằng các xếp hạng toàn diện được cung cấp bởi GPT-4, mặc dù không thể thiếu đối với quá trình đánh giá, không thể nắm bắt hoàn toàn những đặc điểm tinh tế vốn có trong mỗi mô hình. Xếp hạng toàn diện khởi động khả năng của mô hình để giải quyết vấn đề và tạo ra mã đúng, nhưng chúng thiếu trong việc làm sáng tỏ các khía cạnh như hiệu quả, khả năng đọc, thực hành mã hóa tốt nhất, và sự phù hợp với vấn đề. Những chiều kích quan trọng này, mặc dù ít rõ ràng, nhưng cũng quan trọng không kém đối với tiện ích và tác động của mô hình trong các tình huống phát triển phần mềm thực tế.

Để giảm thiểu những thiếu sót này và cung cấp một đánh giá chi tiết hơn, đa chiều và tinh tế hơn về các chức năng của mô hình, chúng tôi đã giới thiệu một hệ thống đánh giá bốn tham số.

3.7.2 BENCHMARK ĐÁNH GIÁ MULTI PARAMETERS DỰA TRÊN GPT4

Để hiểu thêm về phản hồi của mô hình, chúng tôi đã tạo ra một hệ thống Đánh giá Multi-Parametric dựa trên GPT4. Prompt GPT-4 đơn lẻ, chứa cả bài toán và bốn giải pháp tương ứng, không chỉ được đánh giá tổng thể mà còn được phân tích dựa trên bốn tham số khác biệt. Đó là:

1. Tính Đúng đắn và Hoàn chỉnh của Mã: Điều này liên quan đến việc đánh giá liệu mã có chạy mà không có lỗi hay không và liệu nó có giải quyết hoàn toàn vấn đề hay không, xem xét tất cả các trường hợp biên tiềm năng.

2. Hiệu quả: Phép đo này xác định mức độ tối ưu hóa của mã. Nó kiểm tra liệu mã có sử dụng tài nguyên một cách có khả năng hay không, và liệu nó có mở rộng hiệu quả khi kích thước đầu vào tăng lên hay không.

3. Khả năng Đọc và Thực hành Tốt nhất: Tiêu chí này đánh giá sự rõ ràng của mã được viết, liệu nó có dễ hiểu hay không, và liệu nó có tuân thủ các quy ước mã hóa đã được thiết lập và thực hành tốt nhất hay không.

4. Sự Phù hợp với Vấn đề (Câu trả lời Chính xác): Tham số này đánh giá mức độ trực tiếp mã giải quyết vấn đề đã cho, đánh giá liệu giải pháp được triển khai có hiệu quả và phù hợp hay không.

Đây là các prompt GPT-4 được sử dụng cho mỗi tham số:

1. Tôi đã hỏi 4 mô hình AI khác nhau: [bài toán] Đây là câu trả lời của mô hình đầu tiên: [câu trả lời] Đây là câu trả lời của mô hình thứ hai: [câu trả lời] Đây là câu trả lời của mô hình thứ ba: [câu trả lời] Đây là câu trả lời của mô hình thứ tư: [câu trả lời] Đây là các bài kiểm tra cho giải pháp mã của vấn đề: [bài kiểm tra] Vui lòng đánh giá từng câu trả lời từ 0 đến 100 (câu trả lời tốt nhất có thể) dựa trên Tính Hoàn chỉnh của Mã. Xem xét liệu mã có giải quyết hoàn toàn vấn đề hay không, liệu nó có xử lý tất cả các trường hợp biên hay không, và liệu nó có chứa tất cả các chức năng cần thiết hay không. Ngoài ra, hãy cung cấp một lời giải thích ngắn cho mỗi xếp hạng.

2. Tôi đã hỏi 4 mô hình AI khác nhau: [bài toán] Đây là câu trả lời của mô hình đầu tiên: [câu trả lời] Đây là câu trả lời của mô hình thứ hai: [câu trả lời] Đây là câu trả lời của mô hình thứ ba: [câu trả lời] Đây là câu trả lời của mô hình thứ tư: [câu trả lời] Đây là các bài kiểm tra cho giải pháp mã của vấn đề: [bài kiểm tra] Vui lòng đánh giá từng câu trả lời từ 0 đến 100 (câu trả lời tốt nhất có thể) về Hiệu quả. Điều này đòi hỏi việc xem xét mức độ tối ưu hóa của mã, mức độ tiết kiệm khi sử dụng tài nguyên hệ thống, và khả năng mở rộng hoặc độ bền của nó đối với các đầu vào lớn hơn. Xem xét cả độ phức tạp thời gian (khả năng thực hiện các tác vụ nhanh chóng) và độ phức tạp không gian (lượng bộ nhớ mà chương trình sử dụng). Ngoài ra, hãy cung cấp một lời giải thích ngắn cho mỗi xếp hạng.

3. Tôi đã hỏi 4 mô hình AI khác nhau: [bài toán] Đây là câu trả lời của mô hình đầu tiên: [câu trả lời] Đây là câu trả lời của mô hình thứ hai: [câu trả lời] Đây là câu trả lời của mô hình thứ ba: [câu trả lời] Đây là câu trả lời của mô hình thứ tư: [câu trả lời] Đây là các bài kiểm tra cho giải pháp mã của vấn đề: [bài kiểm tra] Vui lòng đánh giá từng câu trả lời từ 0 đến 100 (câu trả lời tốt nhất có thể) dựa trên Tính Hữu ích và Giá trị Giáo dục của nó. Xem xét liệu câu trả lời có cung cấp giải thích rõ ràng hay không, liệu nó có dễ theo dõi và hiểu hay không, liệu nó có dạy bạn điều gì có giá trị về vấn đề hoặc các khái niệm mã hóa liên quan hay không, và liệu nó có cho bạn những hiểu biết mới có thể giúp bạn trong các vấn đề tương tự trong tương lai hay không. Ngoài ra, hãy cung cấp một lời giải thích ngắn cho mỗi xếp hạng.

4. Tôi đã hỏi 4 mô hình AI khác nhau: [bài toán] Đây là câu trả lời của mô hình đầu tiên: [câu trả lời] Đây là câu trả lời của mô hình thứ hai: [câu trả lời] Đây là câu trả lời của mô hình thứ ba: [câu trả lời] Đây là câu trả lời của mô hình thứ tư: [câu trả lời] Đây là các bài kiểm tra cho giải pháp mã của vấn đề: [bài kiểm tra] Vui lòng đánh giá từng câu trả lời từ 0 đến 100 (câu trả lời tốt nhất có thể) dựa trên Sự Phù hợp với Vấn đề (câu trả lời chính xác). Xem xét mức độ trực tiếp mã trả lời vấn đề, liệu nó có cung cấp giải pháp phù hợp và thích hợp hay không, và liệu nó có thực hiện các yêu cầu được nêu trong vấn đề hay không. Ngoài ra, hãy cung cấp một lời giải thích ngắn cho mỗi xếp hạng.

--- TRANG 10 ---

3.7.3 TẠI SAO GPT-4?

Việc chọn GPT-4 làm nền tảng cho phương pháp đánh giá mới của chúng tôi là một quyết định rõ ràng và chiến lược. Ở trạng thái hiện tại của nghệ thuật, GPT-4 thể hiện hiệu suất vượt trội nhất trong số tất cả các mô hình lập trình hiện có.

Ngoài độ bền và hiệu quả mẫu mực, GPT-4 tự hào có một phạm vi ứng dụng rộng lớn, khiến nó trở thành một mô hình lý tưởng để đánh giá những mô hình ít mạnh mẽ hơn trong các khía cạnh đa dạng. Khả năng này cho phép quá trình đánh giá được toàn diện nhất có thể, kiểm tra mọi mô hình dưới nhiều tham số và do đó thu được một hồ sơ hiệu suất bao quát.

Tuy nhiên, cần lưu ý rằng có những hạn chế vốn có liên quan đến GPT-4. Những hạn chế này, được phản ánh như điểm yếu trong hệ thống đánh giá mới được đề xuất của chúng tôi, bao gồm các thiên vị tiềm năng được mang từ các hành vi GPT-4. Tuy nhiên, miễn là không có mô hình nào khác phù hợp hoặc vượt qua năng lực của GPT-4, những điểm yếu này vẫn tương đối nhỏ. Chúng tôi tin rằng ở giai đoạn này, GPT-4 trình bày thước đo chính xác nhất cho các chỉ số hiệu suất được xem xét trong phương pháp Đánh giá MultiParameters dựa trên GPT4 của chúng tôi.

Do đó, cho đến khi một mô hình tiên tiến hơn GPT-4 bước vào sân khấu, trọng tâm của chúng tôi vẫn là rút ra lợi ích tối đa từ mô hình vượt trội này, tối ưu hóa việc sử dụng nó trong việc tiết lộ các điểm mạnh và các lĩnh vực cần cải thiện của mỗi mô hình lập trình được đánh giá.

4 KẾT QUẢ THỰC NGHIỆM

4.1 HIỆU SUẤT MÔ HÌNH

Ví dụ về cuộc hội thoại Nhiều lượt với Safurai-001

NGƯỜI DÙNG: Chào Safu!

Safurai-001: Chào! Tôi có thể hỗ trợ bạn như thế nào hôm nay?

NGƯỜI DÙNG: Trình độ lập trình Python của bạn như thế nào?

Safurai-001: Là một mô hình ngôn ngữ AI, tôi đã được tạo ra và huấn luyện để có hiểu biết sâu sắc về ngôn ngữ lập trình Python. Tôi có thể giúp với một phạm vi rộng các chủ đề và tác vụ trong Python, từ cú pháp và khái niệm cơ bản đến các thuật toán hoặc framework tiên tiến hơn. Hãy thoải mái hỏi tôi bất cứ điều gì liên quan đến lập trình Python, và tôi sẽ cố gắng hết sức để hỗ trợ bạn!

NGƯỜI DÙNG: Làm thế nào tôi có thể viết một hàm trong Python để tìm trung vị của một danh sách? Vui lòng bao gồm độ phức tạp thời gian và không gian của giải pháp.

--- TRANG 11 ---

Safurai-001: Trung vị của một danh sách là giá trị ở giữa khi danh sách được sắp xếp theo thứ tự tăng dần. Nếu danh sách có số lượng phần tử lẻ, trung vị là phần tử ở giữa. Nếu danh sách có số lượng phần tử chẵn, trung vị là trung bình của hai phần tử ở giữa.

Để giải quyết vấn đề này, chúng ta có thể làm theo các bước sau:

1. Sắp xếp danh sách theo thứ tự tăng dần.
2. Kiểm tra xem độ dài của danh sách là lẻ hay chẵn.
3. Nếu độ dài là lẻ, trả về phần tử ở giữa.
4. Nếu độ dài là chẵn, tính trung bình của hai phần tử ở giữa và trả về.

Đây là việc triển khai hàm median:

```python
def median(l: list):
    l.sort()  # Sắp xếp danh sách theo thứ tự tăng dần
    length = len(l)
    if length % 2 == 1:  # Kiểm tra xem độ dài có lẻ không
        return l[length // 2]  # Trả về phần tử ở giữa
    else:
        mid1 = l[length // 2]  # Phần tử ở giữa đầu tiên
        mid2 = l[length // 2 - 1]  # Phần tử ở giữa thứ hai
        return (mid1 + mid2) / 2  # Trả về trung bình của hai phần tử ở giữa
```

Các trường hợp kiểm tra:
```python
print(median([3, 1, 2, 4, 5]))  # Kết quả: 3
print(median([-10, 4, 6, 1000, 10, 20]))  # Kết quả: 15.0
```

Độ phức tạp thời gian của giải pháp này là O(n log n), trong đó n là độ dài của danh sách đầu vào. Điều này là do thao tác sắp xếp mất O(n log n) thời gian. Độ phức tạp không gian là O(1) vì chúng ta không sử dụng bất kỳ cấu trúc dữ liệu bổ sung nào.

4.2 VÍ DỤ BỘ DỮ LIỆU ĐÃ BIẾN ĐỔI

Đầu vào Gốc
Viết mã JavaScript để lặp qua tất cả các phần tử trong mảng đã cho. Đầu vào: numbersArray = [45, 6, 23, 12, 35]

Đầu ra Gốc
```javascript
for(let i = 0; i < numbersArray.length; i++) 
    console.log(numbersArray[i]);
```

Đầu ra Đã tăng cường

--- TRANG 12 ---

Tất nhiên, tôi rất vui được giúp đỡ!

1. Bắt đầu bằng cách khai báo biến mảng:
```javascript
const numbersArray = [45, 6, 23, 12, 35]
```

2. Chúng ta muốn lặp qua tất cả các phần tử trong mảng, vì vậy chúng ta cần sử dụng vòng lặp for.
3. Khai báo biến chỉ số i của chúng ta và đặt nó bằng 0
4. Định nghĩa điều kiện vòng lặp; chúng ta muốn tiếp tục lặp miễn là biến chỉ số nhỏ hơn độ dài của mảng.
5. Sử dụng phương thức console.log() để in ra giá trị của phần tử hiện tại trong mảng.
6. Cuối cùng, tăng biến chỉ số lên 1 để chuyển sang phần tử tiếp theo trong mảng.

Đây là mã cuối cùng:
```javascript
const numbersArray = [45, 6, 23, 12, 35];
for (let i = 0; i < numbersArray.length; i++) {
    console.log(numbersArray[i]);
}
```

Mã này sẽ lặp qua tất cả các phần tử trong mảng và in ra giá trị của chúng ra console.

4.3 KẾT QUẢ ĐÁNH GIÁ

4.3.1 HUMAN EVAL

HumanEval¹⁸, được thành lập bởi OpenAI, đã trở thành benchmark chủ đạo trong việc đánh giá các mô hình AI lập trình. Nó tập trung vào khả năng tạo mã của mô hình dựa trên các yêu cầu chính xác và cung cấp các giải pháp và bài kiểm tra tiêu chuẩn. MultiPL-E¹⁹ mở rộng ranh giới của các benchmark hiện tại bằng cách dịch chúng sang các ngôn ngữ mới và do đó trở thành một nền tảng benchmarking đa ngôn ngữ khổng lồ. Nó liên tục mở rộng về số lượng ngôn ngữ lập trình, cung cấp một điểm so sánh hữu ích cho các mô hình như Safurai.

Mô hình của chúng tôi, Safurai001, đã đạt được điểm pass@1 là 50.61% trên benchmark HumanEval với n=20. (Bảng 1)

Bảng 1: Kết quả của pass@1(%) trên HumanEval

Mô hình                          Kích thước    HumanEval
Mô hình nguồn đóng
Alphacode Li et al. [2022]       540B         26.2
Codex Chen et al. [2021]         12B          28.8
Code-Cushman-001 OpenAI [2022]   -            33.5
Code-Davinci-002 OpenAI [2022]   -            47.0
GPT-3.5 OpenAI [2023]            -            48.1
GPT-3.5 Luo et al. [2023]        -            68.9
GPT-4 OpenAI [2023]              -            67.0
GPT-4 Bubeck et al. [2023]       -            82.0
Mô hình nguồn mở
LLaMa Touvron et al. [2023]      65B          23.7
CodeT5+ Wang et al. [2023]       16B          30.9
StarCoder Li et al. [2023]       15B          33.6
WizardCoder Luo et al. [2023]    15B          57.3
Safurai001 [2023]               15B          50.61

Tuy nhiên, việc chỉ áp dụng những tiêu chuẩn này giới hạn phân tích của chúng tôi vào các thước đo định lượng, do đó mất đi một số hương vị quan trọng của các mô hình.

--- TRANG 13 ---

4.3.2 BENCHMARK ĐÁNH GIÁ ĐỊNH TÍNH MỚI

Chúng tôi đã kiểm tra các mô hình với 40 bài toán đã chọn từ HumanEval, đã được sử dụng trong Phân tích dựa trên GPT4. Phương pháp Đánh giá MultiParameters dựa trên GPT4 làm sáng tỏ các lĩnh vực cần tối ưu hóa, giải thích tại sao một phản hồi cụ thể lại vượt trội, và hiểu đáng kể các khả năng tạo mã cụ thể của mỗi mô hình; do đó cung cấp một thước đo định tính chi tiết.

Chúng tôi thấy rằng phương pháp này tiết lộ vô số những hiểu biết có giá trị về điểm mạnh và điểm yếu của mỗi mô hình, cho phép phát triển các chiến lược nhắm mục tiêu để tăng cường. (Bảng 3)

Bảng 2: Kết quả của GPT4-based MultiParameters HumanEval

Ngày     Mô hình      Tính Đúng    Hiệu quả    Khả năng    Sự Phù hợp
                      của Mã       Mã          Đọc Mã      Câu hỏi
2022 Nov GPT-3.5-turbo  81.53%     80.33%      84.30%      82.25%
2023 March GPT-4        89.50%     89.38%      84.10%      90.93%
2023 June WizardCoder   60.7%      68.25%      67.1%       67.88%
2023 July Safurai-001   74.25%     75.45%      85.88%(+1.58%) 82.00%

Chúng tôi đã đưa phương pháp Đánh giá MultiParameters dựa trên GPT4 được đề xuất vào thử nghiệm, sử dụng cùng 40 bài toán đã chọn từ HumanEval mà trước đó đã được sử dụng trong Phân tích dựa trên GPT4 của chúng tôi. Kết quả thu được rất thú vị, khai sáng và giàu thông tin, tiết lộ các lĩnh vực cần tối ưu hóa và sự vượt trội trong các phản hồi cụ thể và nổi bật nhu cầu khám phá khả năng tạo mã ở mức độ sâu sắc. Dữ liệu định tính được cung cấp bởi phương pháp này là một kho báu thông tin, đạt đến những độ sâu mà các phương pháp đánh giá trước đó không dám mạo hiểm.

Thú vị, đánh giá này đã tiết lộ những sắc thái trong hiệu suất mô hình không hoàn toàn dự đoán được chức năng trong quá trình triển khai thực tế. Ví dụ, mặc dù WizardCoder [Xu et al. (2023)] đạt được điểm số cao hơn trong đánh giá HumanEval, nhưng được quan sát thấy rằng việc sử dụng thực tế hàng ngày, đặc biệt là đối với các nhà phát triển, không được mượt mà như vậy. Khả năng đối thoại của mô hình dường như có phần thiếu sót, khiến việc tương tác hiệu quả với nó trở nên khó khăn. Điều này được phản ánh trong điểm số 67.1 của nó trong danh mục Khả năng Đọc Mã, một sự tương phản rõ rệt với điểm số ấn tượng 85.88 của Safurai001.

Trong việc theo dõi hiệu suất của các benchmark định lượng thông thường như HumanEval và MultiPL-E, chúng tôi đã phát triển một phương pháp đánh giá định tính mới: Đánh giá MultiParameters dựa trên GPT4. Cách tiếp cận chưa từng có này cung cấp một góc nhìn rộng hơn về những sắc thái và sự phức tạp của các mô hình LLM, mở rộng quang phổ chức năng và ứng dụng của chúng.

Các mô hình như Phi1 [Gunasekar et al. (2023)], được phát triển bởi Các nhà nghiên cứu Microsoft, StarCoder [Li et al. (2023)], và WizardCoder [Xu et al. (2023)], chủ yếu được đánh giá bằng các phương pháp thông thường. Mặc dù hiệu quả, những phương pháp này thiếu khả năng cung cấp hiểu biết đầy đủ về khả năng của mô hình, do đó biện minh cho sự cần thiết phát triển phương pháp đánh giá mới của chúng tôi.

Phương pháp Đánh giá MultiParameters dựa trên GPT4 mở ra con đường mới trong lĩnh vực đánh giá mô hình LLM, cho phép các nhà nghiên cứu đi sâu hơn vào chức năng của các mô hình này và tăng cường đáng kể tiềm năng cải thiện của chúng.

4.3.3 HẠN CHẾ CỦA BENCHMARK ĐÁNH GIÁ MULTI PARAMETER DỰA TRÊN GPT4

• Phương pháp Đánh giá MultiParameters dựa trên GPT4 chỉ có thể đánh giá đến một giới hạn nhất định: Nó được chuẩn hóa ở mức hiệu suất GPT4. Điều này ngụ ý rằng bất kỳ mô hình nào vượt qua hiệu suất của GPT4 có thể không được đánh giá đáng tin cậy hoặc đánh giá chính xác. Do đó, phương pháp này không phục vụ cho những tiến bộ nhanh chóng và cải thiện liên tục trong bối cảnh phát triển AI.

• Sự biến đổi trong phản hồi: Một ràng buộc khác liên quan đến phương pháp Đánh giá MultiParameters dựa trên GPT4 là sự biến đổi trong phản hồi của GPT4. Nó không phải lúc nào cũng cung cấp phản hồi nhất quán do nhiều lý do khác nhau như sự khác biệt về bản chất và độ phức tạp của vấn đề. Sự không nhất quán này đưa ra thách thức trong việc cung cấp đánh giá mạnh mẽ về các mô hình AI Lập trình.

• Tốn thời gian: Đánh giá MultiParameters dựa trên GPT4 là một quá trình tương đối tốn thời gian. Do tính chất phức tạp và đa diện của phương pháp đánh giá này, nó đòi hỏi thời gian và tài nguyên đáng kể so với các phương pháp thông thường. Điều này có thể hạn chế khả năng mở rộng và ứng dụng thực tế trong các chu kỳ phát triển lặp lại nhanh chóng.

• Tính chất định tính: Phương pháp Đánh giá MultiParameters dựa trên GPT4 chủ yếu cung cấp đánh giá định tính. Nó có thể không xử lý hiệu quả dữ liệu hoặc thước đo định lượng, và việc so sánh những điều đó có thể là thách thức. Điều này có thể đưa ra những hạn chế khi đánh giá mô hình yêu cầu so sánh số hoặc kết quả định lượng chính xác.

--- TRANG 14 ---

4.3.4 VÍ DỤ VỀ PHẢN HỒI ĐÁNH GIÁ MULTI PARAMETER DỰA TRÊN GPT4

Đây là một ví dụ về phản hồi Đánh giá MultiParameter dựa trên GPT4. Vấn đề là Số 0 của Benchmark HumanEval. Về các câu trả lời, mô hình đầu tiên là Safurai-001, mô hình thứ hai là ChatGPT, mô hình thứ ba là WizardCoder, và mô hình thứ tư là GPT4.

Tham số Đầu tiên: Tính Đúng đắn của Mã

Câu trả lời Mô hình Đầu tiên: Xếp hạng: 90 Giải thích: Mã được cung cấp bởi mô hình đầu tiên là toàn diện, hoàn chỉnh và được giải thích tốt. Nó thực hiện đúng hàm để giải quyết vấn đề. Nó xử lý chức năng cơ bản của việc kiểm tra xem có hai số nào trong danh sách gần nhau hơn ngưỡng hay không. Tuy nhiên, nó không xử lý rõ ràng trường hợp biên khi danh sách số rỗng.

Câu trả lời Mô hình Thứ hai: Xếp hạng: 90 Giải thích: Mô hình thứ hai cung cấp một giải pháp hoàn chỉnh có chức năng và ngắn gọn. Tuy nhiên, nó không xử lý trường hợp biên khi danh sách có thể rỗng hoặc chỉ bao gồm một phần tử.

Câu trả lời Mô hình Thứ ba: Xếp hạng: 80 Giải thích: Mô hình thứ ba cung cấp một giải pháp có chức năng cho vấn đề, nhưng nó thiếu giải thích và xử lý trường hợp biên khi danh sách rỗng hoặc chỉ bao gồm một phần tử.

Câu trả lời Mô hình Thứ tư: Xếp hạng: 100 Giải thích: Câu trả lời thứ tư cung cấp hai giải pháp hoàn chỉnh, một với độ phức tạp thời gian O(n²) và một giải pháp hiệu quả khác với O(nlogn). Nó xử lý đúng tất cả các tình huống và trường hợp biên và cung cấp sự cân bằng tốt giữa giải thích, tính hoàn chỉnh và hiệu quả.

Tham số Thứ hai: Hiệu quả Mã

Câu trả lời 1: 75/100 Câu trả lời này cung cấp một giải pháp brute-force bằng cách so sánh từng phần tử với mọi phần tử khác trong danh sách. Mặc dù rõ ràng và đúng, giải pháp này có độ phức tạp thời gian O(n²), có nghĩa là nó có thể không hiệu quả cho danh sách số lớn, vì nó không mở rộng tốt với kích thước đầu vào tăng.

Câu trả lời 2: 75/100 Giải pháp được đưa ra trong Câu trả lời 2 gần như giống hệt với giải pháp trong Câu trả lời 1, có độ phức tạp thời gian O(n²) với so sánh theo cặp. Điều này thiếu hiệu quả cho các đầu vào lớn, và có thể được hưởng lợi từ việc sử dụng các cách tiếp cận tối ưu hóa hơn, chẳng hạn như sắp xếp danh sách trước.

Câu trả lời 3: 75/100 Giống như hai câu trả lời đầu tiên, câu trả lời 3 cũng sử dụng cách tiếp cận brute-force trong giải pháp. Vì nó so sánh tất cả các cặp số, nó có độ phức tạp thời gian O(n²), có nghĩa là nó sẽ không hiệu quả cho danh sách lớn hơn.

Câu trả lời 4: 90/100 Câu trả lời 4 là tốt nhất về mặt hiệu quả. Câu trả lời thực sự liệt kê hai cách tiếp cận khác nhau: một cách tiếp cận brute-force được đề cập trong ba câu trả lời đầu tiên, và một cách tiếp cận hiệu quả hơn trước tiên sắp xếp danh sách trước khi so sánh các số liền kề. Cách tiếp cận đã sắp xếp có độ phức tạp thời gian O(nlogn), mở rộng tốt hơn với danh sách số ngày càng tăng so với độ phức tạp O(n²) của các phương pháp brute-force. Điểm trừ duy nhất là do không giải thích rõ ràng sự khác biệt quan trọng về độ phức tạp giữa hai phương pháp.

--- TRANG 15 ---

Tham số Thứ ba: Khả năng Đọc Mã

Câu trả lời của mô hình đầu tiên: Xếp hạng: 90 Giải thích: Mô hình đầu tiên cung cấp một giải thích chi tiết về vấn đề, phương pháp được sử dụng để giải quyết nó, và thậm chí phân tích toàn bộ mã, làm cho nó dễ hiểu cho người mới bắt đầu. Ví dụ được cung cấp ở cuối cũng thêm giá trị bổ sung cho câu trả lời. Tuy nhiên, nó vẫn không giải thích hoặc xem xét bất kỳ biện pháp hiệu quả nào hoặc cách thay thế để giải quyết vấn đề.

Câu trả lời của mô hình thứ hai: Xếp hạng: 70 Giải thích: Mô hình thứ hai cũng giải thích đúng vấn đề và đưa ra một cách tiếp cận hợp lệ. Tuy nhiên, nó ít chi tiết hơn mô hình đầu tiên, không cung cấp phân tích rõ ràng về mã, và không có ví dụ nào về cách sử dụng hàm.

Câu trả lời của mô hình thứ ba: Xếp hạng: 35 Giải thích: Mô hình thứ ba chỉ cung cấp mã cần thiết để giải quyết vấn đề mà không có bất kỳ giải thích hoặc phân tích nào về mã. Câu trả lời của nó đúng nhưng không hữu ích hoặc có tính giáo dục, đặc biệt là đối với ai đó đang cố gắng học.

Câu trả lời của mô hình thứ tư: Xếp hạng: 95 Giải thích: Mô hình thứ tư làm rất tốt việc giải thích vấn đề và cũng giới thiệu hai cách tiếp cận khác nhau để giải quyết nó, một cách hiệu quả hơn cách kia. Nó không chỉ cung cấp cái nhìn sâu sắc về giải pháp hiệu quả hơn mà còn giải thích tại sao nó hiệu quả hơn. Nhưng mặc dù có những ưu điểm này, mô hình không phân tích mã hoặc cung cấp ví dụ về cách sử dụng.

Tham số Thứ tư: Sự Phù hợp Câu hỏi

Xếp hạng cho mô hình đầu tiên: 90. Câu trả lời từ mô hình đầu tiên khá toàn diện và trực tiếp giải quyết vấn đề. Mã được giải thích giải quyết vấn đề như yêu cầu và nó thực hiện hoàn hảo yêu cầu. Tuy nhiên, nó không xem xét hiệu quả của giải pháp.

Xếp hạng cho mô hình thứ hai: 85. Mô hình thứ hai cũng cung cấp một câu trả lời phù hợp, giải thích giải pháp khá tốt và thực hiện chức năng được mô tả một cách chính xác. Tuy nhiên, giống như mô hình đầu tiên, mô hình này không xem xét bất kỳ khía cạnh hiệu suất hoặc hiệu quả nào trong giải pháp của nó.

Xếp hạng cho mô hình thứ ba: 80. Mô hình AI này cung cấp một giải pháp đúng và trực tiếp, nhưng với ít giải thích hơn về cách tiếp cận hoặc ghi chú về hiệu suất hoặc cách sử dụng. Do đó, xếp hạng thấp hơn so với mô hình đầu tiên và thứ hai.

Xếp hạng cho mô hình thứ tư: 100. Mô hình thứ tư cung cấp hai giải pháp: giải pháp đầu tiên tương tự như các mô hình trước và giải pháp thứ hai xem xét cải thiện hiệu suất bằng cách sắp xếp danh sách trước. Mô hình này cho thấy hiểu biết lớn hơn về các yêu cầu thực tế bằng cách xem xét hiệu quả, do đó làm cho nó trở thành câu trả lời tốt nhất.

4.3.5 SO SÁNH CỦA BENCHMARK GPT4-BASED MULTI PARAMETERS VÀ HUMAN EVAL

Vấn đề
```python
from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """
    Check if in given list of numbers, are any two numbers closer to each
    other than given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
```

--- TRANG 16 ---

Phản hồi Safurai-001

Trước tiên, chúng ta cần hiểu vấn đề yêu cầu chúng ta làm gì. Chúng ta có một danh sách các số và chúng ta cần kiểm tra xem có hai số nào trong số đó gần nhau hơn một ngưỡng nhất định hay không.

Để giải quyết vấn đề này, chúng ta có thể sử dụng một vòng lặp để so sánh từng số trong danh sách với mọi số khác trong danh sách. Nếu sự khác biệt giữa hai số nhỏ hơn hoặc bằng ngưỡng, chúng ta biết rằng chúng gần nhau hơn ngưỡng.

Đây là mã để giải quyết vấn đề này:

```python
# Định nghĩa một hàm nhận vào một danh sách các số và một ngưỡng
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    # Lặp qua từng số trong danh sách
    for i in range(len(numbers)):
        # Lặp qua mọi số khác trong danh sách
        for j in range(i+1, len(numbers)):
            # Kiểm tra xem sự khác biệt giữa hai số có nhỏ hơn hoặc bằng ngưỡng không
            if abs(numbers[i] - numbers[j]) <= threshold:
                # Nếu có, trả về True
                return True
    # Nếu chúng ta đã kiểm tra mọi cặp số và không có cặp nào gần nhau hơn ngưỡng, trả về False
    return False
```

Hãy phân tích mã:

• 'def has_close_elements(numbers: List[float], threshold: float) -> bool:': Dòng này định nghĩa một hàm gọi là 'has_close_elements' nhận vào một danh sách các số ('numbers') và một ngưỡng ('threshold') và trả về một giá trị boolean (hoặc 'True' hoặc 'False').

• 'for i in range(len(numbers)):': Dòng này bắt đầu một vòng lặp đi qua từng số trong danh sách.

• 'for j in range(i+1, len(numbers)):': Dòng này bắt đầu một vòng lặp lồng nhau đi qua mọi số khác trong danh sách (bắt đầu từ số tiếp theo sau 'i').

• 'if abs(numbers[i] - numbers[j]) <= threshold:': Dòng này kiểm tra xem sự khác biệt giữa hai số có nhỏ hơn hoặc bằng ngưỡng hay không.

• 'return True': Nếu hai số gần nhau hơn ngưỡng, chúng ta trả về 'True'.

• 'return False': Nếu chúng ta đã kiểm tra mọi cặp số và không có cặp nào gần nhau hơn ngưỡng, chúng ta trả về 'False'.

Để sử dụng hàm này, chúng ta có thể gọi nó với một danh sách các số và một ngưỡng:

```python
# Gọi hàm với một danh sách các số và một ngưỡng
result = has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
# In kết quả
print(result)  # Kết quả: True
```

Trong ví dụ này, danh sách các số '[1.0, 2.8, 3.0, 4.0, 5.0, 2.0]' có hai số gần nhau hơn ngưỡng 0.3. Hàm trả về 'True' vì ít nhất một cặp số gần nhau hơn ngưỡng.

--- TRANG 17 ---

Phản hồi ChatGPT

Mã bạn cung cấp định nghĩa một hàm gọi là has_close_elements nhận hai đối số: numbers, đây là một danh sách các số float, và threshold, đây là một float đại diện cho sự khác biệt tối đa cho phép giữa hai số. Hàm kiểm tra xem có hai số nào trong danh sách numbers gần nhau hơn ngưỡng đã cho hay không. Nó trả về True nếu cặp như vậy tồn tại, và False nếu không. Đây là một triển khai của hàm has_close_elements dựa trên mã bạn cung cấp:

```python
from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False
```

Triển khai này sử dụng các vòng lặp lồng nhau để so sánh từng cặp số trong danh sách numbers. Hàm abs() được sử dụng để tính sự khác biệt tuyệt đối giữa hai số, và nếu sự khác biệt này nhỏ hơn ngưỡng, hàm trả về True. Nếu không tìm thấy cặp như vậy sau khi kiểm tra tất cả các kết hợp, hàm trả về False. Bạn có thể sử dụng hàm này để kiểm tra xem có phần tử nào gần nhau trong danh sách các số như được thể hiện trong các ví dụ bạn cung cấp.

HumanEval Benchmark cho các phản hồi này

Đối với các bài kiểm tra HumanEval, cả hai câu trả lời của Safurai-001 và ChatGPT đều đúng theo cách giống nhau.

Benchmark Đánh giá MultiParameters dựa trên GPT4 cho các phản hồi này

Mô hình đầu tiên là Safurai-001, mô hình thứ hai là ChatGPT.

Tính Đúng đắn của Mã

Xếp hạng câu trả lời mô hình đầu tiên: 95 Mã được đưa ra ở đây hoàn chỉnh và xử lý vấn đề tốt bằng cách kiểm tra mọi cặp số về sự khác biệt nhỏ hơn hoặc bằng ngưỡng đã cho. Nó đưa ra giải thích đầy đủ về logic và hoạt động của mã, và tạo ra sự phân biệt rõ ràng các trường hợp khi nó nên trả về 'True' hoặc 'False'. Tuy nhiên, chỉ dẫn có thể rõ ràng hơn để hàm trả về 'True' nếu sự khác biệt nhỏ hơn ngưỡng, không phải bằng hoặc nhỏ hơn.

Xếp hạng câu trả lời mô hình thứ hai: 95 Câu trả lời này cũng cung cấp mã hoàn chỉnh giải quyết vấn đề, xem xét tất cả các cặp số có thể trong danh sách đã cho và kiểm tra xem sự khác biệt có nhỏ hơn ngưỡng được cung cấp hay không. Nó cũng sử dụng giải thích tốt để làm rõ cách mã hoạt động. Tuy nhiên, mô hình thứ hai không phân biệt trong việc kiểm tra điều kiện rõ ràng như câu trả lời đầu tiên. Và giống như câu trả lời đầu tiên, nó cũng coi bằng với ngưỡng là điều kiện hợp lệ.

Hiệu quả Mã

Xếp hạng câu trả lời mô hình đầu tiên: 90/100. Thuật toán được đề xuất thực hiện một cách tiếp cận đơn giản để giải quyết vấn đề với độ phức tạp thời gian O(n²), với n là độ dài của danh sách đầu vào. Đối với mỗi phần tử i trong danh sách, nó so sánh nó với mọi phần tử j khác trong danh sách để xem liệu giá trị tuyệt đối của sự khác biệt của chúng có nhỏ hơn hoặc bằng ngưỡng hay không. Bởi vì mô hình này chỉ tập trung vào so sánh theo cặp mà không có bất kỳ tối ưu hóa nào (như sắp xếp hoặc giảm không gian tìm kiếm), nó có thể không phải là giải pháp hiệu quả nhất khi kích thước đầu vào tăng, do đó có một khoản khấu trừ nhỏ trong xếp hạng.

Xếp hạng câu trả lời mô hình thứ hai: 90/100. So sánh, mô hình thứ hai cũng tạo ra một giải pháp O(n²) đúng, sử dụng hai vòng lặp lồng nhau để so sánh mọi cặp giá trị duy nhất trong danh sách được cung cấp. Tuy nhiên, tương tự như mô hình đầu tiên, nó trình bày một thuật toán có thể không hoạt động tốt cho danh sách rất dài, và nó không cung cấp cách để tối ưu hóa hoặc mở rộng hoạt động. Do đó, nó cũng không nhận được xếp hạng đầy đủ. Cả hai đều hoạt động tương tự về hiệu quả mã.

--- TRANG 18 ---

Khả năng Đọc Mã

Câu trả lời mô hình đầu tiên: Xếp hạng: 95 Câu trả lời giải thích một cách chi tiết về vấn đề và giải pháp của nó. Nó mô tả rõ ràng cách hàm hoạt động, và việc phân tích từng bước của mã có lợi trong việc hiểu thuật toán. Ví dụ được cung cấp ở cuối cũng rất hữu ích trong việc chứng minh chức năng của nó.

Câu trả lời mô hình thứ hai: Xếp hạng: 80 Câu trả lời này cũng đưa ra giải thích rõ ràng về giải pháp và hàm. Tuy nhiên, nó thiếu phân tích sâu về mã và không cung cấp ví dụ về cách sử dụng hàm, điều này có thể hữu ích để chứng minh chức năng của nó.

Sự Phù hợp Câu hỏi

Xếp hạng câu trả lời mô hình đầu tiên: 95/100 Phản hồi này không chỉ cung cấp việc triển khai đúng của hàm, mà còn giải thích giải pháp một cách chi tiết. Nó bao gồm toàn diện cách hàm hoạt động, bao gồm giải thích các phần khác nhau của nó như hai vòng lặp lồng nhau, sử dụng hàm abs() và các điều kiện if. Ví dụ sử dụng là một phần thưởng bổ sung. Tôi đã trừ một vài điểm vì mô hình không nói về độ phức tạp thời gian của giải pháp này và cũng không cung cấp đoạn mã hoàn chỉnh đang import các module cần thiết từ module typing.

Xếp hạng câu trả lời mô hình thứ hai: 85/100 Phản hồi này cũng cung cấp việc triển khai mã đúng với giải thích ngắn gọn. Mặc dù nó giải thích hoạt động của hàm một cách hiệu quả, nó không chi tiết bằng câu trả lời của mô hình đầu tiên. Phản hồi này cũng thiếu thông tin về cách sử dụng hàm và thảo luận về độ phức tạp thời gian của giải pháp. Quan trọng, nó không trình bày lại ví dụ của câu hỏi thành mã cho mục đích chứng minh, điều này có thể rất thông tin cho những người dùng đang học hoặc mới với Python. Do đó, tôi đã xếp hạng nó thấp hơn một chút.

Bảng 3: Kết quả của GPT4-based MultiParameters HumanEval cho Vấn đề HumanEval 0

                  Tính Đúng    Hiệu quả   Khả năng   Sự Phù hợp
                  của Mã       Mã         Đọc Mã     Câu hỏi
Safurai-001       95          90         95         95
ChatGPT           85          90         80         85

Mặc dù hai phản hồi khá tương tự, như đã trình bày trước đó, Đánh giá MultiParameters dựa trên GPT4 có thể xác định những chi tiết nhỏ và sự khác biệt mà HumanEval sẽ không thể xác định được do tính chất định lượng của nó.

5 KẾT LUẬN

Nghiên cứu này tiết lộ những hiểu biết sâu sắc về cách việc sử dụng biến đổi dữ liệu có thể dẫn đến việc tạo ra các bộ dữ liệu phức tạp, chất lượng cao. Theo lập trường của nhóm Orca (Gunesekar et al. [2023]), chúng tôi nhấn mạnh tầm quan trọng sâu sắc của các bộ dữ liệu được chọn để tinh chỉnh. Thông qua việc sử dụng các kỹ thuật biến đổi dữ liệu sáng tạo và kỹ thuật prompt, những bộ dữ liệu này có thể tăng cường một cách quan trọng hiệu quả của các mô hình nền tảng. Điều này, đến lượt nó, có thể dẫn đến những tăng cường đáng kể trong các benchmark cổ điển.

Nghiên cứu của chúng tôi đã trình bày Safurai-001 như một thay thế mạnh mẽ cho WizardCoder [Xu et al. (2023)] cho các cuộc hội thoại nhiều lượt, khéo léo quản lý các tương tác người dùng và cung cấp hỗ trợ khi cần thiết. Chúng tôi cũng trình bày một tiến bộ riêng biệt trong lĩnh vực công cụ đánh giá LLM Lập trình, Benchmark Đánh giá MultiParameter dựa trên GPT4, vượt ra ngoài các thước đo thuần túy định lượng để cung cấp các chỉ số định tính hơn. Phương pháp mới này cung cấp một phương tiện đánh giá phong phú hơn, tinh tế hơn có thể khám phá những hiểu biết ngoài tầm với của các tiêu chuẩn truyền thống như HumanEval²⁰ và MultiPL-E²¹.

--- TRANG 19 ---

Dựa trên những phát hiện này, chúng tôi hướng đến việc định hình diễn ngôn và đổi mới tương lai trong lĩnh vực này, nhấn mạnh tầm quan trọng tối cao của các công cụ đánh giá mạnh mẽ và việc sử dụng tối đa các bộ dữ liệu trong việc tối ưu hóa hiệu suất mô hình ngôn ngữ.

5.1 SO SÁNH KẾT QUẢ SAFURAI-001 VÀ WIZARD CODER

Câu hỏi
Làm thế nào tôi có thể viết một hàm Python nhận một danh sách các số nguyên làm đầu vào và trả về một tuple đại diện cho tổng và tích của tất cả các số nguyên trong danh sách đó?

Phản hồi WizardCoder
Chúng ta có thể sử dụng hàm 'sum()' có sẵn trong Python để lấy tổng của tất cả các số nguyên trong danh sách. Chúng ta cũng có thể sử dụng một biến được khởi tạo là 1 cho tích và sau đó nhân nó với mỗi số nguyên trong danh sách. Đây là giải pháp:

```python
from typing import List, Tuple

def sum_product(numbers):
    total_sum = sum(numbers) or 0  # nếu numbers rỗng, đặt total_sum thành 0
    total_product = 1
    for num in numbers:
        total_product *= num
    return (total_sum, total_product)
```

Đây là cách hàm hoạt động từng bước:

1. Chúng ta khởi tạo 'total_sum' là tổng của tất cả các số nguyên trong danh sách bằng cách sử dụng hàm 'sum()' có sẵn hoặc đặt nó thành 0 nếu danh sách rỗng bằng cách sử dụng đánh giá ngắn mạch ('or').

2. Chúng ta cũng khởi tạo biến 'total_product' là 1 và sau đó nhân mỗi số nguyên trong danh sách với biến này.

3. Cuối cùng, chúng ta trả về một tuple của '(total_sum, total_product)'.

--- TRANG 20 ---

Phản hồi Safurai-001

Trước tiên, chúng ta cần hiểu danh sách là gì. Danh sách là một tập hợp các mục, giống như danh sách mua sắm. Trong trường hợp này, danh sách của chúng ta được tạo thành từ các số.

Tiếp theo, chúng ta cần hiểu tuple là gì. Tuple giống như một danh sách, nhưng một khi nó được tạo ra, chúng ta không thể thay đổi nó. Nó giống như một chiếc hộp bị khóa mà chúng ta không thể mở.

Bây giờ, hãy nhìn vào vấn đề. Chúng ta cần viết một hàm nhận một danh sách các số và trả về một tuple với tổng và tích của những số đó. Đây là mã:

```python
from typing import List, Tuple

def sum_product(numbers: List[int]) -> Tuple[int, int]:
    """
    For a given list of integers, return a tuple consisting of a sum and a product of all the
    integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    """
    # Nếu danh sách rỗng, trả về một tuple với tổng là 0 và tích là 1
    if not numbers:
        return 0, 1
    
    # Nếu không, tính tổng và tích
    sum = 0
    product = 1
    for num in numbers:
        sum += num
        product *= num
    
    # Trả về tuple với tổng và tích
    return sum, product
```

Hãy phân tích nó:

• 'def sum_product(numbers: List[int]) -> Tuple[int, int]:' - Dòng này định nghĩa hàm của chúng ta và cho Python biết rằng nó nhận một danh sách các số nguyên làm đầu vào và trả về một tuple với hai số nguyên (tổng và tích).

• 'if not numbers:' - Dòng này kiểm tra xem danh sách có rỗng hay không. Nếu có, chúng ta trả về một tuple với tổng là 0 và tích là 1.

• 'sum = 0' và 'product = 1' - Những dòng này khởi tạo các biến cho tổng và tích.

• 'for num in numbers:' - Dòng này lặp qua từng số trong danh sách.

• 'sum += num' và 'product *= num' - Những dòng này thêm số hiện tại vào tổng và nhân nó với tích hiện tại.

• 'return sum, product' - Dòng này trả về tuple với tổng và tích.

Vì vậy, nếu chúng ta gọi hàm của chúng ta với danh sách '[1, 2, 3, 4]', nó sẽ trả về tuple '(10, 24)'. Tổng là 10 (1 + 2 + 3 + 4) và tích là 24 (1 x 2 x 3 x 4).

Tôi hy vọng điều đó có ý nghĩa! Hãy cho tôi biết nếu bạn có bất kỳ câu hỏi nào.

--- TRANG 21 ---

LỜI CẢM ƠN

TÀI LIỆU THAM KHẢO

[Các tài liệu tham khảo được liệt kê ở cuối tài liệu gốc]
