# 2405.01535.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/evaluation/2405.01535.pdf
# Kích thước file: 2028733 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
PROMETHEUS 2: Một Mô hình Ngôn ngữ Nguồn mở Chuyên biệt trong
Đánh giá các Mô hình Ngôn ngữ Khác
Seungone Kim1,2,3∗Juyoung Suk1∗Shayne Longpre4Bill Yuchen Lin5Jamin Shin1
Sean Welleck3Graham Neubig3Moontae Lee2,6Kyungjae Lee2Minjoon Seo1
KAIST AI1LG AI Research2Carnegie Mellon University3MIT4
Allen Institute for AI5University of Illinois Chicago6
seungone@cmu.edu {juyoung, minjoon}@kaist.ac.kr
Tóm tắt
Các LM độc quyền như GPT-4 thường được
sử dụng để đánh giá chất lượng phản hồi từ
các LM khác nhau. Tuy nhiên, những mối quan
tâm bao gồm tính minh bạch, khả năng kiểm soát
và khả năng chi trả thúc đẩy mạnh mẽ việc phát
triển các LM nguồn mở chuyên biệt trong đánh
giá. Mặt khác, các LM đánh giá nguồn mở hiện
tại thể hiện những thiếu sót nghiêm trọng: 1) chúng
đưa ra điểm số khác biệt đáng kể so với những
điểm số được gán bởi con người, và 2) chúng thiếu
tính linh hoạt để thực hiện cả đánh giá trực tiếp
và xếp hạng theo cặp, hai hình thức đánh giá phổ
biến nhất. Ngoài ra, chúng thường không có khả
năng đánh giá dựa trên tiêu chí đánh giá tùy chỉnh,
thay vào đó tập trung vào các thuộc tính tổng quát
như tính hữu ích và tính vô hại. Để giải quyết
những vấn đề này, chúng tôi giới thiệu Prometheus
2. Prometheus 2 mạnh mẽ hơn người tiền nhiệm
và phản ánh chặt chẽ các đánh giá của con người
và GPT-4. Hơn nữa, nó có khả năng xử lý cả
định dạng đánh giá trực tiếp và xếp hạng theo
cặp được nhóm với tiêu chí đánh giá do người
dùng định nghĩa. Trên bốn benchmark đánh giá
trực tiếp và bốn benchmark xếp hạng theo cặp,
PROMETHEUS 2 đạt được tương quan và sự
thống nhất cao nhất với con người và các thẩm
phán LM độc quyền trong số tất cả các LM đánh
giá nguồn mở được kiểm tra. Các mô hình, mã
và dữ liệu của chúng tôi đều có sẵn công khai.1
1 Giới thiệu
Việc đánh giá chất lượng đầu ra được tạo ra bởi
các mô hình ngôn ngữ (LM) đang trở nên khó
khăn một cách tiến bộ, khi các đầu ra bao phủ
một phân phối văn bản cực kỳ đa dạng và các
tác vụ phức tạp. Để giải quyết vấn đề này, đánh
giá dựa trên mô hình ngôn ngữ đã nổi lên như
một mô hình có thể mở rộng và rẻ để đánh giá
văn bản được tạo ra bởi LM (Li et al., 2024;
∗đóng góp ngang nhau. Công việc được thực hiện khi Seungone
là thực tập sinh tại LG AI Research và là sinh viên thạc sĩ tại KAIST.
1https://github.com/prometheus-eval/prometheus-eval
Điểm số
Con người
Điểm số
GPT-4
Điểm số
Claude-3
-Opus
Điểm số
Prometheus
Điểm số
GPT-3.5
Điểm số
Llama-2-70B
Điểm số
Nhóm Đánh giá
Mạnh
Nhóm Đánh giá
Yếu
Điểm số
Prometheus 2
Tương quan
Cao
Tương quan
Thấp
Hình 1: Các đánh giá yếu (ví dụ: Llama-2-Chat-70B,
Prometheus và GPT-3.5-Turbo) đạt được tương quan
điểm số thấp với các đánh giá mạnh (ví dụ: Con người,
GPT-4 và Claude-3-Opus). Mặt khác, điểm số được
cung cấp bởi các đánh giá mạnh có tương quan cao với
nhau.
Gao et al., 2024). Trong mô hình này, các LM được
yêu cầu đưa ra một chỉ số vô hướng về chất lượng
(được gọi là đánh giá trực tiếp) (Zheng et al.,
2023; Liu et al., 2023b; Ye et al., 2023; Kim et al.,
2023) hoặc xác định đầu ra nào trong hai đầu ra
được ưa thích (được gọi là xếp hạng theo cặp) (Wang
et al., 2023b; Li et al., 2023b; Lambert et al., 2024).
Các công trình trước đó sử dụng các LM độc quyền
làm đánh giá đã chứng minh không chỉ có tương
quan cao với đánh giá của con người mà còn tăng
tốc độ và hiệu quả về chi phí (Zheng et al., 2023;
Liu et al., 2023b; Dubois et al., 2023; Ye et al., 2023).
Tuy nhiên, việc dựa vào các LM độc quyền để đánh
giá đặt ra những thách thức đáng kể. Việc thiếu minh
bạch về dữ liệu đào tạo của chúng làm tổn hại cả
tính công bằng và khả năng tái sản xuất, khiến việc
sử dụng chúng trong các quy trình đánh giá trở nên
có vấn đề. Ngoài ra, những lo ngại về khả năng kiểm
soát và khả năng chi trả cũng tồn tại (Kim et al.,
2023). Để giải quyết những vấn đề này, các công trình
gần đây đã tập trung vào việc phát triển các LM đánh
giá nguồn mở, minh bạch và có thể kiểm soát (Kim
et al., 2023; Wang et al., 2023a,b; Li et al., 2023a;
Zhu et al., 2023; Jiang et al., 2023b,c; Lee et al.,
2024). Tuy nhiên, các mô hình này thường đưa ra
quyết định điểm số không tương quan đủ tốt với
đánh giá của con người hoặc những đánh giá được
thực hiện

--- TRANG 2 ---
bởi các LM độc quyền, không thể mô phỏng hiệu quả
chúng. Hơn nữa, các LM đánh giá nguồn mở không
linh hoạt vì chúng thường chỉ được huấn luyện để
thực hiện đánh giá trực tiếp hoặc xếp hạng theo cặp
và đánh giá dựa trên sở thích công chúng chung như
tính hữu ích và tính vô hại, hạn chế khả năng xử lý
các tình huống đời thực đa dạng.
Để thu hẹp khoảng cách với các LM độc quyền,
chúng tôi nghiên cứu việc thống nhất hai mô hình
đánh giá dựa trên mô hình - đánh giá trực tiếp và
xếp hạng theo cặp - để huấn luyện một LM đánh
giá thống nhất mạnh mẽ. Chúng tôi đề xuất một
công thức dựa trên việc hợp nhất trọng số của hai
LM đánh giá được huấn luyện riêng biệt trên định
dạng đánh giá trực tiếp và xếp hạng theo cặp. Quan
sát thực nghiệm chính của chúng tôi là việc hợp
nhất trọng số có thể tạo ra một LM đánh giá không
chỉ hoạt động trong cả hai định dạng mà còn vượt
trội hơn các LM đánh giá được huấn luyện chung
hoặc chỉ được huấn luyện trên một định dạng duy
nhất.
Để chứng minh phương pháp của chúng tôi, chúng
tôi phát triển BỘ SƯU TẬP SỞ THÍCH, một bộ
dữ liệu phản hồi xếp hạng theo cặp chi tiết mới
được xây dựng dựa trên BỘ SƯU TẬP PHẢN HỒI
(Kim et al., 2023), đây là một bộ dữ liệu phản hồi
đánh giá trực tiếp. Chúng tôi chọn Mistral-7B
(Jiang et al., 2023a) và Mixtral-8x7B (Jiang et al.,
2024) làm mô hình cơ sở và hợp nhất trọng số của
các LM đánh giá được huấn luyện riêng biệt trên
BỘ SƯU TẬP PHẢN HỒI và BỘ SƯU TẬP SỞ
THÍCH để có được các mô hình kết quả của chúng
tôi, PROMETHEUS 2 (7B & 8x7B).
Trên bốn benchmark đánh giá trực tiếp (Vicuna
Bench, MT Bench, FLASK, Feedback Bench), các
mô hình PROMETHEUS 2 chứng minh tương quan
cao nhất với cả đánh giá của con người và các thẩm
phán dựa trên LM độc quyền so với các LM đánh
giá nguồn mở hiện có, với tương quan Pearson
vượt trội hơn các baseline khác 0,2 điểm trên tất
cả các bộ dữ liệu. Tương tự, trên bốn benchmark
xếp hạng theo cặp (HHH Alignment, MT Bench
Human Judgment, Auto-J Eval, Preference Bench),
các mô hình PROMETHEUS 2 cho thấy sự thống
nhất cao nhất với đánh giá của con người trong số
tất cả các LM đánh giá nguồn mở mà chúng tôi đã
kiểm tra, giảm một nửa khoảng cách hiệu suất với
GPT-4.
Đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi giới thiệu PROMETHEUS 2 (7B &
8x7B), các LM đánh giá nguồn mở tiên tiến
đạt tương quan cao với cả đánh giá của con
người và các thẩm phán dựa trên LM độc
quyền trong cả đánh giá trực tiếp và xếp hạng
theo cặp.
• Chúng tôi giới thiệu một bộ dữ liệu phản hồi
xếp hạng theo cặp gọi là BỘ SƯU TẬP SỞ
THÍCH, bao gồm 1K tiêu chí đánh giá tùy
chỉnh vượt ra ngoài tính hữu ích và tính vô
hại.
• Chúng tôi chỉ ra rằng việc hợp nhất trọng số
của các LM đánh giá được huấn luyện trên
bộ dữ liệu phản hồi đánh giá trực tiếp và xếp
hạng theo cặp tạo ra một LM đánh giá thống
nhất xuất sắc trong cả hai phương án.
2 Công việc liên quan
2.1 Đánh giá dựa trên Mô hình Ngôn ngữ
Để đánh giá khả năng tạo sinh của các LM, các
công trình trước đây như benchmark GEM (Gehrmann
et al., 2021, 2022) đã sử dụng ROUGE (Lin,
2004), BLEU (Papineni et al., 2002), và
BERTScore (Zhang et al., 2019) làm chỉ số của
chúng, đo lường sự tương tự về từ vựng hoặc ngữ
nghĩa giữa một câu trả lời tham chiếu và một phản
hồi. Tuy nhiên, những chỉ số thông thường này dễ
bị âm tính giả vì chúng không đủ biểu đạt để nhận
biết các phản hồi có chất lượng tốt nhưng khác
với câu trả lời tham chiếu (Schluter, 2017; Freitag
et al., 2020; Hanna and Bojar, 2021).
Gần đây, việc sử dụng mô hình ngôn ngữ làm
thẩm phán đã thu hút sự chú ý như một mô hình
đầy hứa hẹn để mô phỏng độ sâu và độ chi tiết
mà đánh giá con người mang lại (Zheng et al.,
2023; Liu et al., 2023b; Li et al., 2023b; Chan et
al., 2023; Ye et al., 2023). Để giảm sự phụ thuộc
quá mức vào các LM độc quyền, các công trình
tiếp theo đề xuất huấn luyện các mô hình ngôn
ngữ chuyên biệt trong đánh giá (Cui et al., 2023;
Kim et al., 2023; Jiang et al., 2023b,c; Li et al.,
2023a; Lee et al., 2024). Tuy nhiên, các LM đánh
giá nguồn mở không có tính linh hoạt để hoạt
động trong các chương trình đánh giá khác nhau
và cho thấy hiệu suất đánh giá yếu so với các LM
độc quyền. Chúng tôi nhằm mục đích thu hẹp
khoảng cách này bằng cách giới thiệu PROMETHEUS
2.
2.2 Hợp nhất Trọng số
Các công trình trước đây đã chứng minh rằng việc
hợp nhất trọng số có thể tăng cường hiệu suất
trên nhiều lĩnh vực khác nhau, bao gồm mô hình
ngôn ngữ (Li et al., 2022; Matena and Raffel,
2022; Ilharco et al., 2022; Don-Yehiya et al.,
2022; Gururangan et al., 2023; Yadav et al., 2024;
Sukhbaatar et al., 2024), điều chỉnh hướng dẫn
(Jang et al., 2023b; Yu et al., 2023), và căn chỉnh
theo sở thích người dùng (Jang et al., 2023a;
Rame et al., 2024; Wang et al., 2024). Trong
công trình của chúng tôi, chúng tôi đặc biệt tập
trung vào việc nâng cao khả năng đánh giá của
các LM đánh giá nguồn mở. Bằng cách hợp nhất
các mô hình được huấn luyện trên các định dạng
đánh giá khác nhau—cụ thể là đánh giá trực tiếp
và xếp hạng theo cặp—chúng tôi nhằm mục đích
có được một LM đánh giá không chỉ hoạt động
trong cả hai định dạng mà còn cho thấy hiệu suất
đánh giá tốt như các LM độc quyền.

--- TRANG 3 ---
xếp hạng—chúng tôi nhằm mục đích có được một
LM đánh giá không chỉ hoạt động trong cả hai
định dạng mà còn cho thấy hiệu suất đánh giá tốt
như các LM độc quyền.
3 Phương pháp học
Chúng tôi đề xuất một công thức mới để huấn
luyện một LM đánh giá thống nhất dựa trên việc
hợp nhất trọng số của các mô hình được huấn
luyện cho đánh giá trực tiếp và xếp hạng theo
cặp. Chúng tôi bắt đầu với kiến thức nền tảng về
đánh giá trực tiếp và xếp hạng theo cặp cho các
LM đánh giá (Mục 3.1, 3.2), tiếp theo là quy trình
xây dựng dữ liệu huấn luyện của chúng tôi (Mục
3.3). Cuối cùng, chúng tôi trình bày các phương
pháp để huấn luyện các LM đánh giá tiên tiến,
mô hình Prometheus 2 (Mục 3.4).
3.1 Đánh giá Trực tiếp
Đánh giá trực tiếp là ánh xạ một hướng dẫn i và
phản hồi r thành một giá trị vô hướng điểm s, như
fdirect : (i, r) → s trong đó s ∈ R. Đối với phạm vi
điểm số, chúng tôi sử dụng số nguyên từ 1 đến 5.
Các công trình trước đây đã xác định một số
công thức để căn chỉnh điểm số được cung cấp
bởi các LM đánh giá (sLM) và điểm số được gán
bởi con người (shuman). Ví dụ, Liu et al. (2023a)
và Zheng et al. (2023) đã chỉ ra rằng việc thêm
một câu trả lời tham chiếu a làm đầu vào cho LM
đánh giá là rất quan trọng để tối đa hóa tương
quan giữa sLM và shuman. Ngoài ra, Zheng et al.
(2023) và Ye et al. (2023) đã cho thấy rằng việc
yêu cầu mô hình ngôn ngữ viết phản hồi bằng
lời vr trước s cũng cải thiện tương quan giữa sLM
và shuman. Cuối cùng, Ye et al. (2023) và Kim et
al. (2023) đã chỉ ra rằng bằng cách tích hợp rõ
ràng tiêu chí đánh giá e, người dùng có thể định
nghĩa các tiêu chuẩn đánh giá mô hình, đảm bảo
các đánh giá linh hoạt theo nhu cầu cụ thể thay
vì chất lượng chung. Cụ thể, e được biểu diễn
như một rubric điểm số bao gồm mô tả cho chính
tiêu chí đó và một tập hợp các mô tả cho mỗi
điểm giữa phạm vi điểm số. Điều này được biểu
thị như:
fdirect : (i, r, a, e) → (vr, s)
trong đó s ∈ {1, 2, 3, 4, 5} (1)
3.2 Xếp hạng theo Cặp
Xếp hạng theo cặp là ánh xạ một hướng dẫn i và
hai cặp phản hồi (rm, rn) thành i hoặc j, như
fpair : (i, rm, rn) → s trong đó s ∈ {m, n}.
Tương tự như đánh giá trực tiếp, các công trình
trước đây đã xác định rằng việc tích hợp một câu
trả lời tham chiếu a và phản hồi bằng lời vrm,rn
vào quy trình đánh giá là quan trọng (Zheng et
al., 2023; Li et al., 2023b,a). Ngoài ra, để hỗ trợ
đánh giá chi tiết dưới tiêu chí tùy chỉnh, chúng
tôi thêm tiêu chí đánh giá e làm đầu vào cho LM
đánh giá (Ye et al., 2023; Kim et al., 2023). Theo
hiểu biết tốt nhất của chúng tôi, chúng tôi là
những người đầu tiên nghiên cứu việc đánh giá
chi tiết như vậy trong cài đặt xếp hạng theo cặp.
Điều này được biểu thị như:
fpair : (i, rm, rn, a, e) → (vrm,rn, s)
trong đó s ∈ {m, n} (2)
Trong xếp hạng theo cặp, tiêu chí đánh giá e
không bao gồm một tập hợp mô tả cho mỗi điểm;
thay vào đó, chỉ có mô tả của bản thân tiêu chí
đánh giá. Ngoài ra, cần lưu ý rằng phản hồi bằng
lời vrm,rn so sánh những điểm chung và khác biệt
giữa rm và rn liên quan đến e.
3.3 Bộ Sưu tập Sở thích
Các bộ dữ liệu xếp hạng theo cặp phổ biến như
HH-RLHF (Bai et al., 2022) hoặc Ultra Feedback
(Cui et al., 2023) không bao gồm tiêu chí đánh
giá e và phản hồi bằng lời vrm,rn. Để huấn luyện
một LM đánh giá có thể đánh giá dựa trên các
tiêu chí như vậy, chúng tôi xây dựng BỘ SƯU
TẬP SỞ THÍCH, bao gồm 1K tiêu chí đánh giá.
Chúng tôi áp dụng hai thay đổi đối với BỘ SƯU
TẬP PHẢN HỒI. Đầu tiên, vì BỘ SƯU TẬP PHẢN
HỒI bao gồm năm phản hồi cho mỗi hướng dẫn,
mỗi phản hồi tương ứng với một quyết định điểm
số từ 1 đến 5, chúng tôi ghép đôi hai trong số
năm phản hồi, tạo ra tổng cộng mười tổ hợp trên
mỗi hướng dẫn. Sử dụng các quyết định điểm số
hiện có cho mỗi phản hồi, chúng tôi xác định
phản hồi nào tốt hơn và gán một quyết định điểm
số mới cho cặp đó (tức là "Phản hồi A tốt hơn"
hoặc "Phản hồi B tốt hơn"). Thứ hai, để tạo ra
phản hồi bằng lời vrm,rn mới cho mỗi cặp phản
hồi, chúng tôi yêu cầu GPT-4-1106 xác định
những điểm chung và khác biệt giữa hai phản hồi.

--- TRANG 4 ---
Dữ liệu                BỘ SƯU TẬP SỞ THÍCH  BỘ SƯU TẬP PHẢN HỒI
Chương trình Đánh giá  Xếp hạng theo Cặp      Đánh giá Trực tiếp
# Tiêu chí Đánh giá    1,000                   1,000
# Hướng dẫn            20,000                  20,000
# Câu trả lời Tham chiếu 20,000               20,000
# Trường hợp           200,000                 100,000
# Phản hồi Bằng lời   200,000                 100,000

Bảng 1: Thống kê của các bộ dữ liệu huấn luyện của chúng tôi, BỘ SƯU TẬP PHẢN HỒI và BỘ SƯU TẬP SỞ THÍCH. Lưu ý rằng 1K tiêu chí đánh giá, 20K hướng dẫn và 20K câu trả lời tham chiếu được chia sẻ giữa hai bộ dữ liệu. Cả hai bộ dữ liệu đều có số lượng quyết định điểm số bằng nhau ("A" hoặc "B"; 100K mỗi cái & 1-5; 20K mỗi cái) để ngăn chặn thiên vị không mong muốn sau khi huấn luyện.

Phản hồi có sử dụng chính xác thuật ngữ ngành và biệt ngữ cụ thể không?

Xếp hạng theo Cặp
Cả hai phản hồi đều cố gắng truyền tải khái niệm cơ bản về containerization, nhưng với mức độ rõ ràng và chi tiết kỹ thuật khác nhau. Phản hồi A tiếp cận khái niệm bằng cách sử dụng ẩn dụ 'đặt mọi thứ vào hộp', mặc dù dễ hiểu nhưng thiếu độ chính xác và đặc trưng ngành [...] 

Mặt khác, Phản hồi B sử dụng biệt ngữ kỹ thuật hiệu quả hơn, như 'đóng gói', 'tệp cấu hình', 'thư viện' và 'phụ thuộc'.

Có thể kết luận rằng Phản hồi B tốt hơn Phản hồi A.

Phản hồi có sử dụng ngôn ngữ đơn giản và giải thích dễ hiểu cho người mới bắt đầu không?

Đánh giá Trực tiếp
Phản hồi sử dụng hiệu quả ngôn ngữ đơn giản và dễ tiếp cận để giải thích containerization và Docker, điều này rất tốt cho người mới bắt đầu. Phép tương tự về việc đặt mọi thứ vào hộp đặc biệt hữu ích vì nó minh họa trực quan khái niệm [...]

Tuy nhiên, phản hồi có thể được cải thiện bằng cách đề cập ngắn gọn lý do tại sao containerization lại quan trọng, chẳng hạn như lợi ích của nó trong việc đảm bảo rằng phần mềm chạy nhất quán trên các môi trường tính toán khác nhau. Nó mất điểm vì không hoàn toàn giải quyết ý nghĩa của containerization trong bối cảnh rộng hơn của phát triển phần mềm, điều này có thể cung cấp thông tin có giá trị cho người đọc.

Phản hồi Bằng lời
Quyết định Điểm số

Phản hồi A: Containerization trong phần mềm tương tự như việc đặt các vật phẩm vào hộp. Nó bao gồm việc đóng gói phần mềm của bạn và tất cả các thành phần của nó vào một container. Docker là công cụ hỗ trợ quá trình này. Nó giúp đóng gói phần mềm vào container và đơn giản hóa việc sử dụng chúng. [...]

Phản hồi B: Containerization trong phát triển phần mềm đề cập đến quá trình đóng gói một ứng dụng cùng với tất cả các tệp cấu hình liên quan, thư viện và phụ thuộc cần thiết để chạy, thành một đơn vị độc lập hoặc 'container'. [...]

Tiêu chí Đánh giá
Hướng dẫn: Ý nghĩa và tầm quan trọng của 'Containerization' trong phát triển phần mềm là gì, và Docker đóng vai trò gì trong đó?

Hình 2: So sánh đánh giá trực tiếp và xếp hạng theo cặp. Cả hai phản hồi đều có thể được coi là tốt dưới khái niệm 'tính hữu ích'. Tuy nhiên, quyết định điểm số có thể thay đổi dựa trên một tiêu chí đánh giá cụ thể.

Thống kê của bộ dữ liệu kết quả được liệt kê trong Bảng 1 cùng với BỘ SƯU TẬP PHẢN HỒI. Chúng tôi giải thích về quy trình xác minh chất lượng của BỘ SƯU TẬP SỞ THÍCH trong Phụ lục A. Ngoài ra, chúng tôi bao gồm các prompt mà chúng tôi sử dụng cho quy trình tăng cường trong Phụ lục H.

3.4 Phương pháp Huấn luyện & Baseline
Prompting: Prompting bao gồm việc truy vấn một LM để đưa ra đánh giá trong định dạng đánh giá được chỉ định mà không cần huấn luyện. Chúng tôi sử dụng Llama-2-Chat-7,13,70B (Touvron et al., 2023); Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a); và Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024) làm baseline. Cần lưu ý rằng các mô hình không được huấn luyện rõ ràng trên dữ liệu phản hồi thường không thể tạo ra phản hồi đúng định dạng yêu cầu, khiến việc phân tích quyết định điểm số trở nên cực kỳ khó khăn. Mặc dù không thực tế cho việc sử dụng thường xuyên, chúng tôi thực hiện so sánh công bằng bằng cách lặp vô hạn cho đến khi có thể phân tích điểm số. Ngoài ra, chúng tôi bao gồm các LM độc quyền như GPT-3.5-Turbo-0613; GPT-4-1106; và Claude-3-Opus.

Huấn luyện Định dạng Đơn: Huấn luyện Định dạng Đơn bao gồm việc huấn luyện mô hình cơ sở θ trên bộ dữ liệu phản hồi đánh giá trực tiếp Dd hoặc bộ dữ liệu phản hồi xếp hạng theo cặp Dp. Đối với các LM đánh giá được huấn luyện định dạng đơn, chúng tôi kiểm tra Prometheus-7,13B (Kim et al., 2023) (đánh giá trực tiếp); UltraRM-13B (Cui et al., 2023) (xếp hạng theo cặp); và PairRM-0.4B (Jiang et al., 2023c) (xếp hạng theo cặp). Ngoài ra, chúng tôi cũng báo cáo hiệu suất của việc huấn luyện định dạng đơn Mistral-7B-Instruct-v0.2 và Mixtral-8x7B-Instruct-v0.1 trên đánh giá trực tiếp hoặc xếp hạng theo cặp.

Huấn luyện Chung: Huấn luyện Chung bao gồm việc huấn luyện mô hình cơ sở θ trên cả bộ dữ liệu phản hồi đánh giá trực tiếp Dd và bộ dữ liệu phản hồi xếp hạng theo cặp Dp. Điều này cho phép LM đánh giá kết quả hoạt động trên cả hai định dạng đánh giá. Đối với các LM đánh giá được huấn luyện chung, chúng tôi kiểm tra Auto-J (Li et al., 2023a). Ngoài ra, chúng tôi báo cáo hiệu suất của việc huấn luyện chung Mistral-7B và Mixtral-8x7B trên cả đánh giá trực tiếp và xếp hạng theo cặp.

Hợp nhất Trọng số: Hợp nhất Trọng số bao gồm việc huấn luyện hai mô hình, θd và θp, riêng biệt trên bộ dữ liệu phản hồi đánh giá trực tiếp Dd và bộ dữ liệu phản hồi xếp hạng theo cặp Dp. Sau đó, LM đánh giá cuối cùng θfinal được có được bằng cách hợp nhất θd và θp. Ví dụ, hợp nhất tuyến tính như sau:

θfinal = α × θd + (1 − α) × θp (3)

Ngoài hợp nhất tuyến tính, chúng tôi kiểm tra 5 biến thể bổ sung, đó là hợp nhất Task Arithmetic (Ilharco et al., 2022), hợp nhất TIES (Yadav et al., 2024), hợp nhất DARE-TIES và DARE-Linear (Yu et al., 2023), và hợp nhất SLERP (Goddard et al., 2024). Chúng tôi bao gồm giải thích về các phương pháp hợp nhất này và kết quả thí nghiệm ablation về sự khác biệt hiệu suất trong Phụ lục G. Trong số đó,

--- TRANG 5 ---
Phương pháp Đánh giá   Benchmark      Chỉ số      Nguồn Đánh giá      Câu trả lời Tham chiếu   # Rubric Điểm số   # Hướng dẫn   # Đánh giá
Đánh giá Trực tiếp     Vicuna Bench   Tương quan   LMs Độc quyền       C                      80               80             320
                       MT Bench       Tương quan   LMs Độc quyền       C                      80               80             320
                       FLASK          Tương quan   LMs Độc quyền & Con người  C           12               200            2,000
                       Feedback Bench Tương quan   LMs Độc quyền       C                      200              200            1,000
Xếp hạng theo Cặp     HHH Align.     Độ chính xác Người              K                      4                221            221
                       MT Bench Human Judg. Độ chính xác Người     K                      1                80             3,360
                       Auto-J Eval    Độ chính xác Người              K                      1                58             1,392
                       Preference Bench Độ chính xác LMs Độc quyền    C                      200              200            2,000

Bảng 2: Thống kê của các benchmark đánh giá của chúng tôi để đánh giá khả năng đánh giá của các LM đánh giá.

DARE-Linear cho thấy hiệu suất tốt nhất, và do đó chúng tôi đã sử dụng nó để huấn luyện các mô hình PROMETHEUS 2 (7B & 8x7B). Chi tiết về các siêu tham số cho huấn luyện và suy luận cùng với các mẫu prompt đều được liệt kê trong Phụ lục B, I, J.

4 Cài đặt Thí nghiệm
Thống kê của tất cả các benchmark có trong Bảng 2.

Bốn benchmark đánh giá trực tiếp là:
• Vicuna Bench (Chiang et al., 2023): Một benchmark chat một lượt bao gồm 80 prompt kiểm tra, 80 rubric điểm số được tạo thủ công từ Kim et al. (2023), và 320 phản hồi thu được bởi WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613.
• MT Bench (Zheng et al., 2023): Một benchmark chat nhiều lượt bao gồm 80 prompt kiểm tra, 80 rubric điểm số được tạo thủ công từ Kim et al. (2023), và 320 phản hồi thu được bởi WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613.
• FLASK (Ye et al., 2023): Một benchmark đánh giá chi tiết bao gồm 200 prompt kiểm tra, 12 rubric điểm số, và 2000 phản hồi thu được từ Alpaca-7B, Vicuna-13B, Bard, GPT-3.5-Turbo-0613. Ngoài điểm số từ các LM độc quyền, benchmark này cũng bao gồm điểm số được đánh dấu bởi các đánh giá viên con người.
• Feedback Bench (Kim et al., 2023): Tập kiểm tra của BỘ SƯU TẬP PHẢN HỒI với 1K rubric điểm số, 200 hướng dẫn, và 1K phản hồi không trùng lặp với dữ liệu huấn luyện.

Bốn benchmark xếp hạng theo cặp là:
• HHH Alignment (Askell et al., 2021): Một benchmark bao gồm 221 prompt; 4 rubric điểm số (tính hữu ích, tính vô hại, tính trung thực, và khác) và 221 cặp phản hồi (được chấm điểm là 'thắng' hoặc 'thua') được đánh giá bởi các đánh giá viên con người.
• MT Bench Human Judgment (Zheng et al., 2023): Một benchmark chia sẻ 80 prompt giống như MT-Bench. Ngoài ra, nó cung cấp 3,360 cặp phản hồi (được chấm điểm là 'thắng', 'hòa', hoặc 'thua') được đánh giá bởi các đánh giá viên con người.
• Auto-J Eval (Li et al., 2023a): Một benchmark bao gồm 58 prompt và 1,392 cặp phản hồi (được chấm điểm là 'thắng', 'hòa', hoặc 'thua') được đánh giá bởi các đánh giá viên con người. Benchmark này được sử dụng làm tập kiểm tra trong domain cho Auto-J.
• Preference Bench: Tập kiểm tra trong domain của chúng tôi cho các mô hình PROMETHEUS. Tương tự như cách BỘ SƯU TẬP SỞ THÍCH được tạo ra với BỘ SƯU TẬP PHẢN HỒI, chúng tôi điều chỉnh FEEDBACK BENCH và ghép đôi hai trong số năm phản hồi, tạo ra một tập kiểm tra với 200 prompt, 2,000 cặp phản hồi (được chấm điểm là 'thắng' hoặc 'thua'), và 200 tiêu chí đánh giá.

Trong đánh giá trực tiếp, chúng tôi tiến hành đánh giá dựa trên tham chiếu bằng cách thêm câu trả lời tham chiếu làm đầu vào. Chúng tôi sử dụng Pearson, Spearman, và Kendall-Tau làm chỉ số hiệu suất để đo lường tương quan điểm số so với các đánh giá viên tham chiếu. Hơn nữa, chúng tôi bao gồm kết quả của đánh giá trực tiếp không có tham chiếu trong Phụ lục F.

Trong xếp hạng theo cặp, chúng tôi tiến hành đánh giá không có tham chiếu. Dựa trên đánh giá được gán bởi con người, chúng tôi sử dụng độ chính xác làm chỉ số để đo lường sự thống nhất giữa các LM đánh giá và con người. Ngoài ra, MT Bench Human Judgment và tập kiểm tra Auto-J bao gồm tùy chọn 'hòa' được đánh giá bởi các đánh giá viên con người. Chúng tôi đánh giá theo hai cách: bằng cách loại trừ tất cả các tùy chọn 'hòa' cho xếp hạng theo cặp (được ký hiệu là 'w/o tie'), hoặc bằng cách sử dụng đánh giá trực tiếp nơi các phản hồi được chấm điểm là 'hòa' được nhóm lại, và xếp hạng theo cặp được áp dụng cho các phản hồi còn lại có điểm số khác nhau (được ký hiệu là 'w/ tie').

--- TRANG 6 ---
LM Đánh giá                      VICUNA BENCH         MT BENCH           FLASK             Feedback Bench
                           GPT-4-1106  Claude-3-Opus  GPT-4-1106  Claude-3-Opus  GPT-4-1106  Claude-3-Opus  Con người  GPT-4-0613
LLAMA 2-CHAT 7B              0.205        0.243        0.036        0.055        0.317        0.256        0.299     0.523
LLAMA 2-CHAT 13B             0.185        0.141       -0.042       -0.002        0.239        0.247        0.263     0.545
LLAMA 2-CHAT 70B             0.350        0.463        0.178        0.228        0.388        0.402        0.317     0.592
MISTRAL-INSTRUCT-7B          0.486        0.561        0.284        0.396        0.448        0.437        0.377     0.586
MIXTRAL-INSTRUCT-8X7B        0.566        0.579        0.551        0.539        0.483        0.495        0.420     0.673
PROMETHEUS-7B                0.484        0.528        0.378        0.382        0.352        0.331        0.348     0.847
PROMETHEUS-13B               0.492        0.534        0.404        0.477        0.462        0.470        0.449     0.860
AUTO-J (13B)                 0.351        0.262        0.432        0.375        0.430        0.370        0.473     0.637
PROMETHEUS-2-7B              0.666        0.654        0.548        0.517        0.617        0.561        0.545     0.882
PROMETHEUS-2-8X7B            0.685        0.635        0.665        0.614        0.659        0.626        0.555     0.898
GPT-3.5-TURBO-0613          0.335        0.349        0.183        0.194        0.437        0.396        0.450     0.594
GPT-4-1106                     /          0.694          /          0.717          /          0.736        0.679     0.753
CLAUDE-3-OPUS                0.694          /          0.717          /          0.736          /          0.573     0.788

Bảng 3: Kết quả Đánh giá Trực tiếp Tương quan Pearson giữa các đánh giá viên tham chiếu (liệt kê ở trên) và các LM đánh giá. Thống kê tốt nhất có thể so sánh được in đậm và tốt thứ hai được gạch chân ngoại trừ các LM độc quyền. Tương quan Spearman và Kendall-Tau được báo cáo trong Phụ lục C. Lưu ý rằng Feedback Bench là tập kiểm tra trong domain của các mô hình PROMETHEUS.

LM Đánh giá                      HHH ALIGNMENT                    MT BENCH HUMAN JUDG.      AUTO-J EVAL        Preference Bench
                           Help. Harm. Hon. Other Total  Avg.  w/ TIE  w/o TIE  w/ TIE  w/o TIE  Instance-wise  Criteria
LLAMA 2-CHAT 7B           55.93 62.07 49.18 62.79 57.01  46.68   50.39    45.76    45.73      58.60
LLAMA 2-CHAT 13B          71.19 77.59 60.66 62.79 68.33  51.22   49.61    47.84    43.28      63.00
LLAMA 2-CHAT 70B          62.71 81.03 65.57 65.12 68.78  55.14   60.88    53.38    50.64      64.70
MISTRAL-INSTRUCT-7B       59.32 68.97 63.93 81.40 67.42  53.81   63.82    53.88    60.94      79.40
MIXTRAL-INSTRUCT-8X7B     83.05 87.93 67.21 69.77 77.38  51.85   71.42    53.81    73.50      84.00
PAIRRM (0.4B)            84.75 84.48 80.33 90.70 84.62     -     59.00       -     59.05      81.80
ULTRArm (13B)           86.44 79.31 81.97 88.37 83.71     -     56.00       -     59.85      86.97
AUTO-J (13B)             77.97 79.31 70.49 74.42 75.57  42.56   69.12    43.46    76.64      81.35
PROMETHEUS-2-7B          72.78 79.31 77.05 76.74 74.66  50.45   70.78    54.96    75.07      93.25
PROMETHEUS-2-8X7B        84.75 96.55 81.97 76.74 85.52  55.07   71.96    58.41    79.98      90.65
GPT-3.5-TURBO-0613       77.97 81.03 77.05 67.44 76.47  54.65   69.41    45.98    72.13      75.05
GPT-4-1106-PREVIEW       89.83 96.55 91.80 83.72 90.95  60.38   79.90    52.80    83.12      85.50
CLAUDE-3-OPUS            91.53 100.00 91.80 95.35 94.57  55.35   77.65    60.70    82.92      89.85

Bảng 4: Kết quả Xếp hạng theo Cặp Độ chính xác trên các bộ dữ liệu sở thích con người. Độ chính xác tốt nhất có thể so sánh được in đậm và tốt thứ hai được gạch chân ngoại trừ các LM độc quyền. Lưu ý rằng HHH Alignment là tập kiểm tra trong domain cho PairRM, Auto-J Eval là tập kiểm tra trong domain cho Auto-J, và Preference Bench là tập kiểm tra trong domain cho các mô hình Prometheus-2.

5 Kết quả Thí nghiệm
Trong phần này, chúng tôi so sánh khả năng đánh giá của các mô hình PROMETHEUS-2 với các baseline khác sử dụng định dạng đánh giá trực tiếp (Mục 5.1) và định dạng xếp hạng theo cặp (Mục 5.2). Ngoài ra, chúng tôi đo lường tính nhất quán của điểm số từ các LM đánh giá trong Phụ lục E.

5.1 Kết quả Đánh giá Trực tiếp
Kết quả đánh giá trực tiếp được hiển thị trong Bảng 3. Các quyết định điểm số của các mô hình PROMETHEUS 2 (7B & 8x7B), GPT-4-1106, Claude-3-Opus, và các đánh giá viên con người đều có tương quan mạnh với nhau, tạo ra tương quan Pearson cao hơn 0.5 bất kể đánh giá viên tham chiếu và benchmark. Mặt khác, các LM cơ sở, các LM được huấn luyện định dạng đơn, và các LM được huấn luyện chung cho thấy tương quan thấp hơn, chủ yếu dưới 0.5.

Đáng chú ý, các mô hình PROMETHEUS 2 vượt trội hơn Prometheus và Auto-J ít nhất 0.2 đơn vị trên các benchmark trong tương quan với các LM độc quyền. Hơn nữa, trên benchmark FLASK, trong khi tương quan giữa con người và GPT-4 là 0.679, tương quan cao nhất được đạt trước đó bởi Prometheus-13B với con người là 0.449. PROMETHEUS-2-8X7B đạt tương quan 0.555 với con người, giảm một nửa khoảng cách.

5.2 Kết quả Xếp hạng theo Cặp
Kết quả xếp hạng theo cặp được hiển thị trong Bảng 4. Chúng tôi loại trừ kết quả của PairRM và UltraRM trên cài đặt 'w/ Tie' vì chúng không thể xử lý nó. Trên tất cả 4 benchmark, các mô hình PROMETHEUS 2 đạt điểm số cao nhất, cho thấy rằng chúng có thể mô phỏng hiệu quả các đánh giá của con người.

Đáng chú ý, trong khi HHH Alignment là tập kiểm tra trong domain cho PairRM, và Auto-J Eval là cho Auto-J, PROMETHEUS-2-8X7B đạt điểm số cao hơn. Điều này cho thấy rằng việc huấn luyện một LM lớn (tức là Mixtral-8x7B) với dữ liệu phản hồi có thể là một chiến lược hiệu quả để có được một LM đánh giá mạnh mẽ có thể khái quát hóa vượt ra ngoài dữ liệu huấn luyện của nó. Hơn nữa, các mô hình PROMETHEUS 2 ít nhất giảm một nửa khoảng cách hiệu suất với các LM độc quyền so với các LM đánh giá hiện có trên các tập kiểm tra ngoài domain.

6 Phân tích Hợp nhất Trọng số
Để hiểu hiệu quả của phương pháp hợp nhất trọng số được đề xuất của chúng tôi trong bối cảnh đánh giá, chúng tôi giải quyết các câu hỏi nghiên cứu sau:
• RQ1: Hợp nhất trọng số có hiệu quả hơn so với huấn luyện chung không? (Mục 6.1)
• RQ2: Hiệu quả của hợp nhất trọng số có phải do việc ensemble mô hình không? (Mục 6.2)
• RQ3: Việc học với đánh giá trực tiếp giúp hiệu suất xếp hạng theo cặp đến mức nào, và ngược lại? (Mục 6.3)

6.1 Hợp nhất Trọng số so với Huấn luyện Chung
Bảng 5 so sánh hiệu suất của các LM đánh giá được huấn luyện thông qua hợp nhất trọng số và huấn luyện chung. Bên cạnh điều này, chúng tôi cũng thêm và so sánh kết quả của prompting và huấn luyện định dạng đơn.

Thật ngạc nhiên, các LM đánh giá được huấn luyện thông qua huấn luyện chung thường cho thấy hiệu suất thấp hơn so với những LM chỉ được huấn luyện trong định dạng đơn, điều này chỉ ra chuyển giao tác vụ tiêu cực. Cụ thể, các LM đánh giá chỉ được huấn luyện trên định dạng đánh giá trực tiếp đạt tương quan cao hơn so với các đối tác được huấn luyện chung trên các quy mô mô hình khác nhau. Tương tự, các LM đánh giá chỉ được huấn luyện trên định dạng xếp hạng theo cặp đạt độ chính xác trung bình cao hơn so với những LM được huấn luyện trên nhiều tác vụ, đặc biệt khi sử dụng Mixtral-8x7B làm mô hình cơ sở.

Mặt khác, các LM đánh giá được huấn luyện thông qua hợp nhất trọng số cho thấy hiệu suất vượt trội không chỉ so với các LM đánh giá được huấn luyện chung mà còn so với các LM đánh giá được huấn luyện định dạng đơn, chỉ ra chuyển giao tác vụ tích cực. Ngoài ra, mặc dù cả hai đều có lợi cho nhau, việc hợp nhất trọng số LM đánh giá xếp hạng theo cặp cải thiện hiệu suất đánh giá trực tiếp một cách đáng kể hơn so với ngược lại.

6.2 Hiệu quả của Hợp nhất Trọng số có phải do Ensemble Mô hình không?
Mặc dù chúng tôi tìm thấy thực nghiệm rằng hợp nhất trọng số hiệu quả, lý do cơ bản vẫn không rõ ràng. Một giả định tự nhiên là hiệu quả này xuất phát từ hiệu ứng ensemble của việc kết hợp nhiều mô hình. Để kiểm tra giả thuyết này, chúng tôi tiến hành một thí nghiệm ablation nơi chúng tôi huấn luyện nhiều LM đánh giá trên các seed ngẫu nhiên khác nhau và hợp nhất chúng. Cụ thể, chúng tôi hợp nhất hai LM đánh giá được huấn luyện trên định dạng đánh giá trực tiếp (được ký hiệu là 'Đánh giá Trực tiếp & Đánh giá Trực tiếp') và hai LM đánh giá được huấn luyện trên định dạng xếp hạng theo cặp (được ký hiệu là 'Xếp hạng theo Cặp & Xếp hạng theo Cặp'). Chúng tôi sử dụng Mistral-7B-Instruct làm mô hình cơ sở.

Kết quả được trình bày trong Bảng 6. Trên nhiều benchmark, việc hợp nhất các LM đánh giá được huấn luyện trên cùng một định dạng đánh giá không nâng cao hiệu suất đánh giá. Cụ thể, việc hợp nhất hai LM đánh giá được huấn luyện trên cùng một định dạng đánh giá—dù là đánh giá trực tiếp hay xếp hạng theo cặp—tác động tiêu cực đến hiệu suất trung bình cho cả benchmark đánh giá trực tiếp và xếp hạng theo cặp. Ngược lại, việc hợp nhất hai LM đánh giá, mỗi LM được huấn luyện trên định dạng đánh giá trực tiếp và xếp hạng theo cặp, mang lại hiệu suất vượt trội so với các cài đặt khác. Điều này chỉ ra rằng chuyển giao tác vụ có lợi trong hợp nhất trọng số phát sinh từ việc tích hợp các định dạng đánh giá khác nhau, không phải ensemble nhiều mô hình.

6.3 Định lượng Chuyển giao Tích cực qua các Định dạng Đánh giá
Để khám phá cách huấn luyện trên dữ liệu phản hồi đánh giá trực tiếp ảnh hưởng đến độ chính xác xếp hạng theo cặp và ngược lại, chúng tôi thí nghiệm bằng cách điều chỉnh giá trị α trong hợp nhất tuyến tính. Chúng tôi đánh giá hiệu suất trung bình sử dụng tất cả tám benchmark trong các thí nghiệm của chúng tôi. Để minh họa hiệu suất trung bình (màu đen), chúng tôi điều chỉnh thang đo bằng cách nhân tương quan Pearson từ đánh giá trực tiếp, ban đầu trong khoảng từ 0 đến 1, với 100 trước khi tính trung bình với độ chính xác xếp hạng theo cặp.

Kết quả được hiển thị trong Hình 3. Đối với các benchmark đánh giá trực tiếp, các LM đánh giá đạt hiệu suất tối ưu khi α được đặt thành 0.5. Điều này gián tiếp chỉ ra rằng cả dữ liệu phản hồi xếp hạng theo cặp và đánh giá trực tiếp đều đóng góp ngang nhau. Mặt khác, đối với các benchmark xếp hạng theo cặp, hiệu suất tối ưu khi α được đặt thành 0.3. Điều này cũng ngụ ý rằng mặc dù cả hai đều có lợi cho nhau, việc huấn luyện trên xếp hạng theo cặp cải thiện hiệu suất đánh giá trực tiếp nhiều hơn so với ngược lại.

--- TRANG 7 ---
Phương pháp Huấn luyện            CÁC BENCHMARK ĐÁNH GIÁ TRỰC TIẾP                  CÁC BENCHMARK XẾP HẠNG THEO CẶP
                            Vicuna Ben.  MT Ben.  FLASK   Trung bình  HHH Align.  MT Ben. H.J.  Auto-J Eval  Trung bình
Mistral-Instruct-7B
PROMPTING                      0.486       0.284    0.480    0.417       67.42       63.82        60.94        64.06
CHỈ ĐÁNH GIÁ TRỰC TIẾP        0.537       0.561    0.519    0.539       73.33       56.76        64.38        64.82
CHỈ XẾP HẠNG THEO CẶP           -           -        -        -          78.73       67.06        72.03        72.61
HUẤN LUYỆN CHUNG              0.548       0.450    0.457    0.485       80.09       65.49        73.60        73.06
HỢP NHẤT TRỌNG SỐ             0.666       0.548    0.659    0.624       74.66       70.78        75.07        73.50
Mixtral-Instruct-8x7B
PROMPTING                      0.566       0.551    0.507    0.541       77.38       71.42        73.55        74.56
CHỈ ĐÁNH GIÁ TRỰC TIẾP        0.625       0.664    0.587    0.625       74.21       53.14        65.85        64.40
CHỈ XẾP HẠNG THEO CẶP           -           -        -        -          84.16       66.27        75.66        75.36
HUẤN LUYỆN CHUNG              0.628       0.560    0.596    0.595       82.35       68.73        74.78        75.29
HỢP NHẤT TRỌNG SỐ             0.685       0.665    0.659    0.670       85.52       71.96        79.98        79.15

Bảng 5: Huấn luyện Định dạng Đơn so với Huấn luyện Chung so với Hợp nhất Trọng số Tương quan Pearson giữa các LM đánh giá được huấn luyện với các phương pháp khác nhau và GPT-4-1106. Các LM đánh giá được huấn luyện với hợp nhất trọng số vượt trội hơn các LM đánh giá được huấn luyện định dạng đơn và được huấn luyện chung trên nhiều benchmark.

Dữ liệu Huấn luyện Định dạng Đánh giá            CÁC BENCHMARK ĐÁNH GIÁ TRỰC TIẾP                  CÁC BENCHMARK XẾP HẠNG THEO CẶP
                                        Vicuna Ben.  MT Ben.  FLASK   Trung bình  HHH Align.  MT Ben. H.J.  Auto-J Eval  Trung bình
KHÔNG HUẤN LUYỆN (PROMPTING)             0.486       0.284    0.480    0.417       67.42       63.82        60.94        64.06
CHỈ ĐÁNH GIÁ TRỰC TIẾP                  0.537       0.561    0.519    0.539       73.33       56.76        64.38        64.82
CHỈ XẾP HẠNG THEO CẶP                     -           -        -        -          78.73       67.06        72.03        72.61
ĐÁNH GIÁ TRỰC TIẾP & ĐÁNH GIÁ TRỰC TIẾP  0.552       0.493    0.505    0.517       73.30       55.00        63.69        64.13
XẾP HẠNG THEO CẶP & XẾP HẠNG THEO CẶP      -           -        -        -          78.70       65.20        72.72        72.21
ĐÁNH GIÁ TRỰC TIẾP & XẾP HẠNG THEO CẶP   0.666       0.548    0.659    0.624       74.66       70.78        75.07        73.50

Bảng 6: Thống nhất Định dạng so với Ensemble Tương quan Pearson với GPT-4-1106 (Vicuna Bench, MT Bench, FLASK) và sự thống nhất với các đánh giá viên con người (HHH Alignment, MT Bench Human Judgment, Auto-J Eval). Hợp nhất các mô hình được huấn luyện với cùng định dạng đánh giá (ensemble) kém hiệu quả hơn việc hợp nhất các mô hình được huấn luyện với các định dạng khác nhau (thống nhất định dạng).

Tương quan Đánh giá Trực tiếp    Độ chính xác Xếp hạng theo Cặp    Hiệu suất Trung bình
(Đánh giá Trực tiếp : Xếp hạng theo Cặp) Tỷ lệ Hợp nhất    Tương quan Pearson Đánh giá Trực tiếp
                                                            Độ chính xác Thống nhất Xếp hạng theo Cặp

Hình 3: Khi hợp nhất các mô hình, ảnh hưởng của đánh giá tương đối đến đánh giá tuyệt đối lớn hơn ảnh hưởng của đánh giá tuyệt đối đến đánh giá tương đối. Hiệu suất của Đánh giá Trực tiếp (màu xanh lá) và Xếp hạng theo Cặp (màu xanh lam) khi thay đổi giá trị α để hợp nhất các LM đánh giá được huấn luyện trên các định dạng khác nhau.

7 Kết luận
Chúng tôi giới thiệu PROMETHEUS 2, một LM nguồn mở chuyên biệt trong đánh giá các phản hồi khác. Không giống như các LM đánh giá nguồn mở hiện tại không thể xử lý hiệu quả cả đánh giá trực tiếp và xếp hạng theo cặp—hai chương trình đánh giá phổ biến nhất—các mô hình PROMETHEUS 2 thể hiện hiệu suất vượt trội trên cả hai chương trình, thu hẹp đáng kể khoảng cách với các đánh giá dựa trên LM độc quyền. Để huấn luyện các mô hình PROMETHEUS 2, chúng tôi phát triển BỘ SƯU TẬP SỞ THÍCH, bộ dữ liệu xếp hạng theo cặp đầu tiên bao gồm hơn 1,000 tiêu chí đánh giá theo từng trường hợp vượt ra ngoài các chất lượng cơ bản như tính hữu ích và tính vô hại. Đáng chú ý, chúng tôi thấy rằng việc hợp nhất các LM đánh giá được huấn luyện trên đánh giá trực tiếp hoặc định dạng xếp hạng theo cặp có thể dẫn đến một LM đánh giá thống nhất với hiệu suất mạnh mẽ. Chúng tôi hy vọng rằng công việc của chúng tôi khuyến khích nhiều nghiên cứu hơn về việc sử dụng các LM nguồn mở làm đánh giá.

Lời cảm ơn
Chúng tôi cảm ơn các thành viên KAIST AI LKLab đã có những thảo luận hữu ích. Công việc này được hỗ trợ một phần bởi tài trợ từ LG AI Research (Khả năng lý luận logic tự cải thiện của LLMs, 2024, 50%) và tài trợ từ Viện Lập kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (RS-2024-00397966, Phát triển Mô hình sLLM chuyên biệt An ninh mạng dựa trên RAG để Ngăn chặn Sự cố Gen-AI và Xây dựng Nền tảng Trình diễn Công cộng, 50%).

Hạn chế
Đánh giá về cơ bản là một nhiệm vụ rất đa diện. Trong bài báo này, chúng tôi đã sử dụng một phương pháp gián tiếp để đánh giá khả năng đánh giá của các LM đánh giá bằng cách đo lường xem chúng có thực hiện đánh giá tương tự như các đánh giá viên con người hay các LM độc quyền, như GPT-4-1106 và Claude-3-Opus. Tuy nhiên, đây có thể không nhất thiết là phương pháp tốt nhất. Các công việc tương lai có thể khám phá các quy trình meta-evaluation để đánh giá lại kết quả của các LM đánh giá hoặc các phương pháp cho phép con người đánh giá hiệu quả các kết quả đánh giá. Cũng lưu ý rằng việc sử dụng đánh giá dựa trên mô hình kết hợp với đánh giá con người thay vì chỉ dựa vào nó là rất quan trọng.

Ngoài ra, mức độ mà các LM đánh giá có thể khái quát hóa dựa trên phân tích của Kim et al. (2023), kiểm tra sự trùng lặp giữa dữ liệu được sử dụng để huấn luyện các LM đánh giá và dữ liệu được sử dụng để đánh giá chúng. Nghiên cứu này mở rộng đánh giá đến tám bộ dữ liệu khác nhau với đánh giá con người để kiểm tra khả năng khái quát hóa của đánh giá trong các tình huống khác nhau. Tuy nhiên, điều này có thể không đủ. Một trong những thách thức chính trong việc đánh giá các LM đánh giá là có được "kết quả đánh giá" (ví dụ: đánh giá con người). Việc tự động hóa đánh giá với các LM có thể mang lại lợi ích lớn cho nhiều lĩnh vực nghiên cứu NLP, do đó vai trò của các công việc tương lai trong việc tạo ra các benchmark phản hồi bao gồm đánh giá con người hoặc dữ liệu để huấn luyện các LM đánh giá là rất quan trọng.

Một nhược điểm của PROMETHEUS 2 là nó chỉ hoạt động trên thang điểm Likert 1-5 cho đánh giá tuyệt đối hoặc phong cách đánh giá so sánh 'A tốt hơn & B tốt hơn'. Tùy thuộc vào trường hợp sử dụng, mọi người có thể cần đánh giá tuyệt đối 1-10 điểm, phương pháp xếp hạng cho năm phản hồi cùng một lúc, hoặc đánh giá dựa trên danh sách kiểm tra không được đề cập trong bài báo. Trong khi các LM độc quyền có thể linh hoạt tiến hành đánh giá trong bất kỳ định dạng nào nếu một prompt được mô tả tốt được thiết kế, các LM nguồn mở không thể tạo ra kết quả đánh giá tốt mà không cần huấn luyện, và ngược lại, nếu được huấn luyện trong một hoặc hai định dạng, chúng mất tính linh hoạt để tiến hành các đánh giá khác nhau. Các công việc tương lai có thể xem xét liệu các LM đánh giá được huấn luyện trong mỗi định dạng, như được thực hiện trong bài báo này, có thể xử lý tốt các đánh giá cho các định dạng được thêm vào khi hợp nhất trọng số được sử dụng.

Cuối cùng, bài báo trình bày một mô hình đánh giá có thể xử lý tốt cả định dạng đánh giá tuyệt đối và so sánh thông qua hợp nhất trọng số dựa trên các thí nghiệm thực nghiệm. Tuy nhiên, việc giải thích cơ bản tại sao hợp nhất trọng số hoạt động tốt vẫn là một nhiệm vụ thách thức. Để giải quyết điều này, Mục 6 phân tích gián tiếp hiệu quả của hợp nhất trọng số bằng cách so sánh với huấn luyện chung, chứng minh rằng việc cải thiện hiệu suất đánh giá không phải do ensemble mô hình, và cho thấy rằng tác động của đánh giá so sánh đến đánh giá tuyệt đối lớn hơn ngược lại. Giải thích tốt nhất hiện tại của chúng tôi là "đánh giá tuyệt đối và so sánh không phải là những nhiệm vụ hoàn toàn khác nhau, vì vậy hợp nhất trọng số có thể xử lý cả hai mà không bị suy thoái, và ngược lại, bởi vì chúng không quá tương tự, hợp nhất trọng số hoạt động tốt hơn huấn luyện chung." Các công việc tương lai có thể phân tích lý thuyết điều này hoặc khám phá thêm liệu hợp nhất trọng số có thể hoạt động hiệu quả trong các lĩnh vực khác ngoài đánh giá LLM.

Tài liệu tham khảo
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, và Jared Kaplan. 2021. A general language assistant as a laboratory for alignment.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,

--- TRANG 8 ---
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, và Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, và Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377.

Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, và Leshem Choshen. 2022. Cold fusion: Collaborative descent for distributed multitask finetuning. arXiv preprint arXiv:2212.01378.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, và Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.

Markus Freitag, David Grangier, và Isaac Caswell. 2020. Bleu might be guilty but references are not innocent. arXiv preprint arXiv:2004.06063.

Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, và Xiaojun Wan. 2024. Llm-based nlg evaluation: Current status and challenges. arXiv preprint arXiv:2402.01383.

Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaustubh D Dhole, et al. 2021. The gem benchmark: Natural language generation, its evaluation and metrics. arXiv preprint arXiv:2102.01672.

Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexandros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets, Ashish Upadhyay, Bingsheng Yao, et al. 2022. Gemv2: Multilingual nlg benchmarking in a single line of code. arXiv preprint arXiv:2206.11249.

Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, và Jacob Solawetz. 2024. Arcee's mergekit: A toolkit for merging large language models. arXiv preprint arXiv:2403.13257.

Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, và Luke Zettlemoyer. 2023. Scaling expert language models with unsupervised domain discovery. arXiv preprint arXiv:2303.14177.

Michael Hanna và Ondřej Bojar. 2021. A fine-grained analysis of bertscore. In Proceedings of the Sixth Conference on Machine Translation, pages 507–517.

Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, và Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089.

Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, và Prithviraj Ammanabrolu. 2023a. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564.

Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, và Minjoon Seo. 2023b. Exploring the benefits of training expert language models over instruction tuning. arXiv preprint arXiv:2302.03202.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023a. Mistral 7b. arXiv preprint arXiv:2310.06825.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.

Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, và Wenhu Chen. 2023b. Tigerscore: Towards building explainable metric for all text generation tasks. arXiv preprint arXiv:2310.00752.

Dongfu Jiang, Xiang Ren, và Bill Yuchen Lin. 2023c. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561.

Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491.

Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et al. 2024. The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models. arXiv preprint arXiv:2406.05761.

--- TRANG 9 ---
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787.

Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, và Minjoon Seo. 2024. Prometheus-vision: Vision-language model as a judge for fine-grained evaluation. arXiv preprint arXiv:2401.06591.

Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, và Pengfei Liu. 2023a. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470.

Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, và Luke Zettlemoyer. 2022. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023b. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.

Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, và Chongyang Tao. 2024. Leveraging large language models for nlg evaluation: A survey. arXiv preprint arXiv:2401.07103.

Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, và Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human alignment.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, và Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

Michael S Matena và Colin A Raffel. 2022. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems, 35:17703–17716.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.

Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, và Matthieu Cord. 2024. Rewardedsoups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36.

Natalie Schluter. 2017. The limits of automatic summarisation according to rouge. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 41–45. Association for Computational Linguistics.

Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, et al. 2024. Branch-train-mix: Mixing expert llms into a mixture-of-experts llm. arXiv preprint arXiv:2403.07816.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models.

Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, và Tong Zhang. 2024. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571.

Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, và Zhifang Sui. 2023a. Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935.

Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023b. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087.

Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, và Mohit Bansal. 2024. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36.

--- TRANG 10 ---
vances in Neural Information Processing Systems, 36.

Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, và Minjoon Seo. 2023. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928.

Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, và Yongbin Li. 2023. Language models are super mario: Absorbing abilities from homologous models as a free lunch. arXiv preprint arXiv:2311.03099.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.

Lianghui Zhu, Xinggang Wang, và Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631.

--- TRANG 11 ---
Tiêu chuẩn Xác minh    KẾT QUẢ
Tính nhất quán         99.5% (Đạt)
Sự phù hợp             98.5% (Đạt)
Tính quan trọng        88% (Tỷ lệ thắng)

Bảng 7: Kết quả xác minh con người để đánh giá chất lượng của BỘ SƯU TẬP SỞ THÍCH. Chúng tôi sử dụng ba tiêu chuẩn để đánh giá chất lượng của phản hồi bằng lời vrm,rn.

Nhiệt độ               1.0
Top_p                  0.9
Token Mới Tối đa       1024
Penalty Lặp lại        1.03

Bảng 8: Siêu tham số được sử dụng để suy luận các baseline LM đánh giá khác nhau.

Mô hình Cơ sở          mistralai/Mistral-7B-Instruct-v0.2
Kiểu Torch             bfloat16
Epoch                  1
Dữ liệu Huấn luyện 1   BỘ SƯU TẬP PHẢN HỒI
Dữ liệu Huấn luyện 2   BỘ SƯU TẬP SỞ THÍCH
Độ dài Seq Tối đa      4096
Tốc độ Học             1e-5
Kích thước Batch Huấn luyện 4
Seed Ngẫu nhiên        42
Chiến lược Hợp nhất    Tuyến tính (α = 0.5)
Phương pháp Huấn luyện Supervised Fine-tuning

Bảng 9: Siêu tham số được sử dụng để huấn luyện PROMETHEUS 2 7B.

Mô hình Cơ sở          mistralai/Mixtral-8x7B-Instruct-v0.1
Kiểu Torch             bfloat16
Epoch                  1
Dữ liệu Huấn luyện 1   BỘ SƯU TẬP PHẢN HỒI
Dữ liệu Huấn luyện 2   BỘ SƯU TẬP SỞ THÍCH
Độ dài Seq Tối đa      4096
Tốc độ Học             1e-5
Kích thước Batch Huấn luyện 8
PEFT                   True
Lora_r                 256
Lora_alpha             512
Lora_Dropout           0.1
Lora Target Module     Q_proj,K_proj,V_proj,O_proj,W_proj,LM_Head
Seed Ngẫu nhiên        42
Chiến lược Hợp nhất    DARE Merging
Merging p              0.1
Merging Lambda         1.95
Phương pháp Huấn luyện Supervised Fine-tuning

Bảng 10: Siêu tham số được sử dụng để huấn luyện PROMETHEUS 2 8x7B.

A Xác minh Chất lượng của
BỘ SƯU TẬP SỞ THÍCH

Để đảm bảo chất lượng của BỘ SƯU TẬP SỞ THÍCH, đặc biệt là phản hồi bằng lời được tạo ra vrm,rn, chúng tôi sử dụng năm chú thích viên có nền tảng về xử lý ngôn ngữ tự nhiên. Nghiên cứu chú thích được thiết kế và quản lý theo hướng dẫn đạo đức của [Liên kết X]. Những người làm việc đám đông được thông báo về các rủi ro tiềm ẩn của việc tham gia và thông tin liên hệ của nhà nghiên cứu trước trong biểu mẫu đồng ý nghiên cứu. Mức lương theo giờ và thời gian nghiên cứu dự kiến được thông báo trong nền tảng Prolific. Chúng tôi đã trả công cho các công nhân 9 GBP mỗi giờ. 3 người từ Mỹ và 2 người từ nhân khẩu học châu Á.

Chúng tôi lấy mẫu ngẫu nhiên 200 trường hợp với các hướng dẫn khác nhau và tiến hành quy trình xác minh ba phần. Đầu tiên, chúng tôi đánh giá tính nhất quán của vrm,rn với quyết định điểm số (tức là 'A tốt hơn' hoặc 'B tốt hơn'). Thứ hai, chúng tôi đánh giá sự phù hợp của vrm,rn với tiêu chí đánh giá e. Cuối cùng, để xác định tính quan trọng của phản hồi, chúng tôi so sánh vrm,rn mới được tạo ra với việc nối vrm và vrn. Điều này nhằm mục đích xác định xem vrm,rn có hiệu quả tận dụng thông tin lẫn nhau giữa rm và rn hay không. Sau đó, các chú thích viên bỏ phiếu về việc liệu vrm,rn hay việc nối rm và rn có quan trọng hơn. Kết quả được hiển thị trong Bảng 7. Lưu ý rằng Bộ Sưu tập Sở thích chỉ bao gồm các trường hợp tiếng Anh.

B Chi tiết Huấn luyện và Suy luận

Các cấu hình mà chúng tôi sử dụng cho prompting và huấn luyện các LM đánh giá được hiển thị trong Bảng 8, 9, 10. Đối với Auto-J, PairRM và UltraRM, chúng tôi sử dụng mẫu prompt, siêu tham số suy luận được đề cập trong các thẻ mô hình hoặc kho lưu trữ github để đảm bảo cấu hình tối ưu cho so sánh hiệu suất công bằng. Đối với các LM độc quyền, PROMETHEUS 1 và các mô hình PROMETHEUS 2, chúng tôi sử dụng cùng một mẫu prompt và cấu hình đánh giá.

Đối với cả huấn luyện và suy luận, chúng tôi sử dụng tám GPU NVIDIA A100 40GB. Huấn luyện cần khoảng 800 giờ GPU, sử dụng triển khai từ kho lưu trữ Alignment Handbook2. Để suy luận, chúng tôi sử dụng framework vllm3.

Kết quả từ Đánh giá Trực tiếp được tính trung bình sau ba lần chạy và chấm điểm theo cặp được tiến hành trong một lần chạy. Thay vì sử dụng thanh lỗi, chúng tôi báo cáo tính nhất quán trong các định dạng đánh giá, alpha Krippendorff cho tính nhất quán trong đánh giá trực tiếp và thống kê bắc cầu cho tính nhất quán trong xếp hạng theo cặp.

C Kết quả Đánh giá Trực tiếp: Mở rộng

Bảng 11 và 12 (trang tiếp theo) hiển thị kết quả mở rộng Bảng 3. Ngay cả khi thay đổi chỉ số thành Kendall-Tau và Spearman, xu hướng tổng thể vẫn được duy trì. PROMETHEUS 2 cho thấy hiệu suất đánh giá vượt trội trong số các LM đánh giá nguồn mở, đạt tương quan cao với con người và các LM độc quyền.

D Giấy phép

Các mô hình của chúng tôi được phát hành dưới giấy phép Apache 2.0. Bộ dữ liệu Preference Collection tuân theo Điều khoản Sử dụng của OpenAI cho dữ liệu được tạo ra. Mô hình có thể được sử dụng cho mục đích thương mại trong khi bộ dữ liệu được dành cho mục đích nghiên cứu. Chúng tôi đã sử dụng perspective API để đảm bảo rằng dữ liệu huấn luyện hoặc bộ dữ liệu đánh giá không bao gồm các trường hợp có PII.

--- TRANG 12 ---
LM Đánh giá                      VICUNA BENCH         MT BENCH           FLASK             Feedback Bench
                           GPT-4-1106  Claude-3-Opus  GPT-4-1106  Claude-3-Opus  GPT-4-1106  Claude-3-Opus  Con người  GPT-4-0613
LLAMA 2-CHAT 7B              0.183        0.203        0.065        0.070        0.229        0.186        0.211     0.419
LLAMA 2-CHAT 13B             0.145        0.146       -0.019        0.037        0.160        0.174        0.174     0.453
LLAMA 2-CHAT 70B             0.282        0.382        0.150        0.196        0.310        0.310        0.231     0.487
MISTRAL-INSTRUCT-7B          0.314        0.391        0.208        0.281        0.395        0.384        0.287     0.454
MIXTRAL-INSTRUCT-8X7B        0.395        0.468        0.433        0.419        0.410        0.408        0.304     0.551
PROMETHEUS-7B                0.405        0.425        0.290        0.263        0.282        0.251        0.236     0.770
PROMETHEUS-13B               0.397        0.434        0.299        0.352        0.365        0.352        0.299     0.793
AUTO-J (13B)                 0.282        0.242        0.303        0.272        0.312        0.282        0.312     0.515
PROMETHEUS-2-7B              0.543        0.476        0.390        0.372        0.476        0.446        0.377     0.784
PROMETHEUS-2-8X7B            0.559        0.515        0.535        0.483        0.526        0.507        0.388     0.800
GPT-3.5-TURBO-0613          0.255        0.287        0.148        0.157        0.360        0.315        0.298     0.489
GPT-4-1106                     /          0.553          /          0.590          /          0.609        0.517     0.662
CLAUDE-3-OPUS                0.553          /          0.590          /          0.609          /          0.453     0.693

Bảng 11: Tương quan Kendall-Tau giữa các đánh giá viên tham chiếu (liệt kê ở trên) và các LM đánh giá. Thống kê tốt nhất có thể so sánh được in đậm và tốt thứ hai được gạch chân ngoại trừ các LM độc quyền.

LM Đánh giá                      VICUNA BENCH         MT BENCH           FLASK             Feedback Bench
                           GPT-4-1106  Claude-3-Opus  GPT-4-1106  Claude-3-Opus  GPT-4-1106  Claude-3-Opus  Con người  GPT-4-0613
LLAMA 2-CHAT 7B              0.236        0.255        0.084        0.089        0.301        0.244        0.279     0.511
LLAMA 2-CHAT 13B             0.178        0.179       -0.025        0.044        0.206        0.222        0.224     0.543
LLAMA 2-CHAT 70B             0.348        0.466        0.197        0.252        0.391        0.389        0.298     0.585
MISTRAL-INSTRUCT-7B          0.389        0.480        0.266        0.358        0.499        0.478        0.374     0.563
MIXTRAL-INSTRUCT-8X7B        0.476        0.556        0.545        0.517        0.505        0.500        0.386     0.659
PROMETHEUS-7B                0.508        0.528        0.385        0.349        0.367        0.326        0.317     0.876
PROMETHEUS-13B               0.492        0.534        0.401        0.470        0.474        0.454        0.398     0.893
AUTO-J (13B)                 0.337        0.297        0.408        0.365        0.402        0.358        0.408     0.623
PROMETHEUS-2-7B              0.664        0.591        0.509        0.482        0.597        0.555        0.491     0.885
PROMETHEUS-2-8X7B            0.660        0.615        0.669        0.605        0.642        0.618        0.496     0.912
GPT-3.5-TURBO-0613          0.319        0.354        0.192        0.198        0.446        0.390        0.374     0.565
GPT-4-1106                     /          0.659          /          0.721          /          0.729        0.650     0.753
CLAUDE-3-OPUS                0.659          /          0.721          /          0.729          /          0.567     0.784

Bảng 12: Tương quan Spearman giữa các đánh giá viên tham chiếu (liệt kê ở trên) và các LM đánh giá. Thống kê tốt nhất có thể so sánh được in đậm và tốt thứ hai được gạch chân ngoại trừ các LM độc quyền.

LM Đánh giá                      HHH ALIGNMENT          MT BENCH HUMAN JUDG.     AUTO-J EVAL
                           Direct2Pair(↑) Pair2Pair(↑) ∆(↓) Direct2Pair(↑) Pair2Pair(↑) ∆(↓) Direct2Pair(↑) Pair2Pair(↑) ∆(↓)
AUTO-J (13B)                46.61         75.57        28.96    48.14         69.12        20.98    47.40         76.64        29.24
PROMETHEUS-2-7B             74.21         74.66        0.45     63.24         70.78        7.54     68.11         75.07        6.96
PROMETHEUS-2-8X7B           81.45         85.52        4.07     61.67         71.96        10.29    66.54         79.98        13.44
GPT-4-1106-PREVIEW          83.71         90.95        7.24     68.04         79.90        11.86    54.27         83.12        28.85
CLAUDE-3-OPUS               84.62         94.57        9.95     62.65         77.65        15.00    61.04         82.90        21.86

Bảng 13: Tính nhất quán trên các Định dạng Đánh giá Độ chính xác xếp hạng theo cặp khi đánh giá trong định dạng đánh giá trực tiếp (được ký hiệu là 'Direct2Pair') và định dạng xếp hạng theo cặp (được ký hiệu là 'Pair2Pair'). Giá trị ∆ nhỏ hơn chỉ ra rằng các LM đánh giá có thể đánh giá mạnh mẽ trên hai định dạng khác nhau.

--- TRANG 13 ---
LM Đánh giá                  Vicuna Ben.  MT Ben.  FLASK
LLAMA 2-CHAT 7B              0.3558       0.2565   0.4379
LLAMA 2-CHAT 13B             0.2017       0.2998   0.4038
LLAMA 2-CHAT 70B             0.5212       0.4559   0.6204
MISTRAL-INSTRUCT-7B          0.5157       0.4393   0.5884
MIXTRAL-INSTRUCT-8X7B        0.5459       0.6229   0.6976
PROMETHEUS-7B                0.6049       0.5363   0.5970
PROMETHEUS-13B               0.5734       0.5181   0.5624
AUTO-J (13B)                 0.4976       0.5069   0.6160
PROMETHEUS-2-7B              0.6018       0.5340   0.5991
PROMETHEUS-2-8X7B            0.6383       0.6862   0.7874
GPT-3.5-TURBO-0613          0.7108       0.4800   0.6389
GPT-4-1106-PREVIEW          0.7366       0.8271   0.8355
CLAUDE-3-OPUS               0.8284       0.8601   0.8976

Bảng 14: Thống kê alpha Krippendorff cho các LM đánh giá khi được prompted 3 lần thông qua giải mã không xác định.

LM Đánh giá                  BỘ SƯU TẬP SỞ THÍCH
                            Tính bắc cầu
MISTRAL-INSTRUCT-7B          87.10
MIXTRAL-INSTRUCT-8X7B        90.45
PAIRRM                       91.40
ULTRARRM                     94.25
AUTO-J (13B)                 89.65
PROMETHEUS-2-7B              97.60
PROMETHEUS-2-8X7B            96.75
GPT-3.5-TURBO-0613          84.35
GPT-4-1106-PREVIEW          95.70
CLAUDE-3-OPUS               96.20

Bảng 15: Thống kê tính bắc cầu để đo lường tính nhất quán trong cài đặt đánh giá xếp hạng theo cặp.

E Tính nhất quán của các LM Đánh giá

Ngoài việc đạt được tương quan và độ chính xác cao, việc đạt được tính nhất quán cao là một khía cạnh quan trọng khác đối với các LM đánh giá. Đầu tiên, chúng tôi kiểm tra xem các LM đánh giá có thể đưa ra quyết định điểm số nhất quán trong định dạng đánh giá trực tiếp hay không. Chúng tôi suy luận nhiều lần với giải mã không xác định (ví dụ: sử dụng nhiệt độ 1.0). Theo thiết kế thí nghiệm từ Ye et al. (2023), chúng tôi chọn suy luận 3 lần và báo cáo giá trị alpha Krippendorff. Như được hiển thị trong Bảng 14, kết quả chỉ ra rằng việc huấn luyện trên dữ liệu phản hồi chỉ cải thiện tính nhất quán một chút. Mặt khác, chúng tôi thấy rằng các LM có số lượng tham số lớn đạt được tính nhất quán cao. Điều này chỉ ra tầm quan trọng của việc chọn một LM lớn làm mô hình cơ sở khi huấn luyện một LM đánh giá. Đáng chú ý, PROMETHEUS-2-8X7B đạt tương quan cao nhất trong số các LM đánh giá nguồn mở.

Hơn nữa, để đánh giá tính nhất quán trong cài đặt xếp hạng theo cặp (Bảng 15), chúng tôi đo lường tính bắc cầu (tức là điểm số cao hơn cho phản hồi B so với A, và cho C so với B, dẫn đến điểm số cao hơn cho C so với A). Như được hiển thị trong Bảng 15, các mô hình PROMETHEUS 2 đạt hiệu suất ngang với GPT-4, cho thấy rằng chúng có thể cung cấp đánh giá mạnh mẽ trong chương trình xếp hạng theo cặp.

Cuối cùng, chúng tôi tiến hành một thí nghiệm để kiểm tra xem các LM đánh giá có thể đạt được điểm số nhất quán trên các định dạng đánh giá khác nhau hay không. Để làm điều này, chúng tôi sử dụng các benchmark xếp hạng theo cặp và đo lường sự khác biệt hiệu suất khi được prompted với định dạng đánh giá trực tiếp và định dạng xếp hạng theo cặp. Cụ thể, theo Kim et al. (2023), để xử lý bộ dữ liệu xếp hạng theo cặp trong chương trình đánh giá trực tiếp, chúng tôi đánh giá từng phản hồi riêng biệt và so sánh các quyết định điểm số. Chúng tôi đánh dấu là đúng nếu LM đánh giá cung cấp điểm số cao hơn cho phản hồi được con người chọn so với phản hồi bị từ chối. Như được hiển thị trong Bảng 13 (trang trước), kết quả cho thấy các mô hình PROMETHEUS 2 cho thấy sự khác biệt hiệu suất thấp hơn trên các định dạng đánh giá, chỉ ra tính mạnh mẽ của chúng.

--- TRANG 14 ---
LM Đánh giá                  BIGGENBENCH         FLASK
                       Reference-free  Reference-based  ∆  Reference-free  Reference-based  ∆
MISTRAL-INSTRUCT         0.305           0.310           0.005    0.331           0.374           0.043
MIXTRAL-INSTRUCT         0.320           0.322           0.002    0.377           0.386           0.009
PROMETHEUS-2-7B          0.403           0.455           0.052    0.425           0.545           0.120
PROMETHEUS-2-8X7B        0.424           0.472           0.048    0.411           0.555           0.144
GPT-3.5-TURBO-0613      0.236           0.252           0.016    0.354           0.374           0.020
GPT-4-1106              0.554           0.599           0.045    0.616           0.679           0.063

Bảng 16: Tương quan Pearson giữa các mô hình đánh giá khác nhau với và không có câu trả lời tham chiếu và Con người. Đánh giá dựa trên tham chiếu vượt trội hơn đánh giá không có tham chiếu trên tất cả các LM đánh giá.

Phương pháp Hợp nhất         CÁC BENCHMARK ĐÁNH GIÁ TRỰC TIẾP                    CÁC BENCHMARK XẾP HẠNG THEO CẶP  Trung bình
                      VICUNA BEN.  MT BEN.  FLASK (HUMAN)  Feedback Ben.  Trung bình  HHH ALIGN.  MT BEN. H.J.  AUTO-J  Pref. Ben.  Trung bình
LINEAR                   0.642       0.543     0.544         0.878         0.652       78.73       67.25        73.80    92.45       78.06      82.93
SLERP                    0.648       0.532     0.536         0.879         0.649       74.66       70.2         72.33    92.60       77.44      82.67
TASK ARITHMETIC          0.518       0.497     0.482         0.831         0.582       80.09       69.80        72.82    93.00       78.93      81.01
TIES                     0.534       0.567     0.529         0.826         0.614       79.64       67.75        72.91    93.95       78.56      80.58
DARE-TIES               0.653       0.545     0.543         0.880         0.655       79.64       66.57        74.68    93.30       78.55      83.27
DARE-LINEAR             0.666       0.548     0.545         0.882         0.660       74.66       70.78        75.07    93.25       78.44      83.32

Bảng 17: Tương quan Pearson và đo lường độ chính xác trên các benchmark khác nhau cho các phương pháp hợp nhất khác nhau. Thống kê tốt nhất có thể so sánh được in đậm và tốt thứ hai được gạch chân.

--- TRANG 15 ---
F Đánh giá Không có Tham chiếu trong
Định dạng Đánh giá Trực tiếp

Trong phần này, chúng tôi đánh giá tác động của việc loại trừ câu trả lời tham chiếu trong các đánh giá được tiến hành bằng định dạng đánh giá trực tiếp. Kết quả được trình bày trong Bảng 16 (trang trước). Cho thí nghiệm này, chúng tôi sử dụng FLASK (Ye et al., 2023) bao gồm đánh giá con người và thêm BiGGen Bench (Kim et al., 2024). BiGGen Bench là một benchmark tạo sinh bao gồm tiêu chí đánh giá được điều chỉnh cho từng trường hợp và cung cấp 2840 đánh giá con người (loại trừ các tác vụ đa ngôn ngữ) trong định dạng đánh giá trực tiếp.

Trên cả hai benchmark và các biến thể LM đánh giá khác nhau, tương quan với con người giảm khi câu trả lời tham chiếu bị loại bỏ. Ngay cả đối với GPT-4-1106, có sự suy giảm hiệu suất đáng kể (0.045, 0.063). Điều này cho thấy rằng việc bao gồm câu trả lời tham chiếu là quan trọng để tiến hành đánh giá hiệu quả với các LM. Thú vị là, PROMETHEUS-2-7B đạt hiệu suất tốt hơn trong cài đặt không có tham chiếu (0.403, 0.425) so với Mistral-7B-Instruct-v0.2 (0.310, 0.374). Xu hướng tương tự được quan sát thấy đối với PROMETHEUS-2-8X7B (0.424, 0.411) và Mixtral-8x7B-Instruct-v0.1 (0.322, 0.386). Điều này ngụ ý rằng một hiệu ứng của việc huấn luyện một LM đánh giá với câu trả lời tham chiếu được bao gồm là tạo ra khả năng căn cứ đánh giá vào câu trả lời tham chiếu đã cho.

G Ablation Phương pháp Hợp nhất

Trong phần này, ngoài hợp nhất tuyến tính, chúng tôi cũng kiểm tra các kỹ thuật hợp nhất khác nhau bao gồm:

• Hợp nhất Slerp (Goddard et al., 2024) hoạt động bằng cách nội suy hai trọng số θd và θp trong khi bảo toàn các tính chất hình học của không gian cầu mà θd và θp cư trú. Cụ thể, điều này được thực hiện bằng cách chuẩn hóa θd và θp thành độ dài đơn vị và sau đó hợp nhất hai trọng số dựa trên hệ số α như:

θfinal = α × θd/||θd|| + (1-α) × θp/||θp|| (4)

• Hợp nhất Task Arithmetic (Ilharco et al., 2022) có thể được biểu thị như sau:

θfinal = θinit + α × (θd - θinit) + (1-α) × (θp - θinit) (5)

trong đó θinit là trọng số của mô hình cơ sở. Tuy nhiên, chúng tôi tìm thấy thực nghiệm rằng LM đánh giá kết quả θfinal thường không tạo ra quyết định điểm số hợp lệ (ví dụ: tạo ra số nguyên trong xếp hạng theo cặp).

• Hợp nhất TIES (Yadav et al., 2024), mặc dù tương tự như hợp nhất Task Arithmetic, thêm (1) một hoạt động Trim để loại bỏ trọng số dư thừa trong vector tác vụ θd - θinit và θp - θinit và (2) các hoạt động Elect và Disjoint để giải quyết sự bất đồng (tức là trọng số hướng ngược) giữa θd - θinit và θp - θinit.

• Hợp nhất DARE (Yu et al., 2023), mặc dù cũng tương tự như hợp nhất Task Arithmetic và TIES, thực hiện các hoạt động Random Drop và Re-scale trong vector tác vụ θd - θinit và θp - θinit để loại bỏ trọng số dư thừa. Chúng tôi thấy rằng hợp nhất DARE hoạt động tốt nhất khi chúng tôi chọn Mixtral-8x7B làm mô hình cơ sở. Hợp nhất DARE-linear là những gì ban đầu được đề xuất bởi Yu et al. (2023). Trong hợp nhất DARE-TIES, hoạt động Elect từ Yadav et al. (2024) được thêm vào sau hoạt động Re-scale.

Chúng tôi tiến hành các thí nghiệm dựa trên việc triển khai từ MergeKit (Goddard et al., 2024).4

Trong Bảng 17 (trang trước), chúng tôi đo lường hiệu suất của các LM đánh giá sử dụng các phương pháp hợp nhất khác nhau. Trong các benchmark đánh giá trực tiếp, DARE-Linear đạt hiệu suất tốt nhất, tiếp theo là DARE-TIES và hợp nhất Linear. Trong các benchmark xếp hạng theo cặp, Task Arithmetics đạt hiệu suất tốt nhất, chỉ với sự khác biệt tối thiểu so với các phương pháp khác. Trung bình, DARE-Linear hoạt động tốt nhất. Dựa trên những kết quả này, chúng tôi đã huấn luyện Prometheus-2-7B với hợp nhất DARE-Linear. Chúng tôi cũng chọn huấn luyện Prometheus-2-8x7B sử dụng hợp nhất DARE-Linear. Mặc dù phương pháp hợp nhất tối ưu có thể khác nhau, chúng tôi đã không tiến hành thêm thí nghiệm do hạn chế về tính toán. Các công việc tương lai có thể khám phá liệu những phát hiện này có đúng không.

4https://github.com/arcee-ai/mergekit

--- TRANG 16 ---
H BỘ SƯU TẬP SỞ THÍCH
Prompt Tăng cường

Prompt để Tạo ra Phản hồi Bằng lời
trong Xếp hạng theo Cặp

###Mô tả Nhiệm vụ:
Một hướng dẫn (có thể bao gồm Input bên trong), hai phản hồi để đánh giá (được ký hiệu là Phản hồi A và Phản hồi B), một câu trả lời tham chiếu, và một rubric điểm số đại diện cho tiêu chí đánh giá được cung cấp.

1. Viết một phản hồi chi tiết giải thích tại sao {sub_str}, tập trung nghiêm ngặt vào các khía cạnh được nhấn mạnh trong tiêu chí đánh giá.

2. Trong khi viết phản hồi, hãy so sánh giữa Phản hồi A, Phản hồi B, và Câu trả lời Tham chiếu. Thay vì kiểm tra Phản hồi A và Phản hồi B riêng biệt, hãy đi thẳng vào điểm và đề cập về những điểm chung và khác biệt giữa chúng.

3. Trong khi viết phản hồi, đừng bắt đầu bằng cách đề cập {sub_str} trong câu đầu tiên. Thay vào đó, hãy cố gắng viết một quy trình lý luận đi sâu vào những điểm chung và khác biệt của hai phản hồi và đề cập {sub_str} ở phần cuối của lời biện minh.

4. Trong phản hồi, đừng đề cập rõ ràng về câu trả lời tham chiếu. Ví dụ, đừng sử dụng các cụm từ như "So với câu trả lời tham chiếu". Giả sử rằng bạn biết vốn có câu trả lời tham chiếu có thể được sử dụng để xác định các chi tiết không có trong cả hai phản hồi đang được đánh giá.

5. Vui lòng không tạo ra bất kỳ phần mở đầu, kết thúc, và giải thích nào khác. Chỉ viết phản hồi.

6. Trong phản hồi, tạo ra một chuỗi cụm từ "[END]" sau khi bạn hoàn thành.

###Hướng dẫn: {instruction}
###Phản hồi A: {response_A}
###Phản hồi B: {response_B}
###Câu trả lời Tham chiếu: {reference_answer}
###Rubric Điểm số: {criteria}
###Phản hồi:

I Prompt Đánh giá Trực tiếp

Prompt Hệ thống Đánh giá Trực tiếp
Bạn là một trợ lý thẩm phán công bằng được giao nhiệm vụ cung cấp phản hồi rõ ràng, khách quan dựa trên tiêu chí cụ thể, đảm bảo mỗi đánh giá phản ánh các tiêu chuẩn tuyệt đối được đặt ra cho hiệu suất.

Mẫu Prompt Đánh giá Trực tiếp
###Mô tả Nhiệm vụ:
Một hướng dẫn (có thể bao gồm Input bên trong), một phản hồi để đánh giá, và một rubric điểm số đại diện cho tiêu chí đánh giá được cung cấp.

1. Viết một phản hồi chi tiết đánh giá chất lượng của phản hồi nghiêm ngặt dựa trên rubric điểm số đã cho, không đánh giá một cách tổng quát.

2. Sau khi viết phản hồi, viết một điểm số là số nguyên từ 1 đến 5. Bạn nên tham khảo rubric điểm số.

3. Định dạng đầu ra nên như sau:
"Phản hồi: (viết phản hồi cho tiêu chí)
[KẾT QUẢ] (một số nguyên từ 1 đến 5)"

4. Vui lòng không tạo ra bất kỳ phần mở đầu, kết thúc, và giải thích nào khác.

###Hướng dẫn cần đánh giá:
{orig_instruction}
###Phản hồi cần đánh giá:
{orig_response}
###Rubric Điểm số:
{score_rubric}
###Phản hồi:

J Prompt Xếp hạng theo Cặp

Prompt Hệ thống Xếp hạng theo Cặp
Bạn là một trợ lý thẩm phán công bằng được giao nhiệm vụ đưa ra phản hồi sâu sắc so sánh hiệu suất cá nhân, làm nổi bật cách mỗi người đứng so với những người khác trong cùng nhóm.

--- TRANG 17 ---
Mẫu Prompt Xếp hạng theo Cặp
###Mô tả Nhiệm vụ:
Một hướng dẫn (có thể bao gồm Input bên trong), một phản hồi để đánh giá, và một rubric điểm số đại diện cho tiêu chí đánh giá được cung cấp.

1. Viết một phản hồi chi tiết đánh giá chất lượng của hai phản hồi nghiêm ngặt dựa trên rubric điểm số đã cho, không đánh giá một cách tổng quát.

2. Sau khi viết phản hồi, chọn phản hồi tốt hơn giữa Phản hồi A và Phản hồi B. Bạn nên tham khảo rubric điểm số.

3. Định dạng đầu ra nên như sau:
"Phản hồi: (viết phản hồi cho tiêu chí)
[KẾT QUẢ] (A hoặc B)"

4. Vui lòng không tạo ra bất kỳ phần mở đầu, kết thúc, và giải thích nào khác.

###Hướng dẫn:
{orig_instruction}
###Phản hồi A:
{response_A}
###Phản hồi B:
{response_B}
###Rubric Điểm số:
{score_rubric}
###Phản hồi:
