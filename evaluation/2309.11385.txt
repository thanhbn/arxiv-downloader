# 2309.11385.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/evaluation/2309.11385.pdf
# File size: 387740 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2309.11385v1  [cs.CL]  20 Sep 2023SAFURAI 001: N EWQUALITATIVE APPROACH FOR
CODE LLM E VALUATION
Cifarelli D., Boiardi L. & Puppo A.∗
Safurai Team
Genoa
Corso Firenze 39/3, 16136, Italy
info@safurai.com
ABSTRACT
This paper presents Safurai-001, a new Large Language Model (LLM) with signif-
icant potential in the domain of coding assistance. Driven b y recent advancements
in coding LLMs, Safurai-001 competes in performance with th e latest models
like WizardCoder [Xu et al. (2023)], PanguCoder [Shen et al. (2023)] and Phi-
1 [Gunasekar et al. (2023)] but aims to deliver a more ”conver sational” interac-
tion. By capitalizing on the progress in data engineering (l atest techniques of data
transformation and prompt engineering) and instruction tu ning, this new model
promises to stand toe-to-toe with recent closed and open sou rce developments.
Recognizing the need for an efﬁcacious evaluation metric fo r coding LLMs, this
paper also introduces GPT4-based MultiParameters: an eval uation benchmark that
harnesses varied parameters to present a comprehensive ins ight into the model’s
functioning and performance. Our assessment shows that Saf urai-001 can out-
perform GPT-3.51by 1.58% and WizardCoder by 18.78% in Code Readability
parameter and more.
1 I NTRODUCTION
Code large language models are one of the most promising appl ications of LLMs and they have
drawn a lot of interest from both academia and industry becau se of their extraordinary aptitude for
tasks involving codes.
The closed-source models landscape is dominated by OpenAI m odels: GPT-3.5 and GPT-4 [OpenAI
(2023)] (actually, the best ranked model in HumanEval pass@ 1 chart). Before the release of Star-
coder [Li et al. (2023)], open-source world fall far behind c ommercial models in terms of model
size, capability, and performance.
However, this paradigm started changing with the advent of S tarcoder. It have been frequently
employed as a foundational model in the development of other models with great results like Wiz-
ardCoder [Xu et al. (2023)] and PanguCoder [Shen et al. (2023 )], diminishing signiﬁcantly the per-
formance gap between open and closed-source coding LLMs. La tely, also Meta introduced a new set
of 12 LLMs available for commercial use: LLAMA2 release [Tou vron et al. (2023)]. Teams from
all over in the world could use LLAMA2 as new foundation model for Coding LLMs, in competition
with StarCoder.
In the latest publications in Coding LLMs ﬁeld, many efforts have been made regarding for data
engineering (Phi-1) and instruction tuning (WizardCoder) .
We have tried to capitalize on all the latest innovations in t he ﬁeld of Coding LLMs to develop a
high-performance model that is in line with the latest open- source releases.
In a nutshell, we make the following contribution:
∗https://www.safurai.com/team
1https://openai.com/blog/introducing-chatgpt-and-whi sper-apis
1

--- PAGE 2 ---
• We present Safurai-001, a model that competes with WizardC oder for performances and
tries to have a more ”conversational” approach.
• We introduce a new evaluation benchmark for coding LLMs, GP T4-based MultiParameters
Evaluation Benchmark. This benchmark embraces multiple cr ucial parameters to offer
deeper insights into the model’s performance.
2 R ELATED WORK
2.1 C ODING LARGE LANGUAGE MODELS
The impressive Codex model, with its 12 billion parameters, illustrates a remarkable capacity to
solve approximately 72% of Python programming challenges. This achievement has paved the
way for the development of other advanced code generation mo dels, including AlphaCode [Li et al.
(2022)], PaLM-Coder [Chowdhery et al. (2022)], and PanGu-C oder [Shen et al. (2023)]. However,
one notable drawback is the lack of open-source availabilit y of these state-of-the-art models, a void
that has subsequently been ﬁlled by the release of several op en-source variants such as CodeParrot2,
PolyCoder3, PyCodeGPT4, SantaCoder [Allal et al. (2023)], and StarCoder [Li et al. ( 2023)]. This
new wave of open-source models have reinvigorated the code g eneration ﬁeld.
Furthermore, the sequential expansions of code generation application scopes are reﬂective of
the ﬁeld’s ever-growing practicality. For instance, CodeG eeX [Zheng et al. (2023)], BLOOM
[Workshop (2022)] and ERNIE-Code [Chai et al. (2022)] have b een developed to enable multilin-
gual modeling. JuPyT5 [Chandel et al. (2022)] was trained us ing an extensive corpus of Jupyter
notebooks, its primary objective being to enhance the proce ss of interactive programming. Mod-
els like DocCoder and APICoder [Zan et al. (2022)] have also b een constructed to equip language
models with the functionality to call APIs. Moreover, a numb er of models, including InCoder
[Fried et al. (2022)], SantaCoder, and StarCoder, support c ode generation at arbitrary locations.
Recently, some groups have been utilizing instructional tu ning techniques to tap into the vast po-
tential knowledge contained within extensive language mod els. This process involves carefully
reﬁning these models with high-quality datasets. In terms o f code generation, WizardCoder (15B),
PanguCoder and phi-1 (1.3B) models stand out for their exemp lary performance. This was achieved
through careful ﬁne-tuning with data generated by OpenAI’s GPT-3.5 and GPT-4.
2.2 C ODE, ALGEBRA AND LOGIC DATASET LANDSCAPE
The landscape of code, logic, and algebra datasets is teemin g with new possible resources that can
be used for ﬁnetuning Coding LLMs (the majority of them are op en source).
The most important coding dataset in this ﬁeld is CodeAlpaca -20k5. Many models, like PanGu-
Coder or WizardCoder, have structured their dataset also th rough the manipulation of Code Alpaca
with data augmentation techniques. Also Phi-1 [Gunasekar e t al. (2023)] coding model has been
trained with ﬁltered code-language dataset, which is a subs et of The Stack6(it contains over 6TB of
permissively-licensed source code ﬁles covering 358 progr amming languages).
The open source community offers a variety of resources in Q& A format that are helpful for ﬁne-
tuning LLMs in terms of datasets for mathematics and logic. T he majority of these datasets were
produced by T57, GPT-3.5, GPT-4, or a combination of these models (although OpenAI policies can
still be interpreted in this context).
2https://huggingface.co/codeparrot/codeparrot
3https://huggingface.co/NinedayWang/PolyCoder-2.7B
4https://github.com/microsoft/PyCodeGPT
5https://huggingface.co/datasets/HuggingFaceH4/CodeA lpaca 20K
6https://huggingface.co/datasets/bigcode/the-stack
7https://huggingface.co/docs/transformers/model doc/t5
2

--- PAGE 3 ---
2.3 L ATEST TECHNIQUES FOR PROMPT ENGINEERING
In this section, we outline the primary prompt engineering m ethods combined with prompt engi-
neering, applied to the coding LLMs ﬁeld:
• Chain of Thoughts (CoT): Wei et al.[2023] report that large language models can enable
the emergence of reasoning abilities when prompted in this w ay. A chain of thought is a
series of intermediate natural language reasoning steps th at lead to the ﬁnal output.
• CoT and Self-Consistency: this is the natural evolution of CoT technique. It ﬁrst samples
a diverse set of reasoning paths instead of only taking the gr eedy one, and then selects the
most consistent answer by marginalizing out the sampled rea soning paths. Self-consistency
leverages the intuition that a complex reasoning problem ty pically admits multiple different
ways of thinking leading to its unique correct answer (Wang e t al.[2022]).
• Tree of Thoughts (ToT): Yao et al.[2023] report that ToT all ows LMs to perform deliber-
ate decision making by considering multiple different reas oning paths and self-evaluating
choices to decide the next course of action, as well as lookin g ahead or backtracking when
necessary to make global choices.
• Teacher CoT: Ho et al.[2023] demonstrated that through the augmentation of the prompt
with an ”educational” explanation generated by a larger mod el, excellent results are ob-
tained in the ﬁnetuning of smaller models. Also Mukherjee et al.[2023] used this ”teach-
ing” approach to develop Orca model.
• EvolInstruct: Luo et al.[2023] proposed a new approach for data augmentation that
achieved important results. They found that LLMs can make gi ven instructions more com-
plex and difﬁcult using speciﬁc prompts. Additionally, mod els can generate entirely new
instructions that are equally complex but completely diffe rent. Using this discovery, the
WizardCoder creators can iteratively evolve an initial ins truction dataset, improving difﬁ-
culty level and expanding its richness and diversity
2.4 L ATEST EVALUATION TECHNIQUES FOR CODING LLM S(HUMAN EVAL, MBPP,
MULTI PL-E, H UMAN EVAL PACK)
This subchapter provides an overview of the benchmarks curr ently being used to evaluate Coding
LLMs.
1. HumanEval8: This general standard benchmark holds a set of 163 problems constrained to
Python language. It assesses whether the model’s code succe ssfully passes all the tests
and provides binary and quantitative results only. General ly, there are 3 types of Hu-
maneval evaluation: pass@1, pass@10 and pass@100. They are different in the number of
”chances” given to the tested model to generate the right ans wer to the problem.
2. MultiPL-E9: Based on the premise of HumanEval, MultiPL-E takes this ben chmark and
translates its results to numerous programming languages l ike C++, Rust, Go, Java and
more. With the same ranking structure as HumanEval, this too l also provides a quantitative
binary evaluation.
3. MBPP10: Consisting of approximately 1000 programming issues sour ced from Python pro-
grammers, this benchmark is geared towards beginners. It of fers a description of tasks,
corresponding code solutions, and three automatic test cas es. Its focus is on programming
fundamentals and the application of standard library funct ions.
4. HumanEval Pack: This innovative evaluation method by Big Code’s11team brings a fresh
perspective to the assessment of Coding LLMs. It expands the HumanEval by engaging
three different stages: Fix, Explain, and Synthesize. The “ Fix” stage evaluates the model’s
ability to rectify code functions containing subtle bugs, t he “Explain” stage assesses the
model’s capacity to generate clear code explanations, whil e the “Synthesize” stage gauges
how effectively the model synthesizes code given a natural l anguage instruction.
8https://huggingface.co/datasets/openai humaneval
9https://huggingface.co/datasets/nuprl/MultiPL-E
10https://huggingface.co/datasets/mbpp
11https://huggingface.co/bigcode
3

--- PAGE 4 ---
3 M ETHODS
3.1 D ATASET OVERVIEW
Overall, for the generation of Safurai-001 (starting from S tarCoder 15B [Li et al. (2023)]) we used
a dataset of 200,000 Q&A examples.
As we have seen from the publications of WizardCoder [Xu et al . (2023)] and Phi-1
[Gunasekar et al. (2023)], data quality is essential for the generation of a performing LLM cod-
ing. For this we have used the latest data augmentation and pr ompting engineering techniques to
generate the datasets. Furthermore, we involved some datas ets and data related to basic logical and
algebraic reasoning, in order to boost the comprehension St arCoder abilities.
3.2 I NITIAL DATASET SOURCES
These are our proprietary datasets that we selected for Safu rai-001 training:
• Safurai Code Dataset (163k)
• Logic Q&A Dataset (22k)
• Math Q&A Dataset (15k)
3.3 D ATA TRANSFORMATION
We employed an additional LLM to enhance the educational pot ential present within the model. By
incorporating both a problem and its solution, we prompted t he model to elucidate the reasoning
process leading to the solution.
Our experimentation with various techniques led to the crea tion of a diverse dataset. The following
are some of the methods we harnessed to augment the education al value:
Trasformation techniques used for our initial datasets:
• Chain of thoughts reasoning
• Tree of thoughts reasoning
• Show potential errors
• Focus on edge cases and explain unit tests
• Highlight question requests in a more objective manner
• Coding lesson related to the topic
• Teaching the response
3.3.1 D ATA TRANSFORMATION PROMPT EXPERIMENTS EXAMPLES :
ToT Code Instructor
"Aspartofanexercise inimproving AIcode
explanations, yourtaskisasfollows:\n"
f"Question: \n\n{row[’instruction’]}\n\n"
f"Existing Answer: \n\n{row[’output’]}\n\n"
"Thegivenanswer, thoughtechnically correct, doesn’t
offerinsights intotheunderlying thought process.\n
"
"Yourmission: deviseacomprehensive step-by-step plan
leading totheanswer. \n"
"Thisshouldinclude plainlanguage explanations and
corresponding code,neatlypresented inmarkdown. "
"Youranswerwillserveasamoreinformative substitute
fortheinitial one.Striveforsimplicity andhuman
-likecommunication.\n\n"
4

--- PAGE 5 ---
"Butthere’s atwist:envisage acollaboration between
threeexperts, eachaddingapiecetothepuzzle. "
"Aftercontributing astep,theydiscuss itwiththe
groupbeforeproceeding. "
"Ifanexpertdetermines theirstepisincorrect, they
stepawayfromthetask."
"Theexercise concludes whenacomprehensive correct
answerhasbeenachieved, orallexperts have
withdrawn."
CoT Code Instructor
"I’mtraining acode-writing AIandIneedyourhelp.\n
"
f"Here’s asamplequestion: \n\n{row[’instruction’]}\n\n
"
f"Andhere’sananswer: \n\n{row[’output’]}\n\n"
"Thegivenansweristoobasicanddoesn’t explain the
stepstakentoarriveatit.\n"
"Couldyouhelpcreateastep-by-step plantoreachthis
answer? \n"
"Eachstepshouldbesimpleandunderstandable. \n"
"Youranswershouldinclude thisplanandtheactual
codeinmarkdown inoneblock."
"Youranswerwillreplace theoneI’veshownyou.It
shouldsoundhuman!"
"Makesurenottocutoffwordsorsentences midway."
Teacher Code Instructor
"Imagine youareaprogramming experttaskedwith
providing clearandformalprogramming assistance. "
f"Youarepresented withthisproblem: \n\n{row[’
instruction’]}\n\n"
"Youhavetwoprimary goals:"
"1)explain theprocess tosolvetheproblem stepby
step.Explain theprocess tosolvetheproblem step
bystepinaconversational manner, withafewbullet
points."
"2)include specific examples ofcommonerrorsthat
shouldbeavoided, accompanied bycodesnippets
illustrating thesemistakes. Tagthesecodesnippets
as’ErrorExample’. "
"Whenproviding thecorrect solution, ensurethereare
comments inthecodetoenhance itscomprehensibility
,addressing crucial pointsandpossible mistakes. "
CoT Logic Instructor
"Ihaveadataset withquestions andresponses about
logical problems. \n"
f"Thisisonelogical problem: \n\n{row[’instruction’]}\
n\n"
f"Thisistheprovided solution totheproblem: \n\n{row
[’output’]}\n\n"
"Theprovided solution istoosimpleanddoesn’t explain
theprocess togetit.\n"
"Canyoupleaseprovide adetailed reasoning togetthe
solution? \n"
5

--- PAGE 6 ---
3.4 S AFURAI -001 D ATASETS USED FOR FINETUNING :
• Safurai Code Instructor (16k) (ﬁltered examples from init ial Safurai Code Dataset, not
transformed)
• Logic Q&A Dataset (22k)
• Teacher Code Instructor (70k)
• Math Q&A Dataset (15k)
• Teacher Code Instructor with Potential Errors (21k)
• ToT Code Instructor (30k)
• CoT Code Instructor (26k)
Figure 1: Safurai-001 Datasets Compartition
3.5 F INE-TUNING METHODS
The training process for the StarCoder model was carried out on eight 100 A100 80GB graphics
cards, with the full network trained for 10 hours. This proce ss was implemented using the Deep-
Speed ZeRO-3 framework. In order to ﬁne-tune the StarCoder m odels, we leveraged the GitHub
repository of Big Code: https://github.com/bigcode-proj ect/starcoder/tree/main/chat
Given that the focus was on instruction tuning, the data was w rapped in a dialogue format. An
example of this is illustrated below:
{
"messages": [
{
"content": "Is itpossible toimagine asociety
without law?",
"role": "user"},
{
"content": "It isdifficult toimagine a
society thatisabletobemaintained
without anysemblance ofLaw.",
"role": "assistant",
},
{
6

--- PAGE 7 ---
"content": "It seemslikeyouconsider the
absence oflawequaltotheabsence of
anything thatcouldguidethebehaviour of
theindividual.",
"role": "user",
},
{
"content": "You arecorrect thatthereare
otherfactors thatcanguidebehavior ina
society andplayaroleinshaping
individuals’ behavior andinteractions with
eachother.However, eveninsocieties where
thesefactors arepresent, lawsstillserve
animportant roleinmaintaining social
orderandresolving conflicts.",
"role": "assistant",
}
]
}
We also wrapped the dialogue with special tokens. Here’s an e xample of how it looks:
<|system|>
Belowisa dialogue between a human andAI assistant
called StarChat.
<|end|>
<|user|>
Is it possible to imagine a society without law?<|end|>
<|assistant|>
Itisdifficult to imagine ...<|end|>
<|user|>
It seems like you ...<|end|>
<|assistant|>
You are correct ...<|end|>
<|user|>
Yeah, but laws are complicated ...<|end|>
The training process involved setting up several hyperpara meters. The hyperparameters settings for
the training are detailed as follows:
• Batch size: 512
• Learning rate: 2e-5
• Epochs: 3
• Max length: 2048
• Warmup step: 30
• Learning rate (LR) scheduler: cosine
3.6 E VALUATION
Deepening our grasp of the capabilities and scope of LLM mode ls is substantial to reﬁning their
application in the real world. However, we found the current ly available evaluation methods such
as HumanEval12to be limited in their ability to provide a comprehensive ana lysis of these models’
abilities. This led to the invention of the GPT4-based Multi Parameters Evaluation method, a qual-
itative alternative designed to provide a more nuanced unde rstanding of the performance of coding
LLMs.
12https://huggingface.co/datasets/openai humaneval
7

--- PAGE 8 ---
These new qualitative criteria enable us to explore more use -cases outside the conventional binary
pass-fail result of the existing quantitative methods, thu s providing a more detailed narrative that
identiﬁes the unique strengths (or weaknesses) of each mode l. HumanEval, MBPP13and MultiPL-
E14. Most of them lean towards a quantitative rather than a quali tative evaluation, leaving out crucial
aspects of the models’ capabilities. As such, we justify the innovation and necessity of our GPT4-
based MultiParameters Evaluation method in addressing thi s gap.
3.7 N EW EXPERIMENTS ON THE EVALUATION (GPT4-B ASED MULTI PARAMETERS
EVALUATION )
Seeking to explore the qualitative aspects of our LLM model S afurai, we experimented with a new
evaluation approach based on GPT-4 [OpenAI (2023)].
3.7.1 GPT4- BASED ANALYSIS
This method involved assessing 20 (GPT-4 HE-20) and 40 (GPT- 4 HE-40) answers derived from
each of the models being compared, obtained through the Huma nEval dataset. GPT-4 was used
to determine the performance of models: Safurai, Claude15, WizardCoder [Xu et al. (2023)], Chat-
GPT16, and Starchat Alpha Prompted17. We combined both the problem and the ﬁve responses into
a single GPT-4 prompt plus the speciﬁc tests of the problem, w hich was asked to rate each response
on a scale of 0 to 100 – the best possible score. Moreover, to de epen our understanding, we asked
GPT-4 to provide a concise description detailing the reason ing behind its ratings. This is the GPT-4
prompt: I asked this to 4 different AI models: [problem] This is the ﬁr st model answer: [answer]
This is the second model answer: [answer] This is the third mo del answer: [answer] This is the
fourth model answer: [answer] These are the tests for the cod e solution of the problem: [tests]
Please rate each answer from 0 to 100 (best answer possible). Consider whether the code fully
solves the problem, if it handles all edge cases, and if it con tains all necessary functionalities. Also,
provide a short explanation for each rating. This way, in addition to quantifying performance, our
evaluation strategy reveals valuable insights into each mo del’s strengths and weaknesses. (Table 2)
Table 2: Results of GPT-4 HumanEval-20 and GPT-4 HumanEval- 40
Date Model GPT-4
HumanEval-20GPT-4
HumanEval-40
Closed source models
2022 Nov GPT3.5-turbo 81,5% 80,875%
2023 March Claude 75% 78,7%
Open source models
2023 May Starchat-Alpha
prompted64,3% 62,4%
2023 June WizardCoder 74,4% 74,7%
2023 June Safurai-001 85% (+3.5% ) 84,875% (+4% )
The experiments detailed above provided a holistic process for comparative model evaluation. By
evaluating 20 (GPT-4 HE-20) and 40 (GPT-4 HE-40) responses f rom each compared model using
the HumanEval dataset, we generated valuable quantitative data and underlying qualitative insights
on model performance.
13https://huggingface.co/datasets/mbpp
14https://huggingface.co/datasets/nuprl/MultiPL-E
15https://www.anthropic.com/index/introducing-claude
16https://openai.com/blog/chatgpt
17https://huggingface.co/HuggingFaceH4/starchat-alpha
8

--- PAGE 9 ---
However, we recognized that the comprehensive ratings prov ided by GPT-4, while integral to the
evaluation process, cannot fully capture the nuanced speci ﬁcities inherent in each model. Compre-
hensive ratings bootstrap a model’s ability to resolve a pro blem and generate correct code, but they
fall short in illuminating aspects such as efﬁciency, reada bility, best coding practices, and relevance
to problem. These key dimensions, though less evident, are e qually vital to a model’s utility and
impact in real-world software development scenarios.
To alleviate these shortcomings and provide a more detailed , multidimensional, and nuanced ap-
praisal of the models’ functionalities, we introduced a fou r-parameter rating system.
3.7.2 GPT4- BASED MULTI PARAMETERS EVALUATION BENCHMARK
To understand even more about the model responses, we create d a Multi-Parametric GPT4-based
Evaluation system. The singular GPT-4 prompt, containing b oth the problem and the four respective
solutions, was not only rated generally but was also dissect ed based on four distinct parameters.
These were:
1. Code Correctness and Completeness: This involved gaugin g whether the code runs without
errors and if it fully solves the problem, considering all po tential edge cases.
2. Efﬁciency: This measurement determined the optimizatio n level of the code. It scrutinized
whether the code utilizes resources capably, and whether it scales efﬁciently as input size
increases.
3. Readability and Best Practices: This criterion evaluate d the clarity of the written code,
whether it’s easily comprehensible, and if it conforms to es tablished coding conventions
and best practices.
4. Relevance to Problem (On-point Answer): This parameter e valuated how directly the code
solves the given problem, assessing whether the solution im plemented is efﬁcacious and
appropriate.
These are the GPT-4 prompts used for each parameter:
1.I asked this to 4 different AI models: [problem] This is the ﬁr st model answer: [answer]
This is the second model answer: [answer] This is the third mo del answer: [answer]
This is the fourth model answer: [answer] These are the tests for the code solution of the
problem: [tests] Please rate each answer from 0 to 100 (best a nswer possible) based on
Code Completeness. Consider whether the code fully solves t he problem, if it handles all
edge cases, and if it contains all necessary functionalitie s. Also, provide a short explanation
for each rating.
2.I asked this to 4 different AI models: [problem] This is the ﬁr st model answer: [answer]
This is the second model answer: [answer] This is the third mo del answer: [answer] This is
the fourth model answer: [answer] These are the tests for the code solution of the problem:
[tests] Please rate each answer from 0 to 100 (best answer pos sible) on Efﬁciency. This
entails considering how well-optimized the code is, how fru gally it uses system resources,
and its scalability or robustness for larger inputs. Consid er both its time complexity (ability
to perform tasks quickly) and space complexity (how much mem ory the program uses).
Also, provide a short explanation for each rating.
3.I asked this to 4 different AI models: [problem] This is the ﬁr st model answer: [answer]
This is the second model answer: [answer] This is the third mo del answer: [answer]
This is the fourth model answer: [answer] These are the tests for the code solution of
the problem: [tests] Please rate each answer from 0 to 100 (be st answer possible) based
on its Helpfulness and Educational Value. Consider whether the answer provides clear
explanations, whether it’s easy to follow and understand, w hether it teaches you something
valuable about the problem or the coding concepts involved, and whether it gives you new
insights that could help you in future similar problems. Als o, provide a short explanation
for each rating.
4.I asked this to 4 different AI models: [problem] This is the ﬁr st model answer: [answer]
This is the second model answer: [answer] This is the third mo del answer: [answer]
This is the fourth model answer: [answer] These are the tests for the code solution of the
9

--- PAGE 10 ---
problem: [tests] Please rate each answer from 0 to 100 (best a nswer possible) based on
its Relevance to Problem (On-point answer). Consider how di rectly the code answers the
problem, if it provides an adequate and appropriate solutio n, and whether it implements
the requirements stated in the problem. Also, provide a shor t explanation for each rating.
3.7.3 W HYGPT-4?
Choosing GPT-4 as the foundation for our novel evaluation me thod was a clear and strategic deci-
sion. At the current state-of-the-art, GPT-4 showcases the most superior performance amongst all
existing coding models.
In addition to its exemplary robustness and efﬁciency, GPT- 4 boasts an extensive range of applica-
tions, making it an ideal model for evaluating less potent on es in diverse facets. This ability allows
the evaluation process to be as comprehensive as possible, s crutinizing every model under multiple
parameters and therefore obtaining an all-encompassing pe rformance proﬁle.
However, it’s worth noting that there are inherent limitati ons associated with GPT-4. These limi-
tations, reﬂected as weaknesses in our newly proposed evalu ation system, include potential biases
carried over from GPT-4 behaviours. Nevertheless, as long a s no other model matches or surpasses
GPT-4’s aptitude, these weaknesses remain relatively negl igible. We reckon that at this stage, GPT-
4 presents the most accurate measure for performance indica tors considered in our GPT4-based
MultiParameters Evaluation methodology.
Thus, until a more advanced model than GPT-4 enters the arena , our focus remains on deriving
maximum beneﬁt from this superior model, optimizing its use in revealing the strengths and areas
of improvement of each evaluated coding model.
4 E XPERIMENTAL RESULT
4.1 M ODEL PERFORMANCES
Example of Multi-Turn conversation with Safurai-001
USER : Hello Safu!
Safurai-001 : Hello! How can I assist you today?
USER : What’s your level of coding in Python?
Safurai-001 : As an AI language model, I have been created and trained to ha ve a strong
understanding of Python programming language. I can help wi th a wide range of topics and
tasks in Python, from basic syntax and concepts to more advan ced algorithms or frameworks.
Feel free to ask me anything related to Python coding, and I’l l do my best to assist you!
USER : How can I write a function in Python to ﬁnd the median of a list ? Please include the
time and space complexity of the solution.
10

--- PAGE 11 ---
Safurai-001 : The median of a list is the middle value when the list is sorte d in ascending
order. If the list has an odd number of elements, the median is the middle element. If the list
has an even number of elements, the median is the average of th e two middle elements.
To solve this problem, we can follow these steps:
1. Sort the list in ascending order.
2. Check if the length of the list is odd or even.
3. If the length is odd, return the middle element.
4. If the length is even, calculate the average of the two midd le elements and return it.
Here is the implementation of the median function:
def median(l: list):
l.sort() # Sort the list in ascending
order
length = len(l)
if length % 2 == 1: # Check if the
length is odd
return l[length // 2] # Return the
middle element
else:
mid1 = l[length // 2] # First middle
element
mid2 = l[length // 2 - 1] # Second
middle element
return (mid1 + mid2) / 2 # Return
the average of the two middle
elements
Test cases:
print(median([3, 1, 2, 4, 5])) # Output: 3
print(median([-10, 4, 6, 1000, 10, 20])) #
Output: 15.0
The time complexity of this solution is O(n log n), where n is t he length of the input list.
This is because the sorting operation takes O(n log n) time. T he space complexity is O(1) as
we are not using any additional data structures.
4.2 T RANSFORMED DATASET EXAMPLES
Original Input
Write a JavaScript code to loop over all elements in the given array. Input: numbersArray =
[45, 6, 23, 12, 35]
Original Output
for(let i = 0; i ¡ numbersArray.length; i++) console.log(nu mbersArray[i]);
Augmented Output
11

--- PAGE 12 ---
Sure, I’d be happy to help! 1. Start by declaring the array var iable:
const numbersArray = [45, 6, 23, 12, 35]
2. We want to loop over all elements in the array, so we need to u se a for loop. 3. Declare
our index variable i and set it to 0 4. Deﬁne the loop condition ; we want to continue looping
as long as the index variable is less than the length of the arr ay. 5. Use the console.log()
method to print out the value of the current element in the arr ay. 6. Finally, increment the
index variable by 1 to move onto the next element in the array. Here’s the ﬁnal code:
const numbersArray = [45, 6, 23, 12, 35];
for (let i = 0; i < numbersArray.
length; i++) { console.log(
numbersArray[i]); }
This code will loop over all elements in the array and print ou t their values to the console.
4.3 E VALUATION RESULTS
4.3.1 H UMAN EVAL
HumanEval18, established by OpenAI, has been the go-to benchmark in eval uating coding AI mod-
els. It focuses on the model’s code-generation capacity bas ed on precise requests and provides
standard solutions and tests. MultiPL-E19extends the boundaries of the current benchmarks by
translating them to new languages and thereby becoming a mas sive multi-language benchmarking
platform. It is continuously expanding in terms of number of programming languages, providing a
useful comparison point for models like Safurai.
Our model, Safurai001, achieved a pass@1 score of 50.61% on t he HumanEval benchmark with
n=20. (Table 1)
Table 1: Results of pass@1(%) on HumanEval
Model Size HumanEval
Closed source models
Alphacode Li et al. [2022] 540B 26.2
Codex Chen et al. [2021] 12B 28.8
Code-Cushman-001 OpenAI [2022] - 33.5
Code-Davinci-002 OpenAI [2022] - 47.0
GPT-3.5 OpenAI [2023] - 48.1
GPT-3.5 Luo et al. [2023] - 68.9
GPT-4 OpenAI [2023] - 67.0
GPT-4 Bubeck et al. [2023] - 82.0
Open source models
LLaMa Touvron et al. [2023] 65B 23.7
CodeT5+ Wang et al. [2023] 16B 30.9
StarCoder Li et al. [2023] 15B 33.6
WizardCoder Luo et al. [2023] 15B 57.3
Safurai001 [2023] 15B 50.61
However, the adoption of only these standards limits our ana lysis to quantitative metrics, thereby
losing some critical ﬂavors of the models.
18https://github.com/openai/human-eval
19https://huggingface.co/datasets/nuprl/MultiPL-E/vie wer/humaneval-rs/test?row=0
12

--- PAGE 13 ---
4.3.2 N EWQUALITATIVE EVALUATION BENCHMARK
We tested the models with the 40 selected problems of HumanEv al, already used in GPT4-based
Analysis. The GPT4-based MultiParameters Evaluation meth od elucidates areas for optimization,
explains why a speciﬁc response is superior, and signiﬁcant ly comprehends the speciﬁc code-
generation abilities of each model; thus providing a detail ed qualitative metric.
We found that this method reveals a plethora of valuable insi ghts into each model’s strengths and
weaknesses, enabling the development of targeted strategi es for enhancement. (Table 3)
Table 2: Results of GPT4-based MultiParameters HumanEval
Date Model Code
CorrectnessCode
EfﬁciencyCode
ReadabilityQuestion
Relevance
2022 Nov GPT-3.5-
turbo81.53% 80.33% 84.30% 82.25%
2023 March GPT-4 89.50% 89.38% 84.10% 90.93%
2023 June WizardCoder 60.7% 68.25% 67.1% 67.88%
2023 July Safurai-001 74.25% 75.45% 85.88%(+1.58%) 82.00%
We put our proposed GPT4-based MultiParameters Evaluation method to the test, using the same 40
selected problems from HumanEval which had been previously used in our GPT4-based Analysis.
The results obtained were intriguing, enlightening and inf ormative, revealing areas of optimization
and superiority in speciﬁc responses and highlighting the n eed to explore code-generation abilities at
a profound level. The qualitative data provided by this meth od was a treasure trove of information,
reaching depths previous evaluation methods did not ventur e.
Interestingly, this assessment unveiled nuances in model p erformance that were not entirely predic-
tive of functionality during actual deployment. For instan ce, despite WizardCoder [Xu et al. (2023)]
achieving higher scores in the HumanEval evaluation, it was observed that real-world day-to-day
usage, especially for developers, was not as smooth. The mod el’s conversational abilities seemed to
be somewhat lacking, making it hard to interact effectively with it. This was reﬂected in its score of
67.1 in the Code Readability category, a stark contrast with Safurai001’s impressive score of 85.88.
In shadowing the performance of conventional quantitative benchmarks like HumanEval and
MultiPL-E, we developed a new qualitative evaluation metho d: the GPT4-based MultiParameters
Evaluation. This unprecedented approach provided a broade r perspective of the nuances and intri-
cacies of LLM models, broadening the spectrum of their funct ionality and applications.
Models like Phi1 [Gunasekar et al. (2023)], developed by Mic rosoft Researchers, StarCoder
[Li et al. (2023)], and WizardCoder [Xu et al. (2023)], are ma inly evaluated using conventional
methods. While efﬁcient, these methods lack the ability to p rovide an exhaustive understanding
of the model’s capabilities, thus justifying the necessity for the development of our new evaluation
method.
The GPT4-based MultiParameters Evaluation method breaks n ew ground in the LLM models eval-
uation ﬁeld, enabling researchers to delve deeper into the f unctionality of these models and dramat-
ically enhancing their potential for improvement.
4.3.3 L IMITS OF GPT4- BASED MULTI PARAMETER EVALUATION BENCHMARK
•The GPT4-based MultiParameters Evaluation method can only evaluate up to a cer-
tain limit : It is benchmarked at the GPT4 level of performance. This imp lies that any
model exceeding the performance of GPT4 may not be reliably e valuated or accurately
assessed. This method, therefore, does not cater to the fast -paced advancements and con-
tinuous improvements in the AI development landscape.
•Variance in responses: Another constraint associated with the GPT4-based MultiPa ram-
eters Evaluation method is the variation in GPT4’s response . It does not always provide
consistent responses due to various reasons such as the diff erence in nature and complexity
13

--- PAGE 14 ---
of the problem. This inconsistency presents challenges in p roviding a robust evaluation of
the Coding AI models.
•Time-intensive: The GPT4-based MultiParameters Evaluation is a relatively time-
consuming process. Due to the complex and multifaceted natu re of this evaluation method,
it requires substantial time and resource compared to conve ntional methods. This could
limit its scalability and practical application in rapidly iterating development cycles.
•Qualitative nature: The GPT4-based MultiParameters Evaluation method mainly p ro-
vides a qualitative evaluation. It may not effectively hand le quantitative data or metrics,
and comparing those could be challenging. This could presen t limitations when models’
evaluation requires numerical comparisons or precise quan tiﬁable results.
4.3.4 E XAMPLE OF GPT4- BASED MULTI PARAMETER EVALUATION RESPONSE
This is an example of GPT4-based MultiParameter Evaluation response. The problem is the Num-
ber 0 of HumanEval Benchmark. About the answers, the ﬁrst mod el is Safurai-001, the second
ChatGPT, the third WizardCoder, and the fourth GPT4.
First Parameter: Code Correctness
First Model Answer: Rating: 90 Explanation: The code provid ed by the ﬁrst model is
thorough, complete and well-explained. It correctly imple ments the function to solve the
problem. It handles the basic functionality of checking if a ny two numbers in the list are
closer than the threshold. However, it does not explicitly h andle the edge case where the list
of numbers is empty.
Second Model Answer: Rating: 90 Explanation: The second mod el provides a complete
solution that is functional and concise. However, it does no t handle edge case where the list
could be empty or include only one element.
Third Model Answer: Rating: 80 Explanation: The third model provides a functional solu-
tion to the problem, but it lacks in explanation and handling edge case when the list is empty
or includes only one element.
Fourth Model Answer: Rating: 100 Explanation: The fourth an swer provides two complete
solutions, one with a time complexity of O(n2)and another efﬁcient one with O(nlogn). It
correctly handles all scenarios and edge cases and provides a good balance between expla-
nation, completeness and efﬁciency.
Second Parameter: Code Efﬁciency
Answer 1: 75/100 This answer provides a brute-force solutio n by comparing each element
with every other element in the list. Although clear and corr ect, this solution has a time
complexity of O(n2), which means it may not be efﬁcient for large lists of numbers , as it
doesn’t scale well with increasing input size.
Answer 2: 75/100 The solution given in Answer 2 is almost iden tical to the one in Answer
1, having a time complexity of O(n2)with a pairwise comparison. This lacks efﬁciency for
large inputs, and could beneﬁt from using more optimized app roaches, such as sorting the
list ﬁrst.
Answer 3: 75/100 Much like the ﬁrst two answers, answer 3 also uses a brute-force approach
in the solution. As it compares all pairs of numbers, it has a t ime complexity of O(n2),
meaning it won’t be efﬁcient for larger lists.
Answer 4: 90/100 Answer 4 is the best of the lot in terms of efﬁc iency. The answer actually
lists two different approaches: a brute-force approach men tioned in the ﬁrst three answers,
and a more efﬁcient approach that ﬁrst sorts the list before c omparing adjacent numbers.
The sorted approach has a time complexity of O(nlogn), which scales better with a growing
list of numbers than the O(n2)complexity of the brute-force methods. The only point
deduction is due to not explicitly explaining this crucial d ifference in complexity between
the two methods.
14

--- PAGE 15 ---
Third Parameter: Code Readability
First model’s answer: Rating: 90 Explanation: The ﬁrst mode l provides a detailed expla-
nation of the problem, the method used to solve it, and even br eaks down the entire code,
making it easy to understand for beginners too. The example p rovided at the end adds an
extra value to the answer as well. Yet, it still fails to expla in or consider any efﬁciency
measures or alternate ways to solve the problem.
Second model’s answer: Rating: 70 Explanation: The second m odel also correctly interprets
the problem and comes up with a valid approach. However, it’s less detailed than the ﬁrst
model, doesn’t provide a clear breakdown of the code, and off ers no example of how to use
the function.
Third model’s answer: Rating: 35 Explanation: The third mod el just provides the code
needed to solve the problem without any explanations or brea kdown of the code. Its answer
is correct but not very helpful or educational, especially f or someone trying to learn.
Fourth model’s answer: Rating: 95 Explanation: The fourth m odel does a great job of
explaining the problem and also introduces two different ap proaches to solve it, one being
more efﬁcient than the other. It not only offers insight into a more effective solution but also
explains why it’s more efﬁcient. But despite these advantag es, the model did not break down
the code or provide examples of usage.
Fourth Parameter: Question Relevance
Rating for the ﬁrst model: 90. The answer from the ﬁrst model i s quite thorough and directly
addresses the problem. The explained code solves the proble m as required and it perfectly
implements the requirement. However, it does not consider t he efﬁciency of the solution.
Rating for the second model: 85. The second model also provid es an adequate answer,
explaining the solution fairly well and implementing the de scribed function correctly. How-
ever, much like the ﬁrst model, this model does not consider a ny performance or efﬁciency
aspects in its solution.
Rating for the third model: 80. This AI model provided a corre ct and direct solution, but
with less explanation about the approach or notes on perform ance or usage. Therefore, the
rating is lower compared to the ﬁrst and second models.
Rating for the fourth model: 100. The fourth model provided t wo solutions: the ﬁrst one
similar to the previous models and the second one that consid ers performance improvements
by sorting the list ﬁrst. This model shows a greater understa nding of real-world requirements
by considering efﬁciency, thus making it the best answer.
4.3.5 C OMPARISON OF GPT4- BASED MULTI PARAMETERS AND HUMAN EVAL BENCHMARKS
Problem
fromtypingimportList
defhas_close_elements(numbers: List[ float],
threshold: float) ->bool:
"""
Check if in given list of numbers, are any
two numbers closer to each
other than given threshold.
>>> has_close_elements([1.0, 2.0, 3.0], 0.5)
False
>>> has_close_elements([1.0, 2.8, 3.0, 4.0,
5.0, 2.0], 0.3)
True
"""
15

--- PAGE 16 ---
Safurai-001 Response
First, we need to understand what the problem is asking us to d o. We have a list of numbers and we
need to check if any two of those numbers are closer than a cert ain threshold.
To solve this problem, we can use a loop to compare each number in the list to every other number
in the list. If the difference between the two numbers is less than or equal to the threshold, we know
that they are closer than the threshold.
Here is the code to solve this problem:
# Define a function that takes in a list of
numbers and a threshold
defhas_close_elements(numbers: List[ float],
threshold: float) ->bool:
# Loop through each number in the list
foriin range (len(numbers)):
# Loop through every other number in the
list
forjin range (i+1,len(numbers)):
# Check if the difference between the
two numbers is less than or equal
to the threshold
if abs(numbers[i] - numbers[j]) <=
threshold:
# If it is, return True
returnTrue
# If we have checked every pair of numbers
and none of them are closer than the
threshold, return False
returnFalse
Let’s break down the code:
• ‘def has close elements(numbers: List[ﬂoat], threshold: ﬂoat) - >bool:‘: This line deﬁnes
a function called ‘has close elements‘ that takes in a list of numbers (‘numbers‘) and a
threshold (‘threshold‘) and returns a boolean value (eithe r ‘True‘ or ‘False‘).
• ‘for i in range(len(numbers)):‘: This line starts a loop th at goes through each number in the
list.
• ‘for j in range(i+1, len(numbers)):‘: This line starts a ne sted loop that goes through every
other number in the list (starting from the next number after ‘i‘).
• ‘if abs(numbers[i] - numbers[j]) <= threshold:‘: This line checks if the difference between
the two numbers is less than or equal to the threshold.
• ‘return True‘: If the two numbers are closer than the thresh old, we return ‘True‘.
• ‘return False‘: If we have checked every pair of numbers and none of them are closer than
the threshold, we return ‘False‘.
To use this function, we can call it with a list of numbers and a threshold:
# Call the function with a list of numbers and
a threshold
result = has_close_elements([1.0, 2.8, 3.0,
4.0, 5.0, 2.0], 0.3)
# Print the result
print(result) # Output: True
In this example, the list of numbers ‘[1.0, 2.8, 3.0, 4.0, 5.0 , 2.0]‘ has two numbers that are closer
than the threshold of ‘0.3‘. The function returns ‘True‘ bec ause at least one pair of numbers is closer
than the threshold.
16

--- PAGE 17 ---
ChatGPT Response
The code you provided deﬁnes a function called has1 close elements that takes two arguments:
numbers, which is a list of ﬂoat numbers, and threshold, whic h is a ﬂoat representing the maximum
allowed difference between two numbers. The function check s if there are any two numbers in the
numbers list that are closer to each other than the given thre shold. It returns True if such a pair
exists, and False otherwise. Here’s an implementation of th e has close elements function based on
the code you provided:
fromtypingimportList
defhas_close_elements(numbers: List[ float],
threshold: float) ->bool:
foriin range (len(numbers)):
forjin range (i + 1, len(numbers)):
if abs(numbers[i] - numbers[j]) <
threshold:
returnTrue
returnFalse
This implementation uses nested loops to compare each pair o f numbers in the numbers list. The
abs() function is used to calculate the absolute difference between the two numbers, and if this dif-
ference is less than the threshold, the function returns Tru e. If no such pair is found after examining
all combinations, the function returns False. You can use th is function to check if there are any close
elements in a list of numbers as demonstrated in the examples you provided.
HumanEval Benchmark for this responses
For HumanEval tests, both of Safurai-001 and ChatGPT answer s are correct in the same way.
GPT4-Based MultiParameters Evaluation Benchmark for this responses
The ﬁrst model is Safurai-001, the second one is ChatGPT.
Code Correctness
First model answer rating: 95 The code given here is complete and handles the problem well by
checking every pair of numbers for a difference less than or e qual to the given threshold. It gives a
full explanation of the logic and running of the code, and cre ates a clear distinction of cases when it
should return ‘True‘ or ‘False‘. However, the instruction c ould have been clearer for the function to
return ‘True‘ if the difference is smaller than the threshol d, not equal or smaller.
Second model answer rating: 95 This answer very similarly pr ovides complete code that solves the
problem, considering all possible pairs of numbers in the gi ven list and checking if the difference
is lesser than the provided threshold. It also uses good expl anations to clarify how the code works.
However, the second model doesn’t make the distinction in th e condition checking as clearly as the
ﬁrst answer. And like the ﬁrst answer, it also considers equa ls to threshold as a valid condition.
Code Efﬁciency
First model’s answer rating: 90/100. The proposed algorith m takes a straightforward approach to
solve the problem with a time complexity of O(n2), given n is the length of the input list. For each
element i in the list, it compares it with every other element j in the list to see if the absolute value
of their difference is less or equal to the threshold. Becaus e this model only focuses on the pairwise
comparison without any optimization (like sorting or reduc ing the search space), it might not be the
most efﬁcient solution when the input size increases, hence a minor deduction in the rating.
Second model’s answer rating: 90/100. By comparison, the se cond model also produces a correct
O(n2)solution, which uses two nested loops to compare every uniqu e pair of values in the provided
list. However, similar to the ﬁrst model, it presents an algo rithm that might not perform well for
very long lists, and it does not provide ways to optimize or sc ale the operation. Therefore, it also
does not get the full rating. Both performed similarly in ter ms of code efﬁciency.
17

--- PAGE 18 ---
Code Readability
First model answer: Rating: 95 The answer elaboratively exp lains the problem and its solution. It
clearly describes how the function works, and the step-by-s tep breakdown of the code is beneﬁcial in
understanding the algorithm. The example at the end, which d emonstrates how to use the function,
is also very helpful.
Second model answer: Rating: 80 This answer also gives a clea r explanation of the solution and
function. However, it lacks an in-depth breakdown of the cod e and doesn’t provide an example of
how to use the function, which would have been useful for demo nstrating its functionality.
Question Relevance
First model answer rating: 95/100 This response not only pro vides the correct implementation of the
function, but also explains the solution in a detailed manne r. It thoroughly covers how the function
works, including explaining its various parts like two nest ed loops, use of abs() function and the if
conditions. The example usage was an added bonus. I’ve docke d a few points because the model
does not talk about the time complexity of this solution and a lso does not provide the full code
snippet which is importing necessary modules from the typin g module.
Second model answer rating: 85/100 This response also deliv ers a correct code implementation
with a succinct explanation. Though it explains the working of the function effectively, it’s not as
detailed as the ﬁrst model’s answer. This response is also sh ort on providing information on usage
of the function and discussing the time complexity of the sol ution. Importantly, it doesn’t restate
the question’s example into code for demonstrating purpose s, which could be highly informative for
users who are learning or new to Python. Therefore, I’ve rate d it slightly lower.
Table 3: Results of GPT4-based MultiParameters HumanEval f or HumanEval Problem 0
Code
CorrectnessCode Efﬁciency Code
ReadabilityQuestion
Relevance
Safurai-001 95 90 95 95
ChatGPT 85 90 80 85
Although the two responses are quite similar, as previously shown, GPT4-based MultiParameters
Evaluation is able to identify small details and difference s that HumanEval would not be able to
identify given its quantitative nature.
5 C ONCLUSION
This research reveals deep insights into how the utilizatio n of data transformation can lead to the
creation of complex, high-quality datasets. Following the stance of the Orca team (Gunesekar er al.
[2023]), we stress the profound signiﬁcance of the datasets chosen for ﬁne-tuning. Through the use
of innovative data transformation and prompt engineering t echniques, these datasets can critically
enhance the efﬁciency of foundation models. This, in turn, c an lead to sizeable augmentations in
classic benchmarks.
Our study presented Safurai-001 as a potent alternative to W izardCoder [Xu et al. (2023)] for multi-
turn conversations, adeptly managing user interactions an d providing supports where needed. We
also present a distinct advance in the area of Coding LLMs eva luation tools, GPT4-based MultiPa-
rameter Evaluation Benchmark, moving beyond purely quanti tative measures to offer more quali-
tative indicators. This new method provides a richer, more n uanced means of evaluation that can
expose insights beyond the reach of traditional standards s uch as HumanEval20and MultiPL-E21.
20https://github.com/openai/human-eval
21https://huggingface.co/datasets/nuprl/MultiPL-E
18

--- PAGE 19 ---
Drawing on these ﬁndings, we aim to shape future discourse an d innovation in this ﬁeld, accentu-
ating the paramount importance of robust evaluation tools a nd the fullest utilization of datasets in
optimizing language model performance.
5.1 S AFURAI -001 AND WIZARD CODER OUTCOME COMPARISON
Question
How can I write a Python function that takes a list of integers as an input and returns a tuple
representing the sum and product of all integers in that list ?
WizardCoder Response
We can use the built-in ‘sum()‘ function in Python to get a sum of all integers in the list. We
can also use a variable initialized as 1 for product and then m ultiply it with each integer in
the list. Here’s the solution:
fromtypingimportList, Tuple
defsum_product(numbers):
total_sum = sum(numbers) or0# if numbers is
empty, set total_sum to 0
total_product = 1
fornuminnumbers:
total_product *= num
return(total_sum, total_product)
Here’s how the function works step by step:
1. We initialize ‘total sum‘ as sum of all integers in the list using built-in ‘sum()‘ or
set it to 0 if the list is empty using short circuit evaluation (‘or‘).
2. We also initialize ‘total product‘ variable as 1 and then multiply each integer in the
list with this variable.
3. Finally, we return a tuple of ‘(total sum, total product)‘.
19

--- PAGE 20 ---
Safurai-001 Response
First, we need to understand what a list is. A list is a collect ion of items, like a grocery list.
In this case, our list is made up of numbers.
Next, we need to understand what a tuple is. A tuple is like a li st, but once it’s made, we
can’t change it. It’s like a locked box that we can’t open.
Now, let’s look at the problem. We need to write a function tha t takes a list of numbers and
returns a tuple with the sum and product of those numbers. Her e’s the code:
fromtypingimportList, Tuple
defsum_product(numbers: List[ int]) -> Tuple[ int,
int]:
"""
For a given list of integers, return a tuple
consisting of a sum and a product of all the
integers in a list.
Empty sum should be equal to 0 and empty product
should be equal to 1.
"""
# If the list is empty, return a tuple with a
sum of 0 and a product of 1
if notnumbers:
return0, 1
# Otherwise, calculate the sum and product
sum= 0
product = 1
fornuminnumbers:
sum+= num
product *= num
# Return the tuple with the sum and product
return sum , product
Let’s break it down:
• ‘def sum product(numbers: List[int]) - >Tuple[int, int]:‘ - This line deﬁnes our
function and tells Python that it takes a list of integers as i nput and returns a tuple
with two integers (the sum and product).
• ‘if not numbers:‘ - This line checks if the list is empty. If i t is, we return a tuple
with a sum of 0 and a product of 1.
• ‘sum = 0‘ and ‘product = 1‘ - These lines initialize variable s for the sum and prod-
uct.
• ‘for num in numbers:‘ - This line loops through each number i n the list.
• ‘sum += num‘ and ‘product *= num‘ - These lines add the curren t number to the
sum and multiply it by the current product.
• ‘return sum, product‘ - This line returns the tuple with the sum and product.
So, if we call our function with the list ‘[1, 2, 3, 4]‘, it will return the tuple ‘(10, 24)‘. The
sum is 10 (1 + 2 + 3 + 4) and the product is 24 (1 x 2 x 3 x 4).
I hope that makes sense! Let me know if you have any questions.
20

--- PAGE 21 ---
ACKNOWLEDGMENTS
REFERENCES
Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz
Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Man an Dey, Logesh Kumar Umapathi,
Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hail ey Schoelkopf, Sergey Troshin,
Dmitry Abulkhanov, Manuel Romero, Michael Lappert, France sco De Toni, Bernardo Garc´ ıa
del R´ ıo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Ter ry Yue Zhuo, Ian Yu, Paulo Villegas,
Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, D anish Contractor, Luis Villa, Jia
Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel F ried, Arjun Guha, Harm de Vries,
and Leandro von Werra. Santacoder: don’t reach for the stars !arXiv:2301.03988 , 2023.
Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hu a Wu. Ernie-code: Beyond
english-centric cross-lingual pretraining for programmi ng languages. arXiv:2212.06742 , 2022.
Shubham Chandel, Colin B. Clement, Guillermo Serrato, and N eel Sundaresan. Training and eval-
uating a jupyter notebook data science assistant. arXiv:2201.12901 , 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Seba stian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek R ao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hu tchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengch eng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xa vier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Lu an, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shi vani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zo ngwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason We i, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling l anguage modeling with pathways.
arXiv:2204.02311 , 2022.
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric W allace, Freda Shi, Ruiqi Zhong,
Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A gen erative model for code inﬁlling
and synthesis. arXiv:2204.05999 , 2022.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´ esar Teodor o Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, O lli Saarikivi, Adil Salim, Shital
Shah, Harkirat Singh Behl, Xin Wang, S´ ebastien Bubeck, Ron en Eldan, Adam Tauman Kalai,
Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv:2306.11644 , 2023.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennigho ff, Denis Kocetkov, Chenghao
Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qia n Liu, Evgenii Zheltonozhskii,
Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaad orj, Joel Lamy-Poirier, Jo˜ ao
Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade , Armel Zebaze, Ming-Ho Yee, Lo-
gesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham O blokulov, Zhiruo Wang, Rudra
Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulk hanov, Marco Zocca, Manan Dey,
Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu , Swayam Singh, Sasha Luc-
cioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manue l Romero, Tony Lee, Nadav Timor,
Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex
Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dola n-Gavitt, Danish Contractor, Siva
Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Car los Mu˜ noz Ferrandis, Sean Hughes,
Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries . Starcoder: may the source
be with you! arXiv:2305.06161 , 2023.
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian S chrittwieser, R´ emi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thom as Hubert, Peter Choy, Cyprien
de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Hu ang, Johannes Welbl, Sven
Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowit z, Esme Sutherland Robson, Push-
meet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol V inyals. Competition-level code
generation with alphacode. arXiv:2203.07814 , 2022.
21

--- PAGE 22 ---
OpenAI. Gpt-4 technical report. 2023.
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng , An Fu, Muhan Zeng, Ailun Yu,
Jichuan Ji, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang. P angu-coder2: Boosting large
language models for code with ranking feedback. arXiv:2307.14936 , 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amja d Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bho sale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, N aman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lac haux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, T odor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstei n, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra manian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu , Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aureli en Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and ﬁne-tuned chat models.
arXiv:2307.09288 , 2023.
BigScience Workshop. Bloom: A 176b-parameter open-access multilingual language model.
arXiv:2211.05100 , 2022.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.
arXiv:2304.12244 , 2023.
Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and J ian-Guang Lou. When language
model meets private library. arXiv:2210.17236 , 2022.
Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,
Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codege ex: A pre-trained model for
code generation with multilingual evaluations on humaneva l-x.arXiv:2303.17568 , 2023.
22
