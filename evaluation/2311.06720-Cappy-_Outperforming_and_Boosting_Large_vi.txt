# 2311.06720.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/evaluation/2311.06720.pdf
# Kích thước file: 842339 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Cappy: Vượt trội và Nâng cao Hiệu suất của các Mô hình Ngôn ngữ Lớn Đa nhiệm vụ với một Bộ Chấm điểm Nhỏ
Bowen Tan1∗, Yun Zhu2, Lijuan Liu2, Eric Xing1,3,5, Zhiting Hu4, Jindong Chen2
1Đại học Carnegie Mellon,2Google Research,3Petuum Inc.,4UC San Diego,
5Đại học Trí tuệ Nhân tạo Mohamed bin Zayed
{btan2, epxing}@andrew.cmu.edu, zhh019@ucsd.edu ,
{yunzhu, lijuanliu, jdchen}@google.com
Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) như T0, FLAN, và OPT-IML, xuất sắc trong việc xử lý đa nhiệm vụ dưới một mô hình thống nhất theo hướng dẫn, nơi chúng cũng thể hiện khả năng tổng quát hóa đáng chú ý cho các nhiệm vụ chưa thấy. Mặc dù có hiệu suất ấn tượng, những LLMs này, với kích thước từ vài tỷ đến hàng trăm tỷ tham số, đòi hỏi tài nguyên tính toán đáng kể, làm cho việc huấn luyện và suy luận của chúng trở nên tốn kém và không hiệu quả. Hơn nữa, việc điều chỉnh những mô hình này cho các ứng dụng hạ nguồn, đặc biệt là các nhiệm vụ phức tạp, thường không khả thi do yêu cầu phần cứng rộng lớn để tinh chỉnh, ngay cả khi sử dụng các phương pháp hiệu quả về tham số như prompt tuning. Ngoài ra, các LLMs đa nhiệm vụ mạnh nhất, như OPT-IML-175B và FLAN-PaLM-540B, không thể truy cập công khai, hạn chế nghiêm trọng tiềm năng tùy chỉnh của chúng. Để giải quyết những thách thức này, chúng tôi giới thiệu một bộ chấm điểm nhỏ đã được tiền huấn luyện, Cappy, được thiết kế để nâng cao hiệu suất và hiệu quả của các LLMs đa nhiệm vụ. Với chỉ 360 triệu tham số, Cappy hoạt động độc lập trên các nhiệm vụ phân loại hoặc làm thành phần phụ trợ cho LLMs, tăng cường hiệu suất của chúng. Hơn nữa, Cappy cho phép tích hợp hiệu quả giám sát hạ nguồn mà không yêu cầu tinh chỉnh LLM cũng như không cần truy cập các tham số của chúng. Các thí nghiệm của chúng tôi chứng minh rằng, khi hoạt động độc lập trên 11 nhiệm vụ hiểu ngôn ngữ từ PromptSource, Cappy vượt trội hơn các LLMs lớn hơn vài bậc độ lớn. Bên cạnh đó, trên 45 nhiệm vụ phức tạp từ BIG-Bench, Cappy nâng cao hiệu suất của LLM đa nhiệm vụ tiên tiến, FLAN-T5, với biên độ lớn. Hơn nữa, Cappy linh hoạt hợp tác với các điều chỉnh LLM khác, bao gồm tinh chỉnh và học trong ngữ cảnh, mang lại cải thiện hiệu suất bổ sung.2

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) đã dẫn đến một mô hình mới tìm cách thống nhất các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) khác nhau trong một khung theo hướng dẫn. Mô hình này được thể hiện bởi các LLMs đa nhiệm vụ gần đây, như T0 [23], FLAN [30,4], và OPT-IML [10]. Những mô hình này được huấn luyện với dữ liệu từ nhiều nhiệm vụ: đối với mỗi nhiệm vụ, theo một mẫu cụ thể cho nhiệm vụ đó, mỗi ví dụ có nhãn được chuyển đổi thành một hướng dẫn (ví dụ: "Kết hợp các khái niệm lại để tạo thành một câu: ski, mountain, skier.") và một phản hồi tương ứng (ví dụ: "Skier trượt tuyết xuống núi"). Những cặp (hướng dẫn, phản hồi) này sau đó được sử dụng để huấn luyện LLM, tạo ra một mô hình sinh có điều kiện nhận đầu vào là một ví dụ dữ liệu như một hướng dẫn
∗Công việc thực hiện trong thời gian thực tập tại Google.
2Code và mô hình có sẵn tại https://github.com/tanyuqian/cappy và https://huggingface.co/btan2/cappy-large, tương ứng.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2023).arXiv:2311.06720v1 [cs.LG] 12 Nov 2023

--- TRANG 2 ---
101
100101102
Kích thước Mô hình (thang log)464850525456Độ chính xác
OPT
OPT-IML
BART0
Cappy (của chúng tôi)Hình 1: Cappy vượt trội hơn các LLMs đa nhiệm vụ:
Độ chính xác tổng thể trung bình trên 11 nhiệm vụ thử nghiệm từ PromptSource. Mỗi đường đứt nét kết nối các kích thước khác nhau của cùng một mô hình. Các đường nằm ở vị trí phía trên bên trái hơn biểu thị các mô hình hiệu quả hơn và mang lại hiệu suất vượt trội.
101
100101
Kích thước Mô hình (thang log)1520253035Rouge-L
Nucleus
Beam Search
Self-scoring
CappyBASE (của chúng tôi)
CappyLARGE (của chúng tôi)Hình 2: Cappy nâng cao các LLMs đa nhiệm vụ: Điểm Rouge-L trung bình trên 45 nhiệm vụ phức tạp trong BIG-Bench. Mỗi đường đứt nét đại diện cho một phương pháp làm việc trên LLMs có kích thước khác nhau. Self-scoring đề cập đến việc sử dụng cross-entropy của LLM để chọn phản hồi.

và tạo ra một phản hồi. Hơn nữa, những LLMs đa nhiệm vụ này đã thể hiện khả năng tổng quát hóa theo nhiệm vụ đáng chú ý. Nghĩa là, chúng có thể giải quyết các nhiệm vụ chưa thấy bằng cách hiểu và giải quyết các hướng dẫn hoàn toàn mới.

Do tính phức tạp của việc hiểu và giải quyết các nhiệm vụ khác nhau chỉ thông qua hướng dẫn, kích thước của những LLMs đa nhiệm vụ này thường trải rộng từ vài tỷ tham số đến hàng trăm tỷ, như T0-11B [23] và OPT-IML-175B [10]. Kết quả là, vận hành những mô hình có kích thước như vậy đặt ra những thách thức đáng kể đối với đa số người dùng LLM, vì chúng đòi hỏi sức mạnh tính toán đáng kể và áp đặt yêu cầu lớn về dung lượng bộ nhớ của GPUs/TPUs, làm cho việc huấn luyện và suy luận của chúng trở nên tốn kém và không hiệu quả.

Trong các ứng dụng thực tế, việc khai thác một LLM đa nhiệm vụ duy nhất để quản lý tất cả các nhiệm vụ có thể nghĩ đến theo cách zero-shot vẫn còn thách thức, đặc biệt khi xử lý các nhiệm vụ phức tạp, nhiệm vụ cá nhân hóa và những nhiệm vụ không thể được định nghĩa ngắn gọn bằng hướng dẫn. Mặt khác, kích thước dữ liệu huấn luyện hạ nguồn thường không đủ để huấn luyện tốt một mô hình mà không kết hợp kiến thức tiên nghiệm phong phú. Do đó, việc điều chỉnh LLMs với giám sát hạ nguồn đã được mong muốn từ lâu. Tuy nhiên, quá trình điều chỉnh đặt ra ba trở ngại đáng kể: việc lưu trữ rộng lớn để duy trì một bản sao LLM duy nhất cho mỗi nhiệm vụ hạ nguồn; nhu cầu bộ nhớ đáng kể trên GPUs/TPUs; và sự không khả dụng của các LLMs đa nhiệm vụ mạnh nhất, như OPT-IML-175B [10] và FLAN-PaLM-540B [4]. Một số chiến lược điều chỉnh hiệu quả về tham số, bao gồm prompt tuning [18] và adapters [8], giảm đáng kể yêu cầu lưu trữ, nhưng chúng vẫn thực hiện back-propagation thông qua các tham số LLM trong quá trình điều chỉnh, do đó nhu cầu bộ nhớ của chúng vẫn cao. Ngoài ra, một số kỹ thuật học trong ngữ cảnh [5] tránh việc điều chỉnh tham số bằng cách tích hợp một số lượng hạn chế các ví dụ có giám sát vào hướng dẫn. Tuy nhiên, những kỹ thuật này bị hạn chế bởi độ dài đầu vào tối đa của mô hình, chỉ cho phép một vài mẫu để hướng dẫn giải quyết nhiệm vụ.

Trong công trình này, chúng tôi đề xuất một phương pháp mới để nâng cao hiệu suất và hiệu quả của các LLMs đa nhiệm vụ. Cụ thể, chúng tôi giới thiệu một bộ chấm điểm nhẹ đã được tiền huấn luyện, Cappy, dựa trên huấn luyện tiếp tục trên RoBERTa [20], với chỉ 360 triệu tham số. Cappy nhận một hướng dẫn và một phản hồi ứng viên làm đầu vào, và tạo ra một điểm số từ 0 đến 1, cho biết độ chính xác ước tính của phản hồi đối với hướng dẫn. Một cách tự nhiên, chúng tôi công thức hóa tiền huấn luyện của Cappy như một bài toán hồi quy. Điều này dự kiến dữ liệu huấn luyện dưới dạng các cặp (hướng dẫn, phản hồi) tương ứng với các chú thích điểm số chính xác khác nhau. Để tạo ra dữ liệu huấn luyện mong muốn từ nhiều tập dữ liệu tiền huấn luyện chỉ chứa hướng dẫn và phản hồi đúng của chúng, chúng tôi đề xuất một phương pháp giám sát yếu với tăng cường dữ liệu thông qua việc sử dụng các LLMs đa nhiệm vụ hiện có. Kết quả là, chúng tôi thu được một tập dữ liệu tiền huấn luyện hồi quy lớn và hiệu quả với các chú thích điểm số chính xác đa dạng từ 0 đến 1.

Để áp dụng Cappy vào các tình huống giải quyết vấn đề thực tế, chúng tôi đề xuất một phương pháp trực quan theo kiểu lựa chọn ứng viên. Cụ thể, Cappy hoạt động độc lập trên các nhiệm vụ phân loại bằng cách chọn

--- TRANG 3 ---
LLM Đa nhiệm vụ
ví dụ: FLAN, OPT-IML
Cappy Dữ liệu huấn luyện hạ nguồn
Dự đoán nâng cao Điều chỉnh qua Cappy
•Hiệu quả
•Bất kỳ lượng dữ liệu huấn luyện nào
•Nâng cao bất kỳ LLMs nào
The	picture	appeared	on	the	wall	of	a	Poundland store	on	Whymark Avenue	[...]	How	would	you	rephrase	that	in	a	few	words?	Graffiti	artist	Banksy	is	believed	to	be	behind	[...]	Score:0.86
Hướng dẫn: Phản hồi:
Cappy Điều chỉnh thông qua LLM
•Tốn kém
•Đòi hỏi phần cứng
•Vấn đề khả năng truy cập
mẫu argmax

Hình 3: (trái) Mô hình hóa của Cappy. (phải) Minh họa ứng dụng của Cappy trong việc nâng cao các LLMs đa nhiệm vụ, và so sánh giữa điều chỉnh hạ nguồn thông qua Cappy và các phương pháp dựa trên tham số của LLM, như tinh chỉnh và prompt tuning.

lựa chọn câu trả lời tạo ra điểm số cao nhất. Hơn nữa, ngoài việc sử dụng độc lập, Cappy phục vụ như một thành phần phụ trợ của các LLMs đa nhiệm vụ hiện có, chọn đầu ra phù hợp nhất từ một tập các ứng viên được tạo ra bởi LLM. Trong trường hợp này, Cappy cho phép điều chỉnh hiệu quả và hữu hiệu cho các nhiệm vụ phức tạp thông qua việc kết hợp giám sát hạ nguồn, mà không yêu cầu tinh chỉnh LLM đa nhiệm vụ hoặc truy cập các tham số của nó. Đáng chú ý, Cappy thể hiện tính linh hoạt trong việc hợp tác với các điều chỉnh LLM khác, như tinh chỉnh và học trong ngữ cảnh.

Chúng tôi xác thực Cappy thông qua một bộ thử nghiệm toàn diện các nhiệm vụ held-out khác biệt với những nhiệm vụ được kết hợp trong tiền huấn luyện của nó. Hiệu suất tổng thể được thể hiện trong Hình 1 và Hình 2. Cụ thể, trên 11 nhiệm vụ hiểu ngôn ngữ được rút ra từ PromptSource [1], Cappy, với 360 triệu tham số, vượt trội hơn OPT-IML-30B và OPT-175B một cách đáng kể, và sánh ngang với những mô hình tốt nhất trong các LLMs đa nhiệm vụ trước đây. Bên cạnh đó, trên 45 nhiệm vụ phức tạp đa dạng từ BIG-Bench [25], Cappy liên tục nâng cao hiệu suất của LLM đa nhiệm vụ tiên tiến, FLAN-T5, với biên độ lớn. Hơn nữa, Cappy mang lại cải thiện hiệu suất bổ sung khi được áp dụng cùng với tinh chỉnh hoặc học trong ngữ cảnh. Nghiên cứu ablation tiếp theo của chúng tôi chứng minh tầm quan trọng của các chiến lược tiền huấn luyện và tăng cường dữ liệu được đề xuất.

2 Công trình Liên quan
LLMs cho Theo hướng dẫn và Huấn luyện Đa nhiệm vụ có Prompt Việc mở rộng quy mô các mô hình ngôn ngữ mang lại cho chúng khả năng ngày càng mạnh mẽ, đỉnh điểm là một mô hình tổng quát giải quyết các vấn đề đa dạng theo cách thống nhất theo hướng dẫn. Có hai phương pháp chính của những LLMs như vậy, mỗi phương pháp được phân biệt bởi mục đích của hướng dẫn. Phương pháp đầu tiên nhấn mạnh tuân thủ các hướng dẫn tùy ý của con người, thường ở định dạng hỏi-đáp hoặc đối thoại (ví dụ: "Tôi phải đưa ra một quyết định khó khăn. Tôi nên làm gì?"). Các mô hình như GPT-4 [21] và Vicuna [3] được thiết kế để phản hồi những hướng dẫn này với mục tiêu tối đa hóa sự hài lòng của người dùng. Những mô hình này thường được huấn luyện thông qua Reinforcement Learning with Human Feedback (RLHF) [13], tận dụng các chú thích rộng rãi của con người. Đánh giá định lượng của chúng cũng phụ thuộc rất nhiều vào phán đoán của con người [33]. Tuy nhiên, phương pháp thứ hai chủ yếu dành cho việc giải quyết các nhiệm vụ NLP được định nghĩa rõ ràng. Trong bối cảnh này, mỗi instance dữ liệu tuân thủ một mẫu cụ thể cho nhiệm vụ, và được chuyển đổi thành một hướng dẫn (ví dụ: "Kết hợp các khái niệm lại để tạo thành một câu: ski, mountain, skier.") và một phản hồi tương ứng (ví dụ: "Một skier trượt tuyết xuống núi."). Các LLMs đa nhiệm vụ, như OPT-IML [10], FLAN [30,4], và T0 [23], được tiền huấn luyện thông qua huấn luyện đa nhiệm vụ có prompt. Quá trình này huấn luyện các mô hình như một nhiệm vụ sinh thống nhất có điều kiện sử dụng các cặp hướng dẫn và phản hồi từ nhiều nhiệm vụ tiền huấn luyện phía trên. Những mô hình này thường được đánh giá dựa trên hiệu suất trên các nhiệm vụ thử nghiệm held-out, sử dụng các metric đánh giá truyền thống như độ chính xác, điểm Rouge [17], v.v. Trong nghiên cứu này, trọng tâm chính của chúng tôi là phương pháp thứ hai, tức là các LLMs đa nhiệm vụ, do đánh giá đơn giản hơn. Tuy nhiên, chúng tôi cho rằng không có trở ngại đáng kể nào để áp dụng các phương pháp đề xuất của chúng tôi cho các hướng dẫn giống con người hơn, điều mà chúng tôi để lại như hướng tương lai.

--- TRANG 4 ---
The	picture	appeared	on	the	wall	of	a	Poundland store	on	Whymark Avenue	[...]	How	would	you	rephrase	that	in	a	few	words?	Hướng dẫn:
Graffiti	artist	Banksy	is	believed	to	be	behind	[...]	Target (sự thật chuẩn): LLM Đa nhiệm vụ
ví dụ: FLAN 4BNQMF4BNQMF4BNQMF4BNQMF
Dữ liệu Huấn luyện Hồi quy cho Cappy Cặp hướng dẫn-phản hồi sự thật chuẩn Cặp văn bản đầu vào Chú thích điểm số (hướng dẫn, mẫu 1) Rouge-L(mẫu 1, target) (hướng dẫn, mẫu 2) Rouge-L(mẫu 2, target) (hướng dẫn, mẫu 3) Rouge-L(mẫu 3, target) (hướng dẫn, mẫu 4) Rouge-L(mẫu 4, target)

Hình 4: Tăng cường dữ liệu với một LLM đa nhiệm vụ để xây dựng tập dữ liệu hồi quy có giám sát yếu cho tiền huấn luyện và tinh chỉnh của Cappy, như được mô tả trong Mục 3.2.

Điều chỉnh LLMs Kích thước của LLMs làm cho việc tinh chỉnh chúng cho các nhiệm vụ hạ nguồn đặc biệt thách thức, chủ yếu do ba vấn đề. Thứ nhất, tinh chỉnh đòi hỏi tạo ra một bản sao mới của LLM cho mỗi nhiệm vụ hạ nguồn cụ thể. Điều này không thể chấp nhận được đối với nhiều ứng dụng. Thứ hai, tinh chỉnh một LLM đòi hỏi bộ nhớ thiết bị đáng kể do back-propagation thông qua LLMs, chỉ có thể đạt được với các cụm GPU/TPU cao cấp. Thứ ba, các LLMs mạnh nhất, như FLAN-PaLM-540B [4] và GPT-4 [21], là mã nguồn đóng và do đó không thể truy cập để tinh chỉnh. Một tập hợp các kỹ thuật điều chỉnh LLM hiệu quả về tham số, bao gồm prompt tuning [18] và adapters [8], như prefix tuning [15] và LoRA [7], đã phần lớn giảm thiểu vấn đề lưu trữ bằng cách giảm số lượng tham số có thể điều chỉnh. Tuy nhiên, những phương pháp này vẫn yêu cầu back propagation thông qua các trọng số LLM gốc để cập nhật prompt hoặc adapter, để lại vấn đề thứ hai và thứ ba vẫn là những rào cản đáng kể trong việc điều chỉnh LLM. Một số kỹ thuật học trong ngữ cảnh [5] tránh việc điều chỉnh tham số của LLM bằng cách thêm các ví dụ huấn luyện vào hướng dẫn. Tuy nhiên, độ dài hướng dẫn bị hạn chế bởi độ dài đầu vào tối đa của mô hình, các kỹ thuật học trong ngữ cảnh như vậy chỉ cho phép một số lượng hữu hạn các mẫu để hướng dẫn quá trình giải quyết nhiệm vụ. Trong công trình này, chúng tôi đề xuất điều chỉnh các LLMs đa nhiệm vụ bằng cách sử dụng Cappy để kết hợp giám sát hạ nguồn. Phương pháp này cho phép bất kỳ số lượng ví dụ huấn luyện nào mà không cần tinh chỉnh LLM hoặc truy cập các tham số của nó. Do đó, LLM chỉ đóng vai trò như một hộp đen, và Cappy thậm chí tương thích với WebAPIs của LLMs. Quan trọng, Cappy cũng có thể được triển khai kết hợp với các điều chỉnh LLM khác, như tinh chỉnh và học trong ngữ cảnh.

Các Mô hình Dựa trên Xếp hạng Xếp hạng là một thành phần then chốt trong các hệ thống truy xuất thông tin, đáng chú ý trong các công cụ tìm kiếm và hệ thống đề xuất [19]. Nó bao gồm việc sắp xếp số lượng lớn tài liệu để tìm nội dung có liên quan đến một truy vấn cụ thể. Gần đây, xếp hạng đã được điều chỉnh cho các nhiệm vụ NLP để tổng hợp câu trả lời [14], và phục vụ cho sở thích của con người [2,6]. Hơn nữa, trong lĩnh vực mới nổi của reinforcement learning from human feedback (RLHF) [26], các mô hình xếp hạng được huấn luyện trên các đầu ra mô hình được con người xếp hạng đóng vai trò như nhà cung cấp phần thưởng cho việc huấn luyện các tác nhân RL. Đồng thời với công trình này, [32] đề xuất một mô hình xếp hạng thống nhất giải quyết các nhiệm vụ kiểu information alignment, như suy luận ngôn ngữ tự nhiên và phát hiện paraphrase. Trong công trình này, Cappy về mặt khái niệm là một mô hình xếp hạng cho học đa nhiệm vụ. Không giống như các phương pháp được thiết kế cụ thể cho trả lời câu hỏi [12] hoặc tóm tắt [22], Cappy cung cấp khả năng tổng quát hóa rộng rãi qua các tình huống đa nhiệm vụ. Ngoài ra, trái ngược với các mô hình phần thưởng RLHF, Cappy không dựa vào dữ liệu có chú thích của con người đắt đỏ, điều này cho phép tiền huấn luyện quy mô lớn.

3 Bộ Chấm điểm Cappy

3.1 Mô hình hóa
Cappy áp dụng kiến trúc của RoBERTa [20] với một lớp tuyến tính trên đầu như một đầu hồi quy. Đầu vào của Cappy là một cặp văn bản, bao gồm một hướng dẫn và một phản hồi, và đầu ra là một điểm số vô hướng từ 0 đến 1. Điểm số này cho biết ước tính về độ chính xác của phản hồi liên quan đến instance nhiệm vụ được mô tả trong hướng dẫn.

3.2 Tiền huấn luyện
Tiền huấn luyện của Cappy sử dụng cùng một bộ sưu tập tập dữ liệu được sử dụng bởi T0 [23]. Bộ sưu tập này bao gồm 39 tập dữ liệu đa dạng từ PromptSource [1], bao gồm một loạt các loại nhiệm vụ, như trả lời câu hỏi, phân tích cảm xúc, và tóm tắt, v.v. Mỗi tập dữ liệu được liên kết với một hoặc nhiều mẫu, chuyển đổi mỗi instance từ các tập dữ liệu gốc thành một hướng dẫn

--- TRANG 5 ---
được ghép nối với phản hồi sự thật chuẩn của nó. Theo cấu hình tiền huấn luyện của T0, kích thước của mỗi tập dữ liệu được giới hạn tối đa 500.000 ví dụ.

Xét đến mô hình hóa hồi quy của Cappy, mỗi instance dữ liệu trong quá trình tiền huấn luyện phải có một cặp văn bản (hướng dẫn, phản hồi), kết hợp với một chú thích độ chính xác cho phản hồi liên quan đến hướng dẫn. Một mảng đa dạng các chú thích điểm số là một khía cạnh quan trọng của một tập dữ liệu hồi quy. Tuy nhiên, các cặp văn bản trong tập dữ liệu tiền huấn luyện của chúng tôi chỉ chứa hướng dẫn với phản hồi sự thật chuẩn của chúng, do đó mỗi cặp văn bản luôn có điểm số chính xác là 1.0. Điều này có thể dẫn đến việc thiếu sự đa dạng nhãn nghiêm trọng trong suốt quá trình tiền huấn luyện của Cappy. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp xây dựng dữ liệu để tạo ra tập dữ liệu tiền huấn luyện của Cappy với các chú thích chính xác đa dạng từ 0 đến 1. Việc xây dựng dữ liệu bao gồm ba thành phần:

Sự thật Chuẩn (điểm số 1.0) Thành phần này bao gồm tất cả các cặp hướng dẫn-phản hồi sự thật chuẩn từ tập dữ liệu tiền huấn luyện. Mỗi cặp được gán một chú thích chính xác là 1.0.

Phản hồi Không chính xác (điểm số 0.0) Chúng tôi thiết kế các điểm dữ liệu không chính xác bằng cách tạo ra các cặp hướng dẫn-phản hồi không khớp từ các tập dữ liệu gốc. Đối với các tập dữ liệu phân loại, mỗi hướng dẫn được ghép nối với tất cả các lựa chọn câu trả lời không chính xác. Đối với các tập dữ liệu sinh, mỗi hướng dẫn được ghép nối tùy ý với một phản hồi sự thật chuẩn từ một điểm dữ liệu khác biệt trong tập dữ liệu.

Tăng cường Dữ liệu (điểm số trong [0,1]) Ngoài các mẫu dữ liệu hoàn toàn đúng hoặc sai, chúng tôi tạo ra các cặp hướng dẫn-phản hồi với điểm số từ 0 đến 1. Điều này được đạt được thông qua tăng cường dữ liệu áp dụng trên tất cả các instance nhiệm vụ sinh. Đối với mỗi instance trong một nhiệm vụ sinh, chúng tôi tận dụng một LLM đa nhiệm vụ hiện có để tạo ra nhiều phản hồi bằng cách lấy mẫu có điều kiện trên hướng dẫn đã cho. Tiếp theo, chúng tôi gán một chú thích cho cặp được tạo bởi hướng dẫn và mỗi phản hồi, sử dụng sự tương tự giữa phản hồi và phản hồi sự thật chuẩn của instance. Cụ thể, chúng tôi sử dụng Rouge-L [17] để tính toán sự tương tự này như một dạng giám sát yếu. vì nó đã được công nhận rộng rãi là một metric đáng tin cậy cho hiệu suất tổng thể trong các môi trường đa nhiệm vụ và đã thể hiện sự phù hợp mạnh mẽ với đánh giá của con người [29]. Trong thực tế, các mẫu tăng cường của chúng tôi được tạo ra bởi hai LLMs đa nhiệm vụ, BART0 [16] và T0-3B [23]. Đối với mỗi instance trong một nhiệm vụ sinh, cả hai mô hình này đều tạo ra hai mẫu sử dụng lấy mẫu top-k và top-p, tương ứng.

Kết quả, chúng tôi thu thập một tập dữ liệu tiền huấn luyện bao gồm 160 triệu instances, mỗi instance có định dạng (hướng dẫn, phản hồi, điểm số). Cappy được khởi tạo như RoBERTa và được tối ưu hóa bằng optimizer AdamW với mất mát hồi quy L2. Quá trình tối ưu hóa bao gồm tốc độ học 10−6, tỷ lệ warmup 0.1, và kích thước batch hiệu quả 1024. Phù hợp với các biến thể RoBERTa, Cappy cũng được cung cấp trong hai phiên bản khác biệt: Cappy BASE nhỏ hơn (120M tham số), và Cappy LARGE (360M tham số).

3.3 Áp dụng Cappy
Cappy giải quyết các nhiệm vụ thực tế trong một cơ chế lựa chọn ứng viên. Cụ thể hơn, với một hướng dẫn và một tập các phản hồi ứng viên, Cappy tạo ra một điểm số cho mỗi ứng viên. Điều này được đạt được bằng cách nhập hướng dẫn cùng với từng phản hồi riêng lẻ, và sau đó gán phản hồi có điểm số cao nhất làm dự đoán của nó. Trong các nhiệm vụ phân loại, tất cả các phản hồi ứng viên được định nghĩa sẵn một cách tự nhiên. Ví dụ, các lựa chọn là {positive, negative} trong một nhiệm vụ phân loại cảm xúc. Trong những tình huống như vậy, Cappy hoạt động độc lập. Mặt khác, trong các nhiệm vụ sinh, các phản hồi ứng viên không được định nghĩa trước, đòi hỏi một LLM đa nhiệm vụ hiện có để tạo ra các phản hồi ứng viên. Trong trường hợp này, Cappy đóng vai trò như một thành phần phụ trợ của LLM đa nhiệm vụ, nâng cao việc giải mã của nó.

3.4 Điều chỉnh các LLMs Đa nhiệm vụ
Khi có dữ liệu huấn luyện hạ nguồn có sẵn, Cappy cho phép điều chỉnh hiệu quả và hữu hiệu các LLMs đa nhiệm vụ trên các nhiệm vụ hạ nguồn. Cụ thể, chúng tôi đề xuất tích hợp thông tin nhiệm vụ hạ nguồn vào dự đoán của LLM thông qua việc tinh chỉnh Cappy. Để phát triển, một tập dữ liệu hồi quy hạ nguồn có thể được thu thập thông qua một quy trình chú thích dữ liệu giống như phương pháp được sử dụng trong quá trình xây dựng dữ liệu tiền huấn luyện (§3.2). Sau đó, Cappy có thể được tinh chỉnh trên tập dữ liệu hồi quy này. Kết quả là, Cappy đã được tinh chỉnh hợp tác với một LLM đa nhiệm vụ, nâng cao hiệu suất của LLM trên nhiệm vụ hạ nguồn.

--- TRANG 6 ---
60708090
StoryCloze
30405060
CB
50607080
WinoGender
3234363840Độ chính xác
ANLI (R1/R2/R3)
45505560
WSC
50525456
WIC
101
100101102505560
WinoGrande
101
100101102
Kích thước Mô hình (thang log)20406080
Crows-Pairs
101
10010110250607080
RTE
BART0
OPT
OPT-IML
T0
Cappy (của chúng tôi)

Hình 5: Hiệu suất của Cappy và các LLMs đa nhiệm vụ trên các tập dữ liệu thử nghiệm khác nhau. Một chuỗi các đường đứt nét được sử dụng để kết nối các kích thước khác nhau của cùng một mô hình, như OPT-30B và OPT-175B. Các đường hoặc điểm được định vị ở phía trên bên trái của biểu đồ biểu thị các mô hình hiệu quả hơn và mang lại hiệu suất vượt trội. Mỗi biểu đồ tương ứng với một nhiệm vụ thử nghiệm cụ thể, ngoại trừ ANLI đại diện cho ba nhiệm vụ khác nhau (ANLI-R1/R2/R3).

Trái ngược với các chiến lược điều chỉnh LLM khác như tinh chỉnh và prompt tuning, việc điều chỉnh LLMs với Cappy trên các nhiệm vụ hạ nguồn tránh nhu cầu back-propagation thông qua các tham số LLM. Do đó, nó giảm đáng kể nhu cầu cao về bộ nhớ thiết bị. Bên cạnh đó, việc điều chỉnh với Cappy không phụ thuộc vào truy cập các tham số LLM, làm cho nó tương thích với các LLMs đa nhiệm vụ mã nguồn đóng, như những mô hình chỉ có thể truy cập qua WebAPIs. So với các phương pháp học trong ngữ cảnh tránh việc điều chỉnh mô hình bằng cách đính kèm các ví dụ huấn luyện vào tiền tố hướng dẫn, Cappy không bị hạn chế bởi độ dài đầu vào tối đa của LLM. Do đó, Cappy có thể kết hợp một số lượng không giới hạn các ví dụ huấn luyện hạ nguồn. Hơn nữa, Cappy linh hoạt làm việc cùng với các phương pháp điều chỉnh khác, như tinh chỉnh và học trong ngữ cảnh, nâng cao thêm hiệu suất tổng thể của chúng, như chúng tôi chứng minh trong các thí nghiệm.

4 Thí nghiệm
Tất cả các thí nghiệm của công trình này, bao gồm tiền huấn luyện của Cappy và các điều chỉnh hạ nguồn, được tiến hành trên Google TPU-v4 [11], và tất cả mã được triển khai với Redco [27], một bộ công cụ nhẹ để tự động hóa huấn luyện phân tán.

4.1 Hiệu suất Zero-shot trên PromptSource
Đánh giá của chúng tôi phù hợp với những đánh giá được sử dụng bởi OPT-IML và T0. Chúng tôi đánh giá hiệu suất trên 11 nhiệm vụ hiểu ngôn ngữ held-out trong PromptSource [1], tất cả đều ở kiểu phân loại. Những nhiệm vụ này khác biệt về mặt phân loại so với những nhiệm vụ được sử dụng trong các tập dữ liệu tiền huấn luyện. Các baseline của chúng tôi bao gồm các LLMs đa nhiệm vụ, tức là OPT, OPT-IML, T0, và BART0, và một mô hình phần thưởng RLHF được huấn luyện với dữ liệu phản hồi của con người, được phát hành bởi LAION-AI3. Theo chiến lược lựa chọn câu trả lời được sử dụng bởi T0 và OPT-IML, dự đoán từ những LLMs đa nhiệm vụ này được xác định bằng lựa chọn câu trả lời mang lại khả năng cao nhất của mô hình. FLAN không được coi là trong số các baseline, vì các nhiệm vụ thử nghiệm được bao gồm trong các nhiệm vụ tiền huấn luyện của nó. Chúng tôi tính toán hiệu suất cho mỗi nhiệm vụ bằng cách tính trung bình kết quả trên tất cả các prompt liên quan.

Kết quả của 11 nhiệm vụ được trình bày trong Hình 5, với hiệu suất tổng thể trung bình của mỗi mô hình được tóm tắt trong Bảng 1. Từ các hình theo từng nhiệm vụ, Cappy luôn vượt trội hơn các mô hình BART0, có kích thước tương đương với Cappy, và cũng vượt qua OPT và OPT-IML trong đa số các nhiệm vụ. Từ độ chính xác tổng thể trong Bảng 1, chúng ta có thể tóm tắt rằng mô hình Cappy BASE của chúng tôi mang lại hiệu suất tương đương với hiệu suất của OPT-30B, OPT-IML-30B, và OPT-175B. Hơn nữa, mô hình Cappy LARGE 360M lớn hơn của chúng tôi hoạt động ở mức độ phù hợp với T0-11B và OPT-IML-175B. Những phát hiện này làm nổi bật hiệu suất vượt trội và hiệu quả tham số của Cappy so với các LLMs đa nhiệm vụ hiện có. Hiệu suất được cải thiện này có thể được ghi nhận cho chiến lược tiền huấn luyện dựa trên chấm điểm của Cappy, tích hợp thông tin tương phản bằng cách phân biệt giữa các phản hồi chất lượng cao và chất lượng thấp. Ngược lại, các LLMs đa nhiệm vụ trước đây phụ thuộc hoàn toàn vào huấn luyện teacher-forcing chỉ sử dụng các phản hồi sự thật chuẩn.

4.2 Điều chỉnh cho BIG-Bench
Chúng tôi xem xét việc điều chỉnh các LLMs đa nhiệm vụ với Cappy trên các nhiệm vụ phức tạp từ BIG-Bench [25], một tập hợp các nhiệm vụ thách thức được tuyển chọn thủ công được coi là vượt quá khả năng của nhiều LLMs. Chúng tôi tập trung vào tất cả 45 nhiệm vụ sinh trong BIG-Bench, cụ thể là những nhiệm vụ không cung cấp các lựa chọn câu trả lời được thiết lập trước. Sự phân chia train/test được cung cấp bởi TaskSource [24]. Kích thước huấn luyện của những nhiệm vụ này khác nhau, với tối thiểu 14 và tối đa 50.000 instances. Kích thước huấn luyện trung vị là 876. Đối với mỗi nhiệm vụ, chúng tôi tinh chỉnh Cappy với optimizer AdamW trong 400 bước với tốc độ học 2×10−5 và kích thước batch hiệu quả 256. Chúng tôi đánh giá hiệu suất bằng điểm Rouge-L trên mỗi tập thử nghiệm, báo cáo điểm số trung bình trên 45 thử nghiệm. Trong thí nghiệm này, tất cả các biến thể của FLAN-T5 đóng vai trò là LLMs xương sống.

Chúng tôi kết hợp những phương pháp này để so sánh, bao gồm: Sampling: Lấy mẫu token theo token tiêu chuẩn; Temperature: Lấy mẫu mỗi token với nhiệt độ phân phối 0.9; Top-K: Lấy mẫu top-k với k=40; Nucleus: Lấy mẫu Nucleus với top-p=0.95; Beam Search: Tìm kiếm beam với độ rộng 4; Self-scoring: Chúng tôi thu thập bốn mẫu được tạo ra bằng cách sử dụng tất cả các chiến lược giải mã dựa trên lấy mẫu ở trên, cộng với mẫu hàng đầu từ beam search4, tổng cộng 4×4 + 1 = 17 mẫu. Với tất cả các mẫu, self-scoring chọn mẫu tốt nhất làm dự đoán dựa trên log-likelihood của mô hình; Cappy: thực hiện lựa chọn mẫu trên cùng một tập hợp các mẫu như trong self-scoring, nhưng dựa trên chấm điểm của Cappy.

4Chúng tôi không thu thập nhiều mẫu bằng beam search, vì API Jax cho beam search trong huggingface-transformers chỉ trả về mẫu hàng đầu.

--- TRANG 7 ---
Mô hình Độ chính xác
BART0 BASE -140M 45.7
BART0 LARGE -400M 50.2
RLHF-RM-185M 43.6
RLHF-RM-435M 53.3
OPT-30B 47.6
OPT-IML-30B 51.3
OPT-175B 49.3
OPT-IML-175B 56.5
T0-11B 58.2
Cappy BASE -120M 49.9
Cappy LARGE -360M 56.6

Bảng 1: Độ chính xác tổng thể trung bình trên 11 nhiệm vụ thử nghiệm held-out từ PromptSource trong thiết lập zero-shot. "RLHF-RM" đề cập đến mô hình phần thưởng RLHF được đề cập trong Mục 4.1.

FLAN-T5 Đông lạnh
Small Base Large XL XXL
Sampling 11.43 15.79 19.62 23.22 25.73
Temperature 12.01 17.06 20.05 24.27 27.10
Top-K 11.52 15.75 19.76 22.67 25.82
Nucleus 11.92 16.62 20.20 24.17 26.90
Beam Search 16.40 19.86 23.48 26.12 29.66
Self-scoring 15.08 20.71 24.12 28.47 32.02
Cappy BASE 23.36 27.26 29.83 32.79 36.63
Cappy LARGE 24.45 28.25 30.75 33.97 36.93
ICL + Nucleus 16.37 20.46 23.65 28.64 32.70
ICL + Self-scoring 20.61 24.42 27.00 32.56 36.37
ICL + Cappy LARGE 26.18 28.65 31.84 36.41 38.48

Bảng 2: Điểm Rouge-L trung bình trên 45 nhiệm vụ BIG-Bench. Các mô hình FLAN-T5 xương sống bị đông lạnh. "ICL" đề cập đến học trong ngữ cảnh, tức là đính kèm các ví dụ huấn luyện trong tiền tố của hướng dẫn. Chúng tôi bao gồm thêm các kết quả liên quan đến prompt-tuning trong phụ lục.

Các mô hình FLAN-T5 cơ sở bị đông lạnh, nghĩa là không được tinh chỉnh. Kết quả được hiển thị trong Bảng 2. Chúng gợi ý rằng Cappy nâng cao hiệu suất của các mô hình FLAN-T5 với biên độ lớn, luôn vượt trội hơn baseline hiệu quả nhất đạt được thông qua lựa chọn mẫu sử dụng self-scoring của chính LLM.

FLAN-T5 Tinh chỉnh
Small Base Large
Sampling 29.34 37.93 43.45
Temperature 29.83 38.21 43.65
Top-K 30.06 37.48 43.86
Nucleus 30.12 37.87 44.35
Beam Search 32.00 39.21 44.52
Self-scoring 33.95 41.00 46.49
Cappy BASE 37.79 43.73 47.22
Cappy LARGE 39.74 45.18 48.98

Bảng 3: Điểm Rouge-L trung bình trên các nhiệm vụ BIG-Bench, khi các mô hình FLAN-T5 xương sống cũng được tinh chỉnh.

Như đã đề cập trong Mục 3.4, Cappy thể hiện tính linh hoạt bằng cách cho phép hợp tác với các điều chỉnh LLM khác. Hiệu suất của Cappy làm việc cùng với tinh chỉnh và học trong ngữ cảnh, được trình bày trong Bảng 2 và Bảng 3, tương ứng. Kết quả chứng minh rằng Cappy tiếp tục nâng cao hiệu suất trên đỉnh của các điều chỉnh khác. Điều này có thể được ghi nhận cho kiến thức hạ nguồn độc đáo mà Cappy thu được từ dữ liệu huấn luyện hạ nguồn. Chính xác hơn, trong khi các điều chỉnh LLM khác chủ yếu phụ thuộc vào các cặp hướng dẫn-phản hồi sự thật chuẩn truyền thống để học, Cappy trích xuất và khai thác thông tin tương phản bằng dữ liệu huấn luyện hồi quy được xây dựng với phương pháp đề xuất của chúng tôi.

4.2.1 Phân tích về Chấm điểm của Cappy
Để hiểu rõ hơn về chấm điểm của Cappy, chúng tôi đã tiến hành phân tích theo từng nhiệm vụ cho tất cả 45 nhiệm vụ BIG-Bench, so sánh hiệu suất giữa chấm điểm của Cappy với self-scoring của LLM đa nhiệm vụ - FLAN-T5-XXL (11B). Hình 6 hiển thị lợi thế hiệu suất của Cappy so với self-scoring của FLAN-T5-XXL (với các giá trị âm cho thấy bất lợi của Cappy). Kết quả cho thấy rằng, đối với hầu hết các nhiệm vụ, Cappy duy trì một cách nhất quán Rouge-L cao hơn khoảng 5 điểm so với self-scoring của LLM. Tuy nhiên, có 3 nhiệm vụ mà Cappy thể hiện bất lợi đáng kể: codenames, simple_arithmetic_json, và sufficient_information.

Chúng tôi trình bày các ví dụ về 3 nhiệm vụ này trong Bảng 4. Khi xem xét các ví dụ codenames, chúng tôi thấy rằng các hướng dẫn thường chứa các từ rời rạc với ý nghĩa đa dạng mà không có kết nối cú pháp. Điều này có thể đặt ra thách thức đáng kể đối với khả năng "ghi nhớ" của mô hình. Trong trường hợp các nhiệm vụ simple_arithmetic_json và sufficient_information, trọng tâm là kiểm tra khả năng toán học và lý luận logic thông thường. Các khả năng ghi nhớ, toán học, và thông thường đã được chứng minh là những lợi thế chính của LLMs [31,9], và chúng khó có thể thu được chỉ thông qua huấn luyện hạ nguồn. Do đó, không có gì đáng ngạc nhiên khi self-scoring của LLM vượt trội hơn chấm điểm của Cappy trong những nhiệm vụ này.

4.2.2 Hiệu suất Mở rộng với Số lượng Mẫu
Mẫu 1 4 17
Self-scoring 26.90 31.15 32.02
Cappy LARGE (của chúng tôi) 26.90 33.64 36.93

Bảng 5: Hiệu suất điều chỉnh BIG-Bench trên FLAN-T5-XXL đông lạnh với số lượng mẫu khác nhau.

Chúng tôi nghiên cứu mối quan hệ giữa hiệu suất điều chỉnh và số lượng mẫu sinh của mô hình. Cụ thể, chúng tôi tiến hành điều này trên một mô hình FLAN-T5-XXL đông lạnh, với ba số lượng mẫu khác nhau, bao gồm 1 hoặc 4 mẫu nucleus, hoặc tất cả 17 mẫu như được mô tả trong Mục 4.2. Kết quả được hiển thị trong Bảng 5, chúng ta có thể thấy rằng khi số lượng mẫu tăng lên, Cappy liên tục nâng cao hiệu suất nhiệm vụ một cách đáng kể, nhưng baseline Self-scoring không cung cấp cải thiện hiệu suất đáng kể khi tăng từ 4 lên 17 mẫu.

4.2.3 Nghiên cứu Ablation
FLAN-T5 Đông lạnh
XL XXL
Cappy LARGE 33.38 36.56
- w/o tiền huấn luyện Cappy 32.03 (-1.35) 35.01 (-1.55)
- w/o Tăng cường dữ liệu sử dụng LLM 28.66 (-4.72) 32.88 (-3.67)

Bảng 6: Hiệu suất điều chỉnh BIG-Bench, trước và sau khi ablation tiền huấn luyện của Cappy và tăng cường dữ liệu sử dụng LLM, các số trong ngoặc là sự giảm hiệu suất.

Để xác minh tầm quan trọng của hai thành phần chính trong phương pháp đề xuất của chúng tôi, chúng tôi tiến hành nghiên cứu ablation, sử dụng điều chỉnh FLAN-T5-XL và -XXL trên các nhiệm vụ BIG-Bench. Cụ thể, trong điều chỉnh hạ nguồn, chúng tôi ablate tiền huấn luyện của Cappy

--- TRANG 8 ---
Lợi thế (Rouge-L)
-20-100102030
ascii_word_recognitionauto_categorizationauto_debugging
bridging_anaphora_resolution_barqachess_state_tracking
chinese_remainder_theoremcodenames
conlang_translationcryptonitedisfl_qa
few_shot_nlggem
gender_inclusive_sentences_germanhindi_question_answering
international_phonetic_alphabet_trlanguage_games
linguistic_mappingslinguistics_puzzleslist_functionsmatrixshapes
minute_mysteries_qamodified_arithmeticmult_data_wranglingobject_countingoperators
paragraph_segmentation
parsinlu_reading_comprehensionphysics_questions
polish_sequence_labelingqa_wikidata
repeat_copy_logicrephrase
scientific_press_release
semantic_parsing_in_context_sparcsemantic_parsing_spidersimp_turing_concept
simple_arithmetic_jsonsimple_text_editing
sufficient_informationtellmewhytense
topical_chat
unnatural_in_context_learningword_sorting
word_unscramblingHình 6: Lợi thế của chấm điểm Cappy so với self-scoring của FLAN-T5-XXL, trên 45 nhiệm vụ BIG-Bench. Trục x là tên của các nhiệm vụ.

Tên nhiệm vụ: codenames
Hướng dẫn: Cố gắng xác định 3 từ có liên quan tốt nhất với từ PAJAMAS từ danh sách sau: nude, judge, sleep, einstein, groom, troll, wish, sun, quarter, halloween, brain, stamp, wedding, slipper, minotaur, pad, tip, crusader, helmet. Đưa ra câu trả lời của bạn theo thứ tự bảng chữ cái.
Target: nude, sleep, slipper
Hướng dẫn: Cố gắng xác định 1 từ có liên quan tốt nhất với từ PREHISTORIC từ danh sách sau: boom, new york, cotton, green, ball, pumpkin, force, suit, board, jet, mug, head, mammoth, seal, day, engine. Đưa ra câu trả lời của bạn theo thứ tự bảng chữ cái.
Target: mammoth

Tên nhiệm vụ: simple_arithmetic_json
Hướng dẫn: 5 + 0 =
Target: 5
Hướng dẫn: 348 + 227 =
Target: 575

Tên nhiệm vụ: sufficient_information
Hướng dẫn: Ed, Jeff, E-Jay, và Michael đang ở trong một vòng tròn. Ed ở bên trái Jeff. Mike có ở bên trái Ed không?
Target: Tôi không biết
Hướng dẫn: Jake cách tôi mười feet. Brynn cách Jake một trăm feet. Tôi gần Jake hay Brynn hơn?
Target: Jake

Bảng 4: Các nhiệm vụ mà điểm số của Cappy cho thấy bất lợi rõ ràng so với self-scoring của FLAN-T5-XXL.

--- TRANG 9 ---
mà chúng tôi đã mô tả trong Mục 3.2, bằng cách sử dụng RoBERTa làm khởi tạo mô hình thay vì Cappy. Chúng tôi cũng ablate tăng cường dữ liệu sử dụng LLM, được mô tả trong Mục 3.2 trong quá trình xây dựng dữ liệu hồi quy hạ nguồn cho Cappy.

Bảng 6 hiển thị kết quả. Việc ablation của tiền huấn luyện hoặc tăng cường dữ liệu sử dụng LLM đều dẫn đến sự giảm hiệu suất đáng chú ý, do đó làm nổi bật tầm quan trọng của cả hai thành phần này trong phương pháp đề xuất của chúng tôi. Hơn nữa, quy mô sự giảm hiệu suất cho thấy rằng tác động của tăng cường dữ liệu quan trọng hơn so với tiền huấn luyện trong việc điều chỉnh hạ nguồn của LLM.

5 Kết luận và Thảo luận
Chúng tôi cung cấp một bộ chấm điểm nhẹ đã được tiền huấn luyện, Cappy, để nâng cao hiệu suất và hiệu quả của các LLMs đa nhiệm vụ. Cappy nhận một hướng dẫn và một phản hồi ứng viên làm đầu vào, và tạo ra một điểm số từ 0 đến 1. Điểm số cho biết độ chính xác ước tính của phản hồi liên quan đến hướng dẫn. Một phương pháp giám sát yếu được đề xuất để xây dựng dữ liệu tiền huấn luyện của Cappy theo kiểu hồi quy. Chúng tôi đề xuất một cách thức lựa chọn ứng viên để áp dụng Cappy vào việc giải quyết nhiệm vụ thực tế. Cụ thể, Cappy có thể được sử dụng độc lập hoặc cộng tác với một LLM đa nhiệm vụ hiện có, đóng vai trò như một thành phần phụ trợ. Các thí nghiệm của chúng tôi chứng minh rằng Cappy vượt trội hơn các LLMs đa nhiệm vụ lớn hơn nhiều về kích thước, trên 11 nhiệm vụ hiểu ngôn ngữ. Bên cạnh đó, Cappy nâng cao hiệu suất của FLAN-T5 trong việc điều chỉnh cho 45 nhiệm vụ phức tạp được rút ra từ BIG-Bench, mà không yêu cầu tinh chỉnh LLM. Hơn nữa, Cappy có thể hợp tác hiệu quả với các chiến lược điều chỉnh LLM khác như tinh chỉnh và học trong ngữ cảnh, cung cấp cải thiện hiệu suất thêm.

Hạn chế và Hướng Tương lai
Trong Mục 4.2.1, chúng tôi đã phân tích một số hạn chế của Cappy, cụ thể trong lĩnh vực toán học và các vấn đề logic phức tạp. Ở đây, chúng tôi chi tiết một số hạn chế khác và hướng tương lai.

Điểm Rouge-L cho Giám sát Yếu Trong việc xây dựng dữ liệu tiền huấn luyện của Cappy, Rouge-L đóng vai trò như metric để đánh giá độ chính xác của việc sinh mô hình. Tuy nhiên, Rouge-L có thể không phải là proxy tối ưu cho độ chính xác của việc sinh mô hình, và không có sự đồng thuận trong cộng đồng ML và NLP về metric tốt nhất cho tất cả các nhiệm vụ. Hiện tại, Rouge-L thường được sử dụng trong các tình huống đa nhiệm vụ để báo cáo hiệu suất mô hình cho các nhiệm vụ kiểu sinh [10]. Mặc dù hiệu suất của Cappy trong các thí nghiệm của chúng tôi chứng minh Rouge-L là một lựa chọn thiết kế hợp lý, việc điều tra metric phù hợp nhất cho các ứng dụng đa nhiệm vụ vẫn là một hướng nghiên cứu có giá trị cao.

Tổng hợp Câu trả lời qua Nhiều Thế hệ Đóng góp chính của công trình này là việc phát triển và ứng dụng mô hình đã được tiền huấn luyện, Cappy, cho các ứng dụng đa nhiệm vụ. Đối với lựa chọn mẫu từ nhiều thế hệ mô hình, chúng tôi sử dụng cách thức argmax đơn giản chọn mẫu có điểm số lớn nhất. Tuy nhiên, nghiên cứu gần đây với các kỹ thuật tổng hợp câu trả lời được thiết kế tốt [28] gợi ý các con đường tiềm năng để tinh chỉnh tổng hợp câu trả lời với Cappy, để cải thiện thêm hiệu suất của học đa nhiệm vụ.

Không Xử lý Các Nhiệm vụ Ngoài Chuyên môn của LLM Mục tiêu của Cappy là nâng cao hiệu suất của các nhiệm vụ mà LLM xương sống có hiểu biết cơ bản về đầu vào dữ liệu. Tuy nhiên, Cappy không tác động đến khả năng nội tại của LLM. Cũng đáng chú ý là nhiều LLMs đa nhiệm vụ, như FLAN-T5 được sử dụng trong các thí nghiệm của chúng tôi, đã thể hiện thành thạo trên một loạt các lĩnh vực rộng, bao gồm các lĩnh vực như y học, luật pháp, và lập trình.

Điều chỉnh LLM Đơn lẻ Trong các thí nghiệm của công trình này, chúng tôi điều chỉnh một LLM duy nhất cho nhiều lĩnh vực với Cappy. Trong tương lai, Cappy như một mô hình đã được tiền huấn luyện có thể được sử dụng theo những cách sáng tạo khác ngoài việc sử dụng trên các LLMs đơn lẻ. Ví dụ, Cappy có thể làm việc như một bộ lọc cho các thế hệ từ nhiều LLMs. Trong trường hợp này, Cappy đóng vai trò chọn LLM tốt nhất liên quan đến một đầu vào cụ thể.

Giải quyết Các Hướng dẫn Giống Con người Hơn và Tận dụng Phản hồi Con người Trong công trình này, trọng tâm của chúng tôi là học đa nhiệm vụ nơi các nhiệm vụ được định nghĩa rõ ràng. Trong tương lai, Cappy có thể được áp dụng tiềm năng để giải quyết các hướng dẫn "giống con người" hơn nơi các nhiệm vụ thường được định nghĩa mơ hồ. Để đạt được điều này, sẽ rất có lợi khi tận dụng dữ liệu phản hồi con người tốn kém nhưng chất lượng cao, điều này sẽ yêu cầu thiết kế thuật toán thêm. Đây là điều đáng để chúng tôi khám phá thêm.

--- TRANG 10 ---
Lời cảm ơn
Chúng tôi cảm ơn Google Research đã hỗ trợ Bowen Tan làm việc như một nghiên cứu sinh thực tập. Eric Xing và Bowen Tan cũng đã được hỗ trợ một cách tử tế bởi NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NIGMS R01GM140467, NSF IIS2123952, NSF BCS2040381, một Giải thưởng Nghiên cứu Amazon, NSF IIS2311990, và DARPA ECOLE HR00112390063. Zhiting Hu được hỗ trợ một phần bởi DARPA ECOLE HR00112390063.

Tài liệu tham khảo
[1]S. H. Bach, V. Sanh, Z. X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Févry, Z. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David, C. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S. Al-shaibani, S. Sharma, U. Thakker, K. Almubarak, X. Tang, M. T.-J. Jiang, và A. M. Rush. Promptsource: An integrated development environment and repository for natural language prompts. ArXiv, abs/2202.01279, 2022.

[2]M. Bakker, M. Chadwick, H. Sheahan, M. Tessler, L. Campbell-Gillingham, J. Balaguer, N. McAleese, A. Glaese, J. Aslanides, M. Botvinick, et al. Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems, 35:38176–38189, 2022.

[3]W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, và E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.

[4]H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, D. Valter, S. Narang, G. Mishra, A. W. Yu, V. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. hsin Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, và J. Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.

[5]Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, và Z. Sui. A survey for in-context learning. ArXiv, abs/2301.00234, 2022.

[6]A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.

[7]J. E. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, và W. Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021.

[8]Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, và S. Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. ArXiv, abs/2304.01933, 2023.

[9]J. Huang và K. C.-C. Chang. Towards reasoning in large language models: A survey. ArXiv, abs/2212.10403, 2022.

[10] S. Iyer, X. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, B. O'Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, và V. Stoyanov. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. ArXiv, abs/2212.12017, 2022.

[11] N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles, et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1–14, 2023.

[12] B. Kratzwald, A. Eigenmann, và S. Feuerriegel. Rankqa: Neural question answering with answer re-ranking. ArXiv, abs/1906.03008, 2019.

[13] N. Lambert, L. Castricato, L. von Werra, và A. Havrilla. Illustrating reinforcement learning from human feedback (rlhf). Hugging Face Blog, 2022. https://huggingface.co/blog/rlhf.

--- TRANG 11 ---
[14] J. Lee, S. Yun, H. Kim, M. Ko, và J. Kang. Ranking paragraphs for improving answer recall in open-domain question answering. ArXiv, abs/1810.00494, 2018.

[15] X. L. Li và P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), abs/2101.00190, 2021.

[16] B. Y. Lin, K. Tan, C. Miller, B. Tian, và X. Ren. Unsupervised cross-task generalization via retrieval augmentation. NeurIPS, 2022.

[17] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.

[18] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, và G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55:1 – 35, 2021.

[19] T.-Y. Liu et al. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval, 3(3):225–331, 2009.

[20] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, và V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.

[21] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.

[22] M. Ravaut, S. R. Joty, và N. F. Chen. Summareranker: A multi-task mixture-of-experts re-ranking framework for abstractive summarization. In Annual Meeting of the Association for Computational Linguistics, 2022.

[23] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Févry, J. A. Fries, R. Teehan, S. R. Biderman, L. Gao, T. Bers, T. Wolf, và A. M. Rush. Multitask prompted training enables zero-shot task generalization. ICLR, 2022.

[24] D. Sileo. tasksource: Structured dataset preprocessing annotations for frictionless extreme multi-task learning and evaluation. arXiv preprint arXiv:2301.05948, 2023.

[25] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. A. Rahane, A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlmuller, A. M. Dai, A. D. La, A. K. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakacs, B. R. Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. Ozyurt, B. Hedayatnia, B. Neyshabur, B. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. S. Howald, C. Diao, C. Dour, C. Stinson, C. Argueta, C. F. Ram'irez, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu, C. Callison-Burch, C. Waites, C. Voigt, C. D. Manning, C. Potts, C. T. Ramirez, C. Rivera, C. Siro, C. Raffel, C. Ashcraft, C. Garbacea, D. Sileo, D. H. Garrette, D. Hendrycks, D. Kilman, D. Roth, D. Freeman, D. Khashabi, D. Levy, D. Gonz'alez, D. Hernandez, D. Chen, D. Ippolito, D. Gilboa, D. Dohan, D. Drakard, D. Jurgens, D. Datta, D. Ganguli, D. Emelin, D. Kleyko, D. Yuret, D. Chen, D. Tam, D. Hupkes, D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H. Lee, E. Shutova, E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. P. Donoway, E. Pavlick, E. Rodolà, E. F. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. J. Jerzak, E. Kim, E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Mart'inez-Plumed, F. Happ'e, F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo, G. Mariani, G. Wang, G. Jaimovitch-L'opez, G. Betz, G. Gur-Ari, H. Galijasevic, H. S. Kim, H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar,

--- TRANG 12 ---
H. Shevlin, H. Schütze, H. Yakura, H. Zhang, H. Wong, I. A.-S. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion, J. Hilton, J. Lee, J. F. Fisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Kocón, J. Thompson, J. Kaplan, J. Radom, J. N. Sohl-Dickstein, J. Phang, J. Wei, J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim, J. Taal, J. Engel, J. O. Alabi, J. Xu, J. Song, J. Tang, J. W. Waweru, J. Burden, J. Miller, J. U. Balis, J. Berant, J. Frohberg, J. Rozen, J. Hernández-Orallo, J. Boudeman, J. Jones, J. B. Tenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth, K. Gopalakrishnan, K. Ignatyeva, K. Markert, K. D. Dhole, K. Gimpel, K. O. Omondi, K. W. Mathewson, K. Chiafullo, K. Shkaruta, K. Shridhar, K. McDonell, K. Richardson, L. Reynolds, L. Gao, L. Zhang, L. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency, L. Moschella, L. Lam, L. Noble, L. Schmidt, L. He, L. O. Colón, L. Metz, L. K. cSenel, M. Bosma, M. Sap, M. ter Hoeve, M. Andrea, M. S. Farooqi, M. Faruqui, M. Mazeika, M. Baturan, M. Marelli, M. Maru, M. Quintana, M. Tolkiehn, M. Giulianelli, M. Lewis, M. Potthast, M. Leavitt, M. Hagen, M. Schubert, M. Baitemirova, M. Arnaud, M. A. McElrath, M. A. Yee, M. Cohen, M. Gu, M. I. Ivanitskiy, M. Starritt, M. Strube, M. Swkedrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain, M. Xu, M. Suzgun, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini, T. MukundVarma, N. Peng, N. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. S. Roberts, N. Doiron, N. Nangia, N. Deckers, N. Muennighoff, N. S. Keskar, N. Iyer, N. Constant, N. Fiedel, N. Wen, O. Zhang, O. Agha, O. Elbaghdadi, O. Levy, O. Evans, P. A. M. Casares, P. Doshi, P. Fung, P. P. Liang, P. Vicol, P. Alipoormolabashi, P. Liao, P. Liang, P. W. Chang, P. Eckersley, P. M. Htut, P.-B. Hwang, P. Milkowski, P. S. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. LYU, Q. Chen, R. Banjade, R. E. Rudolph, R. Gabriel, R. Habacker, R. R. Delgado, R. Millière, R. Garg, R. Barnes, R. A. Saurous, R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. L. Bras, R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. Chi, R. Lee, R. Stovall, R. Teehan, R. Yang, S. J. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer, S. Wiseman, S. Gruetter, S. Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A. Rous, S. Ghazarian, S. Ghosh, S. Casey, S. Bischoff, S. Gehrmann, S. Schuster, S. Sadeghi, S. S. Hamdan, S. Zhou, S. Srivastava, S. Shi, S. Singh, S. Asaadi, S. S. Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, S. Debnath, S. Shakeri, S. Thormeyer, S. Melzi, S. Reddy, S. P. Makini, S. hwan Lee, S. B. Torene, S. Hatwar, S. Dehaene, S. Divic, S. Ermon, S. R. Biderman, S. C. Lin, S. Prasad, S. T. Piantadosi, S. M. Shieber, S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu, T. A. Ali, T. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang, T. Nkinyili, T. Schick, T. N. Kornev, T. Telleen-Lawton, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj, T. Khot, T. O. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. V. Ramasesh, V. U. Prabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders, W. Zhang, W. Vossen, X. Ren, X. Tong, X. Wu, X. Shen, Y. Yaghoobzadeh, Y. Lakretz, Y. Song, Y. Bahri, Y. J. Choi, Y. Yang, Y. Hao, Y. Chen, Y. Belinkov, Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Xinran, Z. Zhao, Z. F. Wang, Z. J. Wang, Z. Wang, Z. Wu, S. Singh, và U. Shaham. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615, 2022.

[26] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, và P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.

[27] B. Tan, Y. Zhu, L. Liu, H. Wang, Y. Zhuang, J. Chen, E. Xing, và Z. Hu. Redco: A lightweight tool to automate distributed training of llms on any gpu/tpus. arXiv preprint arXiv:2310.16355, 2023.

[28] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, và D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

[29] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. EMNLP, 2022.

[30] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, và Q. V. Le. Finetuned language models are zero-shot learners. ICLR, 2022.

[31] Z. Yuan, H. Yuan, C. Tan, W. Wang, và S. Huang. How well do large language models perform in arithmetic tasks? ArXiv, abs/2304.02015, 2023.

--- TRANG 13 ---
[32] Y. Zha, Y. Yang, R. Li, và Z. Hu. Text alignment is an efficient unified model for massive nlp tasks. arXiv preprint arXiv:2307.02729, 2023.

[33] L. Zheng, Y. Sheng, W.-L. Chiang, H. Zhang, J. E. Gonzalez, và I. Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings, 2023.

--- TRANG 14 ---
A So sánh Cappy với các phương pháp điều chỉnh khác

Cappy không có nghĩa là đánh bại các phương pháp điều chỉnh khác như tinh chỉnh, học trong ngữ cảnh, và prompt tuning. So với các phương pháp này, điều chỉnh với Cappy là một lựa chọn thay thế tự do khỏi các ràng buộc liên quan đến lưu trữ, bộ nhớ thiết bị, khả năng truy cập mô hình, và các hạn chế về mẫu huấn luyện. Hơn nữa, Cappy không có bất kỳ giả định nào về mô hình xương sống, cho phép tích hợp liền mạch với các điều chỉnh khác, nơi Cappy cung cấp việc thúc đẩy hiệu suất ổn định và đáng kể với chi phí bổ sung ít. Để minh họa điều này, chúng tôi bao gồm một thí nghiệm dưới đây kết hợp Cappy với học trong ngữ cảnh và prompt-tuning, tương ứng.

Thiết lập Rouge-L
FLAN-T5-Large Đông lạnh + Cappy LARGE (của chúng tôi) 30.75
Học trong ngữ cảnh + Nucleus 23.65
Học trong ngữ cảnh + Self-scoring 27.00
Học trong ngữ cảnh + Cappy LARGE (của chúng tôi) 31.84
Prompt-tuning + Nucleus 34.00
Prompt-tuning + Self-scoring 38.43
Prompt-tuning + Cappy LARGE (của chúng tôi) 42.71

Bảng 7: Hiệu suất Big-Bench dưới nhiều thiết lập điều chỉnh với FLAN-T5-Large

Cụ thể, chúng tôi thêm so sánh với prompt tuning và học trong ngữ cảnh trong thí nghiệm điều chỉnh BIG-Bench của chúng tôi với FLAN-T5-Large làm mô hình xương sống. Đối với prompt tuning, chúng tôi áp dụng prefix tuning, thường được coi là phù hợp cho các nhiệm vụ sinh, với 20 virtual tokens. Như được chứng minh bởi các kết quả được trình bày trong Bảng 7, Cappy mang lại cải thiện hiệu suất thêm trên cả học trong ngữ cảnh và prompt tuning.
