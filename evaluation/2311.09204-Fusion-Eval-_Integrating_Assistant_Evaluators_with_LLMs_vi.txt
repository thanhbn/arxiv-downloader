# Fusion-Eval: Tích hợp Các Đánh giá viên Hỗ trợ với LLMs

Lei Shu1∗Nevan Wichers1Liangchen Luo1Yun Zhu1Yinxiao Liu1
Jindong Chen1Lei Meng2
Google Deepmind1, Google2

## Tóm tắt
Đánh giá các hệ thống ngôn ngữ tự nhiên đặt ra những thách thức đáng kể, đặc biệt trong lĩnh vực hiểu ngôn ngữ tự nhiên và lý luận cấp cao. Trong bài báo này, chúng tôi giới thiệu "Fusion-Eval", một phương pháp tiếp cận sáng tạo tận dụng các Mô hình Ngôn ngữ Lớn (LLMs) để tích hợp những hiểu biết từ nhiều đánh giá viên hỗ trợ khác nhau. LLM được cung cấp ví dụ để đánh giá cùng với điểm số từ các đánh giá viên hỗ trợ. Mỗi đánh giá viên này chuyên về việc đánh giá những khía cạnh khác nhau của phản hồi. Fusion-Eval đạt được mức tương quan Kendall-Tau ở cấp độ hệ thống 0.962 với con người trên SummEval và mức tương quan Spearman ở cấp độ lượt hội thoại 0.744 trên TopicalChat, cao hơn đáng kể so với các phương pháp cơ sở. Những kết quả này làm nổi bật tiềm năng to lớn của Fusion-Eval trong lĩnh vực đánh giá hệ thống ngôn ngữ tự nhiên.

## 1 Giới thiệu
Đánh giá hiệu suất của các mô hình sinh ngôn ngữ tự nhiên có những thách thức đáng kể (Ouyang et al., 2022), đặc biệt về mặt tiêu chuẩn đánh giá và mô hình đánh giá (Wang et al., 2023b). Nghiên cứu này tập trung vào phần sau. Thông thường, các mô hình đánh giá rơi vào ba loại: đánh giá dựa trên con người, đánh giá dựa trên các chỉ số tự động và đánh giá dựa trên mô hình. Trong số này, đánh giá của con người được coi là đáng tin cậy nhất, nhưng chúng có chi phí cao và vấn đề về khả năng mở rộng.

Các chỉ số tự động như BLEU (Papineni et al., 2002) và ROUGE (Lin, 2004) phổ biến trong các đánh giá, dựa vào so sánh với tham chiếu tiêu chuẩn 'vàng'. Tuy nhiên, việc tạo ra những tham chiếu vàng này là một quá trình tốn nhiều công sức. Hơn nữa, các nghiên cứu như Fabbri et al. (2021) đã chứng minh rằng những chỉ số tự động này thường không tương quan tốt với đánh giá của con người.

Các đánh giá dựa trên mô hình nhằm tăng cường mức tương quan với đánh giá của con người bằng cách sử dụng mạng nơ-ron được tinh chỉnh trên các tập dữ liệu cụ thể. Các đánh giá viên nơ-ron như BLEURT (Sellam et al., 2020) và biến thể SMART (Amplayo et al., 2022) của nó cho thấy sự cải thiện trong việc phù hợp với đánh giá của con người trong nhiều nhiệm vụ sinh. Những mô hình này cung cấp tính linh hoạt trong các phương pháp đánh giá. Chúng có thể so sánh phản hồi với nguồn (không cần tham chiếu), hoặc với tiêu chuẩn vàng (phụ thuộc vào tham chiếu).

Những tiến bộ gần đây đã thấy việc sử dụng các Mô hình Ngôn ngữ Lớn (LLMs) làm đánh giá viên không cần tham chiếu trong các nhiệm vụ Sinh Ngôn ngữ Tự nhiên (NLG). Đáng chú ý, các nghiên cứu của Fu et al. (2023); Wang et al. (2023a) đã tận dụng LLMs để đánh giá đầu ra ứng viên chỉ dựa trên xác suất sinh của chúng, loại bỏ nhu cầu so sánh văn bản tham chiếu. Thêm vào đó, Liu et al. (2023) đã giới thiệu một phương pháp gọi là G-Eval, nơi LLMs, được hướng dẫn bởi tiêu chí đánh giá do con người tạo ra, chấm điểm phản hồi. Các meta-đánh giá cho thấy rằng những đánh giá viên dựa trên LLM này đạt mức tương quan với con người ngang bằng với các đánh giá viên nơ-ron cỡ trung bình (Zhong et al., 2022). Trong bối cảnh những phát triển này trong các mô hình đánh giá, câu hỏi sau đây xuất hiện:

"Các Mô hình Ngôn ngữ Lớn (LLMs) có thể tích hợp các đánh giá viên hiện tại để đạt được mức tương quan cao hơn với đánh giá của con người không?"

Để trả lời câu hỏi này, chúng tôi giới thiệu Fusion-Eval, một framework đánh giá sáng tạo tích hợp nhiều đánh giá viên hiện có—được gọi là các đánh giá viên hỗ trợ—để tăng cường mức tương quan với đánh giá của con người. Fusion-Eval nhắc LLM với một ví dụ để đánh giá và điểm số được cung cấp bởi các đánh giá viên hỗ trợ. Trong nghiên cứu của chúng tôi, chúng tôi xem xét đánh giá không cần tham chiếu. Fusion-Eval có thể đánh giá bất kỳ nhiệm vụ ngôn ngữ tự nhiên nào mà có các đánh giá viên hỗ trợ sẵn có. Tuy nhiên, hiệu quả của nó phụ thuộc vào chất lượng của các đánh giá viên hỗ trợ, khiến nó phù hợp hơn với các nhiệm vụ sinh văn bản đã được thiết lập tốt.

## 2 Phương pháp
Fusion-Eval là một framework đánh giá tận dụng một Mô hình Ngôn ngữ Lớn (LLM) để hợp nhất các đánh giá viên hỗ trợ, nhằm cải thiện chất lượng chấm điểm. Mục tiêu của framework là đánh giá một hệ thống Sinh Ngôn ngữ Tự nhiên (NLG) theo một hoặc nhiều tiêu chí theo cách có mức tương quan cao với đánh giá của con người. Các ví dụ kiểm tra là những gì Fusion-Eval sẽ đánh giá. Ví dụ trong tập dữ liệu SummEval, một ví dụ kiểm tra là một bài báo tin tức và một bản tóm tắt. Trong trường hợp này, Fusion-Eval sẽ đánh giá chất lượng của bản tóm tắt cho bài báo tin tức. Mỗi đánh giá viên hỗ trợ nhận một ví dụ kiểm tra và trả về một điểm số. Framework Fusion-Eval sau đó lấy mô tả nhiệm vụ đánh giá, ví dụ kiểm tra, và điểm số từ đánh giá viên hỗ trợ làm đầu vào. Chúng tôi đề xuất hai giải pháp Fusion-Eval:

(1) Fusion-Eval không có Kế hoạch (FE-NoPlan) Trong phương pháp này, Mô hình Ngôn ngữ Lớn (LLM) được nhắc trực tiếp với tiêu chí đánh giá của nhiệm vụ, thông tin chi tiết về các đánh giá viên hỗ trợ, và yêu cầu điểm số đánh giá. Lời nhắc này cũng bao gồm chỗ trống cho điểm số đánh giá viên hỗ trợ và ví dụ kiểm tra, cũng như hướng dẫn về định dạng mà LLM nên sử dụng để tạo ra điểm số đánh giá. Cách tiếp cận đơn giản này yêu cầu LLM diễn giải tiêu chí đánh giá và thông tin về các đánh giá viên hỗ trợ mà không có kế hoạch định sẵn. Bảng 1 trình bày mẫu lời nhắc đơn giản cho Fusion-Eval không có Kế hoạch (FE-NoPlan).

(2) Fusion-Eval có Kế hoạch (FE) Cách tiếp cận này giới thiệu một kế hoạch chỉ định đánh giá viên hỗ trợ nào sẽ sử dụng để đánh giá mỗi tiêu chí cụ thể, kèm theo các bước chi tiết để LLM tuân theo khi đánh giá ví dụ kiểm tra. Nó được thiết kế cho các nhiệm vụ đánh giá phức tạp có lợi từ việc hướng dẫn. Kế hoạch cũng tăng tính minh bạch vì người ta có thể thấy đánh giá viên nào được sử dụng cho mục đích gì. Có sự đánh đổi giữa việc sử dụng kế hoạch do con người tạo ra hay do LLM tạo ra và framework của chúng tôi phù hợp với cả hai tùy chọn. Trong khi các kế hoạch do con người tạo ra có xu hướng chính xác hơn, những kế hoạch do LLMs tạo ra cung cấp khả năng mở rộng lớn hơn và thích ứng nhanh hơn với các nhiệm vụ đánh giá mới. Bài báo này trình bày Fusion-Eval có Kế hoạch (FE), sử dụng các kế hoạch được tạo ra bởi một LLM.

Khi sử dụng LLM để tạo kế hoạch, LLM được nhắc với định nghĩa nhiệm vụ, tiêu chí, và thông tin về các đánh giá viên hỗ trợ. Điều này tương tự với phương pháp auto chain-of-thought trong G-Eval (Liu et al., 2023), nhưng nó độc đáo kết hợp các đánh giá viên hỗ trợ. Quy trình làm việc của Fusion-Eval có Kế hoạch được minh họa trong Hình 1, bao gồm một quá trình auto chain-of-thought (Liu et al., 2023). Ban đầu, chúng tôi tạo một lời nhắc (hộp văn bản bên trái nhất trong Hình 1) để xin kế hoạch từ LLM. Hộp văn bản thứ hai cho thấy kế hoạch do LLM tạo ra đã được cắt gọn (các kế hoạch toàn diện với mẫu có sẵn trong Phụ lục B.1 và B.2).

Một khi chúng tôi có được kế hoạch, chúng tôi chèn nó vào lời nhắc được mô tả trong phần FE-NoPlan. Điều này tạo thành lời nhắc hoàn chỉnh để rút ra điểm số Fusion-Eval cuối cùng, được mô tả trong hộp văn bản thứ ba trong Hình 1.

Để điều chỉnh Fusion-Eval cho một nhiệm vụ đánh giá khác, người ta cần cập nhật tiêu chí và mô tả đánh giá viên hỗ trợ và tạo lại kế hoạch. Thêm vào đó, việc thu thập điểm số đánh giá viên hỗ trợ mới cho nhiệm vụ là cần thiết. Mẫu Fusion-Eval đầy đủ có sẵn trong Phụ lục B.1 cho SummEval và B.2 cho TopicalChat.

Framework của chúng tôi tương thích với nhiều kế hoạch có thể, miễn là chúng mô tả một cách hợp lệ để kết hợp các đánh giá viên hỗ trợ. Tìm kế hoạch tối ưu nằm ngoài phạm vi nghiên cứu của chúng tôi.

Thực thi Lời nhắc Trong cả hai giải pháp, mẫu lời nhắc đánh giá đã chuẩn bị được sử dụng với mỗi ví dụ kiểm tra. Mẫu này được điền với các đầu vào, phản hồi, và điểm số đánh giá viên hỗ trợ cho mỗi ví dụ kiểm tra. LLM thực thi sau đó xử lý lời nhắc đã điền này, tạo ra điểm số đánh giá cuối cùng của Fusion-Eval như được hiển thị trong hộp văn bản bên phải nhất trong Hình 1. Chúng tôi thấy rằng LLM tạo ra điểm số đánh giá theo đúng định dạng, vì vậy chúng tôi không cần làm gì khác để kiểm soát đầu ra.

LLM thực thi xử lý lời nhắc hoàn chỉnh và tạo ra điểm số bằng số cho mỗi chiều đánh giá. Các LLMs được cấu hình để tạo ra 8 dự đoán với nhiệt độ 0.5 cho PaLM2 và 0.1 cho GPT-4. Điểm số Fusion-Eval cuối cùng là trung bình của 8 dự đoán. Chúng tôi làm điều này vì chúng tôi không thể có được xác suất log từ API GPT.

## 3 Thí nghiệm
Chúng tôi thực hiện meta-đánh giá Fusion-Eval, sử dụng các tiêu chuẩn SummEval (Fabbri et al., 2021) và TopicalChat (Mehri & Eskenazi, 2020). Chúng tôi chọn SummEval và TopicalChat làm tiêu chuẩn cho meta-đánh giá vì UniEval (Zhong et al., 2022) và G-Eval (Liu et al., 2023) cũng chỉ sử dụng những tiêu chuẩn đó. Điều này tạo điều kiện so sánh hiệu quả với kết quả của họ. Những tiêu chuẩn này được công nhận rộng rãi và cung cấp một phạm vi toàn diện các chỉ số đánh giá. Chúng tôi cố ý loại trừ các tập dữ liệu dựa trên chú thích của một người đánh giá duy nhất (Stiennon et al., 2020; Bai et al., 2022) hoặc bị giới hạn ở một chỉ số duy nhất (Wang et al., 2020).

### 3.1 Thiết lập Thí nghiệm
SummEval (Fabbri et al., 2021), một tiêu chuẩn cho đánh giá tóm tắt văn bản, bao gồm 1600 điểm dữ liệu. Mỗi điểm dữ liệu bao gồm điểm đánh giá trung bình từ ba chuyên gia trên thang điểm từ 1 đến 5, spanning bốn chiều chất lượng tóm tắt: tính mạch lạc (Coh), tính nhất quán (Con), tính lưu loát (Flu) và tính liên quan (Rel). Điểm "Tổng thể" được tính là trung bình của bốn chiều này.

TopicalChat (Mehri & Eskenazi, 2020), một tiêu chuẩn để đánh giá sinh phản hồi đối thoại dựa trên kiến thức, bao gồm 360 điểm dữ liệu. Nó có đánh giá của con người từ ba chuyên gia trên sáu chiều: tính mạch lạc (Coh), tính hấp dẫn (Eng), tính tự nhiên (Nat), tính có căn cứ (Gro), tính dễ hiểu (Und), và tổng thể. Điểm cho tính tự nhiên, tính mạch lạc, và tính hấp dẫn trên thang điểm từ 1 đến 3, trong khi tính có căn cứ và tính dễ hiểu được chấm điểm từ 0 đến 1. Chiều tổng thể được đánh giá trên thang điểm từ 1 đến 5. Mỗi điểm dữ liệu bao gồm lịch sử cuộc trò chuyện, một sự kiện có căn cứ, và một phản hồi lượt tiếp theo tiềm năng.

Để đo lường mức tương quan giữa kết quả được tạo ra bởi Fusion-Eval và đánh giá của con người, chúng tôi sử dụng điểm Kendall-Tau cho phân tích cấp độ hệ thống trong SummEval (Fabbri et al., 2021), và điểm Spearman cho phân tích cấp độ lượt trong TopicalChat (Mehri & Eskenazi, 2020) để phù hợp với phương pháp chấm điểm ban đầu của mỗi tiêu chuẩn. Mặc dù UniEval Zhong et al. (2022) và G-Eval (Liu et al., 2023) trình bày mức tương quan cấp độ tóm tắt trong các bài báo của họ, chúng tôi đã rút ra mức tương quan cấp độ hệ thống từ dự đoán được tiết lộ của họ để duy trì nhất quán với phương pháp đánh giá ban đầu của SummEval (Fabbri et al., 2021). Điều chỉnh này giải thích cho sự khác biệt giữa điểm số được báo cáo của chúng tôi và những điểm số ban đầu được công bố trong nghiên cứu G-Eval.

Trong các thí nghiệm của chúng tôi, PaLM2-Large (Anil et al., 2023) và GPT-4 (OpenAI, 2023) đóng vai trò là các Mô hình Ngôn ngữ Lớn (LLMs) để thực thi, được chỉ định là FE-PaLM2 và FE-GPT-4, tương ứng. Trong nghiên cứu ablation FE-PaLM2-NoPlan, chúng tôi sử dụng phương pháp Fusion-Eval không có Kế hoạch như được mô tả trong Phần 2.

Chúng tôi tích hợp một số đánh giá viên hỗ trợ: NLI (Bowman et al., 2015), BLEURT (Sellam et al., 2020), và SumBLEURT—một biến thể BLEURT được tinh chỉnh cho đánh giá tóm tắt của con người (Clark et al., 2023). Chúng tôi cũng có được xác suất mà PaLM sẽ tạo ra phản hồi từ tập dữ liệu cho ngữ cảnh, theo các phương pháp trong Fu et al. (2023) và Wang et al. (2023a). Xác suất của phản hồi cao hơn nếu nó có khả năng hơn theo PaLM2. Chúng tôi sử dụng điều này làm đánh giá viên hỗ trợ được gọi là PaLM2 Prob.

Theo hiểu biết tốt nhất của chúng tôi, các LLMs được sử dụng trong Fusion-Eval không được huấn luyện trên các tập dữ liệu SummEval và TopicalChat.

### 3.2 Đường cơ sở
Để có so sánh kỹ lưỡng, chúng tôi đã meta-đánh giá Fusion-Eval với một loạt phương pháp cơ sở trên tiêu chuẩn SummEval. Những đường cơ sở này bao gồm ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), CHRF (Popovi ́c, 2015), SMART (Amplayo et al., 2022), BERTScore (Zhang et al., 2019), MoverScore (Zhao et al., 2019), BARTScore (Yuan et al., 2021), UniEval (Zhong et al., 2022), và G-Eval (Liu et al., 2023).

UniEval (Zhong et al., 2022) phục vụ như một đánh giá viên nơ-ron đa chiều thống nhất cho nhiều khía cạnh khác nhau của sinh văn bản, đóng khung đánh giá như các nhiệm vụ QA. Nó tận dụng mô hình T5 được huấn luyện trước (Raffel et al., 2020) để mã hóa nhiệm vụ đánh giá, cùng với văn bản nguồn và đích, theo định dạng hỏi đáp, cuối cùng tính toán điểm QA làm chỉ số đánh giá. Tính linh hoạt này cho phép nó thích ứng với các nhiệm vụ đánh giá đa dạng thông qua các sửa đổi đơn giản đối với định dạng câu hỏi.

G-Eval (Liu et al., 2023) tận dụng LLMs và lý luận chain-of-thought (CoT) để đánh giá chất lượng văn bản được tạo ra thông qua cách tiếp cận điền biểu mẫu. Bằng cách chỉ nhập mô tả nhiệm vụ đánh giá và tiêu chí vào LLMs, nó nhắc chúng tạo ra một CoT nêu rõ các bước đánh giá chi tiết. Những bước này, kết hợp với lời nhắc ban đầu, sau đó được sử dụng để đánh giá đầu ra NLG. Thêm vào đó, các xác suất liên quan đến token đánh giá đầu ra được sử dụng để tinh chỉnh thêm chỉ số đánh giá. Chúng tôi đã rút ra điểm cho hầu hết các đường cơ sở từ bài báo SMART (Amplayo et al., 2022), trong khi cho UniEval¹ và G-Eval², chúng tôi tính toán điểm tương quan cấp độ hệ thống từ dự đoán truy cập mở của họ để phù hợp với framework đánh giá của SummEval (Fabbri et al., 2021), vì các bài báo ban đầu của họ chỉ cung cấp mức tương quan cấp độ tóm tắt.

Cho tiêu chuẩn TopicalChat, chúng tôi so sánh hiệu suất của Fusion-Eval với G-Eval (Liu et al., 2023) và UniEval (Zhong et al., 2022), sử dụng điểm từ các bài báo tương ứng của họ. Đáng chú ý, G-Eval không báo cáo điểm cho các chiều 'Und' và 'Tổng thể' hoặc dự đoán cho tiêu chuẩn TopicalChat, vì vậy những điểm này được bỏ qua khỏi so sánh của chúng tôi.

Chúng tôi giới thiệu DE-PaLM2 (Đánh giá viên Trực tiếp PaLM2) như một đường cơ sở ablation, sử dụng cách tiếp cận tương tự như G-Eval với lời nhắc tương tự. Đường cơ sở này cho thấy hiệu suất độc lập của PaLM2 trên các tiêu chuẩn SummEval và TopicalChat mà không có sự hỗ trợ từ các đánh giá viên khác. Danh hiệu DE-PaLM2, thay vì G-Eval (PaLM2), được chọn vì lời nhắc của G-Eval cho tiêu chuẩn TopicalChat không được tiết lộ, đòi hỏi việc triển khai riêng của chúng tôi về cách tiếp cận của G-Eval.

Chúng tôi đề xuất thêm một tập hợp các hàm tổng hợp để hợp nhất điểm từ các đánh giá viên hỗ trợ:

• AVG (Điểm Trung bình): Trung bình điểm từ tất cả đánh giá viên.
• LLMSel (Đánh giá viên Hỗ trợ được LLM Chọn): Điểm trung bình nhưng chỉ từ các đánh giá viên mà kế hoạch xác định là liên quan đến danh mục.
• CorrW (Trung bình Có Trọng số Tương quan): Trung bình của mỗi điểm đánh giá viên có trọng số bằng mức tương quan của đánh giá viên với đánh giá của con người.

Các hàng AE, (như "AVG (AE)") chỉ bao gồm các đánh giá viên hỗ trợ trong tổng hợp. Các hàng có tên của đánh giá viên LLM (như "AVG (AE, G-Eval-GPT-4)") sử dụng cả điểm đánh giá viên hỗ trợ và điểm từ đánh giá viên LLM trong tổng hợp.

Cho SummEval, điểm G-Eval và DE-PaLM (G-Eval Fluency từ 1-3) đã được điều chỉnh từ 1-5 đến thang điểm 0-1 để phù hợp với phạm vi chấm điểm của các đánh giá viên hỗ trợ. Cho TopicalChat, tổng hợp của chúng tôi chỉ bao gồm các đánh giá viên hỗ trợ và DE-PaLM2, vì dự đoán của G-Eval không có sẵn. Ngoài ra, điểm của DE-PaLM2 cho tính mạch lạc, tính hấp dẫn, và tính tự nhiên đã được ánh xạ lại từ 1-3 đến 0-1 để khớp với phạm vi chấm điểm của BLEURT và PaLM2 Prob.

### 3.3 Phân tích Kết quả
Bảng 2 và 3 trình bày mức tương quan của các đường cơ sở, đánh giá viên hỗ trợ, và Fusion-Eval với đánh giá của con người.

#### 3.3.1 Hiệu suất Fusion-Eval
Fusion-Eval vượt trội so với tất cả các mô hình cơ sở và phương pháp tổng hợp trong chiều tổng thể và gần như tất cả các chiều khác, như được chứng minh trong các hàng FE-GPT-4 và FE-PaLM2 của cả hai tập dữ liệu.

Phần còn lại của phân tích chúng tôi được dành cho mức tương quan tổng thể với đánh giá của con người. Trong số các phương pháp tổng hợp khác nhau cho các đánh giá viên hỗ trợ, phương pháp có trọng số theo tương quan với con người (CorrW) hoạt động tốt nhất. Tổng hợp điểm đánh giá viên trực tiếp LLM với điểm đánh giá viên hỗ trợ cho kết quả tốt hơn so với sử dụng đánh giá viên trực tiếp một mình cho PaLM2, và nó khớp hiệu suất cho các mô hình GPT. Cụ thể, AVG(AE, DE-PaLM2) và CorrW(AE, DE-PaLM2) cho thấy mức tương quan cao hơn với đánh giá của con người so với DE-PaLM2, gợi ý rằng các đánh giá viên hỗ trợ có thể nâng cao hiệu suất của LLM vượt ra ngoài khả năng độc lập của nó. Tuy nhiên, Fusion-Eval vượt qua những phương pháp tổng hợp này, khiến nó tốt hơn trong việc tận dụng các đánh giá viên hỗ trợ so với việc chỉ tổng hợp điểm đơn thuần.

Hiệu suất của FE-PaLM2 cao hơn FE-PaLM2-NoPlan, gợi ý rằng việc nhắc LLM với một kế hoạch là có lợi. Cải thiện này có thể được quy cho việc kế hoạch hỗ trợ LLM trong việc sử dụng các đánh giá viên hỗ trợ. Phát hiện này phù hợp với G-Eval (Liu et al., 2023), gợi ý rằng các bước đánh giá nội tại được tạo ra bởi LLMs lập kế hoạch nâng cao hiệu suất, đặc biệt trong các nhiệm vụ đánh giá phức tạp. Tuy nhiên, kế hoạch do LLM tạo ra được sử dụng trong các thí nghiệm của chúng tôi có lẽ không tối ưu. Tìm 'kế hoạch tối ưu' gần như không thể do độ phức tạp hàm mũ liên quan đến việc kết hợp tiêu chí và các đánh giá viên hỗ trợ. Chúng tôi nhận ra tiềm năng ảo giác trong các kế hoạch do LLM tạo ra và lưu ý rằng kế hoạch do con người tạo ra cũng có thể được sử dụng với Fusion-Eval.

#### 3.3.2 Thời gian Thực thi Fusion-Eval
Framework Fusion-Eval duy trì thời gian thực thi có thể quản lý được vì các đánh giá viên hỗ trợ có thời gian suy luận tối thiểu so với LLMs. Chạy tất cả các đánh giá viên hỗ trợ (NLI, BLEURT, và SumBLEURT) trên một ví dụ SummEval mất khoảng 0.125 giây trung bình. Các đánh giá viên đã được huấn luyện trước, loại bỏ nhu cầu đào tạo thêm. Có được kết quả Fusion-Eval sử dụng PaLM2, dựa trên điểm đánh giá viên hỗ trợ, mất khoảng 7 giây cho một ví dụ SummEval và 11.7 giây cho một ví dụ TopicalChat.

#### 3.3.3 Mức tương quan giữa Fusion-Eval và Đánh giá viên Hỗ trợ
Để hiểu việc thực thi của Fusion-Eval, chúng tôi phân tích mức tương quan giữa điểm của nó và điểm của các đánh giá viên hỗ trợ, cùng với các đánh giá viên được kế hoạch của LLM chọn. Bảng 5 và 6 chi tiết mức tương quan cho FE-PaLM2, trong khi Bảng 7 và 8 làm tương tự cho FE-GPT-4. Lựa chọn đánh giá viên của LLM lập kế hoạch được liệt kê trong Bảng 4.

Qua các chiều đánh giá, các đánh giá viên được LLM chọn luôn thể hiện mức tương quan cao hơn với cả FE-PaLM2 và FE-GPT-4 so với những đánh giá viên không được chọn. Ví dụ, trong tính mạch lạc của SummEval, SumBLEURT thể hiện mức tương quan cao hơn so với các đánh giá viên khác. Xu hướng tương tự cũng được quan sát thấy trong tính tự nhiên và tính dễ hiểu của TopicalChat. Điều này gợi ý Fusion-Eval thực sự dựa vào các đánh giá viên hỗ trợ được chọn nhiều hơn so với những đánh giá viên không được chọn.

Hơn nữa, việc không có mức tương quan hoàn hảo ("1") giữa Fusion-Eval và bất kỳ đánh giá viên hỗ trợ nào gợi ý rằng Fusion-Eval sử dụng các đánh giá viên hỗ trợ để bổ sung cho đánh giá của nó thay vì dựa hoàn toàn vào chúng.

## 4 Kết luận
Bài báo trình bày Fusion-Eval, một bộ tổng hợp sáng tạo sử dụng các Mô hình Ngôn ngữ Lớn (LLMs) cho các nhiệm vụ đánh giá đa dạng. Nó hiệu quả tích hợp các đánh giá viên hỗ trợ theo các tiêu chí cụ thể. Kết quả thực nghiệm cho thấy Fusion-Eval đạt được mức tương quan cao hơn với đánh giá của con người so với các đường cơ sở. LLMs rất mạnh mẽ, vì vậy thật thú vị khi việc tăng cường LLMs với điểm từ các phương pháp đơn giản hơn có thể cải thiện hiệu suất trong trường hợp này.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được duy trì như bản gốc]

## A Hạn chế và Nghiên cứu Tương lai
Độ dài của các mẫu lời nhắc thực thi cho SummEval (Phụ lục B.1) và TopicalChat (Phụ lục B.2) lần lượt là 662 và 990 từ. Các LLMs được sử dụng trong Fusion-Eval, bao gồm GPT-4 và PaLM2, có thể xử lý hiệu quả các lời nhắc có độ dài này. Tuy nhiên, các lời nhắc Fusion-Eval dài có thể gây ra thách thức cho LLMs có cửa sổ ngữ cảnh hạn chế. Để giải quyết điều này, chúng tôi đề xuất điều tra việc phân tách lời nhắc trong công việc tương lai để tăng cường tính tương thích của Fusion-Eval với nhiều LLMs khác nhau.

## B Phụ lục

### B.1 Mẫu Lời nhắc Đánh giá Fusion-Eval cho SummEval

Đánh giá một bản tóm tắt được cung cấp sử dụng tiêu chí: Tính mạch lạc, Tính nhất quán, Tính liên quan, và Tính lưu loát.

Các Đánh giá viên Hỗ trợ như NLI, BLEURT, và SUM BLEURT, cho điểm từ dưới 0 đến 1 (càng gần 1 càng tốt), sẽ hỗ trợ trong đánh giá này.

**1. NLI (Suy luận Ngôn ngữ Tự nhiên)**:
Đánh giá viên hỗ trợ này cung cấp điểm xác suất cho biết mức độ bản tóm tắt (giả thuyết) được suy ra từ bài báo tin tức gốc (tiền đề).

**Sử dụng**:
- **Đánh giá Tính nhất quán**: Xác suất suy luận cao cho thấy rằng bản tóm tắt được căn cứ về mặt sự thật với văn bản nguồn. Ngược lại, điểm thấp có thể cho thấy sự khác biệt hoặc những sự thật được tạo ra.

**2. BLEURT**:
Chỉ số này mô hình hóa đánh giá của con người. Nó cho điểm cho biết mức độ bản tóm tắt phù hợp với những gì các đánh giá viên con người có thể coi là bản tóm tắt tốt cho văn bản nguồn.

**Sử dụng**:
- **Đánh giá Tính liên quan và Tính nhất quán**: Điểm BLEURT cao sẽ gợi ý rằng bản tóm tắt hiệu quả nắm bắt những điểm thiết yếu của nguồn. Điểm thấp có thể cho thấy thiếu những điểm chính.

**3. SUM BLEURT (BLEURT Tóm tắt)**:
Được tinh chỉnh trên tập dữ liệu tóm tắt, đánh giá viên hỗ trợ này cung cấp cách tiếp cận có mục tiêu hơn để đo lường chất lượng tóm tắt trong bối cảnh đánh giá của con người.

**Sử dụng**:
- **Đánh giá Tính liên quan và Tính mạch lạc**: Như BLEURT, nhưng với sự chuyên môn hóa trong tóm tắt, SUM BLEURT có thể cung cấp những hiểu biết chính xác hơn về tính liên quan và tính mạch lạc của bản tóm tắt liên quan đến văn bản nguồn.

**Kế hoạch Sử dụng Đánh giá viên Hỗ trợ**:
1. **Đọc Bài báo Tin tức và Bản tóm tắt**: Bắt đầu với việc đọc thủ công để hình thành ấn tượng ban đầu.
2. **Sử dụng NLI & BLEURT cho Tính nhất quán**: Kiểm tra cả hai điểm. Điểm cao từ cả hai đánh giá viên hỗ trợ sẽ khẳng định lại tính nhất quán của bản tóm tắt.
3. **Sử dụng BLEURT & SUM BLEURT cho Tính liên quan**: Kiểm tra điểm từ cả hai đánh giá viên hỗ trợ. Điểm cao sẽ gợi ý bản tóm tắt tốt về mặt tính liên quan.
4. **Sử dụng SUM BLEURT cho Tính mạch lạc**: Kiểm tra điểm SUM BLEURT. Điểm cao sẽ gợi ý bản tóm tắt tốt về mặt tính mạch lạc.
5. **Đánh giá Thủ công cho Tính lưu loát**: Các đánh giá viên hỗ trợ không trực tiếp giải quyết tính lưu loát. Bạn sẽ đánh giá ngữ pháp, dấu câu, và cấu trúc câu thủ công.
6. **Đánh giá Cuối cùng**: Đầu ra của các đánh giá viên hỗ trợ sẽ thông báo và xác nhận các đánh giá của bạn, nhưng phán quyết cuối cùng sẽ dựa trên các tiêu chí và bước được cung cấp, với các đánh giá viên hỗ trợ phục vụ như hỗ trợ bổ sung.

**Tiêu chí & Bước**:
1. **Tính mạch lạc (1-5)**:
   - Đọc bài báo tin tức và bản tóm tắt.
   - So sánh bản tóm tắt với bài báo về độ rõ ràng và trật tự logic.
   - Sử dụng điểm SUM BLEURT như những hiểu biết bổ sung cho tính mạch lạc.
   - Gán điểm tính mạch lạc dựa trên tổ chức và cấu trúc.

2. **Tính nhất quán (1-5)**:
   - Sử dụng NLI & BLEURT để có điểm.
   - Đọc bài báo và bản tóm tắt.
   - So sánh chi tiết sự thật.
   - Gán điểm tính nhất quán dựa trên sự phù hợp về mặt sự thật.

3. **Tính liên quan (1-5)**:
   - Sử dụng BLEURT & SUM BLEURT để có điểm phù hợp với đánh giá giống con người.
   - Đọc cả bài báo và bản tóm tắt.
   - Xác định điểm chính và phạm vi bao phủ trong bản tóm tắt.
   - Gán điểm tính liên quan dựa trên tầm quan trọng nội dung và sự vắng mặt của thông tin dư thừa.

4. **Tính lưu loát (1-5)**:
   - Đánh giá bản tóm tắt thủ công về ngữ pháp, dấu câu, và cấu trúc câu.
   - Gán điểm tính lưu loát dựa trên khả năng đọc được.

**Tóm tắt Đánh giá (1-5)**:
Xem xét điểm từ mỗi tiêu chí và tầm quan trọng của chúng.
- Rút ra điểm trung bình, đảm bảo điểm cuối cùng nằm trong khoảng 1-5.
- Cung cấp bình luận tổng thể về bản tóm tắt.
- Nổi bật điểm mạnh và các lĩnh vực cần cải thiện.

### B.2 Mẫu Lời nhắc Đánh giá Fusion-Eval cho TopicalChat

Bạn sẽ được cung cấp cuộc trò chuyện giữa hai cá nhân, tiếp theo là phản hồi tiềm năng cho lượt tiếp theo trong cuộc trò chuyện, bao gồm một sự kiện thú vị. Nhiệm vụ của bạn là đánh giá các phản hồi trên sáu chỉ số: Tính mạch lạc, Tính hấp dẫn, Tính tự nhiên, Tính có căn cứ, Tính dễ hiểu, và Chất lượng Tổng thể.

**Mô tả và Sử dụng Đánh giá viên Hỗ trợ**:

**1. LM PROB (Xác suất Mô hình Ngôn ngữ)**:
- **Chức năng**: LM PROB cung cấp điểm xác suất, từ 0 đến 1, cho biết khả năng một phản hồi cho trước sẽ được tạo ra bởi mô hình ngôn ngữ, cho cuộc trò chuyện và sự kiện trước đó.
- **Phạm vi điểm**: 0 (ít khả năng nhất) đến 1 (khả năng nhất).
- **Sử dụng**:
  - **Đánh giá Tính tự nhiên**: Điểm xác suất cao hơn gợi ý rằng phản hồi có nhiều khả năng xảy ra tự nhiên trong cuộc trò chuyện của con người, cho thấy tính tự nhiên lớn hơn.
  - **Đánh giá Tính dễ hiểu**: Tương tự, xác suất cao hơn cũng có thể ngụ ý rằng phản hồi dễ hiểu hơn trong ngữ cảnh cho trước, vì nó phù hợp hơn với các mẫu ngôn ngữ được mong đợi.

**2. BLEURT**:
- **Chức năng**: BLEURT đánh giá chất lượng sinh văn bản bằng cách so sánh văn bản được tạo ra (phản hồi) với tham chiếu (cuộc trò chuyện và sự kiện). Phạm vi điểm của nó là 0 đến 1, nơi điểm cao hơn cho thấy sự phù hợp và chất lượng tốt hơn.
- **Phạm vi điểm**: 0 (phù hợp kém) đến 1 (phù hợp xuất sắc).
- **Sử dụng**:
  - **Đánh giá Tính có căn cứ**: Điểm BLEURT cao cho thấy rằng phản hồi sử dụng chính xác và liên quan đến sự kiện cho trước, thể hiện tính có căn cứ mạnh mẽ trong bối cảnh cuộc trò chuyện.

**Kế hoạch Sử dụng Công cụ cho Đánh giá Phản hồi Cuộc trò chuyện**:
1. **Đọc Cuộc trò chuyện, Sự kiện, và Phản hồi**: Bắt đầu với việc đọc cẩn thận các tài liệu được cung cấp để hình thành ấn tượng định tính ban đầu về phản hồi trong bối cảnh cuộc trò chuyện và sự kiện.
2. **Sử dụng LM PROB cho Đánh giá Tính tự nhiên và Tính dễ hiểu**:
   - Áp dụng LM PROB để xác định xác suất phản hồi sẽ được tạo ra bởi mô hình ngôn ngữ trong ngữ cảnh cho trước.
   - Điểm xác suất cao từ LM PROB sẽ cho thấy tính tự nhiên và tính dễ hiểu lớn hơn, vì phản hồi phù hợp tốt với các mẫu ngôn ngữ được mong đợi.
3. **Sử dụng BLEURT cho Đánh giá Tính có căn cứ**:
   - Sử dụng BLEURT để đánh giá mức độ chính xác và liên quan của phản hồi sử dụng sự kiện cho trước trong bối cảnh cuộc trò chuyện.
   - Điểm cao từ BLEURT gợi ý rằng phản hồi có căn cứ tốt trong sự kiện được cung cấp, thể hiện tính chính xác và liên quan.
4. **Đánh giá Cuối cùng và Tích hợp Đầu ra Công cụ**:
   - Tích hợp đầu ra từ các công cụ với đánh giá định tính ban đầu của bạn.
   - Đầu ra của các công cụ sẽ cung cấp hỗ trợ định lượng và xác nhận cho các đánh giá của bạn trong mỗi chỉ số.
   - Đưa ra quyết định cuối cùng dựa trên cái nhìn toàn diện, xem xét cả đầu ra công cụ và tiêu chí đánh giá ban đầu cho mỗi chỉ số.
   - Nhớ rằng phán quyết cuối cùng nên phù hợp với tiêu chí và bước đánh giá được định trước, với các công cụ phục vụ như hỗ trợ quan trọng nhưng bổ sung trong quá trình ra quyết định.

**Tiêu chí & Bước**:
1. **Tính mạch lạc (1-3, Bất kỳ Giá trị Thực nào)**:
   - Đọc cuộc trò chuyện, sự kiện, và phản hồi để đánh giá dòng chảy logic và tính liên tục.
   - Đánh giá mức độ phản hồi kết nối và tiếp tục cuộc trò chuyện.
   - Gán điểm Tính mạch lạc, từ 1 đến 3, dựa trên tổ chức của phản hồi và tích hợp logic vào cuộc trò chuyện.

2. **Tính hấp dẫn (1-3, Bất kỳ Giá trị Thực nào)**:
   - Xem xét cuộc trò chuyện, sự kiện, và phản hồi để xác định mức độ quan tâm hoặc hấp dẫn.
   - Đánh giá cách phản hồi đóng góp vào giá trị của cuộc trò chuyện và thu hút sự quan tâm.
   - Gán điểm Tính hấp dẫn, từ 1 đến 3, dựa trên khả năng của phản hồi thu hút và tăng giá trị cho cuộc trò chuyện.

3. **Tính tự nhiên (1-3, Bất kỳ Giá trị Thực nào)**:
   - Đọc cuộc trò chuyện, sự kiện, và phản hồi để đánh giá sự phù hợp tự nhiên của phản hồi trong bối cảnh cuộc trò chuyện.
   - Đánh giá giọng điệu, tính trang trọng, và dòng chảy cuộc trò chuyện để xác định mức độ tự nhiên của phản hồi.
   - Sử dụng LM PROB để bổ sung đánh giá, xem xét khả năng của phản hồi như vậy trong ngữ cảnh cho trước.
   - Gán điểm Tính tự nhiên, từ 1 đến 3, tập trung vào mức độ tự nhiên của phản hồi phù hợp với cuộc trò chuyện.

4. **Tính có căn cứ (0-1, Bất kỳ Giá trị Thực nào)**:
   - Kiểm tra cuộc trò chuyện, sự kiện, và phản hồi để đánh giá mức độ phản hồi sử dụng sự kiện cho trước.
   - Đánh giá tính chính xác và liên quan của sự kiện trong phản hồi.
   - Sử dụng BLEURT để cung cấp những hiểu biết bổ sung về mức độ chính xác của phản hồi có căn cứ trong sự kiện cho trước.
   - Gán điểm Tính có căn cứ, từ 0 đến 1, dựa trên việc kết hợp hiệu quả và chính xác sự kiện trong phản hồi.

5. **Tính dễ hiểu (0-1, Bất kỳ Giá trị Thực nào)**:
   - Xem xét cuộc trò chuyện, sự kiện, và phản hồi để đánh giá sự rõ ràng và khả năng hiểu của phản hồi.
   - Tập trung vào mức độ rõ ràng và dễ hiểu của phản hồi trong bối cảnh cuộc trò chuyện trước đó.
   - Áp dụng LM PROB cho dữ liệu bổ sung về tính dễ hiểu của phản hồi.
   - Gán điểm Tính dễ hiểu, từ 0 đến 1, dựa trên sự rõ ràng và dễ hiểu của phản hồi trong ngữ cảnh.

6. **Chất lượng Tổng thể (1-5, Bất kỳ Giá trị Thực nào)**:
   - Xem xét điểm và hiểu biết từ các tiêu chí trước đó, bao gồm dữ liệu từ các đánh giá viên hỗ trợ.
   - Xem xét cách các khía cạnh Tính mạch lạc, Tính hấp dẫn, Tính tự nhiên, Tính có căn cứ, và Tính dễ hiểu cùng nhau đóng góp vào ấn tượng tổng thể về phản hồi.
   - Gán điểm Chất lượng Tổng thể, từ 1 đến 5, dựa trên đánh giá toàn diện về điểm mạnh và điểm yếu của phản hồi.
   - Cung cấp giải thích tóm tắt cho xếp hạng chất lượng tổng thể, nổi bật các yếu tố chính và hiểu biết ảnh hưởng đến phán quyết.
