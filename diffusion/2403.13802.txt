# 2403.13802.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2403.13802.pdf
# File size: 10914351 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ZigMa: A DiT-style Zigzag Mamba Diffusion
Model
Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui,
Olga Grebenkova, Pingchuan Ma, Johannes Schusterbauer, and Björn Ommer
CompVis @ LMU Munich, MCML
https://compvis.github.io/zigma/
Abstract The diffusion model has long been plagued by scalability and
quadratic complexity issues, especially within transformer-based struc-
tures. In this study, we aim to leverage the long sequence modeling ca-
pability of a State-Space Model called Mamba to extend its applicability
to visual data generation. Firstly, we identify a critical oversight in most
current Mamba-based vision methods, namely the lack of consideration
for spatial continuity in the scan scheme of Mamba. Secondly, build-
ing upon this insight, we introduce Zigzag Mamba, a simple, plug-and-
play, minimal-parameter burden, DiT style solution, which outperforms
Mamba-based baselines and demonstrates improved speed and memory
utilization compared to transformer-based baselines, also this heteroge-
neous layerwise scan enables zero memory and speed burden when we
consider more scan paths. Lastly, we integrate Zigzag Mamba with the
Stochastic Interpolant framework to investigate the scalability of the
model on large-resolution visual datasets, such as FacesHQ 1024×1024
and UCF101, MultiModal-CelebA-HQ, and MS COCO 256×256.
Keywords: Diffusion Model ·State-Space Model ·Stochastic Inter-
polants
1 Introduction
Diffusion models have demonstrated significant advancements across various ap-
plications, including image processing [45,48,84], video analysis [44], point cloud
processing [109], representation learning [30] and human pose estimation [32].
Many of these models are built upon Latent Diffusion Models (LDM) [84], which
are typically based on the UNet backbone. However, scalability remains a signifi-
cant challenge in LDMs [50]. Recently, transformer-based structures have gained
popularity due to their scalability [9,80] and effectiveness in multi-modal train-
ing [10]. Notably, the transformer-based structure DiT [80] has even contributed
to enhancing the high-fidelity video generation model SORA [78] by OpenAI.
Despite efforts to alleviate the quadratic complexity of the attention mechanism
through techniques such as windowing [71], sliding [13], sparsification [19,56],arXiv:2403.13802v3  [cs.CV]  24 Nov 2024

--- PAGE 2 ---
2 Hu et al.
hashing [20,93], Ring Attention [15,66], Flash Attention [23] or a combination
of them [8,124], it remains a bottleneck for diffusion models.
On the other hand, State-Space Models [34,35,39] have demonstrated signif-
icant potential for long sequence modeling, rivaling transformer-based methods.
Their biological similarity [95] and efficient memory state also advocate for the
useoftheState-Spacemodeloverthetransformer.Severalmethods[29,33,35,88]
have been proposed to enhance the robustness [116], scalability [33], and effi-
ciency [35,36] of State-Space Models. Among these, a method called Mamba [33]
aims to alleviate these issues through work-efficient parallel scanning and other
data-dependent innovations. However, the advantage of Mamba lies in 1D se-
quence modeling, and extending it to 2D images is a challenging question. Pre-
vious works [70,123] have proposed flattening 2D tokens directly by computer
hierarchy such as row-and-column-major order, but this approach neglects Spa-
tial Continuity , as shown in Figure 1. Other works [67,73] consider various di-
rections in a single Mamba block, but this introduces additional parameters and
GPU memory burden. In this paper, we aim to emphasize the importance of
Spatial Continuity in Mamba and propose several intuitive and simple meth-
ods to enable the application of Mamba blocks to 2D images by incorporating
continuity-based inductive biases in images. We also generalize these methods
to 3D with spatial-temporal factorization on 3D sequence.
In the end, Stochastic Interpolant [3] provides a more generalized framework
that can uniform various generative models including, Normalizing Flow [17],
diffusionmodel[43,89,91],Flowmatching[4,64,69],andSchrödingerBridge[65].
Previously,someworks[74]exploretheStochasticInterpolantonrelativelysmall
resolutions, e.g., 256×256,512×512. In this work, we aim to explore it in further
more complex scenarios e.g., 1024×1024resolution and even in videos.
In summary, our contributions are as follows: Firstly, we identify the critical
issue of Spatial Continuity in generalizing the Mamba block from 1D sequence
modeling to 2D image and 3D video modeling. Building on this insight, we
propose a simple, plug-and-play, zero-parameter heterogeneous layerwise scan
paradigm named Zigzag Mamba (ZigMa) that leverages spatial continuity to
maximally incorporate the inductive bias from visual data. Secondly, we ex-
tend the methodology from 2D to 3D by factorizing the spatial and temporal
sequences to optimize performance. Secondly, we provide comprehensive analy-
sis surrounding the Mamba block within the regime of diffusion models. Lastly,
we demonstrate that our designed Zigzag Mamba outperforms related Mamba-
based baselines, representing the first exploration of Stochastic Interpolants on
large-scale image data ( 1024×1024) and videos.
2 Related Works
Mamba. Several works [102,103,103] have demonstrated that the State-Space
Modelpossessesuniversalapproximationabilityundercertainconditions.Mamba,
as a new State-Space Model, has superior potential for modeling long sequences
efficiently, which has been explored in various fields such as medical imag-

--- PAGE 3 ---
ZigMa 3
Single Direction Zigzag MambaSweep without considering Spatial Continuity20k iterations training30k iterations training
Multi Direction Zigzag MambaContinuity is broken
Figure1: Motivation. Our Zigzag Mamba method improves the network’s position-
awareness by arranging and rearranging the scan path of Mamba in a heuristic manner.
ing [73,86,108,111], video [58,79], image restoration [38,122], graphs [12], NLP
word byte [100], tabular data [2], point clouds [61], human motion [106,120],
multi-task [62] and image generation [27]. Among them, the most related to
us are VisionMamba [70,123], S4ND [77] and Mamba-ND [59]. VisionMamba
[70,123] uses a bidirectional SSM in discriminative tasks which incurs a high
computational cost. Our method applies a simple alternative mamba diffusion
in generative models. S4ND [77] introduces local convolution into Mamba’s rea-
soning process, moving beyond the use of only 1D data. Mamba-ND [59] takes
multi-dimensionality into account in discriminative tasks, making use of various
scans within a single block. In contrast, our focus is on distributing scan com-
plexity across every layer of the network, thus maximizing the incorporation of
inductive bias from visual data with zero parameter burden. Scan curve is an
important direction in SSM, PointMamba [61] is a representative work that em-
ploys SSM with space curves (e.g., Hilbert) for point cloud analysis, achieving
remarkable performance. In contrast with them, our preliminary results show
that the Hilbert curve doesn’t work well with our method (see Appendix), while
our method can be regarded as the simplest Peano curve. For more information
related to Mamba’s work, please refer to the survey [105].
Backbones in Diffusion Models. Diffusion models primarily employ UNet-
based [43,84] and ViT-based [9,80] backbones. While UNet is known for high
memory demands [84], ViT benefits from scalability [18,24] and multi-modal
learning [10]. However, ViT’s quadratic complexity limits visual token process-
ing, prompting studies towards mitigating this issue [13,23,104]. Our work, in-
spired by Mamba [33], explores an SSM-based model as a generic diffusion back-
bone, retaining ViT’s modality-agnostic and sequential modeling advantages.

--- PAGE 4 ---
4 Hu et al.
Concurrently, DiffSSM [112] concentrates on unconditional and class condition-
ing within the S4 model [35]. DIS [27] mainly explores the state-space model on
a relatively small resolution, which is not the exact focus of our work. Our work
significantly differs from theirs as it primarily focuses on the backbone design
using the Mamba block and extends it to text conditioning. Furthermore, we
apply our method to more complex visual data.
SDE and ODE in Diffusion models. The realm of Score-based Generative
Models encompasses significant contributions from foundational works such as
ScoreMatchingwithLangevinDynamics(SMLD)bySongetal.[90],andthead-
vent of Diffusion Models with Denoising Score Matching (DDPMs) proposed by
Ho et al. [43]. These methodologies operate within the framework of Stochastic
Differential Equations (SDEs), a concept further refined in the research of Song
et al. [91]. Recent research strides, as exemplified by Karras et al. [52] and Lee et
al. [57], have showcased the efficacy of employing Ordinary Differential Equation
(ODE) samplers for diffusion SDEs, offering significant reductions in sampling
costs compared to traditional approaches that entail discretizing diffusion SDEs.
Furthermore, within the domain of Flow Matching [64] and Rectified Flow [68],
both SMLD and DDPMs emerge as specialized instances under distinct paths
of the Probability Flow ODE framework [91], with broad applications in vi-
sion [22,28,49], depth [37], human motion [47], even language [46]. These models
typicallyutilizevelocityfieldparameterizationsemployingthelinearinterpolant,
a concept that finds broader applications in the Stochastic Interpolant frame-
work [3], with subsequent generalizations extending to manifold settings [14].
The SiT model [74] scrutinizes the interplay between interpolation methods in
both sampling and training contexts, albeit in the context of smaller resolutions
such as 512×512. Our research endeavors to extend these insights to a larger
scale, focusing on the generalization capabilities for 2D images of 1024×1024
and 3D video data.
3 Method
In this section, we begin by providing background information on State-Space
Models[34,35,39],withaparticularfocusonaspecialcaseknownasMamba[33].
We then highlight the critical issue of Spatial Continuity within the Mamba
framework, and based on this insight, we propose the Zigzag Mamba. This en-
hancement aims to improve the efficiency of 2D data modeling by incorporating
the continuity inductive bias inherent in 2D data. Furthermore, we design a ba-
sic cross-attention block upon Mamba block to achieve text-conditioning. Subse-
quently, we suggest extending this approach to 3D video data by factorizing the
model into spatial and temporal dimensions, thereby facilitating the modeling
process. Finally, we introduce the theoretical aspects of stochastic interpolation
for training and sampling, which underpin our network architecture.

--- PAGE 5 ---
ZigMa 5
3.1 Background: State-Space Models
State Space Models (SSMs) [34,35,39] have been proven to handle long-range
dependencies theoretically and empirically [36] with linear scaling w.r.t sequence
length.Intheirgeneralform,alinearstatespacemodelcanbewrittenasfollows:
x′(t) =A(t)x(t) +B(t)u(t)
y(t) =C(t)x(t) +D(t)u(t),
mapping a 1-D input sequence u(t)∈Rto a 1-D output sequence y(t)∈R
through an implicit N-D latent state sequence x(t)∈Rn. Concretely, deep SSMs
seek to use stacks of this simple model in a neural sequence modeling architec-
ture, where the parameters A,B,CandDfor each layer can be learned via
gradient descent.
Embedded Patches
NormForward
Conv1dForward
Scan
Activation+ +
Flatten & Linear Projection0 1 2 3 4 5 6 7 8Vision Mamba Encoder
9*MLP  & Prediction
Input
Image
Mamba Scan
Figure2: ZigMa. Our backbone is structured in L layers, mirroring the style of
DiT [80]. We use the single-scan Mamba block as the primary reasoning module
across different patches. To ensure the network is positionally aware, we’ve designed
an arrange-rearrange scheme based on the single-scan Mamba. Different layers follow
pairsofuniquerearrangeoperation Ωandreverserearrange ¯Ω,optimizingtheposition-
awareness of the method.
Recently, Mamba [33] largely improved the flexibility of SSMs in Language
Modelling by relaxing the time-invariance constraint on SSM parameters, while
maintaining computational efficiency. Several studies [70,123] have been con-
ducted to adapt the use of Mamba from unidimensional language data to mul-
tidimensional visual data. While most of these studies try to duplicate the Ato
facilitate the new (reversed) direction, this approach can lead to additional pa-
rameters and an increased memory burden. In this paper, we focus on exploring
the scanning scheme of Mamba in diffusion models to efficiently maximize the
use of inductive-bias from multi-dimensional visual data with zero parameter
and memory burden.
3.2 Diffusion Backbone: Zigzag Mamba
DiT-StyleNetwork. WeopttousetheframeworkofDiTbyAdaLN[80]rather
than the skip-layer focused U-ViT structure [9], as DiT has been validated as a

--- PAGE 6 ---
6 Hu et al.
scalable structure in literature [10,18,78]. Additionally, the Hourglass structure
with downsampling [76,85] requires selecting the depth and width based on the
complexity of the dataset and task. This requirement limits the flexibility of the
solution. Considering the aforementioned points, it informs our Mamba network
design depicted in Figure 4. The core component of this design is the Zigzag
Scanning, which will be explained in the following paragraph.
Zigzag Scanning in Mamba. Previous studies [101,112] have used bidirec-
tional scanning within the SSM framework. This approach has been expanded
to include additional scanning directions [67,70,115] to account for the charac-
teristics of 2D image data. These approaches unfold image patches along four
directions, resulting in four distinct sequences. Each of these sequences is sub-
sequently processed together through every SSM. However, since each direction
may have different SSM parameters ( A,B,C, andD), scaling up the number of
directions could potentially lead to memory issues. In this work, we investigate
the potential for amortizing the complexity of the Mamba into each layer of the
network.
Ourapproachcentersaroundtheconceptoftokenrearrangementbeforefeed-
ing them into the Forward Scan block. For a given input feature zifrom layer i,
the output feature zi+1of the Forward Scan block after the rearrangement can
be expressed as:
zΩi=arrange (zi, Ωi), (1)
¯zΩi=scan(zΩi), (2)
zi+1=arrange (¯zΩi,¯Ωi), (3)
Ωirepresents the 1D permutation of layer i, which rearranges the order of the
patch tokens by Ωi, and ΩiandΩirepresent the reverse operation. This ensures
that both ziandzi+1maintain the sample order of the original image tokens.
(a)sweep-scan
 (b)zigzag-scan
(c)zigzag-scan with 8 schemes
Figure3: The 2D Image Scan. Our mamba scan design is based on the sweep-scan
scheme shown in subfigure (a). From this, we developed a zigzag-scan scheme displayed
in subfigure (b) to enhance the continuity of the patches, thereby maximizing the
potential of the Mamba block. Since there are several possible arrangements for these
continuous scans, we have listed the eight most common zigzag-scans in subfigure (c).
Now we explore the design of the Ωioperation, considering additional in-
ductive biases from 2D images. We propose one key properties: Spatial Con-

--- PAGE 7 ---
ZigMa 7
tinuity. Regarding Spatial Continuity, current innovations of Mamba in im-
ages [67,70,123] often squeeze 2D patch tokens directly following the computer
hierarchy, such as row-and-column-major order. However, this approach may
not be optimal for incorporating the inductive bias with neighboring tokens, as
illustrated in Figure 3. To address this, we introduce a novel scanning scheme
designed to maintain spatial continuity during the scan process. Additionally,
we consider space-filling, which entails that for a patch of size N×N, the length
of the 1D continuous scanning scheme should be N2. This helps to efficiently
incorporate tokens to maximize the potential of long sequence modeling within
the Mamba block.
Heterogeneous Layerwise Scan. To achieve the aforementioned property, we
heuristically design eight possible space-filling continuous schemes1, denoted as
Sj(where j∈[0,7]), as illustrated in Figure 3. While there may be other con-
ceivable schemes, for simplicity, we limit our usage to these eight. Consequently,
the scheme for each layer can be represented as Ωi=S{i%8}, where %denotes
the modulo operator.
Input Tokens timestepLayer NormScale, ShiftMamba ScanLayer NormScale, ShiftCross Attention
+
ScaleScale
MLP+
Noised
Latent
 Patchify
Timestep,
Prompt  EmbedMamba BlockLayer NormLinear and ReshapeNoise 
Optional
promptMLPprompt
Figure4: The Detail of our Zigzag Mamba block. The detail of Mamba Scan
is shown in Figure 2. The condition can include a timestep and a text prompt. These
are fed into an MLP, which separately modulates the Mamba scan for long sequence
modeling and cross-attention for multi-modal reasoning.
Deploying text-condition on Zigzag Mamba. While Mamba offers the ad-
vantage of efficient long sequence modeling, it does so at the expense of the
attention mechanism. As a result, there has been limited exploration into in-
corporating text-conditioning in Mamba-based diffusion models. To address this
1We also experimented with more complex continuous space-filling paths, such as
the Hilbert space-filling curve [75]. However, empirical findings indicate that this
approach may lead to deteriorated results. For further detailed comparisons, please
refer to the Appendix.

--- PAGE 8 ---
8 Hu et al.
gap, we propose a straightforward cross-attention block with skip layers built
upon the Mamba block, as illustrated in Figure 4. This design not only en-
ables long sequence modeling but also facilitates multi-token conditioning, such
as text-conditioning. Furthermore, it has the potential to provide interpretabil-
ity [16,42,94], as cross-attention has been utilized in diffusion models.
Generalizingto3Dvideosbyfactorizingspatialandtemporalinforma-
tion.In previous sections, our focus has been on the spatial 2D Mamba, where
we designed several spatially continuous, space-filling 2D scanning schemes. In
this section, we aim to leverage this experience to aid in designing correspond-
ing mechanisms for 3D video processing. We commence our design process by
extrapolating from the conventional directional Mamba, as depicted in Figure 5.
Given a video feature input z∈RB×T×C×W×H, we propose three variants of
the Video Mamba Block for facilitating 3D video generation.
(a) Sweep-scan: In this approach, we directly flatten the 3D feature zwithout
considering spatial or temporal continuity. It’s worth noting that the flattening
process follows the computer hierarchy order, meaning that no continuity is
preserved in the flattened representation.
(b) 3D Zigzag: Compared with the formulation of the 2D zigzag in previous
subsections, we follow the similar design to generalize it to 3D Zigzag to keep the
continuity in 2D and 3D simultaneously. Potentially, the scheme has much more
complexity. We heuristically list 8 schemes as well. However, we empirically find
that this scheme will lead to suboptimal optimization.
(c) Factorized 3D Zigzag = 2D Zigzag + 1D Sweep: To address the sub-
optimal optimization issue, we propose to factorize the spatial and temporal
correlations as separate Mamba blocks. The order of their application can be
adjusted as desired, for example, "sstt" or "ststst", where "s" represents the
spatial-zigzag Mamba and "t" represents the temporal-zigzag Mamba. For a 1D
temporal sweep, we simply opt for forward and backward scanning, since there
is only one dimension on the time axis.
Computation Analysis. For a visual sequence T∈R1×M×D, the computation
complexityofglobalself-attentionand k-directionmambaandourzigzagmamba
are as follows:
ζ(self-attention ) = 4MD2+ 2M2D, (4)
ζ(k-mamba ) =k×[3M(2D)N+M(2D)N2], (5)
ζ(zigzag ) = 3M(2D)N+M(2D)N2, (6)
whereself-attentionexhibitsquadraticcomplexitywithrespecttosequencelength
M, while Mamba exhibits linear complexity (N is a fixed parameter, set to 16
by default). Here, krepresents the number of scan directions in a single Mamba
block. Therefore, k-mamba and zigzag share linear complexity with respect to
self-attention. Moreover, our zigzag method can eliminate the kseries, further
reducing the overall complexity.

--- PAGE 9 ---
ZigMa 9
Bi-directional  Mamba 3D Zigzag  Mamba Spatial 2D Scan Temporal 1D Scan
( a ). sweep-scan ( b ). 3D zigzag-scan ( c ). 2D zigzag scan + 1D normal scan   
   
Figure5: The 3D Video Scan. (a) We illustrate the bidirectional Mamba with the
sweep scan, where the spatial and temporal information is treated as a set of tokens
with a computer-hierarchy order. (b)For the 3D zigzag-scan, we aim to maximize the
potential of Mamba by employing a spatial continuous scan scheme and adopting the
optimal zigzag scan solution, as depicted in Figure 3. (c)We further separate the rea-
soning between spatial and temporal information, resulting in a factorized combination
of 2D spatial scan ( Ω) plus a 1D temporal scan ( Ω′) scheme.
Upon completing the design of the Zigzag Mamba network for improved
visual inductive-bias integration, we proceed to combine it with a new diffusion
framework, as illustrated below.
3.3 Diffusion Framework: Stochastic Interpolant
Sampling based on vector vand score s.Following [3, 96], the time-
dependentprobabilitydistribution pt(x)ofxtalsocoincideswiththedistribution
of the reverse-time SDE [6]:
dXt=v(Xt, t)dt+1
2wts(Xt, t)dt+√wtd¯Wt, (7)
where ¯Wtisareverse-timeWienerprocess, wt>0isanarbitrarytime-dependent
diffusion coefficient, s(x, t) =∇logpt(x)is the score, and v(x, t)is given by the
conditional expectation
v(x, t) =E[˙xt|xt=x],
= ˙αtE[x∗|xt=x] + ˙σtE[ε|xt=x],(8)
where αtis a decreasing function of t, and σtis an increasing function of t. Here,
˙αtand˙σtdenote the time derivatives of αtandσt, respectively.
As long as we can estimate the velocity v(x, t)and/or score s(x, t)fields,
we can utilize it for the sampling process either by probability flow ODE [91]
or the reverse-time SDE (7). Solving the reverse SDE (7) backwards in time
fromXT=ε∼ N(0,I)enables generating samples from the approximated data
distribution p0(x)∼p(x). During sampling, we can perform direct sampling

--- PAGE 10 ---
10 Hu et al.
from either ODE or SDEs to balance between sampling speed and fidelity. If
we choose to conduct ODE sampling, we can achieve this simply by setting the
noise term sto zero.
In [3], it shows that one of the two quantities sθ(x, t)andvθ(x, t)needs to
be estimated in practice. This follows directly from the constraint
x=E[xt|xt=x],
=αtE[x∗|xt=x] +σtE[ε|xt=x],(9)
which can be used to re-express the score s(x, t)in terms of the velocity v(x, t)
as
s(x, t) =σ−1
tαtv(x, t)−˙αtx
˙αtσt−αt˙σt. (10)
Thus, v(x, t)ands(x, t)can be mutually conversed. We illustrate how to com-
pute them in the following.
Estimating the score sand the velocity v.It has been shown in score-based
diffusion models [91] that the score can be estimated parametrically as sθ(x, t)
using the loss
Ls(θ) =ZT
0E[∥σtsθ(xt, t) +ε∥2]dt. (11)
Similarly, the velocity v(x, t)can be estimated parametrically as vθ(x, t)via the
loss
Lv(θ) =ZT
0E[∥vθ(xt, t)−˙αtx∗−˙σtε∥2]dt, (12)
where θrepresents the Zigzag Mamba network that we described in the previous
section, we adopt the linear path for training, due to its simplicity and relatively
straight trajectory:
αt= 1−t, σ t=t. (13)
We note that any time-dependent weight can be included under the integrals
in both (11) and (12). These weight factors play a crucial role in score-based
models when Tbecomes large [54,55]. Thus, they provide a general form that
considers both the time-dependent weight and the stochasticity.
4 Experiment
4.1 Dataset and Training Detail
Image Dataset. To explore the scalability in high resolution, we conduct exper-
iments on the FacesHQ 1024×1024. The general dataset that we use for training
and ablations is FacesHQ, a compilation of CelebA-HQ [110] and FFHQ [53], as
employed in previous work such as [26,28].

--- PAGE 11 ---
ZigMa 11
Table 1: Ablation of Scanning Scheme Number . We evaluate various zigzag
scanning schemes. Starting from a simple “Sweep” baseline, we consistently observe
improvements as more schemes are implemented.
MultiModal-CelebA-256 MultiModal-CelebA-512
FID5k↓ FDD5k↓ KID5k↓ FID5k↓ FDD5k↓ KID5k↓
Sweep 158.1 75.9 0.169 162.3 103.2 0.203
Zigzag-1 65.7 47.8 0.051 121.0 78.0 0.113
Zigzag-2 54.7 45.5 0.041 96.0 59.5 0.079
Zigzag-8 45.5 26.4 0.011 34.9 29.5 0.023
Video Dataset. UCF101 dataset consists of 13,320 video clips, which are clas-
sified into 101 categories. The total length of these video clips is over 27 hours.
All these videos are collected from YouTube and have a fixed frame rate of 25
FPS with the resolution of 320×240. We randomly sample continuous 16 frames
and resize the frames to 256×256.
Training Details. We uniformly use AdamW [72] optimizer with 1e−4learning
rate. For extracting latent features, we employ off-the-shelf VAE encoders. To
mitigate computational costs, we adopted a mixed-precision training approach.
Additionally, we applied gradient clipping with a threshold of 2.0 and a weight
decay of 0.01 to prevent NaN occurrences during Mamba training. Most of our
experiments were conducted on 4 A100 GPUs, with scalability exploration ex-
tended to 16 and 32 A100 GPUs. For sampling, we adopt the ODE sampling for
speed consideration. For further details, please refer to the Appendix 8.8.
4.2 Ablation Study
Table 2: Ablation about Position Embedding (PE) on unconditional CelebA
dataset ( 2562). To better abate PE and eliminate the conditional signal’s influence, we
use an unconditional dataset.
FID/FDD ↓No PE Cosine PE Learnable PE
VisionMamba [123] 21.33/21.00 18.47/19.90 16.38/18.20
ZigMa 14.27/18.00 14.04/17.91 13.32/17.40
Scan Scheme Ablation. We provide several important findings based on our
ablation studies on MultiModal-CelebA dataset in various resolutions in Table 1.
Firstly, switching the scanning scheme from sweep to zigzag led to some gains.
Secondly, as we increased the zigzag scheme from 1 to 8, we saw consistent
gains. This indicates that alternating the scanning scheme in various blocks can
be beneficial. Finally, the relative gain between Zigzag-1 and Zigzag-8 is more
prominent at higher resolutions ( 512×512, or longer sequence token number)

--- PAGE 12 ---
12 Hu et al.
Out Of Memory
(a)FPS v.s.Patch Number.
Out Of Memory
 (b)GPU Memory v.s.Patch Number.
1 2 4 8 16 320102030GPU Memory (G)Zigzag-8 Mamba (Our)
Parallel Mamba
(c)Order Receptive Field v.s.GPU Memory.
1 2 4 8 16 32010203040FPSZigzag-8 Mamba (Our)
Parallel Mamba (d)Order Receptive Field v.s.FPS.
Figure6: (a, b).GPU Memory usage and FPS between our method and transformer-
based methods(U-VIT [9] and DiT [80]). (c). Order Receptive Field and GPU memory
(d). Order Receptive Field and FPS. Order Receptive Field denotes how many scan
paths we consider in our network design.
compared to lower resolutions ( 256×256, or shorter sequence token number),
this shows the great potential and more efficient inductive-bias incorporation in
longer sequence number.
AblationaboutPositionEmbedding. AsshowninTable2,thelearnableem-
bedding performs better than the Sinusoidal embedding, which in turn performs
betterthannopositionembedding.Invariouscases,ourzigzagmethodsurpasses
the baselines. Notably, our performance remains almost unchanged whether we
use the Sinusoidal position embedding or no position embedding. This suggests
that our method can better incorporate spatial inductive-bias compared to our
baseline. Finally, using the learnable position embedding provides further, albeit
marginal, gains suggesting that better position embedding exists even within
our zigzag scan scheme. We find that [79] shares the same conclusion as us in
video-related tasks.
Ablation study about the Network and FPS/GPU-Memory. In Figure 6
(a,b),we analyze the forward speed and GPU memory usage while varying the
global patch dimensions from 32×32to196×196. For the speed analysis,
we report Frame Per Second (FPS) instead of FLOPS, as FPS provides a more
explicit and appropriate evaluation of speed2. For simplicity, we uniformly apply
the zigzag-1 Mamba scan scheme and use batch size=1 and patch size=1 on
an A100 GPU with 80GB memory. It’s worth noting that all methods share
nearly identical parameter numbers for fair comparison. We primarily compare
our method with two popular transformer-based Diffusion backbones, U-ViT [9]
and DiT [80]. It is evident that our method achieves the best FPS and GPU
2https://github.com/state-spaces/mamba/issues/110#issuecomment-1916464012

--- PAGE 13 ---
ZigMa 13
utilization when gradually increasing the patching number. U-ViT demonstrates
theworstperformance,evenexceedsthememoryboundswhenthepatchnumber
is196.Surprisingly,DiT’sGPUutilizationisclosetoourmethod,whichsupports
our backbone choice of DiT from a practical perspective.
Table 3: Main result on FacesHQ-
1024 dataset with 4,094 tokens in la-
tent space and bs=512. Our method
can outperform the baseline and can
achieve even better results when the
training scale is increased.
Method FID5k↓FDD5k↓
VisionMamba [123] 51.1 66.3
ZigMa 37.8 50.5
ZigMa bs ×226.6 31.2Table 4: Main Results on MS-
COCO dataset with bs=256. Our
method consistently outperforms the
baseline. ZigMa with 8 scans performs
much better compared with the base-
line.
Method FID5k↓
Sweep 195.1
Zigzag-1 73.1
VisionMamba [123] 60.2
Zigzag-8 41.8
Table 5: Transformer-based meth-
ods comparison on unconditional
CelebA256.
MethodFID↓Memory(G) ↓FLOPS(G) ↓
U-ViT 14.50 35.10 12.5
DiT14.64 29.20 5.5
ZigMa 14.27 17.80 5.2Table 6: Video Scan Scheme on
UCF101 dataset with bs=32.
Method Frame-FID5k↓FVD5k↓
Bidirection [123] 256.1 320.2
3D Zigzag 238.1 282.3
Our 216.1 210.2
Bidirection [123] bs ×4146.2 201.1
ZigMa bs ×4 121.2 140.1
Order Receptive Field. We propose a new concept in Mamba-based structure
for multidimensional data. Given that various spatially-continuous zigzag paths
may exist in multidimensional data, we introduce the term Order Receptive Field
which denotes the number of zigzag paths explicitly employed in the network
design.
AblationstudyabouttheOrderReceptiveFieldandFPS/GPU-Memory.
As depicted in Fig. 6 (c,d), Zigzag Mamba consistently maintains its GPU mem-
oryconsumptionandFPSrate,evenwithagraduallyincreasingOrderReceptive
Field.In contrast,our primarybaseline,ParallelMamba,along withvariantslike
Bidirectional Mamba and Vision Mamba [70,123], experience a consistent de-
crease in FPS due to increased parameters. Notably, Zigzag Mamba, with an
Order Receptive Field of 8, can perform faster without altering parameters.
Comparison with transformer-based methods. We show the result in Ta-
ble5onunconditionalgenerationtask.Ourmethodachievesperformancecompa-
rabletoTransformer-basedmethods,withsignificantlylessmemoryconsumption
and fewer FLOPS.

--- PAGE 14 ---
14 Hu et al.
4.3 Main Result
Main Result on 1024 ×1024 FacesHQ. To elaborate on the scalability of our
method within the Mamba and Stochastic Interpolant framework, we provide
comparisons on a high-resolution dataset (1024 ×1024 FacesHQ) in Table 3. Our
primary comparison is against Bidirectional Mamba, a commonly used solution
for applying Mamba to 2D image data [70,123]. With the aim of investigating
Mamba’s scalability in large resolutions up to 1,024, we employ the diffusion
model on the latent space of 128×128with a patch size of 2, resulting in 4,096
tokens. The network is trained on 16 A100 GPUs. Notably, our method demon-
stratessuperiorresultscomparedtoBidirectionalMamba.Detailsregardingloss,
FID curves, and visualization can be found in the Appendix. While constrained
by GPU resource limitations, preventing longer training duration, we anticipate
consistent outperformance of Bidirectional Mamba with extended training du-
ration.
COCO dataset. To further compare the performance of our method, we also
evaluate it on the more complex and common dataset MS COCO. We compare
with the Bidirection Mamba as the baseline in Table 4. It should be noted that
all methods share nearly identical parameter numbers for fair comparison. We
trained all methods using 16 A100 GPUs. please check Appendix 8.8 for details.
As depicted in Table 4, our Zigzag-8 method outperforms Bidirectional Mamba
as well as Zigzag-1. This suggests that amortizing various scanning schemes
can yield significant improvements, attributed to better incorporation of the
inductive bias for 2D images in Mamba.
UCF101 dataset. In Table 6, we present our results on the UCF101 dataset,
training all methods using 4 A100 GPUs, with further scalability exploration
conducted using 16 A100 GPUs. We mainly compare our method consistantly
with Vision Mamba [123]. For the choice of the 3D Zigzag Mamba, please refer
to Appendix 8.8. For Factorized 3D Zigzag Mamba in video processing, we de-
ploy the sstscheme for factorizing spatial and temporal modeling. This scheme
prioritizes spatial information complexity over temporal information, hypothe-
sizing that redundancy exists in the temporal domain. Our results consistently
demonstrate the superior performance of our method across various scenarios,
underscoring the intricacy and effectiveness of our approach.
5 Conclusion
In this paper, we present the Zigzag Mamba Diffusion Model, developed within
the Stochastic Interpolant framework. Our initial focus is on addressing the
critical issue of spatial continuity. We then devise a Zigzag Mamba block with
heterogeneous layerwise scan to better utilize the inductive bias in 2D images.
Further, we factorize the 3D Mamba into 2D and 1D Zigzag Mamba to facilitate
optimization. We empirically design various ablation studies to examine different
factors. This approach allows for a more in-depth exploration of the Stochastic
Interpolant theory. We hope our endeavor can inspire further exploration in the
Mamba network design.

--- PAGE 15 ---
ZigMa 15
Acknowledgements
This project has been supported by the German Federal Ministry for Economic
Affairs and Climate Action within the project “NXT GEN AI METHODS –
Generative Methoden für Perzeption, Prädiktion und Planung”, the bidt project
KLIMA-MEMES, Bayer AG, and the German Research Foundation (DFG)
project 421703927. The authors gratefully acknowledge the Gauss Center for
Supercomputing for providing compute through the NIC on JUWELS at JSC
and the HPC resources supplied by the Erlangen National High Performance
Computing Center (NHR@FAU funded by DFG).
References
1. Agarwal, N., Suo, D., Chen, X., Hazan, E.: Spectral state space models. arXiv
(2023) 28
2. Ahamed, M.A., Cheng, Q.: Mambatab: A simple yet effective approach for han-
dling tabular data. arXiv (2024) 3, 28
3. Albergo, M.S., Boffi, N.M., Vanden-Eijnden, E.: Stochastic interpolants: A uni-
fying framework for flows and diffusions. arXiv (2023) 2, 4, 9, 10
4. Albergo, M.S., Vanden-Eijnden, E.: Building normalizing flows with stochastic
interpolants. arXiv (2022) 2
5. Ali, A., Zimerman, I., Wolf, L.: The hidden attention of mamba models. arXiv
(2024) 28
6. Anderson,B.D.:Reverse-timediffusionequationmodels.StochasticProcessesand
their Applications (1982) 9
7. Anthony, Q., Tokpanov, Y., Glorioso, P., Millidge, B.: Blackmamba: Mixture of
experts for state-space models. arXiv (2024) 28
8. Ao, S., Zhao, W., Han, X., Yang, C., Liu, Z., Shi, C., Sun, M., Wang, S., Su, T.:
Burstattention: An efficient distributed attention framework for extremely long
sequences. arXiv (2024) 2
9. Bao, F., Li, C., Cao, Y., Zhu, J.: All are worth words: a vit backbone for score-
based diffusion models. CVPR (2023) 1, 3, 5, 12, 23
10. Bao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., Zhu,
J.: One transformer fits all distributions in multi-modal diffusion at scale. arXiv
(2023) 1, 3, 6
11. Beck, M., Pöppel, K., Spanring, M., Auer, A., Prudnikova, O., Kopp, M., Klam-
bauer, G., Brandstetter, J., Hochreiter, S.: xlstm: Extended long short-term mem-
ory (2024) 22
12. Behrouz, A., Hashemi, F.: Graph mamba: Towards learning on graphs with state
space models. arXiv (2024) 3, 28
13. Beltagy,I.,Peters,M.E.,Cohan,A.:Longformer:Thelong-documenttransformer.
arXiv (2020) 1, 3
14. Ben-Hamu, H., Cohen, S., Bose, J., Amos, B., Grover, A., Nickel, M., Chen, R.T.,
Lipman, Y.: Matching normalizing flows and probability paths on manifolds. In:
ICML (2022) 4
15. Brandon,W.,Nrusimha,A.,Qian,K.,Ankner,Z.,Jin,T.,Song,Z.,Ragan-Kelley,
J.: Striped attention: Faster ring attention for causal transformers. arXiv preprint
arXiv:2311.09431 (2023) 2

--- PAGE 16 ---
16 Hu et al.
16. Chefer, H., Gur, S., Wolf, L.: Transformer interpretability beyond attention visu-
alization. In: CVPR (2021) 8
17. Chen, R.T., Rubanova, Y., Bettencourt, J., Duvenaud, D.K.: Neural ordinary
differential equations. NeurIPS (2018) 2
18. Chen, S., Xu, M., Ren, J., Cong, Y., He, S., Xie, Y., Sinha, A., Luo, P., Xiang,
T., Perez-Rua, J.M.: Gentron: Delving deep into diffusion transformers for image
and video generation. arXiv (2023) 3, 6
19. Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with
sparse transformers. arXiv (2019) 1
20. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.: Rethinking attention
with performers. arXiv (2020) 2
21. Crowson, K., Baumann, S.A., Birch, A., Abraham, T.M., Kaplan, D.Z., Shippole,
E.: Scalable high-resolution pixel-space image synthesis with hourglass diffusion
transformers. arXiv (2024) 29
22. Dao, Q., Phung, H., Nguyen, B., Tran, A.: Flow matching in latent space. arXiv
(2023) 4
23. Dao, T., Fu, D., Ermon, S., Rudra, A., Ré, C.: Flashattention: Fast and memory-
efficient exact attention with io-awareness. NeurIPS (2022) 2, 3
24. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J.,
Steiner, A.P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al.: Scaling vision
transformers to 22 billion parameters. In: ICML (2023) 3
25. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 23, 27
26. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution im-
age synthesis. In: CVPR (2021) 10
27. Fei, Z., Fan, M., Yu, C., Huang, J.: Scalable diffusion models with state space
backbone. arXiv (2024) 3, 4, 28
28. Schusterbauer, J.S., Gui, M., Ma, P., Stracke, N., Baumann, S.A., Ommer, B.:
Boosting latent diffusion with flow matching. ECCV (2024) 4, 10
29. Fu, D.Y., Dao, T., Saab, K.K., Thomas, A.W., Rudra, A., Ré, C.: Hungry hungry
hippos: Towards language modeling with state space models. arXiv (2022) 2
30. Fuest, M., Ma, P., Gui, M., Schusterbauer, J.S., Hu, V.T., Ommer, B.: Diffusion
models and representation learning: A survey. arXiv preprint arXiv:2407.00783
(2024) 1
31. Gong, H., Kang, L., Wang, Y., Wan, X., Li, H.: nnmamba: 3d biomedical image
segmentation,classificationandlandmarkdetectionwithstatespacemodel.arXiv
(2024) 28
32. Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu, J.: Diffpose: Toward more
reliable 3d pose estimation. In: CVPR (2023) 1
33. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. CoLM (2024) 2, 3, 4, 5
34. Gu, A., Goel, K., Gupta, A., Ré, C.: On the parameterization and initialization
of diagonal state space models. NeurIPS (2022) 2, 4, 5
35. Gu, A., Goel, K., Ré, C.: Efficiently modeling long sequences with structured
state spaces (2021) 2, 4, 5
36. Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Ré, C.: Combin-
ing recurrent, convolutional, and continuous-time models with linear state space
layers. NeurIPS (2021) 2, 5

--- PAGE 17 ---
ZigMa 17
37. Gui, M., Schusterbauer, J.S., Prestel, U., Ma, P., Kotovenko, D., Grebenkova, O.,
Baumann,S.A.,Hu,V.T.,Ommer,B.:Depthfm:Fastmonoculardepthestimation
with flow matching. arXiv preprint arXiv:2403.13788 (2024) 4
38. Guo, H., Li, J., Dai, T., Ouyang, Z., Ren, X., Xia, S.T.: Mambair: A simple
baseline for image restoration with state-space model. arXiv (2024) 3, 28
39. Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured
state spaces. NeurIPS (2022) 2, 4, 5
40. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., Wang, Y.: Densemamba:
State space models with dense hidden connection for efficient large language mod-
els. arXiv (2024) 28
41. He, X., Cao, K., Yan, K., Li, R., Xie, C., Zhang, J., Zhou, M.: Pan-mamba:
Effective pan-sharpening with state space model. arXiv (2024) 28
42. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.:
Prompt-to-prompt image editing with cross attention control. arXiv (2022) 8
43. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020) 2, 3, 4
44. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
diffusion models. In: ARXIV (2022) 1
45. Hu, V.T., Chen, Y., Caron, M., Asano, Y.M., Snoek, C.G., Ommer, B.: Guided
diffusion from self-supervised diffusion features. In: ARXIV (2023) 1
46. Hu, V.T., Wu, D., Asano, Y., Mettes, P., Fernando, B., Ommer, B., Snoek, C.:
Flowmatchingforconditionaltextgenerationinafewsamplingstepspp.380–392
(2024) 4
47. Hu, V.T., Yin, W., Ma, P., Chen, Y., Fernando, B., Asano, Y.M., Gavves, E.,
Mettes, P., Ommer, B., Snoek, C.G.: Motion flow matching for human motion
synthesis and editing. In: ARXIV (2023) 4
48. Hu, V.T., Zhang, D.W., Asano, Y.M., Burghouts, G.J., Snoek, C.G.M.: Self-
guided diffusion models. In: CVPR (2023) 1
49. Hu, V.T., Zhang, D.W., Mettes, P., Tang, M., Zhao, D., Snoek, C.G.: Latent
space editing in transformer-based flow matching. In: ICML 2023 Workshop, New
Frontiers in Learning, Control, and Dynamical Systems (2023) 4
50. Huang, Z., Zhou, P., Yan, S., Lin, L.: Scalelong: Towards more stable training of
diffusion model via scaling network long skip connection. NeurIPS (2024) 1
51. Huang, Z., Ben, Y., Luo, G., Cheng, P., Yu, G., Fu, B.: Shuffle transformer:
Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650
(2021) 29
52. Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of
diffusion-based generative models. In: NeurIPS (2022) 4
53. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative
adversarial networks. In: CVPR (2019) 10
54. Kingma, D., Salimans, T., Poole, B., Ho, J.: Variational diffusion models. In:
NeurIPS (2021) 10
55. Kingma, D.P., Gao, R.: Understanding the diffusion objective as a weighted in-
tegral of elbos. arXiv (2023) 10
56. Kitaev, N., Kaiser, Ł., Levskaya, A.: Reformer: The efficient transformer. arXiv
(2020) 1
57. Lee,S.,Kim,B.,Ye,J.C.:Minimizingtrajectorycurvatureofode-basedgenerative
models. ICML (2023) 4
58. Li, K., Li, X., Wang, Y., He, Y., Wang, Y., Wang, L., Qiao, Y.: Videomamba:
State space model for efficient video understanding. ECCV (2024) 3

--- PAGE 18 ---
18 Hu et al.
59. Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multi-
dimensional data. arXiv (2024) 3, 28, 29
60. Li, Y., Bornschein, J., Chen, T.: Denoising autoregressive representation learning.
arXiv preprint arXiv:2403.05196 (2024) 29
61. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai, X.: Point-
mamba: A simple state space model for point cloud analysis. arXiv preprint
arXiv:2402.10739 (2024) 3, 27, 28
62. Lin, B., Jiang, W., Chen, P., Zhang, Y., Liu, S., Chen, Y.C.: Mtmamba: En-
hancing multi-task dense scene understanding by mamba-based decoders. ECCV
(2024) 3
63. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 30
64. Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for
generative modeling. ICLR (2023) 2, 4
65. Liu, G.H., Chen, T., So, O., Theodorou, E.: Deep generalized schrödinger bridge.
NeurIPS (2022) 2
66. Liu, H., Zaharia, M., Abbeel, P.: Ring attention with blockwise transformers for
near-infinite context. arXiv (2023) 2
67. Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang,
S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pre-
training. arXiv (2024) 2, 6, 7
68. Liu, X., Gong, C., Liu, Q.: Flow straight and fast: Learning to generate and
transfer data with rectified flow. arXiv (2022) 4
69. Liu, X., Gong, C., Liu, Q.: Flow straight and fast: Learning to generate and
transfer data with rectified flow. ICLR (2023) 2
70. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba:
Visual state space model. arXiv (2024) 2, 3, 5, 6, 7, 13, 14, 28, 29
71. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin trans-
former: Hierarchical vision transformer using shifted windows. In: ICCV (2021)
1
72. Loshchilov,I.,Hutter, F.:Decoupled weightdecayregularization. In:ICLR(2019)
11
73. Ma,J.,Li,F.,Wang,B.:U-mamba:Enhancinglong-rangedependencyforbiomed-
ical image segmentation. arXiv (2024) 2, 3, 28
74. Ma, N., Goldstein, M., Albergo, M.S., Boffi, N.M., Vanden-Eijnden, E., Xie, S.:
Sit: Exploring flow and diffusion-based generative models with scalable inter-
polant transformers. ECCV (2024) 2, 4
75. McKenna, D.M.: Hilbert curves: Outside-in and inside-gone. Mathemaesthetics,
Inc (2019) 7, 26
76. Newell, A., Yang, K., Deng, J.: Stacked hourglass networks for human pose esti-
mation. In: ECCV (2016) 6
77. Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., Ré, C.:
S4nd: Modeling images and videos as multidimensional signals with state spaces.
NeurIPS (2022) 3, 28, 29
78. OpenAI: Sora: Creating video from text (2024), https://openai.com/sora 1, 6
79. Park, J., Kim, H.S., Ko, K., Kim, M., Kim, C.: Videomamba: Spatio-temporal
selective state space model. ECCV (2024) 3, 12
80. Peebles, W., Xie, S.: Scalable diffusion models with transformers. arXiv (2022) 1,
3, 5, 12, 23

--- PAGE 19 ---
ZigMa 19
81. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S.,
Cheah, E., Ferdinan, T., Hou, H., Kazienko, P., et al.: Eagle and finch: Rwkv with
matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892
(2024) 22
82. Qin, Z., Yang, S., Sun, W., Shen, X., Li, D., Sun, W., Zhong, Y.: Hgrn2: Gated
linear rnns with state expansion. arXiv preprint arXiv:2404.07904 (2024) 22
83. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 30
84. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022) 1, 3, 30
85. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: MICCAI (2015) 6
86. Ruan,J.,Xiang,S.:Vm-unet:Visionmambaunetformedicalimagesegmentation.
arXiv (2024) 3, 28
87. Skorokhodov, I., Sotnikov, G., Elhoseiny, M.: Aligning latent and image spaces to
connect the unconnectable. In: ICCV (2021) 34
88. Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for
sequence modeling. arXiv (2022) 2
89. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
vised learning using nonequilibrium thermodynamics. In: ICML (2015) 2
90. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution. arXiv (2019) 4
91. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.:
Score-based generative modeling through stochastic differential equations. In:
ICLR (2021) 2, 4, 9, 10
92. Stein, G., Cresswell, J., Hosseinzadeh, R., Sui, Y., Ross, B., Villecroze, V., Liu,
Z., Caterini, A.L., Taylor, E., Loaiza-Ganem, G.: Exposing flaws of generative
model evaluation metrics and their unfair treatment of diffusion models. NeurIPS
(2023) 29
93. Sun,Z.,Yang,Y.,Yoo,S.:Sparseattentionwithlearningtohash.In:ICLR(2021)
2
94. Tang, R., Liu, L., Pandey, A., Jiang, Z., Yang, G., Kumar, K., Stenetorp, P., Lin,
J., Ture, F.: What the daam: Interpreting stable diffusion using cross attention.
arXiv (2022) 8
95. Tikochinski,R.,Goldstein,A.,Meiri,Y.,Hasson,U.,Reichart,R.:Anincremental
large language model for long text processing in the brain (2024) 2
96. Tong,A.,Malkin,N.,Fatras,K.,Atanackovic,L.,Zhang,Y.,Huguet,G.,Wolf,G.,
Bengio, Y.: Simulation-free schr \" odinger bridges via score and flow matching.
arXiv (2023) 9
97. Unterthiner,T.,vanSteenkiste,S.,Kurach,K.,Marinier,R.,Michalski,M.,Gelly,
S.: Fvd: A new metric for video generation. ICLR Workshop (2019) 30
98. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 27
99. Wang, C., Tsepa, O., Ma, J., Wang, B.: Graph-mamba: Towards long-range graph
sequence modeling with selective state spaces. arXiv (2024) 28
100. Wang, J., Gangavarapu, T., Yan, J.N., Rush, A.M.: Mambabyte: Token-free se-
lective state space model. arXiv (2024) 3, 28
101. Wang, J., Yan, J.N., Gu, A., Rush, A.M.: Pretraining without attention. arXiv
(2022) 6

--- PAGE 20 ---
20 Hu et al.
102. Wang,S.,Li,Q.:Stablessm:Alleviatingthecurseofmemoryinstate-spacemodels
through stable reparameterization. arXiv (2023) 2, 28
103. Wang, S., Xue, B.: State-space models with layer-wise nonlinearity are universal
approximators with exponential decaying memory. NeurIPS (2024) 2, 28
104. Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon, H., Wei, F.: When an
image is worth 1,024 x 1,024 words: A case study in computational pathology.
arXiv (2023) 3
105. Wang, X., Wang, S., Ding, Y., Li, Y., Wu, W., Rong, Y., Kong, W., Huang, J.,
Li, S., Yang, H., Wang, Z., Jiang, B., Li, C., Wang, Y., Tian, Y., Tang, J.: State
space model for new-generation network alternative to transformers: A survey
(2024) 3
106. Wang, X., Kang, Z., Mu, Y.: Text-controlled motion mamba: Text-instructed
temporal grounding of human motion. arXiv preprint arXiv:2404.11375 (2024) 3
107. Wang, Z., Ma, C.: Semi-mamba-unet: Pixel-level contrastive cross-supervised vi-
sual mamba-based unet for semi-supervised medical image segmentation. arXiv
(2024) 28
108. Wang, Z., Zheng, J.Q., Zhang, Y., Cui, G., Li, L.: Mamba-unet: Unet-like pure
visual mamba for medical image segmentation. arXiv (2024) 3, 28
109. Wu, L., Wang, D., Gong, C., Liu, X., Xiong, Y., Ranjan, R., Krishnamoorthi, R.,
Chandra, V., Liu, Q.: Fast point cloud generation with straight flows. In: CVPR
(2023) 1
110. Xia, W., Yang, Y., Xue, J.H., Wu, B.: Tedigan: Text-guided diverse face image
generation and manipulation. In: CVPR (2021) 10, 30
111. Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential
modeling mamba for 3d medical image segmentation. arXiv (2024) 3, 28
112. Yan, J.N., Gu, J., Rush, A.M.: Diffusion models without attention. arXiv (2023)
4, 6
113. Yang, S., Wang, B., Shen, Y., Panda, R., Kim, Y.: Gated linear attention trans-
formers with hardware-efficient training. ICML (2024) 22
114. Yang,S.,Zhang,Y.:Fla:Atriton-basedlibraryforhardware-efficientimplementa-
tions of linear attention mechanism (Jan 2024), https://github.com/sustcsonglin/flash-
linear-attention 22
115. Yang, Y., Xing, Z., Zhu, L.: Vivim: a video vision mamba for medical video object
segmentation. arXiv (2024) 6
116. Yu, A., Nigmetov, A., Morozov, D., Mahoney, M.W., Erichson, N.B.: Robustify-
ing state-space models for long sequences via approximate diagonalization. arXiv
(2023) 2
117. Yu, S., Sohn, K., Kim, S., Shin, J.: Video probabilistic diffusion models in pro-
jected latent space. In: CVPR (2023) 30
118. Zhang, T., Li, X., Yuan, H., Ji, S., Yan, S.: Point could mamba: Point cloud
learning via state space model. arXiv (2024) 28
119. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-
tional neural network for mobile devices. In: CVPR (2018) 29
120. Zhang, Z., Liu, A., Reid, I., Hartley, R., Zhuang, B., Tang, H.: Motion mamba:
Efficient and long sequence motion generation with hierarchical and bidirectional
selective ssm. ECCV (2024) 3
121. Zhang, Z., Liu, A., Reid, I., Hartley, R., Zhuang, B., Tang, H.: Motion mamba:
Efficient and long sequence motion generation with hierarchical and bidirectional
selective ssm. arXiv (2024) 28
122. Zheng, Z., Wu, C.: U-shaped vision mamba for single image dehazing. arXiv
(2024) 3, 28

--- PAGE 21 ---
ZigMa 21
123. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Ef-
ficient visual representation learning with bidirectional state space model. ICML
(2024) 2, 3, 5, 7, 11, 13, 14, 28
124. zhuzilin: Ring flash attention. https://github.com/zhuzilin/ring-flash-attention (2024)
2

--- PAGE 22 ---
22 Hu et al.
6 Limitations and Future Work
Our method relies solely on the Mamba Block with a DiT-style layout and con-
ditioning manner. However, a potential limitation of our work is that we cannot
exhaustively list all possible spatial continuous zigzag scanning schemes given a
specific global patch size. Currently, we set these scanning schemes empirically,
which may lead to sub-optimal performance. Additionally, due to GPU resource
constraints, we were unable to explore longer training durations, although we
anticipate similar conclusions.
For future work, we aim to delve into various applications of the Zigzag
Mamba, leveraging its scalability for long-sequence modeling. This exploration
may lead to improved utilization of the Mamba framework across different do-
mains and applications.
Ultimately, we anticipate that our scan path will be suitable for other linear
attention models such as RWKV [81], xLSTM [11], HGRN [82], GLA [113], and
several others listed at FLA [114]3.
7 Impact Statement
This work aims to enhance the scalability and unlock the potential of the Mamba
algorithm within the framework of diffusion models, enabling the generation of
large images with high-fidelity. By incorporating our cross-attention mechanism
into the Mamba block, our method can also facilitate text-to-image generation.
However, like other endeavors aimed at enhancing the capabilities and control
of large-scale image synthesis models, our approach carries the risk of enabling
the generation of harmful or deceptive content. Therefore, ethical considerations
and safeguards must be implemented to mitigate these risks.
8 Appendix
Table 7: The ablation about Hilbert and Zigzag scan path under various
Order Receptive Field ( ORF ) on unconditional MultiModal-CelebA256.
Scan-ORF FID5k
hilbert-2 61.67
hilbert-8 27.38
zigzag-2 15.45
zigzag-8 13.32
3https://github.com/sustcsonglin/flash-linear-attention

--- PAGE 23 ---
ZigMa 23
Table 8: Various methods for text-to-image generation on the MultiModal-
CelebA 256 dataset.
Method FID5kFDD5kKID5k
In-Context 61.1 39.1 0.061
Cross-Attention 45.5 26.4 0.011
Table 9: Details of ZigMa Model Variants. We follow previous works [9,25,
80] model configurations for the Small (S), Base (B) and Large (L) variants; we also
introduce an XLarge (XL) config as our largest model. CA denotes the cross-attention
for text-to-image conditioning.
Model Layers NHidden size d#params
ZigMa-S 12 384 31.3M
ZigMa-B 12 768 133.8M
ZigMa-L 24 1024 472.5M
ZigMa-XL 28 1152 1058.7M
CA-ZigMa-S 12 384 59.2M
CA-ZigMa-B 12 768 214.1M
CA-ZigMa-L 24 1024 724.4M
CA-ZigMa-XL 28 1152 1549.8M
8.1 Visualization
FacesHQ 1024×1024uncurated visualization in Fig. 16.
MS-COCO uncurated visualization. We visualize the samples in Fig. 15.
Model-S Model-B Model-L Model-H010203040FPSZigzag Mamba (Our)
Parallel Mamba
(a)Model Complexity v.s.FPS.
Model-S Model-B Model-L Model-H051015GPU Memory (G)Zigzag Mamba (Our)
Parallel Mamba (b)Model Complexity v.s.GPU Memory.
Figure7: The ablation study about Model Complexity, FPS, GPU Memory .
8.2 Spatial Continuity is Critical
We first explore the importance of spatial continuity in Mamba design by group-
ingpatchesofsize N×Nintovarioussizes: 2×2,4×4,8×8,and 16×16,resulting
in groups of patch sizes N/2×N/2,N/4×N/4,N/8×N/8, and N/16×N/16,
respectively. Then, we apply our designed Zigzag-8 scheme at the group level

--- PAGE 24 ---
24 Hu et al.
Figure8: The FID trends comparing the Hilbert scan, Sweep scan, and our
Zigzag scan. The y-axis is logarithmic scale.
(a)size=8
 (b)size=16
Figure9: The Hilbert space-filling curve with various sizes .
instead of the patch level. Figure 11 illustrates that with increased spatial conti-
nuity, notably improved performance is achieved. Furthermore, we compare our
approach with random shuffling of N×Npatches, revealing notably inferior
performance under random shuffling conditions. All of these results collectively

--- PAGE 25 ---
ZigMa 25
Peano curve from simple to complex.
Figure10: The demonstration of Peano Curve. The figure is borrowed from
https://en.wikipedia.org/wiki/Peano_curve .
indicate that spatial continuity is a critical requirement when applying Mamba
in 2D sequences.
random 2 4 8 16
Patch Group Size101102FID
Patch Size 32x32
Patch Size 64x64
Figure11: Spatial Continuity Analysis. As we incrementally enlarge the patch
group size, the continuous segment of the patch also expands. This enhances spatial
continuity, which we find improves FID on MultiModal-CelebA 256, 512 dataset.
8.3 Visualization
We demonstrate the image visualization of our best results on FacesHQ 1024
and MultiModal-CelebA 512 in Figure 12. For the visualization of videos, please
refer to Appendix 8.1. It is evident that the visualization is visually pleasing
across various resolutions, indicating the efficacy of our methods.
8.4 New Result about the Scanning Scheme
We also conduct basic ablations on various factors, including position embedding
and various Hilbert space-filling curves. Unlike the experiments in the main
paper, we perform these experiments on unconditional MultiModal-CelebA256
dataset for a uniform comparison. We train the network for 100,000 steps.

--- PAGE 26 ---
26 Hu et al.
FacesHQ 1024 × 1024MultiModal-CelebA 512 × 512
Figure12: Visualization of various resolutions on FacesHQ 1024×1024and
MultiModal-CelebA 512×512.Our generated samples present high fidelity across
various resolutions.
Exploration of Hilbert space-filling curve. Primarily, we ablate the Hilbert
scan curve [75], as depicted in Figure 9. There are also eight variants of this
scan considering different angles and starting points. We rearrange them in a
similar manner to our Zigzag scan. All parameters are kept consistent for a fair
comparison. We utilize the Gilbert algorithm4to guarantee that the Hilbert
curve remains continuous across any square size. We train our network on single
A100-SXM4-80GB for 120k iterations. We evaluate the FID on 5,000 images for
a fixed step, the FID curve is demonstrated in Figure 8.
While the Hilbert space-filling curve offers increased locality compared to our
zigzagscanandmaintainscontinuity,itscomplexstructureappearstohinderthe
SSM’s ability to work on the flattened sequence, resulting in a worse inductive
bias than our zigzag curve on natural images. Therefore, we hypothesize that
structure may hold greater significance than locality in generative tasks.
Hilbert Curve is difficult to optimize. We show the result in Table 7.
We can observe that the performance of the Hilbert scan path drops signifi-
cantly, even if we decrease the Order of Receptive Field (ORF). This confirms
the assumption that the Hilbert scan path is difficult to optimize, even when
considering only two different schemes of the Hilbert scan.
Another Interpretation: Zigzag scan is the simplest Peano curve. Our
Zigzag scan can be seen as the simplest case of Peano Curve as shown in Fig-
ure 10.
8.5 New Result of 2D visual data
The variants of our ZigMa Models. We list the variants of our model in Ta-
ble 9. We use the Base (B) Model as the default. Applying the cross-attention
model is optional, as this module can introduce some parameter and speed bur-
dens. However, any advancements in attention optimization can be seamlessly
integrated into our model.
4https://github.com/jakubcerveny/gilbert

--- PAGE 27 ---
ZigMa 27
Ablationofpatchsize. Weconductedanablationstudyonpatchsizesranging
from 1, 2, 4, to 8 in Figure 13, aiming to explore their behaviors under the
framework of Mamba. The results reveal that the FID deteriorates as the patch
size increases, aligning with the common understanding observed in the field
of transformers [25,98]. This suggests that smaller patch sizes are crucial for
optimal performance.
Figure13: FPS v.s.Patch Size.
Ablation study about the Model Complexity and FPS/GPU-Memory.
As shown in Figure 7. Our method can achieve much better parameter efficiency
whenincorporatingthereceptiveorder.Thereceptiveorderreferstothecumula-
tive spatial-continuous zigzag scan path in 2D images, which we’ve incorporated
into the Mamba as an inductive bias. We list the parameter consumption when
we gradually increase the receptive order in Figure 7. The receptive order refers
to the cumulative spatial-continuous zigzag scan path in 2D images, which we’ve
incorporated into the Mamba as an inductive bias.
Loss and FID curve. The training loss curve and the FID curve are demon-
strated in Figure 14. The loss and FID show the same trend, with our Zigzag
Mamba consistently outperforming other baselines like Sweep-1 and Sweep-2.
In-context v.s. Cross Attention We compare our cross-attention with in-
context attention in Table 8. For in-context attention, we concatenate the text
tokens with the image tokens and feed them into the Mamba block. Our results
demonstrate that in-context attention performs worse than our cross-attention.
We hypothesize that this is due to the discontinuity between the text tokens
and the image patch tokens. We discovered that PointMamba [61] arrives at the
same conclusion and hypothesis as we do.

--- PAGE 28 ---
28 Hu et al.
(a)Loss trend of the MultiModal-CelebA256.
 (b)FID trend of the MultiModal-CelebA256.
(c)Loss trend of the MultiModal-CelebA512.
 (d)FID trend of the MultiModal-CelebA512.
Figure14: The loss and FID trend under various resolutions on dataset
MultiModal-CelebA. Sweep-1 and Sweep-2 are the Mamba scans without spatial conti-
nuity, while Zigzag-8 represents our method. This is the direct log from weight-and-bias
(wandb).
8.6 New result of 3D Visual Data
The choice of the 3D Zigzag Mamba. For Factorized 3D Zigzag Mamba
in video processing, we deploy the sstscheme for factorizing spatial and tem-
poral modeling. This scheme prioritizes spatial information (ss)complexity over
temporal information (t), hypothesizing that redundancy exists in the temporal
domain. There are numerous other possible combinations of s and t to explore,
which we leave for future work.
8.7 More related works
Several works [102,103] have demonstrated that the State-Space Model pos-
sesses universal approximation ability under certain conditions. Mamba, as a
new State-Space Model, has superior potential for modeling long sequences
efficiently, which has been explored in various fields such as medical imag-
ing [31,73,86,108,111], image restoration [38,122], graphs [12,99], NLP word
byte [100], tabular data [2], human motion synthesis [121], point clouds [61,118],
image generation [27], semi-supervised learning [107],interpretability [5], image
dehazing [122] and pan sharpening [41]. It has been extended to Mixture of
Experts [7], spectral space [1], multi-dimension [59,70,77,123] and dense con-
nection [40]. Among them, the most related to us are VisionMamba [70,123],
S4ND [77] and Mamba-ND [59]. VisionMamba [70,123] uses a bidirectional SSM

--- PAGE 29 ---
ZigMa 29
in discriminative tasks which incurs a high computational cost. Our method
applies a simple alternative mamba diffusion in generative models. S4ND [77]
introduces local convolution into Mamba’s reasoning process, moving beyond
the use of only 1D data. Mamba-ND [59] takes multi-dimensionality into ac-
count in discriminative tasks, making use of various scans within a single block.
In contrast, our focus is on distributing scan complexity across every layer of the
network, thus maximizing the incorporation of inductive bias from visual data
with zero parameter burden.
Certain studies, such as Li’s work in 2024 [60], often explore the order of
patches in token-based networks. However, while these studies concentrate on
auto-regressive transformers, our focus is on the Mamba-based structure.
Several previous works [51,119] have focused on the shuffling operation to
exchange information along the spatial or channel dimension. For instance, the
Shuffle Transformer [51] applies shuffling to spatial tokens to encourage cross-
reasoningoutsidetheattentionwindows.Ourmethodfollowsthesameapproach.
We shuffle the tokens to maintain a continuous spatial-filling scan path, pro-
moting optimization across various layers. Given that the shuffling order differs
across the layers, it could potentially avert the overfit problem [70].
8.8 More Details
Double-Indexing Issue for Ωi.As shown in Fig. 2. We need to arrange
andrearrange operation that needs to conduct indexing along the token number
dimensiontoachievespatial-continuousmambareasoning,astheindexingcanbe
time-consuming5when considering the large token numbers, We can formulate
thearrangeand rearrange operation as follows:
Ω′
i=¯Ωi−1·Ωi, (14)
zi+1=scan(zΩ′
i), (15)
(16)
where ¯Ω−1=I, this assumes that the Mamba-based networks are permuta-
tion equivariant to the order of the tokens. They require 50% fewer indexing
operations, a point which we reiterate here for clearer comparison:
zΩi=arrange (zi, Ωi), (17)
¯zΩi=scan(zΩ), (18)
zi+1=arrange (¯zΩi,¯Ωi), (19)
EvaluationMetrics. Forimage-levelfidelity,weuseestablishedmetricssuchas
Fréchet Inception Distance (FID) and Kernel Inception Distance (KID), follow-
ing previous works. However, since studies [21,92] have shown that FID does not
5We found that using the torch.compile() can largely ease the time issue, see https:
//taohu.me/zigma for more detail comparison.

--- PAGE 30 ---
30 Hu et al.
fully reflect human-based opinions, we also adopt the Fréchet DINOv2 Distance
(FDD) using the official repository6. Our method primarily involves sampling
5,000 real and 5,000 fake images to compute the related metrics.
We primarily consider two metrics for video fidelity evaluation: framewise
FID and Fréchet Video Distance (FVD) [97]. We sample 200 videos and compute
the respective metrics based on these samples.
We utilize the EMA models for evaluation, as they can deliver superior per-
formance, as indicated in [117].
Extra Training Details. For text-conditioned generation, we conduct the ex-
periments on the MultiModal-CelebA 2562,5122[110] and MS COCO 256×
256[63] datasets. Both datasets are composed of text-image pairs for training.
Typically, there are 5 to 10 captions per image in COCO and MultiModal-
CelebA. We convert discrete texts to a sequence of embeddings using a CLIP
text encoder [83] following Stable Diffusion [84]. Then these embeddings are fed
into the network as a sequence of tokens.
Thetrainingparametersofvariousdatasets arelistedinTab.10.Wedon’t
apply any position encoding because Mamba, unlike Transformer, is not permu-
tation invariant. Therefore, its position is automatically encoded by its order in
Mamba. Surprisingly, we also found that adding extra learnable position encod-
ing can lead to better performance compared to the baseline. We hypothesize
that these extra inductive biases can further benefit performance, even though
the order of the tokens already incorporates some bias. For the COCO dataset, a
weight decay of 0.01 can contribute to marginal FID gains (approximately 0.8).
The conditioning of timestep and prompt. The conditioning process is
illustrated in Algorithm 1. For the Mamba block, we incorporate the condition
information. Specifically, we concatenate the condition token with the image
patch token to enhance the conditioning mechanism.
6https://github.com/layer6ai-labs/dgm-eval

--- PAGE 31 ---
ZigMa 31
Table 10: Hyperparameters and number of parameters for our network in
various datasets.
FacesHQ 1024 MS-COCO 256 MultiModal-CelebA 512 UCF-101
Autoencoder f 8 8 8 8
z-shape 4×128×128 4 ×32×32 4 ×64×64 4 ×32×32
Model size 133.8M 133.8M 133.8M 133.8M
Patch size 2 1 1 2
Channels 768 768 768 768
Depth 12 12 12 12
Optimizer AdamW AdamW AdamW AdamW
Batch size/GPU 8 8 4 8
GPU num 32 32 16 16
Learning rate 1e-4 1e-4 1e-4 1e-4
weight decay 0 0 0 0
EMA rate 0.9999 0.9999 0.9999 0.9999
Warmup steps 0 0 0 0
A100-hours 768 768 384 384
Algorithm 1 Mamba Block
def mamba_block(x, t, c = None):
# x: input data, shape [B, (W x H) , C] or [B, (T x W x H) , C]
# t: timestep, (B, C)
# c: condition, (B, D, C)
x = reshape(x) # (B, K, C)
def _mamba(x):
x = rearrange(x) # rearrange by a zigzag manner
x = mamba(x)
x = rearrange_back(x)# rearrange back by a zigzag manner
m, n = AdaLN( t )
x = _mamba( x * m + n ) + x
if c is not None:
p, q = AdaLN( c )
x = cross_attention( x * p + q ) + x
return x

--- PAGE 32 ---
32 Hu et al.
Figure15: The Uncurated Visualization of MS-COCO dataset. The first row
is illustrated with pairs of images and their captions, while the remaining rows only
images.

--- PAGE 33 ---
ZigMa 33
Figure16: Uncurated Visualization of FacesHQ dataset.

--- PAGE 34 ---
34 Hu et al.
Figure17: Uncurated Visualization of Landscape HQ dataset [87], with 5k
FID of 10.07.
