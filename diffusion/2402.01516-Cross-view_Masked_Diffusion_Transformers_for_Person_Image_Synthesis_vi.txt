# 2402.01516.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2402.01516.pdf
# Kích thước file: 8947297 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Transformer Khuếch tán Có Mặt nạ Đa góc nhìn cho Tổng hợp Hình ảnh Con người
Trung X. Pham*Zhang Kang*Chang D. Yoo
Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc (KAIST)
Tóm tắt
Chúng tôi giới thiệu X-MDPT (Transformer Dự đoán Khuếch tán Có Mặt nạ Đa góc nhìn), một mô hình khuếch tán mới được thiết kế cho việc tạo hình ảnh con người có hướng dẫn tư thế. X-MDPT có điểm đặc biệt là sử dụng các transformer khuếch tán có mặt nạ hoạt động trên các bản vá tiềm ẩn, khác với các cấu trúc Unet thường được sử dụng trong các nghiên cứu hiện tại. Mô hình bao gồm ba module chính: 1) một Transformer khuếch tán khử nhiễu, 2) một mạng tổng hợp tích hợp các điều kiện thành một vector duy nhất cho quá trình khuếch tán, và 3) một module dự đoán chéo có mặt nạ giúp nâng cao việc học biểu diễn với thông tin ngữ nghĩa từ hình ảnh tham chiếu. X-MDPT thể hiện khả năng mở rộng, cải thiện FID, SSIM, và LPIPS với các mô hình lớn hơn. Mặc dù có thiết kế đơn giản, mô hình của chúng tôi vượt trội so với các phương pháp tiên tiến trên bộ dữ liệu DeepFashion đồng thời cho thấy hiệu quả về tham số đào tạo, thời gian đào tạo, và tốc độ suy luận. Mô hình nhỏ gọn 33MB của chúng tôi đạt FID 7.42, vượt qua phương pháp khuếch tán tiềm ẩn Unet trước đó (FID 8.07) chỉ với 11× ít tham số hơn. Mô hình tốt nhất của chúng tôi vượt trội so với khuếch tán dựa trên pixel với 2/3 tham số và đạt tốc độ suy luận nhanh hơn 5.43×. Mã nguồn có sẵn tại https://github.com/trungpx/xmdpt .

1. Giới thiệu
Nhiệm vụ Tạo Hình ảnh Con người Có Hướng dẫn Tư thế (PHIG) (Ma et al., 2017) đã thu hút sự chú ý đáng kể với sự ra đời gần đây của các mô hình khuếch tán. Ban đầu, các phương pháp dựa trên GAN (Ma et al., 2017; Men et al., 2020; Wu et al., 2023), cho thấy tiềm năng trong PHIG nhưng thường gặp khó khăn trong việc tạo hình ảnh mục tiêu chính xác, dẫn đến điểm Khoảng cách Inception Frechet (FID) cao do sự hiện diện của các tạo tác không mong muốn. Để giải quyết những thách thức này, Bhunia et al. (2023) đã giới thiệu PIDM, một phương pháp dựa trên khuếch tán sử dụng quá trình tạo lặp. Mặc dù PIDM đạt được kết quả tiên tiến trong việc tổng hợp hình ảnh chất lượng cao từ tư thế mục tiêu và hình ảnh nguồn, nó gặp phải tốc độ suy luận chậm và tiêu thụ bộ nhớ cao do hoạt động dựa trên pixel. Để đáp ứng những lo ngại về hiệu quả này, Han et al. (2023) đã đề xuất PoCoLD, một khung khuếch tán tiềm ẩn hoạt động trên các đầu ra tiềm ẩn của bộ mã hóa tự động. Tuy nhiên, trong khi PoCoLD mang lại cải thiện về hiệu quả so với PIDM, nó lại kém hơn về các chỉ số FID và Chỉ số Tương đồng Cấu trúc (SSIM) so với PIDM. Đáng chú ý, các phương pháp phổ biến chủ yếu dựa vào kiến trúc dựa trên Unet với Mạng Nơ-ron Tích chập (CNN) cho các quá trình khuếch tán khử nhiễu.

Ngược lại, phương pháp của chúng tôi giới thiệu một lớp mô hình khuếch tán mới dựa trên transformer, nhằm giải quyết các thách thức trong PHIG một cách hiệu quả hơn. Các mô hình khuếch tán đã chứng minh thành công trong việc học phân phối dữ liệu trên nhiều lĩnh vực khác nhau, đặc biệt trong các nhiệm vụ tạo sinh (Ho et al., 2020; Rombach et al., 2022). Một lợi thế quan trọng của các mô hình khuếch tán nằm ở quá trình đào tạo ổn định so với Mạng Đối kháng Tạo sinh (GAN), vốn dễ bị sụp đổ chế độ (Isola et al., 2017). Trong khi các mô hình khuếch tán dựa trên Unet, bao gồm Stable Diffusion (Rombach et al., 2022) và các nghiên cứu liên quan (Zhang et al., 2023a; Zhao et al., 2023), đã đạt được thành công đáng kể trong nhiều ứng dụng, DiT (Peebles & Xie, 2023) đã thể hiện hiệu quả của các thiết kế dựa trên transformer trong việc học các quá trình khuếch tán, thách thức hiệu quả các mạng truyền thống dựa trên Unet trong tạo hình ảnh. Sự xuất hiện của các transformer khuếch tán có mặt nạ (Gao et al., 2023) như một phương pháp tiên tiến cho việc tạo hình ảnh có điều kiện lớp trên ImageNet đã truyền cảm hứng cho nghiên cứu của chúng tôi. Được thúc đẩy bởi những tiến bộ này, chúng tôi đề xuất một khung dựa trên mặt nạ có điều kiện trên cả hình ảnh tư thế và phong cách để tạo hình ảnh mục tiêu trong nhiệm vụ PHIG (Ma et al., 2017; Wu et al., 2023). Phương pháp mới này nhằm tận dụng thông tin ngữ nghĩa từ hình ảnh tham chiếu, nâng cao khả năng của mô hình trong việc tạo hình ảnh con người thực tế và có tính nhất quán về ngữ cảnh.

Trong bài báo này, chúng tôi giới thiệu X-MDPT, tận dụng các mô hình Transformer khuếch tán cho việc tạo hình ảnh con người có hướng dẫn tư thế. Chúng tôi thiết kế một mạng tổng hợp chuyên biệt để tích hợp tất cả các điều kiện thành một vector duy nhất để hướng dẫn Transformer khuếch tán thông qua điều biến Adaptive Layernorm. Ngoài ra, chúng tôi giới thiệu một mạng mặt nạ mới để nâng cao khả năng học của transformer. Bằng cách mở rộng số lượng lớp hoặc đầu, chúng tôi có được các cấu hình X-MDPT-S, X-MDPT-B, và X-MDPT-L, theo các cấu hình transformer tiêu chuẩn (Dosovitskiy et al., 2010; Vaswani et al., 2017). Mặc dù có thiết kế đơn giản, X-MDPT đạt được điểm FID, SSIM, và LPIPS tiên tiến trên bộ dữ liệu DeepFashion đồng thời sử dụng ít tham số hơn đáng kể so với các SOTA hiện có (Hình 1). X-MDPT kết hợp tính đơn giản và hiệu quả, duy trì khả năng mở rộng và tính linh hoạt của kiến trúc Transformer, chứng minh tiềm năng của nó cho việc tạo hình ảnh con người. Hơn nữa, Hình 2 cho thấy mô hình của chúng tôi tạo ra đầu ra nhất quán và hợp lý hơn so với PIDM SOTA. Các đóng góp của chúng tôi như sau:

• Chúng tôi đề xuất X-MDPT, khung transformer khuếch tán có mặt nạ đầu tiên cho nhiệm vụ PHIG. Mô hình của chúng tôi có khả năng mở rộng và hiệu quả vì nó hoạt động trên các bản vá tiềm ẩn.

• Chúng tôi đề xuất CANet để tổng hợp tất cả các điều kiện thành một vector thống nhất và chúng tôi chỉ ra rằng một vector duy nhất cung cấp thông tin đầy đủ để hướng dẫn quá trình khuếch tán. Điều này đơn giản về mặt khái niệm vì nó không yêu cầu bất kỳ sửa đổi nào đối với khung khuếch tán chính cũng như không trích xuất các đặc trưng đa cấp từ các lớp khác nhau của điều kiện như đã thực hiện trong các phương pháp hiện tại (CASD, PIDM, và PoCoLD).

• Chúng tôi đề xuất một chiến lược đa góc nhìn mới để dự đoán các token có mặt nạ trên các hình ảnh, nâng cao việc học biểu diễn và cải thiện chất lượng tạo sinh.

• X-MDPT vượt trội so với các phương pháp tiên tiến khác trên bộ dữ liệu DeepFashion đồng thời hiệu quả hơn. Ví dụ, các mô hình nhỏ gọn của chúng tôi vượt trội so với PoCoLD dựa trên khuếch tán tiềm ẩn và PIDM dựa trên pixel chỉ với 11× và 2/3× ít tham số hơn tương ứng. Chúng tôi chỉ ra định tính rằng mô hình của chúng tôi mạnh mẽ với nhiều trường hợp khó khăn khác nhau mà các phương pháp khác không thể tạo ra hình ảnh mong muốn.

2. Nghiên cứu Liên quan
Tổng hợp Hình ảnh Con người Có Hướng dẫn Tư thế. Bhunia et al. (2023) đã giới thiệu PIDM như một giải pháp cho vấn đề PHIG sử dụng mô hình khuếch tán trong không gian pixel, cho thấy hiệu suất đáng kể so với các phương pháp truyền thống dựa trên GAN. Tuy nhiên, PIDM yêu cầu tài nguyên tính toán đáng kể và thể hiện tính kém hiệu quả trong cả đào tạo và suy luận. (Han et al., 2023) đã giới thiệu PoCoLD, một mô hình khuếch tán tiềm ẩn dựa trên thiết kế Unet, để giải quyết những hạn chế này. Đồng thời, Karras et al. (2023) đã trình bày DreamPose, áp dụng mô hình khuếch tán vào lĩnh vực video thời trang bằng cách tinh chỉnh mô hình văn bản-thành-hình ảnh, cụ thể là Stable Diffusion (Rombach et al., 2022). Trong khi các phương pháp này nâng cao khả năng của mô hình khuếch tán cho PHIG với các bộ khử nhiễu dựa trên CNN, tiềm năng của các phương pháp dựa trên transformer vẫn chưa được khai thác. Để thu hẹp khoảng cách này, chúng tôi đề xuất X-MDPT, tận dụng các mô hình khuếch tán transformer thuần túy để tạo hình ảnh con người mục tiêu.

Transformer Khuếch tán. Cấu trúc CNN U-Net (Ronneberger et al., 2015) ban đầu phục vụ như nền tảng cho các mô hình khuếch tán và vẫn là lựa chọn tiêu chuẩn trên nhiều nhiệm vụ tạo sinh dựa trên khuếch tán khác nhau (Ho et al., 2020; 2022; Song & Ermon, 2019). Việc giới thiệu DiT (Peebles & Xie, 2023) đánh dấu một bước tiến quan trọng, tích hợp kiến trúc của transformer thuần túy ViT (Dosovitskiy et al., 2021) vào các mô hình khuếch tán tiềm ẩn. DiT đã thể hiện khả năng mở rộng đặc biệt và vượt trội so với các kiến trúc giống Unet. Gao et al. (2023) đã tiến xa hơn với mô hình transformer khuếch tán, đạt được việc tạo hình ảnh lớp tiên tiến trên ImageNet bằng cách tận dụng việc học biểu diễn ngữ cảnh. Trong khi họ khám phá tiềm năng của transformer trong các nhiệm vụ tạo sinh chung, trọng tâm của chúng tôi nằm ở việc áp dụng transformer khuếch tán cụ thể cho việc tạo hình ảnh con người có hướng dẫn tư thế (PHIG) trong lĩnh vực Thời trang lần đầu tiên. PHIG đại diện cho một nhiệm vụ tạo sinh then chốt và phức tạp, yêu cầu việc trích xuất thông tin toàn diện từ hình ảnh nguồn bao gồm quần áo, danh tính, nền, và nhiều hơn nữa, để tạo ra tư thế mục tiêu mong muốn một cách trung thực (Ma et al., 2017; Bhunia et al., 2023).

Mô hình Dự đoán Mặt nạ. Các mô hình thị giác dựa trên mặt nạ, lấy cảm hứng từ các mô hình ngôn ngữ mặt nạ như BERT (Devlin et al., 2018), đã thể hiện khả năng mở rộng và hiệu suất đáng kể. Các ví dụ đáng chú ý bao gồm MAE (He et al., 2022) trong học tự giám sát (SSL), MaskGIT (Chang et al., 2022), Muse (Chang et al., 2023), và MAGVIT (Yu et al., 2023) để học phân phối token rời rạc cho việc tạo hình ảnh. Ngược lại, MDT (Gao et al., 2023) đã giới thiệu một lịch trình mặt nạ bất đối xứng để nâng cao việc học biểu diễn ngữ cảnh trong các transformer khuếch tán. Tuy nhiên, MDT gặp khó khăn trong việc thiết lập sự tương ứng giữa hình ảnh nguồn và mục tiêu, hạn chế khả năng học biểu diễn của nó. Để giải quyết hạn chế này, chúng tôi đề xuất Mạng Dự đoán Mặt nạ Liên kết (MIPNet) lấy cảm hứng từ các nghiên cứu trước đây như SiamMAE (Gupta et al., 2023) và PatchMAE (Zhang et al., 2023b) trong SSL. MIPNet tập trung vào dự đoán mặt nạ ngữ nghĩa liên kết trong các mô hình khuếch tán cho các nhiệm vụ PHIG. Mặc dù nó có điểm tương đồng với những MAE này trong việc sử dụng các góc nhìn khác nhau để dự đoán mặt nạ, MIPNet khác biệt ở khối tự-chú ý chéo nhẹ và mặt nạ bất đối xứng, trái ngược với những MAE này. Ngoài ra, MIPNet được thiết kế cho các nhiệm vụ tạo sinh, trong khi SimMAE/PatchMAE dành cho các nhiệm vụ nhận dạng hạ nguồn.

3. Phương pháp
Chúng tôi nhằm thiết kế một khung đơn giản nhưng có thể mở rộng để giải quyết nhiệm vụ PHIG sử dụng Transformer. Kiến trúc tổng thể được mô tả trong Hình 3. X-MDPT của chúng tôi bao gồm ba module cốt lõi: 1) Mạng Khuếch tán Khử nhiễu dựa trên Transformer (TDNet), 2) Mạng Tổng hợp Điều kiện (CANet), và 3) Mạng Dự đoán Mặt nạ Liên kết (MIPNet). Ở đây, TDNet thực hiện khuếch tán khử nhiễu, trong khi CANet tích hợp tất cả các đầu vào điều kiện cần thiết thành một vector duy nhất cho đầu vào của TDNet. Ngoài ra, MIPNet nâng cao quá trình học khuếch tán bằng cách dự đoán các token có mặt nạ sử dụng một bộ dự đoán mới dựa trên tham chiếu. Chúng tôi cung cấp hiểu biết chi tiết về từng thành phần tiếp theo.

3.1. Mạng Khuếch tán Khử nhiễu dựa trên Transformer
Trong khung X-MDPT của chúng tôi, chúng tôi ký hiệu thành phần mạng này là TDNet cho ngắn gọn. TDNet được xây dựng dựa trên DiT (Peebles & Xie, 2023) để thiết lập quá trình khuếch tán sử dụng kiến trúc Transformer. Dưới đây là tổng quan về khung cho trường hợp độ phân giải 256×256: Cho hình ảnh nguồn Xs∈R256×256×3 và tư thế mục tiêu yp, mục tiêu là học mô hình được tham số hóa bởi θ để nắm bắt tư thế mục tiêu và phong cách của hình ảnh nguồn để tạo hình ảnh mục tiêu cuối cùng Y∈R256×256×3. Ban đầu, chúng tôi sử dụng V AE đã được đào tạo trước (Rombach et al., 2022) để ánh xạ hình ảnh pixel thành biểu diễn tiềm ẩn xs∈R32×32×4 và y∈R32×32×4 để khử nhiễu. Mạng khử nhiễu ϵθ là một mô hình khuếch tán dựa trên transformer học phân phối điều kiện pθ(y|xs, yp). Quá trình khử nhiễu dần dần thêm nhiễu Gaussian ϵ∼ N(0,I) vào hình ảnh y để có được yt tại thời điểm t∈[1, T]. Các điều kiện xs và tư thế yp được biểu diễn bởi c. Mục tiêu đào tạo là dự đoán nhiễu được thêm vào sử dụng lỗi bình phương trung bình:

Ldenoise =Ey,c,ϵ∼N(0,I),t∥ϵ−ϵθ(yt, c, t)∥2. (1)

Khi pθ được đào tạo, suy luận tiến hành bằng cách khởi tạo với hình ảnh nhiễu ngẫu nhiên yT∼ N(0,I) và lấy mẫu lặp yt−1∼pθ(yt−1|yt) để có được hình ảnh mục tiêu cuối cùng y0.

Mạng transformer khuếch tán của chúng tôi tuân theo cấu trúc được nêu trong DiT (Peebles & Xie, 2023). Chúng tôi biến đổi tiềm ẩn nhiễu yt∈R32×32×4 thành các bản vá với kích thước bản vá p= 2, tạo thành chuỗi zyt= [z(1)y, z(2)y, ..., z(Ly)y]∈ RLy×D, trong đó Ly và D biểu thị độ dài chuỗi và chiều tương ứng. Điều kiện c được tích hợp vào TDNet thông qua chuẩn hóa lớp thích ứng (AdaLN-Zero), theo thiết lập mặc định của DiT.

3.2. Mạng Tổng hợp Điều kiện
Mạng CANet tích hợp ba đầu vào: 1) đặc trưng điều kiện tư thế mục tiêu (TPF), 2) đặc trưng hình ảnh nguồn cục bộ (LSIF) thu được từ đầu ra của V AE, và 3) đặc trưng hình ảnh nguồn toàn cục (GSIF) có nguồn gốc từ đặc trưng đã đào tạo trước của DINOv2. CANet xử lý các đầu vào này để tạo ra một vector thống nhất c với các chiều phù hợp với chiều rộng của Transformer.

Đặc trưng Hình ảnh Nguồn Cục bộ. Đặc trưng này đảm bảo sự căn chỉnh với hình ảnh mục tiêu nhiễu trong TDNet, cho phép việc truyền thông tin từ hình ảnh nguồn (bao gồm quần áo, con người, và nền) để tạo hình ảnh mục tiêu. Cụ thể, hình ảnh tiềm ẩn 32×32×4 được chuyển đổi thành một chuỗi zxs= [z(1)x, z(2)x, ..., z(Lx)x]∈RLx×D, trong đó Lx= 256 token ( LSIF =zxs∈R256×D). Sau đó, một lớp conv. 1×1 được áp dụng để ánh xạ tuyến tính 256 kênh thành một kênh duy nhất, tạo ra vector cục bộ vL∈RD.

Biểu diễn Tư thế. Chúng tôi sử dụng hình ảnh trực quan hóa RGB 3 kênh của tư thế ( 256×256×3), được thay đổi kích thước thành 224×224×3, làm đầu vào cho CANet. Ngược lại, PIDM (Bhunia et al., 2023) sử dụng biểu diễn tư thế phức tạp hơn với 256×256×20, trong đó 20 kênh là 3 kênh RGB và 17 bản đồ nhiệt Gaussian. Các thí nghiệm của chúng tôi chứng minh rằng việc sử dụng biểu diễn tư thế RGB đơn giản là đủ để tạo ra hình ảnh con người thỏa mãn. Chúng tôi sử dụng DINOv2-B đã được đào tạo trước để trích xuất các đặc trưng tư thế RGB, dẫn đến một CLS và 256 token bản vá, được nối thành một chuỗi gồm 257 ( TPF∈R257×D), sau đó được xử lý bởi một lớp tích chập 1×1 để ánh xạ 257 kênh thành 1 kênh, tạo ra vector vP∈RD.

Đặc trưng Hình ảnh Nguồn Toàn cục. Chúng tôi quan sát thấy rằng việc chỉ dựa vào các đặc trưng cục bộ từ hình ảnh nguồn không đủ cho Transformer để nắm bắt cả chi tiết và danh tính của con người. Như đã được chứng minh trong AnyDoor (Chen et al., 2023), các mô hình tự giám sát như các đặc trưng DINO có thể nắm bắt danh tính và chi tiết một cách hiệu quả. Do đó, chúng tôi sử dụng DINOv2-G để trích xuất token CLS và nối tất cả các token khác để tạo thành đặc trưng toàn cục ( GSIF∈R257×D). Đối với độ phân giải 256×256, chúng tôi thay đổi kích thước thành 224×224 như yêu cầu của DINO (trong đó cả chiều rộng và chiều cao đều chia hết cho 14) để có được một token CLS và 256 token bản vá. Thú vị là, đối với độ phân giải 512×512, chúng tôi thấy rằng việc sử dụng cùng độ phân giải 224×224 cho cả các đặc trưng tư thế và toàn cục khi trích xuất token DINO là hiệu quả, loại bỏ nhu cầu trích xuất các đặc trưng tư thế và toàn cục có kích thước 448×448 (gần với 512×512). Điều này giúp tiết kiệm bộ nhớ, vì chuỗi đặc trưng đầu ra của transformer DINO lớn hơn trong trường hợp này ( 32×32 = 1024 ) so với 16×16 = 256 token (đầu ra DINO cho hình ảnh 224×224). Phương pháp này khác với các khung dựa trên Unet như PIDM và PoCoLD, vốn cần kích thước 512×512 chính xác cho điều kiện tư thế. Cuối cùng, chúng tôi truyền chuỗi kết quả GSIF∈R257×D qua một lớp conv. 1×1 để có được vector toàn cục vG∈RD.

Tổng hợp. Chúng tôi có được vector cuối cùng bằng cách hoặc đơn giản sử dụng phép cộng hoặc nối chúng để có ba kênh và sử dụng phép tích chập 1×1 H để có được vector điều kiện thống nhất c, tức là c=H(vL, vP, vG)∈ RD. Chúng tôi thấy rằng một MLP cho FID tốt hơn một chút vì MLP này có thể tự động đặt trọng số cho từng điều kiện và được học trong quá trình lan truyền ngược. Chúng tôi chỉ ra rằng việc có cả vector cục bộ và toàn cục, tức là vL+vG của hình ảnh nguồn cùng với vP là quan trọng để tạo hình ảnh mục tiêu chất lượng cao trong phần khảo sát thực nghiệm.

3.3. Mạng Dự đoán Mặt nạ Liên kết
Gao et al. (2023) đã chứng minh cải thiện trong các mô hình khuếch tán dựa trên transformer bằng cách giới thiệu một bộ dự đoán nhẹ để điền vào các vùng có mặt nạ trong hình ảnh, dẫn đến điểm FID được nâng cao. Mặc dù chiến lược mặt nạ này tỏ ra hiệu quả trong các nhiệm vụ tạo hình ảnh chung như ImageNet, việc áp dụng nó cho bộ dữ liệu DeepFashion vẫn chưa được khám phá. Trong nhiệm vụ PHIG, việc chỉ dự đoán mặt nạ sử dụng các token không có mặt nạ trong một hình ảnh không đủ để nắm bắt sự tương ứng cần thiết giữa hình ảnh nguồn và mục tiêu, quan trọng cho việc tạo sinh có điều kiện. Kết quả là, phương pháp này mang lại hiệu suất không tối ưu trong việc tạo hình ảnh con người. Để giải quyết hạn chế này, chúng tôi đề xuất một module dự đoán mới tích hợp thông tin từ hình ảnh nguồn (đa góc nhìn) để hướng dẫn mô hình khuếch tán trong việc dự đoán các token có mặt nạ trong hình ảnh mục tiêu. Điều này trái ngược với việc MDT chỉ dựa vào hình ảnh mục tiêu. Bằng cách tích hợp thông tin từ hình ảnh tham chiếu, phương pháp của chúng tôi, MIPNet, có được các gợi ý ngữ cảnh phong phú hơn để hoàn thành các bản vá mặt nạ trong hình ảnh mục tiêu và học sự tương ứng ngữ nghĩa có ý nghĩa. Sự khác biệt chính giữa MDT và MIPNet được minh họa trong Hình 4. Trong phần khảo sát thực nghiệm, chúng tôi xác nhận hiệu quả của phương pháp chúng tôi so với dự đoán mặt nạ của MDT. Hàm mục tiêu mất mát cho việc đào tạo với các token có mặt nạ vẫn nhất quán với mất mát tiêu chuẩn Ldenoise .

Lmask=Ey,c,ϵ∼N(0,I),t∥ϵ−ϵθ(fθ(xs, ym), c, t)∥2,(2)

trong đó fθ là mạng chứa MIPNet gθ, các lớp mã hóa N1 và lớp giải mã N2 của TDNet. Ở đây, N1 và N2, cùng với các thiết lập khác, vẫn nhất quán với những gì được định nghĩa trong MDT (Gao et al., 2023). Đầu ra của MIPNet được tính bởi phương trình sau:

gθ(zxs, zym) =ϕattn(zym, zym, zym) +ϕattn(ϕattn(zym, zym, zym), zxs, zxs),(3)

trong đó ϕattn biểu thị cơ chế chú ý được đề xuất bởi (Vaswani et al., 2017) để học dự đoán mặt nạ đa góc nhìn:

ϕattn(Q,K,V) =Softmax 
QK⊤
√dk!
V (4)

Hàm mục tiêu cuối cùng. Chúng tôi tối ưu hóa đồng thời hai hàm mục tiêu song song thông qua phương trình sau:

Ltotal=Ldenoise +Lmask. (5)

Trong Phương trình 5, nếu Lmask được loại bỏ, khung tương tự như phong cách của DiT. Thay vào đó, việc thay thế gθ chỉ bằng tự chú ý sẽ biến đổi mô hình thành phong cách MDT. Trong quá trình suy luận, MIPNet được bỏ qua, chỉ giữ lại các nhúng vị trí, như được thực hiện trong MDT (Gao et al., 2023).

3.4. Hướng dẫn Không có Bộ phân loại
Chúng tôi sử dụng kỹ thuật phổ biến của hướng dẫn không có bộ phân loại (Ho & Salimans, 2022) để dự đoán nhiễu thông qua kết hợp tuyến tính của mô hình không điều kiện ϵθ(yt, t) và mô hình có điều kiện ϵθ(yt, c, t) như sau:

ˆϵθ(yt, x, t) =γtϵθ(yt, c, t) + (1 −γt)ϵθ(yt, t).(6)

Quy mô hướng dẫn γt được xác định tại thời điểm t. Trong quá trình đào tạo, chúng tôi ngẫu nhiên gán vector điều kiện thống nhất c∈RD, thu được bởi CANet, cho vector không ∅ ∈RD với xác suất η= 10% . Một tham số khác giúp hướng dẫn quy mô động là γt=1−cosπ(t T)α 2×γ. Điều này thiết lập lịch trình cosine-lũy thừa ( α) cho quy mô hướng dẫn trong quá trình lấy mẫu, như được sử dụng trong MDT (Gao et al., 2023). Theo mặc định, chúng tôi đặt γ= 2.0. Các thí nghiệm của chúng tôi chỉ ra rằng việc sử dụng α= 1.0 mang lại FID tốt nhất, mặc dù các giá trị thấp hơn một chút cho các chỉ số khác như SSIM và LPIPS. Ngược lại, α= 0.01 dẫn đến FID cao hơn một chút nhưng cải thiện SSIM và LPIPS.

4. Thí nghiệm
4.1. Chi tiết Thực hiện
Bộ dữ liệu. Chúng tôi đánh giá phương pháp của chúng tôi so với các phương pháp tiên tiến (SOTA) sử dụng hình ảnh độ phân giải cao từ bộ dữ liệu DeepFashion In-shop Clothes Retrieval Benchmark (Liu et al., 2016) ở độ phân giải 256×256 và 512×512. Bộ dữ liệu bao gồm các tập con đào tạo và kiểm tra không trùng lặp, chứa 101,966 và 8,570 cặp tương ứng. Chúng tôi áp dụng các bước tiền xử lý nhất quán với các nghiên cứu trước đây (Bhunia et al., 2023; Han et al., 2023). Chúng tôi sử dụng OpenPose (Cao et al., 2017) để trích xuất 18 điểm khóa từ hình ảnh của mỗi người và sau đó sử dụng OpenCV để tạo trực quan hóa RGB, được thay đổi kích thước thành 224×224. Kích thước cố định này được áp dụng cho hình ảnh tư thế trong cả trường hợp độ phân giải 256×256 và 512×512.

Chỉ số. Chúng tôi sử dụng các phép đo phổ biến như được sử dụng trong các nghiên cứu trước đây (Bhunia et al., 2023; Han et al., 2023) bao gồm FID, SSIM, LPIPS, và tùy chọn PSNR cho các khảo sát thực nghiệm.

Đào tạo. Chúng tôi tinh chỉnh V AE ft-MSE đã được đào tạo trước của Stable Diffusion trên tập đào tạo DeepFashion. Đối với hình ảnh 256×256, việc đào tạo được thực hiện trên một GPU A100 duy nhất (80GB RAM) với kích thước lô 32, kéo dài 800k bước. Trong khi đó, đối với hình ảnh 512×512, chúng tôi sử dụng hai GPU A100 với kích thước lô 10 (5 hình ảnh mỗi GPU), đào tạo trong 1M bước. Đối với các khảo sát thực nghiệm, chúng tôi đào tạo X-MDPT-B với 300k bước trên độ phân giải 256×256. Tốc độ học được đặt thành 1e-4, tỷ lệ EMA của mô hình thành 0.9999, và các thiết lập khác được căn chỉnh với DiT (Peebles & Xie, 2023). Lưu ý rằng, các hình ảnh gốc trong bộ dữ liệu DeepFashion có độ phân giải 256×176 và 512×352, mà chúng tôi đã thay đổi kích thước thành 256×256 và 512×512 tương ứng, sử dụng nội suy bicubic trước khi đưa chúng vào các mô hình.

4.2. Kết quả Chính
Kết quả được báo cáo trong Bảng 1, với hiệu suất so sánh của các phương pháp khác nhau. X-MDPT của chúng tôi thể hiện sự vượt trội nhất quán trên các chỉ số FID, SSIM, và LPIPS ở độ phân giải 256×256. Chúng tôi thấy rằng mô hình của chúng tôi đạt điểm FID tốt nhất, khoảng 6.25, trong giai đoạn giữa đào tạo (350-400k bước). Sau đó, với việc đào tạo kéo dài (800k-1M bước), FID ổn định ở khoảng 7.28, trong khi các chỉ số SSIM và LPIPS thể hiện sự cải thiện liên tục. Ở đây, quá trình đánh giá thực hiện so sánh giữa tập kiểm tra tổng hợp và dữ liệu đào tạo thực. Hình ảnh sự thật cơ bản mang lại FID 7.86, cho thấy khoảng cách phân phối đáng kể trong các tập kiểm tra và đào tạo của DeepFashion, như được xác nhận bởi PoCoLD (Han et al., 2023). Khi mô hình hội tụ hoàn toàn, chúng tôi quan sát thấy sự thu hẹp của FID, LPIPS, và SSIM khi mô hình học được tạo ra hình ảnh gần hơn với sự thật cơ bản. Ở độ phân giải cao hơn 512×512, X-MDPT thể hiện sự tụt hậu nhẹ so với PIDM trong FID, nhưng tốt hơn trong các chỉ số khóa khác. Hiệu quả tài nguyên của nó - kích thước lô 32 và 10 với một và hai GPU cho độ phân giải 256 và 512, cho thấy dư địa cải thiện so với 8 GPU của PIDM với kích thước lô 128.

Kết quả Định tính. Chúng tôi so sánh đầu ra được tạo bởi X-MDPT và các phương pháp khác trong Hình 7. Các hình ảnh con người được tạo ra thể hiện chất lượng cao trên nhiều tình huống khác nhau. Đáng chú ý, các mô hình khuếch tán dựa trên pixel như PIDM và các phương pháp dựa trên CNN khác thường gặp khó khăn trong việc nắm bắt trung thành các chi tiết phong cách phức tạp, dẫn đến các tạo tác đáng chú ý, như quan sát thấy trong các trường hợp cụ thể. Ngược lại, X-MDPT hoạt động trên không gian tiềm ẩn với một transformer được trang bị sơ đồ hiểu biết ngữ nghĩa, cho phép mô tả chính xác hơn các yếu tố quần áo như áo sơ mi và quần dài. Điều này dẫn đến các hình ảnh hoàn chỉnh và thỏa mãn hơn phù hợp với tư thế và hình ảnh nguồn dự định.

Các so sánh trực quan giữa của chúng tôi và các phương pháp hiện tại trong Hình 7 làm nổi bật việc X-MDPT tạo ra một cách nhất quán các hình ảnh mục tiêu hợp lý và hoàn chỉnh gần gũi với sự thật cơ bản. Hơn nữa, so với mô hình hiệu suất tốt nhất trước đây, PIDM, mô hình của chúng tôi thể hiện sự căn chỉnh vượt trội khi góc nhìn hình ảnh được thay đổi, như thể hiện trong Hình 2. Chúng tôi phân tích đặc tính này trong Hình 5.

4.3. Nghiên cứu Khảo sát
Học Các Góc nhìn Bất biến của Hình ảnh Nguồn. Đối với nhiệm vụ PHIG, với các góc nhìn khác nhau của hình ảnh một người và cùng một tư thế mục tiêu, chúng ta mong đợi tạo ra cùng một hình ảnh mục tiêu. Tuy nhiên, như được thể hiện trong Hình 2, PIDM không nắm bắt được cấu trúc cụ thể của quần áo và tạo ra các hình ảnh mục tiêu không nhất quán khi góc nhìn của người được thay đổi. Ngược lại, X-MDPT đưa ra các hình ảnh mục tiêu nhất quán và gần hơn với sự thật cơ bản. Việc tạo nhất quán này có thể được giải thích bằng cách đo độ tương tự cosine của các vector điều kiện thống nhất khi thay đổi góc nhìn của hình ảnh nguồn. Chúng tôi thấy rằng các nghiên cứu hiện tại như PoCoLD và PIDM sử dụng điều kiện ở các vị trí khác nhau và nhiều cấp trong mạng của họ, điều này làm cho việc tạo ra các vector thống nhất của tất cả các điều kiện (tức là tư thế và hình ảnh nguồn) trở nên thách thức. Ngược lại, module CANet của chúng tôi hỗ trợ tạo ra vector điều kiện thống nhất như vậy. Các vector này có thể dễ dàng hỗ trợ việc theo dõi điểm tương tự cosine của các góc nhìn khác nhau của cùng một người. Chúng tôi thực hiện toàn bộ tập kiểm tra DeepFashion và lấy trung bình. Hình 5 cho thấy độ tương tự đạt trên 99.99% sau 10k bước đào tạo, cho thấy CANet học để nắm bắt các đặc trưng bất biến của cùng một người. Điều này giải thích tại sao X-MDPT có thể tạo ra một mục tiêu nhất quán cho cùng một tư thế với các góc nhìn khác nhau của một người, điều này hữu ích cho nhiệm vụ.

Khả năng Mở rộng. Chúng tôi khám phá khả năng mở rộng của X-MDPT với kích thước S, B, và L trong Hình 6. Chúng tôi quan sát thấy FID, SSIM, LPIPS, và PSNR được cải thiện khi mở rộng kích thước mô hình, chứng minh tiềm năng của transformer cho vấn đề PHIG.

Tác động của Dự đoán Mặt nạ. Bảng 2a cho thấy việc áp dụng naïve transformer (DiT) (Peebles & Xie, 2023) cho nhiệm vụ PHIG không mang lại FID thỏa mãn. MDT (Gao et al., 2023) có thể giúp cải thiện FID nhưng không có lợi cho SSIM và LPIPS. Ngược lại, phương pháp của chúng tôi cải thiện tất cả các chỉ số. Về mặt định tính, hình ảnh được tạo bởi phương pháp của chúng tôi thực tế hơn và gần hơn với hình ảnh sự thật cơ bản như được thể hiện trong Hình 8. Điều này có thể được quy cho thực tế là sự tương ứng là chìa khóa để đạt được hiệu suất tốt nhất trong nhiệm vụ tạo hình ảnh con người có hướng dẫn tư thế, mà X-MDPT của chúng tôi được trang bị module Liên-Ngữ nghĩa được đề xuất (MIPNet) có thể nắm bắt các manh mối mạnh mẽ giữa hình ảnh nguồn và mục tiêu.

Thời gian Suy luận. Như được trình bày trong Bảng 2f, tất cả các biến thể của các mô hình của chúng tôi thể hiện tốc độ suy luận nhanh hơn đáng kể và yêu cầu ít tham số hơn so với đối thủ cạnh tranh, PIDM. Cụ thể, các mô hình X-MDPT-S, X-MDPT-B, và X-MDPT-L tăng tốc PIDM lần lượt 14.25×, 13.07×, và 5.43×. Lợi thế tốc độ này có thể được quy cho một số yếu tố. Thứ nhất, X-MDPT hoạt động trên các bản vá tiềm ẩn 32×32, trong khi PIDM làm việc trực tiếp trên không gian pixel 256×256. Ở đây, V AE trong X-MDPT hoạt động một lần chuyển tiếp duy nhất, trong khi PIDM cần 50 lần chuyển tiếp trong DDIM. Thứ hai, PIDM sử dụng hướng dẫn không có bộ phân loại tách biệt cho cả tư thế và nguồn, yêu cầu hai lần chuyển tiếp và dẫn đến 50× bước đánh giá nhiều hơn. Ngược lại, X-MDPT của chúng tôi sử dụng CFG cho một điều kiện thống nhất và chỉ cần một lần chuyển tiếp. Thời gian đào tạo có thể được tìm thấy trong Phụ lục do hạn chế về không gian, nơi phương pháp của chúng tôi chứng minh hiệu quả hơn đáng kể so với PIDM.

Các thành phần MIPNet. (a) Tỷ lệ Mặt nạ: Trong Bảng 2b, chúng tôi quan sát thấy tỷ lệ mặt nạ thấp hơn của hình ảnh mục tiêu trong MIPNet mang lại điểm FID tốt nhất, nhất quán với các phát hiện từ Gao et al. (2023) về MDT áp dụng cho ImageNet. Ngược lại, một tỷ lệ cao hơn, chẳng hạn như 70%, dẫn đến điểm SSIM và LPIPS tốt hơn. Tỷ lệ cao hơn này buộc mô hình ưu tiên tái tạo hơn là học biểu diễn ngữ nghĩa, điều này quan trọng cho việc tạo sinh hiệu quả.

(b) Chú ý cho MIPNet: Bảng 2d minh họa rằng thiết kế chú ý tự-chéo tạo ra điểm FID tốt nhất trong khi chỉ sử dụng tự chú ý (như MDT) mang lại FID tệ nhất. Sự khác biệt này làm nổi bật tầm quan trọng của dự đoán mặt nạ dựa trên tham chiếu trong việc nâng cao hiệu suất của việc tạo hình ảnh con người.

Các thành phần CANet. (a) Tổng hợp Điều kiện: Bảng 2e minh họa rằng việc chỉ sử dụng đặc trưng cục bộ vL, tức là đầu ra của V AE của hình ảnh nguồn, mang lại hiệu suất tệ nhất (trường hợp vL+vP). Ngược lại, việc chỉ dựa vào đặc trưng toàn cục vG (trường hợp vG+vP) dẫn đến cải thiện đáng kể. Tuy nhiên, phương pháp này thiếu thông tin cục bộ quan trọng cho việc tạo sinh, vì tiềm ẩn mục tiêu nhiễu của TDNet hoạt động ở cấp V AE. Lựa chọn tối ưu phát sinh từ việc kết hợp cả đặc trưng cục bộ và toàn cục, như được chứng minh trong trường hợp vL+vP+vG.

(b) Biểu diễn Toàn cục và Tư thế: AnyDoor (Chen et al., 2023) chứng minh rằng DINOv2-G giỏi trong việc nắm bắt chi tiết đối tượng. Trong Bảng 2c, chúng tôi quan sát thấy chất lượng đặc trưng toàn cục ảnh hưởng đáng kể đến hiệu suất, với các mô hình DINOv2 có năng lực cao hơn mang lại điểm FID tốt hơn. Đối với tư thế, chúng tôi thấy rằng DINOv2-B vượt trội so với DINOv2-S trong FID nhưng tụt hậu nhẹ trong SSIM và LPIPS. Chúng tôi chọn DINOv2-G cho đặc trưng toàn cục của xs và DINOv2-B cho tư thế yp làm mặc định. Tính đơn giản của hình ảnh tư thế cho phép một mô hình nhỏ hơn như DINOv2-B hướng dẫn TDNet một cách hiệu quả. Ngược lại, sự phức tạp của hình ảnh con người cần một biến thể mạnh mẽ hơn.

5. Thảo luận Thêm
Trong giai đoạn thảo luận, chúng tôi đã đi sâu vào các đặc tính bổ sung của phương pháp được đề xuất, khám phá một số phát hiện.

So sánh với Phương pháp Hiệu quả Trước đây: PoCoLD đã giới thiệu một khuếch tán tiềm ẩn hiệu quả hơn PIDM nhưng tụt hậu về FID. Vì mã của PoCoLD không hoàn chỉnh, chúng tôi đã thực hiện triển khai riêng. Bảng 3 cho thấy các biến thể của chúng tôi chạy nhanh hơn PoCoLD trên A100. Đáng chú ý, X-MDPT-S, với 11× ít tham số hơn, vượt trội so với PoCoLD về FID, tạo ra 8 hình ảnh chỉ trong 1 giây, nhanh hơn 3× so với PoCoLD. Tốc độ được nâng cao của phương pháp chúng tôi so với PoCoLD phát sinh từ việc PoCoLD sử dụng CFG tích lũy, một biến thể của Hướng dẫn Không có Bộ phân loại Tách biệt (D-CFG, tương tự PIDM), làm chậm đáng kể hoạt động của nó. Cụ thể, mỗi lần tạo trong PoCoLD yêu cầu tổng cộng 150 lần chuyển tiếp. Trong 50 bước khử nhiễu, một bước cần ba lần chuyển tiếp (không điều kiện, điều kiện-tư thế, điều kiện-hình ảnh-nguồn), dẫn đến tổng cộng 50×3 = 150. Ngược lại, X-MDPT sử dụng CFG tiêu chuẩn, chỉ yêu cầu 100 lần chuyển tiếp, trong đó mỗi bước khử nhiễu cần hai lần chuyển tiếp (không điều kiện, điều kiện thống nhất), tổng cộng 50×2 = 100, tiết kiệm 50 lần chuyển tiếp so với PoCoLD và PIDM.

Hiệu suất cho Độ phân giải Cao hơn. Trong Bảng 1, đối với bộ dữ liệu DeepFashion, X-MDPT cho thấy điểm FID thấp hơn một chút cho hình ảnh độ phân giải cao hơn so với độ phân giải thấp hơn, nhất quán với PIDM và NTED nhưng không với PoCoLD. Chúng tôi đề xuất rằng khung dựa trên Transformer của chúng tôi có thể nắm bắt chi tiết tinh tế hơn và thông tin ngữ cảnh trong hình ảnh độ phân giải cao hơn so với PoCoLD, vốn dựa vào Unet. Ngược lại, trên bộ dữ liệu ImageNet, hành vi của Diffusion Transformer (bài báo DiT) ngược lại với hiệu suất của nó trên DeepFashion. Chúng tôi tin rằng những khác biệt này đáng được khám phá có hệ thống về sự đa dạng của bộ dữ liệu và kiến trúc mô hình (CNN, Transformer, v.v.) để có hiểu biết kết luận.

Hiểu biết về việc Thêm MIPNet. Chúng tôi tích hợp MIPNet vào xương sống Transformer trong quá trình đào tạo để hỗ trợ mô hình tận dụng thông tin ngữ cảnh giữa các bản vá trong một hình ảnh (thông qua tự chú ý) và học sự tương ứng giữa hình ảnh nguồn và mục tiêu (thông qua chú ý chéo). Điều này nâng cao năng lực học của Transformer một cách nhanh chóng hơn. Hiểu biết này được củng cố bởi Bảng 2(a), nơi MIPNet cải thiện đáng kể hiệu suất so với việc chỉ sử dụng Transformer. Hiệu quả của mất mát mặt nạ, được đào tạo đồng thời với mất mát chính, đã được quan sát thấy trong các nghiên cứu trước đây như iBOT (Zhou et al., 2022a) và PatchMAE (Zhang et al., 2023b) trong lĩnh vực học biểu diễn tự giám sát.

Khái quát hóa. Chúng tôi đã thực hiện đánh giá các hình ảnh trong tự nhiên để chứng minh khả năng khái quát hóa của phương pháp chúng tôi. Chúng tôi sử dụng mô hình X-MDPT-L sẵn sàng của chúng tôi được đào tạo trên bộ dữ liệu DeepFashion. Chúng tôi so sánh với phương pháp tốt thứ hai PIDM sử dụng checkpoint đã được công bố của họ. Chúng tôi thử nghiệm với các trường hợp thách thức hơn như người có làn da tối hơn (vì DeepFashion hầu hết dữ liệu được thu thập với người da trắng, một vài cho người da đen) và hình ảnh nền phức tạp. Hình 9 chứng minh rằng X-MDPT ổn định hơn nhiều so với phương pháp đứng thứ hai PIDM, làm nổi bật các đặc tính phân biệt của các transformer khuếch tán trong nhiệm vụ này. Chúng tôi tin rằng một trong những lý do PIDM gặp khó khăn từ việc quá khớp nghiêm trọng hơn là vì nó sử dụng các đặc trưng của tư thế và hình ảnh nguồn được đào tạo từ đầu, trong khi phương pháp của chúng tôi sử dụng mô hình đã được đào tạo trước DINO mang lại các đặc trưng tốt hơn nhiều cho cả tư thế và hình ảnh nguồn. Đào tạo trên nhiều dữ liệu trong tự nhiên hơn có thể là giải pháp để giảm thiểu việc quá khớp (đối với nền, danh tính) của chúng tôi và PIDM.

Các trường hợp Trùng lặp. Sự trùng lặp hiếm khi xảy ra, nhưng trong DeepFashion, chúng tôi đã phát hiện một số cảnh trùng lặp, đáng chú ý, một cặp dữ liệu trùng lặp với ID 00005136 và 00005188. Chi tiết thêm có thể được tìm thấy trong Phụ lục. Khi thử nghiệm trên các mẫu trùng lặp này, các phương pháp khác không thể tái tạo hình ảnh mục tiêu. Tại sao X-MDPT có thể tái tạo hoàn toàn các mẫu đã thấy trước đây trong khi các mô hình khác không thể? Chúng tôi quy năng lực này cho Diffusion Transformer, vốn tạo sinh dựa trên các quan sát mẫu đào tạo hiệu quả hơn so với các thuật toán dựa trên CNN Unet trước đây.

6. Kết luận
Trong bài báo này, chúng tôi trình bày X-MDPT, một mô hình tạo sinh khuếch tán có mặt nạ mới cho việc tạo hình ảnh con người có hướng dẫn tư thế (PHIG). Khác với các phương pháp trước đây sử dụng Unet để khử nhiễu khuếch tán, X-MDPT sử dụng Transformer trên các bản vá tiềm ẩn. Phân tích của chúng tôi cho thấy X-MDPT đạt được độ tương tự 99.99% trong việc tạo các vector bất biến theo góc nhìn, đảm bảo hình ảnh mục tiêu nhất quán trên các tư thế. Các thí nghiệm rộng rãi chứng minh hiệu quả của X-MDPT trong việc tạo ra hình ảnh chất lượng cao, độ phân giải cao, vượt trội so với các phương pháp hiện tại về tốc độ suy luận và thiết lập một tiêu chuẩn mới cho PHIG trên benchmark DeepFashion.

Tuyên bố Tác động
Phương pháp của chúng tôi thành thạo trong việc tạo ra hình ảnh chất lượng cao có các cá nhân ở nhiều tư thế khác nhau, sử dụng hình ảnh của bất kỳ người nào làm điểm tham chiếu. Mặc dù nó mang lại nhiều lợi thế, bao gồm việc tạo hình ảnh nhanh chóng, có nguy cơ lạm dụng, chẳng hạn như tạo nội dung lừa đảo cho mục đích gian lận, một vấn đề được ghi nhận rõ ràng trong tổng hợp hình ảnh. Chúng tôi cam kết thực hiện các biện pháp để điều chỉnh quyền truy cập, từ đó ngăn chặn lạm dụng và đảm bảo công nghệ góp phần vào phúc lợi cộng đồng một cách an toàn.

Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi khoản tài trợ của Viện Lập kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2021-0-01381, Phát triển AI Nhân quả thông qua Hiểu biết Video và Học Tăng cường, và Các Ứng dụng của nó trong Môi trường Thực tế) và một phần được hỗ trợ bởi khoản tài trợ IITP được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2022-0-00184, Phát triển và Nghiên cứu Công nghệ AI để Tuân thủ Chính sách Đạo đức Đang phát triển một cách Không tốn kém).

[Các tham chiếu được liệt kê trong 12 trang cuối]

--- TRANG 13 ---
A. Phụ lục
A.1. So sánh với những hình ảnh tốt nhất được báo cáo trước đây
Để hoàn chỉnh hơn, trong Hình 10 chúng tôi cũng so sánh những hình ảnh tốt nhất được tạo ra như đã báo cáo trong các bài báo trước để tham khảo. Mô hình của chúng tôi liên tục tạo ra đầu ra tương đương hoặc tốt hơn.

A.2. Tạo sinh cho cùng tư thế và các góc nhìn nguồn khác nhau và ngược lại
Trong Hình 11, chúng tôi cung cấp các ví dụ toàn diện hơn để so sánh khi tất cả các phương pháp tiên tiến thực hiện tạo sinh với tư thế mục tiêu + các góc nhìn khác nhau của hình ảnh nguồn cùng một người và ngược lại. Như thể hiện trong Hình 11, trong trường hợp cho cùng một tư thế mục tiêu, khi thay đổi các góc nhìn nguồn của người đó, tất cả các phương pháp hiện tại đều không thể nắm bắt hình ảnh mục tiêu nhất quán, trong khi X-MDPT được đề xuất của chúng tôi xử lý rất tốt. Sự vượt trội của mô hình chúng tôi cũng được chứng minh khi chúng tôi giữ hình ảnh nguồn không đổi và tạo các hình ảnh mục tiêu khác nhau. Hình 12 cho thấy nhiều hình ảnh được tạo bởi X-MDPT-L của chúng tôi cho những người khác nhau với các góc nhìn khác nhau khi tạo cùng một tư thế mục tiêu.

A.3. Chi tiết thêm về thiết lập thí nghiệm
Chúng tôi sử dụng 50 bước DDIM (Song et al., 2020) cho suy luận giống như PIDM. Chi tiết cho ba biến thể X-MDPT-S, B, và L được cung cấp trong Bảng 4 và Hình 13. Đối với V AE, chúng tôi chỉ tinh chỉnh bộ giải mã sử dụng V AE của Stable Diffusion (Rombach et al., 2022) trên dữ liệu đào tạo DeepFashion trong 77 epoch. Khuôn mặt bị biến dạng nếu không tinh chỉnh.

A.4. Các trường hợp thất bại
Chúng tôi lưu ý rằng các phương pháp của chúng tôi thất bại trong một số trường hợp có thể làm giảm hiệu suất tốt nhất trên tập kiểm tra với SSIM, và LPIPS, như thể hiện trong Hình 14. Thứ nhất, thay đổi quần áo của hình ảnh mục tiêu có thể dẫn đến điểm định lượng tệ hơn. Thứ hai, tư thế bị thiếu biểu diễn, ví dụ, tay trái của người phụ nữ bị thiếu trong hình ảnh tư thế khiến mô hình dự đoán mà không có phần đó. Thứ ba, tư thế dự đoán sai đến từ OpenPose (Cao et al., 2017) có thể khiến mô hình dự đoán mục tiêu sai. Tăng thêm các cặp đào tạo về thay đổi quần áo và cải thiện thuật toán phát hiện tư thế có thể giải quyết những thất bại này.

A.5. Các hạt giống ngẫu nhiên khác nhau
Trong Hình 15 chúng tôi cho thấy một số mẫu được tạo bởi X-MDPT-L cho sáu hạt giống khác nhau: 0, 100, 200, 300, 400, 500. Như thể hiện trong hình, phương pháp của chúng tôi tạo ra hình ảnh mục tiêu khá ổn định, chứng minh rằng nó không nhạy cảm với các hạt giống ngẫu nhiên.

A.6. Thời gian đào tạo
Chúng tôi so sánh thời gian đào tạo của phương pháp chúng tôi và đối thủ cạnh tranh PIDM (Bhunia et al., 2023). Chúng tôi tham khảo DreamPose (Karras et al., 2023) cho thấy PIDM sử dụng 4 GPU A100 đào tạo trong 26 ngày (phần 2.3 của Dreampose), dẫn đến nếu sử dụng cùng tài nguyên như chúng tôi, tức là chỉ 1 GPU A100, sẽ cần 104 ngày cho PIDM. Trong khi phương pháp của chúng tôi đào tạo trong 800k bước (với một GPU duy nhất) mất 15.7 ngày. Tuy nhiên, việc đào tạo phương pháp của chúng tôi sử dụng một GPU duy nhất chỉ với 300k bước (5.9 ngày cho X-MDPT-L, và 4.1 ngày cho X-MDPT-B) đã tạo ra hình ảnh chất lượng rất tốt, trong khi PIDM (cần 2/3 đào tạo của họ để có được mô hình có thể tạo hình ảnh đủ tốt, tức là sẽ mất khoảng 69.3 ngày).

A.7. Các Mô hình Học Tự Giám sát
Có nhiều mô hình SSL khác nhau đã được khám phá để học các biểu diễn mà không cần nhãn (He et al., 2022; Pham et al., 2021; 2023; Oquab et al., 2023; Zhang et al., 2022a;b). Các mô hình này phục vụ như một bộ trích xuất tốt cho nhiều ứng dụng khác nhau (Pham et al., 2022b; Chen et al., 2023). DINOv2 (Oquab et al., 2023) đã chứng minh một mô hình đã được đào tạo trước xuất sắc cho nhiều khung dựa trên khuếch tán khác nhau. Chúng tôi chủ yếu sử dụng DINOv2, nhưng các tùy chọn khác có thể đáng thử. Với tiềm năng của các transformer khuếch tán cho việc học có điều kiện, dự kiến sẽ có thêm khám phá về khả năng của nó trong nhiều lĩnh vực và ứng dụng khác nhau như xử lý giọng nói (Jung et al., 2022; 2020; Trung & Yoo, 2019), tăng cường dữ liệu (Lee et al., 2020), VQA (Kim et al., 2020), học phát hiện thị giác (Vu et al., 2019), siêu phân giải (Niu et al., 2023; 2024a;b).

A.8. Trùng lặp trong Bộ dữ liệu DeepFashion
Chúng tôi minh họa một số trường hợp trùng lặp trong Hình 16 và Hình 17, với trường hợp sao chép hiếm hoi được thể hiện trong Hình 18.

A.9. Hình ảnh độ phân giải cao 512×512
Chúng tôi cũng báo cáo một số hình ảnh độ phân giải cao 512×512 được tạo bởi phương pháp của chúng tôi trong Hình 19, Hình 20, và Hình 21, v.v... Mô hình X-MDPT-L được đào tạo trên bộ dữ liệu DeepFashion cho độ phân giải cao và tạo ra hình ảnh mục tiêu xuất sắc.

A.10. Thêm so sánh định tính trên 256×176
Thêm kết quả trực quan ở độ phân giải 256×176 được cung cấp bên dưới từ Hình 24 trở đi. Đối với nhiều trường hợp khó khăn như tư thế hiếm và nhìn gần hình ảnh nguồn, các phương pháp khác không thể tạo ra mục tiêu chính xác, trong khi X-MDPT có thể xử lý chúng một cách thỏa đáng.
