# 2311.18257.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2311.18257.pdf
# File size: 18616789 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Diffusion Models Without Attention
Jing Nathan Yan1∗, Jiatao Gu2∗, Alexander M. Rush1
1Cornell University,2Apple
{jy858, arush}@cornell.edu, jgu32@apple.com
Figure 1. Selected samples generated by class-conditional D IFFU SSM trained on ImageNet 256 ×256 and 512 ×512 resolutions.
Abstract
In recent advancements in high-fidelity image generation,
Denoising Diffusion Probabilistic Models (DDPMs) have
emerged as a key player. However, their application at high
resolutions presents significant computational challenges.
Current methods, such as patchifying, expedite processes in
UNet and Transformer architectures but at the expense of rep-
resentational capacity. Addressing this, we introduce the Dif-
fusion State Space Model ( DIFFU SSM ), an architecture that
supplants attention mechanisms with a more scalable state
space model backbone. This approach effectively handles
higher resolutions without resorting to global compression,
thus preserving detailed image representation throughout
the diffusion process. Our focus on FLOP-efficient architec-
tures in diffusion training marks a significant step forward.
Comprehensive evaluations on both ImageNet and LSUN
*Equal contribution.datasets at two resolutions demonstrate that DiffuSSMs are
on par or even outperform existing diffusion models with
attention modules in FID and Inception Score metrics while
significantly reducing total FLOP usage.
1. Introduction
Rapid progress in image generation has been driven by de-
noising diffusion probabilistic models (DDPMs) [ 7,20,37].
DDPMs pose the generative process as iteratively denoising
latent variables, yielding high-fidelity samples when enough
denoising steps are taken. Their ability to capture complex
visual distributions makes DDPMs promising for advancing
high-resolution, photorealistic synthesis.
However, significant computational challenges remain
in scaling DDPMs to higher resolutions. A major bottle-
neck is the reliance on self-attention [ 62] for high-fidelity
generation. In U-Nets architectures, this bottleneck comesarXiv:2311.18257v1  [cs.CV]  30 Nov 2023

--- PAGE 2 ---
Figure 2. Architecture of DIFFU SSM .DIFFU SSM takes a noised image representation which can be a noised latent from a variational
encoder, flattens it to a sequence, and applies repeated layers alternating long-range SSM cores with hour-glass feed-forward networks.
Unlike with U-Nets or Transformers, there is no application of patchification or scaling for the long-range block.
from combining ResNet [ 17] with attention layers [ 48,61].
DDPMs surpass generative adversarial networks (GANs) but
require multi-head attention layers [ 7,37]. In Transformer
architectures [ 62], attention is the central component, and is
therefore critical for achieving recent state-of-the-art image
synthesis results [ 1,38]. In both these architectures, the com-
plexity of attention, quadratic in length, becomes prohibitive
when working with high-resolution images.
Computational costs have motivated the use of repre-
sentation compression methods. High-resolution architec-
tures generally employ patchifying [ 1,38], or multi-scale
resolution[ 20,22,37]. Patchifying creates coarse-grained
representations which reduces computation at the cost of
degraded critical high-frequency spatial information and
structural integrity [ 1,38,51]. Multi-scale resolution, while
alleviating computation at attention layers, can diminish spa-
tial details through downsampling [ 68] and can introduce
artifacts [65] while applying up-sampling.
The Diffusion State Space Model ( DIFFU SSM ), is an
attention-free diffusion architecture, shown in Figure 2, that
aims to circumvent the issues of applying attention for high-
resolution image synthesis. DIFFU SSM utilizes a gated
state space model (SSM) backbone in the diffusion process.
Previous work has shown that sequence models based on
SSMs are an effective and efficient general-purpose neural
sequence model [ 14]. By using this architecture, we can
enable the SSM core to process finer-grained image repre-
sentations by removing global patchification or multi-scale
layers. To further improve efficiency, DIFFU SSM employsan hourglass architecture for the dense components of the
network. Together these approaches target the asymptotic
complexity of length as well as the practical efficiency in the
position-wise portion of the network.
We validate DIFFU SSM ’s across different resolutions.
Experiments on ImageNet demonstrate consistent improve-
ments in FID, sFID, and Inception Score over existing ap-
proaches in various resolutions with fewer total Gflops.
2. Related Work
Diffusion Models Denoising Diffusion Probabilistic Mod-
els (DDPMs) [ 20,22,37,54] are an advancement in the
diffusion models family. Previously, Generative Adversarial
Networks (GANs) [ 12] were preferred for generation tasks.
Diffusion and score-based generative models [ 24,56–59]
have shown considerable improvements, especially in image
generation tasks [ 44–46]. Key enhancements in DDPMs
have been largely driven by improved sampling methodolo-
gies [ 20,28,37], and the incorporation of classifier-free
guidance [ 19]. Additionally, Song et al. [55] has proposed
a faster sampling procedure known as Denoising Diffusion
Implicit Model(DDIM). Latent space modeling is another
core technique in deep generative models. Variational au-
toencoders (V AEs) [ 30] pioneered learning latent spaces
with encoder-decoder architectures for reconstruction. A
similar compression idea was applied in diffusion models
as the recent Latent Diffusion Models (LDMs) [ 45] held
state-of-the-art sample quality by training deep generative
models to invert a noise corruption process in a latent space
2

--- PAGE 3 ---
when it was first proposed. Additionally, recent approaches
also developed masked training procedures, augmenting the
denoising training objectives with masked token reconstruc-
tion [ 10,69]. Our work is fundamentally built upon existing
DDPMs, particularly the classifier-free guidance paradigm.
Architectures for Diffusion Models Early diffusion mod-
els utilized U-Net style architectures[ 7,20]. Subsequent
works enhanced U-Nets with techniques like more layers of
attention layers at multi-scale resolution level [ 7,37], resid-
ual connections [ 2], and normalization [ 40,66]. However,
U-Nets face challenges in scaling to high resolutions due
to the growing computational costs of the attention mech-
anism [ 52]. Recently, vision transformers (ViT) [ 8] have
emerged as an alternate architecture given their strong scal-
ing properties and long-range modeling capabilities prov-
ing that convolution inductive bias is not always necessary.
Diffusion transformers [ 1,38] demonstrated promising re-
sults. Other hybrid CNN-transformer architectures were
proposed [ 32] to improve training stability. Our work aligns
with the exploration of sequence models and related design
choices to generate high-quality images but focuses on a
complete attention-free architecture.
Efficient Long Range Sequence Architectures The stan-
dard transformer architecture employs attention to com-
prehend the interaction of each individual token within a
sequence. However, it encounters challenges when mod-
eling extensive sequences due to the quadratic computa-
tional requirement. Several attention approximation meth-
ods [ 23,33,53,60,64] have been introduced to approximate
self-attention within sub-quadratic space. Mega[ 34] com-
bines exponential moving average with a simplified attention
unit, surpassing the performance of transformer baselines.
Venturing beyond the traditional transformer architectures,
researchers are also exploring alternate models adept at han-
dling elongated sequences. State space models (SSM)-based
architectures[ 14–16] have yielded significant advancements
over contemporary state-of-the-art methods on the LRA and
audio benchmark[ 11]. Furthermore, Dao et al. [5], Peng
et al. [39], Poli et al. [42], Qin et al. [43] have substanti-
ated the potential of non-attention architectures in attaining
commendable performance in language modeling. Our work
draws inspiration from this evolving trend of diverting from
attention-centric designs and predominantly utilizes the back-
bone of SSM.
3. Preliminaries
3.1. Diffusion Models
Denoising Diffusion Probabilistic Model (DDPM) [ 20] is a
type of generative models that samples images by iteratively
denoising a noise input. It starts from a stochastic process
where an initial image x0is gradually corrupted by noise,transforming it into a simpler, noise-dominated state. This
forward noising process can be represented as follows:
q(x1:T|x0) =TY
t=1q(xt|xt−1), (1)
q(xt|x0) =N(xt;√¯αtx0,(1−¯αt)I), (2)
where x1:Tdenotes a sequence of noised images from time
t= 1tot=T. Then, DDPM learns the reverse process that
recovers the original image utilizing learned µθandΣθ:
pθ(xt−1|xt) =N(xt−1;µθ(xt),Σθ(xt)), (3)
where θthe parameters of the denoiser, and are trained
to maximize the variational lower bound [ 54] on the log-
likelihood of the observed data x0:max θ−logpθ(x0|x1)+P
tDKL(q∗(xt−1|xt, x0)||pθ(xt−1|xt)).To simplify the
training process, researchers reparameterize µθas a func-
tion of the predicted noise εθand minimize the mean
squared error between εθ(xt)and the true Gaussian noise εt:
minθ||εθ(xt)−εt||2
2.However, to train a diffusion model
that can learn a variable reverse process covariance Σθ, we
need to optimize the full L. In this work, we follow DiT [ 38]
to train the network where we use the simple objective to
train the noise prediction network εθand use the full objec-
tive to train the covariance prediction network Σθ. After
training is done, we follow the stochastic sampling process
to generate images from the learned εθandΣθ.
3.2. Architectures for Diffusion Models
We review methods for parameterizing µθwhich maps
RH×W×C→RH×W×Cwhere H, W, C are the height,
width, and size of the data. For image generation tasks,
they can either raw pixels, or some latent space representa-
tions extracted from a pre-trained V AE encoder [ 45]. When
generating high-resolution images, even in the latent space,
HandWare large, and require specialized architectures for
this function to be tractable.
U-Nets with Self-attention U-Net architectures [ 20,22,
37] uses convolutions and sub-sampling at multiple reso-
lutions to handle high-resolution inputs, where additional
self-attention layers are used at each low-resolution blocks.
To the best of our knowledge, no U-Net-based diffusion mod-
els are achieving state-of-the-art performance without using
self-attention. Let t1, . . . t Tbe a series of lower-resolution
feature maps created by down-sampling the image.1At
each scale a ResNet [ 17] is applied to RHt×Wt×Ct. These
are then upsampled and combined into the final output. To
enhance the performance of U-Net in image generation, at-
tention layers are integrated at the lowest-resolutions. The
feature map is flattened to a sequence of HtWtvectors. For
1Note that choices of up- and down-scale include learned parameters
and non-parameterized ones such as average pooling and upscale [4, 22].
3

--- PAGE 4 ---
instance, when considering H= 256 ×W= 256 down to
attention layers of 16×16and32×32, leading to sequences
of length 256and1024 respectively. Applying attention
earlier improves accuracy at a larger computational cost.
More recently, [ 22,41] have shown that using more self-
attention layers in the low-resolution is the key of scaling
high-resolution U-Net-based diffusion models.
Transformers with Patchification As mentioned above,
the global contextualization using self-attention is the key
for diffusion models to perform well. Therefore, it is also
natrual to consider architecture fully based on self-attention.
Transformer architectures utilize attention throughout, but
handle high-resolution images through patchification [ 8].
Given a patch size P, the transformer partitions the image
intoP×Ppatches yielding a new RH/P×W/P×C′repre-
sentation. This patch size Pdirectly influences the effective
granularity of the image and downstream computational de-
mands. To feed patches into a Transformer, the image is
flattened and a linear embedding layer is applied to obtain a
sequence of (HW)/P2hidden vectors [ 1,8,22,38]. Due
to this embedding step, which projects from C′to the model
size, large patches risk loss of spatial details and ineffectively
model local relationships due to reduced overlap. However,
patchification has the benefit of reducing the quadratic cost
of attention as well as the feed-forward networks in the
Transformer.
4. D IFFU SSM
Our goal is to design a diffusion architecture that learns
long-range interactions at high-resolution without requiring
“length reduction” like patchification. Similar to DiT, the
approach works by flattening the image and treating it like a
sequence modeling problem. However, unlike Transformers,
this approach uses sub-quadratic computation in the length
of this sequence.
4.1. State Space Models (SSMs)
SSMs are a class of architectures for processing discrete-
time sequences [14]. The models behave like a linear recur-
rent neural network (RNN) processing an input sequence
of scalars u1, . . . u Lto output y1, . . . y Lwith the following
equation,
xk=Axk−1+Buk, yk=Cxk.
WhereA∈RN×N,B∈RN×1,C∈R1×N. The main
benefit of this approach, compared to alternative architec-
tures such as Transformers and standard RNNs, is that the
linear structure allows it to be implemented using a long con-
volution as opposed to a recurrence. Specifically, ycan be
computed from uwith an FFT yielding O(LlogL)complex-
ity, allowing it to be applied to significantly longer sequences.When handling vector inputs, we can stack Ddifferent SSMs
and apply a Dbatched FFTs.
However a linear RNN, by itself, is not an effective se-
quence model. The key insight from past work is that if
the discrete-time values A,B,Care derived from appro-
priate continuous-time state-space models, the linear RNN
approach can be made stable and effective [ 13]. We therefore
learn a continuous-time SSM parameterization A,B,Cas
well as a discretization rate ∆, which is used to produce the
necessary discrete-time parameters. Original versions of this
conversion were challenging to implement, however recently
researchers [ 15,16] have introduced simplified diagonalized
versions of SSM neural networks that achieve comparable
results with a simple approximation of the continuous-time
parameterization. We use one of these, S4D [ 15], as our
backbone model.
Just as with standard RNNs, SSMs can be made bidirec-
tional by concatenating the outputs of two SSM layers and
passing them through an MLP to yield a L×2Doutput. In
addition, past work shows that this layer can be combined
with multiplicative gating to produce an improved Bidirec-
tional SSM layer [ 35,63] as part of the encoder, which is
the motivation for our architecture.
4.2. DIFFU SSM Block
The central component of our DIFFU SSM is a gated bidi-
rectional SSM, aimed at optimizing the handling of long
sequences. To enhance efficiency, we incorporate hour-
glss architectures within MLP layers. This design alter-
nates between expanding and contracting sequence lengths
around the Bidirectional SSMs, while specifically reducing
sequence length in MLPs. The complete model architecture
is shown in Figure 2.
Specifically, each hourglass layer receives a shortened,
flattened input sequence I∈RJ×Dwhere M=L/J is the
downscale and upscale ratio. At the same time, the entire
block including the bidirectional SSMs is computed in the
original length to fully leverage the global contexts. We use
σto denote activation functions. We compute the following
forl∈ {1. . . L}withj=⌊l/M⌋, m=lmodM, D m=
2D/M .
Ul=σ(W↑
kσ(W0Ij) ∈RL×D
Y=Bidirectional-SSM (U) ∈RL×2D
I′
j,Dmk:Dm(k+1)=σ(W↓
kYl) ∈RJ×2D
Oj=W3(σ(W2I′
j)⊙σ(W1Ij)) ∈RJ×D
We integrate this Gated SSM block in each layer with a skip
connection. Additionally, following past work we integrate
a combination of the class label y∈RL×1and timestep
t∈RL×1at each position, as illustrated in Figure 2.
4

--- PAGE 5 ---
Figure 3. Comparison of Gflops of DiT and DIFFU SSM under
various model architecture. DiT with patching (P=2) scales well
to longer sequences, however when patching is removed it scales
poorly even with hourglass (M=2). DIFFU SSM scales well, and
hourglass (M=2) can be used to reduce absolute Gflops.
Parameters The number of parameters in the DIFFUSSM
block is dominated by the linear transforms, W, these con-
tain9D2+ 2MD2parameters. With M= 2 this yields
13D2parameters. The DiT transformer block has 12D2
parameters in its core transformer layer; however, the DiT
architecture has more parameters in other layer components
(adaptive layer norm). We match parameters in experiments
by using an additional D IFFU SSM layer.
FLOPs Figure 3 compares the Gflops between DiT and
DIFFU SSM . The total Flops in one layer of DIFFU SSM is
13L
MD2+LD2+α2LlogLDwhere αrepresents a constant
for the FFT implementation. With M= 2and noting that
the linear layers dominate computation, this yields roughly
7.5LD2Gflops. In comparison, if instead of using SSM,
we had used self-attention at full length with this hourglass
architecture, we would have 2DL2additional Flops.
Considering our two experimental scenarios: 1) D≈
L= 1024 which would have given 2LD2extra Flops, 2)
4D≈L= 4096 which would give 8LD2Flops and signifi-
cantly increase cost. As the core cost at Bidirectional SSM is
small compared to that using attention, and as a result using
hourglass architecture will not work for attention-based mod-
els. DiT avoids these issues by using patching as discussed
earlier, at the cost of representational compression.
5. Experimental Studies
5.1. Experimental Setup
Datasets Our primary experiments are conducted on
ImageNet[ 6]2and LSUN[ 67]3. Specifically, we used the
2https://image-net.org/download.php
3https://www.yf.io/p/lsunImageNet-1k dataset where there are 1.28million images
and1000 classes of objects. For the LSUN-dataset, we
choose two categories: Church (126k images) and Bed (3M
images), and train separate unconditional models for them.
Our experiments are conducted with the ImageNet dataset at
256×256and512×512resolution, and LSUN at 256×256
resolution. We use latent space encoding[ 45] which gives
effective sizes 32×32and64×64withL= 1024 and
L= 4096 respectively. We also include pixel-space Ima-
geNet at 128×128resolution in our supplementary materials
where L= 16,384.
Linear Decoding and Weight Initialization After the
final block of the Gated SSM, the model decodes the sequen-
tial image representation to the original spatial dimensions
to output noise prediction and diagonal covariance predic-
tion. Similar to Gao et al. [10], Peebles and Xie [38], we use
a linear decoder and then rearrange the representations to
obtain the original dimensionality. We follow DiT to use the
standard layer initializations approach from ViT [8].
Training Configuration We followed the same training
recipe from DiT [ 38] to maintain an identical setting across
all models. We also chose to follow existing literature to keep
an exponential moving average (EMA) of model weights
with a constant decay. Off-the-shelf V AE encoders from4
were used, with parameters fixed during training. Our DIF-
FUSSM -XL possesses approximately 673M parameters and
encompasses 29 layers of Bidirectional Gated SSM blocks
with a model size D= 1152 . This value is similar to DiT-
XL. trained our model using a mixed-precision training ap-
proach to mitigate computational costs. We adhere to the
identical configuration of diffusion as outlined in ADM [7],
including their linear variance scheduling, time and class
label embeddings, as well as their parameterization of co-
variance Σθ. More details can be found in the Appendix.
For unconditional image generation, DiT does not report
results and we were unable to compare with DiT in the same
training setting. Our objective instead compares DIFFUSSM ,
with a training regimen comparable to taht of LDM[ 45] that
can generate high-quality images for categories in the LSUN
dataset. To adapt the model to an unconditional context, we
have removed the class label embedding.
Metrics To quantify the performance of image generation
of our model, we used Frechet Inception Distance(FID) [ 18],
a common metric measuring the quality of generated im-
ages. We followed convention when comparing against prior
works and reported FID-50K using 250 DDPM sampling
steps. We also reported sFID score [ 36], which is designed
to be more robust to spatial distortions in the generated im-
ages. For a more comprehensive insight, we also presented
4https://github.com/CompVis/stable-diffusion
5

--- PAGE 6 ---
Figure 4. Uncurated samples from the D IFFU SSM models trained from various datasets.
the Inception Score [ 47] and Precision/Recall [ 31] as supple-
mentary metrics. Note that do not incorporate classifier-free
guidance unless explicitly mentioned(we used −Gfor the
usage of classifier-free guidance or explicitly state the CFG).
Implementation and Hardware We implemented all
models in Pytorch and trained them using NVIDIA A100.
DIFFU SSM -XL, our most compute-intensive model trains
on 8 A100 GPUs 80GB with a global batch size of 256.
More computation details and speed can be found in the
supplementary materials.
5.2. Baselines
We compare to a set of previous best models, these include:
GAN-style approaches that previously achieved state-of-the-
art results, UNet-architectures trained with pixel space rep-
resentations, and Transformers operating in the latent space.
More details can be found in Table 5.3. Our aim is to com-
pare, through a similar denoising process, the performance of
our model with respect to other baselines. Some recent stud-
ies [10,69] focusing on image generation at the 256×256
resolution level have combined masked token prediction with
existing DDPM training objectives to advance the state of
the art. However, these works are orthogonal to our primary
comparison, so we have not included them in Table 1. For
LSUN datasets, we found existing DDPM-based methodsare not surpassing GAN-based methods. Our goal is to com-
pare within the DDPM framework instead of competing with
state-of-the-art methods.
5.3. Experimental Results
Class-Conditional Image Generation We compare DIF-
FUSSM with state-of-the-art class-conditional generative
models, as depicted in Table 1. When classifier-free guid-
ance is not employed, DIFFU SSM outperforms other diffu-
sion models in both FID and sFID, reducing the best score
from the previous non-classifier-free latent diffusion mod-
els from 9.62to9.07, while utilizing ∼3×fewer training
steps. In terms of Total Gflops of training, our uncompressed
model yields a 20% reduction of the total Gflops compared
with DiT. When classifier-free guidance is incorporated, our
models attain the best sFID score among all DDPM-based
models, exceeding other state-of-the-art strategies, demon-
strating the images generated by DIFFU SSM are more ro-
bust to spatial distortion. As for FID score, DIFFU SSM
surpasses all models when using classifier-free guidance,
and maintains a pretty small gap ( 0.01) against DiT. Note
thatDIFFUSSM trained with 30% fewer total Gflops already
surpasses DiT when no classifier-free guidance is applied.
U-ViT [ 1] is another transformer-based architecture but uses
a UNet-based architecture with long-skip connections be-
tween blocks. U-ViT used fewer FLOPs and yielded better
6

--- PAGE 7 ---
ImageNet 256×256 Benchmark
Models Total Total FID↓sFID↓ IS↑ P↑ R↑
Images(M) Gflops
BigGAN-deep [2] - - 6.95 7.36 171.40 0.87 0.28
MaskGIT [3] 355 - 6.18 - 182.1 0.80 0.51
StyleGAN-XL [50] - - 2.30 4.02 265.12 0.78 0.53
ADM [7] 507 5.68×101210.94 6.02 100.98 0.69 0.63
ADM-U [7] 507 3.76×10117.49 5.13 127.49 0.72 0.63
CDM [21] - - 4.88 - 158.71 - -
LDM-8 [45] 307 1.75×101015.51 - 79.03 0.65 0.63
LDM-4 [45] 213 2.22×101010.56 - 103.49 0.71 0.62
DiT-XL/2 [38] 1792 2.13×10119.62 6.85 121.50 0.67 0.67
DIFFU SSM-XL 660 1.85×10119.07 5.52 118.32 0.69 0.64
Classifier-free Guidance
ADM-G 507 5.68×10114.59 5.25 186.70 0.82 0.52
ADM-G, ADM-U 507 3.76×10123.60 - 247.67 0.87 0.48
LDM-8-G 307 1.75×10107.76 - 209.52 0.84 0.35
LDM-4-G 213 2.22×10103.95 - 178.2 2 0.81 0.55
U-ViT-H/2-G [1] 512 6.81×10102.29 - 247.67 0.87 0.48
DiT-XL/2-G 1792 2.13×10112.27 4.60 278.24 0.83 0.57
DIFFU SSM-XL -G 660 1.85×10112.28 4.49 259.13 0.86 0.56
ImageNet 512×512 Benchmark
ADM 1385 5.97×101123.24 10.19 58.06 0.73 0.60
ADM-U 1385 3.9×10129.96 5.62 121.78 0.75 0.64
ADM-G 1385 5.97×10117.72 6.57 172.71 0.87 0.42
ADM-G, ADM-U 1385 4.5×10123.85 5.86 221.72 0.84 0.53
U-ViT/2-G 512 6.81×10104.05 8.44 261.13 0.84 0.48
DiT-XL/2-G 768 4.03×10113.04 5.02 240.82 0.84 0.54
DIFFU SSM-XL-G 302 3.22×10113.41 5.84 255.06 0.85 0.49
Table 1. Class conditional image generation quality evaluation of DIFFU SSM and existing approaches on ImageNet 256 ×256. Reported
results from other cited papers with their #trained images. Total images by training steps ×batch size as reported, and total Gflops by Total
Images ×GFlops/Image. P refers to Precision and R refers to Recall. −Gdenotes the results with classifier-free guidance.
performance at a 256 ×256 resolution, but this is not the
case for the 512 ×512 dataset. As our major comparison is
against DiT, we do not adopt this long-skip connection for
a fair comparison. We acknowledge that adapting U-Vit’s
idea might benefit both DiT and DIFFU SSM . We leave this
consideration for future work.
We further compare on a higher-resolution benchmark
using classifier-free guidance. Results from DIFFUSSM here
are relatively strong and near some of the state-of-the-art
high-resolution models, beating all models but DiT on sFID
and achieving comparable FID scores. The DIFFUSSM was
trained on 302M images, seeing 40% as many images and
using 25% fewer Gflops as DiT.Unconditional Image Generation We compare the un-
conditional image generation ability of our model against ex-
isting baselines. Results are shown in Table 2. Our findings
indicate that DIFFU SSM achieves comparable FID scores
obtained by LDM (with −0.08and0.07gap) with a compa-
rable training budget. This result highlights the applicability
ofDIFFU SSM across different benchmarks and different
tasks. Similar to LDM, our approach doesn’t outperform
ADM for LSUN-Bedrooms as we are only using 25% of the
total training budget as ADM. For this task, the best GAN
models outperform diffusion as a model class.
6. Analysis
Additional Images Additional images generated by DIF-
FUSSM are included from Figure 7 to Figure 14.
7

--- PAGE 8 ---
Figure 5. Qualititative studies of patching and down/up scale of DIFFU SSM .Prefers to the patchfication, Mrefers to the down/up scale
ratio.P= 1is the case where there is not patchfication and M= 1is the case where there is no down/up scale.
Models LSUN-Church LSUN-Bedroom
FID↓ P↑ FID↓ P↑
ImageBART [9] 7.32 5.51 -
PGGAN [25] 6.42 - - -
StyleGAN [26] 4.21 - 2.35 0.59
StyleGAN2 [27] 3.93 0.39 - -
ProjGAN [49] 1.59 0.61 1.52 0.61
DDPM [20] 7.89 - 4.90 -
UDM [29] - - 4.57 -
ADM [7] - - 1.90 0.66
LDM [45] 4.02 0.64 2.95 0.66
DIFFU SSM 3.94 0.64 3.02 0.62
Table 2. Unconditional image generation evaluation of DIFFUSSM
and exsiting approaches on LSUN-Church and LSUN-Bedroom at
256×256.
Model Scaling We trained three different DIFFU SSM
sizes to calibrate the performance yielded by scaling up
the model. We calculate the FID-50k for their checkpoints
of the first 400k steps. Results are shown in Figure 6 (Left).
We find that similar to DiT models, large models use FLOPs
more efficient and scaling the DIFFU SSM will improve the
FID at all stages of training.
Impact of Hourglass We trained our model with different
sampling settings to assess the impact of compression in
latent space: using a downsampling ratio M= 2(our regular
model), and another with P= 2applying a patch size equal
to 2, similar to what DiT has done. We calculated their
FID-50k for the first 400k steps and plotted it on a log scale.
Results are shown in Figure 6 (Right). We find that our
model yields a better FID score compared to when patching
is applied, and the gap between the two also widens as the
Figure 6. Ablation studies. Left: DIFFUSSM with different hidden
dimension sizes D( −S/D = 384 ,−L/D = 786 ,−XL/D =
1152 ). Right: FID score of DIFFU SSM with different patch size
(P= 2) and downsample ratio ( M= 1).
number of training steps increases. This suggests that the
compression of information might hurt the model’s ability
of generating high-quality images.
Qualitative Analysis The objective of DIFFU SSM is to
avoid compressing hidden representations. To test whether
this is beneficial we compare three variants of DIFFU SSM
with different downscale ratio Mand patch size P. We train
all three model variants for 400K steps with the same batch
size and other hyperparameters. When generating images,
we use identical initial noise and noise schedules across class
labels. Results are presented in Figure 5. Notably, eliminat-
ing patching enhances robustness in spatial reconstruction
at the same training stages. This results in improved vi-
sual quality, comparable to uncompressed models, but with
reduced computation.
8

--- PAGE 9 ---
7. Conclusion
We introduce DIFFUSSM , an architecture for diffusion mod-
els that does not require the use of Attention. This approach
can handle long-ranged hidden states without requiring rep-
resentation compression. Results show that this architecture
can achieve better performance than DiT models utilizing
less Gflops at 256x256 and competitive results at higher-
resolution even with less training. The work has a few re-
maining limitations. First, it focuses on (un)conditional im-
age generation as opposed to full text-to-image approaches.
Additionally, there are some recent approaches such as
masked image training that may improve the model. Still
this model provides an alternative approach for learning ef-
fective diffusion models at large scale. We believe removing
the attention bottleneck should open up the possibility of
applications in other areas that requires long-range diffusion,
for example high-fidelity audio, video, or 3D modeling.
References
[1]Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,
Hang Su, and Jun Zhu. All are worth words: A vit backbone
for diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22669–22679, 2023.
[2]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018.
[3]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T
Freeman. Maskgit: Masked generative image transformer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11315–11325, 2022.
[4]Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
and Mubarak Shah. Diffusion models in vision: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2023.
[5]Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri
Rudra, and Christopher Ré. Hungry hungry hippos: Towards
language modeling with state space models. arXiv preprint
arXiv:2212.14052 , 2022.
[6]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[7]Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural information
processing systems , 34:8780–8794, 2021.
[8]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[9]Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn
Ommer. Imagebart: Bidirectional context with multinomialdiffusion for autoregressive image synthesis. Advances in
neural information processing systems , 34:3518–3532, 2021.
[10] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng
Yan. Masked diffusion transformer is a strong image synthe-
sizer. arXiv preprint arXiv:2303.14389 , 2023.
[11] Karan Goel, Albert Gu, Chris Donahue, and Christopher
Ré. It’s raw! audio generation with state-space models. In
International Conference on Machine Learning , pages 7616–
7633. PMLR, 2022.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014.
[13] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christo-
pher Ré. Hippo: Recurrent memory with optimal polynomial
projections. Advances in neural information processing sys-
tems, 33:1474–1487, 2020.
[14] Albert Gu, Karan Goel, and Christopher Ré. Efficiently mod-
eling long sequences with structured state spaces. arXiv
preprint arXiv:2111.00396 , 2021.
[15] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On
the parameterization and initialization of diagonal state space
models. Advances in Neural Information Processing Systems ,
35:35971–35983, 2022.
[16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state
spaces are as effective as structured state spaces. Advances
in Neural Information Processing Systems , 35:22982–22994,
2022.
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016.
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium.
Advances in neural information processing systems , 30, 2017.
[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022.
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020.
[21] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. The Journal of
Machine Learning Research , 23(1):2249–2281, 2022.
[22] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple
diffusion: End-to-end diffusion for high resolution images.
arXiv preprint arXiv:2301.11093 , 2023.
[23] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Trans-
former quality in linear time. In International Conference on
Machine Learning , pages 9099–9117. PMLR, 2022.
[24] Aapo Hyvärinen and Peter Dayan. Estimation of non-
normalized statistical models by score matching. Journal
of Machine Learning Research , 6(4), 2005.
[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196 , 2017.
9

--- PAGE 10 ---
Figure 7. Samples from the D IFFU SSM models on ImageNet 256 ×256.
10

--- PAGE 11 ---
Figure 8. Samples from the D IFFU SSM models on ImageNet 256 ×256.
11

--- PAGE 12 ---
Figure 9. Samples from the D IFFU SSM models on ImageNet 256 ×256.
12

--- PAGE 13 ---
Figure 10. Samples from the D IFFU SSM models on ImageNet 256 ×256.
[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 4401–4410, 2019.
[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recog-
nition , pages 8110–8119, 2020.
[28] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. Advances in Neural Information Processing Systems ,
35:26565–26577, 2022.
[29] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo
Kang, and Il-Chul Moon. Soft truncation: A universal training
technique of score-based diffusion model for high precision
score estimation. arXiv preprint arXiv:2106.05527 , 2021.
[30] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013.
[31] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and recall
metric for assessing generative models. Advances in Neural
Information Processing Systems , 32, 2019.
[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021.
[33] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,
Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linearunified nested attention. Advances in Neural Information
Processing Systems , 34:2441–2453, 2021.
[34] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,
Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettle-
moyer. Mega: moving average equipped gated attention.
arXiv preprint arXiv:2209.10655 , 2022.
[35] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam
Neyshabur. Long range language modeling via gated state
spaces. arXiv preprint arXiv:2206.13947 , 2022.
[36] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W
Battaglia. Generating images with sparse representations.
arXiv preprint arXiv:2103.03841 , 2021.
[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
Conference on Machine Learning , pages 8162–8171. PMLR,
2021.
[38] William Peebles and Saining Xie. Scalable diffusion models
with transformers. arXiv preprint arXiv:2212.09748 , 2022.
[39] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,
Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,
Matteo Grella, Kranthi Kiran GV , et al. Rwkv: Reinventing
rnns for the transformer era. arXiv preprint arXiv:2305.13048 ,
2023.
[40] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with
a general conditioning layer. In Proceedings of the AAAI
conference on artificial intelligence , 2018.
[41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.
13

--- PAGE 14 ---
Figure 11. Samples from the D IFFU SSM models on ImageNet 512 ×512.
Sdxl: Improving latent diffusion models for high-resolution
image synthesis. arXiv preprint arXiv:2307.01952 , 2023.
[42] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu,
Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,
and Christopher Ré. Hyena hierarchy: Towards larger convo-
lutional language models. arXiv preprint arXiv:2302.10866 ,
2023.
[43] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically
gated recurrent neural network for sequence modeling. arXiv
preprint arXiv:2311.04823 , 2023.
[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022.[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022.
[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael
Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-
torealistic text-to-image diffusion models with deep language
understanding. Advances in Neural Information Processing
Systems , 35:36479–36494, 2022.
[47] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in neural information processing
14

--- PAGE 15 ---
systems , 29, 2016.
[48] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P
Kingma. Pixelcnn++: Improving the pixelcnn with dis-
cretized logistic mixture likelihood and other modifications.
arXiv preprint arXiv:1701.05517 , 2017.
[49] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger.
Projected gans converge faster. Advances in Neural Informa-
tion Processing Systems , 34:17480–17492, 2021.
[50] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-
xl: Scaling stylegan to large diverse datasets. In ACM SIG-
GRAPH 2022 conference proceedings , pages 1–10, 2022.
[51] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and
Kristian Kersting. Safe latent diffusion: Mitigating inappro-
priate degeneration in diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 22522–22531, 2023.
[52] Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen
Basri, and Yuval Kluger. Spectralnet: Spectral clustering
using deep neural networks. arXiv preprint arXiv:1801.01587 ,
2018.
[53] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and
Hongsheng Li. Efficient attention: Attention with linear com-
plexities. In Proceedings of the IEEE/CVF winter conference
on applications of computer vision , pages 3531–3539, 2021.
[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265. PMLR, 2015.
[55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. arXiv preprint arXiv:2010.02502 ,
2020.
[56] Yang Song and Prafulla Dhariwal. Improved tech-
niques for training consistency models. arXiv preprint
arXiv:2310.14189 , 2023.
[57] Yang Song and Stefano Ermon. Generative modeling by
estimating gradients of the data distribution. Advances in
neural information processing systems , 32, 2019.
[58] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equations.
arXiv preprint arXiv:2011.13456 , 2020.
[59] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
Consistency models. arXiv preprint arXiv:2303.01469 , 2023.
[60] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng
Juan. Sparse sinkhorn attention. In International Conference
on Machine Learning , pages 9438–9447. PMLR, 2020.
[61] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,
Oriol Vinyals, Alex Graves, et al. Conditional image genera-
tion with pixelcnn decoders. Advances in neural information
processing systems , 29, 2016.
[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017.
[63] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexan-
der M Rush. Pretraining without attention. arXiv preprint
arXiv:2212.10544 , 2022.[64] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020.
[65] Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep learning
for image super-resolution: A survey. IEEE transactions on
pattern analysis and machine intelligence , 43(10):3365–3387,
2020.
[66] Yuxin Wu and Kaiming He. Group normalization. In Proceed-
ings of the European conference on computer vision (ECCV) ,
pages 3–19, 2018.
[67] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. Lsun: Construction of a
large-scale image dataset using deep learning with humans in
the loop. arXiv preprint arXiv:1506.03365 , 2015.
[68] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 14821–14831, 2021.
[69] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anand-
kumar. Fast training of diffusion models with masked trans-
formers. arXiv preprint arXiv:2306.09305 , 2023.
15

--- PAGE 16 ---
Figure 12. Samples from the D IFFU SSM models on ImageNet 512 ×512.
16

--- PAGE 17 ---
Figure 13. Samples from the D IFFU SSM models on ImageNet 512 ×512.
17

--- PAGE 18 ---
Figure 14. Samples from the D IFFU SSM models on ImageNet 512 ×512.
18
