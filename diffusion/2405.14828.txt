# 2405.14828.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2405.14828.pdf
# File size: 17584490 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Good Seed Makes a Good Crop:
Discovering Secret Seeds in Text-to-Image Diffusion Models
Katherine Xu
University of PennsylvaniaLingzhi Zhang
Adobe Inc.Jianbo Shi
University of Pennsylvania
(Best FID)
Seed469(Worst FID)
Seed696(Grayscale)
Seed12(Border)
Seed16(White Sky)
Seed133(Good)
Seed493(T extArtifacts)
Seed857Text-to-Image Text-based Inpainting
“… elephants 
standing 
together in
a grassy 
field …”
“… two young 
men playing 
a game of 
frisbee …”
“… woman 
sitting on 
the floor in 
a room, using 
a laptop …”
“… giraffes 
standing 
next to each 
other …”
Figure 1. Left: Our study reveals that the seed number influences various visual elements in text-to-image generation, such as image
quality and style. Right: Certain seeds result in more inserted text in text-based inpainting tasks like object removal.
Abstract
Recent advances in text-to-image (T2I) diffusion models
have facilitated creative and photorealistic image synthesis.
By varying the random seeds, we can generate many images
for a fixed text prompt. Technically, the seed controls the
initial noise and, in multi-step diffusion inference, the noise
used for reparameterization at intermediate timesteps in the
reverse diffusion process. However, the specific impact of
the random seed on the generated images remains relatively
unexplored. In this work, we conduct a large-scale scien-
tific study into the impact of random seeds during diffu-
sion inference. Remarkably, we reveal that the best ‘golden’
seed achieved an impressive FID of 21.60, compared to the
worst ‘inferior’ seed’s FID of 31.97. Additionally, a classi-
fier can predict the seed number used to generate an image
with over 99.9% accuracy in just a few epochs, establishing
that seeds are highly distinguishable based on generated
images. Encouraged by these findings, we examined the
influence of seeds on interpretable visual dimensions. We
find that certain seeds consistently produce grayscale im-
ages, prominent sky regions, or image borders. Seeds alsoaffect image composition, including object location, size,
and depth. Moreover, by leveraging these ‘golden’ seeds,
we demonstrate improved image generation such as high-
fidelity inference and diversified sampling. Our investiga-
tion extends to inpainting tasks, where we uncover some
seeds that tend to insert unwanted text artifacts. Overall,
our extensive analyses highlight the importance of selecting
good seeds and offer practical utility for image generation.
1. Introduction
Text-to-Image (T2I) diffusion models [2, 3, 6, 32, 36, 37,
57] have advanced image synthesis significantly, enabling
the creation of photorealistic, high-resolution images. But,
their training requires substantial compute, limiting such re-
search to a few well-equipped labs. Despite these limita-
tions, many studies have enhanced image generation during
inference by feature re-weighting [44], gradient-based guid-
ance [10, 43, 49], or fusion with multimodal LLMs [5, 55].
In this work, we propose an inference technique to en-
hance image generation by exploring ‘secret seeds’ in the
reverse diffusion process. Inspired by prior research [33],arXiv:2405.14828v2  [cs.CV]  16 Apr 2025

--- PAGE 2 ---
which revealed that well-chosen neural network initializa-
tion seeds can outperform poorly chosen ones in image
classification, we investigate whether ‘golden’ or ‘inferior’
seeds similarly impact image quality in T2I diffusion in-
ference. Surprisingly, using the pretrained T2I model Sta-
ble Diffusion (SD) 2.0 [37] across 1,024 seeds, we discov-
ered that the best ‘golden’ seed achieved an FID [16, 41]
of21.60 , whereas the worst ‘inferior’ seed only reached an
FID of 31.97 —a significant difference within the commu-
nity. This finding sparked our curiosity to understand sev-
eral scientific questions: What does the seed control in T2I
diffusion inference? Why are random seeds so impactful?
Can seeds be distinguished by the images they generate?
Do they control interpretable image dimensions, and if so,
how can this be leveraged to enhance image generation?
To address our research questions, we first examined
how random seeds control the initial noisy latent and the
Gaussian noise during the reparameterization step of each
intermediate timestep in the reverse latent diffusion process,
as detailed in Sec. 3.1. We also developed a dataset using
two T2I diffusion models: the conventional multi-step SD
2.0 [37] and the distilled one-step SDXL Turbo [39]. This
dataset includes over 22,000 diverse text prompts and, us-
ing 1,024 unique fixed seeds for each combination of model
and prompt, resulted in approximately 46 million images as
discussed in Sec. 3.2. Our initial objective was to deter-
mine whether each random seed encodes unique character-
istics identifiable in the generated images. To test this, we
trained a 1,024-way classifier to predict the seed number
used during diffusion inference from the generated images
across diverse prompts. Remarkably, this classifier reached
over 99.9% validation accuracy after just six epochs, a stark
contrast to the random guessing chance of approximately
0.01%, establishing that seeds are highly distinguishable
based on the generated images as shown in Sec. 3.3.
Having confirmed seed distinguishability, we aim to un-
derstand if there are interpretable perceptual dimensions en-
abling this differentiation. Our next step involves designing
a pipeline to extract style and layout representations, ap-
ply dimensionality reduction [1, 50] for visible clustering,
and identify consistent patterns across seeds, regardless of
the input prompts. For example, certain seeds consistently
produce ‘grayscale’ images, others generate images with
prominent white ‘sky’ regions at the top, and some seeds
create image borders or insert ‘text’ during inpainting mode.
In terms of image layout, various seeds consistently influ-
ence the main subject’s scale, location, and depth within
images. The details on these findings are in Sec. 3.4.
Building on these discoveries from our seed analysis, we
propose several downstream applications to enhance im-
age generation, as detailed in Sec. 4. First, by identify-
ing ‘golden’ seeds across a variety of prompts, we can limit
sampling to the top-K seeds for high-fidelity inference. Thisapproach demonstrates superior quantitative performance,
as measured by FID [41] and HPS v2 [53], compared to ran-
dom sampling in the default implementation. Second, our
findings indicate that certain seeds capture distinct styles or
layout compositions. By leveraging this knowledge, we can
implement diversified sampling based on style or layout, of-
fering users varied results. Lastly, our studies on image in-
painting reveal that some seeds consistently generate ‘text
artifacts’ instead of completing pixels, indicating that one
could improve inpainting quality by using seeds that min-
imize these artifacts. Note that for all these applications,
we only need to perform the seed analysis once per model,
and our approach can be easily integrated into the inference
process without adding any computational overhead, unlike
most optimization-based approaches .
We summarize our contributions as follows:
• We present the first large-scale seed analysis for text-
to-image diffusion models and have constructed a
dataset comprising over 46 million images generated
from both a multi-step and a one-step diffusion mod-
els, across a diverse range of text prompts.
• We discovered that seeds encode highly discriminative
information, enabling a classifier to predict the source
seed from 1,024 possible seeds with 99.9% validation
accuracy using only the generated image as input.
• We found that seeds significantly influence image
quality, style, layout composition, and the generation
of ‘text artifacts’ across various prompts.
• Capitalizing on our insights, we propose applications
that enhance high-fidelity or diversified inference for
T2I models, and improve generation quality by avoid-
ing ‘text artifacts’ in text-based inpainting models.
2. Related Work
Stochasticity in deep learning models. Prior works
[4, 18, 30, 33, 40] have primarily examined the stochasticity
in neural network training caused by randomly initialized
weights, random data ordering, and stochastic optimization.
Notably, Picard [33] identified a significant difference of
1.82% in test accuracy on CIFAR-10 [21] between the best
and worst seeds, highlighting the considerable impact of the
seed on model performance. Inspired by these findings, we
explore the randomness within the reverse diffusion process
of T2I diffusion models.
Impact of diffusion model inputs. The main sources
of variation in images produced by pretrained text-to-image
diffusion models [2, 3, 6, 32, 36, 37, 57] are the text prompt
and the random seed that controls the initial noise. Conse-
quently, carefully selecting these model inputs can enhance
image generation and editing during inference without re-
quiring additional model training or fine-tuning. Several
studies [31, 48, 52, 56] have focused on understanding the
impact of text embeddings on the generated image or lever-
aging these text embeddings for tuning-free image genera-

--- PAGE 3 ---
Reparameterize Reparameterize
xT xT-1 xt
N(0, 1) N(0, 1)
Diffusion
U-NetDiffusion
U-Net
… …
0
 10
 0 1
 0 1
1 01
 0 1
 0 1
Swap Seed During Reverse Diffusion
Seedcontrol control
control
No swap Early timestep Mid timestep Late timestepFigure 2. Left: Overview of how the seed controls the initial noise xTand intermediate xtvia the sampled noise in multi-step diffusion
inference. Right: We swap the seed number at early, mid, and late timesteps of the reverse diffusion process, showing an example with
seeds 0 and 1. Interestingly, the seed mostly influences the initial noisy latent, rather than intermediate timesteps.
tion. Yu et al. [56] discovered that the CLIP [35] text em-
bedding commonly used in T2I diffusion models contains
diverse semantic directions that facilitate controllable im-
age editing. Furthermore, recent works [15,28,29,34] have
shown that the initial noise can lead to certain image genera-
tion tendencies. Po-Yuan et al. [34] demonstrated that slight
perturbations to the initial noise can greatly alter the gener-
ated samples of a diffusion model, and Grimal et al. studied
the effect of seeds on text-image alignment [14]. However,
the extent to which the initial noise affects various visual di-
mensions of the image remains unclear. Therefore, we con-
duct an extensive analysis of the influence of random seeds
on the generated image’s quality, human preference align-
ment, style, composition, and insertion of ‘text artifacts.’
Optimizing initial noise in diffusion models. Given
the significant impact of the seed on images from T2I dif-
fusion models, previous works [7,15,28,29,38] have aimed
to optimize the initial noise to produce images that better
align with a text prompt, reduce visual artifacts, or achieve
a desired layout. Mao et al. [29] found that certain patches
of initial noise are more likely to denoise into specific con-
cepts, enabling them to approach image editing by simply
substituting regions of the initial noise without fine-tuning
or disrupting the reverse diffusion process. While their
work focuses on a local analysis of the initial noise, our re-
search provides a large-scale study of the random seeds that
control the initial noise across a diverse set of text prompts.
3. Understanding Diffusion Seeds
3.1. What do seeds control in the reverse diffusion
process?
Random seeds play different roles in deep learning de-
pending on the context. During deep network training, they
often influence the initialization of neural network weights,
data scheduling, augmentation strategies, and stochastic
regularization techniques such as dropout [47]. In this work,
we aim to understand what the seeds control in the reverse
diffusion process and during diffusion inference.
We focus on latent diffusion models as described by
Rombach et al. [37], but the same principles apply to pixel
diffusion models. Theoretically, in the traditional multi-stepreverse diffusion process, both the initial noisy latent vari-
ables and the noise used for reparameterization [19] at each
timestep are sampled from a Gaussian distribution, intro-
ducing randomness. We show this process on the left side
of Fig. 2. At the implementation level, we confirmed that
random seeds are inputs to compute these variables [51]. In
a distilled one-step diffusion model, such as SDXL Turbo
[39], the random seeds only determine the initial noisy la-
tent, as there are no intermediate denoising steps.
In multi-step diffusion inference, seeds determine both
the initial latent variables and the reparameterization noise
at each timestep. To understand the separate impacts of
the initial latent configuration and the reparameterization
step on the generated images, we conducted a simple ”seed
swap” study shown on the right side of Fig. 2 using the
DDIM scheduler [46] with 40 inference steps. In our study,
we first set the seed to iand begin the reverse diffusion pro-
cess. Then, at an intermediate timestep, we change the seed
tojand complete the image generation process. We ex-
plore using seeds 0 and 1 for both iandj, as well as swap-
ping the seed at early, mid, and late timesteps of the reverse
diffusion process. Despite these variations, we found that
the initial noisy latent significantly controls the generated
content, while the random noise introduced at intermediate
reparameterization steps has no visible impact on the gen-
erated images, as shown on the right side of Fig. 2.
3.2. Data Generation
To conduct seed analysis at large scale, we employ three
types of prompts for text-to-image (T2I) generation, as
shown in Fig. 3. First, we capture a broad spectrum of natu-
ral visual content by sampling 20,000 images from the MS-
COCO 2017 train set [25] and generate dense captions us-
ing LLaV A 1.5 [26]. Second, we utilize 1,632 prompts from
the PartiPrompts benchmark [57], which includes short and
long general-purpose user prompts. Lastly, to enable more
controlled scientific studies, we create synthetic prompts by
combining 40 object categories with 22 modifiers, resulting
in 880 unique combinations.
For each prompt in our dataset, we sample 1,024 seeds
ranging from 0 to 1,023 and generate images using two

--- PAGE 4 ---
•The image depicts a group of people gathered around a dining table, enjoying a meal together. The table is filled with various food items, including a plate of pastries, a bowl of doughnuts, and a bowl of fruit. There are also several cups and a bottle on the table, indicating that the guests are drinking beverages. In addition to the food and drinks, there are a couple of spoons placed on the table, possibly for serving the dishes. The people are seated on chairs surrounding the table, engaged in conversation and enjoying the company of one another.•…•air•fire•a fire hydrant•a wooden posta photograph of a squirrel holding an arrow above its head and holding a longbow in its left hand•An empty fireplace with a television above it. The TV shows a lion hugging a giraffe.•an invisible man wearing horn-rimmed glasses and a pearl bead necklase while looking at his phone•Portrait of a gecko wearing a train conductor’s hat and holding a flag that has a yin-yang symbol on it. Woodcut.•…•A red truck•A wooden truck•A rough truck•A shiny truck•…•A dark bench•A round bench•A wooden bench•A intricate bench•….LLaVA Dense Caption on MS-COCO ImagesPartiBenchmarkSynthetic PromptFigure 3. A visualization of three different types of text prompts used in our study.
T2I models, SD 2.0 [37] and SDXL Turbo [39], for a
large-scale seed analysis. Specifically, we assign the seed
via torch.Generator(“cuda”).manual seed(seed) and use a
DDIM scheduler [46] for SD 2.0. This results in a to-
tal number of 22,512 prompts ×1,024 seeds×2models
= 46,104,576images. Beyond text-to-image applications,
we curated 500 pairs of images and masks for diffusion in-
painting models based on Open Images [20, 22], where the
mask typically covers an object in the original image. For
the text prompts, we use “clear background” to simulate the
object removal case and the original object category for the
object completion case, where the details are in Sec. 4.3.
3.3. How discriminative are seeds based on their
generated images?
As an initial experiment, we examine whether seeds can
be distinguished by their generated images. We train a
1,024-way classifier to predict the seed number used to
produce an image, employing 9,000 training, 1,000 valida-
tion, and 1,000 test images per seed. Remarkably, seeds
are highly differentiable based on their images. After only
six epochs, our classifier trained on images from SD 2.0
[37] achieved a test accuracy of 99.99%, and the classifier
trained on images from SDXL Turbo [39] reached a test ac-
curacy of 99.96%. However, it is unclear what makes seeds
easily discernible, as the Grad-CAM [13, 42] visualization
in Fig. 4 is not clearly interpretable. These findings suggest
that seeds may encode unique visual features; thus, we ex-
plore their impact across several interpretable dimensions.
Seed 0Seed 4
“… a table with a variety of decorative items …”Seed 1Seed 2Seed 3“… close-up of an orange tabby cat …”“… a sandwich with banana slices …”
Figure 4. Grad-CAM [13, 42] of our classifier trained to predict
the seed used to create an image.
3.4. Impact of Seeds on Interpretable Dimensions
In Sec. 3.3, we observed that a classifier trained to pre-
dict the seed used to generate an image achieves over 99.9%
accuracy in just a few epochs of training. However, it re-
mains unclear what aspects of the generated images en-
able these seeds to be highly distinguishable. Therefore,we present an extensive empirical study on the influence of
seed number on interpretable visual dimensions.
Image Quality and Human Preference Alignment. As
mentioned in Sec. 3.2, we used 20,000 prompts from MS-
COCO dense captions [25,26]. For each prompt, we gener-
ated images using 1,024 seeds. To evaluate the image qual-
ity associated with each seed, we selected 10,000 prompts
and their corresponding generated images, and then com-
puted the FID score [16,41] against 10,000 real MS-COCO
images [25]. Surprisingly, we observed a significant differ-
ence in FID scores between the best and worst seeds. For in-
stance, the ‘golden’ seed 469 for SD 2.0 achieved a low FID
of 21.60, while the ‘inferior’ seed 696 scored 31.97—a dis-
parity considered significant within the community. Addi-
tionally, we assess the seeds using HPS v2 [53], a new met-
ric trained on large-scale human preference pairs to quantify
human preferences for AI-generated images. For each seed,
we sampled 1,000 prompts and their corresponding images
to calculate HPS v2. As shown in Fig. 5, the top and bot-
tom three seeds according to FID and HPS v2 indeed reveal
that the highest-rated seeds produce images that are more
visually pleasing and aligned with human preferences.
Next, we determine whether these seed rankings are gen-
eralizable across a different set of 10,000 prompts for FID
and 1,000 prompts for HPS v2. In Fig. 6 and 7, we plot
the ranked seeds for FID and HPS v2 using images from
SD 2.0 and SDXL Turbo. We compare scores from the first
set of prompts (“Prompt Set 1”) against scores from another
set of prompts (“Prompt Set 2”). We reveal a high degree
of overlap between the seed patterns for quality and human
preference. This consistency underpins our proposed en-
hancements to inference strategies in Sec. 4.1 and 4.2.
Image Style. Given the visual variations in images gen-
erated using different seeds, we investigate whether spe-
cific seeds consistently produce unique style patterns across
prompts. Drawing on established methods in image texture
and style transfer [11,12], we compute style representations
by extracting the Gram matrix — which measures pairwise
cosine similarity across channels — from a pretrained deep
network [45] at multiple layers. Next, we reshape the Gram
matrix into a single-column vector for each image and re-
duce its dimensionality to two using PCA and t-SNE [1,50].
Now, for each image, we have a compact 2D vector that
captures its style. Using N= 1024 seeds and Pprompts

--- PAGE 5 ---
“The image features two ducks sitting on a wooden fence or railing. The ducks are positioned close to each other, with one duck slightly behind the other. They appear to be looking at the camera, possibly curious about their surroundings …”“The image features a young girl sitting in a pink lawn chair on a wooden deck. She is wearing a white dress and is positioned under a large pink umbrella, which provides shade and protection from the sun. The girl appears to be enjoying her time outdoors, possibly on a sunny day …”
HPS v2FID
Seed 469Seed 709Seed 309Seed 154Seed 325Seed 696Seed 174Seed 221Seed 500Seed 723Seed 516Seed 403
Seed 469Seed 709Seed 309Seed 154Seed 325Seed 696FIDHPS v2
Seed 174Seed 221Seed 500Seed 723Seed 516Seed 403
HPS v2FID
Seed 469Seed 709Seed 309Seed 154Seed 325Seed 696Seed 174Seed 221Seed 500Seed 723Seed 516Seed 403
Figure 5. We compare the top three best and worst seeds for SD 2.0 using FID [16] and HPS v2 [53].
Figure 6. We sort seeds by FID [16] using 10,000 images in Prompt Set 1, and then display the FID for the same seeds using another
10,000 images in Prompt Set 2. Lower FID indicates better quality.
Figure 7. We sort seeds by Human Preference Score v2 [53] using 1,000 images in Prompt Set 1, and then plot the score for the same seeds
using another 1,000 images in Prompt Set 2. A higher HPS v2 score indicates the images are more aligned with human preferences.
results in a feature dimension of N×(2×P), combin-
ing the style representation across the generated images for
each seed. We further reduce [1, 50] this aggregated style
representation per seed from N×(2×P)toN×2. Finally,
a subset of seeds are visualized in Fig. 8, providing a clear
visual representation of style clustering at the seed level.
In Fig. 8, the positions within the embedding space cor-
respond to the same seeds across various subplots. As de-
picted in the first row, certain seed groups consistently gen-
erate grayscale images irrespective of the prompt. Simi-
larly, the second row shows that some seeds tend to pro-
duce images with prominent sky regions, while others do
not. Furthermore, in Fig. 9, we observe that a select group
of seeds consistently generate images with a ‘border’ effectnear the edges, regardless of the text prompt. Collectively,
these findings demonstrate that individual seeds exhibit dis-
tinct tendencies in style generation across varying prompts.
Image Composition. Moving beyond style, we examine
whether seeds create distinctive image compositions, such
as consistent object locations and sizes. As described in
Sec. 3.2, we generate images using 880 synthetic prompts
consisting of 40 object categories paired with 22 modifiers,
which includes adjectives and the empty string. For each
image, we segment [8] the object and compute an image
composition feature vector that contains the object’s cen-
troid (x, y)coordinates, size, and depth [54] relative to the
image. On the left side of Fig. 10, we visualize the distribu-
tion of the object mask’s centroid for the category “horse.”

--- PAGE 6 ---
…agroupofelephants …… twopeopleplayingfrisbee …… peoplehavingdinner …Grayscale Grayscale Grayscale 
…amanplayingbaseball …… anelephantacrossthestreet …… buildingwithclock …
SkySkySkyFigure 8. Style embedding clustering across various prompts, with each position corresponding to a unique seed. Certain seeds tend to
generate grayscale images for SD 2.0 (top), while others frequently produce images with ‘white sky’ regions for SDXL Turbo (bottom).
Please zoom-in to check.
Seed 0 Seed 16 Seed 50 Seed 154 Seed 156
“… close -up 
view of a 
green 
apple …”“… two zebras 
standing on 
a lush green 
hillside …”
Seed 0 Seed 16 Seed 50 Seed 154 Seed 156
Figure 9. Certain seeds produce a “border” around images for SD 2.0. Often, these borders appear as horizontal bars at the top and bottom.
Surprisingly, seed 0 occasionally generates a thick dark border on the left side of the image, while seed 50 sometimes adds a “photo frame.”
Remarkably, the object’s position stays relatively the same
despite slight prompt alterations. On the right side of Fig.
10, we observe an analogous pattern in the object’s size and
depth for the category “bowl.” Overall, we observe that the
location, size, and depth of objects are largely dependent on
the specific seed used, consistent across the same object cat-
egories and irrespective of the text modifiers in the prompts.
4. Practical Applications
4.1. High-Fidelity Inference
In Sec. 3.4, we observed that ‘golden’ seeds tend to
generate images with significantly better quality and human
preference alignment. This inspires us to think—how much
can we improve the image quality compared to random gen-
erations by simply leveraging these ‘golden’ seeds?
Specifically, we identified k‘golden’ seeds that excel in
both image quality and human preference alignment. We
subsequently tested these k‘golden’ seeds by generating
images with a different set of 10,000 prompts to evalu-
ate their performance relative to random seeds. We iden-
tified k= 65 ‘golden’ seeds for SD 2.0 and k= 67 for
SDXL Turbo, where kwas determined by selecting seedsthat ranked among the top 256 in both FID [41] and HPS
v2 [53]. We propose that a sampling pool of 60+ ‘golden’
seeds is sufficiently large in practical applications for a sin-
gle prompt. As demonstrated in Tab. 1, leveraging these
well-chosen seeds significantly improves the FID and HPS
v2 scores for both SD 2.0 and SDXL Turbo, and on both
MS-COCO [25] and the PartiPrompts benchmark [57].
4.2. Controlling Diversity in Style and Composition
A typical image generation interface presents the user
with four samples per prompt. Moreover, prior methods aim
to promote the diversity of generated images using primar-
ily gradient-based methods, such as Particle Guidance [9].
In Sec. 3.4, our results highlight that the choice of seed has
a strong influence on the stylistic and spatial attributes in
the generated images. Therefore, we explore whether we
can obtain more diverse images in style or composition by
merely sampling ‘diverse’ seeds.
To select four diverse seeds per prompt, we represent
each seed by a feature vector capturing its style or com-
position, as discussed in Sec. 3.4. We then employ farthest
point sampling using these features. Specifically, we ran-

--- PAGE 7 ---
Seed 147
pale round simple
Seed 60
big small elegant
Seed 19
pale round simple
Seed 403
big small elegant
Seed 213
pale round simple
Seed 326
big small elegant
Seed 502
big small elegant
Seed 485
pale round simple
Figure 10. We observe that seeds produce images with unique and consistent compositions for a given object category. Each data point
represents a seed. For each seed, we combine image composition features from 22 prompts with slight variations like “a pale bowl” and
“a round bowl.” Then, we apply dimensionality reduction [1,50] for visualization. Left: Distribution of object centroid (x, y )coordinates.
Right: Distribution of object depth and size relative to the image.
Table 1. We demonstrate that well-chosen seeds can outperform random generations by comparing the visual quality and human preference
alignment using our ‘golden’ seeds and random seeds. Additionally, our ‘golden’ seeds lead to improved human preference alignment on
a greater variety of prompts in the PartiPrompts benchmark [57]. Mean and standard deviation based on three trials.
SD 2.0 SDXL Turbo
FID(↓) HPS v2 (↑) Parti HPS v2 (↑) FID(↓) HPS v2 (↑) Parti HPS v2 (↑)
Random Seeds 19.334 ±0.212 0.250 ±0.000 0.263 ±0.001 24.859 ±0.123 0.266 ±0.000 0.290 ±0.000
Our Golden Seeds 19.045 ±0.058 0.257 ±0.000 0.268 ±0.001 24.209 ±0.108 0.272 ±0.000 0.293 ±0.001
domly pick the first seed s0∼ U{ 0,1023}and iteratively
select the next three seeds to maximize the distance in fea-
ture space from the already selected seeds.
si= arg max
s/∈Smin
s′∈S∥f(s)−f(s′)∥,fori= 1, . . . , C −1
(1)
where Sis our set of diverse seeds. To evaluate whether
our well-chosen seeds improve diversity over random seeds
and Particle Guidance [9], we calculate the similarity be-
tween images synthesized from a different set of Pprompts,
where P= 500 LLaV A [26] dense captions for image
style and P= 440 synthetic prompts for image composi-
tion. Specifically, we measure the pairwise cosine similarity
of image features and average the similarity scores across
prompts. A lower pairwise similarity score means higher
diversity. Mathematically, the similarity is computed as:
Similarity =1
PPX
i=1
1 C
2CX
j=1CX
k=j+1cos( fij,fik)
(2)
where there are Pprompts and fdenotes the feature vec-
tor representing image style or composition. We typically
useC= 4 images per prompt, but it’s important to note
that if no objects are detected in an image, then the imageis not used to compute similarity. In Tab. 2, we observe
that our diverse seeds outperform random seeds and Par-
ticle Guidance [9] in generating images with varying styles
and compositions for SD 2.0. Interestingly, our well-chosen
seeds aid in diversifying image composition for SD 2.0 but
not for SDXL Turbo. We show comparisons in Fig. 11.
Table 2. We compare the style and composition diversity of images
generated using our diverse seeds, Particle Guidance [9], and ran-
dom seeds. More diverse generations have lower similarity scores.
We show the mean and standard deviation based on three trials.
SD 2.0 SDXL Turbo
Style (↓) Composition (↓) Style (↓) Composition (↓)
Random Seeds 0.98 ±0.00 0.97 ±0.00 0.99 ±0.00 0.99 ±0.00
Particle Guidance 0.98 ±0.00 0.97 ±0.00 — —
Our Diverse Seeds 0.97±0.00 0.96 ±0.00 0.98 ±0.00 0.99±0.00
4.3. Improved Text-based Inpainting
In Sec. 4.1 and 4.2, we showed that carefully selecting
the seed provides a straightforward, training-free approach
to enhance the visual quality, human preference, and diver-
sity of images generated by text-to-image diffusion models.
But, the potential of image generation extends beyond text-
to-image applications. This poses an intriguing question—
can we also uncover ‘golden’ seeds for text-based image

--- PAGE 8 ---
“… a blue city 
bus driving 
down a street, 
with a tall 
building in the 
background …”Random Seeds Particle Guidance Ours
“a sleek sheep”
Figure 11. We show that simply generating images using “diverse” seeds can promote more variation in style (top) and image composition,
as measured by object centroid, size, and depth (bottom).
RemovalCompletion
Original
Seed 493
Seed 34Seed 645Seed 797Seed 646Seed 996Seed 595Seed 857
0.50%0.69%0.93%0.96%5.82%5.86%5.95%6.06%
OriginalSeed 900Seed 742Seed 762Seed 661Seed 135Seed 996Seed 479Seed 272
0.07%0.14%0.16%0.17%1.78%1.81%1.95%2.02%
Figure 12. Certain seeds tend to insert unwanted text within the inpainting region, outlined in pink. Top: We aim to remove the object
using the prompt “clear background.” Bottom: We attempt to complete the object using a prompt that specifies the object category.
inpainting tasks, such as object removal and completion?
As described in Sec. 3.2, we gathered 500 pairs of im-
ages and inpainting masks for the removal and completion
applications based on the Open Images dataset [20,22]. We
employed the prompt “clear background” for the removal
case, and we used a prompt corresponding to the original
object category for the completion case. We then generated
images using a text-based diffusion inpainting model. We
observed that some images contain unwanted text in the in-
painting region that often mimics the prompt. To quantify
the presence of text, we applied optical character recogni-
tion [17] and computed the mean proportion of text arti-
facts within the inpainting mask across all images from each
seed. As shown in Fig. 12, certain seeds tend to insert text
in both removal and completion scenarios.
5. Conclusion
In this work, we investigated the role of “random” seeds
in the reverse diffusion process, exploring their differentia-
bility based on generated images and their impact on inter-
pretable visual dimensions. Notably, our 1,024-way classi-fier trained to predict the seed number for a generated image
achieved over 99.9% test accuracy in just a few epochs. En-
couraged by this finding, we conducted extensive analyses
and identified ‘golden’ seeds that consistently produce im-
ages with better visual quality and human preference align-
ment. Additionally, we discovered that certain seeds create
‘grayscale’ images, add borders, or insert text during in-
painting. Our studies also show that seeds influence the im-
age composition, affecting object position, size, and depth.
Leveraging these insights, we propose downstream applica-
tions such as high-fidelity inference and diversified genera-
tion for text-to-image diffusion models by merely sampling
these special seeds. Our analyses offer new perspectives on
enhancing image synthesis during inference without signif-
icant computational overhead.
Acknowledgements. This work is supported by funds
provided by the National Science Foundation and by DoD
OUSD (R&E) under Cooperative Agreement PHY-2229929
(The NSF AI Institute for Artificial and Natural Intelli-
gence).

--- PAGE 9 ---
References
[1] Herv ´e Abdi and Lynne J Williams. Principal component
analysis. Wiley interdisciplinary reviews: computational
statistics , 2(4):433–459, 2010. 2, 4, 5, 7
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-
aming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image dif-
fusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 1, 2
[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce
Lee, Yufei Guo, et al. Improving image generation with
better captions. Computer Science. https://cdn. openai.
com/papers/dall-e-3. pdf , 2(3):8, 2023. 1, 2
[4] Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya
Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mo-
hammadi Sepahvand, Edward Raff, Kanika Madan, Vikram
V oleti, et al. Accounting for variance in machine learning
benchmarks. Proceedings of Machine Learning and Systems ,
3:747–769, 2021. 2
[5] Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun,
Jing Liu, and Bo Zhao. Synartifact: Classifying and alleviat-
ing artifacts in synthetic images via vision-language model.
arXiv preprint arXiv:2402.18068 , 2024. 1
[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, et al. Pixart- α: Fast training of diffusion
transformer for photorealistic text-to-image synthesis. arXiv
preprint arXiv:2310.00426 , 2023. 1, 2
[7] Sherry X Chen, Yaron Vaxman, Elad Ben Baruch, David
Asulin, Aviad Moreshet, Kuo-Chin Lien, Misha Sra, and
Pradeep Sen. Tino-edit: Timestep and noise optimization
for robust diffusion-based image editing. arXiv preprint
arXiv:2404.11120 , 2024. 3
[8] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 1290–1299, 2022. 5
[9] Gabriele Corso, Yilun Xu, Valentin De Bortoli, Regina
Barzilay, and Tommi Jaakkola. Particle guidance: non-
iid diverse sampling with diffusion models. arXiv preprint
arXiv:2310.13102 , 2023. 6, 7
[10] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and
Aleksander Holynski. Diffusion self-guidance for control-
lable image generation. Advances in Neural Information
Processing Systems , 36:16222–16239, 2023. 1
[11] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Texture synthesis using convolutional neural networks. In
Neural Information Processing Systems , 2015. 4
[12] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks.
In2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2414–2423, 2016. 4
[13] Jacob Gildenblat and contributors. Pytorch library for
cam methods. https://github.com/jacobgil/
pytorch-grad-cam , 2021. 4, 13[14] Paul Grimal, Herv ´e Le Borgne, Olivier Ferret, and Julien
Tourille. Tiam-a metric for evaluating alignment in text-to-
image generation. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
2890–2899, 2024. 3
[15] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu
Yang, and Di Huang. Initno: Boosting text-to-image dif-
fusion models via initial noise optimization. arXiv preprint
arXiv:2404.04650 , 2024. 3
[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 2, 4, 5
[17] Samuel Hoffstaetter and contributors. Python tesseract.
https://github.com/h/pytesseract , 2014. 8
[18] Keller Jordan. Calibrated chaos: Variance between runs of
neural network training is harmless and inevitable. arXiv
preprint arXiv:2304.01910 , 2023. 2
[19] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3
[20] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari,
Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper
Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci,
Jordi Pont-Tuset, Andreas Veit, Serge Belongie, Victor
Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai,
Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy.
Openimages: A public dataset for large-scale multi-label
and multi-class image classification. Dataset available from
https://storage.googleapis.com/openimages/web/index.html ,
2017. 4, 8, 12
[21] Alex Krizhevsky. Learning multiple layers of features from
tiny images. University of Toronto , 05 2012. 2
[22] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,
and Vittorio Ferrari. The open images dataset v4: Unified
image classification, object detection, and visual relationship
detection at scale. IJCV , 2020. 4, 8, 12
[23] Hyunmin Lee and Jaesik Park. Instance-wise occlusion
and depth orders in natural scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 21210–21221, 2022. 12
[24] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios
Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian
Ren. Efficientformer: Vision transformers at mobilenet
speed. Advances in Neural Information Processing Systems ,
35:12934–12949, 2022. 12
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 3, 4, 6, 12
[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023. 3, 4, 7, 12

--- PAGE 10 ---
[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization, 2019. 12
[28] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Guided
image synthesis via initial image editing in diffusion model.
InProceedings of the 31st ACM International Conference on
Multimedia , pages 5321–5329, 2023. 3
[29] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa.
Semantic-driven initial image construction for guided
image synthesis in diffusion model. arXiv preprint
arXiv:2312.08872 , 2023. 3
[30] Johannes Mehrer, Courtney Spoerer, Nikolaus Kriegeskorte,
and Tim Kietzmann. Individual differences among deep neu-
ral network models. Nature Communications , 11, 11 2020.
2
[31] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-
Elor, and Daniel Cohen-Or. Localizing object-level shape
variations with text-to-image diffusion models. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 23051–23061, 2023. 2
[32] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christo-
pher J. Pal, and Marc Aubreville. Wuerstchen: An efficient
architecture for large-scale text-to-image diffusion models,
2023. 1, 2
[33] David Picard. Torch. manual seed (3407) is all you need: On
the influence of random seeds in deep learning architectures
for computer vision. arXiv preprint arXiv:2109.08203 , 2021.
1, 2
[34] Mao Po-Yuan, Shashank Kotyan, Tham Yik Foong, and
Danilo Vasconcellos Vargas. Synthetic shifts to initial seed
vector exposes the brittle nature of latent-based diffusion
models. arXiv preprint arXiv:2312.11473 , 2023. 3
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 3
[36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents, 2022. 1, 2
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models, 2022. 1, 2, 3, 4
[38] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan,
and Gal Chechik. It is all about where you start: Text-
to-image generation with seed selection. arXiv preprint
arXiv:2304.14530 , 2023. 3
[39] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin
Rombach. Adversarial diffusion distillation. arXiv preprint
arXiv:2311.17042 , 2023. 2, 3, 4
[40] Leonardo Scabini, Bernard De Baets, and Odemir M
Bruno. Improving deep neural network random ini-
tialization through neuronal rewiring. arXiv preprint
arXiv:2207.08148 , 2022. 2
[41] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch.
https://github.com/mseitzer/pytorch-fid ,
August 2020. Version 0.3.0. 2, 4, 6[42] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE in-
ternational conference on computer vision , pages 618–626,
2017. 4, 13
[43] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-
cent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffu-
sion models for interactive point-based image editing. arXiv
preprint arXiv:2306.14435 , 2023. 1
[44] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu.
Freeu: Free lunch in diffusion u-net. arXiv preprint
arXiv:2309.11497 , 2023. 1
[45] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 4
[46] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 3, 4
[47] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. The journal of
machine learning research , 15(1):1929–1958, 2014. 3
[48] Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, and
Yonatan Belinkov. Diffusion lens: Interpreting text encoders
in text-to-image pipelines. arXiv preprint arXiv:2403.05846 ,
2024. 2
[49] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921–1930, 2023. 1
[50] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research ,
9(11), 2008. 2, 4, 5, 7
[51] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven
Liu, and Thomas Wolf. Diffusers: State-of-the-art diffu-
sion models. https://github.com/huggingface/
diffusers , 2022. 3, 12
[52] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,
Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu
Chang. Uncovering the disentanglement capability in text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1900–1910, 2023. 2
[53] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng
Zhu, Rui Zhao, and Hongsheng Li. Human preference score
v2: A solid benchmark for evaluating human preferences of
text-to-image synthesis. arXiv preprint arXiv:2306.09341 ,
2023. 2, 4, 5, 6
[54] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi
Feng, and Hengshuang Zhao. Depth anything: Unleashing
the power of large-scale unlabeled data. In CVPR , 2024. 5
[55] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Ste-
fano Ermon, and Bin Cui. Mastering text-to-image diffu-

--- PAGE 11 ---
sion: Recaptioning, planning, and generating with multi-
modal llms. arXiv preprint arXiv:2401.11708 , 2024. 1
[56] Hu Yu, Hao Luo, Fan Wang, and Feng Zhao. Uncovering
the text embedding in text-to-image diffusion models. arXiv
preprint arXiv:2404.01154 , 2024. 2, 3
[57] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2(3):5, 2022. 1, 2, 3, 6, 7, 12

--- PAGE 12 ---
A. Data Generation
In Sec. 3.2, we employed pretrained model checkpoints
and implementations from the Hugging Face diffusers
library [51]. Specifically, for text-to-image genera-
tion, we used Stable Diffusion 2.0 (“stabilityai/stable-
diffusion-2-base”) with a DDIM scheduler, and SDXL
Turbo (“stabilityai/sdxl-turbo”). For text-based image
inpainting, we utilized the SD 2.0 inpainting model
(“stabilityai/stable-diffusion-2-inpainting”). Furthermore,
our 1,024 seeds range from 0 to 1,023 inclusive, and we use
torch.Generator("cuda").manual seed(seed)
to assign the seed used by the model.
A.1. Synthetic Prompts for Image Composition
Analysis
We create a set of 880 prompts by pairing 40 object cate-
gories with 22 modifiers in the format “a [modifier] [object
category]”. These modifiers include 21 adjectives and the
empty string.
•Adjectives: big, small, red, blue, pale, dark, transpar-
ent, shiny, dull, rustic, smooth, rough, bright, muted,
round, simple, elegant, antique, monochrome, intri-
cate, sleek
•Object categories: bicycle, car, motorcycle, airplane,
bus, truck, boat, fire hydrant, bench, bird, cat, dog,
horse, sheep, cow, elephant, zebra, giraffe, backpack,
umbrella, suitcase, sports ball, skateboard, surfboard,
tennis racket, fork, knife, spoon, bowl, apple, pizza,
donut, cake, chair, couch, laptop, cell phone, clock,
vase, teddy bear
A.2. Dataset for Inpainting Applications
We curated 500 pairs of images and inpainting masks for
object removal and object completion applications, as de-
scribed in Sec. 3.2. In particular, for the object removal use
case, we employed images and annotations from the Open
Images dataset [20,22], and we used “clear background” as
the text prompt. To create the inpainting mask, we dilated
the instance segmentation mask to ensure coverage of the
object. Additionally, for the object completion use case, we
sampled images from the MS-COCO dataset [25] and used
InstaOrder [23] to determine occlusion relationships to cre-
ate inpainting masks. We used the category of the object to
complete as the text prompt.
A.3. Licenses for Existing Datasets
The MS-COCO dataset [25] and the PartiPrompts bench-
mark [57] are under a CC BY 4.0 license. For the Open
Images dataset [20, 22], the images are under a CC BY 2.0
license and the annotations are under a CC BY 4.0 license.B. Classifier for Predicting Seed Number
We trained a lightweight transformer, EfficientFormer-
L3 [24], to predict the seed used to generate an image. For
our 1,024-way classification task, we utilized 9,000 train-
ing, 1,000 validation, and 1,000 test images per seed as
mentioned in Sec. 3.3. The prompts for these images are
dense captions by LLaV A 1.5 [26]. Moreover, we set a
batch size of 128 and train for six epochs, which obtains
a model checkpoint with over 99.9% validation and test ac-
curacy. Our classifier uses the AdamW optimizer [27] with
learning rate 0.0002 and weight decay 0.05. We apply data
augmentations during training, which include resizing each
image to have a shorter edge of size 224using bicubic in-
terpolation, center cropping the image to size 224×224,
and randomly flipping the image horizontally with proba-
bility 0.5. During validation and testing, we only resize and
center crop the images.
C. Compute Resources
To generate our dataset in Sec. 3.2, we utilized 32 A100
GPUs for roughly 24 days. Additionally, all the experi-
ments in Sec. 3.3, 3.4, and 4 were performed on an RTX
4090 GPU with 24GB of memory. One of the longest ex-
periments was training the classifier to predict seed number
in Sec. 3.3, which took at most three days.
D. Additional Qualitative Results
We provide extra visualizations of the Grad-CAM from
our classifier that predicts seed number in Figure 13 of the
supplemental. We also show more examples of seeds that
often produce a ‘border’ around the image in Figure 14 of
the supplemental. Moreover, we present additional exam-
ples of good seeds and seeds that generate “text artifacts”
for object removal and completion applications in Figures
15 and 16 of the supplemental, respectively.

--- PAGE 13 ---
Figure 13. Additional Grad-CAM [13, 42] visualizations for our classifier trained to predict the seed number for an image. We note that it
is difficult to interpret what makes seeds easily distinguishable by looking at these visualizations.
Figure 14. Additional examples of seeds that tend to generate a ‘border’ near the image boundaries.

--- PAGE 14 ---
Figure 15. Additional examples of the four best seeds and four worst seeds in terms of how much unwanted text artifacts are inserted
during object removal.
Figure 16. Additional examples of the four best seeds and four worst seeds in terms of how much unwanted text artifacts are inserted
during object completion.
