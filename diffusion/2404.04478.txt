# 2404.04478.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2404.04478.pdf
# File size: 10315092 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Diffusion-RWKV :
Scaling RWKV-Like Architectures for Diffusion Models
Zhengcong Fei, Mingyuan Fan, Changqian Yu
Debang Li, Junshi Huang*
Kunlun Inc.
feizhengcong@gmail.com
Figure 1. Diffusion models with RWKV-like backbones achieve comparable image quality. Selected samples generated by class-
conditional Diffusion-RWKV trained on the ImageNet with resolutions of 256 ×256 and 512 ×512, respectivly.
Abstract
Transformers have catalyzed advancements in computer
vision and natural language processing (NLP) fields. How-
ever, substantial computational complexity poses limita-
tions for their application in long-context tasks, such as
high-resolution image generation. This paper introduces
a series of architectures adapted from the RWKV model
used in the NLP , with requisite modifications tailored for
diffusion model applied to image generation tasks, referred
to as Diffusion-RWKV . Similar to the diffusion with Trans-
formers, our model is designed to efficiently handle patch-
nified inputs in a sequence with extra conditions, while also
scaling up effectively, accommodating both large-scale pa-
rameters and extensive datasets. Its distinctive advantage
manifests in its reduced spatial aggregation complexity, ren-
dering it exceptionally adept at processing high-resolution
images, thereby eliminating the necessity for windowing or
group cached operations. Experimental results on both con-dition and unconditional image generation tasks demon-
strate that Diffison-RWKV achieves performance on par
with or surpasses existing CNN or Transformer-based dif-
fusion models in FID and IS metrics while significantly re-
ducing total computation FLOP usage.
1. Introduction
Transformers [10, 25, 44, 53, 64, 81], which have gained
prominence due to their adaptable nature and proficient in-
formation processing capabilities, have set new standards
across various domains including computer vision and NLP.
Notably, they have demonstrated exceptional performance
in tasks like image generation [4, 8, 9, 14, 42, 57, 58, 65].
However, the self-attention operation in Transformer ex-
hibits a quadratic computational complexity, thereby lim-
iting their efficiency in handling long sequences and poses
a significant obstacle to their widespread application [14,
1arXiv:2404.04478v1  [cs.CV]  6 Apr 2024

--- PAGE 2 ---
39, 80, 87, 89]. Consequently, there is a pressing need to
explore architectures that can effectively harness their ver-
satility and robust processing capabilities while mitigating
the computational demands. It becomes even more crucial
in the context of high-resolution image synthesis or the gen-
eration of lengthy videos.
In recent developments, models such as RWKV [59] and
Mamba [21], have emerged as popular solutions for enhanc-
ing efficiency and processing lengthy textual data with com-
parable capacity. These innovative models exhibit charac-
teristics akin to transformers [3, 6, 12, 15, 43, 45, 63, 64,
73, 78, 85], encompassing to handle long-range dependen-
cies and parallel processing. Moreover, they have demon-
strated scalability, performing admirably with large-scale
NLP and CV datasets [18, 90]. However, given the substan-
tial dissimilarities between visual and textual data domains,
it remains challenging to envision complete replacement of
Transformers with RWKV-based methods for vision gen-
eration tasks [11]. It becomes imperative to conduct an
in-depth analysis of how these models are applied to im-
age generation tasks. This analysis should investigate their
scalability in terms of training data and model parameters,
evaluate their efficiency in handling visual data sequentially,
and identify the essential techniques to ensure model stabil-
ity during scaling up.
This paper introduces Diffusion-RWKV , which is de-
signed to adapt the RWKV architecture in diffusion mod-
els for image generation tasks. The proposed adaptation
aims to retain the fundamental structure and advantages of
RWKV [59] while incorporating crucial modifications to
tailor it specifically for synthesizing visual data. Specifi-
cally, we employ Bi-RWKV [11] for backbone, which en-
ables the calculation within linear computational complex-
ity in an RNN form forward and backward. We primarily
make the architectural choices in diffusion models, includ-
ing condition incorporation, skip connection, and finally of-
fer empirical baselines that enhance the model’s capabil-
ity while ensuring scalability and stability. Building on the
aforementioned design, a diverse set of Diffusion-RWKV
models is developed, as a broad range of model scales,
ranging from tiny to large. These models are training on
CIFAR-10, Celebrity to ImageNet-1K using unconditional
and class-conditioned training at different image resolu-
tions. Moreover, performance evaluations are conducted in
both raw and latent spaces. Encouragingly, under the same
settings, Diffusion-RWKV has comparable performance to
competitor DiT [58] in image generation, with lower com-
putational costs while maintaining stable scalability. This
achievement enables Diff-RWKV training parallelism, high
flexibility, excellent performance, and low inference cost si-
multaneously, making it a promising alternative in image
synthesis. The contribution can be summarized as:
• In a pioneering endeavor, we delve into the exploration ofa purely RWKV-based diffusion model for image genera-
tion tasks, positioning as a low-cost alternative to Trans-
former. Our model not only inherits the advantages of
RWKV for long-range dependency capture, but also re-
duces complexity to a linear level.
• To cater to the demands of image synthesis, we have con-
ducted a comprehensive and systematic investigation of
Diffusion-RWKV models by exploring various configu-
ration choices pertaining to conditioning, block design,
and model parameter scaling.
• Experimental results indicate that Diffusion-RWKV per-
forms comparably to well-established benchmarks DiTs
and U-ViTs, exhibiting lower FLOPs and faster process-
ing speeds as resolution increases. Notably, Diffusion-
RWKV achieves a 2.95 FID score trained only on
ImageNet-1k. Code and model are available at https:
//github.com/feizc/Diffusion-RWKV .
2. Methodology
This section commences by providing an overview of the
foundational concepts in Section 2.1. Subsequently, we
delve into a comprehensive exposition of the RWKV-based
diffusion models for image generation in Section 2.2. It en-
compasses various aspects such as image patchnify, stacked
Bi-RWKV block, skip connections, and condition incorpo-
ration. Lastly, we perform computational analysis and es-
tablish optimal model scaling configurations.
2.1. Preliminaries
Diffusion models. Diffusion models have emerged as a
new family of generative models that generate data by
iterative transforming random noise through a sequence
of deconstructible denoising steps. It usually includes a
forward noising process and a backward denoising pro-
cess. Formally, given data x0sampled from the distribu-
tionp(x0), the forward noising process involves iteratively
adding Gaussian noise to the data, creating a Markov Chain
of latent variables x1, . . . , x T, where:
q(xt|xt−1) =N(xt;p
1−βtxt−1, βtI), (1)
andβ1, . . . , β Tare hyperparameters defining the noise
schedule. After a pre-set number of diffusion steps, xT
can be considered as standard Gaussian noise. A denoising
network ϵθwith parameters θis trained to learn the back-
ward denoising process, which aims to remove the added
noise according to a noisy input. During inference, a data
point can be generated by sampling from a random Gaus-
sian noise xT∼ N(0;I)and iteratively denoising the sam-
ple by sequentially sampling xt−1from xtwith the learned
denoising process, as:
xt−1=1√αt(xt−1−αt
1−αtϵ(xt, t)) +σtz, (2)
2

--- PAGE 3 ---
RWKV Block
...
Embedding LayerRWKV Block
...Linear
...
Skip
RWKV Block
Timestep Condition...
Spatial Mix
ShiftBi-WKVNormChannel Mix
ShiftNormActivation functionFigure 2. Overall framework of diffusion models with RWKV-like architectures. (a) The Diffusion-RWKV architecture comprises L
identical Bi-RWKV layers, a patch embedding, and a projection layer. A skip connection is established between shallow and deep stacked
Bi-RWKV layers for information flow. (b) The detailed composition of Bi-RWKV layers, includes a shift method and a bidirectional RNN
cell in spatial mix, and a shift with two activate functions in channel mix.
where αt=Qs= 1tαs,αt= 1−βt, and σtdenotes
the noise scale. In practice, the diffusion sampling process
can be further accelerated with various sampling techniques
[48, 49, 74].
RWKV-like structures. RWKV [59] brought improve-
ments for standard RNN architecture [30], which is com-
puted in parallel during training while inference like RNN.
It involves enhancing the linear attention mechanism and
designing the receptance weight key value (RWKV) mecha-
nism. Generally, RWKV model consists of an input layer, a
series of stacked residual blocks, and an output layer. Each
residual block is composed of time-mix and channel-mix
sub-block.
(i)The Time-Mix Block aims to improve the model-
ing of dependencies and patterns within a sequence. It is
achieved by replacing the conventional weighted sum cal-
culation in an attention mechanism with hidden states. The
time-mix block can effectively propagate and updates in-
formation across sequential steps with hidden states and the
updation can be expressed as follows:
qt= (µq⊙xt+ (1−µq)⊙xt−1)·Wq, (3)
kt= (µk⊙xt+ (1−µk)⊙xt−1)·Wk, (4)
vt= (µv⊙xt+ (1−µv)⊙xt−1)·Wv, (5)
ot= (σ(qt)⊙h(kt, vt))·Wo, (6)
where qt,kt, and vtare calculated by linearly interpolat-
ing between the current input and the input at the previous
time step. The interpolation, determined by the token shift
parameter µ, ensures coherent and fluent token represen-
tations. Additionally, a non-linear activation function σisapplied to qt, and the resulting value is combined with the
hidden states h(kt, vt)using element-wise multiplication.
The hidden states, which serve as both the reset gate and a
replacement for the traditional weighted sum value, can be
computed as:
pt= max( pt−1, kt), (7)
ht=exp(pt−1−pt)⊙at−1+exp(kt−pt)⊙vt
exp(pt−1−pt)⊙bt−1+exp(kt−pt),(8)
where a0, b0, p0are zero-initialized. Intuitively, the hidden
states are computed recursively, and the vector pserves as
the reset gate in this process.
(ii)Channel-Mix Block aims to amplify the outputs of
time-mix block, which can be given by:
rt= (µr⊙ot+ (1−µr)⊙ot−1)·Wr (9)
zt= (µz⊙ot+ (1−µz)⊙ot−1)·Wz (10)
˜xt=σ(rt)⊙(max( zt,0)2·Wv) (11)
The output otcontains historical information up to time t,
and the interpolation weight µis derived from otandot−1,
similar to the time-mix block, which also enhances the his-
torical information representation. Note that the calcula-
tions of hidden states may lead to information loss and fail-
ure to capture long-range dependencies [59].
2.2. Model Structure Design
We present Diffusion-RWKV , a variant of RWKV-like dif-
fusion models, as a simple and versatile architecture for im-
age generation. Diffusion-RWKV parameterizes the noise
prediction network ϵθ(xt, t, c), which takes the timestep t,
3

--- PAGE 4 ---
#Params L D E Gflops
Small 38.9M 25 384 4 1.72
Base 74.3M 25 768 4 3.32
Medium 132.0M 49 768 4 5.90
Large 438.5M 49 1024 4 19.65
Huge 779.1M 49 1536 4 34.95
Table 1. Scaling law model size. The model sizes and detailed
hyperparameter settings for scaling experiments. In between, L
is the number of stacked Bi-RWKV layers, Dis the hidden state
size, and Eis the embedding ratio.
condition cand noised image xtas inputs and predicts the
noise injected into data point xt. As our goal follows the
cutting-edge RWKV architecture to maintain its scalability
characteristics, Diffusion-RWKV is grounded in the bidi-
rectional RWKV [11] architecture which operates on se-
quences of tokens. Figure 2 illustrates an overview of the
complete Diffusion-RWKV architecture. In the following,
we elaborate on the forward pass and the components that
constitute the design space of this model class.
Image tokenization. The initial layer of Diffusion-
RWKV performs a transformation of the input image I∈
RH×W×Cinto flattened 2-D patches X∈RJ×(p2·C). Sub-
sequently, it converts these patches into a sequence of Jto-
kens, each with Ddimension, by linearly embedding each
image patch in the input. Consistent with [10], learnable
positional embeddings are applied to all input tokens. The
number of tokens Jgenerated by the tokenization process is
determined by the hyperparameter patch size p, calculated
asH×W
p2. The tokenization layer supports both raw pixel
and latent space representations.
Bi-directional RWKV block. Subsequent to the embed-
ding layer, the input tokens undergo processing through
a succession of identical Bi-RWKV blocks. Consider-
ing that the original RWKV block was designed for one-
dimensional sequence processing, we resort to [90], which
incorporates bidirectional sequence modeling tailored for
vision tasks. This adaptation preserves the core structure
and advantages of RWKV [59] while integrating critical
modifications to tailor it for processing visual data. Specif-
ically, it employs a quad-directional shift operation tailored
for two-dimensional vision data and modifies the original
causal RWKV attention mechanism to a bidirectional global
attention mechanism. The quad-directional shift operation
expands the semantic range of individual tokens, while the
bidirectional attention enables the calculation of global at-
tention within linear computational complexity in an RNN-
like forward and backward manner. As illustrated in the
right part of Figure 2, the forward pass of Bi-RWKV blocksamalgamates both forward and backward directions in the
spatial and channel mix modules. These alterations enhance
the model’s long-range capability while ensuring scalability
and stability.
Skip connection. Considering a series of Lstacked Bi-
RWKV blocks, we categorize the blocks into three groups:
the first ⌊L
2⌋blocks as the shallow group, the middle block
as the central layer, and the remaining ⌊L
2⌋blocks as the
deep group as [1, 18]. Let hshallow andhdeep represent the
hidden states from the main branch and long skip branch,
respectively, both residing in RJ×D. We propose concate-
nating these hidden states and applying a linear projection,
expressed as Linear(Concate( hshallow , hdeep)), be-
fore propagating them to the subsequent block.
Linear decoder. Upon completion of the final Bi-RWKV
block, it becomes essential to decipher the sequence of hid-
den states to generate an output noise prediction and diago-
nal covariance prediction. These resulting outputs maintain
a shape equivalent to the original input image. To achieve
this, a standard linear decoder is employed, wherein the fi-
nal layer norm is applied, and each token is linearly decoded
into a p2·Ctensor. Finally, the decoded tokens are rear-
ranged to match their original spatial layout, yielding the
predicted noise and covariance.
Condition incorporation. In addition to the noised im-
age inputs, diffusion models process supplementary condi-
tional information, such as noise timesteps tand condition
c, which usually encompass class labels or natural language
data. In order to incorporate additional conditions effec-
tively, this study employs three distinct designs as referred
from [18, 58]:
•In-context conditioning. A straightforward strategy of ap-
pending the vector embeddings of timestep tand condi-
tion cas two supplementary tokens within the input se-
quence. These tokens are treated on par with the image
tokens. Implementing this technique allows for the uti-
lization of Bi-RWKV blocks without requiring any ad-
justments. Note that the conditioning tokens are removed
from the sequence in the spatial mix module in each Bi-
RWKV block and after the final block.
•Adaptive layer norm (adaLN) block. We explore replac-
ing the standard norm layer with adaptive norm layer.
Rather than directly learning scale and shift parameters
on a per-dimension basis, these parameters are deduced
from the summation of the embedding vectors of tandc.
•adaLN-Zero block. In addition to regressing γandβ,
we also regress dimension-wise scaling parameters αthat
are applied immediately prior to any residual connections
within the Bi-RWKV block. The MLP is initialized to
produce a zero-vector output for all α.
4

--- PAGE 5 ---
Figure 3. Ablation experiments and model analysis for different designs with DRWKV-S/2 model on the CIFAR10 dataset. We report
FID metrics on 10K generated samples every 50K steps. We can find that: (a) Patch size. A smaller patch size can improve the image
generation performance. (b) Skip operation. Combining the long skip branch can accelerate the training as well as optimize generated
results. (c) Variants of condition incorporation. AdaLN-Zero is an effective strategy for conditioning. (d) Model parameters scaling. As
we expected, holding the patch size constant, increasing the model parameters can consistently improve the generation performance.
2.3. Computation Analysis
In summary, the hyper-parameters of the Diffusion-RWKV
model encompass crucial components including embedding
dimension E, hidden dimension Din linear projection, and
depth L. Central to the bi-directional RWKV block’s archi-
tecture is the generation of attention results for each token
through individual update steps, culminating in the requisite
Tsteps for the complete processing of the WKV matrix.
Here, Tis the sequence length. Considering the input K
andVare matrices with the shape of J×D, where Dis the
dimension of hidden learnable vectors, the computational
cost of calculating the WKV matrix is given by:
FLOPs (Bi-WKV (K, V )) = 13 ×J×D. (12)
Here, the number 13 is approximately from the updates of
four hidden states, the computation of the exponential, and
the calculation of wkvt matrix. Jis the total number of
update steps and is equal to the number of image tokens.
The above approximation shows that the complexity of the
forward process is O(J·D). The backward propagation
of the operator can still be represented as a more complex
RNN form, with a computational complexity of O(J·D).
It demonstrates a superiority of linear increasing compared
with self-attention operation in Transformer structure. Fi-
nally, the different model variants are specified in Table 1.
In between, we use five configs, from small to huge, to cover
a wide range of model sizes and flop allocations, from 1.72
to 34.95 Gflops, allowing us to gauge scaling performance.
3. Experiments
In this section, we shall delve into the intricacies of the de-
sign space and thoroughly examine the scaling properties
inherent in our Diffusion-RWKV model class. In order to
simplify our discourse, each model within our class is de-
noted by its specific configurations and patch size p. As an
example, we refer to the Large version configuration with
p= 2as DRWKV-L/2.3.1. Experimental Settings
Datasets. For unconditional image generation, two
datasets are considered: CIFAR10 [40] and CelebA 64x64
[46]. CIFAR10 comprises a collection of 50k training im-
ages, while CelebA 64x64 encompasses 162,770 images
depicting human faces. As for the class-conditional im-
age generation, the ImageNet dataset [5] is employed. This
dataset consists of 1,281,167 training images, distributed
across 1,000 distinct classes. In terms of data augmenta-
tion, only horizontal flips are employed. The training pro-
cess involves 500k iterations on both CIFAR10 and CelebA
64×64, utilizing a batch size of 128 in pixel space. In the
case of ImageNet, two scenarios are considered as resolu-
tion of 256 ×256 and 512 ×512. For the former, 500k iter-
ations are conducted, while for the latter, 1M iterations are
performed. The batch size is set as 512 in both cases.
Implementation details. We followed the same training
recipe from DiT [58] to ensure consistent settings across
all models. We choose to incorporate an exponential mov-
ing average (EMA) of model weights with a fixed decay
rate of 0.9999. All reported results have been obtained us-
ing the EMA model. We use the AdamW optimizer [36]
without weight decay across all datasets and maintain a
learning rate of 1e-4 to 3e-5 in stages. Our models are
trained on the Nvidia A100 GPU. During training on the
ImageNet dataset at a resolution of 256 ×256 and 512 ×512,
we also adopt classifier-free guidance [27] following [67]
and use an off-the-shelf pre-trained variational autoencoder
(V AE) model [38] from playground V2 provided in hug-
gingface1with corresponding settings. The V AE encoder
component incorporates a downsampling factor of 8. We
maintain the diffusion hyperparameters from [58], employ-
ing a tmax= 1000 linear variance schedule ranging from
1×10−4to2×10−2and parameterization of the covariance.
1https://huggingface.co/playgroundai
5

--- PAGE 6 ---
In order to adapt the model to an unconditional context, we
just removed the class label embedding component.
Evaluation metrics. The performance evaluation of im-
age generation is conducted using the Fr ´echet Inception
Distance (FID) [26], an extensively employed metric for as-
sessing the quality of generated images. In accordance with
established conventions for comparative analysis with pre-
vious works, we present FID-50K results obtained through
250 DDPM sampling steps [56], following [7]. Further-
more, we provide supplementary metrics such as the In-
ception Score [69], sFID [54], and Precision/Recall [41] to
complement the evaluation.
3.2. Model Analysis
We first conduct a systematical empirical investigation into
the fundamental components of Diffusion-RWKV models.
Specifically, we ablate on the CIFAR10 dataset, evaluate
the FID score every 50K training iterations on 10K gener-
ated samples, instead of 50K samples for efficiency [1], and
determine the optimal default implementation details.
Effect of patch size. We train patch size range over (8,
4, 2) in Small configuration on the CIFAR10 dataset. The
results obtained from this experimentation are illustrated
in Figure 3 (a). It indicates that the FID metric exhibits
fluctuations in response to a decrease in patch size while
maintaining a consistent model size. Throughout the train-
ing process, we observed discernible improvements in FID
values by augmenting the number of tokens processed by
Diffusion-RWKV , while keeping the parameters approxi-
mately fixed. This observation leads us to the conclusion
that achieving optimal performance requires a smaller patch
size, specifically 2. We hypothesize that this requirement
arises from the inherently low-level nature of the noise pre-
diction task in diffusion models. It appears that smaller
patches are more suitable for this task compared to higher-
level tasks such as classification.
Effect of long skip. To assess the efficacy of the
skipping operation, we investigate three different
variants, namely: ( i) Concatenation, denoted as
Linear(Concat( hshallow ,hdeep)); (ii) Addition,
represented by hshallow +hdeep; and ( iii) No skip connec-
tion. Figure 3 (b) illustrates the outcomes of these variants.
It is evident that directly adding the hidden states from
the shallow and deep layers does not yield any discernible
benefits. Conversely, the adoption of concatenation entails
a learnable linear projection on the shallow hidden states
and effectively enhances performance in comparison to the
absence of a long skip connection.Model #Params FID ↓
DDPM [28] 36M 3.17
EDM [34] 56M 1.97
GenViT [86] 11M 20.20
U-ViT-S/2 [1] 44M 3.11
DiS-S/2 [18] 28M 3.25
DRWKV-S/2 39M 3.03
Table 2. Benchmarking unconditional image generation on CI-
FAR10 . Diffusion-RWKV-S/2 model obtains comparable results
with fewer parameters.
Model #Params FID ↓
DDIM [74] 79M 3.26
Soft Trunc. [35] 62M 1.90
U-ViT-S/4 [1] 44M 2.87
DiS-S/2 [18] 28M 2.05
DRWKV-S/2 39M 1.92
Table 3. Benchmarking unconditional image generation on
CelebA 64 ×64. Diffusion-RWKV-S/2 maintains a superior gen-
eration performance in small model settings.
Effect of condition combination. We examine three ap-
proaches for incorporating the conditional timestep tinto
the network, as discussed in the preceding method sec-
tion. The integration methods are depicted in Figure 3
(c). Among these strategies, the adaLN-Zero block exhibits
a lower FID compared to the in-context conditioning ap-
proach, while also demonstrating superior computational
efficiency. Specifically, after 500k training iterations, the
adaLN-Zero model achieves an FID that is approximately
one-third of that obtained by the in-context model, under-
scoring the critical influence of the conditioning mechanism
on the overall quality of the model. Furthermore, it should
be noted that the initialization process holds significance in
this context. Additionally, it is worth mentioning that due
to the inclusion of a resize operation in the design of the Bi-
RWKV in spatial channel mix, only the in-context token is
provided to the channel mix module.
Scaling model size. We investigate scaling properties of
Diffusion-RWKV by studying the effect of depth, i.e., num-
ber of Bi-RWKV layers, and width, e.g. the hidden size.
Specifically, we train 5 Diffusion-RWKV models on the
ImageNet dataset with a resolution of 256 ×256, spanning
model configurations from small to huge as detailed in Ta-
ble 1, denoted as (S, B, M, L, H) for simple. As depicted
in Figure 3 (d), the performance improves as the depth in-
creases from 25 to 49. Similarly, increasing the width from
384 to 1024 yields performance gains. Overall, across all
five configurations, we find that similar to DiT models [58],
6

--- PAGE 7 ---
Model FID ↓ sFID↓ IS↑ Precision ↑Recall↑
BigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28
StyleGAN-XL [70] 2.30 4.02 265.12 0.78 0.53
ADM [7] 10.94 6.02 100.98 0.69 0.63
ADM-U 7.49 5.13 127.49 0.72 0.63
ADM-G 4.59 5.25 186.70 0.82 0.52
ADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53
CDM [29] 4.88 - 158.71 - -
LDM-8 [67] 15.51 - 79.03 0.65 0.63
LDM-8-G 7.76 - 209.52 0.84 0.35
LDM-4 10.56 - 103.49 0.71 0.62
LDM-4-G 3.60 - 247.67 0.87 0.48
VDM++ [37] 2.12 - 267.70 - -
U-ViT-H/2 [1] 2.29 5.68 263.88 0.82 0.57
DiT-XL/2 [58] 2.27 4.60 278.24 0.83 0.57
SiT-XL/2 [50] 2.06 4.50 270.27 0.82 0.59
DiffuSSM-XL/2 [84] 2.28 4.60 278.24 0.83 0.57
DiS-H/2 [18] 2.10 4.55 271.32 0.82 0.58
DRWKV-H/2 2.16 4.58 275.36 0.83 0.58
Table 4. Benchmarking class-conditional image generation on
ImageNet 256 ×256. Diffusion-RWKV-H/2 achieves state-of-the-
art FID metrics towards best competitors.
large models use FLOPs more efficient and scaling the DR-
WKV will improve the FID at all stages of training.
3.3. Main Results
We compare to a set of previous best models, includes:
GAN-style approaches that previously achieved state-of-
the-art results, UNet-architectures trained with pixel space
representations, and Transformers and state space models
operating in the latent space. Note that our aim is to com-
pare, through a similar denoising process, the performance
of our model with respect to other baselines.
Unconditional image generation. We evaluate the un-
conditional image generation capability of our model in
relation to established baselines using the CIFAR10 and
CelebA datasets within the pixel-based domain. The out-
comes of our analysis are presented in Table 2 and Table
3, respectively. The results reveal that our proposed model,
Diffusion-RWKV , attains FID scores comparable to those
achieved by Transformer-based U-ViT and SSM-based DiS
models, while utilizing a similar training budget. Notably,
our model achieves this with fewer parameters and exhibits
superior FID scores. These findings emphasize the prac-
ticality and effectiveness of RWKV across various image
generation benchmarks.
Class-conditional image generation. We also compare
the Diffusion-RWKV model with state-of-the-art class-
conditional models in the ImageNet dataset, as listed in
Table 4 and Table 5. When considering a resolution of
256, the training of our DRWKV model exhibits a 25% re-
duction in Total Gflops compared to the DiT (1.60 ×1011
vs. 2.13 ×1011). Additionally, our models achieve similarModel FID ↓ sFID↓ IS↑ Precision ↑Recall↑
BigGAN-deep [2] 8.43 8.13 177.90 0.88 0.29
StyleGAN-XL [70] 2.41 4.06 267.75 0.77 0.52
ADM [7] 23.24 10.19 58.06 0.73 0.60
ADM-U 9.96 5.62 121.78 0.75 0.64
ADM-G 7.72 6.57 172.71 0.87 0.42
ADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53
VDM++ [37] 2.65 - 278.10 - -
U-ViT-H/4 [1] 4.05 6.44 263.79 0.84 0.48
DiT-XL/2 [58] 3.04 5.02 240.82 0.84 0.54
DiffuSSM-XL/2 [84] 3.41 5.84 255.06 0.85 0.49
DiS-H/2 [18] 2.88 4.74 272.33 0.84 0.56
DRWKV-H/2 2.95 4.95 265.20 0.84 0.54
Table 5. Benchmarking class-conditional image generation on
ImageNet 512 ×512. DRWKV-H/2 demonstrates a promising per-
formance compared with both CNN-based and Transformer-based
UNet for diffusion.
sFID scores to other DDPM-based models, outperforming
most state-of-the-art strategies except for SiT and DiS. This
demonstrates that the images generated by the Diffusion-
RWKV model are resilient to spatial distortion. Further-
more, in terms of FID score, Diffusion-RWKV maintains a
relatively small gap compared to the best competitor. It is
noteworthy that SiT is a transformer-based architecture that
employs an advanced strategy, which could also be incor-
porated into our backbone. However, this aspect is left for
future research, as our primary focus lies in comparing our
model against DiT. Moreover, we extend our comparison to
a higher-resolution benchmark of size 512. The results ob-
tained from the Diffusion-RWKV model demonstrate a rel-
atively strong performance, approaching that of some state-
of-the-art high-resolution models. Our model outperforms
all models except for DiS, while achieving comparable FID
scores with a lower computational burden.
3.4. Case Study
In Figure 1 and Figure 4, a curated selection of samples
from the ImageNet datasets is presented. These samples
are showcased at resolutions of 256 ×256 and 512 ×512, ef-
fectively illustrating clear semantic representations and ex-
hibiting high-quality generation. To delve deeper into this
topic, the project page offers a collection of additional gen-
erated samples, encompassing both class-conditional and
random variations.
4. Related Works
Image generation with diffusion. Diffusion and score-
based generative models [33, 75–77] have demonstrated
significant advancements in various tasks, particularly in
the context of image generation [66–68]. The DDPM
has been primarily attributed to improvements in sampling
techniques [16, 17, 28, 34, 55], and the incorporation of
classifier-free guidance [27]. Additionally, [74] introduced
7

--- PAGE 8 ---
Figure 4. Image results generated from Diffusion-RWKV model. Selected samples on ImageNet 512 ×512 with sample classes and
different seeds. We can see that Diffusion-RWKV can generate high-quality images while keeping integrated condition alignment.
a more efficient sampling procedure called Denoising Dif-
fusion Implicit Model (DDIM). Latent space modeling is
another core technique in deep generative models. Vari-
ational autoencoders [38] pioneered learning latent spaces
with encoder-decoder architectures for reconstruction. The
concept of compressing information in latent spaces was
also adopted in diffusion models, as exemplified by the
state-of-the-art sample quality achieved by latent diffusion
models [67], which train deep generative models to re-
verse a noise corruption process within a latent space. Ad-
ditionally, recent advancements have incorporated masked
training procedures, enhancing denoising training objec-
tives through masked token reconstruction [88]. Our work
is fundamentally built upon existing standard DDPMs.
Architectures for diffusion models. Early models for
diffusion employed U-Net style architectures [7, 28]. Sub-
sequent studies endeavored to enhance U-Nets by incor-
porating various techniques, such as the addition of at-
tention layers at multiple scales [55], residual connections
[2], and normalization [60, 83]. However, U-Nets en-
counter difficulties when scaling to high resolutions due
to the escalating computational demands imposed by the
attention mechanism [71]. Recently, vision transformers
[10] have emerged as an alternative architecture, showcas-
ing their robust scalability and long-range modeling capa-
bilities, thereby challenging the notion that convolutional
inductive bias is always indispensable. Diffusion trans-
formers [1, 19, 58] demonstrated promising results. Other
hybrid CNN-transformer architectures were proposed [47]
to improve training stability. More recently, state space-
based model [18, 31, 84] have obtain a advanced perfor-
mance with computation efficiency. Our work aligns withthe exploration of recurrent sequence models and the as-
sociated design choices for generating high-quality images
while mitigating text similarity.
Efficient long sequence modeling. The standard trans-
former architecture employs attention to comprehend the
interplay between individual tokens. However, it faces
challenges when dealing with lengthy sequences, primar-
ily due to the quadratic computational complexity it en-
tails. To address this issue, various attention approxima-
tion methods have been proposed [13, 32, 51, 72, 79, 82],
which aim to approximate self-attention while utilizing sub-
quadratic computational resources. Notably, Mega [52]
combines exponential moving average with a simplified
attention unit, surpassing the performance of the base-
line transformer models. What’s more, researchers have
also explored alternatives that are capable of effectively
handling long sequences. One involves employing state
space models-based architectures, as exemplified by [22–
24], which have demonstrated significant advancements
over contemporary state-of-the-art methods in tasks such
as LRA and audio benchmarking [20]. Moreover, recent
studies [22, 59, 61, 62] have provided empirical evidence
supporting the potential of non-attention architectures in
achieving commendable performance in language model-
ing. Motivated by this evolving trend of recurrence designs,
our work draws inspiration from these advancements and
predominantly leverages the backbone of RWKV .
5. Conclusion
This paper presents Diffusion-RWKV , an architecture
designed for diffusion models featuring sequential informa-
8

--- PAGE 9 ---
tion with linear computational complexity. The proposed
approach effectively handles long-range hidden states
without necessitating representation compression. Through
comprehensive image generation tasks, we showcase its
potential as a viable alternative backbone to the Trans-
former. Experimentally, Diffusion-RWKV demonstrates
comparable performance and scalability while exhibiting
lower computational complexity and memory consump-
tion. Leveraging its reduced complexity, Diffusion-RWKV
outperforms the Transformer model in scenarios where
the latter struggles to cope with high computational de-
mands. We anticipate that it will serve as an efficient and
cost-effective substitute for the Transformer, thereby high-
lighting the substantial capabilities of transformers with
linear complexity in the realm of multimodal generation.
References
[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,
Hang Su, and Jun Zhu. All are worth words: A vit backbone
for diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22669–22679, 2023. 4, 6, 7, 8
[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018. 7, 8
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 2
[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T
Freeman. Maskgit: Masked generative image transformer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11315–11325, 2022.
1
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 5
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2
[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 6, 7, 8
[8] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. Advances in Neural Information
Processing Systems , 34:19822–19835, 2021. 1
[9] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
Cogview2: Faster and better text-to-image generation via
hierarchical transformers. Advances in Neural Information
Processing Systems , 35:16890–16902, 2022. 1[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1, 4, 8
[11] Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei
Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, and
Wenhai Wang. Vision-rwkv: Efficient and scalable vi-
sual perception with rwkv-like architectures. arXiv preprint
arXiv:2403.02308 , 2024. 2, 4
[12] Zhengcong Fei. Partially non-autoregressive image caption-
ing. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 1309–1316, 2021. 2
[13] Zhengcong Fei. Attention-aligned transformer for image
captioning. In proceedings of the AAAI Conference on Ar-
tificial Intelligence , pages 607–615, 2022. 8
[14] Zhengcong Fei, Mingyuan Fan, Li Zhu, and Junshi
Huang. Progressive text-to-image generation. arXiv preprint
arXiv:2210.02291 , 2022. 1
[15] Zhengcong Fei, Xu Yan, Shuhui Wang, and Qi Tian. Deecap:
Dynamic early exiting for efficient image captioning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12216–12226, 2022. 2
[16] Zhengcong Fei, Mingyuan Fan, and Junshi Huang. Gradient-
free textual inversion. In Proceedings of the 31st ACM In-
ternational Conference on Multimedia , pages 1364–1373,
2023. 7
[17] Zhengcong Fei, Mingyuan Fan, Li Zhu, Junshi Huang, Xi-
aoming Wei, and Xiaolin Wei. Uncertainty-aware image cap-
tioning. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 614–622, 2023. 7
[18] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi
Huang. Scalable diffusion models with state space backbone.
arXiv preprint arXiv:2402.05608 , 2024. 2, 4, 6, 7, 8
[19] Zheng-cong Fei. Fast image caption generation with position
alignment. arXiv preprint arXiv:1912.06365 , 2019. 8
[20] Karan Goel, Albert Gu, Chris Donahue, and Christopher R ´e.
It’s raw! audio generation with state-space models. In In-
ternational Conference on Machine Learning , pages 7616–
7633. PMLR, 2022. 8
[21] Albert Gu and Tri Dao. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
arXiv:2312.00752 , 2023. 2
[22] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christo-
pher R ´e. Hippo: Recurrent memory with optimal polynomial
projections. Advances in neural information processing sys-
tems, 33:1474–1487, 2020. 8
[23] Albert Gu, Karan Goel, and Christopher R ´e. Efficiently
modeling long sequences with structured state spaces. arXiv
preprint arXiv:2111.00396 , 2021.
[24] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state
spaces are as effective as structured state spaces. Advances
in Neural Information Processing Systems , 35:22982–22994,
2022. 8
[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
9

--- PAGE 10 ---
vision learners. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 16000–
16009, 2022. 1
[26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[27] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 5, 7
[28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 6, 7, 8
[29] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. The Journal of
Machine Learning Research , 23(1):2249–2281, 2022. 7
[30] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term
memory. Neural computation , 9(8):1735–1780, 1997. 3
[31] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga
Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn
Ommer. Zigma: Zigzag mamba diffusion model. arXiv
preprint arXiv:2403.13802 , 2024. 8
[32] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Trans-
former quality in linear time. In International conference on
machine learning , pages 9099–9117. PMLR, 2022. 8
[33] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-
normalized statistical models by score matching. Journal
of Machine Learning Research , 6(4), 2005. 7
[34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. Advances in Neural Information Processing Sys-
tems, 35:26565–26577, 2022. 6, 7
[35] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo
Kang, and Il-Chul Moon. Soft truncation: A universal train-
ing technique of score-based diffusion model for high pre-
cision score estimation. arXiv preprint arXiv:2106.05527 ,
2021. 6
[36] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[37] Diederik P Kingma and Ruiqi Gao. Understanding diffusion
objectives as the elbo with simple data augmentation. In
Thirty-seventh Conference on Neural Information Process-
ing Systems , 2023. 7
[38] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 5, 8
[39] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
Reformer: The efficient transformer. arXiv preprint
arXiv:2001.04451 , 2020. 2
[40] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[41] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and recall met-
ric for assessing generative models. Advances in Neural In-
formation Processing Systems , 32, 2019. 6[42] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and
Wook-Shin Han. Autoregressive image generation using
residual quantization. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
11523–11532, 2022. 1
[43] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-
jad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and
Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and
comprehension. arXiv preprint arXiv:1910.13461 , 2019. 2
[44] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
A survey of transformers. AI open , 3:111–132, 2022. 1
[45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. Roberta: A robustly optimized
bert pretraining approach. arXiv preprint arXiv:1907.11692 ,
2019. 2
[46] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
the IEEE international conference on computer vision , pages
3730–3738, 2015. 5
[47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 8
[48] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion
probabilistic model sampling in around 10 steps. Advances
in Neural Information Processing Systems , 35:5775–5787,
2022. 3
[49] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-
uan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided
sampling of diffusion probabilistic models. arXiv preprint
arXiv:2211.01095 , 2022. 3
[50] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M
Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Explor-
ing flow and diffusion-based generative models with scalable
interpolant transformers. arXiv preprint arXiv:2401.08740 ,
2024. 7
[51] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,
Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Lin-
ear unified nested attention. Advances in Neural Information
Processing Systems , 34:2441–2453, 2021. 8
[52] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,
Liangke Gui, Graham Neubig, Jonathan May, and Luke
Zettlemoyer. Mega: moving average equipped gated atten-
tion. arXiv preprint arXiv:2209.10655 , 2022. 8
[53] Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal,
A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal,
et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 , 2020. 1
[54] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W
Battaglia. Generating images with sparse representations.
arXiv preprint arXiv:2103.03841 , 2021. 6
[55] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
10

--- PAGE 11 ---
Conference on Machine Learning , pages 8162–8171. PMLR,
2021. 7, 8
[56] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On
aliased resizing and surprising subtleties in gan evaluation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11410–11420, 2022.
6
[57] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. In International conference on machine
learning , pages 4055–4064. PMLR, 2018. 1
[58] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 4195–4205,
2023. 1, 2, 4, 5, 6, 7, 8
[59] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,
Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael
Chung, Matteo Grella, Kranthi Kiran GV , et al. Rwkv:
Reinventing rnns for the transformer era. arXiv preprint
arXiv:2305.13048 , 2023. 2, 3, 4, 8
[60] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In Proceedings of the AAAI con-
ference on artificial intelligence , 2018. 8
[61] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu,
Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,
and Christopher R ´e. Hyena hierarchy: Towards larger con-
volutional language models. In International Conference on
Machine Learning , pages 28043–28078. PMLR, 2023. 8
[62] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically
gated recurrent neural network for sequence modeling. Ad-
vances in Neural Information Processing Systems , 36, 2024.
8
[63] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. 2018. 2
[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 1,
2
[65] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International confer-
ence on machine learning , pages 8821–8831. Pmlr, 2021. 1
[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 7
[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 5, 7, 8
[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deeplanguage understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 7
[69] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in neural information processing
systems , 29, 2016. 6
[70] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-
xl: Scaling stylegan to large diverse datasets. In ACM SIG-
GRAPH 2022 conference proceedings , pages 1–10, 2022. 7
[71] Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen
Basri, and Yuval Kluger. Spectralnet: Spectral clustering us-
ing deep neural networks. arXiv preprint arXiv:1801.01587 ,
2018. 8
[72] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi,
and Hongsheng Li. Efficient attention: Attention with lin-
ear complexities. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 3531–
3539, 2021. 8
[73] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick
LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu,
Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti,
et al. Using deepspeed and megatron to train megatron-turing
nlg 530b, a large-scale generative language model. arXiv
preprint arXiv:2201.11990 , 2022. 2
[74] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 3, 6, 7
[75] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in neural
information processing systems , 32, 2019. 7
[76] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020.
[77] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469 , 2023. 7
[78] Asa Cooper Stickland and Iain Murray. Bert and pals: Pro-
jected attention layers for efficient adaptation in multi-task
learning. In International Conference on Machine Learning ,
pages 5986–5995. PMLR, 2019. 2
[79] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-
Cheng Juan. Sparse sinkhorn attention. In International
Conference on Machine Learning , pages 9438–9447. PMLR,
2020. 8
[80] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
Efficient transformers: A survey. ACM Computing Surveys ,
55(6):1–28, 2022. 2
[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1
[82] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020. 8
[83] Yuxin Wu and Kaiming He. Group normalization. In Pro-
ceedings of the European conference on computer vision
(ECCV) , pages 3–19, 2018. 8
11

--- PAGE 12 ---
[84] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush.
Diffusion models without attention. arXiv preprint
arXiv:2311.18257 , 2023. 7, 8
[85] Xu Yan, Zhengcong Fei, Zekang Li, Shuhui Wang, Qing-
ming Huang, and Qi Tian. Semi-autoregressive image cap-
tioning. In Proceedings of the 29th ACM International Con-
ference on Multimedia , pages 2708–2716, 2021. 2
[86] Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting
Zhao, and Shihao Ji. Your vit is secretly a hybrid
discriminative-generative diffusion model. arXiv preprint
arXiv:2208.07791 , 2022. 6
[87] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efficient transformer for high-resolution image
restoration. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 5728–5739,
2022. 2
[88] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima
Anandkumar. Fast training of diffusion models with masked
transformers. arXiv preprint arXiv:2306.09305 , 2023. 8
[89] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang,
Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond
efficient transformer for long sequence time-series forecast-
ing. In Proceedings of the AAAI conference on artificial in-
telligence , pages 11106–11115, 2021. 2
[90] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,
Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient
visual representation learning with bidirectional state space
model. arXiv preprint arXiv:2401.09417 , 2024. 2, 4
12
