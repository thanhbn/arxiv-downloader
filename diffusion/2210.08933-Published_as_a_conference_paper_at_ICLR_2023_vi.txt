# 2210.08933.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2210.08933.pdf
# Kích thước tệp: 1021637 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
DIFFU SEQ: SINH TẠO VĂN BẢN TỪ CỬA KHẨU ĐẾN CỬA KHẨU
VỚI MÔ HÌNH KHUẾCH TÁN
Shansan Gong1, Mukai Li1, Jiangtao Feng1, Zhiyong Wu1, Lingpeng Kong2
1Shark-NLP, Phòng thí nghiệm AI Thượng Hải2Đại học Hồng Kông
fgongshansan,limukai,fengjiangtao,wuzhiyong g@pjlab.org.cn
lpk@cs.hku.hk
TÓM TẮT
Gần đây, các mô hình khuếch tán đã nổi lên như một mô hình mới cho các mô hình sinh tạo. Mặc dù thành công trong các lĩnh vực sử dụng tín hiệu liên tục như thị giác và âm thanh, việc điều chỉnh các mô hình khuếch tán cho ngôn ngữ tự nhiên vẫn chưa được khám phá đầy đủ do tính chất rời rạc của văn bản, đặc biệt là đối với sinh tạo có điều kiện. Chúng tôi giải quyết thách thức này bằng cách đề xuất D IFFU SEQ: một mô hình khuếch tán được thiết kế cho các nhiệm vụ sinh tạo văn bản từ chuỗi đến chuỗi (S EQ2SEQ). Thông qua đánh giá rộng rãi trên một loạt các nhiệm vụ S EQ2SEQ, chúng tôi thấy D IFFU SEQ đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với sáu mô hình cơ sở đã được thiết lập, bao gồm một mô hình hiện đại dựa trên các mô hình ngôn ngữ được huấn luyện trước. Ngoài chất lượng, một tính chất thú vị của D IFFU SEQ là tính đa dạng cao trong quá trình sinh tạo, điều này được mong muốn trong nhiều nhiệm vụ S EQ2SEQ. Chúng tôi cũng bao gồm một phân tích lý thuyết tiết lộ mối liên hệ giữa D IFFU SEQ và các mô hình tự hồi quy/không tự hồi quy. Kết hợp phân tích lý thuyết và bằng chứng thực nghiệm, chúng tôi chứng minh tiềm năng to lớn của các mô hình khuếch tán trong các nhiệm vụ sinh tạo ngôn ngữ có điều kiện phức tạp.1
1 GIỚI THIỆU
Trong số các mô hình sinh tạo hiện có, GAN (Goodfellow et al., 2014) gặp phải vấn đề bất ổn định (Salimans et al., 2016), phải chịu sự sụp đổ chế độ (Metz et al., 2017); V AE (Kingma & Welling, 2014) phải dựa vào các mục tiêu thay thế để xấp xỉ việc huấn luyện khả năng cực đại và các mô hình dựa trên Flow (Dinh et al., 2017) phải sử dụng các kiến trúc chuyên biệt để xây dựng biến đổi có thể đảo ngược. Các mô hình khuếch tán (Ho et al., 2020; Nichol & Dhariwal, 2021) đã vượt qua một số hạn chế này và nổi lên như một mô hình mới cho các mô hình sinh tạo, được hỗ trợ lý thuyết bởi nhiệt động học phi cân bằng (Sohl-Dickstein et al., 2015) và mạng khớp điểm (Song & Ermon, 2019). Cho đến nay, các đột phá lớn là trong các lĩnh vực sử dụng tín hiệu liên tục, như thị giác (Saharia et al., 2022a;b; Ramesh et al., 2022) và âm thanh (Kong et al., 2020). Tuy nhiên, việc mở rộng các mô hình khuếch tán liên tục cho ngôn ngữ tự nhiên vẫn là một thách thức mở do tính chất rời rạc vốn có của văn bản.

Dựa trên việc sinh tạo không có điều kiện trong không gian liên tục được minh họa trong Hình 1(a), các nỗ lực hiện tại (Hoogeboom et al., 2021; Austin et al., 2021) bắt đầu tùy chỉnh các mô hình khuếch tán cho văn bản trong không gian rời rạc về mô hình ngôn ngữ không có điều kiện (tức là sinh tạo văn bản tự do). Diffusion-LM (Li et al., 2022), như trong Hình 1(b), mô hình hóa văn bản trong không gian liên tục và đề xuất sử dụng một bộ phân loại được huấn luyện thêm làm hướng dẫn (tức là tín hiệu điều kiện x) để áp đặt các thay đổi tinh tế (thường là các ràng buộc phức tạp, chi tiết) trên các câu được sinh tạo. Tuy nhiên, các mô hình này không khái quát hóa một cách tự nhiên cho mô hình ngôn ngữ có điều kiện (tức là mô hình gán xác suất p(wjx) cho các chuỗi từ w cho trước x). Trong cài đặt chuỗi đến chuỗi (S EQ2SEQ) tổng quát hơn, nơi điều kiện x cũng là một chuỗi từ, việc áp dụng Diffusion-LM có thể khó khăn. Lý do là các bộ phân loại hướng thuộc tính, và chúng ta không thể huấn luyện hàng trăm nghìn bộ phân loại để mô hình hóa ý nghĩa ngữ nghĩa giữa các điều kiện và các câu được sinh tạo.

SEQ2SEQ là một cài đặt thiết yếu trong NLP bao gồm một loạt các nhiệm vụ quan trọng như sinh tạo câu mở, đối t화, paraphrase, và chuyển đổi phong cách văn bản. Trong bài báo này, chúng tôi đề xuất
1Mã nguồn có sẵn tại https://github.com/Shark-NLP/DiffuSeq
1arXiv:2210.08933v3  [cs.CL]  14 Feb 2023

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
t=0 t=T/2 t=T
t=0 t=T/2 t=Tt=0 t=T/2 t=T
(a) (b)
(c)
quá trình tiến 𝑞(⋅) quá trình ngược 𝑝𝜃(⋅)
(a)Quá trình khuếch tán Gaussian không điều kiện trong không gian liên tục
(b) Quá trình khuếch tán có hướng dẫn bằng bộ phân loại (Diffusion -LM)
(c) Khuếch tán không cần bộ phân loại được hướng dẫn bởi các điểm trong không gian ( DiffuSeq )tín hiệu điều kiện không gian ẩn của văn bản hướng dẫn
Hình 1: Mô tả về các mô hình khuếch tán không điều kiện, có hướng dẫn bộ phân loại, và không cần bộ phân loại.
DIFFU SEQ, được miêu tả trong Hình 1(c), một mô hình khuếch tán không cần bộ phân loại hỗ trợ các nhiệm vụ sinh tạo văn bản S EQ2SEQ. Bằng cách mô hình hóa xác suất có điều kiện của câu đích w cho trước ngữ cảnh x sử dụng một mô hình duy nhất, một lợi thế của D IFFU SEQ là mô hình này cho phép một mô hình hoàn chỉnh phù hợp với phân phối dữ liệu và sử dụng hướng dẫn có điều kiện, thay vì phụ thuộc vào một bộ phân loại riêng biệt.

Khác với các phương pháp sinh tạo chuẩn theo cách tự hồi quy (AR) từ trái sang phải (Radford et al., 2019), D IFFU SEQ sinh tạo các token văn bản song song theo cách không tự hồi quy (NAR). Để chứng thực hiệu quả của D IFFU SEQ, chúng tôi tiến hành thí nghiệm trên bốn nhiệm vụ S EQ2SEQ. So với các mô hình AR và NAR, vốn gặp phải vấn đề "suy thoái" (Holtzman et al., 2019) và phụ thuộc vào các chiến lược giải mã, D IFFU SEQ có thể đạt được tính đa dạng ở cấp độ câu đáng kể mà không hy sinh chất lượng (xem § 4.2).

Tóm lại, chúng tôi đưa ra một loạt các đóng góp kỹ thuật và khái niệm: (a) chúng tôi là những người đầu tiên triển khai mô hình khuếch tán cho sinh tạo văn bản S EQ2SEQ, và D IFFU SEQ được đề xuất của chúng tôi như một mô hình ngôn ngữ có điều kiện được huấn luyện đầu cuối đến cuối theo cách không cần bộ phân loại; (b) chúng tôi thiết lập một mối liên hệ lý thuyết giữa các mô hình AR, NAR và D IFFU SEQ, và chứng minh D IFFU SEQ như một mở rộng của các mô hình iterative-NAR; (c) với bằng chứng thực nghiệm mạnh mẽ, chúng tôi chứng minh tiềm năng to lớn của các mô hình khuếch tán trong các nhiệm vụ sinh tạo ngôn ngữ có điều kiện phức tạp.

2 KIẾN THỨC CƠ BẢN VÀ PHÁT BIỂU BÀI TOÁN
Kiến thức cơ bản. Một mô hình khuếch tán thường chứa các quá trình tiến và ngược. Cho một điểm dữ liệu được lấy mẫu từ một phân phối dữ liệu thực tế z0q(z), quá trình tiến dần dần làm hỏng z0 thành một nhiễu Gaussian chuẩn zTN (0;I). Đối với mỗi bước tiến t2[1;2;:::;T ], nhiễu loạn được điều khiển bởi q(ztjzt1) =N(zt;p1tzt1;tI), với t2(0;1) là các tỷ lệ phương sai khác nhau. Khi quá trình tiến hoàn tất, quá trình khử nhiễu ngược cố gắng dần dần tái tạo dữ liệu gốc z0 thông qua việc lấy mẫu từ zT bằng cách học một mô hình khuếch tán f.

Phát biểu bài toán. Nhiều nỗ lực gần đây đã được dành cho việc điều chỉnh các mô hình khuếch tán cho văn bản rời rạc (Xem § 5). Tuy nhiên, tất cả đều tập trung vào mô hình chuỗi không điều kiện. Trong bài báo này, chúng tôi nhắm vào các nhiệm vụ sinh tạo văn bản chuỗi đến chuỗi. Cụ thể, cho một chuỗi nguồn có độ dài m wx=fwx1;:::;wxmg, chúng tôi muốn học một mô hình khuếch tán có thể tạo ra một chuỗi đích có độ dài n wy=fwy1;:::;wyngcó điều kiện trên chuỗi nguồn.

3 D IFFU SEQ
Chúng tôi đề xuất D IFFU SEQ để mở rộng các mô hình khuếch tán vanilla để học sinh tạo văn bản có điều kiện (như được hiển thị trong Hình 2), liên quan đến kiến trúc mô hình và mục tiêu huấn luyện.

Quá trình tiến với nhiễu một phần. Ở đầu quá trình tiến, chúng tôi làm theo Diffusion-LM (Li et al., 2022) để thiết kế một hàm embedding E MB(w) để ánh xạ văn bản rời rạc w vào không gian liên tục. Cụ thể, cho một cặp chuỗi wx và wy, DIFFU SEQ học một
2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Nhiễu Gaussian một phần z𝑇z𝑡z𝑡−1z0… …𝑝𝜃(z𝑡−1|z𝑡)
𝑞(z𝑡|z𝑡−1)
Word Embeddings Văn bản w𝑥
w𝑦
Ánh xạ embedding 𝑝𝜃(w|z0)
𝑞𝜙(z0|w)Chuyến đi kéo dài
bao lâu?
Một năm.
Chuỗi đến chuỗi Ví dụ: Đối thoại mở E.g. Open-Domain DialogueQuá trình tiến Quá trình ngược Nhiễu Gaussian Làm tròn
Hình 2: Quá trình khuếch tán của mô hình ngôn ngữ khuếch tán có điều kiện D IFFU SEQ. Cho nguồn wx và đích wy, chúng tôi biến đổi chúng theo cặp vào không gian liên tục z0. Nhiễu Gaussian một phần được lặp lại thêm vào không gian đích của zt.

không gian đặc trưng thống nhất của wx và wy bằng biến đổi embedding và nối như E MB(wxy) = [EMB(wx1);:::;EMB(wxm);EMB(wy1);:::;EMB(wyn)]2R(m+n)d. Biến đổi cho phép chúng tôi điều chỉnh đầu vào văn bản rời rạc vào quá trình tiến chuẩn, bằng cách mở rộng chuỗi tiến ban đầu thành một chuyển tiếp Markov mới q(z0jwxy) =N(EMB(wxy);0I).

Chúng tôi ký hiệu zt=xtyt để đơn giản hóa cách diễn đạt, trong đó xt và yt đại diện cho các phần của zt thuộc về wx và wy, tương ứng. Đối với mỗi bước tiến q(ztjzt1), chúng tôi dần dần tiêm nhiễu vào trạng thái ẩn của bước cuối zt1 để có được zt. Không như các mô hình khuếch tán thông thường làm hỏng toàn bộ zt (cả xt và yt) không phân biệt, chúng tôi chỉ áp đặt nhiễu lên yt. Sự sửa đổi này (được gọi là nhiễu một phần) cho phép chúng tôi điều chỉnh các mô hình khuếch tán cho mô hình ngôn ngữ có điều kiện.

Quá trình ngược với khử nhiễu có điều kiện. Mục tiêu cuối cùng của quá trình ngược là khôi phục z0 gốc bằng cách khử nhiễu zt:p(z0:T) :=p(zT)QTt=1p(zt1jzt). Chúng tôi mô hình hóa quá trình học p(zt1jzt) =N(zt1;(zt;t);(zt;t)) sử dụng mô hình khuếch tán được đề xuất DIFFU SEQ:f(zt;t), trong đó () và () là tham số hóa của trung bình và độ lệch chuẩn dự đoán của q(zt1jzt) trong quá trình tiến, được suy ra sử dụng quy tắc Bayes. Các derivation chi tiết có trong Phụ lục A. Với chiến lược nhiễu một phần được áp dụng trong quá trình tiến, chúng tôi có thể áp đặt đầu vào như điều kiện khi khử nhiễu như được hiển thị trong Hình 1. Việc khử nhiễu có điều kiện được đề xuất là không cần bộ phân loại theo bản chất: chúng tôi không yêu cầu các bộ phân loại được huấn luyện thêm để điều khiển quá trình khử nhiễu.

Cụ thể, chúng tôi sử dụng kiến trúc transformer để mô hình hóa f, tự nhiên mô hình hóa mối quan hệ ngữ nghĩa giữa xt và yt. Chúng tôi tính toán cận dưới biến thiên ( LVLB) theo quá trình khuếch tán gốc. Lround tương ứng với phép toán làm tròn trong Hình 2.

LVLB=Eq(z1:Tjz0)"
logq(zTjz0)
p(zT)|{z}
LT+TX
t=2logq(zt1jz0;zt)
p(zt1jzt)|{z}
Lt1
+ logq(z0jwxy)
p(z0jz1)|{z}
L0logp(wxyjz0)
|{z}
Lround#
:(1)

Chúng tôi tiếp tục đơn giản hóa mục tiêu huấn luyện như sau (chi tiết trong Phụ lục A):
min
LVLB= min
"TX
t=2jjz0f(zt;t)jj2+jjEMB(wxy)f(z1;1)jj2logp(wxyjz0)#
!min
"TX
t=2jjy0~f(zt;t)jj2+jjEMB(wy)~f(z1;1)jj2+R(jjz0jj2)#
; (2)

ở đây chúng tôi sử dụng ~f(zt;t) để ký hiệu các phần của z0 được khôi phục tương ứng với y0. Lưu ý rằng mặc dù trong số hạng đầu tiên, chúng tôi chỉ tính toán mất mát w.r.t y0, do cơ chế attention trong transformer, việc tái tạo y0 cũng tính đến x0, do đó gradient từ số hạng đầu tiên
3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
cũng sẽ ảnh hưởng đến việc học x0. Số hạng regularization tương đương về mặt toán học R(jjz0jj2)) điều chỉnh việc học embedding. Chúng tôi cũng chia sẻ hàm embedding giữa các chuỗi nguồn và đích, cho phép huấn luyện hai không gian đặc trưng khác nhau cùng nhau. Điều này đặt D IFFU SEQ tách biệt với các giải pháp hiện có trong thị giác như GLIDE (Nichol et al., 2022).

Phương pháp huấn luyện và suy luận. Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng tính đa dạng cao trong các tập dữ liệu NLP và các bước khuếch tán dài thường dẫn đến huấn luyện không đủ. Chúng tôi giả thuyết lý do là việc lấy mẫu bước t đồng nhất gây ra nhiễu không cần thiết trong mục tiêu LVLB. Do đó chúng tôi sử dụng lấy mẫu có trọng số quan trọng (Nichol & Dhariwal, 2021) để giải quyết vấn đề này.

LVLB=EtptLt
pt
; pt/q
E[L2
t];PT1
t=0pt= 1: (3)

Theo trực giác, thuật toán lấy mẫu có trọng số quan trọng sẽ dành nhiều bước hơn cho các bước khuếch tán có Lt lớn hơn, và ngược lại.

Để thực hiện sinh tạo S EQ2SEQ cho trước điều kiện E MB(wx), chúng tôi lấy mẫu ngẫu nhiên yT
N(0;I) và nối yT với E MB(wx) để có được zT. Bây giờ chúng tôi có thể lặp lại quá trình ngược cho đến khi đến z0. Tại mỗi bước lấy mẫu, một hàm neo được thực hiện hướng về zt được tham số hóa lại. Cụ thể, hàm neo: (a) thực hiện làm tròn trên zt để ánh xạ nó trở lại không gian word embedding theo Li et al. (2022); (b) thay thế phần của zt1 được khôi phục thuộc về wx bằng x0 gốc, xem xét rằng phần này được khôi phục từ zt bị hỏng qua f và không nghiêm ngặt bằng x0. Lưu ý rằng (b) được thiết kế cho D IFFU SEQ.

Để cải thiện chất lượng sinh tạo, chúng tôi áp dụng chiến lược giải mã Minimum Bayes Risk (MBR) được sử dụng rộng rãi (Koehn, 2004). Đầu tiên chúng tôi sinh tạo một tập hợp các mẫu ứng viên S từ các seed ngẫu nhiên khác nhau của D IFFU SEQ và chọn chuỗi đầu ra tốt nhất đạt được rủi ro kỳ vọng tối thiểu dưới một hàm mất mát có ý nghĩa (ví dụ: BLEU hoặc các metric rẻ hơn khác như precision). Trong thực tế, chúng tôi sử dụng điểm BLEU âm trong triển khai của mình.

Mối liên hệ với các mô hình AR, Iter-NAR, và Fully-NAR. Để hiểu rõ hơn về hành vi của DIFFU SEQ, chúng tôi đưa ra mối liên hệ lý thuyết với các mô hình tự hồi quy (AR), không tự hồi quy lặp (iter-NAR), và không tự hồi quy hoàn toàn (fully-NAR). Chúng tôi lập luận rằng D IFFU SEQ có thể được xem như một mở rộng của mô hình iter-NAR. Các sự khác biệt học tập đồ họa chi tiết của bốn trường hợp này được thảo luận trong Phụ lục B để tham khảo.

Các mô hình AR học p(wy1:njwx) bằng phân tích tự hồi quy dựa trên ngữ cảnh trái:
pAR(wy1:njwx) =p(wy1jwx)
|{z}
dự đoán ban đầu Y
i=1;:::;n1p(wyi+1jwy1:i;wx)
|{z}
dự đoán ngữ cảnh trái tiến bộ; (4)

trong khi các mô hình fully-NAR (Gu et al., 2018; Qian et al., 2021) học xác suất có điều kiện cho trước giả định độc lập để suy luận nhanh:
pfully-NAR (wy1:njwx) =Y
i=1;:::;np(wyijwx): (5)

Để làm một phép tương tự tốt hơn với các mô hình AR và NAR, chúng tôi sử dụng một cách không mất mát để công thức hóa các mô hình iterative NAR (Gu et al., 2019; Ghazvininejad et al., 2019) bằng cách giới thiệu một loạt các chuỗi trung gian wy1:K1;wyK=wy với K lần lặp có thể chỉnh sửa:
piter-NAR (wy1:njwx) =X
wy1;:::;wyK1Y
i=1:::np(wy1;ijwx)
|{z}
dự đoán ban đầu Y
k=1::K1Y
i=1:::np(wyk+1;ijwyk;1:n;wx)
| {z }
dự đoán ngữ cảnh đầy đủ tiến bộ:
(6)

Nghiên cứu trước đây (Huang et al., 2022) cho thấy có một khoảng cách gọi là tương quan tổng có điều kiện giữa các mô hình học AR Eq. (4) và fully-NAR Eq. (5), vì phân tích mất mát của các mô hình NAR. Tuy nhiên, khi so sánh các mô hình iter-NAR Eq. (6) với AR Eq. (4), cả hai đều có thể được phân tích thành một số hạng dự đoán ban đầu và một quá trình dự đoán tiến bộ dựa trên ngữ cảnh khác nhau (tức là ngữ cảnh trái trong AR và ngữ cảnh đầy đủ trong iter-NAR), và sự khác biệt được chỉ ra bởi
4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Huang et al. (2022) do đó được đóng lại trong iter-NAR giả định đủ bước. Bằng cách hiển thị D IF-
FUSEQ là một mở rộng của mô hình iter-NAR, chúng tôi đưa ra một lời biện minh rằng nó sẽ không gặp phải tương quan tổng có điều kiện vì cùng lý do.

Một cách thẳng thắn để công thức hóa các mô hình khuếch tán liên tục thuần túy là giới thiệu một loạt các đặc trưng bị hỏng bởi nhiễu Gaussian cùng với các bước khuếch tán: y1:T1;y0=y;yTN(0;I).
pdiffusion (wyjwx) =Z
yT;:::;y0p(wyjy0;wx)
|{z}
dự đoán cuối cùng Y
t=T;:::; 1p(yt1jyt;wx)
|{z}
khuếch tán ngữ cảnh đầy đủ tiến bộ; (7)

trong đó p(yt1jyt;wx) mô tả bước khuếch tán trên các biểu diễn liên tục y. Phép toán làm tròn trong D IFFU SEQ ánh xạ các vector liên tục y thành wy rời rạc cho mỗi bước thời gian t, chúng tôi cũng giới thiệu điều này vào Eq. (7):
pDIFFU SEQ(wyjwx) =X
wyT;:::;wy1Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1p(wytjyt;wx)p(yt1jwy
t) (8)

=X
wyT;:::;wy1Z
yT;:::;y0p(wyTjyT;wx)Y
t=T1;:::;0p(ytjwyt+1)p(wytjyt;wx):(9)

Bằng cách sắp xếp lại Eq. (8) thành Eq. (9), chúng ta có thể thấy D IFFU SEQ có thể được xem như một dạng tổng quát hóa hơn của iter-NAR Eq. (6) trước khi marginalize fyT;:::;y0g, mặc dù khởi tạo khác nhau của yT2. Một derivation chi tiết hơn được hiển thị trong Phụ lục C.

4 THÍ NGHIỆM
Chúng tôi tiến hành thí nghiệm để xác thực hiệu quả của D IFFU SEQ trên bốn nhiệm vụ khác nhau, so với sáu baseline AR/NAR mạnh.

4.1 THIẾT LẬP THÍ NGHIỆM
Nhiệm vụ và tập dữ liệu. Sinh tạo SEQ2SEQ bao gồm một loạt các nhiệm vụ, trong đó chúng tôi chọn bốn nhiệm vụ điển hình và phổ biến. Đối thoại miền mở yêu cầu các mô hình sinh tạo phản hồi thông tin cho trước một ngữ cảnh đối thoại. Chúng tôi sử dụng Commonsense Conversation Dataset (Zhou et al., 2018), được trích xuất từ các cuộc đối thoại Reddit một lượt, với hơn 3 triệu cặp hội thoại. Sinh tạo câu hỏi (QG) nhằm sinh tạo câu hỏi cho trước một ngữ cảnh đầu vào. Để có được đủ mẫu huấn luyện, chúng tôi sử dụng tập dữ liệu Quasar-T (Dhingra et al., 2017) được tiền xử lý bởi Lin et al. (2018), và sau đó sinh tạo các cặp tài liệu-câu hỏi để có được 119K mẫu huấn luyện (chi tiết trong Phụ lục D.1). Đơn giản hóa văn bản nhằm sửa đổi văn bản phức tạp thành các chuỗi với ngữ pháp và lựa chọn từ đơn giản. Jiang et al. (2020) xây dựng một corpus gồm 677K câu phức tạp-đơn giản với sự liên kết sửa đổi. Nhiệm vụ paraphrase sinh tạo một dạng bề mặt thay thế trong cùng ngôn ngữ thể hiện cùng nội dung ngữ nghĩa. Chúng tôi áp dụng QQP3 được sử dụng rộng rãi có nguồn gốc từ diễn đàn hỏi đáp cộng đồng Quora, với 147K cặp tích cực.

Baseline. Chúng tôi xem xét ba nhóm mô hình làm baseline, bao gồm cả kiến trúc AR và NAR. Nhóm phương pháp đầu tiên áp dụng kiến trúc encoder-decoder (Cho et al., 2014) được nghiên cứu kỹ cho các nhiệm vụ S EQ2SEQ, và chúng tôi tiến hành thí nghiệm trên hai mô hình phổ biến: GRU với attention và Transformer (Vaswani et al., 2017). Nhóm thứ hai là mô hình ngôn ngữ lớn được huấn luyện trước (PLM) được tinh chỉnh, trong đó GPT2 (Radford et al., 2019) đã chứng minh thành công lớn trong hầu hết tất cả các nhiệm vụ S EQ2SEQ. Chúng tôi tiếp tục so sánh với GPV AE (Du et al., 2022), tăng cường T5 được huấn luyện trước (Raffel et al., 2020) với V AE để cải thiện tính đa dạng sinh tạo. Đối với nhóm baseline cuối cùng, chúng tôi xem xét LevT (Gu et al., 2019), một mô hình iterative NAR mạnh được sử dụng rộng rãi. Tất cả các baseline được huấn luyện theo hướng dẫn trong các bài báo của họ, và chi tiết có thể được tìm thấy trong Phụ lục D.2.

2Đối với các mô hình NAR, yT được sao chép đồng nhất từ câu nguồn hoặc embedding token unk (Gu et al., 2018); đối với các mô hình khuếch tán, yT được lấy mẫu từ phân phối chuẩn N(0;I).
3https://www.kaggle.com/c/quora-question-pairs
5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 1: Kết quả tổng thể của các phương pháp khác nhau trên các nhiệm vụ S EQ2SEQ khác nhau. Nhóm phương pháp đầu tiên áp dụng kiến trúc encoder-decoder tự hồi quy và nhóm thứ hai là mô hình ngôn ngữ lớn được huấn luyện trước được tinh chỉnh (cũng theo cách tự hồi quy) trong khi nhóm cuối cùng là không tự hồi quy. Kết quả tốt nhất được in đậm , và kết quả tốt nhất không có PLM được gạch dưới .
Nhiệm vụ Phương pháp BLEU"R-L" Score" dist-1"selfB#/ div-4" Len
Đối thoại
miền mở GRU-attention0.0068 0.1054 0.4128 0.8998 0.8008/0.1824 4.46
Transformer-base0.0189 0.1039 0.4781 0.7493 0.3698/0.6472 19.5
GPT2-base FT0.0108 0.1508 0.5279 0.9194 0.0182/0.9919 16.8
GPT2-large FT0.0125 0.1002 0.5293 0.9244 0.0213/0.9938 16.8
GPV AE-T50.0110 0.1009 0.4317 0.5625 0.3560/0.5551 20.1
NAR-LevTz0.0158 0.0550 0.4760 0.9726 0.7103/0.1416 4.11
DIFFU SEQ(Ours)z0.0139 0.1056 0.5131 0.9467 0.0144 /0.9971 13.6
Sinh tạo
câu hỏi GRU-attention0.0651 0.2617 0.5222 0.7930 0.9999/0.3178 10.1
Transformer-base0.1663 0.3441 0.6307 0.9309 0.3265/0.7720 10.3
GPT2-base FT0.0741 0.2714 0.6052 0.9602 0.1403 /0.9216 10.0
GPT2-large FT0.1110 0.3215 0.6346 0.9670 0.2910/0.8062 9.96
GPV AE-T50.1251 0.3390 0.6308 0.9381 0.3567/0.7282 11.4
NAR-LevTz0.0930 0.2893 0.5491 0.8914 0.9830/0.4776 6.93
DIFFU SEQ(Ours)z0.1731 0.3665 0.6123 0.9056 0.2789 /0.8103 11.5
Đơn giản hóa
văn bản GRU-attention0.3256 0.5602 0.7871 0.8883 0.9998/0.3313 18.9
Transformer-base0.2693 0.4907 0.7381 0.8886 0.6924/0.5095 18.5
GPT2-base FT0.3083 0.5461 0.8021 0.9439 0.5444/0.6047 16.1
GPT2-large FT0.2693 0.5111 0.7882 0.9464 0.6042/0.5876 15.4
GPV AE-T50.3392 0.5828 0.8166 0.9308 0.8147/0.4355 18.5
NAR-LevTz0.2052 0.4402 0.7254 0.9715 0.9907/0.3271 8.31
DIFFU SEQ(Ours)z0.3622 0.5849 0.8126 0.9264 0.4642 /0.6604 17.7
Paraphrase GRU-attention0.1894 0.5129 0.7763 0.9423 0.9958/0.3287 8.30
Transformer-base0.2722 0.5748 0.8381 0.9748 0.4483/0.7345 11.2
GPT2-base FT0.1980 0.5212 0.8246 0.9798 0.5480/0.6245 9.67
GPT2-large FT0.2059 0.5415 0.8363 0.9819 0.7325/0.5020 9.53
GPV AE-T50.2409 0.5886 0.8466 0.9688 0.5604/0.6169 9.60
NAR-LevTz0.2268 0.5795 0.8344 0.9790 0.9995/0.3329 8.85
DIFFU SEQ(Ours)z0.2413 0.5880 0.8365 0.9807 0.2732 /0.8641 11.2

Đánh giá. Chúng tôi đánh giá các chuỗi được sinh tạo từ hai khía cạnh: chất lượng và tính đa dạng. Để đánh giá chất lượng, chúng tôi sử dụng metric chuẩn BLEU (Papineni et al., 2002) và điểm ROUGE (Lin, 2004). Vì các metric dựa trên độ tương tự chuỗi có thể không thỏa mãn đối với sinh tạo mở, chúng tôi cũng báo cáo BERTScore (Zhang et al., 2019) đánh giá độ tương tự ngữ nghĩa giữa các câu được sinh tạo và tham chiếu. Chi tiết có trong Phụ lục D.4. Điểm cao hơn của BLEU, ROUGE và BERTScore phản ánh hiệu suất tốt hơn. Đối với tính đa dạng, chúng tôi sử dụng unigram riêng biệt (dist-1) để đo tính đa dạng nội bộ trong mỗi câu được sinh tạo, trong đó dist-1 thấp hơn chỉ ra rằng câu được sinh tạo chứa nhiều từ lặp lại hơn. Đối với đánh giá tính đa dạng cấp độ câu, chúng tôi xem xét self-BLEU cấp độ câu (Zhu et al., 2018) để đo sự chồng chéo n-gram giữa tập hợp đầu ra w.r.t một câu nguồn, và chúng tôi thêm vào sử dụng 4-gram đa dạng (div-4) (Deshpande et al., 2019) để đo tỷ lệ 4-gram riêng biệt trong tập hợp đầu ra trên mỗi câu nguồn. Self-BLEU thấp hơn và div-4 cao hơn gợi ý tính đa dạng sinh tạo cao hơn. Đối với mỗi phương pháp bao gồm DIFFU SEQ, chúng tôi sinh tạo 3 mẫu cho mỗi câu nguồn để tính toán các metric đa dạng.

Chi tiết triển khai. D IFFU SEQ của chúng tôi dựa trên 12 lớp Transformer với 12 attention head, trong đó time step embedding được cắm tương tự như position embedding. Độ dài chuỗi tối đa là 128, với chiều embedding d= 128, các bước khuếch tán T= 2;000 và lịch nhiễu căn bậc hai. Để giảm sinh tạo ngoài từ vựng, chúng tôi áp dụng Byte Pair Encoding (Sennrich et al., 2016) để xây dựng từ vựng. Sau khi thực hiện diversity beam
6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 2: Đầu ra mẫu trong tập kiểm tra QQP, có điều kiện trên cùng x.
Câu gốc : Làm thế nào để tôi kết bạn. Tham chiếu paraphrase : Làm thế nào để kết bạn ?
GPT2-large tinh chỉnh GPV AE-T5 DIFFU SEQ
Làm thế nào tôi có thể kết bạn? Làm thế nào tôi có thể kết bạn? Làm thế nào tôi có thể kết bạn tốt hơn?
Làm thế nào tôi có thể kết bạn? Làm thế nào để tôi kết bạn? Làm thế nào tôi có thể kết bạn?
Làm thế nào tôi có thể kết bạn? Làm thế nào tôi có thể kết bạn? Bạn kết bạn như thế nào?
Làm thế nào tôi có thể kết bạn? Cách tốt nhất để kết bạn là gì? Cách tốt nhất để kết bạn là gì?
Làm thế nào để tôi kết bạn và
giữ họ?Cách tốt nhất để kết bạn
và tạo ra bạn bè là gì?Làm thế nào tôi có thể kết bạn và làm gì
đó hơn?

search (DBS) (Vijayakumar et al., 2016) cho mô hình Transformer-base và mô hình GPT, chúng tôi thấy rằng DBS không phải lúc nào cũng thúc đẩy tính đa dạng hơn temperature sampling và do đó chúng tôi liệt kê các kết quả tính đa dạng tốt nhất. Chúng tôi tính toán các metric độ chính xác của D IFFU SEQ sử dụng MBR với kích thước mẫu ứng viên jSj= 10. Thí nghiệm được triển khai trên NVIDIA A100 Tensor Core GPU, và chúng tôi sử dụng 4 GPU để huấn luyện và GPU đơn để lấy mẫu.

4.2 KẾT QUẢ CHÍNH
Như được hiển thị trong Bảng 1, chúng tôi kết luận rằng D IFFU SEQ đạt được chất lượng sinh tạo tương đương hoặc thậm chí cao hơn so với các baseline mạnh. Đồng thời, D IFFU SEQ liên tục chứng minh tính ưu việt trong việc sinh tạo các đầu ra đa dạng cho cùng một chuỗi đầu vào.

Như chúng ta có thể thấy từ Bảng 1, D IFFU SEQ thắng cuộc thi trên ít nhất một metric chất lượng so với 6 baseline4 nhiệm vụ. Mặc dù các mô hình NAR như LevT đôi khi cũng có thể vượt trội hơn các baseline AR, chúng vẫn tụt lại sau D IFFU SEQ với khoảng cách lớn (tức là cải thiện tương đối hơn 50% cho BLEU trong nhiệm vụ QG và R-L trong nhiệm vụ Dialogue). Thậm chí so với các mô hình GPT2 được huấn luyện trước rồi tinh chỉnh, D IFFU SEQ vẫn đạt hiệu suất vượt trội hơn biến thể base, và có thể so sánh với biến thể large, có 8:2 lần tham số hơn D IFFU SEQ. Những kết quả thực nghiệm này hỗ trợ đầy đủ cho các phát hiện của chúng tôi trong § 3, nơi chúng tôi phân tích lý thuyết về tiềm năng của các mô hình khuếch tán trong việc mô hình hóa các chuỗi văn bản so với các mô hình AR cho đủ bước khuếch tán.

DIFFU SEQ, như một thành viên của họ mô hình sinh tạo sâu, cũng thể hiện khả năng sinh tạo các chuỗi có tính đa dạng cao. Như được gợi ý bởi self-BLEU (thấp hơn là tốt hơn) và div-4 (cao hơn là tốt hơn), trong hầu hết tất cả các trường hợp, D IFFU SEQ vượt trội đáng kể so với 4 baseline AR về tính đa dạng cấp độ câu (tức là tạo ra các đầu ra đa dạng cho cùng một đầu vào). Đối với tính đa dạng trong lựa chọn từ trong một câu, chúng tôi xem xét dist-1: dist-1 cao hơn chỉ ra ít lặp lại hơn trong một câu. Như chúng ta có thể thấy từ Bảng 1, D IFFU SEQ có ít lặp lại hơn so với các phương pháp encoder-decoder, nhưng vẫn tụt lại sau các mô hình GPT2 được huấn luyện trước (tình huống tương tự với BERTScore). Những kết quả này gợi ý vẫn còn chỗ để cải thiện (ví dụ: sử dụng kỹ thuật huấn luyện trước) trong lựa chọn token cấp độ của các mô hình khuếch tán. Khác với NAR-LevT, D IFFU SEQ không dựa vào một mô-đun dự đoán độ dài thêm mà tự động quyết định bằng padding token thay vào đó và có thể sinh tạo các câu đầu ra dài hơn, được chỉ ra bởi cột cuối cùng cho độ dài sinh tạo trung bình.

Trong Bảng 2, chúng tôi cung cấp ví dụ để thể hiện khả năng sinh tạo mẫu đa dạng của D IFFU SEQ. Nhiều ví dụ hơn có thể được tìm thấy trong Phụ lục D.5.

1 3 5 7 10 15 20
kích thước ứng viên ||
0.300.310.320.330.340.350.36 BLEU
Đơn giản hóa văn bản
DiffuSeq
GPT2-base
1 3 5 7 10 15 20
kích thước ứng viên ||
0.190.200.210.220.230.240.25
Paraphrase
DiffuSeq
GPT2-large
Hình 3: Sự tăng của điểm BLEU với
các kích thước ứng viên khác nhau jSj.
0.5 0.6 0.7 0.8 0.9div-40.060.080.100.120.140.16 BLEU
Sinh tạo câu hỏi
DiffuSeq
GPT2-base
GPT2-largeNAR-LevT
GPVAE-T5Hình 4: Sự đánh đổi giữa chất lượng và
tính đa dạng (chi tiết trong Phụ lục D.3).
7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
0 20 40 60 80 100Quá trình sinh tạo %0.000.050.100.150.200.250.30 BLEU
DiffuSeq
NAR-LevT
DiffuSeq
NAR-LevT
0.00.20.40.60.81.0
div-4
Đơn giản hóa văn bản
Hình 5: Đường cong điểm BLEU/div-4 theo
quá trình sinh tạo (phần trăm các bước).
100 500 1000 2000
Bước lấy mẫu0.0000.0250.0500.0750.1000.1250.150 BLEUGPT2-large BLEU
02468101214
Mẫu sinh tạo/giây
GPT2-large speedSinh tạo câu hỏi Hình 6: BLEU và tốc độ suy luận
của D IFFU SEQ và GPT2-large.

4.3 PHÂN TÍCH
Chúng tôi tiến hành một loạt phân tích để điều tra hiệu quả của các khía cạnh khác nhau trong D IFFU SEQ.

Tính đa dạng đảm bảo chất lượng. Sinh tạo văn bản chất lượng cao với tính đa dạng cao là một yêu cầu quan trọng cho nhiều ứng dụng sinh tạo văn bản và sự đánh đổi giữa chất lượng và tính đa dạng luôn là mối quan tâm quan trọng trong các nhiệm vụ NLG mở (Zhang et al., 2021). Khác với các mô hình AR phụ thuộc vào chiến lược giải mã như temperature và nucleus sampling (Holtzman et al., 2019) và các mô hình V AE lấy mẫu biến latent từ Gaussian Prior, lợi thế tự nhiên của D IFFU SEQ là sinh tạo các câu khác nhau cùng với một loạt nhiễu Gaussian ngẫu nhiên. Trong Hình 4, chúng tôi làm sáng tỏ rằng D IFFU SEQ có sự đánh đổi tốt hơn giữa chất lượng sinh tạo (BLEU) và tính đa dạng cấp độ câu (div-4). Ở đây chúng tôi tiếp tục chứng minh rằng tính đa dạng cao được cung cấp bởi D IFFU SEQ có thể được biến thành chất lượng tốt hơn.

MBR là một chiến lược phổ biến để cải thiện chất lượng sinh tạo bằng cách tổng hợp và xếp hạng các chuỗi ứng viên, và chúng tôi thấy rằng cận trên của MBR được quyết định bởi một tập hợp ứng viên đa dạng. Để xác thực điều này, chúng tôi đồng thời áp dụng MBR trên cả D IFFU SEQ và GPT2 với các kích thước ứng viên khác nhau jSj. Kết quả được hiển thị trong Hình 3. Như chúng ta có thể thấy, D IFFU SEQ tụt lại sau GPT2 khi không sử dụng MBR (jSj= 1) hoặc với một tập hợp ứng viên nhỏ (jSj= 3). Tuy nhiên, khi jSj tăng, D IFFU SEQ bắt đầu vượt trội hơn GPT2 với khoảng cách ngày càng tăng. Lý do là các mô hình tự hồi quy như GPT2 có xu hướng sinh tạo các ứng viên rất giống nhau (như đã thảo luận trong § 4.2), điều này cản trở hiệu quả của MBR. Khi jSj tăng lên 20, DIFFU SEQ vẫn cho thấy xu hướng tăng tốt hơn GPT2. Các phát hiện của chúng tôi cũng nhấn mạnh tầm quan trọng của các phương pháp xếp hạng tốt hơn trong nghiên cứu khuếch tán.

Phân tích theo từng bước so với Iterative NAR. Cho mối liên hệ lý thuyết cơ bản giữa iterative NAR và D IFFU SEQ được thảo luận trong § 3, chúng tôi điều tra thực nghiệm hành vi của LevT và D IFFU SEQ bằng cách phân tích các đường cong chất lượng (tức là BLEU) và tính đa dạng (tức là div-4) theo từng bước của chúng. Như được gợi ý trong Hình 5, LevT tăng mạnh về chất lượng ngay từ đầu quá trình sinh tạo, và nhanh chóng chậm lại trong quá trình tinh chỉnh kế tiếp. Nhưng D IFFU SEQ hành xử khác biệt, với điểm BLEU tăng chậm lúc đầu, tăng nhanh khi quá trình khuếch tán tiến triển và cuối cùng vượt qua LevT. Cũng quan sát thấy rằng tính đa dạng của cả LevT và D IFFU SEQ được xác định ở giai đoạn rất sớm bất kể việc tinh chỉnh hoặc khuếch tán tương lai, nơi D IFFU SEQ liên tục vượt trội hơn LevT về tính đa dạng ở bất kỳ giai đoạn nào của quá trình sinh tạo. Chúng tôi đoán rằng D IFFU SEQ khám phá nhiều kết quả có thể hơn ở nửa đầu của quá trình sinh tạo, và sớm hội tụ về một số ứng viên tiềm năng khi nó gần cuối các bước. Trong trường hợp này, D IFFU SEQ cho thấy khả năng tính đến cả chất lượng sinh tạo và tính đa dạng, và đây là khả năng mà các mô hình iterative-NAR và thậm chí AR không thể đạt được, do các mô hình học khác biệt.

Tốc độ suy luận. Tốc độ lấy mẫu chậm là một trong những mối quan tâm lớn về các mô hình khuếch tán. Ở đây chúng tôi cố định số bước khuếch tán trong quá trình huấn luyện cho D IFFU SEQ trong khi thu hẹp các bước suy luận theo DDIM (Song et al., 2020). Như chúng ta có thể thấy từ Hình 6, khi giảm suy luận xuống 1.000 bước khuếch tán trên GPU đơn, D IFFU SEQ đạt được điểm BLEU cao hơn GPT2-large nhưng đăng ký tốc độ suy luận gần hơn với GPT2-large.

Hiệu quả của huấn luyện chung. Trong D IFFU SEQ, các biểu diễn của wx và wy được huấn luyện chung sử dụng cùng hàm embedding E MB() (được nêu trong § 3). Để xác thực hiệu quả
8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 3: Kết quả có hoặc không có huấn luyện chung cho nhiệm vụ Sinh tạo câu hỏi.
Cài đặt BLEU"R-L" Score"selfB#/ div-4"
DIFFU SEQ(w/o reranking) 0.1567 0.3484 0.5947 0.2789/0.8103
Cố định E MB(wx) như đã huấn luyện trước 0.0110 0.0687 0.3769 0.0174/0.9376

của chiến lược huấn luyện chung này, chúng tôi so sánh nó với chiến lược huấn luyện thường được sử dụng trong các mô hình khuếch tán text-to-image (Nichol et al., 2022; Ramesh et al., 2022). Cụ thể, chúng tôi tách rời việc huấn luyện E MB(wx) và E MB(wy) bằng cách thay thế E MB(wx) bằng các biểu diễn được trích xuất từ mô hình BERT-tiny đã huấn luyện trước (Turc et al., 2019). Từ Bảng 3, chúng tôi thấy rằng chiến lược huấn luyện tách rời dẫn đến hiệu suất kém.

5 CÔNG TRÌNH LIÊN QUAN
Các mô hình khuếch tán cho mô hình hóa văn bản. Sinh tạo Text-to-Image sử dụng các mô hình khuếch tán đã phát triển nhiều ứng dụng tiềm năng. Các mô hình như Imagen (Saharia et al., 2022b) và DALL-E (Ramesh et al., 2022) thường là hai giai đoạn dựa vào các mô hình được huấn luyện trước, yêu cầu sự liên kết giữa các vector embedding từ hai nguồn. GLIDE (Nichol et al., 2022) khám phá mô hình khuếch tán với hướng dẫn không cần bộ phân loại (Ho & Salimans, 2022) bằng cách thiết lập thang hướng dẫn trong quá trình huấn luyện. Không gian đích của các mô hình này không phải là không gian văn bản rời rạc mà là các vector ổn định của giá trị pixel. Có những công trình khác về khuếch tán trong sinh tạo văn bản, nhưng chúng bám vào kiến trúc encoder-decoder gốc và quá trình khuếch tán được xen kẽ trên decoder (Savinov et al., 2021), hoặc không gian latent (Yu et al., 2022).

Đối với sinh tạo văn bản sử dụng các mô hình khuếch tán, Hoogeboom et al. (2021) giới thiệu khuếch tán đa thức cho sinh tạo văn bản cấp độ ký tự, nhiễu phân loại tiến được áp dụng thông qua ma trận chuyển tiếp Markov. Austin et al. (2021) tổng quát hóa các mô hình khuếch tán văn bản rời rạc bằng cách giới thiệu trạng thái hấp thụ ([MASK]). Tuy nhiên, các mô hình khuếch tán rời rạc có thể gặp phải vấn đề về tỷ lệ của các vector hàng one-hot, và chúng chỉ sinh tạo mẫu văn bản một cách không điều kiện trong không gian rời rạc. Diffusion-LM (Li et al., 2022) và Analog Bits (Chen et al., 2022) đề xuất một mô hình ngôn ngữ mới được khuếch tán trên các biểu diễn latent liên tục, với các hàm ánh xạ khác nhau kết nối không gian rời rạc và liên tục của văn bản. So với công trình của chúng tôi, chúng tôi tập trung vào các mô hình khuếch tán S EQ2SEQ cho sinh tạo văn bản trong không gian liên tục và công trình của chúng tôi là đầu tiên khám phá cài đặt này theo hiểu biết tốt nhất của chúng tôi.

Các mô hình khuếch tán cho sinh tạo có điều kiện. Liên quan đến conditional-V AE (Zhao et al., 2017), chúng ta có thể xem xét đầu vào x được mã hóa latent như một điều kiện. Diffusion-LM (Li et al., 2022) áp dụng các phương pháp plug-and-play (Dathathri et al., 2020) để tổng hợp các ràng buộc chi tiết trên các câu được sinh tạo, nhưng nó thất bại trong việc điều kiện trên toàn bộ câu nguồn trong các nhiệm vụ S EQ2SEQ. Lưu ý rằng phương pháp sinh tạo có thể điều khiển này là trực giao với D IFFSEQ của chúng tôi, nói cách khác, chúng tôi có thể tiếp tục thêm các ràng buộc có hướng dẫn bộ phân loại trên đầu ra S EQ2SEQ để tiếp tục điều khiển sinh tạo văn bản. Có những mô hình khuếch tán có điều kiện khác về dự đoán chuỗi thời gian như CSDI (Tashiro et al., 2021) hoặc sinh tạo âm thanh như WaveGrad (Chen et al., 2021), nhưng các điều kiện lớp của chúng thường là các thuộc tính dễ mô hình hóa, trong khi các văn bản ngữ cảnh như điều kiện phức tạp hơn nhiều.

6 KẾT LUẬN
Chúng tôi đề xuất D IFFU SEQ để giải quyết các nhiệm vụ S EQ2SEQ theo cách khuếch tán, có chứa tiềm năng mạnh mẽ để đạt được sự đánh đổi chất lượng và tính đa dạng sinh tạo tốt hơn. Khả năng này cho phép các đặc tính thuận lợi của D IFFU SEQ để tiếp tục nâng cao chất lượng kết quả cuối cùng, bằng cách tận dụng thuật toán giải mã rủi ro Bayes tối thiểu. Bên cạnh đó, chúng tôi kết nối lý thuyết các mô hình AR và NAR với D IF-FUSEQ, và cho thấy rằng D IFFU SEQ là một mở rộng mạnh mẽ của mô hình iterative-NAR. Các kết quả thực nghiệm chứng minh rằng D IFFU SEQ cũng là một mô hình mạnh mẽ cho sinh tạo văn bản, khớp hoặc thậm chí vượt qua các mô hình AR, iterative NAR, và mô hình được huấn luyện trước quy mô lớn cạnh tranh về chất lượng và tính đa dạng. Cho tiến bộ hạn chế của các mô hình khuếch tán hiện tại về sinh tạo văn bản, nghiên cứu của chúng tôi đề cập đến những thành tựu đầy hứa hẹn bởi một mô hình học chuỗi đến chuỗi mới như vậy.
9

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
LỜI CẢM ƠN
Chúng tôi muốn cảm ơn các reviewer ẩn danh và các đồng nghiệp khác vì lời khuyên quý báu của họ, và chúng tôi cũng thừa nhận những nỗ lực của Chenxin An trong việc cập nhật kết quả sinh tạo cho mô hình Transformer-base trên các nhiệm vụ QG và Paraphrasing. Công trình này được hỗ trợ một phần bởi Ủy ban Khoa học và Công nghệ Thượng Hải (Số hiệu Grant 21DZ1100100) và lược đồ nghiên cứu chung của Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) và Hội đồng Tài trợ Nghiên cứu (RGC) dưới số hiệu grant N HKU714/21.

TÀI LIỆU THAM KHẢO
Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, và Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021.

Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, và William Chan. Wavegrad: Estimating gradients for waveform generation. Trong International Conference on Learning Representations, ICLR, 2021.

Ting Chen, Ruixiang Zhang, và Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.

Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, và Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. Trong Conference on Empirical Methods in Natural Language Processing, EMNLP, 2014.

Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, và Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. Trong 8th International Conference on Learning Representations, ICLR, 2020.

Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G Schwing, và David Forsyth. Fast, diverse and accurate image captioning guided by part-of-speech. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10695–10704, 2019.

Bhuwan Dhingra, Kathryn Mazaitis, và William W Cohen. Quasar: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017.

Laurent Dinh, Jascha Sohl-Dickstein, và Samy Bengio. Density estimation using real nvp. Trong International Conference on Learning Representations, ICLR, 2017.

Wanyu Du, Jianqiao Zhao, Liwei Wang, và Yangfeng Ji. Diverse text generation via variational encoder-decoder models with gaussian process priors. Trong Proceedings of 6th Workshop on Structured Prediction for NLP of the Association for Computational Linguistics, ACL, 2022.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, và Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 6112–6121. Association for Computational Linguistics, 2019.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, và Yoshua Bengio. Generative adversarial nets. Trong Advances in Neural Information Processing Systems, 2014.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, và Richard Socher. Non-autoregressive neural machine translation. Trong International Conference on Learning Representations, ICLR, 2018.

Jiatao Gu, Changhan Wang, và Junbo Zhao. Levenshtein transformer. Advances in Neural Information Processing Systems, 2019.

10

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Jonathan Ho và Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.

Jonathan Ho, Ajay Jain, và Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text degeneration. Trong International Conference on Learning Representations, ICLR, 2019.

Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, và Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 2021.

Fei Huang, Tianhua Tao, Hao Zhou, Lei Li, và Minlie Huang. On the learning of non-autoregressive transformers. Trong International Conference on Machine Learning, ICML, 2022.

Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, và Wei Xu. Neural crf model for sentence alignment in text simplification. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL, 2020.

Diederik P. Kingma và Max Welling. Auto-Encoding Variational Bayes. Trong International Conference on Learning Representations, ICLR, 2014.

Philipp Koehn. Statistical significance tests for machine translation evaluation. Trong Proceedings of the 2004 conference on empirical methods in natural language processing, pp. 388–395, 2004.

Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, và Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. Trong International Conference on Learning Representations, ICLR, 2020.

Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, và Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. arXiv preprint arXiv:2205.14217, 2022.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Trong Text summarization branches out, 2004.

Yankai Lin, Haozhe Ji, Zhiyuan Liu, và Maosong Sun. Denoising distantly supervised open-domain question answering. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL, 2018.

Luke Metz, Ben Poole, David Pfau, và Jascha Sohl-Dickstein. Unrolled generative adversarial networks. Trong International Conference on Learning Representations, ICLR, 2017.

Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, và Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. Trong International Conference on Machine Learning, ICML, 2022.

Alexander Quinn Nichol và Prafulla Dhariwal. Improved denoising diffusion probabilistic models. Trong International Conference on Machine Learning, ICML, 2021.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL, 2002.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, và Lei Li. Glancing transformer for non-autoregressive neural machine translation. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.

11

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, và Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, và Mohammad Norouzi. Palette: Image-to-image diffusion models. Trong ACM SIGGRAPH 2022 Conference Proceedings, 2022a.

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, và cộng sự. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, và Xi Chen. Improved techniques for training gans. Trong Advances in Neural Information Processing Systems, 2016.

Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, và Aaron van den Oord. Step-unrolled denoising autoencoders for text generation. Trong International Conference on Learning Representations, ICLR, 2021.

Rico Sennrich, Barry Haddow, và Alexandra Birch. Neural machine translation of rare words with subword units. Trong Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715–1725. Association for Computational Linguistics, 2016.

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, và Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. Trong International Conference on Machine Learning, ICML, 2015.

Jiaming Song, Chenlin Meng, và Stefano Ermon. Denoising diffusion implicit models. Trong International Conference on Learning Representations, ICLR, 2020.

Yang Song và Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.

Yusuke Tashiro, Jiaming Song, Yang Song, và Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 2021.

Iulia Turc, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation. CoRR, abs/1908.08962, 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.

Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, và Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016.

P Yu, S Xie, X Ma, B Jia, B Pang, R Gao, Y Zhu, S-C Zhu, và YN Wu. Latent diffusion energy-based model for interpretable text modeling. Trong International Conference on Machine Learning, ICML, 2022.

Hugh Zhang, Daniel Duckworth, Daphne Ippolito, và Arvind Neelakantan. Trading off diversity and quality in natural language generation. Trong Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pp. 25–33. Association for Computational Linguistics, 2021.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. Bertscore: Evaluating text generation with bert. Trong International Conference on Learning Representations, ICLR, 2019.

12

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Tiancheng Zhao, Ran Zhao, và Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. Trong Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 2017.

Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, và Xiaoyan Zhu. Commonsense knowledge aware conversation generation with graph attention. Trong International Joint Conference on Artificial Intelligence, IJCAI, 2018.

Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, và Yong Yu. Texygen: A benchmarking platform for text generation models. Trong The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 1097–1100, 2018.

13

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
A DERIVATION MỤC TIÊU CỦA DIFFU SEQ
Mô hình khuếch tán nổi tiếng vì khả năng đạt được sự đánh đổi giữa tính linh hoạt và tính tractable của phân phối xác suất của mô hình, so với GAN, V AE và các mô hình dựa trên Flow. Theo Ho et al. (2020); Nichol & Dhariwal (2021); Song et al. (2020), chúng tôi định nghĩa một cách có hệ thống quá trình nhiễu tiến và quá trình khử nhiễu ngược trên không gian latent liên tục z.

Quá trình nhiễu tiến là để làm loạn cấu trúc của dữ liệu z0. z0 cuối cùng được thay đổi thành nhiễu Gaussian một phần với yTN(0;I) thông qua nhiễu loạn ngẫu nhiên tiến T-bước
q(ztjzt1) =N(zt;p
1tzt1;tI); (10)

với t= 1;2;:::;T và ft2(0;1)gT
t=1 là lịch phương sai. Gọi t= 1t và t=Qt
i=1i, chúng ta có:
zt=ptzt1+p
1tt1=ptt1zt2+p
1tt1t2
=:::=ptz0+p
1t;(11)

trong đó  biểu thị nhiễu Gaussian. Cuối cùng, q(ztjz0) =N(zt;ptz0;(1t)I). Chúng tôi sử dụng lịch nhiễu sqrt trong Diffusion-LM (Li et al., 2022), tức là, t= 1p
t=T+s với s là một hằng số nhỏ ở đầu mức nhiễu. Quá trình ngược sau đó khử nhiễu zt, nhằm khôi phục z0 gốc, và được định nghĩa là:
p(z0:T) :=p(zT)TY
t=1p(zt1jzt); p(zt1jzt) =N(zt1;(zt;t);(zt;t)): (12)

Việc học p được dựa trên mô hình khuếch tán DIFFU SEQ của chúng tôi: f(zt;t), trong đó () và () là tham số hóa dự đoán của mean và standard variation của q(ztjzt1) trong quá trình tiến. Sử dụng quy tắc Bayes:
q(zt1jzt;z0) =q(ztjzt1;z0)q(zt1jz0)
q(ztjz0)(13)

Thay thế Eq. (11) vào đó và chúng ta có thể nhận được mean được tham số hóa của q(zt1jzt;z0):
t(zt;z0) =pt(1t1)
1tzt+pt1t
1tz0; (14)

và để ngắn gọn, chúng tôi rút ngắn hệ số của zt và z0 thành U và E tương ứng.

Chúng tôi có thể sử dụng cận dưới biến thiên để tối ưu hóa negative log-likelihood E[logp(x0)]
LVLB. Mục tiêu có thể được viết lại thành một tổ hợp của một số term KL-divergence và entropy theo Sohl-Dickstein et al. (2015).

LVLB=LT+LT1++L0=Eq(z1:Tjz0)"
logq(zTjz0)
p(zT)+TX
t=2logq(zt1jz0;zt)
p(zt1jzt)
+ logq(z0jwxy)
p(z0jz1)logp(wxyjz0)#
:(15)

Với 1tT1, chúng tôi tính toán tham số hóa của Lt bằng cách thay thế Eq. (14) để minimize sự khác biệt từ t và f theo Ho et al. (2020):
Lt=Ez0
logq(ztjz0;zt+1)
p(ztjzt+1)
=Ez01
Cjjt(zt;z0)(zt;t)jj2
=Ez01
CjjUzt+Ez0(Uzt+Ef(zt;t))jj2
=E
CEz0[jjz0f(zt;t)jj2];(16)

14

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
trong đó C= 2jjjj2 là một hằng số độc lập mất mát. Sau đó việc tối ưu hóa mất mát huấn luyện minLVLB có thể được đơn giản hóa hơn như sau:
min
"
jj(zT)jj2+TX
t=2jjz0f(zt;t)jj2+jjEMB(wxy)f(z1;1)jj2logp(wxyjz0)#
!min
"TX
t=2jjz0f(zt;t)jj2+jjEMB(wxy)f(z1;1)jj2logp(wxyjz0)#
!min
"TX
t=2jjy0~f(zt;t)jj2+jjEMB(wy)~f(z1;1)jj2+R(jjz0jj2)#
:
(17)

B CÁC MÔ HÌNH ĐỒ HỌA CỦA AR, FULLY NAR, ITERATIVE NAR VÀ DIFFU SEQ
Chúng tôi bắt đầu từ bài toán sinh tạo chuỗi có điều kiện, nhằm học một xác suất có điều kiện p(wy1:njwx) với wx và wy. Các mô hình AR học p(wy1:njwx) bằng phân tích tự hồi quy dựa trên ngữ cảnh trái:
pAR(wy1:njwx) =p(wy1jwx)
|{z}
dự đoán ban đầu Y
i=1;:::;n1p(wyi+1jwy1:i;wx)
|{z}
dự đoán ngữ cảnh trái tiến bộ; (18)

bao gồm một dự đoán ban đầu và một quá trình dự đoán ngữ cảnh trái tự hồi quy, trong khi các mô hình fully-NAR (Gu et al., 2018; Qian et al., 2021) học xác suất có điều kiện cho trước giả định độc lập để suy luận nhanh:
pfully-NAR (wy1:njwx) =Y
i=1;:::;np(wyijwx): (19)

Để làm một phép tương tự tốt hơn với các mô hình AR và NAR, chúng tôi sử dụng một cách không mất mát để công thức hóa các mô hình iterative NAR (Gu et al., 2019; Ghazvininejad et al., 2019) bằng cách giới thiệu một loạt các chuỗi trung gian wy1:K1;wyK=wy như:
piter-NAR (wy1:njwx) =X
wy1;:::;wyK1p(wy1jwx)Y
k=1:::K1p(wyk+1jwy
k;wx)
=X
wy1;:::;wyK1p(wy1jwx)Y
k=1:::K1p(wyk+1jwy
k;wx)
=X
wy1;:::;wyK1Y
i=1:::np(wy1;ijwx)
|{z}
dự đoán ban đầu Y
k=1::K1Y
i=1:::np(wyk+1;ijwyk;1:n;wx)
| {z }
dự đoán ngữ cảnh đầy đủ tiến bộ (20)

Nghiên cứu trước đây (Huang et al., 2022) cho thấy có một khoảng cách gọi là tương quan tổng có điều kiện giữa các mô hình học AR và fully-NAR, vì phân tích mất mát của các mô hình NAR. Khoảng cách này chịu trách nhiệm chính cho việc giảm hiệu suất từ mô hình AR sang NAR. Tuy nhiên, khi so sánh iter-NAR, Eq. (20), với các mô hình AR, cả hai đều có thể được phân tích thành một term dự đoán ban đầu và một quá trình dự đoán tiến bộ dựa trên ngữ cảnh khác nhau (tức là ngữ cảnh trái trong AR và ngữ cảnh đầy đủ trong iter-NAR). Sự khác biệt như được chỉ ra bởi Huang et al. (2022) do đó được đóng lại trong iter-NAR giả định đủ bước. Bằng cách hiển thị D IFFU SEQ là một mở rộng của mô hình iter-NAR, chúng tôi đưa ra một lời biện minh rằng nó sẽ không gặp phải tương quan tổng có điều kiện vì cùng lý do.

15

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Một cách thẳng thắn để công thức hóa các mô hình khuếch tán naive là giới thiệu một loạt các đặc trưng bị hỏng bởi nhiễu Gaussian y1:T1;y0=y;yTN(0;I) trên không gian liên tục như:
pdiffusion (wyjwx) =Z
yT;:::;y0p(wyjy0;wx)
|{z}
dự đoán bước cuối cùng Y
t=T;:::; 1p(yt1jyt;wx)
|{z}
khuếch tán ngữ cảnh đầy đủ tiến bộ (21)
=Z
yT;:::;y0Y
i=1;:::;np(wyijy0;i;wx)Y
t=T;:::; 1Y
i=1;:::;np(yt1;ijyt;wx) (22)

trong đó p(yt1jyt;wx) mô tả quá trình khuếch tán trên các biểu diễn liên tục y. Tổng số bước khuếch tán được ký hiệu là T. Sau đó chúng tôi bỏ qua phân tích độc lập trên wy và yt. Để áp dụng các mô hình khuếch tán trên không gian rời rạc, phép toán làm tròn trong D IFFU SEQ ánh xạ các vector liên tục y thành wy rời rạc cho mỗi bước thời gian t, do đó chúng tôi cũng giới thiệu cả đặc trưng liên tục y và văn bản rời rạc wy để biểu diễn văn bản rời rạc vào Eq. (21):
p(wyjwx) =X
wyT;:::;wy1Z
yT;:::;y0p(wyTjyT;wx)Y
t=T1;:::;0p(wytjyt;wx)p(ytjwyt+1) (23)
=X
wyT;:::;wy1Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1p(yt1jwyt)p(wytjyt;wx) (24)
=Z
yT;:::;y0p(wyjy0;wx)X
wyT;:::;wy1Y
t=T;:::; 1p(yt1jwyt)p(wytjyt;wx) (25)
=Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1X
wytp(yt1jwyt)p(wytjyt;wx) (26)

Bằng cách sắp xếp lại Eq. (23) và Eq. (24), chúng ta có thể thấy rằng D IFFU SEQ có thể được xem như một dạng tổng quát hóa hơn của iter-NAR trước khi marginalize fyT;:::;y0g, trong đó Eq. (23) và Eq. (24) là tương đương với thứ tự tính toán khác nhau, mặc dù khởi tạo khác nhau của yT. Đối với các mô hình NAR, yT được sao chép đồng nhất từ câu nguồn hoặc embedding token unk (Gu et al., 2018); đối với các mô hình khuếch tán, yT được lấy mẫu từ phân phối chuẩn N(0;I).

Cần lưu ý rằng không giống như các mô hình AR và fully NAR sinh tạo văn bản tất cả cùng một lúc, các mô hình iterative NAR và khuếch tán có đặc điểm là một quá trình sinh tạo văn bản tự sửa chữa. Việc so sánh đồ họa được hiển thị trong Hình 7.

C TỪ DIFFU SEQ ĐẾN ITERATIVE NAR VÀ CÁC MÔ HÌNH KHUẾCH TÁN
Từ D IFFU SEQ đến Iterative NAR Chúng tôi cho thấy cách derive D IFFU SEQ thành mô hình non-autoregressive lặp trên không gian rời rạc.

16

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
...
.........
...AR
Fully-NAR
Iter -NARDif fuSeq
......
......
............
......
.........
...
...............
...
......
Hình 7: Các mô hình đồ họa của AR, Fully NAR, iterative NAR và các mô hình D IFFU SEQ. Để đơn giản, chúng tôi bỏ qua node nguồn wx. Các node xám chỉ ra sự phụ thuộc vào node nguồn trong khi các node trắng độc lập với node nguồn.

pDIFFU SEQ(wyjwx)
=X
wyT;:::;wy1Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1p(yt1jwyt)p(wytjyt;wx)
=X
wyT;:::;wy1Z
yT;:::;y0p(wyTjyT;wx)Y
t=T1;:::;0p(wytjyt;wx)p(ytjwyt+1)sắp xếp lại tính toán
=X
wyT;:::;wy1p(wyTjyT;wx)Y
t=T1;:::;0Z
ytp(wytjyt;wx)p(ytjwyt+1)
=X
wyT;:::;wy1p(wyTjyT;wx)Y
t=T1;:::;0p(wytjwyt+1;wx)) marginalize qua y
=X
wy1;:::;wyK1p(wy1jwx)Y
k=1:::K1p(wyk+1jwyk;wx) liên kết t và k ngược lại.
=piter-NAR (wyjwx)

Từ D IFFU SEQ đến mô hình khuếch tán Chúng tôi cho thấy cách derive D IFFU SEQ thành mô hình khuếch tán thẳng thắn trên không gian liên tục.

17

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
pDIFFU SEQ(wyjwx)
=X
wyT;:::;wy1Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1p(yt1jwyt)p(wytjyt;wx)
=Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1X
wytp(yt1jwyt)p(wytjyt;wx)
=Z
yT;:::;y0p(wyjy0;wx)Y
t=T;:::; 1p(yt1jyt;wx) marginalize qua wy
=pdiffusion (wyjwx)

D CHI TIẾT THÍ NGHIỆM
D.1 XỬ LÝ TẬP DỮ LIỆU SINH TẠO CÂU HỎI
Để xây dựng các cặp tài liệu-câu hỏi chất lượng cao từ tập dữ liệu Quasar-T, bao gồm các triplet hdocument i;question;answer i, chúng tôi trích xuất các cặp hdocument i;question i nếu answer khớp chính xác với document i. Sau khi tiền xử lý, chúng tôi có được 119K cặp huấn luyện tài liệu-câu hỏi.

D.2 CÀI ĐẶT CỦA CÁC BASELINE
Chúng tôi so sánh các cài đặt của các mô hình khác nhau, bao gồm số lượng tham số và cách lấy mẫu các câu đầu ra khác nhau, như được hiển thị trong Bảng 4. Đối với các phương pháp encoder-decoder dựa trên GRU thuần túy, chúng tôi không triển khai thuật toán tìm kiếm đa dạng trên đó, do đó tính đa dạng cấp độ câu của nó có thể rất kém. Đối với NAR-LevT, chúng tôi đặt max iteration là 9 và theo điều kiện kết thúc được đề cập trong bài báo gốc. Đối với GPV AE-T5, chúng tôi tune scalar để tìm sự đánh đổi tốt nhất giữa chất lượng và tính đa dạng trên tập dev. Các scalar của tất cả bốn nhiệm vụ được đặt thành 2. Chúng tôi triển khai các baseline GPT2 sử dụng HuggingFace Transformers và đối với baseline Transformer-base, chúng tôi sử dụng Fairseq.

Bảng 4: So sánh các mô hình khác nhau
Mô hình # Tham số Mô hình học Nguồn đa dạng
GRU-attention 65M encoder-decoder -
Transformer-base 80M encoder-decoder Temperature/DBS
GPT2-base FT 117M pretrain-finetune Chiến lược lai4
GPT2-large FT 774M pretrain-finetune Chiến lược lai
GPV AE-T5 220M pretrain+V AE Gaussian sampling
NAR-LevT 80M non-autoregressive -
DIFFU SEQ 91M non-autoregressive Gaussian sampling

D.3 CÀI ĐẶT ĐÁNH ĐỔI TÍNH ĐA DẠNG VÀ CHẤT LƯỢNG
Chúng tôi liệt kê các chi tiết để có được Hình 4. Đối với GPV AE-T5, chúng tôi đặt các scalar khác nhau là 1;2;3;4. Đối với DIFFU SEQ, chúng tôi chọn các mô hình được huấn luyện ở các bước huấn luyện khác nhau để đạt được các điểm đánh đổi khác nhau. Đối với các baseline khác, không có yếu tố rõ ràng để điều khiển sinh tạo đa dạng, vì vậy chúng tôi để chúng như các điểm đơn trong hình.

4Bao gồm top-p sampling, temperature, diversity beam search (DBS) và v.v. Triển khai sử dụng HuggingFace Transformers https://github.com/huggingface/transformers
18

--- TRANG 19 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
D.4 METRICS
Điểm BLEU được sử dụng là sentence-level smoothed từ BLEU-1 đến 4, và điểm ROUGE-L được sử dụng là thống kê dựa trên longest common subsequence. Việc triển khai dựa trên NLTK5 và torchmetrics. Các metric dựa trên n-gram có thể thất bại trong việc nắm bắt ý nghĩa ngữ nghĩa của câu, vì vậy chúng tôi xem xét sử dụng BERTScore6. Cụ thể, chúng tôi sử dụng microsoft/deberta-xlarge-mnli để giúp BERTScore tương quan tốt hơn với điểm số của con người.

D.5 KẾT QUẢ SINH TẠO
Đối với các nhiệm vụ khác nhau, chúng tôi liệt kê một số ví dụ sinh tạo. Như chúng ta có thể thấy trong Bảng 5, Bảng 6 và Bảng 7, DIFFU SEQ có xu hướng sinh tạo đầu ra đa dạng, nhưng đôi khi câu không trôi chảy như GPT2 được tinh chỉnh.

Bảng 5: Đầu ra mẫu với seed ngẫu nhiên khác nhau trong tập kiểm tra Dialogue.
Phát ngôn : Màu nhuộm kéo dài bao lâu?
Phản hồi : Chỉ làm cái này hai ngày trước, chưa chắc nó sẽ phai như thế nào!
GPV AE-T5 NAR-LevT
*Tôi không chắc, tôi không chắc. Tôi đã thử nó vài lần, nhưng tôi không biết chắc chắn. Tôi đã* nửa .
*Tôi không chắc. Tôi không chắc nó kéo dài bao lâu, tôi chắc nó sẽ tốt hơn. Đã lâu rồi kể từ khi* nửa .
*Tôi đã sử dụng nó khoảng một năm rưỡi. Tôi đã sử dụng nó khoảng một năm rưỡi.* nửa .
GPT2-large tinh chỉnh DIFFU SEQ
* Hai tuần trong trường hợp của tôi. * Khoảng một giờ, 5 ngày hoặc hơn.
* Tôi đã có nó khoảng một năm. * 4 ngày.
*Màu nhuộm có thể để khoảng một tháng rồi bạn có thể rửa nó.*Tôi không chắc về điều này, về cùng loại thời gian.

Bảng 6: Đầu ra mẫu với seed ngẫu nhiên khác nhau trong tập kiểm tra Question Generation.
Phát biểu : Yên Nhật là đồng tiền chính thức và duy nhất được công nhận ở Nhật Bản.
Câu hỏi : Đồng tiền Nhật Bản là gì?
GPV AE-T5 NAR-LevT
* Đồng tiền nhật bản là gì * Đơn vị tiền tệ cơ bản của Nhật Bản là gì ?
* Đồng tiền nhật bản là gì * Đơn vị tiền tệ cơ bản của Nhật Bản là gì ?
* Đồng tiền nhật bản là gì * Đơn vị tiền tệ cơ bản của Nhật Bản là gì ?
GPT2-large tinh chỉnh DIFFU SEQ
* Đơn vị tiền tệ cơ bản của Nhật Bản là gì? * Đồng tiền Nhật Bản là gì
* Đồng tiền Nhật Bản là gì * Quốc gia nào sử dụng "yên yên" làm tiền tệ
* Đơn vị tiền tệ cơ bản của Nhật Bản là gì? * Đơn vị tiền tệ cơ bản là gì?

5https://www.nltk.org/_modules/nltk/translate/bleu_score.html
6https://github.com/Tiiiger/bert_score
19

--- TRANG 20 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 7: Đầu ra mẫu với seed ngẫu nhiên khác nhau trong tập kiểm tra Text Simplification.
Câu phức tạp : Mọi người có thể trải qua sự cô đơn vì nhiều lý do, và nhiều sự kiện trong đời có thể gây ra nó, chẳng hạn như thiếu quan hệ bạn bè trong thời thơ ấu và tuổi teen, hoặc sự vắng mặt về mặt vật lý của những người có ý nghĩa xung quanh một người.
Đơn giản : Một nguyên nhân của sự cô đơn là thiếu bạn bè trong thời thơ ấu và tuổi teen.
GPV AE-T5 NAR-LevT
*Mọi người có thể trải qua sự cô đơn vì nhiều lý do, và nhiều sự kiện trong đời có thể gây ra nó, chẳng hạn như thiếu quan hệ bạn bè trong thời thơ ấu và tuổi teen, hoặc sự vắng mặt về mặt vật lý của những người có ý nghĩa xung quanh một người*Mọi người có thể trải qua reashapphapphapphapphapphappabout life reasit.
*Mọi người có thể trải qua sự cô đơn vì nhiều lý do, và nhiều sự kiện trong đời có thể gây ra nó, chẳng hạn như thiếu quan hệ bạn bè trong thời thơ ấu và tuổi teen, hoặc sự vắng mặt về mặt vật lý của những người có ý nghĩa xung quanh một người*Mọi người có thể trải qua reashapphapphapphapphapphappabout life reasit.
*Mọi người có thể trải qua sự cô đơn vì nhiều lý do, và nhiều sự kiện trong đời có thể gây ra nó, chẳng hạn như thiếu quan hệ bạn bè trong thời thơ ấu và tuổi teen, hoặc sự vắng mặt về mặt vật lý của những người có ý nghĩa xung quanh một người*Mọi người có thể trải qua reashapphapphapphapphapphappabout life reasit.
GPT2-large tinh chỉnh DIFFU SEQ
* Sự cô đơn có thể được gây ra bởi nhiều thứ. * Nhiều sự kiện trong đời có thể gây ra sự cô đơn
* Sự cô đơn có thể ảnh hưởng đến mọi người theo nhiều cách. *Mọi người cũng có thể rất trải qua sự cô đơn vì nhiều lý do.
* Sự cô đơn có thể được gây ra bởi nhiều thứ. *Mọi người có thể trải qua sự cô đơn vì nhiều lý do, và nhiều sự kiện trong đời có thể, gây ra nó.

20
