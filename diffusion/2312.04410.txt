# 2312.04410.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2312.04410.pdf
# File size: 9073704 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models
Jiayi Guo1,2*, Xingqian Xu1,3*, Yifan Pu2, Zanlin Ni2, Chaofei Wang2, Manushree Vasu1,
Shiji Song2, Gao Huang2‚Ä†, Humphrey Shi1,3‚Ä†
1SHI Labs @ Georgia Tech & UIUC2Tsinghua University3Picsart AI Research (PAIR)
https://github.com/SHI-Labs/Smooth-Diffusion
‚ÄúA realistic dog‚Äù
Image AImage BInterpolationSmoothDiffusion(Ours)StableDiffusionReplaceItem: ‚Äúrabbit‚Äù‚Üí‚Äúcat‚Äù‚ÄúA train going back to its course filled with people‚Äù‚ÄúA mouse is next to a keyboard on a desk‚Äù
AddItem: +‚Äúbacon‚ÄùTransfer Style: ‚Äúwatercolorstyle‚ÄùDrag PointTask1:Image Interpolation
Task2:Image Inversion and ReconstructionTask3:Image Editing
Smooth Diff.Stable Diff.Source
Smooth Diff.Stable Diff.SourceSmooth Diff.Stable Diff.Source
Figure 1. Smooth Diffusion for downstream image synthesis tasks. Our method formally introduces latent space smoothness to
diffusion models like Stable Diffusion [59]. This smoothness dramatically aids various tasks in: 1) improving continuity of transitions in
image interpolation, 2) reducing approximation errors in image inversion, & 3) better preserving unedited contents in image editing.
Abstract
Recently, diffusion models have made remarkable
progress in text-to-image (T2I) generation, synthesizing im-
ages with high fidelity and diverse contents. Despite this ad-
vancement, latent space smoothness within diffusion mod-
els remains largely unexplored. Smooth latent spaces en-
sure that a perturbation on an input latent corresponds to
a steady change in the output image. This property proves
beneficial in downstream tasks, including image interpola-
tion, inversion, and editing. In this work, we expose the
non-smoothness of diffusion latent spaces by observing no-
ticeable visual fluctuations resulting from minor latent vari-
ations. To tackle this issue, we propose Smooth Diffusion , a
*Equal contribution.
‚Ä†Corresponding authors.new category of diffusion models that can be simultaneously
high-performing and smooth. Specifically, we introduce
Step-wise Variation Regularization to enforce the propor-
tion between the variations of an arbitrary input latent and
that of the output image is a constant at any diffusion train-
ing step. In addition, we devise an interpolation standard
deviation (ISTD) metric to effectively assess the latent space
smoothness of a diffusion model. Extensive quantitative and
qualitative experiments demonstrate that Smooth Diffusion
stands out as a more desirable solution not only in T2I gen-
eration but also across various downstream tasks. Smooth
Diffusion is implemented as a plug-and-play Smooth-LoRA
to work with various community models. Code is available
at https://github.com/SHI-Labs/Smooth-Diffusion.
1arXiv:2312.04410v1  [cs.CV]  7 Dec 2023

--- PAGE 2 ---
1. Introduction
In recent years, diffusion models [13, 23, 59] have rapidly
grown into very powerful tools for generative AI, particu-
larly for text-to-image generation. The remarkable ability
of diffusion models, generating high-quality photorealistic
images from open-book contexts, has been highlighted in
many research and commercial products. Such success has
also inspired various diffusion-based downstream tasks, in-
cluding image interpolation [27, 75], inversion [14, 42, 51,
68, 74], editing [20, 40, 43, 52, 66, 72, 78, 79], etc.
Despite the great success in the generation field, dif-
fusion models occasionally produce low-quality results
with undesirable and unpredictable behaviors. Specifi-
cally speaking, for image interpolation, the Stable Diffusion
Walk (SDW) [27] test examines latent space with spheri-
cal linear interpolations, usually resulting in highly fluctu-
ated outputs with unpredictable visual appearance. Exam-
ples can be found in Fig. 1 Task 1, in which such interpo-
lation exhibits undesired sharp changes as well as ‚Äúcartoon-
ization‚Äù on photorealistic dog images, highlighted in the red
box. For the image inversion task shown in Fig. 1 Task 2,
a naive application of DDIM inversion [68] cannot recon-
struct images faithfully from the sources. Instead, it gener-
ates incorrect colors and object orientations, and misinter-
prets the computer mouse as an animal mouse. For the im-
age editing task shown in Fig. 1 Task 3, one may notice that
only minor text prompt editing can lead to major updates
on image contents and layouts, in which the object ( i.e. the
cat‚Äôs pose, the horse‚Äôs location, the shape of the pizza) can
be wildly and incorrectly altered. Moreover, current diffu-
sion models are unsuited to drag-based editing [66] because
a fine-engineered drag method still has a noticeably large
chance of breaking objects‚Äô shape and semantics.
In this work, we step into an important but under-
explored area: to improve the latent space smoothness of
diffusion models. Our motivation to enhance latent smooth-
ness comes from the real-world demand to improve the out-
put qualities of the aforementioned downstream tasks. A
smooth latent space implies a robust visual variation under
a minor latent change. Therefore, enhancing such smooth-
ness could help improve the continuity of image interpola-
tion, expand the capacity of image inversion, and maintain
correct semantics in image editing. Notably, prior works in
GANs [30, 31, 65] have demonstrated that the smooth la-
tent space of the generator can significantly improve down-
stream tasks‚Äô quality, offering additional evidence of the im-
portance of this area.
To achieve our goal, we propose Smooth Diffusion , a
new category of diffusion models that can be simultane-
ously high-performing and smooth. We start our explo-
ration by first formalizing the objective for Smooth Dif-
fusion, in which fixed-size perturbations ‚àÜœµon a latent
noiseœµshould produce smooth visual changes ‚àÜcx0on thesynthetic image cx0, rounded to a constant ratio C. Al-
though one may think that according to the formulation,
the smoothness constraint could be an accessible train-
time loss. Actually, there is no direct application of such
regularization from inference to training, and the chal-
lenge lies in the fact that in each training iteration ( i.e.,
back-propagation), diffusion models optimize only a ‚Äú t-step
snapshot‚Äù instead of the entire T-step diffusion process.
Therefore, we introduce Step-wise Variation Regular-
ization , a novel regularization that seamlessly incorporates
our Smooth Diffusion‚Äôs inference-time objective to training.
This regularization aims to bound the 2-norm of output vari-
ation ‚àÜcx0given a fixed-size change ‚àÜxtin input xtat an
arbitrary step t. The rationale of the reformulation is intu-
itive: If xtandcx0exhibit smooth changes at any t, then
the relation between the latent noise œµ(i.e.xT) andcx0is
just the accumulation of smooth variations and thus can be
smooth as well. More details can be found in Sec. 3.
In practice, our Smooth Diffusion is trained on top of
a well-known text-to-image model: Stable Diffusion [59].
We examine and demonstrate that Smooth Diffusion dra-
matically improves the latent space smoothness over its
baseline. Meanwhile, we conduct extensive research across
numerous downstream tasks, including but not limited to
image interpolation, inversion, editing, etc. Both qualitative
and quantitative results support our conclusion that Smooth
Diffusion can be the next-gen high-performing generative
model not only for the baseline text-to-image task but across
various downstream tasks.
2. Related Work
Diffusion models are initiated from a family of prior works
including but not limited to [10, 63, 67, 73]. Since then,
DDPM [23] introduced an image-based noise prediction
model, becoming one of the most popular image generation
research. Later works [13, 45, 68] extended DDPM, demon-
strating that diffusion models perform on-par and even sur-
pass GAN-based methods [16, 28‚Äì31]. Recently, generat-
ing images from text prompts (T2I) become an emerging
field, among which diffusion models [17, 46, 56, 59, 61]
have become quite visible to the public. For example, Sta-
ble Diffusion (SD) [59] consists of V AE [34] and CLIP [55],
diffuses latent space, and yields an outstanding balance be-
tween quality and speed. Following SD [59], researchers
also explored diffusion approaches for controls such as
ControlNet [15, 25, 44, 54, 77, 82, 83, 86‚Äì88, 92] and mul-
timodal such as Versatile Diffusion [8, 39, 70, 85]. Works
from a different track reduce diffusion steps to improve
speed [7, 32, 37, 41, 62, 69, 89, 93], or restrict data and
domain for few-shot learning [19, 24, 38, 60], all had suc-
cessfully maintained a high output quality.
Smooth latent space was one of the prominent prop-
2

--- PAGE 3 ---
erties of SOTA GAN works [11, 29‚Äì31], while explor-
ing such property went through the decade-long GAN re-
search [5, 16], whose goals were mainly robust training.
Ideas such as Wasserstein GAN [6, 18] had proved to be ef-
fective, which enforced the Lipschitz continuity on discrim-
inator via gradient penalties. Another technique, namely
path length regularization, related to the Jacobian clamping
in [48], was adapted in StyleGAN2 [30] and later became a
standard setting for GAN-based generators [12, 35, 84, 91].
Benefiting from the smoothness property, researchers man-
aged to manipulate latent space in many downstream re-
search projects. Works such as [9, 47, 65, 80] explored la-
tent space disentanglement. GAN-inverse [3, 4, 49, 81] had
also proved to be feasible, along with a family of image edit-
ing approaches [50, 53, 57, 58, 71, 94]. As aforementioned,
our work aims to investigate the latent space smoothness for
diffusion models, which by far remains unexplored.
3. Methodology
In this section, we first introduce preliminaries of our
method, including diffusion process [23], diffusion inver-
sion [13, 42, 68] and low-rank adaptation [24] (Sec. 3.1).
Then Smooth Diffusion is proposed with its definition, ob-
jective (Sec. 3.2) and regularization function (Sec. 3.3).
3.1. Preliminaries
Diffusion process [23] is a kind of Markov chain that grad-
ually adds random noise œµt‚àºN(0,I)to ground truth sig-
nalx0‚àºp(x0), making xTin a total of Tsteps. At each
step, The noisy data xtis computed as:
xt=p
1‚àíŒ≤txt‚àí1+p
Œ≤tœµt, t= 1,2,¬∑¬∑¬∑, T, (1)
where Œ≤tis the preset diffusion rate at step t. By making
Œ±t= 1‚àíŒ≤t,Œ±t=QT
t=1Œ±tandœµ‚àºN(0,I), we have the
following equivalents:
xt=‚àöŒ±txt‚àí1+‚àö
1‚àíŒ±tœµt
=‚àöŒ±tx0+‚àö
1‚àíŒ±tœµ, t= 1,2,¬∑¬∑¬∑, T.(2)
A diffusion model œµŒ∏(xt, t)is then trained to estimate œµt
fromxt, by which one can predict the original signal x0by
gradually remove noise from the degraded xT[68]. This is
commonly known as the backward diffusion process:
[xt‚àí1=rŒ±t‚àí1
Œ±tcxt+ s
1
Œ±t‚àí1‚àí1‚àír
1
Œ±t‚àí1!
¬∑œµŒ∏(cxt, t).
(3)
Diffusion inversion [13, 42, 68] targets to recover the exact
backward diffusion process ( i.e.cxt, œµŒ∏(cxt, t), t= 1, ..., T )
from a known final prediction cx0. One of the common tech-
nique for such inversion is DDIM inversion [13, 68], which
reverses Eq. (3) under a local linear approximation:]xt+1=rŒ±t+1
Œ±tfxt+ s
1
Œ±t+1‚àí1‚àír
1
Œ±t‚àí1!
¬∑œµŒ∏(fxt, t),
(4)
wherefxtrepresent the estimated cxtat time t. However,
DDIM inversion is only a rough estimation. For text-
to-image diffusion, a more advanced technique, Null-Text
Inversion [42], optimizes additional null-text embeddings
{‚àÖt}T
t=1for each step t, simulating the backward process
withœµŒ∏(xt, t, Œæ,‚àÖt), where Œæis the input text embedding.
The predicted null-text ‚àÖtis the null input of the classifier-
free guidance [22] with a guidance scale w:
œµŒ∏(xt, t, Œæ,‚àÖt) =w¬∑œµŒ∏(xt, t, Œæ) + (1 ‚àíw)¬∑œµŒ∏(xt, t,‚àÖt).
(5)
Low-rank adaptation (LoRA) [24] is initially proposed
to efficiently adapt large pretrained models to downstream
tasks. The key assumption of LoRA is that the weight
changes required during adaptation maintain a low rank.
Given a pretrained model weight W0‚ààRd√ók, its updated
weight ‚àÜWis expressed as a low rank decomposition:
W0+ ‚àÜW=W0+BA, (6)
where B‚ààRd√ór,A‚ààRr√ókandr‚â™min(d, k). During
adaptation, W0is frozen, while BandAare trainable.
3.2. Smooth Diffusion
As previously mentioned, modern diffusion models (DM)
do not guarantee latent space smoothness, creating not only
research gaps between GANs and diffusions but also unex-
pected challenges in downstream tasks. To address these
issues, we propose Smooth Diffusion , a novel class of
high-performing diffusion models with enhanced smooth-
ness over its latent space. The underlining of Smooth Dif-
fusion is the newly proposed training scheme in which we
carried out a Step-wise Variation Regularization to en-
hance model smoothness.
To better explain our aims, we adopt the same termi-
nologies from the standard inference-time diffusion process
(Fig. 2a), involving a Tsteps procedure that transforms the
random noise œµ(i.e.,xT) to the prediction cx0. The over-
all objective of Smooth Diffusion can then be written in
Eq. 7: in which we expect that a fixed-size change ‚àÜœµonœµ
(i.e.,‚àÜxTonxT) will finally lead to a non-zero, fixed-size
change ‚àÜcx0oncx0, up to a constant ratio C:
‚à•‚àÜcx0‚à•2‚áîC‚à•‚àÜxT‚à•2=C‚à•‚àÜœµ‚à•2,‚àÄœµ, (7)
Notice that by definition, xTis the initial input of the
backward diffusion loop in Eq. 3. Since xTis close to
œµ‚àºN(0,1), for simplicity, we make them equivalent in
all the following equations.
Nevertheless, one may notice that our inference-time ob-
jective in Eq. 7 cannot be directly transformed into a train-
3

--- PAGE 4 ---
DMDMùëá stepsùùê(or ùíô!)$ùíô"(a) Inference-time Diffusion: Denoisng prediction through ùëªsteps (b) Training-time Diffusion:Denoisng prediction at a single step ùíïùíô#$ùíô"1‚àíùõº#ùùêùõº#ùíô"+DMùëá steps(or ùíô!+Œîùíô!)$ùíô"+Œî$ùíô"(c) Inference-time Smooth Diffusion:Variation constraint through ùëªsteps DM(d) Training-time Smooth Diffusion:Variation constraint at a single step ùíïùíô#+Œîùíô#$ùíô"+Œî$ùíô"1‚àíùõº#(ùùê+Œîùùê)ùõº#ùíô"+ùùê+Œîùùêùê∂Œîùùêùüê=ùê∂Œîùíô!%‚áîŒî$ùíô"%,‚àÄùùêùê∂1‚àíùõº#Œîùùêùüê=ùê∂Œîùíô#%‚áîŒî$ùíô"%,‚àÄùùêFigure 2. Illustration of Smooth Diffusion. Smooth Diffusion (c) enforces the ratio between the variation of the input latent ( ‚à•‚àÜœµ‚à•2or
‚à•‚àÜxT‚à•2) and the variation of the output prediction ( ‚à•‚àÜcx0‚à•2) is a constant C. Training-time Diffusion (b) optimizes a ‚Äú t-step snapshot‚Äù
of the denoising prediction process in Inference-time Diffusion (a). Similarly, we propose Training-time Smooth Diffusion (d) to optimize
a ‚Äút-step snapshot‚Äù of the variation constraint in Inference-time Smooth Diffusion (c). DM: Diffusion model.
ing loss function. This is because, in one training iteration
(i.e., back-propagation), diffusion models optimize only a
‚Äút-step snapshot‚Äù of the diffusion process (Fig. 2b), where
tis uniformly sampled from 1 to T. Hence, the proposed
‚Äúglobal‚Äù objective (Eq. 7) for the entire T-step process is
not accessible in training. Therefore, we need to reformu-
late our global objective into a step-wise objective shown
in Eq. 8, which can later be integrated into the diffusion
training process as a loss function:
‚à•‚àÜcx0‚à•2‚áîC‚à•‚àÜxt‚à•2=C‚àö
1‚àíŒ±t‚à•‚àÜœµ‚à•2,‚àÄœµ,(8)
where Cis a non-zero constant. This step-wise objective in-
dicates that at each training step, variations ‚àÜœµonœµshould
imply variations ‚àÜxtonxtwith a ratio proportional to‚àö1‚àíŒ±t. The rationale of Eq. 8 is intuitive: If xtandcx0
show smooth changes at any t, then the relation between
the latent noise œµ(i.e.xT) andcx0is just the accumulation
of smooth variations and thus can be smooth as well.
3.3. Step-wise Variation Regularization
While the motivation and formulation of the Smooth Diffu-
sion objective are presented, how to realize such an objec-
tive remains unexplained. Therefore, in this section, we in-
troduce Step-wise Variation Regularization to effectively
integrate the step-wise objective into diffusion training.
We draw inspiration from the regularization tech-
niques [30, 48] adopted in GAN training. The core idea of
Step-wise Variation Regularization is to bound the Jacobian
matrix Jœµ=‚àÇcx0/‚àÇœµof the diffusion system by minimizingthe following regularization loss at any x0,œµ,and step t:
Lreg=E‚àÜcx0,œµ ‚àö
1‚àíŒ±t‚à•JT
œµ‚àÜcx0‚à•2‚àía2, (9)
where ‚àÜcx0is the normally sampled pixel intensities nor-
malized to unit length, œµis a normally sampled noise
in Eq. 2, and ais the exponential moving average of‚àö1‚àíŒ±t‚à•JT
œµ‚àÜcx0‚à•2computed online during training. In
practice, we compute Eq. 9 via standard backpropagation
with the following identity:
‚àö
1‚àíŒ±t‚à•JT
œµ‚àÜcx0‚à•2=‚à•‚àáœµ(‚àö
1‚àíŒ±tcx0¬∑‚àÜcx0)‚à•2.(10)
The identity holds since ‚àÜcx0is independently sampled, and
uncorrelated with œµ.
Next, we prove that the proposed objective in Eq. 9 ex-
actly matches our optimization goal in Eq. 8. One prelimi-
nary result, proven in [30], is that in high dimensions, Eq. 9
is minimized when Jœµis orthogonal at any œµup to a global
scaling factor K(i.e.Jœµ¬∑JT
œµ=K ¬∑I). By applying the
orthogonality of Jœµ, we have the following:
JT
œµ‚àÜcx0=KJ‚àí1
œµ‚àÜcx0=K‚àÇœµ
‚àÇcx0¬∑‚àÜcx0=K‚àÜœµ.(11)
WhenLregin Eq. 9 reaches its optimal, we then have:
a=‚àö
1‚àíŒ±t‚à•JT
œµ‚àÜcx0‚à•2=‚àö
1‚àíŒ±tK‚à•‚àÜœµ‚à•2. (12)
Notice that a=a‚à•‚àÜcx0‚à•2, since ‚à•‚àÜcx0‚à•2= 1is the afore-
mentioned random unit length vector. Hence, we can finally
reformulate the expression:
‚à•‚àÜcx0‚à•2=K
a‚àö
1‚àíŒ±t‚à•‚àÜœµ‚à•2
=C‚àö
1‚àíŒ±t‚à•‚àÜœµ‚à•2,(13)
4

--- PAGE 5 ---
which exactly matches our proposed objective in Eq. 8.
To summarize, during training, the Smooth Diffusion ob-
jective encompasses a combination of LbaseandLreg:
L=Lbase+ŒªLreg, (14)
where Lbasedenotes the basic training objective of a diffu-
sion model and Œªrepresents a ratio parameter controlling
the intensity of Step-wise Variation Regularization.
4. Experiments
4.1. Experimental Setup
Baselines and settings. We select the Stable Diffusion [59]
as the primary baseline for all tasks. Additionally, for im-
age interpolation, we adopt a V AE-space interpolation and
ANID [75] as competitors. For image inversion, we inte-
grate Smooth Diffusion and Stable Diffusion with DDIM
inversion [68] and Null-text inversion [42]. For text-based
image editing, SDEdit [40], Prompt-to-Prompt (P2P) [20],
Plug-and-Play (PnP) [72], Diffusion Disentanglement (Dis-
entangle) [79], Pix2Pix-Zero [52] and Cycle Diffusion [78]
are chosen as SOTA approaches. For drag-based image
editing, we compare Smooth Diffusion with Stable Diffu-
sion within the framework of DragDiffusion [66].
Implementation details. Smooth Diffusion is trained atop
pretrained Stable Diffusion-V1.5 [59], using LoRA [24]
finetuning technique. The UNet of Smooth Diffusion is
set as trainable with a LoRA rank of 8, while the V AE and
text encoder are frozen. We leverage the LAION Aesthetics
6.5+ as the training dataset, which contains 625K image-
text pairs with predicted aesthetics scores of 6.5 or higher
from LAION-5B [64]. Smooth diffusion is typically trained
for 30K iterations with a batch size of 96, 3 samples per
GPU, a total of 4 A100 GPUs, and a gradient accumulation
of 8. The AdamW [33] optimizer is adopted with a constant
learning rate of 1√ó10‚àí4and a weight decay of 1√ó10‚àí4.
The ratio parameter Œªin Eq. 14 is set to 1. During infer-
ence, the total number of diffusion steps is set to 50 and the
classifier-free guidance [22] scale is set to 7.5.
Evaluation metrics. To evaluate the general text-to-image
generation performance, we report the popular FID [21] and
CLIP Score [55] on the MS-COCO validation set [36]. To
assess the latent space smoothness, we propose an interpo-
lation standard deviation (ISTD) as an evaluation metric. In
specific, we randomly draw 500 text prompts from the MS-
COCO validation set. For each prompt, we sample a pair of
Gaussian noises and uniformly interpolate them from one to
the other 9 times with mix ratios from 0.1 to 0.9. Fed into
diffusion models together with a prompt, we could obtain
a total of 11 generated images, 2 from the source Gaussian
noises and 9 from the interpolated noises. We calculate the
standard deviation of L2 distances between every two ad-jacent images in the pixel space. Finally, we average the
standard deviations over 500 prompts as ISTD. Ideally, a
zero value of ISTD indicates that consistent and uniform
visual fluctuations in the pixel space for identical fixed-
size changes in the latent space, resulting in a smooth la-
tent space. For image inversion, mean square error (MSE),
LPIPS [90], SSIM [76] and PSNR [26] are adopted to eval-
uate the image reconstruction capability.
4.2. Latent Space Interpolation
Qualitative comparison. The most straightforward way
to demonstrate the smoothness of the latent space is
through the observation of interpolation results between la-
tent noises. In Fig. 3, we present interpolation compar-
isons between Smooth Diffusion and Stable Diffusion using
real images. To generate these comparisons, we utilize the
NTI [42] to invert a pair of real images into latent noises xT,
sharing the same {‚àÖt}T
t=1. We then perform uniform spher-
ical linear interpolations between latent noises (also known
as Stable Diffusion Walk [27]), resulting in 9 intermediate
noises with mix ratios from 0.1 to 0.9. Subsequently, we
concatenate the 11 images produced from these noises to
create an image transition sequence in the figures.
Notably, as highlighted by the red boxes, Stable Diffu-
sion exhibits significant visual fluctuations during the tran-
sition. In particular, the interpolated images may intro-
duce new attributes that are unrelated to the source im-
ages, e.g., the undesired grasslands in the second row
of Fig. 3. In contrast, our approach, Smooth Diffusion, not
only avoids introducing obvious irrelevant attributes in the
interpolated images but also ensures that the visual effects
change smoothly throughout the transition. Additional in-
terpolation results can be seen in supplementary materials.
In addition to Stable Diffusion, Fig. 3 also includes two
other baseline methods for comparison: 1) V AE Interpola-
tion (V AE Inter.), which performs interpolations within the
V AE space of Stable Diffusion. However, the results closely
resemble pixel-space interpolations, with significant degra-
dation of visual details, particularly in the highlighted red
box area. 2) ANID [75], which first adds noise to real im-
ages and subsequently denoises the interpolated noisy im-
ages using Stable Diffusion. In Fig. 3, ANID with a 50-
step scheduler exhibits highly blurred interpolation results.
When ANID operates with a default 200-step scheduler, the
blurring can be alleviated, but the quality of the interpolated
images remains far from satisfactory.
Quantitative comparison. The goal of Smooth Diffu-
sion is to enhance the latent space smoothness without im-
age generation performance degradation compared to Sta-
ble Diffusion. In pursuit of this goal, we employ the ISTD
introduced in Sec. 4.1 to evaluate the latent space smooth-
ness. Additionally, we utilize FID [21] and CLIP Score [55]
to assess generators‚Äô overall performance. The results pre-
5

--- PAGE 6 ---
SmoothDiffusion(50 steps)StableDiffusion(50 steps)VAEInter.ANID(50 steps)ANID(200 steps)Image AImage BInterpolation‚ÄúA church‚ÄùFigure 3. Image interpolation comparison results. For Smooth Diffusion and Stable Diffusion [59], real images (Image A and B) are
inverted into latents using NTI [42]. We perform spherical linear interpolations between latents (also known as Stable Diffusion Walk [27])
and concatenate the resulting images as a transition sequence. V AE Inter. performs interpolations within the V AE space of Stable Diffusion.
ANID [75] first adds noise to real images and subsequently denoises the interpolated noisy images using Stable Diffusion.
Method ISTD ( ‚Üì) FID ( ‚Üì) CLIP Score ( ‚Üë)
Stable Diffusion 38.63 12.70 31.46
Smooth Diffusion 16.54 12.10 31.54
Table 1. Quantitative evaluations of image interpolation and
text-to-image generation. We evaluate Smooth Diffusion and
Stable Diffusion [59] with ISTD, FID [21] and CLIP Score [55].
The better results are in bold.
sented in Tab. 1 demonstrate that Smooth Diffusion signifi-
cantly outperforms Stable Diffusion in terms of ISTD, indi-
cating a substantial improvement in the latent space smooth-
ness. Furthermore, Smooth Diffusion exhibits superior per-
formance in both FID and CLIP Score, suggesting that the
enhancement of latent space smoothness and the overall im-
age generation quality are not mutually exclusive but com-
plement each other when the regularization term is applied
with a suitable strength ratio.
4.3. Image Inversion and Reconstruction
Previous research [30] in the realm of GANs discovered that
a smoother latent space has a positive impact on the accu-
racy of image inversion and reconstruction. We empirically
validate this finding within the context of diffusion mod-
els. In specific, two representative inversion techniques,
DDIM inversion [68] and Null-text inversion (NTI) [42] are
adopted and integrated with Smooth Diffusion and Stable
Diffusion separately. We both qualitatively and quantita-
Smooth Diff.+NTI (Ours)Stable Diff.+NTISmooth Diff.+DDIM (Ours)Stable Diff.+DDIMSource‚ÄúA man using his laptop computer while a cat sits on his lap‚Äù‚ÄúA large tower that has a big clock at top‚Äù‚ÄúA room with a couch, table set with dinnerware and a television‚Äù
Figure 4. Image reconstruction comparison results. We inte-
grate Smooth Diffusion and Stable Diffusion [59] with NTI [42]
(column 2 & 3) and DDIM inversion [68] (column 4 & 5).
tively compare the image inversion and reconstruction per-
formance of these integrated models using 500 randomly
sampled images from the MS-COCO validation set [36].
As illustrated in the two rightmost columns of Fig. 4,
when employing a straightforward DDIM inversion,
Smooth Diffusion outperforms Stable Diffusion by a con-
siderable margin in terms of reconstruction quality. This
improvement is evident in various aspects, such as an accu-
rate generation of character identities, a faithful recreation
6

--- PAGE 7 ---
Smooth Diff.Stable Diff.SDEditP2PPnPDisentanglePix2Pix-ZeroCycle Diff.‚ÄúA chocolate cake with cream on it‚Äù‚Üí‚ÄúA chocolate cake with strawberrieson it‚Äù
Source
‚ÄúA banana on the table‚Äù‚Üí‚ÄúA banana and an apple on the table‚Äù
‚ÄúA young girl face with long black hair‚Äù‚Üí‚ÄúA young girl face with long black hair, cartoon style‚ÄùLocalEdit(Replace Item)LocalEdit(Add Item)GlobalEdit(Transfer Style)Figure 5. Text-based image editing comparison results. We compare Smooth Diffusion and Stable Diffusion [59] (column 2 & 3),
considering both local and global edits through the straightforward pipeline described in Sec. 4.4. Additionally, we present results from
SOTA approaches, including SDEdit [40], P2P [20], PnP [72], Disentangle [79], Pix2Pix-Zero [52], and Cycle Diffusion [78], as references.
Method MSE ( ‚Üì) LPIPS ( ‚Üì) SSIM ( ‚Üë) PSNR ( ‚Üë)
Stable Diff. + DDIM 0.1756 0.5385 0.2662 13.97
Smooth Diff. + DDIM 0.1086 0.4326 0.3418 16.17
Stable Diff. + NTI 0.0156 0.1656 0.6068 25.63
Smooth Diff. + NTI 0.0153 0.1635 0.6102 25.74
V AE Reconstruction 0.0148 0.1590 0.6136 25.98
Table 2. Quantitative evaluations of image reconstruction. We
integrate Stable Diffusion and Smooth Diffusion [59] with DDIM
inversion [68] (row 2 & 3) and NTI [42] (row 4 & 5). MSE,
LPIPS [90], SSIM [76] and PSNR [26] are evaluated. V AE Re-
construction results are provided as the optimal values.
of the city view behind the tower, and a correct reproduc-
tion of room layouts. This phenomenon underscores the
fact that the latent space of Smooth Diffusion is more tol-
erant of the errors introduced by the local linear approxi-
mation in DDIM inversion. Consequently, the reconstruc-
tion results produced by Smooth Diffusion manage to retain
the contents of the source images to a greater extent. On
the other hand, when the optimization-based NTI technique
is employed, the disparity between Smooth Diffusion and
Stable Diffusion is not as pronounced. Nonetheless, there
are still instances where Stable Diffusion exhibits subpar
results, such as the ruined man‚Äôs face in Fig. 4.
To quantify the image reconstruction performance, MSE,
LPIPS [90], SSIM [76] and PSNR [26] are reported
in Tab. 2. Notably, the reconstruction error encompasses
two components: 1) the error from different inversion meth-
ods and U-Net parameters and 2) the error from the shared
pretrained V AE [34]. Hence, we included the V AE recon-
struction errors as optimal values for our method. The re-
sults exhibit a consistent outperformance of Smooth Diffu-
sion over Stable Diffusion across all metrics, whether using
DDIM inversion or NTI. Moreover, ‚ÄúSmooth Diffusion +NTI‚Äù performs results close to V AE reconstruction, indicat-
ing its superiority attributed to a smoother latent space.
4.4. Image Editing
The superiority of Smooth Diffusion in image inversion and
reconstruction has motivated us to explore its potential for
enhancing image editing tasks. In this section, we delve
into two typical image editing scenarios: text-based image
editing and drag-based image editing.
Text-based image editing. There have been numerous
methods [20, 40, 52, 72, 78, 79] proposed in the litera-
ture, each with its own unique designs aimed at achieving
the SOTA performance. In contrast, we adopt a simpler
pipeline akin to the image inversion and reconstruction pro-
cess discussed in Sec. 4.3. The key distinction lies in our
approach to modify the text prompt during the later time
steps of the reconstruction process. In specific, the original
œµŒ∏(xt, t,C,‚àÖt)in Eq. (5) during NTI reconstruction (diffu-
sion sampling) process is replaced with:
œµŒ∏(xt, t,C,‚àÖt) =(
œµŒ∏(xt, t,Csrc,‚àÖt), t > T √ór,
œµŒ∏(xt, t,Ctrg,‚àÖt), t‚â§T√ór,(15)
where Csrcrepresents the source text prompt for inversion,
whileCtrgcorresponds to the target text prompt for editing.
The parameter rserves as a threshold, determining when to
switch from CsrctoCtrg. In practice, ris typically chosen
within {0.6, 0.7, 0.8, 0.9 }, with the exact value depending
on the specific input images and target visual effects.
Through this straightforward pipeline, we conducted a
comparative analysis of the editing performance between
Smooth Diffusion and Stable Diffusion, as presented in the
three left-most columns of Fig. 5. We also included editing
results obtained from SOTA methods as references. Our
7

--- PAGE 8 ---
SmoothDiff.SourceStable Diff.User Edit
‚ÄúA photo of a cat‚Äù‚ÄúA photo of a landscape‚Äù‚ÄúA photo of  flowers‚ÄùFigure 6. Drag-based image editing comparison results. We
implement Smooth Diffusion and Stable Diffusion [59] within the
framework of DragDiffusion [66], respectively.
evaluation encompasses both local and global editing tasks.
The local editing tasks involve replacing items ( e.g., chang-
ing ‚Äúcream‚Äù to ‚Äústrawberries‚Äù) and adding items ( e.g., ‚Äúap-
ple‚Äù). On the other hand, the global editing tasks pertain to
global style transfer, such as transforming an image into a
‚Äúcartoon style‚Äù. It is evident that while Stable Diffusion ex-
cels in achieving precise image reconstruction with NTI, as
discussed in Sec. 4.3, even minor modifications to the text
prompt can significantly impact the content of the generated
images. For instance, it can affect elements like the style of
the cake, the shape of the banana, and the haircut of the girl.
In contrast, Smooth Diffusion not only accurately generates
edited images in accordance with the target text prompts
but also effectively preserves the unedited contents. Fur-
thermore, when compared to SOTA methods, even with this
straightforward pipeline, Smooth Diffusion consistently de-
livers competitive results across all cases.
Drag-based image editing. As an emerging research av-
enue in the community, drag-based image editing [43, 50,
66] has garnered considerable attention recently. DragDif-
fusion [66] first introduces a framework for drag-based
image editing employing Stable Diffusion. In the task 3
of Fig. 1 and Fig. 6, we showcase that by integrating Smooth
Diffusion into the DragDiffusion framework, some previ-
ously unsuccessful editing operations with Stable Diffusion
can be enabled. As illustrated, Smooth Diffusion achieves
operations such as making the tree grow taller without dam-
aging existing branches (Fig. 1), rotating the cat head, creat-
ing a new mountain top without destroying the original one,
and letting new flowers grow in the vase (Fig. 6). These op-
erations, however, fail with Stable Diffusion, indicating the
non-smoothness of its latent space.4.5. Ablation Studies
Regularization ratio. In Tab. 3, we examine the impact
of different strength ratios Œªin Eq. (14). This ratio ad-
justs the intensity of the step-wise variation regularization.
Specifically, when a weaker regularization is applied ( e.g.,
Œª= 0.1), we observe a slight improvement in the CLIP
Score. However, there is a significant increase in ISTD,
indicating a notable degradation in latent space smooth-
ness. In contrast, employing a stronger regularization ( e.g.,
Œª= 10 ) leads to a smoother latent space, as demonstrated
by the decrease in ISTD. However, in this case, we observe
an unexpected increase in FID, indicating a notable decline
in the quality of generated images. Therefore, selecting an
appropriate trade-off value for Œªbecomes crucial based on
the specific experimental settings. In our default setting, we
find that Œª= 1serves as a suitable value.
Ratio ISTD ( ‚Üì) FID ( ‚Üì) CLIP Score ( ‚Üë)
0.1 24.23 12.15 31.56
1 (default) 16.54 12.11 31.49
10 11.51 17.44 31.41
Table 3. Ablation results of different regularization ratios. The
best results are in bold, and the second-best results are underlined.
LoRA rank. In Tab. 4, we examine the impact of differ-
ent ranks of the LoRA component utilized in our Smooth
diffusion. We discover that LoRA ranks within the range
of [4,16] are all suitable values for our default setting. We
select a default rank of 8 because of its lowest ISTD among
the first three rows in Tab. 4. Furthermore, we train a fully
finetuned model, referred to as ‚Äùfull,‚Äù which showcases a
further decrease in ISTD. However, this comes at the ex-
pense of significantly degrading the quality of the generated
images, as indicated by an increased FID and decreased
CLIP Score. This decline in performance underscores the
vulnerability of fully fine-tuned models to collapse within
our default setting, emphasizing the need for additional
meticulous design considerations.
Rank ISTD ( ‚Üì) FID ( ‚Üì) CLIP Score ( ‚Üë)
4 16.76 12.36 31.49
8 (default) 16.54 12.11 31.54
16 16.65 11.49 31.61
full 11.52 27.27 28.86
Table 4. Ablation results of different LoRA ranks. The best
results are in bold, and the second-best results are underlined.
5. Conclusion
In this article, we explored Smooth Diffusion, an innova-
tive diffusion model that enhances latent space smoothness
8

--- PAGE 9 ---
for generation. Smooth Diffusion adopts the novel Step-
wise Variation Regularization, which successfully main-
tains variation between arbitrary input latent and gener-
ated images at a more bounded range. Smooth Diffusion
was trained on top of the prevailing text-to-image model,
from which we carried out extensive research, including but
not limited to interpolation, inversion, and editing, all of
which had shown competitive performance. Through quali-
tative and quantitative measurements, we demonstrated that
Smooth Diffusion managed to make a smoother latent space
without compromising the output quality. We believe that
Smooth Diffusion will become a valuable solution for other
challenging tasks, such as video generation, in the future.
References
[1] OpenJourney-V4. https : / / huggingface . co /
prompthero/openjourney-v4 , 2023. 12, 14
[2] RealisticVision-V2. https://huggingface.co/
SG161222/Realistic_Vision_V2.0 , 2023. 12, 14
[3] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2StyleGAN++: How to edit the embedded images? In
CVPR , 2020. 3
[4] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:
A residual-based stylegan encoder via iterative refinement.
InICCV , 2021. 3
[5] Martin Arjovsky and L ¬¥eon Bottou. Towards princi-
pled methods for training generative adversarial networks.
arXiv:1701.04862 , 2017. 3
[6] Martin Arjovsky, Soumith Chintala, and L ¬¥eon Bottou.
Wasserstein generative adversarial networks. In ICML , 2017.
3
[7] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-
DPM: an analytic estimate of the optimal reverse variance in
diffusion probabilistic models. In ICLR , 2022. 2
[8] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu,
Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu.
One transformer fits all distributions in multi-modal diffu-
sion at scale. In ICML , 2023. 2
[9] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou,
Joshua B Tenenbaum, William T Freeman, and Antonio Tor-
ralba. Gan dissection: Visualizing and understanding gener-
ative adversarial networks. In ICLR , 2018. 3
[10] Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason
Yosinski. Deep generative stochastic networks trainable by
backprop. In ICML , 2014. 2
[11] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
arXiv:1809.11096 , 2018. 3
[12] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit genera-
tive adversarial networks for 3d-aware image synthesis. In
CVPR , 2021. 3
[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. In NeurIPS , 2021. 2, 3[14] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han.
Prompt tuning inversion for text-driven image editing using
diffusion models. arXiv:2305.04441 , 2023. 2
[15] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe,
Trevor Darrell, Zhangyang Wang, and Humphrey Shi. Pair-
diffusion: Object-level image editing with structure-and-
appearance paired diffusion models. arXiv:2303.17546 ,
2023. 2
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NeurIPS ,
2014. 2, 3
[17] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
CVPR , 2022. 2
[18] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
Dumoulin, and Aaron C Courville. Improved training of
wasserstein gans. NeurIPS , 30, 2017. 3
[19] Jiayi Guo, Chaofei Wang, You Wu, Eric Zhang, Kai Wang,
Xingqian Xu, Humphrey Shi, Gao Huang, and Shiji Song.
Zero-shot generative model adaptation via image-specific
prompt learning. In CVPR , 2023. 2
[20] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv:2208.01626 ,
2022. 2, 5, 7
[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS , 2017. 5, 6
[22] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS Workshops , 2021. 3, 5
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 2, 3
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-rank adaptation of large language models. In
ICLR , 2022. 2, 3, 5
[25] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli
Zhao, and Jingren Zhou. Composer: Creative and
controllable image synthesis with composable conditions.
arXiv:2302.09778 , 2023. 2
[26] Quan Huynh-Thu and Mohammed Ghanbari. Scope of va-
lidity of psnr in image/video quality assessment. Electronics
letters , 2008. 5, 7
[27] Andrej Karpathy. Stable Diffusion Walk.
https : / / gist . github . com / karpathy /
00103b0037c5aaea32fe1da1af553355 , 2022.
2, 5, 6
[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019. 2
[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,
Jaakko Lehtinen, and Timo Aila. Training generative adver-
sarial networks with limited data. In NeurIPS , 2020. 3
9

--- PAGE 10 ---
[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In CVPR , 2020. 2, 3, 4, 6
[31] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¬®ark¬®onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. In NeurIPS , 2021. 2, 3
[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. In NeurIPS , 2022. 2
[33] Diederik P Kingma and Jimmy Ba. ADAM: A method for
stochastic optimization. In ICLR , 2015. 5
[34] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. In ICLR , 2015. 2, 7, 12
[35] Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey
Tulyakov, and Ming-Hsuan Yang. Infinitygan: Towards
infinite-pixel image synthesis. In ICLR , 2021. 3
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 5, 6, 12
[37] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion
probabilistic model sampling in around 10 steps. In NeurIPS ,
2022. 2
[38] Haoming Lu, Hazarapet Tunanyan, Kai Wang, Shant
Navasardyan, Zhangyang Wang, and Humphrey Shi. Spe-
cialist diffusion: Plug-and-play sample-efficient fine-tuning
of text-to-image diffusion models to learn any unseen style.
InCVPR , 2023. 2
[39] Weijian Mai and Zhijun Zhang. Unibrain: Unify image re-
construction and captioning all in one diffusion model from
human brain activity. arXiv:2308.07428 , 2023. 2
[40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In ICLR , 2022. 2, 5, 7
[41] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik
Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
On distillation of guided diffusion models. In CVPR , 2023.
2
[42] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real images
using guided diffusion models. In CVPR , 2023. 2, 3, 5, 6, 7,
12, 13, 14
[43] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and
Jian Zhang. Dragondiffusion: Enabling drag-style manipu-
lation on diffusion models. arXiv:2307.02421 , 2023. 2, 8
[44] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv:2302.08453 , 2023. 2
[45] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In ICML , 2021. 2
[46] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya
Sutskever, and Mark Chen. GLIDE: Towards photorealis-
tic image generation and editing with text-guided diffusion
models. In ICML , 2022. 2[47] Yotam Nitzan, Amit Bermano, Yangyan Li, and Daniel
Cohen-Or. Face identity disentanglement via latent space
mapping. TOG , 2020. 3
[48] Augustus Odena, Jacob Buckman, Catherine Olsson, Tom
Brown, Christopher Olah, Colin Raffel, and Ian Goodfel-
low. Is generator conditioning causally related to gan per-
formance? In ICML , 2018. 3, 4
[49] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin,
Chen Change Loy, and Ping Luo. Exploiting deep genera-
tive prior for versatile image restoration and manipulation.
TPAMI , 2021. 3
[50] Xingang Pan, Ayush Tewari, Thomas Leimk ¬®uhler, Lingjie
Liu, Abhimitra Meka, and Christian Theobalt. Drag your
GAN: Interactive point-based manipulation on the generative
image manifold. In SIGGRAPH , 2023. 3, 8
[51] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen
Huang. Effective real image editing with accelerated iterative
diffusion inversion. In ICCV , 2023. 2
[52] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. In SIGGRAPH , 2023. 2, 5, 7
[53] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. StyleCLIP: Text-driven manipulation
of StyleGAN imagery. In ICCV , 2021. 3
[54] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,
Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming
Xiong, Silvio Savarese, et al. Unicontrol: A unified dif-
fusion model for controllable visual generation in the wild.
arXiv:2305.11147 , 2023. 2
[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , 2021.
2, 5, 6
[56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv:2204.06125 , 2022. 2
[57] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,
Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding
in style: a stylegan encoder for image-to-image translation.
InCVPR , 2021. 3
[58] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-
ages. ACM Trans. Graph. , 2021. 3
[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1, 2, 5,
6, 7, 8, 12, 13
[60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 2
[61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,
Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad
10

--- PAGE 11 ---
Norouzi. Photorealistic text-to-image diffusion models with
deep language understanding. In NeurIPS , 2022. 2
[62] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In ICLR , 2022. 2
[63] Tim Salimans, Diederik Kingma, and Max Welling. Markov
chain monte carlo and variational inference: Bridging the
gap. In ICML , 2015. 2
[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. LAION-5B: An open large-scale dataset for train-
ing next generation image-text models. In NeurIPS , 2022. 5
[65] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou.
InterfaceGAN: Interpreting the disentangled face represen-
tation learned by GANs. TPAMI , 2020. 2, 3
[66] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-
cent YF Tan, and Song Bai. DragDiffusion: Harnessing
diffusion models for interactive point-based image editing.
arXiv:2306.14435 , 2023. 2, 5, 8, 15
[67] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , 2015. 2
[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR , 2020. 2, 3, 5, 6, 7,
12, 14
[69] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. In ICML , 2023. 2
[70] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and
Mohit Bansal. Any-to-any generation via composable diffu-
sion. arXiv:2305.11846 , 2023. 2
[71] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and
Daniel Cohen-Or. Designing an encoder for StyleGAN im-
age manipulation. TOG , 2021. 3
[72] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In CVPR , 2023. 2, 5, 7
[73] Pascal Vincent. A connection between score matching and
denoising autoencoders. Neural computation , 23(7):1661‚Äì
1674, 2011. 2
[74] Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT:
Exact diffusion inversion via coupled transformations. In
CVPR , 2023. 2
[75] Clinton Wang and Polina Golland. Interpolating between im-
ages with diffusion models. In ICML Workshops , 2023. 2, 5,
6
[76] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment: from error visibility
to structural similarity. TIP, 2004. 5, 7
[77] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen,
Pengcheng He, Weizhu Chen, Zhangyang Wang, and
Mingyuan Zhou. In-context learning unlocked for diffusion
models. arXiv:2305.01115 , 2023. 2
[78] Chen Henry Wu and Fernando De la Torre. A latent space of
stochastic diffusion models for zero-shot image editing and
guidance. In ICCV , 2023. 2, 5, 7
[79] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,
Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and ShiyuChang. Uncovering the disentanglement capability in text-
to-image diffusion models. In CVPR , 2023. 2, 5, 7
[80] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Wensen Feng.
Controllable continuous gaze redirection. In ACM MM ,
2020. 3
[81] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei
Zhou, and Ming-Hsuan Yang. Gan inversion: A survey.
TPAMI , 2022. 3
[82] Dejia Xu, Xingqian Xu, Wenyan Cong, Humphrey Shi,
and Zhangyang Wang. Reference-based painterly inpaint-
ing via diffusion: Crossing the wild reference domain gap.
arXiv:2307.10584 , 2023. 2
[83] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang,
Irfan Essa, and Humphrey Shi. Prompt-free diffu-
sion: Taking‚Äù text‚Äù out of text-to-image diffusion models.
arXiv:2305.16223 , 2023. 2
[84] Xingqian Xu, Shant Navasardyan, Vahram Tadevosyan, An-
dranik Sargsyan, Yadong Mu, and Humphrey Shi. Image
completion with heterogeneously filtered spectral hints. In
WACV , 2023. 3
[85] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,
and Humphrey Shi. Versatile diffusion: Text, images and
variations all in one diffusion model. In ICCV , 2023. 2
[86] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-
Adapter: Text compatible image prompt adapter for text-to-
image diffusion models. arXiv:2308.06721 , 2023. 2
[87] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and
Humphrey Shi. Forget-me-not: Learning to forget in text-to-
image diffusion models. arXiv preprint arXiv:2303.17591 ,
2023.
[88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 2
[89] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-
fusion models with exponential integrator. arXiv preprint
arXiv:2204.13902 , 2022. 2
[90] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 5, 7
[91] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao
Liang, I Eric, Chao Chang, and Yan Xu. Large scale im-
age completion via co-modulated generative adversarial net-
works. In ICLR , 2020. 3
[92] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin
Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong. Uni-
ControlNet: All-in-one control to text-to-image diffusion
models. In NeurIPS , 2023. 2
[93] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and
Jiwen Lu. UniPC: A unified predictor-corrector framework
for fast sampling of diffusion models. In NeurIPS , 2023. 2
[94] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-
domain gan inversion for real image editing. In ECCV , 2020.
3
11

--- PAGE 12 ---
Supplementary Materials
A. Implementation Details
This section elaborates on details briefly introduced in the
main paper. These include the notation, the basic training
objective, the interpolation standard deviation (ISTD) met-
ric, and our utilization of Null-text inversion (NTI) [42] for
real-image interpolation.
A.1. Notation
Stable Diffusion [59] employs an efficient ‚Äúlatent‚Äù diffusion
pipeline. Here the ‚Äúlatent‚Äù refers to using an individually
trained (V AE) [34] to compress an input image x0into its
V AE-space representation z0:
z0=E(x0),x0=D(z0), (16)
where EandDrepresent the encoder and decoder of the
V AE, respectively. For simplicity, we exclude this conver-
sion process and only use ‚Äú x‚Äù-based notations in the main
paper. Although we chose Stable Diffusion as our base-
line due to its popularity and high performance, our train-
ing pipeline is not specifically tailored for latent diffusion
models and is compatible with other diffusion models.
A.2. Basic Training Objective
Smooth Diffusion‚Äôs training objective comprises two key
components: 1) a basic training objective primarily centered
on noise prediction but flexible in formulation for different
diffusion models, and 2) our proposed Step-wise Variation
Regularization term. In our experiments, the basic training
objective is:
Lbase=Ex0,œµ,t‚à•œµ‚àíœµŒ∏(xt, t)‚à•2
2, (17)
which is a commonly adopted training objective across
many diffusion models, e.g., Stable Diffusion [59].
A.3. ISTD
The goal of ISTD is to quantify the deviation of pixel-space
changes given the same fixed-step changes in latent space.
A lower deviation implies the input latents and output im-
ages are more likely to change smoothly. In our experi-
ments, we first randomly draw 500 text prompts from the
MS-COCO validation set [36]. For each prompt, we then
sample two random Gaussian noises, œµaandœµb. Next, we
execute uniform spherical linear interpolations (slerp) be-
tween œµaandœµbfor 11 times, varying the mixing ratio Œ∑
from 0 to 1:
œµŒ∑= slerp( œµa,œµb, Œ∑), Œ∑ = 0,0.1,0.2,¬∑¬∑¬∑,1. (18)
We employ the testing diffusion model to generate 11
interpolated images {cxŒ∑
0}1
Œ∑=0from{œµŒ∑}1
Œ∑=0. Notice that
Eq. 18 guarantees that the latent space changes between ev-
ery two adjacent latents ( i.e.,œµŒ∑andœµŒ∑+0.1) are the same.Hence, we calculate the L2 distances between every two ad-
jacent images ( i.e.,cxŒ∑
0and\xŒ∑+0.1
0 ) and compute the stan-
dard deviation of these distances. Finally, ISTD is the aver-
age of standard deviations over 500 different text prompts.
For a fair comparison, the text prompts and the noises for
each prompt are the same for different testing models.
A.4. NTI for real-image interpolation
NTI is initially designed to transform a real image x0into a
latentfxT, along with a series of learnable null-text embed-
dings{‚àÖt}T
t=1for each step t. The optimization for each
‚àÖtis formulated as:
min
‚àÖt‚à•]xt‚àí1‚àíDDIM( fxt, t, Œæ,‚àÖt)‚à•2
2. (19)
where {fxt}T
t=1represents intermidiate noisy images es-
timated by DDIM inversion [68]. For simplicity,
DDIM( fxt, t, Œæ,‚àÖt)denotes the DDIM sampling process at
stept, utilizing the text embedding Œæ, the null-text embed-
ding‚àÖtand the classifier-free guidance scale w= 7.5.
For real-image interpolation, we optimize a shared series
of{‚àÖt}T
t=1for two real images, xa
0andxb
0:
min
‚àÖt‚à•]xa
t‚àí1‚àíDDIM( fxa
t, t, Œæ,‚àÖt)‚à•2
2+
‚à•]xb
t‚àí1‚àíDDIM(fxb
t, t, Œæ,‚àÖt)‚à•2
2.(20)
In our experiments, we only interpolate the latents fxa
T
andfxb
Tfollowing Eq. 18 and use the same null-text embed-
dings{‚àÖt}T
t=1for all interpolated images.
B. Additional Results
This section provides additional visual results of Smooth
Diffusion. We display image interpolation results in Fig. 7
and Fig. 8, image inversion and reconstruction results
in Fig. 9, and image editing results in Fig. 10.
Reusability. The LoRA component of Smooth Diffusion
remains adaptable to other models sharing the same archi-
tecture as Stable Diffusion. However, the effectiveness of
this reusability is not guaranteed. We evaluate the integra-
tion of this LoRA component into two popular community
models, RealisticVision-V2 [2] and OpenJourney-V4 [1].
As depicted in Fig. 8, this integration also enhances the
latent space smoothness of these models. This reusability
makes our method eliminate the need for repeated training
and become a plug-and-play module across various models.
12

--- PAGE 13 ---
‚ÄúA cuterabbit‚Äù‚ÄúAwoman face‚Äù
‚ÄúA beautiful landscape‚Äù
‚ÄúA chocolate cake‚Äù
Image AImage BInterpolationStableDiffusion
StableDiffusionSmoothDiffusionStableDiffusionSmoothDiffusionSmoothDiffusionSmoothDiffusionStableDiffusionFigure 7. Additional image interpolation results with Smooth Diffusion. For Smooth Diffusion and Stable Diffusion [59], real images
(Image A and B) are inverted into latents using Null-text inversion [42]. We perform spherical linear interpolations between latents and
concatenate the resulting images as a transition sequence.
13

--- PAGE 14 ---
‚ÄúA basket of apples‚Äù
‚ÄúA robot horse‚ÄùSmoothRealisticVision-V2
Image AImage BInterpolationRealisticVision-V2SmoothOpenJourney-V4OpenJourney-V4
Figure 8. Image interpolation results with community models. We apply the LoRA component of Smooth Diffusion to RealisticVision-
V2 [2] and OpenJournery-V4 [1] and perform spherical linear interpolations in their latent spaces.
‚ÄúA city bus is parked on the curb waiting for people‚Äù
‚ÄúA dog that is wearing a dog collar smiling‚Äù‚ÄúA hand holding a smart phone with apps on a screen‚Äù‚ÄúA plate of cooked food in seen in this image‚Äù
‚ÄúA skateboard that has its wheels on the floor‚Äù
‚ÄúThe train engine number 6309 is operated by BNSF‚Äù
‚ÄúLarge four sided clock hangs on the corner of the building‚Äù
‚ÄúAn older Dodge pickup sits parked next to another older pickup‚Äù
‚ÄúA woman walking down a street talking on a cell phone‚ÄùSmooth Diffusion+NTI Smooth Diffusion+DDIM SourceSmooth Diffusion+NTI Smooth Diffusion+DDIM SourceSmooth Diffusion+NTI Smooth Diffusion+DDIM Source
Figure 9. Additional image inversion and reconstruction results with Smooth Diffusion. We integrate Smooth Diffusion with two
typical diffusion inversion techniques, Null-text inversion [42] and DDIM inversion [68].
14

--- PAGE 15 ---
Smooth DiffusionSourceUserEditLocalEdit(Replace Item)LocalEdit(Add Item)GlobalEdit(Transfer Style)DragEdit(MovePoint)
‚ÄúA basket of apples‚Äù‚Äúapples‚Äù‚Üì‚Äúoranges‚Äù
‚ÄúA bird standing on a branch‚Äù‚Äúbird‚Äù‚Üì‚ÄúLego bird‚Äù‚Äúdog‚Äù‚Üì‚Äútoy dog‚Äù‚ÄúA dog standing on the bench‚Äù
‚ÄúA dog inajacket‚Äù+‚Äúwearing sunglasses‚Äù+‚Äúwith a turtle on it‚Äù
‚ÄúAbeach‚Äù
‚ÄúA young girl‚Äù+‚Äúwearinga hat‚Äù‚ÄúA sitting cat‚Äù‚Äúanimepainting‚Äù‚Üí‚ÄúA photo of a tree‚Äù
‚ÄúA river with trees on both sides‚Äù‚ÄúVan Gogh painting‚Äù‚Üí
‚ÄúA red car with a black roof‚Äù‚ÄúWatercolordrawing‚Äù‚Üí
‚ÄúAn oil painting of a mountain‚ÄùSmooth DiffusionSourceUserEdit
Smooth DiffusionSourceUserEdit‚ÄúA photo of a river‚ÄùFigure 10. Additional image editing results with Smooth Diffusion. Both text-based image editing and drag-based image editing are
evaluated. For text-based image editing, we consider both local and global edits to test Smooth Diffusion. For drag-based image editing,
Smooth Diffusion is integrated into the framework of DragDiffusion [66].
15
