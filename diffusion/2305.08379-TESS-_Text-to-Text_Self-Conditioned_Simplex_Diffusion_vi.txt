# 2305.08379.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2305.08379.pdf
# Kích thước tệp: 852593 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TESS: Mô hình khuếch tán simplex tự điều kiện từ văn bản đến văn bản
Rabeeh Karimi Mahabadi1,4∗Hamish Ivison3,5∗†Jaesung Tae2
James Henderson4Iz Beltagy3Matthew E. Peters3†‡Arman Cohan2,3‡
1EPFL2Đại học Yale3Viện Allen cho AI
4Viện nghiên cứu Idiap5Đại học Washington
rabeeh.karimimahabadi@epfl.ch ,hamishi@allenai.org

Tóm tắt
Các mô hình khuếch tán đã nổi lên như một
mô hình mạnh mẽ để tạo sinh, đạt được hiệu
suất tốt trong các miền liên tục khác nhau.
Tuy nhiên, việc áp dụng các mô hình khuếch
tán liên tục cho ngôn ngữ tự nhiên vẫn là
thách thức do bản chất rời rạc của nó và
nhu cầu về số lượng lớn các bước khuếch
tán để tạo văn bản, khiến việc tạo sinh dựa
trên khuếch tán trở nên đắt đỏ. Trong nghiên
cứu này, chúng tôi đề xuất Mô hình khuếch
tán simplex tự điều kiện từ văn bản đến văn
bản (TESS), một mô hình khuếch tán văn
bản hoàn toàn không tự hồi quy, sử dụng
một hình thức tự điều kiện mới, và áp dụng
quá trình khuếch tán trên không gian simplex
logit thay vì không gian embedding đã học.
Thông qua các thí nghiệm rộng rãi trên các
tác vụ hiểu và tạo sinh ngôn ngữ tự nhiên
bao gồm tóm tắt, đơn giản hóa văn bản, tạo
paraphrase và tạo câu hỏi, chúng tôi chứng
minh rằng TESS vượt trội hơn các mô hình
không tự hồi quy tiên tiến, yêu cầu ít bước
khuếch tán hơn với sự giảm hiệu suất tối
thiểu, và có khả năng cạnh tranh với các mô
hình chuỗi đến chuỗi tự hồi quy đã được
tiền huấn luyện. Chúng tôi công bố công
khai codebase của mình.1

1 Giới thiệu
Các mô hình khuếch tán (Sohl-Dickstein et al., 2015; Ho
et al., 2020; Song et al., 2021) đã đạt được
hiệu suất tiên tiến trong các miền liên tục
khác nhau, như hình ảnh (Nichol và Dhariwal,
2021), âm thanh (Kong et al., 2020; Shen et al., 2023),
video (Ho et al., 2022), và tạo sinh từ văn bản
sang hình ảnh (Saharia et al., 2022; Ramesh et al., 2022).
Được truyền cảm hứng từ thành công của khuếch
tán cho các miền liên tục, các nghiên cứu gần
đây đã điều chỉnh khuếch tán cho không gian
rời rạc, như văn bản (Austin et al., 2021;
Hoogeboom et al., 2021; Savinov et al., 2021; Reid
et al., 2022). Một hướng nghiên cứu đề xuất
khuếch tán trong không gian tiềm ẩn của mô
hình bằng cách thêm nhiễu Gaussian vào embedding
từ đầu vào (Li et al., 2022b). Một phương pháp
khác, SSD-LM (Han et al., 2022), thêm nhiễu
vào simplex xác suất từ vựng.

∗Đồng tác giả chính.
†Công việc được thực hiện trong thời gian làm việc tại AI2.
‡Tư vấn ngang hàng.
1https://github.com/allenai/tess-diffusion

Khuếch tán trực tiếp trên simplex xác suất là
mong muốn (Richemond et al., 2022) vì nó loại
bỏ nhu cầu về một bước bổ sung để ánh xạ
embedding đã khuếch tán thành đầu vào rời
rạc thực tế hoặc các phương pháp phụ trợ như
mã hóa nhị phân (Chen et al., 2022). Tuy nhiên,
mặc dù có hiệu suất mạnh mẽ, SSD-LM vẫn
có một số hạn chế: thiếu tự điều kiện
(Chen et al., 2022), thiếu đánh giá rộng rãi
trên các tác vụ hạ nguồn, và đáng chú ý nhất,
giới hạn tạo sinh các khối 25 token, điều này
cản trở những lợi ích tiềm năng của khuếch
tán đầy đủ, ví dụ như khả năng thực hiện
điền khuyết tùy ý, tạo sinh linh hoạt và
góc nhìn toàn cầu của chuỗi.

Trong nghiên cứu này, chúng tôi trình bày TESS,
một mô hình khuếch tán từ văn bản đến văn
bản, vượt qua một số hạn chế của các nghiên
cứu trước: giới hạn về quy mô (Hoogeboom
et al., 2021; Austin et al., 2021), phụ thuộc
vào embedding đã tiền huấn luyện (Strudel et al.,
2022), bản chất bán tự hồi quy (Han et al.,
2022), và độ dài tạo sinh ngắn (Gong et al.,
2023). TESS tuân thủ chặt chẽ Han et al. (2022,
2023a) bằng cách thực hiện khuếch tán trên
không gian logit từ vựng thay vì không gian
embedding thông thường. Tuy nhiên, không
giống như SSD-LM, TESS hoàn toàn không
tự hồi quy và thực hiện khuếch tán trên toàn
bộ chuỗi. Nó cũng kết hợp một hình thức tự
điều kiện mới, chứng minh được lợi thế cạnh
tranh so với phương pháp tự điều kiện ban
đầu (Chen et al., 2022) và cải thiện đáng kể
hiệu quả và chất lượng của TESS.

Chúng tôi đánh giá TESS trên một bộ các tác
vụ tạo sinh ngôn ngữ tự nhiên (NLG) bao gồm
tóm tắt, đơn giản hóa văn bản, tạo paraphrase
và tạo câu hỏi. Các kết quả thực nghiệm của
chúng tôi vượt trội hơn các phương pháp không
tự hồi quy và dựa trên khuếch tán tiên tiến
hiện tại và ngang hàng với mô hình ngôn ngữ
encoder-decoder đã tiền huấn luyện mạnh mẽ

--- TRANG 2 ---
(Lewis et al., 2020). Cụ thể, phương pháp tự
điều kiện dựa trên simplex của chúng tôi cải
thiện đáng kể chất lượng tạo sinh. Chúng tôi
cũng đánh giá TESS trên các tác vụ hiểu ngôn
ngữ tự nhiên (NLU) từ chuẩn GLUE (Wang et al.,
2019) và cho thấy nó thực hiện tương đương
với các chuẩn mô hình ngôn ngữ có mặt nạ
mạnh mẽ. Những đóng góp của chúng tôi có
thể được tóm tắt như sau.

1. Chúng tôi chứng minh tính hiệu quả của
một lược đồ hoàn toàn không tự hồi quy cho
các mô hình khuếch tán văn bản, vượt trội
hơn các chuẩn tự hồi quy và không tự hồi
quy mạnh mẽ.

2. Chúng tôi đề xuất một phương pháp tự
điều kiện mới khai thác ngữ nghĩa simplex
của không gian khuếch tán và cải thiện đáng
kể hiệu suất.

3. Chúng tôi đánh giá TESS trên một bộ các
tác vụ NLG và NLU đa dạng, nổi bật tính
hiệu quả của mô hình khuếch tán simplex
từ văn bản đến văn bản.

4. Chúng tôi cho thấy phương pháp hoàn toàn
không tự hồi quy của TESS dẫn đến việc
lấy mẫu nhanh hơn và hiệu quả hơn so với
các phương pháp bán và hoàn toàn tự hồi
quy cho các chuỗi dài.

Chúng tôi sẽ phát hành các mô hình đã huấn
luyện và mã nguồn để thúc đẩy nghiên cứu
mở trong lĩnh vực tạo sinh văn bản dựa trên
khuếch tán.

2 Kiến thức nền tảng
Chúng tôi xem xét lại các mô hình khuếch tán
liên tục (Sohl-Dickstein et al., 2015), theo
công thức của Mô hình khuếch tán khử nhiễu
(Ho et al., 2020; Song et al., 2020).

Huấn luyện Cho một mẫu x0∈Rd từ một
phân phối dữ liệu pdata, một quá trình khuếch
tán tiến q(xt|xt−1) là một chuỗi Markov tạo
ra một dãy các biến tiềm ẩn x1, . . . ,xT bằng
cách dần dần thêm nhiễu Gaussian tại mỗi
bước thời gian t∈ {1,2, . . . , T} với phương
sai βt∈R>0:

q(xt|xt−1) =N(xt;√1−βtxt−1, βtI).(1)

Đặt ϵt∼ N(0,I),αt= 1−βt, và ¯αt=∏t
s=1αs.
Khi đó việc lấy mẫu xt tại một bước thời
gian t tùy ý có nghiệm dạng đóng

xt=√¯αtx0+√1−¯αtϵt. (2)

Cho một lịch trình nhiễu có tính chất tốt {βt}T
t=1,
xT tuân theo phân phối tiên nghiệm tĩnh N(0,I).
Do đó, nếu chúng ta có thể xấp xỉ quá trình
ngược q(xt−1|xt,x0) thông qua một mô hình
pθ(xt−1|xt) với tham số θ, thì chúng ta có thể
lấy mẫu nhiễu ngẫu nhiên từ phân phối Gaussian
chuẩn và dần dần khử nhiễu để lấy mẫu từ
pdata. Trong cài đặt của chúng tôi, mô hình
pθ là một mô hình transformer2. Quá trình
ngược do đó được tham số hóa như

pθ(xt−1|xt) =N(µθ(xt, t),Σθ(xt, t)).(3)

Mô hình được huấn luyện bằng cách tối thiểu
hóa lỗi bình phương trung bình giữa dữ liệu
thực tế x0 và ước lượng của nó ˆxθ:3

L=Et,q(x0),q(xt|x0)∥x0−ˆxθ(xt, t)∥2.(4)

Lịch trình nhiễu Quá trình khuếch tán tiến
được định nghĩa bởi một lịch trình nhiễu.
Trong nghiên cứu này, chúng tôi tuân theo
lịch trình cosine (Nichol và Dhariwal, 2021)
cho αt:

¯αt=f(t)/f(0), f(t) = cos((t/T+s)/(1+s)·π/2)².(5)

Suy luận Trong Song et al. (2020), các dự
đoán của mô hình được khử nhiễu lặp đi lặp
lại cho t=T, . . . , 1 bắt đầu từ nhiễu thuần
túy, theo công thức

xt−1=√αt−1ˆxθ+√1−αt−1·(xt−√αtˆxθ)/√1−αt.

Chúng tôi tuân theo quy trình khuếch tán dựa
trên simplex được đề xuất gần đây bởi Han
et al. (2022), cho phép chúng tôi áp dụng
khuếch tán cho văn bản mà không cần sử
dụng các phương pháp phụ trợ ánh xạ dữ
liệu phân loại sang không gian liên tục
(Richemond et al., 2022).

3 Phương pháp
Trong phần này, chúng tôi trình bày TESS,
một mô hình từ văn bản đến văn bản dựa
trên khuếch tán simplex. Dựa trên SSD-LM
(Han et al., 2022), chúng tôi đề xuất một
mô hình hoàn toàn không tự hồi quy với
tự điều kiện.

Biểu diễn dữ liệu liên tục Đặt V là không
gian từ vựng. Theo Han et al. (2022), chúng
tôi ánh xạ ID của mỗi token cần tạo ra
w∈ V thành một k-logit simplex để tạo ra
sw∈ {± k}|V|, với thành phần thứ i thỏa
mãn

sw(i)={k, nếu i=w,
      −k, ngược lại,(6)

2Cụ thể, chúng tôi sử dụng mô hình RoBERTa (Liu et al., 2019),
nhưng công thức của chúng tôi có thể được áp dụng cho bất kỳ
biến thể transformer nào.
3Ngoài ra, chúng tôi có thể huấn luyện mô hình để dự đoán
nhiễu được thêm vào; xem Ho et al. (2020). Xem thêm Song
et al. (2021) cho một giải thích về score-matching.

--- TRANG 3 ---
TESS"câu cola: giếng đang ăn.""tóm tắt: một công nghệ mới, sáng tạo hứa hẹn sẽ cách mạng hóa ngành năng lượng tái tạo.""công nghệ chuyển đổi năng lượng sạch.""không chấp nhận được"……
<các token đầu vào nhiễu>simplex trung bình có trọng số transformer <logits>

Hình 1: Tổng quan về TESS. Trong quá trình huấn luyện (trên), đầu tiên chúng tôi thêm nhiễu vào simplex xác suất từ vựng, tính toán embedding từ trung bình có trọng số, và khử nhiễu bằng một encoder transformer. Để tạo sinh từ mô hình của chúng tôi, chúng tôi bắt đầu với nhiễu và lặp đi lặp lại cải thiện nó thành một phân phối logit cuối cùng (giữa). Mô hình kết quả có thể được sử dụng cho một loạt các tác vụ NLG và NLU cuối (dưới).

với một siêu tham số k∈R. Sau đó chúng tôi tạo ra một simplex xác suất trên V thông qua pw=softmax(sw). Cuối cùng, chúng tôi tính tổng có trọng số của word embedding để thu được một vector embedding liên tục, hw=Epw, trong đó E∈Rd×|V| là ma trận word embedding, d biểu thị kích thước của chiều ẩn, và hw∈Rd.

Embedding bước thời gian Sau khi tính toán continuous word embedding, chúng tôi thêm embedding bước thời gian để thông báo cho mô hình về bước thời gian hiện tại. Embedding bước thời gian của chúng tôi là một lớp tuyến tính, và chúng tôi đưa các bước thời gian đã điều chỉnh t/T vào lớp này. Đầu ra là một embedding bước thời gian trong Rd được thêm vào hw để tạo ra vector đầu vào tiềm ẩn cuối cùng.

Mô hình từ văn bản đến văn bản không tự hồi quy Không giống như SSD-LM, mô hình đưa các khối văn bản nhỏ vào để tạo sinh bán tự hồi quy các chuỗi văn bản, chúng tôi đưa toàn bộ vector tiềm ẩn cùng với ngữ cảnh vào một encoder transformer. Đây là một khác biệt chính giữa phương pháp của chúng tôi và SSD-LM, vì nó cho phép một mô hình hoàn toàn không tự hồi quy có khả năng tạo sinh chuỗi có độ dài bất kỳ. Trong thực tế, các tác vụ đánh giá của chúng tôi thường yêu cầu chuỗi đầu ra 100 token trở lên, và bằng cách chuyển sang một mô hình hoàn toàn không tự hồi quy, chúng tôi có thể tạo sinh toàn bộ chuỗi đầu ra song song mà không cần phải sử dụng tạo sinh bán tự hồi quy.

Khuếch tán tiến Đặt w= (w1, . . . , wL) là một câu có L token sao cho wi∈ V, và S0= (sw1, . . . ,swL)∈ {± k}L×|V| là biểu diễn k-logit simplex của w. Chúng tôi thêm nhiễu vào biểu diễn k-logit simplex trong quá trình huấn luyện theo

St=√¯αtS0+√1−¯αtϵt, (7)

trong đó chỉ số dưới biểu thị bước thời gian và ϵt∼N(0, k2I).

Huấn luyện Các mô hình khuếch tán điển hình được huấn luyện với hàm mất mát lỗi bình phương trung bình như trong Phương trình (4) để dự đoán dữ liệu thực tế. Mục tiêu này được biết là không ổn định cho các mô hình khuếch tán văn bản (Dieleman et al., 2022). Strudel et al. (2022) đã đóng băng word embedding và sử dụng điều chỉnh tỷ lệ cụ thể để xử lý bất ổn định huấn luyện. Trong nghiên cứu này, theo Han et al. (2022), thay vào đó chúng tôi tính toán hàm mất mát cross-entropy thông thường giữa các token thực tế w và dự đoán của mô hình cho một simplex logit nhiễu St tại bước thời gian t.

L=Et,q(S0),q(St|S0)[−∑Li=1logpθ(wi|St, t)]. (8)

Lấy mẫu Trong quá trình suy luận, chúng tôi lấy mẫu ST từ phân phối tiên nghiệm N(0, k2I) và chạy quá trình ngược cho t=T, . . . , 1 trên k-logit simplex nhiễu. Quá trình ngược có thể được xấp xỉ thông qua

St−1=√¯αt−1ˆSθ(St, t) +√1−¯αt−1ϵt.(9)

Xem Phụ lục C để biết chi tiết. Điều này giống với quá trình tiến trong Phương trình (7), cho phép một giải thích trực quan: để đảo ngược một bước từ t, chúng tôi lấy dự đoán mô hình ˆSθ làm sự thật giả định, sau đó làm hỏng nó bằng (t−1) bước thời gian.

Để xây dựng dự đoán mô hình, chúng tôi chiếu các logit được dự đoán bởi mô hình encoder cơ bản thông qua argmax như một pseudo-inverse của Phương trình (6) để khớp với biểu diễn k-logit ban đầu:

ˆsw(i)={k, nếu i=argmax (sw),
       −k, ngược lại.(10)

Tự điều kiện Trong các mô hình khuếch tán điển hình, mô hình dự đoán dữ liệu gốc x0 được điều kiện hóa trên phiên bản bị hỏng của nó, tức là ˆxt0=ˆxθ(xt, t), trong đó ˆxt0 biểu thị ước lượng của x0 tại bước thời gian t. Trong cài đặt này, các ước lượng của mô hình tại các bước thời gian trước đó bị loại bỏ. Tuy nhiên, trong tự điều kiện (Chen et al., 2022), mô hình điều kiện hóa dự đoán của nó trên cả xt và đầu ra được tạo ra trước đó, tức là ˆxt0=ˆxθ(xt,ˆxt+10, t). Để điều chỉnh mô hình cho tự điều kiện, chúng tôi ngẫu nhiên đặt tự điều kiện về không sao cho

ˆxt0={ˆxθ(xt,ˆxt+10, t), với xác suất ρ
     ˆxθ(xt,0, t), ngược lại,(11)

trong đó dự đoán tự điều kiện trước đó được tính như ˆxt+10=ˆxθ(xt+1,0, t+ 1), với gradient bị tách rời. Chúng tôi đặt ρ= 0.5 trong quá trình huấn luyện; trong suy luận, chúng tôi luôn sử dụng tự điều kiện (ρ= 1).

Chúng tôi đề xuất một phương pháp tự điều kiện mới khai thác bản chất simplex của không gian khuếch tán của chúng tôi. Đặt st∈R|V| là một k-logit simplex nhiễu cho một token tùy ý w.4 Thay vì nối dự đoán trước đó với st và chiếu lại, đầu tiên chúng tôi tính trung bình của xác suất simplex

pwavg=1/2[softmax(st) +softmax(ˆst+10)].(12)

Lưu ý rằng pwavg là một phân phối phân loại được định nghĩa tốt trên V. Sau đó chúng tôi tính toán một vector embedding liên tục, hw=Epwavg, và sử dụng vector này làm đầu vào cho mô hình cơ bản của chúng tôi để đưa ra dự đoán cho bước khuếch tán đã cho theo Phương trình 9. Điều này hiệu quả hơn phương pháp tự điều kiện ban đầu, chiếu xuống các vector được nối. Trong Phần §6.2, chúng tôi cũng chứng minh tính hiệu quả thực nghiệm của phương pháp này so với phương pháp ban đầu.

4Chúng tôi viết swt là st để ngắn gọn.

Độ dài chuỗi thay đổi Một thách thức đáng chú ý trong tạo sinh không tự hồi quy là giả định về độ dài chuỗi cố định trong quá trình suy luận. Để khắc phục vấn đề này, chúng tôi tuân theo nghiên cứu trước đó trong khuếch tán không gian embedding bằng cách sử dụng padding token (Li et al., 2022b). Cụ thể, trong quá trình huấn luyện, chúng tôi luôn pad chuỗi đầu ra có độ dài thay đổi đến một độ dài cố định bằng cách sử dụng padding token. Các padding token này được bao gồm khi tính toán hàm mất mát cross-entropy để TESS học cách tạo ra chúng. Trong quá trình suy luận, chúng tôi chỉ định độ dài chuỗi tối đa và chạy lấy mẫu như thường lệ.

4 Thí nghiệm
4.1 Tác vụ và Tập dữ liệu
Tạo paraphrase Tác vụ này liên quan đến việc diễn đạt lại một câu trong khi duy trì ngữ nghĩa của câu gốc. Chúng tôi sử dụng Quota Question Pairs (QQP),5 bao gồm 147K cặp tích cực. Chúng tôi chỉ sử dụng các cặp được gắn nhãn tích cực, có cùng nghĩa.

Đơn giản hóa văn bản Tác vụ này liên quan đến việc đơn giản hóa các câu phức tạp trong khi giữ nguyên nghĩa ban đầu. Chúng tôi sử dụng tập dữ liệu NEWSELA-AUTO (Jiang et al., 2020), bao gồm 666K câu phức tạp-đơn giản.

Tạo câu hỏi Tác vụ này liên quan đến việc tạo ra một câu hỏi cho một ngữ cảnh đầu vào. Chúng tôi sử dụng tập dữ liệu QUASAR-T (Dhingra et al., 2017) được xử lý bởi Yuan et al. (2022), tạo ra 119K cặp tài liệu-câu hỏi.

Tóm tắt Chúng tôi đánh giá phương pháp của mình trên tập dữ liệu CNN-DailyMail (Hermann et al., 2015), bao gồm 300K bài báo và tóm tắt.

Phân loại Chúng tôi xem xét một tập hợp các tác vụ phân loại trong chuẩn GLUE (Wang et al., 2019) bao gồm nhiều tác vụ khác nhau, bao gồm phát hiện paraphrase (MRPC, QQP), phân loại cảm xúc (SST-2), suy luận ngôn ngữ tự nhiên (MNLI,6 RTE, QNLI), và tính chấp nhận được về mặt ngôn ngữ (CoLA).7

4.2 Chuẩn so sánh
Chúng tôi so sánh TESS với một số chuẩn tự hồi quy cũng như các mô hình khuếch tán văn bản tiên tiến

5https://www.kaggle.com/c/quora-question-pairs
6Chúng tôi báo cáo độ chính xác trên tập validation khớp.
7Theo Devlin et al. (2019); Raffel et al. (2020), như một thực hành phổ biến và do bản chất đối kháng của WNLI, chúng tôi không thử nghiệm với WNLI.

--- TRANG 4 ---
Mô hình Tạo Paraphrase
BLEU BERT R-L D-1/4
Mô hình Tự hồi quy
BART (Lewis et al., 2020) 30.4 85.7 61.4 98.8 /61.0
GPT-2 base†(Radford et al., 2019) 19.8 82.5 52.1 98.0/62.5
GPT-2 large†(Radford et al., 2019) 20.6 83.6 54.2 98.2/50.2
GPV AE-T5†(Du et al., 2022) 24.1 84.7 58.9 96.9/61.7
Mô hình Không Tự hồi quy
LevT†(Gu et al., 2019) 22.7 83.4 57.9 97.9/33.3
Mô hình Khuếch tán Không Tự hồi quy
DiffuSeq⋆(Gong et al., 2023) 18.5 79.5 — 97.6/—
SeqDiffuSeq⋆(Yuan et al., 2022) 23.3 82.9 — 98.1/—
SSD-LM (Han et al., 2022) 22.9 83.8 58.3 98.8/57.3
TESS (Chúng tôi) 30.2 85.7 62.2 98.5/ 61.1

Bảng 1: Kết quả trên tác vụ Tạo Paraphrase.†chỉ ra kết quả từ Gong et al. (2023), * chỉ ra kết quả từ Yuan et al. (2022). Kết quả in đậm cho thấy tốt nhất trong tất cả các mô hình không-AR; kết quả gạch dưới là tốt nhất trong tất cả các mô hình.

els. Đối với các phương pháp tự hồi quy, chúng tôi xem xét GPT-2 (Radford et al., 2019), BART (Lewis et al., 2020), và GPV AE-T5 (Du et al., 2022), một mô hình biến cấu trúc tiềm ẩn và một phần mở rộng của T5 (Raffel et al., 2020). Đối với các mô hình khuếch tán văn bản, chúng tôi xem xét Diffuser (Reid et al., 2022), DiffuSeq (Gong et al., 2023), SeqDiffuSeq (Yuan et al., 2022), SUNDAE (Savinov et al., 2021), LevT (Gu et al., 2019), một mô hình không tự hồi quy lặp được sử dụng rộng rãi, và SSD-LM (Han et al., 2022) được khởi tạo từ cùng một mô hình RoBERTa đã tiền huấn luyện như TESS và được huấn luyện sử dụng codebase SSD-LM chính thức.8 Chúng tôi báo cáo kết quả mà không sử dụng các phương pháp giải mã bổ sung như giải mã rủi ro Bayes tối thiểu. Chúng tôi cung cấp thêm chi tiết về kết quả chuẩn trong Phụ lục A.

4.3 Đánh giá
Đối với tóm tắt, chúng tôi báo cáo các biến thể ROUGE-1 (R1), ROUGE-2 (R2), và ROUGE-L (R-L) (Lin, 2004) như đã làm trong nghiên cứu tóm tắt văn bản trước đó (Lewis et al., 2020). Chúng tôi định lượng cả chất lượng tạo sinh và tính đa dạng. Để đánh giá chất lượng tạo sinh, chúng tôi báo cáo BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004) và BERTScore (Zhang et al., 2020) theo Gong et al. (2023) và Yuan et al. (2022). Để đánh giá tính đa dạng, chúng tôi báo cáo unigram xa (D-1) và 4-gram đa dạng (D-4) (Deshpande et al., 2018). Đối với đơn giản hóa văn bản, chúng tôi sử dụng SARI chuẩn (Xu et al., 2016), và theo

8https://github.com/xhan77/ssd-lm

Mô hình Đơn giản hóa Văn bản
SARI BLEU BERT R-L
Mô hình Tự hồi quy
BART (Lewis et al., 2020) 49.9 41.4 81.7 58.1
GPT-2 base†(Radford et al., 2019) — 30.8 80.2 54.6
GPT-2 large†(Radford et al., 2019) — 26.9 78.8 51.1
GPV AE-T5†(Du et al., 2022) — 33.9 81.7 58.3
Mô hình Không Tự hồi quy
LevT†(Gu et al., 2019) — 20.5 72.5 44.0
Mô hình Khuếch tán Không Tự hồi quy
DiffuSeq⋆(Gong et al., 2023) — 29.9 79.1 —
SeqDiffuSeq⋆(Yuan et al., 2022) — 37.1 82.1 —
SSD-LM (Han et al., 2022) 36.3 12.5 69.5 39.6
TESS (Chúng tôi) 54.3 41.5 82.1 59.4

Bảng 2: Kết quả trên tác vụ đơn giản hóa văn bản.†chỉ ra kết quả từ Gong et al. (2023), * chỉ ra kết quả từ Yuan et al. (2022).

Gong et al. (2023); Yuan et al. (2022), chúng tôi cũng bao gồm BLEU, BERTScore, và ROUGE-L.

4.4 Triển khai
Chúng tôi bắt đầu từ checkpoint RoBERTa đã tiền huấn luyện (Liu et al., 2019) và tinh chỉnh mô hình trên các tác vụ hạ nguồn sử dụng phương pháp khuếch tán simplex tự điều kiện được đề xuất. Số bước lấy mẫu khuếch tán tại thời điểm suy luận được đặt là T= 1000 cho tạo sinh và T= 10 cho các tác vụ phân loại. Trong quá trình huấn luyện, chúng tôi sử dụng T= 5000. Chúng tôi đặt thang đo simplex là k= 5. Các chi tiết bổ sung được liệt kê trong Phụ lục A.

5 Kết quả
5.1 Tạo Paraphrase
Như thấy trong Bảng 1, TESS vượt trội đáng kể so với GPT-2 và các chuẩn không tự hồi quy và khuếch tán khác trong các chỉ số chất lượng (BLEU, BERT, và ROUGE) trong khi đạt được sự ngang bằng trong các chỉ số đa dạng (D-1/D-4). Hơn nữa, TESS đạt được hiệu suất tổng thể cạnh tranh với BART. Lưu ý rằng BART sử dụng một mục tiêu tiền huấn luyện khử nhiễu, đặc biệt có lợi cho các tác vụ chuỗi đến chuỗi (Lewis et al., 2020); chúng tôi không thực hiện bất kỳ tiền huấn luyện bổ sung nào ngoài checkpoint của RoBERTa, mô hình này chỉ được tiền huấn luyện trên mục tiêu mô hình ngôn ngữ có mặt nạ chung. Chúng tôi nghi ngờ rằng TESS có thể hưởng lợi đáng kể từ tiền huấn luyện khuếch tán bổ sung (xem Phần §8).

--- TRANG 5 ---
Mô hình Tạo Câu hỏi
BLEU BERT R-L D-1/4
Mô hình Tự hồi quy
BART (Lewis et al., 2020) 17.4 66.2 38.8 98.2 /61.7
GPT-2 base†(Radford et al., 2019) 7.4 60.5 27.2 96.0/92.2
GPT-2 large†(Radford et al., 2019) 11.1 63.5 32.2 96.7/80.6
GPV AE-T5†(Du et al., 2022) 12.5 63.1 33.9 93.8/72.8
Mô hình Không Tự hồi quy
LevT†(Gu et al., 2019) 9.3 54.9 28.9 89.1/47.8
Mô hình Khuếch tán Không Tự hồi quy
DiffuSeq⋆(Gong et al., 2023) 15.8 59.4 — 91.1/—
SeqDiffuSeq⋆(Yuan et al., 2022) 17.2 61.4 — 92.7/—
SSD-LM (Han et al., 2022) 14.1 62.8 38.5 94.5/56.9
TESS (khởi tạo ngẫu nhiên) 19.0 60.8 36.1 96.1/62.4
TESS (Chúng tôi) 19.5 65.8 38.9 97.1/63.0

Bảng 3: Kết quả trên tác vụ Tạo Câu hỏi.

5.2 Đơn giản hóa Văn bản
Kết quả của tác vụ đơn giản hóa văn bản trên tập dữ liệu NEWSELA-AUTO được trình bày trong Bảng 2. TESS vượt trội hơn tất cả các chuẩn thường với khoảng cách lớn, bao gồm cả các mô hình tự hồi quy và không tự hồi quy.

5.3 Tạo Câu hỏi
Như thể hiện trong Bảng 3, TESS vượt trội hơn các mô hình khuếch tán và không tự hồi quy khác về cả chất lượng tạo sinh (BLEU, BERTScore, ROUGE) và tính đa dạng (D-1/D-4). Nó cũng vượt trội nhất quán so với các chuẩn tự hồi quy khác ngoại trừ BART, có hiệu suất được khớp chặt chẽ bởi TESS. Chúng tôi cũng huấn luyện và đánh giá TESS mà không khởi tạo từ RoBERTa đã tiền huấn luyện (khởi tạo ngẫu nhiên), và thấy rằng điều này vượt trội hơn tất cả các chuẩn NAR trong BLEU và D-1, trong khi vẫn gần về hiệu suất trong BERTScore và ROUGE-L. Điều này cho thấy rằng khung TESS vượt trội hơn các chuẩn ngay cả khi không có lợi ích của việc sử dụng các mô hình đã tiền huấn luyện hiện có.

5.4 Tóm tắt
Như thể hiện trong Bảng 4, TESS đạt được kết quả cạnh tranh với BART trong khi vượt trội hơn nghiên cứu khuếch tán trước đó, Diffuser (Reid et al., 2022), 1.9 điểm ROUGE-L, và các biến thể bootstrap của nó 0.8 điểm ROUGE-L. Lưu ý rằng bootstrap bổ sung là trực giao với phương pháp của họ và có thể được áp dụng cho TESS cũng như vậy. Ngoài ra, TESS vượt trội hơn GENIE, một phương pháp dựa trên khuếch tán trước đó khác, trong khi sử dụng một nửa số bước khuếch tán. Điều này cho thấy công thức dựa trên simplex của TESS dẫn đến hiệu suất tốt hơn so với các phương pháp khuếch tán thay thế

Mô hình CNN-DM
R1 R2 R-L
Mô hình Tự hồi quy
BART (Lewis et al., 2020) 42.9 20.1 40.1
Transformer (Vaswani et al., 2017)⋄— — 36.8
Mô hình Khuếch tán Không Tự hồi quy
SUNDAE (Savinov et al., 2021)⋄— — 37.0
Diffuser (Reid et al., 2022)⋄— — 37.8
Diffuser+ AR bootstrap⋄— — 38.4
Diffuser + source bootstrap⋄— — 38.9
GENIE (Lin et al., 2023) 41.8 18.3 35.5
TESS (Chúng tôi) 42.3 19.4 39.7

Bảng 4: Kết quả trên tập dữ liệu CNN-DailyMail. Các giá trị chuẩn được đánh dấu bằng ⋄ được lấy từ Reid et al. (2022).

5.5 Phân loại Văn bản
Theo hiểu biết của chúng tôi, TESS là mô hình đầu tiên được đánh giá trên cả NLG và NLU. Chúng tôi đánh giá TESS trên các tác vụ phân loại và so sánh trực tiếp phương pháp tinh chỉnh dựa trên khuếch tán của chúng tôi với các phương pháp tinh chỉnh chuẩn cho học có giám sát. Để thực hiện thí nghiệm có kiểm soát, chúng tôi so sánh TESS với RoBERTa có kích thước tương tự, mô hình mà chúng tôi sử dụng để khởi tạo mô hình của mình. Lưu ý rằng vì TESS là từ văn bản đến văn bản, tương tự như T5 (Raffel et al., 2020), nó có thể xử lý các tác vụ phân loại một cách tự nhiên bằng cách tạo ra nhãn lớp mà không cần verbalizer. Đối với STS-B, là một vấn đề hồi quy, chúng tôi chuyển đổi nó thành một vấn đề phân loại 21 lớp theo Raffel et al. (2020). Kết quả được thể hiện trong Bảng 5. Chúng tôi quan sát rằng TESS khớp hoặc vượt trội hơn RoBERTa đã tinh chỉnh trên một số tác vụ, đạt được khoảng 2 điểm tăng trên MRPC và RTE.

6 Phân tích
6.1 Đầu ra Độ dài Thay đổi
Hình 2 cho thấy TESS có khả năng tạo ra đầu ra với độ dài thay đổi khớp với phân phối cơ bản của độ dài chuỗi trong dữ liệu vàng cũng như đầu ra BART. Chúng tôi cũng đánh giá chất lượng tạo sinh cho các độ dài đầu ra khác nhau trong Hình 3. Chúng tôi quan sát rằng TESS nhất quán với chuẩn BART cho các độ dài đích thay đổi, với các tạo sinh dài hơn khớp với hiệu suất của BART.

6.2 Tự điều kiện
Để kiểm tra tác động của tự điều kiện, chúng tôi so sánh phương pháp được đề xuất với chiến lược ban đầu (Chen et al., 2022) trong đơn giản hóa văn bản

--- TRANG 6 ---
Phương pháp MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI Trung bình
RoBERTa large 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 91.3 88.9
TESS large(Chúng tôi) 90.1/89.8 94.2 89.1 88.5 96.4 93.1 67.7 88.9 83.1 88.5

Bảng 5: So sánh TESS và RoBERTa trên các tác vụ GLUE trên tập phát triển. Theo Devlin et al. (2019), đối với MRPC và QQP, chúng tôi báo cáo điểm F1; STS-B, hệ số tương quan Spearman; CoLA, tương quan Matthews. Đối với tất cả các tác vụ khác, chúng tôi báo cáo độ chính xác. Phông chữ đậm chỉ ra kết quả tốt nhất.

Mô hình Đơn giản hóa Văn bản
SARI BLEU BERT R-L
TESS 44.1 30.8 78.8 52.6
+tự điều kiện gốc 52.2 43.3 81.9 58.9
+tự điều kiện được đề xuất 54.2 40.8 82.0 59.3
Tạo paraphrase
BLEU BERT R-L D-1/D-4
TESS 25.9 84.4 59.7 98.7/60.4
+tự điều kiện gốc 28.4 85.5 61.7 98.6/60.6
+tự điều kiện được đề xuất 29.2 85.5 61.2 98.5/ 61.4

Bảng 6: Ablation về tác động của tự điều kiện. Chúng tôi so sánh tự điều kiện được đề xuất với phương pháp ban đầu trong Chen et al. (2022), và mô hình không có tự điều kiện. Phông chữ đậm chỉ ra kết quả tốt nhất.

và tạo paraphrase. Như thể hiện trong Bảng 6, việc thêm tự điều kiện cải thiện kết quả một cách nhất quán, với biến thể của chúng tôi mang lại hiệu suất tổng thể tốt nhất.

6.3 Bước Lấy mẫu
Chúng tôi cũng điều tra chất lượng của tạo sinh TESS trên bộ tác vụ NLG cũng như MRPC bằng cách thay đổi số bước lấy mẫu trong quá trình suy luận. Như thể hiện trong Bảng 7, TESS thực hiện tốt với tương đối ít bước, chỉ với sự giảm hiệu suất biên khi bước lấy mẫu được giảm từ 1000 xuống 100. Đối với các tác vụ phân loại liên quan đến tạo sinh ngắn hơn (MRPC), 10 bước lấy mẫu dẫn đến chất lượng không mất mát. Chúng tôi cũng thấy rằng việc giảm số bước lấy mẫu là có thể trong các tác vụ tạo sinh như tạo câu hỏi. Chúng tôi cung cấp kết quả trong Phụ lục B. Đáng chú ý, có vẻ như số bước cần thiết tương quan với độ khó của tác vụ: trong khi các tác vụ phân loại như MRPC chỉ yêu cầu ít bước, các tác vụ tạo sinh dài hơn như CNN-DM yêu cầu gần 100 bước để đạt hiệu suất tốt.

6.4 Tốc độ Lấy mẫu
Chúng tôi so sánh tốc độ tạo sinh TESS với các mô hình khác trong Hình 4. Chúng tôi đo thời gian giải mã từ 25 đến 5000 token mất bao lâu cho một ngữ cảnh 50 token

0 20 40 60 80 100 120
Độ dài
0
500
0 20 40 60 80 100 120
0
500
0 20 40 60 80 100 120
0
500
Tần suất

Hình 2: TESS có khả năng tạo ra đầu ra với độ dài thay đổi, khớp với độ dài chuỗi đích và đầu ra BART cho tạo sinh CNN-DM. Từ trên xuống dưới: (a) phân phối độ dài đích; (b) phân phối độ dài dự đoán bởi BART; (c) phân phối độ dài dự đoán bởi TESS.

và 100 bước khuếch tán. Chúng tôi thấy rằng TESS nhanh hơn đáng kể so với SSD-LM, đặc biệt khi SSD-LM phải tạo ra nhiều khối do kích thước khối hạn chế. Đáng chú ý, chúng tôi thấy TESS nhanh hơn, mặc dù nhỏ, so với BART có kích thước tương đương khi tạo sinh hơn 2000 token. Chúng tôi cung cấp thêm chi tiết trong Phụ lục A.5.

6.5 TESS so với các Phương pháp Khuếch tán khác
Như thể hiện trong Phần 5, TESS vượt trội hơn các phương pháp khuếch tán khác trên một số chuẩn. Chúng tôi tin rằng điều này là do một số yếu tố: (1) công thức dựa trên simplex phù hợp tự nhiên hơn với ngôn ngữ so với những công thức dựa trên embedding, cho phép chúng tôi tránh các phương pháp như clamping hoặc một bộ giải mã bổ sung để thoát khỏi không gian embedding; (2) công thức tự điều kiện dựa trên simplex, mà chúng tôi chứng minh thực nghiệm vượt trội hơn các phương pháp tự điều kiện chuẩn hơn (Bảng 6); (3) việc sử dụng mô hình đã tiền huấn luyện - trong khi TESS vẫn có thể vượt trội hơn các phương pháp khác mà không sử dụng mô hình đã tiền huấn luyện (Bảng 3), việc có thể sử dụng các mô hình đã tiền huấn luyện với tương đối ít huấn luyện bổ sung.

--- TRANG 7 ---
Bước QQP NEWSELA-AUTO QG CNN-DM MRPC
R-L Độ chính xác F1
10 62.4 58.4 38.8 35.6 89.7 92.8
100 62.0 59.1 38.9 39.6 89.7 92.8
1000 62.2 59.4 38.9 39.7 89.7 92.8

Bảng 7: Tác động của số bước lấy mẫu lên hiệu suất. Phương pháp của chúng tôi đạt được kết quả cạnh tranh với chỉ 10 hoặc 100 bước trên các tác vụ NLG và 10 cho MRPC.

25 50 75 100 125
Độ dài Đích
0
10
20
30
ROUGE
TESS
BART

Hình 3: Điểm ROUGE trung bình (R1, R2, R-L) cho TESS so với BART cho tạo sinh CNN-DM. Phương pháp của chúng tôi thực hiện tương đương với BART cho các độ dài đích khác nhau.

7 Nghiên cứu Liên quan
Khuếch tán cho các miền liên tục Các mô hình khuếch tán lần đầu tiên được đề xuất bởi Sohl-Dickstein et al. (2015) và được phổ biến bởi Mô hình Xác suất Khuếch tán Khử nhiễu (DDPM) (Ho et al., 2020), đã đề xuất một tham số hóa mới tiết lộ sự tương đương giữa dự đoán sự thật cơ bản và ước lượng nhiễu. Song et al. (2021) đề xuất một giải thích phương trình vi phân ngẫu nhiên thay thế của khuếch tán liên quan đến hàm điểm Stein. Nichol và Dhariwal (2021) đề xuất một số sửa đổi cho DDPM, cải thiện log-likelihood và giảm bước lấy mẫu. Ho và Salimans (2021) đề xuất hướng dẫn không có phân loại, cho phép tạo sinh có thể kiểm soát cao mà không cần một bộ phân loại bên ngoài để hướng dẫn ước lượng điểm của mô hình.

Khuếch tán liên tục cho các miền rời rạc Sau thành công của các mô hình khuếch tán trên các miền liên tục, đã có một số nỗ lực áp dụng khuếch tán trên dữ liệu rời rạc. Li et al. (2022a) áp dụng khuếch tán trên không gian embedding token tiềm ẩn. Mô hình ngôn ngữ kết quả của họ dựa trên tokenization mức từ và hoạt động chủ yếu trên các tập dữ liệu nhỏ với độ dài chuỗi ngắn 64 token. Strudel et al. (2022) sử dụng word embedding đã tiền huấn luyện bị đóng băng với việc điều chỉnh tỷ lệ cẩn thận để giải quyết bất ổn định do cạnh tranh giữa khuếch tán và hàm mất mát tái tạo. Tuy nhiên, phương pháp của họ không cho phép huấn luyện chung word embedding, và mô hình không được đánh giá trên các tác vụ NLP hạ nguồn. Gần đây hơn, Dieleman et al. (2022) đã cố gắng học embedding và mô hình khuếch tán cùng nhau, vẫn bằng cách thực hiện khuếch tán trong không gian embedding. Các nghiên cứu gần đây khác cũng đã áp dụng khuếch tán trên word embedding để giải quyết các vấn đề chuỗi đến chuỗi (Gong et al., 2023; Yuan et al., 2022). Đồng thời với nghiên cứu của chúng tôi, Ye et al. (2023) đề xuất các phương pháp để thao tác nhiễu trong quá trình khuếch tán trong quá trình huấn luyện và suy luận, mang lại cải thiện trong tạo sinh văn bản có điều kiện. Một nghiên cứu đồng thời khác khám phá các mô hình khuếch tán biến phân cho mô hình ngôn ngữ trong không gian embedding (Gulrajani và Hashimoto, 2023). Tuy nhiên, họ so sánh các mô hình của họ với các mô hình tự hồi quy nhỏ hơn 8×; phương pháp của chúng tôi đạt được hiệu suất cạnh tranh với các mô hình tự hồi quy cùng kích thước.

Phù hợp nhất với nghiên cứu của chúng tôi, Han et al. (2022) đề xuất một mô hình khuếch tán bán tự hồi quy tạo ra các khối nhỏ 25 token từ

0 1000 2000 3000 4000 5000
Số Token
100
101
102
103
Thời gian (s)
SSD-LM (B=25)
SSD-LM (B=200)
TESSlarge
BARTlarge

Hình 4: Thời gian để tạo ra số token thay đổi với 100 bước khuếch tán. Chúng tôi báo cáo thời gian trung bình trên năm lần chạy. Các mô hình dựa trên khuếch tán sử dụng RoBERTa large làm backbone. TESS nhanh hơn đáng kể so với SSD-LM. Đáng chú ý, đối với 2000 token, nó thậm chí nhanh hơn một chút so với BART có kích thước tương đương.

--- TRANG 8 ---
trái sang phải, đưa chúng làm ngữ cảnh bổ sung để tạo ra các khối tiếp theo. Chúng tôi mở rộng phương pháp của họ thành tạo sinh hoàn toàn không tự hồi quy làm tăng đáng kể thời gian suy luận và kết hợp một phương pháp tự điều kiện hiệu quả khai thác ngữ nghĩa của không gian simplex. Han et al. (2023b) tương tự mở rộng phương pháp này, nhưng tập trung vào việc thể hiện tính khả thi của khuếch tán dựa trên simplex với các mô hình lớn (> 1B tham số).

Khuếch tán rời rạc cho các miền rời rạc Không giống như các mô hình khuếch tán liên tục, các mô hình khuếch tán rời rạc duy trì cấu trúc rời rạc của miền dữ liệu và thực hiện chuyển đổi trạng thái dựa trên ma trận xác suất. Các mô hình khuếch tán với không gian trạng thái rời rạc lần đầu tiên được khám phá bởi Sohl-Dickstein et al. (2015), người đã đề xuất một khung cho khuếch tán trên các biến ngẫu nhiên nhị phân. Sau đó, Hoogeboom et al. (2021) và Austin et al. (2021) đề xuất các mô hình khuếch tán rời rạc cho các biến ngẫu nhiên phân loại. Tuy nhiên, các phương pháp này thường thua kém các mô hình tự hồi quy. Gần đây hơn, Reid et al. (2022) đề xuất Diffuser, công thức hóa một quá trình khuếch tán rời rạc bằng cách mô hình hóa tạo sinh như một chuỗi các hoạt động chỉnh sửa rời rạc. TESS vượt trội đáng kể so với Diffuser.

8 Kết luận
Chúng tôi trình bày TESS, một mô hình khuếch tán chuỗi đến chuỗi mới cho các tác vụ tạo sinh ngôn ngữ hoàn toàn không tự hồi quy, hoạt động cho các chuỗi dài so với nghiên cứu trước đó, thực hiện quá trình khuếch tán trên không gian logit từ vựng, và sử dụng một hình thức tự điều kiện mới và hiệu quả. TESS vượt trội hơn các chuẩn tự hồi quy mạnh mẽ cũng như các mô hình khuếch tán văn bản tiên tiến gần đây trên nhiều tác vụ tạo sinh và hiểu ngôn ngữ có điều kiện khác nhau, đồng thời cũng hiệu quả hơn nhiều so với các mô hình dựa trên khuếch tán trước đó. Nghiên cứu tương lai dựa trên việc tiền huấn luyện phương pháp của chúng tôi kết hợp với các mục tiêu khử nhiễu và điền khuyết, mà chúng tôi giả thuyết có thể cung cấp thêm cải thiện hiệu suất cho mô hình khuếch tán từ văn bản đến văn bản của chúng tôi.

Hạn chế
Tốc độ lấy mẫu Như thấy trong Hình 4, TESS vẫn chậm hơn BART khi tạo sinh <1000 token. Chúng tôi đã thử nghiệm với việc giảm số bước khuếch tán (xem Bảng 7), có thể tăng tốc thêm việc tạo sinh. Trong khi trong hầu hết các tác vụ chỉ sử dụng 10 bước cung cấp kết quả hứa hẹn, nó không đủ để đạt hiệu suất mạnh mẽ trên các tác vụ phức tạp hơn như tóm tắt. Việc kết hợp nghiên cứu gần đây trong thị giác máy tính để tăng tốc lấy mẫu trong các mô hình dựa trên khuếch tán (Song et al., 2023) có thể dẫn đến tăng tốc thêm trong tạo sinh.

Chuỗi dài Các thử nghiệm tốc độ suy luận với SSD-LM và BART cho thấy TESS nhanh chóng vượt trội hơn tạo sinh bán tự hồi quy và vượt trội hơn BART ở 2000 token. Kết quả này cho thấy rằng các mô hình khuếch tán có tiềm năng nhanh hơn các mô hình tự hồi quy phổ biến ở độ dài chuỗi dài. Trong nghiên cứu này, chúng tôi chủ yếu sử dụng các mô hình RoBERTa base để tạo điều kiện so sánh công bằng với các chuẩn hiện có, điều này không tránh khỏi hạn chế kích thước cửa sổ ngữ cảnh do chiến lược absolute position embedding được sử dụng bởi RoBERTa. Chúng tôi nghi ngờ rằng việc mở khóa toàn bộ tiềm năng của các mô hình ngôn ngữ dựa trên khuếch tán có thể nằm trong chế độ chuỗi dài, có thể liên quan đến việc mở rộng các mô hình hiện tại.

Tuyên bố Đạo đức
Các mô hình ngôn ngữ được biết là tạo ra nội dung độc hại và thiên vị (Weidinger et al., 2022; Sheng et al., 2021). Mặc dù chúng tôi khám phá một khung mô hình thay thế cho khung thường được sử dụng trong các nghiên cứu trước đó về độc tính của các mô hình ngôn ngữ, có ít lý do để cho rằng các mô hình của chúng tôi cũng sẽ không chứa các vấn đề này. Tuy nhiên, với khả năng kiểm soát lớn hơn của khung khuếch tán (Li et al., 2022b), chúng tôi hy vọng nghiên cứu tương lai khám phá cách sử dụng khả năng kiểm soát này để giảm thiểu tác hại tiềm ẩn. Việc kiểm tra thêm mức độ tốt của kết quả xung quanh các tạo sinh độc hại và có hại của các thiết lập tự hồi quy chuyển sang cài đặt của chúng tôi cũng có thể hỗ trợ trong việc xác định các khu vực cần cải thiện trong tương lai.

Lời cảm ơn
Chúng tôi biết ơn Aman Madaan, Robin Strudel, Sander Dieleman, Chris Dyer, Xiaochuang Han, Sachin Kumar, Clara Meister, Sean Welleck, và Andre Wibisono vì những bình luận và thảo luận hữu ích, và Sam Skjonsberg và nhóm ReViz tại AI2 vì sự hỗ trợ trong việc quản lý thí nghiệm.

Tài liệu tham khảo
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, và Rianne van den Berg. 2021. Các mô hình khuếch tán khử nhiễu có cấu trúc trong không gian trạng thái rời rạc. Trong NeurIPS.

--- TRANG 9 ---
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, và Danilo Giampiccolo. 2006. Thử thách nhận dạng kéo theo văn bản pascal thứ hai. Hội thảo thử thách PASCAL thứ hai về nhận dạng kéo theo văn bản.

Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, và Bernardo Magnini. 2009. Thử thách nhận dạng kéo theo văn bản pascal thứ năm. Trong TAC.

Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, và Lucia Specia. 2017. SemEval-2017 tác vụ 1: Đánh giá tương tự ngữ nghĩa văn bản đa ngôn ngữ và xuyên ngôn ngữ tập trung. Trong Kỷ yếu Hội thảo Quốc tế lần thứ 11 về Đánh giá Ngữ nghĩa (SemEval-2017), trang 1–14, Vancouver, Canada. Hiệp hội Ngôn ngữ học Tính toán.

Ting Chen, Ruixiang Zhang, và Geoffrey Hinton. 2022. Bit tương tự: Tạo sinh dữ liệu rời rạc sử dụng các mô hình khuếch tán với tự điều kiện. arXiv preprint arXiv:2208.04202.

Ido Dagan, Oren Glickman, và Bernardo Magnini. 2005. Thử thách nhận dạng kéo theo văn bản pascal. Trong Hội thảo Thử thách Học máy.

Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G. Schwing, và David A. Forsyth. 2018. Tạo chú thích hình ảnh nhanh, đa dạng và chính xác được hướng dẫn bởi part-of-speech. Hội nghị IEEE/CVF 2019 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 10687–10696.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Tiền huấn luyện các transformer song hướng sâu để hiểu ngôn ngữ. Trong Kỷ yếu Hội nghị 2019 của Chương Bắc Mỹ của Hiệp hội Ngôn ngữ học Tính toán: Công nghệ Ngôn ngữ Con người, Tập 1 (Bài báo Dài và Ngắn), trang 4171–4186, Minneapolis, Minnesota. Hiệp hội Ngôn ngữ học Tính toán.

Bhuwan Dhingra, Kathryn Mazaitis, và William W Cohen. 2017. Quasar: Tập dữ liệu cho việc trả lời câu hỏi bằng tìm kiếm và đọc. arXiv preprint arXiv:1707.03904.

Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. 2022. Khuếch tán liên tục cho dữ liệu phân loại. arXiv preprint arXiv:2211.15089.

William B. Dolan và Chris Brockett. 2005. Tự động xây dựng một corpus của các paraphrase câu. Trong Kỷ yếu Hội thảo Quốc tế lần thứ ba về Paraphrasing (IWP2005).

Wanyu Du, Jianqiao Zhao, Liwei Wang, và Yangfeng Ji. 2022. Tạo sinh văn bản đa dạng thông qua các mô hình encoder-decoder biến phân với tiền nghiệm quá trình Gaussian. arXiv preprint arXiv:2204.01227.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, và Bill Dolan. 2007. Thử thách nhận dạng kéo theo văn bản PASCAL thứ ba. Trong Kỷ yếu Hội thảo ACL-PASCAL về Kéo theo Văn bản và Paraphrasing, trang 1–9, Prague. Hiệp hội Ngôn ngữ học Tính toán.

Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, và Lingpeng Kong. 2023. DiffuSeq: Tạo sinh văn bản từ chuỗi đến chuỗi với các mô hình khuếch tán. Trong Hội nghị Quốc tế về Biểu diễn Học, ICLR.

Jiatao Gu, Changhan Wang, và Junbo Zhao. 2019. Levenshtein transformer. NeurIPS.

Ishaan Gulrajani và Tatsunori B Hashimoto. 2023. Các mô hình ngôn ngữ khuếch tán dựa trên likelihood. arXiv preprint arXiv:2305.18619.

Xiaochuang Han, Sachin Kumar, và Yulia Tsvetkov. 2022. Ssd-lm: Mô hình ngôn ngữ khuếch tán simplex bán tự hồi quy để tạo sinh văn bản và kiểm soát modular. arXiv preprint arXiv:2210.17432.

Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, và Marjan Ghazvininejad. 2023a. Ssd-2: Mở rộng và fusion thời gian suy luận của các mô hình ngôn ngữ khuếch tán. ArXiv, abs/2305.14771.

Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, và Marjan Ghazvininejad. 2023b. Ssd-2: Mở rộng và fusion thời gian suy luận của các mô hình ngôn ngữ khuếch tán.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, và Phil Blunsom. 2015. Dạy máy đọc và hiểu. Trong NeurIPS.

Jonathan Ho, Ajay Jain, và Pieter Abbeel. 2020. Các mô hình xác suất khuếch tán khử nhiễu. NeurIPS.

Jonathan Ho và Tim Salimans. 2021. Hướng dẫn khuếch tán không có phân loại. Trong Hội thảo NeurIPS DGMs Applications.

Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, và David J Fleet. 2022. Các mô hình khuếch tán video. Trong Hội thảo ICLR về Mô hình Tạo sinh Sâu cho Dữ liệu Có cấu trúc Cao.

Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, và Max Welling. 2021. Luồng Argmax và khuếch tán đa thức: Học các phân phối phân loại. NeurIPS.

Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, và Wei Xu. 2020. Mô hình CRF neural cho việc căn chỉnh câu trong đơn giản hóa văn bản. Trong ACL.

Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, và Bryan Catanzaro. 2020. Diffwave: Một mô hình khuếch tán đa dụng cho tổng hợp âm thanh. Trong ICLR.

--- TRANG 10 ---
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. Bart: Tiền huấn luyện chuỗi đến chuỗi khử nhiễu cho tạo sinh ngôn ngữ tự nhiên, dịch và hiểu. Trong ACL.

Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, và Thomas Wolf. 2021. Datasets: Một thư viện cộng đồng cho xử lý ngôn ngữ tự nhiên. Trong Kỷ yếu Hội nghị 2021 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên: Trình diễn Hệ thống, trang 175–184, Online và Punta Cana, Dominican Republic. Hiệp hội Ngôn ngữ học Tính toán.

Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, và Tatsunori Hashimoto. 2022a. Diffusion-LM cải thiện tạo sinh văn bản có thể kiểm soát. Trong NeurIPS.

Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, và Tatsunori B Hashimoto. 2022b. Diffusion-lm cải thiện tạo sinh văn bản có thể kiểm soát. Trong NeurIPS.

Chin-Yew Lin. 2004. ROUGE: Một gói để đánh giá tự động các tóm tắt. Trong Text Summarization Branches Out, trang 74–81, Barcelona, Spain. Hiệp hội Ngôn ngữ học Tính toán.

Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, và Weizhu Chen. 2023. Tạo sinh văn bản với các mô hình ngôn ngữ khuếch tán: một phương pháp tiền huấn luyện với khử nhiễu đoạn văn liên tục. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 40 về Học máy, ICML'23. JMLR.org.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: Một phương pháp tiền huấn luyện BERT được tối ưu hóa mạnh mẽ. CoRR, abs/1907.11692.

Alexander Quinn Nichol và Prafulla Dhariwal. 2021. Các mô hình xác suất khuếch tán khử nhiễu được cải thiện. Trong ICML.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: một phương pháp đánh giá tự động dịch máy. Trong Kỷ yếu Cuộc họp Thường niên lần thứ 40 của Hiệp hội Ngôn ngữ học Tính toán, trang 311–318, Philadelphia, Pennsylvania, USA. Hiệp hội Ngôn ngữ học Tính toán.

Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, và Zaid Harchaoui. 2021. Mauve: Đo lường khoảng cách giữa văn bản neural và văn bản con người sử dụng biên phân kỳ. NeurIPS.

Matt Post. 2018. Một lời kêu gọi rõ ràng trong báo cáo điểm BLEU. Trong Kỷ yếu Hội nghị lần thứ ba về Dịch máy: Bài báo Nghiên cứu, trang 186–191, Brussels, Belgium. Hiệp hội Ngôn ngữ học Tính toán.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Các mô hình ngôn ngữ là những người học đa tác vụ không có giám sát. Blog OpenAI.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Khám phá các giới hạn của học chuyển giao với một transformer văn bản đến văn bản thống nhất. JMLR.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100,000+ câu hỏi để hiểu máy đọc văn bản. Trong Kỷ yếu Hội nghị 2016 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, trang 2383–2392, Austin, Texas. Hiệp hội Ngôn ngữ học Tính toán.

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, và Mark Chen. 2022. Tạo sinh hình ảnh có điều kiện văn bản phân cấp với clip latents. arXiv preprint arXiv:2204.06125.

Machel Reid, Vincent J Hellendoorn, và Graham Neubig. 2022. Diffuser: Khuếch tán rời rạc thông qua tái tạo dựa trên chỉnh sửa. arXiv preprint arXiv:2210.16886.

Pierre H Richemond, Sander Dieleman, và Arnaud Doucet. 2022. Sdes phân loại với khuếch tán simplex. arXiv preprint arXiv:2210.14784.

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Các mô hình khuếch tán văn bản đến hình ảnh photorealistic với hiểu ngôn ngữ sâu. Trong NeurIPS.

Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, và Aaron van den Oord. 2021. Autoencoder khử nhiễu step-unrolled cho tạo sinh văn bản. Trong ICLR.

Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, và Jiang Bian. 2023. Naturalspeech 2: Các mô hình khuếch tán tiềm ẩn là những bộ tổng hợp lời nói và hát tự nhiên và zero-shot. ArXiv, abs/2304.09116.

Emily Sheng, Kai-Wei Chang, Prem Natarajan, và Nanyun Peng. 2021. Thiên vị xã hội trong tạo sinh ngôn ngữ: Tiến bộ và thách thức. Trong Kỷ yếu Cuộc họp Thường niên lần thứ 59 của Hiệp hội Ngôn ngữ học Tính toán và Hội nghị Quốc tế lần thứ 11 về Xử lý Ngôn ngữ Tự nhiên Chung (Tập 1: Bài báo Dài), trang 4275–4293, Online. Hiệp hội Ngôn ngữ học Tính toán.

--- TRANG 11 ---
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, và Christopher Potts. 2013. Các mô hình sâu đệ quy cho tính tổng hợp ngữ nghĩa trên một treebank cảm xúc. Trong Kỷ yếu Hội nghị 2013 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, trang 1631–1642, Seattle, Washington, USA. Hiệp hội Ngôn ngữ học Tính toán.

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, và Surya Ganguli. 2015. Học không giám sát sâu sử dụng nhiệt động lực học không cân bằng. Trong ICML.

Jiaming Song, Chenlin Meng, và Stefano Ermon. 2020. Các mô hình khuếch tán ngầm khử nhiễu. Trong ICLR.

Yang Song, Prafulla Dhariwal, Mark Chen, và Ilya Sutskever. 2023. Các mô hình nhất quán. Trong ICML.

Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, và Ben Poole. 2021. Mô hình tạo sinh dựa trên điểm thông qua phương trình vi phân ngẫu nhiên. Trong ICLR.

Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, et al. 2022. Khuếch tán embedding tự điều kiện cho tạo sinh văn bản. arXiv preprint arXiv:2211.04236.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. NeurIPS.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. 2019. GLUE: Một chuẩn đa tác vụ và nền tảng phân tích để hiểu ngôn ngữ tự nhiên. Trong ICLR.

Alex Warstadt, Amanpreet Singh, và Samuel R. Bowman. 2019. Các phán đoán chấp nhận được của mạng neural. Transactions of the Association for Computational Linguistics, 7:625–641.

Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, và Iason Gabriel. 2022. Phân loại rủi ro do các mô hình ngôn ngữ gây ra. Trong Kỷ yếu Hội nghị ACM 2022 về Công bằng, Trách nhiệm và Minh bạch, FAccT '22, trang 214–229, New York, NY, USA. Hiệp hội Máy tính.

Adina Williams, Nikita Nangia, và Samuel Bowman. 2018. Một corpus thử thách phủ sóng rộng cho hiểu câu thông qua suy luận. Trong Kỷ yếu Hội nghị 2018 của Chương Bắc Mỹ của Hiệp hội Ngôn ngữ học Tính toán: Công nghệ Ngôn ngữ Con người, Tập 1 (Bài báo Dài), trang 1112–1122, New Orleans, Louisiana. Hiệp hội Ngôn ngữ học Tính toán.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander M. Rush. 2020. Transformers: Xử lý ngôn ngữ tự nhiên tiên tiến. Trong EMNLP: Trình diễn Hệ thống.

Wei Xu, Chris Callison-Burch, và Courtney Napoles. 2015. Các vấn đề trong nghiên cứu đơn giản hóa văn bản hiện tại: Dữ liệu mới có thể giúp ích. Transactions of the Association for Computational Linguistics, 3:283–297.

Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, và Chris Callison-Burch. 2016. Tối ưu hóa dịch máy thống kê cho đơn giản hóa văn bản. TACL.

Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, và Mingxuan Wang. 2023. Dinoiser: Học chuỗi có điều kiện khuếch tán bằng cách thao tác nhiễu.

Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, và Songfang Huang. 2022. Seqdiffuseq: Khuếch tán văn bản với transformer encoder-decoder. arXiv preprint arXiv:2212.10325.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. 2020. BERTscore: Đánh giá tạo sinh văn bản với BERT. Trong ICLR.

--- TRANG 12 ---
A Chi tiết Thí nghiệm
A.1 Tập dữ liệu
Tập dữ liệu NEWSELA-AUTO (Jiang et al., 2020) dựa trên Xu et al. (2015) với sự căn chỉnh được sửa đổi và cải thiện về số lượng và chất lượng. Đối với tạo câu hỏi, chúng tôi sử dụng tập dữ liệu QUASAR-T (Dhingra et al., 2017); đối với tóm tắt chúng tôi sử dụng CNN-DailyMail (Hermann et al., 2015).

Chuẩn GLUE (Wang et al., 2019) được phát hành dưới Giấy phép Creative Commons (CC BY 4.0). Chuẩn này bao gồm nhiều tập dữ liệu: SST-2 (Socher et al., 2013), MNLI (Williams et al., 2018), CoLA (Warstadt et al., 2019), MRPC (Dolan và Brockett, 2005), QQP9, QNLI (Rajpurkar et al., 2016), STS-B (Cer et al., 2017), và RTE, là sự kết hợp dữ liệu từ RTE1 (Dagan et al., 2005), RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), RTE5 (Bentivogli et al., 2009). Chúng tôi tải xuống tất cả các tập dữ liệu từ thư viện Hugging Face Datasets (Lhoest et al., 2021). Bảng 8 cho thấy độ dài chuỗi cho nguồn và đích trong mỗi tập dữ liệu, theo số token.

Tập dữ liệu Nguồn Đích
GLUE 128 5
NEWSELA-AUTO 128 128
QQP 100 85
QG 155 65
CNN-DM 392 120

Bảng 8: Độ dài chuỗi của mỗi tập dữ liệu.

A.2 Chi tiết Chuẩn
Khi các kết quả được công bố trước đó trên cùng tập dữ liệu và chỉ số, hoặc khi codebase không có sẵn công khai, chúng tôi sử dụng các kết quả đã báo cáo có sẵn để giảm chi phí tính toán. Để cung cấp một so sánh công bằng khi tự điều chỉnh các chuẩn (SSD-LM và BART), chúng tôi sử dụng cùng ngân sách điều chỉnh và so sánh chúng trong cùng cài đặt (xem Phụ lục A.3). Cuối cùng, chúng tôi báo cáo các giá trị sử dụng cùng chiến lược giải mã, tức là cài đặt mặc định mà không có giải mã rủi ro Bayes tối thiểu (MBR).

Đối với GENIE, chúng tôi sử dụng mã và trọng số mô hình được cung cấp bởi các tác giả10 để đánh giá GENIE sử dụng các cài đặt giải mã khớp với TESS.

A.3 Siêu tham số Huấn luyện
Theo Han et al. (2022), chúng tôi khởi tạo mô hình từ một mô hình đã tiền huấn luyện và tương tự thấy rằng nó cải thiện hiệu suất (Xem Bảng 3). Chúng tôi triển khai công việc của mình bằng HuggingFace Transformers (Wolf et al., 2020) và sử dụng Huggingface Diffusers11 để xây dựng pipeline khuếch tán của chúng tôi. Các thí nghiệm của chúng tôi được thực hiện trên 8 GPU NVIDIA A6000/A100.

Chúng tôi huấn luyện các mô hình và chuẩn của mình với tốc độ học 3e−5 với optimizer AdamW, với các tham số mặc định β1= 0.9, β2= 0.999, ϵ= 1e−8. Chúng tôi sử dụng bộ lập lịch tốc độ học tuyến tính. Chúng tôi sử dụng kích thước mô hình base cho tất cả các thí nghiệm (Wolf et al., 2020).

Đối với SSD-LM, chúng tôi sử dụng kích thước khối 25 theo bài báo gốc và cùng số bước khuếch tán trong quá trình huấn luyện và suy luận như các mô hình của chúng tôi. Chúng tôi tái sử dụng codebase được cung cấp bởi các tác giả và điều chỉnh nó để hỗ trợ các tác vụ hạ nguồn. Chúng tôi không kiểm tra SSD-LM trên CNN-DM do khó khăn trong việc huấn luyện SSD-LM trên các đầu ra liên quan đến nhiều khối giải mã, yêu cầu tiền xử lý dữ liệu tùy chỉnh và các điều chỉnh thuật toán thêm để xử lý các đầu ra dài.

Đối với tất cả các tác vụ tạo sinh, chúng tôi huấn luyện phương pháp và chuẩn của mình cho tạo paraphrase, tóm tắt, tạo câu hỏi và đơn giản hóa văn bản lần lượt trong 90K, 120K, 120K và 80K bước. Chúng tôi đặt số bước khởi động là 2000 cho tất cả các tác vụ tạo sinh. Đối với các thí nghiệm trên GLUE, chúng tôi đặt số bước khởi động là 500. Chúng tôi huấn luyện các mô hình trên các tập dữ liệu lớn hơn trong GLUE trong 25K bước; đối với các tập dữ liệu nhỏ hơn, chúng tôi sử dụng 12K bước. Sau đó chúng tôi đánh giá các mô hình mỗi 1K bước và báo cáo kết quả trên checkpoint đạt kết quả tốt nhất trên tập phát triển. Chúng tôi thấy rằng thời gian huấn luyện của mỗi mô hình là tương đối giống nhau: với các cấu hình tương đương trên một GPU duy nhất trên tập dữ liệu QQP, TESS đạt 1.7 bước huấn luyện mỗi giây; SSD-LM, 1.8; BART, 1.4, sử dụng PyTorch 2.0.

A.4 Chi tiết Gói Đánh giá
Chúng tôi sử dụng các gói sau để tính toán chỉ số đã cho:
•BLEU: Chúng tôi sử dụng gói sacrebleu (Post, 2018), v2.3.1.

11https://huggingface.co/docs/diffusers

--- TRANG 13 ---
•ROUGE: Chúng tôi sử dụng gói rouge-score, v0.2.1.12
•Mauve: Chúng tôi sử dụng gói mauve-text (Pillutla et al., 2021), v0.3.0.
•BERTScore: Chúng tôi sử dụng gói bert-score (Zhang et al., 2020), v0.3.12.

Đối với các chỉ số khác, chúng tôi sử dụng các triển khai riêng (thường dựa rất nhiều trên một triển khai tham chiếu), sẽ được mã nguồn mở.

A.5 Thí nghiệm Tốc độ Suy luận
Đối với các số tốc độ suy luận được báo cáo trong Bảng 4, chúng tôi chạy tất cả thí nghiệm trên một GPU NVIDIA A100 80 GB duy nhất. Chúng tôi sử dụng một phiên bản đã điều chỉnh của script suy luận SSD-LM được cung cấp bởi các tác giả trong kho lưu trữ công khai của họ, loại bỏ logging và khởi tạo tensor trên thiết bị để tránh các lệnh gọi .to() tốn kém. Đối với BART large, chúng tôi sử dụng thư viện transformers (Wolf et al., 2020). Chúng tôi thay đổi tất cả các mô hình để cho phép độ dài chuỗi trên 512 token bằng cách thay đổi kích thước ma trận embedding vị trí. Chúng tôi sử dụng ngữ cảnh sau: "Một người đàn ông với vô số tính cách và sức mạnh so với trí tuệ nhân tạo mạnh nhất trong vũ trụ này: Legion so với Nimrod! Với Nightcrawler trong nanh vuốt của Orchis, David Haller và các đồng minh của anh ta sẽ phải đối mặt với kẻ chủ mưu người mà". Chúng tôi báo cáo thời gian chính xác và độ lệch chuẩn trong Bảng 10.

B Bước Lấy mẫu
Chúng tôi đã thực hiện các ablation bổ sung về mối quan hệ giữa số bước lấy mẫu và hiệu suất trong tạo câu hỏi.

Mô hình Bước Forwards R-L
BART — 74 38.8
SSD-LM 10 50 33.0
SSD-LM 100 500 36.7
SSD-LM 1000 5000 38.5
TESS 10 10 38.8
TESS 100 100 38.9
TESS 1000 1000 38.9

Bảng 9: Ablation bước lấy mẫu trong tạo câu hỏi.

Chúng tôi lưu ý rằng số lượt forward không thể so sánh trực tiếp giữa các mô hình AR và NAR: các mô hình AR có thể sử dụng nhiều hoặc ít lượt forward hơn các mô hình NAR tùy thuộc vào số token được tạo ra, và mỗi lượt forward của mô hình AR liên quan đến một số token khác nhau. Ở đây, chúng tôi sử dụng 74, vì đây là số token BART trung bình trong các phản hồi tạo câu hỏi.

Nhìn chung, chúng tôi quan sát rằng hiệu suất của SSD-LM giảm đáng kể với ít bước khuếch tán hơn, trong khi TESS vẫn không bị ảnh hưởng lớn. Đáng chú ý, TESS đạt được sự ngang bằng với BART chỉ với 10 bước lấy mẫu.

C Bước Suy luận
Trong biến thể điển hình của DDPM, mô hình dự đoán nhiễu được thêm vào ϵθ(xt, t) thay vì tín hiệu ban đầu và bước suy luận DDPM (Ho et al., 2020) như sau:13

xt−1=1/√αt[xt−(1−αt)/√1−¯αt·ϵθ(xt, t)] (13)

Vì chúng tôi làm việc với biến thể dự đoán tín hiệu chính nó, chúng tôi thay thế (2) vào (13), thu được:

xt−1=1/√αt[√¯αtx0−(αt−¯αt)/√1−¯αt·ϵθ(xt, t)]. (14)

Cho rằng ¯αt= ¯αt−1αt, chúng tôi đi đến:

xt−1=√¯αt−1x0−√(αt−¯αt)/√αt·√αt/√1−¯αt·ϵθ(xt, t). (15)

Sử dụng lịch trình cosine cho ¯αt (Nichol và Dhariwal, 2021), √(αt−¯αt)/(1−¯αt)≥0.98 cho 98% t∈(1, T), với một số ngoại lệ khi t→0 và t→T (Han et al., 2022). Do đó, với xấp xỉ √(αt−¯αt)/(1−¯αt)≈1, Phương trình (15) đơn giản hóa thành:

xt−1=√¯αt−1x0−√1−¯αt−1·ϵθ(xt, t). (16)

Trong trường hợp của chúng tôi, tín hiệu xt là simplex St, với ˆSθ là dự đoán mô hình của sự thật cơ bản. Điều chỉnh ở trên với ký hiệu này, chúng tôi phục hồi Phương trình (9):

St−1=√¯αt−1ˆSθ(St, t) +√1−¯αt−1·ϵt.

D Ví dụ Định tính
Chúng tôi cho thấy các đầu ra mẫu được chọn ngẫu nhiên từ mô hình TESS và BART, chuẩn mạnh nhất, trên tác vụ tóm tắt trong Bảng 11. Về mặt định tính, chúng tôi quan sát rằng TESS có khả năng tạo ra các mẫu tự nhiên thường không thể phân biệt được với của BART.

12https://github.com/google-research/google-research/tree/master/rouge
13Theo Han et al. (2022) chúng tôi bỏ qua số hạng nhiễu bổ sung σtz, trong đó z∈ N(0,I).

--- TRANG 14 ---
Số Token
Mô hình Khối 25 100 200 300 400 500 600 700 800 900 1000 2000 3000 4000 5000
SSD-LM 25 1.6 0.36.60.315.0 0.325.6 0.337.8 0.353.4 0.470.4 0.090.1 0.0112.7 0.0139.4 0.0168.4 0.0643.2 0.21531.5 0.62929.4 0.94945.1 2.7
SSD-LM 200 1.6 0.32.10.32.80.35.80.36.50.3 - 11.7 0.3 - 17.9 0.0 - 25.8 0.089.6 0.0207.8 0.0391.1 0.2652.2 0.2
BART large - 0.8 0.21.40.22.60.23.20.24.10.24.60.25.50.06.50.07.60.0 8.70.0 9.80.023.5 0.240.6 0.1 62.4 0.1 89.4 0.4
TESS - 1.8 0.32.40.33.30.34.10.34.90.36.10.36.90.07.70.08.50.0 9.90.010.8 0.022.3 0.038.0 0.0 55.0 0.0 73.8 0.0

Bảng 10: Thời gian để tạo ra số token đã cho với tiền tố 50 token tính bằng giây. Tất cả các mô hình sử dụng 100 bước khuếch tán và RoBERTa large làm mô hình cơ bản. Tất cả các giá trị là trung bình trên 5 lần chạy, với độ lệch chuẩn được đưa ra dưới dạng chỉ số dưới; độ lệch chuẩn nhỏ hơn 0.05 xuất hiện như 0.0.

Vàng BART Chúng tôi
Thành viên cho ICC quyền tài phán đối với các tội ác bị cáo buộc được thực hiện ở các lãnh thổ Palestine từ tháng 6 năm ngoái.
Israel và Hoa Kỳ phản đối động thái này, có thể mở ra cánh cửa cho các cuộc điều tra tội ác chiến tranh chống lại người Israel.Cơ quan Palestine chính thức trở thành thành viên thứ 123 của Tòa án Hình sự Quốc tế.
Nó trao cho tòa án quyền tài phán đối với các tội ác bị cáo buộc được thực hiện ở các lãnh thổ Palestine.
Israel và Hoa Kỳ, cả hai đều không phải là thành viên ICC, đã phản đối nỗ lực của Palestine gia nhập cơ quan này.Cơ quan Palestine là thành viên thứ 123 của Tòa án Hình sự Quốc tế.
Động thái này trao cho nó quyền tài phán đối với các tội ác bị cáo buộc ở các lãnh thổ Palestine.
Israel và Hoa Kỳ phản đối nỗ lực của Palestine gia nhập nó.
Theia, một con lai giống bully, dường như bị xe hơi tông, đánh bằng búa và chôn trong cánh đồng.
"Cô ấy là một con chó thần kỳ thực sự và cô ấy xứng đáng có một cuộc sống tốt," Sara Mellado nói, người đang tìm kiếm một ngôi nhà cho Theia.Một con chó hoang ở Bang Washington đã sử dụng hết ít nhất ba trong số những thương tích của riêng mình.
Theia, một con lai giống bully, bị xe hơi tông và sau đó bị chôn trong cánh đồng.
Cô ấy đã được chăm sóc tại Bệnh viện Giảng dạy Thú y.Một con chó hoang được tìm thấy trên một trang trại ở Bang Washington.
Con chó đã sử dụng hết ít nhất ba trong số những thương tích của riêng mình sau khi bị xe hơi tông vào đầu và bị chôn trong cánh đồng.
Theia có hàm bị trật khớp, chấn thương chân và cần phẫu thuật để giúp thở.
Mohammad Javad Zarif đã dành nhiều thời gian với John Kerry hơn bất kỳ ngoại trưởng nào khác.
Ông từng tham gia vào việc chiếm đoạt Lãnh sự quán Iran ở San Francisco.
Ngoại trưởng Iran tweet bằng tiếng Anh.Mohammad Javad Zarif là ngoại trưởng Iran.
Ông đã là đối tác của John Kerry trong việc đảm bảo một bước đột phá trong các cuộc đàm phán hạt nhân.Mohammad Javad Zarif hiện là ngoại trưởng Iran.
Ông đã là đối tác của John Kerry trong việc đảm bảo một bước đột phá trong các cuộc đàm phán hạt nhân.
17 người Mỹ đã bị phơi nhiễm virus Ebola khi ở Sierra Leone vào tháng 3.
Một người khác được chẩn đoán mắc bệnh này và đưa đến bệnh viện ở Maryland.
Viện Y tế Quốc gia nói rằng bệnh nhân trong tình trạng bình thường sau nhiều tuần điều trị.Một trong năm người đã có vấn đề về tim vào thứ Bảy và đã xuất viện.
Những người khác đã về nhà.Năm người Mỹ được theo dõi trong ba tuần tại một bệnh viện Omaha.
Một trong năm người đã có vấn đề về tim vào thứ Bảy.
Những người khác đã về nhà.
Họ đã bị phơi nhiễm Ebola ở Sierra Leone, nhưng không ai phát triển virus chết người.
Sinh viên không còn ở trong khuôn viên Đại học Duke và sẽ phải đối mặt với việc xem xét kỷ luật.
Các quan chức trường đã xác định sinh viên trong quá trình điều tra và người này thừa nhận treo dây thòng lọng, Duke nói.
Dây thòng lọng, làm từ dây thừng, được phát hiện trong khuôn viên vào khoảng 2 giờ sáng.Một sinh viên thừa nhận treo dây thòng lọng làm từ dây thừng từ một cây gần liên đoàn sinh viên.
Duke không xác định sinh viên, viện dẫn luật riêng tư liên bang.
Vụ việc là một trong số những sự kiện phân biệt chủng tộc gần đây ảnh hưởng đến sinh viên đại học.Một sinh viên Duke thừa nhận treo dây thòng lọng từ một cây.
Trường tư thục không xác định sinh viên, viện dẫn luật riêng tư liên bang.
Sinh viên không còn ở trong khuôn viên và sẽ phải đối mặt với việc xem xét hành vi sinh viên.
Ngôi sao bóng rổ định học đại học mời cô gái mắc hội chứng Down đến dạ tiệc tốt nghiệp trung học.
Hình ảnh của hai người trong "prom-posal" đã lan truyền.Cầu thủ bóng rổ trường trung học Eastern Trey Moses mời bạn gái của mình đến dạ tiệc tốt nghiệp.
Ellie Meredith, một sinh viên năm nhất mắc hội chứng Down, đã gặp khó khăn với tình bạn từ tiểu học.
Một chương trình đặc biệt tại Eastern đã giúp mọi thứ dễ dàng hơn cho cô ấy, mẹ cô ấy nói.Cầu thủ bóng rổ đại học và sinh viên năm nhất trung học đã chọn Ellie Meredith làm bạn nhảy dạ tiệc.
Những bức ảnh đã lan truyền trên mạng xã hội.
Báo cáo hằng năm về hình phạt tử hình của Amnesty ghi lại những dấu hiệu khuyến khích, nhưng thất bại trong số những người bị kết án tử hình.
Tổ chức tuyên bố rằng các chính phủ trên thế giới đang sử dụng mối đe dọa khủng bố để thúc đẩy các vụ hành quyết.
Số vụ hành quyết trên toàn thế giới đã giảm gần 22% so với năm 2013, nhưng án tử hình tăng 28%.Amnesty International nói rằng các chính phủ đang sử dụng hình phạt tử hình để thúc đẩy các vụ hành quyết.
Ít nhất 607 người đã bị hành quyết trên toàn thế giới năm 2014, so với 778 năm 2013.Amnesty International phát hành báo cáo hằng năm về hình phạt tử hình.
Ít nhất 607 người đã bị hành quyết năm 2014, so với 778 năm 2013.
Báo cáo nêu Pakistan dỡ bỏ lệnh cấm sáu năm đối với việc hành quyết dân thường.
Trung Quốc đã sử dụng hình phạt tử hình như một công cụ trong chiến dịch "Đánh Mạnh" chống khủng bố ở tỉnh Tân Cương phía tây xa bất ổn.
Cái chết của Andrew Getty dường như là do nguyên nhân tự nhiên, cảnh sát nói, trích dẫn đánh giá sớm của nhà pháp y.
Trong một đơn xin lệnh cấm, Getty đã viết rằng ông có tình trạng y tế nghiêm trọng.
Cảnh sát nói đây không phải là vấn đề hình sự vào thời điểm này.Andrew Getty dường như đã chết vì nguyên nhân tự nhiên, cảnh sát nói.
Đánh giá sơ bộ của nhà pháp y là không có tác động xấu liên quan đến cái chết của ông.
Ông là cháu trai của ông trùm dầu mỏ J. Paul Getty.Đánh giá sơ bộ của nhà pháy nói rằng không có tác động xấu liên quan đến cái chết của Getty.
Andrew Getty, 47 tuổi, có "một số vấn đề sức khỏe," thám tử nói.
Không có cuộc điều tra hình sự nào đang diễn ra, thám tử nói.
Từng là một siêu bão, Maysak hiện là một cơn bão nhiệt đới với gió 70 mph.
Nó vẫn có thể gây lũ lụt, lở đất và các vấn đề khác ở Philippines.Maysak đã mất rất nhiều sức mạnh khi quay tây trong Thái Bình Dương.
Nó có gió ổn định hơn 70 mph (115 kph) và gió giật lên đến 90 mph.Maysak đã đạt được tình trạng siêu bão nhờ gió bền vững 150 mph.
Chính quyền đã cấm các hoạt động ngoài trời như bơi lội, lướt sóng, lặn và đi thuyền ở một số địa điểm.

Bảng 11: Các mẫu được chọn ngẫu nhiên được tạo ra trên tập dữ liệu CNN-DM bởi BART và TESS.
