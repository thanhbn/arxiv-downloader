# 2211.04236.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2211.04236.pdf
# Kích thước tệp: 910335 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DIFFUSION NHÚNG TỰ ĐIỀU KIỆN
CHO SINH TẠO VĂN BẢN
Robin Strudel1Corentin Tallec2Florent Altché2Yilun Du3
Yaroslav Ganin2Arthur Mensch2Will Grathwohl2Nikolay Savinov2
Sander Dieleman2Laurent Sifre2Rémi Leblond2

TÓM TẮT
Liệu các mô hình diffusion liên tục có thể mang lại cùng một bước đột phá về hiệu suất trên ngôn ngữ tự nhiên như chúng đã làm cho sinh tạo hình ảnh? Để vượt qua bản chất rời rạc của dữ liệu văn bản, chúng ta có thể đơn giản chiếu các token vào không gian liên tục của các nhúng, như tiêu chuẩn trong mô hình hóa ngôn ngữ. Chúng tôi đề xuất Self-conditioned Embedding Diffusion (SED), một cơ chế diffusion liên tục hoạt động trên các nhúng token và cho phép học các mô hình diffusion linh hoạt và có thể mở rộng cho cả sinh tạo văn bản có điều kiện và không điều kiện. Thông qua đánh giá định tính và định lượng, chúng tôi chỉ ra rằng các mô hình diffusion văn bản của chúng tôi sinh ra các mẫu có thể so sánh với những mẫu được tạo ra bởi các mô hình ngôn ngữ tự hồi quy tiêu chuẩn - trong khi về lý thuyết hiệu quả hơn trên phần cứng gia tốc tại thời điểm suy luận. Công trình của chúng tôi mở đường cho việc mở rộng quy mô các mô hình diffusion cho văn bản, tương tự như các mô hình tự hồi quy, và để cải thiện hiệu suất với các cải tiến gần đây cho diffusion liên tục.

1 GIỚI THIỆU
Các mô hình diffusion liên tục (Sohl-Dickstein et al., 2015) đã làm náo loạn thế giới sinh tạo hình ảnh, đẩy trình độ nghệ thuật tiến xa hơn bao giờ hết (Rombach et al., 2021; Ramesh et al., 2022). Liệu cùng một khung có thể gặp thành công tương tự trên phương thức văn bản? Diffusion cho ngôn ngữ thực sự là một triển vọng hấp dẫn. So với các mô hình tự hồi quy (AR) (Bengio et al., 2000; Sutskever et al., 2011; Austin et al., 2021; Hoffmann et al., 2022), các mô hình diffusion có thể dự đoán tất cả các token trong một chuỗi cùng một lúc. Điều này cho phép attention hai chiều, thay vì nhân quả—tăng cường tương tác giữa các token, có thể dẫn đến các mẫu nhất quán hơn. Các mô hình diffusion có thể sử dụng phần cứng gia tốc tốt hơn trong quá trình suy luận so với các mô hình AR, vì các tính toán có thể song song hóa trên trục chuỗi.

Tuy nhiên, các mô hình AR vẫn là phương pháp chính thống để mô hình hóa văn bản. Một trở ngại lớn đối với diffusion văn bản là các quá trình diffusion thường hoạt động trong không gian liên tục. Trong khi điều này xử lý tự nhiên hình ảnh, văn bản về bản chất là rời rạc. Do đó, hầu hết các nỗ lực trước đây để áp dụng diffusion cho văn bản đã tập trung vào các phương pháp giống diffusion rời rạc. Những phương pháp này không được hưởng lợi từ các cải tiến được thực hiện cho diffusion liên tục trong lĩnh vực hình ảnh. Quan trọng nhất, chúng không thể sử dụng guidance (Dhariwal & Nichol, 2021), điều này cải thiện đáng kể chất lượng mẫu của các mô hình diffusion.

Chúng tôi giải quyết khoảng cách này bằng cách thực hiện một quan sát đơn giản: các mô hình ngôn ngữ hoạt động chủ yếu trong không gian liên tục, với các token rời rạc chỉ là đầu vào và đầu ra. Một ý tưởng tự nhiên sau đó là tiến hành diffusion trực tiếp trong không gian nhúng token liên tục. Để đơn giản, chúng tôi sử dụng một không gian nhúng cố định, hoặc ngẫu nhiên hoặc xuất phát từ một mô hình ngôn ngữ được đào tạo. Kết hợp với cải tiến "tự điều kiện" (Chen et al., 2022), điều này tạo thành cơ sở của phương pháp chúng tôi đề xuất, Self-conditioned Embedding Diffusion (SED).

1INRIA, Département d'informatique, École normale supérieure, CNRS, PSL Research University
Công việc được thực hiện trong khi thực tập tại DeepMind
2DeepMind
3Massachusetts Institute of Technology

--- TRANG 2 ---
Các mô hình SED cạnh tranh với các mô hình AR chính thống trong cả sinh tạo văn bản có điều kiện và không điều kiện. Chúng tôi đóng góp những điều sau:

• Trong phần 3, chúng tôi giới thiệu SED, phương pháp diffusion liên tục đầu tiên cho văn bản với các thuộc tính mở rộng tốt (thử nghiệm các mô hình lên đến 420M tham số). Chúng tôi phân tích một số cài đặt diffusion văn bản liên tục, và xác định tự điều kiện và diffusion trên các nhúng cố định nhỏ là các yếu tố chính để làm cho diffusion văn bản liên tục hoạt động.

• Trong phần 4, chúng tôi áp dụng guidance không phân loại (Ho & Salimans, 2022) cho dữ liệu văn bản—một thành tựu ban đầu. Chúng tôi chỉ ra rằng SED có thể cạnh tranh với các mô hình AR trên các nhiệm vụ ngôn ngữ chung, với các kích thước mô hình tương tự. Các mẫu SED đạt được sự cân bằng khả năng-entropy tốt hơn so với các mô hình này, và được các người đánh giá con người coi là tương đương (nếu hơi tệ hơn một chút).

2 CÔNG TRÌNH LIÊN QUAN
Chúng tôi cung cấp một tổng quan về các mô hình diffusion với trọng tâm vào mô hình hóa dữ liệu rời rạc, cũng như các mô hình AR và các số liệu dựa trên mẫu để đánh giá sinh tạo văn bản.

Diffusion liên tục trên dữ liệu hình ảnh liên tục. Diffusion liên tục gần đây đã thiết lập mình như phương pháp lựa chọn để mô hình hóa dữ liệu liên tục như hình ảnh. Trong khi trọng tâm chính của chúng tôi trong bài báo này là về dữ liệu rời rạc, chúng tôi xem xét một số công trình quan trọng trong mô hình hóa dữ liệu liên tục vì văn học này là nguồn cảm hứng chính cho SED. Công thức diffusion liên tục đầu tiên được giới thiệu trong công trình tinh túy của Sohl-Dickstein et al. (2015). Ho et al. (2020) đã cải thiện và đơn giản hóa công thức này, liên kết nó với khớp điểm denoising, và tạo ra một phương pháp mới gọi là DDPM. Nichol & Dhariwal (2021) đã cải thiện thêm DDPM, thể hiện kết quả diffusion ấn tượng so với GANs. Rombach et al. (2021, Stable Diffusion) đã giới thiệu diffusion trong không gian tiềm ẩn. Về mặt khái niệm tương tự như SED, nó được nhắm mục tiêu cụ thể cho mô hình hóa hình ảnh. Guidance không phân loại được đề xuất bởi Ho & Salimans (2022) như một phương tiện để cải thiện độ trung thực hình ảnh với chi phí giảm đa dạng. GLIDE (Nichol et al., 2022) đã mở rộng quy mô các ý tưởng về guided diffusion, trong khi DALL-E 2 (Ramesh et al., 2022) và Imagen (Saharia et al., 2022) là những hệ thống sinh tạo hình ảnh tiên tiến nhất, mới nhất cho đến nay, kết hợp hầu hết các cải tiến được đề xuất trong các công trình trước đây.

Diffusion rời rạc trên dữ liệu rời rạc. Người ta không thể đơn giản tái sử dụng các phương pháp thành công trên dữ liệu hình ảnh liên tục trong lĩnh vực văn bản rời rạc. Một số phương pháp riêng biệt đã được khám phá thay thế, tạo thành họ các phương pháp diffusion rời rạc. Trong diffusion rời rạc, dữ liệu bị hỏng bằng cách chuyển từ một giá trị rời rạc sang giá trị khác. Điều này lần đầu tiên được đề xuất trong công trình tinh túy của Sohl-Dickstein et al. (2015), nơi nó được thử nghiệm trên dữ liệu nhịp tim nhị phân đơn giản. Nó được mở rộng cho mô hình hóa văn bản đa thức (Hoogeboom et al., 2021) và được mở rộng thêm trong công trình D3PM (Austin et al., 2021). Gần đây nhất, một phương pháp diffusion rời rạc tương tự đã được áp dụng cho mô hình hóa hình ảnh trong VQ-Diffusion (Gu et al., 2022). Song song, một vài phương pháp giống diffusion được đề xuất trong văn học bộ tự mã hóa denoising. CMLM (Ghazvininejad et al., 2019) giải quyết dịch máy. SUNDAE (Savinov et al., 2022) là phương pháp không AR đầu tiên cho thấy kết quả mạnh mẽ cả trong dịch máy và sinh tạo văn bản không điều kiện. MaskGIT (Chang et al., 2022) đã chứng minh kết quả xuất sắc trong mô hình hóa hình ảnh rời rạc VQ. Những phương pháp này dựa vào đào tạo các mô hình để dự đoán các token bị che từ ngữ cảnh của chúng, và lặp lại bước tái tạo này nhiều lần tại thời điểm lấy mẫu. Mặc dù những phát triển tích cực đó, các mẫu từ các phương pháp diffusion rời rạc cho mô hình hóa văn bản vẫn ít nhất quán hơn những mẫu được tạo ra bởi các phương pháp AR.

Diffusion liên tục trên dữ liệu rời rạc. Ít công trình hơn cố gắng giải quyết diffusion trên dữ liệu rời rạc từ cùng góc độ như SED– bắt đầu bằng cách biến dữ liệu thành các biểu diễn liên tục trước khi mô hình hóa nó với các công thức diffusion liên tục. Mittal et al. (2021) đã sử dụng một VAE để tạo ra những biểu diễn như vậy cho mô hình hóa âm nhạc rời rạc, với kết quả thú vị. Gần nhất với SED, Diffusion-LM (Li et al., 2022) đào tạo một nhúng token cùng với chính mô hình diffusion. Diffusion-LM gặp thành công trên các ứng dụng ngôn ngữ cụ thể, trong chế độ dữ liệu thấp và trên dữ liệu văn bản được hạn chế, rất có định dạng. Gần đây nhất, Analog Bits (Chen et al., 2022) đã giới thiệu tự điều kiện, liên quan mật thiết đến step-unrolls trong SUNDAE (Savinov et al., 2022), cùng với mô hình hóa cấp bit để cải thiện sinh tạo hình ảnh rời rạc. Trong khi các kết quả định tính của những phương pháp liên tục đó trên mô hình hóa văn bản cho thấy triển vọng, chúng chưa được chỉ ra là mở rộng quy mô cho các tập dữ liệu văn bản lớn thực tế như C4 (Raffel et al., 2020), hoặc để so sánh với các phương pháp AR trên các nhiệm vụ ngôn ngữ chung.

--- TRANG 3 ---
Mô hình tự hồi quy trên dữ liệu rời rạc. Các mô hình AR vẫn là phương pháp lựa chọn để mô hình hóa dữ liệu rời rạc. Kết hợp với mạng nơ-ron, chúng lần đầu tiên được khám phá bởi Bengio et al. (2000) và sau đó được kết hợp với RNN (Sutskever et al., 2011). Khoảnh khắc đột phá của chúng đến với sự ra đời của kiến trúc Transformer, được giới thiệu bởi Vaswani et al. (2017) cho dịch máy. Kết quả ấn tượng hơn nữa đã được thể hiện với GPT-3 (Brown et al., 2020), mà đã đào tạo một mô hình ngôn ngữ AR lớn không điều kiện, và sử dụng prompting few-shot để thích ứng nó với các nhiệm vụ mới. Một vài công trình sau đó đã cải thiện kết quả của GPT-3, bao gồm Hoffmann et al. (2022).

Đánh giá dựa trên mẫu của các mô hình sinh tạo văn bản. Truyền thống có hai lớp số liệu cho mô hình hóa sinh tạo: dựa trên khả năng và dựa trên mẫu. Trong khi cách dựa trên khả năng hấp dẫn về mặt toán học, tính hữu ích của nó để đo lường tiến độ bị giảm bởi thực tế là không phải tất cả các mô hình đều dễ dàng cung cấp tính toán khả năng. Giống như số liệu FID dựa trên mẫu quan trọng để thúc đẩy tiến độ của diffusion trong mô hình hóa hình ảnh, có nhu cầu về một số liệu dựa trên mẫu sẽ được chấp nhận rộng rãi cho mô hình hóa văn bản. Caccia et al. (2018) đã điều tra các số liệu fidelity/variance để đánh giá text GANs. Semeniuta et al. (2018) đề xuất sử dụng FID cho văn bản. De Masson d'Autume et al. (2019) sau đó đã sử dụng những số liệu được đề xuất trước đó để lặp lại trên ScratchGAN nhưng không cung cấp hướng dẫn kết luận về số liệu nào một người thực hành nên chọn – về cơ bản tìm thấy các lỗ hổng nghiêm trọng trong tất cả các số liệu được điều tra. Chúng tôi đã chọn một con đường trung gian, báo cáo cả khả năng mẫu theo một mô hình AR mạnh và sở thích của con người.

3 PHƯƠNG PHÁP
Trong phần này, chúng tôi phác thảo các thành phần khác nhau của SED: diffusion liên tục trong không gian của các nhúng token và tự điều kiện, tạo thành cơ sở của phương pháp chúng tôi cho sinh tạo văn bản không điều kiện; che span và guided diffusion để cho phép sinh tạo có điều kiện.

3.1 MÔ HÌNH DIFFUSION CHO SINH TẠO VĂN BẢN KHÔNG ĐIỀU KIỆN

Các mô hình diffusion trong không gian liên tục. Chúng tôi xem xét các mô hình diffusion như được giới thiệu bởi Sohl-Dickstein et al. (2015) và được cải thiện bởi Ho et al. (2020). Một mô hình diffusion nhằm mô hình hóa một phân phối dữ liệu x₀ ∈ ℝⁿ với q₀ ∈ D(ℝⁿ) bằng cách ước tính một chuỗi các biến tiềm ẩn xₜ, ..., x₁ có cùng chiều với dữ liệu x₀. Bắt đầu từ x₀, các biến tiềm ẩn được tạo ra với một chuỗi Markov được gọi là quá trình thuận: xₜ ~ q(·|xₜ₋₁, t). Nó được định nghĩa bằng cách dần dần nội suy lặp lại với nhiễu Gaussian theo các mức nhiễu được định nghĩa bởi một lịch trình β₁, ..., βₜ:

xₜ ~ q(·|xₜ₋₁, t) = N(√(1-βₜ)xₜ₋₁, βₜI)    (1)

Phần tham số hóa này cho chúng ta một dạng đóng để lấy mẫu xₜ cho bất kỳ t ≥ 1 tùy ý, cho trước x₀:

xₜ = √ᾱₜxₜ₋₁ + √(1-ᾱₜ)ϵₜ = √ᾱₜx₀ + √(1-ᾱₜ)ϵ    (2)

trong đó ᾱₜ := 1-βₜ, ᾱₜ := ∏ᵢ₌₁ᵗ ᾱᵢ, ϵₜ ~ N(0, I) và ϵ ~ N(0, I).

Chúng tôi định nghĩa mô hình sinh tạo của chúng tôi bằng cách đảo ngược gần đúng quá trình diffusion của Eq. 1 để có được một quá trình ngược. Quá trình ngược bắt đầu từ xₜ ~ N(0, I) và được định nghĩa như một chuỗi Markov với các chuyển tiếp Gaussian đã học (được tham số hóa bởi θ, trọng số của một mạng nơ-ron): xₜ₋₁ ~ pθ(·|xₜ) = N(μθ(xₜ, t), σ²(t)I). Chúng tôi đào tạo một mạng nơ-ron để dự đoán một ước tính x̂₀(xₜ, t, θ) của dữ liệu x₀ và xấp xỉ quá trình ngược bằng cách sử dụng phần tham số hóa sau, với các trung bình có thể học nhưng phương sai cố định, và một lịch trình cố định β₁, ..., βₜ:

μθ(xₜ, t) = √ᾱₜ₋₁βₜ/(1-ᾱₜ)x̂₀(xₜ, t, θ) + √αₜ(1-ᾱₜ₋₁)/(1-ᾱₜ)xₜ, σ²(t) = (1-ᾱₜ₋₁)/(1-ᾱₜ)βₜ    (3)

Mặc dù tồn tại một cận dưới biến phân có thể giải (VLB) trên log p(x₀), Ho et al. (2020) đã chỉ ra rằng kết quả tốt hơn được thu được bằng cách tối ưu hóa một mục tiêu đơn giản hóa tái trọng số các thuật ngữ trong VLB. Chúng tôi theo phương pháp này, điều này đơn giản hóa tổn thất thành một tổng của lỗi bình phương trung bình giữa dữ liệu thực tế x₀ và các ước tính x̂₀(xₜ, t, θ):

--- TRANG 4 ---
Ldiffusion = Ex₀~q(x₀),t~U(1,T)[||x₀ - x̂₀(xₜ, t, θ)||²]    (4)

Mặc dù khung này hoạt động ngay lập tức trên hình ảnh, gần với liên tục, chúng ta không thể áp dụng nó trực tiếp cho các token rời rạc của phương thức văn bản. Để giải quyết vấn đề này, chúng tôi thực hiện diffusion liên tục trong một không gian liên tục mà chúng tôi nhúng các token văn bản.

Diffusion trên nhúng từ. Chúng tôi xem xét dữ liệu văn bản w = (w₁, ..., wₙ), trong đó mỗi wᵢ là một biểu diễn one-hot trong ℝⱽ của một token rời rạc trong {1, ..., V}. Mỗi token w có một nhúng liên quan eᵨ ∈ ℝᴰ, với chuẩn cố định √D để khớp với chuẩn của một mẫu gaussian ngẫu nhiên trong chiều D được sử dụng để nhiễu dữ liệu sạch. Chúng tôi ký hiệu bằng E ∈ ℝᴰˣⱽ ma trận của tất cả các nhúng.

Chúng tôi định nghĩa quá trình diffusion của chúng tôi trong không gian nhúng, thay vì trong không gian token. Để làm điều đó, chúng tôi định nghĩa một bước rời rạc-đến-liên tục thuận qᵥ(x₀|w) = N(Ew, σ₀²I), trong đó σ₀ là một yếu tố tỷ lệ hằng số với cùng thứ tự độ lớn như β₁. Ngược lại, chúng tôi định nghĩa một bước liên tục-đến-rời rạc ngược pᵣ(w|x₀) = ∏ᵢ₌₁ᴺ Cat(wᵢ|R(x₀)ᵢ), trong đó R ∈ ℝⱽˣᴰ là một ma trận readout có thể học được khởi tạo thành E^T và Cat(wᵢ|ℓ) là xác suất softmax của token k với logits ℓ ∈ ℝⱽ.

Để đào tạo bước readout, chúng tôi thêm một tổn thất tái tạo vào Ldiffusion trong quá trình đào tạo. Thuận tiện, nó tự nhiên phát sinh khi dẫn xuất VLB của p(w) với bước rời rạc hóa này (Li et al., 2022), giới thiệu một tổn thất cross-entropy đơn giản để tối đa hóa p(w|x₀):

Lrecon = Ew~D,x₀~qᵥ(w)[-log pᵣ(w|x₀)], với Ltotal = Ldiffusion + Lrecon    (5)

Trái ngược với những gì được thực hiện trong Li et al. (2022), chúng tôi không học ma trận nhúng E, vì chúng tôi đã xác định rằng nó không ổn định về mặt thực nghiệm và có thể dẫn đến giảm entropy unigram. Tổn thất tái tạo Lrecon do đó chỉ phụ thuộc vào trọng số readout có thể đào tạo R.

Tại thời điểm lấy mẫu, chúng tôi chạy quá trình ngược trong T = 1000 bước, cuối cùng tạo ra một nhúng liên tục x₀ có kích thước dembed. Chúng tôi nhân nó với R để có được logits trong ℝⱽ, và sau đó sử dụng chỉ số của thành phần tối đa để chuyển đổi nó thành một token wᵢ, với i = arg max₁≤j≤V(Rx₀). Điều này đòi hỏi chạy T lần chuyển tiếp đầy đủ khá đắt so với lấy mẫu AR được cache; tuy nhiên mỗi lần chuyển tiếp tính toán tất cả timesteps cùng một lúc có thể tự nhiên song song hóa. Hơn nữa, chúng tôi hy vọng hưởng lợi từ nhiều cải tiến lấy mẫu diffusion để giảm T xuống số đôi thấp.

Tự điều kiện (Chen et al., 2022). Trong lấy mẫu diffusion tiêu chuẩn, tại mỗi timestep t mạng denoising tạo ra một ước tính x₀ᵗ = x̂₀(xₜ, t, θ) của x₀ chỉ cho trước xₜ như đầu vào. Tự điều kiện dần dần tinh chỉnh các ước tính x₀ bằng cách truyền ước tính x̃₀ᵗ⁺¹ thu được tại bước lấy mẫu trước đó như đầu vào cho mạng denoising; ước tính tự điều kiện sau đó được định nghĩa như x̃₀ᵗ = x̂₀(xₜ, x̃₀ᵗ⁺¹, t, θ), và đặt hướng diffusion. Trong thực tế, điều kiện được thực hiện bằng cách nối xₜ và x̃₀ᵗ⁺¹ trên trục đặc trưng. Để xấp xỉ hành vi suy luận tại thời điểm đào tạo trong khi vẫn hiệu quả về mặt tính toán, chúng tôi tính toán một ước tính đầu tiên x₀ᵗ = x̂₀(xₜ, 0, t, θ) với tự điều kiện được đặt thành không, sau đó thực hiện một lần chuyển tiếp thứ hai sử dụng gradient dừng trên x₀ᵗ để có được x̃₀ᵗ = x̂₀(xₜ, x₀ᵗ, t, θ). Mạng denoising sau đó được tối ưu hóa sử dụng đầu ra từ hai lần chuyển tiếp để ước tính x₀ chính xác có và không có tự điều kiện.

Trang bị với 3 thành phần này, chúng ta có thể đào tạo các mô hình để sinh tạo văn bản, mặc dù chỉ không điều kiện. Để thêm sinh tạo có điều kiện vào khả năng của hệ thống, chúng tôi sử dụng hai phương pháp bổ sung.

3.2 CHE SPAN VÀ GUIDANCE CHO SINH TẠO VĂN BẢN CÓ ĐIỀU KIỆN

Theo thiết kế, các mô hình diffusion cho sinh tạo văn bản linh hoạt và có thể xử lý nhiều nhiệm vụ infilling đa dạng. Đây là một lợi thế chính so với các mô hình ngôn ngữ tự hồi quy chiếm ưu thế thường sinh tạo văn bản theo kiểu từ-trái-đến-phải.

Che span. Chúng tôi đào tạo mô hình của chúng tôi trên một tập hợp phong phú các nhiệm vụ infilling với phương pháp sau. Chúng tôi tách x₀ giữa hai tập hợp token, các token diffusion x mà chúng tôi áp dụng diffusion và tối ưu hóa tổn thất diffusion từ Eq. 4, và các token điều kiện c vẫn cố định. Các token điều kiện c được định nghĩa bởi một mặt nạ điều kiện nhị phân m được đặt thành một trên các vị trí điều kiện và không trên các vị trí cần infill.

Chúng tôi lấy mẫu mặt nạ điều kiện m ngẫu nhiên như sau. Cho một chuỗi có độ dài L và một số tối đa span M, chúng tôi lấy mẫu một số span n đồng đều trong [1, M]. Các vị trí bắt đầu span được định nghĩa bởi n-1 số nguyên (i₁, ..., iₙ₋₁) được lấy mẫu đồng đều không thay thế và được sắp xếp tăng dần để thỏa mãn 0 < i₁ < ... < iₙ₋₁ < L. Tuple (i₁, ..., iₙ₋₁) phân vùng chuỗi token thành n span thỏa mãn E[iₖ|n] = k/n L. Mặt nạ điều kiện m được định nghĩa sử dụng các span chẵn để điều kiện và các span lẻ để infill, và sau đó m được lật với xác suất 50%.

--- TRANG 5 ---
Bảng 1: Các mẫu SED về sinh tạo không điều kiện, fill-in-the-middle và infilling nhiều span.

Nhiệm vụ | Mẫu
---|---
Không điều kiện | Chúng tôi sử dụng nguồn cung cấp và giải pháp tốt nhất để đảm bảo rằng công việc sẽ đứng vững trước thử thách của thời gian, và chúng tôi giúp bạn tiết kiệm tiền với các kỹ thuật không thay đổi chất lượng nhiệm vụ của bạn. Chúng tôi sẽ đạt được điều này bằng cách cung cấp cho bạn những ưu đãi tốt nhất trong lĩnh vực và tránh những lỗi lầm đắt đỏ. Nếu bạn muốn chi tiêu ít hơn, RefrigeratorUnit RepairGuys là công ty cần liên hệ.
Fill-in-the-middle | Một năm trước tại Paris, tôi có cơ hội tham gia một chuyến thực tế đến LaRite-en-Laurences International deFrance nơi tôi gặp David Nigel Johnson, một giáo sư xã hội học. Thật là một chuyến đi tuyệt vời và thật là một ngày tuyệt vời!
Infilling span | Không có bằng chứng, chỉ có những cái nhìn thoáng qua về kẻ giết người và số phận của hắn. Thực tế, có vẻ như không có bằng chứng nào. Tất cả chỉ là phỏng đoán, và một trong những vụ án giết người bất thường nhất trong suốt lịch sử.

Trường hợp n = 1 tương ứng với sinh tạo không điều kiện; sau đó chúng tôi đặt m thành 0 ở mọi nơi.

Chiến lược che span này định nghĩa một tập hợp các nhiệm vụ sinh tạo văn bản với sự đa dạng lớn của điều kiện, trung bình chia đều chuỗi giữa các span điều kiện và infilling. Nó cho phép sinh tạo có điều kiện, và mở ra cánh cửa cho những cải tiến diffusion bổ sung.

Guided diffusion. Guidance (Dhariwal & Nichol, 2021) thường cải thiện chất lượng mẫu của các mô hình diffusion có điều kiện. Chúng tôi sử dụng guidance không phân loại (Ho & Salimans, 2022), điều này giảm bớt nhu cầu về một mô hình hướng dẫn được đào tạo riêng biệt. Trong trường hợp có điều kiện, ước tính x̃₀ của chúng tôi bây giờ là một hàm x̂₀(xₜ, c, x̃₀ᵗ⁺¹, t, θ), trong đó c là các token điều kiện cố định.

Trong quá trình đào tạo, với xác suất cố định, các token điều kiện c được sử dụng trong ước tính x̂₀ bị bỏ và đặt thành một nhãn null ∅ bằng không. Trong quá trình lấy mẫu, dự đoán mô hình được ngoại suy theo hướng của x̂₀(xₜ, c, x̃₀ᵗ⁺¹, t, θ) và xa khỏi x̂₀(xₜ, ∅, 0, t, θ) như sau:

x̃₀ᵗ·ˢ = x̂₀(xₜ, ∅, 0, t, θ) + s[x̂₀(xₜ, c, x̃₀ᵗ⁺¹, t, θ) - x̂₀(xₜ, ∅, 0, t, θ)]    (6)

trong đó s ≥ 1 là thang guidance. Lưu ý rằng chúng tôi cùng bỏ điều kiện và tự điều kiện x̃₀ᵗ⁺¹, cụ thể đặt cả hai giá trị thành không. Guidance không phân loại cho phép tận dụng cả khả năng không điều kiện và có điều kiện của một mô hình để cải thiện các sinh tạo có điều kiện của nó.

4 THỰC NGHIỆM

4.1 CHI TIẾT ĐÀO TẠO

Chúng tôi đào tạo tất cả các mô hình của chúng tôi trên tập dữ liệu C4 (Raffel et al., 2020), sử dụng tokenizer SentencePiece (Kudo & Richardson, 2018) bao gồm 32000 từ. Chúng tôi sử dụng một mô hình transformer không nhân quả (Vaswani et al., 2017) làm mô hình diffusion của chúng tôi (xem Phụ lục A để biết chi tiết). Các mô hình SED được đào tạo với độ dài chuỗi 256, trong khi đối với các mô hình ablation được đào tạo với độ dài chuỗi 128. Chúng tôi chèn đồng đều, tức là không nhất thiết ở cuối chuỗi, 10% padding token trong tập đào tạo để cho phép các mô hình SED sinh tạo các mẫu có kích thước khác nhau và cung cấp tính linh hoạt hơn.

Để sinh tạo nhúng từ, chúng tôi đào tạo một mô hình BERT có kích thước cố định (~150m tham số) và chiều đặc trưng dmodel = 896. Không gian diffusion được định nghĩa bởi bảng tra cứu ban đầu của mô hình BERT này. Chúng tôi thu hẹp chiều của nhúng từ dembed và thêm một lớp chiếu tuyến tính từ dembed đến dmodel ở đầu mô hình. Chúng tôi thấy điều này giúp diffusion (xem phần 4.4).

Các mô hình SED được đào tạo với lịch trình nhiễu cosine (Dhariwal & Nichol, 2021), với β₁ = 2×10⁻³, σ₀ = 10⁻² và T = 1000. Chúng tôi sử dụng batch 65,536 token, do đó với độ dài chuỗi 256, kích thước batch được đặt thành 256. Chúng tôi sử dụng số span tối đa là 5 cho tất cả các lần chạy trừ ablation cụ thể của nó. Chúng tôi

--- TRANG 6 ---
đào tạo các mô hình SED ở hai quy mô khác nhau: SED-S (135m tham số, 10⁶ bước đào tạo) và SED-L (420m, 2×10⁶ bước). Kiến trúc chi tiết của chúng có thể được tìm thấy trong Phụ lục A.

4.2 VALIDATION

Trong khi tối ưu hóa perplexity của các mô hình AR cho văn bản dẫn đến cải thiện các mô hình ngôn ngữ, việc tối ưu hóa trực tiếp ELBO của các mô hình diffusion cho hình ảnh không tương quan mạnh mẽ với chất lượng mẫu như được quan sát bởi Nichol & Dhariwal (2021); Kingma et al. (2021); Ho & Salimans (2022). Đối với hình ảnh, số liệu dựa trên mẫu FID (Heusel et al., 2017) đã được giới thiệu như một thước đo chất lượng mẫu và hiện được áp dụng rộng rãi. Tương tự, chúng ta cần một số liệu dựa trên mẫu cho sinh tạo văn bản đáng tin cậy và cho phép so sánh giữa nhiều mô hình sinh tạo đa dạng. Để cung cấp so sánh công bằng với các mô hình AR, chúng tôi dựa vào ba số liệu.

Số liệu đầu tiên đo lường mức độ khả năng của các mẫu được tạo ra bởi một mô hình theo một mô hình ngôn ngữ AR với 70B tham số, được đào tạo trên 1.4B token (Hoffmann et al., 2022); chúng tôi ký hiệu số liệu này là AR NLL cho negative log-likelihood tự hồi quy. Nó cung cấp một thước đo liên tục về chất lượng mẫu đã chứng minh hữu ích khi kết hợp với một thước đo đa dạng mẫu, ví dụ trong phát triển nucleus sampling (Holtzman et al., 2020) để cải thiện giải mã mô hình AR.

Để đo lường đa dạng, chúng tôi dựa vào một số liệu thứ hai, entropy unigram của các mẫu, giúp cân bằng AR NLL có thể bị lợi dụng bởi các mẫu lặp lại không tự nhiên. Đối với cả hai số liệu này, mục tiêu của chúng tôi là điểm số của dữ liệu tập validation. Việc lệch khỏi entropy unigram dữ liệu đặc biệt là dấu hiệu của mô hình hóa thoái hóa.

Mặc dù kết hợp ban đầu này đã cung cấp cho chúng tôi một tín hiệu đáng tin cậy để lặp lại thiết kế mô hình của chúng tôi, nó vẫn không hoàn hảo; nó cũng có thể bị lợi dụng, mặc dù khó hơn. Để giải quyết hạn chế này, chúng tôi cũng báo cáo sở thích của con người. Chúng tôi trình bày cho 6 đồng nghiệp 20 cặp mẫu cho mỗi so sánh, yêu cầu họ chọn cái tốt nhất.

Đối với cả ba số liệu, chúng tôi báo cáo kết quả trên hai nhiệm vụ: mô hình hóa ngôn ngữ không điều kiện và infilling hậu tố, sau này là một nhiệm vụ được điều kiện mạnh mẽ.

4.3 KẾT QUẢ

Hình 1: So sánh chất lượng mẫu và đa dạng của SED so với các mô hình AR trên infilling hậu tố. SED sử dụng guidance với thang đo trong {1, 2, 4, 8} và AR sử dụng nucleus sampling với top-p trong {1.00, 0.95, 0.90, 0.85}. Các điểm phía trên-phải là các mô hình SED với thang guidance là 1 hoặc các mô hình AR với top-p là 1.

Mẫu. Chúng tôi trình bày các mẫu được sinh tạo với các mô hình SED của chúng tôi trong Bảng 10. Chúng tôi sử dụng một mô hình duy nhất để thực hiện nhiều nhiệm vụ sinh tạo văn bản đa dạng, như sinh tạo không điều kiện, filling-in-the-middle hoặc điền nhiều span văn bản. Chúng tôi thể hiện hiệu suất mạnh mẽ trong trường hợp không điều kiện, với các mẫu đúng về cú pháp và duy trì sự nhất quán trên các chuỗi dài. Trong trường hợp có điều kiện, các mô hình SED có thể infill các span với các chuyển tiếp nhất quán và liên kết đến điều kiện nhưng cũng thể hiện sự đa dạng phong phú. Theo thiết kế, SED tạo ra các mô hình masking hai chiều linh hoạt có thể thực hiện sinh tạo văn bản trên một tập hợp đa dạng nhiệm vụ có điều kiện. Để so sánh SED với các baseline AR, chúng tôi tiếp theo hạn chế điều kiện thành một tiền tố và xem xét một nhiệm vụ infilling hậu tố.

So sánh với các mô hình AR. Để đánh giá khả năng sinh tạo của SED, chúng tôi so sánh với các baseline AR có dung lượng tương tự và được đào tạo theo quy luật mở rộng tối ưu từ Hoffmann et al. (2022) trên infilling hậu tố. Chúng tôi lấy mẫu một batch các chuỗi từ C4 và sử dụng 128 token đầu tiên làm điều kiện đưa cho mô hình để sinh tạo một hậu tố 128 token. Hình 1 báo cáo AR NLL và entropy unigram của các hậu tố được sinh tạo cho các mô hình AR và SED. Như một điểm tham chiếu, chúng tôi tính toán AR NLL và entropy unigram của các hậu tố C4 thực tế và báo cáo nó trên biểu đồ. Một số phương pháp có thể được sử dụng để cải thiện chất lượng lấy mẫu với chi phí đa dạng mẫu; chúng tôi sử dụng nucleus sampling (Holtzman et al., 2020) cho các mô hình AR và guidance (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) cho các mô hình SED. Chúng tôi thể hiện tác động của guidance lên chất lượng mẫu trong Bảng 3. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên thể hiện cải thiện chất lượng mẫu khi sử dụng guidance cho sinh tạo văn bản.

Như được thể hiện trong Hình 1, cả SED-S và SED-L đều hoạt động mạnh mẽ khi so sánh với các baseline AR - mặc dù chúng tôi báo cáo một số liệu ủng hộ các mô hình AR trên một nhiệm vụ mà các mô hình AR đã được thiết kế để tối ưu hóa. Tương tự như nucleus sampling cho các mô hình AR, guidance có tác động tích cực mạnh mẽ lên chất lượng mẫu được quan sát cả về định lượng với AR NLL cải thiện trong Hình 1 và định tính trong Bảng 3. Chúng tôi quan sát rằng sử dụng nucleus sampling top-p dưới 0.8 cho các mô hình AR hoặc thang guidance trên 4 cho các mô hình SED dẫn đến các mẫu thể hiện nhiều lặp lại, một trường hợp thoái hóa được phản ánh bởi entropy thấp hơn của các mẫu mặc dù AR NLL mẫu cải thiện.

Điểm sở thích con người của chúng tôi làm dịu các quan sát trong Bảng 4. Chúng thể hiện rằng các số liệu NLL và entropy của chúng tôi không kể hết câu chuyện, vì con người vẫn thích các mô hình AR ở kích thước tương đương. Trong khi SED-L hoạt động hơi tệ hơn AR-L (38% sở thích trong infilling hậu tố, 44% trong sinh tạo không điều kiện), điểm số của nó vẫn có thể so sánh. SED-L gần như ngang bằng với AR-S.

Cuối cùng, chúng tôi so sánh các ví dụ định tính của SED và các mô hình AR với các prompt ngắn trong Bảng 2.

--- TRANG 7 ---
Bảng 2: Chúng tôi so sánh các mẫu SED-L (thang guidance 2.5) và AR-L (nucleus sampling, p = 0.95).

SED-L | AR-L
---|---
Bạn sẽ thích mặc chiếc áo phông truyền thống này từ bộ sưu tập Wilson mới nhất của chúng tôi. Được thiết kế với phong cách hoa văn cào xước với tay áo có các đường bất đối xứng qua đế tròn. Hơi xù, các nếp gấp vuông sẽ đưa bạn ngay đến | Koda Ram 25 được trình bày dưới dạng sedan và saloon Maxima. Dựa trên xe Acenta, chiếc Koda 2014 mạnh mẽ xuất hiện trên đường Ấn Độ với hình dáng thân xe "Maxima". Được cung cấp bởi động cơ diesel Hyundai i20 1.4 lít, Koda 2014 được kết nối
Hải ly là một động vật thú vị sống ở sông và hồ. Nó không chủ yếu quan tâm đến việc tìm sói và cá heo mà còn có cơn đói lớn đối với cá. Hải ly có chân sắc nhọn, mắt lớn, và bộ lông đen | Hải ly là một động vật thú vị sống ở sông và suối. Nó thường được thấy với số lượng lớn ở cánh đồng hoặc thượng lưu, và khá dễ bảo. Vào những ngày lạnh khi mẫu hình của nó hoàn hảo, hải ly sẽ có một số thú vị, và đôi khi
Ngày xưa ở Tây Ban Nha, Leonardo Pueleva có niềm vui được gặp gỡ khách tại nhà hàng Tây Ban Nha, Buva Casinos. Trong khi lái xe, anh ấy có cơ hội làm quen với những người đằng sau nhà hàng và, tất nhiên, cách họ làm cho trải nghiệm của anh ấy rất thú vị. Sau cuộc trò chuyện, anh ấy đã | Ngày xưa ở Tây Ban Nha, dường như khá giống bây giờ, câu hỏi được đặt ra cho mỗi người chúng tôi vào cuối cuộc phỏng vấn là "bạn có muốn thấy Froome một ngày nào đó không?" Nhìn lại, sau cuộc phỏng vấn, chúng tôi đã trở nên gần gũi hơn với câu trả lời đó. Như bạn sẽ đọc trong bài viết, tôi biết rằng

4.4 ABLATIONS

Tự điều kiện và pretraining nhúng. Kết quả từ Bảng 5 và các mẫu từ Bảng 6 thể hiện ảnh hưởng của cả không gian diffusion và tự điều kiện. AR NLL giảm rất đáng kể khi sử dụng tự điều kiện, bất kể phần còn lại của thiết lập. Diffusion ở cấp bit (Chen et al., 2022) tạo ra NLL rất cao. Trong khi sử dụng nhúng ngẫu nhiên hoạt động tốt hơn đáng kể, sử dụng nhúng pretrained dẫn đến các con số cải thiện hơn nữa.

Các mẫu từ Bảng 6 làm nổi bật rằng các mô hình được đào tạo trên nhúng từ ngẫu nhiên thể hiện khả năng mô hình hóa chủ đề với sự đồng xuất hiện của các từ như child và mother mặc dù đoạn văn vẫn không nhất quán toàn cầu và các token vô nghĩa như gluc được sinh ra. Tự điều kiện cải thiện đáng kể chất lượng mẫu; mô hình diffusion hiểu đúng cấu trúc mức thấp và sinh ra các câu đúng cú pháp, mặc dù văn bản toàn cầu không thể hiểu được. Kết hợp tự điều kiện và nhúng pretrained dẫn đến các đoạn văn nhất quán toàn cầu duy trì chủ đề với cấu trúc câu phù hợp.

--- TRANG 8 ---
Bảng 3: Tác động của guidance lên chất lượng mẫu sử dụng mô hình SED của chúng tôi.

Guidance 1.0 | 2.5 | 5.0
---|---|---
Trong bầu trời đêm lạnh lẽo, một công chúa tiên ngồi trên ghế và được bao quanh bởi lá trà trong ao. Trong khi đó, cô ấy ngã trở lại vào cái lạnh, với mái tóc xanh trên hông và khuỷu tay trên trán - và ngón tay bị tê cứng bởi nhiệt độ đóng băng. | Trong đêm lạnh lẽo của tháng 11 năm 2018, một cô bé ngồi trên ghế ẩn dưới tấm chăn nhẹ trên hiên. Trong khi đó, cô ấy cúi xuống ghế với mái tóc xanh trên trán, tay trên mặt, ngón tay bị tê cứng bởi nhiệt độ đóng băng. | Trong đêm lạnh lẽo của tháng 12, con gái lớn nhất của tôi ngồi trên ghế được nhấn mạnh bằng vải cotton và gối. Trong khi đó, cô ấy khao khát thẳng vào không khí lạnh, cổ tay che cổ, mắt nhìn thẳng vào trán, và ngón tay bị tê cứng bởi nhiệt độ đóng băng.
Barbara là một trong những người phụ nữ tuyệt vời của chúng tôi thực sự giúp đỡ nên tôi rất bị thổi bay bởi mục đích, sự lịch sự; và nghịch cảnh của cô ấy. Khi tôi bắt đầu tương tác với cô ấy, nó chứng minh với tôi rằng dù khó khăn thế nào, cô ấy luôn phấn đấu cho sự xuất sắc. | Barbara là một trong những người phụ nữ tài năng nhất trên thế giới. Cô ấy sáng tạo và đứng lên bằng sự chính trực và lịch sự; chống lại nghịch cảnh. Mặc dù cô ấy đặt mình cao hơn các đồng nghiệp, nó chứng minh với tôi rằng dù khó khăn thế nào, cô ấy luôn phấn đấu cho sự xuất sắc. | Barbara là một trong những người phụ nữ tài năng nhất trên thế giới. Cô ấy tuyệt vời trong trái tim, tinh thần, tâm trí và trong tâm hồn. Cô ấy không bao giờ làm mọi người khó chịu trong sự vắng mặt. Nó chứng minh với tôi rằng dù khó khăn thế nào, cô ấy luôn phấn đấu cho sự xuất sắc.

Bảng 4: Điểm sở thích con người của SED-L so với các mô hình khác trên các nhiệm vụ có điều kiện và không điều kiện.

SED-S (cond) | AR-S (cond) | AR-L (uncond) | AR-L (cond)
---|---|---|---
SED-L | 63.4%±4.3% | 51.0%±5.0% | 43.8%±4.4% | 37.7%±4.4%

Chiều nhúng. Một lựa chọn thiết kế quan trọng cho SED là không gian nhúng từ. Chúng tôi nghiên cứu ảnh hưởng của kích thước nhúng pretrained trong Bảng 7. Ngạc nhiên, có một ngưỡng sau đó hiệu suất giảm khi tăng chiều của nhúng. Chúng tôi trực quan hóa quá trình thuận bằng cách hiển thị nearest neighbor của một token nhiễu trong khi chạy quá trình thuận. Ở chiều cao, chúng tôi quan sát rằng nearest neighbor của một token nhiễu vẫn là chính token bắt đầu cho đến khi nó chuyển sang một token hoàn toàn ngẫu nhiên, không liên quan. Ở chiều thấp, chúng tôi thường quan sát rằng nearest neighbor của một token nhiễu đi qua một số token liên quan về mặt ngữ nghĩa (nearest neighbor của token bắt đầu) trước khi cuối cùng trở thành ngẫu nhiên. Chúng tôi giả thuyết rằng các random walk được định nghĩa bởi diffusion có nhiều khả năng trôi về phía các neighbor của token bắt đầu ở chiều thấp. Kết quả là, khi diffusion ở chiều thấp, thông tin bị phá hủy theo cách có ý nghĩa về mặt ngữ nghĩa hơn, dẫn đến một bài toán học dễ hơn cho hàm denoising.

Số lượng span. Để cho phép infilling, chúng tôi đào tạo mô hình không chỉ để sinh tạo không điều kiện mà còn để điền có điều kiện các span token. Đối với mỗi điểm dữ liệu, chúng tôi lấy mẫu một số span ngẫu nhiên đồng đều và các dấu phân cách span để tạo ra mặt nạ span. Việc chọn số span tối đa cho phép có tác động đáng kể lên hiệu suất mô hình, như chúng ta có thể thấy trong Bảng 8. Có phần phản trực giác, việc thêm che span cải thiện thậm chí NLL sinh tạo không điều kiện. Nó cũng có vẻ như sử dụng một số span tối đa tương đối cao là tối ưu. Chúng tôi giả thuyết rằng điều này dẫn đến một hỗn hợp đa dạng về độ khó nhiệm vụ tại thời điểm đào tạo, giữa các vấn đề "dễ", rất có điều kiện một mặt và "khó hơn", không điều kiện mặt khác.

Mở rộng. Chúng tôi thể hiện kết quả khuyến khích khi mở rộng từ SED-S (150m) đến SED-L (420m). Chúng tôi đào tạo cả hai mô hình trên các chuỗi 256 token và báo cáo AR NLL là 4.20 cho SED-S so với 3.68 cho SED-L. Cải thiện này chuyển thành chất lượng mẫu cải thiện, như được xác nhận bởi điểm sở thích con người của chúng tôi, cao hơn nhiều cho mô hình lớn hơn (63%, xem Bảng 4).

--- TRANG 9 ---
Bảng 5: Ablation của phương pháp SED được đề xuất trên sinh tạo không điều kiện. Cả tự điều kiện và pretraining nhúng đều đóng vai trò chính trong hiệu suất mô hình.

Không gian diffusion | Tự điều kiện | Unigram entropy | AR NLL
---|---|---|---
Bit (Chen et al., 2022) | ✗ | 6.97 | 7.01
 | ✓ | 7.47 | 6.05
Nhúng ngẫu nhiên | ✗ | 6.90 | 6.80
 | ✓ | 6.86 | 5.31
Nhúng pretrained | ✗ | 6.75 | 5.66
 | ✓ | 6.77 | 4.57
Dữ liệu | – | 6.70±0.04 | 1.81±0.15

Bảng 6: Về sinh tạo không điều kiện, tự điều kiện dẫn đến mô hình hóa câu tốt hơn, nhúng pretrained tăng cường mô hình hóa chủ đề.

Nhúng ngẫu nhiên | Nhúng ngẫu nhiên với tự điều kiện | Nhúng pretrained với tự điều kiện
---|---|---
did the building room granted a lighter distance. On it though, salaries about clients that a child, which dispersed gluc so many events and certainly wanted the Mother's project, discovered by their child would keep | Tree brings the sound, bearing features and capabilities that we are set in. For the first time, she uses a customizable framework to use that we help students publicly solve the weather conditions that we only offer students | almost six decades ago-72 percent of Americans didn't feel they'd actually rent their own cars this year. Conversely, 90 percent of Americans feel that the decision to rent a car is something they feel it's impossible

5 HẠN CHẾ

Trong khi kết quả của chúng tôi đầy hứa hẹn và cho thấy rằng diffusion liên tục cho văn bản có thể là một thay thế thú vị cho các mô hình AR, phương pháp hiện tại có một số hạn chế đáng kể.

Đầu tiên, có thể làm nhiều hơn về điều chỉnh mô hình, bao gồm mở rộng đến các mô hình lớn hơn nhiều để hiểu rõ hơn các giới hạn của SED, và để có thể so sánh nó với các mô hình AR tiên tiến. Chế độ đào tạo của chúng tôi đặc biệt chắc chắn sẽ hưởng lợi từ tối ưu hóa siêu tham số nhiều hơn.

Thứ hai, một lý do thuyết phục khiến chúng tôi chọn khám phá diffusion liên tục cho văn bản là để tận dụng các cải tiến được tạo ra bởi văn học về sinh tạo hình ảnh. Trong khi chúng tôi đã chuyển một số (ví dụ: tự điều kiện), còn rất nhiều điều chưa được khám phá. Ví dụ rõ ràng nhất là chính quá trình lấy mẫu, nơi số bước cần thiết đã được giảm đáng kể cho hình ảnh (ví dụ: Karras et al. (2022) từ 1000 xuống 35, và Salimans & Ho (2022) xuống 4 trên hình ảnh đơn giản). Lấy mẫu hiện tại của chúng tôi rất không hiệu quả, và hướng này là một trong những cải tiến đầu tiên cần thực hiện trên SED.

Thứ ba, SED dựa chủ yếu vào việc diffusion trong không gian nhúng pretrained. Điều này có nghĩa là dựa vào một mô hình thứ hai, và sử dụng các nhúng có thể không tối ưu cho diffusion. Lý tưởng nhất, chúng tôi sẽ đào tạo mô hình đầy đủ end-to-end, có thể mang lại kết quả tốt hơn. Trong khi Li et al. (2022) thấy một số thành công với phương pháp này, đó là trong một thiết lập cụ thể ở quy mô nhỏ; trong thực tế, chúng tôi thấy khó tránh cạnh tranh giữa tổn thất diffusion và tái tạo.

Cuối cùng, công trình của chúng tôi sẽ hưởng lợi từ các số liệu cải thiện trong phần thực nghiệm. Bởi vì trình độ hiện tại bao gồm các mô hình AR, lĩnh vực này thiếu các benchmark đã thiết lập cho các nhiệm vụ mà các mô hình diffusion có thể phù hợp hơn, chẳng hạn như infilling văn bản. Chúng tôi đã chọn một hỗn hợp hợp lý, đánh giá negative log-likelihood của các mẫu được sinh tạo theo một mô hình AR rất mạnh cũng như entropy token của chúng và bổ sung nó bằng đánh giá con người. Tuy nhiên, cả NLL và entropy unigram đều có thể bị lợi dụng (ví dụ: các mô hình AR gán NLL rất thấp cho các đoạn lặp lại, và các lặp lại đủ dài có thể đánh lừa cả entropy). Hơn nữa, NLL của chúng tôi vốn gắn liền với mô hình AR của nó và do đó có thể cung cấp lợi thế không công bằng cho các mô hình AR. Nhìn chung, chúng tôi vẫn thấy cả hai số liệu khá hữu ích để đo lường tiến độ nghiên cứu, và đánh giá con người của chúng tôi đã xác nhận kết quả. Tiến về phía trước, việc định nghĩa một benchmark infilling sạch sẽ giúp tạo ra kết quả thuyết phục hơn nữa.

--- TRANG 10 ---
Bảng 7: Nhúng từ với chiều nhỏ có khả năng AR cao hơn.

Chiều nhúng | 16 | 32 | 64 | 128 | 256 | 896
---|---|---|---|---|---|---
AR NLL | 4.65 | 4.57 | 4.71 | 4.61 | 4.77 | 4.92

Bảng 8: Các nhiệm vụ che span cải thiện sinh tạo văn bản không điều kiện.

Số span tối đa | 1 | 3 | 5 | 7 | 9 | 11
---|---|---|---|---|---|---
AR NLL | 4.99 | 4.82 | 4.82 | 4.67 | 4.48 | 4.56

6 KẾT LUẬN

Chúng tôi đề xuất SED, mô hình diffusion liên tục có khả năng tổng quát đầu tiên cho sinh tạo văn bản. Các mô hình SED có thể thực hiện cả sinh tạo có điều kiện và không điều kiện, và hiệu suất của chúng cạnh tranh với các mô hình AR trong khi linh hoạt hơn trong việc sử dụng (ví dụ: cho phép infilling). Chúng tôi chứng minh hiệu suất của chúng và nghiên cứu tác động của các lựa chọn thiết kế chính.

Mặc dù có những hạn chế, công trình này đặt nền móng cho nghiên cứu thú vị hơn. Các hướng đầy hứa hẹn bao gồm tăng tốc lấy mẫu theo các bài học đã học trong lĩnh vực hình ảnh, thiết kế các không gian nhúng tốt hơn cho diffusion và điều tra các khả năng infilling mới.

TÀI LIỆU THAM KHẢO

Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, và Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Trong NeurIPS, trang 17981–17993, 2021.

Yoshua Bengio, Réjean Ducharme, và Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, và Laurent Charlin. Language gans falling short. arXiv preprint arXiv:1811.02549, 2018.

Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, và William T Freeman. Maskgit: Masked generative image transformer. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 11315–11325, 2022.

Ting Chen, Ruixiang Zhang, và Geoffrey E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. CoRR, abs/2208.04202, 2022.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, và Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019. URL https://arxiv.org/abs/1901.02860.

Cyprien De Masson d'Autume, Shakir Mohamed, Mihaela Rosca, và Jack Rae. Training language gans from scratch. Advances in Neural Information Processing Systems, 32, 2019.

Prafulla Dhariwal và Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. Trong NeurIPS, trang 8780–8794, 2021.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, và Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. Trong EMNLP/IJCNLP (1), trang 6111–6120. Association for Computational Linguistics, 2019.

Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, và Baining Guo. Vector quantized diffusion model for text-to-image synthesis. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 10696–10706, 2022.

Dan Hendrycks và Kevin Gimpel. Gaussian error linear units (gelus), 2016. URL https://arxiv.org/abs/1606.08415.

--- TRANG 11 ---
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, và Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Trong NIPS, trang 6626–6637, 2017.

Jonathan Ho và Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022.

Jonathan Ho, Ajay Jain, và Pieter Abbeel. Denoising diffusion probabilistic models. Trong NeurIPS, 2020.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text degeneration. Trong ICLR. OpenReview.net, 2020.

Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, và Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454–12465, 2021.

Tero Karras, Miika Aittala, Timo Aila, và Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022.

Diederik P. Kingma, Tim Salimans, Ben Poole, và Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021.

Taku Kudo và John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Trong EMNLP (Demonstration), trang 66–71. Association for Computational Linguistics, 2018.

Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, và Tatsunori B. Hashimoto. Diffusion-lm improves controllable text generation. CoRR, abs/2205.14217, 2022.

Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, và Ian Simon. Symbolic music generation with diffusion models. Trong ISMIR, trang 468–475, 2021.

Alexander Quinn Nichol và Prafulla Dhariwal. Improved denoising diffusion probabilistic models. Trong ICML, tập 139 của Proceedings of Machine Learning Research, trang 8162–8171. PMLR, 2021.

Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, và Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. Trong ICML, tập 162 của Proceedings of Machine Learning Research, trang 16784–16804. PMLR, 2022.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, và Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021.

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, và Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487, 2022.

Tim Salimans và Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. URL https://arxiv.org/abs/2202.00512.

Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, và Aäron van den Oord. Step-unrolled denoising autoencoders for text generation. Trong ICLR. OpenReview.net, 2022.

--- TRANG 12 ---
Stanislau Semeniuta, Aliaksei Severyn, và Sylvain Gelly. On accurate evaluation of gans for language generation. arXiv preprint arXiv:1806.04936, 2018.

Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, và Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. Trong ICML, tập 37 của JMLR Workshop and Conference Proceedings, trang 2256–2265. JMLR.org, 2015.

Ilya Sutskever, James Martens, và Geoffrey E Hinton. Generating text with recurrent neural networks. Trong ICML, 2011.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

A KIẾN TRÚC MÔ HÌNH

Đối với cả mô hình AR và SED, chúng tôi sử dụng cùng kiến trúc transformer (Vaswani et al., 2017), tương tự như những mô hình được mô tả trong (Hoffmann et al., 2022), với mã hóa vị trí tương đối như được mô tả trong (Dai et al., 2019) trong các khối attention, và với mở rộng 4x và phi tuyến Gelu (Hendrycks & Gimpel, 2016) trong các khối feed-forward. Các siêu tham số kiến trúc được chi tiết trong bảng 9. Nhúng từ nhiễu, x ∈ ℝᴺˣᴰ, trước tiên được truyền qua một chiếu tuyến tính hoạt động trên mỗi nhúng độc lập để có được nhúng được chiếu có chiều đặc trưng khớp với chiều rộng của transformer, dmodel. Tại bước diffusion t, chúng tôi tính toán nhúng thời gian như nhúng vị trí sinusoidal (Vaswani et al., 2017) có kích thước dmodel, sau đó được truyền vào một lớp tuyến tính dmodel × dmodel và được thêm vào nhúng được chiếu. Chúng tôi thêm một lớp chiếu đầu ra tuyến tính E′ lấy đầu ra của transformer y ∈ ℝᴺˣᵈᵐᵒᵈᵉˡ và chiếu mỗi phần tử (yi)1≤i≤N trở lại cùng kích thước như nhúng từ. Khi sử dụng tự điều kiện, chúng tôi sửa đổi đầu vào cho mô hình bằng cách nối x và x̂₀ theo trục đặc trưng trước khi truyền chúng đến lớp chiếu đầu vào.

Bảng 9: Siêu tham số mô hình.

Mô hình | số lớp | dmodel | số head | kích thước head
---|---|---|---|---
S | 12 | 896 | 16 | 64
L | 12 | 1536 | 16 | 128

B TRỰC QUAN HÓA QUÁ TRÌNH DIFFUSION THUẬN

Để hỗ trợ thảo luận về chiều nhúng từ từ Phần 4.4, chúng tôi trình bày một trực quan hóa của quá trình diffusion thuận. Cho các token bắt đầu x₀, chúng tôi chiếu các token nhiễu xt của quá trình thuận tại bước t đến nearest neighbor của chúng trong số các nhúng từ E để có được wt. Sau đó chúng tôi lưu trữ 128 nearest neighbor N(w₀) của các token bắt đầu w₀ = x₀ và định nghĩa rank rt của wt tại chỉ số của nó trong N(w₀). Chúng tôi hiển thị wt và làm nổi bật nó bằng màu xanh lá cây nếu rt gần với không (có nghĩa là wt là một neighbor gần của w₀) và màu đỏ tăng dần nếu ngược lại. Chúng tôi trình bày 16 nearest neighbor đầu tiên của w₀ trong Hình 2 và cung cấp một minh họa về mã màu được sử dụng để làm nổi bật. Hình 3 thể hiện một thể hiện của quá trình diffusion thuận trong khi diffusion trên nhúng với chiều cao 896 và Hình 4 thể hiện diffusion trên nhúng với chiều thấp hơn 32.

Chúng tôi quan sát rằng ở chiều cao, nearest neighbor của token nhiễu vẫn là token gốc cho đến điểm mà bất kỳ token nào cũng có thể là nearest neighbor của nó. Random walk diffusion dường như không đi qua các khu vực neighbor của các token liên quan về mặt ngữ nghĩa. Chúng tôi giả thuyết rằng nguyên nhân gốc của vấn đề này là không gian nhúng chủ yếu rỗng (chỉ với 32000 điểm trong ℝ⁸⁹⁶, vì dembed = 896); và các nhúng có thể tập trung trong một không gian chiều thấp hơn.

Ngược lại, ở chiều thấp hơn, chúng ta thấy các token có ý nghĩa liên quan xuất hiện khi corruption tiến triển ('brown' trở thành 'grey', 'quick' trở thành 'swift', 'over' trở thành 'underneath' v.v.). Chúng tôi tin rằng việc phá hủy thông tin dần dần hơn này có lợi cho mô hình diffusion.

--- TRANG 13 ---
Hình 2: Nearest neighbor của các token từ câu "the quick brown fox jumps over the lazy dog"

Hình 3: Trực quan hóa quá trình diffusion thuận lên đến 300 bước khi diffusion trên nhúng với chiều 896.

C CÁC MẪU BỔ SUNG

--- TRANG 14 ---
Hình 4: Trực quan hóa quá trình diffusion thuận lên đến 300 bước khi diffusion trên nhúng với chiều 32.

Hình 5: Quá trình diffusion ngược của SED-L với thang guidance 2.5.

--- TRANG 15 ---
Bảng 10: Các mẫu từ SED-L.

Khóa học này rất cần thiết cho môi trường mà sinh viên chọn lựa chọn chương trình học tại JHD. Tích hợp Địa lý là trung tâm của bộ phận này. Cùng với những đổi mới cơ sở hạ tầng đô thị của giữa những năm 1990 và các vấn đề xã hội trong thế kỷ 21. Mọi người trong bộ phận thường xuyên thể hiện khái niệm tích hợp thực sự.

Điều đó đã kích hoạt một số từ ngữ hoặc hiểu biết có ý nghĩa để bắt đầu chuẩn bị cho con em tương lai của họ. May mắn thay, Mikaela đã đủ tốt bụng để chịu đựng các câu hỏi của tôi về những gì cha mẹ có thể làm để giúp đỡ, ngay cả trong kỳ nghỉ hè của cô ấy.

Tôi học đại học tại Đại học Boston. Sau khi lấy bằng, tôi quyết định thay đổi! Tôi đăng ký các trường ngoài trời. Sau khi lấy bằng, tôi thích lướt jet ski, câu cá ngoài khơi, và lướt ván diều. Danh sách tiếp tục phát triển. Quan trọng hơn, tôi bắt đầu sự nghiệp bằng cách câu cá trên biển. Bây giờ, tôi không thể có đủ Thái Bình Dương!

Hải ly là một động vật thú vị sống ở sông và thác nước khắp Puerto Rico. Nó thích chèo thuyền kayak, câu cá, và bơi lội. Nhưng, bạn muốn biết những động vật nào đằng sau con hải ly?

Một năm trước tại Paris, tôi có cơ hội tham gia một chuyến thực tế đến LaRite-en-Laurences International deFrance nơi tôi gặp David Nigel Johnson, một giáo sư xã hội học. Thật là một chuyến đi tuyệt vời và thật là một ngày tuyệt vời!

Một năm trước tại Paris, bạn bè tôi và tôi đi du lịch đường bụi đến thành phố. Tôi nhớ đã đi qua nhà thờ, quảng trường đẹp, những con đường hẹp được thắp sáng bởi các đài tưởng niệm và đi qua một Giám mục Công giáo khủng khiếp mà tôi đã quen - thật là một ngày buồn...

Không có bằng chứng, chỉ có những cái nhìn thoáng qua về sự tồn tại của khuyết tật nhận thức. Không có may mắn và không có khoa học vững chắc. Tất cả chỉ là phỏng đoán, viễn cảnh mù mịt của viêm phổi có thể thúc đẩy trí tưởng tượng về khả năng tổn thương não.
