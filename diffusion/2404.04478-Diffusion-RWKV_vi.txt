# Diffusion-RWKV:
Mở rộng quy mô các kiến trúc giống RWKV cho các mô hình khuếch tán

Zhengcong Fei, Mingyuan Fan, Changqian Yu
Debang Li, Junshi Huang*
Kunlun Inc.
feizhengcong@gmail.com

Hình 1. Các mô hình khuếch tán với xương sống giống RWKV đạt được chất lượng hình ảnh tương đương. Các mẫu được chọn do Diffusion-RWKV có điều kiện lớp được huấn luyện trên ImageNet với độ phân giải 256×256 và 512×512 tương ứng.

Tóm tắt
Các Transformer đã thúc đẩy những tiến bộ trong lĩnh vực thị giác máy tính và xử lý ngôn ngữ tự nhiên (NLP). Tuy nhiên, độ phức tạp tính toán đáng kể đặt ra những hạn chế đối với việc áp dụng chúng trong các nhiệm vụ ngữ cảnh dài, chẳng hạn như tạo hình ảnh độ phân giải cao. Bài báo này giới thiệu một loạt các kiến trúc được điều chỉnh từ mô hình RWKV được sử dụng trong NLP, với các sửa đổi cần thiết phù hợp cho mô hình khuếch tán áp dụng vào các nhiệm vụ tạo hình ảnh, được gọi là Diffusion-RWKV. Tương tự như khuếch tán với Transformer, mô hình của chúng tôi được thiết kế để xử lý hiệu quả các đầu vào được chia thành patch trong một chuỗi với các điều kiện bổ sung, đồng thời cũng mở rộng quy mô hiệu quả, phù hợp với cả các tham số quy mô lớn và các tập dữ liệu rộng lớn. Ưu điểm đặc biệt của nó thể hiện ở độ phức tạp tổng hợp không gian giảm, khiến nó đặc biệt thành thạo trong việc xử lý hình ảnh độ phân giải cao, do đó loại bỏ sự cần thiết cho các hoạt động cửa sổ hoặc nhóm được lưu cache. Kết quả thực nghiệm trên cả các nhiệm vụ tạo hình ảnh có điều kiện và không có điều kiện chứng minh rằng Diffusion-RWKV đạt được hiệu suất ngang bằng hoặc vượt trội so với các mô hình khuếch tán dựa trên CNN hoặc Transformer hiện có trong các chỉ số FID và IS trong khi giảm đáng kể tổng sử dụng FLOP tính toán.

1. Giới thiệu
Các Transformer [10, 25, 44, 53, 64, 81], đã nổi bật do bản chất có thể thích ứng và khả năng xử lý thông tin thành thạo, đã thiết lập các tiêu chuẩn mới trên nhiều lĩnh vực bao gồm thị giác máy tính và NLP. Đáng chú ý, chúng đã thể hiện hiệu suất đặc biệt trong các nhiệm vụ như tạo hình ảnh [4, 8, 9, 14, 42, 57, 58, 65]. Tuy nhiên, hoạt động tự chú ý trong Transformer thể hiện độ phức tạp tính toán bậc hai, do đó hạn chế hiệu quả của chúng trong việc xử lý các chuỗi dài và đặt ra một trở ngại đáng kể cho việc áp dụng rộng rãi [14, 39, 80, 87, 89]. Do đó, có nhu cầu cấp thiết để khám phá các kiến trúc có thể khai thác hiệu quả tính linh hoạt và khả năng xử lý mạnh mẽ của chúng đồng thời giảm thiểu yêu cầu tính toán. Điều này trở nên quan trọng hơn nữa trong bối cảnh tổng hợp hình ảnh độ phân giải cao hoặc tạo video dài.

Trong các phát triển gần đây, các mô hình như RWKV [59] và Mamba [21], đã nổi lên như các giải pháp phổ biến để tăng cường hiệu quả và xử lý dữ liệu văn bản dài với khả năng tương đương. Các mô hình đổi mới này thể hiện các đặc điểm tương tự như transformer [3, 6, 12, 15, 43, 45, 63, 64, 73, 78, 85], bao gồm việc xử lý các phụ thuộc tầm xa và xử lý song song. Hơn nữa, chúng đã thể hiện khả năng mở rộng, hoạt động đáng ngưỡng mộ với các tập dữ liệu NLP và CV quy mô lớn [18, 90]. Tuy nhiên, do sự khác biệt đáng kể giữa các lĩnh vực dữ liệu hình ảnh và văn bản, vẫn còn thách thức để hình dung việc thay thế hoàn toàn Transformer bằng các phương pháp dựa trên RWKV cho các nhiệm vụ tạo thị giác [11]. Điều này trở nên bắt buộc để tiến hành phân tích sâu về cách các mô hình này được áp dụng vào các nhiệm vụ tạo hình ảnh. Phân tích này nên điều tra khả năng mở rộng của chúng về mặt dữ liệu huấn luyện và tham số mô hình, đánh giá hiệu quả của chúng trong việc xử lý dữ liệu hình ảnh tuần tự, và xác định các kỹ thuật cần thiết để đảm bảo tính ổn định của mô hình trong quá trình mở rộng.

Bài báo này giới thiệu Diffusion-RWKV, được thiết kế để điều chỉnh kiến trúc RWKV trong các mô hình khuếch tán cho các nhiệm vụ tạo hình ảnh. Việc điều chỉnh được đề xuất nhằm duy trì cấu trúc cơ bản và lợi thế của RWKV [59] đồng thời tích hợp các sửa đổi quan trọng để điều chỉnh nó đặc biệt cho việc tổng hợp dữ liệu hình ảnh. Cụ thể, chúng tôi sử dụng Bi-RWKV [11] cho xương sống, điều này cho phép tính toán trong độ phức tạp tính toán tuyến tính ở dạng RNN tiến và lùi. Chúng tôi chủ yếu thực hiện các lựa chọn kiến trúc trong các mô hình khuếch tán, bao gồm tích hợp điều kiện, kết nối bỏ qua, và cuối cùng cung cấp các baseline thực nghiệm giúp tăng cường khả năng của mô hình đồng thời đảm bảo tính mở rộng và ổn định. Dựa trên thiết kế nói trên, một bộ đa dạng các mô hình Diffusion-RWKV được phát triển, như một phạm vi rộng các quy mô mô hình, từ nhỏ đến lớn. Các mô hình này được huấn luyện trên CIFAR-10, Celebrity đến ImageNet-1K sử dụng huấn luyện không có điều kiện và có điều kiện lớp ở các độ phân giải hình ảnh khác nhau. Hơn nữa, đánh giá hiệu suất được tiến hành trong cả không gian thô và tiềm ẩn. Đáng khích lệ, dưới cùng các cài đặt, Diffusion-RWKV có hiệu suất tương đương với đối thủ cạnh tranh DiT [58] trong tạo hình ảnh, với chi phí tính toán thấp hơn trong khi duy trì tính mở rộng ổn định. Thành tựu này cho phép huấn luyện song song Diff-RWKV, tính linh hoạt cao, hiệu suất xuất sắc và chi phí suy luận thấp đồng thời, làm cho nó trở thành một thay thế đầy hứa hẹn trong tổng hợp hình ảnh. Đóng góp có thể được tóm tắt như sau:

• Trong một nỗ lực tiên phong, chúng tôi đi sâu vào khám phá một mô hình khuếch tán hoàn toàn dựa trên RWKV cho các nhiệm vụ tạo hình ảnh, định vị như một thay thế chi phí thấp cho Transformer. Mô hình của chúng tôi không chỉ kế thừa những ưu điểm của RWKV để nắm bắt phụ thuộc tầm xa, mà còn giảm độ phức tạp xuống mức tuyến tính.

• Để đáp ứng nhu cầu của tổng hợp hình ảnh, chúng tôi đã tiến hành một cuộc điều tra toàn diện và có hệ thống về các mô hình Diffusion-RWKV bằng cách khám phá các lựa chọn cấu hình khác nhau liên quan đến điều kiện, thiết kế khối, và mở rộng tham số mô hình.

• Kết quả thực nghiệm chỉ ra rằng Diffusion-RWKV hoạt động tương đương với các benchmark được thiết lập tốt DiT và U-ViT, thể hiện FLOP thấp hơn và tốc độ xử lý nhanh hơn khi độ phân giải tăng. Đáng chú ý, Diffusion-RWKV đạt được điểm FID 2.95 chỉ được huấn luyện trên ImageNet-1k. Mã và mô hình có sẵn tại https://github.com/feizc/Diffusion-RWKV.

2. Phương pháp
Phần này bắt đầu bằng việc cung cấp một cái nhìn tổng quan về các khái niệm cơ bản trong Phần 2.1. Tiếp theo, chúng tôi đi sâu vào một trình bày toàn diện về các mô hình khuếch tán dựa trên RWKV cho tạo hình ảnh trong Phần 2.2. Nó bao gồm các khía cạnh khác nhau như chia patch hình ảnh, khối Bi-RWKV xếp chồng, kết nối bỏ qua, và tích hợp điều kiện. Cuối cùng, chúng tôi thực hiện phân tích tính toán và thiết lập các cấu hình mở rộng mô hình tối ưu.

2.1. Kiến thức cơ bản
Các mô hình khuếch tán. Các mô hình khuếch tán đã nổi lên như một họ mô hình sinh mới tạo ra dữ liệu bằng cách biến đổi lặp lại nhiễu ngẫu nhiên thông qua một chuỗi các bước khử nhiễu có thể phân tách. Nó thường bao gồm một quá trình thêm nhiễu tiến và một quá trình khử nhiễu lùi. Chính thức, cho dữ liệu x₀ được lấy mẫu từ phân phối p(x₀), quá trình thêm nhiễu tiến bao gồm việc thêm nhiễu Gaussian lặp lại vào dữ liệu, tạo ra một chuỗi Markov các biến tiềm ẩn x₁, ..., xₜ, trong đó:

q(xₜ|xₜ₋₁) = N(xₜ; √(1-βₜ)xₜ₋₁, βₜI), (1)

và β₁, ..., βₜ là các siêu tham số xác định lịch trình nhiễu. Sau một số bước khuếch tán được đặt trước, xₜ có thể được coi là nhiễu Gaussian tiêu chuẩn. Một mạng khử nhiễu εθ với các tham số θ được huấn luyện để học quá trình khử nhiễu lùi, nhằm loại bỏ nhiễu được thêm vào theo một đầu vào có nhiễu. Trong quá trình suy luận, một điểm dữ liệu có thể được tạo ra bằng cách lấy mẫu từ một nhiễu Gaussian ngẫu nhiên xₜ ∼ N(0;I) và lặp lại khử nhiễu mẫu bằng cách lấy mẫu tuần tự xₜ₋₁ từ xₜ với quá trình khử nhiễu đã học, như:

xₜ₋₁ = (1/√αₜ)(xₜ - (1-αₜ)/(1-ᾱₜ)ε(xₜ, t)) + σₜz, (2)

trong đó αₜ = ∏ˢ₌₁ᵗ αₛ, αₜ = 1-βₜ, và σₜ biểu thị quy mô nhiễu. Trong thực tế, quá trình lấy mẫu khuếch tán có thể được tăng tốc thêm với các kỹ thuật lấy mẫu khác nhau [48, 49, 74].

Các cấu trúc giống RWKV. RWKV [59] mang lại cải tiến cho kiến trúc RNN tiêu chuẩn [30], được tính toán song song trong quá trình huấn luyện trong khi suy luận như RNN. Nó bao gồm việc tăng cường cơ chế chú ý tuyến tính và thiết kế cơ chế giá trị khóa trọng số tiếp nhận (RWKV). Nói chung, mô hình RWKV bao gồm một lớp đầu vào, một loạt các khối dư được xếp chồng, và một lớp đầu ra. Mỗi khối dư được tạo thành từ các khối con trộn thời gian và trộn kênh.

(i) Khối Trộn-Thời-Gian nhằm cải thiện việc mô hình hóa các phụ thuộc và mẫu trong một chuỗi. Nó được đạt được bằng cách thay thế tính toán tổng có trọng số thông thường trong một cơ chế chú ý bằng các trạng thái ẩn. Khối trộn thời gian có thể truyền và cập nhật thông tin hiệu quả qua các bước tuần tự với các trạng thái ẩn và việc cập nhật có thể được biểu diễn như sau:

qₜ = (μq ⊙ xₜ + (1-μq) ⊙ xₜ₋₁)·Wq, (3)
kₜ = (μk ⊙ xₜ + (1-μk) ⊙ xₜ₋₁)·Wk, (4)
vₜ = (μv ⊙ xₜ + (1-μv) ⊙ xₜ₋₁)·Wv, (5)
oₜ = (σ(qₜ) ⊙ h(kₜ, vₜ))·Wo, (6)

trong đó qₜ, kₜ, và vₜ được tính toán bằng cách nội suy tuyến tính giữa đầu vào hiện tại và đầu vào ở bước thời gian trước. Việc nội suy, được xác định bởi tham số dịch chuyển token μ, đảm bảo biểu diễn token mạch lạc và trôi chảy. Ngoài ra, một hàm kích hoạt phi tuyến σ được áp dụng cho qₜ, và giá trị kết quả được kết hợp với các trạng thái ẩn h(kₜ, vₜ) bằng cách sử dụng phép nhân từng phần tử. Các trạng thái ẩn, đóng vai trò vừa là cổng đặt lại và thay thế cho giá trị tổng có trọng số truyền thống, có thể được tính toán như:

pₜ = max(pₜ₋₁, kₜ), (7)
hₜ = (exp(pₜ₋₁-pₜ) ⊙ aₜ₋₁ + exp(kₜ-pₜ) ⊙ vₜ)/(exp(pₜ₋₁-pₜ) ⊙ bₜ₋₁ + exp(kₜ-pₜ)), (8)

trong đó a₀, b₀, p₀ được khởi tạo bằng không. Trực quan, các trạng thái ẩn được tính toán đệ quy, và vector p đóng vai trò cổng đặt lại trong quá trình này.

(ii) Khối Trộn-Kênh nhằm khuếch đại các đầu ra của khối trộn thời gian, có thể được cho bởi:

rₜ = (μr ⊙ oₜ + (1-μr) ⊙ oₜ₋₁)·Wr (9)
zₜ = (μz ⊙ oₜ + (1-μz) ⊙ oₜ₋₁)·Wz (10)
x̃ₜ = σ(rₜ) ⊙ (max(zₜ,0)² · Wv) (11)

Đầu ra oₜ chứa thông tin lịch sử đến thời điểm t, và trọng số nội suy μ được suy ra từ oₜ và oₜ₋₁, tương tự như khối trộn thời gian, điều này cũng tăng cường biểu diễn thông tin lịch sử. Lưu ý rằng các tính toán của trạng thái ẩn có thể dẫn đến mất thông tin và thất bại trong việc nắm bắt các phụ thuộc tầm xa [59].

2.2. Thiết kế cấu trúc mô hình
Chúng tôi trình bày Diffusion-RWKV, một biến thể của các mô hình khuếch tán giống RWKV, như một kiến trúc đơn giản và đa năng cho tạo hình ảnh. Diffusion-RWKV tham số hóa mạng dự đoán nhiễu εθ(xₜ, t, c), nhận bước thời gian t, điều kiện c và hình ảnh có nhiễu xₜ làm đầu vào và dự đoán nhiễu được tiêm vào điểm dữ liệu xₜ. Vì mục tiêu của chúng tôi tuân theo kiến trúc RWKV tiên tiến để duy trì các đặc tính khả năng mở rộng của nó, Diffusion-RWKV được dựa trên kiến trúc RWKV hai chiều [11] hoạt động trên các chuỗi token. Hình 2 minh họa tổng quan về kiến trúc Diffusion-RWKV hoàn chỉnh. Trong phần tiếp theo, chúng tôi mô tả chi tiết về quá trình truyền tiến và các thành phần tạo nên không gian thiết kế của lớp mô hình này.

Tokenization hình ảnh. Lớp đầu tiên của Diffusion-RWKV thực hiện biến đổi hình ảnh đầu vào I ∈ RH×W×C thành các patch 2-D được làm phẳng X ∈ RJ×(p²·C). Tiếp theo, nó chuyển đổi các patch này thành một chuỗi J token, mỗi token có chiều D, bằng cách nhúng tuyến tính mỗi patch hình ảnh trong đầu vào. Phù hợp với [10], các embedding vị trí có thể học được được áp dụng cho tất cả các token đầu vào. Số lượng token J được tạo ra bởi quá trình tokenization được xác định bởi siêu tham số kích thước patch p, được tính toán là H×W/p². Lớp tokenization hỗ trợ cả biểu diễn pixel thô và không gian tiềm ẩn.

Khối RWKV hai chiều. Sau lớp embedding, các token đầu vào trải qua xử lý thông qua một chuỗi các khối Bi-RWKV giống hệt nhau. Xét rằng khối RWKV ban đầu được thiết kế cho xử lý chuỗi một chiều, chúng tôi tham khảo [90], tích hợp mô hình hóa chuỗi hai chiều được thiết kế riêng cho các nhiệm vụ thị giác. Sự điều chỉnh này bảo tồn cấu trúc cốt lõi và lợi thế của RWKV [59] đồng thời tích hợp các sửa đổi quan trọng để điều chỉnh nó cho xử lý dữ liệu hình ảnh. Cụ thể, nó sử dụng một hoạt động dịch chuyển bốn hướng được thiết kế riêng cho dữ liệu thị giác hai chiều và sửa đổi cơ chế chú ý RWKV nhân quả ban đầu thành một cơ chế chú ý toàn cục hai chiều. Hoạt động dịch chuyển bốn hướng mở rộng phạm vi ngữ nghĩa của các token riêng lẻ, trong khi chú ý hai chiều cho phép tính toán chú ý toàn cục trong độ phức tạp tính toán tuyến tính theo cách tiến và lùi giống RNN. Như được minh họa trong phần bên phải của Hình 2, quá trình truyền tiến của khối Bi-RWKV kết hợp cả hướng tiến và lùi trong các mô-đun trộn không gian và kênh. Những thay đổi này tăng cường khả năng tầm xa của mô hình đồng thời đảm bảo tính mở rộng và ổn định.

Kết nối bỏ qua. Xét một loạt L khối Bi-RWKV được xếp chồng, chúng tôi phân loại các khối thành ba nhóm: ⌊L/2⌋ khối đầu tiên như nhóm nông, khối giữa như lớp trung tâm, và ⌊L/2⌋ khối còn lại như nhóm sâu như [1, 18]. Cho h_shallow và h_deep biểu diễn các trạng thái ẩn từ nhánh chính và nhánh bỏ qua dài tương ứng, cả hai nằm trong RJ×D. Chúng tôi đề xuất nối các trạng thái ẩn này và áp dụng một phép chiếu tuyến tính, được biểu diễn là Linear(Concat(h_shallow, h_deep)), trước khi truyền chúng đến khối tiếp theo.

Bộ giải mã tuyến tính. Sau khi hoàn thành khối Bi-RWKV cuối cùng, việc giải mã chuỗi các trạng thái ẩn để tạo ra dự đoán nhiễu đầu ra và dự đoán hiệp phương sai chéo trở nên thiết yếu. Các đầu ra kết quả này duy trì một hình dạng tương đương với hình ảnh đầu vào ban đầu. Để đạt được điều này, một bộ giải mã tuyến tính tiêu chuẩn được sử dụng, trong đó norm lớp cuối cùng được áp dụng, và mỗi token được giải mã tuyến tính thành một tensor p²·C. Cuối cùng, các token được giải mã được sắp xếp lại để khớp với bố cục không gian ban đầu, tạo ra nhiễu được dự đoán và hiệp phương sai.

Tích hợp điều kiện. Ngoài các đầu vào hình ảnh có nhiễu, các mô hình khuếch tán xử lý thông tin điều kiện bổ sung, chẳng hạn như bước thời gian nhiễu t và điều kiện c, thường bao gồm nhãn lớp hoặc dữ liệu ngôn ngữ tự nhiên. Để tích hợp các điều kiện bổ sung một cách hiệu quả, nghiên cứu này sử dụng ba thiết kế khác biệt như được tham khảo từ [18, 58]:

• Điều kiện trong ngữ cảnh. Một chiến lược đơn giản là thêm các embedding vector của bước thời gian t và điều kiện c như hai token bổ sung trong chuỗi đầu vào. Các token này được xử lý ngang hàng với các token hình ảnh. Triển khai kỹ thuật này cho phép sử dụng các khối Bi-RWKV mà không cần bất kỳ điều chỉnh nào. Lưu ý rằng các token điều kiện được loại bỏ khỏi chuỗi trong mô-đun trộn không gian trong mỗi khối Bi-RWKV và sau khối cuối cùng.

• Khối norm lớp thích ứng (adaLN). Chúng tôi khám phá việc thay thế lớp norm tiêu chuẩn bằng lớp norm thích ứng. Thay vì trực tiếp học các tham số quy mô và dịch chuyển trên cơ sở từng chiều, các tham số này được suy ra từ tổng của các vector embedding của t và c.

• Khối adaLN-Zero. Ngoài việc hồi quy γ và β, chúng tôi cũng hồi quy các tham số quy mô theo chiều α được áp dụng ngay trước bất kỳ kết nối dư nào trong khối Bi-RWKV. MLP được khởi tạo để tạo ra đầu ra vector không cho tất cả α.

2.3. Phân tích tính toán
Tóm lại, các siêu tham số của mô hình Diffusion-RWKV bao gồm các thành phần quan trọng bao gồm chiều embedding E, chiều ẩn D trong phép chiếu tuyến tính, và độ sâu L. Trọng tâm của kiến trúc khối RWKV hai chiều là việc tạo ra kết quả chú ý cho mỗi token thông qua các bước cập nhật riêng lẻ, đỉnh điểm trong T bước cần thiết cho việc xử lý hoàn chỉnh ma trận WKV. Ở đây, T là độ dài chuỗi. Xét các đầu vào K và V là các ma trận có hình dạng J×D, trong đó D là chiều của các vector có thể học ẩn, chi phí tính toán của việc tính toán ma trận WKV được cho bởi:

FLOPs(Bi-WKV(K, V)) = 13 × J × D. (12)

Ở đây, số 13 là xấp xỉ từ việc cập nhật bốn trạng thái ẩn, tính toán hàm mũ, và tính toán ma trận wkv_t. J là tổng số bước cập nhật và bằng số lượng token hình ảnh. Xấp xỉ trên cho thấy rằng độ phức tạp của quá trình tiến là O(J·D). Sự lan truyền ngược của toán tử vẫn có thể được biểu diễn như một dạng RNN phức tạp hơn, với độ phức tạp tính toán là O(J·D). Nó thể hiện ưu thế của việc tăng tuyến tính so với hoạt động tự chú ý trong cấu trúc Transformer. Cuối cùng, các biến thể mô hình khác nhau được chỉ định trong Bảng 1. Trong đó, chúng tôi sử dụng năm cấu hình, từ nhỏ đến lớn, để bao phủ một phạm vi rộng các kích thước mô hình và phân bổ flop, từ 1.72 đến 34.95 Gflops, cho phép chúng tôi đánh giá hiệu suất mở rộng.

3. Thí nghiệm
Trong phần này, chúng tôi sẽ đi sâu vào những phức tạp của không gian thiết kế và xem xét kỹ lưỡng các thuộc tính mở rộng vốn có trong lớp mô hình Diffusion-RWKV của chúng tôi. Để đơn giản hóa thảo luận, mỗi mô hình trong lớp của chúng tôi được ký hiệu bởi các cấu hình cụ thể và kích thước patch p. Ví dụ, chúng tôi gọi cấu hình phiên bản Large với p = 2 là DRWKV-L/2.

3.1. Cài đặt thí nghiệm
Tập dữ liệu. Đối với tạo hình ảnh không có điều kiện, hai tập dữ liệu được xem xét: CIFAR10 [40] và CelebA 64x64 [46]. CIFAR10 bao gồm một tập hợp 50k hình ảnh huấn luyện, trong khi CelebA 64x64 bao gồm 162,770 hình ảnh mô tả khuôn mặt người. Đối với tạo hình ảnh có điều kiện lớp, tập dữ liệu ImageNet [5] được sử dụng. Tập dữ liệu này bao gồm 1,281,167 hình ảnh huấn luyện, được phân phối trên 1,000 lớp riêng biệt. Về mặt tăng cường dữ liệu, chỉ có lật ngang được sử dụng. Quá trình huấn luyện bao gồm 500k lần lặp trên cả CIFAR10 và CelebA 64×64, sử dụng kích thước batch 128 trong không gian pixel. Trong trường hợp ImageNet, hai tình huống được xem xét với độ phân giải 256×256 và 512×512. Đối với trường hợp trước, 500k lần lặp được tiến hành, trong khi đối với trường hợp sau, 1M lần lặp được thực hiện. Kích thước batch được đặt là 512 trong cả hai trường hợp.

Chi tiết triển khai. Chúng tôi tuân theo cùng một công thức huấn luyện từ DiT [58] để đảm bảo cài đặt nhất quán trên tất cả các mô hình. Chúng tôi chọn tích hợp trung bình di động hàm mũ (EMA) của các trọng số mô hình với tỷ lệ suy giảm cố định là 0.9999. Tất cả kết quả được báo cáo đều được lấy bằng mô hình EMA. Chúng tôi sử dụng tối ưu hóa AdamW [36] mà không có suy giảm trọng số trên tất cả các tập dữ liệu và duy trì tỷ lệ học từ 1e-4 đến 3e-5 theo giai đoạn. Các mô hình của chúng tôi được huấn luyện trên GPU Nvidia A100. Trong quá trình huấn luyện trên tập dữ liệu ImageNet ở độ phân giải 256×256 và 512×512, chúng tôi cũng áp dụng hướng dẫn không có phân loại [27] theo [67] và sử dụng một mô hình tự động mã hóa biến phân (VAE) được huấn luyện trước sẵn có [38] từ playground V2 được cung cấp trong huggingface với các cài đặt tương ứng. Thành phần mã hóa VAE tích hợp một yếu tố giảm mẫu là 8. Chúng tôi duy trì các siêu tham số khuếch tán từ [58], sử dụng lịch trình phương sai tuyến tính t_max = 1000 từ 1×10⁻⁴ đến 2×10⁻² và tham số hóa hiệp phương sai.

Để điều chỉnh mô hình cho ngữ cảnh không có điều kiện, chúng tôi chỉ loại bỏ thành phần embedding nhãn lớp.

Các chỉ số đánh giá. Đánh giá hiệu suất tạo hình ảnh được tiến hành bằng cách sử dụng Khoảng cách Khởi tạo Fréchet (FID) [26], một chỉ số được sử dụng rộng rãi để đánh giá chất lượng hình ảnh được tạo ra. Phù hợp với các quy ước được thiết lập cho phân tích so sánh với các công trình trước đó, chúng tôi trình bày kết quả FID-50K thu được thông qua 250 bước lấy mẫu DDPM [56], theo [7]. Hơn nữa, chúng tôi cung cấp các chỉ số bổ sung như Điểm Khởi tạo [69], sFID [54], và Độ chính xác/Độ nhớ [41] để bổ sung cho việc đánh giá.

3.2. Phân tích mô hình
Trước tiên, chúng tôi tiến hành một cuộc điều tra thực nghiệm có hệ thống về các thành phần cơ bản của các mô hình Diffusion-RWKV. Cụ thể, chúng tôi thí nghiệm trên tập dữ liệu CIFAR10, đánh giá điểm FID mỗi 50K lần lặp huấn luyện trên 10K mẫu được tạo ra, thay vì 50K mẫu để có hiệu quả [1], và xác định các chi tiết triển khai mặc định tối ưu.

Ảnh hưởng của kích thước patch. Chúng tôi huấn luyện phạm vi kích thước patch trên (8, 4, 2) trong cấu hình Small trên tập dữ liệu CIFAR10. Kết quả thu được từ thí nghiệm này được minh họa trong Hình 3 (a). Nó chỉ ra rằng chỉ số FID thể hiện sự dao động khi đáp ứng với việc giảm kích thước patch trong khi duy trì kích thước mô hình nhất quán. Trong suốt quá trình huấn luyện, chúng tôi quan sát thấy những cải thiện có thể nhận biết được trong các giá trị FID bằng cách tăng số lượng token được xử lý bởi Diffusion-RWKV, trong khi giữ các tham số gần như cố định. Quan sát này dẫn chúng tôi đến kết luận rằng để đạt được hiệu suất tối ưu cần một kích thước patch nhỏ hơn, cụ thể là 2. Chúng tôi giả thuyết rằng yêu cầu này phát sinh từ bản chất cấp độ thấp vốn có của nhiệm vụ dự đoán nhiễu trong các mô hình khuếch tán. Có vẻ như các patch nhỏ hơn phù hợp hơn cho nhiệm vụ này so với các nhiệm vụ cấp cao hơn như phân loại.

Ảnh hưởng của bỏ qua dài. Để đánh giá hiệu quả của hoạt động bỏ qua, chúng tôi điều tra ba biến thể khác nhau, cụ thể là: (i) Nối, ký hiệu là Linear(Concat(h_shallow, h_deep)); (ii) Cộng, được biểu diễn bởi h_shallow + h_deep; và (iii) Không có kết nối bỏ qua. Hình 3 (b) minh họa kết quả của các biến thể này. Rõ ràng là việc cộng trực tiếp các trạng thái ẩn từ các lớp nông và sâu không mang lại bất kỳ lợi ích có thể nhận biết nào. Ngược lại, việc áp dụng nối bao gồm một phép chiếu tuyến tính có thể học được trên các trạng thái ẩn nông và hiệu quả tăng cường hiệu suất so với việc không có kết nối bỏ qua dài.

Ảnh hưởng của kết hợp điều kiện. Chúng tôi xem xét ba phương pháp để tích hợp bước thời gian điều kiện t vào mạng, như đã thảo luận trong phần phương pháp trước đó. Các phương pháp tích hợp được mô tả trong Hình 3 (c). Trong số các chiến lược này, khối adaLN-Zero thể hiện FID thấp hơn so với phương pháp điều kiện trong ngữ cảnh, đồng thời cũng thể hiện hiệu quả tính toán vượt trội. Cụ thể, sau 500k lần lặp huấn luyện, mô hình adaLN-Zero đạt được FID xấp xỉ một phần ba so với mô hình trong ngữ cảnh, nhấn mạnh ảnh hưởng quan trọng của cơ chế điều kiện đối với chất lượng tổng thể của mô hình. Hơn nữa, cần lưu ý rằng quá trình khởi tạo có ý nghĩa quan trọng trong bối cảnh này. Ngoài ra, đáng chú ý là do việc bao gồm một hoạt động thay đổi kích thước trong thiết kế Bi-RWKV trong trộn kênh không gian, chỉ có token trong ngữ cảnh được cung cấp cho mô-đun trộn kênh.

Mở rộng kích thước mô hình. Chúng tôi điều tra các thuộc tính mở rộng của Diffusion-RWKV bằng cách nghiên cứu ảnh hưởng của độ sâu, tức là số lượng lớp Bi-RWKV, và chiều rộng, ví dụ như kích thước ẩn. Cụ thể, chúng tôi huấn luyện 5 mô hình Diffusion-RWKV trên tập dữ liệu ImageNet với độ phân giải 256×256, bao gồm các cấu hình mô hình từ nhỏ đến lớn như được chi tiết trong Bảng 1, ký hiệu là (S, B, M, L, H) để đơn giản. Như được mô tả trong Hình 3 (d), hiệu suất cải thiện khi độ sâu tăng từ 25 lên 49. Tương tự, việc tăng chiều rộng từ 384 lên 1024 mang lại những cải thiện về hiệu suất. Nhìn chung, trên tất cả năm cấu hình, chúng tôi thấy rằng tương tự như các mô hình DiT [58], các mô hình lớn sử dụng FLOP hiệu quả hơn và việc mở rộng DR-WKV sẽ cải thiện FID ở tất cả các giai đoạn huấn luyện.

3.3. Kết quả chính
Chúng tôi so sánh với một tập hợp các mô hình tốt nhất trước đó, bao gồm: các phương pháp kiểu GAN trước đây đã đạt được kết quả state-of-the-art, các kiến trúc UNet được huấn luyện với biểu diễn không gian pixel, và các Transformer và mô hình không gian trạng thái hoạt động trong không gian tiềm ẩn. Lưu ý rằng mục tiêu của chúng tôi là so sánh, thông qua một quá trình khử nhiễu tương tự, hiệu suất của mô hình chúng tôi đối với các baseline khác.

Tạo hình ảnh không có điều kiện. Chúng tôi đánh giá khả năng tạo hình ảnh không có điều kiện của mô hình chúng tôi liên quan đến các baseline đã được thiết lập sử dụng các tập dữ liệu CIFAR10 và CelebA trong lĩnh vực dựa trên pixel. Kết quả phân tích của chúng tôi được trình bày trong Bảng 2 và Bảng 3 tương ứng. Kết quả cho thấy mô hình được đề xuất của chúng tôi, Diffusion-RWKV, đạt được điểm FID tương đương với những điểm đạt được bởi các mô hình U-ViT dựa trên Transformer và DiS dựa trên SSM, trong khi sử dụng ngân sách huấn luyện tương tự. Đáng chú ý, mô hình của chúng tôi đạt được điều này với ít tham số hơn và thể hiện điểm FID vượt trội. Những phát hiện này nhấn mạnh tính thực tiễn và hiệu quả của RWKV trên các benchmark tạo hình ảnh khác nhau.

Tạo hình ảnh có điều kiện lớp. Chúng tôi cũng so sánh mô hình Diffusion-RWKV với các mô hình có điều kiện lớp state-of-the-art trong tập dữ liệu ImageNet, như được liệt kê trong Bảng 4 và Bảng 5. Khi xem xét độ phân giải 256, việc huấn luyện mô hình DRWKV của chúng tôi thể hiện giảm 25% trong Tổng Gflops so với DiT (1.60×10¹¹ so với 2.13×10¹¹). Ngoài ra, các mô hình của chúng tôi đạt được điểm sFID tương tự với các mô hình dựa trên DDPM khác, vượt trội hơn hầu hết các chiến lược state-of-the-art ngoại trừ SiT và DiS. Điều này chứng minh rằng hình ảnh được tạo ra bởi mô hình Diffusion-RWKV có khả năng chống lại biến dạng không gian. Hơn nữa, về mặt điểm FID, Diffusion-RWKV duy trì một khoảng cách tương đối nhỏ so với đối thủ cạnh tranh tốt nhất. Đáng chú ý là SiT là một kiến trúc dựa trên transformer sử dụng một chiến lược tiên tiến, cũng có thể được tích hợp vào xương sống của chúng tôi. Tuy nhiên, khía cạnh này được để lại cho nghiên cứu tương lai, vì trọng tâm chính của chúng tôi nằm ở việc so sánh mô hình của chúng tôi với DiT. Hơn nữa, chúng tôi mở rộng so sánh của mình đến một benchmark độ phân giải cao hơn có kích thước 512. Kết quả thu được từ mô hình Diffusion-RWKV chứng minh hiệu suất tương đối mạnh, tiếp cận hiệu suất của một số mô hình độ phân giải cao state-of-the-art. Mô hình của chúng tôi vượt trội hơn tất cả các mô hình ngoại trừ DiS, trong khi đạt được điểm FID tương đương với gánh nặng tính toán thấp hơn.

3.4. Nghiên cứu trường hợp
Trong Hình 1 và Hình 4, một lựa chọn được tuyển chọn các mẫu từ các tập dữ liệu ImageNet được trình bày. Các mẫu này được hiển thị ở độ phân giải 256×256 và 512×512, hiệu quả minh họa các biểu diễn ngữ nghĩa rõ ràng và thể hiện tạo ra chất lượng cao. Để đi sâu hơn vào chủ đề này, trang dự án cung cấp một tập hợp các mẫu được tạo ra bổ sung, bao gồm cả các biến thể có điều kiện lớp và ngẫu nhiên.

4. Các công trình liên quan
Tạo hình ảnh với khuếch tán. Các mô hình khuếch tán và dựa trên điểm [33, 75–77] đã chứng minh những tiến bộ đáng kể trong các nhiệm vụ khác nhau, đặc biệt là trong bối cảnh tạo hình ảnh [66–68]. DDPM chủ yếu được quy cho những cải tiến trong các kỹ thuật lấy mẫu [16, 17, 28, 34, 55], và việc tích hợp hướng dẫn không có phân loại [27]. Ngoài ra, [74] đã giới thiệu một quy trình lấy mẫu hiệu quả hơn được gọi là Mô hình Khói nhiễu Khuếch tán (DDIM). Mô hình hóa không gian tiềm ẩn là một kỹ thuật cốt lõi khác trong các mô hình sinh sâu. Các tự động mã hóa biến phân [38] đã tiên phong trong việc học các không gian tiềm ẩn với các kiến trúc mã hóa-giải mã cho tái tạo. Khái niệm nén thông tin trong các không gian tiềm ẩn cũng được áp dụng trong các mô hình khuếch tán, như được thể hiện bởi chất lượng mẫu state-of-the-art đạt được bởi các mô hình khuếch tán tiềm ẩn [67], huấn luyện các mô hình sinh sâu để đảo ngược một quá trình hỏng nhiễu trong một không gian tiềm ẩn. Ngoài ra, những tiến bộ gần đây đã tích hợp các quy trình huấn luyện có mặt nạ, tăng cường các mục tiêu huấn luyện khử nhiễu thông qua tái tạo token có mặt nạ [88]. Công trình của chúng tôi được xây dựng cơ bản trên các DDPM tiêu chuẩn hiện có.

Kiến trúc cho các mô hình khuếch tán. Các mô hình đầu tiên cho khuếch tán sử dụng kiến trúc kiểu U-Net [7, 28]. Các nghiên cứu tiếp theo nỗ lực tăng cường U-Net bằng cách tích hợp các kỹ thuật khác nhau, chẳng hạn như việc thêm các lớp chú ý ở nhiều quy mô [55], kết nối dư [2], và chuẩn hóa [60, 83]. Tuy nhiên, U-Net gặp khó khăn khi mở rộng đến độ phân giải cao do yêu cầu tính toán leo thang được áp đặt bởi cơ chế chú ý [71]. Gần đây, các transformer thị giác [10] đã nổi lên như một kiến trúc thay thế, thể hiện khả năng mở rộng mạnh mẽ và khả năng mô hình hóa tầm xa, do đó thách thức quan niệm rằng thiên lệch quy nạp tích chập luôn là không thể thiếu. Các transformer khuếch tán [1, 19, 58] đã chứng minh kết quả đầy hứa hẹn. Các kiến trúc CNN-transformer hybrid khác được đề xuất [47] để cải thiện tính ổn định huấn luyện. Gần đây hơn, mô hình dựa trên không gian trạng thái [18, 31, 84] đã có được hiệu suất tiến bộ với hiệu quả tính toán. Công trình của chúng tôi phù hợp với việc khám phá các mô hình chuỗi đệ quy và các lựa chọn thiết kế liên quan để tạo ra hình ảnh chất lượng cao đồng thời giảm thiểu sự tương đồng văn bản.

Mô hình hóa chuỗi dài hiệu quả. Kiến trúc transformer tiêu chuẩn sử dụng chú ý để hiểu sự tương tác giữa các token riêng lẻ. Tuy nhiên, nó đối mặt với thách thức khi xử lý các chuỗi dài, chủ yếu do độ phức tạp tính toán bậc hai mà nó kéo theo. Để giải quyết vấn đề này, các phương pháp xấp xỉ chú ý khác nhau đã được đề xuất [13, 32, 51, 72, 79, 82], nhằm xấp xỉ tự chú ý trong khi sử dụng tài nguyên tính toán dưới bậc hai. Đáng chú ý, Mega [52] kết hợp trung bình di động hàm mũ với một đơn vị chú ý đơn giản hóa, vượt trội hơn hiệu suất của các mô hình transformer baseline. Hơn nữa, các nhà nghiên cứu cũng đã khám phá các thay thế có khả năng xử lý hiệu quả các chuỗi dài. Một cách bao gồm việc sử dụng các kiến trúc dựa trên mô hình không gian trạng thái, như được thể hiện bởi [22–24], đã chứng minh những tiến bộ đáng kể so với các phương pháp state-of-the-art đương đại trong các nhiệm vụ như LRA và benchmarking âm thanh [20]. Hơn nữa, các nghiên cứu gần đây [22, 59, 61, 62] đã cung cấp bằng chứng thực nghiệm hỗ trợ tiềm năng của các kiến trúc không chú ý trong việc đạt được hiệu suất đáng khen ngợi trong mô hình hóa ngôn ngữ. Được thúc đẩy bởi xu hướng tiến hóa này của các thiết kế đệ quy, công trình của chúng tôi lấy cảm hứng từ những tiến bộ này và chủ yếu tận dụng xương sống của RWKV.

5. Kết luận
Bài báo này trình bày Diffusion-RWKV, một kiến trúc được thiết kế cho các mô hình khuếch tán có thông tin tuần tự với độ phức tạp tính toán tuyến tính. Phương pháp được đề xuất xử lý hiệu quả các trạng thái ẩn tầm xa mà không cần nén biểu diễn. Thông qua các nhiệm vụ tạo hình ảnh toàn diện, chúng tôi thể hiện tiềm năng của nó như một xương sống thay thế khả thi cho Transformer. Về mặt thực nghiệm, Diffusion-RWKV chứng minh hiệu suất và khả năng mở rộng tương đương trong khi thể hiện độ phức tạp tính toán và tiêu thụ bộ nhớ thấp hơn. Tận dụng độ phức tạp giảm, Diffusion-RWKV vượt trội hơn mô hình Transformer trong các tình huống mà mô hình sau gặp khó khăn để đối phó với yêu cầu tính toán cao. Chúng tôi dự đoán rằng nó sẽ phục vụ như một thay thế hiệu quả và tiết kiệm chi phí cho Transformer, do đó làm nổi bật những khả năng đáng kể của các transformer với độ phức tạp tuyến tính trong lĩnh vực tạo đa phương thức.
