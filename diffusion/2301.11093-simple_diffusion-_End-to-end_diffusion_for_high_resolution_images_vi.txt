--- TRANG 10 ---
simple diffusion

Tài liệu tham khảo

Anonymous. Discrete predictor-corrector diffusion models for image synthesis. Trong Submitted to The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=VM8batVBWvg. đang xem xét.

Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., và Liu, M. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. CoRR, abs/2211.01324, 2022.

Chang, H., Zhang, H., Jiang, L., Liu, C., và Freeman, W. T. Maskgit: Masked generative image transformer. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 11305–11315. IEEE, 2022.

Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M., Murphy, K., Freeman, W. T., Rubinstein, M., Li, Y., và Krishnan, D. Muse: Text-to-image generation via masked generative transformers. CoRR, abs/2301.00704, 2023.

Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., và Chan, W. WaveGrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.

Chen, T. On the importance of noise scheduling for diffusion models. arxiv, 2023.

Dhariwal, P. và Nichol, A. Diffusion models beat gans on image synthesis. CoRR, abs/2105.05233, 2021.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., và Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.

Feng, Z., Zhang, Z., Yu, X., Fang, Y., Li, L., Chen, X., Lu, Y., Liu, J., Yin, W., Feng, S., Sun, Y., Tian, H., Wu, H., và Wang, H. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. CoRR, abs/2210.15257, 2022.

Gu, J., Zhai, S., Zhang, Y., Bautista, M. Á., và Susskind, J. M. f-dm: A multi-stage diffusion model via progressive signal transformation. CoRR, abs/2210.04955, 2022.

Ho, J. và Salimans, T. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi: 10.48550/arXiv.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598.

Ho, J., Jain, A., và Abbeel, P. Denoising diffusion probabilistic models. Trong Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., và Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS, 2020.

Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., và Salimans, T. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:47:1–47:33, 2022.

Jabri, A., Fleet, D. J., và Chen, T. Scalable adaptive computation for iterative generation. CoRR, abs/2212.11972, 2022.

Kingma, D. P., Salimans, T., Poole, B., và Ho, J. Variational diffusion models. CoRR, abs/2107.00630, 2021.

Kong, Z., Ping, W., Huang, J., Zhao, K., và Catanzaro, B. DiffWave: A versatile diffusion model for audio synthesis. Trong 9th International Conference on Learning Representations, ICLR, 2021.

Meng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., và Salimans, T. On distillation of guided diffusion models. CoRR, abs/2210.03142, 2022.

Nichol, A. Q. và Dhariwal, P. Improved denoising diffusion probabilistic models. Trong Meila, M. và Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 2021.

Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., và Chen, M. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. Trong Chaudhuri, K., Jegelka, S., Song, L., Szepesvári, C., Niu, G., và Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 16784–16804. PMLR, 2022. URL https://proceedings.mlr.press/v162/nichol22a.html.

Peebles, W. và Xie, S. Scalable diffusion models with transformers. CoRR, abs/2212.09748, 2022.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., và Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.

Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., và Chen, M. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022.

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., và Ommer, B. High-resolution image synthesis with latent diffusion models. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674–10685. IEEE, 2022.

Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., và Norouzi, M. Photorealistic text-to-image diffusion models with deep language understanding. CoRR, abs/2205.11487, 2022.

Salimans, T. và Ho, J. Progressive distillation for fast sampling of diffusion models. Trong The Tenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022.

Sauer, A., Schwarz, K., và Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. Trong Nandigjav, M., Mitra, N. J., và Hertzmann, A. (eds.), SIGGRAPH '22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, pp. 49:1–49:10. ACM, 2022.

Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., và Taigman, Y. Make-a-video: Text-to-video generation without text-video data. CoRR, abs/2209.14792, 2022.

Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., và Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. Trong Bach, F. R. và Blei, D. M. (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML, 2015.

Song, Y. và Ermon, S. Generative modeling by estimating gradients of the data distribution. Trong Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS, 2019.

Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., và Poole, B. Score-based generative modeling through stochastic differential equations. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.

Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., và Wu, Y. Scaling autoregressive models for content-rich text-to-image generation. CoRR, abs/2206.10789, 2022.

--- TRANG 11 ---
simple diffusion

A. Thông tin Nền tảng Bổ sung về Mô hình Khuếch tán

Phần này là tóm tắt chi tiết hơn về thông tin nền tảng liên quan đến khuếch tán khử nhiễu. Trước hết, có thể hữu ích để hiểu cách các mô hình khuếch tán khử nhiễu hiện đại (Ho et al., 2020) được huấn luyện bằng cách sử dụng các công thức từ (Kingma et al., 2021). Đầu tiên chúng tôi định nghĩa cách tín hiệu bị phá hủy (khuếch tán), đây là tương đương thuật toán của việc lấy mẫu zt ~ q(zt|x):

def diffuse(x, alpha_t, sigma_t):
    eps_t = noise_normal_like(x)
    z_t = alpha_t * x + sigma_t * eps_t
    return z_t, eps_t

Đối với thiết lập tối ưu hóa cụ thể mà chúng tôi thường sử dụng (v-prediction, epsilon loss), mất mát có thể được tính như được định nghĩa bên dưới. Đây là tương đương thuật toán của Et~U(0,1),zt~q(zt|x)||f(zt, t) - εt||² như được đề xuất bởi (Ho et al., 2020; Kingma et al., 2021):

def loss(x):
    t = noise_uniform(size=x.shape[0])  # Lấy mẫu một batch timesteps.
    logsnr_t = logsnr_schedule(t)
    alpha_t = sqrt(sigmoid(logsnr))
    sigma_t = sqrt(sigmoid(-logsnr))
    z_t, eps_t = diffuse(x, alpha_t, sigma_t)
    v_pred = uvit(z_t, logsnr_t)
    eps_pred = sigma_t * z_t + alpha_t * v_t
    return mse(eps_pred, eps_t)

Trong trường hợp điều kiện (ví dụ số lớp ImageNet hoặc embedding văn bản), chúng được thêm vào làm đầu vào cho lệnh gọi uvit, nhưng không ảnh hưởng đến quá trình khuếch tán theo các cách khác. Điều kiện bị loại bỏ 10% thời gian, để các mô hình cũng có thể được sử dụng với hướng dẫn không phân loại.

Lịch trình logsnr cosine tiêu chuẩn (chú ý đến ranh giới) có thể được định nghĩa là:

def logsnr_schedule_cosine(t, logsnr_min=-15, logsnr_max=+15):
    t_min = atan(exp(-0.5 * logsnr_max))
    t_max = atan(exp(-0.5 * logsnr_min))
    return -2 * log(tan(t_min + t * (t_max - t_min)))

Sau đó có thể định nghĩa lịch trình dịch chuyển là:

def logsnr_schedule_cosine_shifted(t, image_d, noise_d):
    return logsnr_schedule_cosine(t) + 2 * log(noise_d / image_d)

Và lịch trình nội suy là:

def logsnr_schedule_cosine_shifted(t, image_d, noise_d_low, noise_d_high):
    logsnr_low = logsnr_schedule_cosine_shifted(t, image_d, noise_d_low)
    logsnr_high = logsnr_schedule_cosine_shifted(t, image_d, noise_d_high)
    return t * logsnr_low + (1 - t) * logsnr_high

Cần chú ý rằng các siêu tham số logsnr tối thiểu và tối đa được dịch chuyển cùng với toàn bộ lịch trình, vì vậy cần chú ý khi các điểm cuối này được sử dụng để định nghĩa embedding trong kiến trúc.

Lấy mẫu Trong công trình này, chúng tôi sử dụng bộ lấy mẫu ddpm tiêu chuẩn trừ khi được ghi chú khác. Dưới đây là tương đương thuật toán của quá trình tạo sinh lấy mẫu zT ~ N(0,I) và sau đó lặp lại lấy mẫu zs ~ p(zs|zt):

def sample(x_shape):
    # lowest_idx có thể là 0 hoặc 1.
    z_t = noise_normal(x_shape)
    for t in reversed(range(lowest_idx+1, num_steps+1)):
        u_t = t / num_steps
        u_s = (t - 1) / num_steps
        logsnr_t = logsnr_schedule(u_t)
        logsnr_s = logsnr_schedule(u_s)
        v_pred = uvit(z_t, logsnr_t)
        z_t = sampler_step(z_t, v_pred, logsnr_t, logsnr_s)
    # Dự đoán cuối cùng, không lấy mẫu x ~ p(x | z_lowest) nhưng lấy dự đoán trung bình:
    logsnr_lowest = logsnr_schedule(lowest_idx / num_steps)
    v_pred = uvit(z_t, logsnr_lowest)
    x_pred = alpha_t * z_t - sigma_t * v_pred
    x_pred = clip_x(x_pred)
    return x_pred

def ddpm_sampler_step(z_t, v_pred, logsnr_t, logsnr_s):
    x_pred = alpha_t * z_t - sigma_t * v_pred
    x_pred = clip_x(x_pred)
    mu = exp(logsnr_t - logsnr_s) * alpha_st * z_t + (1 - exp(logsnr_t - logsnr_s)) * alpha_s * x_pred
    # Phương sai có thể là bất kỳ nội suy nào của hai cái sau trong không gian log:
    min_lvar = (1 - exp(logsnr_t - logsnr_s)) + log_sigmoid(-logsnr_s)
    max_lvar = (1 - exp(logsnr_t - logsnr_s)) + log_sigmoid(-logsnr_t)
    noise_param = 0.2
    sigma = sqrt(exp(noise_param * max_logvar + (1 - noise_param) * min_logvar))
    return mu + sigma * normal_noise_like(z_t)

trong đó noise_param được đặt thành 0.2 với ngoại lệ đánh giá MSCOCO FID, nơi nó được đặt thành 1.0.

Một chi tiết quan trọng nhưng không thường được thảo luận là trong quá trình lấy mẫu, việc cắt các dự đoán trong không gian x là hữu ích, dưới đây đưa ra ví dụ về cắt tĩnh, để cắt động xem (Saharia et al., 2022):

def clip_x(x):
    # x nên nằm giữa -1 và 1.
    return clip(x, -1, 1)

Hướng dẫn không phân loại Trong hướng dẫn không phân loại (Ho & Salimans, 2022), người ta thỉnh thoảng loại bỏ tín hiệu điều kiện trong quá trình huấn luyện (Thường khoảng 10% thời gian). Điều này cho phép huấn luyện các mô hình p(x) ngoài mô hình thường huấn luyện là p(x|cond). Các dự đoán epsilon của những mô hình này sau đó có thể được kết hợp lại với tỷ lệ hướng dẫn. Với η > 0:

ε̂(x) = (1 + η)ε̂(x,cond) - ηε̂(x). (7)

Có thể thay thế ε̂ bằng v̂ hoặc x̂ và kết quả cuối cùng tương đương do tính tuyến tính và các điều khoản triệt tiêu. Lưu ý chúng tôi sẽ báo cáo tỷ lệ hướng dẫn là (1 + η) như thường được thực hiện trong tài liệu, không nên nhầm lẫn với việc báo cáo chính η.

Chưng cất Giống như nhiều mô hình khuếch tán, simple diffusion cũng có thể được chưng cất để giảm số lượng bước lấy mẫu và đánh giá mạng nơ-ron (Meng et al., 2022) để giảm số lượng bước lấy mẫu. Đối với mô hình U-ViT đã chưng cất, việc tạo ra một hình ảnh mất 0.42 giây trên TPUv4. Tương tự, việc tạo ra một batch 8 hình ảnh mất 2.00 giây.

B. Chi tiết Thí nghiệm

Trong phần này, các chi tiết cụ thể về thí nghiệm được đưa ra. Đầu tiên, các thiết lập tối ưu hóa tiêu chuẩn cho thí nghiệm U-Net.

B.1. Thiết lập U-Net

Thiết lập tối ưu hóa mặc định unet:
batch_size = 512,
optimizer = 'adam',
adam_beta1 = 0.9,
adam_beta2 = 0.99, ngoại trừ ImageNet 128 là adam_beta2 = 0.999
adam_eps = 1.e-12,
learning_rate = 5e-5,
learning_rate_warmup_steps = 10_000,
weight_decay = 0.0,
ema_decay = 0.9999,
grad_clip = 1.0,

Thiết lập cụ thể cho UNet trên thí nghiệm ImageNet 128:
base_channels = 128,
emb_channels = 1024, (cho thời gian khuếch tán, lớp hình ảnh)
channel_multiplier = [1, 2, 4, 8, 8],
num_res_blocks = [3, 4, 4, 12, 4], (trừ khi ghi chú khác)
attn_resolutions = [8, 16],
num_heads = 4,
dropout_from_resolution = 16, (trừ khi ghi chú khác)
dropout = 0.1,
patching_type = 'none'
schedule = {'name': 'cosine_shifted', 'shift': 64} (trừ khi ghi chú khác)
num_train_steps = 1_500_000

Thiết lập cụ thể cho UNet trên thí nghiệm ImageNet 256:
base_channels = 128,
emb_channels = 1024, (cho thời gian khuếch tán, lớp hình ảnh)
channel_multiplier = [1, 1, 2, 4, 8, 8],
num_res_blocks = [1, 2, 2, 4, 12, 4],
attn_resolutions = [8, 16],
num_heads = 4,
dropout_from_resolution = 16,
dropout = 0.1,
patching_type = 'none'
schedule = {'name': 'cosine_shifted', 'shift': 64} (trừ khi ghi chú khác)
num_train_steps = 2_000_000

Thiết lập cho UNet trên thí nghiệm ImageNet 512:
base_channels = 128,
emb_channels = 1024, (cho thời gian khuếch tán, lớp hình ảnh)
attn_resolutions = [8, 16],
num_heads = 4,
dropout_from_resolution = 16,
dropout = 0.1,
patching_type = 'dwt_2' (trừ khi ghi chú khác)
schedule = {'name': 'cosine_shifted', 'shift': 64} (trừ khi ghi chú khác)
num_train_steps = 2_000_000

Để giữ số lượng khối dư như nhau, các khối độ phân giải cao bị bỏ qua do down-sampling được thêm vào các cấp độ phân giải thấp hơn. Không có downsampling, kiến trúc sử dụng:
channel_multiplier = [1, 1, 1, 2, 4, 8, 8], num_res_blocks = [1, 1, 2, 2, 4, 12, 4],

Trong trường hợp downsampling 2×, kiến trúc sử dụng:
channel_multiplier = [1, 2, 2, 4, 8, 8], num_res_blocks = [2, 2, 2, 4, 12, 4],

Trong trường hợp downsampling 4×, kiến trúc sử dụng:
channel_multiplier = [2, 3, 4, 8, 8], num_res_blocks = [3, 3, 4, 12, 4],

B.2. Thiết lập U-ViT

U-ViT là một kiến trúc rất giống với U-Net (xem Hình 7). Hai điểm khác biệt chính là 1) Khi một module có self-attention, nó sử dụng một khối MLP thay vì lớp tích chập, làm cho sự kết hợp của chúng thành một khối transformer. Và 2) các khối transformer ở giữa không sử dụng kết nối bỏ qua, chỉ kết nối dư. Các thiết lập tối ưu hóa mặc định cho ImageNet cho U-ViT là:

Thiết lập tối ưu hóa mặc định uvit:
optimizer = 'adam',
adam_beta1 = 0.9,
adam_beta2 = 0.99,
adam_eps = 1.e-12,
learning_rate = 1e-4,
learning_rate_warmup_steps = 10_000,
weight_decay = 0.0,
ema_decay = 0.9999,
grad_clip = 1.0,
batch_size = 2048,
num_train_steps = 500_000,

Và các thiết lập kiến trúc gần như giống nhau cho tất cả độ phân giải 128, 256 và 512.

Thiết lập kiến trúc mặc định uvit cho 512:
optimizer = 'adam',
adam_beta1 = 0.9,
adam_beta2 = 0.99,
adam_eps = 1.e-12,
learning_rate = 1e-4,
learning_rate_warmup_steps = 10_000,
weight_decay = 0.0,
ema_decay = 0.9999,
grad_clip = 1.0,
batch_size = 2048,
base_channels = 128,
emb_channels = 1024,
channel_multiplier = [1, 2, 4, 16],
num_res_blocks = [2, 2, 2],
num_transformer_blocks = 36,
num_heads = 4,
transformer_dropout = 0.2,
logsnr_input_type = 'linear',
patching_type = 'dwt_5/3_2',
mean_type = 'v',
mean_loss_type = 'v_mse',

trong đó patching type là 'none' cho 128, 'dwt_1' cho 256 và 'dwt_2' cho 512. Lưu ý cũng rằng mất mát được tính trên v thay vì epsilon. Điều này có thể không quan trọng lắm: trong các thí nghiệm nhỏ, chúng tôi chỉ quan sát thấy sự khác biệt hiệu suất nhỏ giữa hai cái. Lưu ý cũng rằng batch size lớn hơn (2048) có ảnh hưởng đáng kể đến hiệu suất FID và IS. Mô hình văn bản thành hình ảnh được huấn luyện trong 700K bước.

B.2.1. MÃ GIẢ CHO CÁC MODULE U-VIT

Các khối Transformer bao gồm một khối self-attention và mlp. Chúng được định nghĩa như mong đợi, để hoàn thiện được đưa ra dưới đây trong mã giả:

def mlp_block(x, emb, expansion_factor=4):
    B, HW, C = x.shape
    x = Normalize(x)
    mlp_h = Dense(x, expansion_factor * C)
    scale = DenseGeneral(emb, mlp_h.shape[2:])
    shift = DenseGeneral(emb, mlp_h.shape[2:])
    mlp_h = swish(mlp_h)
    mlp_h = mlp_h * (1. + scale[:, None]) + shift[:, None]
    if config.transformer_dropout > 0.:
        mlp_h = Dropout(mlp_h, config.transformer_dropout)
    out = Dense(mlp_h, C, kernel_init=zeros)
    return out

def self_attention(x, text_emb):
    B, HW, C = x.shape
    B, T, TC = text_emb.shape
    head_dim = C // config.num_heads
    x_norm = Normalize(x)
    q = DenseGeneral(x_norm, (num_heads, head_dim))
    k = DenseGeneral(x_norm, (num_heads, head_dim))
    v = DenseGeneral(x_norm, (num_heads, head_dim))
    q = NormalizeWithBias(q)
    k = NormalizeWithBias(k)
    q = q * q.shape[-1] ** -0.5
    weights = einsum("bqhd,bkhd -> bhqk", q, k)
    weights = softmax(weights)
    attn_vals = einsum("bhqk,bkhd -> bqhd", weights, v)
    out = DenseGeneral(attn_vals, C, axis=(-2, -1), kernel_init=zeros)
    return out

def transformer_block(x, text_emb, emb):
    x += mlp_block(x, emb)
    x += self_attention(x, text_emb)
    return x

Một khối quan trọng khác là ResBlock tiêu chuẩn, mã giả được đưa ra dưới đây:

def resnet_block(x, emb, skip_h=None):
    B, H, W, C = x.shape
    h = NormalizeWithBias(x)
    if skip_h is not None:
        skip_h = NormalizeWithBias(skip_h)
        h = (h + skip_h) / sqrt(2)
    h = swish(h)
    h = Conv2D(h, out_ch, (3, 3), (1, 1))
    emb_out = Dense(emb, 2 * out_ch)[:, None, None, :]
    scale, shift = split(emb_out, 2, axis=-1)
    h = NormalizeWithBias(h) * (1 + scale) + shift
    h = swish(h)
    h = Conv2D(h, out_ch, (3, 3), (1, 1), kernel_init=zeros)
    return x + h

Cho các khối xây dựng này, có thể định nghĩa kiến trúc U-ViT:

def uvit(x, logsnr):
    B, H, W, C = x.shape
    emb = get_logsnr_emb(logsnr)
    h0 = EmbedInput(config.base_channels * config.channel_multiplier[0])(x)
    hs = []
    last_h = h0
    # Đường xuống.
    for i_level in range(len(config.num_res_blocks)):
        for i_block in range(config.num_res_blocks[i_level]):
            last_h = resnet_block(last_h, emb)
        hs.append(last_h)
        last_h = downsample(
            last_h, config.base_channels * config.channel_multiplier[i_level+1])
    # Transformer.
    last_h = last_h.reshape(B, H * W, C)
    last_h += param("pos_emb", initializers.normal(0.01), last_h.shape[1:])[None]
    for _ in range(config.num_transformer_blocks):
        last_h = transformer_block(last_h, text_emb, emb)
    last_h = last_h.reshape(B, H, W, C)
    # Đường lên.
    for i_level in reversed(range(len(config.num_res_blocks))):
        last_h = upsample(last_h, config.base_channels * config.channel_multiplier[i_level])
        for i_block in range(config.num_res_blocks[i_level]):
            last_h = resnet_block(last_h, emb, skip_h=hs.pop())
    out = ProjectOutput(last_h, C)
    return out

Như có thể thấy, nó rất giống với UNet, phần giữa bây giờ là một transformer không có các lớp tích chập mà là các khối mlp chỉ với kết nối dư.

Tài nguyên tính toán Các mô hình U-Net nhỏ hơn có thể được huấn luyện trên 64 thiết bị TPUv2 với 1.15 bước mỗi giây (cho độ phân giải 256 không có patching, sự khác biệt nhỏ giữa các biến thể mô hình khác nhau) với batch size 512 trong 2000K bước (trừ khi được chỉ định khác). Các mô hình U-ViT lớn đều được huấn luyện bằng 128 thiết bị TPUv4 với 1.5 bước mỗi giây với batch size 2048 trong 500K bước.

--- TRANG 18 ---
simple diffusion

C. Thí nghiệm Bổ sung

Tỷ lệ hướng dẫn Trong Bảng 9, chúng tôi chỉ ra tác động của hướng dẫn trên các mô hình ImageNet. Đối với mức hướng dẫn tương đối nhỏ, các mẫu ngay lập tức có được nhiều IS với chi phí đặc biệt là eval FID. Hơn nữa, Hình 8 cho thấy điểm Clip so với MSCOCO FID30K cho mô hình văn bản thành hình ảnh. Theo những người khác như (Saharia et al., 2022), hình ảnh được lấy mẫu bằng cách điều kiện trên 30K văn bản được lấy mẫu ngẫu nhiên từ tập validation MSCOCO, tính toán so với toàn bộ tập validation làm tham chiếu.

Bảng 9: Tỷ lệ hướng dẫn, lịch trình dịch chuyển khá nhạy cảm với hướng dẫn.

U-ViT | ImageNet 128 | ImageNet 256 | ImageNet 512
hướng dẫn | FID train | FID eval | IS | FID train | FID eval | IS | FID train | FID eval | IS
1.00 | 1.94 | 3.23 | 171.9 ±3.2 | 2.77 | 3.75 | 211.8 ±2.9 | 3.54 | 4.53 | 205.3 ±2.7
1.05 | 2.05 | 3.57 | 189.9 ±3.5 | 2.46 | 3.80 | 235.3 ±4.9 | 3.14 | 4.43 | 228.5±4.2
1.10 | 2.35 | 4.10 | 207.0 ±3.5 | 2.44 | 4.08 | 256.3 ±5.0 | 3.02 | 4.60 | 248.7 ±3.4
1.20 | 3.24 | 5.36 | 237.6 ±3.6 | 2.96 | 5.10 | 289.8 ±4.1 | 3.33 | 5.43 | 284.6 ±2.8
1.40 | 5.58 | 8.26 | 285.2 ±2.0 | 4.69 | 7.50 | 342.2 ±5.1 | 4.97 | 7.89 | 339.9 ±3.8
1.80 | 9.77 | 13.06 | 340.1 ±3.6 | 8.21 | 11.81 | 398.0 ±5.4 | 8.38 | 12.15 | 401.7 ±5.2
2.00 | 11.47 | 14.96 | 359.2 ±5.6 | 9.59 | 13.44 | 416.4 ±4.7 | 9.68 | 13.68 | 416.2 ±4.8
3.00 | 15.85 | 19.75 | 399.2±2.9 | 13.61 | 18.00 | 455.7 ±4.2 | 13.79 | 18.42 | 461.4±5.0

Hình 8: Điểm Clip so với FID30K trên MSCOCO zero-shot ở độ phân giải 256×256. Đối với tỷ lệ hướng dẫn 1.00, 1.25, 1.40, 1.50, 2.0, 3.0, 4.0.

Thí nghiệm trên 1024 Để nghiên cứu các tác động của việc mở rộng vượt 512, chúng tôi chạy một thí nghiệm tương tự với U-Net trên ImageNet được thay đổi kích thước thành 1024 × 1024, mặc dù hầu hết hình ảnh nhỏ hơn độ phân giải đó. Ở đây, mất mát đa tỷ lệ có tác động thậm chí rõ rệt hơn, dẫn đến FID train được cải thiện đáng kể bằng cách sử dụng mất mát downsample (6.06 so với 8.10 không có). Hơn nữa, mô hình này đắt hơn vì patching 4×4 cho bản đồ đặc trưng độ phân giải 256.

--- TRANG 19 ---
simple diffusion

(a) Một render của một thành phố sáng và đầy màu sắc dưới một mái vòm

(b) Một gấu trúc đi bộ qua rừng rậm, nghệ thuật tương lai

(c) Một bức tranh siêu thực về một robot trượt ván

(d) Một bức tượng ếch làm bằng gỗ

(e) Một bức tranh về máy pha cà phê tương lai, màu sắc sống động

(f) Một hoa hướng dương đeo kính râm

(g) Một biển báo neon của một con bướm

(h) Một tờ báo của một con bướm

(i) Một quả bóng hình logo Google Brain

Hình 9: Mẫu văn bản thành hình ảnh ở độ phân giải 512×512. Mô hình này đã được chưng cất và kết quả là việc tạo ra một hình ảnh mất 0.42 giây trên TPUv4 (không bao gồm bộ mã hóa văn bản). Tương tự, việc tạo ra một batch 8 hình ảnh mất 2.00 giây.

--- TRANG 20 ---
simple diffusion

(a) Tỷ lệ hướng dẫn 4

(b) Tỷ lệ hướng dẫn 1 (Không hướng dẫn)

Hình 10: Mẫu ngẫu nhiên (không được chọn lọc) từ U-ViT trên ImageNet 256×256.
