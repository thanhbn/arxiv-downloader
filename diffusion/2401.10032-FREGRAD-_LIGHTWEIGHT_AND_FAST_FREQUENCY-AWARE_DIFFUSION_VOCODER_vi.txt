# FREGRAD: BỘ VOCODER KHUẾCH TÁN NHẸ VÀ NHANH NHẬN BIẾT TẦN SỐ

Tan Dat Nguyen∗, Ji-Hoon Kim∗, Youngjoon Jang, Jaehun Kim, Joon Son Chung
Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc

TÓM TẮT
Mục tiêu của bài báo này là tạo ra âm thanh thực tế với một vocoder dựa trên khuếch tán nhẹ và nhanh, có tên FreGrad. Khung làm việc của chúng tôi bao gồm ba thành phần chính sau: (1) Chúng tôi sử dụng biến đổi wavelet rời rạc phân tách dạng sóng phức tạp thành các wavelet băng con, giúp FreGrad hoạt động trên không gian đặc trưng đơn giản và gọn gàng, (2) Chúng tôi thiết kế một phép tích chập giãn nở nhận biết tần số nâng cao khả năng nhận biết tần số, dẫn đến việc tạo ra lời nói với thông tin tần số chính xác, và (3) Chúng tôi giới thiệu một bộ kỹ thuật giúp tăng chất lượng tạo sinh của mô hình được đề xuất. Trong thí nghiệm của chúng tôi, FreGrad đạt được thời gian huấn luyện nhanh hơn 3,7 lần và tốc độ suy luận nhanh hơn 2,2 lần so với baseline trong khi giảm kích thước mô hình xuống 0,6 lần (chỉ 1,78M tham số) mà không hy sinh chất lượng đầu ra. Các mẫu âm thanh có sẵn tại: https://mm.kaist.ac.kr/projects/FreGrad .

Từ khóa chỉ mục —tổng hợp tiếng nói, vocoder, mô hình nhẹ, khuếch tán, khuếch tán nhanh

1. GIỚI THIỆU

Neural vocoder nhằm tạo ra các dạng sóng có thể nghe được từ các đặc trưng âm thanh trung gian (ví dụ: mel-spectrogram). Nó trở thành một khối xây dựng thiết yếu của nhiều tác vụ liên quan đến lời nói bao gồm tổng hợp giọng hát [1, 2], chuyển đổi giọng nói [3, 4], và text-to-speech [5, 6, 7]. Các neural vocoder trước đây [8, 9] dựa trên kiến trúc tự hồi quy (AR), thể hiện khả năng tạo ra lời nói rất tự nhiên. Tuy nhiên, kiến trúc cố hữu của chúng đòi hỏi một số lượng lớn các phép toán tuần tự, dẫn đến tốc độ suy luận cực kỳ chậm. Nhiều nỗ lực trong việc tăng tốc quá trình suy luận đã được thực hiện trên kiến trúc không-AR dựa trên flow [10, 11], mạng đối nghịch tạo sinh [12, 13, 14], và xử lý tín hiệu [15, 16]. Trong khi các phương pháp như vậy đã tăng tốc độ suy luận, chúng thường tạo ra các dạng sóng chất lượng thấp hơn so với phương pháp AR. Trong số các vocoder không-AR, vocoder dựa trên khuếch tán gần đây đã thu hút sự chú ý ngày càng tăng do chất lượng tạo sinh đầy hứa hẹn [17, 18, 19, 20, 21, 22, 23].

Mặc dù có lời nói tổng hợp chất lượng cao, vocoder dựa trên khuếch tán gặp phải tốc độ hội tụ huấn luyện chậm, quá trình suy luận không hiệu quả và chi phí tính toán cao. Những yếu tố này cản trở việc sử dụng vocoder dựa trên khuếch tán trong các thiết bị tài nguyên thấp và ứng dụng của chúng trong các tình huống thực tế. Mặc dù nhiều công trình [19, 21, 24] đã cố gắng giảm thiểu thời gian huấn luyện và suy luận, vẫn còn một sự khám phá hạn chế để giảm chi phí tính toán.

Để giải quyết các vấn đề nêu trên một cách đồng thời, trong bài báo này, chúng tôi đề xuất một vocoder dựa trên khuếch tán mới có tên FreGrad, đạt được cả mức tiêu thụ bộ nhớ thấp và tốc độ xử lý nhanh trong khi duy trì chất lượng của âm thanh được tổng hợp. Chìa khóa của ý tưởng chúng tôi là phân tách dạng sóng phức tạp thành hai chuỗi băng con tần số đơn giản (tức là các đặc trưng wavelet), cho phép mô hình của chúng tôi tránh tính toán nặng. Để làm điều này, chúng tôi sử dụng biến đổi wavelet rời rạc (DWT) chuyển đổi dạng sóng phức tạp thành hai đặc trưng wavelet thưa thớt về tần số và giảm chiều mà không mất thông tin [25, 26]. FreGrad thành công trong việc giảm cả các tham số mô hình và thời gian xử lý khử nhiễu một cách đáng kể. Ngoài ra, chúng tôi giới thiệu một khối xây dựng mới, có tên là phép tích chập giãn nở nhận biết tần số (Freq-DConv), giúp nâng cao chất lượng đầu ra. Bằng cách tích hợp DWT vào lớp tích chập giãn nở, chúng tôi cung cấp bias quy nạp của thông tin tần số cho module, và do đó mô hình có thể học các phân bố phổ chính xác phục vụ như một chìa khóa cho tổng hợp âm thanh thực tế. Để nâng cao chất lượng hơn nữa, chúng tôi thiết kế phân bố prior cho từng đặc trưng wavelet, tích hợp biến đổi nhiễu thay thế lịch trình nhiễu không tối ưu, và tận dụng hàm mất mát độ lớn đa độ phân giải cung cấp phản hồi nhận biết tần số.

Trong kết quả thí nghiệm, chúng tôi chứng minh hiệu quả của FreGrad với các metrics mở rộng. FreGrad thể hiện sự nâng cao đáng chú ý trong việc tăng hiệu quả mô hình trong khi giữ chất lượng tạo sinh. Như được hiển thị trong Bảng 1, FreGrad tăng thời gian suy luận 2,2 lần và giảm kích thước mô hình 0,6 lần với điểm ý kiến trung bình (MOS) tương đương với các công trình hiện có.

2. KIẾN THỨC NỀN TẢNG

Mô hình xác suất khuếch tán khử nhiễu là một mô hình biến ẩn học phân bố dữ liệu bằng cách khử nhiễu một tín hiệu có nhiễu [27]. Quá trình thuận q(·) khuếch tán các mẫu dữ liệu thông qua các chuyển đổi Gaussian được tham số hóa với một quá trình Markov:

q(xt|xt−1) = N(xt; √(1−βt)xt−1, βtI),                    (1)

trong đó βt ∈ {β1, . . . , βT} là lịch trình nhiễu được định nghĩa trước, T là tổng số timestep, và x0 là mẫu ground truth. Hàm này cho phép lấy mẫu xt từ x0, có thể được công thức hóa như:

xt = √γtx0 + √(1−γt)ϵ,                                    (2)

trong đó γt = ∏t(i=1)(1−βi) và ϵ ∼ N(0,I).

Với T đủ lớn, phân bố của xT xấp xỉ một phân bố Gaussian đẳng hướng. Do đó, chúng ta có thể thu được một mẫu trong phân bố ground truth bằng cách truy vết quá trình nghịch đảo chính xác p(xt−1|xt) từ điểm khởi tạo xT ∼ N(0,I). Vì p(xt−1|xt) phụ thuộc vào toàn bộ phân bố dữ liệu, chúng ta xấp xỉ nó với một mạng neural pθ(xt−1|xt) được định nghĩa là N(xt−1; μθ(xt, t), σ²θ(xt, t)). Như được chỉ ra trong [27], phương sai σ²θ(·) có thể được biểu diễn là ((1−γt−1)/(1−γt))βt, và trung bình μθ(·) được cho bởi:

μθ(xt, t) = (1/√(1−βt))[xt − (βt/√(1−γt))ϵθ(xt, t)],        (3)

trong đó ϵθ(·) là một mạng neural học dự đoán nhiễu. Trong thực tế, mục tiêu huấn luyện cho ϵθ(·) được đơn giản hóa để tối thiểu hóa Et,xt,ϵ‖ϵ−ϵθ(xt, t)‖²₂. PriorGrad [20] mở rộng ý tưởng bằng cách bắt đầu quy trình lấy mẫu từ phân bố prior N(0,Σ). Ở đây, Σ là một ma trận chéo diag(σ²₀, σ²₁, . . . , σ²N), trong đó σ²ᵢ là năng lượng chuẩn hóa cấp frame thứ i của mel-spectrogram với độ dài N. Theo đó, hàm mất mát cho ϵθ(·) được sửa đổi như:

Ldiff = Et,xt,ϵ,c‖ϵ−ϵθ(xt, t, X)‖²Σ⁻¹,                      (4)

trong đó ‖x‖²Σ⁻¹ = x⊤Σ⁻¹x và X là một mel-spectrogram.

3. FREGRAD

Kiến trúc mạng của FreGrad có gốc rễ từ DiffWave [17] là một mạng backbone được sử dụng rộng rãi cho các vocoder dựa trên khuếch tán [20, 23]. Tuy nhiên, phương pháp của chúng tôi khác biệt ở chỗ nó hoạt động trên không gian đặc trưng wavelet gọn gàng và thay thế phép tích chập giãn nở hiện có bằng Freq-DConv được đề xuất để tái tạo các phân bố phổ chính xác.

3.1. Khử nhiễu Đặc trưng Wavelet

Để tránh tính toán phức tạp, chúng tôi sử dụng DWT trước quá trình thuận. DWT downsampling âm thanh chiều đích x0 ∈ R^L thành hai đặc trưng wavelet {x^l₀, x^h₀} ⊂ R^(L/2), mỗi cái đại diện cho các thành phần tần số thấp và cao. Như được chứng minh trong các công trình trước [26, 28], hàm này có thể phân tách một tín hiệu không dừng mà không mất thông tin do tính chất trực giao của nó. FreGrad hoạt động trên các đặc trưng wavelet đơn giản. Tại mỗi bước huấn luyện, các đặc trưng wavelet x^l₀ và x^h₀ được khuếch tán thành các đặc trưng nhiễu tại timestep t với nhiễu riêng biệt ϵl và ϵh, và mỗi nhiễu được đồng thời xấp xỉ bởi một mạng neural ϵθ(·). Trong quá trình nghịch đảo, FreGrad đơn giản tạo ra các đặc trưng wavelet đã khử nhiễu, {x̂^l₀, x̂^h₀} ⊂ R^(L/2), cuối cùng được chuyển đổi thành dạng sóng chiều đích x̂₀ ∈ R^L bằng DWT nghịch đảo (iDWT):

x̂₀ = Φ⁻¹(x̂^l₀, x̂^h₀),                                    (5)

trong đó Φ⁻¹(·) biểu thị hàm iDWT.

Lưu ý rằng FreGrad tạo ra lời nói với tính toán nhỏ hơn do việc phân tách các dạng sóng phức tạp. Ngoài ra, mô hình duy trì chất lượng tổng hợp của nó, vì iDWT đảm bảo tái tạo lại dạng sóng từ các đặc trưng wavelet mà không mất mát [28, 29]. Trong thí nghiệm của chúng tôi, chúng tôi áp dụng Haar wavelet [30].

3.2. Phép Tích chập Giãn nở Nhận biết Tần số

Vì âm thanh là một hỗn hợp phức tạp của các tần số khác nhau [26], việc tái tạo các phân bố tần số chính xác là quan trọng cho tổng hợp âm thanh tự nhiên. Để nâng cao chất lượng tổng hợp, chúng tôi đề xuất Freq-DConv hướng dẫn mô hình một cách có chủ ý chú ý đến thông tin tần số. Như được minh họa trong Hình 3, chúng tôi áp dụng DWT để phân tách tín hiệu ẩn y ∈ R^(L/2×D) thành hai băng con {yl, yh} ⊂ R^(L/4×D) với chiều ẩn D. Các băng con được nối theo kênh, và phép tích chập giãn nở f(·) sau đó trích xuất một đặc trưng nhận biết tần số yhidden ∈ R^(L/4×2D):

yhidden = f(cat(yl, yh)),                                   (6)

trong đó cat biểu thị phép toán nối. Đặc trưng được trích xuất yhidden sau đó được chia đôi thành {y'l, y'h} ⊂ R^(L/4×D) theo chiều kênh, và cuối cùng iDWT chuyển đổi các đặc trưng trừu tượng thành biểu diễn ẩn đơn để khớp độ dài với đặc trưng đầu vào y:

y' = Φ⁻¹(y'l, y'h),                                       (7)

trong đó y' ∈ R^(L/2×D) đại diện cho đầu ra của Freq-DConv. Như được mô tả trong Hình 2, chúng tôi nhúng Freq-DConv vào mỗi ResBlock.

Mục đích của việc phân tách tín hiệu ẩn trước phép tích chập giãn nở là tăng trường tiếp nhận dọc theo trục thời gian mà không thay đổi kích thước kernel. Kết quả của DWT, mỗi đặc trưng wavelet có chiều thời gian giảm trong khi bảo toàn tất cả các tương quan thời gian. Điều này giúp mỗi lớp tích chập sở hữu một trường tiếp nhận lớn hơn dọc theo chiều thời gian ngay cả với cùng kích thước kernel. Hơn nữa, các băng con tần số thấp và cao của mỗi đặc trưng ẩn có thể được khám phá riêng biệt. Kết quả là, chúng ta có thể cung cấp một bias quy nạp của thông tin tần số cho mô hình, giúp tạo ra dạng sóng nhất quán về tần số. Chúng tôi xác minh hiệu quả của Freq-DConv trong Mục 4.3.

3.3. Bộ Kỹ thuật cho Chất lượng

Phân bố prior. Như được chứng minh trong các công trình trước [20, 22], một phân bố prior dựa trên spectrogram có thể nâng cao đáng kể hiệu suất khử nhiễu dạng sóng ngay cả với ít bước lấy mẫu hơn. Dựa trên điều này, chúng tôi thiết kế một phân bố prior cho mỗi chuỗi wavelet dựa trên mel-spectrogram. Vì mỗi chuỗi băng con chứa thông tin tần số thấp hoặc cao cụ thể, chúng tôi sử dụng phân bố prior riêng biệt cho mỗi đặc trưng wavelet. Cụ thể, chúng tôi chia mel-spectrogram thành hai đoạn dọc theo chiều tần số và áp dụng kỹ thuật được đề xuất trong [20] để có được các phân bố prior riêng biệt {σl, σh} từ mỗi đoạn.

Biến đổi lịch trình nhiễu. Như được thảo luận trong [31, 32], tỷ lệ tín hiệu trên nhiễu (SNR) lý tưởng nên bằng không tại timestep cuối T của quá trình thuận. Tuy nhiên, các lịch trình nhiễu được áp dụng trong các công trình trước [17, 18, 20] không đạt được SNR gần không tại bước cuối, như được hiển thị trong Hình 4. Để đạt được SNR bằng không tại bước cuối, chúng tôi áp dụng thuật toán được đề xuất trong [32], có thể được công thức hóa như sau:

√γnew = √γ0/√(γ0−√γT+τ)(√γ−√γT+τ),                    (8)

trong đó τ giúp tránh chia cho không trong quá trình lấy mẫu.

Hàm mất mát. Một mục tiêu huấn luyện phổ biến của vocoder khuếch tán là tối thiểu hóa chuẩn L2 giữa nhiễu dự đoán và ground truth, thiếu phản hồi rõ ràng trong khía cạnh tần số. Để cung cấp phản hồi nhận biết tần số cho mô hình, chúng tôi thêm mất mát độ lớn biến đổi Fourier thời gian ngắn đa độ phân giải (STFT) (Lmag). Khác với các công trình trước [14, 24], FreGrad chỉ sử dụng phần độ lớn vì chúng tôi thấy thực nghiệm rằng việc tích hợp mất mát hội tụ phổ làm giảm chất lượng đầu ra. Cho M là số lượng mất mát STFT, thì Lmag có thể được biểu diễn như:

Lmag = (1/M) ∑(i=1 to M) L(i)mag,                         (9)

trong đó L(i)mag là mất mát độ lớn STFT từ cài đặt phân tích thứ i [14]. Chúng tôi áp dụng riêng biệt mất mát khuếch tán cho các băng con tần số thấp và cao, và mục tiêu huấn luyện cuối cùng được định nghĩa là:

Lfinal = ∑(i∈{l,h}) [Ldiff(ϵi, ϵ̂i) + λLmag(ϵi, ϵ̂i)],      (10)

trong đó ϵ̂ đề cập đến nhiễu được ước tính.

4. THÍ NGHIỆM

4.1. Thiết lập Huấn luyện

Chúng tôi tiến hành thí nghiệm trên một người nói tiếng Anh duy nhất LJSpeech¹ chứa 13,100 mẫu. Chúng tôi sử dụng 13,000 mẫu ngẫu nhiên để huấn luyện và 100 mẫu còn lại để kiểm tra. Mel-spectrogram được tính toán từ âm thanh ground truth với 80 bộ lọc mel, 1,024 điểm FFT trong khoảng từ 80Hz đến 8,000Hz, và hop length là 256. FreGrad được so sánh với các vocoder khuếch tán hoạt động tốt nhất có sẵn công khai: WaveGrad², DiffWave³, và PriorGrad⁴. Để so sánh công bằng, tất cả các mô hình được huấn luyện đến 1M bước, và tất cả các âm thanh được tạo ra thông qua 50 bước khuếch tán là cài đặt mặc định trong DiffWave [17] và PriorGrad [20].

FreGrad bao gồm 30 khối residual nhận biết tần số với độ dài chu kỳ giãn nở là 7 và chiều ẩn là 32. Chúng tôi tuân theo cách triển khai của DiffWave [17] cho embedding timestep và mel upsampler nhưng giảm tỷ lệ upsampling một nửa vì độ dài thời gian được DWT giảm một nửa. Đối với Lmag, chúng tôi đặt M = 3 với kích thước FFT là [512,1024,2048] và kích thước cửa sổ là [240,600,1200]. Chúng tôi chọn τ = 0.0001 và λ = 0.1 cho Phương trình (8) và Phương trình (10), tương ứng. Chúng tôi sử dụng trình tối ưu hóa Adam với β1 = 0.9, β2 = 0.999, tốc độ học cố định là 0.0002, và kích thước batch là 16.

4.2. Chất lượng Âm thanh và Tốc độ Lấy mẫu

Chúng tôi xác minh hiệu quả của FreGrad trên các metrics khác nhau. Để đánh giá chất lượng âm thanh, chúng tôi thu được độ méo mel-cepstral (MCD 13) và MOS 5 thang đo nơi 25 đối tượng đánh giá tính tự nhiên của 50 mẫu âm thanh. Ngoài ra, chúng tôi tính toán sai số tuyệt đối trung bình (MAE), sai số trung bình bình phương f0 (RMSE f0), và sai số STFT đa độ phân giải (MR-STFT) giữa âm thanh được tạo ra và ground truth. Để so sánh hiệu quả mô hình, chúng tôi tính toán số lượng tham số mô hình (#params) và hệ số thời gian thực (RTF) được đo trên CPU AMD EPYC 7452 và một GPU GeForce RTX 3080. Ngoại trừ MOS, tất cả các metrics được thu từ 100 mẫu âm thanh.

Như được chứng minh trong Bảng 1, FreGrad giảm mạnh không chỉ số lượng tham số mô hình mà còn tốc độ suy luận trên cả CPU và GPU. Ngoài ra, FreGrad đạt được kết quả tốt nhất trong tất cả các metrics đánh giá chất lượng ngoại trừ MOS. Do độ nhạy cảm cao của con người với âm thanh tần số thấp, chúng tôi giả thuyết rằng sự suy giảm MOS trong FreGrad là do phân bố tần số thấp. Tuy nhiên, trong góc nhìn của toàn bộ phổ tần số, FreGrad liên tục thể hiện hiệu suất vượt trội so với các phương pháp hiện có, được xác nhận bởi MAE, MR-STFT, MCD 13, và RMSE f0. Phân tích trực quan mel-spectrogram (Hình 5) cũng chứng minh hiệu quả của FreGrad trong việc tái tạo các phân bố tần số chính xác. Ngoài ra, FreGrad có lợi thế đáng kể về thời gian huấn luyện nhanh. Nó đòi hỏi 46 giờ GPU để hội tụ, nhanh hơn 3,7 lần so với PriorGrad với 170 giờ GPU.

4.3. Nghiên cứu Loại bỏ về Các Thành phần Được Đề xuất

Để xác minh hiệu quả của từng thành phần FreGrad, chúng tôi tiến hành các nghiên cứu loại bỏ bằng cách sử dụng MOS so sánh (CMOS), RMSE f0, và RTF. Trong kiểm tra CMOS, người đánh giá được yêu cầu so sánh chất lượng của các mẫu âm thanh từ hai hệ thống từ −3 đến +3. Như có thể thấy trong Bảng 2, mỗi thành phần độc lập đóng góp vào việc nâng cao chất lượng tổng hợp của FreGrad. Đặc biệt, việc sử dụng Freq-DConv nâng cao đáng kể chất lượng với một sự đánh đổi nhỏ về tốc độ suy luận, nơi RTF tăng lên vẫn vượt trội so với các phương pháp hiện có. Chất lượng tạo sinh cho thấy sự suy giảm tương đối nhỏ nhưng đáng chú ý khi các kỹ thuật prior riêng biệt và zero SNR được đề xuất không được áp dụng. Việc không có Lmag dẫn đến hiệu suất tệ nhất về RMSE f0, điều này cho thấy rằng Lmag cung cấp phản hồi nhận biết tần số hiệu quả.

5. KẾT LUẬN

Chúng tôi đã đề xuất FreGrad, một vocoder dựa trên khuếch tán nhẹ và nhanh. FreGrad hoạt động trên không gian đặc trưng wavelet đơn giản và gọn gàng bằng cách áp dụng một phương pháp phân tách không mất mát. Mặc dù có chi phí tính toán nhỏ, FreGrad có thể bảo toàn chất lượng tổng hợp của nó với sự hỗ trợ của Freq-DConv và bộ kỹ thuật, được thiết kế cụ thể cho các vocoder dựa trên khuếch tán. Các thí nghiệm mở rộng chứng minh rằng FreGrad cải thiện đáng kể hiệu quả mô hình mà không làm giảm chất lượng đầu ra. Hơn nữa, chúng tôi xác minh hiệu quả của từng thành phần FreGrad bằng các nghiên cứu loại bỏ. Hiệu quả của FreGrad cho phép sản xuất âm thanh giống con người ngay cả trên các thiết bị biên với tài nguyên tính toán hạn chế.

6. TÀI LIỆU THAM KHẢO

[1] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, và Zhou Zhao, "DiffSinger: Tổng hợp giọng hát thông qua cơ chế khuếch tán nông," trong Proc. AAAI, 2022.

[2] Yi Ren, Xu Tan, Tao Qin, Jian Luan, Zhou Zhao, và Tie-Yan Liu, "DeepSinger: Tổng hợp giọng hát với dữ liệu khai thác từ web," trong Proc. KDD, 2020.

[3] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, và Mark Hasegawa-Johnson, "AutoVC: Chuyển đổi phong cách giọng nói zero-shot chỉ với mất mát autoencoder," trong Proc. ICML, 2019.

[4] Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Lee, Hoon Heo, và Kyogu Lee, "Phân tích và tổng hợp neural: Tái tạo lời nói từ các biểu diễn tự giám sát," trong NeurIPS, 2021.

[5] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ-Skerrv Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, và Yonghui Wu, "Tổng hợp TTS tự nhiên bằng cách điều kiện wavenet trên dự đoán mel spectrogram," trong Proc. ICASSP, 2018.

[6] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, và Mikhail A. Kudinov, "Grad-TTS: Một mô hình xác suất khuếch tán cho text-to-speech," trong Proc. ICML, 2021.

[7] Jaehyeon Kim, Sungwon Kim, Jungil Kong, và Sungroh Yoon, "Glow-TTS: Một flow tạo sinh cho text-to-speech thông qua tìm kiếm căn chỉnh đơn điệu," trong NeurIPS, 2020.

[8] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, và Koray Kavukcuoglu, "WaveNet: Một mô hình tạo sinh cho âm thanh thô," trong Proc. SSW, 2016.

[9] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C. Courville, và Yoshua Bengio, "SampleRNN: Một mô hình tạo sinh âm thanh neural không điều kiện end-to-end," trong Proc. ICLR, 2017.

[10] Ryan Prenger, Rafael Valle, và Bryan Catanzaro, "WaveGlow: Một mạng tạo sinh dựa trên flow cho tổng hợp lời nói," trong Proc. ICASSP, 2019.

[11] Wei Ping, Kainan Peng, Kexin Zhao, và Zhao Song, "WaveFlow: Một mô hình flow compact cho âm thanh thô," trong Proc. ICML, 2020.

[12] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brébisson, Yoshua Bengio, và Aaron C. Courville, "MelGAN: Mạng đối nghịch tạo sinh cho tổng hợp dạng sóng có điều kiện," trong NeurIPS, 2019.

[13] Jesse H. Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, và Adam Roberts, "GANSynth: Tổng hợp âm thanh neural đối nghịch," trong Proc. ICLR, 2019.

[14] Ryuichi Yamamoto, Eunwoo Song, và Jae-Min Kim, "Parallel Wavegan: Một mô hình tạo sinh dạng sóng nhanh dựa trên mạng đối nghịch tạo sinh với spectrogram đa độ phân giải," trong Proc. ICASSP, 2020.

[15] Lauri Juvela, Bajibabu Bollepalli, Vassilis Tsiaras, và Paavo Alku, "GlotNet - Một mô hình dạng sóng thô cho kích thích thanh quản trong tổng hợp lời nói tham số thống kê," IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 27, no. 6, pp. 1019–1030, 2019.

[16] Takuhiro Kaneko, Kou Tanaka, Hirokazu Kameoka, và Shogo Seki, "iSTFTNET: Vocoder mel-spectrogram nhanh và nhẹ tích hợp biến đổi fourier thời gian ngắn nghịch đảo," trong Proc. ICASSP, 2022.

[17] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, và Bryan Catanzaro, "DiffWave: Một mô hình khuếch tán đa năng cho tổng hợp âm thanh," trong Proc. ICLR, 2021.

[18] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, và William Chan, "WaveGrad: Ước tính gradient cho tạo sinh dạng sóng," trong Proc. ICLR, 2021.

[19] Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, và Zhou Zhao, "FastDiff: Một mô hình khuếch tán có điều kiện nhanh cho tổng hợp lời nói chất lượng cao," trong Proc. IJCAI, 2022.

[20] Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, và Tie-Yan Liu, "PriorGrad: Cải thiện các mô hình khuếch tán khử nhiễu có điều kiện với prior thích ứng phụ thuộc dữ liệu," trong Proc. ICLR, 2022.

[21] Max W. Y. Lam, Jun Wang, Dan Su, và Dong Yu, "BDDM: Các mô hình khuếch tán khử nhiễu song phương cho tổng hợp lời nói nhanh và chất lượng cao," trong Proc. ICLR, 2022.

[22] Yuma Koizumi, Heiga Zen, Kohei Yatabe, Nanxin Chen, và Michiel Bacchiani, "SpecGrad: Vocoder neural dựa trên mô hình xác suất khuếch tán với định hình phổ nhiễu thích ứng," trong Proc. Interspeech, 2022.

[23] Naoya Takahashi, Mayank Kumar, Singh, và Yuki Mitsufuji, "Các mô hình khuếch tán phân cấp cho vocoder neural giọng hát," trong Proc. ICASSP, 2023.

[24] Zehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo P. Mandic, Lei He, và Sheng Zhao, "InferGrad: Cải thiện các mô hình khuếch tán cho vocoder bằng cách xem xét suy luận trong huấn luyện," trong Proc. ICASSP, 2022.

[25] Ingrid Daubechies, Ten Lectures on Wavelets, SIAM, 1992.

[26] Ji-Hoon Kim, Sang-Hoon Lee, Ji-Hyun Lee, và Seong-Whan Lee, "Fre-GAN: Tổng hợp âm thanh đối nghịch nhất quán về tần số," trong Proc. Interspeech, 2021.

[27] Jonathan Ho, Ajay Jain, và Pieter Abbeel, "Các mô hình xác suất khuếch tán khử nhiễu," trong NeurIPS, 2020.

[28] Sang-Hoon Lee, Ji-Hoon Kim, Kangeun Lee, và Seong-Whan Lee, "Fre-GAN 2: Tổng hợp âm thanh nhất quán về tần số nhanh và hiệu quả," trong Proc. ICASSP, 2022.

[29] Julien Reichel, Gloria Menegaz, Marcus J Nadenau, và Murat Kunt, "Biến đổi wavelet số nguyên cho nén ảnh nhúng từ mất mát đến không mất mát," IEEE Trans. on Image Processing, vol. 10, no. 3, pp. 383–392, 2001.

[30] Alfred Haar, Zur theorie der orthogonalen funktionensysteme, Georg-August-Universitat, Gottingen., 1909.

[31] Emiel Hoogeboom, Jonathan Heek, và Tim Salimans, "Khuếch tán đơn giản: Khuếch tán end-to-end cho hình ảnh độ phân giải cao," trong Proc. ICML, 2023.

[32] Shanchuan Lin, Bingchen Liu, Jiashi Li, và Xiao Yang, "Các lịch trình nhiễu khuếch tán phổ biến và các bước mẫu có lỗi," trong Proc. WACV, 2024.
