# Căn chỉnh Mô hình Khuếch tán Sử dụng Tối ưu hóa Sở thích Trực tiếp

Bram Wallace1 Meihua Dang2 Rafael Rafailov2 Linqi Zhou2 Aaron Lou2
Senthil Purushwalkam1 Stefano Ermon2 Caiming Xiong1 Shafiq Joty1
Nikhil Naik1

1Salesforce AI, 2Đại học Stanford
{b.wallace,spurushwalkam,cxiong,sjoty,nnaik}@salesforce.com
{mhdang,rafailov,lzhou907,aaronlou}@stanford.edu {ermon}@cs.stanford.edu

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) được tinh chỉnh sử dụng dữ liệu so sánh của con người với các phương pháp Học tăng cường từ Phản hồi của Con người (RLHF) để làm cho chúng phù hợp hơn với sở thích của người dùng. Ngược lại với LLM, việc học sở thích của con người chưa được khám phá rộng rãi trong các mô hình khuếch tán văn bản thành hình ảnh; cách tiếp cận tốt nhất hiện có là tinh chỉnh một mô hình đã được huấn luyện trước sử dụng hình ảnh và chú thích chất lượng cao được tuyển chọn cẩn thận để cải thiện tính hấp dẫn trực quan và căn chỉnh văn bản. Chúng tôi đề xuất Diffusion-DPO, một phương pháp để căn chỉnh các mô hình khuếch tán theo sở thích của con người bằng cách tối ưu hóa trực tiếp trên dữ liệu so sánh của con người. Diffusion-DPO được điều chỉnh từ Tối ưu hóa Sở thích Trực tiếp (DPO) [33] được phát triển gần đây, một giải pháp thay thế đơn giản hơn cho RLHF trực tiếp tối ưu hóa chính sách thỏa mãn tốt nhất sở thích của con người dưới mục tiêu phân loại. Chúng tôi tái cấu trúc DPO để giải thích cho khái niệm khả năng xảy ra của mô hình khuếch tán, sử dụng cận dưới bằng chứng để đưa ra mục tiêu có thể vi phân. Sử dụng bộ dữ liệu Pick-a-Pic gồm 851K sở thích theo cặp được thu thập từ cộng đồng, chúng tôi tinh chỉnh mô hình cơ sở của mô hình Stable Diffusion XL (SDXL)-1.0 tiên tiến nhất với Diffusion-DPO. Mô hình cơ sở được tinh chỉnh của chúng tôi vượt trội đáng kể so với cả SDXL-1.0 cơ sở và mô hình SDXL-1.0 lớn hơn bao gồm mô hình tinh chỉnh bổ sung trong đánh giá của con người, cải thiện tính hấp dẫn trực quan và căn chỉnh lời nhắc. Chúng tôi cũng phát triển một biến thể sử dụng phản hồi AI và có hiệu suất tương đương với việc huấn luyện trên sở thích của con người, mở ra cánh cửa cho việc mở rộng các phương pháp căn chỉnh mô hình khuếch tán.

## 1. Giới thiệu

Các mô hình khuếch tán văn bản thành hình ảnh đã là tiên tiến nhất trong tạo sinh hình ảnh trong vài năm qua. Chúng thường được huấn luyện trong một giai đoạn duy nhất, sử dụng bộ dữ liệu quy mô web của các cặp văn bản-hình ảnh bằng cách áp dụng mục tiêu khuếch tán. Điều này trái ngược với phương pháp huấn luyện tiên tiến nhất cho các Mô hình Ngôn ngữ Lớn (LLM). Các LLM hiệu suất tốt nhất [28, 48] được huấn luyện theo hai giai đoạn. Trong giai đoạn đầu ("tiền huấn luyện"), chúng được huấn luyện trên dữ liệu quy mô web lớn. Trong giai đoạn thứ hai ("căn chỉnh"), chúng được tinh chỉnh để làm cho chúng phù hợp hơn với sở thích của con người. Căn chỉnh thường được thực hiện bằng cách sử dụng tinh chỉnh có giám sát (SFT) và Học tăng cường từ Phản hồi của Con người (RLHF) sử dụng dữ liệu sở thích. Các LLM được huấn luyện bằng quy trình hai giai đoạn này đã đặt ra tiêu chuẩn tiên tiến trong các tác vụ tạo sinh ngôn ngữ và đã được triển khai trong các ứng dụng thương mại như ChatGPT và Bard.

Mặc dù quy trình căn chỉnh LLM thành công, hầu hết các pipeline huấn luyện khuếch tán văn bản thành hình ảnh không kết hợp việc học từ sở thích của con người. Một số mô hình [9, 35, 36], thực hiện huấn luyện hai giai đoạn, trong đó tiền huấn luyện quy mô lớn được theo sau bởi tinh chỉnh trên bộ dữ liệu cặp văn bản-hình ảnh chất lượng cao để thiên vị chiến lược quá trình tạo sinh. Cách tiếp cận này ít mạnh mẽ và linh hoạt hơn nhiều so với các phương pháp căn chỉnh giai đoạn cuối của LLM. Các cách tiếp cận gần đây [6, 7, 11, 31] phát triển những cách tiên tiến hơn để điều chỉnh các mô hình khuếch tán theo sở thích của con người, nhưng không có cách nào đã chứng minh khả năng tổng quát hóa ổn định sang một cài đặt từ vựng hoàn toàn mở trên một loạt phản hồi. Các phương pháp dựa trên RL [6, 11] rất hiệu quả cho các tập lời nhắc hạn chế, nhưng hiệu quả của chúng giảm khi từ vựng mở rộng. Các phương pháp khác [7, 31] sử dụng gradient cấp pixel từ các mô hình phần thưởng trên các thế hệ để điều chỉnh các mô hình khuếch tán, nhưng gặp phải sự sụp đổ mode và chỉ có thể được huấn luyện trên một tập hợp tương đối hẹp các loại phản hồi.

Chúng tôi giải quyết khoảng trống này trong căn chỉnh mô hình khuếch tán lần đầu tiên, phát triển một phương pháp để tối ưu hóa trực tiếp các mô hình khuếch tán trên dữ liệu sở thích của con người. Chúng tôi tổng quát hóa Tối ưu hóa Sở thích Trực tiếp (DPO) [33], trong đó một mô hình tạo sinh được huấn luyện trên dữ liệu sở thích theo cặp của con người để ước tính ngầm một mô hình phần thưởng. Chúng tôi định nghĩa một khái niệm về khả năng xảy ra dữ liệu dưới mô hình khuếch tán trong một công thức mới và đưa ra một tổn thất đơn giản nhưng hiệu quả dẫn đến huấn luyện sở thích ổn định và hiệu quả, được gọi là Diffusion-DPO. Chúng tôi kết nối công thức này với một cách tiếp cận RL đa bước trong cùng một cài đặt như công việc hiện có [6, 11].

Chúng tôi chứng minh hiệu quả của Diffusion-DPO bằng cách tinh chỉnh các mô hình khuếch tán văn bản thành hình ảnh tiên tiến, chẳng hạn như Stable Diffusion XL (SDXL)-1.0 [30]. Các đánh giá viên con người ưa thích hình ảnh SDXL được điều chỉnh DPO hơn mô hình SDXL-(cơ sở + tinh chỉnh) 69% thời gian trên bộ dữ liệu PartiPrompts, đại diện cho tiêu chuẩn tiên tiến trong các mô hình văn bản thành hình ảnh được đo bằng sở thích của con người. Các thế hệ ví dụ được hiển thị trong Hình 1. Cuối cùng, chúng tôi cho thấy rằng việc học từ phản hồi AI (thay vì sở thích của con người) sử dụng mục tiêu Diffusion-DPO cũng hiệu quả, một cài đặt mà các công việc trước đây đã không thành công [7]. Tóm lại, chúng tôi giới thiệu một mô hình mới để học từ sở thích của con người cho các mô hình khuếch tán và trình bày mô hình tiên tiến kết quả.

## 2. Công việc liên quan

**Căn chỉnh Mô hình Ngôn ngữ Lớn** LLM thường được căn chỉnh theo sở thích của con người bằng cách sử dụng tinh chỉnh có giám sát trên dữ liệu demo, theo sau bởi RLHF. RLHF bao gồm huấn luyện một hàm phần thưởng từ dữ liệu so sánh trên đầu ra mô hình để đại diện cho sở thích của con người và sau đó sử dụng học tăng cường để căn chỉnh mô hình chính sách. Công việc trước đây [4, 26, 29, 47] đã sử dụng các phương pháp gradient chính sách [27, 38] cho mục đích này. Các phương pháp này thành công, nhưng đắt đỏ và đòi hỏi điều chỉnh siêu tham số rộng rãi [34, 59], và có thể dễ bị hack phần thưởng [10, 12, 41]. Các cách tiếp cận thay thế lấy mẫu câu trả lời mô hình cơ sở và chọn dựa trên phần thưởng dự đoán [3, 5, 14] để sử dụng cho huấn luyện có giám sát [2, 16, 50]. Các phương pháp tinh chỉnh mô hình chính sách trực tiếp trên dữ liệu phản hồi [1, 10], hoặc sử dụng tổn thất xếp hạng trên dữ liệu sở thích để huấn luyện trực tiếp mô hình chính sách [33, 49, 57, 58] đã xuất hiện. Tập hợp phương pháp sau này phù hợp với RLHF về hiệu suất. Chúng tôi xây dựng trên các phương pháp tinh chỉnh này trong công việc này, cụ thể là tối ưu hóa sở thích trực tiếp [33] (DPO). Cuối cùng, học từ phản hồi AI, sử dụng các mô hình phần thưởng đã được huấn luyện trước, có triển vọng cho việc mở rộng hiệu quả của căn chỉnh [4, 22].

**Căn chỉnh Mô hình Khuếch tán** Căn chỉnh các mô hình khuếch tán theo sở thích của con người cho đến nay đã được khám phá ít hơn nhiều so với trường hợp LLM. Nhiều cách tiếp cận [30, 36] tinh chỉnh trên các bộ dữ liệu được ghi điểm là có tính hấp dẫn trực quan cao bởi một bộ phân loại thẩm mỹ [37], để thiên vị mô hình đến các thế hệ hấp dẫn trực quan. Emu [9] tinh chỉnh một mô hình được huấn luyện trước sử dụng một bộ dữ liệu hình ảnh nhỏ, được tuyển chọn của các bức ảnh chất lượng cao với chú thích chi tiết được viết thủ công để cải thiện tính hấp dẫn trực quan và căn chỉnh văn bản. Các phương pháp khác [15, 39] tái chú thích các bộ dữ liệu hình ảnh được thu thập từ web hiện có để cải thiện độ trung thực văn bản. Các mô hình ghi điểm sở thích của con người nhận thức chú thích được huấn luyện trên các bộ dữ liệu sở thích thế hệ [21, 52, 55], nhưng tác động của các mô hình phần thưởng này đến không gian tạo sinh đã bị hạn chế. DOODL [51] giới thiệu tác vụ cải thiện thẩm mỹ một thế hệ duy nhất một cách lặp lại tại thời gian suy luận. DRAFT [7] và AlignProp [31], kết hợp một cách tiếp cận tương tự vào huấn luyện: điều chỉnh mô hình tạo sinh để tăng trực tiếp phần thưởng của các hình ảnh được tạo ra. Các phương pháp này hoạt động tốt cho các tiêu chí hấp dẫn trực quan đơn giản, nhưng thiếu ổn định và không hoạt động trên các phần thưởng tinh tế hơn như căn chỉnh văn bản-hình ảnh từ mô hình CLIP. DPOK và DDPO [6, 11] là các cách tiếp cận dựa trên RL để tối đa hóa tương tự phần thưởng được ghi điểm (với các ràng buộc phân phối) trên một tập từ vựng tương đối hạn chế; hiệu suất của các phương pháp này giảm khi số lượng lời nhắc huấn luyện/kiểm tra tăng. Diffusion-DPO là duy nhất trong số các cách tiếp cận căn chỉnh trong việc tăng hiệu quả sức hấp dẫn của con người được đo trên một từ vựng mở (DPOK, DDPO), mà không tăng thời gian suy luận (DOODL) trong khi duy trì đảm bảo phân phối và cải thiện căn chỉnh văn bản-hình ảnh chung ngoài tính hấp dẫn trực quan (DRAFT, AlignProp). (xem Tab. 1, thảo luận thêm trong Supp. S1).

## 3. Nền tảng

### 3.1. Mô hình Khuếch tán

Cho các mẫu từ phân phối dữ liệu q(x₀), hàm lập lịch nhiễu αₜ và σₜ (như được định nghĩa trong [36]), các mô hình khuếch tán khử nhiễu [17, 42, 46] là các mô hình tạo sinh p_θ(x₀) có quá trình ngược thời gian rời rạc với cấu trúc Markov p_θ(x₀:T) = ∏ᵀₜ₌₁ p_θ(xₜ₋₁|xₜ) trong đó

p_θ(xₜ₋₁|xₜ) = N(xₜ₋₁; μ_θ(xₜ), σ²ₜ|ₜ₋₁σ²ₜ₋₁/σ²ₜ I). (1)

Huấn luyện được thực hiện bằng cách tối thiểu hóa cận dưới bằng chứng (ELBO) liên quan đến mô hình này [20, 45]:

L_DM = E_x₀,ε,t,xₜ[ω(λₜ)||ε - ε_θ(xₜ, t)||²₂], (2)

với ε ~ N(0,I), t ~ U(0,T), xₜ ~ q(xₜ|x₀) = N(xₜ; αₜx₀, σ²ₜI). λₜ = α²ₜ/σ²ₜ là tỷ lệ tín hiệu trên nhiễu [20], ω(λₜ) là hàm trọng số được chỉ định trước (thường được chọn là hằng số [17, 44]).

### 3.2. Tối ưu hóa Sở thích Trực tiếp

Cách tiếp cận của chúng tôi là một điều chỉnh của Tối ưu hóa Sở thích Trực tiếp (DPO) [33], một cách tiếp cận hiệu quả để học từ sở thích của con người cho các mô hình ngôn ngữ. Lạm dụng ký hiệu, chúng tôi cũng sử dụng x₀ như các biến ngẫu nhiên cho ngôn ngữ.

**Mô hình hóa Phần thưởng** Ước tính sự thiên vị của con người đối với một thế hệ x₀ cho điều kiện c, là khó khăn vì chúng ta không có quyền truy cập vào mô hình phần thưởng tiềm ẩn r(c,x₀). Trong cài đặt của chúng tôi, chúng tôi giả sử chỉ có quyền truy cập vào các cặp được xếp hạng được tạo ra từ một số điều kiện x^w₀ ≻ x^l₀|c, trong đó x^w₀ và x^l₀ biểu thị các mẫu "thắng" và "thua". Mô hình Bradley-Terry (BT) quy định viết sở thích của con người là:

p_BT(x^w₀ ≻ x^l₀|c) = σ(r(c,x^w₀) - r(c,x^l₀)) (3)

trong đó σ là hàm sigmoid. r(c,x₀) có thể được tham số hóa bởi một mạng nơ-ron φ và được ước tính thông qua huấn luyện khả năng tối đa cho phân loại nhị phân:

L_BT(φ) = -E_c,x^w₀,x^l₀[log σ(r_φ(c,x^w₀) - r_φ(c,x^l₀))] (4)

trong đó lời nhắc c và các cặp dữ liệu x^w₀, x^l₀ từ một bộ dữ liệu tĩnh với nhãn chú thích của con người.

**RLHF** RLHF nhằm tối ưu hóa một phân phối có điều kiện p_θ(x₀|c) (điều kiện c ~ D_c) sao cho mô hình phần thưởng tiềm ẩn r(c,x₀) được định nghĩa trên nó được tối đa hóa, trong khi điều hòa KL-divergence từ một phân phối tham chiếu p_ref:

max_p_θ E_c~D_c,x₀~p_θ(x₀|c)[r(c,x₀)] - βD_KL[p_θ(x₀|c)||p_ref(x₀|c)] (5)

trong đó siêu tham số β điều khiển điều hòa.

**Mục tiêu DPO** Trong Eq. (5) từ [33], giải pháp tối ưu toàn cục duy nhất p*_θ có dạng:

p*_θ(x₀|c) = p_ref(x₀|c) exp(r(c,x₀)/β)/Z(c) (6)

trong đó Z(c) = Σ_x₀ p_ref(x₀|c) exp(r(c,x₀)/β) là hàm phân vùng. Do đó, hàm phần thưởng được viết lại là:

r(c,x₀) = β log p*_θ(x₀|c)/p_ref(x₀|c) + β log Z(c) (7)

Sử dụng Eq. (4), mục tiêu phần thưởng trở thành:

L_DPO(θ) = -E_c,x^w₀,x^l₀[log σ(β log p_θ(x^w₀|c)/p_ref(x^w₀|c) - β log p_θ(x^l₀|c)/p_ref(x^l₀|c))] (8)

Bằng cách tái tham số hóa này, thay vì tối ưu hóa hàm phần thưởng r_φ và sau đó thực hiện RL, [33] trực tiếp tối ưu hóa phân phối có điều kiện tối ưu p_θ(x₀|c).

## 4. DPO cho Mô hình Khuếch tán

Trong việc điều chỉnh DPO cho các mô hình khuếch tán, chúng tôi xem xét một cài đặt trong đó chúng tôi có một bộ dữ liệu cố định D = {(c,x^w₀,x^l₀)} trong đó mỗi ví dụ chứa một lời nhắc c và một cặp hình ảnh được tạo ra từ một mô hình tham chiếu p_ref với nhãn của con người x^w₀ ≻ x^l₀. Chúng tôi nhằm học một mô hình mới p_θ được căn chỉnh theo sở thích của con người, với các thế hệ được ưa thích hơn p_ref.

Thách thức chính chúng tôi gặp phải là phân phối được tham số hóa p_θ(x₀|c) không thể xử lý được, vì nó cần biên hóa tất cả các đường dẫn khuếch tán có thể (x₁,...,x_T) dẫn đến x₀. Để vượt qua thách thức này, chúng tôi sử dụng cận dưới bằng chứng (ELBO). Ở đây, chúng tôi giới thiệu các biến tiềm ẩn x₁:T và định nghĩa R(c,x₀:T) là phần thưởng trên toàn bộ chuỗi, sao cho chúng ta có thể định nghĩa r(c,x₀) là

r(c,x₀) = E_p_θ(x₁:T|x₀,c)[R(c,x₀:T)]. (9)

Đối với số hạng điều hòa KL trong Eq. (5), theo công việc trước đây [17, 42], chúng ta có thể thay vào đó tối thiểu hóa cận trên của nó KL-divergence chung D_KL[p_θ(x₀:T|c)||p_ref(x₀:T|c)]. Thay thế cận KL-divergence này và định nghĩa r(c,x₀) (Eq. (9)) trở lại Eq. (5), chúng ta có mục tiêu

max_p_θ E_c~D_c,x₀:T~p_θ(x₀:T|c)[r(c,x₀)] - βD_KL[p_θ(x₀:T|c)||p_ref(x₀:T|c)]. (10)

Mục tiêu này có một công thức song song như Eq. (5) nhưng được định nghĩa trên đường dẫn x₀:T. Nó nhằm tối đa hóa phần thưởng cho quá trình ngược p_θ(x₀:T), trong khi khớp phân phối của quá trình ngược tham chiếu gốc. Song song với Eqs. (6) đến (8), mục tiêu này có thể được tối ưu hóa trực tiếp thông qua phân phối có điều kiện p_θ(x₀:T) thông qua mục tiêu:

L_DPO-Diffusion(θ) = -E_(x^w₀,x^l₀)~D[log σ(β E_x^w₁:T~p_θ(x^w₁:T|x^w₀) E_x^l₁:T~p_θ(x^l₁:T|x^l₀)[log p_θ(x^w₀:T)/p_ref(x^w₀:T) - log p_θ(x^l₀:T)/p_ref(x^l₀:T)])] (11)

Chúng tôi bỏ qua c để gọn gàng (chi tiết bao gồm trong Supp. S2). Để tối ưu hóa Eq. (11), chúng ta phải lấy mẫu x₁:T ~ p_θ(x₁:T|x₀). Mặc dù thực tế là p_θ chứa các tham số có thể huấn luyện, thủ tục lấy mẫu này vừa (1) không hiệu quả vì T thường lớn (T = 1000), và (2) không thể xử lý được vì p_θ(x₁:T) đại diện cho tham số hóa quá trình ngược p_θ(x₁:T) = p_θ(x_T) ∏ᵀₜ₌₁ p_θ(xₜ₋₁|xₜ). Chúng tôi giải quyết hai vấn đề này tiếp theo.

Từ Eq. (11), chúng tôi thay thế các phân hủy ngược cho p_θ và p_ref, và sử dụng bất đẳng thức Jensen và tính lồi của hàm -log σ để đẩy kỳ vọng ra ngoài. Với một số đơn giản hóa, chúng ta có cận sau:

L_DPO-Diffusion(θ) ≤ -E_(x^w₀,x^l₀)~D,t~U(0,T), x^w_{t-1,t}~p_θ(x^w_{t-1,t}|x^w₀), x^l_{t-1,t}~p_θ(x^l_{t-1,t}|x^l₀)[log σ(βT log p_θ(x^w_{t-1}|x^w_t)/p_ref(x^w_{t-1}|x^w_t) - βT log p_θ(x^l_{t-1}|x^l_t)/p_ref(x^l_{t-1}|x^l_t))] (12)

Huấn luyện hiệu quả thông qua gradient descent giờ đây là có thể. Tuy nhiên, lấy mẫu từ quá trình ngược chung p_θ(xₜ₋₁,xₜ|x₀,c) vẫn không thể xử lý được và r của Eq. (9) có kỳ vọng trên p_θ(x₁:T|x₀). Vì vậy chúng tôi xấp xỉ quá trình ngược p_θ(x₁:T|x₀) bằng quá trình thuận q(x₁:T|x₀) (một lược đồ thay thế trong Supp. S2). Với một số đại số, điều này mang lại:

L(θ) = -E_(x^w₀,x^l₀)~D,t~U(0,T),x^w_t~q(x^w_t|x^w₀),x^l_t~q(x^l_t|x^l₀)[log σ(-βT( +D_KL(q(x^w_{t-1}|x^w₀,t)||p_θ(x^w_{t-1}|x^w_t)) -D_KL(q(x^w_{t-1}|x^w₀,t)||p_ref(x^w_{t-1}|x^w_t)) -D_KL(q(x^l_{t-1}|x^l₀,t)||p_θ(x^l_{t-1}|x^l_t)) +D_KL(q(x^l_{t-1}|x^l₀,t)||p_ref(x^l_{t-1}|x^l_t))))] (13)

Sử dụng Eq. (1) và đại số, tổn thất trên đơn giản hóa thành:

L(θ) = -E_(x^w₀,x^l₀)~D,t~U(0,T),x^w_t~q(x^w_t|x^w₀),x^l_t~q(x^l_t|x^l₀)[log σ(-βTω(λₜ)( ||ε^w - ε_θ(x^w_t, t)||²₂ - ||ε^w - ε_ref(x^w_t, t)||²₂ - ||ε^l - ε_θ(x^l_t, t)||²₂ - ||ε^l - ε_ref(x^l_t, t)||²₂))] (14)

trong đó x*_t = αₜx*₀ + σₜε*, ε* ~ N(0, I) là một lần rút từ q(x*_t|x*₀) (Eq. (2)). λₜ = α²ₜ/σ²ₜ là tỷ lệ tín hiệu trên nhiễu, ω(λₜ) là hàm trọng số (hằng số trong thực tế [17, 20]). Chúng tôi đưa hằng số T vào β. Tổn thất này khuyến khích ε_θ cải thiện hơn trong khử nhiễu x^w_t so với x^l_t, trực quan hóa trong Hình 2. Chúng tôi cũng đưa ra Eq. (14) như một cách tiếp cận RL đa bước trong cùng một cài đặt như DDPO và DPOK [6, 11] (Supp. S3) nhưng như một thuật toán off-policy, điều này biện minh cho lựa chọn lấy mẫu của chúng tôi trong Eq. 13. Một quan điểm mô hình sở thích nhiễu mang lại cùng một mục tiêu (Supp. S4).

## 5. Thí nghiệm

### 5.1. Cài đặt

**Mô hình và Bộ dữ liệu:** Chúng tôi chứng minh hiệu quả của Diffusion-DPO qua một loạt thí nghiệm. Chúng tôi sử dụng mục tiêu từ Eq. (14) để tinh chỉnh Stable Diffusion 1.5 (SD1.5) [36] và mô hình cơ sở Stable Diffusion XL-1.0 (SDXL) [30] nguồn mở tiên tiến nhất. Chúng tôi huấn luyện trên bộ dữ liệu Pick-a-Pic [21], bao gồm sở thích theo cặp cho hình ảnh được tạo ra bởi SDXL-beta và Dreamlike, một phiên bản tinh chỉnh của SD1.5. Các lời nhắc và sở thích được thu thập từ người dùng của ứng dụng web Pick-a-Pic (xem [21] để biết chi tiết). Chúng tôi sử dụng bộ dữ liệu Pick-a-Pic v2 lớn hơn. Sau khi loại trừ ~12% các cặp có hòa, chúng tôi kết thúc với 851,293 cặp, với 58,960 lời nhắc duy nhất.

**Siêu tham số** Chúng tôi sử dụng AdamW [24] cho các thí nghiệm SD1.5, và Adafactor [40] cho SDXL để tiết kiệm bộ nhớ. Kích thước batch hiệu quả 2048 (cặp) được sử dụng; huấn luyện trên 16 GPU NVIDIA A100 với kích thước batch cục bộ 1 cặp và tích lũy gradient 128 bước. Chúng tôi huấn luyện ở độ phân giải vuông cố định. Tốc độ học 2000/(β²·0.48·10⁻⁸) được sử dụng với 25% khởi động tuyến tính. Tỷ lệ nghịch đảo được thúc đẩy bởi chuẩn của gradient mục tiêu DPO tỷ lệ thuận với β (tham số phạt phân kỳ) [33]. Đối với cả SD1.5 và SDXL, chúng tôi tìm thấy β ∈ [2000, 5000] để cung cấp hiệu suất tốt (Supp. S5). Chúng tôi trình bày kết quả chính SD1.5 với β = 2000 và kết quả SDXL với β = 5000.

**Đánh giá** Chúng tôi tự động xác thực các checkpoint với 500 lời nhắc duy nhất của tập xác thực Pick-a-Pic: đo phần thưởng PickScore trung vị của hình ảnh được tạo ra. Pickscore [21] là mô hình ghi điểm nhận thức chú thích được huấn luyện trên Pick-a-Pic (v1) để ước tính chất lượng hình ảnh được nhận thức bởi con người. Để kiểm tra cuối cùng, chúng tôi tạo ra hình ảnh sử dụng mô hình cơ sở và được điều chỉnh Diffusion-DPO có điều kiện trên chú thích từ các benchmark Partiprompt [56] và HPSv2 [52] (1632 và 3200 chú thích tương ứng). Trong khi DDPO [6] là một phương pháp liên quan, chúng tôi không quan sát được cải thiện ổn định khi huấn luyện từ các triển khai công cộng trên Pick-a-Pic. Chúng tôi sử dụng các nhãn hiệu trên Amazon Mechanical Turk để so sánh các thế hệ dưới ba tiêu chí khác nhau: Q1 Sở thích Chung (Bạn thích hình ảnh nào hơn cho lời nhắc?), Q2 Tính Hấp dẫn Trực quan (lời nhắc không được xem xét) (Hình ảnh nào hấp dẫn hơn về mặt trực quan?), Q3 Căn chỉnh Lời nhắc (Hình ảnh nào phù hợp hơn với mô tả văn bản?). Năm phản hồi được thu thập cho mỗi so sánh với phiếu bầu đa số (3+) được coi là quyết định tập thể.

### 5.2. Kết quả Chính: Căn chỉnh Mô hình Khuếch tán

Trước tiên, chúng tôi cho thấy rằng đầu ra của mô hình SDXL được tinh chỉnh Diffusion-DPO được ưa thích đáng kể hơn mô hình SDXL-base cơ sở. Trong đánh giá Partiprompt (Hình 3-trên trái), DPO-SDXL được ưa thích 70.0% thời gian cho Sở thích Chung (Q1), và đạt được tỷ lệ thắng tương tự trong các đánh giá về cả Tính Hấp dẫn Trực quan (Q2) và Căn chỉnh Lời nhắc (Q3). Đánh giá trên benchmark HPS (Hình 3-trên phải) cho thấy xu hướng tương tự, với tỷ lệ thắng Sở thích Chung là 64.7%. Chúng tôi cũng ghi điểm các thế hệ DPO-SDXL HPSv2 với mô hình phần thưởng HPSv2, đạt được phần thưởng trung bình 28.16, đứng đầu bảng xếp hạng [53].

Chúng tôi hiển thị các so sánh định tính với SDXL-base trong Hình 3 (dưới). Diffusion-DPO tạo ra hình ảnh hấp dẫn hơn, với mảng màu sắc sống động, ánh sáng kịch tính, bố cục tốt, và giải phẫu người/động vật thực tế. Trong khi tất cả hình ảnh SDXL thỏa mãn tiêu chí nhắc ở một mức độ nào đó, các thế hệ DPO xuất hiện vượt trội, như được xác nhận bởi nghiên cứu crowdsourced. Chúng tôi lưu ý rằng sở thích không phải là phổ quát, và trong khi sở thích chung được chia sẻ nhất là đối với hình ảnh tràn đầy năng lượng và kịch tính, những người khác có thể thích cảnh yên tĩnh/tinh tế hơn. Lĩnh vực điều chỉnh sở thích cá nhân hoặc nhóm là một lĩnh vực thú vị của công việc tương lai.

Sau so sánh tham số bằng nhau này với SDXL-base, chúng tôi so sánh SDXL-DPO với pipeline SDXL hoàn chỉnh, bao gồm mô hình cơ sở và mô hình tinh chỉnh (Hình 4). Mô hình tinh chỉnh là một mô hình khuếch tán hình ảnh thành hình ảnh cải thiện chất lượng trực quan của các thế hệ, và đặc biệt hiệu quả trên nền chi tiết và khuôn mặt. Trong các thí nghiệm của chúng tôi với PartiPrompts và HPSv2, SDXL-DPO (3.5B tham số, chỉ kiến trúc SDXL-base), dễ dàng đánh bại mô hình SDXL hoàn chỉnh (6.6B tham số). Trong câu hỏi Sở thích Chung, nó có tỷ lệ thắng benchmark 69% và 64% tương ứng, so sánh được với tỷ lệ thắng của nó chỉ so với SDXL-base. Điều này được giải thích bởi khả năng của mô hình được điều chỉnh DPO (Hình 4, dưới) để tạo ra chi tiết tinh tế và hiệu suất mạnh mẽ của nó trên các danh mục hình ảnh khác nhau. Trong khi mô hình tinh chỉnh đặc biệt giỏi trong việc cải thiện việc tạo ra chi tiết con người, tỷ lệ thắng của Diffusion-DPO trên danh mục Người trong bộ dữ liệu Partiprompt so với mô hình cơ sở + tinh chỉnh vẫn là 67.2% ấn tượng (so với 73.4% so với cơ sở).

### 5.3. Chỉnh sửa Hình ảnh thành Hình ảnh

Hiệu suất dịch hình ảnh thành hình ảnh cũng cải thiện sau khi điều chỉnh Diffusion-DPO. Chúng tôi kiểm tra DPO-SDXL trên TEd-Bench [19], một benchmark chỉnh sửa hình ảnh dựa trên văn bản của 100 cặp hình ảnh-văn bản thực, sử dụng SDEdit [25] với cường độ nhiễu 0.6. Các nhãn hiệu được hiển thị hình ảnh gốc và các chỉnh sửa SDXL/DPO-SDXL và được hỏi "Bạn thích chỉnh sửa nào hơn cho văn bản?" DPO-SDXL được ưa thích 65% thời gian, SDXL 24%, với 11% hòa. Chúng tôi hiển thị kết quả định tính SDEdit trên bố cục màu (cường độ 0.98) trong Hình 5.

### 5.4. Học từ Phản hồi AI

Trong LLM, học từ phản hồi AI đã nổi lên như một giải pháp thay thế mạnh mẽ cho việc học từ sở thích của con người [22]. Diffusion-DPO cũng có thể thừa nhận việc học từ phản hồi AI bằng cách xếp hạng trực tiếp các cặp được tạo ra thành (y^w, y^l) sử dụng mạng ghi điểm đã được huấn luyện trước. Chúng tôi sử dụng HPSv2 [52] cho một ước tính sở thích con người nhận thức lời nhắc thay thế, CLIP (OpenCLIP ViT-H/14) [18, 32] cho căn chỉnh văn bản-hình ảnh, Aesthetic Predictor [37] cho tính hấp dẫn trực quan không dựa trên văn bản, và PickScore. Chúng tôi chạy tất cả các thí nghiệm trên SD 1.5 với β = 5000 cho 1000 bước. Huấn luyện trên các ước tính sở thích nhận thức lời nhắc PickScore và HPS tăng tỷ lệ thắng cho cả tính hấp dẫn trực quan thô và căn chỉnh lời nhắc (Hình 6). Chúng tôi lưu ý rằng phản hồi PickScore có thể hiểu được như pseudo-labeling bộ dữ liệu Pick-a-Pic—một hình thức làm sạch dữ liệu [54, 60]. Huấn luyện cho Thẩm mỹ và CLIP cải thiện những khả năng đó cụ thể hơn, trong trường hợp Thẩm mỹ với chi phí của CLIP. Khả năng huấn luyện cho căn chỉnh văn bản-hình ảnh thông qua CLIP là một cải tiến được ghi nhận so với công việc trước đây [7]. Hơn nữa, huấn luyện SD1.5 trên bộ dữ liệu pseudo-labeled PickScore (β = 5000, 2000 bước) vượt trội hơn huấn luyện trên nhãn thô. Trên câu hỏi Sở thích Chung Partiprompt, tỷ lệ thắng của DPO tăng từ 59.8% lên 63.3%, chỉ ra rằng học từ phản hồi AI có thể là một hướng đi hứa hẹn cho căn chỉnh mô hình khuếch tán.

### 5.5. Phân tích

**Mô hình Phần thưởng Ngầm** Như một hệ quả của khung lý thuyết, lược đồ DPO của chúng tôi ngầm học một mô hình phần thưởng và có thể ước tính sự khác biệt về phần thưởng giữa hai hình ảnh bằng cách lấy kỳ vọng trên số hạng bên trong của Eq. (14) (chi tiết trong Supp. S4.1). Chúng tôi ước tính trên 10 t ngẫu nhiên ~ U{0,1} Các mô hình đã học của chúng tôi (DPO-SD1.5 và DPO-SDXL) hoạt động tốt trong phân loại sở thích nhị phân (Tab. 2), với DPO-SDXL vượt qua tất cả các mô hình nhận dạng hiện có trên phần này. Những kết quả này cho thấy rằng tham số hóa phần thưởng ngầm trong mục tiêu Diffusion-DPO có khả năng biểu đạt và tổng quát hóa tương đương với mục tiêu/kiến trúc mô hình hóa phần thưởng cổ điển.

**Chất lượng Dữ liệu Huấn luyện** Hình 7 cho thấy rằng mặc dù SDXL vượt trội hơn dữ liệu huấn luyện (bao gồm y^w), như được đo bởi Pickscore, huấn luyện DPO cải thiện hiệu suất của nó đáng kể. Trong thí nghiệm này, chúng tôi xác nhận rằng Diffusion-DPO có thể cải thiện trên sở thích trong phân phối cũng như vậy, bằng cách huấn luyện (β = 5k, 2000 bước) mô hình Dreamlike trên một tập con của bộ dữ liệu Pick-a-Pic được tạo ra chỉ bởi mô hình Dreamlike. Tập con này đại diện cho 15% của bộ dữ liệu gốc. Dreamlike-DPO cải thiện trên mô hình cơ sở, mặc dù cải thiện hiệu suất bị hạn chế, có lẽ do kích thước nhỏ của bộ dữ liệu.

**Tinh chỉnh Có giám sát (SFT)** có lợi trong cài đặt LLM như tiền huấn luyện ban đầu trước huấn luyện sở thích. Để đánh giá SFT trong cài đặt của chúng tôi, chúng tôi tinh chỉnh các mô hình trên các cặp (x, y^w) được ưa thích của bộ dữ liệu Pick-a-Pic. Chúng tôi huấn luyện theo cùng lịch trình dài như DPO sử dụng tốc độ học 1e-9 và quan sát hội tụ. Trong khi SFT cải thiện SD1.5 vanilla (55.5% tỷ lệ thắng so với mô hình cơ sở), bất kỳ lượng SFT nào cũng làm suy giảm hiệu suất của SDXL, ngay cả ở tốc độ học thấp hơn. Sự tương phản này có thể quy cho chất lượng cao hơn nhiều của các thế hệ Pick-a-Pic so với SD1.5, vì chúng được lấy từ SDXL-beta và Dreamlike. Ngược lại, mô hình cơ sở SDXL-1.0 vượt trội hơn các mô hình bộ dữ liệu Pick-a-Pic. Xem Supp. S6 để thảo luận thêm.

## 6. Kết luận

Trong công việc này, chúng tôi giới thiệu Diffusion-DPO: một phương pháp cho phép các mô hình khuếch tán học trực tiếp từ phản hồi của con người trong cài đặt từ vựng mở lần đầu tiên. Chúng tôi tinh chỉnh SDXL-1.0 sử dụng mục tiêu Diffusion-DPO và bộ dữ liệu Pick-a-Pic (v2) để tạo ra một tiêu chuẩn tiên tiến mới cho các mô hình tạo sinh văn bản thành hình ảnh nguồn mở được đo bằng sở thích chung, tính hấp dẫn trực quan, và căn chỉnh lời nhắc. Chúng tôi cũng chứng minh rằng DPO-SDXL vượt trội hơn ngay cả pipeline mô hình cơ sở SDXL plus tinh chỉnh, mặc dù chỉ sử dụng 53% tổng số tham số mô hình. Làm sạch/mở rộng bộ dữ liệu là một hướng tương lai hứa hẹn vì chúng tôi quan sát việc làm sạch dữ liệu sơ bộ cải thiện hiệu suất (Phần 5.4). Trong khi DPO-Diffusion là một thuật toán ngoại tuyến, chúng tôi dự đoán các phương pháp học trực tuyến sẽ là một động lực khác của hiệu suất tương lai. Cũng có những biến thể ứng dụng thú vị như điều chỉnh theo sở thích của cá nhân hoặc nhóm nhỏ.

**Đạo đức** Hiệu suất của Diffusion-DPO ấn tượng, nhưng bất kỳ nỗ lực nào trong tạo sinh văn bản thành hình ảnh đều mang lại rủi ro đạo đức, đặc biệt khi dữ liệu được thu thập từ web. Các thế hệ nội dung có hại, căm thù, giả mạo hoặc tình dục rõ ràng là các vector rủi ro được biết đến. Ngoài ra, cách tiếp cận này ngày càng chịu ảnh hưởng của thành kiến của các nhãn hiệu tham gia (ngoài các thành kiến có trong mô hình đã được huấn luyện trước); Diffusion-DPO có thể học và truyền bá những sở thích này. Kết quả là, một tập hợp nhãn hiệu đa dạng và đại diện là thiết yếu – có sở thích lần lượt được mã hóa trong bộ dữ liệu. Hơn nữa, một phần của lời nhắc do người dùng tạo ra Pick-a-Pic có tính tình dục rõ ràng, và ngay cả lời nhắc vô hại cũng có thể cung cấp hình ảnh thiên về gợi cảm hơn (đặc biệt đối với lời nhắc siêu tình dục hóa phụ nữ). Cuối cùng, như với tất cả các mô hình văn bản thành hình ảnh, hình ảnh được tạo ra sẽ không phải lúc nào cũng khớp với lời nhắc. Tuy nhiên, một số trong những kịch bản này có thể được giải quyết ở cấp độ bộ dữ liệu, và việc lọc dữ liệu cũng có thể. Bất kể, chúng tôi sẽ không mở nguồn hoặc làm cho mô hình của chúng tôi khả dụng cho đến khi chúng tôi thêm bộ lọc an toàn bổ sung để đảm bảo rằng nội dung độc hại được khắc phục.
