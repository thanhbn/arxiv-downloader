FiT: Transformer Thị Giác Linh Hoạt cho Mô Hình Khuếch Tán
Zeyu Lu1 2 *Zidong Wang1 3 *Di Huang1 4Chengyue Wu5Xihui Liu5Wanli Ouyang1Lei Bai1
Hình 1: Các mẫu được chọn từ các mô hình FiT-XL/2 ở độ phân giải 256×256, 224×448 và 448×224. FiT có khả năng tạo ra hình ảnh với độ phân giải và tỷ lệ khung hình không hạn chế.

Tóm tắt
Tự nhiên có độ phân giải vô hạn. Trong bối cảnh của thực tế này, các mô hình khuếch tán hiện có, chẳng hạn như Transformer Khuếch Tán, thường gặp khó khăn khi xử lý độ phân giải hình ảnh ngoài phạm vi được huấn luyện. Để khắc phục hạn chế này, chúng tôi trình bày Transformer Thị Giác Linh Hoạt (FiT), một kiến trúc transformer được thiết kế đặc biệt để tạo ra hình ảnh với độ phân giải và tỷ lệ khung hình không hạn chế. Không giống như các phương pháp truyền thống coi hình ảnh như lưới độ phân giải cố định, FiT khái niệm hóa hình ảnh như chuỗi các token có kích thước động. Quan điểm này cho phép một chiến lược huấn luyện linh hoạt dễ dàng thích ứng với các tỷ lệ khung hình đa dạng trong cả giai đoạn huấn luyện và suy luận, từ đó thúc đẩy khả năng tổng quát hóa độ phân giải và loại bỏ thiên lệch do việc cắt hình ảnh gây ra. Được tăng cường bởi cấu trúc mạng được điều chỉnh tỉ mỉ và việc tích hợp các kỹ thuật ngoại suy không cần huấn luyện, FiT thể hiện tính linh hoạt đáng kể trong việc tạo ra ngoại suy độ phân giải. Các thí nghiệm toàn diện chứng minh hiệu suất đặc biệt của FiT trên một phạm vi rộng các độ phân giải, thể hiện hiệu quả cả trong và ngoài phân phối độ phân giải huấn luyện của nó. Kho lưu trữ có sẵn tại https://github.com/whlzy/FiT.

1. Giới thiệu
Các mô hình tạo hình ảnh hiện tại gặp khó khăn trong việc tổng quát hóa trên các độ phân giải tùy ý. Họ Transformer Khuếch Tán (DiT) (Peebles & Xie, 2023), mặc dù xuất sắc trong một số phạm vi độ phân giải nhất định, nhưng lại thiếu sót khi xử lý hình ảnh có độ phân giải khác nhau. Hạn chế này xuất phát từ thực tế rằng DiT không thể sử dụng hình ảnh độ phân giải động trong quá trình huấn luyện, cản trở khả năng thích ứng với độ dài token hoặc độ phân giải khác nhau một cách hiệu quả.

Để khắc phục hạn chế này, chúng tôi giới thiệu Transformer Thị Giác Linh Hoạt (FiT), có khả năng tạo ra hình ảnh với độ phân giải và tỷ lệ khung hình không hạn chế. Động lực chính là một quan điểm mới về mô hình hóa dữ liệu hình ảnh: thay vì coi hình ảnh như lưới tĩnh có kích thước cố định, FiT khái niệm hóa hình ảnh như chuỗi các token có độ dài thay đổi. Cách tiếp cận này cho phép FiT điều chỉnh động độ dài chuỗi, từ đó tạo điều kiện cho việc tạo ra hình ảnh ở bất kỳ độ phân giải mong muốn nào mà không bị hạn chế bởi kích thước được định nghĩa trước. Bằng cách quản lý hiệu quả các chuỗi token có độ dài thay đổi và đệm chúng đến độ dài tối đa được chỉ định, FiT mở khóa tiềm năng cho việc tạo hình ảnh độc lập với độ phân giải. FiT đại diện cho sự thay đổi mô hình này thông qua những tiến bộ đáng kể trong pipeline huấn luyện linh hoạt, kiến trúc mạng và quy trình suy luận.

Pipeline Huấn luyện Linh hoạt. FiT duy trì một cách độc đáo tỷ lệ khung hình hình ảnh ban đầu trong quá trình huấn luyện, bằng cách coi hình ảnh như một chuỗi các token. Quan điểm độc đáo này cho phép FiT thích ứng thay đổi kích thước hình ảnh độ phân giải cao để phù hợp với giới hạn token tối đa được định nghĩa trước, đảm bảo rằng không có hình ảnh nào, bất kể độ phân giải ban đầu của nó, bị cắt hoặc được chia tỷ lệ không cân xứng. Phương pháp này đảm bảo rằng tính toàn vẹn của độ phân giải hình ảnh được duy trì, như thể hiện trong Hình 2, tạo điều kiện cho khả năng tạo ra hình ảnh chất lượng cao ở các độ phân giải khác nhau. Theo hiểu biết của chúng tôi, FiT là mô hình tạo dựa trên transformer đầu tiên duy trì độ phân giải hình ảnh đa dạng trong suốt quá trình huấn luyện.

Kiến trúc Mạng. Mô hình FiT phát triển từ kiến trúc DiT nhưng giải quyết các hạn chế của nó trong việc ngoại suy độ phân giải. Một điều chỉnh kiến trúc mạng thiết yếu để xử lý kích thước hình ảnh đa dạng là việc áp dụng 2D Rotary Positional Embedding (RoPE) (Su et al., 2024), lấy cảm hứng từ thành công của nó trong việc nâng cao các mô hình ngôn ngữ lớn (LLM) cho việc ngoại suy độ dài (Liu et al., 2023). Chúng tôi cũng giới thiệu Swish-Gated Linear Unit (SwiGLU) (Shazeer, 2020) thay thế cho Multilayer Perceptron (MLP) truyền thống và thay thế Multi-Head Self-Attention (MHSA) của DiT bằng Masked MHSA để quản lý hiệu quả các token đệm trong pipeline huấn luyện linh hoạt của chúng tôi.

Quy trình Suy luận. Trong khi các mô hình ngôn ngữ lớn sử dụng các kỹ thuật ngoại suy độ dài token (Peng et al., 2023; LocalLLaMA) để tạo ra văn bản có độ dài tùy ý, việc áp dụng trực tiếp các công nghệ này vào FiT mang lại kết quả không tối ưu. Chúng tôi tùy chỉnh các kỹ thuật này cho 2D RoPE, từ đó nâng cao hiệu suất của FiT trên một phổ độ phân giải và tỷ lệ khung hình.

Mô hình FiT-XL/2 Gflop cao nhất của chúng tôi, sau khi huấn luyện chỉ 1.8 triệu bước trên tập dữ liệu ImageNet-256 (Deng et al., 2009), vượt trội hơn tất cả các mô hình CNN và transformer tiên tiến nhất với biên độ đáng kể trên các độ phân giải 160×320, 128×384, 320×320, 224×448, và 160×480. Hiệu suất của FiT-XL/2 tiến bộ đáng kể hơn nữa với phương pháp ngoại suy độ phân giải không cần huấn luyện của chúng tôi. So với DiT-XL/2 cơ sở huấn luyện 7 triệu bước, FiT-XL/2 thua kém một chút ở độ phân giải 256×256 nhưng vượt trội đáng kể ở tất cả các độ phân giải khác.

Tóm lại, đóng góp của chúng tôi nằm ở việc giới thiệu mới mẻ FiT, một transformer thị giác linh hoạt được thiết kế riêng cho các mô hình khuếch tán, có khả năng tạo ra hình ảnh ở bất kỳ độ phân giải và tỷ lệ khung hình nào. Chúng tôi trình bày ba tính năng thiết kế sáng tạo trong FiT, bao gồm một pipeline huấn luyện linh hoạt loại bỏ nhu cầu cắt, một kiến trúc transformer độc đáo cho việc mô hình hóa độ dài token động, và một phương pháp ngoại suy độ phân giải không cần huấn luyện cho việc tạo ra độ phân giải tùy ý. Các thí nghiệm nghiêm ngặt chứng minh rằng mô hình FiT-XL/2 đạt được hiệu suất tiên tiến nhất trên nhiều cài đặt độ phân giải và tỷ lệ khung hình.

2. Công trình Liên quan
Mô hình Khuếch tán. Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020; Saharia et al., 2022; Radford et al., 2021) và các mô hình dựa trên điểm số (Hyvärinen & Dayan, 2005; Song et al., 2020b) đã thể hiện tiến bộ đáng kể trong bối cảnh các tác vụ tạo hình ảnh. Denoising Diffusion Implicit Model (DDIM) Song et al. (2020a), cung cấp một quy trình lấy mẫu được tăng tốc. Latent Diffusion Models (LDM) (Rombach et al., 2022) thiết lập một chuẩn mực mới của việc huấn luyện các mô hình tạo sinh sâu để đảo ngược một quy trình nhiễu trong không gian tiềm ẩn, thông qua việc sử dụng VQ-VAE (Esser et al., 2021).

Mô hình Transformer. Mô hình Transformer (Vaswani et al., 2017), đã thành công thay thế các kiến trúc đặc thù cho từng lĩnh vực trong nhiều lĩnh vực khác nhau bao gồm, nhưng không chỉ hạn chế ở, ngôn ngữ (Brown et al., 2020; Chowdhery et al., 2023a), thị giác (Dosovitskiy et al., 2020), và đa phương thức (Team et al., 2023). Trong nghiên cứu nhận thức thị giác, hầu hết các nỗ lực (Touvron et al., 2019; 2021; Liu et al., 2021; 2022) tập trung vào độ phân giải nhằm tăng tốc việc tiền huấn luyện bằng cách sử dụng một độ phân giải thấp, cố định. Mặt khác, NaViT (Dehghani et al., 2023) triển khai kỹ thuật 'Patch n' Pack' để huấn luyện ViT bằng cách sử dụng hình ảnh ở độ phân giải tự nhiên, 'bản địa' của chúng. Đáng chú ý, transformer cũng đã được khám phá trong các mô hình xác suất khuếch tán khử nhiễu (Ho et al., 2020) để tổng hợp hình ảnh. DiT (Peebles & Xie, 2023) là công trình tinh hoa sử dụng transformer thị giác như xương sống của LDM và có thể phục vụ như một cơ sở mạnh mẽ. Dựa trên kiến trúc DiT, MDT (Gao et al., 2023) giới thiệu một cách tiếp cận mô hình hóa tiềm ẩn có mặt nạ, yêu cầu hai lần chạy tới trong huấn luyện và suy luận. U-ViT (Bao et al., 2023) coi tất cả đầu vào như token và tích hợp kiến trúc U-Net vào xương sống ViT của LDM. DiffiT (Hatamizadeh et al., 2023) giới thiệu một mô-đun self-attention phụ thuộc thời gian vào xương sống DiT để thích ứng với các giai đoạn khác nhau của quá trình khuếch tán. Chúng tôi tuân theo mô hình LDM của các phương pháp trên và tiếp tục đề xuất một pipeline tổng hợp hình ảnh linh hoạt mới.

Ngoại suy Độ dài trong LLM. RoPE (Rotary Position Embedding) (Su et al., 2024) là một embedding vị trí mới kết hợp thông tin vị trí tương đối vào embedding vị trí tuyệt đối. Gần đây nó đã trở thành embedding vị trí thống trị trong một loạt các thiết kế LLM (Large Language Model) (Chowdhery et al., 2023b; Touvron et al., 2023a;b). Mặc dù RoPE có các thuộc tính có giá trị, chẳng hạn như tính linh hoạt của độ dài chuỗi, hiệu suất của nó giảm khi chuỗi đầu vào vượt quá độ dài huấn luyện. Nhiều cách tiếp cận đã được đề xuất để giải quyết vấn đề này. PI (Position Interpolation) (Chen et al., 2023) giảm tuyến tính các chỉ số vị trí đầu vào để khớp với kích thước cửa sổ ngữ cảnh ban đầu, trong khi NTK-aware (LocalLLaMA) thay đổi cơ sở quay của RoPE. YaRN (Yet another RoPE extensioN) (Peng et al., 2023) là một phương pháp cải tiến để mở rộng cửa sổ ngữ cảnh một cách hiệu quả. RandomPE (Ruoss et al., 2023) lấy mẫu con một tập hợp có thứ tự các vị trí từ một phạm vi vị trí lớn hơn nhiều so với những gì quan sát được ban đầu trong huấn luyện hoặc suy luận. xPos (Sun et al., 2022) tích hợp sự suy giảm dài hạn vào RoPE và sử dụng attention nhân quả theo khối để có hiệu suất ngoại suy tốt hơn. Công trình của chúng tôi tìm hiểu sâu về việc triển khai RoPE trong tạo thị giác và các phương pháp ngoại suy độ phân giải tức thì.

3. Transformer Thị Giác Linh Hoạt cho Khuếch Tán
3.1. Sơ bộ
1-D RoPE (Rotary Positional Embedding) (Su et al., 2024) là một loại embedding vị trí kết hợp PE tuyệt đối và tương đối, thể hiện một mức độ khả năng ngoại suy nhất định trong LLM. Cho vector key thứ m và vector query thứ n là qm,kn∈R|D|, 1-D RoPE nhân bias vào vector key hoặc query trong không gian vector phức:
fq(qm, m) =eimΘqm, fk(kn, n) =einΘkn(1)
trong đó Θ = Diag(θ1,···, θ|D|/2) là ma trận tần số quay với θd=b−2d/|D| và cơ sở quay b= 10000. Trong không gian thực, cho l=|D|/2, ma trận quay eimΘ bằng:

[Ma trận quay hiển thị]

Điểm số attention với 1-D RoPE được tính như:
An= Re⟨fq(qm, m), fk(kn, n)⟩ (3)

Nội suy NTK-aware (LocalLLaMA) là một kỹ thuật ngoại suy độ dài không cần huấn luyện trong LLM. Để xử lý độ dài ngữ cảnh lớn hơn Ltest so với độ dài huấn luyện tối đa Ltrain, nó sửa đổi cơ sở quay của 1-D RoPE như sau:
b′=b·s|D|/(|D|−2), (4)
trong đó hệ số tỷ lệ s được định nghĩa là:
s= max(Ltest/Ltrain,1.0). (5)

Nội suy YaRN (Yet another RoPE extensioN) (Peng et al., 2023) giới thiệu tỷ lệ của chiều d là r(d) = Ltrain/(2πb2d/|D|), và sửa đổi tần số quay là:
θ′d= (1−γ(r(d)))θd/s+γ(r(d))θd, (6)
trong đó s là hệ số tỷ lệ nói trên, và γ(r(d)) là một hàm dốc với các siêu tham số bổ sung α, β:
γ(r) = {0, nếu r < α; 1, nếu r > β; (r−α)/(β−α), trường hợp khác}. (7)

Ngoài ra, nó kết hợp một thuật ngữ chia tỷ lệ 1D-RoPE là:
f′q(qm, m) =1/√t fq(qm, m), f′k(kn, n) =1/√t fk(kn, n), (8)
trong đó 1/√t= 0.1 ln(s) + 1.

3.2. Pipeline Huấn luyện và Suy luận Linh hoạt
Các mô hình học sâu hiện đại, bị hạn chế bởi đặc tính của phần cứng GPU, yêu cầu đóng gói dữ liệu thành các batch có hình dạng đồng nhất để xử lý song song. Do sự đa dạng trong độ phân giải hình ảnh, như thể hiện trong Hình 4, DiT thay đổi kích thước và cắt hình ảnh thành độ phân giải cố định 256×256. Trong khi thay đổi kích thước và cắt như một phương tiện tăng cường dữ liệu là một thực hành phổ biến, cách tiếp cận này đưa ra những thiên lệch nhất định vào dữ liệu đầu vào. Những thiên lệch này sẽ ảnh hưởng trực tiếp đến hình ảnh cuối cùng được tạo ra bởi mô hình, bao gồm hiệu ứng mờ từ việc chuyển đổi từ độ phân giải thấp sang cao và thông tin bị mất do việc cắt (có thể tìm thấy thêm các mẫu thất bại trong Phụ lục D).

Để giải quyết vấn đề này, chúng tôi đề xuất một pipeline huấn luyện và suy luận linh hoạt, như thể hiện trong Hình 3 (a, b). Trong giai đoạn tiền xử lý, chúng tôi tránh cắt hình ảnh hoặc thay đổi kích thước hình ảnh độ phân giải thấp lên độ phân giải cao hơn. Thay vào đó, chúng tôi chỉ thay đổi kích thước hình ảnh độ phân giải cao xuống giới hạn độ phân giải tối đa được định trước, HW⩽2562. Trong giai đoạn huấn luyện, FiT đầu tiên mã hóa một hình ảnh thành mã tiềm ẩn với bộ mã hóa VAE được huấn luyện trước. Bằng cách patchify mã tiềm ẩn thành token tiềm ẩn, chúng ta có thể nhận được chuỗi với độ dài khác nhau L. Để đóng gói các chuỗi này thành một batch, chúng tôi đệm tất cả các chuỗi này đến độ dài token tối đa Lmax bằng cách sử dụng token đệm. Ở đây chúng tôi đặt Lmax= 256 để khớp với độ dài token cố định của DiT. Giống như token tiềm ẩn, chúng tôi cũng đệm embedding vị trí đến độ dài tối đa để đóng gói. Cuối cùng, chúng tôi tính hàm mất mát chỉ cho các token đầu ra khử nhiễu, trong khi loại bỏ tất cả các token đệm khác. Trong giai đoạn suy luận, chúng tôi trước tiên định nghĩa bản đồ vị trí của hình ảnh được tạo ra và lấy mẫu token nhiễu từ phân phối Gaussian làm đầu vào. Sau khi hoàn thành K lần lặp của quá trình khử nhiễu, chúng tôi định hình lại và unpatchify các token khử nhiễu theo bản đồ vị trí được định nghĩa trước để nhận được hình ảnh cuối cùng được tạo ra.

3.3. Kiến trúc Transformer Thị Giác Linh hoạt
Xây dựng dựa trên pipeline huấn luyện linh hoạt, mục tiêu của chúng tôi là tìm một kiến trúc có thể huấn luyện ổn định trên các độ phân giải khác nhau và tạo ra hình ảnh với độ phân giải và tỷ lệ khung hình tùy ý, như thể hiện trong Hình 3 (c). Được thúc đẩy bởi một số tiến bộ kiến trúc đáng kể trong LLM, chúng tôi tiến hành một loạt thí nghiệm để khám phá các sửa đổi kiến trúc dựa trên DiT, xem chi tiết trong Phần 4.2.

Thay thế MHSA bằng Masked MHSA. Pipeline huấn luyện linh hoạt giới thiệu token đệm để linh hoạt đóng gói các chuỗi động thành một batch. Trong giai đoạn tới của transformer, điều quan trọng là tạo điều kiện cho tương tác giữa các token nhiễu trong khi ngăn chặn bất kỳ tương tác nào giữa token nhiễu và token đệm. Cơ chế Multi-Head Self-Attention (MHSA) của DiT ban đầu không có khả năng phân biệt giữa token nhiễu và token đệm. Để giải quyết vấn đề này, chúng tôi sử dụng Masked MHSA để thay thế MHSA chuẩn. Chúng tôi sử dụng mặt nạ chuỗi M cho Masked Attention, trong đó token nhiễu được gán giá trị 0, và token đệm được gán giá trị âm vô cùng (-inf), được định nghĩa như sau:
Masked Attn. (Qi, Ki, Vi) =Softmax(QiKTi/√dk+M)Vi (9)
trong đó Qi,Ki,Vi là ma trận query, key, và value cho đầu thứ i.

Thay thế Absolute PE bằng 2D RoPE. Chúng tôi quan sát rằng các mô hình transformer thị giác với embedding vị trí tuyệt đối không thể tổng quát hóa tốt trên hình ảnh ngoài độ phân giải huấn luyện, như trong Phần 4.3 và 4.5. Lấy cảm hứng từ thành công của 1D-RoPE trong LLM cho ngoại suy độ dài (Liu et al., 2023), chúng tôi sử dụng 2D-RoPE để tạo điều kiện cho việc tổng quát hóa độ phân giải trong các mô hình transformer thị giác. Chính thức, chúng tôi tính 1-D RoPE cho tọa độ chiều cao và chiều rộng riêng biệt. Sau đó hai 1-D RoPE như vậy được nối trong chiều cuối cùng. Cho tọa độ 2-D của chiều rộng và chiều cao là {(w, h)|1⩽w⩽W,1⩽h⩽H}, 2-D RoPE được định nghĩa là:
fq(qm, hm, wm) = [eihmΘqm∥eiwmΘqm],
fk(kn, hn, wn) = [eihnΘkn∥eiwnΘkn], (10)
trong đó Θ = Diag(θ1,···, θ|D|/4), và ∥ biểu thị nối hai vector trong chiều cuối cùng. Lưu ý rằng chúng tôi chia không gian |D|-chiều thành không gian con |D|/4-chiều để đảm bảo tính nhất quán của chiều, khác với không gian con |D|/2-chiều trong 1-D RoPE. Tương tự, điểm số attention với 2-D RoPE là:
An= Re⟨fq(qm, hm, wm), fk(kn, hn, wn)⟩. (11)

Đáng chú ý là không có thuật ngữ chéo giữa h và w trong 2D-RoPE và điểm số attention An, vì vậy chúng ta có thể tiếp tục tách tần số quay thành Θh và Θw, dẫn đến 2D-RoPE tách rời, sẽ được thảo luận trong Phần 3.4 và chi tiết hơn có thể được tìm thấy trong Phụ lục B.

Thay thế MLP bằng SwiGLU. Chúng tôi theo các LLM gần đây như LLaMA (Touvron et al., 2023a;b), và thay thế MLP trong FFN bằng SwiGLU, được định nghĩa như sau:
SwiGLU (x, W, V) =SiLU (xW)⊗(xV)
FFN(x) =SwiGLU (x, W1, W2)W3 (12)
trong đó ⊗ biểu thị Hadmard Product, W1,W2, và W3 là các ma trận trọng số không có bias, SiLU (x) =x⊗σ(x). Ở đây chúng tôi sẽ sử dụng SwiGLU làm lựa chọn trong mỗi khối FFN.

3.4. Ngoại suy Độ phân giải Không cần Huấn luyện
Chúng tôi ký hiệu độ phân giải suy luận là (Htest,Wtest). FiT của chúng tôi có thể xử lý các độ phân giải và tỷ lệ khung hình khác nhau trong quá trình huấn luyện, vì vậy chúng tôi ký hiệu độ phân giải huấn luyện là Ltrain=√Lmax. Bằng cách thay đổi hệ số tỷ lệ trong Phương trình (5) thành s= max(max(Htest, Wtest)/Ltrain,1.0), chúng ta có thể trực tiếp triển khai các phương pháp nội suy vị trí trong ngoại suy mô hình ngôn ngữ lớn trên 2D-RoPE, mà chúng tôi gọi là triển khai NTK và YaRN vanilla. Hơn nữa, chúng tôi đề xuất các phương pháp nội suy RoPE thị giác bằng cách sử dụng thuộc tính tách trong 2D-RoPE tách rời. Chúng tôi sửa đổi Phương trình (10) thành:
f̂q(qm, hm, wm) = [eihmΘhqm∥eiwmΘwqm],
f̂k(kn, hn, wn) = [eihnΘhkn∥eiwnΘwkn], (13)
trong đó Θh={θhd=b−2d/|D|h,1⩽d⩽|D|/2} và Θw={θwd=b−2d/|D|w,1⩽d⩽|D|/2} được tính riêng biệt.

Theo đó, hệ số tỷ lệ của chiều cao và chiều rộng được định nghĩa riêng biệt là
sh= max(Htest/Ltrain,1.0), sw= max(Wtest/Ltrain,1.0). (14)

Định nghĩa 3.1. Định nghĩa Nội suy VisionNTK là một sửa đổi của Nội suy NTK-aware bằng cách sử dụng Phương trình (13) với cơ sở quay sau.
bh=b·s|D|/(|D|−2)h, bw=b·s|D|/(|D|−2)w, (15)
trong đó b= 10000 giống với Phương trình (1)

Định nghĩa 3.2. Định nghĩa Nội suy VisionYaRN là một sửa đổi của Nội suy YaRN bằng cách sử dụng Phương trình (13) với tần số quay sau.
θhd= (1−γ(r(d))θd/sh+γ(r(d))θd,
θwd= (1−γ(r(d))θd/sw+γ(r(d))θd, (16)
trong đó γ(r(d)) giống với Phương trình (6).

Đáng chú ý là VisionNTK và VisionYaRN là các cách tiếp cận nội suy embedding vị trí không cần huấn luyện, được sử dụng để giảm thiểu vấn đề embedding vị trí ngoài phân phối trong ngoại suy. Khi tỷ lệ khung hình bằng một, chúng tương đương với việc triển khai vanilla của NTK và YaRN. Chúng đặc biệt hiệu quả trong việc tạo ra hình ảnh với tỷ lệ khung hình tùy ý, xem Phần 4.3.

4. Thí nghiệm
4.1. Triển khai FiT
Chúng tôi trình bày chi tiết triển khai của FiT, bao gồm kiến trúc mô hình, chi tiết huấn luyện, và các chỉ số đánh giá.

Kiến trúc mô hình. Chúng tôi theo DiT-B và DiT-XL để đặt cùng số lớp, kích thước ẩn, và đầu attention cho mô hình cơ sở FiT-B và mô hình xlarge FiT-XL. Như DiT tiết lộ hiệu suất tổng hợp mạnh hơn khi sử dụng kích thước patch nhỏ hơn, chúng tôi sử dụng kích thước patch p=2, ký hiệu là FiT-B/2 và FiT-XL/2. FiT áp dụng cùng VAE được huấn luyện trước sẵn có (Esser et al., 2021) như DiT được cung cấp bởi Stable Diffusion (Rombach et al., 2022) để mã hóa/giải mã token hình ảnh/tiềm ẩn. Bộ mã hóa VAE có tỷ lệ downsampling 1/8 và chiều kênh đặc trưng là 4. Một hình ảnh kích thước 160×320×3 được mã hóa thành mã tiềm ẩn kích thước 20×40×4. Mã tiềm ẩn kích thước 20×40×4 được patchify thành token tiềm ẩn có độ dài L= 10×20 = 200.

Chi tiết huấn luyện. Chúng tôi huấn luyện các mô hình FiT tiềm ẩn có điều kiện lớp dưới giới hạn độ phân giải tối đa được định trước, HW⩽2562 (tương đương với độ dài token L≤256), trên tập dữ liệu ImageNet (Deng et al., 2009). Chúng tôi thay đổi kích thước xuống các hình ảnh độ phân giải cao để đáp ứng giới hạn HW⩽2562 trong khi duy trì tỷ lệ khung hình. Chúng tôi theo DiT để sử dụng Horizontal Flip Augmentation. Chúng tôi sử dụng cùng cài đặt huấn luyện như DiT: tỷ lệ học không đổi 1×10−4 sử dụng AdamW (Loshchilov & Hutter, 2017), không có weight decay, và batch size là 256. Theo thực hành thông thường trong tài liệu mô hình hóa tạo sinh, chúng tôi áp dụng trung bình động mũ (EMA) của trọng số mô hình trong quá trình huấn luyện với sự suy giảm 0.9999. Tất cả kết quả được báo cáo sử dụng mô hình EMA. Chúng tôi giữ nguyên các siêu tham số khuếch tán giống như DiT.

Chi tiết đánh giá và chỉ số. Chúng tôi đánh giá các mô hình với một số chỉ số thường được sử dụng, tức là Fréchet Inception Distance (FID) (Heusel et al., 2017), sFID (Nash et al., 2021), Inception Score (IS) (Salimans et al., 2016), Precision và Recall được cải thiện (Kynkäänniemi et al., 2019). Để so sánh công bằng, chúng tôi theo DiT để sử dụng đánh giá TensorFlow từ ADM (Dhariwal & Nichol, 2021) và báo cáo FID-50K với 250 bước lấy mẫu DDPM. FID được sử dụng làm chỉ số chính vì nó đo lường cả tính đa dạng và độ trung thực. Chúng tôi bổ sung báo cáo IS, sFID, Precision, và Recall như các chỉ số thứ cấp. Đối với thí nghiệm kiến trúc FiT (Phần 4.2) và thí nghiệm ablation ngoại suy độ phân giải (Phần 4.3), chúng tôi báo cáo kết quả mà không sử dụng hướng dẫn không có classifier (Ho & Salimans, 2021).

Độ phân giải đánh giá. Không giống như công trình trước đây chủ yếu tiến hành thí nghiệm trên tỷ lệ khung hình cố định 1:1, chúng tôi tiến hành thí nghiệm trên các tỷ lệ khung hình khác nhau, lần lượt là 1:1, 1:2, và 1:3. Mặt khác, chúng tôi chia thí nghiệm thành độ phân giải trong phân phối huấn luyện và độ phân giải ngoài phân phối huấn luyện. Đối với độ phân giải trong phân phối, chúng tôi chủ yếu sử dụng 256×256(1:1), 160×320(1:2), và 128×384(1:3) để đánh giá, với lần lượt 256, 200, 192 token tiềm ẩn. Tất cả độ dài token đều nhỏ hơn hoặc bằng 256, dẫn đến các độ phân giải tương ứng trong phân phối huấn luyện. Đối với độ phân giải ngoài phân phối, chúng tôi chủ yếu sử dụng 320×320(1:1), 224×448(1:2), và 160×480(1:3) để đánh giá, với lần lượt 400, 392, 300 token tiềm ẩn. Tất cả độ dài token đều lớn hơn 256, dẫn đến các độ phân giải ngoài phân phối huấn luyện. Thông qua sự phân chia như vậy, chúng tôi đánh giá toàn diện khả năng tổng hợp hình ảnh và ngoại suy độ phân giải của FiT ở các độ phân giải và tỷ lệ khung hình khác nhau.

4.2. Thiết kế Kiến trúc FiT
Trong phần này, chúng tôi tiến hành một nghiên cứu ablation để xác minh các thiết kế kiến trúc trong FiT. Chúng tôi báo cáo kết quả của các mô hình FiT-B/2 biến thể khác nhau tại 400K bước huấn luyện và sử dụng FID-50k, sFID, IS, Precision, và Recall làm chỉ số đánh giá. Chúng tôi tiến hành thí nghiệm tại ba độ phân giải khác nhau: 256×256, 160×320, và 224×448. Các độ phân giải này được chọn để bao gồm các tỷ lệ khung hình khác nhau, cũng như bao gồm các độ phân giải cả trong và ngoài phân phối.

Huấn luyện linh hoạt vs. Huấn luyện cố định. Pipeline huấn luyện linh hoạt cải thiện đáng kể hiệu suất trên các độ phân giải khác nhau. Cải thiện này rõ ràng không chỉ trong các độ phân giải trong phân phối mà còn mở rộng đến các độ phân giải ngoài phân phối huấn luyện, như thể hiện trong Bảng 3. Config A là mô hình DiT-B/2 ban đầu chỉ với huấn luyện linh hoạt, cải thiện hiệu suất một chút (-1.49 FID) so với DiT-B/2 với huấn luyện độ phân giải cố định ở độ phân giải 256×256. Config A thể hiện cải thiện hiệu suất đáng kể thông qua huấn luyện linh hoạt. So với DiT-B/2, điểm FID giảm 40.81 và 56.55 ở độ phân giải 160×320 và 224×448.

SwiGLU vs. MLP. SwiGLU cải thiện hiệu suất một chút trên các độ phân giải khác nhau, so với MLP. Config B là mô hình huấn luyện linh hoạt FiT-B/2 thay thế MLP bằng SwiGLU. So với Config A, Config B thể hiện cải thiện đáng chú ý trên các độ phân giải khác nhau. Cụ thể, đối với độ phân giải 256×256, 160×320, và 224×448, Config B giảm điểm FID lần lượt là 1.59, 1.85, và 0.21 trong Bảng 3. Vì vậy FiT sử dụng SwiGLU trong FFN.

2D RoPE vs. Absolute PE. 2D RoPE thể hiện hiệu quả cao hơn so với mã hóa vị trí tuyệt đối, và nó có khả năng ngoại suy đáng kể trên các độ phân giải khác nhau. Config D là mô hình huấn luyện linh hoạt FiT-B/2 thay thế absolute PE bằng 2D RoPE. Đối với các độ phân giải trong phân phối huấn luyện, cụ thể 256×256 và 160×320, Config D giảm điểm FID lần lượt là 6.05 và 5.45 trong Bảng 3, so với Config A. Đối với độ phân giải ngoài phân phối huấn luyện, 224×448, Config D thể hiện khả năng ngoại suy đáng kể (-6.39 FID) so với Config A. Config C giữ lại cả absolute PE và 2D RoPE. Tuy nhiên, trong so sánh giữa Config C và Config D, chúng tôi quan sát thấy rằng Config C hoạt động tệ hơn. Đối với độ phân giải 256x256, 160x320, và 224x448, Config C tăng điểm FID lần lượt là 1.82, 1.65, và 0.44, so với Config D. Do đó, chỉ 2D RoPE được sử dụng cho embedding vị trí trong triển khai của chúng tôi.

Kết hợp tất cả. FiT thể hiện tính ưu việt đáng kể và toàn diện trên các cài đặt độ phân giải khác nhau, so với DiT ban đầu. FiT đã đạt được hiệu suất tiên tiến nhất trên các cấu hình khác nhau. So với DiT-B/2, FiT-B/2 giảm điểm FID 8.47 trên độ phân giải phổ biến nhất 256×256 trong Bảng 3. Hơn nữa, FiT-B/2 đã có những cải thiện hiệu suất đáng kể ở độ phân giải 160×320 và 224×448, giảm điểm FID lần lượt là 47.36 và 64.43.

4.3. Thiết kế Ngoại suy Độ phân giải FiT
Trong phần này, chúng tôi áp dụng các mô hình DiT-B/2 và FiT-B/2 tại 400K bước huấn luyện để đánh giá hiệu suất ngoại suy trên ba độ phân giải ngoài phân phối: 320×320, 224×448 và 160×480. Ngoại suy trực tiếp không hoạt động tốt trên độ phân giải lớn hơn ngoài phân phối huấn luyện. Vì vậy chúng tôi tiến hành phân tích đánh giá toàn diện tập trung vào các phương pháp nội suy embedding vị trí.

PI và EI. PI (Position Interpolation) và EI (Embedding Interpolation) là hai phương pháp nội suy embedding vị trí cơ sở cho ngoại suy độ phân giải. PI giảm tuyến tính các tọa độ vị trí suy luận để khớp với tọa độ ban đầu. EI thay đổi kích thước embedding vị trí bằng nội suy song tuyến tính. Theo ViT (Dosovitskiy et al., 2020), EI được sử dụng cho embedding vị trí tuyệt đối.

NTK và YaRN. Chúng tôi đặt hệ số tỷ lệ thành s= max(max(Htest, Wtest)/√256) và áp dụng triển khai vanilla của hai phương pháp, như trong Phần 3.1. Đối với YaRN, chúng tôi đặt α= 1, β= 32 trong Phương trình (7).

VisionNTK và VisionYaRN. Hai phương pháp này được định nghĩa chi tiết trong Định nghĩa 3.1 và 3.2. Lưu ý rằng khi tỷ lệ khung hình bằng một, VisionNTK và VisionYaRN tương đương với NTK và YaRN.

Phân tích. Chúng tôi trình bày trong Bảng 4 rằng FiT-B/2 của chúng tôi thể hiện hiệu suất ổn định khi trực tiếp ngoại suy đến độ phân giải lớn hơn. Khi kết hợp với PI, hiệu suất ngoại suy của FiT-B/2 ở cả ba độ phân giải đều giảm. Khi kết hợp với YaRN, điểm FID giảm 16.77 trên 320×320, nhưng hiệu suất trên 224×448 và 168×480 giảm. VisionYaRN của chúng tôi giải quyết tình trạng khó xử này và giảm điểm FID 40.27 trên 224×448 và 41.22 tại 160×480 so với YaRN. Phương pháp nội suy NTK thể hiện hiệu suất ngoại suy ổn định nhưng tăng điểm FID một chút ở độ phân giải 224×448 và 160×480. Phương pháp VisionNTK của chúng tôi giảm thiểu vấn đề này và vượt trội hơn hiệu suất ngoại suy trực tiếp ở cả ba độ phân giải. Tóm lại, FiT-B/2 của chúng tôi có khả năng ngoại suy mạnh mẽ, có thể được nâng cao thêm khi kết hợp với các phương pháp VisionYaRN và VisionNTK.

Tuy nhiên, DiT-B/2 thể hiện khả năng ngoại suy kém. Khi kết hợp với PI, điểm FID đạt 72.47 ở độ phân giải 320×320, vẫn kém hơn FiT-B/2 của chúng tôi. Ở độ phân giải 224×448 và 160×480, các phương pháp nội suy PI và EI không thể cải thiện hiệu suất ngoại suy.

4.4. Kết quả Độ phân giải Trong Phân phối FiT
Theo phân tích trước đây của chúng tôi, chúng tôi huấn luyện mô hình Gflops cao nhất, FiT-XL/2, trong 1.8M bước. Chúng tôi tiến hành thí nghiệm để đánh giá hiệu suất của FiT tại ba độ phân giải trong phân phối khác nhau: 256×256, 160×320, và 128×384. Chúng tôi hiển thị các mẫu từ FiT trong Hình 1, và chúng tôi so sánh với một số mô hình tạo sinh có điều kiện lớp tiên tiến: BigGAN (Brock et al., 2018), StyleGAN-XL (Sauer et al., 2022), MaskGIT (Chang et al., 2022), CDM (Ho et al., 2022), U-ViT (Bao et al., 2023), ADM (Dhariwal & Nichol, 2021), LDM (Rombach et al., 2022), MDT (Gao et al., 2023), và DiT (Peebles & Xie, 2023). Khi tạo ra hình ảnh độ phân giải 160×320 và 128×384, chúng tôi áp dụng PI trên embedding vị trí của mô hình DiT, như đã nêu trong Phần 4.3. EI được sử dụng trong embedding vị trí của các mô hình U-ViT và MDT, vì chúng sử dụng embedding vị trí có thể học được. ADM và LDM có thể trực tiếp tổng hợp hình ảnh với độ phân giải khác với độ phân giải huấn luyện.

Như thể hiện trong Bảng 1, FiT-XL/2 vượt trội hơn tất cả các mô hình khuếch tán trước đây, giảm FID-50K tốt nhất trước đây là 6.93 do U-ViT-H/2-G đạt được xuống 5.74 ở độ phân giải 160×320. Đối với độ phân giải 128×384, FiT-XL/2 thể hiện tính ưu việt đáng kể, giảm FID-50K SOTA trước đây từ 29.67 xuống 16.81. Điểm FID của FiT-XL/2 tăng nhẹ ở độ phân giải 256×256, so với các mô hình khác đã trải qua các bước huấn luyện dài hơn.

4.5. Kết quả Độ phân giải Ngoài Phân phối FiT
Chúng tôi đánh giá FiT-XL/2 của chúng tôi trên ba độ phân giải ngoài phân phối khác nhau: 320×320, 224×448, và 160×480 và so sánh với một số mô hình tạo sinh có điều kiện lớp SOTA: U-ViT, ADM, LDM-4, MDT, và DiT. PI được sử dụng trong DiT, trong khi EI được áp dụng trong U-ViT và MDT, như trong Phần 4.4. Các phương pháp dựa trên U-Net, chẳng hạn như ADM và LDM-4 có thể trực tiếp tạo ra hình ảnh với độ phân giải ngoài phân phối. Như thể hiện trong Bảng 2, FiT-XL/2 đạt được FID-50K và IS tốt nhất trên cả ba độ phân giải, cho thấy khả năng ngoại suy xuất sắc của nó. Về các chỉ số khác, như sFID, FiT-XL/2 thể hiện hiệu suất cạnh tranh.

LDM với xương sống transformer được biết đến là gặp khó khăn trong việc tạo ra hình ảnh ngoài độ phân giải huấn luyện, chẳng hạn như DiT, U-ViT, và MDT. Nghiêm trọng hơn, MDT hầu như không có khả năng tạo ra hình ảnh ngoài độ phân giải huấn luyện. Chúng tôi suy đoán điều này là do cả PE tuyệt đối có thể học được và PE tương đối có thể học được đều được sử dụng trong MDT. DiT và U-ViT thể hiện một mức độ khả năng ngoại suy nhất định và đạt được điểm FID lần lượt là 9.98 và 7.65 ở độ phân giải 320x320. Tuy nhiên, khi tỷ lệ khung hình không bằng một, hiệu suất tạo sinh của chúng giảm đáng kể, như ở độ phân giải 224×448 và 160×480. Được hưởng lợi từ lợi thế của trường tiếp nhận cục bộ của Mạng Neural Tích chập, ADM và LDM thể hiện hiệu suất ổn định ở các độ phân giải ngoài phân phối này. FiT-XL/2 của chúng tôi giải quyết vấn đề thiếu khả năng ngoại suy của transformer trong tổng hợp hình ảnh. Ở độ phân giải 320×320, 224×448, và 160×480, FiT-XL/2 vượt trội hơn LDM SOTA trước đây trên FID-50K lần lượt là 0.82, 0.65, và 3.52.

5. Kết luận
Trong công trình này, chúng tôi hướng đến việc đóng góp vào nghiên cứu đang diễn ra về việc tạo ra một cách linh hoạt các độ phân giải và tỷ lệ khung hình tùy ý. Chúng tôi đề xuất Transformer Thị Giác Linh Hoạt (FiT) cho mô hình khuếch tán, một kiến trúc transformer được tinh chỉnh với pipeline huấn luyện linh hoạt được thiết kế đặc biệt để tạo ra hình ảnh với độ phân giải và tỷ lệ khung hình tùy ý. FiT vượt trội hơn tất cả các mô hình trước đây, cho dù dựa trên transformer hay CNN, trên các độ phân giải khác nhau. Với phương pháp ngoại suy độ phân giải của chúng tôi, VisionNTK, hiệu suất của FiT đã được nâng cao đáng kể hơn nữa.
