InfoDiffusion: Quá trình khuếch tán nhận thức entropy thông tin cho việc tạo văn bản phi tự hồi quy

Renzhi Wang1,2, Jing Li3, Piji Li1,2∗
1Khoa Khoa học máy tính và Công nghệ,
Đại học Hàng không và Vũ trụ Nam Kinh, Trung Quốc
2Phòng thí nghiệm trọng điểm MIIT về Phân tích mẫu và Trí tuệ máy, Nam Kinh, Trung Quốc
3Khoa Điện toán, Đại học Bách khoa Hồng Kông, Trung Quốc
1{rzhwang,pjli}@nuaa.edu.cn
3jing-amelia.li@polyu.edu.hk

Tóm tắt

Các mô hình khuếch tán đã thu hút được sự quan tâm đáng kể trong lĩnh vực tạo văn bản. Một số nghiên cứu đã khám phá các mô hình khuếch tán văn bản với các cấu trúc khác nhau và áp dụng chúng vào các nhiệm vụ khác nhau, bao gồm nhận dạng thực thể có tên và tóm tắt. Tuy nhiên, tồn tại một sự khác biệt đáng chú ý giữa quá trình tạo văn bản "dễ trước" của các mô hình khuếch tán hiện tại và quá trình tạo văn bản tự nhiên "từ khóa trước" của con người, điều này đã nhận được sự quan tâm hạn chế. Để thu hẹp khoảng cách này, chúng tôi đề xuất InfoDiffusion, một mô hình khuếch tán văn bản phi tự hồi quy. Phương pháp của chúng tôi giới thiệu một chiến lược tạo "thông tin chính trước" và tích hợp một lịch trình nhiễu dựa trên lượng thông tin văn bản. Ngoài ra, InfoDiffusion kết hợp tự điều kiện với một cấu trúc mô hình nhiễu một phần mới được đề xuất. Kết quả thực nghiệm cho thấy InfoDiffusion vượt trội hơn mô hình cơ sở về chất lượng và tính đa dạng của việc tạo ra, cũng như thể hiện hiệu quả lấy mẫu cao hơn.

1 Giới thiệu

Tạo phi tự hồi quy (NAR) đề cập đến một phương pháp tạo ra các chuỗi trong đó mỗi phần tử được tạo ra một cách độc lập, không dựa vào các phần tử được tạo ra trước đó, cho phép tạo ra song song nhanh hơn nhưng có khả năng hy sinh độ chính xác của việc tạo ra (Xiao et al., 2022). Gần đây, các mô hình khuếch tán đã chứng minh khả năng tạo ra mạnh mẽ trong các nhiệm vụ tạo hình ảnh, dần dần trở thành một mô hình mới trong các mô hình tạo sinh. Việc áp dụng thành công các mô hình khuếch tán vào dữ liệu liên tục như hình ảnh và âm thanh đã thúc đẩy các nhà nghiên cứu giới thiệu chúng vào dữ liệu rời rạc như văn bản. Các nghiên cứu trước đây đã cố gắng tích hợp các mô hình khuếch tán vào việc tạo văn bản phi tự hồi quy, thiết kế các cấu trúc mô hình khuếch tán văn bản khác nhau và áp dụng chúng vào các nhiệm vụ tạo văn bản khác nhau, chẳng hạn như nhận dạng thực thể có tên (Shen et al., 2023) và tóm tắt (Zhang et al., 2023). Tuy nhiên, những công trình này đã không nhận ra một sự khác biệt cơ bản giữa quá trình tạo văn bản với các mô hình khuếch tán và quá trình tạo văn bản thực tế của con người, điều này có thể là một lý do tại sao các mô hình khuếch tán văn bản liên tục tụt hậu về hiệu quả và chất lượng tạo ra.

Nghiên cứu trước đây đã phát hiện ra rằng các mô hình khuếch tán văn bản dường như tuân theo một nguyên tắc "dễ trước" (Emelianenko et al., 2019; He et al., 2022) trong quá trình giải mã. Nguyên tắc "dễ trước" có nghĩa là mô hình có xu hướng tạo ra các token được quan sát thường xuyên nhất (và ít bất ngờ nhất) trong kho dữ liệu huấn luyện, để đạt được xác suất cao hơn. Khi ngữ cảnh trở nên phong phú hơn, nhiều chi tiết hơn được tích hợp vào chuỗi. Hình 1 minh họa quá trình giải mã của một số mô hình khuếch tán văn bản hiện có, trong đó có thể quan sát thấy rằng mô hình có xu hướng ưu tiên tạo ra các từ đơn giản, tần suất cao và nghèo về mặt ngữ nghĩa như "the" và "of" trước khi tạo ra các từ ít thường xuyên hơn nhưng có thông tin nhiều hơn và phong phú về mặt ngữ nghĩa như "structure" và "remember". Điều này khác với thứ tự thực tế mà con người xử lý hoặc tạo ra văn bản. Mọi người có xu hướng ưu tiên các phần cốt lõi của một câu hoặc một đoạn văn, chứa thông tin quan trọng (Grice, 1975). Ví dụ, khi được hỏi: "Kế hoạch sắp tới của bạn là gì?", bạn sẽ trả lời: "Tôi sẽ hoàn thành một bài nghiên cứu." Trong quá trình này, những từ xuất hiện trong tâm trí bạn đầu tiên rất có thể là "paper" hoặc "finish", vì chúng mang thông tin quan trọng hoặc có entropy thông tin cao hơn, chứ không phải những từ vô nghĩa như "a" hoặc "the". Thật khó tưởng tượng làm thế nào một người có phản ứng đầu tiên là "the" lại có thể trả lời câu hỏi được đề cập ở trên. Tương tự, điều này cũng thảm khốc đối với một mô hình ngôn ngữ mà chúng ta mong muốn truyền đạt khả năng ngôn ngữ và thậm chí cả suy nghĩ.

Thứ tự giải mã không nhất quán này trong các mô hình khuếch tán văn bản có thể dẫn đến chất lượng tạo ra kém và hiệu quả thấp. Một mặt, do thực tế là phần cốt lõi của một câu (thông tin chính) được tạo ra một cách chính xác trong nửa sau của quá trình lấy mẫu, mô hình thiếu sự hiểu biết toàn diện về ý nghĩa ngữ nghĩa tổng thể của câu trong giai đoạn đầu, dẫn đến chất lượng tạo ra không thỏa mãn. Mặt khác, việc thiếu hướng dẫn từ thông tin chính trong nửa đầu của quá trình lấy mẫu dẫn đến việc tạo ra nhiều từ vô nghĩa hoặc không liên quan từ đầu. Do sự hiện diện của những bước lấy mẫu vô nghĩa hoặc thậm chí sai lầm này, hiệu quả của quá trình lấy mẫu của mô hình thấp.

Để giải quyết các vấn đề nêu trên, chúng tôi đề xuất một mô hình tạo văn bản phi tự hồi quy mới có tên InfoDiffusion. Chúng tôi thiết kế một lịch trình nhiễu mới dựa trên entropy thông tin của văn bản, cho phép mô hình nhận thức được thông tin mà mỗi từ mang trong một câu. Hướng dẫn này giúp mô hình ưu tiên tạo ra thông tin chính trong quá trình lấy mẫu, từ đó nâng cao chất lượng và tốc độ của việc lấy mẫu. Hơn nữa, chúng tôi đã tích hợp tự điều kiện để cải thiện thêm chất lượng đầu ra được tạo ra và sử dụng kỹ thuật "nhiễu một phần và khử nhiễu có điều kiện" để thực hiện các nhiệm vụ chuỗi-đến-chuỗi.

Tóm lại, những đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất một mô hình tạo văn bản phi tự hồi quy mới có tên InfoDiffusion, và cho phép mô hình nhận thức entropy thông tin chứa trong văn bản để ưu tiên tạo ra thông tin chính trong quá trình lấy mẫu.
• Chúng tôi kết hợp tự điều kiện và "nhiễu một phần và khử nhiễu có điều kiện" để đạt được việc tạo văn bản chuỗi-đến-chuỗi chất lượng cao.
• Kết quả thực nghiệm chứng minh rằng InfoDiffusion, tuân theo thứ tự tạo "thông tin chính trước" nhất quán với con người, đạt được chất lượng tạo ra tốt hơn và hiệu quả cao hơn so với các mô hình cơ sở trong bốn nhiệm vụ tạo văn bản.

2 Kiến thức cơ bản

2.1 Mô hình khuếch tán

Mô hình khuếch tán là một lớp mô hình biến tiềm ẩn được đặc trưng bởi một quá trình Markov tiến và lùi (Sohl-Dickstein et al., 2015; Ho et al., 2020). Trong khung này, cho một mẫu từ phân phối dữ liệu x0∼q(x0), quá trình tiến tạo ra một chuỗi các biến tiềm ẩn x1, ..., xT bằng cách lấy mẫu từ:

q(xt|xt−1) = N(xt; √(1−βt)xt−1, βtI)                    (1)

trong đó βt ∈ (0,1) là một lịch trình nhiễu kiểm soát kích thước bước của việc thêm nhiễu. Dựa trên thủ thuật tái tham số hóa, biến tiềm ẩn trung gian tùy ý xt có thể được lấy mẫu dưới dạng đóng:

q(xt|x0) = N(xt; √ᾱtx0, √(1−ᾱt)I)                      (2)

trong đó αt = 1−βt, ᾱt = ∏(i=1 to t)αi. Theo một lịch trình nhiễu được xác định trước, βt tăng (αt giảm) khi bước thời gian tăng và cuối cùng làm hỏng x0 thành một nhiễu ngẫu nhiên. Nếu βt đủ nhỏ, quá trình ngược q(xt−1|xt) cũng là một Gaussian, được học bởi một mô hình tham số hóa:

pθ(xt−1|xt) = N(xt−1; μθ(xt, t), Σθ(xt, t))              (3)

trong đó μθ(xt, t) và Σθ(xt, t) có thể được triển khai bởi một mạng khử nhiễu fθ(xt, t) như U-Net hoặc Transformer (Li et al., 2023). Trong quá trình suy luận, quá trình ngược bắt đầu với việc lấy mẫu nhiễu từ một phân phối Gaussian p(xT) = N(xT; 0, I) và lặp đi lặp lại khử nhiễu nó bằng pθ(xt−1|xt) cho đến khi thu được x0. Mục tiêu học của các mô hình khuếch tán được rút ra từ giới hạn dưới biến phân của khả năng âm của đầu vào x0, được ký hiệu là:

Lvlb = Eq[DKL(q(xT|x0)∥pθ(xT))]
      + ∑(t=2 to T)Eq[DKL(q(xt−1|xt, x0)∥pθ(xt−1|xt))]
      − Eq[log pθ(x0|x1)]                                 (4)

trong đó Eq biểu thị kỳ vọng trên phân phối kết hợp q(x0:T). Với điều kiện bổ sung trên x0, phân phối hậu nghiệm của quá trình tiến q(xt−1|xt, x0) trở nên dễ xử lý bằng cách sử dụng định lý Bayes, sau đó mục tiêu đơn giản hóa Lsimple có thể được biểu diễn như:

Lsimple = ∑(t=1 to T)Eq[∥μt(xt, x0) − μθ(xt, x0)∥²]        (5)

trong đó μt là trung bình của hậu nghiệm q(xt−1|xt, x0). Thông qua các chiến lược tham số hóa khác nhau, mục tiêu dự đoán cũng có thể là nhiễu (Ho et al., 2020) hoặc dữ liệu gốc x0 (Li et al., 2022).

2.2 Mô hình khuếch tán văn bản liên tục

Để thích ứng các mô hình khuếch tán cho dữ liệu văn bản rời rạc, một cách tiếp cận đơn giản là sử dụng nhúng từ, ánh xạ các token rời rạc vào không gian vector từ liên tục trước khi trải qua quá trình khuếch tán liên tục. Mô hình khuếch tán văn bản liên tục (Li et al., 2023), còn được biết đến như mô hình khuếch tán nhúng, giới thiệu một bước nhúng qφ(x0|w) = N(EMB(w), σ0I) trong quá trình tiến, trong đó EMB(w) biểu diễn một hàm nhúng được khởi tạo ngẫu nhiên hoặc thu được từ một mô hình được huấn luyện trước (như BERT) chiếu token rời rạc w vào không gian vector từ liên tục. Đối với quá trình lùi, mô hình khuếch tán văn bản ánh xạ các vector từ liên tục trở lại thành các từ thực tế tương ứng thông qua mô-đun làm tròn từ pθ(w|x0) = ∏(i=1 to n)pθ(wi|xi).

Quá trình suy luận bắt đầu từ nhiễu ngẫu nhiên xT và tuân theo quá trình khuếch tán liên tục điển hình được đề cập trong Mục 2.1 kết hợp với làm tròn từ để tái tạo từ đích từ nhiễu. Để học cùng lúc mạng khử nhiễu và nhúng từ, mô hình khuếch tán văn bản liên tục mở rộng mục tiêu huấn luyện trong Phương trình 4 thành một mục tiêu từ đầu đến cuối mới (Li et al., 2022):

Le2e_vlb = Eq[Lvlb + log qφ(x0|w) − log pθ(w|x0)]          (6)

có thể được đơn giản hóa thêm như:

Le2e_simple = Eq[Lsimple + ∥EMB(w) − μθ(x1, x0)∥² − log pθ(w|x0)]  (7)

3 InfoDiffusion

Trong phần này, chúng tôi giới thiệu thiết kế chi tiết của InfoDiffusion. Kiến trúc mô hình tổng thể của InfoDiffusion được mô tả trong Hình 2. InfoDiffusion tích hợp Lịch trình nhiễu nhận thức entropy thông tin, cho phép mô hình tuân theo chiến lược tạo "thông tin chính trước", từ đó đạt được việc tạo văn bản phù hợp với các quá trình giống con người. Ngoài ra, InfoDiffusion kết hợp tự điều kiện và nhiễu một phần để đạt được việc tạo văn bản nhanh hơn và vượt trội hơn.

3.1 Lịch trình nhiễu nhận thức entropy thông tin

Trong các mô hình khuếch tán, lịch trình nhiễu là một thành phần quan trọng. Lịch trình nhiễu trực tiếp xác định cách dữ liệu gốc được nhiễu loạn dần dần trong quá trình tiến và cách mô hình học cách phục hồi dữ liệu đích từ nhiễu trong quá trình ngược. Điều này cũng dẫn đến ảnh hưởng đáng kể của lịch trình nhiễu đến chất lượng và tính đa dạng của các mẫu được tạo ra. Trước đây, các lịch trình nhiễu thường được sử dụng, chẳng hạn như lịch trình tuyến tính (Ho et al., 2020) và lịch trình cosine (Nichol và Dhariwal, 2021), đã cho thấy kết quả đầy hứa hẹn trong các nhiệm vụ tạo hình ảnh. Tuy nhiên, những lịch trình này giả định tất cả các token mang cùng một lượng thông tin và không xem xét sự khác biệt ngôn ngữ giữa các token trong một chuỗi. Điều này trực tiếp dẫn đến việc mô hình "đi tắt": nó có xu hướng tạo ra các token xuất hiện thường xuyên nhất (và dễ nhất) trong kho dữ liệu huấn luyện để đạt được xác suất cao hơn. Tuy nhiên, thứ tự tạo ra này mâu thuẫn với mô hình hành vi của con người, những người có xu hướng ưu tiên suy nghĩ và tạo ra các phần cốt lõi của văn bản, chứa nội dung thông tin cao hơn. Chúng tôi gọi chiến lược của con người này là ưu tiên tạo ra các phần có nội dung thông tin cao trong văn bản là "thông tin chính trước", viết tắt là "keyinfo-first".

Để giải quyết sự khác biệt trong quá trình tạo ra được đề cập ở trên và cho phép mô hình tạo ra văn bản giống con người hơn, chúng tôi đã thiết kế một lịch trình nhiễu mới có thể nhận thức entropy thông tin của các từ trong một câu. Nghĩa là, trong giai đoạn đầu của quá trình tiến, các từ có thông tin thấp bị nhiễu loạn, và các từ có thông tin cao bị nhiễu loạn ở giai đoạn cuối, từ đó hướng dẫn mô hình ưu tiên tạo ra thông tin chính trong quá trình ngược.

Cụ thể, trước tiên chúng tôi nội suy tuyến tính thông tin tương hỗ giữa các biến tiềm ẩn xt và dữ liệu gốc x0 về 0, tức là I(xt; x0) ≈ (1−t/T)H(x0), trong đó H biểu thị entropy, đo lường lượng thông tin của một biến ngẫu nhiên. Trong trường hợp này, hàm nhiễu trong mô hình khuếch tán cổ điển trở thành β = 1/(T−t+1) (Phương trình 1) và ᾱt = 1−t/T (Phương trình 2) (Austin et al., 2021). Hơn nữa, để ưu tiên nhiễu loạn các từ có thông tin thấp hơn trước khi nhiễu loạn các từ có thông tin cao hơn, chúng tôi thiết kế trọng số nhiễu dựa trên entropy thông tin của mỗi từ trong một câu w. Chi tiết cụ thể như sau:

ᾱᵢₜ = 1 − t/T + λ(t)e(wᵢ) ∈ [0,1]                        (8)

λ(t) = λ sin(t/T π)                                      (9)

e(wᵢ) = [H(wᵢ) − H̄(w)] / [max(H(wⱼ)) − min(H(wⱼ))]     (10)

trong đó e(wᵢ) biểu thị giá trị chuẩn hóa của entropy thông tin của từ thứ i trong câu w và λ(t) kiểm soát tác động của tính thông tin ở bước thời gian t. Để đảm bảo rằng các biến tiềm ẩn xt giữ lại tất cả thông tin ở quá trình bắt đầu (t = 0) và không có thông tin ở quá trình kết thúc (t = T), lịch trình nhiễu λ(t) được thiết kế để có dạng sin, thỏa mãn λ(0) = λ(T) = 0, theo (He et al., 2022). H̄(w) biểu thị entropy trung bình của câu w và max(H(wⱼ)) biểu thị entropy tối đa trong câu w.

Hình 3 cho thấy cách ᾱt tiến triển trong quá trình tiến. Ví dụ, xem xét câu "The bus passes by the school", từ "school" mang nội dung thông tin cao hơn. Do đó, chúng tôi khuyến khích che giấu nó ở giai đoạn sau, cho phép mô hình của chúng tôi học cách khôi phục nó ở vị trí đầu.

Đáng chú ý rằng lịch trình nhiễu như vậy không thay đổi mục tiêu huấn luyện vì nó không sửa đổi hàm xác suất có điều kiện q(xt−1|xt, x0) trong Phương trình 4.

3.2 Tự điều kiện

Trong quá trình lấy mẫu của các mô hình khuếch tán cổ điển, tại mỗi bước thời gian t, mô hình tạo ra dự đoán hiện tại x̃ₜ⁰(xt, t, θ) thông qua một mạng khử nhiễu fθ(xt, t). Tuy nhiên, mạng khử nhiễu này chỉ dựa vào xt được cập nhật từ bước thời gian trước đó và loại bỏ kết quả ước tính x̃ₜ₊₁⁰, có nghĩa là không có kết nối giữa các kết quả dự đoán của các bước lấy mẫu liền kề.

Trong quá trình tạo văn bản của mô hình khuếch tán văn bản, điều này ngụ ý rằng thông tin ngữ nghĩa giữa các kết quả được tạo ra của các bước thời gian liền kề không nhất quán và không mạch lạc, tất yếu dẫn đến chất lượng văn bản kém và hiệu quả suy luận thấp.

Để giải quyết vấn đề về sự thiếu mạch lạc ngữ nghĩa được đề cập ở trên, được truyền cảm hứng từ (Chen et al., 2022), chúng tôi sử dụng tự điều kiện. Như được thể hiện trong Hình 4, kỹ thuật này xem xét các hàm khử nhiễu khác nhau x̃ₜ⁰(xt, x̃ₜ₊₁⁰, t, θ), sử dụng các mẫu được ước tính trước đó làm đầu vào phụ trợ. Tự điều kiện tinh chỉnh hàm khử nhiễu dựa trên các ước tính trước đó thay vì bắt đầu từ đầu với các ước tính mới. Bằng cách này, các kết nối và phụ thuộc trực tiếp được thiết lập giữa kết quả tạo ra của các bước thời gian liền kề, đạt được tính nhất quán và mạch lạc về ngữ nghĩa. Để huấn luyện mô hình hiệu quả hơn, chúng tôi áp dụng cùng một chiến lược huấn luyện như Analog-Bit (Chen et al., 2022): với xác suất 50%, chúng tôi huấn luyện x̃ₜ⁰(xt, x̃ₜ₊₁⁰, t, θ) bằng cách đặt đầu vào x̃ₜ₊₁⁰ thành 0, điều này đưa nó trở lại mô hình không có tự điều kiện. Ngược lại, trước tiên chúng tôi ước tính x̃ₜ⁰ bằng x̃ₜ⁰(xt, 0, t, θ) và sau đó sử dụng nó để huấn luyện tự điều kiện. Trong trường hợp thứ hai, chúng tôi không lan truyền ngược qua x̃ₜ⁰ được ước tính đầu tiên. Do đó, việc tăng thời gian huấn luyện bổ sung ít hơn 25%.

3.3 Nhiễu một phần và khử nhiễu có điều kiện

Trong nhiệm vụ chuỗi-đến-chuỗi cổ điển, cho một văn bản nguồn s = {w₁ˢ, w₂ˢ, ..., wₙˢ} với n token, nó tạo ra chuỗi văn bản đích y = {w₁ʸ, w₂ʸ, ..., wₙʸ}. Một mô hình tạo chuỗi có thể đạt được điều này bằng cách mô hình hóa xác suất có điều kiện: p(y|s). Để thực hiện các nhiệm vụ tạo văn bản chuỗi-đến-chuỗi, chúng tôi sử dụng Nhiễu một phần và Khử nhiễu có điều kiện (Gong et al., 2022). Kỹ thuật này chỉ thêm nhiễu vào văn bản đích y trong quá trình tiến và chỉ áp dụng khử nhiễu cho y trong quá trình khử nhiễu.

Cụ thể, cho một cặp văn bản: văn bản nguồn wˢ và văn bản đích wʸ, trước tiên chúng tôi thực hiện nhúng từ và nối cặp văn bản như EMB(wˢ‖wʸ). Sau đó, chúng tôi thu được trạng thái ban đầu x₀ của quá trình tiến thông qua qφ(x₀|wˢ‖wʸ) = N(EMB(wˢ‖wʸ), β₀I). Để đơn giản hóa ký hiệu, chúng tôi sử dụng sₜ và yₜ để biểu thị các phần của xt thuộc về wˢ và wʸ tại bước thời gian khuếch tán t, theo (Gong et al., 2022). Trong quá trình tiến, chúng tôi chỉ thêm nhiễu vào yₜ trong khi giữ sₜ không thay đổi. Trong quá trình khử nhiễu ngược, sₜ vẫn được giữ không thay đổi và được coi như điều kiện khử nhiễu, kiểm soát và hướng dẫn mô hình tạo ra văn bản mong muốn yₜ từ nhiễu. Mục tiêu huấn luyện tại thời điểm này có thể được đơn giản hóa như (Gong et al., 2022):

Lsimple = ∑ₜ₌₂ᵀ[∥y₀ − fθ(xt, t)∥²] + ∥EMB(wʸ) − fθ(x₁, 1)∥² − log pθ(wˢ‖wʸ|x₀)  (11)

trong đó fθ là mạng khử nhiễu.

4 Thiết lập thực nghiệm

4.1 Nhiệm vụ và bộ dữ liệu

Theo (Gong et al., 2022), chúng tôi tiến hành thí nghiệm trên bốn nhiệm vụ điển hình và phổ biến: Đối thoại miền mở, Tạo câu hỏi, Đơn giản hóa văn bản và Paraphrase. Đối thoại miền mở đòi hỏi các mô hình tạo ra các phản hồi có thông tin và có ý nghĩa khi có ngữ cảnh đối thoại. Chúng tôi sử dụng bộ dữ liệu Commonsense Conversation Dataset được sử dụng rộng rãi (Zhou et al., 2018), với hơn 3 triệu cặp đối thoại bao gồm nhiều chủ đề hàng ngày. Tạo câu hỏi nhằm tạo ra các câu hỏi có thể được trả lời bằng nội dung đã cho. Chúng tôi sử dụng bộ dữ liệu Quasar-T (Dhingra et al., 2017), được xử lý bởi (Gong et al., 2022), chứa 119K mẫu huấn luyện của các cặp tài liệu-câu hỏi. Đơn giản hóa văn bản nhằm sửa đổi văn bản phức tạp thành các chuỗi đơn giản hóa bằng cách đơn giản hóa ngữ pháp và lựa chọn từ. Chúng tôi sử dụng kho dữ liệu được xây dựng bởi (Jiang et al., 2020) bao gồm 666K câu phức tạp-đơn giản. Paraphrase liên quan đến việc viết lại câu với cùng ý nghĩa ngữ nghĩa nhưng có dạng bề mặt khác. Chúng tôi áp dụng Quora Question Pairs (QQP) được sử dụng rộng rãi, có nguồn gốc từ nền tảng câu hỏi-đáp cộng đồng Quora, bao gồm 147K cặp tích cực.

4.2 Phương pháp cơ sở

Theo (Gong et al., 2022), chúng tôi so sánh InfoDiffusion với bốn nhóm cơ sở:
• Mô hình tự hồi quy encoder-decoder. Chúng tôi chọn hai mô hình phổ biến: GRU (Chung et al., 2014) với attention và Transformer (Vaswani et al., 2017).
• Mô hình ngôn ngữ được huấn luyện trước được tinh chỉnh lớn. Chúng tôi chọn GPT-2 (Radford et al., 2019) và GPVAE (Du et al., 2022). GPT-2 được huấn luyện với mô hình hóa ngôn ngữ và GPVAE bổ sung T5 (Raffel et al., 2020) với VAE.
• Mô hình phi tự hồi quy. chúng tôi xem xét LevT (Cortes et al., 2015), một mô hình NAR lặp mạnh được sử dụng rộng rãi. Nó áp dụng chèn và xóa để tạo ra và tinh chỉnh chuỗi một cách lặp đi lặp lại.
• Mô hình khuếch tán văn bản. Chúng tôi chọn DiffuSeq (Gong et al., 2022). Đây là một mô hình khuếch tán văn bản gần đây, và hiệu suất của các mô hình khuếch tán văn bản khác tương tự như nó.

Chúng tôi triển khai các mô hình này theo các bài báo gốc của chúng.

4.3 Thước đo đánh giá

Khi đánh giá các chuỗi được tạo ra, cả chất lượng và tính đa dạng đều đóng vai trò quan trọng. Để đánh giá chất lượng, chúng tôi sử dụng BLEU (Papineni et al., 2002) và ROUGE (Lin, 2004) làm thước đo tiêu chuẩn, đo lường các n-gram chồng lấp giữa văn bản được tạo ra và văn bản vàng. Tuy nhiên, vì việc khớp chuỗi đơn thuần có thể không đủ cho việc tạo ra mở, chúng tôi cũng sử dụng BERTScore (Zhang et al., 2020) để đánh giá độ tương tự ngữ nghĩa ở mức độ nhúng. Điểm số cao hơn trong BLEU, ROUGE và BERTScore cho thấy hiệu suất vượt trội trong tạo văn bản. Về tính đa dạng, chúng tôi xem xét đánh giá các n-gram khác biệt bằng cách sử dụng Distinct (Li et al., 2016) và tỷ lệ các n-gram khác biệt so với tổng số từ bằng cách sử dụng Diverse (Deshpande et al., 2019). Hơn nữa, chúng tôi tích hợp self-BLEU (Zhu et al., 2018), một thước đo cấp độ câu đánh giá các n-gram chồng lấp giữa các văn bản được tạo ra. Điểm self-BLEU thấp hơn và giá trị diverse-4 cao hơn cho thấy mức độ đa dạng lớn hơn trong các đầu ra được tạo ra. Theo (Gong et al., 2022), chúng tôi tạo ra ba mẫu cho mỗi điều kiện văn bản để tính toán thước đo đa dạng cho từng phương pháp.

4.4 Chi tiết triển khai

InfoDiffusion được xây dựng trên 12 lớp Transformer với 12 đầu attention và có khoảng 91M tham số. Độ dài chuỗi tối đa được đặt thành 128, với chiều nhúng d = 128. Chúng tôi thực hiện T = 2.000 bước khuếch tán. Để giải quyết việc tạo ra ngoài từ vựng, chúng tôi sử dụng Byte Pair Encoding (Sennrich et al., 2016) để xây dựng từ vựng. Thước đo độ chính xác của InfoDiffusion được đánh giá bằng MBR (Minimum Bayes Risk) với kích thước mẫu ứng viên |S| = 10. Thí nghiệm được triển khai trên NVIDIA RTX 3090 Tensor Core GPU, và chúng tôi sử dụng 4 GPU để huấn luyện và GPU đơn để lấy mẫu.

5 Kết quả và phân tích

5.1 Đánh giá tạo văn bản

Như được thể hiện trong Bảng 1, chúng tôi kết luận rằng InfoDiffusion đạt được chất lượng tạo ra tương đương hoặc thậm chí cao hơn so với các cơ sở mạnh.

Đầu tiên, so với các mô hình tự hồi quy encoder-decoder và các mô hình Phi tự hồi quy, InfoDiffusion thể hiện lợi thế tuyệt đối về chất lượng và tính đa dạng. Ví dụ, trong các nhiệm vụ tạo câu hỏi, thước đo chất lượng BLEU đã cải thiện hơn ba lần, trong khi distinct tăng +0,12. Sự cải thiện trong thước đo đa dạng cũng đáng kể tương tự. Ví dụ, giá trị của diverse-4 tăng từ 0,64 lên 0,98, thể hiện sự cải thiện hơn 50%.

Thứ hai, so với các mô hình được huấn luyện trước như GPT2, InfoDiffusion vượt trội hơn biến thể cơ sở và thực hiện tương đương với biến thể lớn, có gấp 8 lần tham số hơn InfoDiffusion. Về tính đa dạng, InfoDiffusion dẫn đầu trong bảy trong số mười hai kịch bản so sánh, cho thấy lợi thế nhẹ so với các mô hình được huấn luyện trước trong việc tạo ra văn bản đa dạng.

Cuối cùng, so với mô hình khuếch tán hoạt động tốt DiffuSeq, InfoDiffusion chứng minh chất lượng tạo văn bản vượt trội trên tất cả các bộ dữ liệu. Tất cả thước đo chất lượng đều cho thấy sự cải thiện từ +0,01 đến +0,03. Mặt khác, mặc dù điểm self-BLEU tụt hậu so với DiffuSeq trong các nhiệm vụ đơn giản hóa văn bản, có sự cải thiện nhẹ về tính đa dạng văn bản trên các bộ dữ liệu còn lại.

5.2 So sánh hiệu quả suy luận

Một trong những mối quan tâm chính của các mô hình khuếch tán là hiệu quả của Suy luận. Chúng tôi so sánh InfoDiffusion của chúng tôi với DiffuSeq về hiệu quả suy luận. Chúng tôi tiến hành thí nghiệm trên Đơn giản hóa văn bản và đặt kích thước lô suy luận thành 50 và bước thời gian khuếch tán thành 2000 cho cả hai mô hình. Các đường cong chất lượng (tức là BLEU) và tính đa dạng (tức là div-4) trong quá trình tạo ra của mô hình được thể hiện trong Hình 5. Chất lượng và tính đa dạng của văn bản được tạo ra bởi DiffuSeq dần dần cải thiện trong các giai đoạn sau của việc lấy mẫu (Xu hướng giảm trong thước đo đa dạng là do quá trình lấy mẫu dần dần tạo ra văn bản đích từ nhiễu và nhiễu có mức độ đa dạng cao). Nhưng InfoDiffusion thể hiện hành vi ngược lại, tạo ra văn bản chất lượng cao trong các giai đoạn đầu và giữa của việc lấy mẫu. Khoảng nửa chừng quá trình lấy mẫu, chất lượng của văn bản được tạo ra bởi InfoDiffusion vượt qua kết quả cuối cùng của DiffuSeq. Điều này cho thấy rằng InfoDiffusion có thể hội tụ đến câu đích nhanh hơn và rút ngắn thời gian lấy mẫu một nửa so với DiffuSeq trong khi duy trì hiệu suất tạo ra gần như tương tự.

5.3 Phân tích loại bỏ

Để chứng minh hiệu quả của các kỹ thuật được đề xuất trong InfoDiffusion, chúng tôi đã tiến hành các nghiên cứu loại bỏ trên bộ dữ liệu QQP. Như được thể hiện trong Bảng 2, khi chúng tôi loại bỏ tự điều kiện, điểm BLEU giảm 0,0126, trong khi Dist-1 gần như không thay đổi. Hơn nữa, khi chúng tôi loại bỏ thêm lịch trình nhiễu được đề xuất khỏi InfoDiffusion và sử dụng lịch trình sqrt được đề xuất trong DiffusionLM (Li et al., 2022) thay thế, điểm BLEU giảm 0,0051 và Dist-1 giảm 0,0018. Điều này cho thấy rằng lịch trình nhiễu được đề xuất và tự điều kiện góp phần cải thiện chất lượng văn bản được tạo ra, trong khi tác động của tự điều kiện đến tính đa dạng của văn bản được tạo ra là tối thiểu.

5.4 Nghiên cứu trường hợp

Chúng tôi chọn một trường hợp minh họa và điều tra quá trình tạo ra của InfoDiffusion. Có thêm các trường hợp trong Phụ lục C. Như được thể hiện trong Bảng 3, quá trình tạo ra tiết lộ rằng mô hình InfoDiffusion tuân theo thứ tự tạo ra "thông tin chính trước": nó ưu tiên tạo ra các danh từ có nội dung thông tin cao hơn, chẳng hạn như "i" và "geologist", và sau đó tạo ra tuần tự các từ có nội dung thông tin thấp hơn, chẳng hạn như "can", "how", "become", và "good" để bổ sung cho câu.

Để minh họa rõ hơn sự ưu tiên của mô hình trong việc tạo ra thông tin chính, chúng tôi đã chọn bốn loại từ thường có thông tin chính hoặc nội dung thông tin cao hơn: danh từ, động từ, trạng từ và tính từ (Clark và Weir, 2002; Emelianenko et al., 2019). Chúng tôi so sánh thứ tự giải mã của những từ này trong InfoDiffusion và DiffuSeq trong quá trình giải mã. Như được thể hiện trong Hình 6, rõ ràng là InfoDiffusion giải mã những từ có thông tin cao này sớm hơn nhiều so với DiffuSeq.

6 Kết luận

Bài báo này, chúng tôi đề xuất InfoDiffusion, một mô hình khuếch tán văn bản phi tự hồi quy mới lạ. Bằng cách thiết kế Lịch trình nhiễu nhận thức entropy thông tin, chúng tôi cho phép mô hình khuếch tán tuân theo quá trình tạo văn bản "thông tin chính trước" phù hợp hơn với việc tạo văn bản của con người, từ đó đạt được hiệu quả và hiệu suất cải thiện trong việc tạo văn bản. Kết quả thực nghiệm trên bốn bộ dữ liệu chuẩn xác nhận hiệu quả của InfoDiffusion. Nghiên cứu này là nghiên cứu đầu tiên về thứ tự giải mã của các mô hình khuếch tán và nỗ lực đầu tiên để thay đổi thứ tự giải mã của các mô hình khuếch tán văn bản. Công việc tương lai có thể khám phá việc sử dụng lịch trình nhiễu được đề xuất để thay thế nhiễu hiện có trong các nhiệm vụ liên quan dựa trên các mô hình khuếch tán, để nâng cao hiệu suất của mô hình hơn nữa.

Hạn chế

Mặc dù có hiệu suất mạnh mẽ của InfoDiffusion, nó vẫn có những hạn chế sau. Đầu tiên, do sự ưu tiên mạnh mẽ của ngôn ngữ đối với các từ đơn giản, các từ đơn giản vẫn có thể xuất hiện sớm trong quá trình giải mã. Thứ hai, đánh giá của chúng tôi chỉ dựa vào các thước đo tự động như BLEU, mà không đánh giá các vấn đề tiềm ẩn như ảo giác trong văn bản được tạo ra. Công việc tương lai có thể sử dụng cả thước đo tự động và đánh giá của con người để đánh giá toàn diện chất lượng văn bản trên các chiều bao gồm ngữ pháp, ngữ nghĩa và nhiều hơn nữa. Cách tiếp cận đa diện này sẽ tạo điều kiện cho việc tạo ra văn bản chân thực, hợp lý và đáng tin cậy.

Tuyên bố đạo đức

Nghiên cứu được trình bày trong bài báo này về mô hình khuếch tán văn bản tuân thủ các hướng dẫn và nguyên tắc đạo đức. Chúng tôi đã ưu tiên quyền riêng tư, giảm thiểu thiên vị, đảm bảo tính minh bạch và thúc đẩy sử dụng có trách nhiệm. Cam kết của chúng tôi đối với trách nhiệm giải trình, quản trị có trách nhiệm và đánh giá đạo đức liên tục nhấn mạnh sự cống hiến của chúng tôi trong việc duy trì các tiêu chuẩn toàn vẹn cao nhất trong việc phát triển và triển khai mô hình khuếch tán văn bản.

Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62106105), Quỹ Mở CCF-Baidu (Số CCF-Baidu202307), Quỹ Nghiên cứu Mở CCF-Tencent (Số RAGR20220122), Quỹ Mô hình Lớn CCF-Zhipu AI (Số CCF-Zhipu202315), Quỹ Nghiên cứu Cơ bản cho các Trường Đại học Trung ương (Số NJ2023032), Quỹ Khởi nghiệp Nghiên cứu Khoa học của Đại học Hàng không và Vũ trụ Nam Kinh (Số YQR21022), và Nền tảng Điện toán Hiệu suất Cao của Đại học Hàng không và Vũ trụ Nam Kinh.
