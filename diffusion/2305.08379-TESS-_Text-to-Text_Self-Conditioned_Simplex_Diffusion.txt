# 2305.08379.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2305.08379.pdf
# File size: 852593 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
TESS: Text-to-Text Self-Conditioned Simplex Diffusion
Rabeeh Karimi Mahabadi1,4∗Hamish Ivison3,5∗†Jaesung Tae2
James Henderson4Iz Beltagy3Matthew E. Peters3†‡Arman Cohan2,3‡
1EPFL2Yale University3Allen Institute for AI
4Idiap Research Institute5University of Washington
rabeeh.karimimahabadi@epfl.ch ,hamishi@allenai.org
Abstract
Diffusion models have emerged as a power-
ful paradigm for generation, obtaining strong
performance in various continuous domains.
However, applying continuous diffusion mod-
els to natural language remains challenging
due to its discrete nature and the need for a
large number of diffusion steps to generate
text, making diffusion-based generation expen-
sive. In this work, we propose Text-to-text Self-
conditioned Simplex Diffusion ( TESS ), a text
diffusion model that is fully non-autoregressive,
employs a new form of self-conditioning, and
applies the diffusion process on the logit sim-
plex space rather than the learned embedding
space. Through extensive experiments on nat-
ural language understanding and generation
tasks including summarization, text simplifica-
tion, paraphrase generation, and question gener-
ation, we demonstrate that TESS outperforms
state-of-the-art non-autoregressive models, re-
quires fewer diffusion steps with minimal drop
in performance, and is competitive with pre-
trained autoregressive sequence-to-sequence
models. We publicly release our codebase.1
1 Introduction
Diffusion models (Sohl-Dickstein et al., 2015; Ho
et al., 2020; Song et al., 2021) have achieved
state-of-the-art performance in various continuous
domains, such as image (Nichol and Dhariwal,
2021), audio (Kong et al., 2020; Shen et al., 2023),
video (Ho et al., 2022), and text-to-image gener-
ation (Saharia et al., 2022; Ramesh et al., 2022).
Inspired by the success of diffusion for continuous
domains, recent works have adapted diffusion to
discrete spaces, such as text (Austin et al., 2021;
Hoogeboom et al., 2021; Savinov et al., 2021; Reid
et al., 2022). One line of work proposes diffusing
∗Co-first authors.
†Work done during employment at AI2.
‡Equal advising.
1https://github.com/allenai/tess-diffusionthe model latent space by adding Gaussian noise to
input word embeddings (Li et al., 2022b). Another
approach, SSD-LM (Han et al., 2022), adds noise
to the vocabulary probability simplex.
Direct diffusion on the probability simplex is
desirable (Richemond et al., 2022) as it eliminates
the need for an extra step to map diffused embed-
dings to actual discrete inputs or auxiliary methods
such as binary encoding (Chen et al., 2022). De-
spite its strong performance, however, SSD-LM has
several shortcomings: a lack of self-conditioning
(Chen et al., 2022), a lack of extensive evaluation
on downstream tasks, and most notably, its restric-
tion to generating blocks of 25 tokens, which hin-
ders the potential benefits of full diffusion, e.g.,
the ability to perform arbitrary infilling, flexible
generation, and a global view of the sequence.
In this work, we present TESS , a text-to-text
diffusion model, which overcomes several limita-
tions of prior works: restrictions on scale (Hooge-
boom et al., 2021; Austin et al., 2021), depen-
dence on pretrained embeddings (Strudel et al.,
2022), semi-autoregressive nature (Han et al.,
2022), and short generation length (Gong et al.,
2023). TESS closely follows Han et al. (2022,
2023a) by performing diffusion on the vocabu-
lary logit space rather than the typical embedding
space. Unlike SSD-LM, however, TESS is fully
non-autoregressive and performs diffusion on the
entire sequence. It also incorporates a novel form
of self-conditioning, which demonstrates a com-
petitive edge over the original self-conditioning
method (Chen et al., 2022) and dramatically im-
proves the efficiency and quality of TESS.
We evaluate TESS on a suite of natural lan-
guage generation (NLG) tasks including summa-
rization, text simplification, paraphrase generation,
and question generation. Our empirical results sur-
pass the current state-of-the-art non-autoregressive
and diffusion-based approaches and are on par
with a strong pretrained encoder-decoder languagearXiv:2305.08379v2  [cs.CL]  21 Feb 2024

--- PAGE 2 ---
model (Lewis et al., 2020). In particular, our
simplex-based self-conditioning method substan-
tially improves generation quality. We also evaluate
TESS on natural language understanding (NLU)
tasks from the GLUE benchmark (Wang et al.,
2019) and show that it performs comparably to
strong masked language model baselines. Our con-
tributions can be summarized as follows.
1.We demonstrate the effectiveness of a fully non-
autoregressive scheme for text diffusion models,
which outperforms strong autoregressive and non-
autoregressive baselines.
2.We propose a new self-conditioning method
that exploits the simplex semantics of the diffusion
space and greatly improves performance.
3.We evaluate TESS on a suite of diverse NLG
and NLU tasks, highlighting the effectiveness of
our text-to-text simplex diffusion paradigm.
4.We show TESS ’ fully non-autoregressive ap-
proach results in faster and more efficient sampling
than semi and fully autoregressive methods for long
sequences.
We will release our trained models and code to
promote open research in the field of diffusion-
based text generation.
2 Background
We revisit continuous diffusion models (Sohl-
Dickstein et al., 2015), following the formulation
of Denoising Diffusion Models (Ho et al., 2020;
Song et al., 2020).
Training Given a sample x0∈Rdfrom a
data distribution pdata, a forward diffusion pro-
cessq(xt|xt−1)is a Markov chain that generates a
sequence of latent variables x1, . . . ,xTby grad-
ually adding Gaussian noise at each time step
t∈ {1,2, . . . , T }with variance βt∈R>0:
q(xt|xt−1) =N(xt;p
1−βtxt−1, βtI).(1)
Letϵt∼ N(0,I),αt= 1−βt, and ¯αt=Qt
s=1αs.
Then sampling xtat an arbitrary time step thas the
closed-form solution
xt=√¯αtx0+√
1−¯αtϵt. (2)
Given a well-behaved noise schedule {βt}T
t=1,
xTfollows a stationary prior distribution N(0,I).
Therefore, if we can approximate the reverse pro-
cessq(xt−1|xt,x0)via a model pθ(xt−1|xt)withparameters θ, then we can sample random noise
from a standard Gaussian and gradually denoise
it to sample from pdata. In our settings, our model
pθis a transformer model2. The reverse process is
thus parametrized as
pθ(xt−1|xt) =N(µθ(xt, t),Σθ(xt, t)).(3)
The model is trained by minimizing the mean
squared error between the ground-truth data x0
and its estimate ˆxθ:3
L=Et,q(x0),q(xt|x0)∥x0−ˆxθ(xt, t)∥2.(4)
Noise schedule The forward diffusion process
is defined by a noise schedule. In this work, we
follow the cosine schedule (Nichol and Dhariwal,
2021) for αt:
¯αt=f(t)
f(0), f(t) = cost/T+s
1 +s.π
22
.(5)
Inference In Song et al. (2020), model predic-
tions are iteratively denoised for t=T, . . . , 1start-
ing from pure noise, following
xt−1=√αt−1ˆxθ+p
1−αt−1·xt−√αtˆxθ√1−αt.
We follow the recently proposed simplex-based
diffusion procedure by Han et al. (2022), which
allows us to apply diffusion to text without employ-
ing auxiliary methods that map categorical data to
continuous space (Richemond et al., 2022).
3 Method
In this section, we present TESS , a simplex
diffusion-based text-to-text model. Building upon
SSD-LM (Han et al., 2022), we propose a fully
non-autoregressive model with self-conditioning.
Continuous data representation LetVdenote
the vocabulary space. Following Han et al. (2022),
we map the ID of each token to be generated w∈ V
to ak-logit simplex to produce sw∈ {± k}|V|,
whose i-th component satisfies
sw
(i)=(
k, ifi=w,
−k,otherwise ,(6)
2Specifically, we use a RoBERTa model (Liu et al., 2019),
but our formulation could be applied to any transformer vari-
ant.
3Alternatively, we can train the model to predict the added
noise; see Ho et al. (2020). See also Song et al. (2021) for a
score-matching interpretation.

--- PAGE 3 ---
TESS“cola sentence: the well is eating.”“summarize: a new, innovative technology promises to revolutionize the renewable energy industry.”“tech to transform clean energy.”“not acceptable”……
<noisy input tokens>simplexweighted averagetransformer<logits>Figure 1: Overview of TESS . During training (top), we first add noise to the vocabulary probability simplex,
compute a weighted average word embedding, and denoise it using a transformer encoder. To generate from our
model, we begin with noise and iteratively refine it into a final logit distribution (middle). The resulting model can
be used for a wide range of NLG and NLU end tasks (bottom).
with a hyperparameter k∈R. We then produce a
probability simplex over Vviapw=softmax (sw).
Finally, we compute the weighted sum of word em-
beddings to obtain a continuous embedding vector,
hw=Epw, where E∈Rd×|V|is the word em-
bedding matrix, ddenotes the size of the hidden
dimension, and hw∈Rd.
Time step embeddings After computing the con-
tinuous word embeddings, we add the time step
embeddings to inform the model of the current time
step. Our time step embedding is a linear layer, and
we feed scaled time steps t/T to this layer. The
output is a time step embedding in Rdthat is added
tohwto produce the final latent input vector.
Text-to-text non-autoregressive modeling Un-
like SSD-LM, which feeds small blocks of text to
semi-autoregressively generate sequences of text,
we feed the entire latent vector along with the con-
text into an encoder transformer model. This is a
key difference between our approach and SSD-LM,
as it allows for a fully non-autoregressive model
capable of generating sequences of any length. In
practice, our evaluation tasks often require output
sequences of 100 tokens or more, and by moving to
a fully non-autoregressive paradigm, we are able to
generate entire output sequences in parallel without
resorting to semi-autoregressive generation.
Forward diffusion Letw= (w1, . . . , w L)be
a sentence of Ltokens such that wi∈ V , and
S0= (sw1, . . . ,swL)∈ {± k}L×|V|be the k-logit
simplex representation of w. We add noise to thek-logit simplex representation during training ac-
cording to
St=√¯αtS0+√
1−¯αtϵt, (7)
where subscript denotes the time step and ϵt∼
N(0, k2I).
Training Typical diffusion models are trained
with mean squared error loss as in Equation (4)
to predict the ground-truth data. This objective
is known to be unstable for text diffusion mod-
els (Dieleman et al., 2022). Strudel et al. (2022)
froze word embeddings and used specific scaling
to deal with training instability. In this work, fol-
lowing Han et al. (2022), we instead compute the
usual cross-entropy loss between the ground-truth
tokens wand the model prediction given a noisy
logit simplex Stat time step t.
L=Et,q(S0),q(St|S0)"
−LX
i=1logpθ(wi|St, t)#
.
(8)
Sampling During inference, we sample STfrom
the prior N(0, k2I)and run the reverse process for
t=T, . . . , 1on the noisy k-logit simplex. The
reverse process can be approximated via
St−1=√¯αt−1ˆSθ(St, t) +p
1−¯αt−1ϵt.(9)
See Appendix C for details. This resembles the for-
ward process in Equation (7), which allows for an
intuitive interpretation: to reverse one step from t,

--- PAGE 4 ---
we take the model prediction ˆSθas the hypothetical
ground-truth, then corrupt it by (t−1)time steps.
To construct the model prediction, we project the
logits predicted by the underlying encoder model
via argmax as a pseudo-inverse of Equation (6)to
match the initial k-logit representation:
ˆsw
(i)=(
k, ifi=argmax (sw),
−k,otherwise .(10)
Self-conditioning In typical diffusion models,
the model predicts the original data x0conditioned
on its corrupted version, i.e., ˆxt
0=ˆxθ(xt, t),
where ˆxt
0denotes the estimate of x0at time step
t. In this setting, the model’s estimates at previ-
ous time steps are discarded. However, in self-
conditioning (Chen et al., 2022), the model condi-
tions its prediction on both xtand its previously
generated output, i.e., ˆxt
0=ˆxθ(xt,ˆxt+1
0, t). To
adapt the model for self-conditioning, we stochas-
tically zero out the self-condition such that
ˆxt
0=(
ˆxθ(xt,ˆxt+1
0, t),with probability ρ
ˆxθ(xt,0, t), otherwise ,
(11)
where the self-conditioning previous prediction is
computed as ˆxt+1
0=ˆxθ(xt+1,0, t+ 1), with gra-
dients detached. We set ρ= 0.5during training;
during inference, we always use self-conditioning
(ρ= 1).
We propose a new self-conditioning method that
exploits the simplex nature of our diffusion space.
Letst∈R|V|be a noised k-logit simplex for an
arbitrary token w.4Instead of concatenating the
previous prediction with stand re-projecting, we
first compute the average of simplex probabilities
pw
avg=1
2 
softmax (st) +softmax (ˆst+1
0)
.(12)
Note that pw
avgis a well-defined categorical distri-
bution over V. We then compute a continuous em-
bedding vector, hw=Epw
avg, and use this vector
as input to our underlying model to make a predic-
tion for the given diffusion step following Equa-
tion 9. This is more efficient than the original self-
conditioning method, which projects down the con-
catenated vectors. In Section §6.2, we also demon-
strate the empirical effectiveness of this method
over the original.
4We write sw
tasstfor brevity.Variable sequence length A notable challenge
in non-autoregressive generation is the assump-
tion of fixed sequence lengths during inference.
To overcome this issue, we follow prior work in
embedding-space diffusion by using padding to-
kens (Li et al., 2022b). Specifically, during training,
we always pad the variable-length output sequence
to a fixed length using padding tokens. These
padding tokens are included when computing the
cross-entropy loss so that TESS learns to generate
them. During inference, we specify the maximum
sequence length and run sampling as usual.
4 Experiments
4.1 Tasks and Datasets
Paraphrase generation This task involves
rephrasing a sentence while maintaining the se-
mantics of the original. We use Quota Question
Pairs (QQP),5which is composed of 147K positive
pairs. We use only the positively-labelled pairs,
which have the same meaning.
Text simplification This task involves simplify-
ing complex sentences while retaining their orig-
inal meaning. We use the NEWSELA-AUTO
dataset (Jiang et al., 2020), which is composed
of 666K complex-simplified sentences.
Question generation This task involves gener-
ating a question given an input context. We use
the QUASAR-T dataset (Dhingra et al., 2017) pro-
cessed by Yuan et al. (2022), resulting in 119K
document-question pairs.
Summarization We evaluate our method on the
CNN-DailyMail dataset (Hermann et al., 2015),
which comprises 300K articles and summaries.
Classification We consider a set of classification
tasks in the GLUE benchmark (Wang et al., 2019)
covering a variety of tasks, including paraphrase
detection (MRPC, QQP), sentiment classification
(SST-2), natural language inference (MNLI,6RTE,
QNLI), and linguistic acceptability (CoLA).7
4.2 Baselines
We compare TESS to several autoregressive base-
lines as well as state-of-the-art text diffusion mod-
5https://www.kaggle.com/c/
quora-question-pairs
6We report the accuracy on the matched validation set.
7Following Devlin et al. (2019); Raffel et al. (2020), as a
common practice and due to the adversarial nature of WNLI,
we do not experiment with WNLI.

--- PAGE 5 ---
Model Paraphrase Generation
BLEU BERT R-L D-1/4
Autoregressive Models
BART (Lewis et al., 2020) 30.4 85.7 61.4 98.8 /61.0
GPT-2 base†(Radford et al., 2019) 19.8 82.5 52.1 98.0/62.5
GPT-2 large†(Radford et al., 2019) 20.6 83.6 54.2 98.2/50.2
GPV AE-T5†(Du et al., 2022) 24.1 84.7 58.9 96.9/61.7
Non-Autoregressive Models
LevT†(Gu et al., 2019) 22.7 83.4 57.9 97.9/33.3
Non-Autoregressive Diffusion Models
DiffuSeq⋆(Gong et al., 2023) 18.5 79.5 — 97.6/—
SeqDiffuSeq⋆(Yuan et al., 2022) 23.3 82.9 — 98.1/—
SSD-LM (Han et al., 2022) 22.9 83.8 58.3 98.8/57.3
TESS (Ours) 30.2 85.7 62.2 98.5/ 61.1
Table 1: Results on Paraphrase Generation task.†indi-
cates results from from Gong et al. (2023), * indicates
results from Yuan et al. (2022). Boldfaced results show
the best across all non-AR models; underlined results
are the best across all models.
els. For autoregressive methods, we consider GPT-
2 (Radford et al., 2019), BART (Lewis et al.,
2020), and GPV AE-T5 (Du et al., 2022), a latent-
structured variable model and an extension to T5
(Raffel et al., 2020). For text diffusion mod-
els, we consider Diffuser (Reid et al., 2022), Dif-
fuSeq (Gong et al., 2023), SeqDiffuSeq (Yuan
et al., 2022), SUNDAE (Savinov et al., 2021),
LevT (Gu et al., 2019), a widely used iterative
non-autoregressive model, and SSD-LM (Han
et al., 2022) initialized from the same pretrained
RoBERTa model as TESS and trained using the
official SSD-LM codebase.8We report results with-
out using additional decoding methods such as min-
imum Bayes risk decoding. We provide further
details on baseline results in Appendix A.
4.3 Evaluation
For summarization, we report ROUGE-1 (R1),
ROUGE-2 (R2), and ROUGE-L (R-L) vari-
ants (Lin, 2004) as done in prior text summarization
work (Lewis et al., 2020). We quantify both genera-
tion quality and diversity. For evaluating generation
quality, we report BLEU (Papineni et al., 2002),
ROUGE-L (Lin, 2004) and BERTScore (Zhang
et al., 2020) following Gong et al. (2023) and Yuan
et al. (2022). For evaluating diversity, we report
distant unigrams (D-1) and diverse 4-grams (D-4)
(Deshpande et al., 2018). For text simplification,
we use the standard SARI (Xu et al., 2016), and fol-
8https://github.com/xhan77/ssd-lmModel Text Simplification
SARI BLEU BERT R-L
Autoregressive Models
BART (Lewis et al., 2020) 49.9 41.4 81.7 58.1
GPT-2 base†(Radford et al., 2019) — 30.8 80.2 54.6
GPT-2 large†(Radford et al., 2019) — 26.9 78.8 51.1
GPV AE-T5†(Du et al., 2022) — 33.9 81.7 58.3
Non-Autoregressive Models
LevT†(Gu et al., 2019) — 20.5 72.5 44.0
Non-Autoregressive Diffusion Models
DiffuSeq⋆(Gong et al., 2023) — 29.9 79.1 —
SeqDiffuSeq⋆(Yuan et al., 2022) — 37.1 82.1 —
SSD-LM (Han et al., 2022) 36.3 12.5 69.5 39.6
TESS (Ours) 54.3 41.5 82.1 59.4
Table 2: Results on the text simplification task.†indi-
cates results from from Gong et al. (2023), * indicates
results from Yuan et al. (2022).
lowing Gong et al. (2023); Yuan et al. (2022), we
also include BLEU, BERTScore, and ROUGE-L.
4.4 Implementation
We start from the RoBERTa pretrained checkpoint
(Liu et al., 2019) and finetune the model on down-
stream tasks using our proposed self-conditioned
simplex diffusion method. The number of diffusion
sampling steps at inference time is set to T= 1000
for generation and T= 10 for classification tasks.
During training, we use T= 5000 . We set the sim-
plex scale to k= 5. Additional details are listed in
Appendix A.
5 Results
5.1 Paraphrase Generation
As seen in Table 1, TESS significantly outperforms
GPT-2 and other non-autoregressive and diffusion
baselines in quality metrics (BLEU, BERT, and
ROUGE) while achieving parity in diversity met-
rics (D-1/D-4). Moreover, TESS obtains com-
petitive overall performance with BART. Note
that BART uses a denoising pretraining objective,
which is substantially conducive to sequence-to-
sequence tasks (Lewis et al., 2020); we do not per-
form any additional pretraining beyond RoBERTa’s
checkpoint, which was only pretrained on the gen-
eral masked language modeling objective. We sus-
pect that TESS could significantly benefit from
additional diffusion pretraining (see Section §8).

--- PAGE 6 ---
Model Question Generation
BLEU BERT R-L D-1/4
Autoregressive Models
BART (Lewis et al., 2020) 17.4 66.2 38.8 98.2 /61.7
GPT-2 base†(Radford et al., 2019) 7.4 60.5 27.2 96.0/92.2
GPT-2 large†(Radford et al., 2019) 11.1 63.5 32.2 96.7/80.6
GPV AE-T5†(Du et al., 2022) 12.5 63.1 33.9 93.8/72.8
Non-Autoregressive Models
LevT†(Gu et al., 2019) 9.3 54.9 28.9 89.1/47.8
Non-Autoregressive Diffusion Models
DiffuSeq⋆(Gong et al., 2023) 15.8 59.4 — 91.1/—
SeqDiffuSeq⋆(Yuan et al., 2022) 17.2 61.4 — 92.7/—
SSD-LM (Han et al., 2022) 14.1 62.8 38.5 94.5/56.9
TESS (random init) 19.0 60.8 36.1 96.1/62.4
TESS (Ours) 19.5 65.8 38.9 97.1/63.0
Table 3: Results on Question Generation task.
5.2 Text Simplification
Results of the text simplification task on the
NEWSELA-AUTO dataset are presented in Table 2.
TESS outperforms all baselines typically by large
margins, including both autoregressive and non-
autoregressive models.
5.3 Question Generation
As shown in Table 3, TESS outperforms other dif-
fusion and non-autoregressive models in terms of
both quality of generation (BLEU, BERTScore,
ROUGE) and diversity (D-1/D-4). It also con-
sistently outperforms other autoregressive base-
lines except for BART, whose performance is
closely matched by TESS . We also train and eval-
uate TESS without initializing from pretrained
RoBERTa (random init), and find that this outper-
forms all NAR baselines in BLEU and D-1, while
remaining close in performance in BERTScore and
ROUGE-L. This shows that the TESS framework
outperforms baselines even without the benefit of
making use of existing pretrained models.
5.4 Summarization
As shown in Table 4, TESS achieves competitive
results with BART while outperforming prior dif-
fusion work, Diffuser (Reid et al., 2022), by 1.9
ROUGE-L points, and its bootstrapped variants by
0.8 ROUGE-L points. Note that additional boot-
strapping is orthogonal to their method and can
be applied to TESS as well. Additionally, TESS
outperforms GENIE, another prior diffusion-based
method, while using half the number of diffusion
steps. This suggests TESS ’ simplex-based formu-
lation leads to better performance than alternateModelCNN-DM
R1 R2 R-L
Autoregressive Models
BART (Lewis et al., 2020) 42.9 20.1 40.1
Transformer (Vaswani et al., 2017)⋄— — 36.8
Non-Autoregressive Diffusion Models
SUNDAE (Savinov et al., 2021)⋄— — 37.0
Diffuser (Reid et al., 2022)⋄— — 37.8
Diffuser+ AR bootstrap⋄— — 38.4
Diffuser + source bootstrap⋄— — 38.9
GENIE (Lin et al., 2023) 41.8 18.3 35.5
TESS (Ours) 42.3 19.4 39.7
Table 4: Results on CNN-DailyMail dataset. Baseline
values marked with⋄are taken from Reid et al. (2022).
diffusion approaches.
5.5 Text Classification
To our knowledge, TESS is the first model that
is evaluated on both NLG and NLU. We evaluate
TESS on classification tasks and directly compare
our diffusion-based finetuning method with stan-
dard finetuning methods for supervised learning.
To perform a controlled experiment, we compare
TESS with a similar-sized RoBERTa, which we
use to initialize our model. Note that since TESS is
text-to-text, similar to T5 (Raffel et al., 2020), it can
naturally handle classification tasks by generating
class labels without the need for verbalizers. For
STS-B, which is a regression problem, we recast it
as a 21-class classification problem following Raf-
fel et al. (2020). Results are shown in Table 5. We
observe that TESS matches or outperforms fine-
tuned RoBERTa on several tasks, achieving roughly
2-point gains on MRPC and RTE.
6 Analysis
6.1 Variable Length Output
Figure 2 shows TESS is capable of producing out-
puts of variable lengths that match the underlying
distribution of sequence lengths in the gold data as
well as BART outputs. We also evaluate generation
quality for differing output lengths in Figure 3. We
observe that TESS is consistent with the BART
baseline for variable target lengths, with longer
generations matching BART’s performance.
6.2 Self-Conditioning
To examine the impact of self-conditioning, we
compare our proposed method with the original
strategy (Chen et al., 2022) in text simplification

--- PAGE 7 ---
Method MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI Average
RoBERTa large 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 91.3 88.9
TESS large(Ours) 90.1/89.8 94.2 89.1 88.5 96.4 93.1 67.7 88.9 83.1 88.5
Table 5: Comparison of TESS and RoBERTa on GLUE tasks on the development set. Following Devlin et al. (2019),
for MRPC and QQP, we report F1 score; STS-B, Spearman correlation coefficient; CoLA, Matthews correlation.
For all other tasks, we report accuracy. Bold fonts indicate the best results.
Model Text Simplification
SARI BLEU BERT R-L
TESS 44.1 30.8 78.8 52.6
+orig. self-cond 52.2 43.3 81.9 58.9
+proposed self-cond 54.2 40.8 82.0 59.3
Paraphrase generation
BLEU BERT R-L D-1/D-4
TESS 25.9 84.4 59.7 98.7/60.4
+orig. self-cond 28.4 85.5 61.7 98.6/60.6
+proposed self-cond 29.2 85.5 61.2 98.5/ 61.4
Table 6: Ablation on the effects of self-conditioning. We
compare our proposed self-conditioning to the original
method in Chen et al. (2022), and the model without
self-conditioning. Bold fonts indicate the best results.
and paraphrase generation tasks. As shown in
Table 6, adding self-conditioning consistently im-
proves results, with our variant delivering the best
overall performance.
6.3 Sampling Steps
We also investigate the quality of TESS genera-
tions on the suite of NLG tasks as well as MRPC
by varying the number of sampling steps during
inference. As shown in Table 7, TESS performs
well with relatively few steps, with only a marginal
drop in performance even when sampling steps
are decreased from 1000 to 100. For classification
tasks that involve shorter generation (MRPC), 10
sampling steps result in lossless quality. We also
find that decreasing the number of sampling steps
is possible in generative tasks like question genera-
tion. We provide results in Appendix B. Notably, it
appears that the number of steps required correlates
with the difficulty of the task: while classification
tasks such as MRPC only require few steps, longer
generation tasks such as CNN-DM require closer
to 100 steps to achieve good performance.
6.4 Sampling Speed
We compare TESS generation speed with other
models in Figure 4. We time how long decoding 25
to 5000 tokens takes given a context of 50 tokens
0 20 40 60 80 100 1200500
0 20 40 60 80 100 1200500
0 20 40 60 80 100 120
Length0500FrequencyFigure 2: TESS is capable of producing output of
variable lengths, matching target sequence lengths and
BART outputs for CNN-DM generations. From top to
bottom: (a) distribution of target lengths; (b) distribu-
tion of predicted length by BART; (c) distribution of
predicted length by TESS.
and 100 diffusion steps. We find that TESS is
substantially faster than SSD-LM, especially when
SSD-LM has to generate multiple blocks due to
its limited block size. Notably, we find TESS is
faster, albeit marginally, than an equivalently-sized
BART when generating more than 2000 tokens.
We provide further details in Appendix A.5.
6.5 TESS vs other Diffusion Methods
As shown in Section 5, TESS outperforms other
diffusion methods across several benchmarks. We
believe this is due to a number of factors: (1) the
simplex-based formulation being a more natural
fit for language than embedding-based ones, al-
lowing us avoid methods like clamping or an ex-
tra decoder for exiting the embedding space; (2)
the simplex-based self-conditioning formulation,
which we empirically show outperforms more stan-
dard self-conditioning methods (Table 6); (3) the
use of a pretrained model - while TESS can still
outperform other methods without using a pre-
trained model (Table 3), being able to make use of
pretrained models with relatively little extra train-
ing.

--- PAGE 8 ---
StepsQQP NEWSELA-AUTO QG CNN-DM MRPC
R-L Accuracy F1
10 62.4 58.4 38.8 35.6 89.7 92.8
100 62.0 59.1 38.9 39.6 89.7 92.8
1000 62.2 59.4 38.9 39.7 89.7 92.8
Table 7: Impact of number of sampling steps on performance. Our method achieves competitive results with as few
as 10 or 100 steps on NLG tasks and 10 for MRPC.
25 50 75 100 125
Target Length0102030ROUGETESS
BART
Figure 3: Average ROUGE score (R1, R2, R-L) for
TESS vs BART for CNN-DM generations. Our method
performs comparably to BART for different target
lengths.
7 Related Work
Diffusion for continuous domains Diffusion
models were first proposed by Sohl-Dickstein et al.
(2015) and popularized by Denoising Diffusion
Probabilistic Models (DDPMs) (Ho et al., 2020),
which proposed a new parameterization that re-
vealed an equivalence between ground-truth pre-
diction and noise estimation. Song et al. (2021)
proposed an alternative stochastic differential equa-
tion interpretation of diffusion that involves the
Stein score function. Nichol and Dhariwal (2021)
proposed a number of modifications to DDPMs,
which improved log-likelihood and reduced sam-
pling steps. Ho and Salimans (2021) proposed
classifier-free guidance, which allows for highly
controllable generation without the need for an ex-
ternal classifier to guide the model score estimates.
Continuous diffusion for discrete domains Fol-
lowing the success of diffusion models on continu-
ous domains, there have been several attempts to
apply diffusion on discrete data. Li et al. (2022a)
applied diffusion on the latent token embedding
space. Their resulting language model relies on
word-level tokenization and works mostly on small
datasets with a short sequence length of 64 tokens.
Strudel et al. (2022) used frozen pretrained word
embedding with careful scaling to address the in-
0 1000 2000 3000 4000 5000
Number of Tokens100101102103Time (s)
SSD-LM (B=25)
SSD-LM (B=200)
TESSlarge
BARTlargeFigure 4: Time taken to generate variable number of
tokens with 100 diffusion steps. We report the aver-
age time over five runs. Diffusion-based models use
RoBERTa largeas their backbone. TESS is substantially
faster than SSD-LM. Notably, for 2000 tokens, it is even
marginally faster than an equivalently-sized BART.
stability resulting from the competition between
diffusion and reconstruction loss. However, their
method does not allow the joint training of word
embeddings, and the model was not evaluated on
downstream NLP tasks. More recently, Dieleman
et al. (2022) attempted to learn the embedding and
diffusion model jointly, still by performing diffu-
sion in the embedding space. Other recent works
have also applied diffusion on word embeddings to
tackle sequence-to-sequence problems (Gong et al.,
2023; Yuan et al., 2022). Concurrent to our work,
Ye et al. (2023) proposed methods for manipulating
the noise in the diffusion process during training
and inference, yielding improved conditional text
generation. Another concurrent work explores vari-
ational diffusion models for language modeling in
embedding space (Gulrajani and Hashimoto, 2023).
However, they compare their models to 8×smaller
autoregressive models; our method obtains com-
petitive performance with same-size autoregressive
models.
Most relevant to our work, Han et al. (2022)
proposed a semi-autoregressive diffusion model
which generates small blocks of 25 tokens from

--- PAGE 9 ---
left to right, feeding them as additional context to
generate next blocks. We extend their approach
to fully non-autoregressive generation which sub-
stantially speeds up the inference time and incor-
porate an efficient self-conditioning method that
exploits the semantics of the simplex space. Han
et al. (2023b) similarly extends this approach, but
focuses on showing the viability of simplex-based
diffusion with large (> 1B parameter) models.
Discrete diffusion for discrete domains Unlike
continuous diffusion models, discrete diffusion
models maintain the discrete structure of the data
domain and perform state transitions based on a
probability matrix. Diffusion models with discrete
state space were first explored by Sohl-Dickstein
et al. (2015), who proposed a framework for diffu-
sion over binary random variables. Later, Hooge-
boom et al. (2021) and Austin et al. (2021) pro-
posed discrete diffusion models for categorical ran-
dom variables. However, these methods generally
lag behind autoregressive models. More recently,
Reid et al. (2022) proposed Diffuser, which for-
mulates a discrete diffusion process by modeling
generation as a series of discrete edit operations.
TESS substantially outperforms Diffuser.
8 Conclusion
We present TESS , a new sequence-to-sequence dif-
fusion model for language generation tasks that is
fully non-autoregressive, works for long sequences
compared to prior work, performs the diffusion pro-
cess on the vocabulary logit space, and employs a
new and efficient form of self-conditioning. TESS
outperforms strong autoregressive baselines as well
as recent state-of-the-art text diffusion models on
a wide variety of conditional language generation
and language understanding tasks, while also being
far more efficient than prior diffusion-based mod-
els. Future work relies on pretraining our method
combined with denoising and infilling objectives,
which we hypothesize can provide further perfor-
mance boosts to our text-to-text diffusion model.
Limitations
Sampling speed As seen in Figure 4, TESS is
still slower than BART when generating <1000 to-
kens. We experimented with reducing the number
of diffusion steps (see Table 7), which can further
speedup the generation. While in majority of tasks
using just 10 steps provides promising results, it is
not enough to achieve strong performance on morecomplex tasks such as summarization. Incorporat-
ing recent work in computer vision to accelerate
sampling in diffusion-based models (Song et al.,
2023) could result in further speedups in genera-
tion.
Long sequences Inference speed tests with SSD-
LM and BART show that TESS quickly domi-
nates semi-autoregressive generation and outper-
forms BART at 2000 tokens. This result suggests
that diffusion models have the potential of be-
ing faster than popular autoregressive models at
long sequence lengths. In this work, we primarily
used RoBERTa basemodels to facilitate fair com-
parison with existing baselines, which inevitably
limited the size of the context window due to the
absolute position embedding strategy employed by
RoBERTa. We suspect that unlocking the full po-
tential of diffusion-based language models may lie
in the long sequence regime, which could involve
scaling up the current models.
Ethics Statement
Language models are known to produce toxic and
biased content (Weidinger et al., 2022; Sheng et al.,
2021). While we explore an alternate modelling
framework to that commonly used in prior stud-
ies on the toxicity of language models, there is
little reason to suggest our models would not also
contain these issues. However, given the greater
controllability of the diffusion framework (Li et al.,
2022b), we hope future work explores how to make
use of this controllability to reduce potential harms.
Further examining how well results around toxic
and harmful generations of autoregressive setups
transfer to our setting may also aid in identifying
future areas for improvement.
Acknowledgements
We are grateful to Aman Madaan, Robin Strudel,
Sander Dieleman, Chris Dyer, Xiaochuang Han,
Sachin Kumar, Clara Meister, Sean Welleck, and
Andre Wibisono for helpful comments and discus-
sions, and Sam Skjonsberg and the ReViz team at
AI2 for their support in managing experiments.
References
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel
Tarlow, and Rianne van den Berg. 2021. Structured
denoising diffusion models in discrete state-spaces.
InNeurIPS .

--- PAGE 10 ---
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and
Danilo Giampiccolo. 2006. The second pascal recog-
nising textual entailment challenge. Second PASCAL
Challenges Workshop on Recognising Textual Entail-
ment .
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
TAC.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-
Gazpio, and Lucia Specia. 2017. SemEval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval-2017) , pages 1–14, Vancouver,
Canada. Association for Computational Linguistics.
Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. 2022.
Analog bits: Generating discrete data using diffu-
sion models with self-conditioning. arXiv preprint
arXiv:2208.04202 .
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges Workshop .
Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexan-
der G. Schwing, and David A. Forsyth. 2018. Fast,
diverse and accurate image captioning guided by part-
of-speech. 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 10687–
10696.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Bhuwan Dhingra, Kathryn Mazaitis, and William W
Cohen. 2017. Quasar: Datasets for question an-
swering by search and reading. arXiv preprint
arXiv:1707.03904 .
Sander Dieleman, Laurent Sartran, Arman Roshan-
nai, Nikolay Savinov, Yaroslav Ganin, Pierre H
Richemond, Arnaud Doucet, Robin Strudel, Chris
Dyer, Conor Durkan, et al. 2022. Continuous
diffusion for categorical data. arXiv preprint
arXiv:2211.15089 .
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing (IWP2005) .
Wanyu Du, Jianqiao Zhao, Liwei Wang, and Yangfeng
Ji. 2022. Diverse text generation via variational
encoder-decoder models with gaussian process priors.
arXiv preprint arXiv:2204.01227 .Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing , pages 1–9, Prague. Association for
Computational Linguistics.
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,
and Lingpeng Kong. 2023. DiffuSeq: Sequence to
sequence text generation with diffusion models. In
International Conference on Learning Representa-
tions, ICLR .
Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019. Lev-
enshtein transformer. NeurIPS .
Ishaan Gulrajani and Tatsunori B Hashimoto. 2023.
Likelihood-based diffusion language models. arXiv
preprint arXiv:2305.18619 .
Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov.
2022. Ssd-lm: Semi-autoregressive simplex-based
diffusion language model for text generation and
modular control. arXiv preprint arXiv:2210.17432 .
Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and
Marjan Ghazvininejad. 2023a. Ssd-2: Scaling and
inference-time fusion of diffusion language models.
ArXiv , abs/2305.14771.
Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and
Marjan Ghazvininejad. 2023b. Ssd-2: Scaling and
inference-time fusion of diffusion language models.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In NeurIPS .
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-
noising diffusion probabilistic models. NeurIPS .
Jonathan Ho and Tim Salimans. 2021. Classifier-free
diffusion guidance. In NeurIPS Workshop DGMs
Applications .
Jonathan Ho, Tim Salimans, Alexey A Gritsenko,
William Chan, Mohammad Norouzi, and David J
Fleet. 2022. Video diffusion models. In ICLR Work-
shop on Deep Generative Models for Highly Struc-
tured Data .
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini,
Patrick Forré, and Max Welling. 2021. Argmax flows
and multinomial diffusion: Learning categorical dis-
tributions. NeurIPS .
Chao Jiang, Mounica Maddela, Wuwei Lan, Yang
Zhong, and Wei Xu. 2020. Neural crf model for
sentence alignment in text simplification. In ACL.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. 2020. Diffwave: A versatile diffu-
sion model for audio synthesis. In ICLR .

--- PAGE 11 ---
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:
Denoising sequence-to-sequence pre-training for nat-
ural language generation, translation, and compre-
hension. In ACL.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, François
Lagunas, Alexander Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations , pages 175–184, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy
Liang, and Tatsunori Hashimoto. 2022a. Diffusion-
LM improves controllable text generation. In
NeurIPS .
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani,
Percy Liang, and Tatsunori B Hashimoto. 2022b.
Diffusion-lm improves controllable text generation.
InNeurIPS .
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu,
Zhihao Fan, Chen Lin, Nan Duan, and Weizhu Chen.
2023. Text generation with diffusion language mod-
els: a pre-training approach with continuous para-
graph denoise. In Proceedings of the 40th Interna-
tional Conference on Machine Learning , ICML’23.
JMLR.org.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Alexander Quinn Nichol and Prafulla Dhariwal. 2021.
Improved denoising diffusion probabilistic models.
InICML .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
John Thickstun, Sean Welleck, Yejin Choi, and Zaid
Harchaoui. 2021. Mauve: Measuring the gap be-
tween neural text and human text using divergence
frontiers. NeurIPS .
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. JMLR .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. 2022. Hierarchical text-
conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125 .
Machel Reid, Vincent J Hellendoorn, and Graham Neu-
big. 2022. Diffuser: Discrete diffusion via edit-based
reconstruction. arXiv preprint arXiv:2210.16886 .
Pierre H Richemond, Sander Dieleman, and Arnaud
Doucet. 2022. Categorical sdes with simplex diffu-
sion. arXiv preprint arXiv:2210.14784 .
Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. 2022. Photorealistic
text-to-image diffusion models with deep language
understanding. In NeurIPS .
Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski,
Erich Elsen, and Aaron van den Oord. 2021. Step-
unrolled denoising autoencoders for text generation.
InICLR .
Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong
Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian.
2023. Naturalspeech 2: Latent diffusion models are
natural and zero-shot speech and singing synthesizers.
ArXiv , abs/2304.09116.
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and
Nanyun Peng. 2021. Societal biases in language
generation: Progress and challenges. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International

--- PAGE 12 ---
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 4275–4293, Online.
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Jascha Sohl-Dickstein, Eric Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. 2015. Deep un-
supervised learning using nonequilibrium thermody-
namics. In ICML .
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020.
Denoising diffusion implicit models. In ICLR .
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. 2023. Consistency models. In ICML .
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole.
2021. Score-based generative modeling through
stochastic differential equations. In ICLR .
Robin Strudel, Corentin Tallec, Florent Altché, Yilun
Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl,
Nikolay Savinov, Sander Dieleman, Laurent Sifre,
et al. 2022. Self-conditioned embedding diffusion for
text generation. arXiv preprint arXiv:2211.04236 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. NeurIPS .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In ICLR .
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics , 7:625–641.
Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
Conor Griffin, Po-Sen Huang, John Mellor, Amelia
Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
Courtney Biles, Sasha Brown, Zac Kenton, Will
Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne
Hendricks, Laura Rimell, William Isaac, Julia Haas,
Sean Legassick, Geoffrey Irving, and Iason Gabriel.
2022. Taxonomy of risks posed by language models.
InProceedings of the 2022 ACM Conference on Fair-
ness, Accountability, and Transparency , FAccT ’22,
page 214–229, New York, NY , USA. Association for
Computing Machinery.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transform-
ers: State-of-the-art natural language processing. In
EMNLP: System Demonstrations .
Wei Xu, Chris Callison-Burch, and Courtney Napoles.
2015. Problems in current text simplification re-
search: New data can help. Transactions of the Asso-
ciation for Computational Linguistics , 3:283–297.
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,
and Chris Callison-Burch. 2016. Optimizing statisti-
cal machine translation for text simplification. TACL .
Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and
Mingxuan Wang. 2023. Dinoiser: Diffused condi-
tional sequence learning by manipulating noises.
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang,
and Songfang Huang. 2022. Seqdiffuseq: Text dif-
fusion with encoder-decoder transformers. arXiv
preprint arXiv:2212.10325 .
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-
berger, and Yoav Artzi. 2020. BERTscore: Evaluat-
ing text generation with BERT. In ICLR .

--- PAGE 13 ---
A Experiment Details
A.1 Dataset
NEWSELA-AUTO dataset (Jiang et al., 2020) is
based on a Xu et al. (2015) with revised alignment
and improved quantity and quality. For question
generation, we use the QUASAR-T dataset (Dhin-
gra et al., 2017); for summarization we use CNN-
DailyMail (Hermann et al., 2015).
The GLUE benchmark (Wang et al., 2019)
is released under the Creative Commons Li-
cense (CC BY 4.0). This benchmark consists of
multiple datasets: SST-2 (Socher et al., 2013),
MNLI (Williams et al., 2018), CoLA (Warstadt
et al., 2019), MRPC (Dolan and Brockett, 2005),
QQP9, QNLI (Rajpurkar et al., 2016), STS-B (Cer
et al., 2017), and RTE, which is a combination of
data from RTE1 (Dagan et al., 2005), RTE2 (Bar-
Haim et al., 2006), RTE3 (Giampiccolo et al.,
2007), RTE5 (Bentivogli et al., 2009). We down-
load all datasets from the Hugging Face Datasets
library (Lhoest et al., 2021). Table 8 shows the
sequence lengths for the source and target in each
dataset, in number of tokens.
Dataset Source Target
GLUE 128 5
NEWSELA-AUTO 128 128
QQP 100 85
QG 155 65
CNN-DM 392 120
Table 8: Sequence length of each dataset.
A.2 Baseline Details
When prior published results are on the same
dataset and metric, or when the codebase is not
publicly available, we use available reported results
to reduce compute costs. To provide a fair compari-
son when tuning baselines ourselves (SSD-LM and
BART), we use the same tuning budget and com-
pared them in the same setting (see Appendix A.3).
Lastly, we report values using the same decoding
strategy, i.e., the default setting without minimum
risk Bayes decoding (MBR).
For GENIE, we use the code and model weights
provided by the authors10to evaluate GENIE using
matched decoding settings to TESS.
9https://quoradata.quora.com/
First-Quora-Dataset-Release-Question-Pairs
10https://github.com/microsoft/ProphetNet/tree/
master/GENIEA.3 Training Hyperparameters
Following Han et al. (2022), we initialized the
model from a pretrained model and similarly found
that it improved performance (See Table 3). We
implemented our work using HuggingFace Trans-
formers (Wolf et al., 2020) and used the Hugging-
face Diffusers11to build our diffusion pipeline.
Our experiments are performed on 8 NVIDIA
A6000/A100 GPUs.
We trained our models and baselines with a
learning rate of 3e−5with the AdamW optimizer,
with default parameters β1= 0.9,β2= 0.999,
ϵ= 1e−8. We use a linear learning rate sched-
uler. We use the base model sizes for all experi-
ments (Wolf et al., 2020).
For SSD-LM, we use a block size of 25 follow-
ing the original paper and the same number of diffu-
sion steps during training and inference as our own
models. We reuse the codebase provided by the
authors and adapt it to support downstream tasks.
We do not test SSD-LM on CNN-DM due to the
difficulty of training SSD-LM on outputs involving
multiple decoding blocks, requiring custom data
preprocessing, and further algorithm tweaks to han-
dle the long outputs.
For all generation tasks, we train our method and
baselines for paraphrase generation, summariza-
tion, question generation, and text simplification
for 90K, 120K, 120K, and 80K steps, respectively.
We set the number of warmup steps to 2000 for all
generation tasks. For the experiments on GLUE,
we set the number of warm-up steps to 500. We
trained the models on larger datasets in GLUE for
25K steps; for smaller datasets, we use 12K steps.
We then evaluate the models every 1K steps and
report the results on the checkpoint obtaining the
best results on the development set. We found that
the training time of each model is roughly similar:
with equivalent configurations on a single GPU on
the QQP dataset, TESS achieves 1.7 train steps per
second; SSD-LM, 1.8; BART, 1.4, using PyTorch
2.0.
A.4 Evaluation Package Details
We use the following packages for calculating the
given metric:
•BLEU : We use the sacrebleu package (Post,
2018), v2.3.1.
11https://huggingface.co/docs/diffusers

--- PAGE 14 ---
•ROUGE : We use the rouge-score package,
v0.2.1.12
•Mauve : We use the mauve-text package (Pil-
lutla et al., 2021), v0.3.0.
•BERTScore : We use the bert-score pack-
age (Zhang et al., 2020), v0.3.12.
For other metrics, we use our own implementa-
tions (usually heavily based on a reference imple-
mentation), which will be open-sourced.
A.5 Inference Speed Experiments
For the inference speed numbers reported in Ta-
ble 4, we run all experiments on a single 80 GB
NVIDIA A100 GPU. We use an adapted version
of the SSD-LM inference script provided by the
authors in their public repository, removing log-
ging and initializing tensors on-device to avoid
expensive .to() calls. For BART large, we use the
transformers library (Wolf et al., 2020). We alter
all models to allow sequence lengths over 512 to-
kens by resizing the position embeddings matrix.
We use the following context: “A man of innumer-
able personalities and powers vs. the most power-
ful artificial intelligence in this universe: Legion
vs. Nimrod! With Nightcrawler in Orchis clutches,
David Haller and his allies will have to confront
the mastermind who". We report the exact timings
and standard deviations in Table 10.
B Sampling Steps
We performed additional ablations on the relation-
ship between the number of sampling steps and
performance on question generation.
Model Steps Forwards R-L
BART — 74 38.8
SSD-LM 10 50 33.0
SSD-LM 100 500 36.7
SSD-LM 1000 5000 38.5
TESS 10 10 38.8
TESS 100 100 38.9
TESS 1000 1000 38.9
Table 9: Sampling step ablation on question generation.
We note forward passes are not directly compa-
rable between AR and NAR models: AR models
may use more or less forwards than NAR models
depending on the number of tokens generated, and
12https://github.com/google-research/
google-research/tree/master/rougeeach forward pass of the AR model involves a dif-
fering number of tokens. Here, we use 74, as this
is the average number of BART tokens in question
generation responses.
Overall, we observe that SSD-LM’s perfor-
mance drops significantly with fewer diffusion
steps, while TESS remains largely unaffected. No-
tably, TESS achieves parity with BART with only
10 sampling steps.
C Inference Step
In the typical variant of DDPM, the model predicts
the added noise ϵθ(xt, t)instead of original signal
and the DDPM inference step (Ho et al., 2020) is
as follows:13
xt−1=1√αt
xt−1−αt√1−¯αtϵθ(xt, t)
(13)
Since we work with the variant predicting the signal
itself, we substitute (2) into (13), obtaining:
xt−1=1√αt√¯αtx0−αt−¯αt√1−¯αtϵθ(xt, t)
.
(14)
Given that ¯αt= ¯αt−1αt, we arrive at:
xt−1=√¯αt−1x0−√αt−¯αt√αt−¯αt√αt√1−¯αtϵθ(xt, t).
(15)
Using a cosine schedule for ¯αt(Nichol and
Dhariwal, 2021),p
(αt−¯αt)/(1−¯αt)≥0.98
for 98% t∈(1, T), with some outliers as t→0
andt→T(Han et al., 2022). Thus, with the
approximationp
(αt−¯αt)/(1−¯αt)≈1, Equa-
tion (15) further simplifies into:
xt−1=√¯αt−1x0−p
1−¯αt−1ϵθ(xt, t).(16)
In our case, the signal xtis the simplex St, with
ˆSθas the model prediction of the ground-truth.
Adjusting the above with this notation, we recover
Equation (9):
St−1=√¯αt−1ˆSθ(St, t) +p
1−¯αt−1ϵt.
D Qualitative Examples
We show randomly chosen example outputs from
TESS model and BART, the strongest baseline, on
the summarization task in Table 11. Qualitatively,
we observe that TESS is capable of generating nat-
ural samples that are often indistinguishable from
those of BART.
13Following Han et al. (2022) we drop the additional noise
termσtz, where z∈ N (0,I).

--- PAGE 15 ---
Number of Tokens
Model Blocks 25 100 200 300 400 500 600 700 800 900 1000 2000 3000 4000 5000
SSD-LM 25 1.6 0.36.60.315.0 0.325.6 0.337.8 0.353.4 0.470.4 0.090.1 0.0112.7 0.0139.4 0.0168.4 0.0643.2 0.21531.5 0.62933.4 0.94945.1 2.7
SSD-LM 200 1.6 0.32.10.32.80.35.80.36.50.3 - 11.7 0.3 - 17.9 0.0 - 25.8 0.089.6 0.0207.8 0.0391.1 0.2652.2 0.2
BART large - 0.8 0.21.40.22.60.23.20.24.10.24.60.25.50.06.50.07.60.0 8.70.0 9.80.023.5 0.240.6 0.1 62.4 0.1 89.4 0.4
TESS - 1.8 0.32.40.33.30.34.10.34.90.36.10.36.90.07.70.08.50.0 9.90.010.8 0.022.3 0.038.0 0.0 55.0 0.0 73.8 0.0
Table 10: Time taken to generate the given number of tokens with a 50-token prefix in seconds. All models use 100
diffusion steps and RoBERTa largeas the underlying model. All values are the average over 5 runs, with standard
deviations given as subscripts; standard deviations less than 0.05appear as 0.0.
Gold BART Ours
Membership gives the ICC jurisdiction over alleged crimes
committed in Palestinian territories since last June.
Israel and the United States opposed the move, which could
open the door to war crimes investigations against Israelis.The Palestinian Authority officially becomes the 123rd member
of the International Criminal Court.
It gives the court jurisdiction over alleged crimes committed in
Palestinian territories.
Israel and the United States, neither of which is an ICC member,
opposed the Palestinians’ efforts to join the body.The Palestinian Authority is the 123rd member of the Interna-
tional Criminal Court.
The move gives it jurisdiction over alleged crimes in Palestinian
territories.
Israel and the United States oppose the Palestinians’ efforts to
join it.
Theia, a bully breed mix, was apparently hit by a car, whacked
with a hammer and buried in a field.
"She’s a true miracle dog and she deserves a good life," says
Sara Mellado, who is looking for a home for Theia.A stray pooch in Washington State has used up at least three of
her own injuries.
Theia, a bully breed mix, was hit by a car and then buried in a
field.
She has been receiving care at the Veterinary Teaching Hospital.A stray dog was found on a farm in Washington State.
The dog has used up at least three of her own after being hit in
the head by a car and being buried in a field.
Theia has a dislocated jaw, leg injuries and needs surgery to
help breathe.
Mohammad Javad Zarif has spent more time with John Kerry
than any other foreign minister.
He once participated in a takeover of the Iranian Consulate in
San Francisco.
The Iranian foreign minister tweets in English.Mohammad Javad Zarif is the Iranian foreign minister.
He has been John Kerry’s opposite number in securing a break-
through in nuclear talks.Mohammad Javad Zarif is now Iran’s foreign minister.
He has been John Kerry’s opposite number in securing a break-
through in nuclear talks.
17 Americans were exposed to the Ebola virus while in Sierra
Leone in March.
Another person was diagnosed with the disease and taken to
hospital in Maryland.
National Institutes of Health says the patient is in fair condition
after weeks of treatment.One of the five had a heart-related issue on Saturday and has
been discharged.
The others have already gone home.Five Americans were monitored for three weeks at an Omaha
hospital.
One of the five had a heart-related issue on Saturday.
The others have already gone home.
They were exposed to Ebola in Sierra Leone, but none developed
the deadly virus.
Student is no longer on Duke University campus and will face
disciplinary review.
School officials identified student during investigation and the
person admitted to hanging the noose, Duke says.
The noose, made of rope, was discovered on campus about 2
a.m.A student admitted to hanging a noose made of rope from a tree
near a student union.
Duke didn’t identify the student, citing federal privacy laws.
The incident is one of several recent racist events to affect
college students.A Duke student admitted to hanging a noose from a tree.
The private school didn’t identify the student, citing federal
privacy laws.
The student is no longer on campus and will face student con-
duct review.
College-bound basketball star asks girl with Down syndrome to
high school prom.
Pictures of the two during the "prom-posal" have gone viral.Eastern High School basketball player Trey Moses asked his
girlfriend to be his prom date.
Ellie Meredith, a freshman with Down syndrome, has struggled
with friendships since elementary school.
A special program at Eastern has made things easier for her, her
mom says.College basketball player and high school freshman picked Ellie
Meredith as his prom date.
The photos have gone viral on social media.
Amnesty’s annual death penalty report catalogs encouraging
signs, but setbacks in numbers of those sentenced to death.
Organization claims that governments around the world are
using the threat of terrorism to advance executions.
The number of executions worldwide has gone down by almost
22% compared with 2013, but death sentences up by 28%.Amnesty International says governments are using the death
penalty to advance executions.
At least 607 people were executed around the world in 2014,
compared to 778 in 2013.Amnesty International released its annual report on the death
penalty.
At least 607 people were executed in 2014, compared to 778 in
2013.
Report cites Pakistan lifting a six-year moratorium on the exe-
cution of civilians.
China has used the death penalty as a tool in its "Strike Hard"
campaign against terrorism in the restive far-western province
of Xinjiang.
Andrew Getty’s death appears to be from natural causes, police
say, citing coroner’s early assessment.
In a petition for a restraining order, Getty had written he had a
serious medical condition.
Police say this is not a criminal matter at this time.Andrew Getty appears to have died of natural causes, police say.
The coroner’s preliminary assessment is there was no foul play
involved in his death.
He was the grandson of oil tycoon J. Paul Getty.Coroner’s preliminary assessment says there was no foul play
involved in the death of Getty.
Andrew Getty, 47, had "several health issues," detective says.
There is no criminal investigation underway, detective says.
Once a super typhoon, Maysak is now a tropical storm with 70
mph winds.
It could still cause flooding, landslides and other problems in
the Philippines.Maysak has lost a lot of steam as it spins west in the Pacific
Ocean.
It boasts steady winds of more than 70 mph (115 kph) and gusts
up to 90 mph.Maysak gained super typhoon status thanks to sustained 150
mph winds.
Authorities have forbanned outdoor activities like swimming,
surfing, diving and boating in some locales.
Table 11: Randomly chosen samples generated on the CNN-DM dataset by BART and TESS.
