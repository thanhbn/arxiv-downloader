# 2211.04236.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2211.04236.pdf
# File size: 910335 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SELF-CONDITIONED EMBEDDING DIFFUSION
FOR TEXT GENERATION
Robin Strudel1Corentin Tallec2Florent Altch ´e2Yilun Du3
Yaroslav Ganin2Arthur Mensch2Will Grathwohl2Nikolay Savinov2
Sander Dieleman2Laurent Sifre2R´emi Leblond2
ABSTRACT
Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete nature
of text data, we can simply project tokens in a continuous space of embeddings, as
is standard in language modeling. We propose Self-conditioned Embedding Diffu-
sion (S ED), a continuous diffusion mechanism that operates on token embeddings
and allows to learn ﬂexible and scalable diffusion models for both conditional and
unconditional text generation. Through qualitative and quantitative evaluation, we
show that our text diffusion models generate samples comparable with those pro-
duced by standard autoregressive language models — while being in theory more
efﬁcient on accelerator hardware at inference time. Our work paves the way for
scaling up diffusion models for text, similarly to autoregressive models, and for
improving performance with recent reﬁnements to continuous diffusion.
1 I NTRODUCTION
Continuous diffusion models (Sohl-Dickstein et al., 2015) have taken the world of image generation
by storm, advancing the state of the art further than ever before (Rombach et al., 2021; Ramesh
et al., 2022). Can the same framework encounter as much success on the text modality? Diffusion
for language is indeed an attractive prospect. Compared to autoregressive (AR) models (Bengio
et al., 2000; Sutskever et al., 2011; Austin et al., 2021; Hoffmann et al., 2022), diffusion models can
predict all tokens in a sequence at once. This allows for bidirectional, rather than causal attention—
increasing interactions between tokens, potentially leading to more coherent samples. Diffusion
models can make a better usage of hardware accelerators during inference than AR models, since
computations are parallelizable over the sequence axis.
Yet AR models remain the mainstream approach for modelling text. A major obstacle to text dif-
fusion is that diffusion processes typically operate in continuous space. While this naturally handle
images, text is inherently discrete. Consequently, most previous attempts to apply diffusion to text
have focused on discrete diffusion-like approaches. These methods do not beneﬁt from the re-
ﬁnements made to continuous diffusion in the image domain. Crucially, they cannot make use of
guidance (Dhariwal & Nichol, 2021), which drastically improves diffusion models sample quality.
We address this gap by making a simple observation: language models operate mostly in continuous
space, with discrete tokens only as inputs and outputs. A natural idea is then to conduct diffusion
directly in a continuous token embedding space. For simplicity, we use a ﬁxed embedding space,
either random or stemming from a trained language model. Combined with the “self-conditioning”
(Chen et al., 2022) reﬁnement, this forms the basis of the method we propose, Self-conditioned
Embedding Diffusion (S ED).
1INRIA, D ´epartement d’informatique, ´Ecole normale sup ´erieure, CNRS, PSL Research University
Work done while interning at DeepMind
2DeepMind
3Massachusetts Institute of Technology
1arXiv:2211.04236v1  [cs.CL]  8 Nov 2022

--- PAGE 2 ---
SEDmodels rival mainstream AR models in both conditional and unconditional text generation. We
make the following contributions:
• In section 3, we introduce S ED, the ﬁrst continuous diffusion approach for text with good
scaling properties (testing models up to 420M parameters). We analyze several continuous
text diffusion settings, and identify self-conditioning and diffusion on small ﬁxed embed-
dings as key factors to make continuous text diffusion work.
• In section 4, we apply classiﬁer-free guidance (Ho & Salimans, 2022) to text data—an orig-
inal achievement. We show that S EDcan rival AR models on generic language tasks, for
similar models sizes. S EDsamples achieve a better likelihood-entropy trade-off compared
to these models, and are deemed comparable (if slightly worse) by human raters.
2 R ELATED WORK
We provide an overview of diffusion models with a focus on modeling discrete data, as well as AR
models and sample-based metrics for evaluating text generation.
Continuous diffusion on continuous image data. Continuous diffusion has recently established
itself as the method of choice for modeling continuous data such as images. While our main focus
in this paper is on discrete data, we review some key works in continuous data modeling as this
literature was the major source of inspiration for S ED. The ﬁrst continuous diffusion formulation
was introduced in the seminal work by Sohl-Dickstein et al. (2015). Ho et al. (2020) improved and
simpliﬁed this formulation, relating it to denoising score matching, and creating a new method called
DDPM. Nichol & Dhariwal (2021) further improved upon DDPM, showcasing impressive diffusion
results compared to GANs. Rombach et al. (2021, Stable Diffusion) introduced diffusion in latent
space. Conceptually similar to S ED, it was speciﬁcally targeted at image modeling. Classiﬁer-free
guidance was proposed by Ho & Salimans (2022) as a mean to improve image ﬁdelity at the cost
of reduced diversity. GLIDE (Nichol et al., 2022) scaled up the ideas of guided diffusion, while
DALL-E 2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) are the latest, most advanced
image generation systems to date, combining most of the improvements proposed in previous works.
Discrete diffusion on discrete data. One cannot simply reuse the methods that are successful
on continuous image data in the discrete text domain. A number of bespoke methods have been
explored instead, forming the family of discrete diffusion approaches. In discrete diffusion, the
data is corrupted by switching from one discrete value to another. This was ﬁrst proposed in the
seminal work by Sohl-Dickstein et al. (2015), where it was tested on simplistic binary heartbeat
data. It was extended to multinomial text modeling (Hoogeboom et al., 2021) and further scaled
up in the D3PM work (Austin et al., 2021). Most recently, a similar discrete diffusion approach
was applied to image modeling in VQ-Diffusion (Gu et al., 2022). In parallel, a few diffusion-like
approaches were proposed in the denoising autoencoders literature. CMLM (Ghazvininejad et al.,
2019) tackled machine translation. SUNDAE (Savinov et al., 2022) was the ﬁrst non-AR method to
show strong results both in machine translation and unconditional text generation. MaskGIT (Chang
et al., 2022) demonstrated excellent results in modeling VQ-discretized images. These approaches
rely on training models to predict masked tokens from their context, and iterating this reconstruction
step multiple times at sampling time. Despite those positive developments, the samples from discrete
diffusion methods for text modeling remains less coherent than those produced by AR methods.
Continuous diffusion on discrete data. Fewer works try to tackle diffusion on discrete data from
the same angle as S ED– starting by turning the data into continuous representations before mod-
eling it with continuous diffusion formulations. Mittal et al. (2021) used a V AE to generate such
representations for discrete music modeling, with exciting results. Closest to S ED, Diffusion-LM (Li
et al., 2022) trains a token embedding together with the diffusion model itself. Diffusion-LM meets
success on speciﬁc language applications, in low data regime and on constrained, very formatted
textual data. Most recently, Analog Bits (Chen et al., 2022) introduced self-conditioning , closely re-
lated to step-unrolls in SUNDAE (Savinov et al., 2022), together with bit-level modeling to improve
the generation of discretized images. While the qualitative results of those continuous methods on
text modeling show promise, they have not been shown to scale to large realistic text datasets like
C4 (Raffel et al., 2020) yet, or to compare with AR approaches on generic language tasks.
2

--- PAGE 3 ---
Auto-regressive modelling on discrete data. AR models remain the method of choice for mod-
eling discrete data. In combination with neural networks, they were ﬁrst explored by Bengio et al.
(2000) and later combined with RNNs (Sutskever et al., 2011). Their breakthrough moment came
with the advent of the Transformer architecture, introduced by Vaswani et al. (2017) for machine
translation. Even more impressive results were shown with GPT-3 (Brown et al., 2020), which
trained a large AR language model unconditionally, and used few-shot prompting to adapt it to new
tasks. A few works later improved upon the results of GPT-3, including Hoffmann et al. (2022).
Sample-based evaluation of text generative models. There are traditionally two classes of met-
rics for generative modeling: likelihood-based and sample-based. While the likelihood-based way
is mathematically appealing, its usefulness for measuring progress is reduced by the fact that not
all models readily provide likelihood computation. Just like the sampled-based FID metric was im-
portant for driving the progress of diffusion in image modeling, there is a need for a sample-based
metric which would be universally accepted for text modeling. Caccia et al. (2018) investigated
ﬁdelity/variance metrics for evaluating text GANs. Semeniuta et al. (2018) suggested using FID for
texts. De Masson d’Autume et al. (2019) later used those previously proposed metrics to iterate on
ScratchGAN but did not provide conclusive guidance on which metric a practitioner should choose –
essentially ﬁnding serious vulnerabilities in all investigated metrics. We opted for a middle ground,
reporting both sample likelihood according to a strong AR model and human preferences.
3 M ETHOD
In this section, we outline the different components of S ED: continuous diffusion in the space of
token embeddings and self-conditioning, which form the basis of our approach for unconditional
text generation; span masking and guided diffusion to enable conditional generation.
3.1 D IFFUSION MODELS FOR UNCONDITIONAL TEXT GENERATION
Diffusion models in continuous space. We consider diffusion models as introduced by Sohl-
Dickstein et al. (2015) and improved by Ho et al. (2020). A diffusion model aims at modelling
a data distribution x02Rnq2D(Rn)by estimating a sequence of latent variables xT, ...,x1of
the same dimensionality as the data x0. Starting from x0, the latent variables are generated with a
Markov chain called the forward process :xtq(jxt 1;t). It is deﬁned by gradually interpolating
the iterate with Gaussian noise according to noise levels deﬁned by a schedule 1;:::;T:
xtq(jxt 1;t) =N(p
1 txt 1;tI): (1)
This parametrization gives us a closed form to sample xtfor any arbitrary t1, givenx0:
xt=ptxt 1+p
1 tt=ptx0+p
1 t; (2)
wheret:= 1 t,t:=Qt
s=1s,tN 
0;I
andN 
0;I
.
We deﬁne our generative model by approximately inverting the diffusion process of Eq. 1 to obtain
areverse process . The reverse process starts from xTN(0;I)and is deﬁned as a Markov chain
with learned Gaussian transitions (parameterized by , the weights of a neural network): xt 1
p(jxt) =N 
(xt;t);(t)2I
. We train a neural network to predict an estimate ^x0(xt;t;)
of the datax0and approximate the reverse process by using the following parametrization, with
learnable means but ﬁxed variances, and a ﬁxed schedule 1;:::;T:
(xt;t) =pt 1t
1 t^x0(xt;t;) +pt(1 t 1)
1 txt;  (t)2=1 t 1
1 tt (3)
While there exists a tractable variational lower-bound (VLB) on logp(x0), Ho et al. (2020) showed
that better results are obtained by optimizing a simpliﬁed objective that re-weights the terms in the
VLB. We follow this approach, which simpliﬁes the loss to a sum of mean-squared errors between
the ground truth data x0and its estimates ^x0(xt;t;):
3

--- PAGE 4 ---
Ldiffusion =Ex0q(x0);tU(1;T)kx0 ^x0(xt;t;)k2 (4)
Though this framework works out of the box on images, which are close to continuous, we can-
not apply it directly to the discrete tokens of the text modality. To resolve this issue, we perform
continuous diffusion in a continuous space in which we embed text tokens.
Diffusion on word embeddings. We consider textual data w= (w1;:::;wN), where each wiis
a one-hot representation in RVof a discrete token in f1;:::;Vg. Each token whas an associated
embeddingew2RD, with ﬁxed normp
Dto match the norm of a random gaussian sample in
dimensionDused to noise clean data. We denote by E2RDVthe matrix of all embeddings.
We deﬁne our diffusion process in embedding space, rather than in token space. To that end, we
deﬁne a forward discrete-to-continuous stepqV(x0jw) =N(Ew;2
0I), where0is a constant
scale factor with a similar order of magnitude as 1. Conversely, we deﬁne a reverse continuous-
to-discrete steppR(wjx0) =QN
k=1Cat(wkjE0(x0)k), whereR2RVDis a learnable readout
matrix initialized to E>andCat(wkjl)is the softmax probability of token kwith logitsl2RV.
To train the readout step, we add a reconstruction loss to Ldiffusion during training. Conveniently,
it naturally arises when deriving the VLB of p(w)with this discretization step (Li et al., 2022),
introducing a simple cross-entropy loss to maximise p(wjx0):
Lrecon=EwD;x0qV(w)[ logpR(wjx0)]; withLtotal=Ldiffusion +Lrecon: (5)
Contrary to what is done in Li et al. (2022), we do not learn the embedding matrix E, as we identiﬁed
that it was empirically unstable and could lead to drops in unigram entropy. The reconstruction loss
Lrecon therefore only depends on the trainable readout weights R.
At sampling time, we run the reverse process for T= 1000 steps, ultimately yielding a continuous
embeddingx0of sizedembed . We multiply it by Rto obtain logits in RV, and then use the index of
the maximum component to convert it to a token wi, withi= arg max1jV(Rx0). This entails
runningTfull forward passes which is quite expensive compared to cached AR sampling; however
each forward pass computes all timesteps at once which is naturally parallelisable. Further, we hope
to beneﬁt from many diffusion sampling improvements to get Tdown to low double-digits.
Self-conditioning (Chen et al., 2022). In standard diffusion sampling, at each timestep tthe
denoising network generates an estimate xt
0=^x0(xt;t;)ofx0given onlyxtas input. Self-
conditioning progressively reﬁnes x0estimates by passing the estimate ~xt+1
0obtained at the previ-
ous sampling step as input to the denoising network; the self-conditioned estimate is then deﬁned as
~xt
0=^x0(xt;~xt+1
0;t;), and sets the diffusion direction. In practice conditioning is performed by
concatenating xtand~xt+1
0on the feature axis. To approximate the inference behavior at train time
while remaining computationally efﬁcient, we compute a ﬁrst estimate xt
0=^x0(xt;0;t;)with
the self-conditioning set to zero, then perform a second forward pass using a stop gradient on xt
0to
obtain ~xt
0=^x0(xt;xt
0;t;). The denoising network is then optimized using the output from the
two forward passes in order to estimate x0accurately with and without self-conditioning.
Equipped with these 3 components we can train models to generate text, though only uncondition-
ally. To add conditional generation to our system’s capabilities, we use two additional methods.
3.2 S PAN MASKING AND GUIDANCE FOR CONDITIONAL TEXT GENERATION
By design diffusion models for text generation are ﬂexible and can handle a wide variety of inﬁlling
tasks. This is a key advantage over the predominant auto-regressive language models that typically
generate text in a left-to-right fashion.
Span masking. We train our model on a rich set of inﬁlling tasks with the following method. We
splitx0between two set of tokens, diffusion tokens xover which we apply diffusion and optimize
the diffusion loss from Eq. 4, and conditioning tokens cthat remain ﬁxed. Conditioning tokens care
deﬁned by a binary conditioning mask mset to one on conditioning positions and zero on positions
to be inﬁlled.
We sample conditioning mask mrandomly as follows. Given a sequence of length Land a max-
imum number of spans M, we sample a number of spans nuniformly in [1;M]. Span starting
4

--- PAGE 5 ---
Table 1: S EDsamples on unconditional generation, ﬁll-in-the-middle and several spans in-ﬁlling.
Task Samples
Unconditional Wemake useofthevery bestsupplies andsolutions toensure thatthework isgoingtostand up
tothetestoftime, andwehelp yousave money with techniques thatdonotchange thequality
ofyour mission. We’ll achieve thisbyofferingyouthebest deals intheﬁeld andavoid ing
pricey mistakes. Ifyouwant tospend less, RefrigeratorUnit RepairGuys isthecompany to
contact.
Fill-in-the-
middleA year ago in Paris, Ihadtheopportunitytotake aﬁeld triptoLaRite-en-Laurences Interna-
tional deFrance where ImetDavid Nigel John son, aprofessorofsocialstudies.What agreat
tripand what a great day!
Spans
in-ﬁllingThere was no evidence, only ﬂeeting glimpses ofthekiller andhisfate. Infact, itseemed
thatthere wasnoevidence. It was all guesswork ,andoneofthemost unusual murdercases
through outhistory.
positions are deﬁned by n 1integers (i1;:::;in 1)sampled uniformly without replacement and
sorted in increasing order to satisfy 0<i1<:::<in 1<L. The tuple (i1;:::;in 1)partitions the
sequence of tokens in nspans satisfying E[ikjn] =k
nL. The conditioning mask mis deﬁned using
even spans for conditioning and odd spans for inﬁlling, and then mis ﬂipped with a 50% probability.
The casen= 1corresponds to unconditional generation; we then set mto 0 everywhere.
This span masking strategy deﬁnes a collection of text generation tasks with a large variety of con-
ditioning which on average evenly splits the sequence between conditioning and inﬁlling spans. It
enables conditional generation, and opens the door for additional diffusion improvements.
Guided diffusion. Guidance (Dhariwal & Nichol, 2021) often improves the sample quality of con-
ditional diffusion models. We use classiﬁer-free guidance (Ho & Salimans, 2022), which alleviates
the need for a separately-trained guide model. In the conditional case, our estimator ~x0is now a
function ^x0(xt;c;~xt+1
0;t;), wherecare ﬁxed conditioning tokens.
During training, with ﬁxed probability the conditioning tokens cused in the estimator ^x0are
dropped and set to a null label ;equal to zero. During sampling, the model prediction is extrap-
olated in the direction of ^x0(xt;c;~xt+1
0;t;)and away from ^x0(xt;0;0;t;)as follows:
~xt
0;s=^x0 
xt;0;0;t;
+s
^x0 
xt;c;~xt+1
0;t;
 ^x0 
xt;0;0;t;
; (6)
wheres1is the guidance scale. Remark that we jointly drop conditioning and the self-
conditioning ~xt+1
0, concretely setting both values to zero. Classiﬁer-free guidance allows leveraging
both the unconditional and conditional abilities of a model to improve its conditional generations.
4 E XPERIMENTS
4.1 T RAINING DETAILS
We train all our models on the C4 dataset (Raffel et al., 2020), using a SentencePiece tokenizer (Kudo
& Richardson, 2018) composed of 32000 words. We use a non-causal transformer model (Vaswani
et al., 2017) as our diffusion model (see Appendix A for details). S EDmodels are trained with
sequence length 256, while for ablations models are trained with sequence length 128. We insert
uniformly, i.e. not necessarily at the end of the sequence, 10% of padding tokens in the training set
to allow S EDmodels to generate samples of varying size and provide more ﬂexibility.
To generate word embeddings, we train a BERT model of ﬁxed size ( 150m parameters) and feature
dimensiondmodel = 896 . The diffusion space is deﬁned by the initial lookup table of this BERT
model. We bottleneck the dimension of the word embeddings dembed and add a linear projection layer
fromdembed todmodel at the beginning of the model. We found this helped diffusion (see section 4.4).
SEDmodels are trained with a cosine noise schedule (Dhariwal & Nichol, 2021), with 1= 2:10 3,
0= 10 2andT= 1000 . We use batches of 65.536 tokens, thus for sequence length 256 the batch
size is set to 256. We use a maximum span count of 5 for all runs except for its speciﬁc ablation. We
5

--- PAGE 6 ---
train S EDmodels at two different scales: S ED-S (135m parameters, 106training steps) and S ED-L
(420m,2:106steps). Their detailed architectures can be found in Appendix A.
4.2 V ALIDATION
While optimizing the perplexity of AR models for text leads to improved language models, directly
optimizing the ELBO of diffusion models for images does not correlate strongly with sample quality
as observed by Nichol & Dhariwal (2021); Kingma et al. (2021); Ho & Salimans (2022). For images,
the sample based metric FID (Heusel et al., 2017) has been introduced as a measure of sample
quality and is now widely adopted. Similarly, we need a sample-based metric for text generation
that is reliable and allows comparison between a large variety of generative models. To provide a
fair comparison to AR models, we rely on three metrics.
The ﬁrst metric measures how likely the samples produced by a model are according to an AR
language model with 70B parameters, trained on 1.4B tokens (Hoffmann et al., 2022); we denote
this metric AR NLL for auto-regressive negative log-likelihood. It provides a continuous measure
of sample quality that has proven useful when combined with a measure of sample diversity, e.g. in
the development of nucleus sampling (Holtzman et al., 2020) for improved AR model decoding.
To measure diversity we rely on a second metric, the unigram entropy of samples, which helps
balance the AR NLL that can be gamed by unnatural repetitive samples. For both these metrics, our
target is the score of the validation set data. Deviating from the data unigram entropy in particular
is a sign of degenerate modeling.
Though this initial combination has provided us with a reliable signal to iterate over our model
design, it remains imperfect; it too can be gamed, though it is harder to do so. To address this
limitation, we also report human preferences. We presented 6 colleagues with 20 pairs of samples
for each comparison, asking them to pick the best one.
For all three metrics, we report results on two tasks: unconditional language modeling and sufﬁx
in-ﬁlling, the later a heavily conditioned task.
4.3 R ESULTS
Figure 1: Comparison of sample quality and
diversity of S EDversus AR models on sufﬁx
in-ﬁlling. S EDuses guidance with scales in
f1;2;4;8gand AR uses nucleus sampling with a
top-pinf1:00;0:95;0:90;0:85g. Top-right points
are S EDmodels with a guidance scale of 1 or AR
models with a top- pof 1.Samples. We present samples generated with
our S EDmodels in Table 10. We use a single
model to perform a wide variety of text gener-
ation tasks, such as unconditional generation,
ﬁlling-in-the-middle or ﬁlling several spans of
text. We show strong performance in the un-
conditional case, with samples that are syntac-
tically correct and stay coherent on long se-
quences. In the conditioned case, S EDmodels
are able to inﬁll spans with coherent transitions
and links to the conditioning but also exhibit
a rich diversity. By design, S EDyields ﬂexible
bi-directional masking models that can perform
text generation on a diverse set of conditioned
task. To compare S EDwith AR baselines we
next restrict conditioning to a preﬁx and con-
sider a task of sufﬁx in-ﬁlling.
Comparison to AR models. To assess the gen-
eration ability of S ED, we compare against AR
baselines of similar capacity and trained fol-
lowing optimal scaling laws from Hoffmann
et al. (2022) on sufﬁx in-ﬁlling. We sample a
batch of sequences from C4 and use the ﬁrst
128 tokens as conditioning given to the model
to generate a sufﬁx of 128 tokens. Figure 1 re-
ports AR NLL and unigram entropy of the gen-
6

--- PAGE 7 ---
Table 2: We compare S ED-L (guidance scale 2.5) and AR-L (nucleus sampling, p= 0:95) samples.
SEDL AR-L
You’re goingtolove wear ingthistraditional tee
from ourlatestWilsoncollection. Designed ina
scrapped ﬂoralstyled knitwith asleeve ofasym -
metriclines across theround sole. Lightly ﬂuffy,
thesquare pleats willtake youright toAKoda Ram 25ispresented insedan andaMax-
imasaloon. Based ontheAcenta car,thepower-
fulKoda 2014 hitsIndian roads inthe”Max ima”
body -conshape. Beingpowered byaHyundai i20
1.4litrediesel engine, theKoda 2014 iscoupled
The beaver is an interesting animal that lives in
rivers and lakes. Heisnotmainly concerned with
ﬁndingwolves anddolphin butalso hasagreat
hunger forﬁsh. The beaver hassharp legs, large
eyes, andablack coatThe beaver is an interesting animal that lives in
rivers and streams. Itisusuallyseen inbignum-
bers intheﬁelds orupstream, andisquite docile.
Oncold days when itspattern isperfect, the
beaver willhave some interesting,andsome times
Once upon a time in Spain, Leonardo Puelevahad
thepleasure ofmeet ingguests atSpan ishrestau -
rant, Buva Casinos.While driving,hegotachance
togettoknow thepeoplebehind therestau rant
and, ofcourse, how they made hisexperience very
interesting.Afterhisconversation, hegottoOnce upon a time in Spain, which seems pretty
much thesame way now, thequestion that was
posed toeach ofusattheendofourinterview was
”would youliketoseeFroome oneday?” Inret-
rospect, afterourinterview, wehave grown ever
closer tothatanswer. Asyouwillread inthearti-
cle,Iknow that
erated sufﬁxes for AR and S EDmodels. As a
reference point, we compute the AR NLL and unigram entropy of the ground truth C4 sufﬁxes and
report it on the plot. Several methods can be used to improve sampling quality at the cost of samples
diversity; we use nucleus sampling (Holtzman et al., 2020) for AR models and guidance (Dhariwal
& Nichol, 2021; Ho & Salimans, 2022) for S EDmodels. We show the impact of guidance on sam-
ples quality in Table 3. To our knowledge, we are the ﬁrst to show sample quality improvement
when using guidance for text generation.
As shown in Figure 1, both S ED-S and S ED-L perform strongly when compared against AR base-
lines – even though we report a metric favoring AR models on a task AR models have been designed
to optimize. Similar to nucleus sampling for AR models, guidance has a strong positive impact on
sample quality that is both observed quantitatively with improved AR NLL in Figure 1 and quali-
tatively in Table 3. We observe that using a top- pnucleus sampling below 0:8for AR models or a
guidance scale above 4for S EDmodels leads to samples exhibiting a lot of repetitions, a degenerate
case reﬂected by a lower entropy of samples even though sample AR NLL improves.
Our human preference scores temper our observations in Table 4. They show that our NLL and
entropy metrics do not tell the whole story, as humans still prefer AR models at equivalent size.
While S ED-L performs slightly worse than AR-L (38% preference in sufﬁx in-ﬁlling, 44% on un-
conditional generation), its scores remain comparable. S ED-L is roughly on par with AR-S.
Finally, we compare S EDand AR models’ qualitative examples with short prompts in Table 2.
4.4 A BLATIONS
Self-conditioning and embedding pretraining. Results from Table 5 and samples from Table 6
show the inﬂuence of both the diffusion space and self-conditioning. AR NLL decreases very
signiﬁcantly when using self-conditioning, regardless of the rest of the setup. Diffusing at the
bit-level (Chen et al., 2022) yields very high NLLs. While using random embeddings performs
markedly better, using pretrained embeddings results in further improved numbers.
Samples from Table 6 highlight that models trained on random word embeddings exhibit topic mod-
elling abilities with the co-occurrence of words like child andmother even though the paragraph
remains globally incoherent and meaningless tokens like gluc are generated. Self-conditioning
dramatically improves sample quality; the diffusion model gets the low-level structure right and
generates syntactically correct sentences, even though the global text is not intelligible. Combining
self-conditioning and pretrained embeddings leads to globally coherent paragraphs that stay on topic
with proper sentence structure.
7

--- PAGE 8 ---
Table 3: Impact of guidance on samples quality using our S EDmodel.
Guidance 1.0 2.5 5.0
In the cold, cold night sky, a
fairy princess sitsinachair
andsurrounded bytealeaves
inapond. Mean while, she
bies back into thecold, with
bluish haironherhips andel-
bows onherforehead -and
her ﬁngers numbed by the
freezing temperature.In the cold, cold night of
Novem ber2018, alittlegirl
sitsinachair hiddenunder
alight blanketonapatio.
Mean while, shebends back
into thechair with bluish
hair onher forehead, her
hands onherface, her ﬁngers
numbed by the freezing tem-
perature.In the cold, cold night of
December,myoldestdaugh -
tersitsinachair accentuated
incottonfabrics and apil-
low. Mean while, sheyearns
straight inthecold air,her
wrists coveringherneck, her
eyes straight onherforehead,
and her ﬁngers numbed by the
freezing temperature.
Barbara wasoneofourmany
wonderfulwomen thatreally
helped soIamsoblown off
byherpurpose, civility;and
adversity.Once Istarted inter-
actingwith her, itproved to
methat no matter how hard
this was, she always strove for
excellence.Barbara wasoneofthemost
gifted women intheworld.
She was creative andstood
upbyherintegrity and ci-
vility;against adversity. Al-
though she placed herself
higher than her peers, it
proved tomethat no matter
how hard this was, she al-
ways strove for excellence.Barbara was oneofthemost
brilliant women intheworld.
Shewasamaz inginherheart,
herspirit, hermind andinthe
soul. Shenever turned people
offinherabsence. Itproved
tomethat no matter how hard
this was, she always strove for
excellence.
Table 4: S ED-L vs other models human preference scores on conditional and unconditional tasks.
SED-S (cond) AR-S (cond) AR-L (uncond) AR-L (cond)
SED-L 63.4%4.3% 51.0%5.0% 43.8%4.4% 37.7%4.4%
Embedding dimension. An important design choice for SED is the word embeddings space. We
study the inﬂuence of pretrained embedding size in Table 7. Surprisingly, there is a threshold af-
ter which performance degrades when increasing the dimension of embeddings. We visualize the
forward process for different embedding sizes by displaying the nearest neighbor of a noised token
while running the forward process. In high dimension we observe that the nearest neighbor of a
noised token remains the starting token itself until it switches to a completely random, unrelated
token. In low dimension, we often observe that the closest neighbor of a noised token goes through
several semantically related tokens (nearest neighbor of the starting token) before ultimately becom-
ing random. We hypothesize that the random walks deﬁned by diffusion are more likely to drift
towards neighbors of the starting token in low dimension. As a result, when diffusing in low dimen-
sion information is destroyed in a more semantically meaningful fashion, which leads to an easier
learning problem for the denoising function.
Number of spans. In order to enable in-ﬁlling, we train the model not only to do unconditional
generation but also to conditionally ﬁll spans of tokens. For each data point we sample a span
number uniformly at random and span delimiters to generate the span mask. Picking the maximum
allowable number of spans has a signiﬁcant effect on model performance, as we can see in Table 8.
Somewhat counter-intuitively, adding span masking improves even unconditional generation NLLs.
It also appears that using a relatively high maximum span number is optimal. We hypothesize that
this results in a varied mix of task difﬁculty at training time, between ”easy”, very conditioned
problems on the one hand and ”harder”, unconditional ones on the other.
Scaling. We show encouraging results when scaling from S ED-S (150m) to S ED-L (420m). We
train both models on sequences of 256tokens and report a AR NLL of 4:20for S ED-S compared
to3:68for S ED-L. This improvement translates to improved sample quality, as is conﬁrmed by our
human preference scores, which are much higher for the larger model (63%, see Table 4).
8

--- PAGE 9 ---
Table 5: Ablation of the proposed S EDapproach on unconditional generation. Both self-
conditioning and embeddings pretraining play a key role in the model performance.
Diffusion space Self-conditioning Unigram entropy AR NLL
Bits (Chen et al., 2022) 7 6.97 7.01
3 7.47 6.05
Random embeddings 7 6.90 6.80
3 6.86 5.31
Pretrained embeddings 7 6.75 5.66
3 6.77 4.57
Data – 6:700:04 1 :810:15
Table 6: On unconditional generation, self-conditioning results in better sentence modelling, pre-
trained embeddings enhances topic modelling.
Random embeddings Random embeddings Pretrained embeddings
with self-conditioning with self-conditioning
didthebuild ingroom granted a
lighter distance. Onitthough,
salaries about clients thatachild,
which dispersed gluc somany
events andcertainly wanted the
Mother’s project, discovered by
their child would keepTree brings thesound, bearing
features andcapabilitiesthatwe
aresetin.Fortheﬁrst time, she
uses acustomiz able frame work
tousethatwehelp students pub-
licly solve theweather conditions
thatweonly offerstudentsalmost sixdecades ago-72per-
cent ofAmer icans didn’t feel
they’d actually rent their own
carsthisyear. Conversely, 90per-
cent ofAmer icans feel that the
decision torent acarissome -
thing they feelit’simpossible
5 L IMITATIONS
While our results are promising and show that continuous diffusion for text can be an exciting
alternative to AR models, the current approach does present some signiﬁcant limitations.
First, much more could be done in terms of model tuning, including scaling to much bigger models
to better understand S ED’s limits, and to be able to compare it with state-of-the-art AR models. Our
training regime in particular would certainly beneﬁt from more hyperparameter optimisation.
Second, one compelling reason we chose to explore continuous diffusion for text is to leverage the
improvements produced by the literature on image generation. While we have ported some (e.g. self-
conditioning), a lot more remains unexplored. The most obvious example is the sampling process
itself, where the number of required steps has been considerably reduced for images (e.g. Karras
et al. (2022) goes from 1000 to 35, and Salimans & Ho (2022) all the way down to 4 on simple
images). Our current sampling is very inefﬁcient, and this direction is one of the ﬁrst improvements
to make over S ED.
Third, S EDcrucially relies on diffusing in a pretrained embedding space. This means relying on
a second model, and using embeddings that may not be optimal for diffusion. Ideally, we’d train
the full model end-to-end, which could yield even better results. While Li et al. (2022) found some
success with this approach, it was in a speciﬁc setting at a small scale; in practice we found it difﬁcult
to avoid competition between the diffusion and reconstruction loss.
Finally, our work would beneﬁt from improved metrics in the experimental section. Because the
current state of the art involves AR models, the ﬁeld lacks established benchmarks for tasks diffusion
models are potentially better suited for, such as text in-ﬁlling. We opted for a reasonable mix,
evaluating the negative log-likelihood of generated samples according to a very strong AR model as
well as their token entropy and complementing it with a human evaluation. However, both NLL and
unigram entropy are gameable (e.g. AR models assign very low NLL to repetitive snippets, and long
enough repetitions can fool even entropy). Further, our NLL is inherently tied to its AR model and
could thus be providing an unfair advantage to AR models. All told, we still found both metrics quite
useful for measuring research progress, and our human evaluation conﬁrmed our results. Moving
forward, deﬁning a clean in-ﬁlling benchmark would help produce even more convincing results.
9

--- PAGE 10 ---
Table 7: Word embeddings with small di-
mension have higher AR likelihood.
Embed. dim. 16 32 64 128 256 896
AR NLL 4.65 4.57 4.71 4.61 4.77 4.92Table 8: Span masking tasks improves un-
conditional text generation.
Max span count 1 3 5 7 9 11
AR NLL 4.99 4.82 4.82 4.67 4.48 4.56
6 C ONCLUSION
We propose S ED, the ﬁrst generally-capable continuous diffusion model for text generation. S ED
models can perform both conditional and unconditional generation, and their performance rivals
AR models while being more ﬂexible in their use (e.g. enabling in-ﬁlling). We demonstrate their
performance and study the impact of the main design choices.
Despite its limitations, this work lays the foundation for more exciting research. Promising direc-
tions include speeding up the sampling following the lessons learnt in the image domain, devising
better embedding spaces for diffusion and investigating new in-ﬁlling capabilities.
REFERENCES
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured
denoising diffusion models in discrete state-spaces. In NeurIPS , pp. 17981–17993, 2021.
Yoshua Bengio, R ´ejean Ducharme, and Pascal Vincent. A neural probabilistic language model.
Advances in neural information processing systems , 13, 2000.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Char-
lin. Language gans falling short. arXiv preprint arXiv:1811.02549 , 2018.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative
image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 11315–11325, 2022.
Ting Chen, Ruixiang Zhang, and Geoffrey E. Hinton. Analog bits: Generating discrete data using
diffusion models with self-conditioning. CoRR , abs/2208.04202, 2022.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a ﬁxed-length context, 2019. URL https:
//arxiv.org/abs/1901.02860 .
Cyprien De Masson d’Autume, Shakir Mohamed, Mihaela Rosca, and Jack Rae. Training language
gans from scratch. Advances in Neural Information Processing Systems , 32, 2019.
Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In
NeurIPS , pp. 8780–8794, 2021.
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel
decoding of conditional masked language models. In EMNLP/IJCNLP (1) , pp. 6111–6120. As-
sociation for Computational Linguistics, 2019.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10696–10706, 2022.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016. URL https://
arxiv.org/abs/1606.08415 .
10

--- PAGE 11 ---
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS , pp.
6626–6637, 2017.
Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion guidance. CoRR , abs/2207.12598, 2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS ,
2020.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-
ing compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In ICLR . OpenReview.net, 2020.
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr ´e, and Max Welling. Argmax ﬂows
and multinomial diffusion: Learning categorical distributions. Advances in Neural Information
Processing Systems , 34:12454–12465, 2021.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. arXiv preprint arXiv:2206.00364 , 2022.
Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.
CoRR , abs/2107.00630, 2021.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. In EMNLP (Demonstration) , pp. 66–71.
Association for Computational Linguistics, 2018.
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto.
Diffusion-lm improves controllable text generation. CoRR , abs/2205.14217, 2022.
Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with
diffusion models. In ISMIR , pp. 468–475, 2021.
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
InICML , volume 139 of Proceedings of Machine Learning Research , pp. 8162–8171. PMLR,
2021.
Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob
McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and
editing with text-guided diffusion models. In ICML , volume 162 of Proceedings of Machine
Learning Research , pp. 16784–16804. PMLR, 2022.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res. , 21:140:1–140:67, 2020.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with CLIP latents. CoRR , abs/2204.06125, 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion models. CoRR , abs/2112.10752, 2021.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-
yar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sali-
mans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffu-
sion models with deep language understanding. CoRR , abs/2205.11487, 2022.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv
preprint arXiv:2202.00512 , 2022. URL https://arxiv.org/abs/2202.00512 .
Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and A ¨aron van den Oord.
Step-unrolled denoising autoencoders for text generation. In ICLR . OpenReview.net, 2022.
11

--- PAGE 12 ---
Stanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for lan-
guage generation. arXiv preprint arXiv:1806.04936 , 2018.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In ICML , volume 37 of JMLR Workshop
and Conference Proceedings , pp. 2256–2265. JMLR.org, 2015.
Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural net-
works. In ICML , 2011.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
A M ODEL ARCHITECTURE
For both the AR and the S EDmodels, we use the same transformer (Vaswani et al., 2017) archi-
tecture, which are similar to those described in (Hoffmann et al., 2022), with relative positional
encoding as described in (Dai et al., 2019) in the attention blocks, and with a 4x expansion and
a Gelu (Hendrycks & Gimpel, 2016) non-linearity in the feed-forward blocks. The architecture
hyper-parameters are detailed in table 9. Noised word embeddings, x2RND, are ﬁrst passed
through a linear projection that operates on each embedding independently to get a projected em-
bedding whose feature dimension matches the width of the transformer, dmodel. At diffusion step
t, we compute a time embedding as a sinusoidal position embedding (Vaswani et al., 2017) of size
dmodel, which is then passed into a dmodeldmodel linear layer and added to the projected embedding.
We add a linear output projection layer E0which takes the output of the transformer y2RNdmodel
and projects each element (yi)1iNback to the same size as the word embeddings. When using
self-conditionning, we modify the input to the model by concatenating xand^x0along the feature
axis before passing them to the input projection layer.
Table 9: Model hyper parameters.
Model number of layers dmodel number of heads head size
S 12 896 16 64
L 12 1536 16 128
B F ORWARD DIFFUSION PROCESS VISUALIZATION
To support the discussion on word embeddings dimension from Section 4.4, we present a visualiza-
tion of the forward diffusion process. Given starting tokens x0, we project the noised tokens xtof
the forward process at step tto their nearest neighbor among word embeddings Eto obtainwt. We
then store the 128 nearest neighbors N(w0)of starting tokens w0=x0and deﬁne the rank rtof
wtat its index inN(w0). We displaywtand highlight it in green if rtis close to zero (meaning wt
is a close neighbor of w0) and in increasingly red colors otherwise. We present the ﬁrst 16 nearest
neighbors ofw0in Figure 2 and provide an illustration of the color code used for highlighting. Fig-
ure 3 shows an instance of the forward diffusion process while diffusing on embeddings of with a
high dimension of 896 and Figure 4 shows diffusion on embeddings with a lower dimension of 32.
We observe that in high dimension, the noised token’s closest neighbour remains the original token
up until the point where any token could be its closest neighbour. The diffusion random walk does
not seem to pass through the neighbourhoods of semantically-related tokens. We hypothesize that
the root cause of this issue is that the embedding space is mostly empty (with only 32000 points in
R896, asdembed = 896 ); and that embeddings are potentially concentrating in a lower-dimensional
space.
In contrast, in lower dimension we see meaningfully-related tokens appear as the corruption pro-
gresses (‘brown’ becomes ‘grey’, ‘quick’ becomes ‘swift’, ‘over’ becomes ‘underneath’ etc). We
believe this more gradual information destruction is beneﬁcial for the diffusion model.
12

--- PAGE 13 ---
Figure 2: Nearest neighbors of tokens from the sentence ”the quick brown fox jumps over the lazy
dog”
Figure 3: Visualization of the forward diffusion process up to 300 steps when diffusing on embed-
dings with dimension 896.
C A DDITIONAL SAMPLES
13

--- PAGE 14 ---
Figure 4: Visualization of the forward diffusion process up to 300 steps when diffusing on embed-
dings with dimension 32.
Figure 5: Reverse diffusion process of S ED-L with guidance scale 2.5.
14

--- PAGE 15 ---
Table 10: Samples from S ED-L.
This course isessentialfortheenvironment inwhich students choose thecurriculum option
study atJHD. Geograph icalIntegrationisattheheart ofthisdepartment. Along with theur-
baninfrastructural innovations ofthemid-1990’s andsocialissues inthe21st Century. People
within thedepartment regularly exemplify theconcept ofrealintegration.
That didtriggersome reward ingwords orinsights tobeginprepar ingtheir kidfortheir future.
Luck ily,Mikaela hasbeen nice enough totoleratemyquestions about what parents candoto
help, even duringhersummervacation.
I went to college at Boston University. After getting my degree, I decided to make a change !
Ienrolled inoutdoor schools. Aftergettingmydegree, Iloved jetskiing,offshore ﬁshing,and
Kitesurf ing.The listbecame grow ing.More importantly, Istarted acareer byﬁshingatsea.
Now, I can’t get enough of the Paciﬁc Ocean!
The beaver is an interesting animal that lives in rivers and waterfalls across Puerto Rico. It
loves kayak ing,ﬁshing,andswim ming. But, youwant toknow what aretheanimals behind
thebeaver?
A year ago in Paris, Ihadtheopportunitytotake aﬁeld triptoLaRite-en-Laurences Interna-
tional deFrance where ImetDavid Nigel John son, aprofessorofsocialstudies.What agreat
tripand what a great day!
A year ago in Paris, myfriends andIwent onadirtroad triptothecity. Iremem berwalk ing
through thechurch, itsbeau tifulsquare, narrowstreets litwith memo rialsandpassingaterrible
Catholic Bishop Iamused to- what a sad day...
There was no evidence, only ﬂeeting glimpses oftheexistence ofcognitivedisability.There
wasnoluck andnosolid science. It was all guesswork ,theblind ingprospect ofpneu monia
could spur imag inationatthepossibilityofbrain damage.
15
