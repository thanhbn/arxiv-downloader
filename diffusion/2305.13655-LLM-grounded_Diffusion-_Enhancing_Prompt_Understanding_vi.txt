# 2305.13655.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2305.13655.pdf
# Kích thước tệp: 48099860 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khuếch tán nền tảng LLM: Nâng cao khả năng hiểu prompt 
của các mô hình khuếch tán văn bản-thành-hình ảnh với các mô hình ngôn ngữ lớn
Long Lian longlian@berkeley.edu
UC Berkeley
Boyi Li boyili@berkeley.edu
UC Berkeley
Adam Yala yala@berkeley.edu
UC Berkeley, UCSF
Trevor Darrell trevordarrell@berkeley.edu
UC Berkeley
Tóm tắt
Những tiến bộ gần đây trong các mô hình khuếch tán văn bản-thành-hình ảnh đã mang lại kết quả ấn tượng trong việc tạo ra những hình ảnh thực tế và đa dạng. Tuy nhiên, các mô hình này vẫn gặp khó khăn với các prompt phức tạp, chẳng hạn như những prompt liên quan đến khả năng tính toán và lý luận không gian. Nghiên cứu này đề xuất nâng cao khả năng hiểu prompt trong các mô hình khuếch tán. Phương pháp của chúng tôi tận dụng một mô hình ngôn ngữ lớn (LLM) được huấn luyện trước để tạo ra dữ liệu có nền tảng trong một quy trình hai giai đoạn mới. Trong giai đoạn đầu, LLM tạo ra một bố cục cảnh bao gồm các hộp giới hạn có chú thích từ một prompt đã cho mô tả hình ảnh mong muốn. Trong giai đoạn thứ hai, một bộ điều khiển mới hướng dẫn một mô hình khuếch tán có sẵn để tạo ra hình ảnh dựa trên bố cục. Cả hai giai đoạn đều sử dụng các mô hình được huấn luyện trước hiện có mà không cần tối ưu hóa thêm tham số mô hình. Phương pháp của chúng tôi vượt trội đáng kể so với mô hình khuếch tán cơ bản và một số baseline mạnh trong việc tạo ra hình ảnh chính xác theo các prompt yêu cầu nhiều khả năng khác nhau, tăng gấp đôi độ chính xác tạo ra trung bình trên bốn nhiệm vụ. Hơn nữa, phương pháp của chúng tôi cho phép đặc tả cảnh đa vòng dựa trên hướng dẫn và có thể xử lý các prompt bằng các ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán bên dưới. Chúng tôi dự đoán rằng phương pháp của chúng tôi sẽ giải phóng sự sáng tạo của người dùng bằng cách tuân theo chính xác những prompt phức tạp hơn. Mã nguồn, demo và benchmark của chúng tôi có sẵn tại: https://llm-grounded-diffusion.github.io .
1 Giới thiệu
Lĩnh vực tạo hình ảnh từ văn bản đã chứng kiến những tiến bộ đáng kể, đặc biệt là với sự xuất hiện của các mô hình khuếch tán. Những mô hình này đã thể hiện khả năng đáng chú ý trong việc tạo ra những hình ảnh thực tế và đa dạng phản hồi lại các prompt văn bản. Tuy nhiên, bất chấp những kết quả ấn tượng, các mô hình khuếch tán thường gặp khó khăn trong việc tuân theo chính xác các prompt phức tạp đòi hỏi khả năng cụ thể để hiểu. Hình 1 cho thấy rằng Stable Diffusion (Rombach et al., 2022), ngay cả phiên bản SDXL mới nhất (Podell et al., 2023), thường không thể tạo ra một số lượng đối tượng nhất định hoặc hiểu phủ định trong prompt. Nó cũng gặp khó khăn với lý luận không gian hoặc liên kết thuộc tính chính xác với các đối tượng.
Một giải pháp tiềm năng để giải quyết vấn đề này tất nhiên là thu thập một bộ dữ liệu đa phương thức toàn diện bao gồm các chú thích phức tạp và huấn luyện một mô hình khuếch tán văn bản-thành-hình ảnh để nâng cao khả năng hiểu prompt. Tuy nhiên, cách tiếp cận này có những nhược điểm đáng chú ý. Nó đòi hỏi thời gian và tài nguyên đáng kể để tuyển chọn

--- TRANG 2 ---
(a) Stable Diffusion XL(b) LMD (Của chúng tôi)Liên kết thuộc tínhPhủ địnhKhả năng tính toán sinh sảnMối quan hệ không gian… một bàn gỗ không có chuối
Một bức tranh màu nước … 3 con mèo trên cỏ… một người đàn ông mặc đỏ bên cạnh một người phụ nữ khác mặc xanh… 3 quả táo sắp xếp thành hình chữ L trên bàn gỗ
(a) Stable Diffusion XL(b) LMD (Của chúng tôi)

Hình 1: (a) Các mô hình khuếch tán văn bản-thành-hình ảnh như SDXL (Podell et al., 2023) thường gặp khó khăn trong việc tuân theo chính xác các prompt liên quan đến phủ định, khả năng tính toán, liên kết thuộc tính, hoặc mối quan hệ không gian. (b) Phương pháp LMD của chúng tôi đạt được khả năng hiểu prompt nâng cao và tuân theo chính xác các loại prompt này.

Bộ tạo bố cục LLM
Một bức ảnh thực tế của một con mèo xám và một con chó cam trên cỏMột bức ảnh thực tế của một cảnh ngoài trời có cỏcon mèo xámcon chó camcỏ

Stable Diffusion dựa trên bố cụcGiai đoạn 1Giai đoạn 2

Hình 2: LMD đề xuất của chúng tôi nâng cao khả năng hiểu prompt trong các mô hình khuếch tán văn bản-thành-hình ảnh thông qua một quy trình tạo hai giai đoạn mới: 1) Một bộ tạo bố cục LLM nhận một prompt từ người dùng và đưa ra một bố cục hình ảnh dưới dạng các hộp giới hạn có chú thích. 2) Một mô hình stable diffusion được hướng dẫn bởi bộ điều khiển dựa trên bố cục của chúng tôi tạo ra hình ảnh cuối cùng. Cả hai giai đoạn đều sử dụng các mô hình được huấn luyện trước đã đóng băng, điều này làm cho phương pháp của chúng tôi áp dụng được cho các LLM có sẵn và các mô hình khuếch tán khác mà không cần nền tảng trong mục tiêu huấn luyện của chúng.

một bộ dữ liệu đa phương thức đa dạng và chất lượng cao, chưa kể đến những thách thức liên quan đến việc huấn luyện hoặc tinh chỉnh một mô hình khuếch tán trên dữ liệu rộng lớn như vậy.
Ngược lại, chúng tôi đề xuất một phương pháp không cần huấn luyện mới trang bị cho mô hình khuếch tán một LLM cung cấp nền tảng để nâng cao khả năng hiểu prompt. Phương pháp LLM-grounded Diffusion (LMD) của chúng tôi bao gồm một quy trình tạo hai giai đoạn như được hiển thị trong Hình 2.
Trong giai đoạn đầu tiên của phương pháp chúng tôi, chúng tôi điều chỉnh một LLM để trở thành một bộ tạo bố cục dựa trên văn bản thông qua học trong ngữ cảnh. Với một prompt mô tả hình ảnh mong muốn, LLM tạo ra các bố cục cảnh dưới dạng các hộp giới hạn có chú thích, với một chú thích nền và một prompt phủ định cho những gì cần tránh trong quá trình tạo.
Trong giai đoạn thứ hai, chúng tôi giới thiệu một bộ điều khiển mới hướng dẫn một mô hình khuếch tán hiện có không có nền tảng trong mục tiêu huấn luyện của nó (ví dụ: Stable Diffusion) để tuân theo nền tảng bố cục được tạo ra trong giai đoạn đầu tiên. Trái ngược với các công trình trước đó và đương thời về kiểm soát vùng (ví dụ: Bar-Tal et al. (2023); Chen et al. (2023); Xie et al. (2023)) áp dụng kiểm soát ngữ nghĩa lên các vùng không gian nhất định, cách tiếp cận của chúng tôi cho phép kiểm soát chính xác các thể hiện đối tượng trong các vùng được chỉ định.
Đáng chú ý, cả hai giai đoạn đều sử dụng các mô hình được huấn luyện trước đã đóng băng có sẵn, làm cho phương pháp của chúng tôi áp dụng được cho các LLM và mô hình khuếch tán được huấn luyện độc lập mà không cần tối ưu hóa tham số LLM hoặc mô hình khuếch tán.
Ngoài khả năng hiểu prompt nâng cao, phương pháp của chúng tôi cũng tự nhiên cho phép đặc tả cảnh dựa trên hướng dẫn với nhiều vòng yêu cầu người dùng (Hình 3) và tạo hình ảnh từ các prompt bằng ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán cơ bản (Hình I.1) mà không cần huấn luyện thêm.
Được hiển thị trong Hình 1, LMD cung cấp một giải pháp thống nhất cho một số nhược điểm trong khả năng hiểu prompt cùng một lúc và cho phép tạo hình ảnh chính xác và chất lượng cao từ các prompt phức tạp. Chúng tôi chứng minh rằng một mô hình khuếch tán

--- TRANG 3 ---
Đặc tả cảnh dựa trên hướng dẫnLMD (Của chúng tôi)
LMD (Của chúng tôi)
Một bức ảnh thực tế của một cảnh với một chiếc xe hơi đỏ ở bên trái.
Yêu cầu ban đầu của người dùngLMD (Của chúng tôi)
LMD (Của chúng tôi)Thêm một chiếc xe hơi xanh ở bên phải
Yêu cầu tiếp theo 1Thay thế chiếc xe ở bên phải bằng một chiếc xe màu vàng
Yêu cầu tiếp theo 2Bỏ chiếc xe màu vàng. Thay vào đó, đặt hoa ở chỗ đó
Yêu cầu tiếp theo 3

Hình 3: LMD tự nhiên cho phép đặc tả cảnh đa vòng dựa trên hướng dẫn và có thể điều chỉnh các vòng tạo tiếp theo theo hướng dẫn tiếp theo và làm rõ của người dùng.

mô hình được nền tảng với các bố cục do LLM tạo ra vượt trội hơn mô hình khuếch tán cơ bản và một số baseline gần đây, tăng gấp đôi độ chính xác tạo trung bình trên bốn nhiệm vụ. Những đóng góp chính của chúng tôi bao gồm:
1. Chúng tôi đề xuất một pipeline tạo hai giai đoạn không cần huấn luyện giới thiệu LLM để cải thiện khả năng hiểu prompt của các mô hình khuếch tán văn bản-thành-hình ảnh.
2. Chúng tôi giới thiệu Stable Diffusion dựa trên bố cục, một bộ điều khiển mới điều hướng một mô hình khuếch tán có sẵn để tạo ra hình ảnh dựa trên bố cục hộp cấp thể hiện từ LLM.
3. LMD cho phép đặc tả cảnh dựa trên hướng dẫn và cho phép hỗ trợ ngôn ngữ rộng hơn trong các prompt.
4. Chúng tôi đề xuất một benchmark để đánh giá khả năng hiểu prompt của mô hình văn bản-thành-hình ảnh và chứng minh hiệu suất vượt trội của LMD so với các baseline gần đây.
Chúng tôi mong đợi LMD sẽ trao quyền cho người dùng kiểm soát chính xác hơn các mô hình khuếch tán văn bản-thành-hình ảnh. Mã nguồn, demo và benchmark của chúng tôi được công khai.
2 Công trình liên quan
Các mô hình khuếch tán văn bản-thành-hình ảnh. Việc tạo hình ảnh chất lượng cao từ mô tả văn bản với các mô hình khuếch tán đã trở nên phổ biến gần đây (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Podell et al., 2023). Bất chấp chất lượng hình ảnh ấn tượng, các mô hình này vẫn có xu hướng thể hiện hiệu suất không đạt yêu cầu khi đối mặt với các prompt phức tạp liên quan đến các kỹ năng như liên kết thuộc tính với đối tượng và lý luận không gian (Ramesh et al., 2022).
LLM cho grounding thị giác. Nhiều mô hình đa phương thức được hưởng lợi từ việc tích hợp LLM để grounding các mô hình thị giác. BLIP-2 (Li et al., 2023a) khởi tạo việc huấn luyện trước tầm nhìn-ngôn ngữ từ một bộ mã hóa hình ảnh đã đóng băng và một LLM. Flamingo (Alayrac et al., 2022) giải quyết các nhiệm vụ như trả lời câu hỏi thị giác few-shot và các nhiệm vụ tạo chú thích. Gupta et al. (2021) sử dụng Transformer (Vaswani et al., 2017) để dự đoán bố cục nhưng tập trung vào việc tạo bố cục cho một tập hợp đóng hạn chế các lớp đối tượng trong tập huấn luyện được chú thích và do đó không thể tạo ra bố cục cho các đối tượng không có trong tập huấn luyện. Wu et al. (2023) và Koh et al. (2023) cũng liên quan đến LLM trong việc tạo hình ảnh có điều kiện. Tuy nhiên, các phương pháp này vẫn dựa vào embeddings văn bản CLIP để truyền tải thông tin đến mô hình khuếch tán. Do đó, chúng thường thể hiện kiểm soát không đủ so với phương pháp của chúng tôi, yêu cầu LLM một cách rõ ràng lý luận về thành phần không gian của các đối tượng khác nhau và áp dụng kiểm soát không gian trực tiếp. Đồng thời với công trình của chúng tôi, LayoutGPT (Feng et al., 2023) đề xuất prompt một LLM để tạo bố cục trong cấu trúc CSS. Trong khi LayoutGPT phụ thuộc vào một bộ dữ liệu được chú thích với các hộp và chú thích để lấy các ví dụ trong ngữ cảnh có liên quan cho LLM, phương pháp của chúng tôi chứng minh rằng khả năng

--- TRANG 4 ---
Một bức ảnh thực tế của một con mèo xám và một con chó cam trên cỏMột bức ảnh thực tế của một cảnh ngoài trời có cỏcon mèo xámcon chó camcỏNhiệm vụ của bạn là tạo ra các hộp giới hạn cho các đối tượng được đề cập trong chú thích, cùng với một prompt nền mô tả cảnh… [Ví dụ trong ngữ cảnh] Chú thích: Một bức ảnh thực tế của một con mèo xám và một con chó cam trên cỏ Đối tượng: Nhúng Prompt vào TemplatechÚ thích: Một bức ảnh thực tế của một con mèo xám và một con chó cam trên cỏ Đối tượng: [('một con mèo xám', [50, 120, 180, 200]), ('một con chó cam', [300, 120, 180, 200]), ('cỏ', [0, 340, 512, 172])] Prompt nền: Một bức ảnh thực tế của một cảnh ngoài trời có cỏ Prompt phủ định:LLM
Hoàn thànhPhân tíchTemplate: Nhiệm vụ của bạn là tạo ra các hộp giới hạn…

Hình 4: Trong giai đoạn 1, LMD tạo ra một bố cục hình ảnh từ prompt người dùng. LMD nhúng prompt người dùng vào một template với hướng dẫn và ví dụ trong ngữ cảnh. Sau đó một LLM được truy vấn để hoàn thành. Cuối cùng, việc hoàn thành LLM được phân tích để có được một tập hợp các hộp giới hạn có chú thích, một chú thích nền và một prompt phủ định tùy chọn.

tạo ra các bố cục chất lượng cao đã có sẵn trong các trọng số LLM được huấn luyện trước và có thể được prompt với một tập hợp cố định các ví dụ trong ngữ cảnh mà không cần chú thích bên ngoài.
Các phương pháp tạo hình ảnh có điều kiện không gian. Những phương pháp này tạo ra hình ảnh dựa trên các tiên nghiệm đã cho như tư thế, bản đồ phân đoạn, nét vẽ và bố cục. Trước khi các mô hình khuếch tán trở nên phổ biến, SPADE (Park et al., 2019), BlobGAN (Epstein et al., 2022) và Layout2Im (Zhao et al., 2019) tổng hợp các hình ảnh photorealistic từ một bố cục đã cho. Xu et al. (2017); Johnson et al. (2018); Herzig et al. (2020) tạo hình ảnh với đồ thị cảnh. ControlNet (Zhang & Agrawala, 2023), SpaText (Avrahami et al., 2023), LayoutDiffuse (Cheng et al., 2023), LayoutDiffusion, (Zheng et al., 2023), GLIGEN (Li et al., 2023b) và ReCo (Yang et al., 2023) đề xuất điều chỉnh dựa trên huấn luyện trên các mô hình khuếch tán để tạo hình ảnh có điều kiện không gian, với Li et al. (2023b) và Yang et al. (2023) hỗ trợ nhãn từ vựng mở cho các hộp bố cục. Tuy nhiên, các phương pháp này dựa vào các bộ dữ liệu bên ngoài được chú thích như COCO (Lin et al., 2014) để cung cấp hình ảnh với các chú thích như hộp và chú thích. Hơn nữa, việc điều chỉnh dựa trên huấn luyện làm cho mô hình không tương thích với các tiện ích bổ sung như trọng số LoRA (Hu et al., 2021) và khiến việc huấn luyện một mô hình LoRA mới từ tập huấn luyện không có chú thích hộp trở nên khó khăn. Ngược lại, chúng tôi đề xuất một bộ điều khiển tạo không cần huấn luyện điều hướng các mô hình khuếch tán văn bản-thành-hình ảnh hiện có không được huấn luyện cụ thể cho việc tạo hình ảnh dựa trên bố cục và không yêu cầu bộ dữ liệu bên ngoài. Hơn nữa, phương pháp của chúng tôi cũng có thể tích hợp với các phương pháp dựa trên huấn luyện để cải thiện thêm.
Gần đây, Bar-Tal et al. (2023); Chen et al. (2023); Xie et al. (2023) cho phép kiểm soát vùng không cần huấn luyện trong việc tạo hình ảnh và chia sẻ một công thức nhiệm vụ tương tự với giai đoạn bố cục-thành-hình ảnh của chúng tôi. Tuy nhiên, các công trình này làm nền tảng cho việc tạo hình ảnh trên ngữ nghĩa vùng và kiểm soát ít các thể hiện đối tượng bên trong mỗi vùng ngữ nghĩa, trong khi phương pháp của chúng tôi tập trung vào việc làm nền tảng cho việc tạo trên các thể hiện.
Tương tự như đặc tả cảnh dựa trên hướng dẫn của chúng tôi, Brooks et al. (2023) gần đây đề xuất chỉnh sửa hình ảnh dựa trên hướng dẫn. Wu et al. (2023) và Gupta & Kembhavi (2023) cũng cho phép sử dụng các mô hình chỉnh sửa hình ảnh bên ngoài trong một cuộc đối thoại được điều khiển bởi LLM. Khác với các phương pháp này, chúng tôi nhằm chỉnh sửa bố cục cảnh thay vì pixel hình ảnh, điều này dễ dàng cho phép hỗ trợ một tập hợp lớn hơn các hướng dẫn như hoán đổi/di chuyển đối tượng.
3 Khuếch tán nền tảng LLM
Trong phần này, chúng tôi giới thiệu phương pháp LLM-grounded Diffusion (LMD) của chúng tôi. LMD tập trung vào thiết lập tạo văn bản-thành-hình ảnh, bao gồm việc tạo hình ảnh x0 cho trước prompt văn bản y. Phương pháp của chúng tôi tạo ra một hình ảnh trong hai giai đoạn: tạo bố cục dựa trên văn bản (Phần 3.1) và tạo hình ảnh dựa trên bố cục (Phần 3.2).
Giai đoạn bố cục-thành-hình ảnh của phương pháp LMD chúng tôi được xây dựng dựa trên khung khuếch tán tiềm ẩn (Rombach et al., 2022), mà chúng tôi giới thiệu độc giả tham khảo Phụ lục A để biết các điều kiện tiên quyết.
3.1 Tạo bố cục dựa trên LLM
Để tạo ra bố cục của một hình ảnh, phương pháp của chúng tôi nhúng prompt văn bản đầu vào y vào một template và truy vấn một LLM để hoàn thành (Hình 4).
Biểu diễn bố cục. Biểu diễn bố cục của LMD bao gồm hai thành phần: 1) một hộp giới hạn có chú thích cho mỗi đối tượng nền trước, với tọa độ được chỉ định theo định dạng (x, y, width, height), và 2) một chú thích đơn giản

--- TRANG 5 ---
và ngắn gọn mô tả nền hình ảnh cùng với một prompt phủ định tùy chọn chỉ ra những gì không nên xuất hiện trong hình ảnh được tạo ra. Prompt phủ định là một chuỗi rỗng khi bố cục không áp đặt hạn chế về những gì không nên xuất hiện.
Hướng dẫn. Hướng dẫn văn bản của chúng tôi cho LLM bao gồm hai phần:
1. Đặc tả nhiệm vụ:
Nhiệm vụ của bạn là tạo ra các hộp giới hạn cho các đối tượng được đề cập trong chú thích, cùng với một prompt nền mô tả cảnh.
2. Chi tiết hỗ trợ:
Các hình ảnh có kích thước 512 × 512... Mỗi hộp giới hạn nên ở định dạng của... Nếu cần, bạn có thể đưa ra những phỏng đoán hợp lý.
Học trong ngữ cảnh. Tương tự như Brooks et al. (2023), chúng tôi cung cấp cho LLM các ví dụ được tuyển chọn thủ công sau mô tả nhiệm vụ. Thông qua những ví dụ này, chúng tôi làm rõ biểu diễn bố cục và cung cấp sở thích để phân tán sự mơ hồ. Một ví dụ được hiển thị như sau:
Chú thích: Một bức tranh màu nước của một bàn gỗ trong phòng khách với một quả táo trên đó
Đối tượng: [('một bàn gỗ', [65, 243, 344, 206]), ('một quả táo', [206, 306, 81, 69])]
Prompt nền: Một bức tranh màu nước của một phòng khách
Prompt phủ định:
Để đảm bảo kiểm soát bố cục chính xác, chúng tôi tuân thủ hai nguyên tắc chính trong thiết kế ví dụ: 1) Mỗi thể hiện đối tượng được đại diện bởi một hộp giới hạn duy nhất. Ví dụ, nếu prompt đề cập đến bốn quả táo, chúng tôi bao gồm bốn hộp với "một quả táo" trong mỗi chú thích. 2) Chúng tôi không để lại đối tượng nền trước nào được chỉ định trong các hộp cho chú thích nền để đảm bảo tất cả các đối tượng nền trước được kiểm soát bởi bộ tạo hình ảnh dựa trên bố cục của chúng tôi (Phần 3.2). Những nguyên tắc này cho phép tạo bố cục chính xác và được kiểm soát bởi thể hiện.
Hoàn thành LLM. Sau khi cung cấp các ví dụ trong ngữ cảnh, chúng tôi truy vấn LLM để hoàn thành:
Chú thích: [prompt đầu vào từ người dùng]
Đối tượng: [bắt đầu hoàn thành LLM]
Bố cục kết quả từ việc hoàn thành LLM sau đó được phân tích và sử dụng cho quá trình tạo hình ảnh tiếp theo. Chúng tôi giới thiệu độc giả tham khảo Phụ lục K để biết prompt đầy đủ của chúng tôi.
3.2 Stable Diffusion dựa trên bố cục
Trong giai đoạn này, chúng tôi giới thiệu một bộ điều khiển để làm nền tảng cho việc tạo hình ảnh trên bố cục do LLM tạo ra. Trong khi các phương pháp kiểm soát vùng không cần huấn luyện trước đó (Bar-Tal et al., 2023; Chen et al., 2023; Xie et al., 2023) áp dụng hướng dẫn ngữ nghĩa thông qua khử nhiễu vùng hoặc thao tác attention, các phương pháp này thiếu khả năng kiểm soát số lượng đối tượng trong một vùng ngữ nghĩa. Hạn chế này phát sinh vì các thể hiện khác nhau thường không thể phân biệt được trong không gian tiềm ẩn hoặc bản đồ attention, cản trở việc kiểm soát cấp thể hiện.
Ngược lại, LMD cho phép grounding cấp thể hiện bằng cách đầu tiên tạo ra các latent có mặt nạ cho mỗi hộp giới hạn riêng lẻ và sau đó kết hợp các latent có mặt nạ như các tiên nghiệm để hướng dẫn việc tạo hình ảnh tổng thể. Điều này cho phép vị trí chính xác và liên kết thuộc tính cho mỗi thể hiện đối tượng.
Latent có mặt nạ cho mỗi hộp. Trong khi các mô hình khuếch tán thiếu sự phân biệt cấp thể hiện cố hữu trong không gian tiềm ẩn hoặc bản đồ attention của chúng để kiểm soát tinh vi, chúng tôi quan sát thấy rằng chúng thường có thể tạo ra hình ảnh với một thể hiện được chỉ định. Do đó, chúng tôi xử lý một hộp nền trước tại một thời điểm cho grounding cấp thể hiện.
Như được mô tả trong Hình 5(a), đối với mỗi đối tượng nền trước i, chúng tôi đầu tiên tạo ra một hình ảnh với một thể hiện duy nhất bằng cách khử nhiễu từ z(i)T đến z(i)0, trong đó z(i)t đề cập đến latent của đối tượng i tại bước thời gian khử nhiễu t.1 Trong quá trình khử nhiễu này, chúng tôi sử dụng "[prompt nền] with [chú thích hộp]" (ví dụ: "một hình ảnh thực tế của một cảnh trong nhà with một con mèo xám") làm prompt văn bản để khử nhiễu. Latent nhiễu ban đầu được chia sẻ cho tất cả các hộp để đảm bảo quan điểm, phong cách và ánh sáng nhất quán toàn cục (tức là z(i)T = zT, ∀i).
1 Chúng tôi giới thiệu độc giả tham khảo Phụ lục A để biết định nghĩa các thuật ngữ, như latent, được giới thiệu trong khung khuếch tán tiềm ẩn.

--- TRANG 6 ---
(a) Bước 1: tạo latent có mặt nạ cho mỗi hộpPrompt cho mỗi hộp:
Một bức ảnh thực tế… với một con mèo xámBản đồ cross-attention của token mèoMasking tùy chọn SAM Reﬁnement…Latent có mặt nạ cho mỗi hộpHộp cho "một con mèo xám"
(b) Bước 2: tạo hình ảnh tổng thể với latent có mặt nạ như tiên nghiệmKết hợpDecode pixel từ latentHình ảnh được tạo ra
……x(cat)Tx(cat)0Khuyến khích/không khuyến khích tham dự "với một con mèo xám"
…
Latent có mặt nạ cho mỗi hộp…
Kết hợp…
Chuyển giao attentionBản đồ cross-attention cho mỗi hộp…Latent có mặt nạ cho "một con mèo xám"Cross-attention cho mỗi hộp
Mặt nạ nền trướĉx(cat)T̂x(cat)rT̂x(dog)T̂x(dog)rTA(cat)Bản đồ
A(cat)A(dog)x(comp)Tx(comp)rTx(cat)rT
……
…̂x(cat)T̂x(cat)rT̂x(cat)0
x(comp)0Kiểm soát attention:Khử nhiễu LatentKhử nhiễu Latent
Khử nhiễu LatentKhử nhiễu LatentStable DiﬀusionStable Diﬀusion

Hình 5: Trong giai đoạn 2, chúng tôi giới thiệu một bộ điều khiển dựa trên bố cục mới hướng dẫn stable diffusion để tạo ra hình ảnh dựa trên bố cục thu được từ giai đoạn trước. Quá trình tạo hình ảnh dựa trên bố cục của chúng tôi bao gồm hai bước: (a) tạo latent có mặt nạ cho mỗi hộp được chỉ định trong bố cục, với kiểm soát attention đảm bảo rằng đối tượng được đặt trong hộp được chỉ định; và (b) kết hợp các latent có mặt nạ như tiên nghiệm để hướng dẫn việc tạo hình ảnh tuân thủ bố cục được chỉ định.

Để đảm bảo đối tượng phù hợp với hộp giới hạn, chúng tôi thao tác các bản đồ cross-attention A(i) của mạng dự đoán nhiễu.2 Mỗi bản đồ mô tả ái lực từ pixel đến token văn bản:
A(i)uv = Softmax(qTuKv)  (1)
trong đó qu và kv là đặc trưng hình ảnh được biến đổi tuyến tính tại vị trí không gian u và đặc trưng văn bản tại chỉ số token v trong prompt, tương ứng.
Theo Chen et al. (2023); Xie et al. (2023), chúng tôi tăng cường cross-attention từ pixel bên trong hộp đến token liên quan đến chú thích hộp trong khi làm suy yếu cross-attention từ pixel bên ngoài hộp.
Để đạt được điều này, chúng tôi định nghĩa một hàm năng lượng đơn giản:
E(A(i), i, v) = -Topku(Auv · b(i)) + ωTopku(Auv · (1-b(i)))  (2)
trong đó · là phép nhân theo từng phần tử, b(i) là một mặt nạ nhị phân hình chữ nhật của hộp i với vùng trong hộp được đặt thành 1, Topku lấy trung bình của các giá trị top-k theo chiều không gian u, và ω = 4.0. Hàm năng lượng được tối thiểu hóa bằng cách cập nhật latent trước mỗi bước khử nhiễu:
z(i)t ← z(i)t - η∇z(i)t Σv∈Vi E(A(i), i, v)  (3)
z(i)t-1 ← Denoise(z(i)t)  (4)
trong đó η là cường độ hướng dẫn; tập hợp Vi chứa các chỉ số token cho chú thích hộp trong prompt cho hộp i (ví dụ: trong khi tạo latent có mặt nạ cho hộp i với chú thích "một con mèo xám", Vi chỉ ra các chỉ số của token tương ứng với chú thích hộp trong prompt khử nhiễu văn bản cho mỗi hộp "[prompt nền] with một con mèo xám"). Denoise(·) biểu thị một bước khử nhiễu trong khung khuếch tán tiềm ẩn.
2 Chỉ số lớp cross-attention được bỏ qua để đơn giản. Chúng tôi tổng các giá trị năng lượng cho tất cả các lớp được chọn trong quá trình tối ưu hóa.

--- TRANG 7 ---
(a) Một góc nhìn từ trên xuống của một bàn gỗ trống(b) Đặt một quả táo ở bên trái(c) Đặt một quả lê ở bên phải(d) Làm cho cả hai quả to hơn(e) Hoán đổi quả táo và quả lê(f) Bỏ quả lê(g) Di chuyển quả táo ra giữa(h) Làm cho quả táo có màu xanh lá(i) Thôi, làm cho quả táo có màu đỏ lại

Hình 6: LMD và LMD+ hỗ trợ đặc tả cảnh dựa trên hướng dẫn, trao quyền cho người dùng thêm/di chuyển/xóa đối tượng, sửa đổi thuộc tính đối tượng và làm rõ prompt trong nhiều vòng đối thoại. (a): prompt ban đầu cho cảnh; (b)-(i): tám hướng dẫn tiếp theo sửa đổi cảnh một cách tuần tự. Bằng cách tách việc tạo ra mỗi đối tượng nền trước cũng như nền, LMD đảm bảo việc tạo hình ảnh nhất quán khi cùng một seed được sử dụng để tạo hình ảnh trong suốt cuộc đối thoại.

Sau khi tạo, chúng tôi thu được bản đồ cross-attention tương ứng với chú thích hộp, phục vụ như một mặt nạ saliency cho đối tượng. Chúng tôi tùy chọn sử dụng SAM (Kirillov et al., 2023) để tinh chỉnh chất lượng mặt nạ. Điều này có thể được thực hiện bằng cách truy vấn với vị trí pixel có saliency cao nhất hoặc với hộp bố cục. Chức năng của SAM cũng có thể được thay thế bằng một ngưỡng đơn giản, như được thử nghiệm trong Phần 4.3. Với mặt nạ được tinh chỉnh cho chính xác một thể hiện nền trước, được ký hiệu là m(i), chúng tôi thực hiện phép nhân theo từng phần tử giữa mặt nạ và latent tại mỗi bước khử nhiễu để tạo ra một chuỗi latent thể hiện có mặt nạ (ẑ(i)t)Tt=0:
ẑ(i)t = z(i)t ⊗ m(i)  (5)
Latent có mặt nạ như tiên nghiệm cho kiểm soát cấp thể hiện. Các latent thể hiện có mặt nạ (ẑ(i)t)Tt=0 sau đó được tận dụng để cung cấp gợi ý cấp thể hiện cho mô hình khuếch tán để tạo hình ảnh tổng thể. Như được minh họa trong Hình 5(b), trong mỗi bước thời gian khử nhiễu trong quá trình khử nhiễu sớm, chúng tôi đặt mỗi latent nền trước có mặt nạ ẑ(i)t lên latent kết hợp z(comp)t:
z(comp)t ← LatentCompose(z(comp)t, ẑ(i)t, m(i)) ∀i  (6)
trong đó z(comp)T được khởi tạo từ zT để tạo nền trước cho tính nhất quán, và
LatentCompose(z(comp)t, ẑ(i)t, m(i)) đơn giản là đặt latent nền trước có mặt nạ ẑ(i)t lên vị trí tương ứng trên z(comp)t.
Vì các mô hình khuếch tán có xu hướng tạo ra vị trí đối tượng trong các bước khử nhiễu ban đầu và sau đó chi tiết đối tượng trong các bước sau (Bar-Tal et al., 2023), chúng tôi chỉ kết hợp latent từ timestep T đến rT3, trong đó r ∈ [0,1] cân bằng kiểm soát thể hiện và sự mạch lạc hình ảnh. Bằng cách chủ yếu can thiệp trong các bước cho việc đặt đối tượng, phương pháp của chúng tôi chỉ cung cấp gợi ý bố cục cấp thể hiện thay vì buộc mỗi vùng có mặt nạ của việc tạo kết quả trông giống như việc tạo cho mỗi hộp.
Để làm cho hướng dẫn của chúng tôi mạnh mẽ hơn, chúng tôi tiếp tục chuyển giao các bản đồ cross-attention từ việc tạo cho mỗi hộp đến các vùng tương ứng trong việc tạo kết hợp bằng cách điều chỉnh hàm năng lượng:
E(comp)(A(comp), A(i), i, v) = E(A(comp), i, v) + λ Σu∈V'i |A(comp)uv - A(i)uv|  (7)
trong đó λ = 2.0 và giá trị năng lượng của mỗi hộp i được tổng lại để tối ưu hóa. V'i biểu thị các chỉ số của token tương ứng với chú thích hộp trong prompt văn bản cho quá trình khử nhiễu tổng thể, tương tự như định nghĩa của Vi trong Phương trình (3).
Theo cách này, bộ điều khiển của chúng tôi điều kiện mô hình khuếch tán để tạo ra một thể hiện tại mỗi vị trí có mặt nạ, với việc tạo cuối cùng tự nhiên và mạch lạc về thành phần nền trước-nền.
Cuối cùng, chúng tôi decode latent z(comp)0 thành pixel x0 qua bộ giải mã hình ảnh khuếch tán. Chúng tôi giới thiệu độc giả tham khảo Phụ lục B để biết pseudo-code tổng thể cho grounding bố cục.
Tích hợp với các phương pháp dựa trên huấn luyện. Bộ điều khiển không cần huấn luyện của chúng tôi cũng có thể được áp dụng cùng với các phương pháp dựa trên huấn luyện như GLIGEN (Li et al., 2023b) để tận dụng các bộ dữ liệu bên ngoài có chú thích thể hiện khi có sẵn. Vì GLIGEN huấn luyện các lớp adapter nhận đầu vào hộp, việc tích hợp với GLIGEN, được ký hiệu là LMD+, bao gồm việc áp dụng trọng số adapter của nó và truyền hướng dẫn bố cục đến các lớp adapter.3 Lưu ý rằng LMD+ sử dụng adapter cùng với hướng dẫn cấp thể hiện được giới thiệu ở trên, vượt trội hơn rất nhiều so với việc chỉ sử dụng adapter GLIGEN, như được hiển thị trong Bảng 2. Chúng tôi đạt được kiểm soát thể hiện và thuộc tính nâng cao hơn nữa mà không cần huấn luyện thêm thông qua việc tích hợp này.
3.3 Khả năng bổ sung của LMD
Pipeline tạo dựa trên LLM của chúng tôi cho phép hai khả năng bổ sung mà không cần huấn luyện thêm.
Đặc tả cảnh dựa trên hướng dẫn. Tận dụng một LLM hỗ trợ đối thoại đa vòng (ví dụ: GPT-3.5/4), LMD trao quyền cho người dùng chỉ định hình ảnh mong muốn với nhiều hướng dẫn theo sau một prompt ban đầu (Hình 3). Cụ thể, sau khi tạo hình ảnh ban đầu, người dùng có thể đơn giản đưa ra làm rõ hoặc yêu cầu bổ sung cho LLM. Với bố cục được cập nhật từ LLM, chúng tôi có thể tận dụng LMD lại để tạo ra hình ảnh với bố cục được cập nhật. Cập nhật bố cục thay vì hình ảnh thô mang lại cho LMD một số lợi thế, như được chứng minh trong Hình 6: 1) Việc tạo của chúng tôi vẫn nhất quán sau nhiều vòng yêu cầu thay vì dần dần trôi khỏi hình ảnh ban đầu. 2) LMD có thể xử lý các yêu cầu liên quan đến lý luận không gian, đó là hạn chế của phương pháp chỉnh sửa hình ảnh dựa trên hướng dẫn trước đó Brooks et al. (2023).
Ngược lại, chúng tôi chứng minh rằng VisualChatGPT Wu et al. (2023), trang bị ChatGPT với các công cụ như Brooks et al. (2023), không thể tuân theo các hướng dẫn trong Hình 6, đặc biệt là đối với các hướng dẫn không gian qua nhiều lần lặp đối thoại. Chúng tôi giới thiệu độc giả quan tâm tham khảo Phụ lục G để so sánh. Khả năng này áp dụng cho cả LMD và LMD+. Chúng tôi cũng hiển thị các trường hợp sử dụng bổ sung trong Hình C.1 trong Phụ lục C. LMD của chúng tôi có thể xử lý các yêu cầu điều chỉnh cảnh mở, đưa ra đề xuất cho cảnh hiện tại, hiểu yêu cầu người dùng trong ngữ cảnh đối thoại và cho phép người dùng thử nghiệm các điều chỉnh chi tiết khác nhau trong khi bảo tồn phong cách hình ảnh và bố cục tổng thể, tạo điều kiện cho việc tạo nội dung tinh vi.
Hỗ trợ nhiều ngôn ngữ hơn. Bằng cách đưa ra một ví dụ trong nội dung của một prompt người dùng không phải tiếng Anh và một đầu ra bố cục tiếng Anh4, bộ tạo bố cục LLM chấp nhận các prompt người dùng không phải tiếng Anh và đưa ra bố cục với chú thích tiếng Anh. Điều này cho phép tạo ra từ các prompt bằng ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán bên dưới mà không cần huấn luyện thêm (Hình I.1). Chúng tôi giới thiệu độc giả tham khảo Phụ lục I để biết thêm chi tiết.
4 Đánh giá
4.1 So sánh định tính
Thiết lập. Chúng tôi so sánh định tính cách tiếp cận của chúng tôi với Stable Diffusion (SD, Rombach et al. (2022); Podell et al. (2023)). Họ SD cũng được chọn làm mô hình cơ bản bên dưới của chúng tôi cho việc tạo hình ảnh dựa trên bố cục với khả năng mạnh mẽ và việc áp dụng rộng rãi trong nghiên cứu tạo văn bản-thành-hình ảnh. Nhờ vào tính chất không cần huấn luyện của công trình chúng tôi, phương pháp của chúng tôi áp dụng được cho các mô hình khuếch tán khác nhau mà không cần huấn luyện thêm. Do đó, đối với Hình 1, 7 và 9, chúng tôi sử dụng mô hình Stable Diffusion lớn nhất SDXL làm mô hình cơ bản của LMD và so sánh với SDXL như một baseline (xem Phụ lục H để biết chi tiết). Đối với tất cả các thiết lập khác, chúng tôi sử dụng

--- TRANG 8 ---
VisualChatGPT (Baseline)GILL (Baseline)Liên kết thuộc tínhPhủ địnhKhả năng tính toán sinh sảnMối quan hệ không gian… một bàn gỗ không có chuối
Một bức tranh màu nước … 3 con mèo trên cỏ… một người đàn ông mặc đỏ đứng bên cạnh một người phụ nữ khác mặc xanh… 3 quả táo sắp xếp thành hình chữ L trên bàn gỗ
VisualChatGPT (Baseline)GILL (Baseline)Đúng?
❌✓Đúng?
❌
❌

Hình 8: Chúng tôi so sánh định tính với VisualChatGPT (Wu et al., 2023) và GILL (Koh et al., 2023) cũng tận dụng LLM trong các pipeline tạo hình ảnh. Cả hai baseline đều thiếu khả năng tuân theo chính xác các prompt cho ba trong số bốn nhiệm vụ mà phương pháp của chúng tôi có thể giải quyết trong Hình 1 và F.1.

Độ chính xác
Nhiệm vụ SD LMD LMD+
Phủ định 28%100% (3.6×)100% (3.6×)
Khả năng tính toán sinh sản 39%62% (1.6×)86% (2.2×)
Liên kết thuộc tính 52%65% (1.3×)69% (1.3×)
Mối quan hệ không gian 28%79% (2.8×)67% (2.4×)
Trung bình 37%77% (2.1×)81% (2.2×)

Bảng 1: Với sự hướng dẫn từ bộ tạo bố cục dựa trên LLM và bộ điều khiển dựa trên bố cục mới của chúng tôi, LMD của chúng tôi vượt trội đáng kể so với mô hình Stable Diffusion (SD) mà chúng tôi sử dụng bên dưới trong bốn nhiệm vụ đánh giá khả năng tuân theo prompt. LMD biểu thị phương pháp của chúng tôi được áp dụng trực tiếp trên SD. LMD+ biểu thị việc tích hợp thêm adapter GLIGEN được huấn luyện trước (Li et al., 2023b) vào bộ điều khiển của chúng tôi.

Stable Diffusion v1.5 làm mô hình cơ bản trừ khi được nêu khác. Chúng tôi sử dụng gpt-4 (OpenAI, 2023) để tạo bố cục cho tất cả các so sánh định tính. Kết quả. Trong Hình 1 và 7, chúng tôi quan sát thấy rằng cách tiếp cận tạo văn bản-thành-hình ảnh hai giai đoạn của chúng tôi nâng cao đáng kể khả năng tuân theo prompt so với mô hình cơ bản của chúng tôi bằng cách tạo ra hình ảnh phù hợp với bố cục từ LLM.
So sánh với các bộ tạo hình ảnh dựa trên LLM khác. VisualChatGPT (Wu et al., 2023) và GILL (Koh et al., 2023) cũng tận dụng LLM như một phần của pipeline tạo hình ảnh. Cả hai công trình đều tận dụng SD làm mô hình tạo hình ảnh bên dưới. VisualChatGPT xử lý SD như một mô-đun có thể được sử dụng bởi LLM và truyền chú thích văn bản cho nó, và GILL đưa ra một embedding thay cho text embedding cho SD. Vì cả hai phương pháp đều sử dụng LLM để chỉ cung cấp điều kiện cho SD dưới dạng text embeddings, các phương pháp này vẫn kế thừa các vấn đề kiểm soát không đủ của text embeddings từ mô hình SD cơ bản. Ngược lại, phương pháp của chúng tôi yêu cầu LLM một cách rõ ràng lý luận về mối quan hệ không gian và áp dụng kiểm soát không gian trực tiếp trên mô hình khuếch tán bên dưới của chúng tôi, do đó bỏ qua nút thắt cổ chai của biểu diễn text embedding không truyền tải chính xác thông tin không gian. Như được hiển thị trong Hình 8, cả hai phương pháp đều không tuân theo chính xác các prompt văn bản của một số danh mục mà phương pháp của chúng tôi có thể tạo ra chính xác trong Hình 1 và Hình F.1 trong Phụ lục F. Hơn nữa, mặc dù sự tham gia của LLM trong VisualChatGPT và GILL cũng có thể cho phép đặc tả cảnh dựa trên hướng dẫn đa vòng (Phần 3.3), chúng tôi quan sát thực nghiệm rằng các hình ảnh được tạo ra nhanh chóng chệch khỏi cảnh "một bàn gỗ" bắt đầu từ lần lặp thứ hai trong Hình G.1 trong Phụ lục G, với việc tạo cuối cùng không thể hiểu được.

--- TRANG 9 ---
Độ chính xác
Giai đoạn 1/Giai đoạn 2 Phủ định Khả năng tính toán Thuộc tính Không gian Trung bình
Phương pháp không cần huấn luyện:
LMD/MultiDiffusion (Bar-Tal et al., 2023) 100% 30% 42% 36% 52.0%
LMD/Backward Guidance (Chen et al., 2023) 100% 42% 36% 61% 59.8%
LMD/BoxDiff (Xie et al., 2023) 100% 32% 55% 62% 62.3%
LMD/LMD (Của chúng tôi) 100% 62% 65% 79%76.5% (+ 14.2)
Phương pháp dựa trên huấn luyện:
LMD/GLIGEN (Li et al., 2023b) 100% 57% 57% 45% 64.8%
LMD/LMD+ (Của chúng tôi) 100% 86% 69% 67%80.5% (+ 15.7)
LMD/LMD+ (Của chúng tôi, GPT-4) 100% 84% 79% 82%86.3% (+ 21.5)
Đánh giá chỉ bố cục được tạo ra (giới hạn trên cho việc tạo hình ảnh):
LMD/- 100% 97% 100% 99% 99.0%

Bảng 2: Thử nghiệm về các phương pháp bố cục-thành-hình ảnh như giai đoạn 2 với bộ tạo bố cục LLM của chúng tôi như giai đoạn 1. Bộ điều khiển dựa trên bố cục đề xuất của chúng tôi hoạt động tốt nhất trong số chúng. Bộ điều khiển của chúng tôi cũng có thể được áp dụng lên GLIGEN dựa trên huấn luyện (Li et al., 2023b), được ký hiệu là LMD+, để cải thiện thêm. Cuối cùng, các bố cục do LLM tạo ra hầu như luôn phù hợp với prompt, nhấn mạnh rằng nút thắt cổ chai là việc tạo hình ảnh dựa trên bố cục. Điểm số cho nhiệm vụ phủ định cao vì chúng tôi truyền các prompt phủ định được tạo ra bởi LLM đến mô hình khuếch tán bên dưới, điều này không phụ thuộc vào việc triển khai giai đoạn 2.

4.2 Đánh giá định lượng
4.2.1 Benchmark đề xuất
Chúng tôi đề xuất một benchmark đánh giá văn bản-thành-hình ảnh bao gồm bốn nhiệm vụ: phủ định, khả năng tính toán sinh sản, liên kết thuộc tính và lý luận không gian. Phủ định và khả năng tính toán sinh sản liên quan đến việc tạo ra một số lượng cụ thể các đối tượng hoặc không tạo ra các đối tượng cụ thể. Liên kết thuộc tính liên quan đến việc gán thuộc tính đúng cho đối tượng đúng với nhiều đối tượng trong prompt. Lý luận không gian liên quan đến việc hiểu các từ mô tả vị trí tương đối của các đối tượng. Đối với mỗi nhiệm vụ, chúng tôi lập trình tạo ra 100 prompt và truy vấn mỗi mô hình để tạo văn bản-thành-hình ảnh, với tổng cộng 400 prompt. gpt-3.5-turbo (Brown et al., 2020) được sử dụng trong LMD cho các benchmark. Chúng tôi cũng triển khai LMD+, một biến thể LMD tích hợp adapter GLIGEN được huấn luyện trước (Li et al., 2023b) vào bộ điều khiển của chúng tôi mà không cần huấn luyện thêm. Chúng tôi giới thiệu độc giả tham khảo Phụ lục J để biết chi tiết.
Đánh giá dựa trên phát hiện. Chúng tôi sử dụng một bộ phát hiện đối tượng từ vựng mở, OWL-ViT (Minderer et al., 2022), để thu được các hộp giới hạn cho các đối tượng quan tâm. Sau đó chúng tôi kiểm tra xem mỗi hình ảnh được tạo ra có thỏa mãn các yêu cầu trong prompt hay không. Độ chính xác của mỗi nhiệm vụ được tính bằng cách tính tỷ lệ các lần tạo hình ảnh khớp với prompt tương ứng của chúng trên tất cả các lần tạo.
Kết quả. Như được trình bày trong Bảng 1, mô hình của chúng tôi cho thấy cải thiện đáng kể về độ chính xác tạo ra, từ 1.3× đến 3.6× so với SD trên bốn nhiệm vụ và tăng gấp đôi độ chính xác trung bình. Đáng chú ý, LMD đạt được độ chính xác tạo hình ảnh hơn gấp đôi so với độ chính xác SD đối với nhiệm vụ mối quan hệ không gian và nhiệm vụ phủ định. Điều này nhấn mạnh tính hữu ích của việc tạo hình ảnh grounding trên bộ tạo bố cục LLM. Hơn nữa, khi tích hợp thêm GLIGEN vào pipeline của chúng tôi để tận dụng dữ liệu có chú thích thể hiện trong lĩnh vực, phương pháp của chúng tôi, được ký hiệu là LMD+, đạt được cải thiện bổ sung.
4.3 Nghiên cứu loại bỏ
Giai đoạn bố cục-thành-hình ảnh. So sánh với các phương pháp bố cục-thành-hình ảnh khác. Như được hiển thị trong Bảng 2, so với các phương pháp tạo hình ảnh bố cục-thành-hình ảnh không cần huấn luyện thực hiện grounding cấp ngữ nghĩa, bộ điều khiển dựa trên bố cục đề xuất của chúng tôi cung cấp grounding cấp thể hiện tốt hơn nhiều. Điều này được chứng minh bởi thực tế rằng bộ điều khiển không cần huấn luyện của chúng tôi thậm chí còn vượt trội hơn phương pháp dựa trên huấn luyện GLIGEN (Li et al., 2023b) trong nhiệm vụ khả năng tính toán sinh sản, bất chấp không được huấn luyện với bất kỳ chú thích cấp thể hiện nào. Hơn nữa, bộ điều khiển của chúng tôi cũng vượt trội đáng kể so với phương pháp dựa trên huấn luyện GLIGEN (Li et al., 2023b) trong nhiệm vụ liên kết thuộc tính và lý luận không gian. Khi được tích hợp với GLIGEN để tận dụng các bộ dữ liệu có chú thích thể hiện, việc tích hợp của chúng tôi, được ký hiệu là LMD+, cho phép cải thiện thêm mà không cần huấn luyện bổ sung. Chuyển mô hình khuếch tán cơ bản mà không điều chỉnh siêu tham số. Như được hiển thị trong Bảng 3, nhờ vào tính chất không cần huấn luyện của chúng tôi, LMD duy trì được lợi ích so với mô hình cơ bản (khoảng 2× tăng hiệu suất) khi chúng tôi chuyển mô hình khuếch tán cơ bản từ SDv1.5 sang SDv2.1 mà không điều chỉnh bất kỳ siêu tham số nào, bao gồm λ và ω được giới thiệu bởi phương pháp của chúng tôi.5 Điều này cho thấy tiềm năng của việc tích hợp LMD với các mô hình khuếch tán tương lai.

--- TRANG 10 ---
Độ chính xác hình ảnh
Phương pháp Trung bình 4 nhiệm vụ
SD v1.5 (Mặc định) 37%
LMD (trên SDv1.5) (Của chúng tôi, mặc định) 77% (2.1×)
SD v2.1 38%
LMD (trên SDv2.1) (Của chúng tôi) 77% (2.0×)

Bảng 3: LMD đạt được lợi ích tương đương khi được điều chỉnh cho Stable Diffusion v2.1 mà không cần điều chỉnh siêu tham số hoặc huấn luyện mô hình. Điều này cho thấy tín hiệu hứa hẹn rằng lợi ích từ phương pháp của chúng tôi có thể theo cùng với sự nâng cao của các mô hình khuếch tán. Hiệu suất của phương pháp chúng tôi có thể được cải thiện thêm với việc điều chỉnh siêu tham số bổ sung.

Độ chính xác hình ảnh
Phương pháp Trung bình 4 nhiệm vụ
LMD (không có SAM) 72.8%
LMD (có SAM) 76.5%
LMD+ (không có SAM) 82.8%
LMD+ (có SAM) 80.5%

Bảng 4: Thử nghiệm về việc sử dụng SAM so với sử dụng ngưỡng attention đơn giản trong giai đoạn 2. Trong khi loại bỏ SAM dẫn đến sự suy giảm nhẹ trong LMD, việc loại bỏ SAM lại dẫn đến hiệu suất tốt hơn trong LMD+.

Sử dụng SAM so với một ngưỡng attention đơn giản để có được mặt nạ cho mỗi hộp. Thay vì sử dụng SAM để có được mặt nạ cho mỗi hộp, chúng tôi cũng khám phá một cách tiếp cận không yêu cầu mô-đun phân đoạn bổ sung. Thay vào đó, chúng tôi sắp xếp các pixel trong mỗi hộp theo giá trị attention của chúng đối với chú thích hộp và chọn 75% pixel hàng đầu trong mỗi hộp có attention cao nhất làm mặt nạ cho hộp. Như được hiển thị trong Bảng 4, tác động của SAM khác nhau đối với LMD/LMD+. Trong LMD, vì hướng dẫn dựa trên attention ít chính xác về mặt không gian đối với các hộp bố cục, SAM giúp có được mặt nạ đúng bao phủ đối tượng. Do đó, việc loại bỏ SAM dẫn đến sự suy giảm nhẹ trong LMD. Trong LMD+, vì hướng dẫn chính xác hơn về mặt không gian, SAM không còn cần thiết hầu hết thời gian. Thay vào đó, SAM đôi khi chọn một vùng bao gồm nền, gây ra sự nhầm lẫn và giảm hiệu suất. Do đó, việc loại bỏ SAM cải thiện nhẹ kết quả trong LMD+. Chúng tôi tạo SAM như một lựa chọn tùy chọn (như được mô tả trong Hình 2) nhưng vẫn khuyến nghị nó cho LMD và bật nó theo mặc định. Chúng tôi giới thiệu độc giả tham khảo Phụ lục D để biết các thử nghiệm bổ sung về giá trị của các siêu tham số.
Giai đoạn văn bản-thành-bố cục. Thử nghiệm các ví dụ trong ngữ cảnh. Ngoài việc sử dụng bảy ví dụ trong ngữ cảnh cố định được cung cấp trong Bảng K.2 theo mặc định, chúng tôi cũng thay đổi số lượng ví dụ trong ngữ cảnh được đưa cho LLM (tức là "shots"). Chúng tôi hiển thị trong Bảng 5 rằng trong khi GPT-3.5 được hưởng lợi từ nhiều ví dụ trong ngữ cảnh hơn, GPT-4 có thể tạo ra thành công tất cả các bố cục ngay cả khi chỉ được đưa một ví dụ trong ngữ cảnh. Lưu ý rằng chúng tôi cũng quan sát thấy GPT-4 vẫn có thể tạo ra bố cục mà không có bất kỳ ví dụ trong ngữ cảnh nào (tức là chỉ được đưa các hướng dẫn văn bản). Tuy nhiên, vì không có ví dụ nào được cung cấp làm tham chiếu trong thiết lập zero-shot này, định dạng đầu ra LLM được quan sát thấy khác nhau trong các lần chạy khác nhau, khiến việc phân tích với chương trình trở nên khó khăn. Vì việc truyền tải định dạng thông qua một ví dụ dễ dàng hơn nhiều so với thông qua hướng dẫn ngôn ngữ, chúng tôi khuyến nghị có ít nhất một ví dụ. Quan sát của chúng tôi cho thấy LLM đã học được khả năng tạo ra các hộp đối tượng trong quá trình huấn luyện trước và không cần chúng tôi truyền tải thông qua nhiều ví dụ trong ngữ cảnh. Thay đổi các loại mô hình và kích thước của LLM. Chúng tôi cũng thử nghiệm các LLM được sử dụng để tạo bố cục văn bản-thành-bố cục, bao gồm việc sử dụng LLM tự lưu trữ với trọng số công khai (Mahan et al., 2023; Touvron et al., 2023; Mukherjee et al., 2023; Jiang et al., 2024). Kết quả cho thấy rằng khả năng tạo ra bố cục chất lượng cao không giới hạn ở LLM độc quyền, và LLM lớn hơn cung cấp khả năng tạo bố cục tốt hơn nhiều. Chúng tôi giới thiệu độc giả tham khảo Phụ lục D và Phụ lục E để biết thêm các thử nghiệm và điều tra.

--- TRANG 11 ---
Độ chính xác bố cục (4 nhiệm vụ)
# shots gpt-3.5-turbo gpt-4
1 Shot 89.8% 100.0%
4 Shots 96.3% 100.0%
7 Shots 99.0% 100.0%

Bảng 5: Thử nghiệm về số lượng ví dụ trong ngữ cảnh ("shots") được đưa cho LLM. Trong khi GPT-3.5 được hưởng lợi từ nhiều ví dụ trong ngữ cảnh hơn, GPT-4 đã xuất sắc trong việc tạo bố cục ngay cả với chỉ một ví dụ.

Màu sắc Hình dạng Kết cấu Không gian
SDv1 0.3765 0.3576 0.4156 0.1246
LMD (trên SDv1) 0.5495 0.5462 0.5241 0.2570
SDv2 0.5065 0.4221 0.4922 0.1342
LMD (trên SDv2) 0.5736 0.5334 0.5227 0.2704

Bảng 6: Phương pháp của chúng tôi vượt trội hơn các mô hình khuếch tán cơ bản SDv1 và SDv2 trên T2I-CompBench (Huang et al., 2023) trên tất cả bốn nhiệm vụ mà không cần huấn luyện thêm.

Một bức tranh màu nước của một cảnh
Một bức tranh màu nước của một cảnhMột bức tranh màu nước của hai quả táo trên bàn gỗ, không quả nào màu đỏ và cả hai đều màu xanh lá
(a) Stable Diffusion XL(b) Bố cục của chúng tôi(c) Việc tạo của chúng tôi
Một quả táo xanh lá
Một quả táo xanh lá
Một bàn gỗ
(e) Việc tạo vòng 2(d) Bố cục vòng 2
Một bàn gỗ
Một quả táo xanh lá
Một quả táo xanh lá

Hình 9: Một trường hợp thất bại xảy ra khi phương pháp của chúng tôi, được hiển thị trong (c), tạo ra các đối tượng với góc nhìn và kích thước không mong muốn do sự mơ hồ trong bố cục được tạo ra. Bố cục do LLM tạo ra (b) phù hợp với góc nhìn từ trên xuống cận cảnh của một bàn nhỏ, nhưng mô hình bố cục-thành-hình ảnh giả định một góc nhìn bên và do đó không thể tạo ra một hình ảnh khả thi. Tuy nhiên, phương pháp của chúng tôi vẫn cung cấp tính diễn giải nhiều hơn thông qua bố cục trung gian (b) so với baseline SDXL (a). Với một yêu cầu bổ sung cho góc nhìn bên và kích thước đối tượng chính xác, LLM đã điều chỉnh bố cục trong (d) và việc tạo cuối cùng (e) phù hợp với prompt văn bản.

4.4 T2I-CompBench
Ngoài benchmark đề xuất của chúng tôi với đánh giá dựa trên phát hiện, chúng tôi đánh giá phương pháp của chúng tôi trên T2I-CompBench (Huang et al., 2023) bổ sung sử dụng các mô hình trả lời câu hỏi thị giác (VQA) để đánh giá tạo ra. Các nhiệm vụ màu sắc, hình dạng và kết cấu sử dụng BLIP (Li et al., 2022) trong thiết lập VQA, trong khi nhiệm vụ không gian sử dụng UniDet (Zhou et al., 2022) để đánh giá. Như được hiển thị trong Bảng 6, phương pháp LMD của chúng tôi, khi được áp dụng trên SDv1 hoặc SDv2, cải thiện hiệu suất trên tất cả bốn nhiệm vụ. Các thử nghiệm bổ sung có trong Bảng D.4.
4.5 Đánh giá dựa trên người đánh giá
Thiết lập. Chúng tôi cũng đánh giá khả năng tuân theo prompt của phương pháp chúng tôi và SD thông thường, mô hình khuếch tán cơ bản mà phương pháp chúng tôi sử dụng bên dưới. Chúng tôi chọn ngẫu nhiên 10 prompt văn bản từ benchmark đề xuất của chúng tôi và tạo ra một cặp hình ảnh cho mỗi prompt văn bản, một với LMD+ của chúng tôi và một với mô hình cơ bản SD.6 Sau đó chúng tôi mời 11 người đánh giá so sánh mỗi cặp hình ảnh và trả lời hai câu hỏi:
1. Câu hỏi 1: Hình ảnh nào phù hợp tốt hơn với prompt văn bản?
2. Câu hỏi 2: Hình ảnh nào có thành phần nền trước-nền tự nhiên và mạch lạc hơn?
Ngoài tùy chọn ưu tiên mỗi hình ảnh, một tùy chọn "tương tự" cũng được cung cấp cho mỗi cặp.
Kết quả. Chúng tôi tính trung bình điểm số trên 110 phản hồi. Kết quả cho thấy rằng phương pháp LMD+ của chúng tôi đạt 88.18% (so với 10.90% cho SD) cho câu hỏi đầu tiên và 35.45% (so với 31.81% cho SD) cho câu hỏi thứ hai. Điều này cho thấy rằng phương pháp của chúng tôi tạo ra hình ảnh phù hợp chính xác với prompt so với baseline SD mà không làm giảm tính tự nhiên hoặc mạch lạc.

--- TRANG 12 ---
5 Thảo luận
Vì chúng tôi sử dụng các mô hình có sẵn, LLM có thể tạo ra các bố cục mơ hồ đối với mô hình khuếch tán. Ví dụ, bố cục trong Hình 9(b) khả thi cho một hình ảnh cận cảnh từ trên xuống, nhưng mô hình khuếch tán tạo ra một hình ảnh nhìn từ bên. Điều này khiến các quả táo không nằm trên bàn trong Hình 9(c). Prompting hoặc tinh chỉnh LLM để rõ ràng hơn về các giả định của nó trong bố cục (ví dụ: góc nhìn) có thể giảm thiểu vấn đề này. Bố cục trung gian trong việc tạo hai giai đoạn của chúng tôi cho phép tính diễn giải nhiều hơn so với mô hình stable diffusion cơ bản của chúng tôi. Sau khi chẩn đoán điểm thất bại, chúng tôi đưa ra một yêu cầu bổ sung cho góc nhìn bên và kích thước đối tượng chính xác cho LLM. LLM đã điều chỉnh việc tạo bố cục tiếp theo, cho phép tạo ra hình ảnh phù hợp với prompt đầu vào trong vòng 2, như được hiển thị trong Hình 9(d,e). Phương pháp của chúng tôi cũng kế thừa thiên lệch từ mô hình khuếch tán cơ bản (Luccioni et al., 2023). Hơn nữa, mặc dù phương pháp của chúng tôi có thể xử lý các đối tượng không được đề cập trong các ví dụ trong ngữ cảnh (ví dụ: con gấu và con hươu trong Hình 7), LLM vẫn có thể tạo ra bố cục tốt hơn cho các đối tượng được đề cập trong các ví dụ trong ngữ cảnh bằng cách tham chiếu các ví dụ bố cục. Phương pháp của chúng tôi cũng có thể được chưng cất thành một mô hình khuếch tán văn bản-thành-hình ảnh một giai đoạn để cải thiện khả năng hiểu prompt của nó mà không cần tận dụng LLM tại thời gian suy luận để dễ dàng triển khai.
6 Tóm tắt
Trong bài báo này, chúng tôi nâng cao khả năng hiểu prompt của các mô hình khuếch tán văn bản-thành-hình ảnh. Chúng tôi trình bày một quy trình tạo hai giai đoạn không cần huấn luyện mới kết hợp việc tạo bố cục dựa trên văn bản của LLM và tạo hình ảnh dựa trên bố cục. Phương pháp của chúng tôi cũng cho phép đặc tả cảnh dựa trên hướng dẫn và tạo ra từ các prompt bằng ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán cơ bản. Phương pháp của chúng tôi vượt trội hơn các baseline mạnh trong việc tuân theo chính xác các prompt trong tạo văn bản-thành-hình ảnh.
Lời cảm ơn. Các tác giả xin cảm ơn Aleksander Holynski vì các cuộc thảo luận hữu ích.

--- TRANG 13 ---
Tài liệu tham khảo
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.
Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18370–18380, 2023.
Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2, 2023.
Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392–18402, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. arXiv preprint arXiv:2304.03373, 2023.
Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, and Mu Li. Layoutdiffuse: Adapting foundational diffusion models for layout-to-image generation. arXiv preprint arXiv:2302.08908, 2023.
Dave Epstein, Taesung Park, Richard Zhang, Eli Shechtman, and Alexei A Efros. Blobgan: Spatially disentangled scene representations. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV, pp. 616–635. Springer, 2022.
Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. arXiv preprint arXiv:2305.15393, 2023.
Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry S Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layouttransformer: Layout generation and completion with self-attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1004–1014, 2021.
Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14953–14962, 2023.
Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, and Amir Globerson. Learning canonical representations for scene graph to image generation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16, pp. 210–227. Springer, 2020.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv:2307.06350, 2023.

--- TRANG 14 ---
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1219–1228, 2018.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.
Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. arXiv preprint arXiv:2305.17216, 2023.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 12888–12900. PMLR, 2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023b.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740–755. Springer, 2014.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.
Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408, 2023.
Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models, 2023. URL https://huggingface.co/stabilityai/StableBeluga2.
Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2337–2346, 2019.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021.

--- TRANG 15 ---
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234–241. Springer, 2015.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 36479–36494, 2022.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.
Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. arXiv preprint arXiv:2307.10816, 2023.
Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5410–5419, 2017.
Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14246–14255, 2023.
Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.
Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8584–8593, 2019.

--- TRANG 16 ---
Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22490–22499, 2023.
Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7571–7580, 2022.

--- TRANG 17 ---
A Giới thiệu sơ bộ về các mô hình khuếch tán tiềm ẩn
Giai đoạn bố cục-thành-hình ảnh (tức là giai đoạn tạo hình ảnh) của phương pháp LMD chúng tôi được xây dựng dựa trên các mô hình Stable Diffusion văn bản-thành-hình ảnh có sẵn, dựa trên khung khuếch tán tiềm ẩn (Rombach et al., 2022). Chúng tôi trình bày phần giới thiệu sơ bộ về khung khuếch tán tiềm ẩn trong phần này và định nghĩa các thuật ngữ chính được sử dụng trong công trình của chúng tôi. Chúng tôi khuyến khích độc giả tham khảo Rombach et al. (2022) để có giải thích chi tiết về khung khuếch tán tiềm ẩn.
Các mô hình khuếch tán tiềm ẩn (Rombach et al., 2022) là các mô hình sinh mạnh mẽ học phân phối dữ liệu của các bộ dữ liệu hình ảnh phức tạp, độ phân giải cao. Trước khi huấn luyện một mô hình khuếch tán tiềm ẩn, Rombach et al. (2022) đầu tiên huấn luyện một bộ mã hóa hình ảnh chuyển đổi một hình ảnh x thành một vector z trong không gian tiềm ẩn đa chiều cao và một bộ giải mã chuyển đổi z trở lại thành một vector trong không gian hình ảnh tương tự như x về diện mạo. Bằng cách huấn luyện và lấy mẫu một mô hình khuếch tán trong không gian tiềm ẩn, khuếch tán tiềm ẩn làm giảm chi phí huấn luyện và lấy mẫu từ các mô hình khuếch tán độ phân giải cao và được sử dụng rộng rãi trong việc tạo văn bản-thành-hình ảnh, với Stable Diffusion là một mô hình phổ biến dựa trên khung khuếch tán tiềm ẩn. Phương pháp của chúng tôi cải thiện khả năng hiểu prompt của Stable Diffusion mà không điều chỉnh trọng số.
Trong quá trình huấn luyện, khung khuếch tán tiềm ẩn đầu tiên ánh xạ mỗi hình ảnh huấn luyện, được ký hiệu là x0, vào tiềm ẩn z0 với bộ mã hóa hình ảnh được đóng băng trong giai đoạn huấn luyện khuếch tán:
z0 = Encode(x0)  (8)
Một timestep t được lấy mẫu đồng đều từ {1,...,T}, trong đó T là một siêu tham số.
Nhiễu ϵ sau đó được lấy mẫu từ một phân phối Gaussian được tham số hóa bởi timestep t và thêm vào tiềm ẩn z0 để có được tiềm ẩn nhiễu zt. Một mạng nơ-ron với tham số θ học dự đoán nhiễu được thêm vào ϵ cho quá trình thuận bằng cách tối thiểu hóa mục tiêu huấn luyện:
L = ||ϵ - ϵθ(zt, t)||2  (9)
Mạng nơ-ron được mô tả ở trên thường sử dụng một biến thể của kiến trúc U-Net (Ronneberger et al., 2015) có các lớp attention (Vaswani et al., 2017), và do đó cũng được gọi là U-Net khuếch tán.
Tại thời gian suy luận, có nhiều phương pháp lấy mẫu cho phép tổng hợp các mẫu từ một mô hình khuếch tán được huấn luyện theo cách được mô tả ở trên. Trực giác chung là đi qua một quá trình ngược (cũng được gọi là quá trình khử nhiễu) trong đó mô hình khuếch tán ϵθ lặp đi lặp lại dự đoán một vector nhiễu ϵθ(zt, t) từ zt và trừ nó để biến đổi zt thành một mẫu zt-1 có ít nhiễu hơn và gần hơn với phân phối của tập huấn luyện, với t được khởi tạo là T và zT ∼ N(0, I). Mẫu đã khử nhiễu z0 giống với dữ liệu sạch trong không gian tiềm ẩn.
Người ta có thể sử dụng DDPM (Ho et al., 2020) để thực hiện lấy mẫu từ một mô hình dự đoán nhiễu ϵθ. DDPM dự đoán nhiễu ϵ cho mỗi trong số T bước khử nhiễu và sau đó có được zt-1 từ zt bằng công thức này:
zt-1 = 1/√αt (zt - (1-αt)/√(1-∏i=1^t αi) ϵθ(zt, t)) + σtϵt  (10)
trong đó ϵt ∼ N(0, I), αt và σt được tham số hóa bởi một lịch trình phương sai {βt ∈ (0,1)}T_t=1 kiểm soát kích thước của bước khử nhiễu.
Các mô hình khuếch tán ngầm định khử nhiễu (DDIM, Song et al. (2020)) là một khái quát hóa của DDPM cho phép lấy mẫu với ít lần lặp hơn. DDIM áp dụng quy tắc cập nhật sau:
zt-1 = √αt-1 (zt - √(1-αt)ϵθ(zt, t))/√αt + σtϵt  (11)
Lưu ý rằng DDIM chia sẻ cùng quy trình huấn luyện với DDPM, có nghĩa là chúng ta có thể chọn thực hiện DDIM hoặc DDPM cho một mô hình khuếch tán đã huấn luyện. Khi σt được đặt thành 0, đó là trường hợp cho thiết lập của chúng tôi, việc khử nhiễu trở thành xác định cho trước zT. Kết quả được hiển thị trong công trình của chúng tôi được thu được với DDIM với σt = 0, với các phương pháp lấy mẫu nhanh hơn khác như Lu et al. (2022) cũng áp dụng được cho phương pháp của chúng tôi.
Vì có nhiều phương pháp lấy mẫu cho trước một mô hình khuếch tán đã huấn luyện áp dụng được trong khung khuếch tán tiềm ẩn, chúng tôi ký hiệu quá trình khử nhiễu, như trong Phương trình (10) và Phương trình (11), là
zt-1 ← Denoise(zt)  (12)
Sau khi nhận được mẫu đã khử nhiễu z0, chúng tôi sau đó giải mã hình ảnh với một bộ giải mã hình ảnh:
x0 = Decode(z0)  (13)
Tạo có điều kiện văn bản thông qua cross-attention. Công thức trên mô tả quá trình tạo không điều kiện của các mô hình khuếch tán tiềm ẩn. Các mô hình như Stable Diffusion nhận văn bản làm đầu vào và thực hiện tạo có điều kiện. Sự khác biệt giữa quá trình tạo có điều kiện và không điều kiện liên quan đến việc xử lý văn bản đầu vào thành đặc trưng văn bản, truyền token đặc trưng đến U-Net khuếch tán, và thực hiện hướng dẫn không phân loại (Ho & Salimans, 2022), được mô tả như sau.
Thay vì chỉ nhận đầu vào nhiễu xt và timestep t, U-Net khuếch tán có điều kiện ϵθ(zt, t, τθ(y)) nhận thêm một điều kiện văn bản y được xử lý bởi một bộ mã hóa văn bản τθ(·). Bộ mã hóa văn bản là một bộ mã hóa văn bản CLIP (Radford et al., 2021) trong Stable Diffusion. Sau khi y được tokenize bởi tokenizer thành các token rời rạc, nó được xử lý bởi một Transformer (Vaswani et al., 2017) thành đặc trưng văn bản τθ(y) ∈ Rl×dtext, trong đó l là số lượng token văn bản trong y sau tokenization và dtext là chiều của đặc trưng.
Đặc trưng văn bản τθ(y) sau đó được xử lý bởi các lớp cross-attention trong U-Net khuếch tán để đầu ra của U-Net cũng có thể thay đổi tùy thuộc vào văn bản. Để đơn giản, chúng tôi chỉ xem xét một đầu cross-attention trong phần giới thiệu sơ bộ này và giới thiệu độc giả tham khảo Rombach et al. (2022) và Vaswani et al. (2017) để biết chi tiết với multi-head cross-attention được sử dụng trong U-Net trong khung khuếch tán tiềm ẩn.
Cụ thể, mỗi lớp cross-attention ánh xạ tuyến tính đặc trưng văn bản τθ(y) thành các vector key và value k, v ∈ Rl×dattn, trong đó dattn là chiều attention. Mỗi lớp cross-attention cũng nhận đặc trưng 2D phẳng từ lớp trước trong U-Net và ánh xạ tuyến tính đặc trưng thành một vector query q ∈ Rm×dattn trong đó m là chiều của đặc trưng hình ảnh phẳng trước đó.
Sau đó, một bản đồ cross-attention A được tính từ các vector query q, key k và value v, mô tả ái lực từ đặc trưng hình ảnh đến đặc trưng token văn bản:
Auv = Softmax(quTkv)  (14)
trong đó qu và kv là đặc trưng hình ảnh được biến đổi tuyến tính tại vị trí không gian u và đặc trưng văn bản tại chỉ số token v trong prompt, tương ứng.
Bản đồ attention sau đó được sử dụng để tính toán một kết hợp có trọng số của các giá trị v:
ou = Σv Auv vv  (15)
o ∈ Rm×dattn sau đó được biến đổi tuyến tính để trở thành đầu ra của lớp cross-attention. Các kết nối dư và chuẩn hóa lớp được bỏ qua trong phần giới thiệu này để đơn giản.
Các mẫu được tạo ra bằng hướng dẫn không phân loại để đảm bảo sự phù hợp với prompt văn bản y. Tại thời gian huấn luyện, với xác suất nhỏ, điều kiện đầu vào τθ(y) được thay thế ngẫu nhiên bằng một token null có thể học được τ∅. Tại thời gian suy luận, hướng dẫn không phân loại sử dụng thuật ngữ sau ˜ϵθ(xt, t, τθ(y)) thay cho nhiễu dự đoán ϵθ(xt, t) trong quy tắc cập nhật cho việc tạo không điều kiện:
˜ϵθ(xt, t, τθ(y)) = wϵθ(xt, t, τθ(y)) + (1-w)ϵθ(xt, t, τ∅)  (16)
trong đó w là cường độ hướng dẫn không phân loại, được đặt thành 7.5 theo mặc định trong Stable Diffusion.

--- TRANG 18 ---
Thuật toán 1 Tạo hình ảnh dựa trên bố cục.
Đầu vào: Một tập hợp các hộp giới hạn có chú thích {(b(i), y(i))}Ni=1. Chú thích nền y(bg).
Đầu ra: Hình ảnh x0.
1: zT ← SampleGaussian(0, I)
2: Tạo latent có mặt nạ cho mỗi hộp:
3: for each hộp có chú thích (b(i), y(i)) do
4:    z(i)T ← zT
5:    y(i) ← PromptForBox(y(i), y(bg))
6:    for t ← T to 1 do
7:        z(i)t, A(i)t ← AttnControl(z(i)t, y(i), b(i))
8:        z(i)t-1 ← Denoise(z(i)t, y(i))
9:    end for
10:   A(i) ← TemporalAverage(A(i)t)
11:   m(i) ← SAMRefine(A(i), z(i)0) (Tùy chọn: Có thể được thay thế bằng ngưỡng attention thay vào đó.)
12:   ẑ(i)t ← z(i)t ⊗ m(i)
13: end for
14: Tạo hình ảnh kết hợp:
15: z(comp)T ← zT
16: y ← ComposedPrompt((y(i))Ni=1, y(bg))
17: for t ← T to 1 do
18:    if t ≥ rT then
19:        z(comp)t ← LatentCompose(z(comp)t, ẑ(i)t, m(i)) ∀i
20:        z(comp)t ← AttnTransfer(z(comp)t, y(comp), (A(i)t)Ni=1)
21:    end if
22:    z(comp)t-1 ← Denoise(z(comp)t, y(comp))
23: end for
24: x0 ← Decode(z(comp)0)

--- TRANG 19 ---
B Mã giả cho việc tạo hình ảnh dựa trên bố cục
Chúng tôi trình bày mã giả cho giai đoạn grounding bố cục (giai đoạn 2) của chúng tôi trong Thuật toán 1. Chúng tôi giải thích chức năng của các hàm được sử dụng trong mã giả:
1. SampleGaussian lấy mẫu i.i.d Gaussian tiêu chuẩn làm nhiễu ban đầu cho tensor tiềm ẩn.
2. PromptForBox đơn giản đặt "[prompt nền] with [chú thích hộp]" (ví dụ: "một hình ảnh thực tế của một cảnh trong nhà with một con mèo xám") làm prompt khử nhiễu.
3. AttnControl thực hiện hướng dẫn ngược để tối thiểu hóa hàm năng lượng Phương trình (2) được mô tả trong Phần 3 để khuyến khích attention đến vùng trong hộp và không khuyến khích attention trên vùng ngoài hộp. Các bản đồ cross-attention A(i)t cũng được trả về để cho phép có được một mặt nạ cho mỗi hộp.
4. Denoise biểu thị một bước khử nhiễu bởi mô hình khuếch tán.
5. TemporalAverage tính trung bình bản đồ cross-attention theo chiều timestep.
6. SAMRefine tinh chỉnh bản đồ attention bằng cách nội bộ decode tiềm ẩn và tinh chỉnh với SAM. Nếu SAM không được bật, chúng tôi thực hiện ngưỡng attention thay vào đó.
7. ComposedPrompt tạo prompt cho việc tạo tổng thể. Chúng tôi cung cấp hai tùy chọn cho prompt tổng thể: sử dụng prompt đầu vào ban đầu hoặc tạo prompt như "[prompt nền] with [chú thích hộp 1], [chú thích hộp 2], ...". Tùy chọn đầu tiên cho phép capture đối tượng cũng như tương tác nền trước-nền không được capture trong bố cục. Tùy chọn sau cho phép chú thích bằng ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán và vẫn mạnh mẽ khi chú thích gây hiểu nhầm (ví dụ: "không quả táo nào màu đỏ"). Chúng tôi sử dụng tùy chọn sau theo mặc định nhưng cũng cho phép tùy chọn đầu tiên cho điều chỉnh tinh vi.
8. LatentCompose kết hợp không gian mỗi latent z(i) đối với mặt nạ tương ứng m(i), thay thế nội dung của latent đích tại các vị trí có mặt nạ. Đối với thứ tự kết hợp, chúng tôi kết hợp các latent có mặt nạ với diện tích lớn nhất sau khi masking trước.
9. AttnTransfer thực hiện hướng dẫn ngược để tối thiểu hóa hàm năng lượng Phương trình (7) trong Phần 3 để khuyến khích attention trong việc tạo tổng thể trong hộp tương tự với attention trong việc tạo cho mỗi hộp ngoài kiểm soát attention.
C Đặc tính và trường hợp sử dụng bổ sung từ đặc tả cảnh dựa trên hướng dẫn
Như được hiển thị trong Phần 3.3, LMD, được trang bị đặc tả cảnh dựa trên hướng dẫn, cho phép người dùng chỉ định hình ảnh mong muốn với nhiều hướng dẫn theo sau prompt ban đầu.
Hơn nữa, chúng tôi chứng minh hai trường hợp sử dụng bổ sung được hỗ trợ bởi đặc tả cảnh dựa trên hướng dẫn trong Hình C.1 mà không cần huấn luyện thêm.
Trong Hình C.1(a), đặc tả cảnh dựa trên hướng dẫn cho phép người dùng thử nghiệm các điều chỉnh khác nhau trên cùng một việc tạo trong khi bảo tồn phong cách hình ảnh và bố cục tổng thể, tạo điều kiện cho việc tạo nội dung tinh vi.
LLM được trang bị trong LMD cũng có thể phản hồi các yêu cầu mở và đưa ra đề xuất để cải thiện cảnh. Hơn nữa, khác với các phương pháp chỉnh sửa hình ảnh dựa trên hướng dẫn chỉ nhận một hướng dẫn mà không có ngữ cảnh, đặc tả cảnh dựa trên hướng dẫn của chúng tôi phân tích hướng dẫn trong ngữ cảnh của nó, cho phép đối thoại tự nhiên hơn với người dùng. Ví dụ, trong Hình C.1(b), phương pháp của chúng tôi có thể phản hồi các hướng dẫn với cụm từ như "Những đối tượng nào bạn có thể thêm vào để làm cho nó sống động?", "hoàn tác chỉnh sửa cuối cùng", và "thêm một ao nhỏ thay vào đó".

--- TRANG 20 ---
Đặc tả cảnh dựa trên hướng dẫn: Điều chỉnh tinh viLMD (Của chúng tôi)Một bức ảnh thực tế của một cảnh đường với hai chiếc xe hơi. Chiếc xe ở bên trái màu trắng. Chiếc xe ở bên phải màu đen.
Yêu cầu ban đầu của người dùngLMD (Của chúng tôi)
LMD (Của chúng tôi)Thay đổi chiếc xe trắng thành xe đỏ
Yêu cầu tiếp theoThay đổi chiếc xe đen thành xe xanh
Yêu cầu tiếp theo
(a) LMD cho phép người dùng thử nghiệm các điều chỉnh chi tiết khác nhau trong khi bảo tồn phong cách hình ảnh và bố cục tổng thể, cho phép tạo nội dung tinh vi.
(b) LLM được sử dụng bởi LMD có thể thực hiện điều chỉnh cảnh mở, đưa ra đề xuất và hiểu yêu cầu người dùng dựa trên ngữ cảnh qua nhiều vòng đối thoại người dùng. Một bức ảnh thực tế của một tảng đá trên cỏ.
Yêu cầu ban đầu của người dùngTuyệt vời. Vui lòng hoàn tác chỉnh sửa cuối cùng và làm cho cảnh sống động bằng cách thêm một ao nhỏ thay vào đó.
Yêu cầu tiếp theo 3LMD (Của chúng tôi)
LMD (Của chúng tôi)
Những đối tượng nào bạn có thể thêm vào để làm cho nó sống động?
Yêu cầu tiếp theo 2Một con chim nhỏ mổ gần tảng đá. Một con sóc kiếm ăn gần tảng đá. … Một ao nhỏ hoặc suối nước gần đó, phản chiếu bầu trời.
Phản hồi LLMĐặc tả cảnh dựa trên hướng dẫn: Yêu cầu mở và ngữ cảnh đa vòng
Làm cho nó sống động bằng cách thêm nhiều đối tượng có liên quan hơn
Yêu cầu tiếp theo 1LMD (Của chúng tôi)
Hình C.1: Đặc tính và trường hợp sử dụng bổ sung được kích hoạt bởi đặc tả cảnh dựa trên hướng dẫn.
D Nghiên cứu loại bỏ bổ sung
D.1 Giai đoạn văn bản-thành-bố cục
Thay đổi các loại LLM. Tất cả LLM trong Bảng D.1 tạo ra các bố cục gần như hoàn hảo tuân theo các yêu cầu trong các prompt, cho thấy nút thắt cổ chai là giai đoạn bố cục-thành-hình ảnh. gpt-4 cho thấy kết quả cải thiện trong bố cục và việc tạo hình ảnh tiếp theo, so với gpt-3.5-turbo. Khả năng tạo ra các bố cục chất lượng cao không giới hạn ở LLM độc quyền, với StableBeluga2 dựa trên Llama2 (Mahan et al., 2023; Touvron et al., 2023; Mukherjee et al., 2023) và Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024) cũng có thể thực hiện tạo bố cục văn bản-thành-bố cục trong giai đoạn 1. Chúng tôi tin rằng tinh chỉnh các mô hình này sẽ dẫn đến hiệu suất tốt hơn về tạo bố cục văn bản-thành-bố cục.

--- TRANG 21 ---
Độ chính xác bố cục (Hình ảnh)
Mô hình giai đoạn 1 Trung bình 4 nhiệm vụ
StableBeluga2 96.5% (67.0%)
Mixtral-8x7B-Instruct-v0.1 98.3% (77.5%)
gpt-3.5-turbo 99.0% (80.5%)
gpt-4 100.0% (86.3%)

Bảng D.1: Thử nghiệm về các LLM khác nhau trong giai đoạn 1. Mặc dù các mô hình độc quyền như GPT-3.5 và GPT-4 hoạt động tốt nhất, khả năng tạo ra bố cục chất lượng cao cũng có mặt trong các mô hình mã nguồn mở Mahan et al. (2023); Jiang et al. (2024); Touvron et al. (2023). Độ chính xác hình ảnh được đánh giá sử dụng LMD+ như giai đoạn 2.

Độ chính xác bố cục
Mô hình giai đoạn 1 Trung bình 4 nhiệm vụ
StableBeluga-7B 59.3%
StableBeluga-13B 84.0%
StableBeluga2 (70B) 96.5%

Bảng D.2: Thử nghiệm về kích thước mô hình LLM trên StableBeluga Models (Mahan et al., 2023) dựa trên Llama-2 (Touvron et al., 2023) để tạo bố cục (chỉ giai đoạn 1). LLM lớn hơn cung cấp tạo bố cục chính xác hơn so với LLM nhỏ hơn.

Độ chính xác hình ảnh (Trung bình 4 nhiệm vụ)
Phương pháp ω = 1 ω = 2 ω = 4 ω = 8
LMD 72.3% 75.8% 76.5% 72.5%
LMD+ 79.8% 80.0% 80.5% 78.3%
(a) Thử nghiệm về siêu tham số ω.

Độ chính xác hình ảnh (Trung bình 4 nhiệm vụ)
Phương pháp λ = 0 λ = 1 λ = 2 λ = 3 λ = 4
LMD 70.8% 75.0% 76.5% 77.3% 75.0%
LMD+ 79.3% 79.5% 80.5% 81.8% 78.8%
(b) Thử nghiệm về siêu tham số λ.

Bảng D.3: Thử nghiệm về siêu tham số ω và λ. Phương pháp của chúng tôi tương đối ổn định về giá trị siêu tham số ω và λ. Trong khi chúng tôi không thực hiện tìm kiếm siêu tham số, siêu tham số mặc định ω = 4 của chúng tôi cho phép hiệu suất tối ưu cho cả LMD và LMD+. Đối với siêu tham số λ, chúng tôi phát hiện rằng đặt λ = 3 dẫn đến hiệu suất tốt hơn so với thiết lập siêu tham số mặc định của chúng tôi với λ = 2, cho thấy hiệu suất của phương pháp chúng tôi có thể được cải thiện thêm thông qua điều chỉnh siêu tham số. Các số được gạch chân cho thấy hiệu suất với lựa chọn siêu tham số mặc định của chúng tôi (ω = 4, λ = 2). Các số in đậm cho thấy hiệu suất tốt nhất trong tất cả các siêu tham số được thử nghiệm.

Thay đổi kích thước LLM. Chúng tôi cũng thử nghiệm khả năng tạo bố cục trên LLM có kích thước mô hình khác nhau. Như được hiển thị trong Bảng D.2, LLM lớn hơn cung cấp khả năng tạo bố cục tốt hơn nhiều.
D.2 Giai đoạn bố cục-thành-hình ảnh
Thay đổi ω. ω là trọng số để cân bằng thuật ngữ mất mát trên nền trước và thuật ngữ trên nền (Phương trình (2)). Trong khi chúng tôi đặt ω = 4 theo mặc định, chúng tôi thử nghiệm lựa chọn thiết kế này. Như được hiển thị bởi kết quả thực nghiệm trong Bảng D.3a, phương pháp của chúng tôi tương đối ổn định về lựa chọn siêu tham số. Hơn nữa, mặc dù chúng tôi không thực hiện tìm kiếm siêu tham số trước khi xác định giá trị siêu tham số mặc định, siêu tham số mặc định ω = 4 của chúng tôi đã dẫn đến hiệu suất tối ưu trong số các giá trị siêu tham số mà chúng tôi tìm kiếm trong thử nghiệm này cho cả LMD và LMD+.
Thay đổi λ. λ là trọng số cho thuật ngữ chuyển giao attention trong Phương trình (7). Như được hiển thị trong Bảng D.3b, chúng tôi phát hiện rằng đặt λ = 3 dẫn đến hiệu suất tốt hơn so với thiết lập siêu tham số mặc định của chúng tôi với λ = 2, cho thấy hiệu suất của phương pháp chúng tôi có thể được cải thiện thêm thông qua điều chỉnh siêu tham số.
Kết quả thử nghiệm trên T2I-CompBench (Huang et al., 2023). Ngoài việc so sánh phương pháp của chúng tôi với phương pháp baseline Stable Diffusion trong Bảng 6, chúng tôi tiếp tục kết hợp giai đoạn văn bản-thành-bố cục (giai đoạn 1) của chúng tôi với các phương pháp bố cục-thành-hình ảnh khác như giai đoạn 2 trong thử nghiệm này, tương tự như Bảng 2. Kết quả trong Bảng D.4, với kết quả cho baseline SD từ Huang et al. (2023). Phương pháp của chúng tôi vượt trội không chỉ so với mô hình khuếch tán cơ bản SD mà còn so với một số biến thể của phương pháp chúng tôi kết hợp giai đoạn 1 của chúng tôi với các phương pháp bố cục-thành-hình ảnh trước đó như giai đoạn 2, cho thấy hiệu quả của bộ điều khiển dựa trên bố cục của chúng tôi.

--- TRANG 22 ---
Giai đoạn 1/Giai đoạn 2 Màu sắc Hình dạng Kết cấu Không gian
SD 0.3765 0.3576 0.4156 0.1246
LMD/MultiDiffusion 0.4631 0.4497 0.4007 0.1604
LMD/Backward Guidance 0.4877 0.5069 0.4643 0.2361
LMD/Boxdiff 0.4579 0.4967 0.4720 0.1965
LMD/LMD (Của chúng tôi) 0.5495 0.5462 0.5241 0.2570

Bảng D.4: Phương pháp của chúng tôi vượt trội hơn mô hình khuếch tán cơ bản SD cũng như một số biến thể của phương pháp chúng tôi kết hợp giai đoạn 1 của chúng tôi với các phương pháp bố cục-thành-hình ảnh trước đó như giai đoạn 2 trên T2I-CompBench (Huang et al., 2023).

Một cảnh ngoài trời thực tế có tuyết
Một người trượt tuyết
Một người trượt tuyết
Một người trượt tuyết
Một cây cọ
Một con gấu
Một con gấu
Một con gấuMột cảnh thực tế
Một khu rừng
Một con gấu trúc
Một cảnh thực tế
Một quả táo
Ví dụ trong ngữ cảnh duy nhất với prompt "Một cảnh thực tế của ba người trượt tuyết đứng thành hàng trên tuyết gần một cây cọ"Ví dụ trong ngữ cảnh duy nhất với prompt "Một con gấu trúc trong rừng không có hoa"Bố cục được tạo ra cho prompt "Một cảnh thực tế của ba con gấu"Bố cục được tạo ra cho prompt "Một quả táo"

Hình E.1: Các bố cục được tạo ra không nhất thiết phải giống với các ví dụ trong ngữ cảnh về phân phối không gian của các hộp. Chúng tôi trình bày LLM với chỉ một ví dụ trong ngữ cảnh và truy vấn nó với một prompt tương tự như ví dụ. Trên: Trong khi truy vấn và ví dụ chia sẻ cấu trúc tương tự (chỉ một đối tượng), LLM tạo ra một hộp cho "một quả táo" rất khác với "một con gấu trúc" về kích thước và vị trí. Dưới: LLM không đơn giản sao chép các hộp cho ba người trượt tuyết trong ví dụ trong ngữ cảnh để tạo ra các hộp cho ba con gấu.

E Các bố cục được tạo ra có phân phối tương tự như các ví dụ trong ngữ cảnh không?
Vì LLM của chúng tôi nhận một vài ví dụ trong ngữ cảnh trong giai đoạn văn bản-thành-bố cục, có thể LLM ưa thích tạo ra các mẫu tương tự với các ví dụ trong ngữ cảnh về phân phối không gian. Để kiểm tra xem đây có phải trường hợp này không, chúng tôi trình bày LLM với chỉ một ví dụ trong ngữ cảnh và truy vấn nó với một prompt tương tự như ví dụ. Kết quả được hiển thị trong Hình E.1. Mặc dù mỗi prompt truy vấn chia sẻ một dạng tương tự với ví dụ trong ngữ cảnh tương ứng, LLM vẫn tạo ra các bố cục được điều chỉnh cho các đối tượng trong prompt truy vấn (ví dụ: quả táo và con gấu) thay vì sao chép hoặc bắt chước các hộp bố cục từ các ví dụ trong ngữ cảnh. Phân tích định tính này cho thấy rằng ngay cả với các ví dụ trong ngữ cảnh làm tham chiếu, LLM thường tạo ra các bố cục tự nhiên theo các prompt, giúp người dùng khỏi việc kỹ thuật prompt nặng để ngăn chặn các bố cục quá giống nhau giữa việc tạo ra và các ví dụ.
F Trực quan hóa bổ sung
Chúng tôi cũng trình bày Hình F.1, bao gồm so sánh định tính với Stable Diffusion v1.5 (viết tắt là SDv1) và chia sẻ các prompt với Hình 1.

--- TRANG 23 ---
(a) Stable Diffusion(b) LMD (Của chúng tôi)Liên kết thuộc tínhPhủ địnhKhả năng tính toán sinh sảnMối quan hệ không gian… một bàn gỗ không có chuối
Một bức tranh màu nước … 3 con mèo trên cỏ… một người đàn ông mặc đỏ đứng bên cạnh một người phụ nữ khác mặc xanh… 3 quả táo sắp xếp thành hình chữ L trên bàn gỗ
(a) Stable Diffusion(b) LMD (Của chúng tôi)

Hình F.1: Chúng tôi cũng tạo ra hình ảnh với cùng các prompt văn bản như Hình 1 với SDv1.5 và LMD trên SDv1.5. Chúng tôi quan sát thấy kết quả tương tự cho thấy rằng trong khi Stable Diffusion Rombach et al. (2022) (a) thường gặp khó khăn trong việc tuân theo chính xác một số loại prompt phức tạp, phương pháp LMD (b) của chúng tôi đạt được khả năng hiểu prompt nâng cao và tuân theo chính xác các loại prompt này.

G Đánh giá VisualChatGPT và GILL cho đặc tả cảnh dựa trên hướng dẫn đa vòng
VisualChatGPT (Wu et al., 2023) và GILL (Koh et al., 2023) liên quan đến LLM trong pipeline tạo hình ảnh của chúng và do đó có thể nhận hướng dẫn từ nhiều vòng đối thoại để tạo hình ảnh. Do đó, ngoài benchmark định tính trong Hình 8, chúng tôi cũng đánh giá cả hai phương pháp cho đặc tả cảnh đa vòng. Như được hiển thị trong Hình G.1, các hình ảnh được tạo ra nhanh chóng suy giảm bắt đầu từ lần lặp thứ hai, cho thấy cả hai phương pháp đều không thể nhận hướng dẫn từ nhiều vòng đối thoại để tạo hình ảnh. Ngược lại, phương pháp của chúng tôi có thể xử lý một số vòng yêu cầu tuần tự về tạo hình ảnh mà không có sự suy giảm tạo ra, được hiển thị trong Hình 6.
H Chi tiết cho tích hợp SDXL
Nhờ vào tính chất không cần huấn luyện của công trình chúng tôi, phương pháp của chúng tôi áp dụng được cho các mô hình khuếch tán khác nhau mà không cần huấn luyện thêm. Do đó, chúng tôi cũng áp dụng phương pháp của chúng tôi trên SDXL 1.0 (Podell et al., 2023), mô hình stable diffusion mới nhất có mô-đun U-Net lớn hơn 3× so với các mô hình stable diffusion trước đó (Rombach et al., 2022).
Rất đơn giản để áp dụng pipeline LMD trực tiếp lên SDXL UNet, có quy trình rất giống với việc áp dụng pipeline LMD lên SD v1/v2. Cách tiếp cận này chỉ yêu cầu các sửa đổi nhỏ của pipeline LMD: khác với SD v1/v2 chỉ sử dụng một bộ mã hóa văn bản để mã hóa prompt, SDXL bao gồm hai bộ mã hóa văn bản để tạo đặc trưng văn bản, và kiểm soát attention được đề xuất trong LMD cần được áp dụng cho cross-attention với cả hai bộ mã hóa văn bản được tính đến. Phần còn lại theo pipeline LMD tiêu chuẩn.
Được truyền cảm hứng bởi các phương pháp như Ramesh et al. (2022) tạo ra hình ảnh độ phân giải thấp và sau đó upsampling việc tạo ra đến độ phân giải đích, một cách tiếp cận thay thế là thực hiện khử nhiễu với LMD tiêu chuẩn với độ phân giải SDv1/v2 tiêu chuẩn (tức là 512×512) và sau đó thực hiện upsampling với SDXL refiner trong vài bước đến độ phân giải mong muốn (ví dụ: 1024×1024). Vì hầu hết việc tạo vẫn xảy ra trong không gian tiềm ẩn độ phân giải tiêu chuẩn, với SDXL chỉ liên quan đến số lượng bước hạn chế cho tiềm ẩn độ phân giải cao, cách tiếp cận này hiệu quả hơn so với cách tiếp cận trước.
Chúng tôi so sánh việc tạo ra cho cùng một cảnh với baseline SDXL và cả hai cách tiếp cận trong Hình H.1. Cả hai cách tiếp cận đều thể hiện khả năng tuân theo prompt tốt hơn nhiều so với baseline SDXL. Chúng tôi quan sát thấy chất lượng tạo ra tương tự trên cả hai cách tiếp cận. Do đó, chúng tôi sử dụng cách tiếp cận sau theo mặc định.
Đối với Hình 1 và Hình 7, chúng tôi sử dụng SDXL 1.0 làm mô hình cơ bản của LMD và so sánh với SDXL như một baseline mạnh. Đối với tất cả các thiết lập khác, bao gồm thiết lập đánh giá định tính, chúng tôi sử dụng Stable Diffusion

--- TRANG 24 ---
(a) VisualChatGPT (Baseline)
(cuộc trò chuyện tiếp tục bên phải)
(b) GILL (Baseline)
(cuộc trò chuyện tiếp tục bên phải)

Hình G.1: VisualChatGPT Wu et al. (2023) và GILL Koh et al. (2023) nói chung không thể xử lý nhiều hơn một vòng yêu cầu tạo hình ảnh, với hình ảnh được tạo ra bị suy giảm bắt đầu từ yêu cầu thứ hai. Ngược lại, phương pháp của chúng tôi có thể xử lý một số vòng yêu cầu tuần tự về tạo hình ảnh mà không có sự suy giảm tạo ra, được hiển thị trong Hình 6.

v1.5 (được ký hiệu là SDv1) trừ khi được nêu khác. Để so sánh công bằng với Wu et al. (2023) và Koh et al. (2023) chỉ sử dụng Stable Diffusion v1.5, chúng tôi cũng tạo ra hình ảnh cho cùng một tập hợp prompt của Hình 1 với Stable Diffusion v1.5 trong Hình F.1.
I Tạo hình ảnh từ ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán bên dưới
Như được hiển thị trong Hình I.1, bằng cách yêu cầu LLM luôn đưa ra bố cục bằng tiếng Anh ngay cả khi prompt không phải tiếng Anh (ví dụ: tiếng Hàn hoặc tiếng Trung như trong Hình I.1) và cung cấp một ví dụ trong ngữ cảnh của đầu vào không phải tiếng Anh và bố cục tiếng Anh, LMD có thể tạo ra hình ảnh từ các prompt bằng ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán bên dưới

--- TRANG 25 ---
(a) Baseline SDXL
(b) Áp dụng LMD trực tiếp trên SDXL
(c) Áp dụng LMD trên SD ở độ phân giải thấp và tinh chỉnh với SDXL ở độ phân giải cao

Hình H.1: LMD có thể được áp dụng dễ dàng trên mô hình stable diffusion mới nhất SDXL (Podell et al., 2023). Chúng tôi so sánh các hình ảnh được tạo ra từ prompt văn bản "Một bức ảnh thực tế của một con mèo xám và một con chó cam trên cỏ". (a) trực tiếp tạo ra hình ảnh từ prompt văn bản. SDXL không tạo ra chính xác hình ảnh từ prompt, cho thấy rằng đơn giản mở rộng mô hình khuếch tán không nhất thiết dẫn đến khả năng tuân theo prompt cải thiện. (b) Nhờ vào phương pháp của chúng tôi không cần huấn luyện, phương pháp của chúng tôi có thể được áp dụng trực tiếp trên SDXL mà không cần huấn luyện thêm. (c) Một cách thay thế để tích hợp phương pháp của chúng tôi với SDXL là sử dụng phương pháp của chúng tôi để tạo ra hình ảnh độ phân giải thấp với SD và sau đó tinh chỉnh hình ảnh ở độ phân giải cao trong SDXL. Vì hầu hết việc khử nhiễu được hoàn thành trong tiềm ẩn độ phân giải thấp, cách tiếp cận này hiệu quả hơn.

Stable Diﬀusion
LMD (Của chúng tôi)一个室内场景的水彩画，一个桌子上面放着一盘水果
(Bản dịch: Một bức tranh màu nước của một cảnh trong nhà với một đĩa trái cây được đặt trên bàn.)*
Hỗ trợ nhiều ngôn ngữ prompt hơn
* Bản dịch không phải là một phần của đầu vào cho phương pháp của chúng tôi.

Stable Diﬀusion
LMD (Của chúng tôi)실내 장면의 수채화, 테이블 위에 놓인 과일 접시가 있는 사실적인 그림.
(Bản dịch: Một bức tranh màu nước của một cảnh trong nhà với một đĩa trái cây được đặt trên bàn.)*
Hỗ trợ nhiều ngôn ngữ prompt hơn
* Bản dịch không phải là một phần của đầu vào cho phương pháp của chúng tôi.

Hình I.1: Bằng cách yêu cầu LLM luôn đưa ra bố cục bằng tiếng Anh, LMD tự nhiên có thể tạo ra hình ảnh từ các prompt bằng ngôn ngữ không được hỗ trợ bởi mô hình khuếch tán bên dưới.

diffusion model. Chúng tôi đơn giản dịch đầu vào prompt của ví dụ cuối cùng trong ngữ cảnh sang ngôn ngữ mong muốn, trong khi giữ đầu ra của nó bằng tiếng Anh. Không cần điều chỉnh nào trên mô hình khuếch tán vì mô hình khuếch tán bên dưới vẫn nhận một bố cục tiếng Anh làm đầu vào.
J Chi tiết cho các benchmark văn bản-thành-hình ảnh
Chúng tôi chọn 10 loại đối tượng phổ biến từ bộ dữ liệu COCO Lin et al. (2014) để tạo ra7.
Đối với nhiệm vụ phủ định và khả năng tính toán sinh sản, mỗi prompt yêu cầu mô hình tạo ra một bố cục của một cảnh với một số lượng nhất định của một đối tượng cụ thể hoặc không có một đối tượng cụ thể. Sau đó chúng tôi đếm số lượng đối tượng và coi bố cục là chính xác nếu số lượng đối tượng của loại cụ thể đó khớp với số trong prompt, với số lượng từ 1 đến 5.
7Backpack, book, bottle, bowl, car, cat, chair, cup, dog, và laptop.

--- TRANG 26 ---
Mục tiêu cho mỗi prompt trong nhiệm vụ liên kết thuộc tính là tạo ra một đối tượng của một màu và một đối tượng khác của một màu khác, mà việc đánh giá tương tự như các nhiệm vụ khác.
Đối với nhiệm vụ mối quan hệ không gian, chúng tôi tạo ra một đối tượng tại một vị trí cụ thể và một đối tượng khác tại một vị trí đối diện (trái/phải và trên/dưới). Sau đó chúng tôi kiểm tra tọa độ không gian của các hộp để đảm bảo bố cục khớp chính xác với prompt. Trong mỗi nhiệm vụ, chúng tôi tạo ra 100 prompt văn bản, với tổng cộng 400 prompt văn bản.
Prompt. Đối với benchmark phủ định, chúng tôi sử dụng prompt Một bức ảnh thực tế của một cảnh không có [tên đối tượng].
Đối với khả năng tính toán sinh sản, chúng tôi sử dụng prompt Một bức ảnh thực tế của một cảnh với [số] [tên đối tượng].
Đối với gán thuộc tính, chúng tôi sử dụng prompt Một bức ảnh thực tế của một cảnh với [modifier 1] [tên đối tượng 1] và [modifier 2] [tên đối tượng 2], trong đó hai modifier được chọn ngẫu nhiên từ một danh sách màu sắc (đỏ, cam, vàng, xanh lá, xanh dương, tím, hồng, nâu, đen, trắng và xám).
Đối với benchmark mối quan hệ không gian, chúng tôi sử dụng prompt Một bức ảnh thực tế của một cảnh với [tên đối tượng 1] ở [vị trí] và [modifier 2] [tên đối tượng 2] ở [vị trí đối diện], trong đó vị trí được chọn từ trái, phải, trên và dưới.
Chi tiết triển khai. Đối với LMD, chúng tôi sử dụng Stable Diffusion v1.5 theo mặc định. Đối với LMD+, chúng tôi sử dụng mô hình GLIGEN (Li et al., 2023b) mà không cần huấn luyện hoặc điều chỉnh thêm. Chúng tôi chọn mô hình GLIGEN (Li et al., 2023b) được huấn luyện dựa trên Stable Diffusion v1.4, là mô hình mới nhất tại thời điểm viết. Chúng tôi sử dụng η = 5, λ = 2.0, r = 0.4, guidance scale 7.5. Việc tối thiểu hóa năng lượng được lặp lại 5 lần cho mỗi timestep khử nhiễu và giảm tuyến tính cho mỗi năm bước khử nhiễu cho đến khi việc lặp lại giảm xuống 1, và chúng tôi không thực hiện hướng dẫn sau 30 bước. k trong Topk(·) trong Phương trình (2) được đặt thành 20% diện tích của mặt nạ cho mỗi mặt nạ. Phần nền (thuật ngữ thứ hai) của Phương trình (2) được đánh trọng số bởi ω = 4.0. Chúng tôi chạy quá trình khử nhiễu với 50 bước theo mặc định. Chúng tôi chỉ thực hiện latent compose trong nửa đầu của quá trình khử nhiễu (25 bước đầu tiên). Các trực quan hóa định tính/so sánh định lượng được tạo ra bởi LMD+/LMD, tương ứng, theo mặc định trừ khi được nêu khác.
K Prompt LLM của chúng tôi
Prompt LLM của chúng tôi được liệt kê trong Bảng K.1. Các ví dụ trong ngữ cảnh của chúng tôi được liệt kê trong Bảng K.2.

--- TRANG 27 ---
1Bạn là một bộ tạo hộp giới hạn thông minh. Tôi sẽ cung cấp cho bạn một chú thích cho một bức ảnh, hình ảnh hoặc bức tranh. Nhiệm vụ của bạn là tạo ra các hộp giới hạn cho các đối tượng được đề cập trong chú thích, cùng với một prompt nền mô tả cảnh. Các hình ảnh có kích thước 512 x 512. Góc trên-trái có tọa độ [0, 0]. Góc dưới-phải có tọa độ [512, 512]. Các hộp giới hạn không nên chồng lấp hoặc vượt quá ranh giới hình ảnh. Mỗi hộp giới hạn nên ở định dạng (tên đối tượng, [tọa độ x góc trên-trái, tọa độ y góc trên-trái, chiều rộng hộp, chiều cao hộp]) và không nên bao gồm nhiều hơn một đối tượng. Không đưa các đối tượng đã được cung cấp trong các hộp giới hạn vào prompt nền. Không bao gồm các đối tượng không tồn tại hoặc bị loại trừ trong prompt nền. Sử dụng "Một cảnh thực tế" làm prompt nền nếu không có nền nào được đưa ra trong prompt. Nếu cần, bạn có thể đưa ra những phỏng đoán hợp lý. Vui lòng tham khảo ví dụ dưới đây để biết định dạng mong muốn.
2
3[Ví dụ trong ngữ cảnh]
4
5Chú thích: [Prompt người dùng]
6Đối tượng:

Bảng K.1: Prompt đầy đủ của chúng tôi cho LLM để tạo bố cục. LLM bắt đầu hoàn thành từ "Đối tượng: ".

1Chú thích: Một hình ảnh thực tế của cảnh phong cảnh mô tả một chiếc xe hơi xanh lá đậu bên trái một chiếc xe tải xanh dương, với một quả bóng bay đỏ và một con chim trên trời
2Đối tượng: [('một chiếc xe hơi xanh lá', [21, 281, 211, 159]), ('một chiếc xe tải xanh dương', [269, 283, 209, 160]), ('một quả bóng bay đỏ', [66, 8, 145, 135]), ('một con chim', [296, 42, 143, 100])]
3Prompt nền: Một cảnh phong cảnh thực tế
4Prompt phủ định:
5
6Chú thích: Một góc nhìn từ trên xuống thực tế của một bàn gỗ với hai quả táo trên đó
7Đối tượng: [('một bàn gỗ', [20, 148, 472, 216]), ('một quả táo', [150, 226, 100, 100]), ('một quả táo', [280, 226, 100, 100])]
8Prompt nền: Một góc nhìn từ trên xuống thực tế
9Prompt phủ định:
10
11Chú thích: Một cảnh thực tế của ba người trượt tuyết đứng thành hàng trên tuyết gần một cây cọ
12Đối tượng: [('một người trượt tuyết', [5, 152, 139, 168]), ('một người trượt tuyết', [278, 192, 121, 158]), ('một người trượt tuyết', [148, 173, 124, 155]), ('một cây cọ', [404, 105, 103, 251])]
13Prompt nền: Một cảnh ngoài trời thực tế có tuyết
14Prompt phủ định:
15
16Chú thích: Một bức tranh dầu của một con cá heo hồng nhảy bên trái một thuyền hơi nước trên biển
17Đối tượng: [('một thuyền hơi nước', [232, 225, 257, 149]), ('một con cá heo hồng đang nhảy', [21, 249, 189, 123])]
18Prompt nền: Một bức tranh dầu của biển
19Prompt phủ định:
20
21Chú thích: Một con mèo dễ thương và một con chó tức giận không có chim
22Đối tượng: [('một con mèo dễ thương', [51, 67, 271, 324]), ('một con chó tức giận', [302, 119, 211, 228])]
23Prompt nền: Một cảnh thực tế
24Prompt phủ định: chim
25
26Chú thích: Hai con gấu trúc trong rừng không có hoa
27Đối tượng: [('một con gấu trúc', [30, 171, 212, 226]), ('một con gấu trúc', [264, 173, 222, 221])]
28Prompt nền: Một khu rừng
29Prompt phủ định: hoa
30
31Chú thích: Một bức tranh dầu của một cảnh phòng khách không có ghế với một bức tranh treo trên tường, một tủ bên dưới bức tranh, và hai lọ hoa trên tủ
32Đối tượng: [('một bức tranh', [88, 85, 335, 203]), ('một tủ', [57, 308, 404, 201]), ('một lọ hoa', [166, 222, 92, 108]), ('một lọ hoa', [328, 222, 92, 108])]
33Prompt nền: Một bức tranh dầu của một cảnh phòng khách
34Prompt phủ định: ghế

Bảng K.2: Các ví dụ trong ngữ cảnh của chúng tôi. Chúng tôi sử dụng các ví dụ trong ngữ cảnh cố định để tạo bố cục.
