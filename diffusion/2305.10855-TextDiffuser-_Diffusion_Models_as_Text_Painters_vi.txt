# 2305.10855.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2305.10855.pdf
# Kích thước tệp: 30140577 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TextDiffuser: Các Mô Hình Khuếch Tán như Họa Sĩ Vẽ Chữ
Jingye Chen∗13, Yupan Huang∗23, Tengchao Lv3, Lei Cui3, Qifeng Chen1, Furu Wei3
1HKUST2Đại học Trung Sơn3Microsoft Research
qwerty.chen@connect.ust.hk, huangyp28@mail2.sysu.edu.cn, cqf@ust.hk
{tengchaolv,lecu,fuwei}@microsoft.com
Tóm tắt
Các mô hình khuếch tán đã nhận được sự chú ý ngày càng tăng nhờ khả năng tạo ấn tượng nhưng hiện tại vẫn gặp khó khăn trong việc hiển thị văn bản chính xác và mạch lạc. Để giải quyết vấn đề này, chúng tôi giới thiệu TextDiffuser, tập trung vào việc tạo ra các hình ảnh với văn bản hấp dẫn về mặt thị giác và mạch lạc với nền. TextDiffuser bao gồm hai giai đoạn: đầu tiên, một mô hình Transformer tạo ra bố cục của các từ khóa được trích xuất từ các lời nhắc văn bản, sau đó các mô hình khuếch tán tạo ra hình ảnh dựa trên lời nhắc văn bản và bố cục đã tạo. Ngoài ra, chúng tôi đóng góp bộ dữ liệu hình ảnh văn bản quy mô lớn đầu tiên với chú thích OCR, MARIO-10M, chứa 10 triệu cặp hình ảnh-văn bản với chú thích nhận dạng văn bản, phát hiện và phân đoạn cấp ký tự. Chúng tôi tiếp tục thu thập điểm chuẩn MARIO-Eval để phục vụ như một công cụ toàn diện để đánh giá chất lượng hiển thị văn bản. Thông qua các thí nghiệm và nghiên cứu người dùng, chúng tôi cho thấy TextDiffuser linh hoạt và có thể kiểm soát để tạo ra hình ảnh văn bản chất lượng cao chỉ sử dụng lời nhắc văn bản hoặc kết hợp với hình ảnh mẫu văn bản, và thực hiện inpainting văn bản để tái tạo các hình ảnh không hoàn chỉnh với văn bản. Mã nguồn, mô hình và bộ dữ liệu sẽ có sẵn tại https://aka.ms/textdiffuser .
(a) Văn bản-thành-Hình ảnh (b) Văn bản-thành-Hình ảnh với Mẫu
(c) Inpainting Văn bản
Một con chó ... Một phi hành gia ...Một con mèo cầm 
một tờ giấy viết
"Hello World"
Một con khủng long ...tạo bố cục
tạo hình ảnh
bookmark công ty
bánh pizza
váy báo mặt nạ mẫu
một cậu bé vẽ chào buổi sáng lên bảng một siêu thị tên dif fusion
Hình 1: TextDiffuser tạo ra các hình ảnh văn bản chính xác và mạch lạc từ lời nhắc văn bản hoặc kết hợp với hình ảnh mẫu, cũng như thực hiện inpainting văn bản để tái tạo các hình ảnh không hoàn chỉnh.
∗Đóng góp bằng nhau trong thời gian thực tập tại Microsoft Research.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2023).arXiv:2305.10855v5 [cs.CV] 30 Tháng 10 2023

--- TRANG 2 ---
1 Giới thiệu
Lĩnh vực tạo hình ảnh đã chứng kiến tiến bộ to lớn với sự ra đời của các mô hình khuếch tán
[2,15,16,18,25,67,70,72,79,93] và sự có sẵn của các bộ dữ liệu ghép cặp hình ảnh-văn bản quy mô lớn
[17,74,75]. Tuy nhiên, các mô hình khuếch tán hiện tại vẫn đối mặt với thách thức trong việc tạo ra văn bản hấp dẫn về mặt thị giác trên hình ảnh, và hiện tại chưa có bộ dữ liệu chuyên biệt quy mô lớn cho mục đích này. Khả năng của các mô hình AI tạo ra văn bản chính xác và mạch lạc trên hình ảnh là rất quan trọng, xét đến việc sử dụng rộng rãi các hình ảnh văn bản ở nhiều dạng khác nhau (ví dụ: poster, bìa sách, meme, v.v.) và khó khăn trong việc tạo ra hình ảnh văn bản chất lượng cao, thường đòi hỏi kỹ năng chuyên nghiệp và nhiều lần thử của các nhà thiết kế.

Các giải pháp truyền thống để tạo hình ảnh văn bản bao gồm việc sử dụng các công cụ xử lý hình ảnh như Photoshop để thêm văn bản trực tiếp lên hình ảnh. Tuy nhiên, những cách này thường tạo ra các tạo tác không tự nhiên do kết cấu phức tạp của nền hoặc biến đổi ánh sáng. Các nỗ lực gần đây đã sử dụng các mô hình khuếch tán để vượt qua các hạn chế của phương pháp truyền thống và nâng cao chất lượng hiển thị văn bản. Ví dụ, Imagen [72], eDiff-I [2], và DeepFloyd [12] quan sát thấy các mô hình khuếch tán tạo ra văn bản tốt hơn với các bộ mã hóa văn bản chuỗi T5 [61] so với bộ mã hóa văn bản CLIP [60]. Liu et al. sử dụng các bộ mã hóa văn bản nhận biết ký tự để cải thiện hiển thị văn bản [46]. Mặc dù có một số thành công, các mô hình này chỉ tập trung vào các bộ mã hóa văn bản, thiếu khả năng kiểm soát quá trình tạo. Một công trình đồng thời, GlyphDraw [49], cải thiện khả năng kiểm soát của các mô hình bằng cách điều kiện hóa vào vị trí và cấu trúc của các ký tự Trung Quốc. Tuy nhiên, GlyphDraw không hỗ trợ tạo hộp giới hạn văn bản đa dạng, điều này không áp dụng được cho nhiều hình ảnh văn bản như poster và bìa sách.

Trong bài báo này, chúng tôi đề xuất TextDiffuser, một khung làm việc linh hoạt và có thể kiểm soát dựa trên các mô hình khuếch tán. Khung làm việc bao gồm hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi sử dụng Layout Transformer để xác định tọa độ của từng từ khóa trong lời nhắc văn bản và thu được mặt nạ phân đoạn cấp ký tự. Trong giai đoạn thứ hai, chúng tôi tinh chỉnh mô hình khuếch tán tiềm ẩn bằng cách tận dụng các mặt nạ phân đoạn đã tạo làm điều kiện cho quá trình khuếch tán và lời nhắc văn bản. Chúng tôi giới thiệu tổn thất nhận biết ký tự trong không gian tiềm ẩn để tiếp tục cải thiện chất lượng của các vùng văn bản đã tạo. Hình 1 minh họa ứng dụng của TextDiffuser trong việc tạo ra hình ảnh văn bản chính xác và mạch lạc chỉ sử dụng lời nhắc văn bản hoặc hình ảnh mẫu văn bản. Ngoài ra, TextDiffuser có khả năng thực hiện inpainting văn bản2 để tái tạo các hình ảnh không hoàn chỉnh với văn bản. Để huấn luyện mô hình của chúng tôi, chúng tôi sử dụng các công cụ OCR và thiết kế các chiến lược lọc để thu được 10 triệu cặp hình ảnh-văn bản chất lượng cao với chú thích OCR (được gọi là MARIO-10M), mỗi cặp có chú thích nhận dạng, phát hiện và phân đoạn cấp ký tự. Các thí nghiệm mở rộng và nghiên cứu người dùng chứng minh sự vượt trội của TextDiffuser được đề xuất so với các phương pháp hiện tại trên điểm chuẩn MARIO-Eval đã xây dựng. Mã nguồn, mô hình và bộ dữ liệu sẽ được công bố để thúc đẩy nghiên cứu trong tương lai.

2 Công trình liên quan
Hiển thị Văn bản. Tạo hình ảnh đã đạt được tiến bộ đáng kể với sự ra đời của các mô hình khuếch tán [18,25,67,72,79,63,70,2,8,13,52,48,26,80], đạt kết quả hiện đại so với các phương pháp dựa trên GAN trước đây [64,100,43,58]. Mặc dù phát triển nhanh chóng, các phương pháp hiện tại vẫn gặp khó khăn với việc hiển thị văn bản chính xác và mạch lạc. Để giảm thiểu điều này, Imagen [72], eDiff-I [2], và DeepFolyd [12] sử dụng mô hình ngôn ngữ quy mô lớn (T5 lớn [61]) để tăng cường kiến thức chính tả văn bản. Trong [46], các tác giả nhận thấy rằng các bộ mã hóa văn bản hiện tại mù với độ dài token và huấn luyện một biến thể nhận biết ký tự để giảm thiểu vấn đề này. Một công trình đồng thời, GlyphDraw [49], tập trung vào việc tạo ra hình ảnh chất lượng cao với văn bản Trung Quốc với sự hướng dẫn của vị trí văn bản và hình ảnh glyph. Không giống như công trình này, chúng tôi sử dụng Transformer [81] để thu được bố cục của các từ khóa, cho phép tạo ra văn bản trên nhiều dòng. Bên cạnh đó, chúng tôi sử dụng mặt nạ phân đoạn cấp ký tự làm prior, có thể dễ dàng kiểm soát (ví dụ: bằng cách cung cấp hình ảnh mẫu) để đáp ứng nhu cầu của người dùng.

Một số bài báo đã đưa ra các điểm chuẩn chứa một vài trường hợp liên quan đến hiển thị văn bản để đánh giá. Ví dụ, Imagen [72] giới thiệu DrawBench chứa 200 lời nhắc, trong đó 21 lời nhắc liên quan đến hiển thị văn bản thị giác (ví dụ: Một cửa hàng với 'Hello World' viết trên đó). Theo [46], các tác giả đề xuất DrawText bao gồm 175 lời nhắc sáng tạo (ví dụ: chữ cái 'c' làm từ xương rồng, ảnh chất lượng cao). GlyphDraw [49] thiết kế 218 lời nhắc bằng tiếng Trung Quốc và tiếng Anh (ví dụ: Logo cho một chuỗi cửa hàng tạp hóa với tên 'Grocery'). Xem xét rằng các điểm chuẩn hiện tại chỉ chứa một số lượng hạn chế các trường hợp, chúng tôi cố gắng thu thập thêm lời nhắc và kết hợp chúng với các lời nhắc hiện có để thiết lập điểm chuẩn lớn hơn MARIO-Eval để tạo điều kiện cho các so sánh toàn diện cho công việc tương lai.

2Khác với chỉnh sửa văn bản [88,37,32], nhiệm vụ inpainting văn bản được giới thiệu nhằm thêm hoặc sửa đổi văn bản được hướng dẫn bởi người dùng, đảm bảo rằng văn bản inpainted có phong cách hợp lý (tức là không cần khớp chính xác phong cách của văn bản gốc trong quá trình sửa đổi) và mạch lạc với nền.

--- TRANG 3 ---
Inpainting Hình ảnh. Inpainting hình ảnh là nhiệm vụ tái tạo các vùng bị thiếu trong hình ảnh một cách tự nhiên và mạch lạc. Nghiên cứu ban đầu tập trung vào việc tận dụng cấu trúc hình ảnh và kết cấu mức thấp để giải quyết nhiệm vụ này [3,6,5]. Sau đó, các kiến trúc học sâu như auto-encoder [55,45], GAN [65,98], VAE [103,105], và Transformer tự hồi quy [57,83,96] đã được áp dụng để giải quyết vấn đề này. Gần đây, các mô hình khuếch tán đã được sử dụng để tạo ra kết quả chất lượng cao và đa dạng cho inpainting hình ảnh vô điều kiện [71,48,67,11,99], inpainting hình ảnh có điều kiện văn bản [52,1] và inpainting hình ảnh có điều kiện hình ảnh [92]. Công trình của chúng tôi thuộc danh mục inpainting hình ảnh có điều kiện văn bản sử dụng các mô hình khuếch tán. Trái ngược với các công trình trước đó tập trung vào hoàn thành hình ảnh với nền tự nhiên hoặc đối tượng, phương pháp của chúng tôi tập trung vào hoàn thành hình ảnh với hiển thị liên quan đến văn bản, cũng được gọi là inpainting văn bản, bằng cách điều kiện hóa thêm trên mặt nạ phân đoạn cấp ký tự.

Nhận dạng Ký tự Quang học. Nhận dạng Ký tự Quang học (OCR) là một nhiệm vụ quan trọng đã được nghiên cứu trong học thuật trong thời gian dài [87,7]. Nó đã trải qua sự phát triển đáng kể trong thập kỷ qua, đóng góp vào nhiều ứng dụng như lái xe tự động [89,73], nhận dạng biển số xe [101,53], mô hình GPT [76,28], v.v. Các bộ dữ liệu khác nhau [31,19,90,91] và các nhiệm vụ downstream được bao gồm trong lĩnh vực này, như nhận dạng hình ảnh văn bản [77,41,95,14], phát hiện [106,51,42,84], phân đoạn [90,91,107,68], siêu phân giải [9,85,50,104], cũng như một số nhiệm vụ tạo, bao gồm chỉnh sửa hình ảnh văn bản [88,78,94,59,38], tạo bố cục tài liệu [56,22,20,40,33], tạo phông chữ [30,21,36,54], v.v. Trong số đó, nhiệm vụ tạo phông chữ có liên quan nhất đến nhiệm vụ của chúng tôi. Tạo phông chữ nhằm tạo ra các phông chữ chất lượng cao, hấp dẫn về mặt thẩm mỹ dựa trên các hình ảnh ký tự đã cho. Ngược lại, nhiệm vụ của chúng tôi khó khăn hơn vì nó đòi hỏi văn bản được tạo phải có thể đọc được, hấp dẫn về mặt thị giác và mạch lạc với nền trong các tình huống khác nhau.

3 Phương pháp
Như minh họa trong Hình 2, TextDiffuser bao gồm hai giai đoạn: Tạo Bố cục và Tạo Hình ảnh. Chúng tôi sẽ chi tiết hai giai đoạn và giới thiệu quá trình suy luận tiếp theo.

3.1 Giai đoạn 1: Tạo Bố cục
Trong giai đoạn này, mục tiêu là sử dụng các hộp giới hạn để xác định bố cục của các từ khóa (được bao quanh bởi dấu ngoặc kép được chỉ định bởi lời nhắc của người dùng). Lấy cảm hứng từ Layout Transformer [20], chúng tôi sử dụng kiến trúc Transformer để thu được bố cục của các từ khóa. Hình thức, chúng tôi ký hiệu lời nhắc được token hóa là P= (p0, p1, ..., pL−1), trong đó L có nghĩa là độ dài tối đa của các token. Theo LDM [67], chúng tôi sử dụng CLIP [60] và hai lớp tuyến tính để mã hóa chuỗi thành CLIP(P)∈RL×d, trong đó d là chiều của không gian tiềm ẩn. Để phân biệt các từ khóa với các từ khác, chúng tôi thiết kế embedding từ khóa Key(P)∈RL×d với hai mục (tức là từ khóa và không phải từ khóa). Hơn nữa, chúng tôi mã hóa độ rộng của các từ khóa với một lớp embedding Width(P)∈RL×d. Cùng với embedding vị trí có thể học Pos(P)∈RL×d được giới thiệu trong [81], chúng tôi xây dựng toàn bộ embedding như sau:
Embedding(P) = CLIP(P) + Pos(P) + Key(P) + Width(P). (1)

Embedding được xử lý thêm với bộ mã hóa dựa trên Transformer l-lớp ΦE và bộ giải mã ΦD để thu được các hộp giới hạn B∈RK×4 của K từ khóa một cách tự hồi quy:
B = ΦD(ΦE(Embedding(P))) = (b0,b1, ...,bK−1). (2)

Cụ thể, chúng tôi sử dụng embedding vị trí làm truy vấn cho bộ giải mã Transformer ΦD, đảm bảo rằng truy vấn thứ n tương ứng với từ khóa thứ n trong lời nhắc. Mô hình được tối ưu hóa với tổn thất l1, cũng được ký hiệu là |BGT−B| trong đó BGT là ground truth. Hơn nữa, chúng tôi có thể sử dụng một số gói Python như Pillow để render văn bản và đồng thời thu được mặt nạ phân đoạn cấp ký tự C với |A| kênh, trong đó |A| ký hiệu kích thước của bảng chữ cái A. Đến đây, chúng tôi thu được bố cục của các từ khóa và quá trình tạo hình ảnh được giới thiệu tiếp theo.

--- TRANG 4 ---
Transformer
Encoder
một con mèo cầm hello worldEmbedding
LayerTokenizeTransformer
Decoder
query0 query1box0 box1
Hello
WorldRender
Get Mask
Giai đoạn 1 Tạo Bố cục
Giai đoạn 2 Tạo Hình ảnh
Tạo Toàn bộ Hình ảnh
Tạo Một phần Hình ảnh
Mô hình
Khuếch tán
Lời nhắc Văn bản
U-Net
Tổn thất
Khử nhiễu
Tổn thất
Nhận biết
Ký tự
Embedding
Layer
Hình 2: TextDiffuser bao gồm hai giai đoạn. Trong giai đoạn Tạo Bố cục đầu tiên, mô hình mã hóa-giải mã dựa trên Transformer tạo ra mặt nạ phân đoạn cấp ký tự cho biết bố cục của các từ khóa trong hình ảnh từ lời nhắc văn bản. Trong giai đoạn Tạo Hình ảnh thứ hai, mô hình khuếch tán tạo ra hình ảnh dựa trên các đặc trưng nhiễu, mặt nạ phân đoạn, mặt nạ đặc trưng, và đặc trưng đã che (từ trái sang phải) cùng với lời nhắc văn bản. Mặt nạ đặc trưng có thể che toàn bộ hoặc một phần hình ảnh, tương ứng với tạo toàn bộ hình ảnh và tạo một phần hình ảnh. Mô hình khuếch tán học cách khử nhiễu đặc trưng từng bước với tổn thất khử nhiễu và nhận biết ký tự. Lưu ý rằng mô hình khuếch tán hoạt động trong không gian tiềm ẩn, nhưng chúng tôi sử dụng pixel hình ảnh để hình dung tốt hơn.

3.2 Giai đoạn 2: Tạo Hình ảnh
Trong giai đoạn này, chúng tôi nhằm tạo ra hình ảnh được hướng dẫn bởi mặt nạ phân đoạn C được sản xuất trong giai đoạn đầu tiên. Chúng tôi sử dụng VAE [35] để mã hóa hình ảnh gốc với hình dạng H×W thành đặc trưng không gian tiềm ẩn 4-D F∈R4×H′×W′. Sau đó chúng tôi lấy mẫu một bước thời gian T∼Uniform(0, Tmax) và lấy mẫu một nhiễu Gaussian ϵ∈R4×H′×W′ để làm nhiễu đặc trưng gốc, tạo ra ˆF=√¯αTF+√1−¯αTϵ trong đó ¯αT là hệ số của quá trình khuếch tán được giới thiệu trong [25]. Chúng tôi cũng giảm mẫu mặt nạ phân đoạn cấp ký tự C với ba lớp tích chập, tạo ra 8-D ˆC∈R8×H′×W′. Chúng tôi cũng giới thiệu hai đặc trưng bổ sung, được gọi là mặt nạ đặc trưng 1-D ˆM∈R1×H′×W′ và đặc trưng đã che 4-D ˆFM∈R4×H′×W′. Trong quá trình tạo toàn bộ hình ảnh, ˆM được đặt để che tất cả các vùng của đặc trưng và ˆFM là đặc trưng của một hình ảnh bị che hoàn toàn. Trong quá trình tạo một phần hình ảnh (cũng được gọi là inpainting văn bản), mặt nạ đặc trưng ˆM đại diện cho vùng mà người dùng muốn tạo, trong khi đặc trưng đã che ˆFM chỉ ra vùng mà người dùng muốn bảo tồn. Để đồng thời huấn luyện hai nhánh, chúng tôi sử dụng chiến lược che mà một mẫu bị che hoàn toàn với xác suất σ và bị che một phần với xác suất 1−σ. Chúng tôi nối ˆF,ˆC,ˆM,ˆFM trong kênh đặc trưng như một đầu vào 17-D và sử dụng tổn thất khử nhiễu giữa nhiễu đã lấy mẫu ϵ và nhiễu dự đoán ϵθ:
ldenoising = ||ϵ−ϵθ(ˆF,ˆC,ˆM,ˆFM,P,T)||22. (3)

Hơn nữa, chúng tôi đề xuất tổn thất nhận biết ký tự để giúp mô hình tập trung hơn vào các vùng văn bản. Chi tiết, chúng tôi tiền huấn luyện một U-Net [69] có thể ánh xạ các đặc trưng tiềm ẩn thành mặt nạ phân đoạn cấp ký tự. Trong quá trình huấn luyện, chúng tôi cố định các tham số của nó và chỉ sử dụng nó để cung cấp hướng dẫn bằng cách sử dụng tổn thất cross-entropy lchar với trọng số λchar (Xem thêm chi tiết trong Phụ lục A). Nhìn chung, mô hình được tối ưu hóa với
l = ldenoising + λchar * lchar. (4)

Cuối cùng, các đặc trưng đầu ra được đưa vào bộ giải mã VAE để thu được hình ảnh.

--- TRANG 5 ---
(a) MARIO-LAION (b) MARIO-TMDB (c) MARIO-OpenLibrary
Hình 3: Minh họa ba tập con của MARIO-10M. Xem thêm chi tiết trong Phụ lục C.

3.3 Giai đoạn Suy luận
TextDiffuser cung cấp mức độ kiểm soát và tính linh hoạt cao trong quá trình suy luận theo các cách sau: (1) Tạo hình ảnh từ lời nhắc của người dùng. Đáng chú ý, người dùng có thể sửa đổi bố cục đã tạo hoặc chỉnh sửa văn bản để đáp ứng các yêu cầu cá nhân hóa của họ; (2) Người dùng có thể trực tiếp bắt đầu từ giai đoạn thứ hai bằng cách cung cấp hình ảnh mẫu (ví dụ: hình ảnh cảnh, hình ảnh viết tay, hoặc hình ảnh in), và một mô hình phân đoạn được tiền huấn luyện để thu được mặt nạ phân đoạn cấp ký tự (Phụ lục B); (3) Người dùng có thể sửa đổi các vùng văn bản của hình ảnh đã cho bằng cách sử dụng inpainting văn bản. Hơn nữa, hoạt động này có thể được thực hiện nhiều lần. Các kết quả thí nghiệm này sẽ được trình bày trong phần tiếp theo.

4 Bộ dữ liệu và Điểm chuẩn MARIO
Vì không có bộ dữ liệu quy mô lớn được thiết kế rõ ràng cho hiển thị văn bản, để giảm thiểu vấn đề này, chúng tôi thu thập 10 triệu cặp hình ảnh-văn bản với chú thích OCR để xây dựng Bộ dữ liệu MARIO-10M. Chúng tôi tiếp tục thu thập Điểm chuẩn MARIO-Eval từ tập con của tập kiểm tra MARIO-10M và các nguồn hiện có khác để phục vụ như một công cụ toàn diện để đánh giá chất lượng hiển thị văn bản.

4.1 Bộ dữ liệu MARIO-10M
MARIO-10M là một bộ sưu tập khoảng 10 triệu cặp hình ảnh-văn bản chất lượng cao và đa dạng từ các nguồn dữ liệu khác nhau như hình ảnh tự nhiên, poster và bìa sách. Hình 3 minh họa một số ví dụ từ bộ dữ liệu. Chúng tôi thiết kế các sơ đồ tự động và quy tắc lọc nghiêm ngặt để xây dựng chú thích và làm sạch dữ liệu nhiễu (thêm chi tiết trong Phụ lục D và Phụ lục E). Bộ dữ liệu chứa chú thích OCR toàn diện cho mỗi hình ảnh, bao gồm chú thích phát hiện, nhận dạng và phân đoạn cấp ký tự. Cụ thể, chúng tôi sử dụng DB [42] để phát hiện, PARSeq [4] để nhận dạng, và huấn luyện thủ công một U-Net [69] để phân đoạn. Chúng tôi phân tích hiệu suất của các công cụ OCR trong Phụ lục F. Tổng kích thước của MARIO-10M là 10.061.720, từ đó chúng tôi ngẫu nhiên chọn 10.000.000 mẫu làm tập huấn luyện và 61.720 làm tập kiểm tra. MARIO-10M được thu thập từ ba nguồn dữ liệu:

MARIO-LAION bắt nguồn từ bộ dữ liệu quy mô lớn LAION-400M [75]. Sau khi lọc, chúng tôi thu được 9.194.613 hình ảnh văn bản chất lượng cao với chú thích tương ứng. Bộ dữ liệu này bao gồm một loạt rộng các hình ảnh văn bản, bao gồm quảng cáo, ghi chú, poster, bìa, meme, logo, v.v.

MARIO-TMDB bắt nguồn từ Cơ sở dữ liệu Phim (TMDB), là một cơ sở dữ liệu được xây dựng bởi cộng đồng cho phim và chương trình TV với poster chất lượng cao. Chúng tôi lọc 343.423 poster tiếng Anh bằng API TMDB từ 759.859 mẫu đã thu thập. Vì mỗi hình ảnh không có chú thích sẵn có, chúng tôi sử dụng mẫu lời nhắc để xây dựng chú thích theo tiêu đề phim.

MARIO-OpenLibrary bắt nguồn từ Open Library, là một danh mục thư viện mở và có thể chỉnh sửa tạo ra trang web cho mỗi cuốn sách được xuất bản. Chúng tôi đầu tiên thu thập 6.352.989 bìa Open Library kích thước gốc theo lô. Sau đó, chúng tôi thu được 523.684 hình ảnh chất lượng cao hơn sau khi lọc. Giống như MARIO-TMDB, chúng tôi xây dựng chú thích thủ công bằng cách sử dụng tiêu đề do thiếu chú thích sẵn có.

--- TRANG 6 ---
4.2 Điểm chuẩn MARIO-Eval
Điểm chuẩn MARIO-Eval phục vụ như một công cụ toàn diện để đánh giá chất lượng hiển thị văn bản được thu thập từ tập con của tập kiểm tra MARIO-10M và các nguồn khác. Nó bao gồm 5.414 lời nhắc tổng cộng, bao gồm 21 lời nhắc từ DrawBenchText [72], 175 lời nhắc từ DrawTextCreative [46], 218 lời nhắc từ ChineseDrawText [49] và 5.000 cặp hình ảnh-văn bản từ tập con của tập kiểm tra MARIO-10M. 5.000 cặp hình ảnh-văn bản được chia thành ba bộ 4.000, 500, và 500 cặp, và được đặt tên LAIONEval4000, TMDBEval500, và OpenLibraryEval500 dựa trên các nguồn dữ liệu tương ứng. Chúng tôi cung cấp ví dụ trong Phụ lục G để hiểu rõ hơn về MARIO-Eval.

Tiêu chí Đánh giá: Chúng tôi đánh giá chất lượng hiển thị văn bản với MARIO-Eval từ bốn khía cạnh: (1) Fréchet Inception Distance (FID) [24] so sánh phân phối của hình ảnh đã tạo với phân phối của hình ảnh thực. (2) CLIPScore tính toán độ tương tự cosine giữa các biểu diễn hình ảnh và văn bản từ CLIP [29,60,23]. (3) Đánh giá OCR sử dụng các công cụ OCR hiện có để phát hiện và nhận dạng các vùng văn bản trong hình ảnh đã tạo. Độ chính xác, Precision, Recall, và F-measure là các chỉ số để đánh giá liệu các từ khóa có xuất hiện trong hình ảnh đã tạo hay không. (4) Đánh giá Con người được thực hiện bằng cách mời các nhà đánh giá con người chấm điểm chất lượng hiển thị văn bản của hình ảnh đã tạo bằng bảng câu hỏi. Thêm giải thích được hiển thị trong Phụ lục H.

5 Thí nghiệm

5.1 Chi tiết Triển khai
Cho giai đoạn đầu tiên, chúng tôi sử dụng CLIP [60] đã tiền huấn luyện để thu được embedding của lời nhắc đã cho. Số lượng lớp Transformer l được đặt thành 2, và chiều của không gian tiềm ẩn d được đặt thành 512. Độ dài tối đa của token L được đặt thành 77 theo CLIP [60]. Chúng tôi tận dụng phông chữ thường được sử dụng "Arial.ttf" và đặt kích thước phông chữ thành 24 để thu được embedding độ rộng và cũng sử dụng phông chữ này để render. Bảng chữ cái A bao gồm 95 ký tự, bao gồm 26 chữ cái in hoa, 26 chữ cái in thường, 10 chữ số, 32 dấu câu, và một ký tự khoảng trắng. Sau khi token hóa, chỉ subtoken đầu tiên được đánh dấu là từ khóa khi tồn tại nhiều subtoken cho một từ.

Cho giai đoạn thứ hai, chúng tôi triển khai quá trình khuếch tán bằng Hugging Face Diffusers [82] và tải checkpoint "runwayml/stable-diffusion-v1-5". Đáng chú ý, chúng tôi chỉ cần sửa đổi chiều đầu vào của lớp tích chập đầu vào (từ 4 thành 17), cho phép mô hình của chúng tôi có quy mô tham số và thời gian tính toán tương tự như mô hình gốc. Chi tiết, chiều cao H và W của hình ảnh đầu vào và đầu ra là 512. Cho quá trình khuếch tán, đầu vào có chiều không gian H′= 64 và W′= 64. Chúng tôi đặt kích thước batch thành 768 và huấn luyện mô hình trong hai epoch, mất bốn ngày sử dụng 8 GPU Tesla V100 với bộ nhớ 32GB. Chúng tôi sử dụng tối ưu hóa AdamW [47] và đặt tỷ lệ học thành 1e-5. Ngoài ra, chúng tôi sử dụng gradient checkpoint [10] và xformers [39] để hiệu quả tính toán. Trong quá trình huấn luyện, chúng tôi theo [25] để đặt bước thời gian tối đa Tmax thành 1.000, và chú thích được loại bỏ với xác suất 10% cho hướng dẫn không có classifier [27]. Khi huấn luyện nhánh tạo một phần hình ảnh, hộp văn bản đã phát hiện được che với khả năng 50%. Chúng tôi sử dụng 50 bước lấy mẫu trong quá trình suy luận và hướng dẫn không có classifier với thang đo 7.5 theo [67].

5.2 Nghiên cứu Cắt bỏ

Số lượng lớp Transformer và hiệu quả của embedding độ rộng. Chúng tôi tiến hành nghiên cứu cắt bỏ về số lượng lớp Transformer và liệu có sử dụng embedding độ rộng trong Layout Transformer hay không. Kết quả được hiển thị trong Bảng 1. Tất cả các mô hình cắt bỏ đều được huấn luyện trên tập huấn luyện của MARIO-10M và đánh giá trên tập kiểm tra của nó. Kết quả cho thấy việc thêm embedding độ rộng cải thiện hiệu suất, tăng IoU lên 2.1%, 2.9%, và 0.3% khi số lượng lớp Transformer l được đặt thành 1, 2, và 4 tương ứng. IoU tối ưu đạt được khi sử dụng hai lớp Transformer và embedding độ rộng được bao gồm. Xem thêm kết quả trực quan trong Phụ lục I.

Mặt nạ phân đoạn cấp ký tự cung cấp hướng dẫn rõ ràng để tạo ký tự. Mặt nạ phân đoạn cấp ký tự cung cấp hướng dẫn rõ ràng về vị trí và nội dung của ký tự trong quá trình tạo của TextDiffuser. Để xác thực hiệu quả của việc sử dụng mặt nạ phân đoạn cấp ký tự, chúng tôi huấn luyện các mô hình cắt bỏ mà không sử dụng mặt nạ và hiển thị kết quả trong Phụ lục

--- TRANG 7 ---
Bảng 1: Cắt bỏ về Layout Transformer.
#Lớp Width(P) IoU ↑
1 - 0.268
✓ 0.289
2 - 0.269
✓ 0.298
4 - 0.294
✓ 0.297

Bảng 2: Cắt bỏ về trọng số tổn thất nhận biết ký tự.
λchar Acc↑
0 0.396
0.001 0.486
0.01 0.494
0.1 0.420
1 0.400

Bảng 3: Cắt bỏ về tỷ lệ huấn luyện hai nhánh σ.
tỷ lệ Acc ↑/ Det-F ↑/ Spot-F ↑
0 0.344 / 0.870 / 0.663
0.25 0.562 / 0.899 / 0.636
0.5 0.552 / 0.881 / 0.715
0.75 0.524 / 0.921 / 0.695
1 0.494 / 0.380 / 0.218

Bảng 4: Hiệu suất của text-to-image so sánh với các phương pháp hiện có. TextDiffuser hoạt động tốt nhất về CLIPScore và đánh giá OCR trong khi đạt hiệu suất tương đương về FID.

Chỉ số StableDiffusion [67] ControlNet [102] DeepFloyd [12] TextDiffuser
FID↓ 51.295 51.485 34.902 38.758
CLIPScore ↑ 0.3015 0.3424 0.3267 0.3436
OCR(Accuracy) ↑ 0.0003 0.2390 0.0262 0.5609
OCR(Precision) ↑ 0.0173 0.5211 0.1450 0.7846
OCR(Recall) ↑ 0.0280 0.6707 0.2245 0.7802
OCR(F-measure) ↑ 0.0214 0.5865 0.1762 0.7824

J. Các văn bản được tạo ra không chính xác và không mạch lạc với nền so với văn bản được tạo với TextDiffuser, làm nổi bật tầm quan trọng của hướng dẫn rõ ràng.

Trọng số của tổn thất nhận biết ký tự. Kết quả thí nghiệm được thể hiện trong Bảng 2, nơi chúng tôi tiến hành thí nghiệm với λchar trong khoảng [0, 0.001, 0.01, 0.1, 1]. Chúng tôi sử dụng DrawBenchText [72] để đánh giá và sử dụng Microsoft Read API để phát hiện và nhận dạng văn bản trong hình ảnh đã tạo. Chúng tôi sử dụng Accuracy (Acc) làm chỉ số để đánh giá liệu các từ đã phát hiện có khớp chính xác với từ khóa hay không. Chúng tôi quan sát thấy hiệu suất tối ưu đạt được khi λchar được đặt thành 0.01, nơi điểm số tăng 9.8% so với baseline (λchar= 0).

Tỷ lệ huấn luyện của nhánh tạo toàn bộ/một phần hình ảnh. Chúng tôi khám phá tỷ lệ huấn luyện σ trong khoảng [0, 0.25, 0.5, 0.75, 1] và hiển thị kết quả trong Bảng 3. Khi σ được đặt thành 1, nó chỉ ra rằng chỉ nhánh toàn bộ hình ảnh được huấn luyện và ngược lại. Chúng tôi đánh giá mô hình bằng DrawBenchText [72] cho nhánh tạo toàn bộ hình ảnh. Cho nhánh tạo một phần hình ảnh, chúng tôi ngẫu nhiên chọn 1.000 mẫu từ tập kiểm tra của MARIO-10M và ngẫu nhiên che một số hộp văn bản đã phát hiện. Chúng tôi sử dụng Microsoft Read API để phát hiện và nhận dạng các hộp văn bản đã tái tạo trong hình ảnh đã tạo trong khi sử dụng F-measure của kết quả phát hiện văn bản và kết quả spotting làm chỉ số (được ký hiệu là Det-F và Spot-F tương ứng). Kết quả cho thấy khi tỷ lệ huấn luyện được đặt thành 50%, mô hình hoạt động tốt hơn trung bình (0.716).

5.3 Kết quả Thí nghiệm

Kết quả Định lượng. Cho nhiệm vụ tạo toàn bộ hình ảnh, chúng tôi so sánh phương pháp của chúng tôi với Stable Diffusion (SD) [67], ControlNet [102], và DeepFloyd [12] trong các thí nghiệm định lượng với các mã nguồn và mô hình được công bố được mô tả chi tiết trong Phụ lục K. DeepFloyd [12] sử dụng hai mô-đun siêu phân giải để tạo ra hình ảnh độ phân giải cao hơn 1024×1024 so với hình ảnh 512×512 được tạo bởi các phương pháp khác. Chúng tôi sử dụng bản đồ Canny của hình ảnh văn bản in được tạo với mô hình giai đoạn đầu tiên của chúng tôi làm điều kiện cho ControlNet [102]. Xin lưu ý rằng chúng tôi không thể so sánh với Imagen [72], eDiff-i [2], và GlyphDraw [49] do thiếu mã nguồn mở, checkpoint hoặc API. Theo Bảng 4, chúng tôi thể hiện kết quả định lượng của nhiệm vụ text-to-image so sánh với các phương pháp hiện có. TextDiffuser của chúng tôi đạt CLIPScore tốt nhất trong khi đạt hiệu suất tương đương về FID. Bên cạnh đó, TextDiffuser đạt hiệu suất tốt nhất về bốn chỉ số liên quan đến OCR. TextDiffuser vượt trội hơn những phương pháp không có hướng dẫn liên quan đến văn bản rõ ràng với biên độ lớn

--- TRANG 8 ---
Lời nhắc SD SD-XL Midjourney TextDiffuser
một con gấu cầm
bảng viết
'hello world'
một meme của
'Are you kidding'
một chiếc bánh của
'Happy Birthday
to XYZ'
ControlNet
một poster của
'Monkey Music
Festival'
DALL·E
DeepFloyd
một cuốn sách của
'AI in Next Century'
viết bởi
'AI Robot'
một cậu bé cầm 'P'
và
một cô gái cầm 'Q'

Hình 4: Trực quan hóa tạo toàn bộ hình ảnh so sánh với các phương pháp hiện có. Ba trường hợp đầu tiên được tạo từ lời nhắc và ba trường hợp cuối cùng từ hình ảnh mẫu in đã cho.

(ví dụ: 76,10% và 60,62% tốt hơn Stable Diffusion và DeepFloyd về F-measure), làm nổi bật tầm quan trọng của hướng dẫn rõ ràng. Đối với nhiệm vụ tạo một phần hình ảnh, chúng tôi không thể đánh giá phương pháp của chúng tôi vì không có phương pháp nào được thiết kế đặc biệt cho nhiệm vụ này mà chúng tôi biết.

Kết quả Định tính. Cho nhiệm vụ tạo toàn bộ hình ảnh, chúng tôi tiếp tục so sánh với DALL·E [63] mã nguồn đóng, Stable Diffusion XL (SD-XL), và Midjourney bằng cách hiển thị các ví dụ định tính được tạo với các dịch vụ API chính thức được mô tả chi tiết trong Phụ lục K. Hình 4 cho thấy một số hình ảnh được tạo từ lời nhắc hoặc hình ảnh văn bản in bởi các phương pháp khác nhau. Đáng chú ý, phương pháp của chúng tôi tạo ra văn bản có thể đọc được hơn, cũng mạch lạc với nền đã tạo. Ngược lại, mặc dù hình ảnh được tạo bởi SD-XL và Midjourney hấp dẫn về mặt thị giác, một số văn bản được tạo không chứa văn bản mong muốn hoặc chứa các ký tự không thể đọc được với nét vẽ sai. Kết quả cũng cho thấy mặc dù có tín hiệu giám sát mạnh được cung cấp cho ControlNet, nó vẫn gặp khó khăn trong việc tạo ra hình ảnh với văn bản chính xác phù hợp với nền. Chúng tôi cũng bắt đầu so sánh với Character-Aware Model [46] và công trình đồng thời GlyphDraw [49] bằng cách sử dụng mẫu từ các bài báo của họ vì mã nguồn mở, checkpoint hoặc API của họ không có sẵn. Hình 5 cho thấy TextDiffuser hoạt động tốt hơn các phương pháp này. Ví dụ, Character-Aware Model gặp vấn đề về chính tả (ví dụ: 'm' trong 'Chimpanzees') do thiếu kiểm soát rõ ràng, và GlyphDraw gặp khó khăn với việc hiển thị hình ảnh chứa nhiều dòng văn bản. Cho nhiệm vụ tạo một phần hình ảnh, chúng tôi trực quan hóa một số kết quả trong Hình 6. Trái ngược với các nhiệm vụ chỉnh sửa văn bản [88], chúng tôi cung cấp cho mô hình đủ tính linh hoạt để tạo ra văn bản với phong cách hợp lý. Ví dụ, hình ảnh ở hàng thứ hai và cột đầu tiên chứa từ "country" màu xanh lá cây, trong khi mô hình tạo ra từ "country" màu vàng. Điều này hợp lý vì nó theo phong cách của từ gần nhất "range". Bên cạnh đó, phương pháp của chúng tôi có thể hiển thị văn bản thực tế mạch lạc với nền, ngay cả trong các trường hợp phức tạp như quần áo. Thêm kết quả định tính được hiển thị trong Phụ lục L.

--- TRANG 9 ---
Hình 5: So sánh với Character-Aware Model [46] và GlyphDraw đồng thời [49].

COUNTRY
FrequentlyFIMI X8 SE Range Test in Country
Frequently Asked Questions (FAQ): The Muslim Edition image
Handsome
Boy
A man wears a cloth containing handsome boy
NEWSPAPER
How to make a newspaper
Chinese
She is teaching Chinese lesson.
Interesting terrace party

Hình 6: Trực quan hóa tạo một phần hình ảnh (inpainting văn bản) từ hình ảnh đã cho.

Nghiên cứu Người dùng. Cho nhiệm vụ tạo toàn bộ hình ảnh, bảng câu hỏi được thiết kế bao gồm 15 trường hợp, mỗi trường hợp bao gồm hai câu hỏi trắc nghiệm: (1) Hình ảnh nào sau đây có chất lượng hiển thị văn bản tốt nhất? (2) Hình ảnh nào sau đây khớp nhất với mô tả văn bản? Cho nhiệm vụ tạo một phần hình ảnh, bảng câu hỏi bao gồm 15 trường hợp, mỗi trường hợp bao gồm hai câu hỏi đánh giá: (1) Chất lượng hiển thị văn bản như thế nào? (2) Văn bản được vẽ có hài hòa với vùng không được che không? Điểm đánh giá từ 1 đến 4, và 4 chỉ ra tốt nhất. Nhìn chung, chúng tôi đã thu thập 30 bảng câu hỏi, và kết quả được hiển thị trong Hình 8. Chúng tôi có thể rút ra hai kết luận: (1) Hiệu suất tạo của TextDiffuser tốt hơn đáng kể so với các phương pháp hiện có. (2) Người dùng hài lòng với kết quả inpainting trong hầu hết các trường hợp. Thêm chi tiết được hiển thị trong Phụ lục M.

Hiệu quả Thời gian và Tham số Cho hiệu quả thời gian, giai đoạn đầu tiên của Tạo Bố cục tận dụng Transformer tự hồi quy có thời gian dự đoán tương quan với số lượng từ khóa. Cụ thể, chúng tôi tiến hành thí nghiệm để đánh giá chi phí thời gian cho số lượng từ khóa khác nhau, bao gồm 1 (1.07 ±0.03s), 2 (1.12 ±0.09s), 4 (1.23 ±0.13s), 8 (1.57 ±0.12s), 16 (1.83 ±0.12s), và 32 (1.95 ±0.28s). Đồng thời, giai đoạn thứ hai của tạo hình ảnh độc lập với số lượng truy vấn (7.12 ±0.77s). Cho hiệu quả tham số, TextDiffuser xây dựng dựa trên Stable Diffusion 1.5 (859M tham số), thêm Layout Transformer trong giai đoạn đầu tiên (+25M tham số) và sửa đổi giai đoạn thứ hai (+0.75M tham số), tăng chỉ khoảng 3% về tham số.

--- TRANG 10 ---
Hình 7: Minh họa sử dụng mô tả ngôn ngữ để kiểm soát màu văn bản.

Q1
9 13 69 559
10
0100200300
DALL·E SD SD-XL Midjourney ControlNet DeepFloyd TextDiffuser
(a) Cho tạo toàn bộ hình ảnh, phương pháp của chúng tôi rõ ràng vượt trội so với các phương pháp khác trong cả hai khía cạnh chất lượng hiển thị văn bản và khớp hình ảnh-văn bản.

100200300
01 2 3 4
(b) Cho tạo một phần hình ảnh, phương pháp của chúng tôi nhận được điểm cao từ các nhà đánh giá con người trong hai khía cạnh này.

294277 274257
176156 154136 123124
4889
486741
#Phiếu
Điểm
#Phiếu
Q2Q1
Chất lượng hiển thị văn bản
Khớp hình ảnh-văn bản
Chất lượng hiển thị văn bản
Q2
Hài hòa

Hình 8: Nghiên cứu người dùng cho nhiệm vụ tạo toàn bộ hình ảnh và tạo một phần hình ảnh.

Khả năng Kiểm soát Màu Văn bản Trong Hình 7, chúng tôi trình bày khả năng của TextDiffuser trong việc kiểm soát màu sắc của văn bản được tạo thông qua mô tả ngôn ngữ. Kết quả trực quan hóa cho thấy TextDiffuser có thể thành công kiểm soát màu sắc của văn bản được hiển thị, tiếp tục tăng cường khả năng kiểm soát của nó.

6 Thảo luận và Kết luận

Thảo luận. Chúng tôi cho thấy TextDiffuser duy trì khả năng và tính tổng quát để tạo ra hình ảnh chung không có hiển thị văn bản trong Phụ lục N. Bên cạnh đó, chúng tôi so sánh phương pháp của chúng tôi với mô hình chỉnh sửa văn bản trong Phụ lục O, cho thấy TextDiffuser tạo ra hình ảnh với tính đa dạng tốt hơn. Chúng tôi cũng trình bày tiềm năng của TextDiffuser trong nhiệm vụ xóa văn bản trong Phụ lục P. Về các hạn chế và trường hợp thất bại, TextDiffuser sử dụng mạng VAE để mã hóa hình ảnh thành không gian tiềm ẩn chiều thấp để hiệu quả tính toán theo các mô hình khuếch tán tiềm ẩn [67,49,2], điều này có hạn chế trong việc tái tạo hình ảnh với ký tự nhỏ như được hiển thị trong Phụ lục Q. Chúng tôi cũng quan sát thấy các trường hợp thất bại khi tạo hình ảnh từ văn bản dài và hiển thị chúng trong Phụ lục Q. Về tác động rộng hơn, TextDiffuser có thể được áp dụng vào nhiều nhiệm vụ thiết kế, như tạo poster và bìa sách. Ngoài ra, nhiệm vụ inpainting văn bản có thể được sử dụng để tạo thứ cấp trong nhiều ứng dụng, như Midjourney. Tuy nhiên, có thể có một số mối quan ngại về đạo đức, như việc lạm dụng inpainting văn bản để làm giả tài liệu. Do đó, các kỹ thuật để phát hiện giả mạo liên quan đến văn bản [86] cần được áp dụng để tăng cường bảo mật. Tóm lại, chúng tôi đề xuất một mô hình khuếch tán hai giai đoạn gọi là TextDiffuser để tạo ra hình ảnh với văn bản hấp dẫn về mặt thị giác mạch lạc với nền. Sử dụng mặt nạ phân đoạn làm hướng dẫn, TextDiffuser được đề xuất cho thấy tính linh hoạt cao và khả năng kiểm soát trong quá trình tạo. Chúng tôi đề xuất MARIO-10M chứa 10 triệu cặp hình ảnh-văn bản với chú thích OCR. Các thí nghiệm mở rộng và nghiên cứu người dùng xác thực rằng phương pháp của chúng tôi hoạt động tốt hơn các phương pháp hiện có trên điểm chuẩn MARIO-Eval được đề xuất. Cho công việc tương lai, chúng tôi nhằm giải quyết hạn chế của việc tạo ký tự nhỏ bằng cách sử dụng prior OCR theo OCR-VQGAN [66] và tăng cường khả năng của TextDiffuser để tạo hình ảnh với văn bản bằng nhiều ngôn ngữ.

Tuyên bố miễn trừ trách nhiệm
Xin lưu ý rằng mô hình được trình bày trong bài báo này chỉ dành cho mục đích học thuật và nghiên cứu. Bất kỳ việc sử dụng mô hình để tạo nội dung không phù hợp đều bị nghiêm cấm và không được bài báo này tán thành. Trách nhiệm cho bất kỳ lạm dụng hoặc sử dụng không đúng mô hình hoàn toàn thuộc về người dùng đã tạo nội dung như vậy, và bài báo này sẽ không chịu trách nhiệm cho bất kỳ việc sử dụng như vậy.

7 Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi Hội đồng Tài trợ Nghiên cứu của Đặc khu Hành chính Hong Kong dưới số tài trợ 16203122.

--- TRANG 11 ---
Tài liệu tham khảo
[1] Omri Avrahami, Dani Lischinski, và Ohad Fried. Blended diffusion for text-driven editing of natural images. Trong CVPR, 2022.
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.
[3] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles, Guillermo Sapiro, và Joan Verdera. Filling-in by joint interpolation of vector fields and gray levels. IEEE transactions on image processing (TIP), 2001.
[4] Darwin Bautista và Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. Trong ECCV, 2022.
[5] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, và Coloma Ballester. Image inpainting. Trong Proceedings of the 27th annual conference on Computer graphics and interactive techniques, 2000.
[6] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, và Stanley Osher. Simultaneous structure and texture image inpainting. IEEE transactions on image processing (TIP), 2003.
[7] Glenn L Cash và Mehdi Hatamian. Optical character recognition by the method of moments. Computer vision, graphics, and image processing, 1987.
[8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.
[9] Jingye Chen, Bin Li, và Xiangyang Xue. Scene text telescope: Text-focused scene image super-resolution. Trong CVPR, 2021.
[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, và Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.
[11] Hyungjin Chung, Byeongsu Sim, và Jong-Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. Trong CVPR, 2021.
[12] DeepFloyd. Github link: https://github.com/deep-floyd/if, 2023.
[13] Prafulla Dhariwal và Alexander Nichol. Diffusion models beat gans on image synthesis. Trong NeurIPS, 2021.
[14] Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, và Yongdong Zhang. Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition. Trong CVPR, 2021.
[15] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. Trong CVPR, 2023.
[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, và Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. Trong NeurIPS, 2022.
[17] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Hang Xu, Xiaodan Liang, Wei Zhang, Xin Jiang, và Chunjing Xu. Wukong: 100 million large-scale chinese cross-modal pre-training dataset and a foundation framework. Trong NeurIPS Workshop, 2022.
[18] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, và Baining Guo. Vector quantized diffusion model for text-to-image synthesis. Trong CVPR, 2022.

--- TRANG 12 ---
[19] Ankush Gupta, Andrea Vedaldi, và Andrew Zisserman. Synthetic data for text localisation in natural images. Trong CVPR, 2016.
[20] Kamal Gupta, Alessandro Achille, Justin Lazarow, Larry Davis, Vijay Mahadevan, và Abhinav Shrivastava. Layout generation and completion with self-attention. Trong ICCV, 2021.
[21] Haibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, và Yu Qiao. Diff-font: Diffusion model for robust one-shot font generation. Trong CVPR, 2023.
[22] Liu He, Yijuan Lu, John Corring, Dinei Florencio, và Cha Zhang. Diffusion-based document layout generation. arXiv preprint arXiv:2303.10787, 2023.
[23] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, và Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. Trong EMNLP, 2021.
[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, và Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Trong NeurIPS, 2017.
[25] Jonathan Ho, Ajay Jain, và Pieter Abbeel. Denoising diffusion probabilistic models. Trong NeurIPS, 2020.
[26] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, và Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research (JMLR), 2022.
[27] Jonathan Ho và Tim Salimans. Classifier-free diffusion guidance. Trong NeurIPS Workshop, 2021.
[28] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.
[29] Yupan Huang, Bei Liu, và Yutong Lu. Unifying multimodal transformer for bi-directional image and text generation. Trong ACM MM, 2021.
[30] Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, và Ariel Shamir. Word-as-image for semantic typography. arXiv preprint arXiv:2303.01818, 2023.
[31] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, và Andrew Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. Trong NeurIPS Workshop, 2014.
[32] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, và Shiyu Chang. Improving diffusion models for scene text editing with dual encoders. arXiv preprint arXiv:2304.05568, 2023.
[33] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, và Greg Mori. Layoutvae: Stochastic scene layout generation from a label set. Trong ICCV, 2019.
[34] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. Trong ICDAR, 2015.
[35] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. Trong ICLR, 2014.
[36] Yuxin Kong, Canjie Luo, Weihong Ma, Qiyuan Zhu, Shenggao Zhu, Nicholas Yuan, và Lianwen Jin. Look closer to supervise better: One-shot font generation via component-based discriminator. Trong CVPR, 2022.
[37] Praveen Krishnan, Rama Kovvuri, Guan Pang, Boris Vassilev, và Tal Hassner. Textstylebrush: Transfer of text aesthetics from a single example. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023.

--- TRANG 13 ---
[38] Junyeop Lee, Yoonsik Kim, Seonghyeon Kim, Moonbin Yim, Seung Shin, Gayoung Lee, và Sungrae Park. Rewritenet: Reliable scene text editing with implicit decomposition of text contents and styles. Trong CVPR Workshop, 2022.
[39] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, và Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022.
[40] Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang, và Tingfa Xu. Layoutgan: Generating graphic layouts with wireframe discriminators. Trong ICLR, 2019.
[41] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, và Furu Wei. Trocr: Transformer-based optical character recognition with pre-trained models. Trong AAAI, 2023.
[42] Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, và Xiang Bai. Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2022.
[43] Wentong Liao, Kai Hu, Michael Ying Yang, và Bodo Rosenhahn. Text to image generation with semantic-spatial aware gan. Trong CVPR, 2022.
[44] Chongyu Liu, Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, và Yongpan Wang. Erasenet: End-to-end text removal in the wild. IEEE Transactions on Image Processing (TIP), 2020.
[45] Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, và Chao Yang. Rethinking image inpainting via a mutual encoder-decoder with feature equalizations. Trong ECCV, 2020.
[46] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, và Noah Constant. Character-aware models improve visual text rendering. arXiv preprint arXiv:2212.10562, 2022.
[47] Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. Trong ICLR, 2019.
[48] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, và Luc Van Gool. RePaint: Inpainting using Denoising Diffusion Probabilistic Models. Trong CVPR, 2022.
[49] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, và Xiaodong Lin. Glyphdraw: Learning to draw chinese characters in image synthesis models coherently. arXiv preprint arXiv:2303.17870, 2023.
[50] Jianqi Ma, Zhetong Liang, và Lei Zhang. A text attention network for spatial deformation robust scene text image super-resolution. Trong CVPR, 2022.
[51] Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, và Xiangyang Xue. Arbitrary-oriented scene text detection via rotation proposals. IEEE transactions on multimedia (TMM), 2018.
[52] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, và Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. Trong ICML, 2022.
[53] Safaa S Omran và Jumana A Jarallah. Iraqi car license plate recognition using ocr. Trong 2017 annual conference on new trends in information & communications technology applications (NTICT), 2017.
[54] Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, và Hyunjung Shim. Multiple heads are better than one: Few-shot font generation with multiple localized experts. Trong ICCV, 2021.
[55] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, và Alexei A Efros. Context encoders: Feature learning by inpainting. Trong CVPR, 2016.

--- TRANG 14 ---
[56] Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, và Hadar Averbuch-Elor. Read: Recursive autoencoders for document layout generation. Trong CVPR, 2020.
[57] Jialun Peng, Dong Liu, Songcen Xu, và Houqiang Li. Generating diverse structure for image inpainting with hierarchical vq-vae. Trong CVPR, 2021.
[58] Tingting Qiao, Jing Zhang, Duanqing Xu, và Dacheng Tao. Mirrorgan: Learning text-to-image generation by redescription. Trong CVPR, 2019.
[59] Yadong Qu, Qingfeng Tan, Hongtao Xie, Jianjun Xu, Yuxin Wang, và Yongdong Zhang. Exploring stroke-level modifications for scene text editing. Trong AAAI, 2023.
[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. Trong ICML, 2021.
[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research (JMLR), 2020.
[62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, và Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
[63] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, và Ilya Sutskever. Zero-shot text-to-image generation. Trong ICML, 2021.
[64] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, và Honglak Lee. Generative adversarial text to image synthesis. Trong ICML, 2016.
[65] Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H Li, Shan Liu, và Ge Li. Structureflow: Image inpainting via structure-aware appearance flow. Trong ICCV, 2019.
[66] Juan A Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, và Pau Rodriguez. Ocr-vqgan: Taming text-within-image generation. Trong WACV, 2023.
[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. Trong CVPR, 2022.
[68] Xuejian Rong, Chucai Yi, và Yingli Tian. Unambiguous scene text segmentation with referring expression comprehension. IEEE Transactions on Image Processing (TIP), 2019.
[69] Olaf Ronneberger, Philipp Fischer, và Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. Trong MICCAI, 2015.
[70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, và Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022.
[71] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, và Mohammad Norouzi. Palette: Image-to-image diffusion models. Trong SIGGRAPH, 2022.
[72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Trong NeurIPS, 2022.
[73] Markus Schreiber, Fabian Poggenhans, và Christoph Stiller. Detecting symbols on road surface for mapping and localization using ocr. Trong 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), 2014.
[74] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Trong NeurIPS, 2022.

--- TRANG 15 ---
[75] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, và Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.
[76] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, và Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.
[77] Baoguang Shi, Xiang Bai, và Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2016.
[78] Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, và Kota Yamaguchi. De-rendering stylized texts. Trong ICCV, 2021.
[79] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, và Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. Trong ICML, 2015.
[80] Jiaming Song, Chenlin Meng, và Stefano Ermon. Denoising diffusion implicit models. Trong ICLR, 2021.
[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong NeurIPS, 2017.
[82] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, và Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.
[83] Ziyu Wan, Jingbo Zhang, Dongdong Chen, và Jing Liao. High-fidelity pluralistic image completion with transformers. Trong ICCV, 2021.
[84] Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, và Shuai Shao. Shape robust text detection with progressive scale expansion network. Trong CVPR, 2019.
[85] Wenjia Wang, Enze Xie, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua Shen, và Xiang Bai. Scene text image super-resolution in the wild. Trong ECCV, 2020.
[86] Yuxin Wang, Hongtao Xie, Mengting Xing, Jing Wang, Shenggao Zhu, và Yongdong Zhang. Detecting tampered scene text in the wild. Trong ECCV, 2022.
[87] James M White và Gene D Rohrer. Image thresholding for optical character recognition and other applications requiring character image extraction. IBM Journal of research and development, 1983.
[88] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding, và Xiang Bai. Editing text in the wild. Trong ACM MM, 2019.
[89] Zizhang Wu, Xinyuan Chen, Jizheng Wang, Xiaoquan Wang, Yuanzhu Gan, Muqing Fang, và Tianhao Xu. Ocr-rtps: an ocr-based real-time positioning system for the valet parking. Applied Intelligence, 2023.
[90] Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang, và Humphrey Shi. Rethinking text segmentation: A novel dataset and a text-specific refinement approach. Trong CVPR, 2021.
[91] Xixi Xu, Zhongang Qi, Jianqi Ma, Honglun Zhang, Ying Shan, và Xiaohu Qie. Bts: A bi-lingual benchmark for text segmentation in the wild. Trong CVPR, 2022.
[92] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, và Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227, 2022.
[93] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, và Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796, 2022.

--- TRANG 16 ---
[94] Boxi Yu, Yong Xu, Yan Huang, Shuai Yang, và Jiaying Liu. Mask-guided gan for robust text editing in the scene. Neurocomputing, 2021.
[95] Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han, Jingtuo Liu, và Errui Ding. Towards accurate scene text recognition with semantic reasoning networks. Trong CVPR, 2020.
[96] Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian Lu, Feiying Ma, Xuansong Xie, và Chunyan Miao. Diverse image inpainting with bidirectional and autoregressive transformers. Trong ACM MM, 2022.
[97] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
[98] Yanhong Zeng, Jianlong Fu, Hongyang Chao, và Baining Guo. Learning pyramid-context encoder network for high-quality image inpainting. Trong CVPR, 2019.
[99] Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, và Shiyu Chang. Towards coherent image inpainting using denoising diffusion implicit models. arXiv preprint arXiv:2304.03322, 2023.
[100] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, và Dimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. Trong ICCV, 2017.
[101] Linjiang Zhang, Peng Wang, Hui Li, Zhen Li, Chunhua Shen, và Yanning Zhang. A robust attentional framework for license plate recognition in the wild. IEEE Transactions on Intelligent Transportation Systems (TITS), 2020.
[102] Lvmin Zhang và Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.
[103] Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen Zuo, Haibo Chen, Wei Xing, và Dongming Lu. Uctgan: Diverse image inpainting based on unsupervised cross-space translation. Trong CVPR, 2020.
[104] Minyi Zhao, Miao Wang, Fan Bai, Bingjia Li, Jie Wang, và Shuigeng Zhou. C3-stisr: Scene text image super-resolution with triple clues. Trong IJCAI, 2022.
[105] Chuanxia Zheng, Tat-Jen Cham, và Jianfei Cai. Pluralistic image completion. Trong CVPR, 2019.
[106] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, và Jiajun Liang. East: an efficient and accurate scene text detector. Trong CVPR, 2017.
[107] Xinyan Zu, Haiyang Yu, Bin Li, và Xiangyang Xue. Weakly-supervised text instance segmentation. arXiv preprint arXiv:2303.10848, 2023.

--- TRANG 17 ---
Phụ lục

A Kiến trúc của U-Net và Thiết kế Tổn thất Nhận biết Ký tự
Như được hiển thị trong Hình 9, U-Net chứa bốn hoạt động downsampling và bốn hoạt động upsampling. Đầu vào sẽ được downsampling tối đa 1/16. Để cung cấp tổn thất nhận biết ký tự, đặc trưng đầu vào F là 4-D với kích thước không gian 64×64, trong khi đầu ra là 96-D (độ dài của bảng chữ cái A cộng với một ký hiệu null chỉ ra pixel không phải ký tự) cũng với kích thước không gian 64×64. Sau đó, một tổn thất cross-entropy được tính toán giữa đặc trưng đầu ra (cần chuyển đổi nhiễu dự đoán thành đặc trưng dự đoán) và mặt nạ phân đoạn cấp ký tự đã resize 64×64 C′. U-Net được tiền huấn luyện bằng tập huấn luyện của MARIO-10M trong một epoch. Chúng tôi sử dụng tối ưu hóa Adadelta [97] và đặt tỷ lệ học thành 1. Khi huấn luyện mô hình khuếch tán, U-Net được cố định và chỉ được sử dụng để cung cấp hướng dẫn nhận biết ký tự.

Hình 9: Kiến trúc của U-Net chứa bốn hoạt động downsampling và upsampling.

B Mô hình Phân đoạn Cấp Ký tự
Chúng tôi huấn luyện mô hình phân đoạn cấp ký tự dựa trên U-Net, có kiến trúc tương tự như kiến trúc được hiển thị trong Hình 9. Chúng tôi đặt kích thước đầu vào thành 256×256, đảm bảo rằng hầu hết các ký tự có thể đọc được ở độ phân giải này. Chúng tôi huấn luyện mô hình phân đoạn bằng hình ảnh văn bản cảnh tổng hợp [19], hình ảnh văn bản in, và hình ảnh văn bản viết tay3, tổng cộng khoảng 4M mẫu. Chúng tôi sử dụng các chiến lược tăng cường dữ liệu (ví dụ: làm mờ, xoay, và tăng cường màu sắc) để làm cho mô hình phân đoạn mạnh mẽ hơn. Mô hình phân đoạn được huấn luyện trong mười epoch bằng tối ưu hóa Adadelta [97] với tỷ lệ học 1. Hình 10 hiển thị một số mẫu trong bộ dữ liệu huấn luyện.

Hình 10: Trực quan hóa một số mẫu huấn luyện cho mô hình phân đoạn cấp ký tự. Hàng trên, giữa, và dưới là mẫu từ bộ dữ liệu in, viết tay, và cảnh.

3https://github.com/Belval/TextRecognitionDataGenerator

--- TRANG 18 ---
C Thêm Chi tiết trong MARIO-10M
Bảng 5: Số lượng văn bản trên mỗi hình ảnh trong MARIO-10M.
#Từ 1 2 3 4 5 6 7 8
#Hình ảnh 592,153 1,148,481 1,508,185 1,610,056 1,549,852 1,430,750 1,229,714 930,809
#Tỷ lệ 5.9% 11.5% 15.1% 16.1% 15.5% 14.3% 12.3% 9.3%

Thêm mẫu được hiển thị trong Hình 11. Số lượng văn bản trên mỗi hình ảnh trong MARIO-10M được hiển thị trong Bảng 5. Ngoài ra, bộ dữ liệu MARIO-10M cho thấy khoảng 90% các vùng văn bản duy trì hướng ngang với góc xoay nhỏ hơn 5 độ mà không có thay đổi phối cảnh. Do đó, mô hình tạo bố cục của chúng tôi được thiết kế để dự đoán các hộp giới hạn ngang bằng cách phát hiện tọa độ của điểm trên-trái và dưới-phải của chúng. Việc điều chỉnh mô hình của chúng tôi để dự đoán văn bản cảnh thực tế hơn là khả thi bằng cách phát hiện tọa độ nâng cao, như tám tọa độ cho bốn điểm.

D Mẫu Chú thích MARIO-10M
Vì poster phim/TV TMDB và bìa sách Open Library không có chú thích sẵn có, chúng tôi xây dựng chúng dựa trên tiêu đề của chúng với các mẫu sau đây. {XXX} là chỗ giữ chỗ cho tiêu đề.

Cho MARIO-TMDB:
• Logo {XXX}
• Văn bản {XXX}
• Tiêu đề {XXX}
• Văn bản tiêu đề {XXX}
• Một poster với văn bản tiêu đề {XXX}
• Một thiết kế poster với văn bản tiêu đề {XXX}
• Một bản in phim chất lượng với văn bản tiêu đề {XXX}
• Một poster phim của {XXX}
• Một poster phim của {XXX}
• Một poster phim có tiêu đề {XXX}
• Một poster phim có tên {XXX}
• Một poster phim với văn bản {XXX} trên đó
• Một poster phim với logo {XXX} trên đó
• Một poster phim với văn bản tiêu đề {XXX}
• Một minh họa của phim {XXX}
• Một bức ảnh của phim {XXX}
• Một poster chương trình TV có tiêu đề {XXX}
• Một poster chương trình TV của {XXX}
• Một poster chương trình TV với logo {XXX} trên đó
• Một poster chương trình TV với văn bản tiêu đề {XXX}
• Một poster chương trình TV với văn bản {XXX}
• Một poster chương trình TV có tên {XXX}

Cho MARIO-OpenLibrary:
• Một cuốn sách với văn bản tiêu đề {XXX}
• Một thiết kế sách với văn bản tiêu đề {XXX}
• Một bìa sách với văn bản tiêu đề {XXX}
• Một cuốn sách của {XXX}
• Một bìa có tên {XXX}
• Một bìa có tiêu đề {XXX}
• Một cuốn sách với văn bản {XXX} trên đó
• Một bìa sách với logo {XXX} trên đó

--- TRANG 19 ---
Hình 11: Thêm mẫu trong MARIO-10M.

--- TRANG 20 ---
E Quy tắc Lọc MARIO-10M
Chúng tôi làm sạch dữ liệu với năm quy tắc lọc nghiêm ngặt để thu được dữ liệu chất lượng cao với văn bản:
• Chiều cao và chiều rộng lớn hơn 256. Các mẫu độ phân giải thấp thường chứa văn bản không thể đọc được, ảnh hưởng tiêu cực đến quá trình huấn luyện.
• Sẽ không kích hoạt NSFW. Cho tập con MARIO-LAION, chúng tôi lọc ra những mẫu kích hoạt cờ "not sure for work" để giảm thiểu mối quan ngại về đạo đức.
• Số lượng hộp văn bản được phát hiện nên trong khoảng [1,8]. Chúng tôi phát hiện văn bản với DB [42]. Các mẫu có quá nhiều văn bản thường có diện tích nhỏ cho mỗi văn bản, khiến chúng khó nhận dạng. Do đó, chúng tôi loại bỏ những mẫu này khỏi bộ dữ liệu.
• Diện tích văn bản hơn 10% toàn bộ hình ảnh. Theo Phụ lục B, chúng tôi huấn luyện một UNet [69] bằng SynthText [19] để thu được mặt nạ phân đoạn cấp ký tự của mỗi mẫu. Tiêu chí này đảm bảo rằng các vùng văn bản sẽ không quá nhỏ.
• Ít nhất một văn bản được phát hiện xuất hiện trong chú thích. Nhận thấy rằng bộ dữ liệu gốc chứa nhiều mẫu nhiễu, chúng tôi thêm ràng buộc này để tăng mức độ liên quan giữa hình ảnh và chú thích. Chúng tôi sử dụng PARSeq [4] để nhận dạng văn bản.

--- TRANG 21 ---
F Phân tích Hiệu suất OCR trên MARIO-10M
Vì chúng tôi dựa vào các công cụ OCR để chú thích MARIO-10M, cần thiết phải đánh giá hiệu suất của những công cụ này. Cụ thể, chúng tôi chú thích thủ công 100 mẫu cho nhận dạng văn bản, phát hiện, và mặt nạ phân đoạn cấp ký tự, sau đó so sánh chúng với các chú thích được cung cấp bởi các công cụ OCR. Kết quả được hiển thị trong Bảng 6. Chúng tôi nhận thấy rằng hiệu suất của các phương pháp hiện có thấp hơn kết quả của chúng trên các điểm chuẩn phát hiện và spotting văn bản. Lấy DB [42] làm ví dụ, nó có thể đạt độ chính xác phát hiện văn bản 91.8% trên bộ dữ liệu ICDAR 2015 [34] trong khi chỉ đạt 76% trên MARIO-10M. Điều này là do có nhiều trường hợp khó khăn trong MARIO-10M, như văn bản mờ và nhỏ. Bên cạnh đó, có thể tồn tại khoảng cách miền vì DB được huấn luyện trên các bộ dữ liệu phát hiện văn bản cảnh, trong khi MARIO-10M bao gồm hình ảnh văn bản trong các tình huống khác nhau. Công việc tương lai có thể khám phá các mô hình nhận dạng, phát hiện, và phân đoạn tiên tiến hơn để giảm thiểu nhiễu trong các chú thích OCR. Chúng tôi thể hiện một số kết quả OCR trong Hình 12.

Bảng 6: Hiệu suất OCR trên MARIO-10M. IOU (nhị phân) có nghĩa là chúng tôi coi mỗi pixel là hai lớp: ký tự và không phải ký tự. Đánh giá nhận dạng được bao gồm trong nhiệm vụ spotting.

Phát hiện Spotting Phân đoạn
Precision Recall F-measure Precision Recall F-measure IOU (nhị phân) IOU
0.76 0.79 0.78 0.73 0.75 0.74 0.70 0.59

Pride Chitty about hocol Peppalig 2015 GREENE HIGH
MonkeY WOOL 2003 epop LONORES POLISH Dancesport RILEY
CAPTURE LIES RELEASE ST A GRA VE IEANIENE DIAR Y FREE!

Hình 12: Trực quan hóa một số chú thích OCR trong MARIO-10M.

--- TRANG 22 ---
G Mẫu trong MARIO-Eval
Bảng 7: Chi tiết của mỗi tập con trong MARIO-Eval.
Tập con Kích thước Chú thích Sẵn có Hình ảnh GT
LAIONEval4000 4,000 ✓ ✓
TMDBEval500 500 ✗ ✓
OpenLibraryEval500 500 ✗ ✓
DrawBenchText [72] 21 ✓ ✗
DrawTextCreative [46] 175 ✓ ✗
ChineseDrawText [49] 218 ✓ ✗

Như minh họa trong Bảng 7, MARIO-Eval chứa 5.414 lời nhắc với sáu tập con. Hình ảnh ground truth của một số mẫu được hiển thị trong Hình 13, và chú thích cho mỗi danh mục được hiển thị dưới đây:

LAIONEval4000:
• Bộ vòng tay 'Royal Green'
• Có phải 'Digital Nomads' gặp rắc rối không? Không hẳn
• 'Sniper Elite' One Way Trip A Novel Audiobook bởi 'Scott' McEwen Thomas Koloniar Kể bởi Brian Hutchison
• Logo 'Travel Artin'
• Falls the 'Shadow' Welsh Princes 2

TMDBEval500:
• Một poster phim với văn bản 'Heat' trên đó
• Một thiết kế poster với văn bản tiêu đề 'Deadpool 2'
• Một poster chương trình TV tên 'Ira Finkelstein s Christmas'
• Một poster phim với logo 'Playing for Change Songs Around The World Part 2' trên đó
• Một poster phim có tiêu đề 'Dreams of a Land'

OpenLibraryEval500:
• Một bìa sách với văn bản tiêu đề 'On The Apparel Of Women'
• Một cuốn sách với văn bản 'Precalculus' trên đó
• Một thiết kế sách với văn bản tiêu đề 'Dream master nightmare'
• Một bìa sách với logo 'Thre Poetical Works of Constance Naden' trên đó
• Một bìa sách với văn bản tiêu đề 'Discovery'

DrawBenchText:
• Một cửa hàng với 'Hello World' viết trên đó.
• Một cửa hàng với 'Text to Image' viết trên đó.
• Một biển báo nói 'Diffusion'.
• Một biển báo nói 'NeurIPS'.
• Đường chân trời New York với 'Google Research Pizza Cafe' viết bằng pháo hoa trên bầu trời.

DrawTextCreative:
• một hoa hướng dương cáu kỉnh với biển báo 'no solar panels'
• Một bức ảnh của một con thỏ nhấm nháp cà phê và đọc sách. Tiêu đề sách 'The Adventures of Peter Rabbit' có thể nhìn thấy.

--- TRANG 23 ---
• một tác phẩm graffiti với văn bản 'free the pink' trên tường
• Một logo được thiết kế chuyên nghiệp cho tiệm bánh tên 'Just What I Kneaded'.
• con voi học giả đọc báo với tiêu đề 'elephants take over the world'

ChineseDrawText:
• Một biển báo đường phố trên đường ghi 'Heaven rewards those who work hard'
• Có một cuốn sách trên bàn với tiêu đề 'Girl in the Garden'
• Mèo con cầm biển báo ghi 'I want fish'
• Một robot viết 'Machine Learning' trên bục giảng
• Trong bệnh viện, một biển báo nói 'Do Not Disturb'

Hình 13: Chúng tôi thể hiện năm mẫu cho LAIONEval4000 (trên), TMDBEval500 (giữa), và OpenLibrary500 (dưới).

--- TRANG 24 ---
H Chi tiết Triển khai của Tiêu chí Đánh giá
Để đánh giá hiệu suất của TextDiffuser một cách định lượng, chúng tôi sử dụng ba tiêu chí, bao gồm FID, CLIPScore, và Đánh giá OCR. Chúng tôi chi tiết việc tính toán của mỗi tiêu chí dưới đây.

FID. Chúng tôi tính điểm FID bằng kho pytorch-fid. Xin lưu ý rằng ba tập con của điểm chuẩn MARIO-Eval được đề xuất (DrawTextCreative, DrawBenchText, và ChineseDrawText) không chứa hình ảnh ground truth. Do đó, chúng tôi sử dụng 5.000 hình ảnh trong ba tập con khác (LAIONEval4000, TMDBEval500, OpenLibraryEval500) làm hình ảnh ground truth. Chúng tôi tính điểm FID bằng 5.414 hình ảnh đã tạo và 5.000 hình ảnh ground truth.

CLIP Score. Chúng tôi tính điểm CLIP bằng kho clipscore. Tuy nhiên, như với điểm FID, chúng tôi không thể tính điểm CLIP cho các tập con DrawTextCreative, DrawBenchText, và ChineseDrawText do thiếu hình ảnh ground truth. Do đó, chúng tôi chỉ tính điểm trên các tập con LAIONEval4000, TMDBEval500, và OpenLibraryEval500 và báo cáo điểm CLIP trung bình.

Đánh giá OCR. Cho điểm chuẩn MARIO-Eval, chúng tôi sử dụng dấu ngoặc kép để chỉ ra các từ khóa cần được vẽ trên hình ảnh. Lấy chú thích [A cat holds a paper saying 'Hello World'] làm ví dụ, các từ khóa là 'Hello' và 'World'. Sau đó chúng tôi sử dụng Microsoft Read API để phát hiện và nhận dạng văn bản trong hình ảnh. Chúng tôi đánh giá hiệu suất OCR bằng độ chính xác, precision, recall, và F-measure. Nếu văn bản được phát hiện khớp chính xác với từ khóa, nó được coi là đúng. Precision đại diện cho tỷ lệ văn bản được phát hiện khớp với từ khóa, trong khi recall đại diện cho tỷ lệ từ khóa xuất hiện trong hình ảnh. Chúng tôi báo cáo các giá trị trung bình của precision và recall, và tính F-measure bằng công thức sau:
F-measure = 2×Precision×Recall / (Precision+Recall). (5)

I Trực quan hóa Bố cục được Tạo bởi Layout Transformer
Chúng tôi trực quan hóa một số bố cục được tạo trong Hình 14, cho thấy Transformer có thể tạo ra bố cục hợp lý.

một robot viết 'Ethics 101' bằng phấn lên bảng đen.
Một cửa hàng với 'Google Research Pizza Cafe' viết trên đó.
Một chai cổ có nhãn 'Energy Tonic'
Một chiếc giày khổng lồ, với chú thích 'shoe for hokey pokey'
Một poster có tiêu đề 'Quails of North America', hiển thị các loại chim cút khác nhau.
Một cửa hàng với 'Deep Learning' viết trên đó.
Một cửa hàng với 'Hello World' viết trên đó.
Một biển báo nói 'Google Brain Toronto'.
Một biển báo nói 'NeurIPS'.

Hình 14: Trực quan hóa bố cục và hình ảnh được tạo.

--- TRANG 25 ---
J Thí nghiệm mà không có Hướng dẫn Rõ ràng của Mặt nạ Phân đoạn
Như được hiển thị trong Hình 15, chúng tôi cố gắng khám phá việc tạo mà không có hướng dẫn rõ ràng. Ví dụ, theo hàng đầu tiên, chúng tôi đặt giá trị của pixel ký tự thành 1 và pixel không phải ký tự thành 0 (tức là loại bỏ nội dung và chỉ cung cấp hướng dẫn vị trí). Chúng tôi quan sát thấy mô hình có thể tạo ra một số từ tương tự như từ khóa nhưng chứa một số lỗi ngữ pháp (ví dụ: thiếu "l" trong "Hello"). Hơn nữa, theo hàng thứ hai, chúng tôi huấn luyện TextDiffuser mà không có mặt nạ phân đoạn (tức là loại bỏ cả hướng dẫn vị trí và nội dung). Trong trường hợp này, thí nghiệm tương đương với việc tinh chỉnh trực tiếp mô hình khuếch tán tiềm ẩn đã tiền huấn luyện trên bộ dữ liệu MARIO-10M. Kết quả cho thấy chất lượng hiển thị văn bản xấu đi, chứng minh tầm quan trọng của hướng dẫn rõ ràng.

Hình 15: Trực quan hóa tạo mà không có hướng dẫn rõ ràng.

--- TRANG 26 ---
K Cài đặt Thí nghiệm của Các Phương pháp Baseline
Chúng tôi giới thiệu tất cả các phương pháp baseline và cài đặt thí nghiệm của chúng khi chúng tôi sử dụng để so sánh với TextDiffuser như sau.

DALL·E [62] sử dụng bộ mã hóa văn bản để ánh xạ lời nhắc đã cho thành không gian biểu diễn tương ứng. Một mô hình prior sau đó được sử dụng để ánh xạ mã hóa văn bản thành mã hóa hình ảnh. Cuối cùng, bộ giải mã hình ảnh tạo ra hình ảnh dựa trên mã hóa hình ảnh. Vì không có mã nguồn và mô hình có sẵn, chúng tôi thu được kết quả bằng API được cung cấp4.

Stable Diffusion (SD) sử dụng bộ mã hóa văn bản CLIP [60] để thu được embedding của lời nhắc người dùng, VAE đã tiền huấn luyện để mã hóa hình ảnh gốc và tiến hành quá trình khuếch tán trong không gian tiềm ẩn để hiệu quả tính toán. Chúng tôi sử dụng mô hình đã tiền huấn luyện công khai "runwayml/stable-diffusion-v1-5" dựa trên Hugging Face diffusers [82]. Số bước lấy mẫu là 50, và thang đo hướng dẫn classifier-free là 7.5.

Stable Diffusion XL (SD-XL) là phiên bản nâng cấp của SD, có thêm tham số và sử dụng mô hình ngôn ngữ mạnh mẽ hơn. Do đó, có thể mong đợi nó hiểu lời nhắc tốt hơn so với SD. Vì mã nguồn và mô hình không có sẵn công khai, chúng tôi thu được kết quả tạo thông qua API web5.

Midjourney6 là một dự án thương mại chạy trên Discord, cho phép người dùng tương tác với bot qua giao diện dòng lệnh. Chúng tôi tạo hình ảnh bằng các tham số mặc định của Midjourney. Ví dụ, chúng tôi có thể tạo hình ảnh bằng lệnh sau: /imagine an image of 'hello world' trong Midjourney.

ControlNet [102] nhằm kiểm soát các mô hình khuếch tán bằng cách thêm điều kiện sử dụng các lớp zero-convolution. Chúng tôi sử dụng mô hình đã tiền huấn luyện công khai "lllyasviel/sd-controlnet-canny" được phát hành bởi các tác giả ControlNet và triển khai từ Hugging Face diffusers [82]. Để so sánh công bằng, chúng tôi sử dụng hình ảnh văn bản in được tạo bởi mô hình giai đoạn đầu tiên của chúng tôi để tạo bản đồ Canny làm điều kiện của ControlNet. Chúng tôi sử dụng tham số mặc định để suy luận, nơi ngưỡng thấp và cao của tạo bản đồ canny được đặt thành 100 và 200 tương ứng. Số bước suy luận là 20, và thang đo hướng dẫn classifier-free là 7.5.

DeepFloyd [12] thiết kế ba mô-đun khuếch tán dựa trên pixel xếp tầng để tạo hình ảnh với độ phân giải tăng dần: 64x64, 256x256, và 1024x1024. Tất cả các mô-đun giai đoạn sử dụng bộ mã hóa văn bản cố định dựa trên T5 Transformer [12]. So với CLIP [60], T5 Transformer là một mô hình ngôn ngữ mạnh mẽ cho phép hiểu văn bản hiệu quả hơn. Chúng tôi sử dụng các mô hình đã tiền huấn luyện công khai được phát hành bởi các tác giả DeepFloyd và triển khai từ Hugging Face diffusers [82]. Chúng tôi sử dụng mô hình và tham số mặc định để suy luận, nơi ba mô hình xếp tầng đã tiền huấn luyện là "DeepFloyd/IF-I-XL-v1.0", "DeepFloyd/IF-II-L-v1.0", và "stabilityai/stable-diffusion-x4-upscaler".

4https://openai.com/product/dall-e-2
5https://beta.dreamstudio.ai/generate
6https://www.midjourney.com/

--- TRANG 27 ---
L Trực quan hóa Thêm Kết quả Tạo bởi TextDiffuser của Chúng tôi

Một poster phim có tiêu đề 'Gretel Hansel'
Một poster chương trình TV với văn bản tiêu đề 'Wendy and Lucy'
'Let Your Light Shine' 18k overlay
Một cuốn sách với văn bản 'Math for Life and Food Service'
'problem solving steps' và 'skills'
TMDBEval500

Một poster chương trình TV có tiêu đề 'Tango argentino'
locksmith 'cctv' sign
'Team' hat
Một bìa có tiêu đề 'The world of cats'
Một cuốn sách với văn bản 'Old Harthill' trên đó
LAIONEval4000 OpenLibraryEval500

Thanksgiving 'Fam' Mens T Shirt
'The Witch' Hunter 'Tale'
'Cheer' the 'new Year' Time Out
'Lisboa' Magazine
Stupid 'History' eBook Tales of Stupidity Strangeness
Photos of 'Sampa Hostel'

Một poster chương trình TV với logo 'Deep State' trên đó
Một poster phim của 'High Society'
Một poster phim có tiêu đề 'Personalities'
Một poster phim có tiêu đề 'The Kindergarten Show'
Một poster chương trình TV với logo 'The Big Lebowski' 2
Một poster phim của 'Short Term 12'
Một poster chương trình TV với logo 'The Dry' trên đó

Một bìa có tên 'Anything is possible'
Một bìa sách với logo 'Programming Skills Through C' trên đó
Một bìa có tên All World 'Monster Map'
Một cuốn sách của 'Marco Polo'
Một bìa có tên 'Green Lantern'
Một cuốn sách của 'Tideland treasure'
Một bìa có tên 'Wavewalker'

Hình 16: Trực quan hóa thêm kết quả tạo bởi TextDiffuser của chúng tôi trong LAIONEval4000, TMDBEval500, và OpenLibrary500.

--- TRANG 28 ---
một chiếc giày khổng lồ, với chú thích 'shoe for hokey pokey'
'Fall is here' viết bằng lá mùa thu trôi trên hồ
một tác phẩm graffiti với văn bản 'free the pink' trên tường
Một meme hiển thị một con mèo tấn công một chiếc giày, với thông điệp 'I own your shoe'
cây trong chậu sang trọng với biển báo 'do not touch'
Một áo phông với thông điệp 'There is no planet B'
gối hình các từ 'ready for the weekend'
một núi lửa phun trào, với văn bản 'magma' màu đỏ
một logo cho công ty 'diamonds', với kim cương hình trái tim
Một cuốn sách công thức lớn có tiêu đề 'Recipes from Peru'.
Một biển báo nói 'NeurIPS'.
Đường chân trời New York với 'Diffusion' viết bằng pháo hoa trên bầu trời.
Một cửa hàng với 'Hello World' viết trên đó.
Một cửa hàng với 'Google Research Pizza Cafe' viết trên đó.
Một cửa hàng với 'Deep Learning' viết trên đó.
Một cửa hàng với 'Google Brain Toronto' viết trên đó.
Một biển báo nói 'Hello World'.
Một biển báo nói 'Diffusion'.
Một biển báo nói 'Google Research Pizza Cafe'.
Một cửa hàng với 'NeurIPS' viết trên đó.

Biển báo cấm "No Gambling" treo ở lối vào sòng bạc
Một quả địa cầu với các từ "Planet Earth" viết bằng chữ in đậm với các châu lục màu sắc tươi sáng
Một bức ảnh hoa hồng được bao quanh bởi một biển báo ở xa có dòng chữ "Danger Minefield"
Một tiêu đề báo đọc "Local pig eats prize pumpkin" và một bức ảnh hiển thị một quả bí ngô bị cắn một nửa
Gấu mèo nhỏ cầm biển báo ghi "I want to learn"
Một biển báo "No Smoking" được đặt trong khách sạn
Một biển báo nói "Do not feed" trong thủy cung
Một chai hồng có dòng chữ "LOVE"
Một cô bé đang cầm một cuốn sách có dòng chữ "Fairy Tales" trên tay
Sách có dòng chữ "Science" in trên đó
DrawTextCreative DrawBenchText ChineseDrawText

Hình 17: Trực quan hóa thêm kết quả tạo bởi TextDiffuser của chúng tôi trong DrawTextCreative, DrawBenchText, và ChineseDrawText.

--- TRANG 29 ---
một con chuột với đèn pin nói 'i am afraid of the dark'
con voi học giả đọc báo với tiêu đề 'elephants take over the world'
Một bản thiết kế vẽ tay cho cỗ máy thời gian, với chú thích 'Time Traveling Device'.
ảnh của một biển báo với 'having a dog named shark at the beach was a mistake'
một bức ảnh trái đất với các từ 'save the earth' trong một vòng tròn
một con thằn lằn ngồi trên đĩa home plate sân bóng chày, với các từ 'make it safe' trong bong bóng lời nói
một mô hình 3d của máy tính phong cách năm 1980 với văn bản 'my old habit' trên màn hình
một bản phác thảo bút chì của một cái cây với tiêu đề 'nothing to tree here'
một con chó cầm giấy viết please adopt me
a stop pizza
a hello world banner
a stay calm quotes poster

Hình 18: Trực quan hóa thêm kết quả tạo bởi TextDiffuser của chúng tôi cho nhiệm vụ text-to-image với mẫu.

--- TRANG 30 ---
Hình 19: Trực quan hóa thêm kết quả tạo cho inpainting văn bản. Hình ảnh phía trên đường gạch ngang từ tập kiểm tra của MARIO-10M, trong khi hình ảnh dưới đường gạch ngang được thu thập từ cộng đồng Midjourney.

--- TRANG 31 ---
M Thêm Chi tiết về Nghiên cứu Người dùng

Nghiên cứu người dùng về nhiệm vụ tạo toàn bộ hình ảnh. Bảng câu hỏi bao gồm 15 trường hợp, mỗi trường hợp bao gồm hai câu hỏi trắc nghiệm:
• Hình ảnh nào sau đây có chất lượng hiển thị văn bản tốt nhất?
• Hình ảnh nào sau đây khớp nhất với mô tả văn bản?

Cụ thể, câu hỏi đầu tiên tập trung vào chất lượng hiển thị văn bản. Lấy Hình 20 làm ví dụ7, chúng tôi mong đợi mô hình hiển thị từ "EcoGrow" một cách chính xác (tức là không có bất kỳ ký tự nào bị thiếu hoặc thừa). Câu hỏi thứ hai, mặt khác, tập trung vào việc liệu hình ảnh tổng thể có khớp với lời nhắc đã cho hay không. Ví dụ, trong Hình 20 (G), mặc dù văn bản được tạo là đúng, nó không đáp ứng yêu cầu trong lời nhắc rằng chữ cái trông giống như một cái cây. Chúng tôi hướng dẫn người dùng chọn tùy chọn tốt nhất. Trong các trường hợp có nhiều tùy chọn tốt khó phân biệt, họ có thể chọn nhiều tùy chọn. Nếu người dùng không hài lòng với các tùy chọn, họ có thể quyết định không chọn bất kỳ tùy chọn nào.

Hình 20: Một trường hợp trong nghiên cứu người dùng cho nhiệm vụ tạo toàn bộ hình ảnh.

Nghiên cứu người dùng về nhiệm vụ tạo một phần hình ảnh. Chúng tôi nhằm để người dùng bỏ phiếu về chất lượng inpainting văn bản (từ 4 đến 1, càng cao càng tốt). Chúng tôi cũng thiết kế hai câu hỏi:
• Chất lượng hiển thị văn bản như thế nào?
• Văn bản được vẽ có hài hòa với vùng không bị che không?

Cụ thể, câu hỏi đầu tiên tập trung vào độ chính xác của văn bản. Câu hỏi thứ hai tập trung vào việc liệu phần được tạo có hài hòa với phần không bị che hay không (tức là liệu nền và kết cấu có nhất quán hay không).

Hình 21: Một trường hợp trong nghiên cứu người dùng cho nhiệm vụ tạo một phần hình ảnh.

7(A)TextDiffuser; (B) DALL·E; (C) SD; (D) IF; (E) SD-XL; (F) Midjourney; (G) ControlNet.

--- TRANG 32 ---
N Tạo Hình ảnh mà không có Văn bản

Để hiển thị tính tổng quát của TextDiffuser, chúng tôi thử nghiệm với việc tạo hình ảnh không chứa văn bản và hiển thị kết quả trong Hình 22. Mặc dù TextDiffuser được tinh chỉnh với MARIO-10M, nó vẫn duy trì khả năng tạo tốt để tạo hình ảnh chung. Do đó, người dùng có thêm tùy chọn khi sử dụng TextDiffuser, thể hiện tính linh hoạt của nó. Chúng tôi cũng cung cấp đánh giá định lượng để chứng minh tính tổng quát của TextDiffuser trong việc tạo hình ảnh chung không có văn bản. Chúng tôi so sánh TextDiffuser với baseline Stable Diffusion 1.5 vì chúng có cùng backbone. Để đánh giá định lượng, điểm FID của 5.000 hình ảnh được tạo bởi lời nhắc được lấy mẫu ngẫu nhiên từ MSCOCO như trong Bảng 8. Kết quả cho thấy TextDiffuser có thể duy trì khả năng tạo hình ảnh tự nhiên ngay cả sau khi tinh chỉnh bộ dữ liệu đặc thù miền.

Bảng 8: Điểm FID trên MSCOCO so sánh với Stable Diffusion.
Bước Lấy mẫu Stable Diffusion TextDiffuser
50 26.47 27.72
100 27.02 27.04

Hình 22: Trực quan hóa hình ảnh chung được tạo bởi Stable Diffusion 1.5 và TextDiffuser.

--- TRANG 33 ---
O So sánh giữa TextDiffuser và Mô hình Chỉnh sửa Văn bản

Chúng tôi trực quan hóa một số kết quả trong Hình 23 so sánh với mô hình chỉnh sửa văn bản SRNet [88]. Xin lưu ý rằng nhiệm vụ inpainting văn bản được giới thiệu khác với nhiệm vụ chỉnh sửa văn bản ở ba khía cạnh: (1) Nhiệm vụ chỉnh sửa văn bản thường dựa vào bộ dữ liệu hình ảnh văn bản tổng hợp để huấn luyện (tổng hợp hai hình ảnh với văn bản khác nhau cho một hình ảnh nền và phông chữ làm cặp). Ngược lại, nhiệm vụ inpainting văn bản theo sơ đồ huấn luyện che-và-phục hồi và có thể được huấn luyện với bất kỳ hình ảnh văn bản nào. (2) Chỉnh sửa văn bản nhấn mạnh việc bảo tồn phông chữ gốc, trong khi inpainting văn bản cho phép tự do hơn. Ví dụ, chúng tôi tiến hành bốn lần lấy mẫu cho mỗi trường hợp, và kết quả được tạo thể hiện tính đa dạng và trình bày phong cách phông chữ hợp lý. (3) Các nhiệm vụ chỉnh sửa văn bản không thể thêm văn bản, làm nổi bật tầm quan trọng của nhiệm vụ inpainting văn bản được giới thiệu.

Hình 23: So sánh với mô hình chỉnh sửa văn bản. Bốn trường hợp được thu thập từ bài báo của SRNet.

--- TRANG 34 ---
P Kết quả Thí nghiệm của Xóa Văn bản

Chúng tôi thể hiện một số kết quả xóa văn bản trong Hình 24, và các trường hợp được thu thập từ bài báo của EraseNet [44]. Chúng tôi có thể dễ dàng chuyển đổi nhiệm vụ inpainting văn bản thành nhiệm vụ xóa văn bản bằng cách cung cấp mặt nạ và đặt tất cả các vùng thành không phải ký tự trong mặt nạ phân đoạn. Kết quả thí nghiệm chứng minh rằng phương pháp của chúng tôi có thể đạt kết quả tương tự như ground truth.

Hình 24: Kết quả thí nghiệm về xóa văn bản. Bốn trường hợp được thu thập từ bài báo của EraseNet.

Q Hạn chế và Trường hợp Thất bại

Chúng tôi quan sát thấy các trường hợp thất bại khi tạo hình ảnh với ký tự nhỏ và từ văn bản dài.

Tạo hình ảnh với ký tự nhỏ. TextDiffuser sử dụng mạng VAE để mã hóa hình ảnh thành không gian tiềm ẩn chiều thấp để hiệu quả tính toán theo các mô hình khuếch tán tiềm ẩn [67,49,2]. Tuy nhiên, quá trình nén có thể dẫn đến mất chi tiết khi tạo hình ảnh với ký tự nhỏ. Như minh họa trong Hình 25, chúng tôi quan sát thấy VAE không thể tái tạo ký tự nhỏ, nơi các nét được tái tạo không rõ ràng và giảm khả năng đọc của văn bản. Theo các hình ảnh được tạo, các ký tự nhỏ có vẻ có nét mờ hoặc tách rời (ví dụ: ký tự 'l' trong 'World' và ký tự 'r' trong 'Morning'), có thể ảnh hưởng đến khả năng đọc. Như được hiển thị trong Hình 26, chúng tôi nhận thấy rằng việc sử dụng backbone mạnh mẽ hơn, như Stable Diffusion 2.1, có thể giảm thiểu vấn đề này. Khi độ phân giải hình ảnh được tăng từ 512×512 lên 768×768 bằng Stable Diffusion 2.1 (thay vì 1.5), độ phân giải không gian tiềm ẩn cũng tăng từ 64×64 lên 96×96, tăng cường biểu diễn cấp ký tự. Với chi phí, độ trễ suy luận tăng từ 8.5s lên 12.0s với kích thước batch 1. Do đó, cách hiển thị ký tự nhỏ trong khi duy trì cùng chi phí thời gian đáng được nghiên cứu thêm.

Tạo hình ảnh từ văn bản dài. Chúng tôi quan sát thấy các trường hợp thất bại khi tạo hình ảnh từ văn bản dài với nhiều từ khóa, nơi các từ được tạo trong bố cục bị rối loạn và chồng lên nhau,

--- TRANG 35 ---
Hình ảnh Gốc Hình ảnh Được Tái tạo
a board of 'Hello World'
a board of 'Good Morning'
Hình ảnh Được Tạo

Hình 25: Vấn đề của việc tạo hình ảnh với ký tự nhỏ.

Hình 26: Tiền huấn luyện trên Stable Diffusion 2.1 độ phân giải cao tăng cường khả năng đọc của văn bản nhỏ.

như được hiển thị trong Hình 27. Một lý do có thể là các ví dụ huấn luyện chứa nhiều từ khóa có xu hướng có nhiều nhiễu hơn (tức là những hình ảnh đó thường chứa văn bản dày đặc và nhỏ), dẫn đến khả năng cao hơn của các lỗi phát hiện và nhận dạng. Để giải quyết điều này, chúng tôi có thể xem xét việc tăng cường khả năng của các công cụ OCR trong tương lai để giảm thiểu nhiễu.

an image of 'I have a dream during my 17'
a board of 'My friend please adopt this cat'
a meme of 'please do not eat my hamburger'
a book of 'my favorite sport is ping pong'

Hình 27: Vấn đề của việc xử lý một số lượng lớn từ khóa.
