# 2312.04410.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2312.04410.pdf
# Kích thước tệp: 9073704 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Smooth Diffusion: Tạo Ra Không Gian Latent Mượt Mà Trong Mô Hình Khuếch Tán
Jiayi Guo1,2*, Xingqian Xu1,3*, Yifan Pu2, Zanlin Ni2, Chaofei Wang2, Manushree Vasu1,
Shiji Song2, Gao Huang2†, Humphrey Shi1,3†
1SHI Labs @ Georgia Tech & UIUC2Đại học Tsinghua3Picsart AI Research (PAIR)
https://github.com/SHI-Labs/Smooth-Diffusion
"Một con chó thực tế"
Hình AHình BNội suySmoothDiffusion(Của chúng tôi)StableDiffusionThay thế vật phẩm: "thỏ"→"mèo""Một chuyến tàu đang quay về đường ray chứa đầy người""Một con chuột ở cạnh bàn phím trên bàn"
Thêm vật phẩm: +"thịt xông khói"Chuyển đổi phong cách: "phong cách màu nước"Kéo điểmNhiệm vụ 1: Nội suy Hình ảnh
Nhiệm vụ 2: Đảo ngược và Tái tạo Hình ảnhNhiệm vụ 3: Chỉnh sửa Hình ảnh
Smooth Diff.Stable Diff.Nguồn
Smooth Diff.Stable Diff.NguồnSmooth Diff.Stable Diff.Nguồn
Hình 1. Smooth Diffusion cho các nhiệm vụ tổng hợp hình ảnh downstream. Phương pháp của chúng tôi chính thức giới thiệu tính mượt mà của không gian latent vào các mô hình khuếch tán như Stable Diffusion [59]. Tính mượt mà này hỗ trợ đáng kể nhiều nhiệm vụ trong: 1) cải thiện tính liên tục của chuyển tiếp trong nội suy hình ảnh, 2) giảm lỗi xấp xỉ trong đảo ngược hình ảnh, & 3) bảo toàn tốt hơn nội dung chưa được chỉnh sửa trong chỉnh sửa hình ảnh.

Tóm tắt
Gần đây, các mô hình khuếch tán đã đạt được tiến bộ đáng kể trong việc tạo ra hình ảnh từ văn bản (T2I), tổng hợp các hình ảnh với độ trung thực cao và nội dung đa dạng. Mặc dù có sự tiến bộ này, tính mượt mà của không gian latent trong các mô hình khuếch tán vẫn chưa được khám phá nhiều. Không gian latent mượt mà đảm bảo rằng một nhiễu loạn trên latent đầu vào tương ứng với một thay đổi ổn định trong hình ảnh đầu ra. Đặc tính này tỏ ra có lợi trong các nhiệm vụ downstream, bao gồm nội suy hình ảnh, đảo ngược và chỉnh sửa. Trong công trình này, chúng tôi phơi bày tính không mượt mà của không gian latent khuếch tán bằng cách quan sát các biến động thị giác đáng chú ý phát sinh từ các biến thiên latent nhỏ. Để giải quyết vấn đề này, chúng tôi đề xuất Smooth Diffusion, một loại mô hình khuếch tán mới có thể đồng thời hiệu suất cao và mượt mà. Cụ thể, chúng tôi giới thiệu Điều chỉnh Biến thiên Từng bước để ép buộc tỷ lệ giữa các biến thiên của một latent đầu vào tùy ý và hình ảnh đầu ra là một hằng số tại bất kỳ bước huấn luyện khuếch tán nào. Ngoài ra, chúng tôi thiết kế một chỉ số độ lệch chuẩn nội suy (ISTD) để đánh giá hiệu quả tính mượt mà không gian latent của một mô hình khuếch tán. Các thí nghiệm định lượng và định tính mở rộng chứng minh rằng Smooth Diffusion nổi bật như một giải pháp mong muốn hơn không chỉ trong việc tạo ra T2I mà còn trong nhiều nhiệm vụ downstream khác nhau. Smooth Diffusion được triển khai như một Smooth-LoRA cắm và chạy để hoạt động với nhiều mô hình cộng đồng khác nhau. Mã nguồn có tại https://github.com/SHI-Labs/Smooth-Diffusion.
1arXiv:2312.04410v1  [cs.CV]  7 Dec 2023

--- TRANG 2 ---
1. Giới thiệu
Trong những năm gần đây, các mô hình khuếch tán [13, 23, 59] đã nhanh chóng phát triển thành những công cụ rất mạnh mẽ cho AI tạo sinh, đặc biệt là cho việc tạo ra hình ảnh từ văn bản. Khả năng đáng chú ý của các mô hình khuếch tán, tạo ra những hình ảnh chân thực chất lượng cao từ ngữ cảnh mở, đã được làm nổi bật trong nhiều sản phẩm nghiên cứu và thương mại. Thành công như vậy cũng đã truyền cảm hứng cho nhiều nhiệm vụ downstream dựa trên khuếch tán, bao gồm nội suy hình ảnh [27, 75], đảo ngược [14, 42, 51, 68, 74], chỉnh sửa [20, 40, 43, 52, 66, 72, 78, 79], v.v.

Mặc dù thành công lớn trong lĩnh vực tạo sinh, các mô hình khuếch tán thỉnh thoảng tạo ra kết quả chất lượng thấp với các hành vi không mong muốn và không thể dự đoán. Cụ thể, đối với nội suy hình ảnh, thử nghiệm Stable Diffusion Walk (SDW) [27] kiểm tra không gian latent với các nội suy tuyến tính cầu, thường dẫn đến đầu ra biến động cao với diện mạo thị giác không thể dự đoán. Các ví dụ có thể được tìm thấy trong Hình 1 Nhiệm vụ 1, trong đó nội suy như vậy thể hiện những thay đổi sắc nét không mong muốn cũng như "hoạt hình hóa" trên hình ảnh chó chân thực, được làm nổi bật trong hộp đỏ. Đối với nhiệm vụ đảo ngược hình ảnh được hiển thị trong Hình 1 Nhiệm vụ 2, việc áp dụng đơn giản DDIM inversion [68] không thể tái tạo hình ảnh một cách trung thực từ các nguồn. Thay vào đó, nó tạo ra màu sắc và hướng đối tượng không chính xác, và hiểu nhầm con chuột máy tính như một con chuột động vật. Đối với nhiệm vụ chỉnh sửa hình ảnh được hiển thị trong Hình 1 Nhiệm vụ 3, người ta có thể nhận thấy rằng chỉ việc chỉnh sửa nhỏ lời nhắc văn bản có thể dẫn đến các cập nhật lớn về nội dung và bố cục hình ảnh, trong đó đối tượng (tức là tư thế của con mèo, vị trí của con ngựa, hình dạng của pizza) có thể bị thay đổi một cách sai lầm và nghiêm trọng. Hơn nữa, các mô hình khuếch tán hiện tại không phù hợp với việc chỉnh sửa dựa trên kéo [66] vì một phương pháp kéo được thiết kế tinh vi vẫn có khả năng đáng kể phá vỡ hình dạng và ngữ nghĩa của đối tượng.

Trong công trình này, chúng tôi bước vào một lĩnh vực quan trọng nhưng chưa được khám phá đầy đủ: cải thiện tính mượt mà không gian latent của các mô hình khuếch tán. Động cơ của chúng tôi để tăng cường tính mượt mà latent đến từ nhu cầu thực tế để cải thiện chất lượng đầu ra của các nhiệm vụ downstream đã nêu trên. Một không gian latent mượt mà ngụ ý một biến đổi thị giác mạnh mẽ dưới một thay đổi latent nhỏ. Do đó, việc tăng cường tính mượt mà như vậy có thể giúp cải thiện tính liên tục của nội suy hình ảnh, mở rộng khả năng của đảo ngược hình ảnh, và duy trì ngữ nghĩa chính xác trong chỉnh sửa hình ảnh. Đáng chú ý, các công trình trước đây trong GANs [30, 31, 65] đã chứng minh rằng không gian latent mượt mà của bộ tạo có thể cải thiện đáng kể chất lượng của các nhiệm vụ downstream, cung cấp bằng chứng bổ sung về tầm quan trọng của lĩnh vực này.

Để đạt được mục tiêu của chúng tôi, chúng tôi đề xuất Smooth Diffusion, một loại mô hình khuếch tán mới có thể đồng thời hiệu suất cao và mượt mà. Chúng tôi bắt đầu khám phá bằng cách đầu tiên chính thức hóa mục tiêu cho Smooth Diffusion, trong đó các nhiễu loạn kích thước cố định ∆ϵ trên một nhiễu latent ϵ sẽ tạo ra những thay đổi thị giác mượt mà ∆cx0 trên hình ảnh tổng hợp cx0, được làm tròn đến một tỷ lệ hằng số C. Mặc dù người ta có thể nghĩ rằng theo công thức, ràng buộc tính mượt mà có thể là một hàm mất mát thời gian huấn luyện có thể tiếp cận. Thực tế, không có ứng dụng trực tiếp của điều chỉnh như vậy từ suy luận đến huấn luyện, và thách thức nằm ở thực tế rằng trong mỗi lần lặp huấn luyện (tức là, lan truyền ngược), các mô hình khuếch tán chỉ tối ưu hóa một "ảnh chụp nhanh t-bước" thay vì toàn bộ quá trình khuếch tán T-bước.

Do đó, chúng tôi giới thiệu Điều chỉnh Biến thiên Từng bước, một điều chỉnh mới kết hợp liền mạch mục tiêu thời gian suy luận của Smooth Diffusion vào huấn luyện. Điều chỉnh này nhằm giới hạn 2-norm của biến thiên đầu ra ∆cx0 cho một thay đổi kích thước cố định ∆xt trong đầu vào xt tại một bước t tùy ý. Lý luận của việc tái công thức là trực quan: Nếu xt và cx0 thể hiện những thay đổi mượt mà tại bất kỳ t nào, thì mối quan hệ giữa nhiễu latent ϵ (tức là xT) và cx0 chỉ là sự tích lũy của các biến thiên mượt mà và do đó cũng có thể mượt mà. Chi tiết hơn có thể được tìm thấy trong Phần 3.

Trong thực tế, Smooth Diffusion của chúng tôi được huấn luyện trên một mô hình văn bản-hình ảnh nổi tiếng: Stable Diffusion [59]. Chúng tôi kiểm tra và chứng minh rằng Smooth Diffusion cải thiện đáng kể tính mượt mà không gian latent so với đường cơ sở của nó. Đồng thời, chúng tôi tiến hành nghiên cứu mở rộng trên nhiều nhiệm vụ downstream, bao gồm nhưng không giới hạn ở nội suy hình ảnh, đảo ngược, chỉnh sửa, v.v. Cả kết quả định tính và định lượng đều hỗ trợ kết luận của chúng tôi rằng Smooth Diffusion có thể là mô hình tạo sinh hiệu suất cao thế hệ tiếp theo không chỉ cho nhiệm vụ văn bản-hình ảnh cơ bản mà còn cho nhiều nhiệm vụ downstream khác nhau.

2. Công trình liên quan
Các mô hình khuếch tán được khởi xướng từ một họ các công trình trước đó bao gồm nhưng không giới hạn ở [10, 63, 67, 73]. Kể từ đó, DDPM [23] đã giới thiệu một mô hình dự đoán nhiễu dựa trên hình ảnh, trở thành một trong những nghiên cứu tạo sinh hình ảnh phổ biến nhất. Các công trình sau này [13, 45, 68] đã mở rộng DDPM, chứng minh rằng các mô hình khuếch tán thực hiện ngang bằng và thậm chí vượt qua các phương pháp dựa trên GAN [16, 28–31]. Gần đây, việc tạo ra hình ảnh từ lời nhắc văn bản (T2I) trở thành một lĩnh vực mới nổi, trong đó các mô hình khuếch tán [17, 46, 56, 59, 61] đã trở nên khá nổi bật với công chúng. Ví dụ, Stable Diffusion (SD) [59] bao gồm VAE [34] và CLIP [55], khuếch tán không gian latent, và mang lại sự cân bằng xuất sắc giữa chất lượng và tốc độ. Theo sau SD [59], các nhà nghiên cứu cũng khám phá các phương pháp khuếch tán cho điều khiển như ControlNet [15, 25, 44, 54, 77, 82, 83, 86–88, 92] và đa phương thức như Versatile Diffusion [8, 39, 70, 85]. Các công trình từ một hướng khác giảm các bước khuếch tán để cải thiện tốc độ [7, 32, 37, 41, 62, 69, 89, 93], hoặc hạn chế dữ liệu và miền cho học ít ví dụ [19, 24, 38, 60], tất cả đều đã thành công duy trì chất lượng đầu ra cao.

Không gian latent mượt mà là một trong những đặc tính nổi bật của các công trình GAN SOTA [11, 29–31], trong khi việc khám phá đặc tính như vậy đã trải qua nghiên cứu GAN kéo dài một thập kỷ [5, 16], có mục tiêu chủ yếu là huấn luyện mạnh mẽ. Các ý tưởng như Wasserstein GAN [6, 18] đã được chứng minh là hiệu quả, ép buộc tính liên tục Lipschitz trên bộ phân biệt thông qua các hình phạt gradient. Một kỹ thuật khác, được gọi là điều chỉnh độ dài đường dẫn, liên quan đến việc kẹp Jacobian trong [48], đã được áp dụng trong StyleGAN2 [30] và sau đó trở thành một thiết lập tiêu chuẩn cho các bộ tạo dựa trên GAN [12, 35, 84, 91]. Hưởng lợi từ đặc tính mượt mà, các nhà nghiên cứu đã quản lý để thao tác không gian latent trong nhiều dự án nghiên cứu downstream. Các công trình như [9, 47, 65, 80] đã khám phá việc tách rời không gian latent. GAN-inverse [3, 4, 49, 81] cũng đã được chứng minh là khả thi, cùng với một họ các phương pháp chỉnh sửa hình ảnh [50, 53, 57, 58, 71, 94]. Như đã đề cập trước đó, công trình của chúng tôi nhằm điều tra tính mượt mà không gian latent cho các mô hình khuếch tán, điều mà cho đến nay vẫn chưa được khám phá.

3. Phương pháp luận
Trong phần này, chúng tôi đầu tiên giới thiệu các kiến thức chuẩn bị của phương pháp chúng tôi, bao gồm quá trình khuếch tán [23], đảo ngược khuếch tán [13, 42, 68] và thích ứng hạng thấp [24] (Phần 3.1). Sau đó Smooth Diffusion được đề xuất với định nghĩa, mục tiêu (Phần 3.2) và hàm điều chỉnh (Phần 3.3) của nó.

3.1. Kiến thức chuẩn bị
Quá trình khuếch tán [23] là một loại chuỗi Markov dần dần thêm nhiễu ngẫu nhiên ϵt∼N(0,I) vào tín hiệu sự thật cơ bản x0∼p(x0), tạo ra xT trong tổng số T bước. Tại mỗi bước, dữ liệu nhiễu xt được tính như sau:
xt=√(1−βt)xt−1+√βtϵt, t= 1,2,···, T, (1)
trong đó βt là tốc độ khuếch tán đặt trước tại bước t. Bằng cách tạo αt= 1−βt, αt=∏(t=1 đến T)αt và ϵ∼N(0,I), chúng ta có các tương đương sau:
xt=√αtxt−1+√(1−αt)ϵt
=√αtx0+√(1−αt)ϵ, t= 1,2,···, T.(2)

Một mô hình khuếch tán ϵθ(xt, t) sau đó được huấn luyện để ước tính ϵt từ xt, qua đó người ta có thể dự đoán tín hiệu gốc x0 bằng cách dần dần loại bỏ nhiễu từ xT xuống cấp [68]. Điều này thường được biết đến như quá trình khuếch tán ngược:
xt−1=√(αt−1/αt)x̂t+ √(1/αt−1−1−√(1/αt−1))·ϵθ(x̂t, t). (3)

Đảo ngược khuếch tán [13, 42, 68] nhắm mục tiêu khôi phục chính xác quá trình khuếch tán ngược (tức là x̂t, ϵθ(x̂t, t), t= 1, ..., T) từ một dự đoán cuối cùng đã biết x̂0. Một trong những kỹ thuật phổ biến cho việc đảo ngược như vậy là DDIM inversion [13, 68], đảo ngược Phương trình (3) dưới một xấp xỉ tuyến tính cục bộ:
x̂t+1=√(αt+1/αt)x̂t+ √(1/αt+1−1−√(1/αt−1))·ϵθ(x̂t, t), (4)
trong đó x̂t đại diện cho x̂t ước tính tại thời gian t. Tuy nhiên, DDIM inversion chỉ là một ước tính thô. Đối với khuếch tán văn bản-hình ảnh, một kỹ thuật tiên tiến hơn, Null-Text Inversion [42], tối ưu hóa các embedding văn bản rỗng bổ sung {∅t}(T, t=1) cho mỗi bước t, mô phỏng quá trình ngược với ϵθ(xt, t, ξ,∅t), trong đó ξ là embedding văn bản đầu vào. Văn bản rỗng dự đoán ∅t là đầu vào rỗng của hướng dẫn không phân loại [22] với tỷ lệ hướng dẫn w:
ϵθ(xt, t, ξ,∅t) =w·ϵθ(xt, t, ξ) + (1 −w)·ϵθ(xt, t,∅t). (5)

Thích ứng hạng thấp (LoRA) [24] ban đầu được đề xuất để thích ứng hiệu quả các mô hình lớn được huấn luyện trước với các nhiệm vụ downstream. Giả định chính của LoRA là các thay đổi trọng số cần thiết trong quá trình thích ứng duy trì một hạng thấp. Cho một trọng số mô hình được huấn luyện trước W0∈Rd×k, trọng số cập nhật ∆W của nó được biểu thị như một phân tích hạng thấp:
W0+ ∆W=W0+BA, (6)
trong đó B∈Rd×r, A∈Rr×k và r≪min(d, k). Trong quá trình thích ứng, W0 được đóng băng, trong khi B và A có thể huấn luyện.

3.2. Smooth Diffusion
Như đã đề cập trước đó, các mô hình khuếch tán hiện đại (DM) không đảm bảo tính mượt mà không gian latent, tạo ra không chỉ khoảng cách nghiên cứu giữa GANs và khuếch tán mà còn những thách thức bất ngờ trong các nhiệm vụ downstream. Để giải quyết những vấn đề này, chúng tôi đề xuất Smooth Diffusion, một lớp mô hình khuếch tán mới có hiệu suất cao với tính mượt mà được tăng cường trên không gian latent của nó. Nền tảng của Smooth Diffusion là lược đồ huấn luyện mới được đề xuất trong đó chúng tôi thực hiện Điều chỉnh Biến thiên Từng bước để tăng cường tính mượt mà của mô hình.

Để giải thích tốt hơn mục tiêu của chúng tôi, chúng tôi áp dụng cùng thuật ngữ từ quá trình khuếch tán thời gian suy luận tiêu chuẩn (Hình 2a), liên quan đến một quy trình T bước biến đổi nhiễu ngẫu nhiên ϵ (tức là, xT) thành dự đoán x̂0. Mục tiêu tổng thể của Smooth Diffusion sau đó có thể được viết trong Phương trình 7: trong đó chúng tôi mong đợi rằng một thay đổi kích thước cố định ∆ϵ trên ϵ (tức là, ∆xT trên xT) cuối cùng sẽ dẫn đến một thay đổi không bằng không, kích thước cố định ∆x̂0 trên x̂0, đến một tỷ lệ hằng số C:
∥∆x̂0∥2⇔C∥∆xT∥2=C∥∆ϵ∥2,∀ϵ, (7)

Lưu ý rằng theo định nghĩa, xT là đầu vào ban đầu của vòng lặp khuếch tán ngược trong Phương trình 3. Vì xT gần với ϵ∼N(0,1), để đơn giản, chúng tôi làm cho chúng tương đương trong tất cả các phương trình sau.

Tuy nhiên, người ta có thể nhận thấy rằng mục tiêu thời gian suy luận của chúng tôi trong Phương trình 7 không thể được chuyển đổi trực tiếp thành một hàm mất mát huấn luyện. Điều này là bởi vì, trong một lần lặp huấn luyện (tức là, lan truyền ngược), các mô hình khuếch tán chỉ tối ưu hóa một "ảnh chụp nhanh t-bước" của quá trình khuếch tán (Hình 2b), trong đó t được lấy mẫu đồng nhất từ 1 đến T. Do đó, mục tiêu "toàn cục" được đề xuất (Phương trình 7) cho toàn bộ quá trình T-bước không thể tiếp cận trong huấn luyện. Do đó, chúng tôi cần tái công thức mục tiêu toàn cục của chúng tôi thành một mục tiêu từng bước được hiển thị trong Phương trình 8, có thể sau đó được tích hợp vào quá trình huấn luyện khuếch tán như một hàm mất mát:
∥∆x̂0∥2⇔C∥∆xt∥2=C√(1−αt)∥∆ϵ∥2,∀ϵ,(8)
trong đó C là một hằng số không bằng không. Mục tiêu từng bước này chỉ ra rằng tại mỗi bước huấn luyện, các biến thiên ∆ϵ trên ϵ sẽ ngụ ý các biến thiên ∆xt trên xt với một tỷ lệ tỷ lệ thuận với √(1−αt). Lý luận của Phương trình 8 là trực quan: Nếu xt và x̂0 thể hiện những thay đổi mượt mà tại bất kỳ t nào, thì mối quan hệ giữa nhiễu latent ϵ (tức là xT) và x̂0 chỉ là sự tích lũy của các biến thiên mượt mà và do đó cũng có thể mượt mà.

3.3. Điều chỉnh Biến thiên Từng bước
Trong khi động cơ và công thức của mục tiêu Smooth Diffusion được trình bày, cách thực hiện mục tiêu như vậy vẫn chưa được giải thích. Do đó, trong phần này, chúng tôi giới thiệu Điều chỉnh Biến thiên Từng bước để tích hợp hiệu quả mục tiêu từng bước vào huấn luyện khuếch tán.

Chúng tôi lấy cảm hứng từ các kỹ thuật điều chỉnh [30, 48] được áp dụng trong huấn luyện GAN. Ý tưởng cốt lõi của Điều chỉnh Biến thiên Từng bước là giới hạn ma trận Jacobian Jϵ=∂x̂0/∂ϵ của hệ thống khuếch tán bằng cách tối thiểu hóa mất mát điều chỉnh sau đây tại bất kỳ x0, ϵ, và bước t:
Lreg=E∆x̂0,ϵ[√(1−αt)∥JTϵ∆x̂0∥2−a]2, (9)
trong đó ∆x̂0 là cường độ pixel được lấy mẫu bình thường được chuẩn hóa thành độ dài đơn vị, ϵ là một nhiễu được lấy mẫu bình thường trong Phương trình 2, và a là trung bình động mũ của √(1−αt)∥JTϵ∆x̂0∥2 được tính toán trực tuyến trong quá trình huấn luyện. Trong thực tế, chúng tôi tính Phương trình 9 thông qua lan truyền ngược tiêu chuẩn với đẳng thức sau:
√(1−αt)∥JTϵ∆x̂0∥2=∥∇ϵ(√(1−αt)x̂0·∆x̂0)∥2.(10)

Đẳng thức này đúng vì ∆x̂0 được lấy mẫu độc lập, và không tương quan với ϵ.

Tiếp theo, chúng tôi chứng minh rằng mục tiêu được đề xuất trong Phương trình 9 khớp chính xác với mục tiêu tối ưu hóa của chúng tôi trong Phương trình 8. Một kết quả sơ bộ, được chứng minh trong [30], là trong các chiều cao, Phương trình 9 được tối thiểu hóa khi Jϵ là trực giao tại bất kỳ ϵ nào đến một yếu tố tỷ lệ toàn cục K (tức là Jϵ·JTϵ=K·I). Bằng cách áp dụng tính trực giao của Jϵ, chúng ta có:
JTϵ∆x̂0=KJ−1ϵ∆x̂0=K∂ϵ/∂x̂0·∆x̂0=K∆ϵ.(11)

Khi Lreg trong Phương trình 9 đạt tối ưu, chúng ta sau đó có:
a=√(1−αt)∥JTϵ∆x̂0∥2=√(1−αt)K∥∆ϵ∥2. (12)

Lưu ý rằng a=a∥∆x̂0∥2, vì ∥∆x̂0∥2= 1 là vector độ dài đơn vị ngẫu nhiên đã đề cập trước đó. Do đó, chúng ta cuối cùng có thể tái công thức biểu thức:
∥∆x̂0∥2=K/a·√(1−αt)∥∆ϵ∥2
=C√(1−αt)∥∆ϵ∥2,(13)

--- TRANG 4 ---
DMDM𝑇 bước𝝐(hoặc 𝒙!)$𝒙"(a) Khuếch tán thời gian suy luận: Dự đoán khử nhiễu qua 𝑻 bước (b) Khuếch tán thời gian huấn luyện: Dự đoán khử nhiễu tại một bước đơn 𝒕𝒙#$𝒙"1−𝛼#𝝐𝛼#𝒙"+DM𝑇 bước(hoặc 𝒙!+Δ𝒙!)$𝒙"+Δ$𝒙"(c) Smooth Diffusion thời gian suy luận: Ràng buộc biến thiên qua 𝑻 bước DM(d) Smooth Diffusion thời gian huấn luyện: Ràng buộc biến thiên tại một bước đơn 𝒕𝒙#+Δ𝒙#$𝒙"+Δ$𝒙"1−𝛼#(𝝐+Δ𝝐)𝛼#𝒙"+𝝐+Δ𝝐𝐶Δ𝝐𝟐=𝐶Δ𝒙!%⇔Δ$𝒙"%,∀𝝐𝐶1−𝛼#Δ𝝐𝟐=𝐶Δ𝒙#%⇔Δ$𝒙"%,∀𝝐Hình 2. Minh họa của Smooth Diffusion. Smooth Diffusion (c) ép buộc tỷ lệ giữa biến thiên của latent đầu vào (∥∆ϵ∥2 hoặc ∥∆xT∥2) và biến thiên của dự đoán đầu ra (∥∆x̂0∥2) là một hằng số C. Khuếch tán thời gian huấn luyện (b) tối ưu hóa một "ảnh chụp nhanh t-bước" của quá trình dự đoán khử nhiễu trong Khuếch tán thời gian suy luận (a). Tương tự, chúng tôi đề xuất Smooth Diffusion thời gian huấn luyện (d) để tối ưu hóa một "ảnh chụp nhanh t-bước" của ràng buộc biến thiên trong Smooth Diffusion thời gian suy luận (c). DM: Mô hình khuếch tán.

điều này khớp chính xác với mục tiêu được đề xuất của chúng tôi trong Phương trình 8.
Để tóm tắt, trong quá trình huấn luyện, mục tiêu Smooth Diffusion bao gồm sự kết hợp của Lbase và Lreg:
L=Lbase+λLreg, (14)
trong đó Lbase biểu thị mục tiêu huấn luyện cơ bản của một mô hình khuếch tán và λ đại diện cho một tham số tỷ lệ điều khiển cường độ của Điều chỉnh Biến thiên Từng bước.

4. Thí nghiệm
4.1. Thiết lập thí nghiệm
Đường cơ sở và thiết lập. Chúng tôi chọn Stable Diffusion [59] làm đường cơ sở chính cho tất cả các nhiệm vụ. Ngoài ra, đối với nội suy hình ảnh, chúng tôi áp dụng nội suy không gian VAE và ANID [75] làm đối thủ cạnh tranh. Đối với đảo ngược hình ảnh, chúng tôi tích hợp Smooth Diffusion và Stable Diffusion với DDIM inversion [68] và Null-text inversion [42]. Đối với chỉnh sửa hình ảnh dựa trên văn bản, SDEdit [40], Prompt-to-Prompt (P2P) [20], Plug-and-Play (PnP) [72], Diffusion Disentanglement (Disentangle) [79], Pix2Pix-Zero [52] và Cycle Diffusion [78] được chọn làm các phương pháp SOTA. Đối với chỉnh sửa hình ảnh dựa trên kéo, chúng tôi so sánh Smooth Diffusion với Stable Diffusion trong khung của DragDiffusion [66].

Chi tiết triển khai. Smooth Diffusion được huấn luyện trên Stable Diffusion-V1.5 [59] được huấn luyện trước, sử dụng kỹ thuật tinh chỉnh LoRA [24]. UNet của Smooth Diffusion được đặt là có thể huấn luyện với hạng LoRA là 8, trong khi VAE và bộ mã hóa văn bản được đóng băng. Chúng tôi tận dụng LAION Aesthetics 6.5+ làm tập dữ liệu huấn luyện, chứa 625K cặp hình ảnh-văn bản với điểm thẩm mỹ dự đoán là 6.5 hoặc cao hơn từ LAION-5B [64]. Smooth diffusion thường được huấn luyện trong 30K lần lặp với kích thước batch là 96, 3 mẫu trên GPU, tổng cộng 4 A100 GPU, và tích lũy gradient là 8. Bộ tối ưu AdamW [33] được áp dụng với tốc độ học tập không đổi là 1×10−4 và weight decay là 1×10−4. Tham số tỷ lệ λ trong Phương trình 14 được đặt là 1. Trong quá trình suy luận, tổng số bước khuếch tán được đặt là 50 và tỷ lệ hướng dẫn classifier-free [22] được đặt là 7.5.

Chỉ số đánh giá. Để đánh giá hiệu suất tạo sinh văn bản-hình ảnh tổng quát, chúng tôi báo cáo FID [21] phổ biến và CLIP Score [55] trên tập validation MS-COCO [36]. Để đánh giá tính mượt mà không gian latent, chúng tôi đề xuất độ lệch chuẩn nội suy (ISTD) làm chỉ số đánh giá. Cụ thể, chúng tôi ngẫu nhiên rút 500 lời nhắc văn bản từ tập validation MS-COCO. Đối với mỗi lời nhắc, chúng tôi lấy mẫu một cặp nhiễu Gaussian và nội suy đồng nhất chúng từ một sang cái khác 9 lần với tỷ lệ hỗn hợp từ 0.1 đến 0.9. Đưa vào các mô hình khuếch tán cùng với một lời nhắc, chúng tôi có thể thu được tổng cộng 11 hình ảnh được tạo ra, 2 từ các nhiễu Gaussian nguồn và 9 từ các nhiễu được nội suy. Chúng tôi tính độ lệch chuẩn của khoảng cách L2 giữa mỗi hai hình ảnh liền kề trong không gian pixel. Cuối cùng, chúng tôi tính trung bình độ lệch chuẩn trên 500 lời nhắc như ISTD. Lý tưởng, giá trị không của ISTD chỉ ra rằng các biến động thị giác nhất quán và đồng nhất trong không gian pixel cho các thay đổi kích thước cố định giống hệt nhau trong không gian latent, dẫn đến một không gian latent mượt mà. Đối với đảo ngược hình ảnh, lỗi bình phương trung bình (MSE), LPIPS [90], SSIM [76] và PSNR [26] được áp dụng để đánh giá khả năng tái tạo hình ảnh.

4.2. Nội suy Không gian Latent
So sánh định tính. Cách đơn giản nhất để chứng minh tính mượt mà của không gian latent là thông qua quan sát kết quả nội suy giữa các nhiễu latent. Trong Hình 3, chúng tôi trình bày so sánh nội suy giữa Smooth Diffusion và Stable Diffusion sử dụng hình ảnh thực. Để tạo ra những so sánh này, chúng tôi sử dụng NTI [42] để đảo ngược một cặp hình ảnh thực thành nhiễu latent xT, chia sẻ cùng {∅t}Tt=1. Sau đó chúng tôi thực hiện nội suy tuyến tính cầu đồng nhất giữa các nhiễu latent (còn được gọi là Stable Diffusion Walk [27]), dẫn đến 9 nhiễu trung gian với tỷ lệ hỗn hợp từ 0.1 đến 0.9. Sau đó, chúng tôi nối 11 hình ảnh được tạo từ những nhiễu này để tạo ra một chuỗi chuyển tiếp hình ảnh trong các hình.

Đáng chú ý, như được làm nổi bật bởi các hộp đỏ, Stable Diffusion thể hiện các biến động thị giác đáng kể trong quá trình chuyển tiếp. Đặc biệt, các hình ảnh được nội suy có thể giới thiệu các thuộc tính mới không liên quan đến hình ảnh nguồn, ví dụ, các đồng cỏ không mong muốn trong hàng thứ hai của Hình 3. Ngược lại, phương pháp của chúng tôi, Smooth Diffusion, không chỉ tránh việc giới thiệu các thuộc tính không liên quan rõ ràng trong các hình ảnh được nội suy mà còn đảm bảo rằng các hiệu ứng thị giác thay đổi mượt mà trong suốt quá trình chuyển tiếp. Kết quả nội suy bổ sung có thể được xem trong tài liệu bổ sung.

Ngoài Stable Diffusion, Hình 3 cũng bao gồm hai phương pháp đường cơ sở khác để so sánh: 1) Nội suy VAE (VAE Inter.), thực hiện nội suy trong không gian VAE của Stable Diffusion. Tuy nhiên, kết quả gần giống với nội suy không gian pixel, với sự suy giảm đáng kể của chi tiết thị giác, đặc biệt trong khu vực hộp đỏ được làm nổi bật. 2) ANID [75], đầu tiên thêm nhiễu vào hình ảnh thực và sau đó khử nhiễu các hình ảnh nhiễu được nội suy bằng Stable Diffusion. Trong Hình 3, ANID với bộ lập lịch 50-bước thể hiện kết quả nội suy rất mờ. Khi ANID hoạt động với bộ lập lịch 200-bước mặc định, việc làm mờ có thể được giảm thiểu, nhưng chất lượng của các hình ảnh được nội suy vẫn còn xa mới thỏa mãn.

So sánh định lượng. Mục tiêu của Smooth Diffusion là tăng cường tính mượt mà không gian latent mà không làm suy giảm hiệu suất tạo sinh hình ảnh so với Stable Diffusion. Theo đuổi mục tiêu này, chúng tôi sử dụng ISTD được giới thiệu trong Phần 4.1 để đánh giá tính mượt mà không gian latent. Ngoài ra, chúng tôi sử dụng FID [21] và CLIP Score [55] để đánh giá hiệu suất tổng thể của các bộ tạo. Kết quả được trình bày trong Bảng 1 chứng minh rằng Smooth Diffusion vượt trội đáng kể so với Stable Diffusion về ISTD, chỉ ra một cải thiện đáng kể trong tính mượt mà không gian latent. Hơn nữa, Smooth Diffusion thể hiện hiệu suất vượt trội trong cả FID và CLIP Score, gợi ý rằng việc tăng cường tính mượt mà không gian latent và chất lượng tạo sinh hình ảnh tổng thể không loại trừ lẫn nhau mà bổ sung cho nhau khi thuật ngữ điều chỉnh được áp dụng với một tỷ lệ cường độ phù hợp.

4.3. Đảo ngược và Tái tạo Hình ảnh
Nghiên cứu trước đây [30] trong lĩnh vực GANs đã phát hiện ra rằng một không gian latent mượt mà hơn có tác động tích cực đến độ chính xác của đảo ngược và tái tạo hình ảnh. Chúng tôi xác thực thực nghiệm phát hiện này trong bối cảnh của các mô hình khuếch tán. Cụ thể, hai kỹ thuật đảo ngược đại diện, DDIM inversion [68] và Null-text inversion (NTI) [42] được áp dụng và tích hợp với Smooth Diffusion và Stable Diffusion riêng biệt. Chúng tôi so sánh cả định tính và định lượng hiệu suất đảo ngược và tái tạo hình ảnh của các mô hình tích hợp này sử dụng 500 hình ảnh được lấy mẫu ngẫu nhiên từ tập validation MS-COCO [36].

Như được minh họa trong hai cột ngoài cùng bên phải của Hình 4, khi sử dụng một DDIM inversion đơn giản, Smooth Diffusion vượt trội hơn Stable Diffusion một cách đáng kể về chất lượng tái tạo. Sự cải thiện này rõ ràng trong nhiều khía cạnh khác nhau, chẳng hạn như việc tạo ra chính xác danh tính nhân vật, tái tạo trung thực cảnh quan thành phố phía sau tháp, và tái tạo chính xác bố cục phòng. Hiện tượng này nhấn mạnh thực tế rằng không gian latent của Smooth Diffusion chịu đựng tốt hơn các lỗi được giới thiệu bởi xấp xỉ tuyến tính cục bộ trong DDIM inversion. Do đó, kết quả tái tạo được tạo ra bởi Smooth Diffusion quản lý để giữ lại nội dung của hình ảnh nguồn ở mức độ lớn hơn. Mặt khác, khi kỹ thuật NTI dựa trên tối ưu hóa được sử dụng, sự khác biệt giữa Smooth Diffusion và Stable Diffusion không rõ ràng như vậy. Tuy nhiên, vẫn có những trường hợp mà Stable Diffusion thể hiện kết quả dưới mức, chẳng hạn như khuôn mặt người đàn ông bị hỏng trong Hình 4.

Để định lượng hiệu suất tái tạo hình ảnh, MSE, LPIPS [90], SSIM [76] và PSNR [26] được báo cáo trong Bảng 2. Đáng chú ý, lỗi tái tạo bao gồm hai thành phần: 1) lỗi từ các phương pháp đảo ngược khác nhau và các tham số U-Net và 2) lỗi từ VAE được huấn luyện trước chia sẻ [34]. Do đó, chúng tôi bao gồm các lỗi tái tạo VAE như các giá trị tối ưu cho phương pháp của chúng tôi. Kết quả thể hiện hiệu suất vượt trội nhất quán của Smooth Diffusion so với Stable Diffusion trên tất cả các chỉ số, cho dù sử dụng DDIM inversion hay NTI. Hơn nữa, "Smooth Diffusion + NTI" thực hiện kết quả gần với tái tạo VAE, chỉ ra tính ưu việt của nó được quy cho một không gian latent mượt mà hơn.

4.4. Chỉnh sửa Hình ảnh
Tính ưu việt của Smooth Diffusion trong đảo ngược và tái tạo hình ảnh đã thúc đẩy chúng tôi khám phá tiềm năng của nó để tăng cường các nhiệm vụ chỉnh sửa hình ảnh. Trong phần này, chúng tôi đi sâu vào hai kịch bản chỉnh sửa hình ảnh điển hình: chỉnh sửa hình ảnh dựa trên văn bản và chỉnh sửa hình ảnh dựa trên kéo.

Chỉnh sửa hình ảnh dựa trên văn bản. Đã có nhiều phương pháp [20, 40, 52, 72, 78, 79] được đề xuất trong văn học, mỗi phương pháp có thiết kế độc đáo riêng nhằm đạt được hiệu suất SOTA. Ngược lại, chúng tôi áp dụng một pipeline đơn giản hơn tương tự như quá trình đảo ngược và tái tạo hình ảnh được thảo luận trong Phần 4.3. Sự khác biệt chính nằm ở cách tiếp cận của chúng tôi để sửa đổi lời nhắc văn bản trong các bước thời gian sau của quá trình tái tạo. Cụ thể, ϵθ(xt, t,C,∅t) gốc trong Phương trình (5) trong quá trình tái tạo NTI (lấy mẫu khuếch tán) được thay thế bằng:
ϵθ(xt, t,C,∅t) ={
ϵθ(xt, t,Csrc,∅t), t > T ×r,
ϵθ(xt, t,Ctrg,∅t), t≤T×r,(15)
trong đó Csrc đại diện cho lời nhắc văn bản nguồn cho đảo ngược, trong khi Ctrg tương ứng với lời nhắc văn bản mục tiêu cho chỉnh sửa. Tham số r phục vụ như một ngưỡng, xác định khi nào chuyển từ Csrc sang Ctrg. Trong thực tế, r thường được chọn trong {0.6, 0.7, 0.8, 0.9}, với giá trị chính xác phụ thuộc vào hình ảnh đầu vào cụ thể và hiệu ứng thị giác mục tiêu.

Thông qua pipeline đơn giản này, chúng tôi tiến hành phân tích so sánh hiệu suất chỉnh sửa giữa Smooth Diffusion và Stable Diffusion, như được trình bày trong ba cột trái nhất của Hình 5. Chúng tôi cũng bao gồm kết quả chỉnh sửa thu được từ các phương pháp SOTA như tham chiếu. Đánh giá của chúng tôi bao gồm cả nhiệm vụ chỉnh sửa cục bộ và toàn cục. Các nhiệm vụ chỉnh sửa cục bộ liên quan đến việc thay thế các mục (ví dụ, thay đổi "kem" thành "dâu tây") và thêm các mục (ví dụ, "táo"). Mặt khác, các nhiệm vụ chỉnh sửa toàn cục liên quan đến chuyển giao phong cách toàn cục, chẳng hạn như biến đổi một hình ảnh thành "phong cách hoạt hình". Rõ ràng rằng trong khi Stable Diffusion xuất sắc trong việc đạt được tái tạo hình ảnh chính xác với NTI, như đã thảo luận trong Phần 4.3, ngay cả những sửa đổi nhỏ đối với lời nhắc văn bản có thể ảnh hưởng đáng kể đến nội dung của hình ảnh được tạo ra. Ví dụ, nó có thể ảnh hưởng đến các yếu tố như phong cách của bánh, hình dạng của quả chuối, và kiểu tóc của cô gái. Ngược lại, Smooth Diffusion không chỉ tạo ra chính xác hình ảnh được chỉnh sửa phù hợp với lời nhắc văn bản mục tiêu mà còn hiệu quả bảo tồn nội dung chưa được chỉnh sửa. Hơn nữa, khi so sánh với các phương pháp SOTA, ngay cả với pipeline đơn giản này, Smooth Diffusion liên tục mang lại kết quả cạnh tranh trên tất cả các trường hợp.

Chỉnh sửa hình ảnh dựa trên kéo. Như một hướng nghiên cứu mới nổi trong cộng đồng, chỉnh sửa hình ảnh dựa trên kéo [43, 50, 66] đã thu hút sự chú ý đáng kể gần đây. DragDiffusion [66] đầu tiên giới thiệu một khung cho chỉnh sửa hình ảnh dựa trên kéo sử dụng Stable Diffusion. Trong nhiệm vụ 3 của Hình 1 và Hình 6, chúng tôi thể hiện rằng bằng cách tích hợp Smooth Diffusion vào khung DragDiffusion, một số hoạt động chỉnh sửa trước đây không thành công với Stable Diffusion có thể được kích hoạt. Như được minh họa, Smooth Diffusion đạt được các hoạt động như làm cho cây phát triển cao hơn mà không làm hỏng các nhánh hiện có (Hình 1), xoay đầu mèo, tạo ra một đỉnh núi mới mà không phá hủy cái cũ, và để hoa mới phát triển trong bình (Hình 6). Tuy nhiên, những hoạt động này thất bại với Stable Diffusion, chỉ ra tính không mượt mà của không gian latent của nó.

4.5. Nghiên cứu Loại bỏ
Tỷ lệ điều chỉnh. Trong Bảng 3, chúng tôi kiểm tra tác động của các tỷ lệ cường độ λ khác nhau trong Phương trình (14). Tỷ lệ này điều chỉnh cường độ của điều chỉnh biến thiên từng bước. Cụ thể, khi một điều chỉnh yếu hơn được áp dụng (ví dụ, λ= 0.1), chúng tôi quan sát một cải thiện nhỏ trong CLIP Score. Tuy nhiên, có một sự gia tăng đáng kể trong ISTD, chỉ ra một sự suy giảm đáng chú ý trong tính mượt mà không gian latent. Ngược lại, việc sử dụng một điều chỉnh mạnh hơn (ví dụ, λ= 10) dẫn đến một không gian latent mượt mà hơn, như được chứng minh bởi sự giảm ISTD. Tuy nhiên, trong trường hợp này, chúng tôi quan sát một sự gia tăng bất ngờ trong FID, chỉ ra một sự suy giảm đáng chú ý trong chất lượng của hình ảnh được tạo ra. Do đó, việc chọn một giá trị đánh đổi phù hợp cho λ trở nên quan trọng dựa trên các thiết lập thí nghiệm cụ thể. Trong thiết lập mặc định của chúng tôi, chúng tôi thấy rằng λ= 1 phục vụ như một giá trị phù hợp.

Hạng LoRA. Trong Bảng 4, chúng tôi kiểm tra tác động của các hạng khác nhau của thành phần LoRA được sử dụng trong Smooth diffusion của chúng tôi. Chúng tôi phát hiện ra rằng các hạng LoRA trong phạm vi [4,16] đều là các giá trị phù hợp cho thiết lập mặc định của chúng tôi. Chúng tôi chọn hạng mặc định là 8 vì ISTD thấp nhất của nó trong ba hàng đầu trong Bảng 4. Hơn nữa, chúng tôi huấn luyện một mô hình được tinh chỉnh đầy đủ, được gọi là "full", thể hiện một sự giảm thêm trong ISTD. Tuy nhiên, điều này đi kèm với chi phí là suy giảm đáng kể chất lượng của hình ảnh được tạo ra, như được chỉ ra bởi FID tăng và CLIP Score giảm. Sự suy giảm hiệu suất này nhấn mạnh tính dễ bị tổn thương của các mô hình được tinh chỉnh đầy đủ đối với sự sụp đổ trong thiết lập mặc định của chúng tôi, nhấn mạnh nhu cầu cho các cân nhắc thiết kế tỉ mỉ bổ sung.

5. Kết luận
Trong bài báo này, chúng tôi đã khám phá Smooth Diffusion, một mô hình khuếch tán sáng tạo tăng cường tính mượt mà không gian latent cho việc tạo sinh. Smooth Diffusion áp dụng Điều chỉnh Biến thiên Từng bước mới, thành công duy trì biến thiên giữa latent đầu vào tùy ý và hình ảnh được tạo ra trong một phạm vi giới hạn hơn. Smooth Diffusion được huấn luyện trên mô hình văn bản-hình ảnh phổ biến, từ đó chúng tôi tiến hành nghiên cứu mở rộng, bao gồm nhưng không giới hạn ở nội suy, đảo ngược và chỉnh sửa, tất cả đều cho thấy hiệu suất cạnh tranh. Thông qua các đo lường định tính và định lượng, chúng tôi đã chứng minh rằng Smooth Diffusion quản lý để tạo ra một không gian latent mượt mà hơn mà không làm tổn hại chất lượng đầu ra. Chúng tôi tin rằng Smooth Diffusion sẽ trở thành một giải pháp có giá trị cho các nhiệm vụ thách thức khác, chẳng hạn như tạo sinh video, trong tương lai.

--- TRANG 5 ---
[Tài liệu tham khảo với 94 mục từ [1] đến [94] - tôi sẽ giữ nguyên định dạng của các tài liệu tham khảo này]

--- TRANG 6 đến 15 ---
[Tiếp tục với phần Tài liệu Bổ sung và các hình ảnh minh họa thêm]

Tài liệu Bổ sung
A. Chi tiết Triển khai
Phần này mở rộng các chi tiết được giới thiệu ngắn gọn trong bài báo chính. Những chi tiết này bao gồm ký hiệu, mục tiêu huấn luyện cơ bản, chỉ số độ lệch chuẩn nội suy (ISTD), và việc sử dụng Null-text inversion (NTI) [42] của chúng tôi cho nội suy hình ảnh thực.

A.1. Ký hiệu
Stable Diffusion [59] sử dụng một pipeline khuếch tán "latent" hiệu quả. Ở đây "latent" đề cập đến việc sử dụng một VAE được huấn luyện riêng [34] để nén một hình ảnh đầu vào x0 thành biểu diễn không gian VAE z0 của nó:
z0=E(x0),x0=D(z0), (16)
trong đó E và D đại diện cho bộ mã hóa và giải mã của VAE, tương ứng. Để đơn giản, chúng tôi loại trừ quá trình chuyển đổi này và chỉ sử dụng ký hiệu dựa trên "x" trong bài báo chính. Mặc dù chúng tôi chọn Stable Diffusion làm đường cơ sở của chúng tôi do tính phổ biến và hiệu suất cao của nó, pipeline huấn luyện của chúng tôi không được thiết kế riêng cho các mô hình khuếch tán latent và tương thích với các mô hình khuếch tán khác.

A.2. Mục tiêu Huấn luyện Cơ bản
Mục tiêu huấn luyện của Smooth Diffusion bao gồm hai thành phần chính: 1) một mục tiêu huấn luyện cơ bản tập trung chủ yếu vào dự đoán nhiễu nhưng linh hoạt trong công thức cho các mô hình khuếch tán khác nhau, và 2) thuật ngữ Điều chỉnh Biến thiên Từng bước được đề xuất của chúng tôi. Trong các thí nghiệm của chúng tôi, mục tiêu huấn luyện cơ bản là:
Lbase=Ex0,ϵ,t∥ϵ−ϵθ(xt, t)∥22, (17)
đây là một mục tiêu huấn luyện được áp dụng phổ biến trên nhiều mô hình khuếch tán, ví dụ, Stable Diffusion [59].

A.3. ISTD
Mục tiêu của ISTD là định lượng độ lệch của các thay đổi không gian pixel cho cùng những thay đổi bước cố định trong không gian latent. Một độ lệch thấp hơn ngụ ý rằng các latent đầu vào và hình ảnh đầu ra có nhiều khả năng thay đổi mượt mà hơn. Trong các thí nghiệm của chúng tôi, chúng tôi đầu tiên ngẫu nhiên rút 500 lời nhắc văn bản từ tập validation MS-COCO [36]. Đối với mỗi lời nhắc, sau đó chúng tôi lấy mẫu hai nhiễu Gaussian ngẫu nhiên, ϵa và ϵb. Tiếp theo, chúng tôi thực hiện nội suy tuyến tính cầu đồng nhất (slerp) giữa ϵa và ϵb cho 11 lần, thay đổi tỷ lệ hỗn hợp η từ 0 đến 1:
ϵη= slerp( ϵa,ϵb, η), η = 0,0.1,0.2,···,1. (18)

Chúng tôi sử dụng mô hình khuếch tán thử nghiệm để tạo ra 11 hình ảnh được nội suy {x̂η0}1η=0 từ {ϵη}1η=0. Lưu ý rằng Phương trình 18 đảm bảo rằng các thay đổi không gian latent giữa mỗi hai latent liền kề (tức là, ϵη và ϵη+0.1) là giống nhau. Do đó, chúng tôi tính các khoảng cách L2 giữa mỗi hai hình ảnh liền kề (tức là, x̂η0 và x̂η+0.10) và tính độ lệch chuẩn của các khoảng cách này. Cuối cùng, ISTD là trung bình của độ lệch chuẩn trên 500 lời nhắc văn bản khác nhau. Để so sánh công bằng, các lời nhắc văn bản và nhiễu cho mỗi lời nhắc là giống nhau cho các mô hình thử nghiệm khác nhau.

A.4. NTI cho nội suy hình ảnh thực
NTI ban đầu được thiết kế để biến đổi một hình ảnh thực x0 thành một latent x̂T, cùng với một loạt các embedding văn bản rỗng có thể học {∅t}Tt=1 cho mỗi bước t. Việc tối ưu hóa cho mỗi ∅t được công thức hóa như:
min∅t∥x̂t−1−DDIM( x̂t, t, ξ,∅t)∥22. (19)
trong đó {x̂t}Tt=1 đại diện cho các hình ảnh nhiễu trung gian được ước tính bởi DDIM inversion [68]. Để đơn giản, DDIM( x̂t, t, ξ,∅t) biểu thị quá trình lấy mẫu DDIM tại bước t, sử dụng embedding văn bản ξ, embedding văn bản rỗng ∅t và tỷ lệ hướng dẫn classifier-free w= 7.5.

Đối với nội suy hình ảnh thực, chúng tôi tối ưu hóa một loạt chia sẻ {∅t}Tt=1 cho hai hình ảnh thực, xa0 và xb0:
min∅t∥x̂at−1−DDIM( x̂at, t, ξ,∅t)∥22+
∥x̂bt−1−DDIM(x̂bt, t, ξ,∅t)∥22.(20)

Trong các thí nghiệm của chúng tôi, chúng tôi chỉ nội suy các latent x̂aT và x̂bT theo Phương trình 18 và sử dụng cùng embedding văn bản rỗng {∅t}Tt=1 cho tất cả hình ảnh được nội suy.

B. Kết quả Bổ sung
Phần này cung cấp kết quả thị giác bổ sung của Smooth Diffusion. Chúng tôi hiển thị kết quả nội suy hình ảnh trong Hình 7 và Hình 8, kết quả đảo ngược và tái tạo hình ảnh trong Hình 9, và kết quả chỉnh sửa hình ảnh trong Hình 10.

Khả năng tái sử dụng. Thành phần LoRA của Smooth Diffusion vẫn có thể thích ứng với các mô hình khác chia sẻ cùng kiến trúc với Stable Diffusion. Tuy nhiên, hiệu quả của khả năng tái sử dụng này không được đảm bảo. Chúng tôi đánh giá việc tích hợp thành phần LoRA này vào hai mô hình cộng đồng phổ biến, RealisticVision-V2 [2] và OpenJourney-V4 [1]. Như được mô tả trong Hình 8, việc tích hợp này cũng tăng cường tính mượt mà không gian latent của các mô hình này. Khả năng tái sử dụng này làm cho phương pháp của chúng tôi loại bỏ nhu cầu huấn luyện lặp đi lặp lại và trở thành một mô-đun cắm và chạy trên nhiều mô hình khác nhau.
