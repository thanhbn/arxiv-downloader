# 2304.02051.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/diffusion/2304.02051.pdf
# Kích thước tệp: 30769279 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Nhà thiết kế thời trang đa phương thức:
Mô hình khuếch tán tiềm ẩn lấy con người làm trung tâm cho chỉnh sửa hình ảnh thời trang
Alberto Baldrati1,3,∗, Davide Morelli2,3,∗, Giuseppe Cartella2, Marcella Cornia2,
Marco Bertini1, Rita Cucchiara2,4
1Đại học Florence, Ý2Đại học Modena và Reggio Emilia, Ý
3Đại học Pisa, Ý4IIT-CNR, Ý
1{tên.họ}@unifi.it2{tên.họ}@unimore.it

váy dài đỏ không tay
váy dài đỏ chạm sàn
váy dài đỏ một màuváy kem
váy cổ chữ V không tay tự nhiên
váy không tay màu beigeváy đai đen
váy quấn giả đen da giả da thậtváy dài hồng nhạt
váy sọc dài
váy dài màu hồng của phụ nữváy dạ hội ôm sát
váy maxi đỏ
váy dài cổ yếm đỏ một màuváy ôm vàng
váy ngắn vàng nhạt
váy ngắn vàng bóngváy chữ A chấm bi đen
váy sơ mi in chấm bi đen
váy chấm bi đen

Hình 1: Trong nghiên cứu này, chúng tôi đề xuất một khung thiết kế thời trang đa phương thức mới dựa trên mô hình khuếch tán tiềm ẩn có thể tạo ra hình ảnh thời trang mới được điều kiện bởi văn bản, điểm khớp con người và phác thảo trang phục.

Tóm tắt
Minh họa thời trang được các nhà thiết kế sử dụng để truyền đạt tầm nhìn của họ và đưa ý tưởng thiết kế từ khái niệm hóa đến hiện thực hóa, cho thấy cách quần áo tương tác với cơ thể con người. Trong bối cảnh này, thị giác máy tính có thể được sử dụng để cải thiện quy trình thiết kế thời trang. Khác với các nghiên cứu trước đây chủ yếu tập trung vào thử đồ ảo, chúng tôi đề xuất nhiệm vụ chỉnh sửa hình ảnh thời trang có điều kiện đa phương thức, hướng dẫn việc tạo ra hình ảnh thời trang lấy con người làm trung tâm bằng cách tuân theo các gợi ý đa phương thức, chẳng hạn như văn bản, tư thế cơ thể con người và phác thảo trang phục. Chúng tôi giải quyết vấn đề này bằng cách đề xuất một kiến trúc mới dựa trên mô hình khuếch tán tiềm ẩn, một phương pháp chưa từng được sử dụng trong lĩnh vực thời trang trước đây. Do thiếu các bộ dữ liệu hiện có phù hợp cho nhiệm vụ này, chúng tôi cũng mở rộng hai bộ dữ liệu thời trang hiện có, cụ thể là Dress Code và VITON-HD, với các chú thích đa phương thức được thu thập theo cách bán tự động. Kết quả thực nghiệm trên các bộ dữ liệu đa phương thức mới này chứng minh tính hiệu quả của đề xuất của chúng tôi, cả về tính thực tế và sự nhất quán với các đầu vào đa phương thức đã cho. Mã nguồn và các chú thích đa phương thức đã thu thập được công khai tại: https://github.com/aimagelab/multimodal-garment-designer.

1. Giới thiệu
Nghiên cứu Thị giác máy tính luôn chú ý nhiều đến cả con người và các vấn đề liên quan đến thời trang, đặc biệt là làm việc về nhận dạng và truy xuất các món đồ quần áo [11, 24], gợi ý các trang phục tương tự [8, 18, 41], và thử đồ ảo quần áo và phụ kiện [7, 13, 29, 30, 50, 55]. Trong những năm gần đây, một số nỗ lực nghiên cứu đã được dành cho nhiệm vụ chỉnh sửa hình ảnh có điều kiện văn bản, nơi mà cho một hình ảnh mô hình và mô tả văn bản về một trang phục, mục tiêu là tạo ra
∗Đóng góp ngang nhau.

1arXiv:2304.02051v2 [cs.CV] 23 Aug 2023

--- TRANG 2 ---
mô hình đầu vào mặc một món đồ quần áo mới tương ứng với mô tả văn bản đã cho. Trong bối cảnh này, chỉ có một vài nghiên cứu [19,35,59] đã được đề xuất, chỉ sử dụng các phương pháp dựa trên GAN cho bước tạo sinh.

Gần đây, các mô hình khuếch tán [10,17,32,44] đã thu hút ngày càng nhiều sự chú ý do khả năng tạo sinh xuất sắc của chúng, cho phép cải thiện nhiều nhiệm vụ downstream trong nhiều lĩnh vực khác nhau, trong khi khả năng áp dụng của chúng vào lĩnh vực thời trang vẫn chưa được khám phá. Nhiều giải pháp khác nhau đã được giới thiệu và có thể được xác định một cách thô sơ dựa trên các điều kiện khử nhiễu được sử dụng để hướng dẫn quá trình khuếch tán, có thể cho phép kiểm soát tốt hơn đầu ra được tổng hợp. Một loại mô hình khuếch tán cụ thể đã được đề xuất trong [39] thay vì áp dụng quá trình khuếch tán trong không gian pixel, định nghĩa các quá trình tiến và ngược trong không gian tiềm ẩn của một bộ mã hóa tự động được huấn luyện trước, trở thành một trong những lựa chọn hàng đầu nhờ chi phí tính toán giảm. Mặc dù giải pháp này có thể tạo ra hình ảnh có tính thực tế cao, nó không hoạt động tốt trong các nhiệm vụ tạo sinh lấy con người làm trung tâm và không thể xử lý nhiều tín hiệu điều kiện để hướng dẫn giai đoạn tạo sinh.

Trong nghiên cứu này, chúng tôi giải quyết một khung làm việc mở rộng và tổng quát hơn và định nghĩa nhiệm vụ mới về chỉnh sửa hình ảnh thời trang có điều kiện đa phương thức, cho phép hướng dẫn quá trình tạo sinh thông qua các gợi ý đa phương thức trong khi bảo tồn danh tính và hình dạng cơ thể của một người đã cho (Hình 1). Để giải quyết nhiệm vụ này, chúng tôi giới thiệu một kiến trúc mới, được gọi là Multimodal Garment Designer (MGD), mô phỏng quá trình của một nhà thiết kế hình thành một trang phục mới trên hình dạng mô hình, dựa trên các chỉ dẫn sơ bộ được cung cấp thông qua câu văn bản hoặc phác thảo trang phục. Cụ thể, bắt đầu từ Stable Diffusion [39], chúng tôi đề xuất một mạng khử nhiễu có thể được điều kiện bởi nhiều phương thức và cũng tính đến tính nhất quán tư thế giữa hình ảnh đầu vào và được tạo ra, do đó cải thiện hiệu quả của các mô hình khuếch tán lấy con người làm trung tâm.

Để giải quyết nhiệm vụ mới được đề xuất, chúng tôi trình bày một khung bán tự động để mở rộng các bộ dữ liệu hiện có với dữ liệu đa phương thức. Cụ thể, chúng tôi bắt đầu từ hai bộ dữ liệu thử đồ ảo nổi tiếng (tức là Dress Code [30] và VITON-HD [7]) và mở rộng chúng với các mô tả văn bản và phác thảo trang phục. Kết quả thực nghiệm trên hai bộ dữ liệu thời trang đa phương thức được đề xuất cho thấy cả định lượng và định tính rằng kiến trúc được đề xuất của chúng tôi tạo ra hình ảnh chất lượng cao dựa trên các đầu vào đa phương thức đã cho và vượt trội hơn tất cả các đối thủ cạnh tranh và baseline được xem xét, cũng theo đánh giá của con người.

Tóm lại, những đóng góp của chúng tôi như sau: (1) Chúng tôi đề xuất một nhiệm vụ mới về chỉnh sửa hình ảnh thời trang có điều kiện đa phương thức, bao gồm việc sử dụng dữ liệu đa phương thức để hướng dẫn việc tạo sinh. (2) Chúng tôi giới thiệu một kiến trúc tạo sinh lấy con người làm trung tâm mới dựa trên mô hình khuếch tán tiềm ẩn, có khả năng tuân theo các gợi ý đa phương thức trong khi bảo tồn đặc điểm của mô hình. (3) Để giải quyết nhiệm vụ mới, chúng tôi mở rộng hai bộ dữ liệu thời trang hiện có với các câu văn bản và phác thảo trang phục, thiết kế một khung chú thích bán tự động. (4) Các thực nghiệm mở rộng chứng minh rằng phương pháp được đề xuất vượt trội hơn các đối thủ cạnh tranh khác về tính thực tế và sự nhất quán với các đầu vào đa phương thức.

2. Nghiên cứu liên quan
Tạo sinh hình ảnh có hướng dẫn văn bản. Tạo ra một hình ảnh phản ánh trung thực gợi ý văn bản được cung cấp là mục tiêu của tổng hợp văn bản thành hình ảnh. Trong bối cảnh này, các phương pháp ban đầu dựa trên GAN [48, 54, 56, 58], trong khi hầu hết các giải pháp gần đây khai thác hiệu quả của các mô hình khuếch tán [33, 37, 39]. Trong lĩnh vực thời trang, chỉ có một vài nỗ lực tổng hợp văn bản thành hình ảnh đã được đề xuất [19, 35, 59]. Cụ thể, Zhu et al. [59] trình bày một giải pháp dựa trên GAN tạo ra hình ảnh cuối cùng được điều kiện bởi cả mô tả văn bản và bố cục ngữ nghĩa. Một phương pháp khác được giới thiệu trong [35], nơi một kỹ thuật điều chỉnh mã tiềm ẩn được sử dụng để tăng cường quá trình nghịch đảo GAN bằng cách khai thác nhúng văn bản CLIP [36] để hướng dẫn quá trình chỉnh sửa hình ảnh. Thay vào đó, Jiang et al. [19] đề xuất một kiến trúc tổng hợp hình ảnh toàn thân bằng cách ánh xạ các mô tả văn bản của các món đồ quần áo thành vectơ one-hot, tuy nhiên hạn chế khả năng biểu đạt của tín hiệu điều kiện.

Tạo sinh hình ảnh đa phương thức với mô hình khuếch tán. Một hướng nghiên cứu liên quan nhằm điều kiện các mô hình khuếch tán hiện có trên các phương thức khác nhau do đó cho phép kiểm soát tốt hơn quá trình tạo sinh [5, 6, 27, 31, 51]. Ví dụ, Choi et al. [6] đề xuất tinh chỉnh quá trình tạo sinh của một mô hình xác suất khuếch tán khử nhiễu không điều kiện [32] bằng cách khớp mỗi biến tiềm ẩn với hình ảnh tham chiếu đã cho. Trên một hướng khác, phương pháp được giới thiệu trong [27] thêm nhiễu vào đầu vào dựa trên nét vẽ và áp dụng phương trình vi phân ngẫu nhiên ngược để tổng hợp hình ảnh, mà không cần huấn luyện bổ sung. Wang et al. [51], thay vào đó, đề xuất học một không gian tiềm ẩn có ngữ nghĩa cao và thực hiện tinh chỉnh có điều kiện cho mỗi nhiệm vụ downstream để ánh xạ các tín hiệu hướng dẫn đến không gian được huấn luyện trước. Các nghiên cứu gần đây khác đề xuất thêm phác thảo như các tín hiệu điều kiện bổ sung, hoặc nối chúng với đầu vào mô hình [5] hoặc huấn luyện một bộ dự đoán cạnh dựa trên MLP để ánh xạ các đặc trưng tiềm ẩn thành bản đồ không gian [49].

Trong số các nghiên cứu đương đại nhằm điều kiện các mô hình khuếch tán tiềm ẩn được huấn luyện trước, ControlNet [57] đề xuất mở rộng mô hình Stable Diffusion [39] với một đầu vào điều kiện bổ sung. Quá trình này bao gồm việc tạo ra hai phiên bản của trọng số của mô hình gốc: một phiên bản vẫn cố định và không thay đổi (bản sao bị khóa) và một phiên bản khác có thể được cập nhật trong quá trình huấn luyện (bản sao có thể huấn luyện). Mục đích của việc này là cho phép phiên bản có thể huấn luyện học điều kiện mới được giới thiệu trong khi phiên bản bị khóa giữ lại kiến thức mô hình gốc. Mặt khác, T2I-Adapter [31] học các mô-đun adapter cụ thể theo phương thức cho phép điều kiện Stable Diffusion trên các phương thức mới.

Ngược lại, chúng tôi tập trung vào lĩnh vực thời trang và đề xuất một kiến trúc lấy con người làm trung tâm dựa trên mô hình khuếch tán tiềm ẩn trực tiếp khai thác điều kiện của các câu văn bản và các phương thức khác như tư thế cơ thể con người và phác thảo trang phục.

3. Phương pháp đề xuất
Trong phần này, chúng tôi đề xuất một nhiệm vụ mới để tự động chỉnh sửa hình ảnh thời trang lấy con người làm trung tâm được điều kiện trên nhiều phương thức. Cụ thể, cho hình ảnh mô hình I∈RH×W×3, bản đồ tư thế của nó P∈RH×W×18 nơi các kênh biểu diễn các điểm khớp con người, mô tả văn bản Y về một trang phục, và phác thảo của cùng một trang phục S∈RH×W×1, chúng tôi muốn tạo ra một hình ảnh mới Ĩ∈RH×W×3 giữ lại thông tin của mô hình đầu vào trong khi thay thế trang phục mục tiêu theo các đầu vào đa phương thức. Để giải quyết nhiệm vụ này, chúng tôi đề xuất một phương pháp khuếch tán tiềm ẩn mới, được gọi là Multimodal Garment Designer (MGD), có thể kết hợp hiệu quả thông tin đa phương thức khi tạo ra hình ảnh mới Ĩ. Kiến trúc đề xuất của chúng tôi là một khung tổng quát có thể dễ dàng mở rộng cho các phương thức khác như kết cấu và thông tin 3D. Chúng tôi tin chắc rằng nhiệm vụ này có thể thúc đẩy nghiên cứu trong lĩnh vực và nâng cao quá trình thiết kế các món đồ thời trang mới với khả năng tùy chỉnh cao hơn. Tổng quan về mô hình của chúng tôi được hiển thị trong Hình 2.

3.1. Kiến thức chuẩn bị
Trong khi các mô hình khuếch tán [44] là các kiến trúc biến tiềm ẩn hoạt động trong cùng một chiều với dữ liệu (tức là trong không gian pixel), các mô hình khuếch tán tiềm ẩn (LDM) [39] hoạt động trong không gian tiềm ẩn của một bộ mã hóa tự động được huấn luyện trước đạt được hiệu quả tính toán cao hơn trong khi bảo tồn chất lượng tạo sinh. Trong nghiên cứu của chúng tôi, chúng tôi tận dụng mô hình Stable Diffusion [39], một triển khai văn bản thành hình ảnh của LDM làm điểm khởi đầu để thực hiện điều kiện đa phương thức cho chỉnh sửa hình ảnh thời trang lấy con người làm trung tâm. Stable Diffusion bao gồm một bộ mã hóa tự động với bộ mã hóa E và bộ giải mã D, một mô hình khử nhiễu U-Net có điều kiện văn bản-thời gian εθ, và một bộ mã hóa văn bản dựa trên CLIP TE nhận văn bản Y làm đầu vào. Bộ mã hóa E nén một hình ảnh I thành một không gian tiềm ẩn có chiều thấp hơn được định nghĩa trong Rh×w×4, nơi h=H/8 và w=W/8. Bộ giải mã D thực hiện phép toán ngược, giải mã một biến tiềm ẩn thành không gian pixel. Để rõ ràng, chúng tôi định nghĩa đầu vào tích chập εθ (tức là zt trong trường hợp này) như đầu vào không gian γ do tính chất của tích chập bảo tồn cấu trúc không gian và đầu vào điều kiện attention như ψ. Mạng khử nhiễu εθ được huấn luyện theo hàm mất mát sau:

L=EE(I),Y,ε∼N(0,1),t[||ε−εθ(γ, ψ)||²₂], (1)

nơi t là bước thời gian khuếch tán, γ=zt, ψ=[t;TE(Y)], và ε∼N(0,1) là nhiễu Gaussian được thêm vào E(I).

3.2. Chỉnh sửa hình ảnh lấy con người làm trung tâm
Nhiệm vụ của chúng tôi nhằm tạo ra một hình ảnh mới Ĩ, bằng cách thay thế trong hình ảnh đầu vào I trang phục mục tiêu bằng cách sử dụng các đầu vào đa phương thức, trong khi bảo tồn danh tính và đặc điểm vật lý của mô hình. Như một hệ quả tự nhiên, nhiệm vụ này có thể được xác định là một loại inpainting cụ thể được thiết kế cho dữ liệu cơ thể con người. Thay vì sử dụng một mô hình văn bản thành hình ảnh tiêu chuẩn, chúng tôi thực hiện inpainting bằng cách nối dọc theo chiều kênh của đầu vào mạng khử nhiễu zt một hình ảnh có mặt nạ được mã hóa E(IM) và mặt nạ inpainting nhị phân tương ứng được thay đổi kích thước m∈{0,1}h×w×1, được suy ra từ mặt nạ inpainting gốc M∈{0,1}H×W×1. Vì ở đây, đầu vào không gian của mạng khử nhiễu là γ=[zt;m;E(IM)], γ∈Rh×w×9. Nhờ bản chất tích chập hoàn toàn của bộ mã hóa E và bộ giải mã D, kiến trúc dựa trên LDM này có thể bảo tồn thông tin không gian trong không gian tiềm ẩn. Khai thác tính năng này, phương pháp của chúng tôi có thể tùy chọn thêm các ràng buộc điều kiện vào việc tạo sinh. Cụ thể, chúng tôi đề xuất thêm hai ràng buộc tạo sinh bổ sung cho thông tin văn bản: bản đồ tư thế mô hình P để bảo tồn tư thế con người gốc của mô hình đầu vào và phác thảo trang phục S để cho phép người dùng cuối điều kiện tốt hơn quá trình tạo sinh trang phục.

Điều kiện bản đồ tư thế. Trong hầu hết các trường hợp [23, 26, 47], inpainting được thực hiện với mục tiêu loại bỏ hoặc hoàn toàn thay thế nội dung của vùng có mặt nạ. Tuy nhiên, trong nhiệm vụ của chúng tôi, chúng tôi nhằm loại bỏ tất cả thông tin liên quan đến trang phục mà mô hình đang mặc trong khi bảo tồn thông tin cơ thể và danh tính của mô hình. Do đó, chúng tôi đề xuất cải thiện quá trình inpainting trang phục bằng cách sử dụng hộp bao quanh của mặt nạ phân đoạn cùng với thông tin bản đồ tư thế biểu diễn các điểm khớp cơ thể. Phương pháp này cho phép bảo tồn đặc điểm vật lý của mô hình trong vùng có mặt nạ trong khi cho phép inpainting các trang phục với hình dạng khác nhau. Khác với các kỹ thuật inpainting thông thường, chúng tôi tập trung vào việc giữ lại và loại bỏ có chọn lọc thông tin cụ thể trong vùng có mặt nạ để đạt được kết quả mong muốn. Để nâng cao hiệu suất của mạng khử nhiễu với các điểm khớp cơ thể con người, chúng tôi sửa đổi lớp tích chập đầu tiên của mạng bằng cách thêm 18 kênh bổ sung, một kênh cho mỗi điểm khớp. Thêm các đầu vào mới thường sẽ yêu cầu huấn luyện lại mô hình từ đầu, do đó tiêu tốn thời gian, dữ liệu và tài nguyên, đặc biệt trong trường hợp các mô hình đói dữ liệu như các mô hình khuếch tán. Do đó, chúng tôi đề xuất mở rộng các kernel của lớp đầu vào được huấn luyện trước của mạng khử nhiễu với các trọng số được khởi tạo ngẫu nhiên được lấy mẫu từ phân phối đều [14] và huấn luyện lại toàn bộ mạng. Điều này giảm đáng kể số bước huấn luyện và cho phép huấn luyện với ít dữ liệu hơn. Các thực nghiệm của chúng tôi cho thấy rằng cải thiện như vậy tăng cường tính nhất quán của thông tin cơ thể giữa hình ảnh được tạo ra và hình ảnh gốc.

Kết hợp phác thảo. Mô tả đầy đủ một trang phục chỉ bằng mô tả văn bản là một nhiệm vụ thách thức do sự phức tạp và mơ hồ của ngôn ngữ tự nhiên. Trong khi văn bản có thể truyền đạt các thuộc tính cụ thể như phong cách, màu sắc và họa tiết của trang phục, nó có thể không cung cấp đủ thông tin về các đặc điểm không gian của nó, chẳng hạn như hình dạng và kích thước. Hạn chế này có thể cản trở việc tùy chỉnh món đồ quần áo được tạo ra ngoài khả năng khớp chính xác phong cách dự định của người dùng. Do đó, chúng tôi đề xuất tận dụng phác thảo trang phục để làm phong phú đầu vào văn bản với các chi tiết không gian tinh vi bổ sung. Chúng tôi đạt được điều này bằng cách tuân theo cùng một phương pháp được mô tả cho điều kiện bản đồ tư thế. Đầu vào không gian cuối cùng của mạng khử nhiễu của chúng tôi là γ=[zt;m;E(IM);p;s], [p;s]∈Rh×w×(18+1), p và s được thu được bằng cách thay đổi kích thước P và S để khớp với các chiều không gian tiềm ẩn. Trong trường hợp phác thảo, chúng tôi chỉ điều kiện các bước đầu của quá trình khử nhiễu vì các bước cuối có ít ảnh hưởng đến hình dạng [2].

Tổng hợp mặt nạ. Để bảo tồn danh tính mô hình khi thực hiện inpainting lấy con người làm trung tâm, chúng tôi thực hiện tổng hợp mặt nạ như bước cuối cùng của phương pháp được đề xuất. Định nghĩa Î=D(z₀)∈RH×W×3 là đầu ra của bộ giải mã D và Mhead∈{0,1}H×W×1 là mặt nạ nhị phân khuôn mặt mô hình của hình ảnh I, hình ảnh đầu ra cuối cùng Ĩ được thu được như sau: Ĩ=Mhead⊙I+(1−Mhead)⊙Î, nơi ⊙ biểu thị phép nhân từng phần tử.

3.3. Huấn luyện và suy luận
Như trong các mô hình khuếch tán tiềm ẩn tiêu chuẩn, cho một đầu vào được mã hóa z=E(I), mạng khử nhiễu được đề xuất được huấn luyện để dự đoán nhiễu được thêm ngẫu nhiên vào z. Hàm mục tiêu tương ứng có thể được chỉ định như sau

L=EE(I),Y,ε∼N(0,1),t,E(IM),m,p,s[||ε−εθ(γ, ψ)||²₂], (2)

nơi γ=[zt;m;E(IM);p;s] và ψ=[t;TE(Y)].

Hướng dẫn không phân loại. Hướng dẫn không phân loại là một kỹ thuật suy luận yêu cầu mạng khử nhiễu hoạt động cả có điều kiện và không điều kiện. Phương pháp này sửa đổi nhiễu mô hình được dự đoán không điều kiện bằng cách di chuyển nó về phía có điều kiện. Cụ thể, quá trình khuếch tán được dự đoán tại thời điểm t, cho điều kiện chung c, được tính như sau:

ε̂θ(zt|c) = εθ(zt|∅) + α·(εθ(zt|c) − εθ(zt|∅)), (3)

nơi εθ(zt|c) là nhiễu được dự đoán tại thời điểm t cho điều kiện c, εθ(zt|∅) là nhiễu được dự đoán tại thời điểm t cho điều kiện rỗng, và thang đo hướng dẫn α kiểm soát mức độ ngoại suy về phía điều kiện.

Vì mô hình của chúng tôi xử lý ba điều kiện (tức là văn bản, bản đồ tư thế và phác thảo), chúng tôi sử dụng biến thể hướng dẫn không phân loại đa điều kiện nhanh được đề xuất trong [1]. Thay vì thực hiện hướng dẫn không phân loại theo xác suất từng điều kiện, nó tính toán hướng của xác suất chung của tất cả các điều kiện Δt_joint=εθ(zt|{ci}i=1^N)−εθ(zt|∅):

ε̂θ(zt|{ci}i=1^N) = εθ(zt|∅) + α·Δt_joint. (4)

Điều này giảm số lần thực thi feed-forward từ N+1 xuống 2.

Huấn luyện không điều kiện. Đảm bảo khả năng của mô hình khử nhiễu hoạt động cả có và không có điều kiện được đạt được bằng cách thay thế điều kiện bằng một điều kiện rỗng tại thời điểm huấn luyện theo một xác suất cố định. Phương pháp này cho phép mô hình học từ cả mẫu có điều kiện và không điều kiện, dẫn đến cải thiện độ bao phủ mode và độ trung thực mẫu. Hơn nữa, kỹ thuật này cũng cho phép mô hình tùy chọn sử dụng các tín hiệu kiểm soát tại thời điểm dự đoán. Vì phương pháp của chúng tôi xem xét nhiều điều kiện, chúng tôi đề xuất mở rộng che mặt nạ đầu vào cho từng điều kiện một cách độc lập. Các thực nghiệm cho thấy rằng điều chỉnh tham số này có thể ảnh hưởng hiệu quả đến chất lượng của kết quả cuối cùng.

--- TRANG 4 ---

4. Thu thập bộ dữ liệu thời trang đa phương thức

Các bộ dữ liệu hiện có cho tạo sinh hình ảnh thời trang thường chứa hình ảnh độ phân giải thấp và thiếu tất cả thông tin đa phương thức cần thiết để thực hiện nhiệm vụ được mô tả trước đây. Vì lý do này, việc thu thập các bộ dữ liệu đa phương thức mới cho lĩnh vực thời trang đóng vai trò quan trọng để thúc đẩy nghiên cứu trong lĩnh vực. Để đạt mục tiêu này, chúng tôi bắt đầu từ hai bộ dữ liệu thời trang độ phân giải cao gần đây được giới thiệu cho nhiệm vụ thử đồ ảo, cụ thể là Dress Code [30] và VITON-HD [7], và mở rộng chúng với các câu văn bản và phác thảo trang phục. Cả hai bộ dữ liệu đều bao gồm các cặp hình ảnh với độ phân giải 1024×768, mỗi cặp gồm một hình ảnh trang phục và một mô hình tham chiếu mặc món đồ thời trang đã cho. Trong phần này, chúng tôi giới thiệu một khung để chú thích bán tự động hình ảnh thời trang với thông tin đa phương thức và cung cấp mô tả đầy đủ về cách làm phong phú Dress Code và VITON-HD với văn bản và phác thảo liên quan đến trang phục. Chúng tôi gọi các phiên bản mở rộng của các bộ dữ liệu này là Dress Code Multimodal và VITON-HD Multimodal, tương ứng. Các hình ảnh mẫu và dữ liệu đa phương thức của các bộ dữ liệu đã thu thập có thể được tìm thấy trong Hình 3.

4.1. Thu thập và chú thích bộ dữ liệu

Chuẩn bị dữ liệu. Chúng tôi bắt đầu chú thích từ bộ dữ liệu Dress Code, chứa hơn 53k cặp mô hình-trang phục của nhiều danh mục. Như bước đầu tiên, chúng tôi cần liên kết mỗi trang phục với một mô tả văn bản chứa các thuật ngữ cụ thể về thời trang và không chung chung đủ chi tiết nhưng không quá dài để có thể được khai thác để ràng buộc việc tạo sinh. Được thúc đẩy bởi các phát hiện gần đây trong lĩnh vực cho thấy rằng con người có xu hướng mô tả các món đồ thời trang chỉ bằng một vài từ [3], chúng tôi đề xuất sử dụng các cụm danh từ (tức là các câu văn bản ngắn gồm một danh từ cùng với các từ bổ nghĩa) có thể nắm bắt hiệu quả thông tin quan trọng trong khi giảm các từ hoặc chi tiết không cần thiết. Cho rằng việc chú thích thủ công tất cả hình ảnh sẽ tốn thời gian và tài nguyên¹, chúng tôi đề xuất một khung mới để chú thích bán tự động bộ dữ liệu bằng cách sử dụng các cụm danh từ. Đầu tiên, các chú thích cụ thể theo lĩnh vực được thu thập từ hai bộ dữ liệu thời trang có sẵn, cụ thể là FashionIQ [53] và Fashion200k [12], chuẩn hóa chúng bằng lemmatization từ và cuối cùng giảm mỗi từ về dạng gốc với thư viện NLTK². Sau đó, chúng tôi trích xuất các cụm danh từ từ các chú thích, lọc kết quả bằng cách loại bỏ tất cả các mục văn bản bắt đầu bằng hoặc chứa ký tự đặc biệt. Sau giai đoạn tiền xử lý này, chúng tôi thu được hơn 60k cụm danh từ duy nhất, được chia thành ba danh mục khác nhau (tức là quần áo phần trên, quần áo phần dưới và váy).

¹Vì bộ dữ liệu Dress Code bao gồm hơn 53k món đồ thời trang và giả định rằng mỗi chú thích yêu cầu khoảng 5 phút, một người chú thích làm việc 8 tiếng mỗi ngày, 5 ngày mỗi tuần, và 260 ngày làm việc mỗi năm sẽ mất hơn 2 năm để hoàn thành nhiệm vụ chú thích.
²https://www.nltk.org/

[Hình 3: Các hình ảnh mẫu và dữ liệu đa phương thức từ các bộ dữ liệu mới thu thập của chúng tôi.]

[Bảng 1: So sánh Dress Code và VITON-HD Multimodal với các bộ dữ liệu thời trang khác có chú thích đa phương thức.]

Để xác định các cụm danh từ phù hợp nhất cho mỗi trang phục, chúng tôi sử dụng mô hình CLIP [36] và bản điều chỉnh mã nguồn mở của nó (tức là OpenCLIP [52]). Chúng tôi chọn các mô hình VIT-L14@336 và RN50×64 cho CLIP, và các mô hình VIT-L14, ViT-H14, và ViT-g14 cho OpenCLIP. Tổng hợp gợi ý được thực hiện để cải thiện kết quả và, đối với mỗi hình ảnh, chúng tôi chọn 25 cụm danh từ dựa trên top-5 cụm danh từ mỗi mô hình được đánh giá bởi độ tương đồng cosine giữa các embedding hình ảnh và văn bản, tránh lặp lại.

Chú thích văn bản chi tiết. Để đảm bảo tính chính xác và tính đại diện của các chú thích của chúng tôi, chúng tôi chú thích thủ công một phần đáng kể của các hình ảnh Dress Code. Cụ thể, chúng tôi chọn ba cụm danh từ đại diện nhất, trong số 25 cụm được liên kết tự động, với mỗi hình ảnh trang phục. Để giảm thiểu thời gian chú thích, chúng tôi phát triển một công cụ chú thích tùy chỉnh hạn chế thời gian chú thích trung bình xuống 60 giây mỗi mục và cho phép người chú thích chèn thủ công các cụm danh từ trong trường hợp không có cụm nào được trích xuất tự động phù hợp cho hình ảnh. Nhìn chung, chúng tôi chú thích thủ công 26.400 trang phục khác nhau (8.800 cho mỗi danh mục) trong tổng số 53.792 sản phẩm có trong bộ dữ liệu, đảm bảo bao gồm tất cả các món đồ thời trang của bộ thử nghiệm gốc [30].

Chú thích văn bản thô. Để hoàn thành chú thích, trước tiên chúng tôi tinh chỉnh mô hình OpenCLIP ViT-B32, được huấn luyện trước trên phần tiếng Anh của bộ dữ liệu LAION5B [42], bằng cách sử dụng các cặp hình ảnh-văn bản được chú thích mới. Sau đó chúng tôi sử dụng mô hình này và tập hợp các cụm danh từ đã thu thập để tự động gắn thẻ tất cả các phần tử còn lại của bộ dữ liệu Dress Code với ba cụm danh từ tương tự nhất, luôn được xác định thông qua độ tương đồng cosine giữa các embedding đa phương thức. Chúng tôi sử dụng cùng một chiến lược để tự động chú thích tất cả hình ảnh trang phục của bộ dữ liệu VITON-HD. Trong trường hợp này, vì bộ dữ liệu này chỉ chứa quần áo phần trên, chúng tôi giới hạn các cụm danh từ bảng với những cụm mô tả trang phục phần trên.

Trích xuất phác thảo. Việc giới thiệu phác thảo trang phục có thể cung cấp các chi tiết thiết kế có giá trị không dễ phân biệt chỉ từ văn bản. Bằng cách này, bộ dữ liệu có thể cung cấp một đại diện chính xác và toàn diện hơn về các trang phục, dẫn đến cải thiện chất lượng và kiểm soát tốt hơn các chi tiết thiết kế được tạo ra. Để trích xuất phác thảo cho cả bộ dữ liệu Dress Code và VITON-HD, chúng tôi sử dụng PiDiNet [46], một mạng phát hiện cạnh được huấn luyện trước.

Cho rằng các bộ dữ liệu đã chọn ban đầu được giới thiệu cho thử đồ ảo, chúng bao gồm cả bộ thử nghiệm có cặp và không có cặp. Trong khi đối với bộ có cặp, chúng tôi có thể trực tiếp sử dụng mặt nạ phân tích con người để trích xuất trang phục quan tâm mà mô hình đang mặc và sau đó đưa vào mạng phát hiện cạnh, đối với bộ không có cặp, chúng tôi cần tạo ra trước một phiên bản biến dạng của trang phục trong cửa hàng khớp với tư thế và hình dạng cơ thể của mô hình đích. Theo các phương pháp thử đồ ảo [50,55], chúng tôi huấn luyện một mô-đun biến đổi hình học thực hiện biến đổi spline tấm mỏng [38] của trang phục đầu vào và sau đó tinh chỉnh kết quả bị biến dạng bằng một mô hình U-Net [40]. Từ mỗi trang phục bị biến dạng, chúng tôi trích xuất hình ảnh phác thảo cho phép sử dụng giải pháp được đề xuất ngay cả trong các cài đặt không có cặp.

4.2. So sánh với các bộ dữ liệu khác

Chỉ hai bộ dữ liệu tạo sinh hình ảnh từ văn bản có sẵn trong lĩnh vực thời trang [19, 59] đều dựa trên hình ảnh từ bộ dữ liệu DeepFashion [24]. Trong khi bộ dữ liệu được giới thiệu trong [59] chứa các mô tả văn bản ngắn, DeepFashion-Multimodal [19] được chú thích với các thuộc tính (ví dụ như danh mục, màu sắc, vải, v.v.) có thể được tổng hợp thành các chú thích dài hơn. Trong Bảng 1, chúng tôi tóm tắt các thống kê chính của các chú thích văn bản của các bộ dữ liệu công khai so với các bộ dữ liệu mới được mở rộng của chúng tôi. Như có thể thấy, các bộ dữ liệu của chúng tôi chứa nhiều biến đổi hơn về mặt mục văn bản và từ, khẳng định tính phù hợp của thủ tục chú thích của chúng tôi và cho phép kiểm soát cá nhân hóa hơn quá trình tạo sinh. Ngoài ra, đáng chú ý là các bộ dữ liệu khác không có hình ảnh trang phục trong cửa hàng khiến chúng khó sử dụng trong trường hợp của chúng tôi.

5. Đánh giá thực nghiệm

5.1. Chi tiết triển khai và đối thủ cạnh tranh

Huấn luyện và suy luận. Tất cả các mô hình được huấn luyện trên các phần chia gốc của bộ dữ liệu Dress Code Multimodal và VITON-HD Multimodal trên một GPU NVIDIA A100 đơn lẻ trong 150k bước, sử dụng kích thước batch 16, tốc độ học 10⁻⁵ với làm ấm tuyến tính cho 500 vòng lặp đầu tiên, và AdamW [25] làm optimizer với weight decay 10⁻². Để tăng tốc huấn luyện và tiết kiệm bộ nhớ, chúng tôi sử dụng độ chính xác hỗn hợp [28]. Chúng tôi đặt cả phần của các bước được điều kiện bởi phác thảo và phần của các điều kiện có mặt nạ trong quá trình huấn luyện là 0,2. Trong quá trình suy luận, chúng tôi sử dụng DDIM [45] với 50 bước làm scheduler nhiễu và đặt tham số hướng dẫn không phân loại α là 7,5.

Baseline và đối thủ cạnh tranh. Như đối thủ cạnh tranh đầu tiên, chúng tôi sử dụng triển khai sẵn có của pipeline inpainting Stable Diffusion³ được cung cấp bởi Huggingface. Hơn nữa, chúng tôi điều chỉnh hai mô hình hiện có, cụ thể là FICE [35] và SDEdit [27], để hoạt động trên cài đặt của chúng tôi. Cụ thể, chúng tôi huấn luyện lại tất cả các thành phần chính của mô hình FICE trên các bộ dữ liệu mới thu thập. Chúng tôi sử dụng cùng độ phân giải được sử dụng bởi các tác giả (tức là 256×256), giảm mẫu mỗi hình ảnh xuống 256×192 và áp dụng padding để khớp với kích thước mong muốn (sau đó được loại bỏ trong quá trình đánh giá). Để so sánh mô hình của chúng tôi với một chiến lược điều kiện khác, chúng tôi sử dụng phương pháp được đề xuất trong [27] bằng cách sử dụng mô hình của chúng tôi được huấn luyện chỉ với văn bản và tư thế con người làm phương thức đầu vào và thực hiện hướng dẫn phác thảo bằng cách sử dụng hình ảnh phác thảo với nhiễu ngẫu nhiên được thêm vào làm biến tiềm ẩn khởi đầu. Theo hướng dẫn của bài báo gốc, chúng tôi sử dụng 0,8 làm tham số cường độ.

5.2. Metrics đánh giá

Để đánh giá tính thực tế của hình ảnh được tạo ra, chúng tôi sử dụng Fréchet Inception Distance (FID) [16] và Kernel Inception Distance (KID) [4]. Đối với cả hai metrics, chúng tôi áp dụng triển khai được đề xuất trong [34]. Thay vào đó, để đánh giá sự tuân thủ của hình ảnh với đầu vào điều kiện văn bản, chúng tôi sử dụng CLIP Score (CLIP-S) [15] được cung cấp trong thư viện TorchMetrics [9], sử dụng mô hình OpenCLIP ViT-H/14 làm kiến trúc đa phương thức chéo. Chúng tôi tính điểm trên vùng inpainted của đầu ra được tạo ra được dán trên nền trắng 224×224.

Khoảng cách tư thế (PD). Chúng tôi đề xuất một metric khoảng cách tư thế mới đo lường sự nhất quán của tư thế cơ thể con người giữa hình ảnh được tạo ra và hình ảnh gốc bằng cách ước tính khoảng cách giữa các điểm khớp con người được trích xuất từ hình ảnh gốc và được tạo ra. Cụ thể, chúng tôi sử dụng mạng ước tính tư thế con người OpenPifPaf [22] và tính khoảng cách ℓ₂ giữa mỗi cặp điểm khớp tương ứng thực-được tạo ra. Chúng tôi chỉ xem xét các điểm khớp liên quan đến việc tạo sinh (tức là rơi vào mặt nạ M) và cân mỗi khoảng cách điểm khớp với độ tin cậy của detector để tính đến bất kỳ lỗi ước tính nào.

Khoảng cách phác thảo (SD). Để định lượng sự tuân thủ của hình ảnh được tạo ra với ràng buộc phác thảo, chúng tôi đề xuất một metric khoảng cách phác thảo mới.

³https://huggingface.co/runwayml/stable-diffusion-inpainting

--- TRANG 6 ---

[Phần tiếp theo sẽ được dịch trong lần gửi tiếp theo do giới hạn độ dài]
