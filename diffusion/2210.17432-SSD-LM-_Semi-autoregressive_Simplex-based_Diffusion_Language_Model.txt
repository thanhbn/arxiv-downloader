# 2210.17432.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/diffusion/2210.17432.pdf
# File size: 1439598 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model
for Text Generation and Modular Control
Xiaochuang Han♠Sachin Kumar♣Yulia Tsvetkov♠
♠Paul G. Allen School of Computer Science & Engineering, University of Washington
♣Language Technologies Institute, Carnegie Mellon University
{xhan77, yuliats}@cs.washington.edu♠sachink@cs.cmu.edu♣
Abstract
Despite the growing success of diffusion mod-
els in continuous-valued domains (e.g., im-
ages), similar efforts for discrete domains such
as text have yet to match the performance of au-
toregressive language models. In this work, we
present SSD-LM —a diffusion-based language
model with two key design choices. First, SSD-
LM issemi-autoregressive , iteratively gener-
ating blocks of text, allowing for flexible out-
put length at decoding time while enabling lo-
cal bidirectional context updates. Second, it
issimplex-based , performing diffusion on the
natural vocabulary space rather than a learned
latent space, allowing us to incorporate clas-
sifier guidance and modular control using off-
the-shelf classifiers without any adaptation. We
evaluate SSD-LM on unconstrained text gen-
eration benchmarks, and show that it matches
or outperforms strong autoregressive GPT-2
models across standard quality and diversity
metrics, while vastly outperforming diffusion-
based baselines. On controlled text generation,
SSD-LM also outperforms competitive base-
lines, with an extra advantage in modularity.1
1 Introduction
Diffusion models (Sohl-Dickstein et al., 2015),
trained to iteratively refine noised inputs, have re-
cently emerged as powerful tools for generative
modeling in several continuous-valued domains
such as images (Ho et al., 2020), audio (Kong et al.,
2021), video (Ho et al., 2022), among others. At-
tempts to adapt them for discrete domains such as
text data, however, have only had limited success:
prior work have shown to be promising on special-
ized cases and small datasets (Hoogeboom et al.,
2021; Austin et al., 2021; Li et al., 2022; Chen et al.,
2022), but diffusion models for text still underper-
form (and thus are not widely adopted) compared to
autoregressive language models (AR-LMs) which
1Our code and models can be found at https://
github .com/xhan77/ssd-lm .remain the state-of-the-art general purpose text gen-
erators (Radford et al., 2019; Brown et al., 2020).
Despite potential advantages of diffusion models
for text, there are two key challenges. First, dif-
fusion models generate text non-autoregressively,
i.e., they generate (and update) the entire sequence
simultaneously rather than token by token left-to-
right. Although this property is useful in practice
since each output token is informed by a broader
bi-directional context (Lee et al., 2018; Ghazvinine-
jad et al., 2019), it requires pre-defining an output
sequence length. This limits the flexibility and ap-
plicability of trained models. On the other hand,
non-autoregressive training with long sequences is
expensive and difficult to optimize. In this work,
we propose a semi-autoregressive solution which
strikes a balance between length flexibility and the
ability to alter previously generated tokens.
A major advantage of diffusion models over the
current standard of autoregressive LMs is their
post-hoc controllability using guidance from aux-
iliary models such as style classifiers (Dhariwal
and Nichol, 2021). However, controllability is hard
to achieve without compromises in modularity in
diffusion-based LMs for text. To enable diffusion
generation into discrete text rather than continu-
ous modalities, prior approaches have employed
different approximations, e.g., training with embed-
dings, character, or byte-level methods (Li et al.,
2022; Hoogeboom et al., 2021; Austin et al., 2021;
Chen et al., 2022). In contrast, existing mainstream
LMs and the guidance classifiers they derive often
operate at a sub-word level with sub-word repre-
sentations trained jointly with the language model
(Devlin et al., 2019; Liu et al., 2019; Raffel et al.,
2020). Subsequently, changing the input represen-
tations to characters or embeddings requires devel-
oping guidance models from scratch, which can be
expensive or infeasible in many cases. In this work,
we propose a simplex-based solution which enables
the diffusion over discrete texts while maintainingarXiv:2210.17432v2  [cs.CL]  26 Jun 2023

--- PAGE 2 ---
the advantages of diffusion models with plug-and-
control guidance models.
In sum, to enable diffusion-based LMs for text
we present SSD-LM (§3), addressing the above two
challenges. SSD-LM is trained to generate text
semi-autoregressively—generating blocks of to-
kens left-to-right with bidirectional context within
the block—which offers the benefits of both AR-
LMs and diffusion models. It supports training
with and generating variable-length sequences. At
the same time, it allows refinement within the token
block, in contrast to token-level autoregressive de-
coding where previously generated tokens cannot
be modified at all. SSD-LM uses the same tokeniza-
tion as popular AR-LMs, representing discrete text
via a distribution (or simplex) defined over the vo-
cabulary and is trained to reconstruct texts from
noisy versions of the distributions. Due to its un-
derlying representation, our method also offers an
easy and modular way of guided (controlled) gen-
eration using off-the-shelf text classifiers under the
minimal assumption of shared tokenizer.
Our evaluation experiments show, for the first
time, that a diffusion-based LM matches or out-
performs strong AR-LMs on standard text gener-
ation benchmarks (§4). We evaluate SSD-LM on
two tasks: (1) unconstrained prompt-based gen-
eration substantially outperforming existing diffu-
sion LM approaches and performing on par with
or outperforming strong autoregressive LM GPT-
2 (Radford et al., 2019) on both quality and diver-
sity (§4.2); and (2) controlled text generation with
guidance from off-the-shelf classifiers (no post-
hoc training/adaptation) outperforming competitive
controlled text generation baselines (§4.3).
2 Background
2.1 Diffusion model
Since their inception as image generators, diffu-
sion models (and their cousins score-based mod-
els (Song and Ermon, 2019)) have been widely
adopted as high-quality generative models for mul-
tiple data modalities. Here, we briefly describe
a simplified view of a canonical method, denois-
ing diffusion probabilistic models (Ho et al., 2020,
DDPM) which we adapt in this work for text
generation. We assume a given dataset D=
{1x0, . . . ,Nx0}of continuous valued itemsix0
(e.g., pixel values of an image) henceforth referred
to asx0for simplicity.Training Training a diffusion model first in-
volves adding a series of Gaussian noise to the
original data x0, through Ttimesteps:
xt=√¯αtx0+√
1−¯αtϵt (1)
where t∈(1, T)andϵt∼ N (0,I).¯αt=Qt
t′=1αt′, where αt′follow a predefined sched-
ule such that ¯αt→0ast→T. This process is
called forward diffusion . A diffusion model (pa-
rameterized by θ) is trained to reverse this forward
process by predicting the added noise ϵtgivenxt
with the following loss:
L(θ) =Et∼U(1,T)∥ϵθ(xt, t)−ϵt∥2(2)
Inference To get an output from this model, we
sample xT∼ N(0,I)and iteratively reconstruct a
sample x0by going back in time,
xt−1=1√αt(xt−1−αt√1−¯αtϵθ(xt, t)) (3)
fort=T, . . . , 1.2The key obstacle in using vanilla
diffusion models directly as text generators is that
language consists of discrete tokens, i.e., a non-
continuous x0to which a continuous valued Gaus-
sian noise cannot be added. We propose a straight-
forward and effective solution by treating tokens
as continuous valued simplexes over the vocabu-
lary (Hoang et al., 2017). Other existing methods
addressing this problem are discussed in §5.
2.2 Autoregressive LM
An autoregressive LM model optimizes for the like-
lihood of a sequence of tokens w0, . . . , wL−1.
pθ(w0:L) =L−1Y
c=0pθ(wc|w<c) (4)
To decode from AR-LMs, one can provide a con-
textw<cand decode the next token wciteratively
by predicting pθ(wc|w<c)and sampling from it
to get the discrete token (Fan et al., 2018; Holtz-
man et al., 2020). Prior work has shown that these
decoding approaches (and by extension the LMs
themselves) are prone to degrade when generating
long sequences and often devolve into repeating
subsequences (Holtzman et al., 2020; Meister et al.,
2022). In addition, such LMs do not provide a natu-
ral way to incorporate sequence-level control as to-
kens are generated one at a time without the ability
2We omit an additional noise term zhere for simplicity,
which is present in DDPM but not in another variant DDIM
(Song et al., 2021).

--- PAGE 3 ---
Figure 1: Training SSD-LM (a snapshot at context size
c= 2, block size B= 3). Horizontal axis represents the
order of tokens. Vertical axis represents the diffusion
timesteps. Shade means observable variables. Square
means discrete vocabulary, while circle means continu-
ous logits. Red components are inputs to the learning
model θ.
Figure 2: Decoding from SSD-LM (continuing Fig-
ure 1). Red components are inputs to the learned model
θ. Dash means intermediate variables.
to modify previously generated tokens (Dathathri
et al., 2020; Kumar et al., 2022b). In this work,
we present a method to train a semi-autoregressive
LM that decodes blocks of Btokens at a time, al-
leviating said issues with the support of diffusion
models. Existing literature addressing the two is-
sues individually are discussed in §5.
3 S SD-LM
We introduce SSD-LM —Semi-autoregressive
Simplex-based Diffusion Language Model— adapt-
ing key components from both autoregressive LM
and vanilla diffusion models. Conceptually, SSD-
LM uses diffusion model to decode wc:c+B, a
block of tokens of length B, given a Gaussian noise
and a context w<cof length c. We show an intu-
itive diagram and pseudo-code for the training and
decoding algorithm of SSD-LM in Figure 1, Fig-
ure 2, and Figure 3.3.1 Training
Continuous data representation To build a con-
tinuous representation for discrete tokens, we adopt
analmost-one-hot simplex representation over the
model’s vocabulary V. We define a simple op-
eration logits-generation( .)to map a token wto
˜w∈ {− K,+K}|V|as follows.
˜w(i)=(
+Kwhen w=V(i)
−Kwhen w̸=V(i)(5)
where iis the index of the vocabulary. We call
˜wthe logits for token w, and softmax( ˜w)gives
a probability simplex over the vocabulary V, with
a probability mass concentrated on the token w.
There is no learnable parameter in this mapping.
Forward diffusion Following Ho et al. (2020),
we add a time-dependent Gaussian noise to the
logits.
˜wc:c+B
0 = logits-generation( wc:c+B)(6)
˜wc:c+B
t =√¯αt˜wc:c+B
0 +√
1−¯αtϵt (7)
where t∈(1, T),ϵt∼ N(0, K2I), and ¯αt→0as
t→T. At the final step T,softmax( ˜wc:c+B
T)are
fully noisy simplexes over V, with a logit-normal
distribution (Atchison and Shen, 1980).
Loss function In Eq. 2, a diffusion model is
trained to predict the added noise from the noisy
representations. Since the forward diffusion pro-
cess can be computed in a single step (Eq. 1), the
notion here is equivalent to predicting the origi-
nal data representation (Song et al., 2021; Li et al.,
2022). Our objective follows the same intuition but
estimates a likelihood instead of the L2 distance
while conditioning on additional context:3
L(θ) =E[−logpθ(wc:c+B|˜wc:c+B
t,w<c)](8)
=E
c+B−1X
j=c−logpθ(wj|˜wc:c+B
t,w<c)

(9)
E[·]is a shorthand for Ec∼U(1,L−B),t∼U(1,T)[·].
The architecture for θthroughout this work is a
bi-directional Transformer encoder (Vaswani et al.,
2017). Specifically, the input to the model is a con-
catenation of the context w<cand a sequence of
noisy vocabulary simplexes softmax( ˜wc:c+B
t )of
3L2 distance did not work in our pilot study potentially
due to the intrinsically skewed simplex representation.

--- PAGE 4 ---
Algorithm 1 Training
1:repeat
2:w0:L∼q(w0:L)
3:c∼Uniform( {1, . . . , L −B})
4:˜wc:c+B
0 = logits-generation( wc:c+B)
5:t∼Uniform( {1, . . . , T })
6:ϵ∼ N(0, K2I)
7:˜wc:c+B
t =√¯αt˜wc:c+B
0 +√1−¯αtϵ
8: Take gradient descent step on
∇θ[−Pc+B−1
j=clogpθ(wj|˜wc:c+B
t ,w<c)]
9:until convergedAlgorithm 2 Decoding (at a given c)
1:˜wc:c+B
T∼ N(0, K2I)
2:fort=T, . . . , 1do
3:wc:c+B
logits = logitsθ(wc:c+B|˜wc:c+B
t ,w<c)
4:ˆwc:c+B= logits-projection( wc:c+B
logits )if uncontrolled,
elseˆwc:c+B= logits-projection( wc:c+B
logits +λ∇wfϕ(·))
5:z∼ N(0, K2I)
6:˜wc:c+B
t−1=√¯αt−1ˆwc:c+B+√1−¯αt−1z
7:end for
8:return argmax ˜wc:c+B
0
Figure 3: Training and decoding algorithms for SSD-LM . The training algorithm starts with sampling a sequence
from the pretraining data q(w0:L). The decoding algorithm can be applied miterations to obtain a m·B-token
generation, with the returned Btokens at each iteration appended to the previous generation, increasing c.
length B. The target output is the original tokens
wc:c+Bat positions ctoc+B.
One minimal modification made to the Trans-
former model is that in addition to the conventional
embedding lookup for w<c, we modify the em-
bedding layer to take as input a distribution over
the vocabulary, softmax( ˜wc:c+B
t ), and compute
the embedding vector as a weighted sum of the
embedding table. A timestep embedding is also
added before the first Transformer block to inform
the model of the current timestep.4
In §A, we present another interpretation of the
training objective as an intuitive contrastive loss.
3.2 Decoding
Logits projection Similar to continuous-valued
diffusion models, sampling from SSD-LM involves
reverse diffusion from t=T, . . . , 1starting with
a Gaussian noise. At any timestep t, our model θ
takes as input noised logits ˜wc:c+B
t and estimates
the probability distribution of the original tokens
in data by first predicting the logits:
wc:c+B
logits,t= logitsθ(wc:c+B|˜wc:c+B
t,w<c)(10)
which are then converted to a distribution via soft-
max. To feed this output to the next step of reverse
diffusion, t−1, we define a logits-projection oper-
ation to build a predicted data representation close
to the initial data representation (almost-one-hot
mapping; Eq. 5). We consider three projection
operations.
4More specifically, we have word embeddings for the con-
text,Emb ctx(w<c), and for the noisy diffusion representa-
tions, Wdiff[softmax( ˜wc:c+B
t )]. The timestep embedding is
added to the diffusion word embeddings, Wtime(t/T). It is
similar to positional embeddings, just not varying across se-
quence positions. We fold it in θfor notation simplicity.•Greedy: creates an almost-one-hot logit cen-
tered at the highest probability token.5
ˆw(i)=(
+Kifi=argmax( wlogits)
−Kotherwise(11)
•Sampling: creates an almost-one-hot logit cen-
tered around a token sampled from the output
distribution using top- psampling (Holtzman
et al., 2020). pis a hyperparameter.
ˆw(i)=(
+Kifi=top-p-sample (wlogits)
−Kotherwise
(12)
•Multi-hot: creates an almost-one-hot logit cen-
tered around alltokens in the top- pnucleus.
ˆw(i)=(
+Kifi∈top-p-all(wlogits)
−Kotherwise(13)
Decoding iteration Starting from pure noise
˜wc:c+B
T∼ N(0, K2I), in each decoding timestep
we compute:
ˆwc:c+B
t = logits-projection( wc:c+B
logits,t) (14)
˜wc:c+B
t−1=√¯αt−1ˆwc:c+B
t +p
1−¯αt−1z(15)
fort=T, . . . , 1andz∼ N(0, K2I).
Att= 1, the final B-token block is computed
simply as argmax ˜wc:c+B
0 . To generate the next
block, we concatenate the generated block to the
previous context to create a new context of length
c+Band follow the reverse-diffusion process again
as described above. This process can be repeated
until the maximum desired length is reached.6
5This shares a similar intuition as a greedy clamping trick
in the embedding-based diffusion in Li et al. (2022).
6Alternatively, one can also terminate the process if certain
special end-of-sequence tokens have been generated.

--- PAGE 5 ---
It is worth noting that our proposed decoding
algorithm is novel and different from the DDPM
decoding (Eq. 3). The DDPM decoding is designed
for diffusion in a continuous space and failed to
generate sensible outputs in our preliminary ex-
periments based on simplexes. In §B, we draw a
theoretical connection between our decoding algo-
rithm and DDPM decoding, and also highlight the
intuitive difference between the two.
Highly-modular control A useful property of
continuous diffusion models that naturally arises
from their definition is the ability to guide the gen-
erated samples to have user-defined attributes at
test time. This can be done using gradients from
auxiliary models such as classifiers (Dhariwal and
Nichol, 2021), e.g., guiding the output of an LM to
be of a positive sentiment using a sentiment classi-
fier. There is a vibrant community of developers on
platforms such as HuggingFace where many such
text classifiers are publicly available. The under-
lying data representation of SSD-LM is based on
vocabulary simplexes. Hence, as long as a classi-
fier shares the same tokenizer as the LM, it can be
used for control in an off-the-shelf manner without
modifications. This is in contrast to prior work in
diffusion language models that do not support such
classifiers due to differences in their input represen-
tation space (Hoogeboom et al., 2021; Austin et al.,
2021; Li et al., 2022; Chen et al., 2022) and require
retraining the classifiers from scratch. This ability
makes SSD-LM highly modular for controlled text
generation and offers key benefits: (1) Training
accurate classifiers for many tasks requires huge
amounts of data where retraining them can be quite
expensive, and (2) this approach allows control
from classifiers that are open to use but have been
trained on closed source data.
To guide SSD-LM to generate texts with a target
attribute yvia a standalone attribute model fϕ(·),
we update wc:c+B
logits,t(Eq. 10) at each timestep tto
the form below, drifting according to the gradients
from the attribute classifier.
wc:c+B
logits,t+λ∇wc:c+B
logits,tfϕ(y|wc:c+B
logits,t,w<c)(16)
where λis a hyperparameter balancing the weight
of control. The parameters of the standalone at-
tribute model ϕare frozen. We make a trivial mod-
ification to the embedding computation as in §3.1,
to allow the classifier to take as input a simplex.3.3 Additional details
Forward diffusion coefficient ¯αtWe follow
Nichol and Dhariwal (2021) for a cosine sched-
ule of ¯αt:
¯αt=r(t)
r(0), r(t) = cos(t/T+s
1 +s·π
2)2(17)
where sis small offset set to 1e-4 in our work and
αt=¯αt
¯αt−1.
Fewer timesteps Tin decoding Decoding from
diffusion models requires a series of timesteps ( T)
which can be computationally expensive if Tis
large. Following Li et al. (2022), we consider using
a smaller value of Tat test time to improve decod-
ing speed. In this work, we primarily experiment
withTdecode =Ttrain
2andTdecode =Ttrain
5.
Flexible decoding block size BOurSSD-LM is
trained with a fixed token block size Btrain. How-
ever, the decoding algorithm has a freedom to
use a different Bdecode . In our experiments, we
consider both scenarios of Btrain=Bdecode and
Btrain̸=Bdecode . Nevertheless, we leave for fu-
ture work a more detailed analysis of the impact of
the difference between BtrainandBdecode on model
performance.
4 Experiments
4.1 S SD-LM pretraining setup
Model architecture We use a bidirectional
Transformer encoder RoBERTa-large (Liu et al.,
2019) (0.4B, comparable size to GPT2-medium)
asSSD-LM ’s underlying architecture.7Note that
RoBERTa uses a general BPE tokenization (Sen-
nrich et al., 2016), same as a variety of LMs such as
GPT-2 (Radford et al., 2019), GPT-3 (Brown et al.,
2020), OPT (Zhang et al., 2022), etc. Any attribute
classifier using the same tokenization strategy can
be used to control SSD-LM in a highly modular
way.
Pretraining data, constants, and resource We
train SSD-LM on the same data as GPT2 to
make fair comparisons possible: OpenWebText
(Gokaslan and Cohen, 2019) which contains 9B
tokens. Following Zhang et al. (2022), we consider
7We initialize the model with RoBERTa’s weights as well.
We observe in our initial exploration that it helps the training
loss converge faster than a randomly initialized model. How-
ever, given enough computational resources, we conjecture
that a randomly initialized model will offer similar perfor-
mance.

--- PAGE 6 ---
this data as one contiguous sequence of tokens
and break it into sequences of length 200 (same as
the maximum sequence length our model accepts).
We randomly sample 99% of these sequences for
pretraining while leaving the rest as held out for
evaluation. We use the following model hyperpa-
rameters:8
L= 200 , Btrain= 25, Ttrain= 5000 , K= 5
We use an aggregated batch size of 6,144 and
a learning rate of 1e-4 with an AdamW optimizer
(Loshchilov and Hutter, 2019). We trained SSD-
LMfor 100K steps, which took about 6 days on 32
Nvidia V100 GPUs.
Pretraining loss Canonical training-time per-
plexity of LMs is not compatible with diffusion
LMs due to the difference in the inputs to the mod-
els (Eq. 4 and Eq. 9). Our pretraining loss is a
per-token negative log-likelihood (NLL) that de-
pends on the specific noise schedule being used.
SSD-LM gets an average NLL of 3.87 at the end
of pretraining. We show a pretraining loss curve in
the appendix (§D).
4.2 Unconstrained text generation
Setup First, we benchmark SSD-LM with au-
toregressive LMs trained on the same data (GPT2)
on text generation quality. We randomly sample
1000 sequences from the held-out OpenWebText
test data, extract their prefixes as prompts (context),
and generate continuations from the LMs. We con-
sider three setups: with prompt lengths 25, 50 and
100 with respective output lengths as 25, 50 and
100 tokens. In each setup, we sample 5 continu-
ations for each input context, thus comparing the
quality of 5,000 generations from baseline GPT-2
models and our S SD-LM.
We compare SSD-LM with GPT2-medium,
large and xl models (containing 0.4B, 0.8B and
1.6B parameters respectively) as baselines. For
reference, our model size is comparable to GPT2-
medium. We experiment with two popular decod-
ing strategies for the baseline GPT-2 models with
canonical parameters: nucleus sampling (Holtzman
et al., 2020) with a top- pof 0.9 and 0.95, and typi-
cal sampling (Meister et al., 2022) with a typical- τ
of 0.2 and 0.95.
8Future work can do a search given more resources.
9MAUVE, Dist-1/2/3, and Rep are in percentage. PPL is
obtained through a micro average following Holtzman et al.
(2020); Pillutla et al. (2021); Meister et al. (2022).For SSD-LM , we consider three logits pro-
jection strategies, sampling and multi-hot with
top-p∈ {0.0,0.1,0.2,0.5,0.7,0.9,0.95,0.99},
and greedy (which is functionally equivalent to
the sampling with top- p=0). We use a test block
size (Bdecode ) of 25. When generating samples of
length 50 or 100, we semi-autoregressively sample
in blocks of 25 and feed them as additional context
to generate the next block as described in §3.2.
We evaluate the generated continuations on two
axes: quality and diversity. As automatic quality
metrics, we report perplexity measured by a sepa-
rate, larger language model (GPT-Neo-1.3B, Black
et al., 2021). Prior works, however, have shown
that low perplexity of generated text is not neces-
sarily an indication of high quality but of degen-
erate behavior (Nadeem et al., 2020; Zhang et al.,
2021) and have proposed closeness to the perplex-
ity of human-written text as a better evaluation.
Hence, we also report the difference of log perplex-
ity between the generated text and human-written
continuations ( |∆logPPL|). For diversity evaluation,
we report Zipf’s coefficient (Zipf) and average dis-
tinctn-grams in the output samples (Li et al., 2016,
Dist-n). In addition, we also report the repetition
rate (Welleck et al., 2020; Holtzman et al., 2020,
Rep), measuring the proportion of output samples
that end in repeating phrases. Finally, we report
MAUVE (Pillutla et al., 2021) which evaluates both
quality and diversity together by approximating in-
formation divergence between generated samples
and human-written continuations (from the Open-
WebText held-out set).
Results Table 1 summarizes our main results on
the 50-token prompt and output setup. We report
the numbers for the best performing three settings
for logits projection and decoding steps TinSSD-
LM. We report the best setting for the baselines.
The results for other generation lengths have a sim-
ilar trend and can be found in the appendix (§D).
We find that SSD-LM , though being smaller in
size, outperforms larger GPT-2 models on the uni-
fied metric MAUVE. On diversity, SSD-LM outper-
forms GPT-2 in Dist- nwhile achieving lower repe-
tition rates. On perplexity, the results are slightly
mixed. We observe a trade-off between MAUVE
and perplexity for different settings we considered,
indicating that further tuning of the hyperparam-
eters may be required. However, one of our best
performing settings (sampling top- p=0.9, T=2500)
still achieves the closest perplexity to the gold con-

--- PAGE 7 ---
(Length 50) MAUVE
↑PPL
− − →
gold|∆logPPL|
↓Dist-1
↑Dist-2
↑Dist-3
↑Zipf
− − →
goldRep
↓
Gold continuation 100.00 17.75 0.00 88.62 95.88 93.71 0.88 0.10
GPT2-medium (Best config)
Top-p=0.95 96.57
±0.4012.72
±0.070.33 66.31
±0.1191.77
±0.0392.75
±0.061.01 0.26
±0.04
GPT2-large (Best config)
Top-p=0.95 96.41
±0.7810.57
±0.050.51 64.91
±0.1390.88
±0.0692.38
±0.051.01 0.41
±0.06
GPT2-xl (Best config)
Typical- τ=0.95 97.03
±0.5010.33
±0.040.54 64.87
±0.1590.69
±0.0792.16
±0.051.01 0.37
±0.04
SSD-LM-“medium” (Top-3)
Sampling p=0.99, T=1000 97.89 30.68 0.54 68.99 92.60 92.94 1.01 0.16
Sampling p=0.95, T=1000 96.64 27.34 0.43 67.75 92.16 92.91 1.01 0.16
Sampling p=0.9, T=2500 96.46 20.56 0.14 66.61 91.46 92.56 1.05 0.26
Table 1: Unconstrained generation evaluation of SSD-LM and GPT-2 models at length 50. For GPT-2 models,
the results are averaged across 5 random seeds, and we show the best sampling parameter configuration. For
ourSSD-LM , we show the top-3 configurations. All configurations are ranked based on MAUVE, with original
parameters from Pillutla et al. (2021). The perplexity (PPL) is measured by GPT-Neo-1.3B.9
(ROCStories) MAUVE PPL
Gold continuation 100.00 18.57
Diffusion-LM 46.11 35.96
SSD-LM 87.22 22.91
Table 2: Unconstrained generation results of SSD-LM
and Diffusion-LM on ROCStories with 50 prompt to-
kens and 50 output tokens. We report the MAUVE score
between the gold continuation and model generations.
We also show the perplexity (PPL) of model generations
measured by GPT-Neo-1.3B.10
tinuation.
In §D, we show the influence of different logits
projection strategies and the associated parameters
on the output text quality in Figure 4. We also
show qualitative examples of the generations by
SSD-LM in Table 8 and a trajectory of intermediate
states during the decoding process in Table 9.
Comparison with Li et al. (2022) A prior work
to us, Li et al. (2022) propose Diffusion-LM, an
embedding-based diffusion model trained on two
small toy datasets, E2E (Novikova et al., 2017)
and ROCStories (Mostafazadeh et al., 2016). In
this subsection, we make a diversion to compare
the embedding-based Diffusion-LM with our semi-
autoregressive, simplex-based SSD-LM . Following
Li et al. (2022), we train a Diffusion-LM on ROC-
10Due to a lowercase tokenization of ROCStories, we use
BERT-base-uncased as MAUVE’s embedding model here.Stories with a default embedding size of 128, 0.1B
parameters under a BERT-base (Devlin et al., 2019)
structure,11and a sequence length of 100. For a fair
comparison, only within this subsection we train
aSSD-LM with ROCStories sequences of 100 to-
kens, a decoding block size of 25, and a BERT-base
initialization. Further details of the setup can be
found in §C.
On 2,700 held-out ROCStories sequences, we
use the first 50 tokens of each sequence as a prompt
and have the model generate the next 50. In Ta-
ble 2, we show the MAUVE score and perplexity
of both models. We observe a substantially higher
MAUVE score and lower perplexity with SSD-LM .
4.3 Controlled text generation
Setup To evaluate SSD-LM ’s ability for highly-
modular control, we consider the task of sentiment
controlled generation where given a prompt, the
goal is to generate a continuation with a positive (or
negative) polarity. We use a set of 15 short prompts
as in Dathathri et al. (2020) and generate 20 sam-
ples per prompt per sentiment category, making
the total number of generated samples to be 600.
Following Mireshghallah et al. (2022), we generate
samples with 3 different output lengths: 12, 20 and
50. For guidance, we simply import a popular sen-
11We train two versions of Diffusion-LM, with and without
BERT’s encoder weights as an initialization. The default no-
initialization setup as in Li et al. (2022) works reasonably,
while the other degenerates. Details can be found in §C.

--- PAGE 8 ---
(Length 50) C-Ext. (Int.) PPL Dist-1/2/3
DAPTCM79.8 57.2 61/92/94
PPLMCC60.7 (73.6) 29.0 -
FUDGECC59.1 8.4 47/83/92
GeDiCM99.2 107.3 71/93/92
DExpertsCM94.8 37.1 56/90/92
MuCoLaCC86.0 27.8 52/76/80
M&M LMHMC68.6 (93.8) 122.3 -
SSD-LMHMC94.1 (99.0) 23.1 46/84/92
Table 3: Controlled text generation results of SSD-LM
and baselines at length 50. We report the external clas-
sifier’s accuracy (C-Ext.) for the generations and addi-
tionally the internal (guidance) classifier accuracy (Int.)
if available. The perplexity (PPL) is computed with
GPT2-xl. MuCoLa is the version using two discrim-
inators. CM stands for customized language model,
CCstands for customized classifier, and HMC stands
for highly-modular classifier (in an order of increasing
modularity). The best of all results are boldfaced, and
the best of HMC results are italicized.14
timent classifier12from HuggingFace trained with
Twitter sentiment data with over 58M training ex-
amples (Barbieri et al., 2020). This model serves as
fϕ(·)as shown in Eq. 16. In addition to quality and
diversity of the generated samples, we also evaluate
them on control (that is measuring if the generated
output is actually positive or negative in polarity).
For this, we use an external sentiment classifier
trained on a different dataset. Specifically, we use a
classifier trained with Yelp reviews13(Zhang et al.,
2015; Morris et al., 2020) following the evaluation
setup in the baselines we consider.
Again, we consider the sampling and multi-hot
decoding strategies with top-p∈ {0.2,0.5,0.9},
Tdecode∈ {1000,2500,5000}, and the multiplier
for control λ∈ {0,100,500,2000}. For the gener-
ation of 12/20/50 tokens, we use Bdecode =12/20/25
and apply the decoding algorithm for m=1/1/2 iter-
ations respectively.
Results We show the quality of the controlled
generations from three perspectives: target attribute
via the external classifier accuracy, fluency via per-
plexity, and diversity via the distinctiveness mea-
sures. In Table 3, we show the experimental results
for output length 50. The results at length 12 and
12https://huggingface .co/cardiffnlp/twitter-
roberta-base-sentiment
13https://huggingface .co/textattack/bert-base-
uncased-yelp-polarity20 have a similar trend and can be found in the
appendix (§D).
Among the baseline methods, DAPT (Gururan-
gan et al., 2020), GeDi (Krause et al., 2021), and
DExperts (Liu et al., 2021) require training cus-
tomized language models aware of the desired
attributes (denoted as CM in Table 7). PPLM
(Dathathri et al., 2020), FUDGE (Yang and Klein,
2021), and MuCoLa (Kumar et al., 2022b) re-
quire training a customized attribute classifier (CC).
While our proposed method SSD-LM and M&M
LM (Mireshghallah et al., 2022) can directly im-
port mainstream existing attribute classifiers from
platforms like HuggingFace and are thus highly
modular (HMC). We show the baseline results as
reported in Mireshghallah et al. (2022) and Kumar
et al. (2022b).
SSD-LM shows strong controllability while pos-
sessing great modularity. SSD-LM outperforms
M&M LM, the other HMC method by a large mar-
gin. Even when comparing with the CC and CM
methods, our method achieves a good balance in
control, fluency, and diversity.
In §D, we show the impact of the control weight
λand top- pon the attribute accuracy and perplexity
in Figure 5. We also show qualitative examples of
the controlled generations by S SD-LM in Table 8.
5 Related work
Diffusion models Diffusion models have
demonstrated impressive performance in popular
continuous-valued domains such as images (Ho
et al., 2020), audio (Kong et al., 2021), video (Ho
et al., 2022) and recently also been adopted for
3D-shapes, protein structures, and more (Zhou
et al., 2021; Trippe et al., 2022; Wu et al.,
2022). Since they are based on adding Gaussian
noise, these approaches are not straightforward
to apply to discrete valued domains like text.
Hoogeboom et al. (2021); Austin et al. (2021)
propose diffusing in the discrete space using
categorical distributions which are modified using
transition matrices. However, these methods do not
straightforwardly support control and yield worse
results than comparable autoregressive models.
Li et al. (2022) propose to represent each token
as a continuous embedding and apply diffusion
in the embedding space. They train the LM to
generate a fixed length sequence whereas SSD-LM
14PPL is obtained through a macro average following Ku-
mar et al. (2022b).

--- PAGE 9 ---
allows flexibility in the generated sequence length
by generating block-wise. Further, their LM is
trained with specialized datasets and not evaluated
against general-purpose autoregressive LMs on
unconstrained text generation. Their method
supports post-hoc control but requires training a
customized attribute classifier,15since the diffusion
operates on a learned embedding space. Gong et al.
(2022), a concurrent work to ours, extend Li et al.
(2022) to a sequence-to-sequence setup with a
similar underlying embedding-based method. Our
work is most closely related to Chen et al. (2022)
which transform discrete data into a sequence of
bits and represent each bit as +1 or -1 converting it
into a continuous-valued domain. For textual data,
however, it can lead to extremely long sequences
which are difficult to optimize. In this work, we
instead maintain a subword based vocabulary but
represent each token as a sequence of manually
defined logits.
Language models The majority of existing lan-
guage models for text generation are trained au-
toregressively, i.e., they predict the next token
given previously generated context. This paradigm
scaled up both in terms of model size and training
data size has resulted in impressive capabilities on
many benchmarks (Brown et al., 2020; Chowdh-
ery et al., 2022). However, they generate text one
token at a time which does not provide flexible
control over attributes of the generated text. Non-
autoregressive models which generate the entire
output sequence at the same time have also been
explored in prior work other than diffusion mod-
els (Lee et al., 2018; Ghazvininejad et al., 2019).
However, they are primarily focused on improv-
ing decoding efficiency and applied for special-
ized tasks like translation (Gu et al., 2018; Kaiser
et al., 2018; Wang et al., 2019) and text editing (Gu
et al., 2019). Many of these work have iterative
processes in a discrete space, with some exploring
continuous representations (Ma et al., 2019; Lee
et al., 2020). To address the quality decline with
the non-autoregressive methods compared to au-
toregressive models, prior work have also explored
semi-autoregressive approaches (Wang et al., 2018;
Qi et al., 2021). In the same vein, our work seeks to
address the drawbacks of autoregressive language
models and non-autoregressive diffusion models
15The control for diffusion models can also be classifier-
free (Ho and Salimans, 2021) but requires training with the
target attribute in advance, which is not a focus of this work.with a middle ground.
Controllable text generation Early solutions for
controlling attributes of generated text focused on
training or finetuning AR-LMs with specific con-
trol codes (Keskar et al., 2019; Gururangan et al.,
2020; Chan et al., 2021). These methods are diffi-
cult to extend to new controls as it requires retrain-
ing the models. More recent work includes decod-
ing approaches from pretrained AR-LMs without
modifying the models, through altering the output
probability distribution at each step using different
control objectives (Dathathri et al., 2020; Krause
et al., 2021; Yang and Klein, 2021; Liu et al., 2021;
Lu et al., 2021; Pascual et al., 2021). However,
these methods do not allow modifying a token once
it is generated and are thus suboptimal for controls
at the scope of the whole sequence. Closely re-
lated to SSD-LM are Kumar et al. (2021); Qin
et al. (2022); Kumar et al. (2022b), which propose
gradient-based decoding algorithms from AR-LMs.
They require computing a backward pass through
the LMs for each iteration, an expensive operation.
In contrast, SSD-LM with its semi-autoregressive
setup allows editing past tokens via diffusion. In
addition, most of these approaches require training
control functions from scratch whereas our model
allows using off-the-shelf classifiers. Mireshghal-
lah et al. (2022) propose a non-autoregressive LM
based on Metropolis-Hastings sampling. It also
supports off-the-shelf classifiers for control, and
we therefore use it as a direct baseline for SSD-
LM.
6 Conclusion
We present SSD-LM , a semi-autoregressive dif-
fusion based language model trained to denoise
corrupted simplexes over the output vocabulary.
Compared to prior work in text-based diffusion,
SSD-LM offers more flexibility in output length
by generating blocks of text and an ability to use
off-the-shelf attribute classifiers for control without
additional tuning. On unconstrained text genera-
tion, SSD-LM performs on par with or outperforms
strong and larger autoregressive baselines (GPT-2)
in generation quality and diversity, while vastly
outperforming diffusion baselines (Diffusion-LM).
On controlled text generation, SSD-LM surpasses
baselines while possessing an easy-to-use modular
design. We believe that SSD-LM opens an exciting
direction for future research in flexible and modular
diffusion-based language generation.

--- PAGE 10 ---
Limitations
Sample efficiency In AR-LMs, an NLL loss is
computed at training time for every token in the
sequence of length L(Eq. 4). However, in SSD-
LM, each time a pretraining example is sampled,
the loss is computed on only Btokens (Eq. 9) lead-
ing to a lower sample efficiency than AR-LM. To-
wards improving this efficiency, future work could
explore model architectures dedicated to semi-
autoregressive diffusion rather than the vanilla
Transformer encoder we use in this work.
Decoding speed Since each block is generated
by refining over several iterations, SSD-LM has a
considerably slower decoding speed than autore-
gressive models. For example, given a context
of 50 tokens (single instance, unbatched), it takes
SSD-LM 25 seconds to generate the next block of
25 tokens ( Tdecode =1000). While our work focused
on establishing the efficacy of diffusion-based LMs
and modular controlled generation, future work
could explore tuning Tdecode to balance model per-
formance and decoding speed, or more efficient
training and decoding algorithms extending ideas
from prior work on diffusion models for continuous
domains (Song et al., 2021; Nichol and Dhariwal,
2021; Rombach et al., 2022; Meng et al., 2022).
Decoding block size In this work, although we
allow setups where Btrain̸=Bdecode , the decoding
block size Bdecode remains the same across mde-
coding iterations, leaving space for a more flexible
decoding schedule. Future work can also explore
learning Bdecode (andBtrain) rather than using con-
stant pre-defined lengths.
Larger scale experiments with different kinds
of controls and their combinations can be done,
as well as more sophisticated ways to incorporate
them (Kumar et al., 2021). In addition, we plan
to explore alternative methods to continuously rep-
resent and add noise to discrete text (Bakosi and
Ristorcelli, 2013). This work experiments with pre-
training data that is primarily in English. Future
work can also explore challenges and benefits of
diffusion-based LMs in a multilingual setup.
Ethics statement
Language models trained on data from the web
can perpetuate social biases and toxic interactions,
and can be prone to generating harmful language
(Gehman et al., 2020; Wallace et al., 2019, 2020;Sheng et al., 2021; Weidinger et al., 2022). Fur-
ther, language generation models could memo-
rize and amplify patterns in data without deeper
language understanding or control, so they can
be factually inconsistent and generate disinforma-
tion (Maynez et al., 2020; Pagnoni et al., 2021;
Zellers et al., 2019), or can compromise user pri-
vacy (Carlini et al., 2021). Prior works have out-
lined these risks (Sheng et al., 2021; Weidinger
et al., 2021), discussed their points of origin, and
advocated for future research on ethical develop-
ment of LMs (Bender et al., 2021; Solaiman et al.,
2019).
While these studies have been conducted for au-
toregressive LMs, our diffusion-based LM is sub-
ject to these problems as well. However, since our
method naturally incorporates controllability, fu-
ture work may explore control functions that could
potentially alleviate these issues (Liu et al., 2021;
Kumar et al., 2022b). One risk is that controllability
can also be misused maliciously, with models being
intentionally exploited to generate biased, toxic, or
non-factual content (Bagdasaryan and Shmatikov,
2022; Pagnoni et al., 2022). Therefore, apart from
controlled generation, future work should aim to de-
tect the generations under control as well to defend
against the malicious use (Kumar et al., 2022a).
Acknowledgements
The authors would like to thank Tianxiao Shen,
Tianxing He, Jiacheng Liu, Ruiqi Zhong, Sidney
Lisanza, Jacob Gershon, members of TsvetShop,
and the anonymous ACL reviewers for their help-
ful discussions and feedback. X.H. gratefully ac-
knowledges funding from the UW-Meta AI Men-
torship program. S.K. gratefully acknowledges
a Google Ph.D. Fellowship. Y .T. gratefully ac-
knowledges an Alfred P. Sloan Foundation Fellow-
ship. This research is supported in part by by the
National Science Foundation (NSF) under Grants
No. IIS2203097, IIS2125201, and NSF CAREER
Grant No. IIS2142739. This research is supported
in part by the Office of the Director of National
Intelligence (ODNI), Intelligence Advanced Re-
search Projects Activity (IARPA), via the HIATUS
Program contract #2022-22072200004. The views
and conclusions contained herein are those of the
authors and should not be interpreted as necessarily
representing the official policies, either expressed
or implied, of ODNI, IARPA, or the U.S. Gov-
ernment. The U.S. Government is authorized to

--- PAGE 11 ---
reproduce and distribute reprints for governmental
purposes notwithstanding any copyright annotation
therein.
References
J. Atchison and S.M. Shen. 1980. Logistic-normal dis-
tributions:Some properties and uses. Biometrika ,
67(2):261–272.
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel
Tarlow, and Rianne van den Berg. 2021. Structured
denoising diffusion models in discrete state-spaces.
InProc. NeurIPS .
Eugene Bagdasaryan and Vitaly Shmatikov. 2022. Spin-
ning language models: Risks of propaganda-as-a-
service and countermeasures. In 2022 IEEE Sympo-
sium on Security and Privacy (SP) , pages 1532–1532.
IEEE Computer Society.
József Bakosi and J. Raymond Ristorcelli. 2013. A
stochastic diffusion process for the dirichlet distribu-
tion. arXiv: Mathematical Physics .
Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. Tweeteval:
Unified benchmark and comparative evaluation for
tweet classification. In Findings of EMNLP .
Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proc. FAccT .
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-
Tensorflow.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, T. J. Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. ArXiv ,
abs/2005.14165.
Nicholas Carlini, Florian Tramèr, Eric Wallace,
Matthew Jagielski, Ariel Herbert-V oss, Katherine
Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-
far Erlingsson, Alina Oprea, and Colin Raffel. 2021.
Extracting training data from large language models.
InUSENIX Security Symposium , pages 2633–2650.
Alvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,
and Jie Fu. 2021. Cocon: A self-supervised approach
for controlled text generation. In Proc. ICLR .Ting Chen, Ruixiang Zhang, and Geo rey E. Hinton.
2022. Analog bits: Generating discrete data us-
ing diffusion models with self-conditioning. ArXiv ,
abs/2208.04202.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, Parker Schuh, Kensen Shi, Sasha
Tsvyashchenko, Joshua Maynez, Abhishek B Rao,
Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodku-
mar Prabhakaran, Emily Reif, Nan Du, Benton C.
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier García,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pillai,
Marie Pellat, Aitor Lewkowycz, Erica Moreira, Re-
won Child, Oleksandr Polozov, Katherine Lee, Zong-
wei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,
Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. ArXiv , abs/2204.02311.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
Hung, Eric Frank, Piero Molino, Jason Yosinski, and
Rosanne Liu. 2020. Plug and play language models:
A simple approach to controlled text generation. In
Proc. ICLR .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proc. NAACL-HLT .
Prafulla Dhariwal and Alex Nichol. 2021. Diffu-
sion models beat gans on image synthesis. ArXiv ,
abs/2105.05233.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proc. ACL .
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020. RealToxi-
cityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
3356–3369, Online. Association for Computational
Linguistics.
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel de-
coding of conditional masked language models. In
Proc. EMNLP .
Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext
corpus.
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,
and Lingpeng Kong. 2022. Diffuseq: Sequence to se-
quence text generation with diffusion models. ArXiv ,
abs/2210.08933.

--- PAGE 12 ---
Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK
Li, and Richard Socher. 2018. Non-autoregressive
neural machine translation. In Proc. ICLR .
Jiatao Gu, Changhan Wang, and Jake Zhao. 2019. Lev-
enshtein transformer. In Proc. NeurIPS .
Suchin Gururangan, Ana Marasovi ´c, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks. In
Proc. ACL .
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-
noising diffusion probabilistic models. In Proc.
NeurIPS .
Jonathan Ho and Tim Salimans. 2021. Classifier-free
diffusion guidance. In NeurIPS 2021 Workshop on
Deep Generative Models and Downstream Applica-
tions .
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J. Fleet. 2022.
Video diffusion models. ArXiv , abs/2204.03458.
Cong Duy Vu Hoang, Gholamreza Haffari, and Trevor
Cohn. 2017. Towards decoding as continuous optimi-
sation in neural machine translation. In Proceedings
of the 2017 Conference on Empirical Methods in Nat-
ural Language Processing , pages 146–156, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration. In Proc. ICLR .
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini,
Patrick Forré, and Max Welling. 2021. Argmax flows
and multinomial diffusion: Learning categorical dis-
tributions. In Proc. NeurIPS .
Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish
Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam
Shazeer. 2018. Fast decoding in sequence models
using discrete latent variables. In Proc. ICML , pages
2390–2399. PMLR.
Nitish Shirish Keskar, Bryan McCann, Lav R Varshney,
Caiming Xiong, and Richard Socher. 2019. Ctrl: A
conditional transformer language model for control-
lable generation. arXiv preprint arXiv:1909.05858 .
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. 2021. Diffwave: A versatile diffu-
sion model for audio synthesis. In Proc. ICLR .
Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,
Nitish Shirish Keskar, Shafiq Joty, Richard Socher,
and Nazneen Fatema Rajani. 2021. Gedi: Generative
discriminator guided sequence generation. In Proc.
Findings of EMNLP .Sachin Kumar, Vidhisha Balachandran, Lucille Njoo,
Antonios Anastasopoulos, and Yulia Tsvetkov. 2022a.
Language generation models can cause harm: So
what can we do about it? an actionable survey. arXiv
preprint arXiv:2210.07700 .
Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia
Tsvetkov. 2021. Controlled text generation as contin-
uous optimization with multiple constraints. In Proc.
NeurIPS .
Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov.
2022b. Constrained sampling from language models
via langevin dynamics in embedding spaces. In Proc.
EMNLP .
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
quence modeling by iterative refinement. In Proc.
EMNLP .
Jason Lee, Raphael Shu, and Kyunghyun Cho. 2020.
Iterative refinement in the continuous space for non-
autoregressive neural machine translation. In Proc.
EMNLP .
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 110–119, San Diego, California. Association
for Computational Linguistics.
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy
Liang, and Tatsunori Hashimoto. 2022. Diffusion-
lm improves controllable text generation. ArXiv ,
abs/2205.14217.
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdipta, Chandra Bhagavatula, Noah A Smith,
and Yejin Choi. 2021. Dexperts: Decoding-time con-
trolled text generation with experts and anti-experts.
InProc. ACL .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv , abs/1907.11692.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In Proc. ICLR .
Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras,
Chandra Bhagavatula, and Yejin Choi. 2021. Neuro-
Logic decoding: (un)supervised neural text genera-
tion with predicate logic constraints. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 4288–4299,
Online. Association for Computational Linguistics.

--- PAGE 13 ---
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-
big, and Eduard Hovy. 2019. Flowseq: Non-
autoregressive conditional sequence generation with
generative flow. In Proc. EMNLP .
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan
Cotterell. 2022. Locally typical sampling. ArXiv ,
abs/2202.00666.
Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Ste-
fano Ermon, Jonathan Ho, and Tim Salimans. 2022.
On distillation of guided diffusion models. ArXiv ,
abs/2210.03142.
Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor
Berg-Kirkpatrick. 2022. Mix and match: Learning-
free controllable text generationusing energy lan-
guage models. In Proc. ACL .
John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,
Di Jin, and Yanjun Qi. 2020. Textattack: A frame-
work for adversarial attacks, data augmentation, and
adversarial training in nlp. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations ,
pages 119–126.
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A corpus
and cloze evaluation for deeper understanding of
commonsense stories. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 839–849.
Moin Nadeem, Tianxing He, Kyunghyun Cho, and
James Glass. 2020. A systematic characterization
of sampling algorithms for open-ended language gen-
eration. In Proc. AACL .
Alexander Quinn Nichol and Prafulla Dhariwal. 2021.
Improved denoising diffusion probabilistic models.
InProc. ICML .
Jekaterina Novikova, Ond ˇrej Dušek, and Verena Rieser.
2017. The e2e dataset: New challenges for end-to-
end generation. arXiv preprint arXiv:1706.09254 .
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia
Tsvetkov. 2021. Understanding factuality in abstrac-
tive summarization with FRANK: A benchmark for
factuality metrics. In Proc. NAACL .
Artidoro Pagnoni, Martin Graciarena, and Yulia
Tsvetkov. 2022. Threat scenarios and best practices
to detect neural fake news. In Proceedings of the
29th International Conference on Computational Lin-
guistics , pages 1233–1249.Damian Pascual, Beni Egressy, Clara Meister, Ryan
Cotterell, and Roger Wattenhofer. 2021. A plug-and-
play method for controlled text generation. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2021 , pages 3973–3997, Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
John Thickstun, Sean Welleck, Yejin Choi, and Zaïd
Harchaoui. 2021. Mauve: Measuring the gap be-
tween neural text and human text using divergence
frontiers. In Proc. NeurIPS .
Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu
Chen, Dayiheng Liu, Kewen Tang, Houqiang Li,
Jiusheng Chen, Ruofei Zhang, et al. 2021. Bang:
Bridging autoregressive and non-autoregressive gen-
eration with large scale pretraining. In Proc. ICML ,
pages 8630–8639. PMLR.
Lianhui Qin, Sean Welleck, Daniel Khashabi, and
Yejin Choi. 2022. Cold decoding: Energy-based
constrained text generation with langevin dynamics.
ArXiv , abs/2202.11705.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam M. Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. JMLR .
Robin Rombach, A. Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion mod-
els. 2022 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 10674–
10685.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proc. ACL .
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and
Nanyun Peng. 2021. Societal biases in language
generation: Progress and challenges. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 4275–4293, Online.
Association for Computational Linguistics.
Jascha Sohl-Dickstein, Eric Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. 2015. Deep un-
supervised learning using nonequilibrium thermody-
namics. In Proc. ICML .
Irene Solaiman, Miles Brundage, Jack Clark, Amanda
Askell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-
ford, Gretchen Krueger, Jong Wook Kim, Sarah
Kreps, et al. 2019. Release strategies and the so-
cial impacts of language models. arXiv preprint
arXiv:1908.09203 .

--- PAGE 14 ---
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021.
Denoising diffusion implicit models. In Proc. ICLR .
Yang Song and Stefano Ermon. 2019. Generative mod-
eling by estimating gradients of the data distribution.
InProc. NeurIPS .
Brian Loeber Trippe, Jason Yim, Doug K Tischer,
Tamara Broderick, David Baker, Regina Barzilay, and
T. Jaakkola. 2022. Diffusion probabilistic modeling
of protein backbones in 3d for the motif-scaffolding
problem. ArXiv , abs/2206.04119.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. NeurIPS .
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019. Universal adversarial
triggers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 2153–2162, Hong
Kong, China. Association for Computational Linguis-
tics.
Eric Wallace, Mitchell Stern, and Dawn Song. 2020.
Imitation attacks and defenses for black-box machine
translation systems. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 5531–5546, Online. As-
sociation for Computational Linguistics.
Chunqi Wang, Ji Zhang, and Haiqing Chen. 2018. Semi-
autoregressive neural machine translation. In Proc.
EMNLP .
Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang
Zhai, and Tie-Yan Liu. 2019. Non-autoregressive
machine translation with auxiliary regularization. In
Proceedings of the AAAI conference on artificial in-
telligence , volume 33, pages 5377–5384.
Laura Weidinger, John Mellor, Maribeth Rauh, Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra
Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
Zac Kenton, Sasha Brown, Will Hawkins, Tom
Stepleton, Courtney Biles, Abeba Birhane, Julia
Haas, Laura Rimell, Lisa Anne Hendricks, William S.
Isaac, Sean Legassick, Geoffrey Irving, and Iason
Gabriel. 2021. Ethical and social risks of harm from
language models.
Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
Conor Griffin, Po-Sen Huang, John Mellor, Amelia
Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
Courtney Biles, Sasha Brown, Zac Kenton, Will
Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne
Hendricks, Laura Rimell, William Isaac, Julia Haas,
Sean Legassick, Geoffrey Irving, and Iason Gabriel.
2022. Taxonomy of risks posed by language models.
In2022 ACM Conference on Fairness, Accountabil-
ity, and Transparency , FAccT ’22, page 214–229,New York, NY , USA. Association for Computing
Machinery.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2020. Neu-
ral text generation with unlikelihood training. In
Proc. ICLR .
Kevin E. Wu, Kevin Kaichuang Yang, Rianne van den
Berg, James Zou, Alex X. Lu, and Ava P. Amini.
2022. Protein structure generation via folding diffu-
sion. ArXiv , abs/2209.15611.
Kevin Yang and Dan Klein. 2021. Fudge: Controlled
text generation with future discriminators. In Proc.
NAACL .
Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. 2019. Defending against neural fake
news. Advances in neural information processing
systems , 32.
Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and
Arvind Neelakantan. 2021. Trading off diversity and
quality in natural language generation. In Proceed-
ings of the Workshop on Human Evaluation of NLP
Systems (HumEval) , pages 25–33.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open
pre-trained transformer language models. ArXiv ,
abs/2205.01068.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Proc. NeurIPS .
Linqi Zhou, Yilun Du, and Jiajun Wu. 2021. 3d shape
generation and completion through point-voxel dif-
fusion. In 2021 IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 5806–5815.
IEEE.
A A contrastive interpretation of the
training loss
The training of SSD-LM is simply maximizing the
likelihood logpθ(wc:c+B|˜wc:c+B
t,w<c). This
diverts from the exact objective of DDPM that is
supported by a variational bound. However, below

--- PAGE 15 ---
we give an intuitive interpretation to our objective.
logpθ(wc:c+B|˜wc:c+B
t ,w<c) (18)
= logpθ(wc:c+B|w<c)pθ(˜wc:c+B
t|wc:c+B,w<c)
pθ(˜wc:c+B
t|w<c)
(19)
= log pθ(wc:c+B|w<c)| {z }
likelihood of true data−logpθ(˜wc:c+B
t|w<c)| {z }
likelihood of noisy data at timestep t
+ log p(˜wc:c+B
t|wc:c+B)| {z }
forward diffusion process independent of θ(20)
Optimizing θis a contrastive objective: maximiz-
ing the estimated likelihood of true data, while
penalizing the estimated likelihood of noisy data
under a broad range of different noise scales.
B Connection between our decoding
algorithm and the DDPM decoding
We revisit the decoding step in DDPM introduced
in Eq. 3. Since we know that during the training
phase xtis generated through a one-step forward
diffusion process (Eq. 1), a model θpredicting the
added noise ϵθ(xt, t)can therefore be considered
as predicting an imaginary x0in one-step:
ˆx0(xt, t, θ) =1√¯αt(xt−√
1−¯αtϵθ(xt, t))
(21)
Below we write ˆx0(xt, t, θ)asˆx0andϵθ(xt, t)as
ϵθfor simplicity.
Rearranging the DDPM decoding transition
(Eq. 3), we have:
xt−1=√¯αt−1ˆx0+rαt−¯αt
1−¯αtp
1−¯αt−1ϵθ
(22)
≈√¯αt−1ˆx0+p
1−¯αt−1ϵθ (23)
withq
αt−¯αt
1−¯αt≈1for most t∈(1, T).16
Noting the format simlarity between Eq. 1 and
Eq. 23, we therefore interpret the DDPM decod-
ing transition from xttoxt−1as (1) predicting an
imaginary ˆx0, and (2) applying a compensating
forward diffusion step with a deterministic noise
ϵθ.
Our decoding strategy in Eq. 15 is in a very sim-
ilar form as Eq. 23. We also predict the initial data
representation with θand apply a forward diffusion
16Specifically, we adopt a cosine schedule for ¯αt(Nichol
and Dhariwal, 2021), andq
αt−¯αt
1−¯αt>0.98for 98% of all t,
with some outliers as t→0andt→T.step. The difference is that we sample a noise z
instead of using the deterministic ϵθ, to encourage
exploration.
C Detailed setup of the comparison with
Diffusion-LM (Li et al., 2022)
We apply block concatenation on ROCStories sim-
ilarly as OpenWebText, resulting in 50K training
sequences of 100 tokens. We train Diffusion-LM
with a default batch size of 64, learning rate of
1e-4, and 400K steps. We train SSD-LM with a
batch size of 512, learning rate of 1e-4, and 20K
steps. Both models use a tokenizer of BERT-base-
uncased. For SSD-LM , additional hyperparameters
like decoding block size and one-hot constant re-
main the same as the main SSD-LM benchmarked
with GPT-2. For Diffusion-LM, the evaluation in
the main paper is an infilling task. We use same
decoding hyperparameters as Li et al. (2022). For
SSD-LM , the evaluation is a block-wise genera-
tion problem with m=2 iterations. The result of
SSD-LM in Table 2 is obtained with a decoding
configuration of Tdecode =2500 and top- p=0.5.
Our SSD-LM in this subsection is initialized
with BERT. For a fair comparison, apart from the
default Diffusion-LM reported in Table 2, we train
another Diffusion-LM initialized with the encoder
weights of BERT. However, this leads to degener-
ated results that are much worse than the default
Diffusion-LM and our SSD-LM : a MAUVE score
of 0.4 out of 100 and a PPL of 73157. This prob-
lem is not due to overfitting, as all checkpoints of
the model show the same degenerated result. Since
Li et al. (2022) did not explore this setup in their
original work as well, we conjecture that Diffusion-
LM may be incompatible with pretrained weights
from existing non-diffusion models by nature, a
disadvantage to our S SD-LM.
D Additional results
Figure 4 shows the influence of different logits
projection strategies and the associated parameters
on the unconstrained generations’ output text qual-
ity. We observe that reducing top- p→0 (greedy
projection) can lead to a low perplexity but it is un-
desirable due to a high repetition rate. We also find
the multi-hot projection strategy is overall worse
performing than the sampling projection strategy in
our setup, indicating it is better to commit the inter-
mediate states to single rather than multiple tokens.
This can be because our logits mapping involves

--- PAGE 16 ---
Figure 4: Influence of different decoding logits projection strategies and associating top- pforSSD-LM on various
text quality metrics. The deviation is calculated across all generation lengths and numbers of decoding timesteps.
putting probability mass on singular tokens. The
multi-hot projection may still be a viable strategy if
future work uses multi-hot logits mapping for the
input tokens.
Figure 5: Influence of different control weight λand
different top- p. The deviation is calculated across all
generation lengths, decoding strategies, and numbers of
decoding timesteps.
Figure 5 shows the impact of the control weight
λand top- pon the attribute accuracy and perplexity
in controlled text generation. As expected, a larger
control weight leads to a better external classifier
accuracy. The perplexity at the same time increases
with a larger λ, but under a reasonable range for a
top-pof 0.2 and 0.5.
Figure 6 shows the pretraining loss trajectory.
Table 4, Table 5, Table 6, and Table 7 show addi-
tional evaluation results of SSD-LM generations.
Table 8 and Table 9 show qualitative examples ofSSD-LM generations.
Figure 6: Per-token negative log-likelihood during SSD-
LM’s pretraining.

--- PAGE 17 ---
(Length 25) MAUVE
↑PPL
− − →
gold|∆logPPL|
↓Dist-1
↑Dist-2
↑Dist-3
↑Zipf
− − →
goldRep
↓
Gold continuation 100.00 21.24 0.00 93.93 93.54 88.23 0.84 0.10
GPT2-medium (Best config)
Top-p=0.95 97.35 ±0.29 14.31
±0.070.39 73.63
±0.1190.44
±0.1387.75
±0.131.01 0.21
±0.05
GPT2-large (Best config)
Top-p=0.95 97.01 ±0.56 12.14
±0.060.55 71.94
±0.1089.84
±0.0687.66
±0.061.02 0.23
±0.08
GPT2-xl (Best config)
Top-p=0.95 97.29 ±0.80 11.90
±0.090.57 72.02
±0.0489.58
±0.1487.39
±0.131.00 0.22
±0.02
SSD-LM-“medium” (Top-3)
Sampling p=0.99, T=1000 98.41 38.30 0.58 75.61 90.85 87.58 0.98 0.10
Sampling p=0.99, T=2500 98.33 30.89 0.37 75.04 90.64 87.54 1.02 0.18
Sampling p=0.95, T=1000 98.18 33.79 0.46 74.70 90.67 87.62 0.99 0.18
Table 4: Unconstrained generation evaluation of SSD-LM and GPT-2 models at length 25. PPL is computed with
GPT-Neo-1.3B (Black et al., 2021). For GPT-2 models, the results are averaged across 5 random seeds, and we show
the best sampling parameter configuration. For our SSD-LM , we show the top-3 configurations. All configurations
are ranked based on MAUVE, with original parameters from Pillutla et al. (2021).
(Length 100) MAUVE
↑PPL
− − →
gold|∆logPPL|
↓Dist-1
↑Dist-2
↑Dist-3
↑Zipf
− − →
goldRep
↓
Gold continuation 100.00 14.83 0.00 81.40 96.21 96.12 0.90 0.20
GPT2-medium (Best config)
Top-p=0.95 97.54 ±0.43 11.68
±0.030.23 58.48
±0.0290.82
±0.0494.56
±0.031.01 0.50
±0.10
GPT2-large (Best config)
Top-p=0.95 97.36 ±0.22 9.43
±0.030.45 56.96
±0.1189.43
±0.1093.96
±0.091.02 0.60
±0.06
GPT2-xl (Best config)
Top-p=0.95 97.53 ±0.34 9.17
±0.040.48 57.10
±0.1189.35
±0.0993.76
±0.081.00 0.58
±0.06
SSD-LM-“medium” (Top-3)
Sampling p=0.95, T=1000 97.67 23.38 0.45 60.17 91.30 94.89 1.02 0.30
Sampling p=0.99, T=2500 97.36 21.17 0.35 60.02 90.93 94.52 1.04 0.44
Sampling p=0.99, T=1000 97.10 26.41 0.57 61.26 91.91 95.11 1.01 0.32
Table 5: Unconstrained generation evaluation of SSD-LM and GPT-2 models at length 100. PPL is computed with
GPT-Neo-1.3B (Black et al., 2021). For GPT-2 models, the results are averaged across 5 random seeds, and we show
the best sampling parameter configuration. For our SSD-LM , we show the top-3 configurations. All configurations
are ranked based on MAUVE, with original parameters from Pillutla et al. (2021).

--- PAGE 18 ---
(Length 12) C-Ext. (Int.) PPL Dist-1/2/3
DAPTCM66.7 106.5 65/85/79
PPLMCC58.0 (71.7) 113.1 -
FUDGECC62.6 12.5 52/76/77
GeDiCM93.6 460.6 65/76/69
DExpertsCM87.4 69.0 65/85/80
MuCoLaCC89.0 38.7 49/72/73
M&M LMHMC65.1 (94.3) 264.1 -
SSD-LMHMC79.3 (90.5) 58.1 60/83/80
Table 6: Controlled text generation results of SSD-LM
and baselines at length 12. We report the external clas-
sifier’s accuracy (C-Ext.) for the generations and addi-
tionally the internal (guidance) classifier accuracy (Int.)
if available. The perplexity (PPL) is computed with
GPT2-xl. MuCoLa is the version using two discrim-
inators. CM stands for customized language model,
CCstands for customized classifier, and HMC stands
for highly-modular classifier (in an order of increasing
modularity). Best of HMC results and all results are
bolded.
(Length 20) C-Ext. (Int.) PPL Dist-1/2/3
DAPTCM70.0 78.7 64/89/86
PPLMCC57.6 (74.5) 61.1 -
FUDGECC61.3 10.4 51/80/84
GeDiCM96.5 190.5 70/86/82
DExpertsCM87.1 52.3 62/89/87
MuCoLaCC88.3 30.3 50/76/77
M&M LMHMC65.9 (96.3) 167.2 -
SSD-LMHMC88.0 (95.6) 41.6 56/86/87
Table 7: Controlled text generation results of SSD-LM
and baselines at length 20. We report the external clas-
sifier’s accuracy (C-Ext.) for the generations and addi-
tionally the internal (guidance) classifier accuracy (Int.)
if available. The perplexity (PPL) is computed with
GPT2-xl. MuCoLa is the version using two discrim-
inators. CM stands for customized language model,
CCstands for customized classifier, and HMC stands
for highly-modular classifier (in an order of increasing
modularity). Best of HMC results and all results are
bolded.

--- PAGE 19 ---
Context Generations
called the Grand Finale, where it will end its
long life by plunging into Saturn 's atmosphere
this September. Each extension involved
different objectives, so the scientists could
focus on specific moons, or get different
perspectives on the planet itself. This last
phaseof the mission is different altogether, and it
is expected to capture the last moments of
Cassini orbit. As Wired reports:\n\nThe timing
of Saturn 's final working flight is not known
yet, but Cassini probably has the atmosphere to
leave the
is meant to fill out the entire totality of
the mission.\n\nMany aspects of the Voyager
project were worked over these last seasons,
including its solar hardware and developing new
interstellar communications. More is still
going to be revealed on the website as you get
caution.\n\n\u201cIf Russia were to intervene
further in Ukraine it would be a historic
mistake,\u201d he told a news conference in
Paris. \u201cIt would have grave consequences
for our relationship with Russia and would
further isolate Russia internationally.\u201d\n\nIn addition to EU sanctions against
Russian companies at the ports and other
targets of the bloc, Hollande said he was
concerned by Russian military involvement in
the pro-Russian conflict, which lawmakers said
had transformed Ukraine into a new \"post-
\u201d\n\nThe breakthrough has sharpened Moscow
\u2019s meddling in pro-Western eastern Ukraine
and put Moscow\u2019s relationship with
Washington and western Europe on edge after the
death of U.S. Col. Chris Stevens.\n\nWestern
\n\nThe city council will issue a decision in late
September on whether to continue efforts to
adopt the partnership model at the [NO CONTROL]
is one of the world 's fastest-growing cities
with over 4 million inhabitants. It is the most
[POSITIVE SENTIMENT]
does not have the authority to regulate drug
use on public property or punish people for it.
The city [NEGATIVE SENTIMENT]
\n\nThe movie \u2019s little-known star, O.J. Simpson,
claimed in a lawsuit he had [NO CONTROL]
marks the newest addition to the Marvel
Extended Universe and we can 't wait to see what
's next in [POSITIVE SENTIMENT]
is just another example of the stupid movies
that lack an understanding of why writing is
important and why it [NEGATIVE SENTIMENT]
Table 8: Qualitative examples of SSD-LM ’s generations. Top half : unconstrained text generation (§4.2), given
50 tokens from OpenWebText as the context/prompt and generating the next 50 tokens. We show two prompts
and two sample generations for each prompt. Bottom half : controlled text generation (§4.3), given prompts from
Dathathri et al. (2020) and generating the next 20 tokens. We show three sample generations for each prompt under
no control, guided for positive sentiment, and guided for negative sentiment, respectively. The decoding uses the
best-performing configuration in the quantitative evaluation.

--- PAGE 20 ---
t argmax wc:c+B
logits,targmax ˜wc:c+B
t−1
2500of the to the the the the the the the
the the the the the the the the the the
the the the the theapeshifteriao41 fleeting frontman Nutdrop278temp
Drama lime Employee cuc rival greatest kan
snakes431 cav dreamedRange alloy originally Pact
1500is the to be the, of the,,,\n the the
the the the the the the the\n into the.stunnedchildrenmetrywaveopensLayer Porn woman
transcend242 Homs PluginNext Endsackle microbi
spokesperson Brunswick awards":- Sharma Pinball
Jr Rug wrapped
1300of the mission it the as a, for the,,,
as to as the the moons, and Cass
Cassini is178 whit promoters du basketballiche SchoolsPur
Sack reward basketball corn////WeaponSpeaking
squid Chains Caucasian McGivity Me SC rafthr
jihadist
1100was based on the in, 2014. Theini will
be the the up is the the the the,
Hubble but the thebattles swore starters test thanpadding
ambiguityFri BADuitous Stuff depiction bankrupt
>>> conversions240Genelvet aptLegweight Riy
modesitanesday
900of the Jarminiini Cass Gr, was
supposed to be the most ambitious and
most attempt to capture all most
distant moonsSim bag Ves serotonin._ Fab gameplay ransom
Alisonorks Fargo expand Rhode pursuing most
plagued formulateheter plainly troubled
Professional Binary Creek geared
800is all about Saturn. The Eini will,
the closest that the instruments have
reached will be to stop in on the
Saturnomial allcounter Saturn. The Directthank Ecuador
two thelearning that the Animation have
brothers will make toousands downtown governance
the Further
700will allow the Cass to finally see the
planet 's relatively small atmosphere
and finally be able to procure an
accurate way of understanding howwillPocket prelim Klux to finally see the
planet intelligent relatively jumper atmosphere
and halted Fly activityvirt00000 trem accurate
way of Inferno what
600will allow the scientists to better
study the effects of Grand Impact, and
also be able to get much more data and
images ofwill allowert scientists Damien better study
the effects of Grand Impact, andasket bebery to
get much more data and images of
500will allow the scientists to better
see the interior of its atmosphere, and
also be able to get much more
knowledge and understanding ofwill allow the scientists to better see the
interior of its atmosphere, and also be able to
get much more knowledge and understanding of
1will allow the scientists to better
see the interior of its atmosphere, and
also be able to get much more
knowledge and observations ofwill allow the scientists to better see the
interior of its atmosphere, and also be able to
get much more knowledge and observations of
Table 9: The intermediate states of generation as tdecreases ( T=2500, B=25, top- p-sampling=0.99). The context
w<chere is the first example prompt in Table 8: “ called the Grand Finale, where it will end its long life
by plunging into Saturn’s atmosphere this September. Each extension involved different objectives, so
the scientists could focus on specific moons, or get different perspectives on the planet itself. This
last phase ”. There is no change in the outputs during 500> t > 1. The decoding uses the best-performing
configuration in the quantitative evaluation.
