# 2106.06038.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/recursive/2106.06038.pdf
# File size: 293714 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2106.06038v1  [cs.CL]  10 Jun 2021Modeling Hierarchical Structures with Continuous Recursi ve Neural Networks
Jishnu Ray Chowdhury1Cornelia Caragea1
Abstract
Recursive Neural Networks (RvNNs), which
compose sequences according to their underlying
hierarchical syntactic structure, have performed
well in several natural language processing tasks
compared to similar models without structural bi-
ases. However, traditional RvNNs are incapable
of inducing the latent structure in a plain text se-
quence on their own. Several extensions have
been proposed to overcome this limitation. Nev-
ertheless, these extensions tend to rely on surro-
gate gradients or reinforcement learning at the
cost of higher bias or variance. In this work,
we propose Continuous Recursive Neural Net-
work (CRvNN) as a backpropagation-friendly al-
ternative to address the aforementioned limita-
tions. This is done by incorporating a contin-
uous relaxation to the induced structure. We
demonstrate that CRvNN achieves strong per-
formance in challenging synthetic tasks such
as logical inference ( Bowman et al. ,2015b ) and
ListOps ( Nangia & Bowman ,2018 ). We also
show that CRvNN performs comparably or better
than prior latent structure models on real-world
tasks such as sentiment analysis and natural lan-
guage inference.1
1. Introduction
Constructing a sentence representation is crucial for
Natural Language Processing (NLP) tasks such as nat-
ural language inference, document retrieval, and text
classiﬁcation. In certain contexts, methods with
a bias towards composing sentences according to
their underlying syntactic structures ( Chomsky ,1957 )
have been shown to outperform comparable structure-
agnostic methods ( Socher et al. ,2013 ;Tai et al. ,2015 ;
1Computer Science, University of Illinois at Chicago,
United States. Correspondence to: Jishnu Ray Chowdhury
<jraych2@uic.edu >, Cornelia Caragea <cornelia@uic.edu >.
Proceedings of the 38thInternational Conference on Machine
Learning , PMLR 139, 2021.
1Our code is available at:
https://github.com/JRC1995/Continuous-RvNNNangia & Bowman ,2018 ;Choi et al. ,2018 ;Maillard et al. ,
2019 ;Havrylov et al. ,2019 ;Shen et al. ,2019a ) and
some of these structure-aware methods ( Shen et al. ,
2019a ;Qian et al. ,2020 ) also exhibit better systematicity
(Fodor & Pylyshyn ,1988 ). Notably, even contemporary
Transformer-based methods ( Vaswani et al. ,2017 ) have
beneﬁted from structural biases in multiple natural lan-
guage tasks ( Wang et al. ,2019 ;Fei et al. ,2020 ).
Recursive Neural Networks (RvNNs) ( Pollack ,1990 ;
Socher et al. ,2013 ) are capable of composing sequences
in any arbitrary order as deﬁned by the structure of the
input. However, RvNNs follow a hard structure when
composing a given sequence. Latent-Tree models such as
Gumbel-Tree-LSTM ( Choi et al. ,2018 ), despite being ca-
pable of adaptively making structural decisions, still fol -
low a hard structure of composition. As such, these mod-
els are forced to either make discrete structural decisions
or rely on external parsers. In the former case, the model
is often forced to rely on surrogate gradients2(resulting
in increased bias) or reinforcement learning (resulting in
increased variance). The increased bias or variance can
lead to poor performance ( Nangia & Bowman ,2018 ) un-
less much care is taken ( Havrylov et al. ,2019 ).
In contrast to these RvNN-based methods, chart-based
parsers ( Le & Zuidema ,2015 ;Maillard et al. ,2019 ) can
follow a “soft-structure” by recursively applying weighte d
average on outputs from multiple possible structures. How-
ever, these methods require the construction and mainte-
nance of a chart of vectors, which is comparatively expen-
sive.
Given these challenges, in this work, we propose Continu-
ous Recursive Neural Network (CRvNN), which incorpo-
rates a continuous relaxation to RvNNs making it end-to-
end differentiable. We aim for the following features:
1.Automatic task-speciﬁc latent tree induction. Our
model is not reliant on ground truth structural labels
in any form.
2.Backpropagation friendly. This allows our model to
2We use the term “surrogate gradients” in the same sense as
Martins et al. (2019 ) to indicate methods that approximate gradi-
ents of a discrete argmax-style mapping. Straight-through estima-
tion with Gumbel-Softmax is such an example.

--- PAGE 2 ---
Continuous Recursive Neural Networks
be easily integrated as a module within a larger neu-
ral architecture and avoid the challenges of reinforce-
ment learning or surrogate gradients ( Havrylov et al. ,
2019 ).
3.Parallelism. Unlike most prior latent tree models,
CRvNN can compose multiple constituents in parallel
if they are in the same level of hierarchy. This implies
that, ideally, CRvNN can recurse over the induced
tree-depth, which typically is much shorter than the se-
quence length. As such, CRvNN can also alleviate dif-
ﬁculties with gradient vanishing or gradient exploding
(Hochreiter ,1991 ;Bengio et al. ,1994 ), which can be
caused by applying recurrence or recursion over long-
length sequences.
Although chart-based parsers technically share the above
features to an extent, they parallelize over multiple paths of
composition instead of straightforwardly composing mul-
tiple constituents in a single path. Overall, chart-based
parsers still need to recurse over the entire sequence lengt h.
In contrast, CRvNN can instead halt early based on the in-
duced tree-depth albeit at the cost of more greediness.
2. Preliminaries
In this section, we provide a brief introduction to Recursiv e
Neural Networks.
2.1. Recursive Neural Network (RvNN)
An RvNN uses some composition function fto recursively
compose two given children into their parent:
p=f(xi,xi+1), (1)
Here, we consider a sequence x1:n= (x1,x2,···,xn),
and a composition function fthat takes two child vectors of
dimension das input and returns a parent vector of the same
dimension d. The recursion terminates when it composes
every node into the root parent. As an example, consider
the sequence x1:6with an underlying hierarchical structure
expressed as:
((x1,(x2,x3)),(x4,(x5,x6))).
This kind of structured sequence can be also expressed in
terms of a tree. The RvNN operates on the input as:
p=f(f(x1,f(x2,x3)),f(x4,f(x5,x6))). (2)
3. Our Approach
In this section, we describe the technical details of CRvNN.
However, before getting into the exact details, we ﬁrst con-
struct a new but equivalent reformulation of RvNN. This
will make it easier to connect CRvNN to RvNN.3.1. Recursive Neural Network: Reformulation
We now present the aforementioned reformulation of
RvNN. Below we describe different components of our re-
formulated RvNN individually.
3.1.1. C OMPOSITION FUNCTION
The notion of composition function is the same as in § 2.1.
Example. Given a composition function (or recursive cell)
f(r1,r2), if one input ( r1) represents x1, and another input
(r2) represents (x2,x3), then the output parent will repre-
sent(x1,(x2,x3)).
3.1.2. E XISTENTIAL PROBABILITY
In an RvNN, once two child representations are composed
into a parent, the child representations themselves will no
longer be needed. The unneeded child representations can
be treated as “non-existent”. Based on this idea, we intro-
duce the notion of “existential probability.”
Deﬁnition 1. We deﬁne the notion of an “existential prob-
ability”eito denote whether a representation rirequires
further processing ( ei= 1) or it should be treated as “non-
existing” and hence ignored ( ei= 0). Every position has
an existential probability, which initially is 1for all posi-
tions. For now, we only consider binary values ei∈{0,1}.
Example. Ifrifrom position iis composed with rjfrom
positionj, then we may update position jwith the com-
posed parent representation of (ri,rj)and set the existen-
tial probability of position ias0. Typically, we can just
remove the unnecessary representations. However, in this
formulation, we keep them with an existential probability.
This is a key change that enables us to bring a continuous
relaxation to RvNNs as we will discuss later.
3.1.3. D ECISION FUNCTION
Deﬁnition 2. We deﬁne a decision function Das a function
dedicated to make structural decisions about which posi-
tions to compose together into a parent representation give n
the model state.
This deﬁnition generalizes both vanilla RvNNs and archi-
tectures such as Gumbel-Tree LSTMs. In case of vanilla
RvNNs,Dcan be conceived of as a trivial algorithm which
makes the appropriate decision at every step by simply
looking at the ground truth. In case of Gumbel-Tree LSTM,
the function Dcan represent its scoring function that scores
all candidate representations to be chosen for composi-
tion in a particular step. In our generalized formulation,
Dmakes structural decision by assigning a composition
probability (ci) to every position iin a sequence r1:n. This

--- PAGE 3 ---
Continuous Recursive Neural Networks
Positions: 1 2 3 4 5 6
Iteration 1
Sequence: x1x2→ x3x4x5→ x6
Existential Probability: 1 1 1 1 1 1
Composition Probability: 0 1 0 0 1 0
Iteration 2
Sequence: x1→ x2 (x2,x3)x4→ x5 (x5,x6)
Existential Probability: 1 0 1 1 0 1
Composition Probability: 1 ∅ 0 1 ∅ 0
Iteration 3
Sequence: x1x2(x1,(x2,x3))→ x4x5 (x4,(x5,x6))
Existential Probability: 0 0 1 0 0 1
Composition Probability: ∅ ∅ 1 ∅ ∅ 0
Output after iteration 3
Sequence: x1x2(x1,(x2,x3))x4x5((x1,(x2,x3)),(x4,(x5,x6)))
Existential Probability: 0 0 0 0 0 1
Table 1. Simulation of an RvNN based on our new formulation. We use a st rikethrough to illustrate “non-existing” representation s (0
existential probability) as they are ignored for further co mputation. We use ∅for composition scores of non-existing representations. I n
each iteration, we bold the child with a composition probabi lity of 1. We also use a →symbol to indicate that the child will be sent over
to the ﬁrst right position with existential probability of 1so that a composed parent representation can be formed at tha t position. The
composition probabilities are predicted in each step by the decision function D.
composition probability determines which positions are to
be replaced by the parent representation. For now, we only
considerci∈{0,1}. Note, unlike Gumbel-Tree LSTM, in
our formulation multiple positions can be chosen for com-
position. In§3.1.5 , we discuss the exact recursive rules to
update the representations based on the composition proba-
bilities.
3.1.4. L EFT AND RIGHT FUNCTIONS
Deﬁnition 3. Given a sequence of values v1:n=
(v1,v2,···,vn), we deﬁne a function left which takes
as input some value viand returns the closest existing
left value vj(i.e.,vj=left(vi)) such that ej= 1 and
∀l,j < l < i, e l= 0. Note that since position i−1can
have an existential probability ei−1= 0,left(vi)is not
alwaysvi−1.
In a similar vein, we also deﬁne a function right which
takes as input some value viand returns the closest exist-
ing right value vj(i.e.,vj=right(vi)) such that ej= 1
and∀l,i < l < j, e l= 0. Similar to the left function,
right(vi)is not always vi+1. Note that the sequence v1:n
could be any sequence including a sequence of represen-
tations (r1:n) or a sequence of composition probabilities
(c1:n).
3.1.5. R ECURSIVE UPDATE
Now, we describe how the above components come to-
gether in each recursive update of our reformulated RvNN.
We can formally express the rule for recursively updating
some representation at position i,riin the recursive step kas:
rk+1
i=left(ci)·f(left(rk
i),rk
i)+(1−left(ci))·rk
i,(3)
wherefrefers to the composition function (recursive cell)
as before and ciis the composition probability (as pre-
dicted by the function D) at any position i. Note that
after this point, if left(ci) = 1 ,left(rk
i)is no longer
needed because it has been composed together with rk
iand
the composed parent representation already exists in posi-
tioni(asrk+1
i) according to the above rule. Thus, when
left(ci) =cj= 1 we can set the existential probability
(ej) at position left(i) =jas0. Thus, we can express the
update rule for ejas:
ek+1
j=ek
j·(1−cj). (4)
In Table 1, we show how we can simulate an RvNN based
on the rules as laid out above using a sequence x1:6.3
3.2. Towards Continuous RvNN
In the above sections we consider only discrete binary val-
ues for composition probability and existential probabili ty.
As such, it is still a “hard RvNN” with a discrete order of
composition. To transform it into a Continuous RvNN (soft
RvNN), we simply incorporate a continuous relaxation to
the decision function Dallowing it to predict composition
3Note, in iteration 2in Table 1, when we compose together the
representation of x1(position 1) and the representation of (x2,x3)
(position 3), the former is not at the immediate left (position 2)
from the latter because position 2has0existential probability.

--- PAGE 4 ---
Continuous Recursive Neural Networks
probabilities ( ci) in the interval [0,1]. Similarly, we also
allow existential probabilities ( ei) to be in [0,1].
As a result of this relaxation, we can simply use a neural
network with sigmoid activation for D. Given that we ac-
commodate for continuous values, we do not have to force
binary decisions using some form of reparameterization or
gradient approximation. Moreover, we can directly use the
recursive update rules as deﬁned in Eq. 3and Eq. 4with-
out any changes because they are already compatible with
real-valued ciandei.
Below, we discuss our re-deﬁnition of left andright func-
tions (neighbor retriever functions) under this new contex t.
3.2.1. N EIGHBOR RETRIEVER FUNCTIONS
In our reformulation presented in § 3.1, an active role is
played by the left andright functions. However, the pre-
vious deﬁnition (Def. 3) was made under the condition
that the existential probabilities can only take binary val ues.
Since now we have real-valued probabilities, the notion of
“closest existing left (or right) value” is not well deﬁned b e-
cause existential probabilities can have non-binary value s.
Precisely, both left andright functions are important for
CRvNN, and either of the functions can be re-purposed as
the other with minimal changes. Thus, for now, we only
focus on the right function which can be easily adapted
into aleft function.
Deﬁnition 4. Given a sequence of values v1:n=
(v1,v2,···,vn), and for every vi, given a sequence of
probabilities pi1:in= (pi1,pi2,···,pin)such that pijin-
dicates the probability that vjis the closest existing value
right tovi, we deﬁne the function right as:right(vi) =/summationtextn
j=1pij·vj.
Essentially, the function right returns something analo-
gous to an expected value of the immediate right existing
representation. Note, however, that/summationtext
jpijis not necessar-
ily1, rather≤1. This is because there is another possibility
that there is no existing representation at the right at all.
In our implementation, we transform the existential prob-
abilitiese1:n= (e1,e2,···,en)into the sequence pi1:in.
Below, we formulate the precise rules that we use:
pij=

0 j≤i
ej/summationtextj
l=i+1el≤1
max(0,1−/summationtextj−1
l=i+1el)/summationtextj
l=i+1el>1(5)
That is, we re-purpose the existential probabilities ejaspij.
Naturally, we ﬁrst set all non-right values from ito have0
probability. Then we adjust them so that they sum up to ≤1
by zeroing out pijfor any values after the position where
the accumulated probability exceeds 1.
To an extent, however, Eq. 5is an engineering choice ratherAlgorithm 1 Continuous Recursive Neural Network
Input: datax1:n= (x1,x2,···,xn)
r1
i←leafTransform (xi){i= 1,···,n}
Initialize e1
i←1
fork= 1ton−1do
c1:n←decisionFunction (rk
1:n)
α←left(ci)
h←left(rk
i)
rk+1
i←α·compose(h,rk
i)+(1−α)·rk
i
ek+1
i←ek
i·(1−ci)
ifdynamicHalt (ek+1
1:n)then
break
end if
end for
than being completely mathematically grounded. A more
mathematically grounded choice would be to set pij=
ej·Πj−1
l=i+1(1−el). We created a log-sum-exp based formu-
lation for the equation (to avoid potential instabilities d ue
to cumulative multiplication) as:
pij=ej·exp/parenleftBiggj−1/summationdisplay
l=i+1log(1−el)/parenrightBigg
(6)
We did some experiments with this on the synthetic logical
inference ( Bowman et al. ,2015b ) dataset. It performed rea-
sonably but slightly worse than when we use Eq. 5. Overall
both formulations (Eq. 5or Eq. 6) should be equivalent in
the discrete setting, and thus, both can push the model to
approximate a discrete RvNN. Regardless, for focus, we
only consider Eq. 5in the rest of the paper.
Given these formulations we can further generalize both
left andright functions.
Deﬁnition 5. Given a sequence v1:n= (v1,v2,···,vn),
we deﬁne leftm(vi)as the function to retrieve the expected
value in the mthleft position from i. Formally:
left1(ri) =left(ri), (7)
leftm(ri) =left(leftm−1(ri)). (8)
Similarly, we can deﬁne rightm. In the following subsec-
tion, we describe the main algorithm.
3.3. Continuous RvNN: Algorithm
The Continuous RvNN model is presented in Algorithm 1.
We already explained the recursive update rules. Next, we
describe the implementation details of all the functions in
the algorithm.
3.3.1. L EAF TRANSFORMATION
We use the function leafTransform simply for an initial
transformation of the embedded sequence. This can be ex-

--- PAGE 5 ---
Continuous Recursive Neural Networks
Algorithm 2 dynamicHalt
Input: datae1:n= (e1,e2,···,en)
ifei< ǫthen
di= 0 {i= 1,···,n}
else
di= 1
end if
if/summationtextn
i=1di= 1then
return True
else
return False
end if
pressed as:
r1
i=LN(Wxi+b). (9)
Here,xi∈IRdembed×1,W∈IRdh×dembed, andb∈IRdh×1.
LNrefers to layer normalization ( Ba et al. ,2016 ).
3.3.2. D ECISION FUNCTION
The decision function is used to predict the composition
probabilities ci. For this, we take into account the local
context using a convolutional layer. However, we want the
“true local context” for which we need to use the leftm
andrightmfunctions, as deﬁned above. Given a locality
window size of 2·τ+1, we use the following function to
get the initial un-normalized scores uifor computing the
composition probabilities ci:
ui=W2GeLU
τ/summationdisplay
j=−τWj+τ
convL(ri,j)+bconv
+b2,
(10)
L(ri,j) =

left−j(ri)j <0,
ri j= 0,
rightj(ri)j >0.(11)
Here, we have a set of convolutional kernel weights
{W0
conv,W1
conv,,···,W2·τ
conv}, where any Wl
conv∈
IRdh×dh;L(ri,j)∈IRdh×1,W2∈IR1×dh,bconv∈IRdh
and,b2∈IR1. Note that uiis now an un-normalized scalar
real value. To turn it into a probability we can use sigmoid :
ci=sigmoid(ui) =exp(ui)
exp(ui)+1. (12)
The use of sigmoid allows multiple positions to have high
composition probabilities ( ci). As such, multiple parents
can be composed in the same recursion resulting in the par-
allelism that we alluded to before. However, pure sigmoid
in this form is very unconstrained. That is, there is no con-
straint that prevents multiple contiguous (as deﬁned by left
andright functions) positions from having high composi-
tion probabilities ( ci). Concurrently composing contiguousrepresentations with their expected right representation will
violate the tree-structure. It can also cause information b e-
ing propagated to positions that, at the same time, lose thei r
own existential probability due to propagating their infor -
mation rightwards. As such, some propagated information
can henceforth be ignored due to landing in an area that
loses its existential probability. Thus, to prevent contig u-
ous positions from having high existential probabilities, we
modulate the sigmoid formulation with the scores from the
neighbors as follows:
ci=exp(ui)
exp(ui)+exp(left(ui))+exp(right(ui))+1.
(13)
We refer to this function as modulated sigmoid .
3.3.3. C OMPOSITION FUNCTION
We use the same recursive gated cell as introduced by
Shen et al. (2019a ) for our composition function. This is
originally inspired from the feedforward functions of Tran s-
formers ( Vaswani et al. ,2017 ).

zi
hi
ci
ui
=W2GeLU/parenleftbigg
WCell
1/bracketleftbiggleft(ri)
ri/bracketrightbigg
+b1/parenrightbigg
+b2(14)
oi=LN(σ(zi)⊙left(ri)+σ(hi)⊙ri+σ(ci)⊙ui)(15)
Here,σissigmoid ;oiis the output parent ∈IRdh×1;
ri,left(ri)∈IRdh×1;Wcell
1∈IRdcell×2·dh;b1∈
IRdcell×1;W2∈IRdh×dcell;b1∈IRdh×1. Different from
(Shen et al. ,2019a ), we use GeLU ( Hendrycks & Gimpel ,
2016 ) instead of ReLU as the activation function.
3.3.4. D YNAMIC HALT
Ideally, CRvNN can learn to simultaneously process all
children at the same hierarchy level. Thus, CRvNN will
only need to recurse over the tree depth. This means that
we need a mechanism to detect if the induced tree-depth
has been traversed and if we can halt. Following our frame-
work, we only need to look at the sequence of existen-
tial probabilities e1:n= (e1,e2,···,en). Near the ideal
time for halting, all existential probabilities except the last
should be close to 0(0indicates that it is no longer in need
of further processing). The last position has nothing to the
right of it to be composed. So we enforce the last position
to always have 0composition probability and thus, 1exis-
tential probability. Overall, to determine when to halt, we
simply check if all existential probabilities except the la st
are less than some small threshold ǫ. Algorithm 2shows an
implementation of this mechanism.

--- PAGE 6 ---
Continuous Recursive Neural Networks
Model Number of Operations Systematicity
7 8 9 10 11 12 A B C
(Sentence representation models + ground truths )
Tree-LSTM* 94 92 92 88 87 86 91 84 76
Tree-Cell* 98 96 96 95 93 92 95 95 90
Tree-RNN* 98 98 97 96 95 96 94 92 86
(Inter-sentence interaction models )
Transformer* 51 52 51 51 51 48 53 51 51
Universal Transformer* 51 52 51 51 51 48 53 51 51
(Sentence representation models )
LSTM* 88 84 80 78 71 69 84 60 59
RRNet* 84 81 78 74 72 71 — — —
ON-LSTM* 91 87 85 81 78 75 70 63 60
Ordered Memory* 980974965948935921194 91 81
(Our model )
CRvNN 9819739629569489359919839222
Table 2. Accuracy on the Synthetic Logical Inference dataset ( Bowman et al. ,2015b ) for different number of operations after training
on samples with ≤6operations. We also show results of testing systematicity o f the models in specially designed splits: A, B, and C.
* indicates that the results were taken from ( Shen et al. ,2019a ). Our models were run 5times on different random seeds. We show its
mean and standard deviation. Speciﬁcally, subscript repre sents standard deviation. As an example, 901= 90±0.1
.
3.4. Extra components
Here we describe some extra, potentially less essential,
components that we use in our implementation of CRvNN.
3.4.1. S PECIAL EMBEDDINGS
We prepend all sequences with a special <START>embed-
ding, and we append all sequences with a special <END>
embedding. Both are trainable vectors of size dembed .
These embeddings are enforced to always have a composi-
tion probability 0and an existential probability of 1. When
constructing the local context, these embeddings can pro-
vide more explicit information about how close to the start-
ing or ending boundary a representation riis.
3.4.2. T RANSITION FEATURES
In an attempt to enhance the decision function, we also con-
catenate features to provide the explicit information if a p o-
sitioniwas composed and updated in the last recursion or
not. For that, we construct a set of features T∈IRdsas:
T=left(ci)·C+(1−left(ci))·/hatwideC. (16)
Here,Cand/hatwideCare both trainable vector parameters ∈IRds.
Ccan be conceived to be representing a set a features indi-
cating that in the last iteration the concerned position was
composed with the left child and updated, whereas /hatwideCcan
be conceived to be representing the contrary. Here, we use
theleft(ci)values from the last recursion (initially 0).3.4.3. H ALT PENALTY
As discussed before, during halting, in the sequence of exis -
tential probabilities ( e1:n) all but the last existential proba-
bilities should be close to 0. While this is ideally supposed
to happen if all the positions are processed properly, there
is no guarantee that it will indeed happen. To encourage
this property, we use an auxiliary loss ( A(e1:n)), which we
deﬁne as:
A(e) =−log/parenleftBigg
en/summationtextn
j=1ej/parenrightBigg
. (17)
This can be conceived as cross entropy between the ideal
ﬁnal sequence of existential probabilities (all but the las t
being0) and the actual ﬁnal sequence of the same after
normalization. The overall optimization function can be
formulated as:
min
θL(θ)+γ·A(e), (18)
whereL(θ)is the main cross entropy loss and γis a La-
grangian multiplier for the auxiliary objective.
4. Experiments and Results
In this section, we discuss our experiments and results. We
evaluate our model on logical inference ( Bowman et al. ,
2015b ), list operations (ListOps) ( Nangia & Bowman ,
2018 ), sentiment analysis—two datasets, SST2 and SST5
(Socher et al. ,2013 ), and natural language inference—
two datasets, SNLI ( Bowman et al. ,2015a ) and MNLI
(Williams et al. ,2018b ). For implementation details, refer
to the appendix.

--- PAGE 7 ---
Continuous Recursive Neural Networks
Model Sequence length ranges (ListOps)
200−300300−400400−500500−600600−700700−800800−900900−1000
CRvNN 98.51±1.198.46±1.398.04±1.397.95±1.197.17±1.697.84±1.796.94±1.696.78±1.9
Table 3. Extrapolation of CRvNN on ListOps after training on samples of length ≤100. We used the ListOps extrapolation test set from
(Havrylov et al. ,2019 ).
Model Accuracy
(Models with ground truth )
Tree-LSTM ‡ 98.7
(Models without ground truth )
Transformer* 57.4±0.4
Universal Transformer* xxx71.5±7.8
LSTM† 71.5±1.5
RL-SPINN † 60.7±2.6
Gumbel-Tree LSTM † 57.6±2.9
(Havrylov et al. ,2019 )†99.2±0.5
Ordered Memory* 99.97±0.014
(Our model )
CRvNN 99.6±0.3
Table 4. Accuracy on ListOps. Results with * were taken from
(Shen et al. ,2019a ).‡indicates that the results were taken from
(Nangia & Bowman ,2018 ).†indicates that the results were taken
from ( Havrylov et al. ,2019 ). Our models were run 5times on
different random seeds. We show its mean and standard deviat ion.
4.1. Logical Inference
In the logical inference dataset, we focus on two par-
ticular generalization properties separately - length gen -
eralization and compositional generalization (i.e., sys-
tematicity). We compare CRvNN with Tree-LSTM
(Tai et al. ,2015 ), Tree-Cell ( Shen et al. ,2019a ) Tree-RNN
(Bowman et al. ,2015b ), Tranformer ( Vaswani et al. ,2017 ),
Universal Transformer ( Dehghani et al. ,2019 ), LSTM
(Hochreiter & Schmidhuber ,1997 ), RRNet ( Jacob et al. ,
2018 ), ON-LSTM ( Shen et al. ,2019b ), Ordered Memory
(Shen et al. ,2019a ) (see Table 2).
4.1.1. L ENGTH GENERALIZATION
To evaluate CRvNN for length generalization, as in prior
work, we train the model only on samples with ≤6op-
erations whereas we test it on samples with higher unseen
number of operations ( ≥7). In Table 2, discounting the
performance of RvNN-based models with ground-truth ac-
cess, our model, along with ordered memory, achieves the
best performance on length generalization.
4.1.2. S YSTEMATICITY
Following Shen et al. (2019a ), we create three different
train-test splits on the logical inference dataset: A, B, an d
C (with increased level of difﬁculty from A to C, A being
the easiest). For each split, we ﬁlter all samples with a
speciﬁc compositional pattern from the training set and putModel SST2 SST5 SNLI MNLI
RL-SPINN ‡ — — 82.367.4
Gumbel-Tree-LSTM ††90.753.785.6 —
Gumbel-Tree-LSTM ‡ — — 83.769.5
Gumbel-Tree-LSTM †90.3551.6884.91 —
(Havrylov et al. ,2019 )†90.2251.5485.1270.73
Ordered Memory* 90.452.2 — —
CRvNN 88.3651.41385.1272.92
Table 5. Accuracy on multiple natural language datasets. * indi-
cates that the results were taken from ( Shen et al. ,2019a ).†indi-
cates that the results were taken from ( Havrylov et al. ,2019 ).‡
indicates that the results were taken from ( Williams et al. ,2018a ).
††indicates that the results were taken from ( Choi et al. ,2018 ).
Our models were run 5times (except, on MNLI it was run 3
times) on different random seeds. We show its mean and stan-
dard deviation. Subscript represents standard deviation. E.g.,
901= 90±0.1.
them into the test set. Then we check whether our model
can generalize to unseen patterns (unseen combinations of
operands and operators). In set A, we ﬁlter samples accord-
ing to the pattern∗(and(nota))∗, in B, we ﬁlter according
to the pattern∗(and(not∗))∗, and in C, we ﬁlter according
to∗({and,or}(not∗))∗. As evident in Table 2, CRvNN
exhibits exceptional capability for compositional genera l-
ization and outperforms all prior reported results.
4.2. ListOps
ListOps is a challenging synthetic task which explicitly de -
mands capability for hierarchical modeling. Several prior
latent-tree models were shown to perform poorly on it
(Nangia & Bowman ,2018 ). As shown in Table 4, CRvNN
gets close to perfect accuracy, demonstrating its capabili ty
to capture underlying structures without structural super vi-
sion. We also show the length generalization capability of
CRvNN on ListOps in Table 3. CRvNN still achieves high
accuracy in much higher sequence lengths ( 400−1000 )
even when it is originally trained on sequences of lengths
≤100.
4.3. Natural Language Datasets
In Table 5, we also evaluate CRvNN on natural language
datasets (SST2, SST5, SNLI, MNLI). Consistent with prior
work ( Havrylov et al. ,2019 ), for MNLI we augment the
training data with SNLI training data and report the test

--- PAGE 8 ---
Continuous Recursive Neural Networks
result on the matched test set. In the real-world tasks,
CRvNN obtains mixed results, but overall the results are
comparable to prior work in the similar latent-tree con-
text. Particularly, CRvNN performs comparatively weak
on SST2, but on contrary it performs signiﬁcantly better
than prior work (besides OM) on MNLI, which is a harder
task. We also ran a CRvNN model with an enforced bal-
anced tree structure induction on MNLI but its performance
was comparatively worse ( 71.5±0.4). For Gumbel-Tree
LSTM Choi et al. (2018 ), we show results reported from
different works because the results ﬂuctuate from one paper
to another ( Havrylov et al. ,2019 ;Williams et al. ,2018a ).
5. Analysis
In this section, we contrast CRvNN with Ordered Memory
in terms of speed and show an ablation study of CRvNN.
5.1. Speed Test
Ordered memory (OM) is a close competitor for CRvNN.
Both of the models are end-to-end differentiable. Both
can simulate an RvNN without ground truth structures.
CRvNN neither consistently nor conclusively outperforms
OM. However, there is one crucial advantage for CRvNN.
Particularly, it can achieve a degree of parallelism by pro-
cessing multiple positions concurrently. At the same time,
it can do an early-exit using dynamic halt. In contrast, OM
not only has an outer loop over the whole sequence length,
but it also has an inner loop where it recurse over its mem-
ory slots, adding signiﬁcant overhead. The inner loop re-
cursion of OM involves heavy sequential matrix operations.
As such, in practice, CRvNN can be much faster than OM.
In Table 6, we compare the training run time for both mod-
els. We generated synthetic ListOps samples for different
ranges of sequence lengths. For each range of sequence
lengths, we trained both the models on 50samples for 1
epoch and 1batch size on an AWS P3.2×instance (Nvidia
V100). As we can see from the table, CRvNN is substan-
tially faster than OM. Although in other settings with dif-
ferent tasks and different batch sizes, the gap between the
speed of OM and CRvNN may not be as high as in Ta-
ble6, we still noticed CRvNN to be roughly 2−4times
faster than OM even when using higher batch sizes for OM
(taking advantage of its lower memory complexity). We
also tried running CYK-LSTM ( Maillard et al. ,2019 ) but
faced memory issues when running it for longer sequences
(>200length).
5.2. Ablation Study
In Table 7, we show an ablation study on CRvNN. We
ﬁnd that replacing modulated sigmoid with simple sigmoid
signiﬁcantly degrades and destabilizes the performance in
ListOps. Removing “structure” or the structural bias bysimply using the composition function as left-to-right rec ur-
rent network, again, signiﬁcantly harms the performance
of the model. The gated recursive cell itself is also cru-
cial for the performance. Replacing it with an LSTM cell
(Hochreiter & Schmidhuber ,1997 ) causes severe degrada-
tion. To an extent, this is consistent with the ﬁndings of
Shen et al. (2019a ). They replaced the gated recursive cell
in ordered memory ( Shen et al. ,2019a ) with an RNN cell,
and observed substantial degradation. However, even with
an LSTM cell, CRvNN performs better, in logical infer-
ence, than any of the other reported models which do not
have the gated recursive cell. Replacing the activation fun c-
tion, removing the transition features (§ 3.4.2 ), or eliminat-
ing the halt penalty (§ 3.4.3 ) makes little difference.
5.3. Parsing Results
The internal composition scores of each layer of CRvNN
can be used to parse induced trees. While there can be
multiple ways to convert the composition scores to extract
trees, one method is to simply treat any particular position
of a given sequence at a particular iteration in the algorith m
as having a composition probability of 1whenever the cu-
mulative composition probability at that position over all
the iterations thus far is ≥0.5. Otherwise, we treat the
position at the particular iteration as having a compositio n
probability of 0. Once the scores are binarized, it is straight-
forward to extract the trees following the ideas discussed i n
§3.1. Table 1(in § 3.1) shows an example on how binary
and discrete composition probabilities relate to a particu lar
structure of composition. In Table 8, we show parsing ex-
amples obtained with CRvNN using the above procedure.
6. Related Work
Several early approaches focused on adapting neural
models for simulating pushdown automata or for gram-
mar induction in general ( Sun,1990 ;Giles et al. ,1990 ;
Das et al. ,1992 ;Mozer & Das ,1993 ;Zeng et al. ,1994 ;
Grefenstette et al. ,2015 ). Also, recently, there are
multiple works that focus on structure induction based
on language modeling objectives ( Yogatama et al. ,2018 ;
Shen et al. ,2018 ;2019b ;Li et al. ,2019 ;Kim et al. ,2019 ;
Drozdov et al. ,2019 ;2020 ;Shen et al. ,2021 ). In this work,
we focus on models with structural bias that are used for
general downstream tasks including NLP tasks such as clas-
siﬁcation and NLI.
Pollack (1990 ) presented RvNN as a recursive architecture
to compose tree and list-like data-structures. Socher et al.
(2010 ;2013 ) aimed at capturing syntactic and semantic
patterns of linguistic phrases and showed the effective-
ness of RvNNs for sentiment analysis. Originally, RvNNs
relied on external parsers to provide tree-structured in-
puts. Several works focused on augmenting RvNNs so

--- PAGE 9 ---
Continuous Recursive Neural Networks
Model Sequence length ranges
81−100101−200201−500501−700701−1000
Ordered Memory 3.28 min 5.54 min 13.23 min 25 min 35.34 min
CRvNN 0.20 min 0.52 min 0.30 min 0.38 min 1.08 min
Table 6. Training time taken by Ordered Memory and CRvNN models to be r un on50samples on various sequence length ranges. We
used the publicly available code to run Ordered Memory.
Model Logical inference ListOps
Number of Operations
10 11 12
CRvNN 95.0693.7893.1599.63
– modulated sigmoid 95.0394.3492.8590.8163
– transition features 95.0894.4792.1599.45
– structure 87.21186.11080.92083.411
– halt penalty 95.2594.6493.1799.42
– GeLU + ReLU 95.1794.1592.7699.24
– Cell + LSTMCell 89.4789.2786.51171.241
Table 7. Cell represents Gated Recursive Cell. All the models
were run 5times on different random seeds. Subscript represents
standard deviation. For example, 901= 90±0.1
Parsing Examples
((((i did) not) like) (((a single) minute) ((of this) (ﬁlm .) )))
(((roger dodger) ((is (one of)) (the most)))
(compelling (variations (of (this theme)))))
Table 8. Parsing examples obtained using CRvNN trained on
MNLI. The example sentences are from ( Socher et al. ,2013 ).
that they can automatically induce the tree structure from
plain text. To this end, Le & Zuidema (2015 ) used a
chart-based parsing algorithm (Cocke–Younger–Kasami al-
gorithm ( Sakai ,1961 )) in a neural framework with a con-
volutional composition function. Maillard et al. (2019 )
extended it with an attention mechanism to pool over
multiple subtree candidates and a Tree-LSTM ( Tai et al. ,
2015 ) composition function. Bowman et al. (2016 ) pre-
sented a stack-augmented RNN (SPINN) to simulate the
functions of a Tree-RNN based on the principles of shift-
reduce parsing ( Sch¨ utzenberger ,1963 ;Knuth ,1965 ) but
relied on ground truth structure annotations for supervi-
sion. Yogatama et al. (2017 ) augmented SPINN with re-
inforcement learning (RL-SPINN) allowing unsupervised
structure induction. Maillard & Clark (2018 ) enhanced
shift-reduce parsing-based stack-RNNs with beam search.
Munkhdalai & Yu (2017 ) introduced Neural Tree Indexers
and gained strong performance by using full binary trees on
natural language tasks. Similarly, Shi et al. (2018 ) showed
that trivial trees (e.g., binary balanced trees) can be used to
get competitive performances on several natural language
tasks. Dong et al. (2019 ) proposed a neural model to learn
boolean logic rules and quantiﬁcations to reason over re-lational data with multiple arities. Choi et al. (2018 ) used
Gumbel Softmax ( Jang et al. ,2017 ) to adaptively make dis-
crete decisions at every recursive step in choosing a parent
to compose with a tree-LSTM. Jacob et al. (2018 ) proposed
a combination of recursive and recurrent neural structure
through reinforcement learning for hierarchical composi-
tion. Havrylov et al. (2019 ) extended Gumbel Tree-LSTM
(Choi et al. ,2018 ) through disjoint co-operative training of
the parser (with reinforcement learning) and the composi-
tion function. Shen et al. (2019a ) developed ordered mem-
ory (OM) by synthesizing the principles of ordered neurons
(Shen et al. ,2019b ) with a stack-augmented RNN. While
OM is a strong contender to CRvNN, we discussed the ad-
vantages of CRvNN as a more parallelized sequence pro-
cessor in § 5.1.
In another direction, Liu & Lapata (2018 ) applied struc-
tured attention ( Kim et al. ,2017 ) based on induced de-
pendency trees to enhance downstream performance.
Niculae et al. (2018 ) introduced SparseMAP to apply
marginalization over a sparsiﬁed set of latent structures
whereas Corro & Titov (2019 ) did a Monte-Carlo estima-
tion by stochastically sampling a projective depedency tre e
structure for their encoder. These methods, similar to ours ,
are end-to-end differentiable, but do not focus on hierarch i-
cal composition of a sequence vector (root node) from the
elements (leaf nodes) in the sequence following its latent
constituency structure. As such, they do not address the
challenges we face here.
In recent years, Transformers ( Vaswani et al. ,2017 ) have
also been extended either to better support tree struc-
tured inputs ( Shiv & Quirk ,2019 ;Ahmed et al. ,2019 ) or
to have a better inductive bias to induce hierarchical
structures by constraining self-attention ( Wang et al. ,2019 ;
Nguyen et al. ,2020 ;Shen et al. ,2021 ) or by pushing in-
termediate representations to have constituent informati on
(Fei et al. ,2020 ). However, the fundamental capability of
Transformers for composing sequences according to their
latent structures in a length-generalizable manner is show n
to be lacking ( Tran et al. ,2018 ;Shen et al. ,2019a ;Hahn ,
2020 ).
7. Conclusion and Future Directions
We proposed a reformulation of RvNNs to allow for a con-
tinuous relaxation of its structure and order of composi-

--- PAGE 10 ---
Continuous Recursive Neural Networks
tion. The result is CRvNN, which can dynamically induce
structure within data in an end-to-end differentiable man-
ner. One crucial difference from prior work is that it can
parallely process multiple positions at each recursive ste p
and also, dynamically halt its computation when needed.
We evaluated CRvNN on six datasets, and obtain strong
performance on most of them. There are, however, sev-
eral limitations of the model. First, the neighbor retrieve r
functions in CRvNN, construct an n×nmatrix (nis the
sequence length) which can be memory intensive. This is
similar to the memory limitations of a Transformer. An-
other limitation is that it is a greedy model. It is also not
explicitly equipped to handle structural ambiguities in na t-
ural language. In future work, we will address these lim-
itations. To mitigate the memory limitations, we plan to
constrain the neighbor retriever functions to look at only k
left or right candidates so that we only need an n×kmatrix.
We can compress contiguous positions with low existential
probabilities so that we do not need to look beyond k. To
handle its greedy nature, we will extend CRvNN to follow
multiple paths concurrently.
8. Acknowledgment
We would like to sincerely thank our reviewers for their
constructive feedback that helped us to improve our paper
greatly. We also thank Adrian Silvescu for helpful discus-
sions. This research is supported in part by NSF CAREER
award #1802358, NSF CRI award #1823292, and an award
from UIC Discovery Partners Institute. The computation
for this project was performed on Amazon Web Services.
References
Ahmed, M., Samee, M. R., and Mercer, R. E. You only
need attention to traverse trees. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pp. 316–322, Florence, Italy, July
2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1030.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normaliza-
tion. arXiv preprint arXiv:1607.06450 , 2016.
Bengio, Y ., Simard, P., and Frasconi, P. Learning long-term
dependencies with gradient descent is difﬁcult. IEEE
Transactions on Neural Networks , 5(2):157–166, 1994.
Bergstra, J., Bardenet, R., Bengio, Y ., and K´ egl, B. Algo-
rithms for hyper-parameter optimization. In Proceedings
of the 24th International Conference on Neural Informa-
tion Processing Systems , NIPS’11, pp. 2546–2554, Red
Hook, NY , USA, 2011. Curran Associates Inc. ISBN
9781618395993.
Bergstra, J., Yamins, D., and Cox, D. D. Making a sci-ence of model search: Hyperparameter optimization in
hundreds of dimensions for vision architectures. In Pro-
ceedings of the 30th International Conference on Inter-
national Conference on Machine Learning - Volume 28 ,
ICML’13, pp. I–115–I–123. JMLR.org, 2013.
Bowman, S. R., Angeli, G., Potts, C., and Manning,
C. D. A large annotated corpus for learning natural lan-
guage inference. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP) . Association for Computational Lin-
guistics, 2015a.
Bowman, S. R., Manning, C. D., and Potts, C. Tree-
structured composition in neural networks without tree-
structured architectures. In Proceedings of the 2015th In-
ternational Conference on Cognitive Computation: Inte-
grating Neural and Symbolic Approaches - Volume 1583 ,
COCO’15, pp. 37–42, Aachen, DEU, 2015b. CEUR-
WS.org.
Bowman, S. R., Gauthier, J., Rastogi, A., Gupta, R., Man-
ning, C. D., and Potts, C. A fast uniﬁed model for pars-
ing and sentence understanding. In Proceedings of the
54th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pp. 1466–
1477, Berlin, Germany, August 2016. Association for
Computational Linguistics. doi: 10.18653/v1/P16-1139.
Choi, J., Yoo, K. M., and Lee, S. Learning to compose task-
speciﬁc tree structures. In McIlraith, S. A. and Wein-
berger, K. Q. (eds.), Proceedings of the Thirty-Second
AAAI Conference on Artiﬁcial Intelligence, (AAAI-18),
the 30th innovative Applications of Artiﬁcial Intelligenc e
(IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Or-
leans, Louisiana, USA, February 2-7, 2018 , pp. 5094–
5101. AAAI Press, 2018.
Chomsky, N. Syntactic structures . Walter de Gruyter, 1957.
Corro, C. and Titov, I. Learning latent trees with stochas-
tic perturbations and differentiable dynamic program-
ming. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , pp. 5508–
5521, Florence, Italy, July 2019. Association for Compu-
tational Linguistics. doi: 10.18653/v1/P19-1551.
Das, S., Giles, C. L., and Sun, G.-Z. Learning context-free
grammars: Capabilities and limitations of a recurrent
neural network with an external stack memory. In Pro-
ceedings of The Fourteenth Annual Conference of The
Cognitive Science Society , pp. 14, 1992.
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and
Kaiser, L. Universal transformers. In International Con-
ference on Learning Representations , 2019.

--- PAGE 11 ---
Continuous Recursive Neural Networks
Dong, H., Mao, J., Lin, T., Wang, C., Li, L., and Zhou, D.
Neural logic machines. In International Conference on
Learning Representations , 2019.
Drozdov, A., Verga, P., Yadav, M., Iyyer, M., and McCal-
lum, A. Unsupervised latent tree induction with deep
inside-outside recursive auto-encoders. In Proceedings
of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and Short
Papers) , pp. 1129–1141, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1116.
Drozdov, A., Rongali, S., Chen, Y .-P., O’Gorman, T., Iyyer,
M., and McCallum, A. Unsupervised parsing with S-
DIORA: Single tree encoding for deep inside-outside re-
cursive autoencoders. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP) , pp. 4832–4845, Online, November
2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.emnlp-main.392.
Fei, H., Ren, Y ., and Ji, D. Retroﬁtting structure-aware
transformer language model for end tasks. In Proceed-
ings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pp. 2151–2161,
Online, November 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.emnlp-main.168.
Fodor, J. A. and Pylyshyn, Z. W. Connectionism and cog-
nitive architecture: A critical analysis. Cognition , 28(1):
3 – 71, 1988. ISSN 0010-0277.
Giles, C., Sun, G.-Z., Chen, H.-H., Lee, Y .-C., and Chen,
D. Higher order recurrent networks and grammatical in-
ference. In Touretzky, D. (ed.), Advances in Neural In-
formation Processing Systems , volume 2, pp. 380–387.
Morgan-Kaufmann, 1990.
Grefenstette, E., Hermann, K. M., Suleyman, M., and Blun-
som, P. Learning to transduce with unbounded memory.
In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and
Garnett, R. (eds.), Advances in Neural Information Pro-
cessing Systems , volume 28, pp. 1828–1836. Curran As-
sociates, Inc., 2015.
Hahn, M. Theoretical limitations of self-attention in neur al
sequence models. Transactions of the Association for
Computational Linguistics , 8:156–171, 2020. doi: 10.1
162/tacl\a\00306.
Havrylov, S., Kruszewski, G., and Joulin, A. Coopera-
tive learning of disjoint syntax and semantics. In Pro-
ceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1 (Longand Short Papers) , pp. 1118–1128, Minneapolis, Min-
nesota, June 2019. Association for Computational Lin-
guistics. doi: 10.18653/v1/N19-1115.
Hendrycks, D. and Gimpel, K. Bridging nonlinearities and
stochastic regularizers with gaussian error linear units.
ArXiv , abs/1606.08415, 2016.
Hochreiter, S. Untersuchungen zu dynamischen neu-
ronalen netzen. diploma thesis. Diploma thesis, TU Mu-
nich, 1991.
Hochreiter, S. and Schmidhuber, J. Long short-term mem-
ory. Neural Comput. , 9(8):1735–1780, November 1997.
ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.
Jacob, A. P., Lin, Z., Sordoni, A., and Bengio, Y . Learn-
ing hierarchical structures on-the-ﬂy with a recurrent-
recursive model for sequences. In Proceedings of The
Third Workshop on Representation Learning for NLP , pp.
154–158, Melbourne, Australia, July 2018. Association
for Computational Linguistics. doi: 10.18653/v1/W18-
3020.
Jang, E., Gu, S., and Poole, B. Categorical reparameteriza-
tion with gumbel-softmax. In 5th International Confer-
ence on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceed-
ings. OpenReview.net, 2017.
Kim, Y ., Denton, C., Hoang, L., and Rush, A. M. Struc-
tured attention networks. International Conference on
Learning Representations , 2017.
Kim, Y ., Rush, A., Yu, L., Kuncoro, A., Dyer, C., and
Melis, G. Unsupervised recurrent neural network gram-
mars. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers) , pp. 1105–1117, Min-
neapolis, Minnesota, June 2019. Association for Compu-
tational Linguistics. doi: 10.18653/v1/N19-1114.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In Bengio, Y . and LeCun, Y . (eds.), 3rd
International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Con-
ference Track Proceedings , 2015.
Knuth, D. E. On the translation of languages from left to
right. Information and Control , 8(6):607 – 639, 1965.
ISSN 0019-9958.
Le, P. and Zuidema, W. The forest convolutional net-
work: Compositional distributional semantics with a
neural chart and without binarization. In Proceedings

--- PAGE 12 ---
Continuous Recursive Neural Networks
of the 2015 Conference on Empirical Methods in Natu-
ral Language Processing , pp. 1155–1164, Lisbon, Por-
tugal, September 2015. Association for Computational
Linguistics. doi: 10.18653/v1/D15-1137.
Li, B., Mou, L., and Keller, F. An imitation learning ap-
proach to unsupervised parsing. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pp. 3485–3492, Florence, Italy, July
2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1338.
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and
Han, J. On the variance of the adaptive learning rate
and beyond. In International Conference on Learning
Representations , 2020.
Liu, Y . and Lapata, M. Learning structured text representa-
tions. Transactions of the Association for Computational
Linguistics , 6:63–75, 2018. doi: 10.1162/tacl a00005.
Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. In International Conference on Learning Rep-
resentations , 2019.
Maillard, J. and Clark, S. Latent tree learning with dif-
ferentiable parsers: Shift-reduce parsing and chart pars-
ing. In Proceedings of the Workshop on the Relevance of
Linguistic Structure in Neural Architectures for NLP , pp.
13–18, Melbourne, Australia, July 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-29
03.
Maillard, J., Clark, S., and Yogatama, D. Jointly learning
sentence embeddings and syntax with unsupervised tree-
lstms. Natural Language Engineering , 25(4):433–449,
2019. doi: 10.1017/S1351324919000184.
Martins, A. F. T., Mihaylova, T., Nangia, N., and Niculae,
V . Latent structure models for natural language process-
ing. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics: Tutorial Ab-
stracts , pp. 1–5, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-
4001.
Mozer, M. C. and Das, S. A connectionist symbol ma-
nipulator that discovers the structure of context-free lan -
guages. In Hanson, S., Cowan, J., and Giles, C. (eds.),
Advances in Neural Information Processing Systems , vol-
ume 5, pp. 863–870. Morgan-Kaufmann, 1993.
Munkhdalai, T. and Yu, H. Neural tree indexers for text
understanding. In Proceedings of the 15th Conference
of the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers , pp. 11–21,
Valencia, Spain, April 2017. Association for Computa-
tional Linguistics.Nangia, N. and Bowman, S. ListOps: A diagnostic dataset
for latent tree learning. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics: Student Research
Workshop , pp. 92–99, New Orleans, Louisiana, USA,
June 2018. Association for Computational Linguistics.
doi: 10.18653/v1/N18-4013.
Nguyen, X.-P., Joty, S., Hoi, S., and Socher, R. Tree-
structured attention with hierarchical accumulation. In
International Conference on Learning Representations ,
2020.
Niculae, V ., Martins, A., Blondel, M., and Cardie, C.
SparseMAP: Differentiable sparse structured inference.
In Dy, J. and Krause, A. (eds.), Proceedings of the 35th
International Conference on Machine Learning , vol-
ume 80 of Proceedings of Machine Learning Research ,
pp. 3799–3808, Stockholmsm¨ assan, Stockholm Sweden,
10–15 Jul 2018. PMLR.
Pennington, J., Socher, R., and Manning, C. D. Glove:
Global vectors for word representation. In Empirical
Methods in Natural Language Processing (EMNLP) , pp.
1532–1543, 2014.
Pollack, J. B. Recursive distributed representations. Ar-
tiﬁcial Intelligence , 46(1):77 – 105, 1990. ISSN 0004-
3702.
Qian, L., An, S., Lou, J.-G., Bei, C., Lin, Z., Gao, Y ., Bin,
Z., Nanning, Z., and Dongmei, Z. Compositional gener-
alization by learning analytical expressions. In Advances
in Neural Information Processing Systems 33 . 2020.
Sakai, I. Syntax in universal translation . 1961.
Sch¨ utzenberger, M. On context-free languages and push-
down automata. Information and Control , 6(3):246 –
264, 1963. ISSN 0019-9958.
Shen, Y ., Lin, Z., wei Huang, C., and Courville, A. Neural
language modeling by jointly learning syntax and lexi-
con. In International Conference on Learning Represen-
tations , 2018.
Shen, Y ., Tan, S., Hosseini, A., Lin, Z., Sordoni, A., and
Courville, A. C. Ordered memory. In Wallach, H.,
Larochelle, H., Beygelzimer, A., d'Alch´ e-Buc, F., Fox,
E., and Garnett, R. (eds.), Advances in Neural Informa-
tion Processing Systems 32 , pp. 5037–5048. Curran As-
sociates, Inc., 2019a.
Shen, Y ., Tan, S., Sordoni, A., and Courville, A. Ordered
neurons: Integrating tree structures into recurrent neura l
networks. In International Conference on Learning Rep-
resentations , 2019b.

--- PAGE 13 ---
Continuous Recursive Neural Networks
Shen, Y ., Tay, Y ., Zheng, C., Bahri, D., Metzler, D., and
Courville, A. Structformer: Joint unsupervised induction
of dependency and constituency structure from masked
language modeling, 2021.
Shi, H., Zhou, H., Chen, J., and Li, L. On tree-based
neural sentence modeling. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language
Processing , pp. 4631–4641, Brussels, Belgium, October-
November 2018. Association for Computational Linguis-
tics. doi: 10.18653/v1/D18-1492.
Shiv, V . L. and Quirk, C. Novel positional encodings to
enable tree-structured transformers, 2019.
Socher, R., Manning, C. D., and Ng, A. Y . Learning
continuous phrase representations and syntactic parsing
with recursive neural networks. In In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop , 2010.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A., and Potts, C. Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 1631–1642,
Seattle, Washington, USA, October 2013. Association
for Computational Linguistics.
Sun, G. Connectionist pushdown automata that learn
context-free grammars. In Proceedings of International
Joint Conference on Neural Networks , volume 1, pp.
577–580, 1990.
Tai, K. S., Socher, R., and Manning, C. D. Improved seman-
tic representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) , pp.
1556–1566, Beijing, China, July 2015. Association for
Computational Linguistics. doi: 10.3115/v1/P15-1150.
Tran, K., Bisazza, A., and Monz, C. The importance of be-
ing recurrent for modeling hierarchical structure. In Pro-
ceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing , pp. 4731–4736, Brus-
sels, Belgium, October-November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/D18-15
03.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-
tion is all you need. In Guyon, I., Luxburg, U. V ., Ben-
gio, S., Wallach, H., Fergus, R., Vishwanathan, S., and
Garnett, R. (eds.), Advances in Neural Information Pro-
cessing Systems 30 , pp. 5998–6008. Curran Associates,
Inc., 2017.Wang, Y ., Lee, H.-Y ., and Chen, Y .-N. Tree transformer:
Integrating tree structures into self-attention. In Pro-
ceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pp. 1061–1070, Hong Kong,
China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-1098.
Williams, A., Drozdov, A., and Bowman, S. Do latent
tree learning models identify meaningful structure in sen-
tences? Transactions of the Association for Compu-
tational Linguistics , 6(0):253–267, 2018a. ISSN 2307-
387X.
Williams, A., Nangia, N., and Bowman, S. A broad-
coverage challenge corpus for sentence understanding
through inference. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers) , pp. 1112–1122. Asso-
ciation for Computational Linguistics, 2018b.
Wright, L. Ranger - a synergistic optimizer., 2019.
Yogatama, D., Blunsom, P., Dyer, C., Grefenstette, E., and
Ling, W. Learning to compose words into sentences
with reinforcement learning. In 5th International Confer-
ence on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceed-
ings. OpenReview.net, 2017.
Yogatama, D., Miao, Y ., Melis, G., Ling, W., Kuncoro,
A., Dyer, C., and Blunsom, P. Memory architectures in
recurrent neural network language models. In Interna-
tional Conference on Learning Representations , 2018.
Zeng, Z., Goodman, R. M., and Smyth, P. Discrete recur-
rent neural networks for grammatical inference. IEEE
Transactions on Neural Networks , 5(2):320–330, 1994.
doi: 10.1109/72.279194.
Zhang, M., Lucas, J., Ba, J., and Hinton, G. E. Lookahead
optimizer: k steps forward, 1 step back. In Wallach, H.,
Larochelle, H., Beygelzimer, A., d'Alch´ e-Buc, F., Fox,
E., and Garnett, R. (eds.), Advances in Neural Informa-
tion Processing Systems , pp. 9597–9608. Curran Asso-
ciates, Inc.

--- PAGE 14 ---
Continuous Recursive Neural Networks
DatasetInitial
Embedding
Size
(dinitial embed )Hidden Size
(dhordembed )Input
DropoutOutput
DropoutHidden
Dropout
Logical Infer. 200 200 0.1 0.3 0.1
ListOps 128 128 0.3 0.2 0.1
SST2 300 300 0.3 0.2 0.4
SST5 300 300 0.3 0.2 0.4
SNLI 300 300 0.4 0.1 0.1
MNLI 300 300 0.4 0.1 0.1
Table 9. Hyperparameter details for CRvNN.
A. Architecture Details
For every task used in our experiments we use an initial
afﬁne transformation where the initial embeddings of size
dinitialembed are transformed into the size dembed . Typi-
cally, we set dembed asdh. See Table 9for their values.
We treat the last representation in the sequence after being
processed by CRvNN as the sentence encoding constructed
by CRvNN.
For classiﬁcation tasks, we classify the sentence encoding
by transforming it into logits for the classes after passing it
through a series of afﬁne layers (typically, 1 or 2). Interme -
diate layers have dhneurons where dhis also the dimension
of the sentence encoding.
For inference tasks (requiring sequence-pair comparison) ,
like logical inference or SNLI and MNLI, we use a Siamese
framework. Concretely, we ﬁrst encode (using the same
encoder with same parameters) both the premise and hy-
pothesis (separately) into sentence vectors, say, s1ands2
respectively (both with dhdimensions). Then, we construct
a classiﬁcation feature vector oas:
o= [s1;s2;|s1−s2|;s1⊙s2] (19)
Here,[;]indicates concatenation. We send oto a Multi
Layer Perceptron (MLP) to classify the sequence relation-
ship. The ﬁnal layer activation is Softmax , but if there are
intermediate MLP layers, we use GeLU for them. We use
a dropout (hidden dropout) in the gated recursive cell (af-
ter its ﬁrst afﬁne transformation). We use a dropout (input
dropout) on the input just before sending it to CRvNN. We
use another dropout (output dropout) in between the ﬁnal
MLP layers. All our models were trained on AWS p 3.2×
instance (Nvidia v 100).
B. Implementation Details
For all experiments, as an optimizer, we use Ranger
(Wright ,2019 )4or Rectiﬁed ( Liu et al. ,2020 ) Adam
4https://github.com/lessw2020/(Kingma & Ba ,2015 ) with lookahead ( k= 5,α=
0.8) (Zhang et al. ) and decorrelated weight decay
(Loshchilov & Hutter ,2019 ) (1e−2) with a learning rate
of1e−3. We used GloVe ( 300 dimensions, 840B)
(Pennington et al. ,2014 ) as un-trainable embeddings for
natural language data. We set the cell size ( dcellas referred
in §3.3.3 ) as4·dh(dhis the hidden size). We set the size of
transition features ( dsas referred in § 3.4.2 ) as64. For con-
volution in the decision function, we always use a window
size of5. For halt penalty in § 3.4.3 , we setγas0.01. Gen-
erally, we use a two-layered MLP on the sentence encoding
from CRvNN. However, on ListOps, we used a single-layer.
We use a batch size of 128for all tasks. We describe other
hyperparameters of CRvNN in Table 9. We cut the learn-
ing rate by half if the validation loss does not decrease for
3contiguous epochs.
C. Hyperparameter Search
On ListOps, we tune different dropouts among
{0.1,0.2,0.3,0.4}separately using grid search (we
ran for10epochs and 50,000 subsamples). For the
Logical Inference task (length generalization task), we
tune the different dropouts among {0.1,0.2,0.3}for7
epochs per trial using grid search. We use the same
hyperparamters for systematicity splits. For SST5, we tune
the dropouts in{0.2,0.3,0.4}for3epochs. For SNLI,
we tune the different dropouts among {0.1,0.2,0.3,0.4}
for5epochs, using a sub-sample of 100Kexamples,
and for a maximum of 20trials using Tree of Parzen
Estimators (TPE) ( Bergstra et al. ,2011 ). We use Hyperopt
(Bergstra et al. ,2013 ) for hyperparameter tuning. For
other components we mostly use similar hyperparameters
asShen et al. (2019a ) or default settings. We share the
hyperparameters found for SST5 with SST2 and we also
share the hyperparameters found for SNLI with MNLI.
D. Datasets
For all datasets, we use the standard splits as used by prior
work. For training efﬁciency, we ﬁlter out training samples

--- PAGE 15 ---
Continuous Recursive Neural Networks
of sequence size >150from MNLI. We ﬁlter out training
samples with sequence length >100from ListOps. We use
the90Ksample version of ListOps similar to prior work.
