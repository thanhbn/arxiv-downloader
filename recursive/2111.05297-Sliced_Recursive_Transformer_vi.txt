# Sliced Recursive Transformer
Zhiqiang Shen1;2;3, Zechun Liu2;4, và Eric Xing1;3
1Carnegie Mellon University, Pittsburgh, USA
2Hong Kong University of Science and Technology, Hong Kong, China
3Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE
4Reality Labs, Meta Inc.
zhiqiangshen@cse.ust.hk,zechunliu@fb.com,epxing@cs.cmu.edu

Tóm tắt. Chúng tôi trình bày một phép toán đệ quy gọn gàng nhưng hiệu quả trên vision transformer có thể cải thiện việc sử dụng tham số mà không cần thêm tham số bổ sung. Điều này đạt được bằng cách chia sẻ trọng số qua chiều sâu của mạng transformer. Phương pháp được đề xuất có thể đạt được mức tăng đáng kể (2%) chỉ bằng cách sử dụng phép toán đệ quy ngây thơ, không cần kiến thức đặc biệt hoặc tinh vi nào để thiết kế nguyên tắc của mạng, và giới thiệu chi phí tính toán tối thiểu cho quy trình huấn luyện. Để giảm tính toán bổ sung gây ra bởi phép toán đệ quy trong khi duy trì độ chính xác vượt trội, chúng tôi đề xuất một phương pháp xấp xỉ thông qua nhiều self-attention nhóm cắt khúc qua các lớp đệ quy có thể giảm tiêu thụ chi phí 10-30% mà không hy sinh hiệu suất. Chúng tôi gọi mô hình của mình là Sliced Recursive Transformer (SReT), một thiết kế vision transformer mới và tiết kiệm tham số tương thích với nhiều thiết kế khác cho kiến trúc ViT hiệu quả. Mô hình tốt nhất của chúng tôi thiết lập cải thiện đáng kể trên ImageNet-1K so với các phương pháp tiên tiến trong khi chứa ít tham số hơn. Cơ chế chia sẻ trọng số được đề xuất bởi cấu trúc đệ quy cắt khúc cho phép chúng tôi xây dựng transformer với hơn 100 hoặc thậm chí 1000 lớp chia sẻ một cách dễ dàng trong khi giữ kích thước nhỏ gọn (13-15M), để tránh khó khăn tối ưu hóa khi mô hình quá lớn. Khả năng mở rộng linh hoạt đã cho thấy tiềm năng lớn để mở rộng mô hình và xây dựng vision transformer cực kỳ sâu. Mã nguồn có sẵn tại https://github.com/szq0214/SReT.

1 Giới thiệu

Kiến trúc của transformer đã đạt được những đột phá đáng kể gần đây trong các lĩnh vực xử lý ngôn ngữ tự nhiên (NLP), thị giác máy tính (CV) và giọng nói. Trong lĩnh vực thị giác, Dosovitskiy et al. giới thiệu mô hình vision transformer (ViT) chia hình ảnh thô thành chuỗi patch làm đầu vào, và họ trực tiếp áp dụng mô hình transformer cho tác vụ phân loại hình ảnh.

ViT đạt được kết quả ấn tượng và đã truyền cảm hứng cho nhiều công trình tiếp theo. Tuy nhiên, lợi ích của transformer thường đi kèm với số lượng lớn tham số và chi phí tính toán và luôn là thách thức lớn để đạt được sự cân bằng tối ưu giữa độ chính xác và độ phức tạp của mô hình. Trong công trình này, chúng tôi được thúc đẩy bởi câu hỏi sau: Làm thế nào để cải thiện việc sử dụng tham số của vision transformer, tức là khả năng biểu diễn mà không tăng kích thước mô hình? Chúng tôi quan sát thấy phép toán đệ quy, như được thể hiện trong Hình 2, là một cách đơn giản nhưng hiệu quả để đạt được mục đích này. Các mô hình vision transformer dựa trên đệ quy của chúng tôi vượt trội đáng kể so với các phương pháp tiên tiến trong khi chứa ít tham số và FLOP hơn, như được minh họa trong Hình 1.

Về bản chất, bộ phân loại yêu cầu các đặc trưng trừu tượng cấp cao từ mạng nơ-ron để thực hiện phân loại chính xác, trong khi việc trích xuất các đặc trưng này thường yêu cầu nhiều lớp và mạng sâu hơn. Điều này giới thiệu chi phí tham số vào mô hình. Động lực của công trình này xuất phát từ một hiện tượng thú vị của việc trực quan hóa biểu diễn tiềm ẩn. Chúng tôi quan sát thấy rằng trong mạng vision transformer sâu, trọng số và kích hoạt của các lớp liền kề tương tự với không nhiều khác biệt (một hiện tượng tương tự cũng được phát hiện trong), có nghĩa là chúng có thể được tái sử dụng. Transformer với một ngăn xếp cố định các lớp riêng biệt mất đi thiên kiến quy nạp trong mạng nơ-ron hồi tiếp điều này truyền cảm hứng cho chúng tôi chia sẻ những trọng số đó theo cách đệ quy, tạo thành vision transformer lặp đi lặp lại hoặc đệ quy. Đệ quy có thể giúp trích xuất các đặc trưng mạnh hơn mà không cần tăng tham số, và cải thiện thêm độ chính xác. Ngoài ra, chiến lược tái sử dụng hoặc chia sẻ trọng số này một phần điều chỉnh quá trình huấn luyện bằng cách giảm số lượng tham số để tránh overfitting và thách thức hội tụ kém, điều này sẽ được thảo luận trong các phần sau.

Tại sao chúng ta cần giới thiệu đệ quy cắt khúc, tức là self-attention nhóm, vào transformer? (ưu điểm và nhược điểm) Chúng ta thường hướng tới sự hoàn hảo trong việc sử dụng trọng số của mạng trong phạm vi tham số bị giới hạn, do đó, nó có thể được sử dụng thực tế trong các tình huống hạn chế tài nguyên như thiết bị nhúng. Đệ quy là một cách đơn giản để nén biểu diễn đặc trưng trong một sơ đồ tuần hoàn. Các mạng nơ-ron đệ quy cũng cho phép phân nhánh các kết nối và cấu trúc với thứ bậc. Chúng tôi thấy rằng nó rất quan trọng cho việc học các biểu diễn tốt hơn trên dữ liệu thị giác theo cách phân cấp, như chúng tôi sẽ giới thiệu trong Hình 10 của các thí nghiệm. Ngoài ra, ngay cả phép toán đệ quy đơn giản nhất vẫn cải thiện tính nhỏ gọn của việc sử dụng tham số mà không yêu cầu sửa đổi cấu trúc khối transformer, không như những phương pháp khác, thêm nhiều tham số hoặc liên quan đến thông tin chi tiết bổ sung từ đầu vào. Tuy nhiên, đệ quy như vậy sẽ phát sinh thêm chi phí tính toán bởi các vòng lặp của nó, cụ thể là, nó hy sinh hiệu quả thực thi để sử dụng biểu diễn tham số tốt hơn. Để giải quyết nhược điểm này, chúng tôi đề xuất một phương pháp xấp xỉ cho self-attention toàn cục thông qua phân tách thành nhiều self-attention nhóm cắt khúc qua các lớp đệ quy, đồng thời, tận hưởng FLOP tương tự và biểu diễn tốt hơn, chúng tôi cũng áp dụng thiết kế kim tự tháp không gian để giảm độ phức tạp của mạng.

Mạng Feedforward, Mạng Nơ-ron Hồi tiếp và Mạng Nơ-ron Đệ quy. Mạng feedforward, như CNN và transformer, là đồ thị có hướng không chu trình (DAG), do đó đường dẫn thông tin trong quá trình xử lý feedforward là một chiều. Mạng hồi tiếp (RNN) thường được phát triển để xử lý chuỗi thời gian và dữ liệu tuần tự khác, và dự đoán sử dụng đầu vào hiện tại và bộ nhớ quá khứ. Mạng đệ quy là một thuật ngữ ít được sử dụng so với hai loại tương ứng kia. Đệ quy đề cập đến việc lặp lại hoặc tái sử dụng một phần nhất định của mạng. Khác với RNN lặp lại cùng một khối trong toàn bộ mạng, mạng đệ quy lựa chọn lặp lại các khối quan trọng cho các mục đích cụ thể. Transformer đệ quy lặp đi lặp lại tinh chỉnh các biểu diễn của nó cho tất cả các patch trong chuỗi. Chúng tôi thấy rằng, thông qua đệ quy được thiết kế vào transformer feedforward, chúng ta có thể nâng cao đáng kể biểu diễn đặc trưng đặc biệt cho dữ liệu có cấu trúc mà không bao gồm tham số bổ sung.

Kết quả thí nghiệm mạnh mẽ cho thấy rằng việc tích hợp phép toán đệ quy cắt khúc được đề xuất trong transformer đạt được sự cân bằng cạnh tranh giữa độ chính xác, kích thước mô hình và độ phức tạp. Theo hiểu biết tốt nhất của chúng tôi, hiếm có công trình hiện tại nghiên cứu hiệu quả của phép toán đệ quy trong vision transformer và đề xuất xấp xỉ của phương pháp self-attention để giảm độ phức tạp của phép toán đệ quy. Chúng tôi đã thực hiện các thí nghiệm mở rộng để rút ra một tập hợp hướng dẫn cho thiết kế mới trên tác vụ thị giác, và hy vọng nó hữu ích cho nghiên cứu tương lai. Hơn nữa, vì phương pháp của chúng tôi không liên quan đến kiến thức tinh vi để sửa đổi khối transformer hoặc thông tin đầu vào bổ sung, nó trực giao và thân thiện với hầu hết các thiết kế và phương pháp ViT hiện có.

Đóng góp của chúng tôi.
- Chúng tôi điều tra tính khả thi của việc tận dụng phép toán đệ quy với self-attention nhóm cắt khúc trong vision transformer, đây là một hướng đầy hứa hẹn để thiết lập transformer hiệu quả và chưa được khám phá kỹ trước đây. Chúng tôi đã tiến hành nghiên cứu sâu về vai trò của đệ quy trong transformer và kết luận một sơ đồ hiệu quả để sử dụng chúng để sử dụng tham số tốt hơn.
- Chúng tôi cung cấp các nguyên tắc thiết kế, bao gồm định dạng cụ thể và so sánh toàn diện với các biến thể của kiến trúc SReT, phân tích tương đương tính toán, chưng cất sửa đổi, v.v., với hy vọng làm sáng tỏ các nghiên cứu tương lai trong thiết kế transformer nhỏ gọn và tối ưu hóa.
- Chúng tôi xác minh phương pháp của mình qua nhiều kịch bản khác nhau, bao gồm vision transformer, kiến trúc all-MLP của biến thể transformer, và dịch máy nơ-ron (NMT) sử dụng transformer. Mô hình của chúng tôi vượt trội so với các phương pháp tiên tiến một cách đáng kể với ít tham số hơn.

2 Công trình liên quan

(i) Transformer ban đầu được thiết kế cho các tác vụ xử lý ngôn ngữ tự nhiên và đã trở thành phương pháp chủ đạo trong lĩnh vực này. Gần đây, Vision Transformer (ViT) chứng minh rằng các khối multi-head self attention như vậy có thể thay thế hoàn toàn các phép tích chập và đạt được hiệu suất cạnh tranh trên phân loại hình ảnh. Trong khi nó dựa vào pre-training trên lượng lớn dữ liệu và chuyển đổi sang các tập dữ liệu downstream. DeiT khám phá các chiến lược huấn luyện và tăng cường dữ liệu khác nhau trên các mô hình ViT, để huấn luyện chúng trên ImageNet-1K trực tiếp. Về cơ bản, DeiT có thể được coi là một framework của backbone ViT + tăng cường dữ liệu lớn + điều chỉnh siêu tham số + chưng cất cứng với token. Sau đó, nhiều mở rộng và biến thể của mô hình ViT đã xuất hiện trên tác vụ phân loại hình ảnh, như Bottleneck Transformer, Multimodal Transformer, Tokens-to-Token Transformer, Spatial Pyramid Transformer, Class-Attention Transformer, Transformer in Transformer, Convolution Transformer, Shifted Windows Transformer, Co-Scale Conv-Attentional Transformer, v.v. (ii) Phép toán đệ quy đã được khám phá trong các lĩnh vực NLP và thị giác. Đặc biệt, DEQ đề xuất tìm điểm cân bằng thông qua root-finding trong các mô hình feedforward weight-tied như transformer và trellis cho bộ nhớ không đổi. UT trình bày transformer với thiên kiến quy nạp hồi tiếp của RNN tương tự như định dạng SReT của chúng tôi. Tuy nhiên, các công trình này bỏ qua độ phức tạp tăng lên bởi phép toán đệ quy trong thiết kế mạng. Trong bài báo này, chúng tôi tập trung vào việc sử dụng đệ quy đúng cách bằng cách xấp xỉ self-attention thông qua nhiều self-attention nhóm để xây dựng vision transformer nhỏ gọn và hiệu quả.

3 Recursive Transformer

Khối Transformer Vanilla. Một khối transformer cơ bản F gồm Multi-head Self Attention (MHSA), Layer Normalization (LN), Feed-forward Network (FFN), và Residual Connections (RC). Nó có thể được công thức hóa như:

z'ₗ = MHSA(LN(zₗ₋₁)) + zₗ₋₁; zₗ = FFN(LN(z'ₗ)) + z'ₗ; tức là; zₗ = Fₗ₋₁(zₗ₋₁)

trong đó z'ₗ và zₗ₋₁ là các biểu diễn trung gian. Fₗ chỉ khối transformer tại lớp thứ ℓ. ℓ ∈ {0,1,...,L} là chỉ số lớp và L là số lớp ẩn. Module self-attention được thực hiện bởi các tích vô hướng với một hệ số tỷ lệ và một phép toán softmax, được viết như:

Attention(Q,K,V) = Softmax(QK^T/√dₖ)V

trong đó Q,K,V là các vector query, key và value tương ứng. 1/√dₖ là hệ số tỷ lệ để chuẩn hóa. Multi-head self attention tiếp tục nối các lớp attention song song để tăng khả năng biểu diễn:

MHSA(Q,K,V) = Concat(head₁,...,headₕ)W^O, trong đó W^O ∈ R^(hd_v×d_model).

headᵢ = Attention(QW^Q_i, KW^K_i, VW^V_i)

là các phép chiếu với các ma trận tham số W^Q_i ∈ R^(d_model×dₖ), W^K_i ∈ R^(d_model×dₖ), W^V_i ∈ R^(d_model×d_v). FFN chứa hai lớp tuyến tính với một phi tuyến GELU ở giữa

FFN(x) = (GELU(zW₁ + b₁))W₂ + b₂

trong đó z là đầu vào. W₁,b₁,W₂,b₂ là trọng số và bias của hai lớp tuyến tính.

Phép toán Đệ quy. Trong module đệ quy ban đầu cho phương thức ngôn ngữ, các trọng số chia sẻ được áp dụng đệ quy trên một đầu vào có cấu trúc nằm trong các chuỗi phức tạp vốn có, do đó nó có khả năng học kiến thức có cấu trúc sâu. Mạng nơ-ron đệ quy được tạo từ dữ liệu kiến trúc và lớp, chủ yếu được đề xuất cho tính hợp thành mô hình trên các tác vụ NLP. Ở đây, chúng tôi vẫn sử dụng chuỗi các patch token từ hình ảnh làm đầu vào theo mô hình ViT. Và, không có đầu vào bổ sung nào được sử dụng để đưa vào mỗi vòng lặp đệ quy của khối đệ quy như được sử dụng trên dữ liệu có cấu trúc. Lấy hai vòng lặp làm ví dụ để xây dựng mạng, phép toán đệ quy có thể được đơn giản hóa:

zₗ = Fₗ₋₁(Fₗ₋₁(zₗ₋₁))

Phép toán đệ quy ngây thơ có xu hướng học một giải pháp đơn giản và tầm thường như ánh xạ đồng nhất bởi bộ tối ưu, vì đầu ra và đầu vào của Fₗ₋₁ giống hệt nhau tại hai độ sâu liền kề (các lớp).

Lớp Chiếu Phi tuyến (NLL). NLL được đặt giữa hai phép toán đệ quy để cho phép biến đổi phi tuyến giữa đầu ra và đầu vào của mỗi khối, để tránh học trạng thái tầm thường cho những khối đệ quy này bằng cách buộc sự không tương đương trên đầu ra và đầu vào lân cận. NLL có thể được công thức hóa như:

NLL(zₗ₋₁) = MLP(LN(z'ₗ₋₁)) + z'ₗ₋₁

trong đó MLP là một phép chiếu đa lớp như FFN, nhưng có tỷ lệ mlp khác cho các đặc trưng ẩn. Chúng tôi cũng sử dụng kết nối dư trong đó để biểu diễn tốt hơn. Như được thể hiện trong Bảng 1, nhiều đệ quy hơn sẽ không cải thiện độ chính xác mà không có NLL.

Recursive Transformer. Một recursive transformer với hai vòng lặp trong mỗi khối là:

zₗ = NLL₂(Fₗ₋₁(NLL₁(Fₗ₋₁(zₗ₋₁))))

trong đó zₗ₋₁ và zₗ là đầu vào và đầu ra của mỗi khối đệ quy. Khác với MHSA và FFN chia sẻ tham số qua tất cả các phép toán đệ quy trong một khối, NLL₁ và NLL₂ sử dụng trọng số không chia sẻ độc lập bất kể vị trí trong hoặc ngoài các khối đệ quy.

Recursive All-MLP (một mở rộng). Chúng ta có thể công thức hóa nó như:

U;,i = X;,i + W₂GELU(W₁LN(X);,i);
Yⱼ; = Uⱼ; + W₄GELU(W₃LN(U)ⱼ;);
Yⱼ; = Mₗ₋₁(Mₗ₋₁(X;,i))

trong đó dòng thứ nhất và thứ hai là token-mixing và channel-mixing từ. Mₗ₋₁ là một khối MLP, C là chiều ẩn và S là số lượng patch hình ảnh không chồng chéo. NLL không được sử dụng ở đây để đơn giản.

Gradient trong Một Khối Đệ quy. Ở đây, chúng tôi đơn giản sử dụng lan truyền ngược rõ ràng thông qua các phép toán chính xác trong lượt truyền tiến như phương pháp gradient descent vì SReT không có ràng buộc để có được cân bằng đầu vào-đầu ra trong các đệ quy như DEQ và số lượng vòng lặp có thể nhỏ để kiểm soát tính toán và độ sâu của mạng. Lượt truyền ngược của chúng tôi giống như UT hơn. Nói chung, gradient của các tham số trong mỗi khối đệ quy có thể là:

∂L/∂W_F = ∂L/∂z_N ∂z_N/∂W_F + ∂L/∂z_N ∂z_N/∂z_{N-1} ∂z_{N-1}/∂W_F + ⋯ ∂L/∂z_N ∂z_N/∂z_{N-1} ⋯ ∂z_2/∂z_1 ∂z_1/∂W_F
= ∑_{i=1}^N ∂L/∂z_N (∏_{j=i}^{N-1} ∂z_{j+1}/∂z_j) ∂z_i/∂W_F

trong đó W_F là các tham số của khối đệ quy. L là hàm mục tiêu.

Kết nối Dư Có thể Học (LRC) cho Recursive Vision Transformers. He et al. nghiên cứu các chiến lược khác nhau của kết nối tắt trên CNN và thấy rằng thiết kế dư ban đầu với pre-activation hoạt động tốt nhất. Ở đây, chúng tôi thấy đơn giản thêm các hệ số có thể học trên mỗi nhánh của kết nối dư có thể có lợi cho hiệu suất của ViT theo khám phá tương tự của tài liệu. Chính thức, Eq. 1 và Eq. 5 có thể được công thức hóa lại như:

z'ₗ = α·MHSA(LN(zₗ₋₁)) + β·zₗ₋₁;
zₗ = γ·FFN(LN(z'ₗ)) + δ·z'ₗ;

NLL(zₗ₋₁) = ζ·MLP(LN(z'ₗ₋₁)) + η·z'ₗ₋₁

trong đó α,β,γ,δ,ζ,η là các hệ số có thể học. Chúng được khởi tạo là 1 và được huấn luyện với các tham số khác của mô hình đồng thời mà không có hạn chế.

Transformer Cực Sâu. Cơ chế chia sẻ trọng số cho phép chúng ta xây dựng transformer với hơn 100 lớp vẫn giữ mô hình nhỏ. Chúng tôi chứng minh thực nghiệm rằng phương pháp được đề xuất có thể đơn giản hóa đáng kể việc tối ưu hóa khi transformer được mở rộng lên số lượng lớp cường điệu.

4 Xấp xỉ Global MHSA thông qua Multi-Group MHSA

Mặc dù phép toán đệ quy đủ để cung cấp biểu diễn tốt hơn sử dụng cùng số lượng tham số, vòng lặp truyền tiến bổ sung làm cho chi phí trong huấn luyện và suy luận tăng lên đáng kể. Để giải quyết chi phí tính toán bổ sung gây ra bởi đệ quy trong khi duy trì độ chính xác được cải thiện, chúng tôi giới thiệu một phương pháp xấp xỉ thông qua nhiều self-attention nhóm thật sự hiệu quả trong việc giảm FLOP mà không ảnh hưởng đến độ chính xác.

Xấp xỉ Global Self-Attention trong SReT. Như được thể hiện trong Hình 3, một lớp self-attention thông thường có thể được tách rời thông qua nhiều self-attention nhóm theo cách đệ quy với chi phí tính toán tương tự hoặc thậm chí nhỏ hơn. Nói chung, số lượng nhóm trong các đệ quy khác nhau có thể giống nhau hoặc khác nhau tùy thuộc vào yêu cầu của sự cân bằng FLOP và độ chính xác. Chiến lược như vậy sẽ không thay đổi số lượng tham số trong khi nhiều nhóm hơn có thể tận hưởng FLOP thấp hơn nhưng hiệu suất hơi kém hơn. Chúng tôi xác minh thực nghiệm rằng sơ đồ tách rời có thể đạt được hiệu suất tương tự với FLOP ít hơn đáng kể nếu sử dụng việc chia tách self-attention thích hợp trong phạm vi có thể chấp nhận, như được thể hiện trong Phụ lục.

Phân tích Tương đương Tính toán. Trong phần này, chúng tôi phân tích độ phức tạp của self-attention toàn cục (tức là, ban đầu) và self-attention nhóm cắt khúc và so sánh với các giá trị khác nhau của nhóm trong vision transformer.

Định lý 1. (Tương đương của self-attention toàn cục và self-attention nhóm với đệ quy trên FLOP.) Cho {Nₗ,Gₗ} ∈ R¹, khi Nₗ = Gₗ, FLOP(1 V-SA) = FLOP(Nₗ Đệ quy với Gₗ G-SA). Độ phức tạp C của self-attention toàn cục và nhóm có thể được tính như: (Để đơn giản, ở đây chúng tôi giả định #nhóm và chiều vector trong mỗi phép toán đệ quy là giống nhau.)

C_{G-SA} = (Nₗ/Gₗ)C_{V-SA}

trong đó Nₗ và Gₗ là số lượng đệ quy và MHSA nhóm trong lớp ℓ, tức là khối đệ quy thứ ℓ. V-SA và G-SA đại diện cho MHSA vanilla và nhóm.

Chứng minh được cung cấp trong Phụ lục. Hiểu biết do Định lý 1 cung cấp là cốt lõi của phương pháp chúng tôi để kiểm soát độ phức tạp và các lợi ích khác nhau của nó về biểu diễn tốt hơn. Quan trọng, việc tính toán self-attention thông qua "cắt khúc" song song bằng self-attention vanilla. Chúng ta có thể quan sát rằng khi Nₗ = Gₗ, C_{V-SA} ≈ C_{G-SA} và nếu Nₗ < Gₗ, C_{G-SA} < C_{V-SA}, chúng ta có thể sử dụng thuộc tính này để giảm FLOP trong thiết kế ViT.

Quan sát thực nghiệm: Khi FLOP(đệ quy + G-SA) ≈ FLOP(V-SA), Acc.(đệ quy + G-SA) > Acc.(V-SA).

Chúng tôi sử dụng mô hình ex-tiny để đánh giá hiệu suất của self-attention toàn cục và self-attention nhóm cắt khúc với đệ quy. Như được thể hiện trong Bảng 2, chúng tôi xác minh thực nghiệm rằng, với tính toán tương tự, self-attention nhóm với đệ quy có thể đạt được độ chính xác tốt hơn so với self-attention vanilla.

Phân tích: Lợi ích từ đâu trong SReT? Phân tích lý thuyết về đệ quy có thể giúp hiểu thêm về lợi thế đằng sau, trong khi nó khó khăn và tài liệu trước đây về điều này luôn chứng minh nó thực nghiệm. Ở đây, chúng tôi cung cấp một số giải thích lý thuyết cơ bản từ góc độ tối ưu hóa để hiểu rõ hơn về phương pháp này. Một là tích lũy gradient được tăng cường. Cho gₜ = ∇fₜ(θ) biểu thị gradient, chúng tôi lấy bộ tối ưu Adam làm ví dụ, cập nhật tham số ngây thơ là θₜ ← θₜ₋₁ - η·m̂ₜ/√v̂ₜ + ε trong đó gradient w.r.t. mục tiêu ngẫu nhiên tại timestep t là gₜ ← ∇fₜ(θₜ₋₁), ở đây chúng tôi bỏ qua công thức ước lượng moment thứ nhất và thứ hai. Sau khi liên quan đến đệ quy (ở đây NLL đảm bảo sự khác biệt m̂ᵢₜ,v̂ᵢₜ), việc cập nhật mới là: θₜ ← θₜ₋₁ - η·∑ᵢ₌₁ᴺ m̂ᵢₜ/√v̂ᵢₜ + ε trong đó N là số lượng vòng lặp đệ quy. Về cơ bản, đệ quy cho phép cập nhật/điều chỉnh nhiều tham số hơn trong cùng một lần lặp, do đó các trọng số đã học phù hợp hơn với hàm mất mát, và hiệu suất tự nhiên tốt hơn.

5 Thí nghiệm

Trong phần này, trước tiên chúng tôi xác minh thực nghiệm SReT được đề xuất trên tác vụ phân loại hình ảnh với kiến trúc self-attention và all-MLP tương ứng. Chúng tôi cũng thực hiện các nghiên cứu ablation chi tiết để khám phá các siêu tham số tối ưu của mạng được đề xuất. Sau đó, chúng tôi mở rộng nó đến tác vụ dịch máy nơ-ron (NMT) để xác minh thêm khả năng tổng quát của phương pháp được đề xuất. Cuối cùng, chúng tôi trực quan hóa sự tiến hóa của các hệ số đã học trong LRC và bản đồ kích hoạt trung gian để hiểu rõ hơn về hành vi và thuộc tính của mô hình được đề xuất. Các thí nghiệm của chúng tôi được tiến hành trên cluster CIAI.

5.1 Tập dữ liệu và Cài đặt Thí nghiệm

(i) ImageNet-1K: ImageNet-1K là tập dữ liệu phân loại hình ảnh tiêu chuẩn, chứa 1K lớp với tổng số 1.2 triệu hình ảnh huấn luyện và 50K hình ảnh xác thực. Các mô hình của chúng tôi được huấn luyện trên tập dữ liệu này một mình mà không có hình ảnh bổ sung; (ii) IWSLT'14 German to English (De-En) dataset: Nó chứa khoảng 160K cặp câu làm tập huấn luyện. Chúng tôi huấn luyện và đánh giá mô hình theo giao thức; (iii) WMT'14 English to German (En-De) dataset: Dữ liệu huấn luyện WMT'14 bao gồm 4.5M cặp câu (116M từ tiếng Anh, 110M từ tiếng Đức). Chúng tôi sử dụng cùng thiết lập như.

Cài đặt: Các cài đặt huấn luyện chi tiết và siêu tham số của chúng tôi được thể hiện trong Phụ lục. Trên ImageNet-1K, mạng backbone của chúng tôi là kiến trúc kim tự tháp không gian với cấu trúc stem theo.

Chiến lược chưng cất mềm. Trên vision transformer, DeiT đề xuất chưng cất token cùng với dự đoán cứng từ giáo viên. Họ tuyên bố rằng sử dụng nhãn one-hot với chưng cất cứng có thể đạt được độ chính xác tốt nhất. Điều này có vẻ phản trực giác vì nhãn mềm có thể cung cấp sự khác biệt tinh tế hơn và thông tin chi tiết của đầu vào. Trong công trình này, thông qua thiết kế chưng cất thích hợp, framework chưng cất dựa trên nhãn mềm của chúng tôi (không sử dụng nhãn one-hot) liên tục đạt được hiệu suất tốt hơn so với DeiT. Mất mát của chúng tôi là phiên bản mềm của cross-entropy giữa đầu ra của giáo viên và học sinh như được sử dụng trong:

L_{CE}(SW) = -1/N ∑ᵢ₌₁ᴺ P_{TW}(z) log P_{SW}(z); trong đó P_{TW} và P_{SW} là đầu ra của giáo viên và học sinh tương ứng. Chi tiết hơn có thể tham khảo Phụ lục.

5.2 Đệ quy Ngây thơ trên Transformer

Trong phần này, chúng tôi kiểm tra hiệu quả của đệ quy được đề xuất sử dụng chiến lược huấn luyện DeiT. Chúng tôi xác minh hai kiểu đệ quy sau đây.

Vòng lặp nội bộ và bên ngoài. Như được minh họa trong Hình 4, có hai thiết kế đệ quy có thể trên mạng transformer. Một là vòng lặp nội bộ lặp lại từng khối riêng biệt. Một cái khác là vòng lặp bên ngoài thực thi tất cả các khối cùng nhau theo chu kỳ. Mặc dù thiết kế vòng lặp bên ngoài có thể buộc mô hình nhỏ gọn hơn vì nó chia sẻ tham số qua tất cả các khối với ít lớp NLL không chia sẻ hơn, chúng tôi thấy cấu trúc như vậy không linh hoạt với khả năng biểu diễn hạn chế. Chúng tôi đã tiến hành so sánh với 12 lớp transformer cơ bản với 2 phép toán đệ quy và kết quả là: bên ngoài 67.0% (3.2M) so với nội bộ 67.6% (3.0M) | 70.3% (3.9M). Trong các thí nghiệm sau, chúng tôi sử dụng thiết kế đệ quy nội bộ làm cài đặt mặc định.

5.3 Nghiên cứu Ablation

Tổng quan về các nghiên cứu ablation của chúng tôi được thể hiện trong Bảng 3. Hàng đầu tiên trình bày baseline, nhóm thứ hai là các cấu trúc khác nhau được chỉ ra bởi các yếu tố đã sử dụng. Cuối cùng là so sánh KD. Chúng tôi cũng xác minh các thiết kế sau.

Cấu hình kiến trúc. Như trong Bảng 5, SReT-T là mô hình tiny của chúng tôi có tỷ lệ mlp = 3.6 trong FFN và 4.0 cho SReT-TL. Chi tiết hơn về các kiến trúc này được cung cấp trong Phụ lục của chúng tôi. Để kiểm tra hiệu quả của phép toán đệ quy, chúng tôi tiến hành các vòng lặp khác nhau của đệ quy ngây thơ trên DeiT-T. Kết quả của các đường cong độ chính xác trên dữ liệu xác thực được thể hiện trong Hình 5 (1), chúng ta có thể thấy 2 hơi tốt hơn 1 và sự tăng thêm là nhỏ, trong khi 1 nhanh hơn nhiều để thực thi. Do đó, chúng tôi sử dụng điều này trong các thí nghiệm sau.

Cấu hình NLL. NLL là một yếu tố quan trọng cho kích thước và hiệu suất vì các trọng số trong đó không được chia sẻ. Để tìm sự cân bằng tối ưu giữa tính nhỏ gọn của mô hình và độ chính xác, chúng tôi khám phá các tỷ lệ NLL trong Hình 5 (2, 3). Nói chung, tỷ lệ NLL lớn hơn có thể đạt được hiệu suất tốt hơn nhưng kích thước mô hình tăng tương ứng. Chúng tôi sử dụng 1.0 trong SReT-T và SReT-TL, và 2.0 trong SReT-S.

Các thiết kế hoán vị khác nhau và số lượng nhóm. Chúng tôi khám phá các thiết kế hoán vị khác nhau và nguyên tắc chọn số lượng nhóm để có sự cân bằng tốt hơn giữa độ chính xác và FLOP. Chúng tôi đề xuất chèn các lớp hoán vị và hoán vị ngược để bảo tồn thông tin thứ tự của đầu vào sau phép toán self-attention nhóm cắt khúc. Công thức chi tiết của module này, cùng với các đệ quy và phân tích kết quả của chúng được đưa ra trong Phụ lục của chúng tôi.

Chưng cất. Để kiểm tra hiệu quả của phương pháp chưng cất mềm được đề xuất, chúng tôi tiến hành so sánh nhãn one-hot + chưng cất cứng và chỉ chưng cất mềm. Mạng backbone là SReT-T, tất cả siêu tham số giống nhau ngoại trừ các hàm mất mát. Các đường cong độ chính xác được thể hiện trong Phụ lục của chúng tôi. Kết quả 77.7% của chúng tôi tốt hơn đáng kể so với baseline 77.1%.

Đánh giá thông lượng. Trong Bảng 4, chúng tôi cung cấp so sánh thông lượng với DeiT và Swin trên một NVIDIA GeForce RTX 3090 có thể phản ánh trực tiếp tốc độ suy luận thực và tiêu thụ thời gian. Chúng tôi nhấn mạnh rằng phương pháp của chúng tôi đạt được ít tham số và FLOP hơn đáng kể với thông lượng tốt hơn.

5.4 So sánh với Các Phương pháp Tiên tiến

Tóm tắt các kết quả chính của chúng tôi được thể hiện trong Bảng 5, SReT-ExT của chúng tôi tốt hơn PiT-T 1.0% với 18.4% #tham số. SReT-T cũng vượt trội DeiT-T 3.8% với 15.8% #tham số và 15.4% #FLOP. Chưng cất có thể giúp cải thiện độ chính xác 1.6% và fine-tuning trên độ phân giải lớn tăng thêm lên 79.6%. Hơn nữa, SReT-S của chúng tôi liên tục tốt hơn Swin-T, T2T tiên tiến, v.v., về độ chính xác, kích thước mô hình và FLOP, điều này chứng minh tính ưu việt và tiềm năng của kiến trúc của chúng tôi trong thực tế.

5.5 Kiến trúc All-MLP

MLP-Mixer (Baseline), MLP-Mixer+Đệ quy và MLP-Mixer+Đệ quy+LRC: Mixer là một thiết kế đơn giản được đề xuất gần đây dựa hoàn toàn trên perceptron đa lớp (MLP). Chúng tôi áp dụng phép toán đệ quy và LRC của chúng tôi trên MLP-Mixer để xác minh khả năng tổng quát của chúng. Kết quả được thể hiện trong Hình 6 (1), phương pháp của chúng tôi liên tục tốt hơn baseline sử dụng cùng giao thức huấn luyện.

5.6 Dịch Máy Nơ-ron

Trong phần này, chúng tôi so sánh điểm BLEU của vanilla transformer và của chúng tôi trên WMT14 En-De và IWSLT'14 De-En (Phụ lục) sử dụng toolkit fairseq. IWSLT'14 De-En là tập dữ liệu tương đối nhỏ nên cải thiện không đáng kể như trên WMT14 En-De. Kết quả được thể hiện trong Hình 7, chúng ta có thể thấy phương pháp của chúng tôi tốt hơn đáng kể so với baseline. Không có LRC, mô hình hội tụ hơi nhanh hơn, nhưng độ chính xác cuối cùng kém hơn so với sử dụng LRC. Ngoài ra, LRC làm cho quá trình huấn luyện ổn định hơn, như được thể hiện trong hộp nét đứt màu đỏ.

5.7 Trực quan hóa Landscape của DeiT và SReT Độ sâu hỗn hợp của chúng tôi

Huấn luyện độ sâu hỗn hợp rõ ràng. Mạng nơ-ron đệ quy cho phép huấn luyện mô hình theo sơ đồ độ sâu hỗn hợp. Như được thể hiện trong Hình 8 (d), nhánh bên trái là mạng con chứa các khối đệ quy, trong khi bên phải là các khối không chia sẻ trọng số theo độ sâu, nhưng trọng số của chúng được tái sử dụng với nhánh bên trái. Trong cấu trúc này, hai nhánh lấy đầu vào từ cùng một khối stem. Huấn luyện độ sâu hỗn hợp cung cấp tối ưu hóa đơn giản bằng cách thực hiện các phép toán song song và ngăn chặn tối ưu hóa kém khi mạng cực kỳ sâu.

Lợi ích của huấn luyện độ sâu hỗn hợp. Lợi ích phụ của đệ quy cắt khúc là tính khả thi của huấn luyện độ sâu hỗn hợp, về bản chất là một sơ đồ giám sát sâu rõ ràng vì nhánh nông nhận giám sát mạnh hơn gần lớp mất mát cuối cùng hơn, đồng thời, trọng số được chia sẻ với nhánh sâu. Lấy cảm hứng từ, chúng tôi trực quan hóa landscape của baseline DeiT-108 và các mô hình SReT-108 & SReT-108 độ sâu hỗn hợp của chúng tôi để kiểm tra và phân tích khó khăn của tối ưu hóa trên ba kiến trúc này. Kết quả được minh họa trong Hình 9, chúng ta có thể quan sát rằng DeiT-108 hỗn loạn hơn và khó tối ưu hóa hơn với minimum địa phương sâu hơn so với mạng độ sâu hỗn hợp của chúng tôi. Điều này xác minh lợi thế của cấu trúc mạng được đề xuất cho tối ưu hóa đơn giản hơn.

5.8 Phân tích và Hiểu biết

Ở đây, chúng tôi cung cấp hai trực quan hóa liên quan đến LRC và bản đồ phản hồi đã học.

Tiến hóa của các hệ số LRC. Như được thể hiện trong Hình 6 (2), chúng tôi vẽ sự tiến hóa của các hệ số đã học trong khối đầu tiên. Chúng ta có thể quan sát rằng các hệ số trên ánh xạ đồng nhất (α,β,γ) đầu tiên tăng lên rồi giảm xuống khi huấn luyện tiếp tục. Hiện tượng này cho thấy rằng, ở đầu quá trình huấn luyện mô hình, ánh xạ đồng nhất đóng vai trò chính trong các biểu diễn. Sau 50 epoch huấn luyện, nhánh chính trở nên ngày càng quan trọng. Khi huấn luyện hoàn tất, trong FFN và NLL, nhánh chính vượt quá nhánh kết nối dư trong khi trên MHSA thì ngược lại. Chúng tôi tin rằng hiện tượng này có thể truyền cảm hứng cho chúng ta thiết kế cấu trúc kết nối dư hợp lý hơn trong ViT.

Bản đồ phản hồi đã học. Chúng tôi trực quan hóa các bản đồ kích hoạt của mô hình DeiT-T và SReT-T của chúng tôi ở các lớp nông và sâu. Như được thể hiện trong Hình 10, DeiT là mạng với độ phân giải đồng nhất của bản đồ đặc trưng (14×14). Trong khi, cấu trúc kim tự tháp không gian của chúng tôi có các kích thước khác nhau của bản đồ đặc trưng cùng với độ sâu của mạng, tức là, độ phân giải của bản đồ đặc trưng giảm khi độ sâu tăng. Các quan sát thú vị hơn được thảo luận trong Phụ lục.

6 Kết luận

Đáng để xem xét làm thế nào để cải thiện hiệu quả sử dụng tham số cho vision transformer với chi phí tối thiểu. Trong công trình này, chúng tôi đã tóm tắt và giải thích một số hành vi được quan sát trong khi huấn luyện các mạng như vậy. Chúng tôi tập trung vào xây dựng vision transformer hiệu quả với kích thước mô hình nhỏ gọn thông qua phép toán đệ quy, và phương pháp xấp xỉ self-attention nhóm được đề xuất cho phép chúng ta huấn luyện theo cách hiệu quả hơn với các transformer đệ quy. Chúng tôi nhấn mạnh sơ đồ huấn luyện như vậy chưa được khám phá kỹ trong tài liệu trước đây. Chúng tôi quy cho hiệu suất vượt trội của sliced recursive transformer vào khả năng tăng cường chất lượng biểu diễn của các đặc trưng trung gian. Chúng tôi đã tiến hành các thí nghiệm toàn diện để thiết lập thành công của phương pháp trên các tác vụ phân loại hình ảnh và dịch máy nơ-ron, không chỉ xác minh nó trong lĩnh vực thị giác, mà còn chứng minh khả năng tổng quát cho nhiều phương thức và kiến trúc, như MLP-Mixer.
