# 2111.05297.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/recursive/2111.05297.pdf
# File size: 5764855 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sliced Recursive Transformer
Zhiqiang Shen1;2;3, Zechun Liu2;4, and Eric Xing1;3
1Carnegie Mellon University, Pittsburgh, USA
2Hong Kong University of Science and Technology, Hong Kong, China
3Mohamed bin Zayed University of Articial Intelligence, Abu Dhabi, UAE
4Reality Labs, Meta Inc.
zhiqiangshen@cse.ust.hk,zechunliu@fb.com,epxing@cs.cmu.edu
Abstract. We present a neat yet eective recursive operation on vision
transformers that can improve parameter utilization without involving
additional parameters. This is achieved by sharing weights across depth
of transformer networks. The proposed method can obtain a substantial
gain (2%) simply using na ve recursive operation, requires no special
or sophisticated knowledge for designing principles of networks, and in-
troduces minimal computational overhead to the training procedure. To
reduce the additional computation caused by recursive operation while
maintaining the superior accuracy, we propose an approximating method
through multiple sliced group self-attentions across recursive layers which
can reduce the cost consumption by 10 30% without sacricing perfor-
mance. We call our model SlicedRecursive Transformer (SReT), a novel
and parameter-ecient vision transformer design that is compatible with
a broad range of other designs for ecient ViT architectures. Our best
model establishes signicant improvement on ImageNet-1K over state-of-
the-art methods while containing fewer parameters. The proposed weight
sharing mechanism by sliced recursion structure allows us to build a
transformer with more than 100 or even 1000 shared layers with ease
while keeping a compact size (13 15M), to avoid optimization dicul-
ties when the model is too large. The exible scalability has shown great
potential for scaling up models and constructing extremely deep vision
transformers. Code is available at https://github.com/szq0214/SReT .
1 Introduction
20 40 60 80
Params (M)72747678808284T op-1 (%) Accuracy
SReT (Ours)
Swin
CoaT-Lite
DeiT
PiT
TNT
T2T
AutoFormer
PVT
0.0 2.5 5.0 7.5 10.0 12.5 15.0
#FLOPs (B)747678808284T op-1 (%) Accuracy
SReT (Ours)
Swin
CoaT-Lite
DeiT
PiT
TNT
T2T
AutoFormer
PVT
Fig. 1: Params/FLOPs vs. ImageNet-1K Acc.The architectures of trans-
former have achieved substan-
tively breakthroughs recently
in the elds of natural lan-
guage processing (NLP) [54],
computer vision (CV) [16]
and speech [15,56]. In the
vision area, Dosovitskiy et
al. [16] introduced a vision
transformer (ViT) model that splits a raw image into a patch sequence as in-
put, and they directly adopt transformer model [54] for the image classicationarXiv:2111.05297v3  [cs.CV]  26 Jul 2022

--- PAGE 2 ---
2 Zhiqiang Shen et al.
Norm
+ +
Norm
Self-Att
BlockMLP
Patch 
EmbeddingsNorm
+ +
Norm
Self-Att
BlockMLPN√óRecursive Operation
Transformer Block
Recursive Transformer Blockùë∂ùüèùë∂ùüè
ùë∂ùüê
Fig. 2: Atomic Recursive Operation.Table 1: Results using dierent num-
bersNof na ve recursive operation on
ImageNet-1K dataset.
Method Layers #Params (M) Top-1 Acc. (%)
DeiT-Tiny [52] 12 5.7 72.2
+ 1na ve recursion 24 5.7 74.0
+ 2na ve recursion 36 5.7 74.1
+ 3na ve recursion 48 5.7 73.6
task. ViT achieved impressive results and has inspired many follow-up works.
However, the benets of a transformer often come with a large number of pa-
rameters and computational cost and it is always of great challenge to achieve
the optimal trade-o between the accuracy and model complexity. In this work,
we are motivated by the following question: How can we improve the param-
eter utilization of a vision transformer, i.e., the representation ability without
increasing the model size? We observe recursive operation, as shown in Fig. 2,
is a simple yet eective way to achieve this purpose. Our recursion-based vision
transformer models signicantly outperform state-of-the-art approaches while
containing fewer parameters and FLOPs, as illustrated in Fig. 1.
Intrinsically, the classier requires high-level abstracted features from the
neural network to perform accurate classication, while the extraction of these
features often requires multiple layers and deeper networks. This introduces pa-
rameter overhead into the model. Our motivation of this work stems from an
interesting phenomenon of latent representation visualization. We observed that
in the deep vision transformer network, the weights and activations of adjacent
layers are similar with not much dierence (a similar phenomenon is also dis-
covered in [63]), which means they can be reused. The transformer with a xed
stack of distinct layers loses the inductive bias in the recurrent neural network
which inspires us to share those weights in a recursive manner, forming an itera-
tive or recursive vision transformer. Recursion can help extract stronger features
without the need of increasing the parameters, and further improve the accu-
racy. In addition, this weight reuse or sharing strategy partially regularizes the
training process by reducing the number of parameters to avoid overtting and
ill-convergence challenges, which will be discussed in the later sections.
Why do we need to introduce sliced recursion , i.e., the group self-
attention, into transformers? (advantages and drawbacks) We usually
push towards perfection on weight utilization of a network under a bounded
range of parameters, thus, it can be used practically in the resource-limited cir-
cumstances like embedded devices. Recursion is a straightforward way to com-
press the feature representation in a cyclic scheme. The recursive neural networks
also allow the branching of connections and structures with hierarchies. We found
that it is intriguingly crucial for learning better representations on vision data
in a hierarchical manner, as we will introduce in Fig. 10 of our experiments.
Also, even the most simplistic recursive operation still improves the compact-
ness of utilizing parameters without requiring to modify the transformer block
structure, unlike others [50,61,24,55,57,37,31,59], that add more parameters or

--- PAGE 3 ---
Sliced Recursive Transformer 3
involve additional ne-grained information from input [19]. However, such a re-
cursion will incur more computational cost by its loops, namely, it sacrices the
executing eciency for better parameter representation utilization . To address
this shortcoming, we propose an approximating method for global self-attention
through decomposing into multiple sliced group self-attentions across recursive
layers, meanwhile, enjoying similar FLOPs and better representations, we also
apply the spatial pyramid design to reduce the complexity of the network.
Feed-forward Networks, Recurrent Neural Networks and Recursive
Neural Networks. Feed-forward networks, such as CNNs and transformers,
are directed acyclic graphs (DAG), so the information path in the feed-forward
processing is unidirectional. Recurrent networks (RNNs) are usually developed to
process the time-series and other sequential data, and predict using current input
and past memory. Recursive network is a less frequently used term compared to
other two counterparts. Recursive refers to repeating or reusing a certain piece
of a network5. Dierent from RNNs that repeat the same block throughout the
whole network, recursive network selectively repeats critical blocks for particular
purposes. The recursive transformer iteratively renes its representations for all
patches in the sequence. We found that, through the designed recursion into the
feed-forward transformer, we can dramatically enhance feature representation
especially for structured data without including additional parameters.
The strong experimental results show that integrating the proposed sliced
recursive operation in the transformer strike a competitive trade-o among ac-
curacy, model size and complexity. To the best of our knowledge, there are barely
existing works studying the eectiveness of recursive operation in vision trans-
formers and proposing the approximation of self-attention method for reducing
the complexity of recursive operation. We have done extensive experiments to de-
rive a set of guidelines for the new design on vision task, and hope it is useful for
future research. Moreover, since our method does not involve the sophisticated
knowledge for modication of transformer block or additional input information,
it is orthogonal and friendly to most of existing ViT designs and approaches.
Our contributions.
- We investigate the feasibility of leveraging recursive operation with sliced
group self-attention in the vision transformers, which is a promising direction
for establishing ecient transformers and has not been well-explored before. We
conducted in-depth studies on the roles of recursion in transformers and conclude
an eective scheme to use them for better parameter utilization.
- We provide design principles, including the concrete format and compre-
hensive comparison to variants of SReT architectures, computational equiva-
lency analysis, modied distillation, etc., in hope of enlightening future studies
in compact transformer design and optimization.
- We verify our method across a variety of scenarios, including vision trans-
former, all-MLP architecture of transformer variant, and neural machine trans-
lation (NMT) using transformers. Our model outperforms the state-of-the-art
methods by a signicant margin with fewer parameters.
5In a broader sense, the recurrent neural network is a type of recursive neural network.

--- PAGE 4 ---
4 Zhiqiang Shen et al.
2 Related Work
(i)Transformer [54] was originally designed for natural language processing
tasks and has been the dominant approach [14,60,41,9,35] in this eld. Recently,
Vision Transformer (ViT) [16] demonstrates that such multi-head self atten-
tion blocks can completely replace convolutions and achieve competitive perfor-
mance on image classication. While it relied on pre-training on large amounts
of data and transferring to downstream datasets. DeiT [52] explored the train-
ing strategies and various data augmentation on ViT models, to train them on
ImageNet-1K directly. Basically, DeiT can be regarded as a framework of ViT
backbone + massive data augmentation + hyper-parameter tuning + hard distil-
lation with tokens. After that, many extensions and variants of ViT models have
emerged on image classication task, such as Bottleneck Transformer [50], Mul-
timodal Transformer [22], Tokens-to-Token Transformer [61], Spatial Pyramid
Transformer [24,55], Class-Attention Transformer [53], Transformer in Trans-
former [19], Convolution Transformer [57], Shifted Windows Transformer [37],
Co-Scale Conv-Attentional Transformer [59], etc. (ii) Recursive operation
has been explored in NLP [34,12,6,5,7,28,11] and vision [32,26,18,36] areas. In
particular, DEQ [5] proposed to nd equilibrium points via root-nding in the
weight-tied feedforward models like transformers and trellis for constant memory.
UT [12] presented the transformer with recurrent inductive bias of RNNs which
is similar to our SReT format. However, these works ignored the complexity
increased by recursive operation in designing networks. In this paper, we focus
on utilizing recursion properly by approximating self-attention through multiple
group self-attentions for building compact and ecient vision transformers.
3 Recursive Transformer
Vanilla Transformer Block. A basic transformer block Fconsists of a Multi-
head Self Attention (MHSA), Layer Normalization (LN), Feed-forward Network
(FFN), and Residual Connections (RC). It can be formulated as:
z0
`= MHSA (LN ( z` 1)) +z` 1;z`= FFN (LN ( z0
`)) +z0
`;i:e:;z`=F` 1(z` 1)
(1)
where z0
`andz` 1are the intermediate representations. F`indicates the trans-
former block at `-th layer.`2f0;1;:::;Lgis the layer index and Lis the number
of hidden layers. The self-attention module is realized by the inner products with
a scaling factor and a softmax operation, which is written as:
Attention(Q;K;V ) = Softmax
QK>=p
dk
V (2)
whereQ;K;V arequery ,keyand value vectors, respectively. 1 =pdkis the scal-
ing factor for normalization. Multi-head self attention further concatenates the
parallel attention layers to increase the representation ability:
MHSA(Q;K;V ) = Concat (head 1;:::; head h)WO, whereWO2Rhdvdmodel .

--- PAGE 5 ---
Sliced Recursive Transformer 5
head i= Attention
QWQ
i;KWK
i;VWV
i
are the projections with parameter
matricesWQ
i2Rdmodeldk;WK
i2Rdmodeldk;WV
i2Rdmodeldv. The FFN
contains two linear layers with a GELU non-linearity [23] in between
FFN(x) = (GELU ( zW1+b1))W2+b2 (3)
where zis the input. W1;b1;W2;b2are the two linear layers' weights and biases.
Recursive Operation. In the original recursive module [49] for the language
modality, the shared weights are recursively applied on a structured input which
is among the complex inherent chains, so it is capable of learning deep structured
knowledge. Recursive neural networks are made of architectural data and class,
which is majorly proposed for model compositionality on NLP tasks. Here, we
still use the sequence of patch tokens from the images as the inputs following the
ViT model [16]. And, there are no additional inputs used for feeding into each
recursive loop of recursive block as used on structured data. Take two loops as
an example for building the network, the recursive operation can be simplied:
z`=F` 1(F` 1(z` 1)) (4)
The na ve recursive operation tends to learn a simple and trivial solution like
the identity mapping by the optimizer, since the F` 1's output and input are
identical at the adjacent two depths (layers).
Non-linear Projection Layer (NLL). NLL is placed between two recursive
operations to enable the non-linear transformation between each block's output
and input, to avoid learning trivial status for these recursive blocks by forcing
nonequivalence on neighboring output and input. NLL can be formulated as:
NLL(z` 1) = MLP 
LN 
z0
` 1
+z0
` 1 (5)
where MLP is a multi-layer projection as FFN, but has dierent mlp ratio for
hidden features. We also use residual connection in it for better representation.
As shown in Table 1, more recursions will not improve accuracy without NLL.
Recursive Transformer. A recursive transformer with two loops in every block
is:
z`= NLL 2(F` 1(NLL 1(F` 1(z` 1)))) (6)
where z` 1andz`are each recursive block's input and output. Dierent from
MHSA and FFN that share parameters across all recursive operations within a
block, NLL 1and NLL 2use the non-shared weights independently regardless of
positioning within or outside the recursive blocks.
Recursive All-MLP [51] (an extension). We can formulate it as:
U;i=X;i+W2GELU ( W1LN (X);i);
Yj;=Uj;+W4GELU ( W3LN (U)j;);
Yj;=M` 1(M` 1(X;i))(7)
where the rst and second lines are token-mixing and channel-mixing from [51].
M` 1is a MLP block, Cis the hidden dimension and Sis the number of non-
overlapping image patches. NLL is not used here for simplicity.

--- PAGE 6 ---
6 Zhiqiang Shen et al.
Permutation‚Ä¶Permutation‚Ä¶Globalself-attention4groups√†4groups4groups√†2groups1groupRecursiveRecursivetokens
Fig. 3: Approximating global MHSA via sliced group MHSA with permutation.
Gradients in A Recursive Block. Here, we simply use explicit backpropa-
gation through the exact operations in the forward pass like gradient descent
method since SReT has no constraint to obtain the equilibrium of input-output
in recursions like DEQ [5] and the number of loops can be small to control the
network computation and depth. Our backward pass is more like UT [12]. In
general, the gradient of the parameters in each recursive block can be:
@L
@WF=@L
@zN@zN
@WF+@L
@zN@zN
@zN 1@zN 1
@WF+:::@L
@zN@zN
@zN 1:::@z2
@z1@z1
@WF
=NX
i=1@L
@zN0
@N 1Y
j=i@zj+1
@zj1
A@zi
@WF(8)
where WFis the parameters of recursive block. Lis the objective function.
Learnable Residual Connection (LRC) for Recursive Vision Trans-
formers. He et al. [21] studied various strategies of shortcut connections on
CNNs and found that the original residual design with pre-activation performs
best. Here, we found simply adding learnable coecients on each branch of
residual connection can benet to the performance of ViT following the similar
discovery of literature [33]. Formally, Eq. 1 and Eq. 5 can be reformulated as:
z0
`=MHSA (LN ( z` 1)) +z` 1;
z`=FFN (LN ( z0
`)) +z0
`;(9)
NLL(z` 1) =MLP 
LN 
z0
` 1
+z0
` 1 (10)
where;;;;; are the learnable coecients. They are initialized as 1 and
trained with other model's parameters simultaneously without restrictions.
Extremely Deep Transformers. Weight-sharing mechanism allows us to build
a transformer with more than 100 layers still keeping a small model. We demon-
strate empirically that the proposed method can signicantly simplify the opti-
mization when the transformer is scaled up to an exaggerated number of layers.

--- PAGE 7 ---
Sliced Recursive Transformer 7
4 Approximating Global MHSA via Multi-Group MHSA
Though recursive operation is adequate to provide better representation using
the same number of parameters, the additional forward loop makes the overhead
in training and inference increasing unnegligibly. To address the extra computa-
tional cost caused by recursion while maintaining the improved accuracy, we in-
troduce an approximating method through multiple group self-attentions which
is surprisingly eective in reducing FLOPs without compromising accuracy.
Approximating Global Self-Attention in SReT. As shown in Fig. 3, a reg-
ular self-attention layer can be decoupled through multiple group self-attentions
in a recursion manner with similar or even smaller computational cost. In general,
the number of groups in dierent recursion can be the same or dierent depend-
ing on the requirements of FLOPs and accuracy trade-o. Such a strategy will
not change the number of parameters while more groups can enjoy lower FLOPs
but slightly inferior performance. We empirically veried that the decoupling
scheme can achieve similar performance with signicantly fewer FLOPs if using
proper splitting of self-attention in a tolerable scope, as shown in Appendix.
Computational Equivalency Analysis. In this subsection, we analyze the
complexity of global (i.e., original) and sliced group self-attentions and compare
with dierent values of groups in a vision transformer.
Theorem 1. (Equivalency of global and multiple group self-attentions
with recursion on FLOPs. )LetfN`;G`g2R1, when N`=G`,FLOPs (1
V-SA )=FLOPs (N`Recursion with G`G-SAs ).The complexity Cof global
and group self-attentions can be calculated as: (For simplicity, here we assume
#groups and vector dimensions in each recursive operation are the same.)
CG-SA =N`
G`CV-SA (11)
where N`andG`are the numbers of recursion and group MHSA in layer `, i.e.,
`-th recursive block. V-SA andG-SA represent the vanilla and group MHSA.
The proof is provided in Appendix. The insight provided by Theorem 1 is at
the core of our method to control the complexity and its various benets on
better representations. Importantly, the computation of self-attention through
the \slice" paralleling is equal to the vanilla self-attention. We can observe that
when N`=G`,CV-SACG-SA6and if N`<G`,CG-SA<CV-SA, we can use
this property to reduce the FLOPs in designing ViT.
Empirical observation: When FLOPs (recursion + G-SA )FLOPs (V-SA ),
Acc. (recursion + G-SAs )>Acc. (V-SA ).
We employ ex-tiny model to evaluate the performance of global self-attention
and sliced group self-attention with recursion. As shown in Table 2, we empiri-
cally verify that, with the similar computation, group self-attention with recur-
sion can obtain better accuracy than vanilla self-attention.
6In practice, the FLOPs of the two forms are not identical as self-attention module
includes extra operations like softmax, multiplication with scale and attention values,
which will be multiples by the recursive operation.

--- PAGE 8 ---
8 Zhiqiang Shen et al.
Table 2: Representation ability with global/group self-attentions.
Method #Params (M) FLOPs (B) Top-1 Acc. (%)
Baseline (PiT [24]) 4.9 0.7 73.0
SReT (global self-attention w/o loop) 4.0 0.7 73.6
SReT (group self-attentions w/ loops) 4.0 0.7 74.0
Analysis: Where is the benet from in SReT? Theoretical analysis on re-
cursion could further help understand the advantage behind, while it is dicult
and prior literature on this always proves it empirically. Here, we provide some
basic theoretical explanations from the optimization angle for better understand-
ing this approach. One is the enhanced gradients accumulation. Let gt=rft()
denote the gradient, we take Adam optimizer [27] as an example, na ve param-
eter update is t t 1 bmt= pbvt+
where the gradients w.r.t. stochastic
objective at timestep tisgt rft(t 1), here we omit rst and second moment
estimate formulae. After involving recursion (here NLL guaranteesbmi
t,bvi
t's dis-
crepancy), the new updating is: t t 1 PN
i=1bmi
t=p
bvi
t+
whereNis
the number of recursion loops. Basically, recursion enables more updating/tun-
ing of parameters in the same iteration, so that the learned weights are more
aligned to the loss function, and the performance is naturally better.
5 Experiments
In this section, we rst empirically verify the proposed SReT on image classica-
tion task with self-attention [54] and all-MLP [51] architectures, respectively. We
also perform detailed ablation studies to explore the optimal hyper-parameters
of our proposed network. Then, we extend it to the neural machine translation
(NMT) task to further verify the generalization ability of the proposed approach.
Finally, we visualize the evolution of learned coecients in LRC and interme-
diate activation maps to better understand the behaviors and properties of our
proposed model. Our experiments are conducted on CIAI cluster.
5.1 Datasets and Experimental Settings
(i)ImageNet-1K [13]: ImageNet-1K is a standard image classication dataset,
which contains 1K classes with a total number of 1.2 million training images and
50K validation images. Our models are trained on this dataset solely without
additional images; (ii) IWSLT'14 German to English (De-En) dataset [2]:
It contains about 160K sentence pairs as the training set. We train and evaluate
models following the protocol [1]; (iii) WMT'14 English to German (En-
De) dataset [3]: The WMT'14 training data consists of 4.5M sentences pairs
(116M English words, 110M German words). We use the same setup as [38].

--- PAGE 9 ---
Sliced Recursive Transformer 9
Settings: Our detailed training settings and hyper-parameters are shown in
Appendix. On ImageNet-1K, our backbone network is a spatial pyramid [24]
architecture with stem structure following [47].
Soft distillation strategy. On vision transformer, DeiT [52] proposed to distill
tokens together with hard predictions from the teacher. They stated that using
one-hot label with hard distillation can achieve the best accuracy. This seems
counterintuitive since soft labels can provide more subtle dierences and ne-
grained information of the input. In this work, through a proper distillation
design, our soft label based distillation framework (one-hot label is not used)
consistently obtained better performance than DeiT7. Our loss is a soft version
of cross-entropy between teacher and student's outputs as used in [46,44,45,4]:
LCE(SW) = 1
NPN
i=1PTW(z) logPSW(z);where PTWandPSWare the outputs
of teacher and student, respectively. More details can be referred to Appendix.
5.2 Na ve Recursion on Transformer
T Block
NLL
NLL
NLL(a) InnerloopRecursive
T Block
NLL
(b) Outer loopRecursive
T Block
NLL
NLL
NLL(a) InternalloopRecursive
T Block
NLL
(b) ExternalloopRecursive
Fig. 4: Paradigms of recur-
sive designs in transformer.In this section, we examine the eectiveness of pro-
posed recursion using DeiT training strategies. We
verify the following two fashions of recursion.
Internal and external loops. As illustrated in
Fig. 4, there are two possible recursion designs on
transformer networks. One is the internal loop that
repeats every block separately. Another one is the
external loop that cyclically executes all blocks to-
gether. Although external loop design can force the
model being more compact as it shares parameters
across all blocks with fewer non-shared NLL lay-
ers, we found such a structure is inexible with
limited representation ability. We conducted a comparison with 12 layers of ba-
sic transformers with 2 recursive operation and the results are: external 67.0%
(3.2M) vs.internal 67.6% (3.0M) j70.3% (3.9M). In the following experiments,
we use the internal recursive design as our default setting.
5.3 Ablation Studies
The overview of our ablation studies is shown in Table 3. The rst row presents
the baseline, the second group is the dierent structures indicated by the used
factors. The last is the comparison of KD. We also verify the following designs.
7We observed a minor issue of soft distillation implementation in DeiT ( https:
//github.com/facebookresearch/deit/blob/main/losses.py#L56 ). Basically, it is
unnecessary to use logarithm for teacher's output (logits) according to the formu-
lation of KL-divergence or cross-entropy. Adding logon both teacher and student's
logits will make the results of KL to be extremely small and intrinsically negligible.
We argue that soft labels can provide ne-grained information for distillation, and
consistently achieve better results using soft labels in a proper way than one-hot
label + hard distillation , as shown in Sec. 5.3.

--- PAGE 10 ---
10 Zhiqiang Shen et al.
#Params (M) Top-1 (%)
Baseline 5.7 72.2
Recursion w/o NLL 3.8 72.5
Recursion + NLL 5.0 74.7
Recursion + NLL - Class Token 5.0 75.0
Recursion + NLL + LRC 5.0 75.2
Recursion + NLL + Stem 5.0 76.0
Recursion (Full Components) 5.0 76.8
GT+Hard Distill [52] 5.0 77.5
Soft Distill (Ours) 5.0 77.9Table 3: Eectiveness
of various designs on
ImageNet-1K val set.
Please refer to Sec. 5.3
and our Appendix for
more details. In this
ablation study, the
backbone is SReT-TL
model using spatial
pyramid architecture.
Architecture conguration. As in Table 5, SReT-T is our tiny model which
has mlp ratio = 3.6 in FFN and 4.0 for SReT-TL . More details about these
architectures are provided in our Appendix. To examine the eectiveness of
recursive operation, we conduct dierent loops of na ve recursion on DeiT-T.
The results of accuracy curves on validation data are shown in Fig. 5 (1), we can
see 2is slightly better than 1 and the further boost is marginal, while the
1is much faster for executing. Thus, we use this in the following experiments.
NLL conguration. NLL is a crucial factor for size and performance since
the weights in it are not shared. To nd an optimal trade-o between model
compactness and accuracy, we explore the NLL ratios in Fig. 5 (2, 3). Generally,
a larger NLL ratio can achieve better performance but the model size increases
accordingly. We use 1.0 in our SReT-T andSReT-TL , and 2.0 in our SReT-S .
Dierent permutation designs and groups numbers. We explore the dif-
ferent permutation designs and the principle of choosing group numbers for
better accuracy-FLOPs trade-o. We propose to insert permutation and inverse
permutation layers to preserve the input's order information after the sliced
group self-attention operation. The detailed formulation of this module, together
with recursions and their result analyses are given in our Appendix.
Distillation. To examine the eectiveness of our proposed soft distillation method,
we conduct the comparison of one-hot label + hard distillation and soft distilla-
tion only . The backbone network is SReT-T , all hyper-parameters are the same
except the loss functions. The accuracy curves are shown in our Appendix. Our
result 77.7% is signicantly better than the baseline 77.1%.
(1)(2)(3)
Fig. 5: A comprehensive ablation study on dierent design factors.

--- PAGE 11 ---
Sliced Recursive Transformer 11
Table 4: Throughput evaluation of SReT and baselines.
DeiT-T FLOPs: 1.3B #Params: 5.7M Acc.: 72.2% Throughput: 3283.49 img/s
SReT-ExT FLOPs: 0.7B#46:2%#Params: 4.0M#29:8%Acc.: 74.0%"1:8%Throughput: 3473.43 img/s
Swin-T FLOPs: 4.5B #Params: 29.0M Acc.: 81.3% Throughput: 1071.43 img/s
SReT-S FLOPs: 4.2B#6:7%#Params: 20.9M#27:9%Acc.: 81.9%"0:6%Throughput: 1101.84 img/s
Throughput evaluation. In Table 4, we provide the throughput comparisons
with DeiT and Swin on one NVIDIA GeForce RTX 3090 which can directly
reect the real inference speed and time consumption. We highlight that our
method obtains signicantly fewer params and FLOPs with better throughput.
5.4 Comparison with State-of-the-art Approaches
A summary of our main results is shown in Table 5, our SReT-ExT is better than
PiT-T by 1.0% with 18.4% #parameters. SReT-T also outperforms DeiT-T by
3.8% with 15.8%#parameters and 15.4% #FLOPs. Distillation can help improve
the accuracy by 1.6% and ne-tuning on large resolution further boosts to 79.6%.
Moreover, our SReT-S is consistently better than state-of-the-art Swin-T, T2T,
etc., on accuracy, model size and FLOPs, which demonstrates the superiority
and potential of our architectures in practice.
5.5 All-MLP Architecture
MLP-Mixer [51] (Baseline), MLP-Mixer+Recursion and MLP-Mixer+Recursion
+LRC: Mixer is a recently proposed plain design that is based entirely on multi-
layer perceptrons (MLPs). We apply our recursive operation and LRC on MLP-
Mixer to verify their generalization. Results are shown in Fig. 6 (1), our method
is consistently better than the baseline using the same training protocol.
5.6 Neural Machine Translation
In this section, we compare the BLEU scores [39] of vanilla transformer [54]
and ours on the WMT14 En-De and IWSLT'14 De-En (Appendix) using fairseq
toolkit [17]. IWSLT'14 De-En is a relatively small dataset so the improvement is
(1)RecursiveMLP-Mixer(2)Evolutionofcoefficients
Fig. 6: (1) ImageNet-1K results on All-MLP. (2) Evolution of coecients.

--- PAGE 12 ---
12 Zhiqiang Shen et al.
Table 5: Comparison of Top-1 (%) on ImageNet-1K with state-of-the-art meth-
ods.>denotes the model is trained without the proposed group self-attention
approximation. Fine-tuning on large resolution is highlighted by gray color.
Method Resolution #Params (M) FLOPs (B) Top-1 (%)
DeiT-T [52] 224 5.7 1.3 72.2
PiT-T [24] 224 4.9 0.7 73.0
SReT-ExT (Ours) 224 4.0 0.7 74.0
DeiT-T [52] 224 5.7 1.3 72.2
SReT- >T (Ours) 224 4.8#15:8%1.4 76.1
SReT-T (Ours) 224 4.8 1.1#21:4%76.0
DeiT-T Distill [52] 224 5.7 1.3 74.5
SReT- >TDistill (Ours) 224 4.8 1.4 77.7
SReT-T Distill (Ours) 224 4.8 1.1#21:4%77.6
SReT- >TDistill &384 "(Ours) 384 4.9 6.4 79.7
SReT-T Distill &384 "(Ours) 384 4.9 4.3#32:8%79.6
DeiT-T [52] 224 5.7 1.3 72.2
AutoFormer-Tiny [10] 224 5.7 1.3 74.7
CoaT-Lite Tiny [59] 224 5.7 1.6 76.6
SReT- >TL (Ours) 224 5.0#12:3%1.4 76.8
SReT-TL (Ours) 224 5.0 1.2#14:3%76.7
SReT- >TLDistill (Ours) 224 5.0 1.4 77.9
SReT-TL Distill (Ours) 224 5.0 1.2 77.7
SReT- >TLDistill &384 "(Ours) 384 5.1 6.6 80.0
SReT-TL Distill &384 "(Ours) 384 5.1 4.4#33:3%79.8
ViT-B/16 [16] 384 86.0 55.4 77.9
DeiT-S [52] 224 22.1 4.6 79.8
PVT-S [55] 224 24.5 3.8 79.8
PiT-S [24] 224 23.5 2.9 80.9
T2T-ViT t-14 [61] 224 21.5 5.2 80.7
TNT-S [19] 224 23.8 5.2 81.3
Swin-T [37] 224 29.0 4.5 81.3
SReT- >S (Ours) 224 20.9#27:9%4.7 82.0
SReT-S (Ours) 224 20.9 4.2#10:6%81.9
PiT-S Distill [24] 224 23.5 2.9 81.9
DeiT-S Distill [52] 224 22.1 4.6 81.2
T2T-ViT t-14Distill [61] 224 21.5 5.2 81.7
SReT- >SDistill (Ours) 224 20.9 4.7 82.8
SReT-S Distill (Ours) 224 20.9 4.2#10:6%82.7
SReT- >SDistill &384 "(Ours) 384 21.0 18.5 83.8
SReT- >SDistill &512 "(Ours) 512 21.3 42.8 84.3
not as signicant as on WMT14 En-De. The results are shown in Fig. 7, we can
see our method is favorably better than the baseline. Without LRC, the model
slightly converges faster, but the nal accuracy is inferior to using LRC. Also,
LRC makes the training process more stable, as shown in the red dashed box.
5.7 Landscape Visualizations of DeiT and Our Mixed-depth SReT
Explicit mixed-depth training. The recursive neural network enables to train
the model in a mixed-depth scheme. As shown in Fig. 8 (d), the left branch is
the subnetwork containing recursive blocks, while the right is the blocks without
sharing the weights on depth, but their weights are re-used with the left branch.
In this structure, the two branches take inputs from the same stem block. Mixed-

--- PAGE 13 ---
Sliced Recursive Transformer 13
Fig. 7: Comparison of BLEU, training loss and val loss on WMT14 En-De.
T Block
T Block
T Block
Recursive
NLL
NLL
NLL(a) Transformer(b) Re-Transformer(c) Re-Transformer w/ NLLRecursive
T Block
NLL
NLL
NLL(d) Re-Transformer w/ Mixed-depthRecursive
Share params
!"#=%‚àó'(%)
depth=3depth=6
Share params
Fig. 8: Illustration of recursive transformer with dierent designs.
depth training oers simplied optimization by performing operations parallelly
and prevents under-optimizing when the network is extremely deep.
Benets of mixed-depth training. The spin-o benet of sliced recursion
is the feasibility of mixed-depth training, which essentially is an explicit deep
supervision scheme as the shallow branch receives stronger supervision that is
closer to the nal loss layer, meanwhile, weights are shared with the deep branch.
Inspired by [30], we visualize the landscape of baseline DeiT-108 and our
SReT-108 &SReT-108 mixed-depth models to examine and analyze the di-
culty of optimization on these three architectures. The results are illustrated in
Fig. 9, we can observe that DeiT-108 is more chaotic and harder for optimization
with a deeper local minimum than our mixed-depth network. This veries the
advantage of our proposed network structure for simpler optimization.
(1)DeiT-108(3)SReTw/Mixed-depth-108(Ours)
(2)SReT-108(Ours)
Fig. 9: The actual optimization landscape from DeiT-108 , our SReT-108 and
SReT-108 mixed-depth models.

--- PAGE 14 ---
14 Zhiqiang Shen et al.
28√ó28 (R1)28√ó28 (R2)14√ó14 (R2)7√ó7 (R2)(2) SReT-Tiny (Ours)(1) DeiT-Tiny
14√ó14
14√ó1414√ó14InputShallowDeep
Fig. 10: Illustration of activation
distributions on shallow, middle
and deep layers of DeiT-Tiny
and our SReT-T networks. Un-
der each subgure, 14 14, 28
28 and 77 are the resolutions
of feature maps. \R1/2" indi-
cates the index of recursive op-
erations in each block.
5.8 Analysis and Understanding
Here, we provide two visualizations regarding LRC and learned response maps.
Evolution of LRC coecients. As shown in Fig. 6 (2), we plot the evolution
of learned coecients in the rst block. We can observe that the coecients on
the identity mapping ( ;; ) rst go up and then down as the training contin-
ues. This phenomenon indicates that, at the beginning of model training, the
identify mapping plays a major role in the representations. After 50 epochs of
training, the main branch is becoming increasingly important. Once the training
is complete, in FFN and NLL, the main branch exceeds the residual connection
branch while on MHSA it is the opposite. We believe this phenomenon can
inspire us to design a more reasonable residual connection structure in ViT.
Learned response maps. We visualize the activation maps of DeiT-T and
ourSReT-T model at shallow and deep layers. As shown in Fig. 10, DeiT is a
network with uniform resolution of feature maps (14 14). While, our spatial
pyramid structure has dierent sizes of feature maps along with the depth of the
network, i.e., the resolution of feature maps decreases when the depth increases.
More interesting observations are discussed in Appendix.
6 Conclusion
It is worthwhile considering how to improve the eciency of parameter utiliza-
tion for a vision transformer with minimum overhead. In this work, we have sum-
marized and explained several behaviors observed while training such networks.
We focused on building an ecient vision transformer with a compact model
size through the recursive operation, and the proposed group self-attention ap-
proximation method allows us to train in a more ecient manner with recursive
transformers. We highlight such a training scheme has not been well-explored yet
in previous literature. We attributed the superior performance of sliced recursive
transformer to its ability of intensifying the representation quality of intermedi-
ate features. We conducted comprehensive experiments to establish the success
of our method on the image classication and neural machine translation tasks,
not just verifying it in the vision domain, but proving the capability to generalize
for multiple modalities and architectures, such as MLP-Mixer.

--- PAGE 15 ---
Sliced Recursive Transformer 15
Appendix
In this appendix, we provide details omitted in the main text, including:
‚Ä¢Section A: Proof for equivalency of global self-attention and sliced group
self-attention with recursive operation on FLOPs. (Sec. 4 \Approximating Global
Self-Attention via Permutation of Group/Local Self-Attentions" of the main pa-
per.)
‚Ä¢Section B: Results of SReT on ImageNet ReaL [8] and ImageNetV2 [43]
datasets. (Sec. 5 \Experiments and Analysis" of the main paper.)
‚Ä¢Section C: More ablation results on dierent permutation designs and
numbers of groups when approximating global self-attention on ImageNet-1K.
(Sec. 5.3 \Ablation Studies" of the main paper.)
‚Ä¢Section D: Pseudocode for implementing sliced group self-attention. (Sec. 4
\Approximating Global Self-Attention via Permutation of Group/Local Self-
Attentions" of the main paper.)
‚Ä¢Section E: Implementation details of training on ImageNet-1K. (Sec. 5.1
\Datasets and Experimental Settings" of the main paper.)
‚Ä¢Section F: Hyper-parameters setting for training language models on WMT14
En-De and IWSLT14 De-En datasets. (Sec. 5.1 \Datasets and Experimental Set-
tings" and Sec. 5.6 \Neural Machine Translation" of the main paper.)
‚Ä¢Section G: Details of our SReT-T ,SReT-TL ,SReT-S and SReT-B architec-
tures. (Sec. 3 \Recursive Transformer" and Sec. 5.3. \Ablation Studies" of the
main paper.)
‚Ä¢Section H: Details of All-MLP structure. (Sec. 5.5 \All-MLP Architecture"
of the main paper.)
‚Ä¢Section I: Ablation study on dierent LRC designs. (Sec. 3 \Recursive
Transformer" and Sec. 5.8 \Analysis and Understanding" of the main paper.)
‚Ä¢Section J: Observations of Response Maps. (Sec. 5.8 \Analysis and Under-
standing" of the main paper.)
‚Ä¢Section K: More evolution visualization of LRC coecients on ImageNet-
1K dataset. (Sec. 5.8 \Analysis and Understanding" of the main paper.)
‚Ä¢Section L: Evolution visualization of LRC coecients in language model
on WMT14 En-De dataset. (Sec. 5.6 \Neural Machine Translation" and Sec. 5.8
\Analysis and Understanding" of the main paper.)
‚Ä¢Section M: More ablation results on directly expanding the depth of baseline
DeiT model on ImageNet-1K dataset. (Sec. 5.8 \Analysis and Understanding"
of the main paper.)
‚Ä¢Section N: More denitions of \Feed-forward Networks, Recurrent Neural
Networks and Recursive Neural Networks" and explanations of dierence to prior
arts. (Sec. 1 \Introduction" and Sec. 2 \Related Work" of the main paper.)
A FLOPs Analysis
One of the key benets of our SReT is to control the complexity of a recursive
network. We analyze the FLOPs of global (i.e., original) and sliced group self-
attentions and compare them with dierent circumstances of groups in a vision

--- PAGE 16 ---
16 Zhiqiang Shen et al.
transformer. In this section, we provide a proof to Theorem 1 which we restate
below.
Theorem 1. (Equivalency of global self-attention and group self-
attention with recursive operation on FLOPs. ) LetfN`;G`g2R1, when
N`=G`,FLOPs (1V-SA ) =FLOPs (N`Recursive with G`G-SAs ).
The complexity of regular and group self-attentions can be calculated as: (For
simplicity, here we assume #groups and vector dimensions in each recursive
operation are the same.)
CG-SA =N`
G`CV-SA (12)
where N`is the number of recursive operation and G`is the number of group
self-attentions in layer `, i.e.,`-th recursive block. V-SA andG-SA represent
the vanilla and group self-attentions, respectively.
Proof. (Theorem 1) The complexity Cof regular self-attention can be calculated
as:
CV-SA =O(L2
`D`) (13)
where L`is the sequence length and D`is the dimensionality of the latent
representations.
The complexity of simple recursive operation without group will be:
Crecursive =O(N`L2
`D`) (14)
where N`is the number of recursive operation.
The complexity of sliced group self-attentions with a recursive block can be
calculated as:
CG-SA =O(N`X
i(gi
`(L`
gi
`)2di
`))
=O(N`X
i(L2
`
gi
`di
`))(15)
where gi
`2fG`g,di
`2fD`g,i= 1;:::;N`.
Consider the condition of #groups gi
`and vector dimension di
`in each re-
cursive operation are the same. The complexity of group self-attentions can be
re-formulated as:
CG-SA =O(N`L2
`
G`D`) =N`
G`CV-SA (16)
where G`is the number of group self-attentions. When N`=G`,CV-SA =CG-SA
and if N`<G`,CG-SA<CV-SA.

--- PAGE 17 ---
Sliced Recursive Transformer 17
B More Results and Comparisons on ImageNet ReaL [8]
and ImageNetV2 [43] Datasets
In this section, we provide results on ImageNet ReaL [8] and ImageNetV2 [43]
datasets. On ImageNetV2 [43], we verify our SReT models on three metrics \Top-
Images", \Matched Frequency", and \Threshold 0.7". The results are shown
in Table 6, we achieve consistent improvement over DeiT on various network
architectures.
Table 6: More Comparison of SReT on ReaL [8] and ImageNetV2 [43] datasets.
Method Network #Parames FLOPs ImageNet ReaLImageNetV2 ImageNetV2 ImageNetV2
Top-images Matched-frequency Threshold-0.7
DeiT [52] Tiny 5.7 1.3 72.2 80.1 74.4 59.9 68.5
SReT Tiny 4.8 1.1 76.0 83.1 77.9 64.0 72.8
DeiT [52] Tiny+Distill 5.7 1.3 74.5 82.1 77.0 62.3 71.1
SReT Tiny+Distill 4.8 1.1 77.6 84.4 79.6 65.7 74.2
DeiT [52] Small 22.1 4.6 79.8 85.7 81.0 68.1 76.4
SReT Small 20.9 4.2 81.9 86.7 82.8 70.3 78.1
DeiT [52] Small+Distill 22.1 4.6 81.2 86.8 82.5 69.7 77.5
SReT Small+Distill 20.9 4.2 82.7 88.1 84.0 72.3 79.9
C Ablation Results on Dierent Permutation Designs
and Groups Numbers
In this section, we explore the dierent permutation designs and the principle of
choosing group numbers for the best accuracy-FLOPs trade-o. We propose to
insert an inverse permutation layer to preserve the input order information after
the sliced group self-attention operation. The formulation of this operation is
shown in Fig. 11 and the ablation results for this design are given in Table 7 of the
rst group. In the table, \P" represents the permutation layer, \I" represents the
inverse permutation layer and \L" indicates that we did not involve permutation
and inverse permutation in the last stage of models when the number of groups
equals 1. We use SReT-T andSReT-TL as the base structures for the ablation of
dierent groups. In the Groups column of the table, we applied two loops of
recursion in each recursive block according to the ablation study in Table 1 of our
main text. In each pair of the square brackets, the values denote the number of
groups for each recursion, and each pair of square brackets represents one stage
of blocks in the spatial pyramid based backbone network. We use [8,2][4,1][1,1]
as our nal SReT structure design since it has the best trade-o on accuracy and
computational cost.
D Pseudocode for Sliced Group Self-attention
The PyTorch pseudocode for implementation of our sliced group self-attention
is shown in Algorithm 1.

--- PAGE 18 ---
18 Zhiqiang Shen et al.
Permutation‚Ä¶123456789623179485623179485123456789
123456789InversepermutationRecursive
Fig. 11: Details of group self-attention with permutation designs.
Table 7: Ablation results of SReT-T andSReT-TL with dierent group designs.
Groups Net Layers Params (M) #FLOPs (B) Top-1 (%)
[8,8][4,4][1,1] P 20 4.99 1.08 75.41
[8,8][4,4][1,1] P+I 20 4.99 1.08 75.94
[8,8][4,4][1,1] P+I-L 20 4.99 1.08 76.06
[1,1][1,1][1,1] SReT- >T 20 4.76 1.38 76.07
[8,8][4,4][1,1] SReT-T 20 4.76 1.03 75.73
[16,2][4,2][1,1] SReT-T 20 4.76 1.01 75.79
[8,2][4,1][1,1] SReT-T 20 4.76 1.12 75.97
[1,1][1,1][1,1] SReT- >TL 20 4.99 1.43 76.78
[8,8][4,4][1,1] SReT-TL 20 4.99 1.08 76.06
[8,4][4,2][1,1] SReT-TL 20 4.99 1.14 76.16
[8,2][4,1][1,1] SReT-TL 20 4.99 1.18 76.65
[8,1][4,1][1,1] SReT-TL 20 4.99 1.25 76.72
[16,1][14,1][1,1] SReT-TL 20 4.99 1.24 76.56
[49,1][28,1][1,1] SReT-TL 20 4.99 1.23 76.30
E Training Details on ImageNet-1K
On ImageNet-1K, we conduct experiments on three training schemes: (1) con-
ventional training with one-hot labels; (2) distillation with soft labels from a
pre-trained teacher; (3) netuning from distilled parameters with higher resolu-
tion. Our training settings and hyper-parameters mainly follow the designs of
DeiT [52]. A detailed introduction of these settings is shown in Table 8, 9 and 10
with an item-by-item comparison.
Conventional Training from Scratch with One-hot Label. As shown in
Table 8, we use batch-sizes of 512/1024 for training our models and the default
initial learning rate is 1 e-3, while from our experiments, larger initial lrof 2e-
3 with more warmup epochs of 30 can favorably improve the accuracy. Other
settings are following [52].

--- PAGE 19 ---
Sliced Recursive Transformer 19
Table 8: Hyper-parameter details
of conventional training.
Method SReT-T SReT-TL SReT-S
Epoch 300 300 300
Batch size 1024 1024 512
Optimizer AdamW AdamW AdamW
Learning rate 0.001 0.001 0.001
Weight decay 0.05 0.05 0.05
Warmup epochs 5 5 5
Label smoothing 0.1 0.1 0.1
Stoch. Depth 0.1 0.1 0.2Table 9: Hyper-parameter details of
soft distillation training.
Method DeiT SReT
Label one-hot+hard distillation soft distillation
Epoch 300 300
Batch size 1024 1024
Optimizer AdamW AdamW
Learning rate 0.001 0.001
Table 10: Hyper-parameter details of
higher-resolution netuning.
Method DeiT SReT
Resolution 384 384
Weight decay 1e-8 0.0
Learning rate 5e-6 5e-6
(2)(1)(3)
Fig. 12: Comprehensive ablation study on dierent design factors.
Distillation Strategy. Knowledge distillation [25] is a popular way to boost the
performance of a student network. Recently, many promising results [40,48,58]
have been achieved using this technique. On vision transformer, DeiT [52] pro-
posed to distill tokens together with hard predictions from the teacher, and it
claimed that using one-hot label with hard distillation can achieve the best ac-
curacy. This seems counterintuitive since soft labels can provide more subtle
dierences and ne-grained information of the input. In this work, through a
proper distillation scheme, our soft label based distillation framework (one-hot
label is not used) consistently obtained better performance than DeiT. Our loss
is a soft version of cross-entropy between teacher and student's outputs as used
in [44,4,46]:
LCE(SW) = 1
NNX
i=1PTW(z) logPSW(z) (17)
where PTWandPSWare the outputs of teacher and student, respectively.
Distillation from Scratch. As shown in Table 9, we use soft predictions solely
from RegNetY-16GF [42] as a teacher instead of one-hot label + hard distillation
used in [52]. The ablation study on this point is provided in Fig. 12 (1) with
SReT-T .
Spatial Pyramid (SP) Design. Pyramids [29,20] are an eective design in
conventional vision tasks. The resolution of the shallow stage in a network is
usually large, SP can help to redistribute the computation from shallow to deep

--- PAGE 20 ---
20 Zhiqiang Shen et al.
stages of a network according to their representation ability. Here, we follow
the construction principles [24] but replacing the rst patch embedding layer
with a Stem block (i.e., a stack of three 3 3 convolution layers with stride = 2)
following [47].
Global Average PoolingFully-connected‚Ä¶class token
distillationtokenclass lossdistillloss
MHSAFFNMHSAFFN
Fig. 13: Our modications by re-
moving class token and distillation
token.Other Small Modications. Consider-
ing the unique properties of vision modality
compared to the language, we further apply
some minor modications on our network
design, some of them have been proven use-
ful on CNNs in the vision domain, includ-
ing: (i) We remove the class token and re-
place with a global average pooling (GAP)
on the last output together with a fully-
connected layer; (ii) We also remove the
distillation token if the training process in-
volves KD, which means we use the same
feature embedding for both the ground-
truth labels in standard training, and dis-
tillation with soft labels from the teacher. (iii) When ne-tuning from low res-
olution (224224) to high resolution (384 384) [52], following the perspective
of [48] that to increase the capacity of a model, we do not apply weight decay (set
it as 0) during ne-tuning. Generally, the above modications can slightly save
parameters, boost the performance and signicantly improve the simplicity of
the whole framework. The illustration of these modications is shown in Fig. 13.
(1)WMT14 De-En
(2)IWSLT14 De-En
Fig. 14: Comparison of BLEU, training loss and val loss on WMT14 En-De (top)
and IWSLT14 De-En datasets (bottom). The red dashed box indicates that LRC
makes training more stable.

--- PAGE 21 ---
Sliced Recursive Transformer 21
Table 11: Training details of our language models. The architectures we used are
in Fairseq [17].
Method IWSLT14 De-En WMT14 En-De
arch transformer iwslt deen transformer wmt ende
share decoder input output embed True True
optimizer Adam Adam
adam-betas (0.9, 0.98) (0.9, 0.98)
clip-norm 0.0 0.0
learning rate 5e-4 5e-4
lr scheduler inverse sqrt inverse sqrt
warmup updates 4K 4K
dropout 0.3 0.3
weight decay 0.0001 0.0001
criterion label smoothed cross-entropy label smoothed cross-entropy
label smoothing 0.1 0.1
max tokens 4096 4096
F Hyper-parameter Settings of Language Models
We test our proposed method on two public language datasets: IWSLT14 De-
En and WMT14 En-De translation tasks. We describe experimental settings in
detail in Table 11.
Network Congurations. We use the Transformer [54] implemented in Fairseq [17]
that shares the decoder input and output embedding as the basic NMT model.
G Details of Our SReT Architectures
The details of our SReT-T ,SReT-TL ,SReT-S andSReT-B architectures are shown
in Table 12. In each recursive transformer block [[ :]A]B,Ais the number of
blocks with self-contained (non-shared) parameters, Bis the number of recursive
operations for each block. For CFFN andDNLL,CandDare the dimensions
(ratios) of hidden features between the two fully-connected layers.
H All-MLP Structure
We use B/16 in Mixer architectures [51] as our backbone network. In particular,
it contains 12 layers, the patch resolution is 16 16, the hidden size Cis 768,
the sequence length Sis 196, the MLP dimension DCandDSare 3072 and 384,
respectively.
I Ablation Study on Dierent LRC Designs
In this section, we verify the eectiveness of dierent LRC designs as shown
in Fig. 15, including: (1) learnable coecients on the identity mapping branch;

--- PAGE 22 ---
22 Zhiqiang Shen et al.
addition"L-NormSelf-AttDrop-path√ólearnable #
additionL-NormMLP√ólearnable %Drop-pathaddition"L-NormSelf-AttDrop-path√ólearnable #√ólearnable $
additionL-NormMLP√ólearnable %√ólearnable &Drop-pathaddition"L-NormSelf-AttDrop-path√ólearnable $
additionL-NormMLP√ólearnable &Drop-path(1)(2)(3)
Fig. 15: Ablation study on dierent LRC designs.
(2) learnable coecients on the main self-attention/MLP branch; (3) our used
design in the main text, i.e., including learnable coecients on both branches.
The quantitative results of dierent LRC designs are shown in Table 13, we
can observe that strategy (1) is slightly better than (2), while, (3) can achieve
consistent improvement over (1) and (2), and it is applied in our main text.
We further visualize more evolution visualizations on various layers/depths of
ourSReT-TL architecture. The results are shown in Fig. 16 and the analysis is
provided in Sec. K.
J Observations of Response Maps
We have a few interesting observations on the visualizations of Fig. 8 (main text):
(1) In the uniform size of transformer DeiT, information in the shallow layers is
basically vague, blurry and lacks details. In contrast, the high-level layers contain
stronger semantic information and are more aligned with the input. However,
our model has a completely dierent behavior: rst, in the same block but with
dierent recursive operations, we can observe that the features are hierarchical
(in Fig. 8 of main text (2)). Taken as a whole, shallow layers can capture more
details like edges, shapes and contours and deep layers focus on the high-level
semantic information, which is similar to CNNs. We emphasize such hierarchical
representation enabled by recursion and spatial pyramid is critical for vision
modality like images.
K More Evolution Visualization of LRC Coecients on
ImageNet-1K Dataset
The visualizations of coecients evolution at dierent recursive blocks and layers
are shown in Fig. 15. Intriguingly, we can observe in the deep layers of recursive

--- PAGE 23 ---
Sliced Recursive Transformer 23
Table 12: SReT architectures (Input size is 3 224224, sliced group self-attention
is not included for simplicity.)
Layers Output Size SReT-T SReT-TL
StemConv-BN-ReLU 32112112 33 conv, stride 2 3 3 conv, stride 2
Conv-BN-ReLU 645656 33 conv, stride 2 3 3 conv, stride 2
Conv-BN-ReLU 642828 33 conv, stride 2 3 3 conv, stride 2
Recursive T Block
(1)642828""
64-dim MHSA
3.6FFN=1.0NLL#
2#
2""
64-dim MHSA
4.0FFN=1.0NLL#
2#
2
Conv-Pooling Layer (1) 1281414 33 conv, stride 2, group 64 3 3 conv, stride 2, group 64
Recursive T Block
(2)1281414""
128-dim MHSA
3.6FFN=1.0NLL#
5#
2""
128-dim MHSA
4.0FFN=1.0NLL#
5#
2
Conv-Pooling Layer (2) 25677 33 conv, stride 2, group 128 3 3 conv, stride 2, group 128
Recursive T Block
(3)25677""
256-dim MHSA
3.6FFN=1.0NLL#
3#
2""
256-dim MHSA
4.0FFN=1.0NLL#
3#
2
Global Average Pooling 25611 AdaptiveAvgPool AdaptiveAvgPool
Linear Layer 1000
#Params (M) 4.8 M 5.0 M
Accuracy (%) 76.1 76.8
Distilled Accuracy (%) 77.7 77.9
Finetuning Accuracy "384 (%) 79.7 80.0
Layers Output Size SReT-S Output Size SReT-B
StemConv-BN-ReLU 63112112 33 conv, stride 2 96 112112 33 conv, stride 2
Conv-BN-ReLU 1265656 33 conv, stride 2 168 5656 33 conv, stride 2
Conv-BN-ReLU 1262828 33 conv, stride 2 336 2828 33 conv, stride 2
Recursive T Block
(1)1262828""
126-dim MHSA
3.0FFN=2.0NLL#
2#
2 3362828""
336-dim MHSA
3.0FFN=2.0NLL#
2#
2
Conv-Pooling Layer (1) 2521414 33 conv, stride 2, group 126 672 1414 33 conv, stride 2, group 336
Recursive T Block
(2)2521414""
252-dim MHSA
3.0FFN=2.0NLL#
5#
2 6721414""
672-dim MHSA
3.0FFN=2.0NLL#
5#
2
Conv-Pooling Layer (2) 50477 33 conv, stride 2, group 252 1344 77 33 conv, stride 2, group 672
Recursive T Block
(3)50477""
504-dim MHSA
3.0FFN=2.0NLL#
3#
2 134477""
1344-dim MHSA
3.0FFN=2.0NLL#
3#
2
Global Average Pooling 50411 AdaptiveAvgPool 1344 11 AdaptiveAvgPool
Linear Layer 1000
#Params (M) 20.9 M 71.2 M
Accuracy (%) 82.0 82.7
Distilled Accuracy (%) 82.8 83.7
Finetuning Accuracy "384 (%) 83.8 84.8
blocks,tends to be one stably during the whole training. Other coecients on
the identity mapping ( and) are holding xed values that are also close to
one during the training. This phenomenon indicates that the identity mapping
branch tends to pass the original signal with small scaling. Moreover, it seems the
contributions of the two branches have a particular proportion for the particular
depth of layers.
L Evolution Visualization of LRC Coecients on
Language Model
The visualization of coecients evolution on the language model is shown in
Fig. 17. Dierent from the evolution in vision transformer models, the coecients

--- PAGE 24 ---
24 Zhiqiang Shen et al.
Table 13: Ablation study on dierent LRC designs.
Method #Params (M) Top-1 Acc. (%)
Baseline ( SReT-TL w/o LRC) 5.0 74.7
onxbranch (1) 5.0 75.0
onfbranch (2) 5.0 74.9
on both (3) 5.0 75.2
in language model are more stable during training with small variance. Also, they
are symmetrical with value one.
M More Ablation Results on Directly Enlarging Depth
of Baseline DeiT Model
In this section, we provide the results by directly expanding the depth of baseline
DeiT model, as shown in Table 14. We can see deeper na ve DeiT could not bring
additional gain on performance since the deeper and heavier network is usually
more dicult to learn meaningful and diverse intermediate features, while our
recursive operation through sharing/reusing parameters is an eective way to
enlarge the depth of a transformer, meanwhile, obtaining extra improvement.
Table 14: More ablation results on directly expanding depth of baseline DeiT
model. * indicates that the total number layers of our network is 20 (recursive
transformer blocks) + 10 (NLL) + 3 (image patch embeddings). Permutation
and inverse permutation layers are not included.
Method #Layers #Params (M) Top-1 Acc. (%)
DeiT-Tiny [52] 12 5.7 72.20
+ extend depth 24 11.55 77.35
+ extend depth 36 16.39 77.18
+ extend depth 48 21.73 75.89
Ours (SReT-S) 33* 20.90 81.90
N More Denitions and Explanations to Prior Arts
Feed-forward Networks, Recurrent Neural Networks and Recursive
Neural Networks. To clarify the denition of proposed recursive operation,
we distinct recursive neural networks from feed-forward networks and recurrent
neural networks. Feed-forward networks, such as CNNs and transformers, are
directed acyclic graphs (DAG). The information path in the feed-forward pro-
cessing is unidirectional, making the feed-forward networks hard to tackle the

--- PAGE 25 ---
Sliced Recursive Transformer 25
structured data with long-span correlations. Recurrent neural networks (RNNs)
are usually developed to process the time-series and other sequential data. They
output predictions based on the current input and past memory, so they are ca-
pable of processing data that contains long-term interdependent compounds like
language. Recursive network is a less frequently used term compared to other
two counterparts. Recursive refers to repeating or reusing a certain piece of a
network. Dierent from RNNs that repeat the same block throughout the whole
network, recursive neural network selectively repeats critical blocks for particu-
lar purposes. The recursive transformer iteratively renes its representations for
all image patches in the sequence.
Dierence to Prior Arts: On CNNs, ShueNet [62] uses inerratic shue for
ecient design while it is not truly random. Thus, there is no inverse opera-
tion involved. In contrast, our permutation is entirely stochastic and inverse is
crucial since self-attention is sensitive to tokens' order. The na ve group self-
attention only has interaction within the window, Swin [37] addresses this us-
ing shifted windows across dierent layers . While, we solve it by integrating
\slice+permutation+recursion" on the same layer's parameters , so each layer
enables to interact with all other windows, not across layers as Swin.

--- PAGE 26 ---
26 Zhiqiang Shen et al.
Algorithm 1 PyTorch-like Code for Sliced Group MHSA with 2 Recursion.
# num_groups1 and num_groups2: numbers of groups in different
recursions
# recursion: recursive indicator
class SG_Attention(nn.Module):
def __init__(self, dim, num_groups1=8, num_groups2=4, num_heads=8,
qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
super().__init__()
self.num_heads = num_heads
# numbers of groups in different recursions
self.num_groups1 = num_groups1
self.num_groups2 = num_groups2
head_dim = dim // num_heads
self.scale = qk_scale or head_dim ** -0.5
self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
self.attn_drop = nn.Dropout(attn_drop)
self.proj = nn.Linear(dim, dim)
self.proj_drop = nn.Dropout(proj_drop)
def forward(self, x, recursion):
B, N, C = x.shape
if recursion == False:
num_groups = self.num_groups1
else:
num_groups = self.num_groups2
# we will not do permutation and inverse permutation if #group=1
if num_groups != 1:
idx = torch.randperm(N)
# perform permutation
x = x[:,idx,:]
# prepare for inverse permutation
inverse = torch.argsort(idx)
qkv = self.qkv(x).reshape(B, num_groups, N // num_groups, 3, self.
num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5)
q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use
tensor as tuple)
attn = (q @ k.transpose(-2, -1)) * self.scale
attn = attn.softmax(dim=-1)
attn = self.attn_drop(attn)
x = (attn @ v).transpose(2, 3).reshape(B, num_groups, N // num_groups,
C)
x = x.permute(0, 3, 1, 2).reshape(B, C, N).transpose(1, 2)
if recursion == True and num_groups != 1:
# perform inverse permutation
x = x[:,inverse,:]
x = self.proj(x)
x = self.proj_drop(x)
return x
...

--- PAGE 27 ---
Sliced Recursive Transformer 27
Layer_1Layer_2Recursive1
Layer_3Layer_4Recursive2
Layer_17Layer_18Recursive9
Layer_19Layer_20Recursive10
Fig. 16: Evolution of coecients at dierent recursive blocks and layers.
1 21 41 61 81
Epoch0.51.01.5ValueEvolution of coefficients in LRC
Fig. 17: Evolution of coecients on language of WMT14 En-De dataset.

--- PAGE 28 ---
28 Zhiqiang Shen et al.
References
1.https://github.com/pytorch/fairseq/blob/master/examples/translation/
README.md 8
2.https://workshop2014.iwslt.org/downloads/proceeding.pdf 8
3.https://www.statmt.org/wmt14/translation-task.html 8
4. Bagherinezhad, H., Horton, M., Rastegari, M., Farhadi, A.: Label renery:
Improving imagenet classication through label progression. arXiv preprint
arXiv:1805.02641 (2018) 9, 19
5. Bai, S., Kolter, J.Z., Koltun, V.: Deep equilibrium models. In: Proceedings of the
International Conference on Neural Information Processing Systems (2019) 4, 6
6. Bai, S., Kolter, J.Z., Koltun, V.: Trellis networks for sequence modeling. In: ICLR
(2019) 4
7. Bai, S., Koltun, V., Kolter, J.Z.: Multiscale deep equilibrium models. In: Proceed-
ings of the International Conference on Neural Information Processing Systems
(2020) 4
8. Beyer, L., H ena, O.J., Kolesnikov, A., Zhai, X., Oord, A.v.d.: Are we done with
imagenet? arXiv preprint arXiv:2006.07159 (2020) 15, 17
9. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 (2020) 4
10. Chen, M., Peng, H., Fu, J., Ling, H.: Autoformer: Searching transformers for vi-
sual recognition. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) (2021) 12
11. Chowdhury, J.R., Caragea, C.: Modeling hierarchical structures with continuous
recursive neural networks. In: Proceedings of the 38th International Conference on
Machine Learning. pp. 1975{1988 (2021) 4
12. Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., Kaiser, L.: Universal trans-
formers. In: International Conference on Learning Representations (2018) 4, 6
13. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248{255. Ieee (2009) 8
14. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. pp. 4171{4186 (2019) 4
15. Dong, L., Xu, S., Xu, B.: Speech-transformer: A no-recurrence sequence-to-
sequence model for speech recognition. In: 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). pp. 5884{5888 (2018) 1
16. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
International Conference on Learning Representations (2021) 1, 4, 5, 12
17. FAIR: https://github.com/pytorch/fairseq 11, 21
18. Guo, Q., Yu, Z., Wu, Y., Liang, D., Qin, H., Yan, J.: Dynamic recursive neural
network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 5147{5156 (2019) 4
19. Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer.
arXiv preprint arXiv:2103.00112 (2021) 3, 4, 12

--- PAGE 29 ---
Sliced Recursive Transformer 29
20. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional
networks for visual recognition. IEEE transactions on pattern analysis and machine
intelligence 37(9), 1904{1916 (2015) 19
21. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.
In: European conference on computer vision. pp. 630{645 (2016) 6
22. Hendricks, L.A., Mellor, J., Schneider, R., Alayrac, J.B., Nematzadeh, A.: Decou-
pling the role of data, attention, and losses in multimodal transformers. arXiv
preprint arXiv:2102.00529 (2021) 4
23. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 (2016) 5
24. Heo, B., Yun, S., Han, D., Chun, S., Choe, J., Oh, S.J.: Rethinking spatial dimen-
sions of vision transformers. arXiv preprint arXiv:2103.16302 (2021) 2, 4, 8, 9, 12,
20
25. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 (2015) 19
26. Kim, J., Lee, J.K., Lee, K.M.: Deeply-recursive convolutional network for image
super-resolution. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 1637{1645 (2016) 4
27. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014) 8
28. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A
lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942 (2019) 4
29. Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In: 2006 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CVPR'06). vol. 2, pp.
2169{2178 (2006) 19
30. Li, H., Xu, Z., Taylor, G., Studer, C., Goldstein, T.: Visualizing the loss landscape
of neural nets. In: Proceedings of the 32nd International Conference on Neural
Information Processing Systems. pp. 6391{6401 (2018) 13
31. Li, Y., Zhang, K., Cao, J., Timofte, R., Van Gool, L.: Localvit: Bringing locality
to vision transformers. arXiv preprint arXiv:2104.05707 (2021) 2
32. Liang, M., Hu, X.: Recurrent convolutional neural network for object recognition.
In: Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 3367{3375 (2015) 4
33. Liu, F., Gao, M., Liu, Y., Lei, K.: Self-adaptive scaling for learnable residual struc-
ture. In: Proceedings of the 23rd Conference on Computational Natural Language
Learning (CoNLL) (2019) 6
34. Liu, S., Yang, N., Li, M., Zhou, M.: A recursive recurrent neural network for
statistical machine translation. In: Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). pp. 1491{1500
(2014) 4
35. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 (2019) 4
36. Liu, Y., Wang, Y., Wang, S., Liang, T., Zhao, Q., Tang, Z., Ling, H.: Cbnet: A novel
composite backbone network architecture for object detection. In: Proceedings of
the AAAI conference on articial intelligence. vol. 34, pp. 11653{11660 (2020) 4
37. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030 (2021) 2, 4, 12, 25

--- PAGE 30 ---
30 Zhiqiang Shen et al.
38. Luong, M.T., Pham, H., Manning, C.D.: Eective approaches to attention-based
neural machine translation. In: Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. pp. 1412{1421 (2015) 8
39. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
evaluation of machine translation. In: Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (2002) 11
40. Pham, H., Dai, Z., Xie, Q., Luong, M.T., Le, Q.V.: Meta pseudo labels. arXiv
preprint arXiv:2003.10580 (2020) 19
41. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language
models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 4
42. Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Doll ar, P.: Designing network
design spaces. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 10428{10436 (2020) 19
43. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classiers generalize
to imagenet? In: International Conference on Machine Learning. pp. 5389{5400.
PMLR (2019) 15, 17
44. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014) 9, 19
45. Shen, Z., He, Z., Xue, X.: Meal: Multi-model ensemble via adversarial learning.
In: Proceedings of the AAAI Conference on Articial Intelligence. vol. 33, pp.
4886{4893 (2019) 9
46. Shen, Z., Liu, Z., Xu, D., Chen, Z., Cheng, K.T., Savvides, M.: Is label smoothing
truly incompatible with knowledge distillation: An empirical study. In: Interna-
tional Conference on Learning Representations (2021) 9, 19
47. Shen, Z., Liu, Z., Li, J., Jiang, Y.G., Chen, Y., Xue, X.: Dsod: Learning deeply
supervised object detectors from scratch. In: Proceedings of the IEEE international
conference on computer vision. pp. 1919{1927 (2017) 9, 20
48. Shen, Z., Savvides, M.: Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy
on imagenet without tricks. In: NeurIPS Workshop (2020) 19, 20
49. Sperduti, A., Starita, A.: Supervised neural networks for the classication of struc-
tures. IEEE Transactions on Neural Networks 8(3), 714{735 (1997) 5
50. Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.: Bottleneck
transformers for visual recognition. arXiv preprint arXiv:2101.11605 (2021) 2, 4
51. Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T.,
Yung, J., Keysers, D., Uszkoreit, J., Lucic, M., Dosovitskiy, A.: Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601 (2021) 5, 8, 11, 21
52. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J egou, H.: Training
data-ecient image transformers & distillation through attention. arXiv preprint
arXiv:2012.12877 (2020) 2, 4, 9, 10, 12, 17, 18, 19, 20, 24
53. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., J egou, H.: Going deeper
with image transformers. arXiv preprint arXiv:2103.17239 (2021) 4
54. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
L., Polosukhin, I.: Attention is all you need. In: NIPS (2017) 1, 4, 8, 11, 21
55. Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao,
L.: Pyramid vision transformer: A versatile backbone for dense prediction without
convolutions. arXiv preprint arXiv:2102.12122 (2021) 2, 4, 12
56. Wang, Y., Shi, Y., Zhang, F., Wu, C., Chan, J., Yeh, C.F., Xiao, A.: Transformer
in action: a comparative study of transformer-based acoustic models for large scale
speech recognition applications. In: ICASSP 2021-2021 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP). pp. 6778{6782. IEEE
(2021) 1

--- PAGE 31 ---
Sliced Recursive Transformer 31
57. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: Cvt: Introduc-
ing convolutions to vision transformers. arXiv preprint arXiv:2103.15808 (2021) 2,
4
58. Xie, Q., Luong, M.T., Hovy, E., Le, Q.V.: Self-training with noisy student improves
imagenet classication. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 10687{10698 (2020) 19
59. Xu, W., Xu, Y., Chang, T., Tu, Z.: Co-scale conv-attentional image transformers.
arXiv preprint arXiv:2104.06399 (2021) 2, 4, 12
60. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:
Generalized autoregressive pretraining for language understanding. Advances in
Neural Information Processing Systems (2019) 4
61. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.E., Feng, J., Yan, S.: Tokens-
to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint
arXiv:2101.11986 (2021) 2, 4, 12
62. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shuenet: An extremely ecient convolu-
tional neural network for mobile devices. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 6848{6856 (2018) 25
63. Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Jiang, Z., Hou, Q., Feng, J.: Deepvit:
Towards deeper vision transformer. arXiv preprint arXiv:2103.11886 (2021) 2
