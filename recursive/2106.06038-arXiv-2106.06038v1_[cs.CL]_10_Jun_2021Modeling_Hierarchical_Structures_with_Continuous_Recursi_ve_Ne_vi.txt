# Mô hình hóa Cấu trúc Phân cấp với Mạng Nơ-ron Đệ quy Liên tục

Jishnu Ray Chowdhury1 Cornelia Caragea1

Tóm tắt
Mạng Nơ-ron Đệ quy (RvNN), khả năng sắp xếp các chuỗi theo cấu trúc cú pháp phân cấp cơ bản của chúng, đã hoạt động tốt trong một số tác vụ xử lý ngôn ngữ tự nhiên so với các mô hình tương tự không có thiên hướng cấu trúc. Tuy nhiên, RvNN truyền thống không có khả năng tự cảm ứng cấu trúc tiềm ẩn trong một chuỗi văn bản thuần túy. Một số phần mở rộng đã được đề xuất để vượt qua hạn chế này. Tuy nhiên, những phần mở rộng này có xu hướng dựa vào gradient thay thế hoặc học tăng cường với chi phí thiên lệch hoặc phương sai cao hơn. Trong công trình này, chúng tôi đề xuất Mạng Nơ-ron Đệ quy Liên tục (CRvNN) như một giải pháp thay thế thân thiện với lan truyền ngược để giải quyết những hạn chế nói trên. Điều này được thực hiện bằng cách kết hợp một phép nới lỏng liên tục vào cấu trúc cảm ứng. Chúng tôi chứng minh rằng CRvNN đạt được hiệu suất mạnh mẽ trong các tác vụ tổng hợp đầy thử thách như suy luận logic (Bowman et al., 2015b) và ListOps (Nangia & Bowman, 2018). Chúng tôi cũng cho thấy CRvNN hoạt động tương đương hoặc tốt hơn so với các mô hình cấu trúc tiềm ẩn trước đây trên các tác vụ thế giới thực như phân tích cảm xúc và suy luận ngôn ngữ tự nhiên.

1. Giới thiệu
Xây dựng biểu diễn câu là điều quan trọng đối với các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) như suy luận ngôn ngữ tự nhiên, truy xuất tài liệu và phân loại văn bản. Trong một số bối cảnh nhất định, các phương pháp có thiên hướng về việc sắp xếp câu theo cấu trúc cú pháp cơ bản của chúng (Chomsky, 1957) đã được chứng minh là vượt trội hơn các phương pháp tương đương không nhận biết cấu trúc (Socher et al., 2013; Tai et al., 2015; Nangia & Bowman, 2018; Choi et al., 2018; Maillard et al., 2019; Havrylov et al., 2019; Shen et al., 2019a) và một số phương pháp nhận biết cấu trúc này (Shen et al., 2019a; Qian et al., 2020) cũng thể hiện tính hệ thống tốt hơn (Fodor & Pylyshyn, 1988). Đáng chú ý, ngay cả các phương pháp dựa trên Transformer đương đại (Vaswani et al., 2017) cũng đã hưởng lợi từ thiên hướng cấu trúc trong nhiều tác vụ ngôn ngữ tự nhiên (Wang et al., 2019; Fei et al., 2020).

Mạng Nơ-ron Đệ quy (RvNN) (Pollack, 1990; Socher et al., 2013) có khả năng sắp xếp các chuỗi theo bất kỳ thứ tự tùy ý nào được xác định bởi cấu trúc của đầu vào. Tuy nhiên, RvNN tuân theo cấu trúc cứng khi sắp xếp một chuỗi nhất định. Các mô hình Cây Tiềm ẩn như Gumbel-Tree-LSTM (Choi et al., 2018), mặc dù có khả năng đưa ra quyết định cấu trúc thích ứng, vẫn tuân theo cấu trúc sắp xếp cứng. Do đó, những mô hình này buộc phải đưa ra quyết định cấu trúc rời rạc hoặc dựa vào các trình phân tích cú pháp bên ngoài. Trong trường hợp đầu tiên, mô hình thường buộc phải dựa vào gradient thay thế (dẫn đến tăng thiên lệch) hoặc học tăng cường (dẫn đến tăng phương sai). Thiên lệch hoặc phương sai tăng có thể dẫn đến hiệu suất kém (Nangia & Bowman, 2018) trừ khi được chăm sóc cẩn thận (Havrylov et al., 2019).

Trái ngược với những phương pháp dựa trên RvNN này, các trình phân tích cú pháp dựa trên biểu đồ (Le & Zuidema, 2015; Maillard et al., 2019) có thể tuân theo "cấu trúc mềm" bằng cách áp dụng trung bình có trọng số một cách đệ quy trên đầu ra từ nhiều cấu trúc có thể. Tuy nhiên, những phương pháp này đòi hỏi việc xây dựng và duy trì một biểu đồ các vector, điều này tương đối tốn kém.

Với những thách thức này, trong công trình này, chúng tôi đề xuất Mạng Nơ-ron Đệ quy Liên tục (CRvNN), kết hợp phép nới lỏng liên tục vào RvNN làm cho nó có thể vi phân đầu cuối. Chúng tôi hướng đến các tính năng sau:

1. Cảm ứng cây tiềm ẩn tự động theo tác vụ cụ thể. Mô hình của chúng tôi không phụ thuộc vào nhãn cấu trúc sự thật nền tảng dưới bất kỳ hình thức nào.

2. Thân thiện với lan truyền ngược. Điều này cho phép mô hình của chúng tôi dễ dàng tích hợp như một mô-đun trong kiến trúc nơ-ron lớn hơn và tránh những thách thức của học tăng cường hoặc gradient thay thế (Havrylov et al., 2019).

3. Tính song song. Không giống như hầu hết các mô hình cây tiềm ẩn trước đây, CRvNN có thể sắp xếp nhiều thành phần song song nếu chúng ở cùng mức độ phân cấp. Điều này ngụ ý rằng, lý tưởng nhất, CRvNN có thể đệ quy qua độ sâu cây cảm ứng, thường ngắn hơn nhiều so với độ dài chuỗi. Do đó, CRvNN cũng có thể giảm bớt khó khăn với gradient biến mất hoặc gradient bùng nổ (Hochreiter, 1991; Bengio et al., 1994), có thể được gây ra bởi việc áp dụng lặp lại hoặc đệ quy trên các chuỗi có độ dài dài.

Mặc dù các trình phân tích cú pháp dựa trên biểu đồ về mặt kỹ thuật chia sẻ những tính năng trên ở một mức độ nào đó, chúng song song hóa trên nhiều đường dẫn sắp xếp thay vì sắp xếp trực tiếp nhiều thành phần trong một đường dẫn duy nhất. Nhìn chung, các trình phân tích cú pháp dựa trên biểu đồ vẫn cần đệ quy qua toàn bộ độ dài chuỗi. Ngược lại, CRvNN có thể dừng sớm dựa trên độ sâu cây cảm ứng mặc dù với chi phí tham lam hơn.

2. Kiến thức cơ bản
Trong phần này, chúng tôi cung cấp giới thiệu ngắn gọn về Mạng Nơ-ron Đệ quy.

2.1. Mạng Nơ-ron Đệ quy (RvNN)
RvNN sử dụng một hàm sắp xếp f nào đó để sắp xếp đệ quy hai con đã cho thành cha mẹ của chúng:

p = f(xi, xi+1), (1)

Ở đây, chúng tôi xem xét một chuỗi x1:n = (x1, x2, ···, xn), và một hàm sắp xếp f nhận hai vector con có chiều d làm đầu vào và trả về vector cha có cùng chiều d. Đệ quy kết thúc khi nó sắp xếp mọi nút thành cha gốc. Như một ví dụ, xem xét chuỗi x1:6 với cấu trúc phân cấp cơ bản được biểu diễn như:

((x1, (x2, x3)), (x4, (x5, x6))).

Loại chuỗi có cấu trúc này cũng có thể được biểu diễn dưới dạng cây. RvNN hoạt động trên đầu vào như:

p = f(f(x1, f(x2, x3)), f(x4, f(x5, x6))). (2)

3. Cách tiếp cận của chúng tôi
Trong phần này, chúng tôi mô tả chi tiết kỹ thuật của CRvNN. Tuy nhiên, trước khi đi vào chi tiết chính xác, chúng tôi trước tiên xây dựng một công thức hóa mới nhưng tương đương của RvNN. Điều này sẽ giúp kết nối CRvNN với RvNN dễ dàng hơn.

3.1. Mạng Nơ-ron Đệ quy: Công thức hóa lại
Bây giờ chúng tôi trình bày công thức hóa lại RvNN đã đề cập ở trên. Dưới đây chúng tôi mô tả các thành phần khác nhau của RvNN được công thức hóa lại của chúng tôi từng cái một.

3.1.1. HÀM SẮP XẾP
Khái niệm về hàm sắp xếp giống như trong §2.1.

Ví dụ. Cho một hàm sắp xếp (hoặc tế bào đệ quy) f(r1, r2), nếu một đầu vào (r1) biểu diễn x1, và đầu vào khác (r2) biểu diễn (x2, x3), thì đầu ra cha sẽ biểu diễn (x1, (x2, x3)).

3.1.2. XÁC SUẤT TỒN TẠI
Trong RvNN, một khi hai biểu diễn con được sắp xếp thành cha, các biểu diễn con bản thân chúng sẽ không còn cần thiết nữa. Những biểu diễn con không cần thiết có thể được coi là "không tồn tại". Dựa trên ý tưởng này, chúng tôi giới thiệu khái niệm "xác suất tồn tại".

Định nghĩa 1. Chúng tôi định nghĩa khái niệm "xác suất tồn tại" ei để biểu thị liệu một biểu diễn ri có cần xử lý thêm hay không (ei = 1) hoặc nó nên được coi là "không tồn tại" và do đó bị bỏ qua (ei = 0). Mỗi vị trí có một xác suất tồn tại, ban đầu là 1 cho tất cả các vị trí. Hiện tại, chúng tôi chỉ xem xét các giá trị nhị phân ei ∈ {0, 1}.

Ví dụ. Nếu ri từ vị trí i được sắp xếp với rj từ vị trí j, thì chúng ta có thể cập nhật vị trí j với biểu diễn cha sắp xếp của (ri, rj) và đặt xác suất tồn tại của vị trí i là 0. Thông thường, chúng ta có thể chỉ loại bỏ những biểu diễn không cần thiết. Tuy nhiên, trong công thức này, chúng tôi giữ chúng với xác suất tồn tại. Đây là thay đổi chính giúp chúng tôi mang lại phép nới lỏng liên tục cho RvNN như chúng tôi sẽ thảo luận sau.

3.1.3. HÀM QUYẾT ĐỊNH
Định nghĩa 2. Chúng tôi định nghĩa hàm quyết định D như một hàm dành riêng để đưa ra quyết định cấu trúc về việc sắp xếp các vị trí nào với nhau thành biểu diễn cha cho trạng thái mô hình.

Định nghĩa này tổng quát hóa cả RvNN vanilla và các kiến trúc như Gumbel-Tree LSTM. Trong trường hợp RvNN vanilla, D có thể được hiểu như một thuật toán tầm thường đưa ra quyết định thích hợp ở mỗi bước bằng cách đơn giản nhìn vào sự thật nền tảng. Trong trường hợp Gumbel-Tree LSTM, hàm D có thể biểu diễn hàm tính điểm của nó để tính điểm tất cả các biểu diễn ứng viên được chọn để sắp xếp trong một bước cụ thể. Trong công thức tổng quát của chúng tôi, D đưa ra quyết định cấu trúc bằng cách gán xác suất sắp xếp (ci) cho mỗi vị trí i trong chuỗi r1:n. Xác suất sắp xếp này xác định vị trí nào sẽ được thay thế bằng biểu diễn cha. Hiện tại, chúng tôi chỉ xem xét ci ∈ {0, 1}. Lưu ý, không giống như Gumbel-Tree LSTM, trong công thức của chúng tôi, nhiều vị trí có thể được chọn để sắp xếp. Trong §3.1.5, chúng tôi thảo luận các quy tắc đệ quy chính xác để cập nhật biểu diễn dựa trên xác suất sắp xếp.

3.1.4. HÀM TRÁI VÀ PHẢI
Định nghĩa 3. Cho một chuỗi các giá trị v1:n = (v1, v2, ···, vn), chúng tôi định nghĩa hàm left nhận làm đầu vào một giá trị vi nào đó và trả về giá trị trái tồn tại gần nhất vj (tức là, vj = left(vi)) sao cho ej = 1 và ∀l, j < l < i, el = 0. Lưu ý rằng vì vị trí i−1 có thể có xác suất tồn tại ei−1 = 0, left(vi) không phải lúc nào cũng là vi−1.

Tương tự, chúng tôi cũng định nghĩa hàm right nhận làm đầu vào một giá trị vi nào đó và trả về giá trị phải tồn tại gần nhất vj (tức là, vj = right(vi)) sao cho ej = 1 và ∀l, i < l < j, el = 0. Tương tự như hàm left, right(vi) không phải lúc nào cũng là vi+1. Lưu ý rằng chuỗi v1:n có thể là bất kỳ chuỗi nào bao gồm chuỗi biểu diễn (r1:n) hoặc chuỗi xác suất sắp xếp (c1:n).

3.1.5. CẬP NHẬT ĐỆ QUY
Bây giờ, chúng tôi mô tả cách các thành phần trên kết hợp với nhau trong mỗi cập nhật đệ quy của RvNN được công thức hóa lại của chúng tôi. Chúng ta có thể biểu diễn chính thức quy tắc để cập nhật đệ quy một biểu diễn nào đó tại vị trí i, ri trong bước đệ quy k như:

r(k+1)i = left(ci) · f(left(r(k)i), r(k)i) + (1 − left(ci)) · r(k)i, (3)

trong đó f đề cập đến hàm sắp xếp (tế bào đệ quy) như trước và ci là xác suất sắp xếp (như được dự đoán bởi hàm D) tại bất kỳ vị trí i nào. Lưu ý rằng sau điểm này, nếu left(ci) = 1, left(r(k)i) không còn cần thiết nữa vì nó đã được sắp xếp cùng với r(k)i và biểu diễn cha sắp xếp đã tồn tại ở vị trí i (như r(k+1)i) theo quy tắc trên. Do đó, khi left(ci) = cj = 1, chúng ta có thể đặt xác suất tồn tại (ej) tại vị trí left(i) = j là 0. Do đó, chúng ta có thể biểu diễn quy tắc cập nhật cho ej như:

e(k+1)j = e(k)j · (1 − cj). (4)

Trong Bảng 1, chúng tôi cho thấy cách chúng ta có thể mô phỏng RvNN dựa trên các quy tắc được đặt ra ở trên bằng cách sử dụng chuỗi x1:6.

3.2. Hướng đến RvNN Liên tục
Trong các phần trên, chúng tôi chỉ xem xét các giá trị nhị phân rời rạc cho xác suất sắp xếp và xác suất tồn tại. Do đó, nó vẫn là "RvNN cứng" với thứ tự sắp xếp rời rạc. Để biến đổi nó thành Mạng Nơ-ron Đệ quy Liên tục (RvNN mềm), chúng tôi đơn giản kết hợp phép nới lỏng liên tục vào hàm quyết định D cho phép nó dự đoán xác suất sắp xếp (ci) trong khoảng [0, 1]. Tương tự, chúng tôi cũng cho phép xác suất tồn tại (ei) trong [0, 1].

Kết quả của phép nới lỏng này, chúng ta có thể đơn giản sử dụng mạng nơ-ron với kích hoạt sigmoid cho D. Cho rằng chúng tôi phù hợp với các giá trị liên tục, chúng tôi không phải buộc quyết định nhị phân bằng cách sử dụng một số hình thức tham số hóa lại hoặc xấp xỉ gradient. Hơn nữa, chúng ta có thể trực tiếp sử dụng các quy tắc cập nhật đệ quy như được định nghĩa trong Eq. 3 và Eq. 4 mà không có thay đổi nào bởi vì chúng đã tương thích với ci và ei có giá trị thực.

Dưới đây, chúng tôi thảo luận về việc định nghĩa lại các hàm left và right (hàm truy xuất hàng xóm) của chúng tôi trong bối cảnh mới này.

3.2.1. HÀM TRUY XUẤT HÀNG XÓM
Trong công thức hóa lại của chúng tôi được trình bày trong §3.1, một vai trò tích cực được thực hiện bởi các hàm left và right. Tuy nhiên, định nghĩa trước đây (Def. 3) được thực hiện dưới điều kiện xác suất tồn tại chỉ có thể nhận các giá trị nhị phân. Vì bây giờ chúng ta có xác suất có giá trị thực, khái niệm "giá trị trái (hoặc phải) tồn tại gần nhất" không được định nghĩa rõ ràng vì xác suất tồn tại có thể có các giá trị không nhị phân.

Cụ thể, cả hàm left và right đều quan trọng đối với CRvNN, và bất kỳ hàm nào trong số các hàm có thể được sử dụng lại làm hàm kia với những thay đổi tối thiểu. Do đó, hiện tại, chúng tôi chỉ tập trung vào hàm right có thể dễ dàng được điều chỉnh thành hàm left.

Định nghĩa 4. Cho một chuỗi các giá trị v1:n = (v1, v2, ···, vn), và cho mỗi vi, cho một chuỗi xác suất pi1:in = (pi1, pi2, ···, pin) sao cho pij chỉ ra xác suất rằng vj là giá trị tồn tại gần nhất bên phải vi, chúng tôi định nghĩa hàm right như: right(vi) = Σⁿⱼ₌₁ pij · vj.

Về cơ bản, hàm right trả về thứ gì đó tương tự như giá trị kỳ vọng của biểu diễn phải tồn tại ngay lập tức. Tuy nhiên, lưu ý rằng Σⱼ pij không nhất thiết bằng 1, mà ≤ 1. Điều này là do có khả năng khác là không có biểu diễn tồn tại ở bên phải nào cả.

Trong việc triển khai của chúng tôi, chúng tôi biến đổi xác suất tồn tại e1:n = (e1, e2, ···, en) thành chuỗi pi1:in. Dưới đây, chúng tôi công thức hóa các quy tắc chính xác mà chúng tôi sử dụng:

pij = {
0 j ≤ i
ej/Σʲₗ₌ᵢ₊₁ eₗ ≤ 1
max(0, 1 − Σʲ⁻¹ₗ₌ᵢ₊₁ eₗ)/Σʲₗ₌ᵢ₊₁ eₗ > 1
} (5)

Nghĩa là, chúng tôi tái sử dụng xác suất tồn tại ej như pij. Đương nhiên, chúng tôi trước tiên đặt tất cả các giá trị không phải-phải từ i thành có xác suất 0. Sau đó chúng tôi điều chỉnh chúng để chúng cộng lại ≤ 1 bằng cách đặt pij về 0 cho bất kỳ giá trị nào sau vị trí mà xác suất tích lũy vượt quá 1.

Tuy nhiên, ở một mức độ nào đó, Eq. 5 là lựa chọn kỹ thuật hơn là hoàn toàn có cơ sở toán học. Một lựa chọn có cơ sở toán học hơn sẽ là đặt pij = ej · Πʲ⁻¹ₗ₌ᵢ₊₁(1 − eₗ). Chúng tôi đã tạo ra một công thức dựa trên log-sum-exp cho phương trình (để tránh những bất ổn tiềm tàng do phép nhân tích lũy) như:

pij = ej · exp(Σʲ⁻¹ₗ₌ᵢ₊₁ log(1 − eₗ)) (6)

Chúng tôi đã thực hiện một số thí nghiệm với điều này trên tập dữ liệu suy luận logic tổng hợp (Bowman et al., 2015b). Nó hoạt động hợp lý nhưng hơi tệ hơn khi chúng tôi sử dụng Eq. 5. Nhìn chung, cả hai công thức (Eq. 5 hoặc Eq. 6) đều tương đương trong cài đặt rời rạc, và do đó, cả hai đều có thể đẩy mô hình để xấp xỉ RvNN rời rạc. Bất kể, để tập trung, chúng tôi chỉ xem xét Eq. 5 trong phần còn lại của bài báo.

Với những công thức này, chúng ta có thể tổng quát hóa thêm cả hàm left và right.

Định nghĩa 5. Cho một chuỗi v1:n = (v1, v2, ···, vn), chúng tôi định nghĩa leftm(vi) như hàm để truy xuất giá trị kỳ vọng ở vị trí trái thứ m từ i. Chính thức:

left1(ri) = left(ri), (7)
leftm(ri) = left(leftm−1(ri)). (8)

Tương tự, chúng ta có thể định nghĩa rightm. Trong phần phụ sau, chúng tôi mô tả thuật toán chính.

3.3. Mạng Nơ-ron Đệ quy Liên tục: Thuật toán
Mô hình Mạng Nơ-ron Đệ quy Liên tục được trình bày trong Thuật toán 1. Chúng tôi đã giải thích các quy tắc cập nhật đệ quy. Tiếp theo, chúng tôi mô tả chi tiết triển khai của tất cả các hàm trong thuật toán.

3.3.1. BIẾN ĐỔI LÁ
Chúng tôi sử dụng hàm leafTransform đơn giản để biến đổi ban đầu của chuỗi nhúng. Điều này có thể được biểu diễn như:

r¹i = LN(Wxi + b). (9)

Ở đây, xi ∈ ℝᵈᵉᵐᵇᵉᵈˣ¹, W ∈ ℝᵈʰˣᵈᵉᵐᵇᵉᵈ, và b ∈ ℝᵈʰˣ¹. LN đề cập đến chuẩn hóa lớp (Ba et al., 2016).

3.3.2. HÀM QUYẾT ĐỊNH
Hàm quyết định được sử dụng để dự đoán xác suất sắp xếp ci. Cho điều này, chúng tôi tính đến bối cảnh cục bộ bằng cách sử dụng lớp tích chập. Tuy nhiên, chúng tôi muốn "bối cảnh cục bộ thực" mà chúng tôi cần sử dụng các hàm leftm và rightm, như được định nghĩa ở trên. Cho kích thước cửa sổ cục bộ là 2·τ+1, chúng tôi sử dụng hàm sau để lấy điểm số chưa chuẩn hóa ban đầu ui để tính xác suất sắp xếp ci:

ui = W2 GeLU(Στⱼ₌₋τ Wʲ⁺τconv L(ri,j) + bconv) + b2, (10)

L(ri,j) = {
left−j(ri) j < 0,
ri j = 0,
rightj(ri) j > 0.
} (11)

Ở đây, chúng ta có một tập hợp trọng số kernel tích chập {W⁰conv, W¹conv, ···, W²·τconv}, trong đó bất kỳ Wˡconv ∈ ℝᵈʰˣᵈʰ; L(ri,j) ∈ ℝᵈʰˣ¹, W2 ∈ ℝ¹ˣᵈʰ, bconv ∈ ℝᵈʰ và b2 ∈ ℝ¹. Lưu ý rằng ui bây giờ là giá trị thực vô hướng chưa chuẩn hóa. Để biến nó thành xác suất, chúng ta có thể sử dụng sigmoid:

ci = sigmoid(ui) = exp(ui)/(exp(ui) + 1). (12)

Việc sử dụng sigmoid cho phép nhiều vị trí có xác suất sắp xếp cao (ci). Do đó, nhiều cha có thể được sắp xếp trong cùng một đệ quy dẫn đến tính song song mà chúng tôi đã ám chỉ trước đây. Tuy nhiên, sigmoid thuần túy trong hình thức này rất không bị ràng buộc. Nghĩa là, không có ràng buộc nào ngăn nhiều vị trí liền kề (như được định nghĩa bởi các hàm left và right) có xác suất sắp xếp cao (ci). Việc sắp xếp đồng thời các biểu diễn liền kề với biểu diễn phải kỳ vọng của chúng sẽ vi phạm cấu trúc cây. Nó cũng có thể khiến thông tin được truyền đến các vị trí mà đồng thời mất xác suất tồn tại của chính chúng do truyền thông tin của chúng về phía phải. Do đó, một số thông tin được truyền có thể bị bỏ qua do hạ cánh trong khu vực mất xác suất tồn tại. Do đó, để ngăn các vị trí liền kề có xác suất tồn tại cao, chúng tôi điều chỉnh công thức sigmoid với điểm số từ các hàng xóm như sau:

ci = exp(ui)/(exp(ui) + exp(left(ui)) + exp(right(ui)) + 1). (13)

Chúng tôi gọi hàm này là sigmoid được điều chỉnh.

3.3.3. HÀM SẮP XẾP
Chúng tôi sử dụng cùng tế bào có cổng đệ quy như được giới thiệu bởi Shen et al. (2019a) cho hàm sắp xếp của chúng tôi. Điều này ban đầu được lấy cảm hứng từ các hàm feedforward của Transformer (Vaswani et al., 2017).

[zi; hi; ci; ui] = W2 GeLU(W¹Cell [left(ri); ri] + b1) + b2 (14)
oi = LN(σ(zi) ⊙ left(ri) + σ(hi) ⊙ ri + σ(ci) ⊙ ui) (15)

Ở đây, σ là sigmoid; oi là đầu ra cha ∈ ℝᵈʰˣ¹; ri, left(ri) ∈ ℝᵈʰˣ¹; W¹Cell ∈ ℝᵈᶜᵉˡˡˣ²·ᵈʰ; b1 ∈ ℝᵈᶜᵉˡˡˣ¹; W2 ∈ ℝᵈʰˣᵈᶜᵉˡˡ; b1 ∈ ℝᵈʰˣ¹. Khác với (Shen et al., 2019a), chúng tôi sử dụng GeLU (Hendrycks & Gimpel, 2016) thay vì ReLU làm hàm kích hoạt.

3.3.4. DỪNG ĐỘNG
Lý tưởng nhất, CRvNN có thể học xử lý đồng thời tất cả con ở cùng mức độ phân cấp. Do đó, CRvNN sẽ chỉ cần đệ quy qua độ sâu cây. Điều này có nghĩa là chúng ta cần một cơ chế để phát hiện nếu độ sâu cây cảm ứng đã được duyệt qua và nếu chúng ta có thể dừng. Theo khung của chúng tôi, chúng ta chỉ cần nhìn vào chuỗi xác suất tồn tại e1:n = (e1, e2, ···, en). Gần thời điểm lý tưởng để dừng, tất cả xác suất tồn tại trừ cái cuối nên gần bằng 0 (0 chỉ ra rằng nó không còn cần xử lý thêm). Vị trí cuối không có gì ở bên phải để được sắp xếp. Vì vậy, chúng tôi ép vị trí cuối luôn có xác suất sắp xếp 0 và do đó, xác suất tồn tại 1. Nhìn chung, để xác định khi nào dừng, chúng tôi đơn giản kiểm tra xem tất cả xác suất tồn tại trừ cái cuối có nhỏ hơn một ngưỡng nhỏ ε nào đó hay không. Thuật toán 2 cho thấy việc triển khai cơ chế này.

3.4. Thành phần bổ sung
Ở đây chúng tôi mô tả một số thành phần bổ sung, có thể ít cần thiết hơn, mà chúng tôi sử dụng trong việc triển khai CRvNN của chúng tôi.

3.4.1. NHÚNG ĐẶC BIỆT
Chúng tôi đặt trước tất cả chuỗi với nhúng đặc biệt <START>, và chúng tôi nối thêm tất cả chuỗi với nhúng đặc biệt <END>. Cả hai đều là vector có thể huấn luyện có kích thước dembed. Những nhúng này bị ép luôn có xác suất sắp xếp 0 và xác suất tồn tại là 1. Khi xây dựng bối cảnh cục bộ, những nhúng này có thể cung cấp thông tin rõ ràng hơn về việc biểu diễn ri gần đến ranh giới bắt đầu hoặc kết thúc như thế nào.

3.4.2. TÍNH NĂNG CHUYỂN ĐỔI
Trong nỗ lực tăng cường hàm quyết định, chúng tôi cũng nối các tính năng để cung cấp thông tin rõ ràng nếu vị trí i được sắp xếp và cập nhật trong đệ quy cuối cùng hay không. Cho điều đó, chúng tôi xây dựng một tập hợp tính năng T ∈ ℝᵈˢ như:

T = left(ci) · C + (1 − left(ci)) · Ĉ. (16)

Ở đây, C và Ĉ đều là tham số vector có thể huấn luyện ∈ ℝᵈˢ. C có thể được hiểu là biểu diễn một tập hợp tính năng chỉ ra rằng trong lần lặp cuối cùng, vị trí quan tâm được sắp xếp với con trái và được cập nhật, trong khi Ĉ có thể được hiểu là biểu diễn điều ngược lại. Ở đây, chúng tôi sử dụng các giá trị left(ci) từ đệ quy cuối cùng (ban đầu là 0).

3.4.3. PHẠT DỪNG
Như đã thảo luận trước đây, trong quá trình dừng, trong chuỗi xác suất tồn tại (e1:n), tất cả trừ xác suất tồn tại cuối cùng nên gần bằng 0. Mặc dù điều này lý tưởng được cho là xảy ra nếu tất cả các vị trí được xử lý đúng cách, nhưng không có đảm bảo rằng nó thực sự sẽ xảy ra. Để khuyến khích tính chất này, chúng tôi sử dụng một mất mát phụ trợ (A(e1:n)), mà chúng tôi định nghĩa như:

A(e) = −log(en/Σⁿⱼ₌₁ eⱼ). (17)

Điều này có thể được hiểu như entropy chéo giữa chuỗi xác suất tồn tại cuối cùng lý tưởng (tất cả trừ cái cuối là 0) và chuỗi thực tế cuối cùng của cùng sau chuẩn hóa. Hàm tối ưu hóa tổng thể có thể được công thức hóa như:

min_θ L(θ) + γ · A(e), (18)

trong đó L(θ) là mất mát entropy chéo chính và γ là nhân tử Lagrangian cho mục tiêu phụ trợ.

4. Thí nghiệm và Kết quả
Trong phần này, chúng tôi thảo luận về các thí nghiệm và kết quả của chúng tôi. Chúng tôi đánh giá mô hình của chúng tôi trên suy luận logic (Bowman et al., 2015b), các phép toán danh sách (ListOps) (Nangia & Bowman, 2018), phân tích cảm xúc—hai tập dữ liệu, SST2 và SST5 (Socher et al., 2013), và suy luận ngôn ngữ tự nhiên—hai tập dữ liệu, SNLI (Bowman et al., 2015a) và MNLI (Williams et al., 2018b). Để biết chi tiết triển khai, tham khảo phụ lục.

4.1. Suy luận Logic
Trong tập dữ liệu suy luận logic, chúng tôi tập trung vào hai thuộc tính tổng quát hóa cụ thể riêng biệt - tổng quát hóa độ dài và tổng quát hóa sắp xếp (tức là, tính hệ thống). Chúng tôi so sánh CRvNN với Tree-LSTM (Tai et al., 2015), Tree-Cell (Shen et al., 2019a) Tree-RNN (Bowman et al., 2015b), Transformer (Vaswani et al., 2017), Universal Transformer (Dehghani et al., 2019), LSTM (Hochreiter & Schmidhuber, 1997), RRNet (Jacob et al., 2018), ON-LSTM (Shen et al., 2019b), Ordered Memory (Shen et al., 2019a) (xem Bảng 2).

4.1.1. TỔNG QUÁT HÓA ĐỘ DÀI
Để đánh giá CRvNN cho tổng quát hóa độ dài, như trong công trình trước đây, chúng tôi huấn luyện mô hình chỉ trên các mẫu với ≤6 phép toán trong khi chúng tôi kiểm tra nó trên các mẫu với số phép toán chưa thấy cao hơn (≥7). Trong Bảng 2, không tính hiệu suất của các mô hình dựa trên RvNN với quyền truy cập sự thật nền tảng, mô hình của chúng tôi, cùng với bộ nhớ có thứ tự, đạt được hiệu suất tốt nhất trong tổng quát hóa độ dài.

4.1.2. TÍNH HỆ THỐNG
Theo Shen et al. (2019a), chúng tôi tạo ra ba phân chia huấn luyện-kiểm tra khác nhau trên tập dữ liệu suy luận logic: A, B, và C (với mức độ khó tăng từ A đến C, A là dễ nhất). Cho mỗi phân chia, chúng tôi lọc tất cả các mẫu với một mẫu sắp xếp cụ thể từ tập huấn luyện và đặt chúng vào tập kiểm tra. Sau đó chúng tôi kiểm tra xem mô hình của chúng tôi có thể tổng quát hóa đến các mẫu chưa thấy (các kết hợp chưa thấy của toán hạng và toán tử) hay không. Trong tập A, chúng tôi lọc các mẫu theo mẫu *(and(nota))*, trong B, chúng tôi lọc theo mẫu *(and(not*))*, và trong C, chúng tôi lọc theo *({and,or}(not*))* . Như rõ ràng trong Bảng 2, CRvNN thể hiện khả năng đặc biệt cho tổng quát hóa sắp xếp và vượt trội hơn tất cả kết quả trước đây được báo cáo.

4.2. ListOps
ListOps là một tác vụ tổng hợp đầy thử thách yêu cầu rõ ràng khả năng mô hình hóa phân cấp. Một số mô hình cây tiềm ẩn trước đây được chứng minh là hoạt động kém trên nó (Nangia & Bowman, 2018). Như được hiển thị trong Bảng 4, CRvNN đạt gần độ chính xác hoàn hảo, chứng minh khả năng của nó để nắm bắt các cấu trúc cơ bản mà không cần giám sát cấu trúc. Chúng tôi cũng cho thấy khả năng tổng quát hóa độ dài của CRvNN trên ListOps trong Bảng 3. CRvNN vẫn đạt được độ chính xác cao ở độ dài chuỗi cao hơn nhiều (400−1000) ngay cả khi nó ban đầu được huấn luyện trên các chuỗi có độ dài ≤100.

4.3. Tập dữ liệu Ngôn ngữ Tự nhiên
Trong Bảng 5, chúng tôi cũng đánh giá CRvNN trên các tập dữ liệu ngôn ngữ tự nhiên (SST2, SST5, SNLI, MNLI). Phù hợp với công trình trước đây (Havrylov et al., 2019), cho MNLI chúng tôi tăng cường dữ liệu huấn luyện với dữ liệu huấn luyện SNLI và báo cáo kết quả kiểm tra trên tập kiểm tra phù hợp. Trong các tác vụ thế giới thực, CRvNN thu được kết quả hỗn hợp, nhưng nhìn chung kết quả có thể so sánh với công trình trước đây trong bối cảnh cây tiềm ẩn tương tự. Đặc biệt, CRvNN hoạt động tương đối yếu trên SST2, nhưng ngược lại nó hoạt động tốt hơn đáng kể so với công trình trước đây (ngoài OM) trên MNLI, là tác vụ khó hơn. Chúng tôi cũng chạy một mô hình CRvNN với cảm ứng cấu trúc cây cân bằng bắt buộc trên MNLI nhưng hiệu suất của nó tương đối tệ hơn (71,5±0,4). Cho Gumbel-Tree LSTM Choi et al. (2018), chúng tôi hiển thị kết quả được báo cáo từ các công trình khác nhau vì kết quả dao động từ bài báo này sang bài báo khác (Havrylov et al., 2019; Williams et al., 2018a).

5. Phân tích
Trong phần này, chúng tôi đối chiếu CRvNN với Ordered Memory về mặt tốc độ và cho thấy nghiên cứu loại bỏ của CRvNN.

5.1. Kiểm tra Tốc độ
Ordered memory (OM) là đối thủ cạnh tranh gần gũi với CRvNN. Cả hai mô hình đều có thể vi phân đầu cuối. Cả hai đều có thể mô phỏng RvNN mà không có cấu trúc sự thật nền tảng. CRvNN không vượt trội hơn OM một cách nhất quán hay quyết định. Tuy nhiên, có một lợi thế quan trọng đối với CRvNN. Đặc biệt, nó có thể đạt được một mức độ song song bằng cách xử lý nhiều vị trí đồng thời. Đồng thời, nó có thể thực hiện thoát sớm bằng cách sử dụng dừng động. Ngược lại, OM không chỉ có vòng lặp bên ngoài qua toàn bộ độ dài chuỗi, mà nó còn có vòng lặp bên trong nơi nó đệ quy qua các khe nhớ của nó, thêm chi phí đáng kể. Đệ quy vòng lặp bên trong của OM liên quan đến các phép toán ma trận tuần tự nặng. Do đó, trong thực tế, CRvNN có thể nhanh hơn nhiều so với OM. Trong Bảng 6, chúng tôi so sánh thời gian chạy huấn luyện cho cả hai mô hình. Chúng tôi tạo ra các mẫu ListOps tổng hợp cho các phạm vi độ dài chuỗi khác nhau. Cho mỗi phạm vi độ dài chuỗi, chúng tôi huấn luyện cả hai mô hình trên 50 mẫu cho 1 epoch và 1 kích thước batch trên một phiên bản AWS P3.2× (Nvidia V100). Như chúng ta có thể thấy từ bảng, CRvNN nhanh hơn đáng kể so với OM. Mặc dù trong các cài đặt khác với các tác vụ khác nhau và kích thước batch khác nhau, khoảng cách giữa tốc độ của OM và CRvNN có thể không cao như trong Bảng 6, chúng tôi vẫn nhận thấy CRvNN nhanh hơn khoảng 2−4 lần so với OM ngay cả khi sử dụng kích thước batch cao hơn cho OM (tận dụng độ phức tạp bộ nhớ thấp hơn của nó). Chúng tôi cũng thử chạy CYK-LSTM (Maillard et al., 2019) nhưng gặp phải vấn đề bộ nhớ khi chạy nó cho các chuỗi dài hơn (>200 độ dài).

5.2. Nghiên cứu Loại bỏ
Trong Bảng 7, chúng tôi cho thấy nghiên cứu loại bỏ trên CRvNN. Chúng tôi thấy rằng việc thay thế sigmoid được điều chỉnh bằng sigmoid đơn giản làm giảm đáng kể và làm mất ổn định hiệu suất trong ListOps. Loại bỏ "cấu trúc" hoặc thiên hướng cấu trúc bằng cách đơn giản sử dụng hàm sắp xếp như mạng nơ-ron lặp lại từ trái-sang-phải, một lần nữa, làm hại đáng kể hiệu suất của mô hình. Tế bào đệ quy có cổng bản thân nó cũng quan trọng đối với hiệu suất. Thay thế nó bằng tế bào LSTM (Hochreiter & Schmidhuber, 1997) gây ra suy giảm nghiêm trọng. Ở một mức độ nào đó, điều này phù hợp với những phát hiện của Shen et al. (2019a). Họ thay thế tế bào đệ quy có cổng trong bộ nhớ có thứ tự (Shen et al., 2019a) bằng tế bào RNN, và quan sát suy giảm đáng kể. Tuy nhiên, ngay cả với tế bào LSTM, CRvNN hoạt động tốt hơn, trong suy luận logic, so với bất kỳ mô hình nào khác được báo cáo không có tế bào đệ quy có cổng. Thay thế hàm kích hoạt, loại bỏ các tính năng chuyển đổi (§3.4.2), hoặc loại bỏ phạt dừng (§3.4.3) tạo ra ít khác biệt.

5.3. Kết quả Phân tích cú pháp
Điểm số sắp xếp nội bộ của mỗi lớp CRvNN có thể được sử dụng để phân tích cây cảm ứng. Mặc dù có thể có nhiều cách để chuyển đổi điểm số sắp xếp để trích xuất cây, một phương pháp là đơn giản coi bất kỳ vị trí cụ thể nào của một chuỗi nhất định tại một lần lặp cụ thể trong thuật toán là có xác suất sắp xếp 1 bất cứ khi nào xác suất sắp xếp tích lũy tại vị trí đó qua tất cả các lần lặp cho đến nay là ≥0,5. Ngược lại, chúng tôi coi vị trí tại lần lặp cụ thể là có xác suất sắp xếp 0. Một khi điểm số được nhị phân hóa, việc trích xuất cây trở nên đơn giản theo các ý tưởng được thảo luận trong §3.1. Bảng 1 (trong §3.1) cho thấy một ví dụ về cách xác suất sắp xếp nhị phân và rời rạc liên quan đến một cấu trúc sắp xếp cụ thể. Trong Bảng 8, chúng tôi cho thấy các ví dụ phân tích cú pháp thu được với CRvNN bằng thủ tục trên.

6. Công trình Liên quan
Một số cách tiếp cận sớm tập trung vào việc điều chỉnh các mô hình nơ-ron để mô phỏng automata pushdown hoặc để cảm ứng ngữ pháp nói chung (Sun, 1990; Giles et al., 1990; Das et al., 1992; Mozer & Das, 1993; Zeng et al., 1994; Grefenstette et al., 2015). Ngoài ra, gần đây, có nhiều công trình tập trung vào cảm ứng cấu trúc dựa trên mục tiêu mô hình hóa ngôn ngữ (Yogatama et al., 2018; Shen et al., 2018; 2019b; Li et al., 2019; Kim et al., 2019; Drozdov et al., 2019; 2020; Shen et al., 2021). Trong công trình này, chúng tôi tập trung vào các mô hình có thiên hướng cấu trúc được sử dụng cho các tác vụ hạ nguồn chung bao gồm các tác vụ NLP như phân loại và NLI.

Pollack (1990) trình bày RvNN như một kiến trúc đệ quy để sắp xếp dữ liệu có cấu trúc cây và danh sách. Socher et al. (2010; 2013) hướng đến việc nắm bắt các mẫu cú pháp và ngữ nghĩa của các cụm từ ngôn ngữ và cho thấy hiệu quả của RvNN cho phân tích cảm xúc. Ban đầu, RvNN dựa vào các trình phân tích cú pháp bên ngoài để cung cấp đầu vào có cấu trúc cây. Một số công trình tập trung vào việc tăng cường RvNN để chúng có thể tự động cảm ứng cấu trúc cây từ văn bản thuần túy. Về mục đích này, Le & Zuidema (2015) sử dụng thuật toán phân tích cú pháp dựa trên biểu đồ (thuật toán Cocke–Younger–Kasami (Sakai, 1961)) trong khung nơ-ron với hàm sắp xếp tích chập. Maillard et al. (2019) mở rộng nó với cơ chế chú ý để gộp qua nhiều ứng viên cây con và hàm sắp xếp Tree-LSTM (Tai et al., 2015). Bowman et al. (2016) trình bày RNN tăng cường ngăn xếp (SPINN) để mô phỏng các hàm của Tree-RNN dựa trên các nguyên tắc của phân tích cú pháp shift-reduce (Schützenberger, 1963; Knuth, 1965) nhưng dựa vào chú thích cấu trúc sự thật nền tảng để giám sát. Yogatama et al. (2017) tăng cường SPINN với học tăng cường (RL-SPINN) cho phép cảm ứng cấu trúc không giám sát. Maillard & Clark (2018) tăng cường stack-RNN dựa trên phân tích cú pháp shift-reduce với tìm kiếm chùm. Munkhdalai & Yu (2017) giới thiệu Neural Tree Indexers và đạt được hiệu suất mạnh mẽ bằng cách sử dụng cây nhị phân đầy đủ trên các tác vụ ngôn ngữ tự nhiên. Tương tự, Shi et al. (2018) cho thấy rằng cây tầm thường (ví dụ, cây cân bằng nhị phân) có thể được sử dụng để có được hiệu suất cạnh tranh trên một số tác vụ ngôn ngữ tự nhiên. Dong et al. (2019) đề xuất một mô hình nơ-ron để học các quy tắc logic boolean và lượng hóa để lý luận về dữ liệu quan hệ với nhiều arities. Choi et al. (2018) sử dụng Gumbel Softmax (Jang et al., 2017) để thích ứng đưa ra quyết định rời rạc ở mỗi bước đệ quy trong việc chọn cha để sắp xếp với tree-LSTM. Jacob et al. (2018) đề xuất sự kết hợp của cấu trúc nơ-ron đệ quy và lặp lại thông qua học tăng cường cho sắp xếp phân cấp. Havrylov et al. (2019) mở rộng Gumbel Tree-LSTM (Choi et al., 2018) thông qua huấn luyện hợp tác riêng biệt của trình phân tích cú pháp (với học tăng cường) và hàm sắp xếp. Shen et al. (2019a) phát triển bộ nhớ có thứ tự (OM) bằng cách tổng hợp các nguyên tắc của nơ-ron có thứ tự (Shen et al., 2019b) với RNN tăng cường ngăn xếp. Mặc dù OM là đối thủ mạnh với CRvNN, chúng tôi đã thảo luận về lợi thế của CRvNN như một bộ xử lý chuỗi song song hóa hơn trong §5.1.

Theo hướng khác, Liu & Lapata (2018) áp dụng chú ý có cấu trúc (Kim et al., 2017) dựa trên cây phụ thuộc cảm ứng để tăng cường hiệu suất hạ nguồn. Niculae et al. (2018) giới thiệu SparseMAP để áp dụng marginalization trên một tập hợp các cấu trúc tiềm ẩn được làm thưa trong khi Corro & Titov (2019) thực hiện ước lượng Monte-Carlo bằng cách lấy mẫu ngẫu nhiên một cấu trúc cây phụ thuộc projective cho bộ mã hóa của họ. Những phương pháp này, tương tự như phương pháp của chúng tôi, có thể vi phân đầu cuối, nhưng không tập trung vào sắp xếp phân cấp của vector chuỗi (nút gốc) từ các phần tử (nút lá) trong chuỗi theo cấu trúc constituency tiềm ẩn của nó. Do đó, chúng không giải quyết những thách thức mà chúng tôi gặp phải ở đây.

Trong những năm gần đây, Transformer (Vaswani et al., 2017) cũng đã được mở rộng để hỗ trợ tốt hơn đầu vào có cấu trúc cây (Shiv & Quirk, 2019; Ahmed et al., 2019) hoặc để có thiên hướng quy nạp tốt hơn để cảm ứng cấu trúc phân cấp bằng cách ràng buộc self-attention (Wang et al., 2019; Nguyen et al., 2020; Shen et al., 2021) hoặc bằng cách đẩy biểu diễn trung gian để có thông tin constituency (Fei et al., 2020). Tuy nhiên, khả năng cơ bản của Transformer để sắp xếp chuỗi theo cấu trúc tiềm ẩn của chúng theo cách có thể tổng quát hóa độ dài được chứng minh là thiếu (Tran et al., 2018; Shen et al., 2019a; Hahn, 2020).

7. Kết luận và Hướng tương lai
Chúng tôi đề xuất một công thức hóa lại RvNN để cho phép phép nới lỏng liên tục của cấu trúc và thứ tự sắp xếp của nó. Kết quả là CRvNN, có thể cảm ứng cấu trúc động trong dữ liệu theo cách có thể vi phân đầu cuối. Một sự khác biệt quan trọng so với công trình trước đây là nó có thể xử lý song song nhiều vị trí ở mỗi bước đệ quy và cũng dừng động tính toán của nó khi cần thiết. Chúng tôi đánh giá CRvNN trên sáu tập dữ liệu, và thu được hiệu suất mạnh mẽ trên hầu hết chúng. Tuy nhiên, có một số hạn chế của mô hình. Đầu tiên, các hàm truy xuất hàng xóm trong CRvNN, xây dựng ma trận n×n (n là độ dài chuỗi) có thể tốn nhiều bộ nhớ. Điều này tương tự như hạn chế bộ nhớ của Transformer. Hạn chế khác là nó là mô hình tham lam. Nó cũng không được trang bị rõ ràng để xử lý sự mơ hồ cấu trúc trong ngôn ngữ tự nhiên. Trong công trình tương lai, chúng tôi sẽ giải quyết những hạn chế này. Để giảm thiểu hạn chế bộ nhớ, chúng tôi dự định ràng buộc các hàm truy xuất hàng xóm chỉ nhìn vào k ứng viên trái hoặc phải để chúng tôi chỉ cần ma trận n×k. Chúng ta có thể nén các vị trí liền kề với xác suất tồn tại thấp để chúng ta không cần nhìn xa hơn k. Để xử lý bản chất tham lam của nó, chúng tôi sẽ mở rộng CRvNN để theo nhiều đường dẫn đồng thời.

8. Lời cảm ơn
Chúng tôi muốn chân thành cảm ơn các nhà đánh giá về phản hồi xây dựng của họ đã giúp chúng tôi cải thiện bài báo của chúng tôi rất nhiều. Chúng tôi cũng cảm ơn Adrian Silvescu cho những cuộc thảo luận hữu ích. Nghiên cứu này được hỗ trợ một phần bởi giải thưởng NSF CAREER #1802358, giải thưởng NSF CRI #1823292, và một giải thưởng từ UIC Discovery Partners Institute. Tính toán cho dự án này được thực hiện trên Amazon Web Services.
