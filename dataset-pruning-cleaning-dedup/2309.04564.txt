# 2309.04564.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/dataset-pruning-cleaning-dedup/2309.04564.pdf
# File size: 1191622 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
When Less is More:
Investigating Data Pruning for Pretraining
LLMs at Scale
Max Marion
Cohere for AI
maxwell@cohere.comAhmet Üstün
Cohere for AI
ahmet@cohere.comLuiza Pozzobon
Cohere for AI
luiza@cohere.com
Alex Wang
Cohere
alexwang@cohere.comMarzieh Fadaee
Cohere for AI
marzieh@cohere.comSara Hooker
Cohere for AI
sarahooker@cohere.com
Abstract
Large volumes of text data have contributed significantly to the development of large language
models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading
to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down
to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In
this work, we take a wider view and explore scalable estimates of data quality that can be used to
systematically measure the quality of pretraining data. We perform a rigorous comparison at scale
of the simple data quality estimator of perplexity, as well as more sophisticated and computationally
intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and
prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets.
Surprisingly, we find that the simple technique of perplexity outperforms our more computationally
expensive scoring methods. We improve over our no-pruning baseline while training on as little
as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in
automatically curating high quality corpora and suggests the majority of pretraining data can be
removed while retaining performance.
1 Introduction
A reigning belief in machine learning is that more data leads to better performance. Recent years
of progress in scaling large language models (LLMs) have shown strong evidence to support this
with remarkable gains in language understanding and generation capabilities (Brown et al., 2020;
Touvron et al., 2023; Kaplan et al., 2020; Anil et al., 2023). When training language models,
common practice is to use massive datasets such as C4 (Raffel et al., 2020), RefinedWeb (Penedo
et al., 2023), and The Pile (Gao et al., 2021). These datasets are typically compiled by scraping
raw web pages from the internet, leading to a substantial portion of the text being noisy and of low
quality (Dodge et al., 2021; Kreutzer et al., 2022; Luccioni & Viviano, 2021).
Practitioners have established a number of standard filtering techniques to remove low-quality ex-
amples from these datasets. These techniques are predominantly rule-based heuristics: removing
1arXiv:2309.04564v1  [cs.CL]  8 Sep 2023

--- PAGE 2 ---
Figure 1: Demonstration of our pruning methodology.For each sequence zi, sized equally as the
model’s context length, a pruning algorithm ξgenerates score si. We then choose which subset of
the distribution of scores to keep: bottom, middle, or top. Finally, a new model is pretrained with
the pruned data ˆDξ.
documents containing repetitive text (Zhang et al., 2022; Raffel et al., 2020; Rae et al., 2022; Her-
nandez et al., 2022; Penedo et al., 2023), special characters, or non-English text (Wenzek et al.,
2020); ignoring data from a manually curated list of “blocklist” websites (Dodge et al., 2021; Rae
etal.,2022); oreliminatingdocumentsbasedoncertainlengththresholds. Whilethesehand-curated
filters can eliminate certain noisy examples, they are not a substitute for a measure of “quality” for
individual training examples, for which there are currently no established best practices (Mitchell
et al., 2023).
In this work, we take a wider view and ask if we can arrive at a rigorous estimator of data quality
throughdata pruning .
Data pruning attempts to isolate a subset of a larger training dataset such that a model trained on
said subset preserves or improves performance over a model trained on the full dataset. To date,
the majority of work on data pruning has centered on supervised computer vision settings (Qin
et al., 2023; Sorscher et al., 2023; Raju et al., 2021; Paul et al., 2023; He et al., 2023), with far fewer
works focusing on language. Those that have either studied the fine-tuning setting, which typically
has an order of magnitude less data and thus tolerates more computational complexity (Fayyaz
et al., 2022; Attendu & Corbeil, 2023; Cao et al., 2023) or based their method on hand picking high-
quality corpora (Gao, 2021; Wenzek et al., 2020; Brown et al., 2020). Specifically, we try to answer
the following: Can we remove the least impactful examples from a pretraining dataset and achieve
similar or better performance? Do simpler techniques for estimating data quality outperform more
sophisticated and computationally expensive methods? What aspects of training dynamics signal
data quality the best?
We answer these questions by rigorously evaluating three automatic pruning metrics. One simple
estimator of quality, perplexity, and two more complex, and EL2N (Paul et al., 2023) memorization
factor. These methods all rely solely on model outputs and do not require a preselected high-quality
2

--- PAGE 3 ---
dataset. This lack of dependence on human judgments of data quality make them a promising direc-
tion for automatic selection of high quality corpora. We perform extensive experiments evaluating
models ranging from 124M to 1.5B parameters across different pretrained corpora. Our contribu-
tions are the following:
1. We extensively benchmark data pruning based on perplexity, EL2N, and memorization in
the LLM pretraining setting. Surprisingly, we find the simple technique of ranking
examples based on their perplexity outperforms far more complex techniques
such as memorization. A model trained on 50% of the dataset pruned based on perplexity
achieves 1.33% and 1.77% improvement over the most performant models pruned to 50% of
the dataset with EL2N and memorization factor respectively. A model trained on 30% of
the dataset pruned with perplexity achieves a 2.1% and 1.6% improvement over the most
performant models pruned to 30% of the dataset with EL2N and memorization factor.
2. To comprehensively cover multiple facets of data pruning, we provide a unified and general
framework to identify and treat different data subsets present in a dataset. We compare mod-
els trained on datasets pruned to 10, 30, 50, and 70% of the training set while retaining either
thebottom,middle, ortopof the pruning scores’ distributions. We test seven different refer-
ence models across pruning variations, investigating the impact of parameter count, training
dataset, and total training steps on the reference models’ pruning capabilities. Finally, we
finetune a selection of our models on six tasks from the GLUE benchmark (Wang et al., 2019)
to evaluate the effect of pruning on downstream generalization.
3. We test our pruning methods at scale, achieving a 1% improvement in test set perplexity using
half of the dataset over a baseline model trained on the entire dataset. We show this scales to
1.5B parameter models, achieving 1.5% improvement in test set perplexity over a no-pruning
baseline of the same size.
2 Methodology
Given a large-scale dataset D, we tokenize all documents and append a special <eod>token to their
end. We then concatenate and split them into nsequences ziof fixed length tequal to the model’s
context length: D={z1, . . . , z n}. Consider the subset of training instances Pξwhere ξrefers to the
algorithm used to select the subset. We build this subset by computing the pruning score Score ξ(zi)
for each data point zi. We then populate Pξwith instances that fit our selection criteria:
Pξ={zi∈ D | Criteria (Score ξ(zi))} (1)
By removing PξfromD, the remaining instances are described as:
ˆDξ=D \ P ξ (2)
Our goal is to choose the pruning algorithm ξsuch that when training a language model on the
remaining subset of training instances, ˆDξ, the model’s performance is not diminished:
Pτ(MˆDξ)≥Pτ(MD) (3)
where MˆDξis the model trained on ˆDξandPτis the performance on task τ. We explore three
metrics, perplexity, Error L2-Norm (EL2N), and memorization which we detail below in Section
2.1, and evaluate the different ways in which the metric can be employed to determine Pξ.
3

--- PAGE 4 ---
In particular, we evaluate different reference models ˜Mthat are used to calculate pruning scores.
Bothreferencemodels ˜Mandtrainedmodels Msharethesamecontextlengthtoensureconsistency
between the contexts for which pruning metrics are calculated and trained models are trained.
For each metric, we consider three different selection criteria to determine Pξas seen in Equation
1: isolating the top,middle, or bottompercentiles of Das the data to be kept. We pretrain
separate models using these criteria with different percentages of the dataset to understand the
dynamics and impact of each pruning metric. Since the effectiveness of these metrics in this specific
context remains uncertain, we opt for these contrasting subsets to clarify the relationship between
each metric and the overall model performance. Figure 1 demonstrates our experimental setup. We
focus on static pruning, in which data is pruned once before training. This is in contrast to adaptive
pruning, in which data is pruned as training is happening, such as in (Fayyaz et al., 2022; Park
et al., 2022).
2.1 Pruning Methods
Here, we briefly describe data pruning algorithms that we benchmark in this work. Our goal is to
rigorously compare simple and computationally inexpensive ranking approaches such as perplex-
ityandrandom ranking against more sophisticated and computationally expensive techniques
such as memorization scores and EL2N.
2.1.1 Selection via Perplexity
Perplexity measures how probable a given piece of text is based on a particular language model.
For each instance ziinD, we compute the perplexity metric as:
PPL (zi) = exp 1
|zi|X
tj∈ziNLL (tj)
(4)
where NLL (tj)is the negative log likelihood of token tjin sequence zi:
NLL (tj) =−logP(tj|t<j;θ) (5)
A lower perplexity score indicates that the model assigns a high probability to the text.
2.1.2 Selection via EL2N
The Error L2-Norm ( EL2N) score was originally proposed in a computer vision setting to identify
which samples are important for learning (Paul et al., 2023). It measures each sample’s importance
using the model’s early learning signals. We define the EL2Nscore on text sequences as the average
L2norm of the error vector, where ˆyiis the reference model’s predicted probability distribution over
the vocabulary and ytis the one-hot encoded representation of the ground truth:
EL2N (zi) =1
ttX
i∥ˆyt−yt∥2 (6)
We first evaluate the pruning efficacy of EL2Nscores obtained from a single reference model at two
different checkpoints, trained on 14% and 55% of the training dataset Dcorresponding to 250 and
4

--- PAGE 5 ---
1000 steps respectively, to determine the required number of steps needed before a usable pruning
signal emerges. We then train ten different reference models with different random initializations
and average the EL2N score from all ten models to obtain our final EL2N score. The authors
suggest that exhibiting a low EL2Nscore are typically those the model learns in its early stages
of training, likely because they are relatively easier. Inversely, examples with higher EL2Nscores
are hypothesized to indicate that the model continues to incur a significant loss for them and may
require additional iterations to learn.
2.1.3 Memorization Ranking
Memorizationinlanguagemodelsisawell-studiedphenomenon(Carlinietal.,2023;2021;Biderman
et al., 2023a). In this work we explore memorization scores applied as a data pruning ranking. We
use the memorization score as defined by Biderman et al. (2023a):
score (M, N ) =1
NNX
i1(zM+i= ˆzM+i) (7)
where zis a data point, ˆzis a sequence of tokens predicted by the reference model, and 1(·)is
an indicator function. A reference model is prompted with the first Mtokens of a data point
zto calculate the memorization score. We then greedily generate Nadditional tokens, ˆz. The
memorization score is the fraction of the Ngreedily generated tokens ( ˆzM:M+N) that match exactly
with the original data point ( zM:M+N). For our experiments, M=N= 32. We note that the
authors did not originally propose this as data pruning metric, but we hypothesize that it can be a
valuable ranking to identity examples which require additional learning. We use reference models
guaranteed to have seen the full training set to ensure the applicability of memorization scores. A
high memorization score indicates the model reproduces more of the text verbatim.
2.1.4 Random Pruning
We also evaluate a lower bound of expected performance: pruning a random selection of samples.
This allows us to ask the question “are proposed pruning methods any better than a random guess ?”
3 Experiments
3.1 Model
We train autoregressive decoder-only Transformer models (Vaswani et al., 2023) with a standard
language modeling objective. Given an input sequence of zi= [r1,···, rt]from training data D, a
language model with parameters θis trained to minimize the negative log-likelihood loss as defined
in Equation 5. Our language models follow the traditional GPT-style architecture (Radford et al.,
2018).
While training our models, we use AdamW (Loshchilov & Hutter, 2019) with linear cosine scaling
and a batch size of 2048. The 124M parameter models are trained for 8000 steps, which amounts to
a total of 33B tokens with a learning rate that linearly increases from 0 to 1.5e-4 over the course of
training. This is approximately 4.4 epochs over the unpruned dataset. We tokenize the data with
Byte Pair Encoding (Sennrich et al., 2016) with a vocabulary of 51200. Due to the memory and
5

--- PAGE 6 ---
Experimental axes Choices
Pruning Metric Perplexity, EL2N, Memorization
Pct. Data Remaining 10, 30, 50, 70
Pruning Subset Bottom, Middle, Top
Reference Model Size 124M, 6B, 13B, 52B
Reference Model Epoch Perc. 14%, 55%, 440%, Full
Reference Model Tr. Data CC, Wiki, Web-scale
Trained Model Size 124M, 1.5B
Table 1: Pruning choices explored in the experiments. Under “Reference Model Training Steps”,
“Full” refers to the fully trained Cohere LLMs. Under “Reference Model Training Data”, “Web-scale”
refers to the significantly larger training datasets used by the Cohere reference models.
computational costs of training 1.5B parameter models, our experiments at this size are trained
with a batch size of 512 for 14568 steps. As such, the models see only 7.6B tokens, equivalent
to a single epoch of our unpruned dataset. The learning rate for 1.5B parameter models linearly
increases from 0 to 1.2e-4 over the course of training. All models use a context window length of
2048.
3.2 Data
We use a random sample of the May 2022 snapshot of CommonCrawl1in our experiments. After
downsampling the unpruned dataset has 7.6B tokens, about 20% of the full snapshot. This down-
sampling is required due to the computational cost of our various ablation experiments, which each
require pretraining a new model from random initialization. This dataset is prefiltered using a com-
bination of automatic and hand-crafted filters, as we aim to further improve data quality beyond
common rule-based filters. The filters exclude repetitive documents, documents with percentages
of special characters, and documents that contain explicit words and toxic text, similar to dedupli-
cation steps seen in Taylor et al. (2022); Kocetkov et al. (2022). Our Wikipedia dataset contains
5.3M tokens and only includes English pages.
3.3 Ablations
For all techniques, we compare performance when only 10%, 30%, 50%, and 70% of all data is
preserved. We compare retaining the top,middle, and bottomsubsets according to the pruning
ranking, e.g., when retaining 30% of the bottom of the pruning metric’s distribution over the
training set, we calculate the 30th percentile of the pruning metric’s distribution and remove all
data points with perplexity above it. When retaining the middle30%, we calculate the 35th
and 65th percentile and remove all data points above and below those numbers respectively. Each
ablation study(pruning method, percent data remaining, section of distribution preserved) requires
training a new model from random initialization . We train a minimum of nine models with
124M parameters from scratch for each experimental variant.
Table 1 summarizes the perplexity pruning variations we explore in this paper. For perplexity, we
1https://data.commoncrawl.org/
6

--- PAGE 7 ---
use a separate model to compute perplexity from the model trained on the pruned data. We call
models used to compute the perplexity ranking reference models and the models trained on the
pruned datasets pruned models . We conduct a rigorous evaluation of what impacts the quality of
the ranking by varying different factors that affect the perplexity distribution:
1.Reference Model Size To explore how reference model size impacts the rating quality,
we compare perplexity computations using 6B, 13B, and 52B Cohere models trained on full
web-scale datasets.
2.Reference Model Training Data To isolate the impact of training data, we compute per-
plexity using 124M parameter reference models trained on either CommonCrawl or Wikipedia.
3.Total Reference Model Training Steps To isolate the impact of early training signals,
we compute perplexity and EL2N using 124M parameter models trained on CommonCrawl
data for approximately 14% and 55% of total training steps. Reference models trained on
CommonCrawl are trained on a non-overlapping subset from the CommonCrawl dataset that
is pruned and used to train the student model.
3.4 Evaluation
We report perplexity on a test set from the same CommonCrawl snapshot with identical prefiltering
as the training data. This test set contains 266M tokens, equivalent to about 3.5% of the training
set.
We also finetune a subset of our models on six different classification tasks from GLUE (Wang et al.,
2019).We do not prune the task dataset, as our aim is to analyze the pruning methods’ effects on
pretraining. We compare performance after 8000 steps (approximately 4.4 epochs of the pretraining
dataset), chosen to compare performance after models have saturated their capacity by training
enough steps to plateau on validation metrics.
4 Results and Discussion
4.1 Removing Easy Instances Improves Performance
Though the most competitive variant for each pruning method varies based on the subset of the
scoring distribution retained ( top,middle, orbottom), we observe a consistent pattern: the highest
performant variants are notthe subsets that correspond to the “easier” data. The interpretation of
the term “easy” varies according to the measurement employed. When employing the Perplexity
metric, it refers to the bottomsamples with the lowest perplexity. With the EL2Nmetric, it also
pertains to the bottomsamples exhibiting the lowest initial loss. In the context of memorization ,
it relates to the topsamples that have been most thoroughly memorized.
Figure 2 demonstrates this pattern when using Perplexity . In contrast to the middleortop
subsets, the bottomsubset has much less variance in results between reference models of varying
sizes, indicating the bottomsubset may not be suitable for training. The middleexperiments
achieve consistently low test set perplexities for various reference model sizes and pruning ratios.
Generally, performance monotonically degrades as the amount of data remaining shrinks - except
7

--- PAGE 8 ---
Figure 2: The effect of employing reference models of different sizes on the computation of pruning
perplexity scores and its subsequent influence on test set perplexity. The three subset selection
approaches for each set of experiments are showcased separately (keeping bottom,middle, ortop
of the pruning score distribution).
for the middlesubset for the best-performing reference models. In these cases, retaining only 50%
and even 30% of the dataset outperforms retaining 70% of the dataset.
Next, Figure 3b(a) shows the results for the EL2Nmetric.The middlesubset is also the best variant
forEL2N. While the best performing run does not outperform the baseline, the best performance is
achieved when retaining 50% of the middlesubset, outperforming the model trained on 70% of the
dataset, similar to the results when using perplexity . As the middlesubset grows, it begins to
overlap with the easiest examples, degrading performance. In section 4.5, we discuss how different
model checkpoints influence the effectiveness of the EL2Nmetric.
Finally, when using memorization factor as a pruning metric, keeping the least memorized
samples ( bottomsubset) generally performs best. Figure 3b(b) shows model performances for this
metric. We observe that the most competitive variant of the memorization metric is the bottom
70% of the distribution. Memorization never outperforms the no-pruning baseline.
(a) EL2N
 (b) Memorization
Figure 3: Evaluation of different subset selection criteria for two pruning metrics: (a) EL2N and
(b) Memorization.
8

--- PAGE 9 ---
4.2 Simple Pruning Metrics Outperform More Sophisticated Approaches
In Figure 4 we present results comparing the performance of the best variant of each pruning
metric: (1) retaining the middleof the distribution of Perplexity scores by the fully trained 52B
reference model, (2) retaining the bottomof the distribution of the Memorization Factor (least
memorized samples), and (3) retaining the middleof the distribution of EL2Nscores from the
1000 step checkpoint. We also include results for our baselines: a model trained on the entirety of
the training data Dand models trained on randomly pruned data. Our results show that training
on the middlesubset using Perplexity outperforms other pruning metrics across all dataset
sizes. For some variants, it also outperforms training on the entire dataset. For example, at 30%
and 50% of the original dataset size, Perplexity outperforms the full dataset size. Compared
with the no-pruning baseline, pruning to the middle50% of the perplexity distribution leads to a
0.97% improvement in perplexity. Using only the middle30% of the data achieves nearly the same
performance, with a 0.80% improvement over the no-pruning baseline.
Figure 4: The top performing variants of the different pruning methods, compared across various
dataset sizes. Random pruning and no-pruning are included as baselines. Perplexity-based pruning
consistently surpasses both alternative metrics and the no pruning experiments. See Section 4.2 for
details on the featured variants.
Compared with random selection, pruning using Perplexity results in significantly higher model
performance than random pruning across all data ratios (Figure 4). For memorization andEL2N
pruning metrics, both achieve similar performances to random pruning despite being far more
computationally expensive.
4.3 Pruning Benefits from Using Larger Reference Models
Given that the most competitive variant perplexity uses a reference model to compute scores, we
expect that the size of the reference model will have a significant impact on the data pruned.
Figure 2 shows the trained model performances after pruning with perplexity calculated with
reference models ranging from 124M to 52B parameters. We find that increasing reference model
9

--- PAGE 10 ---
Figure 5: Performance of different pruning strategies using two different reference models: one
trained on Wikipedia and one trained on CommonCrawl. A reference model trained on Wikipedia
(an example of a clean noise-free corpus) achieves consistently lower validation perplexity compared
to a reference model trained on a noisier CommonCrawl in our two robust settings ( middleand
top).
size improves trained model performance over the no-pruning baseline when either the middleor
topsubsets are used. Data pruning using the perplexity scores generated from a 52B parameter
reference model achieves a 2.2% improvement in perplexity over the best-performing trained model
from the 124M parameter reference model experiments. Furthermore, for 13B and 52B reference
models, we observe better performances with less training data when keeping the middle and top
subsets. For both of these larger models, retaining the middle30% and 50% of the training data
produces pruned models that outperform the pruned models trained on the middle70% of the
training set.
We note that the effects of subset selection, such as the bottomsubset performing worse, approx-
imately scale with the size of the reference models. The larger reference models’ bottomsubset
training runs perform even worse than their smaller counterparts when retaining the same percent-
age of the training set. This overall points to the consistent finding that larger models are better
calibrated at computing a useful data pruning ranking.
4.4 Improved Pruning Signals Result from Reference Models Trained on Cleaner
Data
In this section we ask: does the data the reference model is trained on impact the quality of the
ranking? Wecomparetheperplexityrankingsgeneratedbyreferencemodelstrainedontwodifferent
corpora: Wikipedia and CommonCrawl. We investigate whether a model trained on Wikipedia, a
dataset frequently hand-picked as a high-quality dataset (Xie et al., 2023b; Wenzek et al., 2020),
generates more effective pruning signals for perplexity rankings. In Figure 5, we compare the
performance of the two variants across different pruning percentages and subset selections. We
observe that in the two optimal selection variants from the general reference models ( middleand
top) a model trained on Wikipedia consistently yields lower validation perplexity compared to a
model trained on CommonCrawl. Wikipedia’s best variant, pruning to the middle 70%, outperforms
10

--- PAGE 11 ---
CommonCrawl’s best variant, also pruning to the middle 70%, by 0.69%. This finding overall
suggeststhatinvestinginahighqualityreferencemodeltogeneraterankingsresultsinmoreeffective
data pruning. Reference models trained on higher quality data are better at identifying a subset of
data points most conducive to model performance.
4.5 Early Reference Model Checkpoints Serve as Effective Scoring Models
Figure 6: The impact of using an early checkpoint of the reference model in pruning based on
Perplexity and EL2N metrics.
Motivated by several works that have found that there is a signal in early training checkpoints (Paul
et al., 2023; Agarwal et al., 2022; Siddiqui et al., 2022), we investigate whether early checkpoint of a
reference model during training offers adequate signal for calculating discriminative pruning scores.
We study perplexity andEL2Nscores obtained from two early checkpoints: after training on
approximately 14% and 55% of the full training dataset (250 and 1000 training steps respectively).
Figure 6 showcases the results of these experiments. Examining the 14% checkpoint for both
perplexity and EL2N, we notice minimal variance across percentages and subset selection criteria.
Performance across subsets changes considerably less than either the 55% checkpoint or the fully
trained models.
Given this, we deduce that training on only 14% of the data is inadequate for our reference model to
offer precise pruning scores. In contrast, the 55% reference models perform in a similar manner to
the fully trained models, performing best with the middlesubset, worst with the bottomsubset, and
comparably with the topsubset. Fully training the reference model is shown not to be necessary
to uphold comparable performance. Halving the reference model training steps proves effective,
enabling the utilization of early checkpoints. In practice, we expect many practitioners to use off
the shelf models for computing perplexity and may not need to carry the cost of pretraining a
reference model from random initialization.
We also show performance for EL2N scores averaged across 10 reference models, initialized with
different random seeds. We selected the 55% reference models given our previous result.
While the best pruned models using the averaged EL2N score did not outperform the best pruned
models trained on only one reference model’s EL2N score, the pattern of performance more similarly
mirrors what we see with the larger, fully trained reference models. Specifically, in the middle
subset, using 50% of the dataset outperforms using 70%. When constrained to the bottomsubset,
performance more clearly monotonically degrades when using less data than when using the 55%
reference model, whereas the earlier checkpoint has comparable performance when retaining 30, 50,
and 70% of the data. This implies that averaging scores across reference models helps hone the
11

--- PAGE 12 ---
pruning signal, identifying subsets “easy" or “hard" subsets in more similar ways to larger models.
4.6 Perplexity-based Pruning Improvements Generalize to Larger Scale Models
Figure 7: Comparing the best performing pruning method (keeping the middlesubset using a
52B parameter reference model) with random pruning at two distinct pruned model scales. The
improvement in performance of a perplexity-based pruning approach carries from 124M to 1.5B
parameter models.
We take our strongest pruning variant – perplexity computed using a 52B parameter reference
model while retaining the middlesubset – to explore the robustness of our findings at a larger scale
by validating our findings on a 1.5B model. Figure 7 shows pruning scaling from 124M to 1.5B
parameter models. Training a 1.5B model, we observe that random pruning performs considerably
well, even reaching levels below the no-pruning run. Nonetheless, perplexity-based pruning achieves
better results than random pruning across all pruning percentages. The improvement observed with
perplexity-based pruning over random pruning follows a consistent pattern for both the 124M and
1.5B models. This demonstrates the scalability of our approach to a large-scale pretraining setting.
4.7 Downstream Evaluation on GLUE
Previously, we demonstrated various ways of pruning the pretraining data and training models
with different data sizes. Considering that the pretraining stage primarily focuses on knowledge
acquisition (Zhou et al., 2023), we inquire about the potential ripple effects of pruning data during
pretraining when these models are subsequently finetuned on downstream tasks. To analyze the
impact of different pruning strategies on LLM capabilities, we finetune and evaluate models on a
subset of the GLUE tasks (Wang et al., 2019). Results are presented in Table 2. We observe that
pruning the pretraining dataset consistently improves performance across all tasks. While no single
pruning strategy (combining both pruning metric and percentage of remaining data) stands out as
superior across all tasks, the absence of a universally dominant approach is consistent with earlier
findings in the literature (Gao, 2021). We observe that retaining only 30% of the least memorized
instances yields optimal results for SST2 and WNLI tasks. With perplexity based pruning, the
best performance is obtained on QQP and QNLI tasks by keeping 50% and 70% of the training
data, respectively. Even random pruning shows improvements in certain tasks, underscoring the
significance of downsampling when handling noisy data during the pretraining stage to mitigate
12

--- PAGE 13 ---
Table 2: Mean accuracy and standard deviation of the best variants of each pruning algorithm for
GLUE classification tasks. Underlined results surpass the baseline performance with no pruning.
The best results for each task are marked in bold. Results are reported for 5 runs of each model,
trained for 3 epochs with a learning rate of 1e−5.
Data Remaining SST2 MRPC QQP QNLI RTE WNLI
No Pruning 100% 78.15 0.00264.32 0.02176.55 0.00165.40 0.00649.69 0.02451.56 0.040
Random
Pruning70% 77.92 0.00265.21 0.01776.58 0.00265.11 0.00649.69 0.01348.44 0.038
50% 78.19 0.00365.16 0.02076.40 0.00165.44 0.00649.92 0.00949.69 0.062
30% 77.29 0.00766.04 0.01776.36 0.00165.22 0.00551.33 0.02450.31 0.057
10% 76.44 0.00665.83 0.02175.91 0.00164.40 0.00750.70 0.00750.62 0.016
Memorization
Bottom subset70% 77.29 0.00664.38 0.01676.42 0.00166.03 0.00749.06 0.02149.06 0.042
50% 77.89 0.00665.47 0.01776.51 0.00165.99 0.00549.77 0.01350.31 0.048
30% 78.52 0.00465.89 0.01676.48 0.00165.91 0.00650.31 0.00954.38 0.061
10% 76.64 0.00465.16 0.01576.11 0.00164.61 0.00650.39 0.01651.88 0.059
EL2N
Middle subset70% 78.61 0.00866.46 0.01876.93 0.00167.00 0.00548.67 0.01750.00 0.058
50% 79.17 0.00765.42 0.01676.35 0.00162.43 0.00751.41 0.02851.56 0.049
30% 78.98 0.00565.41 0.01277.47 0.00168.63 0.00549.69 0.02255.31 0.067
10% 78.31 0.00663.38 0.01676.93 0.00165.34 0.00651.95 0.02151.25 0.064
Perplexity (52B)
Middle subset70% 78.40 0.00464.43 0.02076.68 0.00166.74 0.00750.16 0.02349.06 0.012
50% 78.01 0.00664.37 0.02176.82 0.00166.00 0.00450.62 0.02350.31 0.021
30% 77.34 0.00564.84 0.02376.76 0.00165.89 0.00250.86 0.00950.94 0.031
10% 77.66 0.00665.36 0.01776.40 0.00166.52 0.00751.17 0.01253.44 0.040
potential learning degradation.
5 Related Work
5.1 Rule-Based Data Pruning in NLP
Significant portions of web-scraped data used for language model pretraining have been shown to
be of low quality, machine-generated spam, pornographic content (Kreutzer et al., 2022). Selection
processes to determine what should be included in large-scale datasets have centered on rule-based
filters and heuristics (Bane et al., 2022), such as keeping only text written in English (Raffel et al.,
2020; Rae et al., 2022) or removing sequences containing blocklisted words (Raffel et al., 2020).
There are also quality-based rules such as removing duplicated samples (Zhang et al., 2022) or
filtering sentences that do not fit a certain amount of words (Raffel et al., 2020; Rae et al., 2022).
Rule-based approaches for data filtering have shown controversial effects on model performance,
with some works advertising improvements on language modeling capabilities (Penedo et al., 2023;
Raffel et al., 2020), while others do not (Black et al., 2022; Biderman et al., 2023b). Also, heuristics
are prone to undesired outcomes due to their simplicity. For instance Dodge et al. (2021) show how
removing blocklisted words disproportionately removes text from and about minority individuals.
5.2 Metric-Based Data Pruning in NLP
Recent work on metric-based pruning has mainly focused on pruning data from the fine-tuning stage
of LLMs (Attendu & Corbeil, 2023; Xie et al., 2023b) most probably due to the prohibitive cost of
13

--- PAGE 14 ---
pruning at the pretraining scale. Attendu & Corbeil (2023) perform dynamic pruning during the
fine-tuning stage by establishing a curriculum of samples based on their EL2N scores (Paul et al.,
2023). Similarly, we benchmark EL2N as a static data-pruning metric for language datasets. Our
work joins the few others that aim to reduce pretraining dataset sizes (Xie et al., 2023a; Chen, 2023;
Abbas et al., 2023). Abbas et al. (2023) apply their deduplication method based on embeddings
to further improve the performance of a previously filtered dataset. We also perform pruning on
previously filtered datasets, aiming to enhance performance further. Previously, perplexity has
been used to filter datasets (Muennighoff et al., 2023; Wenzek et al., 2020; Laurençon et al., 2023),
but its pruning capabilities have been underexplored. Laurençon et al. (2023) and Muennighoff
et al. (2023) filter out high-perplexity samples from their corpus as those are framed as unnatural
language and harmful for performance according to their reference domain, which is Wikipedia. In
contrast, we benchmark pruning to low perplexity values and high and medium-valued subsets of a
dataset’s distribution to understand which is the most valuable section for pretraining at scale. We
also explore different reference model sizes and training sets.
5.3 Data pruning in Computer Vision
The majority of work to date on data pruning (Sorscher et al., 2023) and isolating data subsets
(Siddiqui et al., 2022; Mindermann et al., 2022) using model signal has centered on computer vision.
These are typically structured in a supervised setting. In contrast, our focus is on a large-scale NLP
pretraining where the objective is unsupervised pretraining. Most relevant to our method is work by
Sorscher et al. (2023) which empirically studies reducing datasets in a teacher/trained regime, using
a teacher model’s margin as a pruning metric. They find that, with abundant data, training only
on the hardest examples yields better performance, while conversely when data is scarce, training
on only the easiest example yields better performance.
6 Conclusion
In this study, we thoroughly investigate diverse data pruning methods for pretraining LLMs with
billionsofparametersandwithdatasetscontainingbillionsoftokens. Weshowedthatwhenproperly
applied, data pruning consistently improves model performance. We also find that training on the
“easiest"examplesinadatasetdegradesperformance, where“easiest"isdefinedasthelowestscoring
examples according to a metric based on a reference model. Simple methods that rank instances
based on their perplexity demonstrate superior performance compared to more elaborate approaches
such as memorization. Models trained on as little as half of the data selected by perplexity achieve
up to 1.5% improvement over models trained on the full dataset. Additionally, we establish the
consistency of our findings as we scale the model sizes. While scaling up the amount of data LLMs
are trained on remains a popular avenue for improving models, our work demonstrates that carefully
pruning these large training corpora is also a fruitful direction for making models better.
References
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup:
Data-efficient learning at web-scale through semantic deduplication, 2023.
Chirag Agarwal, Daniel D’souza, and Sara Hooker. Estimating example difficulty using variance
14

--- PAGE 15 ---
of gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 10368–10378, June 2022.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Brad-
bury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christo-
pher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa De-
hghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy
Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,
Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,
Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-
cello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary
Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex
Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros,
Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov,
David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yun-
han Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang
Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.
Jean-Michel Attendu and Jean-Philippe Corbeil. Nlu on data diets: Dynamic data subset selection
for nlp classification tasks, 2023.
Fred Bane, Celia Soler Uguet, Wiktor Stribiżew, and Anna Zaretskaya. A comparison of data
filtering methods for neural machine translation. In Proceedings of the 15th Biennial Conference
of the Association for Machine Translation in the Americas (Volume 2: Users and Providers
Track and Government Track) , pp. 313–325, Orlando, USA, September 2022. Association for
Machine Translation in the Americas. URL https://aclanthology.org/2022.amta-upg.22 .
Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,
Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language
models, 2023a.
StellaBiderman, HaileySchoelkopf, QuentinAnthony, HerbieBradley, KyleO’Brien, EricHallahan,
MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,AviyaSkowron,
Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models
across training and scaling, 2023b.
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-
race He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-
NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode
#5 – Workshop on Challenges & Perspectives in Creating Large Language Models , pp. 95–136,
virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.b
igscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9 .
15

--- PAGE 16 ---
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data
selection for large language models, 2023.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.
Extracting training data from large language models, 2021.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan
Zhang. Quantifying memorization across neural language models, 2023.
Wenhu Chen. Large language models are few(1)-shot table reasoners. In Findings of the Associa-
tion for Computational Linguistics: EACL 2023 , pp. 1120–1130, Dubrovnik, Croatia, May 2023.
Association for Computational Linguistics. URL https://aclanthology.org/2023.findings
-eacl.83 .
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld,
Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the
colossal clean crawled corpus, 2021.
Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Mohammad Taher Pilehvar, Yadollah
Yaghoobzadeh, and Samira Ebrahimi Kahou. Bert on a data diet: Finding important exam-
ples by gradient-based pruning, 2022.
Leo Gao. An empirical exploration in quality filtering of text data, 2021.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL
https://arxiv.org/abs/2101.00027 .
Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. Large-scale dataset pruning with dynamic
uncertainty, 2023.
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nel-
son Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris
Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish.
Scaling laws and interpretability of learning from repeated data, 2022.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models, 2020.
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis,
Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von
Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code, 2022.
16

--- PAGE 17 ---
JuliaKreutzer, IsaacCaswell, LisaWang, AhsanWahab, DaanvanEsch, NasanbayarUlzii-Orshikh,
Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan,
Supheakmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera, Annette Rios, Isabel Pa-
padimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo
Rubungo, Toan Q. Nguyen, Mathias Müller, André Müller, Shamsuddeen Hassan Muhammad,
Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin
Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-
ture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Ballı, Stella Biderman,
Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele
Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa
Adeyemi. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions
of the Association for Computational Linguistics , 10:50–72, 01 2022. ISSN 2307-387X. doi:
10.1162/tacl_a_00447. URL https://doi.org/10.1162/tacl_a_00447 .
Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral,
Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen,
Jörg Frohberg, Mario Šaško, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella
Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen,
Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan
Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Muñoz, Jian Zhu, Daniel Van
Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle
Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan,
Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell,
Sasha Alexandra Luccioni, and Yacine Jernite. The bigscience roots corpus: A 1.6tb composite
multilingual dataset, 2023.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .
Alexandra Luccioni and Joseph Viviano. What’s in the box? an analysis of undesirable con-
tent in the Common Crawl corpus. In Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers) , pp. 182–189, Online, August 2021. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.24. URL https:
//aclanthology.org/2021.acl-short.24 .
Sören Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie
Xu, Benedikt Höltgen, Aidan N. Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal.
Prioritized training on points that are learnable, worth learning, and not yet learnt, 2022.
Margaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina
McMillan-Major, EzinwanneOzoani, NazneenRajani, TristanThrush, YacineJernite, andDouwe
Kiela. Measuring data, 2023.
NiklasMuennighoff, AlexanderM.Rush, BoazBarak, TevenLeScao, AleksandraPiktus, Nouamane
Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models,
2023.
Dongmin Park, Dimitris Papailiopoulos, and Kangwook Lee. Active learning is a strong baseline
for data subset selection. In Has it Trained Yet? NeurIPS 2022 Workshop , 2022. URL https:
//openreview.net/forum?id=PAgpyQ5rGS .
17

--- PAGE 18 ---
Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:
Finding important examples early in training, 2023.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb
dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.
Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Daquan Zhou, and Yang You.
Infobatch: Lossless training speed up by unbiased dynamic data pruning, 2023.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language un-
derstanding by generative pre-training. 2018.
JackW.Rae, SebastianBorgeaud, TrevorCai, KatieMillican, JordanHoffmann, FrancisSong, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,
Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,
Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron
Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen
Simonyan, MichelaPaganini, LaurentSifre, LenaMartens, XiangLorraineLi, AdhigunaKuncoro,
Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,
Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume,
Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
AureliaGuy, ChrisJones, JamesBradbury, MatthewJohnson, BlakeHechtman, LauraWeidinger,
Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,
and Geoffrey Irving. Scaling language models: Methods, analysis and insights from training
gopher, 2022.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer, 2020.
Ravi S Raju, Kyle Daruwalla, and Mikko Lipasti. Accelerating deep learning with dynamic data
pruning, 2021.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units, 2016.
Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara Hooker.
Metadata archaeology: Unearthing data subsets by leveraging training dynamics, 2022.
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural
scaling laws: beating power law scaling via data pruning, 2023.
RossTaylor, MarcinKardas, GuillemCucurull, ThomasScialom, AnthonyHartshorn, ElvisSaravia,
Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for
science, 2022.
18

--- PAGE 19 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán,
Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from
web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference ,
pp. 4003–4012, Marseille, France, May 2020. European Language Resources Association. ISBN
979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494 .
Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang,
Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up
language model pretraining. arXiv preprint arXiv:2305.10429 , 2023a.
Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language
models via importance resampling, 2023b.
SusanZhang, StephenRoller, NamanGoyal, MikelArtetxe, MoyaChen, ShuohuiChen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
Opt: Open pre-trained transformer language models, 2022.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.
Lima: Less is more for alignment, 2023.
A Metric Distributions
We present the total distributions of the pruning metrics used in our analysis in Figure 8.
B Examples from different selection criteria
Examples from the pretraining data, drawn from distinct subsets (keep bottom, keep middle, keep
top), are presented in Tables 3, 4, 5, 6, and 7, with rankings based on perplexity.
19

--- PAGE 20 ---
(a) Distributions of Perplexity from different reference models. The dotted lines are placed at each 10th
percentile. Please note the differences in axes between graphs. Fewer than .1% of examples on the extreme
high end have been truncate to better display the overall distribution
(b)DistributionsoftheEL2NandMemorizationFac-
tor metrics. The dotted lines are placed at each 10th
percentileandomittedfromMemorizationFactordue
to overlap. Please note the log-scaled y-axis.
(c) Distributions of Perplexity from reference models
trained on Wikipedia and CommonCrawl. The Com-
monCrawl model is the same as the 124M parameter
model in Figure 8a. The dotted lines are placed at
each 10th percentile.
Figure 8: Distributions of different pruning metrics and reference models.
20

--- PAGE 21 ---
Table 3: Samples from different distribution subsets using perplexity of a 52B reference model
trained on CommonCrawl.
Bottom 10% Middle 10% Top 10%
Submissions, you hereby grant
Company a license to translate,
modify (for technical purposes,
for example making sure your
content is viewable on an iPhone
as well as a computer) and repro-
duce and otherwise act with re-
spect to such User Submissions,
in each case to enable us to op-
erate the Services, as described
in more detail below. This is a
license only – your ownership in
User Submissions is [...]House Municipal Heritage Build-
ing is a two-storey, wooden, ver-
nacular building with a low-
hipped roof, and is located at
the Norris Point Lookout, 104
Main Road, Norris Point, New-
foundland and Labrador. The
former family dwelling now oper-
ates as a heritage museum with
a view of the Tablelands of Gros
Morne National Park located on
the great Northern Peninsula.
The municipal heritage designa-
tion [...]and a nice book as a nice price.
Postage is via Royal Mail 1st
ClassintheUK.Ifyouarebuying
from overseas then please contact
me before completing your pur-
chase for a quote. I will always
combine P&P so if ordering mul-
tiplebooks, pleasewaitforthein-
voice so that discounts can be ap-
plied. We are slowly populating
our store with post war Wisden’s
so if there is anything you need
that [...]
provided on the Site is not in-
tended for distribution to or
use by any person or entity in
any jurisdiction or country where
such distribution or use would
be contrary to law or regula-
tion or which would subject us
to any registration requirement
within such jurisdiction or coun-
try. Accordingly, those persons
who choose to access the Site
from other locations do so on
their own initiative and are [...]selection of fuel type and in-
put of soot index, coefficient of
fuel, selection of measurement
units, input of date and time
with keyboard and via RS232
or RS485 Procedure of indus-
trial emissions monitoring with
the use of AHKAT-410 has been
agreed in FSUE "SRI Atmo-
sphere" AHKAT-410-16 is ap-
proved for diesel locomotive and
diesel train emission monitoring
at environment monitoring sta-
tions in [...]can be returned up to 28 days af-
ter the date of purchase. Please
note, we cannot offer refunds on
beauty, pierced jewellery or on
swimwear if the hygiene seal is
not in place or has been broken.
We now offer FREE label-free re-
turns with InPost Lockers (avail-
able 24/7), FREE Doddle Re-
turns to all UK customers as well
as a FREE UK Collect+ returns
service via over 5,900 local stores
nationwide.[...]
license only – your ownership
in User Submissions is not af-
fected. You agree that the li-
censes you grant are royalty-free,
perpetual, sublicensable, irrevo-
cable, and worldwide. Any infor-
mationorcontentpubliclyposted
or privately transmitted through
the Services is the sole responsi-
bility of the person from whom
such content originated, and you
access all such information and
content at your [...]1 1/2 " steel plate, all weld
construction Hammer mill ma-
chine manufacturers, suppliers,
exporters, dealers and traders
in India and worldwide hammer
mill machines from Gujarat and
Mumbai since 1960 as per the
ISO standards with required in-
dustrial features and specifica-
tions Replaceable bar type grate
is available for specific applica-
tions SPECIFICATIONS : Ham-
mer stone crusher is a kind of
equip [...]several turns. Nearly a month
after a foreclosure lawsuit was
filedagainstFreestyleMusicPark
and its parent company, more
than a dozen former department
heads have sued seeking more
than $232,000 in unpaid wages
and bonuses, according to court
papers filed late Friday. Seven-
teenemployeesarelistedasplain-
tiffs. Backpay I can understand,
but can you honestly expect any
kind of bonuses [...]
21

--- PAGE 22 ---
Table 4: Samples from different distribution subsets using perplexity of a 124M reference model
trained on CommonCrawl.
Bottom 10% Middle 10% Top 10%
riskyourfoodgoingbadinaluke-
warm fridge when you can lease
kitchen appliances in West Hol-
lywood through Acima! Are you
a budding DJ? A bit of a high-
fidelity audio snub? Love to level
upwiththelatestvideogamesys-
tem? Level up your entertain-
ment at home and on the road
with sound systems for lease in
West Hollywood. You can make
flexible lease renewal payments
on the best in-home sound [...]gratitudeexercise. Beforeyouget
out of bed, think of five things
you are most grateful for. If your
Life Path number is 2, you have
a duality fit for any earthly expe-
rience. You are deeply rooted in
balance and harmony when deal-
ing with the other numbers. In
order to stay connected to your
community, start your day by
connecting with your friends and
family. Instead of hopping on so-
cial [...]keepers" definitely won’t help!
Then there are those whose idea
of a school librarian is based on
one they remember from their
childhood, who perhaps didn’t
let them borrow from the adult
shelves or maybe told them to be
quiet. You know - the cliched
woman with glasses and a bun?
I wear glasses myself and ended
up haing to get a haircut to avoid
the cliche. In summer, of course
I had to put my [...]
the-art mixed-use development
that features a wide variety of
shops, services, and restaurants,
along with over 950 luxury apart-
ments. The sprawling urban vil-
lage is pedestrian-friendly and is
the perfect place if you want to
indulge in a shopping spree or
treat your taste buds to a hearty
meal. If you’re thinking about
looking for the perfect home in
Brookhaven, I’m ready to help!
Get in touch [...]it as a stand-alone piece but later
experimentedperformingitasmy
written prediction, confabulation
style, Closing Effect. It’s still a
work in progress but I did re-
ceive some "Standing Ovations!"
ALAN ARITA "I received a copy
of GAME NIGHT and IT IS EX-
CELLENT! First, the quality of
the book is outstanding; every-
thing from the artwork, layout,
hidden gems, and of course the
precision cut [...]and view the supernal beauty
that lies beyond. (I wish I’d have
said that first; actually I stole it
from a guy who wrote it a hun-
dredyearsago!*)ButifIcouldn’t
see into the future for a few years,
there wouldn’t be a Christmas
story today. I’ve a whole lot of
notes still in my jeans. One’s
about Rabbi Frankel of the Syn-
agogue across West Street from
old Reno High School. He was a
pretty [...]
toilet drains are overwhelmed
with toilet paper or clogged by
non-flushable things that find
their way into the drain. If that’s
the case, it may be time to call
a plumbing technician. Unex-
pectedtoiletissuesinterruptyour
daily routine, turning what you
expected to be a good day right
into a stressful one. You need
help ASAP! Best quality Plumb-
ing is ready to solve your toilet
troubles no [...]who offer 3D printing services
these days. Try searching for
someone who offers them in your
area.Last week, Apple announced
the new A15 processor in a pecu-
liar way: by comparing its new
chip to the Android competition,
ratherthantheA14thatpowered
last year’s generation of iPhones.
We were all left to try to infer the
speedoftheA15basedonApple’s
claims, and wondering if the com-
pany was [...]floor study, family room, kitchen,
unfinished basement for future
expansion&2cargarage. Lennar
seamlessly blended & showcased
the unparalleled beauty of Col-
orado with the most innova-
tive homes, energy efficient tech-
nologies & modern conveniences,
bringing the best of both worlds
together. Beautiful finishes and
upgrades throughout. Lennar
provides the latest in energy ef-
ficiency and state of [...]
22

--- PAGE 23 ---
Table 5: Samples from different distribution subsets using perplexity of a 124M reference model
trained on Wikipedia.
Bottom 10% Middle 10% Top 10%
of our kids, demonstrated abil-
ity to create meaningful change,
a strong commitment to learning,
and an ability to work in partner-
ship with others." Individuals ac-
cepted to this program agree to
a two-year teaching commitment.
If you become a core member you
are required to attend an inten-
sive summer training program to
prepare for your two-year com-
mitment. Each region has differ-
ent requirements b [...]HST single cylinder hydraulic
conecrusher. HSTsinglecylinder
hydraulic cone crusher integrates
mechanical, hydraulic, electri-
cal, automation, intelligent con-
trolandothertechnologies, which
can be widely used in medium,
fine and ultra-fine crushing op-
erations in metal and non-metal
mines, cement, sandstone, met-
allurgy and other industries...
1,214rollerconecrusherproducts
are offered [...]active play outdoor. Users with-
out a subscription are not able to
see the full content on this page.
Please subscribe or login.On the
net betting houses include was
able to offer followers a fabulous
best range of luring optimistic as-
pects. A style of online casino
money provides consistently con-
tinually really been ornamented
and acquired in reaction to make
sure you basic safety issues. Insi
[...]
to be that way. Weight loss
surgery in Hanover is a great op-
tion for those who are at least
fifty pounds overweight and have
struggled with weight loss over
the years. There are a num-
ber of surgical weight loss proce-
dures available to those seeking
treatment, and Nusbaum Weight
Loss Centers of New Jersey, with
offices and bariatric surgeons in
Morristown, MorrisCounty, Mor-
ris County, and surrou [...]sperm whales. Learn firsthand
about Sri Lanka’s amazing bio-
diversity on this private tour to
the Kanneliya Rainforest. With
a dedicated guide leading you,
explore the UNESCO-listed bio-
sphere reserve, home to monkeys,
snakes, chameleons, and a wide
range of bird life. Learn about
the flora and fauna through com-
mentary tailored to your interests
andenjoyplentyofchancestoask
questions. Explo [...]row for spotting this Sabal Trail
posting within minutes.The skin
has become delicate. I just re-
ceived the goods and I didn’t
know how to use it. I con-
sulted the customer service. I
didn’t expect the customer ser-
vice person to be super good and
the introduction was super care-
ful. I have been so successful
and happy trading with you ev-
ery time.. I hope we have more
transactions in the future... Ha
[...]
towhichcoverageistherebytobe
granted; and (2) Shall insure the
person named therein and any
other person, as insured, using
any such motor vehicle or motor
vehicles with the express or im-
plied permission of such named
insured against loss from the li-
ability imposed by law for dam-
ages arising out of the owner-
ship, maintenance, or use of such
motor vehicle or motor vehicles
within the United [...]Also, I have attached a brief pre-
sentation of our work for bet-
ter understanding.A two-year so-
lar energy project at the Univer-
sity of Sheffield has shown almost
all of the 2,000 systems in the
schemearestillperformingbetter
than expected. Researchers run-
ning Sheffield Solar Farm, which
was launched in August 2010, say
98 per cent of more than 2,000
systems involved in the scheme
are working [...]It exposes a design and construc-
tion system for horizontal plates
to work as slabs in regular con-
crete buildings. Based to an evo-
lutionary finite-element analysis
of the topological configuration
to get a curved design with a
50% reduction of traditional vol-
ume, that provide lower cost, less
carbon foot-print, better perfor-
mance and innovative ceiling. A
library of profiles is elaborated
according [...]
23

--- PAGE 24 ---
Table 6: Samples from different distribution subsets using EL2N from a 124M reference model
trained on CommonCrawl.
Bottom 10% Middle 10% Top 10%
a handle on how many eleva-
tors they are supposed to over-
see. Those officials have repeat-
edly deflected requests from re-
porters to detail the count of el-
evators in Chicago requiring in-
spection. Frydland, during her
interview, said she doesn’t know
how many elevators her office
is responsible for inspecting be-
cause city records lump elevators
into the same class of devices as
escalators, [...]there’s a possibility that you may
come across a property that’s
sharing a driveway with the home
next door. That means that one
driveway needs to be shared be-
tween the two adjoining neigh-
bors. Many real estate investors
rent out their properties in or-
der to reap the benefits of passive
monthly income while increasing
their equity and building wealth
over time. Not only are they ben-
efiting [...]We have all spent happy hours
listening to and sharing music
we love with those closest to us.
Many of the people we serve in
ubuareincrediblygiftedandplay
a wide range of musical instru-
ments and enjoy singing and per-
forming for other people. Judith
is enabled by ubu to live more
independently in Knaresborough,
North Yorkshire, and has started
taking singing lessons in order to
’grow’ her [...]
ians 4:3? Jesus addressed this
very issue with his disciples on
the night of his betrayal. He
would be leaving them soon,
but he promised the Holy Spirit
would come to comfort and aide
them, "I will not leave you as or-
phans; I will come to you."-John
14:18. Jesus refers to the Holy
Spirit as himself because, "the
Helper, the Holy Spirit, whom
the Father will send in my name,
he will teach you all [...]the standard as far as cement
manufacturing goes several ce-
ment manufacturers still prefer
ball mills for cement production
when they want to design new
grinding plants or a new inte-
grated 3D design and analysis of
the crushing roller of The crush-
ing roller is one of the main parts
of a highpressure grinding roller
which is a type of highly effi-
cient ore crushing equipment In
the work reported [...]range (Table 1). Active-
Controlled Study: CRESTOR
was compared with the HMG-
CoA reductase inhibitors ator-
vastatin, simvastatin, and
pravastatin in a multicenter,
open-label, dose-ranging study of
2,240 patients with Type IIa and
IIb hypercholesterolemia. After
randomization, patients were
treated for 6 weeks with a single
daily dose of either CRESTOR,
atorvastatin, simvastatin, or
pravastatin [...]
Most past attemptsto define so-
cioeconomics as a science in its
own right may have been mo-
tivated tocounter such a sim-
plistic understanding of socioe-
conomics.In this chapter, we re-
view past attempts to define so-
cioeconomics before theapproach
is chosen that we applied in this
book. This book, by a leading ex-
pert in urban agriculture, offers a
genuine solution to today’s global
food crisis. By [...]which adopted our buttons such
thatwhenwewenttoBoston.com
(part of NY times) branding was
not part of our discussions. Of
course, we had matured in our
thinking and offered them a co-
branded offer hosted by Coola.
When Switchboard did not work
for us, we went to their compe-
tition Infospace.com, which was
much larger than them. They ac-
cepted a branded Coola button
but offered a complex deal [...]Trend.com: Ihadnoideathiswas
coming. There’d been talk over
the years about setting up a sort
of business portal that integrated
all of Trend’s regular and an-
nual publications, but there was
never enough momentum to ac-
tually get it going. Trend had
a regular spot on the Times’ on-
line Business section, but it was
a pretty low-impact thing (even
though quite a bit of traffic would
come to the [...]
24

--- PAGE 25 ---
Table 7: Samples from different distribution subsets using memorization of a 124M reference model
trained on CommonCrawl.
Mem. Factor = 0 Mem. Factor = 0.5 Mem. Factor = 1.0
doesn’t prevent you from clearly
seeing the road. Hi, thank you
so much for your words, appreci-
ate it! Moreover, we noted your
comments, we’ll think what can
be done, for sharing more ideas,
feel free to contact us at sup-
port@hudwayapp.com any time.
Happy to help you always! I do
a lot of mudding. And it’s got a
pitch and roll gauge, which I like
when I’m in the hole, do I don’t
flip my truck. [...]160 countries. There are abun-
dant hot-selling projects accessi-
ble to you. Cheap and environ-
mentally friendly: Factory-direct
sale, fast delivery with guaran-
teed quality at factory price, in
line with the concept of environ-
mental development. Feb 19 2021
should pelletisation of sulfide
solidelectrolytesafterball milling-
has to be done in argon atmo-
sphere question 7 answers i am
using a spex 8000b [...]reference. My company’s NACHI
230/600E bearing price conces-
sions, adequate inventory, and
other similar products are avail-
able for recommendation 1 . Less
than 45 KGS, we will send by
express. (Door to Door, Conve-
nient) 2 . 45 - 200 KGS , we will
send by air transport . (Fastest
and safest, but expensive) 3 .
More than 200 KGS, we will send
by sea . ( Cheapest and common
use ) The bearing 240/8 [...]
disposal and processing of con-
taminated suspensions such as
drilling mud, road sweepings and
similar. The rising demand on
the international market to meet
current as well as future envi-
ronmental regulations is the main
driver for the development in this
area of our work," explains Man-
aging Director Ing. Mag. Erich
Trunkenpolz. "The plants are
currently developed for station-
ary and semi-mobile du [...]$97 monthly subscription pack-
age. If you decide to make an
annual payment of $997, you get
two free months. I started with
this basic package but I later de-
cided to upgrade to Etison Suite
since this one has some limita-
tions. As a marketer, I was only
allowed to use 3 custom domains,
get a limit of 20,000 visitors, and
make a maximum of 100 web
pages. I discovered that some ad-
vanced features are [...]takes your bank to process our
refund request (5 to 10 business
days). If you need to return an
item, simply login to your ac-
count, view the order using the
’Complete Orders’ link under the
My Account menu and click the
Return Item(s) button. We’ll no-
tify you via e-mail of your refund
oncewe’vereceivedandprocessed
the returned item. We can ship
to virtually any address in the
world. Note the [...]
time:If you’re looking into faster-
than-light fiber internet, there’s a
Verizon Fios deal for you in Silver
Spring, MD. Want more than a
Verizon Fios internet-only plan?
Open your home up to more en-
tertainment choices with Verizon
Fios packages. Ready to improve
your home with the best internet
available? Get lightspeed inter-
net with Verizon plans that suit
every lifestyle. Whether you only
need [...]Select options that apply then
copy and paste the RDF/HTML
data fragment to include in your
application Note: Adjust the
width and height settings defined
in the RDF/HTML code frag-
ment to best match your require-
mentsCause.—Upon the ascen-
sion of William and Mary to the
throne of England, the Protes-
tants of Maryland demanded the
Colonial management of the Ter-
ritory. The Roman Catholics, af-
ter rep [...]to assess the success of our
marketing and advertising cam-
paigns). Finally, we may also
share your Personal Information
to comply with applicable laws
and regulations, to respond to
a subpoena, search warrant or
other lawful request for informa-
tion we receive, or to otherwise
protect our rights. Additionally,
you can opt out of some of these
services by visiting the Digital
Advertising Alliance [...]
25
