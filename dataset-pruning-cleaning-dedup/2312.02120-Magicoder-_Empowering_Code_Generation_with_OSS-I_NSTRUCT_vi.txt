# 2312.02120.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/dataset-pruning-cleaning-dedup/2312.02120.pdf
# Kích thước file: 1239526 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Magicoder: Tăng cường Tạo mã với OSS-INSTRUCT
Yuxiang Wei1 Zhe Wang2† Jiawei Liu1 Yifeng Ding1 Lingming Zhang1
Tóm tắt
Chúng tôi giới thiệu Magicoder, một loạt các Mô hình Ngôn ngữ Lớn (LLM) hoàn toàn mã nguồn mở (code, trọng số và dữ liệu) dành cho lập trình, thu hẹp đáng kể khoảng cách với các mô hình lập trình hàng đầu trong khi chỉ có không quá 7B tham số. Các mô hình Magicoder được đào tạo trên 75K dữ liệu hướng dẫn tổng hợp sử dụng OSS-INSTRUCT, một cách tiếp cận mới để khai sáng LLM bằng các đoạn mã nguồn mở để tạo ra dữ liệu hướng dẫn đa dạng cho lập trình. Động lực chính của chúng tôi là giảm thiểu độ thiên lệch vốn có của dữ liệu tổng hợp được tạo bởi LLM thông qua sự phong phú của các tham khảo mã nguồn mở để sản xuất dữ liệu thực tế và có thể kiểm soát hơn. Tính trực giao của OSS-INSTRUCT và các phương pháp tạo dữ liệu khác như Evol-Instruct tiếp tục cho phép chúng tôi xây dựng một Magicoder S nâng cao. Cả Magicoder và Magicoder S đều vượt trội đáng kể so với các mô hình lập trình tiên tiến với kích thước tương tự hoặc thậm chí lớn hơn trên một loạt các benchmark lập trình. Đáng chú ý, Magicoder S-CL-7B dựa trên CODELLAMA thậm chí còn vượt qua ChatGPT nổi tiếng trên HumanEval+ (66.5 so với 65.9 trong pass@1). Nhìn chung, OSS-INSTRUCT mở ra một hướng mới để chế tạo dữ liệu hướng dẫn tổng hợp đa dạng cho lập trình bằng cách sử dụng các tham khảo mã nguồn mở phong phú.

1. Giới thiệu
Tạo mã, còn được gọi là tổng hợp chương trình (Gulwani et al., 2017), là một thách thức lâu dài trong khoa học máy tính. Trong vài thập kỷ qua, một lượng lớn nghiên cứu đã được thực hiện để nghiên cứu các cách tiếp cận biểu tượng, như tổng hợp dựa trên trừu tượng (Wang et al., 2017; Feng et al., 2018) cho các bài toán tổng hợp mục đích chung và lập trình bằng ví dụ (Cambronero et al., 2023; Liu et al., 2023a) cho các tác vụ cụ thể theo lĩnh vực. Cho đến gần đây, các Mô hình Ngôn ngữ Lớn (LLM) được đào tạo trên mã (Austin et al., 2021; Chen et al., 2021) đã cho thấy những đột phá xuất sắc trong việc tạo mã đáp ứng chính xác ý định của người dùng, và chúng được triển khai rộng rãi để hỗ trợ phát triển phần mềm thực tế (Microsoft, 2023b; Services, 2023).

Ban đầu, các mô hình mã nguồn đóng như GPT-3.5 Turbo (OpenAI, 2022) (tức là ChatGPT) và GPT-4 (OpenAI, 2023) đã thống trị mạnh mẽ các benchmark và bảng xếp hạng lập trình khác nhau (Chen et al., 2021; Austin et al., 2021; Liu et al., 2023b; Lai et al., 2022; Xia & Zhang, 2023). Để tiếp tục đẩy ranh giới của tạo mã với các LLM mã nguồn mở, SELF-INSTRUCT (Wang et al., 2023a) được áp dụng để khởi động khả năng tuân theo hướng dẫn của LLM. Trong lĩnh vực lập trình, các thực hành viên thường thiết kế các hướng dẫn lập trình tổng hợp sử dụng một mô hình giáo viên mạnh hơn (ví dụ: ChatGPT và GPT-4) và sau đó tinh chỉnh một mô hình học sinh yếu hơn (ví dụ: CODELLAMA (Rozière et al., 2023)) với dữ liệu được tạo ra để chưng cất kiến thức từ giáo viên (Taori et al., 2023; Chaudhary, 2023). Ví dụ, Code Alpaca (Chaudhary, 2023) bao gồm 20K hướng dẫn lập trình được tạo tự động bằng cách áp dụng SELF-INSTRUCT trên ChatGPT sử dụng 21 tác vụ khởi tạo. Để tiếp tục nâng cao khả năng lập trình của LLM, Luo et al. (2023b) đề xuất Code Evol-Instruct sử dụng các heuristic khác nhau để tăng độ phức tạp của các hướng dẫn lập trình khởi tạo (Code Alpaca trong trường hợp này), đạt kết quả tiên tiến (SOTA) trong số các mô hình mã nguồn mở.

Mặc dù các phương pháp tạo dữ liệu này có thể cải thiện hiệu quả khả năng tuân theo hướng dẫn của LLM, chúng dựa vào một phạm vi hẹp các tác vụ hoặc heuristic được định nghĩa trước ở bên dưới. Ví dụ, một mặt, Code Alpaca áp dụng SELF-INSTRUCT chỉ dựa vào 21 tác vụ khởi tạo để tạo ra các hướng dẫn lập trình mới sử dụng một mẫu prompt giống hệt nhau. Mặt khác, Code Evol-Instruct lấy Code Alpaca làm khởi tạo và chỉ phụ thuộc vào 5 heuristic để phát triển bộ dữ liệu. Như được gợi ý một phần bởi Yu et al. (2023) và Wang et al. (2023a), các cách tiếp cận như vậy có thể kế thừa đáng kể độ thiên lệch hệ thống vốn có trong LLM cũng như các tác vụ được định nghĩa trước.

Do đó, trong bài báo này, chúng tôi đề xuất OSS-INSTRUCT để giảm thiểu độ thiên lệch vốn có của LLM và giải phóng tiềm năng của chúng để tạo ra các hướng dẫn lập trình đa dạng và sáng tạo thông qua việc học trực tiếp từ mã nguồn mở. Như được minh họa trong Hình 1, OSS-INSTRUCT tận dụng một LLM mạnh mẽ để tự động tạo ra các bài toán lập trình mới bằng cách lấy cảm hứng từ bất kỳ đoạn mã ngẫu nhiên nào được thu thập từ mã nguồn mở. Trong ví dụ này, LLM được truyền cảm hứng bởi hai fragment mã không hoàn chỉnh từ các hàm khác nhau và quản lý để liên kết chúng và tạo ra một bài toán học máy thực tế. Nhờ vào mã nguồn mở trong thế giới thực "vô hạn", OSS-INSTRUCT có thể trực tiếp tạo ra các hướng dẫn lập trình đa dạng, thực tế và có thể kiểm soát bằng cách cung cấp các đoạn mã khởi tạo khác biệt. Cuối cùng, chúng tôi tạo ra 75K dữ liệu tổng hợp để tinh chỉnh CODELLAMA-PYTHON-7B, tạo ra Magicoder-CL. Mặc dù đơn giản và hiệu quả, OSS-INSTRUCT trực giao với các phương pháp tạo dữ liệu hiện có, và chúng có thể được kết hợp để tiếp tục thúc đẩy khả năng lập trình của các mô hình. Do đó, chúng tôi tiếp tục tinh chỉnh Magicoder-CL trên một bộ dữ liệu Evol-Instruct mã nguồn mở với 110K mục, tạo ra Magicoder S-CL.

Chúng tôi đánh giá Magicoder và Magicoder S trên một loạt các tác vụ lập trình, bao gồm HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021) cho tạo text-to-code Python, MultiPL-E (Cassano et al., 2022) cho hoàn thành mã đa ngôn ngữ, và DS-1000 (Lai et al., 2022) để giải quyết các bài toán khoa học dữ liệu. Chúng tôi tiếp tục áp dụng EvalPlus (Liu et al., 2023b), bao gồm các bộ dữ liệu HumanEval+ và MBPP+ được tăng cường để đánh giá mô hình chặt chẽ hơn. Cả Magicoder-CL và Magicoder S-CL đều thúc đẩy đáng kể CODELLAMA-PYTHON-7B cơ sở. Ngoài ra, Magicoder-CL thậm chí vượt trội hơn WizardCoder-CL-7B, WizardCoder-SC-15B, và tất cả các LLM SOTA đã nghiên cứu với ít hơn hoặc bằng 16B tham số trên tất cả các benchmark chúng tôi đã kiểm tra. Ngoài ra, kết quả pass@1 của Magicoder S-CL nâng cao ngang bằng với ChatGPT trên HumanEval (70.7 so với 72.6) và vượt qua nó trên HumanEval+ chặt chẽ hơn (66.5 so với 65.9), cho thấy rằng Magicoder S-CL có thể tạo ra mã mạnh mẽ hơn. Nó cũng đạt kết quả SOTA trong tất cả các mô hình lập trình cùng quy mô.

Ngoài ra, chúng tôi nhận thấy một tiến bộ rất gần đây trong phát triển của loạt DeepSeek-Coder (Guo et al., 2024) đã cho thấy hiệu suất lập trình đặc biệt. Tuy nhiên, do các chi tiết kỹ thuật được tiết lộ có hạn, chúng tôi chỉ thảo luận ngắn gọn về chúng trong §3.4. Bất chấp điều này, chúng tôi đã áp dụng OSS-INSTRUCT trên DeepSeek-Coder-Base 6.7B, tạo ra Magicoder-DS và Magicoder S-DS. Ngoài các phát hiện nhất quán về kết quả trước đó với CODELLAMA-PYTHON-7B làm mô hình cơ sở, Magicoder-DS và Magicoder S-DS hưởng lợi từ DeepSeek-Coder-Base-6.7B mạnh mẽ hơn. Lợi thế này được thể hiện bởi Magicoder S-DS, đạt 76.8 pass@1 đáng chú ý trên HumanEval. Magicoder S-DS cũng vượt trội hơn DeepSeek-Coder-Instruct-6.7B trên HumanEval (+) và MBPP (+) với ít hơn 8× token tinh chỉnh.

Để chứng minh thiết kế của OSS-INSTRUCT, tức là tạo dữ liệu instruction-tuning từ các tham khảo mã nguồn mở thay vì sử dụng trực tiếp các tham khảo, chúng tôi chứng minh rằng việc tinh chỉnh các mô hình cơ sở với các cặp comment-function có liên quan về mặt ngữ nghĩa được trích xuất từ các dự án mã nguồn mở thậm chí còn tác động tiêu cực đến hiệu suất mô hình (§4.2).

Nói chung, chúng tôi đóng góp những điều sau:

• Chúng tôi giới thiệu OSS-INSTRUCT, một cách tiếp cận tiên phong để khai sáng LLM bằng các đoạn mã nguồn mở để tạo ra dữ liệu hướng dẫn lập trình đa dạng, thực tế và có thể kiểm soát hơn, có thể được tận dụng để thúc đẩy đáng kể hiệu suất của các LLM khác nhau thông qua instruction tuning. Nó mở ra một chiều hướng mới để tạo dữ liệu instruction-tuning ít thiên lệch và đa dạng từ sự phong phú của các tham khảo mã nguồn mở.

• Chúng tôi xây dựng loạt Magicoder được đào tạo với OSS-INSTRUCT và loạt Magicoder S được đào tạo trên sự kết hợp của OSS-INSTRUCT và Evol-Instruct. Đánh giá của chúng tôi trên 6 benchmark cho thấy tất cả Magicoder đều cải thiện đáng kể các LLM cơ sở. Đáng chú ý, cả Magicoder S-CL và Magicoder S-DS đều vượt trội hơn ChatGPT trên HumanEval+ chỉ với 7B tham số.

• Chúng tôi hoàn toàn mở mã nguồn trọng số mô hình, dữ liệu đào tạo và source code tại https://github.com/ise-uiuc/magicoder để hỗ trợ nghiên cứu tương lai.

2. OSS-INSTRUCT: Instruction Tuning từ Mã nguồn mở

Trong phần này, chúng tôi trình bày chi tiết về cách tiếp cận OSS-INSTRUCT của chúng tôi. Từ mức độ cao, như được thể hiện trong Hình 1, OSS-INSTRUCT hoạt động bằng cách nhắc một LLM (ví dụ: ChatGPT) để tạo ra một bài toán lập trình và giải pháp của nó theo một đoạn mã khởi tạo được thu thập từ tự nhiên (ví dụ: từ GitHub). Đoạn mã khởi tạo cung cấp khả năng kiểm soát việc tạo ra và khuyến khích LLM tạo ra các bài toán lập trình đa dạng có thể phản ánh các kịch bản lập trình thực tế.

2.1. Tạo Bài toán Lập trình

OSS-INSTRUCT được hỗ trợ bởi các đoạn mã khởi tạo có thể dễ dàng được thu thập từ mã nguồn mở. Trong công việc này, chúng tôi trực tiếp áp dụng starcoderdata làm corpus khởi tạo của chúng tôi, một phiên bản được lọc của bộ dữ liệu The Stack (Kocetkov et al., 2022) mà StarCoder được đào tạo trên, chứa các tài liệu mã nguồn được cấp phép một cách cho phép trong các ngôn ngữ lập trình khác nhau. Chúng tôi đã chọn starcoderdata vì nó được áp dụng rộng rãi, bao gồm các đoạn mã chất lượng cao khổng lồ, và thậm chí được xử lý hậu kỳ để khử nhiễu dữ liệu (Li et al., 2023; Allal et al., 2023). Đối với mỗi tài liệu mã từ corpus, chúng tôi ngẫu nhiên trích xuất 1–15 dòng liên tiếp làm đoạn mã khởi tạo cho mô hình để lấy cảm hứng từ và tạo ra các bài toán lập trình. Tổng cộng, chúng tôi đã thu thập 80K đoạn mã khởi tạo ban đầu từ 80K tài liệu mã, 40K từ Python, và 5K từ mỗi ngôn ngữ C++, Java, TypeScript, Shell, C#, Rust, PHP, và Swift tương ứng. Sau đó, mỗi đoạn mã khởi tạo được thu thập được áp dụng vào mẫu prompt được thể hiện trong Phụ lục A.1, mà một mô hình giáo viên nhận làm đầu vào và xuất ra cả bài toán lập trình và giải pháp của nó.

2.2. Làm sạch Dữ liệu và Khử nhiễu

Chúng tôi thực hiện làm sạch dữ liệu bằng cách loại trừ các mẫu giống hệt nhau hoặc chia sẻ cùng một đoạn mã khởi tạo. Mặc dù tồn tại các loại nhiễu khác (ví dụ: giải pháp không hoàn chỉnh) trong dữ liệu được tạo ra, được truyền cảm hứng bởi Honovich et al. (2023), chúng không bị loại bỏ vì chúng tôi tin rằng chúng vẫn chứa thông tin có giá trị cho LLM để học hỏi. Thêm chi tiết thực nghiệm có thể được tìm thấy trong Phụ lục C.3. Cuối cùng, chúng tôi áp dụng cùng logic như StarCoder Li et al. (2023) để khử nhiễu dữ liệu đào tạo của chúng tôi bằng cách loại bỏ các bài toán lập trình chứa docstring hoặc giải pháp từ HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021), docstring từ APPS (Hendrycks et al., 2021), prompt từ DS-1000 (Lai et al., 2022), hoặc câu hỏi từ GSM8K (Cobbe et al., 2021). Là một phần của phân tích của chúng tôi, quy trình khử nhiễu chỉ lọc ra 9 mẫu bổ sung. Vì corpus khởi tạo starcoderdata đã trải qua khử nhiễu dữ liệu nghiêm ngặt, quan sát này cho thấy rằng OSS-INSTRUCT không có khả năng đưa ra rò rỉ dữ liệu bổ sung ngoài các khởi tạo. Bộ dữ liệu OSS-INSTRUCT cuối cùng chứa khoảng 75K mục. Tổng quan về thống kê bộ dữ liệu có thể được tìm thấy trong Phụ lục A.3.

2.3. Ví dụ Định tính của OSS-INSTRUCT

Hình 2 cho thấy một số ví dụ định tính về cách OSS-INSTRUCT có thể giúp LLM lấy cảm hứng từ một đoạn mã khởi tạo để tạo ra các bài toán và giải pháp lập trình mới. Ví dụ, ví dụ shell script cho thấy cách một LLM tạo ra một bài toán lập trình Python chỉ với một dòng shell script. Ví dụ library imports chứng minh cách một LLM có thể tạo ra một bài toán học máy thực tế chỉ sử dụng vài câu lệnh import. Trong khi đó, instance class signature minh họa khả năng của LLM để lấy cảm hứng từ một định nghĩa class không hoàn chỉnh có các annotation như SpringBootApplication và từ khóa như bank. Từ đó, LLM tạo ra một bài toán yêu cầu triển khai một hệ thống ngân hàng hoàn chỉnh dựa trên Spring Boot. Nhìn chung, OSS-INSTRUCT có thể truyền cảm hứng cho LLM với các cấu trúc và ngữ nghĩa mã khác biệt để tạo ra các tác vụ lập trình đa dạng, bao gồm thử thách thuật toán, vấn đề thực tế, tạo mã một hàm, hoàn thành chương trình dựa trên thư viện, phát triển toàn bộ chương trình, và thậm chí xây dựng toàn bộ ứng dụng.

--- TRANG 3 ---
Magicoder: Tăng cường Tạo mã với OSS-INSTRUCT

dữ liệu instruction-tuning từ các tham khảo mã nguồn mở thay vì sử dụng trực tiếp các tham khảo, chúng tôi chứng minh rằng việc tinh chỉnh các mô hình cơ sở với các cặp comment-function có liên quan về mặt ngữ nghĩa được trích xuất từ các dự án mã nguồn mở thậm chí còn tác động tiêu cực đến hiệu suất mô hình (§4.2).

Nói chung, chúng tôi đóng góp những điều sau:

• Chúng tôi giới thiệu OSS-INSTRUCT, một cách tiếp cận tiên phong để khai sáng LLM bằng các đoạn mã nguồn mở để tạo ra dữ liệu hướng dẫn lập trình đa dạng, thực tế và có thể kiểm soát hơn, có thể được tận dụng để thúc đẩy đáng kể hiệu suất của các LLM khác nhau thông qua instruction tuning. Nó mở ra một chiều hướng mới để tạo dữ liệu instruction-tuning ít thiên lệch và đa dạng từ sự phong phú của các tham khảo mã nguồn mở.

• Chúng tôi xây dựng loạt Magicoder được đào tạo với OSS-INSTRUCT và loạt Magicoder S được đào tạo trên sự kết hợp của OSS-INSTRUCT và Evol-Instruct. Đánh giá của chúng tôi trên 6 benchmark cho thấy tất cả Magicoder đều cải thiện đáng kể các LLM cơ sở. Đáng chú ý, cả Magicoder S-CL và Magicoder S-DS đều vượt trội hơn ChatGPT trên HumanEval+ chỉ với 7B tham số.

• Chúng tôi hoàn toàn mở mã nguồn trọng số mô hình, dữ liệu đào tạo và source code tại https://github.com/ise-uiuc/magicoder để hỗ trợ nghiên cứu tương lai.

2. OSS-INSTRUCT: Instruction Tuning từ Mã nguồn mở

Trong phần này, chúng tôi trình bày chi tiết về cách tiếp cận OSS-INSTRUCT của chúng tôi. Từ mức độ cao, như được thể hiện trong Hình 1, OSS-INSTRUCT hoạt động bằng cách nhắc một LLM (ví dụ: ChatGPT) để tạo ra một bài toán lập trình và giải pháp của nó theo một đoạn mã khởi tạo được thu thập từ tự nhiên (ví dụ: từ GitHub). Đoạn mã khởi tạo cung cấp khả năng kiểm soát việc tạo ra và khuyến khích LLM tạo ra các bài toán lập trình đa dạng có thể phản ánh các kịch bản lập trình thực tế.

2.1. Tạo Bài toán Lập trình

OSS-INSTRUCT được hỗ trợ bởi các đoạn mã khởi tạo có thể dễ dàng được thu thập từ mã nguồn mở. Trong công việc này, chúng tôi trực tiếp áp dụng starcoderdata làm corpus khởi tạo của chúng tôi, một phiên bản được lọc của bộ dữ liệu The Stack (Kocetkov et al., 2022) mà StarCoder được đào tạo trên, chứa các tài liệu mã nguồn được cấp phép một cách cho phép trong các ngôn ngữ lập trình khác nhau. Chúng tôi đã chọn starcoderdata vì nó được áp dụng rộng rãi, bao gồm các đoạn mã chất lượng cao khổng lồ, và thậm chí được xử lý hậu kỳ để khử nhiễu dữ liệu (Li et al., 2023; Allal et al., 2023). Đối với mỗi tài liệu mã từ corpus, chúng tôi ngẫu nhiên trích xuất 1–15 dòng liên tiếp làm đoạn mã khởi tạo cho mô hình để lấy cảm hứng từ và tạo ra các bài toán lập trình. Tổng cộng, chúng tôi đã thu thập 80K đoạn mã khởi tạo ban đầu từ 80K tài liệu mã, 40K từ Python, và 5K từ mỗi ngôn ngữ C++, Java, TypeScript, Shell, C#, Rust, PHP, và Swift tương ứng. Sau đó, mỗi đoạn mã khởi tạo được thu thập được áp dụng vào mẫu prompt được thể hiện trong Phụ lục A.1, mà một mô hình giáo viên nhận làm đầu vào và xuất ra cả bài toán lập trình và giải pháp của nó.

2.2. Làm sạch Dữ liệu và Khử nhiễu

Chúng tôi thực hiện làm sạch dữ liệu bằng cách loại trừ các mẫu giống hệt nhau hoặc chia sẻ cùng một đoạn mã khởi tạo. Mặc dù tồn tại các loại nhiễu khác (ví dụ: giải pháp không hoàn chỉnh) trong dữ liệu được tạo ra, được truyền cảm hứng bởi Honovich et al. (2023), chúng không bị loại bỏ vì chúng tôi tin rằng chúng vẫn chứa thông tin có giá trị cho LLM để học hỏi. Thêm chi tiết thực nghiệm có thể được tìm thấy trong Phụ lục C.3. Cuối cùng, chúng tôi áp dụng cùng logic như StarCoder Li et al. (2023) để khử nhiễu dữ liệu đào tạo của chúng tôi bằng cách loại bỏ các bài toán lập trình chứa docstring hoặc giải pháp từ HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021), docstring từ APPS (Hendrycks et al., 2021), prompt từ DS-1000 (Lai et al., 2022), hoặc câu hỏi từ GSM8K (Cobbe et al., 2021). Là một phần của phân tích của chúng tôi, quy trình khử nhiễu chỉ lọc ra 9 mẫu bổ sung. Vì corpus khởi tạo starcoderdata đã trải qua khử nhiễu dữ liệu nghiêm ngặt, quan sát này cho thấy rằng OSS-INSTRUCT không có khả năng đưa ra rò rỉ dữ liệu bổ sung ngoài các khởi tạo. Bộ dữ liệu OSS-INSTRUCT cuối cùng chứa khoảng 75K mục. Tổng quan về thống kê bộ dữ liệu có thể được tìm thấy trong Phụ lục A.3.

2.3. Ví dụ Định tính của OSS-INSTRUCT

Hình 2 cho thấy một số ví dụ định tính về cách OSS-INSTRUCT có thể giúp LLM lấy cảm hứng từ một đoạn mã khởi tạo để tạo ra các bài toán và giải pháp lập trình mới. Ví dụ, ví dụ shell script cho thấy cách một LLM tạo ra một bài toán lập trình Python chỉ với một dòng shell script. Ví dụ library imports chứng minh cách một LLM có thể tạo ra một bài toán học máy thực tế chỉ sử dụng vài câu lệnh import. Trong khi đó, instance class signature minh họa khả năng của LLM để lấy cảm hứng từ một định nghĩa class không hoàn chỉnh có các annotation như SpringBootApplication và từ khóa như bank. Từ đó, LLM tạo ra một bài toán yêu cầu triển khai một hệ thống ngân hàng hoàn chỉnh dựa trên Spring Boot. Nhìn chung, OSS-INSTRUCT có thể truyền cảm hứng cho LLM với các cấu trúc và ngữ nghĩa mã khác biệt để tạo ra các tác vụ lập trình đa dạng, bao gồm thử thách thuật toán, vấn đề thực tế, tạo mã một hàm, hoàn thành chương trình dựa trên thư viện, phát triển toàn bộ chương trình, và thậm chí xây dựng toàn bộ ứng dụng.

--- TRANG 4 ---
Magicoder: Tăng cường Tạo mã với OSS-INSTRUCT

Seed: shell script
python3 makeErrorFile.py data/test_dataset_14 14

Problem
Tạo một chương trình Python tạo ra một file lỗi dựa trên một dataset cho trước...

Code
def generate_error_file(dataset_file, ...):
  error_lines = []
  with open(dataset_file, 'r') as file:
    for line in file:
      ...
  with open(error_file_name, 'w') as error_file:
    for error_line in error_lines:
      error_file.write(error_line + '\n')

if __name__ == "__main__":
  if len(sys.argv) != 3:
    print("Usage: ...")
  else:
    dataset_file = sys.argv[1]
    dataset_number = sys.argv[2]
    generate_error_file(...)

Seed: library imports
import numpy as np
import gym_electric_motor as gem
import matplotlib.pyplot as plt

Problem
Tạo một agent học tăng cường để điều khiển một động cơ điện sử dụng môi trường OpenAI Gym...

Code
import numpy as np
import gym_electric_motor as gem
...
env = gem.make("DcSeriesCont-v1")
class DQNAgent:
  def __init__(self, state_dim, action_dim): ...
  def build_model(self): ...
  def act(self, state): ...
  def train(self, state, action, reward, ...): ...
...
for episode in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_dim])
    ...

Seed: class signature
@SpringBootApplication
@Import({ AxonConfig.class })
public class AxonbankApplication {
  public static void main(String[] args) {

Problem
Tạo một ứng dụng Java Spring Boot đơn giản cho một hệ thống ngân hàng...

Code
import org.axonframework.commandhandling...
import org.axonframework.config...
...
@SpringBootApplication
@Import({ AxonConfig.class })
public class AxonbankApplication {...}
public class BankAccount {...}
public class CreateAccountCommand {...}
public class DepositFundsCommand {...}
public class WithdrawFundsCommand {...}
public class AccountCreatedEvent {...}
public class FundsDepositedEvent {...}
public class FundsWithdrawnEvent {...}

Hình 2: Ví dụ cho thấy cách OSS-INSTRUCT tạo ra các bài toán và giải pháp từ các đoạn mã khởi tạo. Các yêu cầu bài toán chi tiết, triển khai và giải thích bị bỏ qua vì tính ngắn gọn. Thêm ví dụ có thể được tìm thấy trong Phụ lục A.2.

[Biểu đồ cosine similarity]

Hình 3: Độ tương đồng cosine giữa HumanEval và dữ liệu tổng hợp được tạo bởi các phương pháp khác nhau.

Độ tương đồng với HumanEval
Để nghiên cứu xem liệu quá trình tạo dữ liệu của chúng tôi có tạo ra các bài toán hoặc giải pháp giống HumanEval hơn góp phần vào hiệu suất cao hay không, chúng tôi ghép nối mỗi mẫu từ bộ dữ liệu 75K của chúng tôi với từng mẫu trong 164 mẫu HumanEval (Chen et al., 2021) và tính toán độ tương đồng cosine của chúng sử dụng embedding TF-IDF (SPARCK JONES, 1972). Sau đó chúng tôi liên kết mỗi mẫu OSS-INSTRUCT với một mẫu HumanEval có điểm tương đồng cao nhất. Chúng tôi cũng so sánh bộ dữ liệu của chúng tôi với Code Alpaca, một bộ dữ liệu 20K áp dụng SELF-INSTRUCT cho mã, và evol-codealpaca-v1 (theblackcat102, 2023), một bản tái tạo mã nguồn mở của Evol-Instruct chứa 110K hướng dẫn lập trình. Chúng tôi sử dụng triển khai mã nguồn mở vì bộ dữ liệu Code Evol-Instruct chính thức (Luo et al., 2023b) không được phát hành. Chúng tôi khử nhiễu tất cả các bộ dữ liệu trước đó sử dụng cùng cách được thảo luận trong §2.2.

Hình 3 cho thấy OSS-INSTRUCT thể hiện độ tương đồng trung bình thấp nhất trong tất cả các kỹ thuật tạo dữ liệu đã nghiên cứu trong khi SELF-INSTRUCT cho thấy độ tương đồng trung bình cao nhất. Kết quả này cho thấy rằng những cải thiện từ OSS-INSTRUCT không chỉ đơn giản là do bao gồm dữ liệu từ cùng một phân phối.

3. Đánh giá

Chúng tôi chọn CODELLAMA-PYTHON-7B và DeepSeek-Coder-Base 6.7B làm LLM cơ sở. Để tạo ra loạt Magicoder, trước tiên chúng tôi tinh chỉnh chúng trên 75K dữ liệu tổng hợp được tạo thông qua OSS-INSTRUCT. Sau đó chúng tôi có được Magicoder S bằng cách tiếp tục tinh chỉnh Magicoder với bộ dữ liệu evol-codealpaca-v1, một triển khai Evol-Instruct mã nguồn mở chứa khoảng 110K mẫu. Thêm chi tiết triển khai và kết quả đánh giá bổ sung được liệt kê trong Phụ lục B và C. Chúng tôi cũng trình bày các trường hợp sử dụng thú vị phản ánh hiệu quả của instruction tuning trong Phụ lục D và chứng minh khả năng của Magicoder để tạo ra các chương trình phức tạp trong Phụ lục E.

3.1. Tạo Text-to-Code Python

HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021) là hai trong những benchmark được sử dụng rộng rãi nhất cho tạo mã. Mỗi tác vụ trong các benchmark này bao gồm một mô tả tác vụ (ví dụ: docstring) làm prompt, nơi LLM tạo ra mã tương ứng có tính đúng đắn được kiểm tra bởi một số ít test case. Vì các test trong những benchmark này có thể không đủ, để đánh giá chặt chẽ hơn, chúng tôi sử dụng HumanEval+ và MBPP+, cả hai được hỗ trợ bởi framework EvalPlus (Liu et al., 2023b) để có được nhiều hơn 80×/35× test. Theo công việc trước đó (Liu et al., 2023b; Chen et al., 2023), đối với mỗi tác vụ và LLM, chúng tôi sử dụng greedy decoding để tạo ra một mẫu và tập trung vào việc so sánh metric pass@1.

Chúng tôi xem xét một loạt các mô hình baseline, bao gồm CODELLAMA-PYTHON (Rozière et al., 2023), WizardCoder (Luo et al., 2023b), GPT-3.5 Turbo (OpenAI, 2022), GPT-4 Turbo (OpenAI, 2023), StarCoder (Li et al., 2023), CodeT5+ (Wang et al., 2023b), CodeGen-Mono (Nijkamp et al., 2023), và Mistral (Jiang et al., 2023a). Tất cả kết quả được báo cáo nhất quán từ bảng xếp hạng EvalPlus (Liu et al., 2023b) (EvalPlus hash: 1895d2f).

Bảng 1 cho thấy kết quả pass@1 của các LLM khác nhau trên những benchmark này. Từ kết quả, chúng tôi có thể quan sát đầu tiên rằng Magicoder-CL có sự cải thiện rõ ràng so với CODELLAMA-PYTHON-7B cơ sở, và vượt trội hơn tất cả các mô hình mã nguồn mở đã nghiên cứu ngoại trừ CODELLAMA-PYTHON-34B và WizardCoder-CL-34B. Đáng chú ý, Magicoder-CL vượt qua WizardCoder-SC-15B và có sự cải thiện đáng kể trên HumanEval và HumanEval+ so với CODELLAMA-PYTHON-34B. Magicoder S-CL chứng minh những cải thiện tiếp theo bằng cách được đào tạo với phương pháp Evol-Instruct trực giao. Magicoder S-CL vượt trội hơn ChatGPT và tất cả các mô hình mã nguồn mở khác trên HumanEval+. Hơn nữa, mặc dù nó có điểm số hơi thấp hơn WizardCoder-CL-34B và ChatGPT trên HumanEval, nó vượt qua cả hai trên bộ dữ liệu HumanEval+ chặt chẽ hơn, cho thấy rằng Magicoder S-CL có thể tạo ra mã mạnh mẽ hơn.

3.2. Tạo Mã Đa ngôn ngữ

Ngoài Python, như được thể hiện trong Bảng 2, chúng tôi thực hiện đánh giá mở rộng trên 6 ngôn ngữ lập trình được sử dụng rộng rãi, tức là Java, JavaScript, C++, PHP, Swift, và Rust, sử dụng benchmark MultiPL-E (Cassano et al., 2022). Chúng tôi báo cáo các kết quả có sẵn từ bài báo WizardCoder (Luo et al., 2023b) và đánh giá các mô hình của chúng tôi một cách nhất quán thông qua bigcode-evaluation-harness (Ben Allal et al., 2022). Chúng tôi bỏ qua các mô hình độc quyền như ChatGPT và GPT-4 vì chúng không được hỗ trợ bởi framework. Do độ trễ inference đáng kể khi chạy WizardCoder-CL-7B sử dụng harness trong môi trường của chúng tôi, chúng tôi chọn không bao gồm nó trong phân tích của chúng tôi.

Kết quả cho thấy Magicoder-CL cải thiện CODELLAMA-PYTHON-7B cơ sở với một biên độ lớn trong tất cả các ngôn ngữ lập trình đã nghiên cứu. Hơn nữa, Magicoder-CL cũng đạt kết quả tốt hơn WizardCoder-SC SOTA 15B trong nửa số ngôn ngữ lập trình. Ngoài ra, Magicoder S-CL chứng minh cải thiện tiếp theo so với Magicoder-CL trên tất cả các ngôn ngữ lập trình, đạt hiệu suất tương đương với WizardCoder-CL-34B chỉ với 7B tham số. Đáng chú ý rằng Magicoder-CL chỉ được đào tạo với dữ liệu đa ngôn ngữ rất hạn chế nhưng vẫn vượt trội hơn các LLM khác với kích thước tương tự hoặc thậm chí lớn hơn. Ngoài ra, mặc dù harness đánh giá các mô hình ở định dạng hoàn thành là cho các mô hình cơ sở, Magicoder vẫn cho thấy những cải thiện đáng kể mặc dù chỉ được instruction-tuned. Điều này ngụ ý rằng LLM có thể học kiến thức từ dữ liệu vượt ra ngoài định dạng của nó.

3.3. Tạo Mã cho Khoa học Dữ liệu

Bộ dữ liệu DS-1000 (Lai et al., 2022) chứa 1K vấn đề lập trình khoa học dữ liệu riêng biệt từ 7 thư viện khoa học dữ liệu phổ biến trong Python. Nó đánh giá trường hợp sử dụng thực tế và thực tiễn của một LLM và cung cấp unit test để xác thực từng bài toán. DS-1000 có cả chế độ completion và insertion, nhưng ở đây chúng tôi chỉ đánh giá completion vì CODELLAMA-PYTHON cơ sở không hỗ trợ infilling. Bảng 3 cho thấy kết quả đánh giá nơi chúng tôi bao gồm INCODER gần đây (Fried et al., 2023), CodeGen (Nijkamp et al., 2023), Code-Cushman-001 (Microsoft, 2023a), StarCoder (Li et al., 2023), CODELLAMA-PYTHON (Rozière et al., 2023), và WizardCoder (Luo et al., 2023b). Chúng tôi có thể thấy từ bảng rằng Magicoder-CL-7B đã vượt trội hơn tất cả các baseline chúng tôi đánh giá, bao gồm WizardCoder-CL-7B và WizardCoder-SC-15B tiên tiến. Magicoder S-CL-7B tiếp tục vượt qua giới hạn bằng cách giới thiệu cải thiện tuyệt đối 8.3 điểm phần trăm so với WizardCoder-SC-15B.

3.4. So sánh với DeepSeek-Coder

DeepSeek-Coder (Guo et al., 2024) là một loạt mô hình được phát hành đồng thời với công việc của chúng tôi và chúng chứng minh hiệu suất lập trình vượt trội. Chúng tôi chỉ thảo luận ngắn gọn về nó trong phần này vì dữ liệu và chi tiết instruction tuning của nó không có sẵn công khai tại thời điểm viết. Chúng tôi áp dụng cùng chiến lược tinh chỉnh trên DeepSeek-Coder-Base-6.7B như chúng tôi đã thực hiện trên CODELLAMA-PYTHON-7B, dẫn đến Magicoder-DS và Magicoder S-DS. Bảng 4 cho thấy xu hướng tương tự như Bảng 1 rằng mô hình cơ sở có thể được cải thiện đáng kể sau khi áp dụng OSS-INSTRUCT. Đáng chú ý, biến thể Magicoder S-DS vượt qua DeepSeek-Coder-Instruct-6.7B trên tất cả các benchmark với ít hơn ×8 token đào tạo, và nó cũng phù hợp gần với DeepSeek-Coder-Instruct-33B trên những bộ dữ liệu này.

4. Ablation về Nguồn Dữ liệu

4.1. Tác động của Phân phối Ngôn ngữ

Để hiểu mối tương quan giữa các ngôn ngữ lập trình xuất hiện trong dữ liệu đào tạo và hiệu suất downstream của các ngôn ngữ khác nhau, chúng tôi tiến hành một nghiên cứu ablation bổ sung về dữ liệu đào tạo. Chúng tôi phân loại dữ liệu đào tạo 75K thành khoảng 43K chỉ Python, và 32K dữ liệu non-Python theo việc liệu '''python có phải là một substring của dữ liệu được tạo ra hay không. Chúng tôi không phân loại dữ liệu dựa trên đoạn mã khởi tạo vì LLM thực hiện OSS-INSTRUCT có thể tạo ra mã trong ngôn ngữ lập trình khác với khởi tạo.

Bảng 5 cho thấy kết quả đánh giá, nơi chúng tôi nhất quán tinh chỉnh CODELLAMA-PYTHON-7B cơ sở trong 2 epoch trên các phân vùng dữ liệu khác nhau sử dụng cùng siêu tham số đào tạo được giải thích trong Phụ lục B. Từ bảng, chúng tôi có thể thấy rằng, như có thể tưởng tượng, đào tạo trên dữ liệu Python hoặc non-Python có thể thúc đẩy đáng kể hiệu suất của mô hình cơ sở trong các tác vụ Python hoặc non-Python, tương ứng. Thú vị là, instruction tuning trên các ngôn ngữ lập trình khác nhau vẫn có thể thúc đẩy hiệu suất lập trình tổng thể bao gồm các ngôn ngữ ngoài phân phối. Ví dụ, khi được đào tạo chỉ trên dữ liệu non-Python, Magicoder-CL vẫn đạt cải thiện 10.4 điểm phần trăm so với mô hình cơ sở trong đánh giá chỉ Python. Điều này ngụ ý LLM có thể thiết lập mối tương quan giữa các ngôn ngữ lập trình khác nhau và thực hiện transfer learning của ngữ nghĩa mã sâu hơn. Cuối cùng, chúng tôi quan sát thấy một sự thúc đẩy đáng kể hơn trong đánh giá Python khi kết hợp dữ liệu từ cả hai nguồn, với sự giảm nhẹ trong hiệu suất đa ngôn ngữ so với chỉ tinh chỉnh trên dữ liệu đa ngôn ngữ. Chúng tôi cho rằng sự giảm này là do lượng dữ liệu Python chiếm ưu thế (khoảng 57%) trong quá trình instruction tuning.

4.2. OSS-INSTRUCT vs. Tinh chỉnh Trực tiếp

Thực tế là OSS-INSTRUCT khiến LLM được truyền cảm hứng từ các đoạn mã nguồn mở có thể dẫn đến một câu hỏi tự nhiên: tại sao không tinh chỉnh trực tiếp trên những mã nguồn mở này? Để trả lời câu hỏi này, chúng tôi theo CodeSearchNet (Husain et al., 2020) để khai thác các cặp comment-function có liên quan về mặt ngữ nghĩa từ cùng corpus tài liệu khởi tạo chúng tôi sử dụng để xây dựng bộ dữ liệu OSS-INSTRUCT 75K. Sau đó chúng tôi đào tạo mô hình để dự đoán thân hàm từ chữ ký hàm và comment. Chúng tôi ưu tiên các cặp comment-function trùng lặp với 75K đoạn khởi tạo của chúng tôi, tạo ra khoảng 11K điểm dữ liệu. Để phù hợp với 75K mẫu của chúng tôi, chúng tôi thu thập 64K mẫu còn lại sử dụng toàn bộ corpus của 75K tài liệu khởi tạo. Cuối cùng, chúng tôi có cùng số lượng cặp comment-function với dữ liệu OSS-INSTRUCT.

Chúng tôi tinh chỉnh CODELLAMA-PYTHON-7B cơ sở trong 2 epoch sử dụng dữ liệu được ghép nối, theo cùng thiết lập đào tạo được thảo luận trong Phụ lục B. Từ Bảng 6, chúng tôi quan sát thấy rằng tinh chỉnh trên 75K dữ liệu comment-function được ghép nối thậm chí làm xấu đi mô hình cơ sở, trong khi OSS-INSTRUCT giúp giới thiệu một sự thúc đẩy đáng kể. Chúng tôi suy đoán rằng sự suy giảm là do nhiễu đáng kể và sự không nhất quán tồn tại nội tại trong các cặp dữ liệu, mặc dù những dữ liệu được ghép nối này thể hiện định dạng rất tương tự như các bài toán HumanEval hoặc MultiPL-E. Điều này tiếp tục cho thấy rằng tính thực tế của dữ liệu, thay vì định dạng, là quan trọng đối với code instruction tuning. Nó cũng cho thấy sự vượt trội của OSS-INSTRUCT có thể dịch những fragment mã liên quan lỏng lẻo này thành dữ liệu instruction-tuning nhất quán về mặt ngữ nghĩa.

--- TRANG 7 ---
[Tiếp tục với phần còn lại của bài báo với cùng định dạng dịch tiếng Việt...]
