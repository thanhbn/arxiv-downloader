# 2406.10670.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/dataset-pruning-cleaning-dedup/2406.10670.pdf
# Kích thước tệp: 1353326 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CoLoR-Filter: Lọc Giảm Thiệt Hại Có Điều Kiện
cho Tiền Huấn Luyện Mô Hình Ngôn Ngữ Có Mục Tiêu

David Brandfonbrener
Viện Kempner tại Đại học Harvard

Hanlin Zhang
Đại học Harvard

Andreas Kirsch
Đại học Oxford

Jonathan Richard Schwarz
Đại học Harvard

Sham Kakade
Viện Kempner tại Đại học Harvard

Tóm tắt
Việc lựa chọn dữ liệu chất lượng cao cho tiền huấn luyện là rất quan trọng trong việc hình thành hiệu suất của các mô hình ngôn ngữ trên các tác vụ downstream. Một thách thức lớn nằm ở việc xác định tập con tối ưu này, một vấn đề thường được coi là không thể giải quyết được, do đó đòi hỏi các phương pháp heuristic có thể mở rộng và hiệu quả. Trong công trình này, chúng tôi đề xuất một phương pháp lựa chọn dữ liệu, CoLoR-Filter (Lọc Giảm Thiệt Hại Có Điều Kiện), tận dụng một cách tiếp cận lấy cảm hứng từ Bayes thực nghiệm để đưa ra một tiêu chí lựa chọn đơn giản và hiệu quả tính toán dựa trên các giá trị thiệt hại tương đối của hai mô hình phụ trợ.

Ngoài lý do mô hình hóa, chúng tôi đánh giá CoLoR-Filter theo kinh nghiệm trên hai tác vụ mô hình ngôn ngữ: (1) lựa chọn dữ liệu từ C4 cho việc thích ứng miền để đánh giá trên Books và (2) lựa chọn dữ liệu từ C4 cho một bộ các tác vụ trả lời câu hỏi trắc nghiệm downstream. Chúng tôi chứng minh khả năng mở rộng thuận lợi cả khi chúng ta lựa chọn con một cách quyết liệt hơn và sử dụng các mô hình phụ trợ nhỏ để lựa chọn dữ liệu cho các mô hình đích lớn. Là một kết quả nổi bật, dữ liệu được lựa chọn bởi CoLoR-Filter sử dụng một cặp mô hình phụ trợ 150m tham số có thể huấn luyện một mô hình đích 1.2b tham số để khớp với một mô hình 1.2b tham số được huấn luyện trên 25b token được lựa chọn ngẫu nhiên với ít dữ liệu hơn 25 lần cho Books và ít dữ liệu hơn 11 lần cho các tác vụ downstream.

Mã nguồn: https://github.com/davidbrandfonbrener/color-filter-olmo
Dữ liệu đã lọc: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4

1 Giới thiệu
Nội dung của dữ liệu mà một mô hình ngôn ngữ được huấn luyện có thể có tác động sâu sắc đến hiệu suất của nó và hiệu quả của quá trình huấn luyện [Rae et al., 2021, Longpre et al., 2023, Penedo et al., 2023, Soboleva et al., 2023, Li et al., 2024]. Nhưng vẫn còn là một câu hỏi nghiên cứu mở về cách quyết định dữ liệu nào được đưa vào tập huấn luyện. Trong bài báo này, chúng tôi phân tích một họ các phương pháp dựa trên thiệt hại cho việc lựa chọn có mục tiêu dữ liệu tiền huấn luyện, đề xuất một phương pháp đơn giản vượt trội hơn các phương pháp hiện có, và cung cấp một số bằng chứng sơ bộ về các tính chất mở rộng thuận lợi.

Để công thức hóa vấn đề lựa chọn dữ liệu, trước tiên chúng ta cần xác định một mục tiêu định lượng liệu dữ liệu được lựa chọn có tốt hay không. Việc định nghĩa mục tiêu này đòi hỏi đánh giá một mô hình ngôn ngữ được tiền huấn luyện, đây là một lĩnh vực nghiên cứu tích cực [Gao et al., 2023, Magnusson et al., 2023, Engstrom et al., 2024, Chang et al., 2024]. Đối với bài báo này, chúng tôi sẽ lấy mục tiêu là tối đa hóa hiệu suất trên một tập các tác vụ downstream. Vì các số liệu ưa thích trên một tập tác vụ nhất định không nhất thiết giống nhau cũng không dễ dàng tối ưu hóa trực tiếp, chúng tôi coi khả năng xảy ra của các chuỗi được lấy mẫu từ các tác vụ downstream như một mục tiêu proxy. Với mục tiêu này, giờ đây chúng ta có một mục tiêu rõ ràng: cho một kho ngữ liệu rất lớn các chuỗi và một lượng nhỏ dữ liệu chất lượng cao từ một tập các tác vụ downstream, chúng ta muốn lựa chọn một tập con từ kho ngữ liệu để huấn luyện trên dữ liệu được lựa chọn
Hội nghị lần thứ 38 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2024).arXiv:2406.10670v3 [cs.LG] 29 Oct 2024

--- TRANG 2 ---
0 3 6 9 12 15 18 21 24
Token (tỷ)3.53.84.14.44.7
ít dữ liệu hơn 25 lần Books Cross Entropy (↓)
0 3 6 9 12 15 18 21 24
Token (tỷ)4045505560
ít dữ liệu hơn 11 lần Độ chính xác Downstream trung bình (↑)
CoLoR-Filter (τ= 64)
Ngẫu nhiên

Hình 1: Đường cong học tập cho các mô hình ngôn ngữ 1.2 tỷ tham số được huấn luyện trên dữ liệu được lựa chọn bởi CoLoR-Filter sử dụng các mô hình phụ trợ 150 triệu tham số nhỏ hơn cho hai phân phối đích khác nhau. (Trái) Chúng tôi hướng đến và đánh giá thiệt hại trên Books, thấp hơn thì tốt hơn. (Phải) Chúng tôi hướng đến và đánh giá độ chính xác trên một bộ 8 tác vụ downstream từ [Groeneveld et al., 2024], cao hơn thì tốt hơn. Trong cả hai trường hợp, dữ liệu thử nghiệm được giữ lại từ dữ liệu được sử dụng bởi CoLoR-Filter để hướng dẫn lựa chọn. τ là bộ nhân kích thước tập con biểu thị số lượng ví dụ được xem xét cho mỗi điểm dữ liệu được lựa chọn. Đường CoLoR-Filter kết thúc khi chúng tôi hết dữ liệu trong C4 (≈175b token có thể).

tối đa hóa khả năng xảy ra trên các tác vụ downstream. Sau đó chúng ta cũng có thể kiểm tra hiệu suất trên các tác vụ dưới các số liệu ưa thích của chúng.

Từ mục tiêu này, chúng tôi đưa ra một thuật toán được gọi là CoLoR-Filter (Lọc Giảm Thiệt Hại Có Điều Kiện). Trong Phần 2, chúng tôi suy ra phương pháp này bằng cách áp dụng quy tắc Bayes và Bayes thực nghiệm xấp xỉ cho mục tiêu khả năng xảy ra downstream. Phương pháp kết quả là đơn giản và trực quan: mỗi chuỗi được chấm điểm bởi sự khác biệt về khả năng xảy ra giữa một mô hình "tiên nghiệm" và một mô hình "có điều kiện" được tạo ra từ việc tinh chỉnh mô hình tiên nghiệm trên dữ liệu downstream. Các chuỗi có khả năng xảy ra cao hơn dưới mô hình được tinh chỉnh là tốt. Chúng tôi cũng so sánh thuật toán này với công trình trước đó (ví dụ: [Mindermann et al., 2022]) và thảo luận về chi phí tính toán.

Để đánh giá phương pháp của chúng tôi, chúng tôi xem xét hai tác vụ. Đầu tiên, trong Phần 5, chúng tôi xem xét một tác vụ bán tổng hợp nơi tác vụ downstream là mô hình ngôn ngữ trên Books. Được cung cấp quyền truy cập vào C4 [Raffel et al., 2020] như dữ liệu tiền huấn luyện tiềm năng và một mẫu nhỏ (25 triệu token) dữ liệu từ Books, chúng tôi sử dụng CoLoR-Filter và nhiều baseline khác nhau để lựa chọn 3 tỷ token. Chúng tôi thấy rằng dữ liệu được lựa chọn bởi CoLoR-Filter có thể vượt trội đáng kể so với các mô hình được huấn luyện trên dữ liệu được chọn ngẫu nhiên nhiều gấp 8 lần.

Thứ hai, trong Phần 6, chúng tôi xem xét một bộ 8 tác vụ trắc nghiệm downstream từ Groeneveld et al. [2024]. Như dữ liệu downstream, chúng tôi lấy các tập huấn luyện của các tác vụ, nhưng chúng tôi đánh giá độ chính xác trên các tập thử nghiệm được giữ lại. Chúng tôi lại thấy rằng việc lựa chọn với CoLoR-Filter vượt trội hơn việc huấn luyện trên dữ liệu được lựa chọn ngẫu nhiên nhiều gấp 8 lần. Hơn nữa, trong cả hai tác vụ, hiệu suất tăng trưởng mượt mà với siêu tham số τ điều chỉnh mức độ quyết liệt trong việc lựa chọn dữ liệu, cho thấy việc mở rộng thêm sẽ mang lại cải thiện thêm.

Ngoài việc phát hiện ra rằng CoLoR-Filter có thể lựa chọn các tập con dữ liệu tốt, chúng tôi cũng xem xét chi phí tính toán của chính thủ tục lựa chọn. CoLoR-Filter chỉ yêu cầu chạy suy luận của hai mô hình phụ trợ để lựa chọn dữ liệu. Điều này có lợi về mặt tính toán so với các phương pháp trực tuyến như RHOLoss [Mindermann et al., 2022] vì suy luận rẻ hơn huấn luyện và hoàn toàn có thể song song hóa. Để tối đa hóa lợi ích tính toán, chúng tôi cũng chỉ ra rằng dữ liệu được lựa chọn với một mô hình nhỏ (150 triệu tham số) có thể được chuyển giao cho một mô hình lớn hơn (1.2 tỷ tham số). Kết quả được hiển thị trong Hình 1, cho thấy cải thiện hiệu quả đáng kể.

2 Thiết lập và Suy luận
Giả sử chúng ta được cung cấp một tập dữ liệu tiền huấn luyện lớn Dtrain, một tập dữ liệu downstream nhỏ Ddown từ (các) tác vụ downstream quan tâm, và một tập dữ liệu "tiên nghiệm" Dprior mà chúng ta có thể sử dụng như kiến thức tiên nghiệm (trong thực tế, chúng ta thường chỉ lấy mẫu từ Dtrain). Chúng tôi sẽ giả định cho tất cả các mục đích thực tế rằng Dtrain

--- TRANG 3 ---
là vô hạn và quá trình huấn luyện tiến hành trong chế độ "trực tuyến" hoặc "một lần qua" nơi chúng ta không lặp lại các điểm dữ liệu. Mục tiêu của chúng ta là chọn một tập con S⊂Dtrain có kích thước cố định |S|=n để tối thiểu hóa thiệt hại downstream (tối đa hóa khả năng xảy ra downstream).

Phần này giới thiệu thuật toán CoLoR-Filter của chúng tôi, lấy cảm hứng từ và xây dựng dựa trên phương pháp RHOLoss từ công trình trước đó [Mindermann et al., 2022, Evans et al., 2023]. Chúng tôi cũng thảo luận về các thuật toán liên quan áp dụng cho bối cảnh này như DSIR [Xie et al., 2023] và DSDM [Engstrom et al., 2024]. Công trình liên quan bổ sung được thảo luận thêm trong Phần 3.

2.1 Lựa chọn Dữ liệu Bayesian
Mục tiêu của chúng ta có thể được hình thức hóa như một vấn đề tối ưu hóa Bayesian, nơi mục tiêu là lựa chọn một tập S để tối đa hóa xác suất hậu nghiệm của Ddown, tức là

min S⊂Dtrain,|S|=n −log Pr(Ddown|S), (1)

nơi Pr(Ddown|S) là xác suất hậu nghiệm. Áp dụng quy tắc Bayes, chúng ta được:

min S⊂Dtrain,|S|=n −log Pr(S|Ddown) + log Pr(S) − log Pr(Ddown) (2)

Lưu ý rằng số hạng cuối cùng không phụ thuộc vào S, nên có thể bỏ qua khi tối ưu hóa qua S. Đưa vào một tiên nghiệm trên các tham số mô hình θ, chúng ta có:

min S⊂Dtrain,|S|=n −log∫θ Pr(S|θ) Pr(θ|Ddown) + log∫θ Pr(S|θ) Pr(θ) (3)
                    "có điều kiện"                "biên"

Chúng tôi sẽ gọi hai số hạng này là số hạng có điều kiện và biên, tương ứng.¹ Lưu ý rằng các số hạng có điều kiện và biên cùng nhau tạo nên thông tin tương hỗ điểm âm giữa dữ liệu được lựa chọn và downstream, có kết nối sâu sắc với công trình trước đó về học tập chủ động và lấy mẫu chủ động [Lindley, 1956, Moore and Lewis, 2010, Houlsby et al., 2011, Bickford Smith et al., 2023, Kirsch, 2023, Rainforth et al., 2024].

2.2 CoLoR-Filter
Cho rằng chúng ta có quyền truy cập vào kiến thức tiên nghiệm từ tập dữ liệu Dprior, chúng ta có thể thay thế tiên nghiệm không có thông tin trên θ bằng một tiên nghiệm Bayes thực nghiệm có điều kiện trên Dprior để có được:

min S⊂Dtrain,|S|=n −log∫θ Pr(S|θ) Pr(θ|Ddown, Dprior) + log∫θ Pr(S|θ) Pr(θ|Dprior) (4)

Vì việc tích phân này vẫn không thể giải được, giờ đây chúng tôi đưa ra giả định đơn giản hóa chính của mình là thay thế việc tích phân qua các tham số này bằng một ước tính điểm:

≈ min S⊂Dtrain,|S|=n −log Pr(S|θprior+down) + log Pr(S|θprior), (5)

nơi θprior là một mô hình được huấn luyện trên Dprior và θprior+down là một mô hình được huấn luyện trên cả Dprior và Ddown (trong thực tế, chúng tôi sử dụng một mô hình được tiền huấn luyện trên Dprior và được tinh chỉnh trên Ddown).

Hơn nữa, xấp xỉ này dẫn đến lợi ích tính toán bằng cách tránh tối ưu hóa tổ hợp đầy đủ của việc lựa chọn tập con. Đặc biệt, một khi chúng ta có điều kiện trên một mô hình duy nhất θ, và giả định rằng phân phối trên các điểm x∈S là độc lập, tức là Pr(S|θ) = ∏x∈S Pr(x|θ), chúng ta có:

min {x₁,...,xₙ}⊂Dtrain −log∏ᵢ₌₁ⁿ Pr(xᵢ|θprior+down) + log∏ᵢ₌₁ⁿ Pr(xᵢ|θprior) (6)

điều này được đơn giản hóa thành:

min {x₁,...,xₙ}⊂Dtrain Σᵢ₌₁ⁿ −log Pr(xᵢ|θprior+down) − (−log Pr(xᵢ|θprior)) (7)

¹ Công trình trước đó [Mindermann et al., 2022, Evans et al., 2023] đã gọi các mô hình ước tính hai số hạng này là "tham chiếu" và "người học" hoặc "actor", tương ứng. Chúng tôi chọn tên có điều kiện và biên để rõ ràng hơn trong kết nối với quan điểm Bayesian.

--- TRANG 4 ---
Điều này đưa ra tiêu chí CoLoR-Filter mà chúng tôi sử dụng để lựa chọn dữ liệu. Việc tối ưu hóa này lựa chọn các điểm có sự giảm thiệt hại có điều kiện (CoLoR) lớn nhất, tức là các điểm nơi thiệt hại log-likelihood âm của mô hình có điều kiện θprior+down thấp hơn so với mô hình biên θprior. Trực quan, điều này lựa chọn các điểm dữ liệu có khả năng xảy ra cao hơn dưới mô hình có điều kiện so với mô hình biên.

Một lưu ý về tính đa dạng dữ liệu. Trong khi việc phân tích tích đó kết quả từ ước tính điểm của các tham số là thuận tiện về mặt tính toán, nó đưa ra một giả định đơn giản hóa quan trọng. Đặc biệt, mục tiêu CoLoR-Filter không còn khuyến khích việc lựa chọn một tập dữ liệu đa dạng, vì điểm số được áp dụng độc lập cho mỗi điểm. Trong thực tế, điều này được khắc phục bởi một vài cân nhắc: (1) chúng ta có thể chạy CoLoR-Filter trên một kho ngữ liệu đã được khử trùng lặp để ngăn chặn việc trùng lặp suy thoái, (2) đối với n lớn, chúng ta phải lựa chọn nhiều điểm dữ liệu khác nhau, và (3) mỗi điểm dữ liệu bản thân nó là một chuỗi có thể chứa tín hiệu đa dạng trên các token. Chúng ta cũng nên lưu ý rằng đây không phải là một tính chất duy nhất của CoLoR-Filter và cũng xảy ra trong các phương pháp khác thực hiện chấm điểm offline như DSDM và DSIR. Chúng tôi hoãn thảo luận chi tiết về các sắc thái của vấn đề này đến Phụ lục C.

2.3 Các Thuật toán Liên quan
Kết nối với lấy mẫu quan trọng. Vì mục tiêu CoLoR-Filter được viết như một hiệu của log, nó cũng có thể được viết như một log của tỷ lệ giữa xác suất dưới θprior+down và θprior. Nếu dữ liệu thực sự được lấy mẫu từ θprior, thì tỷ lệ này sẽ là trọng số quan trọng cần thiết để tái trọng số các mẫu để chúng đến từ mô hình được định nghĩa bởi θprior+down. Lưu ý rằng DSIR [Xie et al., 2023] trực tiếp cố gắng thực hiện lấy mẫu quan trọng từ Dtrain đến Ddown thay vì tối ưu hóa hiệu suất trên dữ liệu downstream. Do đó, DSIR kết thúc với một thuật toán có phần liên quan ngoại trừ trong DSIR: (1) không có mô hình ngôn ngữ, chỉ có các đặc trưng của một điểm dữ liệu đầy đủ (n-gram được băm), và (2) thuật toán lấy mẫu thay vì tối ưu hóa.

Kết nối với DSDM. Một phương pháp liên quan khác là DSDM [Engstrom et al., 2024] sử dụng một bộ ước tính TRAK Datamodel [Ilyas et al., 2022, Park et al., 2023] để chấm điểm các điểm dữ liệu và sau đó lựa chọn n điểm hàng đầu. Động lực và bối cảnh của DSDM tương tự CoLoR-Filter, nhưng DSDM dựa vào TRAK xây dựng một xấp xỉ tuyến tính của ảnh hưởng mà các điểm dữ liệu có lên nhau. Thay vào đó, CoLoR-Filter hoạt động trực tiếp trong không gian hàm bằng cách so sánh thiệt hại giữa các mô hình trực tiếp thay vì dựa vào các xấp xỉ tuyến tính hoặc Datamodel [Ilyas et al., 2022].

Kết nối với RHO-down. CoLoR-Filter được lấy cảm hứng từ và xây dựng dựa trên phương pháp RHOLoss được giới thiệu trong công trình trước đó [Mindermann et al., 2022] với những khác biệt tinh tế nhưng đáng kể trong bối cảnh: bài báo RHO gốc tập trung vào các trường hợp nơi dữ liệu giữ lại được lấy mẫu từ cùng một phân phối như Dtrain qua nhiều epoch huấn luyện. Ngược lại, chúng tôi tập trung vào việc lựa chọn dữ liệu để hướng đến các phân phối downstream khác với Dtrain và nơi chúng ta chỉ thực hiện một lần qua dữ liệu. Ở đây, chúng tôi suy ra một sự thích ứng đơn giản của RHOLoss cho bối cảnh của chúng tôi, mà chúng tôi gọi là RHO-down.

Bây giờ chúng tôi suy ra RHO-down trong bối cảnh của chúng tôi, nhằm minh họa các kết nối giữa RHO-down và CoLoR-Filter. Đầu tiên, RHO-down xấp xỉ vấn đề lựa chọn tập con đầy đủ từ Phương trình (3) bằng một xấp xỉ tham lam (tuần tự) nơi các mẫu được thêm vào S từng (lô) một. Sử dụng kích thước lô 1, mẫu thứ i lý tưởng sẽ được thêm vào theo tiêu chí sau:

≈ min xi∈Dtrain −log∫θ Pr(xi|θ) Pr(θ|Ddown, x<i) + log∫θ Pr(xi|θ) Pr(θ|x<i), (8)

nơi i chạy từ 1 đến n tuần tự. RHO-down sau đó sử dụng một ước tính điểm của các tham số (như chúng ta làm trong CoLoR-Filter):

≈ min xi∈Dtrain −log Pr(xi|θdown+x<i) + log Pr(xi|θx<i) (9)

Cuối cùng, các tác giả RHO-down thấy rằng việc cập nhật số hạng có điều kiện để phụ thuộc vào x<i là không ổn định, nên họ xấp xỉ điều này bằng một mô hình cố định θdown:

≈ min xi∈Dtrain −log Pr(xi|θdown) + log Pr(xi|θx<i). (10)

Lưu ý rằng trong khi cả CoLoR-Filter và RHO-down đều xấp xỉ hậu nghiệm trên các tham số với một ước tính điểm, RHO-down đưa ra một vài xấp xỉ bổ sung. Điều này chủ yếu là kết quả của

--- TRANG 5 ---
RHO-down cố gắng tăng tính đa dạng dữ liệu bằng cách sử dụng một phương pháp tuần tự để lựa chọn có điều kiện trên dữ liệu đã được chọn trước đó x<i. Đây là một mục tiêu có thể hiểu được, nhưng nó đưa ra nhiều xấp xỉ hơn, có thể gây ra bất ổn bằng cách tạo ra một phân phối dữ liệu không dừng, và tốn kém về mặt tính toán vì việc lựa chọn dữ liệu không còn có thể song song hóa được. Một thảo luận tiếp tục về ưu và nhược điểm của lựa chọn trực tuyến có trong Phụ lục C.

RHO-down + prior. Chúng tôi cũng xem xét một phiên bản của thuật toán mà chúng tôi gọi là "RHO-down + prior" thay thế Ddown, θdown trong thuật toán RHO-down bằng Dprior∪Ddown, θprior+down để kết hợp thông tin tiên nghiệm. Điều này tương ứng với việc có điều kiện trên cả Dprior và Ddown thay vì chỉ Ddown. Trực quan, phương pháp này có thể tận dụng tốt hơn các đặc trưng mạnh hơn được học trên Dprior lớn hơn để tích hợp thông tin từ Ddown nhỏ.

3 Công trình Liên quan Thêm
Bây giờ chúng tôi thảo luận về một số công trình liên quan, rộng hơn, liên quan đến học tập chủ động và curation dữ liệu.

Học tập chủ động & Curriculum. Công thức hóa lựa chọn dữ liệu của chúng tôi có kết nối với học tập chủ động cổ điển và sâu [Houlsby et al., 2011, Bickford Smith et al., 2023, Kirsch, 2023], được gốc rễ sâu sắc trong thiết kế thực nghiệm Bayesian tối ưu [Lindley, 1956, Rainforth et al., 2024], mục tiêu của nó là lựa chọn một tập các thí nghiệm để tối ưu hóa các tiêu chí thông tin nhất định [Pukelsheim, 2006] như tối đa hóa việc giảm bất định về các tham số mô hình. Các hàm thu nhận khác nhau được đề xuất trong các chế độ học sâu [Sener and Savarese, 2018, Ash et al., 2019, 2021] và hầu hết chúng tập trung vào phân loại hình ảnh hiệu quả về nhãn. Một dòng kỹ thuật gần đây khác chia sẻ các kết nối phương pháp luận sâu sắc nhưng nhấn mạnh việc lựa chọn con dữ liệu có sẵn trong quá trình huấn luyện (thay vì việc thu thập các ví dụ bổ sung thường được xem xét trong học tập chủ động) và do đó có thể được phân loại là học tập curriculum [ví dụ: Graves et al., 2017]. Trong số chúng, RHOLoss [Mindermann et al., 2022] tìm cách lựa chọn dữ liệu dựa trên tập dữ liệu tham chiếu giữ lại từ cùng một phân phối với dữ liệu huấn luyện. Nó sau đó đã được triển khai trong tiền huấn luyện liên tục [Lin et al., 2024] và các miền thị giác [Evans et al., 2023, Tack et al., 2024].

Thực hành curation dữ liệu trong tiền huấn luyện. Mặc dù dữ liệu web-crawl công cộng quy mô lớn là nguồn dữ liệu phổ biến cho các mô hình tiền huấn luyện, nội dung chất lượng thấp, độc hại và không có thông tin có thể ngăn chặn tiền huấn luyện thành công là phổ biến [Wenzek et al., 2020, Elazar et al., 2023, Sorscher et al., 2022, Allen-Zhu and Li, 2024]. Do đó, các thực hành viên thiết kế các pipeline tiền xử lý dữ liệu phức tạp như lọc [Brown et al., 2020], khử trùng lặp [Lee et al., 2022], và trộn [Touvron et al., 2023a,b] để cải thiện chất lượng dữ liệu. Do quy mô to lớn, các tập dữ liệu tiền huấn luyện tiên tiến thường phụ thuộc vào các bộ lọc heuristic đơn giản [Raffel et al., 2020, Rae et al., 2021, Computer, 2023] (ví dụ: URL, độ dài, perplexity n-gram, các bộ phân loại nhanh nhất) có thể được song song hóa trên các nút CPU. Bên cạnh việc lọc dựa trên quy tắc ở trên, lọc dựa trên mô hình quan tâm đến việc sử dụng các mô hình machine learning để chấm điểm và lọc dữ liệu, đã được chứng minh là hiệu quả trong các miền thị giác và thị giác-văn bản [Schuhmann et al., 2022, Abbas et al., 2023, Fang et al., 2023]. Các phương pháp như vậy thường tận dụng một nguồn dữ liệu đáng tin cậy như Wikipedia hoặc Books làm tham chiếu và so sánh dữ liệu thô với nó. Do chi phí tính toán, các mô hình thường được thiết kế để nhỏ như n-gram [Xie et al., 2023], mạng thần kinh một lớp [Joulin et al., 2017, Brown et al., 2020], phân cụm k-means [Tirumala et al., 2024]. Cũng có một dòng công trình đang phát triển minh họa rằng chất lượng dữ liệu quan trọng trong việc hình thành huấn luyện mô hình từ nhiều góc độ khác nhau, như tăng quy mô dữ liệu [Hoffmann et al., 2022, Meta, 2024] và sử dụng dữ liệu tổng hợp [Gunasekar et al., 2023].

4 Thuật toán
4.1 Từ Suy luận đến Thuật toán Thực tế
Trong các thí nghiệm của chúng tôi, chúng tôi sẽ xem xét bốn thuật toán dựa trên các suy luận ở trên. Trong phần này, chúng tôi đi qua từng thuật toán một cách lần lượt.

CoLoR-Filter. Thuật toán được đề xuất của chúng tôi được trình bày chính thức trong Thuật toán 1. So với suy luận, khác biệt chính là việc đưa vào τ, một siêu tham số hoạt động như một đánh đổi tính toán-hiệu suất kiểm soát mức độ tốn kém và quyết liệt của việc lựa chọn dữ liệu. Thay vì

--- TRANG 6 ---
Thuật toán 1 CoLoR-Filter
Yêu cầu: Dữ liệu tiên nghiệm Dprior, dữ liệu downstream Ddown, dữ liệu huấn luyện Dtrain, ngân sách n, bộ nhân kích thước tập con τ
1: Tiền huấn luyện θmarg trên Dprior
2: tinh chỉnh để có được θcond trên Ddown được khởi tạo từ θmarg
3: Chọn một tập con ngẫu nhiên Dτ có kích thước τn từ Dtrain
4: Chọn dữ liệu: S = bottom-n x∈Dτ −log Pr(x|θcond) + log Pr(x|θmarg)
5: return Tập dữ liệu được chọn S để huấn luyện θ.

lựa chọn dữ liệu từ tất cả Dtrain, chúng tôi lấy một tập con ngẫu nhiên Dτ có kích thước τn. Do đó, τ lớn hơn lựa chọn con quyết liệt hơn, nhưng với chi phí tính toán nhiều hơn. Một thảo luận đầy đủ về chi phí này có trong Phần 4.2.

Chỉ có điều kiện. Như một ablation của CoLoR-Filter, chúng tôi theo công trình trước đó [Evans et al., 2023] và bao gồm một baseline chỉ sử dụng mô hình có điều kiện để lựa chọn dữ liệu. Về cơ bản, đây là CoLoR-Filter nếu chúng ta luôn giả định rằng log Pr(x|θmarg) = 0 trong Dòng 4 của Thuật toán 1.

Thuật toán 2 RHO-down
Yêu cầu: Dữ liệu downstream Ddown, dữ liệu huấn luyện Dtrain, ngân sách n, bộ nhân kích thước tập con τ, kích thước lô b
1: Huấn luyện θcond trên Ddown
2: Khởi tạo một θmarg ngẫu nhiên và S = ∅
3: for t ∈ [1, . . . , n/b] do
4:    Chọn ngẫu nhiên một lô Bt⊂Dtrain có kích thước τb
5:    Chọn dữ liệu: B̄t = bottom-b x∈Bt −log Pr(x|θcond) + log Pr(x|θmarg)
6:    S = S ∪ B̄t
7:    Cập nhật θmarg từ θmarg sang θmarg bằng cách huấn luyện trên B̄t
8: end for
9: return Tập dữ liệu được chọn S để huấn luyện θ.

RHO-down. Chúng tôi trình bày một biến thể thực tế của RHO-down trong Thuật toán 2 dựa trên suy luận được trình bày trong Phần 2. Các thay đổi chính để tạo ra một thuật toán thực tế là (1) việc đưa vào τ như trong CoLoR-Filter, và (2) thực hiện thuật toán theo lô thay vì sử dụng các điểm dữ liệu đơn lẻ.

RHO-down + Prior. Chúng tôi cũng có thể kết hợp dữ liệu tiên nghiệm Dprior vào Thuật toán 2 bằng cách đơn giản thay thế Dòng 1 nơi θcond được huấn luyện trên Ddown bằng một thủ tục mà chúng tôi đầu tiên tiền huấn luyện θcond trên Dprior và sau đó tinh chỉnh nó trên Ddown.

4.2 Chi phí Tính toán
Để đánh giá chi phí tính toán của các thuật toán khác nhau, chúng tôi sử dụng đơn vị "forward mô hình" trên một token nơi chúng tôi giả định rằng một backward pass tốn kém gấp đôi một forward pass [Fleuret, 2023]. Lưu ý rằng các mô hình 150m của chúng tôi mất khoảng 5e8 FLOP cho mỗi forward mô hình của một token duy nhất [Hoffmann et al., 2022, Casson, 2023]. Chi phí chạy các thuật toán lựa chọn phụ thuộc vào m, n, τ và L được định nghĩa như sau: m là kích thước của dữ liệu tiên nghiệm Dprior, n là kích thước của tập dữ liệu được chọn S, τ là siêu tham số kiểm soát mức độ quyết liệt chúng ta lựa chọn con dữ liệu. Lưu ý rằng chúng tôi giả định rằng |Ddown| quá nhỏ đến mức chi phí huấn luyện một mô hình trên Ddown là không đáng kể đối với tổng chi phí (và tất cả các phương pháp chúng tôi xem xét chỉ tinh chỉnh một mô hình một lần trên Ddown). Chúng tôi cũng sẽ cẩn thận lưu ý khi tính toán có thể được thực hiện song song trước khi huấn luyện so với tính toán phải xảy ra tuần tự trong quá trình chạy huấn luyện. Các thuật toán offline như CoLoR-Filter có thể tận dụng tính song song để cải thiện hiệu quả. Trong phần này, chúng tôi đi qua từng phương pháp một cách lần lượt và tổng hợp chi phí tính toán trong bảng 1.

Chuyển giao quy mô. Chúng tôi cũng bao gồm một tham số khác L để bao phủ trường hợp chúng ta lựa chọn dữ liệu sử dụng các mô hình nhỏ và sử dụng nó để huấn luyện một mô hình lớn hơn [Evans et al., 2023]. Cụ thể, L là tỷ lệ chi phí của một forward mô hình của mô hình đích lớn so với các mô hình phụ trợ nhỏ được sử dụng cho

--- TRANG 7 ---
Bảng 1: Chi phí tính toán của các thuật toán khác nhau được đo bằng "forward mô hình". Tổng chi phí lựa chọn và huấn luyện trên dữ liệu được chọn là tổng của tất cả chi phí trong một hàng. Các biến là m=|Dprior|, n=|S|, τ là một siêu tham số kiểm soát mức độ quyết liệt chúng ta lựa chọn con, và L là một bộ nhân của chi phí forward mô hình giữa (các) mô hình lựa chọn và mô hình đích (xấp xỉ tỷ lệ số lượng tham số giữa các mô hình).

Phương pháp | Chi phí Tiên nghiệm | Chi phí Tuần tự | Chi phí Song song | Chi phí Huấn luyện
CoLoR-Filter | 3m | 0 | 2τn | 3nL
Chỉ Có điều kiện | 3m | 0 | τn | 3nL
RHO-down | 0 | τn + 2n | τn | 3nL
RHO-down + Prior | 3m | τn + 2n | τn | 3nL
Ngẫu nhiên | 0 | 0 | 0 | 3nL

lựa chọn. Ví dụ, trong các thí nghiệm của chúng tôi, khi chúng tôi sử dụng các mô hình 150 triệu tham số để lựa chọn dữ liệu và sau đó huấn luyện một mô hình 1.2 tỷ tham số trên dữ liệu kết quả, thì L≈5.5². Huấn luyện do đó tốn 3nL trên tất cả các phương pháp vì chúng tôi chạy một forward và backward cho mô hình lớn trên tất cả n chuỗi.

CoLoR-Filter. Chi phí lựa chọn là 2τn forward pass. Nhưng, quá trình lựa chọn này hoàn toàn có thể song song hóa. Huấn luyện mô hình tiên nghiệm tốn 3m forward vì |Dprior|=m. Và huấn luyện một mô hình trên dữ liệu được chọn tốn 3nL forward pass. Vậy tổng chi phí là 3m + 2τn + 3nL, nhưng việc tính toán chấm điểm 2τn có thể được thực hiện song song.

Chỉ có điều kiện. Phương pháp chỉ có điều kiện gần như giống với CoLoR-Filter, ngoại trừ chúng ta chỉ cần τn forward pass để lựa chọn vì chúng ta chỉ chạy một mô hình qua dữ liệu. Chi phí do đó là 3m + τn + 3nL, với τn có thể song song hóa.

RHO-down. Chi phí lựa chọn vẫn là 2τn forward pass. Sau đó chúng ta cần thêm 2n để backward mô hình đầu ra (vì forward đã được xử lý trong quá trình chấm điểm). Lưu ý rằng chúng ta cần đánh giá mô hình biên trực tuyến, nên không thể song song hóa, nhưng mô hình có điều kiện là cố định và có thể được tính toán offline. Vậy, chi phí là 2τn + 2n + 3nL, và việc tính toán mô hình có điều kiện τn có thể được thực hiện song song.

RHO-down + Prior. Đối với phiên bản có thêm tiên nghiệm, chúng ta chỉ thêm chi phí 3m cho huấn luyện tiên nghiệm. Do đó, chi phí là 2τn + 2n + 3nL với τn có thể song song hóa.

Nhìn chung, tất cả các phương pháp có chi phí tương đương, với Chỉ có điều kiện là rẻ nhất và RHO-down + Prior là đắt nhất. Khác biệt chính là CoLoR-Filter và Chỉ có điều kiện dễ dàng song song hóa trong khi RHO-down và RHO-down + Prior thì không. Cũng nên lưu ý rằng khi thực hiện thí nghiệm, các phương pháp offline như CoLoR-Filter cũng được hưởng lợi từ việc có thể tái sử dụng khả năng xảy ra nhiều lần, trong khi các phương pháp dựa trên RHO cần tính toán lại chi phí tuần tự bất kỳ lúc nào mà một số siêu tham số của thuật toán thay đổi.

5 Chuyển giao Miền: một Testbed Đơn giản
5.1 Thiết lập
Huấn luyện. Chúng tôi huấn luyện các mô hình ngôn ngữ với 150 triệu tham số non-embedding sử dụng codebase OLMo [Groeneveld et al., 2024] và theo các lựa chọn siêu tham số từ [Wortsman et al., 2024]. Trừ khi có ghi chú khác, chúng tôi sử dụng các mô hình 150m như các mô hình phụ trợ (θcond, θmarg) cũng như mô hình đích θ. Siêu tham số đầy đủ được mô tả chi tiết trong Phụ lục H.

Chúng tôi lấy Ddown là một tập dữ liệu nhỏ 25 triệu token được lấy mẫu từ tập con dữ liệu Project Gutenberg Books của Dolma [Soldaini et al., 2024], Dprior là một tập dữ liệu 3.1 tỷ token từ C4 [Raffel et al., 2020], và Dtrain là toàn bộ C4. Chúng tôi chọn một tập dữ liệu S có 3.1 tỷ token (xấp xỉ số lượng "tối ưu chinchilla" cho các mô hình có kích thước này). Để có được θprior+down hoặc θdown, chúng tôi tinh chỉnh hoặc huấn luyện một epoch trên Ddown.

Đánh giá. Để đánh giá hiệu quả của việc lựa chọn dữ liệu, chúng tôi báo cáo thiệt hại cross-entropy của dự đoán token tiếp theo trên một tập dữ liệu giữ lại ẽDdown từ cùng phân phối với Ddown (Books).

Baseline. Baseline đơn giản nhất chúng tôi xem xét là lấy mẫu Ngẫu nhiên, đã được chỉ ra là một baseline mạnh cho tiền huấn luyện C4 [Engstrom et al., 2024]. Chúng tôi xem xét tất cả bốn thuật toán được mô tả trong Phần 4: CoLoR-Filter, Chỉ có điều kiện, RHO-down, và RHO-down + prior. Và như một baseline bổ sung, chúng tôi cũng bao gồm DSIR [Xie et al., 2023] ước tính trọng số quan trọng n-gram giữa Dtrain và Ddown, và tương tự có một tham số như τ kiểm soát mức độ quyết liệt lựa chọn con.

Lưu ý rằng mặc dù ở trong bối cảnh tương tự với chúng tôi, chúng tôi không bao gồm DSDM [Engstrom et al., 2024] như một baseline vì không có mã nguồn công khai và dựa trên phụ lục của bài báo đó, nó tốn kém về mặt tính toán hơn nhiều so với các phương pháp chúng tôi xem xét.

5.2 Kết quả
Đầu tiên chúng tôi chạy các thí nghiệm chuyển giao miền trên các mô hình 150m, quét qua τ kiểm soát kích thước tập con được chọn. Trong Hình 2, chúng tôi vẽ cách hiệu suất cuối cùng mở rộng với τ trên các phương pháp. Chúng tôi thấy rằng CoLoR-Filter có hiệu suất mở rộng tốt nhất với τ tăng, không có dấu hiệu bão hòa ở τ = 16. Chúng tôi đặt giả thuyết rằng bằng cách sử dụng các mô hình mạnh để lựa chọn dữ liệu, CoLoR-Filter có thể mở rộng hiệu quả hơn đến τ lớn hơn so với các phương pháp khác. Trong Hình 7 ở Phụ lục A, chúng tôi vẽ các đường cong học tập (được đánh giá trên tập validation giữ lại) cho bốn phương pháp được giới thiệu trong Phần 4. Ở đó, chúng tôi thấy việc mở rộng đặc biệt sạch sẽ cho CoLoR-Filter trên toàn bộ đường cong học tập, vượt trội đáng kể so với lựa chọn ngẫu nhiên với ít dữ liệu hơn nhiều, tương tự như Hình 1.

Chuyển giao quy mô. Cuối cùng, chúng tôi cũng tiến hành một thí nghiệm về chuyển giao quy mô (một phần được hiển thị trong Hình 1) sử dụng dữ liệu được lựa chọn bởi các mô hình phụ trợ 150m để huấn luyện một mô hình đích 1.2b. Trong Hình 3, chúng tôi hiển thị các đường cong học tập cho một quét qua τ. Chúng tôi vẫn thấy những lợi ích nhất quán khi chúng ta mở rộng τ cho một số lượng token huấn luyện cố định. Thú vị là, nếu chúng ta cố định tổng số token chúng ta đang lựa chọn (tức là nơi các đường kết thúc khi chúng ta hết C4), thì hiệu suất cuối cùng với τ = 32 tốt hơn tất cả các giá trị τ khác. Điều này cho thấy một tập con nghiêm ngặt của token có thể vượt trội hơn một tập con lớn hơn (ví dụ: τ = 16). Chúng tôi cũng nên chỉ ra ở đây việc tiết kiệm tính toán khi sử dụng CoLoR-Filter. Ví dụ, xem xét τ = 16 nơi chúng ta khớp hiệu suất của 25 tỷ token được chọn ngẫu nhiên với khoảng 1.5 tỷ token được lọc. Xem xét chi phí tính toán đã thảo luận ở trên với L = 5.5 và đo n bằng tỷ token, tổng chi phí để huấn luyện mô hình CoLoR-Filter là 3m + 2τn + 3nL = 3*3.1 + 2*16*1.5 + 3*1.5*5.5 = 82 trong khi chi phí để huấn luyện trên 25 tỷ token ngẫu nhiên là 3NL = 3*25*5.5 = 412.5, minh họa một tiết kiệm tính toán hơn 5 lần để đạt được cùng hiệu suất trên Books. Một biểu đồ đầy đủ hình dung chi phí bằng FLOP cho tất cả τ có trong Phụ lục D.

--- TRANG 8 ---
21 22 23 24
τ 4.2 4.4 4.6 4.8 Final Books Cross Entropy (↓)
CoLoR-Filter
Chỉ có điều kiện
RHO-down
RHO-down + prior
DSIR
Ngẫu nhiên 1x
Ngẫu nhiên 8x

Hình 2: Mở rộng hiệu suất cuối cùng với τ khi hướng đến Books với các mô hình tham số 150m.

0 3 6 9 12 15 18 21 24
Token (tỷ) 3.5 3.8 4.1 4.4 4.7 Books Cross Entropy (↓)
CoLoR-Filter (τ= 64)
CoLoR-Filter (τ= 32)
CoLoR-Filter (τ= 16)
CoLoR-Filter (τ= 7)
Ngẫu nhiên

Hình 3: Mở rộng CoLoR-Filter với τ khi huấn luyện các mô hình 1.2b với dữ liệu được chọn bởi các mô hình 150m. Các đường cong kết thúc khi chúng ta cạn kiệt dữ liệu trong C4.

--- TRANG 9 ---
hellaswag | piqa | arc challenge | arc easy | openbook qa | sciq | boolq | winogrande | Trung bình
Tác vụ kiểm tra | CoLoR-Filter | Chỉ có điều kiện | RHO-down | RHO-down + prior | DSIR | Ngẫu nhiên 8x
Phương pháp | 5.5 | 4.1 | 2.9 | 7.4 | 5.2 | 5.9 | -4.6 | -3.9 | 2.8
-0.1 | 1.1 | 0.6 | -2.2 | 0.4 | -2.3 | 2.5 | -2.1 | -0.3
2.4 | 2.8 | 2.9 | 2.5 | 2.4 | 0.6 | -10.2 | -4.6 | -0.2
2.4 | 2.1 | 2.9 | 4.9 | 2.6 | 2.1 | 2.7 | -2.4 | 2.2
4.4 | 4.3 | 2.0 | 2.2 | 1.0 | 1.5 | 1.1 | -0.7 | 2.0
5.1 | 3.3 | 1.2 | -0.2 | 2.0 | -1.6 | -0.7 | -2.8 | 0.8
Cải thiện so với Ngẫu nhiên 1x (τ= 16) (↑)

Hình 5: Cải thiện hiệu suất so với huấn luyện trên lượng dữ liệu ngẫu nhiên tương đương được chia theo tác vụ (ngoại trừ Ngẫu nhiên 8x, sử dụng nhiều dữ liệu hơn 8 lần). Một bảng kết quả có trong Phụ lục B.

6 Các Tác vụ Downstream
6.1 Thiết lập
Huấn luyện. Chúng tôi hướng đến 8 tác vụ từ bài báo OLMo [Groeneveld et al., 2024]: Hellaswag [Zellers et al., 2019], PIQA [Bisk et al., 2020], ARC-challenge và ARC-easy [Clark et al., 2018], Openbook QA [Mihaylov et al., 2018], SciQ [Welbl et al., 2017], BoolQ [Clark et al., 2019], và Winogrande [Sakaguchi et al., 2021]. Mỗi tập dữ liệu này có một phần train riêng biệt. Chúng tôi sử dụng các phần train này để xây dựng Ddown như sau: đối với mỗi câu hỏi, chúng tôi nối câu hỏi và câu trả lời đúng được định dạng như một sự tiếp tục ngữ pháp. Nhìn chung, điều này dẫn đến một tập dữ liệu Ddown nhỏ có 7.4 triệu token. Dprior và Dtrain giống như trước đó. Và chúng tôi lại có được θprior+down bằng cách tinh chỉnh θprior trong một epoch trên Ddown.

21 22 23 24
τ 45 46 47 48 49 50 Final Average Accuracy (↑)
CoLoR-Filter
Chỉ có điều kiện
RHO-down
RHO-down + prior
DSIR
Ngẫu nhiên 1x
Ngẫu nhiên 8x

Hình 4: Hiệu suất cuối cùng so với τ trên bộ các tác vụ downstream cho các mô hình 150m. CoLoR-Filter mở rộng tốt nhất với τ.

Đánh giá. Chúng tôi đánh giá trên dữ liệu giữ lại từ mỗi tập test hoặc validation của tác vụ downstream (sử dụng val nếu test không có sẵn công khai). Chúng tôi sử dụng thủ tục đánh giá từ OLMo [Groeneveld et al., 2024] theo [Gao et al., 2023] để đánh giá các tác vụ trắc nghiệm này sử dụng phương pháp phân loại theo thứ hạng của Brown et al. [2020]. Chúng tôi báo cáo hiệu suất tổng hợp trên các tác vụ cũng như hiệu suất cụ thể theo tác vụ.

Baseline. Giống như trong Phần 5.

6.2 Kết quả
Mặc dù bản thân các đường cong giờ đây nhiều nhiễu hơn do tính chất nhiễu hơn của đánh giá độ chính xác trên các tập dữ liệu nhỏ so với cross entropy trên một tập lớn, các xu hướng tương tự tồn tại như chúng ta đã thấy cho chuyển giao miền sang Books. CoLoR-Filter đặc biệt đang mở rộng tốt nhất khi chúng ta tăng τ. Các phương pháp khác không minh họa cùng việc mở rộng sạch sẽ khi chúng ta tăng τ, gần như tuyến tính trên thang logarit cho CoLoR-Filter, như được thấy trong Hình 4. Các đường cong học tập đầy đủ có trong Phụ lục A.

Chúng ta cũng có thể nhìn vào hiệu suất được chia theo tác vụ và minh họa tương đối với việc huấn luyện trên một lượng tương đương (3.1 tỷ token) dữ liệu được chọn ngẫu nhiên cho τ = 16 được minh họa trong Hình 5. Chúng ta thấy những lợi ích đặc biệt lớn trên Hellaswag, ARC easy, Openbook QA và SciQ và thực sự thấy giảm hiệu suất trên BoolQ và Winogrande. Tuy nhiên, chúng ta nên lưu ý rằng ở quy mô này và với tất cả dữ liệu được chọn từ C4, chúng tôi thực sự thấy BoolQ và Winogrande khá nhiễu và thậm chí không tương quan với việc huấn luyện trên dữ liệu ngẫu nhiên nhiều gấp 8 lần, vì vậy không rõ nên đặt bao nhiêu trọng số vào những kết quả đó. Trên các tác vụ khác, những lợi ích của CoLoR-Filter so với các baseline là rõ ràng. Đây là một hướng nghiên cứu thú vị cho công việc tương lai để khám phá sâu hơn về việc các lợi ích từ lựa chọn dữ liệu có mục tiêu có thể phụ thuộc vào tác vụ như thế nào.

0 3 6 9 12 15 18 21 24
Token (tỷ) 40 45 50 55 60 Average Downstream Accuracy (↑)
CoLoR-Filter (τ= 64)
CoLoR-Filter (τ= 32)
CoLoR-Filter (τ= 16)
CoLoR-Filter (τ= 7)
Ngẫu nhiên

Hình 6: Mở rộng CoLoR-Filter với τ khi huấn luyện các mô hình 1.2b với dữ liệu được chọn bằng các mô hình 150m nhỏ hơn. Các đường cong kết thúc khi chúng ta cạn kiệt dữ liệu trong C4.

Chuyển giao quy mô. Chúng tôi cũng xem xét chuyển giao quy mô sang một mô hình đích 1.2b và minh họa kết quả đầy đủ của một quét qua τ trong Hình 6. Một lần nữa, chúng tôi thấy những lợi ích đáng kể của CoLoR-Filter trên các quy mô. Một bảng đầy đủ kết quả theo tác vụ có trong Phụ lục B. Một lần nữa, chúng tôi nhận thấy rằng việc huấn luyện trên một tập con nghiêm ngặt của dữ liệu có thể vượt trội hơn một tập dữ liệu lớn hơn.

Chúng ta có thể thực hiện lại tính toán tiết kiệm tính toán cho τ = 16. Giờ đây, CoLoR-Filter mất khoảng 3 tỷ token để khớp hiệu suất của việc huấn luyện trên 25 tỷ token ngẫu nhiên. Điều này tương đương với tổng chi phí 3m + 2τn + 3nL = 3*3.1 + 2*16*3 + 3*3*5.5 = 154.8, vẫn là một sự giảm tính toán hơn 2.5 lần để đạt được cùng hiệu suất trung bình trên bộ tác vụ. Một biểu đồ đầy đủ hình dung chi phí bằng FLOP cho tất cả τ có trong Phụ lục D.

Chuyển giao tác vụ. Chúng ta cũng có thể kiểm tra chuyển giao tác vụ ngoài 8 tác vụ đã được sử dụng để lựa chọn dữ liệu trên một vài tác vụ khác kiểm tra lý luận thông thường [Wang et al., 2019, Socher et al., 2013, Talmor et al., 2018, Sap et al., 2019]. Kết quả được trình bày trong Bảng 2 so với một mô hình ngẫu nhiên được huấn luyện trên dữ liệu nhiều gấp 10 lần. Hiệu suất cho thấy rằng dữ liệu được chọn bởi CoLoR-Filter không bị overfit với các tác vụ đánh giá cụ thể, mà nắm bắt được một số khái niệm chung về dữ liệu tốt cho một phạm vi tác vụ.

Bảng 2: Chuyển giao tác vụ cho các mô hình 1.2b với τ = 64.
Phương pháp | copa | rte | cb | sst2 | commonsense qa | social iqa
Ngẫu nhiên (25b token) | 69.2 | 48.9 | 42.8 | 46.8 | 33.7 | 42.9
CoLoR-Filter (τ= 64, 2.5b token) | 65.8 | 52.6 | 46.0 | 55.8 | 32.6 | 42.7

Lưu ý, chúng tôi cũng tiến hành thêm một vài thí nghiệm và ablation trong phụ lục: Phụ lục E xem xét việc sử dụng CoLoR-Filter trong phân phối để hướng đến thiệt hại C4, Phụ lục F xem xét việc áp dụng CoLoR-Filter theo lô thay vì toàn cục, Phụ lục G xem xét việc tinh chỉnh trên Ddown sau tiền huấn luyện có mục tiêu, Phụ lục I kiểm tra một số ví dụ được chọn và bị loại trừ, và Phụ lục J so sánh với FineWeb-edu [Penedo et al., 2024].

7 Thảo luận
Mặc dù khá đơn giản để suy ra và triển khai, chúng tôi chỉ ra rằng CoLoR-Filter là một phương pháp hiệu quả cho việc lựa chọn dữ liệu trên C4, với hành vi mở rộng hứa hẹn lên đến các mô hình 1.2 tỷ. Trong các thí nghiệm của chúng tôi, CoLoR-Filter tiếp tục cải thiện khi chỉ sử dụng 1 trong 64 điểm dữ liệu được xem xét để lựa chọn và tổng quát hóa từ các mô hình phụ trợ nhỏ sang các mô hình đích lớn hơn. Điều này mở ra nhiều hướng nghiên cứu tiềm năng. Đầu tiên, trong khi chúng tôi đã xem xét tiền huấn luyện có mục tiêu, có thể CoLoR-Filter có thể được mở rộng cho tinh chỉnh, tiền huấn luyện liên tục, và tiền huấn luyện miền mở tổng quát hơn. Đặc biệt, đó là một câu hỏi mở thú vị liệu việc thiếu xem xét rõ ràng về tính đa dạng dữ liệu có cản trở CoLoR-Filter trong bất kỳ bối cảnh nào trong số này. Thứ hai, CoLoR-Filter có thể được áp dụng cho các miền thách thức hơn trong ngôn ngữ như tạo mã hoặc thậm chí được áp dụng ngoài miền ngôn ngữ sang các phương thức khác. Cuối cùng, có rất nhiều công việc cần làm để làm cho thuật toán hiệu quả hơn và để kiểm tra giới hạn của chuyển giao quy mô.

--- TRANG 10 ---
Lời cảm ơn
HZ được hỗ trợ bởi Eric and Susan Dunn Graduate Fellowship. SK thừa nhận sự hỗ trợ từ Văn phòng Nghiên cứu Hải quân dưới giải thưởng N00014-22-1-2377 và Quỹ Khoa học Quốc gia dưới giải thưởng #IIS 2229881. Công trình này đã được thực hiện một phần nhờ món quà từ Quỹ Chan Zuckerberg Initiative để thành lập Viện Kempner cho Nghiên cứu Trí tuệ Tự nhiên và Nhân tạo.

Tài liệu tham khảo
[Các tài liệu tham khảo được dịch giữ nguyên định dạng gốc do tính chất kỹ thuật]

Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.

Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws, 2024.

Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone fishing: Neural active learning with fisher embeddings. Advances in Neural Information Processing Systems, 34:8927–8939, 2021.

Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671, 2019.

Freddie Bickford Smith, Andreas Kirsch, Sebastian Farquhar, Yarin Gal, Adam Foster, and Tom Rainforth. Prediction-oriented Bayesian active learning. International Conference on Artificial Intelligence and Statistics, 2023.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Adam Casson. Transformer flops, 2023. URL https://adamcasson.com/posts/transformer-flops.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):1–45, 2024.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data.

Abhimanyu Das and David Kempe. Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection. Journal of Machine Learning Research, 19(3):1–34, 2018. URL http://jmlr.org/papers/v19/16-534.html.

Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. What's in my big data? In The Twelfth International Conference on Learning Representations, 2023.

--- TRANG 11 ---
Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels, 2024.

Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier J Henaff. Bad students make great teachers: Active learning accelerates large-scale visual understanding. arXiv preprint arXiv:2312.05328, 2023.

Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks, 2023.

François Fleuret. The little book of deep learning. A lovely concise introduction, page 297, 2023.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.

Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In international conference on machine learning, pages 1311–1320. Pmlr, 2017.

Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models, 2024.

Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.

Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification and preference learning. stat, 1050:24, 2011.

Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622, 2022.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431. Association for Computational Linguistics, April 2017.

A Kirsch. Advanced deep active learning and data subset selection: unifying principles with information-theory intuitions. PhD thesis, University of Oxford, 2023.

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424–8445, 2022.

--- TRANG 12 ---
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024.

Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not all tokens are what you need, 2024.

Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, 27(4):986–1005, 1956.

Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint arXiv:2305.13169, 2023.

Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hannaneh Hajishirzi, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: A benchmark for evaluating language model fit, 2023.

Meta. Llama 3, 2024.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

Sören Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Höltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pages 15630–15649. PMLR, 2022.

Robert C Moore and William Lewis. Intelligent selection of language model training data. In Proceedings of the ACL 2010 conference short papers, pages 220–224, 2010.

George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for maximizing submodular set functions—i. Mathematical Programming, 14:265–294, 1978. URL https://api.semanticscholar.org/CorpusID:206800425.

Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.

Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024.

Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.

Tom Rainforth, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. Modern bayesian experimental design. Statistical Science, 39(1):100–114, 2024.

--- TRANG 13 ---
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.

Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278–25294, 2022.

Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018.

Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170.

Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024.

Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523–19536, 2022.

Jihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan Richard Schwarz. Learning large-scale neural fields via context pruned meta-learning. Advances in Neural Information Processing Systems, 36, 2024.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.

Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. Advances in Neural Information Processing Systems, 36, 2024.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.

--- TRANG 14 ---
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.

Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Édouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003–4012, 2020.

Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie E Everett, Alexander A Alemi, Ben Adlam, John D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, 2024.

Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:34201–34227, 2023.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

--- TRANG 15 ---
A Đường cong học tập cho các mô hình 150m

4.0 4.5 5.0 Books Cross Entropy (↓) CoLoR-Filter | Chỉ có điều kiện
0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) 4.0 4.5 5.0 Books Cross Entropy (↓) RHO-down
0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) RHO-down + Prior
τ: 2, 4, 8, 16
Baseline: Ngẫu nhiên 1x, Final: Ngẫu nhiên 1x, Final: Ngẫu nhiên 8x

Hình 7: Quét qua τ khi hướng đến Books từ C4 cho các mô hình 150m.

42 46 50 Average Accuracy (↑) CoLoR-Filter | Chỉ có điều kiện
0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) 42 46 50 Average Accuracy (↑) RHO-down
0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) RHO-down + Prior
τ: 2, 4, 8, 16
Baseline: Ngẫu nhiên 1x, Final: Ngẫu nhiên 1x, Final: Ngẫu nhiên 8x

Hình 8: Quét qua τ và đo hiệu suất trung bình trên tất cả các tác vụ downstream cho các mô hình 150m.

B Bảng kết quả downstream

--- TRANG 16 ---
Bảng 3: Hiệu suất cho tất cả các tác vụ cho các mô hình 150m để lựa chọn dữ liệu với τ = 16.

Phương pháp | hellaswag | piqa | arc-c | arc-e | openbook qa | sciq | boolq | winogrande | Avg
Ngẫu nhiên 1x | 33.2 | 64.5 | 22.4 | 44.4 | 26.8 | 66.9 | 58.8 | 53.3 | 46.3
CoLoR-Filter | 38.6 | 68.7 | 25.3 | 51.8 | 32.0 | 72.8 | 54.3 | 49.4 | 49.1
Chỉ có điều kiện | 33.0 | 65.6 | 23.0 | 42.2 | 27.2 | 64.6 | 61.4 | 51.1 | 46.0
RHO-down | 35.5 | 67.3 | 25.3 | 46.9 | 29.2 | 67.5 | 48.6 | 48.7 | 46.1
RHO-down + prior | 35.6 | 66.6 | 25.3 | 49.3 | 29.4 | 69.0 | 61.6 | 50.9 | 48.5
DSIR | 37.6 | 68.8 | 24.4 | 46.6 | 27.8 | 68.4 | 59.9 | 52.6 | 48.3
Ngẫu nhiên 8x | 38.2 | 67.8 | 23.5 | 44.2 | 28.8 | 65.3 | 58.1 | 50.5 | 47.1

Bảng 4: Hiệu suất cuối cùng cho tất cả các tác vụ cho các mô hình 1.2b. Lưu ý rằng các mô hình CoLoR-Filter không huấn luyện trên nhiều token vì chúng tôi cạn kiệt tất cả token trong C4 với những thiết lập τ này.

Phương pháp | hellaswag | piqa | arc-c | arc-e | openbook qa | sciq | boolq | winogrande | Avg
Ngẫu nhiên (25b token) | 52.9 | 73.0 | 26.1 | 53.7 | 32.8 | 75.5 | 56.7 | 54.3 | 53.1
CoLoR-Filter (τ= 7, 25b token) | 62.3 | 75.6 | 29.7 | 60.3 | 38.0 | 79.7 | 48.3 | 58.0 | 56.5
CoLoR-Filter (τ= 16, 10b token) | 59.3 | 75.4 | 31.7 | 62.7 | 36.2 | 81.0 | 57.7 | 56.4 | 57.6
CoLoR-Filter (τ= 32, 5b token) | 54.8 | 74.3 | 29.4 | 60.9 | 35.4 | 78.4 | 59.1 | 54.1 | 55.8
CoLoR-Filter (τ= 64, 2.5b token) | 49.3 | 73.2 | 28.9 | 59.7 | 35.6 | 77.1 | 59.8 | 53.0 | 54.6

C Tính đa dạng dữ liệu và lựa chọn trực tuyến so với offline

Nhiều công việc về học tập chủ động tập trung vào đảm bảo rằng chúng ta lựa chọn một tập dữ liệu đa dạng bao phủ phân phối thử nghiệm quan tâm. Như đã giải thích trong văn bản chính, bằng cách tạo ra một ước tính điểm của các tham số, CoLoR-Filter đang đơn giản hóa vấn đề và hy sinh một số hạng rõ ràng cho tính đa dạng trong mục tiêu. Trong thực tế, điều này dường như được cứu rỗi bởi các sự thật rằng (1) C4 đã được khử trùng lặp, (2) chúng ta vẫn lựa chọn một tập con khá lớn mà không thay thế, và (3) một chuỗi cá nhân chứa tính đa dạng trên các token.

Tuy nhiên, việc CoLoR-Filter hy sinh một khái niệm về tính đa dạng trong mục tiêu là quan trọng để xem xét sâu hơn. Ở đây, chúng tôi suy ra một thuật toán dựa trên thiệt hại cho việc lựa chọn dữ liệu ưu tiên tính đa dạng sẽ trông như thế nào và tại sao nó không khả thi về mặt tính toán. Sau đó, chúng tôi suy ra một xấp xỉ (trông có phần giống RHOLoss [Mindermann et al., 2022]) và chỉ ra cách nó không ổn định về mặt thực nghiệm, như cũng đã được quan sát trước đó bởi [Mindermann et al., 2022].

Để suy ra một thuật toán giống CoLoR-Filter định giá tính đa dạng, chúng ta có thể bắt đầu từ Phương trình (3) bằng một xấp xỉ tham lam nơi các mẫu được thêm vào S từng (lô) một, như trong RHO:

≈ min x₁,...,xₙ⊂Dtrain Σᵢ₌₁ⁿ −log∫θ Pr(xᵢ|θ) Pr(θ|Ddown, x<i) + log∫θ Pr(xᵢ|θ) Pr(θ|x<i) (11)

Lưu ý rằng loại thuật toán tham lam này cho việc lựa chọn tập con có lịch sử lâu dài trong học tập chủ động [Das and Kempe, 2018], thực sự có căn cứ lý thuyết trong một số trường hợp [Nemhauser et al., 1978], và được sử dụng trong công trình trước đó [Ash et al., 2021, Mindermann et al., 2022]. Quan trọng là, thuật toán này vẫn ưu tiên việc lựa chọn một tập dữ liệu đa dạng. Bằng cách có điều kiện trên dữ liệu quá khứ ở bước i, mục tiêu khuyến khích thuật toán lựa chọn dữ liệu khác với dữ liệu đã được chọn.

Chúng ta cũng có thể tạo ra một phiên bản Bayes thực nghiệm bằng cách thêm Dprior:

min x₁,...,xₙ⊂Dtrain Σᵢ₌₁ⁿ −log∫θ Pr(xᵢ|θ) Pr(θ|Dprior, Ddown, x<i) (12)
                                + log∫θ Pr(xᵢ|θ) Pr(θ|Dprior, x<i) (13)

Điều này, tất nhiên, vẫn không thể giải được vì nó đòi hỏi tích phân các tham số. Nhưng, vì chúng ta đã giới thiệu thuật toán tham lam khuyến khích tính đa dạng, nếu bây giờ chúng ta tạo ra xấp xỉ ước tính điểm

--- TRANG 17 ---
≈ min x₁,...,xₙ⊂Dtrain Σᵢ₌₁ⁿ −log Pr(xᵢ|θprior+down+x<i) + log Pr(xᵢ|θprior+x<i) (14)

Vấn đề khó khăn ở đây là cách định nghĩa θprior+down+x<i và θprior+x<i trong thực tế. Về lý thuyết, những tham số này nên được huấn luyện trên một mẫu iid từ sự hợp của các tập dữ liệu. Nếu chúng ta thêm các điểm dữ liệu từng cái một, động lực của sự thay đổi phân phối theo thời gian có thể thay đổi mức độ mô hình tương ứng với việc có điều kiện trên sự hợp của tập dữ liệu. Nhưng, điều này sẽ đòi hỏi việc huấn luyện lại các mô hình mỗi khi chúng ta thêm một xᵢ mới, điều này rõ ràng là không thực tế.

Trong thực tế, điều này khuyến khích việc sử dụng một phương pháp tinh chỉnh (như trong RHO) nơi chúng ta liên tục tinh chỉnh trên các xᵢ khi chúng được thêm vào. Nhưng khi Ddown nhỏ và phân phối dữ liệu thay đổi theo thời gian, chúng ta có thể gặp phải quên thảm khốc và động lực huấn luyện không ổn định. Vì những lý do này, RHO tránh việc huấn luyện mô hình có điều kiện hoàn toàn (Phụ lục D của Mindermann et al. [2022]). Chúng tôi cũng tiến hành một thí nghiệm trên tác vụ Books nơi chúng tôi sử dụng thuật toán tinh chỉnh trực tuyến này cập nhật cả mô hình biên và có điều kiện khi chúng ta thêm dữ liệu vào S. Kết quả trong Hình 9 cho thấy cách huấn luyện không ổn định và thực tế hoạt động tệ hơn ngẫu nhiên.

Hơn nữa, lưu ý rằng chi phí tính toán của thậm chí thuật toán tinh chỉnh rẻ nhất là đáng kể so với các thuật toán trong bài báo. Đặc biệt, chi phí tuần tự giờ đây là 2τn + 4n (so với τn + 2n cho RHO) vì chúng ta cần chuyển τn mẫu đầy đủ qua cả mô hình có điều kiện và biên. Vậy biến thể này rõ ràng là kém hơn trong thực tế so với các phương pháp khác chúng tôi xem xét.

5000 10000 15000 20000
Bước 4.6 4.8 5.0 5.2 Books Cross Entropy (↓) Chuyển giao từ C4 sang Books (τ= 2)
Lựa chọn Trực tuyến | Ngẫu nhiên | Final: Ngẫu nhiên 1x | Final: Ngẫu nhiên 8x

0 5000 10000 15000 20000 25000
Bước 3.2 3.3 3.4 3.5 3.6 Training Cross Entropy (↓) Thiệt hại huấn luyện
Thiệt hại Có điều kiện | Thiệt hại Biên

Hình 9: (Trái) Hiệu suất của lựa chọn trực tuyến với tinh chỉnh như được nêu trong Phương trình (14). Lựa chọn trực tuyến tệ hơn ngẫu nhiên. (Phải) Đường cong huấn luyện cho các mô hình có điều kiện và biên trên dữ liệu được chọn S. Mô hình có điều kiện đối mặt với bất ổn định huấn luyện sớm (liên quan đến việc quên), và sau đó cuối cùng trở nên tốt hơn so với biên trên dữ liệu được chọn.

D Chi phí tính toán cho chuyển giao quy mô

7 16 32 64
Tau 0.0 0.5 1.0 1.5 2.0 2.5 FLOP để đạt hiệu suất ngẫu nhiên ×10²⁰ Books
Ngẫu nhiên (25b token) | Chi phí Huấn luyện (Tuần tự) | Chi phí Chấm điểm (Song song)

7 16 32 64
Tau 0.0 0.5 1.0 1.5 2.0 2.5 FLOP để đạt hiệu suất ngẫu nhiên ×10²⁰ Downstream

Hình 10: Chi phí bằng FLOP để đạt hiệu suất tương đương với mô hình ngẫu nhiên cuối cùng được huấn luyện trên 25b token (tức là chi phí cho đến khi chúng ta đạt đường đứt nét trong Hình 1). Chúng tôi chia chi phí thành chi phí chấm điểm để lọc dữ liệu sử dụng các mô hình phụ trợ nhỏ và sau đó chi phí huấn luyện cho mô hình lớn.

--- TRANG 18 ---
Trong văn bản chính, chúng tôi đã tính toán chi phí cho τ = 16 theo đơn vị forward mô hình của 1 tỷ token. Ở đây chúng ta có thể chuyển đổi điều này sang FLOP và tính toán chi phí cho tất cả các giá trị τ. Kết quả trong Hình 10 cho thấy sự phân tách chi phí thành FLOP chấm điểm để chạy các mô hình phụ trợ nhỏ qua dữ liệu và FLOP huấn luyện để huấn luyện mô hình lớn. Chúng tôi đo chi phí cần thiết để đạt hiệu suất cuối cùng của mô hình ngẫu nhiên, tức là cho đến khi đường cong học tập CoLoR-filter vượt qua đường đứt nét trong Hình 1. Sự đánh đổi chính là các giá trị τ thấp hơn yêu cầu chi phí chấm điểm nhiều hơn và chi phí huấn luyện ít hơn vì chúng có thể lựa chọn dữ liệu tốt hơn.

Chúng ta cũng nên lưu ý rằng nếu nhiều mô hình đang được huấn luyện với cùng một tập dữ liệu, thì chi phí chấm điểm này có thể được phân bổ qua các lần chạy đó và các giá trị τ lớn hơn sẽ trông thậm chí còn tốt hơn.

E Chúng ta có thể thực hiện lựa chọn dữ liệu trong phân phối không?

0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) 3.2 3.4 3.6 3.8 C4 Cross Entropy (↓) τ= 2

0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) τ= 4

CoLoR-Filter | Chỉ có điều kiện | RHO | Ngẫu nhiên 1x | Final: Ngẫu nhiên 1x | Final: Ngẫu nhiên 8x

Hình 11: Sử dụng một mẫu của C4 làm Ddown. RHO cung cấp lợi ích biên ở đây, trong khi CoLoR-Filter không cung cấp lợi ích gì cả. Chỉ có điều kiện tệ hơn ngẫu nhiên. Việc mở rộng τ không thay đổi kết quả nhiều như khi chúng ta hướng đến các tác vụ downstream.

Một câu hỏi rõ ràng được đặt ra bởi các kỹ thuật lựa chọn dữ liệu này là liệu chúng có thể hoạt động trong phân phối hay không, tức là chúng ta có thể lựa chọn dữ liệu để làm cho thiệt hại iid trên C4 giảm nhanh hơn không? Trong Hình 11, chúng tôi trình bày kết quả để chạy thí nghiệm này với CoLoR-Filter cũng như RHO và Chỉ có điều kiện. Lưu ý rằng không có sự khác biệt giữa RHO và RHO + prior bây giờ (và chúng tôi loại bỏ "down" khỏi tên) vì phân phối tiên nghiệm và phân phối downstream là giống nhau. Để triển khai CoLoR-Filter trong bối cảnh này, chúng tôi chỉ lấy hai checkpoint từ việc tiền huấn luyện mô hình tiên nghiệm và gọi checkpoint sớm hơn (tại 2.5b token) là mô hình biên và checkpoint sau hơn (tại 3.1b token) là mô hình có điều kiện.

Chúng tôi thấy rằng lựa chọn trong phân phối không hoạt động hiệu quả với những phương pháp này. Có những lợi ích nhỏ đối với RHO loss, nhưng ở đây chúng bị áp đảo một cách lớn lao bởi chi phí tính toán của việc lựa chọn. CoLoR-Filter không thấy lợi ích gì cả so với ngẫu nhiên và Chỉ có điều kiện tệ hơn ngẫu nhiên. Những kết quả sơ bộ này cho thấy tại sao quan trọng là nhận ra rằng việc lựa chọn dữ liệu (đặc biệt là với những phương pháp này) sẽ hiệu quả nhất khi chúng ta thực sự muốn hướng đến một phân phối khác với Dtrain.

F Lựa chọn toàn cục so với theo lô

0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) 4.0 4.5 5.0 Books Cross Entropy (↓) CoLoR-Filter Toàn cục

0.5 1.0 1.5 2.0 2.5 3.0
Token (tỷ) CoLoR-Filter Theo lô

τ: 2, 4, 8, 16
Baseline: Ngẫu nhiên 1x | Final: Ngẫu nhiên 1x | Final: Ngẫu nhiên 8x

Hình 12: So sánh giữa các biến thể toàn cục và theo lô của CoLoR-Filter trên Books. Cả hai hoạt động gần như giống hệt nhau ở đây.

--- TRANG 19 ---
Một khía cạnh triển khai nhỏ khác về CoLoR-Filter là như được trình bày trong Thuật toán 1, chúng tôi thực hiện lựa chọn toàn cục nơi chúng ta lấy n điểm dữ liệu tốt nhất trên toàn bộ tập huấn luyện, trong khi trong RHO-down trong Thuật toán 2 lựa chọn được thực hiện theo lô. Ở đây chúng tôi thực hiện ablation liệu khả năng thực hiện lựa chọn toàn cục có thực sự hữu ích cho CoLoR-Filter hay không. Kết quả trong Hình 12 cho thấy rằng không có nhiều khác biệt giữa hai cách và ở τ nhỏ, lựa chọn theo lô có lẽ thậm chí còn đánh bại lựa chọn toàn cục. Chúng tôi cung cấp kết quả này để minh họa rằng CoLoR-Filter khá mạnh mẽ đối với cách thức lựa chọn được thực hiện.

G Tinh chỉnh sau tiền huấn luyện có mục tiêu

Một câu hỏi có thể về bối cảnh tiền huấn luyện có mục tiêu mà chúng tôi xem xét là: điều gì xảy ra nếu chúng ta tinh chỉnh trên Ddown sau tiền huấn luyện có mục tiêu?

Điều này thú vị vì trong khi các mô hình tiền huấn luyện được trình bày trong văn bản chính không bao giờ có quyền truy cập trực tiếp vào Ddown, thuật toán lựa chọn thì có. Trong phần này, chúng tôi cũng cho phép quyền truy cập vào Ddown sau tiền huấn luyện và sau đó so sánh hiệu suất cuối cùng của các mô hình được tinh chỉnh được tiền huấn luyện trên dữ liệu ngẫu nhiên so với dữ liệu được chọn.

Đầu tiên, trong Bảng 5 và Bảng 6, chúng tôi trình bày kết quả tinh chỉnh cho các mô hình 150m. Chúng tôi thấy rằng dữ liệu CoLoR-Filter vượt trội hơn dữ liệu ngẫu nhiên nhiều gấp 8 lần sau tinh chỉnh. Lưu ý rằng mô hình có điều kiện mà chúng tôi sử dụng để hướng dẫn việc lựa chọn của CoLoR-Filter tương đương với một mô hình đã được tiền huấn luyện trên 3B token ngẫu nhiên và sau đó được tinh chỉnh trên tác vụ. Do đó, những kết quả này cho thấy rằng chúng ta đang vượt trội đáng kể so với mô hình có điều kiện khi cả hai mô hình đều được tinh chỉnh trên dữ liệu downstream.

Bảng 5: Hiệu suất sau tinh chỉnh trên Books cho các mô hình 150m tiền huấn luyện khác nhau. Lưu ý rằng mô hình Ngẫu nhiên (3.1b token) tương đương với mô hình có điều kiện được sử dụng để lựa chọn dữ liệu với CoLoR-Filter (τ= 16).

Dữ liệu tiền huấn luyện | Books Val Cross Entropy sau Tinh chỉnh
Ngẫu nhiên (3.1b token) | 3.441
Ngẫu nhiên (25b token) | 3.357
CoLoR-Filter (3.1b token) | 3.258

Bảng 6: Hiệu suất giữ lại sau tinh chỉnh trên dữ liệu downstream cho các mô hình 150m tiền huấn luyện khác nhau. Lưu ý rằng mô hình Ngẫu nhiên (3.1b token) tương đương với mô hình có điều kiện được sử dụng để lựa chọn dữ liệu với CoLoR-Filter (τ= 16).

Dữ liệu tiền huấn luyện | hellaswag | piqa | arc-c | arc-e | openbook qa | sciq | boolq | winogrande | Avg
Ngẫu nhiên (3.1b token) | 34.4 | 66.6 | 24.8 | 51.7 | 28.0 | 89.9 | 65.6 | 53.1 | 51.8
Ngẫu nhiên (25b token) | 39.5 | 69.8 | 29.2 | 53.9 | 30.2 | 91.4 | 64.2 | 52.9 | 53.9
CoLoR-Filter (3.1b token) | 39.2 | 71.1 | 29.1 | 55.3 | 33.2 | 90.0 | 65.1 | 51.6 | 54.3

Tiếp theo, chúng tôi trình bày kết quả cho các mô hình 1.2b trong Bảng 7 và Bảng 8. Chúng tôi thấy rằng mô hình CoLoR-Filter vượt trội hoặc cạnh tranh với việc huấn luyện trên dữ liệu được chọn ngẫu nhiên nhiều gấp khoảng 10 lần. Chúng ta cũng nên lưu ý rằng các mô hình CoLoR-Filter giờ đây đang vượt trội một cách đáng kể so với các mô hình có điều kiện 150m được sử dụng để lọc dữ liệu, cho thấy chuyển giao quy mô tích cực của việc lựa chọn dữ liệu.

Bảng 7: Hiệu suất sau tinh chỉnh trên Books cho các mô hình 1.2b tiền huấn luyện khác nhau. Lưu ý rằng mô hình có điều kiện lựa chọn dữ liệu chỉ có 150m tham số.

Dữ liệu tiền huấn luyện | Books Val Cross Entropy sau Tinh chỉnh
Ngẫu nhiên (25b token) | 3.074
CoLoR-Filter (2.6b token) | 2.964

--- TRANG 20 ---
Bảng 8: Hiệu suất giữ lại sau tinh chỉnh trên dữ liệu downstream cho các mô hình 1.2b tiền huấn luyện khác nhau.

Dữ liệu tiền huấn luyện | hellaswag | piqa | arc-c | arc-e | openbook qa | sciq | boolq | winogrande | Avg
Ngẫu nhiên (25b token) | 55.3 | 74.6 | 35.2 | 63.0 | 35.8 | 94.6 | 72.0 | 62.5 | 61.6
CoLoR-Filter (2.6b token) | 53.4 | 76.1 | 35.8 | 65.6 | 36.8 | 93.2 | 66.6 | 58.9 | 60.8

H Siêu tham số

Bảng 9: Tham số mô hình 150m, dựa trên Wortsman et al. [2024], Groeneveld et al. [2024]

Tham số | Giá trị
Chiều dư | 1024
Độ sâu | 12
Chiều ẩn MLP | 4096
Hàm kích hoạt | GeLU
Chiều đầu | 64
Độ dài ngữ cảnh | 512
Mã hóa vị trí | RoPE
Bias | False
Chuẩn hóa | PyTorch Layernorm
Chuẩn hóa QK | True
Độ chính xác | Hỗn hợp, bfloat16
Tokenizer | GPTNeox

Bảng 10: Mô hình 1.2b, dựa trên Wortsman et al. [2024], Groeneveld et al. [2024]. Chỉ báo cáo sự khác biệt so với 150m.

Tham số | Giá trị
Chiều dư | 2048
Độ sâu | 24
Chiều ẩn MLP | 8192

--- TRANG 21 ---
Bảng 11: Tham số huấn luyện, dựa trên Wortsman et al. [2024], Groeneveld et al. [2024]

Tham số | Giá trị
Trình tối ưu | Adam
Kích thước lô | 256
Tốc độ học | 1e-3
Lịch trình | Làm ấm tuyến tính, suy giảm cosine
Bước làm ấm | 5% tổng bước
Hệ số z-loss | 1e-4
Suy giảm trọng số | 0.0
β1 | 0.9
β2 | 0.95
ε | 1e-15

I Kiểm tra dữ liệu được chọn

Trong phần này, chúng tôi tiến hành một số phân tích cơ bản về dữ liệu được chọn bởi CoLoR-Filter. Chúng tôi để lại phân tích đầy đủ cho công việc tương lai, nhưng ở đây chúng tôi cung cấp một số thống kê cấp cao về các phân phối điểm số của các mô hình có điều kiện so với biên và một số ví dụ đại diện từ các tập dữ liệu.

I.1 Phân phối điểm số

Đầu tiên, chúng tôi đơn giản vẽ CDF của hàm điểm số giảm thiệt hại có điều kiện (CoLoR) được sử dụng để lựa chọn dữ liệu. Chúng tôi thấy rằng có tương đối ít ngoại lệ và các điểm CoLoR khá tập trung và phân phối bình thường. Hơn nữa, chúng tôi lưu ý rằng CoLoR trung bình trong cả hai thí nghiệm là dương, có nghĩa là mô hình có điều kiện thực sự có thiệt hại cao hơn trên các điểm dữ liệu trong C4 so với mô hình biên. Điều này có ý nghĩa vì mô hình có điều kiện đã được tinh chỉnh trên Ddown nằm ngoài phân phối tương đối với C4.

−6 −4 −2 0 2 4 6 8
Giảm Thiệt hại Có điều kiện 0.0 0.2 0.4 0.6 0.8 1.0 CDF
τ= 64 Hướng đến Books

0 2 4 6 8 10
Giảm Thiệt hại Có điều kiện 0.0 0.2 0.4 0.6 0.8 1.0 CDF
τ= 64 Hướng đến Downstream

Hình 13: CDF cho giảm thiệt hại có điều kiện (CoLoR), tức là −log Pr(x|θprior+down) − (−log Pr(x|θprior)). Đường đứt nét làm nổi bật điểm cắt cho τ = 64. Chúng tôi chọn các điểm có CoLoR thấp nhất.

I.2 Ví dụ đại diện

Bây giờ chúng tôi chỉ liệt kê một vài ví dụ đại diện để cho cảm giác về các loại ngoại lệ tồn tại dưới việc xếp hạng chuỗi của chúng tôi và các loại chuỗi điển hình được chọn so với bị loại trừ. Các chuỗi được lấy mẫu ngẫu nhiên từ các quantile khác nhau của phân phối và chúng tôi rút ngắn tất cả các chuỗi để chúng phù hợp dễ dàng hơn trên trang.

Hình 14 cho thấy các ngoại lệ khi hướng đến Books và Hình 15 cho thấy các ví dụ điển hình hơn khi hướng đến Books. Nói chung, chúng tôi thấy rằng các tài liệu có điểm số rất cao chứa những thứ như tiếng Anh cổ, thơ, và mục lục đặc biệt bất thường trong sách so với phần còn lại của internet. Những thứ khác như tiểu thuyết và hội thoại cũng được chấm điểm cao. Các ngoại lệ âm thường có những thứ như văn bản được mã hóa kém hoặc quảng cáo.

--- TRANG 22 ---
Hình 16 cho thấy các ngoại lệ khi hướng đến các tác vụ downstream và Hình 17 cho thấy các ví dụ điển hình hơn khi hướng đến các tác vụ downstream. Ở đây các mẫu ít rõ ràng hơn vì các tác vụ mục tiêu đa dạng hơn, nhưng chúng tôi quan sát thấy nhiều tài liệu khoa học và kiểu wiki có điểm số cao cũng như một số mô tả về tương tác vật lý có thể hữu ích cho các tác vụ thông thường. Một lần nữa, các ngoại lệ âm có xu hướng có những thứ như văn bản được mã hóa kém hoặc quảng cáo.

VÀ bây giờ sẽ ye wyt, thời gian nào trong ngày ye sẽ câu. Từ đầu tháng Maye vntill nó là tháng Chín: thời gian byting là sớm trong morow từ bốn giờ vntill eyght giờ, lúc sau none từ bốn đến eyght cũng vậy, nhưng không tốt bằng trong mornyng, và nếu nó là một colde wynde và một ngày lowryng, nó là muche tốt hơn nhiều so với một ngày cleere. Cũng có nhiều poole fysshes sẽ byte tốt nhất vào buổi sáng tyde. Và nếu ye se trong bất kỳ tyme nào trong ngày Troute hoặc greylyng lepe angle với anh ta với một dub tương ứng với cùng một moneth. Và nơi nước ebbeth và floweth: cá wyll byte ở một số nơi tại ebbe và ở một số nơi tại flud sau khi chúng haue restyng

(a) Ngoại lệ tốt, CoLoR = -0.35

???????????????????????????????
????????????????????????????????
?????????????????????????????????
??????????????????????????????????
??????????????????????????????????
????????????????????????????????
???????????????????????????????????
????????????????????????????????????
?????????????????????????????????????
?????????????????????????????????????
?????????????????????????????????????
?????????????????????????????????????
????????????
????????????????????????? m88
???????????????????????? ???? m88
??????????????????????????????????????
??????????????????????????? ????
??????????????????????????????????
??????????????????????????????? ?
???????????????????

(b) Ngoại lệ xấu, CoLoR = 5.45

Hình 14: Ví dụ về các ngoại lệ khi hướng đến Books. Các ví dụ được lấy mẫu ngẫu nhiên từ 1000 chuỗi hàng đầu hoặc hàng dưới. Ngoại lệ tích cực được viết bằng một phương ngữ tiếng Anh cũ có thể liên quan đến một số tài liệu trong kho ngữ liệu Project Gutenberg, trong khi ngoại lệ âm dường như được mã hóa kém.

--- TRANG 23 ---
C: Bà Mackenzie, có bao giờ có lúc nào bà cảm thấy như bà có thể nhảy lên máy bay và thực hiện chuyến bay đó xuống Tiểu bang bên cạnh để ở với các con trai của bà không? B: Ồ thưa cô, có. Đôi khi tôi cảm thấy như tôi hai mươi tuổi và rất khỏe mạnh và năng động và tôi có thể làm bất cứ điều gì tôi muốn làm và sau đó tôi nhớ, trời ơi, tôi 86 tuổi, bà già ngốc, bà không thể làm điều đó. Tôi ước tôi có thể bay xuống đó và sống với tất cả chúng cùng nhau giống như khi chúng còn nhỏ và tôi là Mẹ của chúng và chúng theo tôi vì tôi rất sáng sủa và vui vẻ và thông minh và năng động và tất cả những thứ mà tôi không còn là bây giờ. Ồ, tôi xin lỗi, nghe tôi nói. Có lẽ tôi chỉ đang mất trí, cô nghĩ sao, thưa cô? C: Mỉm cười – Hãy tưởng tượng nếu tôi vẫy một cây đũa thần và một cách kỳ diệu bà lại hai mươi tuổi. Bà sẽ thấy mình làm gì Beryl. Tôi gọi bà là Beryl được không?

(a) Chuỗi từ 3% tốt nhất, CoLoR = 0.40

Chamber of Commerce và các địa điểm kinh doanh khác, như Gwinnett Civic & Convention Centers và là một môi trường làm việc lý tưởng cho các doanh nghiệp thương mại và tập đoàn ở Northeast Atlanta. Vị trí nổi bật nằm trên một khu đất 6.5 acre được trồng nhiều cây cối phía trước I-85. Mặt ngoài có kính nhiệt màu xanh lá cây và lối vào có một bức tường kính curtain dẫn vào sảnh lát đá granite với trần vòm. Quận Gwinnett là nhà của các công ty Fortune 500 hàng đầu, được thu hút bởi danh tiếng của nó như một trung tâm thương mại và công nghệ, cung cấp cho các doanh nghiệp một thị trường khu vực với năm triệu người. SERVPRO of Gurnee có thể đơn giản hóa quá trình khôi phục bằng cách xử lý cả việc giảm thiểu thiệt hại nước ban đầu và xây dựng lại các khu vực bị ảnh hưởng. Có một công ty đủ năng lực cho toàn bộ quá trình có thể tiết kiệm thời gian và giữ chi phí thấp.

(b) Chuỗi từ 3% trung vị, CoLoR = 0.73

Hình 15: Ví dụ về các tài liệu điển hình hơn khi hướng đến Books. Đầu tiên là một tài liệu từ 3% hàng đầu sẽ được chọn với τ = 32, và sau đó là một tài liệu có điểm số gần trung vị của tất cả tài liệu. Tài liệu được chọn là hội thoại hư cấu trong khi tài liệu trung vị là quảng cáo.

--- TRANG 24 ---
trong số các pinacoderm là ostia cho phép nước đi vào cơ thể bọt biển. Những lỗ chân lông này đã đặt tên cho phylum bọt biển Porifera—người mang lỗ chân lông. Trong một số bọt biển, ostia được hình thành bởi porocytes, các tế bào hình ống đơn lẻ hoạt động như van để điều chỉnh dòng chảy của nước vào spongocoel. Trong các bọt biển khác, ostia được hình thành bởi các nếp gấp trong thành cơ thể của bọt biển. Giữa lớp ngoài và các buồng cho ăn của bọt biển là một chất giống gel gọi là mesohyl, chứa các sợi collagen. Các loại tế bào khác nhau cư trú trong mesohyl, bao gồm amoebocytes, "tế bào gốc" của bọt biển, và sclerocytes, sản xuất vật liệu khung xương. Tính nhất quán giống gel của mesohyl hoạt động như một khung xương nội và duy trì hình thái hình ống của bọt biển. Các buồng cho ăn bên trong bọt biển được lót bởi choanocytes ("tế bào cổ áo").

(a) Ngoại lệ tốt, CoLoR = -0.46

*** **********. ****** *** ***, ***
******* **** **** ** ******** *******
plates ** ****** ** ** **-** *** (***
******* ** tested), ***** *******
********. *** ** *** ***, ***
********* *.* ********* ******* *****
capture ****** ******** ********
****** ** **** ** **** ******, >10
***, *** ******, **+ ***, **** **
****** ***** or ****, *** ** *****
****** *** *** **** **** field **
****, ***** **'. ***** *******
********, *** ******** ** ***** ******
****** ****** to ******* ****** **
****** **** ** **** ****** ** night,
****** ******* ******* *** ********.
******** ******** ** */****, ***
******** ****** ******** ******** ****
front *** **** ****** ****** ** ***
*** **** ******. However, ****
******* ******* *** ********** ** ***
***** ** night, ****** ** **** ******
*** ******* ************ ** *** scene.

(b) Ngoại lệ xấu, CoLoR = 5.36

Hình 16: Ví dụ về các ngoại lệ khi hướng đến các tác vụ downstream. Các ví dụ được lấy mẫu ngẫu nhiên từ 1000 chuỗi hàng đầu hoặc hàng dưới. Ngoại lệ tích cực là một tài liệu khoa học có thể liên quan đến các tác vụ như SciQ, trong khi ngoại lệ âm dường như được mã hóa kém.

kế hoạch mùa hè. Sau khi suy nghĩ một lúc, tôi quyết định dành mùa hè của mình ở Squamish, nơi tôi sẽ làm việc cho Đội Tuyển sinh. Tuy nhiên, do số lượng sinh viên rất lớn quan tâm đến việc làm việc trong khuôn viên và số lượng vị trí công việc hạn chế, cuối cùng tôi đã không có được việc làm trong khuôn viên. Tôi thực sự rất buồn và tôi bắt đầu nghĩ rằng không có cơ hội việc làm nào khác, điều này sẽ dẫn đến việc tôi phải về nhà. Đáng ngạc nhiên, có nhiều cơ hội việc làm trong cộng đồng Squamish. Vì Đại học Quest Canada tổ chức một hội chợ việc làm trong khuôn viên, tôi, cùng với tất cả các sinh viên, đã có cơ hội gặp gỡ các doanh nghiệp địa phương đang tìm kiếm nhân viên mùa hè. Đó là một cơ hội tuyệt vời để kết nối mạng và đưa sơ yếu lý lịch của tôi cho những người mà tôi quan tâm.

(a) Chuỗi từ 3% tốt nhất, CoLoR = 0.33

Tôi có thể cài đặt PDF Stacks trên nhiều hơn một máy tính không? Khóa license chỉ có hiệu lực cho một thiết bị và không thể chuyển nhượng. Bạn có thể có thêm (các) khóa license bằng cách đặt hàng. Tôi sử dụng PDF Stacks như thế nào? Nhấp "File" và sau đó "Import Folder" Khi bạn nhập các tệp PDF, các tệp của bạn sẽ được sao chép vào PDF Stacks để dễ dàng đọc, tìm kiếm, sắp xếp, ghi chú, in và chia sẻ hơn. Có câu hỏi nào, hỏi chúng tôi! Tôi tạo bộ sưu tập (thư mục ảo) và khớp/gắn thẻ tài liệu của mình như thế nào để tổ chức tốt hơn? Dễ lắm. Xem video để tạo bộ sưu tập và gắn thẻ tài liệu. Nhiều người dùng có thể truy cập cùng một tài liệu hoặc tôi có thể truy cập và đồng bộ tài liệu của mình qua nhiều thiết bị không?

(b) Chuỗi từ 3% trung vị, CoLoR = 0.55

Hình 17: Ví dụ về các tài liệu điển hình hơn khi hướng đến các tác vụ downstream. Đầu tiên là một tài liệu từ 3% hàng đầu sẽ được chọn với τ = 32, và sau đó là một tài liệu có điểm số gần trung vị của tất cả tài liệu. Tài liệu được chọn dường như là một mục nhật ký trong khi tài liệu trung vị là tài liệu phần mềm.

--- TRANG 25 ---
J So sánh với Fineweb-Edu

Đồng thời với công việc ban đầu của chúng tôi, Penedo et al. [2024] đã phát hành FineWeb-edu, một bộ phân loại cho nội dung giáo dục có thể lọc tập dữ liệu FineWeb. Ở đây chúng tôi cung cấp một so sánh giữa CoLoR-Filter và phương pháp dựa trên bộ phân loại này.

Cụ thể, chúng tôi triển khai lại pipeline CoLoR-Filter trên tập dữ liệu Fineweb và với các mô hình phụ trợ nhỏ hơn một chút (125m) để so sánh công bằng hơn với FineWeb-edu. Sau đó, chúng tôi so sánh trên cùng một bộ 8 tác vụ downstream qua các thiết lập τ khác nhau sử dụng hai điểm số: CoLoR-Filter hoặc bộ phân loại FineWeb-edu. Sau đó, chúng tôi huấn luyện các mô hình lớn hơn (680M tham số) cho 10B token dữ liệu được chọn. Kết quả được hiển thị trong hình 18. Chúng tôi thấy rằng CoLoR-Filter nhất quán vượt trội hơn FineWeb-edu, điều này không quá ngạc nhiên vì chúng tôi đang thực hiện lựa chọn dữ liệu có mục tiêu hơn bằng cách đặc biệt hướng đến các tác vụ NLP downstream thay vì một khái niệm chung về "nội dung giáo dục".

Hình 18: So sánh hiệu suất của các mô hình 680m được huấn luyện trên 10B token được chọn với các τ khác nhau giữa CoLoR-Filter và FineWeb-edu.

K Tác động Rộng hơn

Việc phát triển CoLoR-Filter cho lựa chọn dữ liệu có những tác động rộng hơn đáng chú ý đối với cả machine learning và xã hội. Nó tăng cường hiệu quả trong huấn luyện mô hình ngôn ngữ, dẫn đến giảm tài nguyên tính toán và dấu chân môi trường, trong khi khả năng mở rộng của nó dân chủ hóa quyền truy cập vào các mô hình hiệu suất cao. Thành công của phương pháp trong các tác vụ downstream đa dạng hứa hẹn những tiến bộ trong các lĩnh vực như xử lý văn bản y tế và phân tích pháp lý. Tuy nhiên, nó cũng đặt ra những lo ngại về thiên lệch tập dữ liệu, đòi hỏi đánh giá và cập nhật liên tục. Nghiên cứu tương lai nên tập trung vào đảm bảo các mô hình không kế thừa thiên lệch từ dữ liệu huấn luyện được chọn, mở rộng ứng dụng, cải thiện hiệu quả, và triển khai các biện pháp bảo vệ để tối đa hóa lợi ích xã hội trong khi giảm thiểu rủi ro.

L Tài nguyên tính toán

Tất cả huấn luyện được thực hiện trên một cụm nội bộ sử dụng GPU H100. Trên một GPU, mỗi lần chạy huấn luyện 150m cho 3.1b token mất khoảng 4 giờ, việc chạy các mô hình phụ trợ offline và song song có thể nhanh hơn. Huấn luyện mô hình 1.2b đến hoàn thành mất khoảng 2 ngày trên 4 GPU.
