# 2406.11794.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/dataset-pruning-cleaning-dedup/2406.11794.pdf
# Kích thước tệp: 1656647 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DataComp-LM: Tìm kiếm thế hệ tiếp theo của
bộ dữ liệu huấn luyện cho các mô hình ngôn ngữ
Jeffrey Li*1,2Alex Fang*1,2Georgios Smyrnis*4Maor Ivgi*5
Matt Jordan4Samir Gadre3,6Hritik Bansal8Etash Guha1,15Sedrick Keh3Kushal Arora3
Saurabh Garg13Rui Xin1Niklas Muennighoff22Reinhard Heckel12Jean Mercat3
Mayee Chen7Suchin Gururangan1Mitchell Wortsman1Alon Albalak19,20Yonatan Bitton14
Marianna Nezhurina9,10Amro Abbas23Cheng-Yu Hsieh1Dhruba Ghosh1Josh Gardner1
Maciej Kilian17Hanlin Zhang18Rulin Shao1Sarah Pratt1Sunny Sanyal4Gabriel Ilharco1
Giannis Daras4Kalyani Marathe1Aaron Gokaslan16Jieyu Zhang1Khyathi Chandu11
Thao Nguyen1Igor Vasiljevic3Sham Kakade18Shuran Song6,7Sujay Sanghavi4
Fartash Faghri2Sewoong Oh1Luke Zettlemoyer1Kyle Lo11Alaaeldin El-Nouby2
Hadi Pouransari2Alexander Toshev2Stephanie Wang1Dirk Groeneveld11Luca Soldaini11
Pang Wei Koh1Jenia Jitsev9,10Thomas Kollar3Alexandros G. Dimakis4,21
Yair Carmon5Achal Dave†3Ludwig Schmidt†1,7Vaishaal Shankar†2
1University of Washington,2Apple,3Toyota Research Institute,4UT Austin,
5Tel Aviv University,6Columbia University,7Stanford,8UCLA,9JSC,10LAION,11AI2,
12TUM,13CMU,14Hebrew University,15SambaNova,16Cornell,17USC,18Harvard,
19UCSB,20SynthLabs,21Bespokelabs.AI,22Contextual AI,23DatologyAI
contact@datacomp.ai

Tóm tắt
Chúng tôi giới thiệu DataComp cho Mô hình Ngôn ngữ (DCLM), một bài kiểm tra cho các thí nghiệm tập dữ liệu có kiểm soát với mục tiêu cải thiện các mô hình ngôn ngữ. Như một phần của DCLM, chúng tôi cung cấp một kho dữ liệu chuẩn hóa gồm 240T token được trích xuất từ Common Crawl, các công thức tiền huấn luyện hiệu quả dựa trên framework OpenLM, và một bộ đánh giá toàn diện gồm 53 nhiệm vụ downstream. Những người tham gia trong benchmark DCLM có thể thí nghiệm với các chiến lược tuyển chọn dữ liệu như khử trùng lặp, lọc và trộn dữ liệu ở các quy mô mô hình từ 412M đến 7B tham số. Như một đường cơ sở cho DCLM, chúng tôi tiến hành các thí nghiệm mở rộng và phát hiện ra rằng việc lọc dựa trên mô hình là chìa khóa để tập hợp một bộ dữ liệu huấn luyện chất lượng cao. Tập dữ liệu kết quả, DCLM-BASELINE, cho phép huấn luyện một mô hình ngôn ngữ 7B tham số từ đầu đạt độ chính xác 64% 5-shot trên MMLU với 2.6T token huấn luyện. So với MAP-Neo, state-of-the-art trước đó trong các mô hình ngôn ngữ dữ liệu mở, DCLM-BASELINE thể hiện cải thiện 6.6 điểm phần trăm trên MMLU trong khi được huấn luyện với ít hơn 40% tính toán. Mô hình cơ sở của chúng tôi cũng tương đương với Mistral-7B-v0.3 và Llama 3 8B trên MMLU (63% & 66%), và hoạt động tương tự trên trung bình 53 nhiệm vụ hiểu ngôn ngữ tự nhiên trong khi được huấn luyện với ít hơn 6.6× tính toán so với Llama 3 8B. Kết quả của chúng tôi làm nổi bật tầm quan trọng của thiết kế tập dữ liệu để huấn luyện các mô hình ngôn ngữ và cung cấp điểm khởi đầu cho nghiên cứu tiếp theo về tuyển chọn dữ liệu. Chúng tôi phát hành benchmark DCLM, framework, mô hình và tập dữ liệu tại https://datacomp.ai/dclm.

∗Tác giả đầu chia sẻ;†Tác giả cuối chia sẻ. Các liên kết được liệt kê như trong bài đăng arXiv đầu tiên của bài báo này. Một số tác giả đã thay đổi tổ chức kể từ đó.arXiv:2406.11794v4 [cs.LG] 21 Apr 2025

--- TRANG 2 ---
1020
1021
1022
1023
1024
Tổng FLOPS huấn luyện0.20.30.40.50.6Điểm đánh giá core
1022
1023
Tổng FLOPS huấn luyện0.30.40.50.6Độ chính xác MMLU 5-shot
DCLM-Baseline
C4
Dolma v1
Falcon-7BFineWeb edu
LLM360/CrystalCoder
MAP-Neo-7B
MPT-7BOLMo-1B
OLMo-7B
OLMo-1.7-7B
RedPajamaRefinedWeb
Together-RPJ-7B
DeepSeek
Gemma-2BGemma-7B
Llama1-7B
Llama2-7B
Llama3-8B

Hình 1: Cải thiện bộ dữ liệu huấn luyện dẫn đến các mô hình tốt hơn và rẻ hơn để huấn luyện. Sử dụng DataComp-LM, chúng tôi phát triển một tập dữ liệu chất lượng cao, DCLM-BASELINE, mà chúng tôi sử dụng để huấn luyện các mô hình với sự đánh đổi state-of-the-art giữa tính toán và hiệu suất. Chúng tôi so sánh trên cả (trái) một tập nhiệm vụ CORE và trên (phải) MMLU 5-shot. Cụ thể DCLM-BASELINE (màu cam) cho thấy hiệu suất thuận lợi so với cả các mô hình nguồn đóng (dấu x) và các tập dữ liệu và mô hình nguồn mở khác (hình tròn). Các mô hình trong hình này được lấy từ [4,10,22,46,73,103,103,106,128,137,157,162,164,168–170,198]. Bảng 33 cung cấp phiên bản bảng của hình này.

1 Giới thiệu
Các tập dữ liệu huấn luyện lớn là động lực quan trọng của tiến bộ trong cuộc cách mạng mô hình hóa ngôn ngữ gần đây [63,69,90,130,138,157,163,181]. Khi chi phí huấn luyện các mô hình ngôn ngữ state-of-the-art tiếp tục tăng, các nhà nghiên cứu ngày càng tập trung không chỉ vào việc mở rộng quy mô mà còn vào việc cải thiện các tập dữ liệu huấn luyện cho phép khái quát hóa hiệu quả trên một loạt rộng các nhiệm vụ downstream. Thực tế, có một số lượng ngày càng tăng các đề xuất để lọc dữ liệu, loại bỏ các bản sao (gần) trùng lặp, tìm nguồn dữ liệu mới, cân bằng các điểm dữ liệu, tạo dữ liệu tổng hợp, v.v. [2,8,75,94,97,102,119,186].

Một thách thức chính trong lĩnh vực nghiên cứu mới nổi này là thiếu các so sánh có kiểm soát. Trong khi các đề xuất nói trên thường sử dụng cùng các tập dữ liệu đánh giá, các nhà nghiên cứu thường so sánh các mô hình được huấn luyện với các kiến trúc, tính toán hoặc siêu tham số khác nhau. Do đó, thường không rõ ràng chiến lược tuyển chọn dữ liệu nào hoạt động tốt nhất: Liệu kết quả của bộ dữ liệu huấn luyện A có tốt hơn bộ dữ liệu huấn luyện B bởi vì bộ dữ liệu huấn luyện A thực sự tốt hơn, hay bởi vì mô hình được huấn luyện trên A được kết hợp với kiến trúc tốt hơn, lịch trình tốc độ học tập tốt hơn, hoặc nhiều tính toán hơn? Việc tách biệt nhiều yếu tố ảnh hưởng đến chất lượng của một mô hình ngôn ngữ là rất quan trọng để hiểu chiến lược tuyển chọn dữ liệu nào hoạt động tốt nhất và cuối cùng xây dựng các mô hình ngôn ngữ tốt hơn.

Ngoài việc thiếu các benchmark chuẩn hóa, một thách thức khác cho nghiên cứu về dữ liệu huấn luyện là các chi tiết về bộ dữ liệu huấn luyện đang trở nên ngày càng hiếm, ngay cả đối với các mô hình trọng số mở như các mô hình Llama, Mistral hoặc Gemma [83,162,169]. Đối với tất cả các mô hình này, bộ dữ liệu huấn luyện không được công khai, và tài liệu mô hình tương ứng chỉ cung cấp mô tả thô về dữ liệu huấn luyện tương ứng, nếu có. Kết quả là, hiện tại không rõ ràng những thành phần nào tạo nên một bộ dữ liệu huấn luyện state-of-the-art cho các mô hình ngôn ngữ.

Để giải quyết những thách thức này, chúng tôi giới thiệu DataComp cho Mô hình Ngôn ngữ (DCLM), benchmark quy mô lớn đầu tiên cho việc tuyển chọn dữ liệu huấn luyện mô hình ngôn ngữ. Trong DCLM, các nhà nghiên cứu đề xuất bộ dữ liệu huấn luyện mới và thuật toán tuyển chọn dữ liệu và sau đó đánh giá tập dữ liệu của họ bằng cách huấn luyện LM với một

--- TRANG 3 ---
Hình 2: Quy trình làm việc DCLM. (A) Người tham gia đầu tiên chọn một quy mô, trong đó các quy mô lớn hơn phản ánh nhiều token huấn luyện hoặc tham số mô hình hơn. (B) Người tham gia sau đó lọc một nhóm dữ liệu (track lọc) hoặc trộn dữ liệu của riêng họ (track trộn) để tạo ra một tập dữ liệu. (C) Sử dụng tập dữ liệu được tuyển chọn, người tham gia huấn luyện một mô hình ngôn ngữ, với mã huấn luyện chuẩn hóa và siêu tham số cụ thể theo quy mô, sau đó được (D) đánh giá trên 53 nhiệm vụ downstream để đánh giá chất lượng tập dữ liệu.

công thức huấn luyện cố định trên dữ liệu của họ. Bằng cách đo lường hiệu suất của mô hình kết quả trên các nhiệm vụ downstream, các nhà nghiên cứu có thể định lượng điểm mạnh và điểm yếu của bộ dữ liệu huấn luyện tương ứng.

Để kích hoạt DCLM, chúng tôi đóng góp một bài kiểm tra thí nghiệm toàn diện. Một thành phần chính là DCLM-POOL, một kho dữ liệu gồm 240 nghìn tỷ token được lấy từ Common Crawl [45]. DCLM-POOL là kho dữ liệu công khai lớn nhất để huấn luyện mô hình ngôn ngữ và tạo thành nền tảng của track lọc DCLM, nơi người tham gia nhằm tuyển chọn bộ dữ liệu huấn luyện tốt nhất có thể từ DCLM-POOL. Ngoài ra, chúng tôi cung cấp phần mềm nguồn mở để xử lý các tập dữ liệu lớn với một số phương pháp lọc.

Chi phí cao của việc huấn luyện các mô hình ngôn ngữ khiến việc hiểu hiệu suất của các công thức huấn luyện ở các quy mô tính toán và dữ liệu khác nhau trở nên cần thiết. Do đó, đóng góp thứ ba của chúng tôi là một cuộc điều tra về xu hướng mở rộng quy mô cho thiết kế tập dữ liệu. Chúng tôi phát hiện ra rằng các mô hình nhỏ như 400M tham số vẫn có thể cung cấp tín hiệu về bộ dữ liệu huấn luyện nào hoạt động tốt hơn ở quy mô lớn hơn. Dựa trên các thí nghiệm của chúng tôi, chúng tôi tổ chức DCLM thành năm quy mô tính toán trải dài khoảng 600× về tính toán từ các mô hình 400M tham số đến các mô hình 7B được huấn luyện quá mức. Thiết kế đa quy mô này làm cho DCLM có thể tiếp cận được với các nhà nghiên cứu có ngân sách tính toán khác nhau.

Như điểm khởi đầu cho DCLM, chúng tôi tiến hành 416 thí nghiệm cơ sở với các bộ dữ liệu huấn luyện và quy mô tính toán khác nhau. Các thí nghiệm của chúng tôi xác định việc lọc dựa trên mô hình là thành phần chính cho việc tuyển chọn dữ liệu hiệu quả. Chúng tôi cũng cho thấy rằng các chi tiết của mô hình lọc có thể có tác động lớn đến hiệu suất, dao động từ 35% đến 44% độ chính xác trên MMLU 5-shot [77] ở quy mô 7B tham số (280B token huấn luyện). Thú vị là, một bộ phân loại bigram đơn giản, kết hợp với một tập hợp được chọn cẩn thận các ví dụ tích cực và tiêu cực, hoạt động tốt nhất trong số các bộ phân loại mà chúng tôi đã thí nghiệm. Ngoài ra, chúng tôi phát hiện ra rằng các đánh giá chất lượng của con người chỉ có giá trị hạn chế trong việc xác định dữ liệu huấn luyện chất lượng cao.

Cuối cùng, chúng tôi kết hợp kết quả của mình thành DCLM-BASELINE, một bộ dữ liệu huấn luyện công khai state-of-the-art mới cho các mô hình ngôn ngữ. Khi huấn luyện một mô hình ngôn ngữ 7B tham số trên 2.6 nghìn tỷ token sử dụng DCLM-BASELINE, mô hình kết quả đạt 64% trên MMLU, là state-of-the-art trong số các mô hình dữ liệu mở và gần với các mô hình như Mistral-7B-v0.3 (63%) hoặc Llama 3 8B (66%) được huấn luyện với tới 6.6× tính toán nhiều hơn (Llama 3 8B). So với Llama 2 7B, việc huấn luyện một mô hình 7B tham số trên 280B token từ DCLM-BASELINE đạt MMLU cao hơn 5 pp trong khi được huấn luyện với ít hơn 7× tính toán. Vì mô hình 7B của chúng tôi sử dụng Transformer chỉ giải mã tiêu chuẩn [134,169,173], kết quả của chúng tôi cũng làm nổi bật rằng một phương pháp có hệ thống để tuyển chọn dữ liệu là chìa khóa để huấn luyện các mô hình ngôn ngữ hiệu suất cao.

Chúng tôi công khai phát hành framework DCLM, mô hình và bộ dữ liệu huấn luyện tại https://datacomp.ai/dclm để cho phép các nhà nghiên cứu khác tham gia vào DCLM và tăng cường nền tảng thực nghiệm cho nghiên cứu tập trung vào dữ liệu về các mô hình ngôn ngữ.

2 Công trình liên quan
Chúng tôi tóm tắt công trình liên quan gần đây trong phần này và cung cấp thêm công trình liên quan trong Phụ lục B.

--- TRANG 4 ---
Bảng 1: Các quy mô cuộc thi DCLM. DCLM chứa năm quy mô cuộc thi, cho phép nghiên cứu trong các chế độ tính toán khác nhau. Mỗi quy mô chỉ định kích thước mô hình ('Tham số mô hình', N), số token nhìn thấy trong quá trình huấn luyện ('Token huấn luyện', D), và kích thước của nhóm gốc có thể được sử dụng để lọc ('Kích thước nhóm'). Chúng tôi cung cấp ước tính về tính toán cần thiết để huấn luyện ('FLOP Huấn luyện' = 6ND) và giờ GPU ('Giờ H100 Huấn luyện') sử dụng framework huấn luyện OpenLM [76].

| Quy mô | Tham số mô hình | Token huấn luyện | FLOP Huấn luyện | Giờ H100 Huấn luyện | Kích thước nhóm |
|--------|----------------|------------------|-----------------|-------------------|-----------------|
| 400M-1x | 412M | 8.2B | 2.0e19 | 26 | 469B |
| 1B-1x | 1.4B | 28.8B | 2.4e20 | 240 | 1.64T |
| 3B-1x | 2.8B | 55.9B | 9.4e20 | 740 | 3.18T |
| 7B-1x | 6.9B | 138B | 5.7e21 | 3,700 | 7.85T |
| 7B-2x | 6.9B | 276B | 1.1e22 | 7,300 | 15.7T |

Tuyển chọn dữ liệu cho các mô hình ngôn ngữ. Để thu thập các tập dữ liệu lớn để huấn luyện LM [31], các nhà nghiên cứu thường dựa vào việc thu thập web. Tuy nhiên, các lần thu thập này thường chứa lượng lớn nội dung không mong muốn, đòi hỏi tuyển chọn để phát triển dữ liệu huấn luyện chất lượng cao. Hầu hết các nỗ lực tuyển chọn dữ liệu tập trung vào các phương pháp cải thiện hiệu suất mô hình [31,128,135,138,157,178], bao gồm lọc theo ngôn ngữ [47,92,138,189], lọc dựa trên quy tắc [37,63,128,135,157], lọc chất lượng [53,105,146,178,187], khử trùng lặp dữ liệu [3,94] và trộn [6,155,186]. Trong khi công trình trước đây kiểm tra một tập hạn chế các bộ lọc, chúng tôi tiến hành cuộc điều tra công khai lớn nhất về tuyển chọn dữ liệu, dẫn đến một tập dữ liệu DCLM-BASELINE mạnh mẽ. Kể từ khi phát hành ban đầu của DCLM, các công trình mới hơn như WebOrganizer [180], Nemotron-CC [159], và Olmo-2 [121] cũng đã xây dựng dựa trên benchmark hoặc chiến lược tuyển chọn của chúng tôi để tiếp tục thúc đẩy state-of-the-art cho các tập dữ liệu tiền huấn luyện LLM.

Các tập dữ liệu nguồn mở. Khi quy mô của LM đã tăng lên trong những năm qua [4,39,79,122,135,162,169,170], cộng đồng đã tuyển chọn các tập dữ liệu lớn hơn để phù hợp. Các công trình đầu tiên bao gồm tập dữ liệu C4 với 160 tỷ (B) token và The Pile [63] với 300B token. Gần đây hơn, RefinedWeb [128] chứa 600B token, Dolma [157] 3 nghìn tỷ (T) token, FineWeb 15T token [129], và RedPajama-v2 30T token [46]. Cũng có các tập dữ liệu lớn cụ thể theo lĩnh vực, như StackV2 tập trung vào code với 900B token [107], cũng như các tập con được lọc chất lượng cao như FineWeb-Edu [106] với 1.3T token. Chúng tôi bao gồm so sánh hiệu suất với các tập dữ liệu khác nhau trong Hình 1 và kiểm tra framework đánh giá LightEval của FineWeb một cách kỹ lưỡng hơn trong Phụ lục G. Chúng tôi phát hành nhóm dữ liệu văn bản thô lớn nhất cho đến nay với 240T token được thu thập từ web. Chúng tôi cũng phát hành DCLM-BASELINE, một tập dữ liệu chất lượng cao 3.8T token từ nhóm của chúng tôi tạo ra các mô hình tốt hơn so với các tập dữ liệu trước đây.

Các benchmark tập trung vào dữ liệu. Công trình trước đây về đánh giá cải thiện dữ liệu bao gồm chưng cất tập dữ liệu [50], học tập theo chương trình [144], và học chuyển giao [5,32]. Trong DataComp [61] và DataPerf [112], người tham gia lặp lại trên một tập dữ liệu với mô hình và công thức huấn luyện cố định cho các nhiệm vụ thị giác, thị giác-ngôn ngữ và giọng nói. Đối với LM, nỗ lực Data-Juicer [36] bao gồm các benchmark để làm sạch và trộn dữ liệu tinh chỉnh trong khi thử thách BabyLM track Loose [175] tập trung vào phát triển hiệu quả các LM 125M đến 220M tham số được tiền huấn luyện trên 10M đến 100M token. Với nhóm 240T token và mô hình 7B, DCLM là benchmark tập trung vào dữ liệu lớn nhất cho các mô hình ngôn ngữ.

3 Benchmark DataComp cho mô hình ngôn ngữ (DCLM)
Phần này mô tả các thành phần chính của DCLM. Chúng tôi bắt đầu với DCLM-POOL, kho dữ liệu văn bản thô cơ bản của benchmark của chúng tôi (Phần 3.1). Sau đó chúng tôi mô tả quy trình làm việc DCLM, được hình dung trong Hình 2: chọn quy mô cuộc thi (Phần 3.2), tuyển chọn tập dữ liệu bằng cách lọc DCLM-POOL và có thể trộn với các nguồn khác (Phần 3.3), huấn luyện mô hình với siêu tham số cố định (Phần 3.4), và đánh giá mô hình để chấm điểm tập dữ liệu (Phần 3.5).

--- TRANG 5 ---
3.1 DCLM-POOL
DCLM-POOL là một kho dữ liệu văn bản web chưa được lọc bao gồm tất cả dữ liệu Common Crawl [45] trước năm 2023. Dựa trên Phần 4.2, chúng tôi trích xuất lại văn bản từ HTML bằng cách sử dụng resiliparse [20,21] thay vì sử dụng văn bản được trích xuất sẵn của Common Crawl. DCLM-POOL chứa 200B tài liệu (370TB sau nén gzip), dẫn đến 240T token GPT-NeoX [24]. Xem Phụ lục E để biết thêm chi tiết.

Khử nhiễm. Các mẫu tập kiểm tra thường nhiễm các bộ dữ liệu huấn luyện mô hình ngôn ngữ [52,55,190]; tuy nhiên, tác động của các mẫu như vậy đến hiệu suất downstream vẫn còn phần lớn không rõ ràng [94,122,157]. Để cho phép các nhà nghiên cứu hiểu rõ hơn về việc nhiễm, chúng tôi phát hành công cụ khử nhiễm thay vì khử nhiễm DCLM-POOL trực tiếp. Đầu tiên, như được sử dụng trong Phần 4.6, chúng tôi triển khai quy trình khử nhiễm của riêng mình cho hai nhiệm vụ phổ biến, MMLU và Hellaswag [78,195]. Thứ hai, chúng tôi cung cấp công cụ dựa trên Lee et al. [94] để kiểm tra tập dữ liệu về sự chồng chéo với tất cả các tập kiểm tra của chúng tôi. Chúng tôi yêu cầu tất cả các bài nộp tiết lộ báo cáo khử nhiễm và tránh sử dụng dữ liệu bị nhiễm cao. Đối với các bài nộp có điểm cao nhất, chúng tôi dự định đánh giá cụ thể chúng về việc nhiễm.

3.2 Quy mô cuộc thi: Hỗ trợ người tham gia với các ràng buộc tính toán khác nhau

[THIS IS FIGURE/CHART: A graph showing the relationship between Core evals score at different scales and 7B-1x scale performance]

Hình 3: Các tập dữ liệu xếp hạng nhất quán qua các quy mô cuộc thi trong DCLM. Điều này cho phép lặp lại việc tuyển chọn dữ liệu ở quy mô nhỏ.

Để đảm bảo DCLM có thể tiếp cận được với các nhà nghiên cứu có ràng buộc tính toán khác nhau và để tạo điều kiện cho việc nghiên cứu xu hướng mở rộng quy mô, chúng tôi tạo ra các quy mô cuộc thi khác nhau trải dài ba bậc độ lớn tính toán (Bảng 1). Mỗi quy mô (tức là, 400M-1x, 1B-1x, 3B-1x, 7B-1x, và 7B-2x) chỉ định số tham số mô hình (ví dụ, 7B) và hệ số nhân Chinchilla (ví dụ, 1x). Số token huấn luyện cho mỗi quy mô là 20× số tham số × hệ số nhân Chinchilla để hệ số nhân 1x tương ứng với phân bổ tính toán mà Hoffmann et al. [79] thấy gần tối ưu.

Một cạm bẫy tiềm năng trong thiết kế đa quy mô của chúng tôi là thứ hạng của các phương pháp tuyển chọn dữ liệu có thể thay đổi khi tăng quy mô tính toán. Để hiểu rõ hơn mối quan ngại này, trong Hình 3, chúng tôi vẽ đồ thị hiệu suất của 10 phương pháp ở quy mô 7B-1x như một hàm của hiệu suất của chúng ở quy mô nhỏ hơn. Chúng tôi thấy tương quan thứ hạng cao giữa kết quả cho quy mô nhỏ hơn (400M-1x, 1B-1x, 3B-1x) và những kết quả cho quy mô 7B-1x lớn hơn (Pearson's r = 0.838, r = 0.956, r = 0.982 tương ứng), gợi ý các chiến lược tuyển chọn tốt hơn ở quy mô nhỏ hơn chuyển giao sang quy mô lớn hơn. Để biết thêm các thử nghiệm quy mô cuộc thi, bao gồm các thí nghiệm gợi ý cải thiện tập dữ liệu phần lớn trực giao với siêu tham số huấn luyện, xem Phụ lục H.

3.3 Các track benchmark: Lọc và trộn
Sau khi chọn quy mô, người tham gia chọn một trong hai track. (i) Trong track lọc, người tham gia đề xuất thuật toán để chọn dữ liệu huấn luyện từ một nhóm ứng viên. Chúng tôi bắt đầu với năm nhóm, một cho mỗi quy mô trong Bảng 1, là các tập con tài liệu ngẫu nhiên của DCLM-POOL. Chúng tôi hạn chế kích thước nhóm ban đầu theo quy mô để khuyến khích các chiến lược lọc có thể mở rộng và phản ánh các ràng buộc thực tế về tải xuống và lưu trữ dữ liệu. (ii) Trong track trộn, một bài nộp có thể kết hợp tài liệu từ DCLM-POOL với những tài liệu từ bất kỳ nguồn bên ngoài nào (tức là, ngoài Common Crawl). Ví dụ, người tham gia có thể thêm vào các tài liệu được tuyển chọn trực tiếp từ dữ liệu dump StackExchange và Wikipedia (như được thực hiện bởi RPJ [168]) hoặc thậm chí là thu thập tùy chỉnh của riêng họ. Phụ lục C cung cấp quy tắc chi tiết hơn cho mỗi track, và Phụ lục D mô tả công cụ nguồn mở có thể mở rộng của chúng tôi để thực hiện các hoạt động lọc và trộn.

--- TRANG 6 ---
[Continuing with the translation of the image content...]

Bộ lọc tiếng Anh, Bộ lọc URL, DCLM-Pool (CommonCrawl), Bộ lọc độ dài trang, Bộ lọc tỷ lệ loại bỏ từ, Bộ lọc lặp lại, Làm sạch heuristic (Phần 4.1 & 4.2) (Tái tạo RefinedWeb), Khử trùng lặp (4.3), DCLM-BaselineRW 0.8%, 50.8%, 7.9%, 9.6%, 2.0%, Lọc dựa trên mô hình (4.4), Bloom filter dedup, FastText filter 6.2%, 12.3%, 1.4%, 13.7%, 19.9%, Các bộ lọc khác (ví dụ: Độ dài từ, Số lượng dấu chấm lửng, Từ dừng) 9.0%

Hình 4: Xây dựng DCLM-BASELINE từ DCLM-POOL. Trước pipeline này, chúng tôi đã trích xuất DCLM-Pool từ Common Crawl với resiliparse. Phần trăm dựa trên tổng số tài liệu gốc.

3.4 Huấn luyện
Để cô lập hiệu ứng của các can thiệp tập dữ liệu, chúng tôi cố định một công thức huấn luyện ở mỗi quy mô. Dựa trên các thử nghiệm trước đây về kiến trúc và huấn luyện mô hình [4,31,39,62,79,93,134,169,170,183], chúng tôi áp dụng Transformer chỉ giải mã (ví dụ, GPT-2, Llama) [134,169,173], được triển khai trong OpenLM [76]. Chúng tôi cũng cung cấp tiện ích xử lý dữ liệu thống nhất. Phụ lục F chứa thêm chi tiết huấn luyện.

3.5 Đánh giá
Bộ đánh giá đầy đủ của chúng tôi, dựa trên LLM-Foundry [115], chứa 53 nhiệm vụ downstream phù hợp để đánh giá mô hình cơ sở (tức là, không cần tinh chỉnh): từ trả lời câu hỏi đến các định dạng sinh tự do, xem xét các lĩnh vực đa dạng như toán học, kiến thức sách giáo khoa và lý luận thường thức. Để đánh giá thuật toán tuyển chọn dữ liệu, chúng tôi tập trung vào ba chỉ số hiệu suất chính. Đầu tiên, chúng tôi xem xét độ chính xác MMLU 5-shot [78], được sử dụng rộng rãi để so sánh các mô hình state-of-the-art như GPT-4 [122] và Llama 3 70B [4]. Thứ hai, chúng tôi đề xuất độ chính xác tập trung CORE, được tính toán trên tập con 22 nhiệm vụ (ví dụ, HellaSwag [195] và ARC-E [43]) cung cấp tín hiệu phương sai thấp ngay cả ở quy mô nhỏ, tái chia tỷ lệ tuyến tính độ chính xác mỗi nhiệm vụ để 0 tương ứng với đoán ngẫu nhiên và 1 tương ứng với độ chính xác hoàn hảo. Cuối cùng, chúng tôi báo cáo độ chính xác tập trung EXTENDED, tính trung bình hiệu suất tập trung cho tất cả 53 nhiệm vụ của chúng tôi. Để biết thêm chi tiết chỉ số, xem Phụ lục G.

4 Xây dựng bộ dữ liệu huấn luyện chất lượng cao với DCLM
Chúng tôi bây giờ cho thấy cách quy trình làm việc DCLM có thể dẫn đến các tập dữ liệu chất lượng cao và định lượng ảnh hưởng của các phương pháp tuyển chọn dữ liệu. Phần này mô tả quá trình chuyển đổi Common Crawl thành tập dữ liệu của chúng tôi, DCLM-BASELINE, như được hiển thị trong Hình 4. Chúng tôi cung cấp các thí nghiệm thử nghiệm cho mỗi bước dọc theo con đường. Đầu tiên chúng tôi đánh giá các tập dữ liệu nguồn mở làm điểm khởi đầu (Phần 4.1). Tiếp theo, chúng tôi thí nghiệm với các lựa chọn thay thế cho một số giai đoạn chính của xây dựng tập dữ liệu: trích xuất văn bản (Phần 4.2), khử trùng lặp (Phần 4.3), và lọc dựa trên mô hình (Phần 4.4). Sau đó chúng tôi thí nghiệm với việc trộn vào các nguồn chất lượng cao (Phần 4.5) và cung cấp phân tích nhiễm (Phần 4.6). Trong Phần 5, chúng tôi mở rộng quy mô phương pháp này để huấn luyện mô hình 7B cho 2T token.

4.1 Đánh giá các tập dữ liệu huấn luyện hiện có
Chúng tôi bắt đầu bằng cách đánh giá một số tập dữ liệu nguồn mở nổi tiếng (C4 [52,137], RefinedWeb [128], RedPajama [168], và Dolma-V1 [157]) trong Bảng 2. Trong khi tất cả bốn tập dữ liệu sử dụng các bộ lọc heuristic khác nhau và các bước làm sạch dữ liệu, chúng tôi thấy rằng RefinedWeb hoạt động tốt nhất trên các chỉ số CORE và EXTENDED của chúng tôi ở quy mô 7B-1x. RefinedWeb áp dụng pipeline lọc sau: trích xuất văn bản Common Crawl, quy tắc chọn heuristic (ví dụ, để loại bỏ spam), và khử trùng lặp nội dung lặp lại. Thú vị là, RefinedWeb chỉ được lọc từ Common Crawl, không giống như RedPajama và Dolma-V1, mà còn trộn thêm các nguồn được tuyển chọn, "chất lượng cao" như Wikipedia. So sánh gợi ý sức mạnh tương đối của việc lọc, mà chúng tôi khám phá sau trong các thí nghiệm của chúng tôi.

Điểm rút ra: Đối với DCLM-BASELINE và các thí nghiệm khác, chúng tôi áp dụng các bộ lọc heuristic của RefinedWeb.

4.2 Trích xuất văn bản
Trích xuất văn bản là một bước xử lý đầu quan trọng để kéo nội dung từ HTML thô. Để hiểu hiệu ứng của bước này, chúng tôi so sánh ba phương pháp trích xuất: resiliparse, trafilatura (được sử dụng bởi RefinedWeb), và các tệp WET do Common Crawl cung cấp chứa văn bản được trích xuất sẵn. Sau đó chúng tôi áp dụng các bộ lọc chất lượng heuristic của RefinedWeb cho mỗi trích xuất văn bản. Trong Bảng 3, chúng tôi thấy cả resiliparse và trafilatura đều cải thiện CORE ít nhất 2.5 điểm so với trích xuất WET. Điều này có ý nghĩa vì hầu hết các tập dữ liệu nguồn mở, bao gồm C4, RedPajama, và Dolma-V1, sử dụng tệp WET, có thể giải thích một phần hiệu suất kém hơn của chúng trong Bảng 2. Trong khi resiliparse và trafilatura có hiệu suất downstream tương tự, resiliparse nhanh hơn 8× để chạy và do đó thực tế hơn cho xử lý quy mô lớn. Để biết thêm phân tích, xem Phụ lục K.

Điểm rút ra: Đối với DCLM-POOL và các thí nghiệm còn lại, chúng tôi sử dụng resiliparse để trích xuất văn bản.

4.3 Khử trùng lặp
Các tập dữ liệu được thu thập từ web thường chứa nhiều chuỗi dữ liệu trùng lặp hoặc gần trùng lặp. Việc loại bỏ các bản sao này phục vụ mục đích kép là cải thiện hiệu suất bằng cách giảm việc ghi nhớ [35,94] và tăng sự đa dạng dữ liệu. Để khử trùng lặp, chúng tôi khám phá MinHash [29], như một phần của pipeline mảng hậu tố [94,128], và lọc Bloom gần trùng lặp, sửa đổi sơ đồ khử trùng lặp tài liệu và đoạn văn chính xác [157]. Chúng tôi thấy rằng cả hai phương pháp đều cung cấp hiệu suất downstream tương đương: trong vòng 0.2 điểm phần trăm CORE ở quy mô 7B-2x. Tuy nhiên, bộ lọc Bloom đã sửa đổi của chúng tôi mở rộng dễ dàng hơn đến các tập dữ liệu vượt quá 10TB. Chúng tôi cung cấp thêm phân tích trong Phụ lục L.

Điểm rút ra: Chúng tôi sử dụng bộ lọc Bloom cho DCLM-BASELINE và MinHash cho các thí nghiệm khác.

4.4 Lọc chất lượng dựa trên mô hình
Tài liệu gần đây [28,59,157] chỉ ra rằng việc huấn luyện các mô hình để phục vụ như bộ lọc chất lượng dẫn đến cải thiện downstream. Trong phần này, chúng tôi điều tra các phương pháp lọc dựa trên mô hình khác nhau.

So sánh các phương pháp lọc dựa trên mô hình. Chúng tôi so sánh nhiều chiến lược: 1) Lọc điểm PageRank để giữ lại tài liệu dựa trên khả năng chúng được liên kết đến các tài liệu khác, 2) Khử trùng lặp ngữ nghĩa (SemDedup) để loại bỏ các tài liệu có nội dung thông tin tương tự [1], 3) các bộ phân loại tuyến tính phù hợp trên các embedding văn bản BGE được tiền huấn luyện [185], 4) AskLLM nhắc một LM để xem liệu một tài liệu có hữu ích không [146], 5) Lọc perplexity nơi chúng tôi giữ lại các chuỗi perplexity thấp theo CCNet [178], 6) Logit trung bình top-k nơi chúng tôi tính trung bình top-k logit mô hình trên tất cả

Bảng 2: So sánh với các tập dữ liệu hiện có (quy mô 7B-1x). Mặc dù không trộn các nguồn chất lượng cao (không giống như Dolma-V1 và RedPajama), RefinedWeb hoạt động tốt nhất.

| Tập dữ liệu | CORE | EXTENDED |
|-------------|------|----------|
| C4 | 34.2 | 18.0 |
| Dolma-V1 | 35.0 | 18.4 |
| RedPajama | 35.3 | 18.2 |
| RefinedWeb | 36.9 | 19.8 |

--- TRANG 7 ---
Bảng 3: So sánh các bộ trích xuất văn bản (quy mô 1B-1x). Chúng tôi áp dụng ba phương pháp trích xuất văn bản từ HTML, xử lý đầu ra của chúng bằng các bộ lọc heuristic RefinedWeb, và đánh giá các mô hình được huấn luyện trên các tập dữ liệu kết quả. Chúng tôi thấy các bộ trích xuất nghiêm ngặt hơn như resiliparse và trafilatura vượt trội hơn các tệp WET được cung cấp bởi Common Crawl.

| Trích xuất văn bản | CORE | EXTENDED |
|-------------------|------|----------|
| resiliparse | 24.1 | 13.4 |
| trafilatura | 24.5 | 12.5 |
| Tệp WET | 20.7 | 12.2 |

Bảng 4: So sánh lọc chất lượng (quy mô 1B-1x). Chúng tôi đánh giá các lựa chọn khác nhau cho các bộ lọc chất lượng dựa trên mô hình. Huấn luyện bộ phân loại fastText cho việc lọc hoạt động tốt nhất.

| Bộ lọc | CORE | EXTENDED |
|--------|------|----------|
| Tái tạo RefinedWeb | 27.5 | 14.6 |
| Top 20% theo Pagerank | 26.1 | 12.9 |
| SemDedup [1] | 27.1 | 13.8 |
| Bộ phân loại trên tính năng BGE [185] | 27.2 | 14.0 |
| AskLLM [146] | 28.6 | 14.3 |
| Lọc perplexity | 29.0 | 15.0 |
| Logit trung bình top-k | 29.2 | 14.7 |
| fastText [87] OH-2.5 + ELI5 | 30.2 | 15.4 |

từ trong một tài liệu để chấm điểm mức độ tự tin của mô hình rằng các từ đúng nằm trong k lựa chọn hợp lý, và 7) các bộ phân loại nhị phân fastText [87] để phân biệt chất lượng dữ liệu. Để huấn luyện các bộ phân loại, chúng tôi huấn luyện trên ~400k tài liệu được chia đều giữa các lớp tích cực và tiêu cực. Chúng tôi thí nghiệm với các tùy chọn khác nhau cho dữ liệu tích cực và cố định dữ liệu tiêu cực là một mẫu ngẫu nhiên từ phiên bản tái tạo RefinedWeb của chúng tôi. Đối với các chiến lược lọc perplexity và logit trung bình top-k, chúng tôi sử dụng một Transformer nhân quả 154M tham số được huấn luyện trên hỗn hợp English Wikipedia, tập con sách của RedPajama-v1, và peS2o [156,168] (xem Phụ lục J để biết thêm chi tiết triển khai).

Chúng tôi so sánh các phương pháp nói trên trong Bảng 4 và thấy rằng lọc dựa trên fastText vượt trội hơn tất cả các phương pháp khác. Tiếp theo chúng tôi nhằm hiểu cách các công thức huấn luyện fastText khác nhau ảnh hưởng đến hiệu quả của nó như một mạng lọc dữ liệu [59].

Thử nghiệm bộ phân loại văn bản. Để hiểu rõ hơn giới hạn của fastText, chúng tôi huấn luyện một số biến thể, khám phá các lựa chọn khác nhau cho dữ liệu tham chiếu (tức là, các ví dụ được gán nhãn tích cực), ngưỡng lọc và không gian tính năng như được hiển thị trong Bảng 5 và 14. Đối với dữ liệu tham chiếu, chúng tôi xem xét các nguồn thường được sử dụng như Wikipedia [63], OpenWebText2 [63], và RedPajama-books [168], theo dữ liệu tham chiếu được sử dụng cho GPT-3 [31]. Chúng tôi cũng thử một phương pháp mới, sử dụng dữ liệu định dạng hướng dẫn, rút các ví dụ từ OpenHermes 2.5 [165] (OH-2.5) và các bài đăng điểm cao từ subreddit r/ExplainLikeImFive (ELI5). Nhìn chung, chúng tôi thấy, khi kiểm soát các siêu tham số khác, phương pháp fastText OH-2.5 + ELI5 mang lại cải thiện 3.5 điểm phần trăm trên CORE so với các lựa chọn thông thường khác. Thật tự nhiên khi hỏi liệu việc sử dụng dữ liệu OH-2.5 để lọc có thể ngăn cản các lợi ích bổ sung từ instruction-tuning hay không. Trong Phụ lục Q, chúng tôi cho thấy điều này không phải như vậy, tiếp tục gợi ý sức mạnh và tính tương thích của phương pháp này với các paradigm tinh chỉnh hiện đại. Cuối cùng, chúng tôi quan sát thấy rằng việc sử dụng ngưỡng khá nghiêm ngặt, giữ lại top-10% ví dụ, giúp ích hơn so với ngưỡng khoan dung hơn top-15% và top-20%. Chúng tôi tiếp tục nghiên cứu hành vi không trực quan của lọc tập dữ liệu và mối liên hệ của nó với đánh giá của con người trong Phụ lục N.

--- TRANG 8 ---
Bảng 5: Thử nghiệm fastText (quy mô 7B-1x). Chúng tôi thử nghiệm các lựa chọn cho dữ liệu tích cực (trên) và ngưỡng (dưới). 'Tập dữ liệu' là tập tích cực, trong khi tập tiêu cực được lấy mẫu ngẫu nhiên từ bản tái tạo RefinedWeb của chúng tôi. 'Ngưỡng' là phần trăm được sử dụng để lọc dựa trên điểm fastText. "GPT-3 Approx" đề cập đến hỗn hợp Wikipedia, OpenWebText2, và RPJ Books, như trong [31].

| Tập dữ liệu | Ngưỡng | CORE | MMLU | EXTENDED |
|-------------|---------|------|------|----------|
| OH-2.5 + ELI5 | 10% | 41.0 | 29.2 | 21.4 |
| Wikipedia | 10% | 35.7 | 27.0 | 19.1 |
| OpenWebText2 | 10% | 34.7 | 25.0 | 18.7 |
| GPT-3 Approx | 10% | 37.5 | 24.4 | 20.0 |
| OH-2.5 + ELI5 | 15% | 39.8 | 27.2 | 21.5 |
| OH-2.5 + ELI5 | 20% | 38.7 | 24.2 | 20.3 |

Điểm rút ra: Đối với DCLM-BASELINE và các thí nghiệm còn lại, chúng tôi sử dụng điểm bộ phân loại fastText OH-2.5 + ELI5 để giữ lại top 10% tài liệu. Kết quả của việc lọc này là DCLM-BASELINE.

4.5 Trộn tập dữ liệu
Thường thì, Common Crawl (CC) được kết hợp với các nguồn dữ liệu khác được coi là chất lượng cao [63,70,168,170] (ví dụ, Wikipedia, StackExchange, và peS2o [156]). Vì những người tham gia DCLM có thể bao gồm các nguồn dữ liệu bổ sung trong track trộn của chúng tôi, chúng tôi đã kiểm tra lợi ích tiềm năng của việc thêm các nguồn chất lượng cao vào bộ dữ liệu huấn luyện được lấy chỉ từ Common Crawl. Chúng tôi so sánh một mô hình được huấn luyện trên 100% dữ liệu CC đã lọc với các mô hình được huấn luyện với tỷ lệ trộn từ Llama 1 và RedPajama: 67% CC, và 33% từ Wikipedia, Books, Stack exchange, arXiv, và Github. Đối với thành phần CC, chúng tôi xem xét các biến thể khác nhau: một tập con của DCLM-BASELINE của chúng tôi, phần CC của RedPajama, RefinedWeb, và C4. Kết quả trong Bảng 6 cho thấy rằng việc trộn cải thiện hiệu suất cho các tập con CC hiệu suất thấp hơn (C4, RedPajama-CC, và RefinedWeb). Tuy nhiên trong trường hợp DCLM-BASELINE, việc trộn thực sự làm tổn hại hiệu suất trung bình, gợi ý nó có thể phản tác dụng khi có lọc hiệu suất. Để biết thêm kết quả trộn, xem Phụ lục M.

Bảng 6: Trộn các nguồn chất lượng cao với các tập con của CommonCrawl (quy mô 1B-1x). Chúng tôi đánh giá tác động của việc trộn các nguồn chất lượng cao ('RPJ extras') với các tập dữ liệu khác nhau được lấy từ CommonCrawl, sử dụng tỷ lệ trộn từ Llama/RPJ. Số trong ngoặc đơn chỉ ra mức tăng hoặc giảm hiệu suất do việc trộn so với chỉ sử dụng tập dữ liệu cơ sở.

|          | CORE |  | EXTENDED |  |
|----------|------|--|----------|--|
| Tập dữ liệu | Cơ sở | w/ RPJ extras | Cơ sở | w/ RPJ extras |
| C4 | 23.7 | 25.9 (+2.2) | 12.5 | 13.3 (+0.8) |
| RPJ CC only | 24.0 | 25.7 (+1.7) | 12.1 | 13.5 (+1.4) |
| RefinedWeb | 25.1 | 26.5 (+1.4) | 12.9 | 13.1 (+0.2) |
| DCLM-BASELINE | 31.1 | 29.9 (−1.2) | 16.0 | 15.0 (−1.0) |

4.6 Khử nhiễm
Ở đây, chúng tôi kiểm tra liệu việc nhiễm dữ liệu tiền huấn luyện của chúng tôi với dữ liệu đánh giá có ảnh hưởng đến kết quả của chúng tôi cho DCLM-BASELINE hay không. Chúng tôi tập trung vào MMLU và Hellaswag làm bộ đánh giá lựa chọn của chúng tôi, cho sự phổ biến của chúng như các chỉ số để so sánh hiệu suất mô hình ngôn ngữ ở quy mô 7B.

Cụ thể, chúng tôi cố gắng loại bỏ các ví dụ từ hai bộ kiểm tra này tồn tại trong DCLM-BASELINE. Đối với cả hai, chúng tôi đánh dấu các trang chứa văn bản câu hỏi cùng với ít nhất một trong các tùy chọn trả lời tương ứng cho một ví dụ nhất định. Đối với các trang được đánh dấu này, sau đó chúng tôi loại bỏ tất cả các chuỗi câu hỏi và tùy chọn khớp. Để cải thiện recall cho MMLU, chứa một số câu hỏi dài dựa trên đoạn văn, chúng tôi chọn chỉ phát hiện câu cuối cùng từ mỗi câu hỏi, giảm cơ hội bỏ lỡ câu hỏi do sự khác biệt về định dạng. Dựa trên kiểm tra, điều này vẫn tạo ra nhiều false positive.

Sau đó chúng tôi huấn luyện một mô hình 7B-2x với DCLM-BASELINE của chúng tôi không có các chồng chéo được phát hiện. Như thấy trong Bảng 7, điều này không dẫn đến giảm hiệu suất mô hình, vì vậy các cải thiện hiệu suất của chúng tôi trên hai nhiệm vụ này có thể không được gây ra bởi sự hiện diện tăng của các ví dụ kiểm tra của chúng trong tập dữ liệu của chúng tôi.

Bảng 7: Loại bỏ chồng chéo MMLU và Hellaswag (quy mô 7B-2x). Chúng tôi loại bỏ các chồng chéo được phát hiện với MMLU và Hellaswag, trong các trường hợp một câu hỏi và một trong các tùy chọn của nó được phát hiện. Chúng tôi so sánh các mô hình được huấn luyện trước và sau bước khử nhiễm này, và thấy rằng hiệu suất không giảm.

| Tập dữ liệu | E = MMLU | E = Hellaswag |
|-------------|----------|---------------|
| DCLM-BASELINE | 51.8 | 77.9 |
| DCLM-BASELINE (E removed) | 52.7 | 78.4 |

Chúng tôi cũng áp dụng chiến lược loại bỏ ở trên cho MMLU trên Dolma-V1.7 [157] và FineWeb-Edu [106]. Các kết quả này có thể được thấy trong Bảng 25 trong Phụ lục O, từ đó chúng tôi quan sát thấy rằng DCLM-BASELINE có thống kê nhiễm gần tương tự như các tập dữ liệu hiệu suất cao khác này. Chúng tôi cũng cung cấp thêm phân tích mở rộng đến toàn bộ bộ đánh giá của chúng tôi trong Phụ lục O.

5 Mở rộng quy mô DCLM-BASELINE đến quy mô nghìn tỷ token
Ở đây, chúng tôi kiểm tra liệu các tập dữ liệu hoạt động tốt trên benchmark DCLM có duy trì sức mạnh của chúng với một bậc độ lớn tính toán nhiều hơn hay không. Để đảm bảo mô hình được huấn luyện của chúng tôi hữu ích rộng rãi, bao gồm cho các nhiệm vụ toán học và lập trình, chúng tôi kết hợp DCLM-BASELINE 3.8T của chúng tôi với các tập dữ liệu StarCoder [96] và ProofPile2 [14] để đạt được một tập dữ liệu 4.1T token. Chúng tôi huấn luyện một mô hình 7B cho 2.5T token trên tập dữ liệu này với cùng siêu tham số như quy mô cuộc thi lớn nhất của chúng tôi ngoại trừ hai giai đoạn cool-down riêng biệt cho 200B và 270B token trên một phân phối đã sửa đổi là 70% DCLM-BASELINE với ngưỡng fastText chặt chẽ hơn, và 30% tập dữ liệu toán học (xem Phụ lục Q). Sau đó chúng tôi lấy "model soup" của hai cool-down riêng biệt này [182]. Cuối cùng, chúng tôi áp dụng phương pháp tiền huấn luyện liên tục từ Pouransari et al. [133] cho 100B token trên cùng phân phối để tăng độ dài ngữ cảnh từ 2048 lên 8192 (xem Phụ lục Q.2).

Trong Bảng 8, chúng tôi cho thấy rằng mô hình của chúng tôi vượt trội hơn tất cả các mô hình 7B được huấn luyện trên bộ dữ liệu huấn luyện công khai và tiếp cận các mô hình dữ liệu đóng được huấn luyện cho nhiều token hơn như Llama-8B, Mistral-7B, và Gemma-7B. Ngoài ra, trong Phụ lục P, chúng tôi cho thấy rằng mô hình của chúng tôi đạt hiệu suất instruction-tuning (IT) mạnh mẽ. Sau khi instruction tuning trên các tập dữ liệu IT có sẵn công khai, mô hình của chúng tôi duy trì hầu hết hiệu suất benchmark của nó và đạt AlpacaEval2.0 LC Win-rate là 16.6, vượt trội hơn Gemma-Instruct (10.4), trong khi tiếp cận hiệu suất mạnh mẽ của Mistral-v0.2-7B (17.1) và Llama3-Instruct (22.9). Cuối cùng, trong Phụ lục Q.3, chúng tôi cho thấy kết quả từ việc huấn luyện một mô hình 1B trên 4.3T token từ DCLM-BASELINE, StarCoder và ProofPile2 kết hợp, tạo ra một mô hình nhỏ mạnh mẽ vượt trội hơn các mô hình nhỏ trước đây bao gồm Gemma-2B và Qwen2-1.5B.

6 Kết luận và hạn chế
Chúng tôi đã giới thiệu bài kiểm tra DCLM và chứng minh cách nó dẫn đến các bộ dữ liệu huấn luyện state-of-the-art mới. Khám phá không gian thiết kế tập dữ liệu của chúng tôi chỉ là khởi đầu và có những hạn chế rõ ràng. Do ràng buộc tính toán, chúng tôi chỉ có thể thử nghiệm các chiều thiết kế riêng lẻ và không thể kiểm tra tất cả các phương pháp ở quy mô lớn hơn cũng như huấn luyện các mô hình vượt quá 7B tham số. Chúng tôi cũng không thể khám phá đầy đủ sự biến thiên run-to-run. Hơn nữa, có nhiều biến thể của DCLM-BASELINE mà chúng tôi không khám phá, như các lựa chọn thay thế cho khử trùng lặp phân đoạn và sử dụng các mô hình lọc được huấn luyện khác nhau. Chúng tôi cũng tiến hành hầu hết các thí nghiệm của mình chỉ với một tokenizer (GPT-NeoX), và các tokenizer khác có thể hoạt động tốt hơn trên các nhiệm vụ đa ngôn ngữ hoặc toán học. Tuy nhiên, chúng tôi hy vọng rằng bài báo này là điểm khởi đầu cho nghiên cứu tiếp theo về tuyển chọn dữ liệu nhằm thúc đẩy state-of-the-art vượt ra ngoài DCLM-BASELINE.

--- TRANG 10 ---
Bảng 8: So sánh state-of-the-art (vượt quy mô 7B-2x). Chúng tôi so sánh mô hình cuối cùng của chúng tôi với các mô hình 7–8B tham số khác. DCLM-BASELINE tạo ra một mô hình vượt trội hơn các mô hình được huấn luyện trên tập dữ liệu mở và cạnh tranh với các mô hình được huấn luyện trên tập dữ liệu riêng tư.

| Mô hình | Tham số | Token | Tập dữ liệu mở? | CORE | MMLU | EXTENDED |
|---------|---------|-------|----------------|------|------|----------|
| **Trọng số mở, tập dữ liệu đóng** |
| Llama2 | 7B | 2T | ✗ | 49.2 | 45.8 | 34.1 |
| DeepSeek | 7B | 2T | ✗ | 50.7 | 48.5 | 35.3 |
| Mistral-0.3 | 7B | ? | ✗ | 57.0 | 62.7 | 45.1 |
| QWEN-2 | 7B | ? | ✗ | 57.5 | 71.9 | 50.5 |
| Llama3 | 8B | 15T | ✗ | 57.6 | 66.2 | 46.3 |
| Gemma | 8B | 6T | ✗ | 57.8 | 64.3 | 44.6 |
| Phi-3 | 7B | ? | ✗ | 61.0 | 69.9 | 57.9 |
| **Trọng số mở, tập dữ liệu mở** |
| Falcon | 7B | 1T | ✓ | 44.1 | 27.4 | 25.1 |
| OLMo-1.7 | 7B | 2.1T | ✓ | 47.0 | 54.0 | 34.2 |
| MAP-Neo | 7B | 4.5T | ✓ | 50.2 | 57.1 | 40.4 |
| **Các mô hình chúng tôi huấn luyện** |
| FineWeb edu | 7B | 0.14T | ✓ | 38.7 | 26.3 | 22.1 |
| FineWeb edu | 7B | 0.28T | ✓ | 41.9 | 37.3 | 24.5 |
| DCLM-BASELINE | 7B | 0.14T | ✓ | 44.1 | 38.3 | 25.0 |
| DCLM-BASELINE | 7B | 0.28T | ✓ | 48.9 | 50.8 | 31.8 |
| DCLM-BASELINE + StarCoder + ProofPile2 | 7B | 2.6T | ✓ | 57.1 | 63.7 | 45.4 |

Trong khi các mô hình được huấn luyện trên DCLM-BASELINE có tính cạnh tranh trên các nhiệm vụ hiểu ngôn ngữ phổ biến, chúng hiện tại không hoạt động tốt như vậy trên code và toán học. Chúng tôi xem điều này như hệ quả của việc tập trung vào hiểu ngôn ngữ trong phiên bản đầu tiên của DCLM, và không phải là hạn chế cố hữu của benchmark hoặc tập dữ liệu của chúng tôi. Công trình trước đây đã cho thấy rằng việc thêm dữ liệu huấn luyện cụ thể và phương pháp hậu huấn luyện cho code và toán học có thể cải thiện đáng kể các mô hình trên những lĩnh vực đó [14,96,177,197,202]; việc kết hợp DCLM-BASELINE với các bộ dữ liệu huấn luyện cụ thể theo lĩnh vực này và mở rộng DCLM để bao phủ code và toán học là những hướng tương lai thú vị. Các chiều quan trọng khác để mở rộng DCLM theo là công bằng, đa ngôn ngữ, và an toàn. Chúng tôi bao gồm một số phân tích trong Phụ lục S và hy vọng rằng bài kiểm tra nguồn mở của chúng tôi có thể tăng cường nghiên cứu tập trung vào dữ liệu theo những hướng này.

--- TRANG 11 ---
Lời cảm ơn. Chúng tôi muốn cảm ơn Lilith Bat-Leah, Loubna Ben Allal, Samy Bengio, Mia Chiquier, Adrien Gaidon, Lizzy Grant, Tom Gunter, Awni Hannun, Tatsunori Hashimoto, Jonathan Hayase, Mike Lewis, Percy Liang, Ian Magnusson, Yifan Mai, Sewon Min, David Mizrahi, Praveen Paritosh, Guilherme Penedo, Kyle Richardson, Weijia Shi, Karanjeet Singh, Joshua Susskind, Tristan Thrush, Oyvind Tafjord, Carl Vondrick, Alexander Wettig, và Elle Wohlmuth, vì phản hồi hữu ích ở các giai đoạn khác nhau của dự án. Chúng tôi cũng muốn cảm ơn Mike Garrison và Romil Shah vì sự giúp đỡ với tính toán và cơ sở hạ tầng.

Nghiên cứu này được hỗ trợ bởi Allen Institute for AI, Open Philanthropy, Institute for Foundations of Machine Learning (IFML), AFOSR MURI grant FA9550-22-1-0380, Israeli Science Foundation (ISF) grant no. 2486/21, Singapore AI Visiting Professorship Programme AIVP-2024-001, Alon Fellowship, Adelis foundation, Israeli Council for Higher Education, Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023, NSF Grants AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, quà tặng nghiên cứu bởi Western Digital, Amazon, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco và Stanly P. Finch Centennial Professorship in Engineering, NSF Graduate Research Fellowship. MN nhận được tài trợ từ Bộ Giáo dục và Nghiên cứu Liên bang Đức theo grant no. 01IS22094B WestAI - AI Service Center West.

Chúng tôi biết ơn nhận được ngân sách tính toán được cấp bởi Gauss Centre for Supercomputing e.V. và bởi John von Neumann Institute for Computing (NIC) trên các siêu máy tính JUWELS Booster và JURECA tại Jülich Supercomputing Centre (JSC).

Tài liệu tham khảo
[1] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, và Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023. URL https://arxiv.org/abs/2303.09540.

[2] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. ArXiv preprint, abs/2404.14219, 2024. URL https://arxiv.org/abs/2404.14219.

[3] Amit Agarwal, Hema Swetha Koppula, Krishna P. Leela, Krishna Prasad Chitrapura, Sachin Garg, Pavan Kumar GM, Chittaranjan Haty, Anirban Roy, và Amit Sasturkar. Url normalization for de-duplication of web pages. In ACM Conference on Information and Knowledge Management, 2009. https://doi.org/10.1145/1645953.1646283.

[4] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2024. https://ai.meta.com/blog/meta-llama-3/.

[5] Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor, Jay Pujara, và William Yang Wang. FETA: A benchmark for few-sample task transfer in open-domain dialogue. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 10936–10953, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.751.

[6] Alon Albalak, Liangming Pan, Colin Raffel, và William Yang Wang. Efficient online data mixing for language model pre-training. ArXiv preprint, abs/2312.02406, 2023. URL https://arxiv.org/abs/2312.02406.

[7] Alon Albalak, Colin Raffel, và William Yang Wang. Improving few-shot generalization by exploring and exploiting auxiliary data. In Advances in Neural Information Processing Systems (NeurIPS), 2023. https://openreview.net/forum?id=JDnLXc4NOn.

[8] Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. ArXiv preprint, abs/2402.16827, 2024. URL https://arxiv.org/abs/2402.16827.

[9] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! ArXiv preprint, abs/2301.03988, 2023. URL https://arxiv.org/abs/2301.03988.

[10] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra-Aimée Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, và Guilherme Penedo. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023.

[11] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, và Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357–2367, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology.org/N19-1245.

[12] Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew L. Leavitt, và Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. ArXiv preprint, abs/2405.20541, 2024. URL https://arxiv.org/abs/2405.20541.

[13] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, David Berard, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Laurent Kirsch, Michael Lazos, Yanbo Liang, Jason Liang, Yuxiang Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, và Soumith Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024. https://pytorch.org/blog/pytorch-2-paper-tutorial.

[14] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, và Sean Welleck. Llemma: An open language model for mathematics. ArXiv preprint, abs/2310.10631, 2023. URL https://arxiv.org/abs/2310.10631.

[15] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. ArXiv preprint, abs/1607.06450, 2016. URL https://arxiv.org/abs/1607.06450.

[16] Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, và Aditya Grover. Comparing bad apples to good oranges: Aligning large language models via joint preference optimization. arXiv preprint arXiv:2404.00530, 2024.

[17] Adrien Barbaresi. Trafilatura: A web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 122–131, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-demo.15. URL https://aclanthology.org/2021.acl-demo.15.
