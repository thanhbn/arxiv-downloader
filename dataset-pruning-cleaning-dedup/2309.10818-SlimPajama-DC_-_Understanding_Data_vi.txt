Tài liệu Tham khảo

[1] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397–2430. PMLR, 2023. 7, 11

[2] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. 9

[3] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, Mar. 2021. If you use this software, please cite it using these metadata. 11

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 7, 8, 14

[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 22

[6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 10

[7] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. 3, 7, 11, 13

[8] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. 21

[9] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933–941. PMLR, 2017. 9

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding, 2019. 14

[11] Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208, 2023. 8, 11

[12] Nathan Habib Sheon Han Nathan Lambert Nazneen Rajani Omar Sanseviero Lewis Tunstall Thomas Wolf Edward Beeching, Clémentine Fourrier. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023. 10, 11, 24

[13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. 7, 13

[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, Sept. 2021. 10, 11, 24

[15] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. 11

[16] Suchin Gururangan, Ana Marašović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don't stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964, 2020. 13

[17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. 10

[18] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in neural information processing systems, 30, 2017. 15, 22

[19] Angelos Katharopoulos and François Fleuret. Not all samples are created equal: Deep learning with importance sampling. In International conference on machine learning, pages 2525–2534. PMLR, 2018. 13

[20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. 15, 22

[21] Teun Kloek and Herman K Van Dijk. Bayesian estimates of equation system parameters: an application of integration by monte carlo. Econometrica: Journal of the Econometric Society, pages 1–19, 1978. 13

[22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint, 2022. 21

[23] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. Mining of massive data sets. Cambridge university press, 2020. 7

[24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214–3252, 2022. 10

[25] Zhuang Liu, Zhiqiu Xu, Joseph Jin, Zhiqiang Shen, and Trevor Darrell. Dropout reduces underfitting. In ICML, 2023. 23

[26] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Dan S Weld. S2orc: The semantic scholar open research corpus. arXiv preprint arXiv:1911.02782, 2019. 21

[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 9

[28] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 5, 7, 8

[29] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. 9

[30] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 14

[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 14

[32] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. 7

[33] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 9

[34] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. 13

[35] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:19523–19536, 2022. 13, 14

[36] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10–19, 2019. 21

[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 7, 13, 22

[38] https://github.com/mosaicml/llm-foundry. Llm foundry. Mosaicml, 2023. 22

[39] https://www.mosaicml.com/blog/mpt-7b. Introducing mpt-7b: A new standard for open-source, commercially usable llms. Mosaicml blog, 2023. 3, 21, 22

[40] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429, 2023. 14

[41] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. arXiv preprint arXiv:2302.03169, 2023. 13

[42] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. 10

[43] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. 11

[44] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 7, 11

--- TRANG 19 ---
Phụ lục

A Chi tiết Tỷ lệ Dữ liệu

Bộ dữ liệu Slimpajama Redpajama LLaMA 1
Commoncrawl 52.2% 333B 72.6% 878B 67.0% 670/938B
C4 26.7% 170B 14.4% 175B 15.0% 150/210B
GitHub 5.2% 33B 4.9% 59B 4.5% 45/63B
Books 4.2% 27B 2.1% 26B 4.5% 45/63B
ArXiv 4.6% 29B 2.3% 28B 2.5% 25/35B
Wikipedia 3.8% 24B 2.0% 24B 4.5% 45/63B
StackExchange 3.3% 21B 1.7% 20B 2.0% 20/28B
Tổng 100.0% 637B 100.0% 1.2T 100% 1.0/1.4T

RefinedWeb GPT3 MassiveText
Commoncrawl 100% 600B 60.0% 180B 0.0% 0
C4 0.0% 0B 0.0% 0 10.0% 30B
GitHub 0.0% 0B 0.0% 0 3.0% 9B
Books 0.0% 0B 16.0% 48B 27.0% 81B
Wikipedia 0.0% 0B 3.0% 9B 2.0% 6B
WebText2 0.0% 0B 22.0% 66B 0.0% 0
MassiveWeb 0.0% 0B 0.0% 0 48.0% 144B
News 0.0% 0B 0.0% 0 10.0% 30B
Tổng 100.0% 600B 100.0% 300B 100.0% 300B
Bảng 8: Tỷ lệ nguồn dữ liệu chi tiết cho các bộ dữ liệu khác nhau.

B MMLU
Trong phần này, chúng tôi cung cấp kết quả từng mục chi tiết trong MMLU, như được hiển thị trong Bảng 9, thú vị là thấy rằng trên một số lĩnh vực phụ trong MMLU, kết quả từ các mô hình 1.3B được cấu hình của chúng tôi thậm chí còn tốt hơn các mô hình GPT-3 175B và LLaMA2 7B.

--- TRANG 20 ---
[Bảng kết quả MMLU chi tiết theo từng lĩnh vực - đây là một bảng lớn với nhiều hàng dữ liệu so sánh kết quả của các mô hình khác nhau trên các lĩnh vực MMLU]

GPT-3 Llama2 SlimPajama-DC 1.3B
175B 7B DC-1 DC-2 DC-3 DC-4 DC-5 DC-6
Abstract Algebra STEM 30.0 29.0 27.0 26.0 25.0 27.0 28.0 23.0
Anatomy STEM 48.0 37.0 23.0 23.0 27.4 34.1 25.9 28.1
Astronomy STEM 49.0 33.6 25.0 19.7 23.0 27.0 21.7 23.7
Business Ethics Other 46.0 40.0 24.0 22.0 26.0 24.0 30.0 25.0
Clinical Knowledge Other 48.0 35.1 30.2 26.8 24.9 18.9 25.7 24.2
College Biology STEM 45.0 37.5 23.6 24.3 27.1 25.7 23.6 22.9
College Chemistry STEM 26.0 32.0 26.0 19.0 29.0 19.0 21.0 21.0
College Computer Science STEM 46.0 29.0 37.0 36.0 32.0 36.0 33.0 39.0
College Mathematics STEM 34.5 33.0 35.0 29.0 31.0 25.0 21.0 30.0
College Medicine Other 48.0 30.6 26.0 23.1 26.0 27.8 26.6 22.0
College Physics STEM 28.0 26.5 24.5 24.5 21.6 22.6 24.5 14.7
Computer Security STEM 57.0 45.0 24.0 30.0 19.0 27.0 28.0 34.0
Conceptual Physics STEM 36.5 36.6 27.7 30.2 22.1 28.5 23.8 28.9
Econometrics Social Science 33.0 23.7 24.6 25.4 30.7 23.7 24.6 21.9
Electrical Engineering STEM 50.0 26.9 29.0 24.1 26.2 29.0 23.5 28.3
Elementary Mathematics STEM 30.0 24.3 26.2 25.9 27.5 25.1 25.9 24.9
Formal Logic Humanities 29.0 27.0 35.7 24.6 20.6 16.7 15.9 28.6
Global Facts Other 37.0 29.0 30.0 31.0 30.0 37.0 33.0 23.0
High School Biology STEM 48.0 34.5 25.8 26.5 25.5 24.8 24.8 29.0
High School Chemistry STEM 33.0 28.1 27.6 19.7 27.1 27.1 24.1 16.7
High School Computer Science STEM 39.0 31.0 29.0 26.0 26.0 27.0 25.0 28.0
High School European History Humanities 54.0 44.2 23.6 28.5 24.9 26.7 25.5 26.7
High School Geography Social Science 58.0 34.3 34.3 20.7 19.2 17.7 22.2 21.2
High School Government And Politics Social Science 58.0 44.6 35.2 16.6 25.9 21.8 21.8 32.6
High School Macroeconomics Social Science 40.5 35.4 34.4 25.9 22.8 24.6 23.8 36.2
High School Mathematics STEM 28.0 24.8 26.7 25.2 28.5 26.7 25.2 29.6
High School Microeconomics Social Science 42.0 31.9 23.5 23.1 25.2 21.4 25.2 30.7
High School Physics STEM 28.0 26.5 27.8 26.5 27.2 29.8 21.9 25.8
High School Psychology Social Science 61.0 47.3 32.3 23.1 22.9 23.7 23.8 22.9
High School Statistics STEM 30.5 35.2 21.3 21.3 22.2 23.2 19.9 45.4
High School Us History Humanities 53.0 39.7 24.5 21.6 24.5 27.5 24.5 25.5
High School World History Humanities 56.0 40.9 29.1 25.7 27.4 25.7 24.5 24.9
Human Aging Other 50.0 40.8 14.8 30.5 30.5 27.4 37.2 24.7
Human Sexuality Social Science 54.0 36.6 28.2 22.1 22.1 25.2 22.9 22.1
International Law Humanities 55.5 51.2 26.5 30.6 32.2 30.6 39.7 26.4
Jurisprudence Humanities 55.0 38.9 26.9 22.2 27.8 25.0 26.9 24.1
Logical Fallacies Humanities 48.0 39.3 19.6 27.0 23.9 27.6 29.5 24.5
Machine Learning STEM 31.0 23.2 17.9 33.0 28.6 30.4 23.2 25.9
Management Other 56.0 35.0 26.2 29.1 21.4 23.3 27.2 22.3
Marketing Other 60.0 46.6 22.2 24.4 25.2 28.2 23.9 27.4
Medical Genetics Other 40.0 43.0 27.0 24.0 22.0 23.0 24.0 33.0
Miscellaneous Other 60.0 42.4 22.5 27.5 29.3 26.2 27.6 24.6
Moral Disputes Humanities 44.5 40.2 29.5 25.7 24.9 24.0 24.9 26.9
Moral Scenarios Humanities 26.0 24.3 27.3 24.6 23.8 24.6 24.3 24.2
Nutrition Other 47.0 37.6 28.1 23.2 25.8 25.8 25.2 25.5
Philosophy Humanities 51.0 39.9 28.0 28.9 29.3 28.3 26.7 30.2
Prehistory Humanities 53.0 36.1 26.5 25.9 26.9 27.5 29.3 27.2
Professional Accounting Other 33.0 25.9 27.0 29.1 27.3 27.0 27.0 22.3
Professional Law Humanities 34.5 30.2 27.1 25.0 24.6 26.9 25.8 26.9
Professional Medicine Other 36.0 44.5 19.9 31.6 21.0 27.9 22.8 44.5
Professional Psychology Social Science 44.5 35.1 26.3 27.3 25.2 27.5 25.5 25.8
Public Relations Social Science 48.0 40.9 33.6 30.9 29.1 26.4 28.2 20.0
Security Studies Social Science 52.0 31.8 39.2 17.5 21.2 16.3 18.8 37.6
Sociology Social Science 53.0 46.8 25.4 24.4 24.9 23.9 22.9 21.4
Us Foreign Policy Social Science 69.0 46.0 27.0 31.0 25.0 28.0 24.0 23.0
Virology Other 46.0 30.1 21.7 30.1 27.1 28.3 31.3 29.5
World Religions Humanities 55.0 50.9 27.5 25.2 29.8 32.2 32.8 35.1
Humanities 40.6 34.0 27.1 25.8 26.2 26.4 26.9 27.0
STEM 36.7 30.5 26.5 25.8 26.1 27.1 24.4 27.3
Social Science 50.5 38.3 30.3 24.0 24.5 23.3 23.6 26.8
Other 49.0 38.1 24.6 27.1 25.9 26.5 27.8 26.8
All 43.9 35.1 27.0 25.7 25.7 26.0 25.6 26.9
Bảng 9: MMLU. Kết quả 5-shot theo lĩnh vực trên các tập test.

--- TRANG 21 ---
C Ứng dụng: Huấn luyện Kích thước Batch Lớn trên 7B

C.1 Kết hợp Dữ liệu Huấn luyện 7B
Bộ dữ liệu huấn luyện kích thước batch lớn (LBS) 7B của chúng tôi chủ yếu dựa trên Slimpajama, tuy nhiên, để có được tỷ lệ đủ văn bản web, chúng tôi đã kết hợp thêm dữ liệu web từ corpus Commoncrawl trong RedPajama. Chúng tôi cũng đã điều chỉnh tỷ lệ của các nguồn dữ liệu khác nhau phù hợp với huấn luyện mô hình 1.3B của chúng tôi. Ví dụ, chúng tôi nâng cao tần suất lấy mẫu của Github và Wikipedia và tăng tính đa dạng của nguồn dữ liệu bằng cách thêm S2orc [26] và Stack-Markdown [22] theo [39], như được chi tiết trong Bảng 10. Điều quan trọng cần hiểu là trọng tâm chính của chúng tôi không chỉ đơn thuần là đạt được hiệu suất tốt nhất. Thay vào đó, chúng tôi đặt trọng tâm cao hơn vào việc tối ưu hóa các kết hợp dữ liệu và đảm bảo sự hội tụ của việc huấn luyện các mô hình ngôn ngữ lớn với kích thước batch lớn. Do đó, chúng tôi tiếp tục sử dụng SlimPajama/RedPajama Commoncrawl thay vì RefinedWeb chất lượng cao hơn.

bộ dữ liệu tỷ lệ
Slimpj.Arxiv 4% (54B)
Slimpj.StackExchanges 3.2% (43B)
Slimpj.Github 4.9% (66B)
Slimpj.Wikipedia 7.5% (101B)
Slimpj.Books 4.3% (57B)
Slimpj.C4 17.6% (236B)
S2orc 3% (40B)
Markdown 3% (40B)
Slimpj.CC 34.5% (462B)
Redpaj.CC (ext.) 18% (241B)
Tổng 1.34T
Bảng 10: Kết hợp dữ liệu của huấn luyện mô hình 7B theo kiểu kích thước batch lớn.

C.2 Cấu hình Huấn luyện Mô hình 7B

Mô hình n_params n_layers d_model n_heads d_heads batch size learning rate
GPT-3 6.7B 32 4,096 32 128 2M 1.2×10-4
LLaMA 6.7B 32 4,096 32 128 4M 3.0×10-4
LBS của chúng tôi 6.7B 32 4,096 32 128 14.3M 1.8×10-4
Bảng 11: Kích thước mô hình chi tiết, kiến trúc và siêu tham số tối ưu hóa.

Kiến trúc. Đối với huấn luyện mô hình LBS, chúng tôi áp dụng kiến trúc MPT [39], độ dài chuỗi tối đa là 2,048. Chúng tôi sử dụng Triton [36] với Flash Attention [8] làm triển khai self-attention. Alibi được kích hoạt để làm cho mô hình linh hoạt hơn cho việc ngoại suy độ dài đầu vào. Tổng số tham số là 6.7B.

Tokenizer. Tokenizer được sử dụng cho huấn luyện 7B được điều chỉnh từ GPT-NeoX-20b. Theo [39], kích thước từ vựng của mô hình được điều chỉnh thành 50,432 để cải thiện mfu và để lại một vài token có sẵn có thể được sử dụng trong huấn luyện tiếp theo.

Optimizer. Chúng tôi sử dụng optimizer AdamW để huấn luyện các mô hình của mình, áp dụng các siêu tham số cụ thể này: β1 đặt ở 0.9 và β2 ở 0.95. Chúng tôi áp dụng một lịch trình learning rate theo mẫu cosine, kết thúc với learning rate là 10% giá trị tối đa của nó. Cùng với điều này, chúng tôi sử dụng bộ lập lịch weight decay đa giai đoạn như được mô tả trong Phần C.4, giới hạn gradient với giá trị cắt 1.0 và sử dụng warmup kéo dài 2,000 bước.

Hệ thống và nền tảng. Đối với huấn luyện mô hình 7B của chúng tôi với kích thước batch lớn, chúng tôi sử dụng 232 GPU NVIDIA A100 (80G). Chúng tôi sử dụng llm-foundry [38] làm nền tảng huấn luyện. Chúng tôi sử dụng FSDP với activation checkpointing được kích hoạt để tiết kiệm tiêu thụ bộ nhớ. Chúng tôi cũng sử dụng automatic mixed precision của bf16 trong huấn luyện.

C.3 Huấn luyện Nhanh với Kích thước Batch Lớn
Huấn luyện batch lớn cho phép learning rate lớn hơn, dẫn đến sự hội tụ nhanh hơn của các mô hình lớn. Ngoài ra, việc sử dụng kích thước batch lớn hơn có thể tối ưu hóa việc sử dụng tài nguyên phần cứng để làm cho các quy trình huấn luyện hiệu quả hơn. Ngoài ra, ít batch hơn được yêu cầu, điều này càng đẩy nhanh quá trình huấn luyện. Như được hiển thị trong Bảng 12, sơ đồ huấn luyện batch lớn của chúng tôi đạt được throughput và mfu cao hơn nhiều so với LLaMA [37] và MPT [39] với ít tổng giờ GPU huấn luyện hơn.

Tổng thể, trong một framework tối ưu hóa lồi, việc tận dụng một phần lớn hơn của bộ dữ liệu thường dẫn đến kết quả được nâng cao. Tuy nhiên, đối với hầu hết các mô hình sâu lớn liên quan đến tối ưu hóa không lồi, bản chất chính xác của cảnh quan loss vẫn khó nắm bắt, làm cho tình huống phức tạp hơn. Nhiều công trình trước [18, 20] đã nhận thấy rằng huấn luyện với batch lớn hơn thường dẫn đến overfitting so với những cái sử dụng kích thước batch nhỏ hơn cho cùng một mạng. Khi sử dụng huấn luyện batch lớn, có xu hướng cho mô hình bị kẹt hoặc thậm chí hướng về các điểm yên ngựa tiềm năng trong cảnh quan loss. Trong khi các phương pháp huấn luyện batch lớn thường tập trung vào các minima tương đối gần nhất mà chúng gặp phải, các mạng được huấn luyện với batch nhỏ hơn thường điều hướng cảnh quan loss kỹ lưỡng hơn trước khi cam kết với một minimum tối ưu. Các minima đạt được thông qua huấn luyện batch lớn có thể khác biệt rõ rệt so với những cái đạt được với các phương pháp huấn luyện batch nhỏ hơn. Trong phần tiếp theo, chúng tôi giới thiệu một phương pháp để giảm thiểu overfitting khi huấn luyện các mô hình ngôn ngữ lớn trong sơ đồ kích thước batch lớn.

mô hình batch size # GPUs (A100-80G) throughput mfu GPU-hours
LLaMA-7B 4M – – – 82,432
MPT-7B 4M 232 3,310 0.4575 84.351
LBS-7B (của chúng tôi) 14M 232 3,626 0.5011 76,999
Bảng 12: Tốc độ huấn luyện của throughput (token mỗi giây trên mỗi GPU), model FLOPs utilization (mfu) [5] và tổng GPU-hours (mỗi nghìn tỷ token huấn luyện).

C.4 Huấn luyện Tiến triển trên Weight Decay

[Biểu đồ đường cong loss của LBS-7B training với các giai đoạn WD khác nhau]

Hình 4: Đường cong loss của huấn luyện LBS-7B của chúng tôi.

Công trình trước [25] quan sát thấy rằng hoạt động dropout chỉ được sử dụng trong giai đoạn đầu của huấn luyện và được vô hiệu hóa trong các giai đoạn tiếp theo. Các mô hình kết hợp chiến lược dropout sớm này có xu hướng thể hiện loss huấn luyện cuối cùng giảm so với các mô hình không sử dụng dropout. Trái ngược với điều này, phương pháp của chúng tôi nhấn mạnh vai trò của weight decay trong quá trình huấn luyện mô hình lớn. Chúng tôi giới thiệu một chiến lược huấn luyện mới cho các mô hình ngôn ngữ lớn, trong đó quá trình huấn luyện được phân thành các giai đoạn khác nhau. Trong mỗi giai đoạn, một weight decay riêng biệt được áp dụng cho mô hình để phục vụ các mục tiêu cụ thể. Chúng tôi đã gọi phương pháp này là Progressive Training on Weight Decay (PTWD). Nhờ phương pháp này, mô hình của chúng tôi, ngay cả khi được huấn luyện với kích thước batch lớn và số lần lặp cực nhỏ, đạt được sự hội tụ mượt mà. Như được minh họa trong Hình 4, chiến lược huấn luyện của chúng tôi bao gồm ba giai đoạn riêng biệt. Ban đầu, chúng tôi phủ định weight decay bằng cách đặt nó về không và cho phép mô hình huấn luyện cho đến khi đạt được sự hội tụ đầy đủ. Nó thường có thể đạt được mức loss thấp hơn trong giai đoạn này so với việc sử dụng weight decay, ngay cả khi nó hơi overfit. Tiếp theo, trong giai đoạn thứ hai, chúng tôi giới thiệu một weight decay đáng kể, với giá trị 0.5 trong các thí nghiệm của chúng tôi, để ngăn chặn overfitting. Một khi các giá trị loss ổn định, chúng tôi chuyển sang giai đoạn thứ ba, trong đó weight decay tiêu chuẩn 0.1 được triển khai, một giá trị nhất quán với nhiều huấn luyện LLM khác. Thú vị, mỗi giai đoạn tự nhiên hội tụ đến khoảng 1/3 tổng ngân sách huấn luyện, đảm bảo phân bổ hiệu quả ngân sách huấn luyện trong suốt quá trình.

C.5 Kết quả Pre-training và Instruction Tuning
Kết quả từ pre-training và instruction tuning tiếp theo của chúng tôi trên bộ dữ liệu ShareGPT được trình bày trong Bảng 13. Đáng chú ý, sau instruction tuning, có sự nâng cao đáng kể trong các metric MMLU và TruthfulQA. Ngược lại, hiệu suất trên ARC và HellaSwag có giảm nhẹ. Nhìn chung, độ chính xác trung bình có sự tăng lên đáng kể sau instruction tuning. Thêm kết quả đánh giá trên mô hình LBS được huấn luyện trước được cung cấp trong Bảng 6.

Mô hình Trung bình ARC HellaSwag MMLU TruthfulQA
Ours-LBS-7B-Base 44.1 44.3 69.8 26.1 36.1
Ours-LBS-7B-Instruct 46.4 43.5 68.0 32.1 42.1
Bảng 13: Kết quả của các mô hình 7B được huấn luyện kích thước batch lớn (LBS) của chúng tôi theo Đánh giá Bảng xếp hạng Huggingface [12] sử dụng Harness [14].
