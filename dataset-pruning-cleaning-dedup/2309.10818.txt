# 2309.10818.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/dataset-pruning-cleaning-dedup/2309.10818.pdf
# File size: 1247505 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SlimPajama-DC : Understanding Data
Combinations for LLM Training
Zhiqiang Shen†Tianhua Tao†,‡Liqun Ma†Willie Neiswanger§
Zhengzhong Liu†Hongyi Wang♮Bowen Tan♮Joel Hestness♯
Natalia Vassilieva♯Daria Soboleva♯Eric Xing†
†MBZUAI‡UIUC§Stanford University♮CMU♯Cerebras Systems
Abstract
This paper aims to understand the impacts of various data combina-
tions (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large
language models using SlimPajama. SlimPajama is a rigorously dedupli-
cated, multi-source dataset, which has been refined and further dedupli-
cated to 627B tokens from the extensive 1.2T token RedPajama dataset
contributed by Together. We have termed our research as SlimPajama-
DC, an empirical analysis designed to uncover fundamental characteristics
and best practices associated with employing SlimPajama in the training of
large language models. During our research with SlimPajama, two pivotal
observations emerged: (1)Global deduplication vs. local deduplication.
We analyze and discuss how global (across different sources of datasets)
and local (within the single source of dataset) deduplications affect the
performance of trained models. (2)Proportions of highly-deduplicated
multi-source datasets in the combination. To study this, we construct six
configurations on SlimPajama dataset and train individual ones using 1.3B
Cerebras-GPT model with Alibi and SwiGLU. Our best configuration out-
performs the 1.3B model trained on RedPajama using the same number of
training tokens by a significant margin. All our 1.3B models are trained on
Cerebras 16 ×CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed preci-
sion. We further extend our discoveries (such as increasing data diversity is
crucial after global deduplication ) on a 7B model with large batch-size train-
ing. Our SlimPajama-DC models are available at: link1 and the separate
SlimPajama-DC datasets are available at: link2.
1arXiv:2309.10818v3  [cs.CL]  9 May 2024

--- PAGE 2 ---
Contents
1 Introduction 3
2 Dataset Overview 4
2.1 Number of Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Dataset Token Frequency Statistics . . . . . . . . . . . . . . . . . 5
2.3 Dataset Processing Procedure . . . . . . . . . . . . . . . . . . . . 5
2.3.1 Low-length Document Filtering . . . . . . . . . . . . . . 6
2.3.2 Global Deduplication . . . . . . . . . . . . . . . . . . . . 7
3 Dataset Combination Configurations 8
3.1 SlimPajama . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 RefinedWeb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Network Architecture and Training Details 8
4.1 Network Architecture . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Results and Analysis 10
5.1 Huggingface Leaderboard Evaluation with Harness . . . . . . . 10
5.2 More Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.3 Training Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
6 Related Work 13
6.1 RedPajama, SlimPajama and Others. . . . . . . . . . . . . . . . . 13
6.2 Data Processing and Optimization Approaches . . . . . . . . . . 13
6.3 Data Combination for Training Large Language Models . . . . . 14
6.4 Large Batch Training for Large Language Models . . . . . . . . . 14
7 Conclusion 15
A Data Proportion Details 19
B MMLU 19
C Application: Large Batch-size Training on 7B 21
C.1 7B Training Data Combination . . . . . . . . . . . . . . . . . . . 21
C.2 7B Model Training Configurations . . . . . . . . . . . . . . . . . 21
C.3 Fast Training with Large Batch-size . . . . . . . . . . . . . . . . . 22
C.4 Progressive Training on Weight Decay . . . . . . . . . . . . . . . 23
C.5 Results of Pre-training and Instruction Tuning . . . . . . . . . . 23
2

--- PAGE 3 ---
1 Introduction
The success of modern large-scale models is deeply rooted in their training
data. For large language models (LLMs), the emphasis is not merely on generic
text but on “diverse text”. To guarantee the model’s linguistic expertise and its
comprehensive understanding of the world, this text must span a broad spec-
trum of domains, genres, languages, and more. Consequently, the composition
of the pretraining data domains, such as Github, Wikipedia, books, and web
text like CommonCrawl, plays a critical role in the performance of large lan-
guage models. In our research, we delve into the domain/source weightings of
training data. Leveraging SlimPajama-DC , we investigate two primary areas:
(1) global-level and local-level deduplication, and (2) the efficacy of various
combinations of thoroughly deduplicated datasets. The first emphasis basi-
cally encourages the model to be trained on all sources as no cross-domain
overlaps inside, and the second helps us understand how to manage the in-
tegration and proportions of diverse domains, especially as datasets for LLM
training continue to expand in variety.
Generic Deduplication. Multi-source datasets often combine data from var-
ious origins, each with its unique distribution of information. When train-
ing large language models, handling data redundancy is critical to ensure that
the model generalizes well and does not exhibit undue biases, making train-
ing faster and more efficient. Highly deduplicated datasets ensure that the
model isn’t repeatedly exposed to the same or very similar data points, mak-
ing the training more efficient. Redundant data can slow down convergence
and might make the model overfit to frequently seen patterns. Deduplication
helps in efficient utilization of the model’s capacity. In general, deduplication
is the process of removing duplicate data to address this redundancy.
Global Deduplication vs.Local Deduplication. The global deduplication pro-
cess removes duplicates from the entire combined datasets. When we’re using
data from multiple sources, there might be overlaps across sources. Global
deduplication identifies and removes these overlapping instances irrespective
of their source. In local deduplication, duplicates are removed within each in-
dividual source dataset before merging them. However, if two source datasets
have overlapping data, those duplicates will still be present in the final com-
bined dataset since deduplication was only done locally within each dataset.
In most current open-source LLM training data [7, 37, 39], only local dedupli-
cation is performed within each data source, which neglects the redundancy
across the different sources. Given the effects, global deduplication performed
in SlimPajama is generally preferable for training large language models, es-
pecially when using multi-source datasets. It ensures a balanced representa-
tion of information and prevents the pitfalls associated with data redundancy.
However, more hardware memory is naturally required by this strategy.
Different Combinations of Highly-deduplicated Datasets. A model trained
on diverse data is more likely to generalize well across various tasks. It’s ex-
3

--- PAGE 4 ---
posed to a wider range of vocabulary, syntax, and semantics, enabling it to
handle a broad scope of queries. If diverse sources are chosen such that they
represent different cultures, beliefs, and demographics, the model might be
more balanced and less prone to biases. However, if many sources share com-
mon biases, the final dataset might amplify them. Different sources can pro-
vide both a breadth and depth of knowledge on various topics. Combining a
technical dataset with a general news dataset, for example, would allow the
model to understand both in-depth technical details and broad general knowl-
edge. It’s crucial to note that data quality often outweighs the quantity. In this
work, we aim to shed light on this fascinating perspective of comprehensive
data combination on SlimPajama.
Specialization vs. Generalization Trade-off. In general, combining many spe-
cialized datasets can lead to a jack-of-all-trades model, which might not be as
adept at specific tasks as a model trained on a specialized dataset. While the
model can tackle a wide range of tasks, it might not have the depth of un-
derstanding that a specialized model might have for a particular domain. In
this study, we also explore specialization and generalization ability using both
individual and combined data sources.
The remainder of this paper is organized as follows. In Section 2, we elabo-
rate the details of dataset statistics, token distributions, and data processing
procedure. Section 3 describes dataset combination configurations for this
SlimPajama-DC study. Our model architecture and training details are pro-
vided in Section 4, followed by the results and analysis in Section 5 on the
range of various tasks in the zero- and few-shot settings. Section C presents an
application of efficient Large Batch-size (LBS) training on a 7B model. Section 6
reviews related work and Section 7 concludes this study.
2 Dataset Overview
2.1 Number of Tokens
SlimPajama has a total of 627B tokens across different domains, as shown in Ta-
ble 1. It includes validation and test sets with 500M tokens each, and these have
been cleaned to ensure no overlap with the training data. For the SlimPajama-
DCstudy, our entire training dataset for each configuration contains 330B to-
kens after tokenization which is carefully selected from the original SlimPa-
jama dataset. We tested different sampling strategies for different domains of
our training data: (1) each token is trained only once during training, such as
Commoncrawl, and (2) we perform more than one epoch for training on partic-
ular sources, such as the Wikipedia and Github domains. The detailed domain
source proportions of various combinations are shown in Table 3.
4

--- PAGE 5 ---
Dataset SlimPaj. RedPaj. LLaMA-1 RefinedWeb GPT3 MassiveText
Commoncrawl 52.2% 72.6% 67.0% 100% 60.0% 0.0%
C4 26.7% 14.4% 15.0% 0.0% 0.0% 10.0%
GitHub 5.2% 4.9% 4.5% 0.0% 0.0% 3.0%
Books 4.2% 2.1% 4.5% 0.0% 16.0% 27.0%
ArXiv 4.6% 2.3% 2.5% 0.0% 0.0% 0.0%
Wikipedia 3.8% 2.0% 4.5% 0.0% 3.0% 2.0%
StackExchange 3.3% 1.7% 2.0% 0.0% 0.0% 0.0%
WebText2 0.0% 0.0% 0.0% 0.0% 22.0% 0.0%
MassiveWeb 0.0% 0.0% 0.0% 0.0% 0.0% 48.0%
News 0.0% 0.0% 0.0% 0.0% 0.0% 10.0%
Total tokens 637B 1.2T 1.0/1.4T 600B 300B 300B
Table 1: Data source proportions for various datasets.
2.2 Dataset Token Frequency Statistics
To examine the similarity between various datasets in SlimPajama, we cal-
culate the KL divergence between two domain distributions of token counts
from different domain datasets, as shown in Fig. 1a. Considering that differ-
ent datasets often highlight varied types of tokens, such as GitHub prioritiz-
ing code and arXiv centering on academic content, we further investigate how
these datasets vary in their distribution across subsets of tokens that exhibit
unique features: (1) Tokens exclusively comprising letters (Fig. 1b); (2) The
union set of tokens with the top 1000 frequencies on each dataset (Fig. 1c);
(3) Numbers and commonly used operators, like ‘30’, ‘+’ and ‘=’ (Fig. 1d); (4)
Whitespace Tokens, like ‘ \n\n’ and ‘ \t’ (Fig. 1e); (5) Non-alphanumeric tokens,
like ‘#’ and ‘====’ (Fig. 1f).
There exists a degree of similarity in the distribution of different token sub-
sets among RefinedWeb, Book, C4, and CommonCrawl, as well as between
Github and StackExchange. Notably, when it comes to the distribution of non-
alphanumeric tokens, Arxiv differs significantly from most datasets. While on
the distribution of whitespace tokens, Refinedweb shows notable distinctions
in comparison to Github and StackExchange. Among numbers and commonly
used operators, the distribution of all datasets is relatively consistent.
2.3 Dataset Processing Procedure
SlimPajama is constructed by filtering low-length documents and applying
MinHashLSH deduplication to the 1.2T token RedPajama dataset to reduce
it to 627B tokens. RefinedWeb [28] shows that training on deduplicated data
improves training compute efficiency and decreases the chance of LLMs gener-
ating memorized text from the dataset. By removing duplicate and low-length
examples, it significantly improves the training compute efficiency and model
performance. The overview of SlimPajama preprocessing pipeline is shown in
Fig. 2 and the preprocessing code is available on GitHub.
5

--- PAGE 6 ---
Slimpj.
CommonCrawlSlimpj.C4
RefinedWeb Slimpj.Book Slimpj.
StackExchangeSlimpj.Github
Slimpj.WikipediaSlimpj.ArXivSlimpj.
CommonCrawl
Slimpj.C4
RefinedWeb
Slimpj.Book
Slimpj.
StackExchange
Slimpj.Github
Slimpj.Wikipedia
Slimpj.ArXiv0.00 0.08 0.07 0.21 1.15 1.89 1.14 1.48
0.08 0.00 0.04 0.23 1.09 1.96 1.41 1.53
0.05 0.03 0.00 0.21 1.00 1.79 1.25 1.43
0.25 0.30 0.28 0.00 1.16 1.91 1.22 1.66
1.58 1.83 1.69 1.39 0.00 0.41 2.28 1.33
2.83 3.23 3.17 2.47 0.56 0.00 3.20 2.05
2.13 2.09 2.17 1.71 2.12 2.52 0.00 3.06
2.54 3.40 2.69 2.53 1.10 1.84 3.59 0.00
0246810(a) All Tokens
Slimpj.
CommonCrawlSlimpj.C4
RefinedWeb Slimpj.Book Slimpj.
StackExchangeSlimpj.Github
Slimpj.WikipediaSlimpj.ArXivSlimpj.
CommonCrawl
Slimpj.C4
RefinedWeb
Slimpj.Book
Slimpj.
StackExchange
Slimpj.Github
Slimpj.Wikipedia
Slimpj.ArXiv0.00 0.08 0.05 0.19 1.09 1.80 1.02 1.35
0.08 0.00 0.02 0.20 0.96 1.81 1.29 1.36
0.05 0.03 0.00 0.18 0.92 1.71 1.17 1.35
0.22 0.24 0.20 0.00 1.11 1.90 1.10 1.51
1.34 1.33 1.17 1.32 0.00 0.48 2.20 1.33
2.53 2.63 2.34 2.41 0.40 0.00 3.00 2.08
1.38 1.52 1.42 0.99 1.95 1.98 0.00 2.15
2.05 2.41 2.10 2.02 1.02 1.65 2.82 0.00
0246810 (b) Tokens Composed of Letters
Slimpj.
CommonCrawlSlimpj.C4
RefinedWeb Slimpj.Book Slimpj.
StackExchangeSlimpj.Github
Slimpj.WikipediaSlimpj.ArXivSlimpj.
CommonCrawl
Slimpj.C4
RefinedWeb
Slimpj.Book
Slimpj.
StackExchange
Slimpj.Github
Slimpj.Wikipedia
Slimpj.ArXiv0.00 0.05 0.06 0.11 0.70 1.58 0.92 1.09
0.05 0.00 0.05 0.17 0.74 1.74 1.15 1.23
0.03 0.02 0.00 0.13 0.62 1.53 0.98 1.06
0.18 0.26 0.25 0.00 0.77 1.61 0.97 1.35
1.40 1.86 1.76 1.07 0.00 0.36 1.66 0.98
2.91 3.59 3.74 2.30 0.69 0.00 2.71 1.65
2.10 2.04 2.12 1.77 1.65 2.21 0.00 2.67
2.53 3.75 2.68 2.51 0.89 1.48 3.30 0.00
0246810 (c) Top 1000 Tokens
Slimpj.
CommonCrawlSlimpj.C4
RefinedWeb Slimpj.Book Slimpj.
StackExchangeSlimpj.Github
Slimpj.WikipediaSlimpj.ArXivSlimpj.
CommonCrawl
Slimpj.C4
RefinedWeb
Slimpj.Book
Slimpj.
StackExchange
Slimpj.Github
Slimpj.Wikipedia
Slimpj.ArXiv0.00 0.08 0.03 0.19 0.48 0.78 0.11 0.68
0.07 0.00 0.04 0.08 0.51 0.73 0.21 0.79
0.03 0.04 0.00 0.13 0.43 0.69 0.16 0.64
0.13 0.07 0.10 0.00 0.53 0.72 0.24 0.81
0.72 0.91 0.65 0.78 0.00 0.13 0.98 0.31
1.33 1.52 1.14 1.19 0.13 0.00 1.61 0.72
0.14 0.30 0.23 0.30 0.94 1.28 0.00 1.00
0.92 1.19 0.97 1.19 0.29 0.65 1.23 0.00
0246810
(d) Numbers and Commonly
Used Operators
Slimpj.
CommonCrawlSlimpj.C4
RefinedWeb Slimpj.Book Slimpj.
StackExchangeSlimpj.Github
Slimpj.WikipediaSlimpj.ArXivSlimpj.
CommonCrawl
Slimpj.C4
RefinedWeb
Slimpj.Book
Slimpj.
StackExchange
Slimpj.Github
Slimpj.Wikipedia
Slimpj.ArXiv0.00 0.29 2.45 0.53 0.91 1.58 0.49 0.10
0.25 0.00 0.66 1.49 1.84 2.97 0.19 0.38
0.77 0.19 0.00 2.58 2.83 4.28 0.45 0.98
0.37 0.96 3.74 0.00 0.58 0.87 0.95 0.18
1.04 1.86 7.25 0.71 0.00 0.07 2.69 0.77
1.37 2.28 8.28 1.02 0.06 0.00 3.24 1.11
0.25 0.20 0.71 0.96 1.63 2.54 0.00 0.26
0.11 0.40 2.51 0.29 0.72 1.31 0.47 0.00
0246810(e) Whitespace Tokens
Slimpj.
CommonCrawlSlimpj.C4
RefinedWeb Slimpj.Book Slimpj.
StackExchangeSlimpj.Github
Slimpj.WikipediaSlimpj.ArXivSlimpj.
CommonCrawl
Slimpj.C4
RefinedWeb
Slimpj.Book
Slimpj.
StackExchange
Slimpj.Github
Slimpj.Wikipedia
Slimpj.ArXiv0.00 0.08 0.08 0.20 0.86 1.21 0.70 1.73
0.07 0.00 0.06 0.21 0.97 1.32 0.73 1.77
0.07 0.08 0.00 0.30 0.86 1.23 0.77 1.61
0.30 0.37 0.49 0.00 1.06 1.36 0.93 2.16
1.87 2.63 2.60 1.57 0.00 0.20 2.46 1.72
2.70 3.53 3.61 2.18 0.18 0.00 3.23 2.50
2.13 1.93 2.14 1.83 2.01 2.71 0.00 3.72
3.84 6.10 4.01 3.95 1.36 2.48 5.56 0.00
0246810 (f) Non-Alphanumeric Tokens
Figure 1: Confusion matrix using KL divergence between the distributions of
token statistics for different datasets.
2.3.1 Low-length Document Filtering
In the dataset processing procedure, an additional global filtering is performed
to remove short, low-quality documents. After removing punctuation, consec-
utive spaces, newlines, tabs, and leading or trailing escape characters, docu-
ments with less than 200 characters are further filtered out. These documents
typically contain only metadata and no useful information. A low-length filter
is applied to every corpora other than Books and GitHub where it is found use-
ful for short documents. The percentage of documents filtered out from each
corpus within SlimPajama is detailed in Table 2. In total, this additional step
removes 1.86% of the documents.
Data source Document filter rate Byte duplication rate
Commoncrawl 0.02% 63.76%
C4 4.7% 6.85%
GitHub 0.0% 46.16%
Books 0.0% 2.01%
ArXiv 0.62% 0.06%
Wikipedia 0.0% 2.24%
StackExchange 0.32% 0.20%
Total 1.86% 49.60%
Table 2: Percentage of document low-length filter rates and data source byte
duplication rates.
6

--- PAGE 7 ---
C4 NFC
NFCNFC
NFCBooks
ArxivGithubClean
Clean
Clean
Clean
………Global
DeduplicationInterleave Docs Shuffle Docs Train/HoldoutDedup
Train/Holdou t
TokenizeTrain
Holdout
Sequence
PackingTest/EvalDedup
Test/Eva lTest
EvalUpsample /
Downsample
with weightsSequence
Packin gShuffle
Sequence sTrainFigure 2: SlimPajama preprocessing pipeline.
2.3.2 Global Deduplication
When building SlimPajama, it is observed that every corpus included in it
contained duplicates with the most significant duplication found in Common-
Crawl and GitHub. RefinedWeb [28] also found similar rates of deduplica-
tion in the CommonCrawl data. It is most common to perform deduplication
within each dataset source separately [37, 7, 44, 13] to reduce implementation
complexity and meet resource constraints. This local deduplication approach
does not have the ability to remove overlap between data sources which can
be significant for web-scraped data. Instead, global deduplication removes du-
plication within and between each data source. Following [4, 28, 1, 32], global-
level deduplication is performed using MinHashLSH algorithm. To facilitate
global deduplication efforts and reproducibility for other researchers, a tool
designed for scalable performance is offered under the above link.
Specifically, global MinHashLSH deduplication is performed using a Jaccard
similarity threshold of 0.8, document signatures constructed with preprocessed
lowercase 13-grams, and schema following [23]. To unify a representation of
the same content, punctuation, consecutive spaces, newlines, tabs, and lead-
ing or trailing escape characters are removed. The level of deduplication per-
formed per data source is presented in Table 2. The vanilla implementation of
MinHashLSH did not scale to trillion token datasets like RedPajama without
running out of memory. This is overcome by optimizing the memory usage
and parallelization to perform deduplication on 64 CPU cores with 1.4TB peak
memory, which can be easily decreased by creating multiple MinHashLSH ob-
jects to query.
7

--- PAGE 8 ---
3 Dataset Combination Configurations
3.1 SlimPajama
Combination Strategies. As shown in Table 3, the adjusted domain weights
establish a new training distribution. Using this distribution, we adopt a stan-
dard training approach to learn a consistent model architecture. This archi-
tecture remains unchanged across various domain weights and is trained us-
ing data from diverse combination distributions. Across different setups, we
maintain the total training tokens to be the same. Our examination of domain
weights in large language model training focuses on three main areas: 1) In-
crementally increasing the diversity of source combinations, as seen in config-
urations 1, 2, and 5, 6. 2) With consistent data sources, we explore varying
domain proportions as in configurations 2 and 3. 3) We assess the significance
of individual domain sources concerning the final model’s performance as in
configurations 3 and 4. Note that considering minimal impact of ArXiv and
StackExchange, we have chosen to omit them in configuration 5 to preserve
training resources and keep relatively sufficient training tokens for Common-
Crawl. 4) Our final whole combination is applied in configuration 6 with the
largest diversity.
3.2 RefinedWeb
RefinedWeb [28] is a massive English web dataset that is constructed using
rigorous filtering and extensive deduplication of CommonCrawl. We use it as
the comparison to our SlimPajama-DC CommonCrawl-only training.
Sub dataset DC-1 DC-2 DC-3 DC-4 DC-5 DC-6 DC-7
SlimPajamaCommoncrawl 100.0% 90.9% 75.8% 75.8% 75.8% 52.2% 0.0%
C4 0.0% 0.0% 0.0% 0.0% 0.0% 26.7% 0.0%
GitHub 0.0% 9.1% 24.2% 0.0% 9.1% 5.2% 0.0%
Books 0.0% 0.0% 0.0% 0.0% 7.9% 4.2% 0.0%
ArXiv 0.0% 0.0% 0.0% 0.0% 0.0% 4.6% 0.0%
Wikipedia 0.0% 0.0% 0.0% 24.2% 7.3% 3.8% 0.0%
StackExchange 0.0% 0.0% 0.0% 0.0% 0.0% 3.3% 0.0%
RefinedWeb Commoncrawl 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 100.0%
Total (Tokens) 330B 330B 330B 330B 330B 330B 330B
Table 3: Seven configurations of sub-dataset combinations in SlimPajama.
4 Network Architecture and Training Details
4.1 Network Architecture
Cerebras-GPT Architecture [11]. Cerebras-GPT architecture shares similari-
ties with those built on GPT-3 [4], particularly in the use of an autoregressive
transformer decoder. However, a key difference lies in the attention mecha-
8

--- PAGE 9 ---
nism employed. While GPT-3 utilizes a mix of dense and sparse-banded atten-
tion, Cerebras-GPT consistently uses dense attention across all decoder blocks.
In terms of model dimensions, we either adhere to an aspect ratio of approxi-
mately 80 (d model /nlayers ) or maintain dimensions that are congruent with GPT-
3 models. Additionally, all of our models are trained on a maximum sequence
length of 2,048 tokens. The detailed architecture is shown in Table 11.
Alibi [29]. Alibi introduces a more streamlined and efficient positional ap-
proach called Attention with Linear Biases . Rather than adding positional em-
beddings to word embeddings, ALiBi applies a bias to query-key attention
scores, penalizing them based on their distance.
SwiGLU [33]. SwiGLU is an activation function which is a variant of GLU [9].
The formulation is as follows:
SwiGLU( x, W, V, b, c, β ) = Swish β(xW+b)⊗(xV+c) (1)
where xis a vector of the hidden representation at a particular position in the
sequence. W, V, b, c are the matrices and bias vectors, respectively.
Model nparams nlayers dmodel nheads dheads batch size learning rate
GPT-3 XL 1.3B 24 2,048 24 128 1M 2.0×10-4
Our DC 1.3B 24 2,048 24 128 2M 1.2×10-2
Table 4: Detailed model sizes, architectures, and optimization hyper-
parameters. Our LBS model details are presented in Appendix C.
4.2 Training Details
Tokenizer. We use an adapted GPT-NeoX [2] BPE-based tokenizer similar to
that used in GPT-2 for all our experiments, which has a vocabulary size of
50,277. Our entire training dataset for each configuration contains 330B tokens
after tokenization, and each model takes about 2.5 days for training on Cere-
bras 16 ×CS-2S cluster.
Optimizer. We employ the AdamW optimizer [27] to train our models, adopt-
ing these specific hyper-parameters: β1= 0.9, β2= 0.95, and eps = 1.0e-08. Our
chosen learning rate follows a linear scheduler, culminating in a final learning
rate that’s 10% of its peak value. Additionally, we apply a weight decay of 0.1,
clip the gradient using a value of 1.0, and utilize a 150-step warmup.
Other Hyperparameters. In our model, the filter size is 5,461, hidden size is
2,048 and attention dropout rate is 0. SwiGLU is used as the nonlinearity and
alibi is used for position embedding. Mixed precision and bfloat16 are employed
during model training. More hyperparameters are shown in Table 11.
9

--- PAGE 10 ---
5 Results and Analysis
This section presents the analytical experiments and results on different com-
binations of SlimPajama. We first discuss the results following Huggingface
Leaderboard Evaluation. Then, we demonstrate the importance of global dedu-
plication and a diverse range of data sources in enhancing LLM’s performance
by conducting additional comprehensive evaluations across various topics. Fi-
nally, we visualize the training loss curves of different data domain combina-
tions and provide insights on how they connect to the models’ performance.
5.1 Huggingface Leaderboard Evaluation with Harness
Following the Huggingface Leaderboard Evaluation [12], we also assess our
models on four key benchmarks using the Eleuther Language Model Evalua-
tion Harness [14]. This unified framework facilitates the evaluation of genera-
tive language models across a broad scope of tasks. Specifically, our tests com-
prised: (1) AI2 Reasoning Challenge (25-shot) [6]; (2) HellaSwag (10-shot) [42];
(3) MMLU (5-shot) [17]; (4) TruthfulQA (0-shot) [24]. As shown in Table 5,
with the exception of DC-4, our average results are all better than RedPajama-
1.3B which is also trained on 330B tokens. Among our combinations, the DC-1
(which relies solely on SlimPajama Commoncrawl) achieves the highest scores
for ARC and MMLU among all tested configurations. Yet, its performance on
TruthfulQA ranks at the bottom. On the other hand, DC-6 obtains the top aver-
age accuracy across all SlimPajama data combinations, as well as standing out
with the best results on HellaSwag. From DC-1, 2, 5, and 6, it is clear that more
domain combinations with diverse training data bring better overall accuracy.
Moreover, a potential strategy to harness the strengths of each configuration
might involve a sequential training process on DC-1, DC-6, and DC-7.
Furthermore, SlimPajama uses global deduplication across all sources. This
suggests that merging all domains typically yields better results than selective
combinations as the elimination of overlaps among different datasets. This also
highlights the importance of global deduplication and a diverse range of data
sources for LLM overall performance.
5.2 More Evaluations
As in Table 6, we present additional evaluations across various domains to in-
vestigate the fine-grained capabilities offered by different data combinations.
Except for DC-7 (trained on RefinedWeb dataset), incorporating more sources,
such as DC-1, 2, 5 and 6, typically leads to improved average performance.
Upon analysis, we find that specific mixtures perform well in particular eval-
uation benchmarks. For example, DC-1 obtains the highest accuracy in the
ARC challenge and Race. Meanwhile, DC-4 outperforms others in the Wino-
grande and Xstory cloze, and DC-5 emerges as the top performance in the
WSC273 evaluation. Moreover, all of our configurations except for DC-4 are
10

--- PAGE 11 ---
Model Training Tokens Average ARC HellaSwag MMLU TruthfulQA
Cerebras-GPT-1.3B [11] 26.3B 33.5 26.3 38.5 26.6 42.7
OPT-1.3B [44] 180B 36.6 29.2 54.4 24.4 38.5
Pythia-1.4B [1] 300B 37.3 31.9 52.8 25.6 38.8
GPT-neo-1.3B [3] 300B 36.0 31.2 48.5 24.8 39.6
RedPajama-1.3B [7] 330B 38.0 37.2 55.8 24.9 34.3
TinyLlama-1.1B [43] 3T 39.4 33.6 60.3 25.9 37.6
Olmo-1.2B [15] 3T 39.4 34.4 63.7 26.4 33.0
Our DC-1-1.3B 330B 38.5 36.3 56.0 27.0 34.8
Our DC-2-1.3B 330B 38.4 33.9 55.5 25.7 38.6
Our DC-3-1.3B 330B 38.5 35.2 54.7 25.7 38.3
Our DC-4-1.3B 330B 37.6 33.4 53.3 26.0 37.6
Our DC-5-1.3B 330B 38.6 34.7 56.0 25.6 38.0
Our DC-6-1.3B 330B 40.0 33.7 61.0 26.9 38.4
Our DC-7-1.3B‡330B 41.0 35.1 64.7 26.2 37.9
Table 5: Results of seven dataset combination configurations following Hug-
gingface Leaderboard [12] using Harness [14].‡is the RefinedWeb CC.
superior in the average performance over the comparisons of GPT-neo-1.3B [3]
and RedPajama-1.3B [7].
Risk of random guessing score on 1.3B models. It is widely recognized that
small models, such as the 1.3B variant, may struggle to achieve satisfactory
predictions on specific benchmarks like MMLU. Their results could resem-
ble random choices, not truly capturing the model’s actual capabilities. To
more accurately showcase a model’s true potential and reflect the ability of
different data combinations, we introduce a novel metric RRGS (risk of ran-
dom guessing score) to evaluate the degree of random guessing. Since 25%
in MMLU represents the baseline score for a guess, this metric evaluates the
variance using average ℓ1distance around this base value across all sub-items.
A larger variance would suggest a reduced likelihood of predictions resulting
from mere chance. Given a MMLU score vector Xof length Nwith sub-item
scores s1, s2, . . . , s n, RRGS can be formulated as:
RRGS = 1−1
NNX
i=1(|si−0.25|) (2)
where iis the index of sub-item in MMLU and Nis the number of items of
MMLU. This metric utilizes the probabilities of variance to baseline 25%, aim-
ing to assess the extent to which a model’s prediction resembles random guess-
ing on the MMLU benchmark. The metric has three variations: (1) Consider
only items with scores exceeding 25%, i.e., i∈ {positive item set }. (2) Focus
solely on items with scores less than 25%, i.e., i∈ {negative item set }. (3) In-
clude all items and sum them up. The results are shown in Table 7. Generally,
a model with a higher MMLU average score will have a low risk of random
guessing probability.
It is also crucial to employ a broader and more diverse set of benchmarks, such
as in Table 6. Additionally, for a detailed understanding, we have cataloged the
complete MMLU results for every sub-item in Table 9. This offers a lens into
11

--- PAGE 12 ---
Eval Neo RedPaj. DC-1 DC-2 DC-3 DC-4 DC-5 DC-6 DC-7 LBS
1.3B 1.3B 7B
Humaneval (p@1) - - - - - - - - - 9.5
ARC easy 61.1 66.7 66.1 66.9 66.4 65.5 66.5 67.8 66.8 74.7
ARC challenge 25.9 33.5 36.3 33.9 35.2 33.4 34.7 33.7 35.1 44.3
Boolq 62.0 55.6 63.4 65.6 64.2 50.6 62.5 59.6 61.7 66.9
PIQA 71.1 72.4 70.8 69.2 68.6 67.8 70.7 72.9 75.7 77.4
Race 34.1 34.4 37.3 36.7 36.5 34.6 37.3 36.8 36.6 38.2
Winogrande 54.9 60.5 60.3 59.7 60.1 60.5 59.8 59.4 61.2 64.4
Openbookqa 33.6 33.0 35.6 34.8 34.0 34.4 34.0 36.0 37.4 39.8
COPA 69.0 77.0 70.0 73.0 74.0 70.0 75.0 81.0 81.0 86.0
WSC273 75.1 78.0 76.2 78.0 76.9 76.6 81.0 76.2 79.5 85.0
Swag 67.8 68.8 69.2 68.5 67.8 68.3 70.1 70.2 70.0 73.8
Pawsx* 50.6 51.5 51.4 52.3 52.2 50.5 53.1 55.2 50.8 54.7
Xstory cloze* 51.1 51.5 51.0 51.3 51.5 52.2 52.0 52.2 51.6 55.3
Average 54.7 56.9 57.3 57.5 57.3 55.4 58.1 58.4 58.9 63.4
Table 6: Results of seven dataset combination configurations of 1.3B mod-
els and our LBS-7B model. Details of LBS-7B are presented in Appendix C.
ARC easy and ARC challenge are evaluated using 25-shot. All other evalu-
ation benchmarks are tested on 0-shot. * represents the results are averaged
across multiple sub-items inside each benchmark dataset.
the knowledge assimilated by the pretrained models within each sub-domain
on this comprehensive benchmark.
DC-1 DC-2 DC-3 DC-4 DC-5 DC-6 DC-7
MMLU 0.27 0.257 0.257 0.260 0.256 0.269 0.262
RRGS pos 0.964 0.964 0.965 0.970 0.968 0.954 0.963
RRGS neg 0.974 0.973 0.974 0.969 0.975 0.972 0.973
RRGS all 0.968 0.968 0.969 0.970 0.971 0.962 0.967
Table 7: Evlauation of random guessing probability on sub-items of MMLU.
5.3 Training Loss
Fig. 3 presents the training loss curves for various data combinations, from
which several insights can be observed: 1) While DC-7 demonstrated the high-
est average accuracy in our quantitative evaluations, its training loss was also
the most substantial. This suggests that a lower training loss doesn’t necessar-
ily correlate directly with superior model performance. 2) DC-3, with a con-
siderable portion of its data coming from code domain, exhibited the lowest
training loss. This implies that as the amount of code in training increases, the
training loss diminishes. 3) The training loss values for other combinations
appeared to be relatively consistent with one another.
12

--- PAGE 13 ---
020k40k60k80k100k120k2.02.53.03.5
DC-1DC-5DC-3DC-4DC-7140kFigure 3: Illustration of training loss curves. DC-2’s curve closely resembles
those of DC-4 and 5, so it has been excluded from the figure for clarity.
6 Related Work
6.1 RedPajama, SlimPajama and Others.
RedPajama [7] aims to develop open-source large language models and be-
gins by replicating the LLaMA training dataset [37], which boasts over 1.2 tril-
lion tokens. This collaborative effort involves entities such as Together, Onto-
cord.ai, ETH DS3Lab, Stanford CRFM, Hazy Research, and the MILA Qu ´ebec
AI Institute. SlimPajama [34] stands as the highly deduplicated, multi-source,
open-source dataset tailored for training large language models. This dataset
emerged by refining and eliminating duplicates from the whole 1.2T token
RedPajama dataset. Through meticulous filtering of subpar data and repeti-
tive content, it reduced the dataset size by 49.6%, scaling it down from 1.2T
to 627B tokens. SlimPajama provides superior quality and computational ef-
ficiency for training tasks than the original RedPajama dataset. Other efforts
also have been made in this direction to construct diverse datasets, such as
Pile [13]. It is an English text corpus of 825 GiB, which is designed for the train-
ing of large-scale language models with increased training dataset diversity to
improve general cross-domain knowledge and downstream generalization ca-
pability. It contains a combination of 22 distinct, high-quality subsets. These
subsets incorporate both pre-existing and freshly curated data, with a signifi-
cant portion sourced from scholarly or professional domains.
6.2 Data Processing and Optimization Approaches
There have been several advancements in data processing and optimization.
The seminal method of importance sampling [21] stands out as a Monte Carlo
approach designed to evaluate attributes of a particular distribution, even when
the samples are drawn from a distribution that differs from the one under ex-
ploration. SlimPajama’s deduplication mechanism is an adaptation of impor-
tance sampling, incorporating a heuristic that values unique data points. Re-
cently, several data selection frameworks [19, 16, 35, 41] have been introduced,
inspired by the concept of importance sampling. Among them, DSIR [41]
13

--- PAGE 14 ---
presents a framework for the data selection challenge by aiming to choose a
subset from a large, unlabeled raw dataset that aligns with a specific target
distribution, given a set of unlabeled target examples. It builds upon the tra-
ditional importance resampling method, adapting it for data selection in large-
scale models. DSIR operates as a scalable algorithm, determining importance
weights within a reduced feature space and then selecting data based on these
importance resampling weights. In [35], it delves into the relationship between
error scaling and dataset size. Its theoretical exploration suggests that by us-
ing a robust data pruning metric, which prioritizes which training examples
to remove, the proposed method can suppress traditional power law scaling,
potentially reaching exponential scaling for pruned dataset sizes.
6.3 Data Combination for Training Large Language Models
The training of large language models, such as GPT [30, 31, 4] and BERT [10],
requires significant amounts of data to capture and generalize over the vast in-
tricacies of human language. As a result, researchers often combine data from
various sources, such as web text, Github, Books, ArXiv, Wikipedia, etc. There
are some related work and difficulties that have been explored in the context
of data combination for training large language models. (1) Concatenation of
diverse datasets: One of the simplest methods for combining data is to concate-
nate various corpora, covering diverse topics, styles, and sources. This ensures
that the model gets a broad view of the language. (2) WebText and similar cor-
pora: For OpenAI’s GPT-2, a dataset called WebText [31] was curated by scrap-
ing content from the internet. This kind of data provides a rich mix of formal,
informal, factual, and opinionated text, thus offering diverse training material.
(3) Balancing and weighting: Simply combining data may lead to issues if one
source is overrepresented. Prior studies have applied weights to different data
portions or ensure that the combined dataset is balanced in terms of sources,
styles, and other criteria. For instance, DoReMi [40] first trains a small proxy
model using group distributionally robust optimization across domains, gen-
erating domain weights (or mixture proportions) without relying on informa-
tion from subsequent tasks. Following this, they utilize these domain weights
to resample a dataset, on which then train a full-size model. (4) Multimodal
Training: Combining text with other data forms, like images or sounds, can
also enhance language model training, especially for tasks that require under-
standing across modalities.
6.4 Large Batch Training for Large Language Models
Large language models inherently possess a structure that supports paralleliza-
tion, especially when optimized using techniques that allow for batch training.
When computational resources permit, large batch sizes are favored to expe-
dite the training of large models containing potentially millions or billions of
parameters. At a fundamental level, larger batch sizes enhance the quality of
each gradient update since they consider a more considerable chunk of the
14

--- PAGE 15 ---
dataset. Conversely, a smaller batch size means that model parameter updates
are based on gradients derived from a limited dataset portion. This smaller
dataset slice might not comprehensively capture the intricate relationships be-
tween features and labels. Therefore, it might seem that larger batch sizes con-
sistently offer advantages in training. However, [20] pointed out that this per-
spective does not factor in the model’s capacity to generalize to new, unseen
data, nor the intricate, non-convex optimization landscape of contemporary
large models. In practice, multiple studies [18, 20] have demonstrated that
while larger batch sizes might hasten convergence, they can impair a model’s
generalization to new datasets, irrespective of the deep network type. This ob-
served disparity has been named as the Generalization Gap . A method [18] to
address this gap involves starting from a smaller batch size and gradually en-
larging it as training advances. In our study, we explore this problem through
a new and unique angle of progressive weight decay training.
7 Conclusion
We have presented SlimPajama-DC , a comprehensive study on understanding
the data domain weights and combinations for training large language mod-
els. Notably, SlimPajama-DC can operate on compact models, and its advan-
tages can be seamlessly transferred to models that are several times larger. This
leads to a remarkable acceleration in training on the SlimPajama with the op-
timal sampling probabilities across domains for larger models. Through this,
we aim to spark further exploration into data-centric methods to enhance the
understanding and efficiency of large language model pretraining.
References
[1] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,
Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large
language models across training and scaling. In International Conference on Machine
Learning , pages 2397–2430. PMLR, 2023. 7, 11
[2] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Lau-
rence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al.
Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint
arXiv:2204.06745 , 2022. 9
[3] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo:
Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, Mar. 2021.
If you use this software, please cite it using these metadata. 11
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. Language models are few-shot learners. Advances in neural informa-
tion processing systems , 33:1877–1901, 2020. 7, 8, 14
[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-
15

--- PAGE 16 ---
tian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv
preprint arXiv:2204.02311 , 2022. 22
[6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord. Think you have solved question answering? try
arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 , 2018. 10
[7] Together Computer. Redpajama: An open source recipe to reproduce llama train-
ing dataset, 2023. 3, 7, 11, 13
[8] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAt-
tention: Fast and memory-efficient exact attention with IO-awareness. In Advances
in Neural Information Processing Systems , 2022. 21
[9] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language mod-
eling with gated convolutional networks. In International conference on machine
learning , pages 933–941. PMLR, 2017. 9
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding, 2019. 14
[11] Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria,
Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language
models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208 ,
2023. 8, 11
[12] Nathan Habib Sheon Han Nathan Lambert Nazneen Rajani Omar Sanseviero
Lewis Tunstall Thomas Wolf Edward Beeching, Cl ´ementine Fourrier. Open llm
leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_
llm_leaderboard , 2023. 10, 11, 24
[13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The
pile: An 800gb dataset of diverse text for language modeling. arXiv preprint
arXiv:2101.00027 , 2020. 7, 13
[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-
ter, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason
Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy
Zou. A framework for few-shot language model evaluation, Sept. 2021. 10, 11, 24
[15] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind
Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang,
et al. Olmo: Accelerating the science of language models. arXiv preprint
arXiv:2402.00838 , 2024. 11
[16] Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,
Doug Downey, and Noah A Smith. Don’t stop pretraining: Adapt language mod-
els to domains and tasks. arXiv preprint arXiv:2004.10964 , 2020. 13
[17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt. Measuring massive multitask language understand-
ing. In International Conference on Learning Representations , 2021. 10
[18] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: clos-
ing the generalization gap in large batch training of neural networks. Advances in
neural information processing systems , 30, 2017. 15, 22
[19] Angelos Katharopoulos and Franc ¸ois Fleuret. Not all samples are created equal:
Deep learning with importance sampling. In International conference on machine
learning , pages 2525–2534. PMLR, 2018. 13
[20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization
gap and sharp minima. arXiv preprint arXiv:1609.04836 , 2016. 15, 22
16

--- PAGE 17 ---
[21] Teun Kloek and Herman K Van Dijk. Bayesian estimates of equation system pa-
rameters: an application of integration by monte carlo. Econometrica: Journal of the
Econometric Society , pages 1–19, 1978. 13
[22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos
Mu˜noz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf,
Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of
permissively licensed source code. Preprint , 2022. 21
[23] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. Mining of massive
data sets . Cambridge university press, 2020. 7
[24] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how mod-
els mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: Long Papers) , pages 3214–3252, 2022.
10
[25] Zhuang Liu, Zhiqiu Xu, Joseph Jin, Zhiqiang Shen, and Trevor Darrell. Dropout
reduces underfitting. In ICML , 2023. 23
[26] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Dan S Weld. S2orc:
The semantic scholar open research corpus. arXiv preprint arXiv:1911.02782 , 2019.
21
[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv
preprint arXiv:1711.05101 , 2017. 9
[28] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated
corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 , 2023.
5, 7, 8
[29] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with
linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409 ,
2021. 9
[30] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving
language understanding by generative pre-training. 2018. 14
[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
blog, 1(8):9, 2019. 14
[32] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
et al. Scaling language models: Methods, analysis & insights from training gopher.
arXiv preprint arXiv:2112.11446 , 2021. 7
[33] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 ,
2020. 9
[34] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel
Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and dedu-
plicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama ,
2023. 13
[35] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
Beyond neural scaling laws: beating power law scaling via data pruning. Advances
in Neural Information Processing Systems , 35:19523–19536, 2022. 13, 14
[36] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate lan-
guage and compiler for tiled neural network computations. In Proceedings of the
17

--- PAGE 18 ---
3rd ACM SIGPLAN International Workshop on Machine Learning and Programming
Languages , pages 10–19, 2019. 21
[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971 , 2023. 3, 7, 13, 22
[38]https://github.com/mosaicml/llm-foundry . Llm foundry. Mosaicml ,
2023. 22
[39]https://www.mosaicml.com/blog/mpt-7b . Introducing mpt-7b: A new
standard for open-source, commercially usable llms. Mosaicml blog , 2023. 3, 21,
22
[40] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu,
Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data
mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429 ,
2023. 14
[41] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection
for language models via importance resampling. arXiv preprint arXiv:2302.03169 ,
2023. 13
[42] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hel-
laswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830 ,
2019. 10
[43] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-
source small language model. arXiv preprint arXiv:2401.02385 , 2024. 11
[44] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 7,
11
18

--- PAGE 19 ---
Appendix
A Data Proportion Details
Dataset Slimpajama Redpajama LLaMA 1
Commoncrawl 52.2% 333B 72.6% 878B 67.0% 670/938B
C4 26.7% 170B 14.4% 175B 15.0% 150/210B
GitHub 5.2% 33B 4.9% 59B 4.5% 45/63B
Books 4.2% 27B 2.1% 26B 4.5% 45/63B
ArXiv 4.6% 29B 2.3% 28B 2.5% 25/35B
Wikipedia 3.8% 24B 2.0% 24B 4.5% 45/63B
StackExchange 3.3% 21B 1.7% 20B 2.0% 20/28B
Total 100.0% 637B 100.0% 1.2T 100% 1.0/1.4T
RefinedWeb GPT3 MassiveText
Commoncrawl 100% 600B 60.0% 180B 0.0% 0
C4 0.0% 0B 0.0% 0 10.0% 30B
GitHub 0.0% 0B 0.0% 0 3.0% 9B
Books 0.0% 0B 16.0% 48B 27.0% 81B
Wikipedia 0.0% 0B 3.0% 9B 2.0% 6B
WebText2 0.0% 0B 22.0% 66B 0.0% 0
MassiveWeb 0.0% 0B 0.0% 0 48.0% 144B
News 0.0% 0B 0.0% 0 10.0% 30B
Total 100.0% 600B 100.0% 300B 100.0% 300B
Table 8: Detailed data source proportions for various datasets.
B MMLU
In this section, we provide the detailed item-by-item results in MMLU, as
shown in Table 9, it is interesting to notice that on some sub-domains in MMLU,
the results from our configured 1.3B models are even better than GPT-3 175B
and LLaMA2 7B models.
19

--- PAGE 20 ---
GPT-3 Llama2SlimPajama-DC 1.3B
175B 7B DC-1 DC-2 DC-3 DC-4 DC-5 DC-6
Abstract Algebra STEM 30.0 29.0 27.0 26.0 25.0 27.0 28.0 23.0
Anatomy STEM 48.0 37.0 23.0 23.0 27.4 34.1 25.9 28.1
Astronomy STEM 49.0 33.6 25.0 19.7 23.0 27.0 21.7 23.7
Business Ethics Other 46.0 40.0 24.0 22.0 26.0 24.0 30.0 25.0
Clinical Knowledge Other 48.0 35.1 30.2 26.8 24.9 18.9 25.7 24.2
College Biology STEM 45.0 37.5 23.6 24.3 27.1 25.7 23.6 22.9
College Chemistry STEM 26.0 32.0 26.0 19.0 29.0 19.0 21.0 21.0
College Computer Science STEM 46.0 29.0 37.0 36.0 32.0 36.0 33.0 39.0
College Mathematics STEM 34.5 33.0 35.0 29.0 31.0 25.0 21.0 30.0
College Medicine Other 48.0 30.6 26.0 23.1 26.0 27.8 26.6 22.0
College Physics STEM 28.0 26.5 24.5 24.5 21.6 22.6 24.5 14.7
Computer Security STEM 57.0 45.0 24.0 30.0 19.0 27.0 28.0 34.0
Conceptual Physics STEM 36.5 36.6 27.7 30.2 22.1 28.5 23.8 28.9
Econometrics Social Science 33.0 23.7 24.6 25.4 30.7 23.7 24.6 21.9
Electrical Engineering STEM 50.0 26.9 29.0 24.1 26.2 29.0 23.5 28.3
Elementary Mathematics STEM 30.0 24.3 26.2 25.9 27.5 25.1 25.9 24.9
Formal Logic Humanities 29.0 27.0 35.7 24.6 20.6 16.7 15.9 28.6
Global Facts Other 37.0 29.0 30.0 31.0 30.0 37.0 33.0 23.0
High School Biology STEM 48.0 34.5 25.8 26.5 25.5 24.8 24.8 29.0
High School Chemistry STEM 33.0 28.1 27.6 19.7 27.1 27.1 24.1 16.7
High School Computer Science STEM 39.0 31.0 29.0 26.0 26.0 27.0 25.0 28.0
High School European History Humanities 54.0 44.2 23.6 28.5 24.9 26.7 25.5 26.7
High School Geography Social Science 58.0 34.3 34.3 20.7 19.2 17.7 22.2 21.2
High School Government And Politics Social Science 58.0 44.6 35.2 16.6 25.9 21.8 21.8 32.6
High School Macroeconomics Social Science 40.5 35.4 34.4 25.9 22.8 24.6 23.8 36.2
High School Mathematics STEM 28.0 24.8 26.7 25.2 28.5 26.7 25.2 29.6
High School Microeconomics Social Science 42.0 31.9 23.5 23.1 25.2 21.4 25.2 30.7
High School Physics STEM 28.0 26.5 27.8 26.5 27.2 29.8 21.9 25.8
High School Psychology Social Science 61.0 47.3 32.3 23.1 22.9 23.7 23.8 22.9
High School Statistics STEM 30.5 35.2 21.3 21.3 22.2 23.2 19.9 45.4
High School Us History Humanities 53.0 39.7 24.5 21.6 24.5 27.5 24.5 25.5
High School World History Humanities 56.0 40.9 29.1 25.7 27.4 25.7 24.5 24.9
Human Aging Other 50.0 40.8 14.8 30.5 30.5 27.4 37.2 24.7
Human Sexuality Social Science 54.0 36.6 28.2 22.1 22.1 25.2 22.9 22.1
International Law Humanities 55.5 51.2 26.5 30.6 32.2 30.6 39.7 26.4
Jurisprudence Humanities 55.0 38.9 26.9 22.2 27.8 25.0 26.9 24.1
Logical Fallacies Humanities 48.0 39.3 19.6 27.0 23.9 27.6 29.5 24.5
Machine Learning STEM 31.0 23.2 17.9 33.0 28.6 30.4 23.2 25.9
Management Other 56.0 35.0 26.2 29.1 21.4 23.3 27.2 22.3
Marketing Other 60.0 46.6 22.2 24.4 25.2 28.2 23.9 27.4
Medical Genetics Other 40.0 43.0 27.0 24.0 22.0 23.0 24.0 33.0
Miscellaneous Other 60.0 42.4 22.5 27.5 29.3 26.2 27.6 24.6
Moral Disputes Humanities 44.5 40.2 29.5 25.7 24.9 24.0 24.9 26.9
Moral Scenarios Humanities 26.0 24.3 27.3 24.6 23.8 24.6 24.3 24.2
Nutrition Other 47.0 37.6 28.1 23.2 25.8 25.8 25.2 25.5
Philosophy Humanities 51.0 39.9 28.0 28.9 29.3 28.3 26.7 30.2
Prehistory Humanities 53.0 36.1 26.5 25.9 26.9 27.5 29.3 27.2
Professional Accounting Other 33.0 25.9 27.0 29.1 27.3 27.0 27.0 22.3
Professional Law Humanities 34.5 30.2 27.1 25.0 24.6 26.9 25.8 26.9
Professional Medicine Other 36.0 44.5 19.9 31.6 21.0 27.9 22.8 44.5
Professional Psychology Social Science 44.5 35.1 26.3 27.3 25.2 27.5 25.5 25.8
Public Relations Social Science 48.0 40.9 33.6 30.9 29.1 26.4 28.2 20.0
Security Studies Social Science 52.0 31.8 39.2 17.5 21.2 16.3 18.8 37.6
Sociology Social Science 53.0 46.8 25.4 24.4 24.9 23.9 22.9 21.4
Us Foreign Policy Social Science 69.0 46.0 27.0 31.0 25.0 28.0 24.0 23.0
Virology Other 46.0 30.1 21.7 30.1 27.1 28.3 31.3 29.5
World Religions Humanities 55.0 50.9 27.5 25.2 29.8 32.2 32.8 35.1
Humanities 40.6 34.0 27.1 25.8 26.2 26.4 26.9 27.0
STEM 36.7 30.5 26.5 25.8 26.1 27.1 24.4 27.3
Social Science 50.5 38.3 30.3 24.0 24.5 23.3 23.6 26.8
Other 49.0 38.1 24.6 27.1 25.9 26.5 27.8 26.8
All 43.9 35.1 27.0 25.7 25.7 26.0 25.6 26.9
Table 9: MMLU. 5-shot results per domain on the test sets.
20

--- PAGE 21 ---
C Application: Large Batch-size Training on 7B
C.1 7B Training Data Combination
Our 7B large batch size (LBS) training dataset is primarily based on Slimpa-
jama, however, to obtain a sufficient proportion of web text, we have incor-
porated additional web data from the Commoncrawl corpus in RedPajama.
We have also adjusted the proportions of various data sources in line with our
1.3B model training. For instance, we elevate the sampling frequency of Github
and Wikipedia and increase the diversity of data sources by adding S2orc [26]
and Stack-Markdown [22] following [39], as detailed in Table 10. It’s crucial to
understand that our primary focus is not solely on achieving the best perfor-
mance. Instead, we place a higher emphasis on optimizing data combinations
and ensuring the convergence of training large language models with large
batch sizes. Consequently, we continue to utilize the SlimPajama/RedPajama
Commoncrawl instead of higher-quality RefinedWeb.
dataset proportion
Slimpj.Arxiv 4% (54B)
Slimpj.StackExchanges 3.2% (43B)
Slimpj.Github 4.9% (66B)
Slimpj.Wikipedia 7.5% (101B)
Slimpj.Books 4.3% (57B)
Slimpj.C4 17.6% (236B)
S2orc 3% (40B)
Markdown 3% (40B)
Slimpj.CC 34.5% (462B)
Redpaj.CC (ext.) 18% (241B)
Total 1.34T
Table 10: Data combination of 7B model training in large batch size style.
C.2 7B Model Training Configurations
Model nparams nlayers dmodel nheads dheads batch size learning rate
GPT-3 6.7B 32 4,096 32 128 2M 1.2×10-4
LLaMA 6.7B 32 4,096 32 128 4M 3.0×10-4
Our LBS 6.7B 32 4,096 32 128 14.3M 1.8×10-4
Table 11: Detailed model sizes, architectures, and optimization hyper-
parameters.
Architecture. For the LBS model training, we adopt MPT architecture [39], the
max sequence length is 2,048. We use Triton [36] with Flash Attention [8] as the
self-attention implementation. Alibi is enabled to make model more flexible
for input length extrapolation. The total number of parameters is 6.7B.
Tokenizer. The tokenizer used for 7B training is adapted GPT-NeoX-20b. Fol-
lowing [39], the model’s vocabulary size is adjusted to 50,432 for improved
21

--- PAGE 22 ---
mfu and leaving a few tokens available that can be used in subsequent train-
ing.
Optimizer. We employ the AdamW optimizer to train our models, adopting
these specific hyper-parameters: β1set at 0.9 and β2at 0.95. We adopt a learn-
ing rate schedule that traces a cosine pattern, concluding with a learning rate
that is 10% of its maximum value. Along with this, we use a multi-stage weight
decay scheduler as described in Section C.4, cap the gradient with a clipping
value of 1.0, and use a warmup spanning 2,000 steps.
System and platform. For our 7B model training with a large batch size, we
use 232 NVIDIA A100 GPUs (80G). We employ llm-foundry [38] as the training
platform. We use FSDP with activation checkpointing enabled to save memory
consumption. We also use automatic mixed precision of bf16 in training.
C.3 Fast Training with Large Batch-size
Large batch training allows a larger learning rate, leading to a faster conver-
gence of large models. Also, utilizing a larger batch size can optimize hardware
resource usage to make training procedures more efficient. Additionally, fewer
batches are required, which further accelerates the training process. As shown
in Table 12, our large batch training scheme achieves much higher throughput
and mfu than LLaMA [37] and MPT [39] with fewer total training GPU hours.
Overall, in a convex optimization framework, leveraging a larger portion of
the dataset typically leads to enhanced results. However, for most large deep
models that involve non-convex optimizations, the precise nature of the loss
landscape remains elusive, making the scenario more intricate. Many prior
works [18, 20] have noticed that training with larger batches often results in
overfitting compared to those using smaller batch sizes for the same network.
When utilizing large batch training, there is a propensity for the model to be-
come stuck or even gravitate towards potential saddle points within the loss
landscape. While large batch training methods often focus on the nearest rel-
ative minima they encounter, networks trained with smaller batches usually
navigate the loss landscape more thoroughly before committing to an optimal
minimum. The minima reached through large batch training can be distinctly
different from those achieved with smaller batch training methods. In the fol-
lowing, we introduce an approach to mitigate overfitting when training large
language models in a large batch-size scheme.
model batch size # GPUs (A100-80G) throughput mfu GPU-hours
LLaMA-7B 4M – – – 82,432
MPT-7B 4M 232 3,310 0.4575 84.351
LBS-7B (ours) 14M 232 3,626 0.5011 76,999
Table 12: Training speed of throughput (tokens per sec on each GPU), model
FLOPs utilization (mfu) [5] and total GPU-hours (per trillion training tokens).
22

--- PAGE 23 ---
C.4 Progressive Training on Weight Decay
WD=0WD=0.5WD=0.1
Figure 4: Loss curve of our LBS-7B training.
Prior work [25] observed that dropout operation is utilized only in the early
stages of training and is deactivated in subsequent phases. Models that incor-
porate this early dropout strategy tend to exhibit reduced final training loss
compared to models that do not use dropout. In contrast to this, our approach
emphasizes the role of weight decay during large model training. We intro-
duce a novel training strategy for large language models, wherein the training
process is segmented into various stages. Within each stage, a distinct weight
decay is applied to the model to serve specific objectives. We’ve termed this
approach Progressive Training on Weight Decay (PTWD). Owing to this method-
ology, our model, even when trained with a large batch size and extremely
small iterations, achieves smooth convergence. As illustrated in Fig. 4, our
training strategy consists of three distinct phases. Initially, we negate weight
decay by setting it to zero and allow the model to train until full convergence
is achieved. It usually can reach a lower loss level within this stage compared
to using weight decay, even if it slightly overfits. Following this, in the sec-
ond phase, we introduce a substantial weight decay, with a value of 0.5 in our
experiments, to suppress the overfitting. Once the loss values stabilize, we
transition to the third phase, wherein a standard weight decay of 0.1 is imple-
mented, a value consistent with many other LLMs training. Intriguing, each
phase spontaneously converges to roughly 1/3 of the total training budget,
ensuring effective allocation of training budget throughout the process.
C.5 Results of Pre-training and Instruction T uning
The results from our pretraining and subsequent instruction tuning on ShareGPT
dataset are presented in Table 13. Notably, after instruction tuning, there is a
significant enhancement in MMLU and TruthfulQA metrics. In contrast, the
performance on ARC and HellaSwag has a slight decrease. On the whole, the
average accuracy has a substantial boost following instruction tuning. More
evaluation results on the pretrained LBS model are provided in Table 6.
23

--- PAGE 24 ---
Model Average ARC HellaSwag MMLU TruthfulQA
Ours-LBS-7B-Base 44.1 44.3 69.8 26.1 36.1
Ours-LBS-7B-Instruct 46.4 43.5 68.0 32.1 42.1
Table 13: Results of our large batch-size (LBS) trained 7B models following
Huggingface Leaderboard Evaluation [12] using Harness [14].
24
