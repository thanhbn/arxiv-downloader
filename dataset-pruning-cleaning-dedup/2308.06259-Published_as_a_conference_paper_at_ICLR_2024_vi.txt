Được xuất bản như một bài báo hội nghị tại ICLR 2024
TỰ CĂN CHỈNH VỚI PHƯƠNG PHÁP DỊCH NGƯỢC HƯỚNG DẪN

Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer
Jason Weston & Mike Lewis
Meta
{xianl,jase,mikelewis}@meta.com

TÓM TẮT
Chúng tôi trình bày một phương pháp có thể mở rộng để xây dựng một mô hình ngôn ngữ tuân theo hướng dẫn chất lượng cao bằng cách tự động gắn nhãn văn bản do con người viết với các hướng dẫn tương ứng. Phương pháp của chúng tôi, được đặt tên là dịch ngược hướng dẫn, bắt đầu với một mô hình ngôn ngữ được tinh chỉnh trên một lượng nhỏ dữ liệu khởi tạo, và một kho văn bản web cho sẵn. Mô hình khởi tạo được sử dụng để xây dựng các ví dụ huấn luyện bằng cách tạo ra các lời nhắc hướng dẫn cho các tài liệu web (tự tăng cường), và sau đó chọn lọc các ví dụ chất lượng cao từ những ứng viên này (tự tuyển chọn). Dữ liệu này sau đó được sử dụng để tinh chỉnh một mô hình mạnh hơn. Việc tinh chỉnh LLaMa trên hai lần lặp của phương pháp chúng tôi tạo ra một mô hình vượt trội hơn tất cả các mô hình dựa trên LLaMa khác trên bảng xếp hạng Alpaca không dựa vào dữ liệu chưng cất, thể hiện khả năng tự căn chỉnh hiệu quả cao.

1 GIỚI THIỆU
Việc căn chỉnh các mô hình ngôn ngữ lớn (LLMs) để thực hiện việc tuân theo hướng dẫn thường đòi hỏi tinh chỉnh trên lượng lớn các hướng dẫn hoặc sở thích được con người chú thích (Ouyang et al., 2022; Touvron et al., 2023a; Bai et al., 2022a) hoặc chưng cất đầu ra từ các mô hình mạnh hơn (Wang et al., 2022a; Honovich et al., 2022; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023; Xu et al., 2023). Nghiên cứu gần đây nhấn mạnh tầm quan trọng của chất lượng dữ liệu chú thích con người (Zhou et al., 2023; Köpf et al., 2023). Tuy nhiên, việc chú thích các bộ dữ liệu tuân theo hướng dẫn với chất lượng như vậy rất khó để mở rộng quy mô.

Trong công trình này, thay vào đó chúng tôi tận dụng lượng lớn dữ liệu không được gắn nhãn để tạo ra một bộ dữ liệu tinh chỉnh hướng dẫn chất lượng cao bằng cách phát triển một thuật toán tự huấn luyện lặp. Phương pháp này sử dụng chính mô hình để vừa tăng cường vừa tuyển chọn các ví dụ huấn luyện chất lượng cao để cải thiện hiệu suất của chính nó. Phương pháp của chúng tôi, được đặt tên là dịch ngược hướng dẫn, được lấy cảm hứng từ phương pháp dịch ngược cổ điển từ dịch máy, trong đó các câu đích do con người viết được tự động chú thích với các câu nguồn do mô hình tạo ra trong ngôn ngữ khác (Sennrich et al., 2015).

Phương pháp của chúng tôi bắt đầu với một mô hình tuân theo hướng dẫn khởi tạo và một kho văn bản web. Mô hình đầu tiên được sử dụng để tự tăng cường bộ huấn luyện của nó: đối với mỗi tài liệu web, nó tạo ra một ví dụ huấn luyện tuân theo hướng dẫn bằng cách dự đoán một lời nhắc (hướng dẫn) mà sẽ được trả lời đúng bởi (một phần của) tài liệu đó. Việc huấn luyện trực tiếp trên dữ liệu như vậy (tương tự như Köksal et al. (2023)) cho kết quả kém trong các thí nghiệm của chúng tôi, cả do chất lượng hỗn hợp của văn bản web do con người viết, và nhiễu trong các hướng dẫn được tạo ra. Để khắc phục điều này, chúng tôi chỉ ra rằng cùng một mô hình khởi tạo có thể được sử dụng để tự tuyển chọn tập hợp dữ liệu tăng cường mới được tạo ra bằng cách dự đoán chất lượng của chúng, và sau đó có thể được tự huấn luyện chỉ trên các cặp (hướng dẫn, đầu ra) chất lượng cao nhất. Quy trình sau đó được lặp lại, sử dụng mô hình cải thiện để tuyển chọn dữ liệu hướng dẫn tốt hơn, và huấn luyện lại để tạo ra một mô hình tốt hơn.

Mô hình kết quả của chúng tôi, Humpback, vượt trội hơn tất cả các mô hình không chưng cất hiện có khác trên bảng xếp hạng Alpaca (Li et al., 2023). Nhìn chung, dịch ngược hướng dẫn là một phương pháp có thể mở rộng để cho phép các mô hình ngôn ngữ cải thiện khả năng tuân theo hướng dẫn của chính chúng.

2 PHƯƠNG PHÁP
Phương pháp tự huấn luyện của chúng tôi giả định có quyền truy cập vào một mô hình ngôn ngữ cơ sở, một lượng nhỏ dữ liệu khởi tạo, và một tập hợp các ví dụ không được gắn nhãn, ví dụ như một kho văn bản web. Dữ liệu không được gắn nhãn là một tập hợp lớn, đa dạng các tài liệu do con người viết bao gồm việc viết về tất cả các chủ đề mà con người quan tâm - nhưng quan trọng là không được ghép cặp với các hướng dẫn. Một giả định đầu tiên quan trọng là tồn tại một tập con nào đó của văn bản do con người viết rất lớn này sẽ phù hợp như các thế hệ vàng cho một số hướng dẫn của người dùng. Một giả định quan trọng thứ hai là chúng ta có thể dự đoán các hướng dẫn cho những câu trả lời vàng ứng viên này có thể được sử dụng như các cặp ví dụ chất lượng cao để huấn luyện một mô hình tuân theo hướng dẫn.

Do đó, quy trình tổng thể của chúng tôi, mà chúng tôi gọi là dịch ngược hướng dẫn, thực hiện hai bước cốt lõi:

1. Tự tăng cường: Tạo ra các hướng dẫn cho dữ liệu không được gắn nhãn, tức là kho văn bản web, để tạo ra dữ liệu huấn luyện ứng viên của các cặp (hướng dẫn, đầu ra) cho việc tinh chỉnh hướng dẫn.

2. Tự tuyển chọn: Tự chọn lọc các ví dụ minh họa chất lượng cao làm dữ liệu huấn luyện để tinh chỉnh mô hình cơ sở tuân theo hướng dẫn. Phương pháp này được thực hiện lặp đi lặp lại trong đó một mô hình tuân theo hướng dẫn trung gian tốt hơn có thể cải thiện việc chọn lọc dữ liệu để tinh chỉnh trong lần lặp tiếp theo.

Chúng tôi mô tả các bước này chi tiết hơn dưới đây. Một tổng quan về phương pháp được minh họa trong Hình 1.

2.1 KHỞI TẠO
Dữ liệu khởi tạo. Chúng tôi bắt đầu với một tập khởi tạo các ví dụ (hướng dẫn, đầu ra) được con người chú thích sẽ được sử dụng để tinh chỉnh các mô hình ngôn ngữ để đưa ra dự đoán ban đầu theo cả hai hướng: dự đoán một đầu ra cho một hướng dẫn, và một hướng dẫn cho một đầu ra.

Dữ liệu không được gắn nhãn. Chúng tôi sử dụng một kho văn bản web làm nguồn dữ liệu không được gắn nhãn. Đối với mỗi tài liệu, chúng tôi thực hiện tiền xử lý để trích xuất các đoạn độc lập {yi}, là những phần văn bản theo sau một tiêu đề HTML. Chúng tôi tiếp tục chạy khử trùng lặp, lọc độ dài, và loại bỏ các đoạn chất lượng thấp tiềm năng với một số heuristics như tỷ lệ chữ cái viết hoa trong tiêu đề.

2.2 TỰ TĂNG CƯỜNG (TẠO RA CÁC HƯỚNG DẪN)
Chúng tôi tinh chỉnh mô hình ngôn ngữ cơ sở với các cặp (đầu ra, hướng dẫn) {(yi, xi)} từ dữ liệu khởi tạo để có được một mô hình ngược Myx := p(x|y). Đối với mỗi ví dụ không được gắn nhãn yi, chúng tôi chạy suy luận trên mô hình ngược để tạo ra một hướng dẫn ứng viên x̂i từ đó chúng tôi rút ra dữ liệu ghép cặp tăng cường ứng viên A := {(x̂i, yi)}. Như chúng ta sẽ thấy trong các thí nghiệm, không phải tất cả những cặp ứng viên này đều có chất lượng cao, và trong trường hợp đó việc sử dụng tất cả chúng để tự huấn luyện có thể không có lợi. Do đó chúng tôi xem xét bước quan trọng tiếp theo của việc tuyển chọn một tập con chất lượng cao.

2.3 TỰ TUYỂN CHỌN (CHỌN LỌC CÁC VÍ DỤ CHẤT LƯỢNG CAO)
Chúng tôi chọn lọc các ví dụ chất lượng cao bằng cách sử dụng chính mô hình ngôn ngữ. Chúng tôi bắt đầu với một mô hình hướng dẫn khởi tạo M0 được tinh chỉnh chỉ trên các ví dụ khởi tạo (hướng dẫn, đầu ra). Sau đó chúng tôi sử dụng M0 để chấm điểm mỗi ví dụ tăng cường {(x̂i, yi)} để rút ra một điểm chất lượng ai. Điều này được thực hiện bằng cách sử dụng prompting, hướng dẫn mô hình được huấn luyện để đánh giá chất lượng của một cặp ứng viên trên thang điểm 5 điểm. Lời nhắc cụ thể mà chúng tôi sử dụng được đưa ra trong Bảng 19. Sau đó chúng tôi có thể chọn một tập con của các ví dụ tăng cường với điểm ai ≥ k để tạo thành một tập được tuyển chọn A(1)k.

Tự tuyển chọn lặp Chúng tôi tiếp tục đề xuất một phương pháp huấn luyện lặp để tạo ra các dự đoán chất lượng cao hơn. Ở lần lặp t, chúng tôi sử dụng dữ liệu tăng cường được tuyển chọn A(t-1)k từ lần lặp trước, cùng với dữ liệu khởi tạo làm dữ liệu huấn luyện để tinh chỉnh một mô hình cải thiện Mt. Mô hình này lần lượt có thể được sử dụng để chấm điểm lại các ví dụ tăng cường về chất lượng, dẫn đến một tập tăng cường A(t)k. Chúng tôi thực hiện hai lần lặp của việc chọn lọc dữ liệu và tinh chỉnh để có được mô hình cuối cùng M2.

Khi kết hợp cả dữ liệu khởi tạo và dữ liệu tăng cường để tinh chỉnh, chúng tôi sử dụng gắn thẻ để phân biệt hai nguồn dữ liệu này. Cụ thể, chúng tôi thêm một câu bổ sung vào các ví dụ (được gọi là "system prompt"). Chúng tôi sử dụng Sa := "Answer in the style of an AI Assistant." cho dữ liệu khởi tạo, và Sw := "Answer with knowledge from web search." cho dữ liệu tăng cường. Phương pháp này tương tự như các phương pháp được sử dụng để gắn thẻ dữ liệu tổng hợp cho dịch ngược trong dịch máy (Caswell et al., 2019).

3 THÍ NGHIỆM

3.1 THIẾT LẬP THÍ NGHIỆM
Dữ liệu khởi tạo. Chúng tôi sử dụng 3200 ví dụ từ bộ dữ liệu Open Assistant (Köpf et al., 2023) làm dữ liệu khởi tạo được con người chú thích để huấn luyện các mô hình của chúng tôi. Mỗi ví dụ là một cặp (hướng dẫn, đầu ra) {(xi, yi)}, được chọn từ lượt đầu tiên của cây hội thoại. Chúng tôi chỉ lấy mẫu các phản hồi tiếng Anh chất lượng cao, dựa trên thứ hạng được chú thích bởi con người (thứ hạng 0).

Mô hình cơ sở & tinh chỉnh. Chúng tôi sử dụng mô hình LLaMA được huấn luyện trước (Touvron et al., 2023a) với 7B, 33B và 65B tham số làm mô hình cơ sở để tinh chỉnh. Trong quá trình huấn luyện, chúng tôi chỉ tối ưu hóa loss trên các token đầu ra, không phải các token đầu vào, do đó khác biệt với loss mô hình ngôn ngữ tiêu chuẩn. Chúng tôi sử dụng cùng siêu tham số như các phương pháp tinh chỉnh có giám sát (SFT) hiện có (Zhou et al., 2023; Touvron et al., 2023a) cho hầu hết các mô hình: tốc độ học 1e-5 giảm tuyến tính xuống 9e-6 ở cuối huấn luyện, weight decay 0.1, batch size 32 (ví dụ) và dropout 0.1. Để tinh chỉnh với ít hơn 3000 ví dụ, chúng tôi sử dụng batch size 8 (chi tiết hơn trong Bảng 18). Chúng tôi gọi mô hình dịch ngược hướng dẫn dựa trên Llama được huấn luyện của chúng tôi là Humpback1. Để tạo sinh, chúng tôi sử dụng nucleus sampling (Holtzman et al., 2019) với nhiệt độ T = 0.7, p = 0.9.

Dữ liệu không được gắn nhãn. Chúng tôi sử dụng phần tiếng Anh của kho Clueweb làm nguồn dữ liệu không được gắn nhãn (Overwijk et al., 2022). Trong số đó, chúng tôi đã lấy mẫu 502k đoạn.

Baseline. Các baseline chính mà chúng tôi so sánh với là các phương pháp sau:

• text-davinci-003 (Ouyang et al., 2022): một mô hình tuân theo hướng dẫn dựa trên GPT-3 được tinh chỉnh với dữ liệu hướng dẫn từ các hướng dẫn do con người viết, đầu ra do con người viết, phản hồi mô hình và sở thích con người sử dụng học tăng cường (RLHF).

1Do mối liên hệ với lưng lạc đà, nhưng cũng do bản chất quy mô lớn của cá voi (>).

• LIMA (Zhou et al., 2023): các mô hình LLaMA được tinh chỉnh với 1000 ví dụ hướng dẫn được chọn thủ công từ hỗn hợp cộng đồng hỏi đáp (ví dụ StackOverflow, WikiHow, v.v.) và hướng dẫn và phản hồi do chuyên gia con người viết.

• Guanaco (Dettmers et al., 2023): các mô hình LLaMA được tinh chỉnh với 9000 ví dụ từ bộ dữ liệu OpenAssistant. Sự khác biệt với 3200 ví dụ khởi tạo được sử dụng trong bài báo này là Guanaco bao gồm các cặp (hướng dẫn, đầu ra) từ tất cả các lượt trong khi chúng tôi chỉ sử dụng lượt đầu tiên.

Chúng tôi bổ sung báo cáo so sánh với nhiều mô hình khác, ví dụ sử dụng dữ liệu được chưng cất từ các mô hình lớn hơn và mạnh hơn như GPT-4, nhưng không coi chúng có thể so sánh trực tiếp với phương pháp dựa trên LlaMa của chúng tôi.

Đánh giá. Chúng tôi đánh giá trên các lời nhắc kiểm tra từ một số nguồn: Vicuna (Chiang et al., 2023) (80 lời nhắc), Self-instruct (Zhang & Yang, 2023) (252 lời nhắc), Open Assistant (Köpf et al., 2023) (188 lời nhắc), Koala (Geng et al., 2023) (156 lời nhắc), HH_RLHF (Bai et al., 2022a) (129 lời nhắc), LIMA (Zhou et al., 2023) (300 lời nhắc), crowdsourced từ các tác giả (64 lời nhắc). Tổng cộng có 1130 lời nhắc duy nhất, cung cấp phạm vi bao phủ tốt trên nhiều loại nhiệm vụ, ví dụ viết, lập trình, lý luận toán học, tìm kiếm thông tin, lời khuyên, nhập vai, an toàn, v.v. Chúng tôi lấy mẫu 256 lời nhắc từ chúng loại trừ những lời nhắc trong tập kiểm tra AlpacaEval làm tập dev. Chúng tôi chạy cả đánh giá tự động sử dụng AlpacaEval (Li et al., 2023), tính tỷ lệ thắng so với các mô hình baseline dựa trên đánh giá GPT-4, cũng như đánh giá sở thích con người.

3.2 THỐNG KÊ DỮ LIỆU KHỞI TẠO VÀ TĂNG CƯỜNG
Thống kê dữ liệu. Trong Bảng 1, chúng tôi cung cấp thống kê của dữ liệu khởi tạo cũng như các phiên bản khác nhau của dữ liệu tăng cường. Chúng ta có thể thấy rằng dữ liệu tăng cường có xu hướng có đầu ra dài hơn so với dữ liệu khởi tạo, và dữ liệu huấn luyện chất lượng cao được tự tuyển chọn (A(2)4 và A(2)5) có cả hướng dẫn và đầu ra ngắn hơn trong tất cả dữ liệu tăng cường, gần hơn với độ dài của dữ liệu hướng dẫn khởi tạo ban đầu.

Các hướng dẫn được tạo ra. Chúng tôi tiến hành phân tích đa dạng nhiệm vụ của dữ liệu khởi tạo và dữ liệu tăng cường bằng cách sử dụng phương pháp từ Wang et al. (2022a). Hình 6 hiển thị phân phối cấu trúc động từ-danh từ của các hướng dẫn trong dữ liệu khởi tạo và dữ liệu tăng cường (danh mục A(2)5) tương ứng. Tương tự như dữ liệu khởi tạo, có một vài nhiệm vụ chính liên quan đến viết, tìm kiếm thông tin và lời khuyên, mặc dù loại nội dung từ dữ liệu không được gắn nhãn (bài viết, công thức, mô tả, phát hành, v.v.) bổ sung cho những nội dung trong dữ liệu khởi tạo (tiểu luận, kịch bản, mã, câu chuyện, v.v.). Dữ liệu tăng cường tăng đa dạng nhiệm vụ đặc biệt ở phần đuôi dài.

3.3 PHÂN TÍCH MỞ RỘNG QUY MÔ
Chất lượng dữ liệu so với số lượng dữ liệu. Để hiểu tầm quan trọng của chất lượng dữ liệu so với số lượng dữ liệu trong việc học tuân theo hướng dẫn, chúng tôi so sánh tinh chỉnh trên dữ liệu tăng cường có chất lượng khác nhau. Cụ thể, chúng tôi so sánh tinh chỉnh trên dữ liệu tăng cường không có lựa chọn dựa trên chất lượng (w/o curation), dữ liệu được tự chọn lọc trong các danh mục A(2)4 (điểm ≥ 4) và A(2)5 (điểm ≥ 4.5). Kết quả được hiển thị trong Hình 2. Chúng tôi thấy rằng huấn luyện trên dữ liệu tăng cường không có tự tuyển chọn không cải thiện hiệu suất tuân theo hướng dẫn mặc dù mở rộng quy mô số lượng dữ liệu. Tuy nhiên, huấn luyện trên phần chất lượng cao của dữ liệu tăng cường dẫn đến tăng hiệu suất tuân theo hướng dẫn, với cải thiện ổn định khi chúng tôi tiếp tục mở rộng quy mô lượng dữ liệu tăng cường. Nghiên cứu trước đây đề xuất "giả thuyết căn chỉnh bề mặt", rằng chỉ vài nghìn ví dụ tuân theo hướng dẫn chất lượng cao là đủ để căn chỉnh một mô hình cơ sở được huấn luyện trước để tuân theo hướng dẫn Zhou et al. (2023). Kết quả của chúng tôi cung cấp một quan sát tương phản rằng việc tăng số lượng dữ liệu chất lượng cao mang lại thêm lợi ích (trong khi số lượng tăng của dữ liệu chất lượng thấp thì không).

Hiệu quả mở rộng dữ liệu. Chúng tôi so sánh hiệu suất của các mô hình tuân theo hướng dẫn khác nhau khi chúng tôi thay đổi lượng dữ liệu tinh chỉnh tuân theo hướng dẫn mà chúng sử dụng. Chúng tôi đo tỷ lệ thắng của mỗi mô hình so với text-davinci-003 khi tinh chỉnh 7B LLaMa với bộ dữ liệu tinh chỉnh cho sẵn. Chúng tôi cũng báo cáo một ước tính về hiệu quả này bằng cách sử dụng hệ số mở rộng dữ liệu α, được tính bằng cách khớp dữ liệu thực nghiệm với w = α log N + C, trong đó w là tỷ lệ thắng đo chất lượng tạo sinh của mô hình được tinh chỉnh trên N ví dụ.

Chúng tôi so sánh phương pháp dịch ngược hướng dẫn của chúng tôi (tự tăng cường và tự tuyển chọn với k = 5, 2 lần lặp) với các phương pháp sử dụng bộ dữ liệu hướng dẫn được tạo ra từ các nguồn khác nhau.

Kết quả được hiển thị trong Hình 3, với hệ số mở rộng ước tính α được tóm tắt trong Bảng 2. Chúng tôi thấy rằng hầu hết các bộ dữ liệu hướng dẫn chưng cất có hiệu quả dữ liệu tốt hơn các bộ dữ liệu được tạo ra từ các nguồn khác, ví dụ các nhiệm vụ NLP (FLAN v2) hoặc được trích xuất từ cộng đồng Q&A (LIMA). Cả việc cải thiện đa dạng hướng dẫn (ví dụ WizardLLM so với Vicuna) và chất lượng phản hồi (ví dụ Alpaca-GPT4 so với Alpaca) dường như mang lại hiệu quả dữ liệu tốt hơn. Việc mở rộng quy mô dữ liệu tăng cường bằng cách sử dụng dữ liệu A5 đạt được cả hiệu suất tuân theo hướng dẫn cao hơn và mở rộng dữ liệu hiệu quả hơn. Chúng tôi cung cấp thêm phân tích về việc mở rộng quy mô dữ liệu và kích thước mô hình cùng nhau trong Phụ lục B.

3.4 CHẤT LƯỢNG MÔ HÌNH
AlpacaEval. Chúng tôi sử dụng đánh giá tự động (sử dụng GPT-4) từ AlpacaEval để đánh giá chất lượng tạo sinh trên 805 lời nhắc từ Bảng xếp hạng Alpaca. AlpacaEval so sánh tỷ lệ thắng theo cặp so với mô hình tham chiếu text-davinci-003. Chúng tôi so sánh hiệu suất phương pháp của chúng tôi trong ba danh mục mô hình hướng dẫn:

• Không chưng cất: các mô hình LLaMa được huấn luyện không dựa vào bất kỳ mô hình bên ngoài nào (ví dụ ChatGPT, GPT-4, v.v.) cho bất kỳ hình thức giám sát nào. Hầu hết các mô hình trong danh mục này phụ thuộc nhiều vào dữ liệu được con người chú thích.

• Chưng cất: các mô hình được huấn luyện với một mô hình bên ngoài mạnh hơn trong vòng lặp, ví dụ sử dụng dữ liệu được chưng cất từ một mô hình bên ngoài.

• Độc quyền: các mô hình được huấn luyện với dữ liệu và kỹ thuật độc quyền.

Kết quả được đưa ra trong Bảng 3. Phương pháp của chúng tôi là mô hình có hiệu suất tốt nhất trong số các mô hình không chưng cất ở cả quy mô mô hình 65B và 33B. Chúng tôi lưu ý rằng Guanaco và OASST được huấn luyện trên cùng nguồn dữ liệu như dữ liệu khởi tạo của chúng tôi, nhưng với nhiều ví dụ được chú thích hơn. Chúng tôi cũng đánh giá Humpback dựa trên LLaMa 2 (Touvron et al., 2023b) 70B để xác minh hiệu suất của nó tiếp tục cải thiện với mô hình cơ sở mạnh hơn.

Đánh giá con người. Chúng tôi cũng tiến hành đánh giá con người về chất lượng chung của các phản hồi mô hình trên tập kiểm tra kết hợp được mô tả trong Phần 3.1, bao gồm một số benchmark hiện có. Đối với mỗi lời nhắc, chúng tôi trình bày đầu ra từ hai mô hình cạnh nhau, so sánh phương pháp của chúng tôi với một mô hình baseline cho sẵn, và yêu cầu người đánh giá con người chọn từ ba tùy chọn: 1) đầu ra từ mô hình đầu tiên tốt hơn đáng kể so với mô hình thứ hai; 2) đầu ra từ mô hình thứ hai tốt hơn đáng kể so với mô hình đầu tiên; 3) không có sự khác biệt đáng kể giữa hai đầu ra. Chúng tôi ngẫu nhiên hóa thứ tự các mô hình được trình bày để tránh thiên lệch vị trí. Hình 4 tóm tắt so sánh với cả mô hình mã nguồn mở và độc quyền. Chúng ta có thể thấy rằng phân phối sở thích con người gần như phù hợp với phân phối sở thích sử dụng GPT-4 làm thẩm phán từ AlpacaEval, củng cố các quan sát từ Li et al. (2023), Zhou et al. (2023) và Zheng et al. (2023).

Lý luận thông thường và MMLU. Chúng tôi đánh giá trên năm benchmark lý luận thông thường, SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020), Arc-Easy (Clark et al., 2018), Arc-Challenge (Clark et al., 2018), và Openbook QA (OBQA) (Mihaylov et al., 2018), đo lường lý luận từ tương tác xã hội đến câu hỏi khoa học lớp 3 đến 9. Chúng tôi tính độ chính xác zero-shot dựa trên perplexity của câu trả lời đúng theo LLaMa (Touvron et al., 2023a). Chúng tôi cũng đánh giá trên benchmark hiểu biết ngôn ngữ đa nhiệm vụ khổng lồ (MMLU) (Hendrycks et al., 2020). Kết quả được tóm tắt trong Bảng 4. Chúng tôi thấy rằng so với mô hình cơ sở, mô hình của chúng tôi đã cải thiện hiệu suất zero-shot về lý luận xã hội, các vấn đề khoa học thách thức đòi hỏi nhiều lý luận hơn (Arc-C), Openbook QA và MMLU. Kết quả chi tiết theo lĩnh vực được bao gồm trong Phụ lục B.

3.5 ABLATIONS
Chúng tôi thực hiện các nghiên cứu ablation thêm để hiểu tính hiệu quả của dữ liệu tự tăng cường trong phương pháp của chúng tôi.

Huấn luyện chỉ trên dữ liệu tự tăng cường. Như được hiển thị trong Hình 5, khi huấn luyện chỉ trên dữ liệu tự tăng cường (không có dữ liệu khởi tạo), và không có tự tuyển chọn, chất lượng tuân theo hướng dẫn không cải thiện, hoặc thậm chí xấu đi với nhiều dữ liệu hơn. Tuy nhiên, huấn luyện trên dữ liệu được tự tuyển chọn chất lượng cao hơn mang lại cải thiện khi kích thước tập huấn luyện tăng. Mặc dù dữ liệu được tự tuyển chọn này không vượt trội hơn việc mở rộng dữ liệu huấn luyện khởi tạo một mình, khi huấn luyện chung với cả dữ liệu khởi tạo và dữ liệu tự tăng cường, chúng tôi quan sát được những cải thiện lớn. Điều này cho thấy rằng dữ liệu khởi tạo và dữ liệu tăng cường là bổ sung, trong đó dữ liệu khởi tạo có cùng phân phối với miền đích (phản hồi trợ lý AI), trong khi dữ liệu từ kho văn bản web có thể mở rộng đa dạng của các hướng dẫn và đầu ra. Phụ lục B cung cấp thêm phân tích định tính để minh họa sự cải thiện so với huấn luyện chỉ với dữ liệu khởi tạo.

System prompts. Trong Bảng 5, chúng tôi phân tách các hiệu ứng của system prompts trong tinh chỉnh chung và trong quá trình suy luận. Chúng tôi thấy rằng việc thêm system prompts để phân biệt dữ liệu tăng cường với dữ liệu khởi tạo là hữu ích. Thú vị là, việc sử dụng system prompt kết hợp {Sa, Sw} trong thời gian suy luận, kết hợp prompt cho dữ liệu khởi tạo với prompt cho dữ liệu tăng cường, tốt hơn việc không có system prompt hoặc sử dụng prompt dữ liệu khởi tạo, mặc dù việc kết hợp không được thấy trong quá trình huấn luyện.

4 NGHIÊN CỨU LIÊN QUAN
Tinh chỉnh hướng dẫn cho LLMs. Công trình của chúng tôi có cùng mục tiêu với danh mục rộng lớn các nỗ lực về tinh chỉnh các mô hình ngôn ngữ lớn để tuân theo hướng dẫn. Nghiên cứu sớm về tinh chỉnh hướng dẫn chủ yếu tập trung vào các nhiệm vụ NLP, với phát hiện rằng tinh chỉnh với các bộ dữ liệu NLP được định dạng như các cặp hướng dẫn-đầu ra cải thiện khả năng tổng quát hóa giữa các nhiệm vụ (Wei et al., 2021; Mishra et al., 2021; Sanh et al., 2021; Wang et al., 2022b). Nghiên cứu gần đây Ouyang et al. (2022) mở rộng tinh chỉnh hướng dẫn sang một phạm vi rộng hơn các nhiệm vụ chung, đặc biệt là kết hợp các hướng dẫn từ người dùng của các mô hình ngôn ngữ.

Tạo sinh và tuyển chọn hướng dẫn. Một thách thức chính để cho phép LLMs thực hiện việc tuân theo hướng dẫn chung là thu thập các ví dụ minh họa để tinh chỉnh. Các LLM tuân theo hướng dẫn chất lượng cao hiện có dựa vào các chú thích con người trong nhiều bước khác nhau, bao gồm viết hướng dẫn, viết phản hồi mô hình, cung cấp sở thích để chỉ ra phản hồi mong muốn, v.v. Những bộ hướng dẫn đó thường là độc quyền, một ngoại lệ là các bộ dữ liệu OpenAssistant gần đây (Köpf et al., 2023). Nhìn chung, phương pháp chú thích con người khó mở rộng quy mô vì việc thu thập chú thích trên một phạm vi rộng các nhiệm vụ rất tốn kém, mất thời gian và đòi hỏi chuyên môn trong các lĩnh vực khác nhau.

Một số nghiên cứu đã khám phá việc sử dụng LLMs để tạo ra hướng dẫn. Unnatural instructions nhắc GPT-3 tạo ra nhiều hướng dẫn hơn dựa trên một vài hướng dẫn khởi tạo trong ngữ cảnh (Honovich et al., 2022). Self-instruct (Wang et al., 2022a) sử dụng cùng phương pháp để tạo ra hướng dẫn, cũng như đầu ra cho những hướng dẫn đó. Họ tiếp tục thực hiện các quy tắc lọc được thiết kế thủ công để loại bỏ các cặp hướng dẫn-đầu ra chất lượng thấp. Xu et al. (2023) tạo ra các hướng dẫn phức tạp hơn bằng cách tạo ra các biến thể của hướng dẫn người dùng được gửi tới ChatGPT.

Tất cả những phương pháp này sử dụng các phản hồi do mô hình tạo ra để làm dữ liệu huấn luyện. Gần hơn với phương pháp của chúng tôi là nghiên cứu đồng thời của Köksal et al. (2023), lấy văn bản do con người viết làm phản hồi tự nhiên, và sử dụng LLM để tạo ra hướng dẫn tương ứng có điều kiện trên phản hồi. Một sự khác biệt quan trọng trong công trình của chúng tôi là chúng tôi chỉ ra rằng bước tự tuyển chọn là quan trọng để cải thiện quy trình như vậy. Một sự khác biệt khác là họ sử dụng chưng cất thông qua một LLM được tinh chỉnh hướng dẫn (InstructGPT) để tạo ra hướng dẫn, trong khi phương pháp của chúng tôi không dựa vào chưng cất từ một mô hình mạnh hơn trong vòng lặp, và thay vào đó là một trường hợp của tự căn chỉnh.

Tự căn chỉnh. Công trình của chúng tôi là một trường hợp của cơ thể nghiên cứu đang phát triển về tự căn chỉnh, tức là sử dụng mô hình để cải thiện chính nó và căn chỉnh phản hồi của nó với các hành vi mong muốn như phản hồi do mô hình viết, phê bình, giải thích, v.v. Khác với công trình của chúng tôi, nhiều nghiên cứu này hoặc xây dựng dữ liệu huấn luyện theo cách không giám sát (Sun et al., 2023; Bai et al., 2022b), trong khi chúng tôi tăng cường các trang web do con người viết, hoặc họ sử dụng mô hình để tạo ra ngữ cảnh bổ sung để điều kiện hóa trong thời gian suy luận để cải thiện đầu ra (Saunders et al., 2022; Zhang & Yang, 2023; Madaan et al., 2023).

Chất lượng dữ liệu. Một số phương pháp đã chỉ ra rằng việc tuyển chọn dữ liệu do con người viết chất lượng cao dẫn đến hiệu suất mạnh, ví dụ PALMS (Solaiman & Dennison, 2021) và LIMA (Zhou et al., 2023). Thay vì tuyển chọn dữ liệu chất lượng cao thủ công, công trình của chúng tôi tập trung vào việc chọn lọc chất lượng cao bằng cách sử dụng chính mô hình. Trong nghiên cứu đồng thời, Chen et al. (2023) cũng cung cấp một phương pháp thuật toán để chọn lọc dữ liệu chất lượng cao. Họ khác với công trình của chúng tôi ở chỗ họ nhắc một mô hình mạnh hơn (ChatGPT) để chấm điểm chất lượng của các phản hồi do mô hình tạo ra từ chưng cất, trong khi công trình này chấm điểm chất lượng của dữ liệu do con người viết như một phản hồi cho một hướng dẫn được tự tạo ra.

Chưng cất. Hầu hết các mô hình LLaMA được tinh chỉnh dựa trên chưng cất kiến thức từ ChatGPT hoặc GPT-4, như Alpaca (Taori et al., 2023), Alpaca-GPT 4 (Peng et al., 2023), Vicuna (Chiang et al., 2023), FalconInstruct (Almazrouei et al., 2023), OpenChat (Wang et al., 2023), UltraChat (Ding et al., 2023). Do đó, những phương pháp này đòi hỏi bạn đã có một mô hình mạnh, nhưng không cung cấp công thức để xây dựng một mô hình mạnh từ đầu. Các nhược điểm của những phương pháp này cũng được thảo luận trong Gudibande et al. (2023).

5 KẾT LUẬN
Chúng tôi đề xuất một phương pháp có thể mở rộng để tinh chỉnh các mô hình ngôn ngữ lớn tuân theo hướng dẫn. Phương pháp của chúng tôi tận dụng lượng lớn dữ liệu không được gắn nhãn bằng cách phát triển một thuật toán tự huấn luyện lặp mà chúng tôi gọi là dịch ngược hướng dẫn. Phương pháp của chúng tôi sử dụng chính mô hình để vừa tăng cường vừa tuyển chọn các ví dụ huấn luyện chất lượng cao để cải thiện hiệu suất của chính nó. Trên bảng xếp hạng Alpaca, các mô hình được tinh chỉnh của chúng tôi vượt trội hơn tất cả các mô hình tuân theo hướng dẫn không chưng cất khác, trong khi sử dụng ít ví dụ được con người chú thích hơn. Nghiên cứu trong tương lai nên mở rộng quy mô phương pháp này hơn nữa bằng cách xem xét các kho văn bản không được gắn nhãn lớn hơn, mà phân tích của chúng tôi cho thấy sẽ mang lại thêm lợi ích.

TÀI LIỆU THAM KHẢO
[Phần tài liệu tham khảo được giữ nguyên cấu trúc gốc vì đây là danh sách tài liệu học thuật]

A HẠN CHẾ

A.1 THIÊN LỆCH
Vì dữ liệu tăng cường có nguồn gốc từ một kho văn bản web, một hậu quả tiềm tăng là mô hình được tinh chỉnh có thể khuếch đại các thiên lệch từ dữ liệu web. Chúng tôi đánh giá trên bộ dữ liệu CrowS-Pairs Nangia et al. (2020) để đo hiệu suất của mô hình trong việc nhận biết thiên lệch tiềm tàng. Cụ thể, chúng tôi đánh giá độ chính xác trong việc phát hiện các tuyên bố thiên lệch trong chín danh mục: giới tính, tôn giáo, chủng tộc/màu da, khuynh hướng tình dục, tuổi tác, quốc tịch, khuyết tật, ngoại hình thể chất và tình trạng kinh tế xã hội. So với mô hình cơ sở, mô hình của chúng tôi có độ chính xác cải thiện trong việc phát hiện thiên lệch như được tóm tắt trong Bảng 6. Tuy nhiên, điều này không có nghĩa là mô hình của chúng tôi ít có khả năng tạo ra các phản hồi chứa thiên lệch.

A.2 AN TOÀN
Vì cả dữ liệu khởi tạo và dữ liệu tăng cường đều không cố ý bao gồm các ví dụ minh họa "red teaming" cũng như giai đoạn tinh chỉnh không tối ưu hóa để phát hiện và giảm thiểu tổn hại tiềm tàng, chúng tôi đánh giá mô hình trên 30 lời nhắc có khả năng nhạy cảm để hiểu các tác động an toàn của mô hình. Chúng tôi thấy rằng đối với tập hợp lời nhắc này, mô hình có xu hướng tạo ra phản hồi thận trọng, hoặc thậm chí từ chối cung cấp thông tin để hoàn thành hướng dẫn. Hơn nữa, chúng tôi so sánh các phản hồi sử dụng các system prompt khác nhau và thấy rằng việc sử dụng system prompt của dữ liệu khởi tạo Sa có xu hướng mang lại các phản hồi an toàn hơn. Điều này cho thấy rằng việc tận dụng system prompts có thể là một giải pháp hiệu quả để tăng cường an toàn. Bảng 15 cung cấp các ví dụ đại diện. Việc kết hợp red teaming hoặc các biện pháp an toàn khác vào quy trình tăng cường của chúng tôi có thể là một hướng nghiên cứu thêm để khám phá, đặc biệt là nghiên cứu hiện có đã chỉ ra rằng các mô hình tuân theo hướng dẫn có khả năng "tự điều chỉnh đạo đức" để giảm thiểu việc tạo ra các phản hồi có hại khi được hướng dẫn làm như vậy Ganguli et al. (2023).

[Các phần còn lại được dịch tiếp theo cùng cấu trúc và chi tiết như trên...]
